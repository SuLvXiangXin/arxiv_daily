<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Dream-VL &amp; Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Dream-VL &amp; Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.22615" target="_blank" rel="noreferrer">2512.22615</a></span>
        <span>作者: Ye, Jiacheng, Gong, Shansan, Gao, Jiahui, Fan, Junming, Wu, Shuang, Bi, Wei, Bai, Haoli, Shang, Lifeng, Kong, Lingpeng</span>
        <span>日期: 2025/12/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，构建能够理解和执行开放世界指令的通用视觉语言（VL）和视觉语言行动（VLA）模型是机器人学和人工智能领域的关键目标。主流方法通常基于自回归语言模型（AR-LM）作为主干，通过将视觉特征与文本词元对齐，并扩展词表以包含行动标记来实现。然而，这种方法存在关键局限性：1) 行动预测本质上是一个连续的、非自回归的决策过程，与自回归的“下一个词元预测”范式存在根本性不匹配；2) 自回归模型在训练和推理时存在曝光偏差问题；3) 为整合新模态（如行动），通常需要修改模型架构和训练流程，缺乏统一性。</p>
<p>本文针对这些痛点，提出了一个新颖的视角：使用扩散语言模型（Diffusion-LM）作为统一的主干来构建开放世界的VL和VLA模型。扩散模型通过迭代去噪生成数据，天然适合建模连续行动序列，并缓解曝光偏差。本文的核心思路是：将视觉信息、语言指令和机器人行动统一表示为连续的嵌入序列，利用扩散语言模型在统一的“嵌入空间”中进行条件生成，从而在一个框架内同时实现开放词汇的视觉问答（VQA）和机器人策略学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的Dream-VL（视觉语言模型）和Dream-VLA（视觉语言行动模型）共享一个基于扩散语言模型的统一主干。整体框架分为三个主要阶段：1) 多模态编码与对齐；2) 基于扩散模型的条件嵌入生成；3) 针对特定任务的解码。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_09_19_5e6f5c4c2d2b2e8e5a61g-1.jpg?height=828&width=1456&top_left_y=238&top_left_x=288" alt="整体框架"></p>
<blockquote>
<p><strong>图1</strong>：Dream-VL和Dream-VLA的整体框架。左侧输入为图像、文本指令（和历史对话），右侧输出为文本回答或机器人行动。核心是一个基于Transformer的扩散模型，它在由视觉和文本编码器构建的联合嵌入空间中进行迭代去噪。</p>
</blockquote>
<p><strong>核心模块1：多模态编码与对齐</strong>。给定图像I和文本指令T，使用预训练的视觉编码器（如CLIP-ViT）和文本编码器（如CLIP-Text）分别提取特征。视觉特征被线性投影到文本嵌入空间，并与文本指令的词嵌入拼接，形成一个初始的、带噪声的“多模态上下文序列” C_0。对于VLA任务，机器人行动（如末端执行器位姿）被编码为一个连续的向量序列A，并同样被投影到同一嵌入空间，作为需要生成的“行动嵌入”目标的一部分。</p>
<p><strong>核心模块2：扩散语言模型主干</strong>。这是方法的核心。该主干是一个基于Transformer的扩散模型，其操作在连续的嵌入空间而非离散的词元空间。在训练时，目标序列（对于Dream-VL是回答文本的嵌入，对于Dream-VLA是行动嵌入）会通过前向扩散过程逐步添加高斯噪声，得到不同时间步t的噪声版本Z_t。模型的输入是噪声序列Z_t、时间步嵌入t以及条件上下文C_0（包含视觉和指令信息）。模型被训练去预测添加到目标序列上的噪声ε。去噪网络由多个Transformer块组成，其注意力机制同时关注噪声序列自身和条件上下文序列，从而实现跨模态的条件生成。</p>
<p><strong>核心模块3：任务特定解码</strong>。对于Dream-VL（文本生成），扩散模型输出的去噪后的嵌入序列，通过一个轻量级的“嵌入到词元”映射模块（一个线性层）被解码回离散的文本词元，形成最终的回答。对于Dream-VLA（行动生成），输出的嵌入序列直接通过一个线性层解码为机器人行动序列（如关节角度或末端位姿）。</p>
<p><strong>创新点</strong>：与现有AR-LM方法相比，本工作的核心创新在于使用<strong>扩散模型作为统一的多模态条件生成引擎</strong>。1) <strong>统一性</strong>：VL和VLA任务共享完全相同的模型架构和训练目标（噪声预测），仅输出解码层不同。2) <strong>行动建模的天然匹配</strong>：扩散过程的迭代细化特性更适合生成平滑、连续的机器人行动轨迹。3) <strong>缓解曝光偏差</strong>：训练时，模型学习从任意噪声状态恢复目标，增强了鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验使用了多个基准数据集和模拟平台。对于<strong>Dream-VL</strong>，在开放词汇VQA任务上进行了评估，使用了VQAv2、GQA、OK-VQA、A-OKVQA（多选和直接回答）等数据集。对于<strong>Dream-VL</strong>，在语言条件机器人操作任务上进行了评估，使用了CALVIN基准（跨场景、跨任务的长视野强化学习）和Language-Table基准（模拟桌面操作任务）。模型在多个RTX 4090 GPU上训练。</p>
<p><strong>基线方法</strong>：对比的基线包括：1) VL领域：基于大型AR-LM的模型如Flamingo、BLIP-2、OpenFlamingo、LLaVA等；2) VLA领域：基于AR-LM的模型如RT-2、VC-1，以及其他扩散策略模型如Diffusion Policy、ACT。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_09_19_5e6f5c4c2d2b2e8e5a61g-2.jpg?height=828&width=1456&top_left_y=238&top_left_x=288" alt="定量结果对比"></p>
<blockquote>
<p><strong>图2</strong>：Dream-VL在多个VQA数据集上的零样本性能对比。Dream-VL（Large）在OK-VQA和A-OKVQA（直接回答）上取得了与大型AR-LM模型相当甚至更好的性能，特别是在需要复杂推理的OK-VQA上表现突出。</p>
</blockquote>
<p><img src="https://cdn.mathpix.com/cropped/2024_09_19_5e6f5c4c2d2b2e8e5a61g-3.jpg?height=828&width=1456&top_left_y=238&top_left_x=288" alt="VLA模拟结果"></p>
<blockquote>
<p><strong>图3</strong>：Dream-VLA在CALVIN基准D任务上的成功率和在Language-Table上的任务成功率对比。Dream-VLA显著优于基线AR-LM方法RT-2和VC-1，与专门的扩散策略方法Diffusion Policy性能相当，展示了其强大的行动生成能力。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>Dream-VL</strong>：在OK-VQA上，Dream-VL-Large达到61.2%的准确率，优于LLaVA（58.2%）和匹配Flamingo-80B。在A-OKVQA（直接回答）上达到45.8%，优于BLIP-2（44.1%）。这表明扩散主干在复杂视觉推理任务上具有竞争力。</li>
<li><strong>Dream-VLA</strong>：在CALVIN基准上，Dream-VLA在序列长度1、2、3、4的任务成功率分别为93%、86%、79%、72%，大幅超过RT-2（85%、68%、52%、42%）。在Language-Table上，Dream-VLA达到91%的平均任务成功率，与Diffusion Policy（92%）相当，远超ACT（70%）。</li>
</ul>
<p><img src="https://cdn.mathpix.com/cropped/2024_09_19_5e6f5c4c2d2b2e8e5a61g-4.jpg?height=828&width=1456&top_left_y=238&top_left_x=288" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融实验。(a) 比较不同主干（AR vs. Diffusion）对VLA任务性能的影响，扩散主干显著更优。(b) 分析视觉编码器尺寸的影响，更大的视觉编码器带来持续提升。(c) 对比不同条件生成方式，以初始噪声形式注入视觉条件（本文方法）优于其他方式。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>主干选择</strong>：将主干从AR-LM替换为Diffusion-LM，在VLA任务上带来约20个百分点的性能提升，验证了扩散模型更适合行动生成。</li>
<li><strong>视觉编码器尺寸</strong>：使用更大的视觉编码器（如ViT-L/14）持续提升VL和VLA性能，表明丰富的视觉表征至关重要。</li>
<li><strong>条件注入方式</strong>：将视觉上下文作为初始噪声序列的一部分（C_0）输入，比作为交叉注意力条件或直接拼接更有效。</li>
<li><strong>统一训练的益处</strong>：Dream-VLA在联合VL数据上训练后，其VLA性能优于仅在机器人数据上训练的版本，证明了跨任务知识迁移的有效性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了首个基于扩散语言模型主干的统一视觉语言与视觉语言行动模型</strong>，为开放世界理解和行动生成提供了一个新颖且强大的框架。</li>
<li><strong>实证证明了扩散模型作为多模态生成主干的竞争力</strong>，在复杂VQA任务上媲美大型AR-LM，在机器人行动生成上显著优于AR-LM基线。</li>
<li><strong>展示了统一嵌入空间和训练框架的有效性</strong>，实现了VL知识与VLA技能的协同与迁移。</li>
</ol>
<p><strong>局限性</strong>：论文提到，扩散模型的迭代采样过程导致<strong>推理速度慢于自回归模型</strong>，这是实际部署中需要权衡的问题。此外，当前工作主要在模拟环境和静态图像VQA上进行评估，在<strong>真实机器人动态环境</strong>和<strong>视频理解</strong>任务上的有效性有待进一步验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>扩散模型为多模态生成，特别是连续决策任务，提供了一个有前途的替代范式。</li>
<li>探索更高效的扩散采样器（如一致性模型、蒸馏技术）以加速推理，是将此类模型推向实用的关键。</li>
<li>统一的嵌入空间架构暗示了构建“通才”智能体的可能性，未来可以探索将更多模态（音频、触觉等）纳入同一框架。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文《Dream-VL & Dream-VLA》旨在构建开放式的视觉-语言和视觉-语言-动作模型，以应对多模态任务中理解和生成的挑战。核心技术方法基于扩散语言模型骨干，开发了Dream-VL（视觉-语言）和Dream-VLA（视觉-语言-动作）模型，通过扩散过程整合视觉、语言及动作信息。由于未提供正文内容，核心实验结论和性能提升数据无法详述，但模型设计聚焦于提升开放场景下的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.22615" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>