<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.01765" target="_blank" rel="noreferrer">2509.01765</a></span>
        <span>作者: Stefan Lee Team</span>
        <span>日期: 2025-09-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在无缆机器人控制中，平衡任务性能与能量消耗至关重要。当前基于强化学习的主流方法是在奖励函数中直接添加能量惩罚项，即 $r = r_{\text{task}} + \lambda r_{\text{energy}}$，并通过调节权重系数 $\lambda$ 来权衡二者。然而，该方法存在关键局限性：最优的 $\lambda$ 值高度依赖于具体任务、环境和机器人本体，导致其难以调节。如图1所示，对于同一机器人的不同任务（如奔跑与行走），或不同机器人（如四足与双足），合适的 $\lambda$ 值差异巨大，不当的权重会导致任务性能严重下降或节能效果不佳。</p>
<p>本文针对能量最小化与任务性能之间难以权衡的痛点，提出了将两者视为具有优先级次序的多目标优化问题的新视角。其核心思路是：在策略梯度更新的每一步，将能量最小化梯度投影到任务奖励梯度的正交方向上，从而确保能量优化不会干扰任务性能的优化，实现“在不影响任务成功的前提下尽可能节能”的目标。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法 PEGrad (Projecting Energy Gradients) 的核心思想是修改策略梯度更新方向，使其在最小化能量的同时不影响任务奖励。整体框架基于多目标演员-评论家算法（如SAC），但策略更新机制不同。</p>
<p><strong>整体框架与输入输出</strong>：算法在标准策略梯度框架内运行。输入为策略参数 $\theta$ 和包含任务轨迹的批次数据 $\mathcal{B}$。算法分别计算任务奖励损失 $\mathcal{L}_R$ 和能量消耗损失 $\mathcal{L}_E$ 关于策略参数的梯度 $g_R$ 和 $g_E$。核心操作是修改 $g_E$，输出一个用于优化器更新的综合梯度方向 $\Delta\theta$。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>多目标设定与损失函数</strong>：采用两个独立的评论家网络分别估计任务动作值 $Q^r_{\phi}(s,a)$ 和能量动作值 $Q^e_{\phi_e}(s,a)$。对应的策略损失分解为：<br>$\mathcal{L}<em>{R}(\theta)=\mathbb{E}</em>{a\sim\pi_{\theta}}[\alpha\log\pi_{\theta}(a\mid s)-Q^{r}<em>{\phi}(s,a)]$ （最大化任务奖励与熵）<br>$\mathcal{L}</em>{E}(\theta)=\mathbb{E}<em>{a\sim\pi</em>{\theta}}[\alpha\log\pi_{\theta}(a\mid s)+Q_{\phi_{e}}^{e}(s,a)]$ （最小化能量消耗与熵）</p>
</li>
<li><p><strong>梯度投影</strong>：核心创新在于计算能量梯度 $g_E$ 在任务梯度 $g_R$ 方向上的正交投影 $g_{E_{\perp R}}$：<br>$g_{E_{\perp R}} = g_E - \frac{g_{R}^{T}g_{E}}{g_{R}^{T}g_{R}}g_{R}$<br>此操作移除了 $g_E$ 中与 $g_R$ 平行的分量，即可能改变任务奖励的部分。</p>
</li>
<li><p><strong>自适应缩放</strong>：为确保能量优化步骤的幅度不超过任务优化步骤，对投影后的梯度进行缩放：<br>如果 $||g_{E_{\perp R}}||<em>2 &gt; ||g_R||<em>2$，则令 $g</em>{E</em>{\perp R}} = g_{E_{\perp R}} \frac{||g_R||<em>2}{||g</em>{E_{\perp R}}||<em>2}$。<br>最终的参数更新方向为 $\Delta\theta = g_R + g</em>{E_{\perp R}}$。</p>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>优先级明确的梯度操作</strong>：不同于PCGrad等仅在检测到冲突（余弦相似度为负）时才进行投影的方法，PEGrad<strong>始终</strong>将能量梯度投影到任务梯度的正交空间，明确赋予任务目标更高优先级。</li>
<li><strong>超参数免费</strong>：避免了手动调节权衡系数 $\lambda$ 的需要。</li>
<li><strong>即插即用</strong>：算法1展示了PEGrad可作为一个梯度预处理模块，轻松集成到现有的策略梯度算法中，仅需额外一次反向传播计算梯度。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.01765v1/x1.png" alt="问题图示与PEGrad效果"></p>
<blockquote>
<p><strong>图1</strong>：传统加权奖励方法（不同λ）与PEGrad方法（★）的对比。左图：在四足狗奔跑任务中，λ=0.1导致策略性能严重下降（爬行而非奔跑）。右图：在双足人坐下任务中，λ=0.01和0.1都能得到节能且有效的策略，显示了λ的跨环境敏感性。PEGrad在这两种情况下均能自动找到高性能、低能耗的策略。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：DM-Control Suite（四足狗和四足机器人任务）、HumanoidBench（Unitree H1人形机器人任务）、IsaacLab（用于Sim2Real的四足机器人训练）。</li>
<li><strong>实验平台</strong>：MuJoCo物理模拟器。</li>
<li><strong>能量定义</strong>：使用施加于所有电机的绝对扭矩之和 $\sum |\tau|$ 作为能量消耗的代理指标，旨在反映电池电流负载。</li>
</ul>
<p><strong>对比基线</strong>：</p>
<ol>
<li><strong>Base</strong>：标准SAC，无显式能量惩罚（DM-Control中记为λ=0）或使用环境默认奖励（HumanoidBench）。</li>
<li>**Multi-Objective (λ=X)**：双评论家SAC，使用加权损失 $\mathcal{L}<em>{\pi} = \mathbb{E} [\alpha\log\pi</em>{\theta}(a\mid s) - Q^{r}<em>{\phi}(s,a) + \lambda Q</em>{\phi_{e}}^{e}(s,a)]$，λ ∈ {0.001, 0.01, 0.1, 0.5}。</li>
<li>**PCGrad+**：将PCGrad方法适配到本设定（任务优先），仅在梯度冲突时投影能量梯度。</li>
</ol>
<p><strong>关键实验结果</strong>：<br>在模拟实验中，PEGrad在保持与Base方法相当的任务回报的同时，平均降低了 <strong>64%</strong> 的能量消耗（以扭矩和衡量）。具体地，在DM-Control的6个任务中，PEGrad在4个任务上取得了超越任何λ值所能达到的帕累托前沿的性能（图2）。在HumanoidBench的4个任务中，PEGrad不仅节能，在行走和奔跑任务上还表现出更高的<strong>样本效率</strong>（图3）。PCGrad+在所有任务中均表现不佳，因过度优化能量而导致任务失败。</p>
<p><img src="https://arxiv.org/html/2509.01765v1/x2.png" alt="DM-Control结果"></p>
<blockquote>
<p><strong>图2</strong>：DM-Control六个任务的帕累托前沿图（扭矩越低、回报越高越好）。PEGrad在所有任务中均实现了高回报与低能耗。在四足行走、奔跑及狗的小跑、奔跑四个任务中，PEGrad的结果点（★）位于所有λ基线构成的帕累托前沿之外，表明其找到了更好的权衡点。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.01765v1/x3.png" alt="HumanoidBench结果"></p>
<blockquote>
<p><strong>图3</strong>：HumanoidBench四个任务的结果。左列：帕累托前沿；中列：样本效率（回报vs步数）；右列：能量效率（扭矩vs步数）。PEGrad实现了高性能与低能耗。在h1-run和h1-walk任务中，PEGrad和λ=0.1的基线还显示出更快的收敛速度（样本效率提升）。</p>
</blockquote>
<p><strong>Sim2Real 真实机器人实验</strong>：<br>在Unitree GO2四足机器人上部署了用于站立和行走（SaW）的策略，并与工厂控制器及调优的AMP+PPO λ基线对比。</p>
<p><img src="https://arxiv.org/html/2509.01765v1/x4.png" alt="真实世界电流与扭矩数据"></p>
<blockquote>
<p><strong>表1</strong>：真实世界站立与行走任务的电流消耗和净扭矩应用对比。PEGrad策略在行走任务中，比手动调优的最佳基线（λ=0.0002）电流消耗降低 **19.74%**，净扭矩降低 **15.8%**，且任务性能相当。在站立任务中，两者性能接近。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>PEGrad</strong>，一种超参数免费的梯度优化方法，通过将能量梯度投影到任务梯度的正交空间，在不影响任务性能的前提下最小化能量消耗。</li>
<li>在多个模拟机器人控制基准上验证了其有效性，平均降低64%能量使用。</li>
<li>成功实现了从模拟到真实世界（Sim2Real）的策略迁移，在真实四足机器人上显著降低了电池电流消耗。</li>
</ol>
<p><strong>局限性</strong>：论文提及，所使用的能量代理指标 $\sum |\tau|$ 忽略了不同电机和齿轮比的异质性，若已知这些参数，可加入适当的系数进行改进。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>提供了一种多目标RL的新范式</strong>：通过梯度操作明确处理目标间的优先级，而非简单加权，这可以扩展到其他具有主次关系的多目标问题。</li>
<li><strong>方法简洁易用</strong>：PEGrad实现为即插即用的梯度修改模块，易于集成到现有RL流程中，促进了其在更广泛机器人控制任务中的应用。</li>
<li><strong>启发了对能量模型的研究</strong>：将更精确的能耗模型（如考虑电机效率、电池模型）与PEGrad框架结合，可能进一步优化真实世界的能效。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>这篇论文解决了强化学习训练机器人控制策略时，能量最小化目标与任务性能目标易发生冲突的问题。提出了一种名为PEGrad的无超参数梯度优化方法，其核心是通过在任务目标和能量目标之间进行策略梯度投影，推导出能最小化能耗且不影响任务性能的策略更新。实验表明，该方法在DM-Control和HumanoidBench基准测试中，能在保持任务性能的同时将能量消耗降低64%，并成功实现了从仿真到真实四足机器人的策略迁移。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.01765" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>