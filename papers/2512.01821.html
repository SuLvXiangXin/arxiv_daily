<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.01821" target="_blank" rel="noreferrer">2512.01821</a></span>
        <span>作者: Cao, Meng, Lin, Haokun, Li, Haoyuan, Tang, Haoran, Xu, Rongtao, An, Dong, Liu, Xue, Reid, Ian, Liang, Xiaodan</span>
        <span>日期: 2025/12/01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前提升多模态大语言模型（MLLMs）空间推理能力的主流方法是口头描述调优（verbal descriptive tuning）。该方法通过构建空间导向的数据集，指导MLLMs以纯文本形式描述空间属性（如相对方向、物体距离）。然而，该范式存在“视觉盲点”（visual illiteracy）问题：模型仅通过文本符号监督学习空间概念，从未接触过空间变换在视觉域中的实际表现形式，导致其常常无法关注到正确的感兴趣区域。</p>
<p>本文针对MLLMs在空间推理中符号与感知割裂这一具体痛点，提出了“隐式空间世界建模”（Implicit Spatial World Modeling）的新视角，旨在模拟人类基于感知经验进行空间想象和心智模拟的认知过程。本文的核心思路是：提出MILO范式，通过集成视觉生成器来提供几何感知的反馈，从而将MLLMs的符号推理隐式地锚定在感知经验中；同时，提出一种相对位置编码（RePE）方案来捕获相机姿态的相对变换，以替代对绝对坐标系的依赖。</p>
<h2 id="方法详解">方法详解</h2>
<p>MILO的整体框架（Pipeline）包含一个用于多模态理解的MLLM 𝒫、一个用于视觉反馈生成的视频扩散模型 𝒟，以及一个用于维度调整的中间连接器。其训练流程分为两个阶段：首先进行基于几何感知变换的视觉生成调优，然后进行口头微调。输入为带有相机参数的视频帧序列和变换指令（如“向右旋转”），MLLM输出隐藏的视觉嵌入，扩散模型以此嵌入为条件生成目标视觉序列作为反馈。</p>
<p><img src="https://arxiv.org/html/2512.01821v2/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：MILO整体框架。由用于多模态理解的MLLM 𝒫、用于视觉反馈生成的视频扩散模型 𝒟，以及中间的连接器组成。相对位置编码（RePE）以相对相机姿态变化为输入，生成高维3D感知位置嵌入。</p>
</blockquote>
<p>核心模块一：相对位置编码（RePE）。该模块旨在为MLLM注入3D感知，同时避免依赖全局绝对坐标系。给定第i帧的相机内参矩阵 𝑲_i 和在世界坐标系中的姿态 𝑻_i^cw，RePE计算其与前一帧（i≥2）的相对几何变换矩阵 𝑮_i（公式1）。对于首帧（i=1），则相对于一个参考相机姿态（𝑲_ref=I, 𝑻_ref^cw=I）进行计算（公式2）。𝑮_i 编码了相邻帧间的相对旋转、平移和内参变换。</p>
<p><img src="https://arxiv.org/html/2512.01821v2/x4.png" alt="RePE示意图"></p>
<blockquote>
<p><strong>图4</strong>：RePE示意图。给定两个连续帧 𝑰<em>i 和 𝑰</em>{i-1}，相对几何变换矩阵 𝑮_i 按公式(1)计算。然后根据公式(3)和(4)将其转换为高维嵌入 𝑬_i^rel。</p>
</blockquote>
<p>随后，将 𝑮_i 的每个元素通过正弦位置编码函数 Φ 投影到高维（公式3），并将所有投影分量拼接，得到第i帧的相对位置嵌入 𝑬_i^rel（公式4）。最终，该嵌入与CLIP提取的2D语义嵌入 𝑬_i^2D 相加，形成3D感知的视觉表示 𝑬_i^3D（公式5），输入MLLM。</p>
<p>核心模块二：两阶段训练流程。</p>
<ol>
<li><strong>视觉生成调优</strong>：此阶段目标是通过视觉反馈监督，让MLLM内化几何变换在视觉域中的表现。以MLLM输出的隐藏视觉嵌入和变换指令 𝑿^q 为条件，视频扩散模型 𝒟 被优化去预测添加到目标视觉序列潜在表示 𝒛_0 中的噪声（公式6）。损失函数 ℒ_MILO 为标准扩散模型的均方误差损失。</li>
<li><strong>口头微调</strong>：在视觉生成调优之后，使用标准的自回归文本损失 ℒ_text（公式8）对模型进行口头描述任务的微调，指令 𝑿^q 变为空间描述性问题（如“相对方向是什么”）。</li>
</ol>
<p>与现有方法相比，MILO的核心创新点在于：1）提出了视觉生成调优范式，通过显式的几何变换指令和视觉结果反馈，在符号推理和视觉感知之间建立隐式关联；2）提出了坐标无关的RePE，显式建模多视图间的几何关系，提升了泛化性。</p>
<p>为支持训练，本文构建了GeoGen数据集，包含约2，241个视频和267，827个“观察-动作-结果”三元组。数据来自扫描的3D场景（ScanNet， ScanNet++）和网络视频（RoomTour3D）。数据标注围绕两个几何感知任务展开：</p>
<ul>
<li><strong>新颖视图合成</strong>：给定参考帧和方向指令，模型输出对应的视频帧。</li>
<li><strong>轨迹生成</strong>：将视频帧视为图节点，基于相机距离和遮挡检测构建边，通过A*搜索找出起点到终点的最短路径作为轨迹。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.01821v2/x6.png" alt="GeoGen数据示例"></p>
<blockquote>
<p><strong>图6</strong>：GeoGen数据集中（a）新颖视图合成和（b）轨迹生成的示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.01821v2/x5.png" alt="轨迹生成pipeline"></p>
<blockquote>
<p><strong>图5</strong>：轨迹生成的流程示意图。每个带有几何标注的视频基于帧间的空间连通性被转换为相机校准图。采用A*算法寻找从起点到目标点的最短路径。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在多个基准上验证MILO的有效性，并基于三个不同的基线MLLM进行构建：Video-3D LLM、VG-LLM和RoboRefer。使用的评估数据集包括：用于3D场景理解的ScanRefer、Multi3DRefer、Scan2Cap、ScanQA、SQA3D；用于空间推理的VSI-Bench；用于具身指代的RefSpatial-Bench。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>3D场景理解</strong>：如表1所示，以Video-3D LLM为基线，MILO在全部五个基准的所有指标上均取得一致提升。例如，在ScanRefer上<a href="mailto:&#65;&#x63;&#x63;&#64;&#48;&#46;&#50;&#x35;">&#65;&#x63;&#x63;&#64;&#48;&#46;&#50;&#x35;</a>达到61.3%，比基线提升3.2%；在Scan2Cap上<a href="mailto:&#x42;&#45;&#52;&#x40;&#x30;&#46;&#53;">&#x42;&#45;&#52;&#x40;&#x30;&#46;&#53;</a>达到47.5%，提升6.2%。</li>
<li><strong>空间推理</strong>：如表2所示，以VG-LLM为基线，MILO在VSI-Bench上的平均准确率达到61.7%，比基线提升2.2%。在涉及空间方向和距离的子任务上提升尤为显著，如相对方向（+5.7%）、相对距离（+3.3%）、绝对距离（+3.7%）。</li>
<li><strong>具身指代</strong>：如表3所示，以RoboRefer为基线，MILO在RefSpatial-Bench的三个子集（位置、放置、未见过的组合空间关系）上均取得竞争力结果，在“位置”和“未见过的”子集上分别有1.00%和1.30%的绝对提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.01821v2/x7.png" alt="视觉对比结果"></p>
<blockquote>
<p><strong>图7</strong>：生成结果（顶部）与真实值（底部）的视觉对比。展示了模型在几何变换指令下生成合理视觉序列的能力。</p>
</blockquote>
<p><strong>消融实验分析</strong>：如表4所示，消融研究验证了各组件贡献。</p>
<ul>
<li><strong>视觉生成调优</strong>（Exp#2）：移除该阶段导致性能全面下降，例如ScanRefer <a href="mailto:&#x41;&#99;&#x63;&#64;&#x30;&#46;&#x32;&#53;">&#x41;&#99;&#x63;&#64;&#x30;&#46;&#x32;&#53;</a>下降1.7%。</li>
<li><strong>GeoGen数据组成</strong>（Exp#3, #4）：仅使用“新颖视图合成”或“轨迹生成”单一数据子集，性能均低于使用完整数据集，表明两者均有重要贡献。</li>
<li><strong>RePE编码</strong>（Exp#5）：用绝对3D坐标编码替换RePE后，多项指标出现下降，证明了相对编码的优势。</li>
</ul>
<p>此外，论文通过“接地注意力分数”（GAS）定量评估了视觉生成调优对缓解“视觉盲点”的作用。如表5所示，引入视觉生成调优后，模型分配给真实视觉区域的注意力质量（GAS）显著提升，同时整体准确率也得到提高。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了MILO，一种通过视觉生成反馈进行隐式空间世界建模的新范式，有效弥补了纯口头描述调优的不足；2）提出了RePE，一种坐标无关的相对位置编码方案，能更好地建模视图间的几何关系；3）构建了大规模的几何感知生成数据集GeoGen，用于支持视觉生成调优。</p>
<p>论文提到的局限性主要在于对带有几何标注（相机参数）数据的依赖。虽然使用了网络视频，但其几何信息仍需通过其他方法估计或标注。</p>
<p>本工作对后续研究的启示在于：1）对于涉及空间、物理等 grounded 认知的任务，为模型提供感知层面的反馈（如视觉生成）可能比纯符号监督更有效；2）在3D或多视图理解中，相对几何关系表示比绝对坐标表示更具泛化优势；3）结合生成式世界模型与推理模型，是迈向具身智能和空间认知的一条有前景的路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多模态大语言模型（MLLMs）空间推理能力不足，以及现有“文本描述调优”方法存在的“视觉文盲”问题，提出了**MILO**范式。该方法通过整合**视觉生成器**提供几何感知反馈，将符号推理隐式锚定于感知经验，并创新性地提出**RePE相对位置编码**以捕获相机姿态变换。为支持训练，构建了大规模**GeoGen**数据集。实验表明，该方法在多个基线和基准测试上**显著提升了模型的空间推理能力**。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.01821" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>