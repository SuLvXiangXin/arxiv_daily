<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.21810" target="_blank" rel="noreferrer">2509.21810</a></span>
        <span>作者: Qinchuan Li Team</span>
        <span>日期: 2025-09-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在四足机器人领域，强化学习已被证明能让机器人掌握复杂环境中的多种技能。基于模仿的强化学习方法，特别是对抗运动先验（AMP），通过利用专家演示数据，为学习复杂技能提供了便捷有效的途径。AMP采用生成对抗网络（GAN）结构，通过判别器区分专家样本与智能体生成样本，从而提供奖励信号，激励智能体产生风格上与专家演示相似的运动行为。然而，GAN的训练过程不稳定，当同时学习多种运动技能时，生成器可能崩溃，仅产生有限的行为子集，无法覆盖完整的数据分布，即发生模式崩溃。这导致生成的样本缺乏多样性，难以捕捉真实专家演示的复杂性。此外，现有的AMP框架通常专注于学习单一目标任务，无法有效处理复杂的多技能学习问题，且缺乏平滑的技能过渡。</p>
<p>本文针对AMP框架在多技能学习中的模式崩溃和技能区分度不足的痛点，引入了条件生成对抗网络（CGAN）的核心思想进行改进。具体而言，通过引入技能类别作为条件信息来指导生成器在不同技能上下文中产生特定的运动序列，从而实现多种技能的同时学习。为了进一步提升技能条件化运动生成的质量和可控性，还引入了一个额外的技能判别器。本文的核心思路是：提出一个基于条件对抗运动先验（CAMP）的多技能学习框架，通过条件判别器和技能判别器的协同作用，使单一策略网络能够从专家演示中高效学习多样且可区分的运动技能，并实现平滑的技能过渡。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架采用非对称的演员-评论家结构，策略由一个条件判别器引导，从专家数据集中学习自然的步态技能，同时利用一个技能判别器构建潜在技能空间，使智能体能够根据需要重建多种步态技能。</p>
<p><img src="https://arxiv.org/html/2509.21810v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：训练框架总览。采用非对称演员-评论家结构，策略由条件判别器引导从专家数据学习步态技能。技能判别器用于构建潜在技能空间，使智能体能按需重建多种步态技能。</p>
</blockquote>
<p><strong>状态与动作空间</strong>：演员网络处理简单的观测值 $o_t \in \mathbb{R}^{48}$，包括：期望角速度、期望技能向量 $g^t$、期望速度指令 $c_t$、来自关节编码器的本体感知数据、重力投影以及上一时刻的网络输出 $a_{t-1}$。技能向量 $g^t$ 设计为独热编码以避免引入隐式序数偏差。策略输出动作 $\in \mathbb{R}^{12}$ 作为关节位置偏移量，目标关节位置定义为 $\theta_{\text{target}} = \theta_{\text{init}} + k a_t$，随后发送至底层PD控制器计算目标扭矩。</p>
<p><strong>奖励函数</strong>：总体奖励由任务奖励 $r^{\text{task}}$、风格奖励 $r^{\text{style}}$ 和技能奖励 $r^{\text{skill}}$ 加权组成：$r_t = \omega^{\text{task}} r_t^{\text{task}} + \omega^{\text{style}} r_t^{\text{style}} + \omega^{\text{skill}} r_t^{\text{skill}}$。任务奖励驱动机器人完成预定运动目标（如平衡、速度跟踪）；风格奖励鼓励智能体生成更自然的运动风格；技能奖励基于技能判别器，激励智能体学习多样且可明确识别的技能行为。具体权重见论文表II。</p>
<p><strong>条件对抗运动先验（CAMP）</strong>：这是方法的核心创新模块。条件判别器 $D_\theta$ 是一个参数化神经网络，用于预测给定的状态转移 $(s_t, s_{t+1})$ 在条件技能潜在变量 $z^p$ 下，是来自专家数据集还是学习到的策略。其训练目标基于最小二乘GAN（LSGAN）公式，并包含一个梯度惩罚项以提升稳定性。风格奖励计算为 $r^{\text{style}} = \max[0, 1 - 0.25(D_\theta(s_t, s_{t+1} | z^p) - 1)^2]$。通过这种基于条件的专家数据学习方法，单一策略网络可以同时学习多步态、多频率的运动技能。</p>
<p><strong>技能判别器与多技能重建</strong>：为了提供更精确的反馈并增强技能的可重用性，引入了技能判别器 $D_{\text{skill}}$。它旨在学习从状态转移对 $[s_t, s_{t+1}]$ 到技能风格空间中潜在技能嵌入 $z \in \mathbb{R}^{d_z}$ 的映射。技能奖励计算为预测的技能潜在变量 $z&#39;$（来自技能判别器）与目标技能嵌入 $z$（来自条件判别器的技能嵌入层）之间的余弦相似度：$r^{\text{skill}} = \cos(z&#39;, z)$。这衡量了策略生成的状态转移与目标技能模式的一致性。技能判别器使用专家数据进行更新，损失函数包括重构损失和梯度惩罚项。</p>
<p><strong>与现有方法的创新点</strong>：1) <strong>条件化AMP</strong>：将原始AMP扩展为条件版本（CAMP），通过引入技能条件变量，使判别器能区分不同技能下的运动风格，有效缓解多技能学习时的模式崩溃。2) <strong>引入技能判别器</strong>：额外引入一个技能判别器，显式地引导策略生成的行为与指定技能的特征对齐，提供了比风格奖励更精确的反馈，增强了技能的可区分性和可控性。3) <strong>统一的端到端框架</strong>：整个框架支持在单一策略网络中同时学习多种技能，无需分阶段训练或外部切换机制，实现了技能间的平滑过渡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真（Isaac Gym）和真实四足机器人（Unitree Go2）上进行。训练时使用了4096个并行智能体，进行了百万步的模拟训练，并采用了广泛的域随机化技术（参数范围见论文表I）以缩小仿真到真实的差距。</p>
<p><strong>对比的基线方法</strong>：</p>
<ul>
<li>Baseline：使用原始AMP训练四足运动。</li>
<li>Ours w/o skill observation：框架中从策略输入移除技能观测 $g^t$。</li>
<li>Ours w/o skill conditioning：消融技能条件化，仅使用标准AMP判别器。</li>
<li>Ours w/o skill discriminator：直接移除技能判别器模型。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>消融实验的定性结果总结于论文表IV。完整方法（Ours）能够学习多种步态并实现步态控制切换。移除技能观测 $g^t$ 后，模型难以区分技能间的行为差异，倾向于只生成一种主导技能。移除技能条件 $z^p$ 后，策略虽能学习自然运动，但区分不同技能的能力显著下降，出现步态模糊或混合的“模式崩溃”现象。当技能数量超过两个时，移除技能判别器会导致生成的技能出现混淆或中性化，无法准确复现特定技能风格，证明了技能判别器在多技能学习中的关键作用。</p>
<p><img src="https://arxiv.org/html/2509.21810v1/motion_transition.png" alt="技能过渡"></p>
<blockquote>
<p><strong>图2</strong>：真实环境中策略的技能过渡性能。机器人能够根据用户命令自主生成并切换多种步态技能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.21810v1/t-sne.png" alt="潜在空间聚类"></p>
<blockquote>
<p><strong>图3</strong>：技能判别器预测的技能聚类（右图）与真实技能标签（左图）对比。可视化表明模型能有效区分不同步态技能在潜在空间中的分布。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.21810v1/DTW.png" alt="DTW距离"></p>
<blockquote>
<p><strong>图4</strong>：潜在空间中不同步态技能的动态时间规整（DTW）距离。颜色越深表示两个步态技能在潜在空间中越接近。对角线区域（如同一步态不同频率）距离较近，表明技能表示对频率扰动具有鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.21810v1/joint_pos.png" alt="关节位置跟踪"></p>
<blockquote>
<p><strong>图5</strong>：FL（左前）腿各电机的目标与实际关节位置对比。蓝色为实际位置，红色为目标轨迹。平均跟踪精度达到91.23%和91.38%，表明策略具备强大的执行和跟踪能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.21810v1/phase_diagram.png" alt="足部接触序列"></p>
<blockquote>
<p><strong>图6</strong>：机器人多技能切换过程中的足部接触序列图。FL、FR、RL、RR分别代表左前、右前、左后、右后腿。该图直观展示了步态间过渡的连续性和稳定性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文核心贡献</strong>：1) 提出了条件对抗运动先验（CAMP）框架，通过引入技能条件变量，有效解决了原始AMP在多技能学习中的模式崩溃问题。2) 设计并引入了技能判别器，通过提供基于技能特征一致性的奖励，增强了策略对多种技能的区分能力和重建精度。3) 实现了在单一策略下的多技能学习与平滑过渡，并在仿真和真实机器人上验证了框架的有效性。</p>
<p><strong>局限性</strong>：论文提到，方法的成功在很大程度上依赖于专家数据的质量和多样性。如果数据集缺乏足够的丰富性，生成器可能无法学习到有用且可区分的技能。此外，虽然比某些多网络方法（如Multi-AMP）更高效，但随着技能数量增加，框架可能仍面临一定的计算复杂度挑战。</p>
<p><strong>对后续研究的启示</strong>：本文的工作展示了将条件生成模型与对抗模仿学习结合用于机器人多技能学习的潜力。技能判别器与条件判别器协同工作的范式，为在复杂、高维动作空间中学习结构化、可组合的技能提供了新思路。未来的研究可以探索更高效的技能表示学习方法，进一步降低对大规模高质量专家数据的依赖，以及将框架扩展到更复杂的非结构化环境和动态任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对四足机器人难以从专家演示中通过单一策略学习多种运动技能且技能转换不流畅的核心问题，提出基于条件对抗运动先验（CAMP）的多技能学习框架。该方法引入条件生成对抗网络（CGAN）思想，通过新颖的技能判别器和技能条件奖励设计实现精确技能重建，支持主动控制与重用多种技能，为复杂环境中学习可泛化策略提供实用方案。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.21810" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>