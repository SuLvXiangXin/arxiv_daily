<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sashimi-Bot: Autonomous Tri-manual Advanced Manipulation and Cutting of Deformable Objects - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Sashimi-Bot: Autonomous Tri-manual Advanced Manipulation and Cutting of Deformable Objects</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.11223" target="_blank" rel="noreferrer">2511.11223</a></span>
        <span>作者: Ekrem Misimi Team</span>
        <span>日期: 2025-11-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人对可变形、有体积物体的高级操作，因其柔软、脆弱、多变以及交互过程中的不确定性，仍然是最大的挑战之一。现有工作主要集中在弹性物体或非体积物体（如布料、绳索）上，而对于天然、柔软、易滑且物理参数难以表征的生物可变形物体（如三文鱼鱼腩）的操作研究甚少。此外，涉及手持工具的形状操作或切割、以及用于增强外在灵巧性的多臂协调操作，仍然是机器人学中很大程度上尚未充分发展的领域。</p>
<p>本文针对上述痛点，以自主制作生鱼片这一复杂现实任务为例，提出了一个三臂协作的机器人系统Sashimi-Bot。该系统旨在模仿人类处理生鱼片的方式：一只手调整鱼腩形状，另一只手握刀切割，同时用空闲的手稳定鱼腩，最后拾取薄片。其核心思路是结合深度强化学习、手持工具操作、以及视觉与触觉反馈，构建一个能够处理天然可变形物体固有变异性的鲁棒、全自主操作框架。</p>
<h2 id="方法详解">方法详解</h2>
<p>Sashimi-Bot系统由三个7自由度机械臂组成，围绕一个共享工作区。机器人A装备一个1自由度柔顺仿人抓手（QBSoftHand），用于抓取辅助工具（如推板、厨刀）和一个提供触觉反馈的GelSight传感器。机器人B装备一个由两个长指和弹性带组成的专用末端执行器，用于在切割时轻柔地稳定鱼腩。这两个机器人由一个静态的、俯视工作区的RGB-D相机以眼到手配置进行控制。机器人C持有一双安装在1自由度平行夹爪上的筷子，并配备一个腕戴式RGB-D相机。</p>
<p><img src="https://arxiv.org/html/2511.11223v1/fig/figure_1.png" alt="系统概览与流程"></p>
<blockquote>
<p><strong>图1</strong>：Sashimi-Bot三臂机器人框架。左侧：系统运行实景。右侧：从鱼腩任意初始状态（I）开始，系统的高级组件包括形状操作（II）、切割（III）以及拾取与放置（IV）。</p>
</blockquote>
<p>整个流水线（图1右）始于一块任意放置在砧板上的三文鱼鱼腩。首先，机器人A对鱼腩进行非抓取式的重新定位和整形，使其从任意形状变为居中且拉直的形态。接着，机器人A抓取厨刀，在机器人B稳定鱼腩的同时，以切片动作进行切割。最后，机器人C拾取新切下的鱼片并放置到餐盘上。步骤III和IV重复进行，直到处理完整个鱼腩或达到预定切割次数。</p>
<p><img src="https://arxiv.org/html/2511.11223v1/x1.png" alt="方法模块总览"></p>
<blockquote>
<p><strong>图2</strong>：Sashimi-Bot流水线的高级组件。a) 静态俯视相机提供工作区点云。b) 形状操作采用基于深度强化学习（DRL）的非抓取方法。c) 切割轨迹由运动规划算法生成。d) 利用来自GelSight传感器的触觉反馈。e) 拾放模块使用独立的腕戴相机反馈和视觉伺服（VS）用筷子抓取鱼片。</p>
</blockquote>
<p><strong>1. 基于DRL的非抓取形状伺服</strong>：该方法（图2b）使用一个手持推板，通过一系列离散的推动动作，将鱼腩驱动至期望形态。鱼腩形状由从工作区点云俯视投影生成的分割图像轮廓中均匀采样的点集表示（当前为蓝色，期望为绿色）。智能体采用基于Transformer架构的神经策略，决定推动哪个轮廓点以及推动多远。该策略完全在仿真中训练，并可直接部署到真实机器人上，无需任何真实世界训练，实现了零-shot sim-to-real泛化。</p>
<p><strong>2. 视觉控制的切割子系统</strong>：一旦鱼腩被拉直定位，切割模块接管机器人A和B的控制（图2c）。根据从点云估计的鱼腩位姿和边界矩形，利用逆运动学生成合适的切割轨迹。运动规划算法控制刀具以一次或多次划动穿过鱼腩，同时逐渐降低刀身接近砧板。生成的路径被约束为：在任何时刻，当前与鱼腩中心相交的刀刃切线段始终平行于砧板。</p>
<p><strong>3. 基于触觉反馈的刀具-砧板接触检测</strong>：由于使用软抓手握持常规厨刀会引入刀具精确位姿的不确定性，系统利用安装在机器人A手部前端的GelSight传感器的触觉反馈来解决此问题（图2d）。传感器接触刀的钝背刃，其凝胶垫的形变由下方的RGB相机捕获为视频流。通过收集和标注传感器数据，训练一个轻量级视觉Transformer分类器网络，以判断传感器图像是否对应砧板接触。训练后的模型可在切割过程中持续分类触觉数据，若检测到过早接触，则调整规划轨迹。</p>
<p><strong>4. 基于视觉伺服的闭环抓取框架</strong>：拾放模块（图2e）与切割控制器协调，定位、拾取并放置新切下的鱼片。切割后，一个搜索区域（如在砧板或刀刃上）被发送给机器人C，机器人C随后将其末端执行器移动到位。接着，利用腕戴RGB-D相机的反馈，通过基于视觉伺服的闭环控制律控制机械臂。鱼片所在的平面方向已知（如果在砧板上则为水平，如果在刀刃上则可通过机器人A的运动学获得），因此只需控制剩余的4个自由度（位置和方向）。鱼片通过HSV带通滤波器分割图像进行定位，视觉特征基于前景像素的3D质心和主轴生成。筷子因其天然的柔顺性和细长轮廓，非常适合轻柔拾取薄鱼片。</p>
<p>与现有方法相比，Sashimi-Bot的创新点具体体现在：1) <strong>零-shot sim-to-real的非抓取形状伺服</strong>，可泛化到不同物理属性的物体；2) <strong>手持常规工具（厨刀）的操作与切割</strong>，而非将工具固定于末端；3) <strong>三臂协作</strong>，专门用于稳定和增强灵巧性；4) <strong>融合视觉与高分辨率触觉反馈</strong>的闭环控制，以应对不确定性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了包括米填充的布料物体（用于形状操作验证）以及三文鱼、猪肉和鳕鱼鱼片（用于切割和拾取评估）在内的多种材料。系统在物理机器人平台上进行测试。</p>
<p><strong>形状操作实验</strong>：将训练好的DRL策略直接部署到真实世界，使用三种不同初始形状（L, C, Z）的米袋进行测试，每种形状进行10次试验。控制器在所有30次试验中均成功拉直物体（图3a）。对于L形、C形和Z形，平均所需推动动作次数分别为1.8、5.0和7.5次（图3a）。控制器展现出鲁棒性，即使偶尔推动过长也能通过闭环特性快速修正。</p>
<p><img src="https://arxiv.org/html/2511.11223v1/x2.png" alt="形状操作结果"></p>
<blockquote>
<p><strong>图3</strong>：基于DRL的形状操作结果。a) 每种初始形状（L, C, Z）达到收敛所需操作步骤数，跨越10次随机试验。b) Z形物体拉直过程示例。</p>
</blockquote>
<p><strong>切割实验</strong>：评估了四种不同配置（刀具是否倾斜约20度，使用单次向后划动或双向前后划动）下的切割效果（图4）。每种模式评估五次厚度为7毫米的非初始切割。所有切割均成功分离，鱼片表面光滑、纹理均匀、厚度一致，表明四种模式均能产生一致的切割性能。倾斜刀刃会产生稍宽的鱼片，而多次划动允许更长的总切割行程。</p>
<p><img src="https://arxiv.org/html/2511.11223v1/x3.png" alt="切割结果"></p>
<blockquote>
<p><strong>图4</strong>：不同配置下的切割结果。我们改变刀具是垂直于砧板还是倾斜（约20度），以及是使用单次向后划动还是双向前后划动。</p>
</blockquote>
<p><strong>触觉反馈实验</strong>：训练的分类模型在包含20条切割轨迹的留出验证集上达到了95%的准确率、99%的精确率和67%的召回率（图5b）。模型能够可靠地检测明确的接触事件，并在检测到接触时调整后续轨迹（图5c），从而为切割控制器提供闭环鲁棒性。</p>
<p><img src="https://arxiv.org/html/2511.11223v1/x4.png" alt="触觉反馈结果"></p>
<blockquote>
<p><strong>图5</strong>：触觉反馈结果。a) 非接触和接触砧板时的GelSight传感器图像示例及两者差异。b) 四种不同场景下预测的砧板接触概率。c) 基于触觉反馈进行闭环轨迹调整的示例（人为抬高砧板诱发过早接触）。</p>
</blockquote>
<p><strong>视觉拾取实验</strong>：使用三文鱼、猪肉和鳕鱼片评估4自由度闭环视觉伺服拾取控制器（图6）。每种肉类使用5片不同的鱼片，每片从随机位姿拾取5次，共75次试验。控制器成功了74/75次，唯一失败是由于光源阴影导致的图像分割错误。系统非常轻柔，没有鱼片在实验中被撕裂或损坏。</p>
<p><img src="https://arxiv.org/html/2511.11223v1/x5.png" alt="拾取实验"></p>
<blockquote>
<p><strong>图6</strong>：拾取实验。左：实验中使用的三文鱼、鳕鱼和猪肉片。右：用于在末端执行器框架内控制抓取的拾取机器人腕戴相机画面示例。蓝色边界为分割掩码，计算出的视觉特征（两个点和一条连接红线标记物体主轴）覆盖在鱼片上，其期望值在图像中心用绿色连接线显示。</p>
</blockquote>
<p><strong>全系统端到端评估</strong>：对包含形状操作、切割和拾取的完整自主流水线进行评估。使用与之前相同的三种初始形状以及一个通过抛掷产生的任意形状。尽管真实三文鱼鱼腩比测试用的米袋更湿、更重，与木质砧板的摩擦力更大，但形状操作控制器仍能从所有初始配置中充分重新定位和拉直鱼腩，无需人工干预。共切割了34片鱼片（厚度6-16毫米，刀具倾斜0-20度）。其中6片粘在刀刃上，全部成功从刀上拾取；28片留在砧板上，其中26片成功从砧板拾取。两次拾取失败发生在拾取非常薄的鱼片时。平均每次切割耗时5.3秒，但由于切割机器人需等待拾取机器人抬起鱼片才能开始下一次切割，连续两次切割之间的平均时间为27.9秒。</p>
<p>消融实验总结：每个核心组件都通过独立实验验证了其有效性。形状操作模块展示了零-shot sim-to-real和跨材料泛化的能力；切割模块证明了多种切割模式的可行性；触觉反馈模块提供了关键的接触检测和轨迹调整；视觉拾取模块实现了对不同材质鱼片的高成功率、轻柔抓取。全系统集成实验则证明了整个流水线的自治性和处理边缘情况（如鱼片粘刀）的能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个完整的、全自主的三臂机器人系统Sashimi-Bot，能够处理天然、柔软、可变形的三文鱼鱼腩，完成从任意初始状态到最终摆盘的生鱼片制作全流程。2) 开发了一种基于DRL的零-shot sim-to-real非抓取形状伺服方法，能够轻柔地操作具有不同物理属性的可变形物体。3) 创新性地整合了视觉与高分辨率触觉反馈，实现了手持常规工具（厨刀）的灵巧操作和基于接触感知的闭环切割控制。</p>
<p>论文自身提到的局限性包括：基于推动的整形方案在鱼腩因摩擦力大而沿其长轴被压缩时，难以完全抚平褶皱；此外，在拾取非常薄的鱼片时出现了滑脱失败案例。</p>
<p>这项工作为可变形体积物体的机器人操作树立了一个里程碑。其对多臂协作、手持工具操作、以及多模态感知融合（视觉与触觉）的探索，为其他复杂的现实世界应用（如食品加工、医疗手术辅助、柔性装配等）提供了重要的技术启示和可行性验证。未来研究可着眼于进一步提高对高摩擦力、粘性材料的整形效果，优化对极薄或脆弱物体的抓取策略，以及探索更高级的任务规划和故障恢复机制。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操纵柔软、易变形且特性不确定的三文鱼柳制作刺身的复杂任务，提出了Sashimi-Bot多机器人协作系统。其核心技术结合了深度强化学习、工具在手机器人操作（包括抓握、切割），并融合视觉与触觉反馈以实现鲁棒性。该系统成功实现了对鱼柳的拉直、协同稳定下的切片以及拾取薄片等一系列自主操作，标志着在可变形物体高级操纵方面取得了重要进展。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.11223" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>