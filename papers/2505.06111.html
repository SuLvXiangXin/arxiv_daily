<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>UniVLA: Learning to Act Anywhere with Task-centric Latent Actions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>UniVLA: Learning to Act Anywhere with Task-centric Latent Actions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.06111" target="_blank" rel="noreferrer">2505.06111</a></span>
        <span>作者: Bu, Qingwen, Yang, Yanting, Cai, Jisong, Gao, Shenyuan, Ren, Guanghui, Yao, Maoqing, Luo, Ping, Li, Hongyang</span>
        <span>日期: 2025/05/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉-语言-动作（VLA）模型的机器人策略取得了显著进展。然而，主流方法（如RT-1、Octo、OpenVLA）严重依赖于带有真实动作标签的交互数据进行监督学习。这极大地限制了其利用互联网规模、来源多样的视频数据（如不同机器人具身、不同视角、甚至人类视频）的可扩展性。同时，不同具身（如Franka、WidowX）和任务（如操作、导航）之间在动作与观测空间上的异构性，对有效的知识迁移构成了重大挑战。</p>
<p>本文针对上述两个关键痛点：1) 对动作标注数据的重度依赖；2) 跨具身、跨环境知识迁移困难。提出了一个名为UniVLA的新框架，其核心思路是从无标注视频中学习一个统一的、任务中心的潜动作表示，从而能够利用任意具身和视角的数据进行大规模预训练，并通过高效的潜动作解码将通用策略部署到各种机器人上。概括而言，本文的核心思路是：通过解耦任务中心动态与无关视觉变化，在统一的潜动作空间中学习通用策略，从而实现可扩展、高效的决策。</p>
<h2 id="方法详解">方法详解</h2>
<p>UniVLA框架包含三个关键步骤：1) 任务中心潜动作学习；2) 通用策略预训练；3) 面向部署的后训练。</p>
<p><img src="https://arxiv.org/html/2505.06111v3/x2.png" alt="任务中心潜动作学习框架"></p>
<blockquote>
<p><strong>图2</strong>：任务中心潜动作学习的两阶段训练框架。第一阶段（左）利用语言指令作为条件，学习捕捉任务无关动态的潜动作。第二阶段（右）在第一阶段模型基础上，引入新的码书学习专门捕捉任务中心动态的潜动作。</p>
</blockquote>
<p><strong>1. 任务中心潜动作学习</strong><br>此步骤旨在从无标注视频中生成伪动作标签（潜动作令牌）。模型基于逆动力学模型（IDM）编码器 ℐ 和正动力学模型（FDM）解码器 ℱ 构建。输入为一对间隔约1秒的连续视频帧 {o_t, o_t+k}，输出为预测的下一帧特征。</p>
<ul>
<li><strong>潜动作量化</strong>：编码器从视频特征中提取一组可学习的动作令牌，并通过向量量化（VQ-VAE）将其离散化为潜动作令牌 a_z。解码器仅使用量化后的动作令牌和当前帧特征来预测未来帧。</li>
<li><strong>关键创新</strong>：为避免基于原始像素重建时引入任务无关的噪声（如纹理、光照、相机抖动），本文提出使用预训练的DINOv2空间块特征作为语义丰富的表示进行重建，其物体中心和空间感知的特性更适合作为预测目标。</li>
<li><strong>潜动作解耦</strong>：这是方法的核心创新。为了将任务相关动态与无关动态（如其他智能体运动、新物体出现）分离，设计了两阶段训练：<ul>
<li><strong>阶段一</strong>：将语言指令嵌入（通过T5编码器获得）同时输入编码器和解码器作为条件。由于码书容量有限，量化后的潜动作 a_TI 被优化为主要编码环境变化和视觉细节，而高级任务信息由语言条件提供，从而学习到一组“任务无关”的潜动作。</li>
<li><strong>阶段二</strong>：冻结第一阶段训练好的模型参数和任务无关码书，并引入一个全新的码书。编码器现在输出两组潜动作（a_TI 和 a_TC），解码器使用两者重建未来帧。通过这种方式，新码书专门学习捕捉“任务中心”的动态（如物体操作、目标导向运动），实现显式解耦。</li>
</ul>
</li>
</ul>
<p><strong>2. 通用策略预训练</strong><br>利用训练好的潜动作模型为所有视频帧标注潜动作标签后，训练一个通用策略。</p>
<p><img src="https://arxiv.org/html/2505.06111v3/x3.png" alt="通用策略架构"></p>
<blockquote>
<p><strong>图3</strong>：通用策略架构。基于Prismatic-7B VLM，将投影后的视觉嵌入和分词后的任务指令作为输入，以自回归方式预测潜动作令牌。为适应特定机器人，使用专门的动作解码器头将潜动作转换为可执行的控制信号。</p>
</blockquote>
<ul>
<li><strong>架构</strong>：策略基于Prismatic-7B VLM构建，包含融合视觉编码器（SigLip + DINOv2）、投影层和LLaMA-2 LLM。</li>
<li><strong>创新点</strong>：不同于OpenVLA等方法将低频词映射到连续动作区间，UniVLA在LLaMA词表中扩展了 |C| 个特殊动作令牌（如ACT_1, ACT_2...）。潜动作根据其在码书中的索引被投影到这些令牌上。策略的输入是观测 o_t、任务指令 l 以及历史潜动作前缀 a_z,&lt;i，通过最小化下一潜动作的负对数似然进行训练。这种设计将动作空间从OpenVLA的 256^7 大幅压缩至 16^4（当|C|=16时），显著加速了模型收敛。</li>
</ul>
<p><strong>3. 面向部署的后训练</strong><br>预训练的策略在部署时仍预测统一的潜动作。为了将其适配到具体机器人，需要学习一个轻量级动作解码器。</p>
<ul>
<li><strong>潜动作解码</strong>：如图3所示，使用多头注意力池化聚合视觉嵌入，并将其作为查询从潜动作嵌入中提取信息，最后通过线性层投影到目标机器人的动作空间。潜动作代表约1秒内的动作，可被解码为动作块以实现平滑控制。</li>
<li><strong>利用历史输出</strong>：受大语言模型思维链启发，在每次决策时将上一步预测的潜动作（编码为N=4个令牌）附加到任务指令中，作为历史上下文输入，以提升策略在长视野任务中的表现。训练时，这些历史动作输入被整合以赋予模型上下文学习能力。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：操作任务使用LIBERO、CALVIN、SimplerEnv；导航任务使用R2R（VLN-CE）；并进行了真实机器人部署测试。</li>
<li><strong>预训练数据</strong>：Open X-Embodiment（机器人操作）、GNM（导航）、Ego4D（人类视频）的子集。</li>
<li><strong>对比基线</strong>：LAPA、Diffusion Policy、Octo、MDT、OpenVLA、MaIL等。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>LIBERO操作基准</strong>：UniVLA在四个任务套件（Spatial, Object, Goal, Long）上均取得最佳性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.06111v3/x4.png" alt="LIBERO任务设置"></p>
<blockquote>
<p><strong>图4</strong>：LIBERO基准的四个任务套件设置，用于评估策略的不同能力维度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.06111v3/x7.png" alt="LIBERO结果表"></p>
<blockquote>
<p><strong>图7</strong>：LIBERO基准结果。UniVLA（Full）以95.2%的平均成功率显著优于所有基线。仅使用Bridge-V2数据预训练的UniVLA（Bridge）也以92.5%超过OpenVLA（76.5%）。</p>
</blockquote>
<ol start="2">
<li><strong>导航基准（R2R）</strong>：UniVLA仅使用单帧RGB输入，其性能与使用了全部历史观测的导航模型NaVid相当，并显著优于OpenVLA。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.06111v3/x6.png" alt="导航结果"></p>
<blockquote>
<p><strong>图6</strong>：在R2R（VLN-CE）上的Oracle成功率。UniVLA仅用单帧输入就达到了与使用全历史的NaVid相当的水平，远超OpenVLA。</p>
</blockquote>
<ol start="3">
<li><strong>真实机器人部署</strong>：设计了四项任务评估策略能力，UniVLA相比之前的SOTA方法平均成功率提升了36.7%，平均得分提升了0.68。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.06111v3/x5.png" alt="真实机器人实验"></p>
<blockquote>
<p><strong>图5</strong>：真实世界机器人实验的四个任务。UniVLA在成功率和任务得分上均有大幅提升。</p>
</blockquote>
<ol start="4">
<li><strong>效率与可扩展性</strong>：<ul>
<li><strong>计算效率</strong>：UniVLA仅用960 A100小时完成预训练，而OpenVLA需要21,500小时，计算成本仅为后者的1/20以下。</li>
<li><strong>数据效率与可扩展性</strong>：仅使用Bridge-V2数据集预训练的UniVLA，其性能已超过使用更大规模Open X-Embodiment数据集训练的OpenVLA和LAPA。当加入异构数据（包括人类视频）时，性能持续提升，证明了其出色的数据利用能力和可扩展性。</li>
</ul>
</li>
</ol>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>任务中心潜动作的有效性</strong>：与基线LAPA（编码所有视觉变化）相比，使用解耦后的任务中心潜动作进行策略训练，收敛更快且性能更鲁棒。</li>
<li><strong>DINOv2特征的作用</strong>：使用DINOv2特征作为重建目标，相比使用原始像素，能学习到更具信息量的潜动作，从而带来显著的性能提升。</li>
<li><strong>历史动作上下文的作用</strong>：引入历史潜动作作为上下文输入，能有效提升策略在长视野任务上的表现。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了UniVLA框架，通过在一个统一的、与具身无关的潜动作空间中进行规划，实现了从网络规模视频中学习可扩展、高效的通用机器人策略。</li>
<li>创新性地提出了一种从跨具身视频中提取任务相关潜动作的方法，利用语言指令和DINOv2特征空间，显式解耦了任务中心动态与无关视觉变化。</li>
<li>在多个操作与导航基准以及真实机器人测试中达到了最先进的性能，并且在预训练计算成本和下游数据需求上实现了数量级级别的效率提升（分别低于OpenVLA的1/20和1/10）。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法依赖于预训练的DINOv2和T5编码器来提供特征和语言条件。这些外部模型的性能上限和偏差可能会影响潜动作学习的质量。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>无标注学习范式</strong>：证明了从无动作标签的互联网视频中学习通用机器人策略的可行性，为利用海量、多样化的视觉数据开辟了新途径。</li>
<li><strong>表示学习的重要性</strong>：强调了对“动作”本身进行有效、紧凑的表示学习（而不仅仅是策略架构）是提升泛化性、数据效率和计算效率的关键。</li>
<li><strong>跨模态与跨具身统一</strong>：通过构建统一的潜动作空间，为真正实现不同机器人形态、不同任务领域之间的知识迁移与共享提供了有希望的框架。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出UniVLA框架，旨在解决通用机器人策略难以跨不同机器形态和环境迁移知识的问题。其核心方法是构建任务中心的潜在动作表示：通过无监督方式从跨形态视频中提取任务相关动作，在DINO特征空间建立潜在动作模型，并利用语言指令过滤任务无关动态。实验表明，UniVLA在多项操作与导航任务上取得SOTA性能，仅用OpenVLA不到1/20的预训练算力和1/10的下游数据即实现更优效果，且加入人类视频等异构数据后性能持续提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.06111" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>