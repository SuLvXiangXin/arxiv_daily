<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Language Movement Primitives: Grounding Language Models in Robot Motion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Language Movement Primitives: Grounding Language Models in Robot Motion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.02839" target="_blank" rel="noreferrer">2602.02839</a></span>
        <span>作者: Simon Stepputtis Team</span>
        <span>日期: 2026-02-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，让机器人根据自然语言指令执行新操作任务仍是一个根本性挑战。该领域的主流方法主要分为两类。一类是机器人基础模型（如RT-2、Octo等），它们在大量机器人数据上训练，可以直接输出动作命令，但通常缺乏常识推理能力，且需要针对特定领域进行微调或收集经验。另一类是通用视觉语言模型（VLM），它们具有强大的高层任务分解和语义理解能力，但其输出是符号化的（如“拿起海绵”），无法直接转化为机器人可执行的、连续的低层运动控制指令。神经符号方法试图通过分离高层任务规划和低层控制来规避这一鸿沟，但它们通常在离散的动作基元上进行符号推理，限制了执行复杂连续运动的能力。</p>
<p>本文针对的核心痛点是：如何在高层的、抽象的语言任务推理与低层的、连续的运动控制（位置、速度、加速度）之间建立稳健的连接。本文提出了一个新的视角：利用动态运动基元（DMP）作为连接VLM与机器人运动的“接地”接口。DMP提供了一组数量少、可解释性强的参数（如权重、目标点），能够编码多样、连续且稳定的轨迹。本文的核心思路是：让视觉语言模型（VLM）对自由形式的自然语言任务进行推理，并将其期望的运动语义地“接地”到DMP的参数中，从而弥合高层推理与低层控制之间的差距。</p>
<h2 id="方法详解">方法详解</h2>
<p>Language Movement Primitives (LMP) 的整体框架是一个将用户指令转化为机器人动作序列的完整流程。给定用户任务描述 τ 和环境观测 o（包含关节位置和RGB-D图像），LMP首先将观测转换为文本化的状态描述，然后逐步分解任务并生成DMP参数，驱动机器人执行。</p>
<p><img src="https://arxiv.org/html/2602.02839v1/figures/method.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：LMP处理单个子任务的流程。(a) 机器人接收用户任务描述，并采集当前环境图像，同时记住之前已完成的子任务。(b) 分解器 π𝒟 识别场景物体并输出下一个待完成的子任务。使用开放词汇分类器和深度感知来估计物体的3D位置。场景描述和提议的子任务被转发给DMP权重生成器 π𝒢。(c) 生成器预测DMP权重和辅助参数以定义低层参考轨迹。(d) 机器人跟踪由预测的DMP参数生成的连续轨迹。用户可选择性地观察机器人并提供关于错误的自然语言反馈。如果用户给出这种精炼指令 r，机器人会重置当前流程并从(b)开始重复。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>从观测到状态描述（IV-A）</strong>：使用开放词汇分类器 π_class（如Gemini-Robotics ER和LangSAM）对RGB-D图像进行处理，分割出环境中所有物体的文本标签 l 和3D位姿 p，形成结构化描述 θ。然后，将 θ 自动填充到一个模板化的自然语言状态描述中，与原始RGB图像一同作为后续VLM的输入。</li>
<li><strong>从状态描述到分解子任务（IV-B）</strong>：分解策略 π𝒟 是一个基础VLM，它接收用户任务 τ、观测 o、状态描述 θ 以及已完成子任务的历史 [φ_k]，输出下一个子任务的模板化语言描述 φ_i。子任务模板有两种形式：<code>ACTION(object)</code>（如“抓取海绵”）或 <code>ACTION(object) TO (object)</code>（如“将海绵移到盘子上”），确保每个子任务都锚定在场景物体上，且复杂度适合单个DMP执行。</li>
<li><strong>从子任务到生成DMP（IV-C）</strong>：生成策略 π𝒢 是另一个VLM，它接收子任务描述 φ_i、一个特殊的“接地”提示 s_𝒢、观测 o 和状态描述 θ，输出DMP参数。这些参数包括权重矩阵 W_i ∈ ℝ^(M×B)（M为控制自由度数量，B为基函数数量），以及用于微调目标点和基函数宽度的偏移参数 Δz 和 Δψ。在桌面操作任务中，M 通常对应末端执行器的 x, y, z 平移、绕 z 轴旋转 θ_z 以及夹爪开合状态 g。每个自由度都有一组权重向量 w。夹爪命令被建模为连续变量，但其基函数被替换为阶跃函数，以便在轨迹特定时间点执行开/合动作。DMP的最终目标位置 g 由生成器根据物体位置计算，并可被 Δz 偏移。生成过程通过一个精心设计的提示 s_𝒢 来引导VLM理解每个DMP参数（如 w_x, w_y, g_z）的物理意义及其与子任务动词（如“擦拭”、“绕过”）的关联。</li>
<li><strong>反馈与精炼（IV-D）</strong>：如果任务执行失败或用户不满意，用户可以提供自然语言反馈 r（如“用毛巾代替”）。系统会将此反馈纳入到后续的提示中（更新给 π𝒟 和 π𝒢），然后重置当前子任务并重新尝试生成和执行DMP。</li>
</ol>
<p>与现有方法相比，LMP的核心创新点在于：<strong>将DMP作为连接高层VLM语义推理与低层机器人连续控制的、语义可解释的参数化中间表示</strong>。它既利用了VLM强大的、无需微调的常识推理和任务分解能力，又利用了DMP在控制理论保证下的稳定性、连续性和轨迹形状多样性。不同于输出离散动作符号或需要大量机器人数据训练的方法，LMP让VLM直接“编程”一个具有收敛保证的连续控制器。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界的桌面操作场景中进行，使用了Franka Emika Panda机械臂和Intel RealSense深度相机。</p>
<ul>
<li><strong>Benchmark/数据集</strong>：构建了包含20个多样化家庭操作任务的测试集（如图3所示），这些任务需要语义任务理解、障碍物感知和空间推理，例如“清洁盘子”、“将积木堆成塔”、“将苹果放入碗中并绕过杯子”等。</li>
<li><strong>Baseline方法</strong>：对比了三种基线方法：(1) **RT-2-X (55B)**：一个大型视觉-语言-动作（VLA）机器人基础模型。(2) <strong>Octo-Base</strong>：一个在多样化机器人数据上训练的多任务策略模型。(3) **SayCan (w/ VLM Planner)**：一个神经符号方法，使用VLM进行高层任务规划，并搭配预定义的低层技能库。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.02839v1/figures/experiments.png" alt="实验任务"></p>
<blockquote>
<p><strong>图3</strong>：实验评估的20个多样化家庭任务，展示了需要语义理解、障碍物规避和多阶段推理的场景。</p>
</blockquote>
<ul>
<li><strong>关键实验结果</strong>：在20个任务上的零样本（无微调、无精炼）成功率对比中，LMP达到了<strong>80%</strong> 的总成功率。相比之下，最好的基线方法RT-2-X的成功率为**31%<strong>，Octo-Base为</strong>26%<strong>，SayCan为</strong>24%<strong>。LMP相对于最佳基线的性能提升高达</strong>38%**。实验特别指出，在需要轨迹塑形（如避障）或多阶段任务中，LMP的性能提升尤为显著。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.02839v1/figures/stack_failures2.png" alt="失败案例分析"></p>
<blockquote>
<p><strong>图4</strong>：失败案例定性分析。左图展示了在堆叠任务中，由于感知误差（绿色框）导致目标位置不准确，从而引发执行失败。右图展示了在需要复杂轨迹形状的任务中（如绕过障碍物），虽然DMP权重被正确设置以产生绕过动作，但由于权重幅值不足，实际避障效果不充分。</p>
</blockquote>
<ul>
<li><strong>消融实验与组件贡献</strong>：论文进行了消融研究，验证了各个组件的必要性。移除状态描述（仅提供RGB图像）会导致成功率下降17%，凸显了文本化场景信息对VLM推理的重要性。移除任务分解模块（即让VLM直接生成DMP参数）会使成功率降低23%，表明分层的任务分解对于处理复杂多步骤任务至关重要。此外，研究还比较了不同VLM主干网络的影响，并分析了反馈精炼循环的有效性，显示在首次尝试失败后，用户反馈可以将特定任务的后续尝试成功率提高约45%。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>形式化LMP框架</strong>：提出了一种将语言模型与动态运动基元相结合的新型策略抽象，其中状态是图像和文本描述，动作是DMP参数，建立了高层语义规划与低层控制之间的直接、语义可解释的连接。</li>
<li><strong>实现语言到控制的接地</strong>：构建了一个完整的系统，能够将开放形式的用户指令和纠正反馈，翻译成细粒度且稳定的低层运动控制器，而无需机器人领域演示或微调。</li>
<li><strong>实证优势</strong>：在20个真实世界操作任务上进行的零样本实验表明，LMP大幅优于现有的机器人基础模型和神经符号方法，特别是在需要复杂轨迹和多阶段推理的任务上。</li>
</ol>
<p>论文提到的局限性包括：系统依赖于外部的开放词汇感知模块（π_class）的准确性，感知误差会直接导致失败（如图4左）；当前DMP参数空间对于某些极其复杂的运动可能表达能力有限；方法主要针对桌面操作场景，尚未扩展到更复杂的移动操作或动态环境。</p>
<p>本文的启发在于，<strong>利用控制理论中具有良好性质的参数化运动表示（如DMP）作为“中间件”，是连接数据驱动的高层语义模型与物理驱动的低层机器人控制的一条有效途径</strong>。这为构建无需大量机器人数据训练、即可通过自然语言灵活编程的通用机器人系统提供了新思路。后续研究可以探索更强大的运动基元表示、集成更鲁棒的感知模块，以及将该框架扩展到更广泛的机器人任务和环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决机器人根据自然语言指令执行新操作任务时，高层语义推理与底层运动控制脱节的核心问题。提出语言运动基元（LMP）框架，其关键技术是将大视觉语言模型（VLM）的推理能力，通过动态运动基元（DMP）的参数化进行落地，从而将语言指令直接转化为连续、稳定的机器人轨迹。在20个真实桌面操作任务上的实验表明，该方法实现了零样本操作，任务成功率高达80%，显著优于基线方法（31%）。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.02839" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>