<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs? - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.16755" target="_blank" rel="noreferrer">2512.16755</a></span>
        <span>作者: Wang, Siqi, Liang, Chao, Gao, Yunfan, Yu, Erxin, Li, Sen, Li, Yushi, Li, Jing, Wang, Haofen</span>
        <span>日期: 2025/12/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于视觉语言模型（VLM）的具身导航研究主要集中于遵循显式、逐步指令的任务（例如“直走直到雕塑喷泉，右转，然后继续直到到达麦当劳”）。这类方法依赖预先构建的导航指令，在动态或新颖的城市场景中面临关键局限，无法处理真实世界中普遍存在的、目标抽象的<strong>隐含需求</strong>（例如“我渴了”、“我需要一个可以临时办公的地方”）。隐含需求涉及功能、空间和语义多个层面，解决这些需求是实现面向目标的“最后一公里”导航的基础。</p>
<p>本文针对VLM在开放世界城市环境中理解和响应隐含人类需求的能力不足这一具体痛点，提出了一个新的评估视角：<strong>隐含需求驱动的视觉接地</strong>。核心思路是构建一个大规模基准（CitySeeker）来系统评估VLM将隐含需求转化为具体视觉搜索的空间推理与决策能力，并基于评估发现的瓶颈，探索一系列受人类认知地图启发的探索性策略（BCR）以提升其空间智能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文工作包含两个核心部分：1）构建CitySeeker基准与评估框架；2）提出并分析BCR探索性策略。</p>
<p><strong>1. CitySeeker基准与评估框架</strong><br>整体任务在一个导航图𝒢 = (𝒱, ℰ)中形式化，其中𝒱是地理参考节点，ℰ是边。在每一步t，智能体位于节点v_t，接收自然语言指令𝒲和当前观察集𝒪_t（包含n个朝向的视角图像）。智能体通过一个策略π_Θ运行，该策略依次生成推理依据Φ_t、选择动作a_t（对应视角索引）以及输出置信度c_t。环境根据动态T(v_t, a_t)转移到下一节点。导航过程被记录为轨迹τ，直到满足停止条件或超过最大步数（35步）。</p>
<p><img src="https://arxiv.org/html/2512.16755v1/Figures/4.png" alt="导航框架"></p>
<blockquote>
<p><strong>图4</strong>：CitySeeker隐含需求驱动的具身城市导航框架。在每个时间步，当前全景图被划分为多个视角。智能体遵循ReAct风格的推理过程：观察（Observe）、思考（Think）以推断导航意图、行动（Act）选择一个视角移动，最后反射（Reflect）输出置信度。此过程迭代进行，且为了隔离核心空间推理能力，每一步决策是独立的，不维护持久记忆。</p>
</blockquote>
<p><strong>2. BCR探索性策略</strong><br>为了应对评估中发现的瓶颈，论文提出了三种受人类认知启发的策略：回溯机制（B）、空间认知增强（C）和基于记忆的检索（R）。</p>
<p><img src="https://arxiv.org/html/2512.16755v1/Figures/bcr.png" alt="BCR策略总览"></p>
<blockquote>
<p><strong>图7</strong>：BCR方法概述，包含回溯机制、空间认知增强和基于记忆的检索，旨在增强VLM在城市VLN任务中的性能。</p>
</blockquote>
<ul>
<li><p><strong>回溯机制（Backtracking Mechanisms）</strong>：旨在纠正累积的导航错误。</p>
<ul>
<li><strong>B1（基本回溯）</strong>：当智能体在滑动窗口k步内的平均内部置信度低于阈值θ时触发，回退到最后可信节点。</li>
<li><strong>B2（步进奖励回溯）</strong>：使用到目标的拓扑距离d_t作为客观进度指标。如果该距离连续k步单调增加，则触发回溯。</li>
<li><strong>B3（人类引导回溯）</strong>：在B1回退后，提供一个方向提示（指向最小化预期未来距离的动作a*），以重新调整轨迹。</li>
</ul>
</li>
<li><p><strong>空间认知增强（Spatial Cognition Enrichment）</strong>：旨在提升智能体的环境全局意识。使用GPT-4.1合成来自不同VLM的成功和错误轨迹知识，然后以两种格式注入智能体提示：</p>
<ul>
<li><strong>C1（轨迹描述）</strong>：提供成功轨迹的文本描述作为上下文。</li>
<li><strong>C2（错误分析）</strong>：提供常见错误模式的分析，帮助智能体避免典型陷阱。</li>
</ul>
</li>
<li><p><strong>基于记忆的检索（Memory-Based Retrieval）</strong>：旨在实现记忆告知的决策。建立一个外部向量数据库，存储过去导航实例的（状态，动作）对。</p>
<ul>
<li><strong>R1（最近邻检索）</strong>：在每一步，根据当前观察和指令检索最相似的k个历史实例，将其动作分布作为先验指导当前决策。</li>
<li><strong>R2（轨迹检索）</strong>：检索与当前部分轨迹最匹配的完整历史轨迹，用其剩余路径作为参考。</li>
<li><strong>R3（混合检索）</strong>：结合R1和R2，既考虑单步相似性，也考虑序列一致性。</li>
</ul>
</li>
</ul>
<p>与现有方法相比，创新点在于首次系统评估VLM对隐含需求的响应能力，并提出了一个受认知科学启发的、模块化的策略框架（BCR）来针对性解决长时程推理、空间认知不足和缺乏经验回忆等关键瓶颈。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：使用CitySeeker基准，包含来自8个城市（北京、上海、深圳、成都、香港、伦敦、纽约）的6,440条轨迹，涵盖7个目标驱动场景。从中采样1,257条作为最终测试集。评估了27个支持多图像的VLM，包括GPT-4o、Gemini系列、Qwen2-VL系列、InternVL系列、Llama系列等。</p>
<p><strong>对比方法</strong>：设置了三个基线：1) <strong>人类基线</strong>（10名参与者）；2) <strong>随机选择基线</strong>（每步随机选方向）；3) <strong>向前方向基线</strong>（始终选择向前方向）。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>整体性能低下</strong>：如表2所示，即使在最宽松的成功标准TCP（终点50米内）下，表现最好的模型（Qwen2.5-VL-32B-Instruct）任务完成率也仅为21.1%。严格的终点匹配标准TCE下，所有模型成功率均低于2.6%。</li>
<li><strong>人类显著优于模型</strong>：人类参与者的TCP达到30.1%，TCC达到13.5%，显著优于所有VLM，尤其在需要城市常识的任务（如“交通枢纽”导航）上优势明显。</li>
<li><strong>任务类别差异</strong>：模型在“品牌特定”导航（TCP约30.4%）上表现最好，而在需要推理间接可观察目标的“潜在POI”（TCP约14.2%）和高度抽象的“抽象需求”（TCP约24.1%）上表现较差，揭示了VLM擅长直接识别但缺乏深度推理的局限。</li>
<li><strong>城市间性能差异</strong>：如图5右侧柱状图所示，模型性能因城市而异。例如，GPT-4o在北京表现不佳，却在纽约得分最高，这可能反映了训练数据偏差或街道布局（网格状vs非网格状）的影响。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.16755v1/Figures/bar+ld-1.jpg" alt="性能对比与城市差异"></p>
<blockquote>
<p><strong>图5</strong>：左图雷达图展示了不同模型在各类别任务上的TCP性能，揭示了模型在不同认知难度任务上的能力差异。右图柱状图展示了不同模型在纽约（左）和北京（右）的TCP性能，显示了跨城市的性能波动。</p>
</blockquote>
<ul>
<li><strong>轨迹分析</strong>：如图6所示，随着路径步数增加（&gt;20步），nDTW指标变得分散，表明长时程推理中错误累积。常见错误模式包括轨迹偏离和振荡式绕路，许多模型表现出循环行为（重复访问同一节点）。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.16755v1/Figures/maps-1.jpg" alt="轨迹分析与错误模式"></p>
<blockquote>
<p><strong>图6</strong>：左图热力图展示了GPT-4o在纽约和北京的导航轨迹分布，右图散点图显示了模型生成轨迹步数与nDTW的关系，表明长路径下性能下降和错误累积。</p>
</blockquote>
<p><strong>BCR策略消融实验</strong>：<br>在650个样本的子集上测试BCR策略（表3）。结果表明：</p>
<ul>
<li><strong>回溯机制</strong>：B2和B3普遍有效，能提升TCP并改善路径效率（降低nDTW）。例如，B3将GPT-4o-Mini的TCP从12.5%提升至18.2%，nDTW从337.1降至258.3。但B1对小模型（如MiniCPM-V-2.6）可能有害，说明其缺乏有效的自我评估能力。</li>
<li><strong>空间认知增强</strong>：C1（轨迹描述）对大多数模型有积极效果，而C2（错误分析）效果不稳定，有时甚至导致性能下降。</li>
<li><strong>基于记忆的检索</strong>：R1（最近邻检索）通常带来最一致的提升，例如将Qwen2.5-VL-32B的TCP从19.9%提升至26.9%。R2和R3在某些模型上也有提升，但效果不如R1稳定。</li>
<li><strong>组件贡献</strong>：总体而言，<strong>基于记忆的检索（R）</strong> 带来的提升最为显著，其次是<strong>回溯机制（B）</strong>，而<strong>空间认知增强（C）</strong> 的效果相对有限且不稳定。这突显了为VLM装备外部记忆/经验库对于解决隐含需求导航的重要性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>CitySeeker</strong>，首个针对多城市环境中隐含需求的大规模具身城市导航基准，涵盖了真实视觉多样性、长时程规划和非结构化指令。</li>
<li>设计了一个基于VLM的评估框架，并受人类认知启发，提出了一套探索性策略（<strong>BCR</strong>），通过迭代的观察-推理循环将隐含需求转化为多步计划。</li>
<li>通过大量实验，揭示了VLM空间推理中的关键瓶颈（长时程错误累积、空间认知不足、经验回忆缺乏），并将这些瓶颈及其修复方案具体化为BCR三元组，为推进空间智能提供了有见地的指导。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>CitySeeker基准的规模（6,440条轨迹）和城市覆盖（8个）仍有扩展空间。</li>
<li>BCR策略仅在较小的子集上进行了初步实验，其组合效果以及对不同模型架构的普适性有待进一步全面验证。</li>
<li>当前的导航框架为了隔离核心能力，故意没有包含持久记忆机制，这与人类持续构建和更新认知地图的过程不同。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>开发更强大的空间认知与记忆机制</strong>：实验表明记忆检索（R）策略最有效，未来研究应致力于为VLM构建更复杂、动态更新的世界模型或外部记忆体，以支持长时程推理和常识积累。</li>
<li><strong>探索跨模态的深度融合</strong>：研究发现提供全局2D地图反而会降低性能，这提示简单的地理信息叠加不足，需要研究如何更有效地将地图先验、视觉流和语言指令进行深度融合与对齐。</li>
<li><strong>推进对“隐含性”的建模</strong>：需要开发能够更好理解功能可供性、空间关联性和主观语义属性的模型，以应对从抽象目标到具体视觉搜索的接地挑战。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型在动态城市环境中理解隐含人类需求（如“我渴了”）进行导航的核心问题，提出了CitySeeker基准。该基准包含8个城市、7类场景下的6440条轨迹。实验发现，即使最优模型任务完成率也仅为21.1%，主要瓶颈在于长视野推理中的错误累积、空间认知不足及经验回忆缺陷。为此，论文借鉴人类认知映射，提出了回溯机制、丰富空间认知和基于记忆检索的BCR策略进行分析。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.16755" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>