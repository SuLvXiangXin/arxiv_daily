<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Swing-up Maneuvers for a Suspended Aerial Manipulation Platform in a Hierarchical Control Framework - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Swing-up Maneuvers for a Suspended Aerial Manipulation Platform in a Hierarchical Control Framework</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.13478" target="_blank" rel="noreferrer">2506.13478</a></span>
        <span>作者: Christian Ott Team</span>
        <span>日期: 2025-06-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前空中机器人操纵系统，通常由多旋翼平台搭载机械臂构成，在执行物理交互任务时存在显著局限：为稳定整个系统需要消耗大量能量，导致飞行时间和有效载荷能力下降；大型螺旋桨还会对周围环境产生湍流。为应对这些挑战，悬挂式空中操作平台（SAM）被提出，它通过刚性缆绳悬挂于载具系统，仅需一个全向平台提供平面推力和偏航扭矩，从而更节能、更紧凑。这类平台已在分层控制框架下展示了与未知环境的顺应交互能力。</p>
<p>然而，本文旨在扩展此类平台的应用，使其能够执行“摆起”动作，从而停栖在仅凭平台推力无法抵达的位置（如建筑工地起重机作业点）。现有的摆起控制方法（如针对单倒立摆或球摆）通常依赖于专门设计的机械系统或工业机器人，并且往往专注于单一摆起任务。本文针对的痛点是：<strong>如何让悬挂式空中操作平台在保持末端执行器特定姿态和位置（以便观察和抓取目标）的同时，执行能量高效的摆起动作</strong>。本文提出了一种新视角：将分层任务优先级控制与强化学习相结合，核心思路是：<strong>利用分层控制框架确保高优先级任务（末端执行器姿态/位置稳定）被严格满足，同时让强化学习智能体在高层任务的零空间内，通过调整低优先级任务（特定关节运动）的参考设定值来学习摆起动作</strong>。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的控制框架是一个层次化的整体-身体控制器，其核心目标是实现任务间的动态解耦，确保低优先级任务不影响高优先级任务的执行。</p>
<p><img src="https://arxiv.org/html/2506.13478v1/x3.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图3</strong>：所提出的控制框架概览。系统动力学输出关节位置 <code>q</code> 和速度 <code>\dot{q}</code>，经过雅可比变换和零空间投影得到解耦的任务空间速度 <code>v</code>。任务1控制器根据高优先级任务参考值 <code>x_{1,ref}</code> 计算任务力 <code>F_1</code>。强化学习（RL）智能体根据系统状态输出低优先级任务（关节 <code>q_5</code>, <code>q_7</code>）的参考值 <code>q_{5,ref}</code>, <code>q_{7,ref}</code>，任务2控制器据此计算任务力 <code>F_2</code>。两者结合并通过逆动力学计算最终关节扭矩 <code>τ</code>。</p>
</blockquote>
<p><strong>1. 系统建模与任务定义</strong>：<br>系统被建模为一个10自由度的系统，其中 <code>q1, q2, q3</code> 表示悬挂缆绳绕其 <code>z, y, x</code> 轴的旋转（平台摆动角），<code>q4</code> 到 <code>q10</code> 为7自由度机械臂（Kinnova Gen3）的关节角。平台可施加平面力/扭矩 <code>[Fx, Fy, τz]^T</code>。</p>
<p>本文定义了两个任务空间：</p>
<ul>
<li>**高优先级任务 <code>x1 ∈ R^5</code>**：<code>[ye, φ, θ, ψ, q1]^T</code>。包括末端执行器沿其 <code>y</code> 轴的位置 <code>ye</code>、末端执行器的欧拉角 <code>φ, θ, ψ</code>（绕 <code>x, y, z</code> 轴）以及平台的偏航角 <code>q1</code>。该任务确保在摆起过程中，搭载的相机始终对准目标。</li>
<li>**低优先级任务 <code>x2 ∈ R^5</code>**：<code>[q2, q4, q5, q6, q7]^T</code>。包括沿摆起轴（<code>x-z</code>平面）的平台摆动角 <code>q2</code>，以及机械臂的四个特定关节 <code>q4, q5, q6, q7</code>。该任务用于驱动摆起动作。</li>
</ul>
<p><strong>2. 分层控制与动态解耦</strong>：<br>为实现任务严格分级，计算任务雅可比 <code>J1</code>, <code>J2</code>。通过定义零空间投影矩阵 <code>N = I - J1^T J1^(M+,T)</code>（其中 <code>M+</code> 表示动态一致伪逆），将第二个任务投影到第一个任务的零空间中。进而定义解耦的雅可比矩阵 <code>\bar{J} = [J1; J2 N^T]</code> 和解耦的任务空间速度 <code>v = \bar{J} \dot{q}</code>。通过对系统动力学方程进行坐标变换，得到解耦的任务空间动力学方程 <code>Λ v̇ + μ v = \bar{J}^{-T} (τ - g)</code>。</p>
<p><strong>3. 控制律设计</strong>：<br>关节控制扭矩设计为 <code>τ = g + τ_u + \bar{J}^T F</code>。其中：</p>
<ul>
<li><code>g</code>：补偿重力。</li>
<li><code>τ_u</code>：补偿变换后的科里奥利耦合项。</li>
<li><code>F = [F1; F2]^T</code>：任务空间力，通过PD控制律计算，分别驱动任务1和任务2。<code>F2</code> 中对应关节 <code>q2</code> 的分量仅用于抵消其重力 <code>g2</code>，而对应 <code>q4, q6</code> 的分量指令为零，以确保摆起动作主要由机械臂关节 <code>q5, q7</code> 的运动产生，从而最小化平台推力消耗，实现能量高效。</li>
</ul>
<p><strong>4. 强化学习智能体集成</strong>：<br>创新点在于使用强化学习（Soft Actor-Critic, SAC算法）来为低优先级任务中的关键关节 <code>q5</code> 和 <code>q7</code> 生成动态的参考轨迹 <code>q_{5,ref}, q_{7,ref}</code>。</p>
<ul>
<li><strong>观察空间</strong>：所有关节的位置 <code>q</code> 和速度 <code>\dot{q}</code>。</li>
<li><strong>动作空间</strong>：两个肘关节 <code>q5</code> 和 <code>q7</code> 的参考位置。</li>
<li><strong>奖励函数</strong>：基于平台摆动角 <code>q2</code> 误差的指数函数，鼓励智能体学习将 <code>q2</code> 摆动到目标角度（例如垂直向上位置）。<br>智能体的策略学习过程被限制在高优先级任务 <code>x1</code> 的零空间内，这意味着它调整 <code>q5, q7</code> 参考值的所有尝试都不会干扰末端执行器的稳定观察任务。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验通过广泛的数值模拟研究进行验证。仿真平台未具体说明，但系统参数基于真实的悬挂平台和Kinnova Gen3机械臂模型。</p>
<p><strong>1. 学习曲线与成功率</strong>：</p>
<p><img src="https://arxiv.org/html/2506.13478v1/x4.png" alt="学习曲线"></p>
<blockquote>
<p><strong>图4</strong>：RL智能体在训练过程中的学习曲线。展示了超过6000个训练周期（约300万时间步）的平均奖励和摆起任务成功率。成功率最终收敛至约95%，证明了学习策略的有效性和稳定性。</p>
</blockquote>
<p><strong>2. 摆起动作的关节轨迹与平台运动</strong>：</p>
<p><img src="https://arxiv.org/html/2506.13478v1/x5.png" alt="关节轨迹与平台角"></p>
<blockquote>
<p><strong>图5</strong>：一次成功摆起动作中，RL控制的关节 <code>q5</code>、<code>q7</code> 的参考轨迹与实际轨迹，以及平台摆动角 <code>q2</code> 的变化。智能体学会了生成协调的周期性关节参考，从而驱动 <code>q2</code> 从初始的 -1.3 rad 摆动到目标值 0 rad（垂直）。</p>
</blockquote>
<p><strong>3. 高优先级任务性能</strong>：</p>
<p><img src="https://arxiv.org/html/2506.13478v1/x6.png" alt="任务1误差"></p>
<blockquote>
<p><strong>图6</strong>：在执行摆起动作期间，高优先级任务 <code>x1</code>（末端执行器位置 <code>ye</code>、姿态 <code>φ, θ, ψ</code>、平台偏航 <code>q1</code>）的跟踪误差。所有误差均保持在极低水平（<code>ye</code> 误差 &lt; 0.004 m，姿态误差 &lt; 0.015 rad），证明了分层控制成功地将摆起动作限制在了高优先级任务的零空间内，未对其产生干扰。</p>
</blockquote>
<p><strong>4. 平台推力使用情况</strong>：</p>
<p><img src="https://arxiv.org/html/2506.13478v1/x7.png" alt="平台力与扭矩"></p>
<blockquote>
<p><strong>图7</strong>：平台在摆起过程中所需的平面力 <code>Fx, Fy</code> 和扭矩 <code>τz</code>。<code>Fx</code>（摆起平面内的力）和 <code>τz</code> 的值非常小（<code>Fx</code> &lt; 2.5 N, <code>τz</code> &lt; 0.8 Nm），验证了方法确实最小化了平台推进力的使用，主要依靠机械臂运动完成摆起。</p>
</blockquote>
<p><strong>5. 消融实验</strong>：<br>论文通过对比实验验证了各核心组件的必要性。</p>
<ul>
<li><strong>仅用平台推力摆起</strong>：作为基准，仅使用平台推力（通过PD控制 <code>q2</code>）进行摆起。结果显示需要很大的峰值推力（<code>Fx</code> &gt; 25 N），且无法同时保证任务1的精度。</li>
<li><strong>无RL的分层控制</strong>：在分层框架下，为 <code>q5, q7</code> 设置固定的正弦参考。此方法失败，因为固定的激励频率无法适应系统变化，无法积累足够能量完成摆起。</li>
<li><strong>完整方法（分层+RL）</strong>：如上所述，成功率高，平台推力小，任务1精度完美保持。</li>
</ul>
<p><strong>6. 不同初始条件下的泛化能力</strong>：<br>论文测试了学习到的策略在不同初始平台摆动角（<code>q2(0)</code> 从 -1.3 rad 到 -1.0 rad）下的表现。</p>
<p><img src="https://arxiv.org/html/2506.13478v1/x8.png" alt="不同初始条件结果"></p>
<blockquote>
<p><strong>图8</strong>：在不同初始条件下摆起动作的成功率、摆起时间以及平台推力 <code>Fx</code> 的范数。成功率保持在90%以上，摆起时间在7.5至8.5秒之间，推力使用依然很低，证明了策略的鲁棒性和泛化能力。</p>
</blockquote>
<p><strong>7. 与模型预测控制（MPC）的对比</strong>：<br>论文还将所提方法与一种分层MPC方法进行对比，其中MPC用于生成 <code>q5, q7</code> 的参考轨迹。</p>
<p><img src="https://arxiv.org/html/2506.13478v1/x9.png" alt="与MPC对比"></p>
<blockquote>
<p><strong>图9</strong>：RL与MPC方法在摆起时间、成功率和计算时间上的对比。RL方法在成功率（95% vs 85%）和计算效率（单步计算时间远低于MPC）方面均优于MPC。</p>
</blockquote>
<p><strong>8. 实物系统参数仿真中的表现</strong>：<br>论文在另一组更接近实际硬件参数的仿真中测试了方法，包括执行器带宽、速率和扭矩限制。</p>
<p><img src="https://arxiv.org/html/2506.13478v1/x10.png" alt="实物参数仿真结果"></p>
<blockquote>
<p><strong>图10</strong>：在考虑实际执行器限制的仿真中，任务1误差、平台摆动角 <code>q2</code> 和平台推力 <code>Fx</code> 的变化曲线。即使存在限制，高优先级任务误差仍然极小，摆起成功完成，且平台推力使用合理，证明了方法的实际可行性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的分层控制与强化学习融合框架，用于解决悬挂式空中操作平台的多任务协同控制问题。</li>
<li>成功演示了在严格保证高优先级任务（末端执行器精确定向与定位）的前提下，通过RL智能体在任务零空间内学习调整机械臂关节参考，实现能量高效的平台摆起动作。</li>
<li>通过全面的仿真研究验证了方法的有效性、鲁棒性以及相较于纯平台推力控制、固定激励控制、模型预测控制（MPC）的优越性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性在于，目前的工作仅包含仿真验证，尚未在物理硬件系统上进行实验。将学习到的策略安全地迁移到真实世界平台是未来的关键步骤。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>证明了分层任务优先级控制为强化学习提供了一个结构化的“安全沙箱”，允许智能体在保证关键约束不被违反的前提下探索和学习复杂技能。这种结合模式可推广至其他需要满足安全或操作约束的机器人学习任务中。</li>
<li>为空中操作机器人实现更节能、更复杂的动态动作（如投掷、摆动穿越）提供了新的思路，即利用机械臂的惯性力作为主要驱动力，而非持续消耗高的平台推力。</li>
<li>后续工作可集中在sim-to-real迁移、考虑更复杂的环境交互（如风扰、负载变化）以及扩展任务层次（如纳入避障作为更高优先级任务）。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对悬挂式空中操作平台无法仅靠推力抵达目标位置的问题，提出一种结合分层控制与强化学习的摆动上升策略。核心方法是在分层控制框架中，高层任务保持末端执行器位姿，强化学习智能体在高层任务的零空间内调整低优先级任务的参考设定点，驱动机械臂实现摆动。该方法通过大量数值仿真验证了有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.13478" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>