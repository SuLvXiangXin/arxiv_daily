<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.18899" target="_blank" rel="noreferrer">2505.18899</a></span>
        <span>作者: Ramazzina, Andrea, Giammarino, Vittorio, El-Hariry, Matteo, Bijelic, Mario</span>
        <span>日期: 2025/05/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉观察模仿（V-IfO）旨在让智能体通过观看专家视频学习策略，而无需动作标签。然而，现有最先进的端到端算法通常假设专家演示与智能体部署环境在视觉上一致。这在真实世界中不成立，光照、颜色、纹理等视觉不匹配会导致模仿失败。现有解决视觉不匹配的方法主要分为两类：基于生成的方法（如域转换）计算成本高且实现复杂；基于特征学习的方法（如对比学习、对抗性特征对齐）试图提取域不变特征，但依赖于计算昂贵且需要领域先验知识的数据增强技术。本文针对这一痛点，提出了一个全新的视角：与其通过增强或学习来应对视觉变化，不如从根本上消除外观信息的影响。其核心思路是，受生物视觉系统（视网膜神经节细胞编码强度变化）和事件相机的启发，将标准RGB视频转换为稀疏的、仅编码时间强度梯度（即事件）的表示，从而分离运动动态与视觉风格，实现对视觉干扰的固有不变性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为EB-LAIfO，其整体流程是：首先将连续的RGB观测帧通过一个确定性的转换模块ζ，生成事件流表示；然后，一个特征提取网络从事件流中提取潜在特征；这些特征同时被送入一个判别器（用于生成模仿奖励）和一个Q函数网络（用于策略优化），最终通过对抗模仿学习框架训练策略。</p>
<p><img src="https://arxiv.org/html/2505.18899v1/extracted/6475408/Figures/EB-LAIFO2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：EB-LAIfO整体框架。给定RGB序列，根据公式2提取对应的事件流。特征提取器网络φ_δ用于生成潜在特征z_δ，该特征同时被Q函数Q_ψ和判别器D_χ用于模仿学习。判别器按公式4训练并返回奖励函数r_χ，随后通过RL步骤（遵循DDPG流程）最大化该奖励。</p>
</blockquote>
<p>核心模块是<strong>事件转换模块ζ</strong>。它将观测空间𝒳分解为目标相关信息𝒳¯和视觉干扰𝒳^。其作用是直接从原始观测中提取𝒳¯，摒弃𝒳^。具体而言，ζ接收连续两帧RGB图像x_t和x_{t-1}，为每个像素(u, v)计算对数亮度变化：L_t(u,v) = log(I_t(u,v))。当亮度变化超过预设阈值C时，则生成一个事件。输出的事件表示x¯_t是一个与输入图像同尺寸的矩阵，每个像素值为：</p>
<ul>
<li>+1，如果 L_t(u,v) - L_{t-1}(u,v) ≥ C</li>
<li>-1，如果 L_t(u,v) - L_{t-1}(u,v) ≤ -C</li>
<li>0， 其他情况<br>这模拟了事件相机的工作原理，生成一个稀疏的、仅包含边缘和运动信息的二值化表示（+1/-1代表亮度增加/减少的极性，0代表无变化）。</li>
</ul>
<p><strong>特征提取与对抗模仿学习流程</strong>：生成的事件流x¯_t被送入一个卷积特征提取器φ_δ，得到潜在特征z_δ。该方法建立在LAIfO（一种隐式对抗模仿学习方法）之上。一个判别器D_χ接收特征z_δ，并试图区分其来自专家轨迹还是智能体轨迹。判别器的输出被转化为奖励r_χ = -log(1 - D_χ(z))，用于驱动强化学习（RL）步骤。RL部分采用DDPG算法，Q函数Q_ψ也接收相同的特征z_δ。特征提取器φ_δ与Q函数联合训练，以从事件流中提取与任务相关的信息。</p>
<p>与现有方法相比，<strong>核心创新点</strong>在于：它不是在学习的特征空间（𝒵）中强制实现域不变性，而是在原始观测空间（𝒳）层面，通过基于物理模型的确定性转换ζ，直接过滤掉非必要的视觉干扰信号（颜色、纹理、恒定光照），同时保留了控制所必需的运动动力学信息。这避免了昂贵的数据增强和复杂的域不变特征学习过程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在DeepMind Control Suite（DMC）的<strong>Cheetah Run</strong>和<strong>Walker Walk</strong>任务，以及<strong>Adroit</strong>灵巧操作平台的<strong>Door-open</strong>和<strong>Hammer-wield</strong>任务上评估。视觉不匹配包括颜色随机化（身体颜色、地板颜色、背景颜色）、光照变化、视频噪声等。实验平台基于PyTorch。</p>
<p><strong>对比方法</strong>：包括处理视觉不匹配的SOTA方法<strong>C-LAIfO</strong>（基于对比学习），以及作为基线的标准V-IfO方法<strong>LAIfO</strong>和<strong>PatchAIL</strong>。还对比了仅使用灰度图（Grayscale）而非事件表示的简化版本。</p>
<p><strong>关键定量结果</strong>：<br>在DMC的Walker Walk任务中，EB-LAIfO在颜色随机化下的最终成功率比C-LAIfO高约13%，比LAIfO高约30%。在Cheetah Run任务中，EB-LAIfO的成功率比C-LAIfO高约10%，比LAIfO高约25%。<br>在Adroit的Door任务中，EB-LAIfO在颜色不匹配下的成功率接近80%，显著高于C-LAIfO（约55%）和LAIfO（约20%）。在Hammer任务中，EB-LAIfO成功率超过70%，而C-LAIfO和LAIfO均低于40%。</p>
<p><img src="https://arxiv.org/html/2505.18899v1/x1.png" alt="DMC定量结果"></p>
<blockquote>
<p><strong>图10</strong>：DeepMind Control Suite任务（Cheetah, Walker）在多种视觉不匹配下的平均成功率曲线。EB-LAIfO（橙色）在几乎所有不匹配设置下都达到最高且最稳定的性能，显著优于对比方法C-LAIfO（蓝色）和基线LAIfO（绿色）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.18899v1/x2.png" alt="Adroit定量结果"></p>
<blockquote>
<p><strong>图11</strong>：Adroit灵巧操作任务（Door, Hammer）在颜色不匹配下的平均成功率曲线。EB-LAIfO（橙色）的性能远超其他方法，展示了其在需要精细运动对齐的复杂任务中的鲁棒性。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文通过替换转换模块ζ进行了消融研究：1) <strong>Identity</strong>：直接使用RGB帧（即LAIfO）；2) <strong>Grayscale</strong>：转换为灰度帧；3) <strong>Canny Edges</strong>：使用Canny边缘检测器；4) <strong>Optical Flow</strong>：计算光流；5) **Events (Ours)**：本文的事件表示。<br>结果表明，在颜色随机化下，Event表示的性能最佳（Walker任务成功率<del>92%），显著高于Grayscale（</del>75%）、Canny Edges（<del>65%）和Optical Flow（</del>50%）。这证明了事件表示在丢弃外观信息、保留运动信息方面的独特优势。</p>
<p><img src="https://arxiv.org/html/2505.18899v1/x3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图22</strong>：在Walker Walk任务颜色随机化下的消融研究。使用不同观测转换模块ζ的最终成功率对比。本文的事件表示（Events）取得了最佳性能，证明了其设计的有效性。</p>
</blockquote>
<p><strong>定性结果</strong>：<br>论文提供了大量可视化图，对比了专家域、智能体域以及它们对应的事件表示。例如，图2-9展示了DMC任务中各种颜色和光照不匹配的RGB帧及其对应的事件帧。可以看到，尽管RGB外观差异巨大，但事件表示却高度一致，主要捕捉了智能体（如Cheetah、Walker）的轮廓和运动边缘。</p>
<p><img src="https://arxiv.org/html/2505.18899v1/extracted/6475408/Figures/Door.png" alt="Door任务可视化"></p>
<blockquote>
<p><strong>图12</strong>：Adroit Door任务的可视化。左侧为专家演示的RGB帧和事件帧，右侧为智能体在完全不同颜色配置下执行时的RGB帧和事件帧。事件表示有效地对齐了关键的运动信息（如手和门把手的相对位置）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个轻量级的、受事件启发的感知模块，通过将RGB视频转换为稀疏事件表示，从根本上消除了颜色、纹理、光照等外观干扰，同时保留了关键的时序梯度（运动）信息。</li>
<li>该方法直接从RGB视频合成事件流，绕过了计算昂贵的域随机化或手工设计的数据增强流程，将视觉不匹配的模仿问题简化为标准V-IfO问题。</li>
<li>通过证明在合成事件流上模仿的鲁棒性，为未来使用低功耗、高时间分辨率的事件相机进行真实世界部署铺平了道路。</li>
</ol>
<p><strong>局限性</strong>：论文提到，事件表示可能不适用于外观信息本身就是任务关键部分的场景（例如，基于颜色的识别任务）。此外，事件生成阈值C的选择可能影响性能，需要根据任务进行调整。</p>
<p><strong>对后续研究的启示</strong>：这项工作展示了改变感知表示本身（而非仅处理数据）的强大潜力。未来的方向包括：探索更复杂的事件表示编码方式；研究将异步事件流直接用于策略学习；结合生成模型，处理事件相机可能缺失的某些静态场景信息；以及在实际机器人平台上集成事件相机进行验证。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决视觉模仿学习中专家演示与代理环境间视觉领域偏移（如光照、颜色差异）导致失败的核心问题。提出事件启发感知方法，将RGB视频转换为稀疏事件表示，编码时间强度梯度并丢弃静态外观特征，从而分离运动动态与视觉风格。在DeepMind Control Suite和Adroit平台的实验验证了该方法能有效实现对外观干扰的不变性，无需依赖计算昂贵的数据增强技术。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.18899" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>