<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.11218" target="_blank" rel="noreferrer">2512.11218</a></span>
        <span>作者: Xu, Kechun, Zhu, Zhenjie, Chen, Anzhe, Zhao, Shuqi, Huang, Qing, Yang, Yifei, Lu, Haojian, Xiong, Rong, Tomizuka, Masayoshi, Wang, Yue</span>
        <span>日期: 2025/12/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前构建通用视觉-语言-动作模型的主流方法是利用预训练的视觉语言模型作为主干，然后在机器人数据集上进行微调。这种方法面临一个关键局限性：灾难性遗忘。微调过程会覆盖VLM主干中预训练的表征，导致模型在训练中见过的指令上表现良好，但失去了遵循未见指令的能力。现有缓解方案是联合训练，即同时使用外部推理数据和机器人数据进行训练，但这需要启发式地调整数据比例并引入额外的数据收集、存储和训练开销。</p>
<p>本文认为联合训练的成功掩盖了一个更深层的问题：VLA数据集中固有的模态不平衡。这种结构性问题表现为多样性的显著差异，即语言指令的多样性远低于对应的视觉观察和动作的多样性。这种不平衡促使模型丢弃细致的语言理解，转而利用视觉捷径，加剧了语言泛化的退化。因此，本文的核心思路是通过贝叶斯分解，将策略结构性地分解为视觉-动作先验和语言条件似然，从而在不依赖外部推理数据的情况下，从架构层面解决模态不平衡问题，提升指令跟随的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的BayesVLA框架基于贝叶斯分解，将VLA策略建模为先验和似然的乘积：π(𝐚|𝐯, ℓ) ∝ π^p(𝐚|𝐯) L(ℓ|𝐯, 𝐚)。其中，π^p(𝐚|𝐯) 是视觉-动作先验，负责根据视觉观察生成多模态动作分布（Seeing to Act）；L(ℓ|𝐯, 𝐚) 是语言条件似然，负责根据语言指令评估和对齐动作先验（Prompting to Specify）。这种分解自然引导出一个两阶段的训练流程。</p>
<p><img src="https://arxiv.org/html/2512.11218v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：BayesVLA整体框架。给定存在模态不平衡的VLA数据集，方法将策略分解为先验和似然。第一阶段训练先验模型 π^p(𝐚|𝐯)，根据视觉输入生成多模态动作分布。第二阶段基于先验训练似然模型，将动作先验与语言指令对齐。</p>
</blockquote>
<p>整体框架分为两个阶段：1）动作生成（先验训练）：训练先验模型仅利用视觉-动作对，学习基础的操作技能，不受具体任务指令影响。2）语言对齐（似然训练）：在固定先验的基础上，训练轻量级的似然模型，利用预训练基础模型的语义知识，评估先验生成的动作提案与语言指令的一致性。</p>
<p>创新点在于将端到端策略学习分解为两个解耦的子问题，从结构上缓解了数据模态不平衡。先验学习充分利用了视觉-动作对中平衡的多样性，而似然学习则通过轻量化的适配，更好地保留了预训练基础模型的泛化能力。</p>
<p>本文进一步将物体操控分解为接触前和接触后两个阶段，并为每个阶段实例化了具体的先验和似然架构。</p>
<p><img src="https://arxiv.org/html/2512.11218v1/x3.png" alt="接触前架构"></p>
<blockquote>
<p><strong>图3</strong>：接触前阶段架构。利用预训练的动作基础模型（如AnyGrasp）作为先验，生成多个接触姿态作为动作令牌。然后，文本令牌和文本感知的视觉令牌通过交叉注意力层注入，预测各动作的概率（似然）。</p>
</blockquote>
<p>对于接触前阶段（如抓取），先验直接采用预训练的动作基础模型来生成多模态的接触姿态提案。似然模型则利用CLIP提取文本感知的视觉特征：计算每个视觉块令牌与全局文本嵌入的余弦相似度，并以此加权视觉令牌。随后，将文本感知视觉令牌与文本令牌拼接，作为键和值，与编码后的动作提案令牌（查询）进行交叉注意力计算，最终输出每个动作提案与指令对齐的概率。训练时固定基础模型参数，仅训练似然部分。</p>
<p><img src="https://arxiv.org/html/2512.11218v1/x4.png" alt="接触后架构"></p>
<blockquote>
<p><strong>图4</strong>：接触后阶段架构。首先训练一个基于扩散的轨迹生成器（先验）。然后，后期注入额外的注意力层，以条件化文本令牌和文本感知视觉令牌，实现语言对齐（似然）。仅微调这些增加的注意力层和动作头。</p>
</blockquote>
<p>对于接触后阶段（如放置、倾倒），先验是一个视觉条件扩散变换器，用于生成密集的多模态操作轨迹。它使用Plücker射线图相机姿态嵌入，并通过FiLM注入历史动作、机器人本体感知和去噪时间步。在似然训练阶段，采用潜在适配策略：在预训练好的扩散骨干网络中后期注入额外的注意力层，将文本感知视觉令牌和文本令牌作为键和值，与先验输出的动作潜在表示（查询）进行交互。仅训练这些新增的注意力层和动作头，从而在保持先验多样性的同时引入语言对齐。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在公开基准和自建仿真环境中进行，包括CALVIN、Language-Table以及自建的多物体、多任务仿真环境。对比的基线方法包括RT-1、Octo、Diffusion Policy、VLA等主流模型。评估涵盖了对未见指令、未见物体和未见环境的泛化能力。</p>
<p><img src="https://arxiv.org/html/2512.11218v1/x5.png" alt="仿真结果"></p>
<blockquote>
<p><strong>图5</strong>：在CALVIN基准上的语言泛化结果。BayesVLA在未见指令上的表现显著优于所有基线方法。</p>
</blockquote>
<p>在CALVIN基准的未见指令泛化测试中，BayesVLA取得了60.2%的成功率，优于VLA的54.1%和Diffusion Policy的39.6%。在Language-Table基准上，对于组合泛化任务，BayesVLA达到85.3%的成功率，比最佳基线VLA高出6.1%。</p>
<p><img src="https://arxiv.org/html/2512.11218v1/x6.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图6</strong>：真实世界语言泛化任务的成功率。BayesVLA在多个任务上表现最佳。</p>
</blockquote>
<p>在真实世界实验中，设计了涉及未见物体和复杂指令的任务。BayesVLA在“放置到相对位置”任务上达到87.5%成功率，在“按属性选择”任务上达到75.0%成功率，均显著高于基线方法。</p>
<p><img src="https://arxiv.org/html/2512.11218v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融研究结果。依次移除贝叶斯分解、两阶段训练和文本感知视觉特征，性能逐步下降，验证了各组件的重要性。</p>
</blockquote>
<p>消融实验验证了各核心组件的贡献：1）<strong>贝叶斯分解</strong>：使用端到端单阶段训练替代，性能下降最显著（-8.3%），证明了分解的有效性。2）<strong>两阶段训练</strong>：在先验和似然中使用相同数据联合训练，性能下降（-5.1%），说明数据分离防止了似然塌缩。3）<strong>文本感知视觉特征</strong>：在似然中使用原始视觉特征替代，性能下降（-3.7%），突出了语言引导视觉关注的重要性。</p>
<p><img src="https://arxiv.org/html/2512.11218v1/x8.png" alt="定性结果"></p>
<blockquote>
<p><strong>图8</strong>：接触前阶段的定性示例。BayesVLA能根据“抓取马克杯手柄”的指令，从多个可行的抓取姿态中正确选择符合语义的那个。</p>
</blockquote>
<p>此外，图9-16展示了更多仿真与真实世界的定量对比、轨迹可视化、失败案例分析以及计算效率对比，进一步支撑了BayesVLA在泛化性和实用性上的优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一个<strong>贝叶斯分解框架</strong>，将VLA策略结构性地分解为视觉-动作先验和语言条件似然，为处理模态不平衡提供了理论依据和架构指导。2）实例化了<strong>BayesVLA模型</strong>，针对物体操控的接触前和接触后阶段，设计了具体的先验与似然实现方案，并创新性地使用了预训练动作基础模型和潜在适配技术。3）提供了<strong>信息论分析</strong>，形式化地验证了数据偏差如何导致捷径学习，以及因子分解如何缓解该问题。</p>
<p>论文提到的局限性包括：方法依赖于预训练的基础模型（如CLIP、AnyGrasp）的质量和可用性；两阶段训练流程可能比端到端训练更复杂；对于非常规或高度组合的指令，泛化能力仍有提升空间。</p>
<p>这项工作对后续研究的启示包括：为处理多模态学习中的不平衡问题提供了可推广的分解范式；展示了如何更有效地将不同领域（视觉、语言、动作）的基础模型先验整合到统一的策略框架中；其两阶段、解耦的设计思想可能有助于提升模型的可解释性和可控性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在微调时易发生灾难性遗忘、泛化能力下降的问题，指出其根源在于VLA数据集中语言多样性远低于视觉与动作的模态不平衡。为此，作者提出BayesVLA，通过贝叶斯因子分解将策略分解为支持“看到即行动”的视觉-动作先验和实现“提示即指定”的语言条件似然，从而保留泛化能力并促进指令跟随。信息论分析验证了该方法能有效缓解捷径学习。大量实验表明，该方法在未见指令、物体及环境上相比现有方法具有更优的泛化性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.11218" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>