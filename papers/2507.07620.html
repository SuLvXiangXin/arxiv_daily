<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ViLU: Learning Vision-Language Uncertainties for Failure Prediction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>ViLU: Learning Vision-Language Uncertainties for Failure Prediction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.07620" target="_blank" rel="noreferrer">2507.07620</a></span>
        <span>作者: Lafon, Marc, Karmim, Yannis, Silva-Rodríguez, Julio, Couairon, Paul, Rambour, Clément, Fournier-Sniehotta, Raphaël, Ayed, Ismail Ben, Dolz, Jose, Thome, Nicolas</span>
        <span>日期: 2025/07/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉语言模型（VLM）在大规模图像-文本数据上预训练，具备强大的零样本图像分类能力。不确定性量化（UQ）是评估模型预测置信度的关键，对于安全关键领域和失败预测等应用至关重要。目前，VLM不确定性量化的主流方法是最大概念匹配（MCM），即直接使用模型预测的最高概率作为置信度。然而，MCM存在根本性缺陷：其设计导致它会对错误的预测赋予高置信度，并且在处理细粒度概念时效果不佳。此外，基于学习视觉不确定性（LVU）的方法通过预测分类器的损失来估计不确定性，但这些方法通常只关注视觉模态，未能建模下游概念之间的关系，因此在应用于VLM时性能受限。</p>
<p>本文针对现有方法无法有效捕捉VLM在细粒度概念上的不确定性，以及在失败预测中表现不佳的具体痛点，提出了一个新的视角：不确定性估计应同时依赖于视觉输入和定义下游任务的所有相关概念集合。通过精细建模图像与目标概念之间的交互，可以更好地预测VLM的失败。本文的核心思路是提出一个名为ViLU的后处理框架，它利用所有任务相关的文本表示来“上下文化”不确定性估计，通过交叉注意力整合多模态信息，并训练一个二分类器来直接预测失败，而非预测损失。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViLU是一个用于VLM失败预测的学习型视觉-语言不确定性量化框架。其整体流程（Pipeline）是后处理的，即不修改预训练VLM的内部参数，仅利用其输出的视觉嵌入和文本嵌入。输入为视觉嵌入 𝒛_v 和一组K个候选概念的文本嵌入集合 {𝒛_t_j}，输出为一个标量不确定性分数（失败概率）ŷ ∈ [0,1]。</p>
<p><img src="https://arxiv.org/html/2507.07620v4/figures/figure_model_v6.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ViLU方法整体框架。给定输入图像和一组候选文本概念，首先通过冻结的VLM编码器获取视觉嵌入和文本嵌入。核心模块包括：1）视觉-文本交叉注意力模块，生成图像条件化的文本表示 𝒛_t^α；2）ViLU嵌入构建器，拼接视觉嵌入 𝒛_v、预测文本嵌入 𝒛_t̂ 和 𝒛_t^α；3）多层感知机（MLP）分类头，输出失败预测概率。该方法支持图像-标签和图像-描述两种任务。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>视觉-文本交叉注意力模块</strong>：该模块以视觉嵌入 𝒛_v 为查询（Query），以所有候选概念的文本嵌入集合为键（Key）和值（Value），通过缩放点积注意力计算注意力权重 α，并生成一个图像条件化的加权文本表示 𝒛_t^α。这使得模型能够根据图像内容动态地关注和融合相关概念的信息，从而捕捉概念间的歧义。</li>
<li><strong>ViLU嵌入构建</strong>：为了有效解决视觉-语言歧义，模型需要捕获区分正确与错误预测的关键信息。ViLU构建了一个丰富的多模态不确定性嵌入：𝒛_ViLU = (𝒛_v, 𝒛_t̂, 𝒛_t^α)。其中，𝒛_t̂ 是VLM预测出的概念（概率最高者）的文本嵌入。仅使用 (𝒛_v, 𝒛_t̂) 只能近似MCM，而加入 𝒛_t^α 使得模型能够考虑模糊的替代概念，从而更可靠地检测错误。</li>
<li><strong>非线性变换与分类头</strong>：将构建的 𝒛_ViLU 嵌入输入一个多层感知机（MLP）g_θ_MLP，以学习复杂的跨模态关系并进行非线性变换，最后通过Sigmoid函数 σ 输出失败预测分数 ŷ_i。论文指出，当g_θ_MLP采用双线性形式时，ViLU可退化为未归一化的MCM分数，因此ViLU是MCM的一致性推广。</li>
</ol>
<p>与现有方法相比，ViLU的创新点具体体现在：</p>
<ul>
<li><strong>新颖的多模态不确定性表示</strong>：首次明确整合了视觉嵌入、预测文本嵌入以及通过交叉注意力得到的图像条件化文本表示，从而能够捕捉图像与候选概念谱系之间的细粒度歧义。</li>
<li><strong>基于交叉注意力的上下文建模</strong>：通过注意力机制动态地根据图像内容加权聚合所有候选文本概念的信息，而非静态地处理概念。</li>
<li><strong>损失不可知的二分类器训练</strong>：与传统的基于损失预测的UQ方法不同，ViLU将不确定性预测器直接训练为一个二分类器，以区分正确与错误的预测。其训练目标是最小化加权二元交叉熵损失（wBCE），权重 w 根据每个小批量中VLM的分类准确率动态调整，以缓解类别不平衡问题。这使得ViLU不依赖于特定VLM的预训练损失函数（如对比损失），成为真正的损失不可知方法，特别适合后处理的黑盒设置。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验使用了CLIP ViT-B/32作为默认的VLM骨干网络。评估涵盖两种设置：1) <strong>图像-标签数据集</strong>：包括CIFAR-10、CIFAR-100、ImageNet-1k等13个标准分类数据集，使用固定的类别名称作为文本查询。2) <strong>图像-描述数据集</strong>：包括CC3M、CC12M和LAION-400M，任务是在批次内为图像匹配最相似的文本描述，更具开放词汇和批次依赖性。评估指标为错误分类检测中常用的<strong>AUC</strong>（曲线下面积，越高越好）和<strong>FPR95</strong>（真阳性率为95%时的假阳性率，越低越好）。</p>
<p><strong>对比基线</strong>：包括：1) 输出分布度量法：MCM、温度缩放后的MCM（TS+MCM）、熵（Entropy）、Doctor；2) 数据驱动预测器：Rel-U、学习视觉不确定性的方法（LVU，如ConfidNet）；3) 最近的VLM专用后处理概率建模方法：BayesVLM。</p>
<p><strong>关键实验结果</strong>：<br>在13个图像-标签数据集上的综合评估结果如表1所示。ViLU在所有数据集和两项指标上均排名第一。与最强的基线相比，ViLU在平均FPR95上显著优于MCM（降低40.7%）、BayesVLM（降低35.2%）和LVU（降低27.5%）。值得注意的是，当VLM的零样本准确率较低时（如EuroSAT数据集准确率35.8%），MCM和BayesVLM的性能会大幅下降（AUC仅64.1%和74.3%），而ViLU依然保持高效（AUC高达98.8%），展现了其在不同准确率范围内的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2507.07620v4/x1.png" alt="表1"></p>
<blockquote>
<p><strong>表1</strong>：在图像-标签数据集上的错误分类检测结果。ViLU（最后一行）在AUC和FPR95指标上全面超越所有基线方法，尤其在FPR95上提升显著。</p>
</blockquote>
<p>在更具挑战性的图像-描述数据集上，结果如表2所示。LVU方法在此设置下甚至无法超越简单的MCM基线，凸显了在开放词汇场景中考虑目标概念的重要性。ViLU再次取得最佳性能，在CC3M、CC12M和LAION-400M数据集上，其FPR95分别比BayesVLM降低了21.5%、28.1%和5.2%。</p>
<p><img src="https://arxiv.org/html/2507.07620v4/x2.png" alt="表2"></p>
<blockquote>
<p><strong>表2</strong>：在大型图像-描述数据集上的错误分类检测结果。ViLU在开放词汇、批次依赖的任务中同样表现最优。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文通过消融实验验证了各组件贡献。关键发现包括：1) <strong>架构消融</strong>：完整的ViLU嵌入（𝒛_v, 𝒛_t̂, 𝒛_t^α）性能最佳；移除交叉注意力模块（即不使用𝒛_t^α）会导致性能显著下降，尤其是在细粒度数据集上，这证明了上下文化文本表示的重要性。2) <strong>训练目标消融</strong>：直接进行失败分类（二元交叉熵）优于预测相似性分数或对比损失，证实了二分类训练目标与失败预测任务的自然对齐。3) <strong>MLP深度分析</strong>：增加MLP的层数可以提升性能，表明学习复杂的跨模态模式对于失败预测是必要的。</p>
<p><img src="https://arxiv.org/html/2507.07620v4/figures/quali_failure1.png" alt="定性结果1"></p>
<blockquote>
<p><strong>图13</strong>：定性示例：ViLU成功为被错误分类为“船”的“邮箱”图像分配了低置信度（失败概率高），而MCM和LVU则给出了高置信度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.07620v4/figures/quali_failure2.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图14</strong>：定性示例：对于被正确分类的“美国爱斯基摩犬”，ViLU给出了高置信度（失败概率低），而其他方法则因与“西伯利亚哈士奇”的相似性而表现出不确定性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了新颖的多模态不确定性表示</strong>：通过视觉-文本交叉注意力机制，构建了一个融合视觉、预测文本及上下文文本信息的嵌入，有效捕捉了VLM预测中的细粒度歧义。</li>
<li><strong>设计了损失不可知的二分类训练方法</strong>：将不确定性量化直接定义为失败预测的二分类任务，并使用动态加权的交叉熵损失进行训练，使其不依赖于特定VLM的损失函数，非常适合后处理的黑盒应用场景。</li>
<li><strong>实现了显著的性能提升</strong>：在广泛的图像-标签和图像-描述数据集上，ViLU在失败预测的AUC和FPR95指标上均大幅超越了包括MCM、LVU和最新VLM专用方法在内的所有基线。</li>
</ol>
<p>论文自身提到的局限性主要在于其是一种后处理方法，其性能依赖于预训练VLM产生的嵌入质量。此外，虽然框架支持可变数量的概念，但在处理极大规模开放词汇任务时，计算所有候选概念的交叉注意力可能带来计算开销。</p>
<p>本文的启示在于：对于VLM的不确定性量化，<strong>显式地建模图像与整个相关概念集合的交互</strong>，并<strong>将不确定性估计直接与失败预测目标对齐</strong>，是比单纯依赖输出概率或预测损失更有效的途径。ViLU的后处理特性使其能够灵活部署于各种预训练VLM之上，为构建可靠、可解释的VLM应用提供了实用工具。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型(VLMs)中可靠不确定性量化与失败预测的挑战，提出ViLU框架。核心方法是构建融合视觉嵌入、预测文本嵌入及图像条件文本表示的不确定性感知多模态表征，并训练一个损失无关的二元分类器来区分预测正确与否。实验在ImageNet-1k、CC12M和LAION-400M等数据集上验证了该方法相较于现有技术的显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.07620" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>