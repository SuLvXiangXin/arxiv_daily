<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.04452" target="_blank" rel="noreferrer">2507.04452</a></span>
        <span>作者: Wu, Mingdong, Wu, Lehong, Wu, Yizhuo, Huang, Weiyao, Fan, Hongwei, Hu, Zheyuan, Geng, Haoran, Li, Jinzhou, Ying, Jiahe, Yang, Long, Chen, Yuanpei, Dong, Hao</span>
        <span>日期: 2025/07/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，在真实世界中应用机器人强化学习（RL）面临着样本效率低、探索缓慢以及对人工干预依赖性强等关键挑战。近期的一些方法通过整合人工收集的演示数据来引导评论家（critic）或利用人工干预来指导探索，显著提升了学习效率。然而，这些方法依赖于劳动密集型的数据收集和手动干预，成本高昂且难以扩展。另一方面，仿真器为大规模探索和数据收集提供了安全高效的环境，但视觉上的仿真到真实（sim-to-real）差距常常成为限制因素，而利用真实到仿真（real-to-sim）技术可以缓解这一差距。</p>
<p>本文针对如何减少人工成本、同时提升真实世界RL的样本效率这一痛点，提出了结合真实世界RL和真实-仿真-真实（real-to-sim-to-real）方法优势的新视角。其核心思路是：首先在数字孪生仿真环境中预训练一个视觉运动策略，然后利用该策略在仿真和真实世界中收集的演示数据来引导评论家训练，并利用该策略的动作提议来改善在线探索。</p>
<h2 id="方法详解">方法详解</h2>
<p>SimLauncher的整体框架是一个结合了仿真预训练和真实世界在线RL的混合框架。其输入是任务场景的RGB图像和机器人本体感知数据，输出是机器人的动作。流程分为两个主要阶段：首先在构建的数字孪生环境中预训练一个视觉运动策略并收集仿真演示数据；然后将预训练策略部署到真实世界收集少量真实演示，并启动在线RL。在线RL阶段，算法同时从仿真演示缓冲区、真实演示缓冲区和在线交互回放缓冲区中采样数据来训练评论家和演员（actor）网络。</p>
<p><img src="https://arxiv.org/html/2507.04452v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SimLauncher 方法概述。在仿真阶段，收集仿真演示并训练视觉策略。随后，在真实世界滚动执行预训练策略以收集真实演示。仿真和真实演示用于真实世界RL中的评论家引导。预训练策略还为演员提供动作和引导建议。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>数字孪生构建与仿真策略预训练</strong>：对于抓放和插入任务，使用3D高斯泼溅（3DGS）重建机器人和任务相关物体的几何与纹理，并利用Isaac Gym进行物理仿真。对于灵巧手抓取任务，使用Mujoco。预训练采用蒸馏方法：先利用特权信息（本体感知和物体状态）训练一个状态基RL策略，生成成功轨迹作为视觉演示；再通过行为克隆（BC）训练一个以RGB图像和本体感知为输入的视觉运动策略。为缓解sim-to-real差距，采取了物理参数校准、相机位姿随机化、数据增强（随机裁剪、颜色抖动）以及使用SAM2进行背景掩码处理等策略。</li>
<li><strong>演示数据收集策略</strong>：<ul>
<li><strong>仿真演示（𝒟_sim）</strong>：利用仿真环境安全高效地收集大量成功轨迹，提供了更广的初始条件和中间状态覆盖。</li>
<li><strong>真实演示（𝒟_real）</strong>：将BC策略部署到真实世界收集少量成功轨迹，用于正则化训练，防止评论家因仿真与真实数据分布差异而低估真实转移的价值。</li>
</ul>
</li>
<li><strong>真实世界在线RL与引导</strong>：基础算法采用RLPD，一种基于先验数据引导的混合RL算法。SimLauncher的关键设计在于对RLPD的扩展：<ul>
<li><strong>数据混合采样</strong>：每个训练批次由50%回放缓冲区数据、25%仿真演示数据和25%真实演示数据构成。</li>
<li><strong>动作与引导提议</strong>：借鉴IBRL，在在线交互时，预训练的BC策略会提出一个备选动作。系统通过基于Q值的玻尔兹曼分布，在RL策略动作和BC策略动作之间进行选择，以加速探索。同时，价值目标的计算也改为考虑这两个动作中的最大Q值（公式4），这改变了标准的引导目标。</li>
</ul>
</li>
</ol>
<p>与现有方法相比，SimLauncher的创新点具体体现在：1) 首次提出利用仿真预训练策略来生成演示数据，以引导真实世界RL的评论家训练，从而大幅扩展了引导数据的覆盖范围和规模；2) 在视觉基设置下（而非状态基）整合了仿真数字孪生和真实世界RL，增强了鲁棒性和适应性；3) 同时利用预训练策略进行探索引导和演示生成，形成协同效应。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了三个具有挑战性的真实世界机器人操作任务作为基准：多阶段任务（Pick and Place）、接触丰富的精确操作任务（Pick and Insert）以及高维动作空间的灵巧手抓取任务（Dex Grasp）。实验平台包括Franka机械臂（配备平行夹爪或Leap Hand）。</p>
<p>对比的基线方法是当前最先进的混合RL方法：<strong>RLPD</strong>（仅使用人工演示进行引导）和<strong>IBRL</strong>（额外使用基于人工演示的BC策略进行动作提议）。为确保公平比较，SimLauncher使用的真实演示数量设置为20条，与基线使用的人工演示数量相同。</p>
<p><img src="https://arxiv.org/html/2507.04452v1/x2.png" alt="任务图示"></p>
<blockquote>
<p><strong>图3</strong>：任务示意图、初始化范围及常见失败模式。展示了三个实验任务的具体设置和挑战。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.04452v1/x3.png" alt="对比结果"></p>
<blockquote>
<p><strong>图4</strong>：与基线的对比。SimLauncher在三个任务上的学习曲线（成功率和回合长度）均显著优于依赖人工演示的IBRL和RLPD方法。</p>
</blockquote>
<p>关键实验结果：如图4和表I所示，SimLauncher在所有任务上都一致地超越了所有基线方法。当SimLauncher收敛到接近完美的性能（100%成功率）时，最强的基线IBRL在Pick and Place、Pick and Insert和Dex Grasp任务上的成功率分别落后41.7%、46.7%和18.3%。这表明SimLauncher相比依赖人工数据的传统混合RL方法，显著提高了样本效率。特别是在阶段更复杂的Pick and Place和Pick and Insert任务上，优势更为明显。</p>
<p><img src="https://arxiv.org/html/2507.04452v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：关键设计选择的消融研究。移除仿真演示（w/o Sim Demo）、真实演示（w/o Real Demo）或动作提议模块（w/o AP）都会导致性能下降，验证了每个组件的必要性。</p>
</blockquote>
<p>消融实验（在Pick and Place任务上进行）总结了每个组件的贡献：</p>
<ol>
<li><strong>移除真实演示（Ours w/o Real Demo）</strong>：性能下降最显著，强调了真实演示对于正则化、防止评论家过拟合的重要性。</li>
<li><strong>移除动作提议（Ours w/o AP）</strong>：在训练初期表现挣扎，因为缺少了成功率达73.3%的预训练策略的引导，但后期仍能较快收敛。</li>
<li><strong>移除仿真演示（Ours w/o Sim Demo）</strong>：性能也低于完整方法，表明大规模的仿真演示数据对于提供广泛的状态覆盖和引导至关重要。</li>
</ol>
<p>此外，分析部分（表II）指出，尽管由于sim-to-real差距，基于20条仿真轨迹训练的BC策略（Sim-BC）性能低于基于20条人工轨迹训练的BC策略（Human-BC），但仿真允许可扩展的数据收集。当使用多达1000条仿真轨迹进行训练时，Sim-BC能在真实世界达到很高的成功率（86.7%），这证明了大规模仿真预训练对于策略迁移的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了一个新颖的框架</strong>：首次系统性地将仿真预训练策略与真实世界在线RL结合，利用仿真生成的大规模演示数据引导评论家，并利用策略进行探索，显著提升了真实世界RL的样本效率。</li>
<li><strong>验证了仿真数据引导的有效性</strong>：通过实验证明，即使存在sim-to-real差距，从仿真预训练策略中收集的演示数据也能有效地引导真实世界RL中的价值函数学习，特别是当与少量真实演示结合时。</li>
<li><strong>展示了视觉基方法的优势</strong>：与先前基于状态的方法不同，SimLauncher在纯视觉观测设置下工作，提高了对真实世界多样化场景的鲁棒性和泛化能力。</li>
</ol>
<p>论文自身提到的局限性在于，本研究是一个概念验证，将任务场景限制在物体和背景固定的环境中，并预训练了任务专用的策略。更复杂的动态场景和通用策略的预训练是未来的挑战。</p>
<p>对后续研究的启示包括：1) 证明了利用大规模仿真预训练来赋能真实世界RL的可行性，为减少对昂贵人工数据的依赖提供了新路径；2) 所提出的框架相对简单，可与其他先进的RL算法、仿真数据生成技术或sim-to-real迁移方法结合，具有可扩展性；3) 鼓励进一步探索如何构建更逼真的数字孪生，以及如何将方法推广到开放世界、多任务场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SimLauncher框架，旨在解决现实世界机器人强化学习中样本效率低、探索缓慢、依赖人工干预的问题。其关键技术是通过数字孪生仿真预训练视觉运动策略，并利用该策略在两方面辅助真实训练：1）使用大量模拟演示及预训练策略产生的真实轨迹进行价值函数引导；2）引入预训练策略的动作提议以改善探索。实验表明，在多阶段、接触密集及灵巧手操作任务上，SimLauncher相比现有方法显著提升了样本效率，并达到了接近完美的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.04452" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>