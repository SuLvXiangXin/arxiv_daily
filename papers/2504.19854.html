<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.19854" target="_blank" rel="noreferrer">2504.19854</a></span>
        <span>作者: Hung, Chia-Yu, Sun, Qi, Hong, Pengfei, Zadeh, Amir, Li, Chuan, Tan, U-Xuan, Majumder, Navonil, Poria, Soujanya</span>
        <span>日期: 2025/04/28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>现有的视觉-语言-动作模型在零样本场景中展现出有前景的任务执行和推理能力。然而，这些模型面临两大关键局限：一是视觉编码的局限性可能导致在物体抓取等任务中失败；二是模型通常规模庞大（参数常超过70亿），导致计算开销高昂，难以满足实时机器人环境对速度和效率的严苛要求。鉴于为特定任务微调VLA模型是常见做法，业界亟需一个更小、更高效的模型，使其能够在消费级GPU上进行微调。</p>
<p>本文针对现有VLA模型计算开销高、部署不实用的痛点，提出了构建小型高效通用模型的新视角。其核心思路是：采用先进的3B参数多模态模型Qwen-2.5-VL作为主干，利用其优越的视觉-语义理解能力，并结合高效的FAST+动作分词器，在大型机器人演示数据集上进行训练，旨在实现性能与效率的平衡。</p>
<h2 id="方法详解">方法详解</h2>
<p>NORA的整体框架基于一个预训练的视觉-语言模型，以自回归的方式预测编码了从时间t到t+N的未来动作的动作块。输入包括自然语言任务指令c和时刻t的视觉观察（n帧图像），二者拼接形成整体输入。模型输出是动作块的离散令牌序列，随后通过FAST+解码器解码为连续动作。</p>
<p><img src="https://arxiv.org/html/2504.19854v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：NORA的整体架构和推理流程。模型基于预训练的VLM主干，接收语言指令和视觉观察作为输入，自回归地预测代表未来动作块的离散令牌序列，最后通过FAST+解码器得到可执行的动作。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>VLM主干</strong>：采用Qwen-2.5-VL-3B作为主干。选择该模型是因为其3B的小参数量，以及在训练中使用原生图像分辨率的特点，这有助于增强模型对真实世界尺度和空间关系的感知，从而提升物体检测和定位等任务的性能。</li>
<li><strong>动作分词与解码</strong>：采用FAST+分词器将连续动作离散化。FAST+对每个时间步的动作维度应用离散余弦变换，以解耦关节动作分量，然后使用字节对编码将其压缩成更短、令牌效率更高的序列。这种表示减少了词汇量，加速了收敛。在推理时，模型将动作块大小设置为1（即预测下一个动作）。此外，模型向VLM分词器的词汇表中添加了FAST+引入的2048个额外令牌。</li>
</ol>
<p>与现有方法相比，NORA的创新点具体体现在：</p>
<ul>
<li><strong>模型小型化</strong>：仅3B参数，显著降低了计算开销和内存需求（推理时约8.3GB GPU内存）。</li>
<li><strong>利用先进视觉编码</strong>：继承了Qwen-2.5-VL原生高分辨率训练带来的优越空间感知和物体定位能力，无需像SpatialVLA那样引入额外的动作网格或空间嵌入模块。</li>
<li><strong>高效动作分词</strong>：采用FAST+分词器，更高效地编码动作序列。</li>
</ul>
<p>此外，论文还提出了一个变体<strong>NORA-Long</strong>，其架构与NORA完全相同，但动作块大小设置为5，旨在通过预测更长的动作视野来提升长时程任务的性能。</p>
<p>训练方面，NORA在Open X-Embodiment数据集上进行预训练，该数据集包含来自不同机器人的多样化任务轨迹。训练持续约三周，使用8张H100 GPU，进行了110万次梯度更新。</p>
<p><img src="https://arxiv.org/html/2504.19854v1/extracted/6394640/img/nora-logo.png" alt="训练损失曲线"></p>
<blockquote>
<p><strong>图2</strong>：(a) 训练损失曲线显示损失总体稳定下降，无显著尖峰。(b) 梯度范数曲线在整个训练过程中偶有尖峰，但未破坏损失的平滑进程。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了两个主要平台进行评估：1) 真实世界的WidowX机器人平台；2) LIBERO模拟基准（包含30个程序生成的解耦任务和10个长时程纠缠任务）。评估指标为任务成功率。</p>
<p>对比的基线方法包括：OpenVLA、SpatialVLA、TraceVLA和RT-1。</p>
<p>关键实验结果如下：<br>在9个真实世界机器人操作任务上，NORA取得了平均56.7%的成功率，显著优于OpenVLA（40%）、SpatialVLA（11.1%）和RT-1（4.4%）。具体而言，在OOD物体抓取任务（如“把胡萝卜放进锅里”）上，NORA成功率高达90%；在需要空间推理的任务上（如“把粉色玩具放到右边角落”），也表现出优势；在多物体抓取任务上表现优于基线，但成功率仍低于50%，有较大改进空间。</p>
<p><img src="https://arxiv.org/html/2504.19854v1/x2.png" alt="真实世界任务设置"></p>
<blockquote>
<p><strong>图3</strong>：真实世界机器人环境和任务设置。评估涵盖9个多样化任务，以检验模型的指令理解、空间推理和多任务运动规划能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.19854v1/x3.png" alt="真实世界任务结果对比"></p>
<blockquote>
<p><strong>图4</strong>：不同类别真实世界机器人任务的实验结果。NORA在OOD物体、空间推理和多物体任务上均显著优于基线模型。</p>
</blockquote>
<p>在LIBERO模拟基准上，经过微调的NORA-Long取得了最佳平均成功率87.9%，显著超越所有基线。实验结果表明，动作分块策略对长时程任务（LIBERO-Long）性能提升至关重要。使用动作分块微调的NORA变体在LIBERO-Long上的成功率达到63%，而未使用分块的版本仅为45%。</p>
<p><img src="https://arxiv.org/html/2504.19854v1/x4.png" alt="模拟基准结果"></p>
<blockquote>
<p><strong>图5</strong>：LIBERO模拟基准上的实验结果。NORA-Long微调后取得了最佳整体性能（87.9%平均成功率），凸显了动作分块对长时程任务的有效性。</p>
</blockquote>
<p>消融实验与分析：</p>
<ol>
<li><p><strong>环境干扰的影响</strong>：在简单任务中引入干扰物后，NORA和OpenVLA的性能均出现显著下降，表明当前策略对环境干扰较为脆弱。<br><img src="https://arxiv.org/html/2504.19854v1/x5.png" alt="干扰任务对比"></p>
<blockquote>
<p><strong>图6</strong>：有干扰和无干扰环境的任务对比示例。引入额外物体作为干扰。<br><img src="https://arxiv.org/html/2504.19854v1/x6.png" alt="干扰下性能下降"><br><strong>图7</strong>：在存在环境干扰的情况下，NORA和OpenVLA的策略性能均大幅下降。</p>
</blockquote>
</li>
<li><p><strong>动作分块在真实与模拟环境中的差异</strong>：在真实WidowX机器人（控制频率较低）上，直接执行NORA-Long预测的5个连续动作会导致机器人碰撞。若只执行每个动作块的第一个动作，则在单物体任务上表现良好（80%），但在多物体拾放任务上完全失败（0%），且对较小物体的抓取点估计不准。这表明在较低控制频率下，动作分块策略的收益有限且可能带来问题。相反，在较高控制频率（20Hz）的LIBERO模拟环境中，动作分块带来了显著的性能提升。<br><img src="https://arxiv.org/html/2504.19854v1/x7.png" alt="分块动作问题"></p>
<blockquote>
<p><strong>图8</strong>：在真实世界WidowX机器人上评估动作分块。直接执行预测的多个连续动作易导致碰撞，而只执行第一个动作则难以完成多物体任务，揭示了动作分块策略在低控制频率环境下的局限性。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li>提出了NORA，一个基于Qwen-2.5-VL-3B主干、仅3B参数的高效VLA模型，通过结合FAST+分词器，在保持强劲任务性能的同时大幅降低了计算开销。</li>
<li>开源了完整的NORA框架（包括模型检查点、训练策略和评估协议），促进了可复现性和后续研究。</li>
<li>通过系统实验验证了动作分块策略的有效性及其对控制频率的依赖性，为长时程策略规划提供了见解。</li>
</ol>
<p>论文自身提到的局限性包括：在处理多物体任务时性能仍有较大提升空间；策略在存在环境干扰时表现脆弱；在低控制频率的机器人平台上，动作分块策略的优势难以发挥，甚至可能带来问题。</p>
<p>对后续研究的启示：</p>
<ul>
<li>继续探索更高效的小型化VLA架构，推动其在资源受限和实时场景中的应用。</li>
<li>需要改进模型对复杂任务（如多物体操作）的规划和执行能力，以及应对环境变化的鲁棒性。</li>
<li>动作分块等策略的有效性与机器人控制频率紧密相关，未来研究需考虑算法与硬件平台的协同设计。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型参数量大、计算开销高、难以在消费级GPU微调的问题，提出小型通用模型NORA。其以Qwen-2.5-VL-3B为骨干增强视觉语义理解，采用FAST+分词器高效生成动作序列，并在97万真实机器人演示数据上训练。实验表明，NORA仅3B参数，在显著降低计算开销的同时，任务性能优于现有大规模模型，更适合实时机器人部署。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.19854" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>