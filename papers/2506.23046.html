<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computation and Language (cs.CL)</span>
      <h1>SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.23046" target="_blank" rel="noreferrer">2506.23046</a></span>
        <span>作者: Fan, Xianzhe, Zhou, Xuhui, Jin, Chuanyang, Nottingham, Kolby, Zhu, Hao, Sap, Maarten</span>
        <span>日期: 2025/06/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前评估机器心智理论（Theory of Mind, ToM）能力的主流方法大多基于静态、文本驱动的场景，例如经典的Sally-Anne测试及其变体，或是在简单2D动画或受限对话中评估信念和意图。这些方法存在关键局限性：1) 缺乏对具身智能体在与环境和他人交互时ToM能力的评估，尤其是在复杂任务和开放世界场景中；2) 未能充分探索多智能体之间多样化的社交关系（如协作与阻碍）及其交互；3) 未能基于不同视角（第一人称与第三人称）和多模态信息进行综合评估。本文针对现有ToM评测基准在具身性、社交复杂性和评估视角单一性上的不足，提出了SoMi-ToM基准。其核心思路是：基于一个名为SoMi的、在Minecraft中运行的具身多智能体社交交互环境，生成丰富的多模态交互数据，并构建一个从第一人称（实时状态推断）和第三人称（全局目标与行为推断）双视角评估模型ToM能力的基准。</p>
<h2 id="方法详解">方法详解</h2>
<p>SoMi-ToM的整体框架分为两个阶段：数据生成与多视角评估。首先，在SoMi交互环境中，三个由大型视觉语言模型（LVLM）控制的智能体在Minecraft中执行包含特定合成目标（如制作船只、箱子等）的任务，其间存在协作或单向阻碍的社交关系。环境会记录交互日志、第一人称游戏截图和第三人称视角视频。其次，基于这些数据构建包含1225道多选题的评测集，并从两个视角进行评估。</p>
<p><img src="https://arxiv.org/html/2506.23046v3/figures/hf-logo.png" alt="SoMi-ToM框架图"></p>
<blockquote>
<p><strong>图1</strong>：SoMi-ToM基准构建于Minecraft中具身AI智能体交互数据之上。共有35个任务，每个任务分配一个合成目标，智能体需要通过收集材料和制作工具来实现。同时，智能体之间存在不同的社交关系：协作或阻碍。评估是多视角的：(1) 第一人称（1050题）：在实时交互中推断状态。(2) 第三人称（175题）：基于完整视频推断目标与行为。示例中绿色选项为正确答案。</p>
</blockquote>
<p>核心模块是两种不同视角的评估：</p>
<ol>
<li><strong>第一人称视角ToM评估（状态推断）</strong>：要求模型扮演其中一个智能体，基于其自身视角的多模态输入（包括对话历史、系统反馈、第一人称游戏截图）来回答状态推断问题。这评估模型是否能理解并推断自身（自ToM）或其他智能体（他ToM）对物理状态（如持有的资源或工具）的信念。具体而言，模型需要：长期跟踪并记忆自身及他人资源的收集与使用；基于对话内容和行为观察推断当前持有物品并计算其数量；整合历史信息与当前状态形成连贯的信念模型。输入包括文本（问题、记忆、合成目标、具体合成规则和“特殊说明”）和图像（每隔4秒截取的第一人称游戏截图，共363张）。</li>
<li><strong>第三人称视角ToM评估（目标与行为推断）</strong>：要求模型作为外部观察者，基于完整的第三人称视角视频（附带字幕，包含对话和所有智能体的系统反馈）来回答两类问题：<strong>目标推断</strong>（推断被观察智能体的最终合成目标）和<strong>行为推断</strong>（推断智能体在视频中的具体行为或动作）。视频模拟了人类观察复杂场景时的选择性注意，焦点集中在执行动作或说话的关键智能体上。</li>
</ol>
<p>与现有方法相比，SoMi-ToM的创新点具体体现在：1) <strong>多视角评估</strong>：同时涵盖基于主观即时体验的第一人称推理和基于客观全局观察的第三人称推理；2) <strong>复杂社交交互</strong>：任务中引入了协作与（单向）阻碍的真实社交动态；3) <strong>具身性与动态性</strong>：评估基于智能体在开放世界环境中的连续、多模态交互过程，而非静态快照。</p>
<p><img src="https://arxiv.org/html/2506.23046v3/x1.png" alt="第一人称视角输入示例"></p>
<blockquote>
<p><strong>图2</strong>：第一人称评估的输入示例。展示了智能体“Jack”在某个时刻的第一人称游戏画面。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.23046v3/figures/3pp.png" alt="第三人称视角视频示例"></p>
<blockquote>
<p><strong>图7</strong>：第三人称评估的输入示例。展示了附带字幕的第三人称视角视频帧，字幕包含了对话和系统反馈。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与平台</strong>：SoMi-ToM基准包含35个任务的数据集，具体有：35个第三人称视角视频（平均时长263.14秒）、363张第一人称视角图像（去重后），以及1225道专家标注的三选一多选题。实验评估了人类参与者和多个先进的LVLM。</p>
<p><strong>Baseline方法</strong>：对比了人类基线以及最新的LVLM，包括GPT-4o、LLaVA 1.6、Gemini 1.5 Pro、Gemini 2.0 Flash、InternVL2.5 78B、Qwen2.5-VL、VideoLLaMA 3和LLaVA-Video。评估在两种提示设置下进行：标准提示（vanilla）和思维链提示（CoT）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>人类表现优异</strong>：在第一人称评估中，人类平均准确率达90.0%（自ToM 91.9%，他ToM 89.0%）；在第三人称评估中，人类平均准确率达93.3%。</li>
<li><strong>LVLM表现显著落后于人类</strong>：在第一人称评估中，所有LVLM表现不佳，最佳模型（GPT-4o with CoT）准确率为59.5%，与人类存在40.1%的平均准确率差距。大多数模型在使用CoT后准确率有所提升，其中GPT-4o提升最大（+23.9%）。自ToM推理对他ToM推理对模型来说更容易。</li>
<li><strong>第三人称评估中LVLM表现相对较好，但仍有差距</strong>：在不使用CoT时，最佳模型（Gemini 2.0 Flash和Qwen2.5-VL）整体准确率为78.3%，与人类存在26.4%的平均准确率差距。目标推断任务对LVLM最容易（Qwen2.5-VL达到100%准确率），行为推断更具挑战性。值得注意的是，除少数模型外，大多数模型在使用CoT后准确率反而下降。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.23046v3/x2.png" alt="第一人称评估结果"></p>
<blockquote>
<p><strong>图8</strong>：第一人称视角评估（状态推断）中人类与LVLM的性能对比。表格显示了自ToM、他ToM推理及加权平均准确率。LVLM在CoT提示下普遍有提升，但与人类（90.0%）差距显著。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.23046v3/x3.png" alt="第三人称评估结果"></p>
<blockquote>
<p><strong>图9</strong>：第三人称视角评估（目标与行为推断）中人类与LVLM的性能对比。表格显示了目标推断、行为推断及整体准确率。LVLM在目标推断上表现较好，但行为推断和整体准确率远低于人类（93.3%）。</p>
</blockquote>
<p><strong>消融实验与分析</strong>：论文通过定性分析总结了LVLM表现不佳的主要原因，包括：忽略或不准确地跟踪资源消耗；对系统反馈依赖不足；被初始意图而非实际行为误导；过度概括或不恰当关联；未能识别层次化目标结构；实体识别混淆；细节错误。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 开发了SoMi，一个在Minecraft中的具身多智能体社交交互环境。2) 提出了SoMi-ToM，一个新颖的用于评估Minecraft中复杂多智能体社交交互下多视角ToM能力的基准。3) 对先进的LVLM进行了系统评估，并通过人类实验验证了数据集，提供了人类基线性能。</p>
<p><strong>局限性</strong>：论文自身提到的局限性包括：任务类型和社交关系（目前主要是单向阻碍）还可以进一步扩展；第一人称评估的图像输入是离散截图而非连续视频流；评估目前仅限于特定的合成任务环境（Minecraft）。</p>
<p><strong>对后续研究的启示</strong>：SoMi-ToM揭示了当前LVLM在复杂、具身、动态的社交交互中进行心智理论推理的能力仍远逊于人类。这指明了未来研究需要着力改进的方向：提升模型在多模态信息整合、长期状态跟踪、层次化目标理解以及复杂社交意图推理方面的能力。该基准为社区提供了一个评估和推动具身社交AI发展的有力工具。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有心理理论评估基准局限于静态文本、与真实动态社交互动存在差距的问题，提出了SoMi-ToM基准。该基准基于SoMi交互环境的多模态数据，构建了包含协作与阻碍关系的任务，并设计了**多视角评估框架**：**第一人称评估**要求模型根据实时多模态输入推断状态；**第三人称评估**要求模型根据完整视频记录推断目标与行为。实验在包含1225个专家标注问题的数据集上进行，结果显示，当前主流大视觉语言模型表现显著差于人类，在第一人称和第三人称评估中平均准确率分别低40.1%和26.4%，表明其在具身复杂社交互动中的心理理论能力亟待提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.23046" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>