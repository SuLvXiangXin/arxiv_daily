<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Feel the Force: Contact-Driven Learning from Humans - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Feel the Force: Contact-Driven Learning from Humans</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.01944" target="_blank" rel="noreferrer">2506.01944</a></span>
        <span>作者: Lerrel Pinto Team</span>
        <span>日期: 2025-06-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操控中的精细力控制是一个核心挑战。目前主流方法依赖于从机器人自身收集的数据或仿真数据来学习策略，但这些方法难以泛化到多样化的真实世界交互中。直接从人类演示中学习提供了可扩展的解决方案，允许演示者以其自然的身体形态在日常环境中执行技能。然而，仅凭视觉演示缺乏推断精确接触力所需的信息。现有方法若尝试整合力反馈，通常严重依赖难以扩展的遥操作，或需要昂贵且复杂的触觉反馈设备。此外，即使能获取触觉数据，许多方法只是被动地将观测到的触觉信息输入策略，这存在两个主要问题：一是由于人与机器人间的形态差异，可执行的力分布不同，策略难以泛化；二是需要更多数据来预测更精确的动作，降低了学习效率。</p>
<p>本文针对“如何高效地从人类触觉体验中赋予机器人鲁棒的、力感知的控制能力”这一具体痛点，提出了一个新视角：不再被动使用触觉作为观测，而是主动预测所需的接触力，并通过一个外环控制器在推理时动态调整夹持器以复现该力。本文的核心思路是：使用触觉手套测量人类演示中的接触力，训练一个闭环策略来预测手部轨迹和期望的接触力，然后在机器人执行时，通过一个PD控制器调制夹持器闭合度以跟踪预测的力，从而实现零样本的、无需机器人训练数据的力敏感操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>FTF框架的整体流程包含三个阶段：1）使用触觉手套和视觉系统采集人类演示数据；2）将数据转换为与形态无关的关键点表示并训练策略；3）在机器人上部署策略，并使用推理时的PD力控制器执行。</p>
<p><img src="https://arxiv.org/html/2506.01944v1/extracted/6504670/ftf_method.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：FTF方法整体框架。左侧为数据采集：人类佩戴触觉手套执行任务，两个校准的摄像头记录视觉观测。右侧为策略学习与部署：从视频中提取3D手部和物体关键点，与力、夹持器状态一同输入Transformer策略；策略预测未来的机器人关键点、夹持器状态和力；预测的机器人姿态和力被用于控制机器人，其中力通过一个PD控制器进行跟踪。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>数据采集硬件</strong>：使用定制的触觉手套（基于AnySkin设计）采集人类演示中的力数据。手套在拇指下方（掌心侧）装有基于磁力计的3D打印传感器，透明设计不遮挡手部视觉外观。机器人端，在Franka夹持器的一个指尖上安装相同的AnySkin传感器，确保传感对应关系。<br><img src="https://arxiv.org/html/2506.01944v1/extracted/6504670/ftf_glove.png" alt="硬件设计"><blockquote>
<p><strong>图3</strong>：硬件设计。(a) 人类数据收集者佩戴的AnySkin增强手套。(b) 左：人类可穿戴设备；中：安装有AnySkin传感器（一个指尖）的Franka Panda夹持器，另一指尖装有普通硅胶帽。</p>
</blockquote>
</li>
<li><strong>形态无关的场景表示</strong>：<ul>
<li><strong>人-机器人形态转换</strong>：使用Mediapipe从双视角视频中提取2D手部关键点，并通过三角测量得到3D关键点。机器人末端执行器的位置计算为拇指和食指指尖连线的中点，方向通过计算初始帧与当前帧手部关键点之间的刚性变换得到。最终，机器人姿态被转换为一组预定义的机器人关键点。</li>
<li><strong>物体关键点表示</strong>：环境通过稀疏的人类标注进行关键点表示。用户在单个演示帧上标注物体上的语义关键点，使用DIFT模型将标注传播到所有演示的首帧，再使用Co-Tracker在每条轨迹中跟踪这些点，最后通过三角测量得到3D物体关键点。</li>
</ul>
</li>
<li><strong>策略学习</strong>：采用Transformer策略架构。输入包括机器人关键点轨迹、物体关键点轨迹、二值化的夹持器状态和连续的力值。这些信息被编码为token输入Transformer，策略预测未来每个机器人关键点的轨迹、未来的夹持器状态和未来的夹持器力预测。使用动作分块和指数时间平均来确保预测平滑，训练损失为机器人关键点轨迹的均方误差。</li>
<li><strong>推理与力控制</strong>（核心创新）：<ul>
<li><strong>动作解析</strong>：策略预测的动作包含末端执行器姿态、夹持器状态和期望的力。</li>
<li><strong>PD力控制器</strong>：当策略预测需要闭合夹持器并施加一个力时，调用一个外环PD控制器。该控制器根据当前测量力与预测力的误差，实时调整夹持器的目标闭合度：<code>Δg_t^τ = k * (F^_t - F_t^τ)</code>，并迭代执行直到测量力收敛到预测力附近（误差小于ε）。这使得机器人能够主动地将其执行器的力输出调节到人类演示所指示的期望水平，从而弥合了形态差异。</li>
</ul>
</li>
</ol>
<p>与现有方法相比，FTF的创新点在于：1) <strong>主动力预测与跟踪</strong>：不同于将触觉作为被动观测，FTF的策略显式预测期望的力，并通过独立的控制器实现，使策略专注于高层运动规划，力跟踪由鲁棒的低层控制器处理；2) <strong>解耦学习与执行</strong>：利用人类自然演示进行训练，无需与机器人动作空间对齐或复杂的遥操作，实现了高效、可扩展的数据收集。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实世界桌面操作环境中，使用Franka Panda机器人进行评估。两个Intel RealSense D435摄像头提供第三人称RGB图像。评估了5个力敏感操作任务：放置鸡蛋、揭开锅盖、揭开杯盖、堆叠杯子、揭开药瓶盖。</p>
<p><strong>基线方法</strong>：</p>
<ul>
<li><strong>基于人类数据的基线</strong>：<code>BC (Vision)</code>：仅使用视觉观测的行为克隆；<code>BC (Vision + Force)</code>：将人类力数据作为额外观测输入的行为克隆（被动使用触觉）。</li>
<li><strong>基于机器人遥操作数据的基线</strong>：<code>BC (Robot Teleop)</code>：使用机器人遥操作数据训练的行为克隆；<code>IL (Robot Teleop)</code>：使用机器人遥操作数据训练的模仿学习方法。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.01944v1/extracted/6504670/allTasks.png" alt="所有任务"></p>
<blockquote>
<p><strong>图5</strong>：评估的五个力敏感操作任务示意图：放置鸡蛋、揭开锅盖、揭开杯盖、堆叠杯子、揭开药瓶盖。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.01944v1/extracted/6504670/human_baselines.png" alt="人类基线结果"></p>
<blockquote>
<p><strong>图8</strong>：与基于人类数据基线的对比结果。FTF在5个任务上的平均成功率为**77%**，显著高于仅使用视觉的BC（20%）和被动使用视觉+力的BC（47%）。这表明主动预测和再现力比被动使用多模态力输入更有效。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.01944v1/extracted/6504670/robot_baselines.png" alt="机器人基线结果"></p>
<blockquote>
<p><strong>图9</strong>：与基于机器人遥操作数据基线的对比结果。FTF（77%）的成功率高于使用机器人遥操作数据训练的BC（60%）和IL（63%），表明触觉手套实现的自然数据收集对于触觉数据收集是有效的，且避免了昂贵的机器人数据收集。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li>FTF在5个力敏感任务上取得了**77%**的平均成功率。</li>
<li>与被动使用人类力数据的基线（47%）相比，FTF成功率高出30%，验证了主动力预测框架的有效性。</li>
<li>与基于机器人遥操作数据的最佳基线（63%）相比，FTF成功率高出14%，证明了从人类自然演示中学习的优势。</li>
<li>在存在对抗性干扰（在药瓶盖上附加重量以改变力分布）的任务中，FTF仍能达到**67%**的成功率，显示出对测试时触觉数据分布变化的鲁棒性。</li>
</ol>
<p><strong>消融实验</strong>：论文通过改变训练数据量进行了分析。<br><img src="https://arxiv.org/html/2506.01944v1/extracted/6504670/scale_data.png" alt="数据量分析"></p>
<blockquote>
<p><strong>图7</strong>：训练数据量对FTF和被动使用力基线（BC-Vision+Force）性能的影响。随着数据量减少，被动方法的性能下降比FTF更剧烈，表明FTF的主动力预测方法具有更高的数据效率。</p>
</blockquote>
<p><strong>定性结果</strong>：<br><img src="https://arxiv.org/html/2506.01944v1/extracted/6504670/rollouts.png" alt="执行过程"></p>
<blockquote>
<p><strong>图10</strong>：FTF策略在机器人上执行任务时的定性结果序列。展示了策略在不同任务中成功控制力的过程。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>FTF框架</strong>，首次通过主动预测期望接触力并利用推理时PD控制器进行跟踪的方式，实现了从人类触觉演示到机器人力敏感操作的零样本转移。</li>
<li>设计并采用了<strong>低成本的定制触觉手套</strong>，使人类能够在其自然形态和日常环境中提供丰富的接触力演示，为触觉模仿学习提供了一种高效、可扩展的数据收集范式。</li>
<li>通过<strong>解耦策略学习与低层力控制</strong>，使学习到的策略对形态差异和传感器噪声更具鲁棒性，并在多个真实力敏感任务上取得了显著优于现有方法的表现。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到两个主要假设：1）人类手部在首帧的姿态与机器人夹持器在复位时的姿态相同；2）需要在已校准的场景中操作（已知相机内参和外参）。这些假设在一定程度上限制了部署的灵活性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>放松初始姿态假设</strong>：可以研究如何从任意初始手部姿态进行初始化，或引入更通用的姿态对齐方法。</li>
<li><strong>扩展传感器配置与任务</strong>：可以探索在手套上集成更多传感器（如多个指尖力传感），并将方法应用于更复杂的双手或全身操作任务。</li>
<li><strong>结合更高级的力控制</strong>：当前的PD控制器较为简单，未来可以探索更先进的自适应或基于学习的力控制方法，以处理更复杂的接触动力学。</li>
<li><strong>无校准设置</strong>：研究如何在不依赖精确相机校准的情况下实现3D关键点重建和动作重定向，进一步提升系统的易用性和普适性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人精细力控制泛化性差、视觉演示缺乏力信息的核心问题，提出FTF接触驱动学习系统。该方法通过触觉手套测量人类演示的接触力，结合视觉模型估计手部姿态，训练闭环策略连续预测操作所需力；利用共享视觉和动作表示将策略重定向至配备触觉夹爪的Franka Panda机器人，并通过PD控制器调制夹爪闭合以跟踪预测力，实现精确力感知控制。在5个力敏感操作任务中，系统达到77%的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.01944" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>