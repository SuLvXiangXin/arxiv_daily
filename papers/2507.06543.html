<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Token Bottleneck: One Token to Remember Dynamics - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Token Bottleneck: One Token to Remember Dynamics</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.06543" target="_blank" rel="noreferrer">2507.06543</a></span>
        <span>作者: Sangdoo Yun Team</span>
        <span>日期: 2025-07-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人与AI领域，让机器在现实环境中进行顺序感知与交互（如视觉跟踪、机器人操作）是核心挑战，这依赖于强大的视觉主干网络。自监督学习（SSL）已成为获取通用视觉表示的关键途径。当前主流方法主要分为两类：针对静态场景的方法（如对比学习SimCLR、MoCo和掩码图像建模MAE）和针对动态场景的改进方法（如SiamMAE、CropMAE、RSP）。然而，静态场景SSL（如MAE）缺乏对时序动态的显式建模，而动态场景SSL（如SiamMAE）虽然引入了跨帧对应学习，但其在顺序场景理解任务（如机器人操作）上的提升有限，有时甚至不如MAE。</p>
<p>本文指出，顺序场景理解任务不仅要求模型能够识别时序变化，更关键的是需要对观察到的场景进行保守的、无损失的总结，并将时序线索嵌入到这种总结后的表示中。现有方法未能同时满足这两点：MAE擅长定位但缺乏时序和整体场景理解；SiamMAE关注对应关系，但可能牺牲了对单个场景的保守编码能力。</p>
<p>本文针对“如何在自监督预训练中同时学习对场景的保守总结和对时序动态的有效识别”这一具体痛点，提出了Token Bottleneck (ToBo)方法。其核心思路是：通过一个“挤压-扩展”的管道，将参考场景压缩成一个瓶颈token，并仅用极少的目标场景patch作为提示来预测目标场景，从而迫使模型在瓶颈token中同时编码场景本质信息和时序动态。</p>
<h2 id="方法详解">方法详解</h2>
<p>Token Bottleneck (ToBo) 的整体流程是一个两阶段管道：挤压（Squeeze）和扩展（Expand）。输入是一对时序上相关的场景图像：参考场景 $\mathbf{x}^t$ 和目标场景 $\mathbf{x}^{t+k}$。输出是重建被高度掩码的目标场景。</p>
<p><strong>整体框架</strong>：在挤压阶段，完整的参考场景 $\mathbf{x}^t$ 通过 Vision Transformer 编码器 $f_\theta$，将其所有patch编码为空间表示，并取其中的 [CLS] token 作为<strong>瓶颈token</strong> $\mathbf{u}<em>{tobo}$，该token被引导以紧凑地总结参考场景。在扩展阶段，目标场景 $\mathbf{x}^{t+k}$ 被以极高的掩码率 $r$（例如95%）进行随机掩码，仅保留极少数（如5%）的patch作为提示。这些未被掩码的目标patch也通过同一个编码器 $f_\theta$ 得到其表示 ${\mathbf{u}^{t+k}<em>i}</em>{i \in \mathcal{M}^c}$。随后，将瓶颈token $\mathbf{u}</em>{tobo}$ 与这些极少量的目标patch表示拼接，并为掩码位置填充掩码token $\mathbf{m}$，共同输入解码器 $d_\phi$。解码器的任务是预测所有被掩码的目标场景patch ${\hat{\mathbf{x}}^{t+k}<em>i}</em>{i \in \mathcal{M}}$。训练目标是最小化重建损失 $\mathcal{L}<em>{\text{ToBo}} = \sum</em>{i \in \mathcal{M}} d(\hat{\mathbf{x}}^{t+k}_i, \mathbf{x}^{t+k}_i)$，其中 $d(\cdot)$ 为余弦距离。</p>
<p><img src="https://arxiv.org/html/2507.06543v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：Token Bottleneck (ToBo) 概览。我们的ToBo根据参考场景 $\mathbf{x}^t$ 的瓶颈token表示和来自目标场景 $\mathbf{x}^{t+k}$ 的极其稀少的patch来重建被掩码的patch。这种极端稀缺性导致解码器 $d_\phi$ 严重依赖参考场景 $\mathbf{x}^t$，从而促进了观察信息在瓶颈token中的保存。</p>
</blockquote>
<p><strong>核心模块与创新细节</strong>：</p>
<ol>
<li><strong>瓶颈机制与极端掩码率</strong>：这是方法的核心创新。对目标场景施加极端高的掩码率（如95%），使得解码器仅凭极少的目标patch提示无法完成重建，从而被迫严重依赖来自参考场景的瓶颈token $\mathbf{u}_{tobo}$。这种设计产生了两个关键效应：(a) 迫使编码器必须将参考场景的“本质信息”压缩进瓶颈token，实现保守总结；(b) 迫使这种总结必须与极少的目标提示相结合才能推演出目标场景，从而将时序动态的识别能力编码进表示中。</li>
<li><strong>解码器结构</strong>：与SiamMAE等使用交叉注意力（cross-attention）来显式建立帧间对应关系不同，ToBo的解码器仅使用自注意力（self-attention）和MLP层。这确保了在扩展阶段，解码器只关注给定的信息（瓶颈token + 少量目标patch），而不引入额外的归纳偏置，让模型自主学习如何利用瓶颈token中的总结信息进行时序推理。</li>
<li><strong>共享编码器</strong>：参考场景和目标场景使用同一个编码器 $f_\theta$ 进行处理，这保证了表示空间的一致性，并鼓励编码器学习到同时适用于单个场景总结和跨场景推理的通用表示。</li>
</ol>
<p>与现有方法相比，ToBo的创新点具体体现在：它不依赖于显式的对应关系匹配或复杂的帧预测，而是通过一个简单的、受极端信息稀缺性驱动的“总结-推理”任务，统一地学习了场景保守编码和时序动态感知。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：预训练数据集使用Kinetics-400。评估主要在两大类顺序理解任务上进行：1) <strong>视频标签传播</strong>（Video Label Propagation），在VIP、DAVIS、JHMDB数据集上评估；2) <strong>机器人视觉策略学习</strong>，在多个模拟基准（Franka Kitchen, CortexBench (包含Adroit, MetaWorld, DMC, TriFinger), RLBench）和真实世界机器人上进行。对比的基线方法包括静态SSL（SimCLR, MoCo v3, DINO, MAE）和动态SSL（SiamMAE, RSP, CropMAE）。骨干网络主要为ViT-S/16。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>模拟机器人操作（表1，2，3）</strong>：在Franka Kitchen、CortexBench和RLBench的多个任务上，ToBo均显著超过所有基线方法。例如在Franka Kitchen的“Knob1 on”任务上，ToBo成功率为57.3%，而最佳基线RSP为31.0%；在CortexBench的DMC域上，ToBo标准化得分为73.5，远超第二佳RSP的61.6。<br><img src="https://arxiv.org/html/2507.06543v1/x4.png" alt="表1"></p>
<blockquote>
<p><strong>表1</strong>：Franka Kitchen任务上的成功率（%）。ToBo在所有任务上均取得最佳性能，且相比第二佳基线有巨大提升（例如Knob1 on任务提升约26个百分点）。<br><img src="https://arxiv.org/html/2507.06543v1/x5.png" alt="表2"><br><strong>表2</strong>：CortexBench任务上的性能。ToBo在Adroit、MetaWorld和DMC上均表现最佳。<br><img src="https://arxiv.org/html/2507.06543v1/x6.png" alt="表3"><br><strong>表3</strong>：RLBench任务上的成功率（%）。ToBo在五个任务中的四个上取得最佳性能。</p>
</blockquote>
</li>
<li><p><strong>真实世界机器人操作（表4，图4）</strong>：将预训练模型部署到物理机器人上，在柜门打开、抽屉关闭、杯子堆叠三个任务上评估。ToBo取得了65.0%、75.0%、80.0%的成功率，全面优于动态SSL基线（SiamMAE, RSP, CropMAE），证明了其强大的现实世界泛化能力。<br><img src="https://arxiv.org/html/2507.06543v1/x7.png" alt="表4"></p>
<blockquote>
<p><strong>表4</strong>：真实世界机器人操作成功率（%）。ToBo在所有三个任务上均大幅领先。<br><img src="https://arxiv.org/html/2507.06543v1/x8.png" alt="真实世界轨迹"><br><strong>图4</strong>：真实世界机器人任务轨迹示例：(a)柜门打开，(b)抽屉关闭，(c)杯子堆叠。展示了任务的初始、中间和最终状态。</p>
</blockquote>
</li>
<li><p><strong>视频标签传播</strong>：论文在VIP、DAVIS、JHMDB数据集上进行了评估，并提供了丰富的定性结果图（图9-图32），展示了ToBo在像素级标签传播中能够更好地处理物体形变、运动模糊和长期依赖，相比基线方法产生的掩码更准确、一致。<br><img src="https://arxiv.org/html/2507.06543v1/extracted/6607834/materials/qual_vlp/vip_masks/videos361/videos361_000000000001_merge.png" alt="定性结果示例1"></p>
<blockquote>
<p><strong>图9</strong>：VIP数据集上的定性比较示例（初始帧）。展示了不同方法预测的掩码与真实掩码的对比。<br><img src="https://arxiv.org/html/2507.06543v1/extracted/6607834/materials/qual_vlp/davis_masks/parkour/00000_merge.png" alt="定性结果示例2"><br><strong>图17</strong>：DAVIS数据集上“parkour”序列的定性比较（初始帧）。</p>
</blockquote>
</li>
<li><p><strong>消融实验（图33）</strong>：论文对目标场景掩码率 $r$ 进行了消融研究。结果表明，当掩码率极高时（90%-99%），模型在机器人操作任务上的性能最佳。当掩码率较低（如75%）时，性能显著下降，这验证了“极端信息稀缺性”对于迫使模型依赖瓶颈token、从而学习有效表示的关键作用。<br><img src="https://arxiv.org/html/2507.06543v1/extracted/6607834/materials/ablation_graphs/graph_tgt_mask_ratio.png" alt="消融实验"></p>
<blockquote>
<p><strong>图33</strong>：目标掩码比率消融实验。性能在极高掩码率（90-99%）时达到峰值，验证了极端稀缺性的必要性。</p>
</blockquote>
</li>
<li><p><strong>模型可扩展性</strong>：论文还在ViT-B/16和ViT-L/16尺度上进行了实验，结果显示随着模型规模增大，ToBo带来的性能增益保持一致甚至扩大，证明了方法的可扩展性。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了Token Bottleneck (ToBo)，一种新颖且直观的自监督学习框架，通过“挤压-扩展”管道和极端目标掩码率，统一地促进了视觉主干网络对动态场景的保守总结能力和时序动态识别能力。</li>
<li>在广泛的顺序场景理解任务（包括模拟与真实机器人操作、视频标签传播）上进行了全面评估，证明了ToBo显著优于现有的静态和动态SSL方法，并具备强大的现实世界泛化能力。</li>
<li>通过消融研究验证了极端信息稀缺性（高目标掩码率）的设计有效性，并展示了方法在不同模型规模上的良好可扩展性。</li>
</ol>
<p><strong>局限性</strong>：论文在讨论中提到，由于ToBo专注于学习顺序动态，其在纯静态图像理解任务（如图像分类）上的表现可能不是最优的，尽管在顺序任务上优势明显。此外，方法在更广泛的视频理解任务（如动作识别）上的有效性仍有待进一步验证。</p>
<p><strong>对后续研究的启示</strong>：ToBo的成功表明，在自监督学习中设计巧妙的、受信息瓶颈原理启发的预测任务，可以有效地引导模型学习到对下游任务至关重要的表示特性（如紧凑总结和时序推理）。这为未来设计面向具身智能、机器人学等需要复杂时空推理领域的预训练方法提供了新思路。同时，如何平衡对静态场景内容和动态时序信息的学习，也是一个值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Token Bottleneck (ToBo)，旨在解决动态场景中紧凑且具有时序感知的视觉表示学习问题，以提升序列场景理解（如视觉跟踪、机器人操作）的性能。其核心方法采用自监督学习框架，包含“压缩”与“扩展”两步骤：先将参考场景编码为单一瓶颈令牌，再结合少量目标图像块预测后续场景，从而迫使模型学习场景间的动态演变。实验表明，ToBo在视频标签传播与仿真机器人操作等任务上优于基线，并在真实机器人部署中验证了其有效性与可扩展性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.06543" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>