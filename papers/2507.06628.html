<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.06628" target="_blank" rel="noreferrer">2507.06628</a></span>
        <span>作者: Jian Cheng Team</span>
        <span>日期: 2025-07-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>离线多任务强化学习（MTRL）旨在仅使用预收集的混合任务数据集，学习一个能够解决多个任务的统一策略，而无需与环境进行任何在线交互。然而，该方法在跨任务有效共享知识方面面临重大挑战。现有方法主要通过共享网络参数、设计特定架构、学习任务特定表示或定制优化程序来实现，但这些方法主要在动作层面进行操作，类似于模仿学习，与人类高效抽象知识的学习方式不同。人类倾向于从过往经验中识别和总结通用策略或模式，形成一组可复用的技能，并动态组合这些高级技能来解决特定任务。</p>
<p>本文针对离线MTRL中跨任务知识共享效率低下的痛点，从人类技能抽象的机制中获得新视角，提出目标导向的技能抽象（GO-Skill）。其核心思路是：从离线任务混合数据集中发现一个可复用的离散技能库，并学习一个基于技能的、能够通过技能组合解决多任务的分层策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>GO-Skill方法包含三个主要阶段：1）目标导向的技能提取；2）技能增强；3）分层策略学习。整体流程是首先从离线数据中提取并量化得到一个离散技能库，然后通过技能增强优化技能解码器，最后以技能作为高层策略的动作空间，学习一个能够为不同任务动态编排技能的分层策略。</p>
<p><img src="https://arxiv.org/html/2507.06628v1/x4.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图4</strong>：GO-Skill整体框架。包含三个阶段：1）<strong>技能提取</strong>：使用目标编码器、向量量化（VQ）和技能解码器从数据中学习一个离散技能库；2）<strong>技能增强</strong>：在技能库稳定后，通过按技能类别重采样数据来平衡训练，优化技能解码器；3）<strong>分层策略学习</strong>：学习一个高层策略，该策略以（收益目标，状态，技能）提示和H步间隔历史轨迹为输入，输出要执行的技能索引。</p>
</blockquote>
<p><strong>核心模块1：目标导向的技能提取</strong><br>该模块旨在从数据中提取一个离散的技能库。一个技能被定义为固定时间跨度H的子轨迹。技能模型包含三个部分：</p>
<ul>
<li><strong>目标编码器</strong>：将技能实现的动态转移（从初始状态 <code>s_t</code> 到目标状态 <code>s_{t+H}</code>）编码为一个潜在嵌入。具体地，目标嵌入计算为 <code>z_{t,H} = G(s_{t+H} - s_t)</code>，其中G是编码器网络。这种基于状态差异的目标表示是任务无关的，排除了因任务而异的奖励信号，专注于技能本身的效果。</li>
<li><strong>技能量化</strong>：使用向量量化（VQ）模块，将连续的目标嵌入 <code>z_{t,H}</code> 映射到离散的技能码本 <code>Z</code> 中。码本包含M个技能嵌入向量。通过VQ损失（公式3）优化码本和编码器，实现技能的离散化。</li>
<li><strong>技能解码器</strong>：作为与环境交互的低层策略。它以技能嵌入 <code>z_t</code> 为提示，结合历史状态序列、已达成目标序列（<code>z_{t,h} = G(s_{t+h}-s_t), h&lt;H</code>）和历史动作序列，使用Transformer架构重构出未来H步内的动作序列（公式7）。已达成目标帮助智能体评估当前技能的完成进度。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.06628v1/x2.png" alt="技能模型"></p>
<blockquote>
<p><strong>图2</strong>：GO-Skill中的技能模型。包含三个关键组件：(1) 目标编码器将状态间的轨迹级差异映射到潜在嵌入空间；(2) 技能码本通过向量量化将H步目标嵌入提炼为离散技能集；(3) 技能解码器使用Transformer架构，结合技能提示和历史序列重构动作。</p>
</blockquote>
<p><strong>核心模块2：技能增强</strong><br>在技能模型训练过程中，技能库会逐渐稳定，但会出现<strong>技能类别不平衡</strong>问题：广泛适用的技能数据丰富，解码器容易掌握；而任务特定技能数据有限，解码效果差。为解决此问题，在技能码本稳定后，冻结目标编码器和技能码本，然后根据技能类别划分数据集。在训练技能解码器时，采用跨技能类别的均匀重采样策略，以增强所有技能（特别是数据少的技能）的解码性能。</p>
<p><strong>核心模块3：分层策略学习</strong><br>高层策略以离散技能空间作为其动作空间。该策略基于Prompt Decision Transformer架构，其输入包括：1）任务特定的（收益目标，状态，技能）提示轨迹；2）最近K步的（收益目标，状态，技能）历史轨迹，其中历史轨迹以H步为间隔进行采样，即每一步历史对应一个完整的技能执行周期。策略的输出是下一个要执行的技能索引，该索引通过技能码本映射为对应的技能嵌入，进而由技能解码器执行。</p>
<p><img src="https://arxiv.org/html/2507.06628v1/x3.png" alt="技能策略"></p>
<blockquote>
<p><strong>图3</strong>：GO-Skill中基于技能的策略示意图。该策略采用Transformer架构，以离散技能空间作为动作空间。其输入包括（收益目标，状态，技能）提示和H步间隔的历史轨迹。在每个决策点，策略预测一个技能索引，随后通过技能码本映射到对应的技能嵌入。</p>
</blockquote>
<p><strong>方法创新点</strong></p>
<ol>
<li><strong>技能表示创新</strong>：提出以状态差异为目标的、任务无关的技能表示方法，替代传统的基于奖励或动作序列的表示，更利于跨任务技能抽象。</li>
<li><strong>技能库构建创新</strong>：利用向量量化从离线数据中自动学习离散的技能原型库，而非手动定义技能。</li>
<li><strong>训练流程创新</strong>：引入技能增强阶段，通过重采样解决技能类别不平衡问题，确保所有技能（包括稀有技能）都能被有效学习。</li>
<li><strong>决策范式创新</strong>：采用分层策略，高层策略在技能空间进行长周期规划，缩短了决策视野，简化了学习问题。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在包含50个机器人操作任务的MetaWorld基准上进行评估。使用任务混合的离线数据集，其中包含专家、中等水平和随机策略收集的轨迹。评估指标是平均任务成功率。</p>
<p><strong>对比方法</strong>：与多种离线MTRL基线方法对比，包括：基于策略融合的MT-ACT、基于上下文学习的Prompt-DT及其变种（MLP和Transformer版本）、基于任务特定模块的SPiRL、以及单任务训练的Decision Transformer（ST-DT）。</p>
<p><img src="https://arxiv.org/html/2507.06628v1/x5.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：在MetaWorld 50个任务上的平均成功率对比。GO-Skill（红色）以90.1%的平均成功率显著优于所有基线方法（最高为MT-ACT的84.3%）。</p>
</blockquote>
<p><strong>关键实验结果</strong>：GO-Skill在50个任务上取得了平均<strong>90.1%</strong> 的成功率，显著优于其他最佳基线方法MT-ACT（84.3%）和Prompt-DT（Transformer, 82.2%）。这证明了其通过技能抽象进行知识共享的有效性。</p>
<p><img src="https://arxiv.org/html/2507.06628v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融实验结果。从左至右分别移除了技能增强（SE）、向量量化（VQ）、目标编码器（使用动作序列作为目标，GA→LA）和分层策略（使用连续技能嵌入，DP）。每个组件的移除都导致性能下降，证明了它们的必要性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>移除技能增强（w/o SE）</strong>：成功率降至86.0%，表明解决技能类别不平衡至关重要。</li>
<li><strong>移除向量量化（w/o VQ）</strong>：使用连续技能嵌入，成功率降至85.8%，说明离散化技能库有利于高层策略的规划和泛化。</li>
<li><strong>替换目标表示（GA→LA）</strong>：将基于状态差异的目标编码器改为基于动作序列的编码器，成功率大幅降至76.9%，验证了目标导向（状态变化）表示优于动作序列表示。</li>
<li><strong>移除分层策略（w/o DP）</strong>：高层策略直接输出连续技能嵌入而非离散索引，成功率降至82.7%，表明离散技能空间简化了高层决策。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.06628v1/x7.png" alt="定性结果"></p>
<blockquote>
<p><strong>图7</strong>：在<code>window-open</code>和<code>door-open</code>任务上的定性结果。两个任务共享“接近并操作手柄”的技能（技能1和技能14），但使用了不同的任务特定技能来完成最终动作（技能7 vs 技能16）。这直观展示了GO-Skill的跨任务技能共享与组合能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.06628v1/x8.png" alt="技能数量分析"></p>
<blockquote>
<p><strong>图8</strong>：技能数量M对性能的影响。性能随着M增加而提升并逐渐饱和，M=256时达到最佳。过少的技能（M=64）不足以覆盖所有行为，过多的技能（M=512）则可能使高层策略学习困难。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.06628v1/x9.png" alt="表示方法对比"></p>
<blockquote>
<p><strong>图9</strong>：不同技能表示方法在<code>pick-place</code>任务上的成功率对比。基于状态差异（SD）的方法显著优于基于动作（Act）和基于状态（State）的表示方法。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>为离线多任务强化学习提出了一个受人类启发的<strong>新视角</strong>，即通过抽象和组合可复用技能来提升知识共享与学习效率。</li>
<li>提出了<strong>GO-Skill方法</strong>，包含目标导向技能提取、技能增强和分层策略学习三个关键技术阶段，系统性地实现了上述视角。</li>
<li>在复杂的MetaWorld基准上进行了<strong>广泛实验</strong>，验证了GO-Skill相对于现有方法的显著优越性，并通过详实的消融研究证实了各组件的作用。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法中技能的数量M需要预先设定，且技能的表示依赖于状态差，对于更复杂的任务可能需要更精细的目标表示（例如利用视觉-语言模型）。</p>
<p><strong>研究启示</strong>：</p>
<ol>
<li><strong>利用次优数据</strong>：GO-Skill善于从次优轨迹中提取有价值的技能片段，为利用质量参差不齐的离线数据提供了思路。</li>
<li><strong>分层决策</strong>：将决策分解为技能选择（高层）和技能执行（低层），缩短了高层策略的规划视野，可以缓解长期信用分配问题，并可能提升策略的泛化性和可解释性。</li>
<li><strong>离散化表示</strong>：学习离散的技能原型库，为高层策略提供了一个结构化、可组合的动作空间，这可能是实现更复杂组合泛化的关键。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对离线多任务强化学习中知识共享效率低的挑战，提出目标导向的技能抽象方法GO-Skill。该方法通过面向目标的技能提取和向量量化构建离散技能库，并引入技能增强阶段平衡技能分布，最后通过分层策略学习动态组合技能以解决不同任务。在MetaWorld机器人操作任务上的实验验证了该方法的有效性和泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.06628" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>