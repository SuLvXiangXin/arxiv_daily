<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.26642" target="_blank" rel="noreferrer">2509.26642</a></span>
        <span>作者: Shanghang Zhang Team</span>
        <span>日期: 2025-09-30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过继承视觉-语言模型的常识推理能力并学习动作生成，在机器人操作任务中展现出泛化能力。然而，大多数VLA模型主要依赖2D图像和语言指令来生成动作，而机器人必须在空间物理世界中感知和交互。现有方法主要基于2D图像，这从根本上不足以捕捉空间依赖关系和建模物理动态。尽管一些研究引入了3D点云或触觉信号以增强感知，但它们通常需要特定模态的编码器，降低了效率，且大语言模型主干因缺乏多感官预训练而难以与新模态特征对齐。另一方面，一些研究试图通过预测未来状态（如图像）来推理物理动态，但局限于2D预测，难以预测对接触密集场景理解和运动规划至关重要的完整点云结构和触觉交互信息。</p>
<p>本文针对如何将多感官模态整合为统一表示并预测其未来状态，以协同增强VLA模型对物理世界的理解和动作生成这一具体痛点，提出了多感官语言-动作模型。其核心思路是：通过一种无编码器的多模态对齐机制，将LLM本身重新用作感知模块来直接解释多模态线索；并设计一种未来多感官生成的后训练策略，使模型能够从语义、几何和交互多个维度推理物理动态，为动作生成提供更鲁棒的条件。</p>
<h2 id="方法详解">方法详解</h2>
<p>MLA模型整体框架基于一个大语言模型，采用三阶段训练范式：大规模预训练、带跨模态对齐的监督微调、以及带未来状态预测的后训练。</p>
<p><img src="https://arxiv.org/html/2509.26642v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：MLA整体框架。a) 除了语言指令和机器人状态，MLA引入了创新的无编码器多模态对齐机制，使LLM能够直接整合RGB图像、点云和触觉信号，并通过令牌级对比学习对齐它们。在输出层面，MLA进一步加入了未来多感官生成后训练策略，允许模型生成未来多感官状态，为动作生成提供更鲁棒的条件。b) MLA采用三阶段训练范式：大规模预训练、带跨模态对齐的监督微调、以及带未来状态预测的后训练。</p>
</blockquote>
<p>模型输入包括图像观测 $I_t$、点云 $P_t$、触觉信号 $T_t$、机器人状态 $S_t$ 和语言指令 $L$。输出为预测的即时动作序列 $a_{t:t+H}$ 以及未来关键帧的多模态观测 $I_{t+N}, P_{t+N}, T_{t+N}$。核心模块包括轻量级分词器、LLM主干、以及未来预测解码器。</p>
<p><strong>轻量级分词器</strong>：为避免引入缺乏与LLM对齐预训练的额外编码器，MLA使用轻量级分词器将原始多感官输入直接转换为共享令牌序列。</p>
<ul>
<li><strong>图像分词器</strong>：将图像分割为 $14\times14$ 的非重叠块，生成 $N_{\mathrm{img}}=256$ 个令牌。</li>
<li><strong>3D点云分词器</strong>：通过最远点采样、K近邻局部聚合和可学习线性层对原始点云进行下采样和编码，生成 $N_{\mathrm{pc}}=256$ 个令牌。</li>
<li><strong>触觉分词器</strong>：一个简单的基于MLP的分词器，处理来自两个夹爪传感器的力信号（法向力、切向力及其方向），生成一个触觉令牌 $f^{\mathrm{tac}} \in \mathbb{R}^{B \times 1 \times d_h}$。<br>所有令牌被投影到共享嵌入空间，与语言令牌一起由LLM处理。扩散动作头所需的噪声令牌附加在序列末尾。</li>
</ul>
<p><strong>无编码器多模态对齐机制</strong>：该方法重新利用LLM的初始Transformer块作为统一的感知模块，通过令牌级对比损失增强多感官表示，而无需额外的模态特定编码器。关键在于构建跨模态位置映射以形成正负样本对：利用相机参数将3D点云令牌对应的中心点以及触觉夹爪的3D位置投影到2D图像平面上，找到对应的图像补丁。对应的2D令牌、3D令牌和触觉令牌构成正样本对 $(f_j^{img} – f_i^{pc} – f^{tac})$，未匹配的令牌作为负样本。通过图像-点云对比损失 $\mathcal{L}<em>{\mathrm{img_pc}}$ 和触觉-图像/点云对比损失 $\mathcal{L}</em>{\mathrm{tac_img/pc}}$ 进行自监督对齐，总体对比目标为 $\mathcal{L}<em>{\mathrm{contrastive}} = \mathcal{L}</em>{\mathrm{img_pc}} + \mathcal{L}<em>{\mathrm{tac_img}} + \mathcal{L}</em>{\mathrm{tac_pc}}$。</p>
<p><strong>未来多感官生成后训练策略</strong>：采用基于Transformer的解码器从LLM的最终隐藏状态预测未来多模态状态。</p>
<ul>
<li><strong>图像预测</strong>：解码器预测未来关键帧图像，使用MSE损失监督，并通过深度图移除背景像素以简化优化。</li>
<li><strong>点云预测</strong>：解码器重建下一关键帧点云。将真实点云通过FPS和KNN分组为局部块，解码器输出预测坐标，使用倒角距离监督。</li>
<li><strong>触觉预测</strong>：解码器输出低维触觉嵌入，用MSE损失监督。<br>未来预测损失 $\mathcal{L}_{\mathrm{future}}$ 仅在后期训练阶段应用，不影响推理效率。</li>
</ul>
<p><strong>整体训练流程</strong>：</p>
<ol>
<li><strong>大规模预训练</strong>：在超过57万条轨迹的大规模图像-动作配对数据集上预训练10个周期，仅使用图像和语言输入，动作生成采用标准DDPM目标 $\mathcal{L}_{\mathrm{diff}}$。</li>
<li><strong>监督微调</strong>：在高质量任务特定数据集上引入所有多感官模态，训练目标为 $\mathcal{L}<em>{\mathrm{sft}} = \mathcal{L}</em>{\mathrm{diff}} + \mathcal{L}_{\mathrm{contrastive}}$。</li>
<li><strong>后训练</strong>：进行未来多感官生成训练，目标为 $\mathcal{L}<em>{\mathrm{post}} = \mathcal{L}</em>{\mathrm{diff}} + \mathcal{L}<em>{\mathrm{contrastive}} + \mathcal{L}</em>{\mathrm{future}}$。</li>
</ol>
<p>与现有方法相比，MLA的创新点具体体现在：1) 避免了引入需要额外预训练且可能效率低下的模态特定编码器，创新性地将LLM本身重新用作统一感知模块；2) 首次联合预测对操作至关重要的图像、点云和触觉信号的未来状态，从多维度增强物理动态理解。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实世界和RLBench模拟器上进行评估。真实世界实验使用Franka单臂和双臂机器人，设计了六个复杂、接触密集的任务：单臂任务包括“按压图章到纸上”、“用橡皮擦白板”、“将盘子放到架子上”、“用锅铲将鸡蛋放到面包上”；双臂任务包括“将爆米花舀到碗里”、“打开锅盖并从锅中取玉米”。每个任务收集200条高质量演示。对比的基线方法是SOTA的2D VLA模型 $\pi_0$ 和SOTA的3D VLA模型 SpatialVLA。所有模型使用相同数量的相机视角，每个任务在一致测试条件下进行15次 rollout 评估。</p>
<p><img src="https://arxiv.org/html/2509.26642v1/x3.png" alt="真实世界实验结果"></p>
<blockquote>
<p><strong>图3</strong>：真实世界实验结果。所有模型在不同物体位置进行了15次 rollout 评估，任务完成度由人工判断。MLA在六个任务中均取得了最佳性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>：如图3所示，MLA在六个真实世界任务中均取得了最高的成功率，平均优于 $\pi_0$ 12%，优于 SpatialVLA 24%。例如，在“用橡皮擦白板”任务中，MLA能有效利用触觉感应来调节擦拭过程中末端执行器的向下和横向运动。MLA的优越性能归因于其能更好地对齐和解释机器人多感官输入，从而增强了其对物理环境的感知表征。相对于SpatialVLA，MLA的优势还在于其生成未来多感官状态的能力，从而改进了物理动态建模。</p>
<p><img src="https://arxiv.org/html/2509.26642v1/x5.png" alt="消融研究"></p>
<blockquote>
<p><strong>图5</strong>：消融研究。系统分析了MLA模型中每个组件的贡献。a) 不同输入模态和对齐策略的影响；b) 对比损失应用于LLM不同层的影响。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>输入模态与对齐策略的影响</strong>（图5a）：实验比较了仅2D图像、2D+3D简单拼接、2D+3D+触觉简单拼接、所有模态+图像级对比对齐、以及本文提出的所有模态+令牌级对比对齐。结果表明，语义、空间和交互感知对于接触密集的操作都至关重要。与简单拼接相比，本文提出的位置引导的令牌级对比对齐带来了显著提升（例如Ex5 vs Ex3）。与图像级对比对齐相比，细粒度的令牌级跨模态对齐在物理世界感知中更具优势（Ex5比Ex4高7%准确率）。</li>
<li><strong>对比损失位置的影响</strong>（图5b）：将令牌级对比学习应用于LLaMA-2主干的不同层（第4、8、12、32层）。结果显示，在第8层应用时性能最佳，因为它在相对浅层对齐特征，同时为后续Transformer块留出足够空间专注于未来状态预测和动作生成。在第32层（最终层附近）应用自监督则收益有限。</li>
</ol>
<p><strong>模拟器实验结果</strong>：在RLBench模拟器的10个任务上进行评估，以验证可复现性。由于模拟中的触觉感知不真实，因此仅使用图像和点云。</p>
<p><img src="https://arxiv.org/html/2509.26642v1/x6.png" alt="RLBench模拟器结果"></p>
<blockquote>
<p><strong>表1</strong>：RLBench基准测试结果。每个模型评估20次 rollout，成功率由RLBench内置评估模块确定。MLA取得了有竞争力的性能。</p>
</blockquote>
<p>如表1所示，MLA在RLBench基准测试中取得了有竞争力的性能，平均成功率为79.3%，优于 $\pi_0$ 和 SpatialVLA。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了MLA模型及其<strong>无编码器多模态对齐机制</strong>，创新性地将LLM本身重新用作感知模块，直接对齐和解释图像、点云和触觉线索；2) 引入了<strong>未来多感官生成后训练策略</strong>，使模型能够联合预测图像、点云和触觉的未来状态，从语义、几何和交互多个维度推理物理动态，为动作生成提供更鲁棒的条件；3) 通过渐进式的预训练、微调和后训练流程，MLA在复杂的真实世界单臂和双臂操作任务中实现了<strong>最先进的成功率和强大的泛化能力</strong>。</p>
<p>论文自身提到的局限性在于：现有的开源真实世界数据集缺乏多感官信息，因此预训练阶段仅使用了大规模图像-动作配对数据集；此外，模拟环境中的触觉感知并不真实，因此触觉信号仅在真实世界实验中使用。</p>
<p>本文的启示在于：1) 对于机器人多模态理解，<strong>跨模态的细粒度对齐</strong>（如基于位置的令牌级对比）比简单的特征拼接或粗粒度对齐更有效；2) <strong>预测未来多感官状态</strong>是一种增强模型物理世界理解和动作生成条件的有效途径；3) 充分利用<strong>预训练大语言模型</strong>的潜力，将其作为统一的感知-推理策略，可以避免引入冗余的编码器参数，提升效率。这些思路为构建更通用、更鲁棒的具身智能模型提供了新的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MLA多感官语言-动作模型，旨在解决机器人操作中现有视觉-语言-动作模型过度依赖2D图像、难以全面感知物理空间动态的问题。关键技术包括：1）无编码器的多模态对齐方案，直接利用大语言模型对齐2D图像、3D点云与触觉标记；2）未来多感官生成后训练策略，增强对物理动态的推理能力。实验表明，MLA在复杂接触式任务中超越此前最优2D与3D方法12%和24%，并展现出更强的泛化性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.26642" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>