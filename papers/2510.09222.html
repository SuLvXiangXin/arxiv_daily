<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.09222" target="_blank" rel="noreferrer">2510.09222</a></span>
        <span>作者: Ivor Tsang Team</span>
        <span>日期: 2025-10-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，基于流匹配（Flow Matching, FM）的策略因其强大的分布建模能力，已成为模仿学习（Imitation Learning, IL）中克隆专家行为的主流方法。然而，这类FM策略本质上是为离线学习（遵循行为克隆范式）设计的，缺乏与环境交互的内置探索机制。这导致其在专家演示未覆盖的新场景中泛化能力差。特别是在专家数据有限或次优时，基于行为克隆的FM策略会过拟合到次优数据，进一步加剧性能下降。一个潜在的解决方案是让FM策略在在线环境中进行探索和优化，但直接通过在线策略梯度方法（如PPO）优化FM策略面临巨大挑战：由于FM策略的迭代性质（如使用数值ODE求解器），需要通过时间反向传播计算梯度，这在实际中极不稳定且计算成本高昂。</p>
<p>本文针对FM策略缺乏在线探索能力、在线优化梯度计算不稳定以及推理成本高这三个关键局限性，提出了一种新的视角：借鉴逆强化学习（IRL）的框架，通过一个师生架构，让一个轻量级的MLP“学生”策略负责在线探索，同时利用一个“教师”FM模型来提供奖励信号并正则化学生策略。本文的核心思路是：利用一个FM模型同时充当增强型奖励模型和策略正则器，引导一个简单的MLP策略进行高效的在线强化学习，从而在保留FM强大分布匹配能力的同时，规避其在线应用时的固有缺陷。</p>
<h2 id="方法详解">方法详解</h2>
<p>FM-IRL的整体框架采用师生架构。给定专家演示，首先训练一个“教师”条件FM模型来拟合专家状态-动作的联合分布。该教师模型承担两个核心角色：1) 作为FM增强的判别器，为在线强化学习提供基于分布相似度的奖励信号；2) 作为基于FM的生成器，产生符合专家分布的状态-动作对，用于正则化“学生”策略的行为。一个结构简单的MLP“学生”策略则负责与环境交互、收集数据，并利用来自教师模型的奖励信号和正则化项进行在线策略优化（如使用PPO）。部署时仅使用高效的学生策略。</p>
<p><img src="https://arxiv.org/html/2510.09222v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：FM-IRL整体工作流程。教师FM模型具有双重作用：(1) 训练一个奖励模型用于学生策略的下游强化学习；(2) 生成状态-动作对以正则化学生策略。学生策略与环境交互收集数据，这些数据与专家数据一同用于训练教师FM模型。</p>
</blockquote>
<p><strong>核心模块一：FM增强的判别器（奖励模型）</strong>。传统对抗模仿学习（AIL）中的MLP判别器仅建模数据点级别的相似性。FM-IRL创新性地将教师FM模型的训练损失值转化为分布级别的相似度度量，以此构建判别器。具体而言，教师FM模型以状态-动作对<code>(s, a)</code>和条件指示符<code>c</code>（<code>c=1</code>代表专家分布，<code>c=0</code>代表智能体分布）为输入进行训练。对于一个给定的<code>(s’, a’)</code>，其到目标分布（由<code>c</code>指定）的距离定义为：<code>Dist_θ(s’, a’|c) = E_t, (s_t, a_t) [|| v_θ((s_t, a_t), t|c) - u_t((s_t, a_t)|(s’, a’), c) ||^2]</code>。其中，<code>v_θ</code>是学习的向量场，<code>u_t</code>是预定义的条件流速度。该距离值越小，表示<code>(s’, a’)</code>越接近目标分布。随后，通过Softmax函数将距离转换为判别器输出：<code>D_FM,θ(s, a) = exp(-Dist_θ(s,a|c=1)) / [exp(-Dist_θ(s,a|c=1)) + exp(-Dist_θ(s,a|c=0))]</code>。此输出值在<code>[0,1]</code>之间，用于按照标准AIL目标（公式5）训练判别器，并最终通过公式<code>r_θ(s, a) = log(D_FM,θ(s, a)) - log(1 - D_FM,θ(s, a))</code>形成为策略优化提供指导的奖励信号。这种方法将教师FM模型对专家行为模式的深刻理解“注入”到了奖励函数中。</p>
<p><strong>核心模块二：基于FM的策略正则化器</strong>。仅使用专家演示和学得的奖励进行在线RL，可能在分布外状态-动作对上出现奖励误估计。为解决此问题并稳定训练，FM-IRL复用训练好的条件FM模型（<code>c=1</code>）作为生成器<code>G_θ</code>，产生与专家分布高度一致的状态-动作对<code>(s_G, a_G)</code>。学生策略<code>π_φ</code>（一个简单的MLP）在优化时，除了最大化累积奖励外，还增加了一个正则化项，使其在生成的状态<code> s_G</code>下输出的动作<code>a_π</code>尽可能接近生成的动作<code>a_G</code>。策略优化目标为：<code>max_φ J(φ) = E_(s,a)~π_φ [Σ γ^k r_θ(s,a)] - β E_(s_G,a_G)~G_θ(·|c=1), a_π~π_φ [||a_π - a_G||^2]</code>，其中<code>β</code>是平衡探索（奖励最大化）和利用（向专家行为靠拢）的超参数。这提供了一种更直接的方式，将教师FM模型的知识“灌输”给学生MLP策略。</p>
<p><strong>创新点总结</strong>：1) <strong>首次将FM用于在线RL的奖励建模与正则化</strong>：提出了利用FM模型的训练损失作为分布相似度度量来构建判别器的新方法。2) <strong>创新的师生架构</strong>：通过将表达能力强的FM模型作为“教师”，仅将其知识通过奖励和正则化项间接指导一个轻量级MLP“学生”策略，完美规避了直接在线优化FM策略时梯度计算不稳定和推理成本高的问题。3) <strong>双重知识注入机制</strong>：通过FM增强的奖励模型和FM生成的正则化样本，从两个互补的途径将专家分布信息传递给学生策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在六个MuJoCo环境中进行评估，涵盖导航（Ant-goal, Maze2d）、运动（Hopper, Walker2d）和操作（Hand-rotate, Fetch-pick）任务。每个实验重复4个随机种子。</p>
<p><strong>基线方法</strong>：对比方法分为两类：1) <strong>监督行为克隆方法</strong>：扩散策略（DP）、流匹配策略（FP）；2) <strong>逆强化学习方法</strong>：GAIL, VAIL, WAIL, AIRL, DRAIL。</p>
<p><img src="https://arxiv.org/html/2510.09222v2/x3.png" alt="学习曲线"></p>
<blockquote>
<p><strong>图3</strong>：FM-IRL与基线方法在六个环境中的学习曲线。FM-IRL在除Ant-goal外的所有环境中都达到了最佳最终性能，且在Hand-rotate和Fetch-pick中接近100%成功率。与其它IRL方法相比，FM-IRL收敛更快、性能更优、方差更低。与离线克隆方法（DP/FP）相比，FM-IRL在大多数环境中优势显著，凸显了在线探索的必要性。</p>
</blockquote>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>学习效率与最终性能</strong>：如表3（附录）所示，FM-IRL在最终成功率/平均回报上显著优于所有基线（除Ant-goal）。在Hand-rotate和Fetch-pick中达到接近100%的成功率。</li>
<li><strong>泛化能力</strong>：在Hand-rotate环境中向初始状态和目标状态添加不同尺度噪声进行测试。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09222v2/x4.png" alt="泛化研究"></p>
<blockquote>
<p><strong>图4</strong>：Hand-rotate环境中不同噪声水平下的方法性能。随着噪声增加，所有方法性能下降，但FM-IRL在所有噪声水平上均优于基线。这证明了在线交互对于处理未见状态的重要性，以及FM在建模多模态分布以应对多样化场景中的关键作用。</p>
</blockquote>
<ol start="3">
<li><strong>对次优专家数据的鲁棒性</strong>：在Walker2d环境中使用不同质量（由episode return衡量）的专家数据进行训练。</li>
</ol>
<blockquote>
<p><strong>表1</strong>：在Walker2d环境中使用次优专家数据的鲁棒性结果。当专家数据质量极差（ID:1）时，DP/FP略优于FM-IRL。一旦专家数据质量超过新手水平（ID:2-5），FM-IRL性能显著超越DP/FP，甚至能达到超越专家的性能（ID:2,3）。这表明FM-IRL对次优数据更具鲁棒性。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过消融实验验证了FM增强的奖励模型（FM-reward）和FM策略正则化（FM-regularizer）两个组件的贡献。结果显示，两者共同作用时性能最佳。移除正则化组件会导致训练不稳定和性能下降；而使用传统MLP判别器替代FM增强判别器，则会导致性能显著降低，尤其是在复杂任务中，这证实了FM在建模复杂专家分布方面的优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了FM-IRL，这是首个将逆强化学习与流匹配无缝结合，赋予FM策略在线探索能力的框架。</li>
<li>设计了一种创新的师生架构，利用FM增强的判别器和正则化器，解决了FM策略在线应用的关键挑战，并提升了部署时的推理效率。</li>
<li>实证表明，相较于FM策略，该方法显著提升了所学策略的泛化能力以及对次优专家数据的鲁棒性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当专家数据质量极低（如随机策略水平）时，FM-IRL难以推断出有意义的奖励模型，导致在线交互收益甚微，此时性能可能不如简单的行为克隆方法。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>架构设计</strong>：师生架构为整合复杂生成模型与高效在线RL提供了一种通用范式，可扩展至其他生成模型（如扩散模型）。</li>
<li><strong>奖励建模</strong>：利用生成模型的训练损失或内部特征作为分布相似性度量，是一种新颖且强大的奖励塑造思路，值得在其他IRL或奖励学习场景中探索。</li>
<li><strong>平衡探索与利用</strong>：FM-IRL通过奖励模型鼓励“像专家一样探索”，通过正则化防止“偏离专家太远”，这种双重机制为在模仿学习中平衡探索与利用提供了新思路。未来可研究更动态或自适应的平衡策略。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出FM-IRL方法，旨在解决基于流匹配（FM）的策略在离线模仿学习中因缺乏环境交互和探索而泛化能力差的问题。核心方法采用“学生-教师”框架：利用一个结构简单的MLP“学生”策略进行在线RL探索和更新；同时，关联一个FM“教师”模型来构建奖励函数，并以此正则化学生策略的行为。实验表明，该方法显著提升了从专家数据（尤其是次优数据）中学习时的效率、泛化性与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.09222" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>