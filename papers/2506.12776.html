<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Native Visual Understanding: Resolving Resolution Dilemmas in Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Native Visual Understanding: Resolving Resolution Dilemmas in Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.12776" target="_blank" rel="noreferrer">2506.12776</a></span>
        <span>作者: Niu, Junbo, Zheng, Yuanhong, Miao, Ziyang, Dong, Hejun, Ge, Chunjiang, Liang, Hao, Lu, Ma, Zeng, Bohan, Zheng, Qiahao, He, Conghui, Zhang, Wentao</span>
        <span>日期: 2025/06/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大多数视觉语言模型（VLMs）依赖于固定尺寸、低分辨率的视觉编码器（如336×336像素）来处理输入图像。然而，真实世界的图像在分辨率和长宽比上具有极大的多样性，从超宽全景图到细长文档，从缩略图标到8K医学影像。这种多样性对VLMs实现人类水平的视觉感知构成了根本性挑战。尽管近期研究探索了动态分辨率视觉编码策略，如上采样、裁剪、混合编码器以及原生分辨率编码，但这些努力在开源社区中仍处于碎片化状态，缺乏系统性的训练框架。同时，现有的多模态评测基准大多仅关注图像语义内容，忽视了分辨率这一关键视觉属性，无法有效评估VLMs在不同视觉条件下的能力，特别是对极端分辨率和长宽比的适应性。这导致了模型设计与评估之间的“分辨率困境”。</p>
<p>本文旨在系统性地解决这一困境。核心思路是：1）提出一个专注于分辨率评估的新基准RC-Bench，以量化VLMs在多样视觉条件下的表现；2）开源一个模块化的原生分辨率训练框架NativeRes-LLaVA，使VLMs能够有效处理任意分辨率和长宽比的图像。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了NativeRes-LLaVA框架，其整体架构包含四个核心模块：1）支持原生分辨率的视觉编码器（ViT），配备2D旋转位置编码（RoPE）；2）用于压缩视觉令牌的压缩模块（平均池化）；3）一个两层的多层感知机（MLP）投影器；4）一个大型语言模型（LLM）。其流程是：任意分辨率的输入图像被视觉编码器动态地转换为可变数量的视觉令牌序列，经过压缩和MLP投影后，与文本指令一起输入LLM进行理解和生成。</p>
<p><img src="https://arxiv.org/html/2506.12776v1/extracted/6536845/figures/model_arch.png" alt="模型架构"></p>
<blockquote>
<p><strong>图5</strong>：NativeRes-LLaVA的整体架构。图像经过原生分辨率视觉编码器（ViT with 2D RoPE）处理，生成可变长度的视觉令牌序列，随后通过压缩模块（Pooling）和MLP投影器，最终与文本指令一起输入LLM。</p>
</blockquote>
<p><strong>核心模块一：原生分辨率视觉编码</strong>。该方法受Qwen2-VL启发，采用类似NaViT的原生编码机制。视觉编码器本身支持动态图像分辨率，并利用2D RoPE进行位置编码，从而能够灵活适应不同尺寸的图像。为了控制输入LLM的令牌数量并提高计算效率，会对相邻的2×2特征块进行平均池化压缩。例如，一张336×336的图像，经patch size为14的ViT编码后产生576个视觉令牌，再经过4倍池化压缩为144个令牌。</p>
<p><strong>核心模块二：多模态序列打包</strong>。由于原生编码产生可变长度的视觉令牌序列，若采用填充（padding）统一批次内序列长度会造成计算冗余。为此，框架引入了NaViT的“Patch n‘ Pack”策略，将批次中不同图像的视觉令牌序列直接拼接成一个长的打包序列，并采用可变长度Flash Attention技术进行计算。注意力计算时，通过累积序列长度信息来界定每个图像序列的边界，确保不同图像的注意力计算相互隔离，从而实现高效的并行处理。</p>
<p>与现有方法（上采样、裁剪、混合编码器）相比，NativeRes-LLaVA的核心创新在于其<strong>纯原生的编码策略</strong>和<strong>高效的序列打包训练框架</strong>。它避免了上采样可能引入的伪影、裁剪造成的信息丢失与几何畸变，也无需维护多个编码器，同时通过序列打包技术解决了可变长度输入带来的训练效率问题，并提供了模块化、开源的完整实现。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了广泛的评测基准。<strong>分辨率中心型（RC）</strong>基准包括TextVQA、OCRBench、DocVQA、ChartQA、InfographicVQA、HR-Bench和MM-Vet，这些任务对图像细节敏感。<strong>语义中心型（SC）</strong>基准包括AI2D、MathVista、SEED-Bench-Image、MME、MMBench和POPE，这些任务对分辨率变化相对鲁棒。此外，还包括新提出的<strong>RC-Bench</strong>。对比的基线涵盖了当前主流VLMs，如LLaVA-NeXT、InternVL、Qwen2.5-VL、DeepSeek-VL2、Kimi-VL等。训练分为两个阶段：阶段一在LLaVA-Pretrain数据集上预训练视觉投影器；阶段二分别在LLaVA-mix665k或LLaVA-NeXT-Data数据集上进行视觉指令微调。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>在通用RC基准上的表现</strong>：当使用缺乏RC任务数据的LLaVA-mix665k微调时，NativeRes-LLaVA在OCRBench上达到81.0%的准确率，显著优于同设置下的LLaVA-NeXT（72.3%）。当使用包含更多OCR数据的LLaVA-NeXT-Data微调后，NativeRes-LLaVA在多个RC基准上达到或接近SOTA水平，例如在TextVQA上达78.6%，在DocVQA上达88.8%。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.12776v1/extracted/6536845/resolution_sensitivity_chart.png" alt="分辨率敏感性对比"></p>
<blockquote>
<p><strong>图3</strong>：随着输入分辨率提升，NativeRes-LLaVA在RC型基准上的性能提升显著，而在SC型基准上几乎不变。这验证了不同任务对分辨率敏感度的差异，以及原生分辨率支持对细节敏感任务的必要性。</p>
</blockquote>
<ol start="2">
<li><strong>在RC-Bench上的表现</strong>：这是本文提出的核心评估基准。NativeRes-LLaVA在该基准上表现优异。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.12776v1/extracted/6536845/models/NativeResLLaVA-AnyRes.png" alt="RC-Bench结果"></p>
<blockquote>
<p><strong>图6</strong>：在RC-Bench上，NativeRes-LLaVA（AnyRes）以69.3%的ANLS得分领先，显著优于固定分辨率（51.2%）和裁剪策略（58.5%），证明了原生分辨率编码在处理多样视觉数据时的优势。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验与策略对比</strong>：论文系统比较了不同视觉编码策略（固定分辨率、裁剪、上采样、原生分辨率）在RC-Bench上的表现。结果显示，随着图像面积增大，固定分辨率方法性能急剧下降，裁剪方法次之，而上采样和原生分辨率方法表现更好且更稳定。其中，原生分辨率方法综合表现最佳。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.12776v1/extracted/6536845/models/4_4096_728_v2.png" alt="不同面积区间表现"></p>
<blockquote>
<p><strong>图7</strong>：在RC-Bench不同图像面积区间上的性能对比。原生分辨率（NativeRes）方法在各个面积区间，尤其是大面积（D-G）区间，表现最为稳定和优异。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.12776v1/extracted/6536845/models/728_728_crop_native_delta.png" alt="不同长宽比区间表现"></p>
<blockquote>
<p><strong>图8</strong>：在RC-Bench不同长宽比区间上的性能对比。原生分辨率方法在极端长宽比（如非常宽BW、非常高BH）的图像上优势最为明显，而裁剪（Crop）方法在这些情况下性能下降严重。</p>
</blockquote>
<ol start="4">
<li><strong>计算效率分析</strong>：虽然原生分辨率编码在性能上占优，但论文也指出其训练和推理的计算成本（FLOPs）通常高于固定分辨率方法，因为处理的视觉令牌数量更多。不过，通过序列打包技术，可以在训练时更高效地利用计算资源。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.12776v1/x4.png" alt="计算量对比"></p>
<blockquote>
<p><strong>图9</strong>：不同方法在训练时的FLOPs对比。原生分辨率方法（NativeRes）由于处理更多视觉令牌，计算量通常更大，但其带来的性能提升是显著的。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文核心贡献</strong>：1）系统性地揭示并定义了VLMs领域的“分辨率困境”，即模型动态编码能力与评估基准缺失之间的矛盾。2）提出了首个系统评估极端视觉条件下VLM能力的基准<strong>RC-Bench</strong>，其平衡了分辨率与长宽比分布，并聚焦于分析图像属性对模型响应的影响。3）设计并开源了模块化的<strong>NativeRes-LLaVA</strong>训练框架，实现了高效的原生分辨率视觉编码，并通过实验验证了其在细节敏感任务上的优越性，推动了该方向的开源生态发展。</p>
<p><strong>局限性</strong>：论文提到，原生分辨率编码由于处理更多视觉令牌，会导致更高的训练和推理计算成本。此外，当前的序列打包策略主要针对单图像批次，对多图像对话场景的支持尚不完善。</p>
<p><strong>对后续研究的启示</strong>：本文的工作为动态分辨率视觉编码的研究提供了系统的评估工具和可复现的训练框架。未来的工作可以沿着以下方向探索：1）开发更高效的原生分辨率编码架构，以降低计算开销；2）将序列打包等技术扩展至更复杂的多轮、多图像交互场景；3）探索如何自适应地选择最佳分辨率处理策略，以在性能与效率之间取得平衡。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型(VLMs)处理多样化分辨率与宽高比图像时面临的“分辨率困境”，提出系统解决方案。核心贡献包括：1) 构建RC-Bench基准，专门评估模型在极端视觉条件（尤其是分辨率与宽高比变化）下的能力；2) 提出开源训练框架NativeRes-LLaVA，使VLMs能够直接以图像原生分辨率与宽高比进行视觉编码。实验表明，原生分辨率视觉编码显著提升了模型在RC-Bench及其他分辨率相关基准上的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.12776" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>