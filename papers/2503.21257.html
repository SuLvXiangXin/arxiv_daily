<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.21257" target="_blank" rel="noreferrer">2503.21257</a></span>
        <span>作者: Wang, Yongxu, Yi, Weiyun, Kong, Xinhao, Li, Wanting</span>
        <span>日期: 2025/03/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用大规模人类数据对人形机器人进行高级模仿学习已成为研究热点。主流方法包括真实机器人上的模仿学习，例如Mobile ALOHA提出的动作分块Transformer（ACT）和HumanPlus提出的HIT算法。然而，这些方法在实际部署中面临关键瓶颈：以自我为中心的视觉作为主要感官输入，易受动态背景杂乱和目标物体变化的影响，导致特征提取不一致和感知域偏移；训练仿真与现实环境之间的物理差异会引入动作执行误差；传统的归一化技术（如批归一化）难以适应实时的新条件，导致在线操作时性能下降。这些感知鲁棒性、物理一致性和动态适应性方面的交织挑战，共同限制了人形机器人模仿学习框架的可扩展性和可靠性。</p>
<p>本文针对模仿学习中的协变量偏移问题，提出了一种名为OminiAdapt的多模态自适应模仿学习框架。其核心思路是：通过视觉预处理过滤背景干扰，利用动态自适应归一化缓解任务间特征分布偏移，并引入注意力机制增强对交互关键区域的关注，从而提升策略的鲁棒性和环境适应性。</p>
<h2 id="方法详解">方法详解</h2>
<p>OminiAdapt的整体框架是一个基于模仿学习的机器人策略架构，旨在处理环境高敏感性和新任务再训练成本高的问题。系统默认使用N=3个摄像头（位于头部、胸部、手腕等位置）以30Hz频率捕获同步图像流。当机械臂运动能量超过阈值时触发有效帧采集。给定当前时刻t的不同RGB相机视图和机器人本体感知状态，OminiAdapt预测接下来T步的机械臂和手部轨迹。</p>
<p><img src="https://arxiv.org/html/2503.21257v1/x2.png" alt="OminiAdapt概述"></p>
<blockquote>
<p><strong>图2</strong>：OminiAdapt概述。来自N个视角的首帧经由基于VLM和GroundingSAM的任务解释器模块处理，为每个视角生成初始化查询帧以启动跟踪算法。随后，所有视角的RGB视频流进行连续目标跟踪，对关键元素进行掩码以过滤与任务无关的背景信息。然后，使用带有动态自适应批归一化层的半冻结骨干网络提取图像特征，并通过通道-空间注意力模块增强特征。增强后的特征与嵌入的机器人本体感知状态一同输入Transformer解码器，以预测未来T步的机器人动作，并应用轨迹平滑以改善运动一致性。</p>
</blockquote>
<p>流程包含四个核心阶段：</p>
<ol>
<li><p><strong>视觉预处理与连续目标跟踪</strong>：首先，所有视角的首帧图像通过任务解释器模块和GroundingSAM进行零样本语义分割以初始化目标定位。后续帧则通过增强的CUTIE跟踪器进行连续目标级跟踪和分割，生成像素级掩码以过滤背景。此过程还包含形态学膨胀以增强遮挡鲁棒性，并施加视图间一致性约束确保多视角掩码对齐。<br><img src="https://arxiv.org/html/2503.21257v1/x3.png" alt="任务解释器模块"></p>
<blockquote>
<p><strong>图3</strong>：任务解释器模块。多模态大模型的输入包括来自不同任务场景的相机捕获图像和相应的任务描述。模型从不同视角识别出机器人最需要关注的图像中的物体或元素，并生成基于文本的提示。</p>
</blockquote>
</li>
<li><p><strong>动态自适应批归一化特征提取</strong>：掩码图像由一个预训练的多视角共享权重的ResNet18骨干网络处理，其中卷积权重被冻结，仅批归一化层在任务特定训练期间被激活并进行联合微调，这种设计称为动态自适应批归一化。每个任务策略维护自己独立的一组BN参数，使得骨干网络能够在共享卷积层参数的同时，通过调整BN参数来快速适应新任务，减少训练时间和资源消耗。</p>
</li>
<li><p><strong>多维注意力机制增强</strong>：从不同视角提取的特征通过通道-空间注意力模块增强。该方法采用CBAM模块，其通道注意力机制通过双路池化计算通道权重，抑制无关特征通道和高频噪声；空间注意力机制通过卷积层生成热力图，使策略能更好地学习手与任务对象之间的空间关系，专注于交互区域的细节。</p>
</li>
<li><p><strong>Transformer解码与动作预测</strong>：增强后的多视角特征与机器人本体感知状态的嵌入进行拼接，输入一个Transformer解码器，以预测未来T步的动作序列。采用分块预测以保证时间连贯性。损失函数包含轨迹精度项和惩罚速度不连续性的平滑项，并引入指数衰减因子实现课程学习。</p>
</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1) <strong>动态手-物分割</strong>：利用视觉基础模型和连续跟踪实时生成掩码，主动过滤动态背景干扰；2) <strong>动态自适应批归一化</strong>：通过任务特定的BN参数微调，高效缓解跨任务特征分布偏移，提升策略可迁移性；3) <strong>多维注意力增强</strong>：结合通道与空间注意力，强化对关键交互特征的鲁棒表示。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在一个双臂移动机器人平台上进行，该平台配备Realman RM75-6F机械臂、Inspire RH56DFX灵巧手和LinkerHand L10灵巧手，感知系统包含三个Intel RealSense D435i相机。实验在四个精细操作任务场景上进行：叠衣服、摘苹果、插花和倒水。每个场景的训练数据量分别为40、50、60、60条演示。</p>
<p><img src="https://arxiv.org/html/2503.21257v1/x4.png" alt="机器人硬件配置"></p>
<blockquote>
<p><strong>图4</strong>：机器人硬件配置。右上角为完整系统，配备双Realman RM75-6F机械臂、Inspire RH56DFX灵巧手和LinkerHand L10灵巧手。感知系统包含三个安装在云迹Water2移动平台上的Intel RealSense D435i相机。</p>
</blockquote>
<p><strong>对比实验</strong>：将OminiAdapt与HIT和ACT算法进行对比。如表II所示，OminiAdapt在所有任务上均取得最佳成功率。<br><img src="https://arxiv.org/html/2503.21257v1/x5.png" alt="对比实验结果"></p>
<blockquote>
<p><strong>图5</strong>：不同方法在细粒度任务中的实验结果对比。OminiAdapt在叠衣服、摘苹果、插花、倒水任务上的成功率分别为100%、90%、65%、60%，显著优于HIT和ACT。</p>
</blockquote>
<p>具体而言，在摘苹果任务中，OminiAdapt成功率高达90%，而HIT和ACT均为35%；在插花任务中，OminiAdapt为65%，优于HIT的25%和ACT的20%；在叠衣服任务中达到100%；在倒水任务中达到60%，远高于HIT的20%和ACT的10%。这证明了该方法在处理复杂、精细操作任务上的有效性和鲁棒性。</p>
<p><strong>消融实验</strong>：研究各核心组件的贡献。移除连续目标跟踪模块导致性能大幅下降，尤其是在背景复杂的摘苹果和插花任务中，突显了过滤背景干扰的重要性。移除动态自适应批归一化模块后，模型适应新任务的速度变慢，跨任务性能下降。移除注意力机制模块则降低了模型对交互区域细节的关注能力，在需要精确手眼协调的任务中成功率降低。<br><img src="https://arxiv.org/html/2503.21257v1/x6.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图6</strong>：消融实验结果。展示了移除连续目标跟踪、动态自适应批归一化、注意力机制等核心组件后，模型在不同任务上的性能下降情况，验证了各模块的有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了一种结合动态手-物分割的视觉预处理流程，有效抑制了模仿学习中的背景干扰；2）设计了动态自适应批归一化模块，通过微调BN层参数实现跨任务特征的快速适应，提升了策略的可迁移性；3）引入了多维注意力机制，增强了模型对关键手-物交互特征的鲁棒表示能力。</p>
<p>论文自身提到的局限性在于，未来工作计划充分利用LinkerHand灵巧手上的触觉传感器数据，这暗示了当前方法尚未完全整合多模态传感信息（如触觉），可能存在对纯视觉依赖过强的问题。</p>
<p>本文对后续研究的启示在于：首先，将基础模型（VLM，SAM）与在线跟踪技术结合，为机器人提供了一种自动化、鲁棒的感知预处理范式。其次，动态自适应归一化的思路为在共享骨干网络上高效学习多任务策略提供了可行路径。最后，注意力机制与特定任务特征增强的结合，指明了提升模仿学习策略在复杂环境中泛化能力的一个关键方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出OminiAdapt算法，旨在解决人形机器人模仿学习中因形态差异、感知复杂性和自中心视觉特征不足导致的协变量偏移问题。该方法通过聚焦主要任务目标、过滤背景干扰，结合通道特征融合与空间注意力机制，并采用动态权重更新策略，以增强环境抗干扰能力。实验表明，该方法在不同任务场景中均表现出鲁棒性和可扩展性，显著提升了人形机器人完成目标任务的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.21257" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>