<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VGGT-DP: Generalizable Robot Control via Vision Foundation Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VGGT-DP: Generalizable Robot Control via Vision Foundation Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.18778" target="_blank" rel="noreferrer">2509.18778</a></span>
        <span>作者: Zhi Wang Team</span>
        <span>日期: 2025-09-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前视觉模仿学习框架主要聚焦于策略设计，但往往忽视了视觉编码器的结构和容量，这限制了机器人对空间关系的理解和泛化能力。近期研究趋势是引入大型语言模型，构建视觉-语言-动作模型，以期利用语言先验提升泛化。然而，本文观察到许多具备高级操控能力的生物（如昆虫）并不依赖语言，其能力更根植于丰富、广泛的视觉和本体感觉处理。这揭示了当前机器人视觉编码器的一个关键痛点：它们通常为了计算效率而采用容量不足的简单架构，难以捕捉复杂空间几何关系，导致环境微小变化即引发性能显著下降。</p>
<p>本文针对视觉编码器容量不足、空间理解薄弱的痛点，提出了一个无需语言、强调几何感知与本体感觉反馈的新视角。其核心思路是将一个在大规模3D重建任务上预训练的视觉基础模型作为强大的几何先验，与扩散策略相结合，以增强机器人操控的空间感知和动作规划能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>VGGT-DP的整体框架旨在从多视角视觉观察和历史本体感觉中，预测未来一系列机器人动作。其输入是时间窗口内的视觉图像序列和本体感觉信号，输出是预测的动作序列。框架包含三个核心组件：1）用于提取几何感知特征的VGGT编码器与令牌剪枝；2）用于高效推理的帧级令牌重用机制；3）用于多步动作预测的本体感觉引导扩散策略。</p>
<p><img src="https://arxiv.org/html/2509.18778v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：VGGT-DP架构图。系统包含三个主要部分：使用令牌剪枝的VGGT编码器用于提取几何感知特征；帧级令牌重用机制用于高效推理；以及用于多步动作预测的本体感觉引导扩散策略。</p>
</blockquote>
<p><strong>VGGT作为特征投影器</strong>：本文采用预训练的视觉几何基础Transformer作为视觉编码器。其关键不是使用低层视觉输出（如深度图），而是利用其聚合器生成的聚合令牌，这些令牌是3D场景紧凑且语义丰富的表示。具体流程为：给定包含T帧、V个相机视角的观测序列，VGGT聚合器生成形状为 <code>[B·T, V, (N_p+1), D]</code> 的视觉令牌，其中包含每个视角的N_p个图像块令牌和1个相机令牌。这些令牌随后通过一个Transformer编码器进一步处理，再经过平均池化得到一个池化特征，最后通过一个MLP投影为适合扩散策略的条件嵌入。</p>
<p><strong>帧级令牌重用</strong>：为避免在重叠的观测窗口中进行冗余计算以降低大模型推理延迟，本文设计了帧级令牌重用机制。在时间步t，系统缓存前T-1帧已计算出的VGGT令牌，仅对最新的第t帧图像计算VGGT特征，然后将缓存令牌与新令牌拼接，构成完整的当前观测令牌序列。</p>
<p><strong>随机令牌剪枝</strong>：在将VGGT令牌送入Transformer编码器前，以概率 <code>r_prune</code> 随机丢弃一部分图像块令牌。这一策略引入了令牌级别的随机性，鼓励模型学习对部分观测丢弃不变的鲁棒表示，同时也能作为轻量级策略提升推理速度。</p>
<p><strong>本体感觉引导的视觉学习</strong>：为了促使视觉编码器学习与机器人内部状态在语义和几何上对齐的特征，本文引入了一个辅助监督模块。该模块设计了一个解码器网络，用于从视觉编码器提取的特征中预测机器人本体感觉状态（如关节角度、末端执行器位置）。训练时，在总损失中加入预测本体感觉的均方误差重构损失，以此作为辅助信号来引导视觉特征学习，从而提升策略在操控任务中的泛化能力。</p>
<p>本文的核心创新点在于：1）首次将在大规模3D重建任务上预训练的大型视觉基础模型（VGGT）作为编码器集成到机器人扩散策略中，提供了强大的几何先验；2）设计了帧级令牌重用和随机令牌剪枝机制，在保持高性能的同时，显著提升了使用大视觉模型时的推理效率与模型鲁棒性；3）提出了本体感觉引导的视觉学习方法，通过多模态对齐增强了闭环反馈控制能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在MetaWorld仿真基准上进行，选取了基线方法（DP和DP3）表现最差的8个挑战性任务，外加2个简单任务（Reach和Peg Unplug Side）进行评估，共计10个任务。所有方法均使用由脚本策略生成的10条专家演示进行训练，并在每个任务上运行20个测试回合计算平均成功率。</p>
<p><img src="https://arxiv.org/html/2509.18778v1/imgs/taskvis.png" alt="任务可视化"></p>
<blockquote>
<p><strong>图3</strong>：评估所用的十个MetaWorld任务可视化，包括Disassemble, Peg Unplug Side等复杂和简单任务。</p>
</blockquote>
<p><strong>关键定量结果</strong>：如表1所示，VGGT-DP在10个任务上的平均成功率达到36.6%，显著高于DP的19.1%和DP3的28.6%。具体而言，在Pick out of Hole、Sweep Into、Stick Pull等需要精确空间推理和动态适应的复杂任务上，VGGT-DP提升尤为显著（例如Pick out of Hole从14%提升至55%）。然而，在Peg Unplug Side等简单任务上，VGGT-DP并未超越基线，作者归因于任务简单导致的过参数化及优化开销。在Shelf Place和Pick Place两个放置类任务上，VGGT-DP表现接近零，作者认为小物体、细长物体或遮挡增加了精确定位难度，现有几何感知仍不足。</p>
<p><strong>效率分析</strong>：图4展示了帧级令牌重用机制对推理延迟的优化效果。在观测步长和批次规模较大时，启用FTR能显著降低延迟，因为它避免了历史帧特征的重复计算。<br><img src="https://arxiv.org/html/2509.18778v1/imgs/performance_overlap_comparison.png" alt="推理延迟对比"></p>
<blockquote>
<p><strong>图4</strong>：不同批次大小和观测步长下，VGGT编码器带（蓝色）与不带（红色）帧级令牌重用机制的推理延迟对比。FTR机制在大时空维度下能显著减少延迟。</p>
</blockquote>
<p><strong>视角扰动鲁棒性</strong>：图5展示了在Stick Pull任务中，对相机姿态施加随机旋转扰动时的性能变化。即使仅有5度的微小扰动，成功率也从39%骤降至5%，在10度及以上扰动时完全失败。这表明，尽管使用了预训练的3D感知模型，VGGT-DP对训练分布外的视角变化仍然非常敏感，其几何先验可能过度依赖一致的多视角对齐。<br><img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0MDAiIGhlaWdodD0iMzAwIj48dGV4dCB4PSIyMDAiIHk9IjE1MCIgZm9udC1mYW1pbHk9IkFyaWFsIiBmb250LXNpemU9IjE2IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIj5GaWd1cmUgNTogVmlld3BvaW50IFJvYnVzdG5lc3Mgb24gU3RpY2tQdWxsIFRhc2suPC90ZXh0Pjwvc3ZnPg0K" alt="视角鲁棒性"></p>
<blockquote>
<p><strong>图5</strong>：在StickPull任务上，视角旋转扰动（度数）对成功率的影响。性能随扰动增大而急剧下降，揭示了方法对视角变化的脆弱性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了VGGT-DP框架，首次将基于3D重建预训练的大型视觉基础模型作为编码器引入扩散策略，显著提升了策略在复杂空间任务上的性能；2）设计了帧级令牌重用和随机令牌剪枝机制，有效平衡了大模型带来的计算开销与模型鲁棒性；3）引入了本体感觉引导的视觉学习方法，通过多模态对齐增强了特征的几何基础。</p>
<p>论文自身指出的局限性包括：在简单任务上可能因模型过参数化而导致优化不稳定甚至性能下降；对训练数据外的相机视角扰动非常敏感，鲁棒性不足；在涉及细小、遮挡物体的精细放置任务上仍然失败。</p>
<p>这项工作对后续研究的启示是：大型、专用于几何理解的视觉基础模型是提升机器人空间感知与泛化能力的一个有前景的方向。然而，如何使这类模型对现实世界中不可避免的视角、光照等变化更具鲁棒性，是亟待解决的问题，可能需要结合视角等变表示或领域随机化数据增强等策略。此外，在简单任务上平衡模型容量与效率，以及解决精细操作中的感知瓶颈，也是未来的重要研究方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>这篇论文针对视觉模仿学习中视觉编码器结构限制空间理解与泛化能力的问题，提出VGGT-DP框架。其关键技术包括：采用VGGT视觉编码器整合预训练3D模型的几何先验；引入本体感受反馈引导的视觉学习策略，以对齐感知与机器人内部状态；设计了帧级令牌重用机制和随机令牌剪枝，以提升推理效率与策略鲁棒性。在MetaWorld任务上的实验表明，该框架显著优于DP、DP3等基线方法，尤其在精密操作和长时程任务中表现突出。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.18778" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>