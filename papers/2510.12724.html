<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.12724" target="_blank" rel="noreferrer">2510.12724</a></span>
        <span>作者: Fei, Xin, Xu, Zhixuan, Fang, Huaicong, Zhang, Tianrui, Shao, Lin</span>
        <span>日期: 2025/10/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧抓取因其高维状态和动作空间的复杂性，一直是机器人领域的核心挑战。现有的基于学习的研究主要探索了两种类型的表示方法：机器人中心式和物体中心式。机器人中心式方法（如用手腕位姿或关节值表示抓取）虽然能通过将观测直接映射到控制信号实现快速推理，但样本效率低、可迁移性差，学习到的表示与特定手部构型隐性绑定，难以泛化到新的形态。物体中心式范式则侧重于用物体特征（如接触点或可供性）描述抓取，这自然促进了跨不同物体和机器人平台的迁移性，但通常在部分观测下很脆弱，并且需要额外的优化阶段（如指尖逆运动学或接触图的约束拟合）来将抽象预测转化为可行的关节配置，使得过程计算昂贵且耗时。</p>
<p>近期，𝒟(ℛ,𝒪) Grasp 引入了用于灵巧抓取的交互中心式表示。与前述两种范式不同，交互中心式表示通过直接建模机器人手与物体之间的空间关系来描述抓取，将两者嵌入一个共享的交互空间。这种形式结合了机器人中心式和物体中心式范式的优点：既保留了对手部构型的感知，又捕获了可迁移的物体级特征。然而，𝒟(ℛ,𝒪) 中构建的手和物体采样点之间的点对点距离矩阵需要大量的GPU内存，因此该表示资源密集且难以扩展。此外，𝒟(ℛ,𝒪) 依赖于一个合适的初始状态作为输入，这使得模型性能容易受到不可行初始化的影响。</p>
<p>为了克服 𝒟(ℛ,𝒪) 表示的局限性，本文提出了 𝒯(ℛ,𝒪) Grasp，这是一个基于 𝒯(ℛ,𝒪) Graph 表示的高效图扩散模型，用于跨构型的灵巧抓取合成。其核心思路是：提出一个统一的 𝒯(ℛ,𝒪) Graph 来表示手部连杆与物体之间的空间变换关系，并基于此图构建一个高效的扩散模型来生成抓取，显著降低了内存消耗并提升推理速度。</p>
<h2 id="方法详解">方法详解</h2>
<p>给定物体点云 𝑃𝑂、灵巧手的URDF描述以及每个连杆局部坐标系中的点云 {𝑃𝑖𝑅}，𝒯(ℛ,𝒪) Grasp 通过首先预测所有连杆的 SE(3) 变换（暂时放松关节约束）来生成多样化的抓取。然后，通过基于 Pyroki 的逆运动学将预测的变换进行细化并转换为可执行的关节值，同时强制执行关节约束以确保可行性。</p>
<p><img src="https://arxiv.org/html/2510.12724v1/fig/pipeline.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：𝒯(ℛ,𝒪) Grasp 方法整体框架。首先定义 𝒯(ℛ,𝒪) Graph 来表示机器人连杆和物体的空间变换及辅助几何信息。接着，引入一个图扩散模型，支持无条件（从随机噪声开始）和条件（给定初始手部状态）的抓取合成。最后通过高效的逆运动学求解器将预测的连杆位姿转换为关节角度。</p>
</blockquote>
<p>整体框架的核心是 <strong>𝒯(ℛ,𝒪) Graph</strong> 及其上的扩散过程。该图是一个统一表示，节点编码物体和连杆的几何属性及其在物体全局坐标系下的空间变换，边则由节点间的相对空间变换构成。由于 SE(3) 是非线性流形，本文采用其李代数表示（与 ℝ6 同构），通过指数和对数映射在 SE(3) 李群和向量空间之间转换，以便于扩散过程处理。</p>
<p><strong>图构建的具体细节如下：</strong></p>
<ul>
<li><strong>物体节点</strong>：将物体点云分割成 𝑃 个块，使用为 3D 断裂装配预训练的 VQ-VAE 编码器提取每个块的几何令牌 𝑓𝑖𝑂。物体节点由块中心位置 𝑐𝑖𝑂、全局物体尺度 𝑠𝑂 和对应的几何令牌拼接而成。</li>
<li><strong>连杆节点</strong>：使用 Basis Point Set (BPS) 算法处理每个连杆的点云，生成固定长度、对输入点数不变的几何特征。一个 MLP 编码器将 BPS 特征与连杆中心 𝑐𝑖𝑅 和尺度 𝑠𝑖𝑅 嵌入为几何嵌入 𝑏𝑖。连杆节点则由该几何嵌入与表示该状态下连杆相对于物体坐标系位姿的 6D 旋转向量 𝜓𝑖𝑅 拼接而成。</li>
<li><strong>边</strong>：仅在物体节点和连杆节点之间以及连杆节点之间定义边，并建立完全连接以施加更强的空间变换约束。物体-连杆边和连杆-连杆边分别计算为对应节点变换矩阵之间的相对变换（取对数后得到 ℝ6 向量）。</li>
</ul>
<p><strong>图扩散模型</strong> 采用 Denoising Diffusion Implicit Model (DDIM) 以加速采样并支持条件生成。在正向过程中，仅对连杆位姿 Ψ𝑅 添加高斯噪声，然后根据噪声节点重新计算边以保持空间一致性。去噪模型 ϵ𝜃(𝒢𝑡,𝑡) 是一个基于 Transformer 的图网络，它将带噪声的图 𝒢𝑡 和时间戳 𝑡 作为输入，预测连杆节点上的噪声。该网络使用 Object-Robot (OR) 和 Robot-Robot (RR) 两种注意力机制迭代更新连杆节点和所有边：OR 注意力将物体几何和空间信息整合到连杆更新中，RR 注意力则让模型能够推理连杆间的空间关系。</p>
<p><strong>反向（生成）过程</strong> 遵循 DDIM 策略，从随机噪声图开始，通过学习的过渡 𝑝𝜃(𝐺𝑡𝑖−1|𝐺𝑡𝑖) 逐步去噪，最终得到表示有效抓取的干净图。对于<strong>条件推理</strong>（例如给定初始手部状态），首先将初始位姿加噪至某个中间时间步 𝑡⋆，然后在去噪过程中使用基于 SO(3) 上测地距离梯度的引导，使生成的手掌朝向朝向初始手掌朝向。</p>
<p><strong>逆运动学优化</strong>：获得干净的连杆位姿后，将其转换为关节值。本文使用高效的 Pyroki 工具包求解一个优化问题，即寻找满足关节限位的关节值 𝒒，使得通过正向运动学计算出的连杆位姿尽可能接近预测的位姿。</p>
<p><strong>与现有方法相比的创新点</strong>：</p>
<ol>
<li><strong>表示层面</strong>：提出了 𝒯(ℛ,𝒪) Graph，用“连杆-物体块”间的变换关系替代 𝒟(ℛ,𝒪) 的“点-点”距离矩阵，大幅降低了内存消耗。</li>
<li><strong>生成过程</strong>：基于 DDIM 的图扩散模型直接生成 SE(3) 位姿，无需依赖可能次优的初始状态（支持无条件生成），并通过梯度引导灵活支持条件生成。</li>
<li><strong>求解效率</strong>：集成了 GPU 加速的 Pyroki IK 求解器，实现了从位姿到关节值的快速转换。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了 YCB 物体集和 GraspNet-1Billion 数据集，在 Allegro、Barrett 和 Shadowhand 三种灵巧手上进行。对比的基线方法包括 𝒟(ℛ,𝒪) Grasp、使用 CVAE 的机器人中心式方法、以及物体中心式方法（Contact-GraspNet 和 GraspME）。评估指标包括抓取成功率、推理时间、吞吐量、内存消耗和生成多样性。</p>
<p><img src="https://arxiv.org/html/2510.12724v1/x1.png" alt="主要结果对比"></p>
<blockquote>
<p><strong>图2</strong>：在 Allegro 手上与基线方法的抓取成功率对比。𝒯(ℛ,𝒪) Grasp 在 YCB 和 GraspNet-1Billion 数据集上均取得最高成功率（分别超过最佳基线 4.79% 和 6.67%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12724v1/x2.png" alt="效率对比"></p>
<blockquote>
<p><strong>图3</strong>：推理效率对比。𝒯(ℛ,𝒪) Grasp 在 NVIDIA A100 GPU 上达到 0.21 秒的平均推理速度和每秒 41 个抓取的吞吐量，显著快于 𝒟(ℛ,𝒪) Grasp，且内存消耗降低超过 50%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12724v1/x3.png" alt="跨构型泛化"></p>
<blockquote>
<p><strong>图4</strong>：跨手部构型的零样本泛化能力。在训练未见的 Barrett 和 Shadowhand 上评估，𝒯(ℛ,𝒪) Grasp 仍能保持高成功率，显著优于 𝒟(ℛ,𝒪) Grasp。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12724v1/x4.png" alt="条件生成与多样性"></p>
<blockquote>
<p><strong>图5</strong>：条件抓取合成与生成多样性展示。左：给定初始手掌位姿（蓝色）作为条件，模型能生成符合该条件的稳定抓取（绿色）。右：对同一物体，从不同噪声采样能生成多种多样的抓取姿态。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ul>
<li><strong>成功率</strong>：在 Allegro 手上的平均抓取成功率达到 **94.83%**，在 YCB 和 GraspNet-1Billion 数据集上分别超过最佳基线 4.79% 和 6.67%。</li>
<li><strong>推理速度与吞吐量</strong>：平均推理时间为 <strong>0.21 秒</strong>，吞吐量达 <strong>41 grasps/s</strong>，比 𝒟(ℛ,𝒪) Grasp 快一个数量级。</li>
<li><strong>内存效率</strong>：训练内存消耗比 𝒟(ℛ,𝒪) Grasp 减少 <strong>50% 以上</strong>。</li>
<li><strong>跨构型泛化</strong>：在训练未见的 Barrett 和 Shadowhand 手上进行零样本评估，成功率分别达到 89.70% 和 88.43%，显著优于基线。</li>
<li><strong>真实世界与闭环操控</strong>：真实世界实验成功率为 83.3%。高效推理使其能够支持<strong>闭环灵巧操作</strong>，在物体被意外推动后能快速重新规划抓取。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.12724v1/x5.png" alt="消融实验：图组件"></p>
<blockquote>
<p><strong>图6</strong>：𝒯(ℛ,𝒪) Graph 中不同组件的消融研究。移除物体几何特征 (w/o Obj. Geom.) 或连杆几何特征 (w/o Link Geom.) 会导致性能下降，同时包含两者效果最佳。使用 BPS 编码连杆几何优于使用 PointNet。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12724v1/x6.png" alt="消融实验：注意力机制"></p>
<blockquote>
<p><strong>图7</strong>：去噪模型中注意力机制的消融研究。同时使用 OR 和 RR 注意力能获得最佳性能，缺少任一种都会降低成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12724v1/x7.png" alt="消融实验：条件引导"></p>
<blockquote>
<p><strong>图8</strong>：条件生成中不同引导策略的消融研究。使用测地距离梯度引导 (Ours (Geodesic)) 在保持抓取质量的同时，能更好地满足朝向条件，优于简单的 L2 距离引导。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12724v1/x8.png" alt="消融实验：IK步骤"></p>
<blockquote>
<p><strong>图9</strong>：逆运动学优化中 Pyroki 迭代步数的影响。增加迭代次数能小幅提升成功率，但即使少量迭代也已能获得良好结果，体现了 Pyroki 的高效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12724v1/x9.png" alt="扩散步数影响"></p>
<blockquote>
<p><strong>图10</strong>：DDIM 采样步数 𝑀 对成功率和推理时间的影响。仅需 20 步即可达到高成功率，进一步增加步数对性能提升有限但会增加耗时，体现了在速度与质量间的良好权衡。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>图组件</strong>：物体和连杆的几何编码都是必要的，BPS 编码优于 PointNet。</li>
<li><strong>注意力机制</strong>：OR 和 RR 注意力共同作用对性能至关重要。</li>
<li><strong>条件引导</strong>：基于测地距离的梯度引导策略效果优于 L2 距离引导。</li>
<li><strong>IK 优化</strong>：Pyroki 即使少量迭代也有效，平衡了速度与精度。</li>
<li><strong>扩散步数</strong>：模型在较少的 DDIM 采样步数（如20步）下就能达到高性能，实现了高效推理。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了 𝒯(ℛ,𝒪) Graph</strong>：这是一个统一、可泛化且内存高效的表示，用于建模所有物体-手对之间的交互，为构建大规模灵巧抓取基础模型奠定了基础。</li>
<li><strong>开发了高效的图扩散模型</strong>：基于该图表示的扩散模型支持无条件和条件抓取合成，降低了对不可行初始化的敏感性，并实现了可靠的闭环灵巧操作。</li>
<li><strong>进行了全面实验验证</strong>：在仿真和真实世界中进行了广泛实验，证明 𝒯(ℛ,𝒪) Grasp 在抓取成功率上显著优于所有基线方法，同时在效率上远超现有方法。</li>
</ol>
<p><strong>论文提到的局限性</strong>包括：1) 依赖于预训练的物体几何编码器（VQ-VAE）；2) 逆运动学求解可能失败，导致潜在的抓取丢失；3) 实验主要在已知的物体模型上进行，在未知的、非结构化的现实物体上的泛化能力有待进一步验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>表示学习的价值</strong>：𝒯(ℛ,𝒪) Graph 作为一种高效的交互中心式表示，展示了其在跨构型灵巧抓取中的潜力，可能推动该领域表示学习的发展。</li>
<li><strong>基础模型方向</strong>：其高效性和泛化能力使其有潜力扩展为灵巧抓取的基础模型，在大规模数据集上训练以覆盖更广泛的物体和手型。</li>
<li><strong>通往实时操控</strong>：高达 41 grasps/s 的吞吐量证明了其实时推理能力，为动态环境下的在线重规划和闭环反馈控制打开了大门，是迈向实际部署的关键一步。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对灵巧抓取中高维状态动作空间复杂、且难以跨不同机器人手型泛化的问题，提出𝒯(ℛ,𝒪) Grasp框架。其核心是𝒯(ℛ,𝒪) Graph这一统一表示法，建模手与物体间的空间变换关系并编码几何属性；结合图扩散模型与高效逆运动学求解器，实现无条件与条件抓取合成。实验表明，该方法在多种灵巧手上平均成功率高达94.83%，推理速度达0.21秒（A100 GPU上每秒生成41个抓取），显著超越现有基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.12724" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>