<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.18323" target="_blank" rel="noreferrer">2601.18323</a></span>
        <span>作者: Mi, Weishi, Bao, Yong, Chi, Xiaowei, Ju, Xiaozhu, Qin, Zhiyuan, Ge, Kuangzhi, Tang, Kai, Jia, Peidong, Zhang, Shanghang, Tang, Jian</span>
        <span>日期: 2026/01/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人控制领域存在两种主流范式。一种是端到端的视觉-语言-动作（VLA）模型，如RT系列、OpenVLA等，它们直接将多模态输入映射为底层动作，在分布内任务上表现良好，但其成功严重依赖大规模、高质量的机器人数据，导致在分布外（OOD）任务、长时序和组合式推理方面泛化能力有限。另一种是使用生成世界模型（如Sora, Cosmos, WoW）作为高层“视觉预见”规划器，通过模拟像素空间的未来状态来指导下游控制器。然而，一个关键的“最后一英里”挑战持续存在：世界模型生成的像素级计划与物理上可执行的机器人动作之间存在错位。这种规划-动作鸿沟使得视觉规划器难以直接部署到物理硬件上。</p>
<p>本文针对上述视觉规划与底层控制之间的鸿沟，提出了一个新的视角：与其追踪不稳定或难以定义的对象状态（尤其是对于可变形物体），不如将控制策略锚定在世界模型所设想的机器人末端执行器（工具）的稳定、定义明确的运动上。本文的核心思路是：提出工具中心逆动力学模型（TC-IDM），利用世界模型生成的视频中工具的运动轨迹作为鲁棒的中间表示，来桥接高层视觉规划与底层物理控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>TC-IDM采用“计划-翻译”的两阶段流程。第一阶段，给定初始RGB帧、深度图和文本指令，利用视频生成世界模型（如WoW）生成作为高层计划的RGB视频序列，并恢复其对应的度量深度序列和相机位姿轨迹。第二阶段，TC-IDM将动作翻译解耦为两个互补的流：视觉驱动的夹爪状态生成和几何基础的末端姿态生成。</p>
<p><img src="https://arxiv.org/html/2601.18323v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：TC-IDM整体框架。给定初始观测和文本指令，世界模型生成视频计划。TC-IDM随后从生成的视频中提取语义特征（通过DINOv3）和时空几何信息（通过SAM3分割和3D运动跟踪器），并分别通过两个解耦的MLP头预测夹爪控制信号和末端执行器6-DoF位姿轨迹。</p>
</blockquote>
<p><strong>核心模块1：时空预测</strong>。世界模型基于初始观测生成RGB视频序列。随后，使用Video Depth Anything (VDA)生成初始深度图序列和相对相机位姿。通过与第一帧的真实度量深度进行最小二乘对齐，获得尺度和平移参数，将所有预测深度和位姿转换为度量坐标系，得到完全度量的相机轨迹和深度序列。</p>
<p><strong>核心模块2：解耦的动作翻译</strong>。</p>
<ul>
<li><strong>视觉驱动的状态生成</strong>：该分支负责预测1-DoF的夹爪开合控制。使用冻结的DINOv3视觉编码器从生成的RGB视频中提取具有时间一致性的语义特征序列，这些特征捕获了任务关键线索（如物体身份、接触状态）。一个轻量级的MLP头（GripperHead）基于这些语义特征预测连续的夹爪孔径参数。</li>
<li><strong>几何基础的姿态生成</strong>：该分支负责预测工具中心点（TCP）的6-DoF位姿轨迹。首先，使用SAM3为每一帧生成密集的夹爪掩码。然后，在已对齐的RGB-D输入和夹爪掩码的条件下，使用3D点跟踪器提取夹爪表面点在机器人基坐标系下的密集3D点轨迹。为了获得稳定的几何表示，算法强制执行刚体运动先验，从密集轨迹中筛选出最符合刚体假设的Top-K轨迹。最后，通过求解连续帧间筛选出的点集之间的最优刚体变换（使用最小二乘刚性对齐），直接解析地恢复出可执行的6-DoF末端执行器相对位姿动作。</li>
</ul>
<p><strong>创新点</strong>：与现有方法（如AVDC、VidBot）试图从视频中追踪不稳定的物体状态不同，TC-IDM的创新核心在于<strong>以工具轨迹为中心</strong>。它将控制策略建立在世界模型“想象”出的工具运动上，这是一个更稳定、定义更明确的中间表示。其<strong>解耦的动作头设计</strong>分别处理语义特征和几何运动，有效利用了异构信息流。这种方法天然对相机视角变化鲁棒（因为工具轨迹在世界坐标系中表示），并且能够灵活处理可变形物体操作（因为无需对布料等物体的复杂状态进行建模，只需关注工具的预期路径）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实机器人上进行评估，使用了多种操作任务，并根据所需自由度（DoF）和精度约束将这些任务分为三个难度等级：简单任务（如拾放）、中等任务（需要精确控制或简单避障）和困难任务（需要精细的6-DoF协调，如插入）。</p>
<p><strong>对比的基线方法</strong>包括：逆动力学模型（IDM）基线如ResNet-MLPs、AVDC、AnyPos、2DtrackerIDM；以及结合不同世界模型（如π0, Cosmos2, WoW）的IDM变体。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>在真实回放视频上的评估</strong>（表1）：TC-IDM在几乎所有任务上都取得了最高的成功率。具体而言，在简单任务上达到93.3%的成功率（接近动作回放的上限），在中等任务上为53.3%，在困难任务上为28.9%，显著优于其他IDM基线。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.18323v1/x3.png" alt="任务示例"></p>
<blockquote>
<p><strong>图3</strong>：按难度分类的代表性操作任务示例。从上到下分别为简单、中等、困难任务，为评估模型在不同空间和时间复杂度下的性能提供了结构化基准。</p>
</blockquote>
<p>表1和表2展示了详细的定量比较结果，TC-IDM在多数任务上领先。<br>2.  <strong>跨世界模型的评估</strong>（表2）：当与不同的视频生成世界模型结合时，TC-IDM始终能提供最稳定、连贯的多步运动预测。统计上，结合世界模型的TC-IDM平均成功率达到61.11%，在简单任务上为77.7%。<br>3.  <strong>泛化实验</strong>：<br>    *   <strong>误差范围分析</strong>（图5）：在飞镖放置任务中，TC-IDM能够高保真地复现世界模型生成的计划，落点误差在4厘米内，而VLA基线则完全失败。<br>    <img src="https://arxiv.org/html/2601.18323v1/x5.png" alt="误差分析"><br>    &gt; <strong>图5</strong>：误差范围分析对比。TC-IDM能准确复现世界模型生成的计划（将飞镖投向靶心），而AVDC和AnyPos的落点偏差较大，VLA基线则完全失败。<br>    *   <strong>跨相机泛化</strong>（图6）：仅在RealSense D457上训练的模型，无需微调即可泛化到Apple Pro和RealSense D435i相机，表现出对显著视觉分布偏移的鲁棒性。<br>    <img src="https://arxiv.org/html/2601.18323v1/x6.png" alt="相机泛化"><br>    &gt; <strong>图6</strong>：跨相机泛化实验。使用在不同相机（Apple Pro和RealSense D435i）下捕获的观测执行相同任务，模型表现出良好的泛化能力。<br>    *   <strong>可变形物体零样本泛化</strong>（图7）：在仅使用刚性物体训练的情况下，TC-IDM能够零样本地执行布料移除等可变形物体操作任务，成功率达到38.46%。<br>    <img src="https://arxiv.org/html/2601.18323v1/x7.png" alt="可变形物体泛化"><br>    &gt; <strong>图7</strong>：可变形物体零样本泛化。顶部为世界模型生成的布料移除计划，底部为TC-IDM控制机器人的真实执行结果，展示了模型对非刚性材料的泛化能力。<br>    *   <strong>长时序泛化</strong>（图4）：TC-IDM能够成功执行世界模型生成的六步连帽衫折叠长时序计划，展示了其处理复杂多阶段任务的能力。<br>    <img src="https://arxiv.org/html/2601.18323v1/x4.png" alt="长时序泛化"><br>    &gt; <strong>图4</strong>：长时序泛化示例。世界模型生成完整的六步折叠计划，TC-IDM将其转化为连续动作并在真实世界中成功执行所有阶段。</p>
<p><strong>消融实验</strong>表明，与仅使用2D跟踪器（2DtrackerIDM）相比，TC-IDM采用的3D几何感知和刚体运动恢复对于提升在困难任务上的性能至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了TC-IDM框架，通过以世界模型生成的视频中工具的运动轨迹作为关键中间表示，有效桥接了世界模型规划与机器人控制之间的鸿沟；2) 通过分别由物理运动线索和语义视觉特征驱动的、解耦的臂部和工具策略头设计，清晰分离并有效利用了异构信息流；3) 通过全面的真实世界实验验证了方法的有效性，为在物理机器人上部署生成世界模型建立了一个新的强基线。</p>
<p>论文自身提到的局限性包括：需要确保机器人的末端执行器在初始帧中可见，以获取其初始位姿并避免后续运动规划中的潜在碰撞；方法的性能在一定程度上依赖于生成视频的质量。</p>
<p>本文的“计划-翻译”范式以及对工具轨迹的聚焦，为后续研究提供了重要启示：将视觉规划与控制解耦，并寻找更鲁棒、更物理基础的中间表示（如工具或接触点的运动），是提升具身智能系统泛化能力和可执行性的有效途径。这种方法尤其有利于处理视角变化、长时序任务以及可变形物体交互等挑战性场景。未来的工作可以探索更鲁棒的轨迹提取方法，或将此框架扩展到多工具或更复杂的末端执行器场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出TC-IDM方法，解决生成式世界模型中像素级规划与物理可执行动作之间的差距问题。该方法以工具为中心，通过视频分割与3D运动估计提取工具点云轨迹，并利用解耦动作头将其映射为6自由度末端执行器运动。实验表明，该方法在真实任务中平均成功率达61.11%（简单任务77.7%，零样本可变形物体任务38.46%），显著优于端到端VLA基线及其他逆动力学模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.18323" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>