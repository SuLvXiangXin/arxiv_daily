<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.05397" target="_blank" rel="noreferrer">2511.05397</a></span>
        <span>作者: Samuel Dickerson Team</span>
        <span>日期: 2025-11-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过从原始图像和自然语言指令直接映射到电机命令，绕过了手动设计的感知或规划模块，从而改变了机器人领域。然而，即使经过大规模互联网预训练，这些模型在陌生光照、新物体、视觉干扰物下仍然脆弱，且难以泛化到分布外任务。此外，最先进的机器人机械臂通常成本高达数千美元，高昂的硬件成本和系统复杂性，加上用于微调模型的遥操作数据收集过程繁琐昂贵，严重限制了可及性并阻碍了更广泛的采用。</p>
<p>本文针对VLA模型在现实场景中脆弱且依赖昂贵硬件这两个关键痛点，提出了一个软硬件协同设计的全栈系统新视角。本文的核心思路是：1）设计一个仅需300美元的低成本6自由度机械臂；2）提出一种协作训练方法，使单个模型联合输出离散和连续动作块；3）引入一种自适应视界集成器，利用两种动作预测之间的差异来估计不确定性并动态触发重规划，以实现安全可靠的操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>EveryDayVLA系统包含两个核心部分：一个经过微调的VLA模型，以及一个自适应视界集成器。整体流程为：给定当前图像观测和语言指令，VLA模型并行预测出离散和连续两种形式的未来K步动作块；自适应视界集成器计算这两种预测在每个时间步的差异，并基于预设阈值动态决定实际执行的动作块长度（即“视界”），当差异过大时提前触发重规划。</p>
<p><img src="https://arxiv.org/html/2511.05397v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：EveryDayVLA架构。VLA接收图像和自然语言指令，通过视觉和语言编码器进行标记化，并送入Llama 2 LLM，LLM同时产生连续和离散动作。这些动作随后被传递给自适应视界集成器，该集成器计算两种动作之间的差异，仅执行差异低于特定阈值的动作。</p>
</blockquote>
<p><strong>核心模块1：EveryDayVLA模型</strong>。模型以图像观测和语言指令作为输入，预测一个7维动作，包括3维相对平移偏移、3维旋转（欧拉角）和1维夹爪状态（开/闭）。模型基于Prismatic-7B VLM构建，其视觉编码器包含预训练的SigLIP和DinoV2，语言主干为Llama 2。为了实现高效的动作生成，模型采用了动作分块和并行解码策略。它配备了两个并行的动作头：一个多层感知机用于输出连续的L1回归动作，另一个通过Softmax在256个离散化区间上产生自回归的离散动作令牌。</p>
<p><strong>核心模块2：协作训练</strong>。受混合VLA方法的启发，本文采用协作训练方法，使用一个组合损失函数同时监督离散和连续动作的输出。离散动作使用交叉熵损失，连续动作使用L1回归损失。总损失为两者之和，在实践中设置权重λ=1以平衡优化。</p>
<p><strong>核心模块3：自适应视界集成器</strong>。这是本文的关键创新。集成器接收模型预测的连续和离散动作块序列。其核心思想是利用两种预测之间的不一致性作为模型不确定性的代理。具体而言，它计算每个时间步上连续与离散动作的均值绝对差。然后，算法设定一个最小执行动作数和差异阈值。它从动作序列起始处开始检查，仅执行那些MAD值低于阈值的离散动作，一旦MAD超过阈值就停止执行剩余动作并触发重规划（获取新的观测并重新进行推理）。这允许模型在不确定时执行更短的动作序列，从而更频繁地重新观察环境并重新规划，提高了在复杂或动态场景中的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2511.05397v1/x3.png" alt="硬件系统"></p>
<blockquote>
<p><strong>图3</strong>：EveryDayVLA硬件。机器人由7个关节组成，包括基座和作为末端执行器的爪形夹持器。总成本为311.98美元，提供6自由度、0.2千克有效载荷、382毫米工作半径、0.7米/秒的最大速度和10毫米内的重复精度。</p>
</blockquote>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>高效动作表示</strong>：用L1回归目标替代训练缓慢的扩散目标，同时保留并行解码和动作分块，实现了训练与推理速度的平衡。</li>
<li><strong>协作训练与自适应执行</strong>：联合训练离散和连续动作头，并创新性地利用两者预测差异（而非单一模式的置信度）来动态调整动作执行视界，实现了实时不确定性估计与重规划。</li>
<li><strong>低成本硬件集成</strong>：设计并实现了总成本约300美元的6自由度机械臂，通过开源组件和简化软件栈大幅降低了使用门槛。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真和真实世界进行评估。仿真基准使用LIBERO套件（包含Spatial, Object, Goal, Long四个任务序列）。真实世界实验使用自建的300美元机械臂和iPhone摄像头，在桌面上进行拾放任务测试。模型在自收集的包含1200条演示（配有语言指令、RGB图像和末端执行器位姿）的数据集上进行微调。</p>
<p><strong>对比方法</strong>：在LIBERO上对比了Diffusion Policy、Octo、DiT Policy、OpenVLA和OpenVLA-OFT。在真实世界中主要与OpenVLA和OpenVLA-OFT对比。此外，还将自适应视界集成器与ACT、HybridVLA、COGAct等动作集成方法进行对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>LIBERO仿真性能</strong>：如表II所示，EveryDayVLA（使用AdaHorizon）在LIBERO上取得了91.4%的平均成功率，仅次于OpenVLA-OFT（95.3%）。特别是在Spatial任务上以96.8%的成功率取得了最佳性能。消融实验表明，AdaHorizon集成策略比单独使用连续动作（89.2%）或离散动作（90.6%）基线平均提升了0.8%。</p>
</li>
<li><p><strong>推理速度</strong>：如表III所示，EveryDayVLA的推理速率在54.2到108.4 Hz之间，几乎与最快的OpenVLA-OFT（109.7 Hz）相当，远高于自回归的OpenVLA（4.2 Hz）。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.05397v1/x4.png" alt="真实世界分布内结果"></p>
<blockquote>
<p><strong>图4</strong>：真实世界分布内任务评估结果。在包含积木、球体和石块的拾放任务上，EveryDayVLA在训练集存在的环境和任务中，平均比之前的方法性能高出49%。</p>
</blockquote>
<ol start="3">
<li><p><strong>真实世界分布内性能</strong>：如图4所示，在拾取积木、球和石块并放置到不同方向的任务中，EveryDayVLA平均比OpenVLA-OFT和OpenVLA高出49%。</p>
</li>
<li><p><strong>泛化与鲁棒性</strong>：如表IV所示，在分布外任务、环境以及静态和动态干扰物测试中，EveryDayVLA表现最佳。例如，在存在动态干扰物（如人在场景中走动）时，性能仅下降10%，而OpenVLA-OFT下降33%，OpenVLA下降40%。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.05397v1/x5.png" alt="干扰物测试"></p>
<blockquote>
<p><strong>图5</strong>：静态和动态干扰物测试。上图：用静态干扰物和杂乱场景进行测试，每次试验后改变物体排列。下图：用动态干扰物测试，人类在场景中走动以干扰模型。</p>
</blockquote>
<ol start="5">
<li><strong>动作集成器对比</strong>：如表V所示，在LIBERO Spatial任务上，AdaHorizon集成器取得了96.8%的最高成功率，优于ACT（95.2%）、HybridVLA（94.2%）和COGAct（93.6%）等其他集成方法。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个完整的低成本机器人操作系统EveryDayVLA，包括一个成本约300美元、性能可用的6自由度开源机械臂硬件设计。</li>
<li>提出了一种协作训练策略和新型自适应视界动作集成器，该集成器通过连续与离散动作预测的差异来估计不确定性，并动态调整规划视界以触发重规划，显著提升了在真实复杂场景中的鲁棒性和成功率。</li>
<li>构建并公开了一个包含1200多条演示的自动化数据收集流程和数据集，用于支持低成本平台上的VLA模型微调。</li>
</ol>
<p><strong>局限性</strong>：论文提到硬件方面尚未确保长期耐用性。在执行精细操作方面存在限制，这源于舵机精度有限以及微调数据集中专家演示数量相对较少（与仿真相比）。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>软硬件协同设计</strong>：证明通过算法创新（如自适应重规划）可以部分补偿低成本硬件在精度和可靠性上的不足，为更广泛的机器人普及提供了可行路径。</li>
<li><strong>不确定性驱动的决策</strong>：利用模型内部多模态预测的不一致性作为实时不确定性度量，是一种轻量且有效的安全与鲁棒性增强策略。</li>
<li><strong>高效的动作表示与训练</strong>：采用L1回归与离散令牌化相结合的协作训练方式，在保持高推理速度的同时获得了优于纯连续或纯离散方法的性能，为VLA的动作表示设计提供了新思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出EveryDayVLA系统，旨在解决现有视觉-语言-动作（VLA）模型依赖昂贵硬件且在陌生、杂乱场景中表现不佳的问题。核心技术包括：1）自适应视野集成器（AdaHorizon），通过监测模型不确定性动态调整动作规划范围并触发实时重规划；2）低成本6自由度机械臂（仅300美元），采用Arduino Uno与PCA9685驱动器实现。实验表明，该系统在LIBERO基准上达到先进水平，在真实任务中分布内性能超越先前方法49%，分布外性能提升34.9%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.05397" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>