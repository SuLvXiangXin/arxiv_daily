<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.08333" target="_blank" rel="noreferrer">2512.08333</a></span>
        <span>作者: Yadav, Yajat, Zhou, Zhiyuan, Wagenmaker, Andrew, Pertsch, Karl, Levine, Sergey</span>
        <span>日期: 2025/12/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，在大规模多样化数据上训练的通用机器人策略已展现出广泛的泛化能力，能够执行未见环境中的多种任务。然而，当面对预训练数据未覆盖的新任务时，这些策略往往表现不佳。常见的解决方案是在新任务的有限演示数据上对通用策略进行微调。但在这种低数据量（通常少于100条演示）的情况下，标准的微调方法（如行为克隆）容易对特定的演示过拟合：这不仅导致策略丧失原有的通用能力，甚至在新任务本身遇到微小变化（如未见过的物体实例、视角、场景）时也表现不佳，无法将基础策略的泛化能力迁移到新任务上。本文针对“如何在有限数据下鲁棒地微调通用策略，使其既能学会新任务，又能泛化到该任务的未见变体，同时保留通用能力”这一具体痛点，提出了在权重空间进行参数合并的新视角。其核心思路是：对微调后的策略权重与微调前的原始权重进行线性插值，得到一个合并后的策略，该策略能继承基础模型的泛化能力并鲁棒地学习新技能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法RETAIN（Robust finE-tuning wiTh pArameter mergINg）整体流程非常简单。给定一个预训练的通用策略参数 θ_pre 和在新任务演示数据 𝔇_η 上微调后得到的策略参数 θ_ft，RETAIN通过线性插值产生最终策略参数 θ̃。</p>
<p><img src="https://arxiv.org/html/2512.08333v2/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：RETAIN方法示意图。天真的微调方法仅在微调数据所见场景（ID）上提升目标任务性能，但无法泛化到未见变体（OOD），且会丧失通用能力。通过将微调前和微调后的策略权重在参数空间进行平均（合并），得到的策略既能显著提升对目标任务未见变体的泛化能力，又能保留在非目标任务上的通用能力。</p>
</blockquote>
<p>核心公式如下：<br>θ̃ = (1 - α) · θ_pre + α · θ_ft<br>其中 α 是一个可调的合并权重。这个操作在推理时没有任何额外成本。</p>
<p>本文在基础权重合并之上，引入了两个改进：</p>
<ol>
<li><strong>协同微调</strong>：在可以获得预训练数据（或子集）的情况下，微调过程可以在目标数据 𝔇_η 和预训练数据 𝔇_pre 的混合上进行，这通常有助于更好地保留通用能力。将这种协同微调与模型合并结合的方法称为 RETAIN-co-FT，而仅在目标数据上微调再合并的方法称为 RETAIN-task-FT。</li>
<li><strong>模态特定合并</strong>：现代通用机器人策略通常是视觉-语言-动作模型，包含视觉编码器（v）、语言模型主干（l）和动作解码器（a）。论文发现，在这种多模态设置中，对不同模态使用独立的合并权重可能更有利。扩展后的合并目标为：<br>(θ̃_v, θ̃_l, θ̃_a)^T = [1 - (α_v, α_l, α_a)^T] · (θ_pre,v, θ_pre,l, θ_pre,a)^T + (α_v, α_l, α_a)^T · (θ_ft,v, θ_ft,l, θ_ft,a)^T<br>消融实验表明，有时仅合并语言模型参数就足够了。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.08333v2/x4.png" alt="持续学习流程"></p>
<blockquote>
<p><strong>图5</strong>：RETAIN支持将新技能持续合并到通用策略中。通过迭代地将微调后的权重合并到当前的基础模型中，并继续从合并后的检查点进行微调，可以顺序地添加多个任务。</p>
</blockquote>
<p>此外，由于RETAIN能帮助微调后的策略保留预训练策略的通用能力，因此可以用于持续学习。对于一系列目标任务 T_η1, …, T_ηN，可以通过迭代合并来累积新任务能力：θ̃<em>n = (1 - α) · θ̃</em>{n-1} + α · θ_ft,n，其中 θ_ft,n 是在第 n 个任务上微调得到的参数。</p>
<p>与现有方法相比，RETAIN的创新点在于将计算机视觉和自然语言处理领域探索的模型权重合并思想，首次系统地引入并验证于机器人策略微调场景，为解决低数据量下的过拟合和灾难性遗忘问题提供了一种极其简单却有效的解决方案。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在真实世界和仿真环境中进行。真实世界使用DROID机器人平台，设计了“擦白板”和“将盘子放入沥水架”两个任务，各收集约50和100条演示。仿真使用LIBERO环境，在三个新任务上微调。评估分为三类：目标任务分布内（ID，与演示数据场景一致）、目标任务分布外（OOD，包含未见物体、背景、视角等变体）以及通用任务（预训练分布中的其他任务）。预训练策略采用当前先进的模型：DROID实验使用 π₀-FAST-DROID，LIBERO实验使用在LIBERO多数据集上微调过的 π₀ 策略。</p>
<p><strong>对比基线</strong>：包括：1) <strong>Task-FT</strong>：仅在目标数据上微调；2) <strong>Co-FT</strong>：在混合数据上微调；3) <strong>LoRA</strong>：使用低秩适应进行微调；4) <strong>Freeze-FT</strong>：冻结语言模型主干进行微调；5) <strong>Scratch</strong>：从零开始在演示数据上训练。</p>
<p><img src="https://arxiv.org/html/2512.08333v2/release_figures/droid_whiteboard_results.png" alt="DROID白板任务结果"></p>
<blockquote>
<p><strong>图7（上）</strong>：DROID“擦白板”任务结果。RETAIN在OOD测试场景上的成功率（接近80%）显著高于所有基线方法（30-50%），且与ID场景性能相近，表明其优秀的泛化能力。在通用任务评估上，RETAIN也最好地保留了原有能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.08333v2/release_figures/droid_plates_results.png" alt="DROID盘子任务结果"></p>
<blockquote>
<p><strong>图7（下）</strong>：DROID“放盘子”任务结果。趋势与白板任务一致，RETAIN在OOD测试上成功率超过60%，远优于基线。通用任务评估同样表现最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.08333v2/release_figures/libero_results.png" alt="LIBERO平均结果"></p>
<blockquote>
<p><strong>图8</strong>：三个LIBERO任务的平均结果。在仿真环境中，RETAIN（尤其是RETAIN-co-FT）同样在OOD评估上优于基线，并很好地保留了通用能力。由于LIBERO基础模型的通用性较弱，其OOD性能提升幅度小于DROID。</p>
</blockquote>
<p><strong>关键实验结果</strong>：在真实世界DROID任务中，基线微调方法在ID场景成功率可达70-80%，但在OOD场景平均降至30-50%。而RETAIN-task-FT和RETAIN-co-FT在OOD场景分别取得了接近80%和超过60%的成功率，平均比先前最好的微调方法高出约40%。在通用任务评估中，RETAIN的表现与原始预训练模型相当，表明其成功保留了通用能力。在LIBERO仿真任务中也观察到一致趋势。</p>
<p><img src="https://arxiv.org/html/2512.08333v2/release_figures/data_scaling.png" alt="预训练数据量缩放"></p>
<blockquote>
<p><strong>图10</strong>：预训练数据量对合并效果的影响。使用不同数据量预训练的基模型进行合并，结果显示，基模型预训练数据越多（通用性越强），合并后策略在目标任务OOD评估上的性能越好。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.08333v2/release_figures/ood_type_and_alpha_ablation.png" alt="OOD类型与α消融"></p>
<blockquote>
<p><strong>图11</strong>：不同OOD变异类型及合并权重α的消融研究。不同OOD类型（如物体位置、实例、背景变化）对策略挑战不同。α的选择对性能有影响，通常在0.5附近有较好表现，且RETAIN对α有一定鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.08333v2/figures/modality_wise_merging.png" alt="模态特定合并消融"></p>
<blockquote>
<p><strong>图12</strong>：模态特定合并的消融实验。横轴为合并的模态组合，纵轴为OOD成功率。结果表明，仅合并语言模型主干（L）参数通常就能取得与合并所有参数相当甚至更好的效果，而仅合并视觉编码器（V）或动作专家（A）效果有限。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>基础模型通用性</strong>：预训练数据量越大、基础模型越通用，参数合并带来的OOD性能提升越显著（图10）。</li>
<li><strong>合并权重α</strong>：α的选择影响性能，但方法对α有一定鲁棒性；不同OOD变异类型的最佳α可能不同（图11）。</li>
<li><strong>模态特定合并</strong>：在多模态VLA策略中，语言模型主干参数的合并至关重要，仅合并该部分通常足以获得大部分性能增益（图12）。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种极其简单却有效的机器人策略鲁棒微调方法RETAIN，其核心是在权重空间线性插值预训练和微调后的模型参数。</li>
<li>在真实世界和仿真机器人任务上进行了广泛评估，证明RETAIN能显著提升微调策略对目标任务未见变体的泛化能力，同时保留其通用能力，平均性能优于现有方法约40%。</li>
<li>首次将参数合并应用于机器人策略的持续学习，展示了顺序合并多个新技能的可能性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，合并的有效性依赖于基础模型的通用性，如果基础模型本身泛化能力有限（如LIBERO实验所示），则合并带来的提升也会减小。此外，合并权重α可能需要根据具体任务进行调整。</p>
<p><strong>后续研究启示</strong>：这项工作表明，在参数空间进行简单的操作可以有效地调和“学习新任务”与“保留旧能力”之间的矛盾，为机器人持续学习提供了一个新颖且低成本的思路。未来的工作可以探索更自动化的α选择策略，研究不同网络架构下参数合并的规律，或将此思想与更复杂的正则化、重放等技术结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对通用视觉-语言-动作机器人策略在有限数据微调时易过拟合、丧失泛化能力的问题，提出RETAIN方法。其核心是通过参数合并（权重平均）将微调模型与预训练模型插值，获得单一稳健策略。实验表明，合并模型在新任务分布外变体上泛化能力显著提升，优于预训练和微调模型，同时保留通用能力，且性能随预训练数据量扩展，支持持续学习而不牺牲原有技能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.08333" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>