<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.25718" target="_blank" rel="noreferrer">2509.25718</a></span>
        <span>作者: Wang, Si-Cheng, Xiang, Tian-Yu, Zhou, Xiao-Hu, Gui, Mei-Jiang, Xie, Xiao-Liang, Liu, Shi-Qi, Wang, Shuang-Yi, Jin, Ao-Qun, Hou, Zeng-Guang</span>
        <span>日期: 2025/09/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，为适应下游任务而对视觉-语言-动作模型进行后训练的主流方法是监督微调，即收集演示数据并学习从观察到动作的映射。该方法虽直接有效，但获取高质量演示数据成本高昂，通常需要数十到上百条演示。强化学习为减少演示数据需求提供了有希望的途径，但在VLA模型的后训练中面临两大挑战：一是在许多操作任务中奖励信号稀疏，导致信用分配困难、训练缓慢；二是从监督预训练转向基于策略梯度的RL优化会引入高方差梯度，造成训练不稳定。现有方法通过引入外部专家反馈来提供密集奖励，或添加固定的行为克隆损失来稳定训练，但前者增加系统开销，后者因演示数据有限且可能次优，在策略性能超越演示后会阻碍进一步优化。</p>
<p>本文针对稀疏奖励和训练不稳定的痛点，提出了一种结合动作分块的PPO算法与基于自收集演示的行为克隆的VLA模型后训练方法。其核心思路是：将连续动作聚合成“动作块”以增加有效奖励反馈密度，并利用训练过程中动态收集的高质量轨迹构建自行为克隆损失，通过在线调整RL与模仿学习的权重来稳定训练过程。</p>
<h2 id="方法详解">方法详解</h2>
<p>所提方法的整体框架如图1所示，包含两个核心模块：动作分块PPO和自行为克隆。</p>
<p><img src="https://arxiv.org/html/2509.25718v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提方法的流程。（a）执行者-评论者架构。（b）在混合动作分块PPO和自行为克隆下的后训练。</p>
</blockquote>
<p><strong>整体流程</strong>：方法采用在线训练设置。给定观察 o_t 和提示 p_t，VLA策略 π_θ 输出一个长度为 h 的动作块 a_{t:t+h-1}。该动作块在环境中执行 h 步后，收集奖励并更新到下一个观察。训练目标 L_online 是动作分块PPO损失 L_PPO 与自行为克隆损失 L_BC 的加权和，权重 β_t 在线调整。</p>
<p><strong>核心模块一：动作分块PPO</strong>。该方法将标准PPO中的单步动作扩展为多步动作块（如公式1所示）。策略更新遵循PPO的裁剪替代目标（公式2），其中似然比 r_t(θ) 和优势函数 Â_t 均基于整个动作块计算。评论者网络 V_φ 通过裁剪价值损失（公式4）进行优化，用于估计状态价值并计算优势。动作分块设计使得每个策略更新步骤对应于 h 个环境步骤，从而提高了奖励反馈的相对频率和信息密度，有助于缓解稀疏奖励问题。</p>
<p><strong>核心模块二：自行为克隆</strong>。为稳定训练，引入了一个辅助的监督损失 L_BC（公式6）。其关键创新在于演示数据集 D_demo 是一个动态缓冲池：初始时仅包含少量专家演示；在训练过程中，当智能体产生新的成功轨迹 x_i 时，若其长度 L(x_i) 不超过设定的阈值 ℓ_limit（初始设为最长专家轨迹的长度），则将其加入缓冲池。此举不断用智能体自身生成的高质量（表现为更快完成任务）轨迹更新演示集，既增加了数据多样性，又可能提升演示数据的整体质量。</p>
<p><strong>在线后训练与权重调整</strong>：总损失为 L_online(θ) = β_t * L_PPO(θ) + L_BC(θ)。权重 β_t 采用渐进式调度：β_t = tanh(t / T_warmup)（公式8）。在训练初期（t=0），β_t≈0，优化主要由行为克隆损失驱动，利用初始演示稳定策略。随着训练进行，β_t 单调增加，逐渐强调PPO损失，实现从模仿学习到强化学习的平滑过渡。这种设计避免了在评论者网络未训练好时，过早应用高方差的RL梯度导致策略退化。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的创新具体体现在：1) <strong>动作分块设计</strong>：将PPO应用于动作块而非单步动作，提升了时间一致性和奖励反馈密度；2) <strong>动态演示缓冲池</strong>：用智能体自生成的优质轨迹持续更新演示集，克服了固定、有限演示集的局限性；3) <strong>在线权重调整机制</strong>：通过tanh调度自适应平衡RL和模仿学习，确保了训练稳定性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在MetaWorld的MT10基准（10个单任务环境）上进行，使用稀疏奖励。以Octo-small VLA模型为骨干，初始化策略头和价值头。演示数据使用环境提供的基于规则的策略收集，每条轨迹最多200步。评估时，每个任务运行128个回合，报告平均成功率、轨迹长度分布的10分位数以及最短10%轨迹的平均长度。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li><strong>SFT</strong>：使用10条演示数据进行监督微调。</li>
<li><strong>SFT 100</strong>：使用100条演示数据进行监督微调（模拟数据丰富的情况）。</li>
<li><strong>PPO</strong>：标准PPO后训练，无额外演示数据。</li>
<li><strong>Ours</strong>：所提方法，仅使用10条演示初始化缓冲池。</li>
</ul>
<p><strong>关键实验结果</strong>：如表I所示，所提方法在平均性能上最优。</p>
<ul>
<li>在平均成功率上，所提方法达到0.93，显著优于仅用10条演示的SFT（0.70），甚至超过了使用十倍数据（100条）的SFT 100（0.89）。</li>
<li>在平均步骤数上，所提方法仅需42.17步，远少于SFT（66.62步）和SFT 100（65.65步），表明RL后训练能发现比初始演示更高效的策略。</li>
<li>标准PPO表现极不稳定，在多数任务上成功率很低或崩溃，凸显了引入行为克隆稳定训练的必要性。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.25718v1/x2.png" alt="消融研究"></p>
<blockquote>
<p><strong>图2</strong>：在MetaWorld Push任务上的消融研究平滑性能曲线。（a）动作分块在PPO中的效果。（b）演示缓冲池的效果。</p>
</blockquote>
<p><strong>消融实验分析</strong>（图2）：</p>
<ul>
<li><strong>动作分块的作用</strong>（图2a）：在PPO中移除动作分块设计会导致性能大幅下降（成功率从0.77降至约0.4），证明了该设计对提升PPO在VLA后训练中有效性的关键作用。</li>
<li><strong>动态演示缓冲池的作用</strong>（图2b）：若缓冲池不更新（仅用初始演示），后训练带来的提升有限；若不加限制地添加所有自生成轨迹，性能会比所提方法下降约10%，强调了筛选高质量演示的重要性。此外，固定PPO与BC损失权重为1:1会导致收敛变慢。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.25718v1/x3.png" alt="案例研究"></p>
<blockquote>
<p><strong>图3</strong>：对比监督微调（10条演示）和所提方法的案例研究，（a）开窗，（b）关抽屉。</p>
</blockquote>
<p><strong>定性结果</strong>（图3）：案例研究直观展示了所提方法学习到更高效策略的能力。例如，在“开窗”任务中，SFT策略先向上移动机械臂，再向下向前推窗；而所提策略直接引导机械臂至窗把手，步骤数减少近半。“关抽屉”任务中也观察到类似现象。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了用于VLA模型后训练的<strong>动作分块PPO算法</strong>，通过聚合连续动作增加了信息反馈的有效密度。</li>
<li>设计了基于<strong>动态演示缓冲池的自行为克隆辅助损失</strong>，该缓冲池在训练中持续收集智能体生成的高质量轨迹，提升了演示数据的多样性和质量。</li>
<li>构建了一个<strong>在线后训练框架</strong>，通过自适应权重调整平滑地从模仿学习过渡到强化学习，显著提升了训练稳定性和最终性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确列出局限性，但从实验设置和未来工作方向可推断：当前验证仅限于模拟环境（MetaWorld），在更复杂、多样化的真实机器人任务和大规模基准上的有效性有待进一步验证；方法的理论分析（如收敛性）尚未深入。</p>
<p><strong>启示</strong>：本工作证实了RL是VLA模型后训练的一个可行且有前景的范式，为减少对大量人工演示数据的依赖提供了实用方案。动态自提升演示池的思想可推广至其他需要结合模仿学习与RL的场景。未来研究可沿着以下方向深入：将该框架应用于真实机器人任务进行大规模验证；探索更智能的轨迹质量评估与筛选机制；对方法的收敛性和稳定性进行理论分析。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对强化学习（RL）用于视觉-语言-动作（VLA）模型后训练时面临的稀疏奖励和训练不稳定问题，提出了一种结合**动作分块PPO**与**自我行为克隆**的方法。关键技术包括：将连续动作聚合为块以改善策略的时间一致性与反馈密度；利用训练中自我收集的高质量演示进行辅助的行为克隆损失；并在线调整两个目标的相对权重以稳定训练。在MetaWorld基准测试中，该方法超越了监督微调，取得了**93%的成功率**和平均**42.17步**的成功步数，验证了RL用于VLA后训练的可行性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.25718" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>