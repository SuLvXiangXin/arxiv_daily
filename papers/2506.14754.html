<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.14754" target="_blank" rel="noreferrer">2506.14754</a></span>
        <span>作者: Mustafa Mukadam Team</span>
        <span>日期: 2025-06-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人触觉感知对于灵巧操作至关重要。目前主流方法主要依赖于单模态的触觉传感，特别是基于视觉的触觉传感器（如GelSight），它们通过捕捉弹性体表面的变形图像来感知接触。然而，人类的触觉是多感官的，整合了皮肤形变、振动、运动和压力等多种信号。尽管像Digit 360这样的新型传感器已经能够在一个紧凑的指尖形态中同时采集高分辨率图像、音频（振动）、惯性测量单元（IMU）运动和压力信号，但如何有效地融合这些互补的异质模态，形成一个统一、可扩展且易于整合的触觉表征，仍然是一个空白。现有工作要么单独使用各模态，要么采用简单的拼接策略，计算效率低且融合不充分。</p>
<p>本文针对多感官触觉信号融合不足这一具体痛点，提出了一个名为Sparsh-X的统一表征学习框架。其核心思路是：利用自监督学习，在一个Transformer骨干网络中将来自Digit 360传感器的图像、音频、运动和压力四种触觉模态进行深度融合，学习一个能够捕捉对机器人操作有用的物理属性的通用表征。</p>
<h2 id="方法详解">方法详解</h2>
<p>Sparsh-X是一个基于Transformer的多感官触觉表征学习骨干网络。其整体目标是将四种异质触觉模态的输入，通过自监督预训练，融合为一个紧凑的、富有表现力的接触嵌入向量。</p>
<p><img src="https://arxiv.org/html/2506.14754v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Sparsh-X整体架构。这是一个Transformer骨干网络，接收图像、音频、加速度计（IMU）和压力四种输入。前 \(L_f\) 层（此处为8层）各模态独立进行自注意力处理；后 \(L_b\) 层（此处为4层）通过引入 \(B\) 个（此处为4个）瓶颈融合令牌，实现跨模态注意力，促进信息在模态间的提炼与交换。</p>
</blockquote>
<p><strong>输入预处理与令牌化</strong>：由于各模态采样频率和数据结构不同，需分别处理。</p>
<ol>
<li><strong>触觉图像</strong>：30 fps采样，以步长5沿通道维度拼接，裁剪鱼眼图像中心区域并调整至224x224x3，然后分割为16x16的图像块，线性投影为768维令牌。</li>
<li><strong>音频</strong>：来自两个接触麦克风，48kHz采样。取0.55秒窗口，转换为128通道的对数梅尔频谱图（使用5ms汉明窗，步长2.5ms）。拼接两个频谱图得到224x256的输入，同样用16x16的块进行令牌化。</li>
<li><strong>IMU（加速度计）</strong>：400Hz采样，取0.55秒窗口，形成224x3的时间序列信号并令牌化。</li>
<li><strong>压力</strong>：200Hz采样，取1.1秒窗口，形成224x1的时间序列信号并令牌化。</li>
</ol>
<p><strong>网络结构与融合机制</strong>：网络总层数 \(L = L_f + L_b = 12\)。前 \(L_f = 8\) 层，各模态的令牌仅在其内部进行自注意力运算，实现模态内特征提取。后 \(L_b = 4\) 层为融合层，创新性地引入了瓶颈注意力机制。具体而言，为每个模态的嵌入序列拼接上 \(B = 4\) 个可学习的瓶颈融合令牌。在融合层的注意力计算中，这些瓶颈令牌作为“多模态摘要器”，参与所有模态的信息交换。每一层融合后，对所有模态的瓶颈令牌取平均，以强制信息共享。这种设计显著降低了传统Transformer简单拼接所有模态令牌所带来的二次方计算复杂度。</p>
<p><strong>自监督训练流程</strong>：采用师生自蒸馏方法进行预训练。师生网络结构相同，均包含编码器和预测头。对学生网络的输入令牌按模态进行随机掩码（局部掩码保留10-50%令牌，全局掩码保留50-100%令牌）。将全局和局部掩码对应的寄存器令牌拼接后送入预测头。训练目标是让学生网络预测教师网络输出的聚类类别（教师网络的输出作为伪标签），使用交叉熵损失进行优化。模型在约100万未标记样本上训练200个周期。</p>
<p><img src="https://arxiv.org/html/2506.14754v1/extracted/6547021/figures/method/pre-training-setup2.png" alt="预训练数据收集"></p>
<blockquote>
<p><strong>图3</strong>：预训练数据收集设置。(a) 使用搭载Digit 360的Allegro手进行随机物体交互；(b) 使用两指手动夹持器执行抓取、滑动、敲击、放置、跌落等原子操作，接触不同材质表面。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与基准</strong>：使用Digit 360传感器收集了约100万次接触丰富的交互数据用于Sparsh-X的预训练。此外，为评估表征能力，构建了多个下游任务数据集，包括物体-动作-表面分类、材料-数量估计、法向力估计，以及用于策略学习的插头插入和手内旋转任务。</p>
<p><strong>对比方法</strong>：主要对比了以下几种基线：(1) <strong>端到端（E2E）模型</strong>：针对特定任务，从零开始训练编码器和解码器。(2) <strong>单模态或模态子集</strong>：仅使用触觉图像、音频、IMU或压力中的一种或几种组合，输入到Sparsh-X（编码器冻结）或E2E模型中。(3) <strong>仅视觉策略</strong>：在策略学习中，不使用任何触觉输入。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>物理属性推断</strong>：在物体-动作-表面分类、材料-数量估计和法向力估计三个任务上，评估冻结的Sparsh-X表征能力。</p>
<p><img src="https://arxiv.org/html/2506.14754v1/x2.png" alt="物理属性推断结果"></p>
<blockquote>
<p><strong>图4</strong>：使用不同触觉输入的冻结Sparsh-X表征在下游任务上的性能。左图：多模态协同显著提升物体-动作-表面识别准确率（例如，音频+IMU比仅图像高32%），且预训练模型在所有数据规模下均优于E2E。中图：所有模态联合在材料-数量估计任务上达到最高精度，比仅用触觉图像的E2E方法高20.5%。右图：所有模态联合将法向力估计的平均误差降至35mN，比仅用图像提升17%。</p>
</blockquote>
<p>综合来看，融合所有模态的Sparsh-X表征在物理属性推断上比仅使用触觉图像的端到端方法准确率平均提升48%，证明了多感官预训练的优势。</p>
</li>
<li><p><strong>策略学习——插头插入（模仿学习）</strong>：<br><img src="https://arxiv.org/html/2506.14754v1/extracted/6547021/figures/plug_insertion/plug-insertion-d360.png" alt="插头插入实验与结果"></p>
<blockquote>
<p><strong>图5</strong>：上：插头插入任务实验设置。下：使用不同触觉感官模式的策略在20次试验中的成功率。利用Sparsh-X的多模态触觉，策略成功率比仅用外部视觉的策略高500%，比用触觉图像进行端到端训练的视觉-触觉策略高63%。</p>
</blockquote>
<p>使用所有模态的Sparsh-X表征，策略成功率高达90%。预训练至关重要，比使用所有模态但从头开始与策略联合训练的方法性能高出90%。值得注意的是，仅使用触觉图像时，端到端训练优于使用冻结的预训练表征，论文分析这是因为该任务触觉图像变化较小，专用编码器可能更有效，但这可能限于分布内数据。</p>
</li>
<li><p><strong>策略学习——手内旋转（仿真到现实的触觉适应）</strong>：在已训练好的纯本体感知仿真策略（Hora）基础上，通过ControlNet结构融入真实的触觉反馈进行适应。</p>
<p><img src="https://arxiv.org/html/2506.14754v1/extracted/6547021/figures/plug_insertion/sparshx_plug_insertion_sr_serif.png" alt="触觉适应方法"></p>
<blockquote>
<p><strong>图6</strong>：通过ControlNet进行真实世界触觉适应的示意图。零初始化卷积层使得触觉信息能够渐进地融入冻结的基础策略。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14754v1/x3.png" alt="手内旋转结果"></p>
<blockquote>
<p><strong>图7</strong>：上：对于标称物体属性，使用Sparsh-X的触觉适应将物体垂直漂移降低了90%。中：在物体动力学属性（摩擦、质量）改变时，使用所有模态的Sparsh-X适应策略表现出最优的稳定性。下：物体质量增加时，有多感官触觉输入的策略能调整手指步态以补偿重量。</p>
</blockquote>
<p>实验表明，使用Sparsh-X（所有模态或仅图像）进行触觉适应，能将物体旋转时的垂直漂移减少90%。当物体摩擦减小或质量增加时，多模态触觉适应策略展现出更强的鲁棒性和适应性，优于仅微调基础策略或使用端到端触觉适应的方案。</p>
</li>
</ol>
<p><strong>消融实验总结</strong>：图4系统地展示了不同输入模态组合对下游任务性能的影响，证明了多模态融合的必要性和优势（所有模态联合始终最佳）。在策略学习中，图5和图7的结果分别验证了多模态触觉和预训练对模仿学习性能的关键提升，以及多模态触觉对于适应物体物理属性变化的互补价值。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>Sparsh-X</strong>，首个融合图像、音频、运动和压力四种触觉模态的统一自监督骨干网络，用于学习通用多感官触觉表征。</li>
<li>创建了首个基于 <strong>Digit 360</strong> 的、用于基准测试的多感官触觉数据集。</li>
<li>通过模仿学习和仿真到现实的触觉适应两种方式，实证了 <strong>Sparsh-X 表征能显著提升真实世界机器人策略学习的性能和鲁棒性</strong>（例如，插入任务成功率提升63%，手内旋转物体状态恢复鲁棒性提升90%）。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在插头插入任务中，当仅使用触觉图像时，端到端训练的专用编码器表现优于冻结的预训练Sparsh-X图像编码器。这表明当前的预训练表征在触觉图像模态上，可能仍需要更丰富多样的数据或针对特定任务的微调才能发挥全部潜力。</p>
<p><strong>启示</strong>：</p>
<ol>
<li><strong>多模态融合是方向</strong>：充分利用触觉的多感官特性，通过互补信号融合来增强感知的鲁棒性和信息丰富度，是提升灵巧操作能力的关键。</li>
<li><strong>自监督与基础模型</strong>：在大规模未标记触觉数据上进行自监督预训练，学习可迁移的通用表征，是实现数据高效和可扩展机器人触觉感知的有效路径，向“触觉基础模型”迈出了一步。</li>
<li><strong>触觉适应新范式</strong>：提出的基于ControlNet的触觉适应方法，为将仿真中训练的策略高效、安全地适配到拥有丰富真实触觉信号的物理世界，提供了一种新颖且有效的技术思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决机器人操作中触觉感知单一化的问题，提出融合多模态触觉信号以提升操作的鲁棒性与精细度。核心方法是 **Sparsh-X**，一个基于Transformer的多感官触觉融合模型，通过自监督学习，统一了来自Digit 360传感器的图像、音频、运动和压力四种触觉模态。实验表明，该表征在模仿学习和触觉适应任务中，相比仅使用触觉图像的端到端模型，**策略成功率提升63%**，从触觉恢复物体状态的**鲁棒性提升90%**；在物理属性推断任务上，**准确率提升48%**。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.14754" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>