<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>KeyMPs: One-Shot Vision-Language Guided Motion Generation by Sequencing DMPs for Occlusion-Rich Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>KeyMPs: One-Shot Vision-Language Guided Motion Generation by Sequencing DMPs for Occlusion-Rich Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.10011" target="_blank" rel="noreferrer">2504.10011</a></span>
        <span>作者: Anarossi, Edgar, Kwon, Yuhwan, Tahara, Hirotaka, Tanaka, Shohei, Shirai, Keisuke, Hamaya, Masashi, Beltran-Hernandez, Cristian C., Hashimoto, Atsushi, Matsubara, Takamitsu</span>
        <span>日期: 2025/04/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>动态运动原语（DMPs）因其基于动力系统的稳定性和鲁棒性，为机器人运动生成提供了一个强大且灵活的框架。它们能够高效地学习和复现运动行为，并在空间和时间维度上具有良好的可扩展性。然而，DMPs框架难以有效处理机器人领域常用的多模态输入，如视觉和语言。基于深度学习的DMP框架（如CIMEDNet、DSDNet）试图从视觉数据中估计DMP参数，但通常需要大量数据集才能实现良好的泛化，这限制了其在真实世界的应用。同时，将语言输入与DMPs关联，并一次性生成长而复杂的运动序列，进一步扩大了所需的特征空间，加剧了对数据的需求。另一方面，视觉语言模型（VLMs）擅长处理多模态数据并理解高层概念，但它们通常缺乏直接推断低层运动细节（如DMP参数）的能力。在遮挡丰富的任务（如切割、蛋糕裱花）中，执行过程中的持续观察可能不可行，因此需要一次性完成基于视觉语言指令的完整运动规划。</p>
<p>本文针对上述痛点，提出了一种结合VLMs与DMPs序列化的新视角。核心思路是：利用VLMs的高层推理能力，通过关键词标记的原语选择来关联语言指令与预定义的DMP参数；同时利用VLMs的空间感知能力，生成关键点对作为DMP序列的空间缩放参数基础，从而一次性规划出符合多模态输入意图的复杂运动。</p>
<h2 id="方法详解">方法详解</h2>
<p>KeyMPs框架旨在通过序列化DMPs，实现一次性、视觉语言引导的运动生成。其整体流程分为三个阶段：1）预处理；2）上下文处理；3）基于DMP的运动生成。</p>
<p><img src="https://arxiv.org/html/2504.10011v2/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：KeyMPs框架概览，以物体切割任务为例。框架接收RGB图像和自然语言文本输入。物体检测器识别物体的全局位置并裁剪图像。裁剪后的图像和文本由基于VLM的组件处理。关键词标记原语选择部分选择DMPs的已学参数，关键点对生成部分创建缩放参数的基础。这些2D关键点对通过与全局位置和物体高度结合，生成3D空间缩放参数，用于缩放运动原语，随后将这些原语序列化以产生最终可执行的运动。</p>
</blockquote>
<p><strong>1. 预处理</strong>：输入包括原始环境观测图像 <code>img_env</code> 和自然语言指令 <code>l</code>。物体检测器用于提取目标物体的全局坐标 <code>pos_global</code> 并裁剪出物体图像 <code>img_obj</code>。此外，还需要获取物体高度 <code>h</code> 等信息。</p>
<p><strong>2. 上下文处理</strong>：此阶段包含两个核心模块，分别处理不同的上下文信息。</p>
<ul>
<li><strong>关键词标记原语选择</strong>：假设存在一个预定义的运动原语词典 <code>D</code>，它将描述性的关键词映射到通过模仿学习获得的DMP基础函数权重 <code>w</code>（即“已学参数”）。一个VLM组件 <code>VLM_keyword</code> 接收裁剪后的物体图像 <code>img_obj</code> 和语言指令 <code>l</code>，输出一个最匹配当前任务上下文的关键词。该系统通过精心设计的提示进行初始化，其中包含任务描述、可用原语列表及示例。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.10011v2/x3.png" alt="原语选择提示"></p>
<blockquote>
<p><strong>图3</strong>：关键词标记原语选择的初始化提示示例。提示中定义了任务、可用的切割原语（如“向下切割”、“锯切”）及其适用场景，并提供了示例以引导VLM做出合适选择。</p>
</blockquote>
<ul>
<li><strong>关键点对生成</strong>：另一个VLM组件 <code>VLM_keypoints</code> 接收相同的 <code>img_obj</code> 和 <code>l</code>，生成 <code>K</code> 个2D关键点对（像素坐标），每个点对代表一个线段的起点和终点。关键点对的数量 <code>K</code> 由VLM根据上下文动态决定。这些点对将作为后续序列化DMPs的起点和终点位置的基础。该VLM同样通过特定提示初始化，指导其如何根据指令生成关键点。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.10011v2/x4.png" alt="关键点生成提示"></p>
<blockquote>
<p><strong>图4</strong>：关键点对生成的初始化提示示例。提示指导VLM根据指令（如“切成两半”）在图像上生成代表切割路径的关键点对，并提供了输出格式示例。</p>
</blockquote>
<p><strong>3. DMP运动生成</strong>：首先，通过后处理将2D关键点对、物体全局坐标 <code>pos_global</code> 和高度 <code>h</code> 结合，转换为机器人任务空间中的3D空间缩放参数 <code>{y0, ygoal}</code>。然后，从原语词典 <code>D</code> 中根据选定的关键词检索出对应的已学参数 <code>w</code>。最后，通过时间序列化多个DMP来生成完整运动：每个DMP片段使用相同的已学参数 <code>w</code>，但应用不同的空间缩放参数 <code>{y0^(k), ygoal^(k)}</code>（由第k个关键点对转换而来），从而构成从起点到终点的连续轨迹。</p>
<p><strong>创新点</strong>：与现有方法相比，KeyMPs的创新在于：1) 将VLM的高层理解与DMP的低层运动生成解耦，避免了直接从VLM生成DMP参数的低效性；2) 利用VLM的动态空间规划能力（生成关键点对）来实现一次性长序列运动规划，克服了需要持续反馈的局限；3) 通过结合物体高度等简单深度线索，增强了VLM的3D空间感知，实现了从2D图像到3D运动的有效映射。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境（使用Isaac Gym）和真实世界（使用Franka Emika Panda机械臂）中进行了评估。任务包括<strong>物体切割</strong>（模拟与真实）和<strong>蛋糕裱花</strong>（模拟）。使用了包括香蕉、黄瓜、蛋糕等多种物体。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li><strong>CIMEDNet</strong>：一种基于深度学习的DMP框架，从单张图像直接回归一组DMP参数。</li>
<li><strong>DSDNet</strong>：作者先前的工作，通过深度学习序列化多个DMP（生成多组参数）。</li>
<li><strong>消融方法</strong>：<ul>
<li><code>w/o KP</code>：不使用关键点对生成，仅使用物体边界框的中心点作为起点和终点，生成单个DMP运动。</li>
<li><code>w/o KS</code>：不使用关键词选择，随机从原语词典中选择一个原语。</li>
<li><code>w/o KS&amp;KP</code>：同时不使用关键词选择和关键点对生成。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2504.10011v2/x5.png" alt="切割任务成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：模拟切割任务中各方法的成功率对比。KeyMPs在所有测试物体上均达到100%成功率，显著优于CIMEDNet、DSDNet及所有消融变体。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.10011v2/x6.png" alt="轨迹误差对比"></p>
<blockquote>
<p><strong>图6</strong>：模拟切割任务中，生成的轨迹与专家演示轨迹之间的动态时间规整（DTW）误差。KeyMPs产生的轨迹误差最小，表明其运动最接近专家演示。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.10011v2/x7.png" alt="真实世界切割结果"></p>
<blockquote>
<p><strong>图7</strong>：真实机器人切割任务的定性结果。KeyMPs能够根据“切成两半”的指令，为不同物体（香蕉、黄瓜）生成合适的切割轨迹并成功执行。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.10011v2/x8.png" alt="蛋糕裱花任务结果"></p>
<blockquote>
<p><strong>图8</strong>：模拟蛋糕裱花任务的成功率。KeyMPs在“写字母”和“画螺旋”两种复杂指令下均能成功规划轨迹，而对比方法完全失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.10011v2/x9.png" alt="消融实验可视化"></p>
<blockquote>
<p><strong>图9</strong>：消融实验的轨迹可视化。<code>w/o KP</code>（仅单点）无法完成切割；<code>w/o KS</code>（随机选原语）可能因选择不合适的原语（如对软香蕉使用“锯切”）导致失败；KeyMPs则能生成适配的正确轨迹。</p>
</blockquote>
<p><strong>消融实验总结</strong>：关键点对生成（KP）组件对任务成功至关重要，它使运动能够覆盖目标区域。关键词选择（KS）组件确保了运动风格（原语）与物体属性的匹配，对提升成功率和轨迹质量有显著贡献。两者缺一不可。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了KeyMPs框架，创新性地将VLMs的高层推理和空间感知能力与DMPs的鲁棒运动表征相结合，通过序列化DMPs实现了从视觉语言输入到复杂机器人运动的一次性生成。</li>
<li>在模拟和真实的遮挡丰富任务（物体切割、蛋糕裱花）上验证了框架的有效性，生成的3D运动符合多模态输入意图，且性能显著优于现有的基于深度学习的DMP方法。</li>
<li>展示了框架出色的任务泛化能力，能够处理未见过的物体和多样化的高级语言指令，而无需针对新任务进行额外的模型微调或收集大量数据。</li>
</ol>
<p><strong>局限性</strong>：论文提到，框架依赖于预定义的运动原语词典，其构建需要特定领域的知识。此外，VLM组件的性能受到其提示设计的影响。</p>
<p><strong>启示</strong>：KeyMPs展示了一条有效利用大规模预训练基础模型（VLMs）进行机器人运动规划的路径，即“高层理解，低层控制”的解耦范式。这为减少机器人学习对大规模、高质量机器人特定数据集的依赖提供了新思路。未来工作可以探索如何自动化或优化原语词典的构建，以及如何将框架扩展到更广泛的机器人操作任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出KeyMPs框架，解决动态运动基元（DMPs）难以整合视觉与语言等多模态输入，以及在存在遮挡的复杂任务（如切割、糖衣涂抹）中一次性生成符合意图的运动序列的问题。方法核心是结合视觉语言模型（VLM），通过关键词标记基元选择参考运动，并利用VLM的空间感知生成关键点对，以序列化DMPs并生成空间缩放参数。实验在仿真与真实环境的物体切割及仿真蛋糕糖衣任务上验证，结果表明其性能优于其他整合VLM的DMP方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.10011" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>