<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.12211" target="_blank" rel="noreferrer">2508.12211</a></span>
        <span>作者: Neary, Cyrus, Younis, Omar G., Kuramshin, Artur, Aslan, Ozgur, Berseth, Glen</span>
        <span>日期: 2025/08/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，预训练的视觉-语言-动作模型已成为构建通用机器人策略的有前途的基础组件。它们能够将自然语言指令和视觉观测直接映射到机器人动作，实现灵活的任务适应。然而，主流方法主要关注通过行为克隆训练有效的VLA架构。这些模型在零样本部署到分布外场景时，由于无法推理其动作的潜在后果，仅依赖于对训练数据中观察到的行为的模仿，常常产生脆弱的行为或不安全的失败。另一方面，基于模型的规划算法虽然能显式地推理未来结果，但通常依赖于精心设计的手工启发式函数来处理搜索空间的组合爆炸问题，这在以自然语言指定、部署于复杂杂乱环境中的多样化机器人任务中尤为困难。</p>
<p>本文针对预训练VLA策略在零样本部署时行为脆弱、缺乏前瞻性推理的关键痛点，提出了将基于模型的搜索整合到VLA推理过程中的新视角。核心思路是利用预训练的VLA模型来引导一个受蒙特卡洛树搜索启发的算法，通过VLA定义的抽象和先验来高效探索语言指定的机器人任务，从而产生比直接遵循VLA动作预测更优的行为。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLAPS框架旨在每个任务决策点，利用世界模型对时序抽象的动作块进行搜索，并使用预训练的VLA策略来指导搜索的扩展和选择过程。整体流程是迭代式的：从当前环境状态开始构建搜索树，进行VLA引导的树搜索，找到目标状态或耗尽计算预算后，返回最有希望的动作序列执行，然后基于新的观测重复此过程。</p>
<p><img src="https://arxiv.org/html/2508.12211v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：VLAPS框架概览。在每个决策点，该方法使用世界模型对时序抽象的动作块进行搜索。预训练的VLA策略被用于指导搜索的扩展和选择过程。一旦搜索找到完成任务的节点或计算预算耗尽，便返回最有希望的动作序列执行，随后基于下一个观测重复该过程。</p>
</blockquote>
<p>VLAPS的核心模块主要体现在利用VLA解决搜索算法面临的两个挑战：1）精炼搜索空间；2）偏置树遍历。</p>
<p><strong>精炼搜索空间（节点扩展）</strong>：为了应对机器人环境高维连续动作空间和搜索树深度指数增长的挑战，VLAPS在由VLA策略输出动态定义的时序抽象动作块集合上进行搜索。具体而言，该方法维护一个从VLA策略或演示数据中采样得到的有限候选动作块库Φ。在搜索中需要扩展一个叶节点v时，VLAPS并非搜索整个Φ，而是从VLA定义的采样分布β_Φ中抽取k个动作块，构成该节点待考虑的候选集Φ_v。分布β_Φ(·|I_t, L_T)以VLA模型在当前观测I_t和指令L_T下采样的动作块u^{vla}为中心，为Φ中每个动作块u^i分配概率，概率值随其与u^{vla}的欧氏距离增大而按指数衰减，并包含一个小的均匀探索项ε。这确保了搜索集中在与当前任务上下文相关的、可行的动作区域。</p>
<p><strong>偏置树遍历（节点选择）</strong>：在树的选择阶段，VLAPS进一步利用VLA定义了一个先验分布ψ_{Φ_v}来影响探索，将树遍历偏置到VLA策略看好的子树。该选择过程使用了一种省略价值估计的策略引导树搜索（即Q值恒为0的PUCT）。在选择时，算法根据公式(2)的分数选择下一个节点，该分数结合了基于VLA的先验概率ψ_{Φ_v}(u^i|I_t, L_T)和节点-动作访问计数N(v, u^i)。先验分布ψ_{Φ_v}的构造与β_Φ类似，也是以VLA输出为中心的softmax风格分布。这种设计使得访问次数大致按VLA先验的比例分配，同时通过访问计数项保证覆盖度。</p>
<p><img src="https://arxiv.org/html/2508.12211v2/x2.png" alt="节点扩展与选择"></p>
<blockquote>
<p><strong>图2</strong>：VLA引导的节点扩展与动作选择。每当遇到新节点时，VLAPS使用VLA策略采样一组离散的候选动作块以供搜索。在整个后续搜索中，VLAPS将动作块选择偏置到与VLA输出相似的那些块。</p>
</blockquote>
<p>与现有方法相比，VLAPS的创新点在于：1）<strong>自动化与适应性</strong>：它利用VLA在运行时自动、动态地构建任务相关、状态依赖的搜索空间，而非依赖手工预定义的技能或符号目标。2）<strong>直接可执行</strong>：它直接在VLA衍生的、机器人可执行的动作序列上进行分支搜索，省去了额外的底层运动规划器或目标条件策略。3）<strong>双向增强</strong>：通过将基于模型的搜索与VLA推理过程集成，既利用VLA的先验使大规模搜索可行，又利用搜索的前瞻性纠正VLA的短视错误。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在LIBERO模拟仿真套件中进行，这是一个语言指定的机器人操作任务集，包含Libero-Spatial, Libero-Goal, Libero-Object, Libero-90, 和 Libero-10五个任务套件。基线方法是直接部署的VLA-only策略（即典型的VLA模型使用方式）。底层VLA模型采用在LIBERO数据集上微调过的Octo-base-1.5模型。VLAPS的实现中，动作块长度H=4，每次节点扩展采样k=10个子节点，候选动作块库Φ通过对VLA-only策略成功运行的动作序列进行归一化和K-Medoids聚类得到，包含2000个原型。</p>
<p><img src="https://arxiv.org/html/2508.12211v2/x3.png" alt="定性结果"></p>
<blockquote>
<p><strong>图3</strong>：VLAPS成功而底层VLA策略失败的示例。上图两个任务中，VLA策略未能抓取目标物体，而VLAPS成功完成了抓放。下图任务中，VLA策略在取出碗之前过早关上了抽屉，而VLAPS正确地将碗放入后再关闭。</p>
</blockquote>
<p>关键定量结果总结如下：VLAPS在所有实验中都显著优于VLA-only基线。在底层VLA模型经过50k步微调后，VLAPS在Libero-Spatial任务上的成功率从34%提升至97%，在Libero-Object任务上从6%提升至73%，提升幅度最高达67个百分点。即使底层VLA性能很差（如10k步微调时），VLAPS也能在部分任务上取得非零的成功率，而VLA-only基线则几乎完全失败。</p>
<p><img src="https://arxiv.org/html/2508.12211v2/x4.png" alt="性能对比"></p>
<blockquote>
<p><strong>图4</strong>：VLAPS与VLA-only策略在LIBERO上的性能随底层VLA微调步数的变化。上图：任务成功率。下图：成功完成每个任务的平均算法运行时间。VLAPS始终获得更高的成功率，且随着基础VLA质量提升，其搜索速度显著加快。</p>
</blockquote>
<p>表I详细列出了各任务套件在不同模型检查点下的成功率。结果表明，VLAPS能够将相对较小的VLA策略的性能提升到与更大、更先进的模型相匹配的水平。例如，使用50k步微调的VLA时，VLAPS在多个套件上的成功率已接近或超过使用更大模型（如200k步）的VLA-only基线。此外，随着底层VLA模型质量的提高，VLAPS搜索到解决方案所需的时间显著减少，说明更好的VLA先验能极大提高搜索效率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了<strong>VLAPS框架</strong>，一种将基于模型的搜索集成到预训练VLA策略推理过程中的原则性方法，以提升其在机器人任务上的性能。2）设计了具体的<strong>引导机制</strong>，利用VLA自动定义任务导向的、可处理的搜索空间（通过β_Φ采样），并偏置树遍历（通过ψ_{Φ_v}先验），从而高效探索语言指定的长视野任务。3）通过系统的实验证明，VLAPS能<strong>显著提升</strong>底层VLA策略的成功率（高达67个百分点），并能将较小VLA的性能提升至与先进大模型相当的水平。</p>
<p>论文提到的局限性包括：当前工作假设可以访问准确的环境模型（模拟器），未探索不完美或学习得到的世界模型的影响；此外，VLAPS在搜索过程中需要多次查询VLA模型和运行模拟，计算成本较高，虽然可以通过调整计算预算进行控制。</p>
<p>本工作对后续研究的启示在于：它展示了<strong>测试时计算</strong>（test-time compute）在机器人及具身推理问题中的价值，为在VLA驱动的决策循环中显式注入先验模型知识提供了机制。这启发未来研究可以探索结合学习的世界模型或价值函数估计，以处理不完美的环境模型或进一步加速搜索。同时，该方法也印证了结合<strong>模仿学习（提供先验）与基于模型的搜索（提供规划）</strong> 这一范式在构建更鲁棒、更具推理能力的通用机器人策略方面的巨大潜力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对预训练的视觉-语言-动作模型在零样本部署时行为脆弱、易失败的问题，提出VLAPS框架。该方法将基于模型的搜索嵌入VLA策略推理过程，核心是使用环境模型驱动、并由VLA策略动作先验引导的改进蒙特卡洛树搜索算法，以高效探索语言指定的任务空间。实验表明，该方法显著优于纯VLA基线，在语言指定任务上的成功率最高可提升67个百分点。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.12211" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>