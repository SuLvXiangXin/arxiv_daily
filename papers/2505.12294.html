<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PartDexTOG: Generating Dexterous Task-Oriented Grasping via Language-driven Part Analysis - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>PartDexTOG: Generating Dexterous Task-Oriented Grasping via Language-driven Part Analysis</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.12294" target="_blank" rel="noreferrer">2505.12294</a></span>
        <span>作者: Wu, Weishang, Shi, Yifei, Chen, Zhizhong, Cai, Zhipong</span>
        <span>日期: 2025/05/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>任务导向抓取是机器人操作中的关键且具有挑战性的任务。尽管近期取得了进展，但现有方法很少关注使用灵巧手进行任务导向抓取。灵巧手能提供更好的精度和多功能性，使机器人能更有效地执行任务导向抓取。然而，直接扩展现有的平行夹爪任务导向抓取方法到灵巧手是不可行的。首先，相比平行夹爪，灵巧手需要更高的自由度，这给网络设计带来了巨大挑战，尤其是在估计具有新几何形状物体的抓取姿态时。其次，灵巧操作的多功能性远超平行夹爪，使得针对平行夹爪的任务导向抓取数据集显得不足。</p>
<p>本文主张通过部件分析来辅助生成灵巧的任务导向抓取。部件是理解如何有效与物体交互的基本元素。同时，大语言模型在机器人操作领域的进展，得益于其开源世界知识推理能力，能够在未见过的场景中实现零样本泛化。通过结合LLMs，部件分析可以通过提供开源世界物体的部件级功能信息来增强机器人抓取，从而避免了对大规模抓取数据集训练的需求。本文提出了PartDexTOG，一种通过语言驱动的部件分析来生成灵巧任务导向抓取的方法。其核心思路是：给定3D物体和语言描述的操作任务，首先利用LLM生成针对该任务的类别级和部件级抓取描述；然后，基于这些描述，通过一个类别-部件条件扩散模型分别为每个部件生成灵巧抓取；最后，通过衡量抓取与部件之间的几何一致性，从生成的候选中选择最合理的抓取与部件组合。</p>
<h2 id="方法详解">方法详解</h2>
<p>PartDexTOG的整体流程如图2所示。输入为已知类别的3D物体点云和语言描述的操作任务，输出为针对该任务的灵巧手抓取姿态及被选中的物体部件。</p>
<p><img src="https://..." alt="方法总览图"></p>
<blockquote>
<p><strong>图2</strong>：PartDexTOG方法总览。给定3D物体和操作任务，方法首先通过LLM生成类别级抓取描述。然后，基于物体类别生成潜在的多尺度部件标签，并使用PartSLIP进行分割以识别有效的点云集合。接着，使用LLM为每个有效部件生成部件级抓取描述。随后，通过交叉注意力聚合描述的特征。这些语言特征与点云特征一同输入条件扩散模型，为每个有效部件分别生成灵巧抓取。最后，通过衡量抓取与部件间的几何一致性，选择最合理的抓取与部件组合。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><p><strong>抓取描述生成</strong>：</p>
<ul>
<li><strong>类别级描述生成</strong>：设计提示词，将物体类别和任务类型输入LLM（如GPT-4o），生成通用的抓取知识描述 <code>D[object_category, task_type]</code>（图3a）。这确保了抓取在类别层面的正确性。</li>
<li><strong>部件级描述生成</strong>：首先，通过另一组提示词（图3b）让LLM生成物体可能的多尺度部件标签列表（如“body”、“head”、“handle”等）。然后，使用PartSLIP部件分割模型，根据这些标签对输入物体的点云进行分割，过滤掉点数过少的部件，得到N个有效部件。最后，针对每个有效部件的标签，使用LLM生成专注于该部件几何形状及其在手-物交互中功能的描述 <code>D[part_label_n]</code>（图3c）。值得注意的是，此步骤不输入任务类型，使得部件描述更具通用性。</li>
</ul>
</li>
<li><p><strong>类别-部件语言特征聚合</strong>：使用预训练的BERT模型分别编码类别级描述和每个部件级描述，得到特征 <code>F_Cat</code> 和 <code>F_Part_n</code>。然后，通过交叉注意力机制（公式2）将类别级特征与每个部件级特征进行聚合，得到融合特征 <code>F_Cat-Part_n</code>。这有助于结合通用知识和具体部件信息。</p>
</li>
<li><p><strong>条件扩散模型生成抓取</strong>：采用去噪扩散概率模型。首先，使用PointNet++分别提取整个物体点云和每个部件点云的几何特征 <code>F_Obj-Geo</code> 和 <code>F_Part-Geo_n</code>。然后，将几何特征与聚合的语言特征 <code>F_Cat-Part_n</code> 拼接，形成条件特征 <code>F_n</code>（公式3）。该条件特征用于指导以MANO灵巧手参数 <code>g ∈ R^61</code>（包含姿态、形状和手掌坐标）为目标的扩散过程。训练时，损失函数为预测噪声与真实噪声的L1范数（公式6）。推理时，从高斯噪声开始，通过以 <code>F_n</code> 为条件的去噪U-Net网络，逐步生成每个部件对应的灵巧手抓取参数。</p>
</li>
<li><p><strong>基于几何一致性的部件选择</strong>：为每个部件生成抓取后，需要选择最合理的组合。由于缺乏抓取部件的直接监督标注，本文提出通过衡量生成的手部模型与部件点云之间的几何一致性来进行选择。具体计算手部表面点云与部件点云之间距离小于阈值λ的接触点比例 <code>E_n</code>（公式9）。选择 <code>E_n</code> 值最高的抓取-部件组合作为最终输出（图4）。</p>
</li>
</ol>
<p><img src="https://..." alt="几何一致性选择"></p>
<blockquote>
<p><strong>图4</strong>：通过衡量手部与部件之间的几何一致性（接触面积比例 <code>E_n</code>）来选择最合理的抓取。图中以“使用乳液泵”任务为例，最终选择了与“head”部件接触最紧密（<code>E_n=0.24</code>）的抓取。</p>
</blockquote>
<p>与现有方法相比，创新点具体体现在：1）<strong>利用LLM进行开源世界知识推理</strong>，无需针对大量特定物体-任务组合进行训练，即可生成具有语义合理性的抓取描述；2）<strong>多尺度部件分析</strong>，增强了方法对不同任务和物体的灵活性与泛化能力；3）<strong>无监督的部件选择机制</strong>，通过几何一致性度量替代了对抓取部件标注的依赖，使方法更通用。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：方法在OakInk-shape数据集（目前最大的灵巧任务导向抓取数据集）上进行训练和测试。该数据集包含50k个手-物姿态和模型，训练集和测试集分别包含923和176个物体，测试集包含未见过的类别，用于评估泛化能力。实验在NVIDIA GeForce RTX 4090 GPU上进行。</p>
<p><strong>对比的基线方法</strong>：包括GrabNet、IntGen、SceneDiffuser和DexGYSGrasp。</p>
<p><strong>关键实验结果</strong>：<br>如表1所示，PartDexTOG在OakInk-shape数据集上取得了最优的综合性能。</p>
<p><img src="https://..." alt="定量结果表1"></p>
<blockquote>
<p><strong>表1</strong>：在OakInk-shape数据集上的定量结果。PartDexTOG在抓取位移、接触率、P-FID、LLM评分、多样性及各项感知评分上表现最佳，在穿透体积和深度上也达到领先或可比水平。</p>
</blockquote>
<ul>
<li><strong>物理合理性</strong>：在衡量抓取稳定性的<strong>抓取位移</strong>指标（均值3.14 cm，方差3.61 cm）和衡量手-物交互的<strong>接触率</strong>（99.33%）上均优于所有基线。在<strong>穿透体积</strong>（4.74 cm³）上仅次于SceneDiffuser，但平衡了穿透避免和抓取稳定性。</li>
<li><strong>语义正确性</strong>：在<strong>P-FID</strong>（14.24）和<strong>LLM评分</strong>（70.5）上表现最佳，表明生成的抓取在语义上更符合任务要求。</li>
<li><strong>多样性与感知评分</strong>：<strong>多样性</strong>得分（0.993）最高，表明能生成更多样的抓取。用户研究的<strong>感知评分</strong>在语义一致性、物理合理性、交互稳定性和部件选择正确性四个子项上均获得最高分（4.68, 4.50, 4.15, 4.41），全面领先。</li>
</ul>
<p><img src="https://..." alt="定性对比图5"></p>
<blockquote>
<p><strong>图5</strong>：PartDexTOG与基线方法生成的任务导向抓取可视化对比。例如，对于“使用喷雾器”任务，PartDexTOG生成了握住喷嘴部位的合理抓取，而其他方法抓取位置或姿态不合理。</p>
</blockquote>
<p><strong>零样本泛化能力</strong>：<br>如表2所示，在未经训练的<strong>新任务</strong>和<strong>新类别</strong>物体上测试，PartDexTOG在穿透体积和抓取位移指标上均优于所有基线，展示了强大的零样本泛化能力。</p>
<p><img src="https://..." alt="零样本泛化表2"></p>
<blockquote>
<p><strong>表2</strong>：在新任务和新类别上的定量比较。PartDexTOG在两项指标上均表现最佳，证明了其语言驱动部件分析带来的泛化优势。</p>
</blockquote>
<p><img src="https://..." alt="零样本可视化图6"></p>
<blockquote>
<p><strong>图6</strong>：在新任务（a）和新类别（b）上生成抓取的可视化结果。即使对于训练中未见的“打开笔记本电脑”任务或“电钻”类别，方法也能生成合理的抓取。</p>
</blockquote>
<p><strong>部件选择效果</strong>：<br>如图7所示，对于相同任务类型“使用”，针对不同物体（喷雾器、酒瓶、锤子、牙刷），PartDexTOG能够自适应地选择不同的关键部件（如喷嘴、瓶身、手柄、刷柄）来生成合理的抓取。即使部件分割算法失败（如锤子未能正确分割出“head”），方法仍能基于其他有效部件生成正确抓取。</p>
<p><img src="https://..." alt="部件选择效果图7"></p>
<blockquote>
<p><strong>图7</strong>：部件选择效果可视化。对于“使用”任务，方法能为不同物体选择不同的合适部件生成抓取，并能在部件分割失败时保持鲁棒性。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>表3的消融研究量化了各关键组件的贡献。</p>
<p><img src="https://..." alt="消融实验表3"></p>
<blockquote>
<p><strong>表3</strong>：PartDexTOG关键组件的消融研究结果。移除任何核心组件都会导致性能下降，验证了各部分设计的必要性。</p>
</blockquote>
<ul>
<li><strong>移除类别级描述</strong>：导致物理指标（如穿透体积增至11.16）和语义指标（LLM评分降至36.5）显著恶化，说明通用类别知识对保证抓取基本正确性至关重要。</li>
<li><strong>移除部件级描述</strong>：虽然部分物理指标尚可，但语义一致性（SC感知评分3.41）和部件选择正确性（CP感知评分3.53）下降，表明部件级细节信息对精细化、任务相关的抓取生成很重要。</li>
<li><strong>移除多尺度部件</strong>：性能全面下降，特别是LLM评分（57.5）和多样性（0.985），证明多尺度分析能提升灵活性和生成质量。</li>
<li><strong>移除交叉注意力</strong>：各项指标均有退化，说明简单的特征拼接不如交叉注意力能有效融合类别与部件信息。</li>
<li><strong>移除几何一致性度量</strong>：抓取位移（均值4.72 cm）和部件选择正确性（CP感知评分2.76）大幅下降，凸显了该无监督选择机制对于筛选合理抓取-部件组合的关键作用。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个通过语言驱动部件分析生成灵巧任务导向抓取的框架，创新性地利用LLMs的开源世界知识推理能力，避免了大规模特定抓取数据集的训练需求。</li>
<li>设计了一系列LLM提示词，用于生成鲁棒的类别级和部件级抓取描述，并结合多尺度部件分割与条件扩散模型，实现了高质量、多样化的抓取生成。</li>
<li>提出了一种基于几何一致性的无监督部件选择机制，无需抓取部件标注即可筛选出最合理的抓取，增强了方法的实用性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法的性能部分依赖于部件分割算法（PartSLIP）的准确性。尽管实验显示在部件分割失败时方法仍有一定鲁棒性（图7），但分割错误理论上可能影响部件级描述的对齐和最终抓取质量。此外，LLM生成的描述可能存在偏差或不准确，虽然通过精心设计的提示词和特征聚合机制进行了缓解。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>融合多模态基础模型</strong>：可以探索结合视觉-语言模型，直接从物体图像或点云中提取或验证部件语义信息，减少对纯文本LLM描述的依赖，可能进一步提升对复杂或新颖物体的理解。</li>
<li><strong>闭环与物理仿真验证</strong>：将生成的抓取与物理仿真或真实机器人执行闭环结合，利用执行反馈来优化抓取生成模型或LLM的提示策略，是实现实际部署的重要方向。</li>
<li><strong>扩展到更复杂的操作序列</strong>：当前工作聚焦于单次抓取生成。未来可以以此为基础，结合任务规划，生成包含抓取、操纵、放置等步骤的完整操作序列，推动面向复杂任务的灵巧操作发展。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对灵巧手任务导向抓取中对象功能分析不足的难题，提出PartDexTOG方法。该方法通过语言驱动的部分分析，利用大型语言模型（LLMs）生成任务相关的类别与部分抓取描述，再基于类别-部分条件扩散模型为各部分生成灵巧抓取，并通过几何一致性度量选择最优组合。在OakInk-shape数据集上，该方法排名第一，Penetration Volume、Grasp Displace和P-FID分别提升3.58%、2.87%和41.43%，且在处理新类别和任务时表现出良好泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.12294" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>