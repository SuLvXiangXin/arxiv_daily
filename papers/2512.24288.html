<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Real-world Reinforcement Learning from Suboptimal Interventions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Real-world Reinforcement Learning from Suboptimal Interventions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.24288" target="_blank" rel="noreferrer">2512.24288</a></span>
        <span>作者: Jian Tang Team</span>
        <span>日期: 2025-12-30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，人机协作强化学习已成为解决复杂现实世界机器人操作任务的一种有前景的范式。现有方法通常将在线策略交互数据、在线人类干预数据和离线人类演示数据混合使用，并假设人类干预在整个状态空间中都是最优的。然而，这一假设忽略了现实：即使是专家操作者也无法在所有状态下始终如一地提供最优动作或完全避免错误。不加区分地将干预数据与机器人收集的数据混合，会继承RL本身的样本低效性；而纯粹模仿干预数据，则可能限制策略最终能达到的性能上限。因此，如何利用可能次优且带有噪声的人类干预来加速学习，同时又不被其限制，成为一个开放性问题。本文针对人类干预在不同状态下的不确定性（即次优性）这一具体痛点，提出了一个状态相关的约束优化新视角。核心思路是：将在线操作问题建模为一个约束RL优化问题，其中每个状态的约束边界由人类干预的不确定性决定，并通过引入状态相关的拉格朗日乘子，以最小-最大优化的方式自适应地平衡RL目标和模仿学习目标。</p>
<h2 id="方法详解">方法详解</h2>
<p>SiLRI的整体框架建立在一个“人作为副驾驶”的遥操作系统之上，其核心优化目标是将学习过程建模为一个约束优化问题，并利用状态相关的拉格朗日乘子进行求解。</p>
<p><img src="https://arxiv.org/html/2512.24288v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SiLRI框架概述。左侧：支持无缝人类干预的“人作为副驾驶”遥操作系统。右侧：SiLRI的整体优化目标示意图。在人类提供一致数据（低熵）的状态下，通过增大的拉格朗日乘子λ约束学习策略π靠近人类行为策略β；在人类干预不一致（高熵）的状态下，约束被放松，鼓励策略主要通过其自身估计的评论家（即RL目标）进行优化。</p>
</blockquote>
<p>具体而言，该方法将标准RL目标J(π)的最大化问题，转化为一个约束优化问题：在最大化J(π)的同时，要求学习策略π与从人类干预数据中推导出的随机参考策略β之间的距离不超过一个边界。为了解决确定性策略π与随机策略β之间距离度量的难题，并避免KL散度的无界性，论文将约束简化为：策略π输出的均值与策略β的均值之间的欧氏距离，应小于等于一个与β的标准差σβ成正比的阈值（κ·σβ）。这意味着，在人类行为一致（低方差）的状态下，约束较紧，策略应靠近人类示范；在人类行为不确定（高方差）的状态下，约束较松，策略可以自由探索。</p>
<p>为了求解此约束问题，论文引入了状态相关的拉格朗日乘子λ(s) ≥ 0，构建拉格朗日函数，并将其转化为一个最小-最大优化问题：对内层优化策略π以最小化包含RL损失和模仿损失（由λ加权）的联合目标；对外层优化λ以最大化对约束违反的惩罚。通过梯度更新共同寻找鞍点。</p>
<p><img src="https://arxiv.org/html/2512.24288v1/x3.png" alt="网络组件"></p>
<blockquote>
<p><strong>图3</strong>：SiLRI中的网络组件。网络Q（评论家）、π（演员）和λ（拉格朗日乘子）在数据收集过程中使用在线缓冲区异步更新；而网络β（行为策略）则在固定数量的新样本被添加到干预缓冲区后定期更新。</p>
</blockquote>
<p>方法包含四个核心网络模块：</p>
<ol>
<li>**演员网络 (π)**：输出确定性动作。其损失函数结合了RL目标（最大化Q值）和模仿目标（最小化与β均值的距离），由状态相关的λ(s)动态加权：ℒ(θ^π) = E_s[1/(λ(s)+1) * (-Q(s, π(s)) + λ(s) * ||π(s) - β(s)||^2)]。当λ(s)很大时，模仿损失主导，迫使策略跟随人类；当λ(s)趋近于0且满足约束时，RL目标主导，允许策略超越人类行为。</li>
<li>**拉格朗日乘子网络 (λ)**：输入状态，输出非负标量值（通过Softplus激活）。其损失函数为：ℒ(θ^λ) = E_s[-λ(s) * ( D(π, β) - κ·σ_β - c )]，其中D(π, β)是π与β均值间的平方距离，c是一个小的松弛常数。该设计使得当策略π偏离β的程度超过允许的容差（κ·σ_β + c）时，λ会增大以加强约束；当满足约束时，λ会衰减。</li>
<li>**行为策略网络 (β)**：从人类干预数据中学习，建模为一个多元高斯策略（输出均值和方差）。通过标准的行为克隆损失ℒ(θ^β) = E_(s,a)~D_I[-log β(a|s)]进行优化，并采用较低的更新频率以防止其熵（方差）过早衰减，保持策略多样性。</li>
<li>**评论家网络 (Q)**：采用双Q网络结构以稳定训练，通过标准的时序差分贝尔曼误差进行优化。</li>
</ol>
<p>与现有方法相比，SiLRI的核心创新在于：1) 明确地将人类干预的不确定性（通过β的方差σ_β量化）纳入约束边界，使约束具有状态自适应性；2) 引入可学习的、状态相关的拉格朗日乘子网络，通过最小-最大优化自动、动态地调整每个状态下RL与模仿学习的权衡，而非使用固定的启发式权重。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两种机器人实体（Franka Emika Panda 和 Unitree H1）上进行了八项具有挑战性的真实世界操作任务评估。</p>
<p><img src="https://arxiv.org/html/2512.24288v1/fig/Experiment_Task.jpg" alt="实验任务"></p>
<blockquote>
<p><strong>图4</strong>：两个机器人实体上的八项真实世界操作任务。(A) 放置面包 (B) 拾起勺子 (C) 折叠抹布 (D) 打开柜门 (E) 关闭垃圾桶 (F) 推动T型物体 (G) 悬挂中国结 (H) 插入USB。</p>
</blockquote>
<p>对比的基线方法包括：1) <strong>在线模仿学习方法</strong>：HACT；2) <strong>最先进的真实世界RL方法</strong>：HIL-SERL和ConRFT。所有方法共享相同的基础网络架构、奖励函数和人机协作系统。</p>
<p><img src="https://arxiv.org/html/2512.24288v1/x4.png" alt="成功率曲线"></p>
<blockquote>
<p><strong>图5</strong>：在八个真实世界任务上的成功率对比曲线。SiLRI（红色实线）在大多数任务上收敛速度最快，最终性能最高，特别是在长视野任务（如悬挂中国结、插入USB）上表现突出。</p>
</blockquote>
<p>关键实验结果如下：SiLRI在达到90%成功率所需的时间上，相比当前最优的RL方法HIL-SERL<strong>至少减少了50%<strong>（对应节省约18分钟）。在长视野、高精度的“悬挂中国结”和“插入USB”任务中，SiLRI实现了</strong>100%的成功率</strong>，而其他RL方法（HIL-SERL, ConRFT）则难以成功或成功率很低。在线模仿学习方法HACT由于无法超越演示数据，在所有任务上的最终性能均存在上限。</p>
<p><img src="https://arxiv.org/html/2512.24288v1/x5.png" alt="训练时间对比"></p>
<blockquote>
<p><strong>图6</strong>：达到90%成功率所需训练时间的定量对比。柱状图清晰显示，SiLRI在所有任务上所需时间最短，相比HIL-SERL有显著提升（至少50%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.24288v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验。对比了SiLRI与其变体：1) 使用固定标量拉格朗日乘子（Fixed-λ）；2) 不使用拉格朗日乘子，即简单的RL+BC混合（W/o λ）。结果表明，状态相关的λ和最小-最大优化机制对性能提升至关重要。</p>
</blockquote>
<p>消融实验验证了核心组件的贡献：</p>
<ol>
<li><strong>状态相关 vs. 固定拉格朗日乘子</strong>：使用固定标量λ的方法（Fixed-λ）性能显著下降，证明了根据状态不确定性自适应调整约束强度的必要性。</li>
<li><strong>最小-最大优化机制</strong>：移除λ及其优化（W/o λ），退化为简单的RL与模仿损失加权求和，性能也较差，说明通过优化动态平衡两个目标是有效的。</li>
<li><strong>约束形式</strong>：论文附录中的实验表明，使用简化后的欧氏距离约束（公式3）比直接使用KL散度约束（公式2）在实践中更稳定、更有效。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.24288v1/x7.png" alt="学习过程可视化"></p>
<blockquote>
<p><strong>图8</strong>：在“悬挂中国结”任务训练过程中，λ(s)值随时间的演化热图。可以看到，在任务初始阶段（拾取），λ值较高（加强模仿），而在需要高精度对准的困难阶段，λ值降低（放松约束，鼓励RL探索）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.24288v1/x8.png" alt="定性结果"></p>
<p><img src="https://arxiv.org/html/2512.24288v1/x9.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图9与图10</strong>：定性结果展示。SiLRI学习到的策略能够成功完成复杂的、需要多步推理和精确对准的长视野任务，例如将中国结挂到钩子上并将USB插入插槽，而基线方法在这些任务中常常失败。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>问题建模创新</strong>：首次明确地将利用次优人类干预的问题形式化为一个状态相关的约束RL优化问题，其中约束边界由人类不确定性决定；2) <strong>算法设计创新</strong>：提出了SiLRI算法，通过引入可学习的、状态相关的拉格朗日乘子，并以最小-最大优化的方式联合优化策略与乘子，实现了RL与模仿学习目标的自适应、状态感知平衡；3) <strong>实证效果显著</strong>：在真实的机器人操作任务上验证了算法的有效性，显著提升了样本效率，并能在长视野任务上实现超越人类演示的100%成功率。</p>
<p>论文自身提到的局限性包括：1) <strong>干预时机的敏感性</strong>：训练效率在一定程度上依赖于人类操作者选择何时进行干预的“标准操作流程”，过于频繁或不当的干预可能降低效率；2) <strong>模型架构依赖</strong>：当前方法基于相对简单的网络架构，未来需要探索如何与更大的视觉语言动作模型结合。</p>
<p>本文对后续研究的启示在于：为处理现实世界中不完美的人类监督信号提供了一个新颖的、原则性的框架。其状态相关约束和自适应权衡的思想，可扩展至更广泛的人机交互、离线强化学习以及安全强化学习场景中，其中数据的质量或安全性约束可能随状态而变化。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现实世界强化学习中人类干预次优且有噪声的问题，提出SiLRI算法。该方法将在线操作任务建模为约束强化学习优化，约束边界由人类干预的不确定性决定，并引入状态-wise拉格朗日乘子，通过min-max优化联合学习策略与乘子。实验表明，SiLRI能有效利用次优干预，相比先进方法HIL-SERL，达到90%成功率所需时间减少至少50%，并在长时域任务中实现100%成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.24288" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>