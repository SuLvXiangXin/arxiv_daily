<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Agent Learning via Early Experience - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Agent Learning via Early Experience</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.08558" target="_blank" rel="noreferrer">2510.08558</a></span>
        <span>作者: Yifan Wu Team</span>
        <span>日期: 2025-10-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前构建语言智能体的主流方法是基于专家演示数据进行监督微调（SFT），即模仿学习。这种方法虽然直接且免于奖励信号，但存在关键局限性：专家数据仅覆盖有限场景，智能体在训练中不与环境交互，无法从自身行动的结果中学习，导致泛化能力差，且高质量专家数据的获取成本高昂、难以扩展。另一方面，强化学习（RL）作为理想的从经验中学习的范式，在许多现实语言智能体环境中难以应用，因为这些环境要么缺乏可验证的奖励信号（如网站操作），要么任务序列过长导致训练低效不稳定。本文针对“如何在缺乏外部奖励信号的情况下，让智能体从其自身经验中学习”这一具体痛点，提出了“早期经验”这一介于模仿学习和强化学习之间的新范式。其核心思路是：智能体通过探索提出替代行动，收集由此产生的未来状态，并将这些状态作为无需奖励的监督信号，用以改进策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>早期经验范式的整体流程始于一个专家数据集 D_expert = {(s_i, a_i)}。在此基础之上，对于数据集中的每个状态 s_i，使用一个初始策略（如经过SFT的LLM）采样 K 个不同于专家行动 a_i 的替代行动 a_i^j。随后，在真实环境中执行这些替代行动，观察并记录产生的下一个状态 s_i^j，从而构建出探索数据集 D_rollout = {(s_i, a_i^j, s_i^j)}。这个数据集捕获了智能体自身行动带来的环境反馈，构成了“早期经验”。本文在此范式下探索了两种将经验转化为监督信号的策略。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/7f0d1a0f0f8348f3b2b2e6b6b6b6b6b6.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：语言智能体训练范式的演进。左：依赖专家演示的“人类数据时代”，无需奖励但数据难以扩展。右：依赖可验证奖励进行强化学习的“经验时代”，但许多环境缺乏此类奖励。中：本文提出的“早期经验”范式，智能体通过自身行动收集未来状态，作为一种可扩展且无需奖励的监督源。</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/direct/7f0d1a0f0f8348f3b2b2e6b6b6b6b6b6.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：两种早期经验方法概述。隐式世界建模（左）用替代行动和预测的下一个状态增强专家轨迹，训练策略内化环境动态。自我反思（右）用自我生成的解释 c_i^j 增强专家行动，训练策略对其决策进行推理和修正。两种方法都使用初始策略（LLM）提出的替代行动。</p>
</blockquote>
<p><strong>1. 隐式世界建模</strong>：该方法将未来状态预测构建为一个辅助任务，使策略直接从早期经验中内化环境动态。具体而言，对于 D_rollout 中的每个三元组 (s_i, a_i^j, s_i^j)，训练目标是最小化下一个状态 s_i^j 的负对数似然，即 L_IWM = -Σ log p_θ(s_i^j | s_i, a_i^j)。这里的状态和行动都以自然语言表示，因此该目标可无缝集成到LLM的下一个词预测框架中。通过让策略学习预测自身行动的结果，模型能够捕捉环境中的常见转移、副作用和无效行动结果，从而增强对分布偏移的鲁棒性。实践中采用两阶段流程：先使用 L_IWM 在 D_rollout 上训练以学习粗略动态，再在 D_expert 上进行监督微调。</p>
<p><strong>2. 自我反思</strong>：该方法引导智能体从其探索结果中进行对比学习。对于每个状态 s_i，除了专家行动 a_i 及其结果状态 s_{i+1}，还考虑每个替代行动 a_i^j 及其结果状态 s_i^j。然后，使用一个语言模型（可以是初始策略本身）生成一段链式推理文本 c_i^j，解释基于结果状态的差异，为什么专家行动 a_i 优于替代行动 a_i^j。这些生成的反思 (s_i, a_i^j, c_i^j) 构成数据集 D_refl。最终的训练目标是让模型根据状态 s_i，联合预测反思文本 c_i^j 和专家行动 a_i，即 L_SR = -Σ log p_θ(c_i^j, a_i | s_i)。这种方法将具体的状态-行动对升级为可迁移的决策原理，例如在购物任务中学习优先考虑预算约束。</p>
<p>两种方法的创新点在于，它们都利用智能体自身行动产生的未来状态作为核心监督信号，无需环境提供任何奖励函数，从而在模仿学习的可操作性和强化学习的经验驱动学习之间架起了一座桥梁。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在八个多样化的语言智能体环境中进行，涵盖具身导航（ALFWorld）、科学模拟（ScienceWorld）、长程规划（TravelPlanner）、多轮工具使用（BFCLv3, Tau-Bench, SearchQA）和网页导航（WebShop, WebArena-Lite）。使用了不同规模的模型（如Llama-3.1-8B/70B, Qwen2.5-7B）作为基础。对比的基线是标准的模仿学习（仅使用D_expert进行SFT）。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/7f0d1a0f0f8348f3b2b2e6b6b6b6b6b6.png" alt="实验结果"></p>
<blockquote>
<p><strong>图3</strong>：在八个环境上的主要结果。条形图显示，隐式世界建模（IWM）和自我反思（SR）方法在所有环境中都一致且显著地优于纯模仿学习（IL）基线。</p>
</blockquote>
<p>关键定量结果显示，两种早期经验方法在所有八个环境中都一致超越了纯模仿学习基线。例如，在ALFWorld上，IWM和SR相比IL分别将成功率绝对提升了6.2%和5.6%；在WebShop上，分别提升了4.9%和7.8%。这表明从早期经验中学习能普遍提升任务有效性。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/7f0d1a0f0f8348f3b2b2e6b6b6b6b6b6.png" alt="实验结果"></p>
<blockquote>
<p><strong>图4</strong>：数据效率与泛化分析。左图显示，在WebShop上，仅使用25%的专家数据，IWM和SR就能达到与使用100%专家数据的IL基线相当或更好的性能。右图显示，在SearchQA的域外泛化测试中，早期经验方法显著优于IL基线。</p>
</blockquote>
<p>在数据效率方面，实验表明早期经验方法能用更少的专家数据达到相同或更好的性能。例如在WebShop上，仅使用25%的专家数据，IWM和SR的性能即可与使用100%专家数据的IL基线媲美。在域外泛化测试（如在SearchQA上使用Musique训练，在HotpotQA等数据集上测试）中，早期经验方法也显著优于IL基线，显示了其更好的泛化能力。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/7f0d1a0f0f8348f3b2b2e6b6b6b6b6b6.png" alt="实验结果"></p>
<blockquote>
<p><strong>图5</strong>：作为强化学习预训练阶段的消融实验。在ALFWorld和ScienceWorld这两个有可验证奖励的环境上，使用早期经验方法（IWM或SR）预训练的检查点，再进行PPO训练，其最终性能显著优于使用标准IL预训练的检查点。</p>
</blockquote>
<p>对于存在可验证奖励的环境，研究进一步将早期经验训练作为强化学习（PPO）的预训练阶段。消融实验表明，与使用标准IL检查点进行热启动相比，使用IWM或SR预训练的检查点能带来显著更强的最终RL性能。例如在ALFWorld上，IWM+PPO比IL+PPO成功率绝对提升8.3%。这证明了早期经验为后续RL提供了更优的起点。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）正式提出并形式化了“早期经验”这一新的训练范式，作为连接模仿学习与强化学习的实用桥梁；2）在该范式下提出了两种具体方法——隐式世界建模和自我反思，它们能将智能体自身行动产生的未来状态转化为有效的监督信号；3）通过覆盖八个多样化环境的大规模实验，实证了该方法在提升任务性能、数据效率、泛化能力以及为RL奠定基础方面的普遍有效性。</p>
<p>论文自身提到的局限性包括：方法需要与环境进行交互以收集早期经验，这可能带来计算成本；对于某些环境，生成高质量的自我反思文本可能存在挑战。</p>
<p>这项工作对后续研究的启示是深远的。它指明了一条不依赖稠密奖励即可实现智能体自我改进的路径。未来研究可以探索更高效的早期经验采样策略、将隐式世界建模与显式规划相结合，或者将早期经验范式与更先进的RL算法进行深度融合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出“早期经验”训练范式，以解决语言智能体在缺乏可验证奖励或需长序列交互的环境中难以从自身经验学习的问题。该方法利用智能体自主交互产生的状态作为监督信号，无需外部奖励。关键技术包括：1）隐式世界建模，利用收集的环境状态使策略适应动态；2）自我反思，从次优行动中学习以改进决策。在八个多样环境上的实验表明，该方法能持续提升智能体效能与跨领域泛化能力，并为后续强化学习提供了有效基础。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.08558" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>