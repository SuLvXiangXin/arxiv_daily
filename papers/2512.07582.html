<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.07582" target="_blank" rel="noreferrer">2512.07582</a></span>
        <span>作者: Yufeng Yue Team</span>
        <span>日期: 2025-12-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过在大规模机器人数据集上训练，展现出端到端机器人控制的潜力，但泛化到其训练分布之外的任务仍然是一个重大挑战。相比之下，人类具备仅通过观察他人一次演示就能高效学习新技能的非凡能力。受此启发，本文旨在研究一种通用的机器人策略，使机器人能够在测试时仅通过观察一次专家演示视频，无需额外训练，即可学习训练分布之外的新任务。本文的核心思路是：提出名为ViVLA的VLA新范式，通过联合处理专家演示视频和机器人视觉观察，预测演示动作序列和后续机器人动作，从而从专家行为中提取细粒度操作知识并无缝转移给智能体。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViVLA的整体框架包含两个关键阶段：（I）基于动作中心循环一致性（A3C）的潜在动作学习；（II）用于一次性任务学习的ViVLA训练。模型在训练时学习基于单个专家演示视频预测后续机器人动作，从而在测试时获得从单个专家演示视频学习新任务的能力。</p>
<p><img src="https://arxiv.org/html/2512.07582v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：ViVLA方法总览。（I）潜在动作分词器（LAT）从观察序列中学习量化的潜在动作，为专家视频和智能体演示获取潜在动作。（II）ViVLA模型被训练来预测学习到的潜在动作序列和后续机器人动作，使机器人能够在测试时仅通过单个专家演示视频获取新操作技能。</p>
</blockquote>
<p><strong>核心模块一：带循环一致性的潜在动作学习</strong><br>此阶段目标是学习一个统一的潜在动作空间，弥合专家（如人）与智能体（机器人）之间的具身鸿沟。潜在动作分词器采用编码器-解码器架构。编码器使用DINOv2提取当前帧和未来帧的嵌入，与可学习的潜在动作令牌拼接后，经由时空（ST）Transformer建模帧间动态，最终输出被量化为离散的潜在动作表示。解码器则根据当前帧和潜在动作重建未来帧。</p>
<p><img src="https://arxiv.org/html/2512.07582v1/x4.png" alt="潜在动作框架"></p>
<blockquote>
<p><strong>图4</strong>：带动作中心循环一致性的潜在动作框架示意图。该方法从观察帧中学习潜在动作表示，同时引入动作中心循环一致性约束以建立统一的潜在动作空间。</p>
</blockquote>
<p>关键创新在于引入了<strong>动作中心循环一致性</strong>约束。方法维护一个潜在动作缓冲区，从中采样一个潜在动作，与一个观察帧一起解码生成一个未来帧。然后，编码器被训练从原始观察帧和生成帧中恢复出这个采样的潜在动作。此约束确保了潜在动作的语义一致性（相同的动作应产生相似的变化）和跨具身统一性（不同具身的相似动作应在潜在空间中对齐），如图3所示，解决了现有方法潜在动作语义不一致、空间割裂的问题。</p>
<p><strong>核心模块二：ViVLA训练</strong><br>训练时，ViVLA的输入包括：经过时间-空间掩码策略处理的专家演示视频、语言指令以及智能体的当前观察图像。模型被训练同时输出两个序列：专家视频对应的潜在动作序列，以及智能体后续应执行的潜在动作序列。</p>
<p><img src="https://arxiv.org/html/2512.07582v1/x5.png" alt="ViVLA训练"></p>
<blockquote>
<p><strong>图5</strong>：ViVLA示意图。该方法联合处理专家演示视频和机器人的视觉观察，以预测演示动作序列和后续机器人动作，促使ViVLA从专家行为中提取细粒度操作知识并无缝转移给智能体。</p>
</blockquote>
<p>主要技术细节包括：</p>
<ol>
<li><strong>并行解码</strong>：模型接收空的动作嵌入作为输入，在单次前向传播中并行生成所有动作令牌。这避免了自回归解码中利用历史真实动作令牌导致的“短路学习”问题，迫使模型基于对视频内容和智能体观察的深入分析进行预测，并显著提升了推理效率。</li>
<li><strong>时间-空间掩码</strong>：在训练时随机掩码专家视频的令牌（跨越时间和空间维度）。这降低了计算复杂度，并创建了一个更具挑战性的学习目标，要求模型从部分观察到的演示中进行动作预测，从而促进对视频的整体理解。</li>
<li><strong>细粒度动作推理</strong>：在生成机器人控制输出之前，模型被训练显式地阐述（预测）演示视频中观察到的操作动作，这增强了其细粒度动作识别和理解能力。</li>
<li><strong>时间定位任务</strong>：将智能体的观察图像插入演示视频序列，训练模型识别这些观察在视频中的时间位置，促进了视频和图像表征之间的跨模态信息交换。</li>
</ol>
<p><strong>数据生成流程</strong><br>为训练ViVLA，本文开发了一个可扩展的专家-智能体配对数据生成流程。该流程以易于获取的人类视频为输入，通过3D高斯泼溅渲染出执行相同任务的机器人演示过程，从而生成大量的人类-机器人配对轨迹样本。此外，还从公开数据集中构建了任务相似的配对。最终，共汇总了892,911个专家-智能体配对样本用于训练。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验主要在LIBERO基准测试中进行，并在真实世界进行了验证。对比的基线方法包括：OSVI-WM、AWDA、T-OSVI、LAPA、UniVLA以及RT-2、OpenVLA等VLA模型。</p>
<p><img src="https://arxiv.org/html/2512.07582v1/x6.png" alt="模拟实验结果"></p>
<blockquote>
<p><strong>图6</strong>：在LIBERO基准测试上的成功率。ViVLA在未见任务上显著优于所有基线方法。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ul>
<li><strong>LIBERO未见任务</strong>：ViVLA取得了<strong>46.7%</strong> 的成功率，相比最佳基线OSVI-WM（15.8%）提升了<strong>超过30个百分点</strong>。</li>
<li><strong>跨具身泛化</strong>：当测试时提供的专家演示视频来自不同具身（如从机器人演示训练，但给人视频测试）时，ViVLA仍能保持<strong>35.0%</strong> 的成功率，相比OSVI-WM（0%）有巨大提升。</li>
<li><strong>真实世界未见任务</strong>：ViVLA从人类视频中有效提取知识，在真实机器人上执行未见任务的成功率达到**38.5%**，显著高于基线方法。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.07582v1/x7.png" alt="跨具身与真实世界结果"></p>
<blockquote>
<p><strong>图7</strong>：（左）使用跨具身演示视频（人类视频）在LIBERO上的成功率。（右）在真实世界未见任务上的成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07582v1/x8.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融研究结果。A3C（动作中心循环一致性）、PD（并行解码）和TSM（时间-空间掩码）每个组件都对性能有重要贡献。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>动作中心循环一致性（A3C）</strong>：是性能提升的关键，移除此组件成功率下降21.7%。</li>
<li><strong>并行解码（PD）</strong>：解决了短路学习问题，将其换为自回归解码会导致性能下降10.0%。</li>
<li><strong>时间-空间掩码（TSM）</strong>：促进了更好的视频理解，移除后性能下降8.4%。</li>
<li><strong>细粒度动作（FA）与时间定位（TL）目标</strong>：二者共同作用提升了模型对视频细节的理解。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.07582v1/x9.png" alt="定性结果"></p>
<blockquote>
<p><strong>图9</strong>：ViVLA在LIBERO基准测试中成功与失败轨迹的定性示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07582v1/x10.png" alt="潜在动作空间可视化"></p>
<blockquote>
<p><strong>图10</strong>：潜在动作空间可视化（t-SNE）。ViVLA学习的潜在动作空间在跨具身（人类与机器人）上显示出更好的对齐和一致性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07582v1/x11.png" alt="数据量影响"></p>
<blockquote>
<p><strong>图11</strong>：训练数据量对性能的影响。使用全量数据（892K样本）训练的模型性能最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07582v1/x12.png" alt="推理速度对比"></p>
<blockquote>
<p><strong>图12</strong>：与自回归基线相比，ViVLA的并行解码显著提高了推理速度（快约7倍）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：1）提出了ViVLA新范式，使策略模型能够在测试时从单次演示中学习新技能，无需微调；2）引入了带循环一致性约束的潜在动作学习框架和并行解码机制，建立了统一的跨具身动作表示并解决了短路学习问题；3）开发了可扩展的专家-智能体配对数据生成流程，构建了大规模数据集；4）实验验证了该方法在模拟和真实环境中从单次演示（尤其是跨具身人类视频）学习未见任务的有效性，性能提升显著。</p>
<p>论文未明确阐述局限性，但大规模配对数据的生成依赖特定流程，且模型可能对演示视频的质量和视角有一定要求。本文的启示在于：为机器人学习建立统一的、语义一致的跨具身动作表示是实现高效知识迁移的关键；并行解码策略为改进VLA模型的训练和推理效率提供了新思路；利用易于获取的人类视频通过技术手段生成机器人训练数据，是解决机器人数据稀缺问题的一条可行路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ViVLA模型，旨在解决机器人操作策略难以泛化至训练分布外新任务的问题。核心方法是构建一个视觉-语言-动作模型，使其能够通过联合处理单次专家演示视频与机器人实时观测，学习并预测动作序列，从而从一次演示中提炼精细操作知识。关键技术包括一个可扩展的专家-智能体配对数据生成流程，用于合成大规模训练数据。实验表明，ViVLA仅凭一次演示视频即可学习新技能，在未见过的LIBERO任务上性能提升超过30%，使用跨体现视频时增益保持在35%以上，真实世界实验中对未见任务的改进超过38%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.07582" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>