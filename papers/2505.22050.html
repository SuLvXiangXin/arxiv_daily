<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Reinforced Reasoning for Embodied Planning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Reinforced Reasoning for Embodied Planning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.22050" target="_blank" rel="noreferrer">2505.22050</a></span>
        <span>作者: Wu, Di, Fan, Jiaxin, Zang, Junzhe, Wang, Guanbo, Yin, Wei, Li, Wenhao, Jin, Bo</span>
        <span>日期: 2025/05/28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>具身规划要求智能体基于动态视觉观察和自然语言目标，做出连贯的多步决策。尽管当前视觉语言模型在静态感知任务上表现出色，但在交互式环境中进行规划所需的时间推理、空间理解和常识基础方面存在明显不足。现有方法主要依赖于精心设计的提示或监督微调，但大多在静态或离线环境中生成计划，缺乏与动态环境的交互，因此在空间基础和时间连贯性上表现不佳。近期，以DeepSeek-R1为代表的强化驱动推理范式在数学和代码问题上展现出增强模型推理能力的潜力，但将其应用于需要空间感知、物理常识和交互式多轮决策的具身规划任务仍面临巨大挑战。</p>
<p>本文针对具身规划中模型推理能力不足的痛点，提出了一个强化微调框架，首次将R1风格的推理增强引入具身规划领域。核心思路是通过一个两阶段的训练范式：先利用大模型蒸馏数据进行监督微调，为模型注入结构化决策先验；再设计面向多步决策质量的规则奖励函数，通过广义强化偏好优化来提升模型在动态环境中的长期、目标导向的推理能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个两阶段的训练框架：监督微调阶段，后接强化微调阶段，以增强视觉语言模型的多步规划能力。</p>
<p><img src="https://arxiv.org/html/2505.22050v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：提出的框架概览。采用包含监督微调和强化微调的两阶段训练范式，以增强视觉语言模型的多步规划能力。最终模型在交互式具身基准Embench上进行评估。</p>
</blockquote>
<p><strong>问题定义</strong>：将具身任务规划定义为部分可观测的决策过程。在每一步t，智能体接收观测o_t，执行动作a_t，形成历史h_t。给定自然语言指令L描述的任务目标g，其成功需满足一组二元目标检查条件C(g)。策略π_θ由视觉语言模型参数化，输出基于观测o_t、历史h_t、指令L和固定提示模板P的动作分布。目标是优化θ以最大化采样轨迹的期望任务成功率。</p>
<p><strong>第一阶段：蒸馏监督微调</strong>。由于具身规划存在多种有效轨迹，人工标注成本高。因此，本文采用蒸馏方法，使用强大的闭源模型Gemini-2.0-flash来解决具身规划任务，并记录其输出以构建SFT数据集。数据形式为提示-响应对(p_i, â_i)，其中â_i包含规划轨迹和关联的推理过程。总共收集了超过4000个训练样本，用于监督开源模型Qwen2.5-VL。通过最大似然估计进行优化，损失函数为L_SFT(θ) = -E_{(p,â)~D_SFT}[log π_θ(â|p)]。实验发现全参数微调比参数高效的LoRA微调效果略好。此阶段使模型继承了大模型的任务分解模式、常识先验和空间基础能力。</p>
<p><strong>第二阶段：强化推理微调</strong>。为了提升在未见场景下的推理泛化能力，本文提出了一个用于长视野具身规划的强化微调框架。首先在ALFRED基准上构建数据集，将每个参考轨迹e分解为多个训练样本。每个样本是一个三元组(L, o, â)，其中L嵌入了任务目标g和之前的动作历史，o是当前步的观测，â是从当前步开始的剩余动作序列。总共收集了43,898个训练样本。</p>
<p><strong>奖励函数设计</strong>：设计了一个复合奖励函数，整合了格式正确性和动作准确性，以引导模型产生结构化、有效且高效的多步计划。总奖励R = R_format + R_accuracy。</p>
<ol>
<li><strong>格式奖励R_format</strong>：针对具身多步规划所需的结构化输出量身定制。期望响应包含一个具有特定键的JSON对象：<code>reasoning_and_reflection</code>、<code>visual_state_description</code>、<code>language_plan</code>和<code>executable_plan</code>。格式奖励由三部分组成：R_structure（检查顶级字段是否存在）、R_valid（检查每一步是否包含有效的action_id和action_name）、R_match（检查(action_id, action_name)对是否与预定义的动作映射一致）。</li>
<li><strong>准确性奖励R_accuracy</strong>：以前缀匹配的方式，将预测动作序列â与参考（黄金）动作序列a*进行比较。令n表示连续匹配的步数（前缀长度），准确性奖励定义为R_accuracy = R(n; k)，其中R(n; k)由多步奖励分配曲线计算。对于单步任务，若模型生成多于一步，则施加-0.25的惩罚。</li>
<li><strong>多步奖励分配曲线</strong>：为反映长视野规划质量，定义了一个渐进式奖励分配曲线，对更长的正确前缀赋予更高奖励。计算公式为R(n; k) = n(n+1) / k(k+1)。该函数随n二次增长，归一化到[0,1]范围，鼓励模型不仅预测正确的最终结果，而且在整个动作序列中保持一致性和正确性。</li>
</ol>
<p><strong>优化方法</strong>：采用分组相对策略优化（GRPO）在基于奖励的监督下优化VLM策略。对于提示x，策略模型π_θ生成G个采样响应{y_1, ..., y_G}。每个响应y_i由奖励函数r_i = R(y_i)评分。GRPO计算每个响应的相对优势A_i，即其与组内平均值的归一化偏差。训练目标鼓励模型在保持与参考策略π_ref稳定性的同时，增加高质量响应的似然。GRPO损失函数包含裁剪后的策略比率与优势的乘积，以及一个KL散度正则项。</p>
<p>此外，框架还采用了<strong>在线数据过滤策略</strong>以提高训练稳定性：在每次训练迭代中，从数据集中采样一批提示，生成响应并计算奖励，然后仅保留奖励高于阈值τ的样本用于梯度更新，从而过滤掉低质量数据。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>评估基准与基线</strong>：实验在Embench基准上进行评估，这是一个用于交互式具身任务的基准，覆盖领域内和领域外场景。对比的基线模型包括：闭源模型GPT-4o-mini，以及多个开源模型（Qwen2.5-VL-7B-Instruct, Qwen2.5-VL-72B-Instruct, Llama-3.2-11B-Vision-Instruct, InternVL2-8B, InternVL2-26B）。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2505.22050v2/x3.png" alt="主要结果"></p>
<blockquote>
<p><strong>图3</strong>：在Embench的EB-ALFRED和EB-TEACh环境上的主要结果。柱状图显示了领域内（ID）和领域外（OOD）场景下的任务成功率。提出的方法（Ours）在几乎所有设置中都取得了最佳性能。</p>
</blockquote>
<p>在EB-ALFRED环境中，本文方法在领域内（ID）测试集上取得了**81.25%<strong>的成功率，显著高于基线中最好的Qwen2.5-VL-72B（71.88%）和GPT-4o-mini（68.75%）。在领域外（OOD）测试集上，本文方法达到</strong>68.75%<strong>，同样优于所有基线（最佳基线为56.25%）。在EB-TEACh环境中，本文方法在ID和OOD测试集上的成功率分别为</strong>52.38%<strong>和</strong>50.00%**，均优于其他对比模型。</p>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2505.22050v2/x4.png" alt="消融研究"></p>
<blockquote>
<p><strong>图4</strong>：消融研究。评估了监督微调、强化微调以及在线过滤策略各自对最终性能的贡献。SFT和RFT都带来了显著的性能提升，而在线过滤进一步提高了稳定性与效果。</p>
</blockquote>
<p>消融实验表明：1) 仅使用SFT（Qwen2.5-VL-7B + SFT）相比原始模型（Qwen2.5-VL-7B）在EB-ALFRED ID上带来了超**30%<strong>的绝对提升（从40.63%到71.88%）。2) 在SFT基础上增加RFT（Qwen2.5-VL-7B + SFT + RFT）进一步将成功率提升至</strong>81.25%**。3) 在RFT中引入在线数据过滤（Ours，即Qwen2.5-VL-7B + SFT + RFT w/ filter）带来了额外的稳定性和小幅性能增益。</p>
<p><strong>泛化能力分析</strong>：<br><img src="https://arxiv.org/html/2505.22050v2/x5.png" alt="泛化分析"></p>
<blockquote>
<p><strong>图5</strong>：泛化分析。展示了模型在训练中未见过的环境（OOD）上的表现，与领域内（ID）性能对比。本文方法在OOD场景下保持了较高的性能，下降幅度相对较小，显示了良好的泛化能力。</p>
</blockquote>
<p>本文方法在OOD场景下的性能下降相对温和（例如EB-ALFRED从81.25%降至68.75%），而某些基线模型（如InternVL2系列）的下降更为剧烈，这表明强化微调有效增强了模型对未知任务的泛化能力。</p>
<p><strong>失败案例分析</strong>：<br><img src="https://arxiv.org/html/2505.22050v2/x1.png" alt="失败案例"></p>
<blockquote>
<p><strong>图1</strong>：GPT-4o-mini在EB-ALFRED环境中的失败案例及错误分解。左：一个代表性的任务失败示例。右：EB-ALFRED任务中失败类型的分布。</p>
</blockquote>
<p>对GPT-4o-mini的失败案例分析显示，主要错误类型包括：<strong>无效动作</strong>（33%）、<strong>错误的对象交互</strong>（27%）、<strong>逻辑错误</strong>（20%）、<strong>重复动作</strong>（13%）和<strong>幻觉</strong>（7%）。这凸显了现有VLMs在具身规划中面临的核心挑战。</p>
<p><strong>定性结果</strong>：<br><img src="https://arxiv.org/html/2505.22050v2/x6.png" alt="定性比较"></p>
<blockquote>
<p><strong>图6</strong>：与基线模型的定性比较。本文方法能生成更符合逻辑、步骤更清晰且包含反思的规划，而基线模型（如Qwen2.5-VL-7B w/o SFT）的规划则存在步骤混乱、动作无效或冗余等问题。</p>
</blockquote>
<p>定性比较显示，经过本文方法训练的模型能够生成结构清晰、包含视觉状态描述、语言计划和可执行计划的输出，并且动作序列更合理。而未经微调的基线模型输出则可能包含不连贯的步骤或无效动作。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>首次将强化微调应用于优化视觉语言模型进行具身规划，显著提升了模型在动态环境中进行连贯多步推理和决策的能力。</li>
<li>提出了一个集成了监督微调与强化微调的综合训练流程，包括精心构建的数据集、为多步决策定制的奖励函数以及在线数据过滤等支持机制，带来了持续且鲁棒的性能提升。</li>
<li>在交互式具身AI基准Embench上进行了广泛评估，表明模型不仅优于同等规模的模型，甚至超越了GPT-4o-mini和参数量超过700亿的开源模型，并展现出对未见领域的强大泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，其奖励函数依赖于与黄金轨迹的前缀匹配，这在存在多种有效解决方案的具身任务中可能不是最优的评估方式。未来的工作可以探索更复杂的奖励设计，例如基于成功条件的稀疏奖励，或集成基于模型的预测。</p>
<p><strong>启示</strong>：本工作证明了强化驱动推理在提升具身AI长视野规划能力方面的潜力。它表明，通过结合大模型蒸馏的监督学习和针对任务特性设计的规则奖励进行强化学习，可以有效地将强大的静态VLM适配到动态交互场景中。这为后续研究指明了方向：探索更高效的奖励塑造方法、将类似框架应用于更复杂的物理交互任务，以及研究如何减少对高质量黄金轨迹或大模型蒸馏数据的依赖。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身智能体在动态交互环境中进行多步规划时存在的时空推理与常识理解不足的核心问题，提出了一种强化微调框架。方法关键包括：从闭源模型蒸馏高质量数据集进行监督微调，并设计基于多步动作质量的规则奖励函数，通过广义强化偏好优化（GRPO）优化策略。在Embench基准测试中，该方法显著超越了同类或更大规模模型（包括GPT-4o-mini和70B+开源基线），并展现出对未知环境的强泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.22050" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>