<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SAFe-Copilot: Unified Shared Autonomy Framework - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>SAFe-Copilot: Unified Shared Autonomy Framework</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.04664" target="_blank" rel="noreferrer">2511.04664</a></span>
        <span>作者: Nguyen, Phat, Aasi, Erfan, Sreeram, Shiva, Rosman, Guy, Silva, Andrew, Karaman, Sertac, Rus, Daniela</span>
        <span>日期: 2025/11/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，自动驾驶系统在罕见、模糊和分布外场景中仍然脆弱，而人类驾驶员可以通过上下文推理成功应对。共享自治作为一种有前景的方法，旨在通过在人机不确定时纳入人类输入来缓解此类失败。然而，现有方法大多将仲裁限制在低层轨迹层面，这些轨迹仅代表几何路径，未能保留底层的驾驶意图。本文针对这一痛点，提出了一种在更高抽象层次上整合人类输入与自动驾驶规划器的统一共享自治框架。本文的核心思路是利用视觉-语言模型的常识推理能力，从多模态线索推断驾驶员意图，并在语义层面仲裁或融合人机计划，以生成更安全、更符合意图的驾驶策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>SAFe-Copilot是一个用于交互式驾驶场景中融合预测的人类与自动驾驶系统计划的统一框架。其输入是人类干预动作（如转向、油门或制动）以及环境表示，输出是由仲裁过程选择的计划所生成的轨迹。该框架支持两种团队协作模式：1) <strong>主动团队协作</strong>：每当人类干预时，系统在驾驶员控制与建议的自动驾驶计划之间进行仲裁和融合；2) <strong>监督式共享控制</strong>：当不确定性检测器标记自动驾驶计划置信度低时，系统明确请求人类输入。</p>
<p><img src="https://arxiv.org/html/2511.04664v1/x2.png" alt="框架总览"></p>
<blockquote>
<p><strong>图2</strong>：SAFe-Copilot框架总览。(a) 支持的两种团队模式：上为主动团队协作，下为监督式共享控制。(b) 系统输入包括驾驶员状态、控制动作及自车状态。(c) 不确定性度量：帧内方差衡量单帧内候选轨迹的差异，帧间方差衡量跨帧平均轨迹的变化。(d) 系统整合驾驶员状态、动作、车辆状态和不确定性度量，通过符号推理模块在人机计划间仲裁，生成一致且安全的轨迹。</p>
</blockquote>
<p>框架由三个核心模块组成：</p>
<p><strong>1. 抽象模块</strong>：将连续的低层信号抽象为离散的高层描述符。</p>
<ul>
<li><strong>计划抽象</strong>：将自动驾驶规划器输出的低层轨迹，根据其净横向/纵向位移和总路径长度，使用阈值分类为“停止”、“左转/右转”、“前进”或“减速”等高层行为。</li>
<li><strong>状态抽象</strong>：将低层状态（如油门、制动、转向、速度）离散化为文本描述符（如“未施加”、“轻度”、“中度”、“向左”等）。驾驶员状态（如注意力）在本工作中采用启发式方法定义。</li>
</ul>
<p><strong>2. 不确定性模块</strong>：估计自动驾驶策略何时产生不可靠计划，从而倾向于人类干预。</p>
<ul>
<li>不确定性分数由<strong>帧内方差</strong>（衡量单帧内候选轨迹的差异）和<strong>帧间方差</strong>（衡量连续帧间平均轨迹的变化）的凸组合计算得出。</li>
<li>当不确定性分数超过阈值时，模块触发请求人类输入，作为自动驾驶信心不足时的安全保障。</li>
</ul>
<p><strong>3. 推理模块</strong>：这是框架的核心，利用视觉-语言模型进行高层语义推理与仲裁。</p>
<ul>
<li>输入为高层文本状态描述和自车前向摄像头连续三帧RGB图像。</li>
<li>该模块使用VLM（本文采用ChatGPT o3）执行三项任务：(i) 分析视觉输入以提取环境语义描述；(ii) 结合多模态线索（如转向、踏板输入）和视觉上下文推断人类意图；(iii) 基于符号抽象进行仲裁决策。</li>
<li>仲裁决策包括直接选择自动驾驶计划或人类计划，或者融合两者的要素生成一个新计划。决策过程通过精心设计的提示词引导VLM进行，使其能够评估场景上下文、安全性和任务目标。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.04664v1/x3.png" alt="定性示例"></p>
<blockquote>
<p><strong>图3</strong>：定性示例。驾驶员为避开 obstructing lane 的打开车门而向左转向，同时有对向车流接近的场景。框架能够：(A) 正确推断人类意图；(B) 评估人类计划的后果；(C) 将其与自动驾驶计划对比；(D) 理解社会驾驶规范，并利用它们将两个计划融合为更安全的轨迹。</p>
</blockquote>
<p>与现有方法的创新点在于：<strong>将共享自治的仲裁层面从低层几何轨迹提升至高层的、基于语言的语义表示</strong>。这使得系统能够利用VLM的常识推理能力，理解驾驶意图的连续性，并在必要时进行符号层面的计划融合，而非简单的轨迹插值，从而更好地保持与人类意图的一致性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在CARLA模拟器中进行，使用Bench2DriveZoo设置。自动驾驶计划由VAD模型生成。评估了两种操作模式，并构建了包含40个场景的数据集进行测试。</p>
<p><strong>1. 模拟人类实验</strong>：使用一个在不同可靠性水平（提出正确计划的比例为75%、50%、25%）下运行的模拟人类进行评估。对比基线包括总是选择人类计划的“Naive”方法和基于LLM生成规则决策树的“Decision Tree”方法。</p>
<p><img src="https://arxiv.org/html/2511.04664v1/x1.png" alt="模拟人类结果表"></p>
<blockquote>
<p><strong>表I</strong>：模拟人类评估结果。SAFe-Copilot在所有可靠性水平下均实现了<strong>100%的召回率</strong>，意味着每当人类提出正确计划时，系统都能一致地选择它。同时，其准确率和精确率远高于Naive基线，也优于决策树基线，展示了其既能保护正确的人类输入，又能拒绝不安全人类计划的能力。</p>
</blockquote>
<p><strong>2. 人类受试者调查</strong>：通过在线问卷向38名参与者展示40个场景，收集他们对框架仲裁结果的看法。</p>
<p><img src="https://arxiv.org/html/2511.04664v1/Figures/human_survey_new.png" alt="人类调查结果"></p>
<blockquote>
<p><strong>图5</strong>：人类调查结果。(a) 绝大多数参与者认为场景是 plausible 的；(b) &amp; (c) <strong>92%的参与者同意框架提出的仲裁计划</strong>，其中85.7%认为该计划与自己的想法相同或更好；(d) 参与者对VLM分析场景和解释决策的准确性评分很高。</p>
</blockquote>
<p><strong>3. Bench2Drive基准测试</strong>：在监督式共享控制模式下，在包含180个场景的Bench2Drive基准上进行评估，对比纯自动驾驶基线。</p>
<p><img src="https://arxiv.org/html/2511.04664v1/x4.png" alt="Bench2Drive评估表"></p>
<blockquote>
<p><strong>表II</strong>：Bench2Drive基准评估结果。与纯VAD自动驾驶相比，SAFe-Copilot框架将碰撞率降低了**15.66%<strong>（从46.11%降至38.89%），路线完成率提高了</strong>13.22%**（从65.53%升至74.19%），平均综合得分从43.74提升至55.97。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.04664v1/Figures/b2d_improvements_score_composed.png" alt="驾驶得分提升"></p>
<blockquote>
<p><strong>图6</strong>：驾驶得分提升。展示了在Bench2Drive基准上，共享自治相较于纯自动驾驶在综合得分上的场景级改进。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.04664v1/x5.png" alt="定性示例"></p>
<blockquote>
<p><strong>图7</strong>：Bench2Drive基准上的定性结果示例。展示了框架处理各种复杂场景的能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个<strong>在语义层面进行仲裁的统一共享自治框架</strong>，将人机融合从低层轨迹提升至高层的意图与目标层面；2) 利用<strong>VLM的常识推理能力</strong>实现了对多模态上下文、人类意图的深度理解，并能进行符号级的计划融合；3) 通过实验验证了框架的有效性，在保持完美召回率的同时显著提升了安全性和任务完成度，并获得了人类用户的高度认可。</p>
<p>论文自身提到的局限性包括：抽象模块中的阈值和状态抽象方法较为启发式；实验基于模拟环境和有限场景；人类调查的样本量有限。</p>
<p>这项工作为后续研究提供了重要启示：<strong>基于语言的语义表示可以成为共享自治的一个核心设计原则</strong>，它为实现具有常识推理能力、并能与人类意图保持连续性的智能协作驾驶系统开辟了新途径。未来的工作可以探索更精细的意图建模、更动态的仲裁策略以及在实际车辆平台上的部署。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自动驾驶系统在罕见、模糊场景下表现脆弱，以及现有共享自治方法局限于低层轨迹、无法保留驾驶意图的问题，提出了统一的共享自治框架SAFe-Copilot。该框架利用视觉语言模型，通过驾驶员动作和环境上下文等多模态线索推断驾驶意图，并在基于语言的语义表征层面进行人机策略仲裁。实验表明，在模拟设置中该方法实现了高准确率与召回率；人类受试者调查中，92%的案例参与者同意仲裁结果；在Bench2Drive基准测试中，相比纯自动驾驶，碰撞率显著降低，整体性能大幅提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.04664" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>