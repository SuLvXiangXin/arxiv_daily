<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.21231" target="_blank" rel="noreferrer">2509.21231</a></span>
        <span>作者: Jang, Jaehwi, Wang, Zhuoheng, Zhou, Ziyi, Wu, Feiyang, Zhao, Ye</span>
        <span>日期: 2025/09/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人在动态移动过程中实现手臂末端执行器的稳定控制至关重要，但因其高自由度和固有的动态不稳定性而充满挑战。当前主流方法分为两类：模型基于的控制器可实现精确控制，但依赖精确的动力学建模和估计，难以捕捉现实因素（如摩擦、齿隙），导致性能下降；基于学习的方法通过探索和域随机化能更好地缓解这些因素，但通常过拟合于训练条件，需要重新训练整个身体，且难以适应未见过的场景。本文针对现有方法在末端执行器稳定控制上精度不足、鲁棒性差、模块化程度低的具体痛点，提出了一种新的混合框架视角。本文的核心思路是：通过模型增强的残差学习，将模型基于的补偿信号与基于学习的策略相结合，并利用扰动生成策略独立训练上身控制器，从而实现对下身诱导扰动的精确、鲁棒补偿，并能零样本适应不同的步态控制器。</p>
<h2 id="方法详解">方法详解</h2>
<p>SEEC框架将人形机器人移动操作控制器解耦为下身（负责移动）和上身（负责操作）控制器。下身控制器遵循成熟的稳健移动控制训练流程。核心创新在于上身控制器，它训练一个残差策略来补偿下身诱导的扰动。</p>
<p><img src="https://arxiv.org/html/2509.21231v1/x2.png" alt="系统框架概述"></p>
<blockquote>
<p><strong>图2</strong>：SEEC系统框架概述。框架将人形机器人移动操作控制器解耦为上、下身控制器。本图描述了核心的上身强化学习模块，该模块训练一个残差策略来补偿下身诱导的扰动。它利用模型基于的加速度补偿信号来指导RL训练，并生成基座加速度剖面来模拟外部扰动，以提升对未见移动控制器的鲁棒性。部署时，训练好的上、下身策略无需联合训练即可转移到机器人上。</p>
</blockquote>
<p>上身控制器的训练遵循一个三阶段流程：</p>
<ol>
<li><strong>模拟基座加速度</strong>：为了避免在仿真中直接对浮动基座施加加速度导致数值不稳定，该方法通过向固定基座模型注入等效的虚拟力/力矩来模拟基座运动。这些力/力矩由基座线速度/角速度及其导数计算得出，包含了线性、欧拉、离心、科里奥利力以及角加速度和陀螺力矩。为了近似移动诱导的扰动，基座加速度信号由两种特征源合成：来自足地接触力的脉冲加速度信号，以及随每一步身体质心摆动产生的周期性摇摆信号。通过随机采样扰动参数（周期、脉冲幅度、振荡幅度、相位偏移），生成覆盖广泛现实信号的丰富扰动剖面，使策略能学习应对不同步态风格的补偿策略。</li>
<li><strong>计算补偿扭矩</strong>：在固定基座模型下，结合模拟的基座运动，计算出末端执行器在全局坐标系下的加速度。为了抵消这些扰动效应，该方法基于操作空间公式，计算出一个最小范数的补偿扭矩，旨在使末端执行器的全局加速度（包含基座运动贡献和虚拟力/力矩引起的响应加速度）趋近于零。此补偿扭矩与任务导向的控制扭矩（如用于跟踪期望末端位姿的操作空间控制扭矩）相结合，共同用于稳定末端执行器。</li>
<li><strong>补偿残差策略训练</strong>：由于传感器噪声、硬件IMU缺失角加速度信号以及仿真到现实的差距，直接将解析计算的补偿扭矩部署在硬件上不可行。因此，本文训练一个RL策略，其输出关节目标给底层PD控制器，以匹配期望的补偿扭矩。观察空间包括末端执行器指令和本体感知数据的历史信息。奖励函数的核心是鼓励测量到的关节扭矩接近理想的补偿扭矩与任务扭矩之和，并辅以惩罚全局加速度、动作平滑度和跟踪误差的辅助奖励。策略使用PPO算法与循环演员-评论家网络进行训练。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1) <strong>模型增强的残差学习</strong>：不是简单地惩罚末端加速度，而是利用模型提供的解析补偿信号以更原理化的方式指导策略学习。2) <strong>扰动生成策略与模块化设计</strong>：通过生成广泛的基座扰动剖面独立训练上身策略，使其不依赖于特定下身策略，从而能够零样本适应未见过的移动控制器，实现了上、下身控制的解耦。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台包括IsaacLab和MuJoCo仿真器，并最终将策略零样本转移到Booster T1（高1.2米，29自由度）真人形机器人上。对比的基线方法包括：逆运动学基线、不包含模型引导或扰动生成的多种RL变体（RL w/o Sim. Acc., RL w Sim. Acc.）以及SEEC框架的多个消融版本（如去掉任务扭矩、去掉扭矩引导奖励、去掉模拟加速度）。</p>
<p>关键实验结果（基于表2）显示，在踏步、前进、侧移和旋转四种移动任务中，完整的SEEC方法在末端执行器线性加速度和角加速度的平均值与最大值上， consistently优于所有基线方法。例如，在踏步任务中，SEEC的线性加速度均值为2.26 m/s²，最大值为5.92 m/s²，而IK基线分别为5.73 m/s²和15.2 m/s²，RL w Sim. Acc.基线分别为3.91 m/s²和16.8 m/s²。</p>
<p><img src="https://arxiv.org/html/2509.21231v1/x3.png" alt="末端执行器稳定性基准结果"></p>
<blockquote>
<p><strong>图3</strong>：在MuJoCo中评估末端执行器稳定性的基准结果图表。展示了不同方法在四种移动任务下末端执行器线性加速度和角加速度的均值和最大值。SEEC方法（深蓝色）在绝大多数指标上表现最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.21231v1/x4.png" alt="与未见过的移动控制器集成"></p>
<blockquote>
<p><strong>图4</strong>：与未见过的移动控制器集成的性能。柱状图比较了SEEC与两种RL基线（预训练和联合训练）在使用训练过的移动策略和未见过的移动策略时，末端执行器加速度的差异。SEEC在切换到未见控制器时性能下降最小，展示了其卓越的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.21231v1/x5.png" alt="消融研究：扰动生成的影响"></p>
<blockquote>
<p><strong>图5</strong>：扰动生成策略的消融研究。左图显示，在训练中使用扰动生成（Ours）比不使用（Ours w/o Sim. Acc.）能获得更低（更好）的末端加速度。右图显示，扰动生成使策略对扰动频率的变化更具鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.21231v1/x6.png" alt="真实机器人实验"></p>
<blockquote>
<p><strong>图6</strong>：真实机器人Booster T1上的定性实验结果。对比了SEEC与IK基线的表现。SEEC能够稳定地握住柔性链条、擦拭白板、端稳一盘零食行走，而IK基线则出现了明显的振荡导致任务失败（链条脱落、零食洒出）。</p>
</blockquote>
<p>消融实验总结：1) <strong>模型引导奖励至关重要</strong>：移除扭矩引导奖励会导致性能显著下降。2) <strong>扰动生成提升鲁棒性</strong>：在训练中移除模拟的基座加速度会损害策略对扰动的补偿能力，尤其是在应对未见扰动时。3) <strong>模块化设计的有效性</strong>：SEEC能够无缝集成并适应未见过的移动控制器，而联合训练的RL基线在此情况下性能严重退化。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了一个<strong>模型增强的残差学习框架</strong>，将模型基于的专家知识与基于学习的适应性相结合，实现了精确的加速度补偿，同时有效解决了模型不准确性和参数不确定性问题。2) 引入了<strong>基座运动数据与扰动生成策略</strong>，使策略在训练中暴露于广泛的移动相关扰动下，从而学习了鲁棒的补偿行为，能够零样本迁移到未见过的移动控制器和步态。3) <strong>首次在完整人形机器人Booster T1上部署并验证了此类混合框架</strong>，通过仿真到现实的零样本转移，在多种移动操作任务中实现了比基线更稳定、更精确的末端执行器控制。</p>
<p>论文自身提到的局限性包括：基于<strong>手臂对基座的反作用可忽略</strong>的假设，这在手臂动态较轻时成立，但可能不适用于重负载操作；当前方法在<strong>局部坐标系中指定末端目标</strong>，虽然通过容错机制平衡了跟踪与稳定，但更稳定的做法是在世界坐标系中指定目标，这需要准确、实时的机器人全局姿态估计，被留作未来工作。</p>
<p>对后续研究的启示：SEEC展示了<strong>混合模型与学习方法的有效性</strong>，特别是在处理具有明确物理结构的问题时。其<strong>模块化、解耦的设计范式</strong>为构建复杂机器人系统提供了新思路，允许独立开发和更新子模块（如移动、操作）。<strong>扰动生成与域随机化策略</strong>对于学习应对现实世界不确定性的鲁棒策略具有重要参考价值。未来的工作可以探索如何放松其假设，例如考虑显著的上身-下身动力学耦合，或将世界坐标系目标跟踪集成到框架中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SEEC框架，旨在解决人形机器人在动态移动时手臂末端执行器难以稳定控制的难题。该方法的核心是模型增强的残差学习，通过模型引导的强化学习配合扰动生成器，训练上半身策略来学习补偿下体运动引起的干扰。该设计使其能适配未见过的运动控制器而无需重新训练。实验在模拟器和Booster T1真实机器人上验证，SEEC能成功处理持链行走、擦白板、端盘行走等多种移动操作任务，稳定效果明显优于逆运动学基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.21231" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>