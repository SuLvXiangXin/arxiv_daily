<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.05336" target="_blank" rel="noreferrer">2601.05336</a></span>
        <span>作者: Yuchen Cui Team</span>
        <span>日期: 2026-01-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在辅助护理、家庭协助等需要人机协作的场景中，为机器人设计直观的控制接口是一大挑战。传统控制方式（如操纵杆、触摸屏）会给用户带来认知或身体负担。视线作为一种快速、非侵入且富含意图的输入模态，成为极具吸引力的选择。然而，现有基于视线的控制方法存在关键局限性：它们通常需要额外的物理控制器（如操纵杆）进行直接控制，或仅限于平面抓放任务，难以实现完整的6自由度操作。此外，视线本身仅能提供位置点估计，存在固有的歧义性，需要额外的上下文推理才能准确解读用户意图。</p>
<p>本文针对“如何仅使用视线实现零样本、上下文感知的机器人操作”这一痛点，提出结合视线追踪与视觉语言基础模型（VLM）的新视角。核心思路是：利用基础模型的强大语义理解和泛化能力，将用户的视线注视点转化为对高级任务意图和低级抓取位姿的推理，从而实现无需任务特定训练的、模块化的机器人自主操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>gamma系统的整体框架遵循“感知-推理-执行”的流程。用户佩戴智能眼镜（如Meta Project Aria），其视线被实时追踪并转换为机器人视角下的注视点。系统利用一系列预训练的视觉模型进行物体分割和抓取位姿生成，并核心依赖VLM进行两级推理：1）根据注视点序列预测用户的高级操作意图；2）在考虑任务上下文（如避障）的前提下，从多个候选抓取位姿中选择最合适的一个。最终，系统根据推理结果调用参数化的机器人技能API（如<code>pick(obj)</code>, <code>place(obj)</code>）生成行为程序并执行。</p>
<p><img src="https://arxiv.org/html/2601.05336v1/figures/pipeline_updated.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：gamma系统的功能模块总览。左侧为感知模块，包括视线检测与转换、物体分割与识别、抓取位姿生成；右侧为基于VLM的推理模块，负责意图预测和上下文感知的抓取选择。</p>
</blockquote>
<p>核心模块包括感知模块和推理模块：</p>
<ol>
<li><p><strong>感知模块</strong>：</p>
<ul>
<li><strong>视线检测与转换</strong>：使用Project Aria的预训练视线估计模型，从眼镜的自我中心视角预测注视点。为获得稳定意图，系统对视线数据进行时间聚合（持续至少2秒，像素移动小于15）。关键挑战在于将自我中心2D注视点（对应一条射线）映射到机器人第三方视角的3D空间。系统采用基于ArUco标记的特征匹配进行实时相机位姿估计，作为将注视点转换到机器人坐标系的实用解决方案。论文指出，要求用户靠近机器人视角以最小化转换误差。</li>
<li><strong>物体分割与识别</strong>：不依赖显式物体检测，而是利用VLM的推理能力。首先，使用SAM2对转换后的注视点进行基于点查询的实例分割，生成物体掩膜。为应对视线估计误差，选择每个注视点周围预定义范围内的最近分割掩膜。随后，融合来自两个机器人相机的物体点云，形成与机器人基座坐标系对齐的统一表示。</li>
<li><strong>抓取位姿生成</strong>：基于Contact-GraspNet实现抓取生成流程。为提高视角不变性，对合并后的物体点云绕X轴进行三次旋转（180°, +135°, -135°），模拟俯视视角。每次旋转后，将抓取候选限制在物体边界框内，并过滤以确保与分割区域接触。选择中位姿态以保证鲁棒性，并进一步应用±45°的俯仰角调整以增加多样性。若无有效抓取，则从所有预测中选择最接近原始注视点的抓取。</li>
</ul>
</li>
<li><p><strong>推理模块</strong>：</p>
<ul>
<li><strong>基于视线的意图预测</strong>：将聚合后的注视点（在机器人视角图像上标注为带有序号的实心圆点）作为视觉提示。设计文本提示引导VLM进行链式推理：先识别注视点附近的物体，再推理用户想用这些物体做什么。意图被编译为一系列原子技能 <code>s(o,a)</code>。</li>
<li><strong>抓取位姿选择</strong>：这是本文的关键创新点，旨在让抓取选择考虑任务上下文（如避免碰撞）。系统使用Contact-GraspNet为物体生成9个不同角度的候选抓取位姿。为了向VLM有效传达3D抓取信息，设计了两种视觉提示：对于仅支持图像的VLM，提供每个候选抓取的两个视角图像并编号；对于支持视频的VLM（如Gemini），渲染一段展示彩色编码的抓取位姿在点云场景中从不同角度观察的短视频。文本提示则要求VLM详细描述每个抓取，分析其稳定性、碰撞风险等，并最终根据任务需求选择最佳的一个。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.05336v1/figures/intent-reasoning-datasets.png" alt="意图推理任务示例"></p>
<blockquote>
<p><strong>图3</strong>：用于评估意图推理的数据集示例。上排：实验室设计的30个桌面操作场景，按难度分级（简单、中等、困难）。下排：从DROID数据集中采样的45个野外操作场景。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.05336v1/figures/grasp-selection-new.png" alt="抓取选择视觉提示"></p>
<blockquote>
<p><strong>图4</strong>：用于抓取选择的两种视觉提示。上图：多视角图像提示（编号）。下图：展示候选抓取位姿的短视频片段。不同的视觉表示会影响VLM的预测。</p>
</blockquote>
<p>与现有方法相比，gamma的创新点在于：1) <strong>全流程利用基础模型</strong>：不依赖手工设计的意图预测模型或任务特定策略，利用VLM的零样本推理能力实现从意图理解到抓取规划的全链条自动化。2) <strong>上下文感知的抓取选择</strong>：首次利用VLM的几何和语义理解能力，对几何抓取预测结果进行基于任务上下文的筛选和优化，超越了仅考虑抓取稳定性的传统方法。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台使用Ufactory Xarm 7机械臂、两个RealSense RGB-D相机以及Meta Project Aria智能眼镜进行视线追踪。评估主要包括两部分：VLM推理能力基准测试和用户研究。</p>
<ol>
<li><strong>VLM推理评估</strong>：<ul>
<li><strong>数据集</strong>：使用自建的30个桌面操作场景（Lab-Tabletop）和从DROID数据集采样的45个野外场景（Wild）。</li>
<li><strong>对比模型</strong>：评估了5个VLM：Gemini 2.0 Flash、Gemini 2.5 Flash、Gemini Pro、Llama4 Maverick和GPT-4o。</li>
<li><strong>意图预测结果</strong>：如表1所示，在桌面任务上，Gemini Pro准确率最高（0.94）；在野外任务上，GPT-4o和Gemini Pro表现较好（均为0.73）。Gemini Pro平均准确率最高（0.84），因此被选为默认的意图预测模型。</li>
<li><strong>抓取选择结果</strong>：如表2所示，使用图像提示时，Gemini Pro成功率最高（0.60），但推理时间较长（24.83秒）。使用视频提示时，Gemini 2.5 Flash成功率最高（0.67），时间成本为15.18秒。基于此，Gemini 2.5 Flash（视频提示）被确定为抓取选择的最佳配置。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.05336v1/figures/user_study.png" alt="用户研究设置"></p>
<blockquote>
<p><strong>图5</strong>：用户研究实验设置。左：基线2D视线控制面板界面。右：不同难度的用户研究任务示例，抓取位姿约束越多，任务越难。</p>
</blockquote>
<ol start="2">
<li><strong>用户研究</strong>：<ul>
<li><strong>基线方法</strong>：面板式视线控制（Gaze Panel），用户通过注视屏幕上的虚拟按钮来控制机械臂的6自由度位姿和夹爪。</li>
<li><strong>参与者与任务</strong>：6名参与者，使用两种方法完成4项操作任务（如将物体放入篮子、搬运植物避开台灯）。</li>
<li><strong>关键结果</strong>：<ul>
<li><strong>效率</strong>：gamma让用户完成任务所需的时间显著少于基线方法（p-value ≪ 0.01），节省了一半以上的时间。</li>
<li><strong>主观感受</strong>：用户认为gamma所需的认知负荷和挫败感更低。</li>
<li><strong>成功率与偏好</strong>：虽然gamma在推断用户意图方面成功率很高，但由于零样本抓取预测和VLM抓取选择的复合误差，未能始终产生可靠的抓取。有趣的是，6名参与者中有4人最终更喜欢基线方法，原因是基线提供了“更直接的控制感”、“交互性更强”、“更一致”。用户认为基线的“感知性能”更高，因为他们能主动从错误中恢复。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.05336v1/figures/user-study-results.png" alt="用户研究结果"></p>
<blockquote>
<p><strong>图6</strong>：用户研究结果汇总。左：主观评价（NASA TLX）平均分，gamma在脑力需求、体力需求、挫败感上得分更低。中：任务各阶段成功率，gamma意图预测成功率高，但抓取和执行成功率有波动。右：用户完成任务所用时间，gamma显著更快。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.05336v1/figures/trajectories.png" alt="轨迹可视化"></p>
<blockquote>
<p><strong>图7</strong>：用户研究中的样本轨迹可视化。gamma生成的轨迹（蓝色）比基线视线面板控制的轨迹（红色）更短、更干净。灰色区域表示用户“注视”的时间，在基线中用户花费更多时间在思考和规划上。</p>
</blockquote>
<p><strong>消融实验总结</strong>：VLM选择与提示方式的消融实验（表1，表2）表明，不同的VLM和视觉提示（图像 vs. 视频）对意图预测和抓取选择的性能有显著影响。Gemini系列模型整体表现更优，视频提示在抓取选择任务上能提供更全面的信息，但代价是更高的token消耗和推理时间。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>gamma框架</strong>，首次将可穿戴视线追踪与视觉/视觉语言基础模型深度结合，实现了仅通过视线进行零样本机器人操作。2) 系统性地<strong>展示了基础模型在视线解读中的强大能力</strong>，不仅用于高级意图识别，还创新性地用于考虑上下文的低级抓取位姿选择。3) 通过用户研究<strong>揭示了人机交互中一个关键权衡</strong>：用户既追求自动化带来的低努力和高效，也高度重视对机器人的直接控制感和代理权。</p>
<p>论文自身提到的局限性包括：1) <strong>基础模型推理的局限性</strong>：VLMs在细粒度的、低级别的推理（如精确的抓取选择）上仍有困难，且推理延迟影响系统实时性。2) <strong>视线转换的挑战</strong>：将2D自我中心视线准确映射到3D机器人坐标系本质上是病态问题，当前基于标记的解决方案有误差，且依赖于用户与机器人视角接近的假设。</p>
<p>对后续研究的启示：1) <strong>混合控制机制</strong>：未来的辅助系统设计应探索允许用户在自主模式和手动控制模式之间流畅切换的接口，以平衡自动化效率与用户控制感。2) <strong>提升基础模型的细粒度推理能力</strong>：需要进一步研究如何提高VLMs对3D几何、物理交互等低级技能的推理精度和速度。3) <strong>个性化与适应性</strong>：系统应能适应不同用户的视线模式（如参考性注视），并通过在线学习或人类反馈来纠正和改善执行效果。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决机器人控制界面不够直观的问题，特别是在辅助护理场景中。作者提出gamma系统，通过结合自我中心视线跟踪和视觉语言模型，将用户的视线注视点映射为场景上下文，从而推断用户意图并自主执行机器人操作任务。该方法无需针对特定任务进行训练。实验在桌面操作任务上评估表明，gamma相比无推理能力的基线视线控制，能提供更鲁棒、直观且可泛化的控制。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.05336" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>