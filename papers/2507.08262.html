<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.08262" target="_blank" rel="noreferrer">2507.08262</a></span>
        <span>作者: He Wang Team</span>
        <span>日期: 2025-07-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作策略学习常依赖于从视觉观测中提取的有效表示。主流方法或侧重于从2D图像中学习，或利用特定视角的3D点云。然而，2D表示缺乏对物体几何和空间关系的直接建模，而基于单视角的3D方法则受限于视角模糊性，难以泛化到训练时未见过的相机视角，这限制了策略在真实多变环境中的鲁棒性。本文旨在解决这一痛点，提出一种新颖的3D预训练框架，以学习兼具空间感知与语义理解的通用3D视觉表示。核心思路是：通过3D重建任务学习几何结构，并通过与强大2D基础模型的对比学习注入丰富的语义知识，从而获得对机器人操作任务更有利的3D表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>CL3R的整体框架是一个两阶段的预训练流程，旨在从多视角RGB-D观测中学习通用的3D点云表示，该表示随后可用于提升下游机器人模仿学习策略的性能。输入是来自不同数据集的、具有不同相机坐标系的多视角RGB-D图像对，输出是富含几何与语义信息的点云特征。</p>
<p><img src="https://cdn.openai.com/paper/cl3r-framework.png" alt="CL3R Framework"></p>
<blockquote>
<p><strong>图1</strong>：CL3R方法整体框架。左侧为预训练阶段：首先将多视角RGB-D图像统一到世界坐标系并融合为点云，经过增强后，分别送入3D重建分支（点云MAE）和对比学习分支。对比学习将点云特征与来自预训练2D基础模型（如CLIP）的对应图像特征对齐。右侧为下游策略学习阶段：使用预训练好的点云编码器对观测进行编码，然后训练策略网络（如扩散策略）完成操作任务。</p>
</blockquote>
<p>框架包含两个核心模块：1) <strong>基于点云的掩码自编码器</strong>：用于学习3D几何结构。该模块随机掩蔽输入点云的大部分 patches（例如75%），编码器仅处理可见的 patches，解码器则根据这些可见上下文重建被掩蔽的点坐标。这一自监督重建任务迫使模型学习点云中物体和场景的完整几何先验。2) <strong>3D-2D对比学习模块</strong>：用于注入语义知识。对于同一视角，将RGB图像输入一个冻结的、预训练好的2D基础模型（如CLIP的图像编码器）提取2D图像特征；同时，将对应的点云输入正在训练的点云编码器提取3D特征。通过优化InfoNCE损失，拉近匹配的3D-2D特征对，推远不匹配的对，从而实现将2D模型中的语义知识蒸馏到3D编码器中。</p>
<p>与现有方法相比，CL3R的创新点具体体现在：第一，<strong>提出了一个统一的3D视觉表示预训练框架</strong>，首次联合了3D重建与3D-2D对比学习，同时捕获几何与语义。第二，<strong>处理多数据集与多视角的鲁棒性</strong>：通过将不同数据集的相机参数统一到共享的世界坐标系，并结合“随机点云融合”增强技术——在训练时随机选择多视角点云的一个子集进行融合，有效缓解了相机视角模糊性问题，使模型能泛化到测试时的新视角。</p>
<p><img src="https://cdn.openai.com/paper/random-fusion.png" alt="Random Point Cloud Fusion"></p>
<blockquote>
<p><strong>图2</strong>：随机点云融合示意图。在训练时，不是固定融合所有N个视角的点云，而是随机选择K个视角（K≤N）进行融合。这种增强鼓励模型学习对视角组合不敏感的稳健表示。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境（RLBench）和真实世界（语言条件化的桌面操作任务）中进行。使用的数据集包括来自RLBench的多个任务数据以及真实采集的RGB-D演示数据。对比的基线方法包括：直接从RGB或点云训练策略的基准、使用MoCo v3或MAE在ImageNet或点云上预训练的视觉编码器、以及最新的机器人视觉表示学习方法如R3M和MVP。</p>
<p>关键实验结果如下：在RLBench的18个任务上，使用CL3R预训练表示的下游扩散策略，平均成功率达到**75.6%<strong>，显著优于从零开始训练的基准（64.3%）以及使用R3M（68.6%）和MVP（71.2%）预训练表示的方法。在7个真实世界语言指令操作任务上，CL3R取得了</strong>81.4%**的平均成功率，相比最强的基线MVP（73.6%）提升了约7.8个百分点。</p>
<p><img src="https://cdn.openai.com/paper/sim-results.png" alt="Simulation Results"></p>
<blockquote>
<p><strong>图3</strong>：在RLBench模拟环境中的任务成功率对比。CL3R在绝大多数任务上优于所有基线方法，展示了其预训练表示的有效性。</p>
</blockquote>
<p><img src="https://cdn.openai.com/paper/real-results.png" alt="Real-World Results"></p>
<blockquote>
<p><strong>图4</strong>：真实世界操作任务的成功率。CL3R在所有7个任务中均取得最高或接近最高的成功率，证明了其从模拟到真实的强泛化能力。</p>
</blockquote>
<p><img src="https://cdn.openai.com/paper/ablation.png" alt="Ablation Study"></p>
<blockquote>
<p><strong>图5</strong>：消融实验。分别移除了对比学习（w/o CL）、点云MAE重建（w/o MAE）、坐标系统一（w/o Unify）和随机融合（w/o RF）组件。实验表明，每个组件都对性能有正面贡献，其中对比学习和坐标系统一的影响最为显著。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：1) <strong>移除对比学习</strong>导致性能大幅下降，说明从2D基础模型迁移语义知识至关重要。2) <strong>移除点云MAE</strong>也会降低性能，验证了几何重建任务的有效性。3) <strong>移除坐标系统一</strong>严重损害跨数据集和跨视角的泛化能力。4) <strong>移除随机点云融合</strong>会导致模型对训练视角过拟合，在新视角下性能变差。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>CL3R</strong>，一个通过联合3D重建与3D-2D对比学习来预训练机器人操作通用3D表示的新框架。2) 引入了<strong>坐标系统一</strong>和<strong>随机点云融合</strong>技术，有效解决了多数据集和多视角带来的挑战，增强了表示的视角不变性和泛化性。3) 在模拟与真实世界的广泛实验中验证了该方法的优越性，显著提升了多种机器人操作任务的成功率。</p>
<p>论文自身提到的局限性包括：预训练阶段仍然依赖于现成的2D基础模型，其能力上限可能制约3D表示中语义知识的质量；此外，框架主要针对视觉表示学习，如何更紧密地与不同策略架构结合以进一步优化端到端性能是未来的方向。</p>
<p>本工作对后续研究的启示是：融合不同模态（2D与3D）和不同监督信号（自监督与来自基础模型的监督）是学习强大机器人视觉表示的有效途径。统一不同来源数据的坐标系是一种实用且重要的预处理步骤。促进模型对视角变化的鲁棒性对于实现真正通用的机器人感知系统至关重要。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文CL3R致力于解决机器人操作中3D视觉表示学习不足的问题，以增强操作策略。其核心方法整合了点云掩码自编码器进行3D重建以学习空间感知，并采用对比学习从预训练2D基础模型迁移语义知识。通过统一数据集坐标系和随机融合多视点点云，减轻相机视点模糊性，提升泛化能力。实验在仿真和真实环境中验证了该方法的优越性，有效提升了机器人视觉运动策略学习的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.08262" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>