<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.12636" target="_blank" rel="noreferrer">2504.12636</a></span>
        <span>作者: Xu, Rongtao, Zhang, Jian, Guo, Minghao, Wen, Youpeng, Yang, Haoting, Lin, Min, Huang, Jianzheng, Li, Zhe, Zhang, Kaidong, Wang, Liqiong, Kuang, Yuxuan, Cao, Meng, Zheng, Feng, Liang, Xiaodan</span>
        <span>日期: 2025/04/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域面临的关键挑战在于理解空间功能可供性（Spatial Affordances），即物体交互的“位置”与“方式”，这对于擦拭白板、堆叠物体等复杂任务至关重要。当前主流方法主要包括两类：一是模块化方法（如ReKep、MOKA），它们利用大型视觉基础模型进行空间理解，但缺乏对物理世界和物体可操作性的深入认知；二是端到端的视觉-语言-动作（VLA）方法（如RDT、π₀），它们直接生成动作，但未能充分理解空间位置，导致在复杂任务中表现不佳。近期出现的基于点（如SpatialVLA）和基于流（如General Flow）的功能可供性方法在建模空间交互方面取得了进展，但通常侧重于密集的空间表示或轨迹建模，计算成本高且与特定机器人平台（具身）绑定。</p>
<p>本文针对现有方法在鲁棒的空间推理能力方面的局限性，提出了一个物体中心、具身无关的新视角。核心思路是：提出一个层次化的功能可供性感知扩散模型A₀，将操作任务分解为高层空间功能可供性理解（预测接触点和接触后轨迹）与低层动作执行两个阶段，并通过一种统一的“具身无关功能可供性表示”来实现跨平台泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>A₀模型采用层次化设计，整体流程如图2所示：首先进行高层空间功能可供性理解，输出物体中心的接触点及轨迹；随后由动作执行模块将这些2D预测转换为3D抓取姿态和路径点并执行。</p>
<p><img src="https://arxiv.org/html/2504.12636v5/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：A₀模型将机器人操作任务分解为两个层次：(1) 高层空间功能可供性理解；(2) 低层动作执行。模型利用具身无关功能可供性表示来预测物体中心的接触点和接触后轨迹。</p>
</blockquote>
<p>模型的核心是学习“具身无关功能可供性表示” ℛ。该表示统一了来自机器人数据、手-物交互（HOI）数据和自定义数据的可操作知识，每条数据包括：物体中心的RGB图像 I、自然语言指令 L、接触点 C（一个2D点）和接触后轨迹 T（一系列2D路径点）。这种点基表示使其通用且易于跨平台部署。</p>
<p><img src="https://arxiv.org/html/2504.12636v5/x3.png" alt="模型总览"></p>
<blockquote>
<p><strong>图3</strong>：A₀模型概览。这是一个基于Transformer的扩散概率模型，用于预测机器人操作的路径点。使用预训练的Qwen2.5-7B和SigLip分别编码语言指令和图像。通过提出的运动令牌增强，利用前一时刻的图像提供运动信息。图像和文本令牌通过交叉注意力交替注入作为条件。</p>
</blockquote>
<p>如图3所示，A₀基于扩散Transformer（DiT）架构。输入包括扩散时间步 k 和带噪声的路径点 𝐱_{t:t+T}（归一化2D坐标）。条件信息为当前帧 I_t、前一帧 I_{t-1}（提供运动信息）和语言指令 ℓ，它们分别由预训练的SigLip视觉编码器和Qwen2.5-7B语言编码器编码为令牌。</p>
<p>模型包含两个关键设计模块：</p>
<ol>
<li><strong>位置偏移注意力（Position Offset Attention）</strong>：为了关注运动信息，将当前帧令牌 I_t^i 与前一帧令牌 I_{t-1}^i 相减得到运动令牌 I_m^i，然后与 I_t^i 沿令牌维度拼接，形成最终的视觉特征。</li>
<li><strong>空间信息聚合层（Spatial Information Aggregation Layer）</strong>：在DiT块之后添加一个非线性MLP解码器，将隐空间映射回物理坐标空间。</li>
</ol>
<p>训练策略分为两步：</p>
<ol>
<li><strong>预训练</strong>：在包含100万个单接触点标注的PixMo-One-Point数据集上，仅使用单张图像 I_t，监督模型预测第一个接触点（起始点）的坐标，损失函数为坐标的均方误差（MSE）。目的是建立通用的物体定位能力。</li>
<li><strong>有监督微调</strong>：在特定任务的标注轨迹数据（如HOI4D-22k, DROID-3k, Maniskill-5k）上微调。此时文本条件扩展为完整指令，输出扩展为 T 个路径点，并使用前一帧图像提供运动信息。损失函数为对所有路径点坐标的MSE损失。</li>
</ol>
<p>与现有方法相比，A₀的创新点在于：1) 提出了统一的、点基的、物体中心的具身无关功能可供性表示；2) 采用了层次化扩散模型架构，明确分离空间理解与动作执行；3) 引入了位置偏移注意力和空间信息聚合层等组件，增强了对运动信息和空间坐标的建模能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了多个基准数据集和机器人平台。离线验证使用PixMo-One-Point、HOI4D-22k、DROID-3k和ManiSkill-5k数据集，以路径点预测的平均绝对误差（MAE）为指标。真实世界实验在Franka Emika、Kinova Gen3、Realman和Dobot X-Trainer机器人平台上进行，评估了“放置物体”、“打开抽屉”、“按下按钮”和“擦拭白板”四个任务的成功率。</p>
<p>对比的基线方法包括：2D功能可供性方法（MOKA, ReKep）、VLM-based方法（Magma, Molmo）以及端到端VLA方法（RDT, π₀）。</p>
<p>关键实验结果如下：</p>
<ul>
<li>在Franka机器人上，A₀-1B取得了62.50%的平均成功率，显著优于最佳基线Molmo（43.75%）。在Kinova机器人上，A₀-1B取得了53.75%的平均成功率，优于最佳基线MOKA（45.00%）。</li>
<li>在需要轨迹跟随的“擦拭白板”任务中，A₀在Kinova平台上取得了50%的成功率，而RDT和π₀均为0%，凸显了其在复杂轨迹任务上的优势（表3）。</li>
<li>A₀执行效率高，通常只需4-5步动作，而RDT和π₀需要25-50步。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.12636v5/x4.png" alt="预训练有效性"></p>
<blockquote>
<p><strong>图4</strong>：在三个数据集上，使用预训练（Pretrain）相比不使用预训练（Scratch）的MAE性能对比（越低越好）。预训练能有效降低特定任务上的路径点预测误差。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.12636v5/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>表1</strong>：网络架构消融研究结果。移除位置偏移注意力（POA）或空间信息聚合层（SIAL）均会导致MAE上升，验证了这两个组件的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.12636v5/x5.png" alt="真实世界任务定性结果（Franka）"></p>
<blockquote>
<p><strong>图5</strong>：在Franka机器人上对四个复杂任务（开抽屉、放物体、按按钮、擦白板）的定性评估。展示了输入图像、模型预测的2D功能可供性（红点与轨迹）以及执行过程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.12636v5/x6.png" alt="真实世界任务定性结果（Kinova）"></p>
<blockquote>
<p><strong>图6</strong>：在Kinova机器人上四个任务的定性评估。模型执行单次推理，实现快速执行。</p>
</blockquote>
<p>消融实验（表1）表明：移除位置偏移注意力（POA）会导致MAE上升（如在ManiSkill-5k上从5.5升至6.3），损害运动感知推理能力；移除空间信息聚合层（SIAL）会导致MAE显著上升（如在HOI4D-22k上从47.5升至61.1），严重影响复杂环境下的路径点预测精度。预训练的有效性也在图4中得到验证，它能提升模型在未见过的物体和环境上的泛化能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：</p>
<ol>
<li>引入了<strong>具身无关功能可供性表示</strong>，通过预测物体中心的接触点和轨迹来高效捕获空间功能可供性，其点基特性使其易于跨平台部署。</li>
<li>提出了<strong>层次化功能可供性感知扩散模型A₀</strong>，该模型先学习上述表示，再生成精确操作动作，并设计了位置偏移注意力等关键组件以增强空间理解。</li>
<li>在<strong>多个机器人平台</strong>上验证了A₀的有效性，展示了其在需要空间功能可供性推理的复杂任务中的优越性能、强大的泛化能力以及平台无关的设计。</li>
</ol>
<p>论文自身提到的局限性包括：模型在高度动态环境中的泛化能力仍有待探索，且当前的动作执行模块依赖于额外的抓取采样器。</p>
<p>本文对后续研究的启示在于：1) <strong>功能可供性表示的通用性</strong>：统一的、物体中心的、具身无关的表示是连接多样数据源与具体机器人平台的有效桥梁。2) <strong>层次化设计的优势</strong>：将复杂的操作任务解耦为空间理解与动作执行，有助于提升模型的泛化性和可解释性。3) <strong>利用互联网与HOI数据</strong>：大规模、多样化的非机器人领域数据（如图像、人-物交互）是预训练机器人空间理解能力的宝贵资源。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中空间可供性（即“何处”与“如何”交互）理解不足的核心问题，提出A₀分层可供性感知扩散模型。该方法将任务分解为高层空间可供性理解与低层动作执行，关键是通过“与具体实现无关的可供性表示”预测物体接触点及接触后轨迹，并采用位置偏移注意力与空间信息聚合层进行特征提取与坐标映射。模型经百万级接触点预训练与轨迹微调，在Franka、Kinova等多机器人平台上实验表明，其能够高效完成复杂操作任务，展现出优越的泛化性能与现实适用性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.12636" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>