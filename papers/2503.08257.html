<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.08257" target="_blank" rel="noreferrer">2503.08257</a></span>
        <span>作者: Zhong, Yiming, Jiang, Qi, Yu, Jingyi, Ma, Yuexin</span>
        <span>日期: 2025/03/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧抓取是机器人操作任务的基础能力。当前主流方法已从早期的解析优化转向数据驱动方法，特别是基于扩散模型的生成方法，因其能捕捉复杂分布并生成多样化的抓取姿态而备受关注。然而，现有扩散模型方法在生成灵巧抓取姿态时，由于缺乏物理规则的约束，常常产生次优结果，例如手与物体发生穿透或接触不足，导致抓取成功率不理想。</p>
<p>本文针对现有扩散模型缺乏物理约束这一具体痛点，提出将物理意识整合到生成模型的训练和采样过程中。核心思路是：设计三种物理约束目标函数，并通过一种物理感知训练范式和物理引导采样器，将它们系统地融入扩散模型，从而引导模型生成既多样又物理可行的灵巧抓取姿态。</p>
<h2 id="方法详解">方法详解</h2>
<p>DexGrasp Anything 方法旨在通过整合物理约束来增强基于扩散模型的灵巧抓取生成器。其整体流程分为训练和采样两个阶段，输入为物体观测O，输出为灵巧手姿态h（包含24维关节角、全局旋转和平移，共33维参数）。</p>
<p><img src="https://arxiv.org/html/2503.08257v2/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：DexGrasp Anything 方法总览。训练阶段（左），物体信息作为条件输入，模型从带噪姿态预测噪声，并通过物理约束引导噪声分布趋向更干净、适合抓取的分布。采样阶段（右），物理引导采样器在每一步去噪时获取当前观测，并基于物理约束进行后验采样，逐步引导至物理可行的抓取配置。</p>
</blockquote>
<p>核心模块包括三种精心设计的物理约束力、物理感知训练目标以及物理引导采样器。</p>
<p><strong>1. 物理约束力</strong></p>
<ul>
<li>**表面拉力 (Surface Pulling Force, SPF)**：确保抓取可行性。它计算灵巧手内表面采样点到物体表面的最近距离，仅对距离小于阈值的点施加“拉力”，使其靠近物体表面，而对已足够远的点无影响。其损失函数为距离平方根的平均值。</li>
<li>**外部穿透排斥力 (External-penetration Repulsion Force, ERF)**：保持手物交互的几何准确性。通过计算手点云到物体表面的有符号距离（利用物体法向量判断内外），来惩罚手与物体之间的显著穿透。</li>
<li>**自穿透排斥力 (Self-Penetration Repulsion Force, SRF)**：维持手部结构几何。计算手部点云内部点对之间的距离，当距离小于阈值时施加排斥力，防止手指间发生碰撞。</li>
</ul>
<p><strong>2. 物理感知训练</strong><br>标准扩散模型训练使用简单的均方误差损失，不包含物理约束。为了在训练中引入物理先验，本文提出物理感知训练。关键是利用扩散过程从带噪样本ht估计出一个相对干净的样本h0_hat，并以此计算物理约束损失。总训练损失是标准损失与各项物理约束损失的加权和，梯度通过h0_hat反向传播至ht。</p>
<p><strong>3. 物理引导采样器</strong><br>在采样（去噪）过程中，物理引导采样器在每一步都根据当前去噪得到的观测h0_hat计算物理约束，并将其作为引导信号来修正下一步的采样方向，从而逐步将生成分布导向物理可行的抓取配置。</p>
<p><strong>4. LLM增强的物体表示提取</strong><br>为了更有效地编码物体信息作为条件，该方法利用大型语言模型提取物体的语义特征，并与空间几何特征（如点云特征）相结合，形成综合的物体表示。</p>
<p><img src="https://arxiv.org/html/2503.08257v2/x2.png" alt="物理约束力可视化"></p>
<blockquote>
<p><strong>图2</strong>：三种物理约束力的可视化说明。红色箭头代表表面拉力，绿色箭头代表外部穿透排斥力，蓝色箭头代表自穿透排斥力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.08257v2/x3.png" alt="物体表示提取"></p>
<blockquote>
<p><strong>图3</strong>：LLM增强的物体表示提取模块。将物体点云和类别名称输入，分别提取几何特征和LLM语义特征，融合后作为扩散模型的条件输入。</p>
</blockquote>
<p>与现有方法相比，创新点具体体现在：首次系统地将多种物理约束同时整合到扩散模型的训练和采样全流程中，提出了可微分的物理感知训练损失和用于迭代优化的物理引导采样策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：方法在五个公开的灵巧抓取数据集上进行了评估：GRAB、HO3D、FHB、ObMan和DexYCB。实验平台为PyTorch，使用ShadowHand模型。评估指标包括抓取成功率（模拟中提起物体并保持稳定）、接触面积、手物穿透体积和手部自穿透体积。</p>
<p><strong>对比方法</strong>：对比的基线方法包括基于优化的GraspIt!、回归方法GraspNet、生成方法GraspDiff、FF-Dex以及另一个扩散模型方法DiffGrasp。</p>
<p><img src="https://arxiv.org/html/2503.08257v2/x4.png" alt="定量结果对比"></p>
<blockquote>
<p><strong>图4</strong>：在五个数据集上的定量结果对比。DexGrasp Anything (Ours) 在几乎所有数据集的所有指标上都达到了最优性能，尤其在抓取成功率上显著领先。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>整体性能</strong>：DexGrasp Anything 在五个数据集上近乎全面领先。例如，在GRAB数据集上，抓取成功率达到86.7%，远超GraspDiff的71.4%和DiffGrasp的65.8%；在HO3D数据集上成功率为83.3%，也显著高于其他方法。</li>
<li><strong>泛化性</strong>：在跨数据集测试中（如在ObMan上训练，在DexYCB上测试），该方法也展现了最佳的泛化能力，成功率达到78.0%，而其他方法均低于70%。</li>
<li><strong>多样性</strong>：该方法生成的抓取姿态在保证高质量的同时，也保持了良好的多样性。</li>
</ul>
<p><img src="https://arxiv.org/html/2503.08257v2/x5.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图5</strong>：与基线方法的定性结果对比。DexGrasp Anything 生成的抓取姿态更自然，手物接触更充分，且避免了明显的穿透（如第二行杯子把手处）。</p>
</blockquote>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2503.08257v2/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融实验。从左至右分别移除了物理感知训练(PAT)、物理引导采样(PGS)以及两者都移除。实验表明，PAT和PGS都对性能有显著贡献，二者结合效果最佳。</p>
</blockquote>
<ul>
<li>**物理感知训练(PAT)与物理引导采样(PGS)**：移除任一组件都会导致性能下降。同时移除两者后，成功率从86.7%大幅降至71.4%，穿透体积急剧增加，证明了物理约束整合的必要性。</li>
<li><strong>各物理约束力的贡献</strong>：表面拉力(SPF)对提升接触面积和成功率最关键；外部穿透排斥力(ERF)能最有效地减少手物穿透；自穿透排斥力(SRF)则专门用于减少手部自穿透。三者相辅相成。</li>
<li><strong>大规模数据集(DGA Dataset)的影响</strong>：使用本文构建的包含超过340万抓取姿态、1.5万物体的DGA数据集进行训练，能显著提升方法的泛化性能和抓取质量。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个物理感知的扩散生成模型用于灵巧抓取生成，首次将表面拉力、外部穿透排斥力和自穿透排斥力三种关键物理约束系统性地整合到扩散模型的训练和采样过程中。</li>
<li>方法在五个公开数据集上达到了最先进的性能，显著提升了抓取成功率和物理合理性。</li>
<li>构建并开源了目前规模最大、最多样的高质量灵巧抓取数据集DGA Dataset，包含超过340万个抓取姿态，对推动领域发展具有重要价值。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法的性能在一定程度上依赖于输入物体点云的质量和完整性。此外，评估主要基于物理模拟，在真实机器人上的部署可能面临动力学、感知误差等额外挑战。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>物理约束的泛化</strong>：本文验证了将物理知识嵌入数据驱动模型的有效性，此范式可推广至其他需要满足物理规则的机器人任务生成中。</li>
<li><strong>数据合成闭环</strong>：采用的“模型在环”数据合成策略（用生成模型和过滤器持续产生高质量数据）为构建大规模机器人数据集提供了新思路。</li>
<li><strong>约束设计</strong>：针对不同任务或手型，可以设计更精细或定制化的物理约束，以进一步提升生成结果的可操作性和鲁棒性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对灵巧手抓取物体时因高自由度与物体多样性导致生成高质量抓取姿态困难的挑战，提出DexGrasp Anything方法。该方法基于扩散模型，通过在训练和采样阶段系统集成物理约束目标，确保生成姿态符合物理规则。核心贡献包括提出一个包含超过340万抓取姿态、覆盖1.5万多种物体的新数据集，并在几乎所有公开数据集上取得了最优性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.08257" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>