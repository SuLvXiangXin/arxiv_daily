<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.17411" target="_blank" rel="noreferrer">2511.17411</a></span>
        <span>作者: Danda Pani Paudel Team</span>
        <span>日期: 2025-11-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为构建通用、端到端机器人控制系统的有前景范式。其成功依赖于两个因素：1）从互联网规模预训练的视觉语言模型中继承的强大视觉-语言理解能力，提供广泛的“常识”知识；2）在大量、多样的机器人演示数据上进行训练。然而，现有的通用VLA策略在跨机器人本体、环境和任务的泛化能力方面仍然存在局限。特别是在相机位置变化和分布外背景的真实世界新环境中，零样本性能表现不佳，例如Franka (DROID)设置中的典型部署场景。</p>
<p>大多数现有VLA模型（如OpenVLA、CogAct、SpatialVLA、MotoVLA）在“玩具”环境中能实现高零样本性能，但在未见过的、具有挑战性的Franka场景中零样本性能不佳，且依赖于任务或环境特定的微调。最近的研究如π₀和π₀.₅推动了更广泛的泛化，但代价是使用了封闭的大规模机器人数据。本文认为，一个主要的瓶颈在于其基础：大多数机器人基础模型是通过微调互联网预训练的视觉语言模型构建的。然而，这些VLM在2D图像-语言任务上训练，缺乏3D世界中进行具身控制所固有的3D空间推理能力。直接用大规模机器人数据来弥补这一差距成本高昂且难以扩展。</p>
<p>本文针对上述痛点，提出了一种新视角：不是依赖大量昂贵的机器人数据来隐式学习3D结构，而是用3D注释来丰富易于收集的非机器人图像数据，并增强预训练VLM的3D理解能力。本文的核心思路是：首先训练一个能从单张2D图像推断物体3D坐标的3D感知VLM，然后在此基础上构建一个将接地的3D感知与语言指令具身控制相结合的机器人基础模型，从而以远少于现有方法的数据量实现卓越的泛化性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>SPEAR-1采用分阶段的训练流程，旨在将互联网规模的2D感知逐步转化为可行动的3D交互行为。</p>
<p><img src="https://arxiv.org/html/2511.17411v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SPEAR-1的训练阶段。阶段0：在Web规模数据上进行通用VLM预训练。阶段1：集成单目深度视觉编码器，构建SPEAR-VLM，并在具身启发的VQA任务上训练。阶段2：添加动作专家，在机器人演示数据上训练SPEAR-1。</p>
</blockquote>
<p><strong>阶段1: SPEAR-VLM (3D感知视觉语言模型)</strong><br>该阶段的目标是通过在3D空间感知任务上微调，增强现成VLM的3D空间理解能力。</p>
<ul>
<li><strong>架构</strong>：以PaliGemma为骨干，集成了MoGe单目深度编码器作为额外的视觉编码器。选择MoGe是因为其仿射不变的建模方法，能适应不同内参的相机，有利于学习通用机器人控制。将MoGe ViT编码器最后4层的中间特征沿特征维度拼接，并通过一个随机初始化的线性投影器映射到LLM嵌入空间。LLM的视觉输入是SigLIP和MoGe投影器输出的平均值。为了将3D信息编码到文本中，扩展了PaliGemma的分词器，增加了N=1024个3D令牌。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.17411v1/x3.png" alt="SPEAR-VLM架构"></p>
<blockquote>
<p><strong>图3</strong>：SPEAR-VLM概述。左：训练数据混合、自动计算的空间注释以及每个类别的示例问答对。右：SigLIP和MoGe编码器之间融合的高级架构，以及用3D令牌扩展的PaliGemma嵌入。</p>
</blockquote>
<ul>
<li><strong>3D预训练任务</strong>：设计了受具身任务启发的视觉问答任务，以嵌入尽可能多与控制相关的3D知识。例如，“输出物体X的3D边界框顶点”或“输出物体X与物体Y之间距离的xyz分量”。这些任务确保模型学习语义3D定位、物体间空间关系和3D坐标系几何。</li>
<li><strong>数据与训练</strong>：由于缺少包含所需注释的公开数据集，设计了一个半自动注释流程，仅需2D图像和现成的视觉基础模型，即可生成物体级分割掩码、语义标签和投影的3D点云。主要标注了EgoExo4D的“烹饪”和“自行车修理”部分（20万张图像），并额外标注了Bridge-V2机器人演示数据集的3万帧。训练分为两个阶段：第一阶段仅训练随机初始化的权重和SigLIP投影器；第二阶段解冻并训练除SigLIP和MoGe编码器外的所有权重，并将3D令牌的下一词预测损失缩放因子λ设为2。</li>
</ul>
<p><strong>阶段2: SPEAR-1 (机器人基础模型)</strong><br>SPEAR-1的整体架构与π₀类似，但以SPEAR-VLM为基础，并在流匹配公式和数据处理等方面进行了多项改进。</p>
<ul>
<li><strong>流匹配公式</strong>：动作序列预测通过条件流匹配进行监督。动作分解为平移、旋转和夹持器分量。对于平移，采用线性空间中的流匹配；对于旋转，采用单位四元数流形𝕊³上的流匹配。训练时，在[0,1]区间采样随机时间步τ，并采样随机噪声，通过线性插值（平移）或球面线性插值（旋转）计算“带噪动作”。模型输入带噪动作序列和观测，输出去噪向量场。训练损失是平移损失和旋转损失之和，其中旋转损失结合了速度预测的余弦损失和积分旋转预测的测地线损失。推理时，从随机噪声开始，通过在平移的线性空间和旋转的𝕊³流形上进行积分来生成动作。</li>
<li><strong>关键设计决策</strong>：<ol>
<li><strong>图像分辨率</strong>：主外部相机分辨率280×210，腕部相机112×112。通过中心裁剪或填充调整大小，避免扭曲图像纵横比（从而避免扭曲相机内参）。</li>
<li><strong>微调视觉编码器</strong>：在VLM训练阶段同时微调SigLIP和MoGe编码器，但在VLA训练阶段冻结MoGe编码器。</li>
<li><strong>控制频率与数据归一化</strong>：使用动作块大小H=5，频率5Hz。采用全局分位数归一化鼓励跨数据集学习运动。</li>
<li><strong>旋转表示</strong>：使用半空间单位四元数，并发现所提出的𝕊³→𝕊³流形公式比线性流匹配ℝ⁴→𝕊³更稳定有效。</li>
<li><strong>评估与检查点</strong>：在SIMPLER WidowX环境中进行消融，并使用指数移动平均检查点以稳定性能。</li>
</ol>
</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，SPEAR-1的核心创新在于：1）<strong>显式3D感知预训练</strong>：在机器人训练之前，通过精心设计的3D VQA任务，将控制相关的空间推理直接嵌入VLM表示中，而非仅从机器人数据中隐式学习。2）<strong>数据高效性</strong>：利用易于获取的非机器人2D图像（附带3D注释）来“替代”大量昂贵的机器人演示数据。3）<strong>改进的流匹配公式</strong>：在𝕊³流形上处理旋转，提高了训练的稳定性和效果。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：使用来自Open X-Embodiment (OXE) 集合的24个数据集（约4500万帧）进行VLA预训练。对于特定评估，在Bridge V2（WidowX）和DROID（Franka）数据集上进行额外的微调。</li>
<li><strong>基线方法</strong>：对比了当前开源的先进VLA模型，包括π₀-FAST、π₀.₅、OpenVLA等。</li>
<li><strong>评估平台</strong>：在仿真（SIMPLER WidowX）和多个真实世界环境（Franka和WidowX机器人）上进行评估。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>SPEAR-VLM消融实验（SIMPLER WidowX）</strong>：<br>如表1所示，仅使用SPEAR-VLM架构但不进行物体级3D任务训练（第2行），相比基于PaliGemma的基线π₀模型（第1行）没有带来有意义的性能提升。然而，在所有3D物体级任务上训练SPEAR-VLM（第3、5行）则能显著提高性能。最佳配置是在VLM预训练阶段同时微调SigLIP和MoGe编码器，然后在VLA训练阶段冻结MoGe（第5行），在四个任务上的平均成功率从20.8%提升至35.4%。</p>
</li>
<li><p><strong>SPEAR-VLM与PaliGemma对比（Franka DROID）</strong>：<br>如表2所示，在Franka (DROID)平台上，使用SPEAR-VLM作为骨干训练的VLA模型，相比使用PaliGemma的模型，在“胡萝卜放盘子”等非DROID数据集任务上取得了明显更好的泛化性能，平均任务进度从34%提升至46%。</p>
</li>
<li><p><strong>真实世界WidowX评估</strong>：</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17411v1/x4.png" alt="WidowX评估"></p>
<blockquote>
<p><strong>图4</strong>：WidowX真实世界评估。SPEAR-1在所有任务上的平均任务进度比强基线OpenVLA高出10%。下图展示了对应的真实世界任务。</p>
</blockquote>
<ol start="4">
<li><strong>真实世界Franka评估</strong>：</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17411v1/x5.png" alt="Franka评估"></p>
<blockquote>
<p><strong>图5</strong>：Franka真实世界评估。在未经目标环境微调的情况下，SPEAR-1显著优于π₀-FAST，并与π₀.₅表现相当，而后两者使用了20倍多的机器人数据。下图展示了SPEAR-1保持强零样本性能的多样化挑战性环境。</p>
</blockquote>
<pre><code>如图1(b)和图5所示，在未见过的、具有挑战性的Franka (DROID)设置中，SPEAR-1的零样本性能优于π₀-FAST，并与π₀.₅相匹配，而SPEAR-1仅使用了后者**20分之1**的机器人演示数据。这证明了其卓越的数据效率。
</code></pre>
<ol start="5">
<li><strong>跨本体与仿真评估</strong>：</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17411v1/x6.png" alt="跨本体评估"></p>
<blockquote>
<p><strong>图6</strong>：跨本体和仿真评估。在多个机器人本体（WidowX, Franka）和不同环境（仿真SIMPLER，真实世界）中，SPEAR-1均表现出强大且一致的性能。</p>
</blockquote>
<pre><code>如图6所示，SPEAR-1在WidowX (Bridge)和Franka (DROID)两种不同的机器人本体上，以及在仿真和真实世界环境中，均展现了强大且一致的性能。
</code></pre>
<p><strong>消融实验总结</strong>：关键组件的贡献包括：1）<strong>物体级3D预训练任务</strong>是性能提升的必要条件；2）<strong>VLM训练阶段同时微调SigLIP和MoGe编码器</strong>，然后在VLA阶段<strong>冻结MoGe</strong>是最优策略；3）所采用的<strong>流匹配公式和数据处理改进</strong>（如保持图像纵横比、旋转表示等）共同提升了模型性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>SPEAR-VLM</strong>：一个具有控制启发的3D能力（如3D物体定位）的视觉语言模型，通过精心设计的VQA任务和丰富的非机器人2D图像数据训练，并能直接提升下游VLA性能。</li>
<li><strong>SPEAR-1</strong>：一个具有3D理解能力的开源机器人基础模型，在使用<strong>20倍</strong>更少机器人演示数据的情况下，显著优于或匹配最强的先进基线模型。</li>
<li><strong>公开资源与广泛验证</strong>：公开了模型权重和3D注释数据集，并通过大量实验验证了模型在多样化设置下的强泛化能力，以及对难以收集的机器人数据的依赖性大幅降低。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，与某些使用更大规模、更多样化专有数据集的闭源模型（如Gemini Robotics 1.0）相比，SPEAR-1是在有限、多样性较低的开源数据上训练的较小模型。这可能在极端分布外泛化方面存在限制。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>3D理解是关键基础</strong>：显式地为VLM注入3D空间推理能力，是构建数据高效、泛化能力强的机器人基础模型的有效途径。</li>
<li><strong>非机器人数据的价值</strong>：利用易于扩展的非机器人数据（辅以自动或半自动3D标注）来预训练模型，可以显著减少对昂贵机器人演示数据的依赖，为扩展模型规模提供了新思路。</li>
<li><strong>模块化与分阶段设计</strong>：将复杂的机器人控制问题分解为3D感知预训练和动作策略学习两个阶段，并通过精心设计的接口（如3D令牌、流匹配）连接，是一种可扩展且有效的系统设计范式。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人基础模型（RFMs）在新环境、任务和体现形式中泛化能力有限的核心问题，指出瓶颈在于现有模型基于缺乏3D空间推理的2D视觉语言模型（VLMs）。提出关键技术方法：通过3D注释增强易收集的非机器人图像数据，训练能单图推断3D物体坐标的SPEAR-VLM，并构建集成3D感知与语言控制的基础模型SPEAR-1。实验表明，SPEAR-1在约45M帧数据上训练，零样本性能在Franka（DROID）设置中优于π0-FAST、匹配π0.5，同时机器人演示数据用量减少20倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.17411" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>