<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1 - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.01358" target="_blank" rel="noreferrer">2512.01358</a></span>
        <span>作者: Songhwai Oh Team</span>
        <span>日期: 2025-12-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人领域的主流方法是基于大规模视觉-语言-动作（VLA）模型的基础机器人策略，如Isaac GR00T，它们利用海量演示数据和基于扩散的动作生成，展现出强大的通用控制能力。然而，这些策略在部署到需要精确感知接触事件、可靠深度推理和交互感知控制的精细操作场景时，仍存在显著局限性。现有机器人数据集因异构的本体、传感配置和控制模式而形成“数据孤岛”，这阻碍了基础模型像语言或视觉模型那样利用互联网规模的数据。此外，跨本体泛化极具挑战性，因为机器人形态和运动学结构从根本上塑造了可行运动和交互策略的分布。</p>
<p>本文针对现有基础策略在感知模态上的关键瓶颈，提出了一个数据中心和模态增强的微调新视角。具体而言，公开的GR1数据集仅提供RGB观测和本体感知状态，缺乏深度、接触指示或力测量等关键交互中心模态。这导致VLA模型必须仅从彩色图像推断接触和物体交互边界，在涉及遮挡或视觉模糊接触转换的场景中，这是一个不适定问题。本文的核心思路是：通过后处理为GR1数据集增加深度和接触信号，并为缺乏公开数据的Unitree G1本体构建全新的包含真实接触力的多模态数据集，从而对基础策略进行针对性模态增强的微调，以提升跨本体操作的鲁棒性和成功率。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架基于Isaac GR00T基础策略进行模态增强的微调。输入是增强后的多模态观测（如RGB-D图像、包含接触信号的本体感知状态），输出是通过扩散去噪过程生成的动作序列。核心模块主要包括针对GR1的模态后处理增强，以及针对G1的全新多模态数据集构建与对应的编码策略。</p>
<p><img src="https://arxiv.org/html/2512.01358v1/gr00t_pic.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：GR00T双系统架构概览。系统2（VLM）将图像观测和语言指令处理成语义令牌，而系统1（扩散Transformer）通过以多模态令牌为条件的迭代去噪生成电机动作。</p>
</blockquote>
<p>首先，对于G1本体，由于没有公开数据集，作者构建了一个高质量的数据收集流程。使用cuRobo的GPU加速运动规划器生成平滑无碰撞的末端执行器轨迹，并通过解析逆运动学求解器转换为G1关节空间命令，从而获得高时序一致性的参考轨迹。在轨迹执行过程中，记录完整的本体感知状态（关节位置、速度、末端执行器姿态）和执行的关节动作。最关键的是，利用G1物理引擎提取指尖和手掌的真实接触力，为策略提供细粒度的交互线索。</p>
<p><img src="https://arxiv.org/html/2512.01358v1/g1_data.png" alt="G1数据收集环境"></p>
<blockquote>
<p><strong>图2</strong>：G1数据收集环境可视化。左图：场景设置的前视图。右图：演示执行期间机器人车载摄像头的自我中心视图。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.01358v1/data_state_action.png" alt="G1关节状态与动作"></p>
<blockquote>
<p><strong>图3</strong>：每个G1关节的状态与动作。对于每个关节，我们可视化了随时间变化的本体感知状态（蓝色）和执行的动作命令（红色）。通过cuRobo规划生成的轨迹在20个自由度的上肢链上展现出平滑、一致的演变，确保了扩散策略微调的高质量监督。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.01358v1/contact_data.png" alt="G1指尖接触力"></p>
<blockquote>
<p><strong>图4</strong>：随时间变化的每根手指接触力。G1灵巧手提供了高保真的指尖和手掌力测量。在执行抓取期间，接触力集中在特定手指（左拇指、中指和手掌），而其他手指保持不活动。这些丰富的交互信号用于训练接触感知的扩散策略。</p>
</blockquote>
<p>其次，对于GR1数据集，作者进行了两种模态增强。一是<strong>深度增强（RGB-D融合）</strong>：使用ZoeDepth单目深度估计器为所有GR1演示生成度量深度图。通过将视觉编码器的patch嵌入层从3通道（RGB）扩展到4通道（RGB-D）来融入深度信息，并对深度通道权重进行RGB核平均初始化以稳定训练。二是<strong>接触增强</strong>，提出了两种策略：1) <strong>轻量级融合</strong>：将模拟器中推导出的二进制接触指示符<code>c_t</code>直接拼接到原始本体感知状态向量中，然后通过本体特定的状态编码器获得接触感知嵌入。2) <strong>专用接触编码器模块</strong>：将接触信号通过一个可学习的MLP进行编码，作为一个独立的模态令牌输入扩散Transformer，使其与视觉、语言和状态模态具有同等重要性。</p>
<p><img src="https://arxiv.org/html/2512.01358v1/contact_encoder.png" alt="专用接触编码器模块"></p>
<blockquote>
<p><strong>图5</strong>：专用接触编码器模块。二进制（或连续）接触信号<code>c_t</code>由可学习的接触编码器处理，生成的嵌入作为独立模态与视觉、语言和状态令牌一同输入DiT块。这种设计将接触视为独立模态，允许策略学习更丰富的交互感知表示。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.01358v1/rgbanddepth.png" alt="RGB与估计深度图"></p>
<blockquote>
<p><strong>图6</strong>：GR1演示的RGB和估计深度图。左图：来自GR1模拟器的原始RGB观测。右图：由ZoeDepth计算出的对应度量深度图。深度通过捕捉物体形状、桌子高度和柜子几何形状等空间结构来增强几何推理。</p>
</blockquote>
<p>与现有方法相比，本文的创新点不在于修改GR00T核心的扩散Transformer架构，而在于<strong>针对性地设计和构建输入模态</strong>。通过数据后处理（对GR1）和高质量数据生成（对G1）来弥补基础模型训练数据中缺失的关键交互模态，并通过轻量级的编码器调整将这些新模态集成到已有的策略框架中。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在单个NVIDIA A6000 GPU上进行，使用官方GR00T微调框架。在线评估在基于robosuite和robocasa的模拟器中通过服务器-客户端推理流水线进行。离线评估使用数据集的轨迹重建均方误差（MSE）。评估了两个平台：GR1（使用HuggingFace上的公开数据集，三个取放任务）和Unitree G1（使用自定义的“Pick Apple to Bowl”任务数据集）。</p>
<p><strong>基线方法</strong>：对比了零样本GR00T N1.5、标准微调（仅用原始或基础数据），以及本文提出的各种模态增强变体（+深度、+接触状态融合、+专用接触编码器）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>GR1实验（模态增强分析）</strong>：在线评估表明，接触状态增强提供了适度增益，而深度增强在视觉模糊场景下显著提高了成功率。结合深度和接触的完整模型在所有评估任务中取得了最高成功率。具体而言，在GR1上，添加接触状态线索和RGB-D融合将在线成功率从51%提高到了63%。离线MSE评估显示，深度增强对轨迹重建质量的改进最为显著，表明几何线索大大减少了动作预测的模糊性。</p>
</li>
<li><p><strong>G1实验（跨本体迁移）</strong>：在线评估结果（见表I数据）显示，零样本GR00T在G1任务上完全失败（0%成功率），凸显了GR1与G1在本体上的巨大差距。使用5K条本体对齐演示进行标准微调将成功率提升至48%。融入接触力测量带来了显著额外增益：将力信号作为辅助模态建模使成功率增至74%；而将力信号直接融合到本体感知状态表示中取得了最佳性能，成功率高达94%。深度增强也贡献了实质改进，达到82%的成功率，但效果不及接触增强。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.01358v1/online_inference.png" alt="在线推理模拟环境"></p>
<blockquote>
<p><strong>图7</strong>：用于在线评估的示例模拟环境。每次 rollout 随机化物体放置、纹理、光照和机器人配置以评估鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.01358v1/eval_data.png" alt="动作预测对比"></p>
<blockquote>
<p><strong>图8</strong>：在前三个动作维度上，真实动作、预测动作和执行推理点之间的比较。模型紧密跟踪演示轨迹，表明基于扩散的去噪过程稳定且动作重建准确。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>在GR1上</strong>，深度和接触信息是互补的。深度主要改善几何理解和离线重建精度，而接触信息有助于在线操作的稳定性，两者结合效果最佳。</li>
<li><strong>在G1上</strong>，接触力信息比深度信息更为关键。特别是将接触力直接融合到本体感知状态中的“早期融合”策略，比使用独立接触编码器效果更好，这表明对于G1的灵巧操作，接触动力学需要与本体状态进行更紧密的耦合。这揭示了最优模态设计是<strong>本体特定</strong>的：GR1主要从几何增强中获益，而G1则更依赖于接触丰富的监督来补偿其更高的操作灵巧性和交互复杂性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>构建了多模态GR1和G1数据集</strong>：通过后处理为GR1添加了深度和接触信号，并利用cuRobo运动规划和真实力测量为G1创建了全新的高质量多模态数据集，解决了当前VLA基础策略的模态瓶颈。</li>
<li><strong>提出了轻量级的架构增强策略</strong>：提出了两种将接触和深度信号融入GR00T扩散策略的方法（轻量级状态融合和专用接触编码器），无需修改核心模型结构。</li>
<li><strong>实证了模态增强微调的有效性</strong>：系统性的实验表明，模态增强微调能显著提高操作成功率，并实现从GR1到G1的强跨本体迁移，缩小了因形态差异带来的性能差距。</li>
</ol>
<p><strong>论文提到的局限性</strong>：研究强调了最优模态选择强烈依赖于机器人形态和传感能力。本文的模态设计是针对GR1和G1两个特定本体进行的，尚未探索更广泛的、自动化的模态选择方法。</p>
<p><strong>对后续研究的启示</strong>：这项工作展示了一条通过针对性模态设计和多模态微调来扩展基础机器人策略的实用路径。未来研究可以探索如何自动确定目标本体所需的关键模态，开发更通用的多模态融合架构，以及利用更高效的数据生成技术（如仿真或生成模型）来规模化构建多模态机器人数据集，从而进一步推动基础机器人策略的泛化能力和实际部署。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对基础机器人策略在跨具身操作中因模态缺失（如接触和深度信息）导致的性能局限，提出模态增强微调框架。方法包括：在GR1上通过后处理添加二进制接触信号与ZoeDepth深度；在G1上构建多模态数据集，集成cuRobo运动规划与真实接触力测量。实验显示，GR1成功率从51%提升至63%；G1“取苹果入碗”任务中，零射击成功率为0%，标准微调达48%，而接触增强模型最高达到94%，验证了模态增强对跨具身泛化的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.01358" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>