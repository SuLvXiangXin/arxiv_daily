<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.09491" target="_blank" rel="noreferrer">2506.09491</a></span>
        <span>作者: Hong Liu Team</span>
        <span>日期: 2025-06-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>透明与反光物体由于其独特的视觉特性（如镜面反射和光线透射），给深度传感器带来了巨大挑战，常导致深度信息不完整或不准确，严重影响基于几何的下游视觉任务。现有方法存在局限：依赖特殊设备的方法缺乏对常用RGB-D相机的适应性；多视角方法虽能改善深度估计，但通常需要多个视角，不适用于仅能获取单视角的现实场景。本文针对单视角RGB-D输入下透明与反光物体深度缺失的具体痛点，提出了一种新的多阶段监督与深度细化模型DCIRNet，旨在有效融合RGB与深度模态以增强深度补全。其核心思路是：通过新颖的多模态特征融合模块提取互补信息，并采用从粗到细的多阶段监督与迭代细化策略，逐步提升深度补全质量，特别是改善物体边界模糊问题。</p>
<h2 id="方法详解">方法详解</h2>
<p>DCIRNet的整体框架主要包括双分支编码架构、多模态融合模块和深度细化模块。输入为RGB图像和带有缺失值的原始深度图，输出为补全后的高质量深度图。</p>
<p><img src="https://arxiv.org/html/2506.09491v1/x2.png" alt="DCIRNet Architecture"></p>
<blockquote>
<p><strong>图2</strong>：DCIRNet架构。网络主要由RGB编码器、深度编码器、多模态融合模块、解码器和深度细化模块组成。RGB图像和深度图首先输入各自编码器提取多阶段特征；这些特征在每一阶段由融合模块进行融合；解码器以融合特征为输入生成初始深度预测；最后，深度细化模块基于解码器特征、初始预测深度和原始稀疏深度图进行迭代优化，生成更高质量的深度补全结果。</p>
</blockquote>
<p><strong>双分支编码架构</strong>：采用双分支设计，分别使用Swin Transformer作为RGB和深度模态的编码骨干，以独立提取各自的特征。相比将RGB和深度简单拼接后输入单分支网络，此设计能获得更优的特征表示。编码器提取的多尺度特征被送入解码器，解码器采用UPerNet架构，利用金字塔池化模块捕获全局上下文并整合多级特征以进行深度预测。</p>
<p><strong>跨模态融合模块</strong>：该模块旨在自适应地整合RGB与深度特征，抑制深度图中无效缺失区域的影响。首先，通过线性层投影各模态特征并进行像素求和，得到融合权重<code>W_fuse</code>（公式1）。接着，使用不同尺寸的卷积核提取各模态的多尺度特征并拼接投影，得到模态特定权重<code>W_rgb</code>和<code>W_depth</code>（公式2）。然后，将融合权重与模态特定权重结合并通过softmax归一化，得到最终的模态权重（公式3）。随后，使用这些权重对原始特征进行交叉加权（公式4），即RGB特征由深度权重调制，深度特征由RGB权重调制，以促进信息互补。最后，通过深度可分离卷积和残差连接进一步处理融合特征，增强局部上下文信息（公式5）。</p>
<p><strong>深度细化模块</strong>：该模块基于空间传播机制，对解码器生成的初始深度预测<code>D&#39;</code>进行迭代优化，以锐化边界、提升精度。细化过程如公式6所示，其中亲和力图<code>κ&#39;</code>由解码器特征<code>F_d</code>经卷积层生成，并施加L1范数约束以保证数值稳定；<code>κ</code>为归一化后的亲和力权重，用于在局部<code>k×k</code>窗口内聚合邻域像素信息进行迭代更新（t表示迭代步）。同时，将原始深度图<code>I</code>中的有效测量值嵌入传播过程，如公式7所示，其中<code>φ</code>为置信度值，<code>𝕀</code>用于提取有效深度值，确保已知深度信息在迭代中得以保持。最终的细化深度是不同卷积核尺寸（<code>k∈{3,5,7}</code>）和不同迭代阶段（<code>t∈{0, ⌊T/2⌋, T}</code>）输出的加权和（公式8），权重<code>α</code>和<code>β</code>可学习。</p>
<p><strong>多阶段监督</strong>：网络对初始深度预测<code>\tilde{D}</code>和细化后的深度输出<code>\hat{D}</code>均施加监督，构成从粗到细的两阶段监督策略。总损失函数如公式9所示，是粗预测损失和细预测损失的加权和。每一项损失<code>L_i</code>（i为<code>\tilde{D}</code>或<code>\hat{D}</code>）均由深度损失<code>L_d</code>、法向损失<code>L_n</code>和梯度损失<code>L_g</code>三部分组成（公式10），以同时约束深度值精度、表面几何和边缘清晰度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与评估指标</strong>：实验在公开数据集DREDS（包含CatKnown和CatNovel两个子集）和TransCG上进行。评估指标包括RMSE、REL、MAE和阈值准确率δ（取1.05， 1.10， 1.25）。</p>
<p><strong>基线方法</strong>：对比方法包括TransCG、DepthFormer、GuideNet、FusionNet、NLSPN、ACMNet、PENet以及基于Swin Transformer的双分支网络等。</p>
<p><strong>关键实验结果</strong>：<br>在DREDS-CatKnown测试集上，DCIRNet在RMSE、REL、MAE和δ1.05等多个指标上均取得最佳性能，例如RMSE为0.0292，显著优于其他方法。<br>在DREDS-CatNovel测试集上，DCIRNet同样表现出最优的泛化能力，RMSE为0.0334。<br>在TransCG数据集上，DCIRNet也取得了最具竞争力的结果。</p>
<p><img src="https://arxiv.org/html/2506.09491v1/x3.png" alt="定量结果对比表（DREDS-CatKnown）"></p>
<blockquote>
<p><strong>图3</strong>：在DREDS-CatKnown测试集上的定量结果对比。DCIRNet在RMSE、REL、MAE和δ1.05等多个指标上均排名第一，展示了其优越性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09491v1/x4.png" alt="定量结果对比表（DREDS-CatNovel）"></p>
<blockquote>
<p><strong>图4</strong>：在DREDS-CatNovel测试集上的定量结果对比。DCIRNet在跨类别泛化测试中依然保持领先，证明了其强大的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09491v1/x5.png" alt="定量结果对比表（TransCG）"></p>
<blockquote>
<p><strong>图5</strong>：在TransCG测试集上的定量结果对比。DCIRNet取得了最具竞争力的结果，表明方法在不同数据集上的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09491v1/x6.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图6</strong>：不同方法的深度补全定性结果对比。DCIRNet能更完整地恢复透明物体的深度，且物体边界更清晰，细节更准确。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09491v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验结果。实验验证了双分支编码、多模态融合模块（CMFM）和深度细化模块（DRM）各自的有效性，三者结合带来最大性能提升。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验表明，双分支编码器相比单分支设计能带来性能提升；引入多模态融合模块（CMFM）进一步降低了误差；而深度细化模块（DRM）的贡献最为显著，大幅提升了所有指标。三者共同构成了DCIRNet高性能的基础。</p>
<p><strong>下游任务应用</strong>：<br>将DCIRNet的深度补全结果集成到多指灵巧抓取框架中，可显著提升抓取成功率。</p>
<p><img src="https://arxiv.org/html/2506.09491v1/x8.png" alt="灵巧抓取成功率提升"></p>
<blockquote>
<p><strong>图8</strong>：在透明与反光物体上的灵巧抓取成功率对比。使用DCIRNet补全深度后，抓取成功率提升了44%，证明了其在机器人操作中的实用价值。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的多模态特征融合模块，通过自适应权重学习和交叉特征调制，有效促进了RGB与深度模态间的信息互补。</li>
<li>设计了一种多阶段监督和深度细化策略，通过从粗到细的迭代优化，显著提升了深度补全的精度和边界清晰度。</li>
<li>在多个公开数据集上进行了全面评估，证明了DCIRNet的优越性能和强大的跨数据集、跨类别泛化能力。</li>
<li>成功将深度补全模型应用于下游灵巧抓取任务，使透明与反光物体的抓取成功率提升了44%，验证了其实际应用潜力。</li>
</ol>
<p><strong>局限性</strong>：论文未明确阐述自身局限性，但其所针对的单视角深度补全问题本身，相较于可利用多视角几何信息的方法，在理论上存在信息不足的固有挑战。</p>
<p><strong>启示</strong>：DCIRNet所采用的融合与细化框架具有通用性，其思路可迁移至其他需要处理多模态输入或需进行结果后处理细化的视觉任务。同时，工作展示了高质量的深度补全对提升机器人复杂操作任务（如灵巧抓取）性能的关键作用，为机器人感知-操作闭环系统提供了有力工具。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对透明与反光物体因光线特性导致深度信息缺失、进而影响机器人灵巧抓取的问题，提出DCIRNet深度补全网络。其关键技术包括：设计多模态特征融合模块，融合RGB与深度图的互补信息；采用多阶段监督与深度细化策略，逐步优化补全结果并锐化物体边界。实验表明，将该模型集成至灵巧抓取框架后，对透明与反光物体的抓取成功率提升44%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.09491" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>