<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16050" target="_blank" rel="noreferrer">2511.16050</a></span>
        <span>作者: Yuki Uranishi Team</span>
        <span>日期: 2025-11-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>水下机器人操作面临波长相关衰减、散射、浑浊度和非均匀照明等挑战，导致同一场景的视觉外观在几秒内发生剧烈变化。这破坏了标准视觉运动策略所需的视觉一致性，即使在简单的拾放任务上也常常导致灾难性的动作漂移。现有的水下图像增强方法（如SelfLUID-Net、DeepSeeColor）旨在改善图像质量，但并未解决根本的控制问题：闭环操作策略必须使感知和动作生成都能适应快速变化的光照。另一方面，传统的双边控制模仿学习（Bi-IL，如Bi-ACT）通过交换位置和力反馈，在接触丰富的操作中展现出鲁棒性，但现有工作均假设视觉环境稳定，未考虑水下特有的剧烈光照变化。因此，目前缺乏一个同时将双边控制模仿学习引入水下机器人操作、并将水下光照作为影响视觉观察的潜变量进行显式建模的框架。</p>
<p>本文针对这一空白，提出了首个用于水下机器人手臂的双边控制模仿学习框架Bi-AQUA。其核心思路是：在Bi-ACT框架基础上，引入一个分层的光照适应机制，通过一个无标签的光照编码器提取光照表征，并利用FiLM调制视觉主干特征以及向Transformer编码器注入显式光照令牌，使视觉运动策略的感知和控制均能适应静态和动态变化的水下光照。</p>
<h2 id="方法详解">方法详解</h2>
<p>Bi-AQUA的整体框架继承自Bi-ACT的双边控制范式，并集成了三层光照感知的视觉处理模块。其输入为多视角水下RGB图像和跟随者关节状态，输出为预测的领导者动作块，用于闭环双边控制。</p>
<p><img src="https://arxiv.org/html/2511.16050v1/fig/model-v5.png" alt="方法总览"></p>
<blockquote>
<p><strong>图5</strong>：Bi-AQUA方法总览。给定多视角水下观测和跟随者关节状态，系统通过光照编码器提取光照表征，利用FiLM调制视觉主干特征，并将光照令牌与潜在编码、关节状态令牌一同输入Transformer编码器。解码器通过交叉注意力访问包含光照信息的记忆，预测领导者动作块。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>无标签光照编码器</strong>：该模块旨在无需人工标注（水下环境中难以定义）的情况下，从RGB图像中提取紧凑的光照表征 $\mathbf{v}<em>L$。它采用双路径架构（见图6）：一个卷积路径通过多层卷积和全局平均池化提取空间特征 $\mathbf{f}</em>{\text{conv}}$；一个直方图路径在图像的饱和度(S)和明度(V)通道上计算2D直方图 $\mathbf{H}$，再通过MLP得到特征 $\mathbf{f}_{\text{hist}}$。两者拼接后通过线性层得到最终的光照嵌入。对于多摄像头，共享编码器并对各视角结果取平均。</p>
</li>
<li><p><strong>FiLM调制的视觉主干</strong>：为了进行光照自适应的特征提取，Bi-AQUA使用FiLM机制调制视觉主干（如ResNet）的特征。具体地，对于主干第 $\ell$ 层的特征图 $\mathbf{F}_\ell$，FiLM操作执行逐通道的仿射变换：$\operatorname{FiLM}(\mathbf{F}_\ell \mid \boldsymbol{\gamma}_\ell, \boldsymbol{\beta}_\ell) = (1 + \boldsymbol{\gamma}_\ell) \odot \mathbf{F}_\ell + \boldsymbol{\beta}_\ell$。其中，缩放和偏移参数 $\boldsymbol{\gamma}_\ell, \boldsymbol{\beta}_\ell$ 由光照嵌入 $\mathbf{v}_L$ 经过线性投影生成。此设计在参数为零时保持恒等映射，利于稳定训练。本文在最后一个ResNet层应用FiLM。</p>
</li>
<li><p><strong>光照令牌与Transformer预测模块</strong>：经过FiLM调制并扁平化的视觉特征令牌与关节状态编码拼接，构成观测序列。关键创新在于，将一个显式的光照令牌添加到Transformer编码器的输入序列中。该令牌与潜在动作编码、观测序列一起被编码器处理，其信息流入编码器的记忆（memory）中。随后，Transformer解码器通过交叉注意力访问该记忆，从而使动作块的生成能够以序列级别（sequence-level）的条件适应于当前光照。</p>
</li>
</ol>
<p><strong>训练与推理</strong>：模型采用条件变分自编码器（CVAE）框架，通过行为克隆进行端到端训练，损失函数为动作预测的L1损失加上潜在空间的KL散度正则项。推理时（见图7），系统在闭环中运行：根据当前观测计算光照嵌入、调制视觉特征、从先验采样潜在编码，并由Transformer解码器生成动作块，通过双边控制器执行。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在一个真实水下拾放任务中评估Bi-AQUA。硬件包括一对三关节加夹爪的机器人手臂（跟随者置于水槽中），以及一个可提供八种可复现光照条件（红、蓝、绿、白、rgbw、青、紫、动态变化）的水族馆LED灯。任务要求机器人从水槽右侧抓取积木并放入左侧的篮子中。使用两个RGB摄像头（顶部和夹爪）提供观测。</p>
<p><strong>数据集</strong>：在五种训练光照条件（红、蓝、绿、白、rgbw）下各收集2条双边遥操作演示，共10条。策略仅从跟随者端观测进行训练和评估，领导者关节状态作为动作标签。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li>**Bi-ACT (Baseline)**：无任何光照建模的双边基线。</li>
<li><strong>Bi-ACT+LE-Token</strong>：仅使用光照编码器和光照令牌，无FiLM调制。</li>
<li><strong>Bi-ACT+LE-FiLM</strong>：仅使用光照编码器和FiLM调制，无光照令牌。</li>
<li>**Bi-AQUA (Ours)**：完整方法，包含全部三个组件。</li>
</ul>
<p><strong>关键定量结果</strong>：</p>
<p><img src="https://arxiv.org/html/2511.16050v1/fig/ex1-auto-v8.png" alt="自主执行结果"></p>
<blockquote>
<p><strong>图12</strong>：Bi-AQUA在不同光照条件下的自主执行示例，展示了其在包括动态变化光照在内的各种挑战性环境中的鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.16050v1/fig/Time-v2.png" alt="执行时间分析"></p>
<blockquote>
<p><strong>图13</strong>：不同方法在不同光照条件下完成任务的执行时间对比。Bi-AQUA的执行时间最接近人类遥操作参考，且在不同光照下保持稳定高效。</p>
</blockquote>
<ol>
<li><strong>光照鲁棒性（表III）</strong>：在全部八种测试光照（包括训练未见过的青、紫、动态变化）下，Bi-AQUA在七种条件下达到100%成功率，在最具挑战性的蓝光下为80%。尤其在光照每2秒循环变化的动态模式下，Bi-AQUA成功率高达100%，而基线Bi-ACT为0%。这证明了显式光照建模的必要性。</li>
<li><strong>消融实验（表II, III）</strong>：Bi-ACT+LE-Token在所有光照下均失败，表明仅高层序列条件不足以纠正水下颜色偏移。Bi-ACT+LE-FiLM在静态或未见静态光下表现良好（部分达100%），但在动态变化光下骤降至20%，说明FiLM单独无法跟踪快速时间变化。只有三者结合的Bi-AQUA保持了全面高性能，验证了分层光照推理的必要性。</li>
<li><strong>执行效率（图13）</strong>：平均而言，Bi-AQUA完成任务耗时15.73秒，最接近人类遥操作（15.39秒），且方差小。而Bi-ACT+LE-FiLM虽在某些静态光下成功率高，但耗时更长（19.61秒），行为更谨慎。Bi-AQUA在保持鲁棒性的同时兼顾了效率。</li>
<li><strong>泛化能力（表III）</strong>：针对新物体（黑色橡胶块、蓝色海绵）和视觉干扰（气泡），Bi-AQUA展现了良好的泛化能力。例如，对于黑色块，在多种光照下成功率介于60%-100%；即使在出现严重外观变化的蓝色海绵和气泡干扰下，仍能保持非零的成功率。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个用于水下机器人手臂的双边控制模仿学习框架Bi-AQUA，将力反馈丰富的双边控制与水下操作的挑战相结合。</li>
<li>设计了一个分层的、显式的光照适应机制，包括无标签光照编码器、FiLM视觉特征调制和Transformer光照令牌，使策略能自适应静态和动态水下照明。</li>
<li>通过真实水下实验验证了框架的优越性，在动态变化光照下取得100%成功率，显著优于基线，并展示了良好的泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，实验仅基于10条演示数据进行训练，虽然展现了泛化潜力，但更大规模的数据集可能进一步提升性能。此外，对于极端视觉扰动（如大量浑浊或气泡）的鲁棒性仍有待探索。</p>
<p><strong>后续启示</strong>：Bi-AQUA将光照作为结构化潜变量进行建模的思路，可扩展到其他存在严重感知退化的领域（如雾天、极低光照）。其分层设计（特征级调制+序列级条件）为构建适应快速变化环境的鲁棒视觉运动策略提供了新范式。未来工作可探索更高效或更具物理可解释性的光照表示学习方法。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对水下机器人操作面临的光照剧烈变化、颜色失真等挑战，提出了首个基于双边控制的模仿学习框架Bi-AQUA。其核心技术是三层光照适应机制：照明编码器自动提取光照表征、FiLM调制实现自适应视觉特征提取、以及在Transformer输入中添加显式光照标记。在真实水下拾放任务的实验中，该框架在不同光照条件下表现出鲁棒性，性能显著优于未建模光照的双边基线，且消融研究证实了所有光照感知组件的关键作用。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16050" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>