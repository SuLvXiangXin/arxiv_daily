<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.27558" target="_blank" rel="noreferrer">2510.27558</a></span>
        <span>作者: Shinkyu Park Team</span>
        <span>日期: 2025-10-31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域，利用基础模型的主流方法存在关键局限性。基于大型语言模型（LLM）的规划器擅长符号推理，但缺乏对物理世界的空间理解。视觉语言模型（VLM）可通过图像标注生成轨迹，但往往容易出错，且难以推广到长时程规划。视觉语言动作（VLA）模型能直接从图像和语言映射到机器人动作，但需要海量训练数据，且泛化能力有限。本文针对这些痛点，提出了一种新的视角：通过整合多个现成的、未经特定领域训练的基础模型，并利用动态维护的场景图作为空间感知和一致推理的核心，来桥接语言与动作之间的鸿沟。本文核心思路是设计一个分层架构，将LLM的推理能力、VLM的感知能力与传统的运动规划和控制相结合，通过场景图提供结构化环境表示，从而实现无需领域特定训练即可处理复杂长时程操作任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架是一个分层架构，旨在将用户的自然语言指令转化为可执行的机器人动作序列。整个流程无需针对具体任务进行训练或微调。</p>
<p><img src="https://arxiv.org/html/2510.27558v1/figures/first_page_illustration.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提出框架的概览。该框架组织为多个层级，旨在将用户的高级自然语言命令翻译成可执行的机器人动作序列。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.27558v1/figures/scenegraph.png" alt="场景图与架构"></p>
<blockquote>
<p><strong>图2</strong>：（a）场景图结构示例。（b）系统架构。各层按自底向上的层次结构组织。</p>
</blockquote>
<p>框架的核心是<strong>场景图</strong>，它作为中央知识库，以节点（代表实体，如工作空间、物体）和边（代表包含关系）的形式编码空间关系、物体属性和语义细节。场景图使用NetworkX库实现，采用分层的JSON表示，可由LLM自动生成或手动创建，并在任务执行过程中由LLM动态更新，以保持环境状态的一致性。</p>
<p>框架包含四个功能层：</p>
<ol>
<li><strong>认知层</strong>：负责高级推理和规划。采用Google的Gemini 2.5 Pro Preview模型，它整合来自场景图、用户目标和工具定义的信息，生成多步骤、考虑约束的任务策略。由于其推理速度较慢，该层专司规划，与执行解耦。</li>
<li><strong>感知层</strong>：连接视觉输入与语义、几何理解。其核心是一个LLM-VLM对话系统，其中LLM（GPT-4.1）向VLM（Qwen2.5-VL）发出结构化查询。主要功能包括：<strong>边界框定位</strong>（VLM输出目标物体的像素级边界框，结合深度数据转换为3D坐标）和<strong>特定点查询</strong>（VLM直接输出图像平面上的像素坐标，用于寻找空闲放置点等）。Qwen2.5-VL因其强大的空间推理和物体定位能力而被选用，且无需针对特定领域重新训练。</li>
<li><strong>交互层</strong>：协调任务执行。采用OpenAI的GPT-4.1模型，它接收认知层产生的任务序列，并逐步调用可用工具（如物体操作原语、感知查询、场景图更新）。该层监控每个函数调用的返回结果，必要时动态重新规划或调整序列，同时与用户进行状态更新和 clarification 的交互。</li>
<li><strong>执行层</strong>：将规划任务转化为安全、精确的机器人运动。采用NVIDIA cuRobo的GPU加速运动规划器来生成无碰撞轨迹，并通过ROS2与上层通信，协调机械臂和夹爪的控制。</li>
</ol>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>无需训练的系统集成</strong>：直接组合现成的LLM、VLM和运动规划器，避免了数据密集型的端到端训练。2) <strong>场景图驱动的空间推理</strong>：将动态场景图作为连接符号推理（LLM）和几何感知（VLM）的桥梁，为长时程规划提供了持续且一致的空间上下文。3) <strong>规划与执行的解耦</strong>：利用强大的但较慢的模型（Gemini）进行规划，利用快速的模型（GPT-4.1）进行执行和交互，兼顾了推理深度和响应效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实机器人平台上进行，使用UR10e协作机械臂（6自由度）、OnRobot RG6夹爪以及手腕安装的Zivid 2 3D相机。由于目前缺乏评估此类框架的成熟基准，实验旨在通过一系列日益复杂的桌面操作任务来展示框架在感知、推理和规划方面的能力。评估指标包括：规划可行性（PF，生成任务序列的可行性）、任务完成率（TCR，成功执行任务的百分比）和场景图处理（SGH，世界模型更新的正确性）。</p>
<p>实验分为三类：</p>
<ol>
<li><strong>实验I：测试基本能力</strong>：包括相对定位（如“将橘子移到苹果和毛线之间”）、语义聚类（根据上下文移动孤立的水果）、基于上下文的操作（异常值检测、根据菜谱选择食材）。</li>
<li><strong>实验II：结构化基准启发式场景下的性能评估</strong>：包括积木堆叠（测试迭代空间推理）和汉诺塔谜题（测试长时程、约束感知的动作排序）。</li>
<li><strong>实验III：基于场景图的高级推理</strong>：包括自主分类整理（将物品按类别放入不同盒子）、自主桌面整理（检测并纠正错放物品，并盖上盒盖）以及处理遮挡的整理任务（需要先打开工具箱并临时放置盒盖）。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.27558v1/x1.png" alt="实验I示意图"></p>
<blockquote>
<p><strong>图3</strong>：实验I-A和I-B示意图。展示了相对定位和语义聚类任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.27558v1/x2.png" alt="实验II示意图"></p>
<blockquote>
<p><strong>图4</strong>：实验II-A和II-B示意图。展示了积木堆叠和汉诺塔谜题任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.27558v1/x3.png" alt="VLM细粒度定位"></p>
<blockquote>
<p><strong>图5</strong>：VLM能够定位细粒度的可操作性部件（如盒盖旋钮）并指定临时放置点。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.27558v1/x4.png" alt="实验III示意图"></p>
<blockquote>
<p><strong>图6</strong>：实验III-A示意图。展示了自主分类整理任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.27558v1/x5.png" alt="实验III-B&amp;C示意图"></p>
<blockquote>
<p><strong>图7</strong>：实验III-B和III-C示意图。展示了自主桌面整理及处理遮挡的任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.27558v1/x6.png" alt="实验结果表"></p>
<blockquote>
<p><strong>表I</strong>：实验结果汇总。显示了各项实验的规划可行性（PF）、任务完成率（TCR）和场景图处理（SGH）百分比。</p>
</blockquote>
<p>关键实验结果总结如下：</p>
<ul>
<li><strong>规划与场景图</strong>：框架在所有实验中均表现出高规划可行性（PF ≥ 95%，除处理遮挡的III-C实验为80%）和完美的场景图处理（SGH = 100%）。</li>
<li><strong>任务完成率</strong>：基础任务（I-A, I-C）和结构化任务（II-A, II-B, III-A）达到了100%的TCR。当用户指令信息不足时（I-B1，TCR仅20%），提供更丰富的上下文（I-B2）可将TCR恢复至100%。在更复杂的场景中，TCR有所下降：菜谱选择（I-D）为80%，复杂整理（III-B）为75%，处理遮挡的整理（III-C）为60%。</li>
<li><strong>长时程任务</strong>：汉诺塔（II-B）的成功完成证明了框架处理需要多步逻辑规划的长时程任务的能力。</li>
<li><strong>消融分析</strong>：虽然未进行形式化的消融实验，但结果间接表明了各组件的贡献：1) <strong>场景图</strong>是实现高SGH和复杂推理（如III-B中的不一致性检测）的关键。2) <strong>LLM-VLM对话</strong>是实现灵活感知（如“之间”的关系定位）的基础，但其性能受指令明确性影响（I-B1 vs I-B2）。3) <strong>规划与执行解耦</strong>使得在保持高级推理能力的同时，能进行快速、可靠的控制执行。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了一个<strong>无需领域特定训练</strong>的分层机器人操作框架，通过集成现成的LLM、VLM和运动规划器，显著降低了构建智能机器人系统的工程负担。2) 引入了<strong>动态场景图</strong>作为核心世界表示，为不同基础模型提供了共享的、可推理的空间上下文，有效桥接了符号推理与几何感知。3) 通过系统实验证明了该框架在从简单重定位到复杂长时程谜题等一系列任务上的<strong>有效性和泛化能力</strong>，为直接基于基础模型构建机器人系统提供了可行路径。</p>
<p>论文自身提到的局限性包括：1) 对模糊或信息不足的用户指令（如Exp. I-B1）以及高度杂乱或存在遮挡的物理场景（如Exp. III-C）的处理能力仍有待提高，这些情况会导致任务完成率下降。2) 实验进行了一些简化假设，例如操作仅限于基于物体质心的抓取-放置，且运动规划未考虑与可操作物体的碰撞。</p>
<p>本文对后续研究的启示在于：它展示了将强大的、通用目的的基础模型与结构化的世界表示（如场景图）相结合，是实现<strong>可扩展、自适应且语义感知的机器人操作</strong>的一条有前景的路径。未来的工作可以探索如何使场景图的构建和维护更加自动化与鲁棒，如何集成更复杂的操作技能（如推、旋转），以及如何将系统扩展到非结构化或动态变化的环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出一种无需领域特定训练的机器人操作框架，旨在解决基于自然语言指令执行准确、长期序列任务的问题。其核心方法是分层整合多个预训练基础模型：LLM解析指令，VLM提供感知，推理模型生成任务序列，并引入动态维护的场景图以增强空间感知与推理。通过桌面操作实验验证，该框架展现了利用现成基础模型构建机器人系统的潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.27558" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>