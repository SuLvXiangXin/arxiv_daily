<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.00557" target="_blank" rel="noreferrer">2602.00557</a></span>
        <span>作者: Dai, Weisheng, Lan, Kai, Zhou, Jianyi, Zhao, Bo, Su, Xiu, Tong, Junwen, Guan, Weili, Yang, Shuo</span>
        <span>日期: 2026/01/31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过在大型机器人遥操作数据集上进行预训练，展现了初步的泛化能力。然而，获取能够全面覆盖多样化任务和环境的机器人数据成本极高，难以扩展。相比之下，人类演示视频提供了丰富且可扩展的多样化场景和操作行为数据源，但其缺乏明确的动作标注，阻碍了直接利用。先前的工作（如LAPA）利用基于VQ-VAE的框架以无监督方式从人类视频中学习潜在动作。但这类方法的训练目标主要侧重于重建视觉外观而非捕捉帧间动态，导致学习到的表示容易依赖虚假的视觉线索，产生短路学习（shortcut learning）和纠缠的潜在表示，从而损害了可迁移性。本文针对从人类视频中提取潜在动作时出现的“短路学习”问题，提出通过对比学习引入动作类别先验和时间先验来解耦视觉和动作表示。其核心思路是：利用对比解耦机制，引导模型从视频中分离出纯粹且语义一致的潜在动作表示，从而更有效地将人类视频中的运动先验迁移到机器人策略学习中。</p>
<h2 id="方法详解">方法详解</h2>
<p>ConLA框架包含三个关键阶段：1）对比潜在动作学习；2）潜在动作预训练；3）动作微调。</p>
<p>在第一阶段，模型的目标是从视频帧对 <code>[O_t, O_{t+k}]</code> 中学习离散化、语义一致的潜在动作表示。其核心创新在于<strong>对比解耦模块</strong>，该模块旨在解决VQ-VAE框架中的短路学习问题。</p>
<p><img src="https://arxiv.org/html/2602.00557v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ConLA方法总览。该方法利用对比学习从视频的视觉噪声中解耦出潜在动作，构建紧凑的潜在动作表示，从而能够从复杂的人类视频中学习运动先验，并提升下游机器人操作任务性能。</p>
</blockquote>
<p>整体流程如下：首先，使用一个时空Transformer编码器（逆动力学模型 <code>I</code>）处理当前帧 <code>O_t</code> 和未来帧 <code>O_{t+k}</code>，提取出原始的潜在动作嵌入 <code>Z ∈ R^d</code>。然后，<code>Z</code> 被送入对比解耦模块进行处理。该模块将 <code>Z</code> 均匀分割为两部分：<code>Z_a&#39;</code>（与动作相关）和 <code>Z_v&#39;</code>（与视觉相关）。之后，通过两个并行的对比学习目标进行优化：</p>
<ol>
<li><strong>动作中心对比学习</strong>：<code>Z_a&#39;</code> 通过一个动作头MLP投影为 <code>Z_a</code>，并利用视频自带的动作类别标签 <code>y</code> 作为弱监督信号，计算监督对比损失 <code>L_action</code>。该损失促使相同动作类别的潜在表示在嵌入空间中聚集，不同类别的表示分离，从而学习到紧凑且语义一致的潜在动作表示，缓解视觉干扰。</li>
<li><strong>视觉中心对比学习</strong>：为了进一步分离视觉内容与运动动态，模型对帧对进行逆序增强，得到 <code>[O_{t+k}, O_t]</code> 并编码得到 <code>Z^I</code>，同样分割为 <code>Z_a&#39;^I</code> 和 <code>Z_v&#39;^I</code>。<code>Z_v&#39;</code> 和 <code>Z_v&#39;^I</code> 通过一个视觉头MLP分别投影为 <code>Z_v</code> 和 <code>Z_v^I</code>。利用时间先验（逆序后运动信息变化大，而视觉内容相对稳定），将 <code>Z_v</code> 和 <code>Z_v^I</code> 视为正样本对，计算InfoNCE损失 <code>L_visual</code>。该目标鼓励模型提取对运动扰动鲁棒的、内容一致的特征，从而促进视觉与动作表示的分离。</li>
</ol>
<p>经过对比学习优化后的动作表示 <code>Z_a</code> 随后被量化（使用VQ-VAE目标）为离散的潜在动作 token <code>Z_aq</code>。最终，一个空间Transformer解码器 <code>F</code> 接收 <code>O_t</code> 和 <code>Z_aq</code>，以重建未来帧 <code>O_{t+k}</code>，重建损失为 <code>||O_{t+k} - O_{t+k}||^2</code>。第一阶段的总损失为重建损失、动作对比损失和视觉对比损失的加权和。</p>
<p><img src="https://arxiv.org/html/2602.00557v1/x2.png" alt="短路学习示例"></p>
<blockquote>
<p><strong>图2</strong>：短路学习示意图。使用从第一行帧对提取的潜在动作来重建第二行的 <code>O_{t+k}</code> 失败，因为重建目标驱动模型去捕捉外观而非运动。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.00557v1/x3.png" alt="对比解耦框架"></p>
<blockquote>
<p><strong>图3</strong>：对比潜在动作学习框架。利用带动作类别标签的样本及其逆序增强样本，通过动作头和视觉头分别进行动作中心与视觉中心的对比学习，以实现解耦表示。优化后的动作表示被量化，并与当前帧一起用于重建未来帧。</p>
</blockquote>
<p>在第二阶段（潜在动作预训练），利用第一阶段训练好的编码器为大量视频生成潜在动作伪标签，构建（观测-指令-伪动作）三元组数据集。然后，使用一个预训练的视觉-语言模型（如7B Large World Model），在其语言模型头后附加一个潜在动作头（单层MLP），根据任务指令和当前帧 <code>O_t</code> 来预测离散的潜在动作 token <code>Z_aq</code>。训练时冻结视觉编码器，优化语言模型。</p>
<p>在第三阶段（动作微调），使用少量包含真实机器人动作轨迹的数据对第二阶段预训练的策略进行微调。此时，丢弃原有的潜在动作头，替换为新的动作头来预测真实的、离散化的机器人动作（如末端执行器位姿）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：在仿真环境 SimplerEnv（“WidowX + Bridge” 设置下的4个任务）和真实世界（7-DoF Franka机械臂，3个桌面操作任务）进行评估。</li>
<li><strong>预训练数据集</strong>：<ul>
<li><strong>机器人视频</strong>：BridgeV2（大规模机器人操作数据集，60,096条轨迹）。</li>
<li><strong>人类视频</strong>：Something-SomethingV2（人类执行日常物体基本动作的视频集，220,847个带类别标签的片段）。</li>
</ul>
</li>
<li><strong>对比基线</strong>：SCRATCH（无预训练）、ACTIONVLA（用真实机器人动作预训练，作为上界）、UNIPI、VPT、LAPA。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在SimplerEnv仿真基准上的平均成功率如下表所示（对应论文表1）。</p>
<p><img src="https://arxiv.org/html/2602.00557v1/x4.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>图4</strong>：在SimplerEnv上的定量结果。ConLA在机器人视频和人类视频预训练下均优于基线。<strong>最关键的是，仅使用人类视频预训练的ConLA取得了64.6%的平均成功率，不仅大幅超越同数据源的LAPA（+12.5%），甚至超过了使用真实机器人轨迹预训练的ACTIONVLA模型（63.5%）1.1个百分点。</strong></p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.00557v1/x5.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界桌面操作任务的成功率。ConLA（人类视频预训练）相比LAPA在三个任务上平均提升了15.9%，验证了其解耦出的潜在动作在真实机器人任务上的有效迁移性。</p>
</blockquote>
<p><strong>消融实验与分析</strong>：<br><img src="https://arxiv.org/html/2602.00557v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。移除动作对比损失（<code>w/o L_action</code>）或视觉对比损失（<code>w/o L_visual</code>）均会导致性能显著下降，证明了两个对比学习组件对于解耦和提升性能都是必要的。两者结合（ConLA）效果最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.00557v1/x7.png" alt="潜在空间可视化"></p>
<blockquote>
<p><strong>图7</strong>：潜在动作空间t-SNE可视化。(a) LAPA的方法导致潜在动作被背景等视觉特征主导而纠缠在一起。(b) ConLA的方法学到的潜在动作依据动作语义（如“推”、“拉”）形成了清晰的聚类，证明了其解耦的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.00557v1/x8.png" alt="定性结果"></p>
<blockquote>
<p><strong>图8</strong>：真实机器人任务的定性结果。ConLA成功完成了“推倒”、“覆盖”和“放入盒子”等任务。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>明确指出并解决了基于VQ-VAE的潜在动作学习方法中存在的“短路学习”问题，即模型过度依赖视觉外观线索而非真实运动动态。</li>
<li>提出了一个新颖的对比解耦框架（ConLA），通过引入动作类别先验（弱监督）和时间顺序先验，利用对比学习有效地从视频中分离出纯粹、语义一致的潜在动作表示。</li>
<li>首次实现了仅使用人类视频预训练的VLA模型性能超越使用真实机器人轨迹预训练的模型，证明了大规模人类视频用于可扩展机器人学习的巨大潜力和可行性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，方法依赖于视频数据中可用的动作类别标签作为弱监督信号。对于完全无标签的互联网视频，其适用性可能需要进一步探索。</p>
<p><strong>启示</strong>：</p>
<ul>
<li>为利用海量、易得的人类视频数据训练通用机器人策略开辟了新途径，降低了对昂贵机器人数据的依赖。</li>
<li>展示了结合语义先验（如动作类别）和时空先验进行表示学习，是提升无监督/弱监督学习效果的有效策略。</li>
<li>解耦出语义清晰的潜在动作空间，可能有利于策略的可解释性以及与其他模块（如大语言模型）的集成。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ConLA框架，解决从无动作标注的人类视频中学习机器人操作策略的难题。现有VQ-VAE方法因侧重外观重建而陷入捷径学习，导致潜在动作表示纠缠、可迁移性差。ConLA引入对比解缠机制，利用动作类别先验与时间线索分离运动动态与视觉内容，从而抑制虚假关联。实验表明，仅用人类视频预训练，ConLA首次超越基于真实机器人轨迹的预训练性能，证明其能提取纯净、语义一致的潜在动作表示，为机器人学习提供可扩展方案。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.00557" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>