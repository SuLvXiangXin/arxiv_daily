<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.15212" target="_blank" rel="noreferrer">2509.15212</a></span>
        <span>作者: Xin Li Team</span>
        <span>日期: 2025-09-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型的发展受限于大规模机器人操作数据的稀缺性，因为通过遥操作物理机器人收集此类数据成本高昂。现有方法主要沿两个方向努力：一是构建机器人操作数据集，但其规模远小于大语言模型或视觉语言模型所用的数据；二是利用预训练的生成模型或视觉语言模型的先验知识来缓解数据不足。然而，这些方法在将高层视觉理解与底层机器人动作空间有效衔接方面仍存在局限。</p>
<p>本文针对VLA模型数据稀缺以及视觉预测与动作生成之间存在鸿沟的痛点，提出了一种新视角：利用大规模人类演示视频进行生成式预训练，并将人类轨迹预测作为中间桥梁。其核心思路是设计一个两阶段预训练课程：首先在自我中心的人类操作视频上进行未来帧预测，学习通用操作动态；然后引入人类关键点轨迹预测，将视觉动态与类似动作的轨迹模式关联；最后将习得的表征迁移到机器人数据上，以提供更有效的VLA模型初始化。</p>
<h2 id="方法详解">方法详解</h2>
<p>RynnVLA-001的训练流程是一个渐进式的三阶段课程，整体框架如图2所示。</p>
<p><img src="https://arxiv.org/html/2509.15212v1/x2.png" alt="方法架构与训练阶段"></p>
<blockquote>
<p><strong>图2</strong>：RynnVLA-001的模型架构与训练阶段。训练包含三个阶段：(1) 自我中心视频生成预训练，训练一个基于Transformer的图像到视频模型进行未来帧预测。(2) 以人为中心的轨迹感知视频建模，扩展I2V模型，加入动作（轨迹）预测头，融合视觉和状态嵌入（蓝色块）。(3) 以机器人为中心的视觉-语言-动作建模，将预训练权重迁移到机器人数据，模型生成的动作嵌入由ActionVAE解码为可执行动作。</p>
</blockquote>
<p><strong>第一阶段：自我中心视频生成预训练</strong>。此阶段旨在从海量人类演示中学习操作动态。模型架构基于自回归Transformer，扩展了图像生成模型Chameleon以执行图像到视频任务。输入为一个初始帧和对应的语言指令，模型被训练来预测后续视频帧。为了与VLA推理过程对齐，语言令牌与视觉令牌在输入序列中交错排列：<code>[语言令牌, 视觉令牌_t, 语言令牌, 视觉令牌_t+1, ...]</code>。训练使用大规模筛选的1200万自我中心人类操作视频以及24.4万机器人操作视频，损失函数为对离散视觉令牌和语言令牌的交叉熵损失。</p>
<p><strong>第二阶段：以人为中心的轨迹感知视频建模</strong>。此阶段是衔接视觉与动作的关键创新。它在第一阶段模型的基础上进行微调，引入多任务目标：同时预测未来视觉帧和对应的人类关键点轨迹。具体使用EgoDex数据集，并仅采用手腕关键点来近似末端执行器位置。更重要的是，模型并非预测原始坐标，而是预测由<strong>ActionVAE</strong>编码的轨迹块的紧凑连续嵌入。为了提供本体感知信息，模型输入中加入了代表当前手腕位置的状态嵌入（图2中蓝色块）。输入序列变为：<code>[语言, 视觉令牌_t, 状态嵌入_t, &lt;动作占位符&gt;, ...]</code>。模型架构扩展了一个轻量级的动作头（单线性层），用于将<code>&lt;动作占位符&gt;</code>对应位置的最后一个隐藏状态映射到动作嵌入的连续潜在空间。动作头的训练由L1损失监督，视觉令牌预测则与第一阶段相同。</p>
<p><strong>第三阶段：以机器人为中心的视觉-语言-动作建模</strong>。此阶段将第二阶段获得的轨迹感知模型适配为机器人控制的VLA模型。架构继承第二阶段，但针对机器人领域进行修改：丢弃为人类轨迹预训练的动作头，初始化一个新的轻量级动作头来预测机器人动作嵌入；视觉输入改为机器人前视和腕部双视角图像；状态嵌入变为机器人状态。模型输入序列结构与第二阶段类似。训练时，模型同时优化两个目标：1) <strong>机器人动作预测</strong>：通过新动作头回归<code>&lt;动作占位符&gt;</code>对应的隐藏状态到机器人动作嵌入，使用L1损失；2) <strong>未来视觉预测</strong>：继续自回归预测下一帧的视觉令牌，作为辅助任务进行正则化。</p>
<p><strong>核心模块：ActionVAE</strong>。这是一个变分自编码器，用于将动作块（短动作序列）压缩为紧凑的潜在嵌入，并在解码时重建为平滑连贯的原始动作序列。本文分别为人类轨迹（第二阶段）和机器人动作（第三阶段）训练了领域特定的ActionVAE。其关键优势在于，一旦在特定具身上训练完成，便可直接用于从新数据中提取动作嵌入，而无需重新训练，从而实现了动作表示的泛化。</p>
<p><strong>推理过程</strong>。在推理时，为追求效率，模型仅预测动作嵌入，而丢弃未来视觉令牌的生成。预测出的动作嵌入由对应的ActionVAE解码器重建为可执行的低层机器人动作序列（块），机器人执行该动作块后，新的观测与指令再次输入模型，形成闭环控制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用自收集的真实世界操作数据集进行训练和评估，该数据通过LeRobot SO100机械臂的专家遥操作收集，包含三个代表性任务（见图3）：拾放绿色积木、拾放草莓、抓笔放入笔座。每个任务在包含仅目标物体、多目标物体、以及包含干扰物体的场景下收集了数百条示教。评估在三种场景下进行：单目标操作、多目标操作、以及带有干扰物的指令跟随。基线方法为当前先进的开放源码VLA模型GR00T N1.5和Pi0，使用相同的SO100数据对它们进行微调以进行公平比较。</p>
<p><img src="https://arxiv.org/html/2509.15212v1/x3.png" alt="评估任务示意图"></p>
<blockquote>
<p><strong>图3</strong>：评估任务示意图。在三个任务上评估VLA模型性能：(1) 拾放绿色积木，(2) 拾放草莓，(3) 抓笔放入笔座。每个任务在三种设置下评估：单目标操作、多目标操作（前三张图）和带有干扰物的指令跟随（最右图）。</p>
</blockquote>
<p><strong>主要结果</strong>：如表1所示，RynnVLA-001在三个任务上的平均成功率高达90.6%，显著优于GR00T N1.5 (55.6%) 和 Pi0 (70.4%)。在单次尝试成功率上，RynnVLA-001 (56.7%) 与 Pi0 (56.3%) 相当。</p>
<p><img src="https://arxiv.org/html/2509.15212v1/x5.png" alt="不同评估设置下的性能对比"></p>
<blockquote>
<p><strong>图4</strong>：不同评估设置下的性能对比表。该表显示，随着任务复杂度增加（从单目标到多目标再到有干扰物），GR00T N1.5和Pi0的性能显著下降，而RynnVLA-001的性能保持稳定且领先，尤其在“带有干扰物的指令跟随”场景下优势明显（91.7% vs. 56.7%和60.0%），证明了其优异的泛化与指令遵循能力。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过消融研究验证了预训练权重和两阶段课程设计的有效性。如表3（对应论文中未提供的表，但根据上下文，此处应指消融实验结果）所示，直接使用第一阶段预训练权重进行VLA微调，相比随机初始化有显著提升（平均SR从60.0%升至78.9%），证明了视频生成预训练的价值。在此基础上，进一步加入第二阶段（轨迹感知建模）的预训练权重，性能得到进一步提升至90.6%，这证实了引入人类轨迹作为中间桥梁对于弥合视觉与动作鸿沟的关键作用。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的两阶段预训练课程（视频生成 → 轨迹感知建模），利用大规模人类演示视频有效地将操作动态先验迁移到机器人VLA模型中，提供了更优的模型初始化。</li>
<li>设计了ActionVAE，一种用于压缩和表示动作块的变分自编码器，它能够生成平滑连贯的动作序列，并支持跨任务的泛化。</li>
</ol>
<p><strong>局限性</strong>：论文提到，其方法依赖于从网络来源筛选高质量的人类自我中心操作视频，视频数据的质量和相关性直接影响预训练效果。此外，ActionVAE是针对特定具身（人类手或特定机器人）训练的，其表征的通用性可能受到限制。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>利用丰富的人类视频数据</strong>：证明了非机器人形态的人类演示数据可以作为学习机器人操作技能的宝贵资源，为缓解机器人数据稀缺问题提供了新路径。</li>
<li><strong>动作表示的创新</strong>：ActionVAE展示了一种有效的动作序列表示与生成方式，未来可探索更通用、更具适应性的动作表示方法。</li>
<li><strong>渐进式课程学习</strong>：从视觉预测到轨迹预测再到动作生成的渐进式训练范式，为构建更复杂、更通用的具身智能模型提供了可借鉴的框架。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RynnVLA-001视觉-语言-动作模型，旨在解决机器人操作任务中大规模数据稀缺的核心问题。其关键技术是两阶段预训练：第一阶段“自我中心视频生成预训练”基于1200万人类示范视频，训练以初始图像和语言指令为条件的图像到视频模型；第二阶段“人体中心轨迹感知建模”联合预测未来关键点轨迹，以桥接视觉与动作预测。此外，提出ActionVAE压缩动作序列为紧凑潜表示。实验表明，经相同下游数据集微调后，该模型性能超越现有先进基线，验证了所提预训练策略能为VLA模型提供更有效的初始化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.15212" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>