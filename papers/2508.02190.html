<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.02190" target="_blank" rel="noreferrer">2508.02190</a></span>
        <span>作者: Miao, Cui, Chang, Tao, Wu, Meihan, Xu, Hongbin, Li, Chun, Li, Ming, Wang, Xiaodong</span>
        <span>日期: 2025/08/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过整合视觉感知、语言理解和机器人控制，极大地推动了机器人操作的发展，使机器人能够根据自然语言指令执行复杂任务。然而，训练此类模型通常依赖于大规模、包含用户特定场景的数据集，这引发了严重的隐私和安全担忧，从而限制了其广泛应用。主流的集中式训练需要将所有用户数据上传至云端服务器，存在隐私泄露风险。</p>
<p>联邦学习作为一种隐私保护方案，允许在分布式客户端上训练模型而无需共享原始数据。然而，现有的联邦学习方法（如FedAvg）主要针对单模态任务设计，无法有效应对机器人操作中多模态（视觉、语言、动作）输入带来的复杂性，并且忽略了不同客户端之间任务存在的显著异构性。此外，混合专家模型虽在多任务学习中展现出潜力，但其通常固定激活专家数量的机制缺乏对任务复杂度的适应性，在资源受限的客户端上可能导致效率低下。</p>
<p>本文针对上述痛点，首次提出了一个专为多模态机器人学习设计的、任务自适应的联邦学习框架。具体而言，本文旨在解决在保护用户数据隐私的前提下，如何有效训练VLA模型并使其能够泛化到不同客户端异构任务的问题。核心思路是：通过指令导向的场景解析增强任务感知的特征提取；设计一种支持双向选择的自适应双门控混合专家机制以提升计算效率；在服务器端采用基于专家相似性的驱动式聚合策略，以实现跨客户端的有效知识融合。</p>
<h2 id="方法详解">方法详解</h2>
<p>FedVLA框架的整体流程如图2(a)所示，包含多个客户端和一个中央服务器。每个客户端本地存储其私有数据，并维护一个完整的VLA模型，该模型由三部分组成：Stem（负责初始特征提取）、Trunk（包含多个DGMoE层的主体网络）和Head（输出动作预测）。在每一轮联邦训练中，客户端利用本地数据更新其模型参数，但仅将Trunk部分的参数更新以及各层专家的激活统计信息上传至服务器。服务器执行专家驱动的聚合，生成一个全局的Trunk模型，然后将其下发至各客户端，替换客户端的本地Trunk，从而开始下一轮训练。</p>
<p><img src="https://i.imgur.com/3Qm1t2c.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：(a) FedVLA框架概览。客户端处理视觉-语言-动作输入，通过Stem、Trunk和Head模块进行本地训练。客户端将更新后的Trunk参数和专家选择统计信息发送至服务器，服务器执行专家驱动的聚合，选择性整合专家特定的更新。聚合后的全局Trunk模块被分发回客户端进行下一轮训练。(b) 双门控混合专家模块的详细结构。输入令牌首先经过令牌侧门控$G_t$，计算每个专家的门控分数（包含来自前一层MoE的残差分数）。只有当计算出的分数超过专家侧门控$G_e$的阈值时，才会激活对应专家来处理该令牌。最后，专家输出被组合以生成最终的输出令牌。</p>
</blockquote>
<p>框架包含三个核心创新模块：</p>
<ol>
<li><p><strong>指令导向场景解析</strong>：如图3所示，该模块旨在根据任务指令分解和增强场景中的对象级特征。给定图像和指令，首先使用命名实体识别从指令中提取目标对象，同时使用YOLOv8检测图像中的前景和背景对象。接着，利用预训练的CLIP模型计算指令文本与检测到的对象名称之间的余弦相似度，将对象分为三类：目标对象（指令中明确提及）、周围对象（提供空间上下文）和背景对象（静态环境）。然后，使用CLIP编码器分别获取各组对象的文本嵌入和整个图像的视觉嵌入。通过计算图像令牌与各组文本嵌入的相似度，为每组分配最相关的图像令牌（每组取top-8）。每组令牌通过一个MoE模块进行特征增强，最后与剩余令牌拼接，作为后续网络的输入。这使得模型能聚焦于任务关键对象。</p>
</li>
<li><p><strong>双门控混合专家</strong>：如图2(b)所示，DGMoE机制引入了双向自适应专家选择。每个DGMoE层包含K个自感知专家。其创新在于两个门控机制：</p>
<ul>
<li>**令牌侧门控$G_t$**：为一个软路由模块，结合了门控残差。对于输入令牌$x$，在第$j$层的输出$G_t^j(x)$不仅取决于当前层的权重$W_t^j$，还通过一个可学习的变换矩阵$W_t^g$聚合了前一层的门控分数$G_t^{(j-1)}(x)$，使得令牌能够继承之前层对最合适专家的选择偏好。归一化后得到令牌对专家的选择分数$s_t(x)$。</li>
<li>**专家侧门控$G_e$**：每个专家拥有一个可学习的自感知参数$W_e$，代表其接受阈值。专家侧门控根据公式$s_e(x) = \text{sign}(s_t(x) - \lambda W_e)$决定是否接受令牌。其中$\lambda$是缩放因子（设为0.5）。只有当$s_t(x)$超过对应专家的阈值时，该专家才会被激活。<br>最终的专家选择函数$g(x)$结合了双向选择：若$s_e(x)&gt;0$则保留$s_t(x)$作为权重，否则权重为0。该层的输出是所选专家输出的加权和。这种方式允许模型根据令牌的复杂度和相关性动态激活不同数量的专家，提升了计算效率。</li>
</ul>
</li>
<li><p><strong>专家驱动聚合</strong>：为缓解任务异构性导致的性能下降，服务器端的聚合策略基于客户端间专家选择的相似性。每个客户端$C_i$在每轮训练中记录一个$L \times K$的专家选择矩阵$V_i$，元素$V_i^{(l,k)}$表示第$l$层专家$k$被激活的次数。对于第$l$层，提取选择向量$v_i^{(l)}$。客户端$C_i$和$C_j$在第$l$层的相似度$s_{i,j}^{(l)}$通过其选择向量的余弦相似度计算。然后，客户端$C_i$在第$l$层的聚合权重$w_{l,i}$根据其与所有其他客户端的相似度之和所占的比例来确定。这种策略确保了专家激活模式相似的客户端在模型聚合中贡献更大，促进了任务对齐的知识融合。</p>
</li>
</ol>
<p>与现有方法相比，FedVLA的创新点具体体现在：1) 首次将联邦学习框架系统性地应用于多模态VLA模型训练；2) 提出了支持专家拒绝机制的动态MoE结构，兼顾效率与性能；3) 设计了基于专家激活模式的语义驱动聚合方法，而非简单的参数平均，以更好地处理联邦环境下的任务异构性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在模拟和真实世界两种场景下验证FedVLA。使用预训练的HPT模型作为VLA主干网络。联邦训练进行1000轮通信，每轮客户端本地训练5个epoch。模拟实验学习率为$5\times10^{-6}$，真实世界为$2\times10^{-5}$，批量大小为256。</p>
<p><strong>基准与对比方法</strong>：由于是首个联邦VLA学习工作，主要与两个基线对比：1) <strong>Centralized</strong>：将所有数据集中到单一服务器上进行联合训练，作为性能上界。2) <strong>FedAvg</strong>：经典的联邦平均算法，在服务器端简单平均客户端模型参数。</p>
<p><strong>数据集与任务</strong>：</p>
<ul>
<li><strong>模拟环境</strong>：基于MuJoCo和Meta-World基准，评估四个家庭任务：Door Lock, Close Drawer, Sweep Into, Open Window。每个任务收集30-80条轨迹。</li>
<li><strong>真实世界</strong>：使用UR3机械臂和RealSense相机，执行四个任务：Clean Up, Trash Collection, Open Drawer, Sorting Pills（如图4所示）。每个任务收集约50条演示轨迹。</li>
</ul>
<p><img src="https://i.imgur.com/7m1t2c.png" alt="真实世界任务示例"></p>
<blockquote>
<p><strong>图4</strong>：真实世界任务执行关键帧示例。从左至右任务分别为：Clean Up（用布擦拭盘子上的垃圾）、Trash Collection（将纸团扔进垃圾桶）、Open Drawer（拉开抽屉）、Sorting Pills（将药瓶放入指定颜色的托盘中）。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>模拟环境</strong>：结果如表1所示。FedVLA取得了63.3%的平均成功率，非常接近集中式训练的65.0%。在Close Drawer任务上甚至超过了集中式训练（80.0% vs. 73.3%）。FedAvg的平均成功率仅为51.7%，显著低于FedVLA，这突显了处理任务异构性的重要性。</li>
<li><strong>真实世界</strong>：结果如表2所示。FedVLA取得了63.3%的平均成功率，与集中式训练的63.4%几乎持平。在Trash Collection和Sorting Pills任务上与集中式模型表现相同。而FedAvg的平均成功率仅为53.3%，比FedVLA低10个百分点，证明了FedVLA在多模态真实场景下的有效性。</li>
</ul>
<p><strong>消融实验</strong>：研究通过依次移除IOSP、DGMoE和EDA模块进行消融分析，结果如表3所示。</p>
<ul>
<li>移除IOSP导致性能显著下降，尤其在物体数量较多的Trash Collection任务上，成功率从46.7%暴跌至13.3%。</li>
<li>移除DGMoE（替换为标准FFN）影响最大的是需要多步交互的Clean Up任务，成功率从53.3%降至20.0%。</li>
<li>移除EDA（使用FedAvg聚合）造成的性能下降最为严重，平均成功率从63.3%骤降至26.7%，这表明简单的参数平均会抵消DGMoE中学到的专家知识。<br><img src="https://i.imgur.com/8m1t2c.png" alt="消融实验验证损失曲线"><blockquote>
<p><strong>图5</strong>：前500个训练周期内，FedVLA完整模型与各消融变体的验证损失曲线。完整模型（FedVLA (Ours)）的损失收敛至最低且最稳定。移除EDA（w/o EDA）的损失最高且波动大，移除DGMoE（w/o DGMoE）次之，移除IOSP（w/o IOSP）也有明显上升，这直观展示了每个模块对模型稳定学习和性能提升的重要性。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个用于机器人操作的隐私保护联邦视觉-语言-动作学习框架FedVLA，在模拟和真实实验中实现了与集中式训练相当的性能，同时确保了用户数据不离开本地设备。</li>
<li>设计了双门控混合专家机制，通过引入自感知专家和双向选择（令牌选专家、专家拒令牌），实现了对任务复杂度的自适应，在保持性能的同时优化了计算效率。</li>
<li>提出了专家驱动聚合策略，利用客户端间专家激活模式的相似性动态调整聚合权重，有效促进了跨异构任务的语义知识融合，解决了传统联邦平均在VLA场景下的局限性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，DGMoE中引入的自感知门控机制可能增加少量计算开销。此外，框架依赖于预训练的视觉语言模型（如CLIP）进行场景解析，其性能可能受限于这些基础模型的能力。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>联邦学习与具身AI</strong>：本研究开辟了联邦学习在具身智能和机器人学习领域的新方向，后续工作可探索更复杂的多智能体协作、连续学习等场景下的隐私保护训练。</li>
<li><strong>动态与高效MoE</strong>：DGMoE中专家侧的门控机制为设计更灵活、高效的稀疏化大模型提供了新思路，可进一步研究如何自动化地优化专家数量和阈值。</li>
<li><strong>个性化联邦聚合</strong>：专家驱动聚合策略本质是一种基于语义相似性的个性化聚合方法，未来可探索将此类方法应用于更广泛的跨模态联邦学习任务中，以更好地处理数据非独立同分布问题。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中视觉-语言-动作模型训练依赖大规模用户数据、引发隐私安全顾虑的核心问题，提出了首个联邦VLA学习框架FedVLA。其关键技术包括：指令导向的场景解析以增强任务理解；双门控专家混合机制，使输入和专家均能自适应激活；以及服务器端的专家驱动聚合策略以实现高效知识迁移。实验表明，所提方法在显著提升计算效率的同时，实现了与集中式训练相当的任务成功率，有效保护了数据隐私。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.02190" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>