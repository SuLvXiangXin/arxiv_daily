<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.22441" target="_blank" rel="noreferrer">2509.22441</a></span>
        <span>作者: Wang, Zhangyuan, Zhu, Yunpeng, Yan, Yuqi, Tian, Xiaoyuan, Shao, Xinhao, Li, Meixuan, Li, Weikun, Su, Guangsheng, Cui, Weicheng, Fan, Dixia</span>
        <span>日期: 2025/09/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>自主水下航行器（AUV）在海洋科学与工业中不可或缺，但水下环境的强非线性流体动力学、极端物理条件、严重感知困难和通信限制，使得AUV的自主性面临巨大挑战。传统方法如PID控制、自适应模型控制和深度强化学习，在非结构化海洋环境中普遍缺乏泛化性和鲁棒性。近年来兴起的视觉-语言-动作（VLA）范式，通过统一感知、自然语言理解和动作生成，在多模态推理和任务泛化方面展现出强大潜力，并已在工业操作、自动驾驶等领域取得成功。然而，VLA在水下机器人领域仍未得到探索，主要存在三个障碍：1）缺乏适合水下环境的控制层次结构，端到端VLA模型无法在严苛的通信与计算约束下，有效解耦高层任务规划与低层实时控制；2）数据饥渴的策略，需要大量昂贵的水下演示数据进行训练，限制了可扩展性；3）缺乏实时流体动力学补偿，非线性流体效应会严重干扰AUV运动，而现有学习型控制器缺乏物理先验，无法在不进行额外任务特定训练的情况下进行实时动态补偿。</p>
<p>本文针对上述痛点，提出了首个专为AUV设计的VLA框架UnderwaterVLA。其核心思路是：通过一个受生物启发的双脑架构，将高层任务推理与低层反应控制解耦；采用零数据训练策略，利用预训练多模态基础模型，最小化对水下演示数据的依赖；并设计一个融合流体动力学知识的模型预测控制（MPC）方案，实时补偿流体效应，从而在通信受限、感知退化的水下环境中实现鲁棒、可解释的自主导航。</p>
<h2 id="方法详解">方法详解</h2>
<p>UnderwaterVLA的整体框架是一个双脑协作系统，旨在应对水下通信带宽受限的挑战。该系统将高层、长期的战略规划（云脑）与低层、实时的闭环控制（小脑）进行解耦。</p>
<p><img src="https://arxiv.org/html/2509.22441v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：双脑协作框架示意图。云脑负责长时程规划，将高级任务分解为一系列顺序执行的子目标；小脑（本地脑）则通过在线/离线视觉语言模型（VLMs）执行闭环、逐步的控制。</p>
</blockquote>
<p><strong>核心模块1：双脑架构</strong></p>
<ul>
<li><strong>云脑（高层规划）</strong>：部署在可间歇性通信的云端（如AUV上浮时）。它接收高级自然语言指令，通过QVQ-MAX模型进行任务分解，生成一系列高层语义子目标序列 {S1, S2, …, SN}。例如，将“导航至珊瑚礁并避开沉船”分解为“通过声纳地标定位”、“规划无碰撞粗略路径”、“传输航点A并以中速前进”等子任务。</li>
<li><strong>小脑（低层执行）</strong>：部署在AUV本地的计算单元上。它接收云脑下发的当前子目标，并利用Qwen-VL模型，根据实时视觉感知和环境上下文，生成JSON格式的即时控制决策 Jt。小脑以闭环方式执行“感知-动作”循环，直至当前子任务完成，再接收下一个子目标。</li>
</ul>
<p><strong>核心模块2：提示工程与思维链</strong><br>为了增强决策的可解释性和故障诊断能力，系统强制云脑和小脑在输出控制指令时，附带结构化的思维链推理过程。云脑通过CoT提示将指令分解为带有时序的子任务列表。小脑在每个控制周期输出标准化的JSON控制元组，包含“reasoning”（推理）、“decision”（动作决策）、“velocity”（速度等级）、“sub_task_done”（子任务完成状态）和“mission_done”（任务完成状态）等字段，确保了决策过程的透明度和可追溯性。</p>
<p><strong>核心模块3：零数据模型预测控制</strong><br>这是将离散VLA命令（方向、速度等级）转化为连续、平滑、且能补偿流体效应的底层控制的关键模块。其核心设计包括：</p>
<ol>
<li><strong>时间约束的运动剖面</strong>：无论是平移（前进/后退）还是旋转（左转/右转）命令，其执行周期都严格限制为1秒，并分为加速（0-0.2秒）、匀速（0.2-0.5秒）和减速（0.5-1.0秒）三个阶段。每个阶段的速度/角速度曲线由公式(2)和(3)精确定义，速度等级（低、中、高）对应不同的目标值。</li>
<li><strong>流体动力学感知的MPC优化</strong>：控制器通过求解优化问题（公式(4)）来计算最优推力指令。代价函数包含三项：跟踪误差项（使实际速度/角度跟随参考剖面）、控制努力惩罚项（节能）、以及流体阻力补偿项。阻力采用二次模型（公式(5)(6)），并在不同运动阶段显式考虑附加质量效应、定常阻力等流体-结构相互作用。</li>
<li><strong>实时流体适应</strong>：系统能够在执行过程中在线估计平移和旋转阻力系数（公式(7)），利用来自IMU的加速度测量值和已知的推力输出，实现动态的流体效应补偿，而无需预先进行大量数据训练。<br>该MPC方案的优势在于其“零数据”特性：它不依赖特定任务的水下数据进行训练，而是基于物理模型和在线估计实现鲁棒控制，确保了确定性的执行周期、精确的停止以及跨不同流体条件的适应性。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在受控的实验室水箱中进行，评估了UnderwaterVLA在视觉引导导航、任务性能对比及鲁棒性方面的表现。</p>
<p><strong>实验平台与基线</strong>：实验使用真实的AUV平台。由于传统的RL和经典控制方法无法理解高级语言指令，因此选择在模拟环境中评估的QUAR-VLA作为主要性能对比基线。此外，还通过消融实验比较了本文的双脑模型与单脑端到端模型在感知退化条件下的表现。</p>
<p><strong>关键定量结果</strong>：如表I所示，在更具挑战性的真实世界条件下，UnderwaterVLA在所有任务类别上均优于QUAR-VLA基线：感知任务（区分字母）成功率提升19.0%，基本导航任务（前往物体）提升20.0%，在隧道穿越等高级场景中提升高达27.0%，在避障任务中仍保持19.0%的优势。特别值得注意的是，本文方法所需的训练数据量为0，而基线需要262K条数据。</p>
<p><img src="https://arxiv.org/html/2509.22441v1/x2.png" alt="实验演示"></p>
<blockquote>
<p><strong>图2</strong>：在杂乱水下测试床中顺序导航与避障的实验演示。AUV自主识别最远目标，执行无碰撞机动，并保持鲁棒的轨迹跟踪。</p>
</blockquote>
<p><strong>鲁棒性消融实验</strong>：为了模拟真实水下环境（光照衰减、水体浑浊），实验通过降低光照和注入硅藻土提高浊度（最高至18 NTU）来制造视觉退化条件。</p>
<p><img src="https://arxiv.org/html/2509.22441v1/x3.png" alt="鲁棒性对比"></p>
<blockquote>
<p><strong>图3</strong>：在模拟海洋退化条件下的鲁棒性对比：双脑模型 vs. 单脑模型。(a)(b)分别展示了两种模型在退化条件下的轨迹路径，(A)-(D)展示了双脑模型在关键决策点的第一人称视觉视角。</p>
</blockquote>
<ul>
<li><strong>双脑模型的适应性</strong>：如图3(a)和(A)-(D)所示，当视觉严重退化导致目标丢失时，云脑生成的包含四个子任务的计划（如“向右对齐”、“中速前进”等）并未被机械执行。小脑在执行了前两个子任务后，根据实时视觉反馈自主评估上下文，判定“保持安全距离”的目标已在姿态调整后达成，因此<strong>主动终止</strong>了后续运动，实现了高效且安全的任务闭合。这体现了低层执行器具备基于感知的动态决策能力。</li>
<li><strong>单脑模型的失效</strong>：如图3(b)所示，单脑端到端模型在目标丢失后，僵化地继续执行最后一条有效指令（“前进”），缺乏重新规划或距离调节能力，最终越过目标区域并违反安全边界，导致任务失败。</li>
</ul>
<p>消融实验表明，双脑架构通过分离战略规划和反应执行，在光学挑战性环境中提供了关键的韧性。其上下文感知和任务提前终止的能力，是优于刚性单脑模型的核心。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了首个水下VLA框架</strong>：系统性地解决了通信受限、感知条件恶劣和数据收集成本高等水下特异性挑战，将VLA范式成功引入水下机器人领域。</li>
<li><strong>创新了零数据训练方法与双脑架构</strong>：通过结合预训练基础模型的提示工程和基于物理模型的MPC，最小化了对水下演示数据的依赖；双脑设计有效解耦了高层推理与低层控制，增强了系统在约束条件下的鲁棒性和可解释性。</li>
<li><strong>进行了全面的实验验证</strong>：在真实水下环境中证明了该框架在导航精度、任务成功率（提升19%-27%）以及感知退化条件下的优雅降级与恢复能力方面，均优于传统基线。</li>
</ol>
<p>论文自身提到的局限性或未来方向包括：计划将该框架扩展到多智能体协同、长时程任务和极深水操作；未来工作将集成实时多模态融合，扩展针对领域特定科学指令的语言 grounding（例如“在2500米处勘测热液喷口”），并在全尺寸开放海域部署中进行性能验证。</p>
<p>本文的启示在于，为新一代海洋机器人提供了一条可扩展且经济高效的路径：通过融合语言理解、物理感知和数据高效的学习，推动具身智能从仿真或实验室环境，走向动态、不确定且通信匮乏的真实深海。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出UnderwaterVLA框架，旨在解决水下自主导航因流体扰动、通信受限及视觉退化导致的控制与感知难题。核心技术包括：1）双脑架构，解耦高层任务规划与底层反应控制；2）首次在水下机器人中引入视觉-语言-动作模型，结合思维链推理实现可解释决策；3）流体动力学模型预测控制，实时补偿流体效应。现场实验表明，该方法在视觉退化条件下降低了导航误差，任务完成率较基线提升19%至27%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.22441" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>