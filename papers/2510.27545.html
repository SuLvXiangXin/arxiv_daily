<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.27545" target="_blank" rel="noreferrer">2510.27545</a></span>
        <span>作者: Davies, Travis, Huang, Yiqi, Gladstone, Alexi, Liu, Yunxin, Chen, Xiang, Ji, Heng, Liu, Huxian, Hu, Luhui</span>
        <span>日期: 2025/10/31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，由生成模型参数化的隐式策略，如扩散策略，已成为机器人策略学习和视觉-语言-动作模型的标准方法。然而，这些方法常面临计算成本高、暴露偏差以及推理动态不稳定等问题，导致在分布偏移下策略容易发散。基于能量的模型通过端到端学习能量景观并建模平衡动态，为解决这些问题提供了潜力，能提供更好的鲁棒性并减少暴露偏差。但历史上，由EBM参数化的策略难以有效扩展。近期关于能量变换器的工作证明了EBM在高维空间的可扩展性，但其在解决具身物理模型核心挑战方面的潜力尚未被充分探索。本文针对扩散策略等方法的局限性，提出了EBT-Policy，一种新的基于能量的架构，旨在解决机器人和真实世界场景中的核心问题。其核心思路是：通过能量变换器直接学习一个显式的、时不变的能量景观，并通过能量最小化过程来生成动作轨迹，从而实现更鲁棒、高效且具备涌现物理推理能力的策略学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>EBT-Policy将视觉运动控制策略表述为能量变换器。其目标是在给定多模态感官观察和可选的自然语言指令条件下，生成机器人的动作序列。策略被建模为一个能量最小化过程，而非学习一个显式策略：π: argmin_{a∈A^n} E_θ(ℓ, o_t, a)，其中能量函数被训练为对符合观察和指令的动作轨迹分配低能量。</p>
<p><img src="https://arxiv.org/html/2510.27545v1/imgs/contour_surface.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：EBT-Policy示意图。EBT-Policy通过在笛卡尔空间或关节空间中搜索低能量动作轨迹（z）来实现功能，其核心是能量最小化过程。</p>
</blockquote>
<p>整体框架上，模型接收输入（语言指令ℓ、历史RGB图像X、本体感知状态z），通过一个上下文编码器f_θ(x)进行编码。然后，从一个噪声初始化的动作轨迹开始，通过迭代的、基于梯度的能量最小化（即朗之万动力学/MCMC采样）对其进行优化，最终输出一个可执行的动作计划。优化过程在能量收敛到最小值时终止。</p>
<p>核心模块是能量变换器，它学习一个将输入和候选动作映射到一个标量能量的能量函数E_θ。与扩散策略的关键区别在于：扩散策略是分数模型，学习去噪以隐式估计数据分布的对数似然梯度；而EBT直接学习显式的能量函数，其负梯度即对应分数。这使得EBT直接对底层数据密度进行建模。</p>
<p>创新点具体体现在以下几个方面：</p>
<ol>
<li><strong>避免噪声调度器</strong>：EBT不依赖外部定义的噪声调度器来建模数据分布，而是端到端学习从噪声到数据的任意最优路径，这减少了与真实演示分布的脱节。</li>
<li><strong>平衡动态与误差累积减少</strong>：通过学习一个单一的、时不变的显式能量景观，EBT能将分布外数据点拉回数据流形，有效减少暴露偏差和复合误差。</li>
<li><strong>不确定性感知与动态推理</strong>：能量标量天然地编码了不确定性。EBT-Policy的推理步骤数是动态分配的，基于能量梯度范数是否低于阈值τ来决定何时停止，实现了对高不确定性状态分配更多计算，对简单状态分配较少计算的自适应行为。</li>
<li><strong>针对多模态和稳定性的训练技巧</strong>：为了学习多模态动作分布并保证训练稳定，论文引入了一系列关键设计：<ul>
<li><strong>随机化采样步骤</strong>：随机化MCMC步数以促进对不同能量模式的探索。</li>
<li><strong>缩放朗之万动力学</strong>：采用余弦退火计划注入受控随机噪声，在高能量区域进行广泛探索，在低能量区域进行精确收敛。</li>
<li><strong>步长随机化与能量缩放步长</strong>：随机化初始步长，并根据预测的能量值缩放更新步长（α_i = η exp(E_θ)），以防止优化过程越过或错过最小值。</li>
<li><strong>涅斯捷罗夫加速梯度</strong>：帮助平滑穿越复杂能量表面并逃离局部极小值。</li>
<li><strong>预采样归一化与梯度裁剪</strong>：对动作轨迹进行RMSNorm归一化，并将全局梯度范数裁剪到最大值1.0，这是确保训练稳定性的最关键组件。</li>
</ul>
</li>
</ol>
<p>训练算法（算法1）在每次迭代中从噪声初始化目标轨迹，执行随机化的MCMC采样（包含归一化和噪声注入），计算能量并更新轨迹，损失函数是每个采样轨迹与真实演示之间的均方误差累加。推理算法（算法2）则从噪声初始化开始，动态进行能量下降，直到达到最大步数或梯度范数低于阈值。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟基准和真实物理机器人环境中进行。使用了两个EBT-Policy变体：EBT-Policy-S（约30M参数）用于模拟环境公平对比，EBT-Policy-R（约100M参数）用于真实世界任务。</p>
<p><strong>Benchmark/数据集</strong>：</p>
<ul>
<li><strong>模拟</strong>：Robomimic基准套件，包括Lift、Can、Square和Tool Hang四个难度递增的操纵任务。</li>
<li><strong>真实世界</strong>：自行收集数据的三个任务：FoldTowel（折叠毛巾）、PlacePan（放置锅具）、PickAndPlace（拾取与放置）。</li>
</ul>
<p><strong>Baseline方法</strong>：主要与Diffusion Policy（扩散策略）进行对比。所有模型在相同数据集上训练，架构和参数量相当（≤150M），以确保公平比较。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>训练与推理效率</strong>：EBT-Policy训练和推理速度远快于扩散策略。在推理时，EBT通常仅需2步即可达到高成功率，而扩散策略需要100步才能达到相同成功率，实现了50倍的减少。</li>
<li><strong>模拟任务成功率</strong>：在Robomimic任务上，EBT-Policy consistently outperforms diffusion-based policies。特别是在Square和Tool Hang任务上取得了最先进的结果，比扩散策略高出多达24%。</li>
<li><strong>动态推理与不确定性建模</strong>：<br><img src="https://arxiv.org/html/2510.27545v1/imgs/main.png" alt="不确定性建模"><blockquote>
<p><strong>图2</strong>：不确定性建模解释。颜色条表示每帧能量，能量越低确定性越高。红色（第7步）标记了触发EBT-Policy重试的失败，绿色（第11步）标记了成功的纠正，展示了模型利用能量不确定性进行决策和调整的物理推理能力。</p>
</blockquote>
</li>
<li><strong>真实世界任务性能</strong>：在真实机器人任务中，EBT-Policy同样表现出色。<br><img src="https://arxiv.org/html/2510.27545v1/imgs/CollectDish.png" alt="真实世界任务演示"><blockquote>
<p><strong>图3</strong>：真实世界任务演示示例。</p>
</blockquote>
</li>
<li><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2510.27545v1/imgs/dp_edp_compare.drawio.png" alt="消融实验"><blockquote>
<p><strong>图11</strong>：消融实验对比。展示了完整EBT-Policy与其变体（如移除动态推理、移除某些稳定化技术）在性能上的对比，验证了各组件贡献。</p>
</blockquote>
</li>
<li><strong>涌现能力</strong>：EBT-Policy展示了无需显式重试训练数据或监督的零样本失败恢复能力。当动作序列失败时，模型能利用其能量感知自发地进行重试并成功纠正，这是在行为克隆中长期追求的能力。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li><strong>提出了EBT-Policy框架</strong>：将可扩展的能量变换器应用于机器人策略学习，提供了一种避免噪声调度、直接学习能量景观的隐式策略新范式。</li>
<li><strong>实现了高效、动态且不确定性感知的推理</strong>：通过能量最小化和动态计算分配，EBT-Policy在显著减少推理步数的同时，实现了对任务难度的自适应和决策过程的直观解释。</li>
<li><strong>展示了涌现的物理推理能力</strong>：模型在仅使用行为克隆数据的情况下，自发地学会了从失败中恢复的重试行为，显示了能量模型在捕获物理世界复杂动态方面的潜力。</li>
</ol>
<p>论文自身提到的局限性包括：当前结果可能受模型容量限制，未来通过超参数调整和使用更大模型有望进一步提升性能。</p>
<p>对后续研究的启示：EBT-Policy证明了基于能量的方法在机器人学习中的强大潜力，特别是在处理分布偏移、实现自适应计算和涌现复杂行为方面。这为构建更鲁棒、通用且具备高级推理能力的机器人系统指明了一个有希望的方向，即更多地利用能量模型的内在特性（如标量能量、平衡动态）来统一感知、推理和控制。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人策略学习中基于扩散的方法存在高计算成本、暴露偏差和推理不稳定等问题，提出了EBT-Policy这一基于能量的新架构。该方法利用基于能量的变换器（EBTs）学习能量景观，通过能量最小化搜索低能量动作轨迹，实现了端到端的均衡动态建模。实验表明，EBT-Policy在模拟和真实任务中均优于扩散策略，训练与推理计算量更低，部分任务仅需2步推理即可收敛（比扩散策略的100步减少50倍），并展现出无需重试训练的零样本失败恢复等新兴能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.27545" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>