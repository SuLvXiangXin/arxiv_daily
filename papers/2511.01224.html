<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Embodiment Transfer Learning for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Embodiment Transfer Learning for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01224" target="_blank" rel="noreferrer">2511.01224</a></span>
        <span>作者: Yaxin Peng Team</span>
        <span>日期: 2025-11-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人学习领域，基于自回归的视觉-语言-动作（VLA）模型，如RT-2和OpenVLA，通过将传感器输入映射到离散的动作令牌序列，在单臂（unimanual）操作任务上取得了显著进展。然而，这些模型在应用于多机器人系统时性能显著下降，甚至无法生成结构上有效的动作序列（例如，生成错误的令牌数量）。其关键局限性在于：1）现有VLA模型严重依赖于预训练数据，而当前开源的大规模机器人数据集（如OXE、Droid）几乎全部专注于单臂系统，导致模型在面对多机器人“具身”（embodiment）时遭遇分布偏移，缺乏理解新具身对应动作令牌的先验知识；2）标准的自回归VLA模型缺乏有效的任务规划能力，难以在多机器人协作场景中协调动作、分配子任务。</p>
<p>本文针对将预训练VLA模型高效、有效地迁移到多机器人系统这一具体痛点，提出了一种名为“具身迁移学习”（Embodiment Transfer Learning）的新视角。其核心思路是：通过合成数据进行的持续预训练来“预热”模型，使其适应新具身的动作令牌结构，再结合一种显式规划方法（具身思维图）来协调多机器人任务执行，从而实现快速微调和高效协作。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架称为ET-VLA，其核心包含两个关键技术：合成持续预训练（Synthetic Continued Pretraining, SCP）和具身思维图（Embodied Graph-of-Thought, EGoT）。整体流程是：首先对基础VLA模型（如OpenVLA）进行SCP，使其具备为多机器人生成正确数量动作令牌的能力；然后，在目标具身数据上微调时，引入EGoT来指导任务分解与协调。</p>
<p><img src="https://arxiv.org/html/2511.01224v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ET-VLA方法整体框架。左侧展示了合成持续预训练（SCP）过程，通过交叉采样合成双机器人数据，使模型学习生成14个动作令牌（每个机器人7个）。右侧展示了具身思维图（EGoT），它将任务分解为子任务节点（如抓取、释放、等待），并定义其依赖关系，以协调双机器人的动作序列。</p>
</blockquote>
<p><strong>核心模块一：合成持续预训练（SCP）</strong><br>SCP旨在解决VLA模型因预训练数据局限而无法为多机器人生成正确数量动作令牌的问题。其技术细节是：给定一个批次n个单机器人数据样本<code>X_n = {x_1, x_2, ..., x_n}</code>，其中每个<code>x_i = {observation, instruction}</code>原本映射到7个动作令牌<code>A_i</code>。为了合成双机器人数据，对于样本<code>x_i</code>，随机选择另一个样本<code>x_j (i≠j)</code>，并将<code>A_j</code>拼接到<code>A_i</code>之后，形成包含14个动作令牌（两个机器人的动作）的新地面真值。通过这种方式，模型在持续预训练中学习将输入映射到更长、结构正确的动作令牌序列，尽管图像/指令到具体动作的映射可能不完全准确，但核心目标是让模型学会“令牌数量-机器人具身”的映射关系。此阶段仅需在单机器人数据集（如BridgeData V2）上训练一个epoch。</p>
<p><strong>核心模块二：具身思维图（EGoT）</strong><br>EGoT旨在解决VLA模型在多机器人任务中缺乏规划与协调能力的问题。它将复杂任务分解为一个有向行动图<code>G = (V, E, T, N)</code>，其中<code>V</code>代表子任务节点，<code>E</code>代表节点间的依赖关系（<code>e_ij = (v_i, v_j)</code>表示<code>v_i</code>必须在<code>v_j</code>之前完成），<code>T</code>是任务类型分类，<code>N</code>指定机器人需求。论文定义了五种任务类型：1) <strong>抓取</strong>：机器人末端执行器参与并占用；2) <strong>释放</strong>：放置或释放物体；3) <strong>等待</strong>：暂停以等待另一机器人完成任务，便于协调；4) <strong>结束</strong>：该机器人所有任务完成；5) <strong>完成</strong>：整个工作流结束的终端节点。这个图结构使VLA模型能够明确区分每个机器人在任务执行中的功能和角色，促进更有效的协作。</p>
<p><img src="https://arxiv.org/html/2511.01224v1/x2.png" alt="EGoT示例"></p>
<blockquote>
<p><strong>图4</strong>：具身思维图（EGoT）的简化示例。展示了如何将“打开袋子放入球”的任务分解为一系列子任务节点（如“左手：抓取袋子”、“右手：等待”、“左手：打开袋子”等），并明确了节点间的顺序依赖关系，从而指导双机器人协同工作。</p>
</blockquote>
<p><strong>创新点</strong><br>与现有方法相比，ET-VLA的创新具体体现在：1) <strong>SCP</strong>：提出了一种低成本、高效的合成数据生成方法，专门针对多机器人动作令牌数量的适应性问题，避免了收集真实多机器人演示数据的昂贵成本。2) <strong>EGoT</strong>：将思维链（CoT）推广到多机器人具身场景，通过图结构显式建模子任务间的时序依赖和机器人分配，赋予了自回归VLA模型此前缺乏的复杂任务规划与协调能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/基准</strong>：真实世界六项双机器人协作任务（PickBread, PickFruits, WipePlate, InsertPlate, PullString, BuildBlocks）；仿真基准RLBench2（13个核心双手机器人任务）和RoboTwin（14个双手机器人任务）。</li>
<li><strong>实验平台</strong>：真实机器人使用两个UR5e机械臂；仿真在对应基准环境中进行。</li>
<li><strong>对比基线</strong>：<ul>
<li><strong>VLA类</strong>：OpenVLA, TinyVLA。</li>
<li><strong>非VLA策略</strong>：Diffusion Policy (DP), ACT。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>真实机器人实验</strong>：如表1所示，ET-VLA在六项任务上的平均成功率远超其他基线。具体而言，ET-VLA取得了46/77（59.74%）的成功率，而OpenVLA仅为5/77（6.49%），性能提升超过53.2%。即使在为OpenVLA提供双倍训练数据和时间的“额外数据”条件下，ET-VLA仍能领先约9%。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.01224v1/sec/figures/8chart.png" alt="学习曲线与成功率对比"></p>
<blockquote>
<p><strong>图3</strong>：（a）学习曲线显示，在BuildBlocks任务中，ET-VLA能快速收敛至高成功率，而OpenVLA即使增加数据和训练时长，成功率仍接近零。（b）六项任务平均成功率对比柱状图，直观展示了ET-VLA相对于OpenVLA及其增强版的显著优势。</p>
</blockquote>
<ol start="2">
<li><p><strong>仿真实验</strong>：</p>
<ul>
<li><strong>RLBench2</strong>：如表2所示，ET-VLA平均成功率为10.2%，显著高于ACT（5.9%）和OpenVLA（1.2%）。</li>
<li><strong>RoboTwin</strong>：如表3所示，ET-VLA平均成功率达40.1%，远高于Diffusion Policy的27.7%和OpenVLA的4.1%。</li>
</ul>
</li>
<li><p><strong>消融实验</strong>：如表4所示，移除EGoT仅保留SCP时，模型平均成功率降至37.66%；同时移除SCP和EGoT（即原始OpenVLA），成功率仅为6.49%。这证明了两个组件均为性能提升做出关键贡献，且SCP是解决令牌生成问题的基础，EGoT在此基础上进一步提升了任务规划能力。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.01224v1/sec/figures/block.png" alt="EGoT实时解释性"></p>
<blockquote>
<p><strong>图5</strong>：EGoT作为实时动作解释器的示例。在BuildBlocks任务中，当人为干扰导致左手机器人掉落积木时，EGoT的输出语言从“放置粉色圆柱体”切换回“抓取粉色圆柱体”，清晰反映了模型根据新状态重新规划的过程，提供了良好的可解释性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.01224v1/sec/figures/robot_setup.png" alt="实验设置"></p>
<blockquote>
<p><strong>图6</strong>：真实世界实验设置。使用两个UR5机械臂和一个顶置的RealSense D457相机，输入为单张224x224 RGB图像。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>深入分析了现有自回归VLA模型在多机器人多任务设置中的局限性，特别是对预训练数据的依赖和规划能力的缺失。</li>
<li>提出了ET-VLA框架，创新性地集成了合成持续预训练（SCP）和具身思维图（EGoT）两项技术，显著提升了VLA在多机器人操作中的性能。</li>
<li>在真实机器人和仿真环境中进行了广泛评估，证明了ET-VLA相对于SOTA VLA模型和其他策略方法的优越性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，当前工作以双手机器人（作为多机器人的简化版本）为例进行验证。框架虽可扩展至更多机器人，但更复杂多机器人系统（&gt;2）中的实际效能仍需进一步探索。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据高效迁移</strong>：SCP为如何利用丰富单机器人数据来赋能多机器人系统提供了一种低成本、高效的思路，可推广至其他需要适应新动作空间或具身的场景。</li>
<li><strong>规划增强的VLA</strong>：EGoT表明，为自回归VLA模型注入显式的、结构化的规划模块是解决其复杂任务协调短板的有效途径，这启发未来研究可以探索更多将高级规划与低级控制紧密结合的架构。</li>
<li><strong>可解释性</strong>：EGoT产生的可解释任务序列是一个涌现属性，展示了如何使黑盒式的VLA决策过程变得透明，这对机器人安全部署和人机交互具有重要意义。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在多机器人协作中表现不佳、难以生成有效动作序列的问题，提出了具身迁移学习框架ET-VLA。其核心技术包括：1）合成持续预训练，利用合成数据预热模型以学习新机器人的正确动作和精确令牌数，无需昂贵真人演示；2）具身思维图，将子任务建模为节点，以区分不同机器人的功能与角色。实验在三个双手机器人平台上验证，该方法在六项真实任务上性能超越OpenVLA达53.2%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01224" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>