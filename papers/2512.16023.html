<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.16023" target="_blank" rel="noreferrer">2512.16023</a></span>
        <span>作者: Abhinav Valada Team</span>
        <span>日期: 2025-12-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人策略学习领域，基于视频生成的方法展现出巨大潜力。现有方法主要分为两大范式：两阶段方法和联合模型。两阶段方法首先使用视频生成模型预测未来的视觉规划，然后附加一个策略模型从生成的视频或其潜在表示中推断动作。这种方法虽能利用强大的预训练视频扩散模型，但其动作生成性能严重依赖于视频生成的质量，且在生成的视频中机器人手臂仅部分可见时表现不佳。联合模型则学习视频和动作的共享潜在空间，通过联合建模加强策略模型。然而，这类方法通常需要从头训练联合扩散模型，或在大量数据上适配视频扩散模型以学习联合分布，在高质量数据有限时往往导致次优性能。</p>
<p>本文旨在解决上述局限性，提出一个既能继承预训练视频扩散模型的通用视觉知识，又能促进视频与动作模态间信息共享的框架。其核心思路是：扩展一个预训练的视频扩散模型，并行附加一个专用的动作扩散模型以保留预训练知识，并引入一种桥接注意力机制来实现有效的跨模态交互，从而协同生成高质量的视频-动作对。</p>
<h2 id="方法详解">方法详解</h2>
<p>CoVAR的整体目标是在给定初始观测图像 (v_0)、机器人初始关节状态 (a_0) 和自然语言指令 (c) 的条件下，训练一个多模态扩散模型以生成对齐指令的视频帧 (v) 和配对动作 (a)。方法采用多模态整流流来加速训练并提升生成质量。</p>
<p><img src="https://arxiv.org/html/2512.16023v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：CoVAR方法总览。(A) 模型基于视频扩散主干，并行附加一个动作DiT来生成动作。(B) 两种模态通过桥接注意力进行交互。(C) 对于低分辨率数据集，引入一个动作精炼模块。</p>
</blockquote>
<p><strong>整体框架</strong>：CoVAR建立在预训练视频扩散模型Open-Sora之上。不同于先前使用单一联合DiT的方法，本框架在视频DiT主干旁并行附加了一个专用的动作DiT模块。这种双分支设计旨在有限数据条件下保持视频模型的预训练知识，同时为动作生成提供专门路径。动作数据维度相对较低，因此使用轻量级MLP编码器获取动作嵌入，而非昂贵的VAE。动作DiT通过交叉注意力以文本指令 (c) 为条件，与视频DiT形成对称结构。两个模态间的信息交换通过提出的桥接注意力机制实现。动作解码器采用UNet架构，以生成更准确的动作。整个模型通过最小化整流流损失（公式2）进行优化。</p>
<p><strong>核心模块与创新点</strong>：</p>
<ol>
<li><p><strong>桥接注意力机制</strong>：这是实现有效跨模态交互的核心创新。先前工作通过将视频和动作token拼接后送入同一DiT来实现早期融合，但这需要将预训练知识适配到统一潜在空间，在数据有限时表现不佳。桥接注意力机制为视频特征 (f_v) 和动作特征 (f_a) 分别参数化独立的查询（(q_1, q_2)）、键（(k_1, k_2)）和值（(v_1, v_2)）。然后，将这些独立的投影结果拼接起来计算注意力，之后再分割回各自的模态特征（公式3）。这种设计相较于传统自注意力（单一QKV导致模态支配）或双向交叉注意力（破坏内部结构）的优势在于，它能减轻跨模态干扰，在保持各自模态内部一致性的同时实现有效的交互。消融实验证明该机制能同时提升视频生成质量和动作准确性。</p>
</li>
<li><p><strong>动作精炼模块</strong>：针对许多机器人数据集视频分辨率较低（如128p）导致生成轨迹不精确的问题，本方法引入了一个基于Transformer的动作精炼模型。该模块以初始图像和文本指令为条件，将模型生成的粗略动作转换为高精度动作序列（见图2C）。具体而言，精炼模型分别对输入的粗略动作和初始图像进行嵌入，将得到的动作token与图像token拼接后通过自注意力模块聚合视觉信息，再经过以文本描述 (c) 为条件的交叉注意力模块融入指令引导，最后解码输出精炼后的机器人动作。</p>
</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在三个数据集上进行：公开的仿真基准Calvin（200x200分辨率）和Libero90（128x128分辨率），以及自收集的真实世界数据集（涉及拾放螺丝、螺母等精细操作）。对比的基线方法包括联合模型（UVA， UWM， PAD）和两阶段模型（Unipi， RoboEnvision）。评估指标包括视频生成质量（PSNR， SSIM， LPIPS， FVD）和动作生成成功率。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>视频生成质量</strong>：如表I所示，在Calvin和Libero90数据集上，CoVAR在PSNR、SSIM、LPIPS、FVD四个指标上均优于三个联合模型基线。与两阶段方法中使用的纯视频扩散模型OpenSora-1.2相比，指标相当，表明引入动作模态并未降低视频生成质量。定性结果（图4）显示，CoVAR生成的视频中物体和机械臂的伪影更少，视觉更清晰、真实。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.16023v1/x4.png" alt="视频生成质量对比"></p>
<blockquote>
<p><strong>图4</strong>：与基线生成的视频对比。相较于其他基线，我们的模型生成的物体和机械臂内容伪影更少，结果更清晰、真实。推演结果显示了生成视频与对应动作间的强对齐性。</p>
</blockquote>
<ul>
<li><strong>动作生成成功率</strong>：<ul>
<li><strong>Calvin数据集</strong>：如表II所示，在开/关抽屉、橱柜、灯、拾取和推动物体五类任务上，CoVAR的成功率显著高于所有基线方法（例如，抽屉任务达到1.000）。</li>
<li><strong>Libero90数据集</strong>：如表III所示，在拾放、开/关、组合任务上，CoVAR（配合动作精炼模块）取得了显著提升（例如，拾放任务从0.592提升至0.873）。图5直观展示了动作精炼模块将粗略的动作趋势转化为精确、可成功执行轨迹的作用。</li>
<li><strong>真实数据集</strong>：如表IV所示，在拾取微小螺母、螺丝、销钉的挑战性任务中，CoVAR的成功率（0.64， 0.74， 0.70）远超两阶段基线（接近0）。图6展示了连续拾放任务的推演结果，生成的视频与真实世界执行高度吻合，体现了强视频-动作对齐性。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2512.16023v1/x5.png" alt="动作精炼效果对比"></p>
<blockquote>
<p><strong>图5</strong>：我们的模型与未使用动作精炼的变体之间的推演对比。未使用动作精炼的模型产生仅反映任务大致趋势的粗略动作，但精度不足；动作精炼提升了精度并使任务成功完成。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.16023v1/x6.png" alt="真实世界推演结果"></p>
<blockquote>
<p><strong>图6</strong>：真实世界实验生成的视频和推演结果。连续成功的拾放操作展示了高精度的动作生成和强的视频-动作对齐性。</p>
</blockquote>
<p><strong>消融实验</strong>：在自收集数据集上的消融研究（表V， 图7）验证了各组件贡献。</p>
<ul>
<li><strong>桥接注意力</strong>：将其替换为标准自注意力（SA）或交叉注意力（CA）后，视频质量和动作成功率均大幅下降，证明了桥接注意力在减轻模态干扰、促进有效信息交换方面的关键作用。</li>
<li><strong>UNet解码器</strong>：将动作头从UNet替换为ResNet后，动作准确性和视频质量均下降，表明UNet的多尺度特征处理能更好地耦合动作与视频流，生成更精确的动作和更逼真的视频。</li>
<li><strong>仅动作DiT</strong>：移除视频DiT，仅训练单模态动作生成模型，成功率急剧下降至0.08，凸显了视频模态提供的预训练知识和互补信息对于准确动作生成至关重要。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.16023v1/x7.png" alt="消融实验定性结果"></p>
<blockquote>
<p><strong>图7</strong>：消融研究的定性结果。红线表示真实动作作为参考。蓝线表示我们生成的动作。在“w/o BA”的轨迹图中，蓝线表示自注意力，绿线表示交叉注意力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个并行多模态扩散框架CoVAR，通过扩展预训练视频扩散模型并附加专用动作DiT，实现了视频与机器人动作的协同生成，在有限数据下保持了预训练知识；2) 设计了桥接注意力机制，通过为各模态维护独立的QKV投影，实现了有效的跨模态交互同时避免了干扰；3) 引入了针对低分辨率数据集的动作精炼模块，将粗略动作转化为精确控制序列，提升了任务执行可靠性。</p>
<p>论文自身提到的局限性在于，对于低分辨率数据集，需要额外的动作精炼模块来提升精度。这启示后续研究可以探索更统一的架构来直接处理不同分辨率的数据。</p>
<p>CoVAR的工作为机器人学习提供了一个可扩展的框架，其设计思路——在利用强大预训练模型与实现紧密跨模态耦合之间取得平衡——对数据受限的具身AI研究具有重要启发意义。它表明，通过精心设计的交互机制，可以协同提升多模态生成的性能，而不仅仅是简单的拼接或单向依赖。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出CoVAR方法，解决机器人操作中视频与动作数据难以协同生成的问题。现有方法存在两阶段流程跨模态信息共享不足，或需从头训练联合扩散模型难以利用预训练知识等局限。CoVAR通过扩展预训练视频扩散模型并附加专用动作扩散模型、引入桥接注意力机制实现跨模态交互、设计动作细化模块提升控制精度，实现了视频与动作的协同生成。实验表明，该方法在多个基准测试中能生成更高质量的视频和更准确的动作，性能显著优于现有基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.16023" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>