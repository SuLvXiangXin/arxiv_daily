<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19573" target="_blank" rel="noreferrer">2509.19573</a></span>
        <span>作者: Olkin, Zachary, Li, Kejun, Compton, William D., Ames, Aaron D.</span>
        <span>日期: 2025/09/23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人跑步是一项极具挑战性的任务，需要在非线性、混合动力系统上执行高度动态的运动。现有方法主要分为两类：基于模型的经典控制方法和无模型的强化学习方法。以混合零动力学（HZD）为代表的经典方法通过离线轨迹优化生成稳定的周期性轨迹，并利用非线性控制理论（如反馈线性化、控制李雅普诺夫函数-CLF）在线跟踪。这类方法虽能提供稳定性证明，但其控制器仅作用于连续动力学，生成瞬态行为和鲁棒行为的能力有限。另一方面，强化学习（RL）因其鲁棒性、轻量级部署和直接从经验中学习接触丰富行为的能力，在腿式机器人控制中日益流行，并已实现双足跑步。然而，许多RL方案需要大量手工调整奖励函数才能产生高性能且鲁棒的策略，奖励函数设计不当会导致学习不稳定、无法达成期望行为或训练时间过长。</p>
<p>本文针对上述痛点，提出将非线性控制理论中的控制李雅普诺夫函数（CLFs）与优化后的动态参考轨迹嵌入到强化学习训练过程中，以塑造奖励函数。其核心思路是利用CLF为RL提供原则性的、有理论依据的奖励信号，替代启发式奖励设计，从而引导智能体学习稳定、动态的跑步行为，并实现准确的全局参考跟踪。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的CLF-RL方法框架包含三个主要步骤：1）通过多域轨迹优化生成参考轨迹库；2）利用这些轨迹构建基于CLF的奖励函数；3）使用该奖励函数在仿真中训练RL策略，最终部署到真实人形机器人。</p>
<p><img src="https://arxiv.org/html/2509.19573v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧通过多域轨迹优化生成期望轨迹，用于构建基于CLF的奖励。右侧使用该奖励在仿真中训练RL策略，并部署到真实机器人上。</p>
</blockquote>
<p><strong>核心模块一：多域轨迹优化</strong>。为了生成跑步的参考轨迹，作者构建了一个包含单支撑相（SSP）、双支撑相（DSP）和飞行相（FLT）的混合系统模型。优化问题采用多段打靶法，固定域序列（例如跑步为SSP后接FLT），但优化每个域内的时间以及状态和输入。通过施加周期性约束、冲击不变性条件（公式7）、虚拟约束匹配（公式10f）以及动力学、输入约束，求解得到一个稳态的周期性步态。通过调整步长约束并利用较快步态的解来热启动较慢步态的优化，高效生成了速度从1.1 m/s到3.0 m/s的步态库。所有基于位置的虚拟约束均相对于支撑足的位置定义。</p>
<p><img src="https://arxiv.org/html/2509.19573v1/x2.png" alt="轨迹优化示意图"></p>
<blockquote>
<p><strong>图2</strong>：多段打靶轨迹优化问题示意图。展示了两个混合域，它们相遇的节点使用了相应的重置映射。每个域内有多个优化节点，并与各域的可变时间相关联。</p>
</blockquote>
<p><strong>核心模块二：CLF引导的RL设计</strong>。这是方法的核心创新。策略训练不再使用手工设计的启发式奖励，而是将CLF直接嵌入奖励函数。首先，定义输出（如关节、末端执行器位姿）与虚拟约束之间的跟踪误差η（公式11），并构建二次型CLF V(η)=η^T P η（公式12）。奖励函数包含四个部分：</p>
<ol>
<li><strong>CLF跟踪奖励</strong> <code>r_v</code>：鼓励当前状态接近期望轨迹，与CLF值V_t成负指数关系（公式13）。</li>
<li><strong>CLF衰减惩罚</strong> <code>r_v˙</code>：惩罚策略不满足CLF指数衰减条件<code>V˙_t + λV_t &lt; 0</code>（公式8），通过有限差分近似<code>V˙_t</code>计算（公式14）。这是引导学习趋向稳定行为的关键。</li>
<li><strong>域奖励</strong> <code>r_dom</code>：根据参考步态当前所处的阶段（SSP或FLT）给予奖励。在SSP鼓励支撑足保持静止（<code>r_hol</code>），在FLT惩罚足部接触力（<code>r_con</code>），以鼓励正确的接触序列。</li>
<li><strong>正则化奖励</strong> <code>r_reg</code>：包括扭矩惩罚、动作变化率惩罚、关节限位惩罚和扭矩超限惩罚。<br>总奖励为 <code>R = r_reg + r_v + r_v˙ + r_dom</code>（公式15）。策略（Actor）和评价（Critic）网络均为三层全连接前馈网络，使用ELU激活函数。为促进从仿真到实物的迁移，训练中采用了广泛的域随机化。此外，为了在跑步机上实现直线跑步，策略的观测中加入了由外环PD控制器生成的侧向和偏航速度指令，并在训练时对这些指令进行随机化，以缓解部署时的分布偏移。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真（IsaacLab/IsaacSim, Mujoco）和硬件（Unitree G1人形机器人）上进行。对比实验主要评估了CLF衰减条件奖励的有效性。关键结果如下：</p>
<p><img src="https://arxiv.org/html/2509.19573v1/x3.png" alt="仿真轨迹跟踪"></p>
<blockquote>
<p><strong>图3</strong>：在IsaacSim中，学习到的策略对选定虚拟约束的轨迹跟踪情况。策略成功跟踪参考轨迹，并且尽管步态库中没有瞬态信息，仍能产生必要的瞬态响应以收敛到稳态运动。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19573v1/x4.png" alt="CLF衰减条件对比"></p>
<blockquote>
<p><strong>图4</strong>：在Mujoco中，有/无CLF衰减条件的跟踪性能对比。虚线显示了轨迹各部分的平均速度。基于CLF的跟踪奖励在更高速度下表现优于纯跟踪奖励，突显了CLF奖励的性能优势。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19573v1/x5.png" alt="硬件实验设置与跟踪"></p>
<blockquote>
<p><strong>图5</strong>：实验设置及户外速度与位置跟踪。左图展示了机载计算机和激光雷达的位置以及直线跑步的分层控制架构。右图显示了户外跑步的硬件位置和速度跟踪数据。策略能很好地跟踪期望的位置和速度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19573v1/x6.png" alt="硬件跑步步态"></p>
<blockquote>
<p><strong>图6</strong>：室内外一个完整周期的跑步步态展示。突出了在物理系统上实现的多域行为（单支撑相和飞行相）。下方展示了户外硬件数据中选定的虚拟约束跟踪曲线，策略能够良好跟踪。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19573v1/x7.png" alt="抗干扰测试"></p>
<blockquote>
<p><strong>图7</strong>：对躯干和脚部施加扰动的硬件实验。左列：对躯干施加脉冲力后，机器人的位置和速度响应。右列：脚部被踩踏后，机器人的位置和速度响应。策略展示了强大的抗干扰能力。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>仿真跟踪性能</strong>：策略能够有效跟踪由轨迹优化生成的虚拟约束，即使初始状态偏离稳态，也能通过瞬态响应收敛（图3）。</li>
<li><strong>CLF奖励有效性</strong>：包含CLF衰减条件（<code>r_v˙</code>）的奖励函数，相比仅有关跟踪奖励（<code>r_v</code>），在高速（2.5 m/s）下能实现更准确的速度跟踪（图4）。</li>
<li><strong>硬件性能</strong>：策略成功部署于Unitree G1机器人，在跑步机上实现了包含明显飞行相的动态跑步，并在户外环境中稳定运行。仅使用机载传感器（激光雷达里程计），即可实现准确的全局位置和速度跟踪（图5，图6）。位置跟踪误差在前进（x）方向小于0.15米，侧向（y）方向小于0.05米；速度跟踪在x和y方向误差均小于0.1米/秒。</li>
<li><strong>鲁棒性</strong>：策略对施加于躯干的脉冲力干扰（导致最大0.25米/秒的速度误差）和脚部被踩踏的干扰均表现出强大的恢复能力（图7）。</li>
</ol>
<p><strong>消融实验</strong>：图4所示的对比实验实质上是对奖励函数中<code>r_v˙</code>组件的消融。结果表明，该组件对于在高速下保持良好的速度跟踪性能至关重要，验证了将CLF稳定性条件嵌入奖励的价值。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了CLF-RL框架，将控制李雅普诺夫函数与优化参考轨迹相结合，为强化学习提供了具有理论依据的奖励信号，避免了繁琐的启发式奖励调整；2）首次在<strong>全尺寸人形机器人</strong>上实现了动态跑步（包含飞行相），并在硬件上展示了准确的全局位置/速度跟踪能力；3）通过分层架构和训练时指令随机化，使策略具备了直线跑步和抗干扰的鲁棒性。</p>
<p>论文提到的局限性包括：生成的步态库速度范围有限（1.1-3.0 m/s），以及直线跑步性能依赖于准确的机器人状态估计（来自激光雷达里程计）。</p>
<p>这项工作对后续研究的启示是：为结合模型优化（提供先验与稳定性指导）与无模型学习（提供鲁棒性与灵活性）提供了一个强有力的范例。CLF作为一种原则性的奖励信号，其思想有望扩展到其他需要保证稳定性的强化学习控制任务中。如何自动生成或在线调整参考轨迹库以覆盖更广泛的操作范围，是未来一个重要的研究方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双足机器人跑步的稳定控制问题，提出了一种结合控制李雅普诺夫函数（CLF）与强化学习（RL）的方法CLF-RL。该方法将非线性控制中的CLF与优化的动态参考轨迹嵌入RL奖励函数，替代手工设计启发式奖励，以引导学习同时确保稳定性。核心是通过轨迹优化生成混合动力学（飞行与单支撑阶段）的可行参考轨迹，并利用CLF构建奖励，训练出能在真实环境中鲁棒跑步的策略。实验表明，该策略在跑步机及户外环境下均能可靠运行，对躯干与脚部扰动具有鲁棒性，且仅凭机载传感器即可实现准确的全局轨迹跟踪。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19573" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>