<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning to Design Soft Hands using Reward Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning to Design Soft Hands using Reward Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.17086" target="_blank" rel="noreferrer">2510.17086</a></span>
        <span>作者: Sha Yi Team</span>
        <span>日期: 2025-10-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>软体机器人手因其顺应性和安全性，在与人、物体及环境交互方面具有巨大潜力。然而，设计出既顺应又能在多样化用例中保持功能性的软体手仍具挑战。当前，硬件与控制协同设计能更好地将形态与行为耦合，但由此产生的搜索空间是高维的，即使基于仿真的评估计算成本也很高。现有方法如基于梯度或规划的方法通常需要大量领域专业知识，而数据驱动的机器学习方法又因缺乏直接监督而面临挑战。</p>
<p>本文针对软体手设计优化中仿真评估成本高昂这一具体痛点，提出了一种新视角：利用预收集的遥操作数据，结合奖励模型来加速设计空间的探索。本文的核心思路是提出一个结合奖励模型的交叉熵方法框架，在基于预收集遥操作策略的基础上，高效优化腱驱动软体机器人手的设计，相比纯优化方法减少一半以上的设计评估次数，同时从数据中学习到一个优化的手部设计分布。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架名为“带奖励模型的交叉熵方法”。其核心是利用预收集的遥操作控制数据作为评估基础，通过交叉熵方法迭代优化手部设计参数的分布，并引入一个神经网络奖励模型来部分替代耗时的仿真评估，从而加速优化过程。</p>
<p><img src="https://arxiv.org/html/2510.17086v1/figures/pipeline.jpg" alt="系统总览"></p>
<blockquote>
<p><strong>图3</strong>：系统总览。首先为每个物体收集多个遥操作控制数据集，在优化过程中随机采样。设计动作分布在CEM循环中被优化，评估来自仿真和共同训练的奖励模型（奖励模型评估的比例在训练期间平滑增加）。动作分布最终收敛到最优的软体手设计。</p>
</blockquote>
<p><strong>整体流程</strong>：首先，使用一个均匀的基线设计在真实世界和仿真中收集遥操作数据。优化开始时，从初始设计分布中采样一批设计候选。每个候选设计会与从数据集中随机采样的遥操作轨迹结合，在仿真中进行并行评估，获得地面真实奖励。同时，一个奖励模型被训练来预测给定设计的奖励。在CEM的每次迭代中，部分候选设计由奖励模型评估，部分由仿真评估，且奖励模型评估的比例ρ随训练平滑增加。根据评估奖励选出精英样本，用于更新CEM的分布参数（均值和方差）。来自仿真的（动作，奖励）数据对被存入缓冲区，用于持续训练奖励模型。此过程迭代进行，直至收敛，输出最优设计。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>设计空间</strong>：设计参数化了一个三指腱驱动软体手的关键变量。<br><img src="https://arxiv.org/html/2510.17086v1/figures/tendon_route.jpg" alt="软体手设计空间"></p>
<blockquote>
<p><strong>图2</strong>：软体机器人手设计空间。(a) 单个手指的3D和侧视图，参数包括分段长度、弯曲部长度、肌腱路径点分布和分段厚度。(b) 三指软体手，手指的安装位置和方向也是设计参数。</p>
</blockquote>
<ul>
<li><strong>手指几何</strong>：包括分段块长度 $l^{seg}$ 和弯曲部长度 $l^{fle}$（范围6-18毫米）。</li>
<li><strong>肌腱路径与厚度</strong>：对于每个分段块i，优化其厚度 $h_i$（4-18毫米）和肌腱路径点高度 $h_i^{ten}$（需低于分段高度）。</li>
<li><strong>安装配置</strong>：每个手指的安装位置 $\mathbf{p}$（由安装角 $\phi$ 决定）和安装方向 $\psi$（范围 $(-\pi/4, \pi/4)$）。</li>
</ul>
</li>
<li><p><strong>奖励函数</strong>：对于一组物体 $\mathcal{O}$，优化目标奖励函数定义为：<br>$\mathcal{R}<em>{opt}=w</em>{1}\sum_{o\in\mathcal{O}}\lVert\Delta q\rVert+w_{2}\sum_{o\in\mathcal{O}}\lvert\min(\Delta q_{y},0)\rvert-\mathcal{I}$<br>其中 $\Delta q$ 是物体相对于手腕的最终位姿位移，$\Delta q_y$ 是其垂直向上分量，$\mathcal{I}$ 是物体与地面碰撞的指示器。该奖励鼓励物体位移小（抓握稳定）、垂直位移非负（被抬起）、且不与地面碰撞。</p>
</li>
<li><p><strong>CEM与奖励模型协同优化</strong>：</p>
<ul>
<li><strong>CEM</strong>：维持一个设计参数的高斯分布 $\mathcal{N}(\mu,\sigma^{2}I)$，每轮迭代采样一批候选，根据奖励选出精英样本，并用精英样本的均值和方差更新分布。</li>
<li><strong>奖励模型</strong>：一个多层感知机，输入为设计动作，输出为预测奖励。其损失函数为预测奖励与仿真地面真实奖励之间的均方误差：$L = \frac{1}{N_{B}}\sum_{i}(r_{i}-Q(a_{i}|\theta))^{2}$。</li>
<li><strong>混合评估策略</strong>：在CEM迭代中，候选设计由仿真和奖励模型按比例 $(1-\rho)$ 和 $\rho$ 评估。评估率 $\rho$ 从 $\rho_{min}$ 线性增长至 $\rho_{max}$，使优化后期更多地依赖已训练好的高效奖励模型。</li>
</ul>
</li>
<li><p><strong>遥操作数据收集</strong>：<br><img src="https://arxiv.org/html/2510.17086v1/figures/teleop.jpg" alt="遥操作数据收集"></p>
<blockquote>
<p><strong>图4</strong>：遥操作数据收集。通过Meta Quest 3捕捉人手姿态，转换为软体手运动控制命令进行实时遥操作。抓取姿态、棱柱关节位移和肌腱运动被收集并在仿真中增强。</p>
</blockquote>
<ul>
<li>使用VR设备（Meta Quest 3）实时遥操作机械臂和软体手。</li>
<li>通过运动重定向，将人手的腕部位移、拇指与食指距离、手指弯曲度分别映射到机械臂末端、软体手棱柱关节位移和肌腱位移。</li>
<li>在仿真中复现并增强这些控制信号，用于后续的设计评估。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与纯CEM优化或纯奖励模型优化相比，本文的创新在于将两者动态结合。通过逐步增加奖励模型在评估中的占比，在保证优化方向可靠的前提下，大幅减少了耗时的仿真调用次数，实现了效率与效果的平衡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/基准</strong>：使用YCB物体数据集进行训练和测试。外域测试集来自Digital Twin Catalog、KIT和YCB数据集的额外物体。</li>
<li><strong>实验平台</strong>：仿真在NVIDIA Warp中基于有限元法实现；硬件使用xArm7机械臂、3D打印的优化手部、Dynamixel电机和Intel RealSense相机。</li>
<li><strong>对比基线</strong>：纯CEM优化、纯随机采样、仅使用奖励模型评估的CEM-RM ($\rho=1$)，以及本文提出的混合评估CEM-RM (hybrid)。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.17086v1/figures/main.jpg" alt="仿真优化结果"></p>
<blockquote>
<p><strong>图6</strong>：仿真结果。(a) 精英奖励和损失随环境交互次数的变化。混合CEM-RM收敛更快且最终性能与纯CEM相当。(b) 不同方法的完整训练时间对比。混合CEM-RM速度最快。(c) 使用单条或多条遥操作数据进行训练的精英奖励消融实验。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>优化效率</strong>：如图6(a)(b)所示，本文的CEM-RM (hybrid)方法在达到与纯CEM相近收敛性能的同时，所需的环境交互次数和总训练时间显著减少。奖励模型的损失也快速收敛。</li>
<li><strong>抓取成功率</strong>：<ul>
<li><strong>仿真</strong>：如表I所示，在域内重型物体上，CEM-RM (hybrid)取得了最高的成功率（87.6%），优于纯CEM（85.6%）和随机采样（68.0%）。在外域物体上，CEM-RM (hybrid)对重型物体的成功率（72.4%）也表现最佳。</li>
<li><strong>实物</strong>：如图7和表II所示，优化后的手部设计在多种形状和重量的物体上，抓取成功率全面优于均匀基线设计。例如，对于某些物体（如ID 9），基线成功率仅10%，而优化设计达到60%，若结合实时遥操作甚至可达90%。<br><img src="https://arxiv.org/html/2510.17086v1/figures/test.jpg" alt="实物对比实验"><blockquote>
<p><strong>图7</strong>：真实世界实验对比。均匀基线手无法抓起勺子、番茄酱罐，也无法稳定夹住夹子、兔子、番茄汤罐（抓取后滑落）。优化设计能稳定抓持所有物体。</p>
</blockquote>
</li>
</ul>
</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>遥操作数据多样性</strong>：图6(c)和表I显示，虽然使用单条遥操作数据训练收敛略快，但使用多条数据训练最终在测试中（尤其是重型物体）获得了更鲁棒和更高的成功率。</li>
<li><strong>种群大小</strong>：实验发现当CEM种群大小超过45后，精英奖励收敛值相近，因此选择45作为后续优化的参数。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.17086v1/figures/object.jpg" alt="测试物体"></p>
<blockquote>
<p><strong>图8</strong>：真实测试物体及其对应重量。涵盖扁平、薄壁、圆柱、不规则和盒状等多种几何形状及不同质量。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个结合奖励模型的交叉熵方法框架，用于高效优化软体机器人手的设计，在保持性能的同时将设计评估次数减少了一半以上。</li>
<li>定义并实现了一个全面的软体手参数化设计空间，并通过并行仿真和实物3D打印进行了验证。</li>
<li>在仿真和实物实验中证实，优化得到的设计在抓取多样化、具有挑战性的物体时，成功率显著优于均匀基线设计。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>仿真与真实世界之间存在差距，仿真的材料属性、接触模型等简化可能影响优化结果向实物的转移。</li>
<li>当前设计空间虽全面，但仍是参数化的，可能限制了某些非直觉创新形态的发现。</li>
<li>优化依赖于预收集的遥操作数据质量，若数据不能覆盖有效的抓取策略，可能限制优化上限。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>证明了“数据驱动设计优化”与“基于模型的优化”相结合的有效性，为其他机器人结构设计问题提供了可借鉴的框架。</li>
<li>展示了利用人类演示（遥操作）作为先验，来引导和加速硬件设计搜索的潜力，弥合了行为与形态之间的鸿沟。</li>
<li>未来可探索将设计空间扩展到非参数化表示（如神经网络生成），或结合更高级的世界模型来进一步降低对真实仿真的依赖。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何高效设计兼具柔顺性与功能性的软体机械手。针对硬件与控制协同设计搜索空间大、仿真评估成本高的问题，提出基于奖励模型的交叉熵方法（CEM-RM），利用预收集的遥操作数据优化手指模块、肌腱布局等设计分布。该方法将设计评估次数减少一半以上，并通过3D打印实现硬件验证。实验表明，优化后的软体手在多种挑战性物体抓取任务中成功率显著优于基线设计。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.17086" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>