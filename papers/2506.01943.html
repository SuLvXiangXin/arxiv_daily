<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.01943" target="_blank" rel="noreferrer">2506.01943</a></span>
        <span>作者: Dahua Lin Team</span>
        <span>日期: 2025-06-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在具身智能领域，大规模、高质量的机器人操作数据稀缺是制约策略学习的瓶颈。近期，视频生成模型为合成逼真的仿真数据提供了可能，其中轨迹条件控制能够实现对机器人运动的细粒度规划。然而，现有方法（如Tora、DragAnything）主要关注驱动单个物体的运动，使用分离的轨迹分别控制机器臂和被操作物体。这种设计在多物体交互（如抓取、移动）的重叠区域会导致特征纠缠，损害生成视频的视觉保真度和物理合理性。这对于后续通过逆动力学模型从视频中提取可执行动作标签至关重要，不准确的交互模拟会导致不可靠的动作提取。</p>
<p>本文针对多物体交互场景中特征纠缠和轨迹控制不精确的痛点，提出了“协作轨迹”的新视角。其核心思路是：将交互过程分解为三个阶段，并使用一个统一的轨迹来建模交互动力学，而非分解物体。具体而言，将交互分解为交互前、交互中和交互后三个阶段，每个阶段由主导物体（机器臂或操作对象）的轨迹和特征引导，从而缓解特征融合问题并提升交互质量。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboMaster是一个基于协作轨迹机制的图像到视频生成框架。其目标是给定初始帧、文本提示、用户定义的对象掩码以及一个描述分解后各阶段运动的协作轨迹，生成逼真的机器人操作视频。</p>
<p><img src="https://arxiv.org/html/2506.01943v3/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：RoboMaster框架总览。给定输入图像和提示，通过编码对象掩码获得外观和形状感知的潜在表示以维持身份一致性。协作轨迹被分解为三个子阶段，每个阶段与特定的对象潜在表示关联。协作轨迹潜在表示通过即插即用的运动注入器注入到视频扩散Transformer中，引导视频动态生成。</p>
</blockquote>
<p><strong>整体流程</strong>：输入包括初始帧 <strong>I</strong>、文本提示 <strong>c</strong>、机器臂掩码 <strong>M_d</strong>、被操作物体掩码 <strong>M_s</strong> 以及协作轨迹 <strong>C</strong>。输出为生成的视频 <strong>X</strong>。流程首先通过VAE编码器将初始帧编码为潜在特征 <strong>z</strong>，同时将对象掩码下采样至相同空间分辨率。接着，通过<strong>主题表示</strong>模块提取对象的外观与形状嵌入，并通过<strong>协作轨迹表示</strong>模块将轨迹与对象特征结合，形成协作轨迹潜在表示 <strong>V</strong>。最后，<strong>运动注入模块</strong>将 <strong>V</strong> 注入到视频扩散Transformer（DiT）中，引导去噪过程以生成视频。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>主题表示（外观与形状嵌入）</strong>：如图4所示，首先根据下采样后的对象掩码 <strong>m</strong>，从潜在特征 <strong>z</strong> 中采样有效像素，并通过平均池化得到外观嵌入 **v~**。为了增强空间感知，以轨迹点 <strong>(x, y)_t</strong> 为中心，根据掩码面积比例确定半径 <strong>r</strong>，构建一个圆形体积表示 <strong>v</strong>。该表示在时空维度上扩展，将外观信息与形状（空间范围）相结合，有助于在视频序列中保持对象身份一致性。<br><img src="https://arxiv.org/html/2506.01943v3/x4.png" alt="主题嵌入"></p>
<blockquote>
<p><strong>图4</strong>：主题嵌入示意图。对象掩码与编码的RGB潜在对齐后，采样有效像素并池化得到嵌入向量，再根据掩码面积扩展为圆形体积表示，以融合外观和形状信息。</p>
</blockquote>
</li>
<li><p><strong>协作轨迹表示</strong>：这是方法的核心创新。与先前方法将多个物体轨迹独立建模不同，RoboMaster使用一个统一的协作轨迹 <strong>C</strong>，并将其按时间分解为三个阶段：</p>
<ul>
<li>**交互前阶段 (C1)**：机器臂（主导物体 <strong>o_d</strong>）运动，物体静止。使用 <strong>v_d</strong> 和 <strong>C1</strong> 建模分布。</li>
<li>**交互阶段 (C2)**：被操作物体（从属物体 <strong>o_s</strong>）运动，其轨迹隐式地引导机器臂运动（因两者动力学关系受限）。使用 <strong>v_s</strong> 和 <strong>C2</strong> 建模分布。</li>
<li><strong>交互后阶段 (C3)<strong>：机器臂再次成为主导，使用 <strong>v_d</strong> 和 <strong>C3</strong> 建模分布。<br>考虑到VAE编码的因果性，后一阶段的生成会依赖前一阶段的潜在特征，因此完整的建模分布如公式(5)所示，是多个对象感知子分布的因子分解。这种设计将特征表示在时间上动态切换（</strong>v_d → v_s → v_d</strong>），为模型提供了行为变化的线索，从而缓解特征混淆。</li>
</ul>
</li>
<li><p><strong>运动注入模块</strong>：协作轨迹潜在表示 <strong>V</strong> 被块化后，依次通过零初始化的2D空间卷积层和1D时间卷积层进行编码，得到紧凑表示 **V<del><strong>。该表示与DiT块输出的隐藏状态 <strong>h</strong> 相加后，输入到后续的DiT块中，公式为：</strong>h = h + norm(V</del>) + V~**。模型通过优化标准的扩散去噪损失函数进行训练。</p>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>轨迹建模创新</strong>：从“分解物体”变为“分解交互过程”，提出三阶段协作轨迹，统一建模交互动力学。</li>
<li><strong>特征引导创新</strong>：在不同阶段动态切换主导对象的特征嵌入（<strong>v_d/v_s</strong>），显式地为模型提供交互阶段的语义和物理约束。</li>
<li><strong>用户交互友好</strong>：只需提供一个分解后的单一轨迹路径，并使用掩码（可通过画笔工具粗略标注）指定物体，简化了用户输入。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：主要在挑战性的Bridge数据集上进行训练和评估，同时也在RLBench和SIMPLER基准上进行机器人动作规划评估。</li>
<li><strong>对比基线</strong>：与轨迹控制的SOTA方法对比，包括Tora、MotionCtrl、DragAnything、IRASim，以及4D基础方法TesserAct。为确保公平，所有基线均基于相同的CogVideoX-5B架构在相同数据集上重新训练。</li>
<li><strong>评估指标</strong>：<ul>
<li><strong>视频质量</strong>：Fréchet Video Distance (FVD↓)、PSNR (↑)、SSIM (↑)。</li>
<li><strong>轨迹精度</strong>：机器臂轨迹误差 (<code>TrajError_robot</code>↓) 和物体轨迹误差 (<code>TrajError_obj</code>↓)。</li>
<li><strong>用户研究</strong>：视觉质量偏好百分比 (Preference↑)。</li>
<li><strong>机器人规划</strong>：下游任务成功率。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：<br>如表2所示，RoboMaster在各项指标上均达到最优。</p>
<ul>
<li>视频质量：FVD最低（147.31），PSNR（21.55）和SSIM（0.803）最高。</li>
<li>轨迹精度：机器臂和物体的轨迹误差最低，分别为16.47和24.16。</li>
<li>用户偏好：获得45.16%的偏好率，显著高于其他方法。<br><img src="https://arxiv.org/html/2506.01943v3/x5.png" alt="定性对比"><blockquote>
<p><strong>图5</strong>：与基线方法的定性对比。RoboMaster在多种操作技能（移动、抓取、关闭、扶正等）上均表现出更优的视觉一致性，特别是在被操作物体的身份保持上（见白色框区域）。<br><img src="https://arxiv.org/html/2506.01943v3/x6.png" alt="分布外泛化"><br><strong>图6</strong>：在分布外（in-the-wild）图像上的泛化能力。RoboMaster对于未见过的物体和场景仍能生成逼真且符合轨迹的视频，展示了良好的泛化性。</p>
</blockquote>
</li>
</ul>
<p><strong>机器人动作规划结果</strong>：<br>在RLBench和SIMPLER的9个任务上评估生成视频用于下游策略学习的有效性。通过生成演示视频并提取动作标签来模拟规划。<br><img src="https://arxiv.org/html/2506.01943v3/x7.png" alt="机器人规划"></p>
<blockquote>
<p><strong>图7</strong>：机器人动作规划任务成功率对比。RoboMaster在多数任务上取得了最高或接近最高的成功率，验证了其作为世界模拟器为机器人学习提供高质量数据的能力。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文通过消融实验验证了各组件贡献。移除协作轨迹设计（即使用分离轨迹）或移除外观-形状嵌入都会导致性能下降，特别是轨迹误差显著增加和视觉质量降低，证明两者对建模精确交互和保持对象一致性均至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出协作轨迹框架</strong>：通过将交互过程分解为三个阶段并统一建模，创新性地解决了多物体交互视频生成中的特征纠缠问题，提升了视觉质量和轨迹精度。</li>
<li><strong>设计外观-形状感知的对象表示</strong>：利用掩码构建圆形体积潜在表示，有效保持了被操作物体在视频序列中的语义一致性。</li>
<li><strong>实现了用户友好的交互式仿真</strong>：简化了用户输入（单一分解轨迹、粗略掩码），使方法更易于用于机器人数据标注和迭代修正，并在多个基准测试中验证了其作为高质量机器人数据模拟器的有效性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法目前处理的是2D轨迹，未来可探索融入3D或6自由度轨迹以支持更复杂的操作。此外，对于极其复杂的多物体、长时间交互场景的建模能力仍有待进一步探索。</p>
<p><strong>启示</strong>：这项工作为视频生成在机器人仿真中的应用提供了一个新范式，即通过显式建模交互的阶段性和主导角色来理解物理交互。这对未来开发更复杂、更物理真实的世界模型具有启发意义，同时其用户友好的设计思路也推动了人机协作在数据生成方面的应用。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对轨迹控制视频生成模型难以处理机器人操作中多物体交互、导致特征纠缠和视觉质量下降的问题，提出RoboMaster框架。其核心方法是**协作轨迹控制**，将交互过程分解为**交互前、中、后**三阶段，并分别在每个阶段以机械臂或操作物体作为**主导物体**进行建模，同时引入外观与形状感知的潜在表示以保持语义一致性。实验在Bridge、RLBench等基准上验证了该方法的有效性，取得了最先进的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.01943" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>