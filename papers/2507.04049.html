<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Breaking Imitation Bottlenecks: Reinforced Diffusion Powers Diverse Trajectory Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Breaking Imitation Bottlenecks: Reinforced Diffusion Powers Diverse Trajectory Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.04049" target="_blank" rel="noreferrer">2507.04049</a></span>
        <span>作者: Yadan Luo Team</span>
        <span>日期: 2025-07-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前端到端自动驾驶（E2E-AD）的主流方法是基于模仿学习，通过最小化预测轨迹与单条专家轨迹之间的差异来学习驾驶策略。这种方法存在关键局限性：无论是早期的单模态轨迹预测，还是后续引入扩散模型的多模态轨迹预测，其训练过程都依赖于单一的专家演示。这导致模型倾向于将所有概率质量集中在唯一的地面真值轨迹上，产生模式崩溃问题，即生成的多条轨迹过度聚集在专家轨迹附近，缺乏行为多样性，难以应对复杂多变的真实驾驶场景。</p>
<p>本文针对模仿学习中固有的模式崩溃这一具体痛点，提出了结合生成模型与策略优化的新视角。核心思路是利用扩散模型的强大生成能力产生多样化的轨迹候选，并引入强化学习，通过优化多样性和安全性等轨迹级奖励信号来直接指导扩散过程，从而打破对单一专家演示的依赖。</p>
<h2 id="方法详解">方法详解</h2>
<p>DIVER是一个结合扩散模型与强化学习的多模态端到端自动驾驶框架。其整体流程是：首先，感知模块处理多视角图像，提取场景的BEV特征、动态智能体特征和静态地图特征。然后，运动规划器以这些感知特征为条件，通过策略感知扩散生成器（PADG）输出一组多样化的未来轨迹。训练采用模仿学习与强化学习混合的范式。</p>
<p><img src="https://arxiv.org/html/2507.04049v3/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：DIVER的整体架构。感知模块编码多视角图像为特征图以提取场景表示，随后预测周围智能体运动，并通过由强化学习指导的条件扩散模型进行规划，生成多样化的多模态轨迹。</p>
</blockquote>
<p>核心模块是<strong>策略感知扩散生成器</strong>。PADG基于条件扩散框架，采用双分支架构：一个分支学习预测轨迹的分布，另一个分支对专家轨迹进行建模以提供指导。其工作流程包括两个关键部分：</p>
<ol>
<li><strong>多样化多模态轨迹前向扩散</strong>：不仅对预测的多模态轨迹 $\tau_0^{(m)}$ 施加高斯噪声，也对地面真值轨迹 $\tau_0^{gt}$ 施加噪声。这鼓励模型在更广阔的轨迹潜在空间中探索，同时通过有监督的去噪学习将生成过程锚定在真实的运动模式上。</li>
<li><strong>条件扩散解码器</strong>：该解码器引导噪声轨迹的去噪过程。它融合了丰富的场景语义（BEV地图、智能体特征）和轨迹锚点特征。具体通过一个两阶段的条件特征融合管道实现：<ul>
<li><strong>轨迹感知特征聚合</strong>：根据噪声轨迹的坐标，从BEV特征中稀疏采样并聚合多尺度图像特征，得到与轨迹局部相关的感知特征 $F_{traj}^{(m)}$。</li>
<li><strong>分层注意力解码</strong>：将轨迹特征 $F_{traj}^{(m)}$ 作为查询，依次与智能体级记忆、地图级记忆和导航级锚点进行多头交叉注意力交互，最终输出 refined 的特征 $F_{out}^{(m)}$，用于预测去噪后的轨迹。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04049v3/x4.png" alt="策略感知扩散生成器"></p>
<blockquote>
<p><strong>图4</strong>：策略感知扩散生成器示意图。通过将预测轨迹和GT轨迹作为输入，PADG在场景上下文（地图和智能体）的引导下，通过条件去噪过程从噪声中重建出多样化的多模态轨迹。</p>
</blockquote>
<p>与现有方法相比，DIVER的核心创新在于引入了<strong>强化学习来指导扩散过程</strong>。论文将扩散过程视为一个随机策略，并采用基于分组相对策略优化（GRPO）的目标函数。通过设计奖励函数（如鼓励轨迹间的多样性、避免碰撞、保持车道内行驶等），GRPO直接优化轨迹级别的长期回报，从而显式地缓解模式崩溃并增强安全性，引导模型探索超越专家演示的、物理上可行的驾驶计划。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在闭环仿真基准 <strong>NAVISIM</strong> 和 <strong>Bench2Drive</strong>，以及开环数据集 <strong>nuScenes</strong> 上进行。对比的基线方法包括：UniAD、VAD、VADv2、SparseDrive、DiffusionDrive、DriveSuprim、Hydra-MDP 和 AlphaDrive。</p>
<p>关键实验结果如下：</p>
<ol>
<li><strong>开环nuScenes评估</strong>：DIVER在提出的<strong>轨迹多样性指标</strong>上显著优于所有基线，平均提升超过40%。例如，在nuScenes验证集上，DIVER的多样性得分为3.21，而次优的DiffusionDrive为2.28，VADv2仅为1.54。这证明了DIVER在生成多样化轨迹方面的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04049v3/x5.png" alt="开环多样性结果"></p>
<blockquote>
<p><strong>图5</strong>：在nuScenes数据集上的开环多模态轨迹多样性评估。DIVER在提出的多样性指标上大幅领先所有基线方法。</p>
</blockquote>
<ol start="2">
<li><strong>闭环NAVISIM评估</strong>：在考虑安全性和交通规则遵守的闭环仿真中，DIVER在<strong>驾驶分数</strong>上达到最高（77.2%），同时保持了较低的碰撞率（1.2%）。这表明其生成的多样化轨迹并未牺牲驾驶安全性。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04049v3/x6.png" alt="闭环NAVISIM结果"></p>
<blockquote>
<p><strong>图6</strong>：在NAVISIM闭环仿真基准上的结果。DIVER在驾驶分数上最优，并且碰撞率较低。</p>
</blockquote>
<ol start="3">
<li><strong>闭环Bench2Drive评估</strong>：DIVER在<strong>成功率</strong>（85.4%）和<strong>进度</strong>（91.0%）两个关键指标上均达到最佳，进一步验证了其在复杂动态场景中完成驾驶任务的能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04049v3/x7.png" alt="闭环Bench2Drive结果"></p>
<blockquote>
<p><strong>图7</strong>：在Bench2Drive闭环仿真基准上的结果。DIVER在成功率和进度上表现最佳。</p>
</blockquote>
<ol start="4">
<li><strong>定性结果与消融实验</strong>：<ul>
<li>定性对比显示，基线方法（如VADv2、DiffusionDrive）的轨迹高度重合，而DIVER能生成明显分化的多种合理轨迹（如换道、跟驰）。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04049v3/x8.png" alt="定性结果"></p>
<blockquote>
<p><strong>图8</strong>：多模态轨迹生成的定性对比。DIVER能产生明显分化的轨迹（红色虚线），而基线方法轨迹高度重合（蓝色实线）。</p>
</blockquote>
<pre><code>*   消融实验表明，**移除强化学习组件**会导致多样性指标大幅下降（从3.21降至2.45），**移除PADG中的多参考GT**也会损害性能。这验证了强化学习指导和多参考GT对于打破模式崩溃都是至关重要的。
</code></pre>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了<strong>DIVER框架</strong>，首次将强化学习与扩散模型相结合用于E2E-AD的轨迹生成，有效解决了模仿学习中的模式崩溃问题；2）设计了<strong>策略感知扩散生成器</strong>，通过条件扩散和场景特征融合生成多样化且可行的轨迹；3）引入了基于<strong>GRPO的强化学习指导</strong>，直接优化多样性和安全性奖励，引导探索；4）提出了一个新的<strong>轨迹多样性评估指标</strong>，更好地衡量多模态预测的多样性。</p>
<p>论文自身提到的局限性包括：尚未探索更复杂的奖励函数设计（如考虑舒适度、燃油经济性等），以及GRPO训练可能带来的计算开销。</p>
<p>这项工作对后续研究的启示是：为生成多样化、安全的驾驶行为提供了一种新的范式，即结合生成模型的表达能力和强化学习的优化目标。未来可以探索更精细的奖励塑造、更高效的策略优化算法，以及将该框架扩展到更长期的规划或包含语言指令的交互式驾驶场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对端到端自动驾驶中模仿学习导致的轨迹保守、模式崩溃问题，提出DIVER框架。该框架结合扩散模型生成多模式参考轨迹，并采用强化学习（Group Relative Policy Optimization）优化轨迹级多样性与安全奖励。实验在闭环NAVSIM、Bench2Drive和开环nuScenes数据集上表明，DIVER显著提升轨迹多样性，有效克服模式崩溃。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.04049" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>