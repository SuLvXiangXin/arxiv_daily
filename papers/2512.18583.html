<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.18583" target="_blank" rel="noreferrer">2512.18583</a></span>
        <span>作者: Xin Xu Team</span>
        <span>日期: 2025-12-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>对抗模仿学习（AIL）是模仿学习中的主流框架，通过从专家示教中推断奖励来指导策略优化。尽管提供更多的专家示教通常能带来更好的性能和稳定性，但在某些场景下收集此类示教数据具有挑战性。现有的基于扩散模型的AIL方法（如DiffAIL、DRAIL）主要利用扩散模型的分布建模能力来增强判别器，却忽视了其强大的数据生成潜力。本文针对专家示教数据稀缺的痛点，提出了一个新视角：重新利用扩散模型的数据生成能力，创造大量合成示教数据来增强判别器的训练。本文的核心思路是利用扩散模型生成伪专家示教，并通过一种优先专家示教回放策略（PEDR）来高效利用这些合成数据，从而提升AIL的性能和收敛速度。</p>
<h2 id="方法详解">方法详解</h2>
<p>SD2AIL的整体框架包含两个交替进行的阶段：判别器训练和策略优化。</p>
<p><img src="https://arxiv.org/html/2512.18583v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：SD2AIL整体框架。(a) 扩散判别器 D_φ 通过优先专家示教回放方法采样专家示教和伪专家示教，并学习区分这些数据与智能体数据。(b) 在优化策略 π_θ 时，扩散判别器根据其输出 D_φ 计算奖励 R，并在去噪过程中生成伪专家示教。策略网络学习最大化判别器给出的奖励。生成的伪专家示教将用于下一轮的判别器训练。</p>
</blockquote>
<p><strong>核心模块一：基于扩散的判别器与合成示教生成</strong><br>本文沿用基于扩散的AIL框架，判别器 D_φ 通过扩散损失（公式1）来评估状态-动作对来自专家分布的可能性，其输出定义为对多个去噪步的负扩散损失取指数平均（公式2）。本文的创新在于重新启用了扩散模型的生成能力。在策略优化阶段，通过迭代去噪过程（公式3）生成大量样本 (s_i, a_i)^0。为了确保生成数据的质量，设定一个动态阈值 τ（公式4），该阈值是当前专家示教置信度 D_φ 的平均值。只有置信度高于 τ 的生成样本才会被认定为有效的伪专家示教 π_pe。这种动态阈值允许在训练早期使用较低阈值以纳入更多伪专家示教，在后期提高阈值以保证样本质量。在判别器训练时，会按特定比例（实验中为7:1）从伪专家示教 π_pe 和真实专家示教 π_e 中采样数据，其优化目标在标准AIL目标基础上增加了对伪专家数据的匹配项（公式5）。</p>
<p><strong>核心模块二：优先专家示教回放（PEDR）</strong><br>由于引入了大量合成数据，不同示教数据的重要性存在差异。PEDR机制旨在更频繁地回放更有价值的示教数据。它使用判别器对第 i 个（伪）专家示教的误差 δ_i = 1 - D_φ 来衡量其重要性。采样概率 P(i) 与 |δ_i| 的 ζ 次幂成正比（公式6），其中 ζ 控制优先程度。为了减少采样偏差，引入了重要性采样权重 w_i（公式7）来调整判别器的损失函数，其中参数 η 随时间逐渐退火至1以稳定训练。最终，判别器的损失函数 ℒ(φ) 被修改为对专家和伪专家数据应用重要性加权后的二元交叉熵损失（公式8）。此外，方法为 N 条专家轨迹分别建立了 N 个优先回放缓冲区，并为所有伪专家示教建立一个缓冲区，以确保训练不偏向任何特定专家轨迹，且每次迭代都包含两类数据。</p>
<p><strong>与现有方法的创新点</strong><br>与DiffAIL等仅利用扩散模型作为判别器（分布建模）的方法不同，SD2AIL的核心创新在于充分挖掘并利用了同一扩散模型的<strong>数据生成能力</strong>，通过合成高质量的伪专家示教来扩充训练数据。其次，针对扩增后的大规模（伪）专家数据池，提出了<strong>PEDR机制</strong>进行智能采样，提升了数据利用效率和收敛速度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在四个经典的MuJoCo连续控制任务（Ant, Hopper, Walker, HalfCheetah）上进行评估，使用包含40条轨迹的数据集。基线方法包括BC、GAIL、SMILING、DiffAIL和DRAIL。实验分别使用随机采样的1、4、16条专家轨迹进行训练，结果取5个随机种子的均值和标准差。</p>
<p><img src="https://arxiv.org/html/2512.18583v1/x3.png" alt="结果对比"></p>
<blockquote>
<p><strong>图3</strong>：在四个任务上的平均回报对比。SD2AIL在Hopper、Walker和HalfCheetah任务上收敛速度显著快于基线。在所有四个任务上，其最终性能均超越或媲美DRAIL和SMILING，并在三个任务上超越了DiffAIL。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li>在仅使用1条专家轨迹的设置下，SD2AIL在Ant、Hopper和HalfCheetah任务上分别取得了5345、3441和5885的最大环境回报，均超过了人类专家水平（分别为4228、3402、4663）。在Hopper任务上，平均回报3441超过了当时最先进方法89。</li>
<li>收敛速度：在单轨迹设置下，SD2AIL在Hopper和HalfCheetah任务上分别仅需约21万步和18万步即可收敛，远快于基线。</li>
<li>随着真实专家轨迹数量增加，SD2AIL性能持续提升且收敛所需步数减少。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.18583v1/x4.png" alt="示教可视化"></p>
<blockquote>
<p><strong>图4</strong>：示教数据可视化。生成的伪专家示教与真实专家示教分布相似，为判别器训练提供了更清晰的指导信号。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.18583v1/x5.png" alt="Fréchet距离"></p>
<blockquote>
<p><strong>图5</strong>：伪专家示教与专家示教分布之间的Fréchet距离（FD）。伪专家示教的FD随训练下降至85.4，远低于随机策略的304.7，证明生成样本的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.18583v1/x6.png" alt="奖励相关性"></p>
<blockquote>
<p><strong>图6</strong>：判别器回报与环境回报的相关性。SD2AIL在四个任务上的皮尔逊相关系数（PCC）分别为93.0%、90.1%、92.3%和85.2%，均高于DiffAIL（92.2%、77.4%、81.3%、81.8%），表明其替代奖励与真实奖励线性相关性更强。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.18583v1/x7.png" alt="去噪步数影响"></p>
<blockquote>
<p><strong>图7</strong>：不同去噪步数对结果的影响。设置去噪步数为10可以在模型效果和实际训练时间之间取得良好平衡。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.18583v1/x8.png" alt="组件消融"></p>
<blockquote>
<p><strong>图8</strong>：各组件贡献的消融研究。在Walker环境中，仅使用伪专家示教（Pseudo-Expert Only）或仅使用PEDR（PEDR Only）均能超越DiffAIL，两者结合（SD2AIL）取得了最佳性能。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>伪专家示教模块</strong>：单独使用即能提升性能（Walker任务峰值回报4557）。</li>
<li><strong>PEDR模块</strong>：单独使用也能带来增益（Walker任务峰值回报4907）。</li>
<li><strong>两者结合</strong>：SD2AIL实现了最佳性能，证明了两个组件的有效性和互补性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了利用扩散模型生成大量合成专家示教来增强AIL中判别器训练的新方法。</li>
<li>设计了优先专家示教回放（PEDR）机制，通过重要性采样高效利用大规模（伪）专家数据池，提升了收敛速度和效率。</li>
<li>在多个连续控制任务上验证了SD2AIL的有效性，其性能优于或媲美现有先进方法，且收敛更快、更稳定。</li>
</ol>
<p><strong>局限性</strong>：论文提到，使用扩散模型可能会相对增加训练时间。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>为缓解模仿学习中专家数据稀缺问题提供了一个新颖且有效的思路，即利用生成模型扩充数据。</li>
<li>PEDR机制表明，在数据丰富的场景下，对训练数据进行智能优先级调度可以进一步提升学习效率，这一思想可迁移至其他类似框架。</li>
<li>未来工作可在更复杂的真实世界任务中验证该方法，并探索进一步优化扩散模型生成效率以降低时间开销。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SD2AIL，旨在解决对抗性模仿学习中专家演示数据有限、收集困难的问题。方法核心是：1）利用扩散模型在判别器中生成合成演示，作为伪专家数据以增强真实演示；2）引入优先专家演示回放策略，从大量演示中筛选高价值样本进行训练。在Hopper仿真任务中，该方法取得了3441的平均回报，超越现有最优方法89分，证明了其有效性与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.18583" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>