<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Refined Policy Distillation: From VLA Generalists to RL Experts - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Refined Policy Distillation: From VLA Generalists to RL Experts</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.05833" target="_blank" rel="noreferrer">2503.05833</a></span>
        <span>作者: Jülg, Tobias, Burgard, Wolfram, Walter, Florian</span>
        <span>日期: 2025/03/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人学习领域存在两种主流范式：一是通过模仿学习训练的通用视觉-语言-动作模型，它们易于训练且泛化性好，但依赖大量人工演示数据，在新环境或任务变体上泛化能力有限，且模型庞大、推理较慢；二是强化学习方法，能够通过环境交互自主学习，产生紧凑高效的专家策略，但通常样本效率低、训练不稳定且对超参数敏感。本文针对的痛点是：如何将VLA模型中蕴含的通用任务知识高效地迁移到紧凑的RL策略中，并克服纯模仿学习性能天花板和纯RL探索效率低的问题。本文提出“精炼策略蒸馏”的新视角，核心思路是：将VLA作为教师策略，在RL学生策略的探索过程中提供动作指导，结合行为克隆与在线RL，实现从通用VLA到高效RL专家策略的蒸馏与性能精炼。</p>
<h2 id="方法详解">方法详解</h2>
<p>RPD方法的整体框架是一个标准的智能体-环境交互循环，但智能体的训练过程同时接收来自VLA教师策略的指导。其目标是蒸馏VLA教师策略中的知识，并通过RL交互精炼出性能超越教师的、紧凑的任务专家策略。</p>
<p><img src="https://arxiv.org/html/2503.05833v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RPD架构。左侧展示了智能体-环境的RL训练循环，右侧的VLA教师策略根据相同的观察（图像和语言指令）生成动作。这些VLA动作被用于指导RL学生策略的探索，并构成RPD损失函数的一部分。</p>
</blockquote>
<p>方法的核心是修改了近端策略优化算法的目标函数。RPD的损失函数结合了标准的PPO强化学习损失和一个行为克隆损失项：<br>$$\mathcal{L}<em>{\text{RPD}}(\theta) = \mathcal{L}</em>{\text{RL}}(\theta) - \mathcal{L}<em>{\text{MSE}}(\theta)$$<br>其中，$\mathcal{L}</em>{\text{RL}}$是PPO的裁剪替代目标函数，用于最大化累积奖励；$\mathcal{L}<em>{\text{MSE}}$是均方误差损失，用于最小化RL学生策略预测的动作均值与VLA教师策略动作期望之间的差异：<br>$$\mathcal{L}</em>{\text{MSE}}(\theta) = \mathbb{E}<em>{t}\left[\left(\mu(\pi</em>{\theta}(a_{t} \mid s_{t})) - \mathbb{E}\left[a^{\text{VLA}}<em>{t}\right]\right)^{2}\right]$$<br>在实践中，论文发现每一步仅采样一个VLA动作来估计其期望已能取得良好效果，这避免了为计算KL散度而进行大量采样带来的巨大计算开销（这是对比方法PPD的缺点）。该MSE损失项在假设动作分布为高斯分布且方差恒定的情况下，与最大化对数似然的行为克隆目标等价。通过最大化这个组合损失$\mathcal{L}</em>{\text{RPD}}$，RL策略既被鼓励探索高奖励区域，又被拉向VLA教师认为合理的动作区域，从而实现了引导探索和策略精炼。</p>
<p>与现有方法相比，RPD的创新点具体体现在：1）首次提出从大规模通用VLA模型中向在线RL策略进行蒸馏；2）采用简单的MSE损失，避免了计算密集型采样，实现了高效的知识迁移；3）最终获得的策略不仅紧凑、推理快，而且性能能够超越VLA教师，即实现了“策略精炼”。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台与数据集：所有实验在ManiSkill3仿真环境中进行，使用了其八个不同的机械臂操作任务，包括LiftPegUpright、PickCube、PullCube等。任务动作空间为7维相对任务空间运动指令。视觉观察使用ManiSkill3的“人类相机”视角（256x256 RGB图像）。由于预训练的Octo和OpenVLA在原始ManiSkill3任务上成功率几乎为0，论文首先使用ManiSkill3提供的RL专家演示数据对这两个VLA进行了微调，以获得具备基本能力的教师策略。其中PullCubeTool和PokeCube两个任务被排除在微调数据集外，用于测试跨任务泛化。</p>
<p>基线方法：对比了标准PPO、以及另一种策略蒸馏方法PPD。此外，论文还比较了RPD的不同损失变体：RPD-MSE（主方法）、RPD-L1（使用L1损失）和RPD-BC（使用最大似然损失）。</p>
<p>关键实验结果：</p>
<ol>
<li><strong>RPD变体与基线比较（PickCube任务）</strong>：在最具挑战性的PickCube任务上，RPD-MSE表现最佳，其蒸馏出的策略迅速超越VLA教师（Octo成功率0%→9%），最终达到约80%的成功率，且收敛速度明显快于标准PPO。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.05833v2/x2.png" alt="PickCube任务上的训练曲线"></p>
<blockquote>
<p><strong>图3</strong>：PickCube任务上各方法验证成功率的训练曲线。RPD-MSE（Octo）和RPD-L1收敛最快且稳定，性能显著超越VLA教师（虚线）。标准PPO虽能达到相近性能，但学习更慢、波动大。RPD-BC和PPD未能有效精炼策略。</p>
</blockquote>
<ol start="2">
<li><strong>多任务密集奖励评估</strong>：在六个包含在VLA微调数据集中的任务上，RPD-MSE均能成功蒸馏并精炼策略。在大多数任务中，RPD-MSE的最终性能与PPO相当或略优，但其样本效率（收敛速度）显著更高。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.05833v2/x3.png" alt="多任务密集奖励性能"></p>
<blockquote>
<p><strong>图4</strong>：六个任务上RPD-MSE与PPO基线的最终成功率和训练步数对比。RPD-MSE在多数任务上以更少的训练步数达到了与PPO相当的成功率，显示了更高的样本效率。</p>
</blockquote>
<ol start="3">
<li><strong>稀疏奖励设置下的性能</strong>：在稀疏奖励（仅任务成功时给予奖励）这一更具挑战性的设置下，RPD的引导探索优势更加明显。在PullCube和StackCube任务上，RPD-MSE能成功学习而PPO完全失败。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.05833v2/x4.png" alt="稀疏奖励设置下的性能"></p>
<blockquote>
<p><strong>图5</strong>：稀疏奖励设置下，RPD-MSE在PullCube和StackCube任务上能学习到成功策略，而PPO基线因探索困难完全失败，凸显了RPD在引导探索方面的价值。</p>
</blockquote>
<ol start="4">
<li><strong>泛化能力评估</strong>：<ul>
<li><strong>跨任务泛化</strong>：在未经过微调的任务PullCubeTool上，使用在PullCube任务上训练的RPD-MSE策略进行零样本测试，取得了约65%的成功率，而底层VLA教师在该任务上成功率仅为12%。</li>
<li><strong>跨视角泛化</strong>：将PushCube任务的相机视角从侧视改为俯视。在此情况下，微调后的VLA教师性能崩溃（成功率从67%降至0%），而RPD-MSE蒸馏出的策略仍能保持约55%的成功率。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2503.05833v2/figures/push_a.png" alt="跨视角泛化定性结果"></p>
<blockquote>
<p><strong>图6</strong>：原始侧视相机视角下的PushCube任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.05833v2/figures/push_b.png" alt="跨视角泛化定性结果"></p>
<blockquote>
<p><strong>图7</strong>：改为俯视相机视角后的PushCube任务。VLA在此视角下失效，而RPD策略仍能工作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.05833v2/x5.png" alt="泛化能力定量结果"></p>
<blockquote>
<p><strong>图8</strong>：跨任务（PullCubeTool）和跨视角（PushCube俯视）泛化的定量结果。RPD策略在VLA教师失效的情况下仍能保持较高的成功率。</p>
</blockquote>
<p>消融实验总结：图3所示的变体比较本身就是一种消融实验。结果表明：1）MSE损失（RPD-MSE）比L1损失（RPD-L1）略优，比最大似然损失（RPD-BC）好得多，因为RPD-BC会迫使策略方差缩小导致过拟合教师；2）与需要计算KL散度的PPD方法相比，RPD的MSE损失更简单、高效且稳定；3）RL组件（$\mathcal{L}_{\text{RL}}$）对于实现策略精炼（超越教师）至关重要，纯BC方法无法突破教师性能上限。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了Refined Policy Distillation这一新颖框架，首次实现了从大规模通用VLA到紧凑、高效RL专家策略的蒸馏与性能精炼；2）通过大量实验证明了RPD在样本效率、稳定性和对稀疏奖励的鲁棒性方面的优势，并展示了其出色的跨任务和跨视角泛化能力；3）为评估VLA在仿真环境中的表现提供了新的见解和基准。</p>
<p>论文自身提到的局限性包括：1）学生从教师指导中获益的程度依赖于教师策略本身在该任务上的性能；2）由于OpenVLA模型庞大且不支持批量推理，部分实验受到计算资源的限制。</p>
<p>对后续研究的启示：RPD成功地将大规模基础模型的知识与在线RL的优化能力相结合，为机器人策略学习提供了一条高效途径。未来的工作可以探索更复杂的损失函数设计、将方法扩展到离线RL设置，或者研究如何自动权衡RL目标与模仿目标之间的权重。此外，该方法也促使我们思考如何更好地将仿真中训练的精炼策略迁移到真实世界。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型（VLA）泛化能力强但成功率不及专家策略、且需频繁微调，以及强化学习（RL）策略紧凑但样本效率低、收敛慢的问题，提出精炼策略蒸馏（RPD）方法。该方法结合on-policy RL与行为克隆，利用VLA教师动作指导RL学生探索，将VLA蒸馏为紧凑高性能的专家策略。实验表明，RPD学习的策略在密集和稀疏奖励设置下均优于VLA教师，且比RL基线收敛更快，对相机视角变化鲁棒，能泛化到基础VLA无法解决的任务变体。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.05833" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>