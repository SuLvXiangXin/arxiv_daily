<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.15428" target="_blank" rel="noreferrer">2507.15428</a></span>
        <span>作者: Li, Jiaao, Li, Kaiyuan, Gao, Chen, Li, Yong, Chen, Xinlei</span>
        <span>日期: 2025/07/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大视觉语言模型在视频理解方面展现出强大潜力，但其计算成本高昂，尤其对于长视频输入。现有方法主要通过token剪枝来提升效率，主要分为两类：针对时间冗余的方法（如基于跨帧余弦相似度去除冗余token或基于查询的关键帧选择）和针对空间冗余的方法（如基于注意力分数或帧内多样性选择重要token）。然而，这些方法主要针对第三人称视频设计，未能有效利用自我运动视频特有的时空连续性和运动约束。</p>
<p>自我运动视频以第一人称视角录制，相机随智能体移动而连续变化，是具身AI的核心视觉输入。与第三人称视频相比，自我运动视频具有更强的时空连续性，这为结构化、几何感知的剪枝提供了机会，但也带来了挑战。现有方法在自我运动场景下存在明显局限：基于固定位置余弦相似度的方法在连续视角变化下因token对齐失效而失败；关键帧选择可能丢弃关键的过渡信息；基于注意力的方法受位置偏差影响、与FlashAttention等优化实现不兼容，且在需要空间完整性的感知密集型任务上表现不佳；基于多样性的方法虽能保留代表性视觉信息，但与任务无关，可能忽略回答用户查询所必需的关键token。</p>
<p>本文针对自我运动视频推理中现有token剪枝方法失效的痛点，提出了一个利用几何连续性的新视角。核心思路是：首先通过透视变换对齐连续帧中的token以准确识别并过滤几何冗余，然后结合任务相关性和视觉多样性，选择信息量最大且紧凑的token子集，从而实现高效、无需训练的自运动视频推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>EgoPrune是一个无需训练的token剪枝方法，专为自我运动视频推理设计。其整体流程建立在EmbodiedR的关键帧选择之上，随后进行两阶段的token压缩。输入为原始自我运动视频帧序列和用户文本提示，输出为经过剪枝的、紧凑的视觉token序列，与文本token拼接后送入大语言模型生成回答。</p>
<p><img src="https://arxiv.org/html/2507.15428v1/extracted/6639776/AnonymousSubmission/LaTeX/figs/method_overview.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：EgoPrune方法总览。基于EmbodiedR的重叠感知关键帧选择，EgoPrune应用两阶段token压缩流水线。阶段1（PARF）利用透视变换对齐连续帧以剪枝冗余token。阶段2利用最大边际相关性选择既与输入文本相关又具有视觉语义多样性的新token子集。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>关键帧选择器</strong>：直接采用EmbodiedR的方法，基于透视变换估计相邻帧的视觉重叠度，当重叠度低于预设阈值时选择新关键帧，实现时间上的高效采样。</li>
<li><strong>透视感知冗余过滤</strong>：这是针对自我运动视频空间冗余的核心创新模块。经过关键帧选择后，相邻帧通常保留50%-60%的视觉重叠。PARF通过估计单应性矩阵H，将前一帧透视变换（warp）到与当前帧对齐。<br><img src="https://arxiv.org/html/2507.15428v1/extracted/6639776/AnonymousSubmission/LaTeX/figs/warpperspective.png" alt="透视变换对齐"><blockquote>
<p><strong>图3</strong>：通过估计连续帧之间的单应性矩阵，将前一帧扭曲以与当前帧对齐。这种对齐建立了token之间的空间对应关系，从而更容易通过余弦相似度识别冗余token。<br>具体实现使用ORB关键点与描述子进行特征匹配，结合FLANN和Lowe比率测试筛选匹配点，再通过RANSAC鲁棒地估计单应性矩阵H。对齐后，计算对应位置token的余弦相似度，将相似度高于75%的token视为冗余并丢弃。该方法复杂度低，主要开销在于计算H（O(K log K)）和token相似度比较（O(Nd)）。</p>
</blockquote>
</li>
<li><strong>基于最大边际相关性的token选择器</strong>：该模块旨在从经过PARF初步筛选的视觉token序列V和用户提示token Q中，选择最终保留的token。它平衡了<strong>任务相关性</strong>和<strong>视觉多样性</strong>。首先计算每个视觉token vi与平均用户提示表示q_avg的余弦相似度作为相关性分数rel(vi)。然后，MMR算法迭代地选择k个token，每一步选择能最大化边际相关性MR_i的token：MR_i = λ * rel(vi) - (1 - λ) * max_{vj∈S} sim(vi, vj)，其中S是已选token集合，λ是平衡参数。λ=1时完全偏向相关性，λ=0时完全偏向多样性。<br><img src="https://arxiv.org/html/2507.15428v1/extracted/6639776/AnonymousSubmission/LaTeX/figs/mmr_visual.jpg" alt="MMR选择器可视化"><blockquote>
<p><strong>图4</strong>：不同λ设置下MMR Token选择器的可视化。较大的λ强调与提示的相关性，较小的λ偏好空间多样性。λ=0.5取得了更好的平衡。</p>
</blockquote>
</li>
</ol>
<p>与现有方法相比，EgoPrune的创新点具体体现在：1) <strong>问题针对性</strong>：首个专门为自我运动视频设计的无需训练token剪枝方法。2) <strong>几何感知</strong>：利用透视变换对齐帧，解决了因视角移动导致的token错位问题，使冗余判断更准确。3) <strong>任务感知与多样性平衡</strong>：通过MMR同时考虑token与用户提示的相关性以及token集合内部的视觉多样性，避免了纯多样性方法可能遗漏关键语义信息的问题。4) <strong>部署友好</strong>：不依赖注意力分数，完全兼容FlashAttention等高效注意力实现，适合边缘部署。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了两个自我运动视频推理基准数据集：<strong>Ego4D-AR</strong>（关联回忆任务）和<strong>EgoVQA</strong>（视频问答任务）。实验平台包括服务器（用于准确率评估）和<strong>Jetson Orin NX 16GB</strong>边缘设备（用于实际部署验证）。基础模型为<strong>VILA-7B</strong>。</p>
<p>对比的基线方法包括：<strong>Uniform Sampling</strong>（均匀采样）、<strong>TimeChat-Online</strong>（基于余弦相似度的跨帧冗余去除）、<strong>DyCoke</strong>（时空token合并与剪枝）、<strong>DivPrune</strong>（基于最大-最小多样性的token选择）、<strong>CDPruner</strong>（基于聚类的多样性选择）以及<strong>PACT</strong>（早期聚类与重要性剪枝）。</p>
<p>关键实验结果如下：</p>
<ul>
<li><strong>性能保持</strong>：在Ego4D-AR和EgoVQA数据集上，在不同剪枝率（保留10%， 20%， 30% token）下，EgoPrune consistently outperforms prior training-free methods across various pruning ratios while significantly reducing FLOPs, memory usage, and latency. 在EgoVQA上保留20% token时，准确率相比原始全token输入仅下降0.4%，显著优于其他基线。</li>
<li><strong>效率提升</strong>：<br><img src="https://arxiv.org/html/2507.15428v1/extracted/6639776/AnonymousSubmission/LaTeX/figs/compare_latency.png" alt="延迟对比"><blockquote>
<p><strong>图5</strong>：在Jetson Orin NX上，不同剪枝方法在不同批次大小下的每帧推理延迟对比。EgoPrune（紫色）在几乎所有设置下都实现了最低的延迟。<br>论文指出，EgoPrune在批处理大小为1时，相比原始模型（无剪枝）实现了<strong>2.5倍的加速</strong>，并显著降低了内存占用和FLOPs。</p>
</blockquote>
</li>
<li><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2507.15428v1/extracted/6639776/AnonymousSubmission/LaTeX/figs/lambda_ablation.png" alt="λ消融实验"><blockquote>
<p><strong>图6</strong>：在EgoVQA和Ego4D-AR上对平衡参数λ的消融研究。λ=0.5在多数设置下取得了最佳或接近最佳的性能，验证了同时考虑相关性和多样性的必要性。<br>消融实验总结了各组件贡献：1) <strong>关键帧选择</strong>是高效时间采样的基础。2) <strong>PARF模块</strong>有效去除了几何冗余token，是性能保持的关键。3) <strong>MMR选择器</strong>中，平衡参数λ=0.5时效果最好，证实了同时兼顾任务相关性和视觉多样性的策略优于单一策略。</p>
</blockquote>
</li>
<li><strong>实际部署验证</strong>：<br><img src="https://arxiv.org/html/2507.15428v1/extracted/6639776/AnonymousSubmission/LaTeX/figs/on_device_frame.png" alt="设备部署帧率"><blockquote>
<p><strong>图7</strong>：在Jetson Orin NX上部署的具身智能体实时处理自我运动视频流。EgoPrune实现了显著的帧率提升。<br><img src="https://arxiv.org/html/2507.15428v1/extracted/6639776/AnonymousSubmission/LaTeX/figs/on_device_rate.png" alt="设备部署速率对比"><br><strong>图8</strong>：在边缘设备上，EgoPrune相比原始模型和其他剪枝方法，在吞吐量（帧/秒）方面有大幅提升，验证了其现实世界效率。<br>部署实验表明，EgoPrune能在Jetson Orin NX上实现实时推理，适用于无人机导航或移动机器人等设备端具身应用。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li>提出了<strong>EgoPrune</strong>，这是首个专门为自我运动视频推理设计的、无需训练的token剪枝方法，有效解决了现有方法因视角连续变化而失效的问题。</li>
<li>设计了一个结合<strong>几何对齐</strong>（PARF）和<strong>任务感知多样性选择</strong>（MMR）的两阶段剪枝流水线，在显著提升推理效率的同时，保持了极高的任务准确率。</li>
<li>在真实的边缘计算设备（Jetson Orin NX）上成功部署并验证了其效率，证明了其在现实世界具身AI应用中的可行性与实用性。</li>
</ol>
<p>论文自身提到的局限性在于：透视变换（单应性）假设场景大致是平面的，或相机运动主要是旋转。在存在显著视差或复杂3D运动的情况下，该假设可能不成立，从而影响PARF模块的对齐精度。</p>
<p>对后续研究的启示包括：1) 可以探索更复杂的运动模型（如基础矩阵估计）来处理非平面场景下的token对齐。2) 将几何感知剪枝的思想与其他高效推理技术（如动态计算、模型蒸馏）相结合，以追求极致的端侧性能。3) 研究如何将类似的方法泛化到其他具有强时空结构的视频类型（如手术视频、驾驶视频）中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身智能体处理自我运动视频时计算成本高的问题，提出无需训练的EgoPrune方法。其核心技术包括：基于EmbodiedR的关键帧选择器、利用视角变换对齐并过滤冗余视觉token的PARF模块，以及综合考虑视觉-文本相关性和帧内多样性的MMR选择器。实验表明，EgoPrune在多种剪枝比例下均优于现有方法，显著降低了计算量（FLOPs）、内存占用和延迟，并在Jetson Orin NX边缘设备上验证了其实时推理效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.15428" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>