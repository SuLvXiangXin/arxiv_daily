<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.06512" target="_blank" rel="noreferrer">2602.06512</a></span>
        <span>作者: Heng Tao Shen Team</span>
        <span>日期: 2026-02-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人领域的主流方法是大规模模仿学习，特别是通用型机器人策略，它们通过在海量、多样化的人类演示数据集上进行训练，旨在让机器人能够根据自然语言指令执行广泛的操控任务。然而，这些模型的现实应用受到一个关键且常被忽视的挑战的阻碍：演示数据天然存在的长尾分布。在大规模机器人数据集中，少数常见的头部任务（如“拿起碗/盘子”）拥有大量演示，而绝大多数尾部任务（如“把酒瓶放在架子上”）仅有极少量的演示示例。在这种不平衡数据上训练的策略，在尾部任务上表现往往不可靠。本文旨在解决这一痛点，揭示了传统长尾学习策略（如重采样）在策略学习中的无效性，并发现其根本原因在于数据稀缺直接损害了策略的空间推理能力。为此，本文提出了“接近阶段增强”（Approaching-Phase Augmentation, APA）方案，其核心思路是通过从数据丰富的头部任务中转移知识，为数据稀缺的尾部任务生成高质量的训练示例，从而提升策略在尾部任务上的空间推理能力，且无需外部演示数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文首先构建了一个基于LIBERO环境的长尾模仿学习基准，包含10个多样化操控任务。通过深入分析，发现策略在尾部任务上失败的主要原因是其空间推理能力退化。由于训练数据稀缺，模型无法学习尾部任务所需的精确空间关系，尽管它已从头部任务中掌握了基本概念。</p>
<p>基于这一关键洞察，本文提出了Approaching-Phase Augmentation (APA)方法。该方法的核心思想是利用头部任务的演示，为尾部任务合成新的、高质量的“接近阶段”轨迹，从而增强模型对尾部任务目标物体空间位置的理解和接近能力。</p>
<p><img src="https://arxiv.org/html/2602.06512v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：Approaching-Phase Augmentation (APA) 方法概览。该方法包含两个阶段：1）<strong>轨迹对齐</strong>：将头部任务（源）和尾部任务（目标）的演示轨迹在目标物体（如酒瓶）的坐标系中对齐；2）<strong>轨迹合成</strong>：将头部任务轨迹的“接近阶段”（即机械臂接近目标物体的部分）与尾部任务轨迹的“执行阶段”（即抓取和放置物体的部分）拼接，生成用于尾部任务训练的新合成轨迹。</p>
</blockquote>
<p>APA方法的具体流程如下：</p>
<ol>
<li><strong>轨迹对齐</strong>：对于每一对头部任务（源）和尾部任务（目标）的演示，首先识别各自轨迹中的目标物体。然后，计算一个刚性变换（旋转和平移），将源轨迹中目标物体的坐标系与目标轨迹中目标物体的坐标系对齐。这使得来自不同任务的轨迹可以在一个共同的空间参考系中进行比较和操作。</li>
<li><strong>轨迹合成</strong>：将对齐后的源轨迹和目标轨迹分别划分为两个阶段：“接近阶段”（<code>τ_appr</code>）和“执行阶段”（<code>τ_exec</code>）。接近阶段指机械臂末端执行器从初始位置移动到即将与目标物体交互（如抓取）之前的子轨迹；执行阶段则包含后续的抓取、移动和放置等动作。APA的核心操作是将对齐后源轨迹的接近阶段（<code>τ_appr^src</code>）与目标轨迹的执行阶段（<code>τ_exec^tgt</code>）拼接起来，形成一条新的合成轨迹 <code>τ_syn = (τ_appr^src; τ_exec^tgt)</code>。这条合成轨迹被用作尾部任务的额外训练数据。</li>
<li><strong>训练</strong>：使用原始的尾部任务演示数据以及通过APA生成的合成数据共同训练策略模型。这相当于为尾部任务增加了数据多样性，特别是丰富了接近目标物体方式的样本，从而直接增强了模型在数据稀缺任务上的空间推理和规划能力。</li>
</ol>
<p>与现有方法相比，APA的创新点在于：</p>
<ul>
<li><strong>针对性</strong>：直接针对已识别的根本问题——空间推理能力不足，通过合成“接近阶段”数据进行增强。</li>
<li><strong>自给自足</strong>：仅利用训练集中已有的头部和尾部任务演示，无需任何外部数据源。</li>
<li><strong>物理合理性</strong>：通过基于坐标系的轨迹对齐和阶段拼接，生成的合成轨迹在运动学上比简单的数据插值（如mixup）更可能保持合理性和有效性。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准数据集</strong>：基于LIBERO环境构建了LIBERO-Core-FULL（平衡数据集）和LIBERO-Core-LT（长尾数据集）。长尾数据集包含10个任务，头部任务（前30%）有46-19条演示，尾部任务（后70%）仅有15-5条演示。</li>
<li><strong>实验平台</strong>：在模拟环境（LIBERO）和真实机器人（Franka Emika Panda机械臂）上进行评估。</li>
<li><strong>基线方法</strong>：比较了标准训练（原始分布）、多种重采样策略（q=0.75, 0.5, 0.25）、以及数据增强方法Mixup。</li>
<li><strong>评估指标</strong>：任务成功率（%），通过对多次 rollout 取平均得到。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.06512v1/x2.png" alt="训练数据分布对比"></p>
<blockquote>
<p><strong>图2</strong>：LIBERO-Core-FULL（蓝色）与LIBERO-Core-LT（黄色）数据集的训练演示数量分布对比。清晰展示了长尾分布特性。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>传统方法无效性验证</strong>：如表II所示，在LIBERO-Core-LT上应用不同强度的重采样策略（q=0.75, 0.5, 0.25），相比基线（26.5%成功率）仅带来微小波动甚至下降，最高仅达到27.1%，证明单纯增加尾部数据曝光量无效。Mixup方法甚至导致性能大幅下降至18.3%。</p>
</li>
<li><p><strong>失败模式诊断</strong>：如表III所示，作者将任务失败分解为“接近阶段失败”（<code>p_appr</code>，未能正确接近目标）和“执行阶段失败”（<code>p_exec</code>，接近后操作失败）。分析发现，在长尾数据集上，尾部任务的接近阶段失败概率（<code>p_appr</code>）显著高于在平衡数据集上的情况，而执行阶段失败概率变化不大。这直接证实了数据稀缺主要损害了空间推理（接近）能力。</p>
</li>
<li><p><strong>APA方法有效性</strong>：</p>
<ul>
<li><p><strong>模拟实验</strong>：在LIBERO-Core-LT的10个任务上，APA将平均成功率从基线的26.5%提升至40.7%，相对提升超过53%。特别是在最困难的尾部任务（Task 10）上，成功率从0%提升至20%。<br><img src="https://arxiv.org/html/2602.06512v1/x4.png" alt="模拟实验结果"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO-Core-LT数据集上，不同方法在各任务上的成功率。APA（橙色）在绝大多数任务上，尤其是尾部任务（Task 4-10），显著优于基线（蓝色）和重采样方法（绿色）。</p>
</blockquote>
</li>
<li><p><strong>真实世界实验</strong>：在6个真实机器人操控任务上，APA将平均成功率从16.7%提升至38.9%。<br><img src="https://arxiv.org/html/2602.06512v1/x5.png" alt="真实世界实验结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界6个任务上的成功率。APA consistently outperforms the baseline across all tasks.</p>
</blockquote>
</li>
<li><p><strong>消融实验</strong>：验证了APA各组件的重要性。移除轨迹对齐（“w/o alignment”）或使用错误的拼接阶段（如拼接执行阶段，即“w/ <code>τ_exec</code> of head”）都会导致性能显著下降，甚至低于基线，证明了轨迹对齐和选择“接近阶段”进行拼接的关键作用。<br><img src="https://arxiv.org/html/2602.06512v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：APA的消融研究结果。完整的APA方法性能最佳，移除对齐或使用错误的源任务阶段都会降低效果，证实了方法设计的有效性。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li><strong>问题诊断</strong>：构建了机器人长尾模仿学习基准，并通过细粒度分析首次明确指出，长尾分布下策略在尾部任务上的性能下降主要源于数据稀缺导致的<strong>空间推理能力退化</strong>，具体表现为“接近阶段”的高失败率。</li>
<li><strong>方法创新</strong>：提出了**Approaching-Phase Augmentation (APA)**，一种简单、自给自足的数据增强方案。它通过几何对齐和阶段拼接，将头部任务中丰富的“接近”模式知识迁移到尾部任务，有效提升了策略的空间推理能力。</li>
<li><strong>实验验证</strong>：在模拟和真实世界的多种操控任务上进行了广泛实验，证实了APA能显著提升长尾分布下的策略性能，特别是在尾部任务上。</li>
</ol>
<p>论文提到的局限性包括：APA目前主要处理单物体抓取和放置任务，其有效性在涉及多物体交互或更复杂动态的任务中仍有待验证。</p>
<p>对后续研究的启示：</p>
<ul>
<li>本文强调了在机器人学习中对失败模式进行细粒度分析的重要性，而非仅仅关注整体成功率。</li>
<li>APA的成功表明，针对特定能力短板（如空间推理）进行<strong>定向知识迁移</strong>是一种解决数据不平衡问题的有效途径。</li>
<li>该方法无需外部数据，计算开销低，为在实际机器人系统中缓解长尾问题提供了一个实用且高效的解决方案。未来的工作可以探索将类似原理应用于其他类型的技能缺陷或更复杂的任务结构中。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习中因训练数据呈长尾分布（少数头部任务数据丰富，大量尾部任务数据稀缺）导致机器人操作策略在尾部任务上泛化性能差的核心问题，提出了一种名为“接近阶段增强”（APA）的方法。该方法通过从数据丰富的头部任务中迁移知识来增强对尾部任务的空间推理能力，无需额外演示数据。实验表明，APA能有效提升策略在尾部任务上的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.06512" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>