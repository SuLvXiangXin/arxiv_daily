<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.16302" target="_blank" rel="noreferrer">2512.16302</a></span>
        <span>作者: Yang Gao Team</span>
        <span>日期: 2025-12-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前的一次性模仿学习（OSIL）方法为机器人技能学习提供了无需大规模数据收集的途径，但主要局限于短视野任务，或要求新任务与训练任务高度相似，或依赖已知的3D物体模型。这些限制阻碍了其在复杂、多阶段的长视野操作场景中的应用。本文针对长视野一次性模仿学习这一具体痛点，提出了一种新视角：围绕物理交互事件来结构化长视野任务，将其重新定义为对交互感知基元的排序，而非直接模仿连续轨迹。本文的核心思路是：通过交互感知的任务分解、不变区域预测与匹配，实现长视野抓取操作任务的有效一次性模仿。</p>
<h2 id="方法详解">方法详解</h2>
<p>ManiLong-Shot框架旨在通过一次性模仿学习执行未见过的长视野操作任务。其整体流程是：给定一个长视野任务的成功演示，首先将其分解为一系列交互感知基元；对于每个基元，预测其关键的不变交互区域；在执行时，将演示中的预测区域与当前观察场景进行匹配，据此计算目标末端执行器位姿并规划运动；迭代此过程直至任务完成。</p>
<p><img src="https://arxiv.org/html/2512.16302v1/x2.png" alt="方法整体训练流程"></p>
<blockquote>
<p><strong>图2</strong>：ManiLong-Shot的整体训练流程。左侧展示了交互感知区域匹配网络的训练与推理过程，用于对齐演示与当前场景的不变区域并预测位姿。右侧展示了交互感知区域预测网络的训练过程，用于从短视野演示中学习预测每个交互阶段的不变区域。</p>
</blockquote>
<p>框架包含三个核心模块：</p>
<ol>
<li><strong>交互感知任务分解</strong>：该模块将演示轨迹组织成基于物理交互阶段的基元序列。受人类策略启发并借鉴现有工作，机器人-物体交互被分为三个阶段：预接触（pre-contact，夹爪张开接近物体）、抓取（grasping，夹爪闭合抓握物体）和后接触（post-contact，夹爪张开放置物体）。短视野任务通常只包含这三个阶段的一个循环，而长视野任务则包含多个循环。分解可通过两种可互换的方式实现：一是基于规则的启发式方法，通过分析关节速度和夹爪状态变化来推断阶段边界；二是利用视觉语言模型（VLM，如GPT-4o），通过定制的提示词和轨迹的结构化表示来自动识别交互阶段。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.16302v1/x3.png" alt="交互阶段可视化"></p>
<blockquote>
<p><strong>图3</strong>：三个物理交互阶段的可视化。以真实机器人执行“将杯子放入杯架”任务为例，展示了预接触、抓取和后接触阶段，及其与人类操作认知理解的对应关系。</p>
</blockquote>
<ol start="2">
<li><p><strong>交互感知区域预测网络</strong>：在从演示中提取出交互感知子任务基元后，该网络负责预测跨交互阶段的功能性交互不变区域。不变区域被定义为在具有相同最优策略的状态下，对SE(3)变换保持结构等变的3D几何子集，其语义和功能在不同任务场景中保持稳定。网络使用仅来自短视野任务的成功演示进行训练。对于每个任务，一个演示被分割为三个阶段。网络以连续状态对的RGB-D观测生成的点云为输入，使用Point Cloud Transformer V3进行处理，通过跨场景交叉注意力和场景内自注意力增强特征。网络输出一个交互概率分布，以激活相关的不变区域。训练采用监督学习，使用仿真中提供的真实实例掩码进行监督。</p>
</li>
<li><p><strong>交互感知区域匹配网络</strong>：该模块使机器人能够将演示中预测的不变区域匹配到当前执行状态，以在不进行额外训练的情况下复现动作。网络在多个短视野任务演示上训练，无需显式的任务分解。对于当前执行状态，一个状态路由网络会从演示轨迹中选择最相似的状态。然后将演示状态的不变区域点云与当前执行状态的全场景点云融合，经由PTV3下采样和定位网络（采用交叉-自-交叉注意力模块）进行特征对齐。基于对齐后的特征，通过双重softmax匹配算法计算两个点云之间的对应矩阵。最后，利用基于对应的位姿回归算法，结合演示中的动作位姿，估算出当前状态下机器人末端执行器的目标位姿。确定目标位姿后，使用RRT-Connect算法进行运动规划，执行后获得新的观察，并迭代进行下一轮匹配。</p>
</li>
</ol>
<p>与现有方法相比，ManiLong-Shot的创新点在于：1）将长视野任务分解为以物理交互事件为边界的基元，而非直接处理长轨迹或依赖预定义基元库；2）引入了分阶段的、交互感知的不变区域预测与匹配机制，专门针对抓取操作中的不同交互阶段（预接触/抓取、后接触）进行建模，提升了长视野任务中动作转移的鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了基于RLBench构建的专用基准<strong>RLBench-Oneshot</strong>，包含10个短视野训练任务和20个未见过的长视野测试任务。长视野任务根据交互次数分为三个难度等级。对比的基线方法包括：模仿学习领域的SOTA多任务模型RVT2、3DDA、ARP，以及OSIL领域的领先方法IMOP。对于长视野任务，部分基线使用了少量演示进行微调（标记为+FT）。评估指标为平均成功率和平均排名。</p>
<p><img src="https://arxiv.org/html/2512.16302v1/x4.png" alt="长视野任务可视化"></p>
<blockquote>
<p><strong>图4</strong>：RLBench-Oneshot中20个长视野操作任务的可视化，包括3个难度等级。等级1（13个任务）包含6次物理交互，等级2（4个任务）包含9次交互，等级3（3个任务）包含12次交互。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>在<strong>短视野训练任务</strong>上，ManiLong-Shot（采用基于规则的分解）取得了最佳性能，平均成功率达到**90.4%**，比最佳基线3DDA（85.0%）相对提升3.8%，并在10个任务中的6个上取得了最高成功率。</p>
<p>在<strong>未见过的长视野任务</strong>上，ManiLong-Shot展示了卓越的一次性模仿学习能力。如表2所示，其平均成功率达到<strong>30.2%<strong>，显著优于所有基线。相比表现最好的基线IMOP（7.4%），取得了</strong>22.8%</strong> 的相对提升。即使在需要微调的基线（RVT2+FT， 4.1%）面前，优势也非常明显。ManiLong-Shot在20个任务中的19个上排名第一。</p>
<p>消融实验分析了各组件贡献：1）<strong>任务分解策略</strong>：基于规则的分解在大多数任务上表现优于VLM分解，但VLM在需要高级语义理解的任务（如“整理绳子”）上更具优势。2）<strong>模块有效性</strong>：移除任务分解（直接使用原始长轨迹）或不变区域预测与匹配模块，性能均出现显著下降，验证了各模块的必要性。</p>
<p><img src="https://arxiv.org/html/2512.16302v1/figs/real.png" alt="真实机器人实验结果"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人实验设置及三个长视野任务（堆叠积木、堆叠杯子、放置杯子）的成功执行过程可视化。模型经过仿真预训练后，通过一次性模仿成功泛化到物体位置略有扰动的真实场景。</p>
</blockquote>
<p><strong>真实机器人实验</strong>在UFatory xArm7上验证了框架的实用性。针对三个长视野任务，在物体初始位置扰动的情况下进行一次性模仿，平均成功率达到了**86.7%**，证明了其良好的从仿真到现实的迁移能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了 <strong>ManiLong-Shot</strong>，一个专门为长视野抓取操作任务设计的新型OSIL框架，通过将任务结构化为序列化的交互基元，实现了有效的端到端一次性模仿执行。2）引入了一种交互感知的任务分解机制，以及一个两阶段的不变区域预测与匹配流程，确保了长视野任务执行的鲁棒性。3）构建了RLBench-Oneshot基准，并系统性地验证了方法在仿真和真实环境中的有效性。</p>
<p>论文自身提到的局限性在于其框架主要关注<strong>抓取操作任务</strong>，基于夹爪与环境的物理交互阶段进行分解，因此不处理纯粹的非抓取行为。</p>
<p>本工作对后续研究的启示包括：1）交互感知的任务分解提供了一种可解释且鲁棒的长任务处理范式，可尝试将其扩展到更广泛的操作类型。2）结合基于规则的稳定性和VLM的语义理解能力，可能催生更灵活、更通用的任务理解与分解方法。3）不变区域的概念及其在长视野任务中的分阶段预测与匹配，为机器人学习可泛化的技能表示提供了新思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决当前一次性模仿学习（OSIL）方法难以处理复杂长时程操作任务的问题。提出了ManiLong-Shot框架，其核心是将长时程任务分解为基于物理交互事件的基元序列，而非直接模仿连续轨迹。该方法利用视觉语言模型或基于状态变化的启发式规则驱动分解，对每个基元预测关键的交互不变区域、建立演示与当前场景的对应关系，并计算目标位姿。实验表明，仅在10个短时程任务上训练的模型，可通过单次演示泛化到20个未见长时程任务，相对现有最佳方法性能提升22.8%，并在真实机器人上得到验证。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.16302" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>