<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.03556" target="_blank" rel="noreferrer">2512.03556</a></span>
        <span>作者: Yong Li Team</span>
        <span>日期: 2025-12-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，机器人策略学习的主流范式包括模仿学习（IL）和强化学习（RL）。模仿学习依赖于专家数据或手工设计的目标，容易过拟合到特定的训练场景和专家轨迹。强化学习虽然通过奖励信号鼓励更广泛的探索，但其泛化能力仍受限于难以设计一个适用于高度异构环境的通用且一致的奖励函数。世界模型作为一种能够预测状态转移动态的预测模型，被视为一种通用的环境代理，有望解决这一瓶颈。然而，现有的世界模型主要专注于预测未来观测，仍然依赖于任务特定、手工设计的奖励函数，无法提供一个真正通用的训练环境。本文针对“缺乏一个能提供通用奖励信号的真正通用训练环境”这一痛点，提出利用世界模型作为RL范式中的通用环境代理。核心思路是提出RoboScape-R框架，利用世界模型作为在线训练环境，并设计一种源自模型内在状态转移动态理解的“内生”通用奖励机制，以大幅提升具身策略的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboScape-R的整体框架主要包括两部分：一个作为环境的世界模型，以及可扩展的策略。世界模型接收策略预测的动作，提供下一帧观测和相应的奖励；策略接收观测并预测动作。</p>
<p><img src="https://arxiv.org/html/2512.03556v1/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图1</strong>：RoboScape-R的整体流程结构。左侧是基于世界模型的环境与通用奖励模块，右侧是策略优化模块。世界模型环境采用双世界模型结构：动作世界模型接收动作并提供预测观测，而文本世界模型则通过生成目标观测来提供奖励信号。</p>
</blockquote>
<p><strong>核心模块1：具有通用奖励的世界模型架构</strong><br>为了在RL训练中同时提供观测和奖励，本文提出了一个包含动作世界模型（WM_act）和文本世界模型（WM_txt）的双世界模型环境。两者共享类似的自回归Transformer架构但参数不同，均能基于历史观测和控制信号预测未来观测和完成（done）信号。前向过程为：(\hat{x_t}, \hat{d_t} = WM(x_{0:t-1}, c))，其中(c)对于动作世界模型是机器人动作序列(a_{0:t})，对于文本世界模型是文本指令(i)。</p>
<p>具体架构包括：</p>
<ol>
<li><strong>视觉分词器</strong>：采用MAGVIT-2将观测视频压缩为离散的潜在token。</li>
<li><strong>控制信号编码器</strong>：文本世界模型使用T5编码器编码指令；动作世界模型使用MLP编码7-DoF机器人动作。</li>
<li><strong>条件时空块（STBlock）</strong>：在基础时空块中引入交叉注意力机制以注入控制信号，用于预测下一个token和完成信号。文本和动作世界模型分别对应STBlock-T和STBlock-A。</li>
<li><strong>视觉解码器</strong>：将预测的token解码为观测图像。</li>
<li><strong>完成信号预测头</strong>：一个MLP，用于预测当前时间步的完成信号(\hat{d_t})。</li>
</ol>
<p>模型优化采用混合损失函数：视觉token预测的交叉熵损失(\mathcal{L}_{vis})和完成信号预测的RMSE损失(\mathcal{L}<em>d)，总损失为(\mathcal{L} = \mathcal{L}</em>{vis} + \mathcal{L}_d)。</p>
<p><strong>核心模块2：基于世界模型的通用奖励</strong><br>为了稳定地训练泛化策略，本文提出了一种内生的、无监督的通用奖励，而非直接拟合手工设计的奖励函数。该奖励由稠密奖励(\mathcal{R}<em>{den})和稀疏奖励(\mathcal{R}</em>{sps})组成。</p>
<ul>
<li><strong>稠密奖励</strong>：计算当前观测(x_t)与目标观测(x_{goal})之间的LPIPS相似度，即(\mathcal{R}<em>{den} = \text{LPIPS}(x_t, x</em>{goal}))。目标观测(x_{goal})提供了任务完成时最终状态的先验，它由文本世界模型基于环境初始观测和文本指令自回归生成。当预测的完成信号(\hat{d_t^{txt}})首次超过阈值(\theta)时，取该帧作为目标观测。</li>
<li><strong>稀疏奖励</strong>：由动作世界模型预测的完成信号(\hat{d^{act}<em>{t}})决定，当该信号超过阈值(\theta</em>{sps})时给予奖励1，否则为0，即(\mathcal{R}<em>{sps}=1 \text{ if } d^{act}</em>{t} \geq \theta_{sps} \text{ else } 0)。</li>
</ul>
<p>完整奖励为：(\mathcal{R} = \mathcal{R}<em>{sps} + \mathcal{R}</em>{den})。</p>
<p><strong>核心模块3：基于世界模型的RL训练流程</strong><br>训练流程主要包含三部分（如算法1所示）：</p>
<ol>
<li><strong>可泛化环境初始化</strong>：利用世界模型从海量场景中学到的动态，可以同时初始化多个不同的环境。同时，使用文本世界模型为每个环境生成目标观测(x_{goal})。</li>
<li><strong>环境中的策略推演</strong>：策略(\pi_\Theta)输出动作序列，动作世界模型据此生成下一观测(x_t)，并根据公式8计算当前时间步的奖励。</li>
<li><strong>策略优化</strong>：这是一个通用灵活的范式，支持符合观测、奖励、完成信号通用接口的各种RL优化算法和策略。策略参数更新遵循公式：(\Theta \leftarrow \Theta + \gamma \cdot \nabla_\Theta J(\Theta; D(X, R)))。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，RoboScape-R的核心创新在于：1）提出了一个源自世界模型本身的内生通用奖励机制，而非依赖外部的、手工设计的奖励函数或奖励模型；2）开创性地将世界模型集成为一个提供所有必要环境信号（观测、奖励、完成）的在线RL训练环境，而非仅作为数据生成器或离线评估器。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集与平台</strong>：使用ManiSkill物理仿真器收集数据，包含80个场景，4个任务（抓放、推、拉、移动到目标），超过30万条轨迹（包含最优和次优轨迹）。使用80%场景训练世界模型，20%未见场景用于OOD评估。</li>
<li><strong>基线方法</strong>：与三种奖励机制对比：1) <strong>基于规则的奖励</strong>（物理仿真器中使用）；2) <strong>嵌入奖励</strong>（iVideoGPT，从隐藏嵌入直接拟合手工奖励标签）；3) <strong>代理奖励</strong>（DiWA，使用外部MLP模型提供奖励）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能</strong>：如表1所示，在MLP和OpenVLA两种策略上，使用世界模型进行RL训练，在领域内（IND）场景下性能与在物理仿真器（ManiSkill）中训练相当甚至略优；在领域外（OOD）场景下，性能显著优于基线，平均提升达37.5%。这表明世界模型能有效作为RL训练环境，并大幅提升策略泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.03556v1/x2.png" alt="不同奖励模块的成功率对比"></p>
<blockquote>
<p><strong>图2</strong>：在抓放任务上，使用不同奖励模块训练OpenVLA策略的成功率随训练时间的变化。基于世界模型的奖励在训练效率和最终成功率上均表现最佳，而外生奖励（嵌入、代理）收敛慢且性能差。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.03556v1/x3.png" alt="奖励曲线对比"></p>
<blockquote>
<p><strong>图3</strong>：在OOD环境下，一次成功抓放轨迹的奖励曲线对比。基于世界模型的奖励提供了更平滑、更具指导性的信号，表明其比嵌入和代理奖励更具泛化性。</p>
</blockquote>
<ol start="2">
<li><strong>消融实验（奖励模块对比）</strong>：图2展示了不同奖励模块的训练效果。基于世界模型的内生奖励取得了最佳的成功率和训练效率。外生奖励（嵌入、代理）由于直接拟合手工奖励导致信号不稳定，收敛缓慢且性能较差。</li>
<li><strong>多任务评估</strong>：如表2所示，在多任务（抓放和移动到目标）训练设置下，在世界模型中进行RL训练相比监督微调（SFT）和物理仿真器RL训练，在多数情况下能带来更均衡或显著的性能提升，显示了其对任务级泛化的益处。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.03556v1/x4.png" alt="领域内轨迹可视化案例"></p>
<blockquote>
<p><strong>图4</strong>：领域内评估的轨迹可视化案例。在世界模型环境中用通用奖励训练的策略，其性能与在物理仿真器中训练的策略相当，而用代理和嵌入奖励训练的策略表现较差。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.03556v1/x5.png" alt="领域外轨迹可视化案例"></p>
<blockquote>
<p><strong>图5</strong>：领域外评估的轨迹可视化案例。只有世界模型环境中训练的策略在OOD场景下具有泛化能力，成功完成任务；在其他环境中训练的策略因只与单一环境交互而泛化能力差。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种基于世界模型的内生通用奖励机制，能够为异构任务生成统一的奖励信号，从而训练出具有广泛泛化能力的策略。</li>
<li>提出了一种开创性的以世界模型为中心的RL方法，将世界模型集成为一个提供所有必要环境信号的通用环境模拟器，实现了高效、可泛化的具身策略训练。</li>
<li>通过大量实验验证了框架的有效性，实证表明在RoboScape-R环境中训练的策略在OOD场景下性能平均提升37.5%。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确列出局限性，但从方法描述和实验设置可推断，世界模型的训练需要大量计算资源（96小时在H20 GPU集群上）和高质量、多样化的轨迹数据。此外，奖励机制中涉及多个阈值（(\theta, \theta_{sps})），其设定可能影响训练效果。</p>
<p><strong>启示</strong>：本文工作为利用大规模预训练的世界模型作为在线、通用的策略训练环境开辟了新路径。它启示后续研究可以进一步探索：1）如何设计更高效、更鲁棒的内生奖励形式；2）如何将更强大的基础模型（如视频生成模型）无缝集成到此类训练范式中；3）如何将该范式扩展到更复杂的真实机器人任务和动态环境中，推动通用机器人学习的发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RoboScape-R框架，旨在解决传统模仿学习与强化学习在机器人策略跨场景泛化方面的局限性。其核心创新在于利用世界模型作为通用环境代理，并设计了一种基于世界模型的通用奖励机制，该机制从模型学习到的状态转移动力学中生成“内生”奖励。实验表明，该方法为策略训练提供了高效通用的环境，在域外场景下相比基线方法平均性能提升37.5%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.03556" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>