<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DiWA: Diffusion Policy Adaptation with World Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DiWA: Diffusion Policy Adaptation with World Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.03645" target="_blank" rel="noreferrer">2508.03645</a></span>
        <span>作者: Abhinav Valada Team</span>
        <span>日期: 2025-08-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，扩散模型已成为机器人策略学习的强大工具，能够通过条件去噪过程捕获复杂的多模态行为。然而，单纯基于离线演示数据通过模仿学习训练的扩散策略，继承了模仿学习的核心局限，例如难以应对分布偏移，并在未见过场景中因专家轨迹不完美或范围狭窄而失败。强化学习为克服这些局限提供了路径，但直接在真实环境中进行在线微调面临样本效率极低和安全性的重大挑战。现有最先进的方法DPPO将扩散去噪过程视为马尔可夫决策过程以进行基于策略梯度的在线微调，但其对大量环境交互的强依赖使其在现实世界中不切实际。</p>
<p>本文针对扩散策略在线微调样本效率低下、成本高昂且不安全的痛点，提出了一种新视角：利用从离线数据中学习的世界模型作为安全的、数据驱动的模拟器，在潜在空间中进行想象的轨迹推演，从而实现扩散策略的完全离线强化学习微调。本文的核心思路是：在学习的潜在空间中进行想象推演，结合策略梯度和行为克隆正则化，对预训练的扩散策略进行离线微调，从而实现安全、高效的机器人技能适应。</p>
<h2 id="方法详解">方法详解</h2>
<p>DiWA框架包含四个阶段：1）从无标签的玩耍数据中学习世界模型；2）在专家演示上预训练扩散策略；3）基于专家演示的潜在状态训练奖励分类器；4）在世界模型的潜在空间内，通过想象推演完全离线地微调策略。推理时，微调后的策略可直接部署到真实环境。</p>
<p><img src="https://arxiv.org/html/2508.03645v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：DiWA框架概述：(1) 在非结构化机器人玩耍数据上训练世界模型以学习潜在动态。(2) 使用学习到的潜在表示在专家演示上预训练扩散策略。(3) 在专家轨迹上训练成功分类器以估计任务奖励。(4) 通过梦想扩散MDP中的想象推演，使用策略梯度和基于分类器的奖励，完全离线地微调扩散策略。</p>
</blockquote>
<p><strong>核心模块1：世界模型学习</strong>。使用递归状态空间模型架构，在无标签玩耍数据集上训练潜在动态模型。模型在每个时间步维护一个确定性递归状态<code>h_t</code>，并从依赖于当前观测<code>x_t</code>的后验中采样一个随机潜在变量<code>z_t</code>。通过最小化证据下界进行训练，学习到的世界模型定义了潜在空间MDP <code>ℳ_wm = (𝒵, 𝒜, P_ϕ)</code>，可在训练后从学习到的先验中推演想象轨迹，而无需额外观测。</p>
<p><strong>核心模块2：扩散策略预训练与奖励估计</strong>。扩散策略通过行为克隆在专家演示上预训练，其输入是经世界模型编码的潜在状态。为赋予世界模型任务特定的奖励信号，在专家演示的潜在状态上训练一个二元分类器<code>C_ψ(z_t)</code>，用于预测任务成功概率。在想象推演中，奖励计算为<code>R_ψ(z_t, a_t) := C_ψ(z_t+1)</code>。</p>
<p><strong>核心模块3：梦想扩散MDP</strong>。这是方法的核心创新。它将扩散去噪过程嵌入到世界模型MDP中，形成一个复合的梦想扩散MDP <code>ℳ_DD</code>。该MDP的索引<code>t̄(t,k)</code>跨越了世界模型时间步<code>t</code>和去噪步<code>k</code>。其状态为<code>(z_t, a_t^k)</code>，动作为<code>a_t^(k-1)</code>。仅当<code>k=1</code>（即产生最终环境动作时）给予基于分类器的奖励，其他去噪步奖励为0。动力学方面，当<code>k&gt;1</code>时，状态在潜在层面保持不变，仅更新动作；当<code>k=1</code>时，使用世界模型动力学<code>P_ϕ</code>转换到下一个潜在状态<code>z_t+1</code>，并重新采样噪声开始新的扩散过程。该公式使得每个去噪步的策略<code>π̄_θ</code>（定义为高斯分布）具有明确的可能性，从而支持策略梯度优化。</p>
<p><strong>核心模块4：离线微调</strong>。在<code>ℳ_DD</code>中使用近端策略优化进行微调。为稳定训练并确保向真实环境可靠迁移，在PPO目标的基础上增加了行为克隆正则化项，约束更新后的策略不要偏离预训练的扩散策略太远。最终优化目标为<code>ℒ_θ = ℒ_PPO - α_BC * 𝔼[ Σ log π_θ_pre(a_t^(k-1) | z_t, a_t^k) ]</code>。</p>
<p>与现有方法（如DPPO）相比，DiWA的创新点具体体现在：1) <strong>完全离线</strong>：用学习的世界模型替代真实或模拟环境进行交互，微调过程无需任何物理交互。2) <strong>复合MDP</strong>：构建了梦想扩散MDP，将扩散去噪的中间步骤与基于世界模型的潜在状态转移统一在一个框架内，使得策略梯度可以直接应用。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要评估在CALVIN仿真基准的环境D中进行，涉及7自由度Franka Emika Panda机器人执行8项桌面操作任务。使用约50万条转移的玩耍数据训练世界模型，每个技能使用50条演示预训练扩散策略。</p>
<p><strong>对比方法</strong>：与Diffusion Policy Policy Optimization (DPPO) 进行对比。评估了DPPO的两个变体：1) **DPPO (Vision)**：原始版本，使用ViT编码原始像素观测。2) **DPPO (Vision WM Encoder)**：使用与DiWA相同的世界模型编码器，从相同的预训练策略开始。</p>
<p><strong>关键实验结果</strong>：DiWA成功地对所有8项技能进行了完全离线的微调，平均成功率相比基础策略有显著提升（例如，开抽屉从57.78%提升至74.44%，关抽屉从59.14%提升至91.95%）。相比之下，DPPO变体需要数十万到数百万次真实环境交互才能达到与DiWA相当的性能水平（例如，关抽屉任务DPPO (Vision WM Encoder)需要约34.56万步，DPPO (Vision)需要约154.56万步）。这凸显了DiWA极高的样本效率和安全性优势。</p>
<p><img src="https://arxiv.org/html/2508.03645v1/figures/visionwm-vs-hybridwm.png" alt="仿真结果对比"></p>
<blockquote>
<p><strong>图3</strong>：三种DiWA变体在仿真微调任务上的比较。蓝色（Vision WM）仅使用视觉输入；绿色（Hybrid WM + Reward Classifier）和红色（Hybrid WM + Latent Decoder）都加入了场景状态监督。结果表明，更具表现力的世界模型和更准确的奖励信号能带来更好的离线微调性能。</p>
</blockquote>
<p><strong>消融实验</strong>：图3的消融研究表明，1) 结合了视觉和特权场景状态监督的混合世界模型，比仅用视觉的世界模型能实现更快、更稳定的微调，这归因于更准确的潜在动力学提升了想象推演质量。2) 在混合世界模型基础上，使用潜在解码器直接重构状态变量来计算奖励（红色），比使用学习的奖励分类器（绿色）通常能获得更强的性能，表明奖励信号的准确性对优化至关重要。</p>
<p><img src="https://arxiv.org/html/2508.03645v1/figures/real_skills.png" alt="真实世界技能"></p>
<blockquote>
<p><strong>图4</strong>：DiWA在真实世界中零-shot部署的定性结果。经过世界模型内离线微调的扩散策略成功完成了开抽屉、移动滑块、开关灯等任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.03645v1/figures/real-adaptation.png" alt="真实世界适应"></p>
<blockquote>
<p><strong>图5</strong>：在真实机器人上，经过世界模型内离线微调（DiWA）的策略相比基础模仿学习策略，在任务成功率上有显著提升，证明了离线适应的有效性。</p>
</blockquote>
<p><strong>真实世界结果</strong>：DiWA在真实机器人上实现了零样本部署。如图4和图5所示，经过世界模型内离线微调的策略在多项操作任务上取得了显著高于基础模仿学习策略的成功率，例如“移动滑块左”任务成功率从约40%提升至超过80%，验证了该方法在现实中的有效性和实用性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) <strong>首次实现了基于世界模型的扩散策略完全离线微调</strong>：提出了DiWA框架和梦想扩散MDP公式，使扩散策略能够通过想象推演进行强化学习优化，无需任何环境交互。2) <strong>实现了样本高效的机器人技能适应</strong>：仅需一次训练的世界模型和离线数据，即可微调多种技能，样本效率比在线模型无关方法高出数个数量级。3) <strong>证明了零样本真实世界部署的可行性</strong>：在仿真或真实数据上训练的世界模型内微调的策略，可直接部署到真实机器人并提升性能。</p>
<p><strong>局限性</strong>：论文提到，世界模型可能包含细微错误，智能体可能在想象中利用这些错误，导致策略在真实环境中失败（复合误差）。此外，依赖于从有限专家数据中学习的奖励分类器，其准确性可能成为性能瓶颈。</p>
<p><strong>对后续研究的启示</strong>：1) <strong>改进世界模型保真度</strong>：提升世界模型尤其是视觉世界模型的动态预测精度，是提高离线微调性能和应用范围的关键。2) <strong>探索更高效的奖励学习</strong>：研究如何从更少或更弱监督的数据中学习更鲁棒、更准确的奖励函数。3) <strong>扩展至更复杂场景</strong>：将框架应用于多任务、长视野规划以及动态变化更剧烈的环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DiWA框架，解决扩散策略在线强化学习微调样本效率低、安全性差的核心问题。其关键技术是引入世界模型，利用少量离线交互数据训练模型，在模型想象的轨迹中完全离线执行策略优化，替代昂贵的真实环境交互。在CALVIN基准测试中，DiWA仅通过离线适应就提升了八项任务的性能，所需物理交互量比无模型基线少数个数量级，首次实现了基于离线世界模型的扩散策略高效微调。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.03645" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>