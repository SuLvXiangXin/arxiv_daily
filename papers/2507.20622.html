<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FMimic: Foundation Models are Fine-grained Action Learners from Human Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FMimic: Foundation Models are Fine-grained Action Learners from Human Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.20622" target="_blank" rel="noreferrer">2507.20622</a></span>
        <span>作者: Yufeng Yue Team</span>
        <span>日期: 2025-07-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前视觉模仿学习领域，主流方法主要分为两类：一类是从大量视频中通过自监督学习开发高效的视觉表示；另一类是从视频中学习任务相关的先验来指导机器人行为或推导启发式奖励函数。然而，这些方法通常在学习和执行精确操作方面面临挑战，并且难以将习得的技能泛化到未见过的环境。与此同时，近期兴起的视觉语言模型因其强大的视觉和语言推理能力，被应用于VIL任务。但现有方法仅将VLMs用于从人类演示中学习高层规划，其底层物理交互的执行仍依赖于预定义的动作基元，这成为机器人系统的主要瓶颈。</p>
<p>本文针对上述痛点，提出了一个核心问题：能否利用基础模型直接从有限的人类视频中学习细粒度的动作层级，从而摆脱对预定义基元的依赖？本文的核心思路是：首先从演示视频中定位人-物交互运动，然后通过提取关键点和路径点来刻画运动属性，并利用分层约束表示来学习细粒度的动作技能；在未见场景中，通过关键点转移和迭代比较来更新技能，实现高效适应；最后，通过交互优化和接触辅助的姿态估计来提升高精度操作任务的执行能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>FMimic的整体框架旨在从人类演示视频中学习细粒度且可泛化的技能，并在高精度任务中执行。其输入是人类执行操作任务的RGB-D视频，输出是机器人可执行的、包含细粒度动作的技能代码。流程主要包含四个核心模块：人-物交互定位、技能学习器、技能适配器和技能优化器。</p>
<p><img src="https://i.imgur.com/4X9c8n4.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：FMimic方法整体框架图解。(a) 典型VIL方法难以泛化到未见环境，(b) 现有方法仅将VLM用作规划器，难以生成低层动作。(c) FMimic通过定位人-物交互获得动作运动，学习包含细粒度动作的技能，并通过技能适配器更新技能以实现泛化。(d) FMimic通过优化交互和姿态估计结果，完成高精度任务。</p>
</blockquote>
<p><img src="https://i.imgur.com/Dg1Q3vT.png" alt="方法流程"></p>
<blockquote>
<p><strong>图2</strong>：FMimic方法详细流程图。人-物交互定位模块捕获交互运动。技能学习器利用这些交互推导细粒度动作技能。技能适配器更新已学技能以适应新场景。对于高精度任务，技能优化器优化交互和姿态估计结果以提升精度。</p>
</blockquote>
<p><strong>1. 人-物交互定位</strong>：该模块将复杂的动作识别问题转化为更易于处理的模式推理问题。其流程分为四个阶段（如图3所示）：<br>    - <strong>任务识别</strong>：从视频中间歇性提取关键帧，使用视觉基础模型检测物体，并指令VLM将视频转录为任务指令（如“制作馅饼”）并提取任务相关物体及其空间关系的结构化文本描述。<br>    - <strong>视频解析</strong>：利用SAM-Track预测手和任务相关物体的掩码，通过反投影生成点云。通过计算手与物体点云之间的距离，识别接触开始和结束的标记点，从而将视频分割成多个子任务片段。根据手部运动轨迹长度过滤无效片段。<br>    - <strong>子任务识别</strong>：指令VLM分析每个视频片段，生成子任务文本描述，并根据交互实体和描述将片段分类为抓取阶段或操作阶段，同时识别主物体和从物体。<br>    - <strong>交互估计</strong>：交互<code>I</code>由从物体<code>O_s</code>和主物体<code>O_m</code>的轨迹<code>ξ_O_s</code>和<code>ξ_O_m</code>表征。使用FrankMocap推断手部姿态，并通过ICP对齐手部网格与分割出的手部点云，得到精确的手部轨迹，进而转换为机器人夹爪姿态轨迹。同时，使用FoundationPose基于物体网格模型生成物体姿态轨迹。</p>
<p><img src="https://i.imgur.com/6V5r7tE.png" alt="人-物交互定位"></p>
<blockquote>
<p><strong>图3</strong>：人-物交互定位模块示意图。(a) 从人类视频中识别任务和相关物体，(b) 将视频解析为多个片段并识别离散子任务，(c) 捕获每个片段内的交互。</p>
</blockquote>
<p><strong>2. 技能学习器与分层表示</strong>：由于VLM难以推理具有内在冗余性的运动信号，本模块首先将定位到的交互<code>I</code>提炼为关键点<code>F</code>和路径点<code>χ</code>，以紧凑地捕获运动的重要属性（如图6所示）。然后，提出<strong>分层约束表示</strong>来分析这些提炼后的交互<code>I = {F, χ}</code>：通过可视化的交互<code>I_V</code>表达语义约束，同时通过对交互数值<code>I</code>的数值分析来指定细粒度的几何约束。<br>    - <strong>关键点-路径点提取</strong>：关键点刚性附着在物体上，有效封装了任务相关的可供性属性，路径点则描绘了它们的相对运动轨迹。对于抓取阶段，关键点定义为手-物接触时刻主物体上的手部姿态。对于操作阶段，从物体关键点位置定义为在接触时刻从物体点云上最靠近主物体点云的点，其方向由预接触运动方向等因素决定（如图6所示）。</p>
<p><img src="https://i.imgur.com/9V5r7tE.png" alt="关键点-路径点提取"></p>
<blockquote>
<p><strong>图6</strong>：关键点-路径点提取示意图。基于主-从接触确定关键点的位置和方向。</p>
</blockquote>
<p><strong>3. 技能适配器与关键点转移及迭代比较</strong>：为使技能适应新场景，本模块采用<strong>区域到关键点的映射策略</strong>进行关键点转移，将已提取的关键点精确转移到新物体上。同时，采用<strong>迭代比较策略</strong>，根据新场景的观察和任务指令，将当前执行与演示知识进行迭代对比，从而修订和更新已学习的技能。</p>
<p><strong>4. 技能优化器与接触辅助优化</strong>：为处理高精度任务，本模块将之前的视觉中心框架提升为多模态架构，整合视觉和接触反馈。<br>    - <strong>碰撞最小化交互优化</strong>：优化定位和转移后的交互，使技能学习和泛化适用于具有严格约束的高精度任务。<br>    - <strong>基于迭代接触的姿态估计</strong>：直接优化感知到的相对姿态，实现有效的姿态优化和精确的技能执行。<br>    - <strong>基于信息增益最大化的接触选择方法</strong>：优化每次接触迭代的效果，增强对陌生物体的泛化能力。</p>
<p><strong>与现有方法相比的创新点</strong>：</p>
<ol>
<li><strong>从物体中心到关键点中心</strong>：提出了以关键点为中心的新范式，通过关键点-路径点提取来提炼核心交互，并通过区域到关键点的映射实现精确的关键点转移，从而促进技能获取和泛化。</li>
<li><strong>从视觉框架到多模态架构</strong>：引入了技能优化器，通过整合视觉和接触反馈来优化交互和姿态估计，提升了高精度任务的处理能力。</li>
<li><strong>直接学习细粒度动作</strong>：摆脱了对预定义动作基元的依赖，直接从视频中学习所有底层动作，拓宽了适用性。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：在RLBench模拟器的多任务实验、真实世界操作任务（包括已见环境、未见环境、未见任务）以及高精度和长视野任务上进行评估。</li>
<li><strong>基线方法</strong>：对比了典型的视觉模仿学习方法（如R3M、MVP）、以及利用VLM进行规划的方法（如GPT-4V for Robotics、VLaMP、Digknow、Demo2code等）。</li>
<li><strong>评估指标</strong>：主要使用任务成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>RLBench多任务实验</strong>：FMimic仅用5个演示视频就显著优于所有其他方法，性能提升超过39%。</li>
<li><strong>真实世界操作任务</strong>：在已见环境中性能提升超过29%，在未见环境中提升超过38%，在未见任务中提升超过32%。</li>
<li><strong>高精度与长视野任务</strong>：在高精度任务上超过基线方法34%以上，在长视野任务上超过47%以上。</li>
</ul>
<p><img src="https://i.imgur.com/4X9c8n4.png" alt="技能学习与泛化"></p>
<blockquote>
<p><strong>图5</strong>：FMimic在技能学习与泛化中的示意图。(a) 人-物交互定位模块解析视频并捕获交互运动。(b) 技能学习器提炼交互，提取知识并推导技能。(c) 在新场景中，技能适配器转移关键点并更新技能以适应。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>论文进行了详细的消融研究，验证了各个组件的贡献。关键点-路径点提取、分层约束表示、关键点转移、迭代比较策略、以及技能优化器中的交互优化和接触辅助姿态估计均为整体性能提升做出了重要贡献。特别是，从物体中心范式转向关键点中心范式，以及引入接触反馈的多模态优化，被证明对处理高精度任务和实现泛化至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了FMimic，一种利用基础模型直接从有限人类视频中学习细粒度、可泛化动作技能的新范式，摆脱了对预定义动作基元的依赖。</li>
<li>引入了关键点中心范式（包括关键点-路径点提取和区域到关键点映射）和分层约束表示，有效促进了技能的获取与跨场景泛化。</li>
<li>设计了技能优化器，通过整合视觉与接触反馈的多模态优化（碰撞最小化交互优化、迭代接触姿态估计），显著提升了机器人执行高精度、强约束操作任务的能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，方法性能在一定程度上依赖于所使用的视觉基础模型（如物体检测、姿态估计）的准确性。此外，虽然减少了对大规模机器人数据的依赖，但整个流程涉及多个模块的串行处理，可能带来一定的计算成本。</p>
<p><strong>对后续研究的启示</strong>：<br>FMimic展示了基础模型在理解复杂物理交互和生成可执行低级控制指令方面的潜力。其关键点表示和分层分析思想为从视频中学习结构化、可解释的技能提供了新思路。将视觉、语言与触觉等多模态信息更紧密地结合，是提升机器人精细操作和泛化能力的重要方向。未来工作可以探索如何进一步简化或端到端优化整个技能学习与适应流程。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>FMimic提出了一种利用基础模型直接从少量人类视频中学习可泛化精细动作技能的新范式，旨在解决现有方法依赖预定义动作基元、难以执行精确物理交互的瓶颈。其核心技术包括：通过关键点与路径点描述运动属性以学习分层约束表示，在未见场景中通过关键点转移与迭代比较进行技能适配，并采用迭代主从接触精炼优化姿态估计以实现高精度操作。实验表明，该方法仅用单个视频即表现良好，使用五个视频时显著优于其他方法，在RLBench多任务和真实操作任务中性能分别提升超过39%和29%，在高精度与长视野任务中分别超出基线34%和47%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.20622" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>