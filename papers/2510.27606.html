<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.27606" target="_blank" rel="noreferrer">2510.27606</a></span>
        <span>作者: Liu, Yuhong, Zhang, Beichen, Zang, Yuhang, Cao, Yuhang, Xing, Long, Dong, Xiaoyi, Duan, Haodong, Lin, Dahua, Wang, Jiaqi</span>
        <span>日期: 2025/10/31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>大视觉语言模型（LVLMs）在空间理解方面仍是一个薄弱环节。现有方法主要依赖监督微调（SFT）和近期基于可验证奖励的强化学习（RLVR）。SFT方法通过合成空间问答对或构建3D场景图来提升性能，但容易过拟合、继承检测器和深度估计的误差，且依赖昂贵的专有模型进行数据标注。RLVR方法通过可验证奖励优化来提升泛化能力，但其可验证的监督信号通常局限于特定的3D扫描环境，需要大量的流程工程，领域覆盖有限。这些方法的关键局限性在于依赖外部、昂贵或受限的监督信号，难以扩展到多样化的真实世界图像领域。本文针对“如何在保留RL优化优势的同时，将可验证的监督扩展到无需人工标注、专用资产或昂贵工具的普通图像领域”这一挑战，提出了利用图像内在结构进行自监督的新视角。核心思路是：设计一系列自监督的“前置任务”，这些任务从普通RGB或RGB-D图像中提取2D和3D空间结构信息（如相对位置、深度排序），其答案可由任务本身确定性地验证，从而为RLVR训练提供免费、可扩展且自然的监督信号。</p>
<h2 id="方法详解">方法详解</h2>
<p>Spatial-SSRL框架包含两个阶段：自监督任务设计与数据构建、以及基于可验证奖励的强化学习训练。</p>
<p><img src="https://arxiv.org/html/2510.27606v2/x3.png" alt="方法总览"></p>
<blockquote>
<p><strong>图3</strong>：Spatial-SSRL框架总览。(a) 自监督数据构建：从原始RGB和RGB-D图像中，自动构建五个前置任务（块重排、块翻转检测、裁剪块修复、区域深度排序、相对3D位置预测），无需任何人工或LLM标注。(b) RL训练：使用基于答案正确性的可验证奖励函数和用于确保格式合规的格式奖励，通过组相对策略优化（GRPO）来优化模型。</p>
</blockquote>
<p><strong>核心模块：自监督任务设计</strong><br>任务设计遵循三个原则：零人工或LLM监督、无需工具即可扩展、自然可验证。所有任务均被构造为LVLM的问答提示，并带有确定性的可验证答案。任务分为两类，共五个：</p>
<ol>
<li><strong>无深度任务（基于RGB）</strong>：<ul>
<li><strong>乱序块重排</strong>：将图像分割为M×N的网格块，随机打乱顺序后重组为输入图像。模型需要预测正确的原始块排序序列。为增加难度，可随机将一个块涂白，迫使模型依赖全局布局而非局部边缘匹配。</li>
<li><strong>翻转块识别</strong>：随机选择一个图像块，对其进行水平或垂直翻转。模型需要识别被翻转块的索引及其翻转方向。</li>
<li><strong>裁剪块修复</strong>：从图像中裁剪出一个方形区域并置零，生成带掩码的输入图像。同时，从该裁剪块生成三个具有视觉相似性的干扰项（旋转90°、内部子区域、外部扩展区域）。模型需从四个候选块中选择正确填充掩码的块。</li>
</ul>
</li>
<li><strong>基于深度的任务（基于RGB-D）</strong>：<ul>
<li><strong>区域深度排序</strong>：在深度图上选择三个深度递增且内部深度一致、彼此分离的区域，在图像上标记其中心并打乱视觉标签。模型需要根据图像和深度线索，将标记区域按从近到远的顺序排序。</li>
<li><strong>相对3D位置预测</strong>：给定图像中两个像素点R1和R2，并指定R1处物体的朝向。模型需要从该物体的视角，预测R2的相对方位（如前、后、左、右及其组合）。通过坐标变换将相机坐标系下的点转换到以R1为原点、物体朝向为正向的坐标系中，根据变换后坐标的符号确定真实答案。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.27606v2/x8.png" alt="相对位置预测坐标定义"></p>
<blockquote>
<p><strong>图8</strong>：相对3D位置预测任务的坐标定义与变换示意图。展示了如何从相机坐标系转换到以物体为中心的坐标系，以确定相对位置。</p>
</blockquote>
<p><strong>数据构建</strong>：使用COCO（RGB）、DIODE和MegaDepth（RGB-D）作为原始数据源，丢弃所有人工标注。通过上述自动化流程，构建了包含81,053个问答对的 <strong>Spatial-SSRL-81k</strong> 数据集，实现了100%的标注准确率。</p>
<p><strong>核心模块：强化学习训练</strong></p>
<ol>
<li><strong>冷启动SFT</strong>：由于任务多样且复杂，直接进行RL训练不稳定。首先在一个约3600个样本的小子集上进行SFT，使模型熟悉任务格式和答案结构。</li>
<li><strong>GRPO优化</strong>：冷启动后，使用GRPO算法在全部数据上进行优化。模型被要求将推理过程放在 <code>&lt;think&gt;...&lt;/think&gt;</code> 标签内，并将最终答案放在 <code>\boxed{}</code> 中。</li>
<li><strong>奖励设计</strong>：总奖励 <code>r = 0.9 * r_acc + 0.1 * r_fmt</code>。<code>r_acc</code> 是准确性奖励（答案完全匹配真实值则为1，否则为0），<code>r_fmt</code> 是格式奖励（输出严格符合指定格式则为1，否则为0）。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，Spatial-SSRL的创新性体现在：1) <strong>完全自监督</strong>：监督信号完全来自图像内在结构，无需任何外部标注或专有模型。2) <strong>无需工具</strong>：避免了复杂的检测、分割或渲染工具链。3) <strong>自然可验证</strong>：前置任务本身提供了确定性的真实答案，完美契合RLVR范式。4) <strong>模块化与可扩展</strong>：可以轻松集成新的自监督任务而无需改变数据源或RL流程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准数据集</strong>：在7个空间理解基准上评估，包括图像任务（Spatial457， 3DSRBench， SpatialEval， QSpatial_plus）和视频任务（What’sUp， ViewSpatial， VSI-Bench）。</li>
<li><strong>基线模型</strong>：以Qwen2.5-VL-3B和7B作为基础模型进行微调对比。同时对比了代表性的空间理解模型，如SpatialLadder-3B和SpaceR-7B。</li>
<li><strong>实验平台</strong>：使用GRPO进行RL训练。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在Qwen2.5-VL-3B和7B基础上应用Spatial-SSRL后，在7个基准上的平均准确率分别提升了4.63%和3.89%。其中，在需要精确感知、强空间解释和多步推理的Spatial457基准上取得了最大提升（+8.67%）。经过针对性RL微调后，模型在通用视觉问答（VQA）、多图像理解和幻觉诊断等非空间任务上性能未出现倒退。</p>
<p><img src="https://arxiv.org/html/2510.27606v2/x4.png" alt="主要结果对比"></p>
<blockquote>
<p><strong>图4</strong>：基于Qwen2.5-VL的模型在空间理解基准上的性能对比表。Spatial-SSRL-3B/7B相比原始基线模型在大多数任务上均有显著提升，平均提升幅度可观。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.27606v2/x5.png" alt="消融实验（任务组合）"></p>
<blockquote>
<p><strong>图5</strong>：不同自监督任务组合的消融实验结果。单独使用无深度任务或基于深度任务均能带来提升，而结合全部五个任务能取得最佳性能，表明两类任务具有互补性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.27606v2/x6.png" alt="消融实验（训练策略）"></p>
<blockquote>
<p><strong>图6</strong>：不同训练策略的消融实验结果。仅用SFT或仅用RL（无冷启动）效果均不佳。SFT+RL（即完整的Spatial-SSRL流程）取得了最好的效果，验证了冷启动SFT和后续RL优化的必要性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>任务贡献</strong>：消融实验表明，无深度任务和基于深度任务各自都能带来性能提升，二者结合时效果最佳，证明了任务设计的全面性和互补性。</li>
<li><strong>训练策略</strong>：仅使用SFT或直接进行RL（无冷启动）的效果均不如完整的Spatial-SSRL流程（SFT冷启动后接GRPO），验证了所提出两阶段训练策略的有效性。</li>
<li><strong>数据质量</strong>：通过自监督构建的数据集具有100%的准确率，而依赖外部工具生成标注的方法会引入噪声，影响最终性能。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>Spatial-SSRL</strong>，一种用于提升LVLM空间理解能力的自监督强化学习新范式。该范式成本低、可扩展、与RLVR自然兼容，且易于扩展新的自监督任务。</li>
<li>设计了一套覆盖2D和3D空间感知的自监督任务，并通过消融研究证明了每个任务对提升空间理解能力的贡献。</li>
<li>为RL训练提供了高质量、富有挑战性的自监督数据构建新见解，开辟了有效结合RLVR与SSL以提升空间理解的新途径。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，基于深度的任务需要RGB-D数据，这在一定程度上限制了数据源的范围（尽管有DIODE、MegaDepth等公开数据集）。此外，自监督任务的难度和多样性仍有进一步探索的空间。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>任务扩展</strong>：Spatial-SSRL的模块化设计鼓励研究者探索更多类型的自监督前置任务，以覆盖更广泛的空间认知维度。</li>
<li><strong>多模态结合</strong>：该方法展示了利用数据内在结构生成监督信号的潜力，可启发在其他模态（如视频时序、音频空间）或跨模态任务中应用类似思想。</li>
<li><strong>训练策略优化</strong>：冷启动SFT与GRPO的结合策略为解决复杂、多格式任务的RL训练不稳定性提供了参考。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文针对大型视觉语言模型（LVLMs）空间理解能力弱、现有监督方法成本高且可扩展性差的问题，提出Spatial-SSRL自监督强化学习范式。该方法从普通RGB或RGB-D图像自动衍生可验证信号，通过五个前置任务（如打乱补丁重排序、区域深度排序等）捕捉2D/3D空间结构，无需人工标注。实验在七个空间理解基准上显示，相比Qwen2.5-VL基线，平均准确率提升4.63%（3B模型）和3.89%（7B模型），显著增强空间推理能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.27606" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>