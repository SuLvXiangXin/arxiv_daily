<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Forceful Robotic Foundation Models: a Literature Survey - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Towards Forceful Robotic Foundation Models: a Literature Survey</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.11827" target="_blank" rel="noreferrer">2504.11827</a></span>
        <span>作者: Xie, William, Correll, Nikolaus</span>
        <span>日期: 2025/04/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人基础模型（Robot Foundation Models）在实现零样本自主操作方面取得了显著进展，但其主要依赖于视觉输入和位置控制。然而，仅依赖位置控制可能不足以实现真正的灵巧性：在刚性系统中，微小的位置误差可能导致巨大的力误差；同时，力作为位置的二阶导数，代表了更丰富、更高频的运动信息，仅关注位置可能在模仿学习中样本效率低下或遗漏高频细节。尽管力与触觉对于精细操作、人机交互和接触丰富的装配任务至关重要，但在策略学习中如何以及为何使用这些模态仍不明确，因为许多力控制的优势可以通过阻抗控制等隐式技术实现。</p>
<p>本文针对下一代机器人基础模型需要整合力与触觉感知（统称为“强力”感知）这一核心挑战，进行了系统性文献综述。文章旨在阐明在机器人操作策略学习中整合力感知的具体方法、时机与原因。核心思路是：通过比较分析现有25篇基于Transformer和扩散模型的端到端学习方法，在数据收集、动作生成与表征学习三个关键维度上，梳理整合力感知的技术路径，为构建强大的、接触感知的机器人基础模型指明方向。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文作为一篇综述，并未提出单一的新方法框架，而是构建了一个系统的分析框架来梳理和比较现有工作。</p>
<p><img src="https://arxiv.org/html/2504.11827v1/extracted/6365838/img/Touchdata.png" alt="任务力与时长分布（按论文）"></p>
<blockquote>
<p><strong>图4</strong>：25篇论文中64个任务（53个独特）的近似力大小与任务时长分布图。横轴为任务时长，纵轴为力大小（对数尺度）。每个点代表一个任务，颜色和形状对应不同的论文。该图直观展示了现有研究覆盖的任务在力-时间谱上的广泛分布。</p>
</blockquote>
<p>整体分析框架基于对25篇相关论文的梳理，共涉及64个操作实验、59个不同的策略和53个独特的任务。作者首先建立了一个以“任务时长”和“力大小”为坐标轴的分析图谱（图4、图5），将现有工作置于此二维空间中审视。在此基础上，论文深入剖析了三个核心模块：1) <strong>数据收集</strong>：如何获取包含力/触觉信息的演示数据；2) <strong>动作生成与控制</strong>：策略如何输出基于力感知的动作；3) <strong>表征学习</strong>：如何对异构的力/触觉数据进行有效编码以供策略学习。</p>
<p>与现有综述相比，本文的创新点在于：第一，明确聚焦于具有良好扩展性、适合构建基础模型的Transformer和扩散模型架构；第二，提出了“力-时间”分析维度，对任务特性进行量化分类；第三，系统性地从数据、动作、表征三个连贯的环节拆解技术选型，而非仅按传感器类型或任务分类。</p>
<p><img src="https://arxiv.org/html/2504.11827v1/extracted/6365838/img/grasp_diagram.png" alt="任务力与时长分布（按任务）"></p>
<blockquote>
<p><strong>图5</strong>：与图4相同坐标轴下，展示每个具体任务的分布。该图揭示了任务的多样性，除了“插孔”任务有5篇论文涉及外，缺乏统一的基准测试任务。</p>
</blockquote>
<p>核心模块的技术细节如下：</p>
<ol>
<li><strong>数据收集</strong>：方法多样，包括人类遥操作（提供力反馈或仅位置控制）、脚本控制器、强化学习以及混合方案。关键挑战在于如何高效获取高质量的接触数据。</li>
<li><strong>动作生成与控制</strong>：策略学习的动作空间主要包括：a) <strong>位置/速度控制</strong>：直接输出关节或末端执行器的目标位置/速度，力效应通过底层阻抗控制等隐式实现；b) <strong>力/扭矩控制</strong>：直接输出目标力或关节扭矩；c) <strong>阻抗参数控制</strong>：输出虚拟弹簧-质量-阻尼器系统的参数（如刚度K）。大多数方法仍采用位置/速度控制。</li>
<li><strong>表征学习</strong>：对于力/触觉数据（如一维力序列、高分辨率触觉图像、关节扭矩），处理方式包括直接使用原始数据、通过编码器（如CNN、Transformer）提取特征，或与视觉等其他模态特征融合。如何学习可扩展、通用的触觉表征仍是一个开放问题。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>本文的“实验”部分是对所综述的25篇论文中64个任务实验的分析总结，而非传统的性能对比实验。</p>
<p><strong>Benchmark与数据集</strong>：分析基于25篇论文各自使用的实验平台和数据集，没有统一的数据集。任务涵盖插孔、倒水、抓取易碎物体、组装、削皮等53个独特接触丰富的操作任务。</p>
<p><strong>对比基线</strong>：分析侧重于比较不同工作所采用的技术路线差异，而非直接的性能数值对比。对比维度包括数据收集方法、动作空间类型、触觉传感器类型（图6）、策略网络架构等。</p>
<p><img src="https://arxiv.org/html/2504.11827v1/extracted/6365838/img/force_diagram.png" alt="触觉传感器类型分布"></p>
<blockquote>
<p><strong>图6</strong>：25篇论文中使用的触觉传感器类型分布。主要包括指尖音频、力、光学（视觉触觉）传感器，全身关节扭矩传感，指尖与关节扭矩结合传感，以及腕部力/扭矩传感器。其中，指尖光学传感器（视觉触觉）使用最为广泛（占36%）。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>任务分布</strong>（图4，图5）：分析显示，任务在力（0.1N到&gt;10N）和时间（&lt;5秒到&gt;20秒）维度上分布广泛。40%的任务是长时间（&gt;5秒）且施加典型力（1N-10N），而需要高精度微弱力（0.1N-1N）的任务占23%。这证实了接触操作任务的多样性。</li>
<li><strong>传感器使用</strong>（图6）：指尖光学传感器（视觉触觉）是最流行的触觉传感模态（36%的论文使用），其次是腕部力/扭矩传感器（16%）和全身关节扭矩传感（16%）。这反映了高分辨率接触成像和直接力测量在策略学习中的实用性。</li>
<li><strong>数据与动作空间</strong>：数据收集高度依赖特定任务和硬件，缺乏标准化。在动作空间上，尽管研究的是“强力”策略，但大多数（72%）仍输出位置、速度或姿态命令，依赖底层控制器隐式处理力交互；只有28%直接输出力/扭矩或阻抗参数。</li>
<li><strong>策略性能</strong>：综述指出，对于倒水、插孔、处理精细物体等少数任务，力感知至关重要。但总体而言，当前模仿学习模型的性能水平尚未完全达到力动态真正起主导作用的程度，且许多力控优势可通过非学习的隐式方法获得。</li>
</ol>
<p><strong>消融实验</strong>：作为综述，本文未进行消融实验，但通过分析指出了不同技术组件的现状：即数据收集方法、传感器类型、动作空间和表征学习方式高度碎片化，尚未形成主导性的最佳实践，每个组件都对最终策略能力有显著影响。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>首次对基于Transformer和扩散模型的、整合力与触觉感知的端到端机器人策略学习进行了系统性综述，明确了该新兴领域的技术图景。</li>
<li>构建了“力大小-任务时长”分析框架，对现有工作进行分类，直观揭示了研究任务的分布特征和缺乏统一基准的现状。</li>
<li>从数据收集、动作生成、表征学习三个关键环节深度剖析了现有方法的异同、优势与挑战，为未来研究指明了具体的技术改进方向。</li>
</ol>
<p><strong>局限性</strong>（论文自身提及）：</p>
<ol>
<li>许多被综述的工作未报告精确的力大小和任务时长数据，图中的部分数据为估计值。</li>
<li>当前研究呈现高度碎片化：任务、传感器、机器人平台、数据收集方法差异巨大，缺乏可比较的基准，阻碍了进展评估。</li>
<li>尚未出现能够处理大量任务的通用“强力”策略模型，现有策略通常仅针对少数任务。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>必要性确认</strong>：研究强化了下一代机器人基础模型必须整合力/触觉输入输出的观点，特别是在需要精细力控和接触丰富的任务中。</li>
<li><strong>关键方向</strong>：未来需着力于：开发高效、可扩展的力/触觉数据收集方法；探索能显式且安全地利用力感知的动作空间与策略架构；研究通用的、可迁移的触觉表征学习技术。</li>
<li><strong>数据集与基准</strong>：呼吁构建大规模、多模态（包含力/触觉）的机器人操作数据集，并建立统一的基准测试任务，以推动领域发展。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文探讨了如何将力/触觉感知整合到机器人操作策略学习中，以弥补当前视觉主导的基础模型在接触丰富任务上的不足。综述系统比较了力感知、数据收集、行为克隆、触觉表示学习及底层控制等方法，特别关注基于Transformer和扩散模型的端到端学习技术。分析发现，在倾倒、孔轴插入等精细操作任务中，现有模仿学习模型的性能尚未达到力动态起关键作用的水平；同时，力/触觉作为可通过多模态隐式推断的抽象量，其整合方式仍待深入探索。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.11827" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>