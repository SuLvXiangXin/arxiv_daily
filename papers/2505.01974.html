<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.01974" target="_blank" rel="noreferrer">2505.01974</a></span>
        <span>作者: Zhang, Di, Yuan, Chengbo, Wen, Chuan, Zhang, Hai, Zhao, Junqiao, Gao, Yang</span>
        <span>日期: 2025/05/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，为灵巧手收集富含细粒度触觉信息的演示数据对于接触丰富的操作任务至关重要。主流方法主要依赖于遥操作或基于视频的运动重定向。遥操作通过虚拟现实头显或数据手套捕捉演示者手部轨迹并映射到机器人手上；基于视频的方法则直接从第一视角视频推断机器人运动。然而，这些方法存在两个关键局限性：第一，由于人手机器手之间的运动学差异，重定向后的轨迹往往无法准确复现人类行为；第二，操作者在示教过程中缺乏来自机器人端的实时触觉反馈，导致任务表现高度依赖操作者的经验，难以获取高质量的触觉数据。</p>
<p>本文针对“如何高效收集包含高保真触觉反馈的专家级演示”这一具体痛点，提出了手把手（hand-over-hand）动觉示教的新范式。核心思路是：通过让操作者直接“穿戴”并引导灵巧手进行物理交互，同步记录视觉、触觉和本体感觉数据，利用修复技术处理视觉遮挡，并基于这些触觉增强的演示训练能够预测并执行目标接触力的视觉运动策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>KineDex框架的整体流程包含四个主要阶段：动觉数据收集、视觉修复预处理、策略学习以及部署时的力控制。输入是操作者通过手把手引导机器人手执行任务的过程，输出是训练好的、能够根据多模态观测（修复后的视觉图像、触觉信号、本体感觉）预测目标关节位置和目标指尖力的策略，该策略在部署时通过改进的力控制实现精确操作。</p>
<p><img src="https://arxiv.org/html/2505.01974v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：KineDex框架总览。通过动觉示教收集触觉增强的演示，其中操作者手部造成的视觉遮挡通过修复技术移除，然后用于策略训练。学习到的策略接收视觉和触觉输入，预测关节位置和接触力，并通过力控制执行以实现鲁棒操作。</p>
</blockquote>
<p><strong>核心模块与技术细节：</strong></p>
<ol>
<li><p><strong>动觉数据收集系统</strong>：硬件由一个配备灵巧手的机械臂组成，并设置一个前向全局摄像头和一个腕戴近距离摄像头。核心创新在于“手把手”引导机制：在机器人手除拇指外的四个手指背面安装环形带，操作者像戴手套一样将手指穿过这些带子来引导手部运动。这种物理耦合确保运动过程中的接触力能即时传递给操作者，提供自然的触觉反馈。由于人手机形差异，拇指由操作者另一只手单独控制。在示教过程中，系统同步记录多视角RGB图像、机器人末端位姿与关节位置（本体感觉）、每个指尖上多个传感点的局部接触力（密集触觉矩阵）以及由此计算出的每个指尖的三维合力向量。</p>
</li>
<li><p><strong>视觉修复预处理</strong>：原始演示中，前向摄像头会拍摄到操作者的身体，若直接用于训练会导致推理时出现分布外偏移。为此，采用基于修复的方法移除操作者身体：首先使用Grounded-SAM从视频帧中提取操作者身体部位的掩码，然后将帧和掩码输入ProPainter模型，对遮挡的人体区域进行修复。尽管修复模型并非针对机器人数据预训练，但实验表明其处理后的数据足以训练高性能策略。</p>
</li>
<li><p><strong>策略学习</strong>：使用修复后的演示，以Diffusion Policy为骨干网络，训练一个以修复视觉观测$o_t$、触觉向量$q_t$和本体感觉$x_t$为条件，预测力感知动作（目标关节位置$x_d$和目标指尖力$f_d$）的策略，即建模为$p(x_d, f_d \mid o_t, q_t, x_t)$。对于记录的三维指尖力$\mathbf{f}=(f_x, f_y, f_z)$，策略训练时使用法向力分量$f_z$进行监督，该分量对应于指尖能主动施加力的主要轴向。推理时，策略产生动作块以实现更平滑的控制。</p>
</li>
<li><p><strong>力控制模块</strong>：传统位置控制仅跟踪目标关节位置，在接触丰富任务中可能导致手指仅接触物体表面而未施加有效力。KineDex利用预测的指尖力$f_d$计算“力感知目标位置”。具体而言，对于每个手指，根据当前指尖关节和基关节的位置（$x^{tip}$, $x^{base}$），按公式 $x_d^{tip}=x^{tip}+K^{tip}\cdot f_d$ 和 $x_d^{base}=x^{base}+K^{base}\cdot f_d$ 计算新的目标位置。其中$K^{tip}$和$K^{base}$是决定运动刚度的超参数。通过将此修正后的目标位置输入PD控制器，机器人手能在接触物体时产生持续的压力，从而精确跟踪策略预测的目标接触力。</p>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，KineDex的创新主要体现在：1) <strong>示教范式</strong>：提出高效、能提供真实触觉反馈的手把手动觉示教；2) <strong>预处理方法</strong>：采用修复技术解决视觉遮挡问题，避免了先前方法需要回放轨迹的低效性；3) <strong>策略与执行</strong>：训练触觉增强的策略并设计新颖的力控制方法，将预测的力转化为可执行的位置控制指令。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在九个强调精确力控、多指协调的日常接触丰富操作任务上进行评估，包括瓶/杯/蛋拾取、瓶盖旋拧、螺母拧紧、插销插入、充电器插拔、牙膏挤到牙刷上以及注射器推压。使用Franka Emika Panda机械臂搭配Robotera XHand1灵巧手（12自由度，每指120个触觉传感点）或Inspire Hand进行实验。</p>
<p><strong>对比方法</strong>：主要与三个消融变体对比：1) <strong>w/o Force Control</strong>：推理时禁用力控制；2) <strong>w/o Tactile Input</strong>：训练时从策略输入中移除触觉信号；3) <strong>w/o Inpainting</strong>：省略对动觉演示的修复预处理。</p>
<p><img src="https://arxiv.org/html/2505.01974v1/x1.png" alt="任务图示"></p>
<blockquote>
<p><strong>图1</strong>：KineDex评估的九个接触丰富的灵巧操作任务示例。</p>
</blockquote>
<p><strong>关键定量结果</strong>：</p>
<ul>
<li><strong>整体性能</strong>：KineDex在多数任务上成功率超过70%，在简单的拾取任务上接近100%。平均成功率为**74.4%**。</li>
<li><strong>力控制的重要性</strong>：消融实验显示，<strong>w/o Force Control</strong>变体的平均成功率骤降至<strong>16.7%<strong>，即使在简单任务上也常失败。KineDex相比该变体有</strong>57.7%</strong> 的性能提升。</li>
<li><strong>触觉输入的重要性</strong>：对于接触密集的任务（瓶盖旋拧、挤牙膏、推注射器），移除触觉输入（<strong>w/o Tactile Input</strong>）会导致平均成功率下降**26.7%**。</li>
<li><strong>修复的必要性</strong>：<strong>w/o Inpainting</strong>变体在所有任务上的成功率均为**0%**，证实了原始视觉数据因包含人体而无法直接用于训练。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.01974v1/x3.png" alt="力控对比"></p>
<blockquote>
<p><strong>图3</strong>：执行任务时拇指预测力与传感力的可视化对比。KineDex（力感知策略）能准确跟踪预测的目标力，而无力控变体执行的力则保持平坦，无法跟踪目标。</p>
</blockquote>
<p><strong>与遥操作的效率对比</strong>：</p>
<ul>
<li><strong>数据收集成功率</strong>：在五个任务上，KineDex在数据收集阶段成功率接近**100%<strong>，而遥操作的平均成功率仅为</strong>39%**。</li>
<li><strong>数据收集速度</strong>：KineDex的数据收集速度是遥操作的两倍以上。例如在“注射器推压”任务上，KineDex耗时约为遥操作的一半；在“瓶拾取”任务上，耗时不到遥操作的三分之一。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.01974v1/x4.png" alt="效率对比"></p>
<blockquote>
<p><strong>图4</strong>：KineDex与遥操作在“瓶拾取”和“注射器推压”两个任务上收集单条演示所需时间的对比。KineDex显著更快。</p>
</blockquote>
<p><strong>用户研究</strong>：五位参与者使用两种系统后的反馈表明，KineDex在收集精确触觉数据、处理复杂任务以及易用性方面均被认为优于遥操作。</p>
<p><img src="https://arxiv.org/html/2505.01974v1/x5.png" alt="用户研究"></p>
<blockquote>
<p><strong>图5</strong>：用户研究结果总结。参与者普遍认为KineDex有助于收集更精确的触觉数据、处理更复杂的任务，且更易使用。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖高效的<strong>手把手动觉示教范式</strong>，能够直接传递人类运动并提供真实触觉反馈，从而收集高质量的触觉增强演示数据。</li>
<li>提出了一个完整的框架，通过<strong>修复技术</strong>处理示教中的视觉遮挡，并训练一个<strong>触觉信息增强的视觉运动策略</strong>，该策略能预测目标关节位置和指尖力。</li>
<li>设计了一种<strong>创新的力控制方法</strong>，将策略预测的力无缝集成到基于位置控制的执行中，实现了对接触力的精确跟踪，显著提升了接触丰富任务的性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在一些对细粒度定位和接触推理要求极高的挑战性任务上（如挤牙膏），性能有所下降（成功率约50%），这可能超出了当前策略输入的表示能力。此外，修复模型并非针对机器人数据专门训练，可能无法完美移除所有遮挡。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>示教范式</strong>：动觉示教在数据收集效率和质量上展现出巨大潜力，为复杂灵巧操作任务的演示获取提供了更优解，可推广至更复杂的硬件平台。</li>
<li><strong>多模态融合</strong>：触觉感知在视觉遮挡严重或力控关键的任务中作用显著，强调了多模态感知（视觉+触觉+本体感觉）对于鲁棒灵巧操作的重要性。</li>
<li><strong>策略与控制的协同</strong>：KineDex展示了将高层策略预测（力）与底层控制器设计紧密结合的有效性，为在传统位置控制框架内实现精细力控提供了新思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出KineDex框架，旨在解决灵巧操作中难以获取高保真触觉演示数据的问题。其核心方法是采用手把手示教范式，直接将操作者动作映射到灵巧手，并利用修复技术处理视觉遮挡，从而采集富含精确触觉反馈的物理演示数据。基于这些数据，训练触觉增强的视觉运动策略，并在部署中实施力控制。实验表明，该方法在接触丰富的复杂任务（如挤牙膏）上平均成功率达74.4%，较无力控制版本提升57.7%；数据收集效率是遥操作的两倍以上，且成功率接近100%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.01974" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>