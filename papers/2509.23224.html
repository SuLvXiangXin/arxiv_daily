<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Leave No Observation Behind: Real-time Correction for VLA Action Chunks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Leave No Observation Behind: Real-time Correction for VLA Action Chunks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.23224" target="_blank" rel="noreferrer">2509.23224</a></span>
        <span>作者: Sendai, Kohei, Alvarez, Maxime, Matsushima, Tatsuya, Matsuo, Yutaka, Iwasawa, Yusuke</span>
        <span>日期: 2025/09/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，为了提高效率和时序一致性，视觉-语言-动作（VLA）模型普遍采用动作块（action chunk）预测的方式，即一次性预测未来多步动作序列。然而，这种方式在面临模型推理延迟和长预测视野时，会严重损害机器人的反应能力。推理延迟导致执行的动作基于过时的观测，而长动作块则使得动作序列在动态环境中无法根据最新反馈进行调整。现有方法如实时分块（Real Time Chunking, RTC）通过异步执行和块间平滑来缓解问题，但仍假定模型预测固定长度的视野，对新感知输入的反应性依然有限。本文针对VLA模型因推理延迟和动作块执行导致的闭环反应性丧失这一具体痛点，提出了一种新的视角：不改变或重训庞大的基础VLA模型，而是为其附加一个轻量级的实时校正模块。本文的核心思路是：在基础VLA模型异步生成动作块的同时，引入一个可每步运行的校正头，利用最新观测对块内的每个待执行动作进行时间感知的残差校正，从而在保持基础模型能力的同时恢复闭环响应能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的异步动作块校正（A2C2）方法是一个插件式框架。其整体流程如下：在时间步 t，观测 o_t 被发送给基础策略（即VLA模型）π，该策略在经历 d 步的推理延迟后，输出一个长度为 H 的动作块 A_t。对于该块内即将在时间步 t+k (d ≤ k ≤ d+e) 执行的基础动作 a_{t+k}，轻量级校正头 π_a2c2 会结合最新的观测 o_{t+k}、该基础动作 a_{t+k}、一个表示该动作在块内相对位置的时间特征 τ_k、基础策略的潜在表示 z_t 以及语言指令 l，预测出一个残差动作 Δa_{t+k}。最终执行的动作是基础动作与残差动作之和。基础策略每 e 步调用一次，而校正头因其模型小巧，可以每个控制步都运行。</p>
<p><img src="https://arxiv.org/html/2509.23224v1/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：A2C2方法整体框架。左侧基础策略 π 根据观测 o_t 生成动作块 A_t。对于块内每个待执行的动作 a_{t+k}，右侧的校正头利用最新观测 o_{t+k}、基础动作、时间特征 τ_k 等输入，输出残差校正量 Δa_{t+k}，相加后得到最终执行动作 a_{t+k}^{exec}。</p>
</blockquote>
<p>核心模块是校正头 π_a2c2，其具体作用是为基础VLA输出的动作提供实时、基于最新观测的微调。技术细节上，时间特征 τ_k 采用正弦嵌入 (sin(2πk/H), cos(2πk/H)) 来编码动作在块内的周期性位置。校正头的网络结构根据任务环境有所不同。</p>
<p><img src="https://arxiv.org/html/2509.23224v1/x2.png" alt="Kinetix校正头架构"></p>
<blockquote>
<p><strong>图3</strong>：在Kinetix环境中使用的校正头架构。一个3层MLP以当前状态、基础动作和位置嵌入的拼接作为输入，直接输出残差动作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.23224v1/x3.png" alt="LIBERO校正头架构"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO环境中使用的校正头架构。基础策略为SmolVLA。校正头包含一个Transformer编码器和一个轻量级MLP，融合了图像特征、语言指令、基础动作、整个动作块、状态、时间特征及基础策略的潜在表示，最终由MLP预测残差动作。</p>
</blockquote>
<p>训练过程中，首先在示范数据集上训练基础策略 π。随后，使用训练好的基础策略对数据集中的每个观测生成推理动作块，构建新的校正训练数据集 D_cor，其中每个样本包含目标动作、基础策略推理出的对应动作以及时间特征。校正头的训练目标是预测目标动作与基础推理动作之间的残差，损失函数为均方误差（MSE）。</p>
<p>与现有方法相比，A2C2的创新点具体体现在：1) <strong>时间感知校正</strong>：通过显式的位置特征，使校正知道正在修改的是动作块中的第几个动作；2) <strong>块级平滑性</strong>：上述机制有助于在整个视野范围内产生更平滑的校正；3) <strong>数据兼容性</strong>：仅需基础策略的示范数据集，无需强化学习微调；4) <strong>实时反馈</strong>：每个控制步都融入最新观测，提升动态任务下的鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了两个基准测试：包含12个高动态操作与运动任务的<strong>Kinetix</strong>仿真套件，以及强调空间推理的机器人操作标准基准<strong>LIBERO Spatial</strong>。实验平台涉及仿真环境。对比的基线方法包括：<strong>Naive async</strong>（新块就绪后直接切换）和**Real Time Chunking (RTC)**。</p>
<p>在Kinetix上的关键结果表明，A2C2在存在推理延迟和长执行视野时，均能保持更高的成功率。随着延迟 d 增加或视野 H 变长，Naive和RTC的性能显著下降，而A2C2下降平缓。</p>
<p><img src="https://arxiv.org/html/2509.23224v1/figures/overall_solve_rate_delay.png" alt="Kinetix实验结果"></p>
<blockquote>
<p><strong>图5a</strong>：固定执行视野，平均成功率随推理延迟 d 的变化。A2C2（红色）始终优于Naive和RTC基线，即使在较大延迟下也能维持较高的成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.23224v1/figures/overall_solve_rate_execution_horizon_d1.png" alt="Kinetix实验结果"></p>
<blockquote>
<p><strong>图5b</strong>：固定延迟 d=1，平均成功率随执行视野 e 的变化。A2C2（红色）在不同视野长度下均表现稳健，而基线方法性能随视野变长而下降。</p>
</blockquote>
<p>具体而言，在延迟 d=4 时，A2C2相比Naive基线实现了近35个百分点的成功率提升；即使在视野 H=7 时，成功率仍保持在85%以上。相比RTC，在存在延迟时平均提升23个百分点，在长执行视野时平均提升7个百分点。</p>
<p>在LIBERO Spatial上的实验结果同样验证了方法的有效性。在存在延迟（d=10）和长视野（e=40）的严苛设置下，A2C2将成功率从Naive基线的64.4%提升至84.2%。即使在无延迟（d=0）但视野很长（e=50）的情况下，也将成功率从72.2%提升至81.6%。</p>
<p><img src="https://arxiv.org/html/2509.23224v1/figures/visualize_libero_spatial_delay.png" alt="LIBERO Spatial实验结果"></p>
<blockquote>
<p><strong>图6a</strong>：LIBERO Spatial上，固定执行视野e=40，成功率随推理延迟 d 的变化。A2C2在延迟下保持鲁棒。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.23224v1/figures/visualize_libero_spatial_horizon.png" alt="LIBERO Spatial实验结果"></p>
<blockquote>
<p><strong>图6b</strong>：LIBERO Spatial上，无推理延迟（d=0），成功率随执行视野 e 的变化。A2C2在不同视野下持续提升性能。</p>
</blockquote>
<p>论文未展示严格的组件消融实验，但通过在不同环境和基线上的成功验证了整体框架的有效性。校正头参数极少（Kinetix上0.31M，LIBERO上32M），相比基础VLA（如SmolVLA的450M）开销极小。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) <strong>形式化了VLA生成动作块时的推理延迟问题</strong>，明确了延迟、执行视野和动作块长度之间的关系及影响。2) <strong>提出了一个轻量级、插件式的异步动作块校正头（A2C2）</strong>，该模块通用，可应用于任何现成的VLA模型，无需重训基础策略，通过实时残差校正恢复闭环反应性。3) <strong>在动态任务和机器人操作基准上进行了广泛验证</strong>，证明该方法能显著提升在推理延迟和长执行视野下的任务成功率。</p>
<p>论文自身提到的局限性在于，尚未在更丰富的语言指令、分布外（OOD）设置以及比LIBERO Spatial更动态的任务上验证其可扩展性。</p>
<p>本工作对后续研究的启示在于：为大规模VLA模型在实时控制中的部署提供了一个实用的解决方案。它指出，与其单纯追求降低大模型延迟或改进块间平滑，引入一个轻量级的、高频运行的校正层是有效且高效的折中路径。这尤其适用于客户端-服务器架构，其中通信延迟可被纳入推理延迟一并处理，使得在利用云端强大VLA能力的同时，保证机器人在本地的响应能力成为可能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型因预测动作块而导致的推理延迟和长视野下反应性降低的问题，提出了一种轻量级实时校正方法A2C2。该方法通过一个校正头，在每个控制步骤结合最新观测、VLA预测的基础动作、动作块内位置特征及基础策略特征，对动作块进行实时时间感知校正，无需重新训练基础策略。在动态Kinetix任务套件和LIBERO Spatial上的实验表明，该方法能显著提升任务成功率（较RTC方法分别提升23%和7%），并在零延迟长视野任务中也增强了鲁棒性，且计算开销极小。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.23224" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>