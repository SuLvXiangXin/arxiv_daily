<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.05791" target="_blank" rel="noreferrer">2511.05791</a></span>
        <span>作者: Aniket Bera Team</span>
        <span>日期: 2025-11-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人抓取检测的主流方法是基于学习的方法，它们在大规模专家标注的抓取数据集上进行训练，虽然能在分布内数据上取得高准确率，但其性能受限于训练数据集的覆盖范围和质量。收集专家标注数据成本高昂，且模拟生成的抓取数据往往偏向于有限的抓取模式。因此，即使是最先进的监督学习方法，也难以泛化到新的物体类别、杂乱环境以及真实世界部署场景。近期工作尝试通过融入语义或多模态线索（如语言条件抓取或分割引导检测）来弥补这一差距，但它们仍然依赖于带标注数据的重新训练，继承了相同的可扩展性局限。</p>
<p>本文针对上述依赖大规模标注数据和有限泛化能力的核心痛点，提出了一个新视角：能否直接利用在互联网规模多模态数据上训练的大型视觉-语言模型（VLM）的推理和生成能力，在无需任何任务特定训练或微调的情况下，实现机器人抓取？VLM编码了来自海量图像-文本语料库的广泛视觉-语义知识，包括对物体可供性和人-物交互的隐式理解，这使其成为机器人抓取领域一个极具潜力的先验知识源。</p>
<p>本文的核心思路是：利用VLM根据单张RGB-D图像和引导性文本提示，生成一个描绘虚拟杆“刺穿”物体的目标图像以表示对握抓取轴，然后通过深度预测和分割将其提升至3D，最后通过几何对齐将生成的物体点云与观测到的物体点云进行配准，从而恢复出可执行的抓取位姿。</p>
<p><img src="https://arxiv.org/html/2511.05791v1/x1.png" alt="方法示意图"></p>
<blockquote>
<p><strong>图1</strong>：VLAD-Grasp方法概览。系统捕获目标物体图像，并配合序列化引导提示查询VLM，使其推理物体几何和可行抓取方式。VLM随后生成一个目标图像，其中一根虚拟杆“刺穿”物体，编码了对握抓取轴。该轴在3D空间中被重建并与观测场景对齐，产生可执行的抓取位姿。</p>
</blockquote>
<h2 id="方法详解">方法详解</h2>
<p>VLAD-Grasp是一个端到端的零样本抓取合成流程，输入为单张RGB-D观测图像（场景图像 (I_S) 和深度 (D_S)），输出为机器人可执行的抓取位姿。整体框架分为三个核心阶段：1) 基于VLM提示生成抓取目标图像；2) 将生成图像提升至3D并进行物体点云对齐；3) 抓取轴投影与抓取位姿计算。</p>
<p><img src="https://arxiv.org/html/2511.05791v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：VLAD-Grasp方法整体框架。捕获物体的RGB-D图像 ((I_S, D_S)) 并掩蔽背景干扰物。将RGB图像 (I_S) 连同结构化引导提示 (T^g_i) 提供给VLM，使其推理 ((R^g)) 物体几何并最终生成目标图像 (I_G)，其中目标抓取由一根穿过物体表面对握抓取点的杆表示。随后，预测生成图像 (I_G) 中物体的点云 (P^o_G)，并将其与原始图像 (I_S) 中物体的点云 (P^o_S) 进行匹配和对齐。</p>
</blockquote>
<p><strong>核心模块一：抓取图像生成</strong><br>该模块利用VLM的跨模态生成能力。作者设计了一个三步提示结构来引导VLM生成包含抓取轴（表示为虚拟杆 (r_G)）的目标图像 (I_G)。</p>
<ol>
<li><strong>初始约束提示</strong>：向模型提供掩蔽背景后的物体图像 (I_S) 和文本提示 (T_0^g)，其中包含期望抓取的约束条件（夹爪尺寸、配置、图像空间可见性等）。模型据此进行推理 (p_\theta(R^g | I_S, T_0^g))。</li>
<li><strong>中间提示生成</strong>：进一步用提示 (T_1^g) 查询模型，使其利用之前的推理 (R^g) 生成一个用于下一步的直接图像生成提示 (T_2^g)。</li>
<li><strong>约束增强的图像生成</strong>：将生成的提示 (T_2^g) 与额外的文本约束 (T_c^g) 和修复掩码 (M_S^b)（即背景掩码）结合，最终查询模型 (p_\theta(I_G | T_c^g, T_2^g, M_S^b, I_S)) 得到目标图像 (I_G)。用“杆”表示抓取有三个优势：将抓具体现为图像中的物理对象有助于模型基于其训练域内的概念进行生成；直杆隐式编码了两个抓取点关于物体表面是对握的；最小化的杆物体能防止模型注意力偏离任务。</li>
</ol>
<p><strong>核心模块二：物体对齐</strong><br>由于VLM生成图像 (I_G) 中的物体 (o_G) 在姿态和细节上可能与原始图像 (I_S) 中的物体 (o_S) 存在差异，需要对齐两个域中的物体以解释抓取轴。具体步骤：</p>
<ol>
<li><strong>点云构建</strong>：使用分割模型获取生成图像中物体和杆的掩码 (M_G^o, M_G^r)，以及原始图像中物体的掩码 (M_S^o)。使用单目深度估计模型预测生成图像的深度 (D_G)，结合掩码和深度信息，分别构建生成图像中物体和杆的点云 (P_G^o, P_G^r)，以及原始图像中物体的点云 (P_S^o)。</li>
<li><strong>基于PCA和对应点云优化的对齐</strong>：假设物体整体形状一致，采用对局部特征不敏感但对全局形状敏感的点云配准方法。首先对两个物体点云 (P_S^o, P_G^o) 进行均值中心化，计算协方差矩阵并进行特征分解，得到各自前三个主成分（特征向量 (\mathbf{v}<em>i^{o_S}, \mathbf{v}<em>i^{o_G}) 及对应特征值 (\lambda_i^{o_S}, \lambda_i^{o_G})）。最优旋转 (\mathbf{R}</em>{S\leftarrow G}^*) 通过最小化对应点云损失 (\mathcal{L})（使用Chamfer距离）在候选旋转矩阵中搜索得到，候选旋转由特征向量和特征值缩放后组合产生（考虑特征方向正负的8种组合）。最终的对齐变换 (\mathbf{T}</em>{S\leftarrow G}) 由去中心化、最优旋转和重新中心化矩阵相乘构成。</li>
</ol>
<p><strong>核心模块三：抓取投影</strong><br>得到对齐变换 (\mathbf{T}<em>{S\leftarrow G}) 后，将杆的点云 (P_G^r) 变换到原始笛卡尔空间：(P_S^r = \mathbf{T}</em>{S\leftarrow G}(P_G^r)^\top)。此时，(P_S^r) “刺穿” (P_S^o)，表示一个6自由度的抓取位姿。为适配实验的二维夹爪抓取矩形表示，将 (P_S^r) 投影回原始图像空间得到杆掩码 (M_S^r)。通过分析 (M_S^r) 沿其最佳拟合线的间断处（即杆穿过物体的位置）来确定可行的抓取位置，并根据间断长度、与物体掩码 (M_S^o) 的重叠度（IoU）等启发式规则选择最优抓取点，进而计算夹爪位姿和抓取宽度。</p>
<p><strong>创新点</strong>：1) <strong>零样本抓取推理</strong>：通过精心设计的VLM提示，完全摆脱了对人工标注抓取数据集的依赖。2) <strong>生成与观测数据的几何一致性对齐</strong>：创新地结合了单目深度预测、基于PCA的配准和无对应点云优化，弥合了合成目标图像与物理场景之间的域差距。3) <strong>无需重新训练即可在真实硬件上部署</strong>：验证了基础模型先验可以直接迁移到物理抓取任务。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在Cornell（真实）和Jacquard（模拟）抓取数据集上评估。使用GPT-5作为VLM，ML Depth Pro进行深度估计，Segment Anything Model (SAM)进行分割。<br><strong>基线方法</strong>：GR-ConvNet, GG-CNN, SE-ResUNet, GraspSAM, LGD（带查询和不带查询）。除LGD使用作者提供的检查点外，其他基线均在数据集上训练。<br><strong>评估指标</strong>：抓取成功率（SR），即预测抓取矩形与至少一个真实标注的IoU ≥ 25%。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>零样本抓取性能</strong>（表I）：在Cornell数据集上，VLAD-Grasp取得了 <strong>91.43%</strong> 的成功率，优于所有基线方法（领先约5%）。在Jacquard数据集上，取得 <strong>85.43%</strong> 的成功率，仅次于SE-ResUNet（88.14%），但SE-ResUNet是在该数据集上训练的，而VLAD-Grasp是零样本的。值得注意的是，同样声称零样本的LGD方法在两个数据集上表现均不佳（Cornell: 37.98%, Jacquard: 24.40%）。</li>
</ul>
<p><strong>表I</strong>：所有方法在Cornell和Jacquard数据集上的成功率（SR）。VLAD-Grasp在Cornell上最优，在Jacquard上具有竞争力。</p>
<p><strong>定性结果</strong>：</p>
<p><img src="https://arxiv.org/html/2511.05791v1/x3.png" alt="定性对比"></p>
<blockquote>
<p><strong>图3</strong>：不同物体上抓取检测的定性对比。与基线方法相比，VLAD-Grasp（Ours）在不同物体类型上检测到了更多成功的抓取。先前的方法常产生粗糙或未对齐的抓取，而本文方法生成的抓取准确、定位良好且与物体几何对齐。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>VLM选择与提示结构</strong>：使用GPT-5并采用完整的三步提示结构取得了最佳性能。若使用其他VLM（如GPT-4V）或简化提示（如直接生成），性能会下降，这证明了强大VLM和结构化提示对于复杂几何推理的必要性。</li>
<li><strong>跨域点云对齐</strong>：对比了基于PCA的对齐与直接使用ICP等方法。基于PCA的方法在物体存在非刚性形变或细节差异时更加鲁棒，是性能提升的关键。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种端到端的<strong>零样本</strong>抓取合成流程VLAD-Grasp，仅需单张RGB-D观测即可映射到可执行机器人抓取，无需任何额外训练或专家监督。</li>
<li>设计了一种结合单目深度预测、基于PCA的配准和无对应点云优化的<strong>几何对齐流程</strong>，能够稳定地从生成图像中恢复抓取位姿。</li>
<li>在标准数据集上取得了与最先进监督方法<strong>相当或更优</strong>的性能，并在真实机器人上展示了零样本泛化能力，凸显了视觉-语言基础模型作为机器人操作强大先验的潜力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法目前主要针对<strong>对握抓取</strong>（二指夹爪），对于其他抓取模式（如包络抓取）的泛化能力尚未探索。此外，依赖于大型VLM和深度估计模型会带来一定的计算成本。</p>
<p><strong>启示</strong>：本工作表明，大规模预训练的基础模型（如VLMs）内部蕴含的丰富视觉-语义和几何知识，可以直接作为机器人感知与规划的强先验，绕过昂贵的数据标注和任务特定训练瓶颈。这为机器人学领域利用通用人工智能模型实现快速、灵活的技能获取开辟了新途径。同时，如何更鲁棒、高效地弥合基础模型生成内容与物理世界之间的“<strong>几何鸿沟</strong>”，是未来值得深入研究的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>VLAD-Grasp旨在解决机器人抓取依赖大规模专家注释和重新训练、泛化能力有限的核心问题。该方法提出一种基于视觉-语言模型的零样本抓取检测技术：首先提示大模型生成以直杆“刺穿”对象表示对握抓取的目标图像；然后预测深度和分割将其提升至3D；最后通过主成分分析（PCA）和无对应优化对齐点云以恢复可执行抓取姿态。实验显示，在Cornell和Jacquard数据集上，其性能与最先进监督模型竞争或更优，并在Franka Research 3机器人上实现了对新现实物体的零样本泛化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.05791" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>