<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.02851" target="_blank" rel="noreferrer">2512.02851</a></span>
        <span>作者: Dzmitry Tsetserukou Team</span>
        <span>日期: 2025-12-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉可通行性估计对于自主导航至关重要。当前主流方法主要包括：1）自监督学习方法，通过机器人自身交互获取监督信号，但需要大量环境交互数据，且严重依赖特定机器人平台（embodiment），难以泛化到新平台；2）基于大型视觉语言模型（VLM）的方法，通过人工提示词（prompt）从RGB图像推断可通行性，但其输出质量对提示词敏感，且通常只输出可通行性地图，需要依赖外部规划器生成轨迹，导致延迟高、流程复杂；3）现有的扩散模型应用于运动规划时，通常依赖预计算的可通行性代价图或外部估计，并未将可通行性推理与轨迹生成统一在一个模型中。</p>
<p>本文针对的关键痛点是：现有方法无法从一个单一的RGB图像中，以端到端、轻量级的方式，联合预测可通行性并生成一条物理上可行的轨迹，同时实现跨异构机器人平台（如四足机器人和无人机）的泛化。本文提出了SwarmDiffusion，其核心思路是：利用一个条件扩散模型，在免规划器生成的合成轨迹监督下，联合学习可通行性预测和轨迹去噪，并通过一个紧凑的机器人状态向量进行调节，从而实现无需提示词、免演示学习、且对机器人平台不敏感的统一导航框架。</p>
<h2 id="方法详解">方法详解</h2>
<p>SwarmDiffusion的整体框架包含两个紧密耦合的组件：可通行性学生模型（Traversability Student）和基于扩散的轨迹生成器（Diffusion-based Trajectory Generation）。输入为单帧RGB图像、机器人本体状态向量、本体类型以及编码了起点-目标位置的二通道张量。输出为预测的可通行性热图（64x64）和一条可行的轨迹热图（64x64）。</p>
<p><img src="https://arxiv.org/html/2512.02851v3/images/Tieser-architect.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SwarmDiffusion整体架构。左侧为可通行性学生模型，使用冻结的DINO-v2 ViT编码图像，机器人状态经MLP生成FiLM参数调制视觉特征，最终解码出可通行性图。右侧为扩散轨迹生成器，一个UNet以噪声轨迹、起点-目标图、预测可通行性图为空间条件，以FiLM调制后的视觉token为跨注意力条件，进行多步去噪，生成干净轨迹。</p>
</blockquote>
<p><strong>核心模块一：免规划器的轨迹构建与监督</strong>。为了训练扩散模型而不依赖专家演示或外部规划器，论文提出一种合成轨迹生成流程：在由教师VLM（AnyTraverse）预测的高可通行性区域内，随机采样起点（靠近图像底部）和远处目标点；中间路径点通过高斯扰动随机采样，并用贝塞尔曲线进行平滑插值；最终将曲线栅格化为64x64的细线轨迹热图，并施加轻微高斯模糊。这为扩散模型提供了多样、平滑且与环境几何一致的监督信号。</p>
<p><strong>核心模块二：可通行性学生模型</strong>。该模块负责从视觉输入预测平台特定的可通行性。采用冻结的DINO-v2 ViT作为视觉骨干网络，附加一个可训练的ViT适配器（ViT-adapter）进行微调。机器人本体状态（6维位姿）通过一个MLP生成FiLM参数，用于调制ViT中间层的特征图（公式10），从而将机器人的动态约束注入视觉表示。调制后的特征经过一个轻量级卷积解码器，上采样并输出64x64的可通行性预测图。其监督信号来自教师VLM生成的可通行性图，使用L1蒸馏损失（公式24）。</p>
<p><strong>核心模块三：条件扩散轨迹生成器</strong>。采用标准UNet架构作为去噪网络。在每个去噪步，UNet的输入是拼接而成的4通道空间条件图：噪声轨迹热图、起点-目标二值图、预测的可通行性图（公式11）。同时，来自可通行性学生模型的FiLM调制视觉token被输入到UNet的跨注意力层，作为token条件。UNet被训练去预测添加到干净轨迹上的噪声，使用标准DDPM目标（公式13）。在推理时，从纯噪声开始，迭代应用UNet去噪，最终生成与场景几何、可通行性及机器人本体约束一致的轨迹热图。</p>
<p><strong>创新点与损失函数</strong>：与仅以可通行性图为条件的基线方法相比，SwarmDiffusion的核心创新在于通过FiLM机制和跨注意力，将机器人本体状态深度集成到特征级条件中，实现了真正的本体感知（embodiment-aware）规划。训练采用多任务损失（公式26），包括：1) 扩散去噪损失；2) 方向性前向流损失，惩罚轨迹中逆向于起点到目标方向的片段（公式19）；3) 可通行性期望奖励，鼓励轨迹落在高可通行性区域（公式22）；4) 质量正则化损失，确保轨迹的几何合理性（如薄度、平滑度）；5) 可通行性蒸馏损失。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了两个机器人平台进行验证：Unitree Go1四足机器人和自定义的8英寸无人机。数据集包含从两个平台采集的共8000对同步RGB图像及对应的合成轨迹。实验平台包括高保真Unity仿真环境和真实世界。评估使用了VICON动捕系统进行精确位姿估计。</p>
<p><strong>Baseline方法</strong>：由于没有现成的联合学习方法，论文实现了一个对照基线。该基线使用相同的条件扩散UNet，但仅以VLM生成的可通行性图和起点-目标图作为空间条件，<strong>不包含</strong>机器人状态向量或特征级的可通行性语义（即没有可通行性学生模型和FiLM调制）。该基线使用A*规划器生成的专家轨迹进行监督。</p>
<p><strong>关键定量结果</strong>：在室内导航任务中，SwarmDiffusion在两个平台上均实现了<strong>80-100%的成功率</strong>，单次推理时间仅为<strong>0.09秒</strong>。模型展现出强大的跨平台泛化能力：仅使用<strong>500个</strong>来自新机器人（目标平台）的额外视觉样本进行状态向量适配器微调（保持网络主干冻结），即可成功适应新平台。</p>
<p><strong>定性结果分析</strong>：论文提供了大量定性对比图，展示了SwarmDiffusion与基线方法在轨迹和可通行性预测上的差异。</p>
<p><img src="https://arxiv.org/html/2512.02851v3/images/results/drone_baseline_traj_1.png" alt="无人机基线轨迹示例"><br><img src="https://arxiv.org/html/2512.02851v3/images/results/drone_baseline_traj_2.png" alt="无人机基线轨迹示例"></p>
<blockquote>
<p><strong>图4-5</strong>：基线方法（仅用可通行性图条件）为无人机生成的轨迹（红色）。轨迹有时会穿过障碍物（如椅子腿），或规划出不符合无人机飞行特点的不安全路径。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.02851v3/images/results/drone_traj_1.png" alt="SwarmDiffusion无人机轨迹示例"><br><img src="https://arxiv.org/html/2512.02851v3/images/results/drone_traj_2.png" alt="SwarmDiffusion无人机轨迹示例"></p>
<blockquote>
<p><strong>图12-13</strong>：SwarmDiffusion为无人机生成的轨迹（绿色）。轨迹能更好地避开障碍，并倾向于在开阔空间（如门框上方、房间中央）规划路径，更符合无人机的飞行约束。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.02851v3/images/results/dog_traj_overlay_1.png" alt="四足机器人轨迹与可通行性叠加示例"><br><img src="https://arxiv.org/html/2512.02851v3/images/results/dog_trav_overlay_1.png" alt="四足机器人轨迹与可通行性叠加示例"></p>
<blockquote>
<p><strong>图20, 24</strong>：SwarmDiffusion为四足机器人生成的轨迹（粉色）与预测的可通行性图（蓝色高亮）叠加展示。轨迹精确地贴合在预测的可通行区域（地面）内，避开不可通行区域（家具）。</p>
</blockquote>
<p><strong>消融实验</strong>：论文进行了消融研究，评估了方向性损失、可通行性奖励、质量正则化等各个损失组件的重要性。结果表明，完整的损失组合对于生成安全、平滑、方向正确且贴合可通行区域的轨迹至关重要。特别是，移除方向性损失会导致轨迹出现回环；移除可通行性奖励会使轨迹冒险进入低可通行性区域。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) <strong>统一的端到端生成导航</strong>：首次提出了一个扩散模型框架，能够从单张RGB图像联合生成可通行性地图和可行轨迹，充当了全局与局部规划器的角色，且无需专家演示。2) <strong>跨机器人平台的本体无关规划</strong>：引入了轻量级状态向量适应机制，仅需数百样本即可泛化到新机器人平台，同时保持网络权重冻结。3) <strong>超越经典优化的鲁棒性</strong>：模型隐式编码场景几何与可通行性，产生全局连贯的轨迹假设，避免了经典优化器（如纯APF规划器）易陷入局部最小值和振荡的问题。</p>
<p><strong>局限性</strong>：论文提到，由于轨迹生成在图像空间进行，将路径投影到3D时（尤其是无人机俯视视角或长距离规划时）会引入轻微的几何不确定性。为此，论文在真实世界部署中引入了一个轻量级人工势场（APF）进行局部修正，作为安全层。</p>
<p><strong>后续启示</strong>：SwarmDiffusion展示了一种无需复杂提示工程或大量真实机器人交互数据，即可学习跨平台导航策略的可行路径。其轻量级适配机制为快速部署到新机器人平台提供了思路。将可通行性推理与轨迹生成统一在一个生成模型中，为构建更紧密耦合、更高效的“感知-规划”一体化系统指明了方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决现有视觉导航方法依赖手工提示、泛化性差且规划速度慢的问题，提出SwarmDiffusion模型。该模型是一种端到端扩散模型，通过免规划器的轨迹构建流程（随机航点采样、贝塞尔平滑与正则化），并利用VLM监督和紧凑的机器人状态调节，直接从单张RGB图像联合预测可通行性并生成可行轨迹。实验表明，该方法在室内环境及不同机器人平台上实现了80-100%的导航成功率，推理仅需0.09秒，且仅用500个样本即可适应新机器人。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.02851" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>