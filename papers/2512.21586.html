<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.21586" target="_blank" rel="noreferrer">2512.21586</a></span>
        <span>作者: Dongbin Zhao Team</span>
        <span>日期: 2025-12-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前从视频中模仿学习的主流方法主要分为两类：逆强化学习方法和监督式模仿学习方法。逆强化学习方法试图从视频中提取奖励信号，然后通过强化学习进行模仿。然而，训练额外的奖励预测网络和RL价值网络都需要对观测空间进行大量探索，这导致其样本效率通常低于基于固定专家奖励的传统RL方法。另一类监督式方法旨在通过环境交互预测缺失的专家动作，并进行行为克隆。这类方法对环境动力学的了解要求较低，在状态模仿中展现了降低环境样本依赖的潜力。但在视觉模仿任务中，解析观测和预测专家动作的难度急剧增加，现有方法常遇到性能瓶颈，无法学到接近专家水平的策略。本文针对的核心痛点是：在仅能访问视频作为监督信号的视觉策略学习中，能否平衡有效性与样本效率？本文提出了一种新视角：通过潜在表示进行行为克隆，利用离线预训练从视频中提取丰富知识，以进行高效的在线适应。其核心思路是：通过自监督任务从视频帧中提取与动作相关的潜在特征，并利用基于动力学的无监督目标预测帧间潜在动作；在线阶段，利用收集的无奖励交互数据微调潜在动作并将其对齐到真实动作空间进行行为克隆，克隆的策略反过来丰富交互经验，形成高效的迭代策略改进。</p>
<h2 id="方法详解">方法详解</h2>
<p>BCV-LR的整体框架包含离线预训练和在线微调两个阶段。输入是专家视频帧序列，输出是可执行的策略。离线阶段，从视频中提取潜在特征并预测潜在动作；在线阶段，利用环境交互微调对齐动作并克隆策略，最终策略由特征编码器、潜在策略和动作解码器构成。</p>
<p><img src="https://arxiv.org/html/2512.21586v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：BCV-LR不同阶段的训练目标。首先在视频上预训练自监督特征编码器f。基于潜在特征，使用可训练的世界模型w和潜在动作预测器p，通过基于动力学的无监督目标优化，获取视频帧间的潜在动作。在线阶段，利用收集的无奖励转移数据，通过预训练的世界模型w微调潜在动作，并通过潜在动作解码器d将其对齐到真实动作空间。同时，训练一个克隆潜在动作的潜在策略π，它共享特征编码器f和动作解码器d与环境交互，从而丰富收集的数据以进行进一步的潜在动作微调，形成迭代改进。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li>**自监督特征编码器 (f)**：作用是从高维视频输入中提取与决策相关的信息。其训练是任务自适应的：对于视觉复杂、动力学相对简单的任务（如Procgen），采用对比学习目标（对齐同一观测的不同随机偏移图像）并结合自重建任务；对于部分可观测、动力学复杂的任务（如DMControl），则采用基于原型的时序关联任务（使用Sinkhorn-Knopp算法对齐时序相邻的观测）。损失函数如论文公式(1)所示，包含对比损失和重建损失。</li>
<li>**潜在动作预测器 (p) 与世界模型 (w)**：在预训练的特征基础上，预测器p用于生成连续帧对之间的潜在动作z_i^v。为了以无监督方式获取z_i^v，引入世界模型w，其目标是根据当前特征s_i^v和预测的（量化后）潜在动作z_i^{vq}重建下一帧特征s_{i+1}^v。为避免平凡解（如p直接复制下一帧特征），对z_i^v进行向量量化离散化。训练损失为重建损失 ℒ_la = ||w(s_i^v, z_i^{vq}) - s_{i+1}^v||^2，p和w通过此损失联合优化。</li>
<li><strong>潜在动作解码器 (d) 与在线微调</strong>：在线阶段，智能体与环境交互收集带真实动作的转移数据。潜在动作预测器p和新增的解码器d在这些数据上进行微调与对齐。学习目标包含两部分：动作预测损失（解码器d输出的动作分布与真实动作的交叉熵或MSE损失），以及由预训练世界模型w提供的未来重建损失，如公式(3)所示。这确保了p在适应非专家动作标签时，仍保持对环境动力学的理解。</li>
<li><strong>潜在策略 (π) 与行为克隆</strong>：通过共享预训练的编码器f和解码器d，只需训练一个潜在策略π，将编码后的专家观测特征s_i^v映射到预测的潜在动作z_i^v。行为克隆损失为 ℒ_bc = ||π(s_i^v) - z_i^v||^2。策略π输出的潜在动作经解码器d解码后执行。克隆的策略以更优性能交互，收集更高质量的数据，进而促进潜在动作的进一步微调，形成迭代改进循环。</li>
</ol>
<p>与现有方法相比，BCV-LR的创新点具体体现在：1）<strong>引入潜在动作作为中介</strong>：通过基于动力学的无监督目标从视频中提取离散化的潜在动作，避免了直接从像素预测真实动作的困难。2）<strong>离线-在线迭代优化机制</strong>：预训练的模型为在线学习提供了良好的初始化；在线微调不仅对齐动作空间，还利用克隆策略收集的新数据持续改进潜在动作预测和策略本身，极大地提升了样本效率。3）<strong>灵活的自监督特征学习</strong>：可根据任务特性（视觉复杂性、时序依赖性）选择或设计最合适的自监督任务来预训练特征编码器。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了三个基准测试集：16个离散控制任务来自Procgen基准，12个连续控制任务来自Deepmind Control Suite和Metaworld。对比的基线方法包括：ILV方法UPESV、LAIFO、ILPO、BCO；以及RL方法（提供环境奖励）LAPO、PPO（Procgen）、DrQv2、TACO（DMControl）。</p>
<p><img src="https://arxiv.org/html/2512.21586v1/x1.png" alt="结果摘要"></p>
<blockquote>
<p><strong>图1</strong>：BCV-LR在仅允许10万次交互的情况下，在离散任务“Bossfight”和连续任务“reacher_hard”上达到了专家级别的策略性能，超越了先进的ILV和RL基线，展示了无专家动作或奖励的样本高效视频模仿学习能力。</p>
</blockquote>
<p>关键实验结果如下：<br>在Procgen的16个任务上，仅允许10万次环境交互，BCV-LR取得了平均13.8分的性能（经视频归一化后的平均分为0.79），显著优于其他ILV基线（UPESV: 9.0/0.58， BCO: 4.8/0.38， ILPO: 2.1/0.22， LAIFO: 2.2/0.22），甚至超过了使用专家视频辅助的RL方法LAPO（6.8/0.48）和仅使用奖励的PPO（2.3/0.22）。在多个任务上（如Bigfish, Bossfight）接近或达到专家水平。</p>
<p><img src="https://arxiv.org/html/2512.21586v1/x3.png" alt="DMControl训练曲线"></p>
<blockquote>
<p><strong>图3</strong>：DMControl上ILV方法的在线训练曲线。BCV-LR能够高效利用环境样本，在5万步（甚至在某些任务上2万步）内学习到有效策略。</p>
</blockquote>
<p>在DMControl的8个连续任务上，BCV-LR同样表现出色。例如在<code>point_mass_easy</code>和<code>reacher_hard</code>任务上，BCV-LR分别获得800分和900分，远超所有其他ILV基线（LAIFO、BCO、UPESV均低于300分），并与需要环境奖励的先进RL方法TACO、DrQv2性能相当甚至更优。</p>
<p><img src="https://arxiv.org/html/2512.21586v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：在Procgen任务上的消融研究。移除世界模型（w）的未来重建损失（<code>Ours w/o w</code>）或移除潜在动作的向量量化（<code>Ours w/o VQ</code>）均会导致性能显著下降，验证了这两个组件对于学习有意义潜在动作的重要性。</p>
</blockquote>
<p>消融实验总结了每个核心组件的贡献：</p>
<ol>
<li><strong>世界模型（w）的未来重建损失</strong>：移除后（<code>Ours w/o w</code>）性能大幅下降，说明该损失对于约束潜在动作包含动力学信息至关重要。</li>
<li><strong>潜在动作的向量量化（VQ）</strong>：移除后（<code>Ours w/o VQ</code>）性能也显著降低，验证了离散化能迫使模型寻找跨转移对的共性，避免平凡解。</li>
<li><strong>在线迭代微调</strong>：仅进行离线预训练而不进行在线微调（<code>Ours w/o online</code>）性能最差，证明了在线交互对齐和策略迭代改进的必要性。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.21586v1/x5.png" alt="视频数据效率"></p>
<blockquote>
<p><strong>图5</strong>：视频数据效率分析。BCV-LR仅需约5-10个专家轨迹（相当于约1-2万帧）的预训练视频数据，性能即趋于饱和，显示出对视频数据的高效利用。</p>
</blockquote>
<p>其他分析实验表明，BCV-LR对预训练视频数据量需求很低（约5-10条轨迹即饱和），且具备一定的跨任务适应潜力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了BCV-LR框架，首次证明了仅使用视频作为专家监督信号即可实现<strong>样本高效的视觉策略学习</strong>，无需专家动作或奖励。2）设计了一种通过<strong>潜在动作表示</strong>进行离线预训练和在线迭代微调的新范式，有效平衡了模仿性能与样本效率。3）在包含离散和连续的28项挑战性视觉任务上进行了广泛实验，结果表明BCV-LR在样本效率上超越了当前先进的ILV和RL基线。</p>
<p>论文自身提到的局限性包括：1）<strong>计算成本</strong>：离线预训练阶段需要额外的计算资源。2）<strong>任务通用性</strong>：当前的自监督任务设计可能无法覆盖所有类型的视觉控制问题。3）<strong>潜在动作离散化</strong>：向量量化可能对某些需要精细连续动作的任务引入近似误差。</p>
<p>对后续研究的启示：1）证明了从无动作视频中提取结构化、可执行知识是可行且高效的路径。2）<strong>离线预训练与在线微调相结合</strong>的范式为解决样本效率问题提供了有力工具。3）<strong>潜在表示与模型预测</strong>的结合，为处理高维观测和复杂动力学开辟了新方向。未来工作可以探索更强大的自监督表征学习方法、更高效的潜在动作空间构建技术，以及将该框架扩展到更复杂的多模态或现实世界任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对从无动作标签的视频中进行高效模仿学习（ILV）的挑战，提出BCV-LR框架。其核心方法是通过自监督任务从视频提取动作相关潜在特征，并利用基于动态的无监督目标预测帧间潜在动作；随后在线微调，将潜在动作与真实动作空间对齐以进行行为克隆，形成迭代的策略改进循环。实验表明，在离散与连续控制任务中，BCV-LR仅需极少量交互即可达到专家级性能，在24/28的任务上样本效率超越了现有ILV基线和有环境奖励的强化学习方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.21586" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>