<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.02483" target="_blank" rel="noreferrer">2505.02483</a></span>
        <span>作者: Huang, Changxin, Liang, Junyang, Chang, Yanbin, Xu, Jingzhao, Li, Jianqiang</span>
        <span>日期: 2025/05/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人技能学习领域，强化学习（RL）因其通过最大化累积奖励来优化策略的能力而成为主流方法。由于机器人运动涉及多种约束，奖励函数通常由多个组件构成。现有方法通常将所有奖励组件不加区分地求和，以此优化价值函数和策略。然而，这种在策略优化中统一包含所有奖励组件的方式被认为是低效的，限制了机器人的学习性能。为了改进，已有工作提出了混合奖励架构（HRA），通过学习每个奖励组件对应的独立价值函数来促进学习。后续的混合动态策略梯度（HDPG）方法进一步引入了动态优先级，在策略优化过程中调整各奖励分支的贡献，使机器人能够先学习“更简单”的组件。但HDPG方法严重依赖人类专家经验来设计动态权重计算规则，且相同的规则难以保证在不同机器人任务中都具有竞争力。</p>
<p>本文针对动态权重规则手工设计困难这一具体痛点，提出了自动化混合奖励调度（AHRS）的新视角，即利用大语言模型（LLM）的语言指令能力来自动生成和调度动态权重规则。其核心思路是：在训练前，利用LLM根据任务描述自动构建一个动态权重规则库；在训练中，基于对各奖励分支策略性能的评估生成语言提示，由LLM从规则库中选择合适的规则来计算各分支权重，从而动态调整策略优化的重点，实现渐进式学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>AHRS框架旨在将多分支价值网络与LLM的动态奖励调整能力相结合。整体流程如算法1及图1所示，输入包括初始策略、任务描述、环境信息和奖励函数代码，输出为训练好的策略。</p>
<p><img src="https://arxiv.org/html/2505.02483v1/x1.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：提出的自动化混合奖励调度（AHRS）框架概述。它包括多分支价值网络、动态权重规则库的构建、规则的选择以及辅助奖励函数的生成。</p>
</blockquote>
<p><strong>整体框架与核心模块</strong>：</p>
<ol>
<li><strong>奖励分解与多分支价值网络</strong>：首先将环境的总奖励 $r_{total}$ 分解为 $K$ 个子奖励组件（如扭矩奖励、角速度奖励），表示为向量形式。为每个奖励组件配备一个独立的价值函数分支，构成多分支价值网络。每个分支学习对应奖励组件的优势函数 $A_{t,k}$。</li>
<li><strong>动态策略梯度</strong>：AHRS继承了HDPG的动态权重思想。策略梯度计算如公式(3)所示：$\nabla_{\theta}J(\pi_{\theta}) \approx \mathbb{E}<em>{s_t,a_t}\sum</em>{k=1}^{K} I_k \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)$，其中 $I_k = w_k A_k$。关键创新在于权重 $w_k$ 的计算方式。</li>
<li><strong>规则库构建（自动化核心一）</strong>：为解决手工设计规则的问题，提出基于语言指令的规则生成方法。在RL训练开始前，将机器人任务描述 $T_t$、环境信息 $T_e$、各奖励组件代码 $C_r$ 以及HDPG中的规则示例 $E_{\text{hdpg}}$ 作为提示输入给LLM（公式4）。LLM利用其推理和生成能力，输出一个包含多条动态权重计算数学规则及解释的规则库 $\mathcal{B}^n$。</li>
<li><strong>自动化混合奖励调度（自动化核心二）</strong>：在训练过程中（例如每100轮），评估当前策略在每个奖励分支上的性能（如回报的均值、方差），将这些评估结果 $\mathbf{S}^R_l$ 转化为文本。将任务描述 $T_t$、环境信息 $T_e$、奖励代码 $C_r$、规则库 $\mathcal{B}^n$ 以及历史性能队列 $\mathbf{S}^R_L$ 共同构成提示，输入给LLM。LLM根据提示选择最合适的规则 $B_{\text{selected}}$，该规则再基于性能数据 $\mathbf{S}^R_l$ 计算出一组权重向量 $[w_1, ..., w_K]$，用于当前轮次的策略梯度更新。</li>
<li><strong>辅助奖励生成（可选增强）</strong>：为进一步提升训练效率，受Eureka工作启发，在训练前额外使用LLM，输入任务描述、环境代码、原始奖励函数代码及框架优化目标，让其设计一个旨在促进技能获取的辅助奖励组件 $r_a$。该辅助奖励作为一个额外的奖励分支加入多分支价值网络。</li>
</ol>
<p><strong>创新点</strong>：与现有方法（如HDPG）相比，AHRS的核心创新在于利用LLM完全自动化了动态权重调度中两个最依赖专家经验的环节：1）<strong>规则设计</strong>：从手工编写变为LLM基于任务上下文自动生成多样化的规则库；2）<strong>规则应用</strong>：从固定应用单一规则变为LLM根据实时训练性能动态选择最合适的规则。这使系统能更灵活、精细地调整学习重点。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在六个高自由度机器人仿真任务上评估AHRS：AnymalTerrain（四足地形行走）、Ant（蚂蚁行走）、ShadowHand（灵巧手操作）、Quadcopter（四旋翼控制）、AllegroHand（另一灵巧手）和Cassie（双足行走）。实验平台基于PPO算法。对比的基线方法包括：标准PPO（将所有奖励求和）、HD-PPO（HDPG方法在PPO上的实现，使用人工设计的固定规则）。本文还设置了消融实验：AHRS w/o A（不使用LLM生成的辅助奖励）和完整的AHRS。</p>
<p><img src="https://arxiv.org/html/2505.02483v1/x2.png" alt="累积奖励对比表"></p>
<blockquote>
<p><strong>表1</strong>：六个任务上的累积奖励对比（均值±标准差）。完整AHRS方法在所有任务上均取得了最佳性能（加粗数字）。</p>
</blockquote>
<p><strong>关键定量结果</strong>：如表1所示，完整AHRS方法在所有六个任务上的平均累积奖励均优于基线。具体而言，相较于PPO基线，AHRS平均性能提升约6.48%；相较于HD-PPO方法，平均提升约5.52%。即使在未使用辅助奖励的消融版本（AHRS w/o A）下，性能也普遍优于或与HD-PPO相当，证明了自动化规则调度的有效性。</p>
<p><img src="https://arxiv.org/html/2505.02483v1/x3.png" alt="AnymalTerrain任务学习曲线"></p>
<blockquote>
<p><strong>图2</strong>：AnymalTerrain任务的学习曲线。AHRS方法收敛更快，且最终性能显著高于PPO和HD-PPO。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.02483v1/x4.png" alt="Ant任务学习曲线"></p>
<blockquote>
<p><strong>图3</strong>：Ant任务的学习曲线。AHRS展示了更稳定的学习过程和更高的样本效率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.02483v1/x5.png" alt="ShadowHand任务学习曲线"></p>
<blockquote>
<p><strong>图4</strong>：ShadowHand任务的学习曲线。AHRS能取得明显更高的最终奖励。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.02483v1/x6.png" alt="规则选择与权重动态变化"></p>
<blockquote>
<p><strong>图5</strong>：在AnymalTerrain任务训练过程中，由LLM选择的规则以及各奖励分支权重的动态变化情况。展示了AHRS框架根据学习阶段自适应调整学习重点的能力。</p>
</blockquote>
<p><strong>消融实验分析</strong>：比较AHRS w/o A和完整AHRS的结果可以看出，LLM生成的辅助奖励组件在大多数任务（AnymalTerrain, Ant, ShadowHand, AllegroHand）上带来了进一步的性能提升，验证了其有效性。图5直观展示了在训练过程中，LLM如何根据策略性能动态切换权重计算规则，以及各奖励分支的权重如何随之变化，体现了“渐进式学习”的调度思想。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了自动化混合奖励调度（AHRS）框架，首次利用大语言模型（LLM）自动化了强化学习中动态奖励权重的生成与调度过程，减轻了对人类专家经验的依赖。</li>
<li>设计了一种基于语言指令的规则库构建方法，以及一种基于策略评估的提示生成方法，使LLM能够理解机器人任务上下文并做出合理的规则选择。</li>
<li>在多个复杂的高自由度机器人技能学习任务上验证了方法的有效性，性能显著超过标准PPO和依赖人工规则的HD-PPO方法。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，该方法的性能在一定程度上依赖于所用LLM的质量和推理能力。此外，规则库的构建虽然自动化，但仍需在训练前进行一次生成，存在一定的计算成本。</p>
<p><strong>后续启示</strong>：本研究展示了LLM在自动化强化学习流程（特别是奖励工程）方面的巨大潜力。未来的工作可以探索将LLM更深层次地集成到RL训练循环中，例如用于动态调整环境参数、生成课程学习序列或提供高层策略指导，进一步推动机器人自主技能学习的发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对高自由度机器人技能学习中，多奖励组件求和优化效率低下的问题，提出基于大型语言模型（LLM）的自动混合奖励调度（AHRS）框架。该框架采用多分支值网络对应不同奖励组件，通过LLM生成规则动态调整各组件权重，实现渐进式学习。实验表明，AHRS在多个任务中平均性能提升6.48%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.02483" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>