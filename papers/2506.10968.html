<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.10968" target="_blank" rel="noreferrer">2506.10968</a></span>
        <span>作者: Angjoo Kanazawa Team</span>
        <span>日期: 2025-06-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人操作领域，行为克隆（BC）是教授机器人技能的主流范式，它通过模仿人类演示来学习策略，无需手工设计奖励函数。然而，现有的BC系统通常使用固定视角的相机（如外部固定相机或腕部相机）进行感知，这在需要大范围操作的任务中存在局限：固定外部相机因分辨率有限而难以执行精细操作，腕部相机则无法主动搜索视野外的目标。</p>
<p>本文针对“如何让机器人像人一样主动‘看’以服务于‘动’”这一具体痛点，提出了一个新视角：将注视行为视为一种为行动服务而自然涌现的能力，而非预先定义或演示的行为。核心思路是：通过一个行为克隆与强化学习（BC-RL）的联合训练循环，让机械眼球学习注视策略，其奖励信号来源于基于该注视视角的机械臂行为克隆策略的成功率，从而实现手眼协调的自然涌现。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架包含三个核心部分：1）用于可扩展经验收集与策略训练的EyeGym仿真环境；2）联合训练机械臂（BC）和眼球（RL）策略的BC-RL循环；3）处理视觉输入的眼球机器人Transformer（FoRT）架构。</p>
<p><img src="https://arxiv.org/html/2506.10968v2/figures/eye-robot-rl-bc.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：EyeRobot框架。左侧为具有两个自由度的机械眼球硬件。右侧展示了BC-RL训练循环：眼球（RL）策略生成注视方向，从360度视频中渲染出第一视角观察，供机械臂（BC）策略预测动作；BC策略的动作预测精度（与演示动作的误差）被用作RL策略的奖励。</p>
</blockquote>
<p><strong>EyeGym环境</strong>：为了在物理世界约束下训练主动视觉策略，本文引入了EyeGym。它并非基于传统的物理仿真器渲染视图，而是直接从真实的360度全景视频和图像中采样来渲染模拟的眼球视角。具体到机器人任务，首先用一台360度相机替换机械眼球，收集带同步机器人轨迹的遥操作演示数据。这些配对数据被导入EyeGym，从而可以在演示的机器人动作之上，仿真任意眼球视角的观察序列。这种方法缩小了仿真与现实的差距，并降低了开销。</p>
<p><strong>BC-RL训练循环</strong>：这是方法的核心创新。目标是训练一个能提升下游任务性能的注视策略，且无需专家注视演示。给定一个控制机械臂的BC策略和一个控制注视的RL策略，BC-RL的关键思想是：观察从RL策略流向BC策略，而任务成功指标从BC策略流向RL策略。在每一优化步中，眼球（RL）策略尝试优化BC代理当前匹配演示动作的性能（以此作为奖励），而BC代理则学习在当前注视行为下如何最好地执行任务。<br>具体操作上，一个训练回合从演示中采样一段视频轨迹，眼球注视随机初始化。在每个时间步，眼球策略接收一个奖励，该奖励基于BC策略预测的动作块与真实动作块之间的误差。为了更好的归一化，奖励计算基于动作块的正向运动学，即计算预测轨迹和真实轨迹末端执行器的位置，奖励为这两条样条曲线之间负的弗雷歇距离（惩罚偏差）。BC代理则使用预测与真实动作块之间的标准L1损失进行训练。</p>
<p><strong>主动视觉预训练（AVP）</strong>：在BC-RL中随机初始化网络会导致BC目标病态（因为随机眼球运动下目标对象常常不可见）。因此，本文使用视觉物体搜索奖励作为预训练任务来初始化网络权重。在EyeGym中使用随机采样的静态360度视频帧进行预训练，当目标物体位于视野中心时奖励眼球。预训练阶段，网络以搜索目标的CLIP嵌入为条件，使其能够学习多个物体；在BC-RL阶段，将此输入替换为零向量，并加载AVP的预训练权重。</p>
<p><strong>眼球机器人Transformer（FoRT）</strong>：眼球和机械臂策略均采用Transformer架构，将观察转换为令牌并预测输出令牌。两者为共享输入使用相同的投影矩阵，但其他Transformer权重独立。</p>
<p><img src="https://arxiv.org/html/2506.10968v2/figures/architecture.png" alt="网络架构"></p>
<blockquote>
<p><strong>图4</strong>：眼球机器人Transformer（FoRT）架构。观察以注视方式处理，每个图像被处理成多个尺度并拼接，与注视方向等信息一同输入Transformer。</p>
</blockquote>
<p>观察特征提取方面，采用了一种基于多尺度裁剪的注视处理机制。输入图像被处理成以中心像素为中心的N个尺度的图像金字塔，所有裁剪图像被缩放到相同的224分辨率。然后使用冻结的DINOv2-ViT/S编码器独立嵌入所有图像块，并将其作为令牌扁平化输入Transformer。此外，每个策略还输入注视方向（3D向量）、当前关节本体感知以及可选的视觉搜索“目标”令牌。输出方面，眼球动作被参数化为8个方位-仰角方向和一个停止动作的类别分布。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了UR5e机械臂和Robotiq夹爪平台。评估基于五个需要在大范围（约210度弧形区域）操作的任务：橡皮擦上架、急停按钮触及、刷子交接、螺丝刀伺服、毛巾入桶。收集了每个任务100-500次演示。对比的基线方法包括固定外部（Exo）相机、腕部（Wrist）相机以及两者结合（Wrist+Exo）的配置。</p>
<p><img src="https://arxiv.org/html/2506.10968v2/figures/tasks.png" alt="任务图示"></p>
<blockquote>
<p><strong>图6</strong>：评估EyeRobot的五个大范围工作空间任务示意图。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>在急停按钮（E-Stop）任务中，EyeRobot在慢速和快速条件下均达到100%成功率，末端执行器到目标的平均距离分别为4.0厘米和4.7厘米。在橡皮擦拾放任务中，EyeRobot成功率为60%，而在放置位置被扰动的情况下，其成功率高达100%，展现了出色的注视调整和跟踪能力。相比之下，固定外部相机因分辨率低从未成功抓取橡皮擦（0%）；腕部相机在目标初始可见时成功率很高（100%），但在放置位置扰动时性能骤降（10%）；两者结合的基线表现也不如EyeRobot。</p>
<p><img src="https://arxiv.org/html/2506.10968v2/figures/views-from-cams.png" alt="相机配置对比"></p>
<blockquote>
<p><strong>图8</strong>：不同相机配置的视角对比。安装在云台上的眼球可以在整个工作空间内以更高分辨率进行观察。</p>
</blockquote>
<p><strong>涌现的眼球行为</strong>：实验观察到眼球在为了促成动作而获得奖励的过程中，学会了三种关键行为：切换、搜索和独立跟踪。在多步骤任务中，眼球会根据机器人状态自动将注视切换到下一个相关物体。对于长视野任务，眼球会搜索视野外的后续目标。当目标位置在放置过程中被移动时，眼球会独立于机械臂跟踪新位置。</p>
<p><img src="https://arxiv.org/html/2506.10968v2/figures/interesting_eye_behavior.png" alt="涌现行为"></p>
<blockquote>
<p><strong>图7</strong>：涌现的眼球行为。（左）在抓取过程中，眼球学会根据手臂状态将注视从毛巾切换到水桶。（中）注视处理导致物体居中的固定行为出现。（右）注视独立于手部跟踪目标物体。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>注视处理消融</strong>：与使用均匀分辨率图像（1x448x448）的模型相比，采用注视处理（4x224x224）的EyeRobot在急停任务中表现更优（平均误差4.4厘米 vs 6.4厘米），完成速度更快（平均5.0秒 vs 6.2秒），并且能更好地忽略干扰物，在远距离跟踪上也更鲁棒。</li>
<li><strong>主动视觉预训练（AVP）消融</strong>：在毛巾任务中，没有AVP的模型虽然也能从任务驱动的BC-RL中涌现出长距离视觉搜索能力，但其抓取性能下降，平均成功率从72.2%降至62.1%，特别是在只有毛巾可见的初始条件下，成功率从95%大幅降至40%。AVP有助于BC-RL的收敛。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了BC-RL联合训练循环，使服务于行动的注视行为能够从任务监督中自然涌现，无需注视演示；2）设计了眼球机器人Transformer（FoRT），通过多分辨率注视处理，以较小的计算预算实现了高性能和鲁棒性；3）开发了基于真实360度视频的EyeGym仿真环境，支持可扩展的主动视觉策略训练并缓解仿真到现实的差距。</p>
<p>论文提到的局限性包括：360度视频无法模拟运动视差（即无法像脖子一样移动以绕过遮挡）；由于演示数据分布较窄（例如工作空间边缘的演示较少），训练出的策略可能在仿真中表现良好但在现实中失败，例如出现“盲目抓取”行为；BC-RL的训练收敛速度明显慢于普通BC。</p>
<p>这项工作启示后续研究可以探索将主动眼球安装在移动机器人上以进一步增加对主动视觉的需求，或加入第二只眼实现立体视觉、结合单目深度估计来改善深度感知，以支持更精细的操作。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出EyeRobot系统，解决机器人如何像人类一样主动环视以辅助操作的问题。核心技术是BC-RL循环：手部通过行为克隆（BC）从眼球观测学习操作，眼球通过强化学习（RL）以手部任务成功为奖励，自主学习注视策略；并采用foveated vision transformer架构实现高效高分辨率视觉。实验在五个全景工作空间操作任务上验证，系统能自主涌现手眼协调行为，仅用单个摄像头即可在大范围工作空间内有效完成操作任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.10968" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>