<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01718" target="_blank" rel="noreferrer">2511.01718</a></span>
        <span>作者: Chen, Jiayi, Song, Wenxuan, Ding, Pengxiang, Zhou, Ziyang, Zhao, Han, Tang, Feilong, Wang, Donglin, Li, Haoang</span>
        <span>日期: 2025/11/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型旨在让具身智能体理解自然语言指令和视觉观察，并执行相应动作。近期研究将未来图像预测整合到理解-行动循环中，形成了能够联合理解、生成和行动的“统一VLA”。然而，现有主流方法存在两大局限：一类方法依赖外部专家模型（如编码器和解码器）来统一模态，导致视觉生成与动作预测之间耦合弱、存在错位且复杂度高；另一类方法虽通过视觉和动作标记器将输入输出统一到标记级空间，但图像生成和动作预测在解码过程中仍是分离的，限制了模型利用丰富的未来视觉信息来指导动作预测的能力，甚至有些方法仅在训练时预测图像，在推理时放弃了未来图像的显式引导价值。</p>
<p>本文的核心痛点是现有方法未能实现理解、生成与行动三者之间内在的、相互促进的协同。为此，本文提出了一个新颖的视角：通过一个<strong>同步的去噪过程</strong>来联合优化视觉生成和动作预测。核心思路是设计一个联合离散去噪扩散过程，将多模态信息整合到单一的去噪轨迹中，使得动作标记在去噪的每一步都能充分关注并受未来图像标记的迭代精炼指导，从而实现从视觉表征到时间结构化动作的有效转化。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的统一扩散VLA模型旨在通过单一Transformer架构桥接视觉-语言理解、未来图像生成和动作预测。</p>
<p><strong>整体框架</strong>：首先，通过离散标记化将所有模态（语言、当前图像、未来图像、动作）统一到同一个标记序列中。然后，基于一个预训练的视觉-语言模型，采用两阶段训练流程：第一阶段在大规模视频数据上注入未来图像生成能力；第二阶段在机器人动作数据集上，通过提出的联合离散去噪扩散过程联合训练图像和动作生成。推理时，采用并行解码和多种技术优化效率与性能。</p>
<p><img src="https://arxiv.org/html/2511.01718v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：Unified Diffusion VLA 整体框架。左侧展示了通过离散标记化构建的统一多模态序列（文本、当前图像、未来图像、动作）。右侧展示了基于预训练VLM的两阶段训练流程，以及推理时采用的联合去噪扩散过程与优化技术。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>统一标记化</strong>：使用VQ标记器将图像离散化为视觉词汇表内的标记，使用FAST动作标记器将连续动作离散化为动作词汇表内的标记。语言标记遵循Emu3的设计。序列格式为 <code>[文本标记；当前图像标记；未来图像标记；动作标记]</code>，并使用特殊标记（如<code>&lt;BOI&gt;</code>, <code>&lt;EOI&gt;</code>, <code>&lt;BOA&gt;</code>, <code>&lt;EOA&gt;</code>）界定不同模态块。</li>
<li><strong>混合注意力机制</strong>：该机制是协调多模态交互的关键。<br><img src="https://arxiv.org/html/2511.01718v1/x2.png" alt="混合注意力机制"><blockquote>
<p><strong>图2</strong>：混合注意力机制示意图。输入（文本和当前图像）内部采用双向注意力。输出被分为“生成块”（未来图像）和“行动块”（动作）。每个块内部采用双向注意力以促进全面交互。跨块之间则采用因果注意力：生成块可以关注所有输入，行动块可以关注所有输入和生成块，但信息不允许从行动块反向流向生成块。这种设计将端到端动作预测分解为耦合的前瞻过程（预测下一视觉状态）和逆运动学过程（基于视觉预测推断动作）。</p>
</blockquote>
</li>
<li><strong>联合离散去噪扩散过程</strong>：这是实现内在协同的核心机制。JD3P将未来图像标记序列和动作标记序列拼接为一个完整序列。噪声过程是一个马尔可夫链，每一步以概率β_t将任意位置的标记替换为统一的掩码标记<code>&lt;MASK&gt;</code>。去噪过程则定义了一个联合的反向条件分布。模型训练采用单步掩码预测目标：采样一个掩码比例ρ_t，将干净序列中相应位置的标记替换为<code>&lt;MASK&gt;</code>，然后优化模型仅在被掩码的位置通过交叉熵损失恢复原始标记。该损失函数对视觉标记进行了降权，以促进更强的视觉-动作交互。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，本文的主要创新在于提出了<strong>JD3P</strong>，首次将未来图像和动作的生成置于同一个迭代去噪轨迹中。这使得动作预测在每一步去噪迭代中都能持续、充分地接受未来视觉信息的指导，实现了真正的跨模态协同解码，而非分离或顺序的解码过程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：本文在三个模拟基准数据集上进行了全面评估：CALVIN（长视野语言条件操作）、LIBERO（包含空间、物体、目标、长任务四个套件的操作基准）以及SimplerEnv（用于评估从真实世界视频到模拟环境迁移的套件）。此外，还进行了真实世界机器人评估。</p>
<p><strong>对比基线</strong>：对比方法包括MCIL、RT-1、Robo-Flamingo、Deer、GR-1、ReconVLA、UniVLA*、MODE、UP-VLA、MDT、Octo、SpatialVLA、CoT-VLA、WorldVLA、ThinkAct等广泛采用的VLA或机器人策略模型。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>CALVIN结果</strong>：UD-VLA在ABCD-&gt;D设置下取得了平均成功长度4.64的SOTA性能，显著优于其他基线。<br><img src="https://arxiv.org/html/2511.01718v1/x3.png" alt="CALVIN结果"><blockquote>
<p><strong>表2</strong>：在CALVIN基准上的综合评估结果。UD-VLA（Ours）在连续完成1到5个子任务的成功率以及平均长度上均表现最佳。</p>
</blockquote>
</li>
<li><strong>LIBERO结果</strong>：在LIBERO基准上，UD-VLA在Spatial、Object、Goal、Long四个套件及平均成功率上全面超越了所有对比方法。<br><img src="https://arxiv.org/html/2511.01718v1/x4.png" alt="LIBERO结果"><blockquote>
<p><strong>表3</strong>：在LIBERO基准上的评估与对比。UD-VLA在所有套件上均取得了最高的成功率。</p>
</blockquote>
</li>
<li><strong>消融实验分析</strong>：<br><img src="https://arxiv.org/html/2511.01718v1/figure/vis_pred.png" alt="消融实验"><blockquote>
<p><strong>图3</strong>：消融研究结果。(a) 移除未来图像预测（No Future）或使用因果注意力（Causal Attn.）均导致性能显著下降，证明了联合去噪和混合注意力机制的有效性。(b) 两阶段训练中，缺少第一阶段（w/o Stage (i)）会严重损害模型在需要强泛化能力任务上的表现。</p>
</blockquote>
</li>
<li><strong>定性结果与效率</strong>：<br><img src="https://arxiv.org/html/2511.01718v1/x5.png" alt="定性结果"><blockquote>
<p><strong>图4</strong>：在CALVIN环境中的定性示例。UD-VLA生成的未来图像清晰准确，并成功引导机器人完成了“打开抽屉”和“将滑块移至绿色方块”的长视野任务。<br><img src="https://arxiv.org/html/2511.01718v1/x6.png" alt="推理时间"><br><strong>图5</strong>：推理时间比较。UD-VLA的并行扩散解码相比UniVLA的自回归解码实现了约4倍的加速，同时保持了更高的成功率。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验总结</strong>：关键组件的贡献得到验证：1) <strong>联合去噪与混合注意力</strong>：移除未来图像预测或使用跨模态因果注意力均会导致性能大幅下降；2) <strong>两阶段训练</strong>：缺少第一阶段（世界动态建模）会严重削弱模型在需要强泛化任务上的能力；3) <strong>推理时技术</strong>：解码空间映射和置信度引导解码等技术对稳定生成和提升性能有积极作用。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了统一扩散VLA框架，通过<strong>联合离散去噪扩散过程</strong>首次实现了理解、生成与行动在解码过程中的内在紧密耦合与相互促进。</li>
<li>设计了<strong>混合注意力机制</strong>，在模态内使用双向注意力以促进丰富交互，在模态间强制因果注意力以保障逆运动学逻辑并防止错误传播。</li>
<li>提出了一套有效的<strong>两阶段训练流程</strong>和多种<strong>推理时优化技术</strong>，在保证高性能的同时显著提升了推理效率（4倍于自回归方法）。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：1) 模型性能依赖于高质量的视觉标记器；2) 在真实世界评估中，由于动作空间和场景的复杂性，要达到与模拟环境相当的性能仍面临挑战。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>跨模态协同范式</strong>：JD3P为多模态生成任务（尤其是需要跨模态强交互的任务）提供了一种新的协同优化范式，可启发视频生成、具身规划等领域的后续研究。</li>
<li><strong>效率与性能平衡</strong>：本文展示的并行扩散解码在效率上的优势，促使人们进一步思考如何为自回归模型注入更高效的跨模态交互能力，或继续优化扩散模型的迭代步数。</li>
<li><strong>从模拟到真实</strong>：如何更好地弥合模拟与真实世界之间的差距，以及如何设计更鲁棒的标记化方案以适应真实世界的复杂变化，是具身智能落地的重要方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有统一视觉-语言-动作模型因依赖外部专家或任务分离导致模态协同弱的问题，提出Unified Diffusion VLA模型和联合离散去噪扩散过程（JD3P），通过同步去噪将多模态集成到单一轨迹，基于统一令牌化空间和混合注意力机制实现理解、生成与动作的内在协同。实验在CALVIN、LIBERO和SimplerEnv基准上取得最先进性能，推理速度比自回归方法快4倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01718" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>