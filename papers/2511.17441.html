<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.17441" target="_blank" rel="noreferrer">2511.17441</a></span>
        <span>作者: Guocai Yao Team</span>
        <span>日期: 2025-11-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人双手操作是实现类人灵巧性的关键，但现有的大规模、多样化双手机器人数据集仍然稀缺，主要障碍在于不同机器人平台间的硬件异构性。尽管已有如Open X-Embodiment等数据集尝试聚合多具身数据，但其主要集中于单臂任务，难以支持复杂的双手交互。专有的双手数据集如π₀虽规模庞大，但其封闭性限制了更广泛的研究。近期出现的AgiBot World和Galaxea Open-World等数据集虽提供了大规模双手数据，但通常受商业考量限制，仅基于单一机器人具身，其多具身适用性有限。此外，现有数据集大多仅提供用于模仿学习的动作轨迹，缺乏对操作过程的结构化描述，这限制了模型对新任务或物理环境的泛化与适应能力。本文针对上述痛点，旨在构建一个开放、大规模、多具身且具有结构化标注的双手操作数据集。其核心思路是：收集来自15个不同机器人平台的超过18万条演示数据，并引入一个包含轨迹级、片段级和帧级标注的分层能力金字塔，以支持从高层概念理解到低层控制的多分辨率学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboCOIN的整体框架包含两大部分：数据集本身（RoboCOIN Dataset）和配套的数据处理框架（CoRobot）。首先，数据通过人类遥操作在15个异构机器人平台上收集，涵盖双臂、半人形和人形机器人，使用同构臂、外骨骼或动作捕捉等遥操作方式。传感器数据包括多视角（如头部、腕部）的RGB/深度图像，以及遵循统一标准（关节角度、末端执行器位姿、归一化的夹爪状态）的机器人运动学状态，所有数据流均保持时间同步。收集到的原始数据经过CoRobot框架处理，最终形成具有分层标注的结构化数据集。</p>
<p><img src="https://arxiv.org/html/2511.17441v2/figures/overview.png" alt="数据集概览"></p>
<blockquote>
<p><strong>图1</strong>：RoboCOIN数据集概览。展示了数据集的规模（18万+演示）、涵盖的平台（15种）、场景（16类）和任务（421种），并突出了其核心创新——分层能力金字塔。</p>
</blockquote>
<p>核心模块是<strong>分层能力金字塔</strong>，它提供了三个层次的结构化标注：</p>
<ol>
<li><strong>轨迹级概念</strong>：捕获全局场景配置，描述环境设置和物体摆放，支持整体规划和空间物理推理。</li>
<li><strong>片段级子任务</strong>：将任务分解为可执行的子任务（允许时间重叠以适应双臂操作），每个片段与特定视频帧对齐，包含逐步指令和异常情况（如抓取失败）标注，支持结构化推理、任务规划和错误恢复。</li>
<li><strong>帧级运动学</strong>：为每一帧提供密集的低层细节，用自然语言描述双臂末端执行器的运动参数（方向、速度、加速度）以及夹爪或灵巧手的状态（开/闭、过渡动作），支持实时的内在反馈控制和精确运动执行。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17441v2/x3.png" alt="分层能力金字塔"></p>
<blockquote>
<p><strong>图4</strong>：分层能力金字塔结构。展示了从轨迹级全局概念、片段级可执行子任务到帧级密集运动学细节的三个标注层级，所有标注在时间上同步。</p>
</blockquote>
<p>为高效构建和管理此数据集，本文提出了<strong>CoRobot数据处理框架</strong>，包含三个关键组件：</p>
<ol>
<li><strong>机器人轨迹标记语言（RTML）</strong>：一种用于自动化轨迹质量评估的领域特定语言。它基于运动稳定性、姿态一致性和执行效率三大原则，通过YAML格式定义全局（如工作空间边界、速度限制）和局部（如分阶段的任务约束）规则。RTML评估器能自动检查轨迹是否符合这些约束，并生成质量报告和分数。</li>
<li><strong>半自动标注工具链</strong>：结合大语言模型、基于规则的工具和人工标注，自动化生成分层标注。例如，用目标检测和LLM生成轨迹级场景描述；用基于规则的工具自动识别关键帧并手动细化以定义片段；用滑动窗口和预定义阈值量化帧间运动，生成帧级文本标签。</li>
<li><strong>集成机器人平台</strong>：基于LeRobot扩展，提供统一的多具身控制与数据管理。它集成了各平台官方SDK和ROS支持，实现了统一的机器人控制；增强了数据句柄以支持片段级和帧级文本标注；采用基于标签的原子化存储策略，便于灵活的数据集组合与分发。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17441v2/x4.png" alt="CoRobot框架"></p>
<blockquote>
<p><strong>图5</strong>：CoRobot数据处理框架概览。包含用于轨迹验证的RTML、用于生成分层标注的半自动工具链，以及用于统一控制和数据管理的即用型集成机器人平台。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>规模与多样性</strong>：首次汇集了15种异构机器人平台的大规模双手操作数据；2) <strong>标注结构</strong>：提出了独特的分层能力金字塔，为结构化学习提供了多粒度监督信号；3) <strong>基础设施</strong>：开发了包含RTML质量评估和统一管理平台的完整开源工具链CoRobot。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界环境中进行，使用了RoboCOIN数据集。评估了两种先进的视觉-语言-动作模型：基于流匹配的 <strong>π₀</strong> 和基于扩散的 <strong>GN00T N1.5</strong>。为了验证分层标注的有效性，本文提出了<strong>分层标注集成（HAI）</strong>方法，在不改变原模型架构和参数的前提下，将轨迹级、片段级和帧级标注作为额外的输入令牌注入模型。在推理时，HAI将人类指令与通过阶段变化检测和状态历史总结自动生成的实时上下文相结合。</p>
<p><img src="https://arxiv.org/html/2511.17441v2/x5.png" alt="HAI方法"></p>
<blockquote>
<p><strong>图6</strong>：模型架构。(a) 标准VLA基线模型。(b) 分层标注集成方法，在推理时融入分层信息。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ul>
<li><strong>多具身适用性（RQ1）</strong>：实验在多个机器人平台（如Realman RMC-AIDA-L、Unitree G1edu-u3）上进行，任务设计遵循相同的模式（如“拿起A，递给右手，拿起B，递给左手，将A和B放入盒子”）。结果表明，基于RoboCOIN数据训练的模型能够成功适应不同的机器人具身，完成复杂的双手长视野任务。</li>
<li><strong>分层标注的改进效果（RQ2）</strong>：HAI方法能显著提升基线模型的性能。例如，在Realman RMC-AIDA-L平台上使用π₀模型执行一系列双手操作任务时，引入HAI后，任务成功率获得全面提升。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.17441v2/x6.png" alt="实验结果"></p>
<blockquote>
<p><strong>图7</strong>：Realman RMC-AIDA-L平台 + π₀模型的实验任务设计与结果。左：统一的任务设计流程；右：使用HAI后，各项任务的<strong>成功率提升了5%到11%不等</strong>，证明了分层标注的有效性。</p>
</blockquote>
<ul>
<li><strong>RTML对数据质量的贡献（RQ3）</strong>：通过对RTML验证分数不同的数据子集进行训练发现，使用RTML高分（高质量）数据训练的模型，其性能显著优于使用低分数据或未经筛选的全部数据训练的模型。这证实了RTML在筛选高质量演示、提升数据一致性方面的价值，并揭示了人类遥操作数据中存在的固有局限性（如运动不平稳、姿态不一致），凸显了统一数据标准的重要性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 构建并开源了<strong>RoboCOIN</strong>，一个迄今规模最大、机器人平台最多样化的双手操作数据集；2) 提出了<strong>分层能力金字塔</strong>标注体系，为机器人学习提供了从高层规划到底层控制的结构化监督信号；3) 开发了<strong>CoRobot</strong>集成数据处理框架，包含创新的RTML质量评估语言、自动化标注工具和统一的多具身管理平台，为社区提供了完整的基础设施。</p>
<p>论文自身提到的局限性主要在于数据来源：所有演示均通过人类遥操作收集，其数据质量受操作者技能和偏好影响，存在固有的不一致性，这通过RTML的分析得以揭示。</p>
<p>本文对后续研究的启示在于：首先，它证明了大规模、多具身、结构化的数据是推动通用双手操作的关键。其次，<strong>分层标注</strong>为模型提供了可解释的学习目标，有望催生更具推理和规划能力的机器人策略。最后，<strong>RTML</strong>作为一种数据质量评估与标准化工具，为未来机器人数据的收集、清洗和共享提供了可借鉴的范式，有助于推动多具身学习领域建立更统一的数据标准。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RoboCOIN，旨在解决双手机器人操作中因硬件异构性导致的大规模多样化数据集稀缺的核心问题。关键技术包括：一个开源的多体现双手机器人数据集，涵盖15个平台、超过18万演示，覆盖16个场景的421个任务；分层能力金字塔提供轨迹级、段级和帧级多级注释；CoRobot处理框架采用机器人轨迹标记语言（RTML）实现质量评估、自动注释和统一管理。实验表明，该数据集在多体现双人操作学习中可靠有效，显著提升了多种模型架构和机器人平台的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.17441" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>