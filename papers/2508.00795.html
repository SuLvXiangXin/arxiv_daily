<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Video Generators are Robot Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Video Generators are Robot Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.00795" target="_blank" rel="noreferrer">2508.00795</a></span>
        <span>作者: Liang, Junbang, Tokmakov, Pavel, Liu, Ruoshi, Sudhakar, Sruthi, Shah, Paarth, Ambrus, Rares, Vondrick, Carl</span>
        <span>日期: 2025/08/01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人灵巧操作领域的主流方法是行为克隆（Behavior Cloning），它通过监督学习从人类演示中学习策略。然而，现有方法面临两个根本性局限：一是难以泛化到感知或行为分布发生偏移的新场景（如物体、背景或任务变化）；二是其性能受限于昂贵且有限的人类演示数据规模。计算机视觉和自然语言处理领域通过收集海量数据来解决泛化问题，但机器人动作和演示数据的收集成本高昂。</p>
<p>本文针对上述两个痛点，提出了一个新颖的视角：<strong>将视频生成模型本身视为机器人策略</strong>。核心思路是利用在大规模多样化视频数据上预训练的视频生成模型（如Stable Video Diffusion）作为策略主干，它编码了关于物体运动和动作如何影响世界的强大先验；然后，仅需一个轻量级的动作解码器，将生成的视频帧映射为可执行的机器人动作。这种方法将视频生成作为策略学习的代理，旨在利用丰富的视频先验实现数据高效且泛化能力强的策略学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>Video Policy 是一个模块化框架，其目标是在给定初始场景图像 (v_0) 和自然语言任务描述 (c) 的条件下，生成机器人执行任务的视频 ({\hat{v_t}}) 和对应的动作序列 ({a_t})。</p>
<p><img src="https://arxiv.org/html/2508.00795v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Video Policy 整体框架。给定初始观测和任务提示，模型联合生成机器人执行任务的视频（上方）以及通过独立的扩散网络生成机器人动作（下方）。这种模块化设计支持从无动作的视频数据中学习，并提升了对未见场景的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.00795v1/x2.png" alt="架构细节"></p>
<blockquote>
<p><strong>图2</strong>：Video Policy 架构。模型以初始环境状态图像及对应未来帧和动作的噪声向量为输入，联合去噪生成视频帧和动作。动作去噪器以视频帧的表示为条件。这种设计允许两个网络分开训练。</p>
</blockquote>
<p><strong>整体架构与流程</strong>：<br>模型包含两个核心扩散网络：视频U-Net (f = \mu_\theta) 和动作U-Net (g = \alpha_\theta)。</p>
<ol>
<li><strong>视频生成</strong>：基于Image-To-Video Stable Video Diffusion (SVD)架构。视频U-Net通过交叉注意力机制条件于任务描述 (c) 的CLIP嵌入 (\phi(c))，并在另一个流中，将VAE编码的初始图像 (z_0) 与编码后的含噪视频帧潜在向量 (z_1, ..., z_t) 进行通道拼接。</li>
<li><strong>动作生成</strong>：动作U-Net (\alpha_\theta) 以视频去噪网络的中间特征为条件。具体来说，在视频U-Net解码器的第9、14、17、20、23层提取五个均匀间隔的时空隐藏嵌入，通过一个CNN适配器将其转换为单个特征向量 (h_i)。该向量作为全局条件输入到一个1D CNN U-Net（基于Diffusion Policy架构），用于生成动作序列 ({a_t} = \alpha_\theta(a_i, i, h_i))。这个过程在每个去噪步 (i) 进行，实现了视频与动作生成的紧密集成。</li>
</ol>
<p><strong>训练策略</strong>：<br>训练分为两个阶段：</p>
<ol>
<li><strong>视频模型训练</strong>：最小化视频损失 (L_{video} = E_{z_0,\epsilon,i}[||\epsilon-\mu_\theta(z_i, i, \phi(c), z_{i,0})||^2])，学习根据初始帧和任务描述预测未来的视频帧。</li>
<li><strong>动作模型训练</strong>：在冻结的视频U-Net权重之上，训练动作头，最小化动作损失 (L_{action} = E_{a_0,\epsilon,i}[||\epsilon-\alpha_\theta(a_i, i, h_i)||^2])。<strong>关键设计</strong>是阻止动作损失 (L_{action}) 的梯度回传到视频U-Net (\mu_\theta)，以确保策略能力主要来源于视频生成模型，动作解码器仅作为接口。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>策略代理</strong>：首次系统性地论证并实现将视频生成作为机器人策略学习的直接且高效的代理。</li>
<li><strong>模块化与梯度隔离</strong>：两阶段训练及梯度隔离的设计，使得视频模型能够从大规模预训练中保留强大的世界动力学先验，而动作解码器只需少量演示数据学习映射。</li>
<li><strong>利用无动作数据</strong>：由于视频模型可以单独在无机器人动作标注的视频数据上预训练或微调，这为学习环境动力学提供了近乎无限的数据源，是提升泛化能力的关键。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在仿真环境中使用RoboCasa（24个任务）和Libero10（10个任务）基准，共计34个操作任务。每个任务提供50条人类演示。在真实世界评估了5个任务（Open Drawer, Pick and Place, M&amp;Ms to Cup, Upright Object, Stack Cups），每个任务收集200条演示。</li>
<li><strong>对比方法</strong>：包括Diffusion Policy (DP) 及其基于ResNet和CLIP的变体、Unified Video Action (UVA)、3DA、DP3、FPV、GR00T、DP-VLA、OpenVLA、UniPi等。</li>
<li><strong>输入</strong>：仿真中使用三个摄像头视图（夹爪上一个，场景两侧各一个），拼接后输入模型预测总共25帧（含1帧初始帧）。动作空间为7维（6自由度夹爪位姿+开合状态）。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.00795v1/x3.png" alt="仿真结果表1"></p>
<blockquote>
<p><strong>表1</strong>：在RoboCasa验证集上的成功率对比（50次回合/任务）。使用50条演示的Video Policy在平均成功率以及大多数单项任务上达到了最先进的性能。使用300条MimicGen演示进一步训练后，性能得到提升。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>仿真性能领先</strong>：如表1所示，Video Policy（50条演示）在RoboCasa上取得了0.63的平均成功率，显著优于其他基线（最佳基线DP-VLA为0.57）。在需要应对训练-测试环境分布偏移的“拾放”任务上，改进尤为明显。在Libero10上（表2），Video Policy也以0.94的平均成功率领先。</li>
<li><strong>数据效率与泛化</strong>：Video Policy仅用50条演示就达到了与使用300条（GR00T）甚至3000条（DP-VLA）演示的基线相当或更优的性能，证明了其卓越的样本效率。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.00795v1/x4.png" alt="消融实验表3"></p>
<blockquote>
<p><strong>表3</strong>：在RoboCasa上的消融研究。两阶段训练（先视频后动作）性能（0.63）优于联合训练（0.57）。若不微调视频模型（No Video Tuning），性能骤降至0.09，证明学习生成策略执行视频对于学习鲁棒的表征是必要且充分的。</p>
</blockquote>
<p><strong>消融分析与深入洞察</strong>：</p>
<ol>
<li><strong>两阶段训练的优势</strong>：如表3所示，先训练视频生成再冻结其权重训练动作头的“两阶段”策略，比视频动作联合训练效果更好。这表明在像素空间学习视频生成是比动作生成更通用的目标。</li>
<li><strong>视频预测视界的重要性</strong>：<br><img src="https://arxiv.org/html/2508.00795v1/figures/video_horizon_plot_v2.png" alt="视频预测视界影响"><blockquote>
<p><strong>图3</strong>：Video Policy成功率随视频预测视界变化的函数。学习环境动力学对于策略学习的泛化至关重要，这在有分布偏移的任务上效果提升更明显。<br>图3表明，更长的视频预测视界能普遍提升性能，尤其对于需要更强泛化能力的任务（有分布偏移的任务）提升更大。</p>
</blockquote>
</li>
<li><strong>利用无动作视频实现零样本任务泛化</strong>：<br><img src="https://arxiv.org/html/2508.00795v1/x5.png" alt="无动作视频泛化"><blockquote>
<p><strong>图4</strong>：通过利用无动作视频数据，泛化到无策略监督的任务。我们的方法（Video Policy）和基线（DP-ResNet）的动作头都只在12个任务（左半部分）上训练，但我们的视频生成模型可以访问全部24个任务的无动作视频。结果显示Video Policy在未见任务上（右半部分）取得了强大的泛化性能。<br>如图4所示，当动作解码器只在半数任务上训练，但视频模型在所有任务的无动作视频上微调后，Video Policy在未见任务上取得了显著的泛化性能，而无法利用无动作数据的DP-ResNet基线泛化能力很弱。</p>
</blockquote>
</li>
</ol>
<p><strong>真实世界验证</strong>：<br><img src="https://arxiv.org/html/2508.00795v1/x6.png" alt="真实世界定性结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界拾放任务泛化实验的定性结果。Video Policy对物体位置、外观和背景颜色表现出强大的鲁棒性。<br>在真实世界的五个任务上，Video Policy成功应对了物体位置变化、未见物体交互和背景外观变化等挑战。定性结果（图5）显示了其鲁棒性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出并验证了新范式</strong>：系统性地论证了“视频生成模型即机器人策略”的可行性，将视频生成作为策略学习的强大代理。</li>
<li><strong>设计了高效模块化框架</strong>：提出了Video Policy，其两阶段训练及梯度隔离的设计，使得能够充分利用大规模视频先验，同时仅需少量演示数据来训练轻量级动作解码器，实现了卓越的样本效率和泛化能力。</li>
<li><strong>揭示了关键机制</strong>：通过详实的分析表明，学习生成准确的策略执行视频是学习鲁棒操作表征的关键；并且，利用无动作视频数据可以显著提升对未见任务的泛化能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，该方法依赖于大规模预训练的视频生成模型（如SVD）的质量和特性。其性能上限可能受限于这些基础模型对物理世界动力学和机器人交互场景的建模能力。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>利用更丰富的视频数据</strong>：未来工作可以探索利用更大规模、更多样化的互联网视频来预训练或微调视频生成模型，以注入更强大的世界先验。</li>
<li><strong>架构与训练机制优化</strong>：本文的两阶段和梯度隔离设计被证明有效，可进一步研究更高效的视频-动作表示对齐与联合优化方法。</li>
<li><strong>迈向通用策略学习</strong>：此范式为构建数据高效、泛化能力强的通用机器人策略提供了一条有希望的路径，鼓励社区进一步探索生成式基础模型在机器人学中的应用。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人视觉运动策略泛化能力差、依赖大量演示数据的问题，提出Video Policy框架，将视频生成作为策略学习代理。该框架模块化结合视频生成与动作生成，通过扩散网络端到端训练，能从无动作视频数据中学习。实验表明，该方法仅需少量演示数据即能提取策略，显著提升样本效率和鲁棒性，在模拟和真实环境中对未见物体、背景和任务表现出强泛化能力，性能优于传统行为克隆。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.00795" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>