<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Generalist Robot Manipulation beyond Action Labeled Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Generalist Robot Manipulation beyond Action Labeled Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19958" target="_blank" rel="noreferrer">2509.19958</a></span>
        <span>作者: Danda Pani Paudel Team</span>
        <span>日期: 2025-09-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通用机器人操作领域的主流方法是视觉-语言-动作模型，它们通过在大量带动作标签的遥操作机器人示教数据上进行训练，将预训练的视觉-语言模型的语义推理能力与具身理解相结合。然而，这类方法严重依赖于高质量、带动作标签的机器人数据，其扩展成本高昂，且模型性能在超出训练分布的任务上会迅速下降。利用互联网规模的人类演示视频作为替代数据源虽具前景，但面临缺乏直接动作监督、人-机器人域差距大以及视频中包含大量与机器人控制无关的冗余信息等关键挑战。现有从人类视频中学习的工作多局限于专用、小规模策略，或需要定制化的逆动力学模型进行执行，或受限于重定向方法带来的巨大域差距。</p>
<p>本文针对“如何有效利用无动作标签的人类及机器人视频来增强通用机器人操作策略的泛化能力”这一具体痛点，提出了一个新视角：将密集的3D动态点云作为一种与具体具身无关的中间动作表示。本文的核心思路是：首先利用大规模无标签视频，通过预测手或夹爪的动态点云序列进行自监督预训练，学习跨具身的运动先验；随后，在一个小规模带动作标签的数据集上，将学到的表示与机器人动作对齐，从而高效地将无标签数据中的知识迁移到机器人控制策略中。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的MotoVLA方法采用两阶段训练流程。整体输入为当前RGB图像、语言指令以及历史信息；第一阶段输出为未来手/夹爪的动态点云序列，第二阶段输出为未来可执行的机器人动作序列。</p>
<p><img src="https://arxiv.org/html/2509.19958v1/figures/Overview.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：MotoVLA方法整体框架。左侧为第一阶段：利用无动作标签的人类或机器人视频，通过预测动态点云进行自监督训练，学习与具身无关的运动表示。右侧为第二阶段：在较小的带动作标签数据集上，将预训练模型微调为动作预测器，实现动作对齐。</p>
</blockquote>
<p>核心模块包括一个预训练的VLM主干和一个预测器。VLM主干基于Paligemma，负责处理图像和文本输入，输出键值缓存用于条件生成。预测器是一个较小的Transformer，其作用在两个阶段不同：在第一阶段作为<strong>3D动态点云预测器</strong>，输入为当前点云和VLM的键值缓存，通过流匹配预测未来点云序列；在第二阶段作为<strong>动作预测器</strong>，结构与第一阶段相同但输入输出不同，输入为当前机器人本体感知信息，输出未来动作序列。预测器的权重在两阶段之间共享初始化。</p>
<p><img src="https://arxiv.org/html/2509.19958v1/x1.png" alt="架构细节"></p>
<blockquote>
<p><strong>图2</strong>：VLA架构与两阶段训练示意图。左图展示了利用3D动态点云预测器从无标签视频学习与具身无关的表示；右图展示了将预测器微调为动作预测器，用于通用操作。</p>
</blockquote>
<p>技术细节上，动态点云从视频中提取：首先使用Grounding DINO检测手/夹爪边界框，再用Segment Anything V2生成分割掩码，从掩码中均匀采样像素点，接着使用BootsTAPIR进行2D跟踪，最后利用MoGE单目深度估计器将2D轨迹提升为3D点云序列。训练使用条件流匹配损失函数。第一阶段损失（公式3）旨在最小化预测的速度场与真实点云减去噪声之间的差异。第二阶段损失（公式4）形式相同，但目标变为机器人动作。创新点具体体现在：1) 首次在端到端通用VLA架构中，利用大规模无标签演示直接学习细粒度运动先验；2) 提出了以可解释、能抽象具身、直接编码时空关系的动态点云作为跨域对齐的中间表示；3) 设计了两阶段训练策略，使得在小规模动作数据上即可高效完成对齐，降低了高质量机器人数据的需求。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在SIMPLER模拟器（基于BridgeData V2重建）和真实的WidowX 250S机器人平台上进行。使用了多个数据集：无标签阶段使用了RH20T（人类视频）、BridgeData V2和Rt-1（机器人视频，但丢弃动作标签）；动作对齐阶段使用了BridgeData V2的机器人动作数据。</p>
<p>对比的基线方法包括：通用VLA模型 OpenVLA (OXE) 和 π0 (B)；采用两阶段训练但用于专用策略的 LAPA (OXE)；以及为公平比较而适配为零射击设置的层次化方法 ATM (B)。此外，还设置了仅使用机器人无标签数据的消融版本 MotoVLA (R)。</p>
<p><strong>域内实验</strong>（任务和场景均包含在动作数据集中）结果如表1所示。MotoVLA (R+H) 在SIMPLER中取得了68.2%的平均成功率，优于所有基线。它比在完整Open X-Embodiment数据集上预训练并针对测试任务精调的LAPA高出14.1%，比在相同BridgeData V2上从头训练的π0 (B)高出11.4%。这证明了动态点云预训练学习到的运动先验能提升下游任务性能，即使这些任务在第二阶段有动作监督。</p>
<p><img src="https://arxiv.org/html/2509.19958v1/x2.png" alt="域外实验结果"></p>
<blockquote>
<p><strong>图3</strong>：真实机器人域外任务评估结果总览。区分了完全域外任务，以及那些在无标签阶段（Rt-1或RH20T数据集中）出现但不在动作训练阶段出现的任务。MotoVLA (R+H)在平均成功率上表现最佳，特别是在无标签数据集中出现过的任务上优势明显。</p>
</blockquote>
<p><strong>域外实验</strong>（测试任务未出现在动作训练阶段）在真实机器人上进行。如图3所示，MotoVLA (R+H) 在平均成功率上领先。性能提升最显著的任务（如“Push Button”、“Cube on Scale”）恰恰在人类演示数据集RH20T中出现过，这表明方法能够直接将从未见过物体和任务的知识从无标签数据跨具身迁移过来。对于完全域外的任务，MotoVLA (R) 表现最好，π0 (B)紧随其后，说明这类任务仍具挑战性。</p>
<p><strong>消融实验</strong>在SIMPLER域内设置下进行，关键结果总结在表2中：使用<strong>2D轨迹</strong>而非3D点云导致性能下降4.0%（模拟器）和12.5%（真实机器人），验证了3D表示在缩小与机器人动作域差距方面的优势。<strong>减少点云数量（32点）</strong> 或<strong>在非夹爪区域均匀采样</strong>均导致性能大幅下降。尝试在第二阶段<strong>联合训练</strong>点云和动作预测，或使用<strong>两个独立的预测器</strong>，效果均不如提出的两阶段顺序训练方案。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了MotoVLA，首个能够利用无动作标签数据学习通用机器人操作所需运动先验的端到端VLA模型；2) 设计了一种两阶段训练方法，以动态点云作为可扩展且直观的、与具身无关的公共表示；3) 通过广泛的模拟与真实实验，证明了该方法在域内、域外及迁移学习任务上的有效性，特别是能够从无标签的人类演示中实现技能迁移。</p>
<p>论文自身提到的局限性包括：动态点云的提取质量依赖于上游的检测、分割与跟踪模型；对于在无标签预训练和动作对齐阶段都完全未出现过的任务（完全域外），性能提升有限，表明泛化能力仍有边界。</p>
<p>这项工作对后续研究的启示是：为利用海量、易得的无标签视频数据（尤其是人类演示）来增强机器人智能提供了一条有效路径。所提出的动态点云作为一种中间表示，兼具物理基础性和抽象性，为跨具身学习提供了一个有前景的研究方向。未来工作可以探索更鲁棒的点云提取方法，以及如何将更广泛的无标签互联网视频纳入训练框架。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决通用机器人操作中高质量动作标记数据稀缺的瓶颈。提出一种新方法，利用无动作标签的人类或机器人演示视频进行学习。关键技术包括：在抓手位置提取密集动态3D点云，利用3D动态预测器进行自监督预训练，再通过少量标记数据对预测器进行动作对齐微调。实验表明，该方法能有效利用无标签数据提升下游策略的泛化性能，使机器人能在真实与仿真环境中学习训练时未见过的动作任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19958" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>