<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RAPTOR: A Foundation Policy for Quadrotor Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RAPTOR: A Foundation Policy for Quadrotor Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.11481" target="_blank" rel="noreferrer">2509.11481</a></span>
        <span>作者: Giuseppe Loianno Team</span>
        <span>日期: 2025-09-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于强化学习的神经网络策略在四旋翼控制中已得到广泛应用。然而，这些策略存在严重的过拟合问题：每个策略通常针对单一的四旋翼平台（特定的动力学模型）或单一的任务轨迹（如特定赛道）进行高度优化。一旦平台或环境发生微小改变（例如更换螺旋桨、电池，或面临仿真到现实的差异），策略就会失效，需要重新进行系统辨识和完整的策略训练。这种数据利用的低效性与人类快速适应新系统的能力形成鲜明对比。</p>
<p>本文旨在解决这一关键痛点，即<strong>为四旋翼控制训练一个高度自适应的、单一的“基础策略”</strong>，使其能够像人类一样，仅通过少量交互（零样本或少样本）即可适应各种前所未见的四旋翼平台。其核心思路是：通过一种新颖的“元模仿学习”方法，将大量针对不同四旋翼的专家策略知识蒸馏到一个具有循环结构的、轻量级的学生策略中，使其具备在线上下文学习能力，从而隐式地、实时地识别并适应新平台的动力学特性。</p>
<h2 id="方法详解">方法详解</h2>
<p>RAPTOR方法的整体流程分为两个主要阶段：预训练和元模仿学习。</p>
<p><img src="https://arxiv.org/html/2509.11481v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RAPTOR方法整体框架。（A）动机：对比人类、现有RL策略和RAPTOR的适应能力。（B）流程：1）从广泛的四旋翼动力学参数分布中采样1000个不同的四旋翼；2）为每个四旋翼预训练一个专门的“教师”策略；3）通过元模仿学习，将所有教师策略的知识蒸馏到一个单一的、具有循环隐藏层的“学生”基础策略中。</p>
</blockquote>
<p><strong>第一阶段：预训练（教师策略生成）</strong></p>
<ol>
<li><strong>动力学参数分布设计</strong>：首先，定义一个覆盖几乎所有物理上合理的四旋翼的广泛动力学参数分布（如质量、惯性、推力曲线、电机延迟等）。</li>
<li><strong>采样与训练</strong>：从该分布中采样1000组不同的动力学参数，对应1000个虚拟的四旋翼。对于每一个四旋翼，使用强化学习（具体为PPO算法）从头开始训练一个专用的、完全马尔可夫（无状态）的神经网络控制策略，即“教师”策略。每个教师策略都是一个三层全连接网络，其目标是学习控制其对应的特定四旋翼。</li>
</ol>
<p><strong>第二阶段：元模仿学习（蒸馏到基础策略）</strong><br>这是方法的核心创新。目标是将1000个教师策略的行为蒸馏到一个单一的、自适应的学生策略中。</p>
<ol>
<li><strong>学生策略架构</strong>：学生策略是一个小型循环神经网络，仅包含一个具有16维隐藏状态的循环层，总参数量仅为2084。其输入是当前观测（位置、姿态、速度等）和参考轨迹，输出是四个电机的PWM指令，实现了真正的端到端控制（从状态到电机指令）。</li>
<li><strong>训练过程</strong>：在训练时，随机选择一个教师策略及其对应的四旋翼动力学。让学生策略在该四旋翼的仿真环境中运行，将其产生的动作序列与对应教师策略产生的“专家”动作序列进行比较。通过最小化两者之间的均方误差（行为克隆损失）来训练学生策略。由于学生策略是循环的，它能够通过历史交互的上下文（隐藏状态）来隐式地推断当前所控制四旋翼的动力学特性，从而模仿不同教师的行为。</li>
<li><strong>创新点</strong>：与现有方法相比，RAPTOR的创新在于：（a）提出了<strong>元模仿学习</strong>这一蒸馏框架，将大量异构专家策略压缩为一个统一策略；（b）学生策略的<strong>循环结构</strong>使其具备了<strong>上下文学习</strong>能力，能够实时、在线地进行隐式系统辨识，而无需显式地重建系统参数；（c）策略极其<strong>轻量</strong>（2084参数），可直接部署在资源受限的飞控微控制器上。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11481v1/x2.png" alt="训练结果"></p>
<blockquote>
<p><strong>图2</strong>：训练结果。（A）预训练阶段1000个教师策略的聚合学习曲线（以 episode 长度衡量）。（B）元模仿学习阶段，学生策略在7个未见过的测试四旋翼上的性能随训练轮次的变化。（C）帕累托前沿：使用不同数量的教师进行蒸馏时，学生策略性能与教师数量的关系。（D）帕累托前沿：不同大小的学生策略（以FLOPS衡量）与其性能的关系。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真和现实中进行了广泛测试。使用了<strong>10个不同的真实四旋翼平台</strong>（重量从32g到2.4kg，尺寸从65mm到500mm，推重比从≈1.75到12，涵盖有刷/无刷电机、刚性/柔性机架、不同飞控（PX4/Betaflight/Crazyflie/M5StampFly）和不同状态估计器）以及<strong>2个仿真器</strong>。对比的基线主要是针对单个平台专门训练的、性能最优的策略。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>零样本适应与泛化</strong>：训练好的RAPTOR基础策略能够<strong>零样本</strong>（无需任何新平台上的微调）直接控制所有10个真实四旋翼，实现稳定悬停和轨迹跟踪。即使对于训练分布外的平台（如推重比高达12，训练时≤5；或柔性机架，训练时均为刚性），策略也表现稳健。</li>
<li><strong>轨迹跟踪性能</strong>：在跟踪“8”字形轨迹的任务中，RAPTOR策略在所有平台上都取得了良好效果。例如，在Crazyflie平台上，对于5.5秒周期的轨迹，RAPTOR的跟踪RMSE为0.19m（包含Z轴）和0.19m（仅XY平面），与专门为该平台训练的专用策略（RMSE 0.17m/0.15m）性能接近，但RAPTOR的单一策略可同时适用于众多其他平台。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11481v1/x5.png" alt="轨迹跟踪"></p>
<blockquote>
<p><strong>图5</strong>：10个真实和2个模拟四旋翼的轨迹跟踪结果。展示了不同速度的“8”字形轨迹跟踪路径及对应的RMSE误差表。所有平台均能有效跟踪，误差主要分布在XY平面。</p>
</blockquote>
<ol start="3">
<li><strong>抗干扰与恢复能力</strong>：策略展现出强大的鲁棒性。如图6所示，它能从高速（4.5 m/s）初始状态中恢复，在被工具猛烈撞击导致姿态倾斜超过90°后仍能恢复稳定飞行，甚至在飞行中承载额外工具或混合安装不同桨叶的螺旋桨时也能维持控制。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11481v1/x6.png" alt="抗干扰测试"></p>
<blockquote>
<p><strong>图6</strong>：RAPTOR策略在不同极端情况下的表现。（A）从高速状态中空中激活并恢复。（D）被撞击后从&gt;90°倾斜中恢复。（E）飞行中承载工具。（F）使用四种不同螺旋桨（2叶和3叶）混合安装时仍能跟踪轨迹。</p>
</blockquote>
<ol start="4">
<li><strong>隐式系统辨识验证</strong>：通过线性探测实验验证了策略隐藏状态确实编码了系统动力学信息。如图3所示，一个简单的线性模型就能从策略的隐藏状态中准确预测当前四旋翼的推重比（测试集R² = 0.949），证实了<strong>涌现的隐式系统辨识</strong>能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11481v1/x3.png" alt="隐式系统辨识"></p>
<blockquote>
<p><strong>图3</strong>：涌现的系统辨识。左图：策略从极端初始状态（位置偏移2m，俯仰90°）中成功恢复的过程，并绘制了隐藏状态轨迹。右图：线性探测结果显示，隐藏状态能高度准确地预测四旋翼的推重比。</p>
</blockquote>
<ol start="5">
<li><strong>消融实验</strong>：图2C和2D的帕累托前沿分析表明：（a）即使使用较少教师（如100个）进行蒸馏，学生策略也能获得基本的控制能力，但使用更多教师（1000个）能获得最佳性能；（b）学生策略的大小（参数量）存在权衡，较小的模型（16维隐藏层）在保证高性能的同时，计算开销极小，适合嵌入式部署。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了RAPTOR框架</strong>：一种通过元模仿学习训练四旋翼基础策略的新方法，该策略具备通过上下文学习进行零样本适应的能力。</li>
<li><strong>实现了高效、轻量的部署</strong>：所得到的基础策略仅含2084个参数，可直接运行在各种飞控的微控制器上，计算占用低于10%，并提供了完整的开源实现。</li>
<li><strong>进行了广泛且严格的实证验证</strong>：在10个极具多样性的真实四旋翼平台上证明了方法的有效性、鲁棒性和强大的分布外泛化能力。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>策略在观测中未包含轨迹前瞻信息，这可能成为跟踪高敏捷轨迹的瓶颈。</li>
<li>对于使用非EKF状态估计器（如Mahony/Madgwick）且存在通信延迟的平台，观察到低频Z轴振荡，需在飞控固件层面进行特定缓解。</li>
</ol>
<p><strong>研究启示</strong>：</p>
<ol>
<li><strong>轻量循环网络是实现快速在线适应的有效途径</strong>：这项工作表明，极小的循环网络即可实现复杂的隐式系统辨识和适应，挑战了“更大模型性能更好”的直觉，为在资源受限的机器人上部署智能体指明了方向。</li>
<li><strong>元模仿学习是整合异构专家知识的强大工具</strong>：该方法为将多个专门策略的知识压缩到一个通用、自适应策略中提供了可扩展的框架。</li>
<li><strong>为通用机器人控制奠定了基础</strong>：RAPTOR证明了训练单一策略控制多种不同形态机器人的可行性，为开发更通用的“机器人基础策略”迈出了重要一步。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RAPTOR方法，旨在解决现有强化学习策略过度专一化、无法适应不同四旋翼平台的问题。该方法通过元模仿学习技术，先为1000种不同四旋翼训练教师策略，再蒸馏为单一学生策略，并利用隐藏层循环实现情境学习，使策略能快速适应新平台。实验表明，仅含2084参数的轻量策略能零样本适应10种真实四旋翼（32克至2.4千克），适应过程仅需毫秒。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.11481" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>