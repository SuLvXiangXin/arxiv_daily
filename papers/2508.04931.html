<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.04931" target="_blank" rel="noreferrer">2508.04931</a></span>
        <span>作者: Nikos Tsagarakis Team</span>
        <span>日期: 2025-08-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>传统机器人控制与规划方法严重依赖精确的物理模型和预定义的动作序列，虽然在结构化环境中有效，但在感知不确定、物体属性未知的非结构化真实世界中，常因建模不准确而失败，且难以泛化到新任务。相比之下，人类通过与环境的持续交互积累了丰富的经验，能够不依赖精确模型，仅凭直觉快速判断物体的稳定性、可达性、运动趋势等交互属性，即“交互直觉”。近年来，将大型语言模型（LLMs）和视觉语言模型（VLMs）引入机器人系统，为提升其认知与决策能力带来了新突破。然而，现有的视觉-语言-动作方法通常需要大规模高质量数据集训练，这对于复杂的高自由度仿人机器人而言难以获取；同时，现有VLMs主要关注静态图像理解，缺乏对物体属性、交互动力学和交互关系建模的能力，并且其高级语义输出与机器人执行所需的低级运动控制之间存在巨大鸿沟。</p>
<p>本文针对上述痛点，旨在为仿人机器人赋予一种类似人类的、通过交互经验学习而来的物理直觉，使其能在非结构化环境中自主规划与执行任务。核心思路是：利用VLMs强大的场景理解能力构建一个“直觉感知器”来提取场景的语义与空间关系，并通过一个基于图的“记忆图”结构积累机器人过往的交互经验，从而在面对新任务时，通过匹配当前场景与历史经验，推断出符合物理直觉的恰当行为。</p>
<h2 id="方法详解">方法详解</h2>
<p>INTENTION框架旨在使机器人通过整合VLM场景推理和交互驱动的记忆，获得学习到的交互直觉和在全新场景中的操作能力。整个流程分为学习（构建记忆）与推断（执行新任务）两个阶段。</p>
<p><img src="https://img.technews.tw/wp-content/uploads/2025/08/08114820/image-103.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：INTENTION框架总览。框架包含四个核心部分：(a) <strong>直觉感知器</strong>：接收RGB图像和人类指令，提取物体交互信息并构建结构化的任务图。(b) <strong>记忆图</strong>：存储先前任务相关的场景知识以及机器人在面对不同任务时采取的行动。(c) <strong>运动库</strong>：包含各种预定义的运动基元。(d) <strong>新场景处理</strong>：面对新场景时，机器人提取当前场景图并与记忆图对比，以选择合适的动作。</p>
</blockquote>
<p><strong>整体流程</strong>：</p>
<ol>
<li><strong>学习阶段</strong>：机器人执行一系列由人类指令引导的交互任务。对于每个任务，直觉感知器处理场景状态（指令+原始感知），提取物体/环境状态（空间与语义配置）。一个基于VLM的规划器根据指令、场景和物体状态，从预定义技能库中选择并执行动作。任务完成后进行评估，并将整个交互信息（场景、物体状态、动作、结果）编码为一个图表示，存入记忆图。</li>
<li><strong>推断阶段</strong>：面对新任务场景，机器人首先用直觉感知器提取当前场景图。然后查询记忆图，通过图匹配算法将当前场景图与所有存储的历史任务图进行相似度比较。匹配结果由一个基于语言模型的评估器进行筛选，最终选择出最适合当前场景的机器人动作并执行。</li>
</ol>
<p><strong>核心模块与技术细节</strong>：</p>
<ul>
<li><strong>直觉感知器</strong>：基于预训练的VLM构建，用于从RGB图像中提取“蒸馏”后的空间几何和语义观察结果。通过精心设计的提示词和函数调用，引导VLM根据语义知识从任务场景中提取明确的空间信息，并以特定的JSON模板格式输出，从而构建结构化的场景图。提示词示例要求模型提取特定物体并推测物体间的空间关系。</li>
<li><strong>记忆图</strong>：这是一个基于图的记忆结构，用于存储从机器人与人类及环境的过往交互中获得的直觉知识和经验。每个任务图被定义为 <code>G = (N, L, I)</code>，包含节点集<code>N</code>（对应物体或智能体）、链接集<code>L</code>（捕获空间或语义关系）和人类指令<code>I</code>。节点、链接和指令的内容均以文本格式编码。</li>
<li><strong>图构建与匹配</strong>：这是实现经验复用的关键。</li>
</ul>
<p><img src="https://img.technews.tw/wp-content/uploads/2025/08/08114820/image-102.png" alt="图构建与匹配"></p>
<blockquote>
<p><strong>图3</strong>：图构建与匹配流程。展示了从当前场景构建场景图，并与记忆图中存储的历史任务图进行相似度匹配的过程。</p>
</blockquote>
<ul>
<li><strong>图匹配算法</strong>：为了在新场景中找到最相关的历史经验，论文提出了一种基于评分的图匹配方法。它通过加权组合三种相似度来计算当前场景图<code>G_t</code>与记忆图中每个任务图<code>G_m</code>的总体匹配分数<code>S_W</code>：<ul>
<li><strong>指令相似度</strong>：使用编码器提取指令<code>I_t</code>和<code>I_m</code>的嵌入，计算余弦相似度<code>S_I</code>。</li>
<li><strong>节点相似度</strong>：计算两个图节点集嵌入的成对相似度矩阵，经过阈值过滤和二分图匹配后得到<code>S_N</code>。</li>
<li><strong>链接相似度</strong>：类似地，计算链接集的相似度<code>S_L</code>。</li>
<li>最终分数 <code>S_W = α·S_N + β·S_L + γ·S_I</code>，其中α, β, γ为权重系数。分数最高的匹配所对应的历史动作将被检索出来，经过评估器分析后生成控制代码供机器人执行。</li>
</ul>
</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，INTENTION的创新主要体现在：1) <strong>首次探索将VLM用于构建真实世界仿人机器人操作的交互直觉</strong>，而不仅仅是静态理解或简单任务规划；2) 提出了<strong>记忆图</strong>这一结构化的经验存储与检索机制，使机器人能够基于过往的物理交互经验进行决策，实现了小样本甚至零样本的泛化；3) 通过<strong>图匹配算法</strong>将高级语义场景与低层运动选择联系起来，弥合了语义理解与运动控制之间的差距。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在一个具有37个自由度、配备深度相机和力传感器的轮腿式仿人机器人上进行。使用OpenAI的<code>o4-mini</code>模型作为直觉感知器和评估器的VLM，使用CLIP作为文本编码器生成图节点的嵌入。运动库中的动作基元基于笛卡尔空间I/O手动设计，无需额外训练。</p>
<p><strong>Baseline对比</strong>：论文将INTENTION与三种基线方法进行了对比：全身模型预测控制（WB-MPC）、基于手动设计行为树的规划器（BT-Planner）以及利用LLM生成行为树的LLM-BT。测试分为两类任务：标准操作任务（拾放）和交互直觉任务（无明确指令，仅依赖物理推理）。</p>
<p><img src="https://img.technews.tw/wp-content/uploads/2025/08/08114820/image-104.png" alt="实验结果对比表"></p>
<blockquote>
<p><strong>表1</strong>：不同方法在仿人机器人操作任务规划上的能力对比。关键指标包括任务规划成功率（Plan↑）、执行成功率（Succ↑）和平均耗时（Avg. Time↓）。INTENTION在需要交互直觉的任务上表现显著优于其他方法，成功率高达72%，而LLM-BT仅为15%。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>对比实验</strong>：如表1所示，在标准拾放任务中，所有方法表现均较好（INTENTION成功率92%）。然而，在更具挑战性的<strong>交互直觉任务</strong>中，WB-MPC和BT-Planner完全失效，LLM-BT成功率仅为15%，而INTENTION取得了<strong>72%</strong> 的成功率，显著优于其他方法，证明了其通过经验学习物理直觉的有效性。</li>
<li><strong>记忆图规模的影响</strong>：如图6所示，随着记忆图中存储的任务图数量增加，INTENTION在<strong>指令模式</strong>和<strong>直觉模式</strong>下的任务规划成功率均呈现上升趋势。这表明积累更多样的交互经验有助于提升系统在新场景中的表现。</li>
</ol>
<p><img src="https://img.technews.tw/wp-content/uploads/2025/08/08114820/image-105.png" alt="任务规划成功率与记忆图规模关系"></p>
<blockquote>
<p><strong>图6</strong>：在指令模式和直觉模式下，任务规划成功率随记忆图中检索到的图数量变化的情况。成功率随着经验（图数量）的增加而提高。</p>
</blockquote>
<ol start="3">
<li><strong>场景提取与记忆图构建</strong>：图5展示了直觉感知器从四个不同任务场景（处理物体、抬桌子、推椅子、续茶）中提取信息并构建记忆图的过程。</li>
</ol>
<p><img src="https://img.technews.tw/wp-content/uploads/2025/08/08114820/image-106.png" alt="场景提取与记忆图构建示例"></p>
<blockquote>
<p><strong>图5</strong>：场景提取与记忆图构建过程示例。展示了从不同任务场景中提取信息并形成结构化记忆的过程。</p>
</blockquote>
<ol start="4">
<li><strong>真实世界任务性能</strong>：在完全依赖交互直觉（无指令）的情况下，INTENTION在四个真实操作任务上进行了各25次试验。如图8所示，其在“处理物体”、“抬桌子”、“推椅子”和“续茶”任务上的成功率分别达到**92%、84%、76%和68%**，平均成功率80%，证明了框架在多样真实场景中的有效性和鲁棒性。</li>
</ol>
<p><img src="https://img.technews.tw/wp-content/uploads/2025/08/08114820/image-108.png" alt="真实世界直觉任务成功率"></p>
<blockquote>
<p><strong>图8</strong>：INTENTION在完全依赖交互直觉（无指令输入）的不同操作任务上的成功率。在四个任务中均表现出色，验证了其物理直觉推断能力。</p>
</blockquote>
<p><strong>消融实验分析</strong>：虽然没有独立的消融实验章节，但图6的结果间接证明了记忆图（经验积累）对性能的关键贡献。此外，与LLM-BT的对比（表1）凸显了结合VLM感知与图结构记忆（INTENTION）相对于仅使用LLM进行高层规划（LLM-BT）在需要物理直觉任务上的巨大优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了INTENTION框架</strong>：首次探索利用接地气的语言模型与物理直觉相结合，为仿人机器人自主任务规划提供了一种新框架，实现了小样本训练与部署。</li>
<li><strong>设计了直觉感知器与记忆图</strong>：利用VLM构建直觉感知器从任务场景中提取语义与空间观察，并进一步构建记忆图来存储接地气的知识，以结合机器人可供性来指导多样化任务的行为选择。</li>
<li><strong>进行了真实系统验证</strong>：在仿人机器人系统上通过真实世界实验验证了所提方法的适用性，证明了其在多样化任务场景中的有效适应能力。</li>
</ol>
<p><strong>局限性</strong>：论文在结论部分提到，未来工作将探索扩展记忆图到更广泛的任务领域、增强跨不同机器人平台的运动泛化能力，以及进一步提高在高度动态环境条件下的记忆检索和动作推断效率。这暗示了当前方法可能存在记忆图规模有限、动作库需预定义且平台依赖性、以及在极端动态环境下性能可能下降等局限性。</p>
<p><strong>启示</strong>：本研究为机器人，特别是仿人机器人，获得类人的物理直觉和情境化决策能力指明了一条有前景的路径。它表明，结合大型基础模型的语义理解能力与结构化、可积累的交互经验记忆，是克服传统方法泛化性差、数据依赖强的有效策略。后续研究可以沿着扩展经验库的规模与多样性、开发更高效和自适应的图匹配与动作生成机制，以及探索如何将低层运动技能也纳入学习框架等方向深入。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文INTENTION旨在解决传统机器人控制依赖精确物理模型、在非结构化环境中泛化能力差的问题，模仿人类通过交互直觉高效适应新场景的能力。提出一种新颖框架，整合视觉语言模型（VLMs）进行场景推理，并引入Memory Graph记录交互记忆、Intuitive Perceptor提取物理关系和可供性。该框架使机器人能在新场景中自主推断交互行为，无需重复训练或指令，显著提升任务适应性和泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.04931" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>