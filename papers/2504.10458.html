<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.10458" target="_blank" rel="noreferrer">2504.10458</a></span>
        <span>作者: Luo, Run, Wang, Lu, He, Wanwei, Chen, Longze, Li, Jiaming, Xia, Xiaobo</span>
        <span>日期: 2025/04/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前构建图形用户界面（GUI）代理的主流方法依赖于在大规模视觉语言模型（LVLM）上进行监督微调（SFT）。然而，这种方法不仅需要海量的高质量训练数据，而且在有效理解GUI截图和泛化到未见界面上存在困难，这严重限制了其在现实世界场景，特别是高级任务中的应用。受大型推理模型（如DeepSeek-R1）中强化微调（RFT）范式的启发，本文针对上述数据效率和泛化能力不足的痛点，首次将基于规则的强化学习范式引入GUI代理领域。核心思路是：通过统一动作空间规则建模，利用少量精心筛选的多平台高质量数据，并结合策略优化算法（如GRPO）更新模型，从而高效提升LVLM在高级现实GUI任务中的执行能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>GUI-R1是一个基于强化学习训练范式的框架，旨在增强GUI代理完成复杂指令任务的能力。其整体流程是：给定高级指令、动作历史和视觉图像输入，策略模型（LVLM）生成多个包含推理步骤的响应；随后，一个基于统一动作空间设计的可验证奖励函数评估这些响应；最后，利用策略梯度优化算法（GRPO）在KL散度约束下更新策略模型。</p>
<p><img src="https://arxiv.org/html/2504.10458v4/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：GUI-R1框架概览。给定高级指令、动作历史和视觉图像输入，策略模型生成多个包含推理步骤的响应。然后，使用可验证奖励（如动作类型奖励、点击点奖励和输入文本奖励）结合策略梯度优化算法来更新策略模型。</p>
</blockquote>
<p><strong>核心模块：可验证奖励函数</strong>。本工作采用统一动作空间建模策略，整合了不同平台（如桌面、移动、Web）的动作类别，形成一个包含 <code>[complete, close/delete, press_home, click, press_back, type, select, scroll, enter]</code> 等原子动作的统一空间，解决了多平台数据联合训练时的动作空间冲突问题。基于此，设计了用于评估响应准确性的可验证奖励。</p>
<p>最终响应奖励 (R_o) 由格式奖励 (R_f) 和精度奖励 (R_{acc}) 加权组成：(R_o = \alpha R_f + \beta R_{acc})。</p>
<ol>
<li><strong>格式奖励</strong>：评估生成的输出是否符合预期的结构化格式（包含 <code>&lt;think&gt;...&lt;/think&gt;</code> 推理过程和 <code>&lt;answer&gt;...&lt;/answer&gt;</code> 最终答案），这对强化微调过程中的自学习和迭代改进至关重要。</li>
<li><strong>精度奖励</strong>：由动作类型奖励 (R_{act})、点击点奖励 (R_{point}) 和输入文本奖励 (R_{text}) 相加构成。<ul>
<li><strong>动作类型奖励</strong>：预测动作类型 (o^{act}) 与真实类型 (gt^{act}) 完全匹配则得1分，否则为0。</li>
<li><strong>点击点奖励</strong>：预测的点击坐标 (o^{point}=[x, y]) 位于真实边界框 (gt^{bbox}) 内则得1分，否则为0。</li>
<li><strong>输入文本奖励</strong>：预测的输入文本 (o^{text}) 与真实文本参数 (gt^{text}) 的语义 (F_1) 分数大于0.5则得1分，否则为0。</li>
</ul>
</li>
</ol>
<p><strong>训练数据构建</strong>：从多个开源数据集中收集了约1400万条 grounding 和低级任务数据，以及约3万条高级GUI数据。为了进行高效的RFT，使用 Qwen2.5VL-7B 模型为每个示例生成10个响应，并用基于规则的奖励函数进行评估过滤，最终构建了一个包含1.5K低级数据和1.5K高级数据的平衡、高质量训练集 <strong>GUI-R1-3K</strong>（总计3K条）。</p>
<p><img src="https://arxiv.org/html/2504.10458v4/x3.png" alt="数据分布"></p>
<blockquote>
<p><strong>图3</strong>：数据集GUI-R1-3K中图像类别和动作类别的数量及难度分布示意图。</p>
</blockquote>
<p><strong>创新点</strong>：1) <strong>首次</strong>将规则化RFT应用于高级GUI代理任务；2) 设计了<strong>统一动作空间</strong>及相应的可验证奖励函数，实现了跨平台、跨任务粒度的可靠评估；3) 利用奖励函数筛选构建了<strong>高质量小规模数据集</strong>，极大提升了训练效率和性能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在八个涵盖三个不同平台（移动、桌面、Web）和三种任务粒度（低级grounding、低级任务、高级任务）的基准上进行了评估，包括 ScreenSpot、ScreenSpot-Pro、AndroidControl-Low、GUI-Act-Web、OmniAct-Web、OmniAct-Desktop、AndroidControl-High 和 GUI-Odyssey。使用动作类型准确率（Type）、点击点准确率（Grounding/GR）和步骤成功率（SR）作为评估指标。对比方法包括 SOTA SFT 方法（如 OS-Atlas）、闭源模型（GPT-4o）以及同样使用 RFT 的同期工作 UI-R1。基模型为 QwenVL2.5-3B/7B。</p>
<p><img src="https://arxiv.org/html/2504.10458v4/x1.png" alt="性能总览"></p>
<blockquote>
<p><strong>图1</strong>：GUI-R1在涵盖多个平台和任务粒度的八个评估数据集上取得了最佳性能，展示了RFT在GUI代理任务中的巨大潜力。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>Grounding能力</strong>：在ScreenSpot和ScreenSpot-Pro基准上，仅使用0.02%数据（3K vs. 14M）的GUI-R1-3B模型超越了使用大规模SFT数据的Os-Atlas-7B。与在同数据集上SFT得到的QwenVL2.5-3B*模型相比，GUI-R1-3B在ScreenSpot和ScreenSpot-Pro上分别取得了26.3%和82.8%的性能提升。</li>
<li><strong>低级任务能力</strong>：在四个低级任务基准上，GUI-R1-3B将平均步骤成功率从基线的55.65提升至80.88。相比同期RFT工作UI-R1-3B，取得了约10个百分点的提升。</li>
<li><strong>高级任务能力</strong>：在AndroidControl-High和GUI-Odyssey基准上，GUI-R1-3B超越了所有对比方法。相比闭源模型GPT-4o，绝对提升达21.06分；相比专注于低级任务的UI-R1-3B，平均提升3.4分，在GUI-Odyssey的步骤成功率上领先27.2%。</li>
</ol>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2504.10458v4/x4.png" alt="消融研究"></p>
<blockquote>
<p><strong>图4</strong>：图像分辨率和数据质量的消融研究。使用过滤后的高质量数据集（GUI-R1-3K）能更快收敛并获得更高性能上限；提高图像分辨率能加速收敛并提升性能。</p>
</blockquote>
<ol>
<li><strong>数据质量与图像分辨率</strong>：使用过滤后的高质量数据集（GUI-R1-3K）相比使用未过滤的低质量数据，模型收敛更快、性能上限更高。将图像分辨率提升一倍（从约100万像素到约200万像素）能加速RFT收敛并提高性能上限。</li>
<li><strong>奖励函数系数</strong>：降低格式奖励系数 (\alpha)，提高精度奖励系数 (\beta)，能带来一致的性能提升，因为放大正确答案的优势能带来更多性能改进。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.10458v4/x5.png" alt="训练过程可视化"></p>
<blockquote>
<p><strong>图5</strong>：GUI-R1训练过程可视化。格式奖励在训练早期快速收敛，而精度奖励在训练后期成为差异奖励的主要来源。平均响应长度先下降后逐渐上升，但未出现“顿悟时刻”。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个利用基于规则的强化微调（RFT）来增强LVLM在高级GUI动作预测任务中推理能力的框架GUI-R1。</li>
<li>设计了一个基于规则的统一动作空间奖励函数，能够高效、可靠地验证跨平台和任务粒度的GUI任务响应，支撑了高质量数据筛选和模型训练。</li>
<li>构建了高质量、多样且复杂的小规模微调数据集GUI-R1-3K，显著提升了训练效率和模型性能，并在八个基准测试中全面领先。</li>
</ol>
<p><strong>局限性</strong>：论文在训练过程可视化中指出，在非交互环境的单图像输入训练方法下，模型未能出现“顿悟时刻”（aha moment），这可能是因为模型无法自主回溯错误动作序列。</p>
<p><strong>启示</strong>：这项工作证明了基于统一动作空间规则建模的强化学习在提升GUI代理执行能力方面的巨大潜力和数据高效性。为未来研究树立了一个强基准。探索交互式环境下的多图像高级任务训练，可能是诱导“顿悟时刻”出现、进一步提升模型复杂推理能力的一个潜在方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有GUI智能体依赖监督微调（SFT）、需要大量数据且泛化能力有限的问题，提出**GUI-R1**——首个基于**强化学习**的通用视觉-语言动作模型。其核心方法是通过**统一动作空间规则建模**，并采用**组相对策略优化（GRPO）** 算法，利用跨平台的少量高质量数据进行训练。实验表明，GUI-R1仅使用先前方法0.02%的数据量（3K vs. 13M），便在移动、桌面、网页等八个基准测试中取得了优越性能，验证了强化学习在提升GUI智能体执行能力方面的巨大潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.10458" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>