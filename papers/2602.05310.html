<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.05310" target="_blank" rel="noreferrer">2602.05310</a></span>
        <span>作者: Kong, Jipeng, Liu, Xinzhe, Lin, Yuhang, Han, Jinrui, Schwertfeger, Sören, Bai, Chenjia, Li, Xuelong</span>
        <span>日期: 2026/02/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人足球是一项对感知-行动能力集成要求极高的挑战，需要机器人同时处理动态视觉感知、全身平衡控制和时空精确的踢球动作。现有方法主要分为两种范式：一是模块化分层架构，将感知、运动规划和底层控制解耦，但常因模块间表示差异导致不稳定；二是端到端强化学习（RL）框架，试图联合优化从视觉输入到运动控制，但面临行走、踢球和撞击后恢复等多个目标间的奖励冲突，导致训练不稳定。尽管近期工作尝试结合对抗性运动先验（AMP），但仍依赖于人工设计的奖励，限制了可扩展性和现实世界鲁棒性。</p>
<p>本文针对上述感知与行动集成中的不稳定性和奖励冲突问题，提出了一个新视角：不应将复杂行为视为单一优化问题，而应通过课程学习的方式渐进式掌握。具体而言，将足球技能学习分解为三个阶段：先通过人类运动跟踪掌握基础动作技能，再通过轻量级感知集成实现位置泛化，最后通过物理感知的模拟到现实（sim-to-real）迁移解决部署问题。本文的核心思路是提出一个名为PAiD（Perception-Action integrated Decision-making）的渐进式架构，通过分阶段分解技能习得过程，建立稳定的基础技能，避免感知集成时的奖励冲突，并最小化模拟到现实的差距。</p>
<h2 id="方法详解">方法详解</h2>
<p>PAiD框架将鲁棒足球技能的习得分解为三个渐进阶段：1）通过运动跟踪获取运动技能；2）通过感知引导实现位置泛化；3）通过物理感知实现模拟到现实迁移。</p>
<p><img src="https://arxiv.org/html/2602.05310v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：感知-行动集成决策（PAiD）框架概览。流程包含四个主要部分：(1) <strong>运动跟踪</strong>：将多样化的人类踢球动作重定向到人形机器人，并通过自适应采样训练统一的跟踪策略，在没有感知噪声的情况下掌握基础技能。(2) <strong>感知引导踢球</strong>：为策略配备自我中心感知和特定任务奖励，使踢球技能泛化到随机化的静态和滚动球目标。(3) <strong>物理感知模拟到现实迁移</strong>：通过将模拟接触动力学与现实世界测量（球下落和滚动测试）对齐，并结合物理引导的观测噪声，来弥合现实差距。(4) <strong>现实世界部署</strong>：在Unitree G1上成功部署PAiD。</p>
</blockquote>
<p><strong>第一阶段：通过运动跟踪获取足球技能</strong><br>本阶段目标是在无感知干扰的环境中，从人类足球运动员的运动捕捉数据中学习高保真、物理可行的踢球原语，作为后续阶段的稳定运动先验。</p>
<ul>
<li><strong>数据准备</strong>：收集了13个不同的人类踢球动作，分为标准踢球（覆盖不同距离和角度）和风格化踢球（模仿著名球员）。通过GMR方法将动作重定向到机器人，并标注了踢球脚（左/右）。</li>
<li><strong>统一运动跟踪</strong>：采用基于BeyondMimic的仅偏航角对齐方法，使机器人能调整朝向以踢向不同角度。为应对不同难度动作（如风格化踢球更难）带来的训练挑战，引入了<strong>自适应采样策略</strong>。该策略维护一个在动作索引和离散化阶段区间上的失败直方图，并根据失败频率采样动作-阶段对，从而自动将训练导向整个动作库中最困难的部分，促进所有技能的均衡掌握。同时，在第一阶段就加入了地形随机化（不规则、不平坦表面），使策略能适应各种地形。</li>
<li><strong>奖励设计</strong>：本阶段主要使用运动跟踪奖励（如锚点位置/朝向、身体位置/朝向、线速度/角速度跟踪）和正则化奖励（如动作速率、关节限位惩罚），具体权重见表I的“Weight (I)”列。</li>
</ul>
<p><strong>第二阶段：感知引导的位置泛化</strong><br>本阶段目标是在保留第一阶段习得运动结构为先验的基础上，引入轻量级感知，使策略能泛化到不同球位并实现导向球门的射门。</p>
<ul>
<li><strong>球位置采样</strong>：对于每个选定的参考动作，在由其终止位置决定的标称位置附近进行随机采样。采样包括角度扰动和半径偏移，从而在预期踢球方向周围形成一个可行的球位置弧线，促进位置泛化而不破坏动作时序。此外，会为球采样一个初始线速度，使策略能够处理滚动的球。</li>
<li><strong>感知观察</strong>：策略的观察空间（公式1）包含机器人本体感知、运动跟踪目标以及<strong>球和球门相对于机器人骨盆坐标系的相对位置</strong>（公式2, 3）。在现实部署中，球位置通过融合视觉和雷达的定位获得。</li>
<li><strong>轻量级感知-行动奖励</strong>：保留跟踪奖励，但放宽全局位置约束（例如将锚点位置跟踪权重设为0），以允许机器人移动和调整站姿。仅添加少量任务奖励（见表I “Weight (II)”列）：1) <strong>球接近度奖励</strong>：激励机器人靠近球，在第一次有效踢球接触后冻结该奖励；2) <strong>接触正确性奖励</strong>：仅在首次接触球的脚与动作标签指定的踢球脚一致时给予一次性奖励；3) <strong>击球方向先验</strong>：鼓励脚部摆动与预期侧向踢球方向对齐；4) <strong>踢后结果塑造奖励</strong>：在正确脚接触后，鼓励球速方向对准目标，并激励足够的平面球速，同时惩罚过高的垂直速度。此外，还加入了腰部动作速率正则化、骨盆直立度惩罚等稳定项，以在感知驱动调整下保持平衡。</li>
</ul>
<p><strong>第三阶段：物理感知的模拟到现实迁移</strong><br>为缩小模拟与现实的差距，本框架提出了结合接触动力学识别和结构化域随机化（DR）的策略。</p>
<ul>
<li><strong>接触动力学识别</strong>：通过简单的现实世界实验（球下落实验和滚动实验）来校准球与地面的接触动力学参数。在模拟器（IsaacSim）中复现实验，并使用CMA-ES优化算法，通过最小化轨迹匹配损失（公式4），识别出一组关键的接触参数（静/动摩擦、恢复系数、线性/角阻尼系数）。论文对硬地面和足球场地表面分别进行了识别，并在策略训练时均匀使用这两组参数集，同时添加高斯噪声扰动以覆盖残差建模误差。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.05310v1/images/systemID_v3.png" alt="物理行为对比"></p>
<blockquote>
<p><strong>图2</strong>：参数识别后，足球在现实世界与模拟中物理行为的对比。(a)-(b)对比球下落实验，(c)-(d)对比滚动实验，显示模拟能紧密匹配现实观测。</p>
</blockquote>
<ul>
<li><strong>物理引导的域随机化</strong>：除了标准的域随机化项（如表II所示的机器人摩擦、关节默认位置、质心偏移、外力扰动等），还针对足球任务引入了<strong>物理引导的观测噪声模型</strong>。对球和球门的相对位置观测注入零均值高斯噪声，其噪声大小（公式5）与目标相对于机器人的速度成正比，与其距离成正比，从而更真实地模拟因机器人运动和部分可观测性导致的感知误差。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Unitree G1人形机器人平台上进行，使用IsaacSim进行训练，并在模拟和现实环境中评估。对比的基线方法包括：1) <strong>AMP-based</strong>：结合AMP奖励和感知的端到端方法；2) <strong>Modular</strong>：模块化流水线方法；3) <strong>Stage-agnostic</strong>：单阶段训练变体（即不进行阶段分解）。评估围绕三个研究问题展开。</p>
<p><strong>Q1: 运动跟踪质量</strong><br>在Mujoco中评估跟踪质量，指标包括G-MPJPE（全局平均关节位置误差）、MPJPE（局部平均关节位置误差）、速度误差和AUJ评分（衡量动作平滑度）。PAiD在所有这些指标上均优于AMP-based和Modular基线，证明了其学习高保真、人类式运动的能力。</p>
<p><strong>Q2: 足球射门能力</strong><br>在模拟中评估静态球和滚动球场景下的成功率和准确率（以球初始方向与目标方向之间夹角的余弦值衡量）。</p>
<p><img src="https://arxiv.org/html/2602.05310v1/images/grid_success_rate-2.png" alt="静态球成功率"></p>
<blockquote>
<p><strong>图3</strong>：静态球场景下，在整个工作空间内的射门成功率空间分布热图。PAiD（右）相比Stage-agnostic（中）和AMP-based（左）方法，在更广的区域保持了高成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.05310v1/images/grid_mean_cosine-2.png" alt="静态球准确率"></p>
<blockquote>
<p><strong>图4</strong>：静态球场景下的踢球准确率（方向余弦值）热图。PAiD在大部分区域实现了接近1的高准确率，显著优于对比方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.05310v1/images/grid_success_rate_rolling_2.png" alt="滚动球成功率"></p>
<blockquote>
<p><strong>图5</strong>：滚动球拦截场景下的成功率热图。PAiD能有效处理动态球，成功率分布与静态球场景类似且优于基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.05310v1/images/grid_mean_cosine_rolling_2.png" alt="滚动球准确率"></p>
<blockquote>
<p><strong>图6</strong>：滚动球场景下的踢球准确率热图。PAiD在滚动球条件下也保持了高准确率。</p>
</blockquote>
<p><strong>关键数值结果</strong>：对于静态球，PAiD实现了<strong>91.3%<strong>的平均成功率，显著高于AMP-based（68.8%）和Modular（56.3%）方法。对于滚动球，PAiD成功率为</strong>86.3%<strong>。消融实验表明，</strong>移除自适应采样</strong>会导致成功率下降约10%，特别是在困难动作上；<strong>移除物理引导的域随机化</strong>会使现实世界性能下降超过20%。</p>
<p><strong>Q3: 现实世界部署</strong><br>将训练好的策略零样本迁移到Unitree G1机器人上，在室内硬地面和室外人工草坪上进行测试。</p>
<p><img src="https://arxiv.org/html/2602.05310v1/images/real_world_exp/scatter_from_csv1_new_01.png" alt="现实世界球门落点"></p>
<blockquote>
<p><strong>图7</strong>：室内硬地面上，射向球门的球落点分布。点越接近球门中心（虚线框）且颜色越暖（表示球速越快），性能越好。PAiD的落点更集中且球速更高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.05310v1/images/real_world_exp/point_pairs_1_new_01.png" alt="现实世界轨迹对比"></p>
<blockquote>
<p><strong>图8</strong>：机器人足部末端执行器轨迹与参考人类运动轨迹的对比。红色为机器人轨迹，蓝色为人类参考轨迹，两者在关键踢球阶段高度吻合，证明了动作的高保真度。</p>
</blockquote>
<p>现实实验显示，PAiD策略能成功应对不同的球初始位置（静态/滚动）、光照变化和物理扰动（如被推动），并在不同地面（室内/室外）上保持一致的执行效果。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了一个渐进式的学习框架（PAiD）</strong>：将复杂的足球技能分解为运动获取、感知-行动集成和模拟到现实迁移三个阶段，有效避免了端到端方法中的奖励冲突，并建立了稳定的技能基础。</li>
<li><strong>设计了一种统一的自适应运动跟踪机制和轻量级感知集成方法</strong>：前者通过自适应采样从多样的人类动作中稳健地学习运动技能；后者通过引入极简的任务奖励实现位置泛化，同时保持了人类动作风格。</li>
<li><strong>提出了一种物理感知的模拟到现实迁移策略</strong>：通过基于真实物理实验的系统识别来校准关键接触参数，并设计物理引导的观测噪声模型，显著提升了策略在现实世界中的鲁棒性和样本效率。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前方法主要针对射门技能，未来需要扩展到更复杂的足球场景，如带球、防守和多人对抗。此外，感知模块目前依赖于特定的定位系统（视觉+雷达）。</p>
<p><strong>启示</strong>：PAiD的“分而治之”策略为复杂具身技能的习得提供了一个可扩展的框架。其核心思想——先掌握与感知无关的基础技能，再逐步、轻量地集成感知进行泛化——可广泛应用于其他需要高精度操作与动态环境适应相结合的任务中，例如机器人灵巧操作、移动抓取等。物理感知的迁移方法也强调了基于真实物理特性进行系统识别的重要性，而非盲目地进行随机化。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人足球技能学习中感知与动作模块割裂或端到端训练目标冲突的问题，提出渐进式感知-动作决策框架（PAiD）。该方法将技能学习分解为三个阶段：通过人体运动跟踪获取基础踢球动作，进行轻量级感知-动作整合以实现位置泛化，最后通过物理感知的模拟到现实迁移。在Unitree G1机器人上的实验表明，该方法能实现高保真、类人的踢球动作，在静态/滚动球、不同位置及干扰条件下均表现鲁棒，并在室内外场景中保持稳定执行。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.05310" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>