<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Human-assisted Robotic Policy Refinement via Action Preference Optimization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Human-assisted Robotic Policy Refinement via Action Preference Optimization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.07127" target="_blank" rel="noreferrer">2506.07127</a></span>
        <span>作者: Xia, Wenke, Yang, Yichu, Wu, Hongtao, Ma, Xiao, Kong, Tao, Hu, Di</span>
        <span>日期: 2025/06/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，作为机器人部署系统基础模型的视觉-语言-动作模型，其主流训练范式依赖于大规模专家演示数据集。这严重限制了其部署后的精炼能力，因为它们缺乏从现实世界中遇到的失败或新场景中持续学习的内在能力。为增强机器人系统的持续学习能力，交互式模仿学习框架被提出，通过迭代的人类在环纠正反馈来精炼易出错的轨迹。其中，行为克隆被广泛用于以监督学习方式微调基础策略模型。然而，行为克隆未能充分利用失败轨迹（这是学习鲁棒策略的宝贵信号），而强化学习方法由于固有的不稳定性以及开发通用价值函数的挑战，在训练大规模VLA模型时面临显著的可扩展性限制。</p>
<p>本文针对VLA模型在下游操作任务中，特别是在次优人类干预范式下的有效适应问题，提出了动作偏好优化这一新范式。该方法超越了行为克隆和强化学习的局限性，通过学习交互过程中捕获的动作级偏好，充分利用失败轨迹中的宝贵信息，同时保持大规模VLA模型所需的优化稳定性。核心思路是：通过人机协作框架收集交互轨迹，并利用基于二元期望信号的自适应重加权偏好对齐算法，使VLA模型能够有效抑制易失败的动作，同时增强纠正动作的适应能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>动作偏好优化方法旨在促进VLA模型的持续迭代改进。其整体流程包括两个关键部分：人机协作部署以收集交互轨迹，以及基于自适应重加权的动作偏好优化过程。</p>
<p><img src="https://arxiv.org/html/2506.07127v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧（a）为人机协作部署框架，用于可靠部署和通过人类干预收集交互轨迹。右侧（b）为动作偏好优化过程，通过自适应重加权使VLA模型从次优交互轨迹中学习。圆圈的大小代表其在训练中的权重。</p>
</blockquote>
<p><strong>1. 人机协作部署</strong>：首先在专家演示数据集上通过行为克隆微调预训练的VLA模型，获得初始基础策略π_θ^0。部署该策略执行任务时，人类操作员监控策略执行，并在策略遇到挑战性场景时进行干预。此过程收集交互轨迹数据集𝒟_h。轨迹中的每个动作被标注类别c_t：c_t=1表示由策略执行的动作，c_t=2表示由人类干预纠正的动作。此外，将人类干预发生前K步内的动作重新标记为不期望的，标注为c_t=0。最终，将专家演示𝒟_e和交互数据集𝒟_h结合用于后续的机器人动作偏好优化。</p>
<p><strong>2. 动作偏好优化</strong>：该方法旨在最大化次优交互轨迹的效用，并确保VLA模型的稳定微调。直接应用LLM的偏好对齐方法到VLA模型面临两大挑战：1) <strong>不可逆交互</strong>：物理交互的不可逆性使得在相同条件下收集完美的正负动作样本极为困难；2) <strong>令牌概率不匹配</strong>：自回归VLA模型将连续动作离散化为令牌，导致令牌概率与真实动作损失之间存在根本性不匹配。</p>
<p>为解决这些问题，APO采用卡尼曼和特沃斯基的前景理论，制定了一个从交互衍生的二元期望信号进行偏好对齐的新目标。该目标放松了对配对偏好的严格要求，使其适用于从不可逆的机器人交互轨迹中学习。具体地，首先根据标准方法估计模型π_θ相对于参考模型π_ref的奖励函数r_θ。然后，构建一个效用函数v，用于估计机器人数据上的相对收益，其中引入惩罚项z_0（定义为π_θ与π_ref之间的KL散度）以防止模型过度偏离参考模型。</p>
<p>核心创新在于<strong>自适应重加权方法</strong>，以弥合离散令牌预测和连续动作回归之间的差距。对于每个样本，首先估计其连续动作的L1损失l_i，并进行批次级别的归一化得到权重w_i。然后，根据样本的期望类别（c_i≠0为期望，c_i=0为不期望）和权重w_i，动态调整公式（2）中的λ_D和λ_U参数：</p>
<ul>
<li>对于期望数据，增加具有高动作预测误差样本的权重（λ_D = 1 - e^{-β_D * w}）。</li>
<li>对于不期望数据，增加其动作接近失败动作的样本的权重（λ_U = e^{-β_U * w}）。</li>
</ul>
<p>通过这种样本级的权重精细化，APO实现了对训练中每个样本相对影响的细粒度控制，从而将梯度优化集中在易出错的交互动作上，增强了VLA模型从次优操作纠正轨迹中学习的性能和优化稳定性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在RoboMimic仿真环境中进行，涵盖了“煮咖啡”、“堆叠三个”、“三件组装”、“方形”等4个长视距精细操作任务。使用预训练的OpenVLA模型作为基础模型，并通过LoRA进行参数高效微调。评估时，每个任务在三个未见过的种子下进行50次试验，报告平均成功率。</p>
<p><strong>对比方法</strong>包括：Dagger（混合数据行为克隆）、Sirius（加权行为克隆）、DPO（使用高斯噪声生成配对负样本）、TPO（基于干预时间的轨迹级偏好优化）、KTO（基于干预时间的二元信号偏好优化）。</p>
<p><img src="https://arxiv.org/html/2506.07127v3/x5.png" alt="实验结果对比表"></p>
<blockquote>
<p><strong>表1</strong>：RoboMimic中4个操作任务的对比实验结果。APO的平均成功率达到48.0%，优于所有基线方法。结果表明，现有的行为克隆方法在VLA模型背景下难以实现高效适应，而APO的自适应重加权偏好优化方法实现了稳定的改进。</p>
</blockquote>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>主任务性能</strong>：APO在平均成功率上达到48.0%，显著优于基础策略（40.5%）及其他所有对比方法。DPO性能最弱，TPO不稳定，而KTO和APO通过KL约束保留了参考模型知识，性能更优。APO通过自适应重加权实现了更精确的样本权重控制，获得了更显著的性能提升。</li>
<li><strong>泛化到新场景</strong>：在位置、背景、纹理三种干扰新场景下测试模型的鲁棒性。APO在干扰场景下的平均成功率为28.0%，优于其他方法（KTO为24.0%）。同时，在原始任务上的测试表明，行为克隆方法存在严重的灾难性遗忘，而偏好优化方法（包括APO）通过参考模型约束缓解了性能下降，且APO在原始任务上的平均成功率（45.3%）甚至超过了基础模型。</li>
<li><strong>终身学习能力</strong>：通过每20次交互滚动更新一次模型，考察迭代改进能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.07127v3/x4.png" alt="终身学习结果"></p>
<blockquote>
<p><strong>图4</strong>：APO的终身学习结果。在“方形”和“堆叠三个”任务中，APO通过迭代的人机交互与优化，性能持续提升，并显著优于使用相同数量专家演示训练的行为克隆基线。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：验证自适应重加权机制的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.07127v3/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。移除自适应重加权（“APO w/o AR”）或二元期望信号（“APO w/o DS”）都会导致性能下降，证明了这两个核心组件的必要性。</p>
</blockquote>
<ol start="5">
<li><strong>真实世界实验</strong>：在精细插入任务中进行真实世界测试，APO在面对位置干扰时表现出更强的鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.07127v3/x7.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图7</strong>：真实世界精细插入任务设置及结果。APO在面对初始位置干扰时，成功率高于基础模型和行为克隆方法。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>动作偏好优化</strong>范式，使VLA模型能够通过人机交互从失败中学习，实现了部署后的持续精炼。</li>
<li>设计了<strong>自适应重加权算法</strong>，有效解决了将LLM偏好对齐方法迁移到VLA模型时面临的不可逆交互和令牌概率不匹配两大挑战。</li>
<li>构建了一个完整的<strong>人机协作部署与优化框架</strong>，并通过仿真与真实实验验证了其在提高任务成功率、泛化鲁棒性和终身学习能力方面的有效性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，APO框架依赖于人类干预来收集交互数据以进行策略优化。这在一定程度上引入了人力成本，并且优化效率受限于人类干预的频率和数据质量。</p>
<p><strong>启示</strong>：这项工作为通过人机协作实现VLA模型的高效、稳定优化提供了新思路。APO将偏好对齐成功应用于机器人领域，表明精心设计的算法可以克服物理交互与离散建模之间的固有障碍。未来研究可探索更自动化的失败检测与干预请求机制，以减少对人力的依赖，或将该框架扩展到更复杂的多任务、多模态场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对Vision-Language-Action (VLA) 模型依赖离线专家演示、部署后难以从失败中学习精炼的核心问题，提出了动作偏好优化 (APO) 方法。该方法构建人-机器人协作框架收集干预轨迹，并设计自适应重加权算法，利用二元期望信号抑制易失败动作、增强纠正动作适应。实验在仿真和真实场景中进行，证明了该人辅助框架在各种操作任务中具有优越的泛化性和鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.07127" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>