<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Learning a Generalizable 3D Scene Representation from 2D Observations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Towards Learning a Generalizable 3D Scene Representation from 2D Observations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.10943" target="_blank" rel="noreferrer">2602.10943</a></span>
        <span>作者: Stefan Wermter Team</span>
        <span>日期: 2026-02-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作需要理解3D场景几何以进行抓取规划、碰撞预测和物体交互推理。然而，当前主流的计算机视觉流程主要基于2D图像和2D骨干网络运行，即使目标任务需要明确的3D推理。本文倡导一种面向嵌入式智能体的“3D优先”范式：从自中心视角的观测中通过自监督方式构建3D场景表示，并利用这些表示进行下游感知与控制。将2D到3D的提升过程显式化，有助于定位几何误差，并为多任务提供一个统一的世界模型。</p>
<p>神经辐射场（NeRF）已成为学习此类3D表示的一个引人注目的解决方案。然而，标准NeRF会过拟合到单个场景，需要为新环境重新训练。可泛化NeRF（GNeRF）虽能跨场景训练，但现有方法通常在相机坐标系下操作，这限制了其在机器人任务（如机械臂运动规划）中的直接应用，因为机器人需要全局、统一的3D工作空间表示。此外，传统的立体视觉方法无法推断被遮挡区域的完整几何。</p>
<p>本文针对机器人操作任务，提出了一个从2D观测中学习可泛化3D场景表示的新视角。核心思路是：构建一个在全局工作空间坐标系下运行的可泛化NeRF模型，它能够从稀疏的自中心视角灵活集成信息，并直接预测出包含遮挡区域的完整3D占用概率，而无需针对新场景进行微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个端到端训练的3D占用预测器，可以从一个或多个连续的自中心源视角估计场景表示。该预测器包含三个神经模块，通过目标视图与其神经渲染之间的图像重建损失进行训练。训练时，随机采样源视图和目标视图；推理时，则使用可用的源视图。</p>
<p><img src="https://arxiv.org/html/2602.10943v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：提出的神经架构和数据流概览。包含三个主要模块：2D特征提取器、在世界坐标系中构建的成本体积，以及用于几何重建和遮挡推理的3D U-Net。最终通过MLP和体渲染得到颜色与密度。</p>
</blockquote>
<p><strong>整体流程与核心模块</strong>：</p>
<ol>
<li><strong>2D特征提取器</strong>：处理每个源视图。使用一个轻量级的9层卷积骨干网络，包含两个下采样阶段，在H/4 × W/4分辨率下生成像素对齐的32通道特征图。</li>
<li><strong>世界坐标系成本体积构建</strong>：这是方法的关键创新。首先，在全局世界坐标系中定义一个体素间距为1厘米的成本体积网格。对于每个3D点，利用相机内参和外参将其投影到每个源视图中，并获取RGB值及对应的像素对齐特征。然后，通过计算每个通道的方差来聚合跨视图的特征，最终形成成本体积体素网格。与在单个相机视锥内构建成本体积的MVSNeRF等方法不同，此工作在全局坐标系下操作，使其输出直接适用于机器人任务。</li>
<li><strong>3D几何重建与推理</strong>：一个3D U-Net接收构建好的成本体积，通过三个步长卷积阶段逐步下采样，然后再上采样回来，输出一个8通道的神经特征体积。最后，一个6层、128维的MLP以双线性插值得到的3D体积特征、点在体积内的相对位置以及观察方向作为输入，回归每个点的体积密度σ和RGB颜色值。通过标准的体渲染公式，可以合成任意目标视角的图像。</li>
</ol>
<p><strong>占用表示与训练</strong>：从预测的密度σ可以计算每个点的不透明度α = 1 - exp(-σ)， α ∈ [0, 1]，本文将其视为概率占用值。在渲染时，对于不透明度累积不完全的像素，使用空场景图像的背景值进行填充，这鼓励模型仅将密度分配给场景中变化的元素（即物体）。训练损失结合了RGB值的均方误差（MSE）和来自Plenoxels的beta损失（L_β），后者鼓励α值趋于二值化（0或1）：L = L_MSE + 0.1 × L_β。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实机器人平台上进行验证。使用人形机器人NICOL，其配备两个4K RGB相机和一个RealSense D435i相机作为头部相机，并额外使用三个静态的RealSense相机从多个视角捕捉工作空间。数据集包含60个场景，每个场景包含5个来自YCB数据集的物体，物体在场景间被重新摆放。使用20个场景作为固定评估集，其中包含从头部相机视角存在遮挡的情况。训练使用40个场景。评估时，将预测的深度图与RealSense深度传感器测量的地面真值进行比较，计算工作空间内有效点的平均绝对误差（MAE_depth）作为几何重建精度的度量，同时用峰值信噪比（PSNR）衡量渲染质量。</p>
<p><strong>对比与消融实验</strong>：论文主要进行了两种消融研究：一是改变源视图配置（单目、双目立体视觉、三个不同头部角度的视图）；二是改变用于训练的场景数量（N_train）。</p>
<p><img src="https://arxiv.org/html/2602.10943v1/figures/nicol_setup_overlay.png" alt="机器人设置"></p>
<blockquote>
<p><strong>图2</strong>：NICOL机器人及所有涉及的工作空间相机设置。展示了机器人头部和三个静态工作空间相机。</p>
</blockquote>
<p>表1展示了关键定量结果。可以得出三个主要结论：首先，PSNR与几何精度强相关，更高的PSNR对应更低的深度误差。其次，视角多样性改善了所有指标，表明模型有效利用了来自不同相机位姿的额外可见几何信息。例如，使用三个头部视角（左、中、右）相比仅使用左眼单目视角，深度误差从37.5毫米降低至26.4毫米，PSNR从22.85提升至23.64。第三，随着训练场景数量从5增加到40，深度误差从45.6毫米持续下降至26.4毫米，这表明方法受益于训练期间更大的场景多样性。</p>
<p><strong>定性结果</strong>：</p>
<p><img src="https://arxiv.org/html/2602.10943v1/figures/predictions_example.png" alt="预测示例"></p>
<blockquote>
<p><strong>图3</strong>：对一个评估场景的预测示例。上行：三个源视图（模型输入）。下行从左至右：目标视图的RealSense真实图像、模型对该视图的神经渲染、模型预测的深度图。注意，尽管下方机翼在输入视图中不可见，但其几何形状被正确重建。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.10943v1/figures/occupancy_prediction_2.jpg" alt="3D占用预测"></p>
<blockquote>
<p><strong>图4</strong>：与图3相同场景的3D占用概率预测可视化。展示了模型推断出的完整3D几何结构。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一种<strong>可泛化的、以工作空间为中心的、几何一致的3D占用预测器</strong>，它直接在全局坐标系中构建表示，适用于机器人操作；2）模型能够<strong>灵活集成任意数量的源视图</strong>，并<strong>泛化到未见过的物体摆放</strong>，无需场景特定微调；3）在真实人形机器人平台上成功实现并验证，并利用深度传感器对预测的3D几何进行了定量评估。</p>
<p>论文提到的局限性包括当前数据集规模有限，以及占用表示是单通道的。作者计划扩展数据集以包含更多物体类别和场景复杂度，并研究跨物体类别的知识迁移。此外，当前的单通道占用表示为抓取等操作任务提供了基础，未来计划将模型扩展为预测多通道语义特征，从而利用相同的NeRF框架将2D语义信息提升到3D表示中，实现直接在3D空间中进行与任务相关的场景理解。</p>
<p>这项工作为机器人领域提供了一个有前景的方向：通过学习一个可泛化的、从2D到3D的神经场景表示，为下游的感知、规划和控制任务提供一个统一且富有物理意义的3D世界模型，推动“3D优先”范式的实际应用。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出一种可泛化的神经辐射场方法，旨在从机器人第一视角的2D观测中预测3D工作空间占用。核心问题是构建不依赖相机坐标系、可直接用于机器人操作的全局工作空间场景表示。关键技术采用全局坐标系下的可泛化神经辐射场，通过多视图特征聚合，无需场景特定微调即可泛化到新物体布局。实验在40个真实场景上训练，模型重建误差达到26毫米，能够推断包括遮挡区域在内的完整几何结构，优于传统立体视觉方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.10943" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>