<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Dejavu: Towards Experience Feedback Learning for Embodied Intelligence - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Dejavu: Towards Experience Feedback Learning for Embodied Intelligence</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.10181" target="_blank" rel="noreferrer">2510.10181</a></span>
        <span>作者: Wu, Shaokai, Ji, Yanbiao, Li, Qiuchang, Zhang, Zhiyi, He, Qichen, Xie, Wenyuan, Zhang, Guodong, Bayramli, Bayram, Ding, Yue, Lu, Hongtao</span>
        <span>日期: 2025/10/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前具身智能领域的主流方法是经过大规模离线训练的统一视觉-语言-动作（VLA）模型，这些模型在固定数据分布上表现出强大的泛化能力。然而，一旦部署到真实环境中，其模型权重（即知识）便被冻结，除非重新训练，否则无法更新。这导致大多数实际系统在部署后“停止学习”，无法通过后续交互经验来提升任务表现。</p>
<p>本文针对智能体部署后知识固化的核心痛点，提出了一种新的视角：借鉴人类利用情景记忆（即“似曾相识”感）通过类比快速适应新问题的能力，探索智能体是否也能通过回忆和重用自身“经验”来改进，而无需基于梯度的权重更新。现有的检索增强方法通常设计用于可训练的策略，或在静态离线语料库上操作，且检索基于紧凑的状态抽象，而非现代VLA模型丰富的开放词汇视觉-语言接口，因此未能为冻结的统一策略提供一个简单的、部署时持续改进的机制。</p>
<p>本文的核心思路是提出一个通用的部署后学习框架Dejavu，其核心组件经验反馈网络（EFN）通过检索执行记忆来增强冻结的VLA策略，利用强化学习训练EFN输出残差动作来修正基础策略，使得智能体能够通过积累和复用自身经验来实现“从经验中学习”。</p>
<h2 id="方法详解">方法详解</h2>
<p>Dejavu框架的核心是在冻结的VLA策略之上，包裹一个以经验为中心的控制器EFN，旨在不更新骨干网络权重的情况下，于部署阶段改进智能体行为。</p>
<p><img src="https://arxiv.org/html/2510.10181v2/x1.png" alt="方法框架-训练"></p>
<blockquote>
<p><strong>图1</strong>：EFN训练流程。EFN接收当前观测和从经验库中检索到的匹配经验（包含当前帧、动作及下一帧），通过SAC算法训练一个残差策略，其目标是修正基础策略的动作，使得执行后的下一帧与检索记忆中的后继帧在语义上匹配。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.10181v2/x2.png" alt="方法框架-推理"></p>
<blockquote>
<p><strong>图2</strong>：EFN部署推理流程。智能体根据当前任务指令过滤经验库候选集，结合视觉相似性和轨迹效率优先级进行检索，应用训练好的EFN残差修正基础动作，并在任务成功后将该轨迹在线加入经验库，实现记忆增长。</p>
</blockquote>
<p>整体流程分为离线训练和在线部署两个阶段。训练阶段，EFN学习一个残差策略；部署阶段，所有网络参数冻结，智能体通过检索和应用训练好的EFN残差来修正动作，并动态增长经验库。</p>
<p><strong>核心模块一：经验库设计</strong>。经验库以完整轨迹为单位组织，存储每个有效交互步的三元组：VLA视觉编码器特征 <code>F_t</code>、由此衍生的用于检索的紧凑键向量 <code>k_t</code>、以及基础策略执行的原生动作 <code>a_t^(0)</code>。同时，每个轨迹关联一个由VLA语言模型编码的任务指令嵌入 <code>l_τ</code>。键向量 <code>k_t</code> 通过一种均值-最大值融合技术从视觉特征中提取，并进行归一化，以捕获丰富的语义信息。</p>
<p><strong>核心模块二：检索机制</strong>。查询时，从当前帧以相同方式生成查询向量 <code>q_t</code>。检索分为两步：首先根据指令嵌入相似性筛选出与当前任务最相关的top-n个轨迹作为候选集；然后在该候选集内，结合视觉键的余弦相似性 <code>s_i</code> 和鼓励短轨迹的效率先验 <code>g(L)</code>，计算综合得分 <code>s~_i</code>，并从得分最高的top-k个候选中按softmax分布采样，最终得到一个匹配的经验步 <code>(F^, a^, F^+)</code>。</p>
<p><strong>核心模块三：基于SAC的残差策略优化</strong>。EFN的核心是学习一个残差策略 <code>π_φ(Δa_t | c_t)</code>，其中上下文 <code>c_t</code> 编码了当前观测、基础动作、检索到的经验及任务指令。最终执行动作为基础动作与残差之和：<code>a_t = a_t^(0) + Δa_t</code>。训练采用软演员-评论家（SAC）算法，其创新之处在于设计了一个密集的语义匹配奖励。该奖励计算执行动作后实际下一帧的特征 <code>u(F_{t+1})</code> 与检索经验中后继帧特征 <code>u(F^+)</code> 的余弦相似度 <code>r_t^sem</code>，并辅以残差幅度的正则化：<code>r_t = λ_sem * r_t^sem - λ_res * ||Δa_t||_2^2</code>。此外，奖励函数还引入了防闲置惩罚、进度奖励等 shaping 项，以鼓励有效探索和缩短轨迹。</p>
<p><img src="https://arxiv.org/html/2510.10181v2/x4.png" alt="奖励分解"></p>
<blockquote>
<p><strong>图4</strong>：一条代表性轨迹上EFN shaping奖励的分解。早期阶段因取得进展获得高奖励；当在良好视角附近闲置时，防闲置惩罚抑制奖励；当再次朝检索到的后继帧移动时，进度和运动奖励项占主导。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>冻结权重</strong>：EFN作为外部修正模块，不更新大型VLA骨干网络参数，实现了纯粹通过记忆更新的部署后改进。2) <strong>实时增长的经验库</strong>：部署时持续添加新的成功轨迹，使智能体能够利用不断积累的经验。3) <strong>密集语义奖励</strong>：利用检索到的“下一帧”作为密集学习信号，替代稀疏的成功/失败反馈，使训练更高效。</p>
<p><img src="https://arxiv.org/html/2510.10181v2/x5.png" alt="残差可视化"></p>
<blockquote>
<p><strong>图5</strong>：潜在动作空间中的残差修正。通过PCA将基础动作和EFN修正后的动作投影到2D空间，箭头表示残差Δa_t。可见EFN施加的是围绕基础动作的细小、局部修正，而非完全覆盖。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在仿真环境LIBERO数据集和真实世界AgiBot-G1平台上进行了实验。评估了EFN与三种不同的冻结VLA基础策略（OpenVLA、UniVLA、GO-1）的结合效果。</p>
<p>对比的基线方法包括：仅检索的kNN-RAG、仅残差的ResAct、检索增强RL方法R2A、以及测试时训练方法GC-TTT。所有方法在相同的交互预算和经验库设置下进行比较。</p>
<p>关键实验结果如下表所示，EFN在成功率和平均步数（效率）上均 consistently 超越所有基线方法。随着经验库容量从300增加到1000，性能得到进一步提升。</p>
<p><img src="https://arxiv.org/html/2510.10181v2/x7.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>表1</strong>：LIBERO数据集上的性能（平均成功率↑，平均步数↓）。EFN在不同基础策略（OpenVLA, UniVLA, GO-1）和不同任务类型（空间、物体、目标、长序列）上均取得最佳或接近最佳性能。例如，在OpenVLA基础上，EFN（Vol=1000）将平均成功率从76.5%提升至87.0%，步数从160.2减少至156.7。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.10181v2/x8.png" alt="真实世界结果表"></p>
<blockquote>
<p><strong>表2</strong>：AgiBot-G1真实平台上的性能（基于GO-1策略）。EFN在四个操纵任务上均取得了最高的成功率，显著优于基础策略和其他增强方法，证明了其在实际机器人部署中的有效性。</p>
</blockquote>
<p>消融实验验证了各个组件的贡献：1) <strong>检索机制</strong>：结合指令过滤和效率先验的检索策略优于纯视觉kNN检索。2) <strong>奖励设计</strong>：包含防闲置惩罚的shaping奖励比简单的语义相似奖励训练更稳定、效果更好。3) <strong>残差学习</strong>：学习残差动作比直接输出完整动作或仅使用检索动作（kNN-RAG）更有效。</p>
<p><img src="https://arxiv.org/html/2510.10181v2/x6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图6</strong>：EFN残差修正的定性可视化。左：当前观测；中：检索到的经验帧；右：修正后动作（半透明）与基础动作对比。即使检索场景与当前场景存在差异（上行），EFN仍能提取出可操作的指导信息。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>提出了EFN</strong>：一个检索条件化的残差模块，通过更新情景记忆库而非微调骨干网络，实现了冻结VLA策略的部署后持续改进。2) <strong>设计了完整的经验反馈学习框架</strong>：包括存储视觉-语言-动作轨迹的经验库、在联合嵌入空间中进行任务相关转移检索的机制，以及利用面向检索后继的相似性塑造奖励训练残差策略的方法。3) <strong>进行了广泛的实验验证</strong>：在仿真和真实机器人平台上，EFN在多种具身任务上均一致性地提升了基础VLA策略的成功率、适应性和鲁棒性，并优于现有的检索增强方法。</p>
<p>论文自身提到的局限性主要在于经验库的管理复杂度可能随着部署时间增长而增加，文中提到可采用标准保留策略（如储层采样）进行管理。</p>
<p>本文对后续研究的启示在于，为大型预训练模型的“后训练”学习提供了一条新路径：即在不触碰模型内部权重的前提下，通过设计外部、可增长的记忆系统和轻量级的反馈控制器，使智能体具备持续从自身交互经验中学习的能力，这更贴近生物智能中“经验积累”的学习范式，并为实际机器人系统在开放环境中的长期适应提供了可行的技术方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身智能体部署后权重固定、无法在线学习以提升任务性能的核心问题，提出了Dejavu框架。该框架采用经验反馈网络（EFN），检索与当前情境相似的先验执行记忆，并以此增强冻结的视觉-语言-动作策略的动作预测；通过强化学习与语义相似性奖励训练EFN，确保动作与过去行为一致。实验表明，EFN在多种具身任务中显著提高了适应性、鲁棒性和成功率，优于冻结基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.10181" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>