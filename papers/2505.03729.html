<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Visual Imitation Enables Contextual Humanoid Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Visual Imitation Enables Contextual Humanoid Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.03729" target="_blank" rel="noreferrer">2505.03729</a></span>
        <span>作者: Allshire, Arthur, Choi, Hongsuk, Zhang, Junyi, McAllister, David, Zhang, Anthony, Kim, Chung Min, Darrell, Trevor, Abbeel, Pieter, Malik, Jitendra, Kanazawa, Angjoo</span>
        <span>日期: 2025/05/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，教人形机器人学习技能主要遵循两种互补的路径。基于奖励的方法在模拟中使用无模型强化学习，通过混合任务项（如速度跟踪）和运动自然性正则化器的手工设计目标来塑造行为，但每个新行为都需要调整用户定义的奖励函数和环境脚本。数据驱动的方法则模仿参考运动，例如动作捕捉片段或单目视频，训练模拟角色跟踪它们，但现有工作通常假设平坦地面或手动设计的设置，限制了环境感知的全身控制；即使那些建模人-场景接触的动画系统也依赖于仪器化的动作捕捉舞台，缺乏可扩展性。</p>
<p>本文针对上述痛点，提出了一种新的视角：能否让人形机器人像人类一样，通过观察日常视频来学习多样化的、与环境上下文相关的全身技能，而无需为每个新行为和环境依赖手工调整的奖励或动作捕捉数据？这种根据环境执行恰当动作的能力被称为上下文控制。</p>
<p>本文的核心思路是构建一个名为VideoMimic的“真实-模拟-真实”管道，从单目视频中联合重建4D人-场景几何，将运动重定向到人形机器人，并训练一个强化学习策略来跟踪参考轨迹，最终蒸馏成一个仅依赖本体感觉、局部高度图和期望根方向指令的统一策略，使其能够在未见过的环境中执行恰当的步进、攀爬或坐下等行为。</p>
<h2 id="方法详解">方法详解</h2>
<p>VideoMimic的整体流程是一个从真实世界视频到模拟器，再部署到真实机器人的管道。输入是单目RGB视频（如智能手机随手拍摄），输出是可直接在真实人形机器人上运行的、具有上下文感知能力的控制策略。</p>
<p><img src="https://arxiv.org/html/2505.03729v5/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：VideoMimic真实到模拟流程。输入为日常视频，首先重建每帧人体运动和2D关键点以及稠密场景点云。通过联合优化对齐运动和点云，利用人体高度先验恢复度量尺度，并根据人体关联点配准人体轨迹。点云随后被转换为网格并与重力对齐，运动在重建场景中被重定向到人形机器人。最终得到世界坐标系下的轨迹和模拟器就绪的网格，用于策略训练。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>预处理</strong>：使用现成的先进方法处理视频。使用Grounded SAM2检测和关联视频中的人；使用VIMO恢复每帧的3D SMPL参数（局部姿态θ^t、形状β、3D关节J_3D^t）；使用ViTPose检测2D关键点J_2D^t；使用BSTRO回归脚部接触状态。对于场景，使用MegaSaM或MonST3R获取世界点云（参数化为每帧深度D^t、相机位姿[R^t|t^t]和共享的内参矩阵K）。</li>
<li><strong>联合人-场景重建</strong>：这是实现度量准确对齐的关键。优化变量包括人体的全局平移γ^(1:T)、全局朝向φ^(1:T)、局部姿态θ^(1:T)以及场景点云尺度α。通过将SMPL模型的人体高度作为度量参考，并利用从2D关键点结合深度图反投影得到的3D关节J_<del>3D^t来细化全局轨迹和局部姿态，联合求解α以弥合人体衍生尺度与场景几何之间的残余不匹配。优化目标结合了3D关节距离损失（L_3D，J_</del>3D^t与J_3D^t之间的L1距离）、2D投影损失（L_2D）和时间平滑正则项（L_Smooth），使用JAX实现的Levenberg–Marquardt求解器进行优化。</li>
<li><strong>生成模拟就绪数据</strong>：将重建结果与真实世界重力方向（使用GeoCalib）对齐，并将噪声、稠密的点云转换为轻量级网格（使用NKSR），以便在物理引擎中施加有意义的几何约束并支持高效并行的策略训练。</li>
<li><strong>策略学习</strong>：在IsaacGym模拟器中使用近端策略优化进行训练。策略的观察空间包括本体感觉（关节位置q、速度q_dot、角速度ω、投影重力向量g、历史动作a^(t-1)，历史长度为5）和目标相关观察（目标关节角度、目标根滚转/俯仰、期望根方向——相对于机器人当前位置的x-y偏移和偏航角）。对于高度图条件策略，还提供一个以躯干为中心的11x11网格（采样间隔0.1米）的高程图。奖励函数完全围绕数据驱动的跟踪项（连杆和关节位置、关节速度、脚部接触信号）设计，以最小化手工调整。</li>
</ol>
<p>策略学习分为四个阶段：</p>
<ul>
<li><strong>阶段1：动作捕捉预训练</strong>：在LAFAN动作捕捉数据（重定向到Unitree G1机器人）上预训练策略，使其初步学习具有挑战性的技能，弥合人-机器人形态差距，并为后续学习噪声较大的视频重建数据打下基础。</li>
<li><strong>阶段2：场景条件跟踪</strong>：从预训练策略初始化，引入环境高度图作为条件，在重建的各种地形上随机采样运动，进行DeepMimic风格的跟踪训练。此阶段策略仍接收运动特定的跟踪条件（目标关节角度、根滚转/俯仰、期望根方向）。</li>
<li><strong>阶段3：蒸馏</strong>：使用DAgger将策略蒸馏至一个不观察目标关节角度或根滚转/俯仰的策略。蒸馏后的策略仅以期望根方向（可由摇杆或高层控制器提供）作为控制信号，从而统一了摇杆跟踪和全局参考跟随这两种先前分离的方法。</li>
<li><strong>阶段4：欠条件RL微调</strong>：对蒸馏后的策略再进行一轮RL微调。因为基于目标关节条件学习的行为对于不观察这些目标的策略来说可能是次优的。此阶段可以显著提升性能，并允许将质量较低的参考运动加入训练集。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.03729v5/x2.png" alt="策略训练流程"></p>
<blockquote>
<p><strong>图3</strong>：模拟中的策略训练流程。RL训练从动作捕捉轨迹数据集开始，然后注入高度图观察，在各种环境中跟踪来自视频的全身参考轨迹，接着蒸馏为仅以机器人根位置为条件的策略，最后使用相同的简化观察集对该策略进行RL微调。</p>
</blockquote>
<p>与现有方法相比，VideoMimic的主要创新在于整个流程的集成：从非结构化的单目视频出发，<strong>联合</strong>重建具有物理一致尺度的人和场景，并将其直接用于基于物理的策略学习，最终得到一个<strong>单一、统一</strong>的上下文感知策略，无需为每个技能单独设计奖励或策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：本文在123个随手录制的智能手机视频（包含室内外多样环境下的坐、站、上下楼梯等日常活动）上训练。使用IsaacGym进行模拟训练，并在真实的23自由度Unitree G1人形机器人上部署验证。重建评估使用了SLOPER4D数据集的一个子集。</p>
<p><strong>对比基线</strong>：在重建评估中，与WHAM*（仅恢复人）和TRAM方法进行了比较。如表1所示，VideoMimic在“真实到模拟”、“上下文控制”和“真实机器人”三个维度上均具备能力。</p>
<p><img src="https://arxiv.org/html/2505.03729v5/x6.png" alt="重建结果对比"></p>
<blockquote>
<p><strong>图7</strong>：重建评估结果对比表。我们的方法在人体轨迹精度（WA/W-MPJPE）和场景几何（Chamfer Distance）上均优于基线。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>重建鲁棒性</strong>：如表2所示，本文方法在WA-MPJPE（112.13 mm vs. 149.48 mm）、W-MPJPE（696.62 mm vs. 954.90 mm）和Chamfer距离（0.75 vs. 10.66）指标上均优于TRAM和WHAM*，证明了联合重建管道的有效性。</li>
<li><strong>管道多功能性</strong>：如图4所示，该管道能够鲁棒地处理来自互联网的具有挑战性运动和多样环境的视频，能够同时重建和重定向多个人，并能进行以自我为中心的RGB-D渲染，展示了框架更广泛的适用性。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03729v5/x3.png" alt="多功能性展示"></p>
<blockquote>
<p><strong>图4</strong>：真实到模拟管道的多功能能力展示。包括对网络视频的鲁棒跟踪、多人同时重建与重定向，以及自我中心RGB-D渲染。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界部署</strong>：如图5所示，蒸馏后的单一策略成功部署在真实Unitree G1机器人上。该策略仅依赖本体感觉和实时的LiDAR高度图（提供躯干周围360度视图），以及操作员通过摇杆输入的期望根方向，即可在未见过的环境中执行上下楼梯、穿越崎岖地形、从椅子或长凳上坐下和站起等一系列上下文感知的全身技能。策略表现出良好的鲁棒性，例如在下楼梯意外打滑后能通过单腿短暂跳跃恢复。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03729v5/x4.png" alt="真实机器人技能展示"></p>
<blockquote>
<p><strong>图5</strong>：策略在真实机器人上执行多种技能：穿越复杂地形、站立和坐下。所有技能均由单一策略完成，该策略根据其高度图上下文和摇杆方向输入决定执行何种动作。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：如图6所示，动作捕捉预训练组件至关重要。移除MPT会显著阻碍策略学习有效行为的能力，因为视频重建的参考运动噪声较大，且初始机器人位姿可能不稳定或与场景有穿透，MPT有助于稳定学习初期阶段。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03729v5/x5.png" alt="MPT消融实验"></p>
<blockquote>
<p><strong>图6</strong>：动作捕捉预训练的影响。预训练有助于策略学习噪声较大的视频捕获参考运动。移除MPT会显著阻碍学习。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个完整的“真实-模拟-真实”管道VideoMimic，能够将日常人类视频转化为人形机器人的环境条件控制策略，为实现从视频中规模化学习机器人技能提供了一条可行路径。</li>
<li>开发了一种联合4D人-场景重建方法，从单目视频中恢复具有物理一致度量尺度的人和周围几何，并生成适用于物理模拟的策略训练数据。</li>
<li>通过多阶段训练和蒸馏，得到一个单一、统一的上下文感知控制策略，该策略仅依赖机器人本体感觉、局部高度图和根方向指令，就能在多样且未见过的环境中执行如爬楼梯、坐下等复杂全身技能，并成功部署到真实机器人。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>重建</strong>：单目4D人-场景重建在野外场景中仍显脆弱，存在相机位姿漂移产生“重影”、动态人物点云误融合为静态部分或位置不准确、点云滤波可能导致几何细节丢失、网格化可能过度平滑高频细节（如狭窄楼梯踏板）等问题。</li>
<li><strong>重定向</strong>：运动学优化器假设每个参考姿态在缩放到机器人尺寸后都是可行的，但在杂乱场景中并非总是如此，严格的脚部接触匹配与碰撞避免之间的冲突可能导致求解器陷入局部最小值。</li>
<li><strong>感知与策略输入</strong>：测试时控制器仅接收11x11的LiDAR高度图，缺乏精确接触、操作或推理悬空障碍物所需的分辨率，也缺乏对环境的语义理解。</li>
<li><strong>模拟保真度</strong>：假设场景可表示为单个刚性网格，难以扩展到铰接或可变形物体。</li>
<li><strong>数据规模与运动质量</strong>：策略仅在123个视频片段上训练，偶尔依赖恢复行为导致运动不够平滑。</li>
</ol>
<p><strong>对后续研究的启示</strong>：<br>VideoMimic展示了直接从丰富但非结构化的日常视频中学习机器人技能的潜力，弥合了计算机视觉与机器人学的界限。未来的工作可以沿着以下几个方向推进：开发更鲁棒的重建方法以更好地分离动态/静态部分并保留细节；集成RGB-D、语义分割等更丰富的感知输入以提升环境理解；扩展模拟器和重建管道以处理铰接或可变形物体；利用更大规模、更多样化的视频数据集进行训练以提升策略的平滑性和泛化能力；探索从真实机器人交互中进行在线微调。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VideoMimic框架，解决如何让仿人机器人通过观察日常视频学习上下文感知控制（如爬楼梯、坐椅子）的问题。其关键技术包括：从单目视频联合重建4D人-场景几何，将运动重定向至机器人形态，并训练单一强化学习策略，使其仅依赖本体感知、局部高度图和根目标指令来适应不同地形。实验表明，该策略能在真实机器人上实现鲁棒的、可重复的上下文技能，如上下楼梯、坐立等，且无需针对每个新环境或行为进行手工奖励设计或动作捕捉。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.03729" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>