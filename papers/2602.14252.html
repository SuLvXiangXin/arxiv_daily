<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GRAIL: Goal Recognition Alignment through Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GRAIL: Goal Recognition Alignment through Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.14252" target="_blank" rel="noreferrer">2602.14252</a></span>
        <span>作者: Reuth Mirsky Team</span>
        <span>日期: 2026-02-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目标识别旨在从智能体的可观察行为中推断其潜在目标。现有方法通常依赖于最优的、目标导向的策略表示，假设被观察的智能体以（近）最优方式追求目标。这一假设在现实场景中存在关键局限性：人类或实际智能体的行为常因认知限制、偏好、习惯或噪声而偏离最优性，表现为次优或系统性偏好（如导航时总选择经过公园的路线）。基于最优假设的目标识别方法会将此类有意义的行为偏差视为噪声，导致识别性能脆弱。</p>
<p>本文针对传统方法无法有效建模非最优行为的痛点，提出了一个新视角：将目标识别问题转化为一系列模仿学习问题。核心思路是，直接利用（可能是次优的）演示轨迹，为每个候选目标学习一个能捕捉其特定行为“指纹”的目标导向策略，从而在推理时通过单次前向传播将观察到的部分轨迹与这些学习到的策略进行匹配，实现与真实行为模型的对齐。</p>
<h2 id="方法详解">方法详解</h2>
<p>GRAIL框架的核心是通过模仿学习为每个候选目标学习行为策略，并在推理时进行快速、无需规划器的单次评分。其整体流程分为两个阶段：离线策略学习和在线单次推理。</p>
<p><img src="https://arxiv.org/html/2602.14252v1/x2.png" alt="方法框架对比"></p>
<blockquote>
<p><strong>图2</strong>：模型基于、RL基于的目标识别方法与GRAIL框架的对比。GRAIL（底部）首先为每个目标通过模仿学习离线训练一个策略，然后在推理时仅通过单次前向传播对观察轨迹进行评分。</p>
</blockquote>
<p><strong>1. 离线策略学习</strong><br>对于每个候选目标 (g \in G)，收集一组演示轨迹 (\mathcal{D}_g)，其中包含智能体追求目标 (g) 的行为（可以是次优或带有偏好的）。随后，采用一种模仿学习算法，利用 (\mathcal{D}_g) 训练一个目标导向的策略 (\pi_g(a|s))。论文探索了三种IL算法作为GRAIL的骨干：</p>
<ul>
<li>**BC (行为克隆)**：将模仿视为监督学习，通过最大化专家动作的似然来训练策略。</li>
<li>**GAIL (生成对抗模仿学习)**：通过对抗训练，使策略产生的状态-动作分布与专家演示的分布匹配。</li>
<li>**AIRL (对抗性逆强化学习)**：同时学习一个奖励函数 (R_g(s,a)) 和一个策略 (\pi_g)，使得诱导的行为能解释专家数据。</li>
</ul>
<p>此阶段结束后，系统获得一个策略“库” ({\pi_g}_{g \in G})，每个策略都吸收了对应目标演示数据中的行为特征，包括任何系统性偏差。</p>
<p><strong>2. 在线单次推理</strong><br>当接收到一个新的部分观察轨迹 (\tau_{obs} = (s_0, a_0, ..., s_T)) 时，GRAIL无需调用规划器或进行环境交互，而是为每个目标 (g) 计算一个相似性分数 (S_g(\tau_{obs}) = \ell(\tau_{obs}; \pi_g))，并选择得分最高的目标作为推断结果 (\hat{g} = \arg\max_{g \in G} S_g(\tau_{obs}))。</p>
<p>为实现轻量级且统一的评分，论文采用<strong>负平均均方误差</strong>作为评分函数 (\ell)：<br>[<br>\ell_{\mathrm{MSE}}(\tau_{obs}; \pi_g) = -\frac{1}{T}\sum_{t=0}^{T-1} |\pi_g(s_t) - a_t|^2<br>]<br>该度量直接比较观察到的动作与策略在当前状态下预测的动作之间的差异，适用于BC、GAIL和AIRL学习到的策略头。</p>
<p><strong>创新点</strong>：与需要在线规划或基于最优Q函数进行复杂匹配的基线方法（如GRAQL, DRACO）相比，GRAIL的创新在于：1) <strong>行为对齐</strong>：通过模仿学习直接建模可能次优或有偏好的真实行为分布；2) <strong>高效推理</strong>：推理时仅需一次前向传播计算MSE，无需环境交互或规划器调用，实现了低延迟。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在离散导航环境MiniGrid（9x9网格）和连续机器人控制环境PandaReach（机械臂到达任务）上评估。对比基线包括基于Q学习的GRAQL和基于PPO与Wasserstein距离的DRACO。评估了三种行为模式：<strong>系统性偏好最优行为</strong>（遵循偏好路径）、<strong>次优行为</strong>（在动作中引入噪声或冗余操作）和<strong>完全最优行为</strong>。性能指标主要为F1分数。</p>
<p><img src="https://arxiv.org/html/2602.14252v1/images/minigrid_gr_bias_optimal.png" alt="系统性偏好最优行为下的性能"></p>
<blockquote>
<p><strong>图4</strong>：在MiniGrid系统性偏好最优行为（2个目标）下的F1分数对比。所有GRAIL变体（BC/GAIL/AIRL）在20%-40%的可观测范围内均达到完美识别（F1=1.0），而GRAQL（蓝色）的F1仅在0.3-0.5之间，性能接近随机猜测。</p>
</blockquote>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>系统性偏好最优行为</strong>：如图4所示，在存在路径偏好的场景下，所有GRAIL变体均实现了近乎完美的目标识别（F1 ≈ 1.0），而GRAQL的F1分数仅为0.3-0.5。这表明GRAIL能有效捕捉并利用行为偏好进行区分，而基于最优假设的方法则失效。</li>
<li><strong>次优行为</strong>：在MiniGrid的次优行为测试中（表3），随着目标数增加和可观测性降低，GRAIL方法普遍优于GRAQL。例如，在6个目标、10%可观测性下，BC-GRAIL的F1为0.42±0.11，GAIL-GRAIL为0.47±0.08，均高于GRAQL的0.31±0.14。在PandaReach环境中（图5），面对动作噪声，BC-GRAIL和GAIL-GRAIL也展现出比DRACO基线更强的鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.14252v1/images/panda_gr.png" alt="PandaReach噪声下的性能"></p>
<blockquote>
<p><strong>图5</strong>：在PandaReach环境中，面对高斯和均匀动作噪声（强度0.1和0.3）时，各方法在4目标、2%可观测性下的F1分数。BC-GRAIL和GAIL-GRAIL在噪声条件下表现优于DRACO基线。</p>
</blockquote>
<ol start="3">
<li><strong>完全最优行为</strong>：在演示轨迹完全最优的情况下（表4），GRAIL方法（尤其是GAIL-GRAIL）与GRAQL基线表现相当或略优，表明其在理想条件下仍能保持竞争力。</li>
<li><strong>消融实验与效率</strong>：论文对评分函数进行了消融实验，验证了负平均MSE的有效性。此外，GRAIL在推理时仅需单次前向传播，相比需要复杂匹配计算的基线（如DRACO需要计算Wasserstein距离）具有显著的效率优势。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>框架创新</strong>：提出了GRAIL，第一个将目标识别系统地构建为一组模仿学习问题的框架，通过直接从演示中学习目标策略来对齐真实（可能非最优）的行为模型。</li>
<li><strong>性能提升</strong>：在次优和系统性偏好行为下，GRAIL显著提升了目标识别的准确性和鲁棒性，F1分数提升最高超过0.5，同时在完全最优设定下保持竞争力。</li>
<li><strong>高效推理</strong>：实现了无需规划器、仅单次前向传播的轻量级推理，为实时交互应用提供了可行性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，GRAIL的性能依赖于演示数据的质量和覆盖范围。在数据有限或质量差的情况下，模仿学习策略可能无法充分捕捉行为模式。此外，为每个目标训练独立策略在目标数量极大时可能带来计算和存储成本。</p>
<p><strong>启示</strong>：这项工作弥合了目标识别与模仿学习/逆强化学习领域之间的隔阂，表明直接学习行为策略比假设一个最优模型更能实现对智能体意图的鲁棒推断。后续研究可探索更高效的多目标策略学习架构、处理开放世界目标的能力，以及如何从更少、更嘈杂的演示中有效学习。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有目标识别方法因假设行为最优而无法准确识别次优或系统偏置行为目标的问题，提出GRAIL方法。该方法结合模仿学习与逆强化学习，直接从演示轨迹中为每个候选目标学习目标导向策略，并通过单次前向评分保留一次性推理能力。实验表明，GRAIL在系统偏置最优行为下F1分数提升超0.5，次优行为下提升约0.1–0.3，噪声最优轨迹下提升达0.4，且在完全最优环境中仍具竞争力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.14252" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>