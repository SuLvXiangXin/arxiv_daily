<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Constraint-Preserving Data Generation for Visuomotor Policy Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Constraint-Preserving Data Generation for Visuomotor Policy Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.03944" target="_blank" rel="noreferrer">2508.03944</a></span>
        <span>作者: Jeannette Bohg Team</span>
        <span>日期: 2025-08-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>大规模演示数据推动了机器人操作领域的关键突破，但收集此类数据仍然成本高昂且耗时。现有一种主流数据生成方法（如 MimicGen）通过收集少量遥操作演示，并利用位姿变换和动作重放算法性地扩增数据。这类方法的核心是利用机器人动作相对于物体位姿的 SE(3) 等变性，实现数据高效的空间泛化。然而，由于依赖 SE(3) 变换，这些方法无法生成适应物体几何形状变化（如不同的长宽比）的演示。例如，成功悬挂一个矮胖酒杯的动作，在应用于一个高瘦酒杯时可能完全失败，即使两者经过了相同的位姿变换。</p>
<p>本文针对现有数据生成方法无法处理物体几何形状变化的痛点，提出了一个新的视角：将机器人技能表达为关键点-轨迹约束。本文的核心思路是：通过将专家演示分解为技能段和自由运动段，并将每个技能段表述为机器人或被抓物体上的关键点必须跟踪一个相对于任务相关物体定义的参考轨迹，从而在数据生成过程中实现几何感知，生成包含新颖物体几何和位姿的演示数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>CP-Gen 的整体流程分为两个主要阶段：源数据处理和新数据生成。输入是单个专家演示轨迹，输出是用于训练视觉运动策略的生成演示数据集。</p>
<p><img src="https://arxiv.org/html/2508.03944v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>: CP-Gen 方法框架。顶部（源数据处理阶段）：从专家演示轨迹开始，将其分割为自由运动和技能段，标注机器人或被抓物体上的关键点（称为执行者关键点），并将每个技能段转换为一个表达在任务相关物体坐标系中的关键点-轨迹约束。底部（新数据生成阶段）：通过对每个任务相关物体施加几何和位姿变换来采样场景变化；对于每个采样场景，通过变换提取的目标关键点轨迹并求解满足更新后约束的机器人配置来适应源技能段，然后运动规划一条从当前关节配置到求解出的配置轨迹起点的无碰撞路径；迭代所有段以生成新场景的演示。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>轨迹分割与关键点标注</strong>：首先将专家演示轨迹分解为交替的自由运动段和机器人技能段。技能段是指无法被点对点无碰撞运动规划替代而不影响任务成功的部分。为每个技能段，在机器人夹爪或被抓物体上手动标注一组与技能完成相关的“执行者关键点”。</li>
<li><strong>关键点-轨迹约束提取</strong>：这是 CP-Gen 的核心创新。对于每个技能段，将执行者关键点从执行者局部坐标系（A）转换到任务相关物体的局部坐标系（O），从而提取出“目标关键点轨迹”。该轨迹定义了执行者关键点相对于物体坐标系应遵循的路径。</li>
<li><strong>技能段适应</strong>：当为新场景（具有新的物体几何和位姿）生成演示时，需要更新关键点-轨迹约束。<ul>
<li><strong>几何变换更新</strong>：若任务相关物体在其局部坐标系中经历了几何变换 <strong>X</strong>（如非均匀缩放），则将此变换应用于该物体的目标关键点轨迹。若执行者关键点锚定在一个被抓物体上，且该物体也发生了几何变换 <strong>X_A</strong>，则同样将此变换应用于执行者关键点。</li>
<li><strong>机器人配置求解</strong>：通过求解一个优化问题来获得每一时间步的机器人关节配置。优化目标是最小化变换后的执行者关键点（通过运动学计算到世界坐标系）与变换后的目标关键点（通过当前物体位姿变换到世界坐标系）之间的距离，并加入关节空间的时间平滑性惩罚项。使用 L-BFGS-B 优化器进行求解。</li>
</ul>
</li>
<li><strong>运动规划</strong>：在获得一个技能段的优化关节配置序列后，使用无碰撞运动规划器生成一条从机器人当前关节配置到下一个技能段起始配置的无碰撞轨迹。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：与 MimicGen 等仅依赖 SE(3) 相对位姿变换的方法不同，CP-Gen 的创新性在于其<strong>关键点-轨迹约束</strong>的表述。这种表述将技能的核心几何关系（关键点跟踪轨迹）从具体的物体实例和位姿中抽象出来。当物体几何发生变化时，通过对关键点轨迹施加相同的几何变换，可以自然地适配新的形状，从而实现了<strong>几何感知的数据生成</strong>。这使得生成的数据能覆盖更广泛的形状变化，而不仅仅是位姿变化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在模拟环境中使用 MimicGen 基准中的 16 个单臂 Franka Panda 机器人任务进行评估。</li>
<li><strong>实验变体</strong>：<ol>
<li><strong>仅位姿变化</strong>：默认的 MimicGen 任务设置，只改变物体位姿。</li>
<li><strong>几何泛化</strong>：本文提出的新变体，在保持位姿变化的同时，对物体施加非均匀缩放以引入几何形状变化（见图3）。</li>
</ol>
</li>
<li><strong>对比基线</strong>：MimicGen 和 DemoGen。DemoGen 在本文中被实现为 CP-Gen 的一个变体（即不使用关键点轨迹约束）。</li>
<li><strong>策略训练</strong>：使用 Diffusion Policy 在所有方法生成的 1000 条成功演示上训练策略。输入模态包括 RGB 图像或深度图加分割掩码。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.03944v1/figures/imgs/sim/tasks_D1.png" alt="模拟任务"></p>
<blockquote>
<p><strong>图6</strong>：MimicGen 基准中的模拟任务示例（仅位姿变化变体）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.03944v1/figures/imgs/sim/tasks_wide.png" alt="几何泛化变体"></p>
<blockquote>
<p><strong>图7</strong>：几何泛化任务变体中采样的物体几何形状示例，展示了同一物体的不同缩放比例。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟实验（表1）</strong>：<ul>
<li>在<strong>仅位姿变化</strong>任务上，CP-Gen 取得了与 DemoGen 相近的最优性能（平均成功率约85%），显著优于 MimicGen（63%）。论文分析优势源于使用了无碰撞运动规划器以及关键点约束能根据抓取方式调整动作。</li>
<li>在<strong>几何泛化</strong>任务上，CP-Gen 的优势极为明显。使用 RGB 输入时，CP-Gen 平均成功率为 **70%**，而 MimicGen 和 DemoGen 分别仅为 <strong>35%</strong> 和 **44%**，领先幅度达 33% 和 30%。这直接验证了 CP-Gen 方法在应对物体几何变化方面的有效性。</li>
</ul>
</li>
<li><strong>真实世界实验（表2，图4）</strong>：<ul>
<li>在四个具有多阶段、非抓取、紧公差操作挑战的真实任务上，使用在模拟中生成的数据训练的策略进行了零样本 sim-to-real 迁移测试。</li>
<li>CP-Gen 训练的策略平均成功率达到 **83%**，远高于 MimicGen 的 **40%**。特别是在“锤子清理”和“酒杯螺旋悬挂”等涉及几何适配和精确路径跟踪的任务上，CP-Gen 表现出了强大的泛化能力。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.03944v1/figures/imgs/real/mug_cleanup.png" alt="真实世界任务"></p>
<blockquote>
<p><strong>图8-11</strong>：四个真实世界评估任务：(a) 杯子清理, (b) 锤子清理, (c) 杯子悬挂, (d) 酒杯螺旋悬挂。这些任务测试了多阶段操作、非抓取交互和紧公差操作。</p>
</blockquote>
<p><strong>消融实验分析</strong>：论文虽然没有独立的消融实验章节，但通过将 DemoGen 实现为 CP-Gen 的无关键点约束版本，并在几何泛化任务上进行对比，实质上完成了对“关键点-轨迹约束”这一核心组件的消融研究。结果表明，移除该约束（DemoGen）后，在几何泛化任务上的性能大幅下降，证明了该组件对于处理几何变化至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>CP-Gen</strong>，一种基于<strong>关键点-轨迹约束</strong>表述的数据生成方法，能够从单条专家演示出发，生成适应新颖物体<strong>几何形状</strong>和<strong>位姿</strong>的机器人演示。</li>
<li>在模拟实验中验证了该方法的有效性，在标准位姿泛化任务上达到先进水平，并在新提出的几何泛化基准上大幅领先基线方法（70% vs. 37%）。</li>
<li>成功实现了从模拟到真实世界的<strong>零样本策略迁移</strong>，在复杂的多阶段真实机器人任务上取得了高成功率（平均83%），证明了生成数据的高质量和实用性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前方法中<strong>执行者关键点的选择是手动的</strong>。虽然未来可以通过视觉语言模型或任务关键点检测器来自动化，但这在现阶段是一个需要领域知识的人工步骤。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据生成范式的转变</strong>：CP-Gen 展示了将泛化能力从策略架构设计转移到数据生成过程的可行性。这为克服内置等变性瓶颈、处理更复杂（如多物体交互、非均匀几何变化）的场景提供了新思路。</li>
<li><strong>自动化关键点标注</strong>：结合新兴的视觉基础模型来自动推断与任务相关的关键点，是使该方法更易用、更通用的一个直接且重要的未来方向。</li>
<li><strong>复杂技能的组合</strong>：关键点-轨迹约束提供了一种模块化表示技能的方式，有可能用于组合和规划更复杂的长期任务。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习中大规模演示数据收集成本高昂、且现有基于位姿变换的数据生成方法无法适应物体几何变化的问题，提出CP-Gen方法。该方法将机器人技能定义为关键点轨迹约束，通过采样物体几何与位姿变换，并优化机器人关节配置以满足变换后的约束，从而从单条专家轨迹自动生成多样化的演示数据。在16个模拟任务和4个真实任务上的实验表明，使用该方法训练的策略平均成功率达到77%，显著优于最佳基线（50%）。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.03944" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>