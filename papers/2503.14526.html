<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.14526" target="_blank" rel="noreferrer">2503.14526</a></span>
        <span>作者: Fang, Yu, Yang, Yue, Zhu, Xinghao, Zheng, Kaiyuan, Bertasius, Gedas, Szafir, Daniel, Ding, Mingyu</span>
        <span>日期: 2025/03/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作（VLA）模型通过在真实机器人数据集（如Open X-Embodiment）上直接训练策略，展现出巨大潜力。然而，真实世界数据收集的高昂成本阻碍了数据的进一步扩展，从而限制了VLA模型的泛化能力。目前，扩展机器人学习主要有三种策略：收集真实世界数据、收集仿真数据，以及使用生成模型扩展真实数据集。真实数据收集资源密集，难以规模化；仿真数据虽然可扩展，但存在显著的模拟到真实（sim-to-real）差距；而基于生成模型的方法（如利用文本到图像修复技术）则常常产生AI伪影，导致物理不真实和时间不一致的机器人视频，引入了新的领域差距。本文针对VLA模型在目标领域“最后一公里”部署的挑战，提出了一种新颖的真实-模拟-真实（real-to-sim-to-real）方法ReBot，用于扩展真实机器人数据集并适应VLA模型。其核心思路是：在仿真环境中回放真实世界机器人轨迹以多样化操作对象（真实到模拟），并将模拟运动与修复后的真实世界背景相结合，合成物理真实且时间一致的机器人视频（模拟到真实）。</p>
<h2 id="方法详解">方法详解</h2>
<p>ReBot的目标是基于原始真实世界片段 τ_i = {o_t, a_t, L}，生成新的合成片段 τ‘_j = {o’_t, a_t, L‘}，从而构建用于适应目标领域的合成数据集 D’。其整体流程包含三个关键步骤，如图1和图2所示。</p>
<p><img src="https://arxiv.org/html/2503.14526v1/x1.png" alt="方法概述"></p>
<blockquote>
<p><strong>图1</strong>：ReBot方法概述。通过回放真实世界机器人轨迹到仿真环境来多样化操作对象（真实到模拟），并将模拟运动与修复的真实世界背景结合以生成逼真合成视频（模拟到真实）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.14526v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ReBot框架总览。包含三个核心组件：A) 真实到模拟轨迹回放；B) 真实世界背景修复；C) 模拟到真实视频合成。整个过程完全自动化。</p>
</blockquote>
<p><strong>A) 真实到模拟轨迹回放</strong>：此步骤旨在获得操作新物体时的模拟运动序列 {o_t^sim}。</p>
<ol>
<li><strong>场景解析与对齐</strong>：在仿真环境（Isaac Sim）中创建与真实场景空间对齐的数字孪生（机器人、相机、桌子）。通过GroundingDINO自动分割初始帧中的“桌子”，并结合深度图点云计算桌子高度，以完成场景对齐。</li>
<li><strong>轨迹回放</strong>：重用真实世界动作序列 {a_t} 来操作新的仿真物体。通过分析夹爪动作序列确定抓取开始时间 t_start 和放置结束时间 t_end。在 t_start 时刻的夹爪位置放置新的仿真物体，并可选地在 t_end 时刻的夹爪位置放置容器。然后回放整个动作序列，记录模拟运动。</li>
<li><strong>回放验证</strong>：通过监控从 t_start 到 t_end 期间物体与夹爪之间的笛卡尔距离，自动验证新物体是否被成功操作，并丢弃失败的片段。</li>
</ol>
<p><strong>B) 真实世界背景修复</strong>：此步骤旨在从原始真实视频 {o_t} 中移除任务特定元素（原始真实物体和机器人），获得任务无关的真实世界背景 {o_t^real}。</p>
<ol>
<li><strong>物体与机器人分割</strong>：使用GroundedSAM2（结合GroundingDINO和SAM2）进行分割与跟踪。首先，在 t_start 帧使用文本提示（“robot”）分割机器人。对于原始真实物体的分割，则利用轨迹回放步骤中估计的3D物体位置，将其投影到 t_start 帧作为2D点提示，通过SAM2进行分割。获得 t_start 帧的语义掩码后，使用SAM2将其传播到所有视频帧，得到序列掩码 {m_t}。</li>
<li><strong>物体与机器人移除</strong>：将视频帧序列 {o_t} 和掩码序列 {m_t} 输入最先进的视频修复模型ProPainter，移除原始真实物体和机器人，得到纯净的背景序列 {o_t^real}。移除真实机器人是为了在后续合成中正确呈现虚拟机器人带来的遮挡和物理交互。</li>
</ol>
<p><strong>C) 模拟到真实视频合成</strong>：最终将模拟运动 {o_t^sim} 与任务无关背景 {o_t^real} 结合，生成新的视频帧 {o‘_t}。具体做法是从 o_t^sim 中提取机器人及被操作物体，并将其融合到 o_t^real 上。同时，根据轨迹回放中使用的新物体和容器，更新原始语言指令 L 为 L’。最终构建出新的合成片段 τ‘_j = {o’_t, a_t, L‘}。由于忠实回放了真实轨迹，动作 a_t 保持不变。</p>
<p><strong>创新点</strong>：与现有生成式扩展方法（如ROSIE）相比，ReBot的创新在于：1) 提出首个用于扩展机器人数据集的真实-模拟-真实全流程方法；2) 结合了仿真的可扩展性与真实数据的保真度，通过真实动作和背景确保了物理真实性与时间一致性；3) 实现了从场景对齐、物体分割到视频合成的全自动化流程，无需人工干预。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用真实机器人数据集BridgeData V2和DROID中的桌面拾放片段。仿真对象来自Objaverse的厨房资产。仿真平台为Isaac Sim 4.1。主要对比方法是基于生成模型的ROSIE。评估模型为两个先进的VLA模型：Octo和OpenVLA。评估三个版本：原始预训练模型（零样本）、使用ROSIE数据微调的模型、使用ReBot数据微调的模型。</p>
<p><strong>视频质量评估</strong>：<br><img src="https://arxiv.org/html/2503.14526v1/x3.png" alt="视频质量对比"></p>
<blockquote>
<p><strong>图3</strong>：合成视频质量定性对比（DROID、BridgeData V2和自建数据集）。ReBot生成的视频具有物理合理的运动和优异的时间一致性，显著优于ROSIE。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.14526v1/x4.png" alt="视频质量定量评估"></p>
<blockquote>
<p><strong>图4</strong>：使用VBench对视频质量进行定量评估。ReBot在主体一致性、背景一致性、运动平滑度和成像质量四个维度上均显著优于ROSIE，且接近原始真实视频的水平。</p>
</blockquote>
<p><strong>仿真环境性能评估（SimplerEnv with WidowX）</strong>：</p>
<p><img src="https://arxiv.org/html/2503.14526v1/x5.png" alt="仿真环境域内性能"></p>
<blockquote>
<p><strong>图5</strong>：在仿真环境SimplerEnv中的域内任务成功率。ReBot将Octo的性能提升了7.2%，将OpenVLA的性能提升了21.8%，大幅超过ROSIE。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.14526v1/x6.png" alt="仿真环境域外泛化"></p>
<blockquote>
<p><strong>图6</strong>：在仿真环境SimplerEnv中的域外泛化任务成功率。ReBot将Octo的泛化性能提升了19.9%，将OpenVLA提升了9.4%。</p>
</blockquote>
<p><strong>真实世界性能评估（Franka机器人）</strong>：</p>
<p><img src="https://arxiv.org/html/2503.14526v1/x7.png" alt="真实世界性能"></p>
<blockquote>
<p><strong>图7</strong>：在真实世界Franka机器人上的任务成功率。ReBot将Octo的成功率提升了17%，将OpenVLA提升了20%，效果显著。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.14526v1/x8.png" alt="真实世界任务示例"></p>
<blockquote>
<p><strong>图8</strong>：真实世界任务执行的定性示例。展示了Octo+ReBot成功完成多个复杂操作任务。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<p><img src="https://arxiv.org/html/2503.14526v1/x9.png" alt="消融实验"></p>
<blockquote>
<p><strong>图9</strong>：消融实验（在仿真环境中）。评估了完整ReBot、仅使用模拟数据（无真实背景）、仅使用修复背景（无模拟运动）以及使用ROSIE的效果。完整ReBot方法取得了最佳性能，证明了模拟运动与真实背景结合的必要性。</p>
</blockquote>
<p><strong>关键结果总结</strong>：</p>
<ul>
<li><strong>视频质量</strong>：ReBot生成的视频在时间一致性、成像质量和多视角一致性上接近真实视频，显著优于ROSIE。</li>
<li><strong>仿真性能</strong>：在WidowX机器人的SimplerEnv中，ReBot将Octo和OpenVLA的域内性能分别提升7.2%和21.8%，域外泛化性能分别提升19.9%和9.4%。</li>
<li><strong>真实性能</strong>：在Franka机器人真实任务中，ReBot将Octo和OpenVLA的成功率分别提升17%和20%。</li>
<li><strong>消融实验</strong>：证明了模拟运动与真实背景相结合的核心设计是有效的，缺少任一部分都会导致性能下降。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ReBot，首个用于扩展真实机器人数据集并适应VLA模型到目标域的真实-模拟-真实全自动化方法。</li>
<li>巧妙结合了仿真的可扩展性与真实数据的保真度，通过回放真实轨迹和使用真实背景，确保了合成视频的物理真实性与时间一致性，最小化了sim-to-real差距。</li>
<li>在仿真和真实环境中进行了广泛实验，验证了ReBot能显著提升VLA模型的域内性能、域外泛化能力及在真实世界中的成功率，效果优于现有的生成式扩展方法。</li>
</ol>
<p><strong>局限性</strong>：论文提到，轨迹回放的成功与否取决于新物体与原始真实物体之间的功能兼容性。对于功能不兼容的物体，回放可能失败。</p>
<p><strong>后续启示</strong>：ReBot为解决机器人学习数据稀缺问题提供了一条新颖且有效的路径。其“真实-模拟-真实”的框架范式可以激励后续研究探索更高效的场景重建、更鲁棒的轨迹迁移方法，以及将该框架应用于更复杂的任务类型（如非刚性物体操作、动态环境交互等）。全自动化的数据生成管道也为构建大规模、高质量的领域特定机器人数据集指明了方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ReBot方法，旨在解决机器人学习中真实数据收集成本高、限制视觉-语言-动作（VLA）模型泛化能力的核心问题。其关键技术“真实-仿真-真实”视频合成，首先在仿真中重放真实轨迹以多样化操作对象，再将仿真运动与修复的真实背景融合，生成物理逼真、时序一致的合成视频。实验表明，ReBot显著提升了VLA模型的性能与鲁棒性：在仿真中，Octo和OpenVLA的域内性能分别提升7.2%和21.8%；在真实Franka机器人评估中，成功率分别提高17%和20%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.14526" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>