<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.05231" target="_blank" rel="noreferrer">2503.05231</a></span>
        <span>作者: Jiang, Shuo, Li, Haonan, Ren, Ruochen, Zhou, Yanmin, Wang, Zhipeng, He, Bin</span>
        <span>日期: 2025/03/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人学习领域的前沿方法，如模仿学习和基础模型，对大规模、高质量数据集提出了巨大需求，这构成了通用智能机器人发展的瓶颈之一。当前数据集普遍存在关键局限：第一，数据主要依赖视频，缺乏动态信息（如力/压力），导致学习性能受限；第二，缺乏一个通用、精细、直观的人类级感知框架，现有数据集仅整合了部分传感模态（如视频、IMU），不足以应对日益复杂的任务。本文针对真实世界同步多模态数据缺失，尤其是在包含动态信息及其细粒度标注的复杂装配场景中的缺失，提出了一个新的视角：构建一个集成人、环境和机器人数据的多模态收集框架。本文的核心思路是创建一个名为Kaiwu的大规模、高质量多模态数据集，旨在通过记录装配过程中丰富的人体运动、动力学、神经信号、注意力及环境信息，为机器人学习、灵巧操作、人类意图研究和人机协作提供支持。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的并非一个算法框架，而是一个<strong>多模态数据收集、同步与标注的完整框架和流程</strong>。其目标是系统性获取复杂装配场景下，与人类演示同步的多种传感模态数据。</p>
<ul>
<li><strong>整体框架与Pipeline</strong>：框架的核心是一个模块化的数据收集平台，能够同步流式传输、存储和可视化来自可穿戴传感器和环境安装传感器的信息。输入是装配任务中人类操作者的多源传感信号，输出是时间同步的、经过标注的多模态数据集。整体流程包括：传感器设置与校准 -&gt; 参与者装配任务执行与多源数据同步采集 -&gt; 数据后处理与多级细粒度标注。</li>
</ul>
<p><img src="https://arxiv.org/html/2503.05231v2/extracted/6498984/Figs/overview-0508.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：数据收集框架总览。展示了在装配环境中，通过可穿戴和环境安装的传感器记录丰富活动信息的整体设置，包括数据手套、EMG/ACC传感器、眼动仪、麦克风、动作捕捉系统和环境视觉/深度相机。</p>
</blockquote>
<ul>
<li><p><strong>核心模块与技术细节</strong>：框架由数据收集硬件平台和标注方案两大核心部分组成。</p>
<ol>
<li><p><strong>多模态传感硬件集成</strong>：针对人类操作的不同维度信息，集成了多种尖端传感器。</p>
<ul>
<li><strong>数据手套</strong>：配备19个手指角度传感器和19个手指压力传感器，以及手掌、前臂和上臂的惯性测量单元（四元数），用于捕捉手部运动轨迹和触觉交互信息。<br><img src="https://arxiv.org/html/2503.05231v2/extracted/6498984/Figs/glove1.png" alt="数据手套"><blockquote>
<p><strong>图2</strong>：配备角度传感器和力传感器的数据手套。</p>
</blockquote>
</li>
<li><strong>肌电图与加速度计</strong>：使用16通道EMG传感器（左右前臂各8个）采集肌肉电信号，集成的9自由度IMU提供同步的加速度数据，用于研究肌肉活动模式。</li>
<li><strong>眼动追踪仪</strong>：双目立体暗瞳追踪设备，记录操作者的注视点移动和第一人称视频，用于分析视觉注意力。</li>
<li><strong>环境视觉与深度</strong>：多视角RGB-D相机记录第三人称视角的环境信息。</li>
<li><strong>环境声音</strong>：多个麦克风记录操作环境中的声音（如工具放置、零件摩擦声）。</li>
<li><strong>地面真值系统</strong>：高精度光学动作捕捉系统，在参与者身体37个关键点放置反光标记，提供精确的3D运动轨迹作为基准真值。</li>
</ul>
</li>
<li><p><strong>同步与校准策略</strong>：平台采用模块化编程和多线程/多进程技术，确保各采集模块同时启动和同步。对于不同采样率的设备，通过记录<strong>绝对时间戳</strong>来实现数据流同步。在实验前，需要对每个参与者的设备（眼动仪、动作捕捉、EMG传感器位置）进行个性化校准，并在每次实验开始前进行手势动作校准以初始化数据同步。</p>
</li>
<li><p><strong>细粒度多级标注方案</strong>：对采集到的原始数据进行结构化标注，以提升数据集的可用性。<br><img src="https://arxiv.org/html/2503.05231v2/extracted/6498984/Figs/annotation0507.png" alt="标注概览"></p>
<blockquote>
<p><strong>图4</strong>：数据标注概览。展示了包括动作分割、手势分割、语义分割、手势分类和感兴趣区域在内的多级标注类型。</p>
</blockquote>
<ul>
<li><strong>动作与手势分割</strong>：基于时间戳序列，对装配任务进行两级分割。首先进行动作级粗分割，划分左右手的整体动作阶段；然后在每个动作间隔内，进一步细分为8种预定义的手势状态（如圆柱抓握、侧捏等）。这为EMG-ACC-手套数据序列提供了绝对时间戳标签。</li>
<li><strong>语义分割</strong>：对RGB视频数据中的30个关键物体（参与者及交互工具）进行像素级标注，丰富场景的语义信息。</li>
<li><strong>手势分类</strong>：根据模型回放，对手部状态进行简化分类（基于拇指和四指的屈伸状态）。</li>
<li><strong>感兴趣区域标注</strong>：在第一人称视角视频中，标注与关键物体操作相关的区域，包含参与者的注意力焦点和意图表达信息。</li>
</ul>
</li>
</ol>
</li>
<li><p><strong>创新点</strong>：与现有数据集相比，Kaiwu的主要创新体现在：1) <strong>模态的全面性与集成性</strong>：首次在复杂装配场景中，同步集成了手部运动与触觉、肌肉电信号、视觉注意力、环境多视角视频与深度、声音以及高精度动作捕捉真值，覆盖了人-机-环境交互的完整信息链。2) <strong>对动力学信息的强调</strong>：通过数据手套的压力传感器和EMG信号，明确包含了传统视频数据集所缺失的力与肌肉活动动态信息。3) <strong>精细化的多级时空标注</strong>：提供了从动作、手势到语义、注意力区域的层次化标注，极大增强了数据的可解释性和对跨模态学习、多模态融合研究的支持能力。</p>
</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>本文作为数据集论文，其“实验”部分主要集中于数据集的构建、统计描述以及与现有数据集的对比分析，而非传统意义上的模型性能评估。</p>
<ul>
<li><p><strong>数据集构建统计</strong>：</p>
<ul>
<li><strong>规模</strong>：招募了20名参与者，操作30个交互物体，产生了总计<strong>11,664个</strong>集成动作实例。</li>
<li><strong>标注量</strong>：提供了丰富的细粒度标注，包括：298个感兴趣区域标注、536,467个封闭区域元素图像分割标注（后文表IV更正为610,778个）、7,197个左右手灵巧操作的运动分割事件、4,467个手势事件标注和4,959个手势分类标注。</li>
<li><strong>数据多样性</strong>：装配过程被设计为15个装配环节，涵盖手动紧固和使用工具在受限空间操作等不同场景。<br>  <img src="https://arxiv.org/html/2503.05231v2/extracted/6498984/Figs/assemble0507.png" alt="装配流程"><blockquote>
<p><strong>图3</strong>：装配过程概览。展示了C8（手动紧固，空间充裕）和C14（工具操作，空间受限）两种不同侧重点的装配场景。</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>与现有数据集的对比</strong>：论文在表I中将Kaiwu与多个前沿数据集进行了详细对比。</p>
<ul>
<li><strong>对比的Baseline数据集</strong>：包括TSU（日常动作）、Harmonic（用餐）、HBOD（工具操作）、HUMBI（身体表达）、Open X-Embodiment（多场景）、ActionSense（厨房活动）等。</li>
<li><strong>关键对比结果</strong>：对比突显了Kaiwu的独特性。在<strong>环境/活动</strong>方面，Kaiwu专注于<strong>工业装配</strong>这一复杂、长视野、需要逻辑推理的场景。在<strong>模态</strong>方面，Kaiwu是唯一一个同时包含<strong>运动捕捉（地面真值）、手部姿态、手臂信息、眼动、EMG、触觉、RGB、深度和音频</strong>全部九种模态的数据集，其模态完整性显著优于其他数据集。例如，OXE主要依赖RGB和深度，缺乏动力学和生理信号；HBOD和Harmonic包含了部分可穿戴传感模态，但未集成高精度动作捕捉作为真值，且场景不同。</li>
</ul>
</li>
<li><p><strong>消融实验</strong>：本文没有进行模型性能的消融实验。但其数据集的构建本身体现了对“模态完整性”重要性的验证思路。通过集成其他数据集所缺失的动力学、神经信号等模态，并提供了精细标注，论文旨在为未来研究中进行模态贡献度分析（即消融研究）奠定基础。</p>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了一个面向通用操作能力的多模态数据收集框架</strong>：该框架具备全情境感知能力，集成了操作动态信息、人类操作神经信号与注意力信息以及多视角操作视觉信息，瞄准复杂的人类装配场景。</li>
<li><strong>构建了一个高质量、大规模、面向长视野自主性的多模态数据集（Kaiwu）</strong>：利用先进的真值技术，对复杂的操作过程进行了细粒度记录和标注，为机器人学习、人机交互等领域提供了宝贵的基准数据资源。</li>
<li><strong>提供了丰富的时空关系标注</strong>：大规模、多层次的跨模态同步数据标注显著增强了数据集的跨模态学习、多模态融合潜力及其可解释性。</li>
</ol>
<ul>
<li><p><strong>局限性</strong>：论文自身提及的局限性包括：数据手套的触觉传感器难以检测微小精密部件的数据变化，因此在数据收集前对小部件进行了预组装以满足采集要求。此外，作为初期工作，数据集的规模和任务场景仍有扩展空间。</p>
</li>
<li><p><strong>对后续研究的启示</strong>：Kaiwu数据集为多个研究方向打开了大门：1) <strong>机器人模仿学习</strong>：可利用包含力、EMG等多模态演示数据，训练能理解并复现人类操作“手感”和策略的机器人策略。2) <strong>人机协作与意图预测</strong>：通过眼动、EMG等信号，使机器人能够提前预测人类操作者的意图和下一步动作，实现更自然高效的协作。3) <strong>多模态融合基础模型</strong>：为训练能够同时处理视觉、动力学、生理信号等多种输入的机器人基础模型提供了高质量数据。4) <strong>人类神经运动机理研究</strong>：为研究人类在执行复杂灵巧任务时的神经肌肉协调机制提供了真实世界的数据支持。</p>
</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人学习缺乏大规模高质量多模态数据集的瓶颈问题，提出了Kaiwu数据集与采集框架。该框架集成了人类、环境与机器人数据，能同步采集手部运动、操作压力、声音、多视角视频、高精度动作捕捉、眼动追踪与肌电信号等七种模态信息，并提供基于绝对时间戳的细粒度多级标注。数据集包含20名受试者操作30个对象产生的11,664个动作实例，旨在为机器人灵巧操作、意图理解与人机协作研究提供支持。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.05231" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>