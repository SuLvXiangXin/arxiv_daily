<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FlowVLA: Thinking in Motion with a Visual Chain of Thought - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FlowVLA: Thinking in Motion with a Visual Chain of Thought</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.18269" target="_blank" rel="noreferrer">2508.18269</a></span>
        <span>作者: Haoang Li Team</span>
        <span>日期: 2025-08-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，许多视觉-语言-动作模型基于通过下一帧预测（v_t → v_{t+1}）训练的内部世界模型。然而，这种范式试图直接预测未来帧的外观，而没有对底层动力学进行显式推理。这种显式运动推理步骤的缺失，常常导致物理上不合理的视觉预测和低效的策略学习。现有方法存在“像素复制陷阱”问题，模型倾向于复制静态背景而非学习时空动态，导致模糊、不一致的长时程预测。此外，这造成了预训练（被动观察）与策略微调（主动行动）之间的领域鸿沟，导致知识迁移效率低下。</p>
<p>本文针对上述痛点，提出了“视觉思维链”这一新范式，旨在迫使模型在生成未来帧之前先对运动动态进行推理。本文的核心思路是：将下一帧预测分解为一个结构化的推理过程，即先预测中间的光流运动表示 f_t，再基于此运动计划生成未来帧 v_{t+1}，形成 v_t → f_t → v_{t+1} 的因果链，从而将学习目标从模式识别转变为结构化的物理推理任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>FlowVLA的整体框架遵循两阶段训练范式：第一阶段为基于视觉思维链的世界模型预训练；第二阶段为策略微调。</p>
<p><img src="https://arxiv.org/html/2508.18269v3/x1.png" alt="两阶段训练范式"></p>
<blockquote>
<p><strong>图1</strong>：FlowVLA的两阶段训练范式。（上）阶段1：基于视觉思维链的世界模型预训练。模型学习从初始帧预测中间运动表示（T时刻的光流），然后预测后续帧（T+1时刻）。这个迭代过程产生了物理上合理的长时程视频预测。（下）阶段2：策略微调。通过微调，预训练的世界模型被调整为从视觉观察生成精确的机器人动作块。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.18269v3/x2.png" alt="模型架构"></p>
<blockquote>
<p><strong>图2</strong>：FlowVLA的模型架构。（左）阶段1：基于视觉思维链的世界模型预训练。输入帧被编码为外观标记（粉色）。模型随后自回归地预测交织的运动标记（蓝色）和未来外观标记序列。提出的 v_t → f_t → v_{t+1} 预测迫使模型在预测未来之前先推理动态。为概念清晰，图像和光流标记器被分开可视化；实际上，它们是应用于外观和运动输入的完全相同的模块。（右）阶段2：策略微调。预训练的世界模型被调整用于动作预测。在文本指令（灰色）和当前观察（洋红色）的条件下，模型自回归地预测被解码为机器人动作块的动作标记（绿色）。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>统一的运动与外观标记化</strong>：这是FlowVLA设计的基石。模型使用<strong>完全相同的VQ-GAN标记器</strong>来处理RGB外观图像和光流运动图。对于光流（2通道的u, v位移场），采用VideoJAM技术将其转换为3通道的RGB图像：运动方向映射为色调（Hue），运动速度通过非线性归一化（使用缩放系数σ=0.15）映射为饱和度和明度（Saturation &amp; Value）。这种设计实现了参数效率、架构简单性，并促进了外观（“有什么”）与运动（“如何动”）在共享潜在空间中的统一表示。</li>
<li><strong>视觉思维链的自回归学习</strong>：给定可选的语言指令 L_instr，模型学习预测一个交织的序列：S_wm = {L_instr, v_0, f_0, v_1, f_1, …, v_T, f_T}。训练采用标准的下一个标记预测目标。世界模型损失 L_WM 是推理步骤（光流标记）和最终状态（下一帧标记）交叉熵损失的总和。具体地，对于每个时间步t，模型首先基于所有先前的标记预测光流 f_t，然后在结合了历史和刚预测的光流的条件下预测下一帧 v_{t+1}。损失函数为：L_WM = Σ_{t=0}^{T-1} [ L_CE(f_t | S_{&lt;v_{t+1}}) + λ * L_CE(v_{t+1} | S_{&lt;v_{t+1}}, f_t) ]，其中λ是平衡超参数（实验中设为1.0）。这个目标显式地强制模型在训练和推理中执行“推理→预测”过程。</li>
<li><strong>策略微调</strong>：策略模型使用预训练的世界模型权重进行初始化。输入序列变为交织的观察和动作：S_policy = {L_instr, v_0, a_0, v_1, a_1, …}。动作使用FAST方法进行离散化为标记。微调损失 L_policy 仅针对动作标记计算，引导模型利用其学到的所有视觉和动态知识来做出正确的动作预测。</li>
</ol>
<p><strong>创新点</strong>：<br>与现有直接进行 v_t → v_{t+1} 预测的方法相比，FlowVLA的核心创新在于引入了<strong>光流作为显式的、可解释的中间运动表示</strong>，形成了视觉思维链。这不仅通过分解任务提供了强大的归纳偏置，使模型学习物理 grounded 的运动表示，而且<strong>紧密对齐了预训练目标（预测动力学）与下游任务（生成动作）</strong>，因为理解运动 f_t 的模型天生更适合生成导致期望运动的动作 a_t。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/数据集</strong>：LIBERO基准（测试空间布局、物体、任务目标和长时程组合的泛化能力）、SimplerEnv-WidowX基准（测试光照、纹理、相机视角等域偏移的鲁棒性）、基于AgileX Cobot双臂机器人的真实世界实验平台（4个操作任务）。</li>
<li><strong>Baseline方法</strong>：包括无世界模型的方法（如Diffusion Policy, Octo, OpenVLA, DiT Policy等）和带世界模型的方法（如WorldVLA, UniVLA, CoT-VLA等）。</li>
<li><strong>实现细节</strong>：基于8.5B参数的Emu3/UniVLA架构。使用RAFT预计算光流。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>LIBERO基准</strong>：FlowVLA在四个测试套件上均达到最先进性能，平均成功率88.1%，显著优于之前的SOTA方法（如pi0-FAST的85.5%和UniVLA的84.0%）。在<strong>长时程任务</strong>上提升尤为显著（72.6% vs. 基线方法普遍在60-70%），证明了其更好的长期规划和推理能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.18269v3/x6.png" alt="LIBERO结果表"></p>
<blockquote>
<p><strong>表1</strong>：LIBERO基准结果。FlowVLA在所有套件上均领先，特别是在长时程任务上优势明显。</p>
</blockquote>
<ol start="2">
<li><strong>SimplerEnv基准</strong>：FlowVLA平均成功率达74.0%，大幅超越其他方法（如UniVLA的65.6%）。在“堆叠积木”等此前具有挑战性的任务上表现突出（62.5%），验证了其运动推理带来的对视觉和物理变化的更强鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.18269v3/x7.png" alt="SimplerEnv结果表"></p>
<blockquote>
<p><strong>表2</strong>：SimplerEnv-WidowX基准结果。FlowVLA在存在显著域偏移的任务上表现出卓越的鲁棒性。</p>
</blockquote>
<ol start="3">
<li><strong>真实机器人实验</strong>：在AgileX Cobot的四个任务上，FlowVLA平均成功率44.0%，优于所有基线（如UniVLA的31.0%）。在需要精确协调的双臂操作任务（如“将两个可乐罐放入盒子”、“用双臂提起锅”）上优势更明显。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.18269v3/x8.png" alt="真实机器人结果表"></p>
<blockquote>
<p><strong>表3</strong>：真实世界AgileX Cobot平台结果。FlowVLA在从单臂到双臂的各类操作任务上均取得最佳成功率。</p>
</blockquote>
<ol start="4">
<li><strong>世界建模能力定性分析</strong>：与直接下一帧预测基线相比，FlowVLA生成的预测在物理合理性和语义一致性方面显著更优。基线常出现机械臂突然消失、物体运动不一致等物理不合理现象，或预测动作与语言指令不符的语义不一致问题。而FlowVLA通过先推理光流，能生成物理合理且语义对齐的预测。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.18269v3/x4.png" alt="物理合理性对比"></p>
<blockquote>
<p><strong>图4</strong>：世界建模定性对比（物理合理性）。基线模型（中）产生了物理上不连贯的预测（如机械臂消失），而FlowVLA（右）的预测连贯合理。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.18269v3/x5.png" alt="语义一致性对比"></p>
<blockquote>
<p><strong>图5</strong>：世界建模定性对比（语义一致性）。基线模型（中）的预测虽然视觉连贯，但动作（推开）与指令“放入”不符。FlowVLA（右）的预测则与指令语义一致。</p>
</blockquote>
<ol start="5">
<li><strong>样本效率（收敛速度）</strong>：消融实验表明，采用视觉思维链预训练的FlowVLA在策略微调阶段收敛速度远快于没有光流预测（即标准下一帧预测）的变体，证明了其有效弥合了预训练与微调间的差距。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.18269v3/figures/convergence_speed_highlighted_5.png" alt="收敛速度对比"></p>
<blockquote>
<p><strong>图9/10</strong>：策略微调收敛曲线。FlowVLA（蓝色）比不使用光流预测的变体（红色）收敛速度快得多，验证了其卓越的样本效率。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>关键组件的贡献通过消融实验得到验证：1) <strong>引入光流预测（视觉思维链）</strong>是性能提升的核心，显著改善了世界模型质量和策略学习效率；2) <strong>统一的标记化方案</strong>确保了架构的简洁和高效；3) 与直接预测下一帧相比，<strong>v_t → f_t → v_{t+1}的因果链</strong>是获得物理合理预测和高效策略的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>视觉思维链</strong>这一世界模型学习新范式，通过显式建模运动（光流）来分解下一帧预测任务，解决了直接预测的物理不合理性和语义不一致问题。</li>
<li>提出了<strong>FlowVLA模型</strong>，通过统一的标记化方案，在单一自回归Transformer中实现了外观与运动推理的统一，无需特定任务架构组件。</li>
<li>在仿真和真实机器人基准上实现了<strong>最先进的策略性能</strong>，并证明了其具有<strong>卓越的样本效率</strong>，验证了视觉思维链能有效桥接世界模型预训练与策略微调。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，计算光流（如使用RAFT）需要额外的预处理步骤，带来了一定的计算开销。此外，光流表示本身可能存在局限性，例如在遮挡严重或非刚性变形极大的场景中可能不够准确。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>显式的中间表示（如光流）对于提升模型的可解释性和物理合理性至关重要，这为构建更可靠、更可信的VLA模型提供了新方向。</li>
<li>“先推理，后生成”的思维链范式可以扩展到其他模态或更抽象的动态表示（如力、接触点），以捕获更丰富的物理交互。</li>
<li>统一的标记化设计展示了多模态统一表征的潜力，鼓励研究者探索更简洁、高效的架构来融合不同信号。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文FlowVLA针对Vision-Language-Action（VLA）模型中直接预测未来帧外观而缺乏显式运动推理，导致物理不合理预测和策略学习低效的核心问题，提出了Visual Chain of Thought（视觉思维链）范式。关键技术FlowVLA采用自回归Transformer，以“当前帧→光流预测→未来帧”的两阶段训练方法，强制模型先通过中间光流编码运动动态再生成帧。实验在机器人操作基准和真实平台上进行，结果表明FlowVLA能生成更连贯、物理合理的视觉预测，并实现了最先进的策略性能，同时显著提高了样本效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.18269" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>