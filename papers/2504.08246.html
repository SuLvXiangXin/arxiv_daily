<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Spectral Normalization for Lipschitz-Constrained Policies on Learning Humanoid Locomotion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Spectral Normalization for Lipschitz-Constrained Policies on Learning Humanoid Locomotion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.08246" target="_blank" rel="noreferrer">2504.08246</a></span>
        <span>作者: Shin, Jaeyong, Cha, Woohyun, Kim, Donghyeon, Cha, Junhyeok, Park, Jaeheung</span>
        <span>日期: 2025/04/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>强化学习在仿真中训练双足机器人步态策略方面展现出巨大潜力，但仿真到现实的迁移仍面临挑战。关键问题在于仿真环境通常假设理想条件，如无限执行器带宽，允许策略输出高频、突变的扭矩指令。然而，真实机器人执行器带宽有限，无法瞬时响应此类指令，导致部署时产生振动和振荡行为。现有方法主要分为两类：一是在奖励函数中加入正则化项（如惩罚关节速度、加速度），但需要大量超参数调优；二是基于梯度惩罚的利普希茨约束策略，其通过惩罚策略梯度来约束输出变化率，但计算梯度带来了显著的GPU内存开销，限制了大规模并行训练的效率。本文针对梯度惩罚方法内存开销大的痛点，提出使用谱归一化作为实施利普希茨约束的更高效替代方案。核心思路是：通过约束策略网络权重矩阵的谱范数来直接限制网络的利普希茨常数，从而抑制动作输出的高频成分，同时避免昂贵的梯度计算，显著降低内存消耗。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法核心是将谱归一化应用于策略网络（Actor Network），以约束其利普希茨连续性，从而替代基于梯度惩罚的利普希茨约束策略。</p>
<p>整体框架基于标准的强化学习训练流程。输入为状态观测，策略网络输出动作均值，并假设动作服从以该均值为中心、固定方差的高斯分布。谱归一化被集成到策略网络的前向传播过程中，作为权重变换的一种形式。</p>
<p>核心模块是谱归一化操作。对于一个权重矩阵 (W)，谱归一化将其除以其最大奇异值 (\sigma(W))，得到归一化后的权重 (\hat{W} = W / \sigma(W))。这确保了该线性变换的利普希茨范数（即谱范数）不超过1。对于一个由 (L+1) 层线性变换和 1-Lipschitz 激活函数（如ReLU）构成的神经网络 (f)，其整体 Lipschitz 常数上界为各层权重谱范数的乘积：(|f|<em>{\text{Lip}} \leq \prod</em>{l=1}^{L+1} \sigma(W_l))。因此，对每一层的权重应用谱归一化（即强制 (\sigma(\hat{W}_l) \leq 1)），可以使整个网络成为 1-Lipschitz 的，意味着网络输出对输入变化的敏感度有界。</p>
<p><img src="https://arxiv.org/html/2504.08246v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：标准行动者网络与采用谱归一化的行动者网络对比。在SN网络中，每一层的权重矩阵 (W_l) 都除以其最大奇异值 (\sigma(W_l))，从而约束了网络的利普希茨常数。与权重可能任意增长的传统网络不同，SN确保了每一层的变换是有界的。</p>
</blockquote>
<p>与基于梯度惩罚的方法相比，创新点在于约束机制的本质不同。梯度惩罚在训练过程中通过额外的损失项显式地惩罚策略梯度的大小，需要计算二阶导数，内存开销大。而谱归一化是一种结构化的权重约束，在网络前向传播时直接生效，无需在优化目标中添加额外项，也无需计算关于输入的梯度，因此计算效率更高，GPU内存占用显著降低。</p>
<p>为了实现对利普希茨常数 (K) 的灵活控制，论文在策略网络的最后一层引入了一个超参数 (\lambda_{\text{SN}})（SN系数），通过缩放该层的权重来调整整体的 Lipschitz 边界。对于高斯策略 (\pi(a|s) = \mathcal{N}(\mu(s), \sigma^2))，理论推导表明，在应用谱归一化后，策略梯度 (\nabla_s \log \pi(a|s)) 的范数上界与 (2\lambda_{\text{SN}}/\sigma) 相关，从而实现了对策略平滑度的直接调控。</p>
<p><img src="https://arxiv.org/html/2504.08246v1/x3.png" alt="梯度范数估计"></p>
<blockquote>
<p><strong>图3</strong>：训练过程中估计的梯度范数平方。该图验证了应用谱归一化后，策略关于状态的梯度范数被有效地约束在一个范围内，满足了利普希茨连续性条件。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Isaac Gym仿真环境和真实的TOCABI人形机器人平台上进行。TOCABI机器人高1.8米，重100公斤，下半身有12个扭矩控制关节。</p>
<p>对比的基线方法包括：1) <strong>Baseline</strong>：标准的PPO算法，无额外平滑约束；2) <strong>GP-LCP</strong>：基于梯度惩罚的利普希茨约束策略；3) <strong>SN（本文方法）</strong>：采用谱归一化的利普希茨约束策略。</p>
<p>关键实验结果如下：</p>
<ol>
<li><strong>动作平滑性与性能</strong>：在仿真中，SN方法在最终训练回报上与GP-LCP方法相当，两者均显著优于振动剧烈的Baseline。SN成功抑制了动作中的高频成分，产生了平滑的扭矩指令。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.08246v1/x1.png" alt="动作对比"></p>
<blockquote>
<p><strong>图1</strong>：使用Baseline、GP-LCP和SN训练的策略对比。Baseline策略的输出存在振动；GP-LCP稳定了动作，但计算开销大；SN以更低的计算开销实现了相似的稳定性，证明了其在强制执行有限带宽动作控制方面的有效性。</p>
</blockquote>
<ol start="2">
<li><strong>内存与训练效率</strong>：这是本文的核心优势。实验表明，在4096个并行环境的大规模训练中，SN方法比GP-LCP节省了约40%的GPU内存。这使得在相同硬件条件下，使用SN可以运行更多并行环境，从而将训练时间缩短了约17%。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.08246v1/x4.png" alt="内存与训练时间对比"></p>
<blockquote>
<p><strong>图4</strong>：GPU内存使用和训练时间对比。左图显示SN方法的内存占用显著低于GP-LCP方法。右图显示SN方法的训练速度快于GP-LCP。</p>
</blockquote>
<ol start="3">
<li><strong>仿真到现实迁移</strong>：将仿真中训练的策略直接部署到TOCABI真实机器人上。Baseline策略产生了导致机器人剧烈振动的扭矩指令，无法稳定行走。而SN策略产生了平滑、物理上可行的扭矩，成功实现了稳定的原地踏步和向前行走。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.08246v1/x5.png" alt="真实机器人实验"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人TOCABI上的实验结果。SN策略能够生成平滑的关节扭矩，使机器人成功完成原地踏步和向前行走任务，而Baseline策略则因振动而失败。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过固定策略标准差 (\sigma) 和调整SN系数 (\lambda_{\text{SN}}) 进行了消融研究。结果表明，(\lambda_{\text{SN}}) 控制了策略的平滑度与敏捷性之间的权衡。较小的 (\lambda_{\text{SN}}) 导致更平滑但可能反应不足的动作；较大的 (\lambda_{\text{SN}}) 允许更快的响应，但可能引入更多高频成分。选择合适的 (\lambda_{\text{SN}}) 对于平衡任务性能与动作质量至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有两点：第一，<strong>提出将谱归一化作为实施利普希茨约束的高效替代方案</strong>，用于训练具有有限带宽输出的机器人策略，在保证动作平滑性的同时，避免了梯度惩罚方法巨大的GPU内存开销。第二，<strong>实证验证了该方法在仿真和现实人形机器人上的有效性</strong>，SN方法在性能上媲美GP-LCP，但训练效率更高，并成功实现了从仿真到真实硬件的迁移。</p>
<p>论文自身提到的局限性在于：谱归一化系数 (\lambda_{\text{SN}}) 和策略的标准差 (\sigma) 是需要调整的超参数，它们共同决定了策略的实际利普希茨边界。此外，该方法主要针对由执行器带宽限制引起的高频振动问题，对于其他类型的仿真与现实差异（如动力学参数误差、传感器噪声等）可能需要结合其他技术。</p>
<p>对后续研究的启示包括：谱归一化这种高效的结构化约束方法可以扩展到更复杂的策略网络架构（如循环网络）或其他需要平滑输出的连续控制领域；可以探索将谱归一化与动态的、可学习的策略标准差相结合，以进一步优化性能；此外，将这种低内存开销的平滑约束技术与更广泛的无模型强化学习算法结合，有望在更复杂的机器人任务中实现高效且可靠的仿真到现实迁移。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对强化学习训练人形机器人策略时，因模拟中执行器带宽无限等不现实假设导致策略依赖高频扭矩变化、难以转移到现实世界的问题，提出采用谱归一化（Spectral Normalization, SN）技术。该方法通过约束神经网络权重的谱范数来强制执行Lipschitz连续性，有效限制策略的高频波动，并大幅减少GPU内存开销。实验表明，SN在模拟和真实机器人上取得的性能与梯度惩罚方法相当，同时显著降低了内存使用，实现了更高效的并行训练。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.08246" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>