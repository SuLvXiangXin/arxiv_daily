<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FindingDory: A Benchmark to Evaluate Memory in Embodied Agents - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>FindingDory: A Benchmark to Evaluate Memory in Embodied Agents</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.15635" target="_blank" rel="noreferrer">2506.15635</a></span>
        <span>作者: Yadav, Karmesh, Ali, Yusuf, Gupta, Gunshi, Gal, Yarin, Kira, Zsolt</span>
        <span>日期: 2025/06/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）在规划和操控任务中展现出强大性能，激发了其在机器人学中的应用兴趣。然而，将其部署到具身环境中仍受限于融入长期经验（通常跨越数天并由大量图像集合表示）的挑战。现有VLMs通常只能一次性处理几百张图像，突显了需要更高效的机制来管理具身环境中的长期记忆。为了有意义地评估这些模型的长时程控制能力，一个基准测试必须针对记忆至关重要的场景。现有的长视频问答基准忽视了具身挑战，如物体操控和导航，这些需要低级技能和对过去交互的细粒度推理。此外，具身智能体中的有效记忆整合既涉及回忆相关的历史信息，又涉及基于该信息执行动作，因此有必要将这两方面结合起来研究。</p>
<p>本文针对上述痛点，提出了一个专门用于评估具身智能体长期记忆能力的新基准FindingDory。该基准的核心思路是：通过一个两阶段（经验收集与交互）的设置，在高度逼真的模拟环境中构造需要基于先前收集的经验进行推理的导航与操控任务，从而隔离并评估智能体的记忆能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>FindingDory基准的整体框架建立在Habitat模拟器之上，包含两个核心阶段：<strong>经验收集阶段</strong>和<strong>交互阶段</strong>。在经验收集阶段，一个拥有完全环境访问权限的特权代理执行随机采样的导航和操控任务（拾取和放置物体），改变环境状态并记录下交互历史（图像、位姿、动作）。在交互阶段，待评估的记忆代理被置于同一环境中，并接收到任务指令和先前的交互历史记录，它必须基于这些历史记忆来决定导航到哪里或检索哪个物体以完成任务。</p>
<p><img src="https://arxiv.org/html/2506.15635v2/figures/teaser.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：FindingDory基准示例。左侧为经验收集阶段，特权代理执行拾取和放置任务。右侧为交互阶段，记忆代理基于特权代理的日志完成依赖记忆的指令。</p>
</blockquote>
<p>基准的核心是一套通过模板生成的多样化任务，旨在探究空间、时间和语义推理。任务模板共60个，分为单目标和多目标类别，具体类别与示例如下：</p>
<ul>
<li><strong>空间对象回忆</strong>：导航到一个{类别}的物体。</li>
<li><strong>交互</strong>：导航到你昨天没有交互过的任何物体。</li>
<li><strong>条件交互</strong>：导航到你从中拾取过物体的{容器类别}。</li>
<li><strong>对象属性</strong>：导航回你昨天交互过的{目标颜色}颜色的物体。</li>
<li><strong>空间关系</strong>：导航到你交互过的、离你当前位置最远的物体。</li>
<li><strong>房间访问</strong>：导航到你昨天没有访问过的房间。</li>
<li><strong>交互顺序</strong>：导航到你在与{物体类别}交互后立即交互的物体。</li>
<li><strong>基于时间</strong>：导航到你在昨天{HH:MM}时间交互的物体。</li>
<li><strong>时长追踪</strong>：导航到花费最长时间重新摆放的物体。</li>
<li><strong>多目标无序重访</strong>：重访你昨天从中拾取过物体的所有容器。</li>
<li><strong>多目标有序重访</strong>：按特定顺序重访你昨天交互过的所有物体。</li>
</ul>
<p>为实现任务的程序化生成和自动验证，论文采用PDDL（规划域定义语言）规范系统。每个任务模板被编码为包含实体（物体、容器）及其属性（交互顺序、目标颜色等）的PDDL模板，并通过谓词和量词表达复杂的逻辑关系。系统通过采样符合约束的实体绑定来实例化任务，并自动验证任务完成情况，避免了人工编写低级规则。</p>
<p>基准的创新点主要体现在：1) <strong>隔离记忆评估</strong>：通过分离经验收集和交互阶段，避免了探索需求对记忆评估的混淆。2) <strong>程序化可扩展性</strong>：通过调整经验收集的复杂度（如交互次数），可以轻松扩展任务的时空推理难度和记忆依赖程度。3) <strong>综合任务设计</strong>：任务不仅要求简单的回忆，还要求基于记忆进行空间关系判断、多跳时序推理和战略决策（如选择最近的有效目标）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Habitat模拟器中进行，使用了Habitat合成场景数据集（HSSD）中的107个训练场景和30个验证场景。评估了多种基线方法，这些方法均采用分层架构：高层模块（基于VLM）处理交互历史和任务指令，输出目标帧索引；低层模块（导航策略）则执行动作以到达目标帧位置。</p>
<p><img src="https://arxiv.org/html/2506.15635v2/x1.png" alt="任务性能对比"></p>
<blockquote>
<p><strong>图2</strong>：不同VLM在FindingDory各任务类别上的高层成功率。专有VLM（Gemini-2.0-flash, GPT-4o）成功率很低，经过监督微调（SFT）的Qwen模型表现最佳，但平均成功率也仅约50%。多目标任务上所有模型表现接近零。</p>
</blockquote>
<p>评估的基线包括：</p>
<ol>
<li><strong>视频VLM代理</strong>：直接处理带时间戳的完整交互视频，预测目标帧索引。评估了专有模型（Gemini-2.0-Flash, GPT-4o）和开源模型（Qwen2.5-VL, Gemma-3, GLM-4.1V-Thinking）。</li>
<li><strong>文本记忆代理</strong>：先将视频分块并由VLM生成文本摘要，再由LLM基于摘要推理目标帧。</li>
<li><strong>监督微调</strong>：在训练数据上微调VLM，使其预测可接受的目标帧列表。</li>
</ol>
<p>关键实验结果如下：</p>
<ul>
<li><strong>最先进的VLMs在FindingDory上表现挣扎</strong>：零样本情况下，GPT-4o取得了最高的27.3%高层成功率（HL-SR），Gemini-2.0-Flash为25.7%。开源模型如Qwen2.5-VL和Gemma-3在13%-16%之间。文本记忆代理表现最差。</li>
<li><strong>监督微调带来显著提升</strong>：经过SFT的Qwen模型平均比冻结模型提升了25%，在时空和多目标任务上改进尤为明显，但整体成功率仍仅约50%，远低于理论可达的99%（Oracle上限）。</li>
<li><strong>多目标任务极具挑战</strong>：所有VLM基线在大多数多目标任务上成功率接近零，表明模型难以跟踪和回忆多个物体-容器的交互序列。</li>
<li><strong>VLMs能识别目标容器但无法精确定位</strong>：对于以容器为目标的`任务，在放宽了空间距离要求的语义覆盖率成功率（SC-SR）指标下，模型表现有大幅提升（图3a），但在严格要求距离的距离到目标成功率（DTG-SR）下提升很小。对于小物体目标的任务则无此现象（图3b）。这表明VLMs能语义上检测到大目标，但缺乏构建精确空间表征的能力。</li>
<li><strong>VLMs难以选择最近的有效实体</strong>：在存在多个有效目标的任务中，高层成功率（HL-SR）与高层路径长度加权成功率（HL-SPL，衡量选择最近目标的效率）之间存在巨大差距（可达50%），说明模型无法进行细粒度的空间分析来选择最近的实体（图3c）。</li>
<li><strong>分层策略导致性能下降</strong>：当将表现最好的高层Qwen模型与低层导航策略（ImageNav或基于地图的策略）结合时，最终的底层成功率（LL-SR）和SPL（LL-SPL）显著下降，体现了高层决策错误与低层导航误差的累积效应（图3d）。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.15635v2/x2.png" alt="结果分析图"></p>
<blockquote>
<p><strong>图3</strong>：(a,b) 针对物体和容器任务的放宽指标分析。(c) 高层成功率与高层SPL对比，显示模型常选择次优目标。(d) 分层策略的完整性能，显示高低层组合后性能下降。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>FindingDory</strong>，一个用于评估具身决策中长期记忆的新基准，包含60个多样化的导航任务，具有高逼真度、程序化可扩展性，并隔离了记忆评估。2) 对结合了高层VLM策略和低层导航策略的多种基线方法进行了全面评估，系统揭示了当前最先进VLMs在记忆密集型任务上的<strong>严重局限性</strong>，尤其是在多目标推理、细粒度空间定位和时序推理方面。3) 提供了一个系统化且可扩展的评估框架与指标，为未来记忆高效具身智能体的发展奠定了基础。</p>
<p>论文自身提到的局限性包括：基准目前基于模拟器，虽然逼真，但与真实世界仍有差距；任务范围主要集中于室内导航与简单操控。</p>
<p>这项研究对后续研究的启示是深远的。它明确指出，尽管VLMs在短上下文任务中表现出色，但将其应用于需要长期记忆的具身AI仍面临巨大挑战。未来的工作亟需开发能够<strong>有效压缩、检索和推理长序列多模态经验</strong>的VLM架构或记忆机制。同时，如何让模型进行<strong>精确的空间推理</strong>和<strong>复杂的多跳时序推理</strong>，也是提升具身记忆能力的关键方向。FindingDory基准为此类研究提供了一个亟需的、严谨的测试平台。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出FindingDory基准，用于评估具身智能体在长期任务中的记忆能力。核心问题是现有视觉语言模型（VLMs）难以处理跨越多天的大规模图像历史，缺乏在需要导航与物体操控的具身环境中进行长期记忆与细粒度推理的评估标准。该基准在Habitat模拟器中构建了60项任务，要求智能体依据先前交互日志完成导航、拾取与放置指令，并可扩展为更长、更复杂的版本。论文进一步建立了结合先进闭源与微调开源VLMs的基线方法，通过实验评估其在记忆密集型任务上的性能，并指出了关键改进方向。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.15635" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>