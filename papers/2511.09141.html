<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.09141" target="_blank" rel="noreferrer">2511.09141</a></span>
        <span>作者: Li, Xuetao, Huang, Wenke, Pan, Nengyuan, Zhao, Kaiyan, Yang, Songhua, Wang, Yiming, Li, Mengde, Ye, Mang, Xuan, Jifeng, Li, Miao</span>
        <span>日期: 2025/11/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前仿人机器人操作领域的主流方法是数据驱动方法，例如基于扩散模型或Transformer的策略，它们需要大量的训练数据（如上万条轨迹）来实现鲁棒的多模态决策和可泛化的视觉运动控制。这些方法存在两个关键局限性：一是忽视了在未见场景中的几何推理能力，导致技能选择（例如抓握与捏取）在遇到不同形状目标时存在歧义；二是对训练数据中机器人-目标关系的建模效率低下，造成了训练资源的巨大浪费。</p>
<p>本文针对上述两个具体痛点，提出了统一几何语义技能推理与数据高效视觉运动控制的新视角。核心思路是：1）通过注入几何归纳偏置的几何先验技能选择器（GSS），使机器人能够基于目标物体的几何形状和空间位置进行类人的技能推理；2）通过自适应递归高斯网络（ARGN），以递归方式编码多尺度空间关系并利用高斯混合模型（GMM）紧凑地参数化机器人-物体交互，从而实现仅需少量演示数据的高效运动合成。</p>
<h2 id="方法详解">方法详解</h2>
<p>RGMP是一个端到端的框架，其整体流程如算法1所示。输入是人类语音指令和当前视觉观察，输出是机器人的动作。框架包含两个核心组件：负责技能选择的Geometric-prior Skill Selector (GSS) 和负责动作生成的Adaptive Recursive Gaussian Network (ARGN)。首先，GSS解析指令，定位目标，并基于几何先验从预定义的技能库中选择一个合适的参数化技能（每个技能对应一个预训练的RGMP模型）。随后，被选中的RGMP模型接收视觉观察，通过ARGN生成精确的关节动作以执行任务。</p>
<p><img src="https://arxiv.org/html/2511.09141v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RGMP框架概览。通过结合来自人类指令的语义线索和来自视觉感知的常识信息，RGMP为任务构建机器人-目标的空间关系。相较于Diffusion Policy，RGMP实现了8%的性能提升和5倍的数据效率。</p>
</blockquote>
<p><strong>核心模块一：几何先验技能选择器 (GSS)</strong><br>GSS旨在解决在未见场景中基于几何进行细粒度技能选择的挑战。它分为两个阶段：第一阶段，使用视觉语言模型（VLM，如Qwen-VL）解析人类指令，在观察图像中识别并定位（用边界框标注）目标物体。第二阶段，基于得到的边界框，系统分析目标物体的常识信息（相对位置和形状信息，后者通过Yolov8n-seg模型获取），并依据这些几何先验从技能库中选择预训练的技能模型。其规划函数为 $\mathcal{P}=plan(\mathcal{I}, \mathcal{O} | \mathcal{C})$，其中 $\mathcal{C}$ 是包含示例的预定义上下文，支持上下文学习。GSS是即插即用、模块化且极简的，仅需约20条基于规则的约束。</p>
<p><strong>核心模块二：自适应递归高斯网络 (ARGN)</strong><br>ARGN旨在从有限演示数据中学习可泛化的视觉运动控制，其核心是自适应地建模机器人与目标物体在空间上的依赖关系。网络结构包含多个阶段，每个阶段由空间混合块和通道混合块组成。</p>
<p><img src="https://arxiv.org/html/2511.09141v2/x3.png" alt="网络结构细节"></p>
<blockquote>
<p><strong>图3</strong>：(a) 空间混合块和 (b) 通道混合块的结构。空间混合块集成了用于动态衰减的自适应衰减机制（ADM）和用于方向感知的RoPE，以增强空间聚合。通道混合块通过整合通道间的相关性来重新分配通道特征响应。</p>
</blockquote>
<p><strong>创新技术细节</strong>：</p>
<ol>
<li><strong>递归计算与空间记忆</strong>：ARGN将特征图切片为 $16\times16$ 的图像块，并在空间混合块中采用递归计算，从第一个视觉块到最后一个逐步建模全局空间关系（公式3）。这形成了机器人对观察图像的空间记忆，使其能识别与任务执行最相关的末端执行器位置。</li>
<li>**自适应衰减机制 (ADM)**：为解决递归计算中的梯度消失问题，ADM通过公式2动态生成内容自适应的衰减因子 $\mathcal{W}$，控制历史记忆的衰减率，防止关键空间记忆丢失，并自适应地放大任务关键图像块的权重。</li>
<li>**旋转位置编码 (RoPE)**：在空间混合块中应用RoPE，通过旋转变换编码位置信息，增强对相对空间偏移的敏感性，且无需可学习的位置参数。</li>
<li><strong>高斯混合模型 (GMM) 精炼</strong>：初始预测动作 $a_{in}$ 通过线性层从融合特征 $F_f$ 生成（公式7）。为避免单高斯回归到均值导致精度下降，使用一个 $K=6$ 分量（对应6自由度机械臂）的GMM对动作分布进行建模（公式9）。通过计算 $a_{in}$ 到各GMM分量的马氏距离（公式10），最终输出动作 $a^*$ 选择距离最近的聚类中心（公式11），从而更精确地表示多模态动作分布。</li>
</ol>
<p>与现有方法相比，RGMP的创新点在于：1) 首次在VLM中显式地通过几何-物体分解机制桥接几何推理与语义任务规划；2) 提出了结合递归空间记忆、自适应衰减和高斯混合精炼的数据高效视觉运动网络结构，避免了扩散模型耗时的迭代去噪过程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在两个物理机器人平台进行：一个仿人机器人（测试上肢）和一个桌面双臂机器人（测试跨具身泛化能力）。数据集包含为技能库收集的120条轨迹，每条轨迹关联一个执行动作前的RGB图像。评估指标包括技能选择成功率 $Acc_s$、任务执行准确率 $Acc_t$ 和综合成功率 $Acc = Acc_s \times Acc_t$。</p>
<p><strong>对比方法</strong>：与多种基线方法对比，包括ResNet50、Transformer、ManiSkill2挑战赛第一名方法、Octo、OpenVLA、RDT-1b、Dex-VLA以及Diffusion Policy。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>泛化能力与数据效率</strong>：RGMP仅在40个“抓握芬达罐”的演示样本上训练，在抓取未见物体（可乐罐、喷雾瓶、人手）时达到了87%的平均成功率，显著优于所有基线（表2）。论文指出RGMP的数据效率是当前最先进模型（Diffusion Policy）的5倍。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.09141v2/x5.png" alt="泛化能力展示"></p>
<blockquote>
<p><strong>图5</strong>：RGMP的泛化能力。尽管仅用40个芬达罐抓握演示训练，RGMP能可靠地从任意位置抓取罐子，并将此能力泛化到未见物体（可乐瓶、喷雾罐、人手）。</p>
</blockquote>
<ol start="2">
<li><strong>GSS的有效性</strong>：如表1所示，在不同骨干网络下，GSS相较于原始的Qwen-VL在技能选择准确率 $Acc_s$ 上带来了15-25%的提升，验证了几何先验注入的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.09141v2/x4.png" alt="交互流程与对比"></p>
<blockquote>
<p><strong>图4</strong>：人机交互流程。在“递给我纸巾”任务中，训练集仅包含40个捏取纸巾的实例。我们的RGMP比DP（Diffusion Policy）表现更好。</p>
</blockquote>
<ol start="3">
<li><strong>模拟器任务性能</strong>：在ManiSkill2模拟器的五个非抓取操作任务上（图6），RGMP也取得了最高的性能，表明其基础技能（抓、举、捏）可以动态组合完成复杂任务。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.09141v2/x6.png" alt="模拟器性能"></p>
<blockquote>
<p><strong>图6</strong>：在ManiSkill2模拟器上的性能。我们评估了RGMP和SOTA模型在ManiSkill2的五项操作任务上的有效性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>GMM的作用</strong>：表3的消融实验表明，在Diffusion Policy上添加GMM能将 $Acc$ 从0.49提升至0.55；而ARGN结合GSS和GMM在抓取压扁的可乐罐任务上达到0.69的 $Acc$，证明了GMM对动作预测的精炼效果。</li>
<li><strong>ARGN组件贡献</strong>：表4验证了RoPE、空间混合块(SMB)和通道混合块(CMB)的贡献。三者结合使用时，在所有测试物体上取得了最高的准确率（芬达0.98，可乐0.78，喷雾0.81，人手0.90）。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了<strong>几何先验技能选择器 (GSS)<strong>，通过向VLM注入极简的几何归纳偏置（如形状/功能启发），实现了对未见场景的、类人的技能推理，无需任务特定的微调。2) 提出了</strong>自适应递归高斯网络 (ARGN)<strong>，通过递归空间记忆、自适应衰减机制和高斯混合模型，实现了从稀疏演示中数据高效地合成灵巧运动。3) 在</strong>真实机器人平台</strong>上进行了全面评估，RGMP框架展现了优异的跨领域泛化能力和数据效率。</p>
<p>论文自身提及的局限性在提供的正文中未明确阐述。本研究对后续工作的启示在于：为机器人操作提供了一条结合显式几何常识推理与高效隐式运动建模的新路径，表明在视觉语言模型中嵌入模块化、可解释的物理先验，并设计专为小数据优化的运动生成网络，是迈向更通用、更数据高效机器人系统的有效方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人操作中数据驱动方法泛化能力差、训练效率低的问题，提出RGMP框架。它通过**几何先验技能选择器**注入几何归纳偏差来推理技能序列，并利用**自适应递归高斯网络**编码多尺度空间关系以实现高效运动合成。实验表明，该框架在泛化测试中任务成功率达**87%**，数据效率比现有最佳模型高**5倍**。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.09141" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>