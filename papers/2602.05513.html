<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.05513" target="_blank" rel="noreferrer">2602.05513</a></span>
        <span>作者: Li, Xukun, Sun, Yu, Zhang, Lei, Huang, Bosheng, Peng, Yibo, Meng, Yuan, Jiang, Haojun, Xie, Shaoxuan, Yao, Guacai, Knoll, Alois, Bing, Zhenshan, Wang, Xinlong, Sun, Zhenguo</span>
        <span>日期: 2026/02/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人灵巧操作领域的主流方法主要依赖于视觉-语言-动作模型和基于扩散模型的策略。这些方法在处理多模态信息（如视觉、本体感觉）时，通常采用耦合的融合方式，即将不同模态的信号以同等重要性进行混合。然而，这种耦合融合存在关键局限性：它未能充分考虑不同模态在动作生成中的差异性贡献。特别是在主动相机导致视觉快速变化，而触觉信号在操作过程中相对稀疏的场景下，平等的融合方式可能无法充分利用视觉的主导作用和触觉在接触感知中的独特价值。</p>
<p>本文针对如何有效且可控地整合多模态信息，尤其是如何将触觉信号高效注入预训练策略这一具体痛点，提出了解耦多模态注入的新视角。核心思路是设计一个解耦的多模态扩散Transformer，通过专门的条件化路径分别处理视觉、本体感觉和触觉信号，并引入一个轻量级的插件式触觉适配器，实现参数高效的触觉信息融合。</p>
<h2 id="方法详解">方法详解</h2>
<p>DECO采用两阶段训练范式。第一阶段，训练一个基于视觉和本体感觉的策略（无触觉）。第二阶段，冻结预训练策略，仅通过一个轻量级触觉适配器引入触觉信号进行微调，从而实现参数高效的触觉感知能力增强。</p>
<p><img src="https://arxiv.org/html/2602.05513v1/figures/sec1_cover.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DECO方法整体框架。DECO是一个基于扩散Transformer的策略，它解耦了多模态条件注入。图像和动作令牌通过联合自注意力交互，本体感觉状态和可选条件通过自适应层归一化注入。触觉信号通过交叉注意力注入，同时使用一个基于LoRA的轻量级适配器来高效微调预训练策略。</p>
</blockquote>
<p>整体框架的核心是多模态扩散Transformer块，其设计原则是将模态特定的条件化与注意力结构解耦，以实现异构感官输入的灵活集成。</p>
<p><img src="https://arxiv.org/html/2602.05513v1/figures/sec3_attn_block.png" alt="多模态扩散Transformer块"></p>
<blockquote>
<p><strong>图3</strong>：具有解耦条件化的多模态扩散Transformer块。图像通过自注意力、本体感觉状态通过AdaLN、触觉信号通过交叉注意力进行独立且高效的集成。</p>
</blockquote>
<p>具体而言，视觉（双目图像）通过ResNet-34编码后，与动作令牌在联合自注意力层中交互，这使得视觉信息能直接引导动作生成。本体感觉状态（关节角度）、扩散时间步和任务条件则通过自适应层归一化注入到视觉和动作令牌的路径中，实现对网络激活的调制。触觉信号则通过一个专用的交叉注意力模块注入，该模块允许触觉嵌入作为键和值，与来自主Transformer块的动作/视觉查询进行交互。这种设计使得触觉能够以插件方式集成，而不改变预训练视觉策略的自注意力结构。</p>
<p>与现有方法相比，DECO的核心创新点在于这种解耦的、结构化的多模态融合机制。它假设不同模态贡献不均，并据此分配了不同的融合接口：视觉（主导）使用自注意力，本体感觉使用AdaLN进行全局调制，触觉使用交叉注意力进行选择性关注。</p>
<p><img src="https://arxiv.org/html/2602.05513v1/figures/sec3_tac_adapter.png" alt="触觉适配器"></p>
<blockquote>
<p><strong>图4</strong>：插件式触觉适配器。原始触觉信息由触觉编码器编码，并通过基于LoRA的交叉注意力模块集成到预训练策略中，实现高效适配。</p>
</blockquote>
<p>为了将触觉高效注入预训练的视觉策略，本文提出了插件式触觉适配器。它包含一个触觉编码器，通过平均和线性投影两种方式提取区域级触觉特征，并经过门控机制融合。这些触觉嵌入通过交叉注意力模块注入冻结的DECO主干网络。关键的是，交叉注意力模块中的投影层使用低秩适应技术进行微调，这使得在第二阶段仅需优化不到10%的模型参数，即可使策略获得触觉感知能力，实现了极高的参数效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在作者新发布的DECO-50数据集上进行，该数据集包含4个场景（拾取放置、物料分拣、垃圾处理、装配）共28个子任务，涵盖超过50小时数据和500万帧，包含双目主动视觉、触觉信号和机器人本体感觉。</p>
<p><img src="https://arxiv.org/html/2602.05513v1/figures/sec4_task_illustration.png" alt="任务图示"></p>
<blockquote>
<p><strong>图5</strong>：DECO-50数据集包含的四个任务场景图示，每个场景由多个子任务组成。</p>
</blockquote>
<p>基线方法包括视觉动作变换器ACT和扩散策略DP。对比变体包括：DP.t（将触觉信号简单拼接至输入并从零开始训练）和DECO.p（在预训练的视觉版DECO上集成触觉适配器进行微调）。</p>
<p>关键实验结果总结如下：在全部任务的平均成功率上，DECO达到72.25%，相比基线（ACT 57.25%， DP 51.25%）有显著提升（最高提升21%）。在接触丰富的任务（垃圾处理、装配）上，DECO.p（带触觉适配器）的平均成功率高达73.13%，相比视觉版DECO的53.13%提升了20个百分点。</p>
<p><img src="https://arxiv.org/html/2602.05513v1/figures/sec4_contact_rich_task_illustrate.png" alt="接触丰富任务定性对比"></p>
<blockquote>
<p><strong>图12</strong>：DECO在有/无触觉情况下，在垃圾处理和装配任务上的定性对比。触觉帮助策略更可靠地感知接触状态（如垃圾桶盖是否关紧、装配是否到位），避免仅依赖视觉可能产生的误判。</p>
</blockquote>
<p>消融实验（表5）深入分析了触觉注入方式的影响。DECO.cs（耦合式注入，即将触觉嵌入与本体感觉一起通过AdaLN注入并从零训练）性能与无触觉的DECO相近，表明简单拼接触觉信息效果有限。DECO.ds（使用与DECO.p相同的解耦交叉注意力注入，但从零开始训练）和DECO.p（插件适配器）性能显著优于DECO，证明了所提出的触觉预处理和交叉注意力机制的有效性。更重要的是，DECO.p的性能与DECO.ds相当，但可训练参数仅需7.97M（表6），而DECO.ds需89.41M，这凸显了插件适配器在参数效率上的巨大优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了DECO，一种解耦多模态注入的扩散Transformer范式，通过不同的机制（自注意力、AdaLN、交叉注意力）结构化地融合视觉、本体感觉和触觉，提升了策略性能；2）设计了一种插件式触觉适配器，能够以极低的参数成本（&lt;10%）将触觉感知能力高效注入预训练的视觉策略，显著提升其在接触丰富任务上的表现；3）发布了大规模双手灵巧操作数据集DECO-50，包含触觉与主动视觉，为相关研究提供了宝贵资源。</p>
<p>论文自身提到的局限性包括触觉信号的稀疏性、传感器噪声，以及更复杂的多物体、长视野任务带来的挑战。这些为后续研究指明了方向，例如开发更鲁棒的触觉表示、探索触觉在更复杂任务规划中的作用，以及将解耦融合范式扩展到更多模态（如听觉、力/力矩）。本文的工作启示我们，对于多模态机器人学习，区分模态角色并设计针对性的融合机制，比简单的特征拼接更为有效；同时，通过适配器进行参数高效的增量学习，是快速赋予基础模型新感知能力的可行路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双手灵巧操作中多模态信息（视觉、本体感觉、触觉）整合的挑战，提出DECO框架。其核心是解耦的多模态扩散Transformer，通过专门路径分离并结构化整合各模态信号，并引入轻量级LoRA触觉适配器高效注入触觉信息。在发布的DECO-50数据集上训练后，真实机器人实验表明，DECO平均任务成功率达72.25%，较基线提升21%；触觉适配器额外带来10.25%的平均成功率提升，在接触密集任务上提升20%，且仅调整不到10%的模型参数。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.05513" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>