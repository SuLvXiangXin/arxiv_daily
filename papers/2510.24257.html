<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.24257" target="_blank" rel="noreferrer">2510.24257</a></span>
        <span>作者: Yue Gao Team</span>
        <span>日期: 2025-10-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人工具操作的主流方法包括数据驱动的端到端深度神经网络方法，以及基于关键点定义工具与环境并设计优化算法的方法。前者缺乏可解释性，且往往不考虑抓取成功后的后续操作过程；后者则侧重于在复杂场景中完成任务，通常忽略了机器人操作轨迹是否与人类相似这一关键问题。例如，在锤击钉子这类复杂任务中，现有方法通常提供一个简单的策略：抓取锤子后直接置于钉子上方并向下敲击。然而，人类锤击包含一个蓄力动作——在敲击前向相反方向挥动锤子以积累动能。这种细腻的类人行为难以用传统的优化方法学习，因为将挥动动作编码为目标方程中的约束十分困难。</p>
<p>本文针对“使机器人学习类人的、任务导向的操作技能”这一具体痛点，提出了结合对抗运动先验（Adversarial Motion Priors, AMP）与强化学习的新视角。核心思路是利用对抗网络从易于获取的人类演示视频中学习运动风格（先验），并通过结合任务奖励和风格奖励的强化学习，训练出既能高效完成任务、其运动轨迹又符合人类行为统计特性的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>HMAMP方法的整体流程如下：首先，从第三人称视角的人类操作视频片段中，提取人体手臂关节和工具的关键点序列。接着，通过直接映射等方式，将这些人类运动关键点对齐到仿真环境中的机器人形态上，形成参考运动数据集。然后，在一个结合了任务环境与对抗网络的强化学习框架中竞争性地训练策略网络和判别器网络。</p>
<p><img src="https://arxiv.org/html/2510.24257v1/figs/framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：HMAMP方法框架。使用人类操作视频片段提取关键点，并完成到机器人域的映射。AMP判别器用于判别状态转移序列是来自真实人类专家运动还是策略网络生成。用于操作任务的目标奖励和AMP奖励相加作为强化学习的总奖励。</p>
</blockquote>
<p>核心模块是奖励函数的设计，总奖励 $r_t$ 由目标奖励 $r_t^g$ 和风格奖励 $r_t^s$ 加权求和构成：$r_t = \alpha^g r_t^g + \beta^s r_t^s$。</p>
<ol>
<li><strong>目标奖励</strong>：针对具体的工具操作任务设计。以锤击任务为例，它由两项组成：$r_t^g = \omega^f r_t^f + \omega^d r_t^d$。第一项 $r_t^f$ 鼓励工具（锤头）与目标（钉子）的接触力达到期望值 $F^d$，任务在检测到接触力后终止。第二项 $r_t^d$ 通过双曲正切函数鼓励工具功能点 $\boldsymbol{x_f}$ 与环境目标点 $\boldsymbol{x_c}$ 之间的距离最小化，起到引导作用。</li>
<li><strong>风格奖励</strong>：由AMP判别器 $D_\phi$ 产生。判别器是一个神经网络，其输入是状态转移 $(s, s&#39;)$，目标是区分其来自智能体策略分布 $d^\pi$ 还是参考运动分布 $d^\mathcal{M}$。判别器的训练目标采用了LSGAN的损失函数，并加入了梯度惩罚项以提升稳定性。风格奖励计算为 $r^s_t = \max\left[0,1-\gamma^d(D_\phi(s,s&#39;)-1)^2\right]$，其值被限制在[0,1]区间内，当生成的状态转移与人类数据越相似，奖励越高。</li>
</ol>
<p>与现有方法相比，HMAMP的创新点具体体现在：1) 将最初应用于计算机图形学和腿部运动的AMP技术，创新性地引入到机器人手臂工具操作领域，以学习类人的运动风格。2) 采用易于获取的第三人称视频作为演示数据来源，并通过关键点提取和简单的直接映射来桥接人-机形态差异，降低了数据获取和处理的成本与难度。</p>
<p><img src="https://arxiv.org/html/2510.24257v1/figs/robot-human-correspondence.png" alt="人机映射"></p>
<blockquote>
<p><strong>图3</strong>：人体手臂与机器人手臂（Kinova Gen3）之间的直接映射关系。将机器人的特定关节和夹爪映射为对应的人体髋部、肘部、腕部和手部。</p>
</blockquote>
<p>策略网络采用PPO算法，观察值 $o_t$ 包含环境信息（如锤子、钉子位置）和机器人本体信息（关节角、速度、末端执行器朝向、上一时刻动作等）。为提升策略的鲁棒性并促进仿真到现实的迁移，训练过程中加入了领域随机化，包括随机化摩擦系数、PD控制器增益，并为观测添加噪声。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在一个具有挑战性的操作任务——锤击钉子上评估HMAMP。实验平台为Isaac Gym仿真器，使用Kinova Gen3七自由度机械臂模型。对比的基线方法包括：1) <strong>直接路径规划控制策略（DPPCP）</strong>：使用PD控制生成从起始点到钉子的规划轨迹。2) <strong>无AMP的强化学习（RL-noAMP）</strong>：除移除AMP组件外，其他配置与HMAMP相同。</p>
<p>评估指标包括：敲击冲量（Knock Impulse）、能量效率（Energy Efficiency，敲击冲量与机械臂能耗之比）、垂直力比率（Vertical Force Ratio）以及衡量运动轨迹相似性的弗雷歇距离（Frechet Distance）。</p>
<p><img src="https://arxiv.org/html/2510.24257v1/figs/train_proc_trajectory.png" alt="训练过程与轨迹对比"></p>
<blockquote>
<p><strong>图4</strong>：（a）和（b）展示了训练过程中目标奖励与风格奖励的对抗与平衡。早期目标奖励引导性强，后期AMP判别器快速收敛，赋予机器人运动类人风格。（c）显示了笛卡尔空间下机器人末端执行器的运动轨迹。HMAMP获得的轨迹与人类专家轨迹最为相似。</p>
</blockquote>
<p>仿真实验结果如表I所示，HMAMP在所有指标上均优于基线方法。具体而言，HMAMP产生的敲击冲量（4238 kg·m/s）远超DPPCP（2351 kg·m/s）和RL-noAMP（2507 kg·m/s），在核心任务效果上具有压倒性优势。同时，其能量效率（1.56 s/m）和垂直力比率（0.99）也最高。最重要的是，HMAMP轨迹与人类轨迹的弗雷歇距离（0.29）最小，证实了其有效学习人类运动风格的能力。</p>
<p><img src="https://arxiv.org/html/2510.24257v1/figs/comp_real_exp.png" alt="仿真与现实实验"></p>
<blockquote>
<p><strong>图5</strong>：第一行：用作运动先验的人类锤击视频片段。第二行：仿真环境中HMAMP策略的执行效果，锤子能以期望的类人轨迹成功完成任务。第三、四行：HMAMP在真实Kinova Gen3机械臂上的实施细节及锤击钉子过程，验证了仿真到现实的迁移能力。</p>
</blockquote>
<p>此外，论文成功将训练好的HMAMP策略迁移到真实的Kinova Gen3机械臂上，完成了锤击任务，证明了该方法的现实应用潜力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 将对抗运动先验（AMP）与任务导向的强化学习相结合，首次在工具操作领域实现了对类人运动风格的学习，并在锤击任务上验证了其优越性。2) 提出了一套利用易于获取的第三人称视频数据来学习机器人操作策略的流程，降低了演示数据获取的门槛。3) 在仿真环境中构建了训练框架，并通过领域随机化等技术，成功将所学策略迁移至真实机器人，验证了方法的实用性。</p>
<p>论文自身提到的局限性在于，目前仅在一个任务（锤击）上进行了评估。未来工作需要将方法扩展到更多样化的工具操作任务中。</p>
<p>这项工作对后续研究的启示在于：为机器人学习复杂、细腻的类人操作技能提供了一条有效路径，即通过对抗学习从非结构化的视觉演示中提取运动先验。后续研究可以探索更精细、更通用的人-机运动重定向方法，以及将此类框架应用于更广泛的工具使用和双手操作场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作动作生硬、缺乏人类自然风格的问题，提出一种基于对抗运动先验（HMAMP）的方法来学习任务导向的人类风格操作技能。其核心是利用对抗网络，通过判别器融合真实人类运动与智能体模拟数据，驱动策略生成符合人类运动统计特性的轨迹。该方法在锤击任务上得到验证，结果表明其能有效学习人类风格的操作技能，性能优于现有基线方法，并已在真实机械臂上成功演示。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.24257" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>