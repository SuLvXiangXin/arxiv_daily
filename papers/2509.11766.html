<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Igniting VLMs toward the Embodied Space - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Igniting VLMs toward the Embodied Space</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.11766" target="_blank" rel="noreferrer">2509.11766</a></span>
        <span>作者: Zhai, Andy, Liu, Brae, Fang, Bruno, Cai, Chalse, Ma, Ellie, Yin, Ethan, Wang, Hao, Zhou, Hugo, Wang, James, Shi, Lights, Liang, Lucy, Wang, Make, Wang, Qian, Gan, Roy, Yu, Ryan, Li, Shalfun, Liu, Starrick, Chen, Sylas, Chen, Vincent, Xu, Zach</span>
        <span>日期: 2025/09/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将强大的视觉语言模型迁移到具身体验领域是构建通用具身智能的关键路径。主流方法主要分为两类：一是“混合设计”，即在预训练VLM后直接附加动作头，通过离散或连续动作建模进行微调，如RT-2和OpenVLA；二是“解耦设计”，即使用独立的分支进行动作预测，并与VLM交互提取信息，如π0。然而，这些方法存在三个根本性鸿沟：1) <strong>模态与数据规模鸿沟</strong>：动作是连续的高频信号，缺乏大规模对齐的视觉-语言-动作数据，且高级指令与具体动作间的跨模态关联困难；2) <strong>预训练分布鸿沟</strong>：VLM预训练所用的互联网图像与具身场景的第一人称视角、鱼眼成像等存在显著差异，导致VLM在空间推理、场景理解等方面存在短板；3) <strong>训练目标鸿沟</strong>：VLM使用离散序列的下一个token似然作为目标，而动作更自然地由扩散或流匹配等条件生成目标建模，直接嫁接会导致token化鸿沟和独立性假设，削弱语言-动作对齐。</p>
<p>本文针对上述痛点，提出了一种名为WALL-OSS的端到端具身基础模型。其核心思路是：通过一个紧密耦合的混合专家架构和分阶段训练课程，在单个可微框架内统一指令推理、子任务分解和细粒度动作合成，从而系统性地弥合上述三个鸿沟。</p>
<h2 id="方法详解">方法详解</h2>
<p>WALL-OSS的目标是开发一个基于Transformer的具身基础模型，既能增强VLM对具身场景的空间理解，又能实现高质量的动作生成。其整体架构和训练流程旨在克服现有VLA模型的局限性。</p>
<p><img src="https://arxiv.org/html/2509.11766v1/x2.png" alt="不同范式对比"></p>
<blockquote>
<p><strong>图2</strong>：将VLM迁移到动作建模的不同范式。蓝色部分表示从预训练VLM主干继承的初始化权重。DAM和CAM分别指离散动作建模和连续动作建模，VL表示视觉-语言，SA表示自注意力。现有方法（a）混合设计和（b）解耦设计分别存在权重漂移导致VL先验退化，以及架构松散耦合削弱指令跟随能力的问题。</p>
</blockquote>
<p>WALL-OSS采用QwenVL2.5-3B作为主干网络，输入为视觉（自我中心和臂载摄像头视图）和文本指令。其创新在于采用了一个紧密耦合的混合专家架构，为不同的训练任务分配不同的前馈网络，从而形成强大的跨模态关联能力。</p>
<p><img src="https://arxiv.org/html/2509.11766v1/x3.png" alt="WALL-OSS架构"></p>
<blockquote>
<p><strong>图3</strong>：WALL-OSS的架构。模型基于VLM主干，通过紧密耦合的MoE设计，将不同的专家（VL-FFN和Action-FFN）分配给不同的模态和任务，并使用静态路由器进行特征路由，以增强跨模态绑定。</p>
</blockquote>
<p>训练过程分为两个主要阶段：启发阶段和整合阶段。</p>
<p><img src="https://arxiv.org/html/2509.11766v1/x4.png" alt="训练与推理流程"></p>
<blockquote>
<p><strong>图4</strong>：训练和推理流程总览。训练分为“启发”和“整合”两个阶段。推理时，模型支持统一的跨层级思维链，可以自适应地决定是否调用CoT/子任务分解，甚至交错进行推理与执行。</p>
</blockquote>
<p><strong>1. 启发阶段</strong>：目标是向VLM注入初步的具身推理和动作感知。此阶段重用预训练VLM的原始FFN，并通过<strong>具身VQA</strong>（涵盖掩码语言建模、图像/视频-文本对比学习、指令跟随、时序/因果建模）来增强其在机器人环境中的空间推理能力。同时，引入<strong>离散动作目标</strong>，使用FAST tokenization将连续动作轨迹离散化为token，并与文本token对齐进行预测。损失函数为VQA损失和离散动作损失的加权和。此阶段使VLM具备了粗略的、语义接地的动作感知，输出包括思维链推理、子任务预测和离散FAST动作token。</p>
<p><strong>2. 整合阶段</strong>：在已有先验的基础上，将离散动作预测替换为通过<strong>流匹配</strong>的连续动作建模。该阶段又分为两步：第一步冻结VLM，仅训练Action FFN下的流匹配头；第二步解冻VLM，进行联合优化。在此阶段，视觉、语言和动作表征通过注意力交互，一个<strong>静态路由器</strong>将动作中心特征导向Action FFN，将视觉-语言特征导向Vision-Language FFN。损失函数为流匹配损失，旨在回归速度场。这种设计使模型能够在紧密耦合的架构中输出细粒度动作，迫使模型整合多模态信息。</p>
<p><strong>统一跨层级思维链</strong>：WALL-OSS将思维链推理的概念从传统的文本逐步推理，推广到一个涵盖整个语义到感觉运动频谱的广义CoT：<code>指令 → 推理(CoT) → 子任务计划 → 连续动作</code>。该统一表述支持在层级抽象级别之间进行任意前向映射。模型通过一个路径丢弃目标进行训练，允许其灵活地以中间推理为条件或绕过它。这实现了全链式和直接 <code>指令→动作</code> 的训练，确保了端到端的可微性，同时保留了推理的灵活性。在推理时，Uni-CoT可以自适应地决定是否调用CoT/子任务分解，甚至可以交错进行推理与执行，为实现实时人机交互提供了灵活、异步的控制能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验旨在从三个核心维度全面评估模型能力：1) 语言指令理解、推理与泛化；2) 长视野、多阶段任务的规划与执行；3) 动作准确性与鲁棒性。</p>
<p><strong>评估任务</strong>：</p>
<ul>
<li><strong>具身VQA基准</strong>：包括场景描述、对象定位和动作规划，用于评估模型对具身场景的理解。通过人工评估对比WALL-OSS与原始基础VLM (qwen2.5-vl-3b)。</li>
<li><strong>机器人操纵任务</strong>：设计了六个任务（如图6所示），其中set-table, tidy-bedroom, place-by-color是预训练未见的新任务，用于评估模型在新任务上的适应能力。<ul>
<li><strong>指令跟随与推理</strong>：Instruction-pick-place（零样本泛化）、Place-by-color（视觉匹配与文本推理）、Block-spell（推理与拼写）。</li>
<li><strong>长视野规划与执行</strong>：Set-Table（摆桌子）、Tidy-Bedroom（整理卧室）。</li>
<li><strong>基础操作</strong>：Pick-Up-Waste（捡垃圾）、Pick-Place-Cup（放置杯子）。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2509.11766v1/x6.png" alt="评估任务概览"></p>
<blockquote>
<p><strong>图6</strong>：微调期间使用的评估任务概览。上方：单指令任务。下方：长视野和推理任务。</p>
</blockquote>
<p><strong>基线方法</strong>：对比了当前代表性的开源VLA模型，包括采用混合设计的 <strong>OpenVLA</strong>、采用解耦设计的 <strong>π0</strong>，以及同样基于QwenVL2.5-3B但采用不同训练方法的 <strong>GR00T N1</strong>。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>总体性能</strong>：在六个操纵任务的综合评估中，WALL-OSS取得了最高的平均成功率（**78.3%**），显著优于OpenVLA（45.8%）、π0（58.3%）和GR00T N1（65.0%）。</li>
<li><strong>指令跟随与推理</strong>：在需要复杂推理的Block-Spell任务中，WALL-OSS成功率高达**86.7%**，远超其他基线（OpenVLA 13.3%， π0 33.3%， GR00T N1 53.3%）。在零样本的Instruction-Pick-Place任务中，WALL-OSS也达到70.0%的成功率，展示了强大的泛化能力。</li>
<li><strong>长视野任务</strong>：在Set-Table和Tidy-Bedroom这两个多步骤任务中，WALL-OSS分别取得了<strong>76.7%</strong> 和 <strong>81.7%</strong> 的成功率，均大幅领先于所有基线方法，证明了其统一CoT在分解和执行复杂任务上的有效性。</li>
<li><strong>具身VQA能力</strong>：在具身VQA基准测试中，经过启发的WALL-OSS模型在场景描述、对象定位和动作规划三项任务上均显著优于原始的基础VLM，例如动作规划准确率从10%提升至**68%**。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11766v1/x7.png" alt="主要结果"></p>
<blockquote>
<p><strong>图7</strong>：在六个机器人操纵任务上的成功率。WALL-OSS在整体平均成功率以及多个子任务上均领先于基线方法。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>论文通过消融实验验证了核心组件的贡献：</p>
<ul>
<li><strong>移除启发阶段</strong>：直接进行整合阶段训练会导致性能严重下降（平均成功率从78.3%降至58.3%），证明了离散动作先验和具身VQA对稳定训练和保持VL先验的重要性。</li>
<li><strong>移除整合阶段的VLM联合训练</strong>（即始终冻结VLM）：性能下降至68.3%，表明联合优化对于紧密耦合多模态对齐至关重要。</li>
<li><strong>使用学习型路由器替代静态路由器</strong>：性能下降至70.0%，说明静态路由器在维持明确的任务分离和防止模态干扰方面更有效。</li>
<li><strong>移除Uni-CoT训练</strong>：在长视野任务（Set-Table, Tidy-Bedroom）上性能下降最明显，平均下降约15%，验证了统一跨层级推理对复杂任务分解和执行的关键作用。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了系统性的解决方案</strong>：针对VLM迁移至具身空间的三个根本性鸿沟（模态、预训练分布、训练目标），提出了WALL-OSS模型，其紧密耦合的MoE架构和两阶段训练课程（启发与整合）有效弥合了这些鸿沟。</li>
<li><strong>引入了统一跨层级思维链</strong>：提出了Uni-CoT范式，在一个端到端可微的框架内统一了从高级指令推理到低层连续动作生成的整个频谱，显著提升了长视野复杂任务的规划与执行成功率。</li>
<li><strong>构建了大规模多源数据集并开源</strong>：为应对对齐VLA数据稀缺的挑战，构建了超过一万小时、包含自采集动作、开源动作和多模态VQA的数据集，并开源了完整的训练代码和模型检查点，推动了领域发展。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1) 尽管整合了多源数据，但高质量、大规模、对齐的机器人动作-视觉-语言数据仍然稀缺，限制了模型的进一步扩展。2) 紧密耦合的端到端训练计算成本较高。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>架构与训练策略的协同设计</strong>：WALL-OSS展示了通过精心设计的架构（紧密耦合MoE）与训练课程（分阶段目标）相结合，可以系统性地解决跨模态对齐问题，这为未来构建更复杂的多模态基础模型提供了思路。</li>
<li><strong>广义推理链的价值</strong>：将推理链从纯文本扩展到包含具体行动计划的语义-感觉运动谱系，并被证明能有效提升任务性能，这启示我们可以探索更丰富、更结构化的中间表示来引导智能体行为。</li>
<li><strong>数据建设的标准化与融合</strong>：论文中对异构开源动作数据进行的标准化处理（坐标系、形态归一化等）为大规模融合不同机器人平台数据提供了可行方案，是 scaling law 在具身领域应用的重要基础。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决现有视觉语言模型在具身空间理解与动作生成方面的瓶颈。作者提出端到端具身基础模型WALL-OSS，通过紧密耦合架构与多策略训练课程，实现了统一的跨层级思维链，将指令推理、子目标分解与细粒度动作合成整合于单一可微分框架。实验表明，该模型在复杂长时程操作任务上取得高成功率，具备强大的指令跟随与复杂推理能力，性能优于现有基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.11766" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>