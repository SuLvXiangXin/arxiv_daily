<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.07813" target="_blank" rel="noreferrer">2505.07813</a></span>
        <span>作者: Tao, Tony, Srirama, Mohan Kumar, Liu, Jason Jingzhou, Shaw, Kenneth, Pathak, Deepak</span>
        <span>日期: 2025/05/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，获取大规模、多样化的机器人数据集是实现灵巧操作策略泛化的关键路径。主流方法是通过遥操作收集数据，其保真度高，但成本高昂、可扩展性差，难以在多样化的真实环境中大规模部署。另一种思路是利用互联网视频数据，但其存在动作捕捉噪声大、数据非结构化等问题，难以获取精细的控制信号。本文针对机器人数据收集的“规模-质量-多样性”难以兼得的痛点，提出了一个新视角：能否让人像在日常生活中一样，直接使用自己的双手来高效收集高质量数据？本文的核心思路是：设计一个便携、易用的数据采集系统（DexWild-System），让未经训练的用户能在多样化的真实环境中用双手收集大规模、高保真的人类演示数据，并通过一个协同训练框架将这些数据与少量机器人数据结合，以训练出能泛化到新环境、新任务和新机器人形态的鲁棒策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>DexWild的整体框架包含两个核心部分：1) DexWild-System 数据采集硬件系统；2) 结合人类与机器人演示数据的协同训练学习框架。</p>
<p><strong>1. 数据采集系统 (DexWild-System)</strong><br>该系统设计围绕三个核心目标：便携性、高保真度、与具体机器人无关（Embodiment-Agnostic）。</p>
<ul>
<li><strong>便携性</strong>：系统仅包含三个组件：一个用于手腕姿态估计的单目跟踪相机、一个用于机载数据采集的电池供电迷你PC，以及一个集成了动作捕捉手套和同步掌上相机的定制传感器舱。该系统无需标定，采用相对状态-动作表示（每个状态和动作记录为相对于上一时间步姿态的差值），从而无需全局坐标系，允许跟踪相机自由放置（以自我为中心或以他人为中心）。</li>
<li><strong>高保真度</strong>：<ul>
<li><strong>手部姿态</strong>：采用基于电磁场（EMF）传感的动作捕捉手套，提供高精度、低延迟且对遮挡鲁棒的手指位置估计。</li>
<li><strong>手腕姿态</strong>：在手套上安装ArUco标记，使用外部相机进行跟踪，避免了基于SLAM手腕跟踪在特征稀疏环境或重度遮挡任务（如开抽屉）中的失效问题。</li>
<li><strong>视觉观察</strong>：在手掌上刚性安装两个全局快门立体相机，以捕获详细、局部化的交互视图，视野宽广且运动模糊最小。</li>
</ul>
</li>
<li><strong>与具体机器人无关</strong>：<ul>
<li><strong>观察空间对齐</strong>：人类和机器人手掌上的相机安装位置是镜像对称的，主要聚焦于环境，最小化手部本身的可见性，从而产生跨形态的视觉一致观察。</li>
<li><strong>动作空间对齐</strong>：基于先前工作，通过优化机器人手运动学以匹配人类演示中观察到的指尖位置，实现动作重定向。该方法通用，适用于任何机器人手形态，且对用户手部尺寸变化鲁棒。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2505.07813v1/x2.png" alt="DexWild-System 数据采集示意图"></p>
<blockquote>
<p><strong>图2</strong>：DexWild-System组成。左：个人使用自己的双手在各种环境中高效捕获高保真数据。右：机器人手上装有与人类相机对齐的相机。我们在两个不同的机器人手和手臂上测试DexWild。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.07813v1/x3.png" alt="观察空间对齐示意图"></p>
<blockquote>
<p><strong>图3</strong>：DexWild对齐人类和机器人之间的视觉观察，以弥合形态差距。这激励模型学习以任务为中心而非以形态为中心的表示。</p>
</blockquote>
<p><strong>2. 训练框架</strong><br>框架收集两种互补数据集：大规模人类演示数据集 (D_H) 和较小的遥操作机器人数据集 (D_R)。通过协同训练策略来结合两者的优势。</p>
<ul>
<li><strong>数据预处理</strong>：对人和机器人的动作分别进行归一化，以解决固有的分布不匹配问题；对收集自非受控环境的人类演示应用启发式过滤流水线，自动检测并移除低质量轨迹。</li>
<li><strong>策略架构</strong>：采用基于扩散模型（Diffusion U-Net）的策略，以有效捕获来自多个人类演示者的多模态行为分布。视觉编码器采用在大规模互联网数据上预训练的Vision Transformer (ViT)，以更好地处理野外任务的视觉多样性。</li>
<li><strong>协同训练</strong>：在每个训练迭代中，根据预设的协同训练权重 ((w_h, w_r))，从 (D_H) 和 (D_R) 中分别采样一批转换数据。训练目标为简单的行为克隆，使用扩散损失函数 (\mathcal{L}<em>{\theta}=|\epsilon_t-\hat{\epsilon}</em>{\theta}|<em>{2}^{2})，其中 (\hat{\epsilon}</em>{\theta}) 是策略预测的噪声。</li>
<li><strong>关键创新</strong>：与单独使用任一种数据训练相比，本文的核心创新在于<strong>协同训练权重的精心调配</strong>。论文发现，调整人类与机器人数据的混合比例对现实世界性能有显著影响，最优的混合能平衡人类数据的多样性与机器人数据的形态 grounding。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.07813v1/x4.png" alt="任务与数据概览"></p>
<blockquote>
<p><strong>图4</strong>：使用DexWild-System，人类可以轻松地在各种环境中用自己的双手收集准确数据。这些数据被直接用于训练任何机器人手，使其能在任何环境中以类人的方式执行灵巧操作。我们在五个代表性任务上验证了这一方法。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：使用DexWild-System，由10名未经训练的用户在93个多样化真实世界环境中收集了9,290条人类演示 ((D_H))，涵盖五个任务。同时，使用xArm和LEAP hand V2 Advanced通过遥操作收集了1,395条机器人演示 ((D_R))。</li>
<li><strong>评估任务</strong>：五个任务用于系统评估：喷瓶（测试功能性抓握）、玩具清理（测试长视距规划与泛化）、倒水（测试跨任务技能迁移）、双手花艺（测试双手协调）、双手叠衣服（测试可变形物体操作）。训练和测试对象如图5所示。</li>
<li><strong>对比基线</strong>：包括仅使用人类数据训练的策略、仅使用机器人数据训练的策略、以及不同的数据混合比例。</li>
<li><strong>评估平台</strong>：在配备不同机器人手（LEAP hand V2 Advanced等）和手臂（xArm）的物理机器人上进行真实世界部署评估。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.07813v1/x5.png" alt="训练与测试对象详情"></p>
<blockquote>
<p><strong>图5</strong>：我们收集了跨类别的多样化物体数据。喷瓶任务 – 25训练，11测试；玩具清理任务 – 64训练，9测试；倒水任务 – 35训练，5测试；花艺任务 - 6训练，2测试；叠衣服任务 - 17训练，6测试。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>在未见环境中的泛化</strong>：在全新的测试环境中，DexWild（协同训练）策略取得了<strong>68.5%</strong> 的平均成功率。这比仅使用机器人数据训练的策略（17.7%成功率）高出近<strong>四倍</strong>。仅使用人类数据训练的策略成功率为53.8%，表明协同训练带来了显著增益。</li>
<li><strong>跨形态泛化</strong>：当将在一种机器人手上训练的策略迁移到另一种不同的机器人手上时，DexWild方法相比基线实现了 <strong>5.8倍</strong> 的性能提升。</li>
<li><strong>数据收集效率</strong>：DexWild-System的数据采集速度比传统的基于机器人的方法快 <strong>4.6倍</strong>。</li>
<li><strong>消融实验（数据混合比例）</strong>：如图6所示，调整人类与机器人数据的混合权重对性能有巨大影响。纯人类或纯机器人数据性能均非最优。在喷瓶任务上，最佳权重（人类:机器人 = 4:1）下的成功率（80%）显著高于纯人类（60%）或纯机器人（20%）数据。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07813v1/x6.png" alt="协同训练消融实验"></p>
<blockquote>
<p><strong>图6</strong>：协同训练如何提供帮助。图表显示了在喷瓶任务上，改变训练批次中人类与机器人数据比例对成功率的影响。最佳性能出现在4:1（人:机）的混合比例下。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>可扩展的数据收集系统</strong>：提出了DexWild-System，一个低成本、便携、免标定、与具体机器人无关的系统，使未经训练的用户能够快速在多样化真实环境中收集大规模、高保真的人类手部演示数据。</li>
<li><strong>高效的协同训练框架</strong>：提出了一个学习框架，通过最优地结合大规模人类演示和少量机器人演示进行协同训练，显著提高了策略对新环境、新任务和新机器人形态的泛化能力。</li>
<li><strong>强大的实证结果</strong>：在五个具有挑战性的灵巧操作任务上进行了广泛的真实世界实验，证明了该方法在未见环境中的成功率（68.5%）相比仅用机器人数据有近四倍的提升，并在跨形态泛化上取得了5.8倍的改进。</li>
</ol>
<p><strong>局限性</strong>：论文提到，尽管系统力求便携，但仍需要佩戴手套和携带一些硬件，并非完全无感。此外，虽然展示了跨任务的一些技能迁移（如从喷瓶到倒水），但跨任务的泛化能力可能仍有局限，特别是在语义差异较大的任务之间。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>数据收集范式</strong>：证明了利用自然人类交互进行大规模数据收集的可行性和巨大价值，为突破机器人数据瓶颈提供了一条新路径。</li>
<li><strong>跨形态学习</strong>：通过精心设计观察与动作空间的对齐，展示了如何有效地利用人类数据为不同形态的机器人训练策略，这对机器人硬件的快速迭代具有重要意义。</li>
<li><strong>协同训练策略</strong>：揭示了异构数据源（人类 vs. 机器人）混合比例对最终性能的关键影响，为未来如何融合不同质量、不同来源的演示数据提供了重要参考。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决机器人灵巧操作策略泛化到新环境时，大规模多样化数据采集成本高昂、可扩展性差的核心难题。提出DexWild方法，其关键技术包括：1) 设计低成本易用的DexWild-System移动设备，支持人类直接用手在多种真实环境中进行数据采集；2) 构建学习框架，协同训练人类演示数据与机器人数据。实验表明，该方法显著提升了策略的泛化能力，在未见环境中成功率高达68.5%，是仅使用机器人数据训练策略的近4倍，并实现了5.8倍的跨体现泛化性能提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.07813" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>