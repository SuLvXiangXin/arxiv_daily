<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.12912" target="_blank" rel="noreferrer">2511.12912</a></span>
        <span>作者: Dongbin Zhao Team</span>
        <span>日期: 2025-11-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在基于深度数据的端到端机器人抓取策略中，将在仿真环境中训练的策略迁移到物理机器人是一种高效且鲁棒的方法。然而，现实深度图中存在的传感器伪影（如空洞和噪声）构成了显著的仿真到现实差距，严重阻碍了策略迁移。现有的训练时策略（如注入程序性随机噪声或学习隐式映射）因噪声模拟不真实而存在数据效率低下的问题，并且通常需要配对的仿真-现实数据集。另一方面，利用基础模型通过中间表示来减小差距的方法，无法完全缓解域偏移，并在部署时引入额外的计算开销。本文旨在同时解决数据效率低下和部署复杂度高的双重挑战，提出了DiffuDepGrasp，一个部署高效的仿真到现实框架，通过仅在仿真中训练策略实现零样本迁移。其核心思路是设计一个扩散深度生成器，利用少量非配对真实数据学习复杂的传感器噪声分布，并将其高保真地注入到几何完美的仿真深度中，从而生成兼具感知真实性和几何准确性的训练数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>DiffuDepGrasp框架旨在实现零样本仿真到现实迁移，其整体流程分为四个阶段，通过结合逼真的数据生成与高效策略学习管道协同工作。</p>
<p><img src="https://arxiv.org/html/2511.12912v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DiffuDepGrasp框架。(A) 教师策略训练阶段：利用仿真中的特权状态信息训练一个基于强化学习的高性能教师策略，用于收集专家演示。(B) 扩散深度生成器阶段：包含两个核心模块。第一个扩散深度模块在收集的真实RGB-D数据上训练，以学习传感器的噪声分布（k表示扩散过程时间步，区别于策略时间步t）。第二个噪声嫁接模块旨在将这些学习到的伪影注入到纯净的仿真几何中。在推理时，完整的DDG算法将仿真的RGB-D数据转换为高保真、带噪声的深度图。(C) 学生策略蒸馏阶段：收集专家轨迹，利用扩散深度生成器将其视觉数据转换为生成的噪声深度，然后通过模仿学习将教师的知识蒸馏到学生策略中。(D) 仿真到现实部署：该学生策略实现零样本部署，直接迁移到物理机器人执行抓取任务。</p>
</blockquote>
<p><strong>核心模块一：教师策略训练</strong><br>首先，在仿真中训练一个基于特权信息的教师策略。该策略的输入观测空间o_t ∈ ℝ^40，包括机器人本体感知状态和任务相关信息，具体为关节位置(ℝ^9)、速度(ℝ^9)、末端执行器位姿(ℝ^7)、目标物体位姿(ℝ^7)、预定义的目标抓取位姿(ℝ^3)以及对数变换后的物体类别ID(ℝ^1)。动作空间为8维连续动作a_t ∈ ℝ^8，前7维代表期望的关节位置变化，第8维是离散指令控制夹爪开合。使用密集设计的奖励函数（公式2），包含接近物体、抬起物体、对齐方向、稀疏成功奖励以及关节加速度惩罚等项，通过PPO算法在Isaac Gym中训练，以学习高效鲁棒的抓取行为。</p>
<p><strong>核心模块二：扩散深度生成器</strong><br>这是框架的核心创新，用于生成具有真实传感器噪声的深度数据，包含两个协同模块。</p>
<ol>
<li><strong>扩散深度模块</strong>：用于学习真实世界传感器噪声伪影的分布。该模块利用先进的视频深度估计器Video Depth Anything生成稳定、时间一致的几何先验c，规避了单图像估计器固有的闪烁和尺度不一致问题。其主干是一个基于条件U-Net的噪声预测网络ε_θ。在训练时，从数据集中采样真实噪声深度图d_0及其对应的条件图c，通过随机时间步k添加高斯噪声得到d_k。网络输入为d_k和c的通道拼接以及时间嵌入k，训练目标（公式8）是预测所添加的噪声ε。通过端到端优化，扩散模型学习将一个随机高斯场去噪为包含复杂传感器伪影的深度图。在推理时，从纯高斯噪声图开始，结合来自仿真的固定条件深度图c，执行K步迭代反向去噪过程，最终输出包含逼真噪声模式的预测深度图d_pred。</li>
<li><strong>噪声嫁接模块</strong>：为确保绝对的几何精度同时模拟逼真的噪声而设计。该模块接收来自扩散深度模块的感知真实深度图d_pred和几何完美的仿真深度d_sim。首先通过预定义的噪声阈值在d_pred中识别传感器伪影区域（如空洞），生成一个二值掩码M。然后使用公式9将d_pred中的伪影模式嫁接到d_sim的几何基础上：d_final = d_sim ⊙ (1 − M) + d_pred ⊙ M，其中⊙表示逐元素相乘。此过程确保最终生成的训练数据在保留仿真完美度量几何的同时，体现了从真实世界学习到的复杂噪声特性。</li>
</ol>
<p><strong>核心模块三：学生策略蒸馏</strong><br>为了将专家知识迁移到仅依赖视觉和本体感知信息的策略上，通过模仿学习将其蒸馏为学生策略。首先利用训练好的教师策略，在应用了动力学和视觉域随机化的仿真中收集约1.2k条成功抓取轨迹。然后，使用预训练的扩散深度生成器，将原始仿真RGB-D数据转换为高保真的噪声深度图，作为学生策略的视觉输入。学生策略网络采用双流架构：视觉编码器使用预训练的ResNet-18主干从堆叠的深度图中提取特征；小型MLP状态编码器处理关节角度。两路特征向量拼接后送入动作头。遵循扩散策略，该网络被训练为预测在给定扩散时间步k下添加到专家动作a_0上的噪声（公式10）。在推理时，策略通过以当前观测为条件，从随机高斯向量迭代去噪来生成动作。</p>
<p><strong>核心模块四：零样本仿真到现实部署</strong><br>部署的学生策略π_D保持与训练时相同的输入规格：3个连续的真实世界原始深度图序列与8维关节角度状态。策略网络通过闭环控制实时处理这些输入，生成8维连续动作以控制机械臂抓取。整个流程无需在线深度修复、几何估计或其他计算密集型预处理，确保了高运行效率和最低的硬件资源需求。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：策略训练和仿真评估在Isaac Gym物理模拟器中进行。真实与仿真使用相同的硬件设置：7自由度Franka Emika Panda机械臂配备UMI平行夹爪。仿真训练集包含6个不同几何和材质的物体类别。为评估策略泛化能力，测试集包含12个物体，分为已见的6个类别和未见过的6个新类别。真实世界视觉由静态安装的Intel RealSense D455深度相机提供。策略在仿真和现实部署中均以10Hz控制频率闭环运行。</p>
<p><strong>对比基线</strong>：</p>
<ul>
<li>RL (State-based)：基于特权状态的教师策略。</li>
<li>GT-based Policies：均在纯净仿真GT深度图上通过模仿学习训练同一底层策略，区别在于处理真实噪声的方式：<ul>
<li>Naive Transfer：直接在原始噪声深度图上部署。</li>
<li>GT + Random Noise：训练时用程序性随机噪声增强GT深度。</li>
<li>GT + Inpaint：部署时用OpenCV修复算法预处理真实深度图。</li>
</ul>
</li>
<li>DAv2：策略在由Depth Anything V2从RGB图像估计的深度图上训练和部署。</li>
</ul>
<p><strong>深度生成结果评估</strong>：<br>论文从定性可视化、特征空间分布对齐和定量分布差异三个角度评估了扩散深度生成器生成数据的真实性。</p>
<p><img src="https://arxiv.org/html/2511.12912v1/x2.png" alt="视觉表示对比"></p>
<blockquote>
<p><strong>图2</strong>：仿真到现实不同视觉表示的对比。展示了仿真RGB、真实RGB、仿真GT深度、真实原始噪声深度，以及各基线方法（随机噪声、修复、DAv2估计深度）和本文DDG生成的深度图。直观显示了DDG生成结果在噪声模式上更接近真实观测。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.12912v1/x3.png" alt="深度生成定性结果"></p>
<blockquote>
<p><strong>图3</strong>：噪声深度数据生成的定性结果。从上到下各行分别为：原始仿真RGB、对应的纯净仿真深度、不带噪声嫁接模块的扩散深度模块生成结果、以及完整DDG（带噪声嫁接模块）的生成结果。显示DDG能成功生成复杂且逼真的传感器伪影（如空洞和边缘噪声）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.12912v1/x4.png" alt="特征空间分布"></p>
<blockquote>
<p><strong>图4</strong>：不同深度图在特征空间中的t-SNE可视化。点表示从仿真GT深度、真实噪声深度、DAv2估计深度、带随机噪声的GT深度以及DDG生成深度中提取的特征。DDG生成的特征分布与真实噪声深度分布重叠度最高，表明其成功模拟了真实噪声的统计特性。</p>
</blockquote>
<p>定量指标上，在FID和KID分数上，DDG生成的数据与真实噪声数据之间的分布差距最小，显著优于随机噪声和DAv2估计深度等方法。</p>
<p><strong>抓取性能结果</strong>：</p>
<p><img src="https://arxiv.org/html/2511.12912v1/x5.png" alt="抓取成功率"></p>
<blockquote>
<p><strong>图5</strong>：在不同物体类别上的抓取成功率。从左至右分别为：仿真中教师策略（特权状态）的性能、各基线方法在真实世界已见和未见物体上的零样本迁移性能、以及本文DiffuDepGrasp框架的性能。DiffuDepGrasp在已见和未见物体上均取得了最高且最稳定的成功率。</p>
</blockquote>
<p>关键实验结果总结：在真实世界12物体抓取任务中，DiffuDepGrasp实现了**95.7%**的平均成功率。具体来看，在已见的6个物体上成功率为96.7%，在未见的6个物体上成功率为94.7%，表现出强大的零样本迁移和泛化能力。相比之下，Naive Transfer直接部署失败（0%成功率），证明了深度域差距的严重性。GT+Random Noise方法成功率仅为43.3%，说明程序性噪声不足以模拟真实噪声。GT+Inpaint和DAv2方法分别达到78.3%和85.0%的成功率，但仍显著低于本文方法，且DAv2方法在未见物体上性能下降更明显。</p>
<p><strong>消融实验</strong>：<br>论文通过移除噪声嫁接模块（DDG w/o G）进行了消融实验。结果显示，虽然DDG w/o G生成的数据在感知上逼真，但由于其预测的深度在度量几何上存在偏差，导致以此训练的学生策略抓取成功率下降。这验证了噪声嫁接模块对于保持几何精度、从而确保下游抓取任务性能的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个部署高效的仿真到现实框架DiffuDepGrasp，实现了基于深度感知的抓取策略的零样本迁移，无需在部署时进行额外的计算处理。</li>
<li>设计了创新的扩散深度生成器，其扩散深度模块利用时间几何先验，仅需少量非配对真实RGB-D数据即可高效学习复杂传感器噪声分布；噪声嫁接模块则将学习到的伪影注入仿真深度，在保持完美度量几何的同时实现感知真实性。</li>
<li>在真实机器人实验中取得了优异的零样本性能（平均成功率95.7%），并展示了对未见物体的强泛化能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，扩散深度生成器的训练依赖于一个预训练的视频深度估计模型来提供几何先验。虽然这减少了对配对数据的需求，但该基础模型的性能上限可能影响生成深度的质量。</p>
<p><strong>启示</strong>：<br>本文工作为仿真到现实迁移，特别是涉及复杂传感器噪声的领域，提供了一条高效、数据需求低的路径。其“学习噪声模式并嫁接至完美几何”的核心思想，可泛化至其他依赖精确几何感知的机器人任务（如装配、操作）。同时，该方法减少了对高性能部署硬件的依赖，有助于在资源受限的平台上实现智能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模拟训练深度抓取策略转移到现实机器人时，因真实深度图中的传感器噪声和空洞导致的sim2real差距问题，提出DiffuDepGrasp框架。其核心创新Diffusion Depth Generator包含两个模块：Diffusion Depth Module利用时间几何先验训练条件扩散模型以捕获复杂噪声分布，Noise Grafting Module在注入噪声时保持度量准确性。实验表明，该框架仅需原始深度输入，在零样本转移下对12个物体的抓取任务达到95.7%的平均成功率，并具强泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.12912" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>