<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.15761" target="_blank" rel="noreferrer">2601.15761</a></span>
        <span>作者: Shu Zhang Team</span>
        <span>日期: 2026-01-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，在现实世界中部署强化学习面临样本效率低、奖励稀疏和视觉观测噪声大等挑战。主流方法主要依赖两类先验知识：一是利用大规模预收集数据集进行离线到在线学习（如IQL、CQL、AWAC），以降低探索成本并稳定训练；二是借助人类在环干预或基于大规模预训练的视觉-语言-动作模型进行微调。然而，这些方法存在关键局限性：离线到在线方法通常需要大量高质量数据集来克服分布偏移，数据获取成本高昂；而基于大模型的方法则依赖大规模预训练和微调，且其可靠性（如幻觉、对齐问题）和在高动态、噪声环境下的鲁棒性尚未得到充分验证。因此，一种数据需求极低、无需大规模预训练的低成本现实世界RL方法仍然缺失。</p>
<p>本文针对上述痛点，提出了一种新的视角：能否仅使用单条专家轨迹，实现从零开始的样本高效学习？本文的核心思路是提出SigEnt-SAC算法，通过引入Sigmoid有界熵机制防止Q函数振荡和策略探索到分布外动作，并结合门控行为克隆利用单条专家轨迹稳定策略优化，从而实现快速收敛和高效学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>SigEnt-SAC框架建立在两个核心机制之上：Sigmoid有界熵和门控行为克隆。整体流程遵循标准的离线策略演员-评论家框架，但修改了评论家更新中的熵项以及策略更新目标，并利用一个专家数据缓冲区提供引导。</p>
<p><strong>核心模块一：Sigmoid有界熵机制</strong><br>该机制旨在解决标准软演员-评论家中，负熵项可能导致Q函数优化偏向分布外动作并引发振荡的问题。对于tanh压扁的高斯策略 $a = \tanh(x)$，其中 $x$ 通过重参数化从策略网络输出采样得到。定义每个维度的惊奇值（负对数密度）为 $s_i = -\log\pi_{\theta,i}(a_i|s)$。有界熵贡献通过一个温度控制的sigmoid函数计算：<br>$$h_{i}(s_{i})=h_{\max}\cdot\sigma!\left(\frac{s_{i}-m}{t}\right), \quad \mathcal{H}<em>{\text{sig}}(s,a)=\sum</em>{i=1}^{d}h_{i}(s_{i})$$<br>其中 $\sigma(\cdot)$ 是sigmoid函数，$h_{\max}&gt;0$ 控制最大每维度熵得分，$m$ 是中心偏移，$t&gt;0$ 是温度参数。由此构造的 $\mathcal{H}<em>{\text{sig}}(s,a) \in (0, d\cdot h</em>{\max})$ 是一个严格有界的正值。在评论家更新中，用 $\mathcal{H}_{\text{sig}}$ 替换标准的负熵项 $- \log \pi(a|s)$，更新规则如论文公式(2)所示。</p>
<p><img src="https://arxiv.org/html/2601.15761v1/figures/draw_entropy_concept_4_compare_Q_H_Z.png" alt="负熵与有界熵对Q函数的影响对比"></p>
<blockquote>
<p><strong>图1</strong>：负熵效应与Sigmoid有界熵效果的示意图。上排：默认的负熵项（第1行第2列）降低了策略动作及其邻域的Q值估计，使得最大化Q的策略改进更可能移向分布外动作。下排：Sigmoid有界熵将熵贡献映射到一个严格有界的正值范围，产生了一个更清晰的高Q值区域和一个更明确的用于最大化的动作集。</p>
</blockquote>
<p><strong>核心模块二：门控行为克隆</strong><br>为了在Q函数尚未收敛或振荡时稳定策略优化，并减少无效探索，本文在策略更新中引入了一个基于专家数据的门控行为克隆损失。定义策略的确定性平均动作为 $a_{\mathrm{mean}}(s) = \tanh(\mu_{\theta}(s))$，门控掩码为：<br>$$p_{\mathrm{gate}}(s,a_{\mathrm{exp}}) = \mathbb{I}!\left[|a_{\mathrm{mean}}(s)-a_{\mathrm{exp}}|<em>{2} &gt; \varepsilon\right]$$<br>其中 $\varepsilon &gt; 0$ 是阈值。只有当策略平均动作与专家动作的偏差超过该阈值时，行为克隆损失才会被激活。最终的策略优化目标是最大化以下统一目标 $\mathcal{J}(\pi</em>{\theta})$（论文公式(4)），它结合了熵增强的Q值期望、Sigmoid有界熵项以及门控的专家塑造项（负的均方误差）。</p>
<p><img src="https://arxiv.org/html/2601.15761v1/x1.png" alt="策略优化与OOD动作关系"></p>
<blockquote>
<p><strong>图2</strong>：使用负熵项的策略优化更可能产生OOD动作；相应地，较小的目标策略熵会导致较高的OOD动作比率。OOD标准由公式(3)定义，阈值设为0.3。图中还对Cal-QL应用了LayerNorm以减轻网络振荡引起的虚假OOD动作。</p>
</blockquote>
<p><strong>算法实现细节</strong><br>评论家损失函数结合了标准的时序差分误差和一个简化的CQL风格正则化项（论文公式(7)），用于抑制在采样OOD动作上的虚假高Q值。最终评论家目标为 $\mathcal{L}<em>{Q_i} = \mathcal{L}</em>{\mathrm{TD}}^{(i)} + \lambda_{\mathrm{ood}},\mathcal{L}<em>{\mathrm{CQL}}^{(i)}$。策略损失则是统一目标 $\mathcal{J}(\pi</em>{\theta})$ 的负值（论文公式(9)），在实现中，门控行为克隆项仅在每样本偏差 $ \frac{1}{d}|a_{\mathrm{mean}}(s)-a_{\mathrm{exp}}|<em>{2}^{2} &gt; \varepsilon</em>{\mathrm{bc}}^{2} $ 时激活。</p>
<p><strong>创新点总结</strong><br>与现有方法相比，SigEnt-SAC的核心创新在于：1) <strong>Sigmoid有界熵</strong>：将无界的负熵信号转换为有界正值，从根本上防止了因熵项主导而导致的Q值振荡和OOD动作探索问题。2) <strong>门控行为克隆</strong>：动态地、有条件地利用单条专家轨迹进行策略塑造，既能在策略偏离时提供稳定梯度，又避免了无条件行为克隆对探索和超越专家性能的限制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：本文在两类基准上评估SigEnt-SAC：1) <strong>D4RL连续控制任务</strong>：包括复杂的操作任务（Kitchen, Adroit），仅提供单条成功轨迹。2) <strong>真实世界机器人任务</strong>：设计了四个任务（见图3），涉及不同形态的机器人（轮式、机械臂、四足、人形），均使用稀疏奖励和单目灰度图像观测。</p>
<p><img src="https://arxiv.org/html/2601.15761v1/x2.png" alt="真实世界任务套件说明"></p>
<blockquote>
<p><strong>图3</strong>：真实世界任务概述。左：任务总览。中：统一的策略输入，由单视角灰度局部观测组成。右：每个任务的随机化设置。</p>
</blockquote>
<p><strong>对比基线</strong>：包括保守Q学习方法（Cal-QL, CQL）、高效的在线微调方法（RLPD）、Q值加权行为克隆（AWAC）、隐式Q学习（IQL）以及用专家数据初始化的标准SAC。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>单次演示下的高效学习</strong>：在D4RL任务的单次演示设置中，SigEnt-SAC比所有基线方法更快达到100%评估成功率，且收敛后性能稳定，无下降。<br><img src="https://arxiv.org/html/2601.15761v1/x3.png" alt="单次设置下的在线学习曲线"></p>
<blockquote>
<p><strong>图4</strong>：单次演示设置下的在线学习曲线。SigEnt-SAC相比其他基线获得了更高的成功率、更快的收敛速度，且收敛后无性能下降。</p>
</blockquote>
</li>
<li><p><strong>对噪声演示的鲁棒性</strong>：在演示轨迹存在数据丢弃、动作噪声或状态噪声的情况下，SigEnt-SAC相比Cal-QL表现出更强的鲁棒性，性能下降更小。<br><img src="https://arxiv.org/html/2601.15761v1/x5.png" alt="对演示质量下降的鲁棒性"></p>
<blockquote>
<p><strong>图6</strong>：在三种演示质量下降情况下的学习性能。SigEnt-SAC对噪声演示 consistently 鲁棒，而Cal-QL性能显著下降。</p>
</blockquote>
</li>
<li><p><strong>真实世界任务性能</strong>：在四个真实世界任务上，SigEnt-SAC仅凭单条演示实现了100%的成功率，显著优于使用30条演示训练的行为克隆模型和零样本VLM智能体（见表2）。此外，学习到的策略平均减少了40.9%的任务完成步数，超越了原始演示的性能。<br><img src="https://arxiv.org/html/2601.15761v1/x4.png" alt="真实世界定性比较与效率提升"></p>
<blockquote>
<p><strong>图5</strong>：(a) Ball-to-Goal任务的定性比较，学习策略比演示更高效。(b) 演示与学习策略完成各任务所需的步数对比，学习策略平均步数减少40.9%。</p>
</blockquote>
</li>
<li><p><strong>超参数敏感性分析</strong>：对门控行为克隆的关键超参数（权重 $\lambda$ 和阈值 $\epsilon$）的分析表明，SigEnt-SAC在一个较宽的参数范围内保持稳定，易于调优。<br><img src="https://arxiv.org/html/2601.15761v1/x6.png" alt="超参数敏感性分析"></p>
<blockquote>
<p><strong>图7</strong>：对门控行为克隆的权重 $\lambda$ 和阈值 $\epsilon$ 的敏感性分析。方法展现了一个宽广的稳定区域。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验总结</strong>：虽然论文未进行严格的模块消融实验，但通过图1、2的理论可视化以及图4、6与基线方法的对比，充分论证了Sigmoid有界熵在稳定Q值估计和减少OOD探索方面的作用，以及结合专家引导（门控行为克隆）在单次演示设置下对加速收敛和提升鲁棒性的贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了SigEnt-SAC，一个仅需单条专家轨迹即可从零开始高效学习的离线策略演员-评论家框架。2) 设计了Sigmoid有界熵机制，有效缓解了保守Q学习中的Q值振荡问题，并防止策略优化至分布外动作。3) 在多个真实世界机器人平台和任务上验证了方法的有效性，展示了其在单目视觉、动态环境下的鲁棒性以及跨不同机器人形态的泛化能力。</p>
<p><strong>局限性</strong>：论文自身提到，引入门控行为克隆项会增加每步训练的计算开销（见表1）。此外，方法性能对门控行为克隆的超参数（如阈值 $\epsilon$ 和权重 $\lambda$）有一定依赖性，尽管其稳定区域较宽。</p>
<p><strong>后续研究启示</strong>：本文为低成本现实世界强化学习指明了一条可行路径，即通过精心设计的学习信号稳定机制（如有界熵）和极少量先验知识（如单条轨迹）的有效利用，可以大幅降低数据需求。未来工作可探索如何进一步自动化超参数选择，或将有界熵的思想与其他稳定RL训练的技术（如价值函数集成、归一化等）更深度地结合，以应对更复杂的多任务或非稳态环境。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现实世界机器人强化学习中数据成本高、训练不稳定的问题，提出SigEnt-SAC方法。其关键技术是sigmoid有界熵，通过将策略的逐维度信息量经sigmoid映射为有界熵信号，防止因负熵主导而导致策略优化偏离分布，并稳定Q函数。实验表明，该方法在D4RL基准上显著减轻了Q值振荡，能更快达到100%成功率；在真实机器人任务中，仅需少量交互即可从原始图像和稀疏奖励中学习成功策略。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.15761" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>