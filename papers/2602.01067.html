<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01067" target="_blank" rel="noreferrer">2602.01067</a></span>
        <span>作者: Jose Barreiros Team</span>
        <span>日期: 2026-02-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通过在大规模多任务机器人数据上进行扩展模仿学习，大型行为模型（LBMs）已展现出强大的灵巧操作能力。然而，其泛化能力仍受限于可用机器人数据的覆盖范围不足。为了在不进行昂贵额外数据收集的情况下扩展覆盖范围，近期研究越来越多地依赖协同训练：联合学习目标机器人数据和异构数据模态。然而，不同的协同训练数据模态和训练策略如何影响策略性能，目前仍缺乏深入理解。本文针对这一痛点，对五种协同训练数据模态（标准视觉-语言数据、机器人轨迹的密集语言标注、跨具身机器人数据、人类视频和离散机器人动作令牌）以及单阶段和多阶段训练策略进行了大规模实证研究。核心思路是：通过系统性的实验，探究不同数据模态和训练策略对视觉-语言-动作（VLA）策略泛化能力的影响，为构建可扩展的通用机器人策略提供实用指导。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架基于一个预训练的视觉-语言模型（VLM）主干网络和一个动作流变换器（Action Flow Transformer）动作头。模型接收一系列图像和文本提示作为输入，通过流匹配（Flow Matching）学习目标预测连续机器人动作，并通过交叉熵损失学习预测离散令牌（文本或动作令牌）。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：数据、模型架构和评估设置概览。策略建立在预训练的视觉-语言模型主干网络和动作流变换器之上。它在目标机器人数据以及异构协同训练模态（包括标准视觉-语言数据、机器人数据的密集语言标注、跨具身机器人数据、人类视频和离散机器人动作令牌）上进行训练。评估在仿真和真实世界中进行，涵盖已见/未见任务、名义条件/分布偏移、语言遵循和长视野灵巧操作。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>预训练VLM主干</strong>：采用PaliGemma2-PT模型初始化，负责编码观测图像和任务语言提示，并可选择性地生成文本或离散动作令牌。</li>
<li><strong>动作流变换器（ActionFT）</strong>：包含8个流变换器层，每个层通过自适应层归一化（adaLN）MLP接收观测特征和流匹配时间步的调节。它接收全局调节嵌入、带噪声的连续动作块和流匹配时间变量，并预测引导动作去噪的流向量。</li>
<li><strong>观测编码令牌</strong>：在主干词汇表中引入一个特殊的观测编码令牌，将其附加到文本提示末尾。从VLM主干最后四层提取该令牌对应的隐藏状态向量，形成一个全局调节嵌入，供ActionFT使用。消融研究表明，这种紧凑表示比使用所有VLM层的注意力键值更能提升模型对未见任务和分布偏移的泛化能力。</li>
</ol>
<p>协同训练策略分为三种：</p>
<ul>
<li><strong>单阶段协同训练</strong>：在单一阶段联合训练目标机器人数据和协同训练数据。</li>
<li><strong>两阶段仅第一阶段协同训练</strong>：第一阶段仅训练协同训练数据，第二阶段仅训练目标机器人连续动作数据。</li>
<li><strong>两阶段完全协同训练</strong>：第一阶段仅训练协同训练数据，第二阶段联合训练协同训练数据和目标机器人数据。</li>
</ul>
<p>此外，研究还探讨了显式思维链（CoT）调节的替代范式：在推理时，VLM主干首先生成从协同训练数据中学到的CoT轨迹，然后将其与观测令牌结合形成视觉语言嵌入，用于调节ActionFT生成连续动作。训练时采用概率性CoT调节。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x2.png" alt="训练数据概览"></p>
<blockquote>
<p><strong>图2</strong>：训练数据概览。数据集包括在仿真和真实世界收集的目标机器人数据，以及五种异构协同训练数据模态：用于常识理解、空间推理和物体定位的标准视觉-语言数据；通过启发式脚本和基于VLM的标注生成的机器人轨迹密集语言标注；捕捉不同机器人形态和环境的跨具身机器人数据；大规模第一人称人类视频；以及通过频率或矢量量化方法离散化的机器人动作令牌。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>研究使用了约4000小时的机器人和人类操作数据以及5000万个视觉-语言样本，训练并比较了89个VLA策略。评估在仿真和真实世界中进行，总计进行了58,000次仿真推演和2,835次真实世界推演。仿真评估基于RoboMimic基准测试，涵盖已见任务、未见任务、名义条件和分布偏移（如背景、光照、物体纹理和姿态变化）场景。真实世界评估侧重于语言遵循和长视野灵巧操作任务。对比的基线包括仅在目标机器人数据上训练的模型，以及结合不同协同训练模态的变体。</p>
<p>关键实验结果如下：</p>
<ol>
<li><strong>不同数据模态的有效性</strong>：协同训练多样化的视觉-语言数据和跨具身机器人数据能持续提升策略对分布偏移、未见任务和语言遵循的泛化能力。而离散动作令牌变体（FAST和VQ-VAE）未带来统计上显著的收益。</li>
<li><strong>模态组合的累积增益</strong>：结合有效的协同训练模态（如VL+LA+XR）能产生累积性能提升。例如，在仿真未见任务的名义条件下，组合模态（VL+LA+XR）的成功率达到68.8%，显著高于仅用机器人数据（53.8%）或单个协同训练模态。</li>
<li><strong>对VLM主干能力的影响</strong>：仅在机器人数据上训练会侵蚀VLM主干的视觉语言理解能力。而有效的协同训练（如VL+LA+XR）有助于保持甚至恢复这种能力，这通过标准视觉-语言、空间推理和多模态推理基准测试的性能提升得到验证。</li>
<li><strong>微调适应性</strong>：在有效的协同训练数据上预训练的模型，能够通过少量微调（50-100个演示）快速适应未见的长视野灵巧任务，成功率显著高于仅在机器人数据上训练的模型。</li>
<li><strong>思维链调节的效果</strong>：在仿真基准测试中，显式地使用从协同训练数据中学到的思维链轨迹来调节动作生成，并未带来性能提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.01067v1/x3.png" alt="仿真评估结果"></p>
<blockquote>
<p><strong>图3</strong>：仿真评估中不同协同训练模态在已见和未见任务上的成功率。展示了名义条件和分布偏移（DS）下的结果。视觉-语言（VL）、语言标注（LA）和跨具身（XR）数据显著提升了性能，而离散动作令牌（DAT）无效。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01067v1/x4.png" alt="真实世界评估结果"></p>
<blockquote>
<p><strong>图4</strong>：真实世界语言遵循任务评估。结合有效协同训练模态（VL+LA+XR）的策略在成功率上显著优于仅用机器人数据训练的基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01067v1/x5.png" alt="VLM主干能力评估"></p>
<blockquote>
<p><strong>图5</strong>：不同训练策略下VLM主干在标准视觉-语言基准测试上的性能。仅在机器人数据上训练严重损害了主干能力，而有效的协同训练（VL+LA+XR）能恢复这些能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01067v1/x6.png" alt="微调适应性结果"></p>
<blockquote>
<p><strong>图6</strong>：在未见长视野灵巧任务上微调后的成功率。使用有效协同训练数据（VL+LA+XR）预训练的模型，仅需少量演示即可达到高成功率，适应速度更快。</p>
</blockquote>
<p>消融实验总结了各组件贡献：紧凑的观测令牌表示、特定的损失权重和数据批次比例对最终性能至关重要。思维链调节在本研究的实验设置中被证明无效。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：</p>
<ol>
<li>通过大规模系统性实验，首次全面评估了五种主流协同训练数据模态和三种训练策略对大型行为模型性能的影响，为领域提供了清晰的实证图谱。</li>
<li>明确了视觉-语言数据和跨具身机器人数据作为有效的协同训练信号，能显著提升策略的泛化能力和VLM主干的知识保留，而离散动作令牌在本研究中未显示优势。</li>
<li>证明了有效协同训练模态的组合能产生累积增益，并能使模型通过少量微调快速适应复杂的未见任务。</li>
</ol>
<p>论文提到的局限性包括：研究主要基于特定的模型架构（PaliGemma2 + ActionFT）和数据集，结论的普适性有待在其他设置中进一步验证；思维链调节未显示效果，但其潜力可能受限于当前CoT数据的质量或模型容量。</p>
<p>对后续研究的启示：应优先考虑整合高质量的视觉-语言知识和多样化的机器人演示数据来增强模型泛化；在设计协同训练策略时，需关注其对基础模型语义和空间理解能力的保护；离散动作表示需要更深入的研究以挖掘其潜在价值；未来的工作可以探索更高效的架构或训练方法来利用异构数据。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文系统研究了如何通过协同训练提升机器人操作大型行为模型的泛化能力。核心问题是解决机器人数据不足导致的泛化局限。研究对比了五种协同训练数据模态（视觉-语言数据、机器人轨迹密集语言标注、跨具身机器人数据、人类视频、离散动作令牌）及单/多阶段训练策略。实验基于4000小时机器人/人类操作数据和5000万视觉-语言样本训练VLA策略，通过数万次仿真与真实环境测试发现：视觉-语言数据与跨具身机器人数据的协同训练能显著提升模型对分布偏移、未见任务和语言指令的泛化能力，而离散动作令牌方案未带来显著收益。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01067" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>