<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01067" target="_blank" rel="noreferrer">2602.01067</a></span>
        <span>作者: Jose Barreiros Team</span>
        <span>日期: 2026-02-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通过在大规模多任务机器人数据上进行训练，大型行为模型（LBMs）已展现出灵巧的操作能力。然而，机器人数据的稀缺性严重限制了这些模型的泛化能力。为了在不增加昂贵数据收集成本的前提下扩展数据覆盖范围，协同训练（co-training）——即联合学习目标机器人数据与异构数据模态——成为了一种有前景的解决方案。现有研究通常只评估了部分数据模态，且实验设置不一致，导致对于不同协同训练数据模态和策略如何影响策略性能，缺乏系统性的理解。</p>
<p>本文针对上述痛点，对五种协同训练数据模态（标准视觉-语言数据、机器人轨迹的密集语言标注、跨具身机器人数据、人类视频、离散机器人动作标记）以及单阶段和多阶段训练策略进行了大规模实证研究。核心思路是：通过大规模、控制变量实验，系统性地评估不同数据模态和训练策略对视觉-语言-动作（VLA）策略在分布偏移、未见任务和语言跟随等维度上性能的影响，为构建可扩展的通用机器人策略提供实证指导。</p>
<h2 id="方法详解">方法详解</h2>
<p>本研究的目标是学习一个能够利用多样化协同训练数据模态的策略πθ。该策略以一系列图像I1:n和文本提示ℓ作为输入。对于连续机器人动作，模型使用流匹配（Flow Matching, FM）作为学习目标。对于文本标记或离散动作标记，则使用交叉熵（CE）损失进行优化。当联合优化连续和离散模态时，损失函数为两者的加权和。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：数据、模型架构和评估设置概览。策略基于预训练的视觉-语言模型（VLM）骨干网络与动作流变换器（Action Flow Transformer）构建。它在目标机器人数据以及异构协同训练模态上训练，并在模拟和现实世界中进行评估。</p>
</blockquote>
<p><strong>模型架构</strong>：采用VLA架构，由预训练的VLM骨干网络（基于PaliGemma2-PT初始化）和动作头（ActionFT）组成。VLM编码观测图像和任务语言提示，并可选择性地生成文本或离散动作标记。为了获得用于动作生成的紧凑表示，在骨干网络的词汇表中引入了一个特殊的观测编码标记，并将其附加到文本提示末尾。从VLM最后四层提取该标记对应的隐藏状态向量，形成一个全局条件嵌入，并馈入ActionFT。ActionFT采用扩散变换器设计，包含8个流变换器层，每层通过自适应层归一化（adaLN）MLP以观测特征和流匹配时间步为条件。它接收全局条件嵌入、带噪声的连续动作块和流匹配时间变量，并预测引导动作迭代去噪的流向量。与主流方法不同，本方法仅使用单个标记作为视觉-语言表示，而非来自所有VLM层的注意力键和值。</p>
<p><strong>协同训练策略</strong>：研究了三种将协同训练数据纳入不同训练阶段（即具有不同数据组成的训练轮次）的策略：</p>
<ol>
<li><strong>单阶段协同训练</strong>：在单一阶段联合训练目标机器人数据和协同训练数据。</li>
<li><strong>两阶段仅第一阶段协同训练</strong>：第一阶段仅在协同训练模态上训练，第二阶段在目标机器人连续动作上训练。</li>
<li><strong>两阶段完全协同训练</strong>：第一阶段与策略2相同，但第二阶段在协同训练数据和目标机器人数据上联合训练。</li>
</ol>
<p>此外，还研究了显式的链式思维（CoT）条件化动作生成范式。在推理时，VLM骨干首先生成从协同训练数据中学到的CoT轨迹，然后将观测标记附加到其末尾，提取由此产生的视觉-语言嵌入（编码图像、任务提示和CoT轨迹）来为ActionFT生成连续动作提供条件。在训练中引入概率性CoT条件化。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x2.png" alt="训练数据概览"></p>
<blockquote>
<p><strong>图2</strong>：训练数据概览。数据集包括目标机器人数据以及五种异构协同训练数据模态，总计约4000小时操作数据和5000万视觉-语言样本。</p>
</blockquote>
<p><strong>数据整理</strong>：研究整理了包含目标机器人专家示范数据和五种不同协同训练模态的综合数据集。</p>
<ol>
<li><strong>目标机器人数据</strong>：采用TRI-Ramen数据集，包含523小时操作数据，涵盖403个任务和53,411条演示。</li>
<li><strong>标准视觉-语言数据</strong>：包含RoboPoint（130万样本）和RefSpatial（250万样本）数据集，用于增强模型的多模态理解。</li>
<li><strong>机器人轨迹的密集语言标注</strong>：通过两种策略为TRI-Ramen轨迹生成每步的文本描述：基于启发式规则的脚本标注和基于VLM（GPT-5）的标注。</li>
<li><strong>跨具身机器人数据</strong>：采用OXE-Ramen数据集，是Open X-Embodiment数据集的子集，包含1,150小时数据，涵盖12种机器人设置和924个任务。</li>
<li><strong>人类视频</strong>：探索两种利用人类视频的方法：一是使用潜在动作模型（LAM）从视频中提取离散的潜在动作标记；二是使用VLM（GPT-5）为视频帧生成语言描述。</li>
<li><strong>离散机器人动作标记</strong>：探索两种形式：使用FAST将连续动作块转换为近乎无损的离散标记序列；使用VQ-VAE将动作块压缩为更紧凑的离散标记。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>研究通过大规模实验评估了89个VLA策略，共进行了58,000次模拟推演和2,835次现实世界推演。评估维度包括：分布内性能、对分布偏移（DS）的鲁棒性、对未见任务的泛化以及语言跟随能力。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x3.png" alt="评估设置"></p>
<blockquote>
<p><strong>图3</strong>：模拟和现实世界评估设置。策略在模拟中于名义条件和分布偏移条件下，在13个已见和8个未见任务上评估。现实世界评估包括语言跟随实验和对未见长视野灵巧任务的微调适应。</p>
</blockquote>
<p><strong>模拟基准结果</strong>：图4至图13展示了不同数据模态和训练策略在模拟基准上的性能。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x4.png" alt="单阶段协同训练结果"></p>
<blockquote>
<p><strong>图4</strong>：单阶段协同训练策略下，不同数据模态在已见任务（名义条件）、已见任务（分布偏移）、未见任务（名义条件）和未见任务（分布偏移）上的平均成功率。结果显示，标准VL数据、密集语言标注和跨具身数据显著提升了在分布偏移和未见任务上的性能，而离散动作标记（FAST, VQ-VAE）和人类视频的潜在动作则没有带来统计显著的收益。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01067v1/x5.png" alt="两阶段协同训练结果"></p>
<blockquote>
<p><strong>图5</strong>：两阶段协同训练策略（第一阶段仅用协同训练数据）的结果。趋势与单阶段类似，标准VL数据、密集语言标注和跨具身数据带来显著提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01067v1/x6.png" alt="两阶段完全协同训练结果"></p>
<blockquote>
<p><strong>图6</strong>：两阶段完全协同训练策略（两阶段都使用协同训练数据）的结果。同样，有效的模态带来了性能提升。</p>
</blockquote>
<p><strong>关键发现</strong>：</p>
<ol>
<li><strong>有效的数据模态</strong>：协同训练各种形式的视觉-语言数据（标准VL数据、密集语言标注）和跨具身机器人数据，能持续提升策略对分布偏移、未见任务和语言跟随的泛化能力。</li>
<li><strong>无效的数据模态</strong>：离散动作标记变体（FAST, VQ-VAE）以及从人类视频中提取的潜在动作标记，未带来统计显著的性能收益。</li>
<li><strong>组合效应</strong>：结合有效的模态（例如，VL + 密集标注 + 跨具身数据）能产生累积性能增益（图7, 8, 9）。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.01067v1/x7.png" alt="模态组合的累积增益"></p>
<blockquote>
<p><strong>图7</strong>：在单阶段协同训练中，结合有效的协同训练模态（VL+DA+XE）能带来累积性能增益，在所有评估维度上均优于单一模态。</p>
</blockquote>
<p><strong>现实世界评估结果</strong>：图10至图13展示了在语言跟随任务上的结果。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x10.png" alt="现实世界语言跟随结果"></p>
<blockquote>
<p><strong>图10</strong>：在现实世界语言跟随任务（已见物体）上，不同协同训练模态和策略的平均任务完成百分比。有效的模态同样提升了语言指令的跟随能力。</p>
</blockquote>
<p><strong>微调适应能力</strong>：研究评估了预训练模型通过微调快速适应未见长视野灵巧任务的能力。图14-16显示，使用有效模态协同训练的策略，在微调后能更快地达到更高的性能。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x14.png" alt="微调适应结果"></p>
<blockquote>
<p><strong>图14</strong>：在未见长视野灵巧任务上微调适应后的性能。使用有效模态（VL+DA+XE）协同训练的预训练模型，微调后性能显著优于仅在机器人数据上训练的基线。</p>
</blockquote>
<p><strong>VLM骨干能力评估</strong>：研究通过标准视觉-语言基准测试评估了协同训练如何影响VLM骨干的视觉-语言理解能力。图17-19显示，仅在机器人数据上训练会损害VLM骨干的原有能力，而与有效模态协同训练则能恢复甚至提升这些能力。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x17.png" alt="VLM骨干基准测试结果"></p>
<blockquote>
<p><strong>图17</strong>：在不同视觉-语言基准测试上的表现。仅在机器人数据上训练（蓝色）导致性能下降，而加入有效的协同训练模态（绿色）能恢复性能。</p>
</blockquote>
<p><strong>链式思维（CoT）条件化</strong>：图20-22显示，在模拟基准中，显式地使用从协同训练数据中学到的CoT轨迹来条件化动作生成，并未带来性能提升。</p>
<p><img src="https://arxiv.org/html/2602.01067v1/x20.png" alt="CoT条件化结果"></p>
<blockquote>
<p><strong>图20</strong>：在单阶段协同训练中，CoT条件化（CoT-DA, CoT-LA）并未带来性能提升。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：</p>
<ol>
<li><strong>系统性实证研究</strong>：首次大规模、控制变量地系统评估了五种主流协同训练数据模态和三种训练策略对VLA策略性能的影响，填补了该领域的知识空白。</li>
<li><strong>明确的效能排序</strong>：明确了标准视觉-语言数据、密集语言标注和跨具身机器人数据能有效提升泛化能力；而离散动作标记和人类视频的潜在动作在当前设置下无效。</li>
<li><strong>揭示骨干网络影响</strong>：揭示了仅使用机器人数据训练会损害VLM骨干的视觉-语言理解能力，而有效的协同训练能保护这种能力，这对模型设计具有重要启示。</li>
</ol>
<p>论文自身提到的局限性包括：显式的链式思维（CoT）条件化在当前的模拟基准中并未显示出性能优势，其潜在价值可能需要更复杂的任务或不同的评估方式来验证。</p>
<p>对后续研究的启示：</p>
<ul>
<li><strong>数据模态选择</strong>：为构建通用机器人策略提供了清晰的数据模态优先级指导，建议优先整合丰富的视觉-语言语义信息和多样化的机器人示范数据。</li>
<li><strong>模型架构设计</strong>：研究采用的紧凑表示（单个观测标记）被证明有利于泛化，这为简化VLA模型架构提供了依据。</li>
<li><strong>评估体系</strong>：论文建立的综合评估框架（涵盖模拟、现实世界、分布偏移、未见任务、语言跟随和微调适应）可作为未来协同训练研究的基准。</li>
<li><strong>探索方向</strong>：尽管显式CoT在本研究中未显效，但如何更有效地利用语言或中间表示进行推理和规划，仍是值得探索的方向。同时，无效模态（如离散动作）为何无效，其背后的原因也需进一步研究。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文系统研究了用于机器人操作的大型行为模型（LBMs）的协同训练问题，旨在解决现有机器人数据覆盖不足导致的泛化能力局限。研究评估了五种协同训练数据模态（标准视觉语言数据、带密集语言标注的机器人轨迹、跨具身机器人数据、人类视频、离散动作令牌）及不同训练策略。核心结论表明，结合视觉语言数据与跨具身机器人数据进行协同训练能显著提升模型对分布偏移、未见任务和语言指令的泛化能力，而离散动作令牌则无显著增益。实验基于4000小时机器人/人类操作数据与5000万视觉语言样本，验证了有效模态的协同训练可恢复并增强视觉语言骨干的理解与推理能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01067" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>