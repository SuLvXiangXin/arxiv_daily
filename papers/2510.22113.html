<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.22113" target="_blank" rel="noreferrer">2510.22113</a></span>
        <span>作者: Yang Ye Team</span>
        <span>日期: 2025-10-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，辅助机器人领域主流的控制接口是基于操纵杆的，但该方法存在精度要求高、参考系不直观、对用户认知和体力负担大等局限性。近年来，基于视线（gaze）的人机交互（HRI）被视为一种有前景的替代方案，能够实现直观的、免手操作。然而，现有的视线控制系统大多依赖于外部屏幕作为交互媒介，使用机器人视角作为操作的主要视点；另一些采用头戴式眼动仪的方法则常常限制用户的头部姿势或依赖额外的光学追踪设备。这些方案限制了交互的直观性和可访问性。</p>
<p>本文针对上述痛点，提出了一种以自我为中心（egocentric）、基于注意力的交互新视角。核心思路是利用可穿戴混合现实（MR）头显，让用户从自然的第一人称视角，通过注视真实世界的物体来直接选择目标，并借助预训练的视觉模型和机械臂完成意图识别与物体操控。</p>
<h2 id="方法详解">方法详解</h2>
<p>RaycastGrasp系统的整体目标是通过用户的自然视线来选择并操控真实物体。其流程是：当用户通过MR头显注视某个物体达到预设时长（2秒）时，系统自动截取头显视角的屏幕截图并记录视线坐标；利用YOLOv8模型识别截图中的物体，同时机器人摄像头也从其视角识别环境中的物体；通过比对两个视角识别出的物体语义标签，确定用户意图操控的目标；最后，机械臂执行相应的抓取操作。</p>
<p><img src="https://arxiv.org/html/2510.22113v1/pipeline.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RaycastGrasp系统概览。以抓取鼠标为例，用户注视目标物体（1），系统捕获截图和视线坐标（2），YOLOv8模型从用户视角识别物体（3），同时机器人摄像头也从其视角进行识别（4），通过语义标签映射确认共同目标（5），最终机器人执行抓取（6）。</p>
</blockquote>
<p>系统包含三个核心模块：</p>
<ol>
<li><strong>视线追踪</strong>：为实现用户在MR穿透式（passthrough）环境中的视线投射（raycast），系统利用MR头显内置的眼动仪捕获视线方向，并将其投射到头显渲染的虚拟穿透层上。该虚拟层与物理空间精确对齐。在此之后，场景中构建了一个对应的碰撞平面（物理层），作为视线投射的实际目标表面。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.22113v1/fffffff.png" alt="视线追踪对齐"></p>
<blockquote>
<p><strong>图2</strong>：虚拟穿透层与物理层的对齐示意图。用户的视线方向被投射到虚拟层，并进一步映射到后方的物理碰撞平面，以确定在真实世界中的注视点。</p>
</blockquote>
<p>当用户注视物体时，系统记录视线方向为一个三维法向量 <strong>n</strong>，并通过射线投射在虚拟画布上得到命中点 <strong>x</strong>，其对应屏幕像素坐标 (u, v)。由于截图导出到主机后的分辨率可能与头显原始分辨率不同，系统会将视线坐标按比例缩放至处理图像的坐标系 (u’, v’)，确保坐标一致性。</p>
<ol start="2">
<li><p><strong>物体识别</strong>：系统采用YOLOv8物体检测框架。为了验证概念，作者构建了一个包含常见桌面物体（如鼠标、笔、瓶子、杯子、胶带）的自定义数据集，并包含了模拟机器人视角的俯视图和模拟用户MR视角的侧视图，以提高模型在不同视角下的鲁棒性。训练好的模型用于定位系统内的物体。每个检测到的物体由一个矩形边界框表示。系统通过判断缩放后的视线像素坐标 (u’, v’) 是否位于某个边界框内，来确定用户注视的是哪个物体，并为其分配对应的语义标签 L_gaze。</p>
</li>
<li><p><strong>语义映射与机器人操控</strong>：此模块旨在将用户视线推断出的语义意图与机器人对环境的空间感知对齐。当用户注视触发（≥2秒）后，系统同时获得用户视角的截图和机器人视角的图像。两个图像分别通过YOLOv8模型进行识别，产生带有类别标签（如“鼠标”、“笔”）的检测结果。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.22113v1/mapping.png" alt="语义映射"></p>
<blockquote>
<p><strong>图3</strong>：用户视线检测与机器人摄像头识别之间的类别标签映射过程。系统比对用户选择的物体标签与机器人场景中检测到的标签集合，当找到匹配项时，即确定该物体为抓取目标。</p>
</blockquote>
<p>映射过程比较用户选择的类别标签与机器人场景中检测到的所有物体标签。当找到匹配项时（例如，用户选择了“鼠标”，机器人也检测到了“鼠标”），系统即判定该物体为用户意图抓取的目标，并指令机械臂执行抓取。</p>
<p><strong>创新点</strong>：与现有大多数基于机器人视角图像进行二次视线分析的方法不同，本文提出的系统实现了真正的第一人称MR交互，允许用户从其自然视点直接选择真实世界目标，这更符合人类直观的交互模式。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与设置</strong>：实验使用Franka Emika协作机械臂、HTC Focus Vision MR头显、一台配置Windows和Linux双系统的计算机。实验场景为一个60cm x 90cm的矩形桌子，上面随机摆放水瓶、笔、胶带、鼠标等常见物体。</p>
<p><img src="https://arxiv.org/html/2510.22113v1/Picture1.png" alt="用户视角"></p>
<blockquote>
<p><strong>图4</strong>：用户在混合现实环境中通过VR头显观察真实物体。用户佩戴MR头显，以第一人称视角观看桌面上的物体。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.22113v1/Picture2.png" alt="机器人视角"></p>
<blockquote>
<p><strong>图5</strong>：机器人搭载摄像头视角下的物体视图。展示了机械臂摄像头所看到的桌面场景，用于从机器人视角进行物体识别。</p>
</blockquote>
<p><strong>实验流程</strong>：用户佩戴MR头显坐在桌前，注视任意物体超过2秒并按下指定按钮，系统随即捕获截图、记录坐标并启动识别与抓取流程。每位参与者独立完成多次物体选择与抓取任务。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>视线追踪精度</strong>：视线追踪系统的空间精度达到0.05米。当用户的视线在0.05米区域内保持至少2秒时，系统能可靠输出对应坐标。</li>
<li><strong>物体识别准确率</strong>：YOLOv8模型在图像识别中 consistently 提供超过88%的物体检测置信度（即单次意图和物体识别准确率 &gt; 88%）。</li>
<li><strong>机器人性能</strong>：Franka Emika机械臂表现出稳健的抓取性能，成功完成了所有测试物体的抓取-放置任务。</li>
</ol>
<p><strong>消融实验分析</strong>：论文未进行严格的模块消融实验，但在讨论部分指出了当前映射策略的局限性，这间接反映了各组件的重要性。系统高度依赖于精确的视线追踪、鲁棒的多视角物体识别以及准确的语义标签匹配，任一环节的失败都可能导致任务失败。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出并实现了一个基于可穿戴MR头显的、以第一人称视角为中心的视线引导机器人操控系统（RaycastGrasp），实现了用户与真实物体的直接、直观交互。</li>
<li>设计了一套结合视线投射、坐标转换、物体识别和语义标签映射的完整流程，将用户意图与机器人感知桥接起来，简化了匹配过程。</li>
<li>通过实验验证了系统的可行性，展示了高精度的视线追踪（0.05米）、高置信度的物体识别（&gt;88%）以及稳定的机器人抓取性能。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>语义映射的歧义性</strong>：在杂乱或复杂环境中，当出现多个同类物体或物体边界框重叠时，当前基于类别标签的映射策略可能无法准确推断用户的真实意图，导致抓取错误。</li>
<li><strong>固定的注视触发时间</strong>：使用固定的2秒驻留时间作为激活阈值，虽然有助于减少误触发，但可能在需要快速响应的任务中引入延迟，缺乏灵活性。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>未来工作可以探索更精细的空间表示（如实例级分割）或时域视线模式分析，以解决复杂环境下的意图消歧问题。</li>
<li>可以考虑基于视线稳定性或上下文感知的自适应阈值机制，以平衡交互的准确性与响应速度。</li>
<li>该系统框架是轻量级且可扩展的，为辅助机器人应用提供了新的交互范式，可进一步扩展到更复杂的场景和任务中。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对传统机器人操纵杆控制精度要求高、参考框架不直观、用户负担重的问题，提出RaycastGrasp系统，利用可穿戴混合现实（MR）头显实现以自我为中心的注视交互。关键技术包括：基于自然注视固定的物体选择、增强视觉提示确认意图，以及集成预训练视觉模型和机器人手臂进行意图识别与操作。实验表明，该系统显著提升操作准确性、降低系统延迟，在多个真实场景中单次意图和物体识别准确率超过88%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.22113" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>