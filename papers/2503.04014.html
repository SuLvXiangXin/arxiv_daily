<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Dexterous Grasping with Real-World Robotic Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Dexterous Grasping with Real-World Robotic Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.04014" target="_blank" rel="noreferrer">2503.04014</a></span>
        <span>作者: Huang, Dongchi, Zhang, Tianle, Li, Yihang, Zhao, Ling, Li, Jiayi, Fang, Zhirui, Xia, Chunhe, He, Xiaodong</span>
        <span>日期: 2025/03/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于学习的灵巧抓取方法主要分为两种范式：级联范式（先生成抓取姿态再通过运动规划执行）和端到端范式（直接学习整个抓取过程）。这些方法大多在模拟器中训练，面临着模拟与现实之间的领域差距问题，限制了其在现实世界中的泛化性和实用性。模仿学习虽可直接从示教中学习以避免该差距，但其泛化能力往往受限于训练数据。本文针对“能否通过在现实世界中直接进行强化学习来学习灵巧抓取策略？”这一具体痛点，提出了一种直接在现实世界环境中训练机器人获得灵巧抓取技能的强化学习框架。本文的核心思路是：通过模仿学习预训练策略以解决探索难题，随后在现实世界中进行在线强化学习微调，并引入自适应正则化机制来平衡策略探索与对预训练知识的保留，以防止灾难性遗忘。</p>
<h2 id="方法详解">方法详解</h2>
<p>DexGraspRL框架包含两个关键组成部分：一个确保自动化和效率的系统，以及一个保证样本高效学习的强化学习算法。整体流程分为两个阶段：首先使用有限专家示教通过模仿学习进行策略预训练；随后在现实场景中通过直接强化学习对策略进行微调。</p>
<p><img src="https://arxiv.org/html/2503.04014v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：DexGraspRL系统概述。系统架构包含执行器（Actor）和学习器（Learner）两个组件，它们通过异步通信实现高效训练。执行器负责与机器人控制器交互并收集数据，学习器负责更新策略。</p>
</blockquote>
<p><strong>系统设计</strong>：</p>
<ol>
<li><strong>奖励指定与二元分类器</strong>：由于为灵巧抓取设计显式奖励函数非常困难，本文通过训练一个二元分类器来预测奖励。首先通过遥操作收集成功任务的示教轨迹，并自动标注成功帧；然后利用这些标注数据训练一个基于图像观察的二元分类器，该分类器在强化学习阶段作为奖励检测器。</li>
<li><strong>高性能机器人控制器</strong>：采用基于gRPC（而非HTTP）的控制器，以降低延迟并提高接收动作和返回观测的稳定性。</li>
<li><strong>分布式架构</strong>：如图2所示，将执行器和学习器分离到两个独立线程。执行器管理机器人控制器，根据当前观测推断并执行动作，同时将收集到的轨迹发送至在线回放缓冲区；学习器则管理在线回放缓冲区和示教回放缓冲区，从中采样来更新策略，并定期更新执行器的策略。当一个回合被分类为成功时，它会被视为高质量回合并转移到示教回放缓冲区。</li>
</ol>
<p><strong>学习算法</strong>：<br>算法分为两个阶段：</p>
<ol>
<li><strong>BC预训练</strong>：为解决从零开始进行现实世界RL面临的探索和样本效率挑战，首先通过行为克隆模仿专家示教来预训练策略。损失函数为：ℒ_BC = 𝔼_(s^e, a^e)∼τ^e [‖a^e - π_BC(s^e)‖²]，其中τ^e代表专家示教。</li>
<li><strong>正则化RL微调</strong>：直接对预训练策略应用RL可能导致灾难性遗忘。为此，本文在策略目标中引入了一个正则化项，以自适应地平衡价值最大化与对预训练策略的保持。修改后的RL目标如公式(2)所示：ℒ_RL = (1-λ(π)) 𝔼_(s,a)∼𝒟<em>β [Q(s,a)] - αλ(π) 𝔼</em>(s^e,a^e)∼τ^e ‖a^e - π(s^e)‖²。其中，λ(π)是一个自适应的平衡系数，其计算方式为：λ(π) = 𝔼_(s,·)∼𝒟<em>β [𝕀</em>{Q(s, π_BC(s)) &gt; Q(s, π(s))}]，即评估预训练策略在当前状态下是否优于正在学习的策略。λ(π)值越高，表明应更注重正则化以保留预训练知识。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，DexGraspRL的创新主要体现在：1）构建了端到端的、直接在现实世界中进行训练的灵巧抓取RL框架；2）提出了结合模仿学习预训练和自适应正则化微调的两阶段算法，有效解决了高自由度灵巧手在稀疏奖励下的探索难题和灾难性遗忘问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>硬件与平台</strong>：使用Inspire机器人及其6自由度灵巧手，观测空间包括机器人胸部和头部的摄像头图像以及本体感觉信息（末端执行器位姿、手指角度），动作空间对应观测空间中的本体感觉信息。</li>
<li><strong>任务与数据集</strong>：设计了四项具有不同几何形状和属性的现实世界抓取任务：抓取杯子（Cup）、抓取扫描仪（Scanner）、捏起立方体（Cube）、抓取环形玩偶（Loopy）。为每项任务收集了30-40条专家示教轨迹（见表I）。</li>
<li><strong>基线方法</strong>：对比了行为克隆（BC）、样本高效机器人强化学习（SERL）、动作分块Transformer（ACT）以及本文方法的不带正则化消融版本（DexGraspRL†）。</li>
</ul>
<p><img src="https://arxiv.org/html/2503.04014v2/x4.png" alt="任务图示"></p>
<blockquote>
<p><strong>图4</strong>：机器人使用本文方法执行各项任务的示意图。每项任务中，机器人从预定义的初始姿势开始，利用本体感觉状态和图像观察执行任务，直到成功获得奖励（绿色框内所示）。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>表II展示了主要定量结果。DexGraspRL在所有任务上均取得了有竞争力的性能，平均成功率高达91.9%，平均任务完成周期（CT）为110.45步。</p>
<ul>
<li>与BC相比：DexGraspRL在成功率上显著超越（平均提升30.5%），尤其是在需要精确手指控制的Cube任务上，成功率从7.5%提升至90%，并且平均CT减少了约23%。这表明通过RL的探索可以优化并超越专家策略。</li>
<li>与SERL相比：专注于现实世界RL的SERL方法由于灵巧手动作空间大、图像观测维度高导致的巨大探索空间，在所有任务上均失败（成功率为0），凸显了预训练的重要性。</li>
<li>消融分析（DexGraspRL†）：移除正则化项后，除了对精确性要求不高的Loopy任务外，其余三项任务均失败。与BC相比，平均成功率下降了36.4%，这直接证明了正则化机制对于防止灾难性遗忘、保持策略稳定性的关键作用。</li>
</ul>
<p><img src="https://arxiv.org/html/2503.04014v2/x3.png" alt="机器人硬件"></p>
<blockquote>
<p><strong>图3</strong>：实验中使用的机器人示意图。机器人任务是抓取杯子，利用来自ZED2和Realsense D435摄像头的图像观察作为输入，控制其手臂和手完成任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.04014v2/x1.png" alt="性能对比"></p>
<blockquote>
<p><strong>图1</strong>：BC、SERL、ACT和本文DexGraspRL在现实世界任务上的性能对比。直观展示了DexGraspRL在成功率和效率上的优势。</p>
</blockquote>
<p><strong>与ACT的对比</strong>：<br>表III进一步比较了与先进IL方法ACT的结果。DexGraspRL在平均成功率（91.9% vs 85.0%）和平均效率（110.45步 vs 91.25步）上均优于ACT，尤其是在Scanner和Cube等更具挑战性的任务上优势明显，证明了通过现实世界RL微调能够发现比单纯模仿学习更优的策略。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了DexGraspRL，一个专门为灵巧抓取设计的、直接在现实世界中进行训练的强化学习框架，为在现实世界中获取灵巧手技能开辟了新途径。</li>
<li>提出了一种高效的现实世界RL算法，该算法利用模仿学习解决灵巧手巨大配置空间带来的探索挑战，并通过自适应正则化机制平衡RL探索与预训练策略的保持。</li>
<li>实验证明，该方法能成功执行多种不同几何和属性的现实世界任务，并通过RL微调发现了超越模仿学习专家策略的新策略，展示了RL超越人类专家能力的潜力。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前方法主要处理桌面场景下的抓取任务，任务的复杂性（如涉及动态环境或更复杂的操作序列）和多样性仍有待扩展。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>框架的通用性</strong>：DexGraspRL的系统设计（如基于分类器的奖励指定、分布式架构）和算法思想（预训练+正则化微调）可为其他复杂的现实世界机器人操作任务提供参考。</li>
<li><strong>探索与利用的平衡</strong>：自适应正则化系数λ(π)的设计提供了一种原则性的方法来解决离线或预训练策略在线微调时的灾难性遗忘问题，值得在其他混合学习范式中进一步探索和优化。</li>
<li><strong>迈向通用抓取</strong>：该方法展示了直接从现实世界交互中学习复杂技能的可行性，鼓励后续研究增加任务和对象的多样性，并探索如何将学习到的技能组合或迁移到更广泛的操纵任务中。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对真实世界中机器人灵巧抓取的挑战，提出DexGraspRL强化学习框架。该方法通过模仿学习预训练策略，再在真实环境中进行强化学习微调，并设计正则化项以缓解灾难性遗忘。实验表明，该框架在真实任务中平均成功率接近92%，且经过强化学习微调后，平均操作周期时间较模仿学习策略减少23%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.04014" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>