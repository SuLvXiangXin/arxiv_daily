<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.15279" target="_blank" rel="noreferrer">2511.15279</a></span>
        <span>作者: Wenzhao Lian Team</span>
        <span>日期: 2025-11-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在具身AI感知系统中，视觉感知应当是主动的：目标不是被动处理静态图像，而是在像素和空间预算约束内主动获取更具信息量的数据。现有的视觉模型和固定的RGB-D相机系统从根本上无法协调宽区域覆盖与细粒度细节获取，严重限制了其在开放世界机器人应用中的效能。尽管视觉语言模型（VLMs）在零样本识别、指令跟随等方面取得显著进展，但主流VLM范式仍假设输入是被动提供、信息充足的静态图像；它们不决定看向何处、以何种尺度感知，或如何自适应地重新分配有限的像素、时间和传感资源。然而，机器人感知是一个序列化的、资源受限的决策过程。固定摄像头与强大VLM骨干的结合，仍然交织着信息冗余（大面积低价值背景）和关键细节遗漏（小型、任务决定性结构），导致在操作瞄准、检查等精度敏感任务上表现不佳。</p>
<p>本文针对语义推理能力超越感知行动的“结构性鸿沟”，提出将机器人中语言条件化的视觉感知从被动推断重新定义为语言引导的主动获取，将“回答所见内容”转变为“决定接下来看什么以减少任务相关的不确定性”。核心思路是提出EyeVLA，一个语言引导的主动视觉框架，将视觉、语言和具身相机控制（平移、倾斜、缩放）统一在一个单一的自回归序列模型中，通过离散化低级感觉运动调整、将其与视觉和文本令牌交错，并施加位置反馈和强化学习约束，以极少量的真实世界数据将预训练的被动VLM转化为视觉-语言-动作（VLA）智能体。</p>
<h2 id="方法详解">方法详解</h2>
<p>EyeVLA系统的目标是根据自然语言指令（如“笔的品牌是什么？”），从初始RGB图像出发，自回归地预测一系列动作令牌，解码为相机控制命令（水平旋转调整Δθ₁、垂直旋转调整Δθ₂、焦距缩放调整Δzoom），驱动“机器人眼球”在3D空间中自主调整视点，使最终的视觉状态与指令表达的语义意图对齐。</p>
<p><img src="https://arxiv.org/html/2511.15279v1/x1.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：EyeVLA流程总览。系统基于Qwen2.5-VL框架构建，集成了视觉感知、语言理解和动作生成能力。为保持训练期间原有的语义对齐，ViT及其投影器模块的参数被冻结不更新。此外，词汇表中引入了代表相机运动的动作令牌，并采用分层编码策略来结构化地建模动作空间。</p>
</blockquote>
<p><strong>核心模块1：分层动作编码与解码</strong><br>为在有限上下文长度内紧凑、令牌高效地表示连续的相机动作，论文提出了分层动作编码方案。对于一个目标角度值x（非负整数），首先将其分解为十进制数字（如个位、十位）。然后，对每个数位上的数字x^(ℓ)，使用基元集合D={5, 2, 1}进行线性组合表示（例如，7 = 5 + 2），目标是使使用的令牌数量最少。这是一个经典的找零问题，而基元集合D是一个规范硬币系统，贪心算法能保证为每个0-9的数字找到全局最优（令牌数最少）的表示。缩放调整值也采用类似方式编码。这种设计的优势在于：理论上的令牌使用最优性、常数时间的编解码复杂度，以及与物理相机控制的结构对齐（高位数字对应粗调，低位数字对应微调）。在500个真实世界样本上的实证分析表明，98.6%的动作落在±29°内，平均令牌长度仅为2.3，远优于每度一令牌的均匀离散化方案（平均长度12.7）。</p>
<p><strong>核心模块2：合成数据生成器</strong><br>由于手动收集真实世界数据成本极高，为赋予模型在开放世界中的零样本能力，论文设计了一个合成数据生成流程。首先，手动收集少量（500个）真实示范数据，包含（指令，初始帧，动作，动作后帧）元组。然后，利用现有的 grounding 数据集（如Rexverse-2M）进行数据合成。具体流程是：选取包含小目标的图像及其标注的边界框（bbox）；将bbox中心坐标归一化后映射为模拟的平移角度（偏航、俯仰）；利用bbox面积占图像总面积的比例变化来建模缩放操作；使用在少量真实数据上训练的随机森林模型学习从bbox信息到动作值的映射关系，进而为大量合成图像生成伪标签的动作值；最后，通过分层动作词汇表，用贪心算法将动作值编码为离散的动作令牌组合。</p>
<p><strong>核心模块3：两阶段训练策略</strong><br>训练分为两个主要阶段：监督微调（SFT）阶段和强化学习（RL）策略精炼阶段。</p>
<ul>
<li><strong>阶段1：基于伪标签的监督对齐</strong>：使用生成的伪标签样本对扩展了词汇表的VLA模型进行监督微调。此阶段主要目标是实现模型输出空间与物理有意义动作语义之间的低成本高效对齐。为保留预训练获得的视觉表征和多模态对齐能力，训练时冻结视觉Transformer编码器和视觉-语言投影器的参数，仅更新语言模型骨干和新引入的动作令牌嵌入。</li>
<li><strong>阶段2：通过强化学习的策略精炼</strong>：在初始对齐后，为纠正伪标签的潜在偏差并提高模型泛化能力，采用强化学习优化策略。具体使用分组相对策略优化（GRPO）方法，其目标函数（公式4）结合了裁剪后的概率比与优势函数，并包含与参考策略（初始VLM）的KL散度惩罚项，以稳定训练。奖励函数R（公式6）是边界框预测IoU以及三个动作输出与真值绝对差值的加权组合，鼓励模型做出精确预测。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，EyeVLA的核心创新在于：1) 将动作生成与信息理解统一在单一自回归流中，而非附加独立的动作头，保留了语义丰富性；2) 提出了理论最优且高效的分层动作编码方案；3) 设计了基于大量合成伪标签和少量真实数据的高效训练流程，并利用基于IoU的奖励机制在RL阶段引导策略优化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集与平台</strong>：构建了一个由两轴云台和变焦相机组成的机器人眼球系统，收集了500个真实世界数据样本（450训练/50测试）。为数据增强，从Rexverse-2M数据集中提取5万张小目标图像生成合成数据。</li>
<li><strong>对比方法</strong>：包括机器学习基线（随机森林）、不同阶段的监督微调模型（SFT1, SFT2, SFT3，区分是否使用IoU过滤样本）、以及在不同SFT阶段基础上进行强化学习训练的模型（RL2, RL3）。</li>
<li><strong>评估指标</strong>：1) 定量指标：预测调整量（Δθ₁, Δθ₂, Δzoom）的平均绝对误差（MAE）；2) 现实场景评估：在50个真实场景中的任务完成率（CR）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2511.15279v1/x2.png" alt="定量结果对比"></p>
<blockquote>
<p><strong>图3</strong>：在不同策略和迭代次数下生成的合成数据上训练的模型推理结果，及其在现实场景中的性能比较。图示场景目标是识别盒子内一支笔的品牌。传统固定视角相机无法做到，而EyeVLA系统通过动态调整相机姿态和变焦实现了对目标的清晰观察。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.15279v1/x4.png" alt="训练过程性能对比"></p>
<blockquote>
<p><strong>图5</strong>：三轮SFT结果对比。展示了模型性能随训练阶段（SFT1, SFT2, SFT3）的迭代而逐步提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.15279v1/x5.png" alt="随机森林拟合关系"></p>
<blockquote>
<p><strong>图6</strong>：随机森林在真实数据上学习的从边界框信息到动作值的映射关系拟合性能。拟合效果良好，呈现强线性对应关系，适用于生成伪数据。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.15279v1/x6.png" alt="IoU策略消融"></p>
<blockquote>
<p><strong>图7</strong>：训练期间使用与不使用边界框替换策略的推理结果，与真实标注的IoU分数对比。证明了高质量伪标签对性能提升的重要性。</p>
</blockquote>
<p><strong>主要结果总结</strong>：<br>根据表1的定量结果，经过三轮SFT和RL精炼，模型性能持续提升。最终RL3模型在测试集上达到：Δθ₁误差2.04°，Δθ₂误差1.68°，Zoom误差65.37（约对应0.73倍缩放），任务完成率高达96%。相比最初的机器学习基线（CR 36%）和SFT1（CR 36%），性能提升显著。消融实验表明，在SFT阶段使用IoU过滤样本（Y）比不使用（N）能获得更低的误差和更高的完成率，验证了高质量伪标签的重要性。此外，实验发现直接将连续动作值作为输出进行SFT训练无法收敛，从而验证了离散动作令牌编码的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了统一的VLA框架</strong>：将相机运动和缩放控制表述为离散的、令牌化的决策过程，并与多模态推理无缝集成和联合优化，将视觉感知从被动帧消费转变为闭环、证据驱动、任务感知的主动获取范式。</li>
<li><strong>设计了分层动作编码方案</strong>：将平移/倾斜/缩放调整分层离散化并嵌入VLM词汇表，实现了图像、语言和动作的统一自回归建模，无需单独的控制头，且编码效率理论最优。</li>
<li><strong>实现了高效的数据与训练流程</strong>：通过结合合成伪标签生成、基于IoU的样本过滤、以及两阶段（SFT+RL）训练策略，仅用500个真实世界样本和伪标签扩展，就成功地将预训练VLM的开放世界语义理解能力迁移到了细粒度的主动控制策略上。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，动作空间的离散化可能对需要非常精细、连续控制的应用构成限制。此外，方法依赖于初始的边界框标注来生成伪标签和设计奖励函数。</p>
<p><strong>对后续研究的启示</strong>：EyeVLA展示了一条实用路径，即基于现有大规模预训练模型，通过高效的算法设计和极少量的真实交互数据，实现语言引导的主动感知与控制。其“统一自回归序列建模”的核心思想，以及“合成数据+RL精炼”的训练范式，为在更多机器人形态和控制维度上实现低开销的具身智能提供了借鉴。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身AI感知中现有视觉系统被动、无法兼顾宽区域覆盖与细粒度细节获取的核心问题，提出EyeVLA机器人眼球。方法将旋转、缩放等动作离散为动作令牌，与视觉语言模型（VLM）集成，实现视觉、语言和动作的联合建模；通过2D边界框坐标引导推理链，并应用强化学习优化视点选择策略，仅用少量真实数据将VLM能力迁移为视觉语言动作（VLA）策略。实验表明，EyeVLA能根据指令主动旋转和缩放，有效理解真实环境场景并获取更准确的视觉信息，提升了环境感知能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.15279" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>