<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GR00T N1: An Open Foundation Model for Generalist Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>GR00T N1: An Open Foundation Model for Generalist Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.14734" target="_blank" rel="noreferrer">2503.14734</a></span>
        <span>作者: NVIDIA, :, Bjorck, Johan, Castañeda, Fernando, Cherniadev, Nikita, Da, Xingye, Ding, Runyu, Fan, Linxi &#34;Jim&#34;, Fang, Yu, Fox, Dieter, Hu, Fengyuan, Huang, Spencer, Jang, Joel, Jiang, Zhenyu, Kautz, Jan, Kundalia, Kaushil, Lao, Lawrence, Li, Zhiqi, Lin, Zongyu, Lin, Kevin, Liu, Guilin, Llontop, Edith, Magne, Loic, Mandlekar, Ajay, Narayan, Avnish, Nasiriany, Soroush, Reed, Scott, Tan, You Liang, Wang, Guanzhi, Wang, Zu, Wang, Jing, Wang, Qi, Xiang, Jiannan, Xie, Yuqi, Xu, Yinzhen, Xu, Zhenjia, Ye, Seonghyeon, Yu, Zhiding, Zhang, Ao, Zhang, Hao, Zhao, Yizhou, Zheng, Ruijie, Zhu, Yuke</span>
        <span>日期: 2025/03/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>构建能够在人类世界中执行日常任务的自主机器人是一个长期目标，也是一个重大的技术挑战。近年来，机器人硬件、人工智能和加速计算的进展为开发通用机器人自主性奠定了基础。实现这一目标需要一个集成硬件、模型和数据的全栈解决方案。其中，人形机器人因其类人的物理形态和多功能性，成为构建机器人智能的理想硬件平台。</p>
<p>然而，训练通用机器人模型面临核心挑战：数据稀缺。与文本和图像领域不同，不存在用于大规模预训练的“人形机器人互联网数据集”。任何单一硬件的数据量都远远不足。尽管机器人学习社区尝试通过跨具身学习汇集不同机器人的数据来扩大数据集，但机器人形态、传感器、执行器自由度、控制模式等方面的巨大差异导致了“数据孤岛”问题，而非训练真正通用模型所需的连贯、互联网规模的数据集。</p>
<p>本文针对上述“数据孤岛”和训练数据稀缺的核心痛点，提出了GR00T N1，一个面向通用人形机器人的开放基础模型。其核心思路是：构建一个采用双系统架构的视觉-语言-动作模型，并通过一个异构的“数据金字塔”策略，整合网络数据、人类视频、合成数据与真实机器人轨迹进行协同训练，以解决数据稀缺并实现跨具身泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>GR00T N1是一个视觉-语言-动作模型，其整体框架采用受人类认知处理启发的双系统组合架构。模型输入为图像观测和语言指令，输出为电机动作。</p>
<p><img src="https://arxiv.org/html/2503.14734v2/x2.png" alt="模型总览"></p>
<blockquote>
<p><strong>图2</strong>：GR00T N1模型概览。模型采用双系统设计，将图像观测和语言指令转换为令牌序列，由视觉语言模型主干处理。VLM的输出与机器人状态和动作编码一起传递给扩散变换器模块以生成电机动作。</p>
</blockquote>
<p><strong>核心模块1：视觉语言模块</strong>。作为<strong>System 2</strong>推理模块，采用在互联网规模数据上预训练的Eagle-2 VLM。图像以224×224分辨率编码，通过像素重排得到每帧64个图像令牌嵌入，这些嵌入与文本一起由Eagle-2的LLM组件进一步编码。在策略训练时，任务文本描述和（可能多张）图像以VLM训练时使用的聊天格式输入。模型使用LLM中间层（GR00T-N1-2B使用第12层）而非最终层的嵌入作为视觉语言特征，这带来了更快的推理速度和更高的下游策略成功率。该模块在NVIDIA L40 GPU上以10Hz频率运行。</p>
<p><strong>核心模块2：扩散变换器模块</strong>。作为<strong>System 1</strong>动作模块，采用基于流匹配训练的扩散变换器变体。它通过交叉注意力机制以VLM输出的令牌嵌入为条件，并采用具身特定的编码器和解码器来处理可变的状态和动作维度，以生成运动。该模块以更高频率（120Hz）生成闭环电机动作。</p>
<p><strong>状态与动作编码</strong>：为处理不同机器人形态的状态和动作，模型为每个形态使用一个MLP，将其投影到共享的嵌入维度作为DiT的输入。动作编码器MLP还将扩散时间步与加噪后的动作向量一起编码。动作以块为单位处理，块大小H=16。</p>
<p><strong>模型训练与推理</strong>：模型使用动作流匹配进行训练。给定真实动作块A_t、流匹配时间步τ∈[0,1]和采样噪声ε，计算加噪动作块A_t^τ = τA_t + (1-τ)ε。模型预测V_θ(φ_t, A_t^τ, q_t)旨在通过最小化损失函数来近似去噪向量场ε - A_t。推理时，通过K步去噪生成动作块，采用前向欧拉积分进行迭代更新，实践中发现K=4在所有形态上效果良好。</p>
<p><img src="https://arxiv.org/html/2503.14734v2/x3.png" alt="模型架构"></p>
<blockquote>
<p><strong>图3</strong>：GR00T N1模型架构。模型在从单臂机器人到双手人形灵巧手的一系列不同形态上进行训练。使用具有具身感知的状态和动作编码器的DiT块来嵌入机器人的状态和动作输入。模型利用Eagle-2模型的潜在嵌入来整合机器人的视觉观测和语言指令。视觉语言令牌随后通过交叉注意力层馈入DiT块。</p>
</blockquote>
<p><strong>创新点</strong>：1) <strong>双系统架构</strong>：将VLM的慢速、深思熟虑的推理（System 2）与DiT的快速、流畅的动作生成（System 1）紧密结合，并端到端联合训练。2) <strong>数据金字塔训练策略</strong>：将异构训练语料库组织成一个金字塔结构，底层是大规模网络数据和人类视频，中层是物理模拟和/或现成神经模型增强生成的合成数据，顶层是物理机器人硬件上收集的真实世界数据。</p>
<p><strong>训练数据生成关键技术</strong>：</p>
<ol>
<li><p><strong>潜在动作</strong>：对于人类自我中心视频和神经轨迹等无动作数据源，训练一个VQ-VAE模型从连续视频帧中提取特征。编码器以当前帧x_t和未来帧x_{t+H}为输入，输出潜在动作z_t。使用编码器作为逆动力学模型，提取连续预量化嵌入作为预训练期间的潜在动作标签。<br><img src="https://arxiv.org/html/2503.14734v2/x4.png" alt="潜在动作"></p>
<blockquote>
<p><strong>图4</strong>：潜在动作。我们在各种形态中检索相似的潜在嵌入。左图展示了对应于将右臂（或手）向左移动的潜在动作，右图展示了对应于将右臂（或手）向右移动的潜在动作。</p>
</blockquote>
</li>
<li><p><strong>神经轨迹</strong>：利用视频生成模型，对内部收集的88小时遥操作数据进行微调，并基于现有初始帧和新语言提示生成827小时视频数据，将数据量增加了约10倍。这能生成捕捉更多反事实场景的训练数据。<br><img src="https://arxiv.org/html/2503.14734v2/x5.png" alt="合成生成视频"></p>
<blockquote>
<p><strong>图5</strong>：合成生成的视频。我们利用现成的视频生成模型创建神经轨迹，以增加训练数据集的数量和多样性。这些生成的数据可用于GR00T N1的预训练和后训练。</p>
</blockquote>
</li>
<li><p><strong>模拟轨迹</strong>：使用DexMimicGen自动合成大规模机器人操作轨迹。它将有限的人类演示通过演示转换和模拟回放进行扩展，生成了780,000条模拟轨迹（相当于6,500小时数据）。</p>
</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在标准模拟基准环境（包括MetaWorld、RLBench、ManiSkill2、Dexterity Benchmark）上评估GR00T N1。对比的基线方法包括最先进的模仿学习方法，如ACT、Diffusion Policy、HULC、VIMA、RT-1、RT-2。此外，还在Fourier GR-1人形机器人上进行了语言条件双手操作任务的真实世界实验。</p>
<p><strong>模拟基准结果</strong>：GR00T N1在多个基准测试中显著优于基线方法。<br><img src="https://arxiv.org/html/2503.14734v2/x6.png" alt="模拟结果汇总"></p>
<blockquote>
<p><strong>图6</strong>：GR00T N1与最先进的模仿学习方法在多个模拟基准上的比较。GR00T N1在大部分任务上取得了最高或接近最高的成功率。</p>
</blockquote>
<ul>
<li>在<strong>MetaWorld</strong>的10个任务上，GR00T N1平均成功率为**92.3%**，而最佳基线Diffusion Policy为81.9%。</li>
<li>在<strong>RLBench</strong>的18个任务上，GR00T N1平均成功率为**65.4%**，最佳基线VIMA为51.8%。</li>
<li>在<strong>ManiSkill2</strong>的“Pick Cube”任务上，GR00T N1成功率为**98.0%**，最佳基线ACT为85.0%。</li>
<li>在<strong>Dexterity Benchmark</strong>的6个任务上，GR00T N1平均成功率为**73.3%**，最佳基线Diffusion Policy为56.7%。</li>
</ul>
<p><strong>真实世界部署</strong>：将GR00T-N1-2B模型部署在Fourier GR-1人形机器人上，执行语言指令的双手操作任务（如“拿起左边的瓶子，放在右边的盒子里”）。<br><img src="https://arxiv.org/html/2503.14734v2/x7.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图7</strong>：GR00T N1在Fourier GR-1人形机器人上的真实世界语言条件操作任务表现。模型在有限数据下进行后训练后，能够可靠地执行复杂的双手操作指令。</p>
</blockquote>
<p>经过仅<strong>50条</strong>真实世界演示数据的后训练，模型在5个评估任务上平均成功率达到**76%**，展示了极高的数据效率。</p>
<p><strong>消融实验</strong>：</p>
<ol>
<li><p><strong>数据源消融</strong>：比较了仅使用真实机器人数据、加入模拟数据、加入神经轨迹数据以及加入人类视频数据（完整数据金字塔）的预训练效果。<br><img src="https://arxiv.org/html/2503.14734v2/x8.png" alt="数据消融"></p>
<blockquote>
<p><strong>图8</strong>：数据源消融研究。在RLBench任务上，逐步添加数据金字塔的每一层（真实→+模拟→+神经→+人类）都能持续提升模型性能，验证了异构数据协同训练的有效性。</p>
</blockquote>
</li>
<li><p><strong>架构消融</strong>：比较了不同系统设计（仅System 1、仅System 2、双系统但解耦训练、双系统端到端训练）的效果。<br><img src="https://arxiv.org/html/2503.14734v2/x9.png" alt="架构消融"></p>
<blockquote>
<p><strong>图9</strong>：架构消融研究。在语言条件任务上，完整的双系统端到端联合训练架构（GR00T N1）性能最佳，显著优于仅使用单一系统或双系统解耦训练的变体。</p>
</blockquote>
</li>
<li><p><strong>后训练数据增强</strong>：在低数据后训练场景下，比较了仅使用有限真实数据、以及使用神经轨迹进行增强的效果。<br><img src="https://arxiv.org/html/2503.14734v2/x10.png" alt="后训练增强"></p>
<blockquote>
<p><strong>图10</strong>：后训练数据增强。在仅使用10%的真实世界数据进行后训练时，加入神经轨迹进行协同训练可以大幅提升模型在真实世界任务上的成功率。</p>
</blockquote>
</li>
</ol>
<p><strong>潜在动作可视化</strong>：展示了学习到的潜在动作空间能够跨形态对齐相似行为。<br><img src="https://arxiv.org/html/2503.14734v2/x11.png" alt="潜在动作检索"></p>
<blockquote>
<p><strong>图11</strong>：跨形态的潜在动作检索。给定一个查询图像对（左上），模型可以从包含机器人形态和人类形态的数据库中检索出执行相似潜在动作的图像对。</p>
</blockquote>
<p><strong>神经轨迹生成示例</strong>：展示了视频生成模型能够产生多样化、反事实的机器人操作视频。<br><img src="https://arxiv.org/html/2503.14734v2/x12.png" alt="神经轨迹多样性"></p>
<blockquote>
<p><strong>图12</strong>：神经轨迹生成的多样性示例。从相同的初始帧出发，通过改变语言指令（如更换操作物体、目标位置），可以生成大量不同的合理操作序列。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>GR00T N1模型</strong>：提出了一个开放的、采用双系统架构的视觉-语言-动作基础模型，专为通用人形机器人设计，实现了推理与动作生成的紧密耦合。</li>
<li><strong>数据金字塔策略</strong>：创新性地构建了异构训练数据金字塔，并开发了潜在动作学习、神经轨迹生成等关键技术，有效解决了机器人领域大规模训练数据稀缺的难题。</li>
<li><strong>卓越的性能与效率</strong>：在多个模拟基准和真实世界部署中证明了其卓越性能，特别是在数据高效的后训练方面表现突出，仅需极少量的真实机器人数据即可适应新任务。</li>
</ol>
<p><strong>局限性</strong>：论文提到，模型的计算成本较高（预训练消耗约50,000 H100 GPU小时），且尽管在已见任务上泛化良好，但在完全未知的复杂现实世界场景中的泛化能力仍有待进一步验证。神经轨迹生成的质量和物理合理性也依赖于基础视频生成模型的性能。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>数据生成技术的深化</strong>：本文验证了利用生成模型合成机器人数据是一条极具潜力的路径。未来可探索更高质量、更长视野、更具物理真实性的视频/轨迹生成方法。</li>
<li><strong>模型效率优化</strong>：推动模型轻量化、推理加速研究，对于在资源受限的机器人本体上实时部署至关重要。</li>
<li><strong>从模仿到推理与规划</strong>：当前模型主要基于模仿学习。未来的通用机器人基础模型可能需要集成更复杂的任务规划、因果推理和物理常识推理能力。GR00T N1的双系统架构为向更高级认知能力的演进提供了良好的起点。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出GR00T N1，一个面向通用人形机器人的开放基础模型，旨在解决人形机器人缺乏大规模预训练数据、难以适应现实世界开放任务的问题。该模型采用视觉-语言-动作架构，包含视觉语言理解模块与实时动作生成的扩散变换器模块，并通过真实机器人轨迹、人类视频和合成数据混合进行端到端训练。实验表明，GR00T N1在多个机器人仿真基准上超越现有模仿学习方法，并在Fourier GR-1人形机器人上高效完成了语言引导的双手操作任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.14734" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>