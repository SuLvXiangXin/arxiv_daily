<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.24766" target="_blank" rel="noreferrer">2512.24766</a></span>
        <span>作者: Ruohan Zhang Team</span>
        <span>日期: 2025-12-31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，生成式视频建模已成为一种强大的工具，能够零样本地对开放世界操作中合理的物理交互进行推理。这些模型能够根据未见过的初始图像和开放式任务指令，合成包含物理交互的像素级视频，隐含地捕捉了直观物理规律以及丰富的物体属性和交互先验。然而，将这些模型集成到机器人操作系统中仍面临挑战，核心在于“具身鸿沟”：最先进的视频生成器通常在人类具身的设定下才能产生最佳交互片段，因为人类交互的数据远比机器人数据丰富。这导致模型预测的动作空间与机器人实际可执行的低级动作之间存在巨大差异。</p>
<p>本文针对如何将视频模型预测的、以人类为中心的合理物体运动，转化为机器人可执行的底层控制这一具体痛点，提出了新视角：将操作问题重构为物体轨迹跟踪问题。具体而言，本文认为视频模型擅长预测符合任务意图的、物理上合理的物体运动（即“世界状态应如何变化”），而机器人控制则负责在自身运动学和动力学约束下实现这些变化（即“如何实现”）。因此，本文的核心思路是：引入<strong>3D物体流</strong>作为中间表示，从生成的视频中重建物体的3D运动轨迹，并让机器人通过轨迹优化或强化学习来跟踪该轨迹，从而桥接视频生成与机器人控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>Dream2Flow的整体框架是一个三阶段的自动化流程：1) 根据任务指令和初始RGB图像生成视频；2) 从视频中提取任务相关物体的3D运动轨迹（3D物体流）；3) 基于3D物体流合成机器人动作。其输入是任务指令ℓ、初始RGB-D观测<code>(I0, D0)</code>以及已知的相机投影矩阵Π，输出是完成任务的机器人动作序列<code>u0:H-1</code>。</p>
<p><img src="https://i.imgur.com/2LQ8v0t.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：Dream2Flow整体框架。给定任务指令和初始RGB-D观测，图像到视频模型合成视频帧。随后利用视觉基础模型获取物体掩码、视频深度和2D点跟踪，进而重建3D物体流。最后，机器人策略通过轨迹优化或强化学习生成可执行动作以跟踪3D物体流。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>视频生成与3D物体流提取</strong>：</p>
<ul>
<li><strong>视频生成</strong>：使用现成的图像到视频生成模型，以任务指令ℓ和没有机器人出现的初始场景图像<code>I0</code>为条件，生成RGB视频<code>{Vt}</code>。论文指出，不包含机器人能获得物理上更合理的物体轨迹。</li>
<li><strong>视频深度估计</strong>：使用SpatialTrackerV2估计生成视频每一帧的深度<code>{Z̃t}</code>。由于单目深度存在尺度和偏移模糊性，通过将第一帧深度与机器人提供的真实初始深度<code>D0</code>对齐，计算全局的尺度<code>s⋆</code>和偏移<code>b⋆</code>，对深度进行校准得到<code>Zt</code>。</li>
<li><strong>3D物体流重建</strong>：<ul>
<li><strong>物体定位与分割</strong>：使用Grounding DINO根据<code>(I0, ℓ)</code>生成物体边界框，并用SAM 2获取该物体的二进制掩码<code>M</code>。</li>
<li><strong>2D点跟踪</strong>：在初始帧的掩码区域采样<code>n</code>个像素点，使用CoTracker3在整个视频中跟踪这些点，得到2D轨迹<code>c_t_i</code>和可见性<code>v_t_i</code>。</li>
<li><strong>3D提升</strong>：利用校准后的深度<code>Zt</code>和相机投影Π，将可见的2D轨迹点提升到3D空间，得到在机器人坐标系下的物体点轨迹<code>P1:T ∈ R^(T×n×3)</code>，即3D物体流。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>基于3D物体流的动作推理</strong>：<br>该方法将状态定义为任务相关物体点和机器人状态<code>xt = (x_obj_t, rt)</code>。其核心是优化问题：最小化预测的物体点位置<code>x_obj_t</code>与从视频流中提取的、经过时间对齐的目标点集<code>P̃_t</code>之间的差距（任务成本<code>λ_task</code>），同时考虑控制成本<code>λ_control</code>，并满足动力学模型<code>f</code>的约束。论文针对不同任务领域实例化了动作空间<code>U</code>和动力学模型<code>f</code>：</p>
<ul>
<li><strong>模拟推T任务</strong>：动作空间是参数化的推动技能（起始位置、方向、距离）。使用基于粒子的前向动力学模型预测场景中所有点的运动。通过随机采样<code>r</code>组推动参数，并选择能使预测点位置最接近3D物体流目标的那组参数来规划动作。</li>
<li><strong>真实世界域</strong>：动作空间是末端执行器的绝对位姿。采用“刚性抓取”动力学模型，假设被抓握的物体部件是刚性的。首先使用AnyGrasp规划抓取，并选择最接近视频中检测到的手部交互点（使用HaMer）的抓取位姿。然后，使用轨迹优化库PyRoki直接优化末端执行器位姿序列，以最小化物体流跟踪成本和平滑性等控制成本。</li>
<li><strong>模拟开门任务</strong>：将3D物体流作为奖励函数，使用SAC算法训练传感器运动策略。奖励函数鼓励机械臂末端接近物体点云的平均位置，并鼓励物体点云匹配3D物体流。这种方法将优化过程编译成参数化策略，允许不同的机器人形态（如四足机器人、人形机器人、固定基座机械臂）针对相同的物体运动目标涌现出不同的操作策略。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有尝试直接从视频中提取刚性变换轨迹（如AVDC、RIGVID）的方法相比，Dream2Flow的创新在于直接重建并跟踪<strong>非刚性、逐点的3D物体流</strong>。这种表示天然适用于刚性、铰接式、可变形和颗粒状物体，并且对部分点被遮挡或跟踪失败的情况更具鲁棒性，因为它不依赖于求解一个可能噪声很大的全局刚性变换。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：论文在模拟和真实世界环境中评估了Dream2Flow，使用了五个任务：模拟的<strong>Push-T</strong>（推T型块）、真实的<strong>Put Bread in Bowl</strong>（放面包入碗）、<strong>Open Oven</strong>（开烤箱）、<strong>Cover Bowl</strong>（用围巾盖碗）以及模拟的<strong>Open Door</strong>（开门）。对比的基线方法包括<strong>AVDC</strong>（基于稠密光流计算刚性变换）和<strong>RIGVID</strong>（基于6D姿态跟踪计算刚性变换）。实验平台涉及OmniGibson、Robosuite仿真器和真实机器人。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>整体性能与对比</strong>：在真实世界的三个任务中，Dream2Flow的性能全面优于AVDC和RIGVID。</p>
<blockquote>
<p><strong>表I</strong>：真实机器人上不同中间表示的对比。Dream2Flow在“放面包入碗”和“开烤箱”任务上成功率最高（8/10），在“盖碗”任务上也优于基线（3/10 vs 2/10, 1/10）。这验证了3D物体流相较于刚性变换轨迹的优越性。</p>
</blockquote>
</li>
<li><p><strong>3D物体流作为RL奖励</strong>：在模拟开门任务中，使用3D物体流作为奖励训练的SAC策略，与使用手工设计的物体状态奖励训练的策略性能相当，且在不同形态的机器人（Franka, Spot, GR1）上都能成功，并涌现出不同的操作策略。<br><img src="https://i.imgur.com/6LQ8v0t.png" alt="不同形态机器人开门策略"></p>
<blockquote>
<p><strong>图6</strong>：使用3D物体流奖励训练的策略在不同形态机器人上的开门策略。(a) Franka机械臂，(b) Spot四足机器人通过移动基座增加可达性，(c) GR1人形机器人使用手指与手掌之间的区域拉动门以增强稳定性。</p>
</blockquote>
</li>
<li><p><strong>消融实验与鲁棒性分析</strong>：</p>
<ul>
<li><strong>视频模型选择</strong>：不同视频生成模型对性能有显著影响。在模拟Push-T任务上，Wan2.1成功率较高（52/100）；在真实开烤箱任务上，Veo 3表现最佳（8/10），而其他模型因错误的关节运动轴或过多相机运动导致失败增多。<blockquote>
<p><strong>表III</strong>：视频生成模型的影响。Wan2.1在模拟域表现更好，Veo 3在真实域（如“开烤箱”）表现更优。</p>
</blockquote>
</li>
<li><strong>动力学模型选择</strong>：在Push-T任务中，基于粒子的动力学模型（成功率52/100）显著优于基于物体位姿的模型（12/100）和简单启发式模型（17/100）。<blockquote>
<p><strong>表IV</strong>：动力学模型消融实验。粒子动力学模型至关重要，能更好地处理物体旋转等复杂运动。</p>
</blockquote>
</li>
<li><strong>泛化与鲁棒性</strong>：Dream2Flow在物体实例、背景和视角变化下保持了相对稳健的性能（图4），并能在同一场景中根据不同的语言指令执行不同任务（图5）。</li>
</ul>
</li>
<li><p><strong>失败分析</strong>：对60次真实实验的失败原因进行了分析（图7），主要瓶颈包括：<strong>视频生成伪影</strong>（物体形变、幻觉新物体，占失败50%）、<strong>流提取失败</strong>（物体严重旋转或短暂出视野导致跟踪丢失，占33%）以及<strong>机器人执行失败</strong>（抓取点选择错误或运动不足，占17%）。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>3D物体流</strong>作为一种新颖的中间接口，将开放世界操作形式化为物体轨迹跟踪问题，从而桥接了视频生成模型与机器人控制。</li>
<li>实现了一个完全基于现成模型的端到端系统（Dream2Flow），在模拟和真实环境中，仅凭RGB-D观测和语言指令，以零样本方式成功完成了对刚性、铰接、可变形等多样物体的操作任务。</li>
<li>通过详实的实验证明了3D物体流相对于刚性变换轨迹的优越性、作为强化学习奖励的有效性，并系统分析了视频模型和动力学模型选择的影响。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前性能瓶颈主要在于上游视频生成模型产生的伪影（物体形变、幻觉）、因遮挡或快速运动导致的跟踪失败，以及下游抓取点选择可能不匹配。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>接口的通用性</strong>：3D物体流作为一种与具身无关的、专注于物体状态变化的表示，为利用大规模视频先验知识指导机器人提供了可扩展的路径。</li>
<li><strong>改进方向</strong>：未来工作可以专注于提升视频生成的物理真实性、开发更鲁棒的3D跟踪方法，以及设计更智能的抓取点选择策略来克服当前局限。</li>
<li><strong>范式分离</strong>：该方法成功验证了将“任务规划”（通过视频想象）与“运动执行”（通过优化/学习跟踪）相分离的可行性，这为构建模块化、可解释的机器人系统提供了新思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Dream2Flow框架，解决从生成视频模型到机器人低层动作执行的“具身鸿沟”问题。其核心方法是利用现成视频生成模型，根据初始图像和任务指令合成视频，并从中提取**3D物体流**作为中间表示；进而将操作任务定义为物体轨迹跟踪，通过轨迹优化或强化学习将其转化为可执行命令。该方法实现了对刚性、铰接、可变形及颗粒状物体的零样本操作，无需任务特定示教，为开放世界机器人操作提供了通用接口。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.24766" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>