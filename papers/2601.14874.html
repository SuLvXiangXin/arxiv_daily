<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.14874" target="_blank" rel="noreferrer">2601.14874</a></span>
        <span>作者: Mahmoud, Yara, Yaqoot, Yasheerah, Cabrera, Miguel Altamirano, Tsetserukou, Dzmitry</span>
        <span>日期: 2026/01/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前人形机器人系统在执行接触丰富的操作任务时，其控制器通常依赖固定、手动调整的阻抗增益（刚度与阻尼）和抓取器配置。这种方法限制了机器人适应场景几何、物体属性或任务意图变化的能力。与此同时，视觉语言模型（VLM）在开放世界感知和上下文推理方面展现出强大潜力，有望用于机器人的高级任务推断。然而，这些模型强大的语义理解能力与决定机器人物理交互行为的底层控制参数之间仍然脱节。本文旨在弥合这一鸿沟，提出了一种检索增强的视觉语言框架，使机器人能够直接从单目视觉输入中，为特定任务自主选择合适的交互参数。本文的核心思路是：利用VLM对第一视角RGB图像进行语义任务推理，并通过基于FAISS的检索增强生成（RAG）模块，从经验验证的数据库中检索对应的笛卡尔阻抗参数和物体特定的抓取器角度，从而实现任务感知的顺应性控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>HumanoidVLM框架的整体流程分为感知推理、参数检索和机器人控制三个阶段。给定来自机器人头部摄像头的第一视角RGB图像，系统首先通过VLM推断出高层操作任务，生成结构化的任务描述。该描述被编码后，由RAG模块在两个自定义数据库中进行相似性搜索：1) 笛卡尔阻抗数据库，存储任务特定的末端执行器刚度和阻尼系数；2) 抓取器角度数据库，指定针对不同物体类别的最优抓取配置。检索到的控制参数（K， D， 和抓取角 γ）被传输至机器人机载计算机，由任务空间笛卡尔阻抗控制器生成顺应的末端执行器轨迹，再通过逆运动学转换为关节目标，由内置的位置控制器执行。</p>
<p><img src="https://arxiv.org/html/2601.14874v1/main_img.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：HumanoidVLM整体架构。左侧为人类操作者和机器人第一视角相机。图像输入至VLM（粉色框“任务推理”）进行任务识别。结构化任务描述送至地面站（紫色框），其中的RAG模块通过FAISS搜索从两个数据库（阻抗数据库和抓取角数据库）中检索参数。检索到的参数被发送至机器人机载PC（绿色框），驱动任务空间阻抗控制器、逆运动学和轨迹规划器。底部序列图展示了机器人执行任务的场景。</p>
</blockquote>
<p><strong>核心模块一：VLM-RAG系统</strong>。该系统使用Molmo-7B-O BnB 4-bit模型处理视觉输入，通过一系列是/否视觉查询来确定任务。推理得到的任务标签通过<code>sentence-transformers/all-MiniLM-L6-v2</code>模型编码为向量，用于后续检索。RAG模块基于FAISS进行相似性搜索。数据库包含9个操作任务，每个任务关联一组通过真实实验验证的笛卡尔阻抗参数（刚度K=[Kx, Ky, Kz]和阻尼D=[Dx, Dy, Dz]）和一个最优抓取角γ_a。检索过程分为两步：首先用任务标签检索阻抗场景，然后将场景描述与任务标签拼接，检索对应的抓取器配置。</p>
<p><strong>核心模块二：机器人阻抗控制</strong>。由于Unitree G1机器人缺乏腕部力/力矩传感和抓取器反作用力反馈，本文在任务空间采用齐次笛卡尔质量-弹簧-阻尼模型来实现无传感器的顺应交互。末端执行器被建模为遵循期望位姿轨迹的6自由度点，该轨迹作为阻抗动力学的虚拟参考。对于每个机械臂a，定义其位置误差e_a = x_a,ref - x_a，阻抗动力学遵循二阶常微分方程：M_a * e_a&#39;&#39; + D_a * e_a&#39; + K_a * e_a = 0。由此产生的虚拟力F_a^virt = K_a * e_a + D_a * e_a&#39; 作为物理接触力的定量代理。检索系统提供的K和D参数用于调节该虚拟力，从而实现不同任务所需的顺应性或刚性行为。旋转阻抗和虚拟质量M在此工作中保持固定以确保稳定性。抓取器控制为单自由度，根据检索到的动作γ_a（开/合）执行预定义的关节角度。</p>
<p><strong>创新点</strong>：与现有将VLM用于策略或位姿生成，但使用固定阻抗增益的方法不同，本文的创新在于通过检索机制，将高级语义理解直接、可解释地映射到低层物理交互参数（刚度和阻尼），为人形机器人实现任务感知的顺应性控制提供了一条新路径。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与设置</strong>：实验在配备两个1-DoF抓取器和头部Intel RealSense RGB-D相机的Unitree G1人形机器人上进行。VLM-RAG流水线在外部工作站（RTX 4090 GPU， i9-13900K CPU）运行，阻抗控制器在机载PC以50Hz运行。评估聚焦于以z轴（法向）交互为主的桌面任务，包括：表面跟随、用按摩球施压、双手放置物体（酱料瓶和鸡蛋）、用叉子戳水果、抓取并抬起物体。</p>
<p><strong>Baseline与评估指标</strong>：论文未与特定基线进行数值对比，但指出相关工作（如OmniVIC， ImpedanceGPT）专注于工业机械臂或无人机群，未涉及人形机器人本体和共享工作空间的安全问题。评估分为两部分：1) <strong>VLM-RAG检索准确性</strong>：在14张涵盖9种任务类型、具有视角和物体位置变化的测试图像上，评估系统能否正确检索阻抗和抓取参数。2) <strong>阻抗控制性能</strong>：分析执行任务时z轴的跟踪误差和虚拟力大小，以验证检索参数是否产生稳定、有界的交互行为。</p>
<p><img src="https://arxiv.org/html/2601.14874v1/actions.jpg" alt="实验场景"></p>
<blockquote>
<p><strong>图2</strong>：代表性操作任务的实验成功场景图。展示了机器人执行表面跟随、施压、抓取、双手放置和工具交互等任务。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>检索准确性</strong>：在14个测试场景中，系统在13个场景中正确检索了参数，准确率达到**93%**。唯一失败案例是由于主要物体被部分遮挡。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.14874v1/vlm-rag.png" alt="检索准确率"></p>
<blockquote>
<p><strong>图3</strong>：VLM-RAG系统在14个场景下的检索准确性柱状图。正确检索13次，错误1次。</p>
</blockquote>
<ol start="2">
<li><strong>控制性能</strong>：如表1所示，所有任务中z轴跟踪误差均保持较小范围（平均误差0.6-1.7 cm，最大误差1.3-3.5 cm），且计算出的虚拟力大小与所选阻抗增益一致地缩放。例如，表面跟随任务使用软刚度（Kz=3.0 N/m），产生了低虚拟力（最大0.103 arb.）；而施压任务使用更高刚度（Kz=5.0 N/m），产生了更大的虚拟力（最大0.334 arb.）。双手放置任务为左右手设置了不对称的刚度（鸡蛋：2.0 N/m， 瓶子：6.0 N/m），以匹配物体的脆弱性，对应的虚拟力也反映了这一差异。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.14874v1/pos_follow_surface.jpg" alt="跟踪误差"></p>
<blockquote>
<p><strong>图4</strong>：表面跟随任务中末端执行器的平移跟踪误差。三条曲线分别展示了X， Y， Z方向测量值与期望轨迹的对比，Z轴误差随曲面轮廓变化，证明了顺应的接触跟踪。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了HumanoidVLM框架，首次将视觉语言模型与检索增强生成相结合，为人形机器人实现了从语义场景理解到任务特定阻抗与抓取器参数选择的自主映射。2) 构建了经过实验验证的小型数据库，将9种操作任务与对应的物理交互参数关联，提供了一种可解释的参数选择机制。3) 在真实的Unitree G1人形机器人平台上进行了系统集成与验证，证明了该框架在多种接触丰富操作任务中的可行性。</p>
<p><strong>局限性</strong>：论文自身指出，当前评估规模和任务多样性有限，结果应视为可行性证明而非全面的鲁棒性评估。数据库规模较小（9个任务），缺乏对旋转阻抗的考虑，且控制系统未集成力或视觉触觉反馈进行闭环适应。</p>
<p><strong>对后续研究的启示</strong>：未来工作可从三方面拓展：1) 引入任务依赖的旋转阻抗控制以支持对方向敏感的行为。2) 用学习到的连续映射替代离散数据库，实现对未见任务和物体类型的泛化与插值。3) 集成力或视觉触觉传感器，实现基于实时交互反馈的闭环阻抗自适应，进一步提升安全性和鲁棒性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出HumanoidVLM框架，解决人形机器人在接触式操作中因依赖固定阻抗参数而缺乏适应性的问题。方法核心为：通过视觉语言模型解析任务语义，利用基于FAISS的检索增强生成模块，从定制数据库中检索实验验证的笛卡尔阻抗参数与抓取角度，并由任务空间阻抗控制器执行。在14个视觉场景的实验中，系统检索准确率达93%，实际操控时Z轴跟踪误差保持在1–3.5 cm内，验证了语义感知与检索式控制结合实现自适应操作的可行性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.14874" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>