<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Point What You Mean: Visually Grounded Instruction Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Point What You Mean: Visually Grounded Instruction Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.18933" target="_blank" rel="noreferrer">2512.18933</a></span>
        <span>作者: Yu, Hang, Zhao, Juntu, Liu, Yufeng, Li, Kaiyu, Ma, Cheng, Zhang, Di, Hu, Yingdong, Chen, Guang, Xie, Junyuan, Guo, Junliang, Zhao, Junqiao, Gao, Yang</span>
        <span>日期: 2025/12/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通用视觉-语言-动作（VLA）模型通过统一感知、语言和低级动作，在具身控制方面取得了显著进展。然而，这些方法普遍依赖自然语言作为唯一的指令接口，这带来了根本性的信息瓶颈。在复杂场景中，仅凭文本指令无法精确描述所有指代情况，例如堆叠或重叠的物体、不规则的形状、未见过的物体类别，或者平面上缺乏视觉锚点的精确位置。这导致了两个核心挑战：<strong>指代不可表达</strong>（语言无法精确指定目标）和<strong>泛化能力有限</strong>（在复杂空间关系或新物体类别上表现不佳）。现有方法，如将图像块与文本标记交错（Interleave），缺乏明确的空间绑定，无法实现精确的像素级指代。</p>
<p>本文针对VLA模型在指代模糊场景下的性能瓶颈，提出了一种新视角：将明确的视觉提示（如边界框）作为指令的补充参数，直接实现语言指代到像素的锚定。核心思路是：在标准的多视角视觉观测和文本指令基础上，额外提供一个在第一帧顶视图中标注了目标（如边界框）的“视觉提示”图像，通过协同训练得到一个既能处理纯文本指令、也能处理视觉增强指令的统一策略，从而解决指代模糊问题。</p>
<h2 id="方法详解">方法详解</h2>
<p>Point-VLA是一个即插即用的策略，其核心是在标准VLA接口上增加一个视觉提示输入。整体框架包括视觉提示构建、自动数据标注和协同训练策略。</p>
<p><img src="https://arxiv.org/html/2512.18933v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：训练流程概览。通过在第一帧顶视图上绘制边界框（可自动或手动标注）获得视觉提示。该标注帧经过轻微的数据增强（如CutMix、平移）后，与轨迹中的每一帧机器人观测配对。模型同时接收当前观测和固定的、带视觉提示的第一帧进行训练，从而实现贯穿整个轨迹的、一致的像素级目标定位。</p>
</blockquote>
<p><strong>整体流程</strong>：在推理时，给定当前多视角观测 <code>I_t</code>、文本指令 <code>l_t</code> 以及一个在第一帧顶视图 <code>I~_g,0</code> 上标注了视觉标记 <code>g</code>（如边界框）的视觉提示，策略预测下一个动作 <code>a_t = π_θ(l_t, I_t, I~_g,0)</code>。训练时，模型接收同样的输入进行学习。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉提示构建</strong>：视觉提示旨在提供文本无法表达的明确空间指代。默认形式是在第一帧顶视图上覆盖一个边界框。文本指令 <code>l_t</code> 仅保留高层意图（如“拿起”、“放置”），所有目标相关信息均来自视觉提示。该方法也兼容其他形式（如掩码、文本坐标），并可零样本使用。</li>
<li><strong>自动数据标注流程</strong>：为大规模获取视觉标注，论文提出一个基于多模态大语言模型（MLLM）的四阶段自动标注流程：a) MLLM基于完整轨迹视频和文本描述进行场景理解；b) 选择目标清晰可见的关键帧；c) 在关键帧上预测目标边界框；d) 将该边界框传播到第一帧顶视图，得到 <code>(I~_g,0, g)</code> 监督对。单帧标注策略提高了效率。</li>
<li><strong>数据增强</strong>：为提高视觉提示的鲁棒性，对标注图像应用两种增强：<ul>
<li><strong>随机平移</strong>：随机平移带框的整张图像，使场景和框一起移动，鼓励策略学习目标在场景中的相对位置，而非绝对像素坐标。</li>
<li><strong>局部CutMix</strong>：在边界框内部，用ImageNet图像块部分替换物体外观，而保持周围上下文不变，防止模型对框内见过的特定物体外观过拟合。</li>
</ul>
</li>
<li><strong>协同训练策略</strong>：为保持与纯文本策略的兼容性，Point-VLA在纯文本指令数据集 <code>D_text</code> 和视觉增强指令数据集 <code>D_visual</code> 上以1:1的比例进行协同训练。这使模型成为一个统一策略，既能从视觉增强样本中学习空间消歧，又能从纯文本样本中保持指令遵循能力。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>视觉提示作为明确参数</strong>：将边界框等视觉标记作为指令的明确组成部分，直接锚定语言指代到像素，而非仅作为语义示例或通过手工解析器。</li>
<li><strong>可扩展的自动标注流程</strong>：利用预训练MLLM从现有轨迹自动生成视觉标注，极大降低了标注成本。</li>
<li><strong>统一的协同训练策略</strong>：通过同时训练文本和视觉两种指令模式，得到一个单一策略，在两种模式下均能有效工作。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在6个需要精确指代的真实世界机器人操作任务上进行评估（见图3c），包括不规则物体抓取、OOD物体抓取、堆叠场景抓取、蛋槽抓取、平面放置和蛋槽放置，共12个不同的场景配置。</li>
<li><strong>实验平台</strong>：主要使用搭载一个顶视相机和两个腕部相机的双臂机器人，也评估了全身人形机器人。</li>
<li><strong>Baseline方法</strong>：1) <strong>Text VLA</strong>：仅使用文本指令的基线VLA模型；2) <strong>Interleave-VLA</strong>：将图像块与文本标记交错输入的方法。</li>
<li><strong>评估指标</strong>：每个任务进行超过30次独立试验，计算成功率。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.18933v1/x3.png" alt="任务与机器人概览"></p>
<blockquote>
<p><strong>图3</strong>：任务与机器人平台概览。(a)数据收集。(b)用于评估的双臂和全身人形机器人。(c)六个代表性任务，均包含难以仅用文本精确指代的目标。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>如表1所示，Point-VLA在所有六个任务上均取得了最高的平均成功率（92.5%），相比Text VLA基线平均绝对提升60.1个百分点，相比Interleave-VLA提升52.5个百分点。特别是在最精细的蛋槽抓取任务上，成功率超过基线75个百分点以上，在未见物体（OOD）抓取任务上提升35个百分点。</p>
<p><img src="https://arxiv.org/html/2512.18933v1/x1.png" alt="主要结果对比"></p>
<blockquote>
<p><strong>表1</strong>：Point-VLA在六个真实世界操作任务上的成功率显著超越纯文本和Interleave基线，尤其是在指代模糊的场景中。</p>
</blockquote>
<p><strong>兼容性实验</strong>：<br>如图4所示，即使仅使用文本指令进行评估（Point-VLA (l)），其性能也匹配或超过了纯文本训练的基线（Text VLA）。这表明，协同训练视觉增强数据本身提升了策略对语言空间指代的理解能力。当使用视觉增强指令时（Point-VLA (VGI)），性能达到最高。</p>
<p><img src="https://arxiv.org/html/2512.18933v1/x4.png" alt="文本指令兼容性"></p>
<blockquote>
<p><strong>图4</strong>：在三种空间指代场景下，Point-VLA即使用纯文本指令（l）也优于基线，而使用视觉增强指令（VGI）时性能最佳，证明了协同训练的有效性和模式兼容性。</p>
</blockquote>
<p><strong>即插即用与泛化性</strong>：<br>如表2所示，Point-VLA在 <code>π_0.5</code> 和 <code>π_0</code> 两种VLA骨干模型上，以及在双臂和全身人形两种机器人平台上进行微调后，均能稳定超越对应的纯文本指令基线，证明了其作为模块化接口在不同架构和具身上的可迁移性。</p>
<p><img src="https://arxiv.org/html/2512.18933v1/x6.png" alt="不同骨干与机器人的结果"></p>
<blockquote>
<p><strong>表2</strong>：Point-VLA在不同VLA骨干模型和机器人平台上均能带来性能提升，展示了其即插即用和良好的泛化能力。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>视觉提示形式</strong>（表3）：对比了文本坐标、物体掩码和边界框覆盖三种形式。边界框覆盖在各项任务上表现最佳，因为它既提供了明确的局部参考，又保留了关键的全局上下文信息。文本坐标容易对绝对位置过拟合，而掩码则会丢失必要的上下文。</li>
<li><strong>数据增强</strong>（表4）：移除随机平移会严重损害在蛋槽任务（执行时托盘被随机移动）上的性能，表明其对学习相对空间关系至关重要。移除CutMix则会显著降低在OOD物体任务上的性能，表明其对防止模型过拟合特定物体外观、提升泛化能力的关键作用。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.18933v1/x5.png" alt="视觉提示形式对比"></p>
<blockquote>
<p><strong>图5及表3</strong>：对比不同视觉提示形式。边界框覆盖（Ours）在保留全局上下文的同时提供精确局部参考，效果最好。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>Point-VLA</strong>，一种通过显式视觉提示（如边界框）增强语言指令的VLA模型，有效解决了复杂和未见场景中的指代模糊问题，实现了精确的对象和位置指定。</li>
<li>设计了一个<strong>统一的协同训练策略</strong>，使单一策略能同时支持纯文本和视觉增强两种指令模式，且不损害纯文本指令的遵循能力。</li>
<li>开发了一个<strong>可扩展的自动数据标注流程</strong>，利用MLLM从现有演示轨迹中自动生成视觉标注，大幅降低了构建视觉增强数据集的成本。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，当前的视觉提示依赖于第一帧的标注，这可能对需要动态跟踪移动目标的任务构成限制。此外，自动标注流程的质量依赖于MLLM的性能。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>多模态指令的标准化</strong>：Point-VLA展示了将视觉提示作为明确指令参数的潜力，未来可以探索更丰富、标准化的多模态指令接口（如手势、视线、草图）。</li>
<li><strong>动态视觉提示</strong>：可研究如何将视觉提示从静态第一帧扩展到动态序列或实时交互，以支持跟踪移动目标或在线纠正。</li>
<li><strong>更高效的自动标注</strong>：进一步优化基于基础模型的自动标注方法，提高标注精度和范围，以支持更复杂的任务和场景。</li>
<li><strong>人机交互界面</strong>：Point-VLA支持通过GUI直接标注或通过MLLM解析自然指向两种交互模式，这为设计更直观、自然的人机协作系统提供了新思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型在仅依赖文本指令时，于杂乱或分布外场景中对象指代能力有限、存在歧义的核心问题，提出了Point-VLA。这是一种即插即用的策略，通过为语言指令附加边界框等显式视觉线索，实现像素级的精准对象接地。为高效扩展视觉接地数据集，还开发了自动数据标注流程。实验表明，该策略在多样化现实指代任务中性能始终优于纯文本VLA，尤其在杂乱或未见对象场景中表现更强，并具备鲁棒的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.18933" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>