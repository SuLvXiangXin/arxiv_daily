<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.06575" target="_blank" rel="noreferrer">2602.06575</a></span>
        <span>作者: Wang, Fangyuan, Zhou, Peng, Qi, Jiaming, Lyu, Shipeng, Navarro-Alarcon, David, Guo, Guodong</span>
        <span>日期: 2026/02/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉-语言-动作模型通常仅在处理流程的后期将机器人本体感知信号作为条件输入，例如通过简单的投影器映射后直接馈送到动作头。这种方式阻止了机器人状态在早期阶段影响对任务指令的理解，也无法在策略的整个计算过程中引导对哪些视觉token的关注。接触丰富的操作任务不仅取决于可见内容和动作指令，还依赖于机器人的具身状态。本文针对“如何让机器人更好地利用其物理接地状态来理解任务和环境”这一核心设计问题，提出了“具身视觉推理”的新视角。其核心思路是：将连续的本体感知信号在VLM的嵌入空间中进行文本化token编码，并在模型输入阶段就与任务指令进行融合，使得具身状态能够参与后续的视觉推理和token选择过程，从而偏向于关注对动作生成至关重要的视觉证据，同时抑制冗余的视觉token。</p>
<h2 id="方法详解">方法详解</h2>
<p>ThinkProprio的整体框架基于“早期融合”原则，是一个双系统VLA策略，包含视觉语言主干和独立的动作头。其输入为多视角RGB图像、语言指令和连续的本体感知状态（如关节角度）。输出为预测的连续动作序列。流程如下：1）将图像编码为视觉token，指令编码为语言token；2）将连续本体感知状态离散化并利用VLM的词表嵌入为文本token；3）将语言token和本体感知token拼接作为引导token，通过交叉注意力机制为每个视觉token生成一个“物理接地”的查询向量，并基于此进行投票式token选择，仅保留任务相关的视觉patch；4）添加一个全局上下文token以保留粗略的场景信息；5）将筛选后的紧凑token序列（选中的视觉token、全局token、本体感知token、语言token）送入VLM进行处理；6）动作头通过交叉注意力条件于VLM输出的特征，并采用流匹配目标进行训练。</p>
<p><img src="https://arxiv.org/html/2602.06575v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ThinkProprio方法概述。本体感知被文本化token编码，并与指令结合，通过交叉注意力引导视觉token选择，仅保留任务相关的patch和一个全局上下文token。紧凑的token集由VLM处理，动作头通过交叉注意力关注生成的特征以产生动作。</p>
</blockquote>
<p>核心模块包括本体感知状态编码和物理接地token选择。本体感知编码采用均匀分箱离散化，将每个连续状态值映射到VLM词表的最后B个token ID之一，从而直接复用VLM的token嵌入表，将本体感知置于与语言相同的语义空间。物理接地token选择机制是另一创新点：首先将语言token和本体感知token拼接为引导token，通过归一化和交叉注意力，为每个视觉token计算一个基于指令和机器人状态的查询向量。然后，该查询向量与所有视觉token进行匹配打分，形成一个得分矩阵。通过引入退火Gumbel噪声并进行行方向的argmax投票，决定保留哪些视觉token（获得至少一票则保留）。训练中使用直通估计器解决离散选择的梯度问题。该方法使token选择直接受到当前机器人配置和任务指令的双重指导。</p>
<p>与现有方法相比，ThinkProprio的核心创新点在于：1）<strong>早期深度融合</strong>：将本体感知以文本token形式在输入阶段与指令融合，而非后期注入，使其能从头影响视觉推理。2）<strong>状态感知的token选择</strong>：利用融合了指令和本体感知的引导信号动态筛选视觉token，实现了计算效率与信息保留的平衡，这与仅基于指令进行token裁剪的方法（如LightVLA）有本质区别。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了CALVIN和LIBERO两个模拟机器人操作基准测试。在CALVIN上使用ABC-&gt;D分割（在环境A、B、C上训练，在未见环境D上测试），评估指标包括不同链长（LH-1至LH-5）的成功率以及平均完成链长。在LIBERO上评估其四个测试泛化能力的子套件（Spatial, Object, Goal, Long）的平均成功率。对比的基线方法包括OpenVLA、GR-1、π0、π0.5、FLOWER以及视觉规划方法如VPP、Seer等。</p>
<p><img src="https://arxiv.org/html/2602.06575v1/x3.png" alt="CALVIN结果"></p>
<blockquote>
<p><strong>图3</strong>：CALVIN ABC-&gt;D结果表。ThinkProprio取得了最佳的平均完成长度（4.55），尤其在最长任务链（LH-5）上达到82.1%的成功率，相较于强基线FLOWER†有显著提升。</p>
</blockquote>
<p>关键实验结果：在CALVIN ABC-&gt;D上，ThinkProprio将平均完成长度从FLOWER基线的4.44提升至4.55，在最具挑战性的五步长任务（LH-5）上，成功率从75.5%提升至82.1%，相对失败率降低了约19%。在LIBERO上，ThinkProprio整体表现（平均成功率97.3%）与最强基线相当，并在测试长时程任务的LIBERO-Long套件上取得了最佳性能（95.2%）。在计算效率方面，ThinkProprio平均仅保留约15%的视觉token（CALVIN上15/100），使端到端推理延迟从FLOWER的52ms大幅降低至22ms（降低58%），峰值VRAM使用量略有增加但仍远低于OpenVLA等方案。</p>
<p><img src="https://arxiv.org/html/2602.06575v1/x4.png" alt="效率对比"></p>
<blockquote>
<p><strong>图4</strong>：CALVIN上的推理效率对比表。ThinkProprio在保持高性能的同时，视觉token数量、延迟和VRAM占用均显著优于或接近其他高效方法。</p>
</blockquote>
<p>消融实验系统地验证了各组件贡献：1）仅添加文本化本体感知token可使性能微升（Avg. Len 4.44 -&gt; 4.48），但延迟增加。2）引入物理接地token选择能大幅降低延迟（55ms -&gt; 20ms），但性能下降至4.35，表明丢失了部分有用信息。3）进一步添加全局上下文token后，性能恢复并超越基线至4.55，延迟仅小幅增至22ms，证明了全局token对补充场景信息的关键作用。此外，消融实验还对比了不同本体感知编码与注入方式，证实文本化token编码并输入VLM的方式优于使用MLP投影或直接注入动作头的方式。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了“具身视觉推理”框架ThinkProprio，通过将本体感知早期融合为VLM文本token，使其能够主动指导任务理解和视觉注意力。2）设计了一种由指令和本体感知共同引导的物理接地视觉token选择机制，在显著提升计算效率（减少85%视觉token，降低58%延迟）的同时，维持甚至提升了长时程操作任务的性能。3）通过系统的消融研究，厘清了VLA模型中本体感知编码、注入点、token选择等设计选择的影响，为社区提供了清晰的见解。</p>
<p>论文自身提到的局限性包括：文本化token编码依赖于预训练VLM的词表，可能对非常精细的连续状态变化不敏感；token选择机制虽然高效，但其计算复杂度随视觉token数量平方增长，对于极高分辨率的图像可能需要进一步优化。</p>
<p>这项工作对后续研究的启示是：在构建具身智能体时，应更深入地将物理状态与感知、推理过程进行早期和紧密的耦合，而不仅仅是将其作为控制信号。ThinkProprio展示了一种通过“状态感知的抽象”来平衡计算开销与任务性能的有效路径，为未来开发更高效、更鲁棒的VLA模型提供了新的设计思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型中本体感觉信息融合不足的问题，提出ThinkProprio方法。其核心是将机器人的本体感觉（如关节位姿）编码为文本标记，并在模型输入层与任务指令进行早期融合，使其能引导后续的视觉推理，并筛选出关键视觉证据。实验表明，该方法在CALVIN和LIBERO等基准测试中性能匹配或优于基线模型，仅保留约15%的视觉标记即可达到全标记集性能，并将端到端推理延迟降低了超过50%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.06575" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>