<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Study on Enhancing the Generalization Ability of Visuomotor Policies via Data Augmentation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>A Study on Enhancing the Generalization Ability of Visuomotor Policies via Data Augmentation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.09932" target="_blank" rel="noreferrer">2511.09932</a></span>
        <span>作者: Hanwen Wang Team</span>
        <span>日期: 2025-11-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于模仿学习的视觉运动策略在机器人非结构化操作任务中已被证明有效。为了降低人工演示数据收集的高昂成本，主流方法（如MimicGen）致力于利用少量人类演示，通过轨迹增强自动生成大量数据，以训练泛化能力更强的策略。这些方法主要关注在水平桌面上随机摆放物体所带来的轨迹多样性，旨在实现物体位置泛化。</p>
<p>然而，现有方法生成的数据仍然缺乏多样性，这限制了训练出的策略的泛化能力。关键局限性在于，它们主要自动化了多样化轨迹的生成，却没有考虑视觉差异。当场景多样性增加时，例如相机位姿、光照颜色、桌面纹理、桌面高度变化以及跨本体部署时，仅靠单一类型的轨迹增强数据变得不足。一些研究虽然探索了广泛领域随机化以弥合视觉差距，但并未系统评估各种随机化因素的影响，且研究条件缺乏足够的多样性。</p>
<p>本文针对现有数据增强方法在视觉多样性上的不足这一具体痛点，提出了一个更全面的新视角：系统研究并自动化生成对泛化能力有显著影响的多种场景布局因素（视觉与几何）的随机化数据。本文的核心思路是：通过引入涵盖相机位姿、光照、纹理、桌面高度及跨本体等多种随机化因素的数据增强，构建一个更广泛随机化的数据集，以系统性探究并提升视觉运动策略的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体目标是通过在场景中应用广泛的数据增强来收集数据，使得策略学习后能获得泛化能力。这些能力体现在对场景视觉变化的鲁棒性、对背景和光照扰动的抵抗性、操纵随机放置在桌面上物体的能力以及跨本体部署的能力。</p>
<p><img src="https://arxiv.org/html/2511.09932v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：方法整体框架。与先前工作相比，我们的轨迹增强引入了桌面高度随机化，以及相机位姿、光照颜色、桌面纹理等视觉因素的随机化。此外，我们的数据收集涵盖了五种类型的机械臂和两种夹爪。</p>
</blockquote>
<p>方法包含三个核心的随机化增强模块：</p>
<ol>
<li><strong>轨迹增强</strong>：基于MimicGen方法，并进行了关键扩展。首先，假设任务可分解为一系列以物体为中心的子任务。根据人类演示（源数据），将轨迹分割为子轨迹段。对于每个子任务段，根据被操纵物体的新位姿，通过坐标变换生成新的末端执行器轨迹段，从而在物体放置位置变化时生成多样轨迹。<strong>关键创新</strong>在于，先前工作仅随机化物体在桌面上的水平位置，而本文通过改变桌面高度，允许物体相对于机器人坐标系被安排在不同高度，从而增强了轨迹在垂直方向上的多样性。</li>
<li><strong>视觉增强</strong>：聚焦于三个关键因素：第三方相机位姿、光照颜色和桌面纹理。<ul>
<li><strong>相机位姿随机化</strong>：以机器人基座坐标系为中心构建一个球体，剔除部署中不可能使用的视角以及机器人工作空间被严重遮挡的部分。在该表面上使用斐波那契采样均匀采样100个相机位姿。在数据收集过程中，仅当轨迹增强生成成功轨迹时才切换到下一个相机位姿，以保持视点分布的均匀性。</li>
<li><strong>光照条件随机化</strong>：通过将每个RGB通道的强度在0-0.5范围内随机化，模拟昏暗的室内环境。</li>
<li><strong>桌面纹理随机化</strong>：从RoboSuite和ARNOLD中选择17种不同的图案，包括常见的木材和金属纹理。</li>
</ul>
</li>
<li><strong>跨本体增强</strong>：利用仿真器的可扩展性。只要机器人的工作空间与原始人类演示中使用的Franka Panda机器人没有显著差异，就可以将轨迹和视觉增强数据收集部署在不同构型的机器人上。本文选择了UR5e、IIWA、Kinova3和Jaco等常用实验室机器人，以及Robotiq85Gripper夹爪。为了与PandaGripper的动作对齐，将Robotiq85Gripper的动作映射到单一的打开/关闭自由度。</li>
</ol>
<p><strong>策略学习方法</strong>：本文采用基于去噪扩散概率模型（DDPM）框架的扩散策略（Diffusion Policy, DP）。该策略以当前观测 <code>s</code> 为条件，对动作序列进行去噪。具体而言，策略参数化 <code>p_θ(a_{k-1} | a_k, s)</code>，并通过行为克隆训练噪声预测网络 <code>ϵ_θ(a_k, s, k)</code>。与单峰高斯策略不同，DP能够捕获未来轨迹的多模态分布。为了时间一致性，策略预测一个动作块 <code>a = {a_t, ..., a_{t+T_p-1}}</code>，机器人执行其中 <code>T_a ≤ T_p</code> 步后重新规划。本文采用了基于CNN的DP实现，以提供更稳定的训练并最小化超参数调整。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/数据集</strong>：使用了RoboMimic基准测试中定义的六个机器人操作任务（如图3所示），主要集中于组装或拾取放置任务。</li>
<li><strong>实验平台</strong>：仿真实验在RoboSuite等仿真器中进行；真实世界验证使用低成本SO-101机械臂和Intel RealSense D435i相机。</li>
<li><strong>对比Baseline</strong>：以仅使用MimicGen生成数据训练的策略作为基线方法（文中称为“original method”）。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.09932v1/x3.png" alt="任务示意图"></p>
<blockquote>
<p><strong>图3</strong>：本研究使用了RoboMimic中的六个机器人操作任务，主要关注难度各不一的组装或拾取放置任务。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>泛化性能对比（表I）</strong>：本文设计了一个比原始RoboMimic更具挑战性的基准，在测试时引入了单一种类的随机化因素。结果表明，使用原始方法训练的模型在引入额外随机化因素后性能显著下降。相比之下，使用本文随机化增强数据训练的模型在这些场景中表现出更强的能力。例如，在“相机位姿随机化”测试环境下，原始方法在Threading任务上的成功率为0%，而本文方法达到72%。</p>
</li>
<li><p><strong>消融实验（表II）</strong>：该实验探究了在训练时引入不同的单因素随机化，是否能在其他随机化测试场景中提升策略泛化。结果表明，这些随机化因素对提升策略的泛化能力具有相互促进的作用。例如，使用<strong>桌面高度随机化</strong>数据训练的策略，在<strong>光照条件随机化</strong>的测试场景中取得了0.53的平均成功率（基线为0.50），显示出一定的跨因素泛化能力。<strong>跨本体数据</strong>本身包含了视觉多样性，进一步增强了这种泛化能力。</p>
</li>
<li><p><strong>真实世界实验（表III）</strong>：<br><img src="https://arxiv.org/html/2511.09932v1/x4.png" alt="真实实验平台"></p>
<blockquote>
<p><strong>图4</strong>：真实世界实验平台，左侧为低成本SO-101机械臂，右侧为第三方视角的RealSense相机。</p>
</blockquote>
<p>为了验证随机化是否有助于零-shot模拟到真实迁移，在SO-101机械臂上进行了Grasp Cube任务实验。使用PPO算法在仿真中训练策略，对比了使用多因素随机化增强训练的策略与无随机化的基线策略。结果表明，<strong>使用随机化增强训练的</strong>策略在真实世界中取得了<strong>0.44</strong>的成功率，高于<strong>无随机化</strong>基线的<strong>0.28</strong>。这说明增强视觉和轨迹多样性显著有助于实现零-shot模拟到真实的机器人操作。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文核心贡献</strong>：</p>
<ol>
<li><strong>系统性分析</strong>：首次系统性地研究了不同随机化因素（相机位姿、光照、纹理、桌面高度、跨本体）对视觉运动策略泛化能力的局限性及其相互增强作用。</li>
<li><strong>综合性增强方法</strong>：提出了一种结合轨迹增强（含高度随机化）与多种视觉随机化的综合性数据增强方法，构建了更广泛随机化的数据集。</li>
<li><strong>零-shot模拟到真实验证</strong>：通过真实世界实验证实，基于随机化增强训练的策略能有效提升零-shot模拟到真实的迁移性能。</li>
</ol>
<p><strong>论文自身提到的局限性</strong>：<br>如果策略仅在本工作提供的仿真数据上训练，很难在真实世界中执行复杂的操作任务。仿真实验未明确将仿真场景与真实世界对齐，导致存在显著的视觉差距和一些动力学差距。此外，组装任务不仅需要视觉和轨迹数据的多样性，还需要理解某些物理世界规则，这是纯视觉模仿学习难以实现的。</p>
<p><strong>对后续研究的启示</strong>：<br>一种可行的方向是首先对仿真与真实环境的视觉配置进行大致对齐，然后结合使用视觉和轨迹增强，并利用强化学习通过与物理世界的交互收集数据，从而获取更多关于物理世界的知识。这指出了结合仿真数据增强与真实世界在线学习（如强化学习）是提升复杂任务泛化和迁移能力的重要途径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本研究旨在提升视觉运动策略的泛化能力，解决现有数据增强方法生成数据多样性不足、限制策略跨场景部署的问题。提出自动生成广泛随机化数据集的方法，仅需少量人类演示，覆盖多种操纵器与夹持器，并随机化相机姿态、光照、桌面纹理及高度等因素。实验表明，所有随机化因素均能增强策略泛化，多样化轨迹可有效桥接视觉差距，显著提升零样本模拟到真实转移的泛化性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.09932" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>