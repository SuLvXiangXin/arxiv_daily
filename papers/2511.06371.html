<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.06371" target="_blank" rel="noreferrer">2511.06371</a></span>
        <span>作者: Zhao, Yingnan, Wang, Xinmiao, Wang, Dewei, Liu, Xinzhe, Lu, Dan, Han, Qilong, Liu, Peng, Bai, Chenjia</span>
        <span>日期: 2025/11/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前人形机器人运动控制的主流方法是针对每个特定技能（如站立、行走、跳跃）分别训练独立的策略。这种方法产生的是行为特定的控制器，虽然在特定任务上表现优异，但在部署于不规则地形和多样化场景时，其泛化能力有限，性能脆弱。直接通过多任务强化学习训练一个掌握多种技能的策略极具挑战性，主要源于不同奖励函数导致的策略梯度冲突，阻碍了多技能策略的收敛。</p>
<p>本文针对人形机器人单一技能控制器缺乏行为切换与地形适应性的核心痛点，提出了一种新的两阶段训练框架。核心思路是：首先通过运动先验引导的策略学习和监督蒸馏，获得一个具备基础多行为（如恢复、行走）的控制器；然后利用样本高效的强化学习对该基础策略进行微调，使其适应复杂地形，最终实现自适应的人形控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的自适应人形控制框架包含两个主要阶段：多行为蒸馏和强化学习微调。</p>
<p><img src="https://arxiv.org/html/2511.06371v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：多任务强化学习与本文提出的两阶段框架对比。直接通过多任务强化学习学习多种技能是困难的。因此，我们采用包含行为蒸馏和强化微调的两阶段框架，使人形机器人能够掌握多种技能并泛化到复杂地形。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.06371v2/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：提出的两阶段框架“自适应人形控制”概述。第一阶段，在平地上训练两个独立的基础策略，然后通过蒸馏将它们融合成一个基础的多行为策略。第二阶段，对蒸馏后的策略进行强化微调，采用梯度手术缓解梯度冲突，并利用行为特定的评论家提供更准确的价值估计。</p>
</blockquote>
<p><strong>第一阶段：多行为蒸馏</strong><br>此阶段旨在获得一个基础的多行为策略 π^d，避免直接从零开始进行多任务强化学习。具体步骤如下：</p>
<ol>
<li><strong>训练行为特定基础策略</strong>：使用近端策略优化算法，分别训练两个基础策略。<ul>
<li><strong>摔倒恢复策略 π_r^b</strong>：目标是从各种跌倒姿势（仰卧或俯卧）中稳健站起。为了获得更自然、平滑的恢复动作，引入了基于对抗运动先验的奖励函数，通过一个判别器引导策略模仿参考运动（如利用手臂支撑地面）。同时，采用多评论家架构来稳定优势估计。</li>
<li><strong>行走策略 π_w^b</strong>：目标是在平地上响应速度指令进行类人行走。同样引入了AMP奖励来加速收敛并提升动作自然度。<br>这两个策略在训练时接收特权信息（如地面摩擦系数）和本体感知作为输入。</li>
</ul>
</li>
<li><strong>行为蒸馏</strong>：使用DAgger算法，将上述两个行为特定策略的知识蒸馏到一个基于混合专家模型的多行为策略 π^d 中。该策略仅以本体感知（69维向量，包含角速度、重力向量、速度指令、关节位置/速度、上一时刻动作）作为输入。蒸馏损失函数如公式(4)所示，根据机器人状态（属于恢复状态空间 𝒮_r 或行走状态空间 𝒮_w）分别使用对应教师策略的动作进行监督。此过程不仅整合了行为，还相互增强了鲁棒性（例如，行走策略学会了从近跌倒姿势中恢复）。</li>
</ol>
<p><strong>第二阶段：强化学习微调</strong><br>此阶段将问题形式化为多任务强化学习问题，对蒸馏得到的基础策略 π^d 进行微调，得到最终策略 π^ft，以增强其在复杂地形上的适应性。</p>
<ol>
<li><strong>网络架构与优化</strong>：采用<strong>共享演员网络与行为特定评论家网络</strong>。演员网络参数由 π^d 初始化，并在所有任务间共享；而为恢复和行走任务分别设置独立的评论家网络，以解决不同任务奖励尺度差异导致的优化不平衡问题。</li>
<li><strong>缓解梯度冲突</strong>：尽管使用了行为特定评论家，共享演员在聚合不同任务梯度时仍可能存在冲突。为此，应用了<strong>投影冲突梯度方法</strong>。在每次优化步骤中，计算每对任务梯度，若它们冲突（余弦相似度为负），则将其中一个梯度投影到另一个的法平面上，从而消除冲突方向同时保留在其他子空间上的进展（公式5）。这确保了多任务学习的高效性。</li>
<li><strong>地形课程与奖励</strong>：采用地形课程学习，根据任务表现自动调整地形难度（平地、斜坡、跨栏、离散障碍）。微调时继续使用AMP奖励来保持动作的类人性。</li>
</ol>
<p>与现有方法相比，本文的创新点在于：1) 通过蒸馏而非直接多任务RL来获取基础多行为策略，避免了初始阶段的梯度冲突问题；2) 在微调阶段结合了行为特定评论家和PCGrad，有效解决了多任务优化中的梯度幅度差异与方向冲突双重挑战。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：训练在IsaacGym模拟器中进行，使用4096个并行环境。行为特定策略训练10,000次迭代，蒸馏4,000次迭代，微调10,000次迭代。最终策略部署在Unitree G1人形机器人上，控制频率为50Hz。评估地形包括平地、斜坡、跨栏和离散障碍。</p>
<p><strong>对比基线</strong>：HOMIE（下半身行走策略）、HoST（站起控制器）、本文第一阶段训练的单独恢复策略 π_r^b、单独行走策略 π_w^b 以及蒸馏得到的基础多行为策略 π^d。</p>
<p><strong>关键实验结果</strong>：<br>如表1所示，在<strong>运动任务</strong>中，最终的AHC策略在所有复杂地形（斜坡、跨栏、离散）上的成功率（Succ.）和平均行进距离（Dist.）均显著优于所有基线。例如，在斜坡上成功率从基础多行为策略 π^d 的16.0%提升至97.5%，在离散地形上从70.2%提升至96.9%。这证明了微调阶段对地形适应性的巨大提升。在<strong>恢复任务</strong>中，AHC策略在各类地形上的成功率接近或达到100%，优于HoST等基线。</p>
<p><img src="https://arxiv.org/html/2511.06371v2/img/figure3.png" alt="恢复动作对比"></p>
<blockquote>
<p><strong>图3</strong>：AHC与HoST的恢复动作对比。在仰卧和俯卧场景下，对比了包含AMP的AHC与不包含AMP的HoST。AHC产生了更平滑的恢复行为，凸显了AMP在引导学习自然恢复动作方面的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.06371v2/x3.png" alt="关节加速度分析"></p>
<blockquote>
<p><strong>图4</strong>：恢复过程中左腿关节加速度分析。左腿髋关节和膝关节的加速度曲线显示，与HoST相比，我们的AHC策略带来了更稳定的关节驱动，突变的波动明显更少。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>研究验证了微调阶段两个关键组件（PCGrad和行为特定评论家）的作用。</p>
<p><img src="https://arxiv.org/html/2511.06371v2/img/figure5.png" alt="价值损失曲线"></p>
<blockquote>
<p><strong>图5</strong>：第二阶段微调期间的价值损失曲线。配备行为特定评论家的策略（AHC-BC-w/o-PC和AHC）相比共享评论家的变体（AHC-SC），表现出更稳定的价值学习。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.06371v2/x4.png" alt="训练回报曲线"></p>
<blockquote>
<p><strong>图6</strong>：第二阶段微调期间的训练回合回报曲线。使用PCGrad和行为特定评论家的AHC在两个任务上取得了更高且更平衡的回报。</p>
</blockquote>
<p>表2显示了不同配置下的任务梯度余弦相似度（越高表示冲突越小）。结果表明，<strong>PCGrad</strong>和<strong>行为特定评论家</strong>都能有效提高相似度（即减少冲突），同时使用两者时效果最佳（相似度0.535）。图5和图6进一步表明，行为特定评论家有助于获得更低、更稳定的价值损失，而PCGrad则确保了策略在两个任务上能平衡、快速地学习。</p>
<p><strong>真实世界部署</strong>：</p>
<p><img src="https://arxiv.org/html/2511.06371v2/x5.png" alt="真实世界部署"></p>
<blockquote>
<p><strong>图7</strong>：真实世界部署快照。机器人在多样化场景中执行恢复和运动，包括在斜坡地形上从俯卧和仰卧姿势站起，以及在行走过程中受到外部推力后恢复。</p>
</blockquote>
<p>如图7所示，训练好的策略无需额外微调即成功部署到Unitree G1机器人上。机器人能够在斜坡地形上从不同跌倒姿势站起，并在行走时承受外部扰动后恢复平衡，验证了方法的鲁棒性和泛化能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的两阶段训练框架（AHC），通过“多行为蒸馏”加“强化微调”的范式，解决了直接训练多技能、多地形自适应策略的难题。</li>
<li>在微调阶段，创新性地结合了行为特定评论家和PCGrad梯度手术，有效缓解了多任务强化学习中奖励尺度差异和梯度方向冲突两大挑战，实现了平衡高效的学习。</li>
<li>在模拟和真实机器人（Unitree G1）上进行了广泛实验，验证了所提方法在多种复杂地形上实现自适应行为切换和鲁棒运动的有效性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前工作主要集中于行走和恢复两种基本行为。未来可以探索将框架扩展到更多样化的技能上。</p>
<p><strong>启示</strong>：本文的工作表明，将模仿学习（通过AMP和蒸馏）与强化学习微调相结合，是获得兼具自然性、多样性和鲁棒性的人形机器人控制器的有效途径。其针对多任务优化的技术（特定评论家、梯度手术）对于其他需要共享策略学习多个子任务的机器人控制问题也具有借鉴意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人需为不同技能（如站立、行走）独立训练策略、导致泛化能力弱和地形适应性差的问题，提出自适应人形控制（AHC）的两阶段框架。首先通过多行为蒸馏，将多个基础策略融合为能自适应切换行为的基础控制器；随后利用在线反馈进行强化微调，提升其在复杂地形上的适应能力。在仿真与Unitree G1实物机器人上的实验表明，该方法在各种地形与场景中均表现出强大的适应性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.06371" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>