<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.22402" target="_blank" rel="noreferrer">2509.22402</a></span>
        <span>作者: Yang Yu Team</span>
        <span>日期: 2025-09-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在视觉机器人操作的强化学习中，奖励设计是一个关键瓶颈。在模拟环境中，通常基于机器人末端执行器与目标位置之间的欧氏距离来设计密集奖励。然而，在真实世界的视觉设置中，由于感知噪声、遮挡和模糊性，精确的几何状态信息通常无法获得，使得手工设计奖励变得困难且难以扩展。一些先前的工作通过从观察中学习的方法来克服这一限制，例如采用不依赖动作输入的对抗性判别器作为奖励函数，但面对高维视觉输入时存在训练困难和稳定性问题。另一些基于启发式策略的视觉奖励方法，要么产生稀疏奖励，要么缺乏明确的结构化学习过程，在具有长期视野或复杂动态的任务中效率低下。因此，需要一个能够从易于获取的无动作视频演示中自动合成信息丰富的密集奖励信号，同时通过结构化的、基于几何的学习课程来指导智能体的框架。本文提出ReLAM（Reward Learning with Anticipation Model），其核心思路是：利用从视频中提取的物体关键点作为任务的中间几何表示，训练一个能够预测通往最终目标路径上基于关键点的中间子目标的“预期模型”，从而为分层强化学习中的底层策略生成结构化、密集的奖励。</p>
<h2 id="方法详解">方法详解</h2>
<p>ReLAM方法分为两个阶段：1) 从视频演示中学习一个预期模型；2) 基于预期模型生成的子目标，设计密集奖励来训练底层目标条件策略。整体框架如下图所示。</p>
<p><img src="https://arxiv.org/html/2509.22402v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：ReLAM方法的整体训练框架。(A) 从视频初始帧选取代表性关键点，并沿视频选择关键帧，将这些帧中的关键点位置转化为子目标。(B) 基于生成的子目标数据集训练预期模型。(C) 利用预期模型生成的子目标，基于点距离的奖励来训练策略。</p>
</blockquote>
<p><strong>第一阶段：基于关键点的预期模型学习</strong><br>该阶段旨在从视频演示中学习一个能预测中间子目标（即关键点配置）的模型。具体包括三个步骤：</p>
<ol>
<li><strong>关键点选择</strong>：为了选取有代表性的关键点，ReLAM首先使用接地气的SAM模型分割出初始帧中的任务相关物体。然后，对每个关键物体对应的像素，使用跟踪模型在整个视频中追踪其运动轨迹。接着，通过预设位移阈值过滤掉运动范围可忽略的像素，最后对剩余像素使用最远点采样选出最终的关键点集。公式化表示为：$\mathcal{P}=\mathbf{FPS}\left({p=(x,y)\in SAM(I_{0}):\max_{0\leq t,t^{\prime}\leq T}(x_{t}-x_{t^{\prime}})^{2}+(y_{t}-y_{t^{\prime}})^{2}\geq\Theta}\right)$。</li>
<li><strong>关键帧选择与子目标数据集生成</strong>：ReLAM假设机器人操作任务可分解为多个“线性运动”段。关键帧位于连续线性运动之间的过渡点。通过追踪关键点的运动，在预设的步长范围$[m, M]$内，根据相邻时间步位移向量之间的夹角变化来识别关键帧：夹角接近零表示处于线性运动中，夹角显著增大则表明是边界关键帧。公式为：$t_{j}=\arg\min_{,t\in[t_{j-1}+m,;t_{j-1}+M]};\sum_{k=1}^{K}\frac{\langle p_{t}^{k}-p_{t-1}^{k},p_{t+1}^{k}-p_{t}^{k}\rangle}{|p_{t}^{k}-p_{t-1}^{k}||p_{t+1}^{k}-p_{t}^{k}|}$。将关键点在关键帧中的坐标记录下来，构成子目标数据集$\mathcal{K}$。</li>
<li><strong>预期模型训练</strong>：预期模型采用自回归的因果Transformer结构，以任务的初始视觉观察$I_0$（和可选的恒定任务指示帧$I_{task}$）为输入。模型首先识别$I_0$中的关键点坐标$P_0$，然后基于$I_0$和$P_0$自回归地预测后续关键帧中这些关键点的坐标。图像通过冻结的DINOv2模型提取patch嵌入，关键点坐标经归一化和MLP映射后与图像嵌入拼接，送入12层因果Transformer块。模型在教师强制策略下，使用预测坐标与真实坐标之间的均方误差损失进行训练。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.22402v1/x3.png" alt="预期模型结构"></p>
<blockquote>
<p><strong>图3</strong>：用于生成子目标的预期模型结构。输入为初始帧和任务指示帧的图像嵌入，以及历史关键点坐标的嵌入（第一步用特殊标记），通过因果Transformer自回归预测下一关键帧的关键点坐标。</p>
</blockquote>
<p><strong>第二阶段：基于点距离奖励的策略学习</strong><br>在分层强化学习框架下训练策略。每个回合开始时，将初始图像输入训练好的预期模型，模型会生成一个子目标序列$P_1, \cdots, P_k$。奖励设计鼓励当前关键点位置$P_0$依次向这些子目标移动。基于线性运动假设，从$P_{j-1}$到$P_j$的移动近似直线，因此奖励可直接基于像素坐标系中的欧氏距离定义。对于第$j$阶段，奖励由密集奖励$r_{\text{dense}}$、阶段成功奖励$r_{\text{success}}$和最终成功奖励构成。密集奖励通过一个分段线性函数将当前关键点与目标子目标$P_j$的平均距离$l$（公式$l=\frac{1}{K}\sum_{k=1}^{K}|p^{k}-p_{j}^{k}|_{2}$）转化为正向信号。当距离小于阈值$\theta_s$时，视为达成当前子目标，进入下一阶段并给予阶段成功奖励。论文在附录中提供了该方法能找到接近最短路径的策略的数学证明。ReLAM可同时适用于在线和离线强化学习设置。</p>
<p><strong>创新点</strong>：与现有方法相比，ReLAM的核心创新在于：1) 提出使用经过筛选和跟踪的物体关键点作为高度抽象且几何基础的任务表示，极大降低了子目标生成的难度，并增强了可解释性；2) 引入了“预期模型”作为高层规划器，通过分析演示视频中的运动规律，自动分解任务并生成结构化的子目标序列，为策略学习提供了与几何目标对齐的课程；3) 基于关键点距离设计了密集且结构化的奖励函数，并在HRL框架下提供了可证明的次优性边界。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准与数据集</strong>：在两个机器人操作仿真环境进行评估：Meta-World（在线RL）和ManiSkill（离线RL）。具体任务包括Meta-World的Drawer Open、Door Open、Button Press Wall，以及ManiSkill的Push Cube、Pick Cube。观测均为256x256像素的固定视角第三人称相机图像。</li>
<li><strong>训练数据</strong>：每个任务收集100条无动作视频演示用于训练预期模型。对于离线RL设置，额外收集每个任务200条包含动作的轨迹（100条专家演示，100条添加随机噪声的演示）作为离线数据集。</li>
<li><strong>基线方法</strong>：对比了四种从视频学习奖励的代表性方法：DACfO（对抗性模仿学习）、Diffusion Reward（使用扩散模型的条件熵作为奖励）、Image Subgoal（使用流匹配模型生成图像子目标，并用DINOv2特征空间余弦相似度作为奖励）、Oracle（使用真实图像子目标的不可实现基线）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2509.22402v1/x4.png" alt="Meta-World结果"></p>
<blockquote>
<p><strong>图4</strong>：不同方法在Meta-World任务上的性能。(a) Drawer Open, (b) Door Open, (c) Button Press Wall。x轴为环境交互步数，y轴为平均成功率（评估30回合）。ReLAM在所有任务上均显著优于其他基线，能快速达到接近100%的成功率。DACfO曲线波动大，训练不稳定；Diffusion Reward在某些种子下完全失败；Image Subgoal因生成图像噪声大、奖励阈值不一致导致成功率很低。</p>
</blockquote>
<p>在Meta-World的在线RL实验中，ReLAM在Drawer Open、Door Open和Button Press Wall任务上均大幅领先所有基线。它能快速学习，达到接近100%的成功率。相比之下，DACfO训练不稳定，曲线波动剧烈；Diffusion Reward依赖辅助探索奖励，导致某些随机种子完全学不会；Image Subgoal由于生成的图像子目标存在噪声和模糊，在特征空间中难以设定一致的相似度阈值，性能很差。</p>
<p><img src="https://arxiv.org/html/2509.22402v1/x5.png" alt="ManiSkill结果与奖励可视化"></p>
<blockquote>
<p><strong>图5</strong>：(a),(b): 不同方法在ManiSkill任务上的离线RL性能。ReLAM性能与使用真实图像子目标的Oracle基线相当，并显著优于其他方法。(c): 不同方法生成奖励的t-SNE投影。ReLAM的奖励在状态空间中形成了清晰的结构化层级，而DACfO和Diffusion Reward的奖励则混杂在一起。</p>
</blockquote>
<p>在ManiSkill的离线RL实验中，ReLAM在Pick Cube和Push Cube任务上的性能与Oracle基线相当，并明显优于其他基线。这表明即使没有特权信息，通过基于关键点的抽象表示，ReLAM不仅能降低生成难度，还能有效捕捉任务的结构信息来指导策略。Image Subgoal在此性能优于Meta-World，主要因为离线数据集包含专家动作且任务子目标序列更短，减少了生成误差累积。</p>
<p><strong>消融实验</strong>：</p>
<p><img src="https://arxiv.org/html/2509.22402v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：(a) 不同奖励函数设计的消融研究。使用分段线性函数的密集奖励（ReLAM）效果最佳。(b) 不同关键点选择策略的对比。ReLAM采用的策略（SAM+跟踪+FPS）优于ATM的平均采样策略。</p>
</blockquote>
<p>消融实验表明：1) 奖励函数设计中，使用分段线性函数将关键点距离转化为密集奖励的方式效果最好（图6a）。2) 关键点选择策略上，ReLAM采用的结合SAM、跟踪模型和FPS的策略，显著优于ATM中简单的平均像素采样策略（图6b）。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ReLAM，一个从无动作视频演示中自动生成密集、结构化奖励的新框架，有效解决了视觉机器人操作中奖励设计的瓶颈问题。</li>
<li>引入了基于关键点的“预期模型”作为高层规划器，能够预测通往最终目标的几何合理的中间子目标序列，为策略学习创建了结构化的课程。</li>
<li>在分层强化学习框架下，基于关键点距离设计了奖励函数，并提供了可证明的次优性边界。在复杂的长视野操作任务上，ReLAM显著加速了学习过程并取得了最先进的性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，ReLAM的性能依赖于关键点选择和跟踪模型的有效性。对于非刚性物体或纹理非常缺乏的物体，关键点的跟踪可能变得困难。此外，方法假设任务可分解为线性运动段，这可能不适用于所有类型的复杂操作。</p>
<p><strong>启示</strong>：ReLAM展示了使用物体关键点作为中间表示在视觉RL中的强大潜力，它将高维图像空间的任务简化为低维几何空间中的点对点运动，从而提高了奖励的泛化性和可解释性。这项工作为从视频演示中学习结构化奖励提供了一条新路径，强调了结合几何先验和分层规划对于解决长视野任务的重要性。未来的研究可以探索更鲁棒的关键点提取与跟踪方法，或将此框架扩展到更动态、非结构化的真实世界环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉机器人操作中奖励设计困难的核心问题，提出ReLAM框架。该方法首先从图像提取关键点以隐式推断空间距离，进而学习一个Anticipation Model作为规划器，在最优路径上生成基于关键点的结构化中间子目标，构建与任务几何目标直接对齐的学习课程。随后，在分层强化学习框架下，依据这些子目标提供连续奖励信号来训练底层策略。实验表明，ReLAM在复杂长视野操作任务上显著加速学习，并取得了优于现有方法的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.22402" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>