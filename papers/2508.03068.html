<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.03068" target="_blank" rel="noreferrer">2508.03068</a></span>
        <span>作者: C. Karen Liu Team</span>
        <span>日期: 2025-08-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，人形机器人执行任务的主流方法通常将导航和伸手视为独立问题。对于导航，现有视觉导航方法常将机器人抽象为二维平面上的质点，仅规划地面移动。对于伸手，则关注机械臂的控制。将两者直接结合的策略往往导致上身操作与下身移动在空间上隔离，或缺乏从导航到伸手行为的无缝协调过渡。此外，端到端地学习所有技能需要同时包含自我中心视觉和全身运动的异构人类数据，训练挑战巨大。</p>
<p>本文针对人形机器人如何协调地利用其类人形态，在复杂三维环境中同时完成导航、移动和伸手这一具体痛点，提出了一种新视角：将导航和伸手统一为对机器人“眼睛”（相机）和“双手”的位姿追踪问题。本文的核心思路是采用模块化方法，将高层（感知与规划）与底层（全身运动控制）解耦，高层策略根据视觉感知预测手眼目标位姿，底层控制器则学习从大规模人类运动数据中追踪这些稀疏目标，从而实现协调的全身控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>HEAD框架是一个模块化系统，由高层策略和底层全身控制器构成。高层策略包含导航模块和伸手模块，其输入是机器人导航相机捕获的RGB图像以及图像中用户指定的2D目标点，输出是未来一段时间内机器人眼睛（相机）和双手的6D位姿目标序列。底层全身控制器以更高的频率运行，接收这些目标位姿，并输出关节力矩指令，控制机器人全身运动以追踪目标。</p>
<p><img src="https://arxiv.org/html/2508.03068v2/images/overview.png" alt="系统总览"></p>
<blockquote>
<p><strong>图2</strong>：HEAD系统概览。系统由高层策略（包含导航和伸手模块）和底层全身控制器组成。高层策略以较低频率提供手眼追踪目标，而全身控制器以较高频率追踪这些目标。基于学习的导航模块从混合训练数据集中学习，将RGB自我中心视觉映射到相机目标轨迹。基于模型的伸手模块生成手眼目标位姿。底层全身控制器在人类运动捕捉数据上使用模仿强化学习进行训练。</p>
</blockquote>
<p><strong>核心模块一：全身控制器</strong>。该控制器负责执行高层指令，其核心是训练一个能够追踪三个关键点（双眼、左手、右手）目标位姿的策略。为了获得类人行为并处理任意目标，该方法采用了一种基于GAN的模仿强化学习方法，从非结构化的运动数据中学习。</p>
<ul>
<li><strong>数据准备</strong>：从AMASS和OMOMO数据集中，通过关键点匹配将5小时的人类运动捕捉数据重定向到Unitree G1机器人，构成训练数据集。</li>
<li><strong>可部署的观测空间</strong>：策略观测仅包含机器人局部坐标系下的连杆位姿（连续两帧）和关节速度，不依赖世界坐标系下的根骨位置和线速度等特权信息，以支持真实世界部署。</li>
<li><strong>运动模仿</strong>：将全身运动解耦为上、下半身两组，并<strong>同时使用两个独立的判别器</strong>进行模仿学习。这使得策略能够学习上下半身姿势的组合，而非受限于数据集中固定的全身姿势。</li>
<li><strong>稀疏目标追踪</strong>：为避免引入全局空间定义的目标信息，策略网络的输入是目标与当前手眼位姿的相对变换。训练时，在每个时间步执行动作后，基于此相对变换设计目标导向的奖励。</li>
<li><strong>仿真到现实</strong>：除了任务奖励，还定义了正则化项，并在训练时对动力学参数和传感器噪声进行广泛的域随机化。使用多目标学习框架同时优化由判别器提供的模仿目标和手动设计的目标追踪奖励。</li>
</ul>
<p><strong>核心模块二：导航模块</strong>。该模块引导机器人到达初始图像中指定的2D目标点。推理时，模型接收当前RGB图像和通过点追踪器跟踪的2D目标，预测未来眼睛（相机）的位置和朝向轨迹。</p>
<p><img src="https://arxiv.org/html/2508.03068v2/images/navigation_module.png" alt="导航训练数据与方法"></p>
<blockquote>
<p><strong>图3</strong>：导航训练数据（左）与导航模块概述（右）。左：对Aria眼镜采集的图像进行去畸变和单应性变换，使其 resemble 机器人视图。右：推理时，给定图像和作为2D点的目标，提取DINO特征，附加目标坐标，馈入Transformer解码器以预测未来的眼睛（相机）轨迹。</p>
</blockquote>
<ul>
<li><strong>架构</strong>：提取输入图像的DINO特征，为2D目标点添加位置编码，然后通过Transformer解码器输出相对于前一帧的未来相机轨迹。</li>
<li><strong>数据收集与域偏移处理</strong>：使用Aria眼镜自动收集以目标为条件的人类训练数据（图像、未来相机轨迹、2D目标）。为应对域偏移：1) 引入大规模自我中心数据集Aria Digital Twin (ADT)以提升对新场景的泛化能力；2) 对Aria的鱼眼视图进行去畸变和单应性变换，生成虚拟的机器人视角；3) 由于机器人移动速度远慢于人类，对训练视频进行降采样；4) 收集少量机器人自身数据（用动捕系统记录其头部位姿），并与人类数据<strong>共同训练</strong>导航模型。</li>
</ul>
<p><strong>核心模块三：伸手模块</strong>。当目标物体进入朝下的RGB-D相机视野且处于可触及范围内时，高层策略从导航切换至伸手模块。</p>
<p><img src="https://arxiv.org/html/2508.03068v2/images/transition.png" alt="导航到伸手切换"></p>
<blockquote>
<p><strong>图4</strong>：机器人在t0时刻远离目标，目标仅在导航相机中可见。在切换时间t1，目标进入伸手相机视野且距离机器人足够近。</p>
</blockquote>
<ul>
<li><strong>切换</strong>：通过对应匹配将目标从导航帧转换到RGB-D帧。</li>
<li><strong>目标生成</strong>：将追踪的目标近似为3D手部位置，并使用逆运动学求解器Mink计算缺失的头部位姿和手部朝向。为确保平滑过渡和自然姿态，IK优化从当前机器人状态初始化，并添加鼓励质心位置和骨盆朝向变化小的目标项。</li>
</ul>
<p><strong>创新点总结</strong>：1) <strong>统一的3点追踪接口</strong>：将导航和伸手统一为对眼睛和双手位姿的追踪，实现了全身协调控制。2) <strong>模块化学习架构</strong>：解耦感知与动作，允许高层和底层使用不同来源的数据（人类视觉探索数据、运动捕捉数据、机器人数据）和算法独立、高效地训练。3) <strong>混合数据训练策略</strong>：针对导航模块，结合大规模人类数据（提升泛化）、中尺度场景内演示（缓解感知域偏移）和少量机器人经验（缓解具身鸿沟），有效应对了域适应挑战。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用Unitree G1人形机器人，配备广角USB网络摄像头（导航）和内置RealSense D435 RGB-D相机（伸手）。在实验室（训练室）和厨房（部署室）两个真实房间中进行评估，房间内布置了多样化的家具布局。所有测试布局和物体均为未见过的。</p>
<p><strong>整体系统性能</strong>：在两种环境的六种不同布局中，评估机器人抓取放置在不同位置和高度的物体。成功标准是机器人用手触碰到目标物体。</p>
<p><img src="https://arxiv.org/html/2508.03068v2/images/setup.png" alt="硬件设置与测试环境"></p>
<blockquote>
<p><strong>图5</strong>：硬件设置与测试环境。(a) Unitree G1机器人及传感器配置；(b) 实验室与厨房环境；(c) 不同高度（地面、椅子、桌子、架子）的测试物体示例。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">Room</th>
<th align="left">Scene</th>
<th align="left">Success rate</th>
<th align="left">Number of misses</th>
<th align="left">Number of collision</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Lab room</td>
<td align="left">Scene 1</td>
<td align="left">3/4</td>
<td align="left">1/4</td>
<td align="left">0/4</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Scene 2</td>
<td align="left">3/4</td>
<td align="left">1/4</td>
<td align="left">0/4</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Scene 3</td>
<td align="left">4/4</td>
<td align="left">0/4</td>
<td align="left">0/4</td>
</tr>
<tr>
<td align="left">Kitchen</td>
<td align="left">Scene 1</td>
<td align="left">2/4</td>
<td align="left">1/4</td>
<td align="left">1/4</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Scene 2</td>
<td align="left">2/4</td>
<td align="left">1/4</td>
<td align="left">1/4</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Scene 3</td>
<td align="left">3/4</td>
<td align="left">0/4</td>
<td align="left">1/4</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：在不同评估场景中的成功次数和各种失败次数。整体成功率为71%，实验室成功率（83%）高于厨房（58%）。失败主要源于目标追踪丢失、运动模糊导致导航错误，以及人类数据路径对机器人而言过于激进导致的碰撞。</p>
</blockquote>
<p><strong>全身控制器消融实验</strong>：在仿真中评估控制器设计选择。性能通过位置误差、朝向误差和失败率（头部高度偏离目标≥0.4米）衡量。</p>
<table>
<thead>
<tr>
<th align="left">Design Choice</th>
<th align="left">Isaac Gym - Unseen Motions</th>
<th align="left">MuJoCo - Sim2Sim Transfer</th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td align="left">Pos Error [m] ↓</td>
<td align="left">Quat Error [rad] ↓</td>
</tr>
<tr>
<td align="left">Ours</td>
<td align="left">0.075</td>
<td align="left">0.120</td>
</tr>
<tr>
<td align="left">Single discriminator</td>
<td align="left">0.149</td>
<td align="left">0.169</td>
</tr>
<tr>
<td align="left">No discriminator</td>
<td align="left">0.540</td>
<td align="left">1.138</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表2</strong>：全身控制器消融实验结果。使用两个独立判别器的配置（Ours）在追踪精度和跨仿真器鲁棒性上均显著优于使用单个全身判别器或无判别器的变体。这表明解耦上下半身奖励有助于策略学习更组合、更协调的运动。</p>
</blockquote>
<p><strong>导航模块消融实验</strong>：评估不同数据组合对导航模块开环预测性能的影响。指标是成功率和平均位置误差（成功阈值0.6米）。</p>
<table>
<thead>
<tr>
<th align="left">Training Data</th>
<th align="left">Lab Room</th>
<th align="left">Kitchen (Deploy Room)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td align="left">SR</td>
<td align="left">Error</td>
</tr>
<tr>
<td align="left">H-Lab</td>
<td align="left">0.14</td>
<td align="left">0.704</td>
</tr>
<tr>
<td align="left">R-Lab</td>
<td align="left">0.71</td>
<td align="left">0.427</td>
</tr>
<tr>
<td align="left">R-Lab + O</td>
<td align="left">0.79</td>
<td align="left">0.399</td>
</tr>
<tr>
<td align="left">R-Lab + H-Lab</td>
<td align="left">0.79</td>
<td align="left">0.374</td>
</tr>
<tr>
<td align="left">2-branch R-Lab + H-Lab + O</td>
<td align="left">0.79</td>
<td align="left">0.356</td>
</tr>
<tr>
<td align="left">shared R-Lab + H-Lab + O (Ours)</td>
<td align="left"><strong>0.86</strong></td>
<td align="left"><strong>0.380</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表3</strong>：导航模块消融实验结果。在实验室，结合机器人数据、室内人类数据和大规模外部人类数据（Ours）取得了最佳性能。在部署室（厨房），当没有机器人训练数据时，加入大规模外部人类数据（O）能显著提升泛化性能（成功率从0.35提升至0.60）。此外，使用共享解码分支比使用独立分支（2-branch）更有利于场景泛化。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了HEAD框架，通过统一的“手-眼”位姿追踪接口，首次实现了人形机器人在真实复杂环境中导航、移动和伸手技能的协调与整合。2) 设计了一种模块化学习范式，有效解耦了自我中心感知与物理动作，使系统能够灵活利用不同来源和模态的人类数据进行高效训练。3) 实证了混合数据策略（大规模人类数据、场景内人类演示、少量机器人数据）对于解决视觉导航中域偏移和具身鸿沟问题的关键作用。</p>
<p><strong>局限性</strong>：1) 控制接口仅使用头和手腕位姿，对于控制下半身存在内在歧义（例如，难以区分“弯腰”和“向前走”的意图），可能导致机器人犹豫。2) 方法仅规划头部轨迹，未显式考虑身体其他部位的避障，在复杂狭窄环境中可能无法充分利用人形机器人的敏捷性，采取更保守的路径。</p>
<p><strong>研究启示</strong>：1) 未来的工作可以探索引入更多可从自我中心设备估计的信息（如脚部位姿），以减少控制歧义。2) 开发能够感知并避免身体各部位碰撞的更精细的全身导航能力，是人形机器人在真实生活环境中实用的重要方向。3) HEAD的模块化设计和3点追踪接口为整合更复杂的操作技能（如抓取）提供了一个可扩展的基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出HEAD框架，解决人形机器人协调导航、运动与抓取的统一控制问题。采用模块化方法：高层策略从人类视觉数据学习，预测手眼目标位姿；底层控制器从运动捕捉数据学习，实现全身运动跟踪。该方法将视觉感知与动作解耦，提升了学习效率与场景泛化能力。实验在仿真与真实环境中验证了机器人在复杂人机环境中的自主导航与抓取能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.03068" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>