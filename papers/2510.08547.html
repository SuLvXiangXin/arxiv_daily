<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.08547" target="_blank" rel="noreferrer">2510.08547</a></span>
        <span>作者: Jiwen Lu Team</span>
        <span>日期: 2025-10-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通过模仿学习训练视动策略已成为学习灵巧操作技能的主流方法，但其高度依赖大量人类收集的演示数据。为了实现空间泛化（即策略在不同物体、环境和机器人自身空间分布下鲁棒工作），通常需要收集覆盖各种空间配置的海量演示，过程繁琐低效。为减少人力，先前工作探索了利用数据生成从少量源演示中获取空间多样化数据的途径。例如，MimicGen及其后续工作在模拟器中合成不同的执行计划，但需要耗时的机上重放以获取真实世界的观察-动作对，存在模拟到现实的差距。近期，DemoGen提出直接在3D域内增强物体点云以合成轨迹，无需模拟器和渲染，效率高且避免了模拟到现实的差距。然而，DemoGen存在几个关键局限性：1）仅关注固定基座场景，未考虑视角变化；2）对输入数据有强假设（需裁剪环境点云、最多支持2个物体、每个技能仅涉及一个目标物体）；3）存在视觉不匹配问题，即大的增强会导致不完整的点云观察。这些限制使其无法完全实现实用的实到实生成，难以处理移动操作和多样的任务配置。</p>
<p>本文针对上述痛点，提出了R2RGen，一个实到实的3D数据生成框架。其核心思路是：给定一个人类收集的源演示，通过精细的场景和轨迹解析，提出分组式数据增强策略以处理复杂的多物体组合和任务约束，并引入相机感知后处理使生成的点云数据分布与真实世界3D传感器对齐，从而直接、高效地生成多样化的真实世界观察-动作对。</p>
<h2 id="方法详解">方法详解</h2>
<p>R2RGen的整体流程分为三个阶段：1）源演示预处理，对点云观察和动作轨迹进行精细解析；2）分组式数据增强，基于解析结果对物体组和轨迹进行空间变换与运动规划；3）相机感知3D后处理，调整生成的点云以匹配真实RGB-D相机的观测分布。</p>
<p><img src="https://arxiv.org/html/2510.08547v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：R2RGen是一个无需模拟器的数据生成框架。给定一个人类收集的演示，R2RGen直接在共享的3D空间中对点云观察和动作轨迹进行解析和编辑。R2RGen在多样复杂的任务上实现了强大的空间泛化。</p>
</blockquote>
<p><strong>核心模块一：源演示预处理</strong><br>目标是将点云观察解析为可编辑的物体组合，并将动作轨迹解析为交替的运动段和技能段。</p>
<ul>
<li><strong>场景解析</strong>：首先，在首帧RGB图像中分割出K个相关物体，并通过基于模板的3D物体跟踪系统进行跟踪和补全，获得所有帧中每个物体的完整点云序列。环境点云通过在收集演示前移除物体获得，并视为静态。机械臂点云通过从原始观察中减去补全后的物体和环境点云得到。</li>
<li><strong>轨迹解析</strong>：通过一个轻量级标注系统，标注者观看RGB视频，标记每个技能段的起始帧和结束帧，并指定每个技能相关的目标物体ID和手中物体ID（如有）。整个过程仅需RGB视频输入，每次演示耗时少于60秒。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.08547v1/x2.png" alt="预处理结果"></p>
<blockquote>
<p><strong>图2</strong>：预处理结果。3D场景被解析为完整的物体、环境和机械臂。轨迹被解析为交替的运动段和技能段。</p>
</blockquote>
<p><strong>核心模块二：分组式数据增强</strong><br>为解决复杂技能涉及多物体空间关系的问题，提出了分组式增强策略，将每个技能关联到一个物体组（包含标注的目标物体和手中物体），而非单个目标物体。</p>
<p><img src="https://arxiv.org/html/2510.08547v1/x3.png" alt="管道流程"></p>
<blockquote>
<p><strong>图3</strong>：R2RGen的流程。给定处理后的源演示，我们回溯技能并应用分组增强以保持目标物体间的空间关系，其中维护一个固定物体集来判断增强是否适用。然后执行运动规划以生成连接相邻技能的轨迹。增强后，我们执行相机感知处理以使点云遵循RGB-D相机的分布。实线箭头表示处理流程，虚线箭头表示固定物体集的更新。</p>
</blockquote>
<ul>
<li><strong>分组回溯</strong>：从最后一个技能开始向前回溯。对于当前技能关联的物体组，如果其与“固定物体集”（当前不可被增强的物体）的交集为空，则对该组所有物体采样一个相同的SE(3)变换矩阵（主要是XY平面平移和Z轴旋转）进行增强。然后更新固定物体集：将当前组的目标物体加入固定集，而手中物体被释放（因为其被抓取前的状态独立于当前技能的约束）。以此方式确保增强不会破坏后续技能所需的空间约束。</li>
<li><strong>技能增强</strong>：如果技能组被增强，则将该变换同样应用于该技能段内所有时间步的末端执行器位姿、组内物体点云和机械臂点云。</li>
<li><strong>运动增强</strong>：对于连接两个技能段的运动段，使用运动规划器生成一条从上一个技能段结束位姿到下一个技能段开始位姿的新轨迹。手中物体和机械臂的点云根据末端执行器相对位姿变换进行相应变换。</li>
<li><strong>环境增强</strong>：最后，对所有帧的环境点云应用一个随机变换，以模拟机器人视角的变化。</li>
</ul>
<p><strong>核心模块三：相机感知3D后处理</strong><br>为使生成的点云观察与真实RGB-D相机观测分布一致，设计了后处理流程：将生成的点云投影到图像平面，进行裁剪（移除图像边界外的像素）、Z-buffer处理（移除当前视角下被遮挡的点，采用基于图像块邻域的策略）和填充（处理因环境增强导致的图像边界附近空洞）。最后将处理后的像素投影回相机坐标系。</p>
<p><strong>创新点</strong>：与DemoGen等先前方法相比，R2RGen的核心创新在于1）分组式增强策略，支持涉及多物体空间关系的复杂技能；2）相机感知后处理，有效缓解视觉不匹配问题，支持大范围视角变化；3）整体框架支持任意数量物体和交互模式，适用于移动操作场景。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在真实世界中设计了8个代表性任务进行评估，包括2个简单单臂任务（开罐、放瓶）、4个复杂单臂任务（锅中放食、挂杯、堆砖、建桥）和2个双臂任务（抓盒子、存物品）。</li>
<li><strong>实验平台</strong>：使用单臂UR5机器人和基于MobileAloha架构的双臂移动操作平台。</li>
<li><strong>对比方法</strong>：主要与DemoGen对比（仅限其能处理的简单和双臂任务），并与使用不同数量（1, 10, 25, 40）人类演示训练的策略进行对比。</li>
<li><strong>策略</strong>：使用iDP3作为视动策略，输入为以自我为中心的点云。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>表1展示了空间泛化的真实世界评估结果（成功率）。仅使用1个人类演示训练的策略泛化能力极差。R2RGen仅用1个源演示生成数据，其策略性能显著优于DemoGen，并且在所有任务上达到了与使用25个人类演示训练的策略相当甚至更优的水平（平均成功率40.3% vs. 41.0%），在几个困难任务上甚至超过了40个人类演示的性能。</p>
<blockquote>
<p><strong>表1</strong>：R2RGen空间泛化的真实世界评估。报告成功率。R2RGen（1个源演示）在各项任务上均优于DemoGen，并与25个源演示的性能相当。</p>
</blockquote>
<p><strong>性能-标注权衡</strong>：<br>图5研究了R2RGen随源演示数量增加时的扩展性。结果表明，随着生成演示数量的增加，策略性能逐渐饱和（受限于轻量级策略的容量）。同时，使用更多源演示能带来更高的饱和性能，证明R2RGen能有效利用额外数据提升效果。</p>
<p><img src="https://arxiv.org/html/2510.08547v1/x5.png" alt="性能扩展"></p>
<blockquote>
<p><strong>图5</strong>：生成演示数量和源演示数量对R2RGen最终性能的影响。性能随生成数据量增加而饱和，更多源演示带来更高饱和性能。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>在Place-Bottle任务上进行了详细的消融研究。</p>
<ul>
<li><strong>点云处理</strong>（表2）：移除点云补全会导致在大空间变化下数据不真实，性能大幅下降至12.5%；移除环境点云或环境增强也会显著降低性能。</li>
<li><strong>相机感知处理</strong>（表3）：移除裁剪、Z-buffer或填充操作均会导致性能下降，其中Z-buffer对确保观察合理性最为关键。</li>
</ul>
<blockquote>
<p><strong>表2</strong>：点云处理的效果。完整的处理流程（R2RGen）取得了50.0%的最高成功率。<br><strong>表3</strong>：相机感知处理的效果。完整的后处理流程对性能至关重要。</p>
</blockquote>
<p><strong>定性结果</strong>：<br>图6展示了填充操作的两种实现（收缩图像或扩展环境点云），以处理图像边界空洞。</p>
<p><img src="https://arxiv.org/html/2510.08547v1/x6.png" alt="填充操作"></p>
<blockquote>
<p><strong>图6</strong>：填充操作的两种实现，即收缩和扩展。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了R2RGen，一个无需模拟器和渲染的实到实3D数据生成框架，能够直接从单个（或少量）人类演示中高效生成大量空间多样化的真实世界点云观察-动作对。</li>
<li>提出了分组式数据增强策略，通过回溯机制和物体组概念，支持涉及复杂多物体空间关系的技能，突破了现有方法对象中心范式的限制。</li>
<li>设计了相机感知3D后处理流程，有效对齐生成点云与真实RGB-D传感器分布，解决了视觉不匹配问题，支持移动操作中的视角变化。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1）视动策略仅预测机械臂动作，移动基座在每个单独任务执行期间保持固定；2）视觉观察由固定在基座上的RGB-D相机捕获，在执行过程中不变。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>R2RGen展示了在3D域直接操作数据的有效性，为高效数据生成提供了新范式。后续工作可探索结合更强大的3D基础模型进行场景理解和轨迹生成。</li>
<li>分组增强策略对复杂任务约束的处理具有启发性，可进一步扩展到更动态或语义化的关系约束中。</li>
<li>相机感知处理针对的是特定传感器模型，未来可研究更具普适性的传感器模拟或域适应方法。</li>
<li>框架目前假设基座固定，未来可探索集成基座运动规划，实现完全意义上的移动操作数据生成。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中空间泛化所需大量人类演示数据、导致数据效率低下的问题，提出R2RGen框架。该框架通过真实到真实的3D数据生成，直接增强点云观察-动作对，无需模拟器或渲染。关键技术包括细粒度场景轨迹注释、分组增强策略处理多对象约束，以及相机感知处理对齐真实传感器分布。实验表明，R2RGen显著提升了数据效率，并在广泛测试中展现出在移动操作中扩展应用的潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.08547" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>