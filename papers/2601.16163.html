<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.16163" target="_blank" rel="noreferrer">2601.16163</a></span>
        <span>作者: Kim, Moo Jin, Gao, Yihuai, Lin, Tsung-Yi, Lin, Yen-Chen, Ge, Yunhao, Lam, Grace, Liang, Percy, Song, Shuran, Liu, Ming-Yu, Finn, Chelsea, Gu, Jinwei</span>
        <span>日期: 2026/01/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用大规模预训练模型进行机器人策略学习主要有两大方向：一是基于视频生成模型的方法，它们从海量视频数据中学习到了时空因果性和隐含物理规律，但现有工作通常需要多阶段训练（如先微调视频模型，再训练独立的动作模块）并引入新的架构组件（如单独的动作扩散器或逆动力学模型），过程复杂；二是基于视觉-语言-动作模型的方法，它们利用在静态图像-文本对上学到的语义概念，但在低层控制所需的时空动态先验方面可能不足。本文旨在解决这些局限性，提出一种简单直接的方法，将预训练视频模型（Cosmos-Predict2）通过单阶段微调，无需任何架构修改，即可转化为一个强大的机器人策略。其核心思路是：将机器人动作、未来状态观测（图像与本体感知）以及状态价值（期望累积回报）编码为视频扩散模型潜在序列中的“潜在帧”，从而直接利用模型原有的学习机制和强大的时空先验，统一学习策略、世界模型和价值函数。</p>
<h2 id="方法详解">方法详解</h2>
<p>Cosmos Policy的整体流程是：给定当前多视角图像和机器人本体感知状态，模型通过其扩散去噪过程，并行或自回归地生成编码了动作块、未来状态和未来状态价值的潜在帧序列。在部署时，可直接执行生成的动作（直接策略模式），或利用生成的世界模型和价值函数进行“最佳N采样”规划，选择预测价值最高的动作执行。</p>
<p><img src="https://arxiv.org/html/2601.16163v1/fig/cosmos_policy_figure1.jpeg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Cosmos Policy整体框架。模型基于预训练的NVIDIA Cosmos-Predict2-2B视频基础模型微调而来，处理多模态输入（多视角图像、本体感知、语言指令），并联合预测（1）机器人动作块，（2）未来状态（由本体感知和图像观测表示），以及（3）价值（未来状态的期望剩余回报）。基础视频模型无需架构改动，所有模态通过视频扩散学习目标联合建模。</p>
</blockquote>
<p>核心创新在于<strong>潜在帧注入</strong>机制。如图2所示，该方法将新模态（如动作、本体感知、价值）和额外相机视角的图像，作为新的“帧”直接插入到视频模型原有的潜在扩散序列中。具体而言，原始视频模型的潜在序列 <code>(1+T&#39;) x H&#39; x W&#39; x 16</code> 仅对应图像帧。Cosmos Policy 在其中按特定顺序交错插入代表不同模态的潜在帧。例如，对于一个双静态第三人称相机加一个腕部相机的机器人，其潜在序列可能包含11个潜在帧，顺序为：空白占位符、当前本体感知、腕部相机图像、第一第三人称相机图像、第二第三人称相机图像、动作块、未来本体感知、未来腕部相机图像、未来第一第三人称相机图像、未来第二第三人称相机图像、未来状态价值。其中，动作、本体感知和价值等非图像模态通过归一化到[-1, +1]并复制填充到整个 <code>H&#39; x W&#39; x C&#39;</code> 的潜在体积中来编码。这种排列实现了 <code>(s, a, s’, V(s’))</code> 的结构。</p>
<p><img src="https://arxiv.org/html/2601.16163v1/fig/cosmos_policy_diffusion_sequence_v2_main_version.001.jpeg" alt="潜在扩散序列"></p>
<blockquote>
<p><strong>图2</strong>：Cosmos Policy的潜在扩散序列示意图，展示了潜在帧注入机制。第一行：原始图像被标记化为潜在帧。第二行：将额外的模态（机器人状态、动作块、状态价值）和相机视角作为新的潜在帧直接插入序列。第三行：模型的任务是在条件帧（保持干净）下，去噪被噪声污染的目标帧。</p>
</blockquote>
<p>通过<strong>联合训练策略、世界模型和价值函数</strong>，模型被微调以掌握这些新模态的分布。训练数据来自演示数据集和策略 rollout 数据集。在每批训练中，50%来自演示数据，用于训练策略 <code>p(a, s’, V(s’)|s)</code>；另外50%来自 rollout 数据，一半用于训练世界模型 <code>p(s’, V(s’)|s, a)</code>，另一半用于训练价值函数 <code>p(V(s’)|s, a, s’)</code>。具体训练哪个功能是通过在潜在序列上应用不同的<strong>条件掩码</strong>来实现的（例如，预测动作时以<code>s</code>为条件，预测未来状态和价值时以<code>s, a</code>为条件）。这种辅助监督（让策略和世界模型同时预测价值）被证明能提升策略性能。</p>
<p>为了进行<strong>基于模型的规划</strong>，Cosmos Policy 采用“双部署”模式：原始检查点作为“策略模型”用于提出动作候选；另一个在策略 rollout 数据上进一步微调（90%批次用于训练世界模型和价值函数）的检查点作为“规划模型”，用于精确预测每个动作候选导致的未来状态及其价值。规划时采用“最佳N采样”：从策略模型采样多个动作提议；用规划模型为每个提议预测未来状态和价值；选择并执行预测价值最高的动作。为提高鲁棒性，会对每个动作进行多次世界模型和价值函数查询，并采用“多数均值”法聚合价值预测（先根据阈值判断多数预测是成功还是失败，然后仅对多数组内的预测值取平均）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个基准上进行：<strong>LIBERO仿真基准</strong>（评估空间、物体、目标和长视野任务）、<strong>RoboCasa仿真基准</strong>（24个厨房操作任务，包含未见过的物体实例和场景风格）以及真实世界的<strong>ALOHA双手操作任务</strong>（四个挑战性任务：“放物体到盘子”、“叠衬衫”、“放糖果到碗”、“放糖果到密封袋”）。</p>
<p>对比的基线方法包括：从头训练的扩散策略（Diffusion Policy）、基于视频模型的策略（如UVA, Video Policy）、以及经过微调的先进视觉-语言-动作模型（如π_0.5， OpenVLA-OFT, CogVLA等）。</p>
<p>关键实验结果如下：</p>
<ul>
<li><strong>作为直接策略</strong>：在LIBERO基准的四个任务套件上，Cosmos Policy取得了平均98.5%的成功率，在RoboCasa基准上取得了平均67.1%的成功率，均达到新的最先进水平。在仅使用50条人类演示数据的情况下，其RoboCasa性能显著优于许多使用更多数据的方法。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.16163v1/fig/aloha_task_performance_results_v2.001.jpeg" alt="LIBERO结果"></p>
<blockquote>
<p><strong>图4</strong>：LIBERO仿真基准结果。表格显示Cosmos Policy在四个任务套件（Spatial, Object, Goal, Long）上的成功率（SR）均领先于所有对比方法，平均达到98.5%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.16163v1/fig/pi05_openvlaoft_aloha_rollouts.001.jpeg" alt="RoboCasa结果"></p>
<blockquote>
<p><strong>图5</strong>：RoboCasa仿真基准结果。Cosmos Policy以67.1%的平均成功率排名第一，超过了其他视频策略和微调的VLA模型，展示了其数据效率和高性能。</p>
</blockquote>
<ul>
<li><strong>真实世界性能</strong>：在真实的ALOHA双手操作任务中，Cosmos Policy取得了93.6%的平均成功率，是所有对比方法中最高的。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.16163v1/fig/cosmos_policy_aloha_rollouts.001.jpeg" alt="真实世界结果"></p>
<blockquote>
<p><strong>图3</strong>：Cosmos Policy在ALOHA机器人任务中的定性展示。模型能够成功执行需要长视野、高精度操作且动作多模态性高的任务。</p>
</blockquote>
<ul>
<li><strong>基于模型的规划提升</strong>：在“叠衬衫”和“放糖果到密封袋”这两个最具挑战性的真实世界任务中，启用基于模型的规划后，Cosmos Policy的任务完成率平均提升了12.5个百分点。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.16163v1/fig/cosmos_policy_planning_rollouts.001.jpeg" alt="规划结果"></p>
<blockquote>
<p><strong>图6</strong>：规划对真实世界任务性能的提升。柱状图显示，在“Fold Shirt”和“Put Candy in Ziploc”任务中，使用规划（橙色）相比直接策略（蓝色）显著提高了成功率。</p>
</blockquote>
<ul>
<li><strong>消融实验</strong>：论文进行了详细的消融研究（见图7），关键结论包括：1) <strong>潜在帧注入</strong>是方法有效的核心；2) <strong>辅助目标</strong>（联合预测未来状态和价值）能提升策略性能；3) 使用<strong>策略 rollout 数据</strong>微调世界模型和价值函数对于有效规划至关重要；4) 在规划中，使用基于世界模型的状态价值函数 <code>V(s’)</code> 比使用模型自由的动作价值函数 <code>Q(s, a)</code> 更有效。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.16163v1/fig/planning_results.001.jpeg" alt="消融与规划分析"></p>
<blockquote>
<p><strong>图7</strong>：消融实验与规划分析结果。左图展示了不同组件（如辅助预测、使用 rollout 数据）对策略成功率的影响；右图比较了不同规划配置（如使用V(s’)与Q(s, a)，不同采样数N）在模拟任务中的性能。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>潜在帧注入</strong>这一简单而强大的机制，无需修改架构即可将预训练视频模型适配为能处理多模态输入输出（动作、状态、价值）的机器人策略模型。2) 实现了<strong>策略、世界模型和价值函数的统一学习与微调</strong>，并利用策略 rollout 数据持续改进后两者，从而支持有效的基于模型的规划。3) 在仿真和真实世界基准上取得了<strong>最先进的性能</strong>，验证了利用视频模型时空先验进行机器人控制的巨大潜力。</p>
<p>论文提到的局限性主要包括：规划时进行多次采样和查询会带来<strong>更高的计算成本</strong>；方法依赖于高质量的预训练视频模型。</p>
<p>这项工作对后续研究的启示在于：它证明了大规模预训练生成模型（尤其是视频模型）本身的学习算法就是强大的多模态序列建模器，可直接用于动作生成等决策任务，无需复杂的解耦设计。这为如何更高效地利用基础模型进行具身智能研究提供了一个简洁而有力的范式。未来的工作可以探索如何降低规划的计算开销，以及将类似思想应用于其他类型的生成模型或更复杂的决策场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Cosmos Policy方法，旨在简化视频模型在机器人策略学习中的应用。其核心是仅通过单阶段后训练，将预训练视频模型Cosmos-Predict2适配为机器人策略，无需修改模型架构。该方法直接在视频扩散过程中将机器人动作、未来状态图像及价值函数编码为潜在帧进行联合建模。实验表明，该方法在LIBERO和RoboCasa仿真基准上分别达到98.5%和67.1%的平均成功率，并在真实世界双手机器人操作任务中取得最高分，性能优于多种基线模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.16163" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>