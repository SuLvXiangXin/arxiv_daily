<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.10477" target="_blank" rel="noreferrer">2512.10477</a></span>
        <span>作者: Ishuov, Timur, Folgheraiter, Michele, Nurmanov, Madi, Gordo, Goncalo, Farkas, Richárd, Dombi, József</span>
        <span>日期: 2025/12/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在无模型、无模仿的强化学习（RL）应用于人形机器人时，存在两个相互冲突的核心目标：样本效率（Sample Efficiency）与动作的安全协调性（Sample Proximity and Safety of Actions）。一方面，像近端策略优化（PPO）这样的同策略算法通过小步幅的安全梯度更新实现了协调的运动，但需要海量样本，样本效率低。另一方面，像软演员-评论家（SAC）及其变体（如RedQ、TQC）这样的异策略算法通过熵正则化和更高的“每帧更新次数”（Update-to-Data ratio）提高了样本效率，但存在时间差分（TD）预测不准确、策略易陷入局部极值以及为追求高效而可能产生导致机器人损坏的“抽搐”动作等问题。特别是，直接增加高斯噪声或使用Q值平均（而非取最小）会加剧动作的不稳定性与风险。</p>
<p>本文针对“在保证高样本效率的同时，实现人形机器人安全、协调的从零学习”这一具体痛点，提出了一个综合性的新视角：通过设计一种介于同策略与异策略之间的“过渡策略”（Transitional-policy）确定性演员-评论家算法，将样本邻近性、动作安全与样本效率统一在一个框架内。其核心思路是：1）利用<strong>时间优势</strong>（Temporal Advantage）实现演员与评论家的统一、高效更新；2）设计<strong>渐褪回放缓冲区</strong>（Fading Replay Buffer）平衡近期与远期经验；3）引入<strong>“襁褓”正则化</strong>（Swaddling Regularization）间接限制动作强度以提升安全性，而非直接惩罚动作或调整噪声。</p>
<h2 id="方法详解">方法详解</h2>
<p>Symphony算法的整体框架是一个确定性演员-评论家架构，其核心创新在于更新机制、经验回放和正则化方法。算法流程如下：初始化后，先进行固定步数的探索以估计奖励归一化因子并预填充回放缓冲区。每个时间步，演员网络根据状态产生动作（附加固定参数噪声），与环境交互后存储转移经验。随后，以较高的更新比例（𝒢=3）从缓冲区采样批次数据，利用目标评论家网络和计算出的时间优势，通过一个结合了特定损失函数的统一目标，同时更新在线演员和在线评论家网络参数，并软更新目标评论家。</p>
<p><img src="https://arxiv.org/html/2512.10477v7/images/Actor_Critic4.png" alt="架构图"></p>
<blockquote>
<p><strong>图11</strong>：Symphony Actor-Critic网络架构。网络接收状态输入，经过层归一化和带有ReSine激活函数的隐藏层。演员网络输出动作、缩放因子σ和温度参数β；评论家网络（三个）各输出128个节点，拼接成384个节点的Q值分布。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><p><strong>时间优势与统一更新</strong>：这是算法的关键。首先，使用在线演员网络根据下一状态预测下一动作 <code>a_{t+1}&#39;</code>。然后，使用目标评论家网络计算该状态-动作对的Q值 <code>Q_target</code> 及其脱钩版本 <code>Q_target*</code>。时间优势（TA）定义为 <code>Q_target</code> 与一个指数移动平均 <code>Q_T</code>（<code>Q_T = α * Q_{T-1}的平均 + (1-α) * Q_target*</code>，其中α≈0.62）之间的差值，并除以 <code>|Q_T|</code> 进行归一化。演员的更新目标是最大化这个归一化的时间优势。同时，评论家使用标准的TD误差进行更新（<code>r_t_hat + γ * Q_target* - Q_online</code>）。创新之处在于，将演员和评论家的损失组合进一个统一的目标函数（公式3），使得两者可以在一次前向-反向传播中同时更新，提升了计算效率。</p>
</li>
<li><p><strong>渐褪回放缓冲区</strong>：为了解决大容量回放缓冲区中样本相关性弱的问题，并平衡近期经验（高相关性）与远期经验（有利于泛化），Symphony使用了非均匀采样的Fading Replay Buffer。采样概率 <code>p</code> 由转换经验在缓冲区中的归一化索引 <code>i_n</code> 通过函数 <code>w_{i_n} = tanh((π * i_n)^e)</code> 计算权重后再归一化得到。该函数赋予近期经验更高的采样概率，形成平滑过渡的“平台”形状，实现了介于同策略缓存与标准异策略经验回放之间的特性。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.10477v7/images/FadingReplayBuffer.png" alt="缓冲区示意图"></p>
<blockquote>
<p><strong>图2</strong>：Fading Replay Buffer采样概率分布。x轴为经验在缓冲区中的位置（0为最新，1为最旧），y轴为采样概率。曲线显示对近期经验有更高的采样权重，且过渡平滑。</p>
</blockquote>
<ol start="3">
<li><p><strong>目标评论家集成与权重聚合</strong>：使用3个评论家网络，每个输出128个节点，拼接形成384个节点的Q值分布。对于目标评论家的输出，将这些节点值从小到大排序，并乘以一组固定的权重 <code>w_n</code>（权重计算公式为 <code>w_{i_n} = tanh((π*(1-i_n))^e)</code> 后归一化，其中 <code>i_n</code> 为节点索引的归一化值）。最终的 <code>Q_target</code> 是这384个节点的加权和。这种聚合方式替代了传统的取最小或简单平均，旨在平衡乐观与悲观估计。</p>
</li>
<li><p><strong>“襁褓”正则化</strong>：这是实现动作安全的核心。不同于直接给动作加平方惩罚（控制成本），Symphony让演员网络额外输出一个与动作同维度的缩放因子向量 <code>σ</code>（通过可训练参数 <code>s</code> 经sigmoid得到）。动作在输出前会乘以 <code>σ</code>，从而间接限制其强度。为了正则化 <code>σ</code> 使其在合适的范围内（围绕噪声标准差1/e浮动），引入了基于反双曲正切函数 <code>ATanh</code> 的“襁褓”损失项 <code>Ω(σ^{1/β})</code>，其中 <code>β</code> 是一个温度参数（也可由网络输出并正则化）。此外，还增加了一个辅助损失 <code>β * ω(σ)</code>（<code>ω(x)=x*ln(x)</code>），其最小值在 <code>x=1/e</code>，进一步引导 <code>σ</code> 趋向期望范围。这种设计使得智能体在训练初期动作幅度小（被“包裹”），随着学习进展，动作幅度可逐渐增大。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.10477v7/images/Swaddling3.png" alt="襁褓正则化示意图"></p>
<blockquote>
<p><strong>图10</strong>：完整的“襁褓”正则化函数图示。结合了 <code>Ω</code>（反双曲正切屏障）和 <code>ω</code>（熵辅助函数），共同约束缩放因子 <code>σ</code> 和温度参数 <code>β</code>。</p>
</blockquote>
<ol start="5">
<li><strong>网络结构与其他组件</strong>：<ul>
<li><strong>ReSine激活函数</strong>：在网络隐藏层使用修正正弦激活函数 <code>F(x) = f(x) * sigmoid(1.5/σ * f(x))</code>，其中 <code>f(x)=σ*sin(x/σ)</code>。它引入了类似傅里叶级数的谐波振荡，有助于探索，并通过Swish门控和比例缩放 <code>σ</code> 来稳定。<br><img src="https://arxiv.org/html/2512.10477v7/images/ReSine.png" alt="ReSine激活函数"><blockquote>
<p><strong>图3</strong>：ReSine激活函数及其缩放机制。引入谐波非线性以增强探索能力。</p>
</blockquote>
</li>
<li><strong>ReHSE/ReHAE损失函数</strong>：使用修正胡贝尔对称误差（<code>ReHSE(x) = x * tanh(x/2)</code>）作为评论家损失，修正胡贝尔非对称误差（<code>ReHAE(x) = |x| * tanh(x/2)</code>）作为演员的优势损失。它们在误差较小时近似二次函数（阻尼小梯度），在误差较大时渐近线性，提供更平滑的梯度流动。<br><img src="https://arxiv.org/html/2512.10477v7/images/ReHSE_ReHAE.png" alt="损失函数"><blockquote>
<p><strong>图4</strong>：ReHSE（对称）与ReHAE（非对称）损失函数曲线。展示了从二次到线性的渐进过渡特性。</p>
</blockquote>
</li>
<li><strong>奖励归一化与探索</strong>：在初始探索阶段（10240步）收集奖励，计算其绝对值的均值 <code>r_bar_n</code>，用于后续所有奖励的归一化。探索数据会被重复50次以完全初始化512k容量的回放缓冲区。</li>
</ul>
</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在MuJoCo仿真环境的两个基准任务上进行：Humanoid-v4（高维、不稳定）和Walker2d-v4。对比的基线算法包括SAC、TQC、AQE以及PPO。评估指标为平均回合累积奖励。</p>
<p><strong>关键定量结果</strong>：</p>
<ol>
<li><p><strong>Humanoid-v4</strong>：Symphony在约<strong>50万</strong>步训练后即能达到超过5000分的性能，且学习曲线平滑上升。相比之下，SAC和TQC需要约1000万步才能达到类似性能，且曲线初期波动大、有下降。PPO虽然最终性能相当，但需要约<strong>1500万</strong>步，样本效率最低。<br>   <img src="https://arxiv.org/html/2512.10477v7/images/Humanoid.png" alt="Humanoid结果"></p>
<blockquote>
<p><strong>图19</strong>：在Humanoid-v4环境上的学习曲线对比。Symphony（蓝色）以极少的步数快速达到高性能，且稳定性显著优于SAC、TQC等基线。</p>
</blockquote>
</li>
<li><p><strong>Walker2d-v4</strong>：Symphony在约<strong>40万</strong>步后达到接近5000分的峰值，样本效率显著高于SAC和TQC（需数百万步）。PPO同样样本效率低下。<br>   <img src="https://arxiv.org/html/2512.10477v7/images/s22.png" alt="Walker2d结果"></p>
<blockquote>
<p><strong>图21</strong>：在Walker2d-v4环境上的学习曲线对比。Symphony再次展示了快速收敛和高样本效率。</p>
</blockquote>
</li>
<li><p><strong>消融实验</strong>：</p>
</li>
</ol>
<ul>
<li><strong>移除“襁褓”正则化</strong>：性能急剧下降，在Humanoid上几乎无法学习，验证了其对安全稳定学习的关键作用。<br> <img src="https://arxiv.org/html/2512.10477v7/images/S2.png" alt="消融襁褓"><br> &gt; <strong>图16</strong>：移除“襁褓”正则化（Symphony w/o Swaddling）导致在Humanoid任务上完全失败。</li>
<li><strong>移除Fading Replay Buffer（使用均匀采样）</strong>：学习速度变慢，收敛性能略有下降，表明其对提升样本效率和稳定训练有积极作用。<br> <img src="https://arxiv.org/html/2512.10477v7/images/S3.png" alt="消融缓冲区"><br> &gt; <strong>图15</strong>：使用均匀回放缓冲区替代Fading Replay Buffer会降低学习速度。</li>
<li><strong>移除时间优势（使用标准确定性策略梯度）</strong>：学习过程变得不稳定，性能波动更大。<br> <img src="https://arxiv.org/html/2512.10477v7/images/SE.png" alt="消融时间优势"><br> &gt; <strong>图14</strong>：移除时间优势（TA）模块导致学习曲线波动加剧。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了Symphony算法框架</strong>：一种启发式归一化校准的优势演员-评论家算法，通过<strong>时间优势</strong>、<strong>渐褪回放缓冲区</strong>和<strong>“襁褓”正则化</strong>三大核心技术的协同，有效解决了人形机器人从零训练中样本效率与动作安全协调性的矛盾。</li>
<li><strong>设计了新颖的“襁褓”正则化机制</strong>：通过可学习的缩放因子间接限制动作强度，配合特定的损失函数，在不过度约束的前提下显著提升了训练过程的安全性（对机器本体和环境的保护）和稳定性。</li>
<li><strong>实现高效统一的Actor-Critic更新</strong>：利用时间优势归一化，将演员和评论家的更新合并进一个统一的目标和计算图中，实现了“单步同时更新”，提升了计算效率，并简化了代码实现。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，算法中包含一些启发式设定的超参数（如黄金比例倒数作为平滑因子α、缓冲区填充方案等），虽然经实验验证有效，但其普适最优性有待进一步理论分析。此外，方法在极其复杂或非平稳的运动任务上的扩展性仍需验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>平衡设计哲学</strong>：Symphony展示了在强化学习中，通过精心设计的模块（如介于同/异策略之间的缓冲区、间接的动作约束）来平衡多个竞争性目标（效率、安全、探索）是可行的，这为机器人及其他高风险领域的RL应用提供了新思路。</li>
<li><strong>模块化组件可移植性</strong>：时间优势计算、渐褪回放缓冲区、“襁褓”正则化等组件可以作为相对独立的模块，被尝试集成到其他RL算法中以改善其特定性能。</li>
<li><strong>对安全RL的启发</strong>：“襁褓”思想——即在智能体能力较弱时施加保护性限制，随着能力提升逐步放宽——是一种符合直观且有效的安全学习范式，可激励更多受生物学习过程启发的安全约束设计。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人从零训练时面临的样本效率低、动作安全性差及硬件损耗风险等问题，提出Symphony算法。该算法是一种启发式归一化校准的优势演员-评论家方法，核心创新包括：采用“襁褓”正则化限制动作强度而非直接噪声；设计衰减回放缓冲区平衡近期与长期记忆；利用时间优势实现演员与评论家网络单步更新。实验表明，该方法能显著提升训练安全性，通过调整动作强度而非增加高斯噪声，有效保护电机与齿轮箱，同时使智能体在需要时能输出更确定性的动作。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.10477" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>