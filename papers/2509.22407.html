<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.22407" target="_blank" rel="noreferrer">2509.22407</a></span>
        <span>作者: Guan Huang Team</span>
        <span>日期: 2025-09-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作模型在机器人操控任务中展现出强大潜力，但其成功严重依赖于大规模、多样化的训练数据。收集真实世界的机器人操作数据耗时且昂贵，限制了数据集的规模和视觉多样性。仿真数据虽可扩展，但存在视觉真实感差距和资产多样性有限的问题。近期，扩散模型被用于生成视觉-动作数据以辅助策略训练。然而，现有方法面临两大关键挑战：首先，多数方法生成单视角视频，无法保证跨视角的一致性，限制了其在依赖多相机输入的机器人任务中的应用；其次，现有工作将生成数据视为静态增强，未考虑如何在训练中有效利用它们。</p>
<p>本文针对上述痛点，提出了EMMA框架，其核心思路是：通过一个名为DreamTransfer的生成模型，将真实或仿真的演示视频转化为多视角一致、几何基础扎实且支持文本控制编辑的多样化操作视频，并结合一种名为AdaMix的自适应训练策略，动态调整训练样本权重以专注于困难样本，从而显著提升VLA策略的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>EMMA框架包含两个核心组件：DreamTransfer生成数据引擎和AdaMix自适应训练策略。整体流程是：首先，DreamTransfer接收真实或仿真的多视角演示视频及文本提示，生成经过视觉编辑（前景、背景、光照）的多视角一致新视频。生成视频经过质量筛选后，与真实数据混合。随后，AdaMix策略基于策略在训练集上的表现，动态计算每个样本的难度分数，并据此调整采样权重，使训练更专注于困难样本。</p>
<p><img src="https://arxiv.org/html/2509.22407v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：EMMA框架总览。左侧，DreamTransfer基于深度图和文本提示对前景、背景和光照条件进行文本控制的视觉编辑，生成多视角一致视频。生成视频经质量过滤器评估。右侧，AdaMix模块进一步根据轨迹性能指标自适应地重新加权训练样本，提升困难样本的权重以改善策略的鲁棒性和泛化性。</p>
</blockquote>
<p><strong>DreamTransfer</strong> 是一个基于扩散Transformer的双分支架构模型，旨在生成几何基础扎实的多视角操作视频。</p>
<p><img src="https://arxiv.org/html/2509.22407v1/x3.png" alt="DreamTransfer框架"></p>
<blockquote>
<p><strong>图3</strong>：DreamTransfer框架概述。多视角深度图沿宽度维度拼接。主分支对潜在视频令牌进行去噪，而并行的ControlNet分支通过融入深度约束来确保几何一致性。</p>
</blockquote>
<p>具体而言，模型将多个同步的视频深度图拼接并编码为统一的潜在表示。同时，使用T5文本编码器将文本提示转换为语义嵌入。主分支以带噪潜在、时间步、深度特征和文本特征为输入，通过扩散Transformer预测去噪后的潜在。一个并行的ControlNet分支提供基于深度的结构引导，以增强几何一致性。最终，去噪潜在通过VAE解码器重建为输出视频，在保持底层3D结构的同时，忠实反映文本指定的外观变化。该设计利用了预训练扩散模型固有的多视角上下文学习能力，确保了跨相机视角的空间和时间连贯性。</p>
<p><strong>AdaMix</strong> 是一种困难样本感知的训练策略，旨在解决均匀采样对长尾困难场景利用不足的问题。其流程分为三步：1) 视频质量过滤：基于深度一致性、多视角一致性和文本-视频相似性指标过滤生成视频，不合格样本初始权重设为零；2) 初始均匀训练：使用真实数据和高质生成数据进行均匀采样训练，直至损失收敛；3) 自适应重加权：在训练集上计算每个样本的三个性能指标：动作预测误差（MSE）、轨迹平滑度（惩罚关节角度突变）和关节角度限制（安全二进制指标）。将这三个归一化后的指标平均，得到每个样本的综合性能分数。在增量训练中，采样权重根据公式 ( p(i) \propto \gamma + \lambda \cdot (1 - s_i) ) 更新，其中 ( s_i ) 是性能分数。这使权重向性能较差（即更困难）的样本倾斜，从而动态地将训练分布转向策略表现不佳的区域，同时保持数据多样性。</p>
<p>与现有方法相比，EMMA的创新点在于：1) DreamTransfer通过深度条件化和多视角联合建模，实现了比先前方法更优的跨视角几何一致性和可控编辑能力；2) AdaMix首次在VLA策略训练中引入了基于策略自身表现的动态样本重加权机制，主动构建了一个针对策略弱点的课程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验评估分为视频生成质量和真实世界机器人部署两部分。使用的基准任务包括<strong>Fold Cloth</strong>（真实到真实迁移）、<strong>Clean Desk</strong>和<strong>Throw Bottle</strong>（仿真到真实迁移），涵盖了刚体与可变形物体、长时程与短时程动作序列。对比的基线生成模型包括Cosmos-Transfer1和RoboTransfer。机器人实验平台为Agilex CobotMagic，配备两个PiPER机械臂和三个Intel RealSense D435i相机。</p>
<p><strong>视频生成质量</strong>：如表1所示，DreamTransfer在多项指标上超越基线。在多视角一致性（像素匹配）上，DreamTransfer平均得分3270，相比第二名RoboTransfer（2298）相对提升42%。在深度一致性上，其平方相对误差为0.54，相比Cosmos-Transfer1（0.71）和RoboTransfer（1.30）分别相对提升24%和58%。在文本-视频对齐（CLIP相似性）上，DreamTransfer也达到最佳平均分24.68。这证明了其生成视频具有卓越的几何保真度和跨视角一致性。</p>
<p><strong>真实世界机器人评估</strong>：</p>
<ol>
<li><p><strong>与基线生成模型对比</strong>：如表2所示，仅使用真实数据训练（No.Aug.）性能最低。使用Cosmos-Transfer1生成数据增强后，性能有中等提升。而使用DreamTransfer生成数据训练（混合比50%）能带来最显著的性能增益，在Fold Cloth任务上成功率从10%提升至65%，平均成功率从28%提升至65%，相对提升超过200%。</p>
</li>
<li><p><strong>生成数据混合比例的影响</strong>：<br><img src="https://arxiv.org/html/2509.22407v1/figures/fold_cloth_mix_ratio.png" alt="混合比例影响-Fold Cloth"></p>
<blockquote>
<p>**图4(a)**：Fold Cloth任务中，成功率随生成数据比例增加而提升，在50%时达到峰值（65%），之后趋于稳定或略有下降。<br><img src="https://arxiv.org/html/2509.22407v1/figures/clean_desk_mix_ratio.png" alt="混合比例影响-Clean Desk"><br>**图4(b)**：Clean Desk任务表现同样在50%混合比附近达到最优。<br><img src="https://arxiv.org/html/2509.22407v1/figures/throw_bottle_mix_ratio.png" alt="混合比例影响-Throw Bottle"><br>**图4(c)**：Throw Bottle任务在50%混合比时成功率最高（50%），当生成数据比例增至90%时，成功率下降至45%，表明过度依赖生成数据可能传播不准确性。</p>
</blockquote>
<p>综合图4(a)(b)(c)可知，引入生成数据能显著提升性能，50%的混合比例通常达到最优。超过此比例，性能不再增长甚至可能下降，尤其在需要高精度的任务中。</p>
</li>
<li><p><strong>AdaMix消融实验</strong>：如表3所示，与固定混合比采样相比，采用AdaMix自适应加权策略能进一步提升所有任务的平均成功率，从65%提升至78%，相对提升13%。此外，如表4所示，AdaMix训练出的策略执行效率更高（平均快3.0秒）、轨迹更平滑、关节超限次数更少（平均减少7次），表明其能有效识别并改善策略在困难样本上的表现。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了EMMA框架，将高质量、多视角一致的生成数据引擎（DreamTransfer）与困难样本感知的自适应训练策略（AdaMix）相结合，以数据为中心增强VLA策略的泛化能力；2) 设计了DreamTransfer模型，通过深度条件化和双分支架构，实现了在保持几何合理性的前提下进行文本控制的视觉编辑，并在多视角一致性和深度保真度上显著优于现有方法；3) 提出了AdaMix策略，通过基于策略性能的动态样本重加权，自动构建针对策略弱点的训练课程，进一步提升了策略在真实世界中的鲁棒性和执行质量。</p>
<p>论文自身提到的局限性体现在生成数据混合比例实验中：当生成数据比例过高（如90%）时，在某些任务上性能会下降，暗示生成数据可能包含细微的视觉或动力学不准确性，过度依赖可能损害策略性能。</p>
<p>这项工作对后续研究的启示包括：1) 生成模型与机器人学习的结合需要同时关注生成质量（如几何一致性）和生成数据的使用策略（如自适应训练）；2) 以数据为中心的方法，特别是动态调整训练分布的策略，是提升模型泛化能力和解决长尾问题的有效途径；3) 未来可以探索更精细的生成数据质量评估与选择机制，以及将AdaMix思想扩展到更广泛的策略学习范式中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出EMMA框架，旨在解决机器人操作中因真实数据收集成本高、视觉多样性不足导致的模型泛化瓶颈。其核心技术包括：DreamTransfer（基于扩散Transformer的多视角一致视频生成方法，支持文本控制的前景、背景和光照编辑）和AdaMix（硬样本感知的动态加权训练策略）。实验表明，生成视频在多视角一致性与几何保真度上显著优于基线；使用生成数据训练的VLA模型在零样本视觉领域任务中，相比仅用真实数据训练获得超过200%的性能提升，结合AdaMix可再提升13%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.22407" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>