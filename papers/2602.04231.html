<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.04231" target="_blank" rel="noreferrer">2602.04231</a></span>
        <span>作者: Hongliang Ren Team</span>
        <span>日期: 2026-02-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>语言引导抓取已成为一种有前景的范式，使机器人能够通过自然语言指令识别和操作目标物体，但在杂乱或遮挡场景中仍极具挑战。现有方法通常依赖于将物体感知与抓取分离的多阶段流水线，这导致跨模态融合有限、计算冗余，以及在杂乱、遮挡或低纹理场景下泛化能力差。具体而言，基于CLIP的方法虽然提升了语义理解，但仍面临以下关键局限性：1）依赖外部物体和抓取检测器，易产生级联误差；2）常用的视觉编码器（如CLIP-ResNet和CLIP-ViT）各有缺陷，前者因静态层次结构无法动态建模物体尺度变化和形变，后者计算成本高且难以捕捉固定分块下的细粒度结构；3）现有的RGB-D方法通常采用双编码器架构，分别处理RGB和深度信息，导致计算冗余，且低质量深度数据可能干扰RGB特征学习；4）跨模态融合策略（如拼接或交叉注意力）未能充分利用多层视觉特征，导致任务相关信号被稀释。</p>
<p>本文针对上述痛点，提出了一种新的视角：构建一个端到端的统一多任务视觉-语言框架，将RGB-D感知与自然语言理解整合到一个共享表示空间中，以增强语义对齐和泛化能力。核心思路是：通过深度引导几何模块将深度信息编码为显式几何先验并注入注意力机制，以增强空间推理和目标辨别能力；同时，通过自适应密集通道集成模块自适应地聚合多层视觉特征，以产生更具判别性和泛化性的视觉表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>GeoLanG是一个端到端的视觉-语言抓取框架，旨在复杂开放场景中实现精确的语言引导抓取。给定RGB-D图像和语言查询，文本编码器提取高级语义特征，RGB编码器处理多尺度视觉信息。框架遵循CLIP范式，将视觉和语言特征对齐到共享语义空间。</p>
<p><img src="https://arxiv.org/html/2602.04231v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：GeoLanG框架概览。给定RGB-D图像和语言查询，文本编码器提取高级语义特征，RGB编码器处理多尺度视觉信息。通过深度引导几何模块（DGGM）融入深度衍生的几何先验以增强结构线索。多层视觉特征经ADCI优化后，输入双路径投影器，生成像素级分割掩码并细化目标物体的抓取姿态。</p>
</blockquote>
<p><strong>整体流程与核心模块</strong>：</p>
<ol>
<li><strong>视觉编码</strong>：采用CLIP-VMamba作为视觉编码器，它集成了ViT和CNN架构的优势，ViT主干捕获长程依赖和全局上下文，CNN风格模块保留高分辨率局部细节。保留最后三个尺度的特征（下采样因子分别为8、16、32），记为 $C = {C_i \in \mathbb{R}^{H/s_i \times W/s_i \times C_i} \mid i=1,2,3}$，以平衡计算成本和语义丰富性。</li>
<li><strong>文本编码</strong>：使用CLIP-BERT对文本指令 $P = {w_1, \dots, w_T}$ 进行编码，获得词元嵌入 $C_t \in \mathbb{R}^{K \times C}$ 和句子嵌入 $C_s \in \mathbb{R}^{C&#39;}$。</li>
<li><strong>深度引导几何模块（DGGM）</strong>：该模块旨在有效融入深度线索，同时避免过高计算开销。它将深度图转换为显式几何先验，并融合到注意力机制中。<ul>
<li><strong>几何先验构建</strong>：将大小为 $h \times w$ 的RGB图像均匀划分为 $H \times W$ 个块。对于位置为 $(m, n)$ 的块，对其对应的深度图块进行平均池化得到代表性深度值 $D_{m,n}$。任意两个块之间的深度差异定义为 $\Delta D_{m,n,m&#39;,n&#39;} = |D_{m,n} - D_{m&#39;,n&#39;}|$，形成深度关系矩阵 $\Delta D \in \mathbb{R}^{HW \times HW}$。空间距离通过块坐标的曼哈顿距离计算：$\Delta S_{m,n,m&#39;,n&#39;} = |m - m&#39;| + |n - n&#39;|$，形成空间关系矩阵 $\Delta S \in \mathbb{R}^{HW \times HW}$。融合的几何先验 $\mathcal{G}$ 由 $\mathcal{G} = \lambda_1 \cdot \Delta D + \lambda_2 \cdot \Delta S$ 构建，其中 $\lambda_1, \lambda_2$ 是可学习参数，平衡深度和空间先验的贡献。</li>
<li><strong>注意力机制集成</strong>：给定RGB特征图 $X \in \mathbb{R}^{HW \times C}$，通过线性投影得到查询、键、值矩阵 $Q, K, V \in \mathbb{R}^{HW \times d}$。几何先验 $\mathcal{G}$ 通过衰减因子 $\eta \in (0,1)$ 集成到注意力中：$\hat{X} = (\text{Softmax}(QK^{\top}) \odot \eta \mathcal{G}) V^{\top}$。$\eta \mathcal{G}$ 中较小的值对应较大的几何距离，从而抑制不相关的键值对，强调几何相关的部分。该公式可扩展到多头注意力，为每个头分配不同的衰减率 $\eta$。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04231v1/x3.png" alt="深度引导几何模块"></p>
<blockquote>
<p><strong>图3</strong>：深度引导几何模块（DGGM）概览。粉色矩形表示从RGB编码器提取的图像特征，绿色矩形表示学习到的几何先验，黄色矩形表示从深度和RGB计算的空间先验。⊗ 表示矩阵乘法，⊙ 表示逐元素乘法。输出特征集成了视觉信息与几何和空间线索，以增强多尺度表示。</p>
</blockquote>
<ol start="4">
<li><strong>自适应密集通道集成（ADCI）</strong>：为了解决现有方法冻结主干、仅传播高层特征而丢弃有价值中低层线索的问题，ADCI模块自适应地聚合多层视觉特征。<ul>
<li><strong>操作流程</strong>：给定从V-Mamba提取的L层图像特征 ${C_1, \dots, C_L}$，将其分为G组，每组包含 $M = L/G$ 个连续层。每组内的可学习权重 $\alpha_i$ 根据输入特征自适应调整各层的贡献，实现动态信息聚合。对于第g组，其组级特征 $GC_g$ 计算为其连续层的加权组合。随后，所有组级特征通过通道注意力机制进一步融合，生成最终的多尺度视觉表示 $C_v \in \mathbb{R}^{N \times C}$，其中 $N = H/16 \cdot W/16$。</li>
</ul>
</li>
<li><strong>多任务解码</strong>：视觉表示 $C_v$ 与句子嵌入 $C_s$ 通过多模态融合颈（Multimodal Fusion Neck）融合，产生像素级多模态表示 $C_m \in \mathbb{R}^{N \times C}$，随后与词元嵌入 $C_t$ 一起输入下游多任务解码器，产生 $C_c \in \mathbb{R}^{N \times C}$。<ul>
<li><strong>分割分支</strong>：$C_c$ 和 $C_s$ 被投影到共享空间，并通过应用于投影嵌入点积的二元交叉熵损失进行监督。最终分割掩码通过对投影特征进行重塑和上采样获得。</li>
<li><strong>抓取头</strong>：从相同的共享特征预测抓取配置，实现感知与动作推理的联合优化。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>统一的端到端框架</strong>：将RGB-D感知与语言理解统一到一个框架中，避免了多阶段流水线的级联误差和计算冗余。</li>
<li><strong>深度引导几何模块（DGGM）</strong>：创新地将深度信息编码为显式几何先验（结合深度差和空间距离），并直接注入注意力机制，增强了模型在遮挡和低纹理条件下的空间推理能力，且无需额外的深度编码器。</li>
<li><strong>自适应密集通道集成（ADCI）</strong>：通过自适应加权聚合多层视觉特征，充分利用了低、中、高层信息，生成了更具判别性和泛化性的视觉表示。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：主要在OCID-VLG数据集上进行评估，该数据集包含杂乱场景中的语言引导抓取任务。</li>
<li><strong>实验平台</strong>：在仿真和真实世界硬件上均进行了实验。</li>
<li><strong>对比方法</strong>：与现有语言引导抓取方法进行对比，包括基于CLIP的多阶段方法以及RGB-D方法。</li>
<li><strong>评估指标</strong>：包括语言分割指标（如mIoU）和抓取指标（如抓取成功率）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在OCID-VLG数据集上，GeoLanG在语言分割和抓取指标上均超越了现有方法。具体而言，在分割任务上，GeoLanG达到了最高的mIoU（平均交并比）。在抓取任务中，GeoLanG也取得了最高的抓取成功率，特别是在遮挡和模糊指令条件下保持了鲁棒性。</p>
<p><img src="https://arxiv.org/html/2602.04231v1/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：在OCID-VLG数据集上的定性结果。GeoLanG能够准确分割和抓取语言指定的目标物体（如“白色杯子”、“红色玩具”），即使在存在遮挡和相似物体的复杂场景中也能有效工作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.04231v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融研究结果。实验表明，移除DGGM或ADCI模块都会导致性能下降，验证了这两个核心组件的有效性。同时，也对比了不同视觉编码器（CLIP-ResNet, CLIP-ViT, CLIP-VMamba）的影响，证明了CLIP-VMamba的优越性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>消融实验验证了各个组件的贡献：</p>
<ol>
<li><strong>DGGM模块</strong>：移除DGGM导致分割和抓取性能显著下降，特别是在遮挡场景中，证明了深度几何先验对于增强空间推理和目标辨别至关重要。</li>
<li><strong>ADCI模块</strong>：移除ADCI模块也导致性能下降，表明自适应聚合多层特征对于生成判别性视觉表示是有效的。</li>
<li><strong>视觉编码器</strong>：将CLIP-VMamba替换为CLIP-ResNet或CLIP-ViT会导致性能降低，证明了所选混合架构在复杂抓取场景中的优势。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04231v1/x6.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图6</strong>：真实世界机器人实验设置和结果。GeoLanG成功部署在真实机器人上，能够根据语言指令在杂乱桌面场景中可靠地抓取目标物体。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.04231v1/x7.png" alt="不同指令复杂度下的性能"></p>
<blockquote>
<p><strong>图7</strong>：在不同复杂度语言指令下的性能。随着指令变得更具描述性或涉及多个物体属性，GeoLanG相比基线方法保持了更高的成功率，展示了其强大的跨模态对齐能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了GeoLanG，一个端到端的统一多任务视觉-语言框架，将RGB-D感知与自然语言理解整合到共享表示空间，显著提升了在杂乱复杂环境中语言引导抓取的鲁棒性和泛化能力。</li>
<li>设计了深度引导几何模块（DGGM），将深度信息编码为显式几何先验并注入注意力机制，有效增强了模型在遮挡和低纹理条件下的空间推理能力，且计算高效。</li>
<li>提出了自适应密集通道集成（ADCI）模块，通过自适应聚合多层视觉特征，充分利用了不同层次的视觉信息，生成了更具判别性和泛化性的视觉表示。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：深度传感器的噪声和缺失可能仍会影响几何先验的准确性；方法在极端遮挡或光照条件下性能可能仍有提升空间。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>统一架构的潜力</strong>：证明了端到端统一多任务框架在机器人感知与操作任务中的有效性，为后续研究提供了新的架构设计思路。</li>
<li><strong>几何先验的利用</strong>：展示了将深度等几何信息作为先验知识（而非独立模态）注入到注意力机制中的有效性，这为多模态融合提供了新方法。</li>
<li><strong>特征融合策略</strong>：ADCI模块强调了自适应利用多层次视觉特征的重要性，这对需要精细空间理解和语义对齐的视觉-语言任务具有普遍启发意义。未来的工作可以探索更高效、更自适应的跨模态特征融合机制。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出GeoLanG框架，旨在解决复杂场景下语言引导抓取任务中跨模态融合不足、泛化能力差的问题。核心方法包括：基于CLIP架构构建端到端多任务框架，实现视觉与语言的统一表征；提出深度引导几何模块（DGGM），将深度信息转化为几何先验并注入注意力机制；设计自适应密集通道集成策略，平衡多层特征贡献。实验表明，该方法在OCID-VLG数据集及仿真/真实场景中实现了精确鲁棒的抓取，为现实场景中的多模态机器人操作提供了可靠方案。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.04231" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>