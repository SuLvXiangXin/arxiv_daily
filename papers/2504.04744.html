<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.04744" target="_blank" rel="noreferrer">2504.04744</a></span>
        <span>作者: Zhu, He, Kong, Quyu, Xu, Kechun, Xia, Xunlong, Deng, Bing, Ye, Jieping, Xiong, Rong, Wang, Yue</span>
        <span>日期: 2025/04/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从2D图像或视频中接地球体可供性的研究众多，但源自2D数据的可供性知识难以直接应用于3D空间。一些工作致力于使用3D点云数据，通过几何学习来分割可供性区域，但它们在接地点见物体的可供性时泛化能力有限，且在遇到相似物体时容易产生混淆。现有的多模态3D可供性方法也存在局限：IAG依赖交互物体的2D检测框，限制了任务可扩展性；OpenAD使用简单的文本标签，缺乏对上下文丰富语言指令的支持；LASO则忽视了真实世界中因点云不完整和物体旋转带来的挑战。</p>
<p>本文受认知科学启发，认为人类学习新物体的可供性结合了图像/视频演示和语言指令，是一个理解语言指令、几何关系和场景信息的复杂综合任务。因此，本文提出了一个结合语言指令、视觉观察和交互来接地3D物体可供性的新任务。核心思路是构建一个多模态、多视角的数据集，并设计一个基于视觉语言模型的端到端框架，将2D与3D空间特征与语义特征融合，实现语言引导的3D可供性接地。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的LMAffordance3D是一个端到端的多模态、语言引导的3D可供性接地框架。其整体流程为：输入包括RGB图像、物体点云和描述交互的语言指令；通过视觉编码器分别提取2D和3D空间特征并融合；将融合后的空间特征通过适配器投影到文本语义空间，并与指令的文本特征拼接，送入视觉语言模型；利用解码器融合视觉语言模型输出的指令特征、语义特征以及空间特征，得到可供性特征；最后通过分割头输出点云上每个点的可供性概率热图。</p>
<p><img src="https://arxiv.org/html/2504.04744v1/x4.png" alt="方法框架"></p>
<blockquote>
<p><strong>图4</strong>：LMAffordance3D模型结构。包含四个主要组件：1) 处理图像和点云的多模态视觉编码器；2) 视觉语言模型及其相关组件（分词器和适配器），用于融合指令令牌、2D和3D视觉令牌；3) 解码器，以2D和3D空间特征为查询，以指令特征为键，语义特征为值，预测可供性特征；4) 用于分割和接地3D物体可供性的头部。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉编码器</strong>：采用轻量化设计。2D视觉编码器使用预训练的ResNet18提取图像特征 $F_{2D} \in \mathbb{R}^{B \times C_I \times H \times W}$。3D视觉编码器使用PointNet++提取点云特征 $F_{3D} \in \mathbb{R}^{B \times C_P \times N_P}$。随后，一个融合模块（MLP+自注意力）将二者结合，得到多模态空间特征 $F_S \in \mathbb{R}^{B \times N_S \times C_S}$。</li>
<li><strong>视觉语言模型与适配器</strong>：选用LLaVA-7B作为主干网络，其参数被冻结。语言指令通过分词器转换为文本特征 $F_T$。设计了一个适配器（包含两个线性层和一个激活层），将空间特征 $F_S$ 投影到与文本特征相同的语义空间并提升通道维度，得到投影特征 $F_{SP} \in \mathbb{R}^{B \times N_S \times C_L}$。$F_{SP}$ 与 $F_T$ 拼接后作为视觉语言模型的输入。</li>
<li><strong>解码器</strong>：基于交叉注意力机制设计。将视觉语言模型的输出拆分为指令特征和语义特征。以空间特征为查询（Query），指令特征为键（Key），语义特征为值（Value），通过解码器得到可供性特征 $F_A$。</li>
<li><strong>分割头</strong>：对可供性特征 $F_A$ 进行上采样，并通过一个由两个线性层、激活层、批归一化层和Sigmoid层组成的头部，最终输出3D物体可供性概率 $O \in \mathbb{R}^{B \times 2048 \times 1}$。</li>
<li><strong>损失函数</strong>：使用焦点损失（Focal Loss）和Dice损失的加权和作为最终损失函数：$Loss = \omega_f L_f + \omega_d L_d$，用于监督点云上的逐点3D热图预测。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，本文首次提出了一个融合语言指令、2D交互图像和3D点云的多模态可供性接地任务与数据集。方法上，创新性地设计了一个适配器-视觉语言模型-解码器的架构，有效利用了大规模视觉语言模型的先验知识，将几何空间特征与丰富的语义指令进行深度融合，从而提升了模型在多种视角（尤其是部分观测和旋转场景）下的泛化能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：实验基于本文新提出的AGPIL数据集，该数据集包含30,972个点云-图像-文本对，涵盖23个物体类别和17种可供性，并提供了全视角、部分视角和旋转视角的数据，每种视角下又分为可见和未见两种设置。实验在单个V100 GPU上使用PyTorch完成。</p>
<p><strong>基线方法</strong>：由于此前没有完全同任务的工作，作者选择了3D AffordanceNet（仅点云）、IAG（图像+点云）、OpenAD（点云+简单文本）和PointRefer（点云+文本）作为基线进行比较。</p>
<p><strong>关键实验结果</strong>：<br>在AGPIL基准测试上的整体结果如表2所示。本文方法（Ours）在所有视角（全视角F、部分视角P、旋转视角R）和两种设置（可见Seen、未见Unseen）下的四项评估指标（AUC、aIOU、SIM、MAE）上均优于所有基线方法。特别是在更具挑战性的未见设置下，性能提升更为显著，这归功于大语言模型蕴含的世界知识。在同一设置下，模型性能随着观测完整性下降而递减：全视角 &gt; 部分视角 &gt; 旋转视角。</p>
<p><img src="https://arxiv.org/html/2504.04744v1/x5.png" alt="可视化结果"></p>
<blockquote>
<p><strong>图5</strong>：不同视角和设置下测试集示例的可视化结果对比。从左至右列为：输入图像、输入点云、真实值（GT）、3D AffordanceNet结果、IAG结果、OpenAD结果、PointRefer结果以及本文方法结果。可以清晰地看到，本文方法在不同条件下的预测结果与真实值最为接近。</p>
</blockquote>
<p>表3进一步展示了本文方法在17种具体可供性类别上的详细性能。在可见设置下，所有类别均有数据。在未见设置中，训练集和测试集的可供性类别完全不重叠（训练集为7类，测试集为10类）。结果显示，模型在多数未见类别上仍能取得合理的性能，证明了其良好的泛化能力。</p>
<p><strong>消融实验</strong>：论文通过消融实验验证了各模块贡献。主要结论包括：1) <strong>多模态融合的有效性</strong>：仅使用点云或仅使用图像的性能均显著低于融合二者；同时使用图像和点云但移除语言指令（仅用类别标签）也会导致性能下降。2) <strong>视觉语言模型的作用</strong>：移除VLM（仅用适配器投影的特征直接解码）或使用更小的VLM（如LLaVA-1.5）都会损害性能，尤其是泛化能力。3) <strong>解码器设计</strong>：使用提出的以空间特征为Query的交叉注意力解码器优于简单的拼接或加法融合策略。4) <strong>训练策略</strong>：在线配对图像与点云（一个图像配对两个点云）的数据增强策略能有效提升性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的、受认知科学启发的多模态3D物体可供性接地任务，并创建了首个支持该任务的多模态、多视角数据集AGPIL。</li>
<li>提出了LMAffordance3D基准方法，这是一个基于视觉语言模型的端到端框架，通过创新的特征融合与解码设计，在复杂视角和未见场景下实现了优越的可供性接地性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法在一定程度上依赖于外部的视觉语言模型（LLaVA），其知识库和性能会影响结果。此外，数据集中图像和点云并非来自同一场景的严格配对，而是基于类别配对，这与完全真实的跨模态对齐存在差距。</p>
<p><strong>对后续研究的启示</strong>：本文的工作表明，结合高层次的语义理解（通过语言指令和VLM）与低层次的几何感知（通过点云和图像），是提升3D理解任务泛化能力的一个有效途径。未来的研究可以探索更高效的VLM集成方式、研究如何在更严格的跨模态对齐数据下进行训练，或将此框架扩展至动态场景、复杂操作规划等更复杂的具身智能任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对三维物体可供性（affordance）的定位问题，即如何在三维空间中确定物体可被操作的位置，以连接感知与行动。提出首个多模态语言引导的三维可供性网络LMAffordance3D，通过视觉语言模型融合2D与3D空间特征及语义信息，并构建了包含全视角、局部视角和旋转视角的AGPIL数据集。实验表明，该方法在AGPIL数据集上有效且优于现有方法，即使在未见过的场景中仍具优越性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.04744" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>