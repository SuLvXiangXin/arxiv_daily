<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PhysiAgent: An Embodied Agent Framework in Physical World - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>PhysiAgent: An Embodied Agent Framework in Physical World</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.24524" target="_blank" rel="noreferrer">2509.24524</a></span>
        <span>作者: Wang, Zhihao, Li, Jianxiong, Zheng, Jinliang, Zhang, Wencong, Liu, Dongxiu, Zheng, Yinan, Niu, Haoyi, Yu, Junzhi, Zhan, Xianyuan</span>
        <span>日期: 2025/09/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前具身智能领域的主流方法分为两类：一是端到端训练的视觉-语言-动作模型，它们直接从指令和观察历史生成控制信号，但泛化能力有限，且数据收集成本高昂；二是分层方法，使用视觉语言模型进行高层任务规划，生成中间子目标或表示来指导底层的VLA模型执行。然而，现有分层方法通常采用僵化的、前馈式的结构，VLM仅作为静态规划器，与VLA之间缺乏有效的双向交互与协调，导致协作效率低下和“接地”能力不足。本文针对VLM与VLA之间协作僵化、无法根据执行情况动态调整这一具体痛点，提出了将大型语言模型智能体领域的模块化协作范式引入物理世界的新视角。本文的核心思路是构建一个免训练的、自主的脚手架框架，通过引入监控、反思、记忆等机制，使VLM能够根据VLA的实时能力反馈，动态地组织和调整任务规划与工具使用，从而最大化VLA的执行潜力。</p>
<h2 id="方法详解">方法详解</h2>
<p>PhysiAgent的整体框架是一个统一的智能体系统，旨在实现VLM与VLA在物理世界中的无缝集成与自适应协作。其核心思想是建立一个双向的信息流：前向流将高层指令分解为可执行的子指令，后向流则通过监控VLA的执行进度并反思，将反馈信息用于调整未来的规划。</p>
<p><img src="https://arxiv.org/html/2509.24524v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：PhysiAgent执行给定任务的工作流程。规划器将高级任务请求转换为VLA可执行的语言指令。同时，监视器跟踪VLA执行这些指令的进度。并行地，反思器评估监视器的输出，生成视觉约束以纠正误判。这些反思结果揭示了VLA的熟练程度，并为规划器下一轮的任务分解提供了有价值的上下文。短期记忆和长期记忆分别捕获步骤级和情节级的执行数据，以支持持续的适应。此外，具身工具箱提供了一系列感知、控制和推理工具，增强了系统的鲁棒性和适应性。</p>
</blockquote>
<p>框架包含五个核心组件：规划器、监视器、反思器、记忆模块和工具箱。</p>
<ol>
<li><strong>规划器</strong>：其核心功能是将原始语言指令l分解为适合VLA执行的可操作中间指令li。其基础形式为 li = Fp(ot, lj&lt;i, l)。它根据当前观察、已执行的子指令和原始任务进行规划。</li>
<li><strong>监视器</strong>：为了解决物理世界中的视觉-语言接地挑战，监视器持续评估VLA执行子指令li的进度。为了平衡效率与效果，它采用一个大小为h的滑动窗口，选取当前帧ot和前序帧ot-h作为输入，评估从ot-h到ot的转变是否反映了朝向li的积极进展。监视器输出离散的进度标志pt ∈ {阻碍、进行中、失败、完成}，即 pt = Fm(ot, ot-h, li, C)，其中C是约束缓冲区。</li>
<li><strong>反思器</strong>：为了纠正监视器可能因VLM固有局限而产生的误判，反思器作为一个双重检查与增强推理模块被引入。它检查视觉转变(ot-h, ot)与预测标志pt之间的一致性。若检测到不一致，反思器会识别失败模式并生成相应的视觉约束c，存入约束缓冲区C，以指导监视器在未来避免类似错误。其公式为：c = Fr(ot, ot-h, li, pt), c → C。</li>
<li><strong>记忆模块</strong>：为使系统能够动态进化，引入了短期记忆Ms和长期记忆Ml。<ul>
<li><strong>短期记忆</strong>：存储每个执行子指令li的情节内的步骤级数据(ot, ot-h, li, pt, c)，为反思器提供细粒度的上下文，使其能基于历史交互进行推理，生成更准确的约束（c = Fr(Ms)）。</li>
<li><strong>长期记忆</strong>：存储每个情节的高级摘要，包括初始帧和最终帧(oinit, ofinal)、指令li以及从短期记忆总结出的VLA执行行为文本摘要di。这使得规划器能够通过分析视觉线索和行为摘要来理解VLA对指令lj≤i的熟练程度，从而进行自适应规划（li+1 = Fp(ot, lj≤i, l, Ml)）。</li>
</ul>
</li>
<li><strong>具身工具箱</strong>：为缩小与物理接地智能体的差距，工具箱提供了一系列VLM可调用的工具，以支持VLA执行并增强系统操作。主要包括三类工具：<strong>感知工具</strong>（查询多摄像头以获得更广视角）、<strong>推理工具</strong>（在遇到困难时协助重新规划或请求人工输入）、<strong>控制工具</strong>（如“回溯”以从失败中恢复）。工具箱被设计为可扩展的。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.24524v1/x3.png" alt="工作流示例"></p>
<blockquote>
<p><strong>图3</strong>：PhysiAgent工作流程的说明性示例。展示了为完成复杂任务，各组件如何以统一和自适应的方式进行协调，并在需要时纳入人类提示来完善规划。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.24524v1/x4.png" alt="视觉约束示例"></p>
<blockquote>
<p><strong>图4</strong>：PhysiAgent视觉约束工作流程示例。1) <strong>误报纠正</strong>：虾被放在桌子上，但任务被错误标记为“完成”。反思器生成视觉约束，指示监视器更仔细地评估虾的位置。2) <strong>漏报纠正</strong>：机械臂正确移向虾，但动作被错误标记为“失败”。反思器生成约束，指出此类运动应被仔细评估。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.24524v1/x5.png" alt="工具箱示例"></p>
<blockquote>
<p><strong>图5</strong>：工具箱示例。包含感知、控制和思考等工具，以增强现实世界应用中的智能体系统。</p>
</blockquote>
<p>与现有方法相比，PhysiAgent的创新点在于：1) 将LLM智能体范式的模块化、双向交互架构引入物理具身系统，取代了传统的刚性分层结构；2) 通过监视-反思闭环与记忆机制，实现了系统在部署期间对VLA能力的自适应理解与进化，这是一种免训练的自适应策略；3) 设计了轻量级、可扩展的具身工具箱，为物理世界中的操作提供了实用的运行时辅助。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在桌面操作场景中进行真实世界实验。使用配备夹爪的6自由度机械臂AIRBOT作为平台，并设置三个RGB摄像头（俯视、正面、腕戴）。VLM使用Gemini 2.0 Flash（规划器、反思器）和Gemini 2.0 Flash Lite（监视器），均未微调。VLA使用了RDT-1B和Diffusion Policy两种策略，并在领域内演示数据上进行了微调。</p>
<p><strong>任务与基线</strong>：基准测试包含三个具有挑战性的桌面操作任务，分为两个复杂度级别：L1任务“抓取含膳食纤维的食物”和“抓取含蛋白质和脂肪的食物”；L2任务“做一顿饭”，需要多步推理和执行。对比的基线包括：1) <strong>Vanilla VLA策略</strong>：直接从观察和指令预测动作；2) <strong>传统分层框架</strong>：静态VLM规划器生成子目标；3) <strong>带人工干预的分层框架</strong>：人类操作员监控执行并在需要时手动提示VLM重新规划子目标，作为理想协调下的性能参考上界。</p>
<p><img src="https://arxiv.org/html/2509.24524v1/x6.png" alt="硬件设置"></p>
<blockquote>
<p><strong>图6</strong>：真实世界实验的硬件设置。</p>
</blockquote>
<p><strong>关键结果</strong>：主要实验结果如图7所示，性能是五次独立试验的平均值。</p>
<p><img src="https://arxiv.org/html/2509.24524v1/x7.png" alt="主要结果"></p>
<blockquote>
<p><strong>图7</strong>：在“我需要更多膳食纤维”、“我想要蛋白质和脂肪”（不超过3个子任务）以及“请为我准备早午餐”（需要5个阶段）等任务上的实验结果。Y轴代表累计任务进度，X轴代表VLA步骤数。结果表明，PhysiAgent框架在任务完成效率方面持续优于原始和分层基线。</p>
</blockquote>
<p>PhysiAgent在三个任务上均以最少的执行步骤成功完成了几乎所有阶段，表现出高效率和鲁棒性。它扩展了RDT和Diffusion Policy的性能边界，能够执行复杂、未见过的任务。相比之下，Vanilla VLA模型难以解释抽象语言指令，在所有任务上均告失败。尽管带人工干预的分层方法也能完成任务，但其性能落后于PhysiAgent，这很可能是由于高层规划器与底层VLA之间的交互不足。PhysiAgent通过其统一架构，实现了VLM与VLA之间的有机整合。</p>
<p><strong>消融实验分析</strong>：虽然论文未展示独立的消融实验图表，但从方法描述和组件功能可以推断各模块的贡献。监视器提供了执行进度的实时评估，是反馈闭环的基础。反思器纠正监视错误并生成约束，是系统实现自我改进和准确接地的关键。短期和长期记忆机制使得反思器和规划器能够基于历史经验进行演化，从而动态适应VLA的能力边界。工具箱则提供了应对物理世界不确定性的实用工具，增强了系统的鲁棒性。这些组件的协同工作共同促成了框架性能的显著提升。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了PhysiAgent，一个免训练的模块化具身智能体框架，能够灵活集成VLM和VLA用于真实世界部署；2) 首次将传统上在语言或模拟领域探索的智能体范式引入物理世界，赋予VLM真实的感知和工具使用能力；3) 在真实机器人操作任务中验证了该框架，展示了任务执行过程中涌现的自我反思能力，并取得了显著的性能提升。</p>
<p>论文自身提到的局限性包括：当前框架的性能仍受限于所使用的基础VLM的固有能力（如视觉理解、推理的准确性），以及需要依赖一个经过一定数据微调、具备基本操作能力的VLA模型。</p>
<p>本文对后续研究的启示在于：它展示了模块化智能体框架在整合异构模型以解决物理世界复杂任务方面的巨大潜力。未来工作可以探索更先进的记忆架构、更复杂的工具集成，以及如何将这种框架推广到更动态、非结构化的环境中。同时，如何量化并优化框架内各组件间的交互效率，也是一个值得深入的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型泛化能力有限，且与视觉语言模型结合时结构僵化、协作效率低的问题，提出了一个面向物理世界的具身智能体框架PhysiAgent。该框架通过引入监控、记忆、自我反思机制及轻量级工具箱，构建了一个自主支架，使视觉语言模型能依据VLA的实时能力反馈动态组织各组件，以最大化利用VLA的潜能。实验表明，该框架在复杂真实机器人任务上显著提升了任务解决性能，实现了VLM的有效自我调节、工具协同与执行过程中的自适应演化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.24524" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>