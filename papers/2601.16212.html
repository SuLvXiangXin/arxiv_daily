<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Point Bridge: 3D Representations for Cross Domain Policy Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Point Bridge: 3D Representations for Cross Domain Policy Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.16212" target="_blank" rel="noreferrer">2601.16212</a></span>
        <span>作者: Haldar, Siddhant, Johannsmeier, Lars, Pinto, Lerrel, Gupta, Abhishek, Fox, Dieter, Narang, Yashraj, Mandlekar, Ajay</span>
        <span>日期: 2026/01/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>构建通用机器人智能体面临的核心瓶颈是缺乏大规模的真实世界交互数据。仿真与合成数据生成提供了一条可扩展的路径，但其有效性受到仿真与现实之间视觉域差距的限制。现有方法如域随机化需要大量人工调优来定义有效的随机化范围；基于任务相关关键点的表示方法虽能抽象视觉外观，但往往依赖人工标注、侧重于桥接形态差异而非视觉差异，且多局限于单任务场景。本文针对“如何利用合成仿真数据实现高效的零样本仿真到现实策略迁移”这一痛点，提出了一个新颖的视角：通过统一的、与领域无关的点云表示来桥接仿真与现实的差距，从而无需显式的视觉或物体级对齐。本文的核心思路是：利用视觉语言模型自动化提取任务相关的点云表示，在此统一表示上训练基于Transformer的策略，最终实现仅使用合成数据训练的智能体在现实世界中的零样本部署。</p>
<h2 id="方法详解">方法详解</h2>
<p>Point Bridge框架包含三个阶段：1）将场景过滤并转换为基于点云的统一表示；2）在此表示上训练基于Transformer的策略；3）部署时使用轻量级流水线进行场景提取以最小化仿真到现实的差距。</p>
<p><img src="https://arxiv.org/html/2601.16212v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：点提取流水线概览。给定场景图像和任务描述，Gemini识别任务相关物体，Molmo和SAM-2进行定位与分割，随后在分割掩码上均匀采样2D关键点，并利用Foundation Stereo提供的深度、相机内外参将其投影至3D，形成统一的点云表示。</p>
</blockquote>
<p><strong>数据收集与合成数据生成</strong>：首先在仿真中为每个任务收集少量人类示教数据 (\mathcal{D}<em>{src})，然后使用MimicGen工具将其扩展为大规模合成数据集 (\mathcal{D}</em>{sim})。MimicGen通过应用SE(3)变换，将源示教片段适配到具有新物体位姿的场景中，从而保留末端执行器与物体间的相对几何关系。</p>
<p><strong>点提取</strong>：这是框架的核心创新模块，旨在为每个观测时刻提取紧凑的任务相关3D关键点集 (\mathcal{P}^{3D}_{t})。</p>
<ol>
<li><strong>VLM引导的场景过滤</strong>：给定初始场景图像 (\mathcal{I}_0) 和自然语言任务描述 (\mathcal{L})，首先使用Gemini-2.5-flash识别任务相关物体类别，然后使用Molmo-7B在图像中定位这些物体，最后使用SAM-2提取并随时间跟踪这些物体的2D分割掩码。</li>
<li><strong>3D投影</strong>：在每个时间步，从每个物体的分割掩码（向内收缩20%以提高鲁棒性）中均匀采样N个2D点。使用Foundation Stereo从立体图像对计算深度图，并结合相机参数将这些2D点提升至3D空间。随后通过最远点采样下采样至M个代表点，并转换到机器人基坐标系。对于仿真数据，则直接从物体网格采样3D点，并模拟真实相机视角和传感器噪声进行投影和加噪，以确保表示一致性。</li>
<li><strong>机器人表示</strong>：机器人末端执行器也被表示为一组关键点，通过在当前机器人位姿 (T_r^t) 上施加一组刚性变换 (T^i) 来计算得到，即 ((\mathcal{P}_r^t)^i) 的位置从 (T_r^t \cdot T^i) 中提取。</li>
</ol>
<p><strong>策略学习</strong>：采用基于解码器的多任务Transformer架构。将机器人点 (\mathcal{P}_r) 和物体点 (\mathcal{P}_o) 组合成点云 (\mathcal{P})，使用PointNet编码器进行编码。对于多任务学习，额外输入由Sentence Transformers编码的语言嵌入 (\mathcal{L})。编码后的表示作为BAKU Transformer策略的输入，其确定性动作头输出末端执行器位姿和夹爪状态。策略使用动作分块和指数时间平均来确保预测轨迹平滑，并通过最小化预测动作与真实动作之间的均方误差进行优化。</p>
<p><strong>策略推理</strong>：部署时，利用与训练时相同的VLM引导流水线从初始图像和指令中提取3D关键点。框架支持多种深度感知策略以平衡性能与吞吐量：主要使用Foundation Stereo（立体图像）；也支持商用RGB-D传感器的深度图；或通过MAST3R和Co-Tracker进行多视角RGB图像的关键点跟踪与三角化。这使得同一训练策略能灵活适配不同的真实世界设置。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在六个真实世界任务上进行评估，包括三个源自仿真的任务（碗放盘子、杯子放盘子、叠碗）和三个仅使用真实数据的任务（叠毛巾、关抽屉、碗放烤箱）。使用Franka Research 3机械臂，传感器包括Intel RealSense RGB-D和ZED 2i立体相机。基线方法为使用图像观测、在仿真与真实数据上联合训练的策略。</p>
<p><img src="https://arxiv.org/html/2601.16212v2/x2.png" alt="任务展示"></p>
<blockquote>
<p><strong>图2</strong>：Point Bridge在六个真实世界任务上的执行情况展示，体现了其跨任务和跨域的能力。</p>
</blockquote>
<p><strong>关键数值结果总结</strong>：</p>
<ul>
<li><strong>零样本仿真到现实迁移</strong>：在单任务设置下，Point Bridge在“碗放盘子”、“杯子放盘子”、“叠碗”任务上的成功率分别为23/30、21/30、24/30，显著优于图像基线（后者在零样本设置下完全失败）。与最强的图像基线（在仿真与真实数据上联合训练）相比，Point Bridge实现了平均39%的性能提升。在多任务设置下，零样本迁移性能进一步提升，平均优于基线44%。</li>
<li><strong>与真实数据联合训练</strong>：当加入少量真实示教数据进行联合训练时，Point Bridge的性能达到近乎完美（单任务：29/30， 30/30， 29/30；多任务：30/30， 30/30， 30/30）。相比仅在真实数据上训练的图像基线，Point Bridge在单任务和多任务设置下分别取得了61%和66%的性能提升。</li>
</ul>
<p>表1和表2以表格形式清晰对比了不同观测模态、数据配置下的任务成功率，直观展示了Point Bridge在零样本迁移和联合训练场景下的显著优势。</p>
<p><strong>消融与分析</strong>：论文系统分析了关键设计选择（Section 5.4）。主要发现包括：1）<strong>点云表示的有效性</strong>：点云表示是性能提升的关键，相比图像表示能更好地泛化到新的物体和场景。2）<strong>深度感知策略比较</strong>：使用Foundation Stereo的立体视觉方案在透明、反光物体上表现最优，但帧率较低（5 Hz）；RGB-D方案速度更快（15 Hz），但对物体材质敏感；纯RGB三角化方案则提供了无需深度传感器的折中选择。3）<strong>多任务学习的增益</strong>：多任务策略通常表现优于或与单任务策略相当，展示了框架的可扩展性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1）提出了Point Bridge框架，利用统一的、领域无关的点云表示，解锁了合成仿真数据用于零样本仿真到现实策略迁移的潜力。2）设计了一个自动化、无需人工标注的VLM引导点云提取流水线，有效桥接了视觉差距。3）提供了支持多种深度感知策略的灵活推理流程，并展示了其在单任务与多任务设置下的卓越性能。</p>
<p><strong>局限性</strong>：论文自身提到，当前方法依赖于现成的视觉语言模型和深度估计模型，其性能上限受这些组件限制；此外，基于立体视觉的流水线会引入计算延迟，影响实时性。</p>
<p><strong>后续启示</strong>：Point Bridge证明了结构化、抽象的场景表示在跨域策略学习中的强大能力。这启发后续研究可以进一步探索更高效、更鲁棒的自动化场景理解与表示构建方法，并尝试将该范式扩展到更复杂的长期任务、动态环境以及涉及非刚性物体的操作中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>Point Bridge框架旨在解决机器人策略学习中真实世界数据稀缺以及模拟与真实环境视觉领域差距的核心问题。该方法采用统一的、领域无关的基于点的3D表示，通过视觉语言模型自动提取点表示，结合Transformer进行策略学习，实现无需显式对齐的零样本模拟到真实转移。实验结果显示，在零样本转移中性能提升高达44%，结合少量真实演示协同训练后提升达66%，显著优于现有基于视觉的协同训练方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.16212" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>