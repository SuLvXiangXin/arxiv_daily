<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.20703" target="_blank" rel="noreferrer">2509.20703</a></span>
        <span>作者: Weiming Zhi Team</span>
        <span>日期: 2025-09-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人领域，从人类演示中学习为遥操作或动觉教学提供了可扩展的替代方案。然而，由于人体与机器人机械臂在形态上的显著差异以及关节可行性约束，直接模仿人类视频演示中的手部运动可能导致机器人违反关节限制。现有方法通常将演示视为严格的运动参考，或采用顺序流程（先确定抓取，再优化轨迹），这可能导致抓取选择不利于后续轨迹的忠实模仿，或忽略了抓取与轨迹之间的相互依赖关系。</p>
<p>本文针对视频演示学习范式下，生成同时满足抓取可行性、轨迹模仿忠实性和运动安全性（无碰撞、符合关节限制）的机器人运动这一具体痛点，提出了新视角：将人类视频演示视为以物体为中心的指导，而非直接模仿人手动作。其核心思路是提出联合流轨迹优化框架，通过一个统一的可微分目标函数，同时优化初始抓取位姿和后续运动轨迹，在尊重机器人自身形态约束的前提下，确保被抓取物体的运动与演示一致。</p>
<h2 id="方法详解">方法详解</h2>
<p>JFTO框架的整体目标是在机器人关节空间中联合优化一条轨迹 ξ = {q_t}，使其最大化一个由三项加权求和的目标函数：轨迹模仿质量 S_T(ξ)、初始抓取质量 S_G(q_0) 和碰撞安全分数 S_C(ξ)。输入是来自视频的物体轨迹、初始物体位姿和静态背景点云，输出是机器人的可行关节轨迹。</p>
<p><img src="https://arxiv.org/html/2509.20703v1/illustrations/methodology_pipeline_colored.png" alt="方法框架"></p>
<blockquote>
<p><strong>图4</strong>：JFTO框架总览。从人类演示视频中提取物体轨迹、抓取位姿和静态场景。这些信息用于定义三个可微分的评分项：轨迹似然（基于流匹配）、抓取分数（结合可行性分类器和与演示的相似性）以及碰撞惩罚。通过梯度下降法在机器人关节空间中对这些项进行联合优化，最终生成可执行的轨迹。</p>
</blockquote>
<p><strong>核心模块一：数据收集与处理</strong>。使用两个校准相机录制演示，通过DUSt3R等3D基础模型重建密集点云，并利用SAM2分割手和物体，HaMeR估计手部位姿。通过ICP对齐连续帧的物体点云，得到物体位姿轨迹 {x̂_t}。静态背景点云被融合用于碰撞检查。</p>
<p><img src="https://arxiv.org/html/2509.20703v1/illustrations/data_collection_figure.png" alt="数据收集"></p>
<blockquote>
<p><strong>图5</strong>：数据收集与处理流程示意图。从双视角图像开始，经过3D重建、手与物体分割、手部姿态估计等步骤，最终提取出物体轨迹、初始抓取位姿和静态背景模型。</p>
</blockquote>
<p><strong>核心模块二：轨迹质量评估（S_T）</strong>。为了对演示中物体位姿轨迹的多模态分布进行概率建模，本文将流匹配扩展到 SE(3) 群上。从多个演示中收集每个时间步 t 的物体位姿样本集 Q_t，视为从分布 p_t(x) 中的采样。学习一个速度场 v_t: SE(3) → R^6，它定义了一个将先验分布（如高斯分布）推向目标分布 p_t(x) 的常微分方程。通过瞬时变量公式，可以计算任何候选位姿 x 在时间 t 的对数似然 log p_t(x)，从而构建密度估计器 f_den(x, t) ≈ log p_t(x)。对于一个候选机器人关节轨迹 ξ，通过正向运动学得到末端执行器轨迹，并假设抓取关系固定，推导出 induced 物体轨迹。轨迹分数 S_T(ξ) 即为该诱导轨迹在所有时间步上 f_den 得分的平均值，衡量其与演示分布的一致性。</p>
<p><img src="https://arxiv.org/html/2509.20703v1/illustrations/trajectory_flow_matching.png" alt="流匹配"></p>
<blockquote>
<p><strong>图6</strong>：在 SE(3) 上进行流匹配的示意图。学习到的速度场可以将基分布（如高斯分布）转换为每个时间步的演示位姿分布。反向积分可以计算样本的对数密度。</p>
</blockquote>
<p><strong>核心模块三：抓取可行性与相似性评估（S_G）</strong>。该模块评估初始关节状态 q_0 对应的抓取位姿 p(q_0)。1) <strong>可行性</strong>：使用抓取位姿生成器生成几何可行的抓取作为正样本，并通过扰动或均匀采样生成负样本。训练一个分类器 f，输入是抓取位姿的傅里叶位置编码 E(p)，输出通过 sigmoid 函数得到可行性分数 σ(f(E(p)))。2) <strong>相似性</strong>：定义一个加权的 SE(3) 距离 D_SE(3)，计算机器人抓取位姿与从视频中提取的一组人类抓取位姿 H 之间的最小距离 D_H(p)。最终抓取分数 S_G(q_0) = σ(f(E(p(q_0)))) - λ * D_H(p(q_0))，平衡了抓取的物理可行性和与人类演示的相似性。</p>
<p><strong>核心模块四：碰撞避免（S_C）</strong>。从融合的背景点云 B 学习一个可微分的距离场 dist_B: R^3 → R。对于轨迹上的每个关节状态 q_t，通过一组代表机器人表面的点 Q(q_t) 来近似其几何。碰撞分数 S_C(ξ) 对所有点和时间步求和，惩罚那些距离小于安全阈值 ℓ 的点，从而在优化中产生避免碰撞的梯度。</p>
<p><strong>创新点</strong>：1) <strong>联合优化</strong>：与顺序方法（先抓取后轨迹）不同，JFTO 同时考虑抓取和下游轨迹，允许选择在整个运动中保持可行的抓取。2) <strong>概率式轨迹模仿</strong>：通过将流匹配扩展到 SE(3)，对演示轨迹进行密度感知的模仿，保留了多模态特性，优于简单的基于距离的目标。3) <strong>统一可微目标</strong>：所有评分项（轨迹似然、抓取分数、碰撞惩罚）都是可微分的，支持端到端的梯度优化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟和真实世界（Agile-X Piper 6-DoF机械臂）中进行评估。涵盖了7种不同的日常操作任务。对比的基线方法包括：1) <strong>顺序基线</strong>：先使用抓取分类器选择最佳抓取，再固定抓取优化轨迹。2) <strong>距离目标</strong>：将轨迹评分 S_T 替换为与演示轨迹的简单点对点距离，而不是流匹配似然。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能</strong>：在真实世界的7个任务中，JFTO（联合优化）的平均任务成功率为 **86.6%**，显著高于顺序基线的 **66.6%**。这验证了联合优化能选择更有利于后续轨迹执行的抓取。</li>
<li><strong>轨迹模仿方法对比</strong>：使用流匹配密度（JFTO-FM）作为 S_T 的方法，其轨迹模仿误差（以最终物体位姿误差衡量）为 <strong>2.21 cm 和 6.72°</strong>，优于使用简单距离目标（JFTO-Dist）的 <strong>3.89 cm 和 11.82°</strong>。这表明概率式建模能更好地捕捉演示本质，生成更准确的模仿轨迹。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.20703v1/illustrations/results.png" alt="结果图"></p>
<blockquote>
<p><strong>图7</strong>：真实世界七个任务的实验结果可视化。每一行展示不同任务下，机器人成功执行由JFTO生成的轨迹序列。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.20703v1/illustrations/trajectory_visualization_background.png" alt="轨迹可视化"></p>
<blockquote>
<p><strong>图8</strong>：轨迹可视化对比。左图为人类演示的物体轨迹（蓝色），右图为JFTO生成的机器人执行所诱导的物体轨迹（红色），显示两者高度一致。</p>
</blockquote>
<ol start="3">
<li><strong>多模态保留</strong>：流匹配能够对演示中的多模态分布进行建模。如图9和图10所示，从学习到的流模型中采样，可以生成多样但合理的物体轨迹变体，而确定性方法则会因模式平均产生不自然的轨迹。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.20703v1/illustrations/multi-modality_visualization_1.png" alt="多模态可视化1"></p>
<blockquote>
<p><strong>图9</strong>：多模态演示的流匹配建模。左侧展示从两个不同演示（红色和蓝色）中提取的物体轨迹。右侧显示从学习到的流模型中采样生成的新轨迹（绿色），它们合理地覆盖了演示的变体。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.20703v1/illustrations/multi-modality_visualization_2.png" alt="多模态可视化2"></p>
<blockquote>
<p><strong>图10</strong>：流匹配（上）与确定性平均（下）在多模态演示情况下的对比。流匹配能生成遵循不同模式的合理轨迹，而确定性平均则产生一条不自然的中庸轨迹。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：通过逐一移除目标函数中的项（S_T, S_G, S_C）进行消融研究。结果显示，缺少任何一项都会导致性能下降，验证了每个组件对于实现可行性、忠实性和安全性都是必要的。例如，移除 S_G 会导致抓取失败，移除 S_C 则会发生碰撞。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.20703v1/illustrations/multi_modal_results.png" alt="消融与多模态结果"></p>
<blockquote>
<p><strong>图11</strong>：左图展示了在多模态搅拌任务上的成功执行。右表总结了消融实验结果，显示了联合优化（Ours）相比顺序基线（Sequential）以及移除各个目标项（Ablations）的优越性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了联合流轨迹优化框架，将抓取相似性、物体轨迹似然和碰撞避免集成到一个统一的可微分目标中，用于视频演示学习。2) 将流匹配扩展到 SE(3)，对演示的物体轨迹进行概率建模，实现了能够保留多模态特性的密度感知模仿。3) 在多种操作任务的模拟和真实实验中验证了JFTO的有效性，表明联合优化相比顺序基线能实现更高的任务成功率和模仿保真度。</p>
<p><strong>局限性</strong>：1) 依赖于上游处理流程（如DUSt3R、SAM2）来从视频中提取物体轨迹和场景，其误差会传播到下游优化。2) 需要多视角视频以进行准确的3D重建。3) 假设抓取关系在整个执行过程中是固定的，未考虑抓握滑移或重抓取。</p>
<p><strong>研究启示</strong>：JFTO为从非结构化视频中生成可行的机器人运动提供了一种原则性的优化方法。后续工作可着眼于：开发更鲁棒的上游感知模块以减少对多视角的依赖；将框架扩展到动态环境或允许抓握调整的任务；探索将学习到的流模型用于更广泛的运动规划或技能组合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对从人类视频演示中学习机器人操作任务时，因形态差异和关节约束导致的运动不可行问题，提出了**联合流轨迹优化**框架。该方法将演示视为**以物体为中心的指导**，而非直接模仿人手动作，通过平衡**可行抓取位姿选择、与演示一致的对象轨迹生成、以及无碰撞的机器人运动**三大目标，生成可行的机器人运动。关键技术包括将**流匹配扩展至SE(3)**空间，对物体轨迹进行概率建模，实现**密度感知的模仿**。框架通过整合抓取相似性、轨迹似然和碰撞惩罚，形成统一的可微优化目标。方法在模拟和真实世界的多种操作任务中得到了验证。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.20703" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>