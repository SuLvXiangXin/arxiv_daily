<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.05536" target="_blank" rel="noreferrer">2510.05536</a></span>
        <span>作者: Farrokh Janabi-Sharifi Team</span>
        <span>日期: 2025-10-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>基于位置的视觉伺服（PBVS）是机器人操作中的关键技术，其性能高度依赖于对目标物体相对于机器人参考坐标系位姿（位置和方向）的准确估计。为了提升估计精度，传统方法常采用集中式传感器融合。在混合眼在手/眼到手视觉配置中，现有方法主要探索了传感器级的数据融合（如虚拟视觉伺服融合、结合扩展卡尔曼滤波EKF与特征选择）或基于距离和可见性条件的动态切换策略。然而，这些方法在动态机器人操作场景中，未能一致地解决从多个摄像头融合位姿估计的挑战，特别是当估计结果存在相关性时。</p>
<p>本文针对动态操作中双视角（眼在手和眼到手）位姿与速度估计的融合难题，提出了一个去中心化的新视角。核心思路是：让两个摄像头独立运行基于李群的自适应扩展卡尔曼滤波器，分别估计目标状态（位姿和速度），然后利用一种专门设计的、能够处理随机位姿且考虑估计间相关性的融合规则，在流形上进行状态级的融合，从而获得更优的融合估计。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架是一个去中心化的双视图估计系统。整体流程如下：一个眼在手摄像头和一个眼到手摄像头分别对运动目标进行观测。每个摄像头独立运行一个自适应李群EKF，该滤波器以目标的位姿测量值作为输入，在状态流形 $\mathbb{SE}(3)\times\mathbb{R}^{3}\times\mathbb{R}^{3}$ （包含位姿、线速度、角速度）上进行预测和更新，输出目标的状态估计及其协方差。最后，将两个滤波器输出的状态估计（表示为李群上的随机成员）通过一个相关性感知融合规则进行融合，得到最终的目标位姿和速度估计。</p>
<p><img src="https://arxiv.org/html/2510.05536v1/general_manipulator.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：系统整体框架示意图。展示了安装在机械臂上的眼在手摄像头和固定在基座的眼到手摄像头共同观测目标物体（带有抓取坐标系{G}）。两个独立的滤波器处理各自视角的测量数据，其后端进行相关性感知融合。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><p><strong>自适应李群EKF</strong>：本文在文献[21]的基础上进行了扩展。滤波器状态定义在矩阵李群 $\mathcal{G_X} = \mathbb{SE}(3)\times\mathbb{R}^{3}\times\mathbb{R}^{3}$ 上，表示为 $\boldsymbol{\mathbf{\mathcal{X}}} = \bar{\boldsymbol{\mathbf{\mathcal{X}}}}\exp([\boldsymbol{\mathbf{\zeta}}]_{\wedge})$，其中 $\bar{\boldsymbol{\mathbf{\mathcal{X}}}}$ 为确定性均值，$\boldsymbol{\mathbf{\zeta}}$ 为零均值高斯扰动。过程模型假设目标在基坐标系{B}下以随机加速度（随机游走）运动。关键创新在于引入了<strong>自适应噪声调协</strong>：通过基于创新序列和指数加权移动平均的方法，在线递归估计过程噪声协方差 $\boldsymbol{\mathbf{Q}}(k)$ 和测量噪声协方差 $\boldsymbol{\mathbf{R}}(k)$，从而增强对测量噪声和目标运动不确定性的鲁棒性。滤波器使用李群上的指数映射和对数映射进行状态更新，并利用右雅可比矩阵 $\mathcal{J}_r$ 进行协方差的传播。</p>
</li>
<li><p><strong>相关性感知融合规则</strong>：这是本文的核心创新。融合问题被形式化为一个在李群上的优化问题（公式7），目标是最小化各估计与融合后估计之间的加权误差平方和，其中权重矩阵是各估计误差协方差（包括交叉协方差，即相关性）的逆。该方法提供了封闭解（公式8-11），能够显式地处理两个独立滤波器估计误差之间的相关性（通过协方差矩阵 $\boldsymbol{\mathbf{P}}<em>{ij}$），从而获得最优的融合均值 $\bar{\boldsymbol{\mathbf{\mathcal{X}}}}</em>{\text{fus}}$ 和协方差 $\boldsymbol{\mathbf{P}}_{\text{fus}}$。</p>
</li>
<li><p><strong>双视图配置与状态定义</strong>：系统使用眼在手和眼到手两种配置。目标状态定义为相对于机器人基座坐标系{B}的位姿 $\boldsymbol{\mathbf{\mathcal{T}}}^{BG} \in \mathbb{SE}(3)$、线速度 $\boldsymbol{\mathbf{v}}^B_G \in \mathbb{R}^3$ 和角速度 $\boldsymbol{\mathbf{\omega}}^B_G \in \mathbb{R}^3$。每个摄像头通过视觉算法（如PnP）提供目标相对于其自身坐标系的位姿测量，这些测量被转换到公共基座坐标系{B}后，作为各自EKF的输入。</p>
</li>
</ol>
<p>与现有方法相比，本文的创新点具体体现在：1) 提出了在新型矩阵李群 $\mathbb{SE}(3)\times\mathbb{R}^{3}\times\mathbb{R}^{3}$ 上进行状态传播和融合的框架；2) 采用了去中心化的状态级融合策略，而非传感器级的测量融合或简单的切换策略；3) 引入了能够处理估计间相关性的最优融合规则；4) 在EKF中集成了自适应噪声估计以提高鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：方法在UFactory xArm 850六轴机械臂上进行了实物验证，并辅以仿真实验。视觉系统采用Intel RealSense D435i摄像头（眼在手）和Intel RealSense L515 LiDAR摄像头（眼到手）。目标物体上附有ArUco标记用于位姿测量。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li>EKF-1：仅使用眼在手摄像头。</li>
<li>EKF-2：仅使用眼到手摄像头。</li>
<li>切换策略：基于机器人-目标距离和可见性，在EKF-1和EKF-2估计之间切换[19]。</li>
<li>传感器级融合：使用集中式EKF直接融合两个摄像头的测量值[18]。</li>
<li>本文方法：所提出的去中心化相关性感知融合框架。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在实物实验1（目标线性运动）中，本文方法在位置和速度估计上均优于所有基线方法。</p>
<p><img src="https://arxiv.org/html/2510.05536v1/result1_position.png" alt="实验1位置误差"></p>
<blockquote>
<p><strong>图6</strong>：实验1中，各方法估计的位置与真实位置在X、Y、Z方向上的对比。本文方法（红色）的轨迹最接近真实值（黑色）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.05536v1/result1_velocity.png" alt="实验1速度误差"></p>
<blockquote>
<p><strong>图7</strong>：实验1中，各方法估计的线速度与真实速度在X、Y、Z方向上的对比。本文方法对速度的估计更平滑、准确。</p>
</blockquote>
<p>具体数值上，本文方法位置估计的均方根误差（RMSE）为0.0086 m，比表现次优的切换策略（0.0128 m）降低了32.9%。速度估计的RMSE为0.0180 m/s，比切换策略（0.0310 m/s）降低了41.9%。</p>
<p>在仿真实验2（目标复杂螺旋运动）中，本文方法同样展现出显著优势。</p>
<p><img src="https://arxiv.org/html/2510.05536v1/position2.png" alt="实验2位置误差"></p>
<blockquote>
<p><strong>图8</strong>：仿真实验2中位置估计的RMSE对比。本文方法在所有方法中误差最低。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.05536v1/velocity2.png" alt="实验2速度误差"></p>
<blockquote>
<p><strong>图9</strong>：仿真实验2中速度估计的RMSE对比。本文方法显著优于其他方法。</p>
</blockquote>
<p>其实验结果显示，本文方法位置RMSE为0.0120 m，比切换策略（0.0254 m）降低了52.8%；速度RMSE为0.0211 m/s，比切换策略（0.0407 m/s）降低了48.1%。</p>
<p><strong>消融分析与组件贡献</strong>：<br>论文通过详细分析阐明了各组件的作用。</p>
<p><img src="https://arxiv.org/html/2510.05536v1/switch_px.png" alt="切换策略分析"><br><img src="https://arxiv.org/html/2510.05536v1/switch_py.png" alt="切换策略分析"><br><img src="https://arxiv.org/html/2510.05536v1/switch_vx.png" alt="切换策略分析"><br><img src="https://arxiv.org/html/2510.05536v1/switch_vy.png" alt="切换策略分析"></p>
<blockquote>
<p><strong>图10-13</strong>：对比了切换策略与本文融合方法在X/Y位置和速度上的表现。切换策略在某个传感器失效时（如被遮挡）会切换至另一个，但其估计在切换点可能不连续。而本文的融合方法能产生更一致、平滑的估计轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.05536v1/adapt_px.png" alt="自适应分析"><br><img src="https://arxiv.org/html/2510.05536v1/adapt_py.png" alt="自适应分析"><br><img src="https://arxiv.org/html/2510.05536v1/adapt_vx.png" alt="自适应分析"><br><img src="https://arxiv.org/html/2510.05536v1/adapt_vy.png" alt="自适应分析"></p>
<blockquote>
<p><strong>图14-17</strong>：对比了固定噪声协方差的EKF与自适应EKF的估计误差。自适应版本能显著降低位置和速度估计的误差，证明了在线调整噪声协方差对提升鲁棒性和精度的有效性。</p>
</blockquote>
<p>总结而言，消融实验表明：1) <strong>相关性感知融合</strong>相较于简单的切换策略，能提供更一致、更准确的估计；2) <strong>自适应噪声调协</strong>机制对于处理实际系统中的不确定性和时变噪声至关重要，能显著提升滤波器的性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li>提出了一个<strong>相关性感知的去中心化融合框架</strong>，用于双视角目标状态（位姿和速度）估计，该框架通过显式考虑估计误差间的相关性，在李群上实现了最优融合。</li>
<li>开发了一种<strong>基于新型矩阵李群 $\mathbb{SE}(3)\times\mathbb{R}^{3}\times\mathbb{R}^{3}$ 的自适应扩展卡尔曼滤波器</strong>，能够在该流形上联合推断目标位姿和速度，并自适应调整噪声统计特性。</li>
<li>在实物和仿真实验平台上进行了全面验证，结果表明所提方法在动态目标跟踪任务中，在估计精度和鲁棒性上均<strong>优于现有的传感器级融合和切换策略</strong>。</li>
</ol>
<p>论文自身提到的局限性主要在于其<strong>运动模型</strong>。该方法假设目标的运动遵循随机游走（随机加速度）模型。虽然该模型简单且适用于多种场景，但在目标动态特性发生剧烈或复杂变化时可能不足以精确描述其运动。</p>
<p>本文工作对后续研究有多方面启示：首先，可以探索集成更复杂、更精确的目标运动模型（如协调运动模型）以进一步提升动态性能。其次，该去中心化融合框架可扩展至两个以上的多传感器系统。最后，将这一高精度的位姿与速度估计器集成到闭环视觉伺服控制或动态抓取规划系统中，验证其在完整机器人任务中的效能，是一个自然的下一步方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对动态机器人操作中姿态和速度估计不准的核心问题，提出一种相关性感知的双视图去中心化融合方法。关键技术包括：使用眼在手和眼到手视觉传感器配置，基于李群（𝕊𝔼(3)×ℝ³×ℝ³）构建两个独立的自适应扩展卡尔曼滤波器进行状态预测与更新，并采用李群上的相关性感知融合规则获得最终融合姿态和速度。实验在配备Intel RealSense相机的UFactory xArm 850机器人上跟踪移动目标，验证了方法的有效性和鲁棒性，相比现有技术有持续改进。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.05536" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>