<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VideoVLA: Video Generators Can Be Generalizable Robot Manipulators - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.06963" target="_blank" rel="noreferrer">2512.06963</a></span>
        <span>作者: Baining Guo Team</span>
        <span>日期: 2025-12-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作的泛化能力是实现开放世界部署和迈向通用人工智能的关键。当前，视觉-语言-动作（VLA）模型通过利用大规模预训练的理解模型（如视觉语言模型）进行感知和指令跟随，减少了对机器人专用数据的需求。然而，现有方法在泛化到新任务、新物体和新环境方面仍然有限。与此同时，大型视频生成模型在给定新颖文本或图像条件时，展现出了卓越的泛化能力和物理合理性，其学习到的物理动态理解和基于指令预测未来状态的能力，与高性能机器人操作器所需的推理和规划能力天然契合。</p>
<p>本文针对现有VLA模型泛化能力不足的痛点，提出了一个全新的视角：能否将大型视频生成模型无缝地转化为通用的机器人操作器？其核心思路是将一个视频扩散Transformer改造为视频-动作扩散Transformer，通过增加动作作为新的输出模态，并联合对视频和动作进行去噪，从而实现给定语言指令和当前视觉观测，同步预测可执行的动作序列及其导致的未来视觉结果。</p>
<h2 id="方法详解">方法详解</h2>
<p>VideoVLA的目标是给定文本指令 𝒯 和当前视觉观测 𝒪，联合预测一个由K个7维动作组成的动作块 𝒜（编码手腕旋转、平移和夹爪状态），以及一个由N帧组成的、描绘执行𝒜后预期视觉未来的视频片段 ℱ（实际预测其潜在表示）。</p>
<p><img src="https://arxiv.org/html/2512.06963v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：VideoVLA整体框架。(a) 文本编码器和视频编码器分别将语言指令和视频片段转换为token序列或潜在表示。(b) 基于Diffusion Transformer的骨干网络，以编码后的语言token和第一帧潜在为条件，联合预测完成任务所需的下一动作块以及代表执行该动作块预期视觉结果的未来帧潜在。粉红色高亮的视频解码器是可选的，仅用于可视化想象的未来帧。</p>
</blockquote>
<p>方法整体流程包含两个核心部分：输入编码和多模态统一未来建模。</p>
<ol>
<li><strong>输入编码</strong>：使用T5文本编码器将语言指令转换为固定长度（226个）的token序列 𝑻。采用CogVideoX中的3D因果VAE作为视频编码器，将视频片段编码为一系列视频潜在 𝒱。由于编码器的因果设计，序列中的第一个潜在 𝑽₁ 仅编码第一帧，即当前观测 𝒪。在训练时编码整个视频片段以获得 𝑽₁ 和未来帧潜在 {𝑽ⱼ}ⱼ=₂ⁿ；在推理时仅编码当前观测以获得 𝑽₁。</li>
<li><strong>统一未来建模</strong>：模型骨干网络采用Diffusion Transformer（DiT）架构，并以预训练的CogVideoX模型初始化。将条件输入（语言token 𝑻 和当前观测潜在 𝑽₁）与预测目标（未来帧潜在 {𝑽ⱼ}ⱼ=₂ⁿ 和动作块 𝒜）在嵌入维度拼接成一个统一的token序列。具体而言，将每个视频潜在的空间维度展平为一维序列，得到 𝑽′₁ 和 {𝑽′ⱼ}ⱼ=₂ⁿ。模型采用DDPM扩散框架，对 {𝑽′ⱼ}ⱼ=₂ⁿ 和 𝒜 添加高斯噪声，并通过扩散损失训练网络去噪。噪声时间步嵌入通过自适应层归一化注入。</li>
</ol>
<p>与现有方法相比，VideoVLA的核心创新点在于：1) <strong>范式转变</strong>：首次大规模利用预训练的视频生成模型（而非理解模型）作为机器人VLA的主干。2) <strong>端到端联合预测</strong>：在单一Transformer架构内，以统一的多模态DiT形式，端到端地联合建模并生成视频和动作，而非将视频生成作为独立的规划模块。3) <strong>双预测策略</strong>：通过同步预测动作及其视觉后果，强制模型学习两者间的强相关性，实验表明高质量的视觉想象与可靠的动作预测正相关，这有助于将视频生成模型的泛化能力迁移到动作领域。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与平台</strong>：使用Open X-Embodiment（OXE）数据集进行预训练，其中包含来自22种不同机器人本体的超过100万条真实世界轨迹。仿真实验在SIMPLER环境中进行，评估谷歌机器人和WidowX机器人。真实世界实验使用Realman机器人（7自由度机械臂加夹爪），并收集了包含“抓取”、“堆叠”、“放置”三个任务的5824个样本进行微调。</p>
<p><strong>基线方法</strong>：对比了RT-1-X、RT-2-X、Octo-Base、Octo-Small、OpenVLA、SpatialVLA、π₀ 和 CogACT。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>域内评估（仿真）</strong>：如表1所示，在SIMPLER的Visual Matching（VM）设置下，VideoVLA在WidowX机器人的4个任务上取得了最高的平均成功率（53.1%）。对于谷歌机器人，在VM和Variant Aggregation（VA）两种设置下的4个任务平均成功率分别为73.1%和62.8%，其中VA设置排名第一，VM设置排名第二。综合所有12个任务，VideoVLA取得了最高的平均成功率（63.0%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.06963v1/figures/SIM_google.png" alt="仿真结果-谷歌机器人"></p>
<blockquote>
<p><strong>图3</strong>：在SIMPLER仿真环境中，谷歌机器人的域内评估结果（VM/VA）。VideoVLA在VA设置下平均成功率最高，在VM设置下排名第二。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.06963v1/figures/SIM_WINX.png" alt="仿真结果-WidowX机器人"></p>
<blockquote>
<p><strong>图4</strong>：在SIMPLER仿真环境中，WidowX机器人的域内评估结果（VM）。VideoVLA取得了最高的平均成功率。</p>
</blockquote>
<ol start="2">
<li><strong>泛化到新物体</strong>：评估谷歌机器人对10个训练未见物体的“抓取”能力。如表2和图5所示，VideoVLA以65.2%的平均成功率大幅领先所有基线方法，并在10个物体中的8个上取得了最佳性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.06963v1/x3.png" alt="新物体泛化"></p>
<blockquote>
<p><strong>图5</strong>：在SIMPLER环境中对谷歌机器人进行新物体泛化评估的结果。VideoVLA在10个新物体中的8个上表现最佳，平均成功率大幅领先。</p>
</blockquote>
<ol start="3">
<li><strong>泛化到新技能</strong>：评估谷歌机器人执行从WidowX机器人迁移而来的、其自身训练数据中未包含的8项技能。如表3和图6所示，VideoVLA以48.6%的平均成功率位列第一，较第二名CogACT（20.4%）高出28.2个百分点，且在所有被评估技能上均表现最佳。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.06963v1/x4.png" alt="新技能泛化"></p>
<blockquote>
<p><strong>图6</strong>：在SIMPLER环境中评估谷歌机器人从WidowX机器人迁移新技能的能力。VideoVLA在所有评估技能上均表现最佳，平均成功率领先第二名28.2个百分点。</p>
</blockquote>
<ol start="4">
<li><strong>真实世界评估</strong>：在Realman机器人上进行“抓取”、“堆叠”、“放置”任务的域内评估。如表4和图7所示，VideoVLA经过微调后取得了64.6%的综合平均成功率，排名第一，尤其在“堆叠”（66.7%）和“放置”（整体成功率56.3%）任务上表现优异。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.06963v1/x5.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图7</strong>：使用Realman机器人在真实世界进行域内评估的结果。VideoVLA在“堆叠”和“放置”任务上表现优异，取得了最高的综合平均成功率。</p>
</blockquote>
<p><strong>消融实验与发现</strong>：论文指出，实验揭示了预测动作与生成视频片段之间的强相关性——当想象的未来（生成视频）与环境实际结果高度一致时，对应的预测动作往往能带来更高的任务成功率。这表明视觉想象的质量可以作为动作可靠性的隐含指标，从而验证了联合预测未来视觉动态与未来动作这一策略的重要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>范式创新</strong>：首次提出并验证了将大规模预训练视频生成模型转化为通用机器人操作器的新范式，突破了现有VLA模型主要依赖预训练理解模型的局限。</li>
<li><strong>有效框架</strong>：提出了VideoVLA，一个基于多模态扩散Transformer的简单而有效的框架，能够端到端地联合预测机器人动作序列及其引发的未来视觉状态。</li>
<li><strong>强泛化能力</strong>：实验表明，得益于预训练视频生成器的知识及其双预测策略，VideoVLA在域内任务、处理新物体以及跨本体技能迁移等方面均展现出卓越的泛化性能，并揭示了视觉想象质量与动作可靠性之间的内在联系。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，视频生成模型通常计算成本较高；此外，当前方法预测的动作序列长度有限（实验中为6步，执行3步），对于长程任务可能需要多次递归调用。</p>
<p><strong>启示</strong>：本工作为机器人学习开辟了一条新路径，即利用生成模型（尤其是视频生成模型）所蕴含的丰富物理世界知识和强大的条件生成能力来提升机器人系统的泛化性与智能水平。随着生成模型能力的持续进步，基于此类模型的机器人系统有望展现出更强大的通用能力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作（VLA）模型在机器人操作中泛化能力有限的问题，提出VideoVLA方法。该方法基于多模态扩散变换器，将预训练的大规模视频生成模型转化为机器人操纵器，核心创新在于采用双预测策略：同时预测动作序列及其引发的未来视觉结果。实验表明，高质量的视觉想象与可靠的动作预测及任务成功高度相关，该方法展现出强大的泛化能力，包括模仿新技能和处理未见过的物体。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.06963" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>