<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.10814" target="_blank" rel="noreferrer">2507.10814</a></span>
        <span>作者: Colin Bellinger Team</span>
        <span>日期: 2025-07-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用目标条件强化学习（GCRL）训练通用机器人抓取系统是主流方法之一。然而，传统方法通常需要智能体从零学习识别每个目标物体以制定合适的抓取策略，这个过程不仅耗时，而且在数据获取上成本高昂。更重要的是，这类系统往往难以泛化到训练分布之外的新物体。现有方法如使用one-hot向量编码或目标物体图像作为目标条件，存在特征共享效率低、对新物体泛化能力差的关键局限性。</p>
<p>本文针对机器人抓取任务中“如何高效地泛化到未见过的物体”这一具体痛点，提出了一种新的视角：将大型预训练接地目标检测模型（如GroundingDINO）与目标条件强化学习相结合。核心思路是利用预训练模型根据文本提示生成目标物体的掩码（Mask）作为目标条件，这种与物体具体外观无关的抽象表示能促进策略学习更高效的特征共享，从而实现对新物体的强泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架旨在学习一个单一策略，用于完成由文本提示（如“右边的苹果”）指定的抓取任务。在每个时间步，流程如下：1) 输入：环境提供的文本目标描述和从机械臂末端执行器摄像头获取的以自我为中心的RGB观测图像；2) 处理：预训练接地目标检测模型根据文本和当前图像，生成一个围绕目标物体的边界框（BB）；3) 转换：将该边界框转换为一个单通道的二值掩码图像（边界框内为白色，外部为黑色）；4) 输出：该掩码与当前的图像和本体感觉观测一同作为策略网络的输入，用于选择下一步动作。掩码生成过程形式化定义为 $g_m(t) = E(o_i(t))$，其中 $E$ 代表根据文本识别目标并生成边界框掩码的整个过程。</p>
<p><img src="https://arxiv.org/html/2507.10814v1/extracted/6615791/Figures/Goal_Represenation.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：三种不同的目标条件表示方法示例（以苹果为目标）。绿色边界框展示了最终用于目标条件的表示形式。从左至右分别为：基于向量的one-hot编码、基于目标物体图像、以及本文提出的基于目标掩码。</p>
</blockquote>
<p>核心模块是<strong>掩码生成模块</strong>和<strong>基于掩码的目标条件策略</strong>。掩码生成模块利用预训练模型（如GroundingDINO）实现开放词汇目标检测，将文本描述映射到图像中的具体区域。策略网络则以当前观测 $o_t$ 和目标掩码 $g_m$ 为条件，输出动作 $a_{t+1}$，即 $\pi(a_{t+1}|o_t, g_m)$。该方法使用PPO算法进行训练，并辅以基于距离的奖励函数。</p>
<p>与现有方法相比，创新点具体体现在使用<strong>掩码</strong>作为目标条件。这种表示具有多个优势：首先，它提供了相对于智能体当前观测的动态目标位置信息；其次，它抽象了目标物体的具体外观细节，使得“到达”动作的学习可以部分独立于特定物体，从而实现了更高效的特征共享和跨物体的策略迁移；最后，掩码的维度低于原始RGB目标图像，且比大型语言模型（LLM）推理成本更低，有利于加快训练。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在MuJoCo模拟器中进行，使用一个UR10e机械臂和2F-85夹爪。数据集包含7个物体（5个用于训练，2个外加1个Blender创建的烧瓶用于测试），均来自object_sim库。</p>
<p>对比的基线方法包括：1) <strong>向量基目标条件</strong>：使用8元素one-hot编码向量；2) <strong>图像基目标条件</strong>：在每一步观测后拼接一个固定的目标物体RGB图像（3x224x224）。</p>
<p>关键实验结果如下：在训练收敛速度和性能上，使用真实（GT）掩码的方法（绿色曲线）明显优于向量基和图像基方法。</p>
<p><img src="https://arxiv.org/html/2507.10814v1/extracted/6615791/Figures/REACH_PLOT_workshop.png" alt="学习曲线与步数对比"></p>
<blockquote>
<p><strong>图2</strong>：（a）不同目标条件方法的学习曲线（回报值）对比，掩码方法收敛更快且达到更高回报；（b）成功抓取所需步数对比，掩码方法能在更少步数内完成任务。</p>
</blockquote>
<p>在泛化能力方面，掩码方法展现出显著优势。如表1所示，对于训练分布内物体，图像基方法成功率为62%，而掩码方法达到89%。对于分布外物体，图像基方法成功率骤降至28%，而掩码方法仍能保持约90%的高成功率。</p>
<p><strong>表1</strong>：不同目标条件方法在分布内和分布外物体上的抓取成功率对比。</p>
<table>
<thead>
<tr>
<th align="left">目标条件方法</th>
<th align="center">分布内成功率</th>
<th align="center">分布外成功率</th>
</tr>
</thead>
<tbody><tr>
<td align="left">One-Hot编码</td>
<td align="center">0.13</td>
<td align="center">0.2</td>
</tr>
<tr>
<td align="left">目标图像</td>
<td align="center">0.62</td>
<td align="center">0.28</td>
</tr>
<tr>
<td align="left">GT掩码</td>
<td align="center"><strong>0.89</strong></td>
<td align="center"><strong>0.9</strong></td>
</tr>
</tbody></table>
<p>进一步，论文评估了使用预训练模型GroundingDINO（G.DINO）生成掩码的实用性。结果表明，直接用G.DINO生成掩码训练的策略，其成功率低于用真实掩码训练的策略。然而，一个关键发现是：<strong>用真实掩码训练的策略，在评估时若换用G.DINO生成掩码，依然能取得较高成功率</strong>（分布内90%）。不过，成功率会随着场景中干扰物体数量的增加而下降（从仅目标物体时的82%降至有2个干扰物时的79%和3个干扰物时的67%），这揭示了预训练模型在复杂场景中可能产生错误检测（假阳性）带来的噪声影响。</p>
<p><img src="https://arxiv.org/html/2507.10814v1/extracted/6615791/Figures/Results.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：使用不同目标条件（GC）在不同任务阶段（开始、中间、结束）的观测可视化。最后一列显示了抓取是否成功，直观展示了掩码方法的有效性。</p>
</blockquote>
<p>消融实验主要体现在对G.DINO生成掩码的评估上。结果总结表明：1) <strong>掩码抽象本身</strong>是提升泛化能力和学习效率的核心贡献；2) <strong>预训练检测模型的噪声</strong>是影响实际部署性能的关键因素，噪声主要来源于对目标物体的错误识别，且随干扰物增多而加剧。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：第一，提出了一个将预训练接地目标检测模型与目标条件强化学习相结合的框架，利用文本生成的目标物体掩码作为抽象的目标条件。第二，通过实验证明，这种掩码基目标条件能实现更快的收敛、更高的性能，并且对分布外物体具有卓越的泛化能力，显著优于传统的向量或图像基方法。第三，深入分析了在实际系统中使用预训练检测模型（G.DINO）引入的噪声及其对策略性能的影响，为后续研究提供了重要参考。</p>
<p>论文自身提到的局限性包括：G.DINO等检测模型在推理时存在时间成本较高的问题，可能影响实时控制；并且检测准确性会随着物体完整性、角度、距离的变化以及干扰物的出现而降低。</p>
<p>这项工作对后续研究的启示在于：掩码作为一种与物体无关的空间目标表示，是促进机器人技能泛化的有效抽象。未来的工作可以集中于：1) 如何减轻或适应预训练感知模型带来的噪声；2) 采用异步学习等范式来解决感知模型推理延迟问题，以实现更高效的实时机器人控制。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何使机器人通过文本指令识别并抓取物体，并能推广至未见过的物体。提出将预训练的基础目标检测模型与目标条件强化学习相结合，利用文本提示生成目标物体的掩码作为抽象目标条件。这种掩码化目标条件提供了与物体类别无关的定位线索，提升了特征共享与泛化能力。在模拟抓取任务中，该方法对训练分布内外的物体均能保持约90%的成功率，且收敛更快、获得更高回报。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.10814" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>