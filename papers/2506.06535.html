<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.06535" target="_blank" rel="noreferrer">2506.06535</a></span>
        <span>作者: Farshad Khorrami Team</span>
        <span>日期: 2025-06-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，语言驱动的机器人抓取（LDRG）主要有两种范式：模块化系统和端到端的视觉-语言-动作（VLA）模型。模块化系统通常先通过视觉基础模型预测目标物体的分割掩码，再使用预训练的抓取检测网络计算抓取姿态。这种方法虽然灵活，但级联的模块可能导致误差累积。VLA模型则直接将从视觉和语言模态到机器人动作的映射建模为一个整体，尽管在特定任务上表现出色，但其部署面临三大挑战：需要大量任务特定的演示数据进行收集、微调计算成本高昂（需数百GPU小时），以及在动态环境（如物体被重新放置或存在视觉相似的干扰物时）中泛化能力有限。</p>
<p>本文针对现有LDRG方法在效率与精度上的平衡问题，提出了一个新的视角：将物体分割掩码作为抓取预测的显式引导机制。核心思路是：一个准确的分割掩码已经指明了查询所指的像素区域，从而隐含地限制了有效抓取的位置；通过一个两阶段训练框架，先预测目标掩码，再利用该掩码对视觉-语言特征进行池化，从而将抓取预测集中在目标区域内，以提升训练效率与测试精度。</p>
<h2 id="方法详解">方法详解</h2>
<p>MapleGrasp的整体框架是一个两阶段训练架构，旨在根据RGB-D图像和自然语言指令预测稳定的抓取姿态。其输入为RGB图像和文本指令，输出为三个密集预测图：抓取质量图、抓取宽度图和抓取角度图，这些图可用于生成4自由度（4-DoF）的俯视抓取矩形，并进一步与外部抓取采样器结合生成6自由度（6-DoF）的定向抓取姿态。</p>
<p><img src="https://arxiv.org/html/2506.06535v3/sec/figures/maplegraspv3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MapleGrasp框架。首先利用预训练的CLIP模型提取视觉和文本嵌入，通过特征金字塔网络和交叉注意力进行融合，得到空间特征图。第一阶段独立训练分割分支以预测物体掩码；第二阶段使用预测的掩码对特征图进行池化，并将池化后的特征输入三个并行的预测头，生成抓取质量、角度和宽度图。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>特征提取与融合</strong>：使用冻结的预训练CLIP模型分别提取图像特征和文本特征。视觉特征通过一个多模态特征金字塔网络处理，并与文本特征通过交叉注意力机制进行融合，生成融合后的空间特征图。</li>
<li><strong>两阶段训练</strong>：<ul>
<li><strong>第一阶段（分割训练）</strong>：冻结抓取预测头，独立训练分割分支。将文本嵌入投影为一个向量，并与空间特征图进行逐像素点积，再经过Sigmoid激活函数生成一个软二值分割掩码。此阶段允许模型在物体定位上适度过拟合，为下游池化提供准确的掩码。</li>
<li><strong>第二阶段（掩码引导的抓取预测）</strong>：端到端训练整个模型。使用第一阶段预测的掩码对空间特征图进行逐元素相乘（掩码引导的特征池化），得到池化后的特征。这些特征经过细化后，输入三个平行的投影头，分别预测抓取质量图、角度图和宽度图。</li>
</ul>
</li>
<li><strong>损失函数</strong>：在标准的Smooth L1损失基础上，引入了加权机制。每个像素的权重为 ( w_p = 1 + \alpha Q_{gt,p} )，其中 ( Q_{gt,p} ) 是表示物体可抓取区域的真实质量图。这使模型更关注适合抓取的区域，在标注稀疏（如每个物体只有一个抓取标注）的数据集上能带来更可靠和高效的学习。</li>
<li><strong>从2D到3D抓取的转换</strong>：对于6-DoF抓取，方法首先从预测的2D抓取矩形中确定点云中的可抓取点子集，然后利用独立的抓取采样器（如Contact-GraspNet）生成一组候选6-DoF抓取姿态。最终选择与可抓取点云子集重叠度最高的候选姿态作为最终抓取。</li>
</ol>
<p><strong>创新点</strong>：与现有同时预测掩码和抓取姿态或端到端预测的方法相比，MapleGrasp的核心创新在于<strong>掩码引导的特征池化</strong>。它将分割任务与抓取预测任务解耦并顺序执行，利用第一阶段得到的准确物体掩码，在第二阶段显式地将抓取预测的搜索范围限制在目标物体区域内，从而缩小了搜索空间，减少了计算量，并降低了在背景或干扰物上预测错误抓取的概率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：主要使用了两个基准数据集：公开的OCID-VLG数据集（包含4-DoF抓取标注）和本文新引入的大规模开源数据集RefGraspNet（包含超过2.19亿个高质量的6-DoF抓取姿态）。此外，还在LIBERO物理仿真环境和真实的Franka机械臂上进行了实验。</p>
<p><strong>对比的Baseline方法</strong>：包括CROG、HiFi-CS、ETRG等基于CLIP的LDRG方法，以及一个结合Molmo（物体识别）、SAM2（分割）和GraspNet（抓取检测）的强零样本基线。在仿真中还与Diffusion Policy、Octo、OpenVLA等VLA模型进行了对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>基准测试</strong>：在OCID-VLG数据集上，MapleGrasp的Top-1抓取成功率达到**88.15%<strong>，比之前最好的方法（ETRG，82.28%）提升了约</strong>7%<strong>。在RefGraspNet数据集的已见物体和未见物体测试集上，Top-1成功率分别达到</strong>89.86%<strong>和</strong>76.92%**，均优于基线方法。</li>
</ol>
<p><img src="%E8%A1%A8%E6%A0%BC%E5%BD%A2%E5%BC%8F%E5%AD%98%E5%9C%A8%E4%BA%8E%E8%AE%BA%E6%96%87%E4%B8%AD%EF%BC%8C%E6%AD%A4%E5%A4%84%E4%BB%A5%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0%E5%85%B3%E9%94%AE%E6%95%B0%E5%80%BC" alt="基准结果表"></p>
<blockquote>
<p>表2展示了在OCID-VLG和RefGraspNet上的定量对比结果，MapleGrasp在各项指标上均取得领先。</p>
</blockquote>
<ol start="2">
<li><strong>训练效率</strong>：MapleGrasp仅需<strong>32个epoch</strong>（约8小时）即可收敛，而其他基线方法需要45-50个epoch（12-16小时）。在数据效率上，仅使用20%-40%的RefGraspNet数据训练时，MapleGrasp的性能提升就显著优于基线。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.06535v3/sec/figures/refgraspnet_training_efficiency_v2.png" alt="训练数据效率对比"></p>
<blockquote>
<p><strong>图6</strong>：在不同比例训练数据下的性能对比。MapleGrasp（橙色线）在任意数据量下均优于基线，且在数据较少时优势更明显，证明了其出色的样本效率。</p>
</blockquote>
<ol start="3">
<li><strong>语言复杂性分析</strong>：将查询指令按提及的属性数量分为4个难度等级（L1简单到L4困难）。随着指令复杂度增加，所有模型性能均下降，但MapleGrasp在所有等级上均保持最高准确率。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.06535v3/sec/figures/refgraspnet_attri_plot_v3.png" alt="语言复杂度分析"></p>
<blockquote>
<p><strong>图5</strong>：不同语言复杂度等级下的Top-1准确率。MapleGrasp（蓝色）在处理包含多个属性（颜色、形状、位置）的复杂指令时鲁棒性更强。</p>
</blockquote>
<ol start="4">
<li><strong>仿真与真实世界实验</strong>：在LIBERO仿真中，MapleGrasp在SPATIAL、GOAL、OBJECT三个任务套件上的抓取成功率与先进的VLA模型（如OpenVLA）相当，且在跨任务套件测试中表现出更好的泛化能力。在真实Franka机械臂实验中，面对未见物体，MapleGrasp取得了**73%<strong>的成功率，比竞争基线高出</strong>11%**。</li>
</ol>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>使用外部模型生成掩码</strong>（Molmo+SAM）：由于外部模型在复杂指代查询上的掩码IoU较低（50-60%），导致抓取准确率显著下降（从88.15%降至78.69%），凸显了任务特定视觉基础模型的重要性。</li>
<li><strong>冻结CLIP层</strong>：虽然有助于物体识别，但限制了视觉-语言特征针对抓取任务的适应性，导致性能下降。</li>
<li><strong>移除交叉注意力，使用MLP-Mixer</strong>：性能略有下降，表明交叉注意力对特征融合的有效性。</li>
<li><strong>使用标准Smooth L1损失</strong>：收敛变慢，最终性能低于加权版本。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>MapleGrasp</strong>，一种新颖的<strong>两阶段、掩码引导特征池化</strong>的LDRG框架，通过将抓取预测限制在目标物体区域内，显著提升了训练效率和抓取精度。</li>
<li>引入了<strong>RefGraspNet</strong>，一个规模空前（超过2亿抓取姿态）的开源语言驱动抓取数据集，为社区提供了宝贵的资源，并证明了其对于提升模型泛化能力的重要价值。</li>
<li>通过大量实验在多个基准（OCID-VLG、RefGraspNet、LIBERO仿真、真实机器人）上验证了方法的优越性，特别是在数据效率、处理复杂语言指令以及泛化到未见场景方面的优势。</li>
</ol>
<p><strong>局限性</strong>：论文指出，当目标物体被严重遮挡或语言查询存在歧义时，仅依靠掩码可能无法准确定位抓取姿态（如定性结果表格的最后一行所示）。</p>
<p><strong>启示</strong>：这项工作表明，在模块化机器人系统中，通过精心设计的、任务解耦的顺序处理流程，可以同时实现高效率、高精度和强泛化能力，为构建完全自主的智能体框架提供了可靠的底层抓取模块。掩码引导的思路也可能启发其他需要精确定位的视觉-语言任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MapleGrasp框架，解决语言驱动机器人抓取中未见物体操作效率低的问题。核心技术是掩码引导特征池化：第一阶段基于CLIP特征预测分割掩码，第二阶段在掩码内池化特征生成像素级抓取预测，降低计算成本。实验表明，在OCID-VLG基准上性能提升7%，在自建RefGraspNet数据集上达到89%抓取准确率，真实机械臂实验对未见物体成功率达73%（超越基线11%）。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.06535" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>