<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.25032" target="_blank" rel="noreferrer">2509.25032</a></span>
        <span>作者: Takanami, Ryosuke, Khrapchenkov, Petr, Morikuni, Shu, Arima, Jumpei, Takaba, Yuta, Maeda, Shunsuke, Okubo, Takuya, Sano, Genki, Sekioka, Satoshi, Kadoya, Aoi, Kambara, Motonari, Nishiura, Naoya, Suzuki, Haruto, Yoshimoto, Takanori, Sakamoto, Koya, Ono, Shinnosuke, Yang, Hu, Yashima, Daichi, Horo, Aoi, Motoda, Tomohiro, Chiyoma, Kensuke, Ito, Hiroshi, Fukuda, Koki, Goto, Akihito, Morinaga, Kazumi, Ikeda, Yuya, Kawada, Riko, Yoshikawa, Masaki, Kosuge, Norio, Noguchi, Yuki, Ota, Kei, Matsushima, Tatsuya, Iwasawa, Yusuke, Matsuo, Yutaka, Ogata, Tetsuya</span>
        <span>日期: 2025/09/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>构建能够在非结构化人类环境中可靠遵循自然语言指令的通用机器人代理是机器人学的核心挑战。当前，Vision-Language-Action (VLA) 模型是达成此目标的有前景范式，但其性能和泛化能力从根本上受限于训练数据集的规模、多样性和质量。现有的大规模真实机器人数据集（如Open X-Embodiment、RT-1、DROID等）主要存在三个关键局限性：1) 主要集中于固定基座的桌面操作，对移动操作（需无缝集成导航与操作）探索不足；2) 很少捕获接触丰富的交互（如按压开关、开门），这些任务需要超越视觉的力-扭矩等多模态感知；3) 缺乏长时域任务轨迹，复杂任务需要分解为子目标和基础动作的层次结构。这些不足阻碍了研究从短时域抓放场景向鲁棒的真实世界辅助任务推进。</p>
<p>本文针对上述瓶颈，提出了AIRoA MoMa数据集，旨在通过一个大规模、多模态的真实机器人数据集，直接应对移动操作、接触丰富交互和长时域任务。其核心思路是构建一个集成了同步六轴腕部力-扭矩信号、视觉与本体感知数据，并带有新颖双层（子目标与基础动作）注释方案的数据集，以促进分层学习和细粒度错误分析，为下一代VLA模型提供关键基准和资源。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的核心是数据集的构建方法与内容，而非一个算法模型。其整体流程是从数据采集、处理到标准化分发的完整pipeline。</p>
<p><img src="https://arxiv.org/html/2509.25032v1/figs/data_collection_pipeline.png" alt="数据处理流程"></p>
<blockquote>
<p><strong>图3</strong>：从数据收集到模型训练的系统化流程。流程始于训练有素的操作员进行数据收集，随后进行初始有效性检查。上传的数据经过严格的质量检查后，通过包括核心分层注释、隐私过滤和同步在内的关键阶段进行处理。最后，数据被标准化为LeRobot格式，确保其处于可供研究社区立即用于模型训练的状态。</p>
</blockquote>
<p><strong>数据采集平台与遥操作</strong>：数据采集使用丰田Human Support Robot (HSR)平台，该机器人配备4自由度机械臂、1自由度手部、2自由度头部和3自由度全向移动基座。为高效收集复杂行为数据，并克服传统游戏杆或基于末端执行器空间控制（需复杂逆运动学求解）的局限性，团队开发了THSR（HSR遥操作系统）领导-跟随装置。该系统通过一对一的关节映射，将领导装置的关节状态直接发送给HSR的对应关节作为角度命令，无需进行逆运动学计算，实现了直观控制。基座平移、头部相机转向和夹爪驱动则通过Joy-Con手柄单独管理。</p>
<p><strong>数据集模态与规模</strong>：数据集提供时间同步、重采样至30 Hz的多模态数据流。每个时间步包括：1) <strong>视觉</strong>：来自头戴式摄像头（全局视角）和腕戴式摄像头（操作特写）的RGB图像（480x640x3）；2) <strong>本体感知</strong>：手臂、夹爪和升降躯干的关节角度，以及机器人基座和末端执行器姿态的速度；3) <strong>力-扭矩</strong>：对接触丰富任务至关重要的六轴腕部力-扭矩信号（Fx, Fy, Fz, Mx, My, Mz）；4) <strong>遥操作信号</strong>：来自遥操作系统的原始操作员控制命令。数据集初始版本包含25,469条轨迹，总计约94小时，平均每条轨迹时长13秒，总数据量约92 GB。</p>
<p><strong>分层任务注释</strong>：数据集的一个核心特征是双层注释方案，旨在促进层次分解。</p>
<ul>
<li><strong>短时域任务</strong>：定义整体目标的高级自然语言指令，例如“烤一片吐司”。</li>
<li><strong>基础动作</strong>：代表原子化、语义连贯片段的低级命令，例如“打开烤箱”、“拿起面包”等。一系列基础动作构成一个SHT的完成。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.25032v1/figs/task_hierarchy.png" alt="任务层次示例"></p>
<blockquote>
<p><strong>图5</strong>：分层任务注释示例。此图说明了如何将“烤一片吐司”这样的高级任务分解为五个基础动作的序列。每个基础动作对应时间序列原始数据中的一个特定片段，并被标注了成功或失败标签，从而能够详细分析错误在整体任务执行中的发生位置。</p>
</blockquote>
<p><strong>数据处理与标准化</strong>：采集后，数据经过同步（将所有传感器数据统一到30Hz）、分层注释、过滤（基于元数据查询、统计异常值检测、手动验证）和隐私处理（使用YOLO检测器自动排除含有人类出现的片段）等流程。最终，数据集被标准化为广泛采用的Hugging Face LeRobot v2.1格式，确保与现有模仿学习方法和基础模型（如RT-1, π0, OpenVLA）的直接兼容性。</p>
<p><strong>创新点</strong>：与现有数据集相比，AIRoA MoMa的创新具体体现在：1) <strong>首次大规模集成</strong>移动操作、接触丰富交互和长时域任务；2) 引入包含<strong>同步力-扭矩信号</strong>的多模态感知；3) 提出<strong>双层分层注释</strong>，支持结构化学习和跨层级错误分析；4) 明确<strong>包含失败案例</strong>（约占6.6%），支持错误检测与恢复研究。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文作为数据集论文，其“实验”部分主要是对数据集本身规模、组成和特性的统计分析，而非模型性能对比。</p>
<p><strong>数据集基准与组成</strong>：数据集在模拟家庭环境的实验室中收集，涵盖厨房、客厅和浴室等场景。包含了七项代表性的家庭任务：打开毛巾架并挂毛巾、按下按钮开关台灯、烤吐司、煮咖啡、拉动链条开关台灯、将拖鞋立在拖鞋架上、用洗碗机洗碗。各任务的具体轨迹数和时长统计如下表所示。</p>
<p><strong>关键统计数据</strong>：</p>
<ul>
<li>总轨迹数：25,469条，总时长约94小时。</li>
<li>失败案例比例：约6.6%，标注了失败原因（如抓取错误、未对准、错误接触）。</li>
<li>基础动作分布：遵循长尾分布，主要集中在“抓取”、“打开”、“放置”等核心操作技能上。</li>
<li>任务时长分布：集中在短至中等时长（4-12秒），表明数据集主要由离散的短时域活动构成，适合训练基础反应式策略。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.25032v1/figs/dataset_graphs.png" alt="技能与时长分布"></p>
<blockquote>
<p><strong>图2</strong>：基础动作技能和持续时间的统计数据。(a) 技能分布呈现长尾模式，其中“抓取”、“打开”和“放置”等基础操作技能构成了收集轨迹的主体。(b) 任务时长分布集中在短至中等范围，大多数任务在4到12秒内完成。这些特征表明数据集主要由离散的短时域活动组成。</p>
</blockquote>
<p><strong>任务分解详情</strong>：</p>
<table>
<thead>
<tr>
<th align="left">短时域任务</th>
<th align="left">轨迹数</th>
<th align="left">成功时长</th>
<th align="left">失败时长</th>
</tr>
</thead>
<tbody><tr>
<td align="left">打开毛巾架并挂毛巾</td>
<td align="left">1315</td>
<td align="left">18.65h</td>
<td align="left">0.49h</td>
</tr>
<tr>
<td align="left">按下按钮开关台灯</td>
<td align="left">746</td>
<td align="left">1.57h</td>
<td align="left">0.08h</td>
</tr>
<tr>
<td align="left">烤吐司</td>
<td align="left">537</td>
<td align="left">17.57h</td>
<td align="left">0.50h</td>
</tr>
<tr>
<td align="left">煮咖啡</td>
<td align="left">483</td>
<td align="left">38.56h</td>
<td align="left">4.27h</td>
</tr>
<tr>
<td align="left">拉动链条开关台灯</td>
<td align="left">401</td>
<td align="left">5.50h</td>
<td align="left">0.33h</td>
</tr>
<tr>
<td align="left">将拖鞋立在拖鞋架上</td>
<td align="left">383</td>
<td align="left">15.08h</td>
<td align="left">1.02h</td>
</tr>
<tr>
<td align="left">用洗碗机洗碗</td>
<td align="left">336</td>
<td align="left">47.50h</td>
<td align="left">2.96h</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表2</strong>：数据集中每个任务的轨迹和记录时长细分。此表总结了七个主要短时域任务的总轨迹数、成功轨迹累计时长和失败轨迹累计时长。包含失败数据旨在支持对更鲁棒策略（如错误检测和恢复）的研究。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 发布了<strong>AIRoA MoMa数据集</strong>，这是一个首次大规模集成移动操作、接触丰富交互和长时域任务的多模态真实机器人数据集；2) 设计了<strong>新颖的双层分层注释方案</strong>，为分层学习和细粒度错误分析提供了结构化监督；3) 明确<strong>包含并标注了失败案例</strong>，为研究错误检测与恢复提供了宝贵资源；4) 提供了完整的<strong>开源数据处理与标准化流程</strong>，确保数据以广泛兼容的LeRobot v2.1格式发布，促进了可重复性和社区应用。</p>
<p>论文自身提到的局限性包括：当前版本仅包含7项主要任务，场景和任务的多样性仍有扩展空间；数据在受控的实验室环境而非完全真实的家庭中收集。</p>
<p>该数据集对后续研究具有重要启示：它为训练和评估需要融合导航、精细操作和物理交互理解的下一代VLA模型提供了关键基准。其分层注释结构鼓励研究任务规划与底层控制分离的层次化学习方法。内含的失败案例为开发具有鲁棒性、能够从错误中学习并执行恢复策略的智能体开辟了新的研究方向。数据集的标准化格式极大降低了使用门槛，有望加速移动操作领域的研究进展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有机器人数据集缺乏移动操作、接触式交互和长时程任务数据的问题，提出了AIRoA MoMa数据集。其关键技术是提供了包含同步RGB、关节状态、六轴腕部力-扭矩信号的多模态数据流，并采用包含子目标和基本动作的两层标注模式，以支持分层学习与错误分析。该数据集初始版本包含25,469个片段（约94小时），为标准化的移动操作研究提供了关键基准。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.25032" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>