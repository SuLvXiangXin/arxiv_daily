<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.13351" target="_blank" rel="noreferrer">2504.13351</a></span>
        <span>作者: Wang, Chen, Xia, Fei, Yu, Wenhao, Zhang, Tingnan, Zhang, Ruohan, Liu, C. Karen, Fei-Fei, Li, Tan, Jie, Liang, Jacky</span>
        <span>日期: 2025/04/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，从人类视频中学习操作任务是教导机器人的一种有前景的方法。然而，许多操作任务（如拧开瓶盖、敲鼓）在执行过程中需要改变控制参数（如力的大小），而仅凭视觉数据无法捕捉这些信息。这限制了机器人仅通过观看人类视频来执行多样化操作任务的能力。</p>
<p>本文针对“从纯视觉数据中难以推断精细控制参数”这一具体痛点，提出利用额外的传感信号（如测量肌肉活动的臂带或记录声音的麦克风）来捕捉人类操作过程中的细节。核心思路是引入一种名为“链式模态”的提示策略，使视觉语言模型能够对多模态人类演示数据（视频结合肌肉或音频信号）进行推理，逐步整合每个模态的信息，从而提炼出任务计划并生成详细的控制参数，最终让机器人能够基于单个多模态人类视频提示执行操作任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体流程包含三个主要组件：1) 收集多模态人类视频；2) 使用链式模态理解多模态人类视频；3) 生成代码并控制机器人。输入是单个多模态人类演示视频（包含RGB图像、肌肉/音频信号、手部姿态），输出是机器人可执行的代码。</p>
<p>核心模块是<strong>链式模态</strong>提示策略。它旨在解决当前先进VLM（如Gemini 1.5 Pro, GPT-4o）在处理交织在一起的多模态长序列输入时，难以关联跨模态信息的问题。CoM不是一次性输入所有模态，而是提示VLM按顺序分析每个模态，提取关键信息，并基于前一步的分析结果逐步聚合，最终生成答案。</p>
<p><img src="https://arxiv.org/html/2504.13351v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：链式模态概述（以力信号为例）。(a) 基线方法 - 合并：将多模态信息（视觉、力、手部姿态）合并为单个输入批次，直接查询VLM生成最终答案。(b) 链式模态：逐步分析每个模态，细化分析以产生最终答案。示例：首先，VLM使用力数据确定何时施加力；然后，结合手部姿态信息，推断人类正在抓握和扭转；接着，结合图像数据，识别动作为拧开瓶盖；最后，VLM将CoM分析转化为机器人可执行的Python程序。</p>
</blockquote>
<p>CoM提示包含三部分：1) 每个模态及其输入数据格式的描述；2) 可用动作集及动作参数的解释；3) 一个视频-分析对的示例，说明应如何分析每个模态以生成带参数的识别动作序列。示例提示仅用于演示输出格式和可用技能库，不包含测试任务或对象。</p>
<p>在技术细节上，肌肉信号（EMG）包含8个通道，采样率200Hz，与60Hz的视频帧率对齐后，取八个通道的最大值作为每个时间步的力信号。音频信号则计算每个时间步的响度。手部姿态使用HaMeR估计指尖的像素位置。</p>
<p>生成机器人代码时，VLM基于CoM的分析结果，结合预定义的机器人API描述，生成具体的Python程序。这些API得益于先进的感知模型，例如，通过查询Gemini 1.5 Pro和RGB-D图像来定位目标物体。</p>
<p>与现有方法相比，创新点在于这种<strong>渐进式、分模态的推理策略</strong>。它更适应当前VLM处理长上下文和多模态关联的能力，能够更有效地利用力信号进行任务阶段分割，并整合视觉和手部姿态信息来细化动作和参数。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了四个任务进行评测：按压方块、插入插头、打鼓、开瓶。这些任务具有长任务视界、力敏感和双手操作等特点。每个任务包含10个测试视频（物体和相机视角不同）。使用了两个先进的VLM模型：Gemini 1.5 Pro和GPT-4o。</p>
<p>对比的基线分为两类：1) <strong>不同输入模态</strong>：仅图像、无图像（力+手姿）、无力、无手姿、全模态；2) <strong>不同VLM推理流程</strong>：合并所有输入直接生成答案、合并输入但为每个模态生成独立答案、分离每个模态输入后直接生成一个最终答案、分离输入并为每个模态生成独立答案再汇总、以及本文的CoM方法。</p>
<p><img src="https://arxiv.org/html/2504.13351v1/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：链式模态的定性结果。展示了CoM为四个评估视频生成的任务计划。CoM成功地将视频分割为子任务，并在每个阶段指定了技能、力度和目标物体。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.13351v1/x5.png" alt="定量结果"></p>
<blockquote>
<p><strong>图5</strong>：链式模态的定量结果。使用Gemini和GPT在三个任务上比较CoM与基线方法。该图显示CoM在准确率上显著优于其他基线方法。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ol>
<li><strong>CoM提升理解能力</strong>：CoM在提取精确任务计划和控制参数方面达到<strong>60%</strong> 的准确率。仅依赖视觉数据的方法准确率为**0%<strong>，而简单合并所有模态一次性查询的方法平均准确率仅为</strong>17%**。CoM相比次优的“分离-分离”基线，在Gemini和GPT上分别带来了超过19%和17%的性能提升。</li>
<li><strong>力信息至关重要</strong>：包含力输入的基线方法（“无手”和“全模态”）显著优于不包含力的方法（“无力”）。力信息帮助VLM更好地分割视频阶段，使提取的任务计划与真实情况之间的相似度得分平均提升了**42%**。</li>
<li><strong>手部姿态辅助精细操作</strong>：在开瓶任务中，只有使用全部模态输入的方法取得了非零的成功率。该任务需要提取指尖旋转方向等精细信息，手部姿态数据提供了关键帮助。</li>
<li><strong>机器人执行结果</strong>：在真实机器人评估（开瓶、插入插头、擦拭白板、打鼓）中，CoM生成的程序平均成功率为**73%<strong>（20次试验中的成功次数，具体见论文表II），在泛化到新物体和配置时表现出色。作为对比，人工编写代码的“Oracle”方法平均成功率为</strong>92%**。</li>
</ol>
<p>消融实验总结：图像对于识别任务物体至关重要；力信息极大地帮助了任务阶段分割和参数提取；手部姿态对于理解需要精细手部运动的操作是关键补充。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li><strong>链式模态提示策略</strong>：一种新颖的提示方法，使VLM能够通过顺序分析逐步整合信息，从而有效推理多模态人类演示视频。</li>
<li><strong>一次性操作程序生成</strong>：构建了一个从单个多模态人类演示视频生成机器人控制程序的完整流程，并能提取精细的控制参数（如力的大小）。</li>
<li><strong>展示了方法的泛化性</strong>：在两种先进VLM上验证了一致性，生成的代码能在不同机器人平台上执行，并泛化到新的物体和任务配置。</li>
</ol>
<p>论文提到的局限性包括：目前依赖专用传感器（肌电臂带、麦克风）来获取力和音频信号；手部姿态依赖于外部视觉模型进行估计。</p>
<p>本研究对后续工作的启示在于：<strong>多模态数据融合</strong>是提升从演示中学习能力的关键方向，特别是对于需要触觉或力反馈的任务；<strong>设计适合基础模型推理范式的交互接口</strong>（如链式推理）能显著提升其解决复杂问题的性能；<strong>代码作为中间表示</strong>结合强大的感知模型API，为跨 embodiment 的任务传递和泛化提供了灵活且有力的途径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决了机器人仅从视觉视频学习复杂操作任务时，难以获取关键控制参数（如力度）的核心问题。为此，作者提出了**Chain-of-Modality (CoM)** 提示策略，该方法利用视觉语言模型，逐步整合视频、肌肉活动与声音等多模态信息，以推理并生成详细的任务计划与控制参数。实验表明，该方法在提取任务计划和控制参数的准确率上相比基线**提升了三倍**，并在真实机器人实验中展现出对新任务和物体的强泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.13351" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>