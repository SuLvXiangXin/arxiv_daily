<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>STORM: Search-Guided Generative World Models for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>STORM: Search-Guided Generative World Models for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.18477" target="_blank" rel="noreferrer">2512.18477</a></span>
        <span>作者: Keze Wang Team</span>
        <span>日期: 2025-12-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，机器人操作领域的主流方法是视觉-语言-动作（VLA）模型，如OpenVLA、RT-2和CogACT。这些模型利用来自网络规模基础模型的预训练知识，展现出强大的泛化能力。然而，它们存在一个关键的架构瓶颈：通常冻结预训练的视觉主干，并将复杂的推理任务委托给大型语言模型（LLM）组件。这种设计导致了一个根本性的不匹配——它将丰富的、连续的时空动态信息，强行投影到离散的、符号化的语言流形上。这使得操作所需的关键信息（如细微的空间关系、接触动态和动作的精确因果后果）变得模糊或完全丢失，导致现有VLA模型在需要物理基础推理的鲁棒操作任务上表现不佳。</p>
<p>本文针对VLA模型在物理交互中“推理与感知脱节”的痛点，提出了“视觉前瞻”的新视角。其核心思路是：与其让LLM用语言推理可能发生什么，不如让智能体“看到”将会发生什么，即“先预测，再行动”。本文提出的STORM框架，通过将扩散动作生成、条件视频预测和基于搜索的规划统一起来，将规划过程建立在显式的、模拟的物理未来之上，以实现更鲁棒的决策。</p>
<h2 id="方法详解">方法详解</h2>
<p>STORM框架是一个用于时空推理的生成式循环。其整体决策流程遵循公式：$A_{t}^{<em>}=\text{MCTS}(s_{t},\pi_{\text{vla}},M_{w})$。即，蒙特卡洛树搜索（MCTS）作为核心协调器，整合了用于提议多样化动作的VLA策略（$\pi_{\text{vla}}$）和用于模拟其结果的视频预测模型（$M_{w}$），最终选择最优动作$A_{t}^{</em>}$执行。</p>
<p><img src="https://arxiv.org/html/2512.18477v1/images/STORM.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：STORM的整体架构。决策循环由MCTS（核心协调器）主导，它利用VLA策略（$\pi_{\text{vla}}$）提议候选动作，并利用视频预测器（$M_{w}$）模拟这些动作的视觉结果，最终选择最优动作$A_{t}^{*}$执行。虚线循环代表了用于前瞻驱动规划的迭代模拟过程。</p>
</blockquote>
<p>框架包含三个核心模块：</p>
<ol>
<li><p><strong>基于扩散的VLA策略（$\pi_{\text{vla}}$）</strong>：该模块作为学习的先验，用于高效引导在高维动作空间中的搜索。它采用扩散模型架构，能够对复杂的、多模态的动作分布进行建模，避免确定性策略可能导致的模式平均与崩溃问题。在每一步，它接收当前的多模态信念状态$s_t$，输出$K$个候选动作序列及其对应的先验概率：$\pi_{\text{vla}}(s_{t})\rightarrow{(A^{(1)},p^{(1)}),\dots,(A^{(K)},p^{(K)})}$。这些提议极大地修剪了搜索空间，使MCTS能够聚焦于任务相关的分支。</p>
</li>
<li><p><strong>生成式视频世界模型（$M_{w}$）</strong>：该模块作为生成式世界模型，近似环境的动态和奖励函数，实现视觉前瞻。它基于iVideoGPT构建，采用自回归Transformer在量化视觉标记上进行预测。观测图像通过VQ-VAE编码为离散标记，Transformer则根据过去的标记、指令嵌入和候选动作$A^{(j)}$自回归地预测未来标记。为了确保预测与任务目标对齐，模型使用混合损失进行微调：$\mathcal{L}=\mathcal{L}<em>{\text{video}}+\lambda</em>{\text{reward}}\mathcal{L}<em>{\text{reward}}$。其中$\mathcal{L}</em>{\text{video}}$是标记上的交叉熵损失，保证视觉保真度；$\mathcal{L}<em>{\text{reward}}$是预测奖励的均方误差损失，引导模型学习任务感知的动态。该模型的输出是模拟的未来帧$s^{\prime}</em>{t}$和标量奖励估计$\hat{r}<em>{t}$：$M</em>{w}(s_{t},A^{(j)})\rightarrow(s^{\prime}<em>{t},\hat{r}</em>{t})$。</p>
</li>
<li><p><strong>基于MCTS的搜索引导规划</strong>：MCTS协调整个循环，进行前瞻规划。它构建一棵树，其中节点是状态，边是动作及其统计信息（访问次数$N$、总价值$W$、平均价值$Q$、先验概率$P$）。每个决策步骤进行$N_{\text{sim}}$次模拟，每次模拟包含四个阶段：</p>
<ul>
<li><strong>选择</strong>：从根节点开始，递归选择能最大化PUCT分数的动作（公式5），该分数平衡了利用（高$Q$值）和探索（高先验$P$但低访问次数$N$）。</li>
<li><strong>扩展</strong>：当到达一个未扩展的叶节点时，调用VLA策略$\pi_{\text{vla}}$为该节点生成$K$个候选动作，并以对应的先验概率$p^{(k)}$初始化子节点。</li>
<li><strong>评估</strong>：对扩展出的一个未访问子节点，使用世界模型$M_{w}$模拟执行其对应动作，得到下一状态和预测奖励$\hat{r}$，以此作为该模拟路径的价值$V$。</li>
<li><strong>反向传播</strong>：将评估得到的价值$V$沿搜索路径反向传播，更新路径上所有边（动作）的统计信息$N, W, Q$。</li>
</ul>
</li>
</ol>
<p>完成所有模拟后，选择根节点处访问次数最多的动作作为最终执行动作$A_{t}^{*}$，这种方法更具鲁棒性，并支持重新规划和失败恢复。</p>
<p>与现有方法相比，STORM的核心创新在于：<strong>用显式的、视觉基础的视频预测模型替代了在抽象潜在空间中进行规划的世界模型</strong>，使得搜索过程建立在具体、可解释的模拟视觉未来之上。此外，其<strong>解耦的架构</strong>将VLA视为黑盒提议策略，使得STORM具有高度模块化特性，无需内部修改或大量重新训练即可与各种现有VLA模型集成。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在SimplerEnv仿真基准中使用WidowX机械臂进行评估。VLA模块使用预训练的CogACT-Base（70亿参数），未进行微调。生成式世界模型基于预训练的iVideoGPT-medium Transformer，在Bridge数据集上进行动作条件化和奖励预测头的微调。MCTS规划器参数为：每步模拟次数$N_{\text{sim}}=8$，规划深度$D=3$，折扣因子$\gamma=0.9$，探索常数$c_{\text{puct}}=1.0$，VLA每次扩展提议$K=8$个动作。</p>
<p><strong>对比方法</strong>：包括RT-1-X、Octo-Base、Octo-Small、OpenVLA以及作为基线的CogACT。</p>
<p><strong>关键实验结果</strong>：在四个具有挑战性的操作任务上，STORM取得了最先进的平均成功率<strong>51.0%<strong>，显著超过了CogACT的</strong>47.9%</strong> 以及其他基线方法。</p>
<p><img src="https://arxiv.org/html/2512.18477v1/images/success_grid.jpg" alt="任务成功率对比表"></p>
<blockquote>
<p><strong>表I</strong>：STORM在四个操作任务上的成功率（%）对比。STORM在所有任务上均取得最佳性能，平均成功率达51.0%，超越了包括其基础模型CogACT在内的先前方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.18477v1/images/video_predict_compare_3_0.png" alt="视频预测定性结果"></p>
<blockquote>
<p><strong>图2</strong>：视频预测定性结果（任务：将胡萝卜放在盘子上）。以VLA提议动作为条件的预测结果（中行）与真实执行结果（底行）高度一致，证明了世界模型捕捉关键因果动态的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.18477v1/images/failure_grid.jpg" alt="失败恢复案例研究"></p>
<blockquote>
<p><strong>图3</strong>：在“将胡萝卜放在盘子上”任务中的失败恢复案例研究。上图：基线CogACT模型在初始抓取失败后陷入重复循环。下图：STORM利用其前瞻规划能力，在相同初始失败后重新评估，并找到了一条新的、成功的轨迹完成任务，展示了其从错误中恢复的战略能力。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过对比“动作+奖励”和“仅动作”两种训练方式的世界模型，分析了奖励监督的影响。结果表明，奖励监督对于学习任务相关的动态至关重要。</p>
<p><img src="https://arxiv.org/html/2512.18477v1/images/radar_chart_video_metrics.jpg" alt="视频预测指标雷达图"></p>
<blockquote>
<p><strong>图4</strong>：奖励监督对学习高保真世界模型关键性的消融研究雷达图。使用完整目标（‘动作+奖励’）训练的模型在所有评估指标（FVD、LPIPS、PSNR、SSIM）上均优于没有奖励监督（‘仅动作’）的模型，表明奖励信号迫使模型学习任务相关的因果结构，而不仅仅是表面视觉模式。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li>提出了<strong>STORM框架</strong>，一种新颖的搜索引导生成式世界模型，它将基于扩散的VLA、生成式视频世界模型和MCTS集成在一起，为机器人操作实现了显式的时空推理。</li>
<li>证明了<strong>奖励增强的视频预测器</strong>可以作为一种高效的生成式世界模型，显著提高了动作条件视觉推演的保真度和任务相关性。</li>
<li>通过实验表明，STORM在任务成功率和失败恢复方面均优于强大的VLA基线，<strong>凸显了将生成式前瞻与基于搜索的规划相结合的优势</strong>。</li>
</ol>
<p>论文自身提到的局限性在于，其性能最终受限于当前世界模型实例的<strong>预测保真度</strong>。例如，在需要精确接触物理控制的“堆叠绿色积木在黄色积木上”任务中，STORM的性能与基线持平，这表明视觉预测模型在捕捉细微、非线性动态方面存在挑战。这并非搜索范式本身的失败，而是一个明确的研究方向。</p>
<p>对后续研究的启示在于：STORM的成功验证了“视觉前瞻”与“搜索规划”结合范式的有效性。未来的工作可以沿着两个方向深入：一是<strong>集成更高保真度、更具物理感知的生成式世界模型</strong>（如融入物理引擎或更精细的动态模型），以突破在精密操作任务上的性能瓶颈；二是<strong>探索更高效的搜索算法或规划策略</strong>，以降低多步视觉模拟带来的计算成本，推动该范式在更复杂、更长期任务中的应用。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型依赖抽象语言推理、难以进行细粒度物理时空推理的问题，提出了STORM框架。其核心方法整合了扩散式动作生成、条件视频预测和蒙特卡洛树搜索规划，通过视觉推演进行基于前瞻的评估与优化。在SimplerEnv基准测试中，STORM取得了51.0%的平均成功率，超越现有最佳模型；其奖励增强的视频预测将FVD分数降低了75%以上，显著提升了时空保真度和长时程任务中的重规划能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.18477" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>