<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>IFG: Internet-Scale Guidance for Functional Grasping Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>IFG: Internet-Scale Guidance for Functional Grasping Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.09558" target="_blank" rel="noreferrer">2511.09558</a></span>
        <span>作者: Deepak Pathak Team</span>
        <span>日期: 2025-11-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大规模视觉语言模型（VLM）在互联网规模数据上训练后，展现出强大的物体分割和语义理解能力。然而，这些模型虽然能将机器人引导至物体的粗略区域，却缺乏精确控制灵巧机械手进行三维抓取所需的几何理解。另一条技术路线是基于仿真的抓取生成框架，它们通过优化力闭合等能量函数来产生大量抓取姿态数据，并用于训练基于扩散模型的抓取采样器。但这类方法存在关键局限：其抓取提议通常通过采样物体凸包周围的点来初始化，导致大量抓取指向物理上不可达或不合适的区域；更重要的是，它们在整个物体表面无差别地生成抓取，无法与下游任务（如抓握把手或按钮）所需的功能性区域对齐。本文旨在填补这一空白，提出IFG方法，其核心思路是：将VLM提供的高层语义理解，与基于物理、任务感知的合成抓取生成相结合，从而生成稳定、自然且与任务需求对齐的抓取，整个过程无需任何人工采集的训练数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>IFG的目标是学习一个通用灵巧抓取可供性模型，其输入为场景点云和指定待抓取物体的文本提示，输出为机器人手的可行抓取姿态。为实现此目标，IFG首先构建了一个包含几何精确且语义丰富的抓取姿态的大规模数据集生成流程，随后将该数据蒸馏至一个扩散模型中，以实现从深度输入到可执行抓取的快速合成。</p>
<p><img src="https://arxiv.org/html/2511.09558v1/flow_chart_v6.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：IFG方法整体框架。给定物体网格和任务提示，首先通过多视角渲染、VLM分割与3D反投影识别任务相关区域；随后在该区域引导下进行基于力闭合目标的几何抓取优化与仿真评估；最终将生成的鲁棒抓取数据蒸馏至一个扩散模型，实现从深度相机点云到抓取姿态的实时推理。</p>
</blockquote>
<p><strong>整体流程</strong>如算法1所示：1) <strong>有用区域提议</strong>：给定物体网格O，从均匀采样的相机视角渲染n张RGB图像V。使用VLM f（如GPT-4V）处理V，生成m个语义标签R，描述物体的有用区域。对于每个标签，结合物体分割模型g（SAM）和部件级分割模型h（VLPart）为每张图像生成分割掩码，并将其反投影至3D物体网格上。通过基于掩码大小的两均值聚类过滤错误分割，并通过投票算法选择排名前60%的网格面作为有用区域U。2) <strong>几何抓取合成</strong>：基于区域U构建分割凸包。为每个抓取初始化，通过最远点采样将手放置在膨胀后的凸包上，并添加随机噪声到手腕位姿和手指关节角。随后优化一个能量函数E以得到候选抓取。3) <strong>仿真评估</strong>：对每个抓取生成d个关节角扰动版本，在IsaacGym中执行Lift或Pick &amp; Shake任务，计算d+1次试验的平均成功率作为该抓取的平滑标签，过滤低成功率抓取，得到鲁棒抓取数据集G。4) <strong>扩散模型蒸馏</strong>：将数据集G用于训练一个条件扩散模型，该模型以从深度相机数据计算得到的Basis Point Set（BPS）和带噪声的抓取假设作为输入，通过去噪过程输出精炼后的可行抓取。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>抓取表示</strong>：一个灵巧抓取g定义为<code>g = (T, R, θ)</code>，其中<code>T∈ℝ³</code>和<code>R∈SO(3)</code>代表手腕的平移和旋转，<code>θ∈ℝ^DoF</code>代表手部关节角（对于LEAP手，DoF=16）。</li>
<li><strong>优化能量函数</strong>：抓取合成阶段的能量函数为<code>E = E_fc + w_dis * E_dis + w_joints * E_joints + w_pen * E_pen + w_spen * E_spen</code>。其中<code>E_fc</code>近似力闭合；<code>E_dis</code>基于手部接触点鼓励手-物接近；<code>E_joints</code>惩罚关节限位违反；<code>E_pen</code>惩罚手-物穿透；<code>E_spen</code>惩罚手部自穿透。在单物体设置中，设<code>w_spen=0</code>以产生更多样化的抓取。</li>
<li><strong>关键改进</strong>：与基线方法Get a Grip相比，IFG的核心创新在于：a) <strong>语义引导初始化</strong>：抓取初始化仅限于由VLM识别的任务相关区域U构建的分割凸包，而非整个物体凸包。b) <strong>使用全域抓取</strong>：采用在全手指内侧区域采样接触点的全域抓取，而非仅在指尖采样的精准抓取，以产生更稳定、成功率更高的抓取。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.09558v1/diffusion_network_diagram_v2.png" alt="扩散模型"></p>
<blockquote>
<p><strong>图2</strong>：扩散模型蒸馏示意图。模型以从深度相机点云计算的BPS和带噪声的抓取作为输入，通过去噪过程输出精炼的、可执行的抓取姿态，其架构设计类似DexDiffuser。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在单物体和密集场景两种设置下生成抓取数据集并评估。单物体场景使用了Get a Grip数据集中24个多样物体，每个物体在5个尺度下采样，IFG与基线各生成200个抓取。密集场景选自DexGraspNet2的35个场景，每个场景平均3-4个物体，每个物体有3-4个分割提示，为每对提示-物体生成200个抓取。所有抓取使用LEAP手在IsaacGym仿真中执行Lift（垂直抬升手腕）和Pick &amp; Shake（抬升并施加扰动）任务进行评估。</p>
<p><strong>对比方法</strong>：主要基线包括Get a Grip（单物体合成抓取生成）、DexGraspNet2及其相关模型GraspTTA、ISAGrasp（密集场景抓取生成）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>单物体抓取</strong>：如表II所示，在Pick &amp; Shake和Lift任务上，IFG的成功率（16.14%， 51.11%）均超过Get a Grip（11.82%， 50.93%）。表I展示了部分物体的详细成功率，IFG在多数物体上表现更优。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.09558v1/single_object_v2_5.png" alt="单物体结果"></p>
<blockquote>
<p><strong>图3</strong>：单物体评估中，IFG在排名前三的分割提示下均优于Get a Grip基线，证明了提示和VLM对抓取生成过程的引导作用。</p>
</blockquote>
<ol start="2">
<li><strong>密集场景抓取</strong>：如表III所示，IFG的Lift成功率（32.23%）与经过预处理和过滤的DexGraspNet2数据集训练的基线模型（如ISAGrasp的32.51%）相当，证明了IFG强大的抓取生成能力。表IV的详细物体分析显示，IFG在许多物体上（如马克杯、电钻）成功率显著高于基线。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.09558v1/x1.png" alt="密集场景对比1"></p>
<blockquote>
<p><strong>图4</strong>：IFG与Get a Grip的定性对比。例如，Get a Grip常抓取瓶子底部，而IFG能稳健地抓握瓶颈，产生更类人的抓取。</p>
</blockquote>
<ol start="3">
<li><strong>语义引导的有效性</strong>：如图6和图7所示，基于置信度的方法（如DexGraspNet2）生成的抓取大多集中在场景中最易抓取的物体上。而IFG通过分割条件控制，可以指定抓取任何特定物体，因此在难抓取、被遮挡的物体上表现更好。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.09558v1/x2.png" alt="密集场景对比2"></p>
<blockquote>
<p><strong>图5</strong>：基于置信度的方法倾向于抓取最容易的物体（如场景边缘的物体），而IFG可通过分割条件控制抓取任意指定物体。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.09558v1/x3.png" alt="密集场景对比3"></p>
<blockquote>
<p><strong>图6</strong>：DexGraspNet2的抓取生成模型回避难抓取物体，而IFG方法因有VLM分割的功能性引导，更专注于这些物体并取得更高成功率。</p>
</blockquote>
<p><strong>消融实验</strong>：论文虽未明确设置独立消融实验章节，但通过对比Get a Grip（无语义引导、使用精准抓取）与IFG（有语义引导、使用全域抓取）的结果，间接证明了语义引导初始化（提升抓取功能性与自然度）和使用全域抓取（提升稳定性与成功率）这两个核心组件的贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了IFG框架，首次将互联网规模VLM的语义理解与基于仿真的力闭合几何抓取优化紧密结合，实现了无需人工数据的、任务导向的灵巧抓取生成。2) 设计了包含VLM分割、3D反投影、语义引导优化和仿真评估的完整数据生成流水线，产生的抓取兼具鲁棒性、自然性和功能性。3) 将生成的数据蒸馏至扩散模型，实现了从深度相机点云到抓取姿态的实时、可部署推理。</p>
<p><strong>局限性</strong>：1) 方法未考虑动态物体，因为分割基于单时刻图像。未来可扩展至连续视频流。2) 方法主要针对力闭合抓取，不适用于非力闭合抓取场景。</p>
<p><strong>启示</strong>：IFG展示了利用大规模预训练模型（VLM）为机器人底层控制（抓取）提供高层语义指导的有效范式。这种“语义先行，几何细化”的思路可推广至其他需要精细物理交互的机器人任务中。同时，其完全基于合成数据生成与蒸馏的流程，为减少对昂贵真实机器人数据依赖提供了可行路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对大型视觉模型缺乏几何理解、无法精确控制机器人手进行3D抓取的问题，提出IFG方法。关键技术是结合互联网规模模型的语义分割（SAM与VLPart）与基于仿真的局部感知力闭合优化，生成针对任务相关区域的功能性抓取位姿，并蒸馏训练扩散模型以实现实时点云抓取合成。核心结论是实现无需人工标注数据的高性能语义抓取。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.09558" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>