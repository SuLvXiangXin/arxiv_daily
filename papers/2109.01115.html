<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2109.01115" target="_blank" rel="noreferrer">2109.01115</a></span>
        <span>作者: Nair, Suraj, Mitchell, Eric, Chen, Kevin, Ichter, Brian, Savarese, Silvio, Finn, Chelsea</span>
        <span>日期: 2021/09/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，让机器人通过视觉学习多样化操作任务的主流方法依赖于目标状态（如目标图像）进行任务指定。目标图像已接地于机器人的观察空间，便于自监督学习。然而，该方法存在关键局限：(a) 人类提供目标图像不便；(b) 容易过度指定任务细节（例如，指定单个物体的推动任务时，目标图像也包含了所有其他物体和机器人自身的位置），导致奖励信号稀疏；(c) 对于非目标到达型任务（如“向右移动”），可能指定不足。自然语言为任务指定提供了一种便捷灵活的替代方案，但面临的核心挑战是如何将语言接地到机器人的高维观察空间。</p>
<p>本文针对从离线数据中规模化学习语言接地这一具体痛点，提出了新视角：结合自主收集的（可能高度次优的）离线机器人数据集与事后众包的自然语言标注。核心思路是：利用这些数据训练一个简单的分类器，预测状态变化是否完成语言指令，从而得到一个语言条件化的奖励函数，进而用于离线多任务强化学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法称为语言条件化离线奖励学习（Language-conditioned Offline Reward Learning, LOReL）。其整体流程如论文图2所示：首先，利用众包为离线交互数据（左）标注自然语言描述；其次，使用这些标注数据训练一个语言条件化的奖励函数（中）；最后，将此奖励函数与一个学习到的视觉动力学模型结合，通过模型预测控制（MPC）来执行由自然语言指定的视觉运动任务（右）。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：语言条件化离线奖励学习（LOReL）流程。左侧为离线机器人交互数据集；中间部分通过众包标注学习语言条件化奖励函数；右侧结合学习到的视觉动力学模型与奖励函数，通过模型预测控制执行语言指定的任务。</p>
</blockquote>
<p><strong>核心模块1：奖励函数学习</strong>。该方法的核心是学习一个参数化的奖励函数 $R_{\theta}(s_0, s, l)$，其输入为初始状态 $s_0$、当前状态 $s$ 和语言指令 $l$，输出一个标量，预测从 $s_0$ 到 $s$ 的转变是否完成了指令 $l$。该函数被实现为一个二元分类器。其训练数据构建如下：</p>
<ul>
<li><strong>正例</strong>：来自标注数据集 $D$ 中的轨迹 $(s_0, s_T, l)$。为增加正例数量，采用噪声标注策略：对于一条标注为 $l$ 的轨迹，任何满足 $i \leq \alpha T$ 和 $j \geq (1-\alpha)T$ 的状态对 $(s_i, s_j, l)$ 都被标记为正例。</li>
<li><strong>负例</strong>：包含两种类型。一是<strong>指令不匹配负例</strong>：选取来自不同语言指令 $l&#39; \neq l$ 的轨迹的初始和最终状态 $(s&#39;_0, s&#39;_T, l)$。二是<strong>时间反转负例</strong>：将同一指令下的状态对顺序反转，即 $(s_T, s_0, l)$，作为负例。</li>
</ul>
<p><img src="https://..." alt="训练数据构建"></p>
<blockquote>
<p><strong>图3</strong>：LOReL训练数据构建。左：正例，初始/最终图像转变满足语言指令；中：负例，初始/最终状态满足另一个不同的指令；右：负例，初始和最终图像顺序反转。</p>
</blockquote>
<p>训练时，使用平衡批次的正例和两种负例，并通过最小化二元交叉熵损失来优化分类器。为防止过拟合，对图像应用仿射变换和颜色抖动等数据增强，并对语言指令的嵌入向量添加均匀噪声。此外，该方法固定使用预训练的 distilBERT 句子编码器将自然语言指令编码为固定长度的向量，以利用预训练语言模型的知识并促进泛化。</p>
<p><strong>核心模块2：基于视觉模型预测控制的策略执行</strong>。学习到奖励函数后，结合一个在全部离线数据上训练得到的、任务无关的视觉前向动力学模型 $s_{t+1} \sim p_{\phi}(s_t, a_t)$。给定语言指令 $l$ 和初始状态 $s_0$，通过模型预测控制来实例化策略：采样 $M$ 条长度为 $H$ 的动作序列，通过动力学模型预测未来状态 $\hat{s}^m_{t+H}$，并使用奖励函数 $R_{\theta}(s_0, \hat{s}^m_{t+H}, l)$ 评估每条序列。然后，使用交叉熵方法（CEM）优化动作序列以最大化奖励，并执行最优序列的第一个动作，在环境中闭环运行。</p>
<p><img src="https://..." alt="策略执行"></p>
<blockquote>
<p><strong>图4</strong>：使用LOReL执行语言条件化策略。从左到右：给定初始状态和任务指令，通过视觉动力学模型预测不同动作序列的未来状态；使用LOReL奖励函数根据指令对这些预测状态进行评分；经过多轮优化后，执行最佳动作序列完成任务。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法相比，LOReL的创新具体体现在：1) 不假设离线数据的最优性，能够利用随机、脚本化或次优的自主收集数据，扩展了可用数据源；2) 采用上下文依赖的（依赖于初始状态）分类器作为奖励函数，能够灵活表示多对多的语言-任务映射，并适用于非目标到达型和需持续执行的任务；3) 将预训练语言模型与离线机器人数据结合，旨在实现从有限或程序化标注到自然语言指令的零样本泛化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>模拟实验</strong>：在基于Meta-World构建的模拟环境中进行，包含一个Sawyer机器人与抽屉、水龙头和两个马克杯交互。离线数据集包含50,000条由随机策略收集的轨迹，并使用环境状态程序化生成了2311条唯一指令进行标注。评估6个已见任务。</li>
<li><strong>真实机器人实验</strong>：在Franka Emika Panda机器人上进行，机器人位于一张IKEA桌上方，桌上有抽屉、柜子和各种物体。使用一个包含3000条轨迹（150,000帧）的现有离线数据集（来自在线RL训练的 replay buffer），并通过亚马逊 Mechanical Turk 进行众包标注，获得了约10,000条自然语言描述。</li>
</ul>
<p><strong>基线方法</strong>：</p>
<ul>
<li><strong>LCBC</strong>：语言条件化行为克隆，模仿离线数据中的行为。</li>
<li><strong>LCRL</strong>：语言条件化离线Q学习，将每条轨迹的最终状态标记为对应语言指令的奖励1，其余为0。</li>
<li>**Goal-Image (Pixel/LPIPS)**：使用真实目标图像作为任务指定，在MPC中分别使用L2像素距离或LPIPS相似性作为成本。</li>
<li><strong>Oracle</strong>：使用真实动力学模型和真实奖励的CEM规划器（性能上界）。</li>
<li><strong>Random</strong>：随机策略。</li>
</ul>
<p><strong>关键实验结果</strong>：<br><img src="https://..." alt="模拟任务成功率对比"></p>
<blockquote>
<p><strong>图6</strong>：在6个模拟语言条件化任务上，LOReL（Ours）的成功率超过语言条件化模仿学习（LCBC）和Q学习（LCRL）以及目标图像任务指定（LPIPS/Pixel）25%以上。结果基于3个随机种子各100次试验计算。</p>
</blockquote>
<p>在模拟实验中，LOReL在6个任务上的平均成功率显著高于所有基线方法，比次优的LCBC高出超过25%。LCRL几乎无法学习，表明联合学习语言接地和控制非常困难。使用像素距离作为成本的目标图像方法性能与随机策略相当，因为机器人试图匹配机械臂位置而非操作物体，凸显了目标图像过度指定任务的局限性。使用LPIPS相似性有所改善，但仍比LOReL差约30%。</p>
<p><strong>泛化能力</strong>：<br>论文测试了LOReL对未见自然语言指令的零样本泛化能力，比较了使用预训练语言模型（LOReL）与不使用（LOReL (-PM)）的情况。</p>
<table>
<thead>
<tr>
<th align="left">指令类型</th>
<th align="left">LOReL 成功率</th>
<th align="left">LOReL (-PM) 成功率</th>
</tr>
</thead>
<tbody><tr>
<td align="left">原始指令</td>
<td align="left">56±1%</td>
<td align="left">40±1%</td>
</tr>
<tr>
<td align="left">未见动词</td>
<td align="left">51±3%</td>
<td align="left">33±2%</td>
</tr>
<tr>
<td align="left">未见名词</td>
<td align="left">51±1%</td>
<td align="left">39±4%</td>
</tr>
<tr>
<td align="left">未见动词+名词</td>
<td align="left">47±2%</td>
<td align="left">17±3%</td>
</tr>
<tr>
<td align="left">未见自然语言指令</td>
<td align="left">46±2%</td>
<td align="left">21±1%</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：对未见指令的泛化能力。使用预训练语言模型（LOReL）相比从头训练（LOReL (-PM)）在已见和未见指令上均有显著性能提升。</p>
</blockquote>
<p>结果表明，使用预训练语言模型后，即使替换动词、名词或两者，甚至使用人类创造性重述的自然语言指令，成功率下降幅度很小（最多10%）。而不使用预训练模型时，性能不仅基线更低，对未见指令的泛化能力也大幅下降（下降最多达23%），证明了预训练语言模型对学习和泛化至关重要。</p>
<p><strong>真实机器人结果</strong>：<br><img src="https://..." alt="真实机器人场景与标注"></p>
<blockquote>
<p><strong>图7</strong>：真实机器人实验场景（左）与部分众包标注示例（右）。机器人成功执行了“open the right drawer”、“move the stapler”等5个语言指令任务。</p>
</blockquote>
<p>在真实机器人实验中，使用众包标注的离线数据集训练后，LOReL方法成功使机器人完成了“open the right drawer”、“move the stapler”、“open the left drawer”、“push the green marker to the right”和“open the cabinet”这五个自然语言指定的视觉运动任务。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了LOReL框架，一种通过结合离线（可能次优）机器人数据与事后众包自然语言标注，来学习语言条件化奖励函数的方法，实现了从自然语言到机器人观察空间的规模化接地。</li>
<li>在模拟和真实机器人实验中证实了该方法的有效性：其性能显著优于语言条件化模仿学习和目标图像指定方法，并能够成功执行真实的语言指令任务。</li>
<li>证明了固定预训练语言模型对于从有限、程序化标注数据中学习语言接地，并实现到自然语言指令零样本泛化的重要性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，该方法形式化主要捕捉那些体现在状态变化中的任务，可能不适用于路径依赖型任务（例如“慢慢关上抽屉”）。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>大规模预训练模型（如语言模型、视觉模型）中蕴含的知识可以显著提升机器人学习任务指定和数据效率。</li>
<li>完全离线的学习范式结合丰富但次优的数据源与灵活的任务指定（如自然语言），是迈向通用机器人技能学习的一条可行且可扩展的路径。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何让机器人根据自然语言指令执行视觉操作任务，以克服目标图像指定方式的不便与局限性。核心方法是利用离线机器人数据集（包括次优的自主收集数据）和众包自然语言标注，训练一个简单的分类器来预测状态变化是否完成指令，从而构建语言条件化的奖励函数，并用于离线多任务强化学习。实验表明，该方法在语言条件化操作任务上，性能比目标图像规范和语言条件化模仿技术提升超过25%，成功使机器人能根据“打开右边抽屉”等自然语言指令完成操作。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2109.01115" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>