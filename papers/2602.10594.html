<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.10594" target="_blank" rel="noreferrer">2602.10594</a></span>
        <span>作者: Penny Sweetser Team</span>
        <span>日期: 2026-02-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习（IL）使机器人能够从演示中学习复杂技能，但通常需要大量演示，收集成本高昂。先前工作探索使用“流”（flow，即点轨迹）作为中间表示，以利用人类视频替代部分机器人演示。然而，现有方法主要关注于预测物体或机器人/手部特定点上的流，无法完整描述交互运动。同时，仅依赖流来实现对人类视频中场景的泛化能力有限，因为流本身无法捕捉精确的运动细节。此外，基于场景观察来产生精确动作，可能导致以流为条件的策略过拟合训练任务，削弱了流所指示的泛化能力。本文针对这些局限性，提出了一种名为SFCrP的新方法，其核心思路是：通过一个跨具身（cross-embodiment）的场景流预测模型（SFCr）来预测任意点的运动轨迹，并设计一个以流和裁剪点云为条件的策略（FCrP），使其既能遵循流的整体运动以实现泛化，又能基于局部观察调整动作以完成精确任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>方法整体框架包含两个核心部分：跨具身场景流预测模型SFCr和以流及裁剪点云为条件的策略FCrP。训练时，SFCr从少量机器人演示（如10个）和人类视频（如30个）中学习，FCrP则仅使用机器人演示进行训练。执行时，系统接收来自单个第三人称视角相机的点云观测，SFCr预测以机器人夹爪为中心的裁剪区域内的点轨迹（流），FCrP则基于此流以及当前及历史的局部点云和本体感知信息，输出精确的机器人动作。</p>
<p><img src="https://arxiv.org/html/2602.10594v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧：使用30个人类视频和10个机器人演示训练跨具身流预测模型SFCr和流条件策略FCrP。右侧：执行期间，从单个相机获取点云观测并进行流预测。右侧图片展示了每个任务的起始和成功状态。</p>
</blockquote>
<p><strong>SFCr：跨具身场景流预测模型</strong>。该模型基于Transformer解码器构建。输入包括：1) <strong>点云令牌</strong>：通过最远点采样获取局部点群的中心，使用PointNet提取特征并加入空间编码；2) <strong>任务嵌入</strong>；3) <strong>流查询令牌</strong>：对应轨迹起始点的空间编码。模型输出每个查询点的预测轨迹。训练时，预测目标为轨迹点相对于其查询点的位移（Fi - F0），并最小化L1损失。为促进跨具身泛化并减少对机器人/手部具体形状的记忆，论文采用了两种关键技术：首先，对图像进行分割，将机器人/手部区域内的点云颜色替换为统一值（1,0,1），并增加一个维度标记点是否属于机器人/手部；其次，随机丢弃那些大部分点被标记为机器人/手部的点云群令牌，迫使模型基于其近似位置进行推理而非记忆形状。</p>
<p><strong>FCrP：以流和裁剪点云为条件的策略</strong>。这是一个基于扩散模型的策略，通过逐步去噪生成动作，条件信息包括预测的流 ℱ 以及三个状态观测 {sf, st-1, st}。每个状态观测包含局部裁剪的点云 X 和本体感知数据 g（夹爪及两指位置）。关键设计在于<strong>局部化与对齐</strong>：1) <strong>点云裁剪</strong>：对于每个状态，仅保留以机器人夹爪为中心的方盒区域内的点云，并以夹爪位置为原点进行中心化。流 ℱ 和本体感知点也以流状态 sf 的夹爪位置为中心进行归一化。这使得策略的观测完全局部化，不含绝对空间信息，从而能够跟随流进行泛化，并基于局部点云调整动作。2) <strong>流-状态-动作对齐机制</strong>：策略预测从流状态 sf 开始的一系列动作，并配有一个执行掩码（execution mask），用于匹配从 sf 到当前状态 st 已执行的动作与流 ℱ 的运动。这使得策略能够基于同一个预测的流，推理出任意数量的后续动作，实现了流预测与策略推理的频率解耦。</p>
<p><strong>创新点与平衡设计</strong>。与现有方法常使用流特征向量作为条件不同，本文直接使用原始流和裁剪点云作为条件。为了在利用点云实现精确操作和避免其导致过拟合之间取得平衡，论文引入了两种关键策略：1) <strong>使用预测流进行策略训练</strong>：在训练FCrP时，使用训练好的SFCr模型对机器人演示预测流，以此替代真实流作为条件。这增强了策略对预测流不准确的鲁棒性。2) <strong>随机掩码点云</strong>：以0.5的概率将整个点云观测替换为零。这迫使策略在训练时更多地依赖流条件，从而缓解因过度依赖点云而导致的过拟合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在三个真实世界任务上评估：叠衣服（Fold Cloth）、开抽屉（Open Drawer）和拿碗（Pick Bowl）。拿碗任务有7个不同变体（#0-#6），其中#4-#6仅在人类视频中见过，用于测试泛化能力。训练数据通常为每个任务10个机器人演示（R10）和30个人类视频（H30），拿碗#4-#6仅使用30个人类视频（R0+H30）。对比的基线方法包括：DP3（点级感知+扩散策略）、RISE（空间卷积+Transformer+扩散策略）、SUGAR（点云分组Token+Transformer）。同时进行了消融实验，包括无分割（w/o SG）、无点云观测（w/o PC）、未使用预测流训练（w/o PF）、不掩码点云（w/o MP）。</p>
<p><img src="https://arxiv.org/html/2602.10594v1/x2.png" alt="拿碗任务设置"></p>
<blockquote>
<p><strong>图2</strong>：拿碗任务中碗的位置（暖色矩形框）和不同实例。#4-#6没有对应的机器人演示。</p>
</blockquote>
<p><strong>流预测评估</strong>：如图3所示，在平均位移误差（ADE）和最终位移误差（FDE）指标上，本文方法（Ours）在完整数据集（R10+H30）和4-Fold验证设置下均优于基线方法ScaleFlow-L。即使仅使用1个机器人演示和9个人类视频（R1+H9），其预测误差也与使用完整数据集相当，证明了方法的高跨具身数据效率。消融实验（Ours w/o SG）表明，在没有机器人数据（R0+H30）时，分割能有效缩小跨具身外观差距。</p>
<p><img src="https://arxiv.org/html/2602.10594v1/x3.png" alt="流预测误差对比"></p>
<blockquote>
<p><strong>图3</strong>：不同设置下（随机采样和方盒内采样），流预测的平均位移误差（ADE）和最终位移误差（FDE）对比（对数坐标）。Ours方法在多种数据配置下均表现优异。</p>
</blockquote>
<p><strong>机器人操作成功率</strong>：表II展示了使用完整数据集（R10+H30）的结果。本文方法（Ours）在所有任务上取得了最高的成功率（平均96.67%），并且在仅见于人类视频的场景（Pick Bowl #4-#6）上表现出强大的空间和实例泛化能力（成功率95%）。在数据受限情况下（表III，每任务仅1个机器人演示），Ours方法仍能达到70%的平均成功率，显著高于基线。消融实验表明：1) 无点云观测（w/o PC）的策略在需要精确操作的任务（如Open Drawer）上完全失败，凸显了点云观察对动作调整的必要性；2) 使用点云但不使用预测流训练和掩码（w/o PF&amp;MP）的策略在精确任务上成功率高，但在泛化场景（Pick Bowl #4-#6）中出现过拟合，成功率下降；3) 完整的Ours方法通过平衡对点云和流的依赖，同时实现了高任务成功率和强泛化能力。</p>
<p><strong>失败模式与深入分析</strong>：DP3在单任务上成功率尚可，但难以区分拿碗任务中不同的碗位置，偶尔会移动到错误位置。RISE和DP3均无法泛化到未见过的碗位置（#4-#6）。SUGAR具有良好的空间泛化能力，但在需要精细点云感知的开抽屉任务上表现不佳（表IV）。本文方法无点云观测的版本（w/o PC）无法完成开抽屉任务的第一阶段（勾住把手）。图4通过对比DP3在全场景点云和本文方法在局部裁剪点云上选取的参考点（reference points），解释了裁剪点云如何通过使参考点更集中、更密集地分布在目标物体上，从而提升策略在精确任务中的性能。</p>
<p><img src="https://arxiv.org/html/2602.10594v1/x4.png" alt="点云裁剪效果对比"></p>
<blockquote>
<p><strong>图4</strong>：在开抽屉任务中，DP3编码器从全场景点云(a)和局部裁剪点云(b)中选择的参考点（红色）对比。裁剪后，参考点更集中于目标物体（抽屉把手）周围。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了SFCr模型，能够高效地从跨具身数据（人类视频和机器人演示）中预测场景中任意点的运动轨迹。2) 设计了FCrP策略，通过以原始流和局部裁剪点云为条件，并引入流-状态-动作对齐、预测流训练、随机点云掩码等机制，在实现精确操作的同时，显著缓解了扩散策略的过拟合问题，获得了强大的空间和实例泛化能力。3) 通过系统的实验验证了流作为一种表示，能够桥接群体级空间关系感知与点级细节识别，对齐机器人演示与人类视频，并有效减少策略过拟合。</p>
<p><strong>局限性</strong>：论文指出，对于需要高动作精度的任务，策略仍然严重依赖点云观测。此外，当机器人或物体快速运动时，流预测可能不准确，且当前方法尚未充分利用动作历史信息来优化流预测。</p>
<p><strong>研究启示</strong>：本文工作表明，将稠密的运动先验（流）与局部化的精细感知（裁剪点云）相结合，是平衡模仿学习策略泛化能力与精确性的有效途径。未来研究可探索更高效的流预测架构、利用动作历史信息来修正流预测，以及将类似框架应用于更复杂的多阶段任务或动态环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习需要大量演示、成本高昂的问题，以及现有流（flow）方法在利用人类视频时无法描述交互运动、泛化能力有限的缺陷，提出了SFCrP方法。该方法包含SFCr场景流预测模型（用于跨体现学习，从人机视频预测点轨迹）和FCrP策略（以流和裁剪点云为条件，遵循流运动并调整精确动作）。实验表明，该方法在多种真实任务中优于最先进基线，并展现出对仅见于人类视频的场景的强空间和实例泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.10594" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>