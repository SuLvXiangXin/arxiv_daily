<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.14577" target="_blank" rel="noreferrer">2602.14577</a></span>
        <span>作者: Yan Wang Team</span>
        <span>日期: 2026-02-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>自动驾驶领域的视觉-语言-动作模型通常采用基于模仿学习并结合强化学习的生成式规划器。当前主流生成式规划器主要分为两类：基于扩散的模型和基于令牌的模型。前者通过多步去噪并行生成连续动作轨迹，但存在模态对齐困难、训练效率低、泛化能力有限等问题；后者将动作离散化为令牌进行自回归解码，实现了多模态统一表示，但存在因果累积误差和不可逆解码的缺陷，即一旦某个令牌被解码，后续过程无法修正，导致对噪声敏感，容易产生轨迹级失败。</p>
<p>本文针对这两种范式优势互补的特点，旨在结合扩散模型的迭代优化能力与令牌模型的灵活解码优势，以提升规划的精确性和鲁棒性。核心思路是：以一个预训练的掩码扩散大语言模型为基础规划器，通过一种新颖的块级混合专家架构，显式地为生成专家注入一个独立的优化专家，并设计混合强化学习策略协同优化两者，从而实现“先生成，后优化”的精确驾驶。</p>
<h2 id="方法详解">方法详解</h2>
<p>DriveFine的整体框架是一个两阶段流程：首先通过生成专家并行解码出初始轨迹，然后由优化专家对其进行一步优化。视觉和语言输入被对齐到统一语言空间，一组被掩码的轨迹令牌经过若干步并行去噪生成初步轨迹，随后该完整轨迹（已解除掩码）作为输入，由优化专家执行单步优化。</p>
<p><img src="https://arxiv.org/html/2602.14577v1/x4.png" alt="方法框架"></p>
<blockquote>
<p><strong>图4</strong>：DriveFine架构总览。视觉和文本输入被联合对齐到一个统一语言空间。一组掩码令牌经过s步并行去噪，然后进行单步优化。生成专家与优化专家的关键区别在于其输入令牌：前者仅解码掩码令牌，而后者对非掩码（已解码）令牌进行操作。</p>
</blockquote>
<p>核心创新模块是<strong>块级混合专家</strong>。该方法将预训练的扩散LLaDA模型的大部分Transformer块（如前28层）作为生成和优化专家共享的公共上下文编码器。仅复制模型的最后n层（如4层）作为独立的“优化专家块”，而原始模型的全部块（包括共享块和最后几层）则整体作为“生成专家”。在推理时，模型根据任务提示显式选择激活相应的专家块。在训练时，生成分支的损失（公式1）仅计算在掩码令牌上，梯度流经所有共享块和生成专家块；而优化分支的损失计算在所有令牌上，但通过梯度阻断技术，梯度仅更新优化专家块，不传播至共享块和生成专家块。这种设计实现了生成与优化能力的完全解耦，在最小化参数量增加和推理开销的同时，保留了预训练模型的基础生成能力，避免了任务干扰和模式崩溃。</p>
<p>另一个核心创新是<strong>混合强化学习策略</strong>。对于生成专家，采用标准的GRPO策略，并行采样一组轨迹，基于组内相对优势（公式2）进行优化（公式3）。对于优化专家，设计了一种结合离线与在线数据的混合优势计算方式。离线优势矩阵（公式5）利用生成专家采样得到的一组轨迹，计算它们之间的奖励差值，提供了密集且零均值的奖励信号。在线优势（公式6）则鼓励优化专家主动探索，对每个生成轨迹采样多个优化版本并计算其奖励提升。最终的混合损失（公式7）结合了离线与在线优势，共同监督优化专家的训练。生成与优化专家在此框架下同步训练，实现协同提升。</p>
<p><img src="https://arxiv.org/html/2602.14577v1/x5.png" alt="强化学习流程"></p>
<blockquote>
<p><strong>图5</strong>：强化微调流程。离线优势和在线优势被联合起来构成混合优势，用于监督优化专家的训练。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在NAVSIM v1、v2和Navhard三个基准上进行。NAVSIM基于nuPlan数据集，提供多相机图像和LiDAR点云，其评估指标包括PDMS和更全面的EPDMS。对比的基线方法涵盖了端到端方法（如UniAD、TransFuser）和多种VLA方法（如AutoVLA、AdaThinkDrive、ReCogDrive、DiffusionDrive等）。</p>
<p>关键实验结果如下：在NAVSIM v1上，DriveFine取得了90.7的PDMS，优于其他VLA方法。当应用额外的基于评分的强化训练后，性能提升至91.8 PDMS。在采样6条轨迹取最佳的策略下，DriveFine达到了94.2 PDMS的SOTA性能。</p>
<p><img src="https://arxiv.org/html/2602.14577v1/x6.png" alt="定性对比"></p>
<blockquote>
<p><strong>图6</strong>：DriveFine与其他SOTA方法的定性可视化对比。最右侧图中，红线和绿线分别表示优化前和优化后的轨迹。可见优化专家能有效纠正导致碰撞或偏离道路的异常令牌，提升轨迹平滑度。</p>
</blockquote>
<p>在NAVSIM v2上，即使在存在分数计算错误的旧版本上评估，DriveFine也达到了87.1 EPDMS。在错误修复后的版本上，其性能进一步提升至89.7 EPDMS，确立了新的SOTA。在更具挑战性的Navhard基准上（零额外训练），DriveFine在阶段1的EPDMS达到74.4，显著超过ReCogDrive的68.9，展示了其强大的泛化能力。</p>
<p>消融实验证明了各核心组件的有效性。仅使用SFT时，PDMS为86.7。加入GRPO训练后提升至89.6。在此基础上，引入优化专家并结合离线强化训练，PDMS提升0.7至90.3。最终，加入在线强化训练后，模型性能达到90.8 PDMS。优化机制几乎改进了所有指标，特别是DAC（可驾驶区域合规性）和Conf（舒适度）。</p>
<p><img src="https://arxiv.org/html/2602.14577v1/x1.png" alt="主流方法解码机制对比"></p>
<blockquote>
<p><strong>图1</strong>：生成式VLA模型中动作令牌解码机制对比。(a) 基于扩散模型：带多步去噪的并行优化。(b) 基于令牌模型：自回归的逐令牌解码。(c) DriveFine：先生成，后优化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.14577v1/x2.png" alt="RFT对两类规划器的影响"></p>
<blockquote>
<p><strong>图2</strong>：面向PDMS的强化微调对两类规划器的影响。基于扩散的规划器在RFT后EPDMS显著下降，而基于令牌的规划器（InternVL）两项指标均提升，显示了后者更强的泛化与扩展能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.14577v1/x3.png" alt="令牌模型不可逆解码失败案例"></p>
<blockquote>
<p><strong>图3</strong>：基于令牌的VLA中不可逆解码导致的失败案例。早期解码的令牌缺乏全局一致性约束，成为异常值且无法修正，导致轨迹级失败（如碰撞、驶离道路）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 深入分析了扩散与令牌两种主流VLA规划器的互补性优劣；2) 提出了DriveFine模型，其创新的块级MoE架构以即插即用的方式，为令牌VLA低成本地注入了优化能力；3) 设计了针对性的混合强化学习策略，进一步释放了模型性能上限。</p>
<p>论文自身提到的局限性在于，所提出的优化专家主要针对轨迹层面的优化，对于更细粒度的控制指令（如油门、刹车）的优化潜力尚未充分探索。</p>
<p>这项工作对后续研究的启示在于：首先，通过模块化设计（如block-MoE）来解耦和增强大模型的特定能力是一个有效且高效的路径。其次，针对不同子任务设计差异化的训练与优化策略（如混合RL），可以实现更精细和稳定的策略提升。最后，它展示了结合不同生成范式优势以构建更鲁棒、更精确的自动驾驶系统的可行性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自动驾驶视觉-语言-动作（VLA）模型中，扩散基规划器存在模态对齐困难、训练效率低，以及令牌基规划器存在累积因果错误、解码不可逆等互补弱点，提出DriveFine模型。该模型采用掩码扩散VLA框架，核心设计块MoE（block-MoE）在生成专家上无缝注入细化专家，通过推理时显式专家选择和训练时梯度阻断实现两专家解耦，以保留预训练权重能力；并辅以混合强化学习策略平衡探索与稳定性。实验在NAVSIM v1、v2和Navhard基准上验证了模型具有强大的效力与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.14577" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>