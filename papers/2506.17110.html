<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.17110" target="_blank" rel="noreferrer">2506.17110</a></span>
        <span>作者: Jingjin Yu Team</span>
        <span>日期: 2025-06-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人6D物体姿态估计主要依赖于基于结构光、飞行时间或立体视觉等技术的深度传感器。这些传感器成本高昂、输出噪声大，且难以处理透明物体。另一方面，最先进的单目深度估计模型通常只能输出仿射不变的深度，即深度值在未知的尺度和偏移下是准确的，缺乏度量信息。一些度量单目深度估计模型在公开数据集上取得了零样本泛化的成功，但在实例级别的度量深度精度不足，无法满足需要物理交互的机器人应用需求。本文针对这一痛点，提出了一种新的视角：无需在新的目标相机/环境设置下重新收集数据或重新训练模型，仅通过一次性的校准，将现成的、泛化能力强的仿射不变深度估计模型的输出，对齐到度量空间。本文的核心思路是：在相机位姿固定的假设下，利用稀疏的、易于获取的真值深度点作为引导，通过一个精心设计的尺度-旋转-平移对齐过程，对单目深度估计模型的输出进行一次性校准，从而恢复出可用于机器人抓取的度量深度。</p>
<h2 id="方法详解">方法详解</h2>
<p>MOMA的整体流程分为离线校准和在线应用两个阶段。对于一个新的相机/环境设置，首先捕获一张RGB图像，利用一个预训练的单目深度估计模型得到原始深度估计。然后，利用从该场景中获取的少量稀疏真值深度点，通过优化求解一组对齐参数。一旦获得这些参数，在后续的在线应用中，对于任何新的RGB图像，只需经过MDEM预测深度，再使用已校准的参数进行对齐变换，即可得到度量深度图用于下游任务（如抓取位姿估计）。</p>
<p><img src="https://arxiv.org/html/2506.17110v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MOMA的整体操作流程。从MDEM获取原始估计深度并归一化得到归一化深度。对于新的相机/环境设置，利用归一化深度和一些真值深度点执行一次性对齐参数估计。随后，该参数可用于校准MDEM输出，为下游应用生成对齐后的度量深度。</p>
</blockquote>
<p>MOMA的核心模块是尺度-平移-旋转对齐方法。现有的对齐方法，如全局尺度-平移对齐和局部加权线性回归，仅考虑尺度和平移，无法处理MDEM预测中可能出现的深度方向反转等严重错误。SSRA方法将MDEM预测的深度图视为一个由伪相机内参生成的3D点云，并将其与由稀疏真值点构成的3D点云进行配准。优化问题旨在找到一个尺度因子s、一个旋转矩阵R和一个平移向量T，最小化两个点云之间的对齐误差。由于机器人任务主要关心深度（Z轴）方向的准确性，SSRA进一步简化了优化目标，仅最小化深度方向的对齐误差。</p>
<p>具体地，对于第k个采样点，其MDEM预测的3D点坐标可通过伪内参和预测深度计算得到。对齐后的深度值由一个映射函数给出，该函数包含了尺度s、绕Y轴和X轴的旋转角θ和φ、深度方向的平移T3以及MDEM的伪内参。因此，待优化的参数向量Θ包含七个变量。通过求解这个非线性最小二乘问题，即可获得最优的对齐参数。</p>
<p><img src="https://arxiv.org/html/2506.17110v1/extracted/6558433/figures/example_fluctuate_l.png" alt="对齐效果对比"></p>
<blockquote>
<p><strong>图3</strong>：一个展示SSRA能力的极端案例。(a) RGB图像。(b) 真值深度。(c) 在TransCG上微调的DAM模型预测的深度，场景对模型来说是未见过的，导致低质量输出。(d) 使用SSRA对齐后的深度，结果优于LWLR和GSSA。(e) 使用GSSA对齐后的深度。(f) 使用LWLR对齐后的深度。</p>
</blockquote>
<p>为了确保一次性校准的鲁棒性，MOMA引入了深度归一化步骤。由于场景中物体的变化会导致MDEM在同一位置预测的原始深度值发生波动，这会使校准失效。通过将原始预测深度减去中值并除以平均绝对偏差进行归一化，可以有效地减少这种场景内容变化带来的影响，使得校准参数在不同物体出现时仍然有效。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台使用了UR-5e机械臂，并测试了两个下游应用：桌面环境下的二指平行夹爪抓取，以及吸盘式箱拣任务。使用的单目深度估计基础模型是DAM，并在透明物体数据集TransCG上对其进行了微调以提升对透明物体的处理能力。</p>
<p>对比的基线方法包括：1) 全局尺度-平移对齐；2) 局部加权线性回归；3) 本文提出的SSRA方法。此外，还进行了消融实验以验证归一化和旋转对齐组件的作用。</p>
<p>关键实验结果如下：在桌面抓取任务中，使用SSRA对齐后的深度进行抓取规划，对10个不同物体（包括透明和反光物体）进行了100次抓取尝试，成功率达到96.7%，显著高于GSSA的78.3%和LWLR的85.0%。在更具挑战性的箱拣任务中，SSRA在120次尝试中取得了91.7%的成功率，而GSSA和LWLR分别为65.8%和80.8%。</p>
<p><img src="https://arxiv.org/html/2506.17110v1/x4.png" alt="深度误差对比"></p>
<blockquote>
<p><strong>图5</strong>：不同对齐方法在测试场景上的深度估计绝对误差热图对比。SSRA（右二）产生的误差明显小于GSSA（左二）和LWLR（右一），尤其是在物体边缘和复杂结构处。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.17110v1/x5.png" alt="抓取成功率"></p>
<blockquote>
<p><strong>图6</strong>：不同方法在桌面抓取（左）和箱拣（右）任务中的成功率对比。SSRA在两个任务中都取得了最高的成功率。</p>
</blockquote>
<p>消融实验表明，深度归一化对于保持校准参数在不同场景间的稳定性至关重要。同时，引入旋转对齐（即SSRA相比仅使用尺度平移）带来了显著的性能提升，特别是在处理MDEM预测出现严重畸变的情况时。</p>
<p><img src="https://arxiv.org/html/2506.17110v1/extracted/6558433/figures/test_objects.jpg" alt="测试物体"></p>
<blockquote>
<p><strong>图7</strong>：实验中使用的部分测试物体，包括透明杯、反光罐、结构复杂的玩具等，涵盖了多种挑战性情况。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了MOMA框架，首次实现了仅需单次校准、无需重新训练即可将通用单目深度估计模型适配到特定机器人工作空间，恢复度量深度；2) 提出了SSRA对齐方法，通过引入旋转对齐和深度归一化，显著提升了深度恢复的精度和鲁棒性，优于现有的全局或局部线性对齐方法；3) 通过将微调后的DAM模型与MOMA结合，成功处理了透明的、反光的等对传统深度传感器不友好的物体，并在真实的机器人抓取和箱拣任务中验证了其高效性。</p>
<p>论文提到的局限性主要在于其假设相机位姿固定，这在固定机械臂场景中成立，但对于移动机器人则需要频繁的重新校准。此外，方法仍然依赖于少量稀疏真值深度点的获取。</p>
<p>本文的启示在于：对于机器人感知任务，利用强大的、数据驱动的视觉基础模型（如MDEM）并辅以轻量级的、基于几何原理的适配模块，是一种低成本、高性能的可行路径。将深度对齐问题建模为3D点云配准问题，并专注于深度方向的优化，是一个新颖且有效的视角。深度归一化对于实现真正的“一次性”校准至关重要，这一思路可推广到其他需要模型适配的任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决机器人抓取中依赖昂贵、易受干扰的深度传感器进行6D姿态估计的问题。提出单次度量深度对齐框架MOMA，基于单目深度估计模型，通过相机标定过程，利用稀疏真实深度点进行一次性的尺度-旋转-平移对齐，从而从单张RGB图像恢复具有物理意义的度量深度。实验在桌面两指抓取和吸盘式箱内拣选任务中验证了该方法的有效性，实现了较高的抓取成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.17110" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>