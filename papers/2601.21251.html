<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.21251" target="_blank" rel="noreferrer">2601.21251</a></span>
        <span>作者: Harold Soh Team</span>
        <span>日期: 2026-01-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>基于扩散的策略最近在机器人操作中展现出强大性能，但其扩展到多任务场景受到模型规模和演示数据成本高昂的阻碍。当前主流方法倾向于扩大策略网络规模，但这导致推理速度对于实时操作不切实际，且所需演示数据集可能随任务多样性指数增长。另一条研究路线是通过基于技能的策略学习进行泛化，但现有方法（如信息论多样性正则化、分层RL）主要设计用于稀疏奖励环境中的探索，而非用于操作的高效技能抽象。最近的混合专家架构已被应用于机器人扩散策略，但现有的MoE公式并未显式解耦和表示可重用的操作技能，限制了其可解释性和可迁移性。</p>
<p>本文针对多任务泛化中模型规模与效率的痛点，提出了一种新视角：不是简单地混合无约束的专家输出，而是学习一组状态自适应的、局部解耦的操作技能（技能基），并通过缓慢变化的（粘性的）门控机制来组合它们。核心思路是提出技能混合专家策略，在局部白化的动作空间中执行技能抽象，学习紧凑、可重用、可迁移的技能，以提高多任务泛化能力和采样效率。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架如论文图2所示。训练时，输入状态s，通过一个轻量级神经网络生成无约束矩阵W(s)，经QR回缩投影得到状态自适应的正交技能基B(s)。动作通过a_t = B(s)(g_t ⊙ z_t)重建，其中g_t是门控权重，z_t是基于扩散模型的系数。训练目标结合了重建损失、扩散损失、门控正则化损失和对齐损失。推理时，使用仅依赖于状态的路由器预测门控均值，通过自适应专家激活机制选择一小组活跃专家，仅对对应的系数进行去噪，然后解码得到动作。</p>
<p><img src="https://arxiv.org/html/2601.21251v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：技能混合专家策略训练框架。左侧 (a)：训练流程，展示了从状态编码到生成正交基B(s)，以及通过门控g和系数z重建动作的过程，并标明了各项损失。右侧 (b)：状态自适应基随时间的调整示意图，基向量随状态变化，而粘性门控保持了专家角色的一致性。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>状态自适应正交技能基</strong>：通过QR分解构建B(s) = qrf(W(s))，并应用符号稳定化以确保基随状态连续演化，保持局部正交性和解耦性。</li>
<li><strong>粘性门控和全局使用</strong>：门控g_t遵循“粘性”狄利克雷马尔可夫动力学，即g_t ~ Dir(κ g_{t-1} + α_0 ϑ)，其中ϑ是全局使用向量。这鼓励门控随时间缓慢变化，形成准平稳的阶段，同时避免崩溃到少数技能。</li>
<li><strong>变分训练目标</strong>：基于证据下界，损失函数包含：<ul>
<li><strong>重建项 ℒ_recon</strong>：在局部白化基中监督动作重建，使用梯度可流的系数目标z^rec。</li>
<li><strong>系数正则项 ℒ_coeff</strong>：在系数空间使用扩散损失，使用停止梯度的系数目标z^sg，确保稳定的专家监督而不更新基B。</li>
<li><strong>门控正则项 ℒ_gate</strong>：将摊销的门控后验拉向粘性门控先验，包括全局使用、初始门控和粘性门控的KL散度。</li>
<li><strong>对齐损失 ℒ_align</strong>：将部署时使用的仅状态路由器p_φ(g_t|s_t)与训练时的门控后验对齐。</li>
</ul>
</li>
<li><strong>自适应专家激活</strong>：在推理时，根据路由器均值ḡ<em>t计算每个专家的质量m_i = (ḡ</em>{t,i})^2。通过选择top-k专家或选择能达到总质量阈值τ_m（如0.9-0.95）的最小专家集，动态激活一个紧凑的专家子集S_t。仅对活跃专家对应的系数z_{t,S_t}进行去噪，其余设为0，从而显著降低计算成本。</li>
</ol>
<p>与现有方法相比，创新点在于：1) 显式地通过状态自适应正交基来学习解耦的技能，确保专家贡献的可加性和非重叠性；2) 引入粘性门控先验，促进时间上一致的技能激活模式，对应高级行为阶段；3) 设计了高效的自适应专家激活机制，在推理时实现稀疏计算。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境（双臂操作基准）和真实世界双臂操作任务上进行评估。对比的基线方法包括：标准扩散策略、仅在前馈网络中应用MoE的FFN-MoE方法，以及另一种扩散MoE基线。</p>
<p><img src="https://arxiv.org/html/2601.21251v1/x3.png" alt="多任务成功率和激活参数"></p>
<blockquote>
<p><strong>图3</strong>：模拟环境中多任务学习的成功率和每次采样激活的参数数量。SMP在保持高成功率（与最佳基线相当或更高）的同时，显著降低了激活参数数量（约降低3-8倍）。</p>
</blockquote>
<p>关键实验结果：在模拟多任务评估中，SMP取得了与强大扩散基线相当或更高的成功率，同时每次采样激活的参数数量显著更少（降低3-8倍），实现了性能与效率的更好权衡。</p>
<p><img src="https://arxiv.org/html/2601.21251v1/x4.png" alt="消融研究"></p>
<blockquote>
<p><strong>图4</strong>：消融研究结果。移除非正交基（w/o orth.）或粘性门控（w/o sticky）都会导致性能下降，验证了这两个组件的必要性。同时，自适应激活（adaptive）在略微影响成功率的情况下大幅提升了效率。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：移除正交基约束或粘性门控都会导致成功率下降（约5-10%），验证了二者对稳定性和性能的重要性。自适应专家激活机制在轻微影响成功率（降低约1-3%）的情况下，大幅减少了激活参数（约60-80%）。</p>
<p><img src="https://arxiv.org/html/2601.21251v1/x5.png" alt="真实机器人任务结果"></p>
<blockquote>
<p><strong>图5</strong>：真实双臂机器人平台上的多任务成功率。SMP在多个任务上取得了最高或接近最高的成功率，同时保持了高效率。</p>
</blockquote>
<p>在真实机器人实验中，SMP在多个任务上取得了最高或接近最高的成功率，并观察到学到的技能在双臂和不同阶段（拾取、调整、移动、释放）具有空间和专业化的特性。</p>
<p><img src="https://arxiv.org/html/2601.21251v1/x6.png" alt="门控切换和技能重用分析"></p>
<blockquote>
<p><strong>图6</strong>：门控切换频率和跨任务技能重用分析。SMP的门控切换显著少于非粘性变体，表明行为更稳定。学到的技能在跨任务间显示出可重用的模式。</p>
</blockquote>
<p>分析表明，SMP的粘性路由导致更少的门控切换和振荡，行为更稳定。学到的技能在跨任务间显示出可重用的模式。</p>
<p><img src="https://arxiv.org/html/2601.21251v1/x7.png" alt="迁移学习性能"></p>
<blockquote>
<p><strong>图7</strong>：少样本迁移学习性能。在预训练的多任务策略基础上，用少量新任务演示进行微调。SMP能够快速适应，在少量演示下达到更高的成功率。</p>
</blockquote>
<p>在少样本迁移学习中，重用SMP的紧凑技能集能够使策略更有效地适应新任务，在少量演示下达到比基线更高的成功率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了SMP，一个基于扩散的混合专家框架，它通过状态依赖的正交动作基和粘性路由显式地抽象可重用的操作技能；2) 设计了一种自适应专家激活策略，在推理时动态选择紧凑的专家子集，在保持动作采样精度的同时降低计算成本；3) 在双臂操作任务（包括多任务和迁移设置）上验证了SMP，证明了其相比强大扩散基线具有更高的成功率和更低的推理成本。</p>
<p>论文自身提到的局限性包括：技能数量K是一个需要预设的超参数。未来工作可以研究自动确定合适的技能数量。</p>
<p>本文对后续研究的启示是：为可扩展、可迁移的多任务操作提供了一条实用路径——学习可重用的技能，仅在需要时激活它们，并在任务变化时快速适应。该方法强调了结构化归纳偏置（正交性、粘性）在提高学习策略的泛化性、效率和可解释性方面的重要性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对扩散策略在多任务机器人操作中模型规模与数据成本过高的问题，提出技能专家混合策略（SMP）。该方法通过学习紧凑正交技能基，利用粘性路由在每一步仅激活少量任务相关专家组合动作，并采用变分训练目标与自适应专家激活实现高效推理。在仿真和真实双臂平台的多任务学习与迁移任务中，SMP相比大型扩散基线取得了更高的成功率，并显著降低了推理成本。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.21251" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>