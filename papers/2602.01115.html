<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01115" target="_blank" rel="noreferrer">2602.01115</a></span>
        <span>作者: Ziyang Wang Team</span>
        <span>日期: 2026-02-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于生成模型的机器人视觉运动策略主要分为扩散模型和流匹配两类。扩散模型通过迭代去噪建模多模态动作分布，性能强大但推理效率低下，需要多步采样和沉重的UNet骨干网络，导致高延迟，难以部署在资源受限的机器人上。流匹配方法通过直接学习一个一步到位的向量场来减轻采样负担，实现了更快的推理速度。然而，现有的流匹配实现仍然继承了大型的UNet风格架构，参数量庞大，限制了其在边缘设备上的部署。</p>
<p>本文针对生成式策略模型（尤其是流匹配方法）中存在的骨干网络参数量大、计算开销高的问题，提出了一种新的视角：利用近期在视觉领域表现出高效建模能力的RWKV（Receptance Weighted Key Value）和KAN（Kolmogorov-Arnold Networks）来构建轻量级且高表达力的策略网络。本文的核心思路是：设计一个结合RWKV序列建模能力和KAN紧凑函数表达能力的RWKV-KAN块，作为流匹配策略的骨干网络，并引入动作一致性正则化来稳定训练，从而在保持高成功率的同时，大幅减少模型参数并实现实时推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>KAN-We-Flow的整体框架是一个基于条件一致性流匹配的单步动作生成策略。其输入包括一个带噪声的动作序列以及由单视角点云、机器人状态和时间步编码拼接而成的条件信息。输出是预测的、用于一步生成目标动作的速度场。</p>
<p><img src="https://arxiv.org/html/2602.01115v1/image/fig2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：KAN-We-Flow方法概览。策略接收带噪声的动作和由点云感知嵌入、机器人状态嵌入及时间嵌入组成的条件。拼接后的表示由一个轻量级的RWKV-KAN U形骨干网络（而非大型UNet）处理；RWKV以线性复杂度混合长程时间/通道上下文，而KAN执行基于可学习样条的特征校准。接着，通过条件一致性流匹配学习一个直线流以产生一步速度场，最终生成的动作具有实时推理速度；额外的动作一致性正则化通过欧拉外推将预测轨迹与示教对齐以稳定训练。</p>
</blockquote>
<p>核心模块是RWKV-KAN UNet，这是一个三阶段的编码器-解码器结构，每个阶段堆叠RWKV-KAN块。该块由RWKV模块和GroupKAN层顺序组成。</p>
<p><strong>RWKV模块</strong>：该模块包含时间混合和通道混合两个核心操作，具有线性复杂度。时间混合操作通过可学习的指数时间衰减权重，以循环方式聚合历史信息，并同时考虑前向和后向扫描以利用完整轨迹上下文。其输出由“接受度”门控并进行线性投影。通道混合分支则应用一个带平方ReLU非线性的门控MLP。最后，采用预归一化残差连接来组合这两个混合操作的输出。</p>
<p><strong>GroupKAN层</strong>：该层基于Kolmogorov-Arnold表示定理，用可学习的1D非线性函数（如B样条）堆栈替代传统的线性层加固定激活函数。在本文的GroupKAN设计中，输入特征被分组，每组特征通过一个独立的KAN层进行处理，这些KAN层共享相同的可学习基函数（样条系数），但具有独立的缩放因子，从而在保持参数效率的同时实现每通道的功能校准。</p>
<p><img src="https://arxiv.org/html/2602.01115v1/image/fig3.png" alt="GroupKAN架构"></p>
<blockquote>
<p><strong>图3</strong>：GroupKAN的架构。展示了输入特征分组后，通过共享基函数但具有独立缩放因子的KAN层进行处理的过程。</p>
</blockquote>
<p><strong>动作一致性正则化</strong>：为了在不增加推理步骤的情况下稳定训练，本文提出了一个轻量级的辅助损失函数ACR。它通过简单的欧拉外推，将策略从中间状态预测的动作轨迹与专家示教在规划时域终点进行对齐，为流匹配目标增加了一个全局的终点锚点，有助于减少漂移和暴露偏差。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>首次将RWKV和KAN架构引入机器人视觉运动策略学习</strong>，用高效的RWKV-KAN块取代了传统的大型CNN或Transformer骨干网络；2) 提出了<strong>动作一致性正则化</strong>，以可忽略的计算成本提高了训练稳定性和策略精度；3) 整体设计实现了<strong>参数量大幅削减和推理速度的显著提升</strong>，同时保持了卓越的性能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个主流的机器人操作基准上进行：Adroit（灵巧手操作）、Meta-World（多任务机械臂操作）和DexArt（灵巧操作任务）。对比的基线方法包括扩散策略的代表DP3，流匹配策略的代表FlowPolicy和MeanFlow，以及基于状态空间模型（Mamba）的MambaPolicy。</p>
<p><img src="https://arxiv.org/html/2602.01115v1/image/fig1.png" alt="性能对比"></p>
<blockquote>
<p><strong>图1</strong>：KAN-We-Flow与SOTA方法FlowPolicy和DP3在准确率、参数量和推理时间上的对比。(a) 在不同基准的困难任务上，KAN-We-Flow取得了更高的成功率；(b) 与FlowPolicy和DP3相比，我们的方法实现了86.8%的参数削减；(c) 在Adroit-Pen任务中，与DP3相比，KAN-We-Flow实现了92.6%的推理时间下降，支持实时控制。</p>
</blockquote>
<p>关键实验结果总结如下：在成功率方面，KAN-We-Flow在三个基准测试的多个任务上均达到了最先进的水平。例如，在Adroit的“Pen”任务上，KAN-We-Flow的成功率为96.7%，显著高于FlowPolicy的92.0%和DP3的90.0%。在参数量方面，KAN-We-Flow仅有约3300万参数，相比FlowPolicy（约2.5亿）和DP3（约2.55亿）减少了86.8%。在推理时间方面，在Adroit-Pen任务上，KAN-We-Flow的单步推理时间约为15毫秒，比DP3（约205毫秒）快了一个数量级，甚至比FlowPolicy（约20毫秒）也有提升，完全满足实时控制需求。</p>
<p><img src="https://arxiv.org/html/2602.01115v1/image/fig4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：在Adroit-Pen任务上的消融研究。从左至右分别移除了RWKV模块、GroupKAN模块和ACR损失。结果表明，每个组件都对最终性能有重要贡献，其中ACR对稳定训练和提升精度作用关键。</p>
</blockquote>
<p>消融实验验证了各个组件的贡献。移除RWKV模块会导致性能显著下降，因为它破坏了长程时间上下文的建模。移除GroupKAN层也会损害性能，表明其紧凑的函数映射对于表达复杂的视觉运动关系至关重要。移除动作一致性正则化（ACR）会导致训练不稳定和最终成功率的降低，证明了其对于提供额外监督、对齐轨迹终点的有效性。</p>
<p><img src="https://arxiv.org/html/2602.01115v1/image/fig6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：在Meta-World任务上的定性结果对比。KAN-We-Flow（右列）能够成功完成“Door Open”和“Window Open”任务，而基线方法FlowPolicy（中列）和DP3（左列）则失败了，突显了本文方法在复杂操作任务上的优越性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01115v1/image/fig7.png" alt="轨迹可视化"></p>
<blockquote>
<p><strong>图6</strong>：在Adroit-Pen任务上的预测动作轨迹与真实示教轨迹的对比可视化。KAN-We-Flow预测的轨迹（红色）与专家示教轨迹（蓝色）高度吻合，展示了其精确的动作生成能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了一种新颖的、基于RWKV和GroupKAN的轻量级流匹配策略KAN-We-Flow，首次将这些高效架构应用于机器人视觉运动策略学习，在保持高成功率的同时实现了参数量的大幅削减和推理速度的显著提升；2) 引入了动作一致性正则化，一种轻量级的辅助损失，通过欧拉外推将预测轨迹锚定在示教终点，有效稳定了训练并提高了策略精度；3) 在多个标准基准上进行了广泛实验，证明了该方法在成功率、效率和实时性方面均优于现有最先进方法。</p>
<p>论文自身提到的局限性在于，RWKV-KAN块的设计和超参数（如分组数、样条网格大小）需要针对不同任务进行调整，其通用最优配置仍有待探索。此外，方法主要关注视觉模仿学习，尚未探索与语言指令等更丰富条件的结合。</p>
<p>本文对后续研究的启示在于：首先，它展示了将视觉领域新兴的高效架构（如RWKV, KAN）迁移到机器人学习领域的巨大潜力，为构建更轻量、更高效的策略网络开辟了新方向。其次，动作一致性正则化提供了一种简单而有效的技术，可以用于改善任何基于轨迹预测的策略的稳定性和精度。最后，这项工作推动了生成式机器人策略向“边缘友好”和“实时可控”的实用化方向迈进了一步。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对基于扩散模型的机器人视觉运动策略推理效率低、计算开销大的问题，提出KAN-We-Flow方法。该方法结合RWKV的时间/通道混合与GroupKAN基于可学习样条的组式函数映射，构建轻量高效的3D流匹配策略主干网络，并引入动作一致性正则化（ACR）辅助损失。实验表明，该方法在多个机器人操作任务上实现了与扩散模型相当的性能，同时显著降低了计算负担，更适合资源受限的机器人部署。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01115" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>