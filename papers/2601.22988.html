<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.22988" target="_blank" rel="noreferrer">2601.22988</a></span>
        <span>作者: Guang Chen Team</span>
        <span>日期: 2026-01-30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域的视觉运动策略主要依赖于2D视觉编码器，这类特征缺乏显式的3D结构感知，在空间复杂的任务中效果有限。近期，利用多视角输入学习3D感知表示的方法（如NeRF、高斯泼溅）展现出潜力，但仍存在关键局限性：推理时依赖多视角观测不实用；场景建模不完整，无法捕捉精细几何结构；缺乏有效的策略训练策略来保留和利用习得的3D知识。本文针对这些痛点，提出了一个统一的表示-策略学习框架GEM3D，旨在从单视角输入学习几何接地的3D视觉表示，并实现强视角泛化的操作。其核心思路是通过多视角监督下的点云重建和前馈高斯泼溅进行单视角3D预训练，学习整体几何表示，再通过多步蒸馏将习得的几何理解迁移到操作策略中。</p>
<h2 id="方法详解">方法详解</h2>
<p>GEM3D框架包含两个关键组件：GEM3D预训练和GEM3D策略。前者学习整体的3D表示，后者将预训练的3D视觉表示蒸馏到视觉运动策略中。</p>
<p><img src="https://arxiv.org/html/2601.22988v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：GEM3D整体框架概览。左侧为GEM3D预训练，通过辅助场景重建任务学习整体3D表示；右侧为GEM3D策略，将预训练的3D视觉表示蒸馏到视觉运动策略中，用于视角泛化的操作。</p>
</blockquote>
<p><strong>GEM3D预训练</strong> 采用三阶段流水线，从单视角RGB-D观测学习密集体素特征、整体几何和精细纹理。</p>
<p><img src="https://arxiv.org/html/2601.22988v1/x3.png" alt="预训练流水线"></p>
<blockquote>
<p><strong>图3</strong>：GEM3D预训练流水线。(a) 将单视角RGB-D观测编码为体素特征；(b) 以从粗到精的Snowflake方式逐步重建场景几何；(c) 通过基于高斯泼溅的新视角渲染学习精细纹理细节。</p>
</blockquote>
<ol>
<li><p><strong>3D特征提取</strong>：输入为单视角RGB-D观测 $o^t = { \mathcal{I}^t, \mathcal{D}^t, \mathcal{K}^t }$。深度图 $\mathcal{D}^t$ 通过相机参数 $\mathcal{K}^t$ 反投影、裁剪和最远点采样得到点云 $\mathcal{P}$。RGB图像 $\mathcal{I}^t$ 通过预训练的DINOv2模型编码为像素级2D特征 $\mathcal{F}_{\text{2D}}$，并投影到 $\mathcal{P}$ 上以增强视觉语义。最后，语义增强的点云被体素化为占据和特征体，并通过3D U-Net融合，生成密集体素特征 $\mathcal{F} \in \mathbb{R}^{D^3 \times 128}$。</p>
</li>
<li><p><strong>从粗到精的点云重建</strong>：为了实现对体素特征 $\mathcal{F}$ 的整体几何理解，本模块首先生成稀疏种子点，然后逐步细化为密集点云。</p>
<ul>
<li><strong>种子点生成</strong>：初始化一个可学习的体素查询集 $\mathcal{Q} \in \mathbb{R}^{d^3 \times 128}$（$d \ll D$）。首先通过平均池化下采样 $\mathcal{F}$ 得到 $\mathcal{F}_{\text{down}}$，执行粗粒度交叉注意力得到捕获粗略空间关系的提议查询 $\mathcal{Q}_p$。然后通过一个3D可变形交叉注意力模块，使 $\mathcal{Q}_p$ 从 $\mathcal{F}$ 中高效聚合局部几何细节。最终，种子令牌通过一个浅层MLP解码为种子点坐标 $\hat{\mathcal{P}}_0 \in \mathbb{R}^{d^3 \times 3}$。</li>
<li><strong>点云恢复</strong>：遵循SnowflakeNet，使用雪花点反卷积块从种子点 $\hat{\mathcal{P}}_0$ 开始逐步细化点集。在每一阶段 $i$，父点 $\hat{\mathcal{P}}_i$ 通过三线性插值查询 $\mathcal{F}$ 得到点特征 $\mathcal{F}_i$，SPD块以 $\hat{\mathcal{P}}_i$、$\mathcal{F}<em>i$ 和当前上下文特征 $\mathcal{F}^c_i$ 为输入，对每个父点上采样因子 $r$，产生 $r$ 个位移向量 $\Delta\mathcal{P}<em>i$ 并更新上下文特征。父点复制 $r$ 份并根据预测偏移位移，形成细化后的子点 $\hat{\mathcal{P}}</em>{i+1}$。使用融合多视角真实点云的完整点集 $\mathcal{P}</em>{\text{full}}$ 进行监督，在每一阶段使用最远点采样获取子集 $\mathcal{P}<em>i$，并使用倒角距离 $L_2$ 作为重建损失 $\mathcal{L}</em>{\text{rec}}$（公式1）。</li>
</ul>
</li>
<li><p><strong>前馈3D高斯泼溅</strong>：基于重建的密集点云 $\hat{\mathcal{P}}$，通过前馈高斯泼溅流水线进行新视角渲染，在多视角监督下捕获精细纹理。高斯中心 $\mu$ 直接由细化点 $\hat{\mathcal{P}}$ 给出。在这些位置，通过三线性插值从密集体素场 $\mathcal{F}$ 中采样高斯特征 $\mathcal{F}<em>{\mathcal{G}}$，然后输入一个ResNetFC网络回归其余的非位置高斯参数（颜色、不透明度、旋转、尺度）。得到的3D高斯参数 $\mathcal{G} = [\mu, c, \sigma, r, s]$ 用于可微分渲染器合成新视角图像 $\hat{\mathcal{I}}^t_v$，并使用焦点损失 $\mathcal{L}</em>{\text{rend}}$（公式3）与对应的多视角RGB观测 $\mathcal{I}^t_v$ 进行监督。</p>
</li>
</ol>
<p><strong>GEM3D策略学习</strong> 采用基于多步蒸馏的策略，而非直接微调预训练编码器，以避免破坏已学习的几何表示。</p>
<p><img src="https://arxiv.org/html/2601.22988v1/x4.png" alt="策略学习框架"></p>
<blockquote>
<p><strong>图4</strong>：GEM3D策略。一个基于多步蒸馏的策略学习框架。策略编码器处理观测得到潜在令牌 $\mathbf{x}^t$，冻结的预训练3D特征提取器生成对应的密集体素特征 $\mathcal{F}^t$ 并分块化为参考令牌 $\tilde{\mathbf{x}}^t$，通过基于余弦相似性的蒸馏损失对齐两者。</p>
</blockquote>
<p>具体而言，初始化一个独立的策略编码器处理单视角RGB-D观测 $o^t$ 为潜在令牌 $\mathbf{x}^t$。同时，冻结的预训练3D特征提取器生成对应的密集体素特征 $\mathcal{F}^t$，随后分块化为参考令牌 $\tilde{\mathbf{x}}^t$。应用基于余弦相似性的蒸馏损失来对齐潜在令牌和参考令牌。为了赋予策略动态理解，引入了一个隐式潜在动力学模型，从当前潜在状态 $\mathbf{x}^t$ 以及本体感知和语言嵌入中预测 $\mathbf{x}^{t+1}$。动作 $a_t$ 然后通过一个动作头从 $\mathbf{x}^{t+1}$ 解码出来。这种设计使得可以进行多步潜在蒸馏以正则化策略的时间一致性，蒸馏损失 $\mathcal{L}<em>{\text{distill}} = \mathcal{L}</em>{\text{cos}}(\mathbf{x}^{t}, \tilde{\mathbf{x}}^{t}) + \mathcal{L}<em>{\text{cos}}(\mathbf{x}^{t+1}, \tilde{\mathbf{x}}^{t+1})$（公式4）。策略通过模仿学习进行训练，总体目标为 $\mathcal{L}</em>{\text{policy}} = |\mathbf{a}_t - \mathbf{a}<em>t^*|<em>2^2 + \lambda</em>{\text{distill}} \mathcal{L}</em>{\text{distill}}$（公式5）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在RLBench的12个任务（覆盖9个场景）上进行评估。预训练使用8个均匀分布环绕摄像头采集的多视角观测，每个场景50条轨迹。策略学习每个任务使用20个从前置固定摄像头收集的专家演示。评估指标为任务成功率（SR）。为评估视角泛化，在三种摄像头视角偏移下进行零样本评估：无偏移（训练视角）、中等偏移（机器人基座周围30°内）、大偏移（60°至90°间的显著变化）。</p>
<p><strong>对比方法</strong>：使用PerceiverIO作为策略主干进行公平比较，基线包括：PerAct（直接使用基于体素的3D表示）、GNFactor（结合NeRF作为辅助表示学习目标）、ManiGaussian（利用3D高斯泼溅进行表示学习并与策略联合训练）。此外，将ManiGaussian表示模块与本文的蒸馏训练策略结合，记为ManiGaussian (w Distill)。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li>在12个RLBench任务上，GEM3D取得了最佳整体性能，平均成功率为44.2%，比之前的SOTA方法ManiGaussian绝对提升了12.7%（表1）。</li>
<li>在六个代表性任务的零样本视角泛化评估中（表2），GEM3D在中等和大视角偏移下的成功率仅分别下降22.03%和29.67%，显著低于ManiGaussian的41.62%和51.52%的下降。在大视角偏移下，GEM3D的成功率最高可达PerAct的6倍。</li>
<li>多任务评估（表3）显示，GEM3D在训练视角和偏移视角下性能下降幅度很小（最大下降6.9%），而PerAct基线则出现显著下降甚至策略崩溃，表明本文的几何接地表示有助于更清晰的任务区分和更可靠的指令遵循。</li>
</ul>
<p><strong>定性分析与重建质量</strong>：<br><img src="https://arxiv.org/html/2601.22988v1/x1.png" alt="定性结果"></p>
<blockquote>
<p><strong>图1</strong>：GEM3D学习精细的3D表示，能够实现更准确的新视角渲染和点云重建，并在推理过程中对视角偏移表现出强大的鲁棒性，保持稳定的视觉运动性能。</p>
</blockquote>
<p>如图1所示，GEM3D重建了更清晰的结构并产生更一致的新视角，而ManiGaussian则存在纹理模糊和几何畸变。定量指标（表4）进一步证实，GEM3D预训练在PSNR、SSIM和倒角距离L2上均显著优于ManiGaussian的预训练方法，平均分别提升+7.08 dB、+0.4298和降低0.0289。</p>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2601.22988v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：GEM3D预训练模块的消融研究。移除可变形交叉注意力（DCA）会导致精细空间线索丢失；移除雪花式（Snowflake）重建则无法逐步细化几何细节。</p>
</blockquote>
<p>消融研究验证了GEM3D预训练中三个关键设计的必要性（图6）：可变形交叉注意力确保高效精确的种子生成；雪花式从粗到精重建实现几何细节的逐步细化；焦点损失增强了对运动引起的表观和几何变化的鲁棒性。移除任一组件都会导致重建质量下降。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了GEM3D，一个统一的表示-策略学习框架，通过单视角3D预训练学习整体且精细的几何接地表示；2）设计了一种基于多步蒸馏的策略学习策略，能够有效保留预训练获得的几何知识并迁移到操作技能中，增强了视角泛化能力；3）在RLBench上实现了SOTA性能，并展示了卓越的零样本视角泛化鲁棒性。</p>
<p>论文自身提到的局限性包括：预训练需要多视角数据，这可能在数据收集受限的场景中构成挑战；方法目前主要在模拟环境中进行评估。</p>
<p>对后续研究的启示：本文证明了通过精心设计的预训练任务（结合几何重建与纹理渲染）学习到的3D表示，对于提升视觉运动策略的空间理解与泛化能力至关重要。所提出的蒸馏策略提供了一种有效整合预训练表示与策略学习的新范式，可扩展至其他骨干架构。未来工作可探索更高效的自监督预训练方式，以及将框架迁移到真实机器人复杂环境中的可行性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中单视图3D几何理解不足、视角泛化能力弱的核心问题，提出了GEM3D框架。其关键技术在于采用单视图3D预训练范式，通过点云重建和前馈高斯溅射学习整体几何表示，并在策略学习阶段通过多步蒸馏保留几何知识。实验表明，该方法在12个RLBench任务上的平均成功率超越先前最优方法12.7%，且在视角大幅变化时，成功率下降幅度显著更小，展现了卓越的零样本视角泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.22988" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>