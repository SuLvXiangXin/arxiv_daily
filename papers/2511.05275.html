<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.05275" target="_blank" rel="noreferrer">2511.05275</a></span>
        <span>作者: Im, Hokyun, Jeong, Euijin, Fu, Jianlong, Kolobov, Andrey, Lee, Youngwoon</span>
        <span>日期: 2025/11/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于大规模机器人数据集训练的视觉-语言-动作模型在单臂操作任务上展现出强大性能。然而，将其扩展到需要双手协调的双臂操作任务时面临核心挑战：公开的双臂演示数据集稀缺，而现有方法（如RDT-1B、π₀）通常依赖于收集和训练大规模、计算成本高昂的私有双臂数据，这限制了方法的可复现性和进一步发展。</p>
<p>本文针对在双臂数据稀缺条件下构建高性能双臂策略这一具体痛点，提出了一个模块化、以协调为中心的新视角。其核心思路是：复制一个在丰富公开单臂数据上预训练好的单臂VLA模型，通过轻量级的协调机制（联合注意力）将两个模型实例组合成一个协调的双臂VLA，从而避免了对大规模双臂预训练数据的需求。</p>
<h2 id="方法详解">方法详解</h2>
<p>TwinVLA的整体框架旨在将一个预训练的单臂VLA模型转化为一个协调的双臂策略。其Pipeline包含三个核心原则：1）选择性复制预训练单臂模型的模块以构建双臂策略；2）引入联合注意力机制实现跨臂协调；3）集成混合专家层以提高共享观察的处理效率。</p>
<p><img src="https://arxiv.org/html/2511.05275v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：TwinVLA概述。受人类双臂协调的启发，TwinVLA复制了一个在跨具身单臂数据上预训练的VLM主干（左），形成两个臂特定的分支，并通过联合注意力（右）连接。共享输入（自我中心视图、语言指令）通过混合专家路由以提高计算效率。仅VLM主干被复制，使模型大小的增加最小化。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>单臂策略复制</strong>：从预训练的SingleVLA出发，通过复制其VLM主干来初始化左右臂的策略，同时<strong>共享</strong>视觉编码器和扩散Transformer动作头。每个臂拥有自己轻量级的本体感觉编码器。这种设计基于一个原则：通用的视觉理解（图像编码）和低层运动控制（动作解码）是具身无关的技能，可以被双臂有效共享；而负责根据编码观察决定输出动作的VLM则需要被完全复制以允许专门化控制。最终得到一个约13亿参数的紧凑模型。</li>
<li><strong>用于跨臂融合的联合注意力</strong>：采用受Mixture of Transformers启发的联合注意力机制，使两个VLM主干能够交换信息。具体实现是仅<strong>共享VLM中的自注意力层</strong>，而其他组件（如前馈网络、投影层）则各自独立。为了在共享和臂特定输入之间实现有效的联合注意力，论文设计了如图3(a)所示的因果注意力掩码。该掩码在每个臂的区域内保持标准的下三角因果掩码，同时允许每个臂关注另一臂的部分令牌，从而实现对称的跨臂交互而不违反自回归约束。</li>
<li><strong>混合专家集成</strong>：为了高效处理共享模态（语言指令、自我中心图像），避免将其重复输入给两个VLM导致序列长度和显存消耗增加，TwinVLA引入了混合专家机制。MoE层将共享模态的令牌路由到两个VLM的专家网络进行处理。对于其他未共享的组件（如投影层），则采用任务算术的思想：将输入分别通过两个VLM的对应组件，然后对输出进行平均，从而在功能上模拟了一个共享层。此外，论文还引入了<strong>注意力重加权</strong>技术，通过重新缩放共享模态的注意力分数，来缓解新增臂特定令牌对预训练注意力模式的干扰，有助于在微调初期保持预训练知识。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.05275v1/x3.png" alt="联合注意力细节"></p>
<blockquote>
<p><strong>图3</strong>：(a) 联合注意力的因果注意力掩码设计，它支持共享、左臂、右臂输入的并行处理。(b) TwinVLA的联合注意力机制。两个VLM共享信息，共享模态通过MoE处理以更高效地利用两个VLM。</p>
</blockquote>
<p><strong>创新点</strong>：与需要混合单/双臂数据进行单体训练的现有跨具身模型相比，TwinVLA的创新性在于其模块化、组合式的设计。它并非从头训练一个庞大的双臂模型，而是复用并协调两个成熟的单臂策略，从而在架构层面引入了双臂协调的归纳偏置，显著提升了数据效率和性能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在真实世界和模拟环境中进行评估。真实世界使用Anubis双臂机器人，执行3个长视界任务。模拟环境包括RoboTwin 2.0基准的50个任务，以及自行构建的Tabletop-Sim环境的5个代表性任务。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>RDT-1B</strong>：可比大小的单体模型基线，需要大量双臂数据和计算。</li>
<li><strong>π₀</strong>：作为性能上限的先进模型，依赖大规模私有数据。</li>
<li><strong>Diffusion Policy</strong>：在低数据区域表现强劲的基线，用于凸显预训练的重要性。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>真实世界实验</strong>：在三个真实双臂任务上，TwinVLA的平均成功率显著优于RDT-1B和Diffusion Policy，并取得了与π₀相当的性能，尽管其预训练使用的单臂数据量（0.5M）和计算成本（<del>25 H100 GPU-days）远低于RDT-1B（1.4M数据，</del>1440 GPU-days）。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.05275v1/x5.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界任务的成功率。TwinVLA在平均成功率上超越了RDT-1B和DP，并与π₀表现相当。</p>
</blockquote>
<ol start="2">
<li><strong>模拟实验</strong>：在RoboTwin 2.0和Tabletop-Sim的广泛任务上，TwinVLA在大多数情况下（尤其是Easy设置和Tabletop-Sim）的表现优于RDT-1B，并接近甚至在某些任务上超过π₀的性能。这证明了TwinVLA在需要较高灵巧性和显著双臂协调的场景中的优势。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.05275v1/x6.png" alt="模拟任务结果"></p>
<blockquote>
<p><strong>图6</strong>：多样化双臂任务的平均成功率。尽管仅在单臂数据集上预训练，TwinVLA的表现优于除π₀外的其他方法。</p>
</blockquote>
<ol start="3">
<li><strong>数据效率</strong>：在Tabletop-Sim Easy任务上，仅使用20、35、50条演示数据进行微调，TwinVLA展现出陡峭的学习曲线，用50条数据即快速超越RDT-1B，突显了其卓越的数据效率。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.05275v1/x7.png" alt="数据效率"></p>
<blockquote>
<p><strong>图7</strong>：Tabletop-Sim Easy任务上，使用不同数量演示数据微调后的平均成功率。TwinVLA展现出快速学习能力。</p>
</blockquote>
<ol start="4">
<li><strong>语言遵循与鲁棒性</strong>：在多任务语言遵循测试中，TwinVLA的成功率超过了RDT-1B和π₀。在涉及未见纹理和物体的“Hard”设置测试中，TwinVLA也展现出足够的鲁棒性，其性能优于未预训练的Diffusion Policy。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.05275v1/x8.png" alt="语言任务与消融"></p>
<blockquote>
<p><strong>图8</strong>：(a) Tabletop-Sim中语言遵循任务的平均成功率，TwinVLA表现最佳。(b) 真实世界和Tabletop-Sim Easy任务上的消融研究结果。</p>
</blockquote>
<ol start="5">
<li><strong>消融实验</strong>：逐步移除核心组件的消融实验表明：<ul>
<li><strong>联合注意力</strong>的贡献最大，移除后导致模拟和真实世界性能分别下降4.0%和8.3%，证实了跨臂协调机制的关键性。</li>
<li><strong>MoE集成</strong>的移除不仅增加了序列长度和显存消耗，还导致性能进一步下降。</li>
<li><strong>注意力重加权</strong>有助于缓解微调初期的分布偏移，提升最终性能。</li>
<li><strong>单臂预训练</strong>至关重要，没有它（从头训练）会导致性能大幅下降。</li>
<li><strong>双结构本身</strong>相比单体模型（RDT-1B）具有优势，TwinVLA在多项评估中均显著超越后者。</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的模块化双臂操作架构TwinVLA，它通过联合注意力与MoE机制，将两个预训练的单臂VLA集成为一个协调的双臂策略。</li>
<li>确立了一种数据高效的学习范式：仅需在少量目标双臂数据上微调，即可获得高性能双臂策略，完全避免了大规模双臂预训练数据的需求。</li>
<li>通过广泛的实验验证，TwinVLA在数据和计算效率显著更高的前提下，取得了与依赖海量私有数据的先进模型相媲美甚至更优的性能。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>当前模型在针对双臂任务微调后，会遗忘其原有的单臂操作技能。</li>
<li>动作空间的选择对知识迁移至关重要，本文采用的绝对末端执行器位姿控制虽具身无关，但可能并非最优解。</li>
</ul>
<p><strong>启示</strong>：</p>
<ul>
<li>为在数据稀缺领域构建复杂技能（如双臂操作）提供了一条可扩展的路径：通过组合成熟的单技能模块并辅以轻量级协调。</li>
<li>未来的研究可以探索防止技能遗忘的机制，以及开发更具通用性的跨具身动作表示，以进一步提升模型的通用性和可迁移性。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出TwinVLA，旨在解决双臂操作任务因公开数据稀缺而依赖大量专有数据的问题。其核心方法是采用模块化框架，复制并组合两个预训练的单臂视觉-语言-动作模型，通过联合注意力机制进行双臂协调。实验表明，该方法无需任何双臂预训练数据，性能即优于同等规模的RDT-1B模型，有效缩小了与依赖海量专有数据的先进模型的差距，实现了数据高效的双臂操作。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.05275" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>