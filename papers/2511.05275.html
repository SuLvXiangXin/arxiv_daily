<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.05275" target="_blank" rel="noreferrer">2511.05275</a></span>
        <span>作者: Im, Hokyun, Jeong, Euijin, Fu, Jianlong, Kolobov, Andrey, Lee, Youngwoon</span>
        <span>日期: 2025/11/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，在大规模机器人数据集上训练的视觉-语言-动作模型（VLA）在单臂操作任务上表现出色。然而，将其扩展到需要双手协调的双臂操作任务时面临挑战。主流方法通常采用单一、整体的跨本体模型，在混合了单臂和双臂数据的大规模数据集上进行训练，例如RDT-1B和π0模型。这些方法存在关键局限性：它们严重依赖大量、通常是专有的双臂数据收集和计算密集的预训练，这限制了可重复性和研究进展。双臂高质量演示数据稀缺，导致从零开始训练的专家方法在复杂、长视野的双臂任务上表现不佳。</p>
<p>本文针对双臂数据稀缺这一具体痛点，提出了一种模块化组合的新视角。受人类双臂协调神经机制的启发，本文认为双臂操作本质上是两个手臂特定运动基元的协调，而非单一的整体控制。因此，核心思路是：<strong>通过复制和协调两个预训练的单臂VLA模型，构建一个高性能的双臂策略，从而避免对大规模双臂预训练数据的依赖</strong>。</p>
<h2 id="方法详解">方法详解</h2>
<p>TwinVLA的核心目标是将一个预训练的单臂VLA（称为SingleVLA）扩展为一个协调的双臂策略。其整体框架是一个模块化架构，通过三个核心原则实现：选择性复制预训练模块、引入联合注意力实现跨臂协调、集成混合专家层以提高计算效率。</p>
<p><img src="https://arxiv.org/html/2511.05275v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：TwinVLA概览。受人类双臂协调启发，TwinVLA将在跨本体单臂数据上预训练的VLA主干（左）复制两份，形成两个臂特定的分支，并通过联合注意力（右）连接。共享输入通过混合专家路由以提高计算效率。</p>
</blockquote>
<p><strong>整体流程</strong>：首先，在公开的大规模单臂数据集（如OXE）上预训练一个轻量级的单臂VLA模型（SingleVLA）。然后，复制该模型的两个实例，并通过轻量级协调机制（联合注意力）将它们集成，形成一个双臂策略。最后，仅需使用少量目标双臂任务的演示数据进行微调，即可适应新任务。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>单臂策略复制</strong>：从SingleVLA构建TwinVLA时，并非完全复制整个模型。<strong>视觉编码器和DiT动作头被共享</strong>，因为它们分别代表的通用视觉理解和低级运动控制技能是跨本体（即与具体手臂无关）的。<strong>VLM主干被完全复制</strong>，以允许每个臂进行专门的决策控制。每个臂还有自己轻量的本体感觉编码器。此设计产生了一个约13亿参数的紧凑模型。</li>
<li><strong>联合注意力实现跨臂融合</strong>：这是实现双臂协调的关键。该方法受混合Transformer启发，通过<strong>仅共享两个VLM主干之间的自注意力层</strong>来实现信息交换，而其他组件（如前馈网络）独立运行。这直接连接了两个VLM，而非像π0那样将VLM连接到动作头。<ul>
<li><strong>因果联合注意力掩码</strong>：为确保有效协调且不违反自回归预测的因果性，设计了特殊的注意力掩码。如图3(a)所示，每个臂在其自身区域内使用标准的下三角掩码，同时能完全访问共享模态的标记，并且每个臂还能关注另一半臂的部分标记，实现对称的跨臂交互。<br><img src="https://arxiv.org/html/2511.05275v1/x3.png" alt="联合注意力机制"><blockquote>
<p><strong>图3</strong>：(a) 用于联合注意力的因果注意力掩码设计。(b) TwinVLA的联合注意力机制。两个VLM共享信息，共享模态通过MoE更高效地利用两个VLM。</p>
</blockquote>
</li>
</ul>
</li>
<li><strong>混合专家集成</strong>：为解决将共享模态（语言指令、自我中心图像）重复输入两个VLM导致的序列长度和显存使用增加问题，引入了混合专家机制。MoE在双VLM主干之间路由共享模态的标记，有效共享了前馈网络，减少了冗余处理。同时，通过输出平均技术模拟了其他关键组件的参数共享，使得模型能在单张40GB GPU上以批次大小8进行训练。</li>
<li><strong>注意力重加权</strong>：在微调阶段，为防止新增的臂特定标记干扰模型在预训练阶段对共享模态（如图像）已习得的注意力模式，对共享模态的注意力分数进行了重新缩放。这有助于保留有价值的预训练知识，使模型能更快地专注于目标任务。</li>
</ol>
<p><strong>训练与推理</strong>：模型使用条件流匹配目标进行端到端训练。动作头被训练为预测从带噪声的动作块到目标动作块的参考流。推理时，使用前向欧拉积分方法从噪声中采样动作。</p>
<p><strong>创新点</strong>：与现有整体式跨本体模型相比，TwinVLA的创新点主要体现在其<strong>模块化、协调中心的设计</strong>。它<strong>避免了双臂预训练</strong>，通过组合利用丰富的公开单臂数据预训练好的策略，并引入轻量的联合注意力机制实现协调，从而在数据和计算效率上实现了显著提升。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：TwinVLA在真实世界和模拟环境中进行了广泛评估。</p>
<ul>
<li><strong>真实世界</strong>：使用双臂机器人Anubis，评估了三个长视野桌面操作任务（胡萝卜入袋、扫入簸箕、取下毛巾）。</li>
<li><strong>模拟环境</strong>：<ol>
<li><strong>RoboTwin 2.0基准</strong>：包含50个双臂任务，在“简单”（场景匹配训练）和“困难”（纹理、物体位置、高度变化）设置下评估。</li>
<li><strong>Tabletop-Sim环境</strong>：设计了5个需要精确协调的代表性任务（如沥水架、交接盒子等），同样有“简单”和“困难”设置。</li>
</ol>
</li>
<li><strong>对比基线</strong>：<ul>
<li><strong>RDT-1B</strong>：可比较大小的整体式模型（12亿参数），需大规模双臂数据预训练和微调。</li>
<li><strong>π0</strong>：33亿参数的先进VLA模型，使用超万小时专有数据训练，作为性能上限参考。</li>
<li><strong>扩散策略</strong>：在低数据状态下表现良好的基线，用于凸显预训练的重要性。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2511.05275v1/x5.png" alt="真实世界任务成功率"></p>
<blockquote>
<p><strong>图5</strong>：真实世界任务上的成功率。TwinVLA平均表现优于RDT-1B和扩散策略，且与π0性能相当，而π0使用了大量专有双臂数据。</p>
</blockquote>
<p>在真实世界任务中，TwinVLA显著优于其直接竞争对手RDT-1B，尽管TwinVLA预训练使用的单臂数据量（约0.5M）和计算成本远低于RDT-1B（1.4M轨迹，~1440 H100 GPU天 vs. ~25 H100 GPU天）。π0展现了最佳性能，但TwinVLA在仅使用目标数据微调的情况下达到了与其相当的水平。</p>
<p><img src="https://arxiv.org/html/2511.05275v1/x6.png" alt="模拟任务平均成功率"></p>
<blockquote>
<p><strong>图6</strong>：多样双臂任务的平均成功率。尽管仅在单臂数据集上预训练，TwinVLA在大多数场景下优于其他方法（π0除外），在Tabletop-Sim简单任务中甚至超越了π0。</p>
</blockquote>
<p>在模拟实验中，TwinVLA在大多数场景下超越了RDT-1B，并在Tabletop-Sim简单任务中取得了与π0相当甚至更好的性能，表明其在需要更高灵巧性和协调性的场景中具有优势。</p>
<p><img src="https://arxiv.org/html/2511.05275v1/x7.png" alt="数据效率曲线"></p>
<blockquote>
<p><strong>图7</strong>：Tabletop-Sim简单任务上，使用不同数量演示数据微调后的平均成功率。TwinVLA展现出陡峭的学习曲线，仅用50条演示数据即超越RDT-1B，凸显其数据效率。</p>
</blockquote>
<p><strong>数据效率</strong>：如图7所示，TwinVLA在微调阶段学习迅速，仅用50条演示数据其性能便超过使用更多数据微调的RDT-1B，证明了其卓越的数据效率。</p>
<p><strong>语言遵循与鲁棒性</strong>：<br><img src="https://arxiv.org/html/2511.05275v1/x8.png" alt="语言任务与消融结果"></p>
<blockquote>
<p><strong>图8</strong>：(a) 语言遵循任务结果。在包含6种指令组合的多任务中，TwinVLA优于RDT-1B和π0。(b) 在真实世界和Tabletop-Sim简单任务上的消融研究结果。</p>
</blockquote>
<p>在语言遵循评估中（图8a），TwinVLA在包含多种颜色组合指令的任务上表现最佳，表明其通过精细微调有效保留了预训练中获得的知识。在应对未见过的场景和指令的鲁棒性方面，TwinVLA在Tabletop-Sim困难设置下比RDT-1B高3.3%，在RoboTwin简单设置下高7.48%。</p>
<p><strong>消融实验</strong>：图8(b)展示了顺序消融研究结果。</p>
<ul>
<li><strong>移除注意力重加权</strong>：使初始微调损失增加40%，最终性能下降约1%。</li>
<li><strong>进一步移除MoE集成</strong>：增加了序列长度和显存使用，性能进一步下降约1%。</li>
<li><strong>进一步移除联合注意力</strong>：导致性能出现最显著的额外下降（模拟4.0%，真实世界8.3%），证实了联合注意力是实现双臂协调的关键机制。</li>
<li><strong>无单臂预训练（从零开始）</strong>：导致性能大幅下降（模拟4.6%，真实世界32.9%），验证了预训练的重要性。</li>
<li><strong>与整体式模型对比</strong>：TwinVLA在真实世界、模拟和语言任务上分别比同规模的整体式模型RDT-1B高出16.2%、5.0%和25.1%，证明了其双胞胎结构本身带来的归纳偏置对双臂操作是有益的。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的模块化双臂操作架构TwinVLA，它通过基于联合注意力和MoE的轻量级协调方法，将两个预训练的单臂VLA副本集成，实现了同步的双臂控制。</li>
<li>提出了一种数据高效的范式，仅需对少量目标双臂数据集进行微调，即可将双胞胎架构适配为强大的双臂策略，而无需额外的双臂预训练，从而消除了对大规模双臂数据的需求。</li>
<li>通过大量实验证明，TwinVLA在真实和模拟的双臂任务上，匹配或超越了使用远多于其双臂数据和计算成本训练的先进模型性能。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>技能遗忘</strong>：模型在针对双臂任务微调后，会遗忘其原有的单臂操作技能。未来研究防止遗忘的机制可以整合更多样化的数据，并可能提升模型的可解释性和对未见任务的泛化能力。</li>
<li><strong>动作空间选择</strong>：当前采用绝对末端执行器位姿控制以实现跨本体策略迁移。探索相对绝对动作或开发跨多样本体的共享表示，可能实现更高效的迁移。</li>
</ol>
<p><strong>启示</strong>：<br>TwinVLA的工作为利用丰富的单臂数据解决双臂操作的数据稀缺问题开辟了一条高效、可扩展的路径。其模块化、协调中心的视角挑战了单一整体模型的主导范式，表明通过组合和协调已有的、专门化的策略模块，可以更高效地构建复杂的机器人技能。这对未来研究如何将不同来源、不同本体的技能模块组合成更通用的机器人策略具有启发意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出TwinVLA，旨在解决双手操作任务缺乏大规模公开数据、依赖昂贵专有数据集的问题。方法上，复制两个预训练的单臂视觉-语言-动作模型，通过联合注意力机制协调双臂，并采用混合专家路由提升计算效率。实验表明，该框架在真实与仿真环境中，无需任何双手预训练数据，即超越同等规模单体模型，并显著缩小了与需海量专有数据的顶尖模型π0的性能差距。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.05275" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>