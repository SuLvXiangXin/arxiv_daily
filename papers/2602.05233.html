<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MobileManiBench: Simplifying Model Verification for Mobile Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MobileManiBench: Simplifying Model Verification for Mobile Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.05233" target="_blank" rel="noreferrer">2602.05233</a></span>
        <span>作者: Baining Guo Team</span>
        <span>日期: 2026-02-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型在机器人操作领域取得了显著进展，但其成功严重依赖于大规模数据集，尤其是以Open X-Embodiment为代表、通过遥操作收集的数据集。这些数据集主要局限于静态、桌面场景，使用有限的家庭物品，且数据收集视角多为固定基座或头戴式。这带来了两个关键局限性：首先，任何硬件配置的修改（如增加新传感器、扩展到移动操作或更换灵巧手）都需要从头开始重新收集数据，成本高昂且效率低下；其次，遥操作流程在扩展到灵巧手或移动平台时变得笨拙，阻碍了模型的快速开发和迭代。本文针对在真实世界部署前验证VLA架构成本高、风险大的痛点，提出了“仿真优先”的新视角，旨在通过仿真环境高效、可控地生成大规模、多模态数据，以支持对模型架构、传感器配置和机器人形态的快速验证。本文的核心思路是：利用NVIDIA Isaac Sim仿真平台和强化学习，自动化生成一个包含丰富标注的大规模移动操作基准数据集MobileManiBench，并基于此数据集训练和评估通用VLA模型，以加速模型创新和数据效率研究。</p>
<h2 id="方法详解">方法详解</h2>
<p>MobileManiBench的构建分为两个主要阶段：MobileManiRL策略训练和MobileManiDataset数据集生成。整体框架旨在通过仿真高效产生大规模、带标注的移动操作轨迹。</p>
<p><img src="https://arxiv.org/html/2602.05233v1/figures/teaser.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：MobileManiBench概览。包含两个移动机器人平台：配备平行夹爪的G1机器人和配备灵巧手（XHand）的机器人。基准包含20个类别的630个物体，支持5种移动操作技能（打开、关闭、拉、推、抓取），涵盖超过100个任务。通过训练通用RL策略，在100个真实感场景中生成了30万条轨迹，数据模态包括语言指令、多视角RGB-深度-分割图像、同步的物体/机器人状态和动作。</p>
</blockquote>
<p><strong>MobileManiRL训练阶段</strong>：此阶段目标是为每个“机器人-物体-技能”三元组训练一个基于状态的强化学习策略。机器人有两种形态：G1（平行夹爪，1自由度末端执行器）和XHand（灵巧手，12自由度末端执行器）。两者均为移动基座（2自由度）搭配7自由度右臂。动作空间为(6+D)维，前6维表示手腕相对于上一帧的位姿位移，通过逆运动学计算移动基座和手臂的目标关节角；后D维是末端执行器的目标关节角（G1为1维，XHand为12维）。状态输入包括物体状态（9维）、机器人本体感知（G1为78维，XHand为135维）、机器人-物体距离（G1为22维，XHand为31维）以及上一帧动作。奖励函数设计围绕关键点（如图2定义的夹爪/手点、物体抓取点、目标点）的距离和方向，并包含成功奖励和正则化项。使用PPO算法进行训练。</p>
<p><img src="https://arxiv.org/html/2602.05233v1/figures/points.png" alt="关键点定义"></p>
<blockquote>
<p><strong>图2</strong>：不同任务中机器人夹爪/手点（蓝色）、物体抓取点（红色）和目标点（绿色）的定义示意图。</p>
</blockquote>
<p><strong>MobileManiDataset生成阶段</strong>：将训练好的RL策略部署到100个多样化的数字场景中（如图3所示），自动收集成功的操作轨迹。每个轨迹包含：1）自然语言指令；2）多模态感知数据：头部和右手腕两个同步相机拍摄的RGB、深度、分割图像；3）物体状态、机器人状态和动作序列。最终生成超过30万条轨迹，构成大规模、多模态的数据集。</p>
<p><img src="https://arxiv.org/html/2602.05233v1/figures/scenes.png" alt="场景示例"></p>
<blockquote>
<p><strong>图3</strong>：MobileManiBench中使用的多样化、真实感数字场景示例。</p>
</blockquote>
<p><strong>MobileManiVLA模型</strong>：基于生成的MobileManiDataset，本文进一步提出了MobileManiVLA模型，这是一个通用的视觉-语言-动作模型。其架构包含：一个视觉编码器（使用CLIP ViT-L/14@336提取头部和手腕图像的视觉特征）、一个语言编码器（使用T5-base编码指令）、一个多模态融合模块（通过交叉注意力将视觉和语言特征融合），以及一个动作预测头（采用扩散模型预测动作序列）。模型在聚合了所有机器人-物体-技能-场景组合的轨迹上进行训练，旨在实现对未见物体和场景的泛化。</p>
<p>与现有方法相比，本文的创新点具体体现在：1）提出了一个完整的“仿真优先”基准构建框架，将RL策略训练与多模态数据生成解耦，实现了数据生成的可控性和可扩展性；2）首次在大规模基准中同时支持移动基座、平行夹爪和灵巧手，并提供了丰富的多视角、多模态（RGB-D-分割）传感器数据；3）基于该基准训练并评估了通用的VLA模型，系统性地研究了不同输入模态和模型架构对移动操作性能的影响。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了MobileManiBench自身构建的基准进行评估。对比的基线方法包括代表性的VLA模型：RT-2（Variant）、RoboFlamingo、Octo以及本文提出的MobileManiVLA。评估在G1和XHand两种机器人上进行，涵盖了打开、关闭、拉、推、抓取五种技能，并在未见物体和未见场景两种设置下测试泛化能力。成功率的评估标准是任务是否在规定的最大步数内完成。</p>
<p><img src="https://arxiv.org/html/2602.05233v1/figures/object_success.png" alt="物体成功率"></p>
<blockquote>
<p><strong>图4</strong>：不同VLA模型在G1和XHand机器人上，针对不同物体类别的平均成功率。MobileManiVLA在大多数类别上表现最佳。</p>
</blockquote>
<p>关键实验结果总结如下：在G1机器人上，MobileManiVLA在未见物体和未见场景设置下的平均成功率分别为65.7%和68.3%，显著高于RT-2（51.0%， 53.7%）、RoboFlamingo（38.3%， 40.7%）和Octo（33.3%， 35.7%）。在更复杂的XHand机器人上，MobileManiVLA同样取得最佳性能（未见物体：46.7%，未见场景：48.3%），而其他模型成功率均低于35%。这表明在移动和灵巧操作任务上，专门在多样化仿真数据上训练的VLA模型具有明显优势。</p>
<p><img src="https://arxiv.org/html/2602.05233v1/x1.png" alt="输入模态消融"></p>
<blockquote>
<p><strong>图5</strong>：输入模态消融实验。使用手腕相机图像、深度图像以及结合多视角图像都能带来性能提升，其中手腕视角对灵巧操作尤为重要。</p>
</blockquote>
<p>本文进行了详细的消融实验以分析各组件贡献：1）<strong>输入模态</strong>：实验表明，同时使用头部和手腕相机图像比仅用头部图像性能更好（G1上提升4.3%）；加入深度图像能进一步提升性能（G1上再提升3.0%）。2）<strong>动作预测器</strong>：比较了扩散模型、MLP和Transformer，扩散模型作为动作预测头效果最好。3）<strong>视觉编码器</strong>：使用更大规模预训练的EVA-CLIP相比CLIP能带来显著提升（G1上提升6.7%）。4）<strong>数据规模与多样性</strong>：实验证明，使用更多物体类别和更多场景进行训练，能持续提高模型在未见物体和场景上的泛化性能。</p>
<p><img src="https://arxiv.org/html/2602.05233v1/x2.png" alt="泛化分析"></p>
<blockquote>
<p><strong>图6</strong>：模型在不同难度级别的未见场景中的泛化性能。随着场景复杂性增加（从简单桌面到复杂室内外场景），所有模型性能下降，但MobileManiVLA始终保持最高成功率。</p>
</blockquote>
<p>此外，论文还提供了在真实机器人上的初步验证实验（图8，图9），展示了将仿真中训练的MobileManiVLA策略通过sim-to-real方式部署到真实G1机器人上，成功完成了打开笔记本电脑等任务，证明了基准和方法的潜在实用性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了MobileManiBench，一个大规模、多模态的移动操作仿真基准，支持移动基座、平行夹爪和灵巧手，并提供了自动化生成的数据集构建流程；2）基于该基准，系统性地评估了现有VLA模型，并提出了性能更强的MobileManiVLA模型，揭示了多视角视觉、深度信息以及大规模预训练对移动灵巧操作的重要性；3）开源了全部代码、数据集和模型，为社区提供了一个可复现、可扩展的研究平台。</p>
<p>论文提到的局限性在于：目前的工作主要集中于仿真环境，虽然进行了初步的真实世界验证，但大规模的sim-to-real迁移仍然是未来的挑战。此外，基准中的技能和物体类型仍有扩展空间。</p>
<p>对后续研究的启示：首先，MobileManiBench为快速原型验证VLA模型的新架构、新传感器提供了低成本试验场。其次，研究结果表明，针对移动和灵巧操作任务，需要设计能够有效融合多视角、多模态信息的模型架构。最后，该工作凸显了利用高质量仿真和强化学习自动生成大规模、多样化训练数据这一路径的潜力，这可能是突破当前机器人学习数据瓶颈的关键方向之一。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对移动操作机器人中视觉-语言-动作模型验证困难的问题，提出了一个仿真优先的验证框架MobileManiBench。该基准基于NVIDIA Isaac Sim构建，利用强化学习自动生成包含丰富标注的多样化操作轨迹。其核心包含2种移动机器人平台、630个物体、5项技能和超过100个任务，共生成30万条轨迹。实验通过该基准对代表性VLA模型进行了评测，为复杂环境下的感知、推理与控制研究提供了可控、可扩展的分析平台，并公开了全部代码与数据。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.05233" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>