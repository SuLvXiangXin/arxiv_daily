<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.15062" target="_blank" rel="noreferrer">2507.15062</a></span>
        <span>作者: Yunzhu Li Team</span>
        <span>日期: 2025-07-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，利用手持夹爪收集人类演示数据因其易于部署和多功能性而日益普及。然而，现有系统大多专注于视觉传感，很大程度上忽视了触觉反馈。这一缺陷限制了它们捕捉人类在真实世界任务中使用的、涉及精细接触的策略的能力。仅依赖视觉的系统容易受到环境变化（如遮挡、光照不佳）的影响，而触觉感知提供了互补且鲁棒的信号，对光照条件和相机视角具有不变性。阻碍视觉触觉数据在野外广泛收集的两个关键挑战是：缺乏便携、鲁棒的触觉硬件，以及从多模态数据中学习有效表示的困难。本文针对这些痛点，提出了一种用于大规模数据收集和多模态策略学习的便携式视觉触觉系统。其核心思路是：开发一个集成柔性触觉传感器的便携夹爪，并设计一个基于掩码自编码的跨模态表示学习框架，将学习到的表征用于下游扩散策略，以实现精细操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个两阶段学习框架：第一阶段通过掩码触觉重建预训练一个视觉触觉编码器；第二阶段将预训练的编码器集成到条件扩散策略中，用于下游操作任务。</p>
<p><img src="https://arxiv.org/html/2507.15062v2/x3.png" alt="方法总览"></p>
<blockquote>
<p><strong>图3</strong>：方法整体框架。第一阶段：利用在多样室内外环境中收集的大规模数据集，通过跨模态重建预训练视觉触觉编码器。第二阶段：将预训练的编码器与机器人本体感觉结合，为下游任务（如物体重定向和插入）的条件扩散策略提供条件。</p>
</blockquote>
<p><strong>整体流程</strong>：输入是同步的RGB图像I和触觉图像T（由两个指尖传感器读数垂直堆叠而成）。在预训练阶段，编码器E_ϕ接收（可能被掩码的）触觉输入和视觉输入，学习融合表征z_fusion，并通过解码器D_ψ重建完整的触觉图像。在策略学习阶段，每个时间步的观测（I_t, T_t, p_t）通过预训练的编码器得到z_t，与本体状态p_t拼接后，作为条件输入扩散策略模型，预测机器人的动作。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>触觉编码器</strong>：触觉图像（1×24×32）通过固定色彩映射转换为3通道，并分割为4×4的补丁。训练时，随机掩码60-80%的补丁。掩码后的触觉图像由一个3层CNN处理，产生768维嵌入z_tac。</li>
<li><strong>视觉编码器</strong>：RGB图像由基于CLIP初始化的ViT-B/16编码器处理，提取最终的[CLS]令牌作为视觉嵌入z_img。</li>
<li><strong>跨模态融合</strong>：采用两轮多头交叉注意力（MHAttn）整合触觉和视觉特征。第一轮以z_tac为查询（Q），z_img为键（K）和值（V），更新触觉特征。第二轮以更新后的z_img为Q，更新后的z_tac为K和V，更新视觉特征。最后将更新后的两个特征拼接，得到融合表征z_fusion ∈ ℝ^(2×768)。</li>
<li><strong>触觉重建解码器</strong>：融合特征z_fusion通过一个两层MLP和Sigmoid激活函数，重建出触觉图像T_hat。预训练损失函数为L2重建损失：ℒ_stage1 = ||T - T_hat||₂²。</li>
<li><strong>扩散策略</strong>：采用条件扩散策略，模型预测添加到动作上的噪声。训练时最小化预测噪声与真实添加噪声之间的MSE损失：ℒ_stage2 = 𝔼[‖ϵ_t^k - ϵ_θ(a_t^k, o_t, k)‖₂²]。推理时通过迭代去噪生成动作。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>硬件</strong>：采用柔性压阻触觉传感器（12×32阵列）集成到便携夹爪中，支持野外大规模数据同步收集。</li>
<li><strong>学习框架</strong>：采用掩码重建目标（而非对比学习）来预训练跨模态编码器，旨在保留触觉特有的几何敏感信号。</li>
<li><strong>融合机制</strong>：使用交叉注意力进行双向特征融合，使模型能够根据视觉上下文推理触觉信号，反之亦然。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与平台</strong>：构建了一个大规模视觉触觉数据集，包含超过260万对视觉触觉数据，来自43个操作任务的2700多次演示，覆盖12个室内外环境。下游任务在配备相同传感器的XArm 850机器人上评估。</p>
<p><strong>对比基线</strong>：</p>
<ol>
<li><strong>Vision-Only</strong>：仅使用RGB图像和CLIP嵌入的扩散策略。</li>
<li><strong>Ours w/o Cross-Attention</strong>：无交叉注意力，简单拼接触觉CNN特征与视觉嵌入。</li>
<li><strong>Ours w/o Pretraining</strong>：使用本文编码器架构但未经视觉触觉预训练。</li>
<li><strong>Ours w/ Pretraining</strong>：完整方法，使用预训练的视觉触觉编码器。</li>
</ol>
<p><strong>关键实验结果</strong>：<br>实验评估了四个精细操作任务，每个任务进行20次试验。</p>
<p><img src="https://arxiv.org/html/2507.15062v2/x6.png" alt="定量结果"></p>
<blockquote>
<p><strong>图6</strong>：四个下游精细操作任务的代表性画面：试管收集、铅笔插入、液体转移和白板擦除。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.15062v2/x4.png" alt="数据分布"></p>
<blockquote>
<p><strong>图4</strong>：预训练数据分布。数据集分为三类：本文四个核心任务、其他室内任务（增加分布多样性）以及野外任务（捕捉复杂真实场景）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.15062v2/x5.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：预训练定性结果。展示了编码器对分布内和分布外输入的触觉重建能力，以及视觉模块的自注意力热图始终关注夹爪接触区域。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.15062v2/x7.png" alt="消融结果"></p>
<blockquote>
<p><strong>图7</strong>：试管收集任务在不同训练演示数量和训练轮数下的消融结果。结果表明，带有视觉触觉预训练的策略在低数据量和低训练轮次下始终优于无预训练的策略。</p>
</blockquote>
<p><strong>结果总结</strong>：</p>
<ul>
<li>在需要<strong>手内状态信息</strong>的任务中（如试管插入、铅笔插入），完整方法（Ours w/ Pretraining）表现最佳。例如，试管收集的“整个任务”成功率达<strong>85%<strong>，显著高于纯视觉的</strong>25%</strong> 和无预训练的**70%<strong>。铅笔插入任务成功率达</strong>85%**，也优于其他基线。</li>
<li>在需要<strong>精细力信息</strong>的任务中（如液体转移、白板擦除），完整方法在液体转移上达到<strong>90%</strong> 成功率，而纯视觉仅为**55%**。触觉帮助策略准确检测力变化和动作阶段转换。</li>
<li><strong>消融实验</strong>表明：1) 交叉注意力模块对性能提升至关重要；2) 视觉触觉预训练能显著提高策略的数据效率和最终性能，尤其在演示数据有限时（图7）。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>硬件上，开发了一个轻量级、便携式手持视觉触觉夹爪，支持在多样化的真实世界环境中进行大规模、同步的多模态数据收集。</li>
<li>算法上，提出了一个基于掩码自编码的跨模态表示学习框架，通过交叉注意力融合视觉与触觉信号，学习到的表征能提升下游策略学习的样本效率和精度。</li>
<li>数据上，创建并承诺开源一个大规模、多样化的野外视觉触觉操作数据集，包含超过260万数据对，为社区提供了宝贵资源。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前触觉传感器的空间分辨率（12×32）仍然有限，可能无法捕捉极其细微的接触模式。此外，虽然系统能感知压力分布，但未明确集成直接的力控制闭环。</p>
<p><strong>后续启示</strong>：</p>
<ul>
<li>探索更高空间分辨率的柔性触觉传感器，以捕获更丰富的接触几何信息。</li>
<li>将学习到的触觉表征与更精确的力控制策略相结合，实现真正意义上的力交互操作。</li>
<li>将本框架扩展到更复杂的、长序列的装配或工具使用任务中，验证其泛化能力。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有手持夹持器缺乏触觉反馈、难以支持精细操作的问题，提出一种集成触觉传感器的便携式视觉-触觉夹持器硬件，以及跨模态表示学习框架。该框架融合视觉与触觉信号，保留各自特性，学习关注物理交互接触区域的可解释表征。在试管插入、移液管流体转移等精细操作任务中，该方法提升了操作的准确性与鲁棒性，尤其在存在外部干扰时表现更优。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.15062" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>