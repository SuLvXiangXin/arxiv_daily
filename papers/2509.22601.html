<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.22601" target="_blank" rel="noreferrer">2509.22601</a></span>
        <span>作者: Xing Sun Team</span>
        <span>日期: 2025-09-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>强化学习是提升大语言模型在长视野、稀疏奖励智能体任务中战略工具使用能力的主流范式，但其面临探索与利用权衡的根本性挑战。现有研究通常通过策略熵的视角来刺激探索，然而这种机械的熵最大化在多轮交互导致的分布漂移下，容易引发强化学习的不稳定性。本文旨在不陷入熵崩溃或失控发散的前提下，在智能体自身经验的指导下实现渐进的探索-利用平衡。</p>
<p>具体而言，当前基于熵最大化的方法在应用于LLM驱动的智能体时是脆弱的：来自环境反馈的低概率token累积会引发严重的分布漂移，常导致模式崩溃。同时，智能体模型也可能因对多轮交互的不确定性而经历持续的熵增长，导致训练不稳定。近期方法尝试通过冷启动的监督微调或RL与SFT的混合方案来缓解此问题，但这限制了策略对SFT语料库之外策略的发现。本文的核心思路是通过课程调度来协调自模仿学习和内在奖励塑造，从而实现从技能级探索到动作级探索的渐进式过渡。</p>
<h2 id="方法详解">方法详解</h2>
<p>SPEAR是一种基于课程的强化学习方法，旨在通过自模仿和内在奖励改善探索-利用平衡。其整体框架遵循经典的自我模仿学习，即准备一个独立的回放缓冲区来存储过去回合中回报超过基线的状态-动作对，并利用此缓冲区来鼓励高回报动作，在稀疏奖励、长视野任务中改进困难探索。</p>
<p><img src="https://arxiv.org/html/2509.22601v4/figures/overview.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：SPEAR方法总览。智能体首先与环境交互生成一组轨迹，这些轨迹经过内在奖励塑造和基于策略的优势估计进行更新。随后，它们被筛选并存入回放缓冲区，通过所提出的自模仿方案进行离策略更新。这种双重集成最大化了过去经验的效用，从而扩展了有效探索空间，同时缓解了持续的不确定性。</p>
</blockquote>
<p><strong>核心模块一：自模仿学习</strong>。该方法旨在挖掘过去成功的经验以进行有效的动作级探索。其包含三个关键设计：</p>
<ol>
<li><strong>优先经验回放</strong>：维护一个回放缓冲区<code>𝒟</code>，仅存储具有正优势值的轨迹。自模仿目标函数如公式(1)所示，通过指示函数<code>𝟏(Âj&gt;0)</code>筛选。</li>
<li><strong>优势重校准</strong>：为解决离策略估计中过去策略的回报与当前策略日益脱节的问题，提出重校准优势值。维护一个包含最近<code>N_𝒟R</code>条轨迹组内平均回报的FIFO缓冲区<code>𝒟R</code>，并使用其50百分位数<code>P50(𝒟R)</code>作为稳健的策略基线估计。重校准后的优势计算为<code>Ã_t^i = R^i - P50(𝒟R)</code>，如公式(2)所示。此举使基线性能与策略变化相关联，并能通过<code>Âj&gt;0</code>和<code>Ãj&gt;0</code>双重条件过滤过时经验，同时缓解组归一化带来的难度偏差。更新后的离策略SIL目标函数如公式(3)和(4)所示。</li>
<li><strong>课程调制的渐进经验利用</strong>：通过一个热身系数<code>γ</code>对自模仿项进行调度，如公式(5)所示。其假设是：在早期阶段，向多样化动作的分布转移比模仿有限的解决方案模式更重要。课程安排旨在早期限制对可能不成熟经验的机械模仿，后期防止对环境状态和策略动作的持续不确定性。</li>
</ol>
<p><strong>核心模块二：内在奖励塑造</strong>。该方法用于技能级探索，引导智能体通过工具调用奖励广泛调查工具使用。</p>
<ol>
<li><strong>奖励构成</strong>：每条轨迹<code>τ_i</code>的复合奖励<code>R^i</code>包括最终结果奖励<code>R_outcome^i</code>、连续工具调用奖励<code>R_tool-call^i</code>和格式奖励<code>R_format^i</code>。</li>
<li><strong>课程调制的渐进奖励调制</strong>：通过系数<code>μ</code>调度工具调用奖励的贡献，如公式(6)所示。在早期加速掌握工具使用以实现向新任务设置的快速分布转移；在后期防止优化振荡和奖励项之间的竞争。论文指出，添加工具调用奖励是一把双刃剑：没有它，智能体会因负面工具响应而放弃编码，退化为纯文本推理；强制使用则可能刺激交互轮数过多，导致响应过长并引发结果准确性的振荡。</li>
</ol>
<p><strong>创新点</strong>：SPEAR在经典SIL基础上进行了三项针对性修改：(1)引入课程来整合技能级和动作级探索；(2)通过优势重校准处理离策略更新的挑战；(3)正则化策略更新以稳定熵并缓解奖励黑客行为。</p>
<p><strong>强基线Dr.BoT</strong>：本文还整合了工业实践中验证的多种RL技巧，提出了一个强基线方法Dr.BoT，作为对比SPEAR有效性的基础。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在三个基准测试上进行了评估：ALFWorld（具身推理）、WebShop（网页导航）和AIME 2024/2025（代码解释器集成推理）。对比的基线方法包括PPO、RLOO、GRPO、GiGPO以及本文提出的Dr.BoT。模型规模覆盖Qwen2.5-1.5B/7B/32B-Instruct和Qwen3-32B-Instruct。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>ALFWorld &amp; WebShop</strong>：如表1所示，SPEAR为GRPO、GiGPO和Dr.BoT带来了显著的性能提升。在ALFWorld上，SPEAR将GRPO/GiGPO/Dr.BoT的成功率分别最高提升了16.1%/5.1%/8.6%；在WebShop上，分别最高提升了20.7%/11.8%/13.9%。这些提升在1.5B和7B模型上具有一致性。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.22601v4/x1.png" alt="ALFWorld &amp; WebShop结果"></p>
<blockquote>
<p><strong>表1</strong>：在ALFWorld和WebShop数据集上的主要结果。SPEAR在所有基线和模型规模上均带来了显著且一致的性能提升。</p>
</blockquote>
<ol start="2">
<li><strong>AIME 2024 &amp; 2025</strong>：如表2所示，在代码解释器任务上，SPEAR将Dr.BoT基线在AIME24和AIME25上分别最高提升了3.8%和6.1%。值得注意的是，SPEAR使Qwen2.5-32B在使用更小token预算（16K上下文）的情况下，达到了与使用更大上下文（32K）或更强模型（Qwen3）基线相当的性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.22601v4/x2.png" alt="AIME结果"></p>
<blockquote>
<p><strong>表2</strong>：在AIME 2024和2025数据集上的结果。SPEAR有效提升了代码解释器任务的性能，并展示了良好的泛化性。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：如表3和表4所示，分别验证了自模仿学习（SI）和内在奖励（IR）两个核心组件的贡献。<ul>
<li><strong>自模仿学习</strong>：在ALFWorld和WebShop上，仅使用SI即可带来显著提升（例如，GRPO+SI在1.5B模型上提升4.5%）。但在AIME24上，仅SI有时会导致性能轻微下降，这可能与模仿多工具调用样本导致交互轮数激增、引发训练不稳定有关。</li>
<li><strong>内在奖励</strong>：对于1.5B模型，IR带来了持续收益。对于7B模型，在某些任务上SI单独即可带来最大增益。在AIME任务上，IR对于鼓励从文本推理向工具集成推理的转变是不可或缺的。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.22601v4/figures/buffer_size.jpg" alt="消融实验表"></p>
<blockquote>
<p><strong>表3与表4</strong>：消融研究表明，自模仿学习和内在奖励塑造均是SPEAR有效的关键组件，其贡献因任务和模型规模而异。</p>
</blockquote>
<ol start="4">
<li><p><strong>泛化到视觉语言智能体</strong>：在Sokoban视觉推理任务上（表5），SPEAR同样显著提升了基线方法的成功率（例如，将GRPO从67.1%提升至86.7%），证明了其跨模态的泛化能力。</p>
</li>
<li><p><strong>效率分析</strong>：SPEAR带来的性能增益仅伴随约10%~25%的理论计算复杂度额外开销，但在实践中每轮迭代的运行时间开销可忽略不计，体现了其即插即用的可扩展性。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了SPEAR，一种用于训练LLM智能体的自模仿学习泛化方法。它绕过了昂贵的专家模仿，允许在智能体自身奖励经验的指导下进行探索。</li>
<li>引入了课程调度，以协调自模仿学习与内在奖励塑造，从而管理策略熵，并实现从基于技能的探索到基于动作的探索的渐进过渡。</li>
<li>提出了一个结合了工业实践中多种RL技巧的强基线Dr.BoT，证实了其有效性以及对现有基线的优越性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，SPEAR带来了约10%~25%的理论计算复杂度开销。此外，消融实验表明，其组件的有效性可能依赖于任务和模型规模（例如，内在奖励对7B模型在某些任务上的增益不如自模仿学习显著）。</p>
<p><strong>后续启示</strong>：SPEAR展示了一种通过课程化利用智能体自身经验来动态管理探索-利用平衡的有效途径。其模块化设计（自模仿+内在奖励+课程调度）和即插即用的特性，为后续设计更稳定、高效的智能体强化学习算法提供了可借鉴的框架。同时，其在不同任务和模型规模上表现出的差异性也提示，未来的自适应调度机制可能需要更精细地考虑任务特性与模型能力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对LLM智能体强化学习中探索与利用的平衡难题，提出SPEAR方法。该方法通过课程调度的自模仿学习，协调内在奖励塑造与经验回放：早期促进工具交互以加速探索，后期强化对成功策略的利用。实验表明，SPEAR在ALFWorld和WebShop任务上将基线成功率最高提升16.1%和20.7%，在AIME竞赛任务上提升最高6.1%，仅增加10%–25%理论复杂度，具备即插即用的扩展性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.22601" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>