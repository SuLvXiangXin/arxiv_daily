<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.05627" target="_blank" rel="noreferrer">2507.05627</a></span>
        <span>作者: Frank Chongwoo Park Team</span>
        <span>日期: 2025-07-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作任务（如目标驱动抓取、三维空间推理）需要从视觉观测中重建物体三维几何并识别物体实例。现有方法大多依赖精确深度或多视角RGB图像，但本文关注一个更实际的场景：仅从稀疏、部分视角的RGB图像（如仅两幅）进行重建。这在真实世界中至关重要，因为深度感知对透明或反光物体常失效，且杂乱环境中的遮挡或机器人工作空间限制使得360度全景覆盖很少可用。</p>
<p>近期方法通过在成对的局部观测与完整场景数据集上进行监督学习来应对此挑战，但其性能受限于数据集的多样性。这凸显了对能超越精编3D训练数据限制、更鲁棒泛化的替代方法的需求。受大规模图像生成模型（经相机位姿与图像配对数据微调后）能根据局部视图图像合理预测场景未观察部分（如物体背面）的启发，本文提出新思路。然而，先前工作仅在干净、前向、无遮挡的单物体图像上表现良好，在物体相互严重遮挡的多物体场景中（这在机器人操作任务中很普遍）往往表现不佳。</p>
<p>本文的核心思路是：首先从局部视图图像重建粗略三维几何，然后通过对比学习将多个物体分割为独立实例，再对每个物体进行三维几何细化。最关键的是，在细化阶段，利用多模态大语言模型从分割图像自动生成的文本描述，为每个物体的几何引入额外偏置，引导重建几何更好地与文本对齐。本文方法名为DreamGrasp，是一种零样本的、从稀疏局部视图图像进行3D重建和实例识别的方法，无需微调即可利用大规模生成模型。</p>
<h2 id="方法详解">方法详解</h2>
<p>DreamGrasp的目标是从稀疏、部分视角的RGB图像中恢复工作空间中物体的三维几何并识别单个物体实例。假设相机内参和外参已知。整体流程分为两个阶段：粗略场景重建阶段和实例级几何细化阶段。</p>
<p><img src="https://arxiv.org/html/2507.05627v1/extracted/6604286/figs/pipeline.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DreamGrasp整体流程。（观测）仅使用两幅局部视角RGB图像作为输入。（输入处理）分别使用SAM和ChatGPT从RGB图像中提取实例掩码和文本提示。（粗略阶段）利用这些输入进行初始场景级几何重建，结合RGB和实例掩码图像，并受Zero123引导的新视角监督。（细化阶段）使用学习到的特征分割粗略场景，并通过实例级RGB输入以及文本条件扩散模型引导的新视角监督来细化每个物体。</p>
</blockquote>
<p><strong>粗略场景重建阶段</strong>：采用3D高斯泼溅表示场景，并增加一个归一化的D_f维特征通道来编码实例感知特征。每个高斯由均值、协方差矩阵、不透明度、颜色和特征向量组成。除了在稀疏输入图像上的RGB渲染损失，还利用Zero123的分数蒸馏来改进未观察区域的形状估计，并使用对比学习（基于2D实例掩码）来学习3D实例感知特征。对比损失鼓励同一掩码内的高斯共享相似特征值，同时推开不同掩码的高斯。</p>
<p>然而，仅从稀疏局部视图图像计算的对比损失不足以学习平滑准确的实例特征场，常导致对有限视角的过拟合。为此，本文提出了一种新颖的正则化项：<strong>表面不变特征正则化器</strong>。它鼓励高斯特征值沿物体表面尽可能恒定。具体地，定义归一化特征场F(x)，SIFR损失衡量F的局部平滑度，并由输入空间度量G(x)加权。通过构造G(x)，使其在表面法线方向的特征值远大于切向方向，从而鼓励F(x)沿表面切向保持不变。所有项均有闭式解并可高效计算。</p>
<p><strong>实例级细化阶段</strong>：首先基于学习到的特征向量f_i对高斯进行聚类，实现3D实例分割。但如论文图2左所示，粗略阶段分割出的高斯常因物体间遮挡而噪声较大。因此，细化阶段引入三个组件来提升分割和几何质量。</p>
<p><img src="https://arxiv.org/html/2507.05627v1/extracted/6604286/figs/after_coarse_refine.png" alt="分割与细化对比"></p>
<blockquote>
<p><strong>图2</strong>：左：粗略阶段后的分割结果；右：细化阶段后的结果。</p>
</blockquote>
<p>第一，<strong>实例级RGB重建损失</strong>：使每个物体的渲染与对应掩码下的观测RGB图像对齐。这需要解决跨视图的实例对应关系（通过基于平均渲染特征的二分图匹配）。损失定义为各图像中各实例掩码区域内的渲染与观测RGB的差异平方和。</p>
<p>第二，<strong>文本引导的分数蒸馏</strong>：由于当掩码因严重遮挡而不完整时，对每个掩蔽物体图像单独应用Zero123效果不足，本文引入额外的分数蒸馏策略。使用多模态大语言模型为每个物体生成文本描述，然后应用文本条件扩散模型的分数蒸馏，使每个实例的渲染外观与对应的文本提示对齐，从而为细化提供几何偏置。</p>
<p>第三，<strong>周期性离群点去除</strong>：虽然分数蒸馏能自然抑制与文本未对齐的无关高斯，但一些噪声高斯会坍缩成对渲染贡献很小的小簇。为消除此类离群点，定期对每个实例的高斯中心应用基于点云的离群点去除算法。这些细化组件与Zero123的新视角监督在整个细化阶段联合优化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在ShelfObject数据集（模拟货架上的杂乱物体）、YCBInEOAT数据集（机器人夹持器视野中的YCB物体）以及真实世界采集的数据上进行评估。对比的基线方法包括Zero123、OpenLRM和SparseGS。评估指标包括几何IoU（与真实网格的交并比）、深度L1误差和分割精度（PQ）。机器人操作实验在配备夹持器的Franka Emika Panda机械臂上进行。</p>
<p><strong>关键定量结果</strong>：在ShelfObject数据集上，DreamGrasp的几何IoU达到0.696，显著高于Zero123的0.248、OpenLRM的0.374和SparseGS的0.484。深度L1误差为0.027m，也优于其他方法。分割精度（PQ）达到0.832。</p>
<p><img src="https://arxiv.org/html/2507.05627v1/extracted/6604286/figs/recog_results3.png" alt="定量结果对比"></p>
<blockquote>
<p><strong>图3</strong>：在ShelfObject和YCBInEOAT数据集上的定量对比。DreamGrasp在几何IoU和深度L1误差上均显著优于基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.05627v1/extracted/6604286/figs/extended_recog_results.png" alt="扩展结果"></p>
<blockquote>
<p><strong>图7</strong>：更多定性结果展示。DreamGrasp能有效处理严重遮挡，重建出完整、分离的物体实例。</p>
</blockquote>
<p><strong>消融实验</strong>：分析了SIFR、实例级RGB损失和文本引导蒸馏各组件的重要性。移除SIFR会导致分割边界模糊和特征不一致；移除实例级RGB损失会降低重建精度；移除文本引导则会使物体形状不完整。三者结合效果最佳。</p>
<p><img src="https://arxiv.org/html/2507.05627v1/extracted/6604286/figs/ss_vs_sifr.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融研究。左：仅使用对比损失（SS Loss），特征场在未观察区域不一致，导致错误聚类。右：加入SIFR后，特征沿表面更平滑一致，实现了准确的3D实例分割。</p>
</blockquote>
<p><strong>机器人操作应用</strong>：</p>
<ol>
<li><p><strong>顺序清理</strong>：任务是从场景中依次抓取移除所有物体。得益于DreamGrasp提供的实例级几何，单次重建即可规划整个清理序列。实验显示，在模拟和真实场景中，使用DreamGrasp规划的清理任务成功率分别达到95%和90%。<br><img src="https://arxiv.org/html/2507.05627v1/extracted/6604286/figs/real_clear_clutter_large2.png" alt="顺序清理"></p>
<blockquote>
<p><strong>图4</strong>：真实世界顺序清理任务。DreamGrasp重建的实例几何用于规划无碰撞抓取，成功清空货架。</p>
</blockquote>
</li>
<li><p><strong>目标检索</strong>：任务是从杂乱场景中抓取指定目标物体。DreamGrasp重建的实例几何允许进行精确的碰撞检查和抓取规划。在真实世界实验中，目标检索成功率达到85%。<br><img src="https://arxiv.org/html/2507.05627v1/extracted/6604286/figs/real_target_retrieval4.png" alt="目标检索"></p>
<blockquote>
<p><strong>图6</strong>：真实世界目标检索任务。根据文本查询（如“蓝色杯子”）识别并抓取目标物体，同时避免与其他物体碰撞。</p>
</blockquote>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.05627v1/extracted/6604286/figs/real_world_objects.png" alt="真实物体"></p>
<blockquote>
<p><strong>图5</strong>：真实世界物体重建示例。输入为两幅稀疏RGB图像，DreamGrasp能重建出具有合理背面几何的分离物体。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了DreamGrasp框架，首次实现了从极稀疏（如两幅）局部RGB图像进行零样本、实例级的三维场景重建，专门针对杂乱、遮挡的多物体环境。</li>
<li>引入了表面不变特征正则化器，有效提升了在稀疏视图下学习一致3D实例特征场的稳定性。</li>
<li>创新性地结合了实例级RGB损失与文本引导的分数蒸馏，在细化阶段显著改善了单个物体的几何完整性和准确性，并成功将重建结果应用于实际的机器人顺序清理和目标检索任务，验证了其下游应用价值。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法依赖于SAM等分割模型来获取初始2D实例掩码，其分割质量会影响后续流程。此外，文本描述由MLLM生成，其准确性和细节程度也会影响文本引导细化的效果。</p>
<p><strong>启示</strong>：DreamGrasp展示了利用大规模预训练生成模型（未经特定场景微调）的“想象力”来弥补三维观测缺失的巨大潜力。其“粗略重建-实例分割-文本引导细化”的流程为处理复杂多物体场景提供了新范式。后续研究可探索如何进一步减少对2D分割模型的依赖，或开发更鲁棒的跨视图实例关联方法，并研究如何将物理常识更有效地融入几何推理过程。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DreamGrasp框架，旨在解决从稀疏、局部视角的RGB图像进行零样本三维多物体重建的难题，以支持机器人操作。该方法核心在于利用大规模预训练图像生成模型的推断能力，通过粗三维重建、基于对比学习的实例分割（结合表面正则化）以及文本引导的实例级细化，实现对复杂遮挡场景中未观察部分的补全。实验表明，该方法能准确恢复物体几何，并有效支持如顺序清理与目标检索等下游任务，取得较高的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.05627" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>