<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.12276" target="_blank" rel="noreferrer">2510.12276</a></span>
        <span>作者: Li, Fuhao, Song, Wenxuan, Zhao, Han, Wang, Jingbo, Ding, Pengxiang, Wang, Donglin, Zeng, Long, Li, Haoang</span>
        <span>日期: 2025/10/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型在机器人遵循语言指令执行精确操作方面展现出巨大潜力。主流VLA模型大多建立在仅使用2D数据预训练的视觉-语言模型（VLM）之上，这些模型缺乏准确的空间感知能力，阻碍了其在3D物理世界中的操作。现有解决方案试图引入显式的3D传感器输入（如深度图或点云），但面临传感器噪声、硬件异构性以及现有数据集中深度信息不完整的挑战。另一种从2D图像估计3D信息的方法则受限于深度估计器的性能。本文针对VLA模型缺乏内在3D理解能力这一痛点，提出了一种新视角：不依赖显式3D输入或深度估计器，而是通过隐式地对齐VLA的中间视觉表示与预训练的3D基础模型的几何表示，来“迫使”VLA模型发展空间理解能力。本文的核心思路是：通过一种称为“空间强迫”（Spatial Forcing, SF）的表示对齐策略，将来自预训练3D基础模型的丰富空间知识隐式地注入到VLA的视觉嵌入中，从而提升其动作生成的精确性。</p>
<h2 id="方法详解">方法详解</h2>
<p>Spatial Forcing（SF）的整体目标是在训练VLA模型时，额外引入一个对齐损失，使其视觉嵌入与外部3D空间表示对齐，从而隐式地学习空间理解能力。</p>
<p><img src="https://arxiv.org/html/2510.12276v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Spatial Forcing（SF）方法示意图。(a) SF将VLA的中间视觉嵌入与来自预训练3D基础模型（VGGT）的几何表示进行对齐。(b) 该策略显著提升了训练效率和测试精度。(c) 深度探测实验证明，经过SF对齐的表示蕴含了空间信息。</p>
</blockquote>
<p><strong>核心流程</strong>：给定机器人采集的多视角图像，一方面，它们被输入VLA的标准视觉编码器（如SigLIP或DINOv2）生成视觉token序列 {𝒙ᵢ^𝒱}。另一方面，同一组图像被输入一个预训练的3D基础模型——视觉几何接地Transformer（VGGT），该模型能输出每张图像的像素级空间表示 f_i^3D(I)。SF的核心操作是，在训练过程中，强制VLA模型中间某层的视觉token表示与VGGT输出的对应空间表示（加上位置嵌入E）在特征空间中对齐。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>目标空间表示生成器</strong>：采用VGGT作为预训练的3D基础模型。VGGT是一个前馈模型，能够根据一系列2D图像直接输出场景的各种3D属性。论文认为其Transformer骨干网络提取的潜在表示天然编码了丰富的空间信息，足以作为3D监督信号。</li>
<li><strong>对齐操作</strong>：首先，对VLA提取的每个视觉token 𝒙ᵢ^𝒱进行批归一化Γ，然后通过一个两层的MLP将其投影到与目标表示兼容的维度。接着，使用余弦相似度作为度量，最大化投影后的视觉token与对应的VGGT空间表示（加上位置嵌入）之间的对齐程度。对齐损失 ℒ_align 定义为负的平均余弦相似度（公式3）。</li>
<li><strong>对齐层选择</strong>：VLA的VLM骨干通常包含多层因果注意力层。实验发现，对齐相对较深但不是最深的层（例如32层中的第24层）效果最佳。这是因为较浅的层特征过于具体，而最深的层视觉特异性丢失过多，趋于模态无关，都不利于接受目标空间表示的监督。</li>
<li><strong>总体训练目标</strong>：最终的训练损失是标准的动作生成损失 ℒ_action 与SF对齐损失的加权和：ℒ_SF = ℒ_action + αℒ_align（公式4）。在推理阶段，应用了SF的VLA模型与标准VLA完全相同，不引入任何额外结构或计算开销。</li>
</ol>
<p><strong>创新点</strong>：与现有依赖显式3D输入或深度估计的方法不同，SF的创新在于提出了一种<strong>隐式的、表示层面的</strong>空间知识注入范式。它不改变VLA的输入模态或模型架构，而是通过一个辅助的对齐损失，引导VLA的中间表示学习到预训练3D模型所蕴含的空间先验，从而“强迫”模型内部发展出空间理解能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真环境LIBERO（包含Spatial, Object, Goal, Long四个任务套件）和RoboTwin（真实到仿真的双手操作基准，包含简单和困难设置）上进行了评估。基线方法包括主流的2D VLA（如OpenVLA、Octo、π₀）和显式3D VLA（如SpatialVLA、GeoVLA、3D-CAVLA）。使用成功率（SR）作为评估指标。</p>
<p><strong>关键实验结果</strong>：<br>在LIBERO基准上（表1），SF取得了平均98.5%的成功率，超越了所有不依赖额外传感器输入的2D VLA方法。值得注意的是，尽管SF不使用任何显式深度或点云输入，其性能与那些使用了额外3D传感器输入的方法（如GeoVLA的97.7%，3D-CAVLA的98.1%）相当甚至更优。</p>
<p><img src="https://arxiv.org/html/2510.12276v2/x4.png" alt="仿真结果对比"></p>
<blockquote>
<p><strong>图4</strong>：在RoboTwin 2.0基准上的对比。SF在平均成功率上达到最高，并且在困难任务上相比基线模型π₀有显著提升，表明其能更好地应对域随机化，关注物体间的空间关系而非虚假关联。</p>
</blockquote>
<p>在RoboTwin基准上（图4），SF在平均成功率上表现最佳，特别是在困难设置中相比基线π₀有大幅提升，证明其增强的空间感知能力有助于模型关注物体位置和空间关系，而非背景、光照等捷径线索。</p>
<p><strong>消融实验分析（表2，图5）</strong>：</p>
<ol>
<li><strong>目标表示选择</strong>：使用VGGT作为对齐目标效果最好（平均SR 96.9%），优于仅使用2D预训练的视觉编码器SigLIP（94.0%）或DINOv2（94.1%），证明补偿3D理解缺失是关键。为目标表示添加位置嵌入（PE）能显著提升长视野任务性能。</li>
<li><strong>对齐层选择</strong>：对齐第24层效果最佳（平均SR 96.9%），验证了选择“相对较深但非最深”层的有效性。</li>
<li><strong>训练效率</strong>：如图5(a)所示，引入SF后模型收敛速度大幅加快，达到相同成功率所需训练迭代次数减少了3.8倍。</li>
<li><strong>数据效率</strong>：如图5(b)和表2所示，SF仅使用5%的数据就能达到75.8%的成功率，在相同数据量下比基线成功率高出25.8%，为达到相同成功率所需数据量减少了5.9倍。</li>
<li><strong>表示对齐可视化</strong>：如图5(c)的t-SNE可视化所示，经过对齐后，VLA的视觉特征分布与目标VGGT特征分布形状高度相似，但同时保持了独立的聚类中心，说明SF在迫使VLA学习空间理解能力的同时，也保留了其原有的表征特性。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.12276v2/x5.png" alt="消融与效率分析"></p>
<blockquote>
<p><strong>图5</strong>：(a) 训练效率对比：SF加速收敛达3.8倍。(b) 数据效率对比：SF仅用5%数据达到75.8%成功率。(c) t-SNE可视化：对齐后VLA特征分布与目标VGGT特征形状相似但中心独立。</p>
</blockquote>
<p><strong>真实世界实验</strong>：<br>在单臂和双臂操作任务上（图6），仅用极少量演示（单臂40条，双臂20条）进行训练。SF在所有任务上均取得了比基线模型更高的成功率，特别是在具有视觉欺骗性（透明杯子）、目标物体变化或需要精确高度估计的任务中提升显著，证明了其强大的空间理解能力和数据利用效率。</p>
<p><img src="https://arxiv.org/html/2510.12276v2/x6.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图6</strong>：真实世界实验设置与结果。(a) 单臂任务在各种变化下的成功率。(b) 双臂任务成功率。(c) 机器人实验平台顶视图。SF在所有任务上均表现出更高的成功率和数据效率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>通过深度探测实验，揭示了仅基于2D图像训练的VLA模型其视觉嵌入中空间信息的不足。</li>
<li>提出了Spatial Forcing（SF），一种简单有效的隐式表示对齐策略，通过将VLA的视觉嵌入与预训练3D基础模型的几何表示对齐，无需显式3D输入即可为VLA注入空间理解能力。</li>
<li>实验证明，SF不仅在各种机器人任务上实现了最先进的性能，还显著提升了训练效率（加速3.8倍）和数据效率（仅需少量数据即可达到高性能），并在真实世界实验中验证了其有效性。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确陈述具体的局限性，但方法的效果可能依赖于所选3D基础模型（如VGGT）的质量和通用性。</p>
<p><strong>启示</strong>：SF为增强VLA乃至其他视觉基础模型的空间感知能力提供了一条新路径。它表明，通过精心设计的中间表示对齐，可以高效地将一种模型（如3D基础模型）的先验知识迁移到另一种模型（如VLA）中，而无需改变其架构或输入接口。这种“隐式知识注入”的范式对解决机器人学习中数据稀缺、模态缺失等问题具有启发意义，未来可探索将其应用于其他感知能力（如物理属性、材质理解等）的迁移中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型因基于2D数据预训练而缺乏空间感知、难以在3D物理世界精确操作的问题，提出了一种名为“空间强迫”的隐式空间表示对齐方法。该方法不依赖显式3D输入或深度估计器，而是通过将VLA模型的中间视觉嵌入与预训练3D基础模型的几何表示进行对齐，迫使模型学习丰富的空间表征。实验表明，该方法在模拟和真实环境中均达到最优性能，训练速度提升最高达3.8倍，并在多种机器人任务中提高了数据效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.12276" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>