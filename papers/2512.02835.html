<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">â† è¿”å›åˆ—è¡¨</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.02835" target="_blank" rel="noreferrer">2512.02835</a></span>
        <span>ä½œè€…: Li, Yifan, Yin, Yingda, Zhu, Lingting, Chen, Weikai, Qian, Shengju, Wang, Xin, Fu, Yanwei</span>
        <span>æ—¥æœŸ: 2025/12/02</span>
      </div>
      <section class="detail-body">
        <h2>ğŸ“ è¯¦ç»†è§£è¯»</h2>
        <p>tags and the final answer between <answer> and </answer> tags. In addition, the first-turn output ğ² ğŸ \mathbf{y_{1}} must include the keyframe index k k and object description ğ \mathbf{d} in JSON format, while the second-turn output ğ² ğŸ \mathbf{y_{2}} must provide the bounding box B k B_{k} in JSON. Based on the degree to which the output o o satisfies these rules, the format reward r f r_{f} is assigned a value in [ 0 , 1 ] [0,1] . â€¢ Temporal Reward r t r_{t} : The keyframe I k I_{k} selected by â„± \mathcal{F} in ğ² ğŸ \mathbf{y_{1}} critically affects subsequent spatial grounding. Beyond merely containing the target object, we encourage selecting frames where the object is clearly visible, minimally occluded, and sufficiently large. We experiment with several temporal reward choices and finally choose the normalized area of the ground-truth bounding box at I k I_{k} : r t = ğŸ™ ( â€– m k âˆ— â€– 1 &gt; 0 ) â‹… ğ’® â€‹ ( m k âˆ— ) âˆ’ min t â¡ ğ’® â€‹ ( m t âˆ— ) max t â¡ ğ’® â€‹ ( m t âˆ— ) âˆ’ min t â¡ ğ’® â€‹ ( m t âˆ— ) . r_{t}=\mathds{1}<em>{(||m</em>{k}^{<em>}||<em>{1}&gt;0)}\cdot\frac{{\mathcal{S}}(m</em>{k}^{</em>})-\min_{t}{\mathcal{S}}(m_{t}^{<em>})}{\max_{t}{\mathcal{S}}(m_{t}^{</em>})-\min_{t}{\mathcal{S}}(m_{t}^{<em>})}. (7) where ğ’® â€‹ ( m t âˆ— ) \mathcal{S}(m^{</em>}<em>{t}) denotes the pixel area of the ground truth bbox at frame I t I</em>{t} and ğŸ™ ( â‹… ) \mathds{1}<em>{(\cdot)} denotes the indicator function. See Tab. 7 for ablations. â€¢ Spatial Reward r s r</em>{s} : This reward measures final detection quality of the predicted B k B_{k} . Following prior works [ liu2025seg , liu2025visionreasoner ] , we use Intersection-over-Union (IoU) between the predicted and ground-truth boxes. If IoU &gt; 0.5 \text{IoU}&gt;0.5 , the prediction is considered correct and r s = 1 r_{s}=1 ; otherwise r s = 0 r_{s}=0 . The total reward for an output o o combines these components with status flags S 1 S_{1} and S 2 S_{2} indicating whether each step succeeds: r = r f + ğŸ™ ( S 1 = succ ) â€‹ r t + ğŸ™ ( S 1 = succ &amp; S 2 = succ ) â€‹ r s . r=r_{f}+\mathds{1}<em>{(S</em>{1}=\texttt{succ})}r_{t}+\mathds{1}<em>{(S</em>{1}=\texttt{succ}\ &amp;\ S_{2}=\texttt{succ})}r_{s}. (8) Objective. We adopt Group Relative Policy Optimization (GRPO) [ shao2024deepseekmath ] , a critic-free variant of Proximal Policy Optimization (PPO) [ schulman2017proximal ] tailored for sequence models. Given an input query q = { V , ğ± } q={V,\mathbf{x}} , GRPO samples a group of n n candidate outputs { o i } i = 1 n {o_{i}}<em>{i=1}^{n} from the current policy Ï€ old \pi</em>{\text{old}} , evaluates their rewards { r i } {r_{i}} , and computes a normalized within-group advantage: A i = r i âˆ’ mean â€‹ ( r 1 , â€¦ , r n ) std â€‹ ( r 1 , â€¦ , r n ) . A_{i}=\frac{r_{i}-\text{mean}(r_{1},\ldots,r_{n})}{\text{std}(r_{1},\ldots,r_{n})}. (9) Since we use on-policy updates ( Ï€ old = Ï€ Î¸ \pi_{\text{old}}=\pi_{\theta} ) in practice, importance sampling ratios are remains at one. The policy loss, combined with KL regularization to a reference model Ï€ ref \pi_{\text{ref}} , yields the final training objective: â„’ â€‹ ( Î¸ ) = ğ”¼ q âˆ¼ P ( Q ) , { o i } âˆ¼ Ï€ Î¸ ( â‹… | q ) â€‹ [ 1 n â€‹ âˆ‘ i = 1 n ( A i âˆ’ Î² â€‹ ğ”» KL â€‹ ( Ï€ Î¸ âˆ¥ Ï€ ref ) ) ] . \mathcal{L}(\theta)=\mathbb{E}<em>{q\sim P(Q),,{o</em>{i}}\sim\pi_{\theta}(\cdot|q)}\left[\frac{1}{n}\sum_{i=1}^{n}\Big(A_{i}-\beta,\mathbb{D}<em>{\text{KL}}(\pi</em>{\theta},|,\pi_{\text{ref}})\Big)\right]. (10) 4 Experiments 4.1 Experiment Settings Table 1: Reasoning video object segmentation performance comparison on ReasonVOS [ bai2024one ] dataset. Method ğ’¥ \mathcal{J} â„± \mathcal{F} ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} MTTR [ botach2022end ] [CVPR&#39;22] 29.1 33.1 31.1 ReferFormer [ wu2022language ] [CVPR&#39;22] 30.2 35.6 32.9 SOC [ luo2023soc ] [NeurIPS&#39;24] 33.3 38.5 35.9 OnlineRefer [ wu2023onlinerefer ] [CVPR&#39;23] 34.6 42.9 38.7 SgMg [ miao2023spectrum ] [ICCV&#39;23] 33.7 38.7 36.2 LISA [ lai2024lisa ] [CVPR&#39;24] 29.1 33.1 31.1 VideoLISA [ bai2024one ] [NeurIPS&#39;24] 45.1 49.9 47.5 GLUS [ lin2025glus ] [CVPR&#39;25] 47.5 52.4 49.9 RGA-3B [ wang2025object ] [ICCV&#39;25] 49.1 54.3 51.7 RGA-7B [ wang2025object ] [ICCV&#39;25] 51.3 56.0 53.6 CoT-RVS-online-7B [ CoTRVS ] [arXiv&#39;25] 49.5 54.5 52.0 CoT-RVS-offline-13B [ CoTRVS ] [arXiv&#39;25] 47.5 54.0 50.7 ReVSeg-7B (Ours) - 61.8 67.7 64.8 Table 2 : Reasoning video object segmentation performance comparison on ReVOS [ yan2024visa ] dataset. Method Type referring reasoning overall ğ’¥ \mathcal{J} â„± \mathcal{F} ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} ğ’¥ \mathcal{J} â„± \mathcal{F} ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} ğ’¥ \mathcal{J} â„± \mathcal{F} ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} MTTR [ botach2022end ] [CVPR&#39;22] Segmentation Specialists 29.8 30.2 30.0 20.4 21.5 21.0 25.1 25.9 25.5 ReferFormer [ wu2022language ] [CVPR&#39;22] 31.2 34.3 32.7 21.3 25.6 23.4 26.2 29.9 28.1 LMPM [ ding2023mevis ] [ICCV&#39;23] 29.0 39.1 34.1 13.3 24.3 18.8 21.2 31.7 26.4 LLaMA-VID [ li2024llama ] + LMPM [ ding2023mevis ] [ECCV&#39;24] 29.0 39.1 34.1 12.8 23.7 18.2 20.9 31.4 26.1 LISA-7B [ lai2024lisa ] [CVPR&#39;24] VLM-Based w/ Latent Tokens 44.3 47.1 45.7 33.8 38.4 36.1 39.1 42.7 40.9 LISA-13B [ lai2024lisa ] [CVPR&#39;24] 45.2 47.9 46.6 34.3 39.1 36.7 39.8 43.5 41.6 TrackGPT(IT)-7B [ zhu2023tracking ] [arXiv&#39;24] 46.7 49.7 48.2 36.8 41.2 39.0 41.8 45.5 43.6 TrackGPT(IT)-13B [ zhu2023tracking ] [arXiv&#39;24] 48.3 50.6 49.5 38.1 42.9 40.5 43.2 46.8 45.0 VISA-7B [ yan2024visa ] [ECCV&#39;24] 51.1 54.7 52.9 36.7 41.7 39.2 43.9 48.2 46.1 VISA-13B [ yan2024visa ] [ECCV&#39;24] 52.3 55.8 54.1 38.3 43.5 40.9 45.3 49.7 47.5 VISA(IT)-7B [ yan2024visa ] [ECCV&#39;24] 49.2 52.6 50.9 40.6 45.4 43.0 44.9 49.0 46.9 VISA(IT)-13B [ yan2024visa ] [ECCV&#39;24] 55.6 59.1 57.4 42.0 46.7 44.3 48.8 52.9 50.9 VRS-HQ-7B [ gong2025devil ] [CVPR&#39;25] 59.8 64.5 62.1 53.5 58.7 56.1 56.6 61.6 59.1 GLUS [ lin2025glus ] [CVPR&#39;25] 58.3 56.0 60.7 48.8 53.9 51.4 - - - HyperSeg [ wei2025hyperseg ] [CVPR&#39;25] 56.0 60.9 58.5 50.2 55.8 53.0 53.1 58.4 55.7 RGA3-3B [ wang2025object ] [ICCV&#39;25] 57.6 61.0 59.3 50.6 55.0 52.8 54.1 58.0 56.1 RGA3-7B [ wang2025object ] [ICCV&#39;25] 58.7 62.3 60.5 53.1 57.7 55.4 55.9 60.0 58.0 ViLLa [ zheng2025villa ] [ICCV&#39;25] - - - - - - 54.9 59.1 57.0 InstructSeg [ wei2025instructseg ] [ICCV&#39;25] 54.8 59.2 57.0 49.2 54.7 51.9 52.0 56.9 54.5 CoT-RVS-online-7B [ CoTRVS ] [arXiv&#39;25] VLM-Based w/ Explicit - - - - - - 43.5 48.8 46.2 CoT-RVS-offline-12B [ CoTRVS ] [arXiv&#39;25] - - - - - - 43.4 50.9 47.1 ReVSeg-7B (Ours) - Reasoning 63.3 68.1 65.7 55.4 61.8 58.6 59.3 65.0 62.1 Training Datasets. For our efficient RL post-training, we rely solely on the video object segmentation (VOS) data, in contrast to previous works [ bai2024one , yan2024visa , gong2025devil , wei2025hyperseg , wei2025instructseg ] that jointly fine-tunes on large, heterogeneous corpora spanning video segmentation, image segmentation, and VQA datasets. Specifically, we curate training data from five benchmarks: Ref-YouTube-VOS [ seo2020urvos ] , MeViS [ ding2023mevis ] , Ref-DAVIS17 [ khoreva2018video ] , ReVOS [ yan2024visa ] and LV-VIS [ wang2023towards ] . For each annotated sequence, we first convert per-frame masks to bounding boxes, which serve as ground-truth signals for post-training rewards. To ensure label quality, we run SAM2 [ ravi2024sam ] on every frame conditioned on its ground-truth box to obtain predicted masks, compute IoU against the annotated masks, and discard all queries from any video whose mean IoU falls below 0.6. This filtering yields approximately 67k data pairs. Benchmarks. We evaluate on five standard VOS benchmarks: two reasoning datasets including ReVOS [ yan2024visa ] and ReasonVOS [ bai2024one ] and three referring datasets including Ref-DAVIS17 [ khoreva2018video ] , Ref-YouTube-VOS [ seo2020urvos ] , MeViS [ ding2023mevis ] . Notably, ReasonVOS has no training split, thus its evaluation is zero-shot, providing a clearer measure of the modelâ€™s generalization ability. Baselines. We benchmark against three families of methods to contextualize our gains. (1) Segmentation Specialists : strong VOS/Ref-VOS systems [ botach2022end , wu2022language , ding2023mevis , li2024llama , seo2020urvos , ding2022language , wu2023onlinerefer , he2024decoupling , miao2023spectrum , luo2023soc ] trained with dense supervision, optimized for mask quality and temporal consistency. (2) VLM-Based with Latent Tokens Methods : methods [ bai2024one , yan2024visa , gong2025devil , lin2025glus , wei2025hyperseg , wei2025instructseg , zheng2025villa , wang2025object ] that fine-tune VLMs to emit task-specific control tokens or logits that drive a downstream mask head, which are the current mainstream for reasoning VOS. (3) VLM-Based with Explicit Reasoning Methods : methods that perform reasoning to explicitly ground targets via boxes/masks, an under-explored paradigm where CoT-RVS [ CoTRVS ] and our method fall. We report results across all three to isolate the advantage of our proposed ReVSeg. Evaluation Metrics. Following previous works [ bai2024one , yan2024visa , gong2025devil , wei2025hyperseg , wei2025instructseg , zheng2025villa , lin2025glus , CoTRVS ] on reasoning video object segmentation, we report region similarity ( ğ’¥ \mathcal{J} ), contour accuracy ( â„± \mathcal{F} ) and their mean ( ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} ) as the primary video-level metrics. Implementation Details. We adopt Qwen2.5-VL-7B [ bai2025qwen2 ] as the default reasoning model â„± \mathcal{F} and SAM2 (Hiera-L) [ ravi2024sam ] as the default video tracker model ğ’¯ \mathcal{T} . For post-training â„± \mathcal{F} with GRPO, each optimizer step processes 128 input data, and we sample n = 8 n=8 rollouts per prompt, yielding an effective batch of 1024 sequences per optimizer step. The learning rate is set to 1 â€‹ e âˆ’ 6 1e-6 , and the KL regularization coefficient Î² = 1 â€‹ e âˆ’ 3 \beta=1e-3 . For each video, we uniformly sample 16 frames as input to â„± \mathcal{F} . All input frames are resized to 448 Ã— 448 448\times 448 before the first round generation. In the second round, the selected keyframe I k I_{k} is resized to 840 Ã— 840 840\times 840 for spatial grounding. The tracker ğ’¯ \mathcal{T} operates on the full video at its original resolution. 4.2 Experimental Results Reasoning Video Object Segmentation. We first evaluate on reasoning VOS benchmarks â€“ ReasonVOS dataset in Tab. 1 and ReVOS dataset in Tab. 2 . On ReasonVOS, ReVSeg-7B achieves a decisive margin over the previous state-of-the-art (SOTA) method RGA-7B [ wang2025object ] , improving ğ’¥ \mathcal{J} by +10.5 points, â„± \mathcal{F} by +11.7 points, and ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} by +11.2 points. The significant performance improvement demonstrates the effectiveness of ReVSeg with the proposed framework. Furthermore, given the zero-shot nature of ReasonVOS, these gains highlight our strong generalization and robustness under challenging open-world queries. On ReVOS, we conduct a comprehensive comparison across nine metrics. Our ReVSeg-7B consistently ranks first, surpassing previous SOTAs, including several larger parameter systems, by a obvious margin. The across-the-board improvements on these reasoning VOS benchmarks substantiate the effectiveness of our explicit reasoning chain and the efficiency of the proposed training recipe. Referring Video Object Segmentation. As previous practice [ yan2024visa , bai2024one , wang2025object , zheng2025villa , ding2023mevis , khoreva2018video , seo2020urvos ] , we report the experiment results on three Ref-VOS benchmarks, i.e., Ref-YouTube-VOS, Ref-DAVIS17, and MeViS. As in Tab. 4 , consistently, our ReVSeg-7B sets a new state of the art, improving ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} by +2.7 points on Ref-YouTube-VOS, +4.8 points on Ref-DAVIS17, and +8.5 points on MeViS against the previous SOTAs. Notably, MeViS is a motion-guided benchmark and is regarded as the most challenging Ref-VOS benchmark in GLUS [ lin2025glus ] . Our substantial gains on MeViS indicate strong adaptability on complex video scenarios with intricate motion patterns. Although referring queries require less semantic reasoning than reasoning queries, our performance gains are still obvious and consistent. These results indicate ReVSeg offers stronger cross-modal video understanding, better temporal aggregation, and more accurate target object detection than prior art. Table 3 : Zero-shot reasoning image segmentation results on ReasonSeg [ lai2024lisa ] dataset. Method test val gIoU cIoU gIoU cIoU Qwen2.5VL-7B 55.9 44.3 59.5 54.0 ReVSeg-7B (Ours) 59.7 47.4 63.7 59.9 Table 4 : Video referring segmentation results on Ref-Youtube-VOS [ seo2020urvos ] , Ref-DAVIS17 [ khoreva2018video ] , MeViS [ ding2023mevis ] datasets. Method Type Ref-YouTube-VOS Ref-DAVIS17 MeViS ğ’¥ \mathcal{J} â„± \mathcal{F} ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} ğ’¥ \mathcal{J} â„± \mathcal{F} ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} ğ’¥ \mathcal{J} â„± \mathcal{F} ğ’¥ &amp; â„± \mathcal{J}&amp;\mathcal{F} URVOS [ seo2020urvos ] [ECCV&#39;20] Segmentation Specialists 45.3 49.2 47.2 47.3 56.0 51.6 25.7 29.9 27.8 LBDT [ ding2022language ] [CVPR&#39;22] 48.2 50.6 49.4 - - 54.1 27.8 30.8 29.3 MTTR [ botach2022end ] [CVPR&#39;22] 54.0 56.6 55.3 - - - 28.8 31.2 30.0 LMPM [ ding2023mevis ] [ICCV&#39;23] - - - - - - 34.2 40.2 37.2 ReferFormer [ wu2022language ] [CVPR&#39;22] 61.3 64.6 62.9 58.1 64.1 61.1 29.8 32.2 31.0 OnlineRefer [ wu2023onlinerefer ] [CVPR&#39;23] 61.6 65.5 63.5 61.6 67.7 64.8 - - - DsHmp [ he2024decoupling ] [CVPR&#39;24] 65.0 69.1 67.1 61.7 68.1 64.9 43.0 49.8 46.4 LISA-7B [ lai2024lisa ] [CVPR&#39;24] VLM-Based w/ Latent Tokens 53.4 54.3 53.9 62.2 67.3 64.8 35.1 39.4 37.2 LISA-13B [ lai2024lisa ] [CVPR&#39;24] 54.0 54.8 54.5 63.2 68.8 66.9 35.8 40.0 37.9 TrackGPT-7B [ zhu2023tracking ] [arXiv&#39;24] 55.3 57.4 56.4 59.4 67.0 63.2 37.6 42.6 40.1 TrackGPT-13B [ zhu2023tracking ] [arXiv&#39;24] 58.1 60.8 59.5 62.7 70.4 66.5 39.2 43.1 41.2 VISA-7B [ yan2024visa ] [ECCV&#39;24] 59.8 63.2 61.5 66.3 72.5 69.4 40.7 46</think>## ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº<br>æ¨ç†ä¸­å¿ƒåŒ–çš„è§†é¢‘ç›®æ ‡åˆ†å‰²ä»»åŠ¡å¤æ‚åº¦é«˜ï¼ŒæŸ¥è¯¢å¾€å¾€æ¶‰åŠåŠ¨æ€ã€å› æœå’Œæ—¶åºäº¤äº’ï¼Œè€Œéé™æ€å¤–è§‚ã€‚ç„¶è€Œï¼Œç°æœ‰ä¸»æµæ–¹æ³•ï¼ˆå¦‚LISAã€VISAã€VideoLISAã€GLUSã€RGAç­‰ï¼‰æ™®éå°†è¿™äº›å› ç´ ç®€åŒ–ä¸ºåŸºäºæ½œåœ¨åµŒå…¥çš„æ¨ç†ï¼Œå³é€šè¿‡å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥ç”Ÿæˆä¸€ä¸ªç‰¹æ®Šçš„æ½œåœ¨åˆ†å‰²æ ‡è®°ï¼ˆå¦‚<SEG>ï¼‰ï¼Œå¹¶ç›´æ¥è§£ç ä¸ºæ©ç è¾“å‡ºã€‚è¿™ç§å•æ­¥æ½œåœ¨é¢„æµ‹å°†å¤šæ­¥æ¨ç†è¿‡ç¨‹ï¼ˆè§£é‡ŠæŠ½è±¡æŒ‡ä»¤ã€è¯†åˆ«å€™é€‰å¯¹è±¡ã€æ—¶ç©ºå®šä½ï¼‰å‹ç¼©ä¸ºä¸€ä¸ªç®€å•ä¸”ä¸é€æ˜çš„ç»“è®ºï¼Œå¸¦æ¥äº†ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼šå¯è§£é‡Šæ€§å—é™ã€è¿«ä½¿VLMè¿›å…¥éåŸç”Ÿè¾“å‡ºç©ºé—´å¯¼è‡´åˆ†å¸ƒåç§»ï¼Œä»¥åŠéœ€è¦å¤§é‡æœ‰ç›‘ç£å¾®è°ƒæ•°æ®ã€‚</p>
<p>æœ¬æ–‡é’ˆå¯¹ä¸Šè¿°ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„è§†è§’ï¼šå°†æ¨ç†è§†é¢‘åˆ†å‰²é‡æ–°å®šä¹‰ä¸ºåœ¨é¢„è®­ç»ƒVLMåŸç”Ÿæ¥å£ä¸­æ‰§è¡Œçš„æ˜¾å¼ã€åºåˆ—åŒ–çš„æ¨ç†é“¾ã€‚æ ¸å¿ƒæ€è·¯æ˜¯ï¼šå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºä¸VLMåŸç”Ÿèƒ½åŠ›å¯¹é½çš„ä¸‰ä¸ªæ˜¾å¼æ“ä½œï¼ˆè¯­ä¹‰è§£é‡Šã€æ—¶åºè¯æ®é€‰æ‹©ã€ç©ºé—´å®šä½ï¼‰ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–è¿™ä¸ªå¤šæ­¥æ¨ç†é“¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç»“æœé©±åŠ¨çš„ä¿¡å·è‡ªæˆ‘ä¼˜åŒ–å†³ç­–è´¨é‡ã€‚</p>
<h2 id="æ–¹æ³•è¯¦è§£">æ–¹æ³•è¯¦è§£</h2>
<p>ReVSegçš„æ•´ä½“æ¡†æ¶æ˜¯ä¸€ä¸ªä¸¤è½®å¯¹è¯çš„æ¨ç†é“¾ï¼Œç”±ä¸€ä¸ªç»Ÿä¸€çš„VLMæ‰§è¡Œã€‚è¾“å…¥æ˜¯è§†é¢‘åºåˆ—Vå’Œè‡ªç„¶è¯­è¨€æŸ¥è¯¢xï¼Œè¾“å‡ºæ˜¯è§†é¢‘ä¸­æ¯ä¸€å¸§çš„äºŒè¿›åˆ¶æ©ç åºåˆ—Mã€‚</p>
<p><img src="https://arxiv.org/html/2512.02835v1/x2.png" alt="æ–¹æ³•æ¡†æ¶"></p>
<blockquote>
<p><strong>å›¾2</strong>ï¼šReVSegæ–¹æ³•æ€»è§ˆã€‚æ¨¡å‹åœ¨è¾“å…¥è§†é¢‘å’ŒæŸ¥è¯¢ä¸Šè¿è¡Œä¸€ä¸ªä¸¤è½®æ¨ç†é“¾ã€‚ç¬¬ä¸€è½®åˆ†æåœºæ™¯å¹¶é€‰æ‹©ä¸€ä¸ªä¿¡æ¯ä¸°å¯Œçš„å…³é”®å¸§ï¼ŒåŒæ—¶ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ç›®æ ‡å¯¹è±¡æè¿°ã€‚ç¬¬äºŒè½®åœ¨è¯¥å…³é”®å¸§ä¸Šå®šä½ç›®æ ‡ï¼Œé¢„æµ‹ä¸€ä¸ªè¾¹ç•Œæ¡†ã€‚å…³é”®å¸§-è¾¹ç•Œæ¡†å¯¹ç”¨äºæ¡ä»¶åŒ–ä¸€ä¸ªç°æˆçš„è§†é¢‘è·Ÿè¸ªå™¨ï¼Œä»¥ç”Ÿæˆå®Œæ•´çš„åˆ†å‰²åºåˆ—ã€‚å¥–åŠ±ç®¡ç†å™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸ºVLMçš„åè®­ç»ƒæä¾›ä¿¡å·ï¼Œä»¥æ”¹è¿›å…³é”®å¸§é€‰æ‹©ã€å®šä½ç²¾åº¦å’Œæ•´ä½“é²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>æ ¸å¿ƒæ¨¡å—ä¸€ï¼šåˆ†è§£çš„æ¨ç†é“¾ç”Ÿæˆ</strong><br>æ¨ç†é“¾åˆ†ä¸ºä¸¤ä¸ªæ˜ç¡®çš„å›åˆï¼š</p>
<ol>
<li><strong>ç¬¬ä¸€è½®æ¨ç†</strong>ï¼šVLM â„± æ¥æ”¶è§†é¢‘-æŸ¥è¯¢å¯¹ (V, x)ï¼Œç”Ÿæˆæ–‡æœ¬å“åº” yâ‚ã€‚æ­¤æ­¥éª¤æ‰§è¡Œè§†é¢‘ç†è§£å’Œæ—¶åºå®šä½ï¼šè§£é‡ŠæŸ¥è¯¢ã€åˆ†æè§†é¢‘å†…å®¹ã€æ¨æ–­ç›®æ ‡å®ä½“ã€è¯†åˆ«å…¶åœ¨åºåˆ—ä¸­å‡ºç°çš„æ—¶é—´ç‚¹ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ã€åŸºäºå…³é”®å¸§çš„ç›®æ ‡å¯¹è±¡æè¿°ã€‚ä¸€ä¸ªè§£æå™¨ ğ’¢ éšåä» yâ‚ ä¸­æå–å…³é”®å¸§ç´¢å¼• kã€ç›®æ ‡æè¿° d ä»¥åŠä¸€ä¸ªè¡¨ç¤ºæå–æˆåŠŸä¸å¦çš„çŠ¶æ€æ ‡å¿— Sâ‚ã€‚</li>
<li><strong>ç¬¬äºŒè½®æ¨ç†</strong>ï¼šå¦‚æœ Sâ‚ ä¸ºæˆåŠŸï¼Œåˆ™è¿›å…¥ç¬¬äºŒé˜¶æ®µã€‚VLM â„± æ¥æ”¶é€‰å®šçš„å…³é”®å¸§ I_kã€ç›®æ ‡æè¿° d ä»¥åŠç¬¬ä¸€è½®çš„ç”Ÿæˆå†å² (V, x, yâ‚)ï¼Œç”Ÿæˆå“åº” yâ‚‚ã€‚æ­¤æ­¥éª¤æ‰§è¡Œç©ºé—´å®šä½ï¼šåœ¨å…³é”®å¸§ I_k ä¸Šå®šä½ç›®æ ‡å¯¹è±¡ï¼Œè¾“å‡ºä¸€ä¸ªè¾¹ç•Œæ¡† B_kã€‚è§£æå™¨ ğ’¢ ä» yâ‚‚ ä¸­æå–è¾¹ç•Œæ¡†å’ŒçŠ¶æ€æ ‡å¿— Sâ‚‚ã€‚</li>
</ol>
<p>æœ€ç»ˆï¼Œé€šè¿‡ä¸€ä¸ªç°æˆçš„è§†é¢‘è·Ÿè¸ªå™¨ï¼ˆå¦‚SAM2ï¼‰ï¼Œåˆ©ç”¨å…³é”®å¸§ä¸Šçš„è¾¹ç•Œæ¡† B_k ä½œä¸ºåˆå§‹åŒ–ï¼Œç”Ÿæˆæ•´ä¸ªè§†é¢‘åºåˆ—çš„åˆ†å‰²æ©ç ã€‚æ•´ä¸ªä¸¤è½®å¯¹è¯åœ¨åŒä¸€ä¸ªVLMä¸­è¿›è¡Œï¼Œç¡®ä¿äº†æ—©æœŸæ¨ç†å»ºç«‹çš„è¯­ä¹‰ä¸Šä¸‹æ–‡èƒ½æ— ç¼ä¼ æ’­åˆ°åç»­æ­¥éª¤ã€‚</p>
<p><strong>æ ¸å¿ƒæ¨¡å—äºŒï¼šåŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–</strong><br>ç”±äºä¸­é—´å†³ç­–çš„æ­£ç¡®æ€§éš¾ä»¥æ ‡æ³¨ï¼Œç›‘ç£å­¦ä¹ éš¾ä»¥ä¼˜åŒ–æ¨ç†é“¾ã€‚å› æ­¤ï¼Œæœ¬æ–‡é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œå°†æ¨ç†VOSè§†ä¸ºä¸€ä¸ªè¡Œä¸ºç­–ç•¥ï¼Œä»…å½“å®Œæ•´çš„å†³ç­–è½¨è¿¹å¯¼è‡´æ­£ç¡®çš„åˆ†å‰²ç»“æœæ—¶æ‰ç»™äºˆå¥–åŠ±ã€‚å…·ä½“é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€ä»·å€¼å‡½æ•°è¯„è®ºå®¶çš„PPOå˜ä½“ï¼Œé€‚ç”¨äºåºåˆ—æ¨¡å‹ã€‚</p>
<p>å¥–åŠ±å»ºæ¨¡æ˜¯å…³é”®ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„å¥–åŠ±ç³»ç»Ÿï¼ŒåŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š</p>
<ul>
<li><strong>æ ¼å¼å¥–åŠ± r_f</strong>ï¼šç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®ï¼ˆå¦‚ä½¿ç”¨ç‰¹å®šæ ‡ç­¾åŒ…è£¹æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºå…³é”®å¸§ç´¢å¼•ã€æè¿°å’Œè¾¹ç•Œæ¡†ï¼‰ï¼Œå€¼åŸŸä¸º[0,1]ã€‚</li>
<li><strong>æ—¶åºå¥–åŠ± r_t</strong>ï¼šé¼“åŠ±é€‰æ‹©ç›®æ ‡å¯¹è±¡æ¸…æ™°å¯è§ã€é®æŒ¡æœ€å°ä¸”å°ºå¯¸è¶³å¤Ÿå¤§çš„å…³é”®å¸§ã€‚å…·ä½“é‡‡ç”¨å…³é”®å¸§ä¸ŠçœŸå®è¾¹ç•Œæ¡†é¢ç§¯çš„å½’ä¸€åŒ–å€¼ã€‚</li>
<li><strong>ç©ºé—´å¥–åŠ± r_s</strong>ï¼šè¡¡é‡é¢„æµ‹è¾¹ç•Œæ¡† B_k çš„æœ€ç»ˆæ£€æµ‹è´¨é‡ã€‚ä½¿ç”¨é¢„æµ‹æ¡†ä¸çœŸå®æ¡†çš„äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ï¼Œè‹¥IoU&gt;0.5åˆ™ r_s=1ï¼Œå¦åˆ™ä¸º0ã€‚</li>
</ul>
<p>æ€»å¥–åŠ± r ç»“åˆäº†è¿™ä¸‰ä¸ªéƒ¨åˆ†ï¼Œå¹¶è€ƒè™‘äº†ä¸¤ä¸ªå›åˆçš„çŠ¶æ€æ ‡å¿—ï¼šr = r_f + ğŸ™(Sâ‚=succ) * r_t + ğŸ™(Sâ‚=succ &amp; Sâ‚‚=succ) * r_sã€‚</p>
<p>è®­ç»ƒç›®æ ‡æ˜¯åœ¨GRPOæ¡†æ¶ä¸‹æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ï¼ŒåŒæ—¶é€šè¿‡KLæ•£åº¦æ­£åˆ™åŒ–çº¦æŸç­–ç•¥æ¨¡å‹ Ï€_Î¸ ä¸å‚è€ƒæ¨¡å‹ Ï€_ref çš„åç¦»ï¼Œä»¥é˜²æ­¢ç­–ç•¥è¿‡åº¦åç¦»ã€‚</p>
<p><strong>ä¸ç°æœ‰æ–¹æ³•çš„åˆ›æ–°ç‚¹</strong></p>
<ol>
<li><strong>æ˜¾å¼åˆ†è§£ vs. éšå¼æ½œåœ¨é¢„æµ‹</strong>ï¼šä¸ä¸»æµæ–¹æ³•å°†æ¨ç†æŠ˜å ä¸ºå•æ­¥æ½œåœ¨æ ‡è®°ä¸åŒï¼ŒReVSegå°†ä»»åŠ¡æ˜¾å¼åˆ†è§£ä¸ºå¤šæ­¥ã€å¯è§£é‡Šçš„æ¨ç†é“¾ã€‚</li>
<li><strong>ç»Ÿä¸€VLM vs. åˆ†ç¦»æ¨¡å—</strong>ï¼šä¸åŒæ ·é‡‡ç”¨æ˜¾å¼æ¨ç†ä½†ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹VLMç³»ç»Ÿçš„CoT-RVSç›¸æ¯”ï¼ŒReVSegåœ¨å•ä¸ªVLMå†…è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œä¿æŒäº†ä¿¡æ¯æµçš„ç»Ÿä¸€å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§ã€‚</li>
<li><strong>RLä¼˜åŒ–æ¨ç†é“¾ vs. ç›‘ç£å¾®è°ƒ</strong>ï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–æ¨ç†é“¾çš„å†³ç­–è´¨é‡ï¼Œå…‹æœäº†ä¸­é—´å†³ç­–ç¼ºä¹ç›‘ç£ä¿¡å·çš„éš¾é¢˜ï¼Œå®ç°äº†åœ¨ä»…æœ‰ç¨€ç–ç»“æœä¿¡å·ä¸‹çš„è‡ªæˆ‘æ”¹è¿›ã€‚</li>
</ol>
<h2 id="å®éªŒä¸ç»“æœ">å®éªŒä¸ç»“æœ</h2>
<p><strong>å®éªŒè®¾ç½®</strong>ï¼š</p>
<ul>
<li><strong>è®­ç»ƒæ•°æ®</strong>ï¼šä»…ä½¿ç”¨è§†é¢‘ç›®æ ‡åˆ†å‰²æ•°æ®ï¼Œä»Ref-YouTube-VOSã€MeViSã€Ref-DAVIS17ã€ReVOSå’ŒLV-VISäº”ä¸ªåŸºå‡†ä¸­ç­›é€‰å‡ºçº¦67kä¸ªæ•°æ®å¯¹è¿›è¡ŒRLåè®­ç»ƒã€‚</li>
<li><strong>è¯„æµ‹åŸºå‡†</strong>ï¼šäº”ä¸ªæ ‡å‡†VOSåŸºå‡†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ¨ç†æ•°æ®é›†ï¼ˆReasonVOSï¼ŒReVOSï¼‰å’Œä¸‰ä¸ªæŒ‡ä»£æ•°æ®é›†ï¼ˆRef-YouTube-VOSï¼ŒRef-DAVIS17ï¼ŒMeViSï¼‰ã€‚ReasonVOSæ— è®­ç»ƒé›†ï¼Œç”¨äºé›¶æ ·æœ¬è¯„ä¼°ã€‚</li>
<li><strong>å¯¹æ¯”æ–¹æ³•</strong>ï¼šä¸ä¸‰ç±»æ–¹æ³•å¯¹æ¯”ï¼š1) åˆ†å‰²ä¸“å®¶ï¼ˆå¦‚MTTR, ReferFormerï¼‰ï¼›2) åŸºäºæ½œåœ¨æ ‡è®°çš„VLMæ–¹æ³•ï¼ˆå¦‚LISA, VISA, GLUS, RGAï¼‰ï¼›3) åŸºäºæ˜¾å¼æ¨ç†çš„VLMæ–¹æ³•ï¼ˆCoT-RVSï¼‰ã€‚</li>
<li><strong>è¯„ä¼°æŒ‡æ ‡</strong>ï¼šåŒºåŸŸç›¸ä¼¼åº¦ ğ’¥ã€è½®å»“ç²¾åº¦ â„± åŠå…¶å‡å€¼ ğ’¥&amp;â„±ã€‚</li>
<li><strong>å®ç°ç»†èŠ‚</strong>ï¼šé»˜è®¤æ¨ç†æ¨¡å‹ â„± ä¸ºQwen2.5-VL-7Bï¼Œè·Ÿè¸ªå™¨ä¸ºSAM2 (Hiera-L)ã€‚ä½¿ç”¨GRPOè¿›è¡Œåè®­ç»ƒï¼Œå­¦ä¹ ç‡1e-6ï¼ŒKLç³»æ•°Î²=1e-3ã€‚</li>
</ul>
<p><strong>å…³é”®å®éªŒç»“æœ</strong>ï¼š</p>
<p><img src="https://arxiv.org/html/2512.02835v1/x1.png" alt="ç»“æœå›¾1"></p>
<blockquote>
<p><strong>å›¾1</strong>ï¼šï¼ˆå·¦ï¼‰é€šè¿‡æ˜¾å¼æ¨ç†é“¾ï¼ŒReVSegå¤„ç†å¤æ‚çš„ã€æŠ½è±¡çš„çœŸå®ä¸–ç•ŒæŸ¥è¯¢ã€‚ï¼ˆå³ï¼‰æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ²¡æœ‰åˆ†è§£çš„æ¨ç†é“¾ï¼ŒåŸºç¡€VLMåŠå…¶RLå˜ä½“åœ¨å¤æ‚æ¨ç†VOSä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼›è€ŒReVSegæ¡†æ¶å³ä½¿åœ¨RLå‰å·²è¡¨ç°å¼ºåŠ²ï¼ŒRLåè®­ç»ƒè¿›ä¸€æ­¥å¸¦æ¥æ˜¾è‘—æå‡ã€‚å›¾ä¸­æŠ¥å‘Šäº†åœ¨Ref-DAVIS17ï¼ˆåŸŸå†…ï¼‰å’ŒReasonVOSï¼ˆåŸŸå¤–ï¼‰æ•°æ®é›†ä¸Šçš„ ğ’¥&amp;â„± æŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>æ¨ç†VOSç»“æœ</strong>ï¼š<br>åœ¨ReasonVOSä¸Šï¼ˆè¡¨1ï¼‰ï¼ŒReVSeg-7Bå–å¾—äº†64.8çš„ ğ’¥&amp;â„± åˆ†æ•°ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„SOTAæ–¹æ³•RGA-7Bï¼ˆ53.6ï¼‰ï¼Œæå‡äº†11.2ä¸ªç™¾åˆ†ç‚¹ï¼ˆğ’¥ +10.5, â„± +11.7ï¼‰ã€‚è¿™è¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚<br>åœ¨ReVOSä¸Šï¼ˆè¡¨2ï¼‰ï¼ŒReVSeg-7Båœ¨æŒ‡ä»£ã€æ¨ç†å’Œæ€»ä½“å­é›†ä¸Šå‡å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ€»ä½“ ğ’¥&amp;â„± è¾¾åˆ°62.1ï¼Œè¶…è¶Šäº†åŒ…æ‹¬æ›´å¤§å‚æ•°æ¨¡å‹åœ¨å†…çš„æ‰€æœ‰å…ˆå‰æ–¹æ³•ã€‚</p>
<p><strong>æŒ‡ä»£VOSç»“æœ</strong>ï¼š<br>åœ¨ä¸‰ä¸ªæŒ‡ä»£æ•°æ®é›†ä¸Šï¼ˆè¡¨4ï¼‰ï¼ŒReVSeg-7BåŒæ ·è®¾ç«‹äº†æ–°çš„SOTAã€‚å…·ä½“åœ°ï¼Œåœ¨Ref-YouTube-VOSä¸Š ğ’¥&amp;â„± ä¸º67.1ï¼ˆæå‡+2.7ï¼‰ï¼Œåœ¨Ref-DAVIS17ä¸Šä¸º69.4ï¼ˆæå‡+4.8ï¼‰ï¼Œåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„è¿åŠ¨å¼•å¯¼æ•°æ®é›†MeViSä¸Šè¾¾åˆ°46.4ï¼ˆæå‡+8.5ï¼‰ã€‚è¿™è¡¨æ˜æ–¹æ³•åœ¨å¤æ‚è¿åŠ¨åœºæ™¯ä¸‹ä¹Ÿå…·æœ‰å¾ˆå¼ºçš„é€‚åº”æ€§ã€‚</p>
<p><strong>é›¶æ ·æœ¬æ¨ç†å›¾åƒåˆ†å‰²</strong>ï¼š<br>åœ¨ReasonSegå›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬è¯„ä¼°ï¼ˆè¡¨3ï¼‰ä¹Ÿæ˜¾ç¤ºï¼Œç»è¿‡VOSæ•°æ®RLåè®­ç»ƒçš„ReVSeg-7Bï¼Œç›¸æ¯”åŸå§‹çš„Qwen2.5VL-7Bæ¨¡å‹ï¼Œåœ¨æµ‹è¯•é›†å’ŒéªŒè¯é›†ä¸Šçš„å…¨å±€IoUå’Œç±»åˆ«IoUå‡æœ‰æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶æ¨ç†èƒ½åŠ›çš„æ³›åŒ–æ€§ã€‚</p>
<p><strong>æ¶ˆèå®éªŒ</strong>ï¼š<br>è®ºæ–‡åœ¨é™„å½•ï¼ˆè¡¨7ï¼‰ä¸­å¯¹æ—¶åºå¥–åŠ±çš„é€‰æ‹©è¿›è¡Œäº†æ¶ˆèå®éªŒï¼Œæœ€ç»ˆé€‰æ‹©äº†åŸºäºå…³é”®å¸§çœŸå®è¾¹ç•Œæ¡†å½’ä¸€åŒ–é¢ç§¯çš„å¥–åŠ±è®¾è®¡ã€‚å›¾1å³ä¾§çš„æ¶ˆèæ›²çº¿ç›´è§‚å±•ç¤ºäº†åˆ†è§£çš„æ¨ç†é“¾å’ŒRLåè®­ç»ƒå„è‡ªçš„é‡è¦è´¡çŒ®ï¼šåŸºç¡€VLMï¼ˆæ— åˆ†è§£ï¼‰æ€§èƒ½å¾ˆå·®ï¼›ä»…åŠ å…¥RLï¼ˆæ— åˆ†è§£ï¼‰æå‡æœ‰é™ï¼›è€Œå¼•å…¥åˆ†è§£çš„æ¨ç†é“¾åæ€§èƒ½å¤§å¹…æå‡ï¼Œå†ç»“åˆRLåè®­ç»ƒè¾¾åˆ°æœ€ä½³ã€‚</p>
<h2 id="æ€»ç»“ä¸å¯å‘">æ€»ç»“ä¸å¯å‘</h2>
<p><strong>æ ¸å¿ƒè´¡çŒ®</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†ReVSegï¼Œå°†æ¨ç†è§†é¢‘åˆ†å‰²ä»»åŠ¡åŸåˆ™æ€§åœ°åˆ†è§£ä¸ºä¸€ä¸ªåŸºäºVLMåŸç”Ÿèƒ½åŠ›æ„å»ºçš„æ˜¾å¼å¤šæ­¥æ¨ç†é“¾ï¼Œæå‡äº†å¯è§£é‡Šæ€§å¹¶é¿å…äº†åˆ†å¸ƒåç§»ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªæ–°é¢–çš„åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä¼˜åŒ–æ¨ç†é“¾æœ¬èº«ï¼Œä½¿æ¨¡å‹åœ¨ç¼ºä¹å¯†é›†ä¸­é—´ç›‘ç£çš„æƒ…å†µä¸‹è‡ªæˆ‘æ”¹è¿›å†³ç­–è´¨é‡ã€‚</li>
<li>åœ¨å¤šä¸ªæ ‡å‡†è§†é¢‘ç›®æ ‡åˆ†å‰²åŸºå‡†ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯å®¡è®¡çš„æ¨ç†è½¨è¿¹ã€‚</li>
</ol>
<p><strong>å±€é™æ€§</strong>ï¼š<br>è®ºæ–‡è‡ªèº«æåˆ°çš„å±€é™æ€§åŒ…æ‹¬ï¼šæ–¹æ³•ä¾èµ–äºä¸€ä¸ªç°æˆçš„è·Ÿè¸ªå™¨æ¥ç”Ÿæˆæœ€ç»ˆæ©ç ï¼Œå…¶æ€§èƒ½ä¸Šé™å—é™äºè¯¥è·Ÿè¸ªå™¨çš„èƒ½åŠ›ï¼›æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒéœ€è¦ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ã€‚</p>
<p><strong>å¯¹åç»­ç ”ç©¶çš„å¯ç¤º</strong>ï¼š</p>
<ol>
<li><strong>æ˜¾å¼æ¨ç†é“¾çš„è®¾è®¡èŒƒå¼</strong>ï¼šä¸ºå…¶ä»–éœ€è¦å¤æ‚æ—¶ç©ºæ¨ç†çš„è§†é¢‘ä»»åŠ¡ï¼ˆå¦‚è§†é¢‘é—®ç­”ã€åŠ¨ä½œå®šä½ï¼‰æä¾›äº†ä¸€ç§å¯å€Ÿé‰´çš„åˆ†è§£ä¸ç»„åˆæ€è·¯ã€‚</li>
<li><strong>å¼ºåŒ–å­¦ä¹ ç”¨äºä¼˜åŒ–ä¸­é—´å†³ç­–</strong>ï¼šå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç»“æœé©±åŠ¨çš„ç¨€ç–å¥–åŠ±ï¼Œé€šè¿‡è®¾è®¡å¯¹é½ä»»åŠ¡å…³é”®ç‚¹çš„ä¸­é—´å¥–åŠ±ä¿¡å·ï¼Œæ¥ä¼˜åŒ–å¤šæ­¥å†³ç­–è¿‡ç¨‹ï¼Œè¿™å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ç¼ºä¹ä¸­é—´æ ‡æ³¨çš„åºåˆ—å†³ç­–ä»»åŠ¡ä¸­ã€‚</li>
<li><strong>åˆ©ç”¨é¢„è®­ç»ƒVLMçš„åŸç”Ÿæ¥å£</strong>ï¼šå¼ºè°ƒäº†å°Šé‡å¹¶åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å·²æœ‰èƒ½åŠ›ï¼ˆå¦‚å¯¹è¯ã€æè¿°ã€å®šä½ï¼‰çš„é‡è¦æ€§ï¼Œè€Œéå¼ºè¡Œå°†å…¶é€‚é…åˆ°æ–°è¾“å‡ºç©ºé—´ï¼Œè¿™æœ‰åŠ©äºæ›´é«˜æ•ˆåœ°åˆ©ç”¨å¤§æ¨¡å‹å¹¶ä¿æŒå…¶æ³›åŒ–æ€§ã€‚</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>ğŸ’¡ ä¸€å¥è¯æ€»ç»“</h2>
        <p>æœ¬æ–‡æå‡ºReVSegæ–¹æ³•ï¼Œè§£å†³ä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„è§†é¢‘ç›®æ ‡åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç°æœ‰æ–¹æ³•å°†å¤æ‚æ¨ç†è¿‡ç¨‹å‹ç¼©ä¸ºå•æ­¥æ½œåœ¨é¢„æµ‹ã€å¯¼è‡´æ¨ç†é“¾ä¸é€æ˜çš„é—®é¢˜ã€‚å…³é”®æŠ€æœ¯é‡‡ç”¨æ˜¾å¼åˆ†è§£æ¡†æ¶ï¼Œåœ¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åŸç”Ÿæ¥å£ä¸­ï¼Œé€šè¿‡è¯­ä¹‰è§£é‡Šã€æ—¶é—´è¯æ®é€‰æ‹©ã€ç©ºé—´å®šä½ä¸‰ä¸ªé¡ºåºå†³ç­–æ­¥éª¤æ‰§è¡Œæ¨ç†ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¤šæ­¥æ¨ç†é“¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç»“æœä¿¡å·è‡ªæˆ‘æ”¹è¿›å†³ç­–è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒReVSegåœ¨Ref-DAVIS17å’ŒReasonVOSæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ï¼Œä¸”å¼ºåŒ–å­¦ä¹ åè®­ç»ƒå¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.02835" target="_blank" rel="noreferrer" class="btn">æŸ¥çœ‹ arXiv åŸæ–‡</a>
        <a href="../index.html" class="btn btn-outline">è¿”å›åˆ—è¡¨</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>æ•°æ®æ¥æºï¼š<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>ç”± GitHub Actions è‡ªåŠ¨æ›´æ–° Â· AI æ‘˜è¦ä»…ä¾›å‚è€ƒ</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>