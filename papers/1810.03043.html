<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Robustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Robustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/1810.03043" target="_blank" rel="noreferrer">1810.03043</a></span>
        <span>作者: Ebert, Frederik, Dasari, Sudeep, Lee, Alex X., Levine, Sergey, Finn, Chelsea</span>
        <span>日期: 2018/10/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于预测的自监督学习是机器人技能学习的一个吸引人的方向，它允许智能系统利用大量未标注的原始数据自主获取通用技能。该领域的主流方法之一是使用条件视频预测模型（如视觉预见力方法）进行控制，通过预测未来的图像观测来规划动作序列。然而，这类方法存在关键局限性：它们主要被证明适用于短期的、基础的操作技能（如推动、抓取），难以处理时间跨度更长的复杂任务。这是因为在长期任务中，预测模型的不准确性会累积，导致机器人“失去”对目标物体（即初始指定的感兴趣物体）的跟踪，从而无法持续、正确地评估与目标的距离，限制了其通过重试来纠正错误的能力。</p>
<p>本文针对“如何利用即使不完美的预测模型来完成长期、复杂的操作任务”这一具体痛点，提出了一个新视角：通过持续重试（Retrying）来实现鲁棒性。其核心思路是，为视觉模型预测控制（Visual MPC）引入一种基于自监督学习图像配准（Image Registration）的成本函数，使得机器人能够在整个任务执行期间持续跟踪目标物体，从而在预测失误或动作失败时能够重新规划并再次尝试，直至任务成功。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法整体上是一个闭环的视觉模型预测控制（Visual MPC）框架。其输入是当前图像观测、用户指定的初始图像与目标图像（以及其中的感兴趣像素点坐标），输出是执行的动作序列。核心流程为：在每一个真实世界时间步，利用训练好的视频预测模型，对多个候选动作序列的未来图像进行预测；同时，利用一个自监督学习的图像配准网络，将当前图像分别配准到初始图像和目标图像，以确定目标物体在当前图像中的位置；基于此位置与目标位置的预期距离（通过视频预测估算）计算规划成本；最后，使用交叉熵方法（CEM）优化选择成本最低的动作序列执行第一步，并在下一个时间步重新开始此闭环规划。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：闭环视觉MPC框架示意图。自主收集的经验用于训练视频预测模型和图像到图像的配准模型，后者使机器人能够跟踪其目标。</p>
</blockquote>
<p>该方法包含两个核心模块：<strong>视频预测模型</strong>和<strong>图像配准网络</strong>。</p>
<ol>
<li><strong>视频预测模型</strong>：采用已有的基于图像变换的架构，其输入为当前图像和候选动作序列，输出为预测的未来图像序列。该模型还能预测指定像素点在未来的概率分布图，用于计算期望的像素位置。</li>
<li><strong>图像配准网络</strong>：这是一个全卷积神经网络，输入为两张图像，输出为一个稠密光流场（Flow Map），描述了两图之间每个像素的相对运动。该网络的作用是建立当前图像与参考图像（初始图或目标图）之间的像素级对应关系。</li>
</ol>
<p><strong>创新点具体体现在成本函数的计算上</strong>。传统方法（如预测传播）依赖视频预测模型自身来传播初始像素位置，误差会累积。本文则利用配准网络主动“查找”目标物体当前位置：</p>
<ul>
<li><strong>测试时流程</strong>：在每一步，分别计算当前图像 <code>I_t</code> 到初始图像 <code>I_0</code> 的光流 <code>F_{0&lt;-t}</code> 和到目标图像 <code>I_g</code> 的光流 <code>F_{g&lt;-t}</code>。通过这些光流，可以将用户指定的初始像素点 <code>d_0</code> 和目标像素点 <code>d_g</code> “映射”到当前图像中，得到估计的当前位置 <code>\hat{d}_{0,t}</code> 和 <code>\hat{d}_{g,t}</code>（实际计算中采用邻域中值滤波以增强稳定性）。</li>
<li><strong>自适应加权成本</strong>：由于配准可能在物体相距较远时失败，本文设计了一种加权机制。通过计算参考图像（<code>I_0</code>或<code>I_g</code>）在指定像素点处的真实像素值与使用估计光流将当前图像<code>I_t</code>变形后在该点得到的像素值之间的光度误差，来评估本次配准的质量。误差小则权重高，误差大则权重低（公式5）。最终的规划成本 <code>c</code> 是各个视角（初始图、目标图）和各个指定像素的成本 <code>c_i</code>（预测的期望距离）的加权和 <code>c = \sum_i \lambda_i c_i</code>。这使得规划器能动态地依赖更可靠的配准结果。</li>
</ul>
<p><img src="https://..." alt="配准网络测试与训练"></p>
<blockquote>
<p><strong>图3</strong>：(a) 测试时，配准网络将当前图像<code>I_t</code>分别注册到初始图像<code>I_0</code>和目标图像<code>I_g</code>。(b) 训练时，网络通过将轨迹中随机选取的两个时间步的图像相互变形来进行自监督学习。</p>
</blockquote>
<p><strong>配准网络的训练</strong>是完全自监督的。使用与训练视频预测模型相同的自主收集数据。训练时，随机采样同一轨迹中的两帧 <code>I_t</code> 和 <code>I_{t+h}</code>，让网络预测双向光流并试图将一帧变形到另一帧，通过最小化光度重构误差和光流平滑正则项来训练网络。训练中，时间间隔 <code>h</code> 会从1逐步增加到8，形成一个课程学习策略。</p>
<p>此外，本文还展示了方法的两个扩展：<strong>1) 多视角视觉MPC</strong>：通过组合多个相机视角的2D任务（每个视角独立进行配准和成本计算，然后加权平均总成本）来定义3D空间中的任务目标。<strong>2) 结合抓取与非抓取操作</strong>：在数据收集阶段引入一个简单的“抓握反射”（当手腕低于桌面阈值时自动闭合夹爪），使训练数据中包含约20%的抓取片段，从而让学习到的模型能够自主决策何时推动、何时抓取。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界环境中使用两个Sawyer机械臂进行。通过自主探索收集了20,000条推动轨迹和15,000条带夹爪控制的轨迹，涉及150个训练物体和5个未见过的测试物体。评估任务是让机器人将物体从起始位置重新放置到目标位置，通过最终物体位置与目标位置的像素距离来衡量性能。</p>
<p><strong>Baseline对比方法</strong>包括：</p>
<ol>
<li><strong>Visual MPC + Predictor Propagation</strong>：先前工作的方法，依靠视频预测模型内部的流传播来跟踪指定像素。</li>
<li><strong>Visual MPC + OpenCV-Tracking</strong>：使用OpenCV中的MIL跟踪器来跟踪物体，作为有额外监督的强基线。</li>
<li>**Visual MPC + Registration Net (Ours)**：本文提出的方法。</li>
</ol>
<p><img src="https://..." alt="长距离推动任务结果"></p>
<blockquote>
<p><strong>图6</strong>：在20个未见物体上的长距离推动任务结果，横轴为最终距离阈值，纵轴为最终距离低于该阈值的任务比例。本文方法明显优于OpenCV跟踪和预测传播方法。</p>
</blockquote>
<p><img src="https://..." alt="短距离推动任务结果"></p>
<blockquote>
<p><strong>图7</strong>：在15个短距离推动任务上的结果。当任务距离较短时，所有方法性能相当。</p>
</blockquote>
<p><strong>关键定量结果</strong>：</p>
<ul>
<li><strong>长距离推动</strong>（平均目标距离30cm）：本文方法成功率达到**66%<strong>（最终距离小于15像素定义为成功），显著高于OpenCV跟踪器的</strong>45%<strong>和预测传播的</strong>20%**（表1，图6）。这表明在长视野任务中，闭环重试和鲁棒的跟踪至关重要。</li>
<li><strong>短距离推动</strong>（平均目标距离15cm）：所有方法成功率均为**83%**（表1，图7），说明在简单任务中，跟踪机制的优势不明显。</li>
<li><strong>结合抓取与非抓取操作</strong>：在启用抓握反射的数据上训练后，本文方法在20个物体重排任务上同样显著优于两个基线（图9），展示了其处理更复杂混合策略任务的能力。</li>
</ul>
<p><img src="https://..." alt="结合操作的结果"></p>
<blockquote>
<p><strong>图9</strong>：在结合抓取与非抓取操作的物体重排任务上的结果。本文方法同样优于基线。</p>
</blockquote>
<p><strong>定性结果与分析</strong>：</p>
<ul>
<li>图5和图8展示了方法在失败后通过重试最终成功的过程。图5中，机器人先尝试推动，失败后改为抓取并将物体移至目标。图8中，在物体意外移动后，跟踪机制使机器人能重新调整并完成任务。</li>
<li>图4可视化了配准过程，显示了在任务不同阶段，向初始图像配准和向目标图像配准的成功情况会动态变化，本文的加权机制能自适应地选择可靠配准。</li>
</ul>
<p><strong>消融实验</strong>：核心组件的有效性已通过上述与基线的对比实验得到验证。实验结果表明，<strong>自监督的图像配准模块</strong>是性能提升的关键，它提供了比有监督跟踪器更鲁棒、且能随数据收集持续改进的目标跟踪能力，从而实现了有效的“持续重试”。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一种用于视觉MPC的、基于<strong>自监督图像配准的成本函数</strong>，使机器人能够在长期任务中持续跟踪目标，实现<strong>通过重试的鲁棒性</strong>。2) 将视觉MPC扩展到<strong>多视角和结合抓取的操作</strong>，展示了从完全自主收集的数据中学习复杂操作技能的潜力。3) 在真实机器人实验中证明，该方法仅用160机器人小时的自主数据，便能泛化到大量未见过的物体上完成复杂操作任务。</p>
<p>论文提到的局限性包括：任务指定仍依赖于用户在初始图像和目标图像中手动标注像素点；尽管加权机制有帮助，但配准在物体外观变化极大或严重遮挡时仍可能失败。</p>
<p>这项工作对后续研究的启示是：自监督学习与闭环重试策略的结合是迈向更通用、更鲁棒机器人操作的有效途径。它表明，即使预测模型不完美，只要系统具备持续评估状态并纠正错误的能力，就能完成复杂任务。未来方向包括改进配准和预测模型，探索更自动化的目标指定方式，以及将框架应用于更动态和多样化的环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决利用自监督学习的视觉预测模型控制机器人完成复杂操作任务时，因预测不准确导致的执行难题。核心方法是提出“视觉模型预测控制（Visual MPC）”，结合基于视频预测的控制器与自监督图像配准算法，使机器人能在执行中持续跟踪目标并自主重试。实验表明，仅使用160小时自主收集的无标注数据训练后，该系统能成功操作训练中未见过的多种物体，完成抓取、重定位等复杂任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/1810.03043" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>