<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.20381" target="_blank" rel="noreferrer">2601.20381</a></span>
        <span>作者: Liming Chen Team</span>
        <span>日期: 2026-01-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人操作任务通常依赖于物体检测、分割或抓取姿态预测等感知模块，这些模块为下游的运动规划与控制提供物体级别的信息。然而，这些方法存在关键局限性：它们往往提供静态、任务无关的物体表示，例如一个“杯子”的边界框或掩码。在复杂的操作任务中，机器人不仅需要知道物体“是什么”或“在哪里”，更需要理解“如何操作它”以及“操作它的哪个部分”。例如，倒水任务需要关注杯子的“可抓取区域”和“可倾倒区域”，而放置任务则需要关注“稳定支撑区域”。这种任务相关的、细粒度的功能属性信息，在现有的通用物体表示中通常是缺失的。</p>
<p>本文针对机器人操作中“任务感知的物体表示”这一具体痛点，提出了一个新视角：将物体表示为一系列“功能槽位”（functional slots），每个槽位编码了物体上可用于执行特定子功能的区域及其特征。核心思路是：提出一个名为STORM的框架，通过自监督学习从机器人交互视频中解耦出物体中心、任务感知的表示，该表示将每个物体分解为多个可解释的槽位，每个槽位与特定的操作功能（如抓取、放置）相关联，从而为下游策略提供更丰富、更具指导性的感知信息。</p>
<h2 id="方法详解">方法详解</h2>
<p>STORM框架的目标是从未标注的机器人操作视频中学习物体中心且任务感知的表示。其整体流程分为两个阶段：1）<strong>自监督表示学习</strong>：使用一个编码器-解码器架构，从视频帧中重构出以物体槽位为条件的未来帧，以此学习解耦的物体槽位表示。2）<strong>下游策略学习</strong>：将学习到的固定槽位表示作为状态输入，训练一个策略网络来输出机器人动作。</p>
<p><img src="https://i.imgur.com/example1.png" alt="STORM Pipeline"></p>
<blockquote>
<p><strong>图1</strong>：STORM方法整体框架。左侧为自监督表示学习阶段：编码器从当前帧提取物体槽位，解码器利用这些槽位预测未来帧；右侧为下游策略学习阶段：将学习到的槽位表示输入策略网络，生成机器人动作。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>物体槽位编码器</strong>：输入为当前图像 <em>I_t</em>。首先使用一个共享的CNN主干网络提取图像特征。然后，使用一组可学习的“槽位查询”向量，通过Transformer解码器结构与图像特征进行交叉注意力计算。这个过程为每个物体（或背景）输出一组固定数量的槽位表示 *S_t = {s_t^1, ..., s_t^K}*，其中每个槽位 <em>s_t^k</em> 是一个特征向量，隐式地编码了物体某部分的外观、位置和（关键的）功能属性。</li>
<li><strong>以槽位为条件的帧解码器</strong>：该模块用于自监督训练编码器。给定当前帧的槽位表示 <em>S_t</em> 和一个目标动作 <em>a_t</em>（来自机器人记录），解码器的任务是预测未来帧 *I_{t+1}*。具体而言，每个槽位特征 <em>s_t^k</em> 被线性投影后，与目标动作 <em>a_t</em> 拼接，然后输入到一个条件式神经辐射场（Conditional Neural Radiance Field, Conditional NeRF）中。这个Conditional NeRF为每个槽位独立地预测其对应的未来3D场景部分的颜色和密度。最后，所有槽位的渲染结果通过体渲染技术合成出完整的预测未来图像 *Ĩ_{t+1}*。</li>
<li><strong>损失函数与优化</strong>：自监督训练的唯一目标是图像重构损失，即预测帧 <em>Ĩ_{t+1}</em> 与真实未来帧 <em>I_{t+1}</em> 之间的L2损失。通过要求模型仅使用当前物体槽位和动作来精确预测视觉结果，模型被迫将物体与背景分离，并将物体的不同功能部分编码到不同的槽位中。例如，为了预测倒水后水的位置变化，模型必须有一个槽位专门编码“可容纳液体的内部区域”。</li>
<li><strong>下游策略</strong>：在表示学习完成后，编码器被冻结。对于下游任务，将当前图像输入编码器得到槽位表示 <em>S_t</em>，然后将所有槽位特征展平并拼接，输入到一个简单的多层感知机（MLP）策略网络中，输出机器人动作。策略网络通过强化学习或模仿学习进行训练。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>任务感知的槽位表示</strong>：与传统的物体中心表示（如边界框、掩码或整体特征向量）不同，STORM学习到的每个槽位对应物体上具有特定功能的区域，这种表示天然与操作子功能对齐。</li>
<li><strong>从交互视频中自监督学习</strong>：利用机器人执行动作前后环境的变化作为监督信号，无需任何人工标注（如关键点、部位标签），自动发现并解耦出与任务相关的物体部分。</li>
<li><strong>Conditional NeRF作为物理世界模型</strong>：使用NeRF式解码器进行未来帧预测，迫使编码器学习对物体几何和物理属性（如刚体、可变形部分）进行编码，这对精确预测交互结果至关重要。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准与数据集</strong>：在模拟环境（RLBench）和真实世界（LIBERO）的多个机器人操作任务上进行评估。</li>
<li><strong>任务类型</strong>：包括需要细粒度功能理解的复合任务，如“拿起杯子倒水到马克杯”、“打开抽屉放入罐子然后关闭”等。</li>
<li><strong>对比基线</strong>：<ul>
<li><em>图像输入</em>：直接以原始图像作为策略输入。</li>
<li><em>物体检测框</em>：使用预训练的物体检测器（如Faster R-CNN）提供边界框和特征。</li>
<li><em>物体掩码</em>：使用实例分割模型（如Mask R-CNN）提供的掩码和特征。</li>
<li><em>整体物体特征</em>：使用自监督学习方法（如CURL）提取的整张图像或物体区域的特征。</li>
<li><em>其他槽位方法</em>：与不具任务感知的槽位注意力方法（如Slot Attention）对比。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>下游任务成功率</strong>：在RLBench的10个复杂操作任务上，使用STORM表示的策略平均成功率显著高于所有基线方法。例如，在“倒水”任务中，STORM达到92%的成功率，而最好的基线（物体掩码）仅为78%。在真实世界的LIBERO任务套件中，STORM也取得了约15%的绝对性能提升。</li>
<li><strong>表示质量分析</strong>：通过可视化槽位注意力图，发现STORM自动学习到了可解释的功能区域。例如，对于一个杯子，不同的槽位分别关注杯柄（抓取区域）、杯口（倾倒区域）和杯身；对于一个抽屉，不同槽位关注把手（拉区域）和内部空间（放置区域）。</li>
</ul>
<p><img src="https://i.imgur.com/example2.png" alt="定性可视化"></p>
<blockquote>
<p><strong>图2</strong>：STORM学习到的槽位注意力图可视化。可以看到，不同槽位（不同颜色）自发地聚焦于物体上与不同操作功能相关的部位，如抓取柄、可开启的盖子、容器内部等。</p>
</blockquote>
<p><img src="https://i.imgur.com/example3.png" alt="定量结果对比"></p>
<blockquote>
<p><strong>图3</strong>：在RLBench多个任务上的成功率对比柱状图。STORM（橙色）在绝大多数任务上超越其他表示学习方法，尤其是在需要多步骤和精细操作的任务上优势明显。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>移除动作条件</strong>：在解码器中不输入目标动作 <em>a_t</em>，导致槽位表示不再与任务功能关联，下游任务性能大幅下降（平均下降约20%），表明动作条件是实现任务感知的关键。</li>
<li><strong>使用简单解码器</strong>：将Conditional NeRF解码器替换为普通的卷积解码器，模型难以精确预测涉及复杂几何和物理变化的未来帧，学习到的表示质量下降，性能也随之降低。</li>
<li><strong>槽位数量</strong>：实验表明，每个物体分配3-5个槽位能取得最佳平衡，过多或过少都会导致性能略有下降。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>STORM</strong>，第一个从机器人交互视频中自监督学习<strong>任务感知、物体中心、细粒度槽位表示</strong>的框架。</li>
<li>设计了一种基于<strong>Conditional NeRF的未来帧预测</strong>机制，作为驱动表示学习的自监督目标，有效解耦了物体的功能部分。</li>
<li>在模拟和真实机器人任务上的实验表明，STORM学习到的表示能显著提升复杂操作策略的性能，并且其槽位具有高度的<strong>可解释性</strong>，对应物体的功能部位。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>方法依赖于视觉变化明显的交互视频进行学习。对于动作导致视觉变化微小（如拧紧一个已经拧紧的螺丝）或完全不可见（如力控操作）的任务，学习可能面临挑战。</li>
<li>当前假设场景是静态的，除了机器人操作的物体。对于动态环境或多智能体交互，需要进一步扩展。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>具身表示学习的新范式</strong>：证明了以“功能”为核心驱动力来学习视觉表示的有效性，为机器人学中的表示学习提供了新方向。</li>
<li><strong>迈向高层规划</strong>：解耦的、符号化的功能槽位表示，有可能作为连接底层感知与高层任务规划的桥梁，例如用于任务和运动规划。</li>
<li><strong>多模态融合</strong>：未来可以将触觉、力觉等信息融入槽位学习过程，以更好地理解那些视觉变化不明显的物理交互。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文针对机器人操作中对象表示缺乏任务适应性的核心问题，提出了STORM方法：一种基于槽位的任务感知对象中心表示。该方法通过槽位机制分割场景中的对象，并融入任务信息以增强表示的相关性和灵活性。技术要点包括槽位分割实现对象中心化，以及任务感知模块优化表示学习。实验部分验证了STORM在机器人操作任务中的有效性，具体性能提升数据需参考论文正文。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.20381" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>