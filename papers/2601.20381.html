<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.20381" target="_blank" rel="noreferrer">2601.20381</a></span>
        <span>作者: Liming Chen Team</span>
        <span>日期: 2026-01-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操纵领域广泛采用预训练的视觉基础模型（如DINOv2）作为感知特征提取器。这些模型提供了强大的、通用的密集特征表示，但它们缺乏显式的物体级结构。这导致下游策略必须隐式地学习关注相关实体，在面对视觉干扰或分布变化时，往往表现出脆弱的泛化能力。同时，基于掩码的分割方法（如SAM）能提供物体级定位，但通常依赖启发式选择，在杂乱场景中可能不稳定，且与任务目标的对齐有限。</p>
<p>另一方面，基于槽的物体中心表示学习（如Slot-Attention）提供了一种将场景分解为离散潜在“槽”的途径，每个槽代表一个物体或部件，从而促进模块化推理和组合泛化。然而，这些方法通常是纯视觉、无监督的，其槽形成过程缺乏语义控制，学习到的槽可能并不对应任务相关的实体，限制了其在机器人控制等面向行动的场景中的实用性。</p>
<p>本文针对“如何将通用但密集的视觉基础模型特征，高效地转化为适合机器人控制的任务感知、物体中心表示”这一具体痛点，提出了一个新视角：通过一个轻量级的、语义感知的槽适应模块，并采用分阶段训练策略来稳定槽的形成并与下游任务对齐。本文的核心思路是：首先通过视觉-语义预训练，利用语言嵌入稳定地学习语义感知的物体中心槽；然后，将这些槽与下游操纵策略进行联合但解耦的训练，使感知表征与任务目标对齐。</p>
<h2 id="方法详解">方法详解</h2>
<p>STORM的整体框架是一个两阶段训练流程，旨在将冻结的DINOv2特征转化为任务感知的物体中心槽表示，供下游策略使用。</p>
<p><img src="https://cdn.mathpix.com/cropped/2025_01_28_9c0b6d3d3f9a5c7e4c10g-1.jpg?height=1060&width=1536&top_left_y=206&top_left_x=384" alt="方法总览图"></p>
<blockquote>
<p><strong>图1</strong>：STORM方法整体框架。<strong>（上部）第一阶段：视觉-语义学习</strong>。冻结的DINOv2提取图像特征，Slot-Attention模块在从任务指令中提取的名词嵌入（通过冻结的CLIP文本编码器获得）的引导下，生成槽和对应的软掩码。通过重构损失、语义对比损失和槽使用惩罚损失进行训练。<strong>（下部）第二阶段：动态任务对齐</strong>。预训练的物体中心模块从相机观测中提取槽表示，与任务嵌入、机器人本体感知和一个可学习的<code>[ACT]</code>令牌一起输入Transformer解码器策略。一个高斯混合模型动作头从<code>[ACT]</code>令牌预测下一个动作。</p>
</blockquote>
<p><strong>核心模块与技术细节：</strong></p>
<ol>
<li><p><strong>多阶段学习策略</strong>：这是STORM的核心创新，旨在解决端到端联合训练导致槽分配不稳定或退化的问题。</p>
<ul>
<li><strong>第一阶段：视觉-语义槽预训练</strong>：此阶段独立于控制策略，训练物体中心模块。给定冻结的DINOv2特征图，Slot-Attention模块生成固定数量的槽。训练目标包括：<strong>重构损失</strong>（通过MLP解码器重建DINOv2特征）、<strong>语义对比损失</strong>（鼓励槽的掩码池化视觉特征与对应的CLIP文本嵌入对齐）、以及<strong>槽使用惩罚损失</strong>（基于掩码权重的熵计算，防止槽崩溃或少数槽主导注意力）。总体损失为三者之和。此阶段不使用任何策略梯度。</li>
<li><strong>第二阶段：联合槽-策略训练</strong>：将预训练的物体中心模块与下游模仿学习策略联合训练。槽表示作为策略的视觉输入。<strong>关键设计</strong>是视觉模块和策略被独立优化：策略损失的梯度不反向传播到视觉主干（DINOv2），并在视觉特征层面应用了梯度截断。这允许物体中心层在适应任务相关视觉统计量的同时，保留第一阶段学到的语义结构，避免策略梯度破坏槽的形成。</li>
</ul>
</li>
<li><p><strong>物体中心分解与语言条件化</strong>：物体中心模块基于DINOSAUR，使用Slot-Attention处理DINOv2特征。为引入语义控制，使用从任务指令中解析出的名词，通过冻结的CLIP文本编码器获得嵌入，用于条件化部分槽的初始化，并作为语义对比损失的目标。</p>
</li>
<li><p><strong>策略架构</strong>：策略接收的观测包括：物体中心模块产生的槽（每个槽附有其掩码的空间特征，如中心坐标）、全局任务指令的CLIP嵌入、机器人本体感知。所有这些令牌与一个可学习的<code>[ACT]</code>令牌一起，由一个Transformer解码器处理（输入包含4帧历史）。动作头是一个高斯混合模型，它将<code>[ACT]</code>令牌映射到下一个相对关节命令。</p>
</li>
</ol>
<p><strong>创新点体现</strong>：<br>与现有方法相比，STORM的创新点具体体现在：1) <strong>轻量级适配</strong>：不重新训练庞大的视觉基础模型，仅在其上添加少量物体中心层。2) <strong>语义引导的槽形成</strong>：利用任务指令中的名词信息引导槽绑定到相关实体，而非纯无监督学习。3) <strong>解耦的多阶段训练</strong>：将物体发现与策略学习分阶段进行，并通过梯度截断防止优化干扰，这是实现稳定、任务感知表征的关键。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>物体发现评估</strong>：使用PASCAL VOC 2012和COCO数据集，评估指标包括FG-ARI和mBO。</li>
<li><strong>机器人操纵评估</strong>：在模拟环境<strong>MetaWorld</strong>和<strong>LIBERO</strong>基准上进行。评估分为<strong>分布内</strong>和引入新视觉干扰物的<strong>新干扰</strong>两种设置。使用成功率为指标。</li>
<li><strong>对比基线</strong>：包括直接使用冻结的DINOv2特征、微调DINOv2主干、以及与其他物体中心模型（DINOSAUR， SPOT， Stable-LSD， Slot-Diffusion， CTRL-O）的比较。</li>
<li><strong>消融实验</strong>：在MetaWorld的10个任务子集上，分析了多阶段训练策略和不同掩码表示的影响。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://cdn.mathpix.com/cropped/2025_01_28_9c0b6d3d3f9a5c7e4c10g-2.jpg?height=476&width=1724&top_left_y=1678&top_left_x=196" alt="物体发现结果表"></p>
<blockquote>
<p><strong>表1</strong>：物体发现性能对比。STORM在分割质量（FG-ARI）上超越了所有无监督模型，并在COCO上取得了与弱监督基线CTRL-O竞争性的结果，甚至在mBO_i上超过了CTRL-O 0.6个百分点。</p>
</blockquote>
<p><img src="https://cdn.mathpix.com/cropped/2025_01_28_9c0b6d3d3f9a5c7e4c10g-3.jpg?height=558&width=1536&top_left_y=2310&top_left_x=384" alt="机器人环境示例"></p>
<blockquote>
<p><strong>图2</strong>：机器人实验环境可视化。顶部为MetaWorld环境，底部为LIBERO环境。</p>
</blockquote>
<p><img src="https://cdn.mathpix.com/cropped/2025_01_28_9c0b6d3d3f9a5c7e4c10g-4.jpg?height=350&width=1724&top_left_y=2994&top_left_x=196" alt="总体操纵性能表"></p>
<blockquote>
<p><strong>表2</strong>：在MetaWorld和LIBERO上的总体操纵性能。STORM在分布内和新干扰设置下均取得了最佳或接近最佳的性能。特别是在泛化方面，STORM相比冻结DINOv2基线，在MetaWorld新干扰设置上提升了12.7个百分点，在LIBERO新干扰设置上大幅提升了19.0个百分点，且几乎无性能衰减。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<p><img src="https://cdn.mathpix.com/cropped/2025_01_28_9c0b6d3d3f9a5c7e4c10g-5.jpg?height=354&width=1724&top_left_y=3460&top_left_x=196" alt="多阶段学习消融表"></p>
<blockquote>
<p><strong>表3</strong>：多阶段学习策略的消融研究。结果显示，从零开始联合训练槽和策略会导致性能显著下降。仅预训练然后冻结槽模块能部分改善泛化。而STORM提出的预训练后联合调优策略取得了最佳效果，验证了多阶段学习的必要性。</p>
</blockquote>
<p><img src="https://cdn.mathpix.com/cropped/2025_01_28_9c0b6d3d3f9a5c7e4c10g-6.jpg?height=558&width=1536&top_left_y=3930&top_left_x=384" alt="槽掩码可视化对比"></p>
<blockquote>
<p><strong>图3</strong>：槽掩码可视化对比。顶部行：从零开始联合训练产生的掩码噪声大、定位差。底部行：STORM两阶段训练产生的掩码更清晰，能稳定绑定到任务相关物体（机械臂、夹爪、抽屉把手、抽屉主体）。</p>
</blockquote>
<p><img src="https://cdn.mathpix.com/cropped/2025_01_28_9c0b6d3d3f9a5c7e4c10g-7.jpg?height=298&width=1724&top_left_y=4604&top_left_x=196" alt="掩码表示消融表"></p>
<blockquote>
<p><strong>表4</strong>：掩码表示形式的消融研究。完全不使用掩码信息会导致性能大幅下降。在多种编码方式中，简单的中心坐标编码表现最佳，表明紧凑稳定的物体位置信息对策略学习已足够且有效。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>STORM</strong>，一个轻量级的、基于槽的任务感知物体中心表示学习框架，能够将冻结的视觉基础模型特征高效适配为适合机器人控制的紧凑表征。</li>
<li>引入了<strong>多阶段适应策略</strong>，通过解耦的视觉-语义预训练和联合策略训练，有效稳定了槽的形成过程，并实现了感知表征与任务目标的对齐。</li>
<li>在模拟机器人操纵任务上的实验表明，STORM在保持分布内性能的同时，显著提升了策略对视觉干扰物的<strong>泛化鲁棒性</strong>。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的主要局限性包括：1) 实验仅在模拟环境中进行，向真实机器人平台的迁移能力尚未验证。2) 方法依赖于固定数量的槽和从任务描述中简单提取名词，对于涉及复杂关系语言或大量物体的任务可能不足。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>分阶段、解耦的训练范式</strong>为将大型基础模型适配到数据有限的具体下游任务（如机器人控制）提供了一种有效且稳定的思路。</li>
<li>结果表明，即使是简单的<strong>语义引导</strong>（如名词）也能显著改善物体中心表示的任务相关性，启发未来研究探索更丰富的语言 grounding 机制。</li>
<li>紧凑的<strong>物体中心表示</strong>（槽）结合关键空间信息（如中心坐标），在提供足够信息供策略学习的同时，能有效过滤无关视觉细节，这对在复杂、动态环境中实现鲁棒控制具有启示意义。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作任务中，视觉基础模型缺乏显式物体结构、导致鲁棒性和可控性不足的问题，提出了STORM方法。该方法是一种轻量级的物体中心表示适配模块，核心采用多阶段训练策略：首先通过视觉语义预训练，利用语言嵌入稳定物体中心槽位；然后与下游操作策略联合适配，将冻结的DINOv2特征转化为任务相关的物体中心表示。实验表明，相比直接使用冻结基础模型特征或端到端训练物体中心表示，STORM能更好地泛化到视觉干扰，并提升控制性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.20381" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>