<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.10274" target="_blank" rel="noreferrer">2510.10274</a></span>
        <span>作者: Xianyuan Zhan Team</span>
        <span>日期: 2025-10-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>构建能够灵活遵循任意人类指令、并能在不同环境和不同机器人本体（Embodiment）间灵巧操作的自主智能体，是机器人领域的核心目标。当前，一个主流方向是将大型语言模型（LLMs）和视觉语言模型（VLMs）扩展为视觉-语言-动作（VLA）模型。然而，VLA模型的成功依赖于在包含广泛机器人系统架构和任务场景的大规模、异构数据集上进行预训练。关键挑战在于数据存在显著的异构性，不仅体现在本体特定的动作空间，还包括相机设置、视觉领域和任务分布等维度的差异。这些差异导致严重的分布偏移和语义错位，混淆模型并损害其性能。</p>
<p>现有VLA方法主要关注为不同本体分配独立的动作解码头来处理动作空间异构性，但不可避免地忽略了其他关键的异构性来源。本文针对这一痛点，提出将多样化的硬件配置和数据类型重新定义为“任务特定特征”，并引入软提示（Soft Prompt）学习技术来捕获这些特征。核心思路是为每个异构数据源分配一组可学习的嵌入作为软提示，从特征融合的早期阶段为VLA模型提供异构性感知的指导，从而增强模型利用和整合跨本体差异的能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>X-VLA是一个基于流匹配（Flow-Matching）的通用VLA框架，其核心创新在于使用软提示来处理跨本体数据中的异构性，并采用简洁的编码管道处理多模态输入。</p>
<p><img src="https://arxiv.org/html/2510.10274v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：X-VLA整体框架。模型为每个异构数据源分配一套独特的可学习嵌入（软提示）。通过堆叠标准的Transformer编码器块，该架构能够整合多视角图像、语言指令和本体感知特征，并生成精确的动作。</p>
</blockquote>
<p><strong>整体流程与核心模块</strong>：训练分为两个阶段。<strong>阶段I（预训练）</strong>：在混合异构数据集上联合优化模型主干和所有数据源对应的软提示，学习一个与本体无关的通用策略。<strong>阶段II（领域适应）</strong>：为目标新领域引入一组新的软提示，先进行“提示预热”（仅优化新提示），然后联合微调新提示和模型主干，使策略有效特化到新本体。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>软提示（Soft Prompts）</strong>：这是处理异构性的核心。为每个数据源分配一组随机初始化、通过端到端训练隐式优化的可学习嵌入 <code>p_i</code>，旨在编码底层硬件配置 <code>h_i</code>。这些提示在动作生成的早期阶段被注入模型，自动引导主干进行本体感知学习。相比语言提示（依赖人工编写的文本描述）和HPT风格投影（易破坏预训练表示），软提示提供了更灵活、可扩展的解决方案。</li>
<li><strong>编码管道（Encoding Pipeline）</strong>：为了有效对齐语义和维度差异巨大的多模态输入，设计了专用编码策略：<ul>
<li><strong>高维观察流</strong>：解耦视觉语言流。主视角图像和语言指令由预训练的VLM编码器（如Florence-Large）处理，以进行高级任务推理；而腕部相机等辅助视图则由共享的视觉骨干网络单独编码，以捕获细粒度操作线索。</li>
<li><strong>低维本体感知-动作流</strong>：将本体感知状态（如关节位置）、动作令牌以及对应的时间嵌入拼接，通过一个轻量级线性层投影到高维特征空间，实现与其它模态的早期融合，确保 robust 的本体感知-时间 grounding。</li>
</ul>
</li>
<li><strong>训练目标</strong>：采用流匹配的行为克隆目标。模型学习一个速度场，将高斯噪声样本逐步“运输”到目标动作块。损失函数为预测速度与噪声到专家数据线性插值路径之间的均方误差。</li>
<li><strong>关键训练技术</strong>：<ul>
<li><strong>自定义学习率</strong>：对软提示以及负责编码视觉和语言输入的模块使用较低的学习率，以减少预训练表示的灾难性漂移风险。</li>
<li><strong>对齐的动作表示</strong>：将动作空间标准化为末端执行器（EEF）姿态表示（Cartesian位置、Rotate6D旋转、离散化的夹爪状态），使用MSE和BCE损失进行优化，确保跨本体的一致性。</li>
<li><strong>意图抽象</strong>：通过时间下采样演示，生成未来4秒内30个锚点序列来总结预期轨迹，而非预测每一步的完整姿态，以建模高级意图并减少人类演示中的噪声。</li>
<li><strong>平衡数据采样</strong>：在跨领域和每个领域内的轨迹间同时打乱样本，确保每次迭代都能接触到多样且平衡的数据混合，减轻分布偏差。</li>
</ul>
</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究评估了模型在<strong>缩放行为</strong>（模型容量、数据多样性、数据量）、<strong>适应性能</strong>和<strong>可解释性</strong>三方面的表现。使用了6个仿真基准（LIBERO, Simpler, VLABench, RoboTwin-2.0, Calvin, NAVSIM）和3个真实机器人平台。预训练数据混合了来自Droid、Robomind和Agibot的290K条轨迹，涵盖7种硬件设置、5种类型机械臂（从单臂到双手）。对比了包括π₀、GR00T、OpenVLA、Octo等在内的众多SOTA基线方法。</p>
<p><img src="https://arxiv.org/html/2510.10274v1/Figures/line_plot_scaling_steps.png" alt="训练曲线比较"></p>
<blockquote>
<p><strong>图4</strong>：不同异构性处理方法的训练曲线。在混合异构数据上，软提示方法（粉色线）相比领域特定动作投影、HPT风格投影和语言提示方法，表现出更稳定、更优越的训练动态和渐近性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.10274v1/x4.png" alt="缩放实验结果"></p>
<blockquote>
<p><strong>图5</strong>：X-VLA在模型大小、数据多样性和数据量三个轴向上的缩放趋势。随着计算资源、数据多样性和数据量的增加，模型的验证预测误差持续下降，且未见饱和迹象，表明仍有性能提升空间。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>仿真基准SOTA性能</strong>：X-VLA-0.9B在绝大多数基准上取得了新的SOTA结果（见表2）。例如，在Simpler-WidowX上达到95.8%成功率，在LIBERO上达到98.1%，在Calvin第一阶段达到97.6%。在VLABench、RoboTwin-2.0和NAVSIM上也取得了显著优于之前最佳结果的成绩。</p>
</li>
<li><p><strong>高效适应</strong>：通过参数高效微调（PEFT），仅优化1%的模型参数（9M），X-VLA-0.9B在LIBERO上达到93%成功率，在Simpler-WidowX上达到54%，其性能与需要优化全部30亿参数的π₀模型相当，但可训练参数量少了300倍。</p>
</li>
<li><p><strong>真实世界评估</strong>：<br><img src="https://arxiv.org/html/2510.10274v1/x5.png" alt="真实世界评估设置"></p>
<blockquote>
<p><strong>图6</strong>：适应实验中评估的设置，涵盖单臂、双手机器人系统及自动驾驶场景。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.10274v1/x6.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图7</strong>：在三个真实机器人平台和五个任务上的评估结果。X-VLA在所有任务上均超越了基线方法，展示了其在简单操作、灵巧操作和快速适应等方面的卓越能力。在极具挑战性的灵巧布料折叠任务中，经过1200次演示训练后，平均能在两分钟内折叠一块布。</p>
</blockquote>
</li>
<li><p><strong>消融实验</strong>：表1展示了从基线模型逐步添加各组件（自定义LR、异构预训练、数据处理技术、架构改进、软提示、模型放大、两步适应）的完整消融路径。每一步都带来了验证误差的降低和下游适应成功率的提升，证实了每个组件的有效性。特别是，引入软提示使Simpler-WidowX的适应成功率从64.6%提升至73.8%。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>软提示机制</strong>：提出并验证了使用可学习的软提示来编码跨本体数据中的异构性（硬件配置、视觉领域等），为VLA模型提供了一种灵活、可扩展且高效的处理异构数据的方法。</li>
<li><strong>简洁高效的架构</strong>：设计了一个基于标准Transformer编码器的流匹配VLA架构，通过解耦的编码管道优雅地处理高维和低维多模态输入，兼顾了可扩展性和简洁性。</li>
<li><strong>有效的训练方法</strong>：提出了一套包含两阶段适应、自定义学习率、对齐动作表示、意图抽象和平衡采样的定制化训练方案，确保了大规模异构数据预训练的稳定性和下游适应的效率。</li>
</ol>
<p><strong>局限性</strong>：论文提到，模型的成功依赖于大规模、高质量的异构数据集进行预训练。对于与训练数据形态完全不同的全新机器人平台，其泛化能力可能存在边界。</p>
<p><strong>对后续研究的启示</strong>：软提示作为一种轻量级、可学习的适配器，为构建真正通用的机器人基础模型提供了新工具。它表明，通过让模型隐式地学习硬件配置的表示，而非依赖显式的手工设计或复杂的投影网络，可以更有效地桥接不同机器人领域。未来工作可探索软提示在更广泛异构性（如仿真到真实迁移、不同传感器模态）中的应用，以及如何使提示学习过程更具可解释性。X-VLA-0.9B在多个基准上的全面SOTA表现，为后续研究设立了一个强有力的基线。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出X-VLA模型，旨在解决跨不同机器人平台（跨具身）的异构数据整合难题，以训练通用的视觉-语言-动作模型。其核心技术是软提示方法，为每个数据源引入可学习的嵌入向量作为特定具身提示，结合基于流匹配的Transformer编码器架构，实现参数高效且可扩展的跨平台学习。实验在6个仿真环境和3个真实机器人上进行，0.9B参数的X-VLA在多项基准测试中达到SOTA性能，展现出从灵巧操作到快速跨平台适应的优异能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.10274" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>