<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.14530" target="_blank" rel="noreferrer">2509.14530</a></span>
        <span>作者: Chen Peng Team</span>
        <span>日期: 2025-09-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>草莓采摘高度依赖人工，而传统机器人采摘通常采用模块化的感知-规划-控制流程。然而，草莓天然簇生，与叶片、茎秆和其他果实相互交织，导致严重遮挡。这种生长习性对机器人采摘构成了巨大挑战：传统的流程难以在杂乱环境中规划出到达果实的轨迹，且成功采摘被遮挡的草莓需要灵巧的操作来绕过或轻柔地移动周围柔软的障碍物，并精确到达花萼上方的理想采摘点。现有方法如主动感知或主动障碍分离策略，要么需要额外计算资源进行多视角处理，要么依赖于对环境的简化建模和完美感知假设，难以应对真实场景中复杂的端到端视动协调问题。</p>
<p>本文针对簇生草莓采摘中因遮挡导致的灵巧操作难题，提出了从模仿学习（Imitation Learning）入手的新视角。核心思路是：构建一个远程遥操作数据采集系统，收集人类专家的采摘演示，并基于改进的动作分块Transformer（ACT）算法，学习一个端到端的精细视动策略，使机器人能够直接从视觉观察中生成灵巧的采摘动作序列。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的系统整体框架包含硬件平台和模仿学习算法两部分。硬件上，采用一个4自由度（4-DoF）的SCARA机械臂，搭配一个双臂遥操作接口用于高效收集人类演示数据。算法上，在原始ACT框架基础上，提出了端位姿辅助的动作分块Transformer（EPACT）。</p>
<p><img src="https://arxiv.org/html/2509.14530v1/figures/headfigure.jpeg" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：工作整体框架。左侧为数据收集阶段：人类操作员通过遥操作臂控制SCARA机器人进行采摘演示，同时记录关节状态和腕部摄像头图像。右侧为策略学习与部署阶段：收集到的演示数据用于训练EPACT策略网络，训练好的策略根据当前的视觉观察和本体感知状态，直接预测未来的机器人动作序列。</p>
</blockquote>
<p>核心创新在于对ACT解码器预测头的改进，引入了端位姿损失（End Pose Loss）和一个基于神经网络的逆运动学模块。原始ACT直接预测关节空间的动作，但对于草莓采摘这类对末端执行器（夹爪）位姿精度要求极高的任务，关节角的微小偏差可能导致末端位姿的显著误差。EPACT的核心思想是让策略网络首先预测机器人的末端位姿序列，再将此位姿解码为关节角，并计算预测位姿与真实位姿之间的L1损失（ℒ_rec_end_pose）以加强网络对末端轨迹准确性的关注。总损失函数为：ℒ = ℒ_rec_action + βℒ_reg + γℒ_rec_end_pose，其中β和γ为权重。</p>
<p>论文设计了两种EPACT变体，区别在于动作预测头的结构：</p>
<p><img src="https://arxiv.org/html/2509.14530v1/figures/arch1.jpeg" alt="EPACT架构变体"></p>
<blockquote>
<p><strong>图4</strong>：EPACT-L和EPACT-EE的动作预测解码器架构。<strong>EPACT-L（潜在）</strong>：使用单一预测头，尝试从预测的6D末端位姿中直接解码出完整的5D动作向量（4个关节+1个夹爪状态）。<strong>EPACT-EE（末端执行器）</strong>：使用解耦的双分支预测头，一个分支（逆运动学网络）将预测的6D末端位姿解码为4个关节动作，另一个分支直接从Transformer的隐藏状态预测1D夹爪动作，最后将两者拼接。</p>
</blockquote>
<p>数据收集在一个仿真的桌草莓采摘场景中进行，使用了带磁吸装置的橡胶草莓和柔性电线模拟茎秆。通过遥操作收集了511条演示数据。机器人末端配备了两个摄像头（腕部上、腕部下），以提供多视角视觉信息，帮助处理遮挡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在实验室构建的仿真采摘场景中，定义了6种草莓簇场景（State 0-5），如图6所示。State 0为无遮挡基线场景，State 1-5为不同遮挡程度的簇生场景。使用成功率作为评估指标。</p>
<p><strong>对比方法</strong>：以原始ACT算法作为基线，与提出的EPACT-L和EPACT-EE进行对比。所有策略使用相同的511条演示数据训练，并测试了不同摄像头配置（仅腕上、仅腕下、双腕）的影响。</p>
<p><img src="https://arxiv.org/html/2509.14530v1/figures/cluster_states.jpeg" alt="实验场景"></p>
<blockquote>
<p><strong>图6</strong>：六种草莓簇实验场景。State 1-4为四种代表性的三果簇场景，State 5为双果簇场景，State 0为无遮挡单果基线场景。每张图左侧为实验所用仿真场景，右侧为对应的真实田间草莓簇照片。</p>
</blockquote>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>总体性能</strong>：如表I所示，在具有遮挡的簇生场景（State 1-5）中，两种EPACT变体均显著优于原始ACT。EPACT-EE取得了最佳平均成功率71.7%，相比ACT的35.0%提升了41.7%。即使在未见过演示数据的简单无遮挡场景（State 0），EPACT-EE也达到了70%的成功率，而ACT仅为10%，这表明端位姿辅助提高了策略的泛化能力。</li>
<li><strong>摄像头视角影响</strong>：如图7所示，使用双摄像头输入的策略性能通常优于单摄像头。对于EPACT-EE，双摄像头配置在多数场景下表现最好，而“腕部下”单视角在某些场景（如State 2, 4）甚至优于双视角，说明不同视角对不同遮挡模式的信息贡献不同。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.14530v1/figures/successful_rate_cams.jpeg" alt="摄像头视角影响"></p>
<blockquote>
<p><strong>图7</strong>：不同摄像头配置下的策略性能对比。柱状图展示了EPACT-L和EPACT-EE在不同摄像头输入（仅腕上、仅腕下、双腕）时，在各个场景下的采摘成功率。结果表明多视角信息通常有益，但最优视角取决于具体场景。</p>
</blockquote>
<ol start="3">
<li><strong>定性分析</strong>：图8展示了成功与失败的轨迹示例。成功轨迹显示，学习到的策略能够使机器人灵巧地绕开或推开障碍物，以毫米级精度接近目标草莓茎秆。失败案例主要包括误识别目标、同时采摘多果以及轨迹错误。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.14530v1/figures/success_and_fail.jpeg" alt="成功与失败轨迹"></p>
<blockquote>
<p><strong>图8</strong>：<strong>上图</strong>：一次成功的簇生草莓采摘轨迹可视化。<strong>下图</strong>：一次失败的采摘尝试，机器人错误地试图采摘非目标草莓（左侧白色未成熟果）。图中的彩色轨迹是将策略预测的未来末端位姿投影到图像空间的结果。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>开发了一套集成4-DoF SCARA机械臂与人类遥操作接口的数据采集系统，并首次成功通过模仿学习实现并部署了针对真实簇生草莓的灵巧视动策略。</li>
<li>提出了EPACT算法，通过引入端位姿损失和神经逆运动学模块，增强了对末端执行器轨迹精度的关注，显著提升了在复杂遮挡场景下的采摘成功率。</li>
<li>进行了全面的实验研究，评估了不同遮挡场景和摄像头配置对算法性能的影响，证明了模仿学习在获取灵巧采摘技能方面的潜力。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>泛化性</strong>：策略在未参与训练的无遮挡简单场景（State 0）上表现并非完美（最高70%），表明纯模仿学习对任务变体的泛化能力仍有局限。</li>
<li><strong>硬件限制</strong>：使用的4-DoF SCARA臂在运动灵活性上不如6-DoF机械臂，可能限制其在更复杂空间中的操作能力。</li>
<li><strong>仿真与真实差距</strong>：数据收集在可控的仿真场景中进行，虽模拟了视觉复杂性和物理交互，但与真实田间环境的动态性和多样性仍有差距。</li>
</ol>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li>可以探索结合强化学习或领域自适应技术，以提升模仿学习策略在未见场景下的鲁棒性和泛化能力。</li>
<li>研究更高效的传感器融合方案（如结合深度信息或触觉反馈），以更好地处理严重遮挡。</li>
<li>将方法扩展到其他簇生、易损的水果采摘任务中，验证其通用性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对簇生草莓采摘中因遮挡导致的机器人操作难题，提出了一种基于模仿学习的解决方案。核心是通过人机协作系统（4-DoF SCARA机械臂与遥操作接口）采集数据，并利用改进的End Pose Assisted Action Chunking Transformer (ACT) 学习精细的视觉运动策略，实现灵巧绕过遮挡物并精准定位花萼上方茎秆的采摘点。实验表明，该方法在多种遮挡场景下显著优于直接使用ACT的基准方法，展现了实际应用的潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.14530" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>