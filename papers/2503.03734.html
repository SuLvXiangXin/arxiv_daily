<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.03734" target="_blank" rel="noreferrer">2503.03734</a></span>
        <span>作者: Huang, Huang, Liu, Fangchen, Fu, Letian, Wu, Tingfan, Mukadam, Mustafa, Malik, Jitendra, Goldberg, Ken, Abbeel, Pieter</span>
        <span>日期: 2025/03/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通常将不同模态的输入独立编码为单独的令牌：多视角图像通过视觉特征提取器编码，语言指令被分词，连同机器人的本体感知状态，一并输入到一个基于Transformer的机器人策略网络中，进行端到端的动作生成。这种主流方法（如图1a所示，代表模型如Octo和OpenVLA）要求策略网络在内部连接视觉和语言信息并执行精确的机器人控制，这在未见过的环境中尤其具有挑战性。现有工作（如RT-2和OpenVLA）虽然展示了在机器人数据上微调预训练视觉语言模型的益处，但这种微调可能会干扰预训练获得的视觉和语言特征。由于机器人数据集的语义多样性远不如VLM训练所用的大规模视觉-语言数据集，微调往往导致模型在未见过的物体或环境上的性能相比已见任务出现显著下降。然而，像CLIP这样的预训练VLM已经表现出强大的视觉-语言对齐能力和在各种下游任务上的零样本性能。这引发了一个思考：更有效的策略或许是提取并利用这种预训练的对齐关系，而不是通过微调去削弱它。</p>
<p>本文针对现有VLA模型因微调预训练VLM而损害其语义对齐与泛化能力的痛点，提出了一个新的视角：冻结预训练的视觉和语言编码器，并通过语言指令引导，有选择性地提取与任务相关的视觉特征。本文的核心思路是：设计一个名为OTTER的VLA架构，它通过一种显式的、文本感知的视觉特征提取机制，仅将与语言指令语义对齐的任务相关视觉特征传递给策略Transformer，从而保留并利用大规模预训练学到的丰富语义理解，实现强大的零样本泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>OTTER的整体流程是：在每个时间步，利用预训练的CLIP模型提取文本感知的视觉特征，然后将这些特征与压缩后的文本特征、编码后的本体感知特征拼接，作为因果Transformer策略网络的输入，以自回归方式预测未来的机器人动作。</p>
<p><img src="https://arxiv.org/html/2503.03734v4/figures/model_arc_vertical.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>: OTTER模型架构。在每个时间步t，从预训练的CLIP模型中提取文本感知的视觉特征f_vl（参见图3）。然后f_vl和文本令牌f_l分别通过独立的注意力池化层进一步压缩，产生两个表示f&#39;_vl和f&#39;_l。机器人的本体感知通过一个MLP层编码为体现特征f_e。将令牌f&#39;_l、f&#39;_vl和f_e拼接起来形成f_t，作为因果Transformer的输入。在长度为T步的上下文窗口中，模型以自回归方式预测每个时间步的未来动作(a_t)。</p>
</blockquote>
<p>核心模块包括文本感知视觉特征提取、特征压缩与策略网络。文本感知视觉特征提取是OTTER的创新关键。它使用预训练的ViT-CLIP模型，但并非直接使用其最终输出特征X_out。受ClearCLIP启发，OTTER利用CLIP最后一个自注意力块的注意力输出X_attn，认为其包含更清晰的语义信息。具体而言，首先分别从CLIP的语言编码器和视觉编码器提取归一化的每令牌文本特征f̂_l和视觉块特征f̂_v。然后通过温度加权的注意力机制计算文本感知的视觉特征：f_vl = softmax(f̂_l f̂_v^⊤ / τ) (f̂_v + PE)。其中，τ是可学习的温度参数，用于控制选择注意力的集中程度；PE是2D正弦余弦位置编码，为策略网络提供每个图像块的空间位置信息。该操作本质上是根据文本与每个视觉块的余弦相似度，为每个语言令牌选择并加权平均最相关的视觉块特征，从而生成与文本语义对齐的视觉表示。至关重要的是，整个CLIP模型在训练期间保持冻结，只有温度参数τ是可学习的。</p>
<p><img src="https://arxiv.org/html/2503.03734v4/x1.png" alt="文本感知特征提取"></p>
<blockquote>
<p><strong>图3</strong>: 文本感知视觉特征提取。计算视觉块特征与每令牌语言特征的相似度，然后在块特征维度上取softmax。直观上，这给出了所有空间位置上的语义相似度分布。然后将视觉块特征与之相乘，以检索出与句子中每个令牌对应的视觉语义特征。</p>
</blockquote>
<p>特征压缩模块将提取的文本感知视觉特征f_vl（每个摄像头对应m个令牌）通过一个可学习的交叉注意力池化操作压缩为单个令牌f&#39;_vl。同样，对文本特征f_l也应用一个独立的可学习交叉注意力池化，得到一个文本令牌f&#39;_l。机器人的本体感知状态通过一个前馈网络编码为体现特征f_e。最后，这三个特征在通道维度上拼接，形成策略网络的输入令牌f_t。</p>
<p>策略网络是一个因果Transformer，包含4层和8个注意力头，隐藏维度为512。它以拼接后的特征f_t为输入，在T=12步的上下文窗口内，自回归地预测未来的动作。动作参数化采用末端执行器的相对位姿变化（Δ位姿）和连续的绝对夹爪状态，构成一个10维的动作表示。</p>
<p>与现有方法（图1a所示的直接特征传递）相比，OTTER的创新点具体体现在：1）<strong>特征提取层面</strong>：不是独立传递所有视觉和语言特征，而是先利用冻结的预训练VLM进行跨模态语义对齐，提取出与任务指令强相关的“文本感知”视觉特征。2）<strong>训练策略层面</strong>：通过冻结VLM，彻底避免了在有限机器人数据上微调对预训练语义对齐的破坏，将任务规划（选择相关视觉特征）与动作规划（预测动作）有效解耦。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了LIBERO仿真基准（包括LIBERO-Spatial/Object/Goal和LIBERO-90）进行仿真评估，并使用Franka机器人在真实世界收集了包含抓放、戳、倒、开/关抽屉四种技能的数据集（DS-PnP和DS-ALL）进行实物验证。实验分为在分布训练任务和零样本泛化到未见任务两种设置。</p>
<p>对比的基线方法包括：1) <strong>Octo</strong>：在Open X-Embodiment数据集上从头训练的Transformer策略。2) <strong>OpenVLA</strong>：在OXE数据集上微调的Prismatic-7B VLM。3) <strong>π0-Fast-Droid</strong>：使用预训练PaliGemma和Fast动作分词器的VLA模型。4) <strong>DFP-OTTER</strong>：OTTER的变体，采用直接特征传递（即文本和视觉令牌独立池化后拼接），用于验证文本感知特征提取的重要性。</p>
<p>在真实机器人单一技能（抓放）实验中，OTTER在训练任务上取得了68%的成功率，在未见任务上取得了62%的成功率，均显著优于所有基线（见表1）。值得注意的是，DFP-OTTER在训练任务上表现尚可（29%），但在未见任务上泛化能力急剧下降至4%，这凸显了文本感知特征提取对于泛化的关键作用。OTTER-OXE（在OXE上预训练后再微调）取得了最佳性能，说明OTTER的性能可随机器人数据规模扩大而提升。</p>
<p><img src="https://arxiv.org/html/2503.03734v4/figures/contrast_results_v2.png" alt="结果对比"></p>
<blockquote>
<p><strong>图1</strong>: （上）VLA模型中不同的特征提取方法对比。（下）真实世界机器人实验结果：在训练和未见的真实机器人抓放任务中，OTTER相比Octo和OpenVLA都展示了更高的成功率。OTTER对未见物体表现出更好的零样本泛化能力。</p>
</blockquote>
<p>在多技能零样本泛化实验中（见表2），OTTER在所有四种技能（倒、抽屉、戳、抓放）的未见任务上都保持了显著高于基线的成功率（60%-93%），而基线模型（如微调后的Octo和OpenVLA）在多数技能上成功率接近0%。这表明OTTER的方法能有效应对更复杂的多技能泛化挑战。</p>
<p>消融实验（表1）总结了各个组件的贡献：1) <strong>移除文本感知特征提取（DFP-OTTER）</strong>：导致未见任务性能崩溃，证实了该机制的核心重要性。2) <strong>微调CLIP</strong>：性能（26%，15%）远低于冻结CLIP的OTTER（68%，62%），验证了冻结预训练VLM以保持语义对齐的优势。3) <strong>移除体现特征f_e或压缩文本特征f‘_l</strong>：都会导致性能下降，说明本体感知和明确的语言指令信息对策略网络仍然必要。</p>
<p><img src="https://arxiv.org/html/2503.03734v4/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>: 使用CLIP的注意力输出X_attn（右）相比最终输出X_out（左）能获得更清晰、与文本语义对齐更好的视觉特征，这解释了OTTER选择X_attn的原因及其有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>提出了OTTER架构</strong>：通过冻结预训练VLM并执行文本感知的视觉特征提取，创新性地将VLA模型的任务规划与动作规划解耦，有效利用了预训练模型的语义对齐知识。2) <strong>展示了卓越的零样本泛化能力</strong>：在仿真和真实机器人实验中，OTTER在未见物体和场景上的性能显著超越现有SOTA VLA模型。3) <strong>揭示了性能扩展路径</strong>：实证表明OTTER的性能可以通过使用更大的预训练VLM编码器、增加策略网络容量以及在更大规模机器人数据集上进行预训练来进一步提升。</p>
<p>论文自身提到的局限性包括：实验评估的任务范围仍然有限（主要是桌面物体操纵），且方法依赖于特定的预训练VLM（如CLIP）及其内部特征（X_attn）的有效性。</p>
<p>本文对后续研究的启示包括：1) <strong>利用而非微调预训练对齐</strong>：为机器人学习领域提供了一种新范式，即更注重如何巧妙提取和利用大规模基础模型中已存在的知识，而非盲目进行端到端微调。2) <strong>解耦设计</strong>：任务规划与动作规划的解耦设计思路可能有助于构建更模块化、可解释和可扩展的机器人智能体。3) <strong>扩展性验证</strong>：指明了通过扩大模型规模和数据规模来持续提升VLA模型泛化能力的明确方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出OTTER模型，解决现有视觉-语言-动作模型因微调破坏预训练语义对齐、导致泛化能力下降的问题。核心技术为**文本感知视觉特征提取**：根据语言指令选择性提取语义对齐的任务相关视觉特征，保持预训练编码器冻结。实验表明，OTTER在仿真和真实机器人任务中显著优于现有模型，成功实现对新物体和环境的**零样本泛化**。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.03734" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>