<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.19717" target="_blank" rel="noreferrer">2505.19717</a></span>
        <span>作者: Rouxel, Quentin, Donoso, Clemente, Chen, Fei, Ivaldi, Serena, Mouret, Jean-Baptiste</span>
        <span>日期: 2025/05/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习是实现人形机器人通用能力的一种有前景的方法，但其规模化从根本上受限于高质量专家演示数据的稀缺性。利用次优、开放式的游戏数据可以缓解此问题，这类数据更易收集且更具多样性。然而，从游戏数据中学习面临两大关键挑战：<strong>最优性</strong>（数据中的动作对于特定任务往往是次优的）和<strong>拼接</strong>（从初始状态到远距离目标的完整轨迹很少被演示，需要跨片段组合）。现有方法中，纯监督学习技术（如基于回报的条件化）缺乏拼接轨迹的原则性机制；而离线强化学习虽能同时处理两者，但其近期进展很大程度上由扩散模型等生成模型驱动。Flow Matching作为扩散模型的替代方案，具有<strong>确定性推理</strong>和<strong>支持任意源分布</strong>的独特优势，推理速度更快，对机器人实时应用至关重要。本文针对从游戏数据中进行目标条件模仿和强化学习时，如何同时实现最优动作选择和长视距轨迹拼接的痛点，提出了一种名为<strong>Extremum Flow Matching</strong>的新方法，其核心思路是利用Flow Matching的确定性传输特性，通过映射均匀源分布的边界来估计目标条件分布的极值（如最小化到目标的距离），从而引导策略选择最优路径。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法基于Flow Matching生成模型。Flow Matching学习一个确定性的连续变换（流向量场 (f)），将简单的源分布 (P_{src}) 映射到复杂的目标分布 (P_{dst})。训练使用Flow Matching损失 (L_{flow} = ||f(x_t, t, c) - (x_{dst} - x_{src})||^2_2)，其中 (x_t = (1-t)x_{src} + t x_{dst})，(t \in [0,1]) 表示插值进度，(c) 为条件变量。推理时，通过从源分布采样并积分学习到的流来生成目标样本。</p>
<p><img src="https://.../%E5%9B%BE1" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：使用Extremum Flow Matching进行基于游戏演示操作的目标条件策略。策略以当前和目标图像为输入，输出机器人指令轨迹（上）。Flow Matching的流向量场（左下）将一维均匀源分布确定性地变换为目标分布。由于流路径不相交，源分布支撑的边界被映射到目标分布支撑的最小值和最大值。</p>
</blockquote>
<p><strong>Extremum Flow Matching</strong> 是该工作的核心创新。其关键洞察是：在Flow Matching的确定性变换下，流路径互不相交。因此，<strong>均匀源分布支撑的边界（即最小值0和最大值1）会被映射到目标分布支撑的边界</strong>。对于一维条件分布 (x(c))，通过训练模型 (F: \varepsilon \sim U(0,1) | c \mapsto x(c))，即可在推理时通过 (F(\varepsilon=0|c)) 和 (F(\varepsilon=1|c)) 分别估计其最小和最大值。</p>
<p><img src="https://.../%E5%9B%BE3" alt="一维分布极值估计对比"></p>
<blockquote>
<p><strong>图3</strong>：期望回归与Extremum Flow Matching在估计一维多模态条件分布极值上的对比。Extremum Flow Matching允许在推理时选择参数 (\varepsilon)，并能提供更紧致的估计。</p>
</blockquote>
<p>对于需要沿特定维度 (z)（如回报 (d)）进行最小化/最大化的多维分布 (x=(z, y))，作者提出了一个分解的生成过程（式3）：先训练一个模型 (F_1: U(0,1) \mapsto z) 来学习边缘分布 (P(z))；再训练一个条件模型 (F_2: P_{src} | z \mapsto y) 来学习条件分布 (P(y|z))。推理时（式4），先通过 (F_1(0)) 或 (F_1(1)) 得到极值 (\tilde{z})，再通过 (F_2(P_{src} | \tilde{z})) 得到对应的 (\tilde{y})。这确保了源分布中 (z) 维度的极值被正确映射到目标分布中 (z) 维度的极值，而单模型基线方法无法保证这一点。</p>
<p><img src="https://.../%E5%9B%BE4" alt="多维分布极值映射"></p>
<blockquote>
<p><strong>图4</strong>：非条件二维分布的极值。红色样本展示了沿 (z) 轴的极值如何从均匀源分布（上）映射到目标分布（下）。左图的单模型基线未能正确映射，而右图的提出的条件模型（式3,4）实现了正确映射。</p>
</blockquote>
<p>基于此，作者设计了一系列目标条件智能体（见表1），它们由<strong>批判者</strong>（估计回报）、<strong>规划者</strong>（生成观测轨迹）、<strong>执行者</strong>（生成动作轨迹）和<strong>世界模型</strong>（预测动态）这四个核心模块以不同方式组合而成。</p>
<ul>
<li><strong>FM-GC</strong>：基线，仅使用Flow Matching的目标条件模仿策略，无最优性概念。</li>
<li><strong>FM-AC</strong>：执行者条件化。先训练批判者估计回报分布，用其最小值 (Critic(0|o,g)) 条件化执行者。</li>
<li><strong>FM-PC</strong>：规划者条件化。用批判者估计的最优回报条件化规划者，执行者作为逆动力学模型实现规划。</li>
<li><strong>FM-PS</strong>：规划者拒绝采样。规划者不依赖目标生成候选观测轨迹，由批判者评估其终点并选择最佳者。</li>
<li><strong>FM-AS</strong>：执行者拒绝采样（带世界模型）。执行者生成候选动作轨迹，经世界模型预测结果后由批判者选择。</li>
</ul>
<p>为赋予智能体<strong>跨片段轨迹拼接</strong>能力，作者为每个基于批判者的智能体定义了 <code>use-RL</code> 变体。该变体在训练时使用<strong>强化学习递归自举</strong>（式5）来增广批次：对于元组 ((o, \tau_o, \tau_a, d, g))，从数据集中均匀采样一个新目标 (g&#39;)，构建增广元组 ((o, \tau_o, \tau_a, d + Critic(\varepsilon_g | g, g&#39;), g&#39;))。这实现了贝尔曼风格的备份，将两段轨迹的回报组合起来，从而增广了回报分布，使批判者能够估计数据集中任意观测对之间的回报。</p>
<p><img src="https://.../%E7%AE%97%E6%B3%951" alt="算法伪代码"></p>
<blockquote>
<p><strong>算法1</strong>：FM-AC-use-RL 的训练与推理伪代码，展示了编码器、批判者和执行者的协同训练流程，以及递归自举增广步骤。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>基准测试</strong>：在<strong>OGBench</strong>基准测试（包含模拟 locomotion 和 manipulation 环境）上，使用低维状态观测评估了所提智能体。对比的基线方法包括 GCBC, GCIVL, GCIQL, QRL, CRL, HIQL 等离线目标条件强化学习算法。</p>
<p><img src="https://.../%E5%9B%BE5" alt="OGBench性能对比"></p>
<blockquote>
<p><strong>图5</strong>：在OGBench基准测试上的性能对比。结果显示，没有单一方法在所有任务上始终领先。本文的某些智能体（如FM-AC-use-RL）在需要组合拼接的<code>cube</code>和<code>puzzle</code>等操作任务上表现出色，但在某些运动任务（如<code>antmaze</code>）上，使用固定超参数时性能相对较弱。</p>
</blockquote>
<p><strong>演示行为影响分析</strong>：在2D非抓取平面推动任务中，研究了三种不同人类演示策略收集的数据集对性能的影响：<strong>专注</strong>（直接推向目标）、<strong>探索</strong>（随机移动）和<strong>混合</strong>。</p>
<p><img src="https://.../%E5%9B%BE6" alt="演示行为与性能分析"></p>
<blockquote>
<p><strong>图6</strong>：（左上）2D平面推动任务示意图。（其余）不同智能体在三种演示行为数据集上的成功率。结果显示，<code>use-RL</code>变体在所有数据集上均显著优于<code>no-RL</code>变体，证明了递归自举对拼接能力的重要性。在<code>探索</code>和<code>混合</code>数据集上，FM-AC/FM-PC等基于条件化的方法优于FM-PS/FM-AS等基于拒绝采样的方法。</p>
</blockquote>
<p><strong>真实机器人验证</strong>：在<strong>Talos人形机器人</strong>上进行了验证，任务是基于高维图像观测，在真实厨房环境中完成包含抓放、开门/关门、托盘拉/推的复杂长视距操作序列。</p>
<p><img src="https://.../%E5%9B%BE7" alt="真实机器人实验"></p>
<blockquote>
<p><strong>图7</strong>：Talos人形机器人成功执行长视距操作任务序列的定性结果截图，证明了方法在真实复杂场景中的有效性。</p>
</blockquote>
<p><strong>消融实验</strong>：实验表明，<code>use-RL</code>变体相比<code>no-RL</code>变体性能有显著提升，验证了递归自举对实现轨迹拼接的关键作用。在<code>探索</code>和<code>混合</code>这类包含更多次优路径的数据集上，基于Extremum Flow Matching条件化（FM-AC, FM-PC）的方法通常优于基于拒绝采样（FM-PS, FM-AS）的方法。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了 <strong>Extremum Flow Matching</strong>，一种利用Flow Matching特性估计条件分布极值的新方法，以原则性的方式解决最优性问题；2) 基于此设计并系统比较了一个<strong>模块化智能体家族</strong>，探索了批判者、规划者、执行者、世界模型的不同组合；3) 引入了<strong>强化学习递归自举</strong>机制，有效赋能智能体进行跨片段轨迹拼接，并通过<strong>仿真基准</strong>、<strong>演示行为分析</strong>和<strong>真实人形机器人复杂操作任务</strong>进行了全面验证。</p>
<p><strong>局限性</strong>：论文提到，最佳智能体配置高度依赖于任务和数据集，理解这种依赖关系仍是一个开放问题。在OGBench的某些运动任务上，使用固定超参数时性能未达最优，表明方法可能对超参数（如递归自举中的 (r_g)）敏感。</p>
<p><strong>后续启示</strong>：1) Extremum Flow Matching为离线强化学习中的分布极值估计和条件化提供了扩散模型之外的一种高效、确定性选择；2) 模块化的智能体设计框架为根据具体任务和数据特性定制算法提供了灵活性；3) 递归自举机制为利用生成模型处理组合性回报结构、实现长视距推理提供了有效途径。未来工作可探索自动配置模块组合、更稳定的训练方案，以及将方法扩展到语言指令等更丰富的目标指定方式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对离线目标条件强化学习中高质量专家演示稀缺、限制模仿学习扩展的问题，提出利用更易获取的次优开放数据。核心方法为Extremum Flow Matching，基于Flow Matching的确定性传输和支持任意源分布的特性，估计学习分布的极值，并开发出多种以当前和目标观察为条件的目标条件策略算法。实验在OGBench基准上评估了2D非抓握推动任务的性能，并成功在Talos类人机器人上实现了基于高维图像观察的复杂厨房操作任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.19717" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>