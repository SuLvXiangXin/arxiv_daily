<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ROSA: Harnessing Robot States for Vision-Language and Action Alignment - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ROSA: Harnessing Robot States for Vision-Language and Action Alignment</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.13679" target="_blank" rel="noreferrer">2506.13679</a></span>
        <span>作者: Wen, Yuqing, Gu, Kefan, Liu, Haoxuan, Zhao, Yucheng, Wang, Tiancai, Fan, Haoqiang, Sun, Xiaoyan</span>
        <span>日期: 2025/06/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将预训练的视觉-语言模型（VLM）通过微调直接适配到机器人控制任务，以构建视觉-语言-动作（VLA）模型，是领域内的主流方法。然而，这种直接微调范式存在显著的时空差距，导致数据效率低下并严重依赖人力收集。空间上，VLM预训练于高层语义理解（如识别物体类别），而机器人动作执行需要低层、细粒度的3D空间信息（如物体的精确位置）。时间上，VLM擅长解释当前图像内容，而VLA模型需要预测未来的机器人动作。</p>
<p>本文针对上述VLM与VLA对齐过程中的核心痛点，提出了一个名为ROSA的新训练范式。其核心思路是：通过引入一个可自动收集的机器人状态估计任务作为辅助监督，显式地增强模型对3D空间的感知和对自身状态的“自意识”，从而更高效地桥接视觉-语言空间与机器人动作空间之间的差距。</p>
<h2 id="方法详解">方法详解</h2>
<p>ROSA的整体框架基于标准的LLaVA架构，其目标是通过联合训练，使模型能够根据视觉观察和语言指令，输出机器人动作（未来目标状态）或当前机器人状态。</p>
<p><img src="https://arxiv.org/html/2506.13679v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：ROSA架构总览。模型采用经典VLM架构：视觉编码器处理RGB图像生成特征，经投影器映射为视觉Token；文本编码器处理语言指令生成文本Token；两者拼接后输入大型语言模型（LLM），以自回归方式预测输出Token序列，最终反标记化为连续的机器人动作或状态值。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>视觉-语言模块</strong>：采用CLIP ViT-L/14作为视觉编码器，Qwen-2.5-7B作为LLM主干，通过一个两层的MLP投影器将视觉特征对齐到文本嵌入空间。</li>
<li><strong>机器人标记化与反标记化</strong>：为了适应LLM的离散Token输出，将连续的7自由度机器人动作/状态（末端执行器的3D位置、3维欧拉角、夹爪开合状态）通过线性量化函数映射为离散整数Token。推理时则执行逆映射恢复连续值。</li>
<li><strong>训练目标</strong>：使用标准的自回归下一个Token预测交叉熵损失进行联合训练。</li>
</ol>
<p>ROSA的创新性主要体现在其使用的训练数据上，它引入了与专家动作数据格式一致但语义不同的机器人状态估计数据。</p>
<p><img src="https://arxiv.org/html/2506.13679v1/x2.png" alt="训练数据"></p>
<blockquote>
<p><strong>图2</strong>：ROSA使用的两种训练数据。(a) 专家动作预测数据：需人工引导机器人完成任务，数据形式为（当前观测，下一时刻动作，任务指令）。(b) 机器人状态估计数据：通过脚本控制机器人在安全范围内执行随机动作自动收集，数据形式为（当前观测，当前状态，统一的状态查询指令）。</p>
</blockquote>
<p><strong>专家动作数据</strong>：形式为 <code>(观测o_t, 动作a_t, 任务指令l)</code>，其中动作 <code>a_t</code> 代表机器人下一时刻应达到的目标状态。<br><strong>机器人状态数据</strong>：形式为 <code>(观测o_t, 状态s_t, 状态指令l_state)</code>，其中状态 <code>s_t</code> 代表机器人在当前时刻的真实状态，指令统一为“What is the current state of the robot?”。状态数据与动作数据在格式上完全相同（都是7维向量），仅语义不同。</p>
<p>通过以固定比例（1:4）混合这两种数据并进行联合训练，ROSA使模型同时学习“根据当前观测推断自身状态”和“根据任务指令预测未来动作”。状态估计任务迫使模型从图像中理解精确的3D空间关系，增强了空间感知能力，并为动作预测提供了更清晰的空间上下文，从而同时缓解了空间和时间上的对齐差距。自动收集状态数据的方式避免了额外的人力成本。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在RLBench模拟环境和真实世界的WidowX机器人平台上进行。基线方法为仅使用专家动作数据微调同一VLM架构。评估指标为任务成功率（SR）。</p>
<p><strong>关键定量结果</strong>：<br>在RLBench的12个任务上，随着专家数据量从50条增至500条，ROSA始终优于基线。在数据稀缺时（50/100条），ROSA平均SR分别提升7.1%和11.4%；即使在数据充足时（500条），仍有1.6%的提升。</p>
<p><img src="https://arxiv.org/html/2506.13679v1/x5.png" alt="RLBench结果表"></p>
<blockquote>
<p><strong>表1</strong>：RLBench上不同专家数据规模下的性能对比。ROSA在所有数据规模下均超越基线，在低数据区域（50，100）提升尤为显著。</p>
</blockquote>
<p>在真实机器人实验中，ROSA的优势更加明显。</p>
<p><img src="https://arxiv.org/html/2506.13679v1/x6.png" alt="真实机器人性能曲线"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人上，随专家数据量增加，ROSA与基线的成功率对比。ROSA在所有数据规模下均显著优于基线，在低数据区域（如50条）甚至将平均成功率提升了约35%。</p>
</blockquote>
<p>在极端数据稀缺的“单样本”实验中（每个任务仅1条专家数据），ROSA在三个测试任务上取得了4%、4%和32%的成功率，而基线成功率均为0。</p>
<p><strong>泛化能力评估</strong>：<br>在真实机器人上测试了涉及未见物体、未见容器及存在干扰物的四个泛化任务。</p>
<p><img src="https://arxiv.org/html/2506.13679v1/x9.png" alt="泛化结果表"></p>
<blockquote>
<p><strong>表3</strong>：在真实机器人未见任务上的性能。ROSA在所有泛化任务上均大幅领先基线，平均成功率达85%（基线为43%），在“Cube in Box”（物体和容器均未见）任务上领先幅度高达60%。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文通过消融实验验证了各组件贡献。主要结论包括：1）<strong>状态数据多样性至关重要</strong>：在固定场景收集的状态数据收益有限，在多样化场景中收集的状态数据能带来最大性能提升。2）<strong>状态指令的作用</strong>：使用统一的状态查询指令训练，比使用任务指令或空指令效果更好。3）<strong>数据混合比例</strong>：专家数据与状态数据比例为4:1时效果最佳。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了<strong>ROSA训练范式</strong>，通过引入机器人状态估计作为辅助任务，显式地增强VLA模型的空间感知和自我状态认知能力，从而更高效地对齐视觉-语言与动作空间。2）提出了一种<strong>低成本、可自动扩展的机器人状态数据收集方案</strong>，无需额外人力标注，显著提升了VLA模型的数据效率。3）在模拟和真实机器人实验中<strong>全面验证了ROSA的有效性</strong>，特别是在低数据区域和泛化任务上表现突出。</p>
<p>论文自身提到的局限性在于，当前方法仅估计机器人末端执行器的状态，未来可以扩展到估计更完整的机器人本体或多关节状态。</p>
<p>本工作对后续研究的启示是：利用自动生成或模拟的辅助监督信号（如物理状态、几何信息）来增强模型的基础空间和物理理解能力，是提升数据效率、降低对大规模专家演示依赖的一个有效且具有前景的方向。这种“状态作为中间表示”的思路可能有助于构建更鲁棒、更具泛化能力的具身智能体。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ROSA方法，旨在解决视觉-语言-动作（VLA）模型在适配过程中存在的时空差距问题：视觉-语言模型（VLM）处理高层语义与当前状态，而机器人控制需低层3D空间信息与未来动作预测，导致直接微调数据效率低下。ROSA的核心是引入机器人状态估计作为桥梁，通过自动化流程获取状态数据，增强模型的空间理解与自我感知能力，从而提升对齐效果。实验表明，该方法在仿真与真实场景中均有效，尤其在低数据条件下性能显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.13679" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>