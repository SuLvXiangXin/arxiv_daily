<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.04412" target="_blank" rel="noreferrer">2602.04412</a></span>
        <span>作者: Wang, Puyue, Hu, Jiawei, Gao, Yan, Wang, Junyan, Zhang, Yu, Dobbie, Gillian, Gu, Tao, Johal, Wafa, Dang, Ting, Jia, Hong</span>
        <span>日期: 2026/02/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于物理的人形机器人控制策略通常依赖于密集的运动捕捉数据，在动力学参数固定的单一模拟器中训练。这类方法存在两个关键局限性：一是训练数据分布有限且稀疏，仅覆盖狭窄的骨架定义、帧率和输入模态，导致运动流形覆盖不全，阻碍了可扩展的学习和公平的方法比较；二是策略会过拟合源域动力学，在遭遇域转移时（如不同的运动模式、地形或物理引擎）表现出灾难性失败。近期基于掩码模仿的方法通过将控制重构为掩码运动修复，虽然能生成视觉上合理的运动，但缺乏泛化鲁棒性，在不同物理引擎中无法保持稳定性。数据碎片化和域转移共同暴露了一个关键缺口：现有方法缺乏一个统一的、鲁棒的表征，能够从高级运动规范映射到扭矩级控制，并跨数据源和物理环境泛化。本文针对这一痛点，提出一个学习框架，旨在能够跟踪来自稀疏运动命令的类人运动，在物理动力学或模拟器的分布变化下鲁棒泛化，并能在线适应未观测到的动态变化。本文核心思路是提出一个两阶段学习框架（HoRD），首先通过历史条件强化学习在多样随机化动力学下训练高性能教师策略，然后通过在线蒸馏将其鲁棒控制能力转移到仅处理稀疏关键点轨迹的基于Transformer的学生策略中。</p>
<h2 id="方法详解">方法详解</h2>
<p>HoRD的整体框架是一个两阶段的教师-学生学习流程，旨在应对部分可观测性下的鲁棒人形控制。</p>
<p><img src="https://arxiv.org/html/2602.04412v2/sec/fig/frame_work_py.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：HoRD框架总览。第一阶段：在模拟中使用PPO训练专家策略π⋆，输入包括特权完整状态观测stfull、密集未来运动意图Ytfull和情节级域随机化参数ψ(e)。共享的HCDR模块将交互历史Ht编码为时序记忆嵌入mt，用于在线动力学推断和自适应调制。第二阶段：可部署的学生策略π仅接收稀疏本体感知stsparse、环境上下文gt和通过SSJR标准化的稀疏运动命令Ytsparse，并通过蒸馏训练以匹配专家动作。</p>
</blockquote>
<p><strong>第一阶段：全观测专家训练</strong>。专家策略π⋆在模拟器中训练，拥有特权信息，包括完整状态stfull（如连杆速度、接触力、精确关节扭矩）和源自高频运动捕捉数据的密集未来运动意图Ytfull。策略网络fθ1接收时序记忆嵌入mt、完整状态stfull、环境上下文gt和密集意图Ytfull，输出控制调制信号αt(1)。专家使用近端策略优化（PPO）进行训练，目标是在域随机化的情节上最大化期望折扣回报J(π⋆)。奖励函数r(·)惩罚与期望运动的偏差、失去平衡、过大的控制努力和摔倒。通过在多样化的ψ(e)分布上进行域随机化训练，专家策略学会处理广泛的潜在动力学和环境变化。</p>
<p><strong>第二阶段：稀疏观测学生蒸馏</strong>。此阶段将专家的行为蒸馏到学生策略πθ2中，学生策略在现实的传感约束下运行。它仅接收稀疏观测stsparse（完整状态的子集）和稀疏未来运动提示Ytsparse，但使用相同的时序记忆嵌入mt和提出的SSJR表示来利用历史交互和标准化运动命令。学生策略网络fθ2输出控制调制αt(2)。学生通过监督蒸馏进行训练，其损失函数Ldistill是学生输出与教师输出之间的均方误差（MSE）期望。通过共享时序记忆表示mt和SSJR运动接口，学生学会仅使用自身的历史交互和部分观测来恢复专家的预期行为。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>SSJR（标准化稀疏关节表示）</strong>：为了将高级运动规范与低级扭矩控制解耦，SSJR使用规范人体骨骼模型中的一组稀疏关键关节J来表示未来运动意图Yt。它将大规模运动捕捉数据投影到此规范表示上，抽象掉平台特定细节，同时保留任务相关的运动信息。这使得策略能够直接处理来自不同运动源的稀疏命令，实现了数据可扩展性和跨平台转移。</li>
<li><strong>情节级域随机化</strong>：为了促进鲁棒的跨域泛化，在每个情节e开始时采样潜在物理参数向量ψ(e)（包括惯性属性、接触动力学、系统延迟和随机外力），并在整个情节中保持不变。这种时间一致的动态变化鼓励策略从其观测中推断潜在物理属性，而不是记忆短期波动，从而更好地模拟了转移到不同物理引擎或现实部署时遇到的情况。</li>
<li><strong>HCDR（历史条件动力学表示）</strong>：这是一个用于在线动态适应的潜在表示模块。它使用查询-转换器架构对历史交互Ht进行建模，作为潜在动力学的证据。该模块维护一组可学习的潜在令牌m0，编码训练中遇到的动力学配置的共享先验。通过跨注意力机制，计算得到时序记忆嵌入mt = QFormer(m0, Ht)。mt作为一个紧凑的“动力学指纹”，捕获了未观测到的物理属性（如表面摩擦、意外质量分布）。通过将控制调制αt条件于mt，策略可以“在上下文中”调整其行为。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，HoRD的创新具体体现在：1) 提出了结合历史条件适应（HCDR）和在线蒸馏的两阶段框架，使学生策略能零样本适应未见域，而无需针对每个域重新训练。2) 引入了SSJR，一种平台无关的稀疏关节命令接口，统一了碎片化的运动数据源，解决了数据可扩展性和跨平台泛化问题。3) HCDR模块实现了无需目标域数据的在线自适应，克服了静态域随机化无法完全捕获部署中动态变化的根本限制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在具有29个驱动自由度的Unitree G1风格人形机器人上进行，控制频率为50Hz。训练在Isaac Lab物理引擎中进行，评估则包括零样本转移到Genesis物理引擎。使用AMASS运动数据集，涵盖 locomotion、平衡、操纵式姿势和恢复运动。运动片段被分割、重定向到人形机器人，并使用SSJR表示转换为稀疏关键关节轨迹。</p>
<p><strong>对比基线</strong>：包括MaskedMimic（掩码模仿）、OmniH2O、ExBody2和Hover等强物理基线和掩码模仿基线。</p>
<p><strong>关键实验结果</strong>：<br>表1展示了在四种设置下的测试性能：IsaacLab（域内）、IsaacLab+DR（域内带随机化）、Genesis（域外，零样本）、Genesis+DR（域外带随机化）。HoRD在所有环境和指标上均取得最佳性能。</p>
<p>在域内环境（IsaacLab），HoRD达到90.7%的成功率，运动跟踪误差（全局姿态误差Eg-mpjpe和局部姿态误差Empjpe）比强物理基线降低了2倍以上。在引入测试时观测噪声和扰动的IsaacLab+DR设置下，优势依然保持。相比之下，MaskedMimic即使在已见环境中也表现不佳（成功率32.1%），在域随机化下几乎完全失败（&lt;10%）。其他基线在训练环境中表现尚可，但一旦引入域转移，其成功率和跟踪精度显著下降。</p>
<p>在未见域（Genesis）的零样本转移中，HoRD取得了86.0%的成功率，而其他方法性能严重下降：ExBody2为73.1%，OmniH2O为72.3%，Hover为16.2%。这证明了HoRD卓越的跨域泛化能力。</p>
<p><img src="https://arxiv.org/html/2602.04412v2/sec/fig/radar_eg_mpjpe_icml.png" alt="雷达图1"></p>
<blockquote>
<p><strong>图3</strong>：各方法在四种测试设置下的全局姿态误差（Eg-mpjpe）雷达图。HoRD（橙色）在所有象限（即所有测试条件）中均保持最小的误差面积，表明其最鲁棒的性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.04412v2/sec/fig/radar_empjpe_icml.png" alt="雷达图2"></p>
<blockquote>
<p><strong>图4</strong>：各方法在四种测试设置下的局部姿态误差（Empjpe）雷达图。同样，HoRD（橙色）在所有测试条件下都实现了最低的误差，验证了其精确的关节级跟踪能力。</p>
</blockquote>
<p><strong>消融实验</strong>：论文进行了消融研究，移除了HCDR模块或SSJR表示。结果表明，两者都是关键组件。移除HCDR会导致在域外（Genesis）性能显著下降，验证了历史条件适应对于处理未见动力学的重要性。移除SSJR（即使用密集关节角度作为命令）会损害从稀疏输入泛化的能力，并增加对特定机器人形态的过拟合风险。</p>
<p><strong>定性结果</strong>：论文展示了HoRD在复杂动作（如拳击、起身、高踢、行走、侧移、武术动作）以及不平坦地形上的执行效果，证明了其生成多样化、稳定运动的能力。</p>
<p><img src="https://arxiv.org/html/2602.04412v2/sec/fig/g1_random_push.png" alt="定性结果示例"></p>
<blockquote>
<p><strong>图5</strong>：HoRD控制的机器人在随机外力推动下的稳定性展示。机器人能够有效抵抗扰动并恢复平衡，体现了策略的鲁棒性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个两阶段学习框架（HoRD），能够将稀疏的根相对3D关节关键点轨迹映射到扭矩级人形控制，并具有强大的跨域泛化能力。</li>
<li>引入了历史条件强化学习框架（HCDR），能够从近期状态-动作序列推断潜在动力学参数，实现在线适应分布变化，显著提高了跨域泛化和零样本转移性能；同时发布了包含7000多种多样化人形运动的大规模轨迹数据集。</li>
<li>提出了标准化稀疏关节表示（SSJR），解决了运动数据碎片化问题，统一了不同数据源，使策略能够直接处理来自多样化运动源的稀疏关键点命令。</li>
</ol>
<p><strong>局限性</strong>：论文提到，学生策略的性能依赖于教师策略的质量。如果教师在某些动态域中表现不佳，那么通过蒸馏得到的学生策略在这些域中也会受到限制。此外，虽然HCDR支持在线适应，但其适应速度受历史窗口长度和网络架构的影响。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>历史条件适应</strong>：利用近期交互历史在线推断和补偿动力学变化是一个有前景的方向，可应用于其他需要应对不确定动态系统的机器人任务。</li>
<li><strong>标准化接口</strong>：像SSJR这样的标准化、抽象化的运动命令接口，有助于整合异构数据源和促进跨不同机器人平台的策略迁移，对构建通用机器人控制系统具有重要意义。</li>
<li><strong>教师-学生蒸馏范式</strong>：该范式结合了在特权信息（模拟）中训练高性能“教师”与在受限观测中部署轻量级“学生”的优势，是连接仿真训练与现实部署的有效途径。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文HoRD解决人形机器人在动态、任务或环境变化下性能下降的鲁棒控制问题。方法采用两阶段框架：首先通过历史条件强化学习训练教师策略，使其从状态-动作轨迹推断动态上下文以在线适应；其次通过在线蒸馏将教师能力转移至基于Transformer的学生策略，处理稀疏关节轨迹。实验表明，HoRD在未见域和外部扰动下优于强基线，提升了鲁棒性和迁移性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.04412" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>