<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RobustDexGrasp: Robust Dexterous Grasping of General Objects - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RobustDexGrasp: Robust Dexterous Grasping of General Objects</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.05287" target="_blank" rel="noreferrer">2504.05287</a></span>
        <span>作者: Zhang, Hui, Wu, Zijian, Huang, Linyi, Christen, Sammy, Song, Jie</span>
        <span>日期: 2025/04/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧抓取是机器人操作的基础能力。当前主流方法主要包括基于静态姿态预测的抓取和基于动态动作学习的抓取。基于姿态的方法通常需要精确的物体模型或预扫描，通过优化接触点或指尖位置来生成静态抓取姿态，并以开环方式执行，这限制了其对未知物体的泛化能力以及在遇到扰动时的实时适应性。基于动态学习的方法，特别是依赖人类或机器人演示数据的方法，数据收集成本高，泛化到分布外场景受限；而基于强化学习的方法，则常因复杂的手-物交互难以探索和模拟，局限于特定场景（如类别级泛化）并受仿真到真实差距的影响。</p>
<p>本文针对上述痛点，旨在实现使用单视图视觉输入、对多种未知物体的零样本动态灵巧抓取，并具备对扰动的鲁棒适应能力。其核心思路是：设计一种专注于潜在接触区域局部形状的、手中心的物体表示方法，以提升对形状变化和感知不确定性的泛化能力；并采用一种结合模仿学习和强化学习的混合课程学习范式，在有限感知条件下，先高效蒸馏抓取能力，再探索对扰动的适应。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法采用教师-学生策略训练范式，整体框架如图2所示。首先，在仿真环境中利用特权信息（全视角物体点云、手-物接触状态与冲量）通过强化学习训练一个教师策略。然后，使用混合课程学习方法训练一个学生策略，其输入为真实机器人上可获取的感知信息：单视图物体点云和带噪声的本体感知（无触觉）。学生策略最终被部署到真实机器人上。</p>
<p><img src="https://arxiv.org/html/2504.05287v3/sections/FIG/pipeline.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：框架总览。首先使用具有特权视觉-触觉感知的教师策略进行RL训练。然后使用我们的混合课程学习方法训练学生策略，该方法以单视图物体点云和带噪声的本体感知为输入，从用于高效教师策略蒸馏的模仿学习开始，逐渐过渡到用于扰动下探索的强化学习。</p>
</blockquote>
<p><strong>核心模块一：手中心物体形状表示</strong><br>为了在有限视角带来的形状不确定性下，有效捕捉对抓取泛化至关重要的形状特征，本文提出了一种稀疏的手中心物体表示。</p>
<p><img src="https://arxiv.org/html/2504.05287v3/sections/FIG/shape.png" alt="手中心形状表示"></p>
<blockquote>
<p><strong>图3</strong>：手中心形状表示。通过拼接从每个手指关节（包括手腕和指尖）到物体点云最近点的距离向量，构建一个51维向量。</p>
</blockquote>
<p>如图3所示，该表示通过连接从每个手指关节（包括手腕和指尖）到物体点云最近点的距离向量，构建一个51维向量。这种紧凑的表示编码了与交互相关的局部形状特征，而非精细的全局几何，从而过滤了非接触区域的无关形状变化，有利于泛化到新物体形状，并提高了感知不确定性下的鲁棒性。</p>
<p><strong>核心模块二：视觉-触觉策略训练</strong><br>教师策略的观察空间为 $s_t = (a_{t-1}, q_t, O_t, c_t)$，包括上一步动作、当前关节角度、实时全视角物体点云、以及各手指连杆与物体的二元接触状态 $b_t$ 和冲量 $f_t$。通过特征提取函数 $\Phi$ 从 $(a, q, O)$ 中提取手中心距离向量 <code>d</code>、各连杆到桌面的垂直距离 <code>h</code>、手腕位姿 <code>T</code> 以及关节跟踪误差 $\Delta q$ 等特征，与 <code>q</code> 和 <code>c</code> 一同输入策略网络。</p>
<p>奖励函数设计为 $r = r_{dis} + r_{contact} + r_{height} + r_{reg}$，分别鼓励手接近物体 ($r_{dis}$)、促进期望接触并惩罚非期望接触（如自碰撞、与桌面碰撞）($r_{contact}$)、避免与桌面过度接近 ($r_{height}$)，以及正则化不必要的物体运动和极端机器人动作 ($r_{reg}$)。具体权重见补充材料。</p>
<p><strong>核心模块三：学生策略训练与混合课程学习</strong><br>学生策略的观察输入为单视图物体点云 $\hat{O}_t$ 和带噪声的关节状态 $\hat{q}_t$。由于缺乏真实的触觉信息，本文使用一个基于LSTM的编码器，从关节状态和动作历史中重建接触状态 $\hat{c}_t = {\hat{b}_t, \hat{f}_t}$。训练时还引入了摩擦系数、关节状态感知噪声以及底层PD控制器增益的随机化，以模拟真实世界扰动。</p>
<p><img src="https://arxiv.org/html/2504.05287v3/sections/FIG/hardware1.png" alt="硬件设置"></p>
<blockquote>
<p><strong>图4</strong>：硬件设置。使用UR5机械臂搭配Allegro灵巧手，以及一个顶视的RealSense D435i相机。</p>
</blockquote>
<p>混合课程学习是学生策略训练的关键。训练开始时，主要使用模仿学习（IL），包含两个损失：接触重建损失 $L_{re}$（重建接触状态 $c_t$）和动作模仿损失 $L_{act}$（模仿教师策略动作），以高效蒸馏教师策略的抓取能力。随后，通过因子 $\lambda$ 逐渐减小 $L_{act}$ 的权重，同时按 $(1-\lambda)$ 的比例增加强化学习奖励的权重（$L_{re}$ 权重固定），使训练逐渐过渡到强化学习（RL）。这一过渡鼓励学生策略在观察噪声和执行器不准确性的扰动下进行探索和适应。学生策略网络使用教师策略权重初始化以加速训练。</p>
<p><strong>创新点</strong><br>与现有方法相比，本文的创新点具体体现在：1) 提出了专为灵巧抓取设计的、稀疏的手中心物体形状表示，增强了泛化性和对感知不确定性的鲁棒性；2) 设计了从模仿学习平滑过渡到强化学习的混合课程学习范式，在有限感知条件下兼顾了训练效率和对扰动的适应能力探索。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验平台为UR5机械臂搭配Allegro灵巧手，使用顶视RealSense D435i相机（图4）。策略在Raisim仿真中训练，使用了35个训练物体。评估指标为抓取成功率（将物体提升0.1米并稳定保持至少5秒）。</p>
<p><strong>大规模评估</strong>：</p>
<ol>
<li><strong>仿真评估</strong>：在Objaverse数据集的247,786个未见过的物体上进行测试，物体按尺寸分为小、中、大。结果如表2所示，整体成功率达97.0%，即使对较难抓取的小物体，成功率也达到94.9%。</li>
<li><strong>真实世界评估</strong>：在512个真实物体上测试，这些物体在形状、重量、材料、尺寸上差异巨大，包括可变形物体。结果如表3所示，平均成功率达94.6%，显示了出色的仿真到真实迁移能力和对各类物体的泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.05287v3/sections/FIG/objects.png" alt="物体示例"></p>
<blockquote>
<p><strong>图5</strong>：真实世界评估中使用的部分物体示例，展示了其多样性。</p>
</blockquote>
<p><strong>方法对比</strong>：<br>在30个3D打印的ShapeNet物体上，与多个基线方法进行对比（表4）。本文方法取得了92.0%的成功率，优于朴素基线（63.3%）、基于优化的DexGraspNet（60.7%）和SpringGrasp（77.1%），与先进的RL方法DextrAH-G（92.7%）性能相当。进一步的扰动测试（施加外部力）表明，本文方法在仿真和真实世界中均比DexGraspNet更具鲁棒性，成功率下降更少。</p>
<p><img src="https://arxiv.org/html/2504.05287v3/sections/FIG/spring_obj.png" alt="弹簧抓取物体"></p>
<blockquote>
<p><strong>图7</strong>：用于与SpringGrasp方法对比的物体示例。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>消融研究验证了关键组件的贡献。移除手中心表示会导致性能显著下降（成功率从<del>95%降至</del>80%），证实了其对于形状泛化的重要性。移除混合课程学习中的模仿学习或强化学习阶段，均会导致性能下降，表明两者结合的必要性。移除接触重建或动作历史观察也会损害性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个面向通用物体的鲁棒灵巧抓取框架，具备实时适应扰动的能力。</li>
<li>设计了一种适用于真实世界灵巧抓取的稀疏手中心物体形状表示，该表示关注交互潜力，提升了对形状变化和感知不确定性的鲁棒性。</li>
<li>提出了一种混合课程学习方法，在有限感知条件下，整合模仿学习以高效蒸馏抓取行为，并结合强化学习以探索适应性运动。</li>
<li>方法在仿真中对247,786个未知物体达到97.0%的成功率，在真实机器人上对512个未知物体达到94.6%的成功率，并展示了对外部力等扰动的鲁棒适应性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，大多数失败案例源于两个因素：对于薄或小物体的噪声点云观测，以及由于执行器硬件限制导致对于重或滑物体提供不足的手指扭矩。</p>
<p><strong>启示</strong>：本文工作表明，通过精心设计的、任务相关的物体表示和分阶段的训练策略，可以显著提升灵巧抓取的泛化能力和鲁棒性。手中心表示将注意力集中于交互区域，为处理感知不确定性提供了新思路。教师-学生框架与混合课程学习的结合，为在仿真中学习复杂技能并迁移到具有感知局限的真实世界提供了一种有效范式。后续研究可进一步探索更精细的触觉重建、结合多模态感知，以及解决执行器扭矩限制带来的挑战。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RobustDexGrasp框架，旨在解决灵巧机器人仅凭单视图视觉输入零样本动态抓取各类物体时，对观测噪声、碰撞等干扰缺乏鲁棒性的核心问题。关键技术采用基于手指关节与物体表面动态距离向量的手中心形状表示，以局部接触区域替代全局几何，提升泛化能力；并融合特权教师策略与混合课程学习，使学生策略能蒸馏抓取能力并自适应干扰。实验显示，方法在247,786个模拟物体上成功率97.0%，在512个真实物体上达94.6%，验证了卓越的泛化性与抗干扰性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.05287" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>