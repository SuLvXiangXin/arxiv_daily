<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.09532" target="_blank" rel="noreferrer">2504.09532</a></span>
        <span>作者: Wen, Congcong, Bethala, Geeta Chandra Raju, Hao, Yu, Pudasaini, Niraj, Huang, Hao, Yuan, Shuaihang, Huang, Baoru, Nguyen, Anh, Wang, Mengyu, Tzes, Anthony, Fang, Yi</span>
        <span>日期: 2025/04/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人的移动操作（loco-manipulation）整合了全身移动与灵巧操作，是机器人学的一项根本性挑战。其难点不仅在于高自由度协调与动态平衡维持，更在于如何理解人类的高级指令并将其转化为连贯的具身动作序列。现有主流方法可分为两类：一类是基于语义映射或语言条件策略的学习方法，它们依赖大量人工标注或预定义任务结构，可扩展性与适应性有限；另一类是近期兴起的基础模型方法，如SayCan和PaLM-E，它们展现了强大的多模态表示与推理能力，但现有工作大多局限于孤立的移动或操作任务，且多在仿真或非人形平台上评估，难以应对人形机器人高维度、强耦合的移动操作挑战。近期基于LLM的行为规划框架（如Wang等人工作）仍未充分利用大语言模型的推理能力，通常依赖于指令到动作的简单映射。</p>
<p>本文针对上述核心痛点——如何弥合高级人类意图与低级运动控制之间的鸿沟，特别是在长视野、非结构化环境中——提出了一个新视角：将人形机器人规划重新概念化为一个认知过程，而非纯粹的几何搜索或语义理解。本文的核心思路是提出Humanoid-COA框架，其通过一种具身动作链推理机制，将高级指令逐步分解为结构化的移动与操作原语序列，从而实现零样本的移动操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的Humanoid-COA框架遵循经典的感知-推理-行动范式，其整体流程如图1所示。</p>
<p><img src="https://arxiv.org/html/2504.09532v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提出的人形代理移动操作框架，包含三个阶段：(i) 感知与理解：将自我中心的观测转换为场景描述，并与人类指令一起进行分词以供推理；(ii) 推理与规划：一个带有具身动作链推理的大语言模型通过可供性、空间和全身推断生成符号化的动作计划；(iii) 执行与控制：将计划落地为原语命令，并转化为低级电机控制以执行。</p>
</blockquote>
<p><strong>1. 感知与理解</strong>：此阶段将环境上下文和任务意图准备为文本形式。给定自我中心的RGB观测O，使用预训练的视觉-语言基础模型将其转换为自然语言场景描述S。同时，接收用户的自然语言指令I。随后，场景描述S和指令I被分词为离散的token序列，作为推理模块的输入。</p>
<p><strong>2. 推理与规划</strong>：这是框架的核心创新阶段，即<strong>具身动作链推理</strong>。如图2所示，该机制通过三个互补的推理过程，将高级任务目标分解为结构化的移动操作原语序列。</p>
<p><img src="https://arxiv.org/html/2504.09532v3/x2.png" alt="推理机制示例"></p>
<blockquote>
<p><strong>图2</strong>：所提出的具身动作链推理示例。给定一个自然语言指令，该框架依次执行物体可供性分析以提取目标属性和可行动作、区域空间推理以处理遮挡和优先搜索区域、以及全身运动推断以将符号化原语映射到人形机器人的感知运动系统。</p>
</blockquote>
<ul>
<li><strong>物体可供性分析</strong>：旨在评估物体的物理属性（如尺寸、重量、刚性、可移动性、可操作性），以确定在物体层面可行的动作。形式化上，每个感知到的物体e_i关联一个可供性向量a_i。该分析将可供性信息整合到中间推理状态中，确保推理过程明确反映物理可行性，从而在生成动作时修剪假设空间，仅保留语义一致且物理可行的动作。</li>
<li><strong>区域空间推理</strong>：针对现实环境中常见的部分可观测性（目标被遮挡或不在视野内）问题。该过程基于上下文线索和语义先验，为被遮挡或未观测到的目标假设合理的位置。具体操作是定义一组候选区域，并为每个区域分配一个基于CLIP视觉-语义相似度的权重，形成概率化的空间先验。推理状态据此更新，从而在动作生成时偏向于最有可能存在隐藏目标的区域，提高了在部分可观测环境下的鲁棒性。</li>
<li><strong>全身运动推断</strong>：确保规划的动作与人形机器人的运动学和动力学约束相一致。该过程对机器人的可控关节进行建模，每个关节j_m由一个可行性描述符b_m刻画，包含运动范围、扭矩限制和功能角色。通过整合关节可行性到推理状态中，约束动作生成过程，使其与涉及关节的运动学/动力学可行性对齐，从而保证计划的动作在身体层面是安全且可执行的。</li>
</ul>
<p>整个推理过程被形式化为一个序列生成问题，如公式(4)所示，模型依次生成中间推理状态R和接地的动作序列A。</p>
<p><strong>3. 执行与控制</strong>：此阶段将符号化的动作链A落地为具体的机器人行为。框架定义了一个结构化的动作库L（如表I所示），包含感知引导、移动和操作三类原语。执行过程被建模为一个策略，将符号动作映射为低级的电机控制指令τ，从而闭环整个感知-推理-行动流程。</p>
<p>与现有方法相比，本文的创新点具体体现在：将基础模型推理专门应用于人形移动操作这一复杂场景；提出了一个集成的、分步的认知推理机制（CoA），而非简单的指令-动作映射；该机制明确融合了物体物理属性、环境空间上下文和机器人本体约束，确保了生成计划的物理可实现性、鲁棒性和可解释性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>机器人平台与环境</strong>：在真实世界中使用两个双足人形机器人进行实验：Unitree H1-2（27自由度）和Unitree G1（21自由度）。测试环境包括一个开放区域和一个标准公寓环境（卧室、浴室、厨房、客厅）。</li>
<li><strong>实现细节</strong>：使用GPT-4V作为视觉-语言模型生成场景描述，使用GPT-4作为大语言模型进行任务规划。集成一个经过训练的策略使机器人在移动和操作时能在外部负载下保持稳定。系统采用迭代反馈循环，在执行每个动作后整合新观测以实时调整计划。</li>
<li><strong>基线方法</strong>：对比了三种代表性基线：1) <strong>Loco-Manipulation Planning</strong>[25]：使用可达性地图进行图搜索；2) <strong>Translated GPT3</strong>[26]：从语言模型中提取可操作知识进行零样本规划；3) <strong>LLM Behavior Planner</strong>[14]：利用接地语言模型进行人形移动操作规划。</li>
<li><strong>评估指标</strong>：使用两个指标：<strong>可执行性</strong>（生成的行动序列在语法上是否有效且可在机器人上执行）和<strong>成功率</strong>（执行序列是否完成了预期任务目标）。</li>
<li><strong>任务设计</strong>：系统评估了操作、移动和移动操作三类任务，每类包含简单和复杂两个难度级别。</li>
</ul>
<p><strong>关键实验结果</strong>：<br><strong>1. 操作任务性能</strong>：结果总结于表II。在简单任务（如物体抓取、物体重定位）上，Humanoid-COA实现了近乎完美的可执行性和成功率（如抓取成功率96.6%），显著优于所有基线。在需要多步推理的复杂任务（如空间放置、顺序操作、重排）上，优势更加明显。例如，在最具挑战性的重排任务上，本文方法的成功率达到73.3%，而LLM Behavior Planner为40.0%，Translated GPT3甚至为0.0%。这证明了CoA推理在长视野场景中的鲁棒性。</p>
<p><strong>2. 移动任务性能</strong>：结果总结于表III。在简单移动任务（如接近目标、导航至地标）上，所有方法表现均较好。然而，在需要结合空间推理的复杂移动任务（如搜索并接近、多房间导航）上，Humanoid-COA展现出明显优势。例如，在“搜索并接近”任务中，本文方法的成功率（93.3%）远高于LLM Behavior Planner（73.3%），突显了区域空间推理在处理部分可观测性方面的价值。</p>
<p><strong>3. 移动操作任务性能</strong>：结果总结于表IV。Humanoid-COA在各项移动操作任务上均取得最佳性能。特别是在“获取并放置”和“顺序清洁”这类需要紧密协调移动与操作的长视野任务中，成功率达到80.0%和76.6%，而表现次佳的基线（LLM Behavior Planner）分别为60.0%和53.3%。这验证了框架整合全身推理的有效性。</p>
<p><img src="https://arxiv.org/html/2504.09532v3/x3.png" alt="实验场景"></p>
<blockquote>
<p><strong>图3</strong>：两个机器人（Unitree H1-2和G1）在两种不同场景（开放区域和公寓环境）中执行的真实世界人形移动操作任务。每个任务由人类指令指定（左），机器人执行相应的动作序列来完成它（右），涵盖操作、移动和集成的移动操作。</p>
</blockquote>
<p><strong>4. 消融实验</strong>：表IV也包含了消融研究结果。移除“可供性分析”导致在涉及重物或易碎物体的操作任务上性能下降；移除“空间推理”则显著损害了需要搜索被遮挡物体的任务性能；移除“全身推断”会导致生成一些物理上不可行的动作，从而降低整体成功率。这验证了CoA推理中每个组件的必要性及其互补性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个集成基础模型推理、用于自然语言指令下零样本人形移动操作的代理框架（Humanoid-COA）。</li>
<li>设计了一种新颖的具身动作链推理机制，通过可供性分析、空间推理和全身推断，将高级人类意图分解为可执行的全身行为序列。</li>
<li>在两种真实人形机器人（Unitree H1-2和G1）上进行了广泛实验，证明了该框架在多样化移动操作任务上具有卓越的零样本泛化能力，显著优于现有基线。</li>
</ol>
<p><strong>局限性</strong>：论文提到，框架的性能在一定程度上依赖于所使用的基础模型（GPT-4V/GPT-4）的推理和描述能力。此外，实时推理和规划的速度可能受到大模型响应时间的影响。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li>本文展示的“认知式”规划范式（分解、推理、落地）可推广至其他需要复杂任务分解和具身推理的机器人领域。</li>
<li>探索更轻量级或领域自适应的大模型，以提升系统的实时性和对特定机器人平台的适配性，是一个有价值的方向。</li>
<li>如何将更丰富的传感器模态（如触觉、力觉）和低级控制器的反馈更紧密地集成到推理循环中，以处理动态变化和不确定性，是未来重要的扩展。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Humanoid-COA框架，解决人形机器人零样本移动操作中“如何将人类高级指令转化为连贯的具身行动序列”的核心难题。其关键技术是**具身行动链机制**，通过多模态基础模型进行可供性分析、空间推理与全身行动推理，将指令分解为结构化的移动与操作基元序列。实验在Unitree H1-2和G1机器人上进行，在开放区域与公寓环境中，该框架在操作、移动及移动操作任务上均显著优于现有基线，展现出对长周期、非结构化场景的鲁棒泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.09532" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>