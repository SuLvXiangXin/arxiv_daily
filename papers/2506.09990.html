<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.09990" target="_blank" rel="noreferrer">2506.09990</a></span>
        <span>作者: Xiao Ma Team</span>
        <span>日期: 2025-06-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人视觉运动策略的主流范式是前向预测，即模型基于当前观测预测下一个或一小段动作。尽管ACT、Diffusion Policy等方法通过动作分块或扩散建模来缓解问题，但它们本质上仍是近视的，优化目标是单步或短视距的动作预测，而非确保长视距任务的最终成功。这导致了执行过程中复合误差的积累，即早期的小误差会随着时间推移被放大，最终导致任务失败。</p>
<p>本文从一个相反的角度切入该问题，提出了“动作链”的新视角。其核心思路是：将轨迹生成过程反转，从一个编码了任务特定目标的关键帧动作开始，以自回归的方式反向推理生成整个动作序列，从而将全局目标作为强约束来指导每个局部动作的生成，从根本上缓解复合误差。</p>
<h2 id="方法详解">方法详解</h2>
<p>Chain-of-Action (CoA) 的核心是轨迹自回归建模，其整体流程是一个反向生成过程。模型输入为初始时刻的观测 O（包括多视角图像 I 和本体感知状态 S），输出是从当前时刻到关键帧的完整动作序列 a_{1:T}。其独特之处在于生成顺序：首先预测代表任务目标的关键帧动作 a_T，然后以 a_T 和已生成的后序动作为条件，自回归地预测前一个动作，直至生成当前待执行的动作 a_1。这形成了一个从目标（全局）到起始状态（局部）的“动作思维链”。</p>
<p><img src="https://arxiv.org/html/2506.09990v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Chain-of-Action 基于轨迹自回归建模的框架。左侧展示了网络架构（训练阶段），右侧展示了执行过程。模型编码视觉和本体感知观测，并通过自回归解码器从预测的关键帧动作开始反向生成动作序列。绿色表示关键帧动作 a_T，后续步骤用渐变色可视化。</p>
</blockquote>
<p>该框架通过四个互补的设计来实现稳定训练和可靠执行：</p>
<ol>
<li><strong>连续动作标记表示</strong>：为避免离散化带来的分辨率损失和误差累积，CoA 使用连续的动作潜在表示。动作通过一个线性编码器 f_enc 映射为连续标记 x_t。为解决连续潜在空间缺乏约束的问题，论文引入了潜在一致性损失 ℒ_consistency = ∥x̂_t - f_enc(a_t)∥²，在训练时对齐预测的潜在标记和真实动作编码后的潜在标记，为自回归解码提供时序一致性正则。</li>
<li><strong>局部动作建模（多标记预测，MTP）</strong>：反向自回归结构擅长传播全局意图，但可能忽略局部动作间的紧密依赖。为此，在训练时，CoA 让 Transformer 解码器的最后 K 层并行预测未来 K 个动作标记（即 x̂_{t}, x̂_{t+1}, ..., x̂_{t+K-1}）。这使得模型在一次前向传播中能感知短时距内的动作相互依赖，增强了局部连贯性和训练稳定性，推理时则移除此设计。</li>
<li><strong>动态停止</strong>：在连续动作空间中，没有离散的序列结束（EOS）标记来终止生成。CoA 设计了一个基于距离的停止机制：当解码器预测出的动作（经解码器 f_dec 还原后）与当前机械臂末端执行器的状态足够接近时，停止生成。这表示反向生成的轨迹已成功“回溯”到当前状态，实现了可变长度轨迹的生成。</li>
<li><strong>反向时序集成</strong>：为减少闭环执行时的方差，CoA 提出了专为反向生成设计的集成策略。与ACT等前向方法的时序集成不同，CoA 以预测的关键帧动作为锚点，对齐多条反向生成的子轨迹进行集成。由于整个轨迹的生成都依赖于关键帧动作的准确性，提升该锚点的精度能有效约束整个轨迹的复合误差。</li>
</ol>
<p>网络架构与ACT类似，使用4层Transformer编码器和7层Transformer解码器。训练时总损失为动作空间回归损失和潜在空间一致性损失的加权和，并包含MTP正则：ℒ_total = Σ_t Σ_k (‖â_{t+k-1}^k - a_{t+k-1}‖² + λ₁‖x̂_{t+k-1}^k - f_enc(a_{t+k-1})‖²)。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验主要在模拟环境RLBench和真实世界机器人上进行。对比的基线方法包括：从头训练的视觉运动策略（ACT, Diffusion Policy）、微调的通用策略（Octo）以及基于3D的分层方法（PerAct, RVT-2等）。主要评估指标是任务成功率。</p>
<p><img src="https://arxiv.org/html/2506.09990v2/x4.png" alt="总体结果对比"></p>
<blockquote>
<p><strong>图4</strong>：在RLBench-60任务集上，CoA相比ACT和Diffusion Policy的成功率提升排序（从高到低）。右侧小表显示，CoA平均成功率为55.2%，显著高于ACT的38.9%和DP的32.6%。</p>
</blockquote>
<p>关键定量结果如下：</p>
<ul>
<li><strong>RLBench-60</strong>：CoA平均成功率达55.2%，超过ACT 16.3%，超过DP 23.2%。在81.7%的任务上优于ACT，在80%的任务上优于DP。</li>
<li><strong>RLBench-10</strong>：如表1所示，CoA平均成功率为75.6%，优于ACT（48.8%）、DP（41.6%）和Octo（64.4%）。</li>
<li><strong>RLBench-18</strong>：如表2所示，CoA在基于图像的策略中表现最佳（平均37.33%），但仍与依赖3D点云和运动规划器的分层方法存在差距。</li>
<li><strong>真实世界任务</strong>：在8个真实机器人操作任务上，CoA相比ACT取得了15%的成功率提升。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.09990v2/x5.png" alt="空间泛化分析"></p>
<blockquote>
<p><strong>图5</strong>：空间泛化能力分析。左图显示所有方法的成功率都随物体空间方差的增大而下降。中图和右图显示，CoA在所有方差水平上都优于基线，且在更具挑战性（高方差）的设置下优势更明显。下表显示CoA的成功率与空间方差的负相关性最弱（Pearson r = -0.1679），表明其对空间扰动更鲁棒。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09990v2/x6.png" alt="插值与外推分析"></p>
<blockquote>
<p><strong>图6</strong>：在“Push Button”任务上，CoA在分布内（插值）和分布外（外推）配置下的性能。CoA在两种情况下都保持更强的性能，且在外推时性能下降幅度远小于基线，展示了优异的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09990v2/x3.png" alt="定性轨迹与注意力"></p>
<blockquote>
<p><strong>图3</strong>：在10个常用任务上预测的子轨迹可视化。红色路径点为真实轨迹，绿色为模型预测。每条预测轨迹都是从关键帧动作反向生成到当前状态，实现了目标条件的一致性轨迹生成。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09990v2/x7.png" alt="注意力分析"></p>
<blockquote>
<p><strong>图7</strong>：解码器中动作标记间注意力图的可视化。注意力呈现出清晰的链式结构，靠近关键帧（Token 0）的标记关注全局信息，而靠近执行动作（Token T）的标记更关注局部信息，验证了全局到局部的推理过程。</p>
</blockquote>
<p>消融实验验证了四个核心设计的贡献：移除潜在一致性损失会导致训练不稳定；移除MTP会损害局部动作连贯性；移除动态停止会导致过度生成或提前终止；移除反向时序集成会增加执行方差。这些组件共同作用，缺一不可。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了“动作链”这一新颖的轨迹自回归建模范式，通过从目标关键帧反向生成动作序列，将全局目标作为强约束，从根本上缓解了复合误差问题；2）设计了四个关键技术组件（连续动作表示、MTP、动态停止、反向集成），确保了该范式在实践中的可行性与高效性；3）在模拟和真实环境中取得了显著的性能提升，尤其在空间泛化方面表现突出。</p>
<p>论文自身提到的局限性包括：1）自回归生成在推理时需要串行解码，可能带来计算开销；2）关键帧检测依赖于简单的启发式规则（夹爪状态变化或关节速度接近零），在更复杂的任务中可能需要更鲁棒的关键帧定义。</p>
<p>这项工作对后续研究的启示是：改变动作序列的生成方向（从目标到起点）可以作为一种强大的结构先验。这种“目标优先”的推理范式可能不仅适用于机器人操作，也可推广到其他需要长视距规划的序列决策问题中。未来的工作可以探索更智能的关键帧学习机制，或将此范式与扩散模型等其他生成模型结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Chain-of-Action（CoA），用于解决机器人操作中传统前向预测策略因“近视”优化导致的误差累积问题。其核心方法是轨迹自回归建模，通过反向推理生成完整轨迹：首先生成编码任务目标的关键帧动作，再以此为基础自回归生成后续动作，形成从全局到局部的约束。关键技术设计包括连续动作标记、动态停止、反向时间集成与多标记预测。实验表明，CoA在60个RLBench任务和8个真实世界操作任务上取得了最先进的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.09990" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>