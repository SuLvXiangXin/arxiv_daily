<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.23125" target="_blank" rel="noreferrer">2506.23125</a></span>
        <span>作者: Yue Gao Team</span>
        <span>日期: 2025-06-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人学习复杂运动技能（如舞蹈、特技动作）仍然是一个重大挑战。尽管强化学习和模仿学习领域取得了进展，但机器人在学习效率和过程稳定性方面仍存在不足。核心问题在于探索与利用的平衡，这常常导致学习速度缓慢和性能欠佳。人类在学习运动技能时，常依赖外部辅助（如婴儿使用学步车，运动员接受教练的物理指导），这些辅助不仅能加速学习，还能防止学习者陷入无效或不安全的策略。</p>
<p>本文针对机器人学习初期探索效率低下、易陷入局部最优的问题，提出从人类学习过程中获得启发，将自适应辅助力的概念引入机器人强化学习框架。核心思路是：训练一个双智能体系统，其中一个专门的辅助力智能体根据状态施加外力来引导机器人度过困难的学习初期，并随着机器人熟练度的提高逐渐减少辅助，最终使机器人获得无需辅助的鲁棒策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的A2CF框架旨在通过自适应辅助课程力加速人形机器人运动技能学习。其核心是一个双智能体系统，协同训练一个运动策略智能体和一个辅助力智能体。</p>
<p><img src="https://arxiv.org/html/2506.23125v1/x1.png" alt="整体算法框架"></p>
<blockquote>
<p><strong>图1</strong>：算法整体框架。在仿真训练阶段，辅助力智能体施加辅助力以加速学习。通过基于超立方体的力边界课程，辅助力被逐渐优化至零。在现实世界部署时，则不再需要任何辅助力。</p>
</blockquote>
<p><strong>整体框架与问题建模</strong>：任务被建模为一个部分可观测马尔可夫决策过程。状态空间包括机器人观测（角速度、重力投影、关节位置与速度、上一时刻动作、当前指令）和仅在仿真中可用的特权信息。整体训练采用非对称演员-评论家结构：演员（即运动策略智能体）仅接收观测和历史信息，而评论家则接收包含特权信息的完整状态。历史信息通过一个变分自编码器进行压缩，以估计机器人线速度，其潜在表示与估计的速度、观测一起作为两个智能体的输入。使用PPO算法进行优化。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>扩展的动作空间</strong>：动作空间被扩展为 <code>a_t = [a_t^motion, a_t^assi]</code>。<code>a_t^motion</code> 对应期望关节位置，由PD控制器执行；<code>a_t^assi</code> 对应一个6维空间力 <code>F_t = [f_t, m_t]</code>（包含线性力和力矩），施加于机器人的基座连杆（骨盆）。两个智能体通过联合动作学习器进行协作。</li>
<li><strong>辅助力课程</strong>：受人类学习启发，引入课程机制来调节辅助力。为辅助力智能体定义一个以原点为中心的6维超立方体有界动作空间 <code>ℬ_k</code>，其半宽 <code>η_k</code> 随训练迭代 <code>k</code> 自适应更新。更新规则（算法1）基于所施加力的归一化幅度 <code>‖F_k‖/‖η_k‖</code>：若低于阈值 <code>(1-ϵ)</code>，则推断辅助贡献减小，将 <code>η_k</code> 缩小 <code>(1-δ)</code>；若高于阈值 <code>(1+ϵ)</code>，则扩大 <code>η_k</code>。一旦运动策略掌握技能，也会强制衰减 <code>η_k</code> 以消除外部支持。</li>
<li><strong>空间力的初始分布</strong>：为辅助力超立方体设计任务依赖的初始边界 <code>η_0</code>，为辅助力智能体提供强先验。例如，行走任务主要在xy平面需要辅助；而后空翻任务在不同阶段（站立、起跳、空中、落地）对xy和z方向的辅助需求不同。这编码了人类启发的先验知识。</li>
<li><strong>具有特权信息的辅助力智能体</strong>：辅助力智能体仅在仿真中运行，因此可以利用仿真中的特权信息（如地形信息、当前辅助力边界、机器人线速度、域随机化参数等），以提供更精准的辅助，帮助运动策略更快地克服困难、逃离局部最优。</li>
<li><strong>空间力的随机掩码</strong>：为防止运动策略过度依赖辅助力，在施加辅助力时引入随机掩码机制。在每个训练迭代中，以概率 <code>ζ</code> 随机屏蔽辅助力的某些分量：<code>F_t^assi = M_t ⊙ F_t</code>。这迫使机器人偶尔需要在没有辅助的情况下执行任务，从而鼓励其发展自主运动控制能力。</li>
</ol>
<p><strong>任务特定训练框架</strong>：奖励函数由运动奖励 <code>r^motion</code> 和辅助力奖励 <code>r^force</code> 组成。后者仅在运动智能体达到技能掌握目标的80%后才引入，以惩罚过度使用辅助力。论文详细设计了行走、舞蹈和后空翻三个任务的奖励函数。以行走任务为例，其运动奖励包括线性/角速度跟踪、脚部空中时间（奖励）、脚部滑动、运输成本、静止站立、动作变化率和关节加速度（惩罚）等多个项。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：方法在三个基准任务上进行验证：双足行走、编舞舞蹈和后空翻。实验平台包括仿真（用于训练）和真实的Unitree H1人形机器人（用于实物部署）。</p>
<p><strong>对比方法</strong>：基线方法包括：1) <strong>PPO</strong>：标准的无辅助PPO。2) <strong>HoST</strong>：一种在恢复任务中施加固定垂直外力的方法，但其力非自适应且与状态无关。3) <strong>课程学习变体</strong>：将A2CF的课程机制应用于HoST，得到 <code>HoST-C</code>；以及一个没有课程衰减的A2CF变体 <code>A2CF w/o curriculum</code>。</p>
<p><img src="https://arxiv.org/html/2506.23125v1/x2.png" alt="学习曲线对比"></p>
<blockquote>
<p><strong>图2</strong>：在行走、舞蹈和后空翻任务上的学习曲线对比。A2CF在所有任务上都实现了最快的学习速度和最高的最终性能。</p>
</blockquote>
<p><strong>关键定量结果</strong>：在仿真中，与基线方法相比，A2CF<strong>收敛速度快了30%<strong>，并且</strong>失败率降低了40%以上</strong>。例如，在后空翻任务中，A2CF的成功率达到约92%，而PPO和HoST分别只有约50%和65%。在实物机器人上，经过A2CF训练的行走策略能够稳健地跟踪各种速度指令，舞蹈和后空翻技能也成功实现零样本转移。</p>
<p><img src="https://arxiv.org/html/2506.23125v1/x3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：在行走任务上的消融研究。移除非对称评论家、特权信息或随机掩码都会导致性能下降，验证了这些组件的必要性。</p>
</blockquote>
<p><strong>消融实验</strong>：论文对A2CF的关键组件进行了消融研究。移除<strong>非对称评论家</strong>导致学习不稳定且性能下降；移除<strong>特权信息</strong>输入会减慢学习速度；禁用<strong>随机掩码</strong>机制则导致策略对辅助力产生依赖，在撤去辅助后性能崩溃。这些实验证实了每个组件的贡献。</p>
<p><img src="https://arxiv.org/html/2506.23125v1/x4.png" alt="辅助力演变"></p>
<blockquote>
<p><strong>图4</strong>：在行走任务训练过程中，辅助力的边界和实际施加力的演变。可见边界和实际力幅值都随着训练成功收敛而逐渐衰减至零。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.23125v1/x5.png" alt="舞蹈任务辅助力分析"></p>
<blockquote>
<p><strong>图5</strong>：舞蹈任务中，辅助力在不同运动阶段（如跳跃、旋转）的分布热图，显示了辅助力如何根据任务需求进行自适应调整。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.23125v1/x6.png" alt="后空翻任务序列"></p>
<blockquote>
<p><strong>图6</strong>：后空翻任务中，使用A2CF训练的智能体从起跳到落地完整动作序列的定性展示。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.23125v1/x7.png" alt="实物机器人实验"></p>
<blockquote>
<p><strong>图7</strong>：在Unitree H1实物机器人上部署A2CF训练策略的截图，展示了行走、舞蹈动作的实机表现。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.23125v1/extracted/6580097/figures/dance_real.jpg" alt="实物机器人舞蹈"></p>
<blockquote>
<p><strong>图8</strong>：实物机器人执行舞蹈动作的另一个场景。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>A2CF</strong>框架，首次将<strong>自适应辅助课程力</strong>系统性地引入人形机器人运动技能学习，通过双智能体协同训练模拟人类“辅助-渐撤”的学习过程。</li>
<li>在辅助学习范式中集成了多项创新设计：<strong>特权信息利用</strong>以提供更优指导、<strong>任务依赖的初始力分布</strong>作为强先验、<strong>随机掩码机制</strong>以防止过度依赖，这些均受人类运动学习原理启发。</li>
<li>在仿真和实物实验上进行了全面验证，证明A2CF能显著提升学习速度（+30%）和任务性能（失败率-40%），并成功实现向物理机器人的零样本转移。</li>
</ol>
<p><strong>局限性</strong>：论文提到，辅助力智能体依赖于仿真中可用的特权信息（如精确地形信息），这在现实世界部署中可能是一个挑战，因为此类信息可能难以实时准确获取。</p>
<p><strong>启示</strong>：这项工作展示了将人类学习原理（外部辅助、课程学习）形式化并融入机器人学习框架的有效性。它为改进高维、复杂机器人技能的学习效率与鲁棒性提供了新思路，未来可探索如何将辅助形式从空间力扩展到其他模态（如虚拟约束），以及如何减少对仿真特权信息的依赖。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人学习复杂运动技能（如行走、舞蹈、后空翻）时探索效率低、学习过程不稳定的核心问题，提出自适应辅助课程力（A2CF）方法。其关键技术是训练一个双智能体系统：一个专用的辅助力智能体根据机器人状态施加引导力，并通过课程学习随技能熟练度逐步减少辅助。在双足行走、编舞舞蹈和后空翻三个基准测试中，该方法比基线收敛速度快30%，失败率降低40%以上，最终能产生无需外部辅助的鲁棒策略。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.23125" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>