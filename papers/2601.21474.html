<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.21474" target="_blank" rel="noreferrer">2601.21474</a></span>
        <span>作者: Zhang, Xingyu, Zhang, Chaofan, Zhang, Boyue, Peng, Zhinan, Cui, Shaowei, Wang, Shuo</span>
        <span>日期: 2026/01/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧操作领域目前主流的数据收集方法包括遥操作（如VR头显或数据手套）和动觉教学（人类直接操控机械手）。然而，遥操作存在两大关键局限性：1）人手与机械手之间的运动学不匹配，阻碍了人类运动轨迹的精确复现；2）操作过程中缺乏触觉反馈，任务表现高度依赖操作者经验。动觉教学虽能提供更优的触觉交互，但现有方法通常仅收集低维触觉信息（如接触力），忽略了对于精细接触密集型任务至关重要的多维信息——接触区域（如指尖、指腹或指侧）。本文针对在接触密集型任务中，如何收集高质量的多维触觉数据并从中学习能够自主选择最佳接触区域的策略这一具体痛点，提出了通过手把手教学（一种动觉教学形式）来获取包含接触力分布和空间接触区域信息的完整视触觉数据的新视角。本文的核心思路是：通过手把手教学收集包含关节状态、视觉图像以及多维触觉信息（力和压力中心CoP）的专家轨迹，训练一个接触感知的策略网络，并在部署时利用预测的力和CoP通过触觉控制器调整机械手运动，使其能在正确接触区域施加合适的力。</p>
<h2 id="方法详解">方法详解</h2>
<p>DexTac框架的整体流程分为三个阶段，如图2所示。第一阶段通过手把手教学收集数据；第二阶段基于模仿学习（IL）训练接触感知策略；第三阶段在真实灵巧手上部署策略，利用触觉控制器执行任务。</p>
<p><img src="https://arxiv.org/html/2601.21474v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：DexTac框架总览。第一阶段：通过手把手教学收集的专家轨迹数据集，包含视觉图像、触觉信息和关节状态。第二阶段：接触感知策略训练过程。第三阶段（上）：方法部署流程。（下）：触觉控制器中位置修正的示意图。</p>
</blockquote>
<p><strong>1. 手把手数据收集</strong>：核心是让人直接操控灵巧手完成任务。为解决人手与机械手运动学不匹配以及人手在视觉图像中引入的域偏移问题，论文设计了指尖护套。在数据收集时，护套遮挡人手；在策略部署时，护套保留在机械手指上。这缓解了域偏移并使操作更直观高效。触觉信息通过安装在指尖的三个GelStereo BioTip传感器获取。每个传感器通过一对立体RGB相机捕捉触觉图像，并基于3D视觉重建生成包含415个点的触觉点云。从点云中计算两个关键的多维触觉信息：<strong>交互力</strong>（公式3，定义为被按压点沿z轴位移的总和）和<strong>压力中心</strong>（公式4，定义为所有被按压点的位置质心，在数据收集中仅关注其x和y轴分量）。最终，每条演示轨迹收集的数据包括：视觉观测（640×480 RGB图像）、触觉观测（3个传感器的6张触觉图像）、本体感知（11维关节状态及其变化）、交互力（3个指尖的z轴力）和CoP（3个指尖的xy平面CoP）。</p>
<p><strong>2. 策略学习</strong>：将任务建模为马尔可夫决策过程（MDP），并采用模仿学习。状态空间包括RGB图像、三个触觉图像和关节角度。动作空间包括关节变化、三个指尖的交互力以及三个指尖的CoP。策略网络基于ACT架构构建。</p>
<p><img src="https://arxiv.org/html/2601.21474v1/x4.png" alt="策略网络结构"></p>
<blockquote>
<p><strong>图4</strong>：接触感知策略网络结构。状态空间由视觉图像、触觉图像和关节状态组成，动作空间由力、CoP和关节变化组成。</p>
</blockquote>
<p>网络使用基于ResNet-18的共享CNN编码器处理一张视觉图像和六张触觉图像，提取特征后经线性投影得到256维嵌入，再输入Transformer编码器以分块方式建模长时域动作序列。训练损失函数（公式5）包含两项：第一项最小化预测动作块与真实动作块之间的L1误差；第二项是KL散度正则项，用于约束潜变量z服从标准高斯先验。训练后的策略网络能一次性输出包含期望关节变化、力和CoP的动作块。</p>
<p><strong>3. 灵巧手部署与触觉控制器</strong>：部署时，实时传感器数据输入策略网络生成初始动作和触觉参考值，再经触觉控制器精细化后控制机械手。传统位置控制在接触密集型任务中易因施力不足或接触区域不准确而失败。因此，论文提出一个结合了阻抗控制思想和CoP修正的触觉控制器。</p>
<p><img src="https://arxiv.org/html/2601.21474v1/x5.png" alt="部署流程与触觉控制器"></p>
<blockquote>
<p><strong>图5</strong>：（a）灵巧手部署过程。（b）触觉控制器结构。位置参考首先基于力进行修正，然后使用CoP进一步细化，最后输出给灵巧手。</p>
</blockquote>
<p>首先，根据策略网络预测的关节变化通过正向运动学计算位置参考。接着，通过<strong>力修正</strong>（公式9）调整z轴位置，使机械手能施加合适的力（基于简化的阻抗控制原理）。最后，引入<strong>CoP修正</strong>（公式10），根据策略网络预测的期望CoP与实际测量CoP的误差，在xy平面进一步调整位置参考，从而引导指尖移动到正确的接触区域。最终得到的触觉感知位置参考同时包含了力和接触区域信息。</p>
<p><strong>创新点</strong>：与现有方法（如KineDex仅关注力）相比，DexTac的核心创新在于系统性地采集并利用了表征接触区域的<strong>压力中心</strong>这一多维触觉信息，并将其整合到策略网络的预测目标以及底层控制器的反馈修正中，实现了真正的“接触区域感知”操作。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实的GEX灵巧手上进行注射任务评估。任务要求机械手从未抓握状态安全抓取注射器，然后用拇指完全压下推杆，过程中食指和中指需在合适接触区域施加适当力以稳定注射器防止滑落。使用了30mL、40mL、50mL三种规格的注射器进行训练和评估。硬件平台包括配备三个GelStereo BioTip传感器的灵巧手和一台RealSense D435i相机。策略网络基于PyTorch和Adam优化器训练。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li><strong>w/o Tactile</strong>：仅使用视觉图像和关节状态作为输入，部署时不进行力和CoP修正（即纯位置控制）。</li>
<li><strong>w/o CoP</strong>：输入与DexTac相同，但部署时仅使用力修正，不使用CoP修正（类似于KineDex）。</li>
<li><strong>DexTac</strong>：本文提出的完整方法。</li>
</ul>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>主实验与消融研究</strong>：在90条演示数据上训练，每种注射器进行20次测试（共60次）。完整DexTac方法取得了<strong>91.67%</strong> 的平均成功率。w/o CoP（仅使用力）的成功率为60%，而w/o Tactile（无触觉）的成功率仅为28.33%。这表明引入触觉信息（力）能大幅提升性能（提升31.67%），而进一步引入CoP信息能带来额外的显著增益（再提升31.67%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.21474v1/x7.png" alt="策略对比结果"></p>
<blockquote>
<p><strong>图7</strong>：手把手教学示例及三种策略的注射任务成功率对比。DexTac（91.67%）显著优于无CoP（60%）和无触觉（28.33%）基线。</p>
</blockquote>
<ol start="2">
<li><strong>定性分析与误差收敛</strong>：图8展示了一个成功案例，机械手根据CoP误差持续调整与注射器的接触区域，最终成功完成任务且防止了手指滑脱。图9的力误差和CoP误差曲线显示，DexTac方法在接触阶段能使力和CoP误差均保持收敛，而w/o CoP方法的CoP误差则持续较大。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.21474v1/x8.png" alt="成功案例"></p>
<blockquote>
<p><strong>图8</strong>：DexTac成功案例。从上到下：原始位置指令（蓝线）、触觉修正（绿箭头）及修正后指令；灵巧手操作场景；触觉信息及CoP参考（蓝点）。系统根据CoP误差持续调整接触区域。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.21474v1/x9.png" alt="力与CoP误差"></p>
<blockquote>
<p><strong>图9</strong>：（a）和（b）分别为三种策略的力误差和CoP误差，浅红色区域为非接触期，浅绿色为接触期。DexTac能确保力和CoP误差均保持收敛。</p>
</blockquote>
<ol start="3">
<li><strong>零样本迁移</strong>：将在30/40/50mL注射器上训练的策略直接应用于未见过的20mL注射器。DexTac取得了<strong>80%</strong> 的成功率，显著高于w/o CoP（35%）和w/o Tactile（5%），证明了其强大的泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.21474v1/x10.png" alt="零样本迁移结果"></p>
<blockquote>
<p><strong>图10</strong>：在未见过的20mL注射器上的零样本迁移任务成功率。DexTac（80%）展现出优异的泛化性能。</p>
</blockquote>
<ol start="4">
<li><strong>数据效率</strong>：随着每个注射器尺寸的演示数据量从10条增加到50条，所有方法的性能都呈上升趋势。DexTac在数据量较少时（如每条尺寸10条演示）已能取得可观的成功率（约65%），并随着数据增加稳步提升至接近100%，展示了良好的数据效率。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.21474v1/x11.png" alt="数据效率曲线"></p>
<blockquote>
<p><strong>图11</strong>：不同演示数据量下的任务成功率。DexTac在不同数据规模下均保持最高性能，且随数据量增加而提升。</p>
</blockquote>
<ol start="5">
<li><strong>纯触觉实验</strong>：在无视觉输入、仅依赖触觉图像的场景下进行测试。实验A（从非抓握状态开始）和实验B（从预抓握状态开始）中，纯触觉的DexTac变体分别达到了73.3%和93.3%的成功率，验证了该方法在纯触觉场景下的可行性。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.21474v1/x12.png" alt="纯触觉实验结果"></p>
<blockquote>
<p><strong>图12</strong>：纯触觉场景下的实验结果。即使没有视觉输入，基于触觉的策略也能在注射任务中取得较高的成功率。</p>
</blockquote>
<p><strong>消融实验总结</strong>：CoP信息的引入是性能提升的关键组件，其贡献（31.67%提升）甚至大于从无触觉到仅引入力信息的提升。完整的触觉信息（力+CoP）与触觉控制器结合，是实现高成功率、接触区域自适应和良好泛化能力的核心。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了DexTac框架，它通过手把手教学收集多维专家视触觉轨迹（包含接触力和接触区域），并通过模仿学习从中训练多模态接触感知策略，使灵巧手能在接触密集型任务中自主选择并维持最佳接触区域。</li>
<li>将表征接触区域质心的<strong>压力中心</strong>引入策略学习框架。在灵巧注射任务中，引入CoP使得成功率达到了91.67%，相比仅基于力的基线方法提升了31.67%。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，GelStereo BioTip传感器存在噪声，可能影响触觉信息的质量。此外，方法需要收集一定数量的人类演示数据。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>接触区域的重要性</strong>：本工作强有力地证明了在精细接触操作中，接触区域信息与接触力信息同等重要，甚至更为关键。这为未来灵巧操作研究指明了超越“力控”、迈向“区域感知”的新方向。</li>
<li><strong>数据收集范式</strong>：研究结果进一步支持了手把手教学在接触密集型任务数据收集中相对于遥操作的优势，特别是在提供真实触觉反馈和避免运动学不匹配方面。</li>
<li><strong>多模态与泛化</strong>：结合视觉、触觉和本体感知的多模态策略展现了优异的泛化能力（零样本迁移），启发了如何利用丰富传感信息来实现更鲁棒的操作。</li>
<li><strong>纯触觉操作潜力</strong>：纯触觉实验的成功表明，在视觉受限或不可靠的场景下，高维触觉传感本身可能足以支持复杂的接触操作任务，这为盲操作或恶劣环境下的机器人应用提供了新思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DexTac框架，旨在解决接触密集型任务中现有方法触觉信息维度低、难以学习接触感知策略的问题。其关键技术是通过手把手动觉示教，采集人类示范中的多维触觉数据（包括接触力分布与空间接触区域），并整合至策略网络，使灵巧手能自主选择并保持最优接触区域。在单手动注射任务上的实验表明，DexTac成功率可达91.67%；针对小尺度注射器的高精度场景，其性能较仅使用力的基线方法提升31.67%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.21474" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>