<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>iFlyBot-VLA Technical Report - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>iFlyBot-VLA Technical Report</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01914" target="_blank" rel="noreferrer">2511.01914</a></span>
        <span>作者: Jia Pan Team</span>
        <span>日期: 2025-11-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用视觉语言模型强大的感知与推理能力来完成长视野、复杂的机器人操作任务成为研究热点。然而，自回归范式在处理需要精确、连续控制信号的任务时存在固有局限。因此，大多数当代视觉-语言-动作模型采用混合设计，即集成一个VLM进行感知，并使用一个扩散或基于流的动作专家进行运动生成。这种组合虽能利用双方优势，但仍存在关键挑战：如何设计训练策略，在使扩散策略产生精确、平滑动作的同时，最大程度地保留VLM的通用感知与推理能力。另一方面，模仿学习仍是当前VLA系统的基础，严重依赖高质量的遥操作数据集。虽然已有工作探索了其他数据源，但仅使用操作数据训练会迅速导致VLM的语言和推理能力退化。因此，一个开放的挑战是：如何平衡多模态训练，使得VLM和动作专家能够协同进化，在整个训练过程中保持感知、语言理解和动作生成能力。本文针对上述痛点，提出了一种新颖的训练框架，其核心思路是通过一个在大规模人类和机器人操作视频上训练的潜在动作模型，以及一个结合了显式离散动作令牌和隐式潜在动作令牌的双层级动作表示框架，来联合监督VLM和动作专家，并采用混合了机器人轨迹数据与通用/空间QA数据的训练策略，以增强VLM的3D感知与推理能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>iFlyBot-VLA的整体训练流程分为三个阶段：1）潜在动作训练：使用大规模人类和机器人操作视频训练一个基于VQ-VAE的潜在动作模型，学习高层、可泛化的潜在动作表示；2）基础预训练：目标是构建一个具有广泛空间感知、物体识别和泛化能力的基础模型，不专门针对任何特定下游任务；3）任务特定后训练：使用高质量的自收集数据集进行第二阶段后训练，使模型适应特定的下游复杂任务。</p>
<p><img src="https://img.paperlib.app/5d1a8b4e-4b4b-4a6d-8b1e-0a8b3c4d5e6c" alt="模型与训练流程总览图"></p>
<blockquote>
<p><strong>图1</strong>：iFlyBot-VLA模型架构与整体训练流程总览。模型主要由语言Transformer主干和动作专家网络组成。训练分为三个阶段：潜在动作训练、基础预训练和任务特定后训练。输入包括语言指令、多视角RGB图像和机器人本体感知状态，输出为动作块。注意，FAST动作令牌（对应隐式规划过程）的特征不会传递给下游的动作专家。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>潜在动作模型</strong>：采用编码器-解码器结构。编码器包含空间和时间Transformer，输入当前帧和未来帧（间隔固定为1秒），输出潜在动作。解码器仅包含空间Transformer，输入潜在动作和当前帧，重建未来帧。该模型基于VQ-VAE目标进行训练，使用大小为32的码本，每步检索8个离散码。为解决梯度崩溃问题，采用了NSVQ算法，用基于噪声的近似替换原始的直通估计器。解码时，对当前帧应用停止梯度操作，迫使模型依赖潜在动作进行解码。监督损失是重建图像与目标图像之间的均方误差。</li>
</ol>
<p><img src="https://img.paperlib.app/5d1a8b4e-4b4b-4a6d-8b1e-0a8b3c4d5e6c" alt="潜在动作令牌编码专家网络架构"></p>
<blockquote>
<p><strong>图2</strong>：潜在动作令牌编码专家网络架构。编码器处理当前帧和未来帧对，输出潜在动作。解码器利用潜在动作和当前帧重建未来帧。该模型以自监督方式从大规模视频中学习高层动作表示。</p>
</blockquote>
<ol start="2">
<li><p><strong>离散动作令牌编码</strong>：采用FAST方法，对由多个未来动作帧构成的动作窗口进行编码。首先对连续动作应用离散余弦变换进行压缩，然后使用字节对编码得到离散令牌。这些令牌仅用于监督VLM组件，鼓励模型学习与动作相关的语义并辅助隐式动作规划，其对应特征在训练和推理中均不被使用，以避免过拟合和保证推理效率。</p>
</li>
<li><p><strong>VLA模型架构</strong>：基于Qwen2.5-VL视觉语言主干，并引入一个基于流匹配的扩散Transformer作为动作专家网络。VLM主干除了处理图像和语言，还通过一个占位符令牌引入机器人本体状态。在训练中，VLM被监督预测两个序列：潜在动作令牌集和离散动作令牌集。下游动作专家网络接收来自VLM的KV缓存（仅保留与潜在动作令牌相关的部分）、机器人状态、当前时间步以及由目标动作窗口、时间步和高斯噪声构成的加噪动作，并预测从加噪动作朝向真实动作的去噪方向。损失函数定义为预测的去噪向量场与真实向量场之间的均方误差。动作专家采用完全双向注意力掩码，允许同一窗口内的所有动作令牌相互关注，从而并行去噪，既促进了动作间的时间连续性，又提高了生成效率。</p>
</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1) 提出了一个在大规模跨具身操作数据上预训练的潜在动作模型，为VLA训练提供高层意图监督；2) 设计了同时包含隐式（潜在动作）和显式（离散动作）监督的双层级动作表示框架，使VLM能直接促进动作生成；3) 采用了混合训练策略，将机器人轨迹数据与通用QA、空间QA数据按优化比例混合，以保持并增强VLM的通用理解与空间感知能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在LIBERO仿真基准和真实世界机器人平台上进行。</p>
<ul>
<li><strong>Benchmark/数据集</strong>：LIBERO仿真基准（包含Spatial, Object, Goal, Long四个任务套件，每套10个任务）。真实世界实验使用自收集的数据集，包括通用拾放、长视野操作和灵巧双臂操作任务。</li>
<li><strong>实验平台</strong>：LIBERO模拟器（Franka机器人），真实世界双机器人平台。</li>
<li><strong>Baseline方法</strong>：LAPA、OpenVLA、π0。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在LIBERO模拟器上，iFlyBot-VLA在四个任务套件上的平均准确率达到93.8%，显著优于π0（86%）和OpenVLA（76.5%）。除了在LIBERO-Goal套件上（93%）略低于π0（95%）外，在其他所有套件上均取得了最先进性能。</p>
<p><img src="https://img.paperlib.app/5d1a8b4e-4b4b-4a6d-8b1e-0a8b3c4d5e6c" alt="LIBERO数据集中的任务套件"></p>
<blockquote>
<p><strong>图5</strong>：LIBERO数据集中的四个任务套件示例。每个套件包含10个任务，旨在评估空间推理、对象泛化、目标导向适应性和长视野规划等不同能力。</p>
</blockquote>
<p><img src="https://img.paperlib/app/5d1a8b4e-4b4b-4a6d-8b1e-0a8b3c4d5e6c" alt="iFlyBot-VLA与代表性VLA模型在LIBERO模拟器数据集上的对比"></p>
<blockquote>
<p><strong>图6</strong>：iFlyBot-VLA与代表性VLA模型在LIBERO模拟器数据集上的性能对比。柱状图显示iFlyBot-VLA在多个任务套件及平均成功率上超越了基线方法。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>在LIBERO上的消融研究表明，完整的iFlyBot-VLA模型性能最佳。移除FAST模块（w/o Fast）的成功率为87.8%，完整模型提升了6%。移除LAM模块（w/o LAM）的成功率为90.3%，完整模型提升了3.5%。同时移除两者（w/o Fast and LAM）的成功率降至73%，完整模型带来了20.8%的提升。这表明显式（FAST）和隐式（LAM）规划机制均对任务执行有积极贡献，尤其在长视野操作任务中，它们的组合效果最为显著。</p>
<p><img src="https://img.paperlib.app/5d1a8b4e-4b4b-4a6d-8b1e-0a8b3c4d5e6c" alt="iFlyBot-VLA在LIBERO模拟器中的消融结果"></p>
<blockquote>
<p><strong>图7</strong>：iFlyBot-VLA在LIBERO模拟器中的消融结果，展示了不同组件对模型性能的影响。完整模型（Full）性能最优，移除任一或全部核心组件均会导致性能下降。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出并训练了一个基于大规模跨具身操作视频的潜在动作模型，为VLA学习提供了高层、可泛化的动作表示；2）设计了一个双层级动作表示框架，通过联合监督潜在动作令牌和离散动作令牌，对齐了语言、视觉和动作的表示空间，使VLM能直接贡献于动作生成；3）采用了一种混合训练策略，将机器人数据与空间QA数据相结合，有效保持了VLM的通用感知与推理能力，并增强了其具身相关的空间感知。</p>
<p>论文自身提到的局限性包括对高质量、大规模数据集的依赖，以及训练多阶段框架所需的计算成本。</p>
<p>对后续研究的启示：1）证明了从大规模视频（包括人类视频）中学习潜在动作表示对于提升VLA泛化能力的有效性；2）展示了混合不同模态（机器人操作与语言QA）数据进行训练是保持VLM能力的关键策略；3）为构建更通用、更鲁棒的具身智能体提供了可借鉴的框架设计思路。作者计划开源部分自建数据集，也将促进社区在该方向上的进一步研究。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文介绍了iFlyBot-VLA，一个旨在解决视觉-语言-动作模型中，如何协同训练视觉语言模型与动作专家以生成精确连续控制信号这一核心问题的框架。关键技术包括：基于大规模操作视频预训练的潜在动作模型；一种同时监督VLM与动作专家的双级动作表示框架，使VLM能预测隐含高层意图的潜在动作和编码显式底层动态的结构化离散动作令牌；以及结合机器人轨迹与QA数据的混合训练策略以增强3D感知。在LIBERO Franka基准测试中，该框架展示了优越性，并在真实世界多样化的操作任务中取得了有竞争力的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01914" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>