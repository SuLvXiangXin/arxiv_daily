<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>KV-Efficient VLA: A Method to Speed up Vision Language Models with RNN-Gated Chunked KV Cache</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.21354" target="_blank" rel="noreferrer">2509.21354</a></span>
        <span>作者: Xu, Wanshun, Zhuang, Long, Shan, Lianlei</span>
        <span>日期: 2025/09/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过统一的Transformer架构整合感知与策略，展现出强大的泛化能力。然而，其在真实世界长时程任务中的扩展性受到两个关键因素限制：注意力机制的高计算成本，以及在推理过程中为保留历史图像令牌作为上下文而存储大量键值对所带来的巨大内存开销。现有方法多集中于扩展主干架构以提升性能，而对实时应用至关重要的推理效率问题关注不足。具体而言，主流VLA模型（如OpenVLA、HybridVLA）的推理速度约为6 Hz，远低于机器人灵巧操作或动态导航等任务通常所需的50-100 Hz。此外，研究观察到，在VLA系统中包含未经提炼的原始历史帧会引入冗余信息，甚至导致性能下降约6%，这凸显了对高效时间上下文整合机制的需求。</p>
<p>本文针对VLA模型在长序列推理时KV缓存内存与计算开销过大的具体痛点，提出了一种模型无关的内存压缩新视角。其核心思路是：通过一种轻量级机制，将KV缓存分块聚合，并利用循环门控模块根据学习到的效用分数对历史上下文进行选择性保留与过滤，从而在保持近期细粒度细节的同时，积极剪枝陈旧、低相关性的记忆。</p>
<h2 id="方法详解">方法详解</h2>
<p>KV-Efficient VLA的整体框架旨在无缝集成到现有的VLA堆栈中，在不修改下游控制逻辑的前提下实现可扩展的推理。其输入为提示、图像和机器人状态，输出为机器人动作向量。方法的核心是一个轻量级的KV-Efficient模块，该模块对注意力机制生成的原始KV缓存进行处理。</p>
<p><img src="https://arxiv.org/html/2509.21354v2/KVEfficientModule.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：KV-Efficient框架。框架包含两个主要步骤：(1) 将KV缓存分割为固定大小的块并进行聚合；(2) 使用RNN模块更新并选择要保留或丢弃的块，从而为注意力机制实现高效、选择性的压缩。</p>
</blockquote>
<p>该框架的工作流程如下：首先，将按时间顺序存储的键值对序列划分成长度为C的非重叠块。对于每个块，使用一个多层感知机（一个用于键，一个用于值）将块内的所有键和值分别聚合为一个紧凑的代表性向量对。此聚合步骤将每个块总结为单个表示，显著减少了内存占用。</p>
<p>随后，为了决定保留哪些压缩后的块，引入了一个循环门控机制。将序列化的块代表向量输入一个LSTM门控模块，该模块维护一个隐藏状态，并为每个块输出一个介于0到1之间的门控分数，以量化其重要性。基于一个可学习的阈值τ，若某块的门控分数大于等于τ，则将其保留在缓存中；否则将其丢弃。为了保留细粒度的近期信息，最近的长度为r（小于总窗口W）的令牌始终保持未压缩状态。此更新策略保持了严格的因果性。最终，注意力计算将联合关注未压缩的近期窗口和所有被保留的压缩块。</p>
<p><img src="https://arxiv.org/html/2509.21354v2/algorithm.png" alt="算法伪代码"></p>
<blockquote>
<p><strong>图3</strong>：KV-Efficient内存压缩方法的算法伪代码，清晰展示了分块、聚合、门控评分与选择性保留的流程。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在两个方面：1) <strong>分块KV策略</strong>：通过将过去令牌分段为固定大小的块并用MLP聚合，减少了全序列缓存。2) <strong>LSTM门控机制</strong>：利用LSTM门仅保留信息量最大的聚合块。</p>
<p>在计算成本方面，该方法通过压缩和选择性保留，将注意力计算的有效内存长度从原始序列长度n大幅减少为n&#39; = W（未压缩近期窗口）+ M（保留的压缩块数）。论文给出了详细的FLOPs计算公式对比。基线注意力FLOPs与序列长度n成线性及平方关系，而KV-Efficient注意力FLOPs与压缩后的n&#39;相关，并增加了与块数(n/C)成正比的聚合与门控开销，但总体开销显著降低。理论分析表明，在典型配置下，注意力层级可获得约1.61倍的速度提升，内存加速比达2.44倍。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了Open X-Embodiment大规模机器人数据集进行微调，并在RLBench仿真环境中进行评估。对比的基线方法包括主流的VLA模型：OpenVLA (7B)、CogACT (7B) 和 HybridVLA (7B)。实验测量了推理速度（Hz）、总FLOPs和KV内存使用量。</p>
<p>关键实验结果总结如下：如表1所示，在三个基线模型上应用KV-Efficient模块后，平均总FLOPs减少了约24.6%。推理吞吐量显著提升：OpenVLA-KV-Efficient达到7.6 Hz，CogACT-KV-Efficient达到13.8 Hz，HybridVLA-KV-Efficient达到8.3 Hz。平均实现了约1.34倍的推理加速。同时，KV-Efficient变体将KV缓存存储的内存使用量减少了约1.87倍。所有变体保持参数量不变，证实效率提升源于计算节省而非模型缩小。</p>
<p><img src="https://arxiv.org/html/2509.21354v2/Loss.png" alt="训练损失曲线"></p>
<blockquote>
<p><strong>图4</strong>：集成KV-Efficient模块的HybridVLA训练损失曲线。损失在前100次迭代内下降，随后稳定收敛，表明该机制与原有训练动态兼容。</p>
</blockquote>
<p>图4展示了训练损失曲线，表明KV-Efficient模块的引入没有破坏训练稳定性，损失迅速下降并收敛。</p>
<p>论文指出，随着输入序列长度增加，其效益更加明显。但局限性在于，由于编译限制，准确性和推理速度仅在单个基准任务上进行了评估，可能无法完全反映模型的通用性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了一种模型无关的、轻量级的KV缓存压缩框架（KV-Efficient模块），通过分块聚合与循环门控选择性保留历史上下文，有效解决了VLA长序列推理的内存与计算瓶颈。2) 从理论上分析了该方法的计算成本，并实证表明其在多个主流VLA模型上能平均带来24.6%的FLOPs节省、1.34倍推理加速和1.87倍KV内存减少，且不增加参数量。</p>
<p>论文自身提到的局限性主要在于评估范围有限，仅在单一任务上测试了推理速度与准确性，未来需要在更广泛的任务和真实闭环机器人场景中进行验证。</p>
<p>这项工作对后续研究的启示在于：为大型Transformer模型的高效推理提供了一种新颖的、基于学习的动态内存管理思路。未来的工作可以深入消融研究块大小、门控策略等超参数的影响，并探索将该机制部署到对实时性要求极高的实际机器人系统中，以评估其响应性与安全性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言动作（VLA）模型在长视野任务中推理效率低、KV缓存内存消耗大的问题，提出KV-Efficient VLA方法。该方法采用模型无关的内存压缩策略，通过分块KV缓存和RNN门控模块，根据学习到的效用分数选择性保留高价值上下文，并修剪陈旧低相关内存。实验表明，该方法平均节省24.6% FLOPs，实现1.34倍推理加速和1.87倍KV内存减少。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.21354" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>