<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Gemini Robotics: Bringing AI into the Physical World - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Gemini Robotics: Bringing AI into the Physical World</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.20020" target="_blank" rel="noreferrer">2503.20020</a></span>
        <span>作者: Gemini Robotics Team, Abeyruwan, Saminda, Ainslie, Joshua, Alayrac, Jean-Baptiste, Arenas, Montserrat Gonzalez, Armstrong, Travis, Balakrishna, Ashwin, Baruch, Robert, Bauza, Maria, Blokzijl, Michiel, Bohez, Steven, Bousmalis, Konstantinos, Brohan, Anthony, Buschmann, Thomas, Byravan, Arunkumar, Cabi, Serkan, Caluwaerts, Ken, Casarini, Federico, Chang, Oscar, Chen, Jose Enrique, Chen, Xi, Chiang, Hao-Tien Lewis, Choromanski, Krzysztof, D&#39;Ambrosio, David, Dasari, Sudeep, Davchev, Todor, Devin, Coline, Di Palo, Norman, Ding, Tianli, Dostmohamed, Adil, Driess, Danny, Du, Yilun, Dwibedi, Debidatta, Elabd, Michael, Fantacci, Claudio, Fong, Cody, Frey, Erik, Fu, Chuyuan, Giustina, Marissa, Gopalakrishnan, Keerthana, Graesser, Laura, Hasenclever, Leonard, Heess, Nicolas, Hernaez, Brandon, Herzog, Alexander, Hofer, R. Alex, Humplik, Jan, Iscen, Atil, Jacob, Mithun George, Jain, Deepali, Julian, Ryan, Kalashnikov, Dmitry, Karagozler, M. Emre, Karp, Stefani, Kew, Chase, Kirkland, Jerad, Kirmani, Sean, Kuang, Yuheng, Lampe, Thomas, Laurens, Antoine, Leal, Isabel, Lee, Alex X., Lee, Tsang-Wei Edward, Liang, Jacky, Lin, Yixin, Maddineni, Sharath, Majumdar, Anirudha, Michaely, Assaf Hurwitz, Moreno, Robert, Neunert, Michael, Nori, Francesco, Parada, Carolina, Parisotto, Emilio, Pastor, Peter, Pooley, Acorn, Rao, Kanishka, Reymann, Krista, Sadigh, Dorsa, Saliceti, Stefano, Sanketi, Pannag, Sermanet, Pierre, Shah, Dhruv, Sharma, Mohit, Shea, Kathryn, Shu, Charles, Sindhwani, Vikas, Singh, Sumeet, Soricut, Radu, Springenberg, Jost Tobias, Sterneck, Rachel, Surdulescu, Razvan, Tan, Jie, Tompson, Jonathan, Vanhoucke, Vincent, Varley, Jake, Vesom, Grace, Vezzani, Giulia, Vinyals, Oriol, Wahid, Ayzaan, Welker, Stefan, Wohlhart, Paul, Xia, Fei, Xiao, Ted, Xie, Annie, Xie, Jinyu, Xu, Peng, Xu, Sichun, Xu, Ying, Xu, Zhuo, Yang, Yuxiang, Yao, Rui, Yaroshenko, Sergey, Yu, Wenhao, Yuan, Wentao, Zhang, Jingwei, Zhang, Tingnan, Zhou, Allan, Zhou, Yuxiang</span>
        <span>日期: 2025/03/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型多模态模型（LMMs）在数字领域展现出卓越的通用能力，但其向机器人等物理智能体的迁移仍面临重大挑战。实现通用机器人需要其能够理解周围的物理世界并与之进行安全、有效的交互。现有方法通常需要组合多个专用模型（如感知、规划、控制模型），或依赖于在有限机器人数据上训练的模型，这限制了其开放词汇理解、泛化能力和对复杂物理交互的掌握。本文的核心痛点是：如何将前沿视觉语言模型（如Gemini 2.0）强大的多模态理解和推理能力，与在物理世界中行动所需的具身推理（Embodied Reasoning）和灵巧控制能力相结合。本文提出了一种新思路：基于Gemini 2.0构建一个专门为机器人设计的AI模型家族，通过增强其空间、时间和物理理解（具身推理），并最终将其“接地”到真实的机器人动作数据中，从而直接控制机器人完成复杂任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文介绍了Gemini Robotics模型家族，其整体框架基于Gemini 2.0多模态基础模型，通过两个阶段实现从数字智能到物理智能的跨越。</p>
<p><img src="https://arxiv.org/html/2503.20020v1/x1.png" alt="Gemini Robotics模型家族概览"></p>
<blockquote>
<p><strong>图1</strong>：Gemini Robotics具身AI模型家族概览。Gemini 2.0已具备与机器人相关的初步能力（如语义安全理解、长上下文）。针对机器人的专门训练及可选的专门化过程，使Gemini Robotics模型展现出多种机器人专用能力，包括生成灵巧反应式运动、快速适应新机器人构型、利用高级视觉空间推理指导动作。</p>
</blockquote>
<p>核心包含两个模型：</p>
<ol>
<li>**Gemini Robotics-ER (Embodied Reasoning)**：这是一个视觉语言模型（VLM），核心是增强了具身推理能力。它通过在Gemini 2.0基础上进行更全面的训练，获得了对物理世界更深入的空间和时间理解。其能力包括：开放词汇的2D物体检测与指向、2D轨迹预测、俯视抓取预测、多视角对应点匹配以及从单目图像预测度量3D边界框。这些能力使得模型能够执行复杂的空间推理，而无需任何机器人动作数据。</li>
<li><strong>Gemini Robotics</strong>：这是一个先进的视觉-语言-动作（VLA）模型。它在Gemini Robotics-ER的基础上，进一步整合了真实的机器人动作数据进行训练，从而将强大的具身推理先验与低层次的灵巧机器人控制连接起来。该模型能够接收视觉观测和语言指令，并直接输出高频的机器人控制指令（如末端执行器位姿），实现平滑、反应式的运动。</li>
</ol>
<p>创新点具体体现在：</p>
<ul>
<li><strong>统一模型替代组合系统</strong>：与之前需要组合多个模型（感知、VLM、规划器、控制器）的工作不同，Gemini Robotics-ER和Gemini Robotics将感知、空间推理、规划和控制能力集成在单个模型中。</li>
<li><strong>从具身推理到动作接地的递进</strong>：提出了清晰的演进路径：基础VLM (Gemini 2.0) → 增强具身推理的VLM (Gemini Robotics-ER) → 可直接控制机器人的VLA (Gemini Robotics)。这强调了强大的世界理解是有效物理交互的基础。</li>
<li><strong>灵活的适应机制</strong>：模型支持零样本（通过代码生成）、少样本（通过上下文学习）以及额外的专门化微调，以适应新任务、新技能或全新的机器人构型。</li>
</ul>
<p><img src="https://arxiv.org/html/2503.20020v1/x2.png" alt="Gemini 2.0的具身推理能力示例"></p>
<blockquote>
<p><strong>图2</strong>：Gemini 2.0在具身推理能力上的表现——检测2D物体和点，利用2D指向进行抓取和轨迹预测，以及在3D中进行点对应和物体检测。所有结果均由Gemini 2.0 Flash获得。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>评估基准与平台</strong>：</p>
<ol>
<li><strong>ERQA（Embodied Reasoning Question Answering）基准</strong>：本文新提出的开源基准，包含400道多选题，评估空间推理、轨迹推理、动作推理、状态估计、指向、多视角推理、任务推理等7大类能力。</li>
<li><strong>其他基准</strong>：包括RealworldQA、BLINK、Paco-LVIS、Pixmo-Point、Where2Place、SUN-RGBD等，用于评估具体的2D指向、3D检测等能力。</li>
<li><strong>机器人实验平台</strong>：主要使用双灵巧手ALOHA 2机器人，在模拟环境和真实世界中进行测试。专门化实验还涉及了其他机器人构型。</li>
</ol>
<p><strong>对比的Baseline方法</strong>：</p>
<ul>
<li>在ERQA等推理基准上：对比了GPT-4o、Claude 3.5 Sonnet等前沿VLMs。</li>
<li>在2D指向基准上：对比了GPT、Claude以及专门的指向模型Molmo。</li>
<li>在3D检测基准上：对比了ImVoxelNet、Implicit3D、Total3DUnderstanding等专家模型。</li>
<li>在机器人控制任务上：主要对比了Gemini 2.0 Flash、Gemini Robotics-ER以及不同上下文学习设置下的性能。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>具身推理能力评估</strong>：在ERQA基准上，Gemini 2.0 Flash和Pro Experimental取得了最佳性能（46.3%和48.3%）。使用思维链（CoT）提示后，性能进一步提升至50.3%和54.8%。这表明Gemini 2.0具备先进的逐步推理能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.20020v1/x4.png" alt="ERQA基准性能对比"></p>
<blockquote>
<p><strong>图3</strong>：ERQA基准上各模型的性能对比表。Gemini 2.0模型在评估具身推理能力的三个基准（ERQA, RealworldQA, BLINK）上均达到SOTA。</p>
</blockquote>
<ol start="2">
<li><p><strong>具体能力量化</strong>：</p>
<ul>
<li><strong>2D指向</strong>：在Paco-LVIS、Pixmo-Point和Where2Place基准上，Gemini Robotics-ER显著优于Gemini 2.0 Flash，并在前两个任务上超越了专门模型Molmo 7B-D（见表3）。</li>
<li><strong>3D检测</strong>：在SUN-RGBD基准上，Gemini Robotics-ER达到了48.3%的AP@15，超越了所有对比的专家模型，创造了新SOTA（见表4）。</li>
</ul>
</li>
<li><p><strong>机器人零样本和少样本控制</strong>：</p>
<ul>
<li><strong>模拟任务</strong>：在ALOHA 2模拟任务套件上，Gemini Robotics-ER的零样本平均成功率（53%）几乎是Gemini 2.0 Flash（27%）的两倍。对于“香蕉递送”等任务，成功率从24%提升至60%。通过上下文学习（ICL），Gemini Robotics-ER的平均成功率进一步提升至65%。</li>
<li><strong>真实世界任务</strong>：在真实ALOHA 2上，Gemini Robotics-ER在“香蕉递送”、“叠衣服”、“擦拭”三个任务上，零样本平均成功率为25%，ICL下达到65%。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2503.20020v1/x10.png" alt="零样本控制API与流程"></p>
<blockquote>
<p><strong>图4</strong>：用于零样本控制的感知与控制API及智能体协调流程概览。Gemini模型迭代接收图像和状态，输出可执行代码来控制机器人。</p>
</blockquote>
<ol start="4">
<li><strong>Gemini Robotics VLA模型性能</strong>：<ul>
<li>经过机器人动作数据训练后，Gemini Robotics在灵巧操作任务上表现显著优于仅具推理能力的模型。例如，在“叠纸狐狸”、“玩纸牌”等长视距、高灵巧度任务上取得成功。</li>
<li>模型展现出强大的泛化能力：对未见过的物体、位置、环境和指令分布变化具有鲁棒性。</li>
<li>专门化微调实验表明，模型可以从约100次演示中快速学习新任务，并能适应全新的双臂平台和高自由度人形机器人构型。</li>
</ul>
</li>
</ol>
<p><strong>消融实验总结</strong>：<br>论文通过对比Gemini 2.0 Flash、Gemini Robotics-ER和Gemini Robotics在相同机器人任务上的表现，清晰地展示了每个阶段组件带来的贡献：</p>
<ul>
<li><strong>Gemini 2.0 Flash</strong>：提供了基础的多模态理解和初步的具身推理能力，可实现基本的零样本代码控制。</li>
<li><strong>Gemini Robotics-ER</strong>：通过增强的具身推理训练，显著提升了空间理解精度，从而使零样本和少样本机器人控制成功率大幅提高。</li>
<li><strong>Gemini Robotics</strong>：通过整合真实机器人动作数据，实现了从“理解”到“灵巧控制”的飞跃，能够处理更复杂、需要精细动力学交互的任务。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了Gemini Robotics模型家族</strong>：包括增强具身推理的VLM（Gemini Robotics-ER）和可直接控制机器人的VLA（Gemini Robotics），为构建通用物理智能体提供了从强大世界模型到灵巧控制的有效路径。</li>
<li><strong>引入了ERQA基准</strong>：这是一个专注于评估对物理智能体至关重要的具身推理能力的开源基准，弥补了现有VLM基准的不足，促进了标准化评估。</li>
<li><strong>系统展示了从零样本到专门化的灵活适应能力</strong>：模型支持通过代码生成、上下文学习和微调等方式，快速适应新任务、新技能和新机器人构型，体现了其作为机器人基础模型的通用性和灵活性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，尽管性能显著提升，但在一些需要极高精度的灵巧任务（如叠衣服）上，零样本控制的成功率仍然有限。此外，报告也专门讨论了这类大型机器人模型的安全考量，包括潜在的风险和缓解措施，表明其能力提升伴随新的责任挑战。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>具身推理是物理AI的核心基础</strong>：研究表明，在将基础模型应用于机器人之前，有目的地增强其空间、时间和物理理解（具身推理）至关重要，这能显著提升下游控制性能。</li>
<li><strong>统一模型架构的趋势</strong>：Gemini Robotics展示了将感知、推理、规划和控制集成于单一模型的潜力，这可能是简化机器人系统、提升泛化能力的重要方向。</li>
<li><strong>安全与负责任开发必须前置</strong>：随着机器人模型能力的不断增强，其安全影响和社会风险需要与技术开发同步进行深入研究，并制定相应的缓解指南。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决将大型多模态模型从数字领域迁移到物理机器人中的核心挑战，使机器人能感知并安全交互物理世界。关键技术方法是基于Gemini 2.0构建Gemini Robotics模型家族，包括视觉-语言-动作（VLA）通用模型和Embodied Reasoning模型，通过增强空间与时间理解实现直接控制、鲁棒操作及开放指令跟随。实验表明，该模型能执行复杂任务如折叠折纸狐狸和玩牌，从仅100个演示学习新任务，并适应双臂平台和高自由度人形机器人等新平台，支持零样本和少样本应用。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.20020" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>