<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.06571" target="_blank" rel="noreferrer">2512.06571</a></span>
        <span>作者: Xu, Zifan, Seo, Myoungkyu, Lee, Dongmyeong, Fu, Hao, Hu, Jiaheng, Cui, Jiaxun, Jiang, Yuqian, Wang, Zhihan, Brund, Anastasiia, Biswas, Joydeep, Stone, Peter</span>
        <span>日期: 2025/12/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人足球长期以来是测试集成感知、控制和决策系统的基准领域。近年来，强化学习在人形机器人运动技能学习上取得了进展，包括稳健行走、运球等。然而，结合了高速肢体运动、单脚支撑平衡以及对嘈杂感知和外部扰动鲁棒性的全身控制任务（如射门）仍然极具挑战。现有研究多侧重于运球或通用运动技能，而在嘈杂感知下敏捷、精准的射门技能相对未被充分探索。本文针对这一具体痛点，提出了一种从嘈杂感官输入中学习稳健、自适应人形足球机器人射门技能的系统。其核心思路是设计一个包含四个连续训练阶段（长距离追球、定向射门、教师策略蒸馏、学生适应与精炼）的师生框架，结合课程学习、真实的噪声建模以及在线约束强化学习，以在感知不确定的情况下实现零样本的真实世界部署。</p>
<h2 id="方法详解">方法详解</h2>
<p>系统目标是学习一个控制策略 $a_t = \pi(o_{t-H:t})$，将长度为 $H$ 的历史观测 $o_{t-H:t}$ 映射到人形机器人50Hz频率下的关节位置目标动作 $a_t$。每个观测 $o_t$ 包含本体感知 $o^p_t$、估计的球位置 $o^b_t$ 和估计的目标位置 $o^g_t$。一个完整的射门周期包含三个关键阶段：(i) 从远距离接近球，(ii) 执行将球踢向球门的动作，(iii) 重新定向以再次定位球并无缝开始下一次射门尝试。</p>
<p><img src="https://arxiv.org/html/2512.06571v2/x1.png" alt="方法框架与网络结构"></p>
<blockquote>
<p><strong>图2</strong>：左：教师和学生网络的架构；右：多阶段训练框架：(1) 长距离追球；(2) 定向射门；(3) 教师策略蒸馏；(4) 学生适应与精炼。</p>
</blockquote>
<p>训练框架包含四个阶段：</p>
<ol>
<li><strong>长距离追球（教师）</strong>：教师策略 $\hat{a}_t = \hat{\pi}(s_t)$ 使用特权状态信息（包括真实球位置）学习从不同初始配置稳健接近球的行走步态。其输入状态 $s_t$ 包含本体感知、速度命令、步态周期和特权观测。环境设计使机器人的指令线速度始终与机器人-球向量对齐，角速度指令使机器人转向球的方向。奖励函数基于行走步态设计，主要包含对指令速度的跟踪奖励以及一系列正则化奖励（如基座高度、关节扭矩等），具体权重见表II。同时应用了广泛的域随机化（关节参数、身体质量、外部扰动等）以提高鲁棒性。</li>
<li><strong>定向射门（教师）</strong>：教师策略在长距离追球的基础上，进一步使用真实的球和球门位置信息，学习精确且鲁棒的射门动作。环境模拟标准足球场，并鼓励机器人持续循环进行追球和射门。额外引入了对球的随机扰动以产生不完美状态，迫使策略学习恢复行为。奖励设计在追球奖励基础上，增加了<strong>球速度奖励</strong>（鼓励将球高速踢向球门方向）和<strong>头部追踪球奖励</strong>（鼓励保持球在视野内）。</li>
<li><strong>教师策略蒸馏（学生）</strong>：将习得基本射门和恢复能力的教师策略蒸馏至学生策略 $\pi(o_{t-H:t})$，后者在嘈杂感知下运行。学生策略的输入是长度为 $H=50$ 的历史观测序列，通过一个1D卷积编码器提取时序特征，再与当前观测拼接后通过MLP输出动作。<strong>噪声建模</strong>是关键，包含三部分：a) <strong>速度依赖噪声</strong>：对球/球门位置注入的噪声大小与其在机器人头部坐标系中的速度成正比；b) <strong>延迟更新</strong>：本体感知以50Hz更新，球位置以约10Hz异步更新；c) <strong>帧丢失</strong>：若球在视野外或在发布间隔内，其位置观测被掩码为零。</li>
<li><strong>学生适应与精炼（学生）</strong>：使用在线约束强化学习对学生策略进行进一步适应和精炼。作者发现，在射门周期中，任务奖励（如球速度奖励）分布不均，导致在即将踢球的关键时刻，固定的正则化惩罚系数会施加过大的惩罚，可能引起抖动或不安全的急剧转向动作。为解决此<strong>信用分配不均</strong>问题，将定向射门任务重构为一个约束RL问题：最大化任务性能 $J_{\text{task}}$，同时约束正则化奖励 $r_{\text{regu}}$ 不超过上限 $h$。该问题使用N-P3O算法求解，旨在产生更平滑、更安全的射门动作。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.06571v2/content/figures/reward_to_go_subplots.png" alt="奖励曲线"></p>
<blockquote>
<p><strong>图5</strong>：四个完整射门周期内的信用分配不均示意图。y轴显示了正则化奖励、头部追踪奖励和球速度奖励的归一化未来回报。正则化奖励曲线中的大幅下降表明存在抖动的踢球动作。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟和真实世界（Booster T1人形机器人）中进行评估。模拟评估在一个 $9 \times 9$ 的网格上进行，覆盖球场前侧区域 $[-1.5, 6.5] \times [-4.0, 4.0]$，每个位置进行50次独立试验。真实部署使用YOLOv8进行球检测，并采用一个数据驱动的腿式惯性里程计模块（受Legolas启发）进行定位，平均位置误差为 $0.14 \pm 0.09$ 米。</p>
<p><img src="https://arxiv.org/html/2512.06571v2/content/figures/main_eval.png" alt="主要评估结果"></p>
<blockquote>
<p><strong>图7</strong>：不同初始球位置下的成功率、踢球精度、最大球速和能量成本的可视化。结果显示策略在大部分区域表现稳健，且当球靠近球门时，策略会自发采用能量节省策略（轻推而非重踢）。</p>
</blockquote>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>模拟性能</strong>：学习的策略实现了平均 <strong>79.5%</strong> 的成功率（球穿过球门线）和平均 <strong>0.956</strong> 的踢球精度（踢后球速方向与球-球门方向向量的余弦相似度）。最大球速达到 <strong>4.13 m/s</strong>，平均能量成本为 <strong>110.8 J/s</strong>。</li>
<li><strong>真实世界性能</strong>：在Booster T1机器人上，在五种不同的球-球门配置下，取得了平均 <strong>66.7%</strong> 的成功率。</li>
<li><strong>消融实验</strong>：证明了最终适应阶段和约束RL精炼的必要性。移除适应阶段会导致成功率显著下降；使用约束RL（N-P3O）比使用固定正则化系数的PPO能获得更高的任务回报和更低的能量消耗，且动作更平滑。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个完整的四阶段师生训练系统，用于从嘈杂感知中学习稳健、持续的人形机器人射门技能，并成功实现了零样本的真实世界部署。</li>
<li>引入了包含速度依赖噪声、延迟和帧丢失的真实感知不确定性建模，以及用于解决信用分配不均、精炼运动质量的在线约束RL方法。</li>
<li>将人形机器人射门确立为一个具有挑战性的全身视觉运动技能学习基准任务，并提供了详实的模拟与实物实验结果。</li>
</ol>
<p><strong>局限性</strong>：论文提到，尽管系统表现稳健，但在感知噪声极大或完全丢失的情况下性能可能会下降。此外，该方法依赖于特定的硬件平台和传感器配置。</p>
<p><strong>启示</strong>：这项工作展示了通过精心设计的课程学习、真实的感知模拟和高级策略优化技术，可以让人形机器人掌握复杂的动态交互技能。其多阶段训练框架和针对感知-控制耦合问题的解决方案（如约束RL处理信用分配），为未来在更复杂、非结构化环境中学习更高级的机器人技能提供了有价值的参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形足球机器人需在噪声感知输入下学习快速、稳健的连续踢球技能这一核心问题，提出了一种基于强化学习的四阶段训练框架。方法包括：长距离追球、定向踢球（教师策略）、策略蒸馏以及学生策略的适应与精炼，并结合定制奖励函数、真实噪声建模和在线约束强化学习以缩小仿真到现实的差距。实验表明，该系统在多种球-球门配置下均表现出较强的踢球准确性和得分成功率，消融研究验证了约束强化学习、噪声建模及适应阶段的必要性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.06571" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>