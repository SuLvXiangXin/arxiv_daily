<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.01819" target="_blank" rel="noreferrer">2509.01819</a></span>
        <span>作者: Dieter Fox Team</span>
        <span>日期: 2025-09-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域，基于扩散模型和流匹配的模仿学习策略在建模高维多模态动作分布方面取得了显著进展。然而，现有方法在执行复杂的灵巧操作任务时，仍面临效率、鲁棒性和泛化能力方面的局限。具体而言，它们难以捕捉多指交互的复杂性、保持动作序列的时间一致性、泛化到未见场景，并且在处理视觉、语言、本体感觉等多种数据源时存在架构限制。</p>
<p>本文针对现有流匹配策略在复杂灵巧操作任务中效率与质量不足、以及对多模态输入条件化能力有限的痛点，提出了两个核心改进：一是将一致性训练目标融入流匹配过程以“拉直”流轨迹，实现高质量、少步数的动作生成；二是设计了名为DiT-X的新型Transformer架构，通过自适应交叉注意力机制更有效地融合多模态输入。本文的核心思路是：通过结合一致性训练的流匹配（Consistency Flow Training）和增强的多模态条件化架构，构建一个能够高效生成高质量、高维灵巧动作的通用机器人操作策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>ManiFlow的整体框架是一个视觉运动模仿学习策略，它接收2D图像或3D点云、机器人状态、语言指令等多模态输入，并输出一系列动作。其核心训练目标结合了流匹配损失和一致性训练损失，并采用了改进的DiT-X Transformer架构进行高效的特征融合。</p>
<p><img src="https://arxiv.org/html/2509.01819v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ManiFlow的策略架构。系统处理2D或3D视觉观测、机器人状态或语言作为输入，并输出动作序列。它利用DiT-X Transformer架构，通过结合连续时间一致性训练目标的流匹配模型进行优化，确保为具有挑战性的灵巧任务生成高质量动作。</p>
</blockquote>
<p><strong>核心模块一：一致性流训练</strong><br>该方法在标准流匹配基础上引入了连续时间一致性训练目标。标准流匹配通过线性插值在数据点 <code>x_1</code> 和噪声点 <code>x_0</code> 之间构建路径，并训练模型预测从噪声指向数据的瞬时速度 <code>v_t = x_1 - x_0</code>。ManiFlow的模型 <code>v_θ(x_t, t, Δt)</code> 额外接收一个步长参数 <code>Δt</code>。一致性训练的关键是强制流轨迹上不同点预测的一致性：采样当前点 <code>x_t</code> 和步长 <code>Δt</code>，得到下一时间点 <code>t1 = t+Δt</code> 及对应点 <code>x_t1</code>；然后利用指数移动平均（EMA）模型 <code>θ-</code> 预测从 <code>x_t1</code> 指向更远目标 <code>x_t2</code> 的速度 <code>v_t1</code>，并据此估算出一个虚拟目标数据点 <code>x̃_1 = x_t1 + (1-t1)·v_t1</code>；最终计算从 <code>x_t</code> 到 <code>x̃_1</code> 的平均速度目标 <code>ṽ_target</code>，并约束当前模型对其的预测。一致性损失 <code>ℒ_CT</code> 与流匹配损失 <code>ℒ_FM</code> 共同构成总损失。此方法旨在拉直学习到的流轨迹，使模型能在更少的去噪步骤中生成准确动作。</p>
<p><img src="https://arxiv.org/html/2509.01819v1/x2.png" alt="一致性训练示意图"></p>
<blockquote>
<p><strong>图3</strong>：ManiFlow一致性训练原理。给定一个将动作平滑转换为噪声的流路径，通过线性插值采样多个中间点（如 <code>x_t</code>, <code>x_t1</code>, <code>x_t2</code>）。训练期间，模型学习将流轨迹上的任何中间点映射回其原点 <code>x_1</code>，并确保同一轨迹上采样点的自一致性。</p>
</blockquote>
<p><strong>核心模块二：DiT-X 架构</strong><br>为了高效处理多模态输入，本文提出了DiT-X块。它改进了图像生成中的DiT块和机器人策略中的MDT块。其核心创新在于对交叉注意力层应用了AdaLN-Zero条件化。对于时间步等低维输入，通过AdaLN-Zero生成尺度与偏移参数 (<code>α, γ, β</code>)。这些参数不仅像之前工作那样调节自注意力和前馈层，还被用来动态调整交叉注意力层的输入和输出特征。这种设计使得网络能够以选择性的方式对视觉和语言等高维特征令牌进行缩放，从而实现动作令牌与多模态观测令牌之间更细粒度、更自适应的特征交互。</p>
<p><img src="https://arxiv.org/html/2509.01819v1/x3.png" alt="DiT-X架构对比"></p>
<blockquote>
<p><strong>图4</strong>：DiT-X块与DiT（仅自注意力）和MDT（基础交叉注意力）的对比。DiT-X将AdaLN-Zero条件化应用于低维机器人状态输入，并用学习到的缩放和偏移参数调整交叉注意力的输入和输出，确保动作令牌与多模态输入令牌之间自适应、细粒度的特征交互。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.01819v1/figures/ablate_ditx_mse_error.png" alt="训练误差对比"></p>
<blockquote>
<p><strong>图5</strong>：在10个语言条件化的MetaWorld任务中，DiT-X与未在交叉注意力中使用AdaLN-Zero条件化的模型在训练动作误差上的对比。DiT-X块显示出更快的收敛速度。</p>
</blockquote>
<p><strong>其他技术细节</strong></p>
<ol>
<li><strong>时间空间采样策略</strong>：论文系统评估了五种时间步 <code>t</code> 的采样策略，发现专注于高噪声区域的Beta分布采样对机器人控制任务尤其有效。对于一致性训练中的步长 <code>Δt</code>，连续时间采样表现更优。</li>
<li><strong>3D视觉编码器</strong>：与先前工作使用最大池化压缩特征不同，ManiFlow的3D编码器避免池化操作，保留逐点特征，以维持更丰富的空间几何细节，这对需要精确交互的任务有益。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真基准</strong>：使用了三个灵巧操作基准（Adroit, Dexart, RoboTwin 1.0）共12个任务；以及MetaWorld基准的48个任务进行语言条件化多任务学习评估。使用RoboTwin 2.0基准测试鲁棒性和泛化能力。</li>
<li><strong>真实机器人</strong>：在三个机器人平台上评估了8个任务：单臂Franka、双手xArm（带灵巧手）、以及Unitree H1人形机器人（带双手仿人灵巧手）。</li>
<li><strong>对比方法</strong>：2D图像输入对比Diffusion Policy和Flow Matching Policy；3D点云输入主要对比3D Diffusion Policy (DP3) 和自建的3D Flow Matching Policy*。在鲁棒性测试中还对比了基于多视图图像、并在域随机化数据上微调的大规模预训练模型 <code>π_0</code>。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>主要仿真性能</strong>：如表1所示，在12个灵巧任务上，2D ManiFlow相比2D扩散和流匹配策略平均提升43.4%和45.6%；3D ManiFlow相比3D DP3和流匹配策略平均提升15.9%和11.0%。在最具挑战性的RoboTwin双手任务上，3D ManiFlow以61.9%的成功率显著优于DP3的42.7%。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.01819v1/x6.png" alt="主要仿真结果表"></p>
<blockquote>
<p><strong>表1</strong>：三个灵巧基准上12个任务的成功率。ManiFlow在基于图像和点云的输入上都取得了优越性能。</p>
</blockquote>
<ol start="2">
<li><strong>多任务与语言条件化</strong>：如图6所示，在48个MetaWorld任务的多任务学习中，ManiFlow取得了78.1%的平均成功率，相对3D扩散和流匹配基线分别有31.4%和34.9%的相对提升，在“极难”任务上提升尤为显著（125%和73.6%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.01819v1/figures/metaworld_multitask_SR.png" alt="多任务学习性能"></p>
<blockquote>
<p><strong>图6</strong>：在48个MetaWorld任务上的语言条件化多任务学习对比。ManiFlow在所有难度级别上都优于3D扩散和流匹配策略，平均相对提升31.4%和34.9%。</p>
</blockquote>
<ol start="3">
<li><strong>学习效率与泛化</strong>：如图7a所示，在RoboTwin 2.0的4个双手任务上，仅使用50个域随机化演示从头训练的ManiFlow，平均成功率比经过微调的 <code>π_0</code> 模型高出58%。ManiFlow在应对新物体、杂乱背景、不同光照等环境变化时展现了强大的泛化能力（图8）。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.01819v1/figures/maniflow_vs_pi0_robotwin.png" alt="学习效率与泛化"></p>
<blockquote>
<p><strong>图7a</strong>：在RoboTwin 2.0基准的4个双手任务上评估ManiFlow和 <code>π_0</code>。与大规模预训练的 <code>π_0</code> 模型相比，仅使用点云输入从头学习的ManiFlow显示出更优的学习效率和泛化到新物体及背景的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.01819v1/x4.png" alt="域随机化评估可视化"></p>
<blockquote>
<p><strong>图8</strong>：域随机化评估可视化。为全面测试策略鲁棒性，在RoboTwin 2.0基准上进行了包含随机干扰物、新物体、多样背景纹理、各种光照条件和桌面高度变化等挑战性域随机化评估。</p>
</blockquote>
<ol start="4">
<li><strong>数据扩展性</strong>：如图7b所示，在“提起锅”任务中，ManiFlow表现出更强的数据扩展能力。在低数据区域（50个演示）优势明显，并且随着数据量增加持续提升，在500个演示时达到99.7%的成功率，而 <code>π_0</code> 在相同数据量下为94.0%。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.01819v1/figures/maniflow_scaling_performance_lift_pot.png" alt="扩展行为"></p>
<blockquote>
<p><strong>图7b</strong>：在“提起锅”任务上，演示数量从10到500变化的扩展性能结果。ManiFlow在低数据区域和最终扩展到500个演示时都持续优于 <code>π_0</code>，最终达到99.7%的成功率。</p>
</blockquote>
<ol start="5">
<li><p><strong>推理效率</strong>：ManiFlow仅需1-2个推理步骤即可达到63.7%-64.5%的成功率，而对比方法需要10步才能达到42.7%-48.1%的成功率，实现了高效推理。</p>
</li>
<li><p><strong>真实机器人结果</strong>：如图9和表2所示，在8个真实机器人任务中，ManiFlow平均成功率达到69.6%，几乎是DP3（37.6%）的两倍。在需要高灵巧度的任务（如人形机器人倒水）和双手协调任务（如交接瓶子）上提升尤为显著。同时，ManiFlow在操作未见物体和应对环境扰动时也表现出更强的鲁棒性。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.01819v1/x5.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图9</strong>：（上）在3个机器人平台测试的8个真实机器人任务。（下）采样的4个真实机器人任务的3D点云可视化。ManiFlow平均成功率达69.6%，几乎是DP3性能的两倍。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.01819v1/x7.png" alt="真实机器人鲁棒性测试"></p>
<blockquote>
<p><strong>图10</strong>：真实世界鲁棒性测试。在部署期间用不同扰动测试策略鲁棒性，如不同自我中心视角、新物体和背景、从失败中恢复、添加位置受人为扰动的各种干扰物等。ManiFlow在有限数据下对这些扰动具有鲁棒性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>时间采样策略</strong>：Beta分布采样和连续时间步长采样在流匹配和一致性训练中表现最优。</li>
<li><strong>DiT-X架构</strong>：引入交叉注意力层的AdaLN-Zero条件化带来了显著的性能提升和更快的训练收敛速度（图5，图13）。</li>
<li><strong>一致性训练</strong>：结合一致性目标是实现高质量、少步数推理的关键。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li><strong>高质量高效的动作生成</strong>：通过将连续时间一致性训练目标与流匹配联合优化，ManiFlow能够以极少的去噪步骤（1-2步）生成高质量的高维灵巧动作，显著提升了推理速度。</li>
<li><strong>高效的多模态条件化</strong>：提出的DiT-X Transformer架构通过自适应交叉注意力机制和AdaLN-Zero条件化，实现了动作与多模态观测（视觉、语言、状态）之间更细粒度的特征交互，从而提升了策略的表达能力和任务性能。</li>
<li><strong>卓越的真实世界鲁棒性与泛化能力</strong>：在包含单臂、双手和人形机器人的复杂真实任务中，ManiFlow相比先前方法实现了近乎翻倍的成功率，并展现出对未见物体、环境变化和扰动的强大泛化与适应能力。</li>
</ol>
<p>论文提到的局限性包括：DiT-X中引入的自适应交叉注意力机制带来了轻微的计算开销；以及对时间采样策略等组件的消融研究仍需在更广泛的任务上进行。</p>
<p>本文的启示在于：将生成模型中的一致性训练思想与流匹配结合，是提升机器人策略推理效率和动作质量的有效途径；针对机器人多模态感知与控制的特点，对通用Transformer架构进行针对性的改进（如细粒度的条件化机制）能带来显著收益；在3D感知中保留几何细节对于灵巧操作至关重要。这些方向为后续构建更高效、更通用的机器人操作策略提供了有价值的参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ManiFlow，旨在解决现有流匹配策略在执行复杂灵巧操作任务时效率、鲁棒性和泛化性不足的问题。其关键技术包括：1）将一致性训练目标融入流匹配损失，以“拉直”流路径，实现1-2步快速推理；2）提出DiT-X架构，通过自适应交叉注意力和AdaLN-Zero调节实现多模态观测与动作令牌的细粒度交互。实验表明，ManiFlow在多样化仿真基准中性能持续提升，在单臂、双臂及人形机器人真实任务上成功率近乎翻倍，并对新物体和背景变化表现出强鲁棒性与泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.01819" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>