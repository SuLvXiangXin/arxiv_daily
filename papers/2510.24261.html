<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.24261" target="_blank" rel="noreferrer">2510.24261</a></span>
        <span>作者: Gang Hua Team</span>
        <span>日期: 2025-10-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，学习泛化性强的机器人操作策略面临真实世界训练数据稀缺的关键挑战。近期研究试图通过自监督表征学习来缓解此问题，但主流方法存在两类关键局限：一类依赖2D视觉预训练范式（如掩码图像建模），主要关注静态语义或场景几何，缺乏对未来动态的建模；另一类利用大规模视频预测模型，虽强调2D动态，但缺乏对底层3D场景结构的显式感知。少数探索3D动态学习的工作（如使用动态高斯）引入了显著的结构复杂性，限制了其在下游策略学习中的灵活性和可扩展性。</p>
<p>本文针对现有方法未能联合学习机器人操作所需的三维几何、语义和动态这一具体痛点，提出了一种新的视角：通过可微分体积渲染进行掩码重建和未来预测，从而学习具有3D感知和动态信息的三平面特征。本文核心思路可概括为：利用多视角RGB-D视频数据进行预训练，通过掩码重建和未来帧预测的渲染监督，联合捕获空间几何、未来动态和任务语义，形成统一的三平面表征，并迁移至下游操作任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>DynaRend的整体框架是一个三阶段的流程：1）从多视角RGB-D观测构建3D场景的三平面特征表示；2）对三平面特征进行掩码，并先后通过重建网络和预测网络，生成当前和未来场景的特征体；3）通过可微分体积渲染，将特征体渲染为RGB、深度和语义图，与目标视图进行对比以提供自监督信号。预训练完成后，重建与预测网络作为三平面编码器，与动作解码器一起在下游任务上进行微调。</p>
<p><img src="https://arxiv.org/html/2510.24261v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：DynaRend框架总览。(a) 从多视角RGB-D输入重建点云，编码后投影到三个正交平面生成三平面特征。(b) 掩码部分三平面特征，先后通过重建网络和预测网络得到当前和未来场景表示。预训练时，两个特征体通过体积渲染生成RGB、深度和语义图，由对应的当前和未来目标视图监督。(c) 微调时，两个网络作为三平面编码器，与动作解码器在演示数据上训练。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>三平面场景表示</strong>：输入为多视角校准的RGB-D图像集。首先通过深度反投影重建场景级点云，经MLP编码得到逐点特征。随后将3D工作空间划分为规则体素网格，通过轴对齐的最大池化将点特征投影到三个正交平面（xy, xz, yz），形成三平面特征 $\mathcal{V}$。该表示平衡了效率与表达能力。</li>
<li><strong>掩码未来预测</strong>：随机掩码部分三平面特征块，替换为可学习的掩码嵌入，得到 $\bar{\mathcal{V}}$。结合语言指令（CLIP文本编码），首先通过重建网络 $\mathcal{E}<em>{\text{recon}}$ 恢复完整的当前场景三平面特征 $\mathcal{V}</em>{\text{now}}$。随后，预测网络 $\mathcal{E}<em>{\text{pred}}$ 以 $\mathcal{V}</em>{\text{now}}$ 和指令为输入，预测最近未来关键帧的三平面特征 $\mathcal{V}_{\text{future}}$。两个网络均为四层Transformer，采用了SwiGLU、QK Norm和RoPE等技术。</li>
<li><strong>可微分体积渲染</strong>：对 $\mathcal{V}<em>{\text{now}}$ 和 $\mathcal{V}</em>{\text{future}}$ 分别进行渲染以提供监督。对于目标视图中的像素，沿相机射线采样点。对每个采样点，将其3D坐标投影到三平面上，通过双线性插值查询特征，并将三个平面的特征求和得到该点的特征描述符。轻量级MLP头根据该特征预测密度、RGB值和语义特征。最后，沿射线积分得到渲染的RGB $\hat{\mathbf{C}}$、语义 $\hat{\mathbf{S}}$ 和深度 $\hat{\mathbf{D}}$。</li>
<li><strong>目标视图增强</strong>：为解决真实世界相机视角有限的问题，提出利用预训练生成模型（See3D）合成新视角。具体做法是：随机选择一个基视图，扰动其相机姿态作为目标姿态；将多视角重建的点云通过投影和反投影扭曲到目标姿态；以扭曲后的视图为条件，用See3D生成真实感图像，并用Depth Anything v2估计深度图，作为额外的监督数据。</li>
<li><strong>损失函数</strong>：预训练损失是重建损失 $\mathcal{L}<em>{\text{recon}}$ 和预测损失 $\mathcal{L}</em>{\text{pred}}$ 的加权和。二者形式相同，均包含RGB的MSE损失、语义特征的MSE损失（语义特征由RADIOv2.5从目标视图提取）以及深度的尺度不变对数损失（SiLog）。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>统一的3D动态表征学习</strong>：首次通过基于渲染的掩码重建与未来预测联合学习3D几何、语义和动态，形成适用于操作任务的统一表征。</li>
<li><strong>实用的视图合成监督</strong>：创新性地使用预训练视觉条件生成模型合成新视角监督，降低了对密集相机设置的依赖，增强了现实世界的适用性。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：RLBench（18任务子集和71任务全集）和Colosseum（20任务，含12类环境扰动）。</li>
<li><strong>实验平台</strong>：仿真实验使用7-DoF Franka Emika Panda机械臂，输入为前、左肩、右肩、手腕四个固定视角的RGB-D图像。每个任务使用100条专家演示训练。</li>
<li><strong>对比基线</strong>：在RLBench上对比了C2F-ARM-BC、PerAct、RVT、3D-MVP、3D Diffuser Actor、RVT-2等策略模型或架构。在71任务设置中还对比了不同预训练策略（MVP、VC-1、SPA）。在Colosseum上对比了MVP、VC-1、3D-MVP和从头训练的RVT。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在RLBench 18任务上，DynaRend取得了最高的平均成功率（83.2%）和最佳的平均排名（1.5），推理速度（19.6 Hz）也接近最快的RVT-2。在多个任务上达到或接近100%成功率，如Push Buttons、Slide Block、Close Jar等。</p>
<p><img src="https://arxiv.org/html/2510.24261v1/x3.png" alt="RLBench 18任务结果"></p>
<blockquote>
<p><strong>图3</strong>：RLBench 18个任务的成功率对比表。DynaRend在平均成功率（Avg. S.R.）上领先所有基线，并在多数具体任务上表现优异。</p>
</blockquote>
<p>在RLBench 71任务上，基于RVT-2架构，使用DynaRend预训练表征的方法显著优于使用其他2D或3D预训练方法（如MVP、VC-1、SPA）以及从头训练的方法。</p>
<p><img src="https://arxiv.org/html/2510.24261v1/x4.png" alt="RLBench 71任务结果"></p>
<blockquote>
<p><strong>图4</strong>：RLBench 71个任务上的成功率对比。DynaRend预训练在两组任务上均大幅提升了下游策略性能，证明了其表征的泛化能力。</p>
</blockquote>
<p>在Colosseum基准测试中，DynaRend在12类环境扰动下的平均成功率最高（76.9%），尤其在物体<strong>颜色</strong>、<strong>纹理</strong>、<strong>尺寸</strong>和<strong>光照</strong>变化上展现出最强的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2510.24261v1/x5.png" alt="Colosseum扰动泛化结果"></p>
<blockquote>
<p><strong>图5</strong>：Colosseum基准上针对12类环境扰动的平均成功率。DynaRend在所有扰动类型上均优于基线，显示出卓越的泛化能力。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>研究验证了各核心组件的贡献。移除<strong>未来预测</strong>目标会导致动态理解缺失，性能显著下降。移除<strong>语义蒸馏</strong>损失也会损害性能。使用<strong>视图合成</strong>进行数据增强能有效提升在真实世界设置下的性能。<strong>三平面表示</strong>相比体素网格表示更高效且性能相当或更优。</p>
<p><img src="https://arxiv.org/html/2510.24261v1/x6.png" alt="消融研究结果"></p>
<blockquote>
<p><strong>图6</strong>：消融实验分析。结果表明未来预测、语义蒸馏和视图合成三个组件对最终性能均有重要贡献。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.24261v1/x7.png" alt="真实世界实验结果"></p>
<blockquote>
<p><strong>图7</strong>：真实世界五个操作任务的定性结果。DynaRend成功完成了放置杯子、开抽屉、堆叠杯子等任务，证明了其从仿真到真实世界的有效迁移。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了DynaRend，一个通过掩码未来渲染学习通用三平面特征的表示学习框架，首次联合建模了3D几何、语义和动态。</li>
<li>系统研究了不同预训练策略（重建与预测目标、掩码策略、视图合成）对下游策略学习的影响。</li>
<li>在仿真（RLBench, Colosseum）和真实世界机器人实验中进行了广泛验证，证明了方法在成功率、泛化性和实用性上的一致提升。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法依赖于多视角RGB-D输入来重建初始点云，在只有单目或少数视角的场景中可能面临挑战。此外，视图合成依赖于特定的预训练生成模型（See3D）。</p>
<p><strong>对后续研究的启示</strong>：DynaRend展示了将神经渲染与自监督目标（掩码重建、未来预测）相结合，学习适用于具身智能的3D动态表征的有效性。这为机器人学习领域指明了一个方向：即超越2D图像或视频表征，发展更紧密扎根于3D物理世界的、能够推理未来状态变化的表征学习方法。如何将此类方法进一步扩展到更稀疏的观测输入（如单视角），或与更复杂的动作生成范式结合，是值得探索的后续方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DynaRend框架，旨在解决机器人操作中因缺乏多样化真实数据而导致的策略泛化难题。其核心方法是利用可微分体积渲染进行掩码重建与未来预测，从多视角RGB-D视频中学习统一的三平面特征，以联合捕获空间几何、未来动态和任务语义。实验表明，该方法在RLBench和Colosseum基准测试及真实机器人实验中，显著提升了策略成功率、对环境扰动的泛化能力及跨任务的实用性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.24261" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>