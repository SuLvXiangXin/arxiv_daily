<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>WPT: World-to-Policy Transfer via Online World Model Distillation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>WPT: World-to-Policy Transfer via Online World Model Distillation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.20095" target="_blank" rel="noreferrer">2511.20095</a></span>
        <span>作者: Xu Yan Team</span>
        <span>日期: 2025-11-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，自动驾驶领域利用世界模型的方法主要分为两类：第一类是将世界模型直接集成到驾驶策略中，用于增强特征演化或轨迹推理，例如Drive-OccWorld和WoTE。这类方法虽然提升了性能，但由于依赖对未来状态的准确预测和自回归展开，步骤间的序列依赖严重阻碍了实时效率。第二类是将世界模型作为模拟器，通过闭环强化学习来训练策略，如图1(c)所示。这类方法高度依赖模拟器生成数据的保真度，主要局限于合成环境评估。现有方法普遍存在运行时紧耦合或依赖离线奖励信号的问题，导致推理开销大或阻碍端到端优化。</p>
<p>本文针对自动驾驶策略如何利用世界知识同时避免额外计算开销这一痛点，提出了一种新的训练范式。核心思路是：在训练阶段，策略与世界模型交互，通过一个可训练的奖励模型将世界模型预测的未来动态知识注入教师策略；随后，通过在线蒸馏将教师的推理能力迁移到一个轻量级学生策略中，从而在部署时无需世界模型，兼顾性能与实时性。</p>
<p><img src="https://arxiv.org/html/2511.20095v1/x1.png" alt="不同训练范式"></p>
<blockquote>
<p><strong>图1</strong>：结合世界模型的自动驾驶策略不同训练范式对比。(a) 模仿学习，策略使用专家监督训练。(b) 世界模型直接集成到策略中，用于增强特征演化和轨迹推理。(c) 基于模拟器的强化学习，使用模拟世界训练策略。(d) 本文提出的WPT范式，策略在训练时与世界模型交互，教师策略(T)和学生策略(S)都利用世界模型进行知识迁移。训练完成后，世界模型将被丢弃。</p>
</blockquote>
<h2 id="方法详解">方法详解</h2>
<p>WPT框架包含四个核心组件：自动驾驶策略、世界模型、奖励模型和世界知识蒸馏。整体流程如图2所示，分为训练和蒸馏两个阶段。在训练阶段，预训练的世界模型根据动作条件预测未来世界状态，教师策略生成多模态候选轨迹，奖励模型评估这些轨迹并产生奖励信号以优化教师。在蒸馏阶段，学生策略通过两种机制从教师处学习：策略蒸馏对齐规划表征，世界奖励蒸馏使学生轨迹的奖励逼近教师最优轨迹的奖励。</p>
<p><img src="https://arxiv.org/html/2511.20095v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：WPT框架总览。训练阶段（上半部分），预训练的世界模型在给定动作条件下预测未来世界，教师策略（T）生成多模态轨迹。奖励模型评估这些轨迹以产生世界奖励。蒸馏阶段（下半部分），学生策略（S）通过两种机制向教师学习：（1）策略蒸馏，对齐师生间的规划表征；（2）世界奖励蒸馏，鼓励学生在预测的未来世界中匹配教师的最优奖励轨迹。</p>
</blockquote>
<p><strong>1. 自动驾驶策略</strong>：采用标准的端到端架构，输入多视角图像序列，编码为世界表征（如BEV特征）。一个基于交叉注意力的规划解码器利用规划查询与世界表征交互，最终由轻量级MLP规划头解码为未来轨迹。教师策略为多模态策略，生成N条候选轨迹供奖励模型评估选择。学生策略为单模态轻量级策略，直接输出单一轨迹，以满足实时推理需求。</p>
<p><strong>2. 世界模型</strong>：采用通用结构，包括观察编码器、特征聚合器和世界解码器。它根据历史观测和动作条件（如历史轨迹、导航命令）自回归地预测未来世界状态的嵌入表示。在训练时，教师策略的规划解码器使用世界模型预测的未来世界嵌入 <code>F_{t+1}^w</code> 作为输入，使其规划与预测的未来动态对齐。</p>
<p><strong>3. 奖励模型</strong>：这是将世界模型知识注入策略的关键模块。其结构如图3所示，它接收预测的未来世界状态和一条候选轨迹，输出对该轨迹的评估。奖励模型提供两种互补的监督信号：</p>
<ul>
<li><strong>模仿奖励</strong>：评估轨迹与专家驾驶行为的对齐程度，通过计算轨迹与专家轨迹的L2距离并经softmax归一化得到目标分数，使用交叉熵损失进行监督。</li>
<li><strong>模拟奖励</strong>：从环境中心视角评估轨迹的安全性、舒适性和效率，包含无碰撞、可行驶区域合规、碰撞时间、自我进度和舒适度五个指标，使用二元交叉熵损失进行监督。<br>最终奖励是两种奖励的对数加权和，以此平衡人类驾驶偏好和环境感知评估，从而选择最优轨迹。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.20095v1/x3.png" alt="奖励模型"></p>
<blockquote>
<p><strong>图3</strong>：奖励模型概览。包含世界编码器、轨迹编码器和两个奖励头（模仿奖励和模拟奖励），通过结合两种奖励来选择最优轨迹。奖励模型的监督信号来自模拟和模仿。</p>
</blockquote>
<p><strong>4. 世界知识蒸馏</strong>：为将教师能力迁移至轻量学生，设计了两种仅用于训练的蒸馏策略：</p>
<ul>
<li><strong>策略蒸馏</strong>：最小化学生规划查询 <code>Q^S</code> 与教师规划查询 <code>Q^T</code> 之间的L2距离，以对齐规划意图。</li>
<li><strong>世界奖励蒸馏</strong>：将学生生成的单条轨迹和教师经奖励模型选出的最优轨迹分别输入奖励模型，最小化二者最终奖励分数的L2距离，使学生隐式学习教师的评估标准。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，WPT的核心创新在于提出了一个纯训练阶段的范式。它通过一个<strong>可训练的、交互式的奖励模型</strong>作为桥梁，将世界模型的预测能力转化为对策略的监督信号，而非在部署时依赖世界模型展开。同时，通过<strong>策略蒸馏和世界奖励蒸馏</strong>的双重机制，实现了世界知识向高效策略的有效迁移，解决了性能与效率的矛盾。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：在开环规划评估中使用nuScenes数据集；在闭环规划评估中使用基于CARLA的Bench2Drive基准。遵循各自的标准数据划分和评估协议。</p>
<p><strong>对比方法</strong>：对比了广泛的基线方法，包括基于模仿学习的方法（如UniAD, VAD）、基于世界模型的方法（如Drive-OccWorld, WoTE, DriveDPO）以及其他前沿方法（如GenAD, DiffAD）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>开环性能（nuScenes）</strong>：如表1所示，WPT-Teacher取得了最佳性能，平均L2误差为0.61米，平均碰撞率仅为0.11%。WPT-Student也表现出色，平均L2误差为0.66米，碰撞率0.24%，显著优于基线。</li>
<li><strong>闭环性能（Bench2Drive）</strong>：如表2所示，WPT-Teacher以79.23的驾驶分数和54.54%的成功率显著超越所有对比方法。WPT-Student也达到了72.61的驾驶分数和45.45%的成功率。</li>
<li><strong>推理效率</strong>：学生策略实现了高达4.9倍的推理加速，同时保留了大部分性能增益。</li>
<li><strong>消融实验</strong>：如图6所示，实验验证了奖励模型中模仿奖励和模拟奖励各自的有效性，以及两种蒸馏策略（策略蒸馏PD和世界奖励蒸馏WRD）的贡献。两者结合能取得最佳性能。</li>
<li><strong>定性结果</strong>：如图7所示，在复杂交互场景中，基线方法可能产生碰撞或偏离，而WPT能规划出安全、平滑且符合人类驾驶习惯的轨迹。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.20095v1/x4.png" alt="开环结果"></p>
<blockquote>
<p><strong>图4</strong>：在nuScenes验证集上的开环规划性能对比。WPT-Teacher取得了0.61m的平均L2误差和0.11%的平均碰撞率，达到SOTA。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.20095v1/x5.png" alt="闭环结果"></p>
<blockquote>
<p><strong>图5</strong>：在Bench2Drive上的闭环规划性能对比。WPT-Teacher以79.23的驾驶分数和54.54%的成功率显著超越基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.20095v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融实验结果。展示了奖励模型组件（模仿奖励R_im和模拟奖励R_sim）以及蒸馏策略（策略蒸馏PD和世界奖励蒸馏WRD）对性能的贡献。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.20095v1/x7.png" alt="定性结果"></p>
<blockquote>
<p><strong>图7</strong>：定性结果对比。在复杂交互场景中，基线方法（红色）可能产生碰撞或偏离，而WPT（绿色）能规划出安全、平滑且符合人类驾驶习惯的轨迹。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>WPT训练范式</strong>，通过在线蒸馏将世界建模与可训练奖励模型结合，实现了可解释、与世界一致的规划，且部署时无需世界模型。</li>
<li>设计了<strong>策略蒸馏和世界奖励蒸馏</strong>机制，成功将大型模型的推理能力迁移至轻量级策略。</li>
<li>在开环和闭环基准上进行了广泛实验，证明了WPT在达到最先进性能的同时，能通过学生策略实现显著的推理加速。</li>
</ol>
<p><strong>局限性</strong>：论文提到，WPT的性能依赖于预训练世界模型的质量。世界模型的预测准确性直接影响奖励模型和后续蒸馏的效果。</p>
<p><strong>启示</strong>：</p>
<ul>
<li><strong>训练与部署解耦</strong>：WPT展示了将复杂模型能力“压缩”进轻量级模型的可行路径，为平衡AI系统性能与效率提供了新思路。</li>
<li><strong>知识迁移机制</strong>：所提出的双重蒸馏策略（表征对齐与奖励对齐）可推广至其他需要将复杂模型知识迁移到轻量模型的场景。</li>
<li><strong>端到端优化</strong>：未来可探索将世界模型与策略进行端到端的联合训练，以进一步提升整体系统的协同性。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文WPT解决现有世界模型方法因运行时耦合紧密或依赖离线奖励导致的推理开销大、端到端优化困难问题。提出World-to-Policy Transfer训练范式，通过在线世界模型蒸馏，利用可训练奖励模型将候选轨迹与预测动态对齐，注入世界知识到教师策略，再经策略蒸馏和世界奖励蒸馏转移至轻量学生策略。实验显示，WPT在开环基准碰撞率0.11，闭环驾驶得分79.23，超越世界模型和模仿学习方法，且学生策略推理速度提升达4.9倍，保持高性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.20095" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>