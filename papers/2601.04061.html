<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.04061" target="_blank" rel="noreferrer">2601.04061</a></span>
        <span>作者: Zhang, Chubin, Wang, Jianan, Gao, Zifeng, Su, Yue, Dai, Tianru, Zhou, Cai, Lu, Jiwen, Tang, Yansong</span>
        <span>日期: 2026/01/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通用视觉-语言-动作（VLA）模型的发展受到机器人数据稀缺的限制，而人类视频演示则非常丰富。现有的潜在动作模型（LAMs）试图利用视频数据，但通常存在视觉纠缠问题，即捕获了背景变化、物体形变等噪声，而非纯粹的操控技能。这导致学习到的表示与机器人的物理动作空间不一致，需要复杂的后处理训练才能映射到机器人控制，严重限制了从人类视频直接迁移技能的能力。</p>
<p>本文针对现有LAMs中视觉潜在空间与可执行动作空间未对齐这一关键痛点，提出了对比潜在动作预训练（CLAP）的新视角。其核心思路是通过对比学习，将人类视频中的视觉状态转移与机器人轨迹中的本体感知潜在空间对齐，从而学习到一个量化、物理上可执行的潜在动作表示，并基于此构建了一个兼具高层推理与高频控制能力的双框架VLA模型。</p>
<h2 id="方法详解">方法详解</h2>
<p>CLAP框架分为两个连贯的阶段：1）通过对比潜在动作预训练（CLAP）进行跨模态对齐；2）分层策略训练，包括CLAP-NTP和CLAP-RF两个模型。</p>
<p><img src="https://arxiv.org/html/2601.04061v1/x3.png" alt="方法流程总览"></p>
<blockquote>
<p><strong>图3</strong>：CLAP的完整流程。(a) 对比潜在动作预训练：通过对比学习，将视频中的视觉状态转移与量化的机器人动作对齐，建立一个共享的、物理基础的潜在空间。(b) VLA框架：我们引入了用于离散自回归规划的CLAP-NTP，以及通过Rectified Flow专家实现连续高频控制的CLAP-RF。</p>
</blockquote>
<p><strong>第一阶段：对比潜在动作预训练（CLAP）</strong><br>此阶段旨在建立一个连接人类视频（仅有视觉观察）和机器人数据（带有动作标签）的共享离散潜在动作空间𝒵。</p>
<ol>
<li><strong>语义动作量化（Act-VAE）</strong>：首先，使用一个基于Transformer的VQ-VAE（称为Act-VAE）将连续的机器人动作轨迹 𝐚 量化为离散的令牌序列 𝐳_a。其损失函数包括动作重建损失、编码器输出与量化码本的承诺损失，以及码本向量与编码器输出的码本损失。</li>
<li><strong>跨模态动态对齐（VD-VAE）</strong>：然后，引入视觉动态VQ-VAE（VD-VAE），作为一个逆动力学模型。它接收一对视频帧（𝐨<em>t, 𝐨</em>{t+H}），通过一个冻结的视觉骨干网络（如DINO）提取特征，并由一个逆动力学编码器将帧间转移分解为两个解耦的潜在流：动作相关潜在 𝐳_{v,a} 和动作无关潜在 𝐳_{v,i}。关键创新在于，强制 𝐳_{v,a} 通过冻结的Act-VAE码本进行量化，从而将其锚定在机器人的控制空间。同时，使用对比损失（采用SigLIP损失函数）来对齐视觉编码的连续潜在 𝐳_{v,a} 和Act-VAE编码的连续动作潜在 𝐳<em>a。对于未标注的人类视频，采用自监督方式，将 𝐳</em>{v,a} 作为自身的正样本与批次内的其他样本进行对比。此外，对 𝐳_{v,i} 施加L1正则化以鼓励稀疏性，确保其仅捕获干扰信息。VD-VAE的总损失结合了动态重建损失、VQ约束损失、对比对齐损失和动作无关潜在的正则化损失。</li>
</ol>
<p><strong>第二阶段：双框架VLA模型训练</strong><br>基于对齐的潜在空间，训练两个协同进化的策略模型。</p>
<ol>
<li><strong>CLAP-NTP：离散推理与规划</strong>：这是一个自回归VLA模型，将复杂指令 ℐ 分解为中间子任务和离散动作令牌序列 Y = [𝐲<em>sub, 𝐳_a]。它通过下一令牌预测损失进行训练，统一使用机器人演示（真实 𝐳_a）和人类视频（由VD-VAE推断的伪标签 𝐳</em>{q,a}）进行训练，从而保留了骨干VLM的推理和指令遵循能力。</li>
<li><strong>CLAP-RF：基于整流流的高频控制</strong>：为解决自回归解码速度慢的问题，将NTP模型的能力蒸馏到CLAP-RF中。CLAP-RF使用扩散Transformer（DiT）作为连续动作专家。DiT通过交叉注意力机制查询冻结的VLM骨干的内部表示（Key和Value缓存），并利用停止梯度操作构建单向信息桥，以保护VLM骨干免受动作生成的高方差梯度影响。动作专家通过最小化整流流损失进行训练：对动作块 𝐚_{1:H} 添加噪声得到 𝐚^τ_{1:H}，模型训练预测向量场 𝐯 = 𝐚_{1:H} - ϵ。</li>
</ol>
<p><strong>知识匹配（KM）正则化策略</strong><br>为了在微调过程中缓解错误累积和灾难性遗忘，提出了知识匹配策略。它作为一个正则化项，通过约束微调模型与预训练模型在相同输入下的输出分布（如动作令牌的logits）保持接近，将策略更新锚定在预训练模型的可信区域内，从而在适应特定任务的同时保留语义知识。</p>
<p><img src="https://arxiv.org/html/2601.04061v1/x4.png" alt="知识匹配算法"></p>
<blockquote>
<p><strong>图4</strong>：知识匹配算法示意图。灰色块代表输入观察和指令，KM损失约束微调模型的预测分布与预训练模型的分布保持一致。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要评估平台为Astribot S1双臂机器人。使用了多个数据集：内部收集的机器人操作数据集、AgiBot Go-1数据集以及人类视频数据集Ego4D。评估任务包括6类日常长视野操作任务（如“打包礼物”、“整理玩具”），共18个具体任务。<br><strong>对比方法</strong>：对比了多个强基线，包括RT-1、RT-2-X、Octo-Base、Octo-Small、MOO、VLA-V、VLA-V (FT)、VLA-L、VLA-L (FT) 以及 π₀。<br><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能</strong>：在18个任务上，CLAP-NTP和CLAP-RF均显著优于所有基线。CLAP-NTP取得了最高的平均成功率（76.3%），比最佳基线VLA-V高出15.8%。CLAP-RF也达到了73.3%的成功率。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.04061v1/x5.png" alt="主要实验结果"></p>
<blockquote>
<p><strong>图5</strong>：在Astribot S1上18个长视野操作任务的成功率。CLAP-NTP和CLAP-RF均显著优于所有基线方法。</p>
</blockquote>
<ol start="2">
<li><strong>泛化到新物体</strong>：在“包装未见过的物体”任务中，仅使用人类视频（Ego4D）进行训练，CLAP-NTP在6个新物体上实现了平均71.7%的成功率，证明了其通过人类视频实现零样本物体泛化的强大能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.04061v1/x6.png" alt="物体泛化结果"></p>
<blockquote>
<p><strong>图6</strong>：零样本物体泛化结果。CLAP-NTP仅通过观看人类视频，就能成功泛化到在训练中未见过的新物体。</p>
</blockquote>
<ol start="3">
<li><strong>高频控制精度</strong>：在需要精细操作的任务（如“折叠布料”、“打包礼物”）中，CLAP-RF的表现优于专门设计的基线π₀。CLAP-RF在NVIDIA RTX 3090上的推理延迟仅为183毫秒。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.04061v1/x7.png" alt="精细操作结果"></p>
<blockquote>
<p><strong>图7</strong>：精细操作任务上的成功率。CLAP-RF在需要高精度和高频控制的任务上超越了基线π₀。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：<ul>
<li><strong>CLAP预训练的有效性</strong>：移除对比对齐（仅用重建损失）会导致性能大幅下降（从76.3%降至61.1%），证明了跨模态对齐对于学习可执行表示至关重要。</li>
<li><strong>知识匹配（KM）的作用</strong>：在微调时加入KM正则化能稳定提升性能并防止遗忘，而未使用KM则会导致性能波动和下降。</li>
<li><strong>双框架设计的优势</strong>：CLAP-NTP在需要强推理的任务上表现更好，而CLAP-RF在需要快速、精确控制的任务上更优，验证了双框架设计的互补性。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.04061v1/x8.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图8</strong>：消融研究结果。(a) 对比学习对齐是性能提升的关键。(b) 知识匹配正则化能有效防止微调时的性能下降。(c) CLAP-NTP和CLAP-RF在不同类型任务上各有优势。</p>
</blockquote>
<ol start="5">
<li><strong>潜在空间可视化</strong>：学习到的潜在动作令牌在语义上对齐了不同机器人（Astribot, AgiBot）和人类（Ego4D）领域，例如“向右移动”、“放置”、“抓取”等动作类别在不同领域具有一致的语义。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.04061v1/x1.png" alt="潜在空间可视化"></p>
<blockquote>
<p><strong>图1</strong>：对齐的潜在动作空间可视化。展示了聚类后的动作令牌样本，证明了其在不同机器人（Astribot, AgiBot）和人类（Ego4D）领域间的语义对齐。组1-3分别对应向右移动、放置和抓取。Astribot S1帧上的红色箭头是将潜在动作解码并投影到图像平面上的预测3D轨迹可视化，证实了所学表示的物理可执行性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>指出了现有潜在动作模型中视觉纠缠的根本问题，并提出了CLAP预训练框架，通过对比学习显式地将人类视觉动态与机器人动作空间对齐，学习到物理可执行的潜在表示。</li>
<li>提出了一个双框架VLA模型，包括擅长指令遵循和零样本泛化的自回归模型CLAP-NTP，以及专为高频、精确控制设计的基于整流流的策略CLAP-RF。</li>
<li>引入了知识匹配正则化策略，有效缓解了潜在动作模型微调时的错误累积和灾难性遗忘问题。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，方法在涉及极高动态（如投掷）或复杂背景/遮挡的任务上可能面临挑战。此外，当前框架主要针对桌面操作场景。</p>
<p><strong>启示</strong>：CLAP为利用海量、多样化的人类视频数据来增强机器人学习提供了有效路径。其核心思想——通过跨模态对比学习建立与物理控制基础对齐的表示——可以推广到其他需要从非结构化观察中学习可执行技能的领域。双框架设计平衡了推理与控制，为构建兼具“大脑”和“小脑”的通用具身智能体提供了参考。知识匹配策略对持续学习和微调大模型具有普适的借鉴意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对从人类视频学习机器人技能时存在的视觉纠缠问题，提出对比性潜在动作预训练（CLAP）框架。其核心是通过对比学习，将视频视觉潜在空间与机器人本体感知潜在空间对齐，并映射到量化可执行的动作码本。基于此，论文构建了包含自回归模型（CLAP-NTP）和整流流策略（CLAP-RF）的双框架VLA模型，并引入知识匹配正则化以缓解微调时的灾难性遗忘。实验表明，该方法显著优于现有基线，实现了从人类视频到机器人操作的有效技能迁移。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.04061" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>