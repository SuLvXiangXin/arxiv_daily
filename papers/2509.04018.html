<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.04018" target="_blank" rel="noreferrer">2509.04018</a></span>
        <span>作者: Jingtai Liu Team</span>
        <span>日期: 2025-09-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作是自动化的基石。传统方法采用“感知-规划”流水线，依赖精确的物体识别与几何建模，在开放、动态任务中灵活性不足。而新兴的端到端视觉-语言-动作模型通过模仿学习直接映射视觉和语言指令到动作，展现出强大的泛化能力，但其训练数据通常仅包含成功轨迹，导致模型缺乏预测及从潜在失败中恢复的能力。此外，现有方法在动作执行时往往只采用预测序列的一部分，忽略了动作轨迹的多模态特性，且缺乏一种标准化、可扩展的方法来生成大规模的失败预测与纠正数据。</p>
<p>本文针对VLA模型缺乏失败预见与恢复机制这一具体痛点，提出了一个集成监督器的双模型新框架。核心思路是：在VLA模型执行动作前，引入一个基于VLM的监督器，在关键帧处主动评估动作可行性，若预测到潜在失败，则生成结构化语言纠正指令；同时，通过双流动作融合模块聚合历史预测，以产生更平滑可靠的动作输出。</p>
<h2 id="方法详解">方法详解</h2>
<p>FPC-VLA是一个包含三个核心模块的框架：1) 自动化的机器人失败数据集生成方法；2) 基于VLM的失败预测与纠正监督器；3) 双流动作融合模块。</p>
<p><img src="https://arxiv.org/html/2509.04018v2/x2.png" alt="方法架构"></p>
<blockquote>
<p><strong>图2</strong>：FPC-VLA整体架构。输入为观测图像和自然语言指令。VLA模型首先预测动作序列，随后由双流动作融合模块整合历史预测与当前预测，生成机械臂末端执行器位姿增量和夹爪状态。在夹爪状态发生变化的关键帧，基于VLM的监督器检测潜在失败并提供纠正指导以优化动作执行。</p>
</blockquote>
<p><strong>整体流程</strong>：输入为时间步t的RGB图像I_t和语言指令l。输出为经过修正的动作a_t&#39; ∈ ℝ^7。VLA模型首先基于当前观测预测未来动作。该预测随后送入双流动作融合模块，与基于过去N步观测所做的历史预测进行融合，得到平滑后的动作a_t_hat。在夹爪状态g_t发生变化的关键帧，监督器被触发，评估当前动作的可行性。若预测到失败，监督器生成包含方向和幅度调整的自然语言纠正，进而修正融合后的动作，得到最终输出a_t&#39;。</p>
<p><strong>核心模块1: Vision-Language-Action模型</strong>：该模型负责根据多模态输入生成初始动作预测。<br><img src="https://arxiv.org/html/2509.04018v2/x3.png" alt="VLA模型架构"></p>
<blockquote>
<p><strong>图3</strong>：VLA模型架构。输入图像通过DINO和SigLIP双视觉编码器提取特征，经MLP投影器对齐后与分词化的语言特征拼接，输入Llama2获取认知特征。这些特征作为条件输入到扩散Transformer中，通过逐步去噪生成基于当前观测的预测动作序列。</p>
</blockquote>
<ul>
<li><strong>视觉模块</strong>：采用双编码器架构，分别使用DINO和SigLIP backbone提取图像特征，拼接后通过线性投影层对齐到语言模型的嵌入空间，得到视觉token V。</li>
<li><strong>语言模块</strong>：基于Llama2，将语言指令l分词为token序列 L，并与视觉token V以及BOS token拼接，形成多模态嵌入E_multi，输入Transformer后输出认知token Z。</li>
<li><strong>动作模块</strong>：采用扩散Transformer。在训练时，模型学习从加噪的ground-truth动作序列x_t中预测噪声ε，条件为认知token Z和时间步t。推理时，从随机噪声x_T开始，通过T步迭代去噪（以Z为条件）生成最终动作序列x_0。</li>
</ul>
<p><strong>核心模块2: 双流动作融合模块</strong>：旨在利用历史预测的多模态信息，产生更平滑、鲁棒的动作。其核心创新在于对位姿（pose）和夹爪状态（gripper）进行<strong>解耦的、差异化的融合策略</strong>。</p>
<ol>
<li><strong>历史预测收集</strong>：收集基于过去N步观测对当前时刻动作的预测集合A_t = {a_t^(k) | k=0,..., N-1}，其中a_t^(k) = [p_t^(k), g_t^(k)]，分别代表6维相对末端位姿和标量夹爪状态。</li>
<li><strong>位姿流融合</strong>：<ul>
<li><strong>相似性加权</strong>：计算每个历史位姿预测p_t^(k)与最新预测p_t^(0)的余弦相似度sim^(k)，并通过sigmoid函数平滑为s^(k)。这衡量了历史预测与当前意图的语义对齐度。</li>
<li><strong>时间衰减</strong>：应用指数衰减d^(k)=exp(-λ·k)，赋予近期预测更高权重。</li>
<li><strong>综合权重与融合</strong>：结合相似性权重和时间衰减，并引入正则化因子β防止过拟合，计算最终融合权重w^(k)。融合位姿p_t_hat为各历史位姿的加权平均。</li>
</ul>
</li>
<li><strong>夹爪流融合</strong>：<strong>论文明确指出，由于夹爪状态与末端位姿在语义、量级和行为上解耦，不宜使用基于位姿计算的相似度进行加权</strong>。因此，夹爪状态的融合仅基于时间衰减权重d~(k)进行加权平均，得到g_t_hat。</li>
<li><strong>输出</strong>：最终融合动作为a_t_hat = [p_t_hat, g_t_hat]。</li>
</ol>
<p><strong>核心模块3: 基于VLM的失败预测与纠正监督器</strong>：</p>
<ul>
<li><strong>触发机制</strong>：在检测到夹爪状态g_t发生变化的关键帧被激活。</li>
<li><strong>查询与评估</strong>：监督器（一个VLM）接收当前图像I_t和一个精心构建的结构化提示P_t作为输入。P_t通常包含任务描述、关于夹爪动作可行性的具体问题（例如，“夹爪是否对准了物体？”）以及答案格式约束。</li>
<li><strong>纠正生成</strong>：VLM输出响应R_t。如果R_t以“No”开头，表明预测到潜在失败，则从后续文本中解析出平移纠正Δp ∈ ℝ^3和绕Z轴旋转纠正Δr_z ∈ ℝ，用于修正融合后的动作a_t_hat，得到最终安全动作a_t&#39;。</li>
</ul>
<p><strong>自动化失败数据集生成</strong>：为解决数据瓶颈，论文提出从现有RLDS格式数据自动生成大规模失败预测与纠正数据集的方法。核心步骤包括：1）通过检测夹爪状态变化识别抓取事件；2）为每个事件保留相关帧；3）计算真实动作与失败动作间的位姿差异；4）根据阈值（δ_small, δ_large）将该差异转换为“向左/右/前/后微调/大幅调整”等结构化语言描述；5）结合任务和答案约束形成训练监督器所需的问答对提示P_t。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真基准</strong>：在SIMPLER和LIBERO基准上进行评估。</li>
<li><strong>机器人平台</strong>：使用了多种仿真机器人实体（WidowX, Google Robot, Franka）以及真实机器人系统（小米机器人、ALOHA）。</li>
<li><strong>对比方法</strong>：与多个先进的VLA模型进行对比，包括RT-1、RT-2、Octo、OpenVLA、SpatialVLA、TraceVLA等。</li>
</ul>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2509.04018v2/x4.png" alt="SIMPLER基准结果"></p>
<blockquote>
<p><strong>图4</strong>：在SIMPLER基准上的成功率对比。FPC-VLA在WidowX、Google Robot和Franka三种实体上均取得最高成功率，显著优于其他SOTA方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.04018v2/x5.png" alt="LIBERO基准结果"></p>
<blockquote>
<p><strong>图5</strong>：在LIBERO基准上的任务成功率。FPC-VLA在10个长视野任务中平均成功率达到77.3%，远超其他对比模型，尤其是在需要复杂序列操作的任务上优势明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.04018v2/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：在SIMPLER上的消融实验。依次移除双流动作融合模块（w/o Fusion）、失败纠正监督器（w/o Correction）和失败预测监督器（w/o Prediction）会导致性能逐步下降，证明了各模块的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.04018v2/x7.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图7</strong>：在小米机器人上的真实世界部署结果。FPC-VLA在多种日常物体操作任务中均取得成功，展示了其强大的泛化能力和实用性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.04018v2/x8.png" alt="失败预测与纠正定性结果"></p>
<blockquote>
<p><strong>图8</strong>：失败预测与纠正的定性示例。左列展示了监督器成功预测到夹爪未对准杯子把手，并生成“向右微调”的指令，从而避免了失败。右列展示了在堆叠积木任务中，监督器预测到抓取位置不佳并给出纠正。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.04018v2/x9.png" alt="动作融合效果"></p>
<blockquote>
<p><strong>图9</strong>：双流动作融合的定性效果。对比原始VLA输出、仅时间衰减融合以及FPC-VLA的双流融合，可见双流融合产生的末端执行器路径（红色）最平滑、抖动最少。</p>
</blockquote>
<p><strong>消融实验总结</strong>：图6的消融实验清晰展示了每个组件的贡献。移除整个双流动作融合模块（w/o Fusion）导致性能下降约11.2%。进一步移除失败纠正（w/o Correction）和失败预测（w/o Prediction）功能，性能继续分别下降约5.4%和4.3%。这验证了动作融合、纠正和预测三者对于提升整体系统可靠性的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了一个集成VLM监督器的双模型框架</strong>：通过主动预测和语言纠正，使VLA模型具备了关键的失败预见与恢复能力。</li>
<li><strong>设计了双流动作融合模块</strong>：通过解耦并差异化地融合历史位姿与夹爪状态预测，产生了更平滑、鲁棒的动作序列，有效利用了动作的多模态特性。</li>
<li><strong>引入了自动化失败数据集生成方法</strong>：为大规模训练失败预测与纠正模型提供了一种可行的解决方案，缓解了该领域的数据瓶颈。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，其失败预测与纠正依赖于在关键帧（夹爪状态变化时）触发监督器进行查询。这种离散的、基于事件的检查可能无法覆盖所有类型的连续动作失败。此外，监督器的性能依赖于其底层VLM的视觉推理和语言遵循能力。</p>
<p><strong>启示</strong>：FPC-VLA的工作标志着从“被动执行”到“主动监控与修正”的VLA范式转变。它为构建更可靠、安全的自主机器人系统提供了新思路：将大型基础模型的强语义理解与推理能力，作为传统控制循环中的“安全层”或“反思模块”。未来研究可探索更细粒度、更频繁的在线监控机制，以及将纠正指令直接闭环到策略网络的在线更新中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出FPC-VLA框架，旨在解决机器人操作中单端到端视觉-语言-动作模型缺乏失败预测与恢复机制的问题。核心技术包括：一个基于视觉语言模型的监督器，通过结构化查询评估动作并生成语言纠正；一个从现有数据自动生成失败数据集的流程；以及一个使用余弦相似度和时间衰减的双流动作融合模块，用于平滑动作。实验表明，FPC-VLA在多个仿真平台与机器人实体上优于现有方法，并在真实机器人上成功部署，验证了其泛化能力与实用性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.04018" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>