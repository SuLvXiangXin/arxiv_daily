<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Spotlight on Token Perception for Multimodal Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Spotlight on Token Perception for Multimodal Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.09285" target="_blank" rel="noreferrer">2510.09285</a></span>
        <span>作者: Huang, Siyuan, Qu, Xiaoye, Li, Yafu, Luo, Yun, He, Zefeng, Liu, Daizong, Cheng, Yu</span>
        <span>日期: 2025/10/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于可验证奖励的强化学习（RLVR）已显著提升了大语言模型在文本领域的推理能力。然而，将这一成功扩展到大型视觉语言模型（LVLMs）时，主流方法大多聚焦于数据增强、奖励工程或其他算法调整等外部组件，却普遍忽视了视觉感知在核心优化过程中的关键作用。有效的推理依赖于准确的感知，它为逻辑演绎提供了基础。现有RLVR框架（如GRPO）直接将单一的、基于结果的粗粒度奖励广播给轨迹中的所有token，这带来了两个关键局限：一是轨迹级模糊性，无法区分真正基于视觉证据的推理路径和仅靠语言先验或幻觉得到正确答案的路径；二是token级均匀性，未能选择性地奖励那些导致正确结果的、关键的视觉推理时刻。</p>
<p>本文针对多模态RLVR中感知与优化过程脱节的痛点，提出了一个新颖的“token感知”视角，即度量每个生成token的视觉依赖性。通过对思维链（CoT）过程的细粒度分析，本文发现两个关键现象：首先，一个轨迹中的token感知是稀疏分布的，只有少数token对视觉推理具有高依赖性；其次，不同轨迹在整体视觉依赖性上存在显著差异。基于这些观察，本文的核心思路是提出一种新颖的策略梯度算法（VPPO），显式地利用token感知来重塑学习信号，从而增强LVLMs的多模态推理能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的视觉感知策略优化（VPPO）是一个分层重塑学习信号的策略梯度算法，其整体框架如图2所示。给定原始图像和扰动后的图像输入，模型生成对应的输出分布。核心流程是：首先计算每个token的视觉依赖性得分；随后，这些token级得分被用于生成两个层次的控制信号——在宏观层面，它们被平均为轨迹级依赖性以重塑优势函数；在微观层面，得分最高的top-k% token被选出以创建一个稀疏的二元token梯度掩码。最终，均匀的优势信号被转化为细粒度的、有针对性的学习信号用于策略更新。</p>
<p><img src="https://arxiv.org/html/2510.09285v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：VPPO框架概述。给定原始图像和掩码图像输入，首先获得对应的输出分布，然后计算每个token的视觉依赖性得分。这些得分用于生成轨迹级优势重塑和token级梯度过滤两种控制信号。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>Token视觉依赖性量化</strong>：这是VPPO的基础度量。对于一个给定的状态（查询和已生成token），token在步骤t的视觉依赖性定义为策略在真实图像I和扰动版本I&#39;条件下输出分布的KL散度：𝒮(s_t, I) := D_KL(π_θ(·|s_t, I) ∥ π_θ(·|s_t, I‘))。高𝒮值表明图像为该token的预测提供了关键信息，标志着这是视觉推理的关键时刻。</li>
<li><strong>关键发现分析</strong>：基于上述度量，对MathVerse基准的分析揭示了两个核心洞察，如图3和图4所示。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09285v1/x3.png" alt="Token依赖性分布"></p>
<blockquote>
<p><strong>图3</strong>：token级视觉依赖性的偏态分布。频率随依赖性增加呈指数下降，表明只有一小部分token对视觉推理至关重要。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.09285v1/x4.png" alt="轨迹依赖性分布"></p>
<blockquote>
<p><strong>图4</strong>：轨迹级依赖性的分布。分布呈现异质性且右偏，表明存在一个明显的高依赖性轨迹子集。</p>
</blockquote>
<ol start="3">
<li><strong>VPPO的双重机制</strong>：<ul>
<li><strong>微观层面：Token级梯度过滤（TGF）</strong>：受洞察1启发，VPPO将学习信号聚焦于关键token。对于每个轨迹，识别视觉依赖性得分最高的top-k% token的索引集合𝒦_i，并构建一个二元梯度掩码m_i,t。该掩码确保策略梯度仅针对连接视觉和语言的关键token进行计算，有效过滤了来自通用token的噪声。</li>
<li><strong>宏观层面：轨迹级优势重塑（TAS）</strong>：受洞察2启发，VPPO优先从高质量、高依赖性轨迹中学习。它为批次ℬ中的每个轨迹τ_i计算一个整形因子α(τ_i)，通过将其轨迹依赖性（token依赖性的均值）归一化到范围[β_min, β_max]内得到。该因子重新缩放原始的GRPO优势，创建了一个“重塑优势”：A^‘(τ_i) = α(τ_i) · A^_GRPO(τ_i)，从而自适应地放大对高视觉参与轨迹的更新，抑制对较少视觉基础轨迹的更新。</li>
</ul>
</li>
<li><strong>VPPO目标函数</strong>：整合上述两种调制，得到最终的VPPO目标函数。它通过掩码m_i,t将重塑优势A^_i‘ 专门引导至最具依赖性的token：ℒ^VPPO(θ) = 𝔼[ (1/G) Σ_i=1^G (1/|o_i|) Σ_t=1^|o_i| m_i,t · min( r_i,t(θ) A^_i‘, clip(r_i,t(θ), 1-ε, 1+ε) A^_i‘ ) ]，其中A^_i‘ = α(τ_i) · A^_GRPO,i。</li>
</ol>
<p><strong>创新点</strong>：与现有方法（如GRPO、DAPO）对所有token广播均匀学习信号不同，VPPO的创新性在于首次从token感知的视角分析多模态RLVR，并基于此提出了层次化的、感知中心的优化策略。它通过内部干预，利用视觉依赖性来区分轨迹和聚焦关键token，直接解决了信号稀释和轨迹模糊性问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验基于Qwen2.5-VL-7B和Qwen2.5-VL-32B基础模型，在ViRL39K数据集上进行训练。在八个多模态推理基准上进行全面评估，包括DynaMath、Geo3k、MathVerse、MathVision、MMK12、We-Math、LogicVista和MMMU-Pro，覆盖数学、几何、逻辑和多学科推理。采用精确匹配评分。对比的基线包括DAPO、MM-Eureka、ThinkLite、VL-Rethinker、NoisyRollout、R1-ShareVL、PAPO-D等领先的开源RL调优模型。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>主实验结果</strong>：如表1所示，VPPO在7B和32B模型规模上均一致性地超越了所有开源竞争对手。在7B类别中，VPPO平均准确率达到57.5%，显著优于次优的PAPO模型（55.3%）。其优势直接扩展到32B类别，VPPO以64.6%的平均准确率领先。与基础模型相比，7B版本实现了19.2%的平均准确率提升，32B版本实现了7.6%的提升。</li>
<li><strong>训练动态</strong>：如图5所示，VPPO展现出显著更快的初始收敛速度，能更高效地达到更高性能，表明其目标学习信号起到了有效的隐式正则化作用。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09285v1/x5.png" alt="训练动态"></p>
<blockquote>
<p><strong>图5</strong>：VPPO与基线的训练动态对比。VPPO表现出更快的初始收敛和更高的最终性能。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>组件贡献</strong>：如表2所示，轨迹级优势重塑（TAS）和token级梯度过滤（TGF）各自均能超越基线（DAPO）。TGF提供了最大的单组件贡献，凸显了将学习信号导向关键token的重要性。而两者结合（完整VPPO）实现了最佳性能，证实了层次化设计的协同价值。</li>
<li><strong>超参数敏感性</strong>：<ul>
<li><strong>梯度过滤比率k</strong>：如图6所示，性能在k=0.4附近达到峰值。k值过低则学习信号不足，过高则会重新引入非关键token的噪声，验证了稀疏更新策略的有效性。</li>
<li>**优势重塑范围[β_min, β_max]**：如表3所示，采用保守下界和动态上界（β_min=0.9, β_max=Dyn.）是最优配置。该设置能根据每批次内的依赖性分布自适应地重新加权优势，在防止过度激进更新的同时，仍能奖励视觉基础的推理。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09285v1/x6.png" alt="梯度过滤比率消融"></p>
<blockquote>
<p><strong>图6</strong>：梯度过滤比率k的消融研究。平均分数在k=0.4时达到峰值，显示了稀疏更新的重要性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>首次从token感知的创新视角分析了多模态RLVR的感知机制，发现了token视觉依赖性的稀疏分布以及轨迹间视觉基础的异质性。</li>
<li>提出了VPPO，一种新颖的策略梯度算法，它显式地利用token视觉依赖性，通过轨迹级优势重塑和token级梯度过滤双重机制，将学习信号聚焦于感知关键点和高质量轨迹。</li>
<li>在八个具有挑战性的多模态推理基准上进行了广泛实验，证明了VPPO在7B和32B模型规模上的卓越性能、更快的收敛速度和优秀的训练稳定性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，VPPO在计算token视觉依赖性时需要前向传播两次（原始图像和扰动图像），这带来了额外的计算开销。此外，依赖性的计算质量依赖于图像扰动策略的有效性。</p>
<p><strong>启示</strong>：本研究为多模态强化学习提供了一个新的、细粒度的分析框架和优化方向。它表明，超越简单的结果奖励，深入理解模型内部推理过程（如感知依赖模式）并据此设计针对性的学习信号，是进一步提升复杂多模态任务性能的有效途径。未来的工作可以探索更高效的依赖性估计方法，或将此感知中心的优化理念与其他RL算法、模型架构相结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多模态强化学习（RLVR）中视觉感知在优化过程中被忽视的核心问题，提出了**令牌感知**的新视角，以度量每个生成令牌对视觉信息的依赖程度。研究发现，推理轨迹中的令牌感知分布稀疏且轨迹间差异显著。基于此，作者提出了**视觉感知策略优化（VPPO）**算法，其通过双重机制优化学习信号：依据轨迹整体视觉依赖性重新加权优势函数，并集中更新对感知至关重要的关键令牌。在八个基准测试上的实验表明，VPPO显著超越了现有的开源RL调优模型，其有效性在7B和32B模型规模上均得到一致验证。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.09285" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>