<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Exploring Conditions for Diffusion models in Robotic Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Exploring Conditions for Diffusion models in Robotic Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.15510" target="_blank" rel="noreferrer">2510.15510</a></span>
        <span>作者: Taekyung Kim Team</span>
        <span>日期: 2025-10-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习领域已广泛采用冻结的预训练视觉编码器（如CLIP、VC-1）来提取视觉表示，以替代在有限控制数据上从头训练编码器。然而，这种方法是任务无关的，即在策略学习过程中视觉表示保持冻结，无法适应下游控制任务的具体需求。这导致其性能在不同任务间波动显著，且难以预先确定哪种表示最适合特定任务。微调视觉编码器虽是一种直接解决方案，但容易在模仿学习中过拟合特定场景，导致泛化能力下降。</p>
<p>本文针对“如何使预训练视觉表示在机器人控制中实现任务自适应”这一痛点，提出探索利用预训练文本到图像扩散模型（如Stable Diffusion）的条件机制。这类模型在其他视觉感知任务（如语义分割）中已证明，通过精心设计的文本提示作为条件，可以显著提升下游性能，而无需微调模型本身。本文的核心思路是：探究并设计适用于机器人控制任务的“条件”，以引导扩散模型生成任务自适应的视觉表示，从而超越传统任务无关的冻结表示方法。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架名为ORCA，旨在通过学习任务提示和视觉提示来为扩散模型提供条件，从而在机器人控制中实现任务自适应的视觉表示。</p>
<p><img src="https://arxiv.org/html/2510.15510v1/x4.png" alt="方法框架"></p>
<blockquote>
<p><strong>图4</strong>：ORCA框架整体流程。输入观测图像首先通过VQGAN编码器转换为潜变量，并添加噪声。同时，观测图像通过一个预训练的视觉编码器（DINOv2）提取密集特征，并经过一个小的卷积层投影为视觉提示。可学习的任务提示与视觉提示共同输入扩散模型的文本编码器，生成条件。该条件与噪声潜变量、时间步一起输入扩散U-Net，从其早期和下采样块中提取的特征作为视觉表示，最终输入策略网络以预测动作。所有可学习参数（任务提示、视觉提示、策略网络）通过行为克隆损失进行端到端优化。</p>
</blockquote>
<p>整体流程的输入是单帧观测图像，输出是预测的动作。核心模块包括两个可学习的提示：</p>
<ol>
<li><strong>任务提示</strong>：为一组可学习的参数，在训练过程中所有观测图像共享。其作用是隐式地学习捕捉对解决下游任务至关重要的物体或区域，替代可能产生错误接地的文本描述。如图3所示，学习到的任务提示的交叉注意力图能同时高亮任务相关区域（如按钮和机械臂）。</li>
<li><strong>视觉提示</strong>：为了将每帧图像的动态、细粒度视觉信息融入条件，ORCA使用一个预训练的视觉编码器（DINOv2）提取输入图像的密集特征表示，然后通过一个小的卷积层进行投影。这些投影后的特征作为视觉提示，为条件提供帧级别的细粒度细节。如图3和图6所示，视觉提示的注意力图能高亮图像中更精细的区域（如机器人的前腿和后腿）。</li>
</ol>
<p>与现有方法相比，ORCA的创新点具体体现在：</p>
<ul>
<li><strong>针对控制任务设计条件</strong>：不同于在通用图像领域（如分类、分割）直接使用文本提示或优化文本token的方法，ORCA明确指出文本条件在模拟控制环境中因领域差距而经常失效（见图3），因此提出了专门适配控制环境的可学习任务提示。</li>
<li><strong>融合动态视觉信息</strong>：认识到控制任务基于动态视频流且需要细粒度交互，ORCA创新性地将密集视觉特征作为提示的一部分，实现了对每帧独特视觉状态的动态适应，而无需生成逐帧的文本描述。</li>
<li><strong>端到端高效学习</strong>：任务提示和视觉提示的参数与策略网络一同，仅通过标准的行为克隆损失进行端到端优化，无需额外的预优化或训练阶段，简洁高效。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在三个广泛使用的视觉机器人学习基准上进行了评估，共包含12个任务。</p>
<ol>
<li><strong>DeepMind Control</strong>：5个连续控制任务（Walker-stand, Walker-walk, Reacher-easy, Cheetah-run, Finger-spin），报告标准化得分。</li>
<li><strong>MetaWorld</strong>：5个机械臂操作任务（Assembly, Bin-picking, Button-press-topdown, Drawer-open, Hammer），报告成功率。</li>
<li><strong>Adroit</strong>：2个灵巧手操作任务（Pen, Relocate），报告成功率。<br>实验采用模仿学习设定，每个任务使用少量演示（2-5条）。策略学习使用行为克隆目标。</li>
</ol>
<p><strong>对比方法</strong>：</p>
<ul>
<li><strong>任务无关基线</strong>：CLIP、VC-1、SCR（使用冻结的Stable Diffusion特征）。</li>
<li><strong>任务自适应基线</strong>：<ul>
<li><code>Text (Simple)</code>：使用简单手工文本提示。</li>
<li><code>Text (Caption)</code>：使用Gemini 2.5视觉语言模型生成的描述作为提示。</li>
<li><code>CoOp</code>：在文本提示位置学习连续可优化上下文向量。</li>
<li><code>TADP</code>：结合学习到的风格修饰符和VLMs生成的文本描述。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2510.15510v1/x5.png" alt="任务示例"></p>
<blockquote>
<p><strong>图5</strong>：评估任务可视化示例，涵盖了DeepMind Control、MetaWorld和Adroit三个基准中的不同任务。</p>
</blockquote>
<p>表1、表2、表3分别展示了在DMC、MetaWorld和Adroit上的定量结果。ORCA在所有三个基准的12个任务上均取得了最佳平均性能。</p>
<ul>
<li>在<strong>DMC</strong>（表1）上，ORCA平均得分74.3，显著优于最佳基线SCR（68.3）和TADP（70.7）。在Cheetah-run任务上提升尤为明显（50.0 vs. SCR的43.4）。</li>
<li>在<strong>MetaWorld</strong>（表2）上，ORCA平均成功率95.2%，优于所有基线，在Button-press任务上达到88.0%，显著高于TADP的80.0%。</li>
<li>在<strong>Adroit</strong>（表3）上，ORCA平均成功率65.3%，远超其他方法，特别是在Relocate任务上达到44.0%，而其他基于扩散模型的方法均在34.7%左右。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.15510v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融研究结果。比较了ORCA完整模型、仅使用任务提示、仅使用视觉提示以及两者都使用但冻结（不学习）的情况。结果表明，任务提示和视觉提示都是有效的，且同时学习两者（完整ORCA）能获得最佳性能。冻结提示会导致性能下降，证实了学习的重要性。</p>
</blockquote>
<p><strong>消融实验分析</strong>（图7）：</p>
<ul>
<li><strong>任务提示与视觉提示的贡献</strong>：仅使用任务提示或仅使用视觉提示均能带来性能提升，但二者结合（完整ORCA）效果最佳，说明它们捕获了互补的信息。</li>
<li><strong>提示学习的必要性</strong>：当任务提示和视觉提示被随机初始化并冻结时，性能显著下降，甚至低于无条件的基线（“w/o cond”），这证实了通过下游行为克隆损失来学习这些提示对于实现有效条件是至关重要的。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.15510v1/x6.png" alt="注意力可视化"></p>
<blockquote>
<p><strong>图6</strong>：在Adroit的Relocate任务中，跨帧可视化了任务提示和两个视觉提示的交叉注意力图。注意力图随帧动态变化，并聚焦于与任务相关的不同物体部分（如手掌、目标物体、桌子），验证了条件捕捉动态和细粒度信息的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.15510v1/x8.png" alt="性能分析"></p>
<blockquote>
<p><strong>图8</strong>：不同条件方法在DMC任务上的性能分析。ORCA在大多数任务上表现最佳且最稳定。文本条件（Caption）在部分任务（Cheetah-run）上表现不佳，甚至低于无条件（Null）情况，印证了文本条件在控制环境中的不可靠性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>问题发现与论证</strong>：首次系统地探究了在机器人控制中为扩散模型设计条件的必要性，并通过实验揭示了直接沿用其他视觉任务中成功的文本条件策略在控制领域会因领域差距而失效甚至产生负面效果。</li>
<li><strong>方法创新</strong>：提出了ORCA框架，通过引入可学习的任务提示和基于密集视觉特征的视觉提示，为扩散模型提供了能够适应控制环境、捕捉动态细粒度信息的有效条件，且无需微调扩散模型本身。</li>
<li><strong>性能突破</strong>：在三个主流机器人控制基准上实现了最先进的性能，显著超越了现有的任务无关和任务自适应基线方法，验证了所提条件的有效性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，实验主要在模拟环境中进行。尽管模拟环境是标准测试平台，但将方法迁移到具有更大视觉领域差距的真实世界机器人场景中，可能需要进一步调整或额外的适应技术。</p>
<p><strong>后续研究启示</strong>：</p>
<ul>
<li>本工作强调了为特定领域（如机器人控制）定制生成模型条件机制的重要性，而非直接套用其他领域的方案。</li>
<li>所提出的“可学习提示+视觉特征”的条件范式，为在其他序列决策或动态视觉任务中利用大型预训练生成模型提供了新思路。</li>
<li>如何将这种方法高效地扩展到更复杂的多模态指令跟随或长视野任务，是未来一个有趣的研究方向。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文探索如何利用预训练文本到图像扩散模型为机器人控制获取任务自适应的视觉表示，而无需微调模型本身。核心问题是发现直接使用文本条件（在其他视觉任务中有效）在控制任务中收效甚微甚至有害，归因于训练数据与控制环境间的领域差距。为此，作者提出ORCA方法，引入**可学习的任务提示**以适应控制环境，并设计**视觉提示**以捕捉细粒度、帧特定的动态信息。该方法通过促进任务自适应表示，在多个机器人控制基准测试中取得了最先进的性能，显著超越了先前方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.15510" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>