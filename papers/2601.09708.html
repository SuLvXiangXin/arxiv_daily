<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.09708" target="_blank" rel="noreferrer">2601.09708</a></span>
        <span>作者: Huang, Chi-Pin, Man, Yunze, Yu, Zhiding, Chen, Min-Hung, Kautz, Jan, Wang, Yu-Chiang Frank, Yang, Fu-En</span>
        <span>日期: 2026/01/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过在大规模机器人演示数据上进行模仿学习，在基础技能上表现出色，但在训练分布之外的泛化能力（如长视野规划、失败自校正和新场景适应）上存在局限。为解决此问题，推理型VLA方法通过引入显式的思维链（CoT）等中间推理过程来提升泛化能力。然而，这些方法需要生成冗长的推理轨迹（例如约250个token），导致每次决策产生数秒的高推理延迟，这严重阻碍了在需要高频率实时决策（如1-15 Hz）的具身AI应用中的部署。近期虽有工作尝试通过推理丢弃等技术加速，但直接减少文本推理长度可能导致关键信息丢失和性能下降。</p>
<p>本文针对推理型VLA模型在效率（高延迟）与性能（强泛化）之间的关键瓶颈，提出了一个新视角：将显式的、冗长的文本推理压缩为紧凑的、可言语化的潜在表示进行内部推理。核心思路是通过偏好引导的蒸馏和操作轨迹对齐，将教师模型的优质语言与视觉规划能力压缩到学生模型的连续潜在空间中，从而实现既高效又具备强表达能力的规划。</p>
<h2 id="方法详解">方法详解</h2>
<p>Fast-ThinkAct的整体框架分为两大阶段：1）高效具身推理：训练一个学生VLM，将教师VLM的显式长文本CoT蒸馏为紧凑的潜在表示；2）推理增强的策略学习：将训练好的学生VLM的潜在规划表示与一个动作模型连接，指导其生成具体的机器人动作。</p>
<p><img src="https://arxiv.org/html/2601.09708v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Fast-ThinkAct整体框架。(a) 高效具身推理阶段：给定观测 <code>o_t</code> 和指令 <code>l</code>，文本教师VLM <code>ℱ_θ^T</code> 生成显式推理链。潜在学生VLM <code>ℱ_θ</code> 在奖励偏好的指导下将其蒸馏为紧凑的潜在token <code>z</code>。言语化器LLM <code>𝒱_ψ</code> 将潜在解码为文本以进行基于偏好的学习 (<code>ℒ_verb</code>)，同时 <code>ℒ_distill</code> 从教师处迁移视觉规划能力，空间token通过 <code>ℒ_ans</code> 实现并行的视觉轨迹预测。(b) 推理增强的策略学习阶段：动作模型 <code>π_ϕ</code> 在冻结学生VLM和状态编码器的条件下，使用模仿学习损失 <code>ℒ_IL</code> 进行训练。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li>**文本教师模型 (<code>ℱ_θ^T</code>)**：首先通过GRPO（Group Relative Policy Optimization）训练，利用动作对齐的视觉奖励（如目标完成度、轨迹对齐度）最大化目标函数 <code>𝒥_GRPO</code>，从而生成质量各异的显式文本推理链。其优势函数 <code>A(τ)</code> 作为推理链 <code>τ</code> 的质量指标。</li>
<li>**潜在学生模型 (<code>ℱ_θ</code>)**：目标是学习生成紧凑的连续潜在向量序列 <code>z = {z_m}</code>（例如M=6个）以及K个可学习的空间token <code>{s_i}</code>，以并行预测K个路径点。其训练包含三个损失：<ul>
<li>**可言语化潜在CoT (<code>ℒ_verb</code>)**：引入一个言语化器LLM <code>𝒱_ψ</code>，将学生生成的潜在 <code>z</code> 解码为文本。采用基于DPO的偏好学习目标，鼓励言语化器将 <code>z</code> 解码为高优势（高质量）的教师推理 <code>τ+</code> 的概率，高于解码为低优势推理 <code>τ-</code> 的概率。这驱使学生VLM编码能对应高质量推理的潜在。</li>
<li>**动作对齐的视觉规划蒸馏 (<code>ℒ_distill</code>)**：为确保潜在编码视觉规划能力，最小化教师与学生模型在编码视觉规划的<code>&lt;answer&gt;</code> token隐藏状态 <code>h_t^T</code> 和 <code>h_t</code> 之间的L2距离，以此对齐轨迹级表示。</li>
<li>**答案预测 (<code>ℒ_ans</code>)**：学生模型并行为每个空间token <code>s_i</code> 预测一个2D路径点 <code>p_i</code>，通过MLP投影其输出隐藏状态得到，并与真实路径点计算L2损失。<br>学生模型的总损失为：<code>ℒ_student = ℒ_verb + ℒ_distill + ℒ_ans</code>。</li>
</ul>
</li>
<li><strong>推理增强的策略学习</strong>：学生VLM <code>ℱ_θ</code> 训练完成后，将其冻结。从空间token的Key-Value缓存中提取视觉潜在规划 <code>c_t</code>，将其与动作模型（如基于扩散Transformer的RDT）的状态编码器KV对拼接。动作模型 <code>π_ϕ</code> 的交叉注意力层同时关注视觉规划上下文和状态观测。随后，在机器人动作数据上使用模仿学习损失 <code>ℒ_IL</code> 仅对动作模型进行微调，从而将高层视觉规划转化为低层机器人动作。</li>
</ol>
<p><strong>创新点</strong>：与现有生成显式文本或视觉推理轨迹的方法不同，Fast-ThinkAct的核心创新在于通过<strong>偏好引导的蒸馏</strong>，将语言和视觉规划能力共同压缩到<strong>紧凑的连续潜在空间</strong>中。这避免了冗长的自回归文本生成，并通过空间token实现了视觉轨迹的<strong>并行预测</strong>，从而在保留丰富规划信息的同时大幅提升推理效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>模型</strong>：以Qwen2.5-VL 3B为VLM骨干，动作模型采用DiT-Policy或RDT。</li>
<li><strong>训练数据</strong>：推理训练使用单/双臂视觉轨迹及多个具身QA数据集；策略学习使用OXE数据集等机器人动作数据。</li>
<li><strong>评估基准</strong>：<ul>
<li><strong>机器人操作</strong>：LIBERO（Spatial, Object, Goal, Long子任务）、SimplerEnv-Google、RoboTwin2.0（双手机器人操作，分Easy/Hard设置）。</li>
<li><strong>具身推理</strong>：EgoPlan-Bench2（准确率）、RoboVQA（BLEU分数）、OpenEQA（LLM评分）。</li>
</ul>
</li>
</ul>
<p><strong>对比方法</strong>：与基础VLA（OpenVLA, <code>π_0</code>, RDT）、监督推理VLA（CoT-VLA, MolmoAct）以及RL微调推理VLA（ThinkAct）进行对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>机器人操作性能与延迟</strong>：<br><img src="https://arxiv.org/html/2601.09708v1/x3.png" alt="结果对比"></p>
<blockquote>
<p><strong>图3</strong>：(a)-(e) 在LIBERO和SimplerEnv基准上的成功率对比。Fast-ThinkAct在所有任务上均取得最高成功率。(f) 推理延迟对比。Fast-ThinkAct-3B比ThinkAct-7B延迟降低89.3%，比ThinkAct-3B快7倍。</p>
</blockquote>
<ul>
<li>在LIBERO和SimplerEnv-Google上，Fast-ThinkAct成功率全面超越所有基线（图3 a-e）。</li>
<li>在复杂的双手机器人基准RoboTwin2.0上（表1），Fast-ThinkAct在Easy和Hard设置下的平均成功率分别达到65.7%和26.4%，优于所有对比方法，例如比RDT分别高出9.3%和3.6%。</li>
<li>推理延迟方面（图3f），Fast-ThinkAct-3B相比ThinkAct-7B降低89.3%，相比ThinkAct-3B快7倍，实现了数量级的速度提升。</li>
</ul>
</li>
<li><p><strong>具身推理能力</strong>：</p>
<ul>
<li>在EgoPlan-Bench2、RoboVQA和OpenEQA三个推理基准上（表2），Fast-ThinkAct-3B均取得最佳成绩，例如在RoboVQA上BLEU-4分数达到53.0，比第二名ThinkAct-3B（49.6）高出5.5 BLEU分数，展示了强大的场景理解和多步推理能力。</li>
</ul>
</li>
<li><p><strong>长视野规划与失败恢复</strong>：<br><img src="https://arxiv.org/html/2601.09708v1/x4.png" alt="轨迹可视化"></p>
<blockquote>
<p><strong>图4</strong>：在长视野任务上预测的视觉轨迹（黄色/红色轨迹）和执行结果的定性可视化。展示了模型能够生成合理的多步空间规划。</p>
</blockquote>
<ul>
<li>可视化结果（图4）显示模型能预测合理的多步视觉轨迹，支撑长视野任务完成。<br><img src="https://arxiv.org/html/2601.09708v1/x5.png" alt="失败恢复"><blockquote>
<p><strong>图5</strong>：在RoboFAC基准上的失败恢复能力。左：对操作错误提供纠正指导的定性示例。右：在仿真和真实机器人设置上的定量评估，显示Fast-ThinkAct能有效指导纠正动作。</p>
</blockquote>
</li>
<li>在RoboFAC基准上（图5），Fast-ThinkAct能有效提供纠正指导以从失败中恢复，在仿真和真实机器人设置上均表现优异。</li>
</ul>
</li>
<li><p><strong>小样本适应</strong>：<br><img src="https://arxiv.org/html/2601.09708v1/x6.png" alt="小样本适应"></p>
<blockquote>
<p><strong>图6</strong>：在RoboTwin2.0上使用每任务10个演示进行小样本适应的结果。Fast-ThinkAct在少量数据下能快速适应并提升性能。</p>
</blockquote>
<ul>
<li>小样本适应实验（图6）表明，仅用每个任务10条演示数据进行微调，Fast-ThinkAct能快速提升在目标环境中的性能。</li>
</ul>
</li>
</ol>
<p><strong>消融实验</strong>：论文通过消融研究验证了各损失组件的重要性。移除 <code>ℒ_verb</code> 会导致推理能力下降（在EgoPlan-Bench2上准确率下降）；移除 <code>ℒ_distill</code> 会损害视觉规划能力（在RoboVQA上BLEU分数下降）；移除 <code>ℒ_ans</code> 则直接影响轨迹预测精度和动作执行成功率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个高效的推理框架Fast-ThinkAct，通过将推理压缩到<strong>可言语化的潜在思维</strong>中，在保持强规划表达能力的同时，实现了极低的推理延迟。</li>
<li>设计了<strong>偏好引导的蒸馏与操作轨迹对齐</strong>方法，成功将教师模型的优质语言推理和视觉规划能力迁移到学生的紧凑潜在表示中。</li>
<li>通过<strong>推理增强的策略学习</strong>，有效地将高层的潜在视觉规划与低层的动作执行桥接起来，在多个具身操作和推理基准上实现了性能与效率的同步提升。</li>
</ol>
<p><strong>局限性</strong>：论文未明确阐述自身方法的局限性，但潜在挑战可能包括：1）训练过程相对复杂，涉及多阶段（SFT、CoT-SFT、教师GRPO训练、学生蒸馏、策略微调）；2）方法依赖于一个预先训练好的、能够生成高质量CoT的教师模型；3）潜在表示的“黑盒”特性可能在一定程度上降低了推理过程的透明度和可解释性。</p>
<p><strong>研究启示</strong>：</p>
<ol>
<li><strong>效率与性能的权衡</strong>：为实时具身AI系统提供了一个新颖的设计范式，证明通过精心设计的蒸馏和潜在表示学习，可以大幅提升效率而不牺牲（甚至提升）性能。</li>
<li><strong>多模态潜在规划</strong>：展示了将语言和视觉信息共同编码到紧凑潜在空间进行联合规划的可行性，这为开发更高效的多模态推理模型指明了方向。</li>
<li><strong>从模仿到推理增强</strong>：展示了如何将纯粹的模仿学习策略升级为具备内部推理能力的策略，通过注入规划先验来增强模型对长视野、新场景和失败情况的处理能力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Fast-ThinkAct，旨在解决视觉-语言-动作模型中因显式思维链推理轨迹冗长导致的高推理延迟问题。其核心技术为可言语化的潜在规划，通过从教师模型蒸馏学习紧凑的潜在思维链，并采用偏好引导目标对齐操作轨迹，从而将语言与视觉规划能力迁移到具身控制中。实验表明，该方法在多种具身操作与推理基准上实现了强性能，推理延迟比当前最优的推理VLA降低高达89.3%，并在长时程规划、少样本适应和失败恢复方面保持有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.09708" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>