<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.19647" target="_blank" rel="noreferrer">2511.19647</a></span>
        <span>作者: Dorsa Sadigh Team</span>
        <span>日期: 2025-11-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基础模型（如视觉语言模型VLM）在视觉和语言任务上展现出强大的零样本能力。然而，这些模型严重依赖从互联网获取的、经过清洗和整理的预训练数据，这些数据往往偏向于某些语言和领域，导致其在非结构化、真实的“野外”环境中表现脆弱。现实部署中遇到的混乱数据（如低分辨率图像、被遮挡的标识、多语言文本）在现有数据集中代表性严重不足。机器人作为具身智能体，具有独特的优势来弥补这一差距：它们可以在物理环境中行动，大规模收集真实世界的数据，为基础模型训练补充其当前最缺乏的示例。</p>
<p>本文的核心痛点是：互联网规模的数据集无法充分代表机器人经常部署的、特定领域或小众的真实世界场景。本文提出将机器人从基础模型的“消费者”转变为“数据生成者”，构建一个“机器人驱动的数据飞轮”。核心思路是：通过部署搭载基础模型的机器人执行有用任务，同时收集具有领域代表性的真实数据，利用这些数据持续微调基础模型，从而形成一个良性循环——部署改进模型，改进的模型又使部署更成功。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了“机器人驱动的数据飞轮”框架，其目标是通过在野外部署机器人来执行任务并收集真实世界数据，从而持续改进基础模型。该框架的核心是一个迭代的数据收集与模型适应闭环。</p>
<p><strong>整体框架</strong>：该框架定义了一系列离散的迭代周期 <code>t = 1, 2, ...</code>。在每次迭代 <code>t</code> 中，机器人使用前一次迭代更新后的模型 <code>FM_{t-1}</code> 进行部署，在最大时间范围 <code>T</code> 内收集原始数据集 <code>D_t^raw</code>。随后，通过一个整理函数 <code>Curate: D_t^raw ↦ D_t</code> 对原始数据进行清洗和标注，生成高质量的数据子集 <code>D_t</code>。所有迭代中整理后的数据被累积成数据集 <code>𝒟_t = ⋃_{k=1}^{t} D_k</code>。然后，使用累积数据集 <code>𝒟_t</code> 对初始预训练模型 <code>FM_0</code> 进行微调，生成更新后的模型 <code>FM_t</code>，用于下一次机器人部署。这样就形成了一个完整的闭环：<code>D_t^raw → Curate → D_t → Aggregate → 𝒟_t → Fine-tune → FM_t → Robot → D_{t+1}^raw</code>。</p>
<p><img src="https://arxiv.org/html/2511.19647v1/figure/library_session_shelves.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Scanford系统与数据飞轮框架示意图。左侧展示了部署在图书馆的Scanford机器人系统（移动底座+机械臂+传感器）。右侧阐释了机器人驱动数据飞轮的核心闭环：机器人使用基础模型（VLM）执行书架扫描任务，同时收集原始数据；数据经过自动整理后，用于微调基础模型；改进后的模型使下一次机器人部署更有效，从而收集更多、更好的数据。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>机器人任务系统（Scanford实例）</strong>：该系统是数据飞轮框架的一个具体实现，用于图书馆书架库存管理。</p>
<ul>
<li><strong>硬件平台</strong>：采用TidyBot++移动底座搭载Franka FR3机械臂的组合，以满足在书架过道中水平移动和垂直扫描的需求。传感器包括腕部安装的Intel RealSense D435 RGB-D相机（用于拍摄书架图像）和底座安装的Unitree L2 LiDAR（用于导航和定位）。</li>
<li><strong>机器人控制</strong>：机器人预编程了对应于书架各层的高度。它在过道中分段前进，机械臂在每个停靠点遍历所有高度拍摄图像。由于在狭窄、视觉同质的书架过道中，里程计和传统SLAM容易失效，系统利用LiDAR点云进行启发式漂移校正：通过拟合过道两侧书架点云形成的平面，估计边界并校正机器人位姿，确保扫描覆盖的连续性。</li>
<li><strong>数据标注</strong>：扫描后，系统将图像与对应的图书馆区域（对应美国国会图书馆分类法的索书号范围）关联。然后，使用当前VLM <code>FM_{t-1}</code> 预测图像中书籍的标题和索书号。为了提高预测准确性，采用了检索增强生成（RAG）方法，将图书馆数据库中该区域的候选书籍列表作为上下文输入VLM的提示词中，约束其输出在有效范围内。</li>
</ul>
</li>
<li><p><strong>数据自动整理模块</strong>：这是实现自动化飞轮的关键。目标是验证VLM的原始预测标签 <code>L̂_t</code> 的可靠性，生成高质量的标注数据 <code>D_t</code>。</p>
<ul>
<li><strong>整理方法</strong>：利用图书馆数据库中有序的候选书籍列表，对每个预测的书籍标签序列进行验证：(1) 使用字符串相似度匹配（如Ratcliff/Obershelp模式匹配算法）检查预测的每个书籍是否在候选集中；(2) 进行局部顺序检查，确保预测中相邻书籍的顺序与数据库中的顺序一致。</li>
<li><strong>筛选标准</strong>：相似度得分超过阈值的图像-标签对被接受进入训练数据集 <code>D_t</code>，否则被丢弃。这个过程实现了无需人工标注的、可扩展的高质量数据标注。</li>
</ul>
</li>
<li><p><strong>模型适配模块</strong>：使用累积的整理后数据集 <code>𝒟_t</code> 微调基础模型。</p>
<ul>
<li><strong>模型选择</strong>：实例中选择Qwen2.5-VL（7B）作为基础VLM，因其开源且具备多语言（中英文）能力。</li>
<li><strong>微调细节</strong>：使用AdamW优化器，学习率2e-7，有效批次大小16，混合精度（bfloat16）训练，权重衰减0.01，并采用带3%预热比例的余弦学习率调度器。在每个迭代 <code>t</code>，都在累积数据集 <code>𝒟_t</code> 上对初始的 <code>FM_0</code> 进行微调（而非在上一个模型 <code>FM_{t-1}</code> 上继续训练），生成 <code>FM_t</code>。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有仅利用机器人数据改进特定任务性能的工作不同，本文框架的创新性在于强调机器人收集的“野外”数据能够填补互联网预训练数据中<strong>领域代表性不足的空白</strong>。因此，微调不仅能提升<strong>领域特定</strong>任务（如图书识别）的性能，还能增强模型在<strong>领域相邻</strong>任务（如通用多语言OCR）上的<strong>泛化能力</strong>，实现了从“任务适配”到“能力增强”的跨越。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>部署环境与数据</strong>：在斯坦福大学东亚图书馆进行了为期两周（10天，每天4小时）的实地部署。书架上的书籍主要为中文、日文和韩文，存在损坏、褪色、遮挡等真实挑战。共扫描了2103个书架。</li>
<li><strong>基准与基线</strong>：<ul>
<li><strong>领域特定任务</strong>：图书馆书籍识别。在71张人工标注的测试图像上评估。</li>
<li><strong>领域相邻任务</strong>：多语言OCR。使用两个公开挑战性数据集：一个英文OCR数据集（Baek et al., 2019）的“困难案例”子集（644张图像），和一个中文OCR数据集（Chen et al., 2021）的“困难案例”子集（500张图像）。</li>
<li><strong>对比模型</strong>：预训练的Qwen2.5-VL (7B) 和 Gemini 模型作为基线，与使用Scanford收集数据微调后的Qwen2.5-VL进行对比。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2511.19647v1/figure/adaptation_results.png" alt="适应结果"></p>
<blockquote>
<p><strong>图2</strong>：使用飞轮数据微调VLM的适应结果。<strong>左图</strong>显示，在领域特定的图书馆书籍识别任务上，微调使Qwen2.5的准确率从32.4%大幅提升至71.8%（绝对提升39.4%）。<strong>右图</strong>显示，在领域相邻的多语言OCR任务上，同一份数据也使Qwen2.5在英文困难案例上的准确率从24.8%提升至46.6%，在中文困难案例上从30.8%提升至38.0%。图中还显示，大部分性能增益在部署约1.5小时（收集约1352张图像）后即达到平台期。</p>
</blockquote>
<ul>
<li><strong>领域特定性能提升</strong>：微调后的Qwen2.5-VL在图书馆书籍识别任务上的准确率达到71.8%，相比其预训练基线（32.4%）绝对提升了39.4%。Gemini预训练模型准确率为43.7%。</li>
<li><strong>领域相邻泛化提升</strong>：微调显著提升了模型在具有挑战性的多语言OCR任务上的表现。对于英文OCR困难案例，微调后Qwen2.5准确率从24.8%提升至46.6%；对于中文OCR困难案例，从30.8%提升至38.0%。作为对比，预训练的Gemini在中文困难案例上准确率仅为3.4%，作者推测其预训练数据中中文占比可能较低。</li>
<li><strong>数据效率</strong>：性能提升在部署初期（约1.5小时，收集1352张图像）最为显著，之后趋于平缓，表明即使短时间部署也能收集到对模型改进极具价值的数据。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.19647v1/figure/scanford_deployment_results.png" alt="部署结果"></p>
<blockquote>
<p><strong>图3</strong>：Scanford部署结果汇总。<strong>左图</strong>：Scanford扫描2103个书架，据图书馆员估计，节省了18.7小时的人工劳动时间。<strong>中图</strong>：随着部署天数（数据量）增加，VLM在书籍识别任务上的性能持续提升。<strong>右图</strong>：在为期10天、总计40小时的部署中，仅需要26次人工干预（平均每天2.6次），每次干预时间小于5分钟，主要用于纠正机器人漂移。</p>
</blockquote>
<ul>
<li><strong>任务部署成功与人力节省</strong>：Scanford成功完成了库存扫描任务。图书馆员估计，手动完成同等工作量需要18.7小时，Scanford实现了人力节省。在整个部署过程中，仅需26次人工干预（平均每次&lt;5分钟），自动化程度高。</li>
<li><strong>消融实验</strong>：虽然没有严格的组件消融，但实验结果表明了数据飞轮闭环的有效性。<strong>数据自动整理</strong>模块确保了训练数据的质量（从8232张原始图像中筛选出5019张高质量图像用于微调）。<strong>在初始模型 <code>FM_0</code> 上持续微调</strong>的策略（而非序列化微调）被证明是有效的。核心贡献——<strong>利用机器人收集的领域特定数据提升领域相邻泛化能力</strong>——通过跨任务性能提升得到了直接验证。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了“机器人驱动的数据飞轮”框架</strong>：这是一个将机器人部署、真实世界数据收集与基础模型持续适应相结合的通用范式，将机器人定位为填补互联网数据空白的关键数据生成者。</li>
<li><strong>实现了框架实例化并验证其双重收益</strong>：通过Scanford系统在复杂真实环境（多语言图书馆）中的长期部署，实证表明该框架既能<strong>有效完成实际任务、节省人力</strong>，又能<strong>显著提升基础模型在领域特定及领域相邻任务上的性能</strong>。</li>
</ol>
<p><strong>局限性</strong>：论文提及的局限性包括：1) 当前系统需要针对特定环境（如图书馆过道几何）进行设置和调整；2) 自动化数据整理依赖于领域特定的结构化知识（如图书馆目录），在其他无类似结构化数据源的场景中推广可能面临挑战。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>开拓机器人新角色</strong>：本研究为机器人学提供了一个新视角，即机器人不仅是任务的执行者，更是宝贵、稀缺的真实世界数据的主动收集者，可以用于改进上游AI模型。</li>
<li><strong>推动基础模型进化途径</strong>：指出了一条不依赖于扩大互联网爬取数据，而是通过具身智能主动探索物理世界来增强模型鲁棒性和泛化能力的新路径。</li>
<li><strong>促进软硬件协同设计</strong>：未来研究可探索如何为数据收集优化机器人硬件、感知和控制策略，以及如何设计更通用的自动化数据整理和模型适应算法，以降低新场景的应用门槛。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对基础模型在真实世界非结构化环境中因训练数据缺乏而表现脆弱的问题，提出“机器人驱动数据飞轮”框架，将机器人从模型消费者转变为数据生成器。关键技术包括部署机器人（如Scanford移动操作器）在真实场景（如图书馆）中自主收集数据，利用视觉语言模型（VLM）识别书籍，并通过目录自动标注图像以微调模型。实验结果表明，基于2103个书架收集的数据，VLM在多语言书籍识别准确率从32.0%提升至71.8%，域相邻多语言OCR任务中英语从24.8%提升至46.6%、中文从30.8%提升至38.0%，同时节省约18.7小时人力，验证了该框架能持续优化模型并减少人工投入。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.19647" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>