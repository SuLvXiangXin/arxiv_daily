<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.08112" target="_blank" rel="noreferrer">2507.08112</a></span>
        <span>作者: Raafat E. Shalaby Team</span>
        <span>日期: 2025-07-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>移动机器人导航主要分为经典方法和端到端方法。经典方法依赖多传感器（如相机、激光雷达）构建环境地图，再通过规划器生成控制指令，流程复杂且工程量大。端到端方法则直接从传感器输入数据映射到机器人的控制命令（如角速度），简化了流程并允许在更高语义层面进行导航，无需精确定位或建图。然而，现有的端到端方法在处理多模态传感器数据（如视觉与深度）时，其融合策略的有效性有待深入探究。</p>
<p>本文针对移动机器人避障任务，提出了一种基于端到端卷积神经网络（CNN）的传感器融合新视角。具体而言，研究旨在比较两种不同的多模态（RGB-D）数据融合策略——简单拼接与自适应门控融合——在预测机器人转向角速度任务上的性能。核心思路是：利用一个紧凑的RGB-D深度相机同时获取颜色和深度图像，设计并训练两种双分支CNN网络，直接学习从原始感知数据到控制命令的映射，以实现高效的障碍物规避。</p>
<h2 id="方法详解">方法详解</h2>
<p>研究的整体流程分为三个阶段：1）<strong>数据收集</strong>：通过远程操控机器人，同步记录RGB-D图像和人类操作员给出的角速度命令，构建配对数据集；2）<strong>模型训练</strong>：基于该数据集，分别训练两种融合架构的CNN模型；3）<strong>评估与部署</strong>：在测试集上评估模型性能，并计划部署到真实机器人进行实时避障测试。</p>
<p>核心模块是两个受文献启发的双分支CNN架构：NetConEmb 和 NetGated。两个网络均以240x240像素的RGB图像和深度图像作为输入，经过各自的特征提取分支（由卷积层、ReLU激活和最大池化层构成），最终输出预测的机器人角速度。两个分支在特征提取阶段结构对称。</p>
<p><img src="https://arxiv.org/html/2507.08112v1/extracted/6611004/netconemb_arch.png" alt="NetConEmb架构"></p>
<blockquote>
<p><strong>图4</strong>：NetConEmb架构图。该网络采用早期融合策略：分别从RGB和深度分支提取的特征向量被展平后直接拼接，然后通过一系列全连接层（FC-2880, FC-288, FC-32）进行融合与回归，最终输出角速度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08112v1/extracted/6611004/netgated_arch.png" alt="NetGated架构"></p>
<blockquote>
<p><strong>图5</strong>：NetGated架构图。该网络引入了门控融合机制。两个分支的特征向量首先被投影到1440维并拼接，然后经过一个FC-64层降维，再由一个FC-2层生成两个标量门控权重。这两个权重分别与各自模态的1440维特征向量相乘，结果相加后，再经过FC-720和FC-32层，最终输出角速度。</p>
</blockquote>
<p>两种架构的主要区别在于融合方式：</p>
<ul>
<li><strong>NetConEmb（简单拼接融合）</strong>：将两个模态的特征向量简单拼接，视为早期融合。其融合逻辑固定，平等对待两个模态。</li>
<li><strong>NetGated（门控融合）</strong>：引入了一个可学习的门控机制（Gated Multiply）。网络通过额外的全连接层自适应的生成两个标量权重，用于在融合前调制RGB和深度特征向量的贡献度。这种机制允许网络根据输入动态决定更依赖哪个模态。</li>
</ul>
<p>创新点具体体现在：1）<strong>传感器选择</strong>：使用Intel RealSense D415这种集成了RGB和深度感知的紧凑型相机，替代了传统的多传感器（如分离的LiDAR和RGB相机）方案，简化了硬件配置和数据同步问题。2）<strong>融合策略对比</strong>：系统地设计并对比了简单拼接与自适应门控这两种特征层融合策略在端到端避障任务中的性能，为多模态融合方法的选择提供了实证依据。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究使用自行收集的数据集进行训练和评估，该数据集包含7566对RGB-D图像（尺寸240x240），采集自不同光照条件和包含静态/动态障碍物的多样化环境。数据按60%/20%/20%划分训练、验证和测试集。实验在配备NVIDIA Jetson Nano的差分驱动机器人平台上进行。</p>
<p><strong>对比方法</strong>：研究的主要对比是在提出的两种网络NetConEmb和NetGated之间进行。</p>
<p><strong>关键实验结果</strong>：<br>在测试集上的性能对比如下表所示（数值越低越好，除方差得分VS越高越好）：<br>NetConEmb在各项误差指标（MAE, RMSE, MedAE）上均低于NetGated，同时拥有更高的方差得分（VS）和更短的推理时间（34.511 ms vs 41.518 ms）。这表明在此特定任务和数据集上，结构更简单、参数更多的NetConEmb反而取得了更优的性能。</p>
<p><img src="https://arxiv.org/html/2507.08112v1/extracted/6611004/losses.png" alt="训练损失曲线"></p>
<blockquote>
<p><strong>图7</strong>：NetConEmb和NetGated在前200个epoch的训练和验证损失曲线。NetConEmb（蓝线）达到了比NetGated（橙线）更低的训练和验证损失。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08112v1/extracted/6611004/scatter_plot_comparison.png" alt="散点图对比"></p>
<blockquote>
<p><strong>图8</strong>：测试集上真实角速度与预测角速度的散点图。理想情况应沿红色虚线分布。NetConEmb（左）的点更集中在对角线附近，表明其预测误差更小；NetGated（右）的点则相对分散。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08112v1/extracted/6611004/residual_plots_comparison.png" alt="残差图对比"></p>
<blockquote>
<p><strong>图9</strong>：测试集上预测残差（真实值-预测值）与真实值的分布图。两个模型的残差大致围绕零线分布，但NetGated的残差在高真实值区域表现出更大的离散度，预测稳定性不如NetConEmb。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>研究进行了模态消融实验，评估了每个网络仅使用RGB或仅使用深度图像时的性能。关键结论如下：</p>
<ol>
<li><strong>双模态优势</strong>：无论是NetConEmb还是NetGated，使用RGB-D双模态输入的性能（参见表III）均显著优于仅使用单一模态的性能（表IV）。</li>
<li><strong>模态重要性</strong>：仅使用RGB图像时，两个模型的性能下降相对较小；而仅使用深度图像时，性能下降非常显著（误差大幅增加，VS大幅降低）。这表明在此数据集中，RGB信息对预测角速度的贡献可能比深度信息更大。</li>
<li><strong>架构鲁棒性</strong>：在单模态测试下，NetConEmb依然在大多数指标上优于NetGated。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.08112v1/extracted/6611004/netgated_weights.png" alt="门控权重演化"></p>
<blockquote>
<p><strong>图6</strong>：NetGated模型中RGB和深度分支的门控标量权重在整个训练过程中的变化。RGB权重始终高于深度权重，证实了网络在学习过程中更依赖RGB特征，这与消融实验结果一致。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了两种用于移动机器人避障的端到端CNN传感器融合网络（NetConEmb和NetGated），并详细比较了简单拼接与自适应门控两种融合策略。</li>
<li>构建并公开了一个新的、在多样化环境中采集的RGB-D视觉导航数据集，同步记录了图像与操控命令。</li>
<li>通过全面的实验（包括性能指标对比、消融研究、残差分析）表明，在此特定任务上，结构相对简单的早期拼接融合网络（NetConEmb）在预测精度和推理速度上均优于引入门控机制的NetGated网络。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，当前工作主要是在收集的数据集上进行训练和离线测试。虽然计划进行真实机器人部署，但截至论文成文时，尚未提供在真实物理机器人上进行实时避障测试的闭环性能结果。此外，数据集中角速度命令集中在零值附近，对于较大转向角速度的样本覆盖不足，这可能影响了模型在极端情况下的预测能力。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>门控机制设计</strong>：NetGated性能未达预期，提示未来需要设计更精细或更有效的多模态自适应融合机制，例如基于注意力或更复杂的门控结构。</li>
<li><strong>任务与场景拓展</strong>：可以在更复杂、动态性更强的环境，或光照条件极差（深度信息可能更可靠）的场景下测试，以进一步验证不同融合策略的泛化能力。</li>
<li><strong>实时系统集成</strong>：将训练好的模型集成到完整的机器人导航栈中进行端到端实时测试，是验证其实际有效性的关键下一步。</li>
<li><strong>数据均衡</strong>：未来数据收集可考虑更均匀地覆盖整个角速度命令范围，以提升模型在各类转向情况下的鲁棒性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对移动机器人在已知和未知环境中的避障导航问题，提出了一种基于端到端卷积神经网络（CNN）与传感器融合的模仿学习方法。核心方案是设计并训练两个定制CNN模型，融合深度相机采集的彩色与深度图像，直接输出机器人转向所需的角速度命令。研究收集了包含不同光照与动态障碍的新视觉数据集，并利用ROS系统同步记录图像与转向数据。通过均方误差、方差得分及前馈时间等多项指标对比评估了两个网络的性能，明确了更适用于实际应用的网络模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.08112" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>