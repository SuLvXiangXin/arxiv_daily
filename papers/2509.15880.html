<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.15880" target="_blank" rel="noreferrer">2509.15880</a></span>
        <span>作者: Ian Reid Team</span>
        <span>日期: 2025-09-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的基于RGB的模仿学习方法（如ACT、DP）通常采用ResNet或ViT等传统2D视觉编码器，这些编码器缺乏对全局几何结构的显式建模能力，难以理解物体间的三维空间关系。虽然基于点云等3D几何表征的方法能直接编码空间结构，但它们通常依赖外部处理或基础模型，且难以像RGB观测那样在真实机器人系统中便捷获取。近年来，以VGGT为代表的几何基础视觉模型展现了强大的3D推理能力，但这类模型通常计算开销巨大，推理速度慢，限制了其在实时机器人系统中的部署。</p>
<p>本文针对“如何将强大的几何感知能力高效地集成到机器人模仿学习中”这一具体痛点，提出了一个新视角：通过知识蒸馏技术，将大型几何模型VGGT压缩成一个轻量高效的编码器eVGGT，并直接替换现有模仿学习策略中的传统视觉编码器。本文的核心思路可概括为：通过知识蒸馏构建一个高效且保持几何感知能力的视觉编码器eVGGT，并将其作为通用视觉前端嵌入模仿学习框架，从而以较低的计算成本显著提升机器人操作策略对三维场景的理解和成功率。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程分为两步：首先，通过知识蒸馏从大型几何模型VGGT得到一个轻量高效的编码器eVGGT；其次，将训练好的eVGGT作为冻结的视觉前端，集成到现有的模仿学习策略（如ACT、DP）中，替换其原有的视觉编码器。</p>
<p>核心模块包括：</p>
<ol>
<li><strong>教师模型VGGT</strong>：它是一个基于Transformer的视觉编码器，以多视角RGB图像为输入，通过一个微调过的DINOv2-ViT-L骨干网络进行分块标记化，并添加相机和注册令牌。特征序列随后经过L=24个交替注意力（全局注意力和帧内自注意力）的Transformer块处理，最终通过不同的预测头输出相机位姿、深度图和点云。</li>
<li><strong>学生模型eVGGT</strong>：为提升效率，对VGGT架构进行了压缩：将Transformer块数从L=24减少到L=4（这是保持与教师模型预测头兼容的最小数量），并将骨干网络从DINOv2-ViT-L替换为更小的DINOv2-ViT-S。这些改动旨在适应机器人场景常见的低分辨率输入。</li>
<li><strong>知识蒸馏训练</strong>：学生模型eVGGT通过模仿教师模型VGGT的输出进行训练。蒸馏损失包括对相机参数、深度图和点云输出的均方误差损失，以及一个额外的深度图梯度损失，以缓解因骨干网络规模差异导致的预测噪声问题。训练时仅对学生模型的输入进行数据增强（色彩抖动、随机灰度化、高斯模糊），以增加输入多样性，防止学生对教师输出过拟合，从而提升泛化能力。</li>
<li><strong>策略集成</strong>：将训练好的eVGGT冻结，移除其预测头，仅提取其Transformer块中的潜在几何特征。通过自适应MLP将这些特征投影到与策略头匹配的维度。该方法被集成到两个代表性模仿学习框架中：ACT（基于Transformer的动作分块预测）和DP（基于扩散模型的策略）。集成方式简单直接：用eVGGT编码的几何感知特征替换原策略中的视觉观测嵌入。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.15880v1/x2.png" alt="VGGT架构"></p>
<blockquote>
<p><strong>图3</strong>：VGGT架构。VGGT是一个基于Transformer的视觉编码器，从多视角输入生成几何感知特征。它使用DINOv2-ViT-L骨干进行标记化，并添加特殊令牌，随后通过交替注意力Transformer块处理，最终由相机头或DPT头输出几何信息。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15880v1/x3.png" alt="集成到ACT"></p>
<blockquote>
<p><strong>图4</strong>：eVGGT集成到ACT。eVGGT为ACT的编码器和解码器提供几何感知的视觉特征。观测首先由eVGGT编码为特征y_t，然后与动作序列或潜在变量拼接，输入到ACT的Transformer中。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15880v1/x4.png" alt="集成到DP"></p>
<blockquote>
<p><strong>图5</strong>：eVGGT集成到DP。eVGGT将观测序列编码为全局上下文嵌入Y_t，为扩散去噪过程提供几何表征指导。去噪网络f_ω以噪声动作、几何嵌入和扩散步数为条件进行预测。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1) 首次提出通过知识蒸馏为机器人任务定制高效的几何感知视觉编码器，在保持VGGT强大3D推理能力的同时，大幅降低了计算开销；2) 提出了一种简单通用的集成方法，即用冻结的几何编码器潜在特征直接替换传统2D编码器的特征，无需对策略架构进行复杂改动或对编码器进行任务特定的微调。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：本文在仿真和真实世界两个环境中进行评估。</p>
<ul>
<li><strong>仿真基准</strong>：使用了<strong>RoboTwin</strong>（双手操作任务）和<strong>ManiSkill</strong>（单臂操作任务）两个模拟器。在RoboTwin上评估了6个代表性任务，在ManiSkill上评估了Pick Cube、Push Cube、Stack Cube三个任务。</li>
<li><strong>实验平台</strong>：仿真实验在NVIDIA RTX 4090 GPU上进行，知识蒸馏训练在4张NVIDIA A100 GPU上进行了6天。</li>
</ul>
<p><strong>对比基线</strong>：</p>
<ul>
<li><strong>模仿学习策略</strong>：ACT、DP，以及更强的基础模型方法RDT。</li>
<li><strong>3D输入方法</strong>：DP3（使用点云输入）。</li>
<li><strong>几何模型</strong>：在编码器评估中，与原始VGGT和另一个几何基础模型DUSt3R进行比较。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>机器人操作性能提升</strong>：在RoboTwin上，集成eVGGT后，ACT+eVGGT和DP+eVGGT分别比原始ACT和DP平均成功率提升了2.1%和6.5%。DP+eVGGT（23.3%）的性能与使用后处理点云的DP3（23.4%）相当，且超过了大规模预训练的RDT（22.1%）。在ManiSkill上，DP+eVGGT取得了最佳性能（84.0%），超过了使用RGBD输入的DP（79.7%）和RDT（82.3%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.15880v1/x5.png" alt="结果表1"></p>
<blockquote>
<p><strong>表I</strong>：RoboTwin模拟器上的成功率。DP+eVGGT（ours）在多个任务上超越原始DP和RDT，平均提升显著。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15880v1/x6.png" alt="结果表2"></p>
<blockquote>
<p><strong>表II</strong>：ManiSkill模拟器上的成功率。DP+eVGGT取得了最高的平均成功率，表明几何感知表征的有效性。</p>
</blockquote>
<ol start="2">
<li><p><strong>视觉编码方法对比</strong>：在RoboTwin的两个任务上，DP+eVGGT的性能优于使用ResNet或ViT作为视觉编码器的DP变体。同时，实验发现直接使用eVGGT生成的点云作为DP3的输入会导致性能大幅下降（-20.5%），这证明了本文采用的“潜在特征集成”方式比直接使用其显式几何输出（如点云）更为有效。</p>
</li>
<li><p><strong>eVGGT效率与重建能力</strong>：eVGGT相比教师模型VGGT，参数量减少了5倍（0.26B vs 1.29B），在4张图像输入下的推理速度快了约9倍（0.069s vs 0.601s），内存占用减少了6.2GB。在3D重建精度上，eVGGT在相机位姿估计、深度估计等指标上均大幅优于DUSt3R，且与VGGT的差距很小。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.15880v1/x7.png" alt="效率对比表"></p>
<blockquote>
<p><strong>表IV</strong>：模型效率对比。eVGGT在参数量、推理时间和内存占用上均显著优于VGGT和DUSt3R。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15880v1/figures/Point-cloud-visualization.png" alt="3D重建可视化"></p>
<blockquote>
<p><strong>图6</strong>：RoboTwin上的定性3D重建结果。eVGGT重建的点云质量与VGGT接近，且明显优于DUSt3R（后者需300次迭代优化）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15880v1/figures/Depth-estimation.png" alt="深度估计可视化"></p>
<blockquote>
<p><strong>图7</strong>：深度估计可视化。在RoboTwin和ManiSkill场景中，eVGGT预测的深度图与真实值更为接近，细节优于DUSt3R。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：论文通过对比不同视觉编码器（ResNet, ViT, eVGGT）以及不同几何信息利用方式（潜在特征 vs. 显式点云），验证了eVGGT作为几何感知编码器的有效性以及“潜在特征集成”策略的优越性。数据增强策略也被证明对提升学生模型的泛化能力至关重要。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li>提出了一个轻量高效的几何感知视觉编码器eVGGT，它通过知识蒸馏从VGGT中获得，在保持强大3D重建能力的同时，实现了近9倍的加速和5倍的模型压缩。</li>
<li>提出了一种简单而有效的方案，将几何感知表征集成到机器人模仿学习策略中，即用冻结的eVGGT潜在特征替换传统2D编码器特征，无需任务特定微调。</li>
<li>在仿真和真实机器人实验上系统验证了所提方法的有效性，表明几何感知编码能显著提升操作成功率，且效率满足实际部署需求。</li>
</ol>
<p>论文自身提到的局限性包括：蒸馏得到的eVGGT性能仍略低于教师模型VGGT；研究主要关注静态场景的几何理解，对动态场景中物体运动的理解可能不足。</p>
<p>本文工作对后续研究的启示在于：1) 证明了将前沿几何视觉模型的高层推理能力高效迁移到机器人领域的可行性，为机器人感知提供了新思路；2) “冻结编码器+轻量适配”的集成范式简单有效，可推广至其他需要复杂感知的决策任务；3) 未来可探索将更广泛的具身智能基础模型（如视频模型、物理推理模型）通过类似蒸馏方式赋能机器人系统。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习中传统2D视觉编码器（如ResNet、ViT）缺乏3D几何理解能力的问题，提出集成几何感知视觉编码器以提升操作性能。核心方法是：1）将几何感知编码器融入ACT、DP等模仿学习框架；2）提出轻量级变体eVGGT，通过知识蒸馏使模型体积缩小5倍、速度提升9倍。实验表明，该方法在仿真和真实世界的单/双手操作任务中，成功率最高提升6.5%，同时保持了高效的几何推理能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.15880" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>