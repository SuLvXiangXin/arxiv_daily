<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.12224" target="_blank" rel="noreferrer">2505.12224</a></span>
        <span>作者: Lu, Weifeng, Ye, Minghao, Ye, Zewei, Tao, Ruihan, Yang, Shuo, Zhao, Bo</span>
        <span>日期: 2025/05/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型在机器人操作领域取得了显著进展，能够将自然语言指令和视觉信息转化为顺序控制动作。然而，这些模型在开放世界场景中往往表现不佳，因为它们主要是在成功的专家演示数据上训练的，缺乏失败恢复的能力。当任务执行失败时，VLA模型难以自主恢复到正确的动作序列。为解决此问题，近期研究探索使用通用多模态大语言模型（MLLMs）作为外部批评者，利用其强大的感知和推理能力来检测失败并提供帮助。但这些通用模型并非针对机器人操作失败数据专门训练，在应用于机器人任务的具体失败分析和纠正时仍存在困难。尽管有研究尝试收集机器人失败数据并微调MLLMs，但现有数据集通常局限于简单任务，缺乏对失败的综合分析，且未提供跨不同执行层级的纠正建议。</p>
<p>本文针对VLA模型失败恢复能力有限这一具体痛点，提出了一个全新的、全面的机器人失败分析与纠正框架。核心思路是：首先构建一个大规模、多样化的机器人失败视频问答数据集（RoboFAC数据集），然后基于此数据集训练一个专用的多模态大模型（RoboFAC模型），使其具备任务理解、失败分析和失败纠正的能力，并最终将其作为外部监督集成到真实的VLA控制流程中，以提供纠正指令，帮助机器人从失败中恢复。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboFAC框架包含两大核心部分：数据集的构建和专用模型的训练与部署。</p>
<p><img src="https://cdn.openai.com/placeholder/1/1" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RoboFAC框架总览。<strong>左半部分</strong>展示了RoboFAC数据集的多样性，涵盖了不同复杂度的任务、真实世界任务、多种背景和相机视角，并为八个不同问题类型提供了详细的视频问答标注。<strong>右半部分</strong>通过视觉示例详细说明了六种失败分类法。底部提供了数据集的定量统计：包含16个任务、53个场景、6种失败分类、10722条轨迹（其中9440条失败，1282条成功）、78K个问答对（70K来自仿真，8K来自真实世界）。</p>
</blockquote>
<p><strong>1. RoboFAC数据集构建</strong><br>数据收集在仿真（ManiSkill环境）和真实世界（SO-100机械臂）中进行，覆盖了16个多样化任务。为了生成失败数据，研究者在选定的子阶段用生成错误轨迹的代码片段替换原始专家策略，从而导致整个机器人任务失败。每个失败视频都配有文本描述，包括失败发生的子阶段、失败类型和详细的错误解释。</p>
<p>数据标注旨在构建基于视频的问答样本，对应八个问题类型，全面评估模型在<strong>任务理解</strong>、<strong>失败分析</strong>和<strong>失败纠正</strong>方面的能力。这八类问题包括：1) 任务识别、2) 任务规划（分解为子阶段）、3) 失败检测、4) 失败识别（分类）、5) 失败定位（确定错误子阶段）、6) 失败解释、7) 高级纠正（指定应执行的子任务序列）、8) 低级纠正（提供末端执行器移动方向的细粒度指导）。对于前五类有明确答案的问题，参考答案直接从视频的文本描述中提取；对于后三类语义更丰富的问题，则使用GPT-4o结合视频和文本描述生成参考答案，并经过人工审核修正。</p>
<p><strong>2. RoboFAC模型</strong><br>模型基于先进的开源多模态模型Qwen2.5-VL进行微调。该模型支持不同分辨率的单图、多图和视频输入，在视觉问答任务上表现出色。</p>
<ul>
<li><strong>任务理解</strong>：模型能够通过视频识别机器人正在执行的任务（任务识别），并通过分析视频中机器人的操作方式，将任务分解为一系列子阶段（任务规划）。</li>
<li><strong>失败分析</strong>：模型具备全面的失败分析能力，包括：判断任务是否成功（失败检测）；若失败，则确定失败类型（失败识别）；定位错误发生在哪个步骤（失败定位）；并提供对失败原因的详细解释（失败解释）。</li>
<li><strong>失败纠正</strong>：这是模型的核心创新能力，旨在帮助VLA模型从失败中恢复。它提供两种纠正建议：<ul>
<li><strong>高级纠正</strong>：提供明确的指导，指定模型为从失败中恢复应执行的子任务序列。这特别适用于解决任务规划错误（如遗漏子任务或子任务顺序错误）。</li>
<li><strong>低级纠正</strong>：提供细粒度的控制指导，特别是关于末端执行器移动方向的建议，帮助机械臂准确到达正确位置。这更适用于解决机器人的低级执行错误（如未能到达正确位置或轨迹不当）。</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.openai.com/placeholder/2/2" alt="数据构建与模型部署流程"></p>
<blockquote>
<p><strong>图3</strong>：RoboFAC框架流程。<strong>顶部</strong>：RoboFAC数据集的构建流程，包括通过运动规划代码和错误代码片段生成失败视频，以及后续的数据清理和标注。<strong>左下</strong>：通过微调Qwen2.5-VL模型构建RoboFAC模型，使其具备任务理解、失败分析和失败纠正能力。<strong>右下</strong>：将RoboFAC模型部署在真实世界VLA控制任务中，有效帮助VLA从失败中恢复。</p>
</blockquote>
<p>与现有方法相比，RoboFAC的创新点具体体现在：1) <strong>数据集层面</strong>：构建了首个大规模、覆盖多层次失败类型（任务规划、运动规划、执行控制）并提供多维度（理解、分析、纠正）问答标注的机器人失败视频数据集；2) <strong>模型层面</strong>：训练了一个专用的轻量级模型，特别强化了<strong>失败纠正</strong>能力，尤其是区分并提供“高级”与“低级”两种不同粒度的纠正建议，这是通用MLLMs所欠缺的；3) <strong>系统层面</strong>：设计并验证了将专用失败分析模型作为外部批评者集成到实时VLA控制流程中的完整管道。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/数据集</strong>：使用自行构建的RoboFAC评估基准。训练集包含60K个仿真QA对，测试集包含10K个仿真QA对和8K个真实世界QA对。测试集包含了训练时未见的仿真视频和模型从未接触过的真实世界任务（InsertCylinder和PlaceCube），以评估泛化能力和仿真到真实的迁移能力。</li>
<li><strong>Baseline方法</strong>：对比了开源模型（Qwen2.5-VL-3B/7B）、专有模型（Gemini-2.0, GPT-4o）以及本文提出的RoboFAC-3B/7B。</li>
<li><strong>评估指标</strong>：对于失败检测、识别、定位采用选择题准确率；对于其他语义更丰富的任务，使用外部LLM从正确性、相关性和完整性三个维度评分，并取平均。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>在RoboFAC基准上的综合性能</strong>：如表2所示，RoboFAC-7B在所有任务类别（短视界、中视界、长视界、动态和真实世界任务）上均一致优于所有基线模型，平均得分79.10，显著超过GPT-4o（57.42）和Gemini-2.0（51.11）。即使较小的RoboFAC-3B也达到了76.80的平均分。</li>
</ol>
<p><img src="https://cdn.openai.com/placeholder/3/3" alt="基准测试性能对比表"></p>
<blockquote>
<p><strong>表2</strong>：各多模态模型在RoboFAC基准上的性能。该基准评估了模型在短视界、中视界、长视界、动态和真实世界操作任务五个类别上的能力。分数代表每个类别上的成功率（%），最后一列报告了所有任务的平均性能。我们提出的RoboFAC模型（3B和7B）在所有类别上均一致优于开源（Qwen2.5-VL, Gemini-2.0）和闭源（GPT-4o）基线。</p>
</blockquote>
<ol start="2">
<li><strong>多维度能力分析</strong>：图4（左）进一步分解了模型在八个关键维度的表现。RoboFAC模型在任务规划、低级纠正以及所有三个失败相关能力（检测、识别、定位）上均取得了最高或接近最高的分数，展示了其在复杂任务分解和从执行失败中恢复方面的强大能力。相比之下，GPT-4o和Gemini-2.0在任务规划和分层纠正方面表现有限。</li>
</ol>
<p><img src="https://cdn.openai.com/placeholder/4/4" alt="各维度能力得分"></p>
<blockquote>
<p><strong>图4</strong>：RoboFAC基准上不同维度的得分。<strong>左图</strong>：仿真数据集上不同问题维度的性能。<strong>右上</strong>：真实世界数据集上不同问题维度的性能。<strong>右下</strong>：不同真实世界任务上的性能。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界操作性能</strong>：研究者将RoboFAC模型集成到基于SO-100机械臂和GR00T-N1 VLA模型的真实控制流程中。如表3所示，在四个真实世界任务上，使用RoboFAC-7B（低级纠正）提供的指令进行多次尝试后，平均成功率达到了61.25%，相较于无纠正（47.5%）有显著提升，也优于GPT-4o（56.25%）和Qwen2.5-VL-7B（50.0%）。仅经过一轮纠正，RoboFAC也显示出强于其他方法的改进。</li>
</ol>
<p><img src="https://cdn.openai.com/placeholder/5/5" alt="真实世界操作成功率"></p>
<blockquote>
<p><strong>表3</strong>：真实世界操作的成功率。比较了无纠正、GPT-4o、Qwen2.5-VL-7B、RoboFAC-7B（低级）和RoboFAC-7B（高级）五种条件在首次尝试和总共5次尝试（最多4次纠正）后的成功率。RoboFAC-7B（低级）始终获得最高的平均成功率。</p>
</blockquote>
<p><strong>消融实验分析</strong>：从表3的对比可以看出不同纠正策略的贡献。<strong>低级纠正</strong>在提升真实世界任务成功率方面效果最为显著（平均61.25%），而<strong>高级纠正</strong>的效果相对较弱（平均50.0%），这表明对于实验中测试的、主要由低级执行错误导致失败的任务，提供细粒度的末端执行器运动指导比仅提供高级任务序列更有帮助。同时，与通用模型GPT-4o和未经专门失败数据微调的Qwen2.5-VL相比，RoboFAC模型的显著优势凸显了<strong>专用失败数据训练</strong>这一组件的关键贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个大规模、多样化、覆盖多层次失败类型并提供多维度（任务理解、失败分析、失败纠正）问答标注的机器人失败视频数据集（RoboFAC数据集）及相应的评估基准。</li>
<li>基于该数据集训练了一个轻量级但高效的专用多模态模型（RoboFAC模型），该模型具备全面的失败分析与纠正能力，特别是在提供“高级”与“低级”两种粒度纠正建议方面表现出色。</li>
<li>成功将RoboFAC模型作为外部批评者集成到真实世界VLA控制流程中，并通过实验验证了其能有效提升VLA系统在遇到失败时的恢复能力和任务整体成功率。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，RoboFAC模型在提供高级纠正建议方面的性能仍有提升空间（如表3所示，高级纠正效果弱于低级纠正）。此外，尽管数据集涵盖了多种任务，但对于极其复杂或非结构化的长视界任务，模型的泛化能力可能仍需进一步测试和加强。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>专用失败分析模型的价值</strong>：本研究证明了针对机器人失败场景专门训练模型的有效性，其性能可以超越通用的、能力更强的MLLMs。这为提升机器人系统的鲁棒性指明了一条切实可行的路径。</li>
<li><strong>多层次纠正策略</strong>：区分“高级”与“低级”纠正并分别进行优化，符合机器人任务执行的层次化特点。后续工作可以进一步探索如何根据失败类型自适应地选择或融合不同粒度的纠正策略。</li>
<li><strong>数据集的构建方法</strong>：论文中结合仿真环境生成、大模型辅助标注和人工审核的数据构建流程，为创建其他机器人专项能力数据集提供了可借鉴的范式。数据集的开放也将推动该子领域的研究。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在开放世界机器人操作中缺乏失败恢复能力的问题，提出了RoboFAC框架。其核心技术包括构建大规模、多样化的机器人失败数据集，并基于此开发具备任务理解、失败分析与失败纠正能力的RoboFAC模型。实验表明，该模型在评估基准上性能超越GPT-4o达34.1%，集成到真实VLA控制流程后，在四个真实任务上平均相对性能提升29.1%，有效提升了机器人从失败中恢复的能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.12224" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>