<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.07869" target="_blank" rel="noreferrer">2510.07869</a></span>
        <span>作者: Gu, Junwen, Wu, Zhiheng, Si, Pengxuan, Qiu, Shuang, Feng, Yukai, Sun, Luoyang, Luo, Laien, Yu, Lianyi, Wang, Jian, Wu, Zhengxing</span>
        <span>日期: 2025/10/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>水下环境具有复杂的流体动力学、有限的能见度和受限的通信等独特挑战，使得机器人操作异常困难。尽管数据驱动方法已推动了地面机器人具身智能的发展，并催生了一些针对特定任务的水下自主机器人，但开发能够自主执行多种任务的通用型水下智能体仍然极具挑战，主要原因在于缺乏大规模、高质量的水下数据集。现有水下数据集多为任务特定型，缺乏构建统一框架所需的多样性，而真实水下数据采集成本高昂、风险巨大。</p>
<p>本文针对上述“数据孤岛”和模型泛化能力不足的痛点，提出了一个从数据到任务的系统性新视角：首先利用仿真平台高效构建大规模、多任务的水下视觉-语言-动作（VLA）数据集，然后基于此数据集训练一个专为水下机器人设计的通用VLA模型。核心思路是：通过仿真构建涵盖9种场景、20种任务的USIM数据集，并基于此数据集开发U0模型，该模型通过多模态传感器融合和基于卷积注意力的感知聚焦增强（CAP）模块，提升水下空间理解和移动操作能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架分为三个主要阶段：仿真环境构建、数据集生成和VLA模型训练。首先，使用Stonefish仿真器构建了9种不同的水下场景（如海底环境、工业水池、现代沉船等）和一个配备机械臂的BlueROV2机器人模型。通过集成ROS实现自动化数据采集，并应用地图随机化、光照和水体清澈度变化来增强场景多样性。基于此仿真环境，生成了USIM数据集。最后，利用USIM数据集训练了提出的U0模型。</p>
<p><img src="https://arxiv.org/html/2510.07869v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：整体框架。使用Stonefish仿真器构建水下场景和BlueROV2机器人，通过ROS进行数据采集和控制，生成USIM数据集。基于该数据集，开发了具有双系统架构的U0模型，整合多模态传感器融合和基于卷积注意力的感知聚焦增强，同时输出目标感知和机器人动作。</p>
</blockquote>
<p><strong>核心模块一：多模态传感器融合</strong>。针对水下机器人浮游状态、需协调推进器与机械臂运动、深度频繁变化以及视觉感知受环境干扰严重等特点，U0在状态空间中融合了水下特有的传感器数据：双目相机图像、压力传感器、惯性测量单元（IMU）和多普勒速度计程仪（DVL），以提升本体感知和定位能力。动作空间则结合了归一化的推进器PWM信号和机械臂关节角度。一个关键创新是采用机器人中心坐标系表示目标位姿（公式2），而非全局坐标系。这种相对位姿表示能更好地捕捉水下动态运动特性，减少对固定世界坐标系的依赖，并符合以自我为中心的生物决策原则，从而增强了模型在不同任务间的泛化能力。</p>
<p><strong>核心模块二：基于卷积注意力的感知聚焦增强模块（CAP）</strong>。由于水下环境视觉退化严重，仅依赖基础VLM（GR00T N1.5）的视觉能力难以准确捕捉目标。因此，U0引入了一个由VLM特征引导的CAP模块，以增强模型对目标物体的检测和定位能力。</p>
<p><img src="https://arxiv.org/html/2510.07869v3/x6.png" alt="CAP模块架构"></p>
<blockquote>
<p><strong>图6</strong>：CAP模块架构。该模块由卷积层、注意力层、池化操作和多层感知机构成，接收VLM提取的特征，通过卷积和注意力机制增强对目标物体的感知能力，输出目标预测。</p>
</blockquote>
<p>具体而言，首先通过VLM处理双目图像提取特征Token（公式3），随后经过卷积操作（公式4、5）和通道注意力机制，最后通过池化和MLP预测目标位姿T（公式6）。CAP模块的训练目标是最小化预测目标T与真实目标T_gt之间的均方误差（MSE）损失（公式7）。在整体训练中，CAP损失作为辅助任务，与动作模块的损失加权求和构成总损失（公式8，其中α为权重因子）。在推理阶段，该分支可被禁用，因此不会增加模型尺寸或计算延迟。</p>
<p>与现有主要面向地面机器人的VLA模型（如GR00T N1.5）相比，U0的创新点具体体现在：1) 针对水下机器人独特的运动模式和感知挑战，设计了专门的多模态状态和动作空间表示；2) 引入了机器人中心坐标系的相对位姿表示方法；3) 新增了可训练的CAP模块，专门用于增强水下退化视觉环境下的目标感知能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在基于Stonefish构建的仿真环境中进行。评估使用了自建的测试数据集（包含20个任务，共35K帧，约1小时数据）以及在线闭环仿真测试。对比的基线方法包括：原始未微调的GR00T N1.5模型、在USIM数据集上微调的GR00T模型（GR00T FT）、以及提出的U0模型（评估了单目和双目输入版本）。</p>
<p><strong>关键离线评估结果</strong>：如表II所示，直接应用原始GR00T N1.5模型到水下任务时，其动作误差（e_action）比微调后的模型高出一个数量级，凸显了预训练领域（人形机器人）与水下领域之间的巨大差距。在USIM上微调后，性能大幅提升。值得注意的是，微调后，双目输入模型的动作精度和感知精度整体优于单目输入。与GR00T FT相比，U0在单目和双目输入下的平均e_action分别降低了7.7%和4.2%，证明了CAP模块在提升目标感知和动作精度方面的有效性。特别是在单目抓取任务中，U0相比GR00T FT有显著提升，表明CAP模块有效缓解了单目3D空间感知的固有局限。</p>
<p><strong>关键在线测试结果</strong>：在7个非抓取任务（如检查、避障、扫描、动态跟踪）的在线测试中，U0（双目）取得了平均80%的成功率。</p>
<p><img src="https://arxiv.org/html/2510.07869v3/x7.png" alt="在线成功率对比"></p>
<blockquote>
<p><strong>图7</strong>：在线测试中各模型在多个任务上的成功率对比。U0（双目）在非抓取任务上平均成功率80%，整体优于GR00T FT和U0（单目）。</p>
</blockquote>
<p>在5个具有挑战性的移动抓取任务中，如表III所示，U0（双目）将机器人与目标物体的平均距离相比GR00T FT减少了21.2%，表明其能更准确地定位目标并启动抓取尝试。</p>
<p><strong>消融实验总结</strong>：实验结果实质上对比了不同配置（原始模型、仅微调、微调+CAP模块；单目 vs 双目），起到了消融研究的作用。结果表明：1) USIM数据集对于领域适应至关重要；2) CAP模块能有效提升感知和动作性能，尤其在单目视觉下优势明显；3) 针对水下环境，双目视觉输入相比单目输入具有明确优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1) 构建了首个大规模、多任务的水下视觉-语言-动作数据集USIM，包含超过56.1万帧数据，涵盖9个场景下的20种任务；2) 提出了专为水下机器人设计的VLA模型U0，通过多模态传感器融合和创新的CAP模块，显著提升了水下空间理解和移动操作性能；3) 建立了一个从仿真数据到多任务执行的、可扩展的水下机器人智能框架，验证了利用仿真数据训练通用型水下VLA模型的可行性。</p>
<p>论文自身提到的局限性包括：模型性能仍有提升空间，特别是在移动抓取任务中；目前主要依赖视觉和常规传感器，在深水或低能见度环境下感知能力可能受限。仿真环境与真实世界之间存在差距。</p>
<p>这项工作对后续研究的启示在于：为水下机器人通用智能研究提供了一个高质量的数据集和强基线模型。它展示了利用仿真高效构建专业领域机器人数据集、并通过领域自适应和专用模块设计来提升模型性能的有效路径。未来研究方向包括：丰富仿真场景和数据收集策略以进一步提升模型鲁棒性；融合声纳等额外模态以增强恶劣环境下的感知能力；以及最终在真实水下机器人上进行部署和验证，实现从仿真到现实的迁移。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对水下机器人缺乏大规模高质量数据集、难以实现多任务自主智能的核心问题，提出了仿真视觉-语言-动作数据集USIM与通用模型U0。USIM包含约15.6小时、20种任务的交互数据；U0通过多模态融合整合双目视觉等传感器，并引入卷积注意力感知增强模块（CAP）提升空间理解与操作能力。实验表明，该框架在多项任务中成功率达80%，在移动操作任务中比基线方法将目标距离缩短21.2%，验证了其有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.07869" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>