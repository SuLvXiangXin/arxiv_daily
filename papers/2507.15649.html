<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.15649" target="_blank" rel="noreferrer">2507.15649</a></span>
        <span>作者: Xu, Haocheng, Zhang, Haodong, Chen, Zhenghan, Xiong, Rong</span>
        <span>日期: 2025/07/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人模仿人类运动的主流框架主要分为两类：解耦策略（如PMP）和全身策略（如HumanPlus、Exbody）。解耦策略仅通过强化学习（RL）生成下半身动作以保持平衡，上身目标动作被直接执行，但这可能超出机器人的执行能力，导致失衡。全身策略则使用RL策略控制全身关节来模仿全身运动目标，但这可能在上身和基座动作中引入振动和偏差，导致稳定性和相似性奖励之间存在冲突。</p>
<p>本文针对人形机器人在站立状态下模仿人类上身运动时，稳定性和动作保真度难以兼得的痛点，提出了一种新的视角：引入一个可执行运动先验模块来优化输入的上身运动目标，同时让RL策略专注于控制下半身以维持整体平衡。核心思路是：首先通过运动重定向网络将人类动作转化为机器人动作数据集，然后训练一个RL策略来跟踪优化后的上身目标并控制下半身保持站立稳定，其中优化过程由一个基于当前机器人状态调整目标动作的可执行运动先验模块完成。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架包含三个核心阶段：运动重定向、RL策略训练和可执行运动先验。输入是人类上身运动序列，输出是机器人能够稳定执行的上身关节角度和相应的下半身控制动作。</p>
<p><img src="https://arxiv.org/html/2507.15649v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：本文方法整体框架概述。左侧<strong>运动重定向</strong>：使用图卷积网络将人类动作 $\bm{m}<em>{source}$ 映射为人形机器人关节动作 $\bm{m}</em>{target}$。中间<strong>RL策略</strong>：训练一个策略，以上身运动目标 $\bm{g}_t$ 和当前状态 $\hat{\bm{s}}_t$ 为输入，输出下半身动作以保持平衡。右侧<strong>可执行运动先验</strong>：基于当前状态 $\hat{\bm{s}}_t$ 调整目标运动 $\bm{g}_t$，输出优化后的目标 $\tilde{\bm{g}}_t$ 以提高稳定性。</p>
</blockquote>
<p><strong>1. 运动重定向网络</strong>：该模块用于生成训练RL策略所需的大规模机器人运动数据集。网络架构借鉴VQ-VAE，包含运动编码器、VQ码本层和运动解码器。将人类和机器人的上身骨骼视为图，输入是人类关键节点（腰、躯干、肩、肘、腕）的位置 $\bm{Q}_A$ 和边特征 $\bm{E}<em>A$。编码器 $f_e$ 将其编码为潜在特征 $\bm{z}<em>A$，再通过一个变换网络 $f</em>{tf}$ 转换为目标骨架的潜在特征 $\bm{z}<em>B$。码本层根据最近邻原则从码书中选取嵌入向量 $\bm{z}<em>e$，最后由解码器 $f_d$ 结合机器人边特征 $\bm{E}<em>B$ 生成机器人关节角度 $\bm{Q}<em>B$。训练损失包括末端执行器方向损失 $L</em>{ee}$、末端执行器旋转矩阵损失 $L</em>{ori}$、肘部方向损失 $L</em>{elb}$、嵌入损失 $L</em>{emb}$ 和承诺损失 $L</em>{com}$，权重配置见表I。</p>
<p><strong>2. RL控制策略</strong>：这是一个目标条件策略 $\pi: \textbf{G} \times \textbf{S} \longrightarrow \textbf{A}$，负责在跟踪上身运动目标的同时控制下半身保持平衡。策略被解耦为下半身RL策略 $\pi_{lower}$ 和上身开环控制器 $\pi_{upper}$（即EMP模块）。状态 $\bm{s}_t$ 包括当前关节位置 $\bm{q}<em>t$、上一时刻动作 $\bm{a}</em>{t-1}$、基座欧拉角 $\bm{rpy}_t$ 以及当前目标 $\bm{g}_t$。策略输入为过去T=15帧的历史状态 $\bm{S}_t$。动作 $\bm{a}_t$ 是12维的下半身关节目标位置，通过PD控制器转换为关节扭矩。奖励函数设计（见表II）专注于站立稳定性，包括基座朝向、投影重力、高度、线速度/角速度、加速度、腿部关节位置、脚部接触与滑动等正则化项，以及动作范围、变化率、加速度和能量消耗（扭矩、关节速度/加速度）等惩罚项。训练中采用了广泛的域随机化（见表III），包括摩擦、质量、惯性、控制器增益、电机参数甚至随机推动机器人，以增强策略的鲁棒性。</p>
<p><strong>3. 可执行运动先验模块</strong>：这是本文的核心创新点。受人类在执行动作时能预判危险并调整的启发，EMP模块在RL控制器之前工作，作为一个动作优化器。它基于机器人的当前状态 $\hat{\bm{s}}_t$ 对上身运动目标 $\bm{g}_t$ 进行调整，输出一个新的、更合理的上身目标 $\tilde{\bm{g}}_t$。其网络结构基于变分自编码器，同时编码当前状态和动作目标到潜在空间，并解码为新的动作目标，旨在将不稳定的动作转化为稳定的动作，同时最小化对动作幅度的改变。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真（Isaac Gym）和真实世界（Unitree H1和Walker X）两个平台上进行。使用了Human3.6M等人类动作数据集，并经由运动重定向网络生成机器人动作数据集用于训练。</p>
<p>对比的基线方法包括：<strong>HumanPlus</strong>（全身RL策略）、<strong>Exbody</strong>（全身RL控制）以及<strong>PMP</strong>（解耦策略，仅RL控制下半身，直接执行上身目标）。评估指标包括<strong>成功率</strong>（机器人能在整个动作序列中保持平衡的比例）、上身关节的<strong>平均跟踪误差</strong>以及<strong>动作幅度相似度</strong>。</p>
<p><img src="https://arxiv.org/html/2507.15649v1/x1.png" alt="不同框架对比"></p>
<blockquote>
<p><strong>图1</strong>：不同运动模仿框架对比。(a) 解耦策略（如PMP）。(b) 全身策略（如HumanPlus, Exbody）。(c) 本文方法，引入可执行运动先验优化上身目标，RL策略提供下半身动作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.15649v1/x3.png" alt="仿真实验结果"></p>
<blockquote>
<p><strong>图3</strong>：在Unitree H1仿真模型上的量化结果。本文方法（EMP）在成功率（97.5%）上显著高于HumanPlus（77.5%）、Exbody（85%）和PMP（42.5%），同时保持了最低的平均跟踪误差和较高的动作幅度相似度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.15649v1/x4.png" alt="真实机器人实验结果"></p>
<blockquote>
<p><strong>图4</strong>：在真实Unitree H1机器人上的运动模仿结果可视化。本文方法能稳定完成挥手、画圆、打拳等多样化的上身动作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.15649v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融实验。移除可执行运动先验（EMP）模块后，成功率从97.5%下降至82.5%，跟踪误差增大，证明了EMP对稳定性的关键贡献。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.15649v1/x6.png" alt="世界模型辅助训练"></p>
<blockquote>
<p><strong>图6</strong>：使用世界模型预测下一状态以辅助EMP网络训练。该图展示了状态转移过程，世界模型 $f_w$ 根据当前状态 $\hat{\bm{s}}_t$ 和调整后动作 $\tilde{\bm{g}}<em>t$ 预测下一状态 $\hat{\bm{s}}</em>{t+1}$，用于计算稳定性奖励并回传梯度。</p>
</blockquote>
<p>关键实验结果总结：在仿真中，本文方法取得了**97.5%**的最高成功率，远高于HumanPlus（77.5%）、Exbody（85%）和PMP（42.5%）。同时，其平均跟踪误差（约0.05 rad）与全身策略相当，但显著优于PMP；动作幅度相似度也优于其他方法。消融实验表明，移除EMP模块会导致成功率下降15个百分点至82.5%，并增大跟踪误差，验证了该模块的有效性。真实机器人实验成功演示了挥手、画圆、打拳等多种上身运动，证明了框架的sim-to-real迁移能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出一个完整的基于RL的人形机器人上身运动模仿框架，集成了运动重定向网络和鲁棒RL策略训练流程；2) 创新性地引入了<strong>可执行运动先验模块</strong>，能够根据机器人当前状态动态优化上身运动目标，在确保站立稳定性的前提下最大化动作保真度，有效解决了稳定性与相似性之间的冲突；3) 利用世界模型模拟环境状态转移以进行梯度反传，并成功实现了向两款真实人形机器人的系统迁移。</p>
<p>论文提到的局限性在于，当前工作主要聚焦于<strong>站立状态下的上身运动模仿</strong>，未涉及移动基座或全身动态运动。</p>
<p>对后续研究的启示：EMP模块作为一种在指令层进行实时优化的“过滤器”，为机器人安全、稳定地执行复杂或动态任务提供了一种新思路。这种将高层运动先验与底层RL控制相结合的分层架构，可以扩展到更广泛的机器人操作和移动任务中。同时，如何学习更具普适性和自适应性的运动先验，以应对未知或极端动作，是未来值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在站立状态下模仿人类上半身动作时，因可控范围有限而导致的整体稳定性问题，提出了一种基于强化学习的框架。核心技术包括：1）设计重定向网络生成大规模上半身动作数据集以训练RL策略；2）提出可执行动作先验（EMP）模块，根据机器人当前状态动态调整输入目标动作，在最小化动作幅度改变的前提下确保安全稳定。通过仿真与实物测试验证了该框架的实用性，有效提升了机器人在模仿过程中的站立稳定性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.15649" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>