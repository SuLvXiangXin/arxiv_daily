<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.19356" target="_blank" rel="noreferrer">2510.19356</a></span>
        <span>作者: Jie Zhao Team</span>
        <span>日期: 2025-10-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，以扩散模型和流匹配方法为代表的迭代生成模型已成为机器人模仿学习的有力工具，但其推理过程需要进行多步迭代去噪或积分，导致计算成本高昂，难以在物理机器人上实时部署。为解决推理速度问题，研究者提出了知识蒸馏方法和一致性模型等方法。其中，Shortcut框架通过引入期望步长目标，将策略蒸馏重新表述为在线训练范式，旨在实现高效的一步推理。然而，Shortcut模型的一步生成质量仍不令人满意，其本质是原始流匹配模型的降步蒸馏，这种耦合模式增加了优化难度。</p>
<p>本文针对现有一步推理方法（如Shortcut）性能不佳的痛点，提出了一种新视角：通过多步一致性监督来改善一步生成的质量。具体而言，本文的核心思路是：在Shortcut模型基础上扩展多步一致性损失，将一步损失分解为多步损失，并引入自适应梯度分配方法以平衡优化过程，从而在保持高速推理的同时提升性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法整体上是对Shortcut框架的扩展，旨在训练一个支持多步积分的一致性模型，以实现高质量的一步推理。模型的输入是噪声-动作对 <code>(a0, a1)</code> 和观测 <code>o</code>，输出是条件化的动作生成速度场 <code>v_θ(a_t, t, d, o)</code>。训练目标包含流匹配损失和多步一致性损失两部分，并通过自适应梯度分配算法进行优化。</p>
<p><img src="https://arxiv.org/html/2510.19356v1/overview.png" alt="方法框架对比"></p>
<blockquote>
<p><strong>图1</strong>：扩散模型、Shortcut模型与本方法的概念对比。扩散模型依赖小步长的迭代去噪；Shortcut引入期望步长以更少的迭代生成策略；本方法进一步扩展Shortcut，通过在多步上平均一步损失来提高生成质量而不牺牲速度。</p>
</blockquote>
<p>核心创新点主要体现在两个模块：</p>
<ol>
<li><p><strong>多步一致性损失</strong>：Shortcut的自洽性目标基于两个小步长，当其外推步长过大时误差会急剧增加。为解决此问题，本文提出了一种更通用的、在同一轨迹上进行多步积分的过程（公式7-8）。通过将长距离的捷径分解为多个局部近似，为路径上的多个区间建立了约束，从而将一步误差分布到多个区间，约束了每段的误差积累。最终，对于轨迹上的 <code>n</code> 个积分点，可以得到 <code>(n-1)</code> 个一致性约束，形成多步一致性损失 <code>L_MC</code>（公式10）。</p>
</li>
<li><p><strong>自适应梯度分配</strong>：在机器人模仿学习中，低维动作空间的结构性较弱，且多步一致性损失在数值上通常远小于流匹配损失（如图2、3所示），导致联合训练时流匹配损失的梯度主导优化，多步一致性目标训练不足。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.19356v1/block_hammer_beat_loss.png" alt="损失差异示例1"></p>
<blockquote>
<p><strong>图2</strong>：（Block Hammer Beat任务）流匹配损失（蓝色）与多步一致性损失（橙色）的数值差异巨大，后者在训练初期接近于0。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.19356v1/bottle_adjust_loss.png" alt="损失差异示例2"></p>
<blockquote>
<p><strong>图3</strong>：（Bottle Adjust任务）同样展示了两种损失量级的不平衡，这会导致优化困难。</p>
</blockquote>
<p>为解决梯度不平衡问题，本文将联合目标视为一个多任务学习问题，并提出自适应梯度分配算法。该算法通过一个缩放因子 <code>c</code> 来调控复合梯度在两个任务梯度方向上的投影强度（公式12）。<code>c</code> 的值会根据两个任务损失下降速率 <code>v_i</code> 的动态变化进行自适应调整（公式14-15），确保下降较慢的任务（通常是多步一致性损失）获得更强的梯度投影。算法前期则采用PCGrad方法处理多步一致性损失无效的初始阶段。整个梯度分配逻辑旨在平衡 <code>L_FM</code> 和 <code>L_MC</code> 的贡献，缓解优化难题。</p>
<p><img src="https://arxiv.org/html/2510.19356v1/grad_1.jpg" alt="梯度分配示意图1"></p>
<blockquote>
<p><strong>图4</strong>：通过调整缩放因子 <code>c</code>，可以控制复合梯度（红色箭头）在不同方向上的投影强度。蓝色箭头表示其在不同任务梯度方向（u1, u2）上的投影。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.19356v1/grad_2.jpg" alt="梯度分配示意图2"></p>
<blockquote>
<p><strong>图5</strong>：复合梯度的方向随着学习进程，逐渐从 u1 方向向 u2 方向偏移。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.19356v1/grad_3.jpg" alt="梯度分配示意图3"></p>
<blockquote>
<p><strong>图6</strong>：展示了梯度方向调整的另一种情况。</p>
</blockquote>
<p>网络架构方面，本文采用之前工作中提出的UDiT1d架构作为速度场预测网络。</p>
<p><img src="https://arxiv.org/html/2510.19356v1/UDiT.jpg" alt="网络架构"></p>
<blockquote>
<p><strong>图7</strong>：UDiT1d 网络架构。本研究未使用自监督学习，因此移除了滑动解码器，保留了标准的编码器-解码器结构，并包含 Rotary 位置编码和 addLLN 组件。</p>
</blockquote>
<p>此外，在推理阶段，为避免从高斯分布中采样到低概率样本导致性能下降，本文构建了一个从高斯分布采样的先验码本，将多步一致性目标和一步推理的采样限制在该码本内。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在两个仿真基准和五个真实世界任务上评估了所提方法。</p>
<ul>
<li><strong>仿真基准1：RoboTwin</strong>。一个用于双臂机器人工具使用和人机交互场景的基准测试，使用点云作为观测，包含8个任务（图8-15）。</li>
<li><strong>仿真基准2：Franka Kitchen</strong>。一个专注于长序列任务的测试环境，使用状态作为观测（图16-17）。</li>
<li><strong>真实世界任务</strong>：包括分类、用餐、动态干扰环境下的操作，以及结合触觉感知的“插孔”和“擦拭”任务（图18-23）。</li>
</ul>
<p>对比的基线方法包括：Diffusion Policy (DP) / 3D Diffusion Policy (3DP)、Meanflow 和 Shortcut。其中Diffusion使用UNet，其他方法均使用UDiT1d。</p>
<p><strong>关键实验结果</strong>：<br>在RoboTwin的8个任务中，本文方法（Multi-step Shortcut）在绝大多数任务上取得了最高成功率。例如，在“Block Hammer Beat”任务上达到96%成功率（Shortcut为92%，3DP为94%）；在“Bottle Adjust”任务上达到100%成功率（Shortcut为98%，3DP为96%）。在Franka Kitchen的长序列任务中，本文方法也取得了最佳或极具竞争力的性能。</p>
<p><img src="https://arxiv.org/html/2510.19356v1/block_hammer_beat.jpg" alt="RoboTwin任务场景1"></p>
<blockquote>
<p><strong>图8</strong>：Block Hammer Beat 任务场景。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.19356v1/bottle_adjust.jpg" alt="RoboTwin任务场景2"></p>
<blockquote>
<p><strong>图9</strong>：Bottle Adjust 任务场景。</p>
</blockquote>
<p>（由于篇幅，仅展示部分任务图，其他任务图10-15类似，展示了不同的操作场景。）</p>
<p><img src="https://arxiv.org/html/2510.19356v1/frankakitchen1.jpg" alt="Franka Kitchen场景1"></p>
<blockquote>
<p><strong>图16</strong>：Franka Kitchen 场景一，机器人交互对象包括水壶、燃烧器、滑块和橱柜。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.19356v1/frankakitchen2.jpg" alt="Franka Kitchen场景2"></p>
<blockquote>
<p><strong>图17</strong>：Franka Kitchen 场景二，机器人交互对象包括水壶、微波炉、顶部燃烧器和灯。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>消融实验主要验证了自适应梯度分配（AGA）和多步数 <code>K</code> 的影响。</p>
<ul>
<li><strong>自适应梯度分配的有效性</strong>：对比了使用和不使用AGA的训练损失曲线。如图24和25所示，在“Bottle Adjust”任务中，不使用AGA时多步一致性损失 <code>L_MC</code> 的下降严重滞后且不平稳；而使用AGA后，<code>L_MC</code> 能够与流匹配损失 <code>L_FM</code> 同步、平稳地下降，证明AGA有效平衡了优化过程。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.19356v1/withada.png" alt="带AGA的训练损失"></p>
<blockquote>
<p><strong>图24</strong>：使用自适应梯度分配（AGA）时，流匹配损失（<code>L_FM</code>）和多步一致性损失（<code>L_MC</code>）能够同步平稳下降。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.19356v1/withoutada.png" alt="不带AGA的训练损失"></p>
<blockquote>
<p><strong>图25</strong>：不使用自适应梯度分配时，多步一致性损失（<code>L_MC</code>）的下降严重滞后且波动大，优化不平衡。</p>
</blockquote>
<ul>
<li><strong>多步数 <code>K</code> 的影响</strong>：实验表明，随着 <code>K</code> 增大（从2到5），模型性能先提升后略有下降，在 <code>K=4</code> 时达到最佳。这验证了多步监督的有效性，但步数过多可能引入累积误差。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li>提出了一种用于机器人模仿学习的<strong>多步一致性算法</strong>，通过对多步预测进行监督，显著提升了一步生成策略的质量。</li>
<li>开发了一种<strong>自适应梯度分配优化方案</strong>，有效平衡了流匹配和多步一致性两个目标在联合训练中的梯度贡献，缓解了优化不平衡问题，提升了推理性能。</li>
<li>在仿真和真实机器人操纵任务上进行了广泛验证，证明了该框架作为通用模仿学习方法的有效性和潜力。</li>
</ol>
<p>论文提到的局限性主要在于方法可能对某些特定任务或观测模态的泛化能力有待进一步验证。此外，构建先验码本以稳定推理的策略可能依赖于特定的数据分布。</p>
<p>本文工作对后续研究的启示在于：为基于生成模型的一步高效策略学习提供了一个新颖的优化视角，即通过<strong>分解长距离目标为多段一致性约束</strong>并结合<strong>动态的梯度管理机制</strong>，可以在不依赖外部教师模型的情况下，直接训练出高性能的一步策略。这一思路可扩展到其他需要平衡不同目标或损失的联合训练场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习中扩散模型和流匹配方法因迭代去噪导致推理时间高、难以实时部署的问题，提出一种基于多步一致集成快捷模型的一步策略。方法通过扩展多步一致性损失，将一步损失拆分为多步损失以提升性能，并引入自适应梯度分配方法稳定优化过程。实验在两个仿真基准和五个真实环境任务中验证了算法的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.19356" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>