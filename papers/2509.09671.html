<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.09671" target="_blank" rel="noreferrer">2509.09671</a></span>
        <span>作者: Wei Yang Team</span>
        <span>日期: 2025-09-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，从人类演示中学习灵巧操作的主流方法通常采用三阶段流程：首先将人类动作捕捉（MoCap）数据重定向到机器人运动学模型，然后使用低级控制器进行跟踪，最后添加残差或校正项以补偿跟踪误差和本体差异。然而，这种方法存在关键局限性：重定向误差可能传播并影响下游学习，严格的指尖对应等要求可能导致对人类可行的抓取对机器人来说不可行或次优；此外，分阶段的流程可能导致演示信号利用不足，且误差在各阶段间累积。</p>
<p>本文针对上述痛点，提出了一种新的视角：避免严格的重定向和事后残差校正，而是将演示数据视为保留操作意图的“软参考”，允许机器人发现与其自身本体兼容的运动。本文的核心思路是提出一个统一的单循环优化框架 Dexplore，通过“参考范围探索”机制，直接从大规模人类MoCap数据中学习机器人控制策略，并最终蒸馏为可部署的、基于视觉的生成式控制器。</p>
<h2 id="方法详解">方法详解</h2>
<p>Dexplore 的整体流程分为两个阶段，如图2所示。第一阶段训练一个基于状态的模仿控制策略，通过参考范围探索（RSE）从原始MoCap数据中学习可扩展的灵巧操作技能。第二阶段，将该策略蒸馏为一个基于视觉的生成控制策略，将多样化的操作行为编码在一个统一的潜在空间中，支持跨对象泛化和真实世界部署。</p>
<p><img src="https://arxiv.org/html/2509.09671v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：Dexplore 方法总览。 (I) 首先训练一个基于状态的策略，通过参考范围探索从原始MoCap数据中获取可扩展的灵巧操作技能。(II) 然后将这些技能蒸馏到一个基于视觉的策略中，该策略将多样化的操作行为嵌入到一个统一的潜在空间。</p>
</blockquote>
<p><strong>第一阶段：基于参考的模仿控制</strong><br>该阶段将参考模仿建模为一个强化学习问题，使用PPO进行优化。</p>
<ul>
<li><strong>状态</strong>：策略的输入包括机器人本体感知和特权物体观测 <code>x_t</code>，以及源自参考演示的目标导向参考组件 <code>\hat{x}_t</code>。具体包括手部运动学、物体位姿、粗略几何线索（从手部关节指向物体最近表面点的向量）和简化的接触指示器。参考组件包含未来多个时间步的参考状态差值（如旋转差、位置差）和绝对状态。由于机器人和人类手部骨架结构不同，引入了一个映射 <code>M</code>（例如映射到五个指尖）将两者投影到共同的关节子集上。</li>
<li><strong>动作</strong>：控制分为两部分：6自由度浮动手腕信号和一组局部手指命令。每个元素都是一个比例-微分（PD）目标。策略在浮动手腕控制上使用残差动作空间，将6自由度位置偏移添加到当前手腕状态以形成PD目标，以提高稳定性和泛化能力。</li>
<li><strong>奖励与核心创新——参考范围探索（RSE）</strong>：奖励由状态-参考匹配项和能量正则化项组成。匹配项鼓励模拟状态与演示对齐，包括手部关节旋转/位置、物体方向/位置、几何和接触的匹配奖励。<br>Dexplore 的核心创新在于其学习机制，它通过两个关键设计减少对严格跟踪奖励的依赖，并整合了重定向、跟踪和校正：<ol>
<li><strong>自适应奖励加权</strong>：运动学匹配权重与手-物体表面距离成正比，而能量惩罚与之成反比。这鼓励在跟踪约束放松时进行更平滑、更低能量的运动。</li>
<li><strong>自适应早期终止阈值</strong>：对于每个终止准则（如手部位置、物体位置等奖励低于阈值），初始化一个较大的阈值，并根据失败rollout的比例逐步收紧：<code>κ = κ_init * N_fail / N_total</code>。这允许策略在早期进行广泛探索，并在可行时促进精确跟踪。</li>
</ol>
</li>
<li><strong>早期终止与状态初始化</strong>：定义了多项终止条件（如匹配奖励过低、接触模式长时间不匹配等），以在交互严重偏离参考时及时终止回合。状态初始化仅根据参考姿势设置机器人手掌的全局旋转和平移，其他自由度设为零，并采用缓存高质量状态和优先采样的策略提高效率。</li>
</ul>
<p><strong>第二阶段：基于视觉的生成控制</strong><br>目标是学习一个策略 <code>\tilde{π}</code>，使用部分观察（和可选稀疏目标）来操控物体。</p>
<ul>
<li><strong>观察</strong>：包括手部本体感知和从单视角深度图像重建的物体点云。使用PointNet++ backbone编码视觉输入。</li>
<li><strong>目标</strong>：通过选择性“解掩”参考运动的部分（如手腕轨迹）来定义稀疏目标，策略利用其潜在表示推断被掩码部分并生成多样化的操作行为。</li>
<li><strong>学习生成控制</strong>：框架包含一个编码器 <code>q_φ(z|x, \hat{x})</code>、一个先验网络 <code>p_ψ(z|\tilde{x}, P)</code> 和一个解码器策略 <code>\tilde{π}_θ(\tilde{a}|z, \tilde{x})</code>。编码器接收特权状态和参考，输出潜在分布；先验网络基于部分观察输出分布。潜在技能嵌入 <code>z</code> 在每个时间步采样，并在一个episode内保持噪声一致以确保时间一致性。使用DAgger进行在线模仿学习，以第一阶段的策略作为专家教师。训练目标包括动作重建损失和KL正则化项，鼓励编码器分布接近先验，确保其不捕获推理时不可用的信息。在推理时，编码器被省略，潜在变量直接从学习到的先验中采样。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：主要使用GRAB数据集（658个序列，51个物体），并额外使用TACO数据集测试泛化性。</li>
<li><strong>机器人平台</strong>：在仿真中测试两种机器人手：Inspire手（12自由度，6个驱动，欠驱动）和Allegro手（16自由度，全驱动）。</li>
<li><strong>对比基线</strong>：DexTrack（结合AnyTeleop或本文方法进行重定向）。</li>
<li><strong>评估指标</strong>：物体旋转误差 <code>R_err</code>、物体平移误差 <code>T_err</code>、指尖误差 <code>E_finger</code>、成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在GRAB数据集上的定量评估结果如表1所示。Dexplore 在两种机器人手上都取得了最高的成功率（Inspire: 87.7%， Allegro: 78.7%），显著优于基线方法 DexTrack w/ AnyTeleop（Inspire: 7.4%， Allegro: 45.9%）。即使与使用本文重定向结果改进的 DexTrack w/ Ours 相比，Dexplore 在Inspire手上仍有显著优势（87.7% vs. 69.8%）。</p>
<p><img src="https://arxiv.org/html/2509.09671v1/media/x9.png" alt="定量结果表"></p>
<blockquote>
<p><strong>表1</strong>：在GRAB数据集上与基线的定量评估比较。跟踪误差报告为在成功rollout上/在所有帧上的平均值。Dexplore 在成功率上显著优于所有基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.09671v1/x3.png" alt="与基线对比"></p>
<blockquote>
<p><strong>图3</strong>：左：与AnyTeleop（上）和重定向MoCap数据（下）的对比。对于自由度有限的手，重定向常产生不自然的姿势，而我们的跟踪结果（中）更自然。右：与DexTrack（上）对比。对于抓握区域小的物体，基线无法可靠抓握，而我们的方法（下）成功。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>表1中包含了消融实验 <code>Dexplore (w/o RSE)</code>，即禁用参考范围探索机制。结果显示，没有RSE时，两种手的成功率均大幅下降至29.9%，证明了自适应空间范围和早期终止阈值这一核心机制的关键作用。</p>
<p><img src="https://arxiv.org/html/2509.09671v1/x4.png" alt="泛化到Allegro手"></p>
<blockquote>
<p><strong>图4</strong>：我们的模仿控制框架也适用于具有不同形态（四指、更大尺寸）的Allegro手。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.09671v1/x5.png" alt="真实世界部署"></p>
<blockquote>
<p><strong>图5</strong>：基于视觉的策略部署在配备Inspire灵巧手和深度相机的真实XArm-7机械臂上。</p>
</blockquote>
<p><strong>真实世界部署</strong>：<br>论文成功将蒸馏后的基于视觉的生成策略部署到真实的 Inspire 灵巧手系统上（图5），仅使用单视角深度感知和本体感知，在测试时无需动作捕捉参考、姿态估计器或力传感器。策略以标准控制频率闭环运行，并能根据紧凑的技能代码执行抓取操作。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 Dexplore，一个统一的单循环优化框架，通过将演示作为自适应空间范围内的软参考，直接从人类MoCap数据学习灵巧操作，无需显式的重定向和残差校正。</li>
<li>将学习到的基于状态的跟踪策略蒸馏为一个基于视觉的、技能条件化的生成控制策略，该策略能够编码多样化的操作技能，并仅使用单视角深度输入进行部署。</li>
<li>在真实灵巧手系统上成功部署了所提出的框架，验证了其有效性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到其方法目前依赖于模拟训练，且大规模强化学习需要大量计算资源。</p>
<p><strong>启示</strong>：<br>Dexplore 为利用不完美的人类演示数据提供了一种新范式，即将其视为可适应的引导而非地面真值。这种“参考范围探索”的思想可以推广到其他存在本体差异或演示噪声的模仿学习场景中。此外，从特权状态策略到基于视觉策略的蒸馏路径，为在保持性能的同时解决部分可观测性问题提供了可行方案。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Dexplore方法，旨在解决利用人类手部运动捕捉数据训练机器人灵巧操作策略时，因演示不精确和本体差异导致的三阶段流程误差累积与数据利用不足问题。其核心是统一的单循环优化框架，将重定向与跟踪联合进行，以演示为软参考，通过自适应空间范围约束和强化学习，直接学习控制策略。该方法能保留演示意图、涌现机器人专属策略、提升抗噪性，并可扩展至大规模演示库。最终策略被提炼为基于视觉的技能条件生成控制器，支持跨物体泛化与真实部署。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.09671" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>