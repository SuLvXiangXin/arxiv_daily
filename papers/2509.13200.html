<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>StageACT: Stage-Conditioned Imitation for Robust Humanoid Door Opening - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>StageACT: Stage-Conditioned Imitation for Robust Humanoid Door Opening</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.13200" target="_blank" rel="noreferrer">2509.13200</a></span>
        <span>作者: Shayegan Omidshafiei Team</span>
        <span>日期: 2025-09-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人开门是一个关键但极具挑战性的长时程全身协调操作任务。当前主流方法主要分为两类：一类是基于显式建模和模块化控制器的传统方法，依赖几何模型、接触点估计或外部传感（如AR标签）；另一类是基于模仿学习或强化学习的端到端方法，已在四足或轮式平台上取得进展，但鲜少扩展到人形机器人。现有方法存在关键局限性：显式方法缺乏泛化能力且依赖先验知识；而模仿学习方法在应对开门任务的部分可观测性时，容易因观察歧义性（例如，视觉相似的场景可能对应门锁已开或未开的不同状态）导致“模式崩溃”，产生动作混淆或顺序错误。</p>
<p>本文针对模仿学习在长时程、部分可观测任务中易产生模式崩溃的具体痛点，提出了“阶段条件”这一新视角。其核心思路是：受人类将复杂任务分解为阶段的自然策略启发，在低层策略的输入中显式地加入任务阶段信息，从而为策略提供时间上下文，以消除观察歧义并引导更鲁棒的行为。</p>
<h2 id="方法详解">方法详解</h2>
<p>StageACT的整体框架建立在Action Chunking Transformer (ACT) 之上，核心创新在于为策略增加了阶段条件输入。</p>
<p><img src="https://arxiv.org/html/2509.13200v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：StageACT框架概述。该方法在低层策略中结合了阶段级引导。策略（基于CVAE的Transformer解码器）接收当前图像观测、上身关节状态、潜在变量z以及阶段标签作为输入，输出未来动作块（包括关节目标位置和躯干速度命令）。</p>
</blockquote>
<p>整体流程如下：首先，通过全身遥操作收集人类演示数据。每个演示轨迹被表示为一串观测-动作对 <code>(o_t, a_t)</code>。观测 <code>o_t</code> 包括头戴相机RGB图像 <code>I_t</code> 和上身29维关节状态 <code>q_t^upper</code>；动作 <code>a_t</code> 是32维向量，包含目标关节位置和三维躯干速度命令 <code>V_t^xyz</code>。训练时，轨迹被分割成固定长度 <code>H</code>（100步，约3秒）的动作块，形成数据集 <code>D = {(o_t, a_t:t+H)}</code>。推理时，策略接收当前观测和阶段标签，预测未来 <code>H</code> 步的动作序列，并通过时间平滑执行。</p>
<p>核心模块是基于条件变分自编码器（CVAE）的Transformer策略网络。其编码器将当前关节状态和专家动作序列映射为潜在变量 <code>z</code>，以捕捉演示的多样性。解码器（即策略）以Transformer架构聚合多模态输入：当前图像 <code>I_t</code>、关节状态 <code>q_t^upper</code>、潜在变量 <code>z</code> 以及<strong>新增的阶段条件向量</strong>。它输出预测的动作 <code>a_t</code>。训练目标包括重建损失（模仿专家）和KL散度正则项（约束潜在空间）。</p>
<p>与原始ACT相比，StageACT的核心创新点是<strong>阶段条件机制</strong>。具体而言，作者将开门任务人工分解为五个阶段（S1: 搜索把手，S2: 接近把手，S3: 旋转把手，S4: 推门，S5: 停止）。演示数据通过结合视觉和本体感知线索（如关节力矩突变）进行后处理标注阶段标签。在训练和评估时，阶段标签被编码为独热向量，与图像、状态等一同输入策略网络。</p>
<p><img src="https://arxiv.org/html/2509.13200v2/x3.png" alt="任务阶段分解"></p>
<blockquote>
<p><strong>图3</strong>：长时程开门任务的阶段分解时序快照。机器人从不同初始位置开始，依次执行搜索把手、用左拳下压旋转把手、用右臂推门并行走通过等阶段。</p>
</blockquote>
<p>这一设计的有效性在于：它为策略提供了高层的时间上下文。如图4所示，仅凭视觉，阶段S2（接近）和S4（推门）难以区分。阶段条件使策略能够区分这些视觉相似的观测，从而避免产生介于两个阶段之间的“平均”动作或跳序执行，有效缓解了模式崩溃。此外，阶段标签在测试时可作为提示，用于引导恢复行为或非顺序执行。</p>
<p><img src="https://arxiv.org/html/2509.13200v2/x4.png" alt="观察歧义示例"></p>
<blockquote>
<p><strong>图4</strong>：开门任务的自我中心视角时序快照，按子阶段分割。仅凭视觉观察，阶段2（接近）和阶段4（推门）难以区分，因此后处理中的阶段标注也依赖于本体感知线索。图中显示的双肩俯仰关节位置和力矩轨迹能够捕捉与接触事件相关的过渡线索。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界办公室环境中进行，使用Unitree G1人形机器人。数据集包含在两个不同办公室（光照、背景、门动力学不同）收集的135条成功演示。评估在一个<strong>先前未见过的门</strong>上进行，该门在外观和动力学（把手刚度、铰链阻尼）上均与训练门不同。机器人的初始距离和朝向被随机化。</p>
<p>对比的基线方法包括：1) <strong>ACT</strong>：原始模仿学习基线；2) <strong>ACT with history</strong>：在输入中加入最近5帧观测以缓解部分可观测性。评估指标包括成功率、任务完成时间以及上身关节和躯干速度的跟踪误差。</p>
<p>关键定量结果如下表所示：</p>
<ul>
<li><strong>StageACT取得了55%的成功率</strong>，在50次试验（20, 10, 20次分别对应三个方法）中大幅领先。这比最好的基线（ACT，20%）提高了一倍以上。</li>
<li><strong>StageACT的平均任务完成时间最短（20.7秒）</strong>，并且跟踪误差最低，表明其动作更准确、更稳定。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.13200v2/x1.png" alt="性能对比表"></p>
<blockquote>
<p><strong>表I</strong>：不同ACT变体的开门性能。成功率（SR）越高、时间（Time）越低表示性能越好。StageACT在各项指标上均最优。</p>
</blockquote>
<p>分阶段成功率分析（表II）揭示了阶段条件的优势。基线ACT在最具歧义的接近阶段（S2）成功率仅为7/20，常出现模式崩溃：要么双臂同时抬起（混合了接近和推门动作），要么直接从搜索（S1）跳至推门（S4）。而StageACT在S2阶段成功率达到17/19。</p>
<p><img src="https://arxiv.org/html/2509.13200v2/figures/mode_collapse_v2.jpg" alt="模式崩溃示例"></p>
<blockquote>
<p><strong>图5</strong>：成功案例（A）与基线模仿学习策略下两个失败案例（B, C）的对比。(A)中指令动作（红色）紧密跟踪专家轨迹（绿色）。(B)中，左手因未充分抬高而碰撞把手，反映了双臂被同时抬起的轨迹平均问题。(C)中，底座过早启动行走，触发了顺序错误的动作。</p>
</blockquote>
<p>除了提升性能，阶段条件还实现了<strong>行为引导</strong>。如图6所示，当旋转把手（S3）失败时，可以通过阶段提示让策略回到S1阶段进行恢复重试，这是基线策略不具备的能力。作者还测试了<strong>非顺序阶段提示</strong>（如 S1 → S4 → S5），策略能根据提示而非即时观察执行相应动作，展示了可控性。</p>
<p><img src="https://arxiv.org/html/2509.13200v2/x5.png" alt="恢复行为"></p>
<blockquote>
<p><strong>图6</strong>：通过阶段命令引导低层策略，能够在接触密集的把手旋转阶段实现恢复行为。没有阶段引导时，无论门锁是否释放，右臂都会在左臂移动后立即前进。有了阶段条件，这种过早的动作被抑制，使得策略能够重试并完成旋转。</p>
</blockquote>
<p>消融实验（表III）证实了阶段信息的必要性：使用恒定零向量或随机标签作为阶段输入，成功率降至0%；而使用人工标注的真实阶段标签则能保持高性能（60%）。这表明策略确实利用了阶段条件作为有意义的控制信号。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了<strong>StageACT</strong>，一个用于长时程任务的阶段条件模仿学习框架，通过为低层策略提供阶段信息，有效解决了部分可观测性带来的观察歧义和模式崩溃问题；2) 在真实人形机器人上<strong>首次实现了完全基于人类演示、不依赖外部传感或门先验知识的自主开门全身协调策略</strong>，并在未见过的门上达到55%的成功率；3) 展示了阶段条件不仅提升鲁棒性，还支持通过<strong>阶段提示进行行为引导</strong>，实现恢复和非顺序执行等超出演示数据范围的新行为。</p>
<p>论文提到的局限性包括：当前方法专注于推门动作，未来可扩展到拉门；阶段标注依赖于人工，且泛化到更多样化的门类型和配置仍需探索。</p>
<p>这项工作对后续研究的启示在于：<strong>阶段条件是一种轻量级但强大的机制</strong>，能有效提升长时程操作任务的鲁棒性。未来的方向包括：利用视觉语言模型（VLM）实现训练数据的自动阶段标注，以及在测试时作为高层分类器；探索如何参数化阶段以泛化到更广泛的任务和场景；在机器人基础模型背景下，研究如何跨任务类别共享子轨迹和任务片段。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人开门任务中长视野、部分可观测性（如门闩状态不可直接观察）导致的模式崩溃问题，提出StageACT阶段条件模仿学习框架。该方法通过向低级策略注入任务阶段信息，增强对不确定性的鲁棒性。实验显示，在真实办公室环境中，StageACT对未见过的门实现55%的成功率，比最佳基线提高一倍以上，并支持阶段提示实现行为恢复。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.13200" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>