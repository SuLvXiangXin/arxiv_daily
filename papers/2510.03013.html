<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Distributional Inverse Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Distributional Inverse Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.03013" target="_blank" rel="noreferrer">2510.03013</a></span>
        <span>作者: Anqi Wu Team</span>
        <span>日期: 2025-10-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的离线逆强化学习方法，如最大熵逆强化学习及其变体，旨在从专家演示中恢复一个确定性的奖励函数，其核心是匹配专家与学习策略的期望回报。然而，这一假设在许多现实场景中存在关键局限性：奖励信号本身可能是随机的。例如，在涉及可变形或易碎物体的机器人操作中，接触不确定性会导致相同状态-动作对的奖励存在变异性；在神经科学中，驱动动物行为的多巴胺信号也存在显著的试次间差异。这种奖励的随机性直接影响学习策略的鲁棒性和安全性。此外，即使贝叶斯IRL方法能推断奖励参数的后验分布，其优化目标仍是期望回报，未能利用由随机奖励诱导的完整回报分布所提供的更丰富结构信息。因此，仅基于期望回报最大化的方法，其策略仅受均值影响，对奖励的方差或高阶矩不敏感，无法为准确估计完整的奖励分布提供足够信号。</p>
<p>本文针对“如何从专家演示中有效学习奖励的完整分布”这一具体痛点，提出了一个分布式的视角。核心思路是：提出一个分布逆强化学习框架，通过最小化一阶随机占优违规来联合建模奖励函数和回报分布的不确定性，从而捕获专家行为中更丰富的结构，并学习分布感知的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架是一个基于变分推断的对抗训练过程。输入是固定的专家演示数据集，输出是学习到的奖励分布（例如偏态正态分布）和分布感知的策略。框架包含三个核心模块的交替优化：一个用于建模回报分布的分布评论家，一个用于优化特定失真风险度量的策略，以及一个用于建模奖励分布的奖励网络。</p>
<p><img src="https://arxiv.org/html/2510.03013v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>: DistIRL 方法整体框架。展示了训练流程中三个核心模块的交替更新：分布评论家通过分位数回归学习回报分布；策略通过最大化失真风险度量进行更新；奖励网络通过最小化一阶随机占优（FSD）损失和KL正则化进行更新。</p>
</blockquote>
<p><strong>核心模块1：通过随机占优学习奖励分布</strong>。传统MaxEntIRL的目标是匹配期望回报（均值占优）。为了学习完整的奖励分布，本文提出了一个分布式的目标，即最小化智能体回报分布 $Z^{\pi}$ 相对于专家回报分布 $Z^E$ 的一阶随机占优违规。具体损失函数为 $\mathcal{L}<em>{\text{FSD}} = \int</em>{-\infty}^{\infty} [F_{Z^{\pi}}(z) - F_{Z^{E}}(z)]<em>{+} dz$，其中 $F$ 为累积分布函数。该积分可通过变量变换转化为在分位数空间的计算：$\int</em>{0}^{1} [F_{Z^{\pi}}^{-1}(v) - F_{Z^{E}}^{-1}(v)]<em>{+} dv$。奖励分布 $q</em>{\phi}(r|s,a)$ 采用参数化模型（如偏态正态分布），并通过变分推断进行学习，其最终优化目标 $\mathcal{L}_{r}(\phi)$ 包含两项：FSD损失的期望以及变分分布与先验分布 $p_0(r)$ 之间的KL散度作为正则项 $\psi(r)$。</p>
<p><strong>核心模块2：风险感知策略学习</strong>。在奖励分布固定的情况下，策略 $\pi_{\varphi}$ 的原始优化目标是最大化 $\int_{0}^{1} [F_{Z^{\pi_{\varphi}}}^{-1}(v) - F_{Z^{E}}^{-1}(v)]<em>{+} dv$ 加上策略熵 $\mathcal{H}(\pi</em>{\varphi})$。这等价于在FSD被违反的分位数区域（即智能体回报分位数低于专家回报分位数的区域）提升智能体的回报。由于指示函数 $\mathcal{I}(v)$ 难以直接优化，本文提出用一个失真风险度量 来替代。失真风险度量 $M_{\xi}(Z^{\pi}) = \int_{0}^{1} F_{Z^{\pi}}^{-1}(v) d\tilde{\xi}(v)$ 通过一个非递减的失真函数 $\xi$（或其对偶 $\tilde{\xi}$）对回报分位数进行加权，从而实现对分布不同区域的强调。例如，CVaR风险度量对应一个凹的 $\tilde{\xi}$，会赋予较低回报（分布左尾）更大的权重，从而产生风险厌恶的策略。最终，策略的优化目标变为 $\max_{\varphi} M_{\xi}(Z^{\pi_{\varphi}}) + \mathcal{H}(\pi_{\varphi})$。理论分析表明，若对所有的失真函数 $\xi$ 都能优化此目标，则可实现一阶随机占优；在实践中，使用一个特定的DRM（如CVaR）作为可处理的近似。</p>
<p><strong>核心模块3：分布评论家与算法流程</strong>。为了计算FSD损失和策略的DRM目标，需要估计回报分布 $Z^{\pi}$ 和 $Z^E$。本文采用分位数回归方法，使用一个评论家网络输出N个分位数 $\theta_i(s, a)$ 来近似回报分布，并通过分位数Huber损失进行训练。算法1展示了整体训练流程：在每次迭代中，从专家数据中采样批次，为每个状态采样动作和奖励，计算回报样本；然后依次更新分布评论家（分位数回归损失）、策略（DRM目标+熵）、奖励网络（FSD损失+KL正则化）。</p>
<p><strong>创新点</strong>：与现有方法相比，DistIRL的核心创新在于首次在离线IRL中原则性地学习<strong>奖励的完整分布</strong>，而不仅仅是其期望或参数不确定性。这是通过将匹配准则从“均值占优”推广到“一阶随机占优”实现的。同时，它将失真风险度量自然地集成到策略学习中，使学到的策略是<strong>分布感知</strong>的。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了三类基准：1) <strong>合成数据</strong>：用于验证奖励分布恢复能力；2) <strong>真实世界神经行为数据</strong>（小鼠在迷宫中的决策序列），首次在IRL研究中应用；3) <strong>高维连续控制任务</strong>：MuJoCo环境（HalfCheetah, Hopper, Walker2d, Ant）的离线模仿学习。</p>
<p>对比的基线方法包括：确定性奖励的离线IRL方法（MaxEnt IRL, IQ-Learn, SMODICE），贝叶斯IRL方法（BIRL，作为分布估计的对比），以及模仿学习方法（行为克隆BC）。</p>
<p><strong>关键实验结果</strong>：<br>在合成数据上，DistIRL能够准确恢复出双峰、偏态等复杂形状的奖励分布，而BIRL等方法只能给出围绕均值的不确定性区间，无法捕获分布形态。</p>
<p><img src="https://arxiv.org/html/2510.03013v2/x3.png" alt="奖励分布恢复"></p>
<blockquote>
<p><strong>图3</strong>: 在合成数据上恢复奖励分布。DistIRL（右）能够准确恢复真实的双峰奖励分布，而BIRL（中）的后验预测区间无法捕捉分布的双峰形态。</p>
</blockquote>
<p>在神经行为数据分析中，DistIRL学习到的奖励分布方差与小鼠决策的波动性表现出相关性，为理解行为变异性提供了新视角。</p>
<p><img src="https://arxiv.org/html/2510.03013v2/x4.png" alt="神经行为数据应用"></p>
<blockquote>
<p><strong>图4</strong>: 在真实小鼠迷宫决策数据上的应用。学习到的奖励分布标准差与小鼠选择转向的波动性（Shannon熵）呈正相关，表明DistIRL捕获了与行为变异性相关的奖励不确定性。</p>
</blockquote>
<p>在MuJoCo离线模仿任务中，DistIRL在大多数环境和数据设置下取得了最优或接近最优的性能。</p>
<p><img src="https://arxiv.org/html/2510.03013v2/x5.png" alt="MuJoCo性能对比"></p>
<blockquote>
<p><strong>图5</strong>: 在MuJoCo HalfCheetah环境上的归一化得分对比。DistIRL在专家和中等质量数据集上均表现优异。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.03013v2/x6.png" alt="多任务性能汇总"></p>
<blockquote>
<p><strong>图6</strong>: 四个MuJoCo任务在专家数据集上的性能汇总。DistIRL在大多数任务中领先。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>研究验证了框架中各组件的重要性。使用确定性奖励（Det.）或移除风险度量（即仅优化期望回报，W/O DRM）都会导致性能显著下降，这证明了建模奖励分布和使用DRM进行策略学习的必要性。</p>
<p><img src="https://arxiv.org/html/2510.03013v2/x9.png" alt="消融实验"></p>
<blockquote>
<p><strong>图9</strong>: 消融研究。使用确定性奖励（Det.）或移除失真风险度量（W/O DRM）都会损害性能，证明了分布奖励和风险感知策略学习的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.03013v2/x10.png" alt="不同风险偏好"></p>
<blockquote>
<p><strong>图10</strong>: 不同失真风险度量（CVaRα）的影响。不同的α值（风险厌恶程度）会带来不同的策略行为，展示了方法在产生不同风险偏好策略上的灵活性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个用于<strong>离线逆强化学习中奖励分布学习</strong>的原则性框架，通过一阶随机占优准则匹配完整回报分布；2) 通过集成失真风险度量，实现了<strong>分布感知的风险敏感策略学习</strong>；3) 在合成数据、真实神经行为数据和机器人控制任务上进行了实证验证，证明了其在恢复有意义的奖励分布和取得先进模仿性能方面的有效性。</p>
<p>论文提到的局限性主要在于计算方面：基于采样估计分位数和FSD损失可能在高维问题上带来计算开销；同时，方法性能可能受到所选失真风险度量（如CVaR的α参数）的影响。</p>
<p>这项工作对后续研究的启示是多方面的。它为在奖励信号本身具有随机性或不确定性的场景（如机器人交互、神经科学、金融决策）中应用IRL开辟了新方向。未来工作可以探索更高效的分布估计算法，研究如何自动选择或学习适合任务的失真风险度量，以及将框架扩展到在线学习或部分可观测环境。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出分布逆强化学习方法，解决离线环境下传统IRL仅能恢复确定性奖励估计、无法建模随机奖励分布的问题。关键技术是建立联合建模奖励函数不确定性与回报完整分布的框架，通过最小化一阶随机占优违规，将失真风险度量整合至策略学习，从而同时恢复奖励分布与分布感知策略。实验表明，该方法在合成基准、真实神经行为数据及MuJoCo控制任务中恢复了高表现力的奖励表示，并取得了最先进的模仿性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.03013" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>