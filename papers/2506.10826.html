<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RationalVLA: A Rational Vision-Language-Action Model with Dual System - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RationalVLA: A Rational Vision-Language-Action Model with Dual System</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.10826" target="_blank" rel="noreferrer">2506.10826</a></span>
        <span>作者: Song, Wenxuan, Chen, Jiayi, Li, Wenxue, He, Xu, Zhao, Han, Cui, Can, Su, Pengxiang Ding Shiyan, Tang, Feilong, Cheng, Xuelian, Wang, Donglin, Ge, Zongyuan, Zheng, Xinhu, Liu, Zhe, Wang, Hesheng, Li, Haoang</span>
        <span>日期: 2025/06/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人领域，为实现遵循自然语言指令的操控，主流方法是利用预训练视觉语言模型（VLM）的泛化能力，在机器人数据集上微调，得到视觉-语言-动作（VLA）模型。然而，现有工作通常假设指令与环境完美对齐，这限制了其在现实场景中的鲁棒性和泛化能力。现实场景中，指令可能存在模糊、不相关或不可行（即有缺陷）的情况。现有VLA模型在微调后常出现语言能力的灾难性遗忘，且其操控性能往往不如任务专用的策略；而将VLM与专用策略简单组合的架构，又无法将复杂的语言理解有效传递给底层策略。</p>
<p>本文针对机器人需要处理<strong>有缺陷指令</strong>（Defective Instructions）这一具体痛点，提出了一个<strong>双系统</strong>（Dual System）的新视角。核心思路是设计一个端到端训练的双系统VLA模型，通过引入可学习的潜在空间嵌入作为高层多模态大语言模型（MLLM）与底层操控策略之间的接口，使模型既能继承MLLM的语言理解和推理能力以拒绝缺陷指令，又能保留专用策略的精准操控技能以执行可行指令。</p>
<h2 id="方法详解">方法详解</h2>
<p>RationalVLA的整体框架是一个双系统模型，集成了一个高层MLLM和一个底层机器人策略。输入为视觉观察（RGB图像）和可能带有缺陷的文本指令。高层MLLM负责多模态理解和推理，输出文本描述以及一个特殊的 <code>&lt;ACT&gt;</code>（执行）或 <code>&lt;REJ&gt;</code>（拒绝）令牌。该令牌最后一层的隐藏层嵌入被投影后，作为潜在目标条件传递给底层策略。底层策略根据当前观测、本体感知和该潜在条件，输出具体的机器人动作。两个系统可异步运行，MLLM低频更新推理结果，策略高频根据最新环境状态和潜在条件生成动作。</p>
<p><img src="https://arxiv.org/html/2506.10826v2/extracted/6538894/figure/robogsva.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图6</strong>：RationalVLA 概述。给定视觉观察和可能包含缺陷的文本指令，MLLM生成任务文本描述以及一个<code>&lt;ACT&gt;</code>或<code>&lt;REJ&gt;</code>令牌。<code>&lt;ACT&gt;</code>令牌的嵌入作为高级潜在目标传递给下游策略，<code>&lt;REJ&gt;</code>令牌的嵌入则用于拒绝当前场景中的缺陷指令。该双系统模型可以异步运行高层推理和底层策略执行循环。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li>**高层MLLM (<code>f_ϕ</code>)**：由一个视觉编码器、一个投影器和一个大语言模型（LLM）组成。它在词汇表中引入了两个新的可学习特殊令牌：<code>&lt;ACT&gt;</code> 和 <code>&lt;REJ&gt;</code>。模型根据指令在当前场景下是否可执行，自回归地生成相应的令牌。<code>&lt;ACT&gt;</code>令牌的隐藏嵌入 <code>y_&lt;ACT&gt;</code> 承载了融合视觉-语言信息的动作潜在表示。</li>
<li><strong>接口与投影</strong>：一个轻量级的投影器 <code>Ψ</code> 将 <code>y_&lt;ACT&gt;</code> 映射到与底层策略条件空间对齐的潜在嵌入 <code>z</code>。对于 <code>&lt;REJ&gt;</code> 令牌，其投影结果被设为零张量 <code>z_0</code>，以停止动作生成。</li>
<li>**底层策略 (<code>π_θ</code>)**：采用 3D Diffuser Actor 作为预训练的底层策略。它以当前时间步的视觉观察 <code>o</code>、本体感知 <code>p</code> 以及从高层传来的潜在条件 <code>z</code> 作为输入，输出当前的动作 <code>a_t</code>。策略本身不直接处理原始语言指令，而是通过潜在条件 <code>z</code> 来理解高层意图。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：</p>
<ul>
<li><strong>双系统与潜在接口</strong>：不同于直接微调VLA（导致遗忘）或简单串联VLM与策略（语言理解传递不畅），RationalVLA通过可学习的 <code>&lt;ACT&gt;</code>/<code>&lt;REJ&gt;</code> 令牌嵌入作为接口，实现了高层语言推理与底层动作执行在潜在空间上的对齐与高效通信。</li>
<li><strong>缺陷指令的显式处理</strong>：通过引入 <code>&lt;REJ&gt;</code> 令牌及相应的训练数据，模型被明确训练具备拒绝六类缺陷指令的能力，这是对传统VLA任务设定的重要扩展。</li>
<li><strong>最小化干预</strong>：仅增加两个令牌到词汇表，最大程度保留了预训练MLLM原有的语言和视觉-语言能力，有助于缓解灾难性遗忘。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.10826v2/extracted/6538894/figure/illustration.jpg" alt="架构对比"></p>
<blockquote>
<p><strong>图5</strong>：处理RAMA的不同方法对比。<strong>上</strong>：直接微调VLA模型。<strong>中</strong>：使用预训练VLM作为高层标识器来判断指令，再将可执行指令传递给底层策略。<strong>下</strong>：本文提出的双系统VLA，通过动作潜在嵌入结合高层VLM和底层策略，继承了VLM的语言理解能力并无损地将潜在动作表示传递给策略。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>基准与数据集</strong>：实验主要在提出的 <strong>RAMA</strong> 基准上进行，该基准基于 <strong>CALVIN</strong> 仿真环境构建。CALVIN包含四个场景（A、B、C、D），使用ABC场景训练，D场景测试（ABC→D）。RAMA数据集包含 <strong>14,412</strong> 条语言指令，其中 <strong>14,253</strong> 条用于训练，<strong>159</strong> 条用于测试。缺陷指令涵盖视觉、物理、语义、运动、安全和上下文无关六个维度。</p>
<p><img src="https://arxiv.org/html/2506.10826v2/extracted/6538894/figure/dataset.jpg" alt="数据集统计"></p>
<blockquote>
<p><strong>图2</strong>：RAMA基准评估模型的六个缺陷维度：视觉、物理、语义、运动、安全和上下文无关。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.10826v2/extracted/6538894/figure/smalltable.jpg" alt="数据统计表"></p>
<blockquote>
<p><strong>表1</strong>：RAMA基准在六个维度上的数据统计细节。</p>
</blockquote>
<p><strong>对比方法</strong>：对比了多种基线方法，包括：专用于CALVIN的操控策略 <strong>3DDA</strong>；微调VLA模型 <strong>RoboFlamingo</strong>；将 <strong>GPT-4o</strong> 作为高层标识器与 <strong>3DDA</strong> 策略结合的 <strong>GPT-4o (ID)<strong>；以及其他双系统VLA模型如 <strong>LCB</strong>、</strong>HiRT</strong>、<strong>DP-VLA</strong> 和 <strong>OpenHelix</strong>。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>RAMA基准性能</strong>：评估序列以一个缺陷指令开始，后续为CALVIN的长时序多任务语言条件任务。RationalVLA在<strong>最终任务成功率</strong>上达到 **54.5%**，比最佳基线（GPT-4o (ID)）高出 <strong>14.5%<strong>。在</strong>平均任务长度</strong>（衡量序列中连续成功完成的任务数）上达到 <strong>4.94</strong>，比最佳基线高出 <strong>0.94</strong>。这证明了其处理缺陷指令和复杂未见指令的综合优势。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.10826v2/extracted/6538894/figure/vis.jpg" alt="定量结果"></p>
<blockquote>
<p><strong>图7</strong>：在RAMA基准（ABC→D）上的定量结果。RationalVLA在最终任务成功率和平均任务长度上均显著优于所有基线方法。</p>
</blockquote>
<ol start="2">
<li><strong>标准CALVIN任务性能</strong>：在原始CALVIN基准测试中（不包含缺陷指令），RationalVLA取得了与专用策略3DDA（92.8%）相当的成功率（90.5%），并且显著优于直接微调的VLA模型RoboFlamingo（76.3%），表明其设计并未牺牲标准操控性能。</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>模型架构</strong>：将RationalVLA的双系统设计与直接微调MLLM输出动作（“VLA w/o policy”）对比，后者性能大幅下降，证明了底层策略的必要性。</li>
<li><strong>训练数据</strong>：在训练中移除缺陷指令（“w/o defective instr.”）会严重降低模型在RAMA上的拒绝能力和整体成功率，凸显了缺陷指令数据的重要性。</li>
<li><strong>潜在条件</strong>：将潜在条件 <code>z</code> 替换为指令的文本嵌入（“w/ text embedding”）会导致性能下降，证明了学习到的 <code>&lt;ACT&gt;</code> 令牌嵌入比原始文本包含更丰富的、与策略对齐的语义信息。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.10826v2/extracted/6538894/figure/real_deploy.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融研究结果。验证了双系统架构、缺陷指令数据以及学习到的潜在嵌入（相对于文本嵌入）各自的重要贡献。</p>
</blockquote>
<ol start="4">
<li><strong>真实世界验证</strong>：论文展示了RationalVLA在真实机器人手臂上的部署结果，能够成功处理可行指令（如“打开抽屉”）并拒绝各种缺陷指令（如要求移动不存在的物体），验证了其在实际应用中的有效性和鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.10826v2/extracted/6538894/figure/real.png" alt="真实实验"></p>
<blockquote>
<p><strong>图9</strong>：真实世界实验。RationalVLA成功执行了“打开抽屉”的可行指令，并正确拒绝了“将苹果移到盘子里”（场景中无苹果）、“滑动粉色方块”（方块为红色）等缺陷指令。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li><strong>提出RAMA基准与数据集</strong>：首次在机器人操控领域系统性地引入并定义了六类“有缺陷指令”，构建了包含超过1.4万个样本的数据集，使评估更贴近现实复杂场景。</li>
<li><strong>设计RationalVLA双系统模型</strong>：创新性地使用可学习的 <code>&lt;ACT&gt;</code> 和 <code>&lt;REJ&gt;</code> 令牌嵌入作为接口，将高性能MLLM与专用操控策略在潜在空间对齐，实现了语言理解、推理与精准动作执行的高效统一，并能明确拒绝缺陷指令。</li>
<li><strong>全面的实验验证</strong>：在仿真RAMA基准、标准CALVIN任务以及真实机器人实验上均证明了方法的优越性、鲁棒性和实用性。</li>
</ol>
<p>论文提到的局限性主要在于其性能依赖于所选用的预训练MLLM和底层策略的质量，且双系统架构可能带来一定的计算成本。</p>
<p>这项工作对后续研究的启示在于：为处理现实世界中不完美、有噪声的指令提供了新的基准和框架思路；展示了通过精心设计的轻量级接口（如特殊令牌）来融合大模型与领域专用模型的有效性；未来可探索更高效的双系统协同机制，并将此范式扩展到更复杂的任务场景和机器人形态中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RationalVLA，一种双系统视觉-语言-动作模型，旨在解决机器人执行自然语言指令时，对模糊、无关或不可行指令缺乏鲁棒性的问题。该模型通过可学习的潜在空间嵌入，将高层视觉语言模型与底层操作策略集成，实现对指令的推理、拒绝不可行命令并执行有效操作。在包含逾1.4万样本的RAMA基准测试中，RationalVLA相比基线方法成功率提升14.5%，平均任务长度达0.94，且在标准操作任务中保持竞争力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.10826" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>