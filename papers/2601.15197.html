<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.15197" target="_blank" rel="noreferrer">2601.15197</a></span>
        <span>作者: Kai Chen Team</span>
        <span>日期: 2026-01-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过在大规模人类演示数据上训练，学习将自然语言指令映射为机器人动作的策略，在分布内任务上展现出潜力。然而，主流方法在泛化到新指令或复杂多任务场景，尤其是在分布外（OOD）环境中时，面临显著挑战。论文指出，这一脆弱性的根源在于当前机器人数据集普遍存在的“目标驱动”收集偏差。在此类数据集中，仅从视觉观察就能高度确定性地预测语言指令（例如，看到柜子总是对应“打开柜子”），导致动作与指令之间的条件互信息（CMI）消失，作者称之为“信息坍缩”。其后果是，模型退化为纯视觉策略，忽略了语言约束，在遇到模糊场景或OOD环境时失败。</p>
<p>本文针对VLA模型因数据集偏差而忽视语言指令这一具体痛点，提出了一个新的贝叶斯分解视角。核心思路是：通过引入可学习的潜在动作查询构建双分支架构，分别估计视觉先验和语言条件后验策略，并最大化动作与指令之间的点互信息（PMI），从而惩罚视觉捷径，强制模型依据语言指令生成动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>LangForce的整体框架是一个共享VLM权重的双分支架构，旨在同时建模视觉先验 p(a|v) 和完整的语言条件策略 π(a|v,ℓ)。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：LangForce框架。采用共享VLM权重的双分支架构。先验分支（左）处理<code>[v, Q, ℓ]</code>，利用因果掩码学习视觉先验p(a|v)。后验分支（右）处理<code>[v, ℓ, Q]</code>，学习完整策略π(a|v,ℓ)。潜在动作查询Q作为瓶颈接口，LLR目标鼓励模型最大化动作与指令间的信息。</p>
</blockquote>
<p>核心模块是<strong>潜在动作查询</strong>。作者在VLM词表中扩展了K=64个可学习的令牌<code>Q = {&lt;|action_1|&gt;, ..., &lt;|action_K|&gt;}</code>及其对应的嵌入向量。这些查询作为VLM（如Qwen3-VL）与连续动作头（一个扩散变换器DiT）之间的专用瓶颈。关键设计在于，仅将这些查询令牌的隐藏状态<code>H_Q</code>馈送给动作专家，而非所有输入令牌的隐藏状态。这结合解码器VLM固有的因果注意力掩码，使得通过简单改变<code>Q</code>在输入序列中的位置，就能精确控制编码到<code>H_Q</code>中的信息。</p>
<p><strong>双分支训练策略</strong>具体如下：</p>
<ol>
<li><strong>先验分支（视觉先验）</strong>：输入序列构造为<code>[v, Q, ℓ]</code>。由于因果掩码，<code>Q</code>只能关注视觉<code>v</code>而无法看到其后的语言<code>ℓ</code>。因此，<code>H_Q^{prior}</code>编码了纯视觉信息。使用这些特征预测动作，通过流匹配损失<code>L_prior</code>学习数据集中固有的动作偏差。优化<code>L_prior</code>时，<code>H_Q^{prior}</code>的梯度被截断，确保视觉先验的更新仅限于DiT动作头，防止VLM主干学习视觉捷径。</li>
<li><strong>后验分支（完整策略）</strong>：输入序列构造为<code>[v, ℓ, Q]</code>。此时<code>Q</code>可以关注<code>v</code>和<code>ℓ</code>。<code>H_Q^{post}</code>编码了完整上下文。通过流匹配损失<code>L_main</code>学习专家动作。</li>
</ol>
<p><strong>优化目标</strong>是最大化动作与指令的条件点互信息（PMI），即最大化后验策略与先验策略的对数似然比（LLR）。具体地，利用VLM的语言建模损失作为<code>log p(ℓ|...)</code>的代理，定义LLR损失为：<code>L_LLR = log p(ℓ | v, H_Q^{prior}) - sg(log p(ℓ | v))</code>，其中<code>sg(·)</code>表示停止梯度操作。最大化此项（即最小化<code>-L_LLR</code>）强制动作表征<code>H_Q</code>携带能解释指令<code>ℓ</code>的信息。停止梯度操作是为了防止模型通过破坏基线<code>p(ℓ|v)</code>（即损害VLM的通用语言能力）来最大化比值。</p>
<p><strong>总训练目标</strong>结合了动作预测损失和LLR正则化项：<code>L_total = (1-λ)L_FM(ψ; H_Q^{post}) + λL_FM(ψ; H_Q^{prior}) - βL_LLR</code>。实验中设置λ=0.3，β=0.1。在推理时，仅使用后验分支，因此相比标准VLA基线没有额外的计算开销。</p>
<p>与现有方法相比，创新点主要体现在：1）明确识别并形式化了由数据集偏差引起的“信息坍缩”问题；2）提出了通过潜在动作查询实现贝叶斯分解的双分支架构；3）设计了最大化LLR的训练目标，直接惩罚视觉捷径，鼓励模型利用语言信息。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个模拟基准上进行：SimplerEnv、LIBERO和RoboCasa。训练基于StarVLA框架，使用8张NVIDIA H100 GPU，遵循默认协议。对比的基线方法广泛，包括RT-1-X、Octo系列、OpenVLA、π0、π0.5、Isaac-GR00T以及基于相同骨干网络的不同变体（QwenGR00T、QwenOFT等）。</p>
<p><strong>在SimplerEnv上的结果</strong>：模型在BridgeDataV2和Fractal数据集上微调，评估四个操作任务。</p>
<p><img src="https://..." alt="结果表格"></p>
<blockquote>
<p><strong>表1</strong>：SimplerEnv基准上的成功率（%）。LangForce取得了66.5%的平均成功率，超越了所有对比基线，相比其直接基线QwenGR00T（55.2%）有11.3%的绝对提升。在“Put Carrot on Plate”和“Put Eggplant in Yellow Basket”等需要精确识别的任务上提升显著。</p>
</blockquote>
<p><strong>在LIBERO上的结果</strong>：LIBERO包含Spatial, Object, Goal, Long四个子集。LangForce在具有视觉模糊性的Goal子集上取得了99.4%的成功率，优于基线Qwen3-VL-GR00T的97.4%（+2.0%）。</p>
<p><img src="https://..." alt="结果表格"></p>
<blockquote>
<p><strong>表2</strong>：LIBERO基准上的成功率（%）。LangForce在四个子集上均表现优异，平均成功率98.4%。特别是在Goal子集上，其性能显著优于纯视觉基线（9.8%），证明了其解决模糊性的能力。</p>
</blockquote>
<p><img src="https://..." alt="视觉捷径示例"></p>
<blockquote>
<p>**图2(a)**：LIBERO Goal中同一视觉场景对应多个任务的示例（如“将碗放入抽屉”或“将碗放在炉子上”）。这揭示了纯视觉模型无法解决的模糊性。</p>
</blockquote>
<p>为了定量分析，作者计算了LIBERO Goal数据集上条件熵<code>H(ℓ|v)</code>的代理指标——给定视觉观察时真实指令的负对数似然（NLL）和困惑度（PPL）。LangForce的NLL（9.47 nats/token）和PPL（12964.9）显著高于基线QwenGR00T（8.51 nats/token, 4964.1），表明其防止了条件熵的坍缩，保留了必要的任务不确定性，从而迫使策略利用语言指令来消歧。</p>
<p><strong>在RoboCasa上的结果</strong>：评估了24个桌面操作任务。</p>
<p><img src="https://..." alt="结果表格"></p>
<blockquote>
<p><strong>表4</strong>：RoboCasa基准上的成功率（%）。LangForce取得了52.6%的平均成功率，达到SOTA，超越了包括其基线QwenGR00T（47.8%）在内的所有方法。同时，其性能也显著高于纯视觉基线（44.7%），例如在“PnP Novel From Placemat To Bowl”任务上达到62.0%，远超纯视觉的32.0%。</p>
</blockquote>
<p><img src="https://..." alt="视觉捷径现象"></p>
<blockquote>
<p><strong>图1</strong>：RoboCasa中视觉捷径的示例。训练数据视觉多样但任务有限，导致模型学习基于特定视觉线索而非语言指令来执行任务。图(b)显示，即使在推理时没有语言指令，模型也能执行“PnPCanToDrawerClose”任务。</p>
</blockquote>
<p>此外，消融实验（通过纯视觉基线体现）验证了各组件贡献：纯视觉模型在分布内（如RoboCasa）或非模糊数据集上能达到与完整模型相近的性能甚至损失（见图2(b)），但在模糊（LIBERO Goal）或OOD（SimplerEnv）场景下性能暴跌，这反向证明了引入语言条件化和LLR目标对于打破视觉捷径、实现泛化的必要性。</p>
<p><img src="https://..." alt="动作损失曲线"></p>
<blockquote>
<p>**图2(b)**：在BridgeDataV2和Fractal数据集上的动作损失曲线。纯视觉模型能达到与完整语言条件模型相当的低损失，表明即使在多样化的数据集中也存在视觉捷径，模型通过过拟合特定视觉模式来最小化训练目标。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1）识别并实证验证了当前VLA训练中因数据集偏差导致的“视觉捷径”病理，即模型倾向于忽略语言指令；2）提出了LangForce框架，通过潜在动作查询和双分支贝叶斯目标，从有偏数据中恢复出真正的语言条件策略；3）在多个基准测试上证明了LangForce的有效性，特别是在OOD泛化上取得了显著提升（如SimplerEnv上11.3%的改进）。</p>
<p>论文自身提及的局限性包括其为“进行中的工作”，且所有实验均在模拟环境中进行。对后续研究的启示在于：为缓解数据偏差问题，除了收集更平衡的数据外，从模型架构和训练目标层面进行干预（如本文的贝叶斯分解和信息最大化）是一条有效路径。潜在动作查询作为一种灵活的瓶颈设计，为连接VLM与下游控制模块提供了新思路。最大化动作与指令的互信息这一原则，可能泛化至其他需要多模态对齐的任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型在泛化至新指令或多任务场景时性能下降的问题，指出其根源在于目标驱动数据集导致“信息塌缩”，即语言指令可从视觉观察中完全预测，使得模型退化为忽略语言的纯视觉策略。为解决此问题，作者提出LangForce框架，通过贝叶斯分解强制模型遵循指令。其核心方法是引入可学习的潜在动作查询，构建双分支架构分别估计视觉先验p(a|v)与语言条件后验π(a|v,ℓ)，并通过最大化动作与指令间的条件点互信息来优化策略，从而惩罚视觉捷径。实验表明，该方法无需新数据，即在OOD基准上显著提升泛化能力，如在SimplerEnv上性能提升11.3%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.15197" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>