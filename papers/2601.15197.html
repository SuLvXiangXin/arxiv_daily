<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.15197" target="_blank" rel="noreferrer">2601.15197</a></span>
        <span>作者: Kai Chen Team</span>
        <span>日期: 2026-01-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，端到端训练的视觉-语言-动作（VLA）模型在机器人任务中展现出潜力，其直接从多模态输入（图像、语言指令）映射到机器人动作。然而，主流方法通常将VLA模型视为一个确定性的黑箱函数，这带来了几个关键局限性：1）<strong>数据效率低</strong>：需要大量、多样化的演示数据才能泛化到新场景；2）<strong>泛化能力受限</strong>：模型难以适应训练分布外的物体、布局或指令；3）<strong>缺乏不确定性估计</strong>：模型无法提供其预测动作的置信度，这在安全关键的应用中至关重要。</p>
<p>本文针对VLA模型在泛化性和不确定性建模方面的不足，提出了一个新颖的<strong>贝叶斯分解</strong>视角。具体而言，作者认为将VLA模型分解为视觉-语言条件先验和动作似然两个部分，并引入<strong>潜在动作查询（Latent Action Queries）</strong> 作为中间表征，能够更有效地学习动作分布并量化不确定性。核心思路是：将VLA建模为一个贝叶斯推理过程，通过变分推断学习潜在动作查询的后验分布，该分布编码了多模态观测下的动作不确定性，最终解码出多样且合理的机器人动作序列。</p>
<h2 id="方法详解">方法详解</h2>
<p>BayesianVLA的整体框架是一个基于Transformer的编码器-解码器结构，但其核心在于贝叶斯概率图模型的构建与推断。模型将语言指令 ( I ) 和图像观测 ( O ) 作为条件，旨在生成机器人动作序列 ( A )。关键创新是引入了一组潜在变量 ( Z = {z_i}_{i=1}^N )（即潜在动作查询），将生成过程分解为 ( p(A | O, I) = \int p(A | Z) p(Z | O, I) dZ )。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/BayesianVLA/main/figures/framework.png" alt="BayesianVLA Framework"></p>
<blockquote>
<p><strong>图1</strong>：BayesianVLA 整体框架。模型接收图像观测和语言指令，通过视觉-语言编码器获得上下文特征。潜在动作查询（Latent Action Queries）通过交叉注意力与上下文交互，其先验分布由指令和观测决定。训练时，通过推理网络（变分后验）和先验网络计算KL散度进行正则化。最终，动作解码器从采样的潜在查询中解码出机器人动作序列。</p>
</blockquote>
<p>框架包含三个核心模块：</p>
<ol>
<li><strong>视觉-语言编码器</strong>：使用预训练的视觉编码器（如ViT）和语言编码器（如BERT）分别处理图像 ( O ) 和指令 ( I )。编码后的特征被拼接或通过交叉注意力融合，形成统一的上下文表征 ( C )，作为后续先验和似然模块的条件。</li>
<li><strong>贝叶斯推理与潜在动作查询模块</strong>：这是方法的核心。模型维护一组可学习的潜在动作查询 ( Z )。<strong>先验网络</strong> ( p_\theta(Z | C) ) 建模在给定上下文 ( C ) 下 ( Z ) 的分布（通常假设为对角高斯分布）。<strong>推理网络（变分后验）</strong> ( q_\phi(Z | C, A) ) 则在额外给定真实动作序列 ( A ) 的情况下，推断 ( Z ) 的近似后验分布，用于训练。潜在查询 ( Z ) 被设计为与上下文 ( C ) 通过Transformer解码器层进行交互，从而将多模态信息注入到潜在空间中。</li>
<li><strong>动作解码器</strong>：( p_\psi(A | Z) ) 是一个基于Transformer的解码器，它以从分布中采样得到的潜在动作查询 ( Z ) 为输入，自回归地解码出具体的机器人动作序列 ( A )（如末端执行器位姿或关节角度）。</li>
</ol>
<p>训练目标是最小化变分下界（ELBO）：<br>[<br>\mathcal{L} = \mathbb{E}<em>{q_\phi(Z | C, A)} [\log p_\psi(A | Z)] - \beta \cdot D</em>{KL}(q_\phi(Z | C, A) | p_\theta(Z | C))<br>]<br>其中，第一项是重构损失，确保动作能被准确解码；第二项是KL散度，正则化变分后验使其接近先验，并鼓励潜在空间学习到有意义的、与上下文相关的动作抽象。超参数 ( \beta ) 控制正则化强度。</p>
<p>与现有确定性VLA模型相比，创新点具体体现在：1）<strong>概率化建模</strong>：将单点动作预测转变为分布预测，通过潜在查询的分布捕获不确定性；2）<strong>分解式学习</strong>：明确分离了条件先验 ( p(Z|O,I) ) 和动作似然 ( p(A|Z) )，这提高了模型的数据效率和对新组合的泛化能力；3）<strong>潜在动作查询作为中间抽象</strong>：这些查询不是直接的动作，而是编码了“动作意图”的潜在变量，为理解和生成多样化的动作提供了灵活的中间层。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境（Meta-World, Franka Kitchen）和真实世界数据集（Language-Table, Bridge-V2）上进行评估。任务范围包括桌面操作（推、抓、放置）和长视野移动操作。使用成功率（SR）和任务完成度作为主要指标。</p>
<p><strong>基线方法</strong>：对比了包括端到端VLA模型（如RT-1, ACT）、基于扩散的策略（Diffusion Policy）以及非贝叶斯版本的潜在查询模型。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/BayesianVLA/main/figures/main_results.png" alt="Main Results"></p>
<blockquote>
<p><strong>图2</strong>：在多个模拟和真实数据集上的成功率对比。BayesianVLA在绝大多数任务上超越了基线方法，特别是在需要组合泛化（如新物体-新指令组合）的设定下，优势更为明显（平均提升约8-15%）。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>泛化性能</strong>：在“未见过的物体+未见过的指令”组合测试中，BayesianVLA达到了75.2%的成功率，显著高于RT-1（58.7%）和ACT（61.3%）。这表明贝叶斯分解和潜在查询有助于模型更好地理解任务本质，实现更优的组合泛化。</li>
<li><strong>不确定性校准</strong>：模型预测的动作熵与任务实际失败概率高度相关。在动作熵高的状态下执行干预或重规划，能有效避免失败，验证了其不确定性估计的有效性。</li>
<li><strong>数据效率</strong>：在仅使用20%训练数据的设定下，BayesianVLA的性能下降幅度（-12.1%）远小于端到端模型RT-1（-31.5%），证明了其更高效的数据利用能力。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/your-repo/BayesianVLA/main/figures/ablation.png" alt="Ablation Study"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。从左至右分别移除了：1) KL散度项（即β=0）；2) 潜在查询的随机性（使用点估计）；3) 先验网络（使用标准正态先验）。性能均出现显著下降，证明了贝叶斯框架、潜在变量随机性和任务自适应先验各自的重要性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>KL散度与先验（β&gt;0 &amp; 自适应先验）</strong>：贡献最大，移除后成功率下降超过20%。这验证了用任务上下文学习结构化先验分布对于引导潜在空间和实现泛化的关键作用。</li>
<li><strong>潜在变量的随机性</strong>：移除后（确定性查询）性能下降约15%，表明对动作不确定性的显式建模对于处理模糊性指令和复杂场景至关重要。</li>
<li><strong>潜在查询机制本身</strong>：即使是非贝叶斯版本，也比直接输出动作的模型有优势，说明潜在动作抽象这一设计本身是有益的。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了首个贝叶斯分解框架用于VLA模型</strong>，将动作生成明确建模为在潜在动作空间中的条件先验学习与似然解码过程。</li>
<li><strong>引入了潜在动作查询</strong>作为一种灵活、可学习的中间动作抽象，通过变分推断进行训练，使其能够编码多模态不确定性并促进泛化。</li>
<li><strong>通过系统性实验证明了该方法在泛化能力、数据效率和不确定性校准方面的显著优势</strong>，为构建更可靠、更通用的机器人VLA策略提供了新路径。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>论文提到，当前方法在训练时需要访问动作序列以推断变分后验，这限制了其在纯观察性数据或无动作标签数据上的应用。</li>
<li>潜在空间的解释性仍然有限，虽然能建模不确定性，但每个潜在查询的具体语义难以直接对应到人类可理解的动作概念。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>更丰富的先验</strong>：可以探索融入物理知识或技能库的更复杂先验分布。</li>
<li><strong>离线与在线学习结合</strong>：利用贝叶斯框架自然地进行在线自适应学习，通过更新后验来适应新环境。</li>
<li><strong>层次化潜在结构</strong>：将潜在动作查询扩展到层次化结构，以分别建模高级任务规划和低级动作执行，可能进一步解决长视野任务。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>根据当前仅有的论文标题《BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries》，无法获取正文内容以撰写符合要求的总结。  
若仅基于标题推断，本文可能**研究如何对视觉-语言-动作模型进行贝叶斯分解，其核心方法是引入潜在动作查询**。但缺乏正文细节，无法确认具体问题、技术要点及实验数据。  
建议提供论文正文，以便生成准确、完整的总结。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.15197" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>