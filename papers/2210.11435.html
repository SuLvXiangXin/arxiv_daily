<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning and Retrieval from Prior Data for Skill-based Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Learning and Retrieval from Prior Data for Skill-based Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2210.11435" target="_blank" rel="noreferrer">2210.11435</a></span>
        <span>作者: Nasiriany, Soroush, Gao, Tian, Mandlekar, Ajay, Zhu, Yuke</span>
        <span>日期: 2022/10/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习为机器人学习通用行为提供了有希望的途径，但传统方法存在可扩展性有限的问题，主要受限于高昂的数据监督需求和脆弱的泛化能力。当前，尽管行为克隆等方法在多样化操作任务上取得了不错的表现，但它们通常被局限于相对小规模的领域。对于真实世界问题，现有算法往往需要大量任务演示，而这些演示难以获取且成本高昂。近期一系列工作开始探索利用先前的交互数据来提高新任务上模仿学习的样本效率，这些先验数据形式多样，包括与任务无关的探索性“玩耍”数据或为不同任务收集的演示数据。然而，一个悬而未决的关键问题是如何最好地从这些大规模先验数据集中提取知识，并利用这些知识来促进学习新任务。</p>
<p>基于技能的模仿学习是解决上述问题的一种有前景的方法，其目标是从先验数据中学习短视距子轨迹的潜在空间（技能学习），随后学习一个策略来调用这些技能以解决特定的下游任务（策略学习）。这种方法具有优势：策略受益于技能封装的时间抽象，使其能专注于更高层次的“做什么”的推理；同时，通过将目标任务与先验数据训练的技能联系起来进行推理，机器人能够隐式地将先验数据中丰富多样的交互知识提炼到策略中。尽管如此，现有的基于技能的模仿学习方法相对于简单的行为克隆方法带来的改进有限。本文旨在识别现有方法的根本局限性，并设计新方法来解决这些局限性。</p>
<p>本文认为，一个实用的方法应包含两个关键属性：第一，潜在技能空间应作为下游策略学习的<strong>可预测</strong>表示，使策略能准确推断在新情况下使用哪种合适的技能；第二，机器人应利用先验数据来学习技能和策略。现有基于技能的方法主要关注使用先验数据进行技能学习，而非策略学习，导致在小规模目标任务演示上学习的策略容易严重过拟合和出现协变量偏移。针对这些局限性，本文提出了<strong>SAILOR</strong>方法，其核心思路是：通过引入辅助的时序可预测性目标来塑造技能表示，并通过基于检索的数据增强机制，选择性地检索与目标任务相关的先验数据来增强策略学习的监督。</p>
<h2 id="方法详解">方法详解</h2>
<p>SAILOR方法包含两个阶段：任务无关的技能学习阶段和任务特定的策略学习阶段。整体框架如下图所示。</p>
<p><img src="https://via.placeholder.com/700x300.png?text=Figure+1+Overview" alt="方法概述"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。首先，在先前数据上学习一个潜在技能模型，通过目标确保技能表示可预测。给定目标任务演示，使用该潜在空间从先前数据中检索相似行为，以扩展对策略的监督。然后训练一个输出潜在技能的策略。</p>
</blockquote>
<p>方法的具体模型架构与流程如下图所示。</p>
<p><img src="https://via.placeholder.com/700x400.png?text=Figure+2+Model+Overview" alt="模型概述"></p>
<blockquote>
<p><strong>图2</strong>：模型概述。我们的方法包括技能学习和策略学习两个阶段。（左）在技能学习阶段，我们通过变分自编码器学习子轨迹的潜在技能表示，并加入额外的时序可预测性项以学习更一致的潜在表示。（右）在策略学习阶段，我们训练策略根据子轨迹前的观察历史来预测潜在技能。执行策略时，使用技能解码器对预测的潜在变量进行解码。</p>
</blockquote>
<p><strong>1. 学习可预测的技能表示</strong><br>技能表示学习采用变分自编码器（VAE）。给定一个子轨迹 τ，使用LSTM编码器 q_φ 将其编码为潜在技能上的高斯分布。解码器是一个LSTM网络 p_ψ，在每一步解码潜在变量 z 和给定观察 o_t 以重建动作 â_t。此外，采用一个学习到的先验 p_θ，鼓励具有相似起始和结束观察的子轨迹拥有相似的潜在表示。VAE损失目标为：<br>L_VAE(φ,ψ,θ) = -E_{z∼q_φ(z|τ)} [ Σ_{t=0}^{H-1} log p_ψ(a_t|z,o_t) ] + β · D_KL( q_φ(z|τ) || p_θ(z|o_0, o_H) )<br>其中 β 控制KL散度项的影响。</p>
<p>为了学习一个一致且可预测的行为表示，本文引入了<strong>时序可预测性（Temporal Predictability）</strong> 损失项。给定来自同一条轨迹、相隔 t 个时间步的两个子轨迹 τ1 和 τ2，学习一个模型 m_ω 来根据对应的技能均值嵌入预测 t：<br>L_TP(ω,φ) = ( m_ω( μ(q_φ(z|τ1)), μ(q_φ(z|τ2)) ) - t )^2<br>其中 μ 表示取分布的均值。通过技能编码器模型反向传播 L_TP，使得该项能够塑造学习到的技能表示。总体技能学习目标是VAE损失和时序可预测性损失的加权组合：L_Skill(φ,ψ,θ,ω) = L_VAE(φ,ψ,θ) + α L_TP(ω,φ)。</p>
<p><strong>2. 基于检索的策略学习</strong><br>在策略学习阶段，使用一个LSTM策略，输出接下来要执行的技能 z。策略在数据集 D_policy = { (o_fs^i, z^i = μ(q_φ(τ^i)) ) } 上训练，其中 τ^i 是长度为 H 的子轨迹，z^i 是该子轨迹的均值编码，o_fs^i 是子轨迹前 F 个观察的帧堆叠历史。使用标准的行为克隆损失训练策略从 o_fs^i 预测 z^i。执行时，以闭环方式展开LSTM技能解码器 p_ψ。</p>
<p>为了增加监督信号，本文引入了<strong>基于检索的数据增强机制</strong>。核心思想是检索 D_prior 中与 D_target 中的子轨迹具有高相似性的子轨迹，相似性度量基于学习到的技能嵌入空间。具体步骤为：</p>
<ol>
<li>随机采样 D_prior 和 D_target 中的子轨迹，获取其技能嵌入：Z_prior = {μ(q_φ(τ^i))}, τ^i ∼ D_prior；Z_target = {μ(q_φ(τ^j))}, τ^j ∼ D_target。</li>
<li>计算先验和目标数据集技能嵌入之间的两两 L2 距离矩阵 D[i][j] = ‖Z_prior^i - Z_target^j‖_2。</li>
<li>对于每个先验数据集技能嵌入 z^i ∈ Z_prior，找到最接近的对应目标数据集技能距离 D_min[i] = min(D[i][:])。</li>
<li>检索 D_prior 中具有最小距离的前 n 个子轨迹，形成检索数据集 D_ret。<br>策略在 D_ret 和 D_target 的聚合子轨迹集上训练，使用行为克隆损失，并为 D_ret 的损失项加权因子 γ 以控制检索数据相对于目标数据集的影响。在训练策略的同时，还在 D_target 上对技能模型进行微调。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个模拟机器人操作领域进行：<strong>Franka Kitchen</strong> 和 <strong>CALVIN</strong>。具体任务设置如下图所示。</p>
<p><img src="https://via.placeholder.com/700x300.png?text=Figure+3+Simulated+Tasks" alt="模拟任务"></p>
<blockquote>
<p><strong>图3</strong>：模拟任务设置。（左）Franka Kitchen：目标任务涉及四个子任务的特定排列顺序；考虑两个先验数据集：包含所有子任务的演示，以及不包含与微波炉交互子任务的演示。（右）CALVIN：采用 Mees 等人的玩耍数据集作为先验数据；评估两个目标任务：布置游戏室环境和清理环境。</p>
</blockquote>
<p><strong>Baseline方法</strong>：对比了六种基线方法，包括：BC-RNN（标准行为克隆）、BC-RNN (FT)（在先验数据上预训练，在目标数据上微调）、BC-RNN (R3M)（使用R3M预训练视觉编码器）、IQL（离线强化学习）、IQL (UDS)（使用统一数据调度器混合先验和目标数据）、FIST（一种基于技能的模仿学习方法）。</p>
<p><strong>关键定量结果</strong>如下表所示。SAILOR在所有任务上均显著优于基线方法。</p>
<p><img src="https://via.placeholder.com/700x200.png?text=Table+1+Quantitative+evaluation" alt="定量评估结果"></p>
<blockquote>
<p><strong>表1</strong>：在两个模拟领域的定量评估。报告了超过三个随机种子的平均任务成功率和标准差（BC-RNN (FT) 因方差高使用了六个种子）。SAILOR 在所有任务上显著优于基线。</p>
</blockquote>
<p><strong>消融研究</strong>结果如下图所示，探讨了先验数据、时序可预测性（TP）目标和基于检索的数据增强（Retrieval）各自的作用。</p>
<p><img src="https://via.placeholder.com/500x300.png?text=Figure+4+Ablation+Study" alt="消融研究"></p>
<blockquote>
<p><strong>图4</strong>：消融研究。在 CALVIN-Setting Up 任务上，移除任一组件都会导致性能下降，验证了先验数据、时序可预测性目标和基于检索的数据增强各自的重要性。</p>
</blockquote>
<p>消融实验表明：</p>
<ol>
<li><strong>先验数据</strong>：不使用任何先验数据（仅用目标数据训练技能和策略）性能最差，凸显了利用先验知识的重要性。</li>
<li><strong>时序可预测性目标</strong>：移除该目标（α=0）会导致性能显著下降，验证了学习可预测技能表示对下游策略学习的必要性。</li>
<li><strong>基于检索的数据增强</strong>：用随机检索或使用全部先验数据（无检索）替代，性能均不及完整的SAILOR，说明选择性检索相关数据对于有效利用先验数据、避免冲突行为干扰至关重要。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了SAILOR，一个基于技能模仿学习的框架，通过从先验数据中学习可预测的技能表示并利用基于检索的数据增强来高效学习新任务。</li>
<li>引入了时序可预测性目标，以学习更一致、对下游策略学习更友好的技能潜在表示。</li>
<li>设计了一种新颖的检索机制，能够选择性地从先验数据中提取与目标任务相关的行为，显著增强了策略学习的监督信号并提高了数据效率。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于离线数据集的质量和多样性。此外，固定的技能长度可能并不适合所有类型的任务。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>技能表示学习</strong>：探索除时序可预测性之外的其他辅助目标（如时间对比学习）来进一步改善技能空间的结构。</li>
<li><strong>框架扩展性</strong>：SAILOR的技能学习管道可以与其他多任务模仿学习方法（如语言条件或任务条件方法）互补，未来可考虑融入语言或任务ID等监督信号。</li>
<li><strong>数据利用</strong>：该方法展示了如何通过智能检索而非简单混合来有效利用异构先验数据集，为其他利用离线数据的领域提供了借鉴。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对传统模仿学习数据需求高、泛化能力弱的问题，提出利用先前任务数据高效学习新任务的技能框架。核心方法包括：1）从先验数据中学习可预测的时间扩展技能表示；2）通过检索机制进行数据增强以提升策略训练。在模拟和真实机器人操作任务上的实验表明，该方法显著优于现有模仿学习与离线强化学习方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2210.11435" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>