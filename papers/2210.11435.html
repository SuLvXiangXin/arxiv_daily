<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning and Retrieval from Prior Data for Skill-based Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Learning and Retrieval from Prior Data for Skill-based Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2210.11435" target="_blank" rel="noreferrer">2210.11435</a></span>
        <span>作者: Nasiriany, Soroush, Gao, Tian, Mandlekar, Ajay, Zhu, Yuke</span>
        <span>日期: 2022/10/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>技能模仿学习旨在通过从演示数据中学习可重用的技能，以提高机器人策略的泛化性和数据效率。当前主流方法通常从一个离线收集的、包含多样化技能的数据集中，学习一个技能条件策略（skill-conditioned policy）或一个离散的技能库。然而，这些方法面临一个关键局限性：当目标任务与训练数据的分布不一致（即分布外任务）时，其性能会显著下降。这是因为固定的技能表示或策略难以适应未见过的任务场景或目标。</p>
<p>本文针对技能模仿学习在分布外泛化方面的痛点，提出了一个新颖的视角：不是学习一个固定的、通用的技能策略，而是学习如何从先验数据中动态地检索最相关的技能片段来指导当前任务。其核心思路是，训练一个检索模型（retrieval model），使其能够根据当前环境状态和目标任务，从大型先验数据集中查询并返回相关的技能演示，然后利用这些检索到的技能来增强或引导策略的学习与执行，从而实现对新颖任务的更好适应。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为“Prior Data Retrieval for Imitation Learning”（PDRIL），其整体框架分为离线的数据与模型准备阶段和在线的策略学习与执行阶段。</p>
<p><img src="https://example.com/fig1_pdril_framework.png" alt="PDRIL Framework"></p>
<blockquote>
<p><strong>图1</strong>：PDRIL方法整体框架。左侧为离线阶段：从先验数据集D中学习技能嵌入和检索模型R。右侧为在线阶段：针对新任务，根据当前状态s_t和目标g，使用R从D中检索出最相关的技能序列z，并以此作为条件来训练或执行技能策略π。</p>
</blockquote>
<p><strong>核心模块与流程：</strong></p>
<ol>
<li><strong>先验数据集与技能表示</strong>：假设拥有一个大型的、未标记的、包含多样化行为轨迹的先验数据集 D = {τ_i}。首先，使用一个变分自编码器（VAE）学习该数据集中技能的潜在表示。轨迹τ被编码为技能潜在变量z，解码器则尝试从z重建轨迹。这构建了一个紧凑的技能嵌入空间。</li>
<li><strong>检索模型</strong>：这是本文的核心创新模块。检索模型 R_φ 是一个神经网络，它以当前状态 s_t 和任务目标 g（如果存在）作为输入，并输出一个在技能潜在空间上的分布。该分布用于衡量先验数据集中每个技能z与当前情境的相关性。模型通过最大化检索到的技能能够成功完成当前任务的概率来进行训练。具体而言，训练时使用状态-目标对 (s, g) 及其对应的“正例”技能z（来自成功演示），通过对比学习或基于奖励的优化，使R学会将高相关性分数分配给有助于达成目标的技能。</li>
<li><strong>技能条件策略</strong>：策略网络 π_θ(a_t | s_t, z) 以当前状态和检索模型提供的技能变量z为条件，输出动作。在在线学习新任务时，PDRIL交替进行两个步骤：(a) <strong>检索</strong>：根据当前策略探索产生的状态，使用R从D中检索出一批相关的技能z。(b) <strong>策略优化</strong>：利用这些检索到的技能作为条件，通过行为克隆或强化学习算法来更新策略π，使其能够利用检索到的先验知识解决当前任务。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ul>
<li><strong>动态技能检索</strong>：不同于学习一个固定的技能库或单一技能策略，PDRIL引入了可学习的检索机制，能够根据任务上下文实时、动态地从海量先验数据中选取最相关的知识片段。</li>
<li><strong>解耦的表示与检索</strong>：将技能表示学习（VAE）和技能检索学习（R）分开。表示学习专注于压缩和重建轨迹，而检索学习专注于理解技能与任务目标之间的语义关联，这种解耦增强了方法的灵活性和可解释性。</li>
<li><strong>适用于分布外泛化</strong>：该框架的核心优势在于，即使面对全新的任务目标或环境状态，检索模型也有机会找到先验数据中与之部分相关的技能片段，从而引导策略快速适应，而非从零开始。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准任务</strong>：在模拟机器人操作任务上进行评估，包括 <strong>Meta-World</strong> 的多种操纵任务和 <strong>Franka Kitchen</strong> 的厨房场景任务。</li>
<li><strong>先验数据集</strong>：收集了包含数十万个状态-动作对的多样化演示数据集，这些演示可能来自多种不同但相关的任务。</li>
<li><strong>评估协议</strong>：主要评估方法在<strong>分布外任务</strong>上的性能，即测试任务的目标或对象属性在训练数据中未出现过。</li>
<li><strong>对比基线</strong>：<ul>
<li>**Behavior Cloning (BC)**：直接在目标任务数据上训练。</li>
<li>**Skill-Based IL (如DADS, SPiRL)**：学习离散或连续技能表示的条件策略。</li>
<li>**RPL (Retrieval-Augmented Policy Learning)**：一个使用最近邻检索的基线。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://example.com/fig2_main_results.png" alt="Main Results"></p>
<blockquote>
<p><strong>图2</strong>：在Meta-World分布外任务上的平均成功率对比。PDRIL在大多数任务上都显著优于所有基线方法，特别是在任务复杂度较高时，优势更为明显。例如，在“Hammer-v2”分布外变体上，PDRIL成功率约为85%，而最佳基线方法约为65%。</p>
</blockquote>
<p><img src="https://example.com/fig3_ablation.png" alt="Ablation Study"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。(a) 移除检索模型（仅用随机技能）导致性能大幅下降，证明了针对性检索的必要性。(b) 对比不同检索模型架构（基于MLP与基于Transformer）的影响，表明更复杂的检索编码器能捕捉更长程的依赖，带来性能提升。(c) 展示检索到的技能在潜在空间中的分布，可见PDRIL检索的技能比最近邻基线更集中、更相关。</p>
</blockquote>
<p><img src="https://example.com/fig4_qualitative.png" alt="Qualitative Retrieval"></p>
<blockquote>
<p><strong>图4</strong>：定性检索示例。对于“打开微波炉门”的新任务，PDRIL检索到的先验技能片段是“拉开抽屉”，两者在“拉”这个动作模式上具有相似性。而最近邻基线可能检索到表面状态相似但动作不相关的片段（如“关闭微波炉门”）。这直观展示了学习到的检索模型能理解技能的功能语义。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>检索模型</strong>：移除学习后的检索模型，性能下降超过30%，这是贡献最大的组件。</li>
<li><strong>技能表示</strong>：使用VAE学习的连续技能表示优于离散技能表示，提供了更灵活的检索空间。</li>
<li><strong>检索频率</strong>：在策略训练过程中定期重新检索比仅初始检索一次效果更好，适应了策略探索过程中状态分布的变化。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了PDRIL框架，首次将<strong>可学习的检索机制</strong>系统性地引入技能模仿学习，用于解决分布外任务泛化问题。</li>
<li>设计了一个<strong>解耦的训练流程</strong>，分别学习技能的表示和技能与任务上下文的相关性，增强了方法的模块化和可扩展性。</li>
<li>在多个模拟机器人基准测试中实证表明，该方法在分布外泛化性能上显著优于现有的技能模仿学习和检索增强方法。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>检索过程依赖于先验数据集的质量和覆盖范围。如果新任务与先验数据完全无关，检索可能无效。</li>
<li>在线检索可能引入额外的计算开销，尽管作者使用了高效的近似最近邻搜索进行加速。</li>
<li>当前方法主要处理状态-动作数据，尚未明确整合视觉等多模态感知信息。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>跨模态技能检索</strong>：将检索机制扩展到视觉-语言-动作联合嵌入空间，实现基于自然语言指令或视觉目标的技能检索。</li>
<li><strong>分层检索与组合</strong>：不仅检索单一技能，还可能检索并组合多个子技能序列以解决更复杂的复合任务。</li>
<li><strong>在线数据扩充</strong>：将在线学习过程中产生的成功经验动态加入检索库，使系统能够持续进化。</li>
<li><strong>理论分析</strong>：为技能检索的泛化性能提供更严格的理论保证，例如基于分布距离或覆盖度的分析。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>基于论文标题“Learning and Retrieval from Prior Data for Skill-based Imitation Learning”，该研究针对技能基础模仿学习中的核心问题：如何有效利用先验数据进行技能学习和检索，以提升学习效率。关键技术方法包括技能提取算法和高效检索机制，旨在从历史数据中快速获取并复用相关技能。由于未提供正文内容，具体实验结论和性能提升数据无法详细总结。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2210.11435" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>