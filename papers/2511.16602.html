<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16602" target="_blank" rel="noreferrer">2511.16602</a></span>
        <span>作者: Zhang, Yi, Liu, Che, Ren, Xiancong, Ni, Hanchu, Zhang, Yingji, Zhang, Shuai, Ding, Zeyuan, Hu, Jiayu, Shan, Haozhe, Qi, Junbo, Bai, Yan, Li, Dengjie, Luo, Jiachen, Wang, Yidong, Dai, Yong, Xu, Zenglin, Shen, Bin, Wang, Qifan, Tang, Jian, Ju, Xiaozhu</span>
        <span>日期: 2025/11/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前构建通用具身智能系统主要面临两大挑战：关键的具身数据瓶颈（真实世界数据稀缺且昂贵）和现有算法的低效性（资源消耗巨大）。主流方法可分为两类：一是异质数据规模化，通过融合网络、仿真和真实世界轨迹构建庞大的“数据金字塔”；二是面向控制的架构精炼，改进高自由度机器人的连续控制。然而，这两种策略都未从根本上解决“具身瓶颈”。它们本质上是离线模仿学习范式的延伸，仅被动扩大数据规模或控制精度，并未提升真正的数据效率或实现元认知的自我改进。关键局限在于，缺乏持续适应机制：模型以一次性、离线方式训练，无法在真实世界部署中基于稀疏、有针对性的反馈自主识别弱点或精炼技能。</p>
<p>本文针对数据稀缺和算法低效这两个具体痛点，提出了一个名为“刻意练习策略优化”（DPPO）的新视角，其核心思路是模仿人类的元认知学习过程，设计一个动态交替进行强化学习（弱点发现）和监督微调（弱点精炼）的“元循环”训练框架，使模型能够自动识别自身弱点并高效分配学习资源，从而在有限数据下最大化学习效率。</p>
<h2 id="方法详解">方法详解</h2>
<p>DPPO是一个元认知的“元循环”训练框架，其核心是动态交替执行RL（用于弱点发现）和SFT（用于弱点精炼）两个阶段，形成一个持续的自我诊断与自我精炼循环。</p>
<p><img src="https://arxiv.org/html/2511.16602v1/1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DPPO框架总览。该框架实现了一个迭代的RL-SFT元循环，利用轨迹记录和难度感知采样进行动态数据整理。这个自适应过程在RL阶段揭示弱点，在SFT阶段精炼弱点，形成一个持续的自我诊断与自我精炼循环。</p>
</blockquote>
<p><strong>整体流程</strong>：如算法1所示，DPPO以初始模型参数开始，进行K次迭代。每次迭代包含两个阶段：1) <strong>RL弱点检测阶段</strong>：当前策略进行轨迹采样，通过难度感知采样构建训练数据集，使用GRPO目标进行策略更新，并基于停滞分数自动终止；同时收集完全失败的样本（SR=0）作为弱点数据。2) <strong>SFT弱点精炼阶段</strong>：利用RL阶段收集的弱点数据、从数据集中检索的相关具身数据以及用于防止遗忘的通用数据，共同构建SFT数据集，并通过标准的负对数似然损失进行监督微调。最后清空缓冲区，开始下一轮循环。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>RL弱点发现模块</strong>：此阶段将RL重新定义为探索性诊断过程。采用GRPO框架，其策略梯度如公式3所示。关键设计包括：</p>
<ul>
<li><strong>多模态多任务奖励</strong>：构建了一个基于规则的奖励函数，涵盖六个核心目标：可供性推理、计数与距离估计、因果与时间推理、任务成功评估、任务规划、任务预测。此外，还包含一个<strong>格式奖励</strong>，用于约束模型输出结构（需包含逐步推理链和最终答案）。总奖励为格式奖励与任务特定奖励的加权和（公式4）。</li>
<li><strong>难度感知采样</strong>：在RL训练前，通过公式5计算每个样本的成功率分数SR。随后进行数据重平衡：剔除SR=1（已掌握）的样本，并限制SR=0（完全失败）样本的数量不超过部分成功样本（0&lt;SR&lt;1）的总数，以构建更稳定的训练集。</li>
<li><strong>自适应停止准则</strong>：在RL优化过程中，通过公式6和7计算每个任务的停滞分数SS。当SS ≥ 0.7时，自动终止当前RL阶段，防止无效计算和过拟合，并将SR=0的样本传递给SFT阶段。</li>
</ul>
</li>
<li><p><strong>SFT弱点精炼模块</strong>：此阶段旨在将RL探索到的洞察转化为强化和泛化的能力。其数据集由三部分动态构成：𝒟_SFT = 𝒟_weak ∪ 𝒟_rel ∪ 𝒟_gen。其中𝒟_weak来自RL阶段识别的困难样本，𝒟_rel是根据弱点维度从数据集中检索的相关具身样本，𝒟_gen是用于回放以防止遗忘的通用数据。随后使用标准的SFT损失（公式1）进行训练，以巩固技能。</p>
</li>
</ol>
<p><strong>理论创新</strong>：论文从理论上将SFT和RL统一到一个连贯的偏好学习框架下（公式8）。SFT对应于优化单一专家轨迹的似然，是一种稳定但被动的知识注入；RL（GRPO）则对应于优化排序轨迹对的偏好，能够从对比中学习以纠正细微缺陷。DPPO的元循环巧妙地协同利用了这两种机制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>模型与数据</strong>：训练了名为Pelican-VL的视觉语言模型（基于Qwen2.5-VL），规模包括7B和72B。为7B模型策划了总计约394K实例的数据集（SFT 200K， RL 194K），涵盖空间感知、时间推理等四个基本能力领域。</li>
<li><strong>训练过程</strong>：进行了3个元循环，每个循环包含一个RL阶段和一个SFT阶段。采用渐进式时间覆盖策略：第一循环限制视频片段短于32秒，第二循环放松至64秒，以鼓励策略从短时任务泛化到更长期、组合性的行为。</li>
<li><strong>评估基准</strong>：使用了多个具身与通用基准，包括MVBench（通用）、EgoSchema、RefSpatialBench、VSI-Bench、Where2Place和COSMOS。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>性能持续提升</strong>：如图2所示，72B模型在五个具身基准上的性能随着每个训练循环而逐步提高。在通用基准MVBench上性能保持稳定，表明没有发生灾难性遗忘。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16602v1/a_MVBench.png" alt="性能演化"></p>
<blockquote>
<p><strong>图2</strong>：Pelican-VL 72B在不同训练阶段的性能演化。模型在具身基准上持续改进，同时在通用数据集MVBench上保持稳定结果（发现1）。</p>
</blockquote>
<ol start="2">
<li><strong>RL阶段数据分布变化</strong>：图3展示了7B模型在RL训练期间，各基准相关任务上“可回答”与“不可回答”问题数量的分布变化。随着训练进行，模型完全无法回答的问题数量逐渐减少（例如VSI-Bench-MCQ中从800降至500），表明弱点被有效识别和针对。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16602v1/b_RefSpatial-Bench.png" alt="数据分布变化"></p>
<blockquote>
<p><strong>图3</strong>：Pelican-VL 7B模型在RL训练期间，训练数据相对于不同基准的分布变化。颜色渐深的线条表示RL训练的进程，可见未学会的任务稳步减少，成功解决的任务相应增加（发现2）。</p>
</blockquote>
<ol start="3">
<li><strong>遗忘对比</strong>：如图4所示，在7B模型上对比SFT、RL和DPPO。DPPO在目标具身基准（VSI-Bench）上获得了最大的性能增益（+54.3），同时在未见过的通用基准（如MMStar）上性能下降最小（仅1.9），远低于SFT（5.0）和RL（24.8）的下降幅度。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16602v1/f_COSMOS.png" alt="遗忘对比"></p>
<blockquote>
<p><strong>图4</strong>：SFT、RL和DPPO在7B模型上的性能增益（VSI-Bench）与遗忘（通用基准）对比。DPPO获得了显著性能提升，同时在未见数据集上的性能下降非常有限（发现3）。</p>
</blockquote>
<ol start="4">
<li><strong>与基线方法对比</strong>：如表1所示，在相同数据预算下，DPPO（51.0平均分）显著优于单独使用RL（40.7）或SFT（39.9），在多个数据集上展现出更强的整体性能和泛化能力。</li>
<li><strong>表征轨迹可视化</strong>：图5通过t-SNE可视化了7B模型在不同基准上的轨迹嵌入质心在DPPO元循环中的演化。不同任务将模型推向不同的表征方向，这证明了单一训练阶段不足，从而支持了多阶段DPPO设计的动机。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16602v1/combined_stage_cluster_7b_dppo.png" alt="表征轨迹"></p>
<blockquote>
<p><strong>图5</strong>：Pelican-VL 7B模型的轨迹嵌入质心在DPPO元循环中的分布变化（t-SNE可视化）。不同基准产生不同的演化轨迹，说明单一训练阶段不足，需多阶段DPPO设计（发现5）。</p>
</blockquote>
<p><strong>最终性能</strong>：使用DPPO训练的72B模型（Pelican-VL 1.0）相比其基础模型性能提升了20.3%，并超越了开源100B参数规模的模型10.6%。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出DPPO框架</strong>：首次将元认知原理计算化，通过一个动态交替RL（弱点发现）与SFT（弱点精炼）的自适应元循环，实现了从被动数据积累到主动能力扩展的范式转变。</li>
<li><strong>验证训练高效性</strong>：DPPO作为一个智能数据引擎，能将稀疏数据转化为高价值信号。实验表明其能带来显著性能提升（72B模型+20.3%），并有效缓解灾难性遗忘。</li>
<li><strong>推动开源社区</strong>：开源了Pelican-VL 1.0模型（7B–72B）及完整的DPPO pipeline，为社区高效构建通用具身智能体提供了首个系统性框架。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，DPPO的元循环设计涉及交替训练，可能带来较高的计算成本。此外，其泛化能力仍需在更广泛、更复杂的真实世界任务中进行验证。</p>
<p><strong>研究启示</strong>：DPPO为突破具身智能的数据瓶颈提供了一条新路径，即不单纯追求数据量的扩大，而是通过算法设计提升数据利用的“质效”。其元认知、自我诊断与精炼的思想可启发更通用的持续学习框架。未来的工作可探索更高效的弱点评估指标、将循环扩展至在线真实世界交互，以及将该框架应用于其他数据稀缺的序列决策领域。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身智能系统面临的数据稀缺与算法效率低下两大核心瓶颈，提出**Deliberate Practice Policy Optimization (DPPO)** 方法。该框架引入元认知“Metaloop”训练机制，通过动态交替进行**监督微调（能力拓展）** 与**强化学习（技能精炼）**，实现自动弱点识别与针对性资源分配，旨在从有限数据中最大化学习效率。实验表明，基于DPPO训练的视觉语言具身模型Pelican-VL 1.0，性能比基础模型提升**20.3%**，并超越100B参数规模的开源模型**10.6%**。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16602" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>