<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.16909" target="_blank" rel="noreferrer">2512.16909</a></span>
        <span>作者: Ju, Yuanchen, Liang, Yongyuan, Wang, Yen-Jen, Gireesh, Nandiraju, Ju, Yuanliang, Lee, Seungjae, Gu, Qiao, Hsieh, Elvis, Huang, Furong, Sreenath, Koushil</span>
        <span>日期: 2025/12/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在家庭环境中工作的移动机械臂需要同时具备导航和操作能力，这要求一种紧凑、语义丰富的场景表示，能够捕获物体的位置、功能以及可操作部件。场景图是一种自然的选择，它将场景中的物体及其关系组织成图结构。然而，现有方法存在关键局限：1）通常将空间关系和功能关系分离，导致表示不完整且可操作性差；2）大多将场景视为静态快照，缺乏对物体状态或时间更新的建模；3）忽略了与完成当前任务最相关的信息，缺乏任务相关性。本文针对这些痛点，提出了一种为具身智能体设计的、能够整合空间与功能关系并引入部件级交互节点的统一场景图表示新视角。其核心思路是采用“先建图，再规划”的策略，即首先根据任务指令和视觉观察生成一个任务特定的、结构化的场景图作为中间表示，然后基于此图进行高层任务规划，从而显著提升规划的准确性和鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>MomaGraph方法旨在为具身智能体构建一个任务导向的、状态感知的统一场景图。给定一个室内房间的多视角图像集 ${\mathcal{I}<em>{i}}</em>{i=1}^{n}$ 和一个自然语言任务指令 $\mathcal{T}$，目标是构建一个指令条件化的任务场景图 $\mathcal{G}<em>{\mathcal{T}}=(\mathcal{N}</em>{\mathcal{T}},\mathcal{E}<em>{s}^{\mathcal{T}},\mathcal{E}</em>{f}^{\mathcal{T}})$。其中，$\mathcal{N}<em>{\mathcal{T}}$ 表示与任务相关的物体（及部件）节点集合，$\mathcal{E}</em>{s}^{\mathcal{T}}$ 编码节点间的空间关系，$\mathcal{E}_{f}^{\mathcal{T}}$ 捕获节点间的功能关系。</p>
<p><img src="https://arxiv.org/html/2512.16909v2/Figures/logo.png" alt="方法总览图"></p>
<blockquote>
<p><strong>图1</strong>：MomaGraph方法总览。给定任务指令，MomaGraph构建一个任务特定的场景图，突出显示相关物体、部件及其空间-功能关系，使机器人能够进行空间理解和任务规划。</p>
</blockquote>
<p>方法的核心是通过强化学习训练一个视觉语言模型来生成这种场景图，并支持动态更新。具体流程如下：</p>
<ol>
<li><p><strong>场景图生成与强化学习训练</strong>：现有的开源VLM直接从多视角观察和指令生成准确的任务场景图能力有限。为此，本文基于Qwen2.5-VL-7B-Instruct模型，使用DAPO强化学习算法在自建的MomaGraph-Scenes数据集上进行训练，得到模型MomaGraph-R1。其关键在于设计了一个基于图的奖励函数 $\mathcal{R}(\mathcal{G}<em>{\mathcal{T}}^{\text{pred}},\mathcal{G}</em>{\mathcal{T}}^{\text{gt}})$，该函数从三个方面评估预测图与真实图的匹配程度：</p>
<ul>
<li><strong>动作类型预测</strong> ($R_{\text{action}}$)：确保模型能根据指令 $\mathcal{T}$ 正确预测所需的动作类型。</li>
<li><strong>边的关系整合</strong> ($R_{\text{edges}}$)：联合评估预测图中每条边的空间关系和功能关系与真实图的语义相似度。</li>
<li><strong>节点完整性</strong> ($R_{\text{nodes}}$)：计算预测与真实任务相关节点集合的交并比相似度。<br>最终奖励还结合了格式验证 ($R_{\text{format}}$) 和长度控制 ($R_{\text{length}}$)。该奖励设计直接体现了本文的核心思想：场景图必须同时捕获空间布局和功能关系，并与任务要求紧密耦合。</li>
</ul>
</li>
<li><p><strong>状态感知的动态场景图更新</strong>：在真实环境中，同类物体可能多个并存，其任务对应关系初始时可能不确定（例如，灶台有多个旋钮，但只有一个控制当前任务所需的炉头）。MomaGraph通过捕捉并整合观察到的环境状态变化来解决这种歧义。在时间步 $t$，场景图为 $\mathcal{G}<em>{\mathcal{T}}^{(t)}$，其中功能关系 $\mathcal{E}</em>{f}^{\mathcal{T},(t)}$ 可能包含一对多的假设映射。当智能体执行动作 $a_t$ 并观察到新状态 $s_{t+1}$ 后，通过更新函数 $\mathcal{U}(\cdot)$ 对图进行细化：$\mathcal{G}<em>{\mathcal{T}}^{(t+1)} = \mathcal{U}(\mathcal{G}</em>{\mathcal{T}}^{(t)}, a_t, s_{t+1})$。该函数根据观察到的状态转移，移除不一致的假设并加强已确认的对应关系。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.16909v2/Figures/Failure.png" alt="动态更新示例"></p>
<blockquote>
<p><strong>图3</strong>：MomaGraph捕捉环境中的状态变化并相应地动态更新任务特定场景图，使图能够随着交互的发生而演化，并反映更新后的空间-功能关系。</p>
</blockquote>
<p>与现有方法相比，MomaGraph的创新点具体体现在：1) 首次统一建模空间与功能关系，并引入部件级节点；2) 提出用强化学习与专门设计的图奖励来训练VLM生成高质量的任务场景图；3) 设计了状态感知的动态更新机制，使场景图能随交互演化；4) 整体上实现了“先建图，再规划”的范式，用一个模型联合完成图构建与规划。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文构建了新的数据集MomaGraph-Scenes和评估基准MomaGraph-Bench，并在此之上进行了系统实验。</p>
<ul>
<li><strong>数据集</strong>：MomaGraph-Scenes包含约1050个任务导向子图和6278张多视角RGB图像，覆盖超过350个不同的家庭场景和93个独特任务指令。它联合编码了9种空间关系类型和6种功能关系类型，并显式标注了手柄、按钮等交互部件。</li>
<li><strong>评估基准</strong>：MomaGraph-Bench被构建为一个多选VQA任务，包含294个室内场景、1446张多视角图像和352个任务场景图。它系统评估六种核心推理能力：动作序列推理、空间推理、物体可供性推理、前提与效果推理、目标分解和视觉对应性。任务难度分为四个层级。</li>
<li><strong>对比方法</strong>：对比了顶尖的闭源模型（Claude-4.5-Sonnet, GPT-5, Gemini-2.5-Pro）和领先的开源模型（InstructBLIP, LLaVA-V1.5, DeepSeek-VL2, InternVL2.5, LLaVA-OneVision, Qwen2.5），并对比了各模型在“直接规划”和“先建图再规划”两种设置下的性能。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.16909v2/Figures/logo.png" alt="基准评估示例"></p>
<blockquote>
<p><strong>图4</strong>：MomaGraph-Bench中评估多选VQA任务的示例。展示了涵盖六种核心推理能力的样例问题。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ol>
<li><strong>整体性能</strong>：MomaGraph-R1在MomaGraph-Bench上达到了71.6%的总体准确率，超过了所有开源基线模型，比最佳基线提升了+11.4%。在最具挑战性的第4层级任务上，其优势尤为明显。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.16909v2/Figures/Teaser.png" alt="直接规划与图规划对比"></p>
<blockquote>
<p><strong>图2</strong>：即使是像GPT-5这样的强闭源模型，直接规划也常常失败，产生错误动作或遗漏关键步骤；而采用结构化场景图的“先建图再规划”方法则能产生与真实逻辑一致的准确、完整的任务序列。</p>
</blockquote>
<ol start="2">
<li><strong>“先建图再规划”的有效性</strong>：实验表明，与直接从原始场景图像进行任务规划相比，采用“先建图再规划”策略能显著提升任务规划的准确性和鲁棒性。即使是强大的闭源模型，直接规划也常产生错误或遗漏步骤，而基于场景图的规划则能产生正确完整的动作序列。</li>
<li><strong>消融实验（关系类型统一的重要性）</strong>：如表1所示，将MomaGraph-R1和LLaVA-Onevision模型分别限制为仅使用空间关系或仅使用功能关系进行训练和评估，其性能均显著低于使用统一空间-功能关系的版本。例如，MomaGraph-R1的统一版本整体准确率为71.6%，而仅空间版为59.9%，仅功能版为64.9%。这证实了对于具身智能体，整合两种关系的统一表示是更完整有效的基础。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了MomaGraph，这是首个联合建模空间与功能关系并纳入部件级交互节点的场景图表示，为具身智能提供了紧凑、动态且任务对齐的知识结构；2) 构建了首个大规模、细标注、任务驱动的家庭环境场景图数据集MomaGraph-Scenes，以及系统评估六种推理能力的统一基准MomaGraph-Bench；3) 开发了MomaGraph-R1，一个利用强化学习优化空间-功能推理的7B视觉语言模型，实现了“先建图再规划”范式下的零样本规划。</p>
<p>论文自身提到的局限性在于，其重点在于如何捕捉和整合观察到的状态变化以更新场景图，而非关注智能体具体的交互策略本身。这对后续研究的启示在于：MomaGraph提供了一种强大的结构化中间表示，未来工作可以探索如何将这种动态的场景图更紧密地与低层控制策略、长期记忆模块或更复杂的多智能体协作规划相结合，以处理更开放、复杂的具身任务。此外，其“先建图再规划”的范式及基于图的奖励设计，也为提升其他VLM在结构化推理任务上的性能提供了可借鉴的思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对移动操作机器人在家庭环境中需要同时导航和操作的核心问题，提出MomaGraph统一场景图表示，以解决现有方法分离空间与功能关系、忽略对象状态及任务相关性的局限。关键技术包括：集成空间-功能关系和部分级交互元素的MomaGraph场景表示；构建大规模任务驱动数据集MomaGraph-Scenes和评估套件MomaGraph-Bench；基于视觉-语言模型的MomaGraph-R1，采用Graph-then-Plan框架进行零样本任务规划。实验结果显示，模型在基准测试上达到71.6%的准确率，相比最佳基线提升11.4%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.16909" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>