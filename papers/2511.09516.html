<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.09516" target="_blank" rel="noreferrer">2511.09516</a></span>
        <span>作者: Li, Runhao, Guo, Wenkai, Wu, Zhenyu, Wang, Changyuan, Deng, Haoyuan, Weng, Zhenyu, Tan, Yap-Peng, Wang, Ziwei</span>
        <span>日期: 2025/11/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，预训练的视觉-语言-动作模型已成为机器人操作领域的新范式，如RT-2、OpenVLA和π₀。这些模型通过端到端训练，能够将原始视觉观察和语言指令直接映射为机器人动作，展现出强大的泛化能力。然而，这些方法存在一个关键局限：它们在任务执行时缺乏记忆机制，仅依赖即时的感官输入来决定后续动作。训练演示中包含的丰富经验仅在离线训练时用于更新模型参数，而在执行时无法作为历史记忆被显式访问。这种无状态的执行方式对于长视野任务尤其不利，机器人可能在与专家曾遇到的类似挑战性场景中偏离预期轨迹。现有的一些上下文学习方法要么需要针对特定架构进行大量训练，要么专注于大语言模型而不适用于预训练的VLA模型。本文旨在弥合这一差距，为VLA模型赋予一种轻量级的情景记忆能力。其核心思路是：通过提示调优技术，将专家演示中的阶段特定知识编码为可学习的软提示，构建一个外部记忆库；在执行时，通过轨迹相似性匹配检索相关记忆提示，并动态地将其集成到冻结的VLA模型中，以增强动作生成。</p>
<h2 id="方法详解">方法详解</h2>
<p>MAP-VLA的整体框架是一个为冻结的预训练VLA模型增加演示衍生记忆提示的即插即用模块。它包含两个核心阶段：离线的记忆提示构建和在线的记忆增强动作生成。</p>
<p><img src="https://arxiv.org/html/2511.09516v1/x1.png" alt="现有VLA方法与MAP-VLA的简化执行流程对比"></p>
<blockquote>
<p><strong>图1</strong>：现有VLA方法与MAP-VLA的简化执行流程对比。左侧：现有VLA模型仅基于当前观察生成动作。右侧：MAP-VLA在生成动作前，会从记忆库中检索与当前执行阶段最相关的记忆提示进行增强。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.09516v1/x2.png" alt="MAP-VLA框架总览"></p>
<blockquote>
<p><strong>图2</strong>：MAP-VLA框架总览。框架分为两部分：1) <strong>记忆提示构建</strong>：将专家演示分割并对齐到不同阶段，通过提示调优为每个阶段生成阶段特定的记忆提示，构建记忆库。2) <strong>记忆增强动作生成</strong>：在任务执行时，根据当前轨迹检索最相关的记忆提示和演示动作，通过记忆感知提示集成机制，动态融合基于记忆提示和基础提示生成的动作，输出最终动作。</p>
</blockquote>
<p><strong>1. 记忆提示构建</strong><br>此阶段目标是从专家演示中提取阶段特定知识，编码为可检索的软提示库。</p>
<ul>
<li><strong>阶段分割与对齐</strong>：首先，选择一个表现良好的演示作为参考，使用Ramer-Douglas-Peucker算法从其末端执行器状态轨迹中提取关键位姿，这些位姿标志着重要的任务转换（如抓取）。以连续关键位姿的中点为界，将参考轨迹分割为K个阶段。然后，使用动态时间规整算法将所有训练演示的轨迹与参考轨迹对齐，确保所有演示的第k个片段都对应同一个语义任务阶段。</li>
<li><strong>阶段特定提示调优</strong>：对于每个阶段k，定义一组可学习的软提示向量序列𝒱_k。在训练时，对于属于阶段k的每个时间步t，其基础提示𝒫_base（由当前观察通过编码器得到）与𝒱_k按元素相加，形成阶段特定的记忆提示𝒫_k（公式2）。通过优化公式3所示的流匹配损失，使模型在使用𝒫_k时预测的动作与专家动作对齐，从而将阶段k的演示记忆编码到𝒱_k中。对所有任务的所有阶段执行此过程，即构建出包含阶段提示{𝒱_k}、演示轨迹和动作序列的记忆库。</li>
</ul>
<p><strong>2. 记忆增强动作生成</strong><br>此阶段在机器人实时执行任务时运作，利用记忆库增强每一步的动作生成。</p>
<ul>
<li><strong>记忆检索</strong>：定义固定窗口大小W，将机器人最近W步的状态记为当前轨迹段𝐒<em>cur。在记忆库的所有演示轨迹上进行滑动窗口搜索，计算𝐒_cur与每个候选窗口的ℓ2距离（公式4）。为提高效率并保证阶段连贯性，检索被限制在当前阶段标签k_cur的相邻阶段（即|k_j^i - k_cur| ≤ 1）内。选择距离最小的演示索引i<em>和位置j</em>（公式5），并据此更新k_cur，从记忆库中检索出对应的阶段提示𝒱</em>{k_cur}和未来动作序列𝐀_j^i。</li>
<li><strong>记忆感知提示集成</strong>：冻结的VLA模型分别使用基础提示𝒫<em>base和检索到的记忆提示𝒫</em>{k*}进行前向传播，生成基础动作预测𝐀_t^base和记忆动作预测𝐀_t^mem。同时，检索到的专家动作𝐀_j^i作为先验。通过公式6计算动态权重系数α_t，该系数基于𝐀_t^mem和𝐀_t^base与𝐀_j^i的接近程度（通过负平方距离的softmax归一化）。最终执行的动作是两者的加权和：𝐀_t^{MemAug} = α_t 𝐀_t^mem + (1-α_t) 𝐀_t^base（公式7）。这种机制能动态平衡基础提示的任务泛化能力和记忆提示的阶段特异性，并对抗检索不准确或阶段边界模糊的问题。</li>
</ul>
<p><strong>创新点</strong>：与现有VLA方法相比，MAP-VLA的创新在于提出了一种轻量级、即插即用的记忆增强机制。它不更新VLA模型内部权重，而是通过提示调优将演示记忆外化为可检索的软提示，并通过在线检索与集成，使模型在执行长视野任务时能动态获得阶段指导，模仿了人类利用情景记忆解决问题的方式。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：方法基于π₀模型实现。仿真实验使用LIBERO基准测试，特别是LIBERO-Long（10个长视野任务）套件，评估设置与OpenVLA一致（每个任务3个随机种子×50次运行）。真实世界实验在6自由度Galaxea A1机械臂上进行，每个任务进行20次运行。</p>
<p><strong>对比基线</strong>：主要对比了当前先进的VLA模型：OpenVLA和π₀。</p>
<p><strong>关键实验结果</strong>：<br>在LIBERO-Long仿真基准上，MAP-VLA取得了最佳性能。</p>
<p><img src="https://arxiv.org/html/2511.09516v1/x3.png" alt="LIBERO-Long仿真性能对比表"></p>
<blockquote>
<p><strong>表I</strong>：LIBERO-Long仿真基准上的性能对比。MAP-VLA在全部10个任务上均超越基线，平均成功率高达83.4%，相比最强的记忆无关基线π₀（76.4%）取得了7.0%的绝对提升和9.2%的相对提升。同时，MAP-VLA的成功率标准差（0.7%）低于π₀（2.3%），表明其具有更好的鲁棒性和一致性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.09516v1/x4.png" alt="全LIBERO基准性能对比"></p>
<blockquote>
<p><strong>图3</strong>：在全LIBERO基准（包括多个任务套件）上的性能对比。MAP-VLA在所有任务套件上均 consistently 优于基线方法，证明了其优势不限于长视野设置。</p>
</blockquote>
<p>在真实机器人评估中，MAP-VLA同样展现出显著优势。</p>
<p><img src="https://arxiv.org/html/2511.09516v1/x5.png" alt="真实机器人长视野任务成功率对比表"></p>
<blockquote>
<p><strong>表II</strong>：真实机器人长视野任务成功率对比。MAP-VLA在“部分成功”（完成第一个子任务）和“完全成功”（完成所有子任务）指标上均大幅超越π₀基线。平均完全成功率从23.3%提升至48.3%，取得了25.0%的绝对增益和107.1%的相对增益，说明记忆增强对维持长序列正确行为至关重要。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.09516v1/x6.png" alt="真实任务案例可视化对比"></p>
<blockquote>
<p><strong>图5</strong>：真实世界任务2（将绿色立方体和橙子放入碗中）的可视化对比。红色圆圈标出了关键阶段。基线π₀策略在抓取和放置阶段表现出不一致和对齐模糊的行为，常导致任务失败。而MAP-VLA则展现出更强的鲁棒性和精确性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.09516v1/x7.png" alt="视觉变化下的性能对比"></p>
<blockquote>
<p><strong>图6</strong>：在存在视觉变化（如物体颜色、纹理、背景改变）的LIBERO-Long任务上的性能对比。MAP-VLA相比π₀基线保持了稳定的性能优势，表明其记忆增强机制在不同视觉条件下依然有效。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文通过消融实验验证了各个组件的贡献。</p>
<p><img src="https://arxiv.org/html/2511.09516v1/x8.png" alt="消融研究结果表"></p>
<blockquote>
<p><strong>表III</strong>：在LIBERO-Long上的消融研究。从基线VLA（76.4%）开始，逐步增加组件：<strong>Universal Prompt</strong>（为整个任务学习单个提示）带来微小提升（76.9%）；<strong>Task Prompt</strong>（为每个任务学习独立提示）提升至79.3%；<strong>Stage Prompt</strong>（阶段特定提示）进一步提升至81.4%；完整的<strong>MAP-VLA</strong>（包含记忆检索和动态集成）达到最佳性能83.4%，且标准差最低。这证明了阶段特定记忆编码以及在线检索与集成机制的有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了MAP-VLA框架</strong>：一种新颖的、即插即用的框架，通过演示衍生的记忆提示来增强预训练的VLA模型，无需修改模型内部权重。</li>
<li><strong>设计了两个核心模块</strong>：记忆提示构建模块将阶段特定知识编码为可学习的软提示库；记忆增强动作生成模块实现了基于轨迹相似性的记忆检索和动态的记忆感知提示集成。</li>
<li><strong>进行了全面的实验验证</strong>：在仿真和真实机器人实验中，MAP-VLA在长视野操作任务上 consistently 超越现有最先进方法，最高取得7.0%（仿真）和25.0%（真实）的绝对性能增益，并展现出更好的鲁棒性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：1) 记忆提示的质量依赖于训练演示的质量和多样性；2) 检索机制依赖于固定的轨迹窗口大小，可能对非常规的运动速度敏感；3) 当前方法主要针对已知任务，在完全未知任务上的零样本迁移能力有待进一步探索。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>轻量化适配范式</strong>：MAP-VLA展示了通过提示调优和外部记忆库来增强冻结大模型的潜力，这为机器人领域的高效适配提供了新思路。</li>
<li><strong>记忆与泛化的平衡</strong>：动态提示集成机制巧妙地平衡了任务泛化与阶段特异性记忆，这种设计思想可应用于其他需要结合先验知识与在线适应的场景。</li>
<li><strong>扩展记忆形式</strong>：未来工作可以探索更丰富的记忆形式（如失败经验、技能片段），以及更高效的检索策略（如基于语义或视觉特征的检索），以应对更复杂的开放世界任务。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在长时程机器人操作任务中缺乏记忆能力、仅依赖即时感知的局限性，提出MAP-VLA框架。其核心方法是：从演示数据构建记忆库，将任务阶段信息编码为可学习的软提示；执行时通过轨迹相似度匹配检索相关记忆，并动态集成到冻结的VLA模型中，以增强动作生成。实验表明，该方法在模拟基准上取得最高7.0%的性能提升，在真实机器人长时程任务中提升达25.0%，优于现有方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.09516" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>