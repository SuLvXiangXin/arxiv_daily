<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.14930" target="_blank" rel="noreferrer">2510.14930</a></span>
        <span>作者: Yunzhu Li Team</span>
        <span>日期: 2025-10-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于扩散策略的行为克隆方法在从有限的人类遥操作演示中学习双手视觉-触觉策略方面显示出潜力。然而，将其扩展到现实世界的高精度装配任务面临两大挑战：首先，收集真实世界演示成本高昂，且任务精度和接触复杂度越高，对数据量的需求越大；其次，当前的演示接口通常缺乏触觉反馈，难以捕捉人类如何利用触觉进行精细操作，导致收集的演示数据通常省略了对于接触丰富任务至关重要的探索性行为（如迭代调整），从而产生次优的训练数据。仿真为扩展视觉-触觉策略学习提供了有希望的途径，但现有工作主要关注视觉模态或对触觉依赖有限的任务。虽然最近有工作探索了基于仿真的触觉输入数据收集，但这些努力通常局限于更简单的任务或设置，或者尚未解决触觉关键型双手任务的大规模训练或鲁棒的模拟到现实迁移问题。</p>
<p>本文针对上述痛点，提出了一个新颖的“真实-模拟-真实”框架，旨在解决精确的双手装配任务。其核心思路是：首先收集少量真实世界演示来预训练一个双手视觉-触觉扩散策略，然后在并行化仿真环境中的场景数字孪生上使用强化学习对该策略进行微调，最后将微调后的策略从仿真迁移回现实世界。</p>
<h2 id="方法详解">方法详解</h2>
<p>VT-Refine 的整体框架是一个两阶段流程：真实世界预训练和仿真微调，目标是学习一个从多模态观测映射到机器人动作的通用且鲁棒的控制策略。</p>
<p><img src="https://arxiv.org/html/2510.14930v2/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：两阶段视觉-触觉策略训练。<strong>第一阶段</strong>：收集带有视觉和触觉模态的真实世界人类演示，并预训练一个扩散策略。<strong>第二阶段</strong>：在仿真中模拟相同的感官模态，并使用基于策略梯度的强化学习对预训练的扩散策略进行微调。</p>
</blockquote>
<p><strong>核心模块与技术细节：</strong></p>
<ol>
<li><p><strong>视觉-触觉表征</strong>：为实现鲁棒的模拟到现实迁移，采用基于点云的表征。观测包含三个模态：（1）<strong>视觉</strong>：由以自我为中心的相机捕获的无色点云；（2）<strong>触觉</strong>：源自触觉传感器的点云，表示传感单元的3D位置及其连续的传感器读数（每个传感器垫由12×32=384个触觉点组成）；（3）<strong>本体感觉</strong>：来自两个机械臂和两个夹爪的关节位置。视觉和触觉点云被合并成一个统一的视觉-触觉表征，触觉传感器的位置通过正向运动学计算并转换到相机的3D坐标系中，以保留两种模态之间的空间关系。合并后的点云由PointNet编码器处理，其输出与由多层感知机编码的本体感觉特征拼接，作为去噪扩散网络的条件输入。</p>
</li>
<li><p><strong>触觉传感器与仿真</strong>：<br><img src="https://arxiv.org/html/2510.14930v2/x2.png" alt="触觉传感"></p>
<blockquote>
<p><strong>图2</strong>：真实与仿真中的触觉传感。(a) 真实世界硬件设置，包括压阻式触觉传感器的设计。四个触觉传感器垫（每手两个）安装在软夹爪上以捕获接触力。(b) 仿真中触觉传感过程的复现。使用弹簧-阻尼器模型模拟触觉点与物体之间的交互，以生成逼真的触觉信号。</p>
</blockquote>
<ul>
<li><strong>硬件（FlexiTac）</strong>：采用基于电阻传感矩阵的柔性传感器，每个手指传感器垫由12×32个传感单元组成，空间分辨率为2毫米。选择该传感器主要因其兼容性好（可安装于多种末端执行器）和模拟到现实迁移性强。</li>
<li><strong>仿真</strong>：基于GPU触觉仿真库TacSL（集成于Isaac Gym）。使用基于穿透的触觉力模型，每个触觉点与刚性物体之间的交互通过开尔文-沃伊特模型（线性弹簧和粘性阻尼器并联）建模，计算接触法向力。通过查询接触物体的符号距离场来计算穿透深度，触觉点的位置通过正向运动学实时更新。</li>
</ul>
</li>
<li><p><strong>第一阶段：真实世界预训练</strong>：收集少量真实世界演示数据集（例如30条轨迹），使用行为克隆预训练一个扩散策略。该策略预测一个动作块。由于演示数量有限，训练后的模型可能无法始终成功，但有望偶尔完成任务，从而为强化学习微调提供奖励信号。</p>
</li>
<li><p><strong>第二阶段：仿真微调</strong>：使用扩散策略策略优化（DPPO）在仿真中对预训练的扩散策略进行端到端微调。DPPO通过将去噪过程形式化为马尔可夫决策过程，利用近端策略优化（PPO）来优化扩散策略。预训练的扩散策略初始化行动者网络，评论者网络随机初始化。采用稀疏奖励函数：当零件成功组装时智能体获得奖励1，否则为0。</p>
</li>
</ol>
<p><strong>创新点</strong>：1) 通过在仿真中进行基于强化学习的微调来增强视觉-触觉扩散策略，允许在初始人类演示附近的状态-动作区域进行探索以改进策略。2) 在基于GPU的物理模拟器中开发了GPU并行化的触觉仿真模块，以准确模拟可可靠捕获法向力信号的压阻式触觉传感器，显著缩小了模拟到现实的差距。3) 采用基于点的视觉和触觉模态表征，促进了无缝的真实-模拟-真实迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/数据集</strong>：任务选自AutoMate数据集，涉及五种不同的插头-插座配对进行空中插入。初始姿态在3厘米范围内随机化。</li>
<li><strong>实验平台</strong>：在两个机器人平台上评估：（1）桌面双手机器人手臂设置（两个WidowX手臂，软夹爪）；（2）半人形机器人设置（两个Kinova Gen3手臂，Robotiq夹爪）。均配备四个触觉传感器垫和一个以自我为中心的RealSense D455相机。</li>
<li><strong>Baseline方法</strong>：主要对比<strong>视觉策略</strong>（仅视觉输入）与<strong>视觉-触觉策略</strong>（本文方法）。同时比较<strong>预训练策略</strong>与<strong>强化学习微调后策略</strong>的性能。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2510.14930v2/x6.png" alt="仿真微调性能"></p>
<blockquote>
<p><strong>图6</strong>：预训练策略的仿真微调性能。比较视觉-触觉（蓝色）和仅视觉（橙色）策略的微调性能。视觉-触觉策略不仅从更高的预训练性能开始，而且持续改进，在微调后达到更高的最终性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.14930v2/x7.png" alt="策略性能对比"></p>
<blockquote>
<p><strong>图7</strong>：预训练策略在仿真和真实环境中的性能，以及微调策略在仿真和真实环境中的性能。展示了策略在跨域（真实-模拟-真实）迁移时的性能变化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.14930v2/x8.png" alt="策略执行轨迹"></p>
<blockquote>
<p><strong>图8</strong>：策略执行轨迹。(a) 成功轨迹：双臂持续重新定向或移动零件，直到它们平滑地滑动在一起，屏幕上不断演变的触觉图表明了这一点。(b) 失败轨迹：机器人要么以错位的姿态停止，要么以不良角度推动，导致卡住和不完全插入。</p>
</blockquote>
<ul>
<li><strong>微调提升精确操作</strong>：在真实世界实验中（表1），微调后的策略将仅视觉变体的成功率提高了约20%，将视觉-触觉变体的成功率提高了约40%。在仿真实验中（表2），两种策略都从微调中受益，但视觉-触觉策略不仅起点更高，而且收敛到更高的精度。微调引入了必要的探索行为（如快速的“摆动-对接”微调），这些行为在演示中并未明确捕获，但对紧密配合的插入至关重要。</li>
<li><strong>触觉反馈增强策略微调</strong>：视觉-触觉策略在预训练后具有更强的基线性能，并且在微调后能达到更高的精度。仅视觉策略常见的失败模式是在插头悬停在插座正上方时停滞，无法闭合最后2毫米的间隙，而视觉-触觉策略会持续调整直至成功插入。</li>
<li><strong>表征跨域迁移有效性</strong>：如图7所示，即使采用低差距的触觉模态，从真实迁移到仿真仍会导致成功率下降约5–10%，而从仿真到真实的迁移造成的下降更小（有时可忽略）。由于仿真中的强化学习微调将成功率提高了30%以上，这种迁移损失是可以接受的。</li>
<li><strong>消融实验：预训练数据量的影响</strong>：如表3所示，仅用10条演示训练的基策略性能很差，但强化学习微调仍将其成功率提升至约30%。用30条和50条演示训练的基策略达到了合理的性能，并且都微调到接近完美的成功率。从30条增加到50条演示对基策略的改善有限，因为主要的瓶颈在于插入阶段所需的精细、实时调整，这些微动作用适度增加演示数据难以捕捉。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个结合仿真强化学习微调的视觉-触觉策略学习框架（VT-Refine），有效提升了双手精密装配任务的性能。</li>
<li>开发了一种能够高保真模拟密集触觉传感网格的触觉仿真方法，并实现了仿真与真实世界之间的强对齐，缩小了模拟到现实的差距。</li>
<li>采用了基于点的统一视觉-触觉表征，促进了策略在真实与仿真环境间的鲁棒双向迁移。</li>
</ol>
<p><strong>论文自身提到的局限性</strong>：</p>
<ol>
<li><strong>与视觉触觉传感器的权衡</strong>：本文的FlexiTac传感器在分辨率（2毫米 vs. &lt;1毫米）和剪切力感知方面不及基于视觉的触觉传感器，但以这些为代价换取了更可靠的模拟到现实对齐和更易集成的系统设计。</li>
<li><strong>方法适用范围的限制</strong>：当前流程受模拟器能力限制（不支持可变形物体）；更长视野的任务需要更多训练时间；缺乏剪切力传感限制了在更复杂任务中的应用。</li>
<li><strong>对CAD模型的依赖</strong>：训练微调策略仍需物体的CAD模型，限制了向更广泛日常物体的扩展。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>探索更高保真度或支持更复杂物理（如可变形物体、剪切力）的触觉仿真方法，以扩展方法的适用范围。</li>
<li>研究无需CAD模型的“即插即用”管道，例如利用实例分割和3D重建来自动生成仿真所需的对象模型。</li>
<li>进一步优化真实-模拟对齐的自动化校准流程，减少人工干预。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VT-Refine框架，解决机器人执行精确双手装配任务时，因人类演示数据有限且缺乏触觉反馈，导致策略鲁棒性不足的问题。方法结合真实演示与高保真触觉模拟：首先基于少量视觉-触觉演示数据预训练扩散策略，随后在配备GPU加速触觉传感器的数字孪生仿真环境中，通过大规模强化学习对策略进行微调，以提升泛化能力。实验表明，该方法通过增加数据多样性和强化微调，有效提升了模拟与真实环境中的装配性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.14930" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>