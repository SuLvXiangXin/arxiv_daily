<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.11364" target="_blank" rel="noreferrer">2509.11364</a></span>
        <span>作者: Yizhao Wang Team</span>
        <span>日期: 2025-09-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于特定数据集训练的6D物体姿态估计方法已取得良好性能，而零样本方法则仅需CAD模型即可估计新物体的姿态。然而，现有方法大多假设单一视角的观测包含足够信息来估计唯一姿态。在实际应用中，自遮挡和物体间遮挡会掩盖物体的关键特征区域，导致姿态估计问题本身是病态的。这一问题在具有对称结构和无纹理表面的工业金属零件中尤为突出。姿态模糊性在实际场景中不可避免，因此解决它至关重要。</p>
<p>虽然直接重定向目标物体或移除遮挡物可以解决模糊性，但在涉及高精度组件或精密仪器的任务中，此类直接干预通常不可行。相比之下，让相机主动调整视角（模仿人类观察）是一种更通用的解决方案。先前的研究尝试预测姿态模糊性，但它们不仅依赖昂贵的先验信息，而且未能提供消除模糊性的可行方案。一些方法训练神经网络来预测用于主动姿态估计的“最佳下一个视角”，但这些方法需要大量的离线训练。因此，如何在操作过程中确定最优的相机运动以减轻姿态模糊性，仍然是一个开放且具有挑战性的问题。</p>
<p>本文针对上述痛点，提出了一种主动姿态估计流程，结合视觉语言模型和“机器人想象”来实时动态检测和解决模糊性。核心思路是：离线阶段基于CAD模型渲染密集视图并计算姿态熵，构建包含明确和模糊示例的几何感知提示；在线阶段，系统通过VLM评估实时图像的模糊性，若检测到模糊则通过渲染虚拟视图想象候选相机姿态，并综合VLM模糊概率与姿态熵进行评分，从而规划最佳下一个视角以获取明确的姿态估计。</p>
<h2 id="方法详解">方法详解</h2>
<p>ActivePose框架包含两个紧密集成的模块：用于解决6-DoF姿态模糊性的主动姿态估计流程，以及用于在物体运动期间维持准确姿态估计的基于等变扩散策略的主动姿态跟踪模块。</p>
<p><img src="https://arxiv.org/html/2509.11364v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ActivePose整体框架。它集成了一个主动姿态估计模块（通过几何感知的VLM提示和NBV规划解决姿态模糊性）以及一个基于等变扩散的主动姿态跟踪模块，该模块持续为机器人操作生成相机轨迹。</p>
</blockquote>
<p><strong>主动姿态估计</strong>：该流程分为离线几何感知提示构建和在线模糊性检测与最佳下一个视角选择两个阶段。</p>
<ol>
<li><strong>离线阶段</strong>：给定物体的CAD模型，系统渲染一组密集的视图{R_k}。对于每个渲染视图，调用FoundationPose获取其6-DoF旋转分布并计算其香农熵H_k。然后根据H_k对视图排序，选择一组熵最低的“明确”视图U（如轴测图、底部、后视图）和一组熵最高的“模糊”视图A。几何感知提示由U ∪ A中的图像聚合而成。</li>
<li><strong>在线阶段</strong>：获取实时图像I_live，并通过VLM计算模糊性概率p_amb。如果p_amb低于阈值τ（设为0.5），则立即调用FoundationPose(I_live)获得最终6-DoF姿态。否则，进入“机器人想象”环节：对于每个候选虚拟相机姿态C_j，渲染想象视图Î_j，计算其次级模糊性概率p_amb,j和姿态熵H_j。然后为每个候选视图计算综合得分S_j = λ H_j + (1-λ) p_amb,j（λ为融合权重，平衡几何不确定性和语义模糊性）。选择得分最低的视角索引j<em>，将相机移动到C_j</em>，捕获新的实时图像，并最终通过FoundationPose获得去模糊后的姿态。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11364v1/x2.png" alt="主动姿态估计流程"></p>
<blockquote>
<p><strong>图2</strong>：主动姿态估计流程。(1) 离线：系统从渲染视图中计算姿态熵，并使用模糊和明确示例构建几何感知提示。(2) 在线：通过VLM评估实时图像的模糊性，并通过渲染和熵引导的评分选择NBV。</p>
</blockquote>
<p><strong>主动姿态跟踪</strong>：在获得物体6D姿态后，将主动跟踪表述为一个条件扩散问题，以保持持续的可见性和精确的姿态准确性。该方法训练一个等变扩散网络来模仿演示的相机运动，而非手工设计重规划目标。<br>在每个时间步t，构建包含过去H帧物体姿态和末端执行器姿态的观测O_t。初始化一个标准高斯噪声，然后通过K个学习的反向扩散步骤进行去噪。其中，ε_θ是一个轻量级的等变卷积网络，使用连续6D表示处理旋转。在运行时，采用滚动时域方案：去噪获得预测的未来末端执行器姿态后，执行最后k个姿态，然后向前滚动窗口并重复。这产生了平滑、自适应的相机运动，能自然地保持物体在视野中并维持姿态准确性，模糊性减少作为学习策略的隐式目标出现。</p>
<p><img src="https://arxiv.org/html/2509.11364v1/x3.png" alt="主动姿态跟踪流程"></p>
<blockquote>
<p><strong>图3</strong>：主动姿态跟踪流程。编码当前观测，经过K步反向扩散去噪获得连续的SE(3)姿态，然后在一个滚动时域循环中执行最后k个姿态。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1) 提出了一个零样本、几何感知的VLM提示框架，用于鲁棒的模糊性检测和主动姿态估计，无需物体特定训练；2) 引入了结合VLM语义评分与FoundationPose几何熵的统一评分机制，以及用于想象和评估候选视角的可微分渲染模块；3) 提出了一个基于模仿学习训练的等变扩散策略的主动姿态跟踪模块，能够生成保持物体可见性和减少模糊性的平滑相机轨迹。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在仿真（PyBullet）和真实世界中进行。真实实验使用两个Franka Emika Panda机械臂，一个搭载腕部Intel RealSense D435 RGB-D相机用于感知，另一个用于操作。选择了来自MP6D数据集的三个物体以及一个用于装配任务的物体进行评测，所有物体均具有高姿态模糊性。使用ChatGPT-4o作为VLM。主动姿态跟踪模块使用每个任务10条专家演示进行训练。</p>
<p><strong>主动姿态估计结果</strong>：评估了两种放置条件：随机放置和高熵（故意模糊）放置。在仿真中（表I），Fixed-View基线在随机放置下成功率为60.0%，在高熵放置下骤降至20.0%。Random-NBV选择将成功率分别提升至62.5%和52.5%。相比之下，本文的主动姿态估计流程在随机放置下达到97.5%的成功率，在高熵放置下达到95.0%。在真实世界试验中（表II），本文方法也保持了92.5%和95.0%的高成功率，显著优于基线。</p>
<p><img src="https://arxiv.org/html/2509.11364v1/x4.png" alt="实验物体"></p>
<blockquote>
<p><strong>图4</strong>：实验物体的CAD模型（Obj. 1–3来自MP6D；Obj. 4用于装配）及一个模糊视角的示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.11364v1/x5.png" alt="成功案例"></p>
<blockquote>
<p><strong>图5</strong>：真实世界中成功姿态估计的一个示例。</p>
</blockquote>
<p><strong>主动姿态跟踪结果</strong>：在四种挑战性场景下评估（表III）。在长距离线性运动中，本文的扩散策略跟踪器成功率为87.5%，显著优于Pose-Servo（61.3%）和固定世界相机（48.8%）。在圆形旋转运动中，本文方法达到91.3%的成功率，而Pose-Servo完全失败（0.0%）。在临时遮挡和随机空间运动场景下，本文方法也分别取得了52.5%和72.5%的成功率，均优于基线方法。</p>
<p><strong>集成估计与跟踪结果</strong>：在标准的“孔轴装配”任务中评估（表IV）。仅使用Fixed-View成功率为40%。结合Pose-Servo提升至50%。使用Random-NBV+Pose-Servo达到70%。而本文的ActivePose框架通过在选择抓取视角时进行去模糊，并采用扩散策略持续跟踪物体直至插入孔中，取得了90%的最高成功率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了一个利用VLM和每视图姿态熵实现零样本模糊性检测和主动6D姿态估计的几何感知提示框架；2) 引入了统一的评分机制和可微分渲染模块来选择NBV以解决模糊性并提高估计置信度；3) 提出了一个基于模仿学习训练的等变扩散策略的主动姿态跟踪模块，以在物体运动下生成保持可见性和准确性的平滑相机轨迹。</p>
<p>论文自身提到的局限性在于：尽管等变扩散策略能生成平滑的相机轨迹，但当前方法严重依赖姿态作为输入特征，使其对旋转变化的敏感性高于平移变化。</p>
<p>本文的成果对后续研究的启示在于：将大模型（VLM）的语义推理能力与几何感知、主动感知相结合，为解决姿态估计中的模糊性问题提供了新的零样本思路。同时，将扩散模型应用于主动相机轨迹规划，展示了其在处理动态场景和保持观测质量方面的潜力。未来可探索结合图像特征等更合适的输入表示来提升跟踪鲁棒性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ActivePise系统，以解决机器人操作中因遮挡、对称性或单视角观测导致的6D物体姿态估计歧义问题。核心技术包括：1）主动姿态估计模块，利用视觉语言模型进行实时歧义检测，并结合预计算的熵图与“机器人想象力”规划最佳下一视角以消除歧义；2）基于等变扩散策略的主动姿态跟踪模块，通过模仿学习生成相机轨迹以维持目标可见性。实验表明，该方法在仿真和真实场景中均显著优于传统基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.11364" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>