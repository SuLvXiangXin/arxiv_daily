<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.08500" target="_blank" rel="noreferrer">2509.08500</a></span>
        <span>作者: Jiao, Kechen, Fang, Zhirui, Liu, Jiahao, Li, Bei, Wang, Qifan, Liu, Xinyu, Ruan, Junhao, Qiao, Zhongjian, Zhu, Yifan, Xu, Yaxin, Wang, Jingang, Li, Xiu</span>
        <span>日期: 2025/09/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将视觉语言模型（VLMs）强大的泛化能力应用于具体、动态的具身智能任务面临显著挑战。尽管监督微调（SFT）后的模型能更好地与现实物理世界对齐，但在动态变化的环境中仍存在响应迟缓、产生幻觉等问题，需要进一步对齐。现有的SFT后优化方法主要依赖强化学习（RL）和思维链（CoT）方法，但受限于稀疏奖励和仅优化最终动作，导致样本效率低、一致性差和模型性能退化。具体来说，传统RL方法（如PPO）在优化动作概率或联合优化思维-动作时，会破坏模型内部的语义一致性，如图1所示，可能生成非法动作。本文针对稀疏奖励和优化过程破坏模型一致性的痛点，提出了一种以思维链为中心的优化新视角。其核心思路是利用逐步的偏好学习直接优化模型的中间推理过程（思维链），并通过引入动作策略一致性约束来保持输出的一致性，从而在提升决策能力的同时缓解模型退化。</p>
<h2 id="方法详解">方法详解</h2>
<p>TCPO框架包含两个核心组件：偏好感知微调和动作策略一致性约束，整体流程如图2所示。模型接收环境观察，通过CoT推理生成空间分析和可执行动作；在线交互的决策轨迹存入回放缓冲区；通过基于逐步偏好判断的对比学习来优化思维-动作分布；同时，通过APC约束进行正则化。</p>
<p><img src="https://arxiv.org/html/2509.08500v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：TCPO框架概览。上层阶段实现偏好驱动的CoT微调：VLM通过CoT处理环境观察，生成推理和动作。在线交互将决策轨迹存入回放缓冲区，通过逐步偏好判断的对比学习优化思维-动作分布。下层阶段通过L2损失正则化执行APC约束，以保持预训练模型的思维-动作映射。“火焰”和“雪花”符号表示在该训练阶段是否对相应参数进行梯度反向传播。</p>
</blockquote>
<p><strong>1. 样本对构建</strong>：为解决稀疏奖励问题，TCPO将稀疏的标量奖励转化为轨迹片段间的相对偏好排序。对于任何时间步t，构建偏好元组 𝒫_t = ⟨τ_win^t, τ_lose^t⟩，其中包含奖励较高的优选轨迹段和奖励较低的劣选轨迹段，且两者共享相同的历史轨迹 τ_1:t-1。这种方法能有效利用奖励非零但远低于最大值的次优转移，通过两两比较放大策略更新信号，并整合时序一致的状态-动作历史以保持轨迹连贯性。</p>
<p><strong>2. 基于CoT的偏好微调</strong>：这是方法的核心创新点。传统微调过度优先动作优化而忽视CoT连贯性。TCPO则优先优化推理过程的质量。其损失函数建立在直接偏好优化（DPO）基础上，但进行了关键改进。推导出的逐步优化公式（公式4）的梯度表明，直接的动作概率影响被消除，优化直接作用于思维链文本的概率。在实践中，TCPO采用了带动作概率加权（APW）的损失函数（公式6），其中动作概率 p(a|𝒯) 作为权重来强化对应思维的概率。直觉上，CoT后动作概率越高，表明思维与动作的对齐越强，该样本应获得更大权重；反之则抑制高随机性正样本的产生，促进生成更确定、与思维对齐的样本。论文假设预训练模型已具备良好对齐，因此 p(a|𝒯) 接近1，并通过图5(c)的分布验证了该假设的合理性。</p>
<p><strong>3. 动作策略一致性（APC）约束</strong>：为了缓解微调可能导致的模型固有语言模式改变（灾难性遗忘）及思维-动作脱节问题，TCPO在第二阶段引入了APC约束。该约束通过L2正则化项（公式8）强制微调后模型在给定思维下生成的动作文本输出与参考基础模型（通常是SFT后的模型）的输出保持一致。这专门针对“思维到动作”的映射过程，而非整体输出分布匹配。如图1对比所示，没有APC约束的模型可能产生看似合理的中间推理，却最终输出与推理无关的非法动作；而加入APC约束的模型能保持推理与动作的逻辑一致性，生成有效动作。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个基准上进行：包含数字线（NL）、简单拾取（EZP）、24点（P24）、21点（BJ）四个核心任务的GymCards环境，以及包含六类家庭任务（拾取放置、拾取两件放置、清洁放置、冷却放置、加热放置、检查）的ALFWorld环境。模型基于LLaVA-v1.6-Mistral-7B架构实现。</p>
<p>对比的基线方法包括：CNN+RL、GPT4-V、Gemini、LLaVA-sft、RL4VLM（PPO）以及偏好学习方法DPO和D3PO。在ALFWorld中，由于环境只提供稀疏的任务进度和完成信号，作者设计了偏好评分公式：P = 50 * 成功率 - 1_{非法动作}，用于构建训练所需的偏好对。</p>
<p><strong>关键定量结果</strong>：如表1所示，在ALFWorld的六个任务上，TCPO取得了26.67%的平均成功率，相比最强的RL基线RL4VLM（20.0%）提升了6.67个百分点，在多个具体任务（Pick、Clean、Heat）上优势明显。在GymCards上，TCPO平均成功率为42.9%，也略优于PPO的42.7%。表2进一步显示，TCPO显著优于标准的DPO和D3PO方法。</p>
<p><img src="https://arxiv.org/html/2509.08500v1/x4.png" alt="训练曲线"></p>
<blockquote>
<p><strong>图4</strong>：ALFWorld环境中的训练曲线。在前2000步，TCPO方法相比基于PPO的交互方法表现出更优的收敛速度和效率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.08500v1/x5.png" alt="动作合法性分析"></p>
<blockquote>
<p><strong>图5</strong>：(a) 训练过程中非法动作比例：TCPO+APC能有效减少非法动作生成。(b) 消融研究：同时使用APC约束和思维中心优化（TCPO-full）效果最佳。(c) 预训练模型中动作概率 p(a|𝒯) 的分布：验证了其接近1的假设。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.08500v1/x1.png" alt="案例对比"></p>
<blockquote>
<p><strong>图1</strong>：TCPO与PPO方法的结果对比。TCPO强调由思维逻辑生成动作的一致性，并加入了APC约束。而传统PPO方法在训练中可能损害一致性，导致生成如右图所示的非法动作（“slice”不是一个可接受动作）。</p>
</blockquote>
<p><strong>消融实验总结</strong>：图5(b)的消融研究表明，完整的TCPO（包含思维中心优化和APC约束）性能最佳。移除APC约束或移除思维中心优化（即退化为类似传统偏好优化）都会导致性能下降，证明了两个核心组件的必要性和互补性。图5(a)进一步显示，APC约束能显著降低训练过程中非法动作的产生比例。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了<strong>TCPO框架</strong>，一种通过逐步对齐思维链过程来优化具身智能体决策的算法，它通过偏好学习将稀疏奖励转化为密集监督，并优先优化推理质量；2）引入了<strong>动作策略一致性约束</strong>，通过正则化思维到动作的映射，有效缓解了在线适应过程中的策略一致性退化问题；3）在ALFWorld等环境上的实验验证了TCPO的有效性，相比SOTA方法取得了显著提升，并证明了其更高的样本效率和更好的训练稳定性。</p>
<p>论文自身提到的局限性包括：方法的性能依赖于环境提供的偏好信号质量，在更复杂或奖励定义模糊的环境中可能需要进一步调整；此外，APC约束依赖于一个较好的参考模型（SFT模型）。</p>
<p>这项工作对后续研究的启示在于：为VLMs的具身决策微调提供了一个新范式，即<strong>将对齐重点从最终动作转向中间推理过程</strong>，这可能更符合智能决策的认知本质。同时，它表明在微调大型生成模型时，<strong>显式地施加输出一致性约束</strong>对于保持模型原有能力和防止退化至关重要。如何将这种思维中心的优化理念与更复杂的多模态规划、以及处理部分可观环境等挑战相结合，是未来值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型在具身决策动态任务中泛化不足、现有微调后方法样本效率低、一致性差及模型退化的问题，提出思想中心偏好优化方法TCPO。该方法通过基于步骤的偏好优化将稀疏奖励转化为丰富样本对，并引入行动策略一致性约束来对齐模型的中间推理过程与输出。在ALFWorld环境中的实验表明，该方法平均成功率达到26.67%，相较RL4VLM提升了6%，有效缓解了模型退化问题。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.08500" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>