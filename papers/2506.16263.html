<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CapsDT: Diffusion-Transformer for Capsule Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>CapsDT: Diffusion-Transformer for Capsule Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.16263" target="_blank" rel="noreferrer">2506.16263</a></span>
        <span>作者: Hongliang Ren Team</span>
        <span>日期: 2025-06-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在消化内窥镜领域，传统的诊断和治疗方法在复杂三维消化环境中的精确控制面临局限。胶囊内窥镜的出现提供了一种微创替代方案，但缺乏主动控制的胶囊机器人主要依赖肠道蠕动被动移动，限制了其诊断和治疗精度。近年来，视觉-语言-动作模型在多种应用中展现出巨大潜力，但其在消化内窥镜机器人，特别是执行消化道内操作的胶囊机器人领域的性能尚未被探索。将VLA模型集成到内窥镜机器人中，有望实现更直观高效的人机交互，提高诊断准确性和治疗效果。本文针对胶囊机器人在胃内操作的特定挑战，提出了CapsDT模型，其核心思路是：通过处理交错的视觉输入和文本指令，利用扩散Transformer模型推断相应的机器人控制信号，以实现内窥镜任务的自主执行。</p>
<h2 id="方法详解">方法详解</h2>
<p>CapsDT是一个为胶囊机器人操作定制的扩散Transformer模型，其整体框架遵循链式控制架构。模型接收来自夹持器摄像头和外部摄像头的连续图像帧、自然语言指令以及机器人本体感知信息，输出预测的7维动作向量，用于控制机械臂末端执行器的运动。这些动作参数编码了平移速度、旋转速度以及一个归一化的夹持器驱动信号。</p>
<p><img src="https://arxiv.org/html/2506.16263v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：CapsDT框架。模型整合了多模态输入：作为去噪输入的有本体感知、带噪声的动作块、控制频率和扩散时间步；作为条件输入的有图像（夹持器和外部摄像头的一组图像）和语言指令。输出为去噪后的动作。</p>
</blockquote>
<p>模型的核心是基于扩散的概率建模方法。由于多模态输入可能导致完成同一任务存在大量可行的动作，若将其建模为确定性映射并回归训练数据中的动作，策略将学习到动作模式的“平均值”，可能产生完全不可行的动作。因此，CapsDT选择对连续条件分布 <code>p(a_t | ℓ, o_t)</code> 进行建模。具体而言，扩散策略首先从标准正态分布中采样一个完全噪声化的动作 <code>a_t^K</code>，然后通过K步去噪，从目标分布中生成一个干净的动作样本 <code>a_t^0</code>。去噪过程由一个可学习的去噪网络 <code>f_θ</code> 完成，该网络以语言指令 <code>ℓ</code>、观测 <code>o_t</code>、噪声动作 <code>a_t^k</code> 和扩散步数 <code>k</code> 为输入，预测干净动作 <code>a_t^0</code>。训练目标是最小化去噪的均方误差损失。实践中，模型倾向于一次性预测一个动作序列（动作块），以鼓励时间一致性并减少任务中的决策次数。</p>
<p>多模态输入编码器是另一个关键技术模块。针对不同模态（低维机器人物理量、图像、文本）格式和维度差异巨大的问题，CapsDT借鉴RDT的思想，将它们编码到统一的潜在空间。具体编码方式如下：1) 对于机器人本体感知、动作块、控制频率等低维向量，使用带有傅里叶特征的MLP进行编码，以有效捕捉其可能的高频变化。2) 对于图像输入，使用预训练的图文对齐视觉编码器SigLIP来提取紧凑的表示。3) 对于文本指令，使用预训练的Transformer语言模型BART-large进行编码。为防止模型过度依赖信息量更大的某一种输入（例如外部摄像头视图），在编码过程中会对每个多模态输入以特定概率进行独立的随机掩码。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在一个完整的胶囊内窥镜机器人系统上进行，该系统包括一个由7自由度KUKA机械臂控制的永磁体、一个内置磁偶极子的胶囊机器人以及一个模拟人体胃部环境的硅胶胃模型。实验定义了四个渐进式内窥镜任务，难度递增：基础导航、旋转控制、在充水环境中的导航、在充水环境中的导航与旋转。</p>
<p><img src="https://arxiv.org/html/2506.16263v1/x3.png" alt="任务定义与可视化"></p>
<blockquote>
<p><strong>图3</strong>：任务定义和可视化。展示了四个挑战性任务的语言指令、随机化设置和每个子任务的定义，并提供了夹持器视图、外部视图和胶囊机器人的视图。夹持器和外部摄像头的照片将作为CapsDT的输入。</p>
</blockquote>
<p>实验使用了自主收集的多任务胶囊机器人数据集进行微调，该数据集包含超过1000条轨迹，是当前最广泛的胶囊机器人数据集之一。Baseline方法选择了Octo和OpenVLA。</p>
<p>表II展示了定量性能对比（成功率%）。CapsDT在四个内窥镜任务上均超越了基线模型。具体而言，在“导航”、“旋转”、“视角调整”和“带旋转的视角调整”四个任务上，CapsDT相比性能最好的基线模型，成功率分别提升了21.25%、8.75%、11.25%和13.75%，平均提升幅度为13.75%。在真实世界模拟操作中，CapsDT取得了26.25%的成功率。这些结果表明CapsDT能够作为鲁棒的视觉-语言通才模型，在各种难度的内窥镜任务中实现最先进的性能。</p>
<p>消融实验方面，论文指出多模态输入随机掩码策略对于防止模型过度依赖单一信息源（如外部视图）至关重要，这提升了模型对深度信息的感知能力。此外，使用动作块预测而非单步动作预测，有助于保持时间一致性并减少误差累积。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 开发了一套完整的胶囊内窥镜机器人系统，并创建了包含四个不同任务、超过1000条轨迹的广泛数据集；2) 提出了CapsDT，一种新颖的视觉-语言-动作扩散Transformer模型，能够有效融合机器人传感器数据、视觉信息和自然语言指令，生成精确的机器人动作，在多项任务上显著超越现有基线。</p>
<p>论文自身提到的局限性在于，尽管在大规模数据集上进行了广泛的预训练，CapsDT在零样本泛化到目标胶囊机器人时，可能仍会因“具身鸿沟”而遇到限制。为此，作者专门针对目标机器人收集了多任务数据集进行微调以应对此挑战。</p>
<p>这项工作对后续研究的启示在于：将先进的生成式AI模型（如扩散模型）与Transformer架构结合，为解决医疗机器人领域复杂、多模态、需精确控制的操作任务提供了新思路。同时，构建高质量、任务特定的机器人数据集对于弥合预训练模型与真实物理世界之间的差距至关重要。未来研究可以探索更高效的数据收集方法、模型在更复杂临床场景下的泛化能力，以及如何将系统扩展到小肠等更复杂的消化道环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对胶囊内窥镜机器人在复杂胃部环境中缺乏主动控制、依赖被动蠕动导致操作精度不足的问题，提出CapsDT模型。该方法是一种结合扩散模型与Transformer的视觉-语言-动作模型，通过处理视觉输入与文本指令，推断机器人控制信号，并构建了由机械臂磁控系统和胃部模拟器组成的实验平台。实验表明，CapsDT在多种内窥镜任务中达到先进性能，并在真实模拟操作中取得26.25%的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.16263" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>