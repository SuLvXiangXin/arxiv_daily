<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.22028" target="_blank" rel="noreferrer">2507.22028</a></span>
        <span>作者: Bolei Zhou Team</span>
        <span>日期: 2025-07-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，导航基础模型的主流方法是通过大规模网络视频和人类演示进行预训练。这些方法主要依赖于被动视觉学习，即从海量视频数据中模仿观察到的行为。然而，此类离线数据虽然捕捉了真实世界的多样视觉观察，但缺乏明确的物理信息和因果关系，而这对现实世界中的决策至关重要。因此，仅基于离线数据训练的导航策略通常对环境的反应能力有限，难以适应环境中的动态物体和运动。视觉模仿教会了智能体“动作看起来是什么样”，但无法教会其“当环境变化时如何适应、恢复或推理反事实结果”。因此，导航基础模型必须从“看见”迈向“体验”：主动与世界交互、接收反馈并通过试错来优化行为。</p>
<p>强化学习（RL）为智能体提供了缩小观察与行动之间差距的途径，并提供了一种可扩展的交互式学习范式，以丰富模型能力，超越静态数据集的行为克隆。然而，仅靠强化学习在构建可泛化的导航模型方面取得的成功有限。先前的方法在狭窄的合成环境中通过强化学习训练智能体，但由于采样效率低下和缺乏归纳先验，所得模型难以在现实世界中实现可扩展和可泛化的导航能力。</p>
<p>本文针对现有导航基础模型缺乏交互性和对动态环境适应能力的痛点，提出了“从看见到体验”的新视角。核心思路是提出一个名为S2E的混合学习框架，通过结合离线视频预训练和模拟环境中的强化学习后训练，在保持从大规模预训练中获得的可泛化视觉表征的同时，扩展导航基础模型的交互能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>S2E框架旨在学习一个视觉导航策略π，使机器人能够从源路径点坐标𝒑<em>s导航到目标路径点坐标𝒑_d。在每个时间步t，观测是过去k帧的RGB帧𝒐</em>(t-k+1:t)和目标坐标𝒑_d。输出是作为动作𝒂的短期相对路径点（频率为5Hz），然后运动模型可以执行它以逐步向目标移动。</p>
<p>该框架包含两个关键技术组件：1）用于鲁棒骨干网络预训练的锚点引导分布匹配策略；2）用于通过强化学习注入能力的残差注意力模块。</p>
<p><img src="https://arxiv.org/html/2507.22028v1/corl-pipeline-07.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：S2E框架整体示意图。模型接收连续RGB帧和目标位置作为上下文信息，并使用预定义的与具体机器人形态无关的锚点作为查询进行预测。上下文嵌入通过自注意力模块整合，输出作为键（K）和值（V）。锚点特征作为查询（Q）。随后，RAM块基于锚点查询Q计算K和V的加权特征，生成精炼的锚点特征。分类头和回归头解码锚点特征，预测分数和带速度尺度的归一化轨迹。预训练阶段用NLL和回归损失端到端训练；微调阶段仅用策略梯度优化RAM块内的参数。</p>
</blockquote>
<p><strong>1. 基于锚点引导分布匹配的预训练</strong><br>机器人导航轨迹本质上是多模态的——给定相同的观测，可能存在多个不同的有效动作。有效建模这种多模态对于可泛化的策略至关重要。常见的表示方法（如离散动作或单峰高斯分布）缺乏表现力，而扩散模型虽然表现力强，但过于灵活，在导航中难以控制，常常产生影响安全性和稳定性的碎片化轨迹。</p>
<p>为此，本文提出使用<strong>锚点引导的高斯混合模型</strong>来表示城市导航中的机器人动作。锚点在机器人前进方向上均匀采样，作为可解释的高级意图。每个锚点对应混合模型中的一个高斯模式，其学习到的分数反映了可能性。策略学会基于这些锚点生成和选择轨迹，从而实现多样且与目标一致的行为。该方法结合了表现力和训练稳定性，也非常适合用强化学习进行微调。</p>
<p><img src="https://arxiv.org/html/2507.22028v1/x2.png" alt="锚点引导分布匹配示意图"></p>
<blockquote>
<p><strong>图4</strong>：锚点引导分布匹配示意图。(a) 人行道场景中锚点引导高斯混合行为的图示，其中锚点引导多样行为生成。(b) 不同表示方法预测的动作分布比较。我们的锚点引导GMM提供了结构化的多模态分布，其中不同的锚点专用于不同的高级意图。</p>
</blockquote>
<p>具体而言，模型除了RGB帧和目标位置外，还输入M个均匀采样的代表性意图点（锚点）𝒑<em>a。在观测𝒐</em>(t-k+1:t)下，时间步t的动作w_t的分布是一个高斯混合模型。模型端到端地使用两种损失进行训练：第一种是负对数似然损失，用于监督分类头和轨迹头；第二种是L2回归损失，用于优化速度尺度v。</p>
<p><strong>2. 基于残差注意力模块的强化学习</strong><br>为了让训练好的模型获得交互性，一个简单的解决方案是在模拟环境中微调全部模型参数。然而，这会使学习到的表征专门针对合成图像，并导致与真实世界数据的分布漂移。这对视觉编码器等敏感组件尤其关键。</p>
<p>本文分析了哪些架构组件既与智能体-环境交互紧密耦合，又对模拟到真实的差距相对不变——这些组件被确定为适应的目标模块。视觉编码器和自注意力层主要处理场景上下文特征，因此对领域偏移特别敏感。相比之下，交叉注意力层明确地建模智能体-环境交互，并且这些关系模式在不同领域间保持一致。因此，交叉注意力层被确定为适合在模拟环境中微调的关键组件。</p>
<p>为了有效微调这个与交互相关的层，本文引入了<strong>残差注意力模块</strong>。该模块从原始预训练权重初始化，同时保持原始模块冻结。RAM维护目标层的一个并行副本，并在此副本上进行微调。形式上，假设在真实世界数据集上训练的Transformer解码器为ψ_D。为了将RAM集成到预训练模块中，我们冻结目标块的原始参数Θ，并创建一个具有参数Θ_l的可训练副本。这个复制的块被两个零初始化的线性层包裹，分别放置在克隆模块的前后。这样的设计确保在初始化时，添加的自适应分支对原始模型输出没有影响，同时允许在微调期间随着零线性层逐渐学习调节与交互相关的特征。</p>
<p><img src="https://arxiv.org/html/2507.22028v1/x4.png" alt="残差注意力模块示意图"></p>
<blockquote>
<p><strong>图6</strong>：残差注意力模块。通过冻结原始预训练的交叉注意力层，并添加一个具有零初始化线性层的可训练副本，构成一个残差结构，使模型能在保留预训练知识的同时学习新的交互行为。</p>
</blockquote>
<p>在训练过程中，模型使用PPO算法在256个并行环境中进行训练。奖励函数定义为R = R_vel + R_dis + R_arrive + R_collision，分别鼓励方向对齐的运动、惩罚位移、给予到达目标和发生碰撞的终止奖励。策略梯度用于调整RAM的参数Θ_r。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置：</strong></p>
<ul>
<li><strong>预训练数据</strong>：35小时从各种机器人和平台收集的导航视频，覆盖广泛环境。</li>
<li><strong>RL后训练环境</strong>：在包含随机分布障碍物的30×30区域模拟环境中进行。</li>
<li><strong>评估基准</strong>：提出了<strong>NavBench-GS</strong>，一个基于真实场景的逼真3D高斯泼溅重建的综合端到端评估基准，结合了逼真的视觉外观和物理交互，用于系统评估导航基础模型的泛化性和安全性。</li>
<li><strong>对比基线</strong>：包括基于VLM的方法、扩散策略、NWM、CityWalker等最先进的导航基础模型。</li>
</ul>
<p><strong>关键实验结果：</strong><br>强化学习后训练能显著提升策略性能，并缓解仅靠离线数据扩展时常见的收益递减问题。在NavBench-GS基准测试中，S2E框架在障碍物和行人场景下均取得了最佳性能。</p>
<p><img src="https://arxiv.org/html/2507.22028v1/x5.png" alt="强化学习有效性"></p>
<blockquote>
<p><strong>图7</strong>：强化学习的有效性。(a) 使用不同数据量训练的模型成功率，显示RL微调相对于纯监督学习的增益。(b) 在NavBench-GS基准测试中，对比不同方法在障碍物和行人场景下的成功率。S2E (Ours) 在所有场景中都取得了最佳性能。</p>
</blockquote>
<p>具体而言，随着预训练数据量增加，仅使用监督学习（SFT）的模型性能提升会出现饱和（收益递减），而经过RL后训练的S2E模型能持续提升成功率。在NavBench-GS的障碍物场景中，S2E的成功率达到85.6%，显著高于最佳基线CityWalker的66.7%；在行人场景中，S2E达到78.9%，而CityWalker为55.6%。</p>
<p><img src="https://arxiv.org/html/2507.22028v1/x6.png" alt="NavBench-GS评估结果"></p>
<blockquote>
<p><strong>图8</strong>：在NavBench-GS基准上的评估结果。S2E在包含障碍物和行人的挑战性场景中，成功率（SR）和碰撞率（CR）均优于所有基线方法。</p>
</blockquote>
<p><strong>消融实验：</strong><br>消融实验验证了锚点引导分布匹配（AGDM）和残差注意力模块（RAM）各自的重要性。移除AGDM会导致碰撞率上升；移除RAM（即进行全模型微调）会严重损害模型的泛化能力，在真实世界评估中成功率大幅下降；而完整的S2E框架在模拟和真实评估中均表现最佳。</p>
<p><img src="https://arxiv.org/html/2507.22028v1/x7.png" alt="消融研究"></p>
<blockquote>
<p><strong>图9</strong>：消融研究。展示了在NavBench-GS上，移除锚点引导分布匹配（AGDM）或残差注意力模块（RAM）对性能的影响。完整S2E框架性能最优。</p>
</blockquote>
<p><strong>真实世界评估：</strong><br>S2E框架在轮式机器人和四足机器人的真实世界城市场景中进行了零样本泛化测试，成功展示了其避障、避让行人等交互能力。</p>
<p><img src="https://arxiv.org/html/2507.22028v1/x1.png" alt="真实世界部署"></p>
<blockquote>
<p><strong>图1</strong>：S2E的真实世界部署。S2E实现了跨环境和跨机器人形态的零样本泛化。我们在多样化的城市场景中，在轮式和四足机器人上展示了其有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献：</strong></p>
<ol>
<li>提出了<strong>S2E学习框架</strong>，一种结合离线视频预训练和模拟环境强化学习后训练的混合范式，用于扩展导航基础模型的交互能力，同时保持其从大规模数据中获得的泛化性。</li>
<li>引入了两项关键技术：<strong>锚点引导分布匹配策略</strong>，用于稳定学习并建模多样化的运动模式；<strong>残差注意力模块</strong>，用于在强化学习后训练中获取反应性行为而不遗忘预训练知识。</li>
<li>建立了<strong>NavBench-GS评估基准</strong>，一个基于逼真3D高斯泼溅重建的、支持物理交互的端到端评估基准，为导航基础模型的泛化性和安全性提供了标准化、可复现的测试平台。</li>
</ol>
<p><strong>局限性：</strong><br>论文提到，模拟环境与真实世界之间仍然存在差距（sim-to-real gap）。尽管使用了逼真的3DGS重建，但物理交互的复杂性和不可预测性可能无法完全模拟。此外，NavBench-GS中的场景数量目前有限，未来需要扩展以涵盖更广泛的环境。</p>
<p><strong>对后续研究的启示：</strong></p>
<ol>
<li><strong>强化学习的作用</strong>：研究表明，对于需要与动态环境交互的机器人基础模型，强化学习是一种比纯监督微调更有效的后训练方法，能够注入关键的反应和适应能力。</li>
<li><strong>混合学习范式</strong>：结合大规模离线预训练（获取先验和泛化性）与针对性在线交互学习（获取特定技能）的框架，是提升机器人基础模型综合能力的有效途径。</li>
<li><strong>评估基准的重要性</strong>：开发像NavBench-GS这样兼具视觉逼真度和物理交互性的评估基准，对于推动导航基础模型朝着更安全、更实用的方向发展至关重要。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对仅靠离线数据训练的导航基础模型缺乏交互推理与安全适应能力的问题，提出S2E学习框架，通过强化学习提升模型交互性。关键技术包括：用于稳定离线预训练的Anchor-Guided Distribution Matching策略，以及强化学习中避免遗忘预训练知识的Residual-Attention Module。实验表明，S2E缓解了纯离线数据扩展的收益递减问题，证明结合交互式在线经验对扩展机器人基础模型至关重要。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.22028" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>