<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.06620" target="_blank" rel="noreferrer">2602.06620</a></span>
        <span>作者: Toshiaki Tsuji Team</span>
        <span>日期: 2026-02-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在接触丰富的任务中，位置轨迹通常易于获取，但合适的力指令往往是未知的。虽然可以设想使用预训练的基础模型（如视觉-语言-动作模型）来生成力指令，但力控制高度依赖于机器人的特定硬件，这使得此类模型的应用面临挑战。模仿学习在接触丰富任务中存在局限性，通常只能处理非常静态的运动。双边控制模仿学习能够收集人类操作者的力感知数据，但存在两个关键问题：同时生成位置和力指令会增加模型维度，使训练更困难；基于学习的运动生成通常比经典的基于模型的方法重现性低。本文旨在提出一种力生成模仿学习方法，从给定的位置轨迹估计力指令，并通过引入反馈控制机制来应对未见过的轨迹，提高泛化能力。本文核心思路是构建一个分层模型，将具有记忆能力的上层与无记忆、可反馈控制的下层分离，上层处理位置轨迹以智能预测未来状态，下层基于当前状态和上层轨迹预测下一步的状态和指令（包括力指令），并结合PID控制来补偿误差。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个分层力生成模仿学习框架，其核心是明确分离具有记忆的上层和无记忆、可反馈控制的下层。</p>
<p><img src="https://arxiv.org/html/2602.06620v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：所提出的用于校正输出误差的分层模型。上层（顶部虚线框）输出未来10步的关节角度和角速度轨迹（上层轨迹）。下层神经网络接收当前状态 <code>s_k</code> 和未来第10步的上层轨迹 <code>[θ_{k+10}, θ̇_{k+10}]</code> 作为输入，预测下一步的状态 <code>ŝ_{k+1}</code> 和动作 <code>â_{k+1}</code>。预测的状态用于计算与上层轨迹的PID控制误差 <code>u_{k+1}</code>，该误差与预测的动作 <code>â_{k+1}</code> 结合后发送给机器人执行。</p>
</blockquote>
<p>整体流程如下：上层模型（具有记忆，如循环神经网络）负责从当前时刻 <code>k</code> 预测未来第 <code>k+10</code> 步的关节角度 <code>θ_{k+10}</code> 和角速度 <code>θ̇_{k+10}</code>，这些输出构成“上层轨迹”。该轨迹每10步更新一次，在更新间隔内保持恒定。下层模型是一个无记忆的多层感知机，其输入是当前状态 <code>s_k</code>（包括关节角度、角速度和力矩）以及上层轨迹 <code>[θ_{k+10}, θ̇_{k+10}]</code>。其输出是预测的下一步状态 <code>ŝ_{k+1}</code> 和动作 <code>â_{k+1}</code>。动作 <code>â_{k+1}</code> 包含力指令，因此该模型能够从位置轨迹生成力指令。</p>
<p>核心创新点在于将记忆功能与可控性解耦。论文发现，若下层模型具有内部记忆（如RNN），则反馈控制可能无法收敛。因此，下层采用无记忆的MLP，确保其在控制回路中的行为是确定且可稳定的。上层负责长时程的智能规划，下层负责基于当前状态和未来目标进行一步的、反应式的预测和力指令生成。</p>
<p>为了提升鲁棒性，系统引入了PID控制。具体而言，利用下层预测的状态 <code>ŝ_{k+1}</code> 中的角度 <code>θ̂_{k+1}</code> 和角速度 <code>θ̇̂_{k+1}</code>，与上层轨迹中对应时刻的目标值 <code>θ_{k+1}</code> 和 <code>θ̇_{k+1}</code> 进行比较，计算误差 <code>u_{k+1}</code>（公式1）。该误差经过微分得到 <code>u̇_{k+1}</code>，然后与预测的动作 <code>â_{k+1}</code> 相加，共同作为最终发送给机器人的指令。这样，PID控制器能够在线补偿模型预测的偏差，使系统即使面对未见过（或模型预测不准）的轨迹也能稳定跟踪。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实的机器人书写任务上进行验证。任务环境如图3所示，机器人使用毛笔在纸上书写字符。</p>
<p><img src="https://arxiv.org/html/2602.06620v1/figures/task_env.png" alt="任务环境"></p>
<blockquote>
<p><strong>图3</strong>：机器人书写任务实验环境。</p>
</blockquote>
<p>使用的数据集通过双边控制遥操作收集，包含了多种书写动作的演示数据。对比的基线方法包括：1）仅使用模仿学习（无PID控制）；2）使用具有记忆的下层模型（如RNN）并结合PID控制。</p>
<p>关键实验结果如下：如图1所示，与无PID控制的基线相比，提出的方法（分层模型+PID）显著提高了书写精度。</p>
<p><img src="https://arxiv.org/html/2602.06620v1/x1.png" alt="书写精度对比"></p>
<blockquote>
<p><strong>图1</strong>：所提方法提高了字符书写的准确性。图中对比了不同方法书写字符的轨迹与目标轨迹（灰色）的贴合程度。</p>
</blockquote>
<p>图4、5、6展示了在书写不同字符（“tsu”、“ku”、“me”）时，使用所提方法（红色实线）与仅使用模仿学习（蓝色虚线）的轨迹跟踪误差对比。结果表明，引入PID控制后，跟踪误差显著减小。</p>
<p><img src="https://arxiv.org/html/2602.06620v1/x3.png" alt="字符“tsu”的跟踪误差"></p>
<blockquote>
<p><strong>图4</strong>：书写字符“tsu”时，x和y方向的轨迹跟踪误差。所提方法（红色）的误差远小于仅模仿学习（蓝色）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06620v1/x4.png" alt="字符“ku”的跟踪误差"></p>
<blockquote>
<p><strong>图5</strong>：书写字符“ku”时的轨迹跟踪误差对比。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06620v1/x5.png" alt="字符“me”的跟踪误差"></p>
<blockquote>
<p><strong>图6</strong>：书写字符“me”时的轨迹跟踪误差对比。</p>
</blockquote>
<p>消融实验方面，论文验证了下层模型无记忆的重要性。图7、8、9展示了当下层模型为有记忆的RNN时，PID控制的比例（Kp）、微分（Kd）、积分（Ki）增益对系统稳定性的影响。当增益增大时，系统均变得不稳定（轨迹发散）。这证实了记忆模型在反馈回路中会阻碍稳定性。</p>
<p><img src="https://arxiv.org/html/2602.06620v1/figures/Kp_x_comparison.png" alt="Kp增益对有记忆模型的影响"></p>
<blockquote>
<p><strong>图7</strong>：当下层为有记忆模型时，增大比例增益Kp导致x方向轨迹发散。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06620v1/figures/Kd_x_comparison.png" alt="Kd增益对有记忆模型的影响"></p>
<blockquote>
<p><strong>图8</strong>：当下层为有记忆模型时，增大微分增益Kd导致x方向轨迹发散。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06620v1/figures/Ki_x_comparison.png" alt="Ki增益对有记忆模型的影响"></p>
<blockquote>
<p><strong>图9</strong>：当下层为有记忆模型时，增大积分增益Ki导致x方向轨迹发散。</p>
</blockquote>
<p>此外，图13对比了所提分层模型与一个普通的、未明确分离记忆的端到端模型（Ordinary Model）在相同PID增益下的性能。普通模型无法稳定控制，而所提模型表现良好。</p>
<p><img src="https://arxiv.org/html/2602.06620v1/figures/compare_ordinalmodel.png" alt="与普通模型对比"></p>
<blockquote>
<p><strong>图13</strong>：所提分层模型与普通端到端模型在相同PID参数下的性能对比。普通模型无法实现稳定控制。</p>
</blockquote>
<p>图10、11、12进一步展示了所提方法在接触力控制方面的表现。机器人能够生成适当的垂直方向（z轴）力，以维持稳定的笔纸接触，同时精确跟踪xy平面内的轨迹。</p>
<p><img src="https://arxiv.org/html/2602.06620v1/x6.png" alt="接触力与位置跟踪（字符“tsu”）"></p>
<blockquote>
<p><strong>图10</strong>：书写“tsu”时，z轴接触力与xy平面位置跟踪情况。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06620v1/x7.png" alt="接触力与位置跟踪（字符“ku”）"></p>
<blockquote>
<p><strong>图11</strong>：书写“ku”时，z轴接触力与xy平面位置跟踪情况。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06620v1/x8.png" alt="接触力与位置跟踪（字符“me”）"></p>
<blockquote>
<p><strong>图12</strong>：书写“me”时，z轴接触力与xy平面位置跟踪情况。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了一个分层的力生成模仿学习框架，明确将基于记忆的上层规划与无记忆、可反馈控制的下层执行分离，实现了从位置轨迹生成力指令。2）揭示了在反馈控制环内嵌入记忆会阻碍系统稳定性，并论证了通过时间尺度分离（上层慢规划、下层快反应），可以使基于学习的力生成同时具备鲁棒性和稳定性。3）该框架能够处理传统阻抗或混合控制难以应对的非平凡或定义不明确的控制目标（如复杂的接触式书写），同时仍能通过经典反馈控制确保稳定性。</p>
<p>论文提到的局限性在于，上层预测的未来步长（10步）是依据先前工作设定的，对于不同任务，如何确定合适的预测范围仍需进一步研究。</p>
<p>本文的启示在于：为结合学习与控制的系统架构设计提供了重要见解，即“记忆”与“实时可控性”应解耦。这种分层、模块化的设计思路有利于集成不同的上游规划器（如VLA模型），并独立训练或微调下层控制器，提高了系统的可扩展性和任务泛化能力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对接触密集型任务中，位置轨迹易得而精确力指令未知的问题，提出了一种力生成模仿学习方法。该方法通过构建力生成模型，从给定的位置轨迹估计出力指令，并引入反馈控制机制来提升模型对未见轨迹的泛化能力。实验表明，当力生成模型输出不准确时，反馈控制无法收敛，从而验证了所提方法在桥接轨迹与力指令方面的必要性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.06620" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>