<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.04769" target="_blank" rel="noreferrer">2505.04769</a></span>
        <span>作者: Sapkota, Ranjan, Cao, Yang, Roumeliotis, Konstantinos I., Karkee, Manoj</span>
        <span>日期: 2025/05/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在VLA模型出现之前，人工智能在机器人领域的发展主要分散在各自独立的领域：专注于图像获取、解释和识别的视觉系统；能够理解和生成文本的语言系统；以及控制运动的动作系统。这些孤立系统各自表现良好，但难以协同工作，无法泛化到新场景或适应现实世界复杂且不可预测的挑战。传统的计算机视觉模型（如CNN）被设计用于物体检测或分类等狭窄任务，需要大量标注数据，且环境或目标稍有变化就需要繁琐的重新训练。这些模型能“看见”但缺乏语言理解能力或无法将视觉洞察转化为期望的动作。语言模型（特别是LLMs）革新了基于文本的理解和生成，但仅限于处理语言，缺乏对物理世界的感知或推理能力。同时，机器人中的动作系统严重依赖手工策略或强化学习，虽然能实现特定行为（如物体操控），但需要大量工程工作，且无法泛化到特定设计场景之外。尽管视觉-语言模型（VLMs）通过结合视觉和语言取得了令人印象深刻的多模态理解，但仍存在明显的整合缺口：无法基于多模态输入生成或执行连贯的动作。因此，需要一个能够联合感知、理解和行动的系统，以实现智能自主行为。</p>
<p>本文针对视觉、语言和动作模态长期割裂，导致机器人系统难以灵活适应新任务和环境、泛化能力脆弱的关键痛点，提出了将三者统一到一个端到端计算框架中的新视角。其核心思路是：通过引入动作令牌（数值或符号化的机器人运动命令表示），将视觉-语言模型扩展为能够从配对的视觉、语言和轨迹数据中学习的VLA模型，从而实现感知、推理和控制的统一。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLA模型是一类联合处理视觉输入、解释自然语言指令并生成可在动态环境中物理机器人硬件上执行的动作表示的智能系统。技术上，VLA模型结合了视觉编码器（如CNN、ViT）、语言模型（如LLM、Transformer）以及策略模块或规划器，以实现任务条件控制。这些模型通常建立在视觉-语言模型中确立的多模态融合技术（如交叉注意力、拼接嵌入或令牌统一）之上，并将其扩展以对齐感官观测、语言指令和动作表示。</p>
<p><img src="https://arxiv.org/html/2505.04769v2/x5.png" alt="VLA核心概念流程图"></p>
<blockquote>
<p><strong>图5</strong>：VLA模型的基础概念（以苹果采摘场景为例）。该图描绘了机械臂在VLA模型指导下自主采摘果园中成熟苹果。右侧流程图概述了VLA模型的四个关键阶段：多模态整合、令牌化与表示、学习范式、以及自适应控制与实时执行。</p>
</blockquote>
<p>一个典型的VLA模型通过相机或传感器数据观察环境，解释以语言表达的目标（例如“捡起红苹果”），并输出可由自动化系统执行的低级或高级动作序列。如图5所示，其整体流程涵盖从多模态输入到动作执行的完整链条。</p>
<p>VLA模型的关键创新在于其基于令牌的表示框架，该框架支持对感知、语言和物理动作空间的整体推理。现代VLA模型使用离散令牌对世界进行编码，将视觉、语言、状态和动作所有模态统一到一个共享的嵌入空间中。这使模型不仅能理解“需要做什么”（语义推理），还能以完全可学习和组合的方式执行“如何去做”（控制策略）。</p>
<p><img src="https://arxiv.org/html/2505.04769v2/x7.png" alt="令牌化与表示过程示意图"></p>
<blockquote>
<p><strong>图7</strong>：说明VLA模型中端到端令牌化与表示过程的示意图。视觉输入（如杂乱桌面）由视觉编码器（如ViT）编码，而自然语言指令（如“堆叠绿色积木”）由语言编码器（如T5）处理。系统通过Transformer融合前缀、状态和动作令牌，并自回归地预测运动动作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.04769v2/x8.png" alt="令牌在现实场景中的应用"></p>
<blockquote>
<p><strong>图8</strong>：说明VLA模型如何在现实场景中使用前缀、状态和动作令牌。在机器人操作中，状态令牌检测机械臂在脆弱物体附近的伸展情况，从而实现路径调整。在导航中，它们表示LiDAR和里程计数据。苹果采摘任务展示了前缀令牌如何指导目标理解，而动作令牌则生成用于目标抓取和执行的运动序列。</p>
</blockquote>
<p>具体而言，令牌化与表示包含三个核心部分：</p>
<ol>
<li><strong>前缀令牌</strong>：编码上下文与指令。作为VLA模型的上下文主干，这些令牌将环境场景（通过图像或视频）和伴随的自然语言指令编码为紧凑的嵌入，为模型的内部表示奠定基础。它们实现了跨模态的语义和空间指代消解。</li>
<li><strong>状态令牌</strong>：嵌入机器人配置。这些令牌编码关于智能体内部物理状态的实时信息，如关节位置、力扭矩读数、夹爪状态、末端执行器位姿，甚至附近物体的位置。这对于确保情境感知和安全性至关重要，如图8所示，状态令牌使模型能够进行动态的、上下文感知的决策。</li>
<li><strong>动作令牌</strong>：自回归控制生成。这是VLA令牌流水线的最后一层，动作令牌由模型自回归生成，以表示运动控制中的下一步。每个令牌对应一个低级控制信号（如关节角度更新、扭矩值、轮速）或高级运动基元。在推理过程中，模型以前缀和状态令牌为条件，逐步解码这些令牌，从而将VLA模型转变为语言驱动的策略生成器。</li>
</ol>
<p>与现有方法相比，VLA的创新点在于将原本分离的感知、理解和执行模块，通过统一的令牌表示和Transformer架构进行端到端的整合，从而支持语义接地、上下文感知推理和时序规划，显著提升了泛化能力和适应性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文是一篇综述性论文，并未报告具体的定量实验数据，而是通过对80多个VLA模型（2022-2025年间发表）的系统性回顾，分析了该领域的进展、应用与挑战。研究涵盖了广泛的学术和技术资料库，包括arXiv、IEEE Xplore、Springer Nature等。</p>
<p><img src="https://arxiv.org/html/2505.04769v2/x6.png" alt="VLA模型演进时间线"></p>
<blockquote>
<p><strong>图6</strong>：视觉-语言-动作模型的综合时间线（2022–2025年），按年份组织，采用高对比度颜色编码和主题分组。该图清晰展示了VLA从基础整合、规模化、专业化到泛化与高级部署的演进路径。</p>
</blockquote>
<p>如图6所示，VLA模型的发展经历了三个阶段：</p>
<ol>
<li><strong>基础整合阶段（2022-2023）</strong>：早期VLA通过多模态融合架构建立了基本的视觉运动协调。例如，CLIPort结合了CLIP嵌入和运动基元；Gato在604个任务中展示了通用能力；RT-1通过规模化模仿学习在操作中实现了97%的成功率；VIMA通过基于Transformer的规划器引入了时序推理；RT-2实现了视觉思维链推理；Diffusion Policy通过扩散过程推进了随机动作预测。</li>
<li><strong>专业化与具身推理阶段（2024）</strong>：第二代VLA融入了特定领域的归纳偏差。例如，Deer-VLA通过检索增强训练增强了少样本适应能力；Uni-NaVid通过3D场景图集成优化了导航；ReVLA引入了可逆架构以提高内存效率；Occllama通过物理信息注意力解决了部分可观测性问题。</li>
<li><strong>泛化与安全关键部署阶段（2025）</strong>：最新系统优先考虑鲁棒性和人类对齐。例如，SafeVLA集成了形式化验证以进行风险感知决策；Humanoid-VLA通过分层VLA展示了全身控制；EdgeVLA针对嵌入式部署优化了计算效率；CogAct结合了神经符号推理进行因果推断。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.04769v2/x10.png" alt="VLA应用领域"></p>
<blockquote>
<p><strong>图10</strong>：VLA模型的应用领域概览。展示了VLA在机器人操作与操控、导航与移动机器人、自动驾驶汽车、人形机器人、医疗机器人、农业机器人、工业4.0以及交互式AR/VR导航等多个领域的应用实例。</p>
</blockquote>
<p>在应用方面，如图10所示，VLA模型已广泛应用于机器人操作、导航、自动驾驶、人形机器人、医疗、农业、工业自动化和AR/VR导航等多个领域。</p>
<p><img src="https://arxiv.org/html/2505.04769v2/x11.png" alt="VLA训练效率策略"></p>
<blockquote>
<p><strong>图11</strong>：VLA模型训练效率策略。总结了在架构创新、数据高效学习、参数高效建模和模型加速策略方面的进展，这些对于将VLA系统扩展到现实世界应用至关重要。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.04769v2/x12.png" alt="VLA推理加速方法"></p>
<blockquote>
<p><strong>图12</strong>：VLA模型的推理加速方法。概述了通过模型压缩、硬件优化、实时调度和边缘部署等技术来克服推理瓶颈的策略。</p>
</blockquote>
<p>在进展方面，论文总结了在训练效率（图11）和推理加速（图12）方面的关键策略，包括架构创新、数据高效学习、参数高效建模以及模型压缩、硬件优化等方法，这些都是实现VLA模型规模化实际部署的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>系统性地梳理了VLA模型的概念框架</strong>：明确了VLA作为统一视觉、语言和动作的端到端智能系统的定义，并详细阐述了其多模态整合、令牌化表示、学习范式及自适应执行的核心原理。</li>
<li><strong>清晰地勾勒了VLA领域的发展脉络与进展</strong>：通过时间线（图6）和主题分析，系统回顾了从2022年至2025年VLA模型从基础整合、专业化到追求泛化与安全部署的演进过程，并总结了在架构、训练效率和推理加速方面的关键技术进步。</li>
<li><strong>全面总结了应用与挑战，并提出了未来方向</strong>：广泛列举了VLA在多个现实领域的应用潜力（图10），同时深入分析了当前面临的主要挑战，包括实时控制、多模态动作表示、系统可扩展性、对未知任务的泛化以及伦理部署风险，并提出了诸如智能体AI适应、跨具身泛化和统一神经符号规划等前瞻性解决方案。</li>
</ol>
<p>论文自身作为一篇综述，其局限性在于它是对现有工作的综合与分析，而非提出一种新的具体方法并附有实验验证。然而，它成功地为该领域的研究者和实践者提供了一份结构化的“路线图”。</p>
<p>本文对后续研究的启示在于：它指出了VLA领域正从技术可行性探索转向可靠性、安全性和泛化能力攻坚的关键阶段。未来的研究需要重点关注如何设计更具数据效率和参数效率的模型，如何实现安全、可信且符合伦理的部署，以及如何构建能够跨不同机器人平台和任务泛化的通用具身智能体。神经符号推理、持续学习、仿真到现实的迁移以及人机交互等方向将成为推动VLA模型走向成熟应用的重要前沿。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文是一篇关于视觉-语言-动作（VLA）模型的综述。其核心问题是解决传统AI中视觉、语言与动作系统相互割裂、难以协同适应复杂现实任务的局限。论文提出VLA模型通过统一的计算框架，整合视觉语言模型、动作规划器与分层控制器，并采用动作标记化、高效训练等关键技术，旨在构建能感知、理解并执行任务的具身智能体。文章系统回顾了80多个近期模型，指出该领域在架构、训练与推理方面进展迅速，但仍在实时控制、任务泛化和伦理部署等方面面临挑战。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.04769" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>