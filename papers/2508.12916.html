<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.12916" target="_blank" rel="noreferrer">2508.12916</a></span>
        <span>作者: Wang, Hecheng, Ren, Jiankun, Yu, Jia, Qi, Lizhe, Sun, Yunquan</span>
        <span>日期: 2025/08/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人目标检索系统的主流方法主要依赖两类感知条件：一是精心布置的固定摄像头，能够观察所有任务相关内容；二是部署多个冗余摄像头以覆盖整个工作空间。然而，这些方法的适应性有限，任务目标或环境的变化会显著影响最佳观测视角。近期研究尝试通过交互感知让机器人通过物理交互（如打开容器、移动遮挡物）来获取信息，以解决部分可观测性问题，但这些方法通常在固定摄像头设置下运行，缺乏自主调整机器人视角的能力。另一方面，主动感知研究允许机器人控制摄像头视角，但主要针对2D空间的移动机器人导航开发，而非遮挡和空间约束更为复杂的6自由度操作任务。此外，现有操作领域的主动感知策略通常是硬编码或任务特定的，缺乏解释和遵循开放式语言目标的能力，限制了其对多样化指令和场景的适应性。</p>
<p>本文针对在复杂、部分可观测场景中，仅使用单腕戴RGB-D摄像头和自由形式自然语言指令的机器人自主目标检索这一具体挑战，提出了一个新的视角：通过构建和更新动态分层场景图来统一和协调主动感知、交互感知与操作。本文的核心思路是：构建一个动态更新的图形式记忆来编码场景语义、几何和物体间关系，并基于此记忆和任务指令，通过一个新颖的视觉提示方案驱动推理视觉语言模型，实现任务感知、场景接地的主动感知，从而协同引导物理交互与操作，最终完成目标检索。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboRetriever框架包含四个核心模块：接地模块、记忆模块、监督模块和动作模块。其整体流程是：在时间步t，接地模块处理当前观测（RGB-D图像与机器人位姿）以提取物体几何信息；记忆模块基于先前的记忆、当前观测和提取的信息更新内部动态分层场景图；监督模块基于用户指令和更新后的记忆推理目标物体和高级动作，并生成语言目标描述；动作模块根据高级动作和记忆生成低级可执行动作（主动感知、交互感知或操作）。</p>
<p><img src="https://arxiv.org/html/2508.12916v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：系统概览。包含四个关键模块：接地模块、记忆模块、监督模块、动作模块。系统接收观测和指令，通过迭代更新记忆和推理，协调主动感知、交互感知和操作动作来探索并检索目标物体。</p>
</blockquote>
<p><strong>接地模块</strong>：接收RGB-D图像和机器人位姿，通过手眼标定和位姿变换生成基坐标系下的3D点云。利用GPT-4o推断物体类别，并利用DINO-X和SAM获取物体边界框和分割掩码，进而得到每个物体的分割点云和裁剪图像作为输出信息。</p>
<p><strong>记忆模块</strong>：维护一个动态分层场景图 G=(V,E)，其中节点V代表物体，边E代表物体间关系。每个节点关联一个低层语义-几何物体条目，包含裁剪图像历史、点云、细粒度语义标签、是否可移动、置信度、是否持续被遮挡、是否因视角限制持续部分观测、描述语句等属性。边编码五种空间或语义关系：<code>behind</code>、<code>belong</code>、<code>inside</code>、<code>on</code>、<code>under</code>。记忆更新时，使用GPT-4o进行实例匹配，使用ZeroMatch算法进行点云配准与融合，并使用GPT-o3更新节点语义属性。基于几何、语义和动作历史信息，通过规则和VLM推理建立关系。对于暗示存在遮挡或未探索区域的关系（如<code>inside</code>），会提示GPT-4o推理是否应引入一个代表假设物体的<code>Unknown</code>节点。</p>
<p><strong>监督模块</strong>：以动态场景图、当前观测、动作历史和任务指令作为输入，利用GPT-o3作为推理引擎，预测三个输出：目标物体、高级动作类型（主动感知、交互感知、操作）以及解释动作目标的语言描述，确保行为在时间上连贯且目的明确。</p>
<p><strong>动作模块</strong>：根据监督模块的指令调用相应子模块。<strong>主动感知</strong>子模块采用新颖的视觉提示方案：以目标物体点云为中心定义一个虚拟主动感知球体，在其表面采样N个候选移动方向，渲染场景点云的三个标准视图（前、左、右）作为视觉提示，引导GPT-o3根据动作目标选择最信息丰富的方向；接着沿该方向采样M个候选位姿，再次提示GPT-o3选择最优的下一个摄像头6D位姿，过程中摄像头始终朝向物体中心。<strong>交互感知</strong>子模块实现了四个动作基元：<code>Open</code>（打开抽屉/柜门）、<code>Close</code>（关闭）、<code>Pick&amp;Place</code>（拾放以移除遮挡物）、<code>Rotate</code>（旋转物体以暴露隐藏面）。<strong>操作</strong>子模块利用AnyGrasp实现抓取，并将物体放置到指定位置。</p>
<p><img src="https://arxiv.org/html/2508.12916v1/x3.png" alt="主动感知示意"></p>
<blockquote>
<p><strong>图3</strong>：主动感知示意图。上图：摄像头移动方向选择。下图：沿选定方向的摄像头位姿选择。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界桌面场景中进行，使用JAKA Zu 7机械臂，腕部搭载Gemini 336 RGB-D摄像头，配备DH-Robotics AG-95平行夹爪。设计了六个具有不同属性的任务类别进行评估，包括<code>Hidden Inside</code>、<code>Recursive Search</code>、<code>Reposition to Reveal</code>、<code>Sequential Retrieval</code>、<code>Semantic Targeting</code>和<code>Compositional Reasoning</code>。</p>
<p><img src="https://arxiv.org/html/2508.12916v1/x4.png" alt="任务示例"></p>
<blockquote>
<p><strong>图4</strong>：各任务类别示例。左图：任务开始时机器人腕部唯一摄像头的RGB-D观测。右图：任务环境的第三人称概览（仅用于说明，系统无法访问）。</p>
</blockquote>
<p>对比的基线方法包括：<strong>RoboExp</strong>（固定摄像头下的交互感知建图框架）、<strong>AP-VLM</strong>（基于桌面虚拟3D网格的主动感知方法）、<strong>GPT-o3</strong>（直接向GPT-o3提供观测和指令，由其生成动作，交互和操作为人工提供真值）。评估指标包括：成功率、物体发现率（ODR）以及场景图编辑距离（GED）。</p>
<p><img src="https://arxiv.org/html/2508.12916v1/x5.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：递归搜索任务的定性结果。展示了任务执行过程、高层场景图更新和系统决策过程。蓝色节点为新添加物体，绿色为已存在物体，红色为未探索区域，斜纹节点表示因视角限制持续部分观测的物体。</p>
</blockquote>
<p>关键实验结果总结如下表所示，本文方法在所有任务类别和评估指标上均显著优于所有基线。例如，在<code>Hidden Inside</code>任务上成功率达90%（基线最高20%），在<code>Recursive Search</code>任务上成功率达80%（基线最高10%），在<code>Compositional Reasoning</code>复杂任务上成功率也达到70%（基线均为0%）。这证明了其在有限感知条件下处理复杂真实场景的鲁棒性和适应性。</p>
<p>消融实验评估了主动感知和记忆模块的有效性。在<code>Hidden Inside</code>任务上，对比了固定单摄像头、固定三摄像头、基于网格的主动感知（AP-VLM）和本文方法。结果显示，本文的主动感知模块能更快、更高效地发现物体。在<code>Semantic Targeting</code>和<code>Sequential Retrieval</code>任务上，消融了记忆模块（即不保留历史记忆，每次重新建图）。结果表明，拥有记忆模块的系统在语义目标任务中能利用语义线索进行针对性探索，在顺序检索任务中能复用先前记忆避免冗余探索，从而显著提高成功率和效率。</p>
<p><img src="https://arxiv.org/html/2508.12916v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：(A-C) 消融研究中使用的任务变体。(D) 主动感知模块与基线的消融结果对比（物体发现率随时间变化）。(E) 记忆模块的消融结果（成功率对比）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了一个新颖的机器人框架，能够动态更新图形式记忆、假设未探索区域，并协调主动与交互感知策略，实现仅用单RGB-D摄像头的有效目标检索。2）引入了一种新颖的视觉提示方案，将自然语言任务描述与接地的3D场景知识连接起来，使推理VLM能够执行任务感知、场景接地的主动感知。3）设计了一种持续更新的分层动态场景图，以捕捉物体的语义、几何和关系信息，实现逐步细化的场景理解。</p>
<p>论文提到的局限性包括：系统依赖于大型VLM（GPT-4o/GPT-o3），可能产生幻觉或错误推理；动作基元库目前有限，可能无法处理所有类型的交互；实验主要在桌面场景进行，在更复杂、非结构化的环境中的泛化能力有待进一步验证。</p>
<p>本文的研究启示在于：为单摄像头具身智能探索提供了一个强大的集成框架，证明了动态场景图作为记忆表征在复杂任务规划中的有效性。其基于VLM的视觉提示方案为将高级语义推理与低级几何控制相结合提供了新思路。未来的工作可以扩展动作基元、探索更高效或轻量化的记忆更新机制，并将框架应用于更广泛的移动操作或非结构化环境任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RoboRetriever框架，解决仅使用单个腕戴式RGB-D摄像头和自然语言指令，在杂乱、部分可观察的真实环境中检索目标物体的核心问题。其关键技术是构建并更新动态分层场景图以编码物体语义、几何与关系，并协调一个集成动作模块。该模块结合了基于任务感知场景的主动感知（通过视觉提示方案确定6-DoF相机位姿）、交互感知与操作。实验表明，该系统在多样化真实检索任务中，仅凭单一摄像头即展现出强大的适应性与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.12916" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>