<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.10423" target="_blank" rel="noreferrer">2508.10423</a></span>
        <span>作者: Liu, Qi, Zhang, Xiaopeng, Tan, Mingshan, Ma, Shuaikang, Ding, Jinliang, Li, Yanjie</span>
        <span>日期: 2025/08/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，双足机器人运动控制的主流方法主要分为基于模型的方法和基于学习的方法。基于学习的方法又可进一步分为两类：1）分阶段策略训练与系统集成方法，该方法将上下半身控制解耦训练再集成，但可能导致全身协调性不足，对复杂任务的适应性受限；2）基于模仿学习或单智能体深度强化学习的全身运动方法，这类方法依赖高质量的人类运动数据，存在采集成本高、泛化能力有限的问题，且未能充分利用强化学习的试错探索机制。具体到应用深度强化学习控制单个双足机器人时，主流方法是采用单智能体深度强化学习算法，但这种方法在处理机器人系统内在的协调挑战时可能存在局限。</p>
<p>本文针对上述痛点，提出了一个全新的视角：将单个双足机器人的运动学习建模为一个协作-异构的多智能体强化学习问题。本文的核心思路是将机器人的每个肢体（双臂和双腿）视为一个独立的智能体，在一个协作的多智能体强化学习框架中进行训练，通过经验共享和协作策略学习来优化整体运动性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为 MASH。其核心是将单个双足机器人的运动控制问题建模为一个部分可观测的分散式马尔可夫决策过程。在该框架下，每个肢体（左腿、右腿、左臂、右臂）被定义为一个独立的智能体。这些智能体共享一个全局的奖励函数，但各自拥有独立的策略网络（演员）和局部观察。训练采用集中训练分散执行的范式，使用一个能够访问全局状态的批评家网络来指导各个智能体的策略更新。</p>
<p><img src="https://arxiv.org/html/2508.10423v1/marl_model.png" alt="MARL模型"></p>
<blockquote>
<p><strong>图1</strong>：单个双足机器人的多智能体强化学习模型。将机器人的四肢（左腿、右腿、左臂、右臂）分别建模为独立的智能体。每个智能体基于自身的局部观察输出动作（关节扭矩），所有智能体的动作共同作用于机器人身体，并从环境中获得一个共享的全局奖励。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.10423v1/mash_method.png" alt="MASH框架"></p>
<blockquote>
<p><strong>图2</strong>：MASH方法框架。左侧为训练阶段：每个智能体的演员网络基于其局部观察生成动作，所有动作汇聚并输入到物理仿真器中；一个集中的批评家网络接收全局状态信息，用于计算优势函数并更新所有演员网络的参数。右侧为执行阶段：训练好的演员网络被部署到真实机器人上，每个肢体智能体基于其局部观察独立地生成控制动作。</p>
</blockquote>
<p>整体框架基于多智能体近端策略优化算法。创新点具体体现在：1）<strong>问题重构</strong>：首次将协作-异构多智能体强化学习应用于单个双足机器人的运动控制，将四肢视为独立但需协作的智能体。2）<strong>网络结构设计</strong>：采用了共享参数的演员网络，即双腿共享一个演员网络参数，双肩共享另一个演员网络参数。这既降低了计算开销，又自然地编码了人体左右对称的物理约束和协调需求。3）<strong>观察空间增强</strong>：为每个智能体的独立观察增加了共享信息，如躯干的欧拉角、角速度、时序指导器以及智能体ID，以提升协调能力。</p>
<p><strong>技术细节</strong>：</p>
<ul>
<li><strong>状态与观察</strong>：演员网络的输入是每个智能体的局部观察。对于腿部智能体，观察包括关节位置/速度、上一时刻动作、时序指导、躯干姿态/角速度、控制命令（如目标速度）和智能体独热编码，共32维，因此双腿演员网络总输入为64维。对于臂部智能体，观察共26维，双肩演员网络总输入为52维。批评家网络的输入是106维的全局状态，包括所有关节信息、躯干状态、控制命令、外部扰动（力/扭矩）以及环境物理参数（摩擦、质量）等。</li>
<li><strong>动作空间</strong>：双腿演员网络输出12维连续动作（对应12个腿部关节的扭矩），双肩演员网络输出8维动作。</li>
<li><strong>奖励函数</strong>：设计了一个包含多项奖励和惩罚的综合奖励函数，用于引导机器人学习稳健、高效的运动。关键项包括关节位置跟踪、线速度和角速度跟踪奖励，以及惩罚项如关节扭矩、速度、加速度（鼓励节能平滑）、足部空中时间、离地高度、触地数量（促进协调步态），以及朝向、碰撞、足部打滑、基座高度等稳定性与安全项，还有动作平滑度和扭矩变化率惩罚。</li>
<li><strong>时序指导器</strong>：用于协调四肢的运动节奏，定义为正弦函数 <code>T_i(t) = sin(2π(kt + Δ_i))</code>，其中k是步态周期缩放因子，Δ_i是第i个肢体的相位偏移。</li>
<li><strong>仿真到现实</strong>：采用了域随机化技术来增强策略的鲁棒性，包括在环境初始化时随机化物理参数（如质量、惯性、摩擦），以及在每个仿真步随机化动作延迟、扭矩噪声和外部扰动。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Isaac Gym仿真环境中进行，使用BanXing双足机器人模型。对比的基线方法包括：1）<strong>单智能体PPO</strong>：将整个机器人视为一个智能体进行训练。2）<strong>分阶段训练</strong>：先独立训练下半身行走策略，收敛后再集成训练上半身策略。</p>
<p>实验结果表明，MASH方法在训练效率和最终性能上均优于基线方法。具体表现为训练曲线收敛更快，并且能够获得更高的平均回报。</p>
<p><img src="https://arxiv.org/html/2508.10423v1/sim.png" alt="仿真训练曲线"></p>
<blockquote>
<p><strong>图3</strong>：在仿真中的训练曲线对比。MASH方法（蓝色曲线）相比单智能体PPO（红色曲线）和分阶段训练（绿色曲线），收敛速度更快，并且最终达到的平均回报值更高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.10423v1/x1.png" alt="实验1"></p>
<blockquote>
<p><strong>图4</strong>：实验场景一：平地行走。展示了MASH控制下的机器人步态。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.10423v1/x2.png" alt="实验2"></p>
<blockquote>
<p><strong>图5</strong>：实验场景二：携带负载行走。展示了机器人双臂协调配合，稳定搬运箱子的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.10423v1/x3.png" alt="实验3"></p>
<blockquote>
<p><strong>图6</strong>：实验场景三：抗扰动测试。展示了机器人在受到侧向推力时，能够迅速调整姿态恢复稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.10423v1/x4.png" alt="实验4"></p>
<blockquote>
<p><strong>图7</strong>：实验场景四：复杂地形行走。展示了机器人在有起伏的坡道上行走。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：通过定性实验演示，MASH方法使机器人能够完成稳健的平地行走、双臂协调携物行走、抵抗外部推力扰动以及在起伏坡道上行走等任务。这些结果验证了该方法在提升运动协调性、鲁棒性和对复杂任务适应性方面的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了MASH框架，创新性地将单个双足机器人的运动学习重构为一个协作-异构的多智能体强化学习问题。2）设计了一种共享参数演员与全局批评家的网络架构，在保持多智能体协作学习优势的同时，契合了机器人身体的对称性与协调约束。3）实验证明，该方法相比传统单智能体方法，能加速训练收敛并提升全身协调运动能力。</p>
<p>论文自身提到的局限性主要在于实验目前主要在仿真中进行，虽然采用了域随机化，但仿真到现实的迁移性能仍需在更广泛的物理机器人测试中进一步验证。</p>
<p>本工作对后续研究的启示在于：为复杂高自由度机器人的控制提供了一种新的“内部协作”范式。未来可以探索将更多身体部位（如躯干、头部）或更细粒度的关节组定义为智能体，以处理更复杂的全身协同任务；同时，该框架也有潜力扩展到需要紧密协调运动与操作的移动操纵任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对单个人形机器人运动优化问题，提出MASH方法。传统单智能体强化学习或分阶段训练存在全身协同不足的局限。MASH采用合作异构多智能体强化学习，将四肢视为独立智能体，通过共享全局批评器进行协同训练。实验表明，该方法能加速训练收敛，提高全身合作能力，性能优于传统单智能体强化学习方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.10423" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>