<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.07339" target="_blank" rel="noreferrer">2602.07339</a></span>
        <span>作者: Ganesh Krishnasamy Team</span>
        <span>日期: 2026-02-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>自动驾驶轨迹规划领域的主流方法正从传统的基于物理模型和规则的方法转向基于学习的方法。模仿学习（IL）虽然能学习人类驾驶行为，但难以处理多模态分布，容易导致“模型坍塌”，产生平均化、不安全的轨迹。离线强化学习（Offline RL）通过奖励函数优化策略，但面临行为正则化的挑战，传统方法容易产生“模式覆盖”，选择训练数据分布之外的危险动作。近年来，扩散模型因其强大的多模态分布建模能力成为最先进的轨迹规划方法，例如DiffusionPlanner。然而，扩散模型依赖迭代的随机采样过程，导致推理延迟高（例如100毫秒），无法满足安全关键型自动驾驶系统对实时性和确定性的严苛要求。</p>
<p>本文针对扩散模型在实时部署中面临的高延迟和不确定性痛点，提出了一种新的视角：将预训练的扩散模型作为行为先验，通过策略蒸馏提取一个高效、确定性的规划策略。核心思路是利用分数正则化策略优化（SRPO）算法，直接利用扩散模型的分数函数来正则化策略学习，同时引入一个基于预测驾驶员模型（PDM）的评论家来提供密集的、以安全为中心的监督，从而在保持性能的同时实现8倍加速的实时确定性规划。</p>
<h2 id="方法详解">方法详解</h2>
<p>RAPiD框架的整体流程分为三个阶段：离线回放缓冲区构建、评论家训练和确定性策略训练。</p>
<p><img src="https://arxiv.org/html/2602.07339v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：RAPiD框架总览。第一阶段（离线回放缓冲区构建）：原始传感器数据通过冻结的DiffusionPlanner编码器处理，生成丰富的潜在状态嵌入（s）。真实轨迹由PDM评分器评估，根据安全和舒适度指标分配奖励（r），创建带分数的数据集。第二阶段（评论家训练）：通过隐式Q学习，评论家学习估计Q值 ℒ_Q(φ)，基于冻结嵌入（s）评估真实轨迹（a），有效区分安全（高PDM分）和不安全行为。第三阶段（确定性策略训练）：一个基于Transformer的策略通过代理损失梯度 ∇_θℒ_π^surr(θ) 进行蒸馏。该梯度融合了评论家的指导（最大化安全奖励）和冻结扩散先验的分数函数，将策略正则化到真实的行为流形上，从而实现快速的一步确定性推理。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>预训练扩散先验</strong>：采用已训练的DiffusionPlanner作为冻结的行为先验模型。它基于扩散Transformer（DiT）架构，能对复杂的、多模态的人类驾驶行为分布进行建模。其分数函数 ∇_x log μ(x|s) 提供了行为分布密度的梯度信息。</li>
<li><strong>基于PDM的评论家</strong>：为了获得与闭环安全强相关的奖励信号，评论家（Q函数）的训练不依赖于标准的nuPlan指标，而是通过模仿一个预测驾驶员模型（PDM）控制器来进行。具体使用隐式Q学习（IQL），其损失函数为：ℒ_Q(φ) = 𝔼_(s,a)∼𝒟[L_2^τ(Q_φ(s,a) - (r + γV_ψ(s‘)))]，其中目标值V通过期望回归学习：ℒ_V(ψ) = 𝔼_(s,a)∼𝒟[L_2^τ(Q_φ(s,a) - V_ψ(s))]。这使评论家能够评估轨迹在安全和舒适度（PDM指标）方面的优劣。</li>
<li><strong>分数正则化策略优化（SRPO）</strong>：这是策略蒸馏的核心。目标是学习一个确定性策略π_θ。SRPO通过一个反向KL散度目标进行优化，该目标本质上是模式寻求的：max_θ ℒ_π(θ) = 𝔼_(s∼𝒟<em>μ, a∼π_θ)[Q_φ(s,a)] - (1/β) D_KL[π_θ(·|s) || μ(·|s)]。其中，KL散度项通过扩散模型的分数函数进行近似：∇_θ D_KL ≈ 𝔼_s[∇_a log μ(a|s)|</em>(a=π_θ(s)) · ∇_θ π_θ(s)]。这允许直接利用扩散先验的梯度来正则化策略，使其输出贴近数据分布中的高密度区域，而无需进行耗时的扩散采样。策略网络采用Transformer架构。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>首次将SRPO应用于自动驾驶轨迹规划</strong>：将最先进的扩散规划器的生成能力蒸馏到确定性策略中。</li>
<li><strong>利用分数函数进行直接正则化</strong>：避免了传统策略蒸馏或离线RL中所需的从扩散模型采样动作的过程，极大地提高了效率。</li>
<li><strong>安全导向的奖励塑造</strong>：使用基于PDM的评论家提供奖励，确保了策略优化直接面向真实的驾驶安全与舒适性，而非仅仅追求基准测试分数。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：nuPlan大规模自动驾驶数据集及其闭环仿真基准。</li>
<li><strong>评估指标</strong>：主要使用基于预测驾驶员模型（PDM）的闭环评估指标，关注碰撞率、可行驶区域合规性和进度，而非开环位移误差。同时在interPlan基准上测试泛化能力。</li>
<li><strong>对比方法</strong>：与扩散模型基线DiffusionPlanner进行主要对比，同时也与其他基于学习的规划器进行比较。</li>
<li><strong>实验平台</strong>：在nuPlan的多个闭环评估分割上测试，包括val14、test14和test14-hard。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在非反应式场景（val14, test14, test14-hard）中，RAPiD达到了与DiffusionPlanner基线相竞争的性能，同时实现了超过<strong>8倍的推理加速</strong>。在interPlan基准测试中，RAPiD在基于学习的规划器中取得了最先进的泛化性能。</p>
<p><img src="https://arxiv.org/html/2602.07339v1/x1.png" alt="定性对比1"></p>
<blockquote>
<p><strong>图1</strong>：定性对比。(a-1)和(a-2)展示了基线DiffusionPlanner的局限性。其计算密集的随机采样（100毫秒延迟）导致反应延迟，最终发生碰撞。(b-1)和(b-2)展示了RAPiD框架。通过将扩散先验蒸馏为基于PDM安全指标训练的确定性策略，RAPiD实现了8倍加速，能够及时做出决策，完成平滑、无碰撞的机动。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07339v1/following_lane_with_lead_diffusion_planner.png" alt="跟随前车场景"><br><img src="https://arxiv.org/html/2602.07339v1/following_lane_with_lead_our_method.png" alt="跟随前车场景"></p>
<blockquote>
<p><strong>图3 &amp; 图4</strong>：跟随前车场景对比。左图（DiffusionPlanner）规划的轨迹（绿线）在多个采样步骤间波动较大，导致控制不平稳。右图（RAPiD）的轨迹更加平滑稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07339v1/stopping_with_lead_diffusion_planner.png" alt="停车场景"><br><img src="https://arxiv.org/html/2602.07339v1/stopping_with_lead_our_method.png" alt="停车场景"></p>
<blockquote>
<p><strong>图5 &amp; 图6</strong>：停车场景对比。左图显示DiffusionPlanner的轨迹在停车时存在不必要的侧向摆动。右图显示RAPiD的轨迹更笔直，停车行为更舒适自然。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07339v1/changing_lane_diffusion_planner.png" alt="换道场景"><br><img src="https://arxiv.org/html/2602.07339v1/changing_lane_our_method.png" alt="换道场景"></p>
<blockquote>
<p><strong>图9 &amp; 图10</strong>：换道场景对比。左图中DiffusionPlanner的轨迹侵入相邻车道过多。右图中RAPiD的换道轨迹更贴合目标车道中心。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07339v1/diffPlan_near_col.png" alt="碰撞风险对比"><br><img src="https://arxiv.org/html/2602.07339v1/RAPiD_clear.png" alt="碰撞风险对比"></p>
<blockquote>
<p><strong>图11 &amp; 图12</strong>：高风险场景对比。左图DiffusionPlanner规划的轨迹（绿线）与预测的他车轨迹（红点）非常接近，有碰撞风险。右图RAPiD规划的轨迹保持了更安全的距离。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07339v1/acc_plot_updated.png" alt="运动曲线对比"></p>
<blockquote>
<p><strong>图7</strong>：加速度曲线对比。RAPiD（蓝线）的加速度曲线比DiffusionPlanner（橙线）更平滑，突变更少，表明乘坐舒适性更佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07339v1/speed_plot_updated.png" alt="运动曲线对比"></p>
<blockquote>
<p><strong>图8</strong>：速度曲线对比。两者速度曲线相似，但RAPiD同样表现出平滑的特性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07339v1/yaw_rate_updated.png" alt="运动曲线对比"></p>
<blockquote>
<p><strong>图13</strong>：横摆角速度曲线对比。RAPiD的横摆角速度变化更平缓，意味着转向更柔和。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07339v1/x3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图15</strong>：消融实验（碰撞率）。对比了完整RAPiD、不使用扩散先验分数正则化（w/o Diff Prior）、以及不使用PDM评论家（w/o PDM Critic）的变体。完整模型在test14-hard上碰撞率最低，表明扩散先验和PDM评论家都对提升安全性有贡献。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07339v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图16</strong>：消融实验（进度得分）。完整模型在进度方面也表现最佳，说明其能在保证安全的前提下有效前进。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>消融实验表明，移除扩散先验的正则化（w/o Diff Prior）或移除PDM评论家（w/o PDM Critic）都会导致性能下降，碰撞率上升，进度得分降低。这验证了<strong>扩散行为先验</strong>和<strong>安全导向的评论家</strong>都是RAPiD框架不可或缺的组件，共同确保了策略在行为逼真度和安全性上的平衡。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了RAPiD框架，首次将分数正则化策略优化（SRPO）应用于自动驾驶轨迹规划，成功地将最先进的扩散规划器蒸馏成一个高效的确定性策略，在保持性能的同时实现了超过8倍的推理加速。</li>
<li>证明了基于分数的梯度正则化能够有效桥接扩散模型的表达能力和确定性策略的计算效率，为安全关键的实时自动驾驶规划提供了新方案。</li>
<li>采用以安全为中心的PDM指标进行评估，在nuPlan非反应式场景中展示了有竞争力的性能，并分析了在反应式场景中存在的性能差距及知识蒸馏的挑战。</li>
</ol>
<p><strong>局限性</strong>：<br>论文指出，在高度动态的“反应式”场景中，RAPiD的性能与扩散基线相比存在差距。这揭示了从生成式模型中提取知识到确定性策略时所面临的挑战，即确定性策略在应对需要高度多模态推理的突发状况时，其灵活性和多样性可能不足。</p>
<p><strong>后续研究启示</strong>：</p>
<ul>
<li>如何改进蒸馏方法，使确定性策略能更好地保留生成式模型在处理极端多模态和反应式场景下的能力，是一个关键方向。</li>
<li>可以探索将本文的蒸馏框架与其他高效架构（如更小的模型、硬件感知优化）结合，进一步推动实时部署。</li>
<li>安全评估范式从开环指标转向PDM等闭环安全指标的趋势值得延续和深化。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RAPiD框架，旨在解决扩散模型轨迹规划器因迭代随机采样导致实时性差、难以部署于安全关键自动驾驶系统的问题。其核心方法是通过分数正则化策略优化，将预训练扩散规划器的分数函数作为行为先验，蒸馏出一个确定性高效策略，并引入模仿预测性驾驶员控制器的评论家网络提供密集安全监督。实验表明，RAPiD在nuPlan闭环场景中性能与扩散基线相当，推理速度提升8倍，并在interPlan基准上实现了基于学习的规划器中最优的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.07339" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>