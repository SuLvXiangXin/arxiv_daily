<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.08416" target="_blank" rel="noreferrer">2506.08416</a></span>
        <span>作者: Lijun Zhu Team</span>
        <span>日期: 2025-06-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>双足机器人稳定高效的步态生成，尤其是在动态非结构化环境中，仍是一个基本挑战。传统方法严重依赖基于模型的优化或启发式规则，虽然能提供理论保证，但往往缺乏对精确动力学的适应性或计算复杂度过高。近年来，强化学习通过环境交互学习运动策略，提供了有前景的替代方案。然而，纯粹基于学习的方法存在显著局限，包括可解释性差、训练时间长，且生成的步态常表现出不规则的周期性，难以部署到物理平台上。一个关键的研究空白在于弥合基于模型的鲁棒性与数据驱动适应性之间的鸿沟。现有混合方法往往难以平衡高维人形系统实时全身关节轨迹规划的效率与学习过程的探索需求。</p>
<p>本文针对上述挑战，提出了一种将实时步态规划与强化学习中结构化奖励组合相结合的新框架。其核心思路是：通过动力学解耦与简化模型（H-LIP）实时生成物理可行的参考关节轨迹，并以此为基础设计组合奖励函数，引导强化学习策略高效收敛至周期性双足步态。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个由实时步态生成驱动的训练框架，整体流程如图2所示。首先，将3D人形机器人模型解耦为两个2D平面模型（X模型和Y模型），并分别近似为混合倒立摆模型进行轨迹规划，生成各模型的关节轨迹，再组合成全模型的期望关节轨迹。最后，在机器人运动学习框架内，结合生成的关节轨迹设计奖励函数，训练强化学习策略。</p>
<p><img src="https://arxiv.org/html/2506.08416v2/pipline.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：实时步态驱动的训练框架。包含步态设计和网络训练两部分。首先将机器人模型解耦为X轴和Y轴模型，近似为H-LIP生成关节轨迹，组合后得到最终轨迹。随后，在学习框架中设计包含该轨迹信息的奖励函数。</p>
</blockquote>
<p><strong>核心模块1：机器人模型解耦与H-LIP近似</strong><br>假设手臂质量可忽略，上身视为刚体。如图3所示，将人形机器人解耦为一个沿x轴的五连杆平面机器人（X模型）和一个沿y轴的三连杆平面机器人（Y模型）。定义广义坐标用于后续轨迹设计。尽管两个模型的动力学存在正交方向的耦合，但论文假设其影响很小。</p>
<p><img src="https://arxiv.org/html/2506.08416v2/decoupled_diagram.jpg" alt="解耦模型"></p>
<blockquote>
<p><strong>图3</strong>：从机械结构解耦出的模型。左侧为X模型（五连杆），右侧为Y模型（三连杆）。</p>
</blockquote>
<p>H-LIP是一个质心高度恒定、具有两条伸缩腿的点质量模型（图4c）。行走由单足支撑阶段和双足支撑阶段组成。论文假设双足支撑阶段是瞬时的，发生在摆动腿触地时。单足支撑阶段的动力学是线性的，其闭合形式解已知。通过设定质心高度恒定以及步态周期始末状态满足特定关系（公式15），可使机器人动力学近似遵循H-LIP动力学，这为步态规划器提供了理论基础。</p>
<p><img src="https://arxiv.org/html/2506.08416v2/SSP_DSP.jpg" alt="步态相位与H-LIP"></p>
<blockquote>
<p><strong>图4</strong>：(a) 单足支撑阶段；(b) 双足支撑阶段；(c) 混合倒立摆模型。</p>
</blockquote>
<p><strong>核心模块2：基于H-LIP的实时步态规划器</strong><br>规划器的目标是设计一类满足H-LIP动力学的步态。对于X模型，论文通过定理IV.1给出了期望关节轨迹需满足的5个属性，这些属性共同保证了机器人动力学近似为H-LIP动力学且满足触地条件。为实现实时生成，采用贝塞尔多项式参数化轨迹，并通过引入特定条件（如公式33，即触地时大腿与 stance 腿、大腿与 swing 腿的角度相等）和推论IV.1，将复杂的非线性优化问题转化为一个可通过算法1高效求解的参数计算过程。该算法在给定步长和步行速度后，能快速计算出贝塞尔多项式的所有参数，从而实时生成关节轨迹。Y模型的设计思路类似，但因其为三连杆模型，规划更为简单。</p>
<p><img src="https://arxiv.org/html/2506.08416v2/phase_Y_model.jpg" alt="Y模型相位图"></p>
<blockquote>
<p><strong>图5</strong>：Y模型的相位图。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.08416v2/Y_model_des.jpg" alt="Y模型期望轨迹"></p>
<blockquote>
<p><strong>图6</strong>：Y模型的期望关节轨迹。</p>
</blockquote>
<p><strong>核心模块3：基于奖励组合的强化学习框架</strong><br>学习框架如图7所示。策略网络以机器人的状态观测为输入，输出关节扭矩。核心创新在于设计了一个由三部分组成的奖励函数：</p>
<ol>
<li><strong>周期性奖励</strong>：鼓励关节角度与步态规划器生成的参考轨迹之间的误差具有周期性，通过计算当前误差与前一周期对应相位误差的差异来实现。</li>
<li><strong>时间效率奖励</strong>：惩罚智能体花费过长的时间完成一个步态周期，鼓励快速行走。</li>
<li><strong>跟踪奖励</strong>：直接最小化当前关节角度与参考轨迹之间的误差。<br>总奖励是这三个奖励的加权和。这种组合奖励明确地引导策略学习周期性、时间高效且跟踪一致的步态。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.08416v2/framework.jpg" alt="学习框架"></p>
<blockquote>
<p><strong>图7</strong>：机器人运动学习框架。策略网络接收状态观测，输出关节扭矩。奖励函数由周期性奖励、时间效率奖励和跟踪奖励组合而成。</p>
</blockquote>
<p><strong>创新点总结</strong></p>
<ol>
<li><strong>动力学解耦与实时规划</strong>：将复杂的3D规划问题分解为两个可实时求解的2D问题，通过H-LIP近似和贝塞尔多项式参数化，实现了高维关节轨迹的快速生成。</li>
<li><strong>结构化奖励设计</strong>：提出的组合奖励函数系统地融合了周期性、时间效率和轨迹跟踪目标，直接针对强化学习训练中的常见问题（如局部最优、行为不稳定）进行优化，显著提高了学习效率。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真平台</strong>：PyBullet 物理引擎。</li>
<li><strong>机器人模型</strong>：自定义的12自由度人形机器人（如图1）。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>PPO</strong>：使用常见启发式奖励（如前进速度、保持直立）的基准策略。</li>
<li>**PPO + Gait Tracking (GT)**：在PPO基础上增加关节轨迹跟踪奖励。</li>
<li>**PPO + Periodic + Gait Tracking (PGT)**：在PPO基础上增加周期性和轨迹跟踪奖励（即本文的完整奖励组合）。</li>
</ul>
</li>
<li><strong>评估指标</strong>：训练曲线（累积奖励）、步态周期性指标（通过相位图分析）、步行速度、步长等。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.08416v2/Mechanical_structure.jpg" alt="机器人结构"></p>
<blockquote>
<p><strong>图1</strong>：人形机器人的电机分布（左）和机械结构（右）。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>学习效率对比</strong>：如图12和图13所示，使用完整奖励组合（PGT）的方法收敛速度最快，且最终性能（累积奖励）最高。纯PPO方法收敛慢且性能差，仅添加跟踪奖励（GT）有所改善，但不如PGT。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.08416v2/train_1.jpg" alt="训练曲线1"></p>
<blockquote>
<p><strong>图12</strong>：不同奖励设置下的训练曲线对比。PGT（本文方法）收敛最快且最终奖励最高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.08416v2/train_2.jpg" alt="训练曲线2"></p>
<blockquote>
<p><strong>图13</strong>：训练曲线对比（另一视角）。再次证实PGT的优越性。</p>
</blockquote>
<ol start="2">
<li><strong>步态周期性分析</strong>：如图14所示，PGT方法生成的步态在相位图上呈现出清晰、闭合的极限环，表明其具有高度周期性和稳定性。而PPO和GT方法的相位图则杂乱无章，缺乏周期性。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.08416v2/x5.png" alt="相位图对比"></p>
<blockquote>
<p><strong>图14</strong>：步态周期性分析（相位图）。PGT方法产生清晰闭合的极限环，而PPO和GT方法则非常杂乱。</p>
</blockquote>
<ol start="3">
<li><strong>双足行走成功学习</strong>：图15展示了采用本文框架（PGT奖励）成功学习到的稳定、周期性双足步态。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.08416v2/train_biped.jpg" alt="双足步态"></p>
<blockquote>
<p><strong>图15</strong>：使用PGT奖励学习到的双足行走步态。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：图16的消融实验表明，周期奖励和时间效率奖励都对最终性能有重要贡献。缺少其中任何一个，学习速度都会变慢，最终步态的周期性或效率会下降。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.08416v2/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图16</strong>：奖励函数的消融研究。移除周期性奖励或时间效率奖励都会导致性能下降。</p>
</blockquote>
<ol start="5">
<li><strong>仿真与实验验证</strong>：论文还将训练好的策略迁移到实际机器人进行测试。图17和图18显示，机器人能够实现稳定的原地踏步和向前行走，验证了方法的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.08416v2/exp_1.jpg" alt="实验1"></p>
<blockquote>
<p><strong>图17</strong>：实际机器人实验：原地踏步。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.08416v2/exp_3.jpg" alt="实验2"></p>
<blockquote>
<p><strong>图18</strong>：实际机器人实验：向前行走。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种动力学解耦策略，将3D人形运动简化为计算上易处理的2D H-LIP近似，从而在平衡和运动学约束下生成可行的参考轨迹。</li>
<li>设计了一种多目标奖励组合，明确促进周期性、时间高效和跟踪一致的步态，解决了强化学习中的局部最优和 erratic 行为等常见挑战。</li>
<li>通过大量的基准测试验证了方法在仿真环境中的优越学习效率和运动质量，并展示了部署到物理人形平台上的潜力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文在讨论部分提到，当前的步态规划器主要针对平坦地面行走设计，未考虑外部扰动（如推动）或复杂地形（如斜坡、楼梯）。对于这些情况，需要扩展规划器或引入额外的适应性模块。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>框架扩展</strong>：可将此“模型简化规划+奖励组合学习”的框架扩展到更复杂的运动技能（如跑步、跳跃、抗扰动恢复）和非结构化地形导航中。</li>
<li><strong>规划器增强</strong>：探索将更复杂的模型（如带有角动量的模型）或在线适应性机制集成到实时规划器中，以处理动态环境。</li>
<li><strong>奖励自动化</strong>：进一步研究如何自动化或半自动化地设计此类结构化奖励，减少对领域知识的手工依赖。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在动态非结构化环境中实现稳定、周期性双足步态的核心挑战，提出了一种步态驱动的强化学习框架。关键技术包括：1）新型步态规划器，将3D模型解耦为两个2D混合倒立摆（H-LIP）以规划期望关节轨迹；2）基于该规划器设计了三种奖励函数，构成奖励组合以引导学习。该方法在仿真与实验中有效减少了学习时间，并提升了运动的周期性与整体性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.08416" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>