<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.21378" target="_blank" rel="noreferrer">2508.21378</a></span>
        <span>作者: Yuanchao Shu Team</span>
        <span>日期: 2025-08-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用大型语言模型（LLMs）赋能机器人操作已成为一个活跃的研究领域。主流方法通过LLM理解用户指令并生成相应的策略代码（Policy Code）来控制机器人执行任务，这种方法具有设计复杂度低、解决效率高的特点。尽管LLMs在推理和代码生成方面展现出强大能力，但在真实世界任务需求多样性和用户指令内在复杂性的影响下，实现可靠的策略代码生成仍是一个重大挑战。现有研究主要关注对抗性攻击（如越狱）带来的安全风险，但忽略了良性指令的自然变化以及任务复杂性对机器人操作可靠性的影响。</p>
<p>本文针对LLM生成策略代码的可靠性这一具体痛点，提出了一个系统性评估的新视角。核心思路是设计一个名为RoboInspector的评估流水线，从操作任务复杂性和用户指令粒度两个维度出发，全面揭示并表征LLM使能的机器人操作中策略代码的不可靠性，识别导致失败的具体行为模式，并基于反馈提出精炼方法以提升可靠性。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboInspector的整体框架是一个系统性的评估流水线，旨在分析任务复杂性和指令粒度如何影响LLM生成策略代码的可靠性。其输入是预先定义的不同复杂度的任务、不同粒度的指令以及选定的LLM模型；输出是任务执行的成功率以及对失败案例中不可靠行为的分类与统计。</p>
<p><img src="https://example.com/robotics-pipeline-image-url" alt="RoboInspector Pipeline"></p>
<blockquote>
<p><strong>图2</strong>：RoboInspector评估流水线。绿色模块表示LLM生成的正确级联代码，红色模块表示四种不可靠行为对应的错误代码示例。流程从用户指令开始，经过LLM Planner理解分解，Composer生成步骤代码，最后由Low-level Actuator执行。</p>
</blockquote>
<p>核心模块包括任务选择、指令构建和结果处理：</p>
<ol>
<li><strong>任务选择与复杂度定义</strong>：通过分析分解常见机器人操作，论文指出几乎所有操作都由抓取（Grasp）、移动（Move）和旋转（Rotate）三个基本动作组合而成。基于此，选择了8个任务（T={T1, T2, ..., T8}），包括3个仅含单一基本动作的简单任务（Grasp, Movement, Rotation）和5个来自RLBench的复合任务（如SlideBlockToTarget, OpenWineBottle）。任务复杂度定义为该任务所包含的<strong>不同</strong>基本动作的数量，即 f_φ(T_i) = |T_i| ∈ {1,2,3}。</li>
<li><strong>指令构建与粒度定义</strong>：用户指令被形式化地定义为四元组 I:=(O, A, P, C)，分别代表对象（Object）、动作（Action）、目的（Purpose）和条件（Condition）。基于此构建了三种不同粒度的指令：<ul>
<li><strong>I_A</strong> (粒度=2): 仅包含对象和动作（如“throw the rubbish”）。</li>
<li><strong>I_P</strong> (粒度=3): 包含对象、动作和目的（如“drop the rubbish into the bin”）。</li>
<li><strong>I_C</strong> (粒度=4): 包含对象、动作、目的和条件（如“Grasp the rubbish and place it in the bin, with the executable space defined as (100, 100, 100)”）。<br>指令粒度定义为指令中包含的非空元素的数量，即 f_θ(I_i) = |{I_i ∈ {O,A,P,C} | I≠∅}|。在评估时，指令会与一段固定的演示代码（Demonstration Code）结合，构成完整提示词输入LLM。</li>
</ul>
</li>
<li><strong>结果处理与不可靠行为识别</strong>：LLM根据提示词生成策略代码，其典型流程是：Planner理解指令并分解任务步骤 → Composer为每个步骤生成执行代码 → Low-level Actuator调用API控制机器人。论文通过分析失败案例中的生成代码和执行过程，识别并定义了四种导致操作失败的不可靠行为（见图2红色模块）。</li>
</ol>
<p>与现有方法相比，RoboInspector的创新点在于首次系统性地从“任务复杂性”和“指令粒度”这两个正交维度构建评估组合，并对LLM生成的策略代码进行细粒度的失败根因分析，而非仅仅关注最终成功率或对抗性安全威胁。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境（RLBench和PyBullet）中进行，使用了两个主流的LLM生成策略代码框架：VoxPoser和Code as Policies。评估涵盖了8个任务、3种指令粒度与8个LLM（包括GPT-3.5/4/4o系列、Qwen-max/plus/turbo系列以及开源的DeepSeek-V3）构成的168种组合。每个组合进行50次试验。Baseline即为这些LLM在给定任务和指令下的原始表现。</p>
<p>关键实验结果总结如下：</p>
<ol>
<li><strong>成功率受任务复杂度和指令粒度显著影响</strong>：如表2所示，对于仅含单一基本动作的简单任务（Grasp, Movement, Rotation），即使在最低粒度指令（I_A）下，平均成功率也约为70%，在使用最高粒度指令（I_C）时提升至约82%。而对于复合任务，成功率普遍更低，且任务越复杂（包含的基本动作越多），成功率越低。例如，最复杂的OpenWineBottle任务在I_A指令下的平均成功率仅为14%。<strong>对于同一任务，更高粒度的指令 consistently 带来更高的成功率</strong>，验证了指令粒度与可靠性的正相关性，以及任务复杂度与可靠性的负相关性。</li>
</ol>
<p><img src="https://example.com/table-image-url" alt="Comparison of manipulation success rate"></p>
<blockquote>
<p><strong>表2</strong>：在VoxPoser框架下，不同任务、指令和LLM组合的操纵成功率对比。加粗下划线标明了同一指令下成功率最高的模型，以及同一任务下平均成功率最高的指令。数据清晰展示了任务复杂度增加导致成功率下降，以及指令粒度提升有助于提高成功率。</p>
</blockquote>
<ol start="2">
<li><strong>四种不可靠行为的分布</strong>：图3展示了不同LLM在不同指令下，导致操作失败的四种不可靠行为的比例。</li>
</ol>
<p><img src="https://example.com/figure3-image-url" alt="Proportion of unreliable behaviors"></p>
<blockquote>
<p><strong>图3</strong>：不同模型在不同指令下，导致操纵失败的四种不可靠行为的比例分布图。可以看出，GPT-3.5-turbo和Qwen-turbo的失败主要由“Nonsense”行为导致；而在I_A指令下，所有模型的失败中“Disorder”行为占比较高；对于I_P指令，“Infeasible”行为较为突出。</p>
</blockquote>
<ol start="3">
<li><strong>不可靠行为定性示例</strong>：图4提供了四种不可靠行为的可视化案例。<ul>
<li><strong>Nonsense</strong>：LLM生成了不符合要求的“import”语句或无关文本，导致代码无法执行，机器人动作中止。</li>
<li><strong>Disorder</strong>：LLM生成了不合理的动作步骤序列（如先“打开夹爪”再“移动到垃圾桶”），导致任务逻辑失败。</li>
<li><strong>Infeasible</strong>：LLM基于感知模块提供的、超出机器人实际可达工作空间的目标位置生成了轨迹，导致机器人在边界停滞。</li>
<li><strong>Badpose</strong>：LLM/控制算法生成的轨迹未考虑末端执行器姿态对目标物体的影响（如开酒瓶时倾斜旋转），导致操作失败。</li>
</ul>
</li>
</ol>
<p><img src="https://example.com/figure4-image-url" alt="Examples of each unreliable behavior"></p>
<blockquote>
<p><strong>图4</strong>：四种不可靠行为的示例图像序列。(a) Nonsense行为导致操作中断；(b) Disorder行为导致动作顺序错误；(c) Infeasible行为导致机器人无法到达目标；(d) Badpose行为导致末端姿态不当。</p>
</blockquote>
<ol start="4">
<li><strong>精炼方法的有效性</strong>：论文进一步提出了一种基于失败策略代码和不可靠行为描述反馈的精炼方法。通过在模拟和真实世界环境中的评估，该方法将LLM使能的机器人操作成功率最高提升了35%。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了首个从任务复杂性和指令粒度两个维度系统评估LLM生成机器人策略代码可靠性的框架RoboInspector；2) 通过大规模实验（168种组合）揭示了可靠性与上述两个因素的相关性，并识别、表征了导致失败的四种具体不可靠行为（Nonsense, Disorder, Infeasible, Badpose）；3) 基于分析洞察，提出了一种反馈精炼方法，显著提升了操作成功率。</p>
<p>论文自身提到的局限性包括：实验主要关注静态环境下的操作，未考虑动态环境或移动基座机器人；指令变化主要在于文本描述，未深入探索多模态（如图像）指令的影响。</p>
<p>这项工作对后续研究具有重要启示：首先，在开发LLM使能的机器人系统时，应谨慎选择指令遵循能力强的模型，并提供尽可能清晰、目标明确的高粒度指令。其次，需要从系统层面考虑感知范围与执行工作空间的匹配，或对输入LLM的感知数据进行约束预处理。最后，针对识别出的不可靠行为设计针对性的缓解策略（如代码后处理、约束条件注入）是提高系统鲁棒性的有效途径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对大语言模型（LLM）驱动机器人操控时，其生成的策略代码不可靠的核心问题展开研究。论文提出RoboInspector分析框架，从任务复杂性和用户指令粒度两个维度，系统性地揭示并表征了策略代码生成中的不可靠性。通过在两个主流框架中进行168组综合实验，该方法识别出导致操控失败的四种主要不可靠行为。基于对失败代码的反馈，论文进一步提出一种改进方法，经仿真和真实环境评估，可将策略代码生成的可靠性提升高达35%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.21378" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>