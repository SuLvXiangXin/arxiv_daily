<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.08748" target="_blank" rel="noreferrer">2508.08748</a></span>
        <span>作者: Yukiyasu Domae Team</span>
        <span>日期: 2025-08-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在零售环境（如便利店）中，机器人执行抓取放置任务面临诸多挑战，包括物体密集排列、相互遮挡以及物体在颜色、形状、大小、纹理上的巨大差异。传统方法通常依赖预定义启发式规则、结构化环境或显式物体分割，在面对新物体和动态布局时缺乏适应性。本文针对在复杂、多变场景中实现高效、自适应机器人操作的痛点，提出了一种结合标注引导的视觉提示（Visual Prompting）与动作分块Transformer（ACT）的新视角。其核心思路是：使用边界框标注作为轻量级空间指引来简化感知，并利用基于模仿学习的ACT模型直接预测连贯的动作序列，从而替代传统的分步规划，实现更流畅、数据驱动的操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的系统是一个端到端的感知-行动流程。输入是带有视觉提示（即边界框标注）的RGB图像、机器人关节状态以及一个从变分自编码器（CVAE）采样的潜在风格变量<code>z</code>；输出是预测的一段分块动作序列（<code>a_{t:t+k}</code>），直接驱动机械臂和夹爪完成抓取放置任务。</p>
<p><img src="https://arxiv.org/html/2508.08748v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：基于ACT与视觉提示的模型架构。训练时，RGB图像被添加边界框提示（绿色为抓取目标，红色为放置目标）。一个CNN和Transformer编码器处理提示后的图像、关节状态以及潜在变量<code>z</code>。Transformer解码器生成对应的分块动作序列。</p>
</blockquote>
<p>核心模块是基于Action Chunking with Transformers (ACT)的模仿学习算法，其关键组件与流程如下：</p>
<ol>
<li><strong>视觉提示生成</strong>：在训练和推断时，向原始RGB图像<code>I_t</code>添加边界框，生成提示图像<code>I_t^vp</code>。边界框用于明确标识抓取物体（绿色）和放置位置（红色），为模型提供结构化空间指引。</li>
<li><strong>动作分块（Chunking）</strong>：ACT不预测单步动作，而是将连续<code>k</code>步动作作为一个“块”进行预测。这有助于模型学习更高层次的、连贯的任务子目标（如“抓取”和“放置”），减少长时程任务中的误差累积。</li>
<li><strong>编码与潜在变量采样</strong>：一个CVAE编码器<code>q_ϕ</code>根据观测（不含图像）和真实动作序列编码并采样出潜在变量<code>z</code>，用于捕获演示数据中的动作风格多样性。</li>
<li><strong>动作序列预测</strong>：提示图像<code>I_t^vp</code>、当前观测<code>o_t</code>和潜在变量<code>z</code>被送入一个由CNN和Transformer编码器组成的网络进行编码。随后，Transformer解码器基于此编码信息预测未来<code>k</code>步的动作序列<code>â_{t:t+k}</code>。</li>
<li><strong>训练与损失函数</strong>：模型通过最小化预测动作与演示动作之间的均方误差（<code>L_reconst</code>）进行训练，同时使用KL散度（<code>L_reg</code>）对潜在变量<code>z</code>的分布进行正则化，使其接近标准正态分布。总损失为<code>ℒ = L_reconst + βL_reg</code>。</li>
</ol>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>轻量级视觉提示</strong>：仅使用边界框这种简单标注作为空间指引，避免了复杂的场景解析或多模态融合，降低了感知复杂度。2) <strong>分块序列预测</strong>：将ACT模仿学习算法应用于此场景，使机器人能够学习并生成流畅的、适应性的动作序列，而非执行僵化的、预编程的轨迹。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在真实机器人平台进行，使用Universal Robots UR5e机械臂和Robotiq两指夹爪。视觉系统采用两个Intel RealSense相机（D435i手眼相机和D415固定视角相机）。使用RoboManipBaselines框架进行系统集成。评估对象为6种日本便利店常见商品（巧克力盒、饼干盒、茶盒、茶瓶、小罐子、面条碗），它们在形状、大小、材质上差异显著。</p>
<p><strong>实验场景</strong>：设计了三种渐进复杂度的任务场景进行评估：</p>
<ul>
<li><strong>简单场景</strong>：9个形状大小相似的盒状物体按3x3网格排列，一个物体被标注为抓取目标。</li>
<li><strong>复杂场景</strong>：9个多样化的物体按3x3网格排列，一个物体被标注为抓取目标。</li>
<li><strong>更复杂场景</strong>：9个多样化物体随意摆放，一个物体被标注为抓取目标，另一个位置被标注为放置目标。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.08748v1/x4.png" alt="实验设置俯视图"></p>
<blockquote>
<p><strong>图4</strong>：实验设置的俯视图，展示了以不同位置摆放的各种产品。绿色边界框指示抓取目标，红色边界框指示放置目标。</p>
</blockquote>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>成功率</strong>：在简单场景下，系统取得了**90%<strong>的成功率。在复杂场景下，初始成功率为</strong>70%<strong>；当针对失败案例增加20%的演示数据后，成功率提升至</strong>100%<strong>。在更复杂场景下，初始成功率为</strong>70%**；在将每种产品的演示数据量翻倍后，成功率得到改善（具体数值见表I）。</li>
<li><strong>注意力分析</strong>：通过可视化ACT编码器的注意力热力图发现，在简单设置中，注意力主要集中在边界框中心。随着任务复杂度增加（需要导航至放置点），注意力会从抓取目标位置快速转移到放置目标位置，表明模型能根据视觉提示自适应地调整注意力焦点。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.08748v1/x5.png" alt="注意力热力图"></p>
<blockquote>
<p><strong>图5</strong>：用于评估ACT在处理边界框提示时关注区域的注意力热力图。这些热图基于Transformer编码器的不同层生成，显示了注意力从抓取对象（绿色框）到放置位置（红色框）的转移。</p>
</blockquote>
<ul>
<li><strong>按物体类别分析</strong>：边界框提示对刚性物体（如巧克力盒、饼干盒）效果最好，成功率高达**90%**。对于反射表面（茶瓶）或光滑表面（小罐子）的物体，初始成功率较低，但通过增加训练数据可以得到显著提升。柔性物体（面条碗）因可抓握性问题被排除在评估之外。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.08748v1/images/fp1.png" alt="失败案例分析"></p>
<blockquote>
<p><strong>图6</strong>：系统在训练不足时出现的各种失败模式示例，包括：(a)手指未对齐、(b)放置失败、(c)表面太滑、(d)抓握力不足、(e)放置未对齐、(f)目标位置过于狭窄。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过调整数据量间接进行了消融实验。结果表明，<strong>增加多样化的人类演示数据</strong>是提升系统在复杂场景和面对困难物体（如反光、光滑物体）时性能的关键。例如，在复杂场景下增加20%数据使成功率从70%提升至100%。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出标注引导的视觉提示方法</strong>：利用简单的边界框标注为机器人操作提供明确的空间指引，显著降低了在杂乱动态场景中进行全面场景理解的感知复杂度。</li>
<li><strong>实现了基于ACT的自适应抓取放置系统</strong>：成功将ACT模仿学习算法应用于零售机器人操作任务，使机器人能够预测并执行连贯的分块动作序列，提升了操作的流畅性和适应性。</li>
<li><strong>设计了系统性的渐进式评估框架</strong>：通过定义三种不同复杂度的任务场景，并分析不同物体属性（刚性、反光、光滑）对性能的影响，为视觉提示在机器人操作中的有效性提供了细致的评估。</li>
</ol>
<p><strong>局限性</strong>：论文明确指出，该方法<strong>对数据需求较高</strong>，其有效性严重依赖于多样化和高质量的人类演示数据。收集这些数据可能耗时且需要专业知识。</p>
<p><strong>后续启示</strong>：</p>
<ol>
<li><strong>数据效率提升</strong>：未来的工作可以专注于开发数据增强技术或结合强化学习、世界模型等方法，以减少对大量人工演示的依赖。</li>
<li><strong>提示方式扩展</strong>：可以探索除边界框外的其他提示形式（如关键点、分割掩码、自然语言指令），并研究多模态提示的融合。</li>
<li><strong>泛化能力研究</strong>：进一步测试该方法在完全未知的物体、更极端的遮挡或非结构化环境中的泛化能力，是迈向实际部署的关键步骤。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对零售环境（如便利店）中机器人抓取放置任务面临的物体密集、遮挡及属性多样等挑战，提出了一种基于标注引导视觉提示的感知-行动框架。核心方法采用边界框标注提供空间指引，并结合模仿学习算法ACT，使机械臂能够根据人类示范预测分块动作序列，实现自适应操作。实验表明，该系统提升了抓取准确性与环境适应性，并通过成功率与抓取行为分析验证了其在零售场景中的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.08748" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>