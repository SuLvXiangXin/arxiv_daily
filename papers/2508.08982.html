<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.08982" target="_blank" rel="noreferrer">2508.08982</a></span>
        <span>作者: Sehoon Ha Team</span>
        <span>日期: 2025-08-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，结合深度强化学习（RL）与足式机器人运动控制已在敏捷性方面取得显著进展。然而，现有方法通常依赖于额外技术来掌握复杂技能，主要包括：1）基于领域知识的奖励工程；2）专家演示数据集；3）精心设计的课程学习。这些方法限制了方法的通用性，并引入了大量人工工程。本文针对“如何在没有人工课程或演示的情况下，让机器人自主探索并学习敏捷运动技能”这一具体痛点，提出了将无监督技能发现（Unsupervised Skill Discovery）作为高层级探索策略的新视角。本文的核心思路是：提出一个名为SDAX的框架，通过一个可学习的平衡参数λ，动态调节任务奖励与技能多样性奖励的权重，使机器人既能探索多样行为，又能最终优化任务性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>SDAX的整体目标是在马尔可夫决策过程（MDP）中，训练一个技能条件策略π_θ(a|s, z)，其中技能向量z在每轮训练中从固定先验分布中采样并保持不变。策略优化的目标是最大化组合奖励J^(task+div) = E[∑ γ^t (r_t^task + λ r_t^div)]，其中r_t^task是简单的任务奖励（如跟踪前进速度），r_t^div是鼓励行为多样性的奖励，λ是可学习的平衡参数。</p>
<p><img src="https://arxiv.org/html/2508.08982v1/img/method_overall5.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SDAX的双层优化框架。策略参数θ通过组合奖励（任务奖励+多样性奖励）的梯度进行优化；而平衡参数λ则仅通过任务奖励的梯度进行优化，其目标是最大化最终的任务性能。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>多样性奖励（r_div）</strong>：采用无监督技能发现方法（如METRA）来定义。以METRA为例，其目标是最大化技能与状态之间的Wasserstein依赖度量（IWDM）。这转化为优化奖励r_div = (φ(s_{t+1}) - φ(s_t))^T z，其中φ是一个将状态映射到技能潜在空间的可学习表示函数，并受Lipschitz约束（通过拉格朗日乘子κ和双梯度下降确保）。</li>
<li><strong>平衡参数λ的训练</strong>：这是SDAX的关键创新。λ通过双层优化进行训练：内层优化θ以最大化J^(task+div)；外层优化λ以仅最大化J^(task)。由于无法直接计算J^(task)对λ的梯度，论文通过链式法则推导出近似梯度公式：∇_λ J^(task) ≈ α A^(task) ∇_θ‘ log π_θ’(a|s,z) · A^(div) ∇_θ log π_θ(a|s,z)。直观上，如果任务奖励和多样性奖励产生的梯度向量方向相似，λ会增加；反之则减少。这使λ能根据训练阶段动态调整探索强度。</li>
<li><strong>策略与值函数</strong>：使用PPO算法训练技能条件策略π_θ。由于存在两个奖励源，为避免因λ变化导致奖励尺度不稳定，SDAX引入了两个独立的值网络v_ψ1^task和v_ψ2^div分别估计任务回报和多样性回报。</li>
</ol>
<p>与现有方法相比，SDAX的创新点在于：1）将无监督技能发现方法（如DIAYN、METRA）作为高层探索模块集成到RL训练循环中，而非仅用于事后技能多样化；2）提出了一个可自动、动态调整多样性奖励权重的双层优化方案，取代了手动调整固定权重λ的繁琐过程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在Isaac Gym仿真环境中使用Unitree A1四足机器人进行实验。评估任务包括：跨越48cm沟壑的“跳跃”（Leap）、攀爬25cm平台的“攀爬”（Climb）、钻过29cm高度的“爬行”（Crawl），以及一个更复杂的“蹬墙跳”（Wall-jump）任务。使用PPO算法，训练约10k-20k次迭代。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li><strong>Task-only</strong>：仅使用任务奖励r_task。</li>
<li><strong>Div-only</strong>：仅使用多样性奖励r_div。</li>
<li><strong>RND</strong>：结合任务奖励和基于RND的探索奖励。</li>
<li><strong>SDAX with DIAYN</strong>：使用DIAYN计算r_div。</li>
<li><strong>SDAX with METRA</strong>：使用METRA计算r_div。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.08982v1/img/curves_new/leap_more_width_2.png" alt="训练曲线对比"></p>
<blockquote>
<p><strong>图3a</strong>：跳跃任务的训练曲线（通过障碍物数量）。SDAX with METRA能够成功解决任务，并表现出比Task-only基线更好的样本效率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.08982v1/img/curves_new/climb_more_width.png" alt="训练曲线对比"></p>
<blockquote>
<p><strong>图3b</strong>：攀爬任务的训练曲线。SDAX方法同样成功，而Div-only和RND基线效果不佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.08982v1/img/curves_new/crawl_more_width.png" alt="训练曲线对比"></p>
<blockquote>
<p><strong>图3c</strong>：爬行任务的训练曲线。趋势与前述任务一致，SDAX with METRA性能最佳。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>敏捷运动技能学习</strong>：如图3所示，SDAX with METRA成功学习了所有三项任务（跳跃、攀爬、爬行）。与仅用任务奖励的基线相比，引入多样性奖励有助于学习敏捷运动技能。仅用多样性奖励（Div-only）则无法获得有意义的技能，表明任务与多样性奖励的平衡至关重要。与RND对比表明，基于技能发现的探索优于基于状态的探索。</li>
<li><strong>技能发现促成高层探索</strong>：图4展示了训练中同一策略在不同技能向量z下产生的成功与失败行为。例如在跳跃任务中，某些技能能产生强力起跳，而另一些则导致跳跃无力。这验证了技能发现作为一种高层探索机制，让智能体尝试了多种策略（如不同身体高度），其中一些最终导向任务成功。</li>
<li><strong>平衡参数λ的自适应学习</strong>：在跳跃任务上比较了固定λ值（0.01, 0.1, 1, 10）与SDAX自适应λ。如图5所示，自适应方法在样本效率和最终性能上均优于所有固定λ设置。图5b显示，λ值从10.0开始并逐渐下降，表明算法学会了随时间减少多样性奖励的权重以最大化任务奖励。</li>
<li><strong>技能的“正向收敛”</strong>：如表1所示，随着训练进行，从先验分布中随机采样的技能向量z的成功率显著提升（例如，跳跃任务从43.1%升至97.1%）。这表明一旦发现可行解，不同技能会收敛到相似的成功行为，缓解了训练后技能选择的问题。</li>
<li><strong>复杂任务：蹬墙跳</strong>：在仅用任务奖励（基于轨迹指南）训练失败的情况下，SDAX通过将机器人基体的滚转、俯仰、偏航角提供给技能发现模块，成功探索并学会了蹬墙所需的特定身体姿态，从而完成了这一复杂动作（图6）。</li>
<li><strong>实物部署</strong>：经过带观测噪声和域随机化的微调后，学习到的策略成功迁移到真实的Unitree A1机器人上，完成了跳跃46cm沟壑、攀爬25cm平台和爬行穿过27cm障碍等任务（图1，图7）。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了SDAX框架，将无监督技能发现无缝集成到RL训练中，作为自动化、高层级的探索策略，以减少对奖励工程、演示或课程学习的需求。2）设计了一种双层优化方案，能够自动、动态地调整任务奖励与多样性奖励之间的平衡参数λ。3）在仿真和实物机器人上验证了框架的有效性，使四足机器人学会了包括蹬墙跳在内的一系列敏捷运动技能。</p>
<p>论文自身提到的局限性：1）训练需要手动指定状态空间的特定子维度（如身体高度）来引导探索，而非完全自动。2）技能发现模块对观测噪声敏感，过大的噪声会干扰多样性奖励信号。因此，实践中需先低噪声训练技能，再高噪声微调以提升鲁棒性。</p>
<p>对后续研究的启示：SDAX展示了将技能发现用于引导RL探索的潜力。未来的工作可以致力于提升技能发现模块的鲁棒性，使其能在噪声更大或随机化更强的条件下有效工作；此外，探索如何自动化地确定需要探索的状态子维度，或开发更通用的技能表示，将进一步提高该框架的适用性和自动化程度。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SDAX框架，旨在解决四足机器人学习敏捷步态时依赖人工奖励设计、专家示范或课程学习的问题。其核心方法是结合无监督技能发现与双层优化：通过技能向量引导策略探索多样行为（如爬行、攀爬、跳跃），同时动态调整探索强度以平衡任务奖励与多样性奖励。实验表明，该框架使机器人能自主掌握包括垂直墙面跳跃在内的复杂动作，并成功迁移到真实硬件平台。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.08982" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>