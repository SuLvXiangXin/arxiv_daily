<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.06677" target="_blank" rel="noreferrer">2506.06677</a></span>
        <span>作者: Si Liu Team</span>
        <span>日期: 2025-06-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型在机器人领域的应用主流是将其作为快速、反应式的“系统1”模块，即视觉-语言-动作模型，用于将多模态输入直接映射为低级控制信号。然而，这种方式未能充分利用VLM在语义抽象、关系理解和上下文推理方面的优势，这些能力本质上是与慢思考的“系统2”过程（如长期规划、子目标分解）相一致的。尽管近期的一些基准测试已将任务从单步指令扩展到多步骤程序，但它们通常在时间跨度和结构复杂性上存在局限，通常只涉及2到5个子任务，动作步骤少于500步。这些基准测试无法捕捉真实世界对层次化目标分解、时间抽象和自适应规划的需求，导致VLM的“系统2”能力，特别是高级推理和长程规划，仍未得到充分探索。</p>
<p>本文针对现有基准测试在时间尺度和任务复杂性上的不足，提出了一个专注于评估长程机器人操作中“系统2”推理能力的新基准测试RoboCerebra。其核心思路是通过一个自上而下的流程构建包含超长动作序列、动态场景变化和细粒度标注的大规模仿真数据集，并设计一个结合高级VLM规划器与低级VLA控制器的分层框架，以系统评估规划、反思和记忆等关键认知维度。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboCerebra基准测试包含三个核心组成部分：1）一个设计用于捕捉长程现象的任务集；2）一个沿多个维度标注的多样化数据集；3）一个多方面的评估协议。</p>
<p><strong>数据集构建流程</strong>：采用模块化、自上而下的流程在Libero仿真平台上构建结构化、可执行的任务。</p>
<ol>
<li><strong>级联任务生成</strong>：从仿真器物品库中随机采样物体，并将其转换为基于类别和属性的结构化表示。使用GPT-4o根据这些表示生成高级任务描述（如“用微波炉加热牛奶”），并进一步分解为连贯的逐步子任务指令。提示策略被设计用于促进长程组合性和时空一致性。</li>
<li><strong>场景初始化与验证</strong>：将结构化的任务计划解析为一组空间和关系约束，通过基于规则的映射转换为仿真器可执行的代码以构建完整场景。通过两级验证确保质量：符号仿真循环验证对象状态和关系约束的一致性；视觉-语言验证循环利用GPT-4o从多视角RGB-D渲染中评估空间合理性。</li>
<li><strong>人类演示与标注</strong>：由人类操作员在仿真中执行任务指令，生成多样且真实的动作轨迹。每条轨迹都标注了细粒度的子任务边界，以实现精确的时间分割和语言指令到运动片段的关联。为确保质量，投入了400小时进行轨迹和时间标注，以及200小时进行单独的验证。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.06677v2/x2.png" alt="任务生成流程"></p>
<blockquote>
<p><strong>图2</strong>：RoboCerebra中的任务生成流程。(a) 物体从物品库中随机采样并转换为结构化表示。(b) 结构化数据输入LLM以生成高级任务描述并层次化分解为低级子步骤。(c) 生成的任务计划通过基于规则的转换解析为可执行的仿真器代码，并通过包含符号检查和VLM视觉-语言一致性的闭环过程进行验证。</p>
</blockquote>
<p><strong>分层规划与执行框架</strong>：为了充分利用高级推理和低级控制的互补优势，论文提出了分层规划与执行框架。</p>
<ol>
<li><strong>训练流程</strong>：采用两阶段监督微调范式。第一阶段，使用从长程演示中提取的（图像，指令，动作）对训练VLA模型（基于OpenVLA），使其能够基于自我中心观察和步骤级指令执行细粒度动作。第二阶段，构建包含步骤级指令和成功/失败标签的视频-指令数据集，训练VLM模型评估任务进度，使其能够基于实时视觉反馈进行进度感知的规划和重新规划。</li>
<li><strong>分层任务规划与执行</strong>：在推理时，VLM将高级任务指令解析为一系列步骤级子目标，存储于记忆库中。VLA持续查询当前活跃子目标，并基于高频视觉观察执行相应的低级动作。同时，VLM周期性地关注近期观察以监控执行进度，在检测到子目标完成或偏离时更新记忆库中的计划。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.06677v2/x4.png" alt="HPE框架概述"></p>
<blockquote>
<p><strong>图4</strong>：HPE框架概述。左侧：VLA模型训练使用配对的图像和单步指令来优化视觉令牌策略。VLM训练使用带有成功标签指令的执行视频进行时序关联。右侧：执行期间，VLM处理低频观察以更新存储在记忆库中的低级计划，而VLA则消耗高频观察以基于详细计划执行细粒度动作。</p>
</blockquote>
<p><strong>评估协议</strong>：提出了一个多维度的评估协议，包含四个互补的指标：任务成功率、规划准确率、规划效率和动作完成准确率。其中动作完成准确率通过一个包含人工编写二元问题的VideoQA基准进行评估，与模型的反思能力紧密相关。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在RoboCerebra基准上进行评估，包含1000条训练轨迹和60个测试任务（每个任务10次试验，共600次 rollout）。系统1模型通过在100个任务实例上微调OpenVLA获得。系统2模型评估了三大类：预训练VLM（GPT-4o, Qwen2.5-VL, LLaVA-Next-Video）、盲LLM（禁用视觉输入）以及在视频-指令数据集上监督微调的VLM。基线方法包括纯系统1模型、仅规划器模型以及完整的分层框架。</p>
<p><strong>主要结果</strong>：</p>
<ul>
<li><strong>系统1在长程任务中表现挣扎</strong>：在静态、完全可观察的理想设定下，微调后的OpenVLA成功率仅为7.84%，而在混合了记忆需求和动态场景变化的Mix设定下，成功率降至0.00%，凸显了其处理长程依赖和部分可观察性的困难。</li>
<li><strong>系统2提升了系统1在复杂任务中的性能</strong>：在Mix设定下，规划器+OpenVLA和分层框架分别取得了11.48%和13.21%的成功率，表明引入系统2规划带来了显著增益。分层框架通过迭代推理在记忆密集型场景中提供了额外益处。</li>
<li><strong>分层框架在简单任务中可能引入额外开销</strong>：在理想设定下，分层框架的成功率（21.10%）略低于规划器+OpenVLA（21.92%），表明在简单任务中额外的推理开销可能带来次优决策。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.06677v2/x3.png" alt="数据集统计分析"></p>
<blockquote>
<p><strong>图3</strong>：RoboCerebra数据集的统计分析。(a) 每个任务的最小步骤数分布，突出其长程性质。(b) 动作类别频率，主导性基本动作和罕见的细粒度动作。(c) 每个任务的动作类别数量，显示超过10%的任务涉及五种或更多动作类型，表明高组合多样性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.06677v2/x1.png" alt="轨迹长度对比"></p>
<blockquote>
<p><strong>图1c</strong>：RoboCerebra与现有机器人操作基准测试的轨迹长度对比。RoboCerebra的平均轨迹长度（2972.4仿真步骤）约为现有数据集的6倍。</p>
</blockquote>
<p><strong>消融实验（规划器模型）</strong>：如表4所示，使用地面真值计划直接执行（GT-plan）取得了最佳平均成功率（25.16%），为性能提供了上限。在纯语言模型（盲）中，GPT-4o表现最佳（15.10%）。在具备视觉能力的VLM中，GPT-4o同样表现最好（16.04%），且其性能优于其盲版本，说明了视觉输入的重要性。监督微调的VLM（表中GPT-4o*，未在提供片段中显示具体数值）预期能进一步提升性能。</p>
<p>表 3：六种子任务上的性能比较。* 表示在我们的数据上微调的模型。报告了每种方法在随机扰动、观察不匹配、记忆探索、记忆执行、混合和理想任务上10次试验的平均成功率（%）。</p>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="center">Avg</th>
<th align="center">Dynamic</th>
<th align="center">Memory</th>
<th align="center">Mix</th>
<th align="center">Ideal</th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td align="center"></td>
<td align="center">Ran.</td>
<td align="center">Obs.</td>
<td align="center">Exp.</td>
<td align="center">Exe.</td>
</tr>
<tr>
<td align="left">OpenVLA-Libero100</td>
<td align="center">2.00</td>
<td align="center">4.59</td>
<td align="center">1.35</td>
<td align="center">0.18</td>
<td align="center">1.86</td>
</tr>
<tr>
<td align="left">OpenVLA*</td>
<td align="center">4.57</td>
<td align="center">7.84</td>
<td align="center">8.65</td>
<td align="center">1.06</td>
<td align="center">2.06</td>
</tr>
<tr>
<td align="left">Planner+OpenVLA*</td>
<td align="center">16.04</td>
<td align="center">18.63</td>
<td align="center">19.45</td>
<td align="center">8.04</td>
<td align="center">16.69</td>
</tr>
<tr>
<td align="left">Hierarchical Framework</td>
<td align="center">16.55</td>
<td align="center">18.63</td>
<td align="center">19.18</td>
<td align="center">9.06</td>
<td align="center">17.83</td>
</tr>
</tbody></table>
<p>表 4：不同规划器模型的消融研究（%）。Blind表示无视觉输入的模型，GT-plan表示直接遵循地面真值计划。粗体数字表示最佳性能，下划线数字表示次佳性能。</p>
<table>
<thead>
<tr>
<th align="left">Planner Model</th>
<th align="center">Avg</th>
<th align="center">Dynamic</th>
<th align="center">Memory</th>
<th align="center">Mix</th>
<th align="center">Ideal</th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td align="center"></td>
<td align="center">Ran.</td>
<td align="center">Obs.</td>
<td align="center">Exp.</td>
<td align="center">Exe.</td>
</tr>
<tr>
<td align="left">GT-plan</td>
<td align="center">25.16</td>
<td align="center">26.85</td>
<td align="center">30.68</td>
<td align="center">19.47</td>
<td align="center">23.48</td>
</tr>
<tr>
<td align="left">Qwen2.5-VL-Blind</td>
<td align="center">11.87</td>
<td align="center">18.90</td>
<td align="center">12.88</td>
<td align="center">7.02</td>
<td align="center">10.87</td>
</tr>
<tr>
<td align="left">LLaVA-Next-Blind</td>
<td align="center">8.00</td>
<td align="center">13.97</td>
<td align="center">12.33</td>
<td align="center">3.54</td>
<td align="center">3.54</td>
</tr>
<tr>
<td align="left">GPT-4o-Blind</td>
<td align="center">15.10</td>
<td align="center">20.00</td>
<td align="center">17.03</td>
<td align="center">7.02</td>
<td align="center">16.09</td>
</tr>
<tr>
<td align="left">Qwen2.5-VL</td>
<td align="center">11.19</td>
<td align="center">14.25</td>
<td align="center">14.25</td>
<td align="center">2.63</td>
<td align="center">12.61</td>
</tr>
<tr>
<td align="left">LLaVA-Next-Video</td>
<td align="center">11.37</td>
<td align="center">16.71</td>
<td align="center">16.16</td>
<td align="center">1.07</td>
<td align="center">10.87</td>
</tr>
<tr>
<td align="left">GPT-4o</td>
<td align="center">16.04</td>
<td align="center">18.63</td>
<td align="center">19.45</td>
<td align="center">8.04</td>
<td align="center">16.69</td>
</tr>
</tbody></table>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了RoboCerebra基准测试</strong>：这是一个大规模、长程的机器人操作评估基准，其任务序列长度约为现有基准的6倍，并明确包含了动态变化、记忆依赖和细粒度分解，旨在系统评估VLM的“系统2”推理能力。</li>
<li><strong>设计了高效的数据生成与验证流程</strong>：通过LLM生成与分解任务、人类演示、以及符号与视觉双重验证，实现了高质量、可扩展的长程操作数据收集。</li>
<li><strong>引入了分层规划与执行框架及多维度评估协议</strong>：提出了结合VLM规划器和VLA控制器的HPE框架，并设计了超越二元成功率的评估指标（规划准确率、效率、动作完成准确率），以全面衡量高级认知功能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，由于关注重点是高级推理而非低级控制，因此使用仿真环境构建数据集，仿真到现实的差距在此背景下不那么关键，但这仍然是该方法应用于真实机器人的一个潜在限制。</p>
<p><strong>启示</strong>：RoboCerebra强调了在机器人研究中区分和专门评估“系统2”能力的重要性。其构建的数据集和评估框架为未来开发更擅长长期规划、情境记忆和自适应推理的机器人智能体奠定了基础。实验结果表明，当前最先进的VLM作为规划器仍有很大提升空间（远低于地面真值计划性能），激励后续研究在提升VLM的长程时序理解、状态跟踪和规划可靠性方面继续努力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有基准测试在评估机器人长时程操作中高层语义推理与规划能力（System 2）方面的不足，提出了RoboCerebra大规模基准测试。其核心方法包括：1）通过GPT生成并分解任务指令，结合人工模拟执行，构建了包含长序列、细粒度子任务和动态场景的大规模仿真数据集；2）设计了结合高层VLM规划器与低层VLA控制器的分层评估框架。该基准的任务轨迹长度约为现有基准的6倍，并包含动态变化与时间标注等关键特征，旨在系统评估规划、反思与记忆等高级认知能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.06677" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>