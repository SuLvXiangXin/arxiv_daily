<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LLaDA-VLA: Vision Language Diffusion Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>LLaDA-VLA: Vision Language Diffusion Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.06932" target="_blank" rel="noreferrer">2509.06932</a></span>
        <span>作者: Xiaoyan Sun Team</span>
        <span>日期: 2025-09-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人操作领域的视觉语言动作模型几乎完全基于自回归视觉语言模型构建。这类方法依赖顺序令牌生成，存在效率限制，且由于其固有的单向生成特性，在复杂的多模态机器人任务中灵活性不足。与此同时，掩码扩散模型在文本生成和多模态应用中开始展现出与自回归模型相竞争的性能，催生了一系列基于扩散的视觉语言模型。然而，如何利用此类模型进行机器人策略学习仍是一个未被探索的领域。</p>
<p>本文针对将预训练的扩散视觉语言模型适配到机器人任务时面临的两个关键挑战：1）模型预训练使用的大规模通用数据集与机器人任务所需的低层级视觉理解之间存在显著的领域鸿沟；2）掩码扩散的生成范式并不天然适合生成结构化的动作序列，其解码策略难以建模机器人动作内部及动作间强烈的层次依赖关系。为此，本文提出了首个基于预训练扩散视觉语言模型的视觉-语言-扩散-动作模型LLaDA-VLA。其核心思路是：通过局部特殊令牌分类策略缩小领域鸿沟，并设计分层动作结构解码策略来显式建模动作序列的结构化依赖，从而将扩散模型高效地应用于机器人动作生成。</p>
<h2 id="方法详解">方法详解</h2>
<p>LLaDA-VLA的整体框架基于预训练的扩散视觉语言模型构建，旨在根据语言指令和视觉观察生成机器人动作序列。</p>
<p><img src="https://arxiv.org/html/2509.06932v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：LLaDA-VLA概述。(a) 整体架构。视觉编码器提取的特征经投影器映射到文本空间，与文本令牌拼接，并与掩码令牌一同输入大语言扩散模型，通过局部特殊令牌分类生成动作序列，并经由分层动作结构解码进一步细化。(b) 分层动作结构解码策略。从完全掩码的动作序列开始，模型迭代预测掩码令牌，基于置信度执行动作级和令牌级重掩码，直到完整序列被解码。</p>
</blockquote>
<p><strong>模型架构</strong>：模型包含三个主要组件：语言主干网络、视觉编码器和投影器。采用LLaDA作为语言主干，SigLIP-2作为视觉编码器，MLP作为投影器。输入为语言指令和前视RGB图像。视觉特征被投影并与文本令牌拼接，输入扩散模型以生成动作序列。</p>
<p><strong>动作令牌化与分块</strong>：为生成动作，将连续动作值离散化为<code>𝒱_a</code>个区间，并在原始词汇表中添加<code>𝒱_a</code>个特殊动作令牌<code>𝒮</code>来表示这些离散值（<code>𝒱_a ≪ 𝒱</code>）。每个时间步的动作由<code>D=7</code>个特殊动作令牌表示（3个位置、3个旋转、1个夹持器状态）。为生成多步轨迹，模型预测连续<code>K</code>个时间步的动作块，共<code>K×D</code>个令牌，这些令牌可反令牌化为连续值以执行。</p>
<p><strong>核心创新点1：局部特殊令牌分类</strong>：预训练d-VLM的目标是对每个令牌进行全词汇表分类。为生成动作，模型仅需预测特殊动作令牌。保留全词汇表分类目标会使学习过程复杂化。因此，本文提出将分类空间限制在特殊动作令牌子集<code>𝒮</code>上。训练时，仅对属于<code>𝒮</code>的令牌标签计算交叉熵损失，非动作令牌的损失被忽略。推理时，仅在目标子集<code>𝒮</code>上进行预测，并将预测的局部类别索引映射回原始令牌索引。此设计将学习集中在动作相关令牌上，降低了适应难度。</p>
<p><strong>核心创新点2：分层动作结构解码</strong>：原始LLaDA解码策略平等对待所有输出令牌，忽略了动作块内部的结构化依赖关系。本文提出的分层解码策略显式建模了动作内和动作间的相关性。</p>
<ol>
<li><strong>动作级置信度排序</strong>：计算每个动作的置信度得分 <code>C_a^(i) = Σ_{j=1}^D c_{i,j}</code>，其中<code>c_{i,j}</code>为动作内第<code>j</code>个令牌的置信度。根据<code>C_a</code>对所有动作排序。</li>
<li><strong>分层重掩码</strong>：在每个解码步骤，首先进行<strong>动作级重掩码</strong>：保留置信度最高的动作，其余动作的全部令牌被重新掩码。然后，在保留的动作内部进行<strong>令牌级重掩码</strong>：根据令牌级置信度排序，仅保留高置信度令牌，其余令牌被重掩码。</li>
<li><strong>迭代生成</strong>：被重掩码的令牌在后续扩散步骤中重新预测。此过程迭代进行，确保轨迹以动作级为单位生成，同时允许对每个动作内部进行细化，从而生成更连贯合理的动作轨迹。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真环境SimplerEnv和CALVIN，以及真实世界的WidowX机器人上进行评估。使用预训练的LLaDA-V权重进行微调。主要对比的基线方法包括：OpenVLA（自回归VLA）、CogACT、π0、RT-1-X、Octo等。</p>
<p><strong>关键定量结果</strong>：</p>
<ul>
<li><p><strong>SimplerEnv仿真</strong>：如表1所示，LLaDA-VLA平均成功率为55.5%，显著超越OpenVLA（4.2%）和CogACT（51.3%）。<br><img src="https://arxiv.org/html/2509.06932v2/x1.png" alt="SimplerEnv结果表"></p>
<blockquote>
<p><strong>表1</strong>：在SimplerEnv Visual Matching设置下与基线方法的成功率对比。LLaDA-VLA取得了最佳平均性能。</p>
</blockquote>
</li>
<li><p><strong>CALVIN仿真</strong>：如表2所示，LLaDA-VLA在连续完成5个任务的平均长度指标上达到4.01，优于OpenVLA的3.27，提升0.74。<br><img src="https://arxiv.org/html/2509.06932v2/x2.png" alt="CALVIN结果表"></p>
<blockquote>
<p><strong>表2</strong>：在CALVIN ABC-D设置下的性能对比。LLaDA-VLA在平均长度和连续任务成功率上均领先。</p>
</blockquote>
</li>
<li><p><strong>真实机器人（域内任务）</strong>：如表3所示，LLaDA-VLA在四个任务上平均成功率达58%，超越π0（35%）和CogACT（30%）。<br><img src="https://arxiv.org/html/2509.06932v2/x3.png" alt="真实机器人结果表"></p>
<blockquote>
<p><strong>表3</strong>：真实机器人域内任务成功率对比。LLaDA-VLA表现最佳。</p>
</blockquote>
</li>
<li><p><strong>泛化能力（域外任务）</strong>：如表4所示，在涉及未见物体、容器和干扰物的任务上，LLaDA-VLA平均成功率达40%，显著优于π0的15%。<br><img src="https://arxiv.org/html/2509.06932v2/x4.png" alt="泛化结果表"></p>
<blockquote>
<p><strong>表4</strong>：真实机器人域外（OOD）任务泛化能力对比。LLaDA-VLA展现出更强的泛化性能。</p>
</blockquote>
</li>
</ul>
<p><strong>消融实验</strong>：<br>如表5所示，在CALVIN上验证两个核心设计的贡献。</p>
<ul>
<li><strong>局部特殊令牌分类</strong>：在基线（平均长度2.64）上加入此策略，平均长度提升至3.43（+0.79），证明了其有效降低了适应难度。</li>
<li><strong>分层动作结构解码</strong>：在已有局部分类的基础上加入此策略，平均长度进一步提升至4.01（+0.58），证明了显式建模动作结构依赖的有效性。<br><img src="https://arxiv.org/html/2509.06932v2/x5.png" alt="消融实验表"><blockquote>
<p><strong>表5</strong>：局部特殊令牌分类与分层动作结构解码的消融研究。两者均带来显著性能提升。</p>
</blockquote>
</li>
</ul>
<p><strong>动作块大小分析</strong>：如表6所示，动作块大小<code>K</code>影响性能。<code>K=5</code>时取得最佳平衡，<code>K</code>过大（8,10）时因预测难度增加导致性能下降。<br><img src="https://arxiv.org/html/2509.06932v2/x6.png" alt="动作块大小表"></p>
<blockquote>
<p><strong>表6</strong>：不同动作块大小对性能的影响。适中的块大小（如5）能平衡轨迹平滑性与预测准确性。</p>
</blockquote>
<p><strong>定性结果</strong>：</p>
<ul>
<li><p><strong>仿真任务</strong>：图3和图4展示了LLaDA-VLA在CALVIN和SimplerEnv中成功完成多步长视野和精确操作任务。<br><img src="https://arxiv.org/html/2509.06932v2/x3.png" alt="CALVIN定性结果"></p>
<blockquote>
<p><strong>图3</strong>：LLaDA-VLA在CALVIN任务中的执行序列，展示了长视野操作能力。<br><img src="https://arxiv.org/html/2509.06932v2/x4.png" alt="SimplerEnv定性结果"><br><strong>图4</strong>：LLaDA-VLA在SimplerEnv任务中精准定位、抓取和放置物体。</p>
</blockquote>
</li>
<li><p><strong>真实机器人任务</strong>：图5和图6展示了LLaDA-VLA在真实场景中可靠执行已知任务，并能泛化到处理新物体（立方体）、新容器（纸盒）及存在干扰物的复杂环境。<br><img src="https://arxiv.org/html/2509.06932v2/x5.png" alt="真实机器人域内定性"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人域内任务执行示例。<br><img src="https://arxiv.org/html/2509.06932v2/x6.png" alt="真实机器人域外定性"><br><strong>图6</strong>：真实机器人域外（OOD）泛化任务执行示例，展现了处理新物体和干扰物的能力。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>开创了新范式</strong>：提出了首个基于预训练扩散视觉语言模型的视觉-语言-扩散-动作模型LLaDA-VLA，为机器人策略学习开辟了新方向。2) <strong>提出了关键技术</strong>：设计了局部特殊令牌分类策略以降低领域适应难度，以及分层动作结构解码策略以有效生成结构化的动作序列。3) <strong>验证了卓越性能</strong>：在仿真和真实机器人基准测试中均取得了最先进的性能，证明了扩散模型在机器人操作领域的巨大潜力。</p>
<p>论文自身提到的一个局限性是<strong>动作块大小的选择需要仔细权衡</strong>，过大的块会增加预测难度。这启示后续研究可能需要更动态或自适应的序列生成机制。</p>
<p>本文的工作对后续研究的重要启示是：<strong>基于扩散的生成范式因其并行迭代生成的特性，可能在机器人任务的高效性、灵活性和可控性方面带来新的优势</strong>。未来研究可以探索如何将更先进的扩散模型技术（如条件控制、更快采样）与机器人学习更深度地结合，并进一步研究如何更好地利用扩散模型处理复杂、结构化的输出空间。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出首个基于预训练扩散视觉语言模型（d-VLM）的机器人操作模型LLaDA-VLA，旨在解决将d-VLM适配到机器人领域的两大挑战：视觉语义的领域差距与动作序列的结构化生成难题。关键技术包括：1）局部特殊令牌分类策略，用特定动作令牌分类替代全词汇分类以降低适配难度；2）分层动作结构化解码策略，在解码时考虑动作内外的依赖关系。实验表明，该模型在仿真和真实机器人任务上均显著优于当前最先进的视觉语言动作模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.06932" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>