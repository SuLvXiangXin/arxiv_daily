<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.02538" target="_blank" rel="noreferrer">2510.02538</a></span>
        <span>作者: Hao Su Team</span>
        <span>日期: 2025-10-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，在仿真中训练机器人策略或从离线数据集中学习，然后在真实世界中进行微调，是一种常见的机器人控制策略。然而，现有方法通常要么需要明确定义的奖励函数用于强化学习训练，要么依赖于大规模的离线专家数据集用于模仿学习。本文研究一个更具挑战性的设定：在仿真和真实环境中都只有有限的专家演示，且没有任何奖励信号可用。这一设定给离线模仿学习带来了显著挑战，因为有限的专家数据覆盖会导致过拟合并加剧偏差累积；而由于缺乏奖励信号，强化学习方法也无法应用。</p>
<p>本文针对在仅有少量无奖励专家数据下的仿真到真实迁移问题，提出了一个新的视角：通过在线模仿学习来预训练世界模型。核心思路是，在仿真阶段利用在线交互（无奖励）和少量专家数据预训练一个世界模型，以扩大状态空间的覆盖；随后在真实世界阶段，仅使用少量真实专家数据对该模型进行离线微调，从而实现高效、鲁棒的策略迁移。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的仿真到真实框架基于世界模型，整体流程分为三个阶段：仿真中的在线模仿预训练、使用真实世界数据的离线微调，以及真实世界部署。</p>
<p><img src="https://arxiv.org/html/2510.02538v1/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图1</strong>：本文提出的方法预训练和微调流程。预训练阶段，世界模型使用一个联合训练的判别器（用于区分专家数据和在线交互数据）产生的奖励信号进行训练。微调阶段，使用真实世界的小型专家数据集对预训练的编码器和策略进行精调。</p>
</blockquote>
<p><strong>1. 在线模仿预训练</strong><br>由于预训练阶段没有显式奖励，无法直接应用标准的世界模型强化学习方法（如TD-MPC2）。因此，本文采用了基于CDRED世界模型的在线模仿学习框架。CDRED世界模型包含五个核心组件：编码器 $h$（将状态 $\mathbf{s}_t$ 映射为潜在表示 $\mathbf{z}_t$）、潜在动力学模型 $d$（预测下一潜在状态 $\mathbf{z}_t&#39;$）、CDRED奖励模型 $R$（预测即时奖励 $\hat{r}_t$）、价值函数 $Q$ 和策略 $\pi$。</p>
<p>其创新关键在于<strong>CDRED奖励模型</strong>，它通过区分专家数据和行为（在线交互）数据来生成内在奖励。该模型包含一个专家预测器 $f_\phi$、一个行为预测器 $f_\psi$ 和一组 $K$ 个冻结的目标网络 ${f_{\bar{\theta}_k}}$。奖励构造公式为：<br>$R(\mathbf{z}_t, \mathbf{a}_t) = -\zeta<del>b(\mathbf{z}_t, \mathbf{a}_t, f_\phi) + (1-\zeta)</del>b(\mathbf{z}_t, \mathbf{a}_t, f_\psi)$<br>其中 $b(\cdot)$ 是奖励红利项，近似于相对于某个分布（专家或行为）的负对数似然。超参数 $\zeta$ 用于平衡鼓励利用专家数据的“专家分布红利”和鼓励探索的“行为分布红利”。训练时，模型通过最小化一个联合损失函数来优化，该损失包含用于训练编码器、动力学和价值函数的基础损失 $\mathcal{L}^{\text{base}}$，以及用于训练奖励模型的损失 $\mathcal{L}^{\text{reward}}$。策略则通过基于潜在状态和价值的最大熵目标进行优化。</p>
<p><strong>2. 离线微调</strong><br>预训练完成后，获得一个在源域（仿真）表现良好的世界模型。为了将其迁移到新的仿真域或真实环境，本文使用从目标环境收集的小型专家数据集 $\mathcal{D}<em>{\text{new}}$ 进行<strong>完全离线</strong>的监督微调。微调目标是对编码器 $h_\xi$ 和策略 $\pi_\xi$ 进行最大似然估计：<br>$\mathcal{L}</em>{\text{sft}}(\xi) = -\sum_{t=0}^{H}\lambda^t~\mathbb{E}_{(\mathbf{s}_t, \mathbf{a}<em>t)\sim \mathcal{D}</em>{\text{new}}}\log\pi_\xi(\mathbf{a}_t|h_\xi(\mathbf{s}_t))$。</p>
<p><strong>3. 真实世界部署</strong><br>在真实世界部署时，模型输入状态由两部分组成：机器人本体感知状态（关节位置、速度等）和物体姿态。物体姿态通过一个感知管道进行估计：使用RGB-D相机捕获图像，通过SAM2模型获取物体分割掩码，再结合深度图、RGB图像、分割掩码和物体的3D网格，输入FoundationPose模型来估计并跟踪物体姿态。最后将估计的姿态与本体感知状态融合，输入到微调后的CDRED世界模型中生成控制机器人的动作。</p>
<p><img src="https://arxiv.org/html/2510.02538v1/x2.png" alt="状态预处理流程"></p>
<blockquote>
<p><strong>图2</strong>：真实世界部署期间的状态预处理流程。使用SAM2和FoundationPose从RGBD相机观测中估计物体姿态，该姿态与机器人的本体感知状态结合，形成世界模型的输入，进而生成机器人控制指令。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在ManiSkill3仿真环境和真实世界中进行。使用了两种机器人：定制机器人（7自由度机械臂，带导轨）和Franka Emika Panda机器人（仅仿真）。评估了六个任务：打开柜门、推立方体、抓取立方体、插入木钉、滚球、竖直提起木钉。前三个任务同时进行了仿真到仿真和仿真到真实评估，后三个任务仅进行仿真到仿真评估。</p>
<p><strong>基线方法</strong>：对比了两种标准的离线模仿学习方法：行为克隆（BC）和扩散策略（DP）。每种基线均在两种设置下评估：1) <strong>直接</strong>：仅在新域的微调数据集上训练；2) <strong>预训练+微调</strong>：先在原域专家数据集上预训练，再在新域数据集上微调。</p>
<p><strong>仿真到仿真迁移结果</strong>：考虑了两种域差距：1) <strong>有偏姿态估计</strong>：在新域中为物体姿态引入偏差；2) <strong>机器人运动学差距</strong>：改变机械臂导轨的位置并附加姿态偏差。</p>
<p><img src="https://arxiv.org/html/2510.02538v1/x3.png" alt="仿真到仿真域差距"></p>
<blockquote>
<p><strong>图3</strong>：仿真到仿真域迁移中考虑的两种域差距图示。在目标域微调时，策略必须适应有偏的姿态估计或导轨位置的移动。</p>
</blockquote>
<p>表I展示了详细结果。在所有6个任务的9种不同设定下，本文的CDRED-WM方法均取得了最佳性能。平均成功率高达76.0%，显著优于最好的基线方法（44.3%）。特别是在插入木钉、竖直提起木钉等任务中，性能提升超过30%。这表明在线模仿预训练结合离线微调在域迁移任务中具有强大的鲁棒性和泛化能力。</p>
<p><strong>仿真到真实迁移结果</strong>：在打开柜门、推立方体、抓取立方体三个真实任务上评估。使用30条真实专家轨迹进行微调，并在10个回合中进行测试。<br>表IV结果显示，CDRED-WM方法在总成功次数（27/30）上显著优于所有基线方法，证明了其在仿真到真实迁移中的有效性。</p>
<p><strong>消融分析与优势</strong>：论文通过可视化对比了离线专家数据集和在线探索数据的覆盖范围。</p>
<p><img src="https://arxiv.org/html/2510.02538v1/x6.png" alt="数据覆盖对比"></p>
<blockquote>
<p><strong>图4</strong>：在插入木钉任务中，专家数据集和CDRED-WM在线探索所覆盖的状态空间（以机器人六个关节角表示）可视化。结果表明，在线探索有效地覆盖了专家数据集中未出现的状态空间区域。</p>
</blockquote>
<p>分析指出，在线模仿预训练带来了两大核心优势：1) <strong>更好的数据覆盖</strong>：在线交互扩大了状态空间的覆盖，改善了泛化能力并缓解了偏差累积，这在插入木钉等任务中至关重要。2) <strong>微调后性能退化更少</strong>：相比于仅拟合离线专家数据的BC和扩散策略，基于在线探索预训练的方法在用小数据微调后，性能下降幅度更小，对分布偏移更加鲁棒。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个针对有限数据 regime 的高效仿真到真实框架，其核心是通过在线模仿学习预训练世界模型，随后进行离线微调。</li>
<li>通过系统实验证明，与离线模仿学习方法相比，该方法在分布外泛化、以及对小样本微调后的性能退化鲁棒性方面均表现更优。</li>
<li>提供了实证分析，将性能提升归因于在线探索阶段实现的更优数据覆盖。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法仍然依赖于仿真器进行预训练，且其实世界部署管道依赖于物体姿态估计模块（SAM2, FoundationPose），这些模块的误差可能影响最终性能。</p>
<p><strong>研究启示</strong>：本文的工作表明，在模仿学习中引入主动的、奖励自由的在线探索，是解决小数据下仿真到真实迁移和分布泛化问题的有效途径。这为未来研究指明了方向：可以探索更高效或更通用的世界模型结构，尝试将类似的在线预训练思想与更复杂的观测（如原始图像）结合，以及研究如何进一步减少对精确状态估计或特定物体模型的依赖。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对真实专家数据有限的模仿学习问题，提出一种高效的模拟到现实迁移框架。该方法采用两阶段流程：先在仿真中通过在线模仿预训练世界模型，扩大状态覆盖；再使用少量真实演示进行离线微调。关键技术包括利用潜在世界模型高效学习，并采用CDRED奖励模型从交互中生成奖励信号。实验表明，该方法显著提升了泛化能力和微调鲁棒性，在模拟到模拟迁移中成功率至少提升31.7%，在模拟到现实迁移中至少提升23.3%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.02538" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>