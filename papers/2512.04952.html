<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.04952" target="_blank" rel="noreferrer">2512.04952</a></span>
        <span>作者: Hang Zhao Team</span>
        <span>日期: 2025-12-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人领域的视觉-语言-动作（VLA）模型主要分为扩散模型和自回归模型两类。扩散模型在操作精度上表现出色，但在利用关键视觉和语言线索方面存在不足。自回归VLA模型在指令跟随、场景泛化和常识知识迁移方面潜力巨大，且与成功的视觉语言模型（VLM）架构最为相似。然而，自回归VLA模型面临两个核心挑战：一是需要一种合适的<strong>动作标记化</strong>方案，将连续动作序列离散化为动作标记；二是其<strong>推理效率</strong>显著低于扩散模型。一个有效的动作标记化方法需满足四个要求：高压缩效率、鲁棒的重建质量、对动作序列二维结构（动作维度和时间维度）的建模能力以及跨任务和具身的灵活性。现有方法未能全面满足这些原则。本文针对自回归VLA模型在动作标记化效率与重建质量之间的权衡以及推理速度慢的痛点，提出了FASTer框架。其核心思路是：通过一个可学习的神经动作标记器（FASTerVQ）高效压缩动作序列，并基于此构建一个采用块状自回归解码的VLA模型（FASTerVLA），从而实现更快的推理速度和更强的任务性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>FASTer框架包含两个互补组件：动作标记器FASTerVQ和自回归VLA策略FASTerVLA。</p>
<p><img src="https://arxiv.org/html/2512.04952v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：FASTer结合了可学习的动作标记器（FASTerVQ）和自回归VLA模型（FASTerVLA），在八个真实和模拟的具身环境中实现了高效压缩、快速控制和强大性能。</p>
</blockquote>
<p><strong>FASTerVQ（动作标记器）</strong>：其目标是学习将连续动作块编码为离散代码。流程如下：</p>
<ol>
<li><strong>动作分块器</strong>：首先对原始动作序列进行二维非均匀分块。时间维度被均匀分成<code>m</code>组，每组长度<code>h</code>。动作维度则根据物理特性（如末端执行器位置、姿态、夹爪状态）非均匀地分成<code>n</code>组。每组填充至最大组尺寸<code>d</code>，最终得到一个<code>(m·n)</code>个形状为<code>(h·d)</code>的补丁张量。这解决了动作维度数据分布不均的问题，并提高了每个令牌的信息密度。</li>
<li><strong>残差VQ动作标记器</strong>：采用基于Transformer的残差向量量化架构。编码器<code>φ_enc</code>将动作补丁下采样为潜在嵌入<code>z</code>。随后应用具有<code>N_c</code>个量化级的残差VQ：<code>r_1 = z</code>，对于第<code>i</code>级，量化器<code>Q_i</code>从码本中找到<code>r_i</code>的最邻近向量，残差<code>r_{i+1} = r_i - Q_i(r_i)</code>，量化后的潜在嵌入<code>z_q</code>为各级量化向量的和。这产生一个离散代码张量<code>C</code>，作为下游策略的动作令牌。解码器<code>φ_dec</code>将<code>z_q</code>重建为动作补丁。</li>
<li><strong>训练目标</strong>：损失函数包含三部分：动作信号本身的L1重建损失、动作信号离散余弦变换（DCT）的L1重建损失（用于捕捉全局趋势），以及一项承诺损失（commitment loss）<code>λ · ||z - sg(z_q)||_2^2</code>，其中<code>sg</code>表示停止梯度操作。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.04952v2/x2.png" alt="标记器细节"></p>
<blockquote>
<p><strong>图2</strong>：FASTerVQ。(a) 原始动作序列根据其具身配置被分块成紧凑令牌。(b) FASTerVQ采用DCT和L1重建损失，并应用RVQ，将动作编码为<code>N_c</code>个代码级别；每个级别可重塑为<code>C_h × C_a</code>张量。</p>
</blockquote>
<p><strong>FASTerVLA（自回归VLA策略）</strong>：基于FASTerVQ构建的高效策略模型。</p>
<ol>
<li><strong>架构</strong>：遵循标准VLM结构（视觉塔、投影层、基于Transformer的语言主干），以确保与预训练权重的兼容性。关键创新包括：<ul>
<li><strong>轻量级动作专家</strong>：添加一个与主干架构相同但参数更少的轻量级专家网络。主干编码多模态上下文一次，而该专家则自回归地从这些特征中解码动作令牌。</li>
<li><strong>位置编码与间隔增强</strong>：采用RoPE位置编码。在训练时，对相邻动作令牌之间的相对位置偏移施加一个小的整数抖动，以减轻模型对绝对位置的过拟合。</li>
</ul>
</li>
<li><strong>块状自回归解码</strong>：这是提升推理效率的核心。传统自回归逐个预测令牌（公式2）。BAR将令牌序列<code>C</code>划分为<code>J</code>个连续的块，每块大小为<code>B</code>。训练目标变为预测下一个块中的所有令牌（公式3）。推理时，使用块状因果掩码，允许块内令牌相互关注，从而将所需的前向传播次数从<code>N</code>减少到约<code>N/B</code>。</li>
<li><strong>解码顺序</strong>：FASTerVLA按照码本优先、时间维度其次的顺序进行分层解码。即，对于一个码本，先沿时间维度<code>0,1,...,C_h-1</code>解码所有令牌，再进入下一个码本。这符合RVQ从粗到细的特性，提高了表示效率并稳定了训练和推理。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.04952v2/x3.png" alt="VLA模型细节"></p>
<blockquote>
<p><strong>图3</strong>：FASTerVLA。(a) 模型将RGB图像、本体感知状态和语言指令输入基于Transformer的VLM。动作专家自回归地生成离散动作令牌，VQ解码器将其映射为最终的连续动作序列。(b) 代码按码本顺序生成，然后在时间维度上推进，比按时间优先的解码（红色箭头）更稳定。(c) 左：普通因果掩码；右：块状因果掩码，块内令牌可以关注前面及块内的令牌。令牌颜色表示不同模态。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法相比，FASTer的主要创新在于：1) 提出了一个专门为机器人动作序列设计的、结合非均匀分块和Transformer-RVQ的高效动作标记器；2) 引入了块状自回归解码，显著减少了自回归步数；3) 设计了与VLM主干对齐的轻量级动作专家，以参数高效的方式弥合语言推理与连续控制之间的模态差距。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在九个基准测试上进行了评估，涵盖五个不同的具身环境（模拟与真实世界），包括变形物体操作、全身控制、指令跟随和长时程操作等任务。基线模型包括扩散模型（如Diffusion Policy, Octo-Base）和自回归VLA模型（如π₀-FAST、OpenVLA、MiniVLA等）。模型通常从在大规模机器人数据上预训练的检查点初始化。</p>
<p><img src="https://arxiv.org/html/2512.04952v2/x4.png" alt="主要结果表"></p>
<blockquote>
<p><strong>图4</strong>：策略在不同具身和环境下的性能。结果报告于分布内设置，涵盖两个真实世界具身和三个模拟设置。</p>
</blockquote>
<p><strong>关键实验结果（策略性能）</strong>：</p>
<ul>
<li>在<strong>LIBERO</strong>基准测试（四个长时程操作任务）上，FASTer的平均成功率达到**97.9%**，超越了所有基线，包括当前最强的自回归模型π₀-FAST-D（94.2%）和扩散模型π₀5（96.8%）。</li>
<li>在<strong>Simpler-Bridge</strong>基准测试（四个模拟桌面操作任务）上，FASTer的平均成功率为**87.9%**，显著优于π₀-FAST-D的76.5%和其他自回归模型。</li>
<li>图4展示了跨多个具身和环境的综合性能，FASTer在大多数任务上取得了最佳或接近最佳的表现。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.04952v2/x5.png" alt="标记器重建质量"></p>
<blockquote>
<p><strong>图5</strong>：不同动作标记器在多个误差容忍度σ下的平均VRR（方差减少率）。FASTer在所有尺度上都取得了最佳性能，并表现出清晰的数据缩放行为。</p>
</blockquote>
<p><strong>FASTerVQ标记器分析</strong>：</p>
<ul>
<li><strong>重建质量与压缩率</strong>：图6显示，在不同动作时程下，FASTerVQ在压缩率（令牌数少）和重建误差之间取得了最佳平衡，特别是对于长序列。</li>
<li><strong>数据缩放性</strong>：图5表明，随着训练数据量增加，FASTerVQ的重建质量（以方差减少率衡量）持续提升，显示出良好的数据缩放行为。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.04952v2/x6.png" alt="压缩与重建权衡"></p>
<blockquote>
<p><strong>图6</strong>：压缩率与重建权衡跨动作时程的比较，其中FASTer实现了最佳平衡，尤其是对于长序列。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ul>
<li><strong>块状自回归解码（BAR）的作用</strong>：在Simpler-Bridge任务上，移除BAR的FASTer（w/o BAR）性能为81.0%，而完整FASTer达到87.9%，表明BAR对性能有显著提升。</li>
<li><strong>解码顺序</strong>：实验表明，按码本优先（Codebook-first）的解码顺序比时间优先（Horizon-first）的顺序产生更稳定的训练损失和更好的最终性能。</li>
<li><strong>标记器设计</strong>：消融实验证实，非均匀分块、RVQ结构以及结合时域和频域（DCT）的损失函数都对标记器的最终性能有积极贡献。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>FASTerVQ</strong>，一个紧凑、高压缩率的动作标记器，它通过Transformer-RVQ架构和针对动作序列特性的设计，在重建保真度和代码长度之间实现了优越的权衡。</li>
<li>提出了<strong>FASTerVLA</strong>，引入了<strong>块状自回归解码</strong>和<strong>轻量级动作专家</strong>，使得自回归VLA模型在保持高精度的同时，推理速度大幅提升（论文中提到最高可达3倍加速）。</li>
<li>建立了一个覆盖多具身、多任务的综合基准，并进行了系统性的实验，证明了FASTer框架在模拟和真实场景中的先进性、高效性和强泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，FASTerVLA的性能在一定程度上依赖于预训练的VLM主干。此外，虽然推理速度显著提升，但在需要极低延迟的实时控制场景中，可能仍需进一步优化。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>动作标记化作为独立模块</strong>：FASTerVQ的成功表明，专门为机器人动作设计的、可学习的标记器是一个富有前景的研究方向，可以独立于具体的VLA策略进行优化和评估。</li>
<li><strong>高效自回归解码范式</strong>：块状自回归解码（BAR）为缓解自回归模型固有的序列生成延迟提供了一种有效思路，可被借鉴到其他需要长序列生成的机器人学习任务中。</li>
<li><strong>跨模态统一表征</strong>：FASTer框架尝试将动作编码为与语言、视觉对齐的离散令牌，这启示了未来构建更统一的、所有模态共享同一离散令牌空间的多模态具身智能体可能性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自回归视觉语言动作模型在动作标记化时面临的重建保真度与推理效率的权衡问题，提出FASTer框架。其核心技术包括：1) 可学习的动作标记器FASTerVQ，将动作块编码为单通道图像以捕获时空依赖性；2) 基于此的自回归策略FASTerVLA，采用块状解码与轻量级动作专家。实验表明，该框架在模拟与真实基准测试中，同时实现了更快的推理速度与更高的任务性能，超越了现有先进模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.04952" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>