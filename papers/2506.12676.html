<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Goal-based Self-Adaptive Generative Adversarial Imitation Learning (Goal-SAGAIL) for Multi-goal Robotic Manipulation Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Goal-based Self-Adaptive Generative Adversarial Imitation Learning (Goal-SAGAIL) for Multi-goal Robotic Manipulation Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.12676" target="_blank" rel="noreferrer">2506.12676</a></span>
        <span>作者: George Vogiatzis Team</span>
        <span>日期: 2025-06-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在多目标机器人操作任务中，强化学习面临目标空间多样性和复杂性、奖励稀疏等挑战。后见经验回放（HER）通过目标重标记提高了数据效率。生成对抗模仿学习（GAIL）及其目标条件变体（Goal-GAIL）结合了演示数据以加速训练。然而，现有方法严重依赖于演示数据的质量和数量。在实践中，从人类遥操作获取的演示数据往往是有限且次优的，这导致学习过程偏向于掌握较简单的子任务，而非更具挑战性的目标。此外，虽然自适应性生成对抗模仿学习（SAIL）通过用智能体生成的高质量轨迹替换次优演示来提升性能，但其设计并未考虑多目标场景，因为直接比较不同目标的轨迹回报是无效的。本文针对“在有限、次优演示下，如何高效地进行多目标模仿学习”这一痛点，提出将自适应性学习原则扩展到目标条件的GAIL中。其核心思路是：设计一个基于目标的机制，选择相对于现有演示更高质量的自生成成功轨迹，并将其纳入专家数据集，从而在整个学习过程中实现平滑有效的自适应过渡。</p>
<h2 id="方法详解">方法详解</h2>
<p>Goal-SAGAIL的整体框架建立在Goal-GAIL之上，核心创新在于引入了一个基于目标的自生成轨迹选择模块，用于动态更新专家回放缓冲区。其流程主要分为三步：1）数据收集与存储（包含轨迹选择）；2）判别器训练；3）结合HER的策略更新。</p>
<p><img src="https://arxiv.org/html/2506.12676v1/extracted/6542149/Diagrams/plappert2018multi_fetch_push.png" alt="算法流程"></p>
<blockquote>
<p><strong>图1</strong>：Fetch Push任务环境示意图。该任务是论文中用于验证方法的多个多目标机器人操作任务之一。</p>
</blockquote>
<p>核心模块是<strong>基于目标的自生成轨迹选择机制</strong>。该方法旨在为每个新收集的成功轨迹 τi，在专家缓冲区 RE 中找到一个难度最相似的专家轨迹 τe(min) 进行公平比较。轨迹的难度通过“目标对”（goal-pair） gp=[ginit, gd] 来表征，其中 ginit 是初始达成目标，gd 是原始目标目标。两个轨迹 τi 和 τe 之间的组合目标对距离定义为：dcomb(τi, τe)=d(giniti, ginite)+d(gdi, gde)。在专家缓冲区中寻找具有最小 dcomb 的轨迹作为 τe(min)。如果最小距离 dcomb(τi, τe(min)) 小于等于阈值 Ccomb，则进一步比较两条轨迹的回合累计回报。若自生成轨迹的回报更高，则将其加入专家缓冲区 RE，否则存入智能体自生成缓冲区 RB。如果最小距离大于阈值 Ccomb，说明该自生成轨迹与所有现有演示都显著不同，则直接将其视为高质量轨迹加入 RE。专家缓冲区 RE 采用先进先出（FIFO）管理，确保其持续更新。</p>
<p>在模型训练方面，<strong>判别器 D(s,g,a)</strong> 的训练与Goal-GAIL一致，其损失函数为公式（2），旨在区分来自专家策略和智能体策略的（状态，目标，动作）三元组。<strong>策略学习</strong>部分采用DDPG作为基础RL算法，并集成HER。在经验回放阶段，从 RB 和 RE 的联合池中均匀采样，并应用HER进行目标重标记。奖励函数结合了环境稀疏奖励 renv 和GAIL奖励 rGAIL = λ · log(D(s,g,a))，其中 λ 是平衡系数。算法伪代码清晰展示了数据收集与选择（第9-21行）、判别器训练（第22-27行）和策略更新（第28-34行）三个阶段的循环。</p>
<p>与现有方法相比，Goal-SAGAIL的创新点具体体现在：1）将SAIL的自适应思想成功推广到多目标领域；2）提出了基于“目标对距离”的轨迹难度度量与比较方法，解决了多目标下轨迹质量无法直接对比的问题；3）通过动态更新专家数据集，使学习过程能够逐步超越初始次优演示的限制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Gymnasium-Robotics仿真环境中的多个多目标机器人操作任务上进行，包括：Fetch Push（图1）、Fetch Pick and Place（图2）、Hand Egg（图3）和Hand Block（图4）。此外，还使用Leap Motion采集了人类遥操作Shadow Hand完成方块旋转任务的真实演示数据进行学习（图9，图10）。对比的基线方法包括：DDPG+HER、GAIL、Goal-GAIL以及SAIL。</p>
<p><img src="https://arxiv.org/html/2506.12676v1/x1.png" alt="多任务成功率曲线"></p>
<blockquote>
<p><strong>图5</strong>：在四个仿真任务上，不同方法的平均测试成功率随训练周期变化曲线。Goal-SAGAIL（红色实线）在大部分任务上收敛速度最快且最终性能最高。</p>
</blockquote>
<p>关键实验结果显示，在使用次优演示（仅包含简单目标）时，Goal-SAGAIL在所有四个仿真任务上的学习效率均显著优于Goal-GAIL。例如，在Hand Block任务中，Goal-SAGAIL在约40个周期后成功率超过80%，而Goal-GAIL需要约80个周期。在最终性能上，Goal-SAGAIL也表现出优势。</p>
<p><img src="https://arxiv.org/html/2506.12676v1/x2.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：在Fetch Push任务上的消融实验结果。比较了Goal-SAGAIL与其变体：无自适应机制（即Goal-GAIL）、无阈值Ccomb（Always Compare）、以及不同Ccomb值的影响。</p>
</blockquote>
<p>消融实验验证了各个组件的贡献。图6表明，移除自适应机制（即使用固定次优演示）性能显著下降；而“Always Compare”策略（即无Ccomb阈值，总是比较回报）性能也不佳，说明在轨迹难度差异过大时强制比较是有害的。选择合适的Ccomb阈值对性能至关重要。</p>
<p><img src="https://arxiv.org/html/2506.12676v1/extracted/6542149/Diagrams/LfD_fromhuman_teleop_humandemo200Alltask_distribution.png" alt="人类演示数据分布及学习结果"></p>
<blockquote>
<p><strong>图11</strong>：人类遥操作演示数据的目标对分布。可见演示集中在相对容易的区域（对角线附近），分布不均匀且存在聚类，属于典型的有限、次优演示。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.12676v1/extracted/6542149/Diagrams/LfD_success_rate_demoweight0_5anneal_human200demoAlltask.png" alt="从人类演示学习的结果"></p>
<blockquote>
<p><strong>图12</strong>：使用200条人类遥操作演示进行学习时，Goal-SAGAIL与Goal-GAIL的成功率对比。Goal-SAGAIL能更快地从有限的、分布有偏的人类演示中学习并取得更高成功率。</p>
</blockquote>
<p>在使用真实人类遥操作数据的实验中，Goal-SAGAIL同样展现出优势，能够更快地适应并超越人类演示的水平，验证了其在现实场景中的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了Goal-SAGAIL框架，首次将自适应性学习原则与目标条件GAIL相结合，专门用于解决多目标操作任务中演示数据次优且有限的问题。2）设计了基于“目标对”的轨迹难度度量与选择机制，实现了多目标场景下自生成轨迹与专家轨迹的公平质量比较。3）在仿真及真实人类演示数据上的大量实验表明，该方法能显著提升学习效率，使智能体能够逐步超越初始演示的限制。</p>
<p>论文自身提到的局限性包括：组合距离阈值Ccomb的选择需要根据任务经验进行调整；自生成轨迹的不断引入可能带来潜在偏差，需要谨慎管理缓冲区。</p>
<p>这项工作对后续研究的启示在于：为从次优演示中学习多目标技能提供了新思路，其基于目标的适应性机制可以扩展到其他结合演示学习的范式中。未来可以探索更智能的阈值自适应方法，或者将该框架与能主动生成困难样本的课程学习等方法结合，以进一步缓解演示数据分布偏差问题。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多目标机器人操作任务中示范数据有限且不理想、导致模仿学习偏向简单子任务的问题，提出了一种基于目标的自适应生成对抗模仿学习框架（Goal-SAGAIL）。该方法将自适应学习机制与目标条件生成对抗模仿学习（GAIL）相结合，以提升在次优示范下的学习效率。实验表明，该方法在包括复杂手内操作在内的多种多目标场景中，能显著提高学习效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.12676" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>