<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.21126" target="_blank" rel="noreferrer">2509.21126</a></span>
        <span>作者: Wu, Xiefeng, Zhao, Jing, Zhang, Shu, Hu, Mingyu</span>
        <span>日期: 2025/09/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在线强化学习（RL）在无需专家演示的情况下展现出巨大潜力，但其关键局限在于样本效率低下，智能体需要大量探索才能发现有效策略。为提升智能体在新任务上的性能，当前主要有两大方向：一是利用少量专家演示微调预训练的视觉语言模型（VLM）以获取视觉语言动作（VLA）策略，但这仍需要收集专家数据；二是利用VLM为新任务构建奖励函数（即奖励塑形），随后进行RL训练，这虽然免去了专家数据，但其有效性受限于VLM偏好评分的准确性。</p>
<p>本文针对上述两种方法的局限性，提出了一种新的在线强化学习框架VARL。其核心思路是：不将VLM用作奖励设计器或直接的动作提供者进行微调，而是将其作为“动作建议器”，在训练期间为RL智能体提供启发式动作建议，并通过一种策略塑形机制将这些建议整合到策略学习中，从而丰富探索、提高样本效率，同时保证不改变原任务的最优性和收敛性。</p>
<h2 id="方法详解">方法详解</h2>
<p>VARL框架包含两个关键组件：一个基于VLM的动作生成器，以及一个策略塑形模块。</p>
<p><img src="https://arxiv.org/html/2509.21126v1/x2.png" alt="VLM动作生成过程"></p>
<blockquote>
<p><strong>图2</strong>：VLM动作生成过程。该过程分为两个阶段：(1) 从智能体的轨迹中收集转移数据；(2) 使用转移信息连同提示词查询VLM，以获得建议动作。这使得强化学习智能体能够通过整合VLM建议的动作来优化其策略。</p>
</blockquote>
<p><strong>整体流程</strong>：如算法1所示，在标准RL训练循环中，VARL周期性地（根据触发步集合 S_h）从经验回放缓冲区 D 中采样一批近期转移数据。对于这批数据中的每个状态 s，通过一个VLM动作生成器函数 ℱ_vlm 获取一个建议动作 a_vlm，并将状态-动作对 (s, a_vlm) 存入一个独立的启发式缓冲区 D_llm 中。策略更新时，会同时利用标准RL损失和基于 D_llm 的策略塑形损失。</p>
<p><strong>核心模块1：VLM动作生成器</strong>。该模块 ℱ_vlm: (s, a) ↦ a_vlm 以状态-动作对为输入，输出一个精炼的或替代的动作建议。它仅在特定训练步骤被调用，显著降低了VLM推理的计算开销。其有效性很大程度上依赖于提示词的设计，提示词需编码任务指令、上下文信息和动作空间约束，以确保生成的启发式动作有意义且符合环境要求。</p>
<p><strong>核心模块2：策略塑形</strong>。此模块通过一个门控的行为克隆（BC）损失，将VLM生成的启发式动作整合到策略训练中。设 π_θ(a|s) 为策略，Q(s,a) 为 critic 网络输出的Q值（取双Q网络的最小值）。策略塑形损失 ℒ_ps 定义为：<br>ℒ_ps = λ * 𝔼_{(s, a_llm)~D_llm} [ g(s, a_llm) * ( -log π_θ(a_llm|s) ) ]<br>其中 λ 是控制引导权重的系数。**关键创新在于门控函数 g(s, a_llm)**：</p>
<ul>
<li><strong>离散动作空间</strong>：g(s, a_llm) = 1 [ a_llm ≠ argmax_a Q(s, a) ]。即仅当VLM建议的动作与critic认为的最优动作不一致时，才施加BC损失，避免对同一动作的重复强化导致策略熵快速下降和critic学习不稳定。</li>
<li><strong>连续动作空间</strong>：g(s, a_llm) = 1 [ (a_llm - μ_θ(s))^⊤ Σ_θ(s)^{-1} (a_llm - μ_θ(s)) &gt; κ^2 ]。其中策略分布为高斯分布 π_θ(·|s) = N(μ_θ(s), Σ_θ(s))。仅当VLM动作落在当前策略的高概率椭球区域之外时，才应用BC损失。</li>
</ul>
<p>最终的演员（actor）损失结合了标准策略损失 ℒ_π（如SAC的熵正则化损失）和策略塑形损失：<br>ℒ_actor = ℒ_π + 1_{t ≤ N_s} * ℒ_ps<br>其中 1_{t ≤ N_s} 是一个指示函数，仅在训练步数 t 小于预设的截止步数 N_s 时激活策略塑形项，以防止策略过度拟合到可能次优的启发式动作上，确保最终收敛到局部最优策略。</p>
<p><strong>与现有方法的创新对比</strong>：1) <strong>与VLM奖励塑形方法对比</strong>：VARL提供动作建议而非设计奖励，避免了因VLM偏好不准确而引入的奖励偏差，保证了RL算法的原有最优性；同时计算开销更低，无需为训练奖励模型而进行大量轨迹收集和重复的VLM查询。2) <strong>与VLA策略微调对比</strong>：VARL不需要任何专家演示数据，完全在线进行。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：VARL在10个任务上进行了评估，覆盖多样化环境：包括来自Meta-World的3个状态型、2个视觉型机械臂操控任务；来自AI2-THOR的3个视觉型导航任务；以及2个真实世界（使用Realman RM-65B机械臂）的视觉型操控任务。使用SAC作为RL求解器，GPT-5作为VLM。</p>
<p><strong>对比基线</strong>：在样本效率实验中，对比了原始SAC和用专家数据预填充缓冲区的SAC。在与奖励塑形方法的对比中，比较了RL-VLM-F和ERL-VLM。</p>
<p><img src="https://arxiv.org/html/2509.21126v1/x4.png" alt="不同基线在七个环境中的学习曲线"></p>
<blockquote>
<p><strong>图4</strong>：VARL、SAC+专家数据和原始SAC在七种环境中的学习曲线。VARL在所有任务中都显著提高了样本效率，在稀疏奖励或事件驱动奖励任务中优势尤其明显，甚至在部分密集奖励任务中超过了使用专家数据的SAC。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>作为提升样本效率的通用框架</strong>：如图4所示，VARL在各种任务设置下一致地提升了样本效率。在仅提供事件驱动奖励（稀疏奖励）的环境中，原始SAC因难以通过随机探索触发奖励而学习缓慢，而VARL利用VLM提供的全局观察进行定向探索，能够有效学习。在密集奖励任务中，VARL凭借策略塑形机制实现了快速策略拟合，其收敛速度甚至超过了使用专家演示的SAC。</li>
<li><strong>低计算开销与性能优势</strong>：与奖励塑形方法RL-VLM-F和ERL-VLM相比，VARL在更具挑战性的任务初始化设置下（图5a），取得了可比甚至更优的性能（图5b）。更重要的是，VARL的VLM查询次数和总参考样本数远低于奖励塑形方法（表I），例如VARL仅需3次查询（每次批量大小500），而对比方法需要5000次查询，计算开销大幅降低。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.21126v1/x5.png" alt="与奖励塑形方法的对比"></p>
<blockquote>
<p><strong>图5</strong>：(a) 对比实验的任务设置差异，VARL面对更难的初始位置。(b) 在三个操控任务中，VARL与奖励塑形方法RL-VLM-F和ERL-VLM的成功率对比。VARL取得了可比或更优的性能。</p>
</blockquote>
<p><strong>表I：VLM查询使用情况对比</strong></p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">#Queries</th>
<th align="left">mbsize</th>
<th align="left">Total samples</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ERL-VLM</td>
<td align="left">5000</td>
<td align="left">50</td>
<td align="left">250,000</td>
</tr>
<tr>
<td align="left">RL-VLM-F</td>
<td align="left">5000</td>
<td align="left">128</td>
<td align="left">640,000</td>
</tr>
<tr>
<td align="left"><strong>VARL</strong></td>
<td align="left"><strong>3</strong></td>
<td align="left"><strong>500</strong></td>
<td align="left"><strong>1,500</strong></td>
</tr>
</tbody></table>
<ol start="3">
<li><strong>实现真实世界在线RL</strong>：在真实机器人实验中，VARL仅使用用户友好的稀疏奖励信号（如基于距离），便能从零开始在线学习。如图4(f,g)所示，VARL在大约3000次交互后学会了到达特定目标位置，在30k-50k次交互内学会了将方块推到指定位置。</li>
<li><strong>消融与敏感性分析</strong>：如图6所示，对关键超参数λ（引导权重）和 N_s（截止步数）的敏感性分析表明，VARL对这些超参数并不高度敏感，在较大取值范围内性能保持稳健。适中的λ值（如50）能取得最佳性能，而过大的λ（如100）会抑制自主学习导致性能下降。较小的 N_s 会减少早期收益但允许更快收敛到长期最优策略，过大的 N_s 则会使智能体过度偏向启发式动作。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.21126v1/x6.png" alt="超参数敏感性分析"></p>
<blockquote>
<p><strong>图6</strong>：超参数敏感性分析。左图：不同引导权重λ下的成功率曲线。右图：不同截止步数 N_s 下的成功率曲线。结果表明VARL对这些超参数不高度敏感，具有较强的鲁棒性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) <strong>提出VARL框架</strong>：创新性地将VLM作为动作建议器而非奖励设计器引入在线RL，通过策略塑形整合启发式动作，显著提高了样本效率，且不改变RL的最优性。2) <strong>引入门控策略塑形机制</strong>：设计了离散和连续动作空间下的门控函数，有效防止了策略过度拟合到可能次优的VLM动作，保证了训练的稳定性。3) <strong>验证了真实世界在线RL的可行性</strong>：展示了VARL在仅使用稀疏奖励的真实机器人任务中，能够从零开始进行有效的在线学习，为免演示、免精确建模的机器人学习提供了新路径。</p>
<p><strong>局限性</strong>：论文自身提及的局限性包括：未来计划将VARL扩展到更复杂的真实世界任务，并研究集成视频生成技术以进一步丰富启发式引导，以及将其应用扩展到更广泛的具身环境（超越机器人操控）。</p>
<p><strong>对后续研究的启示</strong>：VARL为结合大型模型先验知识与在线RL提供了一种高效、轻量的新范式。其“动作建议”的思路避免了直接修改奖励函数可能引入的偏差，且计算开销极低，这使得在真实物理环境中直接进行在线RL训练变得更为可行。门控机制的设计也为如何安全、有效地将外部知识（可能不完美）整合到RL训练中提供了有价值的参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对在线强化学习在复杂任务中样本效率低下的核心问题，提出VARL框架。其关键技术是利用视觉语言模型作为动作顾问，在训练过程中为智能体提供动作建议，而非设计启发式奖励，从而保证最优性与收敛性不变。该方法通过增加样本多样性来提升效率，尤其在稀疏奖励任务中效果显著。实验表明，VARL能大幅提高样本效率，且未引入显著计算开销。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.21126" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>