<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.02324" target="_blank" rel="noreferrer">2509.02324</a></span>
        <span>作者: Zhou, Changshi, Xu, Haichuan, Gu, Ningquan, Wang, Zhipeng, Cheng, Bin, Zhang, Pengpeng, Dong, Yanchao, Hayashibe, Mitsuhiro, Zhou, Yanmin, He, Bin</span>
        <span>日期: 2025/09/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，语言引导的机器人操作主要依赖两类方法：一是基于视觉语言模型的方法，它们能将语言目标与视觉区域对齐，实现开放词汇操作，但通常将语言视为静态目标描述符，缺乏对复杂多步指令的语义推理能力；二是基于大语言模型的任务规划方法，它们利用组合推理能力将高级目标分解为结构化子任务，但大多在状态空间简单、动态稳定的刚性物体领域验证，在视觉复杂、可形变的操作任务中效果尚未充分探索。本文针对的具体痛点是：如何有效结合LLMs的推理能力与VLMs的视觉基础能力，以应对可变形物体（如布料）操作中面临的高自由度、复杂动力学以及需要将高层次语言指令转化为可执行的低级动作序列的挑战。本文的核心思路是提出一个统一框架，利用LLM将高级语言指令分解为可解释的子任务，再通过一个经过微调的VLM感知模块将每个子任务落实到具体的视觉场景中，从而实现从语言到动作的端到端控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架是一个集成了自动语音识别、任务规划、视觉感知和动作执行的完整流程。输入是用户的口头命令和初始RGB-D观测，输出是机器人执行的动作序列。具体流程如算法1所示：首先，ASR模块将语音转录为文本指令；接着，任务规划模块将该指令解析为一序列细粒度的子任务；然后，系统进入感知-动作循环，针对每个子任务，视觉模块根据当前观测和子任务指令预测一个低级动作（抓取和放置位置），并由动作执行器执行；执行后更新观测，循环直至所有子任务完成。</p>
<p><img src="https://arxiv.org/html/2509.02324v1/fig1.jpg" alt="方法概述"></p>
<blockquote>
<p><strong>图1</strong>：方法整体概览。展示了在真实世界布匹操作任务中，集成自动语音识别、任务规划、视觉感知和动作执行的机器人-LLM系统工作流程。</p>
</blockquote>
<p>核心模块包括任务规划模块和视觉感知模块。</p>
<ol>
<li><strong>任务规划模块</strong>：使用GPT-4o作为规划器。通过精心设计的提示（结合上下文指令学习和思维链推理），将用户的高级语言指令分解为一系列与机器人可执行原语（如抓放操作）对齐的标准化子任务。例如，指令“将裤子从左到右对折”被分解为“抓住裤子的左腰并放到右腰”和“抓住左裤腿并放到右裤腿”两个原子动作。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.02324v1/fig2.jpg" alt="任务分解"></p>
<blockquote>
<p><strong>图2</strong>：任务分解示例。展示了使用基于LLM的规划器从高级指令进行任务分解的例子。</p>
</blockquote>
<ol start="2">
<li><strong>视觉感知模块</strong>：这是实现细粒度视觉基础的关键。给定RGB-D观测和一个子任务指令，该模块预测对应的抓取和放置位置。其架构如图3所示，包含三个部分：<ul>
<li><strong>多模态特征编码</strong>：采用冻结的SigLIP2模型分别编码图像和文本指令，得到视觉特征和语言特征。为了适配布料任务同时保留预训练知识，对注意力层的查询和值矩阵应用了权重分解低秩适应进行高效微调。语言特征根据连词“and”被分割为抓取和放置两个文本段。</li>
<li><strong>跨模态融合</strong>：提出一种双向交叉注意力机制。以抓取分支为例，一方面计算图像到语言的注意力（视觉特征作为查询，语言特征作为键和值），使视觉特征关注相关的文本描述；另一方面计算语言到图像的注意力（语言特征作为查询，视觉特征作为键和值），使文本特征聚焦于相关的视觉区域。两个注意力输出拼接后经投影并与原始视觉特征残差连接，得到融合后的特征。放置分支同理。</li>
<li><strong>动作解码</strong>：使用两个相同的卷积上采样网络解码器，分别处理抓取和放置分支的融合特征，通过Sigmoid激活输出空间概率热图。最终动作选择为热图中概率最高的像素位置。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.02324v1/fig3.jpg" alt="视觉感知模块架构"></p>
<blockquote>
<p><strong>图3</strong>：视觉感知模块架构。该模块使用冻结的SigLIP2模型从RGB-D图像和自然语言指令中提取特征。指令按“and”分割为抓取和放置段。每个段通过双向交叉注意力与视觉特征融合，其中文本和视觉特征被联合对齐。采用DoRA对冻结的SigLIP2模型进行高效微调以适应布料操作。融合后的特征通过卷积和上采样层解码，以预测相应的抓取和放置位置。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1) 提出了一个整合语音、LLM规划、VLM感知和执行的<strong>统一端到端框架</strong>，专门针对语言引导的长视野布料操作；2) 设计了一种<strong>双向交叉注意力融合机制</strong>，增强了文本指令与动态视觉场景之间的细粒度对齐；3) 采用<strong>DoRA微调策略</strong>，在高效适配下游任务的同时，最大程度保留了大规模预训练模型的知识。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真（基于SoftGym和PyFleX）和真实世界（UR5机器人）两种设置下进行。仿真评估使用了五个布料折叠任务：双直折、双三角折、四角内折、T恤折叠和裤子折叠。评估指标包括平均粒子距离、平均交并比和成功率。指令条件分为可见指令、不可见指令和不可见任务。对比的基线方法是Deng等人和Barbany等人的语言引导折叠方法。</p>
<p>关键实验结果：在仿真中，本文方法在三个指令条件下均优于基线（表I）。在可见指令下，平均成功率达91.70%，优于Barbany等人的89.47%；在不可见指令下，达89.60%；在最具挑战性的不可见任务下，达77.92%，显著优于Barbany等人的44.59%和Deng等人的42.80%，相对提升约33.3%。</p>
<p><img src="https://arxiv.org/html/2509.02324v1/fig4.jpg" alt="仿真定性结果（可见指令）"></p>
<blockquote>
<p><strong>图4</strong>：在五个折叠任务上（可见指令）的仿真定性结果。每个任务用两行表示：顶行显示俯视相机捕捉的动作序列，底行显示网络预测的抓取和放置热图。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.02324v1/ut_gai1.jpg" alt="仿真定性结果（不可见任务）"></p>
<blockquote>
<p><strong>图5</strong>：仿真中不可见任务的定性结果。每行展示了在不同布料类型上，于不可见任务设置下执行的多步折叠序列。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.02324v1/real.jpg" alt="真实机器人实验"></p>
<blockquote>
<p><strong>图6</strong>：真实机器人实验设置。展示了UR5机器人和工作区域。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.02324v1/real_exp1.jpg" alt="真实世界多步折叠结果"></p>
<blockquote>
<p><strong>图9</strong>：真实世界多步折叠任务的定性结果。展示了从语言指令到最终折叠状态的全过程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.02324v1/compare.jpg" alt="真实世界与基线对比"></p>
<blockquote>
<p><strong>图8</strong>：真实世界中与基线方法的定性对比。显示了不同方法在执行相同折叠指令后的最终状态。</p>
</blockquote>
<p>消融实验总结：</p>
<ol>
<li><strong>架构与主干网络</strong>（表II）：比较了不同VLM主干（SigLIP2, BLIP2, CLIP）和融合策略（交叉注意力 vs. Transformer）。结果表明，使用SigLIP2主干配合双向交叉注意力融合（即本文方法）取得了最佳的平均成功率（86.40%）。</li>
<li><strong>参数高效微调</strong>（表III）：比较了不同的微调方法（LoRA, IA3, DoRA）。结果显示，使用DoRA微调的性能最优，优于LoRA、IA3和不微调的情况。</li>
<li><strong>LLM规划器</strong>（表IV）：比较了不同LLM（GPT-4o, DeepSeek-V3, Grok-3, Doubao-1.5）的规划能力。GPT-4o在所有任务上均达到100%的分解成功率，表现最佳。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了一个集成语音输入、LLM驱动任务规划和VLM视觉基础的统一框架，用于语言引导的布料操作；2) 设计了基于LLM的规划器，能将高级指令分解为与机器人原语对齐的可解释子任务；3) 引入了基于冻结SigLIP2和双向交叉注意力的视觉感知模块，并采用DoRA进行高效微调，实现了细粒度的语言-视觉对齐。</p>
<p>论文自身提到的局限性包括：框架依赖于离线收集的专家演示数据，且视觉感知模块需要针对特定任务（如布料操作）进行微调。</p>
<p>本文工作的启示在于：它成功展示了如何将LLMs的抽象推理能力与VLMs的视觉基础能力深度结合，以解决可变形物体长视野操作这一复杂问题。这为未来在更具动态性、需要更高层次语义理解和物理交互的机器人任务中，融合不同基础模型的优势提供了有价值的范例。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对语言引导的可变形物体长时程操作难题，提出统一框架。核心结合基于LLM的规划器（将高级指令分解为动作原语）和基于VLM的感知系统（采用SigLIP2架构与双向交叉注意力融合、DoRA微调，实现细粒度视觉-语言对齐）。实验表明，在仿真中，该方法在已见指令、未见指令和未见任务上分别超越SOTA基线2.23%、1.87%和33.3%；真实机器人能鲁棒执行多步布料折叠，展现强泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.02324" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>