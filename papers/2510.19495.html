<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.19495" target="_blank" rel="noreferrer">2510.19495</a></span>
        <span>作者: Abhishek Gupta Team</span>
        <span>日期: 2025-10-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习通过专家人类演示训练机器人执行复杂任务已被证明是有效的。然而，该方法受限于对高质量、任务特定数据的依赖，限制了其适应现实世界中多样化的物体配置和场景的能力。目前主流方法（如基于扩散模型的监督学习）虽然在训练分布相似的条件下表现优异，但在分布外场景下非常脆弱，微小的物体配置或环境条件偏差都可能导致失败。解决策略脆弱性的一个自然方法是收集更多专家数据以扩大覆盖范围，但这通常成本高昂且难以扩展。</p>
<p>相比之下，非专家数据——例如随意玩耍数据、次优演示、部分任务完成数据或次优策略的 rollout——能提供更广泛的覆盖且收集成本更低。然而，传统的模仿学习方法无法有效利用这些数据。本文针对模仿学习过度依赖高质量专家数据、难以利用广泛但次优的数据来提升策略鲁棒性的具体痛点，提出了通过离线强化学习的视角来利用非专家数据的新思路。本文核心思路是：通过给专家数据赋予奖励+1、非专家数据赋予奖励0，将问题转化为离线RL问题，利用动态规划“缝合”非专家数据中的有用片段以恢复至专家状态，并通过引入策略平滑性来改善低数据覆盖下的“缝合”能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架基于离线强化学习，旨在利用专家数据集 $\mathcal{D}<em>E$ 和非专家数据集 $\mathcal{D}</em>{NE}$ 学习一个鲁棒的策略。输入是混合数据集，输出是能够从广泛初始状态成功执行任务并恢复的策略。</p>
<p><img src="https://arxiv.org/html/2510.19495v2/figures/imgs/rise_figure_downscaled.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RISE 方法示意图。它使非专家数据能够与专家数据“缝合”在一起，为机器人操作提供鲁棒、高覆盖度的行为。这允许利用所有收集的数据，使策略能够从新颖的分布外状态中恢复。</p>
</blockquote>
<p>核心方法建立在隐式扩散Q学习（IDQL）之上，但进行了关键修改。具体步骤如下：</p>
<ol>
<li><strong>奖励伪标注</strong>：将所有专家数据 $(s, a)$ 转换的奖励标记为 $r=+1$，所有非专家数据转换的奖励标记为 $r=0$。这避免了手动设计奖励函数的需要。</li>
<li><strong>价值函数学习</strong>：使用期望回归（expectile regression）目标学习Q函数 $Q_\phi(s,a)$ 和状态价值函数 $V_\psi(s)$。损失函数如论文公式(3)(4)所示，本质是在混合数据集上对Q函数进行时序差分学习，其中专家数据的目标值包含+1奖励。</li>
<li><strong>行为策略学习</strong>：通过最大似然估计在混合数据集 $\mathcal{D}<em>E \cup \mathcal{D}</em>{NE}$ 上学习一个行为策略 $\pi_B(a|s)$，该策略被参数化为条件扩散模型，代表了数据中动作的边际分布。</li>
<li><strong>策略提取</strong>：在测试时，从行为策略 $\pi_B(a|s)$ 中采样 $K$ 个动作候选，然后选择其中Q值最高的动作作为最终策略输出：$\pi^*(a|s) = \text{argmax}_{a \sim \pi_B(a|s)} Q_\phi(s, a)$。</li>
</ol>
<p>与现有方法（如SQIL）相比，本文的创新点在于解决了在现实世界典型稀疏数据覆盖下，标准离线RL方法难以有效“缝合”轨迹的问题。作者发现，尽管学习到的Q函数在训练数据邻域内能够准确插值并暗示有效的“缝合”路径，但行为策略 $\pi_B$ 学习到的动作分布往往过于保守和狭窄，导致基于采样的策略提取无法找到Q函数所建议的最优“缝合”动作。</p>
<p><img src="https://arxiv.org/html/2510.19495v2/figures/imgs/nospectral_overlay_2.png" alt="方法细节可视化"></p>
<blockquote>
<p><strong>图3</strong>：在平面推动任务上可视化谱归一化的效果。(a) 朴素使用IDQL导致边缘动作分布过窄，性能差。(b) 在行为策略损失中添加谱范数惩罚后，动作分布显著拓宽。(c) 投影到1D轴上的边缘动作分布与学习到的Q函数（红色实线）被绘制在一起。狭窄的动作分布（蓝色）常常无法包含最优动作。</p>
</blockquote>
<p>为解决此问题，RISE引入了两种关键技术来拓宽行为策略的动作分布，从而改善“缝合”能力：</p>
<ol>
<li><strong>强制策略Lipschitz连续性</strong>：通过对策略网络权重施加谱范数惩罚（公式(7)），隐式地强制策略在相邻状态间平滑变化，避免动作分布突变，使邻近状态的动作分布相似。</li>
<li><strong>基于距离的数据增强</strong>：对于数据集中的每个状态转换对 $(s, a)$ 和 $(s‘, a’)$，如果它们的状态在某个距离度量 $d$（如DINOv2特征空间中的欧氏距离）下足够接近（$d(s, s‘) &lt; T$），则将 $(s, a’)$ 添加到增强数据集 $\mathcal{D}_{aug}$ 中（公式(8)）。然后在 $\mathcal{D}<em>E \cup \mathcal{D}</em>{aug}$ 上训练行为策略，显式地拓宽其动作分布。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：模拟环境使用Robomimic基准（任务包括 lampshade, square-peg, piece-assembly, threading, cloth folding, mug-cleanup, one-leg）；真实机器人使用Furniture Bench基准（家具组装任务）。平台为Franka Panda机器人。</li>
<li><strong>评估场景</strong>：1) 从非结构化玩耍数据中学习恢复；2) 利用次优/失败数据提高成功率；3) 在策略自身评估rollouts上进行迭代策略改进。</li>
<li><strong>对比基线方法</strong>：行为克隆（BC，仅用专家数据）、统一行为克隆（BCU，用混合数据）、ILID（数据过滤方法）、SQIL（离线SAC with 0/1奖励）、CQL（保守Q学习）、IDQL（RISE的基础版本，无改进）。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.19495v2/figures/imgs/spectral_overlay_2.png" alt="任务展示"></p>
<blockquote>
<p><strong>图4</strong>：模拟和真实世界中的任务展示。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>在模拟的 square-peg（方钉插入）任务中，RISE的成功率达到 $50.7% \pm 5.8%$，显著高于仅使用专家数据的BC（$18.7% \pm 2.4%$）、统一行为克隆BCU（$0.0% \pm 0.0%$）、ILID（$35.3% \pm 3.5%$）以及其他离线RL基线（SQIL: $0.0%$, CQL: $12.4%$, IDQL: $19.6%$）。在 square-hook 和 piece-assembly 任务上也观察到类似的趋势，RISE均取得最佳性能。</p>
<p><img src="https://arxiv.org/html/2510.19495v2/x2.png" alt="主要结果表"></p>
<blockquote>
<p><strong>图6</strong>：在三种不同类型非专家数据（覆盖数据、次优数据、迭代策略改进数据）下的模拟任务成功率对比表。RISE在绝大多数任务和设置下均取得最高成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.19495v2/x3.png" alt="消融研究"></p>
<blockquote>
<p><strong>图8</strong>：消融实验展示了谱归一化（SN）和基于距离的数据增强（DA）对RISE性能的贡献。在 square-peg 任务上，完整RISE（SN+DA）性能最佳（$50.7%$），移除任一组件都会导致性能下降（仅SN: $36.4%$, 仅DA: $30.0%$, 两者都无: $19.6%$），验证了两种改进技术的有效性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>消融实验明确显示，强制Lipschitz连续性（谱归一化）和基于距离的数据增强这两个组件都对最终性能有显著贡献。在 square-peg 任务中，移除两者会使性能降至与基础IDQL相近的水平（约$19.6%$），而同时使用两者则能达到$50.7%$的成功率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个利用离线强化学习框架（RISE）来使用非专家数据增强模仿学习策略鲁棒性的方法，仅需简单的0/1奖励伪标注。</li>
<li>揭示了标准离线RL方法在低数据覆盖的现实场景中难以实现轨迹“缝合”的问题，并提出了通过强制策略平滑性和数据增强来拓宽行为策略分布的有效解决方案。</li>
<li>通过大量实验证明，RISE能够有效利用多种非专家数据（玩耍数据、失败演示、策略rollouts、多任务数据），在模拟和真实机器人操作任务上显著提升策略的恢复能力和泛化性能。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，该方法依赖于一个假设：学习到的Q函数在训练数据动作分布的某个邻域内是准确的。如果这个假设不成立，拓宽行为策略分布可能不会带来益处，甚至可能有害。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>探索更优的策略平滑性或分布拓宽技术，以进一步提高在极端稀疏数据下的“缝合”能力。</li>
<li>将方法扩展到更复杂的动态场景和长期任务中。</li>
<li>研究如何自动选择或加权不同类型和质量的非专家数据，以最大化学习效率。</li>
<li>本文表明，简单地混合所有数据（BCU）会导致策略性能崩溃，这凸显了算法设计（而不仅仅是数据量）对于利用异构数据的重要性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习过度依赖高质量专家数据、难以适应现实世界多样场景的问题，提出利用离线强化学习工具来有效利用非专家数据（如游戏数据、次优演示），以增强策略的鲁棒性。核心方法是对标准离线RL算法进行简单修改，以在稀疏数据覆盖的现实条件下利用此类数据，从而扩展策略分布的支持范围。实验表明，该方法能显著提升策略的恢复与泛化能力，在操作任务中大幅扩大策略成功的初始条件范围，并能有效利用所有收集到的部分或次优数据来提升任务性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.19495" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>