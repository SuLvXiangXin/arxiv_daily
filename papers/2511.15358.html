<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Platform-Agnostic Reinforcement Learning Framework for Safe Exploration of Cluttered Environments with Graph Attention - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Platform-Agnostic Reinforcement Learning Framework for Safe Exploration of Cluttered Environments with Graph Attention</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.15358" target="_blank" rel="noreferrer">2511.15358</a></span>
        <span>作者: George Nikolakopoulos Team</span>
        <span>日期: 2025-11-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在森林、矿井等障碍物丰富的环境中进行高效、安全的自主探索是机器人系统面临的关键挑战。传统探索策略（如随机探索、基于边界的方法）在现实环境中往往存在适应性差、路径规划次优、决策短视等局限性。另一方面，基于强化学习的策略通过优化长期奖励和动态响应环境变化，能够实现自适应决策。其中，基于图神经网络的架构因其能够利用图结构表示环境中的空间关系而受到关注。然而，学习到的策略对决策过程洞察有限，且除了在测试环境中的统计评估外，无法提供明确的行为正确性保证，这在安全敏感的应用中尤为关键。因此，当前研究的一个重点是将安全机制集成到基于RL的框架中，在保持适应性的同时确保遵守安全约束。本文针对学习策略缺乏显式安全保证这一痛点，提出了一种新颖的平台无关分层框架，将基于GNN的探索策略与安全过滤器相结合。核心思路是：利用强化学习训练GNN策略选择下一个航点以最大化探索效率，同时引入一个安全过滤器来验证和修正策略动作，确保无碰撞导航。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架是一个分层架构，整合了基于GNN的策略和安全过滤器，为探索生成下一步航点。</p>
<p><img src="https://arxiv.org/html/2511.15358v1/concept.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提出的用于杂乱环境探索的安全强化学习框架概述。分层架构集成了基于GNN的策略和安全过滤器，以生成探索的下一步航点。产生的高层运动既可以应用于Gymnasium模拟环境进行策略训练和消融研究（左分支），也可以应用于配备Unitree Go1四足机器人的实验室环境进行物理实验（右分支）。</p>
</blockquote>
<p>整体流程如下：在每个时间步t，探索智能体占据栅格地图中的一个单元，并拥有已重建环境的部分地图。从该地图中，为策略构建一个基于图的观测o_t，编码智能体位置、导航目标和探索边界（已探索与未探索区域的交界）。基于GNN的策略π_θ接收o_t，从8个相邻单元格中选择一个移动动作a_t^π。随后，安全过滤器σ评估a_t^π的可行性。如果动作可行，则直接执行；否则，过滤器根据公式（1）将其替换为可行动作集A_t^f中与原始策略动作余弦相似度最高的可行动作a_t^σ，以此最小化角度偏差。此过程重复进行，直至探索比例达到阈值或交互步数达到上限。</p>
<p>框架的核心模块包括环境表征（观测空间）、GNN策略与评论家网络、安全过滤器以及奖励函数。</p>
<ol>
<li><strong>观测空间</strong>：观测o_t是一个探索图，包含节点集V_t和边集E_t。节点包括下一航点V_t^n（智能体相邻8个格子）和探索边界V_t^f（与未知单元格相邻的自由单元格）。边编码空间关系：下一航点节点间双向连接，每个边界节点单向连接到其最近的下一航点节点。每个节点关联一个8维特征向量，前4维使用独热编码表示节点类别（智能体位置、可行/不可行下一航点、边界），后4维编码节点周围(2k+1)×(2k+1)邻域内自由、未知、占据、边界单元格的归一化数量。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.15358v1/graph.png" alt="探索图与GNN策略"></p>
<blockquote>
<p><strong>图2</strong>：用作观测o_t的探索图以及基于GNN的策略示意图。图像显示了智能体地图的一部分，其中占据、未知和自由单元格分别用黑色、灰色和白色表示。智能体位置、可行和不可行的下一步导航目标以及边界分别用蓝色、绿色、红色和橙色表示。图结构编码节点关系，边粗细与连接节点之间的距离成正比。右侧突出显示了一个边界及其提取的特征向量，以及探索策略的内部架构，其中强调了主要层。</p>
</blockquote>
<ol start="2">
<li><strong>GNN策略与评论家网络</strong>：策略网络π_θ和评论家网络V_ϕ均为基于注意力消息传递的图神经网络，采用两阶段架构。第一阶段结构相同（但参数独立），由两个堆叠的GATv2Conv层组成（超参数见表I），利用注意力机制编码图中的局部关系依赖。第二阶段不同：对于π_θ，将节点嵌入映射到动作概率（对V_n中的节点）；对于V_ϕ，将图级表示映射到标量值估计。</li>
<li><strong>安全过滤器</strong>：作为显式安全机制，它根据智能体的重建地图验证策略动作。其可行动作集A_t^f包含所有能使智能体保持在地图边界内、避免与非可通行单元格碰撞的动作，并对角线移动要求至少一个相邻的正交移动是可通行的，以防止切角碰撞。</li>
<li><strong>奖励函数</strong>：本文提出了一种基于势场塑造的奖励函数（SGA），旨在鼓励高效探索同时惩罚安全过滤器干预。其逐步奖励r_t^step根据公式（11）计算：若发现新单元格(n_d&gt;0)，则奖励n_d；若未发现新单元格但前后时刻均存在边界，则奖励势场值Φ的减少量（Φ_t-1 - Φ_t），其中势场Φ_t定义为所有边界中“未知单元格数量/到智能体距离”的最大值（公式12）；其他情况给予固定小负奖励r_0。此外，当策略动作被安全过滤器覆盖时，给予惩罚r_unsafe<em>；当探索比例达到阈值ρ</em>时，给予大奖励r_exp*。</li>
</ol>
<p>本文的创新点具体体现在：1) 提出了一个平台无关的分层框架，将GNN驱动的RL探索策略与可覆盖不安全动作的安全过滤器相结合；2) 设计了一种新颖的注意力增强图神经网络，从自定义的环境图表示中提取探索相关特征；3) 提出了一种结合边界信息增益和距离的势场奖励函数。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟（Gymnasium）和实验室环境（使用Unitree Go1四足机器人）中进行。模拟环境参数如表II所示，地图大小为50×50格，包含45个随机分布的圆形障碍物。训练使用PPO算法，在HPC2N集群上进行。</p>
<p>对比的基线方法是针对所提奖励函数SGA的消融变体。通过消融研究，评估了八种奖励函数：Safety-Gated Exploration (SGE)、Safety-Gated Discovery (SGD)、Safety-Gated Pure Exploration (SGPE)、Safety-Gated Adaptive (SGA) 以及各自移除不安全惩罚的对应版本（FE, FD, FPE, FA）。</p>
<p><img src="https://arxiv.org/html/2511.15358v1/mean_total_reward.png" alt="训练性能曲线"></p>
<blockquote>
<p><strong>图3</strong>：训练性能曲线，显示平均总奖励与训练步数的关系，奖励归一化至[0,1]并使用1000步简单移动平均进行平滑。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>训练收敛</strong>：如图3所示，使用SGA奖励的策略训练曲线总体呈上升趋势，在大约2×10^4步后趋于稳定，并在约1×10^5步后收敛。</li>
<li><strong>探索效率与安全性</strong>：在100个随机生成的测试环境中进行模拟评估（图4）。图4(a)显示，采用SGA奖励的策略覆盖地图的速度最快、最一致，其中位性能在大约605步内达到95%的地图探索。图4(c)显示，SGA策略触发安全过滤器干预的动作比例中位数约为6.5%，低于大多数其他策略变体，表明其能更有效地学习安全行为。</li>
<li><strong>消融实验贡献</strong>：对比八种奖励函数（图4）表明，完整的SGA配方（包含不安全惩罚、发现奖励和势场项）在探索效率和减少安全干预方面表现最佳。移除不安全惩罚（FA）会导致安全干预显著增加（中位数约17.5%）。仅使用稀疏奖励（SGE）或简单发现奖励（SGD）的策略探索效率较低。</li>
<li><strong>鲁棒性</strong>：如图5所示，在不同环境大小和障碍物数量的测试中，SGA策略保持了较高的探索效率和较低的安全干预比例，证明了其鲁棒性。</li>
<li><strong>实物实验</strong>：在实验室定制场地中使用Unitree Go1机器人进行测试（图6）。在4m×5m和6m×5m的场地中，SGA策略均能实现有效的探索覆盖，验证了框架在真实物理平台上的可行性。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.15358v1/sim.png" alt="模拟评估结果"></p>
<blockquote>
<p><strong>图4</strong>：在100个随机生成的Gymnasium环境中训练策略模拟收集的数据。从左到右：(a) 每个时间步的中位地图覆盖率（彩色线），变异范围显示为±1标准差（阴影区域）。(b) 1000步后智能体地图覆盖率的分布。(c) 每个策略在100次测试模拟中，安全过滤器干预动作比例的分布。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.15358v1/rob.png" alt="鲁棒性测试结果"></p>
<blockquote>
<p><strong>图5</strong>：从左到右：在不同环境大小(a)和树木数量(b)的100个随机生成探索环境中，所提SGA策略的中位地图覆盖率轨迹。在不同大小(c)和树木数量(d)的环境中，SGA策略在100次测试模拟中安全过滤器干预动作比例的分布。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.15358v1/experiment.png" alt="实物实验结果"></p>
<blockquote>
<p><strong>图6</strong>：从左到右：(a) 实验室环境中构建的定制探索场地（红色标出），其中障碍物由方块定义，Unitree Go1四足机器人作为探索智能体。Unitree Go1在10个随机放置障碍物的4m×5m场地(b)和10个6m×5m场地(c)中获得的中位地图覆盖率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：1) 提出了一种平台无关的安全分层探索框架，将基于GNN的RL策略与可覆盖不安全动作的安全过滤器相结合，兼顾了学习策略的适应性和显式安全机制的可靠性；2) 设计了一种新颖的注意力增强图神经网络，用于从编码了航点、边界和遮挡区域的自定义环境图表示中提取特征；3) 提出了一种由势场塑造的奖励函数，该函数同时考虑了智能体到未探索区域的接近程度和到达这些区域的预期信息增益，并通过消融研究验证了其有效性；4) 在模拟和实验室实物机器人平台上验证了框架的有效性。</p>
<p>论文自身提到的局限性包括：框架目前处理的是静态环境；安全过滤器依赖于局部地图的准确性；实验环境规模相对有限。</p>
<p>本文对后续研究的启示在于：证明了将学习型策略与形式化安全机制（如安全过滤器）相结合是部署复杂RL策略到真实机器人的可行路径；基于图的结构化环境表征与注意力机制的结合，能有效提升策略对空间关系的理解；精心设计的奖励函数（如结合信息增益的势场）对引导智能体进行高效、安全的探索至关重要。未来工作可扩展至动态环境、更复杂的地图形以及多智能体协同探索场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自主机器人在障碍物密集环境（如森林、矿井）中实现高效且安全探索的核心挑战，提出了一种平台无关的强化学习框架。其关键技术是结合图神经网络（GNN）策略与安全过滤器：使用PPO算法训练GNN策略进行路径点选择，同时设计一个安全过滤器来修正不可行动作；奖励函数则融合了势场，以权衡接近未探索区域和预期信息增益。通过在仿真和真实实验室环境中的广泛评估，该方法被证明能够实现杂乱空间中的高效与安全探索。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.15358" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>