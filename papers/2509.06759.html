<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.06759" target="_blank" rel="noreferrer">2509.06759</a></span>
        <span>作者: Nguyen, Thanh Thi, Wilson, Campbell, Dalins, Janis</span>
        <span>日期: 2025/09/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>大型视觉语言模型（LVLMs）代表了人工智能的重要进展，能够理解和生成跨越视觉和文本模态的内容。尽管大规模预训练推动了显著进步，但如何对这些模型进行微调以使其与人类价值观对齐或适应特定任务行为，仍然是一个关键挑战。目前，指令学习和偏好学习是两种互补的微调方法。指令学习通过指令-响应对进行监督微调，教导模型遵循明确的自然语言命令。而偏好学习则基于比较性判断（如排序或成对偏好）来细化模型行为，尤其在存在多个有效响应的情况下，使模型输出与人类偏好和价值观对齐。</p>
<p>本文聚焦于使用深度强化学习（DRL）和直接偏好优化（DPO）这两种偏好学习方法来微调和对齐LVLMs。从纯文本LLMs过渡到LVLMs引入了显著的复杂性，因为输入本质上是多模态的。LVLMs必须处理和整合来自图像和文本的多种模态信息，这不仅需要理解语言，还需要解释视觉内容并将其与语言对齐，这带来了更复杂的输入空间和对更深层次多模态推理的需求。因此，开发LVLMs比其纯文本LLM对应物更加资源密集且技术要求更高。</p>
<p>本文的核心思路是：系统性地综述和剖析如何利用DRL和DPO框架来对齐LVLMs，通过分析各自的算法原理、奖励或偏好数据来源、应用研究以及优缺点，为构建稳健且与人类对齐的LVLMs提供清晰的路线图和技术洞见。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文是一篇综述性论文，并未提出一个统一的新方法框架，而是对现有基于DRL和DPO的LVLM对齐方法进行了系统性的分类和阐述。其整体组织框架如图1所示，将LVLM的偏好学习微调方法主要分为DRL和DPO两大类进行探讨。</p>
<p><img src="https://arxiv.org/html/2509.06759v1/x1.png" alt="方法分类框架"></p>
<blockquote>
<p><strong>图1</strong>：本综述论文的结构图，展示了用于微调大型视觉语言模型（LVLMs）的偏好学习方法分类，主要包括深度强化学习（DRL）和直接偏好优化（DPO）两大范式。</p>
</blockquote>
<p>DRL和DPO在微调流程上存在根本区别，图2清晰地展示了这一差异。</p>
<p><img src="https://arxiv.org/html/2509.06759v1/x2.png" alt="DRL与DPO流程对比"></p>
<blockquote>
<p><strong>图2</strong>：微调LVLMs的DRL（左）与DPO（右）方法对比（改编自[4]）。与LLM微调不同，图像可作为LVLM微调过程输入的一部分。DRL使用偏好数据训练一个独立的奖励模型，进而通过该模型的奖励信号指导策略优化；而DPO则直接利用偏好数据训练，使模型自身充当隐式的奖励模型。</p>
</blockquote>
<p><strong>DRL方法详解</strong>：<br>DRL方法将LVLM微调问题形式化为马尔可夫决策过程（MDP）。其中，状态 <code>s_t</code> 包含图像、提示词以及过去已生成的令牌序列。动作 <code>a_t</code> 是从词汇表中选择下一个令牌 <code>token_t</code>。执行动作后，状态转移到 <code>s_{t+1}</code>。奖励函数 <code>R</code> 的设计是关键挑战，可以是基于人类或AI反馈学习的奖励模型，或是基于启发式规则设计的规则奖励。</p>
<p>近端策略优化（PPO）及其变体（如群组相对策略优化GRPO）是常用的策略优化算法。以GRPO为例，其流程是：给定待优化的策略模型 <code>π_θ</code> 和参考模型 <code>π_ref</code>，旧策略模型 <code>π_θ_old</code> 首先生成一组对查询 <code>q</code> 的补全 <code>{o_1, o_2, ..., o_N}</code>。然后计算这组补全的奖励 <code>{r_1, r_2, ..., r_N}</code>，并利用公式 <code>A_i = (r_i - mean({r_j})) / std({r_j})</code> 计算每个补全在组内的优势值 <code>A_i</code>。最后，策略模型通过最小化GRPO目标函数 <code>L(θ)</code> 进行优化，该函数鼓励模型偏好组内具有更高优势的补全，同时通过KL散度项最小化与初始参考模型的偏差。</p>
<p><strong>DPO方法详解</strong>：<br>DPO方法无需构建显式的奖励模型，它直接利用收集到的偏好数据（包含查询 <code>x</code>、图像 <code>I</code>、优选响应 <code>y_w</code> 和劣选响应 <code>y_l</code>）进行训练。其核心是一个分类损失函数，旨在直接优化策略 <code>π_θ</code>，使其更倾向于产生与人类判断一致的响应。DPO目标函数定义为：<code>L(θ) = -E_{(x,y_w,y_l)} [ log σ( β log (π_θ(y_w|x,I)/π_ref(y_w|x,I)) - β log (π_θ(y_l|x,I)/π_ref(y_l|x,I)) ) ]</code>，其中 <code>β</code> 是缩放因子，<code>σ</code> 是sigmoid函数。该函数最大化优选响应与劣选响应对数概率的差值，从而将模型的输出分布与观察到的人类偏好对齐。</p>
<p>与现有方法相比，本文的创新性体现在系统性的视角梳理和对比分析上，而非提出单一新算法。它明确指出了DRL和DPO在流程、复杂度和数据依赖上的根本差异，并深入探讨了各自内部的技术变体（如不同的奖励来源、DPO目标函数的改进）及其适用场景。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>作为一篇综述论文，本文并未进行传统的定量实验对比，而是通过总结和分析大量已有研究，来论证DRL和DPO方法的有效性、特点及应用场景。论文的核心“实验结果”体现为对现有研究的归纳比较。</p>
<p><strong>方法对比总结</strong>：<br>论文通过表I系统比较了DRL和DPO在微调LVLMs时的各项特征。</p>
<blockquote>
<p><strong>表I</strong>：DRL与DPO在微调LVLMs时的特征对比。DRL依赖度低（可不需偏好数据）、复杂度高、训练可能不稳定、资源需求高、灵活性高（支持复杂多信号奖励），适用于具有自定义目标的复杂对齐任务；DPO依赖度高（需要显式偏好数据）、复杂度低、训练稳定、资源需求低、灵活性中等（限于二元偏好对），适用于需要效率的大规模偏好调优。</p>
</blockquote>
<p><strong>DRL应用研究总结</strong>：<br>表II总结了若干采用DRL微调LVLMs的研究，涵盖了使用的DRL算法、基础模型、奖励来源和任务。</p>
<blockquote>
<p><strong>表II</strong>：基于DRL的LVLM微调方法总结。研究采用了标准PPO或其变体（如PPO-clip、GRPO），奖励来源包括基于人类反馈训练的奖励模型、基于CLIP编码器的AI反馈以及针对特定任务（如目标定位、Blackjack游戏）设计的规则奖励。任务涵盖减少幻觉、图像描述、推理、问答和控制任务。</p>
</blockquote>
<p><strong>DPO应用研究总结</strong>：<br>表III总结了基于DPO及其变体微调LVLMs的研究，包括基础模型、DPO变体、偏好数据来源和任务。</p>
<blockquote>
<p><strong>表III</strong>：基于DPO变体的LVLM微调方法总结。研究采用了标准DPO或改进版本（如动态奖励缩放DPO、迭代DPO、POVID、rDPO）。偏好数据排名可由人类或机器（如GPT-4V、Qwen2、模型自身）提供。任务主要集中在减少幻觉、增强推理、视觉问答（VQA）和对话能力。</p>
</blockquote>
<p><strong>可用数据集总结</strong>：<br>为了支持基于偏好的微调，论文在表IV中汇总了当前可用的偏好或排名数据集，指明了数据来源（人类/机器）和标注细节。</p>
<blockquote>
<p><strong>表IV</strong>：可用的偏好或排名数据集。例如，Preference-10K包含1万个人类标注的偏好对，用于减少幻觉；MM-RLHF包含12万个由人类专家评分和排名的比较对，涵盖图像、视频理解和安全领域；SPA-VL和VLFeedback则利用GPT-4V作为机器标注者，生成了大量用于增强安全性、视觉忠实度和帮助性的偏好数据。</p>
</blockquote>
<p>通过这些总结，论文表明：DRL和DPO均能有效用于对齐LVLMs，并在减少幻觉、提升推理能力、增强安全性等任务上取得了进展。DRL在需要复杂、定制化奖励的场景中更具灵活性，而DPO则以更高的训练稳定性和效率见长，尤其适合拥有大规模偏好数据的场景。同时，高质量偏好数据的获取（无论是通过昂贵的人类标注还是依赖于现有模型的机器标注）是决定对齐效果的关键因素。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>系统性梳理</strong>：首次从偏好学习的角度，系统性地梳理和分类了用于大型视觉语言模型（LVLM）对齐的两种核心范式——深度强化学习（DRL）和直接偏好优化（DPO），为研究者提供了清晰的技术图谱。</li>
<li><strong>深度技术剖析</strong>：不仅阐述了DRL和DPO的基本原理和形式化定义，还深入分析了其内部的关键技术细节，如DRL中不同的奖励设计（人类反馈、AI反馈、规则奖励）以及DPO中多样的偏好数据来源和算法变体，并明确对比了二者的优缺点及适用场景。</li>
<li><strong>资源与挑战总结</strong>：汇总了当前可用的公开偏好数据集，为资源有限的研究者提供了宝贵的实验基础；同时，明确指出了该领域在可扩展性、样本效率、泛化与鲁棒性、奖励建模以及安全伦理等方面面临的开放性挑战。</li>
</ol>
<p>论文自身作为一篇综述，其局限性在于未提出全新的算法或给出突破性的定量实验结论，其价值主要体现在整合与分析现有知识。</p>
<p>本文对后续研究的重要启示在于指明了多个关键的未来方向：</p>
<ul>
<li><strong>可扩展的人类反馈集成</strong>：需要发展如主动学习、半监督学习等技术，以更低成本、更高效率地利用人类反馈。</li>
<li><strong>样本高效的DRL算法</strong>：针对大模型训练成本高的问题，需探索离策略学习、基于模型的DRL等样本效率更高的算法。</li>
<li><strong>泛化与鲁棒性</strong>：应通过领域随机化、对抗性训练等策略，确保对齐后的模型在未知环境和任务中也能表现稳健。</li>
<li><strong>更好的奖励建模</strong>：未来需专注于训练能够捕捉多模态行为细微差别的、稳健且可解释的奖励模型，偏好基奖励学习是一个有前景的方向。</li>
<li><strong>伦理与安全</strong>：随着LVLMs更深入地融入社会，必须构建反映伦理规范的奖励模型，并探索人在环路的DRL等机制，以确保模型决策的透明、公平与安全。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文探讨如何将大型视觉语言模型与人类偏好和价值观对齐这一核心挑战。论文重点分析了两种关键技术方法：深度强化学习通过奖励信号优化模型行为，而直接偏好优化则直接使策略与偏好对齐，无需显式奖励模型。这些方法旨在提升模型的任务性能，并实现自适应的多模态交互。文章综述了相关范式，并对偏好数据来源、奖励信号以及可扩展性、样本效率等开放性问题进行了讨论。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.06759" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>