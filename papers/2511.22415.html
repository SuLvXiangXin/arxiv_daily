<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Exposing Vulnerabilities in RL: A Novel Stealthy Backdoor Attack through Reward Poisoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Exposing Vulnerabilities in RL: A Novel Stealthy Backdoor Attack through Reward Poisoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.22415" target="_blank" rel="noreferrer">2511.22415</a></span>
        <span>作者: Junfeng Wu Team</span>
        <span>日期: 2025-11-27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>强化学习（RL）系统在诸多领域取得了显著成功，但其对奖励信号的依赖也带来了严重的安全漏洞。目前，针对RL的后门攻击研究有限，现有方法通常通过操纵状态、动作或奖励来实施攻击。这些方法会引入环境动态的不一致性，使其更易被检测，且大多是启发式的，缺乏对RL后门攻击问题的正式理论定义。</p>
<p>本文针对现有后门攻击隐蔽性不足、通常需要白盒知识（如知晓智能体学习算法或环境动态）的痛点，提出了一种新颖的视角：通过精心毒化奖励信号来植入后门，并<strong>最小化对原始数据的扭曲</strong>以提升隐蔽性。本文的核心思路是：在训练阶段，攻击者通过求解一个基于惩罚的双层次优化问题，以最小的奖励扰动引导智能体学习一个预设的目标后门策略；在部署阶段，当特定触发器出现时，后门被激活，导致智能体性能崩溃，而在非触发状态下其行为与正常智能体无异。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体攻击框架分为训练和部署两个阶段。</p>
<p><img src="https://arxiv.org/html/2511.22415v1/x1.png" alt="攻击方案"></p>
<blockquote>
<p><strong>图1</strong>：提出的攻击方案。<strong>训练阶段</strong>：攻击者拦截智能体的环境交互数据，使用这些数据更新其攻击策略模型（奖励扰动网络 Δ 和 Q值网络 Q̄）。在将奖励扰动 Δ 添加到真实奖励数据后，毒化数据被传输到智能体的经验回放缓冲区，引导其学习目标后门策略。<strong>部署阶段</strong>：当攻击者插入特定触发器时，植入的后门被激活。攻击的隐蔽性在于智能体在正常情况下的名义行为，仅在触发器激活时性能才会灾难性下降。</p>
</blockquote>
<p><strong>攻击模型</strong>：攻击者旨在通过操纵存储在经验回放缓冲区中的奖励来影响RL智能体的训练过程。攻击者知识受限，对智能体的学习算法或底层环境动态（如奖励或转移概率）一无所知，仅能根据回放缓冲区中可用的数据自适应地调整毒化策略。在每个训练轮次，攻击者将原始奖励 <code>r</code> 替换为修改后的奖励 <code>r+Δ</code>。训练完成后，攻击者可以通过在智能体观测 <code>s</code> 上添加一个小扰动 <code>δ</code>（触发状态 <code>s̃ := s+δ</code>）来激活后门。</p>
<p><strong>目标后门策略设计</strong>：攻击者首先通过标准RL训练获得一个正常策略 <code>π_n</code>。目标后门策略 <code>π†</code> 设计如下：对于所有正常状态 <code>s</code>，<code>π†(a|s) = π_n(a|s)</code>；对于所有触发状态 <code>s̃</code>，<code>π†(a|s̃) = 1(a=a_bad)</code>，其中 <code>a_bad</code> 是特定于任务的、能严重降低智能体性能的“坏动作”。</p>
<p><img src="https://arxiv.org/html/2511.22415v1/imgs/hopper.pic.jpg" alt="触发位置"></p>
<blockquote>
<p>**图2 (a)**：Hopper环境中触发器的插入位置（圆圈区域）。触发器通过修改智能体观测中对应点的角度信息引入。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.22415v1/imgs/walker2d.pic.jpg" alt="触发位置"></p>
<blockquote>
<p>**图2 (b)**：Walker2D环境中触发器的插入位置（圆圈区域）。</p>
</blockquote>
<p><strong>优化公式</strong>：攻击者需要在毒化奖励的同时最小化引入的扰动 <code>Δ</code>。这引出了一个约束优化框架。核心是确保诱导的Q值函数满足贝尔曼方程，并且对于任何 <code>s</code> 和 <code>a ≠ π†_s</code>，有 <code>Q(s, π†_s) ≥ Q(s, a) + ϵ</code>，其中 <code>ϵ</code> 是量化 <code>π†_s</code> 优势的毒化强度参数。</p>
<p>为了高效求解并避免“双重采样”问题，论文采用了<strong>双层次重构</strong>。上层问题更新 <code>Q̄</code> 变量以最小化目标函数和惩罚函数，下层问题更新 <code>Δ</code> 以实现等式约束的可行性。下层问题有一个直接解：<code>Δ^θ,*_s,a = Q̄^λ_s,a - r(s,a) - γ Σ_s&#39; P(s&#39;|s,a) Q̄^λ_s&#39;,π†_s&#39;</code>。利用隐函数定理可以处理 <code>Δ^θ_s,a</code> 对 <code>λ</code> 的嵌套依赖，从而计算两层的精确梯度。</p>
<p><strong>更新规则与算法</strong>：攻击者使用单循环算法求解双层次优化问题。由于无法访问转移概率，攻击者使用采样转移计算随机梯度。关键更新目标如下：</p>
<ol>
<li><code>Δ_target</code>：根据下层问题的最优条件计算，用于更新奖励扰动网络参数 <code>θ</code>。</li>
<li><code>Q̄_target</code>：通过从当前值中减去随机梯度得到，用于更新攻击者的Q值网络参数 <code>λ</code>。</li>
<li><code>Q_target_s&#39;,π†_s&#39;</code>：与 <code>Δ</code> 相关联，确保时序一致性。</li>
</ol>
<p>惩罚系数 <code>ρ_k</code> 动态增加以满足不等式约束。整个算法（算法1）在线迭代地调整神经网络参数 <code>θ</code> 和 <code>λ</code>，以学习 <code>Q̄_target</code> 和 <code>Δ_target</code>，同时智能体使用毒化的奖励更新其策略。</p>
<p><strong>创新点</strong>：与需要白盒知识或启发式操纵的现有方法相比，本方法的核心创新在于：1) <strong>黑盒攻击</strong>：无需知晓智能体算法或环境模型；2) <strong>隐蔽性优先</strong>：通过形式化的双层次优化框架，明确以最小化奖励扰动为目标，降低了统计检测的可能性；3) <strong>理论驱动</strong>：提供了将后门攻击目标（诱导特定策略）与隐蔽性约束（最小化扰动）统一起来的优化问题表述及求解方法。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：经典控制任务 CartPole 以及 MuJoCo 机器人控制任务 Hopper 和 Walker2D。</li>
<li><strong>实验平台</strong>：使用 OpenAI Gym 和 MuJoCo 模拟器。</li>
<li><strong>智能体算法</strong>：CartPole 使用深度Q学习，Hopper 和 Walker2D 使用近端策略优化（PPO）。</li>
<li><strong>对比基线</strong>：在 CartPole 环境中对比了邻域攻击者、最小最大攻击者和随机攻击者。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>后门有效性</strong>：如表1所示，在触发场景下，中毒智能体性能显著下降。例如，在 Hopper 任务中（<code>ϵ=0.25</code>），性能下降达85.01%；在 Walker2D 任务中（<code>ϵ=0.25</code>），性能下降达71.27%；在 CartPole 任务中（<code>ϵ=4.0</code>），性能下降70.69%。</li>
<li><strong>后门隐蔽性</strong>：在正常（非触发）场景下，中毒智能体性能与正常智能体高度接近。性能下降在 CartPole、Hopper 和 Walker2D 任务中分别仅为0.62%、6.72%和4.59%（最佳参数设置下）。在所有参数设置中，性能下降最高也未超过2.76%、23.42%和23.91%。</li>
<li><strong>毒化强度参数 <code>ϵ</code> 的影响</strong>：<code>ϵ</code> 较小（如0.01）时，后门有效性有限；<code>ϵ</code> 较大（如4.0）时，后门有效性更高，但会牺牲部分隐蔽性，因为更大的参数引入了更多的数据操纵，可能导致训练不稳定。</li>
</ol>
<p><strong>攻击强度对比</strong>：<br>表2对比了在 CartPole 环境中的不同攻击方法。所有方法都能成功植入后门（触发时性能差，非触发时性能正常）。关键指标是<strong>毒化强度</strong>（被修改奖励与原始奖励的L2偏差之和）。论文提出的方法在全局状态-动作对和触发状态-动作对上的毒化强度均<strong>低于所有基线方法</strong>。这表明该方法能以最小的奖励函数修改诱导出目标恶意行为，具有更优的隐蔽性和效率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的、以最小化数据扭曲为核心的奖励毒化后门攻击算法，显著提升了攻击的隐蔽性。</li>
<li>建立了一个形式化的双层次优化框架，将后门策略植入问题转化为一个可求解的约束优化问题，为RL后门攻击提供了理论分析基础。</li>
<li>在多种模拟环境中验证了该方法的有效性和隐蔽性，并证明了其在黑盒设定下的可行性，揭示了部署RL系统面临的关键安全威胁。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：攻击依赖于对奖励信号的访问；实验环境相对标准，在更复杂、高维或部分可观测环境中的表现有待进一步验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>防御机制</strong>：这项工作凸显了开发针对训练时数据毒化（尤其是奖励毒化）的鲁棒防御算法的紧迫性。需要研究能够检测微小奖励扰动或减轻其影响的算法。</li>
<li><strong>攻击拓展</strong>：该方法为RL后门攻击提供了一个可扩展的框架，未来可以探索更复杂的目标策略（而非简单的“坏动作”），或在多智能体、离线RL等更复杂场景中的应用。</li>
<li><strong>安全评估</strong>：为RL系统的安全性评估提供了一个新的基准，即不仅要测试其正常性能，还需评估其在面对隐蔽奖励操纵时的脆弱性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文揭示了强化学习（RL）因依赖奖励信号而面临的安全漏洞，提出了一种通过在训练阶段投毒奖励信号来植入隐蔽后门的攻击方法。其核心技术是一种基于奖励扰动网络和Q值网络的算法，能在保证攻击有效性的同时，最小化对正常奖励数据的扰动，从而确保隐蔽性。在Hopper和Walker2D环境中的实验表明，该攻击具有极强的隐蔽性（正常场景性能仅下降2.18%和4.59%）与高破坏性（触发后性能最大下降82.31%和71.27%），对RL系统的安全构成了严重威胁。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.22415" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>