<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.05585" target="_blank" rel="noreferrer">2504.05585</a></span>
        <span>作者: Li, Yuxuan, Gao, Yicheng, Yang, Ning, Xia, Stephen</span>
        <span>日期: 2025/04/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在强化学习（RL）中，情节式任务常因稀疏奖励信号和高维状态空间而难以高效学习。此外，这些任务中常存在隐藏的“陷阱状态”——即导致任务永久失败但又不提供明确负奖励的状态，使得智能体容易重复犯错。为设计密集奖励函数，逆强化学习（IRL）通过从专家示范中推断奖励函数成为一种主流方法，如最大熵IRL（MaxEnt）、生成对抗模仿学习（GAIL）和对抗性逆强化学习（AIRL）。然而，现有IRL方法存在两个关键局限性：一是大多仅依赖于成功的专家示范进行模仿，这限制了智能体的探索能力，可能导致其收敛到次优行为并无法避开陷阱状态；二是忽略了失败的示范，降低了数据效率，尤其在复杂环境中早期训练失败普遍时，错失了帮助智能体学习避开不良轨迹的宝贵信息。</p>
<p>本文针对上述痛点，提出了一个新颖的视角：同时利用成功和失败的示范，并引入时间信息来学习一个更准确、与环境对齐的密集奖励函数。本文的核心思路是：通过时间加权函数捕获轨迹中的时序动态，并利用对比奖励学习框架，从正负样本（成功与失败轨迹）中学习一个能识别关键状态（目标状态与陷阱状态）的奖励函数，从而引导智能体高效探索并避免陷阱。</p>
<h2 id="方法详解">方法详解</h2>
<p>TW-CRL的整体框架包含两个核心组件：时间加权函数（Time-Weighted function）和对比奖励学习（Contrastive Reward Learning）。其流程是：首先，收集包含成功与失败轨迹的示范数据集；其次，利用时间加权函数为每条轨迹中每个时间步的状态计算一个权重；然后，根据轨迹的成功/失败属性，为每个状态分配一个基于该权重的估计奖励标签；最后，使用对比奖励学习损失函数，以监督学习的方式训练一个奖励函数网络，使其预测值逼近这些标签。</p>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/method3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：TW-CRL方法整体框架。左侧输入为成功（绿色）与失败（红色）的示范轨迹。通过时间加权模块为轨迹中不同时刻的状态赋予权重（颜色深浅代表权重高低）。对比奖励学习模块则根据轨迹类型（成功/失败）和对应权重，为每个状态生成奖励标签（正/负），并以此监督训练奖励函数网络 $r_\phi$。</p>
</blockquote>
<p><strong>时间加权函数</strong>：该模块旨在为轨迹中不同时刻的状态赋予不同的重要性权重。其核心思想是，在成功轨迹中，越靠近末尾的状态越可能是目标状态；在失败轨迹中，越靠近末尾的状态越可能是陷阱状态。作者将智能体的动态建模为一个吸收马尔可夫链。对于失败轨迹，陷阱状态是吸收态。通过推导（假设失败轨迹的终点是陷阱状态，起点不是），得到在给定轨迹失败的条件下，时刻 $t$ 的状态是陷阱状态的条件概率。经过简化与参数化（设转移概率 $f(t, T) \propto e^{\alpha t}$，并归一化），得到了统一用于成功和失败轨迹的时间加权函数 $w(t)$：<br>$$<br>w(t) = 1 - \left(1 - \frac{e^{\alpha t} - 1}{e^{\alpha T} - 1}\right)^t<br>$$<br>其中 $\alpha &gt; 0$ 是超参数，$T$ 是轨迹总步长。$w(t)$ 随 $t$ 单调递增，意味着给轨迹后期的状态赋予更高权重。</p>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/wt.png" alt="时间加权函数曲线"></p>
<blockquote>
<p><strong>图23</strong>：时间加权函数 $w(t)$ 在不同超参数 $\alpha$ 下的曲线。横轴为归一化的时间步（t/T），纵轴为权重值。可见 $\alpha$ 控制着权重曲线增长的“锐利”程度，$\alpha$ 越大，后期状态的权重越高。</p>
</blockquote>
<p><strong>对比奖励学习</strong>：该模块用于训练奖励函数网络 $r_\phi$。首先，利用时间加权函数为数据集中的每个状态 $s_t$ 生成估计奖励标签 $r_{\text{est}}(s_t)$：<br>$$<br>r_{\text{est}}(s_t) = \begin{cases}<br>w(t), &amp; \text{if } s_t \in \tau^+ \<br>-w(t), &amp; \text{if } s_t \in \tau^-<br>\end{cases}<br>$$<br>即成功轨迹中的状态获得正权重作为标签，失败轨迹中的状态获得负权重作为标签。这实质上构建了一个对比学习的目标：鼓励奖励函数将成功轨迹中的状态（尤其是后期状态）与高奖励值关联，将失败轨迹中的状态（尤其是后期状态）与低奖励值关联。然后，使用简单的均方误差损失函数进行监督训练：<br>$$<br>\mathcal{L}<em>{\text{CRL}} = \frac{1}{N} \sum</em>{\tau \in \mathcal{D}} \sum_{s_t \in \tau} \left( r_\phi(s_t) - r_{\text{est}}(s_t) \right)^2<br>$$<br>其中 $N$ 是总状态数。训练完成后，学得的奖励函数 $r_\phi$ 即可用于替代环境原始稀疏奖励，指导智能体策略学习（例如使用任何标准的强化学习算法）。</p>
<p><strong>创新点</strong>：与现有IRL方法相比，TW-CRL的创新具体体现在：1) <strong>利用失败示范</strong>：与仅使用成功示范的MaxEnt、GAIL、AIRL等方法不同，TW-CRL明确使用失败示范作为负样本，使智能体能学习避开导致失败的路径。2) <strong>引入时间加权</strong>：与IRLF等方法忽略失败时序信息不同，TW-CRL通过时间加权函数建模状态与轨迹最终结果（成功/失败）的概率关联，为奖励标签提供了更细粒度的、与时间相关的密集监督信号。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：论文在八个基准测试环境中评估TW-CRL，包括：PointMaze-Umaze、TrapMaze-v1、TrapMaze-v2、CartpoleSwingUp、HumanoidStandup、AntMaze、PandaReach和PandaPush。实验平台基于MuJoCo和PyBullet。使用PPO作为策略优化算法。</p>
<p><strong>对比方法</strong>：对比的基线方法包括：GAIL、AIRL、MaxEnt IRL、SASR（自适应奖励塑形）以及IRLF（从失败中学习IRL）。</p>
<p><strong>关键实验结果</strong>：论文通过平均回合回报（Average Episodic Return）和成功率（Success Rate）衡量性能。主要定量结果如下：</p>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/main_v2/umaze.png" alt="PointMaze-Umaze结果"></p>
<blockquote>
<p><strong>图2</strong>：在PointMaze-Umaze环境中的学习曲线。TW-CRL（红色）收敛速度最快，且最终平均回报最高，显著优于所有基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/main_v2/trapmaze.png" alt="TrapMaze-v1结果"></p>
<blockquote>
<p><strong>图3</strong>：在含有陷阱的TrapMaze-v1环境中的学习曲线。TW-CRL展现出绝对优势，回报远超其他方法。这证明了其在识别和避免陷阱状态方面的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/main_v2/ant.png" alt="AntMaze结果"></p>
<blockquote>
<p><strong>图7</strong>：在复杂的AntMaze环境中的学习曲线。TW-CRL同样取得了最佳性能，表明其方法在高维连续控制任务中的可扩展性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/main_v2/pandapush.png" alt="PandaPush结果"></p>
<blockquote>
<p><strong>图9</strong>：在机器人操作任务PandaPush中的成功率曲线。TW-CRL的成功率最终接近100%，且学习速度优于对比方法。</p>
</blockquote>
<p><strong>定性结果分析</strong>：论文可视化了学得奖励函数在迷宫中的热力图。</p>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/mid_rew/twcrl_2_3x1_plot.png" alt="TW-CRL奖励热力图"></p>
<blockquote>
<p><strong>图10</strong>：TW-CRL在TrapMaze-v2中学得的奖励函数热力图。可见目标区域（右上）奖励最高，陷阱区域（左下凹坑）奖励极低，且路径上的奖励呈现合理的梯度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/mid_rew/gail_3x1_plot.png" alt="GAIL奖励热力图"></p>
<blockquote>
<p><strong>图11</strong>：GAIL学得的奖励函数热力图。奖励主要集中在专家轨迹经过的路径上，但对陷阱区域没有明确的负奖励信号。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过消融研究验证了两个核心组件的贡献。</p>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/ab_v2/ab_t_1.png" alt="时间加权消融实验"></p>
<blockquote>
<p><strong>图19</strong>：在TrapMaze-v1中，移除时间加权（即对所有状态赋予相同权重）的性能对比。使用时间加权的完整TW-CRL性能更优。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.05585v2/extracted/6464848/figs/ab_v2/ab_sfd_1.png" alt="失败数据消融实验"></p>
<blockquote>
<p><strong>图20</strong>：在TrapMaze-v1中，仅使用成功示范的消融实验性能。仅用成功数据（TW-CRL w/o FD）的性能显著下降，证明了利用失败数据的必要性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1. <strong>形式化陷阱状态问题</strong>：明确定义了情节式任务中的陷阱状态，并分析了其对智能体学习的阻碍。2. <strong>提出TW-CRL框架</strong>：创新性地结合时间加权函数与对比奖励学习，能够同时从成功和失败示范中学习准确、密集的奖励函数，有效引导智能体避开陷阱并促进探索。3. <strong>实证验证</strong>：在八个具有挑战性的基准测试中，TW-CRL在学习效率和最终性能上均超越了现有的先进方法。</p>
<p><strong>局限性</strong>：论文提到，TW-CRL的一个核心假设是失败轨迹的最终状态是一个陷阱状态。在现实中，某些失败可能并非由于进入严格的吸收态，而是由于在时限内未达到目标，这可能会影响时间加权函数推导的精确性。</p>
<p><strong>对后续研究的启示</strong>：1. <strong>失败数据的价值</strong>：研究表明，失败示范是提升IRL数据效率和策略鲁棒性的宝贵资源，未来工作可以探索更精细的失败模式利用方式。2. <strong>时序信息的重要性</strong>：在从示范中学习奖励时，考虑状态在轨迹中的时间位置可以提供更丰富的监督信号，这一思路可扩展到其他序列决策学习问题中。3. <strong>奖励函数的可解释性</strong>：TW-CRL学得的奖励函数具有直观的解释性（高奖励指向目标，低奖励标识危险），这有助于增强人机交互与信任。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对强化学习中稀疏奖励、高维状态空间及隐藏“陷阱状态”导致的效率低下问题，提出了一种高效的逆强化学习框架TW-CLR。其核心方法是**时间加权对比奖励学习**，通过结合成功与失败演示的时间信息，学习一个能识别关键成败状态的密集奖励函数，从而引导智能体避免陷阱并鼓励探索。在八个基准测试上的实验表明，该方法超越了现有最优方法，实现了更高的学习效率与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.05585" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>