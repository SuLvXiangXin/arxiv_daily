<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.17428" target="_blank" rel="noreferrer">2601.17428</a></span>
        <span>作者: Marco Hutter Team</span>
        <span>日期: 2026-01-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在足式机器人领域，使机器人能够灵巧、多能地穿越复杂非结构化环境仍然是一个重大挑战。课程强化学习（CRL）通过将训练过程构建为从简单任务到困难任务的渐进式学习，已成为应对这一挑战的关键方法。目前主流的CRL应用主要依赖于手动设计的课程，其中智能体按照预先定义的、沿单一难度轴排序的任务序列进行训练，进阶由人工调整的性能阈值控制。然而，在现实场景中，机器人需要执行的任务集通常不具备明确定义的难度排序，例如比较在楼梯上慢走和在碎石上快跑的难度是模糊的。这些任务共同构成了一个非结构化的任务空间，其中缺乏明确排序的任务序列使得手动设计课程变得不可行。</p>
<p>本文针对在复杂、广泛的任务空间中难以预先定义难度结构的核心痛点，提出了基于学习进度的自动课程强化学习新视角。该方法旨在无需任务空间难度分布的先验知识，通过在线估计智能体的学习进度并自适应调整任务采样分布，实现自动课程生成。核心思路是利用智能体在各任务实例上的学习进度（定义为相邻课程阶段平均回合奖励的变化）来动态指导采样，优先学习当前最具提升潜力的任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法称为基于学习进度的自动课程强化学习（LP-ACRL）。其整体框架是一个闭环过程：智能体根据当前的任务采样分布与环境交互收集数据，用于策略更新；同时，系统评估智能体在各个任务实例上的表现，计算学习进度，并据此更新下一阶段的任务采样分布，形成课程。</p>
<p><img src="https://arxiv.org/html/2601.17428v1/figs/3_Approach/LP-ACRL.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：基于学习进度的自动课程强化学习（LP-ACRL）工作流程。任务采样分布基于学习进度进行更新，优先考虑对策略进一步改进最具信息量的任务。</p>
</blockquote>
<p>核心模块是学习进度的计算与采样分布的更新机制。方法操作于一个离散的任务空间 𝒯，每个任务实例 ζ 由分类维度（如地形类型）和离散化的连续维度（如目标速度命令区间）共同定义。</p>
<p>对于一个任务实例 ζ，其学习进度 LP 被定义为当前课程阶段 j 与前一阶段 j-1 的期望回合奖励之差：<br>LP_{c_j}(ζ) = R_{c_j}(ζ) - R_{c_{j-1}}(ζ)<br>其中，R_{c_j}(ζ) 是在分布 c_j 下从轨迹估计的期望回合奖励。</p>
<p>基于计算得到的所有任务实例的学习进度值，下一课程阶段的任务采样分布通过一个softmax算子进行更新：<br>c_{j+1}(ζ) = e^{LP_{c_j}(ζ)/β} / ∑_{ζ′∈𝒯} e^{LP_{c_j}(ζ′)/β}<br>其中 β 是温度参数，用于控制分布的尖锐程度。</p>
<p>与现有方法相比，LP-ACRL的创新点在于其简单而有效的课程生成机制。它无需任何关于任务难度或期望性能的先验知识，仅依赖于智能体自身学习动态的在线评估（即回合奖励的变化）。通过将采样概率与学习进度正相关，方法能够自动聚焦于那些智能体正在快速掌握或仍有提升空间的任务（通常对应“适中难度”区域），同时避免在已掌握或暂时无法掌握的任务上浪费采样资源。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在IsaacLab仿真环境中进行，使用ANYmal D四足机器人模型。评估了三个逐步复杂的任务空间：1）平坦地形上的多级线性速度跟踪；2）多类型崎岖地形穿越；3）大规模异构任务空间（结合速度命令、地形类型和地形难度级别）。对比的基线方法包括：绝对学习进度（ALP）、优先级别回放（PLR）、简单手工课程（SC）、低奖励优先课程（LRPC）和均匀采样（Uniform）。引入了一个综合评估指标：带稳定性惩罚的回合百分比跟踪误差（EPTE-SP），该指标同时惩罚跟踪误差和因摔倒导致的提前终止。</p>
<p><img src="https://arxiv.org/html/2601.17428v1/x2.png" alt="性能评估"></p>
<blockquote>
<p><strong>图3</strong>：在平坦地形多级速度跟踪任务上的EPTE-SP评估。LP-ACRL训练的策略在大多数速度区间都保持了最低的EPTE-SP和最窄的变异范围，表现显著优于所有基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.17428v1/x3.png" alt="奖励收敛"></p>
<blockquote>
<p><strong>图4</strong>：训练过程中各任务实例的回合奖励演化。LP-ACRL在几乎所有速度区间都表现出更快的收敛速度和更高的渐近性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.17428v1/x4.png" alt="采样分布演化"></p>
<blockquote>
<p><strong>图5</strong>：不同课程方法下的任务采样分布热图。LP-ACRL在训练早期快速推断出任务空间的潜在难度结构（从易到难），并在中后期灵活调整采样，平衡高难度探索与已掌握技能的保持。</p>
</blockquote>
<p>在第一个实验（平坦地形速度跟踪）中，LP-ACRL在EPTE-SP和奖励收敛方面均取得最佳整体性能。如图5所示，其采样分布演化清晰地展示了从易到难、再根据学习进度动态调整的自适应过程。</p>
<p><img src="https://arxiv.org/html/2601.17428v1/x5.png" alt="多地形奖励收敛"></p>
<blockquote>
<p><strong>图6</strong>：多类型崎岖地形任务上的回合奖励演化。即使在难度结构不明确的任务空间中，LP-ACRL在收敛速度和最终性能上也优于基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.17428v1/x6.png" alt="多地形采样分布"></p>
<blockquote>
<p><strong>图7</strong>：多类型崎岖地形任务上的任务采样分布热图。LP-ACRL能有效捕捉潜在难度结构（例如，上下楼梯和斜坡可能更具挑战性），并避免过度关注特定异常任务。</p>
</blockquote>
<p>在第二个实验（多类型崎岖地形）中，LP-ACRL同样展现出优越性，验证了其在非结构化任务空间中的有效性。</p>
<p><img src="https://arxiv.org/html/2601.17428v1/x7.png" alt="大规模任务空间"></p>
<blockquote>
<p><strong>图8</strong>：大规模异构任务空间示意图。该空间结合了多种地形类型、几何参数以及多级线性和角速度命令，任务难度在策略训练时未知。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.17428v1/x8.png" alt="成功率与成功集奖励"></p>
<blockquote>
<p><strong>图9</strong>：大规模任务空间上的成功率演化（上）和由LP-ACRL在3000次迭代时定义的成功集上的回合奖励（下）。LP-ACRL在1500次迭代内达到80%成功率，远超基线，并在掌握的任务上获得更高性能。</p>
</blockquote>
<p>在第三个也是最关键的实验（大规模异构任务空间，包含600个任务实例）中，LP-ACRL展示了强大的可扩展性。如图9所示，它在1500次训练迭代内达到了80%的成功率，而大多数基线方法在3000次迭代后仍难以收敛。同时，在它成功掌握的任务子集上，其策略性能（平均回合奖励）也高于其他方法。</p>
<p>最终，通过师生蒸馏框架，将LP-ACRL训练的策略成功部署到真实的ANYmal D机器人上。该策略在平坦地形上实现了高达3.0 m/s的线性速度，在楼梯、斜坡、碎石等挑战性地形上实现了高达2.5 m/s的线性速度，并在所有地形上实现了高达3.0 rad/s的角速度。据论文所述，这是在该硬件平台上达到的最高崎岖地形运动速度。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了LP-ACRL框架</strong>：一种利用从回合奖励推导的学习进度指标来动态调整任务采样分布的自动课程强化学习方法，无需为可扩展的机器人学习手动构建课程。</li>
<li><strong>在多尺度任务空间中验证了有效性</strong>：在从结构化速度跟踪到非结构化大规模异构地形的三个不同复杂度的机器人运动任务空间中，系统地验证了LP-ACRL相对于多种手工和自动课程基线的优越性。</li>
<li><strong>实现了高性能的真实机器人部署</strong>：通过策略蒸馏，将仿真中训练的单一策略成功迁移到物理ANYmal D平台，实现了前所未有的高速、鲁棒的崎岖地形运动能力。</li>
</ol>
<p>论文提到的局限性主要在于方法基于离散化的任务空间运行。虽然连续维度被离散化为区间，但这可能限制了课程在连续参数空间中进行更细粒度调整的能力。</p>
<p>本文对后续研究的启示在于：为复杂、多轴、非结构化的机器人学习任务空间提供了一个强大且简单的自动课程生成基线。其核心思想——利用智能体自身学习动态的反馈（学习进度）来指导课程——可以扩展到其他需要处理高维、耦合任务参数的领域。未来的工作可以探索如何将离散的学习进度估计与连续的任务参数空间更紧密地结合，或者将学习进度的概念与其他课程生成信号（如新奇性、不确定性）相结合，以进一步提升学习效率和最终策略的性能边界。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对腿式机器人在复杂、无结构崎岖地形上难以实现高速稳定运动的问题，提出了**基于学习进度的自动课程强化学习（LP-ACRL）框架**。该框架的核心在于**在线估计智能体的学习进度，并据此自适应调整任务采样分布**，从而无需预先定义任务难度即可自动生成训练课程。实验表明，采用LP-ACRL训练的策略使ANYmal D四足机器人在楼梯、斜坡、碎石等多种地形上实现了**2.5 m/s的线速度和3.0 rad/s的角速度**，超越了此前方法在高速与复杂地形性能上的局限。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.17428" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>