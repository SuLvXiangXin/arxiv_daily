<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.14161" target="_blank" rel="noreferrer">2511.14161</a></span>
        <span>作者: Jiayu Chen Team</span>
        <span>日期: 2025-11-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前家庭整理领域的主流方法存在关键局限性。传统方法需要用户为每个物品显式指定目标容器，过程繁琐且计算效率低；或者通过平均用户数据学习非个性化的“典型放置”规则，忽略了个人偏好差异。TidyBot等近期工作尝试从小样本推断用户偏好，但其缺乏可扩展、物理逼真的基准，且在多样化环境中的鲁棒性和泛化能力有限。同时，现有的具身智能基准大多针对通用任务执行，未能提供一个在逼真的视觉和物理条件下，统一评估高层推理与低层、反馈驱动控制的整理任务平台。</p>
<p>本文针对上述痛点，提出了一个支持视觉-语言-动作（VLA）和视觉-语言-导航（VLN）的统一基准，用于语言引导的家庭整理。其核心思路是：创建包含500个逼真3D高斯溅射（3DGS）家庭场景的数据集，将整理任务形式化为“动作（物品，容器）”列表，并提供大量高质量的操作与导航演示轨迹，以支持从模拟到现实的全面训练与评估。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboTidy是一个在NVIDIA Isaac Sim中构建的模块化框架，集成了物理和逼真模拟。系统以多视角观测为输入，利用Qwen2.5-VL提取物品和容器的类别与属性语义，为新遇到的物品直接选择目标容器和四种操作动作之一。整体框架包含四个核心模块。</p>
<p><img src="https://arxiv.org/html/2511.14161v2/pictures/framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RoboTidy基准框架与数据集概览。它涵盖了导航、物品排序和操作：Qwen2.5-VL将观测解析为“动作（物品，容器）”列表并执行为操作动作。数据集提供500个3DGS家庭场景、500个物品和容器、6.4k条操作轨迹和1.5k条导航轨迹，用于模拟到现实评估。</p>
</blockquote>
<p><strong>1. 整理模块（对象排序）</strong>：该方法无需额外输入。系统被动扫描工作空间内容器及其内部物品，获取现有的“物品→容器”对应关系。利用视觉语言模型（VLM）提取物品语义和属性，自动将这些观测转换为可解析的代码式提示。在此基础上，大语言模型（LLM）将观察到的对应关系抽象为个性化的物品排序“动作（物品，容器）”列表。对于归纳规则未覆盖的新物品，则联合决定目标容器和执行的动作。设计了四种基本操作动作：拾取放置、拾取投掷、打开容器、关闭容器。为建模多样化偏好，为每个物品标注了一组可接受的容器，并设定了基于属性、功能、安全性、卫生等准则的优先级。</p>
<p><img src="https://arxiv.org/html/2511.14161v2/pictures/action_list.png" alt="对象排序流程"></p>
<blockquote>
<p><strong>图2</strong>：对象排序流程。从工作空间观测中，Qwen2.5-VL识别物品和容器并生成动作（物品，容器）列表，系统执行操作动作以完成排序。</p>
</blockquote>
<p><strong>2. 操作动作模块</strong>：使用逆运动学（IK）求解器与运动规划器为操作动作规划并执行轨迹，并收集用于VLA训练和评估的演示轨迹。给定物品的6D位姿和目标容器位置，系统自动生成预抓取、抓取、预放置、放置等路径点；IK求解器提出候选关节路径点，运动规划器在关节限制、速度、加速度和环境碰撞约束下计算无碰撞轨迹。夹爪采用闭环宽度阈值标准来决定抓取和释放。同步记录多视角RGB图像、关节配置、夹爪状态及相应的动作基元标签。</p>
<p><strong>3. 导航模块</strong>：由路径规划器和底层控制器组成。在数据生成阶段，在InteriorGS的2D语义地图上运行A*规划器规划参考路径，并将其时间参数化、离散化为航点；PID控制器使用机器人实时位姿跟踪轨迹，输出速度命令，并记录完整的导航轨迹用于VLN训练和评估。在VLM评估阶段，不提供地图，相机图像和语言指令构成多模态输入，驱动VLM直接预测导航速度命令。底层控制由近端策略优化（PPO）策略将速度命令转换为关节级电机动作。</p>
<p><strong>4. 传感器模块</strong>：支持使用RGB-D相机和LiDAR进行多模态数据收集。在模拟环境中，通过将新传感器安装在机器人或场景中、配置参数并在主配置中引用其prim路径来集成。在现实世界中，传感器数量受计算资源、I/O吞吐量和接口带宽限制。</p>
<p><strong>评估指标</strong>：</p>
<ul>
<li><strong>物品放置准确率（OPA）</strong>：将放置视为从场景的候选容器闭集中进行选择。计算每个场景及整体的正确预测比例。</li>
<li><strong>有效排序成功率（VSSR）</strong>：一个物品的成功条件是其预测容器有效且所需操作动作成功完成。计算每个场景及整体的成功比例。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：基于InteriorGS构建了500个涵盖客厅、卧室、厨房的3DGS家庭场景（每个场景是3DGS-网格混合体，3DGS用于高保真渲染，分解的凸网格用于碰撞）。包含500个日常物品的3D资产。提供了6.4k条高质量操作演示轨迹（覆盖四种动作）和1.5k条导航轨迹（在三个房间间生成）。使用NVIDIA Isaac Sim 5.0进行模拟实验，并使用Cobot-Magic双臂移动平台进行现实世界验证。</p>
<p><strong>对比的基线方法</strong>：</p>
<ul>
<li><strong>对象排序</strong>：RoBERTa（基于句子BERT嵌入）、CLIP（基于CLIP文本嵌入）、TidyBot（小样本偏好归纳）。</li>
<li><strong>操作（VLA）</strong>：ACT（基于CVAE的模仿学习）、RDT（扩散增强的Transformer）、π0.5（具有语义规划和流匹配控制的VLA策略）。</li>
<li><strong>导航（VLN）</strong>：VLN-CE（CMA）、NaVid（基于视频的VLM）、NaVILA（两级规划框架）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><strong>1. 对象排序任务</strong>：在零样本（ZS）和小样本（FS）设置下评估。如表2所示，RoboTidy方法（使用Qwen2.5-VL-72B）在两种设置下均达到最先进性能，平均OPA显著优于所有基线。这表明其使用的强大VLM能够通过多模态理解抽象排序规则。表4显示，随着Qwen2.5-VL模型规模增大，OPA有所提升，但提升并非线性。</p>
<p><strong>表2</strong>：不同对象排序方法的比较。报告了每种方法在ZS和FS设置下的OPA（%），均值和标准差超过3个随机种子。RoboTidy在客厅、厨房、卧室及平均OPA上均表现最佳。</p>
<p><strong>表4</strong>：不同规模的Qwen2.5-VL在ZS和FS设置下的OPA（%）比较。72B版本在两种设置下均获得最高OPA。</p>
<p><strong>2. 操作任务</strong>：在未见过的家庭场景、物品和容器上评估VLA策略的泛化能力。如表5所示，未经预训练的模型（ACT）表现不佳；而预训练模型（RDT， π0.5）在不同场景和物品上保持了更强的鲁棒性，其中π0.5取得了最佳整体性能。这证实了RoboTidy提供的多样化、背景丰富的轨迹有助于提升泛化能力。</p>
<p><strong>表5</strong>：不同VLA方法在RoboTidy基准上的比较，报告了成功率（%）±标准差。π0.5在四项操作任务上成功率最高。</p>
<p><strong>3. 导航任务</strong>：在RoboTidy基准上评估VLN模型。如表3所示，除NaVILA外，其他模型的成功率（SR）均低于0.2。例如，NaVid-base在VLN-CE R2R Val-Unseen上SR为0.22，而在RoboTidy上降至0.16，表明RoboTidy基准对当前VLN模型提出了更大挑战。表6显示，仅在RoboTidy数据集上微调的模型（Navid-R， NaVILA-R）在VLN-CE基准上也取得了稳健提升。</p>
<p><strong>表3</strong>：VLN方法在RoboTidy基准上的比较。报告SR、OSR和SPL，均值和标准差超过3个随机种子。NaVILA-R获得了最高的SR（0.26）和SPL（0.23）。</p>
<p><strong>表6</strong>：不同VLN方法在VLN-CE基准R2R Val-Unseen设置下的比较。使用RoboTidy数据微调的Navid-R和NaVILA-R相较于其基础版本有所提升。</p>
<p><strong>4. 模拟到现实迁移实验</strong>：</p>
<ul>
<li><strong>实验E1</strong>：评估四种基本操作动作。如图5所示，在训练中融入RoboTidy合成演示轨迹（FS设置）显著提高了现实世界操作成功率。仅使用100条RoboTidy合成演示（ZS设置）训练的策略仍保持有竞争力的性能，在“拾取投掷”任务上与使用50条真实演示训练的表现相当。</li>
<li><strong>实验E2</strong>：多物品整理任务。如表7所示，结合合成与真实数据（FS）的设置获得了最高的成功次数（SR 8/12， VSSR 7/12）。仅使用合成数据（ZS）训练的策略取得了4/12的成功，与仅使用50条真实演示的结果（5/12）差距较小。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.14161v2/pictures/E1_and_E2.png" alt="真实世界任务可视化"></p>
<blockquote>
<p><strong>图3</strong>：真实世界任务可视化。E1：四种操作动作任务。E2：家庭整理任务，策略遵循“动作（物品，容器）”列表对物品进行排序。任务从左向右进行。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.14161v2/pictures/plat.png" alt="真实世界实验设置"></p>
<blockquote>
<p><strong>图4</strong>：真实世界实验设置。(a) 包含物品和容器的工作空间。(b) Cobot-Magic双臂移动操作平台。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.14161v2/pictures/E1.png" alt="真实世界实验结果（E1）"></p>
<blockquote>
<p><strong>图5</strong>：真实世界实验结果（E1）。报告了三种不同设置下四种操作动作任务的成功率。FS设置（结合真实与合成数据）在各项任务上成功率最高。</p>
</blockquote>
<p><strong>表7</strong>：真实世界实验结果（E2）。报告了三种不同设置下家庭整理任务的SR和VSSR（成功次数/总数）。FS设置表现最佳。</p>
<p><strong>5. 消融实验</strong>：如表8所示，在模拟和现实世界中，对比了指令中包含与不包含对象排序语义的变体。无论ZS还是FS设置，包含对象排序的指令都带来了显著的性能提升（模拟中VSSR提升约20%，现实中成功次数更多），这验证了高层语义规划对于完成复杂整理任务的重要性。</p>
<p><strong>表8</strong>：在ZS和FS设置下，包含与不包含对象排序的消融实验。在模拟中报告SR和VSSR（%）的均值和标准差；在现实世界中报告超过12个物品和3个容器的计数。包含对象排序的方法在所有设置下均表现更好。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了一个统一的语言引导家庭整理基准</strong>：将整理任务形式化为“动作（物品，容器）”列表，并联合支持视觉-语言-动作（VLA）和视觉-语言-导航（VLN）的训练与评估。</li>
<li><strong>提供了大规模、逼真的数据集</strong>：包含500个基于3D高斯溅射（3DGS）的光照真实家庭场景、500个物品和容器、6.4k条操作演示轨迹和1.5k条导航轨迹，为模拟到现实研究提供了丰富资源。</li>
<li><strong>验证了模拟到现实的有效迁移</strong>：通过现实世界机器人整理实验，证明了基于RoboTidy合成数据训练的策略能够有效迁移到真实环境，缩小仿真与现实间的差距。</li>
</ol>
<p><strong>局限性</strong>：论文提到，3DGS资产仅包含外观信息，需与碰撞网格结合使用；现实世界中传感器数量受计算资源和接口带宽限制。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li>RoboTidy基准的更高难度（体现在VLN任务的低成功率）为开发更鲁棒、泛化能力更强的VLN/VLA模型提供了新的挑战和评估平台。</li>
<li>所展示的模拟到现实迁移有效性，鼓励利用高质量合成数据来减少对昂贵、有限真实数据收集的依赖。</li>
<li>“动作（物品，容器）”列表的形式化及高层语义规划与低层控制结合的方法，为构建可解释、可泛化的具身智能系统提供了可借鉴的框架。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有家务整理基准缺乏用户偏好建模、移动性支持且泛化能力差的问题，提出了RoboTidy统一基准。该基准基于3D高斯泼溅（3DGS）技术构建了500个光真实感家庭场景，包含500个对象和容器，并提供6.4k操作轨迹与1.5k导航轨迹，支持视觉-语言-动作（VLA）和视觉-语言-导航（VLN）的训练与评估。通过将整理任务形式化为“动作（对象，容器）”列表，RoboTidy实现了语言引导机器人的整体仿真到现实评估，填补了体现AI中的关键空白。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.14161" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>