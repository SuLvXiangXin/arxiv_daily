<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>U-DiT Policy: U-shaped Diffusion Transformers for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>U-DiT Policy: U-shaped Diffusion Transformers for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.24579" target="_blank" rel="noreferrer">2509.24579</a></span>
        <span>作者: Zhongxue Gan Team</span>
        <span>日期: 2025-09-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>基于扩散模型的策略学习已成为机器人端到端视觉运动控制的一个强大范式。现有主流方法主要采用基于U-Net架构的扩散策略（DP-U），该方法虽然有效，但存在全局上下文建模能力有限和过度平滑伪影的问题。其时间卷积固有的归纳偏置倾向于低频信号，这限制了其对快速变化动作的建模能力，并导致动作序列预测的平滑化。为缓解此问题，后续研究引入了基于Transformer的扩散策略（DP-T），利用Transformer的全局建模能力减轻了过度平滑。然而，DP-T及其改进版本（如使用AdaLN模块）虽然受益于全局上下文建模，却放弃了U-Net的U形结构，而该结构以其强大的多尺度特征融合能力著称。</p>
<p>本文旨在解决DP-U在全局信息建模方面的局限性，同时保留U形设计的多尺度优势。为此，论文提出了U形扩散Transformer策略（U-DiT Policy）。其核心思路是：将Transformer嵌入U-Net的每一层，通过注意力机制加强全局上下文建模，同时利用U形结构保留层次化的多尺度表征，从而在复杂的长时程机器人操控任务中实现更优的性能和鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>U-DiT Policy的整体框架是一个条件去噪扩散过程，它将机器人动作生成建模为给定观测条件下的去噪问题。策略以最近T_o步的观测数据O_t（包括视觉观测和本体感知状态）为输入，输出未来动作序列A_t。其核心噪声预测网络采用了U形扩散Transformer（U-DiT）作为主干。</p>
<p><img src="https://arxiv.org/html/2509.24579v1/main.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：U-DiT策略概览。a) U-DiT整体结构类似于DiT，但多个DiT块之间的连接被U-DiT层取代。策略以观测数据为输入，输出动作序列。b) U-DiT层的具体形式，每个Transformer块的宽度表示其嵌入维度。c) AdaLN模块采用自适应层归一化将条件信息融合到带噪声的动作嵌入中，带来更稳定的训练和更好的推理性能。</p>
</blockquote>
<p>U-DiT的整体架构遵循经典U-Net设计，包含编码器和解码器，两者具有相同数量的层级。编码器通过步长为2的下采样将输入动作序列压缩为不同阶段的表征，同时特征维度加倍；解码器则逐步上采样，将编码序列恢复至原始分辨率。每一层级的U-DiT层由多个DiT块组成，其大小随跨阶段的下采样或上采样而变化。编码器与解码器对应阶段之间通过跳跃连接进行特征融合，以补偿编码器下采样造成的信息损失，并保留局部细节。论文针对动作空间（32×7）设置了三个阶段，特征被下采样两次后恢复至原始大小。</p>
<p><strong>核心模块与创新点</strong>：</p>
<ol>
<li><strong>DiT块与AdaLN条件建模</strong>：在每个Transformer块内，动作序列的噪声嵌入首先通过多头自注意力建模时间依赖性。为了融入条件信息（观测O_t和扩散步数k），论文采用了自适应层归一化（AdaLN）。该机制将条件信息融合为缩放因子γ和偏移因子β，用于调整归一化后的输出：<code>AdaLN(x) = (1+γ(k, O_t))·LN(x) + β(k, O_t)</code>。相比交叉注意力，AdaLN允许条件信息直接调制动作表征的分布，有助于稳定训练梯度并提升预测性能。</li>
<li><strong>非对称解码器设计</strong>：这是U-DiT的一个关键创新。在解码器部分，论文将计算重心从编码器阶段（下采样部分）转移至解码器阶段（上采样部分）。具体而言，最终层的嵌入维度不再减少，而是保持与倒数第二层相同。这种非对称设计增强了输出端的表征能力，使模型在生成动作序列时能更好地保留细粒度特征，防止因过度压缩而损失控制性能。</li>
<li><strong>双向注意力掩码</strong>：不同于原始扩散策略中使用的因果注意力掩码（仅允许关注历史信息），U-DiT采用了双向注意力掩码。由于模型是对紧凑的动作轨迹表示（32×7）进行整体预测，双向掩码允许序列中任何时间步关注所有其他时间步，从而在扩散过程中充分利用全局上下文，更好地捕捉复杂依赖关系。</li>
</ol>
<p>与现有方法相比，U-DiT的创新点具体体现在：<strong>1）架构上</strong>，首次将U形结构与扩散Transformer结合，兼具全局建模和多尺度融合；<strong>2）设计上</strong>，引入非对称解码器增强输出表征；<strong>3）条件融合上</strong>，使用AdaLN和双向注意力掩码，实现更高效稳定的训练。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真实验</strong>：在RLBench基准的12个模拟操控任务上进行评估。使用双视角（前置摄像头和腕部摄像头），每个方法用50条演示轨迹训练，在100个未见过的episode上进行测试。</li>
<li><strong>真实机器人实验</strong>：在6-DOF Jaka机械臂上评估了4个任务（叠放积木、合上笔记本盖、叠放水果、将积木放入柜子）。使用两个RealSense D435摄像头，通过人类演示收集数据集（每任务50条轨迹，合上笔记本盖任务为30条）。</li>
<li><strong>对比基线</strong>：包括动作分块Transformer（ACT）、基于U-Net的扩散策略（DP-U）、基于Transformer的扩散策略（DP-T）以及纯Transformer架构的扩散策略（DiT）。为确保公平比较，将DP-U和DiT的参数量调整至与U-DiT相当的93M。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.24579v1/RLBench.png" alt="RLBench任务"></p>
<blockquote>
<p><strong>图2</strong>：RLBench仿真任务。实验在RLBench的12个模拟任务上进行。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>仿真性能对比</strong>：如图3所示，U-DiT在大多数任务上取得了最高的成功率。在“合上微波炉门”和“按下按钮”任务上优势最为明显。平均而言，U-DiT相比DP-U提升了10%，相比DiT提升了6%，相比DP-T提升了17%。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.24579v1/image.png" alt="仿真评估"></p>
<blockquote>
<p><strong>图3</strong>：仿真评估结果。U-DiT策略与原始DP及基于DiT主干的DP在12个RLBench任务上进行比较。U-DiT在大多数任务上超越了DiT和DP，平均成功率比DP-U高10%，比DiT高6%。</p>
</blockquote>
<ol start="2">
<li><strong>数据效率</strong>：在仅使用10条演示数据的低数据体制下，U-DiT平均成功率为23%，优于DiT的18%，显示出U形设计在数据稀缺时的优势。</li>
<li><strong>真实机器人性能</strong>：如表II所示，U-DiT在所有四个真实任务上均优于基线。相比DP-U平均提升22.5%，相比DiT提升17.5%，相比ACT提升18.75%。这突显了U-DiT在复杂真实场景中的有效性和鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.24579v1/real_task.png" alt="真实世界任务"></p>
<blockquote>
<p><strong>图4</strong>：Jaka机器人上的真实世界任务。任务1：叠放积木，任务2：合上笔记本盖，任务3：叠放水果，任务4：将积木放入柜子。每个任务评估20次试验。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：如表III所示，<strong>非对称结构</strong>相比对称结构带来6.25%的平均性能提升；<strong>使用AdaLN</strong>相比使用标准交叉注意力块带来显著提升（73.75% vs 47.5%），证实了AdaLN对训练稳定性和性能的重要性；<strong>双向注意力掩码</strong>相比因果注意力掩码，在多阶段任务上表现更佳，能有效缓解连续动作间的时间不一致性。</li>
</ol>
<p><strong>泛化与鲁棒性实验</strong>：</p>
<ol>
<li><strong>干扰物泛化</strong>：如图5所示，在“叠放积木”和“合上笔记本盖”任务中引入不同颜色、形状的干扰物或改变背景。在干扰较强的情况下（如T2、T3），U-DiT在关键步骤（如对齐黄色积木）上的表现通常比DP-U更稳定、更精确。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.24579v1/Distractor_Generalization.png" alt="干扰物泛化"></p>
<blockquote>
<p><strong>图5</strong>：干扰物泛化测试。在叠放积木和合上笔记本盖任务上测试这些能力。</p>
</blockquote>
<ol start="2">
<li><strong>光照泛化</strong>：如图6和表IV所示，测试了正常、弱光和无光三种照明条件。在弱光条件下，U-DiT在两个任务上的成功率均高于DP-U。在无光条件下，虽然任务成功率极低，但U-DiT的机械臂接近目标物体的精度更高。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.24579v1/light_generalization.png" alt="光照泛化"></p>
<blockquote>
<p><strong>图6</strong>：光照泛化。在叠放积木和合上笔记本盖任务上进行评估。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>U形扩散Transformer（U-DiT）</strong>这一新颖的主干网络，将U-Net的多尺度特征融合优势与Transformer的全局上下文建模能力相结合，并设计了非对称解码器以增强动作表征能力。</li>
<li>提出了一种<strong>增强的条件建模策略</strong>，整合了AdaLN和双向注意力掩码，实现了观测条件与时间嵌入的高效融合，提升了训练稳定性和动作生成的一致性。</li>
<li>在<strong>大规模仿真和真实机器人平台</strong>上进行了广泛实验，验证了U-DiT在性能、数据效率、以及对视觉干扰和光照变化的鲁棒性方面均显著优于现有基线方法，展现了强大的实用潜力。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确讨论其工作的局限性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>架构设计</strong>：U-DiT成功证明了在序列生成任务中结合U形结构与Transformer的可行性，这为其他需要同时建模局部细节和全局依赖的时序预测问题（如视频预测、运动规划）提供了新的架构思路。</li>
<li><strong>条件融合与训练</strong>：AdaLN在扩散策略中表现出的稳定性和有效性，鼓励进一步探索更高效、更鲁棒的条件信息注入机制。双向注意力掩码在整体序列预测中的优势，也提示在非自回归的机器人策略学习中，放松因果约束可能是有益的。</li>
<li><strong>机器人学习</strong>：U-DiT在仿真和真实世界中展现出的优异泛化能力和鲁棒性，表明其作为一种基础策略学习框架的潜力，可推动机器人应对更加复杂、开放环境下的操控任务。其数据效率方面的表现也值得在少样本模仿学习场景下进一步探索。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有基于U-Net的扩散策略（DP-U）在机器人操作中全局上下文建模能力有限、易产生过度平滑伪影的问题，提出U-DiT Policy框架。该方法结合U-Net的多尺度特征融合优势与Transformer的全局上下文建模能力，通过U-DiT层和AdaLN块增强策略表达力。实验表明，在模拟任务中U-DiT比基线平均性能提升10%，优于同类Transformer策略（DP-T）6%；在真实机器人任务中比DP-U平均提升22.5%，并展现出更强的泛化性与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.24579" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>