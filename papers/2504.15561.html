<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.15561" target="_blank" rel="noreferrer">2504.15561</a></span>
        <span>作者: Xu, Jingkai, Nie, Xiangli</span>
        <span>日期: 2025/04/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域的主流方法包括模仿学习（IL）和强化学习（RL）。模仿学习通过模仿专家示范来提取策略，避免了设计奖励函数的困难，但端到端学习在复杂长视野任务中容易因误差累积而失败。为处理复杂任务，现有方法引入了任务规划器或技能原语选择框架，但它们存在两大局限：要么依赖不灵活、非自适应的固定技能库，要么难以通过手动指定确定合适的技能抽象层级。此外，这些方法大多基于固定任务集，而现实世界要求机器人具备持续适应新任务的能力。持续学习（CL）为解决灾难性遗忘提供了框架，但现有CL方法主要在图像分类基准上验证，忽略了机器人操作同时需要扩展感知和运动能力的特性。当前应用于机器人的持续模仿学习（CIL）方法，或依赖显式数据回放带来隐私风险，或受限于生成伪示范的质量，或面临可扩展性约束，且普遍未能充分考虑机器人操作的内在特性，导致知识迁移效率低下。</p>
<p>本文针对机器人持续模仿学习中技能抽象僵硬、知识迁移效率低下的核心痛点，提出了一种基于技能提示的分层架构新视角。核心思路是：通过一个可扩展的技能代码本和注意力驱动的技能选择机制，实现技能的隐式持续获取与复用，并结合分层策略与模式近似，在技能和任务两个层面同时促进高效的知识迁移。</p>
<h2 id="方法详解">方法详解</h2>
<p>SPECI是一个端到端的分层持续模仿学习策略架构，其整体框架如论文图1(a)和图2(a)所示，由三个紧密耦合的层次模块组成：多模态感知与融合模块、高层技能推断模块和低层动作执行模块。策略被分解为 $\pi = \pi^H \cdot \pi^L$，通过潜在技能变量 $\boldsymbol{z_t}$ 将高层技能选择与低层运动控制分离。</p>
<p><img src="https://arxiv.org/html/2504.15561v1/x1.png" alt="方法概览"></p>
<blockquote>
<p><strong>图1</strong>：(a) SPECI概览，包含三个分层模块：多模态感知、技能推断和动作执行。(b) 四种持续学习场景的图示，每种场景要求不同的知识迁移能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.15561v1/x2.png" alt="详细框架"></p>
<blockquote>
<p><strong>图2</strong>：(a) 所提出的用于机器人持续模仿学习的SPECI框架。架构包含三个层次。多模态感知与融合模块对任务描述、工作空间和腕部视图图像以及机器人本体感知状态进行编码。在高层技能推断模块中，基于输出的状态嵌入 $\boldsymbol{s^{k,e}_t}$ 动态选择和加权技能向量。低层动作执行模块随后解码并以潜在技能变量 $\boldsymbol{z_t}$ 为条件采样动作 $\boldsymbol{a^k_t}$。此外，我们通过模式近似增强了最后两个层次中的Transformer解码器，以改善跨任务的知识共享与隔离。(b) 技能选择和模式近似机制的图示。</p>
</blockquote>
<p><strong>1. 多模态感知与融合模块</strong>：该模块负责处理异构的传感器输入。它使用特定模态的编码器（如用于图像的ResNet、用于语言的BERT、用于本体感知的MLP）分别对任务语言描述 $\boldsymbol{l^k}$、工作空间图像 $\boldsymbol{I_w}$、腕部图像 $\boldsymbol{I_h}$ 和本体状态 $\boldsymbol{s^{k,p}_t}$ 进行编码。然后，通过一个跨模态Transformer融合器整合这些编码，输出一个统一的状态表示 $\boldsymbol{s^{k,e}_t}$，为后续模块提供包含跨模态任务相关性的上下文信息。</p>
<p><strong>2. 高层技能推断模块</strong>：这是实现持续技能获取与复用的核心。模块维护一个<strong>可扩展的技能代码本</strong> $\mathcal{C} = {\boldsymbol{c_i}}_{i=1}^{M}$，其中每个 $\boldsymbol{c_i}$ 是一个可学习的技能嵌入向量，代表一种潜在的技能语义。当学习新任务 $T^k$ 时，会初始化一个新的技能子集并添加到代码本中，从而实现代码本的动态扩展。技能选择通过一个<strong>分解的注意力驱动机制</strong>实现：首先，计算状态嵌入 $\boldsymbol{s^{k,e}_t}$ 与代码本中所有技能键（与技能嵌入相关联）的注意力权重；然后，根据这些权重对对应的技能值进行加权求和，合成一个临时的潜在技能表示 $\boldsymbol{\tilde{p}}$；最后，通过模式近似（下文详述）将 $\boldsymbol{\tilde{p}}$ 分解为用于注意力计算的键成分 $\boldsymbol{p_K}$ 和值成分 $\boldsymbol{p_V}$，它们共同构成最终选定的潜在技能变量 $\boldsymbol{z_t} = (\boldsymbol{p_K}, \boldsymbol{p_V})$。这个过程实现了基于当前上下文动态检索和组合已有技能，无需手动定义。</p>
<p><strong>3. 低层动作执行模块</strong>：该模块是一个以技能为条件的概率策略。它接收状态嵌入 $\boldsymbol{s^{k,e}_t}$ 和选定的技能变量 $\boldsymbol{z_t}$，通过一个Transformer解码器输出参数化的动作分布（如高斯分布的均值和方差），进而采样生成具体的机器人动作 $\boldsymbol{a^k_t}$。这种设计能够捕捉技能执行中存在的行为多样性。</p>
<p><strong>4. 模式近似</strong>：这是增强任务层面知识迁移的关键创新。为了在Transformer解码器的注意力参数中同时容纳任务特定和任务共享的知识，论文将每个任务的查询投影矩阵 $\boldsymbol{W_Q}$ 分解为：$\boldsymbol{W_Q^k} = \boldsymbol{W_Q^s} + \boldsymbol{\Lambda_Q^k}$。其中，$\boldsymbol{W_Q^s}$ 是跨任务共享的基矩阵，$\boldsymbol{\Lambda_Q^k}$ 是任务 $T^k$ 特有的低秩偏差矩阵。类似地，价值投影矩阵 $\boldsymbol{W_V}$ 也进行同样分解。这种分解允许模型在保留任务间通用知识（通过共享基）的同时，灵活地适配特定任务的需求（通过任务特定偏差），从而在促进正向迁移（旧技能帮助新任务）和减少负向迁移（新任务干扰旧任务）之间取得平衡。</p>
<p><strong>创新点总结</strong>：与现有方法相比，SPECI的创新主要体现在：1) <strong>分层策略架构</strong>将技能推理与动作执行分离，更适合复杂长视野任务；2) <strong>动态可扩展技能代码本与注意力选择机制</strong>实现了技能的隐式、持续、自动化获取与复用，克服了手动定义技能的局限性；3) <strong>模式近似</strong>通过在参数层面显式分解，同时优化了任务级的知识共享与隔离。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境（Meta-World和RLBench）中构建了四个持续学习任务套件进行评估：1) <strong>不同物体</strong>（MW-10/50）：任务目标相同但涉及不同物体；2) <strong>不同任务</strong>（RLBench-5）：在相同场景中执行不同任务；3) <strong>不同布局</strong>（MW-Layout）：相同任务但物体初始布局不同；4) <strong>混合变化</strong>（MW-Mixed）：任务、物体和布局均不同。评估指标包括平均成功率（AP）、前向迁移（FWT）、后向迁移（BWT）和全局平均成功率（AUC）。</p>
<p><strong>对比方法</strong>：与七种基线方法对比，包括：1) 微调（Finetune）；2) 基于正则化的方法（EWC， PackNet）；3) 基于回放的方法（ER， DER）；4) 基于蒸馏的方法（LWF）；5) 多任务学习上限（MTL）。</p>
<p><img src="https://arxiv.org/html/2504.15561v1/x3.png" alt="主要结果对比"></p>
<blockquote>
<p><strong>图3</strong>：在四个任务套件上的主要性能对比。SPECI在大多数指标上显著优于所有基线方法。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li>在MW-10上，SPECI的AP达到95.1%，AUC达到93.7%，均大幅领先最佳基线（PackNet的AP 87.7%，AUC 86.1%）。</li>
<li>在更具挑战性的MW-50上，SPECI的AP（89.4%）和AUC（88.2%）依然保持领先，而其他方法性能下降明显（如Finetune的AP仅49.7%）。</li>
<li>在RLBench-5上，SPECI的AP（85.0%）和AUC（83.7%）同样最优，显示出处理不同任务序列的能力。</li>
<li>在MW-Layout和MW-Mixed套件上，SPECI也一致性地取得了最佳性能。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.15561v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融研究结果。移除技能代码本（w/o SCB）、模式近似（w/o MA）或两者（w/o SCB&amp;MA）都会导致性能显著下降，验证了核心组件的必要性。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过系统性的消融实验验证了各核心组件的贡献。</p>
<ul>
<li><strong>移除技能代码本（w/o SCB）</strong>：AP和AUC在所有套件上均大幅下降（例如在MW-10上AP从95.1%降至86.4%），证明了隐式技能获取与复用机制的有效性。</li>
<li><strong>移除模式近似（w/o MA）</strong>：性能也有明显下降（如MW-10上AP降至91.2%），表明任务特定与共享参数的分解对知识迁移至关重要。</li>
<li><strong>同时移除两者（w/o SCB&amp;MA）</strong>：性能下降最为严重，接近或低于部分基线方法。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.15561v1/x5.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：在MW-50套件上的定性结果。SPECI成功完成了涉及新物体的任务（如“放置杯子”），而基线方法（如PackNet）则失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.15561v1/extracted/6357075/Pictures/Ours_VS_RESNET-T_VS_RESNET-RNN_VS_ViT-T_PackNet_VS_SEQ_VS_FWT_new.jpg" alt="知识迁移分析"></p>
<blockquote>
<p><strong>图6</strong>：前向迁移（FWT）分析。SPECI在所有任务套件上均取得了最高的正向知识迁移分数，表明先前学习的技能有效促进了新任务的学习。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.15561v1/extracted/6357075/Pictures/Ours_VS_RESNET-T_VS_RESNET-RNN_VS_ViT-T_PackNet_VS_MTL_VS_AUC_new.jpg" alt="全局性能分析"></p>
<blockquote>
<p><strong>图7</strong>：全局平均成功率（AUC）对比。SPECI的AUC在所有设置下都接近或超过多任务学习（MTL）的上限，表明其在整个持续学习过程中保持了卓越且稳定的整体性能。</p>
</blockquote>
<p><strong>知识迁移分析</strong>：SPECI在正向迁移（FWT）和反向迁移（BWT）指标上均表现优异。高FWT值说明旧技能帮助了新任务学习；同时，其BWT值也为正或接近零（优于严重的负值），说明在学习新任务时对旧任务的遗忘得到了有效缓解，实现了优异的双向知识迁移。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>SPECI框架</strong>，一个将多模态感知融合、上下文感知技能推断和低层动作执行统一到连贯管道中的分层持续模仿学习框架，显著提升了复杂长视野任务的性能。</li>
<li>设计了<strong>可扩展技能代码本和注意力驱动选择机制</strong>，实现了技能的隐式持续获取与高效跨任务复用，无需手动进行技能抽象。</li>
<li>引入了<strong>模式近似</strong>，通过分解Transformer解码器的注意力参数，将任务特定与任务共享知识融入策略，增强了任务层面的知识迁移。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，SPECI的性能在一定程度上依赖于多模态示范数据的质量和数量。在数据稀缺或噪声较大的场景下，技能抽象和知识迁移的效果可能会受到影响。</p>
<p><strong>对后续研究的启示</strong>：SPECI展示了在机器人持续学习中显式建模技能层次并进行参数化知识分解的有效性。未来的工作可以探索：1) 将技能代码本与大规模基础模型（如视觉-语言模型）结合，以利用更丰富的先验知识；2) 在更复杂的真实机器人平台和动态环境中验证该框架；3) 研究技能的可解释性，使机器人能够解释其决策过程。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中传统模仿学习难以适应动态环境、现有持续模仿学习方法跨任务知识迁移不佳的问题，提出了基于技能提示的分层持续模仿学习框架SPECI。该框架通过多模态感知、高层动态技能推理与低层动作执行的层级结构，利用可扩展技能码本和注意力机制实现隐式技能获取与重用，并结合模式近似法增强知识迁移。实验表明，SPECI在多种操作任务上全面优于现有先进方法，展现出卓越的双向知识迁移与整体性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.15561" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>