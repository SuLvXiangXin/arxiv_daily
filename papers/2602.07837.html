<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.07837" target="_blank" rel="noreferrer">2602.07837</a></span>
        <span>作者: Zang, Hongzhi, Yu, Shu&#39;ang, Lin, Hao, Zhou, Tianxing, Huang, Zefang, Guo, Zhen, Xu, Xin, Zhou, Jiakai, Sheng, Yuze, Zhang, Shizhe, Gao, Feng, Tang, Wenhao, Yue, Yufeng, Zhang, Quanlu, Chen, Xinlei, Yu, Chao, Wang, Yu</span>
        <span>日期: 2026/02/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前具身智能领域的常见范式是在仿真环境中训练策略，然后将其部署到真实世界。然而，仿真与真实世界在动力学、感知和交互方面不可避免的差距，常常导致策略迁移后性能显著下降。这促使研究者日益关注直接在物理世界中学习策略。与仿真不同，真实世界的在线学习无法任意加速、重置或复制。机器人以实时速度运行，平台具有异构性，网络可能不稳定，实验过程漫长且经常被打断。这些特性使得真实世界在线策略学习不仅是一个算法挑战，更是一个紧密耦合物理执行、通信和优化的系统问题。</p>
<p>目前，可扩展的真实世界学习面临几个系统级挑战：首先，机器人通常被视为外部环境，难以在分布式部署中与加速器联合调度。其次，真实世界在线学习越来越依赖云-边缘基础设施，特别是对于大规模视觉-语言-动作模型，其数据收集在边缘，训练在云端。这些组件跨越异构且隔离的网络域，导致带宽不对称和跨域延迟。此外，物理世界无法加速，使得数据效率成为主要瓶颈。仿真中常用的同步流水线因此在真实世界中学习效率低下。再者，具身学习正朝着使用高维视觉流和长时程的数据驱动训练发展，这极大地增加了数据量和实验时长。然而，现有的缓冲区和流水线仍然以内存为中心且生命周期短暂，缺乏对持久性、恢复和跨阶段数据重用的支持。</p>
<p>本文针对这些系统级痛点，提出应将真实世界机器人学习视为一个系统问题，而非仅仅依赖孤立的算法进步。为此，论文提出了USER系统，其核心思路是：通过一个统一的硬件抽象层将物理机器人视为与GPU同等的一类硬件资源，并构建一个包含自适应通信平面、完全异步学习框架和持久缓存感知缓冲区的可扩展系统，为真实世界在线策略学习提供统一的基础设施。</p>
<h2 id="方法详解">方法详解</h2>
<p>USER的整体设计旨在为跨越异构机器人和分布式计算资源的真实世界策略学习提供统一且可扩展的系统支持。其核心是将物理机器人和加速器虚拟化为可调度的一等硬件，并通过稳健的云-边缘通信层将它们连接起来。这一设计将学习逻辑与物理部署细节解耦，实现了统一的资源管理、可靠的跨域网络以及跨多机器人、多节点环境的低延迟数据交换。</p>
<p><img src="https://arxiv.org/html/2602.07837v2/x1.png" alt="系统架构"></p>
<blockquote>
<p><strong>图2</strong>：USER的系统架构设计。底层是统一硬件抽象层和自适应通信平面构成的基础设施，上层是基于此构建的学习框架，包括完全异步流水线、持久缓存感知缓冲区以及可扩展的策略、算法和奖励抽象。</p>
</blockquote>
<p><strong>1. 统一硬件抽象层</strong><br>USER的核心思想是将物理机器人视为与GPU、TPU等加速器同等的一类硬件资源。为此，USER在一个特殊的硬件抽象层中，将所有类型的机器人和加速器统一抽象和管理为可调度的硬件单元。该层提供了统一的接口来扩展新型硬件、自动发现可用硬件资源，并为不同的学习任务调度硬件。</p>
<ul>
<li><strong>节点与硬件抽象</strong>：USER将一次部署建模为一个节点集群，每个节点导出一组硬件单元。一个硬件单元是调度器可分配的最小原子实体——可以是一个GPU设备，也可以是一个物理机器人（可选地捆绑其所需的摄像头、空间鼠标等外设）。节点本质上是异构的，可以托管多种类型的硬件单元。USER通常部署三种类型的节点：用于策略推理的带GPU的Rollout节点、在仅CPU机器上运行以在边缘执行动作的机器人节点，以及用于集中式训练的大规模加速器训练节点。</li>
<li><strong>硬件注册与发现</strong>：HAL使用可插拔的检查器接口来注册和扩展新硬件。在集群初始化时，USER通过在每个节点上启动轻量级硬件探测进程并调用已注册的HAL检查器来发现可用硬件，从而构建全局硬件清单。对于物理机器人，发现过程包括网络可达性、所需摄像头存在性等可选验证。</li>
<li><strong>硬件调度</strong>：USER通过一个基于秩的单一放置接口来调度机器人和加速器。调度器确定性地将进程秩映射到这些资源秩，并启动每个进程，将其显式绑定到仅分配给它的硬件。这种统一机制使得单个作业中可以实现异构放置。</li>
</ul>
<p><strong>2. 自适应通信平面</strong><br>为了应对跨异构云-边缘网络域的通信挑战，USER设计了自适应通信平面。</p>
<ul>
<li><strong>基于隧道的云-边缘网络</strong>：为了桥接不同管理网络域之间的隔离，USER的通信平面建立在基于UDP隧道技术的扁平化TCP/IP基底上，使所有机器人、训练和Rollout节点能够通过隧道建立双向连接。</li>
<li><strong>分布式数据通道</strong>：USER通过通道抽象统一节点间的数据交换。一个通道代表连接系统组件的数据流导管。与传统集中式通信相比，USER提供<strong>分布式数据通道</strong>：一个由轻量级通道服务托管、基于数据键进行内部分片的命名FIFO生产者-消费者队列。这种设计将流量本地化在边缘或云区域内，减少了跨域传输。</li>
<li><strong>SM感知权重同步</strong>：当通过NCCL进行权重同步时，其集合操作作为消耗流式多处理器的CUDA内核执行，会占用GPU执行资源。USER通过一个可调配置，限制权重传输期间使用的NCCL CTA最大数量，从而抑制NCCL的SM占用，防止后台权重同步垄断GPU资源并降低Rollout延迟和吞吐量。</li>
</ul>
<p><strong>3. 学习框架设计</strong><br>在系统架构之上，USER的学习框架定义了运行时数据和计算的组织方式。</p>
<p><img src="https://arxiv.org/html/2602.07837v2/x2.png" alt="学习框架总览"></p>
<blockquote>
<p><strong>图3</strong>：学习框架设计概览：一个包含持久缓存感知缓冲区以及可扩展的策略、算法和奖励模型抽象的完全异步真实世界学习流水线。</p>
</blockquote>
<ul>
<li><strong>完全异步流水线</strong>：针对真实世界数据收集是主要瓶颈的问题，USER将真实世界策略学习组织为一个<strong>完全异步流水线</strong>。数据生成端，多个环境工作器通过Rollout工作器在物理机器人上持续执行策略，流式传输观测和动作，不受优化过程阻塞。训练端，学习工作器异步地从缓冲区采样小批量数据来更新模型参数。更新后的权重被周期性地同步回Rollout工作器，在保持机器人执行不间断的同时完成闭环。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.07837v2/x3.png" alt="异步流水线示意图"></p>
<blockquote>
<p><strong>图4</strong>：完全异步流水线示意图。USER将数据生成、训练、数据传输和权重同步解耦，显著提高了数据收集和训练吞吐量。</p>
</blockquote>
<ul>
<li><strong>持久缓存感知缓冲区</strong>：为支持长时程、非平稳策略和异步流水线，USER采用了一个<strong>持久、基于索引的缓冲区</strong>。轨迹被异步写入磁盘，而缓冲区存储带有策略版本、时间戳等元数据的轻量级索引，支持长时程的时间感知和策略感知采样。为了平衡效率和内存，USER增加了一个有界的内存缓存（FIFO替换）。新样本首先进入缓存；当缓存满时，旧条目被逐出但仍保留在磁盘索引中。被逐出的样本在请求时会透明地重新加载。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.07837v2/x4.png" alt="持久缓存感知缓冲区"></p>
<blockquote>
<p><strong>图5</strong>：持久缓存感知缓冲区。USER采用持久、基于索引的缓冲区。近期数据存储在内存中，而历史数据持久化到磁盘，有效平衡了访问效率和存储容量。</p>
</blockquote>
<ul>
<li><strong>可扩展的策略、算法和奖励</strong>：USER对模型架构和学习算法不可知，暴露统一接口。策略层面支持从CNN/MLP控制器、流匹配生成策略到大型VLA模型。算法层面支持多种学习范式，如SAC、SAC-Flow、RLPD、HG-DAgger等。奖励规范也是模块化的，支持基于规则的奖励、人工提供的标签和学习的奖励模型。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>论文在仿真和真实世界的一系列具身任务上评估USER，通过五组实验验证其系统架构和学习框架设计的有效性。实验平台包括搭载Franka机械臂的真实机器人系统，计算资源涉及本地工作站（RTX 4090）和服务器（4×A100 80GB）。</p>
<p><img src="https://arxiv.org/html/2602.07837v2/x5.png" alt="任务示意图"></p>
<blockquote>
<p><strong>图6</strong>：五个真实世界操作任务示意图：(a) 销钉插入，(b) 充电器插拔，(c) 抓放，(d) 瓶盖拧紧，(e) 桌面清理。</p>
</blockquote>
<p><strong>1. 主要结果（框架可扩展性）</strong><br>实验设计了五个真实世界操作任务（图6），并测试了多种策略（CNN、Flow、π₀ VLA模型）、算法（SAC、RLPD、SAC-Flow、HG-DAgger）和奖励类型（规则、人工、奖励模型）的组合。</p>
<p><img src="https://arxiv.org/html/2602.07837v2/x6.png" alt="强化学习训练曲线"></p>
<blockquote>
<p><strong>图7</strong>：多个操作任务上的强化学习训练曲线。x轴为挂钟时间，y轴为以窗口大小20计算的移动平均成功率。(a)(b)显示在Peg Insertion和Charger任务上，RLPD、SAC和SAC-Flow均在约2000秒内达到接近完美的成功率。(c)显示RLPD在Cap Tightening和Pick-and-Place任务上也成功收敛。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07837v2/figures/main-dagger-legend.png" alt="模仿学习训练曲线"><br><img src="https://arxiv.org/html/2602.07837v2/figures/main-dagger.png" alt="模仿学习训练曲线"><br><img src="https://arxiv.org/html/2602.07837v2/figures/main-dagger-size.png" alt="缓冲区大小增长"></p>
<blockquote>
<p><strong>图8</strong>：模仿学习（HG-DAgger）训练曲线。(a) 报告每回合的干预步数，更少的干预步数直接反映更高的策略性能。(b) 显示缓冲区大小增长。对于π₀模型，在线训练后性能显著提升（表II），例如Pick-and-Place任务成功率从65%提升至96.7%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07837v2/figures/reward-model.png" alt="奖励模型性能"></p>
<blockquote>
<p><strong>图9</strong>：在Peg Insertion任务中，使用训练的奖励模型（作为二元分类器）提供的监督信号，与人工奖励相比，达到了可比拟的性能。</p>
</blockquote>
<p><strong>2. 统一硬件抽象层的优势</strong><br>实验表明，统一硬件抽象层支持高效的多机器人训练和跨异构 embodiment 的鲁棒学习。</p>
<p><img src="https://arxiv.org/html/2602.07837v2/figures/2arms.png" alt="多机器人并行训练"></p>
<blockquote>
<p><strong>图10</strong>：在两个Franka机械臂上的并行训练。统一硬件层支持在单一系统框架内进行分布式数据收集和多任务学习。</p>
</blockquote>
<p><strong>3. 自适应通信平面的有效性</strong><br>通过测量跨域和同域网络设置下的分布式通道性能，验证了其效率。</p>
<p><img src="https://arxiv.org/html/2602.07837v2/x7.png" alt="通信性能对比表"></p>
<blockquote>
<p><strong>表III</strong>：分布式通道在跨域和同域网络设置下的通信性能。启用分布式通道在跨域部署中将回合生成时间减少了多达3倍。</p>
</blockquote>
<p><strong>4. 持久缓存感知缓冲区的性能</strong><br>测试了不同缓存比例下的采样吞吐量。结果表明，即使缓存比例低至1%，USER仍能保持高采样吞吐量（约20k样本/秒），而纯内存缓冲区在数据量超过内存容量时吞吐量骤降。</p>
<p><img src="https://arxiv.org/html/2602.07837v2/figures/sampling_throughput_vs_cache_ratio.png" alt="采样吞吐量 vs 缓存比例"></p>
<blockquote>
<p><strong>图21</strong>：采样吞吐量与缓存比例的关系。USER的持久缓存感知缓冲区在很小缓存比例下仍能维持高吞吐量，而纯内存缓冲区在数据超载时崩溃。</p>
</blockquote>
<p><strong>5. 完全异步流水线的优势</strong><br>通过消融实验比较同步与异步流水线，以及不同权重同步间隔的影响。</p>
<p><img src="https://arxiv.org/html/2602.07837v2/figures/async_sync_ablation.png" alt="异步与同步消融实验"></p>
<blockquote>
<p><strong>图23</strong>：异步与同步流水线的消融实验。完全异步流水线相比同步流水线，显著提高了学习效率和收敛速度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07837v2/figures/sync_weight_interval.png" alt="权重同步间隔影响"></p>
<blockquote>
<p><strong>图24</strong>：权重同步间隔对训练的影响。适中的同步间隔（如10秒）在保持策略新鲜度和避免通信开销之间取得了良好平衡。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为以下四点：</p>
<ol>
<li><strong>统一硬件抽象</strong>：将物理机器人视为与加速器同等的一类硬件资源，实现了异构机器人的自动发现、统一管理和灵活调度。</li>
<li><strong>自适应通信平面</strong>：通过基于隧道的网络、分布式数据通道和SM感知权重同步，解决了云-边缘跨域学习中的稳定高效通信问题。</li>
<li><strong>完全异步学习框架与持久缓存感知缓冲区</strong>：解耦数据收集与训练，并引入支持磁盘持久化和内存缓存的缓冲区，显著提升了长时程真实世界学习的效率和鲁棒性。</li>
<li><strong>可扩展的策略、算法和奖励抽象</strong>：在一个统一的执行和数据流水线中支持从传统控制器到大模型VLA的多种策略，以及从RL到模仿学习的多种算法范式，提供了高度的灵活性。</li>
</ol>
<p>论文自身提到的局限性或未来工作方向包括：扩展支持更多机器人类型（如移动机器人），以及进一步探索系统支持下的多任务和终身学习范式。</p>
<p>USER的工作对后续研究的重要启示在于：它强调了系统设计对于真实世界在线学习的重要性，并提供了一个可复用的开源系统基础。这使研究人员能够更专注于算法和模型的创新，而无需重复构建底层基础设施，有望加速真实世界具身智能的发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身AI在现实世界中直接进行在线策略学习时面临的系统级挑战，提出统一可扩展系统RLinf-USER。核心问题是解决物理世界无法加速、重置和复制导致的异构机器人调度、云边通信与长时训练效率低下。系统通过硬件抽象层统一管理机器人资源，采用自适应通信平面优化网络，并构建完全异步学习框架支持数据持久化与恢复。实验表明，该系统能有效支持多机器人协同、异构机械臂操控、云边大模型协作及长期异步训练，为现实世界在线策略学习提供了系统基础。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.07837" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>