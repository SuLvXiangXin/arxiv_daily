<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Vision-Language Models on the Edge for Real-Time Robotic Perception - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Vision-Language Models on the Edge for Real-Time Robotic Perception</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.14921" target="_blank" rel="noreferrer">2601.14921</a></span>
        <span>作者: Ahmad, Sarat, Hafeez, Maryam, Zaidi, Syed Ali Raza</span>
        <span>日期: 2026/01/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLM）在机器人感知与交互中展现出强大潜力，但其实际部署主要依赖云端推理。这种方法存在关键局限性：传输大量敏感多模态数据会引入显著的广域网延迟、带宽消耗以及隐私风险，并且依赖不稳定的网络连接，难以满足机器人对实时响应的严苛要求。针对延迟、资源受限和隐私这一具体痛点，本文提出了利用6G网络中的开放无线接入网（ORAN）和多接入边缘计算（MEC）基础设施，将VLM推理任务从云端迁移至网络边缘的新视角。本文的核心思路是：在真实的ORAN/MEC边缘节点上部署不同规模的VLM，通过人形机器人测试平台进行系统性评估，量化边缘部署相较于云端的延迟与精度权衡，为实时机器人感知提供可行的部署方案。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文设计了一个集成了实体机器人、实时通信和边缘推理的完整系统。整体流程如下：Unitree G1人形机器人作为多模态数据源，通过WebRTC协议将RGB视频流和文本查询实时传输至ORAN边缘节点；边缘节点上部署的VLM服务接收数据并进行推理，生成自然语言响应；响应通过WebRTC数据通道返回给机器人的React操作界面或语音模块，完成交互闭环。</p>
<p><img src="https://arxiv.org/html/2601.14921v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：边缘部署VLM的机器人感知系统架构概览。展示了Unitree G1机器人、基于WebRTC的通信与控制层、以及部署在ORAN边缘节点上的VLM推理服务之间的交互。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>实体机器人平台（Unitree G1）</strong>：作为用户设备（UE）和多模态数据源。配备RealSense D435i深度相机提供RGB视频流，以及麦克风阵列用于语音输入（通过ASR转为文本）。机器人同时是交互终端，通过TTS模块将VLM的文本响应转换为语音。</li>
<li><strong>操作控制界面（React应用）</strong>：提供实时视频监控、自然语言查询输入（聊天模块）以及系统反馈（如端到端延迟、模型响应时间、答案准确性）显示。</li>
<li><strong>边缘VLM部署</strong>：<ul>
<li><strong>模型选择与配置</strong>：部署了两个模型进行对比。一是大规模模型 <strong>Llama-3.2-11B-Vision-Instruct</strong>，采用4位NF4量化（含双量化）以减少内存占用。二是轻量级模型 <strong>Qwen2-VL-2B-Instruct</strong>，专为资源受限环境优化。推理使用贪婪解码，最大生成长度为50个token，并支持提前终止（EOS）。</li>
<li><strong>部署流水线</strong>：边缘节点通过FastAPI提供推理服务。处理流程标准化为：图像解码与预处理（调整大小、颜色转换）→视觉编码与多模态融合→基于Transformer的语言生成→文本解码并返回最终响应。该设计针对交互式的单帧推理进行了优化。</li>
<li><strong>硬件</strong>：边缘节点采用配备24GB VRAM的NVIDIA L4 GPU，代表典型的ORAN/MEC节点配置。</li>
</ul>
</li>
<li><strong>通信与控制层（WebRTC）</strong>：为实现低延迟交互，采用基于WebRTC的P2P框架。通过信令服务器协调会话建立；RGB帧通过SRTP视频轨道传输；文本、查询和控制元数据则通过WebRTC数据通道交换，确保加密和可靠交付。</li>
</ol>
<p>与现有方法相比，本文的创新点在于：<strong>在真实的ORAN/MEC边缘硬件和实体机器人平台上，对先进的大规模VLM和轻量VLM进行了端到端的系统集成与实证评估</strong>，而非在模拟或虚拟化环境中进行测试，从而直接揭示了在实际部署条件下的性能权衡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在 <strong>Unitree G1人形机器人</strong> 测试平台上进行了实验。使用了两个数据集：1) <strong>Robo2VLM</strong> 基准数据集，这是一个用于评估机器人操作场景下VLM性能的大规模基准；2) <strong>机器人自采数据集</strong>，在实验室环境中收集的200个Q&amp;A对，涵盖社交导航、手势识别、人类存在检测等更广泛的人机交互领域（如图2所示）。对比的基线方法包括：<strong>云端部署的Llama-3.2-11B-Vision-Instruct</strong>（通过NVIDIA NIM API）、<strong>边缘部署的同一Llama模型</strong>以及<strong>边缘部署的轻量模型Qwen2-VL-2B-Instruct</strong>。评估指标为任务准确率和端到端（E2E）延迟（从机器人捕获帧到收到VLM文本响应的总时间）。</p>
<p><img src="https://arxiv.org/html/2601.14921v1/x3.png" alt="评估结果"></p>
<blockquote>
<p><strong>图3</strong>：评估结果汇总。(a) 在Robo2VLM基准上的准确率与延迟；(b) 在机器人自采数据集上的准确率与延迟；(c) 本地部署的Llama-3.2端到端延迟分布分解。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ul>
<li><strong>精度对比</strong>：在Robo2VLM基准上，边缘部署的Llama-3.2达到41%准确率，Qwen2为28.02%，存在约13%的差距，表明复杂推理任务对模型规模敏感。在自采数据集上，边缘Llama-3.2的准确率为82.63%，甚至略高于云端部署的82.04%，而Qwen2为77.08%。</li>
<li><strong>延迟对比</strong>：边缘部署的Llama-3.2相比云端部署，实现了<strong>5%的延迟降低</strong>（自采数据集上从1685.20 ms降至1600.03 ms）。轻量模型Qwen2优势显著，将延迟<strong>降低了一半以上</strong>，达到707.85 ms，实现了亚秒级响应。</li>
<li><strong>延迟瓶颈分析</strong>：图3(c)显示，对于本地部署的Llama模型，<strong>文本生成阶段占据了超过85%的端到端延迟</strong>，而图像解码和预处理耗时可忽略，明确指出自回归解码是主要瓶颈。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.14921v1/x4.png" alt="分类准确率"></p>
<blockquote>
<p><strong>图4</strong>：VLM在机器人自采数据集上各人机交互领域的分类准确率。展示了不同模型在不同任务类型上的性能差异。</p>
</blockquote>
<p>图4的消融实验（按任务类别分析）进一步揭示了模型特性：边缘部署的Llama-3.2在所有领域均表现均衡，显示出更强的多模态推理能力；而Qwen2在“人类存在检测”、“指令跟随”等感知驱动型任务上表现有竞争力，但在“空间关系”、“社交导航”等需要深度推理的领域则落后。这提示了<strong>混合部署策略的潜力</strong>：轻量VLM处理快速的感知任务，大型VLM则被选择性调用用于复杂推理。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>系统集成</strong>：成功设计并实现了在真实ORAN/MEC边缘节点上部署VLM的实时机器人感知流水线。2) <strong>实证评估</strong>：通过标准化基准和机器人自采数据，系统比较了云与边缘部署的性能，量化了部署位置的影响，表明边缘部署能在保持接近云端精度的同时降低延迟。3) <strong>揭示权衡</strong>：通过对比大型与紧凑型VLM，清晰展示了在边缘计算资源约束下的<strong>延迟-精度权衡</strong>，大型模型是性能最优解，轻量模型是效率最优解。</p>
<p>论文自身提到的局限性在于，<strong>自回归解码是延迟的主要瓶颈</strong>，这限制了系统响应速度的进一步提升。这为后续研究指明了方向：未来工作应聚焦于<strong>模型-系统协同设计以减少延迟</strong>，特别是针对文本生成阶段，例如采用量化、推测解码或模型蒸馏等技术；以及探索<strong>自适应的混合部署策略</strong>，根据任务复杂度动态选择调用不同规模的模型，以在延迟和精度间取得最佳平衡。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何在资源受限的机器人系统中部署视觉语言模型，以解决云端推理带来的延迟、带宽与隐私问题。核心方案是利用6G边缘智能，特别是ORAN/MEC架构，将计算靠近数据源，并设计了基于WebRTC的多模态数据流管道。实验在Unitree G1人形机器人上进行，对比了边缘与云端部署：边缘部署LLaMA-3.2-11B模型在保持接近云端精度的同时，端到端延迟降低5%；而轻量化模型Qwen2-VL-2B实现了亚秒级响应，延迟减少一半以上，但精度有所下降。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.14921" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>