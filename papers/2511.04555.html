<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.04555" target="_blank" rel="noreferrer">2511.04555</a></span>
        <span>作者: Lin, Tao, Zhong, Yilei, Du, Yuxin, Zhang, Jingjing, Liu, Jiting, Chen, Yinxinyu, Gu, Encheng, Liu, Ziyan, Cai, Hongyi, Zou, Yanwen, Zou, Lixing, Zhou, Zhaoye, Li, Gen, Zhao, Bo</span>
        <span>日期: 2025/11/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉-语言-动作模型通常包含数十亿参数，并严重依赖大规模机器人数据（如OXE、DROID）进行预训练，导致训练计算成本高昂，且在实时推理时部署困难、控制频率低。此外，广泛采用的端到端训练范式往往会损害视觉-语言骨干网络的表征能力，导致模型在下游任务中过拟合且泛化能力差。本文针对VLA模型参数量大、计算成本高、语义表征易受损等痛点，提出了一种轻量级VLA模型新视角，旨在降低训练成本、提升部署效率，同时通过保持骨干网络的语义对齐来提升泛化性能。本文的核心思路是：采用一个紧凑的原生多模态视觉-语言模型作为骨干，设计交叉调制扩散变换器进行动作生成，并通过一种两阶段训练范式渐进地对齐感知与动作，从而在保持骨干网络强大语义理解能力的同时，高效适应下游控制任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>Evo-1的整体架构是一个模块化的VLA框架，包含三个核心组件：视觉-语言骨干网络、交叉调制扩散变换器（动作专家）以及集成模块。输入包括多视角RGB观测图像 <code>{I_t^i}</code>、语言指令 <code>L_t</code> 和机器人状态 <code>s_t</code>，输出为连续动作向量 <code>a_t</code>。</p>
<p><img src="https://arxiv.org/html/2511.04555v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Evo-1架构概览。左侧：输入RGB观测和语言指令首先由紧凑的视觉-语言骨干编码。其融合表征通过优化的集成模块与机器人状态对齐，然后由交叉调制扩散变换器处理以生成动作。右侧展示了在三个仿真基准上的结果。</p>
</blockquote>
<p><strong>1. 视觉-语言骨干网络</strong>：采用InternVL3-1B作为骨干，它是一个在原有多模态范式下预训练的模型，能联合学习语言和视觉理解。视觉编码器使用轻量级的InternViT-300M，图像被调整为448×448后，经过像素重排下采样操作，将视觉token数量减少4倍。语言分支使用Qwen2.5-0.5B。多模态融合通过将图像块嵌入替换语言序列中特定的<code>&lt;img&gt;</code>占位符来实现，并由共享的Transformer解码器进行联合推理。为了更好适应具身任务，仅保留语言分支的前14层，因为中间层被经验证具有更强的跨模态对齐能力。骨干网络输出融合多模态表征 <code>z_t</code>。</p>
<p><strong>2. 交叉调制扩散变换器</strong>：作为动作专家，基于流匹配范式预测连续控制动作。它采用纯由堆叠交叉注意力层构成的扩散变换器，不同于先前VLA模型中交替的自注意力和交叉注意力结构。具体地，通过在真实动作 <code>A_t</code> 和随机噪声 <code>ε</code> 之间线性插值生成含噪动作序列 <code>A_t^τ = τ A_t + (1-τ)ε</code>，其中插值权重 <code>τ</code> 采样自Beta分布并截断至[0.02, 0.98]。训练目标是学习一个依赖于时间的速度场 <code>v_θ</code>，驱动 <code>A_t^τ</code> 在给定多模态上下文 <code>z_t</code> 和机器人状态 <code>s_t</code> 的条件下趋向真实动作 <code>A_t</code>，损失函数为流匹配公式 <code>L^τ(θ) = E[||v_θ(A_t^τ, z_t, s_t) - u(A_t^τ|A_t)||^2]</code>。</p>
<p><strong>3. 集成模块</strong>：该模块用于在条件化扩散变换器之前，有效融合多模态和本体感知信息。具体做法是从视觉-语言骨干的第14层提取融合表征 <code>z_t</code>，然后将其与机器人状态 <code>s_t</code> 直接拼接，而非投影到共享嵌入空间。这个拼接后的特征作为动作专家中Transformer块的键值输入，为动作生成提供全局且信息保留的上下文。</p>
<p><strong>创新点</strong>：与现有方法相比，Evo-1的主要创新在于：1) 采用紧凑的原生多模态VLM作为骨干，大幅减少参数量；2) 动作专家采用纯交叉注意力层的DiT设计，提升了效率；3) 集成模块采用简单的拼接策略，保留了完整信息；4) 提出了关键的两阶段训练范式以保持语义对齐。</p>
<p><strong>两阶段训练程序</strong>：</p>
<ul>
<li><strong>阶段一：动作专家对齐</strong>：冻结整个视觉-语言骨干，仅训练动作专家和集成模块。这使得随机初始化的动作专家权重能在不将噪声梯度反向传播到预训练骨干的情况下，逐渐与多模态嵌入空间对齐。</li>
<li><strong>阶段二：全规模微调</strong>：在集成和动作模块充分对齐后，解冻VLM骨干，对整个架构进行全规模微调。这使得预训练的视觉-语言骨干和动作专家能够联合优化，实现更深度的集成并更好地适应多样化的操作任务。</li>
</ul>
<p>该训练策略有效保留了骨干网络的多模态语义空间。如图2所示，经过两阶段训练后，Evo-1使用的InternVL3-1B产生的注意力图保持了清晰的结构和语义一致的区域，而OpenVLA使用的Prismatic-7B则表现出明显的语义漂移和对齐退化。</p>
<p><img src="https://arxiv.org/html/2511.04555v2/pic/evo1_atten_boy.jpg" alt="注意力图对比1"><br><img src="https://arxiv.org/html/2511.04555v2/pic/evo1_atten_bus.jpg" alt="注意力图对比2"><br><img src="https://arxiv.org/html/2511.04555v2/pic/evo1_atten_dog.jpg" alt="注意力图对比3"></p>
<blockquote>
<p><strong>图2</strong>：训练后视觉-语言注意力图对比。(a) Evo-1 (InternVL3-1B) 产生空间一致且语义对齐的激活。(b) OpenVLA (Prismatic-7B) 的注意力图显示出退化的连贯性。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：Evo-1在三个仿真基准（Meta-World、LIBERO、RoboTwin）和真实世界任务上进行了评估。对比的基线方法包括：Diffusion Policy, TinyVLA, π₀, SmolVLA, OpenVLA, CoT-VLA, π₀-FAST, GR00T N1, ACT, RDT等。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>仿真基准</strong>：如表1所示，Evo-1在仅0.77B参数且无需机器人数据预训练的情况下，在三个基准上均取得领先或极具竞争力的性能。<ul>
<li><strong>Meta-World</strong>：平均成功率80.6%，超越了之前的SOTA SmolVLA (68.2%) 12.4个百分点，并在所有四个难度级别上均表现最佳。</li>
<li><strong>LIBERO</strong>：平均成功率94.8%，超过了π₀ (94.2%) 和 SmolVLA (88.8%)。</li>
<li><strong>RoboTwin</strong>：平均成功率37.8%，超越了之前的SOTA π₀ (30.9%) 6.9个百分点，在双手机器人操作任务上表现出色。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.04555v2/x2.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>表1</strong>：在Meta-World、LIBERO和RoboTwin上的仿真基准结果。Evo-1与代表性基线在三个广泛使用的仿真基准上进行比较。Params表示模型大小（单位：十亿）；Robo-Pretrain显示模型是否在机器人数据上预训练；加粗标记最佳结果，下划线标记次优结果。</p>
</blockquote>
<ol start="2">
<li><strong>真实世界实验</strong>：设计了四个操作任务（抓放易拉罐、倾倒泡沫、手递手交付、易拉罐堆叠）。如图4所示，Evo-1在四个任务上的平均成功率达到78%，显著优于SmolVLA (50%)、OpenVLA-OFT (55%) 和 π₀ (73%)。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.04555v2/x3.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图4</strong>：真实世界实验结果。四个真实世界评估任务的成功率（左侧四个子图）以及跨任务的整体平均成功率（最右侧子图）。</p>
</blockquote>
<ol start="3">
<li><strong>推理效率分析</strong>：如表2所示，Evo-1在RTX 4090d GPU上仅消耗2.3 GB显存，实现了16.4 Hz的最高推理频率，同时取得了78%的最高真实世界成功率，在效率与性能间取得了最佳平衡。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.04555v2/x4.png" alt="效率对比表"></p>
<blockquote>
<p><strong>表2</strong>：推理效率比较。在RTX 4090d GPU上比较模型大小、推理效率和真实世界性能。Params (B): 参数量（十亿）；GPU Mem.(GB): 推理期间平均内存使用量；Infer. Freq.(Hz): 平均推理频率；Success (%): 真实世界任务的整体成功率。</p>
</blockquote>
<ol start="4">
<li><strong>泛化实验</strong>：在“抓放易拉罐”任务中引入未见过的干扰物、背景色变化、目标位置变化和高度变化。如表3所示，Evo-1在所有干扰设置下均 consistently 优于SmolVLA，展现了更强的泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.04555v2/x5.png" alt="泛化结果表"></p>
<blockquote>
<p><strong>表3</strong>：泛化实验的成功率。在真实世界任务泛化实验中，比较SmolVLA和我们的方法在不同干扰条件下的成功率。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ul>
<li><strong>集成模块分析</strong>：论文比较了四种不同的集成策略设计（模块A-D）。如图6所示，Evo-1采用的策略（模块D，即拼接多模态表征 <code>z_t</code> 与机器人状态 <code>s_t</code> 作为动作专家交叉注意力块的键值）取得了最佳性能，证明了这种信息保留的全局上下文方式的有效性。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.04555v2/x6.png" alt="集成模块消融"></p>
<blockquote>
<p><strong>图6</strong>：集成模块的消融研究。评估四种不同的VLM与动作专家集成策略（模块A-D）对整体性能的影响。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个轻量高效的VLA架构Evo-1，仅含0.77B参数，降低了训练成本并提升了在消费级GPU上的实时部署推理速度；2) 引入了一种两阶段训练范式，在保持VLM固有多模态理解能力与适应下游动作生成之间取得了平衡，有效增强了模型在多样化操作任务上的泛化能力；3) 广泛的实验表明，Evo-1在不依赖大规模机器人数据预训练的情况下，在仿真和真实世界任务中均达到了最先进的性能，大幅减少了对昂贵数据收集的需求。</p>
<p>论文自身提到的局限性并未在正文中明确阐述，但从实验设计来看，其实验主要集中在桌面机械臂的短视距操作任务，对于更复杂的移动操作、长视距规划或动态环境中的任务，其性能有待进一步验证。</p>
<p>本文对后续研究的启示在于：证明了通过精心设计的紧凑架构和训练策略，轻量级VLA模型可以在保持高性能的同时实现高效率，这为在资源受限的边缘设备上部署强大的多模态机器人策略提供了可行路径。同时，保持预训练骨干网络的语义对齐是提升VLA模型泛化能力的关键，这一思想可被进一步推广和应用。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前视觉-语言-动作模型参数庞大、依赖机器人数据预训练、计算成本高且会损害视觉语言骨干网络语义表征的问题，提出了轻量级VLA模型Evo-1。其核心方法是基于原生多模态VLM，引入交叉调制扩散transformer与优化集成模块，并采用两阶段训练范式以保持语义对齐。实验表明，仅含7.7亿参数的Evo-1在多个基准测试中达到SOTA，在Meta-World和RoboTwin上分别超越之前最佳模型12.4%和6.9%，在真实世界评估中取得78%的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.04555" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>