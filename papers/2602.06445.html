<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.06445" target="_blank" rel="noreferrer">2602.06445</a></span>
        <span>作者: Huang, Weidong, Zhang, Jingwen, Li, Jiongye, Zhang, Shibowen, Wu, Jiayang, Wang, Jiayi, Liu, Hangxin, Yang, Yaodong, Su, Yao</span>
        <span>日期: 2026/02/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在仿人机器人领域，实现稳定且高能效的运动对于其在现实世界中的持续运行至关重要。当前主流方法主要依赖于模型预测控制（MPC）和强化学习（RL）。RL方法（如基于PPO的框架）通常将能量相关的指标（如最小化关节扭矩、加速度）作为奖励函数中的一项，与其他目标（如速度跟踪、稳定性）进行多目标优化。这种方法存在关键局限性：首先，奖励函数中各项的权重需要大量的超参数调优，这个过程不直观且耗时，因为权重配置的影响不直接，且评估单个系数选择可能需要数小时到数天的训练时间；其次，奖励函数内部相互冲突的目标可能导致次优解甚至收敛失败，例如，学习到的策略可能为了最小化能耗而牺牲稳定性，或者为了稳定性而大幅增加能耗。</p>
<p>本文针对上述“多目标奖励函数调优困难且易导致次优策略”的具体痛点，提出了一个新视角：将能量消耗从奖励函数中分离出来，重新表述为明确的、具有物理意义的不等式约束，从而形成一个约束强化学习（Constrained RL）框架。本文的核心思路是：提出一个名为ECO（Energy-Constrained Optimization）的框架，通过拉格朗日方法强制执行能量消耗和参考运动（对称性）约束，在保证稳定行走的同时，实现更高效、直观的能效优化。</p>
<h2 id="方法详解">方法详解</h2>
<p>ECO的整体框架是一个标准的基于actor-critic的约束RL训练与部署流程。策略网络（Actor）以速度指令和本体感知数据（包含连续K_f帧的历史观测）作为输入，输出期望的关节位置偏移量。这些动作在100Hz的频率下输出，随后馈入一个PD控制器，该控制器以1kHz的频率计算并施加关节扭矩。奖励评论员（Critic）使用包含特权信息（如身体线速度、外部扰动、摩擦系数等）的扩展状态进行训练。仿真器提供奖励值、能量成本和对称性成本，用于计算奖励回报和成本回报。策略则通过拉格朗日公式进行更新，以平衡奖励最大化和成本约束满足。训练后的策略可直接部署到真实机器人上。</p>
<p><img src="https://arxiv.org/html/2602.06445v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：ECO框架的训练与部署过程概述。策略网络接收速度指令和本体感知输入，输出期望关节位置至PD控制器。奖励评论员基于特权观测进行训练。仿真器计算奖励和成本，策略通过拉格朗日公式更新。</p>
</blockquote>
<p>核心模块在于约束的设计与优化方法的选择：</p>
<ol>
<li><p><strong>约束选择与设计</strong>：论文指出，由于人形机器人的可行收敛区域比四足机器人小得多，设置过多约束会过度限制搜索空间。因此，ECO聚焦于两个主要约束：</p>
<ul>
<li><strong>能量约束（C1）</strong>：以电机消耗的绝对功率积分作为成本函数，即 <code>C1 = Σ|τ_j · q̇_j|</code>，采用折扣累积和约束形式（公式5），确保长期能量消耗低于阈值 <code>b1</code>。阈值 <code>b1</code> 基于机器人的速度范围和行走时长通过物理上直观的线性搜索算法确定。</li>
<li><strong>参考运动约束（C2）</strong>：采用镜像损失（Mirror Loss）作为成本函数，以促进对称、自然的步态。该约束要求当输入状态被镜像时，策略输出的动作也应被镜像（公式16）。采用平均和约束形式（公式6），确保平均镜像误差低于阈值 <code>b2</code>。</li>
</ul>
</li>
<li><p><strong>奖励函数</strong>：由于能量和参考运动已被处理为约束，奖励函数得以简化，主要包含任务目标（如线速度和角速度跟踪）、稳定性奖励（如脚部接触力、身体姿态）等，具体权重和组成见论文表I。这避免了在奖励函数内部进行复杂的多目标权衡。</p>
</li>
<li><p><strong>优化方法</strong>：论文比较了四种约束RL算法：PPO-Lagrangian（PPO-Lag）、CRPO、IPO和P3O。通过实验验证，最终选择PPO-Lagrangian作为ECO的优化方法。其拉格朗日函数为 <code>L(θ, λ) = J^R(π_θ) - Σ λ_i (J^{C_i}(π_θ) - b_i)</code>，通过原始-对偶更新交替优化策略参数θ和拉格朗日乘子λ，从而求解带约束的优化问题。</p>
</li>
</ol>
<p>与现有方法相比，ECO的核心创新点在于：<strong>将能量消耗这一关键但调参困难的指标从多目标奖励函数中解耦，转变为具有明确物理含义和直观调参方式的硬约束</strong>。这使得优化目标（奖励）和必须满足的条件（约束）分离，降低了对奖励权重精细调优的依赖，并能更可靠地控制能耗水平。</p>
<h2 id="实验与结果">实验与结果</h2>
<ul>
<li><strong>实验平台/数据集</strong>：实验在仿真环境IsaacGym和真实世界中进行，使用的机器人平台是儿童尺寸的仿人机器人BRUCE。进行了sim-to-sim和sim-to-real的迁移验证。</li>
<li><strong>对比基线方法</strong>：ECO与以下方法进行了对比：1) 模型预测控制（MPC）；2) 标准RL（PPO）配合奖励塑形（即传统多目标优化方法）；3) 四种先进的约束RL方法：PPO-Lag、CRPO、IPO、P3O。</li>
<li><strong>关键实验结果</strong>：<ul>
<li><strong>能耗对比</strong>：ECO在能耗方面显著优于所有基线。在真实机器人测试中，ECO的能耗比MPC基线低约6倍，比标准PPO基线低约2.3倍。</li>
<li><strong>约束RL算法比较</strong>：在四种约束RL算法中，PPO-Lag在训练稳定性、约束满足以及最终达到的能耗水平上表现最佳，因此被选为ECO的优化器。</li>
<li><strong>收敛性与性能</strong>：ECO能够稳定收敛，并成功将能耗控制在设定阈值以下，同时保持良好的行走性能（如速度跟踪）。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2602.06445v1/x3.png" alt="能耗与性能对比"></p>
<blockquote>
<p><strong>图3</strong>：不同方法在真实BRUCE机器人上的能量消耗（左）和行走速度跟踪误差（右）对比。ECO在能耗上大幅领先，同时保持了可比的跟踪性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06445v1/x4.png" alt="训练曲线对比"></p>
<blockquote>
<p><strong>图4</strong>：ECO与标准PPO、PPO-Lag的训练曲线对比。ECO（PPO-Lag）能稳定地将能量成本控制在阈值（虚线）以下，而标准PPO无法做到。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06445v1/x5.png" alt="消融实验：约束的影响"></p>
<blockquote>
<p><strong>图5</strong>：消融实验展示不同约束设置的影响。“ECO (Ours)”代表同时使用能量和镜像约束，其能耗最低。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06445v1/x6.png" alt="不同约束RL算法性能"></p>
<blockquote>
<p><strong>图6</strong>：四种约束RL算法在训练过程中的奖励（上）和能量成本（下）曲线。PPO-Lag（红色）表现最稳定且能耗低。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06445v1/x7.png" alt="步态对称性分析"></p>
<blockquote>
<p><strong>图7</strong>：ECO策略诱导出的涌现行为：更轻的步态和减少的身体晃动，这有利于上身操作任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06445v1/x8.png" alt="sim-to-real 速度跟踪"></p>
<blockquote>
<p><strong>图8</strong>：ECO策略在真实机器人上进行sim-to-real部署时，对不同速度指令的跟踪情况。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06445v1/x9.png" alt="关节轨迹对比"></p>
<blockquote>
<p><strong>图9</strong>：ECO与基线方法在左髋关节滚动角度轨迹上的对比，展示了不同的运动模式。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06445v1/x10.png" alt="能量约束阈值调优"></p>
<blockquote>
<p><strong>图10</strong>：通过线性搜索调整能量约束阈值 <code>b1</code> 的过程，展示了物理直观的调参方式。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06445v1/x11.png" alt="多约束设置实验"></p>
<blockquote>
<p><strong>图11</strong>：测试更多约束（如关节速度、接触力）时的训练情况，表明约束过多可能导致人形机器人训练不稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06445v1/x12.png" alt="运动学效率分析"></p>
<blockquote>
<p><strong>图12</strong>：ECO策略通过更充分的膝关节伸展（右图）来提高运动效率，与基线（左图）对比明显。</p>
</blockquote>
<ul>
<li><strong>消融实验总结</strong>：<ul>
<li><strong>能量约束</strong>：是降低能耗的关键组件。将其从约束改为奖励中的惩罚项（即标准PPO），即使调整权重，也难以稳定地将能耗降至阈值以下。</li>
<li><strong>镜像约束</strong>：有助于产生更对称、稳定的步态，与能量约束结合能实现最低的能耗。</li>
<li><strong>约束数量</strong>：实验表明，为人形机器人设置过多约束（如额外添加关节速度、接触力约束）会使得训练难以稳定收敛，验证了论文关于人形机器人可行域较小的观点。</li>
</ul>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了ECO框架</strong>：创新性地将仿人机器人行走的能量优化问题重构为约束RL问题，把能量消耗作为具有明确物理含义的不等式约束，从而提供了一种高效、直观的超参数调优方法。</li>
<li><strong>验证了PPO-Lagrangian方法的有效性</strong>：通过系统比较，确定了PPO-Lagrangian是适用于人形机器人能效约束优化问题的有效算法，并诱导出如充分伸膝、轻踏步态等提升能效的涌现行为。</li>
<li><strong>实现了sim-to-real的能效行走</strong>：在真实的仿人机器人BRUCE上验证了ECO的有效性，显著降低了能耗，为能效仿人 locomotion 设立了新的基准。</li>
</ol>
<p>论文自身提到的局限性主要在于：由于人形机器人动力学复杂，其满足多重约束的可行策略空间（可行域）比四足机器人更小。因此，像在四足机器人上那样将大量奖励项转化为约束的方法，可能使人形机器人的训练难以收敛。ECO目前只聚焦于能量和对称性两个核心约束。</p>
<p>本文对后续研究的启示在于：首先，它展示了约束RL在解决人形机器人特定、物理意义明确的目标（如能耗上限）上的巨大潜力。其次，约束的设计需要与机器人的形态和任务特性紧密结合，过度的约束可能会适得其反。最后，ECO中能量约束的阈值调优方式（基于物理指标的线性搜索）为其他具有明确物理量的约束设计提供了参考范式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ECO框架，解决双足机器人在保持稳定行走的同时实现高能效的核心问题。该方法采用约束强化学习，将能耗指标从奖励函数中分离，重新表述为明确的不等式约束，并通过拉格朗日方法强制执行能耗与参考运动约束，使能耗具有清晰物理意义且便于调参。在kid-sized机器人BRUCE上的实验表明，ECO相比模型预测控制、标准强化学习及多种先进约束RL方法，能显著降低能耗，同时维持稳健的行走性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.06445" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>