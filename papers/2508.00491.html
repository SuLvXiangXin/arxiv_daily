<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.00491" target="_blank" rel="noreferrer">2508.00491</a></span>
        <span>作者: Lorenzo Natale Team</span>
        <span>日期: 2025-08-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前商业假肢主要利用肌电（EMG）或肌机（MMG）信号解码用户意图，但通常只能同时控制一个关节，随着自由度增加，用户认知负荷高，导致设备弃用。为解决此问题，共享自主权框架引入视觉等额外输入模态，通过半自主系统执行抓取的部分阶段以减轻用户负担。然而，现有视觉方法通常依赖手动标注的数据（如分割掩码、抓取位姿），这在复杂、非结构化的场景中（如人-假手交接）难以获取和定义。模仿学习（IL）在机器人操作领域已展现出从示教中学习复杂任务的潜力，但其在假肢控制中的应用尚待探索。</p>
<p>本文针对假肢在非结构化环境中实现灵巧、自主抓取的痛点，提出将模仿学习，特别是扩散策略（Diffusion Policy, DP），应用于假肢控制的新视角。核心思路是：通过收集包含视觉和本体感觉的示教数据，训练一个统一的扩散策略模型，使其能够根据手内摄像头图像和关节编码器读数，直接预测控制手腕朝向和手指闭合的动作序列，从而实现从演示中学习的端到端抓取控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架（HannesImitationPolicy）是一个基于扩散策略的仿生假手控制流程。输入是来自假手手内摄像头的RGB图像以及手开合、手腕屈/伸编码器的测量值，输出是未来八步的动作序列，用于控制手开合、手腕屈/伸（位置控制）和手腕旋前/旋后（速度控制）。为平衡长时程规划与假肢控制所需的快速反应，每次执行四步动作后重新规划。</p>
<p><img src="https://arxiv.org/html/2508.00491v1/x2.png" alt="控制架构"></p>
<blockquote>
<p><strong>图2</strong>：HannesImitationPolicy 的控制架构。观测（手内摄像头图像和编码器读数）经过特征提取后拼接，输入到噪声预测网络（1D时序卷积U-Net）。该网络通过扩散过程的去噪迭代，生成未来八步的动作序列（控制三个自由度），每次执行前四步。</p>
</blockquote>
<p>核心模块包括数据收集、策略网络架构及训练推理流程：</p>
<ol>
<li><strong>HannesImitationDataset</strong>：为支持策略学习，本文创建了首个用于假肢模仿学习的数据集。包含三种非结构化场景（桌面抓取、货架抓取、人-假手交接）下，对15个YCB物体的450条示教轨迹。每条轨迹记录了摄像头图像、编码器读数以及对应的三个自由度的控制动作。</li>
<li><strong>观测编码</strong>：图像特征通过一个从头训练的ResNet-18提取，然后与当前时刻的编码器测量值（手开合、手腕屈/伸位置）拼接，形成策略的输入观测。</li>
<li><strong>策略网络与训练</strong>：采用DP方法，策略被表述为去噪扩散概率模型（DDPM）。噪声预测网络采用1D时序卷积U-Net，但为优化计算和推理时间，对其进行了精简：使用两层卷积（通道数32和64），核大小为3，并将扩散步长嵌入维度降至32。训练时，对真实动作序列添加随机高斯噪声，然后训练网络预测所添加的噪声，损失函数为均方误差（MSE）。</li>
<li><strong>策略推理</strong>：推理是一个迭代去噪过程。首先从标准高斯分布采样初始动作，然后利用训练好的噪声预测网络进行多步（本文设为10步）去噪，得到最终的动作序列预测。</li>
</ol>
<p>与现有方法相比，创新点在于：首次将扩散策略应用于假肢控制，实现了从原始视觉和本体感觉观测到多自由度连续动作的端到端映射；通过收集涵盖多种场景的示教数据集，使单一策略能够适应不同的抓取任务和环境；该方法避免了传统方法中对手动标注（如分割）的依赖，直接从演示中学习抓取策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台为Hannes假手（3自由度：手腕屈/伸、旋前/旋后、手指开合），配备手内摄像头。使用自建的HannesImitationDataset进行训练和验证。对比的Baseline方法是基于分割的视觉伺服手腕控制器（引用自[19]）。</p>
<p><strong>关键实验结果：</strong></p>
<ol>
<li><strong>离线验证</strong>：在验证集上评估预测动作与真实动作的绝对误差。结果显示误差主要集中在低值区域，且在不同场景和动作间趋势相似，表明策略已学会在不同条件下执行任务。手腕旋前/旋后（Wrist P/S）的误差略高，可能因该关节无位置编码器。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.00491v1/x3.png" alt="动作误差分布"></p>
<blockquote>
<p><strong>图3</strong>：在验证集上，三个任务中各自由度预测动作的绝对误差分布。误差已根据动作范围归一化。结果显示策略在不同条件下均能有效学习，Wrist P/S误差稍高。</p>
</blockquote>
<ol start="2">
<li><strong>在线部署测试</strong>：在真实假手上对15个已知物体进行测试（每物体每场景10次试验）。总体成功率为79.3%。具体而言，桌面抓取（#1）成功率为80.6%，货架抓取（#2）为68%，人-假手交接（#3）为89.3%。货架场景成功率较低归因于视觉视角和物体摆放更具挑战性。人-假手交接场景成功率最高，得益于用户与交接者之间的自然协作。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.00491v1/x4.png" alt="桌面抓取示例"></p>
<blockquote>
<p><strong>图4</strong>：桌面抓取任务（#1）的定性结果。展示了外部视角、手内摄像头观测、编码器读数以及执行的动作序列。策略能够协调手腕旋转、屈伸和手指闭合。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.00491v1/x5.png" alt="货架抓取示例"></p>
<blockquote>
<p><strong>图5</strong>：货架抓取任务（#2）的定性结果。策略成功使手腕旋后以对齐手柄，并在旋转未完全结束时开始闭合手指，展现出类人的抓取行为。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.00491v1/x6.png" alt="人-假手交接示例"></p>
<blockquote>
<p><strong>图6</strong>：人-假手交接任务（#3）的定性结果。在非结构化背景下，策略能够通过轻微的手腕旋转和手闭合成功抓取物体。</p>
</blockquote>
<ol start="3">
<li><p><strong>泛化至未见物体</strong>：对5个训练中未出现的YCB物体进行测试，无需微调。整体成功率为76%（桌面68%，货架74%，交接86%），与在已知物体上的性能相当，证明了策略良好的泛化能力。</p>
</li>
<li><p><strong>与视觉伺服对比</strong>：与基于分割的视觉伺服手腕控制器[19]相比，HannesImitationPolicy在非结构化的<strong>人-假手交接</strong>场景中优势显著（成功率86% vs 16%），平均性能提升13.8%。尽管在结构化的桌面和货架场景中视觉伺服略优或相当，但本文方法在更复杂、非预定义的环境中展现了更强的鲁棒性和适应性。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有两点：1) 提出了HannesImitationPolicy，一种基于扩散策略的模仿学习方法，首次将其应用于假肢控制，实现了从视觉和本体感觉到多自由度动作的端到端抓取策略学习；2) 创建并开源了HannesImitationDataset，这是首个用于假肢模仿学习的抓取示教数据集，包含多种非结构化场景。</p>
<p>论文提到的局限性包括：当前系统无法自主移动，需要用户将假手引导至目标物体附近；策略仅控制手腕和手，未集成手臂运动；实验场景虽多样，但物体集合和背景仍有限。</p>
<p>本工作对后续研究的启示在于：证明了模仿学习，特别是扩散模型，在假肢控制这一对实时性、鲁棒性要求极高的领域具有应用潜力。未来方向可能包括：集成手臂控制以实现完全自主的取放任务；结合肌电信号实现更自然的混合控制；在更广泛、动态的真实世界环境中收集数据以提升泛化能力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究模仿学习在假肢手控制中的应用，旨在解决传统肌电控制自由度有限、认知负荷高的问题。提出HannesImitationPolicy方法，基于扩散策略（Diffusion Policy），利用单一眼内摄像头数据，训练统一的抓取策略以预测手腕方向与手指闭合。实验表明，该方法在多种物体与非结构化场景中均能成功抓取，并在非结构化环境下性能优于基于分割的视觉伺服控制器。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.00491" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>