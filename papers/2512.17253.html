<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Mitty: Diffusion-based Human-to-Robot Video Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Mitty: Diffusion-based Human-to-Robot Video Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.17253" target="_blank" rel="noreferrer">2512.17253</a></span>
        <span>作者: Mike Zheng Shou Team</span>
        <span>日期: 2025-12-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，从人类演示视频中学习机器人技能的主流方法依赖于关键点、轨迹或深度图等中间表示。这些方法首先从人类视频中提取中间表示，然后驱动渲染模块合成机器人视频。然而，这种方法无法充分利用演示视频中蕴含的丰富信息，难以捕捉对鲁棒泛化至关重要的细粒度时空动态，并且中间估计阶段的累积误差会进一步降低性能。因此，本文旨在解决一个核心痛点：能否绕过中间表示，直接实现端到端的人类到机器人（Human2Robot， H2R）视频生成？这面临三大挑战：外观与场景一致性、动作与策略对齐，以及高质量人类-机器人配对视频数据的稀缺。</p>
<p>本文提出了Mitty，一个基于扩散Transformer（DiT）的视频上下文学习（In-Context Learning）框架。其核心思路是：以预训练的大规模视频扩散模型（Wan 2.2）为基础，通过上下文学习范式，将人类演示视频压缩为条件令牌，并在扩散过程中通过双向注意力机制与机器人去噪令牌融合，从而直接、端到端地生成对应的机器人执行视频，无需动作标签或中间抽象表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架基于预训练的Wan 2.2视频扩散模型，采用上下文学习范式。输入是成对的人类演示视频 $V^H$ 和机器人执行视频 $V^R$，目标是建模条件分布 $p_\theta(V^R|V^H)$。框架支持两种设定：H2R（零帧生成，仅以人类视频为条件）和HI2R（首帧条件生成，额外提供机器人初始帧以定义初始状态）。在训练时，人类视频通过VAE编码为干净的条件令牌，机器人视频编码后则被注入噪声作为去噪目标。这两组令牌沿时间维度拼接后，输入到配备了双向注意力机制的扩散Transformer中，实现跨模态信息流。</p>
<p><img src="https://arxiv.org/html/2512.17253v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Mitty的整体架构。构建于基于扩散Transformer的视频生成模型之上，采用上下文学习范式。人类演示视频（输入）和带噪声的机器人视频潜变量（去噪流）被拼接，噪声仅注入机器人分支。双向注意力机制实现了跨模态信息流动，使模型能够直接从人类操作演示中学习生成机器人视频。</p>
</blockquote>
<p>核心模块是<strong>视频上下文学习与双向注意力耦合</strong>。具体而言，人类条件令牌 $\mathbf{C}$ 和机器人去噪令牌 $\mathbf{D}$ 在添加了时间嵌入和模态嵌入后，通过双向注意力进行信息交换（公式4）。在每个Transformer层，通过计算交叉注意力权重，人类令牌从机器人令牌获取信息（$\tilde{\mathbf{C}}$），机器人令牌也从人类令牌获取信息（$\tilde{\mathbf{D}}$）。更新后的令牌 $[\tilde{\mathbf{C}}; \tilde{\mathbf{D}}]$ 被拼接并送入后续块。网络在机器人分支上预测噪声 $\epsilon_\theta(\mathbf{x}^R_t, \mathbf{C}, t)$，并按照标准扩散过程进行反向更新（公式5），最终通过VAE解码得到生成的机器人视频 $\hat{\mathbf{V}}^R$（公式6）。</p>
<p>与现有方法相比，Mitty的主要创新点在于：1）<strong>端到端映射</strong>：摒弃了关键点、轨迹等中间表示，直接学习从人类视频到机器人视频的映射，避免了信息损失和误差累积。2）<strong>上下文学习范式</strong>：利用预训练大模型的强视觉-时序先验，通过简单的条件输入（人类视频）实现对新任务的快速适应，无需针对每个任务重新训练。3）<strong>统一的双向注意力设计</strong>：通过一个统一的架构同时支持零帧生成和首帧条件生成，实现了细粒度的控制。</p>
<p><img src="https://arxiv.org/html/2512.17253v1/x3.png" alt="数据合成流程"></p>
<blockquote>
<p><strong>图3</strong>：自动合成配对数据的流程。从人类演示视频开始，依次进行手部检测与分割、手部关键点检测、视频修复以获得干净背景、将手部关键点映射到机器人末端执行器位姿，最后将机器人手臂渲染到视频中。经过人工筛选，最终构建了高质量的人类-机器人配对视频数据集。</p>
</blockquote>
<p>为了缓解配对数据稀缺问题，本文还设计了一个<strong>自动化的数据合成流程</strong>（图3）。该流程以大规模自我中心（egocentric）人类视频（如EPIC-Kitchens）为输入，通过手部网格恢复（HaMeR）、检测分割（Detectron2, SAM2）、视频修复（E2FGVI）、位姿映射和机器人渲染（RobotSuite）等一系列步骤，合成出机器人手臂执行视频。通过人工参与（human-in-the-loop）的筛选机制，最终构建了高质量的训练数据，为模型提供了强监督信号。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在<strong>Human2Robot（H2R）</strong> 和<strong>EPIC-Kitchens</strong>两个基准数据集上进行。H2R数据集包含11,788个剪辑，EPIC-Kitchens通过合成流程得到34,820个剪辑，并划分为“已见”和“未见”场景以测试跨环境泛化能力。对比的基线方法分为两类：1）基于Wan 2.2家族和上下文学习设置构建的模型（包括不同规模模型和消融变体）；2）通用视频编辑方法（Aleph, Kling, MoCha）。评估指标包括视频质量指标（FVD, PSNR, SSIM, MSE）、任务成功率（SR）、人类偏好和具身一致性。</p>
<p><img src="https://arxiv.org/html/2512.17253v1/x4.png" alt="生成结果示例"></p>
<blockquote>
<p><strong>图4</strong>：Mitty在Human2Robot和EPIC-Kitchens数据集上的生成结果。每组结果中，第一行是人类演示视频，第二行是我们方法（零帧生成）的输出，第三行是真实的机器人执行视频。结果显示Mitty能准确保持场景布局和物体交互，并产生平滑、时序一致的机器人动作。</p>
</blockquote>
<p>关键实验结果如下：在Human2Robot数据集上，性能最佳的T2V 14B模型取得了FVD 6.48， PSNR 22.7， SSIM 0.851， MSE 0.0069， 任务成功率（SR）93%。在更复杂的EPIC-Kitchens数据集上，T2V 14B在“已见”和“未见”场景的SR分别为90%和89%。与基线对比（表3）显示，Mitty取得了最高的任务成功率（84.5%）和人类偏好（68.0%），以及第二高的具身一致性（92.6%），在正确性、视觉保真度和结构稳定性之间取得了最佳平衡。</p>
<p><img src="https://arxiv.org/html/2512.17253v1/x5.png" alt="与Masquerade数据流程对比"></p>
<blockquote>
<p><strong>图5</strong>：与Masquerade多阶段合成流程的定性对比。Masquerade的流程（红色高亮部分）容易出现复合错误（如关键点检测、修复和渲染失败）。相比之下，我们利用其合成数据经过筛选后训练的端到端模型能产生更可靠的Human2Robot映射。</p>
</blockquote>
<p>消融研究（表2）表明：1）<strong>人类参考视频至关重要</strong>：移除人类视频（w/o ref vid.）导致所有指标显著下降，在H2R上SR从91%跌至65%，在EPIC-Kitchens（已见）上从88%跌至75%。2）<strong>任务描述作用较小</strong>：移除文本描述（w/o task desc.）仅引起轻微变化，说明Mitty更依赖视觉演示。3）<strong>分数据集训练优于混合训练</strong>：由于两个数据集在任务和环境上差异较大，针对每个数据集单独训练（Sep. train.）的模型性能普遍优于混合训练（Mixed train.）的模型。</p>
<p><img src="https://arxiv.org/html/2512.17253v1/x6.png" alt="与视频编辑方法对比"></p>
<blockquote>
<p><strong>图6</strong>：与通用视频编辑方法的定性对比。这些方法仅使用单张机器人手臂参考图像进行替换，导致生成的手臂出现变形、结构错误和扭曲。而Mitty基于配对数据训练，能持续保持正确的外观和结构。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了<strong>第一个基于视频扩散Transformer的端到端Human2Robot视频生成框架Mitty</strong>，绕过了易错的中间表示。2）技术上，创新地采用了<strong>视频上下文学习与双向注意力机制</strong>，实现了外观、场景与动作的一致性，显著提升了跨任务泛化能力。3）设计了一套<strong>高效的自动数据合成与筛选策略</strong>，结合现有数据集进行训练，大幅增强了模型在未知任务和环境上的泛化能力。</p>
<p>论文自身提到的局限性在于：Mitty目前主要是一个视频生成模型，<strong>尚不能直接输出可执行的机器人控制信号</strong>，未形成完整的“视频到策略”闭环。这为后续研究指明了方向：如何将生成的高保真、时序对齐的机器人视频可靠地反转为控制指令，是实现从人类观察到机器人执行的端到端映射的关键下一步。此外，这项工作也证实了利用大规模预训练视频生成模型所蕴含的强时空先验，以及通过自动化流程扩展机器人学习数据的可行性，为 scalable 的机器人学习提供了新的见解。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对从人类示范视频直接生成机器人执行视频的核心问题，旨在避免依赖关键点等中间表示导致的信息丢失与累积错误。提出Mitty方法，基于扩散变换器的视频上下文学习技术，利用预训练视频扩散模型，将人类视频压缩为条件令牌，通过双向注意力与机器人去噪令牌融合，实现端到端生成；并开发自动合成管道从自我中心数据生成高质量配对数据以缓解数据稀缺。实验在Human2Robot和EPIC-Kitchens数据集上取得最先进结果，展现出对未见环境的强泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.17253" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>