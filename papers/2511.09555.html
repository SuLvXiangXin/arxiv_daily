<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.09555" target="_blank" rel="noreferrer">2511.09555</a></span>
        <span>作者: Gao Huang Team</span>
        <span>日期: 2025-11-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域的空间表示方法主要分为两类。基于点云的方法（如PointNet++）显式表示3D几何，但存在稀疏采样导致细粒度语义丢失的问题，且3D标注成本高限制了预训练规模。基于图像的多视角RGB-D方法（如RVT系列）则将语义和几何在共享特征空间中联合建模，虽能获得密集语义并受益于强大的2D预训练先验，但其纠缠的表示对现实世界中固有的深度噪声（如传感器噪声、光照变化、表面反射）非常敏感，这会破坏语义和几何理解。例如，轻微的噪声即可导致RVT-2性能下降8.9%。此外，现有方法主要保留高层几何信息，而忽视了对于通过提供细粒度2D-3D对应关系来实现精确交互至关重要的低层空间线索。</p>
<p>本文针对现有方法对深度噪声敏感且缺乏精确空间对应关系的痛点，提出了解耦表示的新视角。核心思路是：1）将语义与几何显式解耦，以减轻跨模态干扰；2）将几何信息进一步分解为互补的高层几何表示和低层空间线索，从而构建一个鲁棒且精确的空间表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架接受多视角RGB图像、深度图、机器人本体感知状态和语言指令作为输入，最终输出机械臂末端执行器的3D位姿和夹爪状态。</p>
<p><img src="https://arxiv.org/html/2511.09555v2/x2.png" alt="整体框架"></p>
<blockquote>
<p><strong>图2</strong>：SpatialActor的整体框架。采用分离的视觉和深度编码器。语义引导的几何模块（SGM）通过门控融合自适应地融合来自预训练深度专家的鲁棒但粗略的几何先验与噪声深度特征，得到高层几何表示。在空间变换器（SPT）中，低层空间线索被编码为位置嵌入以驱动空间交互。最后，视角级交互精化视角内特征，场景级交互整合跨视角的跨模态信息以支持后续的动作头。</p>
</blockquote>
<p>核心模块包括语义引导的几何模块（SGM）和空间变换器（SPT）。SGM旨在构建鲁棒的高层几何表示。具体而言，分别从RGB图像通过一个冻结的大规模预训练深度估计专家（如Depth Anything v2）提取鲁棒但粗略的几何先验，以及从原始深度图通过深度编码器提取精细但带噪声的几何特征。两者通过一个多尺度门控机制进行自适应融合：首先将两个特征拼接后通过MLP和Sigmoid激活函数生成门控图，然后使用该门控图对两个特征进行加权求和。这使得模型能够学习保留可靠的深度细节同时抑制噪声。</p>
<p><img src="https://arxiv.org/html/2511.09555v2/x3.png" alt="核心模块"></p>
<blockquote>
<p><strong>图3</strong>：语义引导的几何模块（SGM）和空间变换器（SPT）。(a) SGM通过门控机制自适应地融合两个互补的几何表示。(b) SPT利用相机参数和深度值将3D点转换为使用RoPE的空间位置嵌入，以建立2D-3D对应关系，随后进行视角级和场景级交互以精化空间令牌。</p>
</blockquote>
<p>SPT则负责利用低层空间线索实现精确的2D-3D映射并促进空间特征交互。首先，利用相机内参、外参和深度值，通过透视投影将每个像素映射到机器人坐标系下的3D坐标。然后，采用旋转位置编码（RoPE）为每个3D坐标生成独特的空间位置嵌入，并将其注入到特征中。SPT包含两个交互阶段：视角级自注意力精化同一视角内令牌的关系；随后将所有视角的令牌与语言特征拼接，进行场景级自注意力以整合跨视角和跨模态的上下文信息，形成最终的场景表示。动作头通过一个轻量级解码器（ConvexUp）生成每视角2D热图，通过argmax获得目标2D位置后利用相机模型反投影至3D，再通过MLP回归旋转和夹爪状态。损失函数包括2D热图的交叉熵损失、离散化欧拉角的交叉熵损失和夹爪状态的二分类损失。</p>
<p>创新点具体体现在：1）<strong>表示解耦</strong>：区别于现有方法在共享特征空间联合建模，显式分离语义与几何表示；2）<strong>互补几何融合</strong>：通过SGM自适应融合来自深度专家的鲁棒先验与原始深度的精细细节，兼顾鲁棒性与精确性；3）<strong>空间感知的Transformer</strong>：通过SPT引入基于3D坐标的低层空间位置编码，使Transformer具备明确的几何感知和交互能力，实现精确的2D-3D映射。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真和真实世界中进行，使用了多个基准测试：RLBench（18个任务，249个变体）、ColosseumBench（评估空间扰动的20个任务）以及真实机器人平台（WidowX机械臂，8个任务15个变体）。对比的基线方法包括C2F-ARM-BC、PerAct、RVT、RVT-2、SAM-E、3D Diffuser Actor等。</p>
<p>在RLBench上的关键结果表明，SpatialActor达到了87.4%的平均成功率，超越了之前的SOTA方法约6.0%。在需要高空间精度的任务上提升尤为显著，例如在“Insert Peg”任务上达到93.3%（比RVT-2高53.3%），在“Sort Shape”任务上达到73.3%（比RVT-2高38.3%）。</p>
<p><img src="https://arxiv.org/html/2511.09555v2/x5.png" alt="RLBench结果"></p>
<blockquote>
<p><strong>图5/表1</strong>：在RLBench 18个任务上的性能对比。SpatialActor取得了最高的整体性能，并在“Insert Peg”等高精度空间任务上优势明显。</p>
</blockquote>
<p>在噪声鲁棒性测试中，模拟了轻（20%点云噪声，标准差0.05）、中（50%，0.1）、重（80%，0.1）三种噪声水平。SpatialActor相比RVT-2，平均成功率分别提升了13.9%、16.9%和19.4%。在“Insert Peg”任务上，提升幅度分别达到88.0%、78.6%和61.3%。</p>
<p><img src="https://arxiv.org/html/2511.09555v2/x6.png" alt="噪声鲁棒性结果"></p>
<blockquote>
<p><strong>图6/表2</strong>：在不同噪声水平下的性能。SpatialActor在所有噪声条件下均显著优于RVT-2，展示了强大的鲁棒性。</p>
</blockquote>
<p>在小样本泛化实验中，使用多任务预训练模型，仅用每个新任务10个演示（原数据的1/10）进行微调。SpatialActor在19个新任务上平均成功率达到79.2%，远超RVT-2的46.9%，提升约32.3%。</p>
<p><img src="https://arxiv.org/html/2511.09555v2/x7.png" alt="小样本结果"></p>
<blockquote>
<p><strong>图7/表3</strong>：小样本泛化性能。SpatialActor仅用极少演示便能有效适应新任务，泛化能力显著更强。</p>
</blockquote>
<p>在ColosseumBench空间扰动测试中，SpatialActor在无扰动、操作物体尺寸变化、接收物体尺寸变化和相机位姿扰动四种条件下，平均成功率分别为57.4%、59.2%、62.0%和54.2%，均优于其他对比方法。</p>
<p><img src="https://arxiv.org/html/2511.09555v2/x8.png" alt="空间扰动结果"></p>
<blockquote>
<p><strong>图8/表4</strong>：在ColosseumBench空间扰动下的性能。SpatialActor在不同类型的空间变化下均表现稳健。</p>
</blockquote>
<p>消融实验（论文中Tab.5，对应图片可能未直接列出所有数值表）总结了各组件贡献：1）<strong>解耦语义与几何</strong>：在无噪声和重噪声下，成功率分别从基线81.4%和57.0%提升至85.1%和68.7%；2）<strong>引入SGM</strong>：进一步将重噪声下性能提升至73.9%；3）<strong>引入SPT</strong>：最终将无噪声和重噪声下性能推至87.4%和76.4%。这验证了每个模块的有效性。</p>
<p><img src="https://arxiv.org/html/2511.09555v2/x4.png" alt="真实世界任务"></p>
<blockquote>
<p><strong>图4</strong>：真实世界实验的8个任务场景。SpatialActor在这些任务上表现优于RVT-2，验证了其实际部署的有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一个<strong>解耦的表示学习框架</strong>，将语义、高层几何和低层空间线索分离，以应对噪声并提升空间理解；2）设计了<strong>语义引导的几何模块</strong>，通过门控自适应融合来自深度专家的鲁棒先验和原始深度细节，实现了对噪声鲁棒且精确的几何表示；3）引入了<strong>空间变换器</strong>，利用基于3D坐标的旋转位置编码，为Transformer注入低层空间感知能力，实现了精确的2D-3D映射和空间令牌交互。</p>
<p>论文自身提到的局限性包括：方法性能在一定程度上依赖于大规模预训练模型（如CLIP、Depth Anything）提供的先验知识；此外，空间变换器中复杂的空间编码和交互可能带来一定的计算开销。</p>
<p>本工作对后续研究的启示在于：在机器人感知与操作中，<strong>解耦不同属性的表示</strong>（如语义、几何、空间）可能比联合建模更具优势，尤其是在面对真实世界不确定性和需要高精度控制时。同时，<strong>利用大规模预训练专家模型提供互补的鲁棒先验</strong>，并与原始传感器数据进行自适应融合，是一条提升系统鲁棒性的有效路径。如何设计更高效、轻量的空间感知架构，以平衡性能与计算成本，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中现有视觉方法对深度噪声敏感、空间线索利用不足的问题，提出SpatialActor框架。其核心是通过**解耦表示**，分离语义与几何。关键技术包括：**语义引导的几何模块**，融合噪声深度与专家先验的互补几何信息；**空间变换器**，利用低层空间线索实现精确的2D-3D映射。在超过50个任务的实验中，该方法在RLBench上达到87.4%的SOTA性能，并在不同噪声条件下性能提升13.9%至19.4%，证明了其强鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.09555" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>