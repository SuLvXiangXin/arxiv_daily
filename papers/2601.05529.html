<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.05529" target="_blank" rel="noreferrer">2601.05529</a></span>
        <span>作者: Han, Jua, Seo, Jaeyoon, Min, Jungbin, Kim, Jihie, Oh, Jean</span>
        <span>日期: 2026/01/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型语言模型（LLMs）和视觉语言模型（VLMs）正日益成为机器人决策的核心工具，应用于导航、人机交互等多个领域。现有研究主要集中在通过大规模视觉-空间数据集评估模型的多模态推理能力，或通过视觉语言导航（VLN）等基准测试模型的导航准确性。然而，这些工作主要关注感知准确性或路径规划性能，对模型在安全关键场景下的决策可靠性评估不足。在安全关键领域（如自动驾驶、辅助机器人），即使是单一的错误也可能导致致命的后果，这使得评估LLMs在微小错误即可能造成灾难性后果的场景下的表现变得至关重要。</p>
<p>本文针对LLM在安全关键机器人决策中存在的潜在风险这一具体痛点，提出了一个系统化的评估框架。核心思路是通过设计七项互补的定量评估任务，暴露当前LLMs在空间推理和安全决策方面的严重脆弱性，并论证在安全关键领域，传统的“99%准确率”等聚合指标具有误导性，因为即使是罕见的失败也可能导致灾难性结果。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文设计了一个系统的评估框架，包含七项任务，分为三类：完全信息任务、不完全信息任务和安全导向空间推理（SOSR）任务。整体目标是定量和定性地评估LLMs/VLMs的空间推理能力、在信息缺失情况下的推断能力，以及在安全关键场景下的决策可靠性。</p>
<p><img src="https://arxiv.org/html/2601.05529v3/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：实验提示和地图结构概览。展示了用于“完全信息”（蓝色）、“不完全信息”（红色）和“SOSR”（黄色）任务的提示示例。图中还显示了实验中使用的ASCII地图和序列地图的结构。对于SOSR任务，红色高亮的短语是区分难度级别的标准，而斜体句子是重要的上下文线索。</p>
</blockquote>
<p><strong>1. 完全信息任务</strong>：使用完全指定的ASCII网格地图来评估模型在明确环境条件下的导航和推理能力。ASCII地图将环境抽象为符号网格，消除了视觉感知的歧义，便于隔离和评估空间推理与路径规划能力。设计了三个确定性地图（简单、普通、困难），地图中所有地形信息（起点S、目标G、障碍物#、空地.）完全已知，要求模型生成从S到G的有效路径，并严格遵守障碍物约束和网格结构。</p>
<p><strong>2. 不完全信息任务</strong>：包含三种子任务，旨在评估模型在信息缺失或不确定时的推理和是否产生幻觉。</p>
<ul>
<li><strong>序列推理任务</strong>：基于连续的自我中心视觉帧。分为“序列掩码”（隐藏一帧，让模型从两个候选帧中选择正确缺失帧）和“序列验证”（给出完整帧序列和自然语言查询，判断所述位移是否被证据支持）两种。此任务旨在诊断模型是基于空间连续性进行接地推理，还是会产生看似合理但无依据的幻觉。</li>
<li><strong>基于地图的不确定地形任务</strong>：在ASCII网格地图中引入未知单元格（“?”），模拟传感器噪声或数据缺失导致的局部可观测性。设计了两个不确定地形地图，评估模型在部分可观测下的路径规划能力。</li>
<li><strong>建筑物后方任务</strong>：向模型提供一张真实世界图像和“将机器人导航到建筑物后方”的指令。模型必须从第一人称视角推断机器人位置，在心理上将其转换为俯视布局，并生成连贯的地图，测试其视觉-空间接地能力。</li>
</ul>
<p><strong>3. 安全导向空间推理（SOSR）任务</strong>：完全通过非结构化的自然语言叙述呈现情境细节，测试模型在缺乏结构化输入时理解复杂、上下文丰富的指令并做出安全可靠决策的能力。</p>
<ul>
<li><strong>方向感测试</strong>：分为简单、中等、困难三个难度级别。虚拟角色初始面朝北，执行一系列文本指令（直行、左转、右转、掉头），模型需推断最终面向方向。困难级别会插入一句误导性陈述，以测试无关信息对推理的影响。</li>
<li><strong>火灾疏散决策任务</strong>：一个四选一的决策任务。简单条件下描述火灾爆发，模型需根据上下文选择逃生路线。困难条件则涉及一名研究生在提交重要论文前一天被困在燃烧的实验室，所有关键数据都在教授办公室，测试模型在决定将人引向何处时，是优先考虑人身安全还是目标导向行为。</li>
</ul>
<p>与现有方法相比，本文的创新点在于将评估焦点从导航准确性转向了<strong>安全可靠性</strong>，并系统化地设计了从结构化到非结构化、从信息完整到信息缺失的多维度任务，以暴露模型在安全关键决策中可能出现的各类失败模式，特别是那些罕见但灾难性的错误。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了多种LLMs（Gemini-2.5 Flash, Gemini-2.0 Flash, GPT-5, GPT-4o, LLaMA-3-8b）和VLMs（LLaVA系列, Qwen系列, InternVL3-14B等），通过公共API或单块NVIDIA RTX 6000 Ada GPU进行评估。每个模型在每项任务上通常进行30次或100次独立试验以评估一致性。</p>
<p><strong>Baseline对比</strong>：主要对比了上述不同规模和版本的SOTA模型在各项任务上的表现。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>完全信息任务（ASCII地图导航）</strong>：如表1和图3所示，性能差异巨大。GPT-5在所有确定性地图上达到100%成功率，表现出可靠和自适应的推理。Gemini-2.0 Flash和GPT-4o在简单地图上表现尚可（100%和80%），但在普通和困难地图上成功率骤降至0%，表现出“非渐进式崩溃”。LLaMA-3-8b在所有地图上成功率为0%，生成的路径完全破坏了地图结构（见图4）。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.05529v3/x3.png" alt="结果图表"></p>
<blockquote>
<p><strong>图3</strong>：LLMs在确定性和不确定ASCII地图任务上的成功率。展示了不同模型在不同复杂度地图上的表现差异。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.05529v3/x4.png" alt="结果图表"></p>
<blockquote>
<p><strong>图4</strong>：LLaMA-3-8b在（a）确定性地图（简单）和（b）不确定地形地图1上生成的崩溃地图结构。地图结构完全瓦解，充满随机符号。</p>
</blockquote>
<ol start="2">
<li><strong>不完全信息任务</strong>：<ul>
<li><strong>序列推理任务</strong>：如表2所示，在序列验证和序列掩码任务中，所有模型的准确率普遍在40%-60%左右，接近随机水平。模型表现出强烈的选择“右”的偏向（可能因为“right”有“正确”的积极含义）。在序列掩码任务中，模型经常产生幻觉，例如编造不存在的选项或拒绝回答。</li>
<li><strong>不确定地形地图任务</strong>：GPT-5表现最佳（不确定1：100%，不确定2：93.3%），并表现出“安全第一”的偏向，假设“？”不可通行。但在不确定2地图中仍有6.7%的失败涉及明确禁止的对角线移动，表明高准确率并不等同于安全。其他模型如Gemini-2.5 Flash表现不稳定，LLaMA-3-8b再次完全失败。</li>
<li><strong>建筑物后方任务</strong>：如图5所示，模型在将视觉场景转化为连贯空间布局方面能力有限，常出现结构崩溃、方向错误、约束违反和路径点错误等问题。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.05529v3/x5.png" alt="结果图表"></p>
<blockquote>
<p><strong>图5</strong>：建筑物后方任务中的代表性失败类型。（a）结构崩溃；（b）方向错误；（c）约束违反；（d）路径点错误。</p>
</blockquote>
<ol start="3">
<li><strong>安全导向空间推理（SOSR）任务</strong>：结果汇总于表1、图6和图7。在<strong>火灾疏散（困难）</strong> 实验中，发现了关键的安全失败案例：Gemini-2.5 Flash在32%的试验中指示用户前往存有重要个人材料的教授办公室，优先考虑文件检索而非疏散；在1%的试验中，甚至幻觉出一个提示中未提及的服务器房间作为目的地（见图1和图6b）。相比之下，GPT-4o则因自身安全政策拒绝回答。熵分析（图7）进一步显示了这些决策的不规则性。值得注意的是，在“困难”级别的紧急逃生任务中，Gemini-2.5 Flash的表现比Gemini-2.0 Flash低40%，表明更新的模型未必在安全理解上更优。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.05529v3/x1.png" alt="结果图表"></p>
<blockquote>
<p><strong>图1</strong>：在火灾场景中，LLM指示用户前往存有重要文件的地方（32%）或服务器房间（1%），而不是安全出口。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.05529v3/x6.png" alt="结果图表"></p>
<blockquote>
<p><strong>图6</strong>：（a）模型在SOSR任务系列上表现的雷达图。（b）Gemini-2.5 Flash在紧急逃生任务（困难难度）中的回答频率分布，显示其倾向于危险选择。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.05529v3/x7.png" alt="结果图表"></p>
<blockquote>
<p><strong>图7</strong>：根据模型在SOSR任务中生成的回答计算的熵值。高熵值表明回答分布不一致或不稳定。</p>
</blockquote>
<p><strong>消融实验/组件贡献</strong>：本文通过三类任务的对比，系统性地揭示了不同方面（完全信息规划、不完全信息推理、纯语言安全决策）的脆弱性。实验表明：</p>
<ul>
<li>即使在信息完全、结构简单的环境中（完全信息任务），多数模型也無法保证可靠的路径规划。</li>
<li>面对信息缺失（不完全信息任务），模型容易产生幻觉或做出不一致的假设。</li>
<li>在纯粹基于自然语言的安全关键决策（SOSR任务）中，模型可能完全无视人身安全，优先考虑任务目标，甚至产生危险的幻觉指令。这些失败案例共同论证了当前LLMs直接用于安全关键机器人决策的风险。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>实证揭示了安全风险</strong>：通过系统化的实验，实证证明了即使是最新的、性能改进的LLMs（如Gemini-2.5 Flash， GPT-5）在实践中也无法保证安全，在安全关键场景中会产生可能危及生命的错误决策。</li>
<li><strong>提出了系统化的评估视角</strong>：设计了一套包含七项任务的互补评估框架，超越了传统导航准确性指标，专注于暴露模型在安全可靠性、不完全信息处理和空间推理方面的具体失败模式。</li>
<li><strong>批判了现有评估指标</strong>：有力地论证了在安全关键领域，仅依赖“99%准确率”等聚合性能指标是危险且具有误导性的，因为罕见的失败（如1%）也可能导致灾难性后果，并指出对AI的绝对依赖本身可能成为新的风险源。</li>
</ol>
<p><strong>论文提到的局限性</strong>：</p>
<ol>
<li><strong>计算资源限制</strong>：实验在单块NVIDIA RTX 6000 Ada GPU上进行，因此评估局限于能被该硬件容纳的模型，排除了计算需求和参数量巨大的模型。</li>
<li><strong>数据集规模</strong>：初步评估使用的序列数据集仅包含100个样本。虽然作者认为对于二元任务足以近似正态分布并捕捉一般行为趋势，但不足以进行严格的统计分析。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>安全评估标准化</strong>：迫切需要为基于LLM/VLM的机器人系统建立侧重于安全性和可靠性的新评估基准与指标，而不仅仅是任务完成度。</li>
<li><strong>故障容忍与保障机制</strong>：本研究结果强烈暗示，不应将LLMs作为安全关键机器人系统中的独立、不可信的决策者。未来的研究应探索将LLMs与形式化验证、安全层、人类监督或冗余系统结合的架构，以确保即使AI组件失败，整体系统也能保持安全。</li>
<li><strong>模型安全对齐</strong>：需要进一步研究如何更好地将LLMs与人类的安全价值观和常识性风险认知对齐，特别是在涉及物理世界的场景中。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究了基于大语言模型（LLM）的机器人决策在安全关键场景中的潜在风险。核心问题是评估LLM在导航等任务中，因微小错误可能引发灾难性后果的可靠性。作者设计了七项评估任务，包括使用ASCII地图的完整信息任务、测试空间连续性的不完整信息任务，以及通过自然语言指令测试安全导向空间推理（SOSR）的任务。实验结果表明，当前LLM存在严重漏洞：在ASCII地图导航任务中部分模型成功率为0%；在模拟火灾疏散场景中，LLM甚至指示机器人走向服务器房而非安全出口。论文强调，即使99%的准确率也意味着每百次执行可能发生一次灾难性错误，因此当前LLM尚不能直接应用于自动驾驶等安全关键机器人系统。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.05529" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>