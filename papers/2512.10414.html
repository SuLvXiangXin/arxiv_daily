<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.10414" target="_blank" rel="noreferrer">2512.10414</a></span>
        <span>作者: Yu, Yang, Chen, Zhuangzhuang, Wang, Siqi, Li, Lanqing, Li, Xiaomeng</span>
        <span>日期: 2025/12/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，基于强化学习（RL）的微调是提升视觉语言模型（VLMs）推理能力的常用方法。其中，组相对策略优化（GRPO）因其简洁性和可扩展性受到广泛关注。然而，GRPO在微调过程中难以维持足够的策略熵，导致探索能力不足和性能停滞，即“熵崩溃”问题。现有研究主要通过控制策略优化阶段特定令牌的更新来进行熵干预，例如限制动作概率与优势值之间协方差高的令牌的更新。这些方法忽视了在RL采样阶段进行熵干预的可能性，而该阶段通过提高响应多样性同样可以提升GRPO性能。</p>
<p>本文针对RL采样阶段熵干预缺失这一具体痛点，提出了一个新的干预视角：利用熵引导的对抗样本在采样阶段进行可控的熵干预。其核心思路是：将采样响应的策略熵构建为对抗目标，通过反向传播其梯度来攻击并扭曲视觉输入，从而生成能够迫使策略模型探索更大答案空间的对抗样本。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为选择性对抗熵干预（Selective-adversarial Entropy Intervention, SaEI），其整体流程包含两个核心模块：熵引导对抗采样（Entropy-guided Adversarial Sampling, EgAS）和令牌选择性熵计算（Token-selective Entropy Computation, TsEC）。</p>
<p><img src="https://arxiv.org/html/2512.10414v1/x3.png" alt="方法总览"></p>
<blockquote>
<p><strong>图3</strong>：选择性对抗熵干预（SaEI）的整体框架。它包含熵引导对抗采样（EgAS）和令牌选择性熵计算（TsEC）。EgAS通过将熵作为对抗目标，利用其梯度扰乱视觉输入，从RL采样的角度干预策略熵。TsEC则丢弃熵值最高和最低的令牌，仅保留中等熵值的令牌用于对抗目标的熵计算。</p>
</blockquote>
<p><strong>整体流程</strong>：给定一个图像-问题对 (q=(I, Q))，首先使用旧策略模型 (\pi_{\theta_{old}}) 基于干净图像 (I) 生成 (n_1) 个响应。TsEC模块基于这些响应的令牌熵进行筛选，然后EgAS模块利用筛选后的熵值计算对抗目标，并通过投影梯度下降（PGD）攻击生成对抗图像 (I_{adv})。接着，使用 (I_{adv}) 再生成 (n_2) 个响应。最后，将来自干净图像和对抗图像的两组响应（共 (n_1+n_2) 个）混合，用于后续的奖励计算和策略优化。</p>
<p><strong>核心模块1：熵引导对抗采样（EgAS）</strong>：该模块旨在通过对抗性扰乱视觉输入来增强策略熵。具体而言，首先计算基于干净图像生成的响应组的策略熵 (\mathcal{H}(\pi_{\theta_{old}},{y^i}<em>{i=1}^{n_1}))（公式3）。将此熵作为需要增大的对抗目标，对初始图像 (I) 执行一步PGD攻击（公式4），其中步长 (\alpha) 为负标量，意味着沿着熵增大的梯度方向扰动图像。生成的对抗图像 (I</em>{adv}) 能够诱导策略模型产生熵更高的响应，从而在采样阶段扩大了探索空间。在策略优化时，损失函数（公式6）会同时考虑来自干净样本和对抗样本的响应，但当前策略模型 (\pi_\theta) 的概率计算仅基于干净图像，以保持其输入分布的一致性。</p>
<p><strong>核心模块2：令牌选择性熵计算（TsEC）</strong>：该模块旨在提升EgAS中对抗攻击的有效性，同时避免破坏VLMs内部的事实知识。其动机在于，响应中不同熵模式的令牌对推理的影响不同：熵值最低的令牌通常用于完成既定的语言结构并包含记忆的事实知识；熵值最高的令牌则是决定推理轨迹方向的关键决策点；中等熵值的令牌则混合了延续和导向功能。因此，TsEC将每个响应中的令牌按熵值排序并三等分，仅选择中等熵值区间的令牌用于计算对抗目标熵（公式7）。这样既避免了对包含事实知识的低熵令牌造成破坏性干扰，也避免了对本已具备强探索能力的高熵令牌进行不必要的干预。</p>
<p><strong>创新点</strong>：与主要在策略优化阶段进行干预的现有方法（如KL-Cov）不同，SaEI的创新性体现在从RL采样阶段进行熵干预。与另一基线NoisyRollout（向输入添加随机高斯噪声）相比，SaEI的对抗扰动具有明确的方向性（增大熵），因此能实现更精准、更有效的干预。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与基准</strong>：训练使用两个数学相关数据集：Geometry3K（几何问题）和MM-Eureka（多模态K-12数学）。评估分为域内测试（上述两数据集的测试集）和域外泛化测试，后者包括MathVerse、MathVision、MathVista和HallusionBench四个视觉推理基准。</p>
<p><strong>基线方法</strong>：对比了Vanilla GRPO、KL-Cov（在策略优化阶段干预熵）和NoisyRollout（向视觉输入添加带退火策略的随机噪声）。</p>
<p><img src="https://arxiv.org/html/2512.10414v1/x2.png" alt="熵与准确率动态"></p>
<blockquote>
<p><strong>图2</strong>：在Geometry3K数据集上，SaEI与Vanilla GRPO在训练过程中的熵动态和准确率动态对比。SaEI能显著提升并维持更高的策略熵（上图），并最终获得更高的测试准确率（下图）。</p>
</blockquote>
<p><strong>关键数值结果</strong>：</p>
<ul>
<li>在MM-Eureka上训练后，SaEI的域内准确率达到64.45%，比Vanilla GRPO（62.45%）高出2.00%，也优于NoisyRollout（62.93%）和KL-Cov（63.34%）。其域外测试平均准确率（56.05%）也最高。</li>
<li>在Geometry3K上训练后，SaEI的域内准确率为56.18%，优于所有基线。在较小的组大小（n=8）设置下，SaEI（55.02%）也显著优于Vanilla GRPO（52.47%）和KL-Cov（53.86%）。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.10414v1/x5.png" alt="主要结果表"></p>
<blockquote>
<p><strong>图5/表1</strong>：在MM-Eureka数据集上的主要实验结果。SaEI在域内（MM-Eureka）和域外平均（OOD Avg.）指标上均取得最佳性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.10414v1/x6.png" alt="几何结果表"></p>
<blockquote>
<p><strong>图6/表2</strong>：在Geometry3K数据集上的主要实验结果。SaEI在域内（Geometry3K）准确率上领先，并在大多数域外基准上表现优异。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ul>
<li><strong>EgAS与TsEC的贡献</strong>：消融实验表明，单独使用EgAS（即使用全部令牌计算熵）能带来显著提升，但结合TsEC后性能进一步改善，证明了令牌选择的有效性。</li>
<li><strong>与随机噪声对比</strong>：SaEI显著优于使用随机噪声扰乱输入的方法，且性能对噪声步长不敏感，而随机噪声的性能受噪声步长影响很大。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.10414v1/x4.png" alt="随机噪声敏感性"></p>
<blockquote>
<p><strong>图4</strong>：随机噪声干预的性能对噪声步长非常敏感，难以找到合适的步长以实现精确的熵控制，这凸显了熵引导对抗攻击的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.10414v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：在Geometry3K上的消融实验。w/o TsEC表示不使用令牌选择（即使用所有令牌）。结果表明，完整的SaEI（EgAS+TsEC）效果最佳，两者缺一不可。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了熵引导对抗采样（EgAS），首次将策略熵作为对抗目标来攻击视觉输入，从而在RL采样阶段实现可控的熵干预，以扩大策略的探索空间。</li>
<li>提出了令牌选择性熵计算（TsEC），通过基于熵排名的令牌选择策略，在增强对抗攻击效果的同时，保护了模型的事实知识不被破坏。</li>
</ol>
<p><strong>局限性</strong>：论文提到，对抗攻击在提升熵的同时，理论上仍有可能扭曲输入中的事实信息，尽管TsEC旨在缓解此问题。此外，方法涉及对抗攻击步长等超参数，可能需要针对不同数据集进行调整。</p>
<p><strong>后续启示</strong>：</p>
<ol>
<li>为克服RL训练中的探索不足问题提供了一个新颖的视角，即通过对抗学习在数据输入层面进行主动干预。</li>
<li>TsEC所依据的“不同熵值令牌承担不同功能”的洞见，可能泛化到其他需要精细控制语言模型生成过程的场景。</li>
<li>该方法主要针对视觉输入进行扰动，如何安全有效地对文本输入进行类似的对抗性熵干预，是一个值得探索的方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有基于强化学习（RL）的视觉语言模型微调方法中，熵干预仅局限于策略优化阶段、忽略采样阶段的问题，提出选择性对抗熵干预（SaEI）方法。该方法包含熵引导对抗采样（EgAS）与令牌选择性熵计算（TsEC）：EgAS将采样响应的熵作为对抗目标来扰动视觉输入，以扩大答案探索空间；TsEC则选择性计算熵以保持模型事实知识。实验表明，该方法能有效提升策略探索能力，从而显著增强模型的视觉推理性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.10414" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>