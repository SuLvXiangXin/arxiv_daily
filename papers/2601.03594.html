<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Jailbreaking LLMs &amp; VLMs: Mechanisms, Evaluation, and Unified Defense - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Cryptography and Security (cs.CR)</span>
      <h1>Jailbreaking LLMs &amp; VLMs: Mechanisms, Evaluation, and Unified Defense</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.03594" target="_blank" rel="noreferrer">2601.03594</a></span>
        <span>作者: Chen, Zejian, Li, Chaozhuo, Li, Chao, Zhang, Xi, Zhang, Litian, He, Yiming</span>
        <span>日期: 2026/01/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>随着以GPT系列、Llama系列为代表的大型语言模型以及LLaVA、BLIP等视觉语言模型的能力飞速发展，其安全与伦理对齐问题日益凸显。开发者已为模型部署了多种安全机制以限制有害输出，但这些限制催生了“越狱”攻击——即通过精心设计的输入诱导模型绕过安全限制，产生不当内容。现有研究对LLM越狱攻击已有一定探索，但主要集中在文本领域，对多模态视觉语言模型的安全威胁关注不足，且缺乏对幻觉与越狱之间区别与联系的系统分析，也缺少从攻击、防御到评估的全面综述。</p>
<p>本文针对当前越狱攻击与防御研究分散、缺乏统一框架的痛点，提出了一个系统性的三维调查视角。本文核心思路是：首先阐明越狱漏洞源于训练数据不完整、语言歧义和生成不确定性等结构性因素，并区分幻觉与越狱；然后构建一个涵盖攻击、防御、评估三个维度的完整框架，系统梳理现有方法；最后，基于对共享机制的归纳，提出一套适用于文本和多模态模型的统一防御原则。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文构建了一个三维的越狱攻击与防御调查框架，旨在系统化地梳理和理解该领域。</p>
<p><img src="https://arxiv.org/html/2601.03594v1/figs/attack_llm.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：越狱攻击方法整体分类框架。左侧为针对LLMs的攻击，包括模板攻击、隐写攻击、基于上下文学习的攻击等七大类；右侧为针对VLMs的攻击，包括基于提示的图像注入、组合提示与图像扰动以及代理模型转移三大类。</p>
</blockquote>
<p><strong>1. 攻击维度</strong><br>攻击方法被系统性地分类，针对LLMs和VLMs各有不同侧重点。</p>
<ul>
<li><p><strong>针对LLMs的攻击</strong>：</p>
<ul>
<li><strong>模板攻击</strong>：通过设计复杂、嵌套的提示模板（如虚构场景、角色扮演）来伪装恶意意图，绕过安全过滤器。例如，通过构建多层虚构世界，引导模型在“故事”中生成有害内容。<br><img src="https://arxiv.org/html/2601.03594v1/figs/template.png" alt="模板攻击示例"><blockquote>
<p><strong>图2</strong>：模板攻击示意图。通过设计包含特定结构的提示模板，诱导模型生成其通常被禁止输出的内容。</p>
</blockquote>
</li>
<li><strong>隐写攻击</strong>：将敏感词替换为模型不易识别但人类可读的形式，如ASCII艺术字符画，以规避基于关键词的过滤。<br><img src="https://arxiv.org/html/2601.03594v1/figs/prompt.png" alt="隐写攻击示例"><blockquote>
<p><strong>图3</strong>：隐写攻击（ArtPrompt）示意图。将“bomb”一词替换为ASCII艺术图案，从而绕过安全机制。</p>
</blockquote>
</li>
<li><strong>基于上下文学习的攻击</strong>：在提示中提供少量恶意示例，利用模型的上下文学习能力诱导其生成类似的有害输出。<br><img src="https://arxiv.org/html/2601.03594v1/figs/ica.png" alt="基于上下文的攻击示例"><blockquote>
<p><strong>图4</strong>：基于上下文学习的攻击示意图。通过提供恶意的“少样本”示例，引导模型遵循有害模式。</p>
</blockquote>
</li>
<li><strong>对抗性攻击</strong>：在输入提示中添加人眼难以察觉的微小扰动，显著改变模型输出，使其产生有害内容。<br><img src="https://arxiv.org/html/2601.03594v1/figs/ADV.png" alt="对抗性攻击示例"><blockquote>
<p><strong>图5</strong>：对抗性攻击示意图。对输入嵌入添加微小扰动，导致模型输出发生巨大变化。</p>
</blockquote>
</li>
<li><strong>强化学习攻击</strong>：将越狱视为一个优化问题，使用强化学习（如PPO）迭代优化攻击提示，以最大化攻击成功率。<br><img src="https://arxiv.org/html/2601.03594v1/figs/RL.png" alt="强化学习攻击示例"><blockquote>
<p><strong>图6</strong>：强化学习攻击示意图。通过奖励函数引导攻击策略的优化，以生成有效的越狱提示。</p>
</blockquote>
</li>
<li><strong>LLM辅助攻击</strong>：利用一个LLM（如攻击者）自动生成或优化针对另一个LLM（如受害者）的越狱提示。</li>
<li><strong>微调攻击</strong>：直接对模型参数进行恶意微调，注入后门或改变其行为，使其在特定触发条件下输出有害内容。<br><img src="https://arxiv.org/html/2601.03594v1/figs/FT.png" alt="微调攻击示例"><blockquote>
<p><strong>图8</strong>：微调攻击示意图。通过在有恶意指令的数据集上微调，将有害行为植入模型。</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>针对VLMs的攻击</strong>：</p>
<ul>
<li><strong>基于提示的图像注入越狱攻击</strong>：通过在文本提示中嵌入恶意指令，诱导VLM生成不当的图像描述或执行有害操作。</li>
<li><strong>组合提示与图像扰动越狱攻击</strong>：同时对输入图像和文本提示施加微小扰动，误导VLM产生错误的跨模态理解与生成。</li>
<li><strong>代理模型转移越狱攻击</strong>：攻击者在一个可访问的代理VLM上开发攻击方法，然后将该攻击策略迁移到目标黑盒VLM上。</li>
</ul>
</li>
</ul>
<p><strong>2. 防御维度</strong><br>防御措施同样分为针对LLMs和VLMs两类，并可按干预层次划分。</p>
<ul>
<li><strong>针对LLMs的防御</strong>：<ul>
<li><strong>提示级防御</strong>：在输入阶段进行干预，例如提示混淆（重新表述用户查询）、输入过滤或添加安全前缀。</li>
<li><strong>模型级防御</strong>：在模型内部或输出阶段进行干预，包括安全感知解码（在生成过程中调整采样策略）、输出审查（使用辅助模型检查输出安全性）以及对齐微调（使用人类反馈强化学习等技术优化模型的安全偏好）。</li>
</ul>
</li>
<li><strong>针对VLMs的防御</strong>：<ul>
<li><strong>基于提示扰动的防御</strong>：修改多模态输入以消除攻击意图。</li>
<li><strong>基于响应评估的防御</strong>：使用辅助VLM评估生成内容的安全性并进行迭代改进。</li>
<li><strong>基于模型微调的防御</strong>：在混合数据集上调整VLM参数，增强其对有害内容的敏感性。</li>
</ul>
</li>
</ul>
<p><strong>3. 评估维度</strong><br>本文总结了评估越狱攻击与防御效果的关键指标：</p>
<ul>
<li><strong>攻击成功率</strong>：衡量攻击有效性的核心指标。</li>
<li><strong>毒性分数</strong>：评估生成内容有害程度的量化指标。</li>
<li><strong>查询/时间成本</strong>：衡量攻击效率的指标。</li>
<li><strong>多模态清洁准确率与属性成功率</strong>：针对VLMs，评估防御措施在保证正常功能（清洁准确率）的同时抵御攻击（属性成功率）的能力。</li>
</ul>
<p><strong>创新点</strong>：与先前综述相比，本文的创新性体现在：1) <strong>覆盖面更全</strong>：首次系统地将调查范围从纯文本LLMs扩展到多模态VLMs，涵盖了从文本到多模态的完整谱系。2) <strong>机制归纳</strong>：不仅罗列方法，更强调归纳越狱漏洞的共享结构性根源（数据、歧义、不确定性）。3) <strong>提出统一防御原则</strong>：基于对攻击机制的深入理解，提出了跨文本和多模态的统一防御框架，包括感知层的变体一致性与梯度敏感性检测、生成层的安全感知解码与输出审查，以及参数层的对抗性增强偏好对齐。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文是一篇系统性综述，因此其主要“实验”部分是对现有文献的梳理、分类和对比分析，而非提出新方法并进行定量实验验证。</p>
<p><strong>1. 文献综述与分类对比</strong><br>论文通过表格形式，将本工作与已有的相关综述进行了详细对比。</p>
<p><img src="https://arxiv.org/html/2601.03594v1/x1.png" alt="综述对比表"></p>
<blockquote>
<p><strong>图9</strong>：本文与现有越狱综述的对比表。表明本文是首个全面覆盖LLM攻击/防御、VLM攻击/防御以及评估方法的系统性综述。</p>
</blockquote>
<p><strong>2. 现有基准与评估指标总结</strong><br>论文总结了当前用于评估大模型安全性的多模态基准数据集，例如MM-SafetyBench、MMHal-Bench等，并详细说明了前文提到的各项评估指标的具体含义和应用场景。</p>
<p><strong>3. 统一防御框架的提出</strong><br>基于对攻击机制和现有防御方法的分析，论文提出了一个分层的统一防御原则框架，该框架旨在为未来的防御策略设计提供指导。</p>
<p><img src="https://arxiv.org/html/2601.03594v1/img/pipeline.png" alt="统一防御框架"></p>
<blockquote>
<p><strong>图24</strong>：提出的统一防御框架示意图。该框架分为感知层、生成层和参数层，每层针对不同的攻击机制部署相应的防御策略。</p>
</blockquote>
<p><strong>关键要点</strong>：本文的实验部分并非传统意义上的性能对比实验，而是通过系统的文献调研，清晰地勾勒出越狱攻击与防御领域的研究全景图，识别出现有研究的空白（如VLM安全评估的缺乏），并在此基础上提出了一个具有前瞻性的统一防御理论框架。图表主要服务于分类展示和概念说明。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>系统性概述</strong>：提供了对LLMs和VLMs越狱攻击与防御方法的首次全面、系统性综述，涵盖了从攻击、防御到评估的完整链条。</li>
<li><strong>概念辨析与机制深化</strong>：清晰区分了“幻觉”与“越狱”，指出越狱是一种具有特定恶意意图的“结构性幻觉”，并深入分析了其源于模型数据、歧义和不确定性的根本原因。</li>
<li><strong>三维框架与统一防御原则</strong>：提出了攻击-防御-评估的三维调查框架，并基于对跨模态共享攻击机制的理解，创新性地提出了一套分层（感知层、生成层、参数层）的统一防御原则，为未来研究提供了理论指导。</li>
</ol>
<p><strong>局限性</strong>：论文自身作为一篇综述，其局限性在于主要进行归纳总结，而未提出全新的具体攻击或防御算法进行实证验证。此外，论文也指出当前领域存在评估标准不统一、缺乏跨模态协同防御研究等普遍局限。</p>
<p><strong>未来启示</strong>：</p>
<ol>
<li><strong>自动化红队测试</strong>：开发自动化工具以持续、大规模地测试模型的安全性漏洞。</li>
<li><strong>跨模态协同防御</strong>：研究如何利用文本和视觉模态之间的互补信息进行更鲁棒的联合防御。</li>
<li><strong>标准化评估</strong>：推动建立公开、统一、全面的越狱攻击与防御评估基准和协议，以促进该领域的公平比较和健康发展。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文是一篇系统性综述，旨在解决大型语言模型（LLMs）和视觉语言模型（VLMs）面临的“越狱”攻击这一核心安全问题。论文指出，越狱漏洞根源于训练数据不完整、语言歧义和生成不确定性等结构性因素。作者构建了一个三维分析框架，系统梳理了从文本到多模态场景下的攻击方法（如基于模板、对抗学习）、防御策略（如提示混淆、模型对齐）和评估指标（如攻击成功率）。其核心贡献在于提出了统一的防御原则，包括感知层的变体一致性检测、生成层的安全感知解码以及参数层的对抗增强偏好对齐。由于是综述性论文，本文未报告具体的实验性能提升数据，主要贡献在于对现有机制的系统性梳理和防御框架的整合。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.03594" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>