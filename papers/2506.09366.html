<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.09366" target="_blank" rel="noreferrer">2506.09366</a></span>
        <span>作者: Kuang, Yuxuan, Geng, Haoran, Elhafsi, Amine, Do, Tan-Dzung, Abbeel, Pieter, Malik, Jitendra, Pavone, Marco, Wang, Yue</span>
        <span>日期: 2025/06/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人因其灵活性和类人形态，在多样化环境中执行日常任务方面具有巨大潜力。当前主流方法主要分为两类：基于最优控制的方法，如构建动力学模型进行模型预测控制，以及基于无模型强化学习的方法。然而，这些方法存在关键局限性：基于最优控制的方法需要为每个任务精确构建和调整动力学模型及复杂的成本函数，优化过程耗时，可扩展性差；而基于强化学习的方法大多专注于运动模仿或遥操作等表现性任务，缺乏执行多样化自主运动操作的能力，并且每个任务都需要大量繁琐的奖励塑形来平衡任务成功、姿态、步态等多个目标，以防止奖励欺骗，这严重限制了其应对日常场景中无限任务变体的通用性和可扩展性。</p>
<p>本文针对“为每个新任务进行大量任务特定调优”这一具体痛点，提出了一种新视角：模仿人类运动技能发展过程，即先习得行走、抓取等基本能力，再将其组合用于复杂任务。本文的核心思路是提出一个“预训练-后混合”的分层强化学习框架SkillBlender，首先预训练一组与任务无关、可重用、物理可解释的基元技能，然后通过高层控制器动态混合这些技能来完成复杂的全身运动操作任务，从而将每个任务所需的奖励项减少到仅需一两个。</p>
<h2 id="方法详解">方法详解</h2>
<p>SkillBlender是一个分层强化学习框架，其整体流程遵循“预训练-后混合”范式。首先，离线预训练一组低层目标条件基元技能策略；然后，给定一个新的高层任务，高层控制器学习为选定的技能生成子目标并进行加权混合，以产生最终动作，在此过程中仅使用极简的任务特定奖励。</p>
<p><img src="https://arxiv.org/html/2506.09366v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：SkillBlender 方法整体框架。左侧为预训练阶段，学习四种目标条件的基元技能。右侧为混合阶段，高层控制器接收任务目标与状态，输出子目标与关节级权重向量，对冻结的低层技能动作进行加权混合以执行新任务。</p>
</blockquote>
<p>核心模块包括低层基元技能学习与高层技能混合控制器。</p>
<ol>
<li><p><strong>低层基元技能</strong>：预训练四种通用、与任务无关的技能策略，每个策略 $\pi_L^i(a_t^i | g_t^i, s_t)$ 以当前状态 $s_t$（本体感知信息）和技能特定子目标 $g_t^i$ 为输入，输出整个人形机器人身体的目标关节位置，并通过PD控制器转换为扭矩。这四种技能是：</p>
<ul>
<li><strong>行走</strong>：响应XY平面及偏航轴上的指令速度。</li>
<li><strong>伸手</strong>：保持静止，用双腕触及周围3D目标点。</li>
<li><strong>蹲下</strong>：蹲下或站起以达到指定根高度。</li>
<li><strong>踏步</strong>：踏到指定的地面点上。</li>
</ul>
</li>
<li><p><strong>高层技能混合</strong>：对于新任务，首先通过技能选择器（可手动或利用基础模型）选定相关技能子集 ${\pi_L^j, ..., \pi_L^k}$。高层控制器 $\pi_H({g_t^i}, {W_t^i} | g_t, s_t)$ 以任务总目标 $g_t$ 和状态 $s_t$ 为输入，为每个选定技能输出原始子目标 $\tilde{g}_t^i$ 和原始关节级权重向量 $\tilde{W}_t^i \in [0,1]^d$（$d$为动作维度）。原始子目标被裁剪处理得到最终子目标 ${g_t^j, ..., g_t^k}$ 输入给低层技能。关键创新在于对原始权重向量施加关节级的Softmax非线性处理：<br>$W_t^n[m] = \frac{e^{\tilde{W}<em>t^n[m]}}{\sum</em>{i=j}^{k} e^{\tilde{W}<em>t^i[m]}}$<br>其中 $W_t^n[m]$ 是第 $n$ 个技能权重向量中第 $m$ 个关节的权重标量。此操作避免了动作的简单线性组合导致的奖励欺骗。最终动作 $a_t$ 通过加权和计算：<br>$a_t = \sum</em>{i=j}^{k} a_t^i \odot W_t^i$<br>训练时仅更新高层控制器，低层技能保持冻结。</p>
</li>
</ol>
<p>与现有分层强化学习方法相比，创新点具体体现在：1）使用了多个目标条件、物理可解释、可重用的基元技能作为先验；2）提出了独特的向量化加权混合机制，特别是关节级Softmax，实现了更灵活、准确的动作合成。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在作者提出的SkillBench模拟基准上进行。该基准基于NVIDIA Isaac Gym，具有高度并行化特性，包含3种不同的人形机器人形态、4种基元技能和8个具有挑战性的运动操作任务。</p>
<p><img src="https://arxiv.org/html/2506.09366v1/x3.png" alt="SkillBench基准概览"></p>
<blockquote>
<p><strong>图3</strong>：SkillBench 基准包含三个机器人形态（Atlas、Digit、Talos）、四种基元技能和八个运动操作任务环境。</p>
</blockquote>
<p>对比的基线方法包括：1）<strong>PPO</strong>：使用与SkillBlender高层相同的任务奖励进行端到端训练。2）<strong>HRL (Single LLM)<strong>：分层RL，但仅使用一个低层通用策略（而非多个技能）。3）</strong>MCP</strong>：一种现有的多技能组合方法。4）<strong>Oracle</strong>：为每个任务手工设计的最优控制器，代表性能上界。</p>
<p>关键实验结果：在八个任务上的平均成功率，SkillBlender达到 **93.8%**，显著优于PPO (39.4%)、HRL (Single LLM) (75.0%) 和 MCP (81.3%)，接近Oracle (95.0%)。</p>
<p><img src="https://arxiv.org/html/2506.09366v1/x4.png" alt="主要定量结果"></p>
<blockquote>
<p><strong>图4</strong>：在SkillBench八个任务上的成功率对比。SkillBlender在几乎所有任务上都优于其他学习基线，且平均成功率最高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09366v1/x5.png" alt="技能混合可视化"></p>
<blockquote>
<p><strong>图5</strong>：“开门”任务中技能混合的可视化。高层控制器自动协调“行走”和“伸手”技能，并动态调整不同身体部位的权重（颜色深浅），展示了全身协调。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09366v1/x6.png" alt="可行性评估"></p>
<blockquote>
<p><strong>图6</strong>：运动可行性评估。SkillBlender在步态周期、接触力、动作平滑度等可行性指标上均优于或与基线相当，表明其能产生更自然、真实的运动。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：</p>
<ol>
<li><p><strong>关节级Softmax的重要性</strong>：移除Softmax层（直接线性混合）会导致严重的奖励欺骗和性能下降（成功率从93.8%降至约70%），验证了其必要性。<br><img src="https://arxiv.org/html/2506.09366v1/x7.png" alt="Softmax消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融Softmax非线性层（“w/o Softmax”）导致性能显著下降，并出现不自然的动作，证明了其防止奖励欺骗的关键作用。</p>
</blockquote>
</li>
<li><p><strong>使用多个技能 vs. 单个技能</strong>：与仅使用一个低层策略的HRL方法相比，使用多个专用基元技能的SkillBlender性能更优。</p>
</li>
<li><p><strong>技能选择的影响</strong>：实验表明，为任务选择合适的技能子集对性能有积极影响。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.09366v1/x8.png" alt="跨形态与任务泛化"></p>
<blockquote>
<p><strong>图8</strong>：跨机器人形态的零样本泛化。将在Atlas上训练的高层控制器直接迁移到Digit和Talos上，仍能取得良好性能，体现了方法的通用性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.09366v1/x9.png" alt="定性结果展示"></p>
<blockquote>
<p><strong>图9</strong>：SkillBlender在多种任务上的定性结果展示，包括搬运、开门、踢球等，显示了其执行多样化全身协调任务的能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了 <strong>SkillBlender</strong>，一个通过预训练与混合基元技能来实现通用人形全身运动操作的框架，仅需极简任务奖励。2）引入了 <strong>SkillBench</strong>，一个并行、跨形态、多样化的模拟基准，并配套了兼顾任务准确性与运动可行性的科学评估指标。3）开源了一套结构化、可重用、与任务无关的人形基元技能、任务环境及预训练模型，以促进社区研究。</p>
<p>论文自身提到的局限性主要在于目前工作完全在模拟中进行，模拟到现实的差距尚未探索。此外，技能库目前是手动定义和选择的，未来可探索更大规模的技能库和自动技能发现与选择机制。</p>
<p>对后续研究的启示包括：该方法展示了利用模块化、可解释的技能先验来简化复杂机器人任务学习的有效性；SkillBench为公平、全面的评估设立了新标准；所开源的基元技能库可作为未来人形机器人研究的基础构建模块，推动更通用、更易部署的机器人智能发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SkillBlender，以解决人形机器人全身移动操作任务中缺乏通用性的问题。现有方法需为每个任务进行繁琐的特定调整，难以适应多样化日常场景。为此，作者提出一种分层强化学习框架：首先预训练目标条件、任务无关的原始技能，随后通过动态技能混合完成复杂任务，极大减少了任务特定的奖励设计。在包含多机器人形态、多技能的模拟基准测试中，该方法显著优于所有基线，能自然规避奖励滥用，产生更准确、可行的运动行为。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.09366" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>