<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.23877" target="_blank" rel="noreferrer">2503.23877</a></span>
        <span>作者: Shi, Junyao, Zhao, Zhuolun, Wang, Tianyou, Pedroza, Ian, Luo, Amy, Wang, Jie, Ma, Jason, Jayaraman, Dinesh</span>
        <span>日期: 2025/03/31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域的主流模仿学习方法严重依赖于在特定机器人、特定场景、特定物体上收集的高质量演示数据。这种数据收集方式成本高昂，难以扩展，成为发展通用机器人的主要障碍。与此同时，网络上已存在大量记录人类在多样化环境中执行操作技能的视频数据，这些数据蕴含了丰富的技能信息。本文旨在解决的核心痛点是：能否仅利用这些“野外”人类视频，而无需任何额外的机器人特定演示或探索，直接提炼出可立即部署的机器人技能策略？ZeroMimic 提出了一个新的视角，通过巧妙结合现代语义与几何视觉理解、抓取可操作性检测器以及模仿策略模型，从人类网络视频中蒸馏出图像目标条件化的技能策略。其核心思路是：将技能分解为抓取与抓取后两个阶段，分别利用人类视频学习任务相关的交互可操作性点以及手腕运动轨迹，从而实现零样本跨场景、跨物体、跨机器人平台的技能部署。</p>
<h2 id="方法详解">方法详解</h2>
<p>ZeroMimic 将操作技能分解为两个顺序执行的阶段：抓取阶段和抓取后阶段。整体流程如图3所示。抓取阶段的输入是当前RGB图像和任务描述（如“打开抽屉”），输出是机器人执行抓取的动作序列。抓取后阶段的输入是当前RGB图像、当前夹爪姿态以及一张描述任务完成状态的人类目标图像，输出是预测的6自由度夹爪运动轨迹。</p>
<p><img src="https://arxiv.org/html/2503.23877v1/extracted/6322871/figures/new_method_figure/ZeroMimic_method_v4.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：ZeroMimic 方法整体框架。上半部分为抓取阶段，利用基于人类可操作性的抓取来执行与任务相关的抓取。下半部分为抓取后阶段，是一个在网络视频上训练的模仿策略，用于预测6D手腕轨迹。训练好的模型可直接部署到机器人上。</p>
</blockquote>
<p><strong>1. 抓取阶段：基于人类可操作性的抓取</strong><br>此阶段的目标是让机器人执行一个与任务意图相符的抓取。由于人类手部与机器人二指夹爪的形态差异巨大，直接模仿人类抓取姿态不可行。因此，该阶段进一步分为两个子步骤：</p>
<ul>
<li><strong>可操作性预测</strong>：使用预训练于EpicKitchens数据集上的VRB模型。给定RGB图像和自然语言任务描述，VRB预测图像空间中与任务相关的交互接触点（例如，为了打开抽屉，应该接触哪里）。</li>
<li><strong>抓取选择</strong>：在VRB预测的接触点附近，使用在机器人数据上预训练的AnyGrasp模型，为二指夹爪生成一系列可行的抓取位姿候选。系统选择最合适的抓取，并规划末端执行器的线性运动来执行它。</li>
</ul>
<p><strong>2. 抓取后阶段：基于人类运动的抓取后机器人策略</strong><br>一旦机器人抓取物体，此模块负责生成完成任务的6D末端轨迹。该策略完全从野外人类视频中蒸馏而来，具体流程如下：</p>
<ul>
<li><strong>从网络视频中提取人类手腕轨迹</strong>：使用EpicKitchens数据集。首先，运行先进的预训练手部追踪模型HaMeR，获得每帧图像中手部关节的3D位姿（相对于规范手部坐标系）。然后，利用EPIC-Fields提供的、通过COLMAP运动恢复结构算法得到的相机参数，将这些基于像素坐标的手部位姿转换到世界坐标系。最终，提取出世界坐标系下的6D手腕轨迹序列。</li>
<li><strong>策略训练与执行</strong>：为了处理人类演示的多模态性（同一观察下存在多种可行的操作方式），采用动作分块Transformer（ACT）策略类来学习动作序列的生成模型。模型的输入是当前图像 $I_t$、目标图像 $I_g$ 和当前手腕姿态 $h_t$，输出是未来一段时间内的手腕姿态序列 ${h_i}_{i=t+1}^{t+n+1}$（分块大小 $n=10$）。为减轻模型在测试时（使用静态相机）预测相机参数的负担，在训练时利用每帧的相机外参将所有当前及未来的手腕姿态转换到当前帧的相机坐标系中。每个技能训练一个独立的策略模型，共得到9个技能策略。部署时，将训练好的策略直接用于机器人：提供当前RGB观测、当前夹爪姿态（在相机坐标系中）以及一张人类完成任务的图像作为目标图像，模型预测出相机坐标系下的6D轨迹，再转换到机器人坐标系执行。</li>
</ul>
<p><img src="https://arxiv.org/html/2503.23877v1/extracted/6322871/figures/HAT_open_drawer.jpeg" alt="策略输出可视化"></p>
<blockquote>
<p><strong>图4（a）</strong>：在未见过的图像上，抓取后策略输出的6D手腕姿态预测示例（打开抽屉）。红、绿、蓝箭头分别表示手腕在相机坐标系中x, y, z轴的方向。</p>
</blockquote>
<p>与现有方法相比，ZeroMimic 的主要创新点在于：1) <strong>系统化整合</strong>：首次构建了一个完整的系统，将可操作性预测、抓取生成和轨迹模仿无缝结合，实现了从原始视频到可执行策略的端到端蒸馏。2) <strong>几何感知</strong>：在轨迹提取阶段显式利用运动恢复结构（SfM）提供的相机参数，将嘈杂、晃动的第一人称视频中的手部运动准确地锚定到3D世界坐标，这是生成精确、可迁移轨迹的关键。3) <strong>零样本与泛化</strong>：整个流程完全不依赖任何任务或机器人特定的数据，所有组件均使用预训练模型或网络视频数据，从而实现了对新物体、新场景和新机器人平台的零样本泛化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实世界和仿真环境中进行评估。真实世界实验在3个不同的真实厨房中进行，使用了Franka Emika Panda和Trossen Robotics WidowX 250 S两种不同的机器人臂，共涵盖30个不同场景、18个物体类别。仿真实验在RoboCasa中进行，评估了4个技能策略，每个策略在20个随机化的厨房试验中测试。所有评估物体和场景均未出现在训练数据中。</p>
<p><strong>对比基线</strong>：进行了详尽的消融实验以验证各组件重要性。抓取阶段对比了：1) 完整方法（Ours）；2) 不使用VRB交互可操作性，仅用AnyGrasp分数选择抓取（Ours w/o interaction affordance）；3) 不使用AnyGrasp抓取模型，直接移动到VRB预测的3D点并闭合夹爪（Ours w/o grasp model）。抓取后阶段对比了：1) 完整方法（Ours）；2) 去除SfM相机参数处理（Ours w/o SfM）；3) 使用仅输出2D像素轨迹的VRB方法。此外，还与最新的零样本机器人系统ReKep进行了对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>消融实验</strong>：如表I和表II所示，完整的ZeroMimic方法在“打开抽屉”和“打开橱柜”任务上均取得最佳成功率。抓取阶段，移除交互可操作性或抓取模型都会导致性能严重下降。抓取后阶段，移除SfM相机信息或使用仅2D的轨迹预测方法同样显著降低了成功率，证明了3D几何感知和完整6D轨迹预测的必要性。</li>
<li><strong>整体零样本部署性能</strong>：</li>
</ol>
<p><img src="https://arxiv.org/html/2503.23877v1/extracted/6322871/figures/skill_success_rates/skill_success_rates_cameraready.png" alt="零样本性能概览"></p>
<blockquote>
<p><strong>图5</strong>：ZeroMimic 零样本性能概览图。展示了在真实世界（Franka和WidowX机器人）和仿真环境中，9种不同技能的整体成功率，体现了其跨任务、跨机器人、跨环境的强泛化能力。</p>
</blockquote>
<p>ZeroMimic 在真实世界中整体成功率达到71.9%（Franka）和65.0%（WidowX），在仿真中达到73.8%。具体技能上，滑动开合、铰链开合、拾放、倾倒、切割和搅拌等技能均表现出色。论文特别指出，模型能够学习到诸如根据刀与物体的相对位置调整切割角度、在搅拌时保持平移基本不变而连续旋转等复杂行为。</p>
<ol start="3">
<li><strong>与ReKep对比</strong>：如表III所示，在4个需要空间推理的任务上，ZeroMimic 的表现显著优于基于视觉语言模型（VLM）的ReKep方法。ReKep的失败主要源于VLM生成的关键点约束不准确或空间推理能力有限。</li>
</ol>
<p><strong>失败分析</strong>：对87个失败试验的分析显示，31.1%失败于AnyGrasp阶段（如点云感知失败），24.1%失败于VRB阶段（如对大家具预测不准），44.8%失败于抓取后策略阶段（如相机-机器人配置偏离第一人称视角、从视频重建的动作存在噪声）。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ZeroMimic，首个从纯粹“野外”人类网络视频中蒸馏出零样本即可部署的机器人操作技能策略的系统，并在多样化的真实场景和物体上进行了验证。</li>
<li>实证表明，系统能够学习9种不同的技能，在真实世界和仿真中取得高成功率，并能泛化到训练中未见过的物体类别、场景和机器人平台。</li>
<li>通过系统的消融研究，揭示了从野外人类视频中学习和执行机器人技能的关键因素，例如结合交互可操作性与抓取模型、利用SfM进行3D轨迹重建、预测完整的6D相对动作等。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前方法利用了简化的抓取前/后技能结构；直接将人类手腕运动重定向到机器人，未考虑形态差异；无法学习手内操作、非抓取交互、夹爪释放或需要双臂协同的任务。</p>
<p><strong>启示</strong>：这项工作证明了大规模网络视频作为机器人技能学习数据源的巨大潜力。后续研究可以沿着以下方向推进：设计更复杂、更连续的技能结构以处理更广泛的任务；开发更精细的运动重定向方法以桥接人类与机器人的形态鸿沟；探索如何从视频中学习非抓取交互和双手协作策略。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ZeroMimic系统，旨在解决机器人如何直接从大量现成的网络人类操作视频（而非机器人特定演示）中提取可部署技能策略的核心问题。方法关键点在于：利用语义与几何视觉理解技术分析人类视频，结合抓取可供性检测器与模仿策略模型，蒸馏出以图像为目标的技能策略。通过在EpicKitchens数据集上训练，该系统在真实与模拟的多种厨房场景及不同机器人实体上实现了开箱即用的多样化操作能力（如开合、倾倒、抓放等）。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.23877" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>