<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Diffusion Policy from Primitive Skills for Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Diffusion Policy from Primitive Skills for Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.01948" target="_blank" rel="noreferrer">2601.01948</a></span>
        <span>作者: Dong Xu Team</span>
        <span>日期: 2026-01-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>扩散策略（DP）在机器人操作的动作生成方面展现出巨大潜力。然而，现有方法通常依赖全局指令来生成短期控制信号，这可能导致动作生成过程中的不匹配或模糊行为。例如，当机器人需要执行“合上夹爪”这一具体操作时，高层任务描述“拿起柠檬放入锅中”显得过于抽象，无法提供明确的指导。本文认为，原始技能——即细粒度、短周期的操作，如“向上移动”、“打开夹爪”——为机器人学习提供了更直观和有效的接口。本文针对高层指令与短期动作之间粒度不匹配的痛点，提出将复杂任务分解为原始技能序列，并训练一个技能条件化的扩散策略来生成与技能对齐的精确动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的技能条件化扩散策略（SDP）的整体框架如图3所示。其流程分为上下两部分：上半部分根据当前视觉观察和语言指令预测一个描述即将执行操作的原始技能；下半部分则是一个单技能策略，它整合状态信息并生成与该技能严格对齐的动作序列。</p>
<p><img src="https://arxiv.org/html/2601.01948v1/x3.png" alt="方法总览"></p>
<blockquote>
<p><strong>图3</strong>：SDP方法整体框架。上半部分通过视觉语言模型（VLM）和路由器网络为当前状态分配一个原始技能；下半部分将分配的技能嵌入、本体感知等信息注入扩散策略，通过技能依赖的FFN层生成精确的动作。</p>
</blockquote>
<p><strong>核心模块1：原始技能分配</strong><br>该模块旨在为每个状态动态分配一个最合适的原始技能。首先，作者抽象出八个跨任务可重用的原始技能：<code>roll</code>, <code>yaw</code>, <code>open the gripper</code>, <code>move up</code>, <code>translate</code>, <code>close the gripper</code>, <code>move down</code>, <code>rotate</code>。通过设计的组合提示集成（CPE）方法，为每个技能生成统一的文本提示（如“the robot arm is going to {skill}.”），并使用冻结的CLIP文本编码器将其转换为技能提示嵌入 $\bm{p}$。</p>
<p>其次，利用一个视觉语言模型（VLM）处理来自静态和腕部摄像头的视觉观察以及高层语言指令，生成视觉语言表征 $\bm{z}_{vl}$。该VLM包含一个共享的图像编码器和一个文本嵌入层，后接一个Transformer进行特征融合。</p>
<p>最后，设计一个轻量级路由器网络进行技能选择。它将 $\bm{z}<em>{vl}$ 沿token维度平均后，通过一个MLP层和Softmax函数计算八个技能的重要性分数，并选择分数最高的技能。该技能的嵌入 $\bm{z}$ 通过加权求和（$\bm{z}=\sum</em>{i=1}^{8}R(\bm{z}<em>{vl})</em>{i}\cdot \bm{p}_{i}$）获得，并用于指导后续的动作生成。</p>
<p><strong>核心模块2：技能条件化扩散策略学习</strong><br>此模块的目标是学习一个以分配到的技能为条件的扩散策略，生成技能对齐的动作。首先进行先验信息注入：时间步和本体感知通过一个小型MLP编码器处理，然后通过改进的AdaLN操作注入到扩散模型中；视觉和语言信息则通过VLM的输出token，经线性投影和RMSNorm后，在扩散Transformer的每个块中以交叉注意力（Cross-Attention）方式注入。</p>
<p>创新的核心在于<strong>技能依赖的FFN层</strong>。为了在原始技能和动作生成之间建立明确的依赖关系，作者在原始FFN层（$\operatorname{FFN}<em>{\text{ori}}$）基础上，增加了一个类似LoRA的FFN分支。该分支的两个权重矩阵 $\bm{W}</em>{\bm{z}}^{1}$ 和 $\bm{W}<em>{\bm{z}}^{2}$ 由技能嵌入 $\bm{z}$ 通过一个MLP动态生成。最终的FFN层输出为：<br>$\operatorname{FFN}(\bm{x})=\bm{W}</em>{\bm{z}}^{2}(\operatorname{SwishGLU}(\bm{W}<em>{\bm{z}}^{1}\bm{x}))+\operatorname{FFN}</em>{\text{ori}}(\bm{x})$。<br>这相当于一个混合专家系统，其中第一项是技能依赖的专家，第二项是共享专家，共同构成了一个单技能策略。</p>
<p><strong>训练目标</strong>在标准的去噪分数匹配损失 $\mathcal{L}<em>{\text{SM}}$ 基础上，增加了一个正交损失 $\mathcal{L}</em>{\text{Orth}}$ 来降低不同技能嵌入之间的余弦相似度，总损失为 $\mathcal{L}(\theta)=\mathcal{L}<em>{\text{SM}}(\theta)+\gamma\mathcal{L}</em>{\text{Orth}}(\theta)$，其中 $\gamma=0.01$。</p>
<p><strong>与现有方法的创新对比</strong></p>
<ol>
<li><strong>技能表示</strong>：不同于VQ等方法隐式学习潜在技能代码，SDP显式地定义了一组人类可理解的、可跨任务组合的原始技能。</li>
<li><strong>条件化机制</strong>：不同于传统语言条件化DP直接将全局指令映射到动作，SDP通过路由器网络显式地为每个状态分配一个具体技能，并利用参数合成（超网络思想）技术，让技能嵌入动态生成策略网络的部分参数，从而更精确地控制动作生成。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真环境中使用了CALVIN和LIBERO两个具有挑战性的基准。CALVIN评估了场景泛化能力（ABCD → D 和 ABC → D），LIBERO评估了在空间关系、物体、目标、长周期四个任务套件上的性能。同时，在真实世界的6-DoF机械臂上设计了9个任务，评估多任务学习和视觉泛化能力。基线方法包括扩散策略（DiffPolicy）、Octo、MDT、MoDE等SOTA扩散策略，以及RoboFlamingo、GR-1、OpenVLA、UniVLA等视觉语言动作（VLA）策略。</p>
<p><strong>关键实验结果</strong>：<br>在CALVIN基准上（表1），SDP在两种设置下均 consistently 优于所有基线。在最具挑战性的ABC → D设置中，SDP完成连续5个指令链的成功率达到76.9%，比之前最好的MoDE（62.4%）高出14.5个百分点，平均完成指令长度从3.92提升至4.49。</p>
<p><img src="https://arxiv.org/html/2601.01948v1/x4.png" alt="CALVIN结果表"></p>
<blockquote>
<p><strong>图4/表1</strong>：在CALVIN基准上的性能对比。SDP在ABCD→D和ABC→D两种设置下，完成1-5个连续任务的序列成功率以及平均完成长度均显著领先于所有基线方法。</p>
</blockquote>
<p>在LIBERO基准上（表2），SDP在四个任务套件上的平均成功率高达96.9%，远超其他方法（如UniVLA的92.5%），尤其在长周期任务（LIBERO-Long）上优势明显（93.8% vs 87.5%）。</p>
<p>在真实世界实验中（对应论文图4），SDP在涉及空间感知、工具使用、语义理解的多任务学习，以及对未见物体、复杂视觉干扰物的泛化任务中，成功率均显著高于对比的DiffPolicy和MoDE基线。</p>
<p><img src="https://arxiv.org/html/2601.01948v1/imgs/bar_chart.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图4</strong>：真实世界9个任务的成功率。SDP（粉色）在多任务学习（前6个任务）和视觉泛化（后3个任务）方面均 consistently 优于基线方法（绿色和橙色），展示了优异的泛化能力和抗干扰鲁棒性。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>如表3所示，作者逐步添加核心组件进行消融研究。基线DP性能很低；添加交叉注意力注入视觉语言信息带来巨大提升；进一步添加先验信息（改进的AdaLN）注入时间步和本体感知带来持续改进；引入原始技能抽象（Skill Abs.）进一步提升性能；最终，采用组合提示集成（CPE）获得最佳性能。这验证了每个设计组件的有效性。此外，对比了不同的技能条件化策略（直接相加、拼接、FiLM），本文提出的技能依赖FFN层（公式4）策略效果最好。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了SDP，一个将细粒度原始技能学习与条件化动作生成相结合的技能条件化扩散策略，有效缓解了全局指令与短期动作间的粒度不匹配问题。</li>
<li>引入了八个跨任务可重用、人类可解释的原始技能，并设计了轻量级路由器网络进行动态技能分配，为机器人学习提供了结构化、可解释的动作空间。</li>
<li>设计了一种新颖的单技能扩散策略，通过技能依赖的FFN层动态参数化，显式地建立了原始技能与底层控制信号之间的依赖关系，实现了更精确的动作生成。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前定义的原始技能集合是预定义且固定的。虽然这八种技能能组合出广泛任务，但在更复杂或新颖的场景中可能需要扩展或发现新的技能。</p>
<p><strong>启示</strong>：SDP为基于技能的机器人学习提供了一个新范式。它表明，在生成式动作规划（如扩散模型）中，引入显式的、可解释的技能层，能够显著提升策略的精确性、泛化能力和可解释性。这启发后续研究可以探索如何自动发现或学习技能，以及如何将技能条件化机制与其他先进的模型架构和训练策略更深度地结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中扩散策略依赖全局指令导致动作生成不对齐的问题，提出技能条件扩散策略SDP。该方法将复杂任务分解为“上移”“开爪”等八个可重用基础技能序列，通过视觉语言模型提取观测与指令的离散表示，并设计轻量路由器网络为每个状态分配单一技能，从而构建技能对齐的动作生成策略。实验表明，SDP在两个仿真基准和真实机器人部署中均优于现有方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.01948" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>