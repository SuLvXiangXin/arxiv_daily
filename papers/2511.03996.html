<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.03996" target="_blank" rel="noreferrer">2511.03996</a></span>
        <span>作者: Mingguo Zhao Team</span>
        <span>日期: 2025-11-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人足球是具身智能的代表性挑战，要求机器人在紧密耦合的感知-动作循环中运作。然而，现有系统通常依赖解耦的模块（将感知、规划、低级控制分离），或专注于孤立技能（如跑动、带球、射门），导致在动态环境中响应延迟、行为不连贯。同时，现实世界的感知局限（如噪声、延迟、视野受限）进一步加剧了这些问题。基于特征的运动模仿方法（如DeepMimic）虽然能复现细节，但依赖严格的时间对齐和显式运动匹配，在需要偏离参考轨迹以达成任务目标的动态场景中缺乏适应性。尽管已有工作尝试用统一强化学习策略学习人形足球行为，甚至引入视觉，但其反应性不足且严重依赖特定建模的视觉环境，泛化到真实比赛条件受限。</p>
<p>本文针对“在无约束环境中实现连贯、鲁棒的视觉驱动反应式足球技能”这一开放挑战，提出了新视角：将对抗运动先验（AMP）框架扩展到真实世界动态环境的感知设置中，直接整合视觉感知与运动控制。其核心思路是：通过一个结合了编码器-解码器架构和虚拟感知系统的统一强化学习控制器，让策略从有噪声的观测中恢复特权状态，并建立感知与动作之间的主动协调，从而学习反应式的足球技能。</p>
<h2 id="方法详解">方法详解</h2>
<p>该方法整体框架包含仿真训练与零样本真实世界部署。在仿真中，策略通过强化学习训练，目标是将球踢向球门，并受到AMP的指导以模仿专家足球运动数据。策略网络接收包含历史信息的观测向量，输出关节位置命令。真实部署时，机器人通过机载相机获取图像，经检测处理后投影到鸟瞰图空间，提取紧凑的球检测和球门信息（由里程计模块提供）作为策略输入。</p>
<p><img src="https://arxiv.org/html/2511.03996v1/x1.png" alt="系统总览"></p>
<blockquote>
<p><strong>图1</strong>：系统概览。真实机器人配备机载相机进行视觉感知。图像检测结果被投影到鸟瞰图空间。球检测信息直接提供给策略，而场地标志线则由里程计模块处理，从长期信息中推断球门位置。该感知流程旨在为RL策略高效提取和表示视觉特征。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><strong>基于对抗运动先验（AMP）的模仿学习</strong>：采用GAN范式，使用判别器区分策略生成的运动片段与专家演示数据片段，提供隐式的、无需时间对齐的奖励信号。这使策略能灵活地将任务目标（踢球）与模仿人类足球运动风格相结合，避免了基于特征模仿方法的刚性。</li>
<li><strong>编码器-解码器网络架构</strong>：策略网络采用编码器-解码器设计。<ul>
<li><strong>编码器</strong>：将包含历史观测（本体感觉、球检测、球门信息）的序列压缩为与控制相关的潜在表征，用于感知-动作耦合。</li>
<li><strong>解码器</strong>：从有噪声、部分可观的输入中恢复（解码）出特权状态（如球的地面真实位置）。该设计使策略能够对感知数据进行降噪和预测，即使在球短暂离开视野时也能估计其轨迹，指导主动搜索和跟踪行为。</li>
</ul>
</li>
<li><strong>虚拟感知系统</strong>：在仿真中引入，用于模拟真实机载视觉的关键特性（如检测噪声、视野限制），并向策略提供与真实部署流程一致的紧凑检测线索（而非原始图像）。这有效缩小了仿真与现实的差距，并促进了策略对感知噪声的鲁棒性和主动感知协调能力。</li>
<li><strong>多评论家框架</strong>：采用多个评论家网络来分别估计不同奖励项（如任务奖励、AMP奖励、球跟踪奖励）的价值，以减轻不同奖励目标之间的干扰，稳定训练过程。</li>
</ol>
<p>与现有方法相比，创新点主要体现在：1) 将原本用于本体感觉模仿的AMP框架，成功扩展至需要外感受传感器（视觉）的动态任务中；2) 通过编码器-解码器网络处理感知噪声和部分可观测性，实现状态估计和主动协调；3) 设计虚拟感知系统，在训练中模拟真实感知特性，有效促进感知-动作协调并实现sim-to-real迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台为Booster T1人形机器人（高1.2米，重30公斤），搭载Intel RealSense D435i相机和Jetson AGX Orin板载计算机。策略在仿真中训练，并零样本部署到真实机器人。评估遵循RoboCup成人尺寸人形联盟标准。对比的基线方法是当前RoboCup顶尖水平（亚军团队）使用的<strong>基于规则的策略</strong>，该策略依赖机器人内置步行控制器和预编程的行为树来生成决策速度命令。</p>
<p>关键实验结果总结如下：</p>
<ol>
<li><strong>真实场景鲁棒性能</strong>：控制器在多种地面（草地、石板、土壤、沥青、橡胶）和视觉条件（颜色、光照变化）下均表现出鲁棒的适应能力，能可靠地进行球跟踪和移动，甚至在缺乏场地标志时也能依靠本体感觉里程计维持性能。在RoboCup 2025和2025世界人形机器人游戏中，搭载该控制器的团队赢得冠军，共打进76球仅失11球。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.03996v1/x2.png" alt="多种场景性能"></p>
<blockquote>
<p><strong>图2</strong>：控制器在多种场景下的性能。(A-F) 杂乱环境中带有干扰的真实比赛表现。(G-I) 对球的反应式响应和实时适应。(J-L) 在不同地形和视觉多样化环境中的鲁棒行为。</p>
</blockquote>
<ol start="2">
<li><strong>踢球成功率</strong>：在仿真和真实硬件上测试了球在场地上不同起始位置的踢球成功率。如图3A所示，靠近球门的区域成功率极高。随着距离增加或与球门法线方向的角度偏移增大，性能下降，主要源于对踢球角度精度的容差要求更严。硬件实验的成功率与仿真结果高度吻合，验证了虚拟感知系统有效缩小了sim-to-real差距。所有测试中机器人均未摔倒。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.03996v1/x3.png" alt="验证与行为分析"></p>
<blockquote>
<p><strong>图3A</strong>：成功率热图。背景网格颜色代表8192次仿真测试的成功率，圆点代表10次连续硬件测试的成功率。策略在硬件上提供了与仿真结果紧密匹配的可靠性能。</p>
</blockquote>
<ol start="3">
<li><strong>感知-动作协调分析</strong>：<ul>
<li><strong>主动感知</strong>：策略能主动调整躯干和头部方向，使球大部分时间保持在相机视野内（图4C）。即使球滚向视野边缘，策略也能自适应地朝球离开的方向旋转，将其重新置于视野中以持续跟踪（图4D）。</li>
<li><strong>状态估计降噪</strong>：策略解码器对球位置的估计显著降低了原始感知的噪声，在踢球前最后1秒内的最小RMSE从0.344米降至0.186米（图4B）。考虑到机器人足弓长度仅0.23米，这一改进对精确定位和可靠击球至关重要。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.03996v1/x4.png" alt="感知-动作协调"></p>
<blockquote>
<p><strong>图4</strong>：感知-动作协调分析。(B) 策略的球位置估计误差显著低于原始感知。(C) 球与相机中心角度距离的分布，大部分时间球在视野内。(D) 当球移出视野时，策略的估计能预测球运动，引导机器人重新获取视觉检测。</p>
</blockquote>
<ol start="4">
<li><strong>步态行为多样性</strong>：通过UMAP对策略生成的关节空间轨迹降维可视化，揭示了5个不同的步态簇（行走、左转、右转、左脚踢、右脚踢），表明单一策略能产生多样行为并平滑切换（图5）。策略轨迹覆盖了参考运动数据集，并超越了它，合成了任务特异性行为（如当球门在身后时出现的支撑脚旋转钩踢）。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.03996v1/x5.png" alt="多样化步态行为"></p>
<blockquote>
<p><strong>图5</strong>：使用UMAP将策略生成的关节轨迹和参考运动数据降维至2D可视化。五个不同的簇显示了策略整合参考运动并泛化生成任务特异性行为的能力。</p>
</blockquote>
<ol start="5">
<li><strong>与规则基线的定量比较</strong>：在机器人从距离球1.5米处启动的场景下对比（图6）。<strong>学习策略在所有方向上的踢球时间更短且变化更小</strong>（规则方法在目标方向位于机器人后方时需要约5秒，而学习策略时间短且稳定），<strong>最大角速度更高</strong>，表明其敏捷性更强。这归功于学习策略能动态调整脚部放置，无缝衔接接近球和踢球动作。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.03996v1/x6.png" alt="与规则基线的比较"></p>
<blockquote>
<p><strong>图6</strong>：学习策略与规则基线的比较。(A, B) 学习策略在所有接近角度下，从启动到触球的时间更短，且该过程中的最大角速度更高，显示出更优的敏捷性。</p>
</blockquote>
<ol start="6">
<li><strong>适应性步态</strong>：策略能根据球的距离和运动状态调整步态（图3D,E）。距离球较远时步频较慢以保持跟踪并节省能量；接近球时步频加快、步幅缩短，以精细调整脚部落点。在追逐滚动球时，能触发快速的侧向步或极速旋转机动（图7）。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.03996v1/x7.png" alt="追逐滚动球的敏捷步态"></p>
<blockquote>
<p><strong>图7</strong>：追逐滚动球时的敏捷步态行为。(A) 球横向滚动时，敏捷的侧向移动和踢球。(B) 球滚向机器人后方时，敏捷的转身和踢球。</p>
</blockquote>
<p><strong>消融实验分析</strong>：图4B表明，移除解码器（或将其与策略分开训练）会导致球位置估计停留在噪声水平，验证了编码器-解码器集成设计对状态估计和降噪的重要性。同时，虽然移除显式的球跟踪奖励后主动感知和踢球行为仍能保留，但加入该奖励能使机器人将球保持在视野中心附近，提高了真实部署的鲁棒性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>将对抗运动先验（AMP）框架成功扩展到视觉驱动的动态任务中</strong>，实现了在真实世界动态环境下结合视觉反馈与运动模仿的学习；2) <strong>提出了编码器-解码器架构与虚拟感知系统</strong>，使策略能够从有噪声的观测中恢复状态、预测球运动，并主动协调感知与动作，有效解决了感知噪声与精确运动控制之间的错位问题，并促进了sim-to-real迁移；3) <strong>通过单一阶段训练的统一策略</strong>，在严格约束的真实机器人竞赛中展示了其高效性，实现了反应式的搜球、追球、多方向踢球等连贯行为。</p>
<p>论文提到的局限性包括：在球位于机器人与球门连线上或非常靠近机器人时成功率较低，因为前者难以选择踢球脚，后者则限制了精确定位和步态适应的时间。此外，策略依赖于紧凑的状态表示（而非原始图像），这虽然有利于泛化，但也可能限制其处理更复杂视觉场景的能力。</p>
<p>这项工作对后续研究的启示包括：探索将更复杂的视觉输入（如原始或特征图像）直接融入此类反应式控制框架；将方法扩展到处理多智能体交互和团队协作足球场景；进一步研究在部分可观测动态任务中，感知表征学习与运动技能学习之间的耦合机制。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>这篇论文针对人形机器人足球中感知与动作模块解耦导致的响应延迟、行为不连贯问题，提出一种基于强化学习的统一控制器。方法核心是扩展**Adversarial Motion Priors**至现实动态感知场景，并引入**编码器-解码器架构**与**虚拟感知系统**，从有噪、受限的视觉观测中恢复特权状态，实现感知与动作的主动协调。最终控制器在包括真实RoboCup比赛在内的多种场景中，展现出强大的反应能力和鲁棒的足球技能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.03996" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>