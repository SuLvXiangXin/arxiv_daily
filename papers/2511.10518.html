<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.10518" target="_blank" rel="noreferrer">2511.10518</a></span>
        <span>作者: Liqiang Nie Team</span>
        <span>日期: 2025-11-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉-语言-动作（VLA）的模型在机器人操作任务中取得了进展，但实际部署仍面临两大关键瓶颈。首先，<strong>视觉感知冗余</strong>：主流VLA框架（如OpenVLA、CoT-VLA等）采用通用的、与指令无关的视觉编码器（如ViT、CLIP），对观察到的所有像素进行统一处理，导致背景杂波、任务无关干扰和环境噪声被不加区分地编码，带来了巨大的计算开销，并稀释了对任务关键线索的注意力。其次，<strong>指令-视觉语义对齐肤浅</strong>：大多数VLA模型仅依赖大语言模型进行通用的跨模态对齐，难以捕捉机器人操作中复杂的语义关系，无法识别全局动作线索、局部语义锚点以及结构化指令-空间依赖关系，限制了模型在复杂环境中的任务落地能力。</p>
<p>本文针对上述感知冗余和语义对齐不足的痛点，提出了一个名为SemanticVLA的新框架。其核心思路是：通过<strong>语义对齐的剪枝与增强</strong>，动态地筛选与任务最相关的视觉证据，并建立紧密的感知-动作对应关系，从而在提升任务性能的同时，显著降低计算成本。</p>
<h2 id="方法详解">方法详解</h2>
<p>SemanticVLA的整体框架旨在处理输入上下文 $\mathbf{X} = {\mathcal{V}, \mathbf{q}, \ell}$（视觉观察、机器人本体状态、语言指令），并预测未来 $K$ 个动作 $\mathbf{A}$。其核心创新在于三个集成模块：语义引导的双视觉剪枝器（SD-Pruner）、语义互补的分层融合器（SH-Fuser）和语义条件的动作耦合器（SA-Coupler）。</p>
<p><img src="https://arxiv.org/html/2511.10518v1/figures/framework6.1.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：SemanticVLA框架总览。观测通过两条并行路径处理：基于SigLIP的指令驱动剪枝器进行指令感知编码，以及基于DINOv2的空间聚合剪枝器进行空间感知编码。两者通过共享的语义互补分层融合器紧密融合。动作输入通过语义条件动作耦合器初始化，以优化大语言模型中从稀疏化感知到动作类型的转换。</p>
</blockquote>
<p><strong>1. 语义引导的双视觉剪枝器（SD-Pruner）</strong><br>该模块利用编码器专长——SigLIP用于指令落地，DINOv2用于空间几何，对两个编码器进行独立剪枝。</p>
<ul>
<li><strong>指令驱动剪枝器（ID-Pruner）用于SigLIP</strong>：计算指令嵌入与SigLIP视觉token之间的余弦相似度矩阵 $\mathbf{S}$，通过两条互补路径进行剪枝。<ul>
<li><strong>视觉到语言映射</strong>：聚合每个指令token与所有视觉token的相似度，选出最显著的top-$k$个指令token（如目标名词、动作动词），并聚合其对应的视觉token，形成全局动作线索特征 $\mathcal{V}^{\text{VL}}$。这解决了“知道目标但不知步骤”的问题。</li>
<li><strong>语言到视觉过滤</strong>：计算每个视觉token对所有指令token的总体响应强度，选出最相关的top-$h$个视觉token，形成局部语义锚点特征 $\mathcal{V}^{\text{LV}}$。这缓解了“看不见就做不到”的问题。</li>
<li>最终输出为两者的并集：$\mathcal{V}^{\text{VL}} \cup \mathcal{V}^{\text{LV}}$，平衡了全局线索与局部锚点。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2511.10518v1/figures/detail3.png" alt="ID-Pruner与SA-Coupler细节"></p>
<blockquote>
<p><strong>图3</strong>：左：SigLIP的指令驱动剪枝器（ID-Pruner）示意图，展示了视觉到语言映射和语言到视觉过滤的双路径。右：语义条件动作耦合器（SA-Coupler）示意图，将7-DoF动作分解为平移、旋转、夹持三个语义token。</p>
</blockquote>
<ul>
<li><strong>空间聚合剪枝器（SA-Pruner）用于DINOv2</strong>：在DINOv2视觉token后追加一组零初始化的聚合token $\mathcal{V}^{\text{Agg}}$。通过一个轻量级的FiLM层，利用池化后的指令表示生成缩放和偏移参数 $(\gamma, \beta)$，对视觉和聚合token进行调制，使密集的空间特征能根据任务上下文动态调整，并最终聚合到 $\mathcal{V}^{\text{Agg}}$ 上，形成几何丰富且与语义相关的紧凑表示。</li>
</ul>
<p><strong>2. 语义互补的分层融合器（SH-Fuser）</strong><br>该模块分层地集成来自ID-Pruner的稀疏语义特征和来自SA-Pruner的密集几何特征，而非简单的后期拼接。</p>
<ul>
<li><strong>密集融合器</strong>：在SigLIP和DINOv2编码器的多个Transformer块之间（如浅层、中层、深层）插入融合操作。通过MLP融合对应块的视觉特征，确保语义线索在每一阶段都得到空间几何先验的增强。</li>
<li><strong>稀疏融合器</strong>：在最终阶段，将ID-Pruner输出的 $\mathcal{V}^{\text{LV}}$ 与SA-Pruner输出的 $\mathcal{V}^{\text{Agg}}$ 进行合并，通过MLP形成统一的紧凑表示 $\mathbf{Z}^{\text{Fusion}}$。这种设计将视觉token减少了8-16倍，同时利用了语义落地和几何精度的互补优势。</li>
</ul>
<p><strong>3. 语义条件动作耦合器（SA-Coupler）</strong><br>此模块革新了传统的视觉到动作映射流程。</p>
<ul>
<li><strong>Token级语义对齐</strong>：摒弃将7-DoF动作（3平移、3旋转、1夹持）离散为7个独立token的做法，改为用3个语义token分别表示平移、旋转和夹持这三种基本运动原语：$\mathbf{0}_i = {\mathbf{t}_i^0, \mathbf{r}_i^0, \mathbf{g}_i^0}$。</li>
<li><strong>Head级模块化动作预测</strong>：将融合后的视觉表示 $\mathbf{Z}$、本体状态 $\mathbf{q}$、指令 $\ell$ 以及 $K$ 组动作placeholder输入大语言模型进行并行解码。解码后，使用三个专门化的预测头（平移头、旋转头、夹持头）分别回归连续的运动参数。这种结构化的表述使得动作解码更高效且更可解释。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：仿真评估在LIBERO基准上进行，包含Spatial、Object、Goal、Long四个任务套件，共2000条演示。真实世界实验在AgileX Cobot Magic平台上进行，涵盖物体放置、抽屉操作、多步可变形物体任务等。对比的基线包括OpenVLA、OpenVLA-OFT、PD-VLA、$\pi_0$、STAR等SOTA方法。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>仿真性能</strong>：如表1所示，SemanticVLA在LIBERO基准上取得了最高的总体成功率（97.7%，排名第1），相比OpenVLA提升了21.1%。其轻量版SemanticVLA-Lite也达到95.8%（排名第3）。</p>
<blockquote>
<p><strong>表1</strong>：仿真结果。在LIBERO基准的四个套件中比较任务成功率（SR）和排名（RK）。SemanticVLA在所有套件中均取得最佳或接近最佳成绩。</p>
</blockquote>
</li>
<li><p><strong>仿真效率</strong>：如表2所示，SemanticVLA在显著提升性能的同时，大幅降低了计算开销。其视觉输入token仅为基线（256个）的1/8（32个），动作token从7个减至3个。训练成本降低约3.0倍（3.9小时 vs. 11.7小时），推理延迟降低约2.7倍（0.089秒 vs. 0.240秒），吞吐量提升超过21倍（89.9 Hz vs. 4.2 Hz）。</p>
<blockquote>
<p><strong>表2</strong>：仿真效率结果。比较视觉/动作token数、FLOPs、训练成本、延迟和吞吐量。SemanticVLA在效率和性能间取得了最佳权衡。</p>
</blockquote>
</li>
<li><p><strong>真实世界性能</strong>：如表3所示，在三个具有挑战性的长视野真实任务中，SemanticVLA取得了77.8%的总体成功率，优于最佳基线OpenVLA-OFT（55.6%）22.2个百分点。</p>
<blockquote>
<p><strong>表3</strong>：真实世界结果。在AgileX Cobot Magic平台的任务和子任务成功率比较。SemanticVLA在所有任务上领先。</p>
</blockquote>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.10518v1/figures/vis1.png" alt="真实任务可视化"></p>
<blockquote>
<p><strong>图4</strong>：SemanticVLA在三个长视野真实世界任务操作过程的可视化，展示了关键执行阶段的观测。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><p><strong>SD-Pruner组合</strong>（表4）：SigLIP搭配ID-Pruner（指令驱动剪枝）与DINOv2搭配SA-Pruner（空间聚合剪枝）的组合效果最佳（97.1%），验证了针对不同编码器特性设计专用剪枝策略的必要性。</p>
<blockquote>
<p><strong>表4</strong>：SD-Pruner消融。验证了SigLIP+ID-Pruner与DINOv2+SA-Pruner组合的有效性。</p>
</blockquote>
</li>
<li><p><strong>剪枝比率</strong>（表5）：选择8倍的剪枝比率（R=8）在性能（97.7%）和效率间取得了最佳平衡。R=16定义了轻量版（95.8%）。与同等压缩水平的即插即用剪枝方法（FastV， SliME）相比，SemanticVLA性能显著更优，说明其指令感知剪枝与结构保持融合的有效性。</p>
<blockquote>
<p><strong>表5</strong>：剪枝比率消融及与其它剪枝方法的比较。显示了不同压缩率下的性能-效率权衡。</p>
</blockquote>
</li>
<li><p><strong>注意力可视化</strong>（图5）：展示了ID-Pruner捕获的全局动作线索、局部语义锚点，以及SA-Pruner聚合token关注的空间特征，直观体现了模块的语义对齐能力。<br><img src="https://arxiv.org/html/2511.10518v1/figures/vis2.png" alt="注意力可视化"></p>
<blockquote>
<p><strong>图5</strong>：注意力可视化。1) 视觉到语言token对图像块的注意力，捕获全局动作线索；2) 选中的语言到视觉token集（高亮块为局部语义锚点）；3) 聚合token对图像块的注意力，反映通过HF-Fuser补充ID-Pruner的空间特征。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>语义引导的双视觉剪枝器（SD-Pruner）</strong>，通过指令感知的token过滤和几何感知的聚合，针对性地剪枝SigLIP和DINOv2编码器，显著消除了感知冗余。</li>
<li>设计了<strong>语义互补的分层融合器（SH-Fuser）</strong>，通过密集与稀疏双流融合，在编码全过程整合语义与空间几何信息，增强了表示的一致性。</li>
<li>引入了<strong>语义条件动作耦合器（SA-Coupler）</strong>，将动作输出结构化为语义类型，实现了从稀疏化感知到动作的更直观、高效映射。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在高度动态和快速变化的环境中，模型的泛化能力仍需进一步验证；此外，当前方法依赖于预训练的SigLIP和DINOv2编码器，其固有的偏见可能影响在极端未见场景下的表现。</p>
<p><strong>后续启示</strong>：本文工作表明，在VLA模型中，<strong>紧耦合的语义对齐是同时提升性能与效率的关键</strong>。未来的研究可沿以下方向深入：探索更自适应的剪枝策略以应对极端环境；将语义对齐的思想扩展到多模态输入（如触觉、音频）；以及研究如何将此类高效VLA模型部署到资源更受限的嵌入式平台。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中VLA模型存在的**感知冗余**与**指令-视觉语义对齐表浅**两大核心问题，提出了SemanticVLA框架。其关键技术包括：**SD-Pruner**进行指令引导的视觉特征稀疏化，**SH-Fuser**融合语义与几何特征，**SA-Coupler**增强感知到动作的转换。实验表明，该框架在LIBERO基准上的成功率比OpenVLA提升21.1%，同时训练成本和推理延迟分别降低3.0倍和2.7倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.10518" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>