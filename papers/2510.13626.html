<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.13626" target="_blank" rel="noreferrer">2510.13626</a></span>
        <span>作者: Xipeng Qiu Team</span>
        <span>日期: 2025-10-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前视觉-语言-动作（VLA）模型在标准化机器人操作基准测试（如LIBERO）上报告了令人印象深刻的成功率，但这些高分可能掩盖了模型在鲁棒性方面的根本弱点。主流评估方法侧重于静态、理想条件下的聚合成功率，虽然为比较不同方法提供了有价值的基线，但未能捕捉学习到的策略在现实变化下的稳定性和可靠性。这掩盖了模型处理现实任务设置中固有细微变化的能力不足，例如在相机视角或机器人初始配置发生轻微偏移时，模型可能失效。</p>
<p>本文针对VLA模型在基准测试高分与真实能力之间存在巨大差距这一具体痛点，提出了系统化的脆弱性分析新视角。核心思路是：通过引入对七个关键维度（对象布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声）的受控扰动，全面诊断当前先进VLA模型的鲁棒性，揭示其表面能力之下的一致脆弱性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文并非提出新的VLA模型，而是构建了一个系统化的分析框架来评估现有模型的鲁棒性。整体流程（pipeline）是：首先定义七类单维度扰动，然后在LIBERO基准任务上，将这些扰动分别应用于多个代表性VLA模型的评估过程，观察并量化模型性能（任务成功率）的变化。输入是施加了特定扰动的任务场景（视觉观测、语言指令），输出是模型在该扰动下的任务成功率。</p>
<p>核心“模块”是七种精心设计的扰动因素，具体作用和技术细节如下：</p>
<ol>
<li><strong>对象布局</strong>：添加干扰对象和/或移动目标对象的位置。</li>
<li><strong>相机视角</strong>：改变第三人称相机的视角/位姿和视野。</li>
<li><strong>机器人初始状态</strong>：改变机械臂的初始姿态。</li>
<li><strong>语言指令</strong>：重写任务指令以增加语言的丰富性和复杂性。</li>
<li><strong>光照条件</strong>：改变光照强度、方向、颜色和阴影模式。</li>
<li><strong>背景纹理</strong>：修改桌子/场景的纹理和材质。</li>
<li><strong>传感器噪声</strong>：向输入图像注入光度畸变（如抖动、高斯模糊）。</li>
</ol>
<p>与现有评估方法相比，创新点具体体现在：1）<strong>系统性</strong>：首次同时对七个现实且关键的扰动维度进行大规模、受控的分析；2）<strong>诊断性</strong>：不仅报告性能下降，还通过设计极端实验（如输入空白指令、遮挡相机视图）深入探究脆弱性根源；3）<strong>组合性分析</strong>：超越单维度扰动，研究多维度扰动组合下的组合泛化差距。</p>
<p><img src="https://arxiv.org/html/2510.13626v3/x6.png" alt="LIBERO-Plus基准架构"></p>
<blockquote>
<p><strong>图6</strong>：LIBERO-Plus基准的架构。该基准包含10,030个任务，组织在七个扰动因子和二十一个底层组件之下。它是通过系统性地扩展原始LIBERO基准并基于模型表现进行难度分级构建而成。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用LIBERO仿真基准作为诊断工具。分析了10个代表性的开源VLA模型，涵盖自回归与扩散式架构，以及网络数据协同训练、世界模型、强化学习等多种训练范式，包括OpenVLA及其变体、π0及其变体、Nora、WorldVLA、UniVLA和RIPT-VLA。</p>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>整体脆弱性显著</strong>：如表1所示，即使轻微的扰动也可能导致性能急剧下降。例如，OpenVLA在原始成功率76.5%的情况下，相机视角扰动使其成功率降至1.1%，绝对下降75.4个百分点；机器人初始状态扰动使其成功率降至4.1%，下降72.4个百分点。</li>
<li><strong>对语言指令不敏感</strong>：语言扰动导致的平均性能下降最小（-25.3），这并非源于强大的语言泛化能力。后续实验表明，模型往往忽略语言指令。</li>
<li><strong>手腕相机增强鲁棒性</strong>：包含手腕（第一人称）相机的模型（如OpenVLA-OFT）对相机视角和光照变化表现出更强的鲁棒性。</li>
<li><strong>位置偏见而非语义理解</strong>：模型能忽略干扰物，但当目标物体位置改变时性能骤降，表明其依赖记忆的位置线索而非不变的物体语义。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.13626v3/x1.png" alt="对象布局扰动鲁棒性"></p>
<blockquote>
<p><strong>图1</strong>：模型在对象布局扰动下的鲁棒性。左、中图分别展示了在添加干扰物体和目标物体位移两种子扰动下的成功率，右图为总体鲁棒性。结果表明模型对目标物体位移极其敏感，揭示了其位置偏见。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.13626v3/x2.png" alt="光照鲁棒性与极端消融测试"></p>
<blockquote>
<p><strong>图2</strong>：光照鲁棒性与极端消融测试。“Light”表示施加光照扰动。“3rd Black”和“All Black”分别表示仅遮挡第三人称视图和遮挡所有相机输入。结果显示，手腕视图提供了对光照变化稳定的几何和接触线索。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.13626v3/x3.png" alt="指令移除与目标修改任务准确率"></p>
<blockquote>
<p><strong>图3</strong>：不同模型在指令移除(a)和目标修改(b)任务上的准确率。浅色条：有语言指令的原始成功率；(a)深色条：移除指令后的成功率；(b)深色条：任务目标及指令被替换后的成功率。实验证明模型在很大程度上忽略了语言输入。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.13626v3/x4.png" alt="配对扰动下的条件概率热图"></p>
<blockquote>
<p><strong>图4</strong>：配对扰动下的条件概率热图。上三角条目表示基于独立性的单维度概率乘积，下三角条目显示实际联合扰动下的结果。普遍存在的负组合泛化差距（Δij &lt; 0）表明扰动之间存在负交互效应。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.13626v3/x5.png" alt="不同难度级别下的模型性能趋势"></p>
<blockquote>
<p><strong>图5</strong>：模型性能随四种扰动维度强度增加的趋势。折线图显示了每个模型在五个难度级别（Level-1至Level-5）上的成功率，表明随着扰动难度增加，所有模型性能均下降。</p>
</blockquote>
<p><strong>消融实验与贡献</strong>：本文通过一系列诊断性实验揭示了各因素的贡献。例如，通过“空白指令”实验，证明了语言模态常被忽略；通过“全黑/第三人称黑”视图实验，证明了手腕相机对光照鲁棒性的关键作用；通过分解“对象布局”扰动，揭示了模型的位置偏见。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>系统化的脆弱性分析</strong>：首次对当前先进VLA模型在七个关键现实维度上的鲁棒性进行了大规模、受控的评估，揭示了其表面高能力之下普遍存在的严重脆弱性。</li>
<li><strong>深入的诊断性见解</strong>：通过设计巧妙的实验（如移除指令、遮挡视图），深入揭示了脆弱性的根源，包括对语言指令的忽视、对位置线索的过度依赖以及手腕相机对视觉鲁棒性的关键作用。</li>
<li><strong>提出LIBERO-Plus基准与改进方向</strong>：基于分析构建了一个包含10,030个任务的增强基准（LIBERO-Plus），并证明使用广义数据集进行训练能显著提升模型鲁棒性（如将相机视角鲁棒性从55.6%提升至92.8%）。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括分析主要在模拟环境中进行，虽然扰动设计旨在贴近现实，但与真实物理世界的不确定性仍存在差距。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>评估实践的转变</strong>：研究结果强烈挑战了“高基准分数等于真正能力”的假设，呼吁社区重新评估当前实践，开发能够系统评估模型在现实变异下可靠性的基准和方法。</li>
<li><strong>模型设计的重点</strong>：未来VLA模型的设计需要更加注重对多模态信息的均衡利用（特别是语言指令）、对不变语义（而非表层线索）的学习，以及整合能够提供稳定信息（如手腕相机）的传感模态。</li>
<li><strong>数据与训练策略</strong>：采用多样化的数据分布和强调泛化的训练策略（如协同训练）被证明对鲁棒性至关重要，这为构建更强大的VLA模型指明了数据工程和算法优化的方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在基准测试中高成功率掩盖鲁棒性不足的问题，提出LIBERO-Plus分析框架。方法上，基于LIBERO基准引入七维度受控扰动（物体布局、相机视角、机器人初始状态等），对多个先进模型进行系统性脆弱性分析。核心发现表明，模型对相机视角、初始状态等扰动极度敏感，性能可从95%骤降至30%以下；同时模型倾向于完全忽略语言指令，揭示了当前VLA模型在真实变化下的脆弱本质。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.13626" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>