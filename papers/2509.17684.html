<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.17684" target="_blank" rel="noreferrer">2509.17684</a></span>
        <span>作者: Zidong Chen Team</span>
        <span>日期: 2025-09-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉的机器人策略学习通常采用在ImageNet等数据集上进行监督预训练的视觉编码器作为主干网络，例如ResNet-18/34。这类方法依赖于大规模的人工标注数据。与此同时，自监督视觉模型（如DINO、DINOv2）已在多种计算机视觉任务上展现出强大的无标签学习与特征迁移能力，但在机器人视觉运动策略学习领域，尚未有研究系统性地将其作为核心视觉主干进行评估。特别是最新、性能更强的DINOv3模型，其在机器人动作扩散策略框架中的潜力尚不明确。</p>
<p>本文针对这一空白，旨在系统评估DINOv3这一先进的大规模自监督视觉主干网络，在视觉运动扩散策略学习中的效能。核心研究问题是：与标准的监督预训练视觉主干（如ResNet-18）相比，引入DINOv3自监督视觉主干如何影响基于动作扩散的策略性能与泛化能力？本文的核心思路是：在一个统一的、基于FiLM条件化的扩散策略框架下，分别在“从头训练”、“冻结预训练权重”和“微调预训练权重”三种典型训练模式下，对比DINOv3与ResNet-18在多个机器人操作任务上的表现，以验证自监督大视觉模型作为感知前端的效果。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架遵循标准的视觉条件化扩散策略学习流程，其核心是将视觉编码器替换为待评估的DINOv3模型。</p>
<p><img src="https://arxiv.org/html/2509.17684v1/pipeline.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：使用DINOv3作为视觉嵌入模型的通用扩散策略训练流程。RGB末端执行器观测图像通过基于DINOv3的图像编码器处理以提取视觉特征，这些特征用于条件化一个U-Net DDPM策略网络 $\epsilon_{\theta}(o,a)$。该策略网络通过 $k$ 个扩散步骤迭代地去噪动作序列，预测噪声 $\Delta E(a)$ 以生成用于机器人控制的一系列动作 $a_{t}, a_{t+1},\ldots,a_{t+n}$。</p>
</blockquote>
<p><strong>整体流程与输入输出</strong>：策略学习以多视角的RGB图像序列（观测 $o$）作为输入。视觉编码器（DINOv3或ResNet-18）将这些图像映射为紧凑的潜在嵌入。该视觉嵌入随后被输入到一个以U-Net为架构的扩散策略网络中，该网络通过一个条件化的去噪过程来建模动作序列 $a$ 的分布。在推理时，策略网络从随机噪声开始，经过多步去噪，输出未来一段时间内的机器人动作序列。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉编码器</strong>：本文对比了两类编码器。一是经典的监督预训练CNN模型ResNet-18（11.7M参数，预训练数据集ImageNet-1K）。二是自监督预训练的Vision Transformer模型DINOv3-small/16（21M参数，预训练数据集为LVD-1689M）。图像被调整为84x84分辨率后输入编码器。</li>
<li><strong>策略网络与条件化方式</strong>：策略网络采用基于CNN的U-Net架构，其条件化机制不是使用计算量较大的多头交叉注意力，而是采用了更稳定的<strong>FiLM（Feature-wise Linear Modulation）</strong>。视觉特征通过FiLM层调制U-Net中间层的特征，以此将观测信息注入到动作生成过程中。</li>
<li><strong>训练策略</strong>：对每个视觉编码器，在三种模式下进行评估：<ul>
<li>**No Pretrained (Scratch)**：编码器权重随机初始化，与策略网络一起从头开始端到端训练。</li>
<li><strong>Frozen</strong>：加载预训练权重（ImageNet-1K for ResNet-18, LVD-1689M for DINOv3），并在训练过程中保持编码器权重冻结，仅作为固定的特征提取器。</li>
<li><strong>Finetuning</strong>：加载预训练权重，并在训练过程中对编码器和策略网络一起进行端到端微调。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：本文的主要创新点并非提出一个全新的策略架构，而是<strong>首次对最先进的自监督视觉模型DINOv3作为视觉运动扩散策略的独立视觉主干，进行了系统性的实证评估与比较</strong>。研究重点在于探究不同训练策略（冻结 vs. 微调）下，自监督预训练模型与监督预训练模型在机器人操作任务上的性能差异，从而论证大规模无标签预训练在机器人感知中的价值。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准任务</strong>：使用了四个来自Robomimic等基准的模拟机器人操作任务：Push-T（动态推动）、Lift（拾取）、Can（开罐）、Square（方块排列）。</li>
<li><strong>实验平台</strong>：在NVIDIA RTX A6000 Ada GPU上进行实验。</li>
<li><strong>对比方法</strong>：核心对比是ResNet-18（监督）与DINOv3-small/16（自监督）在不同训练策略下的表现。</li>
<li><strong>训练细节</strong>：每个实验训练100个epoch，使用FiLM条件化的U-Net扩散策略。评估指标为任务成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>实验结果总结于表I。主要发现如下：</p>
<ol>
<li><strong>微调模式下DINOv3表现优异</strong>：在PushT任务上，微调DINOv3的测试成功率（0.84）超过了微调ResNet-18（0.89）以外的所有配置。在更具挑战性的Can任务上，微调DINOv3取得了0.90的测试成功率，显著高于微调ResNet-18的0.80，实现了<strong>10%的绝对性能提升</strong>。在Square任务上，微调DINOv3（0.90）与取得完美性能（1.00）的微调ResNet-18接近。</li>
<li><strong>冻结DINOv3具有竞争力</strong>：即使在权重冻结、仅作为特征提取器的情况下，DINOv3在多个任务上仍能保持有竞争力的性能。例如在Square任务测试集上，冻结DINOv3成功率为0.60，远高于冻结ResNet-18的0.10。这表明DINOv3通过自监督学习到的特征具有强大的<strong>可迁移先验知识</strong>，无需针对任务进行微调即可提供有效的视觉表征。</li>
<li><strong>任务复杂度的影响</strong>：在简单的Lift任务上，两种主干网络在所有训练策略下均能达到接近完美的成功率，说明任务本身对视觉主干的选择不敏感。</li>
<li><strong>样本效率与鲁棒性</strong>：根据论文正文描述，自监督特征带来了样本效率和鲁棒性的提升。图2的模拟运行截图也直观表明，使用DINOv3的策略在训练过程中收敛更快、行为更精准。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.17684v1/task.png" alt="任务表现对比"></p>
<blockquote>
<p><strong>图2</strong>：来自10,000次优化步骤的模拟运行截图，显示DINOv3模型比ResNet-18变体始终更快、更简洁。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>本文的核心“消融”实验即对比三种训练策略（无预训练、冻结、微调）。结果清晰表明：</p>
<ul>
<li><strong>微调</strong>通常是获得最佳性能的策略，无论是对于监督还是自监督主干。</li>
<li><strong>冻结预训练权重</strong>模式下，DINOv3相比ResNet-18展现出更强的优势，证明了其预训练特征的质量和通用性。</li>
<li><strong>从头训练</strong>的效果普遍最差，突出了预训练（无论监督还是自监督）的重要性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>实证验证</strong>：首次系统性地评估并证明，大规模自监督视觉模型DINOv3可以作为监督预训练视觉主干（如ResNet-18）的有效甚至更优的替代品，用于视觉运动扩散策略学习。</li>
<li><strong>深入比较</strong>：通过对比“冻结”与“微调”两种使用模式，揭示了DINOv3自监督特征具有卓越的可迁移性，即使在不更新权重的情况下也能为策略提供强有力的视觉先验。</li>
<li><strong>性能提升</strong>：在Can等挑战性任务上，微调DINOv3相比ResNet-18带来了最高10%的绝对成功率提升，同时展现出更快的收敛速度和潜在的鲁棒性优势。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，实验均在模拟环境中进行，未来工作需要探索DINOv3及类似模型在<strong>更复杂、长视界的真实世界任务</strong>上的效益。</p>
<p><strong>对后续研究的启示</strong>：<br>本研究为机器人学习领域提供了一个明确的方向：<strong>利用大规模、无标签数据训练的自监督视觉模型作为通用的感知前端</strong>。这可以减少对昂贵人工标注数据的依赖，并可能从更丰富、更多样的网络数据中学习到更具泛化能力的视觉表征。后续工作可以探索更大规模的DINOv3变体（如ViT-B/L）、结合多模态视觉语言模型、或将此范式应用于更广泛的机器人策略学习框架中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究自监督大规模视觉模型DINOv3在机器人视觉运动扩散策略学习中的性能。核心问题是评估其相比传统监督式预训练骨干网络（如ResNet-18）在策略性能与泛化能力上的表现。方法上，论文在统一的FiLM条件扩散策略框架下，对DINOv3进行了从头训练、冻结和微调三种模式的测试。实验表明，微调后的DINOv3在多个基准任务上匹配或超越了ResNet-18，例如在Can任务上实现了高达10%的绝对成功率提升；冻结的DINOv3也表现优异，证明了其强大的可迁移先验知识。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.17684" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>