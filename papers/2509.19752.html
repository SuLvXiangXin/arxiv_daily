<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19752" target="_blank" rel="noreferrer">2509.19752</a></span>
        <span>作者: Yang, Rushuai, Wei, Hangxing, Zhang, Ran, Feng, Zhiyuan, Chen, Xiaoyu, Li, Tong, Zhang, Chuheng, Zhao, Li, Bian, Jiang, Su, Xiu, Chen, Yi</span>
        <span>日期: 2025/09/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型展现出强大的泛化能力，但其性能严重依赖大规模人类演示数据集。人工遥操作收集数据成本高昂、劳动密集，从根本上限制了VLA模型的扩展性。强化学习为自主生成演示数据提供了潜在途径，但传统的RL算法在处理长视野、稀疏奖励的机器人操作任务时，常因探索困难导致学习过程不稳定，生成高方差、次优的轨迹。本文针对人工数据收集瓶颈以及传统RL生成数据质量不佳的具体痛点，提出利用改进的扩散策略优化算法来生成高质量、低方差的轨迹，构建一个扩散RL驱动的VLA训练流程。本文的核心思路是：利用扩散模型强大的表达能力捕捉人类演示的多模态分布作为预热，再通过在线RL优化获得最优策略，其迭代去噪过程能隐式正则化动作空间，生成平滑、一致的高质量数据用于训练VLA模型。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的端到端流程包含三个阶段：1）训练专家RL策略：为每个任务训练一个基于扩散模型的专家策略；2）生成合成数据集：部署收敛的专家策略自主收集最优轨迹数据集；3）训练通用VLA模型：使用合成数据集微调通用VLA模型。方法的核心是采用条件扩散模型作为策略架构。</p>
<p><img src="https://arxiv.org/html/2509.19752v2/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。包含三个阶段：训练专家扩散RL策略（左）、使用专家策略生成合成数据集（中）、利用合成数据训练通用VLA模型（右）。</p>
</blockquote>
<p><strong>第一阶段：基于多模态行为克隆的预热</strong>。直接使用原始RL训练长视野任务具有挑战性。本文首先利用少量多模态人类演示数据，通过行为克隆预热扩散策略。扩散策略 $\pi_{\theta}$ 通过一个噪声预测网络 $\epsilon_{\theta}(a_{k}, s_{t}, k)$ 对动作块 $a_0$ 的分布进行建模，其训练采用简化的扩散目标 $\mathcal{L}_{\text{BC}}(\theta)$，使策略能够有效捕捉人类行为的复杂多模态分布，为后续RL阶段提供高质量的起点。</p>
<p><strong>第二阶段：使用PPO进行在线强化学习</strong>。预热后，使用在线RL（具体为PPO）来最大化期望累积奖励。由于扩散策略 $\pi_{\theta}(a_0|s_t)$ 的概率难以直接计算，本文将去噪过程视为一个子决策轨迹，利用单步转移似然 $p_{\theta}(a_{k-1}|a_k, s_t)$ 来应用策略梯度方法。同时训练价值网络 $V_{\phi}(s)$ 以最小化时序差分误差 $\mathcal{L}<em>{\text{Value}}(\phi)$。扩散策略 $\pi</em>{\theta}$ 通过最大化PPO-clip目标 $\mathcal{L}_{\text{Policy}}(\theta)$ 进行更新，其梯度在环境交互期间生成的每个动作的去噪步骤序列上计算。</p>
<p><strong>稳定化RL过程的关键设计</strong>。直接优化上述目标存在稳定性挑战。为此，本文提出了多项关键改进：</p>
<ol>
<li><strong>网络架构</strong>：采用ResNet主干与U-Net解码器的组合，而非ViT+MLP。ResNet在低数据量下样本效率更高，U-Net能有效建模人类演示的多模态性。此外，通过FiLM机制集成本体感知状态信息，为RL更新提供更稳定的条件信号。</li>
<li><strong>高效动作采样</strong>：将标准的随机DDPM采样器替换为更快的确定性DDIM采样器，在RL阶段仅使用5个去噪步骤来生成每个动作，这降低了计算开销和动作方差，稳定了策略梯度。</li>
<li><strong>训练策略</strong>：采用余弦退火学习率调度，初期鼓励探索，后期稳定收敛并保留BC阶段学到的先验。同时，使用大量并行环境来确保经验回放缓冲区具有足够大且多样化的数据批次，防止高容量扩散策略因小批次相关数据而陷入模式崩溃。</li>
</ol>
<p><strong>第三阶段：在生成数据上训练VLA</strong>。收敛的专家扩散RL策略用于生成合成数据集 $\mathcal{D}<em>{\text{RL}}$。随后，通过最小化行为克隆损失 $\mathcal{L}</em>{\text{VLA}}$ 来训练VLA模型 $\Pi_{\text{VLA}}$，即最大化生成数据中专家动作的对数似然。</p>
<p><img src="https://arxiv.org/html/2509.19752v2/x2.png" alt="扩散RL框架"></p>
<blockquote>
<p><strong>图3</strong>：扩散RL框架。（左上）策略网络架构通过ResNet主干和FiLM条件化集成本体感知和视觉观测，以参数化去噪过程。（右上）策略网络通过迭代扩散去噪采样动作，价值网络评估这些动作的价值以指导策略网络更新。（底部）训练循环在并行环境rollout中执行基于扩散的动作采样，收集任务完成奖励来更新网络并提升策略性能。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在包含130个操作任务的LIBERO基准上进行验证。该基准任务具有长视野、稀疏奖励的特点，并包含多个子集以评估不同泛化能力。比较了四种数据源训练的VLA模型：原始人类数据、高斯RL生成数据、本文的扩散RL生成数据、以及人类+扩散RL混合数据。评估指标为任务成功率。</p>
<p><strong>主要结果</strong>：</p>
<ol>
<li><strong>分布内性能</strong>：如表I所示，仅使用扩散RL生成数据训练的VLA模型平均成功率达到81.94%，显著优于使用人类数据（76.64%）和高斯RL数据（69.32%）的模型，分别提升了+5.3%和+12.6%。在LIBERO-Object和LIBERO-Spatial子集上提升尤为明显。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.19752v2/x7.png" alt="分布内成功率结果表"></p>
<blockquote>
<p><strong>表I</strong>：分布内成功率。使用扩散RL生成数据训练的VLA模型在各项任务上的平均成功率最高。</p>
</blockquote>
<ol start="2">
<li><strong>分布外泛化</strong>：如表II所示，在仅使用LIBERO-90预训练，然后零样本评估于全新任务套件的挑战性设置下，仅使用人类数据或扩散RL数据泛化能力均有限（平均成功率分别为1.47%和2.06%）。然而，将人类数据与扩散RL数据混合后，泛化性能提升至5.20%，显示出两种数据源的互补协同效应。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.19752v2/x8.png" alt="分布外泛化结果表"></p>
<blockquote>
<p><strong>表II</strong>：分布外成功率。混合人类与扩散RL数据训练的VLA模型在零样本泛化任务上表现最佳。</p>
</blockquote>
<p><strong>消融实验</strong>：如图5所示，实验验证了各设计选择的必要性。ResNet+U-Net架构优于ViT+MLP；DDIM采样器（5步）比DDPM更快且性能相当或更好；余弦退火学习率对稳定性至关重要；来自并行环境的高数据多样性对于防止模式崩溃、获得稳定高性能策略是必需的。</p>
<p><img src="https://arxiv.org/html/2509.19752v2/x4.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图5</strong>：关于架构、扩散采样、学习率和数据多样性的消融研究。展示了不同设计选择对策略性能的影响。</p>
</blockquote>
<p><strong>数据质量分析</strong>：</p>
<ol>
<li><strong>任务效率</strong>：扩散RL智能体学得的策略最有效率，完成任务所需的平均轨迹长度最短（图6a），且消除了人类数据中存在的“无操作”动作。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.19752v2/x5.png" alt="轨迹效率与平滑度对比"></p>
<blockquote>
<p><strong>图6</strong>：轨迹质量定量比较。（a）扩散RL算法生成最高效的策略，完成任务所需的平均轨迹长度最短；（b）它同时生成最平滑的运动，与人类和高斯RL数据相比，表现出最低的均方加加速度。</p>
</blockquote>
<ol start="2">
<li><strong>轨迹平滑度</strong>：扩散RL生成的轨迹具有最低的均方加加速度，表明其运动最平滑，避免了高斯RL策略常见的抖动（图6b）。</li>
<li><strong>动作一致性</strong>：如图7所示，在相同任务的多次成功轨迹中，扩散RL策略的动作方差极低，表现出近乎确定性的最优行为。相比之下，人类数据方差高（多模态性），高斯RL策略则表现出不稳定和抖动。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.19752v2/x6.png" alt="动作一致性对比"></p>
<blockquote>
<p><strong>图7</strong>：y轴动作随时间的一致性对比。扩散RL策略方差最低，行为最一致；人类数据方差高；高斯RL策略不稳定且存在高频抖动。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1）提出了一个由扩散RL驱动的VLA训练流程，能够自主生成高质量、低方差的数据，包含经过验证有效的模型架构和训练策略改进。2）在包含130个复杂任务的LIBERO基准上提供了有力证据，表明本文生成的合成数据能提供优于人类演示的训练信号，显著提升了VLA模型的分布内性能和分布外泛化能力。3）进行了深入的定量分析，将轨迹级属性（效率、平滑度、一致性）与微调后VLA的性能联系起来，清晰解释了为何优化后的数据更有效。</p>
<p><strong>局限性</strong>：论文自身提及的局限性包括：直接通过在线RL微调VLA模型因RL的样本低效而计算成本高昂；在极具挑战性的分布外泛化设置上，性能仍有很大提升空间。</p>
<p><strong>后续启示</strong>：本研究为突破VLA模型对人工数据的依赖提供了有效路径。扩散模型与RL的结合在生成高质量机器人控制数据方面展现出独特优势。未来工作可探索将RL生成数据与人类数据、世界模型生成数据等其他来源更紧密地结合，以进一步丰富数据的多样性和覆盖范围，从而训练出泛化能力更强的通用机器人模型。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型依赖昂贵人工演示数据、可扩展性受限的核心问题，提出一种改进的扩散策略优化算法。该方法利用扩散模型的高表达能力探索复杂行为，并通过迭代去噪过程的隐式正则化生成平滑、低方差的轨迹，构建了扩散RL驱动的VLA训练流程。在LIBERO基准的130项长视野操纵任务上，所生成轨迹优于人类演示与高斯RL策略。仅使用扩散RL生成数据训练的VLA模型平均成功率达81.9%，较人类数据提升5.3%，较高斯RL数据提升12.6%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19752" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>