<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Interactive Imitation Learning in State-Space - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Interactive Imitation Learning in State-Space</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2008.00524" target="_blank" rel="noreferrer">2008.00524</a></span>
        <span>作者: Jauhri, Snehal, Celemin, Carlos, Kober, Jens</span>
        <span>日期: 2020/08/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习（IL）使智能体能够通过演示而非手动编程来学习任务行为。然而，其性能受限于可用演示数据的质量。交互式模仿学习（Interactive IL）允许教师在智能体执行任务时提供反馈，从而能提高学习效率。当前，大多数IL和交互式IL方法需要演示者提供动作空间（action-space）的演示或反馈，例如机器人的关节力矩或角度。这对于非专家演示者来说通常不直观且困难。人类通常通过理解任务所需的状态转换来学习行为，而非具体的动作序列。因此，在状态空间（state-space）提供反馈（例如，指示末端执行器应向某个方向移动）往往更为自然。本文针对“动作空间反馈不直观”这一具体痛点，提出了在状态空间进行交互式学习的新视角。其核心思路是：允许人类演示者通过二元信号（增加/减少）对智能体的当前状态提供纠正性反馈，然后利用一个学习的前向动力学模型，通过间接逆动力学的方式，将期望的状态变化映射为执行的动作，从而在线更新策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法称为“状态空间教学模仿策略”（Teaching Imitative Policies in State-space， TIPS）。其整体目标是允许智能体执行其策略，同时人类演示者观察并在任何时刻对智能体访问的状态提出修改建议。该反馈被用于在线更新智能体的策略。</p>
<p><strong>整体框架</strong>：TIPS的学习框架如图1所示，包含两个主要阶段：初始模型学习阶段和教学阶段。在教学阶段的每一步，智能体观察当前状态 <code>s_t</code>，若收到非零的人类反馈 <code>h_t</code>，则计算期望的下一状态 <code>s_des_t+1</code>，然后通过间接逆动力学计算并执行相应动作 <code>a_des_t</code>，同时用该状态-动作对更新策略。若无反馈，则执行当前策略 <code>π(s_t)</code> 的动作。所有交互经验都被存储以用于后续更新前向动力学模型和策略。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：TIPS学习框架的高级表示。智能体在环境中执行动作，人类观察状态并提供反馈<code>h_t</code>。系统根据反馈和误差常数<code>e</code>计算期望状态<code>s_des_t+1</code>，然后通过间接逆动力学（利用学习的前向动力学模型<code>f</code>）计算并执行动作<code>a_des_t</code>，同时更新策略<code>π</code>。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>纠正性反馈处理</strong>：人类反馈 <code>h_t</code> 是二元信号（-1， 0， +1），分别表示对某个状态维度值进行减少、无操作或增加。每个状态维度有一个对应的误差常数超参数 <code>e</code>。期望状态计算为：<code>s_des_t+1 = s_t + h_t · e</code>。反馈可以针对全状态或部分状态，这允许演示者仅对其理解或易于观察的维度提供指导。为了捕捉反馈的趋势（如持续同向反馈表示需要较大幅度调整），方法采用了类似于D-COACH的重放记忆机制来利用历史反馈信息。</li>
<li><strong>状态转换到动作的映射（间接逆动力学）</strong>：为了将当前状态 <code>s_t</code> 转换到人类期望的状态 <code>s_des_t+1</code>，需要计算相应的动作 <code>a_des_t</code>。论文没有使用传统的逆动力学模型（IDM），因为人类反馈可能只针对部分状态维度，且期望的状态转换在单步内可能不可行。TIPS采用了一种间接方法：从动作空间 <code>A</code> 中均匀采样 <code>N_a</code> 个动作 <code>a</code>，利用学习的前向动力学模型（FDM） <code>f</code> 预测执行每个动作后的下一状态 <code>f(s_t, a)</code>。然后选择其预测状态最接近期望状态的动作。数学表示为：<code>a_des_t = arg min_a || f(s_t, a) - s_des_t+1 ||</code>。距离计算可以基于全状态或部分状态。</li>
<li><strong>训练机制</strong>：策略 <code>π</code> 和前向动力学模型 <code>f</code> 均用前馈神经网络表示。策略更新受到D-COACH启发，包含三种方式：a) <strong>即时更新</strong>：每当收到反馈，就用当前计算出的状态-动作对 <code>(s_t, a_des_t)</code> 更新策略；b) <strong>记忆重放更新</strong>：定期从演示缓冲区 <code>D</code> 中采样一批历史反馈数据进行更新；c) <strong>周期性批量更新</strong>：每 <code>T_update</code> 步从 <code>D</code> 中采样进行额外更新以充分训练网络。<strong>关键的是，计算出的动作 <code>a_des_t</code> 会被立即执行</strong>，这使得智能体能快速到达演示者期望的状态区域，从而接收后续反馈以学习下一步动作。前向动力学模型 <code>f</code> 在每个训练周期结束后，使用经验缓冲区 <code>E</code> 中累积的所有交互数据（包括新数据和旧数据）进行更新。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ul>
<li><strong>反馈空间</strong>：与D-COACH等在动作空间提供反馈的方法不同，TIPS在状态空间接收反馈，这对人类演示者更直观。</li>
<li><strong>动作生成机制</strong>：不同于依赖精确逆动力学模型或预定义建议算子的方法，TIPS通过“采样-评估”的间接逆动力学方式，利用学习的前向动力学模型来生成动作，能处理部分状态反馈和潜在不可行的状态转换。</li>
<li><strong>即时执行与学习</strong>：计算出的纠正动作被立即执行，加速了学习过程，使智能体能快速探索修正后的状态区域。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>评估环境</strong>：使用OpenAI Gym中的三个模拟任务：CartPole（离散动作）、Reacher（连续动作，固定目标）、LunarLanderContinuous（连续动作）。</li>
<li><strong>机器人验证</strong>：使用KUKA LBR iiwa机械臂完成两个操作任务：“钓鱼”（Fishing）和“激光绘图”（Laser Drawing）。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>动作空间遥操作</strong>：演示者直接控制动作。</li>
<li><strong>状态空间遥操作</strong>：演示者提供状态指令，系统用类似TIPS的方式计算动作。</li>
<li><strong>行为克隆（BC）</strong>：使用成功的遥操作数据（回报≥归一化范围的40%）进行监督学习。</li>
<li><strong>生成对抗模仿学习（GAIL）</strong>：使用成功的遥操作数据。</li>
<li><strong>D-COACH</strong>：动作空间的交互式模仿学习方法（二元纠正反馈）。</li>
</ul>
</li>
<li><strong>参与者</strong>：非专家人类演示者（年龄25-30岁），无任务先验知识。共进行22组试验。</li>
<li><strong>评估指标</strong>：累积回报（归一化）、NASA任务负荷指数（TLX）问卷评分、学习曲线、反馈率。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://..." alt="性能对比"></p>
<blockquote>
<p><strong>图2a</strong>：TIPS与遥操作、BC、GAIL在三个模拟任务上的性能对比。条形图显示了归一化后的平均回报。TIPS在三个任务上均取得了最佳或接近最佳的性能，显著优于仅使用遥操作数据的BC和GAIL，这表明交互式反馈能带来持续改进。</p>
</blockquote>
<p><img src="https://..." alt="学习曲线对比"></p>
<blockquote>
<p><strong>图2b</strong>：TIPS（状态空间反馈）与D-COACH（动作空间反馈）在训练过程中的性能变化。在CartPole和Reacher任务中，TIPS的学习效率更高（更快达到高性能），且在Reacher任务中最终性能更优。在LunarLander任务中，两者最终性能相似，但TIPS达到该性能所需的训练时间更短。</p>
</blockquote>
<p><img src="https://..." alt="任务负荷指数"></p>
<blockquote>
<p><strong>表2</strong>：演示者使用TIPS和D-COACH教学时的NASA TLX平均评分（值越小负荷越低）。在CartPole和Reacher任务中，使用TIPS（状态空间反馈）时，演示者的精神负担和挫败感评分显著降低。这表明状态空间反馈对演示者更友好。但在复杂的LunarLander任务中，两者负荷相近。</p>
</blockquote>
<p><img src="https://..." alt="机器人验证任务"></p>
<blockquote>
<p><strong>图3</strong>：在真实机器人上的验证任务。(a) “钓鱼”任务场景：机械臂末端用线悬挂小球，目标是将球放入杯中。(b) “激光绘图”任务场景：机械臂末端激光笔在白板上绘图。(c) 经过约7分钟训练后，机器人绘制的字符轨迹（相机跟踪）。</p>
</blockquote>
<p><img src="https://..." alt="机器人学习曲线"></p>
<blockquote>
<p><strong>图4</strong>：两个机器人验证任务的学习曲线和反馈率。随着训练进行，智能体回报提升（“钓鱼”任务回报从约-25升至-10；“激光绘图”任务Hausdorff距离下降），同时演示者提供的反馈频率显著下降，表明智能体成功学会了任务，且所需的人工干预减少。</p>
</blockquote>
<p><strong>消融实验与组件贡献</strong>：论文虽未设置严格的消融实验，但通过对比不同方法间接体现了各组件的贡献：</p>
<ol>
<li><strong>状态空间反馈 vs. 动作空间反馈</strong>：通过对比TIPS和D-COACH（图2b， 表2），证明了状态空间反馈在多数任务中能提高学习效率、最终性能并降低演示者负荷。</li>
<li><strong>交互式学习 vs. 离线模仿学习</strong>：通过对比TIPS与BC、GAIL（图2a），证明了交互式持续反馈能克服演示数据不一致和泛化能力差的问题，获得更优性能。</li>
<li><strong>间接逆动力学的作用</strong>：该方法使系统能够在未知或难以建模动力学的环境中，仅通过学习的FDM来处理部分状态反馈并生成动作，这是实现状态空间交互教学的关键。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>TIPS方法</strong>，首次将交互式模仿学习的框架引入到状态空间反馈中，允许人类以更直观的“改变状态”方式指导智能体。</li>
<li>设计了一种<strong>间接逆动力学动作生成机制</strong>，通过采样评估和学习的前向动力学模型，将状态空间的纠正反馈映射为可执行的动作，避免了传统逆动力学模型在部分状态反馈和不可行转换上的局限性。</li>
<li>通过<strong>非专家演示者的人因实验</strong>，在模拟和真实机器人任务上验证了TIPS的有效性。结果表明，TIPS智能体性能超越了演示者自身及传统模仿学习方法，并且显著降低了演示者在多数任务中的教学负担。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>前向动力学模型学习</strong>：FDM需要从智能体与环境的交互中学习，若训练数据不足或探索不充分，会导致模型不准确，进而产生错误的动作，影响学习效果和演示体验。</li>
<li><strong>高维动作空间的计算成本</strong>：动作选择机制需要对整个动作空间进行采样评估。在论文涉及的低维动作空间中计算成本可忽略，但在高维动作空间中，这种穷举式搜索将变得计算昂贵，难以实时应用。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>改进动力学模型学习</strong>：可以研究更高效的数据采集策略（如主动学习、基于课程的探索）来提升FDM的准确性和样本效率。</li>
<li><strong>高效的动作选择</strong>：针对高维动作空间，需要开发更高效的动作搜索或优化方法（如基于梯度的方法、在策略分布附近采样等）来替代当前的均匀采样，以降低计算开销。</li>
<li><strong>任务与接口适配性</strong>：实验表明状态空间反馈的优势具有任务依赖性（如对LunarLander任务帮助有限）。未来工作可深入研究何种任务特性最适合状态空间反馈，并探索更灵活、多模态的反馈接口。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对交互式模仿学习中反馈需在动作空间提供、对人类演示者不直观且困难的核心问题，提出新方法TIPS（Teaching Imitative Policies in State-space），利用状态空间反馈训练代理，允许演示者以直观的“改变状态”方式提供指导。实验在OpenAI Gym控制任务和KUKA机器人操作任务中进行，结果表明，在非专家演示场景下，通过TIPS训练的代理性能优于演示者和传统模仿学习代理。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2008.00524" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>