<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.14830" target="_blank" rel="noreferrer">2510.14830</a></span>
        <span>作者: Huazhe Xu Team</span>
        <span>日期: 2025-10-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于学习的机器人操作取得了显著进展，特别是生成式扩散策略和机器人基础模型通过在大规模、高质量的人类演示数据上训练或微调，为机器人提供了强大的人类先验，使其能够习得熟练遥操作员的高效策略。然而，高质量的真实机器人数据仍然稀缺：遥操作存在感知和控制延迟，倾向于缓慢、保守的运动，且大规模收集依赖熟练操作员，成本高昂。这导致状态-动作覆盖有限，削弱了泛化能力和可靠性。因此，这种纯监督范式受到“模仿天花板”的限制：在纯监督目标下，性能实际上受限于演示者的技能，并继承了人类的低效、偏见和偶尔的错误。</p>
<p>强化学习通过优化交互回报而非模仿保真度，提供了一条互补的路径，能够发现人类演示中罕见或缺失的策略。但同时，仿真到现实的RL需要应对仿真与现实之间的视觉和动力学差距，而在真实硬件上直接训练基于学习的生成策略则具有风险且样本效率低下。这引出了一个核心问题：如何构建一个机器人学习系统，既能利用强大的人类先验，又能通过自主探索持续自我改进？</p>
<p>本文针对上述痛点，提出了RL-100框架。其核心思路是：从人类先验（模仿学习）开始，与人类基础目标（如成功率、完成时间、鲁棒性）对齐，并通过真实世界RL后训练（结合迭代离线学习和少量在线学习）超越人类性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>RL-100是一个统一框架，将模仿学习与强化学习相结合，包含三个阶段：1) 基于人类演示的模仿学习预训练；2) 基于策略 rollout 数据增长的迭代离线RL后训练；3) 针对剩余罕见故障模式的少量在线RL后训练。其创新点在于将离线与在线RL通过一个应用于扩散去噪步骤的共享PPO风格目标统一起来，并引入轻量级一致性蒸馏以压缩推理延迟。</p>
<p><img src="https://arxiv.org/html/2510.14830v3/x1.jpg" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：RL-100整体框架概述。包含三个阶段：首先通过模仿学习从人类演示中学习；然后进行迭代离线RL，利用不断扩大的策略 rollout 缓冲区进行数据扩充和离线更新；最后进行在线微调以优化最终性能。</p>
</blockquote>
<p><strong>1. 模仿学习预训练：</strong> 该阶段通过行为克隆在人类遥操作轨迹上初始化策略。使用条件扩散从演示中学习鲁棒的视觉运动策略。给定同步数据元组 <code>(观测 o_t, 本体感知 q_t, 动作 a_t)</code>，将最近的观测融合为条件向量 <code>c_t</code>。扩散目标 <code>a_t^{τ_0}</code> 可以是单步动作或动作块。通过前向过程将干净动作加噪，并使用噪声预测目标 <code>L_IL</code> 训练去噪网络 <code>ε_θ</code>。视觉编码器（支持RGB或点云）与本体感知编码器联合训练，并添加重建和变分信息瓶颈正则化以稳定表示。</p>
<p><strong>2. 统一的离线与在线RL微调：</strong> 这是方法的核心。RL-100将生成每个环境动作的K步扩散过程建模为一个嵌入的“去噪MDP”。该子MDP的初始状态为带噪声的动作，每一步（对应一个去噪步骤）的子策略是高斯分布，其均值和方差由扩散网络参数化。关键的创新是使用统一的PPO风格目标 <code>J_i(π)</code>，在整个K步去噪过程中优化策略，以实现端到端的任务奖励优化。离线与在线阶段的区别仅在于优势估计方式：离线阶段使用IQL风格的价值函数，在线阶段使用GAE。</p>
<p><strong>3. 轻量级一致性蒸馏：</strong> 为了满足部署延迟要求，在RL微调过程中交错引入一个轻量级的蒸馏损失。该损失将经过微调的K步扩散策略（教师）压缩成一个一步的一致性模型（学生），通过回归教师输出（经过停止梯度）来训练。这使得在部署时只需单步前向传播即可生成动作，显著降低推理延迟。</p>
<p><strong>4. 任务、本体与表征无关性：</strong> 框架支持单步和动作块两种控制模式，仅输出头不同。视觉表征支持3D点云和2D RGB图像，通过交换观测编码器即可，无需修改框架其余部分。采用自监督视觉编码器，在RL探索和更新过程中提供稳定、任务相关的特征。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置：</strong> 在真实机器人上评估了七个多样化的操作任务（见图1和表1）：动态推-T、敏捷保龄球、倾倒、软毛巾折叠、动态拧螺丝、橙汁榨取（包含放置和移除两个子任务）。任务涵盖刚体动力学、可变形物体、流体和精密装配，并使用了多种机器人本体（UR5, Franka, xArm等）。控制模式根据任务需求选择单步或动作块。主要评估部署中心指标：可靠性（成功率）、效率（完成时间）和鲁棒性（长时部署稳定性及抗扰动能力）。</p>
<p><strong>基线方法：</strong> 主要对比了纯模仿学习（IL）方法。</p>
<p><img src="https://arxiv.org/html/2510.14830v3/x7.png" alt="关键实验结果汇总"></p>
<blockquote>
<p><strong>图7</strong>：所有七个任务的最终成功率。RL-100在所有任务上均达到100%成功率（总计900/900次试验成功），包括在单个任务上连续250/250次成功。Pure IL (DP3) 在多个任务上无法达到100%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.14830v3/x8.png" alt="完成时间对比"></p>
<blockquote>
<p><strong>图8</strong>：与人类遥操作员完成时间的对比。RL-100在多个任务上达到或超越了熟练操作员的效率。</p>
</blockquote>
<p><strong>关键实验结果：</strong></p>
<ol>
<li><strong>可靠性：</strong> RL-100在所有七个任务上实现了100%的成功率，总计900次试验全部成功，其中单个任务（如折叠）实现了连续250次成功。</li>
<li><strong>效率：</strong> 在完成时间上，RL-100达到人类遥操作水平，并在多个任务（如保龄球、拧螺丝）上匹配甚至超越了熟练操作员。</li>
<li><strong>鲁棒性与泛化：</strong> 未经重新训练，单一策略在环境和动力学变化下实现了约90%的零样本成功率；在少量样本微调下能适应显著的任务变化（86.7%）；对激进的人为扰动保持约95%的鲁棒性。橙汁榨取机器人在商场零样本部署时，连续服务随机顾客约7小时未发生故障。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.14830v3/x21.png" alt="消融实验：各阶段贡献"></p>
<blockquote>
<p><strong>图21</strong>：三阶段训练（IL → 迭代离线RL → 在线RL）的消融研究。迭代离线RL带来了大部分性能提升，而在线RL阶段则解决了剩余的罕见故障，将成功率从高位（如95%）推至接近完美（99%+）。</p>
</blockquote>
<p><strong>消融实验总结：</strong></p>
<ul>
<li><strong>三阶段必要性：</strong> 模仿学习提供安全、低方差的基础；迭代离线RL贡献了成功率提升的主要部分；在线RL使用小预算针对性地解决罕见故障，实现“最后一英里”的可靠性。</li>
<li><strong>一致性蒸馏有效性：</strong> 蒸馏后的一步策略在保持高性能的同时，将推理速度提升了约5倍（K=5时）。</li>
<li><strong>表征鲁棒性：</strong> 使用自监督编码器与重建正则化，能有效防止RL微调过程中的表征漂移，保持训练稳定性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献：</strong></p>
<ol>
<li><strong>统一的训练框架：</strong> 提出了一个三阶段（IL预训练、迭代离线RL、在线RL）的真实世界RL框架，在扩散策略之上实现了从人类先验出发，对齐人类目标，并最终超越人类性能的可靠路径。</li>
<li><strong>高效部署方案：</strong> 设计了统一于去噪过程的PPO目标进行RL微调，并辅以轻量级一致性蒸馏，在提升性能的同时满足了低延迟部署需求。</li>
<li><strong>广泛的实证验证：</strong> 在七项多样化的真实机器人任务上系统性地验证了框架的可靠性、效率和鲁棒性，首次展示了跨任务模态和本体的、基于视觉的RL后训练能达到100%成功率。</li>
</ol>
<p><strong>局限性：</strong> 论文提到，在线RL阶段虽然在样本效率上已优化，但在真实硬件上进行调整（参数调优、重置、审批）仍然是资源密集型的。因此，框架将大部分学习预算分配给迭代离线更新，仅使用少量有针对性的在线预算。</p>
<p><strong>对后续研究的启示：</strong></p>
<ul>
<li><strong>离线优先的RL范式：</strong> 证明了迭代离线RL在利用策略自身生成的数据进行稳定、高效改进方面的巨大潜力，为安全、样本高效的真实世界RL提供了可行方案。</li>
<li><strong>统一策略优化：</strong> 将扩散模型的生成过程重新表述为可优化的子MDP，为生成式策略与RL的更深度结合开辟了新思路。</li>
<li><strong>部署驱动的研究：</strong> 强调以部署就绪的指标（长期可靠性、完成时间、抗扰动）作为评估核心，推动研究向实用化方向发展。RL-100在商场的长时成功部署，为学习型机器人走出实验室提供了鼓舞人心的案例。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文解决真实世界机器人操作需高可靠性、效率及鲁棒性的核心问题。提出RL-100框架，基于扩散视觉运动策略，关键技术包括：统一模仿与强化学习于PPO风格目标，应用于去噪过程；通过轻量级一致性蒸馏将多步扩散压缩为一步控制器，满足低延迟部署。实验在七项多样任务中达成100%成功率（900/900次），匹配或超越专家操作速度，零样本环境下成功率约90%，少量样本适应任务变体达86.7%，抗人为干扰约95%，榨汁机器人零样本部署连续运行约7小时无故障。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.14830" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>