<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01158" target="_blank" rel="noreferrer">2602.01158</a></span>
        <span>作者: Matteo Matteucci Team</span>
        <span>日期: 2026-02-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为通用机器人操作的主导范式，它将感知与控制统一在一个端到端的架构中。代表性的模型如RT-2、OpenVLA、π₀和SmolVLA，通过利用大规模多模态数据和强大的语言模型骨干，在遵循语言指令执行操作任务上展现出强大能力。然而，这些模型在受控研究环境中表现优异，其在实际场景中的可靠部署却严重受限于对视觉干扰的脆弱性。现有研究广泛解决了由场景几何结构引起的物理遮挡问题，通常通过主动感知或多视图推理来解决。但一个同样关键却未被充分探索的视觉退化来源是<strong>图像损坏</strong>。这些传感器层面的伪影，如电子噪声、坏点、镜头污渍等，会在视觉信号被VLA模型解读之前直接损害其完整性。本文系统性地量化了这种脆弱性，并针对这一具体痛点，提出了一种新的解决视角：在VLA模型上游插入一个轻量级、即插即用、模型无关的图像恢复模块，以净化损坏的视觉输入。本文的核心思路是设计一个<strong>损坏恢复变换器</strong>，通过对抗训练学习从损坏观测中重建干净的图像，从而免疫VLA模型免受传感器干扰的影响。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架的目标是在标准的机器人控制流程中，于VLA模型之前插入一个专用的恢复模块——损坏恢复变换器。该模块接收从相机获取的原始损坏观测帧，进行处理以减少损坏，然后将恢复后的图像传递给策略网络以预测动作。</p>
<p><img src="https://arxiv.org/html/2602.01158v1/pipeline.png" alt="系统流程"></p>
<blockquote>
<p><strong>图1</strong>：系统流程图。CRT模块被插入VLA模型的上游。它拦截来自模拟器的损坏观测x‘，将其恢复为x̂，并将干净的估计图像传递给策略网络进行动作预测。</p>
</blockquote>
<p><strong>核心模块：损坏恢复变换器</strong><br>CRT是一个基于Transformer的重建模型，旨在将损坏的图像观测映射回其原始版本。其架构灵感来源于高效的图像恢复框架，并针对机器人感知领域进行了专门化改造。为了处理VLA所需的高分辨率输入（LIBERO为360×360，Meta-World为480×480）并保留对操作至关重要的细粒度纹理和物体细节，作者对基线架构进行了显著扩容：增大了潜在向量尺寸、增加了Transformer块的层数（以建模图像上复杂的损坏伪影统计）并扩展了注意力头的数量。</p>
<p>CRT架构包含三个关键机制以确保所需的精度和效率：</p>
<ol>
<li><strong>移位补丁标记化</strong>：用于保留相邻像素间的空间关系，解决标准ViT缺乏局部归纳偏置的问题。</li>
<li><strong>旋转位置编码</strong>：用于编码绝对和相对位置，使模型能够泛化到动作预测中使用的较长动作序列。</li>
<li><strong>局部自注意力</strong>：用于锐化注意力分数，聚焦于局部纹理细节。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.01158v1/CRT.png" alt="CRT架构"></p>
<blockquote>
<p><strong>图3</strong>：损坏恢复变换器的架构。该模型利用三种专门机制：1）移位补丁标记化以恢复局部空间依赖；2）旋转位置编码以实现鲁棒的相对空间编码；3）局部自注意力以锐化纹理细节。深层的Transformer主干使模型能够从底层语义场景中解耦复杂的损坏伪影，最终将潜在标记重塑为恢复的RGB观测。</p>
</blockquote>
<p><strong>对抗训练策略</strong><br>为了在恢复全局结构的同时保留对机器人操作至关重要的高频细节，避免仅使用像素级重建损失导致的模糊输出，本文采用了对抗训练框架。CRT模块作为生成器G，并引入一个判别器D来区分真实的干净观测x和重建的观测x̂ = G(x‘)。</p>
<p><img src="https://arxiv.org/html/2602.01158v1/adversarial_scheme.png" alt="对抗训练"></p>
<blockquote>
<p><strong>图2</strong>：CRT对抗训练示意图。CRT生成器接收损坏输入x‘并生成重建观测x̂。网络通过一个多目标损失函数进行优化，该函数结合了像素级保真度损失ℒ_L1、结构相似性损失ℒ_SSIM以及来自判别器D的对抗反馈损失ℒ_adv，以确保有效的重建。</p>
</blockquote>
<p>训练目标是一个加权组合的三部分损失：</p>
<ul>
<li><strong>对抗损失</strong>：使用标准二元交叉熵目标，鼓励感知真实性。</li>
<li><strong>重建损失</strong>：<ul>
<li><strong>L1损失</strong>：促进稀疏性，相比L2减少模糊。</li>
<li><strong>结构相似性指数度量损失</strong>：保留亮度、对比度和结构信息。<br>最终的生成器总损失为：ℒ_total = λ_L1 ℒ_L1 + λ_SSIM ℒ_SSIM + λ_adv ℒ_adv^G。通过经验优化，超参数设置为λ_L1=10.0，λ_SSIM=1.0，λ_adv=0.05。对抗项权重较低，确保训练以内容重建为基础，同时提供足够的梯度信号来细化纹理细节并消除伪影。</li>
</ul>
</li>
</ul>
<p><strong>创新点</strong><br>与现有方法相比，本文的创新点主要体现在：1）首次系统性地关注并量化了VLA模型对传感器层面图像损坏的脆弱性；2）提出了一种<strong>模块化、即插即用</strong>的解决方案，无需对计算昂贵的底层VLA模型进行微调；3）将针对自然图像设计的先进Transformer恢复架构，通过扩容和对抗训练策略，成功适配到高分辨率的机器人视觉任务中。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：本文在LIBERO和Meta-World这两个广泛认可的机器人操作基准上评估CRT模块的有效性。集成了两种先进的VLA模型：在LIBERO和Meta-World上评估的SmolVLA，以及仅在LIBERO套件上评估的π₀.₅。</p>
<p><strong>对比基线</strong>：对比了VLA模型在干净观测下的基线性能，以及在五种图像损坏类型下的性能（无任何缓解策略）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>VLA对图像损坏的脆弱性</strong>：实验结果证实了最先进的VLA模型对传感器伪影高度敏感。例如，在LIBERO-10上，π₀.₅在水平线损坏（0.5强度）下，成功率从90%暴跌至2%。在Meta-World MT50上，SmolVLA在相同条件下性能从58%减半至20.6%。</li>
<li><strong>CRT的有效性</strong>：集成CRT后显示出显著的性能恢复，特别是对于π₀.₅架构。在LIBERO上，面对最严重的“水平线（0.5）”损坏，π₀.₅+CRT的成功率从2%反弹至87%，几乎与干净基线持平。在“水滴”损坏下从68%恢复至87%。对于Meta-World上的SmolVLA，CRT在所有损坏类型上均持续优于未缓解的模型，例如在“水平线”损坏上成功率提高了超过11%（从20.6%到32.2%）。</li>
<li><strong>基线性能保持与权衡</strong>：CRT对π₀.₅模型引入的开销可忽略不计，在干净数据上保持了89%的成功率（基线为90%）。然而，对于更轻量级的SmolVLA架构，观察到性能权衡：在Meta-World上，使用CRT的基线性能从58%下降到47%。这表明较小的VLA对重建过程引入的细微分布偏移的鲁棒性较低，泛化能力不足。尽管如此，这种权衡被在恶劣条件下提供的巨大可靠性增益所显著抵消。</li>
<li><strong>效率</strong>：CRT模块是一个轻量级神经网络，仅需10-50毫秒的推理时间和约1GB的VRAM，相比VLAs动辄数百毫秒的令牌生成延迟，其开销可忽略不计。</li>
</ol>
<p><strong>消融实验</strong>：本文虽未进行标准的组件消融实验，但通过架构设计（SPT、RoPE、LSA的引入）和损失函数组合（L1、SSIM、Adv的加权）的选择， implicitly 验证了这些设计对于处理高分辨率机器人视觉重建任务的有效性。</p>
<p><img src="https://arxiv.org/html/2602.01158v1/corruptions.png" alt="视觉损坏类型"></p>
<blockquote>
<p><strong>图4</strong>：视觉损坏类型示例。从上左至下右：(a) 干净基线，(b) 中心方块，(c) 高斯噪声，(d) 水平线（0.5），(e) 水平线（0.2），(f) 水滴。这些损坏模拟了硬件故障、环境条件和信号传输错误。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>问题界定与量化</strong>：首次系统性地识别并量化了当前VLA模型对图像损坏（传感器伪影）缺乏鲁棒性这一关键漏洞，揭示了即使在严重损坏下成功率下降高达97.78%的极端情况。</li>
<li><strong>模块化解决方案</strong>：提出了损坏恢复变换器，一种即插即用、模型无关的专用模块，通过对抗训练恢复视觉完整性，无需重新训练或微调底层VLA，为增强VLA的环境鲁棒性提供了一条可扩展的路径。</li>
<li><strong>全面验证</strong>：在主流机器人基准上进行了广泛实验，证实了CRT对于大型VLA（如π₀.₅）能近乎完美地恢复性能，对于其他模型也能带来显著增益。</li>
</ol>
<p><strong>局限性</strong>：<br>论文明确指出，对于SmolVLA这类较小的架构，CRT在提升损坏条件下性能的同时，会对其在干净观测上的基线性能造成一定程度的下降（约18.96%），这揭示了在恢复有效性和输入保真度之间存在权衡。较小的VLA对恢复过程引入的细微分布偏移更为敏感。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>条件式恢复</strong>：鉴于CRT的模块化特性，未来工作可以探索仅在检测到损坏时才激活恢复流程的机制，从而在无损坏时完全保持VLA的原始性能。</li>
<li><strong>VLA与恢复模块的协同设计</strong>：可以研究将恢复目标以更紧密的方式整合到VLA的预训练或微调过程中，或许能减轻对小型模型造成的分布偏移影响。</li>
<li><strong>扩展到更广泛的损坏类型与真实世界数据</strong>：将方法应用于更多样的真实世界传感器故障和恶劣环境条件，并最终在物理机器人系统上进行验证。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在真实部署中因图像损坏（如电子噪声、坏点）导致性能严重下降的问题展开研究。作者提出了一种即插即用、模型无关的损坏恢复变换器，通过对抗训练直接修复损坏的视觉输入，无需微调原模型。实验表明，该方法能使模型在严重视觉损坏下保持接近基线的成功率，有效解决了VLA模型对传感器干扰的脆弱性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01158" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>