<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01158" target="_blank" rel="noreferrer">2602.01158</a></span>
        <span>作者: Matteo Matteucci Team</span>
        <span>日期: 2026-02-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作模型已成为通用机器人操作的主导范式，将感知与控制统一在单一的端到端架构中。然而，尽管在受控环境中取得了成功，其在真实世界中的可靠部署因其对视觉干扰的脆弱性而受到严重阻碍。现有文献广泛解决了由场景几何引起的物理遮挡问题，但一个关键模式在很大程度上未被探索：图像损坏。这些传感器层面的伪影，包括电子噪声、坏点和镜头污染物，在视觉信号被解释之前就直接损害了其完整性。本文量化了这种脆弱性，证明了最先进的VLA模型（如π₀.₅和SmolVLA）在常见信号伪影下性能会灾难性下降，成功率从90%降至低至2%。为了缓解此问题，本文引入了损坏恢复变换器，这是一个即插即用且与模型无关的视觉变换器，旨在使VLA模型对传感器干扰具有免疫力。本文的核心思路是：在VLA模型上游插入一个轻量级的图像恢复模块，通过对抗性训练从损坏的输入中恢复干净的观测，从而保护下游策略网络，而无需对基础模型进行昂贵的微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架是在标准的机器人控制流程中，在VLA模型的上游插入一个专用的恢复模块——损坏恢复变换器。来自机器人相机的原始观测帧x′（已损坏）首先被CRT处理，恢复为干净的估计值x̂，然后才被送入VLA策略网络进行动作预测。</p>
<p><img src="https://arxiv.org/html/2602.01158v1/pipeline.png" alt="系统流程"></p>
<blockquote>
<p><strong>图1</strong>：系统流程。CRT模块被插入到VLA的上游。它拦截来自模拟器的损坏观测x′，将其恢复为x̂，并将干净的估计值传递给策略网络进行动作预测。</p>
</blockquote>
<p>CRT是一个基于变换器的重建模型𝒢，其目标是将损坏的图像观测映射回其原始版本。该方法受图像恢复框架的启发，并针对机器人感知领域进行了专门化。为了满足VLA所需的精度和效率，CRT采用了三个关键的架构机制：1) 移位补丁标记化，用于保留相邻像素间的空间关系，解决标准ViT缺乏局部归纳偏置的问题；2) 旋转位置嵌入，用于编码绝对和相对位置，使模型能够泛化到动作预测中使用的较长动作序列；3) 局部自注意力，用于锐化注意力分数并聚焦于局部纹理细节。由于原始基线架构是为低分辨率任务设计的，而机器人应用需要处理包含密集语义信息的高分辨率视觉输入，因此作者显著扩展了模型容量：增加了潜在向量大小以保留对机器人操作至关重要的细粒度纹理和物体细节，扩展了变换器块的数量以提供对整幅图像中复杂损坏伪影统计进行建模的能力，并增加了注意力头的数量以允许模型并行推理多种关系。</p>
<p><img src="https://arxiv.org/html/2602.01158v1/adversarial_scheme.png" alt="CRT对抗训练"></p>
<blockquote>
<p><strong>图2</strong>：CRT对抗训练。CRT生成器接收损坏的输入x′并产生重建的观测x̂。网络通过一个多目标损失函数进行优化，该函数结合了像素级保真度损失ℒ_L1、结构相似性损失ℒ_SSIM以及来自判别器𝒟的对抗性反馈损失ℒ_adv，以确保有效的重建。</p>
</blockquote>
<p>为了在恢复高频细节（这对机器人操作至关重要）的同时避免模糊输出，本文采用了对抗性训练框架，将CRT模块视为GAN中的生成器𝒢。引入了一个判别器网络𝒟，用于区分真实的干净观测x和重建的观测x̂。训练目标采用多目标优化策略，结合了三个不同的损失分量：对抗性损失（使用标准二元交叉熵目标以鼓励感知真实性）、像素级L1损失（确保像素级保真度）和结构相似性损失（捕捉人类感知的结构信息）。总损失是这些损失的加权和。</p>
<p><img src="https://arxiv.org/html/2602.01158v1/CRT.png" alt="损坏恢复变换器架构"></p>
<blockquote>
<p><strong>图3</strong>：损坏恢复变换器架构。该模型利用四种专门机制：1) 移位补丁标记化以恢复局部空间依赖性；2) 旋转位置嵌入以实现鲁棒的相对空间编码；3) 局部自注意力以锐化纹理细节。深度变换器主干使模型能够将复杂的损坏伪影与底层语义场景解耦，最终将潜在标记重塑为恢复的RGB观测。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1) 首次系统性地量化了图像损坏（而非物理遮挡）对最先进VLA模型性能的灾难性影响；2) 提出了一种模块化、即插即用且模型无关的解决方案，无需重新训练或微调计算昂贵的VLA模型本身；3) 将高效的变换器图像恢复架构针对机器人感知的高分辨率需求进行了扩展和专门化，并结合对抗性训练以保留对下游任务至关重要的高频细节。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个机器人操作基准上进行：LIBERO（10个任务，分辨率360×360）和Meta-World（10个任务，分辨率480×480）。评估的VLA模型包括SmolVLA和π₀.₅。对比的基线是这些VLA在干净观测和损坏观测下的原始性能。引入了六种视觉损坏类型进行评估：中心方块、高斯噪声、两种密度的水平线以及水滴。</p>
<p>关键实验结果显示，在严重损坏下，VLA模型的性能会急剧下降。例如，在LIBERO基准上，SmolVLA在干净图像上的成功率为90.0%，但在“中心方块”损坏下骤降至2.0%，在“水平线(0.5)”损坏下降至10.0%。同样，π₀.₅在干净图像上的成功率为85.0%，在“中心方块”损坏下也降至2.0%。在集成CRT模块后，性能得到了显著恢复。对于SmolVLA，在“中心方块”损坏下的成功率从2.0%恢复至82.0%，在“水平线(0.5)”损坏下从10.0%恢复至84.0%。对于π₀.₅，在“中心方块”损坏下的成功率从2.0%恢复至80.0%。在Meta-World基准上也观察到了类似的趋势，CRT帮助模型在大多数损坏类型下恢复或接近其基线性能。</p>
<p><img src="https://arxiv.org/html/2602.01158v1/corruptions.png" alt="视觉损坏类型"></p>
<blockquote>
<p><strong>图4</strong>：视觉损坏类型。从左上方起：(a) 干净基线，(b) 中心方块，(c) 高斯噪声，(d) 水平线(0.5)，(e) 水平线(0.2)，(f) 水滴。该图展示了实验中使用的六种不同类型的图像损坏，用于模拟传感器伪影。</p>
</blockquote>
<p>消融实验评估了CRT中不同组件的重要性。实验比较了：1) 完整CRT模型；2) 移除对抗性训练；3) 使用U-Net作为恢复骨干网络。结果表明，完整CRT模型在所有损坏类型上均取得了最佳性能。移除对抗性训练会导致性能下降，特别是在处理“高斯噪声”和“水滴”等需要恢复精细纹理的损坏时，这证实了对抗性损失在保留高频细节方面的作用。与U-Net骨干相比，基于变换器的CRT在恢复质量和计算效率方面都表现更好，突显了其专门化设计的优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 首次对图像损坏（传感器伪影）给最先进VLA模型带来的严重性能退化进行了系统性的量化分析，揭示了此前被忽视的鲁棒性漏洞；2) 提出了损坏恢复变换器，一个轻量级、即插即用且模型无关的预处理模块，能够有效恢复被损坏的视觉观测，使VLA在严重损坏下仍能保持接近基线的成功率，而无需重新训练VLA本身；3) 通过结合专门的变换器机制和对抗性训练，设计了一个适用于机器人高分辨率感知需求的图像恢复架构，并在两个主流基准上进行了全面验证。</p>
<p>论文自身提到的局限性在于，实验是在模拟环境中进行的，虽然损坏类型模拟了真实世界的传感器问题，但将其直接应用于物理机器人仍需进一步验证。此外，CRT模块会增加一定的推理延迟，尽管其设计是高效的，但对于有严格实时要求的应用仍需考虑。</p>
<p>本文对后续研究的启示在于：1) 强调了在追求VLA模型能力扩展的同时，必须重视其对现实世界干扰（尤其是传感器层面）的鲁棒性；2) 展示了一种有效的模块化增强思路，即通过上游专用模块处理特定类型的输入退化，这可以扩展到其他类型的干扰（如运动模糊、极端光照）；3) 将图像恢复领域的先进技术（如高效变换器、对抗性训练）成功适配到机器人感知任务中，为跨领域技术融合提供了范例。未来的工作可以探索将恢复模块与VLA进行端到端的联合轻量化微调，或扩展以处理视频序列中的时序相关性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在真实世界中因图像损坏（如传感器噪声、坏点）导致性能严重下降的问题，提出了一种即插即用、模型无关的解决方案。核心方法是引入**损坏恢复变换器**，通过对抗性训练目标，直接从损坏的视觉输入中恢复出干净的观测，无需对底层模型进行微调。实验表明，该方法能有效抵御视觉损坏，使模型在严重损坏下仍能保持接近基准的高成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01158" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>