<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>From Generated Human Videos to Physically Plausible Robot Trajectories - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>From Generated Human Videos to Physically Plausible Robot Trajectories</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.05094" target="_blank" rel="noreferrer">2512.05094</a></span>
        <span>作者: Roei Herzig Team</span>
        <span>日期: 2025-12-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，类人机器人控制领域的数据驱动方法主要依赖于高质量的人类运动捕捉数据来学习模仿策略。同时，视频生成模型在合成新颖场景下的人类动作方面取得了快速进展，有潜力作为机器人高层规划器。然而，实现这一潜力的核心挑战在于：生成视频通常包含噪声和形态失真，与真实视频相比，直接模仿这些动作对类人机器人而言非常困难。本文针对“如何让类人机器人零样本地执行生成视频中的人类动作”这一具体痛点，提出了一个结合4D人体重建与鲁棒强化学习策略的新视角。其核心思路是：首先将生成视频像素提升为4D人体运动轨迹并重定向至机器人形态，然后训练一个对噪声鲁棒、以3D关键点为条件、并辅以对称性正则化的强化学习策略（GenMimic）来实现零样本模仿。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架是一个两阶段流水线，如图3所示。第一阶段（训练与测试共用）：输入是生成的人类动作RGB视频，使用4D人体重建模型（如TRAM）提取每帧的全局姿态和SMPL参数，形成4D人体运动轨迹；随后通过重定向（如使用PHC）将其转换为适应目标机器人形态的关节空间目标（qt^goal）和全局3D关键点目标（pt^goal）。第二阶段（训练）：在IsaacGym模拟器中，使用重定向后的大型运动捕捉数据集AMASS作为训练数据，通过师生框架训练GenMimic策略。第二阶段（测试）：将来自生成视频并经过重定向的3D关键点目标（pt^goal）和机器人本体感知信息输入给已训练的GenMimic策略，策略输出期望关节角度（qt^des），最终由PD控制器生成扭矩驱动真实机器人。</p>
<p><img src="https://arxiv.org/html/2512.05094v2/x3.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图3</strong>：GenMimic方法整体框架。左侧为训练流程：在IsaacGym中使用重定向后的AMASS轨迹，通过师生框架训练策略，融入加权关键点奖励和对称性损失。右侧为测试流程：生成视频被提升为4D人体运动并重定向，作为目标输入给GenMimic策略，最终在真实世界执行。</p>
</blockquote>
<p>核心模块是GenMimic鲁棒跟踪策略，其创新点体现在两个核心设计上：</p>
<ol>
<li><strong>加权跟踪奖励</strong>：传统的跟踪奖励对所有身体关键点一视同仁。本文认为某些关键点（如末端效应器）对于任务执行和物理稳定性更为关键。因此，设计了加权跟踪奖励函数（公式1），通过对每个关键点的误差赋予不同的权重w_j（所有权重之和为1），使策略能够有选择地关注目标中最可靠和任务相关的部分。对于生成视频，这种设计偏向于末端效应器并远离不准确的下半身，从而产生稳定的模仿。</li>
<li><strong>对称性损失</strong>：人体具有固有的双边对称性。本文假设，让策略显式学习左右关键点之间的对称相关性，可以使其对生成视频中的单点噪声具有更强的鲁棒性。为此，在标准的PPO训练目标中增加了一个辅助的对称性损失（公式2）。该损失通过修改的概率比（公式3）来计算，其中Ts和Ta分别是状态和动作的双边对称映射函数。这个损失有效地增加了在对称状态Ts(st)下选择对称动作Ta(at)的可能性，前提是原始动作at在状态st下能产生高优势。</li>
</ol>
<p>在策略学习细节上，训练数据是经过筛选和重定向的AMASS数据集（8123个动作）。奖励函数除核心的加权跟踪奖励和对称性损失外，还包括关节角度、速度、身体位置、旋转的跟踪奖励，能量正则化项，模拟器特定的惩罚项（如脚滑、关节限位）以及稳定性鼓励项。此外，还应用了输入噪声和关键模拟参数（摩擦、质量、PD增益等）的域随机化以增强鲁棒性。策略网络是一个多层感知机（MLP），运行频率为50 Hz。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了论文自建的合成人体运动数据集<strong>GenMimicBench</strong>，该数据集包含428个视频，由两个先进的视频生成模型Wan2.1和Cosmos-Predict2生成，涵盖室内受控场景和野外风格场景，动作包括简单手势、复合动作和物体交互。</p>
<p><img src="https://arxiv.org/html/2512.05094v2/x2.png" alt="数据集示例"></p>
<blockquote>
<p><strong>图2</strong>：GenMimicBench数据集示例。展示了使用Wan2.1和Cosmos-Predict2生成的不同主体、环境和动作类型的合成视频。</p>
</blockquote>
<p>对比的<strong>基线方法</strong>包括：GMT（通用运动跟踪）、TWIST（师生策略）、BeyondMimic（自适应采样策略）。在模拟实验中，评估指标包括成功率（SR）、全局平均每关键点位置误差（MPKPE）、局部平均每关键点位置误差（LMPKPE），以及仅在未终止步长上计算的误差（MPKPE-NT, LMPKPE-NT）。</p>
<p><img src="https://arxiv.org/html/2512.05094v2/x5.png" alt="模拟实验结果"></p>
<blockquote>
<p><strong>表1</strong>：GenMimicBench上的模拟实验结果对比。GenMimic在特权教师策略（πs）上取得了86.77%的最高成功率，远超BeyondMimic的23.81%和TWIST的2.69%；在非特权学生策略（πo）上也以29.78%的成功率显著优于GMT（4.29%）和TWIST（7.52%）。这表明GenMimic在从噪声生成视频中模仿动作方面具有卓越的零样本泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.05094v2/x6.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>表2</strong>：在Unitree G1类人机器人上的真实世界部署结果。将43个动作分类评估视觉成功率（VSR）。原地动作（如挥手、抬手）成功率高达1.0，而涉及转身、迈步的复合动作成功率在0.2-0.6之间，展示了方法在真实硬件上产生连贯、物理稳定运动的能力，同时也揭示了动态平衡任务的挑战性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.05094v2/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>表3</strong>：在AMASS测试集和GenMimicBench上的消融研究。关键发现：1）使用3D关键点作为目标观测（3DP）比使用关节自由度（DoFs）在GenMimicBench上成功率更高（40.0% vs 23.8%）；2）在3DP基础上增加加权奖励（3DP+Weights）能大幅提升在AMASS和GenMimicBench上的成功率（分别达到97.7%和77.4%）；3）进一步引入对称性损失（3DP+Weights+Symmetry）实现了最佳性能，在GenMimicBench上成功率高达86.8%，且MPKPE-NT误差最低（20.46 cm），证明了两个核心组件对于处理生成视频噪声的有效性和互补性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了首个使类人机器人能够执行视频生成模型所产生动作的通用框架，将生成视频转化为物理可行的机器人轨迹；2）设计了GenMimic策略，通过加权关键点奖励和对称性损失，实现了对噪声合成视频的鲁棒零样本模仿；3）构建了GenMimicBench合成人体运动数据集，为评估零样本泛化和策略鲁棒性建立了基准。</p>
<p>论文提到的局限性包括：框架依赖外部的4D人体重建和重定向模块，这些模块的误差会影响最终性能；当前的策略未明确处理与物体的交互；在涉及复杂动态平衡（如快速转身）的任务上，真实世界的性能仍有下降。</p>
<p>这项工作为利用视频生成模型进行机器人高层规划开辟了一条有希望的路径。其启示在于：将强大的生成模型与精心设计、具有物理归纳偏好的低层控制策略相结合，是实现通用机器人行为合成的可行方向。后续研究可以探索端到端的优化、更复杂的物体交互处理，以及如何将环境上下文更直接地纳入控制循环。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何利用生成式视频模型作为机器人高级规划器，核心挑战是生成视频存在噪声与形态失真，导致人形机器人难以直接零样本模仿其中的人类动作。为此，作者提出两阶段方法：首先通过4D人体重建模型从视频提取人体运动轨迹并重定向至机器人形态；其次训练GenMimic策略——一种基于3D关键点、采用对称正则化与加权跟踪奖励的物理感知强化学习策略。实验基于合成的GenMimicBench数据集验证，该方法在仿真中优于基线，并在Unitree G1机器人上实现了无需微调的、连贯且物理稳定的零样本运动跟踪。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.05094" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>