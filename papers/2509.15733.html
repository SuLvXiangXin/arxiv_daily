<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.15733" target="_blank" rel="noreferrer">2509.15733</a></span>
        <span>作者: Deli Zhao Team</span>
        <span>日期: 2025-09-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作的有效性依赖于对3D场景几何的精确理解。当前主流方法主要分为两类：一类是显式几何方法，直接编码点云等3D数据，但依赖深度传感器，硬件成本高且易受噪声干扰；另一类是隐式几何方法，从标准RGB图像生成3D表示，但往往泛化能力不足，空间表示对多样化、未见过的环境不够鲁棒。本文针对“无需依赖深度传感器，同时实现鲁棒且精细的3D几何感知”这一具体痛点，提出了一个新视角：利用大规模预训练的3D重建模型，通过面向机器人任务的微调，从多视角RGB图像中提取密集的、像素级的空间特征，并结合语言引导的注意力机制来聚焦任务相关信息。本文的核心思路是：首先微调一个通用的3D重建模型（VGGT）使其适应机器人场景（RoboVGGT），然后提出一种全局注意力特征调制机制（G-FiLM），利用语言指令动态引导策略关注多视角输入中的任务相关区域，最终构建一个仅需RGB输入即可实现几何感知操作的策略GP3。</p>
<h2 id="方法详解">方法详解</h2>
<p>GP3的整体框架是一个两阶段训练流程。输入包括来自机载或第三方摄像头的多视角RGB图像序列、机器人本体感知状态以及语言指令。目标是预测对应的机器人动作序列。</p>
<p><img src="https://arxiv.org/html/2509.15733v1/resources/brief-3d.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：GP3架构总览。第一阶段（几何模型微调）：通过为RoboVGGT配备多视角相机参数头和深度估计头进行微调。第二阶段（动作预测训练）：引入基于全局注意力的特征线性调制（G-FiLM），使编码器能够通过自适应特征调制聚焦于任务相关的感兴趣区域（ROI）并抑制任务无关噪声。提取的空间特征随后与本体感知信息结合以预测动作。</p>
</blockquote>
<p>核心模块一：几何感知空间编码器（RoboVGGT）。该模块基于在通用3D重建上表现优异的VGGT模型构建。为了克服原模型高分辨率输入导致的计算瓶颈，并弥合其预训练数据与机器人操作场景之间的领域差距，论文在224×224分辨率下对模型进行微调。微调使用了来自RLBench、MetaWorld和RoboTwin的15万帧模拟数据以及2万帧真实世界数据，并采用包含相机损失和深度损失的多任务监督。相机损失使用Huber损失监督预测的相机参数，深度损失结合了基于不确定性的加权和梯度监督，以提升深度估计质量。微调后的RoboVGGT在机器人场景的重建质量上显著提升。</p>
<p><img src="https://arxiv.org/html/2509.15733v1/resources/robot_twins_518_ori.png" alt="重建结果对比"></p>
<blockquote>
<p><strong>图3</strong>：原始VGGT模型与微调后的RoboVGGT模型在模拟和真实输入下的重建结果对比。我们微调后的空间编码器在机器人操作场景上显示出明显的改进。</p>
</blockquote>
<p>核心模块二：基于全局注意力的特征线性调制（G-FiLM）。论文发现，单纯增加输入视角数量可能因引入冗余和任务无关信息而降低性能。受FiLM启发，G-FiLM利用语言嵌入投影得到的缩放和偏移向量，对视觉特征进行仿射变换。其关键创新在于，该调制仅应用于Transformer架构中的全局注意力层（负责捕捉跨视角关联），而非局部帧内注意力层。这使得模型能够根据语言指令，动态地增强任务相关空间特征并抑制噪声。</p>
<p><img src="https://arxiv.org/html/2509.15733v1/resources/aloha_before_518.png" alt="G-FiLM注意力可视化"></p>
<blockquote>
<p><strong>图5</strong>：提出的G-FiLM有效引导注意力关注与按钮按压任务相关的视觉内容。图中显示了归一化后的全局注意力图，证明了G-FiLM帮助模型聚焦于任务相关区域。</p>
</blockquote>
<p>动作训练阶段，将RoboVGGT提取的空间特征与本体感知状态、语言指令（通过G-FiLM）融合，通过轻量级策略头（MLP或扩散策略头）预测动作。整个训练过程使用LoRA进行参数高效微调，仅更新引入的低秩矩阵，保持预训练权重冻结。</p>
<p>与现有方法相比，GP3的创新点具体体现在：1) <strong>领域适应的几何编码器</strong>：首次将SOTA通用3D重建模型通过大规模机器人数据微调，获得针对机器人场景的、像素级的精细几何理解能力；2) <strong>任务感知的多视角融合</strong>：提出G-FiLM，实现了语言引导的、对多视角全局注意力的动态调制，有效解决了多视角信息冗余问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境中，使用了MetaWorld（50个任务）和RLBench（6个任务）基准。在真实世界中，基于Mobile ALOHA平台评估了4个家务式任务。实验平台使用了H20 GPU。</p>
<p><strong>对比方法</strong>：GP3与三大类共7种基线方法进行了对比：1) <strong>2D表示方法</strong>：CLIP、R3M、VC-1；2) <strong>显式3D策略</strong>：DP3（点云输入）、Lift3D（点云输入）；3) <strong>隐式3D表示方法</strong>：SPA（RGB输入）。所有方法遵循统一的训练协议。</p>
<p><strong>关键实验结果</strong>：<br>在模拟实验中，GP3仅使用多视角RGB输入即达到了SOTA性能。在MetaWorld上，平均成功率达到86.7%，相比最佳隐式3D基线SPA提升9.8%，相比最佳显式3D基线Lift3D提升16.9%。在RLBench上，平均成功率达到78.7%，分别超越SPA和Lift3D达26.7%和25.4%。值得注意的是，即使使用单视角输入（GP3-S），其性能也超越了多数基线，证明了其几何表示的有效性。</p>
<p><img src="https://arxiv.org/html/2509.15733v1/resources/appendix/metaworld/image_0.png" alt="模拟实验结果表"></p>
<blockquote>
<p><strong>表1</strong>：在MetaWorld和RLBench基准上的操作成功率对比。GP3在多视角和单视角设置下均取得了最佳或极具竞争力的性能。</p>
</blockquote>
<p>在真实世界实验中，GP3（多视角）在四个任务上均取得了最高成功率（Task1: 18/20, Task2: 19/20, Task3: 15/20, Task4: 17/20），显著优于Diffusion Policy和SPA等基线。论文还设计了一个3D感知验证实验（图6），其中GP3（多视角）是唯一未被平面打印纸球欺骗的方法，证明了其强大的真实3D理解能力。</p>
<p><img src="https://arxiv.org/html/2509.15733v1/resources/aloha_after_224.png" alt="真实世界3D感知验证"></p>
<blockquote>
<p><strong>图6</strong>：3D感知验证的实验设置，其中只有多视角输入的GP3没有被打印的平面纸球所欺骗。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>论文在MetaWorld的15个任务上进行了消融研究，关键发现如下：</p>
<ol>
<li><strong>微调的影响</strong>：对空间编码器进行机器人数据微调（w/ FT）带来了显著提升，例如单视角成功率从56.0%提升至70.9%。</li>
<li><strong>多视角输入的影响</strong>：增加视角数（如从1到2）初期能提升性能，但超过一定数量（如到4）后，未经调制的模型（w/o FT）性能会下降，说明引入了冗余噪声。</li>
<li><strong>G-FiLM的作用</strong>：在微调基础上加入G-FiLM（w/ FT+GF）取得了最佳效果（4视角平均成功率83.9%），其提升幅度超过了使用传统FiLM（w/ FT+F，80.3%），证明了G-FiLM在抑制多视角噪声、聚焦任务信息方面的优越性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>RoboVGGT</strong>，一个通过在精心策划的混合（模拟+真实）机器人数据集上微调而得的几何感知3D重建模型，能够在多样化机器人场景中实现鲁棒且可泛化的3D重建。</li>
<li>提出了<strong>G-FiLM</strong>，一种基于全局注意力的调制机制，能够促进对任务相关信息的关注，同时抑制多视角感知中的冗余信息。</li>
<li>基于上述贡献，提出了<strong>GP3</strong>策略框架，在不依赖特定传感器（如深度相机）的情况下，实现了鲁棒高效的多视角空间推理，在多个模拟和真实世界基准上达到了最先进的性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，多视角输入可能引入冗余和任务无关信息，若不加处理（如未使用G-FiLM）可能导致性能下降。此外，尽管使用了LoRA，基于Transformer的3D重建模型仍然具有较高的计算成本。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>多视角信息融合</strong>：G-FiLM的成功表明，针对机器人任务动态选择和理解多视角信息至关重要，这为设计更智能的视角选择或融合机制提供了方向。</li>
<li><strong>从通用模型到具身智能</strong>：本工作展示了将通用计算机视觉模型（VGGT）通过领域特定数据与目标有效适配到机器人领域的可行性，这一范式可推广至其他基础模型。</li>
<li><strong>效率优化</strong>：尽管性能卓越，计算效率仍是部署的挑战。后续工作可探索更轻量化的3D表示或更高效的注意力机制，以实现在嵌入式平台上的实时运行。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出GP3，旨在解决机器人操作中依赖专用深度传感器或RGB图像3D表示泛化性差的问题。其核心是RoboVGGT空间编码器，通过多视角RGB图像推断密集空间特征以估计深度和相机参数，构建紧凑的3D场景表示，并结合语言指令通过轻量策略头输出动作。实验表明，GP3在模拟基准测试中持续优于现有方法，并能以最小微调有效迁移至无深度传感器的真实机器人。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.15733" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>