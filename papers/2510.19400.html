<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.19400" target="_blank" rel="noreferrer">2510.19400</a></span>
        <span>作者: Baining Guo Team</span>
        <span>日期: 2025-10-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLM）是具身智能和视觉-语言-行动模型的基础，但对其的评估大多集中于单视角场景，对其整合多视角信息的能力探索不足。与此同时，多摄像头设置因其能提供互补视角以减轻遮挡和深度模糊，正日益成为机器人平台的标准配置。然而，VLM能否有效利用此类多视角输入进行机器人推理仍是一个开放性问题。现有空间推理基准（如EmbSpatial-Bench、Visual Spatial、OmniSpatial等）主要关注单视角关系、抽象推理或非具身的多视角感知，缺少将具身操作与多视角感知相结合的评价。本文针对这一空白，提出了首个专门为评估机器人操作场景中VLM的多视角空间推理能力而设计的基准测试MV-RoboBench。其核心思路是，从真实机器人演示数据中构建一个包含空间理解和机器人执行两大类、共八个子任务的手工标注问答对数据集，用以系统评估VLM在多视角下的感知与决策能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>MV-RoboBench的整体构建流程（Pipeline）包含三个阶段：数据收集、QA生成和人工参与的质重审查（Human-in-the-loop Quality Review）。输入是来自AgiWorld和BridgeV2数据集的同步多视角图像对，经过筛选和标注后，输出是高质量的、包含1708个多项选择题的基准测试集。</p>
<p><img src="https://arxiv.org/html/2510.19400v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MV-RoboBench的构建流程，包含数据收集、QA生成和人工质量审查三个阶段。</p>
</blockquote>
<p>基准的核心是八个精心设计的子任务，分为两个互补的类别：</p>
<ol>
<li><strong>空间理解</strong>：侧重于跨多个相机视角的感知和推理，评估多视角观察是否能被整合为场景的一致3D表征。<ul>
<li><strong>跨视角匹配</strong>：识别不同视角下的同一物体。</li>
<li><strong>距离判断</strong>：评估物体间的相对距离。</li>
<li><strong>视角识别</strong>：推理视角变换关系。</li>
<li><strong>3D空间一致性</strong>：检验模型是否能保持物体在3D空间中相对位置的一致性。</li>
</ul>
</li>
<li><strong>机器人执行</strong>：将空间推理延伸至具身决策，探究多视角信息是否能有效支持操作任务。<ul>
<li><strong>行动规划</strong>：选择完成任务的合适多步序列。</li>
<li><strong>步骤执行</strong>：验证下一个单步移动是否正确。</li>
<li><strong>轨迹选择</strong>：评估候选运动路径的可行性。</li>
<li><strong>功能可供性识别</strong>：评估物体特定交互的可行性。</li>
</ul>
</li>
</ol>
<p>这些任务大多以成对图像作为输入，强调互补视角的整合，以解决遮挡和深度模糊问题。基准的构建通过任务特定模板和人工标注完成，并经过多轮人工审查和答案分布再平衡，以确保质量和公平性。</p>
<p><img src="https://arxiv.org/html/2510.19400v1/x1.png" alt="任务示例"></p>
<blockquote>
<p><strong>图1</strong>：MV-RoboBench八个任务的代表性问答实例简化版（左为空间任务，右为机器人任务）。</p>
</blockquote>
<p>本文的创新点在于首次将同步多视角输入与机器人操作场景中的空间和机器人推理结合起来，创建了一个系统性的评估基准。此外，论文还探索了三种受思维链启发的增强方法用于多视角理解：1) <strong>文本增强</strong>：使用GPT-4.1生成场景描述作为文本CoT；2) <strong>视觉增强</strong>：使用VGGT进行新颖视角合成作为视觉CoT；3) <strong>结构增强</strong>：使用MoGe-2提供深度先验作为结构CoT，以分析这些辅助信息对模型性能的影响。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在MV-RoboBench基准上进行，使用了广泛的模型，包括：盲测模型（随机、GPT-3.5/4-turbo）、专有模型（GPT-4o系列、GPT-4.1系列、Claude-3.5/3.7、Gemini-2.x flash）、专有推理模型（o4-mini、GPT-5系列、Claude-3.7-think、Gemini-2.5-pro）、开源模型（Gemma-3系列、InternVL3系列、Qwen2.5-vl系列）以及开源MoE模型（Llama-4-Scout/Maverick）。评估指标为多项选择题的准确率，并进行了独立的人类评估作为参考。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>主流模型性能</strong>：如表2所示，性能呈现从感知导向系统向显式推理优化架构提升的趋势。专有多模态模型如GPT-4.1达到30.90%准确率，开源VLM如Qwen2.5-vl-72B为24.29%。最大的提升出现在专有推理类别：GPT-5达到56.41%，Gemini-2.5-pro为49.52%，o4-mini为46.47%。然而，所有模型均远低于人类91.04%的准确率。</li>
<li><strong>任务分析</strong>：3D空间一致性任务尤其具有挑战性，大多数非推理模型的性能接近甚至低于随机猜测准确率（19.07%），表明它们未能有效利用多视角信息。机器人子任务在推理架构下也显示出显著改进。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.19400v1/x3.png" alt="实验结果"></p>
<blockquote>
<p><strong>图4</strong>：各模型家族领先代表与人类在MV-RoboBench上的性能对比。所有模型均显著低于人类水平，且3D空间一致性任务表现最差。</p>
</blockquote>
<ol start="3">
<li><strong>CoT风格增强效果</strong>：如表3所示，三种增强方法的效果因模型而异且不均衡。对于Qwen2.5-vl-7B，辅助线索带来可忽略或负面变化；Gemma-3-12B从CoT提示中受益显著；GPT-4.1从深度先验中获益最明显。总体而言，合成新视角更可能损害性能，深度先验仅在骨干网络有足够容量时才有效，CoT增强对中等容量的开源模型最有效。</li>
<li><strong>内部相关性分析</strong>：如图5所示，在多视角操作任务中，空间准确率与机器人准确率存在正相关，但这种关系高度依赖于模型。专有和推理优化系统显示出单调上升趋势，而大多数开源VLM聚集在随机猜测准确率附近。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.19400v1/x4.png" alt="相关性分析"></p>
<blockquote>
<p><strong>图5</strong>：模型在MV-RoboBench上空间任务与机器人任务准确率的散点图。显示出正相关趋势，但模型依赖性很强。</p>
</blockquote>
<ol start="5">
<li><strong>外部可迁移性分析</strong>：以OmniSpatial作为单视角空间基准进行对比。如图6所示，除了专有推理模型外，强大的单视角准确率并不能可靠地迁移到多视角具身推理中。许多在OmniSpatial上表现良好的模型在MV-RoboBench上仍接近随机水平。这表明多视角机器人推理提出了根本不同的需求。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.19400v1/x5.png" alt="泛化分析"></p>
<blockquote>
<p><strong>图6</strong>：模型在单视角基准OmniSpatial与MV-RoboBench上准确率的对比。表明单视角空间能力不能可靠地迁移到多视角具身推理中。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个在机器人操作场景中整合空间与机器人推理、并采用同步多视角输入的基准测试MV-RoboBench。</li>
<li>通过大量实验表明，当前最先进的VLM在多视角机器人场景中仍面临巨大挑战，性能远低于人类，并揭示了CoT风格增强方法效果参差不齐。</li>
<li>提供了相关性分析，发现多视角机器人场景中空间智能与机器人任务执行呈正相关，且现有通用单视角空间基准的强性能不能可靠地迁移到本文评估的机器人空间任务中。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括，基准的规模（1.7k QA对）可能限制其对日益增长的大型VLM的挑战性；对增强方法（如CoT、视图合成）的探索是初步的，未来需要更深入的研究。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>多视角机器人推理是一个独特且具有挑战性的领域，需要像MV-RoboBench这样的专门基准来推动进展。</li>
<li>简单的提示级增强不足以可靠提升多视角推理性能，未来的进展需要显式几何理解与结构化推理之间更紧密的耦合。</li>
<li>研究应关注开发能够有效整合多视角信息、形成连贯3D场景理解并支持具身决策的新模型架构和训练方法。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前视觉语言模型评估集中于单视角、未能充分考察其在机器人多视角场景中空间推理能力的问题，提出了专门用于评估机器人操作中多视角空间推理的基准测试MV-RoboBench。该基准包含1.7k个涵盖空间理解与机器人执行的问答对。通过评估现有主流及增强模型，核心实验结论表明：当前最优模型性能远低于人类水平；多视角下空间智能与任务执行呈正相关；且单视角基准上的优势无法有效迁移至该多视角机器人任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.19400" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>