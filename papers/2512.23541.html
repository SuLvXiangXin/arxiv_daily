<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Act2Goal: From World Model To General Goal-conditioned Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Act2Goal: From World Model To General Goal-conditioned Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.23541" target="_blank" rel="noreferrer">2512.23541</a></span>
        <span>作者: Jianlan Luo Team</span>
        <span>日期: 2025-12-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作任务指定需要兼具表达性和精确性。视觉目标提供了一种紧凑且明确的指定方式，因此目标条件策略（GCPs）通过将当前观测和目标视觉目标直接映射到动作来学习策略。然而，现有方法在长视野操作任务中表现不佳，其根本原因在于它们依赖直接的单步动作预测，缺乏对任务进展、中间可行性或长视野一致性的显式建模。这导致策略难以区分哪些动作是真正向目标迈进的，哪些只是匹配了演示数据中观察到的局部状态-动作相关性。近期世界模型的进展为解决此问题提供了可能，其能够生成以任务指令为条件的未来视觉状态预测。本文提出一个目标条件的世界模型，它能生成连接当前观测与目标视觉目标的一系列合理中间状态，从而为GCPs提供显式的、基于视觉的任务演化表示。然而，仅预测视觉路径还不够，执行长视野任务还需要平衡全局一致性与局部反应性。本文的核心思路是：提出Act2Goal，一个集成了目标条件视觉世界模型与新颖的多尺度时序分解机制（MSTH）的通用目标条件策略，以实现对长视野目标的推理与对局部扰动的快速响应。</p>
<h2 id="方法详解">方法详解</h2>
<p>Act2Goal的整体框架分为离线训练与在线改进两个主要部分。给定当前观测和目标视觉目标，模型首先通过目标条件世界模型（GCWM）想象出一条通向目标的中间视觉状态序列。随后，通过多尺度时序哈希（MSTH）机制将该视觉轨迹分解为用于细粒度闭环控制的稠密近端帧，以及用于锚定全局任务一致性的稀疏远端帧。最后，一个同构的动作专家通过端到端的交叉注意力，耦合来自世界模型的多尺度特征与机器人本体感知状态，生成遵循MSTH结构的动作序列。在部署阶段，模型可通过基于 hindsight 目标重标记和 LoRA 微调的在线自主改进机制，利用自身交互数据持续优化。</p>
<p><img src="https://arxiv.org/html/2512.23541v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：系统总览。Act2Goal集成视觉世界模型与多尺度时序控制，以应对长视野操作。经过大规模离线模仿学习后，模型在已见场景表现出高性能，并对未见场景有强泛化能力。无需奖励的在线自主改进阶段通过rollout-目标重标-优化循环进一步提升模型性能。</p>
</blockquote>
<p>核心模块包括目标条件世界模型和多尺度时序哈希机制。世界模型基于 Genie Envisioner 架构修改，移除了所有语言条件组件，构建为一个纯视觉模型。它采用连续流匹配方法进行生成建模，学习一个从随机噪声到以当前观测潜在表示 (z_t) 和目标潜在表示 (z_g) 为条件的结构化视觉序列的变换：(z_{pred}=f_{\theta}(z_{t},z_{g},\epsilon))。推理时通过确定性流过程逐步 refine 噪声潜在表示。动作专家网络与世界模型同构（DiT块数量相同但宽度更窄），其流匹配过程同时以世界模型提供的分层过渡特征 (c_w) 和本体感知状态 (c_p) 为条件，生成动作 (a_{pred}=g_{\phi}(c_{w},c_{p},\zeta))。</p>
<p><img src="https://arxiv.org/html/2512.23541v1/x3.png" alt="模型架构"></p>
<blockquote>
<p><strong>图3</strong>：模型架构。左侧，多视角输入帧（当前观测和目标）通过视频编码器编码为潜在表示，与噪声潜在表示拼接后，通过Video DiT块精炼为MSTH潜在帧。右侧，机器人状态和来自世界模型的多尺度特征通过交叉注意力输入到同构的Action DiT块中，生成MSTH结构的动作。</p>
</blockquote>
<p>多尺度时序哈希（MSTH）是该方法的创新关键。对于总长度为 (K) 的想象轨迹，MSTH将其划分为近端和远端两段。近端段包含高频的短视野视觉状态 ({s_{t+kr}}<em>{k=1}^{P/r})，用于捕捉细粒度局部动态。远端段包含 (M) 个稀疏采样的视觉状态 ({s</em>{t+d_m}}_{m=1}^{M})，其索引 (d_m) 由对数间隔公式 (d_m=P+\lfloor\frac{K-P}{\log(M+1)}\cdot\log(m+1)\rfloor) 确定，导致时间间隔随视野延长而增加，提供粗略但目标对齐的长期指导。预测的动作序列遵循相同的多尺度结构，但近端动作在每一步都预测，以实现密集运动控制，而远端动作与远端视觉状态对齐，作为长期指导。部署时仅执行近端动作，远端预测作为潜在指导。</p>
<p>离线训练分为两个阶段。第一阶段联合训练世界模型和动作专家，使用流匹配损失（视觉损失 (\mathcal{L}<em>v) 和动作损失 (\mathcal{L}<em>a)）的组合目标 (\mathcal{L}</em>{\text{stage1}}=\mathcal{L}</em>{v}+\lambda\cdot\mathcal{L}<em>{a})（(\lambda=0.1)），确保生成的视觉轨迹不仅视觉合理而且可执行。第二阶段使用行为克隆，仅用动作流匹配损失 (\mathcal{L}</em>{\text{stage2}}=\mathcal{L}_{a}) 端到端微调整个模型，优化视觉表示以更好地服务于动作规划。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在 Robotwin 2.0 仿真基准和真实世界的 AgiBot Genie-01 机器人平台上进行。评估任务包括仿真中的四个任务（移动罐锅、拾取双瓶、放置空杯、放置鞋子，各有简单和困难模式）以及真实世界的三个任务（白板写字、甜点摆盘、插拔操作），并设计了域内（ID）和域外（OOD）测试配置。对比的基线方法包括 DP-GC、(\pi_{0.5})-GC 和 HyperGoalNet。</p>
<p>在 Robotwin 2.0 仿真基准上，Act2Goal 在全部简单模式任务和三个困难模式任务上表现最优。例如，在“拾取双瓶”任务中，简单模式成功率达 0.80，远超 (\pi_{0.5})-GC 的 0.13；在困难模式下，Act2Goal 取得了 0.43 的成功率，而其他基线均为 0.00，显示了其卓越的泛化能力。</p>
<p><img src="https://arxiv.org/html/2512.23541v1/x4.png" alt="真实世界评估"></p>
<blockquote>
<p><strong>图4</strong>：真实世界评估。展示了三个真实世界任务（白板写字、甜点摆盘、插拔操作）的域内和域外测试配置。对于每个任务，“头部视角目标”显示目标，“模型执行过程”展示机器人执行过程；这些设置用于以成功率作为指标评估模型的泛化能力。</p>
</blockquote>
<p>在真实世界任务中，仅经过离线模仿学习的 Act2Goal 显著优于所有基线。例如在白板写字任务中，ID 成功率 0.93，OOD 成功率 0.90；在甜点摆盘任务中，ID 成功率 0.75，OOD 成功率 0.48；在插拔操作任务中，ID 成功率 0.45，OOD 成功率 0.30。而多个基线在部分任务上成功率为 0.00，凸显了 Act2Goal 方法的鲁棒性。</p>
<p>在线自主改进实验表明，模型可通过几轮在线交互快速提升性能。在 Robotwin 2.0 的四个困难模式场景中，经过约3轮在线训练后性能收敛，最大成功率达到预训练基线的8倍。消融实验对比了三种数据选择策略：仅用成功、全部使用、仅用失败。结果表明，使用全部 rollout 数据效果最优，但即使仅使用失败的 rollout 也能带来明显改进，验证了 hindsight 重标记的有效性。</p>
<p><img src="https://arxiv.org/html/2512.23541v1/x5.png" alt="在线自主改进场景"></p>
<blockquote>
<p><strong>图5</strong>：在线自主改进场景。展示了来自RoboTwin 2.0基准的四个域外场景，对应于移动罐锅、拾取双瓶、放置空杯和放置鞋子的困难测试模式。这些场景作为验证自主改进有效性的测试平台。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.23541v1/x6.png" alt="在线训练性能"></p>
<blockquote>
<p><strong>图6</strong>：Robotwin 2.0中的在线训练性能。（左）四个困难模式场景的多轮成功率，显示在收敛前经过约3轮持续改进。（右）三种用于rollout的数据选择策略性能：使用所有rollout产生最优结果，而即使仅使用失败rollout也能实现明显改进。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1) 提出了一个端到端的目标条件策略 Act2Goal，首次将纯视觉的目标条件世界模型与运动控制集成，实现了对未见物体、环境和目标的强零样本泛化。2) 引入了多尺度时序哈希（MSTH）机制，通过时序分解平衡了长视野规划与闭环局部控制。3) 开发了一种基于 hindsight 目标重标记和 LoRA 微调的、无需外部奖励的在线自主改进方法，使模型能在部署中快速自我适应。</p>
<p>论文自身提到的局限性在于，尽管经过离线模仿学习，模型在部署到物理机器人时，面对全新的任务、环境、物体和运动链，实现高性能仍然具有挑战性——这也是模仿学习策略的普遍局限。为此，论文提出了在线改进机制作为解决方案。</p>
<p>本工作对后续研究的启示在于：首先，世界模型可以作为提供结构化中间指导的强大表示，弥补传统目标条件策略在长视野推理上的不足。其次，多尺度时序抽象对于协调全局一致性与局部反应性至关重要。最后，高效的参数微调技术（如LoRA）使得在边缘设备上进行快速、持续的在线策略适应成为可能，推动了真正自主、自改进机器人系统的发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉目标条件策略在长时程操作中因缺乏任务进展显式建模而性能不佳的问题，提出Act2Goal方法。其核心是整合目标条件视觉世界模型与多尺度时间控制：世界模型预测中间视觉状态序列，多尺度时间哈希（MSTH）技术将其分解为用于细粒度控制的密集近端帧和保证全局一致性的稀疏远端帧，并通过交叉注意力与运动控制耦合。真实机器人实验表明，该方法在分布外任务上实现了零样本强泛化，通过在线自适应，成功率在数分钟内从30%显著提升至90%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.23541" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>