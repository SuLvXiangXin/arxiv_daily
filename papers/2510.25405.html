<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sim-to-Real Gentle Manipulation of Deformable and Fragile Objects with Stress-Guided Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Sim-to-Real Gentle Manipulation of Deformable and Fragile Objects with Stress-Guided Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.25405" target="_blank" rel="noreferrer">2510.25405</a></span>
        <span>作者: Florian T. Pokorny Team</span>
        <span>日期: 2025-10-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，易变形和易碎物体（DFOM）的机器人操作主要依赖于精确的物体模型、专用传感器（如触觉、力觉）或特殊设计的软体末端执行器。这些方法增加了系统复杂性和成本，且往往缺乏泛化能力。本文针对的核心痛点是：在确保物体不被损坏（即内部应力低于损伤阈值）的前提下，完成操作任务。这面临两大挑战：复杂的接触与物体动力学难以建模，以及严格的安全约束。本文提出了一种新的视角：利用模拟器中可计算的应力信息作为物理先验，通过设计应力惩罚奖励，在基于视觉的强化学习（RL）框架中显式地鼓励轻柔操作行为。核心思路是：在模拟中训练一个结合了应力惩罚奖励、课程学习和离线演示的RL策略，然后将其零次迁移到现实世界，实现无需精确模型或额外传感器的安全操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的应力引导强化学习框架整体流程如图2所示，包含三个阶段：1）课程学习：代理首先在与任务对象形状相同的刚性替代物上进行训练，以快速学习任务目标；2）应力引导学习：切换到软体仿真环境，引入应力惩罚奖励以鼓励安全操作，同时利用离线演示引导策略；3）零次现实迁移：将训练好的策略直接部署到与现实设置匹配的机器人系统上。</p>
<p><img src="https://arxiv.org/html/2510.25405v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：应力引导RL框架概览。训练从课程学习开始（先刚性后软体），在软体阶段结合应力惩罚奖励和专家演示进行学习，最终实现零次模拟到现实迁移。</p>
</blockquote>
<p><strong>核心模块与技术细节：</strong></p>
<ol>
<li><strong>状态、观测与动作空间</strong>：问题被建模为部分可观测马尔可夫决策过程（POMDP）。状态空间包含机器人关节信息、物体粒子状态及其内部应力。观测空间包括物体部分点云的低维编码、点云质心、机器人末端执行器位姿和夹爪开合度。动作空间控制末端执行器的笛卡尔位移、旋转位移（轴角表示）和夹爪位移。</li>
<li><strong>应力惩罚奖励设计</strong>：奖励函数由任务成功奖励 $R_{\text{success}}$ 和应力惩罚奖励 $R_{\text{stress}}$ 组成。$R_{\text{success}}$ 包含完成任务（如抓取、推动）的稀疏奖励和实现子目标（如接近物体）的稠密奖励。$R_{\text{stress}}$ 是关键创新，旨在惩罚可能导致损伤的高应力。直接使用最大应力 $\sigma_{\text{max}}$ 方差过大，而全局统计量（如平均应力 $\bar{\sigma}$）又可能掩盖局部高应力区域（如图3所示）。因此，本文提出一种平衡方案，惩罚项结合了平均应力 $\bar{\sigma}$ 和粒子应力前10%的中位数 $\hat{\sigma}<em>{\text{top10}}$，并使用二次变换来更严厉地惩罚高应力：$R</em>{\text{stress}} = -\frac{1}{\beta}{(\alpha\hat{\sigma}_{\text{top10}}+(1-\alpha)\bar{\sigma})^{2}}$，其中 $\alpha$ 用于平衡，$\beta$ 是缩放因子。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.25405v1/x2.png" alt="应力统计量对比"></p>
<blockquote>
<p><strong>图3</strong>：全局统计量（平均应力和中位应力）可能无法反映局部大变形（右图），而最大应力能更可靠地捕捉此信息。</p>
</blockquote>
<ol start="3">
<li><strong>策略训练与架构</strong>：策略网络使用PointNet编码物体点云，使用MLP编码机器人状态，将两者的潜在表示拼接后，输入到基于MLP的Actor和Critic网络。策略优化采用软演员-评论家（SAC）算法。</li>
<li><strong>离线演示</strong>：为避免策略为最小化应力而避免接触物体，导致任务失败，本文引入了人类演示数据来引导策略。采用RLPD框架，在经验回放池中对称混合50%的离线演示和50%的在线交互数据，从而利用人类先验知识提升样本效率和学习稳定性。</li>
<li><strong>课程学习</strong>：为加速训练，策略先在刚性物体替代品上学习任务基本技能，收敛后再切换到软体仿真中学习考虑变形的精细操作。这利用了早期学习阶段（如接近物体）对物体动力学不敏感的特性。</li>
<li><strong>缩小模拟与现实差距</strong>：采用域随机化策略，包括随机化物体的摩擦系数、杨氏模量和泊松比，以及物体和机器人末端的初始位姿。此外，在观测（点云、质心、末端位姿、夹爪位置）上添加零均值高斯噪声以应对感知不确定性。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用UFactory xArm 7机械臂和标准平行夹爪。对象为豆腐（低刚度、低屈服应力）。模拟器采用Genesis（内置Taichi软体物理引擎，使用MPM方法）。现实世界使用Intel RealSense L515相机获取点云。评估了两个任务：<strong>抓取</strong>（拿起豆腐）和<strong>推动</strong>（将豆腐推到目标区域）。</p>
<p><strong>对比方法</strong>：包括多个消融版本：Naive（无课程C、无应力惩罚奖励SPR、无演示D）、仅C、仅SPR、仅D、Ours-1 (SPR+D)、Ours-2 (C+SPR+D)，以及行为克隆（BC）基线。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟定量结果（圆柱形豆腐）</strong>：如表I和表II所示。<ul>
<li><strong>抓取任务（表I）</strong>：Ours-2 (C+SPR+D) 在保持高成功率（0.98）的同时，显著降低了所有应力指标（$\bar{\sigma}$, $\bar{\sigma}<em>{\text{top5}}$, $\sigma</em>{\text{max}}$），例如 $\sigma_{\text{max}}$ 相比Naive方法降低了约48.6%。仅使用SPR会导致策略过于保守，成功率降至0。仅使用演示（Demo）虽能保持高成功率，但降应力效果有限。Ours-1 (SPR+D) 表现与Ours-2相当甚至略优。</li>
<li><strong>推动任务（表II）</strong>：趋势类似，Ours-1和Ours-2在成功率和应力间取得了良好平衡。BC方法应力最低，但成功率也最低（0.36）。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.25405v1/x3.png" alt="抓取任务定性结果"></p>
<blockquote>
<p><strong>图5</strong>：圆柱形豆腐抓取任务的定性结果。绿色勾表示任务成功且无可见损伤，红叉表示任务失败或豆腐受损。本文方法（Ours-1, Ours-2）实现了成功且轻柔的抓取。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.25405v1/x4.png" alt="推动任务定性结果"></p>
<blockquote>
<p><strong>图6</strong>：圆柱形豆腐推动任务的定性结果。损害豆腐的策略（如Naive）在推动时会紧紧夹住豆腐，而本文的方法（Ours-1, Ours-2）则轻柔合拢夹爪进行可靠推动。</p>
</blockquote>
<ol start="2">
<li><strong>形状泛化（立方体豆腐）</strong>：如表III和表IV所示，在立方体豆腐上的实验结果与圆柱体类似，证实了方法对不同形状的适应性。</li>
<li><strong>现实世界零次迁移</strong>：使用完全吸水的海绵作为易碎替代物进行抓取实验，以“水分流失百分比”间接量化挤压损伤程度。如表V和图7所示，本文方法（Ours-1, Ours-2）在保持高抓取成功率（1.00, 0.90）的同时，水分流失远低于无应力惩罚的Naive方法（41.90%），与BC方法（9.52%）接近，但成功率远超BC（0.40）。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.25405v1/figs/foam-quality.png" alt="海绵水分流失"></p>
<blockquote>
<p><strong>图7</strong>：不同方法执行抓取任务后，从吸水海绵中挤出的水分（紫色标记区域）。面积越小表示损伤越小。本文方法（Ours-1）在高效完成任务的同时，造成的损伤很小。</p>
</blockquote>
<p><strong>消融实验总结</strong>：应力惩罚奖励（SPR）是降低应力的必要组件；离线演示（D）是克服SPR导致策略保守、避免接触的关键引导机制；课程学习（C）主要作用是加速训练，对最终性能的贡献与演示有一定互补性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了首个将物体易碎性显式纳入学习过程的、面向3D易变形易碎物体操作的模拟到现实视觉运动学习框架。2) 设计了一种结合局部与全局应力信息的应力惩罚奖励函数，有效引导策略学习轻柔操作。3) 通过整合离线演示和课程学习，稳定了在冲突奖励（任务成功 vs. 低应力）下的学习过程，并实现了零次模拟到现实迁移。</p>
<p><strong>局限性</strong>：论文提到方法依赖于模拟器中应力的计算精度，且模拟到现实的迁移效果受限于域随机化的范围和仿真保真度。</p>
<p><strong>后续启示</strong>：该方法展示了利用模拟中的物理量（如应力）作为安全先验来引导策略学习的有效性，为安全机器人操作提供了新思路。未来工作可探索更高效的仿真方法、更精细的损伤模型，以及将类似框架扩展到其他需要安全约束的操作任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对柔性和脆弱物体的机器人操作，核心问题是避免因过度应力造成物体损坏。提出了一种应力引导的强化学习框架，关键技术包括：在奖励函数中引入应力惩罚以抑制损伤，结合离线演示进行学习引导，并设计从刚性代理到柔性物体的渐进式课程。实验表明，该策略能零样本从仿真迁移到现实，成功完成豆腐抓取等任务。与普通强化学习策略相比，在达成任务目标的同时，能将施加于物体的应力降低36.5%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.25405" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>