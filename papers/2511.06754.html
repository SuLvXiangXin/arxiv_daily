<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.06754" target="_blank" rel="noreferrer">2511.06754</a></span>
        <span>作者: Ngan Le Team</span>
        <span>日期: 2025-11-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人视觉-语言-动作（VLA）模型的主流方法，如OpenVLA、π0等，普遍依赖于预训练的视觉编码器（如DINOv2、SigLIP）来生成密集的视觉令牌（通常为256-512个）。这些令牌虽然信息丰富，但常常将物体、背景和交互线索纠缠在一起，导致计算成本高昂、可解释性差，并可能模糊动作解码器所需的关键任务相关信息。</p>
<p>本文针对这一痛点，提出了一种新的视角：将物体及物体间关系作为核心的、紧凑的表征基础，用于多任务机器人操作。本文认为，纯粹的物体中心表示无法捕捉关键的关系线索（如夹爪与物体的交互），而密集表示则效率低下。因此，核心思路是设计一个基于槽注意力的框架，从密集视觉输入中提取少量任务相关的物体槽和关系槽，形成结构化、高效且可解释的表征，用于驱动动作解码。</p>
<h2 id="方法详解">方法详解</h2>
<p>SlotVLA的目标是通过将密集的视觉嵌入转化为紧凑的物体-关系表征，实现令牌高效的视觉运动推理。整体框架采用两阶段训练策略。</p>
<p><img src="https://arxiv.org/html/2511.06754v2/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：SlotVLA整体框架。<strong>阶段1</strong>：训练任务感知物体中心编码器，包含带时序一致性的槽注意力和任务感知槽过滤器。<strong>阶段2</strong>：冻结阶段1的参数，引入关系中心编码器，并与动作解码器联合训练，实现关系推理和最终动作预测。</p>
</blockquote>
<p><strong>阶段1：任务感知物体中心编码器</strong><br>该编码器旨在将密集视觉令牌 <code>V_t</code>（N个）压缩为少量物体中心槽 <code>S_t</code>（<code>N_S</code>个，约4个），并过滤掉任务无关的物体。它包含两个核心模块：</p>
<ol>
<li><strong>带时序一致性的槽注意力</strong>：在视觉编码器之上，使用槽注意力机制将视觉特征映射到一组可学习的槽 <code>˜S_t</code>。通过GRU进行迭代优化。为确保物体身份的时序一致性，采用“槽结转”机制：除第一帧随机初始化外，后续帧的槽以前一帧优化后的槽进行初始化。</li>
<li><strong>任务感知槽过滤器</strong>：并非所有槽都与当前任务相关。该模块通过双向交叉注意力（BCA）和变换器层，计算每个槽相对于语言任务描述 <code>P</code> 的相关性分数 <code>π_t</code>。在训练和推理时，仅保留得分最高的 <code>N_S</code> 个槽，形成最终的任务感知物体中心令牌 <code>S_t</code>。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.06754v2/x4.png" alt="槽分解结果"></p>
<blockquote>
<p><strong>图4</strong>：槽分解可视化示例。给定任务“将碗放在炉子上”，任务相关的槽（碗、炉子）正确绑定到对应物体，而无关的槽则分散在背景中。</p>
</blockquote>
<p><strong>阶段2：关系中心编码器</strong><br>为捕捉对操作至关重要的交互关系（如夹爪-物体），本模块引入一组可学习的关系查询 <code>˜R_t</code>。这些查询通过一个交叉注意力块（CAB）依次整合密集视觉特征 <code>V_t</code> 和物体中心令牌 <code>S_t</code> 的信息，输出关系中心表征 <code>R_t</code>（<code>N_R</code>个令牌）。公式为：<code>R_t = CAB(CAB(˜R_t, V_t), S_t)</code>。</p>
<p><strong>动作解码</strong><br>将物体槽 <code>S_t</code>、关系槽 <code>R_t</code>、语言嵌入 <code>P</code> 和本体感知 <code>o_t</code> 拼接，输入到一个采用LoRA微调的大型语言模型（LLM）中进行动作解码。动作预测被定义为分类问题，通过贪心解码从动作logits <code>A_t</code> 中获得离散化动作 <code>a_t</code>。由于 <code>N_S + N_R</code> 远小于原始密集令牌数 <code>N</code>，该方法实现了显著的令牌效率提升。</p>
<p><strong>训练目标与策略</strong></p>
<ul>
<li><strong>阶段1损失</strong>：监督物体中心编码器，包含槽注意力监督损失（边界框损失、物体性损失、分割损失）和时序跟踪损失（鼓励同一物体在不同帧中的槽特征相似），以及任务感知槽过滤器监督损失（用加权的二元交叉熵监督相关性分数）。</li>
<li><strong>阶段2损失</strong>：冻结阶段1参数，使用标准交叉熵损失训练关系编码器和动作解码器，以预测正确的动作。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在提出的LIBERO+基准数据集上进行评估，该数据集包含四个子集：LIBERO-Goal、LIBERO-Object、LIBERO-Spatial和LIBERO-Long，涵盖了不同布局、物体数量和任务复杂度，强调物体-关系推理。</p>
<p><img src="https://arxiv.org/html/2511.06754v2/x2.png" alt="数据集概览"></p>
<blockquote>
<p><strong>图2</strong>：LIBERO+数据集概览。在原始演示数据基础上，增加了边界框、物体掩码、实例级时序ID跟踪、深度图（掩码区域）以及任务相关物体标注。</p>
</blockquote>
<p><strong>对比方法</strong>：主要对比基线是使用密集令牌的OpenVLA。同时，本文还对比了仅使用物体中心槽（OC）的简化版本（即SlotVLA去掉关系编码器）。</p>
<p><strong>关键定量结果</strong>：</p>
<p><img src="https://arxiv.org/html/2511.06754v2/x5.png" alt="主要结果表"></p>
<blockquote>
<p><strong>表II</strong>：在LIBERO+四个子集上的成功率对比。SlotVLA（ORC）在多数子集上取得了最佳或极具竞争力的平均成功率（如LIBERO-Goal: 0.86，LIBERO-Object: 0.91），同时使用的视觉令牌数（20-28个）相比OpenVLA（256个）减少了9-13倍，计算量（GFLOPs）减少了约3倍。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<blockquote>
<p><strong>表III</strong>：任务感知槽过滤器的消融研究。对比显示，在物体中心（OC）和物体-关系中心（ORC）两种设置下，启用语言引导的任务感知过滤（✓）通常能带来性能提升或保持高性能，尤其是在物体数量较多的LIBERO-Object和LIBERO-Long子集上，证明了过滤无关物体的必要性。</p>
</blockquote>
<p><strong>效率与性能权衡</strong>：</p>
<p><img src="https://arxiv.org/html/2511.06754v2/x1.png" alt="令牌策略对比"></p>
<blockquote>
<p><strong>图1</strong>：(a-c) 视觉运动令牌化策略对比：密集令牌、物体中心令牌和本文的物体-关系中心令牌。(2) 折线图显示，在LIBERO-Goal任务上，SlotVLA（ORC）方法在使用极少令牌数（约20个）的情况下，达到了比基线更高的成功率，实现了效率与性能的更好平衡。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>引入了**LIBERO+**，一个具有细粒度物体中心标注（边界框、掩码、时序跟踪）的机器人操作基准数据集，旨在支持和评估物体-关系推理。</li>
<li>提出了<strong>SlotVLA</strong>，一个新颖的物体-关系中心VLA框架。它通过槽注意力提取任务相关的物体槽，并显式建模关系槽，以极少的令牌数形成紧凑、可解释的表征，从而高效驱动动作生成。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前方法在极度杂乱或包含大量动态物体的场景中的泛化能力仍有待探索。此外，关系编码目前依赖于可学习的查询，未来可以探索更显式的关系归纳偏置。</p>
<p><strong>启示</strong>：这项工作为机器人VLA模型指明了一条通向高效、可解释学习的新路径。它表明，结构化、基于物体的表征不仅能够降低计算开销，还能通过分离任务相关实体和关系来提升性能。后续研究可以沿着以下方向进行：将物体-关系槽扩展到更复杂的多物体交互和长时程任务规划中；探索在完全无监督或弱监督下学习此类表征；以及将此类模块化表征用于机器人技能组合与迁移。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人多任务操作中现有模型依赖密集嵌入、导致效率低和可解释性差的核心问题，提出了SlotVLA框架。该框架基于槽注意力技术，通过槽基视觉标记器保持物体表示一致性，关系中心解码器生成任务相关嵌入，以及LLM驱动模块转换为可执行动作。同时，引入了LIBERO+数据集以支持评估。实验表明，物体中心槽和物体关系槽表示能大幅减少所需视觉标记数量，同时保持竞争力的泛化性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.06754" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>