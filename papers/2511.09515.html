<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>WMPO: World Model-based Policy Optimization for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>WMPO: World Model-based Policy Optimization for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.09515" target="_blank" rel="noreferrer">2511.09515</a></span>
        <span>作者: Zhu, Fangqi, Yan, Zhengyang, Hong, Zicong, Shou, Quanxin, Ma, Xiao, Guo, Song</span>
        <span>日期: 2025/11/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为通用机器人操作的有力范式，其主流训练方法依赖于大规模人类演示的模仿学习。然而，模仿学习训练的策略在面对训练分布之外的状态时往往表现脆弱，缺乏从失败中学习和自我纠正的能力。强化学习通过与环境交互提供了自我改进的途径，但直接应用于真实机器人面临样本效率极低、成本高昂且难以实现在线策略优化的挑战。现有提升样本效率的策略，如依赖人工干预或使用仿真器，分别存在难以扩展或构建成本高的问题。</p>
<p>本文针对VLA模型策略优化的样本效率低下和难以实现在线强化学习这一具体痛点，提出了一个基于世界模型的新视角。核心思路是：通过在大规模机器人轨迹上预训练一个像素级的视频生成世界模型，并将其作为“想象”的训练场，使VLA策略能够完全在模型内部进行高效的在线策略优化，从而摆脱对真实环境交互的依赖。</p>
<h2 id="方法详解">方法详解</h2>
<p>WMPO的整体框架基于模型强化学习，其训练流程完全在一个学习得到的世界模型内部进行。如图2所示，该框架包含三个核心组件：1) <strong>想象轨迹生成</strong>：给定一个从真实环境采样的初始状态，旧策略与世界模型交替交互，生成完整的想象轨迹；2) <strong>轨迹采样</strong>：从同一初始状态采样多组想象轨迹，并由奖励模型评估其成功与否；3) <strong>策略更新</strong>：利用评估后的轨迹，通过GRPO算法更新策略参数。此过程迭代进行。</p>
<p><img src="https://arxiv.org/html/2511.09515v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：WMPO训练流程概览。从初始状态s0开始，包含三个主要步骤：(1) 想象轨迹生成：策略模型πθ_old与世界模型pφ交替生成完整轨迹；(2) 轨迹采样：采样多组轨迹并由奖励模型Rψ评估；(3) 策略更新：根据评估结果通过公式4优化策略参数θ。</p>
</blockquote>
<p><strong>核心模块1：生成式世界模型</strong><br>世界模型负责根据历史帧和动作块预测未来的像素级帧序列。其架构基于OpenSora的视频扩散模型，但进行了关键修改：用SDXL的2D VAE替换3D VAE，以更好地保留细粒度运动细节；在VAE的潜在空间进行扩散过程，但为了利用VLA的预训练知识，最终将图像解码回像素空间。为应对长视野自回归生成中的误差累积问题，WMPO引入了<strong>噪声帧条件</strong>技术，即在训练时对条件帧施加50/1000步的扩散噪声，增强了模型对不完美条件输入的鲁棒性。为实现精确的动作控制，模型采用了<strong>帧级动作控制</strong>机制，通过扩展AdaLN模块，将动作信号和扩散时间步嵌入在帧级别注入Transformer块，以调制特征。此外，为解决预训练数据（如OXE）与策略行为数据之间的分布不匹配，WMPO提出了<strong>策略行为对齐</strong>：在预训练后，使用策略自身收集的少量真实轨迹对世界模型进行微调，使其能更真实地模拟包括失败在内的各种策略行为。</p>
<p><strong>核心模块2：奖励模型</strong><br>为了在想象轨迹上提供自动化的成功判断信号，WMPO训练了一个轻量级奖励模型。该模型以VideoMAE为编码器，后接线性头。训练数据来自真实轨迹：成功轨迹的末端视频片段作为正样本，成功轨迹的中间片段和失败轨迹的任意片段作为负样本。推理时，模型以滑动窗口方式遍历想象轨迹，若任何片段的成功概率超过阈值，则判定该轨迹成功。</p>
<p><strong>核心模块3：在线强化学习</strong><br>WMPO采用分组相对策略优化作为策略优化算法。其优势在于完全在世界模型内部进行，避免了昂贵的真实环境交互，并天然支持从同一初始状态重复采样，这对GRPO的大规模训练至关重要。策略更新时，移除了KL散度正则化（无需参考模型），鼓励策略探索新行为。目标函数如公式4所示，通过比较同一组内轨迹的相对优势来更新策略。</p>
<p>与现有方法相比，WMPO的创新点具体体现在：1) <strong>像素级预测</strong>：与世界模型在潜在空间中操作的主流方法不同，WMPO坚持像素级生成，以匹配VLA模型在真实图像上预训练的特征空间；2) <strong>策略行为对齐</strong>：通过微调使世界模型适应策略自身的状态-动作分布，能更真实地模拟失败场景；3) <strong>完整的试验生成</strong>：通过片段级自回归视频生成完整的任务试验，便于分配基于结果的奖励，避免了短视野预测中的奖励黑客问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验主要在Mimicgen仿真环境中的四个精细操作任务（Coffee, StackThree, ThreePieceAssembly, Square）上进行，并在真实机器人上进行了验证。对比的基线方法包括在线RL方法GRPO和离线RL方法DPO。所有方法使用相同的真实轨迹采样预算P（P=128或1280）。</p>
<p><img src="https://arxiv.org/html/2511.09515v1/x5.png" alt="实验结果对比表"></p>
<blockquote>
<p><strong>图5/表1</strong>：在Mimicgen仿真基准上四种操作任务的策略优化方法对比。结果显示，在不同预算下，WMPO的成功率均 consistently 优于GRPO和DPO基线。当预算从128增至1280时，WMPO的平均成功率从47.1%提升至57.6%，展示了其数据效率和可扩展性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.09515v1/x3.png" alt="自我纠正行为"></p>
<blockquote>
<p><strong>图3</strong>：Square任务的行为分析。与基础策略在碰撞后持续推挤导致失败不同，WMPO策略展现了自我纠正能力：它学会抬起方块、重新对齐并成功插入。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.09515v1/x6.png" alt="轨迹长度分析"></p>
<blockquote>
<p><strong>图6</strong>：成功轨迹的长度分布。WMPO策略产生的成功轨迹显著更短，表明其避免了“卡住”行为，执行任务更快更流畅。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.09515v1/x4.png" alt="泛化测试场景"></p>
<blockquote>
<p><strong>图4</strong>：用于评估泛化能力的三种干扰场景：(a) 位置干扰（改变木棍位置）；(b) 背景干扰（更换桌面背景）；(c) 纹理干扰（更换底座纹理）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.09515v1/x7.png" alt="泛化结果表"></p>
<blockquote>
<p><strong>图7/表2</strong>：在三种干扰场景下的成功率。WMPO在所有干扰类型上均取得最佳性能（平均29.6%），展示了更强的泛化能力。</p>
</blockquote>
<p>关键实验结果总结如下：</p>
<ol>
<li><strong>性能对比</strong>：在P=128的小预算下，WMPO平均成功率（47.1%）已超过最强基线DPO（37.3%）9.8个百分点。预算增至1280时，优势扩大至15.2个百分点（WMPO: 57.6%， DPO: 42.4%）。</li>
<li><strong>涌现行为</strong>：如图3所示，WMPO策略学会了基础策略中未出现的自我纠正行为。如图6所示，其成功轨迹更短，执行更高效。</li>
<li><strong>泛化能力</strong>：在位置、背景、纹理三种干扰下（图4），WMPO均表现最佳（表2），平均成功率达29.6%，优于基础策略（23.7%）和基线方法。</li>
<li><strong>奖励模型有效性</strong>：奖励模型在所有任务上的F1分数超过0.95，能可靠区分成功与失败。</li>
<li><strong>真实机器人验证</strong>：论文在真实机器人上进行了抓取和重新定位任务的初步实验（图8-11），表明WMPO能够迁移到现实世界。</li>
</ol>
<p>消融实验方面，论文虽未以独立章节呈现，但在方法分析中隐含了关键组件的贡献：<strong>策略行为对齐</strong>确保了世界模型能模拟策略的真实行为分布；<strong>噪声帧条件与帧级动作控制</strong>是实现稳定长视野生成的关键；<strong>在世界模型内进行在线GRPO</strong>是取得高性能和高效数据利用的核心。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了WMPO框架</strong>：首次验证了利用高保真、像素级视频世界模型进行可扩展VLA在线强化学习的可行性，完全在“想象”中训练策略，大幅提升样本效率。</li>
<li><strong>引入了策略行为对齐等关键技术</strong>：通过微调世界模型适应策略数据分布，并采用噪声帧条件、帧级动作控制，解决了长视野模拟中的分布偏移和质量退化问题。</li>
<li><strong>实现了高效在线优化与涌现能力</strong>：在世界模型支持下实现了高效的在线GRPO，使策略获得了模仿学习不具备的自我纠正、更流畅执行以及更强的泛化能力。</li>
</ol>
<p>论文自身提到的局限性包括：当前框架假设状态完全可观测（仅依赖图像），未来可扩展到部分可观测MDPs；世界模型在极端未见状态下的保真度仍有提升空间。</p>
<p>本研究对后续工作的启示在于：为VLA模型的持续自我改进提供了一条不依赖昂贵真实交互的可行路径。未来方向可能包括：探索更高效的世界模型与VLA策略的协同优化架构，研究如何将世界模型学习与策略学习更紧密地结合以实现终身学习，以及进一步推动该范式在复杂真实场景中的应用和仿真到现实的迁移。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型依赖专家演示、缺乏自我纠正能力，以及强化学习在真实机器人上样本效率低的问题，提出了世界模型策略优化框架。其核心是采用基于像素预测的世界模型，使“想象”轨迹能与网络规模图像预训练的VLA特征对齐，从而支持高效的在线策略优化。实验表明，该方法显著提升了样本效率与整体性能，并涌现出自我纠正等行为，具备良好的泛化与持续学习能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.09515" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>