<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.10442" target="_blank" rel="noreferrer">2505.10442</a></span>
        <span>作者: Gao, Dechen, Wang, Hang, Zhou, Hanchu, Ammar, Nejib, Mishra, Shatadal, Moradipari, Ahmadreza, Soltani, Iman, Zhang, Junshan</span>
        <span>日期: 2025/05/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人策略学习领域的主流方法包括模仿学习（IL）和强化学习（RL）。IL通过行为克隆模仿专家演示，学习稳定但泛化能力有限，且容易因分布偏移导致复合误差。RL通过环境交互最大化累积奖励，具备探索和泛化潜力，但普遍存在不稳定、样本效率低下以及对超参数敏感的问题，在稀疏奖励和长时域任务中尤为严重。为结合两者优势，当前主流范式是采用“IL预训练 + RL微调”的两步法，然而，RL微调阶段仍然面临性能崩溃、不稳定和样本效率低下的关键挑战。现有改进方法，如在回放缓冲区中加入专家数据（需奖励标注和复杂采样策略）或添加正则化项（需精细调参），均存在局限性。</p>
<p>本文针对RL微调阶段不稳定和低效的具体痛点，提出了一种新的视角：将专家数据的利用贯穿整个微调过程，而非仅限于预训练阶段。核心思路是提出IN-RIL（交错强化与模仿学习）方法，在微调过程中周期性地在多次RL更新后注入IL更新，从而在整个学习过程中协同IL的稳定性和RL的探索能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>IN-RIL的整体流程遵循“预训练-微调”范式，但其创新在于微调阶段的更新机制。首先，使用行为克隆在专家数据集 $\mathcal{D}<em>{\text{exp}}$ 上预训练策略 $\pi_\theta$，最小化负对数似然损失 $\mathcal{L}</em>{\mathrm{IL}}(\theta)$，得到一个初始化策略 $\pi_0$。随后进入微调阶段，IN-RIL不是单纯进行RL更新，而是系统性地交错进行IL和RL更新。</p>
<p>具体而言，在第 $t$ 次迭代中，先执行一次IL梯度更新：$\theta_{t+\frac{1}{1+m(t)}} = \theta_t - \alpha_{\mathrm{IL}} \nabla_\theta \mathcal{L}<em>{\mathrm{IL}}(\theta_t)$。紧接着，连续执行 $m(t)$ 次RL梯度更新：对于 $j \in {1, \ldots, m(t)}$，有 $\theta</em>{t+\frac{1+j}{1+m(t)}} = \theta_{t+\frac{j}{1+m(t)}} - \alpha_{\mathrm{RL}} \nabla_\theta \mathcal{L}<em>{\mathrm{RL}}(\theta</em>{t+\frac{j}{1+m(t)}})$。其中 $m(t)$ 是RL更新与IL更新的比率，$\mathcal{L}_{\mathrm{RL}}(\theta)$ 是最大化期望Q值的损失函数。这种机制使得IL更新能够定期将策略拉回专家演示分布附近，提供稳定性并引导探索，而RL更新则致力于提升任务表现。</p>
<p><img src="https://arxiv.org/html/2505.10442v1/x2.png" alt="IN-RIL框架示意图"></p>
<blockquote>
<p><strong>图2</strong>：IN-RIL方法示意图。策略网络同时接受IL和RL目标的更新。微调过程周期性地交错进行：先执行一次IL更新，然后执行多次（图中为m次）RL更新。</p>
</blockquote>
<p>由于IL和RL目标可能不同甚至冲突，直接交错更新可能导致梯度相互干扰。为此，本文提出了两种梯度分离机制作为核心创新点：</p>
<ol>
<li><strong>梯度手术（Gradient Surgery）</strong>：基于多任务学习中的思想，通过将RL和IL的梯度投影到彼此正交的子空间来减轻冲突。具体计算梯度冲突并修正更新方向。</li>
<li><strong>网络分离（Network Separation）</strong>：引入一个残差策略网络。基础策略网络仅通过IL梯度更新以保持专家行为模式，而新增的残差策略网络则通过RL梯度更新，学习对基础策略的改进。最终动作为两者之和。这种方法物理上隔离了两种更新流。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.10442v1/x3.png" alt="IL与RL优化景观"></p>
<blockquote>
<p><strong>图3</strong>：IN-RIL的优化景观示意图。曲面高度和等高线代表IL损失，颜色梯度代表RL奖励（蓝低白高）。IL和RL都是具有多个局部最优的非凸优化。单独使用任一方法（IL only/RL only轨迹）易陷入次优解。IN-RIL通过交错更新，使IL帮助RL跳出低奖励区域，RL帮助IL在损失景观的不同局部极小值间移动。</p>
</blockquote>
<p>本文还提供了理论分析，为交错比率 $m(t)$ 的选择提供指导。分析表明，最优比率 $m_{\text{opt}}(t) \geq 1$，且与IL和RL梯度之间的余弦相似度 $\rho(t)$ 有关。当梯度相互对立时（$\rho(t)&gt;0$），需要更多的RL更新来取得进展；当梯度对齐时（$\rho(t)&lt;0$），可以减少RL更新次数。这为动态调整交错比率提供了理论依据。</p>
<p>与现有方法相比，IN-RIL的创新点在于：1) 将IL更新作为周期性正则化器融入整个RL微调过程，而非一次性预训练；2) 无需为专家数据标注奖励，也无需设计复杂的离线/在线数据采样策略；3) 通过梯度分离机制有效管理目标冲突，避免性能损害；4) 算法无关，可作为插件与多种先进的on-policy和off-policy RL算法结合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个机器人学习基准的14个任务上展开：<strong>FurnitureBench</strong>（家具组装）、<strong>OpenAI Gym</strong>（MuJoCo运动控制）和<strong>Robomimic</strong>（稀疏奖励机械臂操作）。实验平台涉及模拟环境。</p>
<p>对比的基线方法包括：纯<strong>IL</strong>（行为克隆）、纯<strong>RL</strong>微调（在IL预训练模型上）、以及多种结合专家数据的RL方法，如<strong>AWAC</strong>、<strong>Hy-Q</strong>、<strong>IBRL</strong>等。IN-RIL与多种RL骨干算法结合测试，包括<strong>DPPO</strong>、<strong>IDQL</strong>和<strong>Residual PPO</strong>。</p>
<p>关键实验结果如下：</p>
<ul>
<li><strong>Robomimic Transport任务</strong>：这是一个具有挑战性的多阶段、稀疏奖励任务。IN-RIL（与IDQL结合）将最终成功率从纯RL微调的约<strong>12%</strong> 大幅提升至<strong>88%<strong>，提升幅度达</strong>6.3倍</strong>。而纯IL方法成功率饱和在较低水平（约20%）。</li>
<li><strong>FurnitureBench Lamp任务</strong>：IN-RIL同样表现出色，成功率从RL微调的约10%提升至超过80%。</li>
<li><strong>样本效率</strong>：在多个任务中，IN-RIL达到相同性能水平所需的交互步数（环境样本）远少于RL微调基线。</li>
<li><strong>稳定性</strong>：IN-RIL的训练曲线更加平滑，成功避免了RL微调中常见的性能崩溃现象。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.10442v1/x1.png" alt="不同任务上IN-RIL与基线方法性能对比"></p>
<blockquote>
<p><strong>图1</strong>：在Transport、Round-Table和Lamp等具有挑战性的多阶段稀疏奖励任务上，IN-RIL微调与RL微调的对比。IN-RIL在所有任务上均显著优于RL微调。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.10442v1/x4.png" alt="Robomimic Transport任务学习曲线"></p>
<blockquote>
<p><strong>图4</strong>：在Robomimic Transport任务上的学习曲线。IN-RIL（橙色）相比RL微调（蓝色）和纯IL（绿色），实现了更高、更稳定的成功率，且样本效率显著提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.10442v1/x5.png" alt="消融实验：交错比率的影响"></p>
<blockquote>
<p><strong>图5</strong>：消融实验展示不同RL:IL更新比率（m）对IN-RIL性能的影响。比率适中（如m=10）时效果最佳，比率为0（即纯RL）或过大（如m=50）时性能下降，验证了交错更新的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.10442v1/x6.png" alt="消融实验：梯度分离机制的影响"></p>
<blockquote>
<p><strong>图6</strong>：消融实验对比使用梯度手术（IN-RIL-GS）、网络分离（IN-RIL-NS）以及无分离机制（IN-RIL w/o Sep.）的效果。两种分离机制均能有效提升性能，而无分离机制的性能与纯RL微调相近，证明了梯度分离的关键作用。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.10442v1/x7.png" alt="在不同RL骨干算法上的表现"></p>
<blockquote>
<p><strong>图7</strong>：IN-RIL作为插件与不同RL骨干算法（DPPO, IDQL, Residual PPO）结合的性能。在所有算法上，IN-RIL均能带来显著提升，证明了其算法无关的通用性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.10442v1/x8.png" alt="FurnitureBench任务结果汇总"></p>
<blockquote>
<p><strong>图8</strong>：在FurnitureBench多个家具组装任务上的最终成功率。IN-RIL在大多数任务上优于或与其他结合专家数据的方法相当。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.10442v1/x9.png" alt="OpenAI Gym运动任务结果"></p>
<blockquote>
<p><strong>图9</strong>：在OpenAI Gym运动控制任务上的平均回报。IN-RIL在密集奖励的短时域任务中也表现出稳定的改进。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.10442v1/x10.png" alt="理论分析与实验验证"></p>
<blockquote>
<p><strong>图10</strong>：梯度对齐度 $\rho(t)$ 在训练过程中的变化。实验观察到 $\rho(t)$ 经常为正值（梯度对立），这从实证角度支持了理论分析中需要 $m(t) \geq 1$ 的结论。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了IN-RIL方法</strong>：一种在策略微调过程中交错进行RL和IL更新的新范式，通过周期性注入IL更新来持续利用专家数据的稳定性和引导作用，有效解决了RL微调的不稳定和低样本效率问题。</li>
<li><strong>开发了梯度分离机制</strong>：针对IL与RL目标可能冲突的问题，提出了梯度手术和网络分离两种技术，将可能冲突的梯度更新隔离在正交子空间中，确保了交错更新的有效性。</li>
<li><strong>提供了理论分析并进行了全面验证</strong>：从优化角度分析了IN-RIL有效的原理，推导了交错比率的指导原则，并通过在3个基准、14个任务、与多种SOTA RL算法结合的广泛实验，证明了方法的通用性和显著性能提升（如在Robomimic Transport任务上取得6.3倍成功率提升）。</li>
</ol>
<p>论文提到的局限性包括：方法仍然依赖于一定数量的专家演示数据；理论分析中的梯度对齐假设在实践中可能需要监测；网络分离方法会增加少量参数。</p>
<p>本文对后续研究的启示在于：1）微调阶段持续利用监督信号（如演示）是一种提升RL稳定性和效率的有效途径；2）管理多目标优化中的梯度冲突是关键挑战，本文的分离机制提供了可行思路；3）IN-RIL的插件式设计使其易于集成到现有RL框架中，为机器人及其他领域的策略学习提供了实用的改进工具。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人策略学习中“模仿学习预训练+强化学习微调”两阶段范式在微调时不稳定、样本效率低的问题，提出IN-RIL方法。其核心是在强化学习微调过程中，定期交错进行模仿学习更新，并引入梯度分离机制，将两种学习可能冲突的梯度置于正交子空间以避免干扰。该方法在14个机器人任务上验证有效，例如在Robomimic Transport任务上将成功率从12%显著提升至88%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.10442" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>