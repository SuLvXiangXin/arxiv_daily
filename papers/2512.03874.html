<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.03874" target="_blank" rel="noreferrer">2512.03874</a></span>
        <span>作者: Zhang, Lei, Zheng, Diwen, Bai, Kaixin, Bing, Zhenshan, Marton, Zoltan-Csaba, Chen, Zhaopeng, Knoll, Alois Christian, Zhang, Jianwei</span>
        <span>日期: 2025/12/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于生成模型或强化学习的灵巧抓取生成方法，主要致力于从物体视觉观察中合成稳定的抓取位姿，以提高对不同物体和环境的抓取鲁棒性与泛化能力。然而，在真实场景中，灵巧手需要执行具有多维语义意图的抓取操作，例如“使用拇指、食指和中指以三指抓握方式抓握水壶的把手”。现有语义引导的抓取生成方法主要聚焦于功能抓取，缺乏对抓取分类学（基于手指、接触点配置、对抗类型和力方向的结构化分类）和接触语义的显式敏感性。这导致当前模型难以利用手指弯曲、接触配置等细粒度语义线索，生成的抓取动作可控性和语义多样性有限。此外，现有灵巧抓取数据集大多通过基于优化的方法构建，以满足微分力闭合等质量指标，但抓取类型分布稀疏（如严重偏向捏取抓握），缺乏对结构化抓取语义的建模和泛化能力。尽管有研究引入了手动标注的抓取类型标签，但如何自动生成多样化且语义有意义的抓取类型仍是一个开放挑战。</p>
<p>本文针对上述语义建模不足和数据多样性有限的痛点，提出了一个集成功能可供性、接触模式和手指配置语义的新视角。核心思路是构建一个统一框架，通过数据生成、语义理解和抓取生成三个组件，实现语义敏感的灵巧抓取生成。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的OmniDexVLG框架包含三个核心组件：用于生成具有丰富语义的抓取数据集的OmniDexDataGen、基于大语言模型进行多维语义理解的OmniDexReasoner，以及基于3D视觉语言模型进行语义引导抓取生成的OmniDexGraspNet。</p>
<p><img src="https://arxiv.org/html/2512.03874v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：OmniDexVLG框架总览。框架集成了三个核心组件：OmniDex-DataGen（功能与抓取分类学感知的灵巧抓取数据集生成）、OmniDexReasoner（使用大语言模型进行多维语义理解）、以及OmniDexGraspNet（一个由跨抓取类型、可供性、接触和手指配置的语义指令引导的3D视觉语言抓取生成模型）。</p>
</blockquote>
<p><strong>1. OmniDexDataGen：功能可供性与接触敏感的抓取数据集生成</strong><br>该组件旨在生成具有抓取分类学多样性、接触语义和功能可供性敏感的抓取数据。其流程如图2所示，包含以下几个关键步骤：</p>
<p><img src="https://arxiv.org/html/2512.03874v1/x2.png" alt="数据生成流程"></p>
<blockquote>
<p><strong>图2</strong>：OmniDexDataGen框架。该抓取合成框架引入了接触级表示。每个抓取由一个多层次配置描述，包括活动手指、涉及的连杆和相应的接触区域。基于抓取对抗类型，抓取分类学感知的微分力闭合采样器（Tax-DFCSampler）计算初始抓取位姿，功能可供性接触点采样器（AC-Sampler）在物体特定可供性区域进行接触采样。样本使用微分力空间度量进行定量评估。通过碰撞过滤的高质量抓取进入手部位姿优化阶段，并通过物理仿真验证以确保其鲁棒性和适用性。</p>
</blockquote>
<ul>
<li><strong>抓取分类学感知的配置采样</strong>：根据抓取分类学，为指定抓取类型 <code>t_g</code> 生成结构化的抓取配置 <code>G = {S_f, S_l, S_c, v, c}</code>，其中包含活动手指集 <code>S_f</code>、涉及的连杆 <code>S_l</code>、接触区域 <code>S_c</code>、力方向 <code>v</code> 和抓取中心点 <code>c</code>。</li>
<li><strong>功能可供性与抓取类型感知的微分力闭合抓取采样</strong>：在给定抓取配置 <code>G</code> 和带有可供性标注的物体网格 <code>M_obj</code> 后，首先通过AC-Sampler从物体表面的可供性区域采样两个主要接触点。然后使用微分力闭合（DFC）估计器 <code>f_DFC</code> 验证这些接触点是否满足力闭合约束。接着，以接触点中点为抓取中心，根据抓取类型初始化手部位姿（例如，对于侧边对抗抓握，会调整拇指初始位姿），并将所有活动手指的指尖接触点沿力方向投影到物体表面以获得完整候选接触点集。非活动手指会被随机扰动以增加多样性。最后进行并行碰撞检测，仅保留碰撞次数低于阈值的候选作为初始抓取位姿 <code>G_init</code>。</li>
<li><strong>抓取优化与仿真验证</strong>：对初始抓取位姿进行优化，优化目标包括：1) <strong>语义手-物体交互损失</strong>：鼓励手部网格与物体表面在指定接触区域发生语义有效的接触，同时最小化穿透；2) <strong>微分力闭合估计损失</strong>：促进高质量的力闭合抓取；3) <strong>穿透和关节限位正则化</strong>。优化后，在物理仿真环境中使用关节阻抗控制策略验证抓取的稳定性，只有成功完成仿真的抓取才会被保留到最终数据集中。</li>
</ul>
<p><strong>2. OmniDexReasoner：基于LMM的多维语义理解</strong><br>该组件旨在为生成的抓取数据提供丰富的多维语义标注，其框架如图3所示。</p>
<p><img src="https://arxiv.org/html/2512.03874v1/x3.png" alt="语义理解框架"></p>
<blockquote>
<p><strong>图3</strong>：OmniDexReasoner框架。这是一个基于LMM的灵巧抓取多维语义推理框架，涵盖功能可供性、接触语义信息、手指配置、抓取分类学和人类一致性。</p>
</blockquote>
<p>该框架包含三个关键模块，并利用多智能体协作、思维链推理和检索增强生成策略：</p>
<ul>
<li><strong>手-物体交互理解模型</strong>：利用物体属性、机械手配置、手-物体接触数据和抓取场景图像等多模态信息，自动预测手与物体之间的语义关系。输出包括最终的功能可供性估计分类结果 <code>a_sem</code>、文本物体描述、接触手指和连杆、手指弯曲配置、手掌接触状态以及**接触语义图 <code>M_c</code>**。<code>M_c</code> 通过计算手部各连杆与物体表面点的空间距离来构建，为物体表面点分配语义接触标签（如涉及哪根手指的哪个连杆）。</li>
<li><strong>抓取分类学推理模型</strong>：基于交互理解模型的输出，进一步推理抓取类型。该模型结合了视觉-语言基础模型的视觉推理能力和大型语言模型的逻辑推理能力。</li>
<li><strong>灵巧抓取多维语义生成器</strong>：整合前两个模块的输出，生成全面、连贯的多维语义描述，包括功能可供性、接触语义和抓取分类学。</li>
</ul>
<p><strong>3. OmniDexGraspNet：语义感知的3D视觉语言抓取位姿生成模型</strong><br>该组件是一个条件生成模型，以部分物体点云和自然语言指令（描述功能、抓取类型、接触配置等）为输入，输出灵巧手抓取位姿。其架构如图4所示。</p>
<p><img src="https://arxiv.org/html/2512.03874v1/x4.png" alt="抓取生成模型"></p>
<blockquote>
<p><strong>图4</strong>：OmniDexGraspNet架构。模型以物体点云和文本指令作为输入。点云通过一个3D视觉编码器处理，文本指令通过一个语言编码器处理。编码后的特征通过交叉注意力模块融合，然后输入到一个抓取解码器中，以生成手部关节角度和手腕位姿。</p>
</blockquote>
<ul>
<li><strong>编码器</strong>：使用3D视觉编码器（如PointNet++）处理物体点云，使用预训练语言模型（如CLIP文本编码器）处理文本指令。</li>
<li><strong>跨模态融合</strong>：通过交叉注意力机制融合视觉和语言特征，确保生成过程对语义指令敏感。</li>
<li><strong>解码器</strong>：使用条件变分自编码器或扩散模型作为抓取解码器，从融合特征中解码生成手部关节角度和手腕位姿参数。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，本文的创新性体现在：1) <strong>统一的多维语义建模</strong>：首次在数据生成、语义理解和抓取生成全流程中，系统性地统一建模功能可供性、接触语义和抓取分类学；2) <strong>自动化、多样化的语义数据生成</strong>：提出的OmniDexDataGen能够自动生成具有丰富语义和结构多样性的抓取数据，而非依赖手动标注或简单启发式规则；3) <strong>LMM驱动的细粒度语义推理</strong>：利用LMM进行细粒度的抓取语义理解与标注，超越了现有方法中常见的粗粒度部分级或功能级语义。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在DexGraspNet数据集和作者使用OmniDexDataGen生成的自建数据集上进行。评估平台包括物理仿真环境（用于验证抓取成功率）和真实机器人平台（进行实物验证）。评估指标包括抓取成功率、接触语义对齐准确率、抓取类型分类准确率以及抓取多样性指标。</p>
<p><strong>对比方法</strong>：对比的基线方法包括：DexGraspNet、ContactDexNet、Dexonomy、GraspGen和AnyDexGrasp。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>抓取成功率与语义对齐</strong>：在DexGraspNet测试集上，OmniDexVLG达到了<strong>93.7%</strong> 的抓取成功率，优于DexGraspNet (91.2%) 和 ContactDexNet (92.1%)。在接触语义对齐方面，OmniDexVLG的准确率达到**89.5%**，显著高于其他基线（例如ContactDexNet为82.3%）。</li>
<li><strong>抓取类型多样性</strong>：在自建数据集上，OmniDexVLG生成的抓取覆盖了<strong>33种</strong>不同的抓取类型，而DexGraspNet仅覆盖了<strong>9种</strong>，且后者中捏取抓握占比过高（&gt;70%）。OmniDexVLG显著提高了抓取类型的分布均匀性和多样性。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.03874v1/x5.png" alt="定性对比"></p>
<blockquote>
<p><strong>图5</strong>：不同方法的抓取生成定性对比。OmniDexVLG生成的抓取在语义上更符合指令（如“抓握锤子的手柄”），且抓取类型（如强力抓握、三指抓握）更加多样和合理。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.03874v1/x6.png" alt="定量对比"></p>
<blockquote>
<p><strong>图6</strong>：在DexGraspNet测试集上的抓取成功率定量对比。OmniDexVLG取得了最高的平均成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.03874v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验。分别移除语义理解模块（OmniDexReasoner）、语义引导的数据生成（OmniDexDataGen）和接触语义建模，模型性能（成功率和语义对齐准确率）均出现下降，验证了各核心组件的必要性。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：消融研究表明，OmniDexReasoner（语义理解）对提升语义对齐准确率的贡献最大（约8%的绝对提升）；OmniDexDataGen（语义数据）对提升抓取成功率和多样性的贡献显著；而接触语义建模是连接功能可供性和抓取分类学的关键，移除后模型对细粒度指令的响应能力下降。</li>
<li><strong>真实机器人实验</strong>：在实物抓取任务中，OmniDexVLG对未见过的物体和复杂语义指令（如“用侧捏方式拿起遥控器”）表现出良好的泛化能力，任务成功率超过85%。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.03874v1/x8.png" alt="多样性分析"></p>
<blockquote>
<p><strong>图8</strong>：抓取类型多样性分布对比。OmniDexVLG生成的数据集（右）相比DexGraspNet（左），抓取类型分布更广泛、更均匀。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>OmniDexDataGen</strong>，一种能够自动生成具有功能可供性敏感、接触语义敏感和抓取分类学多样性的灵巧抓取数据集的方法，解决了现有数据语义稀疏和类型单一的问题。</li>
<li>提出了<strong>OmniDexReasoner</strong>，一个基于大语言模型的多维语义理解框架，能够对灵巧抓取进行细粒度的功能、接触和分类学联合推理与标注。</li>
<li>提出了<strong>OmniDexGraspNet</strong>，一个3D视觉语言抓取生成模型，能够根据多维度语义指令生成语义对齐且物理合理的灵巧抓取位姿，显著提升了生成的语义敏感性和多样性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法对高度复杂或非刚性物体的泛化能力仍有待进一步验证。此外，语义推理模块（OmniDexReasoner）的性能依赖于高质量的多模态数据集和LMM的能力，可能存在对训练数据偏差敏感的问题。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>更细粒度的语义控制</strong>：未来工作可以探索如何将力/力矩约束、动态操作意图等更物理的语义维度集成到生成框架中。</li>
<li><strong>扩展到动态操作任务</strong>：当前工作聚焦于静态抓取生成，可进一步扩展到包含物体使用、工具操作等动态任务序列的语义规划。</li>
<li><strong>数据与训练效率</strong>：如何更高效地生成大规模语义数据，以及如何设计更高效的网络架构来降低训练和推理成本，是值得探索的方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有灵巧抓取生成方法缺乏对抓取分类学、接触语义等细粒度控制，导致语义多样性有限的问题，提出OmniDexVLG框架。该框架集成三个核心组件：OmniDex-DataGen用于生成功能与分类学感知的数据集；OmniDexReasoner利用大型多模态模型进行抓取类型、可供性等多维度语义理解；OmniDexGraspNet作为3D视觉语言模型，在语义指令引导下生成抓取。通过视觉语言模型整合结构化语义，旨在提升抓取动作的可控性和适应性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.03874" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>