<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.09617" target="_blank" rel="noreferrer">2602.09617</a></span>
        <span>作者: Feng, Ruoxuan, Zhou, Yuxuan, Mei, Siyu, Zhou, Dongzhan, Wang, Pengwei, Cui, Shaowei, Fang, Bin, Yao, Guocai, Hu, Di</span>
        <span>日期: 2026/02/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，光学触觉传感器虽能提供丰富的接触信息，但现有的触觉数据集和模型大多局限于静态物体级属性（如材质、硬度）的识别，主要依赖“仅按压”或“随机滑动”等简单动作收集数据。这些数据缺乏丰富的动态交互信息，导致模型难以捕捉细微的时空形变和底层的力相关物理原理，从而限制了其在复杂接触式操作任务中的应用。本文针对动态触觉感知数据与模型的双重匮乏，提出了一个系统性的视角：将触觉感知能力组织成一个分层的“触觉动态金字塔”。基于此，本文的核心思路是：首先构建一个大规模、分层的动态触觉数据集ToucHD来填补高层级数据的空白，并在此基础上提出一个统一的表征学习框架AnyTouch 2，以同时学习物体级语义和细粒度的、力感知的动态感知能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>AnyTouch 2是一个通用的光学触觉表征学习框架，其目标是从模型角度学习多层级的动态感知能力。整体框架统一了像素级动态细节、语义级触觉特征和物理级动态属性的学习。</p>
<p><img src="https://raw.githubusercontent.com/gewu-lab/AnyTouch2/main/assets/framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：AnyTouch 2框架概览。模型以掩码后的触觉视频为输入，通过一个共享的编码器，并利用多个解码器和学习目标来共同优化。这些目标包括：原始视频与帧差重建（像素级）、多模态对齐与跨传感器/动作匹配（语义级），以及接触力与力增量预测（物理级），从而学习到一个支持广泛下游任务的通用触觉表征。</p>
</blockquote>
<p>框架的核心模块与技术细节如下：</p>
<ol>
<li><p><strong>像素级动态细节学习</strong>：作为动态感知的基础，该模块旨在增强模型对细微时空形变的敏感性。输入为背景归一化后的触觉视频序列。采用视频掩码自编码器进行训练，其创新点在于引入了<strong>帧差重建</strong>任务。具体而言，模型不仅需要重建被掩码的原始视频帧（损失L_ori_rec），还需要重建每个后续帧与第一帧的差值（损失L_dif_rec）。这种双重重建策略迫使模型关注帧间的细微变化，而不仅仅是全局模式。</p>
</li>
<li><p><strong>语义级触觉特征学习</strong>：该模块旨在学习可泛化于不同物体、传感器和动作的语义特征。包含三个子目标：</p>
<ul>
<li><strong>多模态对齐</strong>：遵循CLIP范式，将触觉特征与配对的视觉和语言特征在共享语义空间中对齐（损失L_Align）。</li>
<li><strong>跨传感器匹配</strong>：使来自不同传感器但接触同一物体的触觉信号在特征空间中接近，以学习传感器不变的物体级特征（损失L_obj）。</li>
<li><strong>动作匹配</strong>：本文引入的创新点。利用ToucHD数据集中标注的原子动作（如特定方向的滑动、旋转），引导模型将相同动作的触觉视频序列在特征空间中聚类，同时分离不同动作的序列（损失L_act）。这显式地将结构化动态交互的语义信息编码到表征中。</li>
</ul>
</li>
<li><p><strong>物理级动态属性学习</strong>：为了对驱动交互的底层物理属性进行建模，本文引入了<strong>力预测</strong>任务。利用ToucHD(Force)中的大规模触觉-力配对数据，训练模型从触觉视频中预测3D接触力序列（F）。为进一步增强对动态形变的敏感性，同时预测<strong>力增量</strong>（ΔF，即相邻帧间的力变化）。两者均使用L1损失（L_Force）进行监督。这为表征提供了明确的物理基础，使其能够关联形变与力的动态变化。</p>
</li>
</ol>
<p><strong>训练策略</strong>：采用课程任务调度，逐步引入不同层级的任务并增加其权重，以确保模型先学习稳健的低层模式，再学习更复杂的高层能力。总损失是像素级、语义级和物理级损失的加权和。</p>
<p>与现有方法相比，AnyTouch 2的创新点在于系统性地集成了与“触觉动态金字塔”层级对应的多级动态增强模块（特别是帧差重建、动作匹配和力/力增量预测），从而能够更全面地捕捉从像素级形变到语义级动作，再到物理力学的多层次动态信息。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>预训练数据</strong>：整合了9个触觉数据集，包括新提出的ToucHD。</li>
<li><strong>评估基准与数据集</strong>：<ul>
<li><strong>静态物体属性理解</strong>：使用TAG（材质分类）和Cloth（布料状态分类）数据集。</li>
<li><strong>动态物理属性感知</strong>：使用Sparsh（位姿/滑动分类、Δ力回归）数据集和新构建的ToucHD Bench（在10个未见过的压头数据上评估力回归）。</li>
</ul>
</li>
<li><strong>对比基线</strong>：包括单帧模型（UniTouch, T3）和多帧模型（MAE (Sparsh), VJEPA (Sparsh), AnyTouch 1）。为公平对比，还训练了使用相同训练数据（含ToucHD）的MAE (Sparsh)†。</li>
<li><strong>涉及传感器</strong>：GelSight (GS), DIGIT (DG), GelSight Mini (Mini)。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>表1总结了在物体级和动态物理级基准测试上的结果。</p>
<p><img src="https://raw.githubusercontent.com/gewu-lab/AnyTouch2/main/assets/table1.png" alt="结果表格"></p>
<blockquote>
<p><strong>表1</strong>：在ObjectBench、SparshBench和ToucHD Bench上的评估结果。AnyTouch 2在大多数任务上取得了最佳或极具竞争力的性能。特别是在动态物理感知任务（Sparsh Bench的Δ力F1/RMSE、ToucHD Bench的力RMSE）上，AnyTouch 2显著优于所有基线模型，证明了其动态感知能力的优越性。例如，在DIGIT传感器的Δ力预测上，AnyTouch 2的RMSE（80.83）远低于MAE (Sparsh)†（98.85）和AnyTouch 1（162.41）。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文通过逐步添加各模块来验证其贡献。</p>
<p><img src="https://raw.githubusercontent.com/gewu-lab/AnyTouch2/main/assets/ablation.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：在TAG（静态）和Sparsh Δ力预测（动态）任务上的消融实验。基线（仅MAE）性能有限。依次加入帧差重建（+Diff）、动作匹配（+Act）、跨传感器匹配（+Obj）和力预测（+Force）后，模型在静态和动态任务上的性能均持续提升。这证明了每个动态增强模块的有效性，以及它们共同作用带来的协同效益。</p>
</blockquote>
<p>消融实验表明：1) <strong>帧差重建</strong>是提升动态感知的基础；2) <strong>动作匹配</strong>对理解结构化动态交互至关重要；3) <strong>力预测</strong>任务极大地增强了模型对物理属性的建模能力。所有组件共同作用，使得AnyTouch 2在静态和动态任务上均实现了最佳性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了“触觉动态金字塔”的系统性视角，用以指导动态触觉感知的数据收集与模型设计。</li>
<li>构建了大规模分层触觉数据集ToucHD，极大地丰富了高层级（特定动作、操作、力配对）动态触觉数据，形成了一个完整的数据生态系统。</li>
<li>提出了通用触觉表征学习框架AnyTouch 2，通过集成像素级重建、语义级匹配和物理级力预测等多级动态增强模块，统一了物体级理解与细粒度动态感知，在静态和动态任务上均取得了卓越性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，更高层级的动态数据（如操作和力数据）收集更具挑战性且更稀缺。此外，模拟数据与真实数据之间仍存在差距。</p>
<p><strong>启示</strong>：这项工作为动态触觉感知研究奠定了坚实的数据和模型基础。其分层框架（金字塔）和统一学习范式（多级目标）为后续研究提供了清晰的路线图，例如，可以进一步探索更复杂的动态交互建模、更好的模拟到真实迁移方法，以及将学习到的触觉表征应用于更广泛的灵巧操作任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人动态触觉感知中缺乏细粒度时序动态数据与统一模型的问题，提出了大规模分层触觉数据集ToucHD与通用触觉表示学习框架AnyTouch 2。该框架能同时捕捉跨帧的像素级形变、动作特定形变并显式建模物理力动力学，从而学习多层次的动态感知能力。实验表明，该模型在涵盖静态属性与动态物理属性的多种基准测试及真实操作任务中均表现出色，验证了其作为通用动态触觉感知模型的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.09617" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>