<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16449" target="_blank" rel="noreferrer">2511.16449</a></span>
        <span>作者: Bo Zhao Team</span>
        <span>日期: 2025-11-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作（VLA）模型在具身智能领域展现出潜力，但其处理连续视觉流时计算开销巨大。视觉token剪枝（保留显著token，丢弃冗余token）已成为加速视觉-语言模型（VLM）的主流技术，为解决VLA模型效率问题提供了自然思路。然而，现有的VLM专用token剪枝方法仅依赖语义显著性指标（例如预填充注意力）来选择token，忽略了VLA模型兼具高层语义理解和低层动作执行的内在双系统特性。这导致剪枝偏向于保留语义线索，丢弃了对动作生成至关重要的信息，显著降低了VLA性能。为弥补这一差距，本文提出了VLA-Pruner，一个即插即用的VLA token剪枝方法，它契合VLA模型的双系统特性并利用机器人操作中的时间连续性。其核心思路是：引入一个兼顾语义相关性和执行精确性的双目标重要性准则，并设计一个双层token选择策略，在给定计算预算下自适应地保留一组紧凑的视觉token。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLA-Pruner采用双目标重要性准则和相应的双层token选择策略。其整体目标是在给定目标token数量 <code>M~</code> 的情况下，从原始 <code>M</code> 个视觉token中选择一个子集，以最小化剪枝前后模型输出的差异。</p>
<p><img src="https://arxiv.org/html/2511.16449v3/x3.png" alt="方法框架图"></p>
<blockquote>
<p><strong>图3</strong>：VLA-Pruner概述（以预算k=3为例）。它联合了在语义或动作层面显著的token，然后过滤冗余，得到一个紧凑的子集，该子集保留了语义理解和动作执行所必需的信息。</p>
</blockquote>
<p><strong>双目标重要性准则</strong>：VLA-Pruner采用两个重要性指标：1) <strong>语义层面相关性</strong>：使用视觉-语言预填充注意力分数 <code>𝒮_vl</code>（公式2）来衡量；2) <strong>动作层面重要性</strong>：使用动作解码注意力分数 <code>𝒮_act</code>（公式4）来衡量。</p>
<p><strong>动作注意力的时态平滑估计</strong>：由于动作解码注意力在预填充阶段不可用，VLA-Pruner利用机器人操作的时间连续性来估计当前时刻的动作注意力。观察到连续时间步的动作注意力图有高度重叠（见图2），因此采用衰减窗口平均机制进行估计：<code>𝒮̂_act^t = (∑_{i=1}^w γ^i 𝒮_act^{t-i}) / (∑_{i=1}^w γ^i)</code>，其中 <code>w</code> 是窗口大小，<code>γ</code> 是衰减率。这为当前步骤的动作注意力提供了一个简单有效的估计。</p>
<p><strong>双层token选择策略</strong>：为了避免简单的分数加权融合带来的超参数敏感性和冗余问题，VLA-Pruner遵循“先组合后过滤”的补丁级范式，受最小冗余-最大相关性（mRMR）原则启发：</p>
<ol>
<li><strong>双层级Top-k选择</strong>：分别根据预填充注意力 <code>𝒮_vl</code> 和估计的动作注意力 <code>𝒮̂_act</code>，各选出前 <code>M~</code> 个最重要的token，形成候选集 <code>𝒞_vl</code> 和 <code>𝒞_act</code>。</li>
<li><strong>相关性最大化池化</strong>：将两个候选集取并集，得到 <code>𝒞_dual = 𝒞_vl ∪ 𝒞_act</code>。这确保了最终候选池包含了满足双阶段任务相关性的必要token。</li>
<li><strong>冗余最小化过滤</strong>：由于 <code>|𝒞_dual|</code> 通常大于 <code>M~</code>，需要从中筛选出 <code>M~</code> 个token。这被形式化为一个在候选池 <code>𝒞_dual</code> 内的最大-最小多样性问题（MMDP），目标是找到大小为 <code>M~</code> 的子集 <code>𝒞̃</code>，使得其中任意两个token嵌入 <code>v_i</code>, <code>v_j</code> 之间的最小余弦距离 <code>d(v_i, v_j)</code> 最大化。采用贪心算法求解，以最大化所选token集的多样性，减少冗余。</li>
</ol>
<p><strong>创新点</strong>：与现有VLM剪枝方法相比，VLA-Pruner的创新在于：1) 首次明确指出并利用VLA模型预填充与解码阶段注意力模式的差异（双系统特性）；2) 提出双目标重要性准则，通过时态平滑技术预估动作注意力，弥补了预填充阶段动作信息不可知的缺口；3) 设计了基于mRMR原则的双层选择策略，避免了直接分数融合的弊端，能更鲁棒地保留对语义和动作都关键的信息。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境LIBERO（90个任务）和RLBench（18个任务）上评估，使用OpenVLA和π0两种VLA模型架构。对比的基线方法包括：不剪枝的原始模型、VLM剪枝方法（FastV， SparseVLM）、以及最新的VLA加速方法（VLA-Cache， EfficientVLA）。评估指标为任务成功率和推理速度（帧每秒，FPS）。</p>
<p><img src="https://arxiv.org/html/2511.16449v3/x1.png" alt="不同剪枝方法的性能对比"></p>
<blockquote>
<p><strong>图1</strong>：不同视觉token剪枝/缓存方法在各种剪枝/缓存比率下的比较。图表报告了在LIBERO基准上的平均成功率。VLA-Pruner在所有基线方法中表现最优，尤其是在高剪枝比率下。在50%比率时，VLA-Pruner甚至能提升模型性能。</p>
</blockquote>
<p><strong>关键定量结果</strong>：</p>
<ul>
<li>在LIBERO上，使用OpenVLA模型，在87.5%的高剪枝比率下，VLA-Pruner仍能保持85.6%的成功率，而最佳基线SparseVLM的成功率降至53.3%。在50%剪枝率下，VLA-Pruner取得了最高的90.0%成功率，甚至超过了原始模型（88.9%）。</li>
<li>在RLBench上，使用π0模型，VLA-Pruner在50%剪枝率下实现了1.99倍的加速，同时成功率仅下降1.1%（从68.9%到67.8%），而其他基线方法性能下降显著（例如FastV下降14.4%）。</li>
<li>综合来看，VLA-Pruner在保持与原始模型相当性能（平均差距&lt;3%）的同时，能实现1.3倍至2.0倍的推理加速。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.16449v3/x4.png" alt="速度-性能权衡曲线"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO基准上，不同方法的速度（FPS）与性能（成功率）权衡曲线。VLA-Pruner（红色曲线）最接近左上角理想区域，表明其在相同速度下性能最高，或在相同性能下速度最快。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ul>
<li><strong>双目标准则的有效性</strong>：仅使用预填充注意力（即VLM方式）或仅使用估计的动作注意力进行剪枝，性能均显著差于结合两者的VLA-Pruner。</li>
<li><strong>时态平滑估计的有效性</strong>：使用简单的上一帧注意力或均值作为估计，性能均不如使用衰减窗口平均（EMA）机制。</li>
<li><strong>双层选择策略的有效性</strong>：将“并集+多样性过滤”策略替换为直接的加权分数融合，会导致性能下降，并引入敏感的权重超参数。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.16449v3/x2.png" alt="注意力模式可视化"></p>
<blockquote>
<p><strong>图2</strong>：VLA推理中不同的注意力模式。(a-b) 顶部关注图像块的重叠比率分析，显示预填充与解码注意力差异大，而连续解码注意力之间重叠度高。(c-d) 同一帧的预填充和解码注意力可视化，预填充注意力覆盖广泛语义区域，而解码注意力集中于局部执行区域。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.16449v3/x7.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图7</strong>：在“打开抽屉”任务上的定性对比。原始模型和VLA-Pruner成功，而仅使用语义注意力剪枝的方法（FastV）因丢失了抽屉把手附近的细节token而失败。这直观展示了兼顾动作注意力信息的重要性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>揭示了VLA模型的双系统推理特性</strong>：通过实证分析，明确了VLA模型在视觉-语言预填充和动作解码两个阶段存在截然不同的注意力模式，这解释了现有VLM剪枝方法直接应用于VLA时性能下降的根本原因。</li>
<li><strong>提出了VLA-Pruner通用框架</strong>：这是一个无需训练、即插即用的方法，首次为VLA模型设计了同时考虑语义和动作重要性的双目标token剪枝准则，并创新性地利用时态连续性来估计动作注意力。</li>
<li><strong>实现了高效加速且性能保持</strong>：在多个基准和VLA架构上验证了其有效性，能够实现接近2倍的推理加速，同时性能损失极小，甚至在中等剪枝率下能提升性能，展现了卓越的实用价值。</li>
</ol>
<p><strong>局限性</strong>：论文提到，时态平滑估计在注意力发生突变（例如任务目标切换）时可能失效。虽然当前的双层选择策略提供了一定鲁棒性，但这仍是未来可改进的方向。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>VLA高效推理的专用设计</strong>：本研究强调了为VLA设计专用优化工具的重要性，不能简单套用VLM的技术。后续工作可继续探索契合VLA多阶段、时序依赖特性的其他加速维度。</li>
<li><strong>时态建模的深化</strong>：时态平滑是轻量级估计，未来可探索更精细的时序动态建模或轻量级预测器来应对注意力突变场景。</li>
<li><strong>与模型压缩技术结合</strong>：VLA-Pruner作为推理期动态剪枝方法，可与量化、知识蒸馏等静态压缩技术结合，进一步追求极致的部署效率。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型处理连续视觉流时计算开销大的问题，指出现有令牌修剪方法仅依赖语义显著性，忽略动作执行信息，导致性能下降。为此，提出VLA-Pruner方法，采用时间感知的双重目标重要性准则（结合语义相关性和动作解码注意力）和双重级别令牌选择策略，自适应保留关键令牌。实验表明，VLA-Pruner在多个VLA架构上优于现有方法，实现最高1.99倍加速，且在50%修剪比例下提升模型性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16449" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>