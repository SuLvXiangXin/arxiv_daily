<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Reachability Weighted Offline Goal-conditioned Resampling - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Reachability Weighted Offline Goal-conditioned Resampling</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.02577" target="_blank" rel="noreferrer">2506.02577</a></span>
        <span>作者: Joni Pajarinen Team</span>
        <span>日期: 2025-06-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>离线目标条件强化学习（GCRL）旨在从固定数据集中学习可达成多样化目标的策略。为了学习泛化的策略，主流方法（如动态规划中的Q学习）通常采用均匀采样，即从数据集中随机选择状态-动作对和目标进行组合。然而，这种均匀采样存在关键局限性：它需要覆盖所有可能组合的巨大数据集，这在实践中难以实现；更重要的是，它会生成大量从当前状态-动作对出发无法达到的目标，这些“不可达”的样本会引入噪声并严重降低策略性能。本文针对离线GCRL中<strong>随机目标采样导致大量不可达、次优样本，进而损害策略学习</strong>这一具体痛点，提出了一个新视角：采样应倾向于那些能够促成目标达成的转移样本。其核心思路是，通过一个基于Q值的可达性分类器，为数据集中每个（状态，动作，目标）三元组估计一个“可达性”得分，并以此作为采样优先级，从而在训练中更频繁地使用潜在可达的目标样本。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法称为可达性加权采样（Reachability Weighted Sampling, RWS），它是一个即插即用的模块，可无缝集成到标准的离线RL算法（如MCQ、TD3BC、ReBRAC）中。其整体流程是：利用离线数据集训练一个目标条件Q函数；基于此Q函数和数据集，通过正未标记（Positive-Unlabeled, PU）学习训练一个可达性分类器；该分类器将Q值映射为可达性得分，经过指数变换和归一化后，得到每个目标的条件采样权重；在策略训练时，根据此权重对目标进行加权重采样。</p>
<p>核心模块是<strong>基于PU学习的可达性分类器</strong>。其具体作用和技术细节如下：</p>
<ol>
<li><strong>构建PU学习数据集</strong>：正例（Positive）通过后见之明重标记（Hindsight Experience Replay, HER）生成，即对于数据集中的一条轨迹，将其实际到达的后续状态作为目标，并标记为“可达”。未标记样本（Unlabeled）通过均匀目标采样生成，即从整个数据集中随机选取一个目标，与当前状态-动作对组合，这些样本的真实可达性未知，可能包含可达（正）和不可达（负）的混合。</li>
<li><strong>分类器输入与训练</strong>：分类器 (c(Q(s,a,g))) 的输入是目标条件Q值 (Q(s,a,g))，输出是可达概率。论文将Q值视为衡量到达目标距离或未来状态密度的代理特征。分类器使用<strong>非负PU学习损失</strong>进行优化：<br>[ \mathcal{L}<em>{PU} = -\left[\eta_p \mathbb{E}</em>{(x,y)\sim D_P}[\log \hat{y}] + \max \left( \mathbb{E}<em>{(x,y)\sim D_U}[\log (1-\hat{y})] - \eta_p \mathbb{E}</em>{(x,y)\sim D_P}[\log (1-\hat{y})], 0 \right) \right] ]<br>其中 (\eta_p) 是正例的先验概率，(\hat{y}=c(Q(s,a,g)))。这个损失函数确保在只有正例和未标记样本的情况下，能够无偏地估计二元分类风险，并通过max操作防止过拟合。</li>
<li><strong>生成采样权重</strong>：训练好的分类器为每个三元组 ((s,a,g)) 输出一个原始得分 (c(Q(s,a,g)))。为了将其转化为采样权重，首先进行指数变换 (w(s,a,g) = \exp(\beta \cdot c(Q(s,a,g))))，其中 (\beta) 是温度系数，用于控制分布的锐度。然后，在给定 ((s,a)) 的条件下，对所有可能的目标 (g) 进行归一化，得到最终的条件采样概率：(P(g|s,a) = w(s,a,g) / \sum_{g&#39;} w(s,a,g&#39;))。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.02577v1/extracted/6507367/figs/PU_sampling.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：RWS方法整体框架。左侧展示了PU学习数据集的构建：正例通过后见之明重标记从离线数据集中生成，未标记样本通过均匀采样目标生成。右侧展示了基于Q值的可达性分类器的训练过程，以及如何将分类器输出转化为归一化的采样权重。</p>
</blockquote>
<p>与现有方法相比，RWS的创新点具体体现在：1）<strong>针对性地解决目标采样问题</strong>：不同于通用的离线数据加权方法（如优势加权、密度比估计），RWS专门针对目标条件设置，聚焦于评估和加权“目标”的可达性。2）<strong>利用PU学习进行无负例可达性估计</strong>：无需构建显式的状态图（这在状态空间高维或数据集大时不实用），而是巧妙地利用Q值和PU学习来区分可达与不可达的目标，解决了缺乏明确负例（不可达样本）标注的问题。3）<strong>即插即用</strong>：模块化设计使其能够方便地与任何使用目标重采样的离线GCRL算法结合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在六个复杂的模拟机器人操作任务上进行，包括Fetch机械臂环境（FetchPush, FetchPickAndPlace, FetchSlide）和Adroit灵巧手环境（HandReach, HandBlock-Z, HandBlock-Y）。实验平台基于这些标准的GCRL仿真环境。数据集为每个任务收集的离线数据集。</p>
<p><strong>对比方法</strong>：将RWS集成到三种主流的离线RL算法中作为基础学习器：MCQ、TD3BC和ReBRAC。基线是使用相同基础学习器但采用<strong>均匀目标采样</strong>（Uniform Sampling）的方法。此外，在消融实验中还对比了基于优势函数的加权（Advantage Weighting）以及一种朴素的PU分类器（不使用非负损失）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li>在所有六个任务和三种基础学习器上，RWS均显著且一致地超越了均匀采样基线。</li>
<li>性能提升幅度显著，例如在HandBlock-Z任务上，与均匀采样基线相比，RWS将平均成功率提升了近50%。</li>
<li>RWS还表现出更高的数据效率，在仅使用部分数据集（如50%）进行训练时，其性能下降远小于均匀采样方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.02577v1/x2.png" alt="实验结果"></p>
<blockquote>
<p><strong>图6</strong>：在六个模拟机器人操作任务上的平均成功率对比。RWS（橙色）在所有任务和三种离线RL算法（MCQ, TD3BC, ReBRAC）上都显著优于均匀采样基线（蓝色），尤其是在HandBlock-Z任务上提升接近50%。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文在PointMaze-2D任务上进行了消融实验，比较了不同加权策略。</p>
<p><img src="https://arxiv.org/html/2506.02577v1/extracted/6507367/figs/map_weight_comparison.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验对比不同加权策略在PointMaze-2D任务上的性能。RWS（橙色）显著优于均匀采样（Uniform，蓝色）、基于优势的加权（Advantage，绿色）以及仅使用正例训练的朴素分类器（Naïve PU，红色）。</p>
</blockquote>
<p>消融实验表明：1) <strong>均匀采样性能最差</strong>，证实了随机目标引入噪声的问题。2) <strong>基于优势的加权</strong>有一定提升，但不如RWS，说明通用优势加权无法像RWS那样精准评估目标可达性。3) <strong>朴素PU分类器</strong>（不使用非负损失）性能不稳定且较差，验证了非负PU损失对于稳定训练的重要性。这些结果共同证明了RWS各个组件（PU学习框架、非负损失、基于Q值的特征）的必要性和有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：1) 提出了一个新颖的、即插即用的<strong>可达性加权采样（RWS）</strong> 方案，专门用于提升离线目标条件强化学习的数据质量。2) 创新性地将<strong>正未标记（PU）学习</strong>应用于基于Q值的可达性分类，从而能够在没有明确负例标注的情况下，有效区分可达与不可达的目标。</p>
<p>论文自身提到的局限性主要在于<strong>对Q函数估计质量的依赖</strong>。RWS的有效性建立在目标条件Q函数能够合理反映状态-目标距离或未来状态密度的基础上。如果离线Q学习本身因函数逼近误差或外推误差而失效，那么以此为输入的RWS分类器也可能产生不准确的权重。</p>
<p>本文对后续研究的启示包括：1) <strong>模块化设计思路</strong>：RWS的成功表明，针对数据重采样或加权的模块化改进可以广泛推广到各类离线RL算法中。2) <strong>PU学习在RL中的应用</strong>：为解决RL中普遍存在的“只有成功样例，缺乏明确失败标注”的问题（类似PU学习设定）提供了新工具。3) <strong>超越均匀采样</strong>：在离线GCRL乃至更广泛的序列决策学习中，对数据分布进行主动的、基于模型理解的重新加权，是提升学习效率和最终性能的一个富有前景的方向。未来工作可以探索更复杂的可达性建模方式，或将此思想扩展到多任务、分层强化学习等场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对离线目标条件强化学习中均匀采样产生大量不可达状态-目标-动作对、降低策略性能的问题，提出可达性加权采样方法。该方法通过正未标记学习训练一个可达性分类器，将目标条件状态-动作值映射为可达性得分，并以此作为采样优先级，构成一个即插即用模块。在六个复杂模拟机器人操作任务上的实验表明，该方法显著提升了性能，其中HandBlock-Z任务性能较基线提升近50%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.02577" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>