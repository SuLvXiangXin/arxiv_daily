<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.16565" target="_blank" rel="noreferrer">2506.16565</a></span>
        <span>作者: Ran Tian Team</span>
        <span>日期: 2025-06-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>世界模型使机器人能够根据当前观测和计划动作“想象”未来的观测，并越来越多地被用作广义动力学模型来促进机器人学习。然而，尽管前景广阔，这些模型在遇到训练中罕见的物体和背景元素等新颖视觉干扰物时仍然脆弱。具体而言，新颖的干扰物会破坏动作结果的预测，当机器人依赖世界模型的想象进行规划或动作验证时，会导致下游失败。现有方法主要通过训练时策略来缓解视觉干扰物，例如利用特权奖励信号学习任务相关表征，或训练独立的网络分支来分别建模任务相关和任务无关组件。但这些方法主要针对训练分布内的干扰物，且与训练时的任务上下文紧密耦合。当测试时任务上下文发生变化，或引入新颖干扰物时，其性能会下降甚至完全失败。本文针对世界模型在开放世界中不可避免的新颖视觉干扰物面前预测不可靠的具体痛点，提出了一种测试时、即插即用的新策略。核心思路是：在测试时识别当前观测中的干扰物并将其修复，使其更接近训练分布，然后用修复后的观测重新想象未来结果，最后事后重新引入干扰物以保持视觉一致性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了 Reimagination with Observation Intervention (ReOI)，一个用于视觉模型预测控制的测试时策略。其整体流程是：给定当前机器人观测，首先通过分析世界模型预测序列来识别新颖视觉干扰物；然后通过分割和修复修改当前观测，移除这些干扰物；接着，使用修改后的观测作为输入，通过原始世界模型“重新想象”未来动作结果；最后，通过深度感知合成将移除的干扰物事后重新引入到每个预测帧中，以保持视觉一致性供下游规划验证使用。</p>
<p><img src="https://arxiv.org/html/2506.16565v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：基于世界模型的机器人规划中，通过观测干预进行重新想象的示意图。上图（基线）：新颖视觉干扰物在预测观测中扭曲，导致模型产生幻觉并抹去关键障碍物，验证器选择了不安全动作计划。下图（我们的方法）：测试时观测干预使世界模型生成更符合现实的预测，验证器能识别潜在碰撞并正确选择安全计划。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><p><strong>新颖视觉干扰物识别</strong>：关键洞察是，训练分布中未充分表示的视觉干扰物在世界模型展开中会表现出物理上不合理的行为（快速扭曲、消失或不自然变形）。利用这一洞察，作者使用一个视觉语言模型（VLM，如GPT-4o）来分析世界模型对一个分布内“安全检查”动作计划所预测的观测序列。VLM被提示分析物体的时间演化，并将那些在帧间发生快速扭曲的物体标记为潜在的新颖视觉干扰物。<br>   <img src="https://arxiv.org/html/2506.16565v1/x2.png" alt="干扰物识别"></p>
<blockquote>
<p><strong>图2</strong>：新颖视觉干扰物识别与观测干预。利用VLM分析物体的时间演化，标记快速扭曲的物体为潜在干扰物（左侧）。这些有问题的干扰物被修复，使观测更接近世界模型的训练分布（右侧）。</p>
</blockquote>
</li>
<li><p><strong>分割与修复</strong>：识别出干扰物后，使用分割模型（Grounded-SAM2）定位并分割相应区域。然后将这些区域传递给图像修复模型，移除干扰物并填充掩码区域，生成修改后的观测。</p>
</li>
<li><p><strong>基于修改观测的重新想象与事后合成</strong>：使用修改后的观测作为输入，通过原始世界模型预测动作计划的结果。为了保持视觉一致性，需要将移除的干扰物重新引入预测中。具体方法是：使用Grounded-SAM2将每个预测观测分解为一组物体层（如机器人、任务相关物体、背景），并为每层估计深度。同时，从原始观测中提取修复的干扰物层并基于其原始空间位置估计深度。然后，将该干扰物层添加到每个预测帧的层集合中。最后，按照从后到前的深度顺序合成这些层来重建每个预测帧，确保正确的遮挡关系。<br>   <img src="https://arxiv.org/html/2506.16565v1/x3.png" alt="重新想象流程"></p>
<blockquote>
<p><strong>图3</strong>：基于修改观测的重新想象。修改输入后的预测未来观测经过后处理，通过深度感知合成重新引入之前修复的干扰物，确保正确的遮挡（例如包裹盒出现在夹爪后方）。</p>
</blockquote>
</li>
</ol>
<p>与现有方法相比，创新点在于：这是首个利用测试时观测干预来解决基于世界模型的机器人策略验证与选择中新颖视觉干扰物问题的工作。它不依赖于任务特定的监督或假设训练-测试上下文一致，而是直接在部署时检测并处理未知干扰物。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界机器人操作设置中进行，使用Fanuc LR Mate 200iD/7L 6自由度机器人，任务是在玩具厨房环境中拾取和放置物体。使用了预训练的Diffusion Policy作为模仿动作计划采样器。世界模型基于DINO-WM，使用500条机器人-环境交互轨迹进行训练。</p>
<p>对比的基线方法包括：基础的DINO-WM模型，以及TrustRegion方法（该方法在输入观测-动作对超出可信区域时拒绝世界模型预测，以确保安全）。</p>
<p>关键实验结果：</p>
<ol>
<li><p><strong>预测质量评估</strong>：使用SSIM和LPIPS指标评估预测的视觉动作结果质量。如表I所示，ReOI在完整观测评估和修复干扰物后的分布内组件评估上，均优于DINO-WM，表明其能产生更准确、更一致的预测，并有效缓解了因新颖干扰物引起的分布内物体动态预测幻觉。<br>   <img src="https://arxiv.org/html/2506.16565v1/x4.png" alt="定性预测对比"></p>
<blockquote>
<p><strong>图4</strong>：预测质量的定性示例。新颖干扰物导致DINO-WM产生幻觉，预测失败的计划会成功（左列），并错误地从预测中抹去目标物体（右列）。ReOI通过测试时干预产生了更接近地面真相的预测。</p>
</blockquote>
</li>
<li><p><strong>系统级任务成功率</strong>：在动作计划验证的背景下评估系统性能。如表II所示，ReOI实现了70%的任务成功率，显著高于DINO-WM的20%和TrustRegion的0%。同时，ReOI保持了与TrustRegion相当的低碰撞率（10%），远低于DINO-WM的40%。这表明ReOI通过测试时干预实现了更有效且安全的视觉规划。<br>   <img src="https://arxiv.org/html/2506.16565v1/x5.png" alt="规划定性对比"></p>
<blockquote>
<p><strong>图5</strong>：机器人视觉规划的定性示例。新颖干扰物导致DINO-WM产生幻觉并抹去关键障碍，使验证器接受了不安全计划。ReOI生成的预测更符合现实，使验证器能识别潜在碰撞并选择安全计划。</p>
</blockquote>
</li>
</ol>
<p>消融实验方面，通过修复干扰物后评估SSIM和LPIPS（表I右半部分），结果显示即使移除干扰物后，ReOI的预测在分布内物体动态方面仍比基线更一致，证明了测试时干预对于防止干扰物破坏分布内动态预测的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了一种简单有效的测试时观测干预策略（ReOI），使世界模型在开放世界遇到不可避免的新颖干扰物时能预测更可靠的动作结果；2）首次将测试时观测干预用于解决基于世界模型的机器人策略验证中的新颖视觉干扰物问题；3）在真实机器人操作任务上验证了该方法，在存在新颖干扰物时将任务成功率提升了高达3倍。</p>
<p>论文提到的局限性包括：当前工作主要关注缓解静态新颖视觉干扰物的影响，处理动态干扰物仍是一个开放挑战；此外，事后重新引入干扰物的方法可能引入物理上不真实的交互（如机器人夹爪看似穿过重新插入的干扰物），但作者指出这不会影响策略验证，因为任何表现出此类违规的轨迹都会在验证过程中被自动拒绝。</p>
<p>对后续研究的启示：该方法展示了测试时适应在处理开放世界不确定性方面的潜力。未来工作可以探索如何将类似原理应用于动态干扰物，或者研究更高效的干扰物识别与修复模块，以减少对大型VLM和分割/修复模型的依赖。此外，如何将这种干预策略更紧密地集成到世界模型的训练或微调过程中，也是一个值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉模型预测控制中，世界模型对训练时未见过的新视觉干扰物（如陌生物体、背景）敏感，导致预测失准和下游任务失败的问题，提出一种名为ReOI的测试时观测干预方法。该方法通过检测预测中物理不合理的变化来识别干扰物，修改当前观测以去除干扰、使其更接近训练分布，然后重新进行未来状态“想象”，最后再恢复干扰物以保持视觉一致性。实验表明，ReOI能有效应对分布内外的视觉干扰，在存在新干扰物时，将任务成功率提升高达3倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.16565" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>