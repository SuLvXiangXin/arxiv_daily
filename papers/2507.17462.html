<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.17462" target="_blank" rel="noreferrer">2507.17462</a></span>
        <span>作者: Hesheng Wang Team</span>
        <span>日期: 2025-07-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习依赖于4D多视角时序图像进行训练。然而，高质量专家演示数据采集成本高昂、数量稀缺，严重制约了如视觉-语言-动作（VLA）模型等具身智能策略的泛化与应用。数据增强是克服数据稀缺的有力策略，但目前缺乏专门针对机器人操作任务的4D多视角时序图像编辑方法。现有方法如CACTI和ROSIE主要聚焦于编辑单帧静态图像，这对于需要时空连续4D数据进行训练的现代VLA模型（如RDT、OpenVLA）来说是远远不够的。编辑4D机器人多视角序列图像面临三大核心挑战：1）在动态视角和长时间跨度上保持几何与外观一致性，特别是处理相机与物体同时运动产生的运动模糊；2）在有限计算成本下扩展有效工作窗口；3）确保机器人手臂等关键对象的语义完整性，避免自回归误差累积导致的质量退化。</p>
<p>本文针对机器人4D数据编辑这一空白领域，提出了一种新颖的数据增强框架ERMV。其核心思路是：基于单帧编辑和机器人状态条件，通过引入视觉引导、稀疏时空采样、运动感知的几何约束以及反馈干预机制，高效、一致地编辑整个多视角序列。</p>
<h2 id="方法详解">方法详解</h2>
<p>ERMV是一个基于潜空间扩散模型（LDM）的条件生成框架，旨在学习条件概率分布 $p(\mathcal{X}&#39;|\mathcal{X}, \mathcal{C}<em>{guide}, \mathcal{C}</em>{state}, \mathcal{C}<em>{history})$，其中 $\mathcal{X}$ 是原始4D图像序列，$\mathcal{X}&#39;$ 是编辑后的序列。条件包括视觉引导 $\mathcal{C}</em>{guide}$、机器人及相机状态 $\mathcal{C}<em>{state}$ 和历史记忆特征 $\mathcal{C}</em>{history}$。</p>
<p><img src="https://arxiv.org/html/2507.17462v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ERMV整体流程。1. <strong>条件设置</strong>：以用户编辑的单帧图像为视觉引导，并融入机器人及相机状态作为物理条件。2. <strong>稀疏时空（SST）模块</strong>：通过稀疏采样提取历史帧并选择大工作窗口内待编辑的未来帧。3. <strong>扩散模型</strong>：在生成模型中，提出的对极运动感知注意力（EMA-Attn）利用对极几何引导多视角特征融合并捕捉运动信息。4. <strong>反馈干预机制</strong>：由多模态大语言模型（MLLM）驱动的干预模块评估关键对象质量，若检测到退化则建议专家介入，注入掩码条件以确保核心对象质量。</p>
</blockquote>
<p>整体流程包含以下核心模块：</p>
<ol>
<li><strong>视觉引导条件</strong>：为解决文本提示的模糊性，ERMV采用精确的视觉引导策略。选取具有全局信息的第一视角首帧图像 $x_{t=1}^{(v=1)}$，通过高级修复模型或手动编辑生成目标引导图像 $x&#39;<em>{guide}$。该图像经CLIP编码器嵌入后，作为明确、空间感知的语义目标 $\mathcal{C}</em>{guide}$，指导扩散模型在所有视角和时间步上一致地传播编辑效果。</li>
<li><strong>机器人及相机状态注入</strong>：为使模型理解场景的精确几何和动态状态，条件 $\mathcal{C}_{state}$ 包含两部分：a) <strong>位姿与状态条件</strong>：为每个目标图像 $x_t^{(v)}$ 提供对应的相机位姿 $\mathbf{P}_t^{(v)}$ 和机器人动作（如关节位置）$q_t$；b) <strong>运动动力学条件</strong>：提供连续时间步的位姿和状态，使模型能够推断运动趋势，这对于在动态环境中真实渲染运动模糊至关重要。</li>
<li><strong>稀疏时空（SST）模块</strong>：为在有限计算资源下扩展工作窗口，ERMV将密集的时空注意力问题重构为低成本的单帧多视角编辑问题。SST模块将一个大工作窗口（例如包含$M$个时间步）解耦为时空两部分：在时间维度上，稀疏地采样少量历史帧（如$K$帧，$K \ll M$）以提供上下文；在空间维度上，同时处理当前时间步的所有$N$个视角图像。这样，模型在一次前向传播中即可基于稀疏的历史信息和完整的当前多视角信息，编辑未来多个时间步（窗口内剩余帧）的图像，极大提升了效率。</li>
<li><strong>对极运动感知注意力（EMA-Attn）</strong>：这是扩散模型U-Net中的核心注意力机制，用于确保动态场景下的时空一致性。传统基于对极几何的约束在存在运动模糊时失效，因为特征对应关系被破坏。EMA-Attn分两步解决此问题：首先，通过一个轻量级运动预测网络，从条件状态中学习每个像素因相机和物体运动引起的偏移量；然后，在应用对极线约束进行跨视角特征聚合时，将这些预测的偏移量纳入考虑，从而在运动条件下也能建立准确的特征对应，保持一致性并逼真地渲染运动模糊。</li>
<li><strong>反馈干预机制</strong>：为缓解自回归生成中的错误累积，特别是对机器人手臂等关键对象的影响，ERMV引入了由MLLM驱动的实用反馈循环。在生成一批编辑后的图像后，MLLM会自动检查关键对象在编辑前后的语义一致性（例如，手臂形状是否扭曲）。如果检测到不一致，系统会提示专家介入。专家仅需提供关键对象的分割掩码，该掩码将作为额外条件注入到下一轮编辑中，进行针对性修正，从而以最小专家工作量保障核心对象质量。</li>
</ol>
<p>与现有方法相比，ERMV的创新点具体体现在：1) 针对机器人动态多视角场景，提出了结合运动预测的几何约束注意力（EMA-Attn），解决了运动模糊下的编辑一致性问题；2) 通过SST模块，以稀疏采样的方式实现了大工作窗口下的高效编辑，降低了计算门槛；3) 引入MLLM辅助的反馈干预机制，有效控制了长序列编辑中的误差传播。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：ERMV在公开的RoboTwin仿真基准、真实世界的RDT数据集以及自建的真实双机械臂平台上进行了验证。训练使用AdamW优化器，批量大小为4，在8张A100 GPU上进行。</p>
<p><strong>对比基线</strong>：包括：1) <strong>单帧编辑方法</strong>：ROSIE；2) <strong>视频生成方法</strong>：EVAC（生成模型，非编辑模型）；3) <strong>多视角编辑方法</strong>：BEVControl, MagicDrive（适用于固定相机环）；4) <strong>3D一致编辑方法</strong>：DGE, 3D-Adapter。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>数据增强提升VLA策略性能</strong>：在RoboTwin仿真环境中，使用ERMV增强数据训练的VLA模型，在未知环境中的任务成功率显著提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.17462v1/x4.png" alt="仿真环境VLA策略性能"></p>
<blockquote>
<p><strong>图4</strong>：在RoboTwin仿真基准上，使用ERMV增强数据训练的VLA模型（ERMV-VLA）与基线模型（Orig-VLA）在未知环境中的性能对比。ERMV-VLA取得了显著更高的成功率。</p>
</blockquote>
<p>具体而言，在“推到目标”任务中，基线VLA模型成功率为42.5%，而使用ERMV增强数据训练的VLA模型成功率提升至72.5%。在“抓取与放置”任务中，成功率从35%提升至65%。</p>
<ol start="2">
<li><strong>编辑效果与一致性验证</strong>：ERMV能够有效地对4D序列进行多样化编辑，如更换背景、添加/改变物体，并保持跨视角和跨时间的时空一致性。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.17462v1/x5.png" alt="编辑效果定性展示"></p>
<blockquote>
<p><strong>图5</strong>：ERMV编辑效果定性展示。能够一致地改变场景背景（如从实验室变为厨房）、添加新物体（如墙上的画），并保持机器人手臂和操作物体的一致性。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：实验验证了各核心组件的贡献。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.17462v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。移除SST模块会导致时空不一致；移除EMA-Attn会破坏运动模糊区域的连续性；移除反馈干预则会导致关键对象（机械臂）质量随序列延长而下降。</p>
</blockquote>
<ul>
<li><strong>SST模块</strong>：移除后，模型只能处理极小窗口，导致长序列前后不一致。</li>
<li><strong>EMA-Attn机制</strong>：替换为标准的对极注意力后，在快速运动帧中会出现明显的伪影和几何错位。</li>
<li><strong>反馈干预机制</strong>：关闭后，在长序列生成后期，机器人手臂的形态会发生明显畸变。</li>
</ul>
<ol start="4">
<li><strong>工作窗口与效率</strong>：SST模块使得ERMV能够在单张消费级GPU（RTX 4090）上处理长达128帧的大工作窗口，而密集时空注意力方法在同样硬件上仅能处理16帧。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.17462v1/x7.png" alt="工作窗口比较"></p>
<blockquote>
<p><strong>图7</strong>：不同方法在固定GPU内存（24GB）下所能支持的最大工作窗口大小比较。ERMV（SST）显著优于密集时空注意力方法。</p>
</blockquote>
<ol start="5">
<li><strong>缩小仿真到真实差距</strong>：ERMV能够将仿真图像编辑成真实风格。用此风格转换后的仿真数据训练VLA模型，其在真实机器人平台上的任务成功率，比直接用原始仿真数据训练的模型有显著提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.17462v1/x8.png" alt="Sim-to-Real结果"></p>
<blockquote>
<p><strong>图8</strong>：Sim-to-Real实验。ERMV能将仿真图像（左）编辑为真实风格（中）。使用编辑后数据训练的模型，在真实机器人（右）上执行抓取任务的成功率更高。</p>
</blockquote>
<p>在真实机器人抓取任务中，使用ERMV进行风格转换后的数据训练的模型，相比使用原始仿真数据训练的模型，成功率从30%提升到了60%。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个针对4D机器人多视角时序图像的编辑框架ERMV，为缓解VLA模型训练的数据稀缺问题提供了有效的数据增强工具。</li>
<li>通过创新的对极运动感知注意力（EMA-Attn）解决了动态场景下运动模糊的时空一致性问题，通过稀疏时空（SST）模块实现了大工作窗口下的高效编辑，并通过MLLM辅助的反馈干预机制控制了长序列误差累积。</li>
<li>在仿真、真实数据集和真实机器人平台上进行了广泛实验，验证了ERMV增强数据能显著提升下游VLA策略的鲁棒性和泛化能力，并证明了其具备缩小仿真到真实差距的潜力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，ERMV的编辑灵活性在一定程度上依赖于初始提供的单帧视觉引导图像的质量和完整性。对于极其复杂的场景变换，可能需要更精细的分层引导。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>ERMV展示了将生成式模型作为“世界模型”用于机器人数据合成和编辑的可行性，为构建交互式仿真器提供了新思路。</li>
<li>所提出的结合物理状态（位姿、动作）的生成框架，为构建更物理可信的生成模型指明了方向。</li>
<li>利用MLLM进行生成质量自动评估与反馈的机制，可以推广到其他需要长序列、高保真度生成的任务中，以实现更智能的生成过程控制。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习中4D多视角序列图像数据稀缺且编辑方法缺失的问题，提出了ERMV数据增强框架。其核心技术包括：用于保证运动模糊时空一致性的Epipolar Motion-Aware Attention机制；通过解耦时空视图与稀疏采样以扩大编辑窗口、降低计算成本的Sparse Spatio-Temporal模块；以及利用多模态大语言模型进行反馈干预以减轻错误累积的机制。实验表明，ERMV增强的数据显著提升了VLA模型在仿真与真实环境中的鲁棒性和泛化能力，并能有效缩小仿真到现实的差距。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.17462" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>