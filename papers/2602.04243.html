<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.04243" target="_blank" rel="noreferrer">2602.04243</a></span>
        <span>作者: Wenzhao Lian Team</span>
        <span>日期: 2026-02-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习（IL）是让机器人从专家示范中学习任务的主流方法。当前大多数IL方法依赖于固定的相机设置，无论是单相机还是多相机。固定单相机设置成本低，但视野有限，可能遮挡环境关键部分，影响任务性能。多相机设置旨在提供更全面的场景覆盖，但引入了冗余或无关信息，可能淹没学习算法并降低效率。这些被动的静态多视角设置并不总能提供最任务相关、最清晰的信息，导致决策次优。</p>
<p>受人类主动感知（动态调整视角以获取最相关、噪声最少的信息）的启发，本文提出从被动、静态感知转向主动感知，即在任务执行过程中动态调整视角以优化信息获取。本文针对单相机机器人系统，提出了一个名为MAE-Select的主动视角选择框架。其核心思路是：利用预训练的多视角掩码自编码器（MV-MAE）的表征能力，通过模仿学习目标隐式地学习最优视角选择策略，无需手动标注最优视角标签，从而动态地为每个时间块选择信息量最大的下一个视角。</p>
<h2 id="方法详解">方法详解</h2>
<p>MAE-Select框架旨在使机器人智能体能够为操作任务主动选择信息丰富的视角。其核心在于通过模仿学习目标隐式地学习最优视角选择，无需显式监督或强化学习。与之前仅使用预训练编码器的工作不同，该方法利用了多视角掩码自编码器（MV-MAE）的编码器和解码器，使智能体能够从单视图构建丰富的场景表征，这对于准确的动作预测和明智的视角选择至关重要。</p>
<p><img src="https://arxiv.org/html/2602.04243v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧为多视角掩码自编码器（MV-MAE）的预训练阶段，学习联合嵌入。中间展示了使用模仿学习训练我们框架的过程。右侧演示了推理阶段框架如何运行。</p>
</blockquote>
<p><strong>整体流程与问题定义</strong>：将机器人操作任务定义为在观测和动作序列上的学习问题。智能体在时间t的状态由本体感觉信息s_t和来自固定N_v个可用视角集合V的视觉观测O_t组成。目标是学习一个策略π，它基于当前状态生成未来T个动作的块a_{t:t+T-1}。在主动感知设置中，策略无法同时访问所有视角，而是每次只能操作一个选定的视角o_t^{v_t}。因此，目标是学习两个关键策略：1) 动作策略π_θ(a_{t:t+T-1} | o_t^{v_t}, s_t)，基于当前单视角观测和本体感觉状态预测未来动作序列；2) 视角选择策略π_ψ(v_{t+T} | o_t^{v_t}, s_t, a_{t:t+T-1})，基于当前块的信息为下一个时间块选择最有利的视角v_{t+T}。两个策略通过模仿学习框架联合训练。</p>
<p><strong>多视角掩码自编码器预训练</strong>：在示范数据上预训练一个MV-MAE，以学习紧凑的、具有3D感知的场景表征。给定多视角观测O_t，提取特征图并应用双重掩码：1) 块掩码：随机掩码每个视图内的大部分图像块；2) 视图掩码：随机丢弃整个视图以鼓励跨视图推理。剩余的块与视图和位置嵌入一起输入Transformer编码器f_φ，产生潜在表示z_t^m。解码器g_φ从z_t^m、掩码令牌和机器人状态嵌入s_t重建所有视图，通过最小化重建损失L_MAE（公式1）进行训练。这使得模型能够从部分或被遮挡的输入中推断完整的3D场景，用于下游任务。</p>
<p><strong>下一个更优视角选择</strong>：预训练后，联合微调模型并训练动作和视角选择策略。动作解码器使用真实动作进行训练，但定义真实的“最优”视角是不切实际的。本文提出一种无标签策略来训练视角选择器π_ψ：将未来数据块D_{t+T}上的动作预测损失作为当前块D_t中所选视角的监督信号。具体流程如下：</p>
<ol>
<li><strong>处理当前块（D_t）</strong>：从随机选择的单视图o_t^v开始，将其通过预训练的MV-MAE（f_φ, g_φ）生成估计的多视角特征上下文C_t = g_φ(f_φ(o_t^v), s_t)。C_t被输入到基于扩散的动作解码器π_θ，该解码器被训练来预测添加到专家动作轨迹a_{t:t+T-1}上的噪声ε_t，计算动作损失L_action^(t)（公式2）。</li>
<li><strong>为下一个块选择视角（D_{t+T}）</strong>：视角选择器π_ψ（一个Transformer编码器）接收特征上下文C_t和当前块的真实动作轨迹a_{t:t+T-1}，预测下一个块的视角概率分布ĉ = Softmax(π_ψ(C_t, a_{t:t+T-1}))（公式3）。为了在保持反向传播可微性的同时选择视角，采用VQ-VAE中的直通估计器。前向传播时执行argmax操作得到表示所选视图的独热向量y；反向传播时，梯度通过连续的softmax概率ĉ流动（公式4）。</li>
<li><strong>处理下一个块（D_{t+T}）</strong>：独热向量y用于从下一个观测集O_{t+T}中选择一个单视角o_{t+T}^{v̂}（通过点积实现，也允许梯度流回选择策略）。该选定视图通过与第一个块相同的流程处理，产生相应的动作损失L_action^(t+T)。</li>
<li><strong>更新视角选择器</strong>：没有显式的“视角损失”。视角选择器π_ψ使用来自未来动作损失L_action^(t+T)的梯度进行训练，该梯度通过动作解码器和STE选择的观测反向传播，直接优化π_ψ以选择能最小化未来动作预测误差的视角。</li>
<li><strong>总体目标</strong>：总体损失结合了两个动作损失和辅助重建损失：L_total = L_action^(t) + λ1 L_action^(t+T) + λ2 L_MAE（公式5）。其中π_θ由两个动作损失更新，而π_ψ仅由未来动作损失优化。</li>
<li><strong>推理</strong>：推理过程是自回归的。从一个随机初始视角开始，预测当前动作块和下一个块的最优视角。预测的动作与新选择的视角一起用于后续步骤，形成一个动态的感知-动作循环。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个具有挑战性的场景和八个高要求的模拟任务中进行评估，包括ACT、RLBench和MuJoCo中的模拟，以及三个真实世界任务。实现基于扩散策略架构，采用单相机控制设置：训练时使用多个相机视角，部署时使用单个可移动相机，每个时间块相机被放置在一个训练视角上。</p>
<p><strong>对比实验</strong>：主要对比方法包括Diffusion Policy（基线）、MAE-Diffusion（使用预训练MAE编码器但不进行主动选择的变体）以及MAE-Select。实验指标为成功率。</p>
<p><img src="https://arxiv.org/html/2602.04243v1/x1.png" alt="实验结果表"></p>
<blockquote>
<p><strong>表1</strong>：对比实验结果。对于八个模拟任务，报告成功率（每次50次策略评估）。对于真实世界任务，报告成功次数（使用人类数据训练，10次评估）。加*表示存在干扰的场景。MAE-Select在训练时使用所有视角，测试时每个时间块仅使用一个视角。粗体和下划线字体分别表示最佳和次佳结果。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>模拟任务</strong>：在“Bimanual Insertion”任务（Top视角）中，MAE-Select达到52%，优于Diffusion Policy（42%）和MAE-Diffusion（48%）。在“Put Box In Cabinet”任务（Top视角）中，MAE-Select达到88%，显著高于Diffusion Policy（16%）和MAE-Diffusion（42%）。在存在干扰（*）的“Put Box In Bin”任务中，MAE-Select达到58%，也优于其他方法。在“Phone On Base”任务（Front视角）中，MAE-Select达到92%，表现最佳。</li>
<li><strong>真实世界任务</strong>：在“Put Eggplant To Bowl”任务中，MAE-Select获得6/10的成功次数，优于Diffusion Policy（2/10）和MAE-Diffusion（4/10）。在存在干扰的任务中，MAE-Select获得6/10，同样优于基线。</li>
<li><strong>与ACT结合</strong>：如表2所示，将MAE-Select与ACT框架结合（MAE-ACT）并在其上应用主动选择（MAE-Select），在“Bimanual Insertion”（Top视角）和“Phone On Base”（Front视角）任务上分别达到36%和70%的成功率，均优于原始ACT和MAE-ACT。</li>
</ul>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2602.04243v1/x1.png" alt="消融实验表"></p>
<blockquote>
<p><strong>表3</strong>：关于MAE编码器和解码器利用的消融研究。比较了仅使用MAE编码器（MAE-Encoder）与使用完整MAE编码器-解码器（MAE-Diffusion）的性能。</p>
</blockquote>
<p>消融实验表明，使用完整的MAE编码器-解码器（MAE-Diffusion）比仅使用编码器（MAE-Encoder）性能有显著提升。例如，在“Put Box In Cabinet”（Both视角）任务中，MAE-Diffusion为46%，而MAE-Encoder为34%；在“Phone On Base”（Both视角）任务中，MAE-Diffusion为88%，MAE-Encoder为80%。这验证了利用解码器重建多视角上下文对于提供丰富场景表征的重要性。</p>
<p><strong>定性结果</strong>：<br><img src="https://arxiv.org/html/2602.04243v1/x2.png" alt="视角选择可视化"></p>
<blockquote>
<p><strong>图2</strong>：实验中选定视角的可视化，展示了模拟和真实世界环境。每一行代表一个特定任务的过程，表明了选择不同视角的必要性。</p>
</blockquote>
<p>图2可视化了MAE-Select在任务执行过程中动态选择的视角。例如，在“Put Box In Cabinet”任务中，初始选择“Top”视角以观察全局和箱子位置，随后切换到“Left”视角以更好地观察柜子内部和插入过程。这直观地展示了框架能够根据任务进展自适应地选择信息量最大的视角。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了MAE-Select，一种无需手动标注、能够在每个时间块动态选择下一个最优视角的新机制。</li>
<li>提出了一个充分利用预训练MAE表征进行机器人操作的模仿学习框架，特别是利用了完整的编码器-解码器架构来从单视图构建多视角上下文。</li>
<li>通过大量实验证明，MAE-Select能够提升单相机设置下的操作精度，在某些任务中甚至超越了多相机系统。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确列出局限性，但根据方法描述，其视角选择范围被限制在预定义的一组固定视角中，而非连续空间中的任意视角。此外，框架的性能依赖于预训练MV-MAE的质量和示范数据的覆盖范围。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>主动感知范式</strong>：将视角视为一个可优化的决策变量，而非固定输入，为机器人感知开辟了新方向。未来工作可以探索更精细的连续视角控制或结合其他传感器模态。</li>
<li><strong>自监督表征的利用</strong>：展示了如何将大规模预训练的自监督视觉模型（如MAE）有效地融入机器人决策循环，为利用基础模型解决机器人问题提供了参考。</li>
<li><strong>无标签策略学习</strong>：通过将未来任务性能作为监督信号来训练视角选择策略，这种无标签学习方法可以扩展到其他需要学习隐式、难以标注的决策变量的场景中。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习中固定摄像头视角限制适应性和覆盖范围的问题，提出MAE-Select框架。该方法利用预训练的多视角掩码自编码器表示，动态选择信息量最大的下一个观测视角，无需标注数据。实验表明，该框架提升了单摄像头系统的性能，部分任务甚至超越多摄像头配置。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.04243" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>