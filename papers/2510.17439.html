<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.17439" target="_blank" rel="noreferrer">2510.17439</a></span>
        <span>作者: Zhang, Zhengshen, Li, Hao, Dai, Yalun, Zhu, Zhengbang, Zhou, Lei, Liu, Chenchen, Wang, Dong, Tay, Francis E. H., Chen, Sijin, Liu, Ziwei, Liu, Yuxiao, Li, Xinghang, Zhou, Pan</span>
        <span>日期: 2025/10/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉-语言-动作模型通常构建在2D视觉语言模型之上，虽然具备强大的语义理解能力，但因其本质工作在2D图像域，与机器人必须交互的3D物理世界之间存在显著差距。这种“空间推理鸿沟”导致VLA模型在需要几何、深度或空间关系推理的场景中泛化能力和适应性受限。现有尝试整合3D信息的方法主要分为两类：一类直接依赖显式3D输入（如点云、深度图），但需要专用传感器且模态可迁移性差；另一类引入弱3D线索（如伪深度估计或可学习的空间嵌入），但存在空间表征能力有限、无法有效利用高质量3D输入以及可能破坏预训练视觉-语言对齐的问题。</p>
<p>本文针对VLA模型在空间表征、模态可迁移性和模态对齐三个方面的关键局限性，提出了一种新的范式：不将空间信息强行注入视觉-语言主干，而是将其作为丰富的先验知识，直接整合到负责动作生成的“动作头”中。本文的核心思路是利用空间基础模型从RGB图像中提取强几何先验，并通过一个可选的、灵活的具身空间模型融合额外的3D模态（深度、位姿），最后在解耦的空间增强动作头中完成语义特征与空间特征的融合，以生成精确的机器人动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>FALCON的整体框架是一个端到端的VLA模型，包含三个核心组件：用于多模态语义表征的2D视觉语言模型、用于提取3D结构特征的具身空间模型，以及融合两者信息以生成机器人动作的空间增强动作头。</p>
<p><img src="https://arxiv.org/html/2510.17439v2/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：FALCON框架概述。模型由三部分组成：2D VLM（如Kosmos-2）处理视觉观测和语言指令，输出语义动作令牌；具身空间模型编码第三视角图像及可选的几何输入（深度、位姿），生成空间令牌；空间增强动作头融合两类令牌，最终生成机器人动作序列。</p>
</blockquote>
<p><strong>整体流程</strong>：在每一时间步t，给定图像观测（如全局场景的第三视角图像和/或腕部相机图像）和语言指令L，2D VLM将其令牌化并处理，输出一个代表语义动作上下文的隐藏状态。同时，具身空间模型处理第三视角图像，并可选地融合深度图和相机位姿，输出一组编码了全局3D几何先验的空间令牌。最后，空间增强动作头将语义动作令牌与空间令牌融合，并通过动作预测器生成动作序列。</p>
<p><strong>核心模块1：具身空间模型</strong>：该模块基于VGGT等空间基础模型构建，旨在生成富含空间信息的令牌。其核心是一个空间编码器，它将由DINO生成的图像视觉令牌与一个可学习的相机令牌连接后，通过交叉注意力和自注意力块进行处理，输出空间令牌和精炼的相机令牌。为了提升模态可迁移性，模型设计了可选的3D条件注入机制：通过MLP编码器将相机位姿编码为GT相机令牌；通过卷积深度编码器将归一化的深度图及其有效掩码编码为深度令牌。在训练时采用随机条件策略，以一定概率选择注入深度和/或位姿条件，使模型既能利用高质量3D输入，又能在仅有RGB时保持强大的空间推理能力。</p>
<p><strong>核心模块2：空间增强动作头</strong>：该模块负责将几何表征与语义特征融合。首先，通过最大池化将空间令牌压缩为一个统一向量，并经由一个轻量级MLP适配器投影至VLM的特征空间，得到对齐后的空间特征。然后，该空间特征与VLM输出的语义动作令牌通过高效的逐元素相加进行融合。融合后的特征向量被送入动作预测器（可以是MLP或用于长时序任务的LSTM网络）以生成最终的动作序列。</p>
<p><strong>创新点</strong>：与现有方法相比，FALCON的创新具体体现在：1) <strong>空间令牌的来源</strong>：利用空间基础模型从RGB中提取强几何先验，而非弱嵌入或伪深度。2) <strong>模态可迁移性设计</strong>：通过具身空间模型中的随机条件策略，实现了对RGB、RGB-D、带位姿等多种输入模式的灵活支持，无需重新训练或改变架构。3) <strong>解耦的对齐方式</strong>：将空间信息注入专门的动作头，而非VLM主干，避免了破坏预训练的视觉-语言对齐，更符合“大脑”（VLM处理语义）与“小脑”（动作头处理精细空间控制）的功能分工理念。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真评估中，使用了CALVIN和SimplerEnv（包含WidowX机器人和Google机器人设置）基准。在真实世界评估中，设计了涵盖11项任务的综合测试，包括基础任务、空间能力评估和少样本适应。</p>
<p><strong>对比方法</strong>：对比了包括RT-1、RT-2-X、OpenVLA、RoboVLM、SpatialVLA、Octo-Base、GR-1、UP-VLA等在内的多种先进通用机器人策略和VLA模型。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>CALVIN基准</strong>：在长序列任务完成率指标上，FALCON在ABC→D和ABCD→D两种设置下均达到SOTA。例如，在零样本的ABC→D设置中，FALCON完成了75.5%的5任务连续序列，平均序列长度达4.40，优于依赖真实点云的3D Diffuser Actor（41.2%，3.35）等方法。</li>
<li><strong>SimplerEnv基准</strong>：在WidowX任务上，FALCON平均成功率56.3%，显著优于SpatialVLA的42.7%。在Google机器人任务上，FALCON平均成功率62.9%，其中在极具挑战性的“打开顶层抽屉并放置苹果”任务上达到41.7%的成功率，远超RT-2-X的3.7%和其他基线。</li>
<li><strong>真实世界实验</strong>：在包含杂乱场景和语义理解的9项基础任务中，FALCON相比强大的基线RoboVLM有显著提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.17439v2/x3.png" alt="基础任务在杂乱场景中的评估"></p>
<blockquote>
<p><strong>图3</strong>：在杂乱场景基础任务上的评估结果。FALCON在语言 grounding（处理随机干扰物）和语义理解（未见过的物体姿态）任务上均优于基线RoboVLM，展示了更强的鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.17439v2/x4.png" alt="消融研究：融合策略与组件贡献"></p>
<blockquote>
<p><strong>图4</strong>：不同特征融合策略的消融研究。逐元素相加（Addition）在性能和参数效率上均优于连接（Concatenation）或交叉注意力（Cross-attention）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.17439v2/x5.png" alt="消融研究：训练策略与空间提示"></p>
<blockquote>
<p><strong>图5</strong>：两阶段训练策略的消融研究（左），以及模型在空间提示条件化任务上的性能（右）。两阶段训练至关重要，而FALCON在理解“顶部”、“左侧”等抽象空间指令方面表现优异。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.17439v2/x6.png" alt="真实世界空间能力评估"></p>
<blockquote>
<p><strong>图6</strong>：真实世界空间能力评估。FALCON在需要处理物体高度变化、尺度变化和复杂空间关系的任务中，成功率和鲁棒性均大幅领先于基线RoboVLM。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>融合策略</strong>：逐元素相加在保持高性能的同时参数效率最高。</li>
<li><strong>训练策略</strong>：两阶段训练（先冻结VLM对齐适配器，再联合微调VLM和适配器）对最终性能至关重要。</li>
<li><strong>空间令牌与3D条件</strong>：使用空间基础模型令牌比可学习嵌入带来显著提升；引入可选的深度和位姿条件能进一步提高空间任务性能。</li>
<li><strong>模型组件</strong>：完整的FALCON框架（VLM+ESM+动作头）贡献了最大的性能增益，缺少任何一部分都会导致性能下降。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>新颖的范式</strong>：提出了FALCON范式，通过向动作头注入来自空间基础模型的丰富3D空间令牌，而非干扰VLM主干，来增强VLA的空间理解能力。</li>
<li><strong>灵活的架构</strong>：设计了具身空间模型，采用随机条件策略，使模型能够灵活、可选地利用深度、位姿等额外3D模态，显著提升了模态可迁移性。</li>
<li><strong>有效的集成</strong>：开发了空间增强动作头和两阶段训练策略，成功地将强几何先验与语义理解相结合，并在多个仿真和真实世界基准上实现了SOTA性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，空间基础模型和VLM的集成会带来额外的计算开销。此外，空间基础模型本身需要大规模3D数据进行预训练。</p>
<p><strong>后续研究启示</strong>：FALCON的解耦设计（语义与空间处理分离）为构建更强大的具身AI系统提供了新思路。未来工作可以探索更高效的空间令牌提取与融合机制，以及如何将这一范式扩展到更复杂的动态场景和多机器人协作任务中。同时，如何进一步降低对大规模3D预训练数据的依赖也是一个值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型因依赖2D视觉编码器而缺乏可靠3D空间理解能力的问题，提出了FALCON新范式。其关键技术是：利用空间基础模型从RGB图像中提取几何先验，生成3D空间令牌，并通过一个独立的空间增强动作头来使用这些令牌，从而避免损害视觉-语言对齐。实验表明，该方法在三个仿真基准和十一个真实任务中达到了最先进的性能，在物体尺度、高度变化及杂乱环境下均表现出优异的稳健性和泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.17439" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>