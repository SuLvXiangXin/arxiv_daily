<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.09851" target="_blank" rel="noreferrer">2512.09851</a></span>
        <span>作者: Yixin Zhu Team</span>
        <span>日期: 2025-12-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作需要丰富的多模态感知和有效的学习框架来处理复杂的现实世界任务。结合了触觉和视觉感知的“透视皮肤”传感器具备有前景的感知能力，而现代模仿学习为策略获取提供了强大工具。然而，现有的STS设计缺乏同步的多模态感知，并且存在触觉跟踪不可靠的问题。此外，将这些丰富的多模态信号集成到基于学习的操作流程中仍然是一个开放的挑战。本文的核心思路是：提出一种能够实现同步视觉感知和鲁棒触觉信号提取的STS传感器TacThru，并构建一个利用这些多模态信号进行操作的模仿学习框架TacThru-UMI。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的系统包含两个核心部分：新型STS传感器TacThru和基于模仿学习的操作框架TacThru-UMI。</p>
<p><img src="https://arxiv.org/html/2512.09851v2/fig_hardware_v2_c" alt="TacThru硬件与集成"></p>
<blockquote>
<p><strong>图1</strong>: TacThru的制造与在TacThru-UMI中的集成。(a) 通过在透明弹性体上依次喷涂内部（黑色）和外部（白色）标记来制造关键线标记。(b-c) TacThru-UMI包括一个机器人末端执行器（左）和一个数据收集器（中），两者具有相同的本体和TacThru手指。末端执行器的手指由伺服电动缸驱动（右）。</p>
</blockquote>
<p><strong>TacThru传感器设计</strong>：TacThru通过三个设计原则实现同步触觉-视觉感知：1) <strong>完全透明的弹性体</strong>以实现清晰的视觉访问；2) <strong>持续照明</strong>以消除模式切换；3) <strong>鲁棒的关键线标记</strong>以实现可靠的触觉跟踪。其设计与标准VBTS制造流程兼容，主要区别在于弹性体材料。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>透明弹性体与持续照明</strong>：采用完全透明的弹性体和持续的LED照明（24颗LED阵列），放弃了深度感知，换取了连续的视觉感知。接触检测通过两种机制实现：接触界面处的光反射变化以及弹性体变形导致的标记偏移。</li>
<li><strong>关键线标记</strong>：为解决透明背景下传统实心标记难以检测或易受环境噪声干扰的问题，设计了由对比色同心圆组成的“关键线标记”。内圆（黑色，半径0.6mm）和外圆（白色，半径1.0mm）确保内边缘在任何背景下都作为可检测的“关键线”可见。在40mm x 40mm的弹性体上部署了64个标记，间距3.5mm。</li>
<li><strong>鲁棒高效的标记跟踪</strong>：采用卡尔曼滤波跟踪每个标记的状态（图像中的位置）。系统模型采用随机游走模型，观测模型为直接位置观测。测量获取流程包括：图像灰度化、基于阈值τ_i的二值化、斑点检测以及数据关联（将最近的斑点匹配为测量值）。为防止极端情况下的误匹配，强制执行标记运动连续性约束：如果帧间位移超过阈值τ_z，则拒绝该测量，保持状态估计不变。参数经校准和经验调优确定（σ_v=0.42像素，σ_w=0.11像素，τ_i=0.784，τ_z=10像素）。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.09851v2/fig_markers_c" alt="关键线标记设计与跟踪评估"></p>
<blockquote>
<p><strong>图2</strong>: 关键线标记设计和滤波实现了鲁棒的跟踪。(a) 在抓取瓶子过程中比较两种标记类型（关键线 vs. 实心标记）的评估设置。(b) TacThru视图比较显示，关键线标记（左）在复杂背景下仍保持 distinct，而实心标记（右）变得不可见。(c) 定量结果表明，经过滤波的关键线方法能稳定跟踪全部64个标记，同时保持高效率（6.08ms处理时间），且滤波步骤拒绝了误报。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.09851v2/x1.png" alt="时间消耗分析"></p>
<blockquote>
<p><strong>图3</strong>: TacThru在不同频率下的运行时间消耗。测试了15、30、60和120Hz，并报告了每个计算步骤的时间消耗。误差条上方的文字：迭代时间消耗（蓝色）和等效FPS（黄色）。FPS的小幅超调是由于板载振荡器实际频率和UVC相机的缓冲策略。</p>
</blockquote>
<p><strong>TacThru-UMI学习框架</strong>：该框架基于UMI和扩散策略，扩展了多模态触觉-视觉观测。数据收集器适配了UMI设计，用STS传感器替换标准手指。策略采用基于Transformer的扩散策略，学习从多模态观测到机器人动作的映射。</p>
<p><img src="https://arxiv.org/html/2512.09851v2/fig_model_c" alt="策略架构"></p>
<blockquote>
<p><strong>图4</strong>: 策略架构。多模态观测（手腕相机图像、传感器图像、标记偏移和本体感知）被编码为令牌，并用于条件化一个基于Transformer的扩散策略，该策略将高斯噪声去噪为机器人动作块。</p>
</blockquote>
<p><strong>策略输入与编码</strong>：在时间步t，观测包括手腕相机帧序列、传感器帧序列、标记偏移序列和本体感知序列。视觉观测使用DINOv2编码（手腕相机用ViT-B，TacThru帧用ViT-S）。标记偏移和本体感知使用专用的MLP编码。每种模态都接收可学习的嵌入以实现Transformer的可区分性。拼接后的令牌与位置嵌入一起，用于条件化扩散策略π_θ，该策略将高斯样本去噪为动作块。每个动作包括相对末端执行器姿态和夹持器宽度目标。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在五个不同的真实世界操作任务上评估TacThru-UMI，涵盖抓放、分类和插入场景（PickBottle, PullTissue, SortBolt, HangScissors, InsertCap）。为公平比较，在夹持器的一个手指上装备TacThru，另一个手指上装备GelSight型传感器，确保不同模态变体使用相同的训练轨迹。每个任务收集62-147次演示。</p>
<p><strong>对比基线</strong>：训练了四种策略变体进行消融和比较，所有策略均包含手腕相机和本体感知作为基础输入：</p>
<ul>
<li><strong>TT-M</strong>: TacThru图像和标记偏移。</li>
<li><strong>TT</strong>: 仅TacThru图像（标记可见但未显式跟踪提供）。</li>
<li><strong>GS-M</strong>: GelSight图像（经空闲图像校正以隔离接触）和标记偏移（触觉基线）。</li>
<li><strong>Wrist</strong>: 仅手腕相机（视觉基线）。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.09851v2/fig_task_settings_c" alt="任务设置"></p>
<blockquote>
<p><strong>图5</strong>: 五个操作场景的任务演示。(a) PickBottle: 基本抓放，(b) PullTissue: 薄软物体操作，(c) SortBolt: 视觉辨别，(d) HangScissors: 触觉辨别，(e) InsertCap: 多模态融合。顶部：初始物体配置。中部：对应的手腕相机视图。底部：对应的TacThru和GelSight图像。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.09851v2/x2.png" alt="定量结果"></p>
<blockquote>
<p><strong>图6</strong>: 跨操作任务和感知模态的定量结果。TT-M: 带标记的TacThru，TT: 仅TacThru图像，GS-M: 带标记的GelSight，Wrist: 仅视觉。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>整体性能</strong>：TacThru-UMI（TT-M）在五个任务上的平均成功率达到<strong>85.5%<strong>，显著优于触觉基线GS-M（</strong>66.3%<strong>）和视觉基线Wrist（</strong>55.4%<strong>），性能分别是后两者的</strong>1.29倍</strong>和<strong>1.54倍</strong>。</li>
<li><strong>任务分析</strong>：<ul>
<li><strong>PickBottle</strong>：所有策略均达到≥95%的成功率，验证了系统集成多模态信号而不损害基本操作的能力。</li>
<li><strong>PullTissue</strong>：TT-M策略成功率为**85%<strong>，而GS-M和Wrist策略分别仅为</strong>15%<strong>和</strong>10%**。这凸显了TacThru通过指尖直接视觉观察处理薄软物体的优势。</li>
<li><strong>SortBolt</strong>：TT-M策略成功率为**87.5%<strong>，显著高于GS-M（</strong>54.2%<strong>）和Wrist（</strong>20.8%**）。TacThru的同步感知能够区分颜色和形状，而触觉无法区分颜色，全局视觉无法分辨小螺栓细节。</li>
<li><strong>HangScissors</strong>：TT-M和GS-M策略成功率相近（<strong>95%</strong> vs. **90%<strong>），而Wrist策略仅为</strong>65%**。这表明在需要触觉反馈确认悬挂成功的任务中，触觉信息至关重要。</li>
<li><strong>InsertCap</strong>：TT-M策略成功率达到**100%<strong>，而GS-M和Wrist策略分别为</strong>40%<strong>和</strong>45%**。TacThru的视觉感知能力实现了对瓶盖和安装座的直接视觉伺服对齐。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2512.09851v2/fig_res_c" alt="定性策略执行过程"></p>
<blockquote>
<p><strong>图7</strong>: 代表性的策略执行过程。每列(a-f)显示了一个任务执行的时间进程（从上到下），并配有同步视图：第三人称视图（左）、手腕相机（中）、以及叠加了标记偏移的TacThru视图（右；放大4倍以增强可见性）。标注突出了关键操作阶段和感知反馈。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ol>
<li><strong>标记与滤波的重要性</strong>（图2c）：关键线标记设计（Keyline）相比实心标记（Solid）大幅提升了可检测性。进一步的卡尔曼滤波（Keyline, filtered）有效拒绝了环境噪声引起的误报，实现了全部64个标记的稳定跟踪，且处理延迟仅6.08ms。</li>
<li><strong>多模态输入的有效性</strong>（图6）：对比TT-M和TT策略，在PullTissue、SortBolt和InsertCap任务中，显式提供标记偏移（TT-M）比仅提供图像（TT）带来了显著的性能提升，表明<strong>同时利用图像外观信息和触觉的几何/力信息</strong>对于精细操作至关重要。</li>
<li><strong>同步感知的优势</strong>：在多个任务中，TT-M策略的表现全面优于仅依赖触觉（GS-M）或仅依赖视觉（Wrist）的基线，证明了<strong>触觉与视觉信号的同步提供</strong>能够互补各自的局限性，实现更鲁棒和精确的操作。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>TacThru</strong>，一种新颖的STS传感器，通过完全透明弹性体、持续照明和鲁棒的关键线标记设计，实现了高效、鲁棒的<strong>同步触觉-视觉感知</strong>，无需模式切换。</li>
<li>开发了<strong>TacThru-UMI</strong>，一个将TacThru集成到基于UMI的模仿学习框架中的系统，利用基于Transformer的扩散策略动态关注同步的多模态信号，实现了数据驱动的高性能操作策略。</li>
<li>通过<strong>全面的实验验证</strong>，在五个具有挑战性的真实世界任务上证明了TacThru的同步多模态感知能够实现优于传统单模态方法的精细化和接触丰富的操作，平均成功率显著提升。</li>
</ol>
<p><strong>局限性</strong>：TacThru的设计明确优先考虑全局接触状态和指尖视觉上下文，而非精细的表面几何重建，因此<strong>牺牲了深度感知能力</strong>。论文认为这种权衡对于实现鲁棒的接触丰富操作是值得的。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>本文展示了将物理传感器创新（同步感知）与先进学习框架（扩散策略）紧密结合的潜力，为未来机器人感知-学习一体化研究提供了范例。</li>
<li>关键线标记和高效跟踪算法为解决透明弹性体背景下触觉标记的鲁棒检测问题提供了新思路。</li>
<li>实验表明，在多模态学习中，如何有效编码和融合不同模态（如图像外观与标记偏移的几何信息）对性能有重要影响，这值得进一步探索。</li>
<li>该系统在薄软物体操作、需要融合视觉与触觉信息的辨别任务以及精密插入任务中表现突出，为这些特定难点场景提供了有效的解决方案。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中多模态感知与学习框架融合的挑战，提出了一种同步触觉-视觉感知系统。核心问题是现有透皮（STS）传感器无法实现同步多模态感知且触觉跟踪不可靠。作者提出了TacThru传感器（采用全透明弹性体与关键线标记）和TacThru-UMI模仿学习框架（基于Transformer的扩散策略）。在五项真实世界任务实验中，该系统平均成功率高达85.5%，显著优于仅触觉（66.3%）和仅视觉（55.4%）的基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.09851" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>