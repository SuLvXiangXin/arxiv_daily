<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.05808" target="_blank" rel="noreferrer">2506.05808</a></span>
        <span>作者: Hiroshi Bito Team</span>
        <span>日期: 2025-06-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习旨在通过人类演示数据训练机器人策略，但获得泛化性强的策略通常需要大量数据，成本高昂。一个潜在的解决方案是借助人类演示者强大的泛化认知与决策能力，特别是从他们的凝视行为中提取任务相关线索。然而，模仿学习中的数据收集通常依赖于模仿机器人本体（如机械臂、夹爪）或视觉条件（如头戴显示器提供的机器人视角）的演示设备，以缩小策略学习的领域差距。这引发了一个关键问题：这些模仿机器人条件的设备是否会改变人类自然的凝视行为，从而影响任务相关线索的提取？本文针对这一具体痛点，提出了一个系统性的实验分析框架，旨在探究不同演示设备对演示者凝视行为及任务相关线索提取能力的影响。本文核心思路是：通过对比从捕捉自然人类行为到模仿机器人条件的多种设备，发现越自然的设备越能帮助演示者通过凝视有效提取任务线索，而这些线索能显著提升模仿学习策略在环境变化下的鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个实验框架，旨在系统分析演示者在一系列演示设备上的凝视行为。框架围绕两个研究问题展开：1) 本体差异如何影响基于凝视的任务相关线索提取？2) 视觉条件差异如何影响基于凝视的任务相关线索提取？</p>
<p><img src="https://arxiv.org/html/2506.05808v1/x2.png" alt="实验框架总览"></p>
<blockquote>
<p><strong>图2</strong>：提出的实验框架示意图。(a) 本体实验中的Wearable、UMI和Leader设置。(b) 本体实验中的Leader-Follower设置。(c) 视觉条件实验中的Wearable、HMD-Ego和HMD-Top-down设置。(d) 实验中使用的物体。</p>
</blockquote>
<p><strong>整体框架与实验设计</strong>：实验选取了模仿学习中的三类代表性设备条件：(A) 用于收集自我中心视频的可穿戴相机（Wearable），作为捕捉自然人类行为的基线；(B) 模仿本体的设备，如通用操作接口（UMI）、仅领导者（Leader）和领导者-跟随者（Leader-Follower）；(C) 模仿视觉条件的设备，如提供自我中心视角（HMD-Ego）或机器人俯视视角（HMD-Top-down）的头戴显示器（HMD）。实验任务为拾取-放置，要求参与者用叉子或勺子放入杯子或刀架上。该任务需要精确感知手-物体和物体-物体空间关系，并能通过设计环境揭示由本体差异引起的凝视行为变化。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>指标定义</strong>：基于认知科学中凝视支持对象定位和引导手部运动等监控功能的见解，本文将“任务相关线索提取能力”量化为：在拾取阶段凝视目标物体，在放置阶段凝视目标位置的能力。具体计算二维图像平面上两个欧氏距离：(1) 凝视点与目标/目的地物体的距离；(2) 凝视点与末端执行器（演示者手部或机器人夹爪）的距离。通过比较每个试次中这些距离的平均值，判断参与者是更关注任务相关线索还是末端执行器。最终能力量化为能够凝视任务相关线索的试次数量。</li>
<li><strong>指令设计</strong>：包含两种条件。首先是仅提供高级任务指令；其次是额外提供凝视相关指令，明确要求参与者在拾取阶段看目标，放置阶段看目的地，以评估简单语言指令是否能帮助对齐凝视与任务线索。</li>
<li><strong>约束分析</strong>：论文详细列出了不同设备施加的约束。对于本体设备（表1），约束包括末端执行器偏移、低自由度夹爪、低自由度手臂、控制延迟、远距离观察和缺乏触觉反馈，这些约束从Wearable到Leader-Follower逐步增强。对于视觉条件设备（表2），差异主要体现在是否使用HMD设备以及视角（自我中心vs.俯视）。</li>
</ol>
<p><strong>创新点</strong>：与现有工作相比，本文的创新在于首次系统性地分析了模仿学习中不同演示设备对演示者凝视行为的影响。先前认知科学研究主要关注自然、无约束的人类场景，而模仿学习研究要么完全忽略凝视行为，要么未深入分析其特性。本文的实验框架填补了这一空白，直接探究了设备约束如何影响任务相关线索的提取。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验使用了拾取-放置任务，物体为常见家居物品（图2d）。评估指标如上所述，并辅以NASA-TLX量表测量工作负荷。参与者被要求使用不同设备完成一系列试次。</p>
<p><strong>对比基线</strong>：实验本身是受控的人因研究，主要在不同设备条件（Wearable, UMI, Leader, Leader-Follower, HMD-Ego, HMD-Top-down）和指令条件（有/无凝视指导）之间进行对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>本体差异的影响（RQ1）</strong>：<br><img src="https://arxiv.org/html/2506.05808v1/x3.png" alt="本体设备对线索提取能力的影响（无指导）"></p>
<blockquote>
<p><strong>图3a</strong>：在没有凝视相关指令时，不同本体模仿设备对任务相关线索提取能力的影响。能力值越高，表示能有效凝视任务线索的试次越多。趋势显示，从Wearable到Leader-Follower，能力逐步下降。<br><img src="https://arxiv.org/html/2506.05808v1/x4.png" alt="本体设备对线索提取能力的影响（有指导）"><br><strong>图3b</strong>：提供凝视相关指令后，所有设备的能力普遍提升，但Wearable仍保持最高，Leader-Follower最低的排序不变。</p>
</blockquote>
<p>结果显示（图3），Wearable设备始终表现出最高的任务相关线索提取能力，其次是UMI、Leader和Leader-Follower。提供凝视相关指令后，大多数设备的能力得到改善。工作负荷分析（表3，图4）表明，设备类型对工作负荷有显著影响（p &lt; 0.001），从Wearable切换到UMI增加约10分，从UMI到Leader增加约20分，从Leader到Leader-Follower再增加约20分，与本体约束增强的直觉一致。</p>
</li>
<li><p><strong>视觉条件差异的影响（RQ2）</strong>：<br><img src="https://arxiv.org/html/2506.05808v1/x5.png" alt="视觉条件设备对线索提取能力的影响（无指导）"></p>
<blockquote>
<p><strong>图5a</strong>：在没有凝视相关指令时，Wearable和HMD-Ego能力较高，HMD-Top-down较低。表明改变为机器人俯视视角会损害线索提取。<br><img src="https://arxiv.org/html/2506.05808v1/x6.png" alt="视觉条件设备对线索提取能力的影响（有指导）"><br><strong>图5b</strong>：提供凝视相关指令后，所有设备的能力变得相似，说明指令可以弥补非自然视角带来的部分劣势。</p>
</blockquote>
<p>结果显示（图5），在没有凝视指令时，Wearable和HMD-Ego能力较高，HMD-Top-down较低；提供指令后，所有设备能力相似。工作负荷分析（表4，图6）显示，设备类型和有无凝视指令均影响显著，但设备类型影响更大。从Wearable切换到HMD-Ego增加约20分负荷，从HMD-Ego切换到HMD-Top-down再增加约30分。</p>
</li>
<li><p><strong>策略性能对比实验</strong>：<br><img src="https://arxiv.org/html/2506.05808v1/x12.png" alt="策略评估环境"></p>
<blockquote>
<p><strong>图12</strong>：策略评估的分布内（ID）和分布外（OOD）环境设置。OOD环境改变了物体位置、背景和光照，用于测试策略鲁棒性。</p>
</blockquote>
<p>为了验证从自然设备收集的凝视数据是否真能提升策略，论文进行了策略对比实验（表5）。使用从不同设备收集的凝视数据训练策略，并在分布内（ID）和分布外（OOD）环境中测试拾取和放置任务的成功率。</p>
<ul>
<li><strong>关键数值结果</strong>：使用Wearable设备凝视数据训练的策略，在OOD环境下的拾取成功率从基线（非凝视）的18.8%提升至68.8%，放置成功率从37.5%提升至81.3%。其性能与使用人工标注凝视真值（Oracle）的策略相当。</li>
<li><strong>意外发现</strong>：在ID拾取任务中，Wearable（75.0%）甚至超过了基线（43.8%），表明凝视有助于提取小而难见物体的线索。而UMI设备的凝视数据在放置任务上表现较差，揭示了将零-shot风格的凝视预测应用于策略的局限性。</li>
</ul>
</li>
</ol>
<p><strong>消融实验总结</strong>：实验本身可视为对“设备类型”和“有无凝视指导”两个核心变量的消融。结果表明：1) 设备越自然（约束越少），凝视提取任务线索的能力越强，工作负荷越低；2) 提供明确的凝视指导能普遍提升所有设备的线索提取能力，但对工作负荷无显著影响；3) 从Wearable设备获得的凝视数据对策略泛化能力提升贡献最大。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>桥梁作用</strong>：将认知科学中关于眼动的见解与模仿学习中演示设备的使用联系起来，提出了一个新颖的研究视角。</li>
<li><strong>分析框架</strong>：引入了一个系统的实验框架，用于分析不同演示设备谱系下的演示者凝视行为。</li>
<li><strong>实用指南</strong>：识别出能够有效通过凝视行为高亮任务相关线索的合适设备（如Wearable），并证明从此类设备收集的凝视数据能显著提高模仿学习策略在环境变化下的鲁棒性（OOD拾取成功率提升50个百分点）。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，将零-shot风格的凝视预测（即直接使用从一种设备/条件下收集的凝视模式）应用于策略存在局限性，例如UMI设备的凝视数据在策略测试中表现不佳，这提示需要更具泛化性的表征方法。</p>
<p><strong>后续启示</strong>：</p>
<ol>
<li><strong>架构探索</strong>：未来工作可以探索能够联合从凝视行为（以自我中心视频形式）和演示数据中学习的策略架构，例如扩展EgoMimic等研究。</li>
<li><strong>高层次应用</strong>：除了提取任务相关线索，未来研究可以利用凝视来捕捉子目标，并将此能力应用于分层策略学习。</li>
<li><strong>设备选择策略</strong>：为平衡高质量凝视线索与低领域差距演示数据，本文结果建议采用“Wearable提取凝视线索，Leader-Follower收集动作数据”的分离式数据收集策略。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究机器人模仿学习中演示设备对人类凝视行为的影响，核心问题是模拟机器人身体或视觉条件的设备如何损害演示者通过凝视提取任务相关线索的能力。作者提出一个实验框架，系统分析不同演示设备（模拟机器人身体或视觉条件）下的凝视行为。实验结果表明，这些模拟设备会损害凝视提取线索的能力，损害程度与模拟程度相关；使用捕捉自然人类行为的设备收集凝视数据，在环境变化下能将策略的任务成功率从18.8%显著提升至68.8%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.05808" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>