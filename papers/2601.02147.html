<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.02147" target="_blank" rel="noreferrer">2601.02147</a></span>
        <span>作者: Gupta, Sunny, Das, Shounak, Sethi, Amit</span>
        <span>日期: 2026/01/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉语言模型（VLMs）如CLIP展现出卓越的零样本泛化能力，但其决策常依赖于虚假相关性（如背景上下文）而非真正的因果特征，导致在分布外（OOD）场景下可靠性降低。现有提升VLM鲁棒性的方法主要分为两类：一是区域感知方法，通过微调或修改架构显式引导视觉焦点，但这些方法通常成本高昂且可能损害泛化能力；二是更轻量的测试时提示调优方法，例如测试时提示调优（TPT）和虚假特征擦除器（SEraser），它们在不修改模型权重的情况下进行适应。然而，这些方法存在关键局限：TPT依赖于“虚假特征会产生低置信度预测”的错误假设；而SEraser虽能有效擦除视觉虚假特征，但仅专注于视觉偏差，忽略了静态文本提示中存在的语言偏差（如强烈的类名先验），且其依赖的随机擦除机制可能不稳定，会无意中移除因果特征。本文针对现有方法仅处理单一模态偏差导致部分鲁棒性这一痛点，提出同时缓解视觉和文本偏差的新视角。其核心思路是提出一个双边提示优化框架（BiPrompt），通过平衡提示归一化减少文本偏差，并通过结构化注意力引导擦除抑制视觉虚假线索，从而在测试时实现更稳健的因果推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>BiPrompt是一个在测试时进行双边去偏的框架，旨在联合优化视觉和文本表示，以抑制两种模态中的虚假特征依赖。</p>
<p><img src="https://arxiv.org/html/2601.02147v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：BiPrompt框架。给定一个测试样本，框架通过联合优化视觉和文本表示进行双边去偏。结构化注意力引导擦除抑制虚假视觉线索，平衡提示归一化对齐文本嵌入。优化后的双边提示用于零样本推理，在分布偏移下产生因果且鲁棒的预测。</p>
</blockquote>
<p>整体流程如下：对于一个测试图像 (x) 和一组类别文本提示 (t_c)，BiPrompt首先计算图像的Grad-CAM注意力图 (m(x))，以区分因果（前景）和虚假（背景）区域。基于此，生成前景视图 (x_{\text{fg}} = m(x) \odot x) 和背景视图 (x_{\text{bg}} = (1 - m(x)) \odot x)。同时，对文本嵌入应用平衡提示归一化，得到各向同性的文本表示。随后，通过最小化一个综合损失函数来更新少量可学习参数（如归一化中的门控参数 (\alpha)），最终使用优化后的视觉和文本特征进行预测。</p>
<p>框架包含两个核心模块：</p>
<ol>
<li><strong>平衡提示归一化</strong>：用于缓解文本空间中的语言偏差。标准的文本嵌入 (f_t(t_c)) 往往呈现各向异性，偏向于主导或频繁出现的类别。为此，BiPrompt学习一个归一化的文本嵌入：<br>[<br>\hat{f}_t(t_c) = \alpha f_t(t_c) + (1 - \alpha)\bar{f}_t,<br>]<br>其中 (\bar{f}<em>t = \frac{1}{C}\sum</em>{c=1}^{C} f_t(t_c)) 是全局语义质心，(\alpha) 是一个可学习的门控参数。这种自适应插值鼓励文本嵌入趋向各向同性，减少了语言主导性，提升了在域偏移下的稳定性。</li>
<li><strong>结构化虚假区域擦除</strong>：用于更精准地抑制视觉虚假特征，替代SEraser中的随机擦除。利用Grad-CAM注意力图引导，构建前景和背景视图。其损失函数旨在强制前景视图与原始图像的预测一致性，同时促进背景视图与原始图像预测的正交性（即不相关性）：<br>[<br>\mathcal{L}<em>{\text{BSE}} = D</em>{\mathrm{KL}}!\big(p(y|x_{\text{fg}}),|,p(y|x)\big) - \beta,\mathrm{cos}!\big(p(y|x_{\text{bg}}), p(y|x)\big).<br>]<br>其中第一项KL散度确保模型对因果区域保持预测不变性，第二项余弦相似度（带负号）则旨在最小化虚假背景区域对预测的贡献。</li>
</ol>
<p>与现有方法相比，BiPrompt的创新点具体体现在：1) <strong>双边优化</strong>：首次在测试时适应中同时明确处理视觉和文本模态的偏差；2) <strong>结构化擦除</strong>：用注意力引导的、基于区域分离的擦除取代随机擦除，更精确地定位和抑制虚假特征，同时保护因果特征；3) <strong>文本空间正则化</strong>：引入可学习的平衡提示归一化，直接对可能携带偏差的文本嵌入进行各向同性校正。</p>
<p>整体的测试时优化目标整合了交叉熵损失 (\mathcal{L}<em>{\text{CE}})、结构化擦除损失 (\mathcal{L}</em>{\text{BSE}}) 和用于防止预测退化为均匀分布的熵正则化损失 (\mathcal{L}<em>{\text{ent}})：<br>[<br>\mathcal{L}</em>{\text{total}} = \mathcal{L}<em>{\text{CE}} + \lambda_1 \mathcal{L}</em>{\text{BSE}} + \lambda_2 \mathcal{L}_{\text{ent}}.<br>]<br>整个优化过程仅更新少量参数，高效且内存友好。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在两种设置下进行。1) <strong>真实世界OOD数据集</strong>：包括Tiny-ImageNet、CUB-200和ImageNet-A。2) <strong>模拟虚假偏差场景</strong>：使用Waterbirds基准数据集以及通过S2E协议生成的CamelDeer和SpiderCrab数据集，这些数据集明确建模了背景-对象的虚假关联。<br><strong>对比基线</strong>：包括Vanilla CLIP、TPT、RoSHOT、α-CLIP以及SEraser的多个变体（Patches, Images, Blocks）。主干网络主要为预训练的CLIP ViT-B/32。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>真实世界OOD性能（表1）</strong>：BiPrompt在三个数据集上的平均Top-1准确率达到42.4%，优于所有基线。相比最强的SEraser变体（Blocks，40.5%），提升了1.9%。在CUB-200和ImageNet-A上，BiPrompt分别取得了2.1%和2.5%的显著增益。</li>
<li><strong>模拟虚假偏差场景性能（表2）</strong>：在衡量对虚假相关性鲁棒性的关键指标——最差组准确率上，BiPrompt展现出巨大优势。在Waterbirds、CamelDeer和SpiderCrab三个数据集上，BiPrompt的最差组准确率分别达到66.6%、92.8%和95.4%，平均比Vanilla CLIP提升了35.5个百分点，且全面超越了仅处理视觉偏差的SEraser。</li>
<li><strong>不同模型架构的泛化性（表3）</strong>：在CLIP ViT-L/14和BLIP-2模型上于Waterbirds数据集进行测试。BiPrompt在CLIP-L/14上取得了平均准确率88.4%和最差组准确率60.1%的最佳结果；在BLIP-2上也显示出稳定的提升趋势，证明了方法的灵活性和泛化能力。</li>
</ul>
<p><strong>消融实验</strong>：论文通过替换或移除核心组件进行了消融研究。结果表明，同时使用平衡提示归一化和结构化擦除能带来最佳性能。仅使用结构化擦除（即仅处理视觉偏差）的性能优于仅使用平衡归一化，但二者结合能实现进一步的显著提升，特别是在最差组准确率上，这验证了双边去偏策略的必要性和有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了首个在测试时适应中同时处理视觉和文本偏差的双边优化框架BiPrompt；2) 设计了平衡提示归一化，通过可学习的重新中心化机制减少文本嵌入的各向异性，缓解语言偏差；3) 引入了结构化注意力引导擦除，利用Grad-CAM更精准地分离并抑制虚假视觉特征，同时强制因果区域的一致性。<br>论文自身提到的局限性可能包括对Grad-CAM生成的注意力图质量的依赖，以及文本归一化过程中可能平滑掉部分有用的类间判别信息。对后续研究的启示在于：探索更鲁棒的视觉-文本解缠方法以进一步降低对额外注意力机制的依赖；将双边去偏框架扩展到图像描述、视觉问答等多模态任务；研究更高效或无需梯度更新的测试时适应策略，以提升实用性和部署便捷性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型（如CLIP）在分布外场景中，因同时依赖视觉背景和文本先验等虚假相关性而导致性能下降的问题，提出双边提示优化框架BiPrompt。其关键技术包括：视觉侧采用**结构化注意力引导擦除**，利用注意力图分离并抑制虚假区域；文本侧引入**平衡提示归一化**，学习各向同性的语义空间以对齐类别嵌入。实验表明，该方法在多个偏差基准测试上，其平均与最差组准确率均优于现有测试时去偏方法，为实现轻量且可靠的模型适应提供了有效路径。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.02147" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>