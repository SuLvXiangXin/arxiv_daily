<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Dexbotic: Open-Source Vision-Language-Action Toolbox - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Dexbotic: Open-Source Vision-Language-Action Toolbox</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.23511" target="_blank" rel="noreferrer">2510.23511</a></span>
        <span>作者: Xie, Bin, Zhou, Erjin, Jia, Fan, Shi, Hao, Fan, Haoqiang, Zhang, Haowei, Li, Hebei, Sun, Jianjian, Bin, Jie, Huang, Junwen, Liu, Kai, Liu, Kaixin, Gu, Kefan, Sun, Lin, Zhang, Meng, Han, Peilong, Hao, Ruitao, Zhang, Ruitao, Huang, Saike, Xie, Songhan, Wang, Tiancai, Liu, Tianle, Tang, Wenbin, Zhu, Wenqi, Chen, Yang, Liu, Yingfei, Zhou, Yizhuang, Liu, Yu, Zhao, Yucheng, Ma, Yunchao, Wei, Yunfei, Chen, Yuxiang, Chen, Ze, Li, Zeming, Wu, Zhao, Zhang, Ziheng, Liu, Ziming, Yan, Ziwei, Zhang, Ziyu</span>
        <span>日期: 2025/10/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>具身智能领域的视觉-语言-动作模型发展迅速，但研究现状存在显著的碎片化问题。不同机构采用不同的深度学习框架和模型架构，导致用户在比较不同策略时必须配置多个实验环境和数据格式，过程繁琐。此外，许多现有的VLA模型基于过时的视觉语言模型构建，使得用户难以受益于最新的先进VLM。更关键的是，确保每个被比较的策略都得到充分优化非常困难，导致了不公平的比较。</p>
<p>本文针对这些挑战，提出了Dexbotic开源工具箱，旨在为具身智能领域的研究者提供一个一站式的VLA研究服务。其核心思路是通过统一模块化的VLA框架来标准化模型架构，并提供基于最新基础模型的强大预训练模型，同时采用以实验为中心的开发模式，实现VLA策略的快速开发、公平比较与性能提升。</p>
<h2 id="方法详解">方法详解</h2>
<p>Dexbotic工具箱的整体架构分为三个核心层次：数据层、模型层和实验层，共同为VLA模型的训练和服务提供完整解决方案。</p>
<p><img src="https://arxiv.org/html/2510.23511v1/x2.png" alt="整体架构"></p>
<blockquote>
<p><strong>图2</strong>：Dexbotic的整体架构。该框架分为数据、模型和实验三个核心层，共同协作，为VLA模型的训练和服务提供完整解决方案。</p>
</blockquote>
<p><strong>1. 数据层：统一高效的Dexdata格式</strong><br>为了统一不同数据源并节省存储空间，Dexbotic设计了Dexdata格式。该格式主要包含两个部分：存储mp4格式视频文件的<code>video</code>目录，以及存储对应jsonl文件的<code>jsonl</code>目录。每个jsonl文件对应一个机器人操作片段，其中每一行数据包含了多视角图像、机器人状态和文本指令的详细信息。</p>
<p><img src="https://arxiv.org/html/2510.23511v1/x3.png" alt="数据格式"></p>
<blockquote>
<p><strong>图3</strong>：Dexdata格式概览。(a) 数据集目录结构。(b) jsonl文件中一行的示例，详细说明了多视图图像、机器人状态和文本提示。</p>
</blockquote>
<p><strong>2. 模型层：模块化的VLA框架与强大预训练模型</strong><br>模型层是构建VLA策略的核心。Dexbotic首先提出了一个统一的理解：现有VLA策略均可分解为视觉语言模型和动作专家两部分。VLM部分通常包含视觉编码器（如CLIP）、投影器（如两层MLP）和大语言模型（LLM）。动作专家则接收来自VLM的表示（如多模态token或认知token），并生成连续的动作序列。</p>
<p>基于此，Dexbotic构建了自己的基础VLM——DexboticVLM。它采用CLIP作为视觉编码器，两层MLP作为投影器，并集成了最新的Qwen2.5作为LLM。其训练遵循两阶段流程：先冻结视觉编码器和LLM，仅训练投影器进行跨模态对齐；随后解冻并更新整个网络的参数。训练数据结合了LLaVA和Cambrian的数据集。</p>
<p>Dexbotic提供了两类预训练模型以满足不同需求：</p>
<ul>
<li>**离散预训练模型 (Dexbotic-Base)**：在DexboticVLM基础上，进一步在单臂机器人数据（包括Open-X Embodiment子集、多个仿真器数据及部分真实机器人数据）上进行预训练。该模型将连续动作离散化为256个区间，并预测对应的离散token，可直接用于初始化任何基于VLM的离散或连续动作策略的VLM部分。</li>
<li><strong>连续预训练模型</strong>：在Dexbotic-Base基础上，针对特定连续动作策略（如CogACT）进行全模型（VLM+动作专家）的端到端预训练。论文提供了<strong>单臂连续模型 (Dexbotic-CogACT)</strong> 和<strong>双臂混合模型</strong>。后者通过修改噪声token数量（从7增至16）以支持双臂任务，并引入了额外的双臂数据集（如Robomind、AgiBot World及私有ALOHA数据）进行训练。为支持多视角输入，多个视角共享同一个视觉编码器，其视觉token被处理后拼接输入LLM。</li>
</ul>
<p><strong>3. 实验层：以实验为中心的快速开发框架</strong><br>实验层是Dexbotic工具箱最重要的部分，它采用“分层配置+工厂注册+入口调度”的方法，实现快速、灵活的实验开发。</p>
<p><img src="https://arxiv.org/html/2510.23511v1/x4.png" alt="实验配置架构"></p>
<blockquote>
<p><strong>图4</strong>：实验层中的分层配置架构。每个实验类包含训练器、数据、优化器、模型和推理的配置。</p>
</blockquote>
<p>Dexbotic首先构建了一个<code>base_exp</code>脚本作为所有VLA策略的基础配置。要开发现有策略（如CogACT）的实验脚本，只需继承<code>base_exp</code>并修改目标策略对应的配置即可。用户要运行新实验，只需修改这些配置设置，无需复制整个文件。对于自定义策略，用户则需要继承并修改配置类和模型类。一旦实验脚本创建完成，用户可以通过命令（如<code>python xxx_exp.py -task train</code>）直接运行训练或推理任务。</p>
<p><strong>训练与推理流程</strong>：<br>训练时，系统接收观察图像、文本指令和机器人状态作为输入。视觉编码器和文本编码器分别处理图像和文本，生成token并经投影器对齐后，拼接输入LLM。对于离散表示策略，LLM输出的token可直接解码为动作；对于连续表示策略，则需要附加的动作专家（如扩散Transformer）来生成连续值动作序列，并通过相应的动作损失进行监督。</p>
<p><img src="https://arxiv.org/html/2510.23511v1/x5.png" alt="训练流程"></p>
<blockquote>
<p><strong>图5</strong>：Dexbotic的训练流程。展示了从多模态输入到动作生成的完整数据流。</p>
</blockquote>
<p>推理时，Dexbotic提供了基于Web API的服务。客户端发送请求，服务器端接收并处理数据，通过VLA模型推理生成连续动作序列，最后将结果返回给客户端执行。</p>
<p><img src="https://arxiv.org/html/2510.23511v1/x6.png" alt="推理服务"></p>
<blockquote>
<p><strong>图6</strong>：Dexbotic的推理服务架构。展示了从客户端请求到模型推理并返回动作的完整服务流程。</p>
</blockquote>
<p><strong>创新点</strong>：Dexbotic的主要创新在于其系统级设计：1) 提出统一的Dexdata格式和模块化VLA框架，解决了数据与模型架构的碎片化问题；2) 基于最新基础模型构建了强大的预训练模型（DexboticVLM及后续模型），显著提升了现有策略的性能上限；3) 设计了高度灵活、以实验为中心的分层配置开发框架，极大降低了VLA研究的开发与比较门槛。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>为验证所提供预训练模型的有效性，论文在多个主流仿真基准上进行了广泛的实验，包括：SimplerEnv、CALVIN、ManiSkill2、RoboTwin2.0和LIBERO。对比的基线方法是各基准上原始的开源VLA策略，如CogACT、OpenVLA-OFT和MemoryVLA。Dexbotic版本的方法在名称前冠以“DB-”（如DB-CogACT）。</p>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>SimplerEnv（WidowX机器人）</strong>：DB-CogACT相比原始CogACT绝对提升18.2%（平均成功率51.3% → 69.5%）。DB-OFT相比原始OFT提升高达46.2%（30.2% → 76.4%）。DB-MemoryVLA也取得了12.5%的绝对提升（71.9% → 84.4%）。</li>
<li><strong>CALVIN（长视野任务）</strong>：在ABC→D泛化设置下，DB-CogACT的平均连续完成任务数达到4.06，优于原始CogACT的3.25。DB-OFT也略优于原始版本。</li>
<li><strong>RoboTwin2.0（双臂任务）</strong>：在四个选定任务上，DB-CogACT的平均成功率为58.5%，相比原始CogACT的43.75%提升了14.75个百分点，证明了预训练模型在双臂具身下的有效性。</li>
<li><strong>LIBERO</strong>：由于基线性能已接近饱和，提升幅度较小。DB-CogACT和DB-MemoryVLA分别带来了1.3%和0.3%的平均成功率提升。</li>
<li><strong>ManiSkill2（基础抓放）</strong>：DB-OFT表现尤为突出，将原始OFT极低的21%平均成功率大幅提升至63%（+42%）。DB-CogACT也将原始版本的40%提升至58%（+18%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.23511v1/x9.png" alt="仿真结果对比表1"></p>
<blockquote>
<p><strong>表1</strong>：在SimplerEnv-Bridge基准上与WidowX机器人的性能对比。Dexbotic版本的所有策略均显著优于原始开源版本。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.23511v1/x10.png" alt="仿真结果对比表2-5"></p>
<blockquote>
<p><strong>表2-5</strong>：在RoboTwin2.0、LIBERO、CALVIN和ManiSkill2基准上的性能对比。数据表明，Dexbotic提供的预训练模型在不同类型任务和基准上均能带来一致且显著的性能提升。</p>
</blockquote>
<p><strong>真实世界性能</strong>：<br>论文展示了使用Dexbotic工具箱在UR5e、ALOHA、ARX5和Franka等多种真实机器人上完成日常任务的视频集锦。经过基于预训练模型的微调，系统能够完成诸如“摆放盘子”、“搜寻绿盒”等任务，成功率分别达到100%和80%。然而，论文也指出，对于一些精细操作任务（如“撕碎废纸”、“将薯条倒入盘子”），现有的VLA策略仍面临挑战。此外，像MemoryVLA这样的先进策略能够解决需要长视野和记忆的任务，例如“顺序按下按钮”。</p>
<p><strong>Real2Sim评估</strong>：<br>为降低真实世界评估的成本，Dexbotic提出了开源数字孪生系统DOS-Twins。它通过3D高斯溅射等技术，在视觉、运动和交互三个维度上确保仿真与真实环境的高度一致性，从而允许用户在仿真中进行可靠的模型能力评估。</p>
<p><img src="https://arxiv.org/html/2510.23511v1/x7.png" alt="真实与仿真对比"></p>
<blockquote>
<p><strong>图7</strong>：真实世界与仿真渲染的对比，突出了机器臂和物体在执行相同动作时在两个环境中的高度一致性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.23511v1/x8.png" alt="真实任务视频集锦"></p>
<blockquote>
<p><strong>图8</strong>：由Dexbotic工具箱生成的（真实世界）任务视频集锦。展示了在不同机器人上完成的各种日常操作任务。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>统一的开源工具箱</strong>：提出了首个旨在统一碎片化VLA研究生态的工具箱Dexbotic，通过标准化的数据格式（Dexdata）、模块化的模型框架（VLM + Action Expert）以及实验中心化的开发模式，极大地降低了VLA研究的入门、开发和比较门槛。</li>
<li><strong>强大的预训练基础模型</strong>：发布了基于最新LLM（Qwen2.5）构建的DexboticVLM及一系列针对不同场景（离散/连续、单臂/双臂）的预训练模型。实验证明，这些模型能显著提升现有主流VLA策略在各种基准上的性能，为实现更公平和更高水平的比较提供了基础。</li>
<li><strong>推动仿真与真实评估</strong>：不仅提供了高效的训练推理框架，还通过DOS-Twins数字孪生系统倡导Real2Sim2Real的评估范式，为解决真实世界机器人评估成本高昂的难题提供了可行路径。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，尽管性能大幅提升，但对于一些需要<strong>精细操作</strong>的真实世界任务（如撕纸、倒薯条），现有的VLA策略仍然面临挑战，成功率有待提高。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>促进公平比较与迭代</strong>：Dexbotic为社区提供了一个共同的起跑线和测试平台，有助于研究者更公平地比较算法核心创新，并基于强大的基础模型进行快速迭代。</li>
<li><strong>降低真实机器人研究门槛</strong>：通过提供预训练模型、统一部署脚本和Real2Sim评估工具，Dexbotic有望吸引更多研究者投身于真实机器人VLA算法的开发与测试。</li>
<li><strong>开辟系统优化方向</strong>：该工作表明，在算法层面之外，构建一个良好的、支持快速迭代的研究基础设施（包括数据管道、模型框架、实验管理）本身就能对领域发展产生巨大的推动作用。未来可能有更多工作关注于优化VLA研究的全流程工具链。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身智能领域Vision-Language-Action（VLA）模型研究分散、环境配置繁琐且模型基于过时视觉语言模型的问题，提出了开源工具箱Dexbotic。关键技术包括：统一模型架构为视觉语言模型（含视觉编码器、投影器和LLM）与动作专家两部分，引入Dexdata格式标准化数据，并提供支持多策略的代码库，用户可通过修改Exp脚本快速开发实验。通过集成更强预训练模型，该工具箱显著提升了现有VLA策略的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.23511" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>