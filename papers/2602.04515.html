<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.04515" target="_blank" rel="noreferrer">2602.04515</a></span>
        <span>作者: Bai, Yu, Yu, MingMing, Li, Chaojie, Bai, Ziyi, Wang, Xinlong, Karlsson, Börje F.</span>
        <span>日期: 2026/02/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>部署类人机器人于真实世界环境面临根本性挑战，这要求在不完整信息观察和动态变化的环境下，紧密集成感知、移动和操作能力，并能在不同类型的子任务间稳健过渡。当前主流方法，如SayCan，通过学习的可供性将语言指令分解为可执行技能，但通常依赖于预定义的技能库，因此不太适合具有复杂和多样化具身的类人机器人。同时，现有方法往往只专注于单一任务类型（如导航或操作），缺乏对移动、头部朝向、手部操作、人机交互等全身行为的协调规划能力。</p>
<p>本文针对上述痛点，提出了一个新的任务——EgoActing（自我行动），该任务要求模型具备强大的空间理解能力，能够直接将高级、可操作的指令基于自我中心观察、动作历史及可用技能，转化为可执行的、空间感知的类人机器人动作序列。为了实例化此任务，本文提出了EgoActor，一个统一的、可扩展的视觉语言模型（VLM）。其核心思路是：利用VLM的推理能力，并增强其空间理解，以直接预测范围广泛的、以语言形式表述的低层动作，从而桥接抽象任务规划与具体运动执行。</p>
<h2 id="方法详解">方法详解</h2>
<p>EgoActor的整体框架是一个基于Transformer架构的视觉语言模型，其输入包括自然语言任务指令 $I$、历史自我中心RGB观察序列 $O_{1:t}$、历史动作序列 $a_{1:t-1}$ 以及可用低层策略集合 $\Pi$，输出是下一步要执行的动作 $a_t$。该模型旨在联合预测移动、主动感知、操作和人机交互动作，以实现协调和精确的执行。</p>
<p><img src="https://arxiv.org/html/2602.04515v1/intro.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：EgoActor概述。该模型通过联合预测移动、主动感知、操作和人机交互动作来控制类人机器人，以实现协调和精确的执行，使类人机器人能够执行自然语言描述的长视野多步骤任务指令。</p>
</blockquote>
<p>模型的核心创新在于其<strong>语言化的动作表示</strong>，分为两类：</p>
<ol>
<li><strong>结构化语言动作（SLAs）</strong>：用于移动和主动感知。这些动作以简洁的自然语言模板描述，指定动作类型、方向和幅度，例如“Turn left 30.5 degrees”或“Look up 10.2 degrees”。它们涵盖偏航和俯仰旋转、前后和横向线性平移以及垂直高度调整。其核心作用是基于RGB观察解释空间关系，并将机器人定位到适当位置以执行后续任务。</li>
<li><strong>自然语言动作（NLAs）</strong>：用于操作和人机交互。不限制于固定技能集，而是直接使用自然语言表示，例如“Ask Could you please guide me to a meeting room?”。这提供了超越预定义技能的泛化能力，并能有效复用低层视觉-语言-动作模型。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04515v1/procedure.png" alt="工作流程"></p>
<blockquote>
<p><strong>图2</strong>：给定任务“Approach and pick up the orange on the desk”时，EgoActor的工作流程可视化。灰色块代表结构化语言动作（SLAs），绿色块代表自然语言动作（NLAs）。</p>
</blockquote>
<p>在训练方面，模型基于Qwen3-VL架构，使用LoRA对所有线性层进行微调。关键的创新在于其<strong>多样化的数据配方</strong>：</p>
<ul>
<li><strong>互联网视频数据</strong>：主要来自EgoTaskQA数据集，处理后生成大量EgoActing训练样本。</li>
<li><strong>本地环境数据</strong>：在本地录制自我中心视频，捕捉环境布局变化。</li>
<li><strong>虚拟环境数据</strong>：从VLN-CE和Habitat-Sim模拟器中采样，提供受控的空间导航和监督。</li>
<li><strong>空间推理数据</strong>：来自MindCube数据集，增强模型的空间推理能力。</li>
<li><strong>视觉语言理解与规划数据</strong>：来自GQA、RoboVQA、EgoPlan、ALFRED等，保持视觉语言理解和高层规划能力。</li>
<li><strong>无监督移动预测数据</strong>：通过预测自我中心图像对之间的移动转换来增强空间理解。</li>
<li><strong>DAgger经验数据</strong>：通过真实世界执行收集的在线策略轨迹，用于纠偏。</li>
</ul>
<p>这种混合监督使模型能够做出稳健的、上下文感知的决策，并实现流畅的动作推断（在1秒以内）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟和真实世界环境中进行，使用了多个基准测试。</p>
<p><strong>基准与数据集</strong>：包括人机交互基准（单人与多人场景）、移动操作基准（在未见过的布局环境中接近并拾取/放置物体）、通过性基准（评估机器人在狭窄空间中安全移动而不碰撞的能力）以及虚拟环境基准（基于Habitat-Sim模拟器）。</p>
<p><strong>对比的基线方法</strong>：为了验证EgoActor在移动规划方面的优势，论文对比了三个基于视觉语言基础模型的代表性导航模型：NaVid、Uni-NaVid和NaVILA（仅使用其VLM组件）。这些模型主要专注于导航，不具备EgoActor的全身动作协调能力。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>人机交互</strong>：在单人人机交互任务中（表I），EgoActor-4B和8B在所有任务（接近、打招呼、询问位置、请求物品）上都达到了接近或达到12/12的成功率，而基线模型（NaVid, UniNaVid）仅在“接近”任务上表现尚可（8/12），且无法生成交互动作。在更具挑战性的多人物场景中（表II），EgoActor-8B在区分不同属性（服装、配饰、姿势等）的人物方面表现优异，成功率普遍高于EgoActor-4B。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04515v1/ending_action.png" alt="自然语言动作示例"></p>
<blockquote>
<p><strong>图3</strong>：EgoActing中的自然语言动作（NLA）示例。EgoActor根据获得的RGB观察结果预测相应的动作。</p>
</blockquote>
<ol start="2">
<li><p><strong>移动操作</strong>：在未见布局环境下的移动操作基准测试中（表III），对于已见物体（苹果、瓶子），EgoActor-8B在“接近并拾取”和“接近并放置”任务上几乎达到全成功（6/6）。对于未见物体（笔筒、粉色杯子），EgoActor-8B也表现出良好的泛化能力，成功率为4/6到6/6。EgoActor-4B性能稍弱。</p>
</li>
<li><p><strong>通过性</strong>：在通过性基准测试中（表IV），EgoActor-4B和8B在已见和未见环境中的“进入房间”和“离开房间”任务上均大幅领先所有基线模型。例如，在已见环境中进入右侧房间，EgoActor-8B达到12/12，而最好的基线NaVid仅为5/12。</p>
</li>
<li><p><strong>虚拟环境</strong>：在虚拟环境基准测试中（表V），EgoActor在“距离目标位置”的各个阈值成功率上均远超基线（例如，在0.5米内，EgoActor-8B为51.4%，而基线最高仅8.8%）。此外，EgoActor还能预测自然语言动作（F1分数约0.6），这是基线不具备的能力。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04515v1/obstacle_case_study.png" alt="避障泛化"></p>
<blockquote>
<p><strong>图4</strong>：当面对未见过的绳索障碍物时，我们模型的避障泛化能力多步骤示意图。展示了模型通过侧向移动和调整姿态进行避障的决策过程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.04515v1/x1.png" alt="主动感知轨迹"></p>
<blockquote>
<p><strong>图5</strong>：EgoActor主动感知轨迹的第一人称视图。彩色描述块突出了模型的行为，例如“向右看以定位目标”和“向上看以检查障碍物”。</p>
</blockquote>
<p><strong>消融实验分析</strong>：论文通过消融实验分析了不同数据源的贡献。移除所有非EgoActing数据（仅保留互联网、本地、虚拟EgoActing数据）会导致性能显著下降，尤其是在需要强空间推理的通过性任务上。单独移除空间推理数据（MindCube）或视觉语言规划数据也会对特定能力产生负面影响，证明了混合数据配方的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>EgoActing</strong>这一新任务，强调将语言指令直接转化为基于自我中心观察的可执行动作序列，突出了真实世界类人机器人部署的挑战。</li>
<li>提出了<strong>EgoActor</strong>，一个以视觉为中心的模型，通过统一移动、感知、操作和人机交互，充分利用了类人机器人的能力。其采用<strong>结构化与自然语言相结合的动作表示</strong>是关键创新。</li>
<li>在真实世界和模拟环境中进行了广泛实验验证，并开源了代码、模型、数据集和基准测试，以促进可重复性和未来研究。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，EgoActor依赖于一组预定义的低层全身控制和操作策略（$\Pi$）。此外，虽然推断延迟低于1秒，但在高度动态的环境中实现真正的实时协调仍是一个挑战。评估主要基于成功/失败计数，对动作流畅性和类人行为质量的量化评估可以进一步深入。</p>
<p><strong>对后续研究的启示</strong>：EgoActor展示了使用单一、统一的VLM来协调类人机器人多种低级动作的可行性，为构建更通用、更自主的具身智能体提供了新思路。其语言化动作表示方法为技能泛化和组合提供了灵活框架。未来工作可以探索如何进一步减少对预定义低层策略的依赖，以及如何将更复杂的动态环境理解和长期规划整合到此类框架中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出EgoActor框架，旨在解决人形机器人将高层指令直接转化为精确、空间感知动作的核心挑战。方法基于统一可扩展的视觉语言模型（VLM），通过自我中心RGB数据、空间推理问答与仿真演示进行监督训练，能够联合预测移动、主动感知、操纵及人机交互等多类动作。实验表明，该模型可实现1秒内的流畅动作推断，并在仿真与真实环境中有效桥接任务规划与运动执行，泛化至多样任务及新场景。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.04515" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>