<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing &amp; Sparsification - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing &amp; Sparsification</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.21046" target="_blank" rel="noreferrer">2508.21046</a></span>
        <span>作者: Li, Wei, Zhang, Renshan, Shao, Rui, He, Jie, Nie, Liqiang</span>
        <span>日期: 2025/08/28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于预训练视觉语言模型（VLM）构建的视觉-语言-动作（VLA）模型（如RT-2、Octo、OpenVLA、π0）在实现端到端机器人控制方面展现出潜力。然而，将这些模型与连续动作空间对齐需要进行大量后训练，导致计算开销巨大，严重限制了可扩展性和实际部署。现有的稀疏化加速策略（如混合深度、层跳跃、早期退出）主要聚焦于语言模型内部的计算优化，而忽视了视觉、语言和动作模态间的语义耦合。这种模块化的优化范式导致了跨模态语义退化，具体表现为：视觉编码器内的压缩丢弃了任务相关的细粒度特征；语言模型内的令牌跳跃破坏了指代消解所需的上下文连贯性；动作生成缺乏对多模态状态转换的因果推理。</p>
<p>本文针对现有方法导致的跨模态语义退化和高计算成本这一痛点，提出了一种受人类多模态协调机制启发的新视角。核心思路是提出一个名为CogVLA的认知对齐VLA框架，通过指令驱动的路由与稀疏化，在感知到控制的端到端流程中建立任务语义一致的联合优化机制，从而同时提升计算效率和任务性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>CogVLA的整体框架是一个受人类认知启发的三阶段渐进式架构，旨在模仿“视觉注意系统（VAS）-辅助运动区（SMA）-前运动皮层（PMC）”的协调过程。输入为视觉观测和语言指令，输出为未来K个时间步的动作块预测。其核心创新在于三个阶段：1) 编码器-FiLM聚合路由（EFA-Routing）在视觉编码器阶段压缩视觉令牌；2) LLM-FiLM剪枝路由（LFP-Routing）在语言模型阶段进一步修剪无关视觉令牌；3) V-L-A耦合注意力（CAtten）确保压缩后的多模态表征仍能支持连贯的动作生成。</p>
<p><img src="https://arxiv.org/html/2508.21046v2/main_figs/frameworkv7.png" alt="CogVLA框架总览"></p>
<blockquote>
<p><strong>图2</strong>：CogVLA框架总览。该框架采用指令驱动的路由与稀疏化策略进行高效的动作块预测，包含任务引导的视觉聚合、语义剪枝和连贯解码三个阶段，确保从感知到控制的高效跨模态表征对齐。</p>
</blockquote>
<p><strong>核心模块1：Encoder-FiLM based Aggregation Routing (EFA-Routing)</strong><br>此阶段对应VAS功能，目标是根据指令选择性聚合视觉信息，减少冗余。具体分为两步：1) <strong>编码器内聚合</strong>：使用N个视觉编码器（如SigLIP和DINOv2）提取图像令牌。每个编码器内部引入Encoder-FiLM模块，该模块利用从指令嵌入（𝐭ᵣ）生成的调制参数（尺度γ和偏移β），动态地引导图像令牌与一个可学习的聚合令牌（𝐯_agg⁽ⁱ⁾）进行交互。经过多个编码器块的迭代，最终仅保留聚合令牌，丢弃原始图像令牌，将视觉令牌数量压缩至原始的25%。2) <strong>跨编码器聚合</strong>：将来自不同编码器分支的聚合令牌进行融合。通过一个由指令条件化的路由门（一个轻量级MLP+Sigmoid）计算融合权重α，动态平衡不同编码器特征的贡献，最终得到双聚合视觉令牌𝐯_agg。</p>
<p><strong>核心模块2：LLM-FiLM based Pruning Routing (LFP-Routing)</strong><br>此阶段对应SMA功能，旨在将动作意图注入语言模型中的视觉上下文，实现指令驱动的令牌级稀疏化。在Transformer层的第l层，给定视觉令牌𝐙_l和指令𝐭_l，LFP-Routing首先通过一个轻量级MLP为每个视觉令牌计算路由权重R_lʲ。然后，根据预设的令牌保留比例β，确定一个百分位阈值P_β^l。仅保留权重高于该阈值的令牌，并对其进行由指令调制的自注意力和前馈网络处理；低于阈值的令牌则直接跳过计算。此过程显著减少了语言模型中与任务无关的视觉令牌的注意力计算负担。</p>
<p><strong>核心模块3：V-L-A Coupled Attention (CAtten)</strong><br>此阶段对应PMC功能，为确保在双重压缩的视觉输入下仍能生成语义一致且时序连贯的动作序列，CAtten引入了一种分层的混合注意力机制。它将多模态输入序列𝐗̃ = [𝐙_l, 𝐭_l, 𝐀_l]（视觉、语言、动作块）的注意力分为三个部分：1) <strong>视觉-语言因果注意力</strong>：在视觉和语言令牌之间应用因果（单向）注意力，保留指令条件下的视觉推理能力。2) <strong>动作块双向注意力</strong>：在动作块内部应用双向注意力，允许未来动作令牌之间进行全上下文交互，以支持连贯的并行解码。3) <strong>统一的混合注意力掩码</strong>：通过一个全局的混合注意力掩码𝐌_hybrid来强制执行上述分层依赖关系，该掩码确保视觉-语言段内部因果、动作段内部双向，且动作段可以关注视觉-语言段，反之则不可。</p>
<p><img src="https://arxiv.org/html/2508.21046v2/main_figs/3stagev5.png" alt="CogVLA三阶段渐进设计"></p>
<blockquote>
<p><strong>图3</strong>：CogVLA的三阶段渐进设计。EFA-Routing（阶段1）、LFP-Routing（阶段2）和CAtten（阶段3）分别对应VAS、SMA和PMC。图(c)突出了CAtten相较于先前注意力机制在结合单向与双向注意力、注入动作意图、实现并行解码和利用稀疏视觉令牌方面的优势。</p>
</blockquote>
<p>与现有方法相比，CogVLA的核心创新在于将受认知启发的跨模态联合优化思想具体化为一个可操作的三阶段架构，通过指令驱动的路由在感知和推理的早期阶段就进行语义感知的压缩，而非事后或在单一模态内进行孤立优化，从而在提升效率的同时保持了端到端的语义连贯性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：仿真实验在LIBERO基准上进行，该基准包含Spatial, Object, Goal, Long四个任务套件，每个套件10个任务，指令长且多样。真实世界实验在Cobot Agilex ALOHA平台上进行，包含物体放置、抽屉操作和T恤折叠三个长视野任务。对比的基线方法包括Diffusion Policy、Octo、OpenVLA、π0、π0.5-KI、OpenVLA-OFT、SpatialVLA、PD-VLA、STAR、Dita、CoT-VLA等。</p>
<p><strong>关键性能结果</strong>：<br>在LIBERO基准上，CogVLA取得了最高的平均成功率97.4%，在四个套件中的三个（Spatial， Object， Long）排名第一，仅在Goal套件排名第二。</p>
<p><img src="https://arxiv.org/html/2508.21046v2/main_figs/et.png" alt="仿真实验结果表"></p>
<blockquote>
<p><strong>表1</strong>：LIBERO基准上的仿真实验结果对比。CogVLA在平均成功率上达到97.4%，排名第一，展示了其卓越的指令遵循和任务完成能力。</p>
</blockquote>
<p>在真实世界任务中，CogVLA在物体放置、抽屉操作和T恤折叠任务上分别取得了70.0%的整体成功率，显著优于其他基线方法。</p>
<p><img src="https://arxiv.org/html/2508.21046v2/main_figs/red.png" alt="真实世界实验结果表"></p>
<blockquote>
<p><strong>表2</strong>：真实世界机器人任务性能对比。CogVLA在所有三个复杂的长视野任务中均取得了最高的成功率（70.0%），证明了其方法的有效性和泛化能力。</p>
</blockquote>
<p><strong>效率优化结果</strong>：<br>与OpenVLA相比，CogVLA实现了2.79倍的推理加速（0.091s vs. 0.254s）、22.54倍的吞吐量提升（87.9 Hz vs. 3.9 Hz）、3.12倍的FLOPs降低（2.72 T vs. 8.48 T）以及2.49倍的训练成本减少（4.7 h/10k steps vs. 11.7 h/10k steps）。即使在性能相近的基线（如OpenVLA-OFT）中，CogVLA也保持了显著的效率优势。</p>
<p><img src="https://arxiv.org/html/2508.21046v2/main_figs/purple.png" alt="效率优化结果表"></p>
<blockquote>
<p><strong>表3</strong>：效率优化结果对比。CogVLA在保持最高性能之一的同时，实现了最低的推理延迟、最高的吞吐量、最少的计算量和最低的训练成本，验证了其路由与稀疏化策略的有效性。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>移除阶段1（EFA-Routing）或阶段2（LFP-Routing）的变体模型在推理时间、FLOPs和训练成本上均差于完整的CogVLA，证明了两个路由模块对于提升效率的互补性和必要性。</p>
<p><strong>定性结果</strong>：<br><img src="https://arxiv.org/html/2508.21046v2/main_figs/visualizationv4.jpg" alt="可视化对比图"></p>
<blockquote>
<p><strong>图4</strong>：CogVLA与OpenVLA-OFT的可视化对比。该图总结了CogVLA在仿真和真实任务上的性能优势（更高成功率）和效率优势（更低推理时间、FLOPs和训练时间），直观展示了其综合优势。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个受人类多模态协调机制启发的认知对齐VLA框架（CogVLA），其“VAS-SMA-PMC”三阶段渐进式架构为跨模态联合优化提供了新范式；2) 设计了协同工作的EFA-Routing和LFP-Routing模块，实现了指令驱动的、贯穿感知-推理流程的视觉稀疏化；3) 提出了V-L-A耦合注意力（CAtten）机制，确保在高度压缩的多模态表征下仍能保持逻辑一致性和动作连贯性。</p>
<p>论文自身提到的局限性在于，为了追求效率，CogVLA将视觉输入减少了8倍，这可能导致在LIBERO-Goal套件（需要精细的空间关系理解）上的性能略低于某些专门方法。</p>
<p>本文对后续研究的启示在于：在构建高效VLA模型时，应超越单一模态或孤立阶段的优化，更多地考虑跨模态的语义耦合与端到端的连贯性。受认知科学启发的设计思路（如选择性注意、意图注入、协调规划）为解决VLA模型的计算效率与语义保真度之间的权衡提供了富有前景的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型计算开销大、可扩展性受限的问题，提出CogVLA框架。其核心技术包括：基于指令的聚合路由与修剪路由，分别压缩视觉令牌并剪枝无关的视觉语义；以及V-L-A耦合注意力机制，增强感知到动作的连贯性。实验表明，该模型在LIBERO基准和真实机器人任务上分别达到97.4%和70.0%的成功率，同时训练成本降低2.5倍、推理延迟减少2.8倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.21046" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>