<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.10637" target="_blank" rel="noreferrer">2510.10637</a></span>
        <span>作者: Hua Zou Team</span>
        <span>日期: 2025-10-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人学习，特别是面向开放世界的通用策略学习，其扩展性受到真实世界数据收集高成本和高人力投入的根本性制约。模拟数据虽提供了可扩展的替代方案，但由于模拟环境与真实世界在视觉外观、物理属性和物体交互方面存在显著差距，往往难以泛化到现实。为应对此挑战，Real2Sim2Real (R2S2R) 范式应运而生，其核心思想是利用辐射场方法（如NeRF、3DGS）从真实场景重建高保真数字孪生，再将其置入模拟器以缩小域差距。然而，以Robo-GS为代表的现有R2S2R方法虽然实现了高视觉保真度，却在一个关键维度上存在不足：物理交互性。它们侧重于静态的视觉真实感，导致构建的世界基本是静态和非交互的，限制了其在复杂、接触丰富的操作任务中的应用。</p>
<p>本文针对现有R2S2R方法在“物理交互性”方面的关键缺陷，提出了RoboSimGS框架。其核心思路是：构建一个结合了3DGS视觉保真度与网格物体物理交互性的混合场景表示，并首次利用多模态大语言模型（MLLM）从多视角图像中自动推断物体的物理属性和运动学结构，从而将静态场景转化为动态、交互式的“沙盒”，最终生成可用于训练并实现零-shot迁移到真实世界的高保真模拟数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboSimGS是一个两阶段的管道，旨在从多视角图像生成真实且物理交互的模拟环境。第一阶段是<strong>场景重建</strong>，构建混合表示；第二阶段是<strong>模拟环境对齐</strong>，将这些组件无缝导入物理模拟器。</p>
<p><img src="https://arxiv.org/html/2510.10637v1/x1.png" alt="方法管道"></p>
<blockquote>
<p><strong>图1</strong>：RoboSimGS整体管道。从多视角图像开始，首先进行场景重建，创建包含高保真3DGS背景和交互式网格物体的混合表示。关键步骤是利用多模态大语言模型（MLLM）进行自动物理估计和关节推断。随后通过模拟环境对齐将场景与模拟器对齐。最后，应用整体场景增强生成多样化的模拟数据。在此数据上训练的策略可直接部署到真实世界。</p>
</blockquote>
<p><strong>核心模块1：场景重建</strong><br>采用解耦的重建策略：使用3DGS进行静态背景重建以实现视觉保真，使用显式网格表示交互物体以确保物理合理性。</p>
<ul>
<li><strong>背景重建</strong>：采用3D高斯泼溅（3DGS）对场景进行建模。为了在模拟器中实现精确的空间对齐，论文为每个高斯附加了一个可学习的语义特征向量。这些特征的训练受到语言驱动的监督：使用CLIP文本编码器将预定义的语义类别（如“机械臂”、“红色方块”）编码为目标文本嵌入，并通过对比损失最小化渲染出的2D特征图与对应文本嵌入之间的距离。3DGS的可微渲染器通过投影变换和泼溅算法合成颜色和特征。</li>
<li><strong>物体重建</strong>：使用ARCode自动分割并生成物体网格。为了支持复杂操作，论文引入了自动化流程，为这些静态网格赋予物理合理、可关节化的结构。这依赖于以下两个MLLM驱动的关键创新：<ul>
<li><strong>MLLM驱动的关节推断</strong>：向MLLM（如GPT-4o）提供重建网格的多视角渲染图，MLLM会识别物体类别并推断其可能的关节结构（如关节类型：平移或旋转）以及需要分离的部件语义标签（如“抽屉主体”和“主柜体”）。然后利用开放词汇分割方法（如AffordDex）根据生成的文本标签直接分割网格。最后，MLLM再次被提示以确定精确的关节参数（如轴、运动限制），用于自动定义URDF兼容的关节。</li>
<li><strong>MLLM驱动的物理估计</strong>：引入一个由GPT-4o驱动的“物理专家代理”，通过处理3D资源的四个正交视图，自动估算其基本物理参数：密度（ρ）、杨氏模量（E）和泊松比（ν）。这些参数对于高保真物理模拟至关重要。</li>
</ul>
</li>
</ul>
<p><strong>核心模块2：模拟环境对齐</strong><br>此阶段将重建的混合表示与物理模拟器对齐。</p>
<ul>
<li><strong>世界坐标系对齐</strong>：以机器人几何作为共同锚点，使用迭代最近点（ICP）算法，通过最小化3DGS重建点云与机器人URDF模型点云之间的误差，求解出最优的刚性变换矩阵，将3DGS世界坐标系与模拟器坐标系对齐。</li>
<li><strong>相机位姿对齐</strong>：将模拟相机与其实世界对应物精确对齐表述为一个优化问题，最小化从给定相机位姿渲染出的图像与真实参考图像之间的光度误差（L1范数）。利用3DGS渲染器的可微性，通过梯度下降迭代优化相机位姿参数。</li>
</ul>
<p><strong>核心模块3：整体场景增强</strong><br>为了训练能够处理真实世界多变性的鲁棒策略，对模拟环境进行系统性随机化以生成多样化数据集。</p>
<ul>
<li><strong>物体级增强</strong>：随机化交互物体的6-DoF位姿（位置和方向）和均匀缩放比例。</li>
<li><strong>相机视角增强</strong>：在已对齐的部署相机位姿基础上，施加随机平移和旋转扰动，生成以部署视角为中心的一系列视图。</li>
<li><strong>光照条件增强</strong>：通过对3D高斯的视觉属性（如颜色）施加随机缩放、偏移和噪声，来模拟全局的颜色对比度、整体亮度变化以及传感器噪声。</li>
<li><strong>轨迹增强</strong>：在机器人运动到最终目标之前，先命令其末端执行器通过逆运动学（IK）求解器运动到一个随机化的中间路径点，以打破轨迹的确定性，增加路径分布的丰富性。</li>
</ul>
<p><strong>创新点总结</strong><br>与现有方法相比，RoboSimGS的核心创新体现在：1) <strong>混合场景表示</strong>：解耦了视觉保真（3DGS背景）与物理交互（网格物体）的需求；2) <strong>MLLM自动化</strong>：开创性地使用MLLM从视觉数据中自动化推断物体的物理属性和复杂运动学结构，这是实现动态、交互式“沙盒”的关键。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在真实和模拟环境中进行。设计了八个不同的真实世界机器人操作任务进行评估，包括堆叠方块、拾放、可变形物体拾放、竖立瓶子、移动瓶子、关闭抽屉、关闭盒子和擦拭。</li>
<li><strong>实验平台</strong>：真实数据收集和策略评估使用LeRobot框架；模拟数据生成在配备Intel i5-14400F CPU和NVIDIA RTX 5060 Ti GPU的工作站上进行。</li>
<li><strong>对比方法</strong>：评估了两种最先进的策略：单任务方法Diffusion Policy (DP) 和通用视觉语言动作模型 π₀。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>零-shot Sim2Real转移</strong>：仅在RoboSimGS生成的模拟数据上训练的策略，能够在多个真实世界任务上实现成功的零-shot转移。例如，DP策略在“堆叠方块”、“拾放香蕉”、“竖立瓶子”等任务上分别达到了0.57、0.83、0.82的成功率（使用100个模拟演示）。</li>
<li><strong>增强现有SOTA方法</strong>：将有限的真实数据与RoboSimGS生成的模拟数据结合，能显著提升现有模型的性能和泛化能力。如表1所示，DP在50个真实演示基础上增加50个RoboSimGS模拟演示后，在多个任务上成功率达到或超过0.9，优于仅使用100个真实演示或100个模拟演示。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.10637v1/x2.png" alt="模拟与真实数据对比"></p>
<blockquote>
<p><strong>图2</strong>：RoboSimGS生成的模拟数据（左）与真实世界数据（右）的定性对比。展示了模拟环境在视觉上的高保真度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.10637v1/x3.png" alt="任务图示"></p>
<blockquote>
<p><strong>图3</strong>：用于真实世界评估的八个操作任务图示，涵盖了从简单拾放到复杂关节物体操作等多种技能。</p>
</blockquote>
<ol start="3">
<li><strong>提升泛化能力</strong>：如表2和图4所示，在面对光照条件、物体尺寸、场景杂乱度、相机位姿和桌面外观变化等挑战性泛化设置时，使用RoboSimGS数据（单独或与真实数据结合）训练的DP策略，其成功率远高于仅使用真实数据训练的策略。例如，在光照条件变化下，“50 Real”成功率为0，而“50 RoboSimGS”达到0.46，“50 Real + 50 RoboSimGS”达到0.54。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.10637v1/x4.png" alt="泛化性能可视化"></p>
<blockquote>
<p><strong>图4</strong>：在四种设计用于测试鲁棒性的挑战性泛化设置下的策略性能可视化。展示了策略对光照、物体尺寸等变化的适应能力。</p>
</blockquote>
<ol start="4">
<li><strong>数据效率分析</strong>：如图5所示，在“堆叠方块”任务上，使用RoboSimGS生成的模拟数据进行训练表现出很高的数据效率。DP策略使用200个模拟演示达到的成功率，与使用100个真实演示训练的策略成功率相当。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.10637v1/x5.png" alt="数据缩放分析"></p>
<blockquote>
<p><strong>图5</strong>：在堆叠方块任务上对Diffusion Policy的数据缩放分析。比较了使用不同数量真实数据与纯RoboSimGS模拟数据训练的策略成功率。模拟数据表现出较高的数据效率。</p>
</blockquote>
<ol start="5">
<li><strong>消融实验</strong>：表1的下半部分展示了消融研究结果。移除“物理估计”模块会导致在涉及物理交互（如可变形物体拾放、擦拭）的任务上性能显著下降。移除“整体场景增强”则在所有任务上导致性能大幅降低，凸显了增强对于政策鲁棒性和泛化能力的至关重要性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的Real2Sim2Real框架（RoboSimGS），能够从真实场景生成兼具高视觉保真度和物理交互性的模拟环境，其基础是3DGS与网格的混合表示。</li>
<li>开创性地使用多模态大语言模型（MLLM）自动化创建物理合理、可关节化的资产，从视觉数据中推断物理属性（密度、刚度）和运动学结构（铰链、滑轨）。</li>
<li>通过大量实验证明，RoboSimGS生成的模拟数据不仅能实现卓越的零-shot Sim2Real迁移，还能作为高质量、可扩展的数据源，显著提升现有最先进模型的性能和泛化能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，为每个特定任务场景进行数据采集需要大约10分钟的手动扫描。此外，尽管MLLM自动化了物理和关节推断，但其推理的准确性和对极端或罕见物体的泛化能力可能存在边界，这依赖于底层MLLM的能力。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>混合表示是趋势</strong>：将神经渲染的视觉保真与显式几何的物理可靠性相结合，是构建高保真交互模拟器的有效路径。</li>
<li><strong>利用基础模型先验</strong>：利用MLLM等基础模型从视觉中提取物理和功能语义，为自动化构建交互式数字孪生提供了强大工具，可大幅减少人工标注和建模成本。</li>
<li><strong>数据生成与策略学习协同</strong>：高质量、多样化的模拟数据生成本身可以成为提升机器人学习性能的关键杠杆，特别是在数据稀缺的领域。未来的工作可以探索如何进一步优化生成数据的分布，以针对性地提升策略的薄弱环节。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人学习中真实数据收集成本高、仿真数据因视觉与物理差距难以迁移到真实世界（Sim2Real鸿沟）的核心问题，提出了RoboSimGS框架。其关键技术是采用混合场景表示：利用3D高斯泼溅实现高保真视觉重建，结合交互物体的网格图元确保精确物理模拟，并创新性地使用多模态大语言模型自动推断物体的物理属性与运动结构。实验表明，完全使用该方法生成的数据训练的策略，能成功实现跨多种真实操作任务的零样本迁移，并显著增强现有先进方法的性能与泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.10637" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>