<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>3D-VLA: A 3D Vision-Language-Action Generative World Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>3D-VLA: A 3D Vision-Language-Action Generative World Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2403.09631" target="_blank" rel="noreferrer">2403.09631</a></span>
        <span>作者: Zhen, Haoyu, Qiu, Xiaowen, Chen, Peihao, Yang, Jincheng, Yan, Xin, Du, Yilun, Hong, Yining, Gan, Chuang</span>
        <span>日期: 2024/03/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉-语言-动作模型依赖于2D图像输入，缺乏与更广泛的3D物理世界的整合。此外，这些模型通过学习从感知到动作的直接映射来进行动作预测，忽略了世界的丰富动态以及动作与世界动态之间的关系。相比之下，人类拥有能够描绘未来场景想象的世界模型，并据此规划行动。现有具身基础模型主要学习从感知到动作的直接映射，缺乏对世界动态的更广泛理解。构建此类类人的3D世界模型面临挑战：首先，现有基础模型专注于语言生成，无法想象语言之外的多模态未来状态以促进动作生成；其次，现有具身数据集主要包含2D图像或视频，缺乏用于3D空间推理和规划的3D相关标注。</p>
<p>本文针对现有VLA模型缺乏3D理解和生成式世界建模能力的痛点，提出了3D-VLA，一个通过生成式世界模型无缝连接3D感知、推理和动作的新系列具身基础模型。其核心思路是：在基于3D的大语言模型基础上，引入交互令牌与环境互动，并通过训练一系列具身扩散模型并将其对齐到LLM中，来注入预测目标图像和点云的生成能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>3D-VLA的整体目标是构建一个用于具身环境中3D推理、目标生成和决策的世界模型。其Pipeline基于一个3D大语言模型骨干，并引入了交互令牌以增强与3D世界的交互能力，最后通过一个对齐阶段将预训练的多模态扩散模型解码器集成进来，赋予模型多模态目标生成能力。</p>
<p><img src="https://arxiv.org/html/2403.09631v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>: 3D-VLA方法总览。左侧展示了模型的目标生成能力：根据用户输入（初始状态图像/点云和语言指令），模型可以想象最终状态的图像和点云。生成的目标状态可以反馈给模型以指导机器人控制。右侧展示了模型处理交错3D-文本数据的能力。</p>
</blockquote>
<p><strong>核心模块1：3D-VLA骨干与交互令牌</strong><br>模型以3D-LLM的方法论为基础，利用BLIP2-FlanT5 XL作为预训练模型，通过多视角特征生成3D场景特征。为了增强模型对3D场景的理解并促进交互，引入了一系列新的交互令牌：</p>
<ol>
<li><strong>对象令牌</strong> <code>&lt;obj&gt; &lt;/obj&gt;</code>：包裹解析句子中的对象名词，使模型能更好地捕捉被操作或被提及的对象。</li>
<li><strong>位置令牌</strong> <code>&lt;loc0-255&gt;</code>：用于接地被提及对象，由六个令牌表示3D边界框（AABB形式）。</li>
<li><strong>场景令牌</strong> <code>&lt;scene&gt; &lt;/scene&gt;</code>：包裹静态场景的嵌入，通过组合场景令牌，模型可以理解动态场景并管理交错的3D场景和文本输入。</li>
<li><strong>动作令牌</strong>：如<code>&lt;aloc0-255&gt;</code>, <code>&lt;arot0-255&gt;</code>, <code>&lt;gripper0/1&gt;</code>，表示机器人7自由度的动作（绝对位置、旋转、夹爪开合），由<code>&lt;ACT_SEP&gt;</code>分隔。</li>
</ol>
<p><strong>核心模块2：向3D-VLA注入目标生成能力</strong><br>为了使模型具备人类般的“预可视化”能力，论文分两步注入图像、深度和点云的目标生成能力：</p>
<ol>
<li><strong>预训练具身扩散模型</strong>：为了解决现有扩散模型在具身环境下的局限性，使用构建的3D-语言视频数据训练条件扩散模型。<ul>
<li><strong>RGB-D生成</strong>：以Stable Diffusion V1.4为基础，将RGB潜变量和深度潜变量拼接作为图像条件。</li>
<li><strong>点云生成</strong>：以Point-E为基础，添加点云条件输入。</li>
</ul>
</li>
<li><strong>桥接LLM与目标生成</strong>：为了将不同模态的预训练解码器无缝集成到LLM中，设计了一个对齐阶段。<ul>
<li>引入特殊令牌如<code>&lt;image&gt; &lt;/image&gt;</code>和<code>&lt;pcd&gt; &lt;/pcd&gt;</code>来告知解码器输出的模态类型。</li>
<li>使用一个<strong>基于Transformer的投影器</strong>，将LLM的输出特征和嵌入映射到扩散模型框架的潜空间中，从而连接高层语言理解和多模态目标生成。</li>
<li>训练时，使用LoRA微调不同的扩散模型，仅训练新引入的特殊令牌嵌入、对应的输出线性层以及整个投影器，以保持效率并避免灾难性遗忘。损失函数结合了LLM和扩散模型的去噪损失。</li>
</ul>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>3D理解</strong>：从依赖2D特征转向利用3D点云等特征进行理解和规划。</li>
<li><strong>交互令牌</strong>：引入结构化令牌系统，显式地建模对象、位置、场景和动作，增强了模型在具身环境中的推理和交互能力。</li>
<li><strong>生成式世界模型</strong>：不是直接学习感知-动作映射，而是通过生成对未来多模态目标状态的想象（世界模型的核心）来指导动作规划。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在多个基准和数据集上进行评估，包括：Open-X Embodiment数据集（包含11个子集）、RoboVQA、RT-1、RLBench和CALVIN。实验平台涉及真实世界和仿真机器人数据。</p>
<p><strong>对比的基线方法</strong>包括：3D-LLM，以及2D视觉语言模型如BLIP2、OpenFlamingo、LLaVA。对于定位任务，对比了Kosmos-2和CoVLM。对于目标生成，对比了Instruct-P2P、SuSIE、NeXT-GPT（图像）和Point-E（点云）。对于动作规划，对比了LanCon-Learn等。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>3D推理与定位</strong>：如表1所示，3D-VLA在所有语言推理任务（具身QA、任务描述、假设QA、密集描述）上显著优于所有2D VLM方法。例如，在具身QA任务上，3D-VLA的BLEU-4得分达到26.80，而表现次优的BLIP2 FlanT5 XL（经训练）为15.48。3D-LLM在这些机器人任务上表现不佳，凸显了在机器人相关3D数据集上训练的必要性。</li>
</ol>
<p><img src="https://arxiv.org/html/2403.09631v1/x4.png" alt="推理能力评估"></p>
<blockquote>
<p><strong>表1</strong>：在内部数据上评估推理能力。3D-VLA在各项指标上全面领先，证明了其3D理解和推理优势。</p>
</blockquote>
<ol start="2">
<li><strong>定位能力</strong>：如表2所示，3D-VLA在交并比（IoU）和准确率（Acc@25， Acc@50）上大幅超越基于2D检测并投影到3D的基线方法（如CoVLM），证明了其直接进行3D定位的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2403.09631v1/x5.png" alt="定位结果"></p>
<blockquote>
<p><strong>表2</strong>：在机器人数据集上的定位结果。3D-VLA直接进行3D定位，性能显著优于使用真实深度信息进行2D到3D投影的基线。</p>
</blockquote>
<ol start="3">
<li><strong>多模态目标生成</strong>：<ul>
<li><strong>图像生成</strong>：如表3所示，在与直接零样本迁移到机器人领域的方法相比时，3D-VLA在PSNR、SSIM等指标上表现优异。即使与在同一机器人数据集上训练的Instruct-P2P*相比，3D-VLA也保持领先，证明了LLM整合带来的更好指令理解能力。消融实验显示，在输入提示中排除预测的边界框会导致性能轻微下降，证明了中间预测（边界框）有助于模型聚焦于相关对象。</li>
<li><strong>点云生成</strong>：如表4所示，3D-VLA在点云FID和Chamfer距离上优于Point-E，且包含预测边界框的版本效果最好。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2403.09631v1/x6.png" alt="目标生成定量结果"></p>
<blockquote>
<p><strong>表3与表4</strong>：RGB图像和点云目标生成的定量结果。3D-VLA在多个指标上优于基线，且包含预测边界框（BBox）的版本性能最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2403.09631v1/x3.png" alt="生成结果可视化"></p>
<blockquote>
<p><strong>图3</strong>：生成的RGB-D目标图像可视化。第一行来自训练集未见过的测试数据，第二行来自互联网或日常生活的未知环境。模型能保持背景不变，准确识别并修改指令指定的目标对象状态，生成的图像与真实目标在视觉和语义上高度一致。</p>
</blockquote>
<ol start="4">
<li><strong>具身动作规划</strong>：在RLBench任务上（表5），3D-VLA在“放刀”、“拿伞”、“拿起杯子”等任务上的成功率（68， 52， 40）远高于基线方法LanCon-Learn（即使有历史信息）。在未见过的杯子变体任务上也能达到24%的成功率，展示了泛化能力。在CALVIN的长视野多任务序列评估中，3D-VLA的成功率（34%）也大幅超过最佳基线VoxPoser（12%）。</li>
</ol>
<p><strong>消融实验总结</strong>：目标生成实验（表3和表4）表明，在输入中提供模型预测的中间边界框（作为对指令中对象的定位）能持续提升图像和点云生成的质量，这验证了通过交互令牌进行对象定位这一设计的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>3D-VLA</strong>，一个统一3D感知、推理和动作的生成式世界模型新框架；2) 构建了一个大规模的<strong>3D具身指令微调数据集</strong>，解决了现有数据缺乏3D信息的问题；3) 引入了<strong>交互令牌系统</strong>并成功<strong>注入多模态目标生成能力</strong>，通过投影器将扩散模型与LLM对齐。</p>
<p>论文提到的局限性包括：所构建数据集的规模和多样性仍有提升空间；模型在极其复杂或动态场景中的泛化能力有待进一步验证。</p>
<p>本工作对后续研究的启示在于：1) <strong>3D理解是高级具身智能的关键</strong>，超越2D感知对于空间推理和精确交互至关重要；2) <strong>生成式世界模型是一条有前景的路径</strong>，通过预测未来状态来规划动作，更接近人类的认知方式；3) <strong>高质量、多模态的具身数据构建</strong>是推动领域发展的基础，自动化的标注和丰富流程（如本文所用）值得借鉴。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出3D-VLA模型，以解决现有视觉-语言-动作模型局限于2D输入、缺乏对3D物理世界动态理解的问题。其核心方法是在3D大语言模型基础上，引入交互令牌与环境互动，并通过训练嵌入式扩散模型并与LLM对齐，实现对未来目标图像和点云的生成预测。实验表明，该模型在具体环境中的推理、多模态生成与规划能力显著提升，展现了在真实场景中的应用潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2403.09631" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>