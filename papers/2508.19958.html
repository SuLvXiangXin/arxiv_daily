<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.19958" target="_blank" rel="noreferrer">2508.19958</a></span>
        <span>作者: Donglin Wang Team</span>
        <span>日期: 2025-08-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型已成为机器人策略学习的基石，能够利用大规模多模态数据进行鲁棒且可扩展的控制。然而，现有VLA框架主要针对短视野任务，其在长视野、多步骤机器人操作上的有效性仍然有限，主要挑战在于技能链（Skill Chaining）和子任务依赖性问题。长视野操作的主流方法是任务分解，即将复杂任务分解为一系列子任务，每个子任务由独立的局部策略管理。这种方法虽然降低了学习单个行为的复杂度，但未能充分建模子任务间的过渡与依赖（即技能链问题），导致动态耦合和跨子任务边界的误差传播，从而显著降低整体性能。尽管存在一些解决技能链的方法（如在线自适应优化、使用不同输入模态解耦运动规划与执行），但它们往往与VLA模型端到端、可扩展的训练范式不兼容。</p>
<p>本文针对在保持VLA模型可扩展性和数据效率的同时，解决长视野任务中的技能链问题这一核心痛点，提出了一个新视角：在统一的端到端VLA模型内部，通过输入级的自适应策略来隐式地管理任务阶段。本文的核心思路是：将每个子任务细分为移动阶段和交互阶段，并设计一种阶段感知的输入掩码策略，使模型能动态关注阶段相关的感官线索，从而减少表征偏移并改善技能链。</p>
<h2 id="方法详解">方法详解</h2>
<p>Long-VLA是一个专为长视野操作设计的端到端VLA模型。其整体框架首先对数据进行阶段分解，然后在统一的模型训练中引入输入级自适应掩码策略，最后进行端到端训练。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Long-VLA方法总览。(a) 任务分解，对齐视觉观察和语言标注。(b) 通过掩码实现输入级自适应，使模型在注意力计算中能选择性地关注相关token。(c) 使用分解后的数据和阶段感知掩码进行端到端训练。</p>
</blockquote>
<p><strong>数据与阶段分解</strong>：首先，将每个语言标注的轨迹分解为移动阶段和交互阶段。移动阶段从开始到切割点 <code>d</code>，交互阶段从 <code>d+1</code> 到轨迹结束。切割点设置在物体状态变化前的10-15帧，以确保阶段对齐。为了在统一的端到端框架中进行训练和推理，原始动作表示被扩展，增加了一个一维的阶段标识符 <code>s_p</code>（移动阶段为-1，交互阶段为1）。最终的动作token表示为 <code>[x, y, z, eu_x, eu_y, eu_z, s_g, s_p]</code>，包含末端执行器的坐标、欧拉角、夹爪状态和阶段标识。</p>
<p><strong>输入级自适应策略（核心创新）</strong>：本文认为，在移动阶段，模型应专注于使用第三人称相机视图进行精确的物体导航，因为此时夹爪相机视图信息量最小；而在交互阶段，注意力应转移到夹爪相机，以减轻视觉分布偏移并实现精确操作。为此，提出了输入级自适应策略，通过掩码动态调整不同阶段的视觉输入关注度。具体而言，为每个token分配一个二元掩码 <code>m ∈ {0,1}</code>，<code>m_i = 1</code> 表示第i个token参与注意力计算。该向量被扩展为注意力掩码矩阵 <code>M</code>，确保注意力仅在有源token对之间计算。在计算查询-键相似度矩阵 <code>P</code> 后，掩码后的注意力权重 <code>A</code> 通过公式 <code>A_{ij} = exp(P_{ij}) * M_{ij} / Σ_k exp(P_{ik}) * M_{ik}</code> 计算。此策略使模型能够在不改变输入结构的情况下，选择性地关注相关token，从而保持模态一致性的同时适应不同任务阶段。</p>
<p><strong>模型架构与训练</strong>：Long-VLA策略 <code>π(a^t | s^t, d^t, g)</code> 预测动作 <code>a^t</code>，条件为当前观察 <code>s^t</code>（夹爪视图和静态视图）、检测输入 <code>d^t</code> 和潜在目标 <code>g</code>。观察由可训练的ResNet-18编码。目标编码器利用冻结的CLIP模型，将未来观察（作为视觉目标）或语言标注编码为目标特征。为了支持动态场景中的精确导航与交互，集成了检测信息：使用LoRA微调的Grounding DINO进行细粒度物体定位，检测到的边界框通过可训练的位置编码器投影为特征，并采用FiLM策略调制静态相机特征。多模态编码器基于GPT-2风格的Transformer，将所有模态特征拼接编码为潜在感知token。动作解码器采用条件扩散模型，通过DDIM采样从高斯噪声中逐步去噪生成动作。训练损失包含用于动作生成的分数匹配损失 <code>L_diff</code> 和用于确保视觉目标与语言指令语义一致的InfoNCE损失 <code>L_goal</code>，总损失为 <code>L = L_diff + αL_goal</code>（α=0.1）。</p>
<p>与现有方法相比，Long-VLA的核心创新在于：1) <strong>统一模型解决长视野问题</strong>：不同于以往需要分解为独立模型的方案，它保持了端到端VLA的统一性。2) <strong>输入级阶段感知掩码</strong>：通过软性的注意力掩码机制，而非硬性切换输入模态，实现了对移动和交互阶段的动态自适应，这是解决技能链问题的关键。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境中，基于CALVIN平台并提出了新的<strong>L-CALVIN</strong>基准，将任务序列从5步扩展到10步。在真实世界中，设计了两个任务：1) 按顺序将积木放入碗中（序列长度8）；2) 复杂的厨房清洁任务（序列长度4）。基线方法包括：基础策略MDT、视频生成类VLA模型（GR-1, UP-VLA）、VLM类VLA模型（RoboVLMs, VLAS, OpenVLA）以及方法7（用于真实世界实验）。</p>
<p><strong>关键实验结果</strong>：<br>在L-CALVIN模拟基准上，Long-VLA相比基础策略（MDT）取得了显著提升。如图4所示，在D→D和ABCD→D场景下，随着任务步数增加，性能提升愈加明显。例如，在D→D的10步任务中，成功率从11%提升至20%（相对提升81%）。在数据有限的场景下提升更大。</p>
<p><img src="https://..." alt="模拟性能"></p>
<blockquote>
<p><strong>图4</strong>：Long-VLA在L-CALVIN模拟基准上的性能。随着任务序列增长，相比基础策略（Base Policy）的提升百分比（括号内）显著增加。</p>
</blockquote>
<p>在真实世界排序任务中（图5），基础策略在第7个任务后成功率降至0，而Long-VLA在所有8个任务上仍能保持接近25%的成功率，且相对性能增益随序列长度增加。</p>
<p><img src="https://..." alt="真实世界排序性能"></p>
<blockquote>
<p><strong>图5</strong>：真实世界排序任务性能。在随机定位、未知光照、视觉干扰三种未见设置下，Long-VLA相比基础策略的成功率（及提升百分比）。</p>
</blockquote>
<p>在更具挑战性的真实世界清洁任务中（图6），Long-VLA在所有时间视野和不同干扰条件下均实现了显著改进，提升幅度甚至超过排序任务，表明其能有效处理视觉干扰和复杂环境。</p>
<p><img src="https://..." alt="真实世界清洁性能"></p>
<blockquote>
<p><strong>图6</strong>：真实世界清洁任务性能。Long-VLA在三种未见设置下均显著超越基础策略。</p>
</blockquote>
<p>与SOTA方法对比（表2，图7），Long-VLA在模拟和真实世界实验中均达到最佳性能。在L-CALVIN的D→D场景中，其平均完成序列长度（Avg. Len）为4.75，显著高于GR-1（2.96）和RoboVLMs（2.88）。在真实世界，其性能也持续超越方法7。</p>
<p><img src="https://..." alt="与SOTA对比"></p>
<blockquote>
<p><strong>图7</strong>：与SOTA方法在真实世界场景的对比（左：清洁；右：排序）。Long-VLA表现最佳。</p>
</blockquote>
<p><strong>消融实验</strong>：表3的消融研究验证了三个关键设计组件（分解策略、输入级自适应、统一模型）的贡献。仅使用分解策略或输入级自适应都能带来提升，但二者结合（且通过掩码机制在统一模型中实现）时性能最佳。例如，在真实清洁任务中，完整Long-VLA的Avg. Len为2.8，显著高于仅用分解（2.0）或仅用输入自适应（1.7）的版本。这证明了统一训练结合阶段自适应掩码策略的有效性。</p>
<p><strong>可扩展性验证</strong>：如表4所示，Long-VLA范式具有架构无关性。当基础骨架从MDT替换为HULC时，Long-VLA依然能带来一致的性能提升（Avg. Len从2.65提升至3.30），证明了该方法的普适性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>首个为长视野机器人操作设计的端到端统一VLA模型</strong>（Long-VLA），成功解决了技能链问题。2) 引入了<strong>新颖的阶段感知输入掩码策略</strong>，通过输入级自适应使模型能动态聚焦于阶段相关线索，提升了子任务兼容性和鲁棒性。3) 提出了<strong>L-CALVIN基准</strong>以系统评估长视野操作，并验证了Long-VLA在模拟和真实任务上显著超越SOTA的性能。</p>
<p>论文自身提到的局限性包括：1) 训练数据集的阶段分解目前仍依赖人工，未来或可借助VLM实现自动化。2) 所考虑的长视野任务范围仍有限，模型虽能减少初始状态差距，但尚需进一步优化以处理更长序列中的失败情况。3) 测试的序列长度范围仍受约束。</p>
<p>本工作对后续研究的启示在于：为在统一VLA框架内解决长视野问题提供了有效且可扩展的范例。其输入级掩码自适应机制作为一种轻量级、架构无关的模块，可轻松集成到现有VLA模型中。未来方向包括自动化阶段分解、探索更长的任务序列、以及将该范式应用于更多样化的机器人框架和任务领域。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在长视界机器人操作中存在的技能链和子任务依赖性挑战，提出了首个端到端的Long-VLA模型。其关键技术是阶段感知输入掩码策略，通过自适应将子任务分割为移动和交互阶段，使模型聚焦阶段相关感官线索，增强子任务兼容性。该架构无关模块可集成到现有VLA中。实验基于新提出的L-CALVIN基准，结果表明Long-VLA在模拟和真实任务中显著优于先前最先进方法，为长视界控制设立了新基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.19958" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>