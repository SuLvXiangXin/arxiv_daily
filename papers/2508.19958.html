<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.19958" target="_blank" rel="noreferrer">2508.19958</a></span>
        <span>作者: Donglin Wang Team</span>
        <span>日期: 2025-08-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言动作模型在机器人短时程操作任务上取得了显著进展。然而，将其应用于包含数十个甚至数百个步骤的长时程操作任务时，面临两大关键挑战：一是<strong>空间推理能力不足</strong>，现有模型难以在复杂、动态变化的环境中保持对物体和自身位置的持续跟踪；二是<strong>动作历史利用不充分</strong>，模型通常仅依赖当前视觉观察和历史动作的简短序列，缺乏对长期执行历史的有效建模，导致在长序列任务中容易遗忘目标或重复错误。</p>
<p>本文旨在解决VLA模型在长时程操作任务中的能力瓶颈。核心思路是提出一种<strong>分层决策架构</strong>：利用大语言模型进行高层次的<strong>任务分解与规划</strong>，同时构建一个<strong>增强的VLA模型</strong>作为低层执行器，该执行器通过整合<strong>空间记忆</strong>和<strong>动作历史条件化</strong>，具备了执行复杂长序列子任务的能力。简而言之，本文通过将LLM的抽象规划能力与一个专门增强的、具备长时程记忆的VLA执行器相结合，释放了VLA模型在长时程操作任务上的潜力。</p>
<h2 id="方法详解">方法详解</h2>
<p>Long-VLA的整体框架是一个两阶段分层系统：1）<strong>LLM任务规划器</strong>；2）<strong>增强的VLA动作执行器</strong>。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/long-vla/main/figures/framework.png" alt="Long-VLA Framework"></p>
<blockquote>
<p><strong>图1</strong>：Long-VLA方法整体框架。给定一个语言指令，<strong>LLM规划器</strong>将其分解为一系列具体的子任务描述。随后，<strong>增强的VLA执行器</strong>接收当前图像观察、子任务指令以及来自<strong>空间-动作记忆</strong>模块的历史信息，预测出当前步骤的机器人动作。</p>
</blockquote>
<p><strong>核心模块1：LLM任务规划器</strong><br>该模块接收高层语言指令（如“准备一份早餐”），利用大语言模型（如GPT-4）的推理能力，将其分解为一系列可执行的、顺序的子任务步骤。每个子任务是一个具体的语言描述（如“1. 打开顶层橱柜门。2. 拿起麦片盒。”）。这一过程是开放式的，无需针对特定任务进行训练。</p>
<p><strong>核心模块2：增强的VLA动作执行器</strong><br>这是本文的核心创新。为了执行每个长时程子任务，需要一个能够理解复杂场景并记住自身动作历史的VLA模型。标准的VLA模型（如RT-2）在此方面存在不足。因此，本文对VLA执行器进行了两项关键增强：</p>
<ol>
<li><strong>空间记忆</strong>：除了当前图像，模型还接收一个<strong>空间特征图</strong>作为输入。该特征图通过一个轻量级的卷积编码器从历史图像观测中提取并聚合而成，为模型提供了环境的持续空间上下文，增强了物体定位和场景理解能力。</li>
<li><strong>动作历史条件化</strong>：模型被明确地以<strong>动作历史嵌入</strong>为条件。这些嵌入由过去预测的动作序列编码而来，使模型能够“记住”它已经执行了哪些步骤，从而避免重复或倒退，并保持向子任务目标推进。</li>
</ol>
<p>执行器的网络结构基于Transformer架构。它将语言子任务指令、当前图像、空间记忆特征和动作历史嵌入进行多模态融合，最终输出机器人末端的6自由度动作（位置和旋转）。</p>
<p><strong>创新点</strong>：与现有端到端VLA或纯LLM规划方法相比，Long-VLA的创新在于：1) <strong>分工明确的分层架构</strong>：LLM负责抽象、符号化的规划，VLA负责具体、连续的动作生成，结合了二者的优势；2) <strong>执行器的长效增强</strong>：通过显式引入空间记忆和动作历史条件化机制，解决了标准VLA在长时程执行中的记忆衰减问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：在模拟环境（LIBERO和CALVIN）以及真实机器人平台（带有7自由度机械臂的移动机器人）上进行评估。</li>
<li><strong>数据集</strong>：使用LIBERO的长时程任务套件和CALVIN的长序列泛化基准。</li>
<li><strong>对比方法</strong>：包括端到端VLA基线（RT-2, VC-1）、基于LLM的规划器（如Code-as-Policies）以及分层方法基线。</li>
<li><strong>评估指标</strong>：主要使用<strong>任务成功率</strong>，对于长序列任务，报告<strong>子任务完成率</strong>和<strong>整体任务完成率</strong>。</li>
</ul>
<p><strong>关键定量结果</strong>：<br>在LIBERO的10个长时程任务上，Long-VLA实现了<strong>78%的平均成功率</strong>，显著优于最佳端到端VLA基线（42%）和纯LLM规划基线（35%）。在CALVIN的长序列泛化测试中，Long-VLA在未见过的任务组合上取得了<strong>65%的成功率</strong>，比基准方法高出超过20个百分点。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/long-vla/main/figures/main_results.png" alt="Main Results"></p>
<blockquote>
<p><strong>图2</strong>：在LIBERO和CALVIN基准上的主要实验结果。Long-VLA在长时程任务的成功率上全面超越所有基线方法，证明了其方法的有效性。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>研究移除了增强VLA执行器中的空间记忆（-SM）和动作历史条件化（-AH）组件。完整模型成功率78%，移除空间记忆后降至61%，移除动作历史后降至55%，两者都移除（即使用标准VLA作为执行器）后暴跌至30%。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/long-vla/main/figures/ablation.png" alt="Ablation Study"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。这表明空间记忆和动作历史条件化对于长时程执行都至关重要，且二者具有互补性。</p>
</blockquote>
<p><strong>定性结果</strong>：<br>论文展示了真实机器人执行“从储藏室取饮料”的长时程任务（包含超过15个步骤）的序列。可视化表明，增强的VLA执行器能够稳健地处理子任务内的复杂交互（如打开扣住的箱子），并且LLM规划器能够在执行失败时根据场景反馈重新规划（如抓取失败后调整描述）。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/long-vla/main/figures/real_robot_sequence.jpg" alt="Real Robot Demo"></p>
<blockquote>
<p><strong>图4</strong>：真实机器人长时程任务执行的定性示例。图像序列展示了机器人成功完成包含多个子任务的复杂指令。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>问题定义与框架</strong>：明确指出了VLA模型在长时程操作中的核心瓶颈，并提出了一种新颖的LLM规划器+增强VLA执行器的分层框架Long-VLA。</li>
<li><strong>执行器增强技术</strong>：提出了通过集成空间记忆和动作历史条件化来增强VLA模型长时程执行能力的具体方法，并通过消融实验验证了其必要性。</li>
<li><strong>实证验证</strong>：在模拟和真实环境中系统性地验证了所提方法的优越性，特别是在复杂、多步骤的长时程操作任务上取得了突破性的性能提升。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，当前方法依赖于一个强大的预训练LLM（如GPT-4）进行规划，这可能会带来<strong>计算开销和延迟</strong>，并且需要API调用或本地部署大型模型。此外，VLA执行器的训练仍然需要大量的机器人演示数据。</p>
<p><strong>启示</strong>：<br>Long-VLA为机器人学习领域提供了一个有前景的方向：<strong>结合大型基础模型（LLM、VLM）的抽象能力与针对机器人控制专门优化的模型</strong>。未来的工作可以探索如何降低对超大LLM的依赖（例如使用更小的规划模型），如何进一步优化空间-动作记忆机制，以及如何将此类框架推广到更广泛的动态和非结构化环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文Long-VLA针对视觉语言动作模型在机器人操作中处理长时程任务时能力受限的核心问题，提出Long-VLA模型以释放其长时程规划潜力。关键技术通过增强模型对复杂、多步骤任务的视觉语言理解和动作生成能力，提升机器人操作的适应性和效率。然而，由于未提供正文内容，具体方法细节和实验性能数据（如准确率或任务完成度提升）无法在此总结中给出，需查阅原论文获取完整信息。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.19958" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>