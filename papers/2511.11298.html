<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.11298" target="_blank" rel="noreferrer">2511.11298</a></span>
        <span>作者: Xi Zheng Team</span>
        <span>日期: 2025-11-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域正从传统的任务专用控制器转向基于模仿学习和基础模型的方法。特别是视觉-语言-动作（VLA）模型，通过整合大规模预训练的视觉和语言模型，展现出实现通用操作的巨大潜力。然而，当前研究存在关键局限性：尽管VLA模型发展迅速，但系统性的真实世界评估和跨模型比较仍然匮乏。现有基准测试多集中于单臂、纯仿真环境，且评估指标不统一，难以全面反映模型在实际部署中的可靠性、指令遵循能力和空间泛化能力。</p>
<p>本文针对上述痛点，提出了一个经验驱动的统一基准测试框架，旨在对代表性的VLA模型进行公平、系统的比较。本文的核心思路是：在一个包含双臂协调、工具使用、可变形物体操作和语言理解的复杂任务集上，对比一个专用的模仿学习策略（ACT）与三个通用VLA模型（OpenVLA–OFT， RDT-1B， π₀），通过标准化指标（成功率、效率、适应性、指令遵循准确度）在仿真和真实机器人平台（ALOHA Mobile）上进行评估，以揭示不同架构在精度、泛化能力和部署成本之间的权衡。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文建立了一个统一的评估框架，用于在四个双臂操作任务上对四个代表性模型进行系统评测。这四个模型分别是：专用模仿学习策略ACT，以及三个通用VLA模型OpenVLA–OFT、RDT-1B和π₀。评估框架的核心在于从三个维度衡量模型性能：1) <strong>准确性与效率</strong>：任务成功率和成功所需时间；2) <strong>适应性</strong>：在训练分布内（ID）、空间分布外（相同物体实例但新位置/姿态/布局）以及实例+空间分布外（新物体实例+新空间配置）三种设置下的表现；3) <strong>语言指令遵循准确度</strong>。</p>
<p><strong>核心模块（被评测模型）及其技术细节</strong>：</p>
<ol>
<li><strong>ACT</strong>：一种任务专用的模仿学习策略，用于ALOHA平台。它使用ResNet-18视觉编码器和基于Transformer的CVAE策略头，输出未来的“动作块”（关节位置序列），以减少误差积累并捕捉人类演示的非马尔可夫、多模态特性。它是一个纯视觉模型，针对单个任务从头训练，不具备语言条件化或多任务泛化能力，在本文中作为强大的视觉专用基线。</li>
<li><strong>OpenVLA–OFT</strong>：一个7B参数的开源VLA模型，在近百万条演示的Open X-Embodiment数据集上训练。它集成双视觉变换器（DINOv2, SigLIP）和Llama-2（7B）语言主干，形成统一的多模态操作策略。其优化微调（OFT）方法采用并行解码、连续动作表示和L1回归目标，以解决原始OpenVLA推理慢（3–5 Hz）和双臂设置不稳定的问题，实现了约15 Hz的推理速度。</li>
<li><strong>RDT-1B</strong>：一个10亿参数的基于扩散的VLA模型，用于双臂操作。它集成冻结的SigLIP视觉和T5-XXL语言编码器，投影到统一的变换器令牌空间，并通过MLP头在“物理可解释统一动作空间”中输出动作。在包含超过100万条轨迹的46个数据集上预训练，并在一个6k轨迹的双臂数据集上微调，能生成平滑、协调的多步双臂动作。</li>
<li><strong>π₀</strong>：一个基于3B参数PaliGemma视觉-语言主干和3亿参数动作头的通用机器人基础模型。它引入流匹配来对齐异构数据，支持高达50 Hz的高频控制。采用两阶段训练：在OXE超集（10,000+小时，68个任务，7种机器人类型）上进行大规模预训练，然后进行任务特定的微调。</li>
</ol>
<p><strong>与现有方法相比的创新点</strong>：<br>本文的创新点不在于提出新模型，而在于建立了一个<strong>系统性的、跨模型的比较基准</strong>。与以往通常孤立评估单个模型或在单一范式内比较的工作不同，本文首次将专用模仿学习策略与多个通用VLA模型置于<strong>完全相同的任务、硬件平台和评估协议</strong>下进行直接对比。这种设计能够清晰揭示“专家”策略与“通才”基础模型在不同性能维度上的实际权衡。</p>
<p><strong>评估任务设计</strong>：<br>评测包含四个需要双臂协调的家庭启发式任务，如图2所示。</p>
<p><img src="https://arxiv.org/html/2511.11298v1/clean_dish.jpg" alt="清洁盘子"></p>
<blockquote>
<p><strong>图2a</strong>：清洁盘子任务。机器人需用左夹具拿起盘子并稳定，右夹具拿起海绵擦拭盘子表面，测试精确力控和双臂协调。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/put_pot.jpg" alt="将海绵放入锅中"></p>
<blockquote>
<p><strong>图2b</strong>：将海绵放入锅中任务。指令明确指定目标物体（如“把海绵/甜甜圈放进锅里”），测试物体选择、空间理解和多步规划。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/unzip_bag.jpg" alt="拉开背包拉链"></p>
<blockquote>
<p><strong>图2c</strong>：拉开背包拉链任务。涉及对可变形物体（包）和微小物体（拉链）的精确、双臂操作，测试精细操作和张力维持。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/folding_shorts.jpg" alt="折叠短裤"></p>
<blockquote>
<p><strong>图2d</strong>：折叠短裤任务。对可变形衣物进行两次折叠，是长视野、高精度的双臂协调任务，测试对布料形态的维持和重新抓取能力。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>硬件平台</strong>：ALOHA Mobile双臂机器人平台，配备三个Intel RealSense D405 RGB-D相机（一个顶部，两个腕部）。</li>
<li><strong>计算配置</strong>：NVIDIA RTX 5090 GPU，单步控制时间预算为40毫秒（25 Hz）。</li>
<li><strong>数据集</strong>：每个任务通过遥操作收集演示（清洁盘子、放入锅中、拉开拉链各100条；折叠短裤200条）。演示涵盖了多样的初始物体位姿和语言指令表述。</li>
<li><strong>评估协议</strong>：每个（任务、模型、设置）组合进行n=50次试验。评估三种设置：ID、空间OOD、实例+空间OOD。使用共享的随机种子初始化状态以确保公平性。</li>
<li><strong>对比的Baseline方法</strong>：ACT， OpenVLA–OFT， RDT-1B， π₀。</li>
<li><strong>仿真基准</strong>：在MuJoCo桌面环境中补充评估，包含三个彩色方块，测试零样本语言 grounding 和空间推理（仅OpenVLA–OFT和π₀）。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.11298v1/sim_pick_blue/6.jpg" alt="仿真环境快照"></p>
<blockquote>
<p><strong>图3</strong>：仿真环境快照，展示机器人在MuJoCo中操作彩色方块。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能与权衡</strong>：π₀在OOD场景下表现出最强的适应性，而ACT在ID设置下提供了最高的稳定性。通用VLA模型（尤其是π₀）在需要理解新物体或新指令的OOD设置中优势明显，而专用模型ACT在训练分布内的任务上精度更高。</li>
<li><strong>效率与计算成本</strong>：训练时间差异巨大（见表1及图1描述）。ACT训练最快（约0.17天），π₀次之（约2天），而OpenVLA–OFT和RDT-1B各需约21天。在推理延迟上，π₀和ACT能满足50 Hz控制，OpenVLA–OFT优化后稳定在25 Hz。</li>
<li><strong>数据缩放效应</strong>：在“折叠短裤”这类长视野、可变形物体任务上，模型性能在演示数据量从100增加到200时出现饱和，表明仅增加有限的数据可能不足以持续提升此类复杂任务的性能。</li>
<li><strong>指令遵循准确度</strong>：在需要区分相似物体（如“海绵” vs “甜甜圈”）的任务中，VLA模型展现出基于语言理解进行正确选择的能力，而纯视觉的ACT则无法处理此类语言指定的目标变化。</li>
</ol>
<p><strong>失败模式分析（关键贡献之一）</strong>：<br>论文通过大量试验，总结并图示了VLA模型在真实操作中反复出现的失败模式，形成了结构化的错误分类法。</p>
<p><img src="https://arxiv.org/html/2511.11298v1/issues_discovered/xy_miss_grasp.jpg" alt="XY平面抓取未对准"></p>
<blockquote>
<p><strong>图6</strong>：XY平面抓取未对准（Near-miss grasp）。夹具在物体旁边闭合，未能成功抓取，是常见的执行失误。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/issues_discovered/z_axis_misalign.jpg" alt="Z轴未对准"></p>
<blockquote>
<p><strong>图7</strong>：Z轴未对准。夹具高度估算错误，在物体上方或下方错误闭合。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/issues_discovered/misgrasp_lid.jpg" alt="错误理解指令抓取锅盖"></p>
<blockquote>
<p><strong>图8</strong>：语言理解错误。指令是“把海绵放进锅里”，模型却错误地试图抓取锅盖，属于符号 grounding 失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/issues_discovered/wrong_lang_understand.jpg" alt="错误理解指令抓取错误物体"></p>
<blockquote>
<p><strong>图9</strong>：另一例语言理解错误。应抓取海绵，却试图抓取甜甜圈。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/issues_discovered/misgrasp_zipper.jpg" alt="错误抓取拉链"></p>
<blockquote>
<p><strong>图10</strong>：对微小物体（拉链）的抓取失败，涉及精确操作挑战。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/issues_discovered/bag_misgrap.jpg" alt="错误抓取背包"></p>
<blockquote>
<p><strong>图11</strong>：抓取可变形物体（背包）时位置不当，导致后续操作失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/issues_discovered/failed_fold.jpg" alt="折叠失败"></p>
<blockquote>
<p><strong>图12</strong>：折叠短裤任务失败，衣物滑落或形成一团糟，属于长视野状态漂移。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/issues_discovered/misgrasp_1st_folded_shorts.jpg" alt="错误抓取已折叠的短裤"></p>
<blockquote>
<p><strong>图13</strong>：在折叠中途重新抓取时失败，未能稳定住部分折叠的衣物形态。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/sim_issues/openvla_sim_misgrasp.jpg" alt="仿真中OpenVLA抓取未对准"></p>
<blockquote>
<p><strong>图14</strong>：仿真环境中，OpenVLA-OFT抓取方块时出现未对准。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/sim_issues/pi0_misgrasp_pose.jpg" alt="仿真中π₀姿态错误"></p>
<blockquote>
<p><strong>图15</strong>：仿真环境中，π₀在抓取时出现不自然的夹具姿态。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11298v1/sim_issues/pi0_stack_misalign.jpg" alt="仿真中π₀堆叠未对准"></p>
<blockquote>
<p><strong>图16</strong>：仿真环境中，π₀执行堆叠任务时，方块未对齐。</p>
</blockquote>
<p>这些定性结果直观展示了模型在感知、规划、执行各环节可能出现的具体问题，为诊断和改进模型提供了明确方向。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>首个针对通用VLA模型的双臂操作系统实证评估</strong>：提供了在统一、真实的双臂操作任务上比较专用策略与通用VLA模型的基准测试框架、协议和结果。</li>
<li><strong>失败分类法与诊断分析</strong>：通过对大量试验的观察，总结出VLA模型常见的错误模式（如时空漂移、符号grounding失败、执行失误），并进行了可视化归类，为理解模型弱点提供了结构化视角。</li>
<li><strong>关于鲁棒性与数据缩放的实证洞察</strong>：揭示了模型在精度与泛化之间的明确权衡，并指出在复杂长视野任务上，性能会随有限演示数据缩放而饱和，这对数据收集策略具有启示意义。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1) 评估的模型数量有限（四个），且主要是开源或已有检查点的模型；2) 任务集虽然具有代表性，但尚未覆盖所有类型的操作挑战（如动态交互、非常规姿态抓取等）；3) 仿真基准仅用于补充分析语言和空间推理，与真实任务复杂度有差距。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>模型选择</strong>：在实际部署中，需根据需求权衡。若任务环境高度可控且固定，专用模仿策略（如ACT）可能提供更高精度和效率；若需要处理新物体、新指令或环境变化，通用VLA模型（如π₀）的泛化能力更为重要。</li>
<li><strong>评估标准化</strong>：强调了对机器人学习模型进行系统性、多维度（精度、效率、泛化、指令遵循）和跨平台（仿真与真实）评估的必要性，呼吁社区建立更统一的基准。</li>
<li><strong>改进方向</strong>：识别出的失败模式（如近失抓取、长视野漂移）为未来研究指明了具体的技术改进目标，例如开发更鲁棒的抓取点预测、更好的时序一致性保持机制，以及更精准的语言-动作 grounding 方法。同时，巨大的训练成本差距也推动了对更高效训练和微调方法的需求。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中视觉-语言-动作模型缺乏系统性真实评估的问题，建立了一个标准化评测框架，对ACT、OpenVLA-OFT、RDT-1B和π₀四种代表性VLA模型进行了基准测试。评估围绕三个维度展开：任务成功率与用时、对分布内及分布外场景的适应性、以及语言指令跟随准确性。核心实验发现，π₀模型在分布外场景下适应性最强，而ACT模型在分布内任务中稳定性最高。分析还揭示了模型在计算需求、数据扩展行为及常见失败模式（如抓取未遂、过早释放）上的差异，为实际部署中的精度、泛化与成本权衡提供了依据。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.11298" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>