<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.05007" target="_blank" rel="noreferrer">2511.05007</a></span>
        <span>作者: Huazhe Xu Team</span>
        <span>日期: 2025-11-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人视觉运动控制领域的主流方法是基于扩散策略的生成模型，其在短期任务中表现出色。然而，在长时程、多阶段的任务中，这些方法存在关键局限性：缺乏阶段意识。当中间子任务（如抓取）失败时，策略通常会继续执行后续动作序列，仿佛失败从未发生，导致任务级联失败。这种脆弱性源于策略学习到的表示通常是高维且高度纠缠的“黑盒”，难以解释，也阻碍了从局部错误中恢复的能力。</p>
<p>本文针对扩散策略在长时程任务中鲁棒性不足和可解释性差的具体痛点，提出了引入混合专家层进行技能分解的新视角。核心思路是在视觉编码器和扩散模型之间插入一个混合专家层，将策略的知识分解为一组专业化的专家，这些专家被动态激活以处理任务的不同阶段，从而增强鲁棒性并实现可解释的技能分解。</p>
<h2 id="方法详解">方法详解</h2>
<p>MoE-DP的整体框架是在标准扩散策略的视觉编码器和扩散模型之间插入一个MoE层。输入是历史观测（全局相机视图、腕部相机视图和机器人本体感知状态），经视觉编码器（从头训练的ResNet18）处理后得到潜在特征向量 <strong>z_t</strong>。该向量随后被输入MoE层，MoE层的路由器根据输入动态选择Top-k个专家网络进行处理，生成一个精炼的条件特征向量 <strong>z’_t</strong>。此向量最后作为条件输入到扩散模型中，用于生成动作序列。输出是预测的机器人动作。</p>
<p><img src="https://arxiv.org/html/2511.05007v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MoE-DP方法整体框架。在自主模式下，系统编码观测输入，MoE层的路由器自动为当前观测选择合适的专家。所选专家的输出作为扩散策略生成动作时的条件输入。该架构还支持高级控制：外部智能体（如人类操作员或视觉语言模型）可以通过覆盖路由器的默认选择来引导策略，从而实现如重新排列子任务等灵活行为。</p>
</blockquote>
<p>核心模块是MoE层，由一组专家网络 <strong>{E_i}</strong> 和一个轻量级路由门控网络组成。每个专家是一个多层感知机，旨在专攻操作任务的特定阶段。路由器计算N个专家的路由权重 **g_t = Softmax(W_g z_t)<strong>，并采用稀疏的Top-k路由策略。最终的条件向量是所选专家输出的加权和：</strong>z’<em>t = Σ</em>{i ∈ Top-k(g_t)} g_{t,i} · E_i(z_t)**。这种设计鼓励不同的专家专注于任务的不同阶段（技能分解），路由机制可以重新激活合适的专家以重试失败的子任务，从而提升阶段感知的鲁棒性。</p>
<p>训练中的一个关键挑战是防止路由器崩溃。为此，论文引入了辅助损失，由负载平衡损失和熵损失两项组成。负载平衡损失鼓励专家间的负载分布更均匀；熵损失鼓励路由器为每个样本做出低熵（高置信度）的分配，从而增强专家的专业化。完整的训练目标是标准扩散去噪损失与加权辅助损失之和：<strong>L = L_diff + λ L_load + β L_entropy</strong>。</p>
<p>与现有方法相比，创新点具体体现在：1) 将MoE架构集成到扩散策略中，用于单任务内的细粒度技能分解，而非多任务分配；2) 通过精心设计的辅助损失实现端到端学习，获得可解释、任务优化的技能；3) 利用学习到的专家-技能映射，在推理时无需重新训练即可通过外部指令（如VLM）灵活控制行为执行顺序。</p>
<p><img src="https://arxiv.org/html/2511.05007v1/x3.png" alt="技能分解与控制"></p>
<blockquote>
<p><strong>图3</strong>：通过组合技能分解实现推理时控制。上图展示了策略按演示顺序执行任务时，将过程分解为三个不同的子任务，每个子任务调用一致的专家激活序列。下图展示了在推理时，通过手动改变子任务顺序（先子任务2，后子任务1），策略通过重新排序学习到的技能模块成功执行了新任务。关键的是，单个子任务（如子任务1）的专家激活模式在两种场景下保持一致，证明了MoE-DP学习了真正可组合的技能。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了6个长时程模拟任务（基于MimicGen启发构建）和3个真实世界任务（在Franka Emika机器人上部署，使用RealSense D435i相机）。对比的基线方法是标准扩散策略。实验评估了正常执行和受干扰执行两种条件，干扰通过在特定子任务（如成功抓取物体后）将物体重置回初始位置来模拟失败。</p>
<p>关键实验结果如下：在模拟任务中（表1），正常条件下MoE-DP与基线性能相当（平均成功率79.1% vs 74.1%）。但在受干扰条件下，MoE-DP表现出显著优势，平均成功率达到53.8%，相比基线的17.8%，实现了36%的平均相对提升。在真实世界任务中（表2），趋势相同，正常条件下两者性能接近（平均成功率91.1% vs 88.3%），受干扰条件下MoE-DP平均成功率达71.0%，远高于基线的28.0%。</p>
<p><img src="https://arxiv.org/html/2511.05007v1/x1.png" alt="VLM控制框架"></p>
<blockquote>
<p><strong>图4</strong>：基于VLM的规划与控制框架概述。系统利用VLM进行高级任务规划，分为两个阶段：① 技能总结阶段，VLM通过分析演示的标注帧，构建机器人能力的文本知识库；② 任务执行阶段，VLM利用此知识库、高级目标指令和实时图像，推理当前任务阶段并预测应激活的专家。该架构使系统能够动态规划和重新排列子任务顺序，而无需重新训练。</p>
</blockquote>
<p>消融实验（表5）分析了辅助损失中负载平衡损失和熵损失的作用。结果表明，同时使用两种损失项的组合在所有任务中都能取得最佳性能（标记为“LE”），单独使用任一项或完全不使用辅助损失（“N”）的性能均有所下降。这证实了辅助损失对于实现平衡且专业化的专家使用至关重要。</p>
<p>此外，论文通过可视化（图3）证实了MoE-DP学习到了有意义的技能分解，例如在“将鸭子放入碗中”的子任务中，不同的专家分别专攻“接近鸭子”、“抓取”和“移入碗中”阶段。基于此分解，论文展示了推理时行为控制的可行性（表4），通过人类操作员或VLM覆盖路由器选择，可以成功执行训练中未见过的新子任务顺序，尽管VLM控制的成功率因空间理解和进度识别限制而低于人工控制。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1) 提出了MoE-DP方法，通过在扩散策略中集成MoE层，显著提升了策略在长时程任务中面对干扰时的鲁棒性，在模拟和真实实验中均取得显著性能增益；2) 通过辅助损失引导，模型学习到了可解释的技能分解，不同专家对应不同的语义任务阶段；3) 利用学习到的技能模块化表示，实现了无需重新训练的推理时行为控制，允许通过外部指令（如VLM）重新组合技能以泛化到新的任务结构。</p>
<p>论文自身提到的局限性包括：基于VLM的推理时控制性能仍低于直接人工控制，部分源于VLM在空间理解和精确进度识别方面的不足；此外，不同任务需要经验性地调整MoE的超参数（如专家数量、top-k值、损失权重）以达到最佳性能。</p>
<p>本研究对后续工作的启示在于：证明了MoE架构可用于机器人策略内部的细粒度技能分解，而不仅仅是多任务分配；精心设计的辅助损失对于实现有效分解至关重要；这种模块化、可解释的策略表示为实现更灵活、可组合和可交互的机器人控制开辟了新途径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MoE-DP方法，旨在解决扩散策略在长时程、多阶段机器人操作任务中缺乏阶段意识、无法从子任务失败中恢复且表示难以解释的问题。其核心是在视觉编码器与扩散模型间插入混合专家层，将策略知识分解为多个专家，动态激活以处理任务不同阶段。实验表明，该方法在6个长时程仿真任务受干扰条件下，成功率平均相对提升36%，并在真实世界验证了鲁棒性优势，同时学习到的专家对应可解释的技能基元。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.05007" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>