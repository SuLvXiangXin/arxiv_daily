<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.08240" target="_blank" rel="noreferrer">2508.08240</a></span>
        <span>作者: Wang, Kaijun, Lu, Liqin, Liu, Mingyu, Jiang, Jianuo, Li, Zeju, Zhang, Bolin, Zheng, Wancai, Yu, Xinyi, Chen, Hao, Shen, Chunhua</span>
        <span>日期: 2025/08/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，开放世界的移动操作是具身智能领域的重大挑战，需要融合语义推理、泛化操作和自适应运动。现有方法存在三个关键局限：第一，尽管大语言模型通过习得的语义先验增强了空间推理和任务规划能力，但其应用仍局限于桌面场景，未能解决移动平台固有的感知受限和执行范围有限的问题。第二，当前的操作策略在面对开放世界中多样的物体配置时，表现出泛化能力不足。第三，在非结构化环境中，同时保持平台的高机动性和末端执行器的精确控制这一双重需求，在文献中研究不足。本文针对这些痛点，提出了一个统一框架ODYSSEY，旨在为配备机械臂的敏捷四足机器人实现长视程任务。其核心思路是：通过一个由视觉语言模型驱动的分层规划器，将高层语言指令分解为可执行的原子动作序列，并利用一个新颖的、能泛化到挑战性地形的全身控制策略，协同完成移动与操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>ODYSSEY框架包含三个核心组件：1）由粗到细的任务规划器；2）四足机器人全身控制策略；3）移动操作基准测试集。其整体流程实现了从多模态语义感知、地图感知的全局规划、几何约束的动作落地到由学习到的底层策略逐步执行的完整闭环。</p>
<p><img src="https://arxiv.org/html/2508.08240v1/figure/icon.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ODYSSEY整体流程。涵盖了长视程任务的全过程，包括多模态语义感知、地图感知的全局规划、几何约束的动作落地以及由学习到的底层策略逐步执行。</p>
</blockquote>
<p><strong>1. 长视程任务规划器</strong><br>该规划器采用分层结构，旨在弥合语义推理导航与细粒度、可泛化操作之间的依赖关系。</p>
<ul>
<li><strong>地图感知的任务级规划</strong>：首先，通过融合机载RGB和激光雷达数据，利用预训练的基础模型构建包含物体几何和语义信息的实例图。如图1所示，给定实例级语义地图，使用GPT-4.1将自由形式的自然语言指令分解为一系列原子动作（如导航、拾取、放置、推/拉/拖）。对于涉及空间位移的动作，模型会输出一个粗略的目标路径点，该点被投影到通过在线SLAM构建的2D占据栅格图上，并通过局部搜索找到一个无碰撞的目标位姿，从而生成与场景上下文对齐且物理可行的全局任务计划。</li>
<li><strong>几何约束的局部操作</strong>：对于需要近距离操作的原子动作，系统利用腕部深度观测，引导视觉语言模型生成精确的末端执行器位姿。具体而言，给定RGB观测和当前原子动作的文本描述，使用具有像素级接地能力的Qwen2.5-VL-72B-Instruct模型推断图像空间中的任务相关接触点 (p^{*})，并通过深度图反投影得到其在机器人坐标系下的3D位置 (\mathbf{P}<em>{ee})。模型进一步被提示生成末端执行器的方向 (\mathbf{R}</em>{ee})，并受到两种几何约束：1）<strong>轴对齐约束</strong>：当目标物体或接触区域存在主导轴 (\mathbf{a}) 时，末端执行器的x轴和z轴应与之正交；2）<strong>表面法线约束</strong>：如果物体附着在法向量为 (\mathbf{n}) 的平面上，则末端执行器的z轴应在不违反第一个约束的前提下与表面法线对齐。通过结合视觉语言模型的表达能力和可解释的几何约束，系统实现了可靠的局部操作引导。</li>
</ul>
<p><strong>2. 全身控制策略</strong><br>为实现对高层规划器命令的有效执行并适应多样地形，本文提出了一种基于强化学习的两阶段训练策略，由一个神经网络将观测映射为目标关节位置。</p>
<p><img src="https://arxiv.org/html/2508.08240v1/x1.png" alt="策略框架"></p>
<blockquote>
<p><strong>图2</strong>：移动操作策略及其两阶段训练框架概述。</p>
</blockquote>
<ul>
<li><strong>策略定义</strong>：策略 (\pi) 将包含运动命令 (\mathbf{c}_t)、末端目标 (\mathbf{e}_t)、局部地面高度图 (\mathbf{m}_t)、重力向量 (\mathbf{g}<em>t)、上一时刻动作 (\mathbf{a}</em>{t-1}) 以及本体感知状态 (\mathbf{s}_t) 的综合观测向量，映射为目标动作 (\mathbf{a}_t \in \mathbb{R}^{18})。最终的目标关节位置 (\mathbf{q}_t^{target} = \mathbf{q}^{default} + \mathbf{a}_t) 通过PD控制器转换为扭矩。</li>
<li><strong>两阶段课程学习</strong>：<ul>
<li><strong>第一阶段</strong>：固定机械臂关节，专注于在静态负载下的运动训练。引入了步态奖励 (r_{gait})（鼓励特定的同步/异步足部接触模式）和频率奖励 (r_{fre})（调节步态节奏）来结构化机器人的步态。</li>
<li><strong>第二阶段</strong>：控制所有18个关节（腿和机械臂）。奖励函数扩展为包含末端执行器跟踪奖励 (r_{arm})，以引导策略训练。</li>
</ul>
</li>
<li><strong>关键创新：地形不变的末端采样</strong>：为确保在不同地形上的鲁棒性能，采样策略在世界坐标系中（以机械臂基座为中心）采样目标位置，并固定其世界坐标系下的Z轴高度，然后再转换到机器人移动基座坐标系。这种方法将末端目标与机器人基座俯仰或地形高度变化引起的扰动解耦，从而提高了任务执行中的交互精度。</li>
<li><strong>域随机化</strong>：在训练过程中广泛使用域随机化（如动力学参数、末端质量等）以缩小模拟到现实的差距。</li>
</ul>
<p><strong>3. 移动操作基准</strong><br>本文提出了首个针对室内外环境的长视程移动操作模拟基准，包含丰富的资产库、领域风格变化以及多阶段任务套件。</p>
<ul>
<li><strong>资产与场景</strong>：包含50个刚性物体、15个容器、30个铰接结构、10个可拖拽物品等资产，以及10个真实场景（5个室内家庭、2个超市、1个餐厅、2个带坡道和楼梯的户外庭院）。</li>
<li><strong>任务套件</strong>：<ul>
<li><strong>短视程ARNOLD任务</strong>：集成了四个单步操作任务（拾取物体、重定向物体、打开柜门、关闭柜门）。</li>
<li><strong>长视程移动操作任务</strong>：构建了8个多阶段任务（如室内收集、餐厅导引、购物车配送等），共304种变体，涵盖抓取、重定向、容器放置、铰接操作和在复杂地形上的长期导航。</li>
</ul>
</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了本文提出的ODYSSEY基准测试集，对比了PerAct（用于规划评估）和RoboDuet（用于控制策略评估）等基线方法。</p>
<p><strong>1. 高层规划器性能</strong></p>
<ul>
<li><strong>ARNOLD短视程任务</strong>：在四个迁移的ARNOLD任务上，与基于大规模人类轨迹训练、使用五个外部摄像头的PerAct模型对比。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.08240v1/x2.png" alt="表1结果"></p>
<blockquote>
<p><strong>表1</strong>：在ARNOLD任务上的性能对比（成功率%）。我们的方法在多数任务上显著优于PerAct，尤其是在“Novel”泛化分割上表现稳定，展示了处理分布外物体配置的泛化能力。</p>
</blockquote>
<ul>
<li><strong>ODYSSEY长视程任务</strong>：在8个长视程任务上的整体及逐动作成功率。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.08240v1/x3.png" alt="表2结果"></p>
<blockquote>
<p><strong>表2</strong>：8个ODYSSEY长视程任务的总体成功率及逐动作成功率（%）。系统在所有任务上保持了40%以上的整体成功率，各原子技能类别成功率超过60%，验证了其在泛化长视程任务中的鲁棒协调能力。导航成功率普遍很高（&gt;85%），拾取和放置成功率也表现强劲。</p>
</blockquote>
<p><strong>关键发现</strong>：低层策略在室内外不规则地形上实现了可靠的运动和位姿跟踪；视觉语言模型引导的接地实现了较高的拾取和放置成功率；全局任务规划器在实例图上展现了强大的符号推理能力，确保了多阶段任务的可靠分解。</p>
<p><strong>2. 底层策略性能</strong><br>在模拟器中与RoboDuet基线进行定量比较，评估了静态（站立）和动态（移动）条件下的基础跟踪误差与末端执行器位姿误差。</p>
<p><img src="https://arxiv.org/html/2508.08240v1/x4.png" alt="表3结果"></p>
<blockquote>
<p><strong>表3</strong>：静态和动态条件下的定量结果对比。我们的方法在基础速度跟踪误差（(e_x, e_y, e_{\omega})）上显著优于RoboDuet，这归因于策略观测中包含了地形数据。末端执行器位姿跟踪性能与基线相当。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.08240v1/x5.png" alt="表4"></p>
<blockquote>
<p><strong>表4</strong>：训练和评估阶段使用的命令采样范围。为确保公平和测试泛化性，两者在更大的相同工作空间中进行评估。</p>
</blockquote>
<p><strong>消融实验贡献</strong>：论文指出，地形不变的末端采样策略通过将末端目标与基座姿态扰动解耦，提高了交互精度；两阶段训练课程和包含地形信息的观测提升了策略的鲁棒性和跟踪性能；广泛的域随机化是成功实现模拟到现实转移的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1）提出了一个连接以自我为中心感知和语言条件化任务的分层视觉语言规划器，能够将长视程指令分解为可执行动作；2）提出了首个能泛化到挑战性地形、并协同运动与操作的全身控制策略；3）引入了首个涵盖广泛真实室内外场景的长视程移动操作基准；4）成功实现了高层规划器和底层控制策略的模拟到现实转移，在真实部署中展示了强大的泛化性和鲁棒性。</p>
<p><strong>局限性</strong>：论文提到，部分失败源于视觉语言模型对物体几何的空间推理存在局限（如夹爪对齐不优），以及对于复杂交互（如拖拽）中细长把手或被部分遮挡物品的定位不准确。</p>
<p><strong>后续启示</strong>：这项工作证明了腿式移动操作器在非结构化环境中部署的可行性和实用性。后续研究可着眼于：1）增强视觉语言模型对复杂物体几何和空间关系的理解；2）开发更鲁棒的动态交互（如推、拉、拖）策略；3）进一步探索在更极端地形和动态环境中的长期任务规划与执行。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ODYSSEY框架，旨在解决四足机器人在开放世界中执行语言引导长时程任务的核心挑战，包括具体语义推理、泛化操作和自适应运动。方法集成高级任务规划与低级全身控制：采用视觉-语言模型驱动的分层规划器进行指令分解与动作执行，以及新颖的全身策略实现运动与操作在复杂地形中的鲁棒协调。通过成功的模拟到真实迁移，实验证明系统在真实世界非结构化环境中具有良好泛化能力和鲁棒性，推动了腿式操作器的实用化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.08240" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>