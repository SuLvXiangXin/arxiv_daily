<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Adaptive Diffusion Policy Optimization for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Adaptive Diffusion Policy Optimization for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.08376" target="_blank" rel="noreferrer">2505.08376</a></span>
        <span>作者: Jiang, Huiyun, Yang, Zhuang</span>
        <span>日期: 2025/05/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>强化学习（RL）在机器人控制领域已取得显著成功，但传统方法使用高斯分布或高斯混合模型来参数化策略，往往在动作空间中产生单峰分布，限制了智能体探索环境的能力，也难以表达复杂的多模态策略。扩散模型作为一种强大的生成模型，在行为克隆中表现出色，能够学习复杂分布，为克服RL的上述局限性提供了巨大潜力。近期研究已开始将扩散模型应用于RL策略（即扩散策略），但如何快速、稳定地优化这类基于扩散模型的策略，目前研究相对有限。</p>
<p>现有方法如扩散策略策略优化（DPPO）、行为正则化扩散策略优化（BDPO）、Q加权变分策略优化（QVPO）等，均直接或间接地聚焦于优化策略更新的计算公式。本文则从一个全新的视角切入：优化器本身。针对扩散策略优化过程中可能存在的收敛慢或不稳定问题，本文提出了基于Adam的自适应扩散策略优化（ADPO）。其核心思路是，首次将自适应策略梯度（ADAPG）方法引入基于扩散的RL，通过在自适应梯度方法（如Adam）中引入一个折扣因子，使其能够在不同优化器特性之间进行插值，从而在保留各自优势的同时提升策略优化性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的ADPO框架旨在为机器人控制任务中基于扩散策略的微调提供一个快速、稳定的算法框架。其整体遵循标准的Actor-Critic架构，其中策略（Actor）由扩散模型参数化，价值函数（Critic）由神经网络估计。创新的核心在于策略参数的优化器采用了自适应策略梯度（ADAPG）方法。</p>
<p><img src="https://arxiv.org/html/2505.08376v1/x1.png" alt="ADPO框架图"></p>
<blockquote>
<p><strong>图1</strong>：ADPO方法整体框架。左侧展示了基于扩散模型的策略（Actor）接收状态并生成动作的过程，以及Critic网络评估状态-动作值。右侧核心是自适应策略梯度（ADAPG）优化器，它利用当前梯度、历史梯度矩估计以及折扣因子λ来更新扩散策略的参数θ。</p>
</blockquote>
<p>ADAPG是ADPO的核心模块。它并非提出一种全新的策略梯度计算公式，而是对策略梯度下降的<strong>优化过程</strong>进行改进。具体而言，ADAPG将机器学习中成熟的自适应梯度方法（如Adam、RMSProp）与强化学习背景相结合。其关键是在自适应梯度更新中引入了一个即时折扣因子λ。该因子用于调整历史梯度二阶矩估计的权重，其作用类似于RL中的折扣因子，使得优化器能够在倾向于Adam（λ接近1）和倾向于RMSProp（λ接近0）的特性之间平滑过渡。这种设计允许算法根据任务特性动态调整优化行为，以追求更好的性能。</p>
<p>同时，为了应对噪声带来的误差累积和更新过程中的振荡问题，ADAPG借鉴了Katyusha动量方案。该方案在参数更新时结合了“梯度步”和“动量步”，有助于减少振荡，避免收敛到局部最优，从而进一步提升优化的稳定性。</p>
<p>与现有扩散RL方法（如DPPO、DAWR等）主要修改策略目标函数或引入额外正则化项不同，ADPO的创新点在于其优化机制。它保持了策略梯度更新公式的通用性（例如，可以采用类似DPPO的PPO裁剪目标），但通过替换底层优化器为ADAPG，从优化过程层面提升了训练效率和稳定性。这使得ADPO能够作为一个相对独立的模块，与不同的扩散策略基础算法结合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在MuJoCo模拟器和Adroit手部操作环境的六个标准机器人控制任务上进行：HalfCheetah， Hopper， Walker2d， Ant， Pen（笔旋转）和 Relocate（重新摆放）。实验平台为Python 3.8和PyTorch。</p>
<p>对比的基线方法包括六种流行的基于扩散的RL方法：扩散策略策略优化（DPPO）、扩散优势加权回归（DAWR）、免模型在线扩散策略RL（DIPO）、行为正则化扩散策略优化（BDPO）、Q加权变分策略优化（QVPO）以及扩散Q学习（DQL）。</p>
<p><img src="https://arxiv.org/html/2505.08376v1/x2.png" alt="六个任务上的性能对比曲线"></p>
<blockquote>
<p><strong>图2</strong>：ADPO与六种基线方法在六个标准机器人任务上的学习曲线对比。横轴为环境交互步数（百万），纵轴为平均回报。图中显示，ADPO（红色实线）在大多数任务（如HalfCheetah， Hopper， Walker2d， Ant， Pen）上获得了最高或接近最高的最终性能，且训练曲线通常更为平滑稳定。</p>
</blockquote>
<p>关键实验结果显示，ADPO在整体有效性和训练稳定性方面优于其他对比方法。具体而言，在Walker2d任务上，ADPO的最终性能比表现次优的DPPO高出约8.5%；在Ant任务上，ADPO的性能比DPPO高出约12.3%。即使在表现相近的任务（如HalfCheetah）上，ADPO的训练曲线也显示出更小的波动和更快的初始收敛速度。在最具挑战性的Adroit手部操作任务（Pen， Relocate）上，ADPO也取得了具有竞争力的结果。</p>
<p><img src="https://arxiv.org/html/2505.08376v1/x3.png" alt="消融实验：折扣因子λ的影响"></p>
<blockquote>
<p><strong>图3</strong>：折扣因子λ消融实验。在Walker2d任务上测试了λ取不同值（0， 0.5， 0.9， 0.99， 0.999）时ADPO的性能。结果表明，λ=0.9和λ=0.99时性能最佳，验证了在Adam和RMSProp之间进行适当插值的有效性。λ=0（纯RMSProp）和λ=1（近似纯Adam）性能均有所下降。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.08376v1/x4.png" alt="消融实验：Katyusha动量影响"></p>
<blockquote>
<p><strong>图4</strong>：Katyusha动量消融实验。对比了使用Katyusha动量（KM）与不使用（None）在Walker2d任务上的性能。使用KM的ADPO获得了更高且更稳定的回报，证明了该组件对于减少振荡、提升稳定性的贡献。</p>
</blockquote>
<p>消融实验系统分析了ADPO中超参数的影响。如图3所示，折扣因子λ对性能有显著影响，最优值在0.9附近，证实了混合优化器特性的优势。如图4所示，引入Katyusha动量（KM）有效提升了训练稳定性和最终性能。此外，论文还对批大小、扩散步数等超参数进行了敏感性分析，为实际应用提供了指导。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为两点：第一，首次提出了基于Adam的自适应扩散策略优化（ADPO）框架，将自适应策略梯度（ADAPG）方法引入基于扩散模型的强化学习，从优化器层面解决了扩散策略训练中的稳定性和效率问题。第二，通过在标准机器人任务上的大量实验，验证了ADPO相对于现有多种扩散RL方法的优越性，并系统分析了关键超参数的影响，为后续应用提供了实践指导。</p>
<p>论文自身提到的局限性在于，尽管ADPO提升了性能，但基于扩散模型的策略本身在采样时需要进行多步去噪，其计算成本仍然高于传统策略。此外，ADAPG中引入了额外的超参数（如λ），需要根据任务进行一定调整。</p>
<p>这项工作对后续研究的启示在于，为强化学习算法优化开辟了一个新的方向：即不局限于改进策略或价值目标函数本身，而是可以深入优化其训练过程中的梯度下降机制。将成熟的优化理论（如自适应梯度法、动量方案）与RL的特殊性（如序列决策的折扣思想）相结合，有望催生出更高效、更稳定的新一代RL优化器。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操控中基于扩散模型的策略优化速度慢、稳定性差的核心问题，提出了自适应扩散策略优化（ADPO）方法。该方法采用Adam自适应梯度下降框架，高效微调扩散策略。在标准机器人任务实验中，ADPO与六种基线扩散RL方法相比，取得了更好或可比的性能，验证了其优化效果。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.08376" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>