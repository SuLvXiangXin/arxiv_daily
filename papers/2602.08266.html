<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.08266" target="_blank" rel="noreferrer">2602.08266</a></span>
        <span>作者: Ayoung Kim Team</span>
        <span>日期: 2026-02-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在存在不可避免遮挡和观测不完整的杂乱场景中，选择信息丰富的视角对于构建可靠的场景表示至关重要。3D高斯泼溅（3DGS）因其显式表示的优势，可以显式地指导后续视角选择并利用新观测优化表示。然而，现有基于3DGS的视角选择方法（如FisherRF、POp-GS）主要依赖几何线索，忽略了与机器人操作相关的语义信息，并且倾向于“利用”已观测良好的区域而非“探索”未充分观测的区域。此外，这些方法旨在重建整个场景，对于针对被遮挡、小型或远离场景中心的特定目标物体的操作任务不够鲁棒。</p>
<p>本文针对上述痛点，提出了一种结合实例感知信息增益的、以目标物体为中心的“最佳下一个视角”（NBV）策略。核心思路是：通过将物体实例信息蒸馏为独热（one-hot）物体向量，并以此计算置信度加权的信息增益，从而引导NBV策略优先选择与不确定且任务相关的区域（即错误和不确定的高斯分布区域）对应的视角，实现从全局场景覆盖到目标物体聚焦的视角规划。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架包含三个核心组件：1) 物体感知的3D高斯泼溅，用于增量式场景重建与分割；2) 改进的不确定性量化与信息增益策略，以鼓励探索；3) 面向特定目标物体的视角选择及后续操作。</p>
<p><img src="https://arxiv.org/html/2602.08266v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：整体NBV系统示意图。给定物体分割后的3D高斯地图G和候选相机位姿，CUDA光栅化器计算渲染输出相对于高斯参数的雅可比矩阵。每个高斯参数间的相关性由块对角Hessian矩阵捕获，用于估计信息增益。通过比较候选视角的增益，选择信息最丰富的视角，无需候选视角的真实图像。</p>
</blockquote>
<p><strong>核心模块一：物体感知的3D高斯泼溅</strong><br>该方法在标准3DGS参数（位置、尺度、旋转、不透明度、球谐系数）基础上，为每个高斯附加一个物体特征向量 <code>o_i ∈ ℝ^(n+1)</code>，其中n为场景中不同物体的数量，额外一维对应背景。通过将2D基础模型（如SAM、GroundingDINO）提供的实例掩码作为监督，优化该特征向量。</p>
<p><img src="https://arxiv.org/html/2602.08266v1/x2.png" alt="渲染与学习过程"></p>
<blockquote>
<p><strong>图2</strong>：独热物体向量的渲染与学习过程可视化。每个高斯存储n+1个类别的logits（n个物体类别和背景）。logits经过softmax函数，并按照3DGS中RGB相同的alpha混合方式进行混合。归一化后，得到沿每条射线的逐像素类别概率。通过用实例掩码获得的独热向量监督这些概率，可以优化每个高斯的logits。</p>
</blockquote>
<p>渲染时，对每个高斯的物体特征向量 <code>o_i</code> 应用softmax得到物体概率 <code>p_i</code>，然后进行alpha加权混合，得到像素级的物体向量 <code>O‘</code>。由于混合后总和可能小于1，将残差值加到背景通道以确保总和为1（公式2），最终 <code>O_k</code> 可解释为像素属于类别k的概率。监督损失结合L1损失和Dice损失（公式3）。此方法能有效抑制背景高斯的透明度，减少高斯总数和漂浮物，并可根据物体概率阈值 <code>δ_obj</code> 修剪无关高斯。</p>
<p><strong>核心模块二：基于不确定性量化的NBV策略</strong><br>信息增益 <code>IG(T,c)</code> 通过比较加入新候选视角c前后不确定性值来量化。关键创新在于引入了<strong>实例感知的置信度加权</strong>。每个高斯的物体概率 <code>p_i</code> 用于计算一个置信度得分 <code>w_i = max_k(p_i,k)</code>，即该高斯属于其最可能类别的概率。信息增益计算时，对每个高斯的贡献用其置信度得分进行加权，从而降低已明确归属物体（高置信度）的高斯在信息增益中的权重，使策略更倾向于选择那些不确定性高（渲染误差大）且置信度低（物体归属不明确）的区域对应的视角，有效平衡了探索与利用。</p>
<p><strong>核心模块三：以目标物体为中心的NBV与操作</strong><br>通过指定目标物体ID，可以将NBV规划聚焦于该物体。具体而言，在计算信息增益时，仅考虑属于目标物体（根据当前高斯物体概率判断）的高斯，或显著提高它们的权重。这使得相机视角选择优先覆盖目标物体未被充分观测的部分。基于重建和分割结果，系统可进一步生成杂乱场景中物体的6自由度抓取位姿。</p>
<p><strong>创新点总结</strong></p>
<ol>
<li><strong>实例感知的信息增益</strong>：通过物体置信度加权，使NBV优先探索观测不足但任务相关的区域，克服了现有方法偏向“利用”的局限。</li>
<li><strong>快速物体感知重建</strong>：通过独热物体向量监督，将分割融入3DGS优化，仅需不到10,000次迭代即可完成重建与分割，远少于标准3DGS的30,000次迭代，且能有效抑制背景。</li>
<li><strong>以目标物体为中心的视角规划</strong>：可将NBV灵活应用于整个场景或用户指定的单个物体，提升了针对特定物体操作的鲁棒性。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：合成数据集（8个杂乱桌面场景）和真实世界GraspNet数据集。</li>
<li><strong>对比基线</strong>：基于几何的NBV（View Selection）、FisherRF、POp-GS。</li>
<li><strong>评估指标</strong>：深度估计误差（Depth L1 Loss）、新视角合成质量（PSNR、SSIM、LPIPS）、分割精度（mIoU）。</li>
<li><strong>平台</strong>：真实机器人手臂（Franka Emika Panda）进行抓取实验验证。</li>
</ul>
<p><strong>关键定量结果</strong>：<br>在合成数据集上，与最佳基线相比，本文方法将深度误差降低了高达77.14%。在真实世界GraspNet数据集上，深度误差降低了34.10%。与针对整个场景的NBV相比，针对特定目标物体的NBV能额外降低该物体深度误差25.60%。在分割任务上，mIoU达到87.47%，优于对比方法。</p>
<p><img src="https://arxiv.org/html/2602.08266v1/x4.png" alt="定量结果对比"></p>
<blockquote>
<p><strong>图4</strong>：在合成数据集上的深度误差（左）和新视角合成质量（右）对比。本文方法（Ours）在深度误差上显著低于所有基线方法，在PSNR和SSIM指标上均达到最优。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.08266v1/x5.png" alt="真实数据结果"></p>
<blockquote>
<p><strong>图5</strong>：在真实世界GraspNet数据集上的深度估计与分割定性结果。本文方法能更准确地重建物体几何并清晰分割实例，尤其是在遮挡区域。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>消融实验验证了各组件贡献。移除物体置信度加权会导致性能下降，深度误差增加，证明了加权机制对鼓励探索的有效性。移除物体感知重建（即仅使用RGB）会导致分割失败且重建质量下降。使用更复杂的CLIP特征代替独热向量会导致优化不稳定且速度变慢。</p>
<p><img src="https://arxiv.org/html/2602.08266v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。(a) 移除物体置信度加权（Ours w/o CW）导致深度误差上升。(b) 移除物体感知重建（Ours w/o OA）导致分割mIoU大幅下降。(c) 使用CLIP特征（Ours w/ CLIP）优化更慢且不稳定。</p>
</blockquote>
<p><strong>机器人抓取验证</strong>：<br><img src="https://arxiv.org/html/2602.08266v1/x7.png" alt="机器人抓取"></p>
<blockquote>
<p><strong>图7</strong>：真实机器人抓取实验流水线及结果。本文方法通过物体中心的NBV规划，成功在杂乱场景中定位并抓取目标物体（杯子）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.08266v1/x8.png" alt="抓取成功率"></p>
<blockquote>
<p><strong>图8</strong>：不同方法在真实机器人抓取任务上的成功率对比。本文方法取得了最高的抓取成功率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种集成实例感知置信度加权的NBV策略，有效平衡了视角选择中的探索与利用矛盾。</li>
<li>开发了一种高效的物体感知3DGS，能够快速重建并分割杂乱场景，为NBV提供物体级语义指引。</li>
<li>实现了从全局场景覆盖到目标物体聚焦的灵活NBV规划，并在真实机器人抓取任务中验证了其有效性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，当前方法依赖于2D基础模型提供初始实例掩码，其分割质量会影响后续3D重建和NBV规划的效果。此外，方法在极端遮挡或物体外观高度相似的情况下可能面临挑战。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li>探索更鲁棒或在线学习的2D-3D语义关联方法，以减少对预训练2D模型的依赖。</li>
<li>将动态物体或场景变化考虑进NBV规划框架。</li>
<li>研究信息增益中探索与利用权重的自适应调整机制，以应对不同任务需求。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对杂乱场景中物体感知的3D高斯溅射重建问题，提出了一种信息驱动的物体中心化“下一最佳视角”选择方法。核心是设计了一个实例感知的NBV策略，通过将实例级信息蒸馏为独热物体向量，计算基于置信度的信息增益，从而优先选择与错误和不确定高斯区域相关的未充分探索区域。该方法能有效引导视角选择，专注于特定目标物体，提升重建的完整性和准确性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.08266" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>