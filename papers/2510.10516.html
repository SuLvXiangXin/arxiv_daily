<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.10516" target="_blank" rel="noreferrer">2510.10516</a></span>
        <span>作者: Jeethu Sreenivas Amuthan Team</span>
        <span>日期: 2025-10-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，高维连续机器人控制任务主要依赖于深度强化学习（DRL）方法，其利用深度神经网络（DNN）作为强大的函数逼近器。然而，DRL优异的性能通常以高能耗和较慢的执行速度为代价，这限制了其在资源受限的机器人平台上的应用。另一方面，脉冲神经网络（SNN）因其基于事件的异步计算特性，在神经形态硬件上展现出极高的能效优势，被视为第三代神经网络。但SNN在处理复杂的高维连续控制问题时面临挑战，现有的基于SNN的强化学习方法多局限于低维或离散动作空间。</p>
<p>本文针对机器人控制中“能效”与“高性能”难以兼得的痛点，提出将SNN的能效优势与DRL的强大策略优化能力相结合的新视角。具体而言，本文引入了群体编码（Population Coding）来增强SNN的表征能力，以应对高维观测和连续动作空间。本文的核心思路是：提出一个混合框架，使用群体编码的脉冲执行器网络（PopSAN）作为策略网络，与一个深度评论家网络协同训练，通过基于梯度的DRL算法（如PPO）进行优化，从而在保持高控制性能的同时大幅降低能耗。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个结合了群体编码SNN（PopSAN）和深度强化学习（DRL）的混合框架。整体流程如下：对于给定的高维观测<code>s</code>，PopSAN将其编码为脉冲活动，经过多层SNN处理，最终解码产生连续动作<code>a</code>；同时，一个传统的深度神经网络（DNN）作为评论家网络，评估状态或状态-动作对的价值<code>V(s)</code>或<code>Q(s,a)</code>，并据此计算策略梯度来更新PopSAN的参数。</p>
<p><img src="https://arxiv.org/html/2510.10516v1/algorithm_1_9_15.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：PopSAN整体框架与流程。观测值首先被编码器编码为独立的分布，驱动输入神经元群体产生脉冲。这些脉冲馈入一个多层全连接SNN进行处理。SNN输出层的群体活动在每个时间步结束后被解码，以确定对应的连续动作维度。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>编码器</strong>：负责将观测值<code>s</code>的每个维度<code>si</code>独立地映射到一组输入神经元群体的活动。编码器为每个输入群体<code>i</code>定义了一个高斯接收场，其均值<code>μ(i)</code>和标准差<code>σ(i)</code>是可学习的参数。给定观测值，每个神经元根据其与观测值的“距离”以概率发放脉冲，从而将连续观测值转化为稀疏的脉冲事件序列<code>X(t)</code>。</li>
<li><strong>SNN核心</strong>：采用基于电流的漏积分发放（LIF）神经元模型构建的多层全连接网络。其动力学遵循两步：首先，将来自前一层<code>k-1</code>的脉冲<code>o(t)(k-1)</code>通过权重<code>W(k)</code>积分到电流<code>c(t)(k)</code>中；其次，电流被积分到膜电位<code>v(t)(k)</code>中。当膜电位超过阈值时，神经元发放脉冲<code>o(t)(k)</code>，并采用硬重置机制（发放后膜电位重置为静息电位）。脉冲在同一时间步内瞬时传递。</li>
<li><strong>解码器</strong>：在SNN前向传播<code>T</code>个时间步后，对输出神经元群体的活动进行解码以产生动作。对于第<code>i</code>个动作维度，首先计算对应输出神经元群体的平均发放率<code>fr(i)</code>，然后通过一个可学习的线性解码权重<code>Wd(i)</code>和偏置<code>bd(i)</code>将其映射为连续的動作值<code>αi</code>。</li>
</ol>
<p>训练时，PopSAN的参数通过梯度下降法更新。损失函数<code>L</code>由所选的DRL算法（如PPO）定义。参数更新分为三部分：</p>
<ul>
<li><strong>解码器参数</strong>：直接根据动作梯度<code>∇αi L</code>更新<code>Wd(i)</code>和<code>bd(i)</code>（公式1）。</li>
<li><strong>SNN核心参数</strong>：使用扩展的时空反向传播（STBP）进行更新。通过矩形函数<code>z(v)</code>对脉冲的不可微性进行近似，计算损失对电流<code>c(t)(k)</code>的梯度，并跨时间步聚合以更新权重<code>W(k)</code>和偏置<code>b(k)</code>（公式2）。</li>
<li><strong>编码器参数</strong>：同样通过反向传播的梯度，更新每个输入群体的编码参数<code>μ(i)</code>和<code>σ(i)</code>（公式3）。</li>
</ul>
<p>与现有方法相比，本文的主要创新点在于：1）将群体编码方案系统性地集成到SNN策略网络中，以高效表示高维连续观测；2）实现了整个PopSAN（包括编码器、SNN和解码器）的端到端梯度训练，使其能够与标准DRL算法无缝结合，解决复杂的连续控制问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在NVIDIA Isaac Gym高性能机器人仿真平台中进行，使用PixMC基准测试中的任务。主要测试机器人是Franka机械臂，任务为“抓取”（Pick）。对比的基线方法是使用传统人工神经网络（ANN）作为执行器的DRL方法。训练迭代次数上，ANN方法为20,000次，SNN方法为40,000次。</p>
<p>关键实验结果如下：</p>
<ol>
<li><p><strong>训练过程对比</strong>：论文通过成功率、回合长度和奖励三个指标对比了SNN和ANN的训练曲线。<br><img src="https://arxiv.org/html/2510.10516v1/Experiment-A/SNN_Mean_success_rate.png" alt="SNN平均成功率"></p>
<blockquote>
<p>**图3(a)**：SNN训练期间的平均成功率。显示学习过程缓慢且波动较大，收敛速度慢。<br><img src="https://arxiv.org/html/2510.10516v1/Experiment-A/ANN_Mean.jpeg" alt="ANN平均成功率"><br>**图3(b)**：ANN训练期间的平均成功率。显示学习过程平滑且快速，能较早达到高成功率。<br><img src="https://arxiv.org/html/2510.10516v1/Experiment-A/SNN_Mean_episode_length.png" alt="SNN平均回合长度"><br>**图4(a)**：SNN训练期间的平均回合长度。波动显著，表明策略稳定性建立较慢。<br><img src="https://arxiv.org/html/2510.10516v1/Experiment-A/Ann_Mean-Episode-Length.jpeg" alt="ANN平均回合长度"><br>**图4(b)**：ANN训练期间的平均回合长度。快速上升并保持稳定。<br><img src="https://arxiv.org/html/2510.10516v1/Experiment-A/SNN_Mean_reward.png" alt="SNN平均奖励"><br>**图5(a)**：SNN训练期间的平均奖励。提升缓慢，优化过程存在噪声。<br><img src="https://arxiv.org/html/2510.10516v1/Experiment-A/ANN_Mean-Reward.jpeg" alt="ANN平均奖励"><br>**图5(b)**：ANN训练期间的平均奖励。快速且一致地达到高奖励。</p>
</blockquote>
<p>这些图表共同表明，由于事件驱动、异步计算的特性以及依赖替代梯度，SNN的训练收敛速度比ANN慢，且过程更不稳定。但经过充分训练后，SNN能达到与ANN相当的性能水平。</p>
</li>
<li><p><strong>控制性能定性评估</strong>：<br><img src="https://arxiv.org/html/2510.10516v1/test.png" alt="轨迹跟踪"></p>
<blockquote>
<p><strong>图6</strong>：Franka抓取任务中手指位置（X, Y, Z轴）和目标高度跟踪曲线。红色线为指令（目标）轨迹，蓝色和紫色线为左右手指的实际测量位置，绿色线和红色虚线分别为指令高度和测量高度。结果显示，训练好的SNN控制器能够使手指实际轨迹紧密跟踪目标轨迹，并在抓取后稳定保持目标高度，验证了其控制精度和稳定性。</p>
</blockquote>
</li>
<li><p><strong>能耗对比</strong>：这是本文的核心优势。论文基于45nm工艺下32位浮点运算的能耗模型（一次乘累加MAC消耗4.6pJ，一次累加AC消耗0.9pJ）进行计算对比。<br><strong>表I：能量对比（×10^-6 mJ）</strong></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>Actor(T=1)</th>
<th>Actor(T=2)</th>
<th>Actor(T=3)</th>
</tr>
</thead>
<tbody><tr>
<td>ANN模型</td>
<td>85.96</td>
<td>85.96</td>
<td>85.96</td>
</tr>
<tr>
<td>SNN模型（本文）</td>
<td>3.35</td>
<td>15.36</td>
<td>35.22</td>
</tr>
<tr>
<td>节能比例</td>
<td>96.10%</td>
<td>82.13%</td>
<td>59.03%</td>
</tr>
<tr>
<td>实验结果表明，本文的PopSAN框架能效显著优于传统ANN。在推理时间步长T=1时，能耗降低超过96%。即使T增加到3，仍能节省约59%的能耗。这主要归功于SNN中大部分运算为稀疏的累加（AC）操作，而非密集的乘累加（MAC）。</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一个新颖的混合框架，将群体编码的SNN与DRL相结合，用于高维连续机器人控制；2）设计了可端到端训练的PopSAN架构，通过群体编码和梯度训练解决了SNN在高维控制中的表征和学习难题；3）通过实验验证了该框架在保持与ANN相当控制性能的同时，能实现显著的能效提升（最高达96%）和更快的推理速度。</p>
<p>论文自身提到的局限性包括：当前实现仅针对Franka机械臂的“抓取”任务进行了测试，在机器人类型和任务多样性方面的泛化能力尚未验证。</p>
<p>这项工作对后续研究具有重要启示：首先，它证明了将生物启发的计算模型（SNN+群体编码）与先进的机器学习范式（DRL）相结合，是开发高性能、低功耗机器人控制器的可行路径。其次，论文为未来在更复杂任务、更多机器人平台以及结合异步事件相机等传感器方面扩展SNN的应用奠定了基础。最后，作者展望了将大语言模型（LLM）或多模态大语言模型（MLLM）集成到该控制管道中的可能性，以赋予机器人更高层的语义理解和任务规划能力，同时保持底层控制的能效优势。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对高维连续机器人控制中能量效率与计算性能难以平衡的核心问题，提出了一种结合群体编码脉冲神经网络（SNN）和深度强化学习（DRL）的新框架。关键技术是群体编码脉冲行动者网络（PopSAN），它将高维观测编码为神经元群体活动，通过基于梯度的更新实现策略优化。实验在Isaac Gym平台使用PixMC等基准测试，结果表明该方法在能量效率、延迟降低和连续动作空间稳健性方面均有显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.10516" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>