<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.11464" target="_blank" rel="noreferrer">2602.11464</a></span>
        <span>作者: Qin Jin Team</span>
        <span>日期: 2026-02-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习通常受限于收集大规模真实世界数据的高昂成本。这一挑战对于旨在家庭使用的低成本机器人尤为突出，因为它们必须兼具用户友好性和可负担性。当前主流方法包括专家遥操作，其能提供高质量数据但受限于昂贵设备和复杂操作；以及展现出强大零样本能力的视觉-语言-动作模型，但其在多样家庭场景中实现精确可靠操作通常仍需领域内微调，凸显了对可扩展、低成本数据获取的迫切需求。利用普通人视频进行模仿学习是一个有前景的方案，但其落地面临两大根本差距：1) 视觉外观差距：人手与机器人夹爪在纹理和形状上完全不同；2) 动作空间差距：人臂与机器人机械臂的运动学结构、关节限制和工作空间差异巨大。现有工作试图解决这些问题，但往往依赖计算昂贵的图像生成技术或需要昂贵的手部追踪硬件，违背了低成本家庭机器人的目标。本文针对数据成本高和人-机器人具身差距的挑战，提出了EasyMimic框架，一个仅使用消费级设备的简单高效模仿学习框架。其核心思路是：通过3D手部姿态估计提取关键运动信息并设计稳定重定向算法映射到机器人动作；在视觉层面采用轻量级视觉增强策略，通过随机颜色渲染手部网格迫使模型学习跨具身通用模式；然后结合处理过的人类数据和少量机器人数据对模型进行协同训练。</p>
<h2 id="方法详解">方法详解</h2>
<p>EasyMimic框架旨在使低成本机器人能高效利用人类视频演示。整体框架包含三个核心组件：1) 使用消费级硬件低成本收集人类和机器人演示数据；2) 跨动作和视觉领域的物理对齐以弥合具身差距；3) 有效融合人类和机器人数据以训练统一策略模型的协同训练策略。</p>
<p><img src="https://arxiv.org/html/2602.11464v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：EasyMimic框架总览。左侧为数据收集与处理流程：从人类RGB视频中提取3D手部关键点和网格；通过动作空间对齐模块将手部运动重定向为机器人末端执行器位姿；通过视觉空间对齐模块对手部网格进行随机颜色增强。右侧为协同训练策略：处理后的混合数据（人类与机器人）输入共享的视觉语言编码器和DiT模块，但使用独立的动作编码器/解码器处理不同具身的数据。</p>
</blockquote>
<p><strong>数据收集系统与硬件设计</strong>：遵循低成本与可访问性原则。机器人平台采用升级为6自由度（通过增加一个关节）的LeRobot SO100-Plus机械臂。数据收集使用Nintendo Joy-Con进行机器人遥操作，并部署两个消费级RGB摄像头：一个固定为第一人称视角（人类与机器人数据收集共享），另一个安装在机器人手腕以捕捉近距离操作细节。人类视频处理利用先进的HaMeR模型从单目输入中提取3D手部信息，为每帧视频精确重建手部形态，提供相机坐标系下的21个手部关键点坐标和778个手部网格顶点坐标。</p>
<p><strong>物理对齐</strong>：该模块系统性地解决动作空间和视觉空间的对齐问题。</p>
<p><img src="https://arxiv.org/html/2602.11464v1/x2.png" alt="物理对齐"></p>
<blockquote>
<p><strong>图2</strong>：物理对齐过程。从视频中提取人手关键点和网格。手部运动通过动作空间对齐模块重定向为机器人动作，同时手部网格通过视觉空间对齐模块进行增强，以弥合人与机器人之间的物理差距。</p>
</blockquote>
<p><em>动作空间对齐</em>：目标是将人手关键点序列映射为机器人末端执行器位姿序列。位置对齐选择鱼际隆起中心（拇指近端指间关节与食指掌指关节的中点）作为重定向锚点，因其在抓取过程中相对稳定。方向对齐通过食指四个关节和拇指PIP关节这五个关键点拟合一个平面，以平面法向量定义Z轴，食指MCP到PIP的向量定义X轴，构建完整的3D坐标系并转换为方向表示。夹爪状态对齐计算拇指与食指尖端的欧氏距离，并将其归一化到[0, 1]范围作为连续的夹爪状态。对齐后，使用预先标定的手眼变换矩阵将状态从相机坐标系转换到机器人基座坐标系。最后，定义动作为下一时刻的状态，并形成未来h个动作组成的动作块。</p>
<p><em>视觉空间对齐</em>：提出一种轻量级的视觉增强策略，基于域随机化思想。其核心目标是迫使模型忽略与任务无关的表面特征（如颜色和皮肤纹理），转而学习更基础的几何信息（如手部姿态和形状）。具体而言，重用提取的3D手部网格，在渲染到原始图像上时，对整个手部应用随机颜色变换，生成外观标准化的增强视图。通过在训练中使用这种增强数据，模型被被动地暴露于多样的具身形态中，从而在数据层面引导模型学习跨具身的视觉表示。</p>
<p><strong>训练策略</strong>：采用协同训练策略以有效利用人类和机器人数据。将处理过的人类演示数据集与少量真实世界机器人数据集混合。所有数据首先通过一个共享的冻结视觉语言编码器，然后输入一个共享的扩散Transformer模块进行跨具身策略学习。为处理数据不平衡，使用平衡采样确保每个小批量包含来自人类和机器人源的数据。针对人类视频缺少手腕摄像头视图的问题，对相应的图像数据进行零填充以保持输入维度一致。此外，虽然核心DiT模块共享，但为每个具身（人类和机器人）设计了独立的状态和噪声动作编码器与解码器，以灵活处理两种数据类型在维度、数值范围和物理含义上的差异，从而实现更稳定的共享策略学习过程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：硬件平台为6-DoF so100-plus机械臂，配备两指夹爪。视觉系统包括两个单目RGB摄像头：一个固定在机器人基座上方提供全局俯视图，另一个安装在手腕提供以末端执行器为中心的第一人称视图。在四个桌面操作任务上评估方法：拾取放置、拉推抽屉、堆叠、语言条件任务。使用预训练的Gr00T N1.5-3B作为基础VLA模型。策略网络配置为输出绝对动作。数据收集方面，每个任务收集100条人类视频演示和20条机器人遥操作轨迹。</p>
<p><strong>基准方法</strong>：包括仅使用机器人数据（10条轨迹）、仅使用机器人数据（20条轨迹）、预训练-微调（先在处理过的人类视频数据上预训练，然后在机器人数据上微调）。</p>
<p><strong>关键实验结果</strong>：如表II所示，仅使用少量机器人数据（10或20条轨迹）性能有限，平均成功率分别为0.26和0.51。结合人类数据通过预训练-微调方法平均得分为0.75。EasyMimic框架在所有任务上取得最佳性能，平均得分为0.88，超过预训练-微调方法0.13分，超过仅机器人（10轨迹）基线0.62分。在语言条件任务中，仅使用20条机器人轨迹得分为0.40，而EasyMimic通过结合人类数据将得分提升至0.90。</p>
<p><img src="https://arxiv.org/html/2602.11464v1/x4.png" alt="数据规模效应"></p>
<blockquote>
<p><strong>图4</strong>：数据集规模的影响。(a) 在固定机器人数据（10条轨迹）情况下，变化人类数据量。增加人类演示数量持续提升所有任务的性能，但在超过50条演示后收益递减。(b) 在固定人类数据（50条视频）情况下，变化机器人数据量。性能在轨迹数从5增至10时显著提升，之后快速饱和。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><em>对齐模块效果</em>：如表III所示，移除动作对齐导致平均性能下降0.27分，在需要精确手部旋转的任务中尤为明显。移除视觉对齐导致平均得分骤降0.47分，表明没有视觉增强，跨具身知识转移严重受阻。</li>
<li><em>视觉对齐设计</em>：如表IV所示，仅对拇指和食指进行部分掩码的策略性能很差（平均0.27分），说明完整的手部视觉增强对于弥合视觉差距至关重要。</li>
<li><em>独立动作头</em>：如表V所示，使用独立动作头比使用共享动作头的变体平均性能高0.40分，表明显式分离头部可以防止人类和机器人数据间的干扰。</li>
<li><em>预训练影响</em>：如表VI所示，结合EasyMimic与预训练动作专家取得了最高分0.87，突出了人类演示数据和预训练的互补优势。仅依赖预训练而不利用人类视频效果仍然差得多。</li>
<li><em>泛化到未见物体</em>：如表VII所示，在拾取放置任务中，EasyMimic在未见颜色（绿鸭子）和未见几何形状与功能（粉立方体）的物体上均优于仅机器人基线，平均得分为0.65对0.35，表明利用人类演示数据使模型能够学习超越训练所见物体的更泛化的任务策略。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.11464v1/x5.png" alt="失败案例分析"></p>
<blockquote>
<p><strong>图5</strong>：不同任务的失败案例分析。(a) 拾取放置中提前释放夹爪。(b) 抽屉操作中把手抓取不精确。(c) 堆叠过程中碰撞导致物体掉落。(d) 放置不稳定导致物体倾倒。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11464v1/x6.png" alt="视觉对齐可视化"></p>
<blockquote>
<p><strong>图6</strong>：视觉对齐模块可视化。(a) 原始人手图像。(b) 对拇指和食指进行部分掩码。(c) 对手部区域进行完整掩码。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了一个完整、低成本的流水线，支持从标准RGB摄像头捕获的人类演示中进行机器人策略训练。2) 设计了一个动作对齐模块，能有效将3D人手轨迹重定向为可执行的机器人动作。3) 采用了一种基于手部颜色随机化的轻量级视觉增强策略来缓解视觉差距。4) 系统性的评估表明，在人类数据和有限机器人数据上的协同训练能够弥合领域差距并显著提高任务性能。</p>
<p>论文自身提到的局限性主要体现在失败案例分析中：机器人偶尔因对空间关系理解不足而在到达目标位置前释放夹爪；在抽屉操作中有时无法准确抓取把手，特别是当把手方向与训练示例不同时；在堆叠操作中可能意外碰倒已有物体，显示出空间意识的局限；有时将物体放置在边缘附近的不稳定位置导致后续掉落。</p>
<p>本研究对后续工作的启示在于：为低成本机器人学习提供了一个切实可行的框架，显著降低了对昂贵机器人数据收集的依赖。其轻量化的对齐策略（特别是视觉增强）为跨具身模仿提供了高效方案。协同训练策略展示了如何有效结合丰富但存在领域差异的人类先验知识与精确但稀缺的机器人数据。未来工作可以探索更高效的人类数据利用方式、结合更强大的基础模型，以及将该框架扩展到更复杂的多阶段或非结构化环境任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对低成本家用机器人模仿学习数据收集成本高、人类与机器人存在视觉外观和动作空间差距的核心问题，提出EasyMimic框架。该方法从RGB视频提取3D手部轨迹，通过动作对齐模块映射到机器人夹爪控制空间，并采用手部视觉增强策略弥合领域差距；结合协同训练，利用人类视频和少量机器人数据微调模型。在LeRobot平台上的实验表明，该框架在各种操作任务中实现高性能，显著降低对昂贵机器人数据收集的依赖。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.11464" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>