<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.02906" target="_blank" rel="noreferrer">2512.02906</a></span>
        <span>作者: Yang, Fan, Zhang, Kaihao</span>
        <span>日期: 2025/12/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>多模态大语言模型（MLLMs）在处理高分辨率图像时面临显著挑战。主流方法通常将输入图像调整至固定的低分辨率，这会导致细节丢失和形状失真。近期，一种“定位-放大”策略被广泛采用以增强细节感知。其中，基于检索增强生成（RAG）的训练免费方法（如RAP）通过将高分辨率图像分割为小块（crops），计算每个小块与查询的语义相似度，并选择最相关的部分来定位目标并抑制无关信息，取得了显著进展。然而，此类基于图像块的方法存在关键局限：首先，分割操作可能将完整物体碎片化到多个不相交的小块中，破坏其整体语义，导致相似度计算出现偏差；其次，图像块的分辨率是一个关键但难以调优的超参数，过大或过小都会影响性能；最后，在背景杂乱的高分辨率图像中，相似度度量容易出现误报。</p>
<p>本文针对RAP等方法中因物体被分割导致的语义相似度偏差以及全局定位不精确的痛点，提出了多分辨率检索-检测融合（MRD）的新视角。其核心思路是：通过多分辨率语义融合校正因物体分割造成的相似度偏差，并引入开放词汇目标检测模型提供全局定位置信度，将两者融合以更精确地引导MLLMs关注高分辨率图像中的关键区域。</p>
<h2 id="方法详解">方法详解</h2>
<p>MRD是一个无需训练、通用的高分辨率图像理解框架。其整体目标是通过融合多分辨率语义信息和开放词汇检测结果，生成更准确的目标区域定位图，以指导后续的检索-探索搜索过程，为MLLM提供最相关的图像区域。</p>
<p><img src="https://arxiv.org/html/2512.02906v2/fig/MRD_framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图4</strong>：MRD框架详解。首先，使用VisRAG以不同分辨率划分图像块，获得多分辨率语义相似度图。同时，利用开放词汇检测模型LLMDet，通过滑动窗口方法在全图检测从查询中提取的目标对象，生成全局检测置信度图。最后，将多分辨率语义相似度图与检测置信度图进行线性融合，融合后的分数用于指导后续搜索，选择包含目标对象的图像块。</p>
</blockquote>
<p>框架包含两个核心模块：</p>
<ol>
<li><strong>多分辨率语义融合</strong>：为缓解单分辨率分割导致的语义偏差，该方法在高低两种比例分辨率下计算语义相似度。设低分辨率图像块集合为 $P = {p_1, p_2, ..., p_n}$，高分辨率块集合为 $\hat{P} = {\hat{p}_1, \hat{p}_2, ..., \hat{p}_m}$，且满足 $\hat{l} = k \cdot l$，$n = k^2 \cdot m$。每个高分辨率块 $\hat{p}<em>i$ 对应 $k^2$ 个低分辨率块 $\tilde{p}</em>{i,j}$。分别计算查询与高、低分辨率图像块嵌入的余弦相似度，得到相似度集合 $\hat{S}$ 和 $S$。通过映射函数 $H(\cdot)$ 将高分辨率相似度 $\hat{S}$ 映射到与低分辨率网格对齐的 $\tilde{S}$。最后，通过一致性融合（公式：$s^f_t = \sqrt{\tilde{s}_t \cdot s_t}$）得到校正后的多分辨率语义相似度 $S^f$，并转换为二维语义相似度图 $s^f(i,j)$。该融合能在低分辨率视图下物体被分割时，通过高分辨率信息增强物体各部分的相似度，尽可能保持物体的完整性。</li>
<li><strong>开放词汇检测器增强</strong>：为更直接地进行全局目标定位，该方法引入开放词汇检测模型LLMDet。首先，利用大语言模型（LLM）的上下文学习能力从查询中提取主要目标对象，作为检测器的目标类别。由于图像分辨率极高，采用滑动窗口策略遍历全图：将图像划分为 $H \times W$ 的非重叠网格，使用大小为 $h \times w$ 的窗口以一定步长滑动，获得多个窗口 $W_t$。在每个窗口内使用LLMDet检测目标，生成边界框及置信度，经过阈值过滤后，为每个窗口生成局部检测置信度图 $\mathbf{c}^w_t$。最后，通过平均融合所有滑动窗口的贡献，生成与语义图对齐的全局检测置信度图 $\mathbf{c}^g(i,j)$。检测置信度图提供了全局尺度的直接空间定位指导。</li>
</ol>
<p>最终，通过线性组合将多分辨率语义相似度图与检测置信度图融合，得到最终的引导分数图：$s^F(i,j) = (1-w) \cdot s^f(i,j) + w \cdot c^g(i,j)$。参数 $w$ 用于平衡两者贡献。此融合既能实现精确的目标定位，又能突出目标物体内部的细微差异，从而在后续的检索-探索搜索过程中更准确地提取关键区域。</p>
<p>与现有方法（如RAP）相比，MRD的创新点具体体现在：1）首次系统性地利用多分辨率融合策略来校正因物体碎片化导致的语义相似度偏差；2）首次引入开放词汇目标检测器为MLLMs的高分辨率图像理解提供全局、直接的定位能力，并与语义信息形成协同增强。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个高分辨率理解基准上进行评估：1) <em><em>V</em> Bench</em>*，平均分辨率2246×1582，包含属性识别和空间推理两个子任务；2) <strong>HRBench</strong>，包含4K和8K两个版本，每个版本包含细粒度单实例感知（FSP）和细粒度跨实例感知（FCP）子任务。使用的MLLMs包括LLaVA-v1.5-7B和LLaVA-ov-0.5B。</p>
<p>对比的基线方法包括：原始MLLMs、训练免费方法Zoom Eye以及当前最先进的训练免费方法RAP。</p>
<p><img src="https://arxiv.org/html/2512.02906v2/fig/Resolution.png" alt="结果对比表"></p>
<blockquote>
<p><strong>表1</strong>：MRD与现有方法在高分辨率基准上的对比结果。MRD在所有数据集、模型配置和子任务上均取得了最佳或极具竞争力的性能。</p>
</blockquote>
<p>关键实验结果：如表1所示，MRD相较于基线MLLMs和之前的先进方法，在所有子任务、数据集和模型配置上均带来了显著的性能提升。在使用LLaVA-v1.5-7B模型的V<em>数据集上，MRD取得了95.6%的整体准确率，相比原始模型（48.7%）绝对提升了46.9%，性能近乎翻倍。与当前最强的训练免费基线RAP相比，MRD在V</em>（LLaVA-v1.5-7B）、HR-Bench 4K（LLaVA-ov-0.5B）和HR-Bench 8K（LLaVA-ov-0.5B）上分别取得了4.5%、3.3%和1.0%的绝对提升，平均提升2.8%。MRD在单对象任务（如V*的属性识别、HRBench的FSP）上表现尤为突出，这归因于检测模块为孤立对象提供了更精确的定位。</p>
<p><img src="https://arxiv.org/html/2512.02906v2/fig/multi_res.png" alt="模块效果可视化"></p>
<blockquote>
<p><strong>图5</strong>：MRD中不同模块效果的定性可视化。上半部分展示了多分辨率语义融合的效果：单低分辨率（左）或单高分辨率（中）的相似度图存在偏差或噪声，而融合后（右）能更完整、准确地突出目标物体（消防栓）。下半部分展示了检测器增强的效果：检测置信度图（中）能直接定位目标区域（多个甜甜圈），与语义图（左）融合后（右）能进一步细化并突出关键区域。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.02906v2/fig/Weight.png" alt="消融实验图1"></p>
<blockquote>
<p><strong>图9</strong>：检测置信度权重 $w$ 的消融实验。结果显示，在V*和HR-Bench 4K数据集上，适中的权重（如0.3-0.5）能取得最佳性能，验证了语义信息与检测信息融合的有效性。权重为0或1时性能下降，说明两者缺一不可。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.02906v2/fig/Window_Size.png" alt="消融实验图2"></p>
<blockquote>
<p><strong>图10</strong>：滑动窗口大小的消融实验。窗口大小对检测模块的性能有显著影响。实验表明，在HR-Bench 4K上，窗口大小为图像短边的1/4时效果最佳，过小或过大都会导致性能下降。</p>
</blockquote>
<p>消融实验总结：论文通过系统性的消融实验验证了各个组件的贡献。关键发现包括：1) <strong>多分辨率融合</strong>：相比单一分辨率，多分辨率融合策略能稳定提升性能；2) <strong>检测器增强</strong>：引入开放词汇检测器带来了显著的性能增益，特别是在单对象任务上；3) <strong>融合权重</strong>：语义图与检测图的线性融合权重存在一个最优区间（约0.3-0.5），证明两者协同工作的必要性；4) <strong>检测窗口大小</strong>：滑动窗口的大小是影响检测效果的关键参数，需要根据图像尺寸进行调整。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>首创性地系统整合开放词汇目标检测器</strong>来增强MLLMs的高分辨率图像理解，通过精确的全局定位有效抑制了无关区域干扰；2) <strong>提出了多分辨率语义融合方法</strong>，通过一致性融合校正了因物体分割导致的语义相似度偏差，更好地保持了目标物体的完整性；3) <strong>构建了一个无需训练且通用的MRD框架</strong>，在多个高分辨率基准和不同MLLMs上实现了领先性能，验证了其有效性和泛化能力。</p>
<p>论文自身提到的局限性包括：1) <strong>计算成本</strong>：该方法涉及多分辨率语义计算和滑动窗口检测，增加了额外的计算开销；2) <strong>依赖外部模型</strong>：其性能部分依赖于所使用的视觉RAG模型（VisRAG）和开放词汇检测器（LLMDet）的质量。</p>
<p>对后续研究的启示：MRD展示了将传统计算机视觉任务（如目标检测）与基于检索的MLLM增强策略相结合的有效性。未来的工作可以探索更高效的融合机制、设计端到端的可训练架构以降低推理成本，或者研究如何自适应地确定最优的分辨率比例和滑动窗口参数，以进一步提升方法的实用性和自动化程度。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多模态大语言模型理解高分辨率图像时，因图像分块处理导致物体被割裂、语义相似性计算失准的核心问题，提出了一种无需训练的多分辨率检索-检测融合框架。其关键技术包括：1）多分辨率语义融合，通过整合不同分辨率下的语义相似性图，以保持目标物体的完整性；2）引入开放词汇目标检测模型，通过滑动窗口在全局范围直接定位物体区域。实验在多个高分辨率图像理解基准测试上证明了该方法的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.02906" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>