<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.09958" target="_blank" rel="noreferrer">2511.09958</a></span>
        <span>作者: Changbo Wang Team</span>
        <span>日期: 2025-11-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型在机器人操作领域取得了显著进展。然而，现有方法主要依赖于单一的视觉感知模态，这带来了根本性的限制。视觉感知无法充分捕捉操作任务中固有的丰富动态信息，例如接触事件和交互反馈，而这些对于精确的操作控制至关重要。尽管先前的工作尝试使用触觉信号来补偿VLA的动态信息感知不足，但触觉传感器价格昂贵、部署困难，且其低采样频率限制了其对工具-物体交互和操作动态的感知能力。</p>
<p>相比之下，接触音频提供了一种互补的感知模态，能够提供关于物体交互、材料属性和操作过程中动态过程的丰富信息。音频信号能够穿透遮挡，并捕捉视觉难以感知的时间动态。接触音频利用接触式麦克风捕捉物体交互产生的振动信号，其频率比传统触觉传感器高1000倍，且不受光照和颜色等环境影响。然而，将音频信息集成到现有VLA框架中存在技术挑战，例如如何从高频接触音频中提取接触事件信息，以及缺乏音频增强的训练和评估环境。</p>
<p>本文针对VLA模型在感知物体交互和动态过程方面的视觉局限性，提出了Audio-VLA，一种结合声学和视觉感知的多模态操作策略。其核心思路是通过引入接触音频感知，使模型能够提取接触事件信息以克服视觉感知的局限，并提出了任务完成率指标来系统评估动态操作过程。</p>
<h2 id="方法详解">方法详解</h2>
<p>Audio-VLA的整体目标是将接触音频感知集成到VLA框架中，以增强对操作动态过程的理解。模型的输入包括自然语言指令、视觉观察（第三人称和腕部摄像头图像）、接触音频信号以及机器人本体感状态，输出是未来K个时间步的连续机器人动作。</p>
<p><img src="https://arxiv.org/html/2511.09958v1/x2.png" alt="方法架构"></p>
<blockquote>
<p><strong>图2</strong>：Audio-VLA的架构。模型由多模态编码器（音频、视觉和本体感模块）、将异构特征映射到统一表示空间的多模态投影器、作为骨干网络的7B参数Llama2语言模型，以及用于生成连续动作的四层MLP动作头组成。</p>
</blockquote>
<p><strong>整体框架与核心模块</strong>：</p>
<ol>
<li><p><strong>多模态编码器</strong>：负责处理不同模态的原始输入。</p>
<ul>
<li><strong>视觉编码器</strong>：采用预训练的DINOv2和SigLIP作为双视觉编码器，以捕获丰富的视觉特征和空间理解。使用LoRA进行微调以适应特定任务。输入为每个时间步t的第三人称视图图像 \(I_t^{3rd}\) 和腕部摄像头图像 \(I_t^{wrist}\)。提取的视觉特征 \(\mathbf{F}^{vis}\) 通过拼接得到。</li>
<li><strong>音频编码器</strong>：采用预训练的AudioCLIP作为音频编码器，并在机器人操作数据集ManiWAV上进行了额外训练，同样使用LoRA微调以优化接触事件检测。为了充分挖掘高频接触事件信息，论文对AudioCLIP中的频率B样条投影层进行了优化，减少了跳跃长度和窗口长度，以在保持足够频率分辨率的同时增强时间分辨率。输入为每个时间步t的音频信号 \(\mathcal{S}_t\)。处理流程包括FBSP层生成复数值频谱图、功率谱计算、对数缩放，最后通过ResNeXt层提取音频特征嵌入 \(\mathbf{F}^{aud}\)。</li>
<li><strong>本体感编码器</strong>：通过一个MLP层 \(\phi_{state}(\cdot)\) 处理机器人状态 \(\mathbf{p}_t\)（如关节角度），得到状态嵌入 \(\mathbf{F}^{prop}\)。</li>
</ul>
</li>
<li><p><strong>多模态投影器</strong>：将不同模态的特征映射到语言模型统一的嵌入空间。包含三个投影函数：视觉投影 \(\phi_v\)（线性变换）、音频投影 \(\phi_a\)（三层MLP）和本体感投影 \(\phi_p\)（两层MLP）。投影后的特征沿序列维度拼接，形成统一的时序表示 \(\mathbf{X}\)，保留了模态内的时间连续性并建立了跨模态的时间对齐。</p>
</li>
<li><p><strong>语言模块</strong>：基于7B参数的Llama2架构，负责整合多模态感官数据与自然语言指令进行认知推理。首先将语言指令l通过分词器和嵌入层转换为语言令牌 \(\mathbf{E}<em>{lang}\)。然后将语言令牌、统一的多模态表示 \(\mathbf{X}\) 以及 \(K \cdot D\) 个可学习的空动作嵌入 \(\mathbf{E}</em>{empty}\) 拼接，形成完整的输入序列 \(\mathbf{X}<em>{in}\)，送入Llama2进行并行解码，得到解码后的隐藏状态序列 \(\mathbf{H}</em>{dec}\)。</p>
</li>
<li><p><strong>动作头</strong>：一个四层MLP \(\phi_{act}\)，独立处理从 \(\mathbf{H}<em>{dec}\) 中提取的动作隐藏状态 \(\mathbf{H}</em>{act}\)，生成K个连续动作，最终重塑为动作块 \(\hat{\mathbf{A}}_t\)。</p>
</li>
</ol>
<p><strong>训练目标</strong>：模型通过最小化预测动作块 \(\hat{\mathbf{A}}_t\) 与专家演示的真实动作块 \(\mathbf{A}^*_t\) 之间的平均L1损失来训练。</p>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>多模态融合</strong>：首次将接触音频感知系统性地集成到VLA框架中，形成视觉-音频-语言-动作的统一模型。</li>
<li><strong>高频音频处理</strong>：优化音频编码过程（调整FBSP参数），以实现对高频接触事件的细粒度时间建模和瞬时检测。</li>
<li><strong>时序对齐</strong>：多模态投影器和输入序列的构建方式强调了跨模态的时间同步，使LLM能够理解多模态信息的时序演变和因果关系。</li>
</ul>
<p><strong>音频增强的仿真环境</strong>：为了训练和评估Audio-VLA，论文在LIBERO和RLBench仿真环境中集成了基于物理碰撞检测触发的真实音频反馈。通过在实际物体上执行等效操作并录制接触音频，构建了一个按材料对、交互类型和力大小索引的音频库。仿真中检测到碰撞时，会根据碰撞参数查询音频库并动态调制播放。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真基准</strong>：使用音频增强后的LIBERO和RLBench（5个任务：关罐子、插入方钉、放入灯泡、打开抽屉、放入物品到抽屉）仿真环境。</li>
<li><strong>真实世界任务</strong>：设计了两个具有不同接触动态的任务：“擦除所有白板笔迹”和“舀取5克燕麦片”，使用Mobile ALOHA平台进行实验。</li>
<li><strong>对比方法</strong>：包括π0-FAST、CoT-VLA、OpenVLA-OFT等先进的VLA模型。</li>
<li><strong>评估指标</strong>：除了任务成功率，本文提出了<strong>任务完成率</strong>（TCR），用于量化操作执行过程中动态过程感知反馈能力，衡量已取得进展与任务目标的比率。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.09958v1/x4.png" alt="实验结果表"></p>
<blockquote>
<p><strong>图4/表I</strong>：Audio-VLA与对比方法在LIBERO和RLBench基准上的任务成功率（%）。在标准环境和域偏移条件下，Audio-VLA在平均成功率上均优于所有对比方法，特别是在接触密集的RLBench Task2和Task3上优势明显。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>仿真实验</strong>：<ul>
<li>在标准环境下，Audio-VLA在LIBERO上达到97.6%的平均成功率，在RLBench上达到55.1%，均优于所有对比方法。</li>
<li>在接触密集型任务（RLBench Task2和Task3）上，Audio-VLA相比第二好的OpenVLA-OFT分别高出2.7%和10.2%。</li>
<li>在域偏移（光照和桌面材质颜色变化）条件下，Audio-VLA同样保持领先，证明了其更好的泛化能力。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.09958v1/x5.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图5/表II</strong>：真实世界操作任务性能。Audio-VLA在“已见”和“未见”环境下的成功率和TCR指标均显著优于基线方法，成功率达到基线方法的至少三倍。</p>
</blockquote>
<ol start="2">
<li><strong>真实世界实验</strong>：<ul>
<li>在“已见”环境下，Audio-VLA在EAWM和S5GO任务上的成功率分别达到60%和30%，而最佳基线方法仅为20%和10%。</li>
<li>在“未见”环境下（改变笔迹形状或燕麦种类），Audio-VLA仍能取得30%和20%的成功率，而基线方法性能急剧下降甚至为零。</li>
<li>在TCR指标上，Audio-VLA也全面领先，表明其能更好地理解并推进动态操作过程。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.09958v1/x3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3/表III &amp; IV</strong>：消融研究结果。左图展示了真实世界实验的硬件设置和任务；右表（对应论文中的表III和表IV）显示，移除音频模态（Vision-only）或LoRA微调（w/o LoRA）都会导致性能显著下降，验证了音频感知和LoRA微调的有效性。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：<ul>
<li><strong>移除音频（Vision-only）</strong>：在RLBench上平均成功率从55.1%下降至48.0%；在真实世界EAWM任务上，成功率从60%骤降至20%，TCR从73%降至46%。这证明了接触音频感知的关键作用。</li>
<li><strong>移除LoRA微调（w/o LoRA）</strong>：在RLBench和真实世界任务上的性能也出现明显下降，表明对预训练编码器进行轻量级适配对于实现鲁棒的跨模态理解是必要的。</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了Audio-VLA</strong>：第一个将接触音频感知集成到VLA框架中的多模态操作策略，通过接触音频克服了纯视觉VLA在感知交互动态和接触事件方面的根本局限。</li>
<li><strong>引入了TCR评估指标</strong>：提出了任务完成率这一新指标，能够系统性地评估机器人对操作动态过程的理解和感知能力，而不仅仅是最终结果。</li>
<li><strong>创建了音频增强的仿真环境</strong>：为LIBERO和RLBench添加了基于碰撞的逼真音频生成，为多模态操作研究提供了新的训练和评估平台。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，Audio-VLA的性能依赖于高质量的接触音频数据和有效的跨模态对齐。此外，处理音频信号可能会增加一定的计算开销。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>多模态融合的深化</strong>：Audio-VLA验证了音频在操作中的价值，启发研究者探索其他模态（如力觉、热觉）与VLA的融合。</li>
<li><strong>动态过程评估的标准化</strong>：TCR指标的提出呼吁社区关注对操作过程而不仅仅是结果的评估，可能推动更细粒度的评估体系发展。</li>
<li><strong>仿真到实物的迁移</strong>：论文中构建音频仿真环境的方法为在其他模拟器中集成多模态感知提供了可行路径，有助于降低实物实验成本。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中仅依赖视觉的VLA模型在感知接触事件和动态过程方面的局限性，提出Audio-VLA模型。该模型通过集成接触音频感知，采用DINOv2、SigLIP和AudioCLIP分别编码视觉与音频，以Llama2为骨干，并利用LoRA微调和多模态投影层实现跨模态对齐。在增强音频反馈的仿真环境（RLBench、LIBERO）和真实任务上的实验表明，Audio-VLA性能优于纯视觉方法，所提的任务完成率（TCR）指标有效量化了动态过程感知能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.09958" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>