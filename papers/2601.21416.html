<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.21416" target="_blank" rel="noreferrer">2601.21416</a></span>
        <span>作者: Liming Chen Team</span>
        <span>日期: 2026-01-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前最先进的机器人视觉运动策略学习主要依赖于从预训练视觉模型中提取的全局特征或密集特征。全局特征通过池化操作将整张图像压缩为单个向量，而密集特征则保留空间细节，提供逐块的嵌入。然而，这两种方法都存在关键局限性：它们往往将任务相关与无关的信号纠缠在一起，并且可能过度关注图像的特定区域，导致策略在面对光照变化、新纹理或环境杂乱等视觉分布偏移时显得脆弱。</p>
<p>本文针对现有视觉表示在泛化能力上的不足，提出采用结构化、以对象为中心的表示作为更优的替代方案。具体而言，本文聚焦于<strong>基于槽的对象中心表示</strong>，其核心思路是通过一个额外的Slot Attention层，将密集特征重组为一小组“类对象”的向量（槽），从而解耦场景，使策略能够聚焦于任务相关的实体，忽略无关的视觉变化。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文构建了一个统一的策略框架，用于公平地比较全局、密集和对象中心三类视觉表示。整体流程如下：首先，一个<strong>冻结的预训练视觉模型</strong>从原始图像中提取视觉特征；然后，一个<strong>基于Transformer的观察主干</strong>将这些视觉特征与其他模态（如语言指令、本体感知）以及可学习的动作令牌进行整合；最后，一个<strong>策略头</strong>根据最终的动作令牌预测下一步动作。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：机器人操作策略架构概述。我们使用预训练的视觉模型从原始图像中提取视觉特征。这些特征随后与其他模态（例如语言指令、本体感知）在带有额外动作令牌的观察主干中结合。最终，策略头根据该额外令牌预测下一个动作。我们实验了产生全局、密集或对象中心表示的不同类型视觉模型。</p>
</blockquote>
<p>核心模块是<strong>基于槽的对象中心表示</strong>的生成与集成。给定输入图像，首先通过视觉主干（如DINOv2）编码为密集特征令牌。随后，<strong>Slot Attention模块</strong>通过迭代的、具有竞争机制的交叉注意力，将这些密集特征绑定到一个紧凑的槽集合中。该过程鼓励不同的槽专注于输入图像的不同部分，从而产生结构化的、实体级别的表示。本文在DINOSAUR方法基础上进行了改进：使用DINOv2作为主干，并引入一个Transformer层在时间步之间递归传递信息，以处理视频序列，改进后的模型称为DINOSAUR*。</p>
<p>与现有方法相比，创新点具体体现在：1) <strong>结构化表示的引入</strong>：与扁平的全局或密集特征不同，SOCRs明确地将场景分解为对象实体，提供了组合性的归纳偏置。2) <strong>与策略学习的无缝集成</strong>：设计的策略架构能够灵活地将槽集合作为令牌序列处理，使Transformer能够利用其组合结构，同时确保与处理其他特征类型的公平性。3) <strong>机器人领域预训练</strong>：除了在通用数据集（COCO）上预训练DINOSAUR<em>，本文还收集了包含超过18.8万条轨迹的机器人视频数据集（BridgeData V2, Fractal, DROID）对模型进行预训练，得到DINOSAUR-Rob</em>，以弥合与机器人操作领域的分布差距。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在模拟和真实世界环境中进行了系统性评估。使用的基准/数据集包括：模拟环境<strong>MetaWorld</strong>（桌面操作）和<strong>LIBERO-90</strong>（复杂多物体场景）；真实世界环境使用<strong>Franka机械臂</strong>执行四项桌面操作任务。实验平台涉及上述仿真环境和真实机器人系统。</p>
<p>对比的基线方法涵盖了七种预训练视觉模型：ResNet-50（密集）、R3M（全局）、DINOv2（密集）、VC-1（全局）、Theia（密集）、SAM+DINOv2（分割驱动对象中心）、以及本文提出的DINOSAUR<em>和DINOSAUR-Rob</em>（SOCRs）。所有视觉编码器在策略训练时均保持冻结。</p>
<p>关键实验结果如下：</p>
<ul>
<li><strong>策略学习效能（Q1）</strong>：在LIBERO复杂多物体场景和真实世界任务中，基于SOCRs的策略（尤其是DINOSAUR-Rob<em>）达到了与最佳密集或全局基线相当或更优的总体性能。在真实世界中，DINOSAUR-Rob</em>取得了56%的成功率，DINOSAUR*为48%，显著优于DINOv2（28%）。而朴素的分割驱动表示（SAM+DINOv2）在所有设置中均未能学习到有效策略。</li>
<li><strong>分布偏移下的泛化（Q2）</strong>：在应对干扰物、新纹理和光照变化时，SOCRs表现出显著更强的鲁棒性。在MetaWorld中，DINOSAUR<em>和DINOSAUR-Rob</em>在纹理和光照变化场景下大幅超越所有基线。在真实世界中，DINOSAUR-Rob*在泛化场景下的平均成功率达到41%，远超其他方法（ResNet-50为12%）。</li>
</ul>
<p><img src="https://..." alt="整体成功率"></p>
<blockquote>
<p><strong>图4</strong>：在域内和泛化场景下的整体成功率。每个视觉模型在MetaWorld（左）、LIBERO（中）和使用Franka的真实机器人（右）所有任务上的平均成功率。绿点：域内性能；橙点：所有泛化场景（干扰物、新纹理、光照变化）的平均性能；红色数字：从域内到泛化设置的性能相对下降百分比。</p>
</blockquote>
<p><img src="https://..." alt="MetaWorld泛化结果"></p>
<blockquote>
<p><strong>表II</strong>：MetaWorld泛化结果。比较MetaWorld中不同泛化水平。SOCRs模型在纹理和光照变化下表现突出。</p>
</blockquote>
<p><img src="https://..." alt="真实世界泛化结果"></p>
<blockquote>
<p><strong>表III</strong>：真实世界泛化结果。比较真实机器人中的不同泛化水平。SOCRs模型在两种泛化场景下均大幅领先。</p>
</blockquote>
<p>消融实验体现在机器人预训练的效果上。DINOSAUR-Rob<em>（经过机器人数据预训练）在大多数情况下性能优于仅在COCO上预训练的DINOSAUR</em>，特别是在真实世界泛化任务中优势明显，这验证了在高质量、多样化的机器人数据上进行预训练能进一步放大SOCRs的优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 首次在模拟和真实机器人操作任务中，对全局、密集和对象中心视觉表征进行了大规模、系统性的比较。2) 实证表明，基于槽的对象中心表示不仅能实现有效的策略学习，还能在多种现实分布偏移下显著提升泛化能力，且无需任务特定调优。3) 证明了对象中心方法可以通过大规模（包括机器人领域）预训练进一步提升其下游性能，挑战了先前认为其无法从此类预训练中受益的假设。</p>
<p>论文自身提到的局限性包括：SOCRs方法在训练和推理时会产生额外的计算开销；评估的任务范围虽多样，但并未涵盖所有可能的机器人操作场景。</p>
<p>本文的发现对后续研究具有重要启示：在机器人感知中引入结构化的视觉抽象（如对象中心表示）是迈向更鲁棒、更通用操作策略的一条有前景的路径。未来的工作可以探索如何进一步优化SOCRs的计算效率，并将其应用于更广泛的决策制定序列中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中视觉运动策略泛化能力受限的问题，指出当前基于全局或密集特征的方法易混淆任务相关与无关信号，导致对视觉分布变化（如光照、纹理和杂乱）的鲁棒性不足。为此，提出基于槽位的对象中心表示（SOCR），将场景结构化分解为有限的对象类实体，以聚焦任务相关特征。通过大规模系统实验，在模拟和真实任务中验证SOCR能实现更优的策略性能，并显著提升泛化能力，无需任务特定调整；预训练SOCR可进一步强化这些优势。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.21416" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>