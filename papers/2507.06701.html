<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.06701" target="_blank" rel="noreferrer">2507.06701</a></span>
        <span>作者: Martin Riedmiller Team</span>
        <span>日期: 2025-07-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习（Imitation Learning）是使智能体学习专家行为的关键技术。当前主流方法如行为克隆（BC）或离线强化学习（RL）需要大量带有动作标签的专家演示或精心设计的奖励函数，这限制了其在大规模应用中的可扩展性。从观察中模仿学习（IfO）则提供了一条有前景的路径，因为它可以利用无动作标签的专家演示（例如视频），从而规避了获取动作标签或奖励函数的昂贵成本。</p>
<p>然而，当前的IfO研究大多聚焦于理想化的场景，其背景数据分布通常是<strong>双模态的</strong>（例如，混合了专家数据和随机策略数据）。这种简化的分布限制了研究结果的现实意义，因为在实际的大规模学习场景中，智能体收集的自身数据（背景数据）质量可能是连续变化的，而非简单的“好”与“坏”的二元对立。</p>
<p>本文针对上述痛点，旨在探索更<strong>细致、连续的数据分布</strong>对IfO算法的影响，并提出一种新方法，使得模仿学习能够通过<strong>自我改进（Self-Improvement）</strong> 的方式迭代进行。本文的核心思路是：提出一种基于价值函数的离线IfO方法（VfO），通过状态价值函数将无动作标签的专家数据中的知识，传递到有动作标签但质量不一的背景数据上，从而学习出模仿策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为<strong>Value from Observations (VfO)<strong>。其设定是：智能体可以访问两类数据——</strong>无动作标签的专家状态轨迹数据集</strong> (D_E)，以及<strong>可能由自身收集的、带有动作标签但质量未知的背景状态-动作轨迹数据集</strong> (D_B)。目标是从中学习一个策略 (\pi(a|s)) 来模仿专家行为。</p>
<p><img src="https://arxiv.org/html/2507.06701v1/x2.png" alt="VfO算法伪代码"></p>
<blockquote>
<p><strong>图2</strong>：VfO算法伪代码。核心步骤包括：根据是否提供判别器（Discriminator）计算奖励 (r)；在混合数据集 ( (1-\alpha)D_E + \alpha D_B ) 上通过时序差分误差（公式2）更新价值函数 (v)；在背景数据集 (D_B) 上通过指数加权的优势回归（公式3）更新策略 (\pi)。蓝色部分标出了与标准离线RL的关键区别：奖励来源和数据混合。</p>
</blockquote>
<p>VfO的整体框架基于成熟的离线RL机制（类似于优势加权回归AWR），其创新点在于<strong>使用状态价值函数（state-value function）而非状态-动作价值函数（Q函数）</strong>。这是因为在IfO设定下，专家数据没有动作标签，无法为Q函数提供关于专家动作的信号。而价值函数仅依赖于状态，因此可以同时在专家数据（无动作）和背景数据（有动作）上进行评估和学习。</p>
<p>方法包含两个核心变体，区别在于奖励 (r(s’, z)) 的定义：</p>
<ol>
<li><strong>VfO-bin（二元奖励）</strong>：直接为专家数据分配奖励1，为背景数据分配奖励0。这借鉴了SQIL的思想，但应用于价值函数学习。此时，学得的价值函数 (v_{\bar{\pi}}(s_t)) 可以解释为：从状态 (s_t) 开始，未来访问专家状态的可能性（经折扣累积）。最大化此价值的策略会倾向于访问专家状态。</li>
<li><strong>VfO-disc（判别器奖励）</strong>：预训练一个判别器 (d(s)) 来区分专家状态和背景状态（使用公式5的二元分类损失）。然后将判别器的输出 (d(s’)) 作为奖励。这借鉴了ORIL和对抗性模仿学习的思想。</li>
</ol>
<p>无论是哪个变体，算法的核心流程一致（如算法1所示）：</p>
<ul>
<li><strong>价值函数学习（步骤4）</strong>：定义一个虚拟混合策略 (\bar{\pi})，它以一定比例混合了专家和背景数据生成过程。在由混合系数 (\alpha) 控制的混合数据集 ( (1-\alpha)D_E + \alpha D_B ) 上，通过最小化时序差分误差（公式2）来学习状态价值函数 (v)。这步实现了知识从专家数据到背景数据的转移。</li>
<li><strong>策略改进（步骤5）</strong>：在背景数据集 (D_B) 上，使用学得的价值函数计算“优势”信号 ( \gamma v(s’) + r(s’, z) - v(s) )，并通过<strong>指数加权的优势回归</strong>（公式3）来更新策略 (\pi)。温度参数 (\lambda) 控制着对高优势动作的利用和对背景数据分布的保守程度。</li>
</ul>
<p>与现有IfO方法（如基于逆贝尔曼更新的SMODICE、DILO）相比，VfO的创新点在于其<strong>简单性和基于成熟RL框架的稳定性</strong>。它绕过了复杂的对抗性训练或对偶问题求解，直接利用价值函数作为连接专家观察与背景动作的桥梁。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：本文引入了 <strong>Self-Improvement Benchmark (SIBench)<strong>。通过用不同数量的演示训练多个BC策略（质量从L0到L9），并用这些策略收集数据，构建了背景数据质量</strong>连续变化</strong>的数据集。同时，也使用了传统的<strong>双模态（Bimodal）</strong> 基准（混合专家和随机策略数据）进行对比。</li>
<li><strong>实验平台</strong>：主要在模拟机器人任务（如MetaWorld的<code>reach-v2</code>, <code>push-v2</code>, <code>pick-place-v2</code>）和Adroit灵巧手任务（如<code>pen</code>, <code>door</code>）上进行。</li>
<li><strong>对比方法</strong>：包括IfO方法 <strong>SMODICE</strong>、<strong>DILO</strong>，以及作为基线的 <strong>行为克隆（BC）</strong>、<strong>逆动力学模型（IDM）+BC</strong>。在部分实验中还对比了在线方法。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.06701v1/x1.png" alt="不同数据配置的回报分布"></p>
<blockquote>
<p><strong>图1</strong>：不同数据配置下背景数据的回报分布。对比了双模态（专家+随机）、自改进基准（来自不同质量单一策略的数据）和迭代自改进过程中遇到的数据。该图直观展示了不同算法在不同数据分布下可能受益的性质。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>不同数据分布下的算法表现</strong>：在传统的双模态基准上，VfO-disc与SMODICE、DILO表现相当或略优。然而，在更现实、更具挑战性的<strong>SIBench自改进基准</strong>上，<strong>VfO-bin表现尤为突出</strong>，显著优于其他对比方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.06701v1/x3.png" alt="SIBench基准上的性能"></p>
<blockquote>
<p><strong>图3</strong>：在SIBench基准（<code>reach-v2</code>任务）上的性能。横轴为背景数据策略的质量等级（L0最差，L9最好），纵轴为学习到的策略的成功率。VfO-bin（橙色）在大多数数据质量级别上表现最佳，尤其是在中等质量数据上提升显著。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.06701v1/x4.png" alt="多任务SIBench基准上的性能"></p>
<blockquote>
<p><strong>图4</strong>：在三个MetaWorld任务构成的SIBench上的平均成功率。VfO-bin（橙色）整体表现最优，特别是在背景数据质量中等（L4-L6）时，其优势明显。</p>
</blockquote>
<ol start="2">
<li><strong>与基线方法的对比</strong>：VfO-bin consistently outperforms BC and IDM+BC across different data quality levels, demonstrating the effectiveness of its value-based knowledge transfer.</li>
</ol>
<p><img src="https://arxiv.org/html/2507.06701v1/x5.png" alt="与基线方法对比"></p>
<blockquote>
<p><strong>图5</strong>：在<code>reach-v2</code>任务上，VfO-bin与基线方法（BC, IDM+BC）的对比。VfO-bin在不同背景数据质量下均能取得比单纯克隆背景数据（BC）或通过IDM标记专家数据后克隆（IDM+BC）更好的性能。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：研究验证了混合系数 (\alpha) 和温度参数 (\lambda) 的重要性。适当的 (\alpha) 值（如0.5）能平衡专家与背景数据的信息利用。较低的 (\lambda) 值（更具利用性）在背景数据质量较高时表现更好，而较高的 (\lambda) 值（更具保守性）在数据质量较差时更稳定。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.06701v1/x7.png" alt="超参数消融实验"></p>
<blockquote>
<p><strong>图7</strong>：VfO-bin在<code>reach-v2</code>任务上的超参数消融研究。（左）不同混合系数 (\alpha) 的影响。（右）不同温度参数 (\lambda) 在不同背景数据质量下的影响。结果表明这些参数对性能有显著影响，需要根据数据质量进行调整。</p>
</blockquote>
<ol start="4">
<li><strong>迭代自改进的初步探索</strong>：论文进行了概念验证实验，将VfO应用于迭代过程：使用当前策略收集数据，并将其作为下一轮的背景数据。实验表明，VfO-bin能够利用自收集的数据实现性能的迭代提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.06701v1/x8.png" alt="迭代自改进结果"></p>
<blockquote>
<p><strong>图8</strong>：迭代自改进实验。每一轮使用当前策略收集新的背景数据，并利用固定的专家观察数据通过VfO-bin进行学习。在<code>reach-v2</code>任务上，经过几轮迭代，策略性能得到了持续提升。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出新的评估基准（SIBench）</strong>：推动了IfO研究从理想化的双模态数据分布向更现实、更细致的连续质量数据分布发展，更好地反映了自改进场景的需求。</li>
<li><strong>提出新颖算法（VfO）</strong>：设计了一种简单而有效的基于状态价值函数的离线IfO方法。其核心创新在于通过价值函数在无动作专家数据和有动作背景数据之间架起桥梁，并提供了二元奖励（VfO-bin）和判别器奖励（VfO-disc）两种变体。实验表明，VfO-bin尤其在具有挑战性的自改进基准上表现优异。</li>
<li><strong>展示了IfO用于迭代自改进的潜力</strong>：通过初步实验验证了将IfO与迭代数据收集相结合的可能性，为通向大规模行为学习开辟了新视角。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1) 方法依赖于背景数据中至少包含一些与任务相关的信息，如果背景数据完全无关，则难以学习；2) 实验主要在相对简单的模拟环境中进行，尚未在更复杂的真实世界视觉观察数据上进行验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>基准的重要性</strong>：SIBench的提出提示未来IfO研究需要关注更复杂、更连续的数据分布，以更好地评估算法的鲁棒性和实用性。</li>
<li><strong>简单性与有效性</strong>：VfO的成功表明，基于成熟RL框架的简单方法可能在复杂问题中表现出色，这为算法设计提供了新思路。</li>
<li><strong>自改进的路径</strong>：将IfO与迭代数据收集相结合，是迈向无需人工奖励或精细动作标注的大规模行为学习的一条有前景的路径，值得进一步深入探索。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究从观察中模仿学习（IfO）的核心问题：如何利用无动作标签的专家示范和可能不匹配的非专家动作数据，实现大规模、可扩展的行为学习。提出一种关键技术方法，通过价值函数在专家与非专家数据间传递信息，将基于强化学习的模仿学习适配到无动作示范场景。实验评估了不同数据分布下算法的适用性，揭示了现有方法的局限，为开发更鲁棒、实用的IfO技术提供了关键见解。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.06701" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>