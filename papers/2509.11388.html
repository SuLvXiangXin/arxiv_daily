<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Quantum deep reinforcement learning for humanoid robot navigation task - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Quantum deep reinforcement learning for humanoid robot navigation task</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.11388" target="_blank" rel="noreferrer">2509.11388</a></span>
        <span>作者: Lokossou, Romerik, Girma, Birhanu Shimelis, Tonguz, Ozan K., Biyabani, Ahmed</span>
        <span>日期: 2025/09/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>经典强化学习（RL）方法在处理复杂、高维环境（如人形机器人导航）时面临挑战，主要受限于所需参数量指数级增长以及随机、非确定性环境带来的训练不稳定和收敛速度慢等问题。量子计算凭借其叠加、纠缠和干涉等特性，为机器学习领域带来了提升处理能力和效率的潜力。量子深度强化学习（QDRL）作为一种混合方法应运而生，旨在减少参数空间并加速复杂任务的学习。</p>
<p>尽管已有研究将量子RL应用于轮式机器人、机械臂等具有较小观察和动作空间的环境，但尚未拓展到人形机器人这一更具挑战性的领域。本文针对经典RL在人形机器人高维连续控制任务中的效率瓶颈，提出了将量子深度强化学习应用于人形机器人导航任务的新视角。其核心思路是：构建一个混合量子-经典策略网络，利用参数化量子电路（PQC）直接处理高维状态空间，旨在以更少的参数和训练步骤实现更高效、更稳定的学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了一种用于深度强化学习的变分量子软演员-评论家（QuantumSAC）方法，将量子计算的优势与软演员-评论家（SAC）算法的鲁棒性相结合。该方法专门针对人形机器人导航任务中的高维观察和动作空间设计。</p>
<p>整体框架是一个混合量子-经典策略网络。输入是来自MuJoCo Walker2d-v4环境的17维状态向量 <code>s</code>，输出是代表6个关节扭矩的6维动作分布的均值 <code>μ</code> 和标准差 <code>σ</code>。</p>
<p><img src="https://arxiv.org/html/2509.11388v1/quantum_sac_architecture.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：QuantumSAC的混合量子-经典策略网络架构。17维输入状态 <code>s</code> 由编码电路 <code>U_θenc</code> 和变分电路 <code>U_θvar</code> 在17个量子位的量子层中处理。测量结果随后传递给经典神经网络层，输出代表关节扭矩的6维动作分布的均值 <code>μ</code> 和标准差 <code>σ</code>。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>输入层</strong>：接收17维状态特征向量。</li>
<li><strong>数据重上传参数化量子电路（Re-uploading PQC）层</strong>：这是一个包含17个量子位的量子层。它采用数据重上传策略，即在量子电路中多次编码输入数据，从而显著增强了量子电路的表达能力。该电路处理17维输入并产生测量结果，供后续经典处理。</li>
<li><strong>可观测量-策略（Observables-Policy）层</strong>：一个经典的顺序网络层，处理来自量子层的输出，并生成6维输出向量（<code>μ</code> 和 <code>σ</code>）。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11388v1/x1.png" alt="量子深度学习模型"></p>
<blockquote>
<p><strong>图3</strong>：嵌入了参数化量子电路的深度学习模型示意图。</p>
</blockquote>
<p>QuantumSAC算法遵循最大熵强化学习框架，优化目标函数 <code>J(θ)</code> 同时考虑预期回报和策略熵（公式1）。其中，<code>Ã_θ</code> 表示使用重参数化技巧采样的动作，<code>α</code> 是控制熵重要性的温度参数。</p>
<p>与现有方法相比，本文的创新点具体体现在：</p>
<ul>
<li><strong>应用领域拓展</strong>：首次将QDRL（具体为QuantumSAC）应用于具有高维状态和动作空间的人形机器人（以Walker2D为测试平台）导航任务，超越了此前量子RL研究多集中于简化环境或机械臂的局限。</li>
<li><strong>高效的混合架构</strong>：采用了数据重上传的PQC策略，增强了量子电路的表达性。该方法实现了显著的参数效率：本文的2层数据重上传量子电路仅使用41个可学习参数，就达到了与拥有1250个参数的等效经典网络相当的性能。</li>
<li><strong>完整的量子化策略</strong>：与一些仅将量子计算用于评论家网络的工作不同，本文在QuantumSAC中实现了基于PQC的策略网络（演员）。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在<strong>MuJoCo Walker2d-v4</strong>仿真环境中进行，这是一个具有17维连续观察空间和6维连续动作空间的双足步行机器人基准测试环境。</p>
<p><img src="https://arxiv.org/html/2509.11388v1/walker.png" alt="Walker2D环境"></p>
<blockquote>
<p><strong>图1</strong>：Walker2D-v4 Gym 仿真环境。</p>
</blockquote>
<p>对比的基线方法是经典的<strong>软演员-评论家（SAC）算法</strong>。评估指标包括单次回报、平均回报、每秒步数（SPS）以及累计步数等。</p>
<p>关键实验结果如下：<br>量子SAC在<strong>更少的训练步骤下取得了更高的平均回报</strong>。具体而言，量子SAC以比经典SAC少92%的步骤，实现了8%的平均回报提升（量子SAC：246.40，经典SAC：228.36）。</p>
<p><img src="https://arxiv.org/html/2509.11388v1/return.png" alt="回报对比"></p>
<blockquote>
<p><strong>图4</strong>：经典SAC与量子SAC在训练过程中的单次回报对比。量子SAC更快达到峰值性能，且回报曲线波动更小，表明学习过程更稳定、高效。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.11388v1/avg_return.png" alt="平均回报对比"></p>
<blockquote>
<p><strong>图5</strong>：经典SAC与量子SAC的平均回报随训练步骤的变化。量子SAC在训练早期平均回报上升更快，并能持续维持比经典SAC更高的性能水平。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.11388v1/scatter_1.png" alt="效率对比"></p>
<blockquote>
<p><strong>图6</strong>：达到特定平均回报水平所需的每秒步数（SPS）对比。经典SAC初始SPS很高（约1200），随着回报提升而下降；量子SAC的SPS始终接近0，但能以更少的“计算步骤”达到更高的回报，突显了其本质不同的、更高效的学习模式。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">指标</th>
<th align="left">经典SAC</th>
<th align="left">量子SAC</th>
</tr>
</thead>
<tbody><tr>
<td align="left">总训练回合数</td>
<td align="left">192</td>
<td align="left">169</td>
</tr>
<tr>
<td align="left">平均回报</td>
<td align="left">135.07</td>
<td align="left">172.66</td>
</tr>
<tr>
<td align="left">最大回报</td>
<td align="left">500.52</td>
<td align="left">716.71</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：经典SAC与量子SAC的关键性能指标对比。量子SAC在更少的训练回合下，获得了更高的平均回报和最大回报。</p>
</blockquote>
<p>消融实验主要体现在<strong>参数数量的对比</strong>上。如前所述，量子SAC仅用41个参数实现了与1250个参数的经典网络相当的性能，这直接证明了引入量子电路带来的<strong>参数效率</strong>这一核心贡献。量子SAC更稳定的学习曲线和更快的奖励提升也间接反映了其混合架构的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：</p>
<ol>
<li><strong>开创性应用</strong>：首次将量子深度强化学习（QDRL）成功应用于高维、连续控制的人形机器人导航任务，为该领域探索了新的技术路径。</li>
<li><strong>效率验证</strong>：提出的Variational Quantum SAC方法在MuJoCo Walker2d-v4环境中，仅用极少参数（41 vs 1250）即实现了优于经典SAC的性能，并以显著更少的训练步骤达到更高回报，实证了量子计算在强化学习中的参数效率和加速学习潜力。</li>
<li><strong>稳定优势</strong>：实验表明量子SAC具有更稳定、波动更小的学习曲线，展现了其在复杂环境中鲁棒学习的优势。</li>
</ol>
<p>论文自身提到的局限性包括：实验完全在仿真环境（MuJoCo）中进行；由于当前量子硬件仅支持少量量子位，因此选择在计算复杂度较低的Walker2d-v4而非更复杂的Humanoid-v4环境中进行验证，实际硬件部署面临挑战。</p>
<p>本文对后续研究的启示在于：</p>
<ul>
<li><strong>算法扩展</strong>：未来工作可将此框架扩展到更复杂的环境（如Humanoid-v4）、更多样的任务，并探索更先进的量子电路架构。</li>
<li><strong>硬件协同</strong>：随着量子硬件的进步，研究如何优化算法以适应真实的量子处理器至关重要。</li>
<li><strong>理论深化</strong>：需要更深入的理论研究来阐明量子强化学习获得性能优势的内在机制（如叠加、纠缠在探索和表示中的具体作用）。</li>
<li><strong>实用化探索</strong>：探索在训练阶段使用量子计算、部署阶段使用经典模型的实用化路径，并最终推动在真实机器人及自动驾驶等复杂现实场景中的应用。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对经典强化学习在人形机器人高维导航任务中参数需求大、收敛慢的问题，提出量子深度强化学习方法。采用参数化量子电路与混合量子-经典架构，直接处理高维状态空间，避免了传统映射规划。核心实验表明，量子Soft Actor-Critic相比经典版本，在平均回报上提升8%（达246.40），且训练步数减少92%，显著加速了学习过程。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.11388" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>