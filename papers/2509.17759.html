<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.17759" target="_blank" rel="noreferrer">2509.17759</a></span>
        <span>作者: Yang Gao Team</span>
        <span>日期: 2025-09-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习在机器人操作领域进展迅速，但大规模真实机器人数据的收集成本高昂、劳动密集，成为提升机器人操作能力的主要瓶颈。为解决数据稀缺问题，研究者转向利用辅助数据源，如互联网图像或文本数据，以帮助策略训练。然而，尽管互联网数据能为策略学习提供丰富的视觉-语言知识，获取运动知识仍然是一个重大挑战。人类数据，特别是通过VR设备记录的手部姿态数据，因其丰富性、易于收集性和多样化的操作行为，成为一个极具潜力的来源。先前工作已利用人类演示提取任务感知表征（如可供性或关键点流）来支持运动迁移，但引入的中间表征阻碍了其与主流端到端策略的集成。近期研究开始探索将人类运动数据直接用于机器人策略的协同训练或预训练，并显示出在视觉基础、鲁棒性和训练效率方面的益处。然而，人类数据最大的优势——即让机器人策略直接学习完成任务所需的新运动——能否实现，仍不明确。</p>
<p>本文旨在系统性地探索这一潜力，核心思路是通过多任务人机协同训练，构建一个名为MotionTrans的框架，将人类数据转化为机器人可执行的格式，并加权协同训练，从而直接从人类数据中学习可部署的端到端机器人策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>MotionTrans框架旨在实现显式的人到机器人运动迁移。其核心思想是先将人类数据转换为机器人数据格式，然后在机器人观察-动作空间内，从人类和机器人数据中联合学习。通过在机器人空间中训练策略，可以直接将策略部署到真实世界的机器人上，执行人类数据集中的任务。</p>
<p><img src="https://arxiv.org/html/2509.17759v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MotionTrans框架示意图。包含人机数据收集系统、将人类数据转换为机器人格式的流程，以及加权的人机多任务协同训练策略。训练后，可将训练好的策略直接部署到真实机器人上，执行人类数据集中的任务。</p>
</blockquote>
<p><strong>整体问题定义与观察-动作空间</strong>：目标是在多任务人机协同训练框架下实现显式的人到机器人运动迁移。策略在数据集 $D = D_{\text{robot}} \cup D_{\text{human}}$ 上训练，其中人类和机器人任务集不重叠。训练后，在零样本（评估任务无对应机器人训练数据）或少样本微调（有少量机器人演示）设置下，评估策略在 $D_{\text{human}}$ 任务上的表现。策略的输入输出定义在机器人观察-动作空间 $S = (I_t, P_t, A_t)$。输入包括以自我为中心的RGB图像 $I_t$ 和本体感觉状态 $P_t$（包含手腕位姿和手部关节状态的历史序列）。输出为动作块预测 $A_t$。</p>
<p><strong>数据收集系统</strong>：人类数据使用便携式商用VR头显（Meta Quest 3）收集，扩展了ARCap系统，记录手部关键点位置 $K_t$、手腕位姿 $W_t$ 和RGB图像流 $I_t$。通过自定义标定方法将手部信息从VR坐标系转换到RGB相机坐标系。收集时引导操作者尽量减少头部运动，并提供实时反馈确保手部在相机视野内且VR记录准确。机器人数据通过遥操作系统（基于Open-Television）收集，使用Franka机械臂和Inspired灵巧手组合，通过VR设备实时捕获人类手腕和手部姿态并驱动机器人复现。</p>
<p><strong>人类数据到机器人格式的转换</strong>：为使人类数据能与机器人策略协同训练，需要将其转换到机器人的观察-动作空间。</p>
<ol>
<li>**图像观察 $I_t$**：人类和机器人都使用自我中心视图，使完成相似任务时场景中物体的空间关系相似。</li>
<li>**手腕位姿 $W_t$**：统一使用自我中心相机坐标系进行测量。</li>
<li>**手部关节状态 $H_t$**：使用基于优化的逆运动学求解器（dex-retargeting库）将人类手部关键点 $K_t$ 映射到机器人手部关节状态 $H_t$。</li>
</ol>
<p>转换后，人类数据可在真实机器人上回放。基于此，论文发现两个关键问题并提出了解决方案：</p>
<ul>
<li><strong>(O1) 速度差异</strong>：人类操作速度远快于机器人。通过位姿和手部关节状态插值，将人类数据速度减慢2.25倍。</li>
<li><strong>(O2) 工作空间分布差异</strong>：人类手部位置分布与机器人舒适工作空间不匹配。采用两种方法缓解：一是使用基于动作块的相对位姿作为手腕动作表示，以减少分布差异；二是鼓励数据收集者在不同轨迹记录间改变视角，增加相机视图与操作目标之间位置关系的多样性，促使策略适应更大的手部位姿分布。</li>
</ul>
<p><strong>加权多任务人机协同训练</strong>：统一观察和动作空间后，可以在共享的端到端机器人策略下联合训练人类和机器人数据。</p>
<ol>
<li><strong>策略架构</strong>：探索了两种流行的端到端策略架构。一是**扩散策略 (DP)<strong>，将其扩展为多任务版本，每个任务对应一个可学习的嵌入作为条件，并使用DINOv2作为视觉编码器。二是</strong>视觉-语言-动作模型 ($\pi_0$-VLA)**，采用其网络结构，直接使用语言指令来指定任务，并加载 $\pi_0$-droid 预训练权重。</li>
<li><strong>统一动作归一化</strong>：为提高训练稳定性，对本体感觉状态和动作应用Z-score归一化。与先前工作对人和机器人数据独立归一化不同，本文采用跨人类和机器人数据的统一归一化，以避免训练（人类归一化）与推理（机器人归一化）之间的不匹配。</li>
<li><strong>加权协同训练策略</strong>：针对人机数据潜在的不平衡，采用加权训练目标：$\mathcal{L}<em>{D} = \alpha \mathcal{L}</em>{D_{\text{robot}}} + (1-\alpha) \mathcal{L}<em>{D</em>{\text{human}}}$，其中权重 $\alpha = \frac{|D_{\text{human}}|}{|D_{\text{human}}|+|D_{\text{robot}}|}$，确保两种数据源的权重之和平衡。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>硬件平台</strong>：机器人使用Franka机械臂+Inspired灵巧手，配备ZED2相机提供自我中心视图。人类数据收集使用Meta Quest 3 VR头显和ZED2相机。</li>
<li><strong>数据集</strong>：构建了MotionTrans数据集，包含3213条演示，涵盖15个人类任务和15个机器人任务，超过10个场景。人类与机器人任务集不重叠，但包含相似的运动模式（如抓放），人类数据还包含一些机器人数据中没有的运动（如拔插头、关闭、抬起等）。</li>
<li><strong>评估任务与指标</strong>：评估聚焦于人类任务（共13个可部署任务）。使用成功率（SR）和运动进展评分（Score，量化任务完成过程中的运动质量，归一化到[0,1]）进行评估。每个任务进行10轮测试。</li>
</ul>
<p><strong>零样本实验结果</strong>：直接在真实机器人上部署经MotionTrans框架训练的模型，评估其在未见过机器人数据的人类任务上的表现。</p>
<p><img src="https://arxiv.org/html/2509.17759v1/x6.png" alt="零样本结果"></p>
<blockquote>
<p><strong>图6</strong>：MotionTrans在零样本实验设置下的结果。扩散策略（DP）和$\pi_0$-VLA均实现了成功的人到机器人运动迁移。在没有任何对应机器人数据的情况下，9个任务获得了非零的成功率。对于剩余任务，MotionTrans仍能产生有意义的任务完成运动，表现为非平凡的运动进展评分。</p>
</blockquote>
<p>扩散策略（DP）和$\pi_0$-VLA模型分别有7个和6个任务获得了非零的成功率，总计9个不同的任务实现了零样本成功。即使对于未成功的任务，策略也展现出有意义的运动，如伸手抓取目标物体。</p>
<p><img src="https://arxiv.org/html/2509.17759v1/x7.png" alt="零样本可视化"></p>
<blockquote>
<p><strong>图7</strong>：MotionTrans框架实现零样本人机运动迁移的可视化结果。所示任务均未涉及任何机器人数据收集，完全从人类数据中学习。这些结果证明MotionTrans通过人机协同训练，能够实现用于任务完成的显式人机运动迁移。</p>
</blockquote>
<p><strong>少样本微调实验结果</strong>：在零样本策略基础上，使用少量（10条）对应人类任务的机器人演示进行微调。</p>
<p><img src="https://arxiv.org/html/2509.17759v1/x8.png" alt="少样本结果"></p>
<blockquote>
<p><strong>图8</strong>：少样本微调实验的定量结果。与零样本性能（蓝色）相比，使用MotionTrans数据集预训练后进行少样本微调（橙色）能带来平均约40%的成功率提升。从零开始训练（灰色）的性能则差得多。</p>
</blockquote>
<p>实验表明，使用MotionTrans数据集进行预训练后微调，相比零样本性能，平均带来约40%的成功率提升，并且显著优于从零开始训练（仅使用少量机器人数据）的策略。</p>
<p><strong>消融实验与机制分析</strong>：</p>
<ul>
<li><strong>机器人数据协同训练的必要性</strong>：仅使用人类数据训练的策略在零样本部署时完全失败（成功率为0），验证了与机器人数据协同训练对于运动迁移至关重要。</li>
<li><strong>统一 vs. 独立归一化</strong>：使用统一动作归一化的策略在零样本任务上表现显著优于使用独立归一化的策略。</li>
<li><strong>任务相关运动覆盖范围的重要性</strong>：在消融实验中，逐步从训练数据集中移除与目标人类任务运动相似的其他任务（人类或机器人任务）数据。结果表明，当训练数据中缺乏足够广泛的任务相关运动覆盖时，零样本运动迁移性能会下降。例如，在训练数据中移除所有涉及“扭转”动作的任务后，评估“开瓶盖”任务（需要扭转）的性能大幅降低。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.17759v1/x11.png" alt="运动覆盖消融"></p>
<blockquote>
<p><strong>图11</strong>：任务相关运动覆盖范围对运动迁移影响的消融研究。从左到右，逐步移除训练数据中与目标任务（Open Bottle）运动相似的任务。随着相关运动覆盖的减少，零样本性能（成功率和运动进展评分）显著下降。</p>
</blockquote>
<ul>
<li><strong>视觉渲染的影响</strong>：尝试将机器人渲染到人类视频中以缩小视觉差距，但实验表明这种方法并未比直接在人类视频上训练带来显著改进。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>MotionTrans框架</strong>，首次系统性地验证并实现了从人类数据到可部署端到端机器人策略的<strong>运动级学习</strong>。该框架包括数据收集系统、人类数据转换流程和加权协同训练策略。</li>
<li>构建并开源了<strong>MotionTrans数据集</strong>，包含大量多样化的人机任务演示，显著提升了任务/运动覆盖和场景多样性。</li>
<li>通过实验揭示了成功运动迁移的<strong>两个关键因素</strong>：与机器人数据的协同训练，以及训练数据中足够广泛的任务相关运动覆盖。</li>
</ol>
<p><strong>局限性</strong>：论文提到，由于机器人手部的硬件设计限制，人类数据集中的“折叠毛巾”和“倾倒牛奶瓶”两个任务无法部署评估。此外，在数据转换中，仅通过固定倍数减慢人类动作速度，更先进的自适应降速策略留待未来探索。</p>
<p><strong>启示</strong>：这项工作解锁了利用人类数据进行运动级学习的潜力，为有效利用人类数据训练机器人操作策略提供了清晰的框架和原则。它表明，通过精心设计的数据对齐和协同训练，丰富的、易于收集的人类演示可以直接转化为机器人的运动技能，为解决机器人数据稀缺问题开辟了一条新途径。未来的工作可以探索更复杂的行为、动态环境以及更高效的数据转换与融合方法。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MotionTrans框架，旨在解决机器人模仿学习中真实数据稀缺的核心瓶颈，探索如何利用人类VR数据使机器人策略直接学习新动作以完成任务。方法包含VR数据采集系统、数据转换流程及加权协同训练策略，通过多任务人机协同训练实现运动知识迁移。实验表明，在30个任务的协同训练中，成功将13个任务的人类动作直接迁移至机器人策略，其中9个任务实现零样本有效执行；预训练-微调性能提升达40%成功率。关键成功因素为与机器人数据协同训练及广泛的任务相关动作覆盖。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.17759" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>