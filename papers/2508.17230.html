<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>4D Visual Pre-training for Robot Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>4D Visual Pre-training for Robot Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.17230" target="_blank" rel="noreferrer">2508.17230</a></span>
        <span>作者: Huazhe Xu Team</span>
        <span>日期: 2025-08-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从大规模网络数据集中学习通用的视觉表征，并在机器人操作任务上实现数据高效学习已取得巨大成功。然而，这些预训练表征大多基于2D图像，忽略了世界的固有3D特性。虽然使用3D点云作为机器人操作的视觉输入已被证明在真实任务中具有高效性和泛化能力，但由于大规模3D数据的稀缺，很难从网络数据集中提取一个通用的3D表征。因此，本文寻求一个通用的视觉预训练框架，作为一种替代方案来提升各种3D表征的性能。</p>
<p>本文针对如何为3D输入进行预训练并提取对机器人有用的表征这一具体问题，提出了一个新颖的视角：将视觉预训练目标构建为下一帧点云预测问题。核心思路是利用扩散模型，以前一帧的点云和机器人动作为条件，预测当前帧的点云，从而学习蕴含时空动态和物理交互信息的3D视觉表征。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法称为FVP（4D Visual Pre-training），其整体框架是一个自监督的预训练流程，旨在为下游机器人操作任务提升3D视觉编码器的表征能力。</p>
<p><img src="https://arxiv.org/html/2508.17230v2/sec/Figure/teaser.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：FVP的3D点云表征学习流程。与对比学习和掩码信号建模等先前工作不同，FVP利用前一帧点云，并采用扩散模型来预测当前帧的点云。</p>
</blockquote>
<p>FVP的pipeline包含三个阶段：</p>
<ol>
<li><strong>演示收集与输入</strong>：输入是来自真实世界机器人任务的演示轨迹 $\mathbf{X} = { x^0, x^1, \dots, x^T }$，其中每帧 $x^t = (o^t, a^t)$ 包含时间 $t$ 的3D点云观测 $o^t$ 和机器人关节位置动作 $a^t$。</li>
<li><strong>4D视觉预训练</strong>：这是核心预训练阶段。首先，使用一个通用的3D视觉编码器（如 PointNet++、Point Transformer、DP3 Encoder）将前一帧的点云 $o^{t-1}$ 编码为潜在视觉表征 $\mathbf{z} \in \mathbb{R}^{N \times C_v}$。然后，以 $\mathbf{z}$ 为条件，一个改进的Point-Voxel扩散模型逐步将随机高斯噪声去噪，生成下一帧的点云 $o^t$。具体而言，扩散模型的输入是带噪声的当前帧点云 $o^t_T$ 与条件表征 $\mathbf{z}$ 的拼接 $o^t_{T,+} = [o^t_T, \mathbf{z}] \in \mathbb{R}^{N \times (C_v+3)}$。扩散网络 $\epsilon_\theta: \mathbb{R}^{N \times (C_v+3)} \to \mathbb{R}^{N \times 3}$ 的目标是预测所添加的噪声 $\epsilon$，其训练损失函数为 $\mathcal{L} = E_{\epsilon \sim \mathcal{N}(0,\mathbf{I})} [|\epsilon - \epsilon_\theta(o^t_{+,T}, T)|_2^2]$。</li>
<li><strong>下游任务微调</strong>：获得预训练的视觉表征后，将其应用于下游机器人操作任务。在给定专家演示的情况下，训练诸如RISE、DP3等以点云为输入的3D视觉运动策略。直接用预训练的表征替换原始视觉编码器，并在训练过程中以端到端的方式微调视觉表征和策略主干网络。</li>
</ol>
<p>与现有方法相比，FVP的核心创新点在于其预训练目标。不同于将同一时间步的点云作为正样本对的对比学习方法，也不同于掩码部分点云进行重建的方法，FVP利用当前的机器人观测来预测后续的机器人观测。这种下一帧预测机制使视觉模型能够学习基于当前观测预测机器人下一步动作，从而更好地捕捉机器人的运动特性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真基准</strong>：使用了Adroit（灵巧操作任务）和MetaWorld（50种模拟机器人操作任务）基准。</li>
<li><strong>真实世界任务与机器人平台</strong>：涵盖了12个真实任务，使用的机器人包括：配备夹爪或灵巧手（LeapHand）的UR5单臂机器人、AgileX双臂机器人、天工仿人机器人。任务示例如图4所示。</li>
<li><strong>数据收集</strong>：针对不同类型的机器人，采用了键盘控制、基于手部姿态检测的遥操作、辅助臂主从控制、动作捕捉服映射等多种方式收集专家演示（见图5）。</li>
<li><strong>评估</strong>：主要评估FVP预训练对3D模仿学习方法（如DP3、RISE）性能的提升，并与多种2D/3D预训练方法进行对比。实验考虑了使用领域内（in-domain）和领域外（out-of-domain，如Robomind数据集）数据进行预训练的情况。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.17230v2/sec/Figure/simulation.png" alt="仿真实验结果对比"></p>
<blockquote>
<p><strong>图3</strong>：在仿真任务中，FVP与多种3D预训练方法、2D预训练方法以及Diffusion Policy变体的对比。结果显示，无论在领域内还是领域外数据集上预训练，FVP预训练的DP3都取得了最佳性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>仿真实验</strong>：在Adroit和MetaWorld基准上，FVP预训练的DP3相比其他预训练方法取得了最先进的性能。使用领域内数据预训练时，平均任务成功率提升16.9%；使用领域外数据预训练时，平均提升24.7%。即使在预训练数据量远超FVP（超过3亿）的情况下，基于2D图像的预训练方法（如MVP、R3M）对Diffusion Policy任务的提升效果也不及FVP。</li>
<li>**真实世界实验 (Q1 &amp; Q2)**：<br><img src="https://arxiv.org/html/2508.17230v2/sec/Figure/real.png" alt="真实世界任务成功率"><blockquote>
<p><strong>图6</strong>：真实世界机器人任务上，不同模仿学习方法及预训练方法的成功率对比。FVP预训练的RISE在所有任务中达到SOTA性能，显著超越了2D和3D的单任务模仿学习方法。</p>
</blockquote>
<ul>
<li><strong>超越其他模仿学习方法</strong>：FVP预训练的DP3和RISE在真实任务上大幅超越了ACT、Diffusion Policy等2D方法以及未预训练的3D方法。特别是在灵巧手任务上，FVP通过引入时间帧辅助理解复杂运动轨迹，显著提升了任务成功率。</li>
<li><strong>超越其他预训练表征</strong>：无论是使用领域内还是领域外（Robomind）数据，FVP预训练方法都比其他3D/4D预训练方法（PointMAE, STRL, C2P）学习到了更有效的视觉特征，从而帮助DP3/RISE更高效地完成真实任务。</li>
<li><strong>与大规模2D通用模型对比</strong>：<br>表1: 2D预训练视觉表征在扩散策略上的成功率（%）<table>
<thead>
<tr>
<th align="left">Diffusion Policy for Robotic Action</th>
<th align="center">R3M</th>
<th align="center">MVP</th>
<th align="center">MAE (Soup-1M+100 DoH)</th>
<th align="center">DP3+FVP</th>
</tr>
</thead>
<tbody><tr>
<td align="left">PickSquare</td>
<td align="center">15/20</td>
<td align="center">17/20</td>
<td align="center">18/20</td>
<td align="center">20/20</td>
</tr>
<tr>
<td align="left">PlaceBottle</td>
<td align="center">13/20</td>
<td align="center">15/20</td>
<td align="center">15/20</td>
<td align="center">20/20</td>
</tr>
<tr>
<td align="left">PickPlace</td>
<td align="center">14/20</td>
<td align="center">16/20</td>
<td align="center">16/20</td>
<td align="center">17/20</td>
</tr>
<tr>
<td align="left">FlipCup</td>
<td align="center">14/20</td>
<td align="center">17/20</td>
<td align="center">15/20</td>
<td align="center">16/20</td>
</tr>
<tr>
<td align="left">Assembly</td>
<td align="center">9/20</td>
<td align="center">10/20</td>
<td align="center">11/20</td>
<td align="center">13/20</td>
</tr>
<tr>
<td align="left">ArtiManip</td>
<td align="center">11/20</td>
<td align="center">14/20</td>
<td align="center">14/20</td>
<td align="center">16/20</td>
</tr>
<tr>
<td align="left"><strong>Average</strong></td>
<td align="center"><strong>12.5/20</strong></td>
<td align="center"><strong>15.5/20</strong></td>
<td align="center"><strong>15.3/20</strong></td>
<td align="center"><strong>16.4/20</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：在相同策略生成器（DP3）下，对比FVP与大规模2D预训练模型（R3M, MVP, MAE）在六个真实任务上的微调结果。FVP使用Robomind数据集预训练，取得了最高的平均成功率。</p>
</blockquote>
</li>
</ul>
</li>
<li>**提升VLA模型 (Q3)**：<br>表2: 使用不同配置的RDT-1B在真实世界任务上的成功率（%）<br>| Input Style | PickSquare | PlaceBottle | PutBox | StackBowl | WipePlate |<br>| :--- | :---: | :---: | :---: | :---: | :---: |<br>| 2D Image Input | 13/20 | 15/20 | 14/20 | 15/20 | 12/20 |<br>| 3D point cloud Input | 16/20 | 18/20 | 17/20 | 18/20 | 15/20 |<br>| 2D Image Input by R3M | 15/20 | 17/20 | 16/20 | 17/20 | 14/20 |<br>| 3D encoder pretrained by FVP | <strong>18/20</strong> | <strong>19/20</strong> | <strong>18/20</strong> | <strong>19/20</strong> | <strong>16/20</strong> |<br>&gt; <strong>表2</strong>：在视觉-语言-动作大模型RDT-1B中，引入3D点云输入并使用FVP预训练编码器，相比仅使用2D图像输入或使用R3M预训练的2D编码器，在多个真实任务上取得了更高的成功率。<ul>
<li>将FVP预训练的3D点云编码器集成到视觉-语言-动作大模型RDT-1B中，可以显著提升其在真实世界任务上的性能、空间感知、语言理解和任务泛化能力（任务设计见图7）。</li>
</ul>
</li>
<li><strong>可视化分析</strong>：<br><img src="https://arxiv.org/html/2508.17230v2/sec/Figure/prediction.png" alt="下一帧点云预测可视化"><blockquote>
<p><strong>图2</strong>：在“Assembly”任务时间线上，下一帧点云预测结果与条件帧、真实帧的对比可视化。预测的点云分布与真实下一帧分布高度相似，表明FVP有效学习了时空动态特征。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了新颖的4D视觉预训练框架FVP</strong>：通过将预训练目标定义为“下一帧点云预测”，并利用扩散模型进行实现，为机器人学习提供了一种通用的、可提升各类3D视觉编码器性能的预训练范式。</li>
<li><strong>证明了FVP的有效性与通用性</strong>：在涵盖单臂、双臂、仿人机器人等多种平台和任务的广泛实验中，FVP能显著提升3D模仿学习方法（如DP3、RISE）的任务成功率，平均提升达28%，并超越了现有2D/3D预训练方法。同时，FVP适用于不同的3D编码器和数据集。</li>
<li><strong>拓展了3D表征在大模型中的应用</strong>：成功将FVP预训练的3D编码器集成到视觉-语言-动作大模型（RDT-1B）中，证明了3D点云输入结合专用预训练能有效提升此类模型在真实任务中的性能。</li>
</ol>
<p>论文自身提到的局限性主要在于数据依赖：FVP的有效性依赖于包含完整点云信息的机器人操作数据集进行预训练。</p>
<p>本文的启示在于：为机器人学习设计预训练任务时，考虑其时空特性和物理交互本质（如帧间预测）比单纯应用通用的视觉预训练范式（如对比学习、重建）可能更为有效。这为未来探索更贴合机器人感知-行动循环本质的自监督学习目标指明了方向。同时，将3D感知与大规模基础模型结合是一个富有前景的研究方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人学习中现有视觉预训练主要基于2D图像、缺乏对3D世界有效表征的问题，提出了一种名为FVP的4D视觉预训练框架。其核心是将预训练目标构建为“下一个点云预测”问题，并采用条件扩散模型，利用历史帧点云与机器人动作信息进行预测。在12个真实机器人操作任务上的实验表明，FVP将3D扩散策略（DP3）的平均成功率提升了28%，达到了模仿学习领域的先进性能，并能适配多种点云编码器与数据集。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.17230" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>