<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.19236" target="_blank" rel="noreferrer">2508.19236</a></span>
        <span>作者: Gao Huang Team</span>
        <span>日期: 2025-08-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，得益于大规模跨本体机器人数据和预训练视觉语言模型（VLM），视觉-语言-动作（VLA）模型在机器人操作领域取得了显著进展。然而，主流VLA模型（如OpenVLA和π₀）通常仅依赖于当前时刻的观测，忽视了任务中的时序依赖关系，因此在长视野、时序相关的非马尔可夫任务上表现不佳。例如，在“按下按钮”任务中，按下前后的视觉状态几乎相同，仅凭单帧图像难以判断动作是否已完成，这凸显了对时序建模的需求。</p>
<p>认知科学研究表明，人类通过双记忆系统处理操作任务：工作记忆通过瞬时神经活动缓冲短期表征以支持即时控制；而由海马体支持的长期情景记忆则以两种形式编码过往经验——保存精确细节的逐字表征和捕捉抽象语义的要义表征。执行时，工作记忆会从情景记忆中检索与决策相关的上下文，将其与当前表征整合以指导动作，同时将新经验巩固到情景记忆中。</p>
<p>受此启发，本文提出了MemoryVLA，一个用于机器人操作的认知-记忆-动作框架，旨在通过显式的感知-认知记忆机制来建模长视野时序依赖。其核心思路是：利用VLM将观测编码为形成工作记忆的感知与认知令牌，并通过一个感知-认知记忆库长期存储从中巩固的低层细节与高层语义；工作记忆从库中检索决策相关的历史条目，与当前令牌自适应融合，并更新记忆库；最终，一个记忆条件化的扩散动作专家基于这些令牌生成具有时序感知的动作序列。</p>
<h2 id="方法详解">方法详解</h2>
<p>MemoryVLA是一个端到端的机器人操作框架。输入为当前RGB图像和语言指令，输出为未来一系列7自由度动作。</p>
<p><img src="https://arxiv.org/html/2508.19236v2/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：MemoryVLA的整体架构。RGB观测和语言指令被一个7B参数的VLM编码为感知令牌和认知令牌，形成短期工作记忆。工作记忆查询感知-认知记忆库（PCMB）以检索相关的历史上下文（包括高层语义和低层视觉细节），通过门控机制自适应地与当前令牌融合，并通过合并最相似的邻居来巩固PCMB。记忆增强后的令牌随后条件化一个扩散Transformer来预测未来的动作序列。</p>
</blockquote>
<p><strong>1. 视觉语言认知模块</strong>：该模块基于一个在Open-X Embodiment数据集上进一步预训练的7B参数Prismatic VLM构建。视觉编码采用并行的DINOv2和SigLIP主干网络处理RGB图像，将特征拼接为原始视觉令牌。一个使用SE瓶颈结构的感知压缩模块将这些令牌压缩为一组紧凑的感知令牌<code>p</code>（维度为256×<code>d_p</code>）。同时，原始视觉令牌通过线性层投影到语言嵌入空间，并与分词后的指令拼接，输入到LLaMA-7B中。取句子结束（EOS）位置的输出作为认知令牌<code>c</code>（维度为1×<code>d_c</code>），代表紧凑的高层认知语义。<code>p</code>和<code>c</code>共同构成下游模块使用的短期工作记忆。</p>
<p><strong>2. 感知-认知记忆模块</strong>：此模块包含一个长期存储的感知-认知记忆库（PCMB），其灵感来源于人类海马体。PCMB为感知流和认知流分别维护最多L个条目，每个感知条目<code>m_i^p</code>存储细粒度的视觉细节，每个认知条目<code>m_i^c</code>编码高层语义摘要。</p>
<p><img src="https://arxiv.org/html/2508.19236v2/x2.png" alt="记忆模块细节"></p>
<blockquote>
<p><strong>图3</strong>：记忆模块的细节。(a) 检索：当前感知和认知令牌通过带有时间步位置编码的交叉注意力查询PCMB，以获取相关的历史特征。(b) 门控融合：通过门机制自适应地融合当前令牌和检索到的令牌。(c) 合并：融合后的令牌被更新到PCMB中。当PCMB达到容量上限时，计算相邻条目之间的相似度，并合并最相似的一对以保持紧凑性。</p>
</blockquote>
<ul>
<li><strong>记忆检索</strong>：当前工作记忆<code>M_wk</code>（包含<code>p</code>和<code>c</code>）作为双查询，从PCMB中检索当前决策所需的历史信息。每个记忆条目都通过正弦嵌入与其时间步关联，作为位置编码加入。然后，当前令牌与堆叠的记忆张量进行缩放点积注意力计算，再经过一个包含两层Transformer的模块，得到最终的检索嵌入<code>H^p</code>和<code>H^c</code>。</li>
<li><strong>记忆门控融合</strong>：通过学习的门控向量将检索到的嵌入<code>H^p</code>和<code>H^c</code>与当前工作记忆表征集成。门控向量由一个小型MLP处理当前令牌和检索嵌入的拼接后经Sigmoid激活得到。记忆增强后的表征<code>x̃ = g^x ⊙ H^x + (1 - g^x) ⊙ x</code>，其中<code>x</code>代表<code>p</code>或<code>c</code>。</li>
<li><strong>记忆合并</strong>：融合后的表征<code>p̃</code>和<code>c̃</code>被送往动作专家，同时更新到PCMB。当存储条目数超过L时，在每个流内计算相邻条目的余弦相似度，选择相似度最高的一对，通过向量平均进行合并，以此减少冗余，保持记忆库的紧凑性。</li>
</ul>
<p><strong>3. 记忆条件化动作专家</strong>：该专家基于记忆增强的工作记忆<code>{p̃, c̃}</code>预测未来T=16步的动作序列。采用基于扩散Transformer（DiT）的架构，使用DDIM采样，共10步去噪。在每一步去噪时，带噪声的动作令牌与去噪时间步的正弦编码以及认知表征<code>c̃</code>拼接。一个认知注意力层利用高层语义进行条件化引导，一个感知注意力层则用感知特征<code>p̃</code>补充细粒度视觉细节。组合后的表征通过前馈网络进行精炼，得到该步的去噪动作。模型使用预测动作与目标动作之间的均方误差（MSE）损失进行训练。</p>
<p><strong>创新点</strong>：与现有方法（如简单拼接帧、使用LSTM传播潜在表征、或将历史状态绘制为轨迹）相比，MemoryVLA的创新在于受认知科学启发的双流（感知/认知）记忆设计，以及包含检索、自适应门控融合和主动合并的完整记忆机制，从而能同时利用高层语义和低层细节进行有效的长视野时序建模。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟和真实世界中进行，涵盖3种机器人（WidowX, Google, Franka）、6个基准测试、超过150个任务和500种变体。</p>
<p><img src="https://arxiv.org/html/2508.19236v2/x3.png" alt="实验设置概览"></p>
<blockquote>
<p><strong>图4</strong>：实验设置概览。顶部：四个模拟基准测试。底部：真实世界评估。总共在3个机器人、6个基准测试上进行了评估，涵盖超过150个任务和500种变体。</p>
</blockquote>
<p><strong>1. 模拟基准测试结果</strong>：</p>
<ul>
<li><strong>SimplerEnv-Bridge</strong>：如表1所示，MemoryVLA平均成功率为**71.9%<strong>，比CogACT-Large基线高出</strong>+14.6%**，也超过了π₀等方法。</li>
<li><strong>SimplerEnv-Fractal</strong>：如表2所示，总体成功率为**72.7%<strong>，比CogACT高</strong>+4.6%**。在视觉匹配（VM）和视觉聚合（VA）设置下分别达到77.7%和67.7%。</li>
<li><strong>LIBERO</strong>：在五个套件（Spatial, Object, Goal, Long-10, Long-90）上的评估结果如表3所示。MemoryVLA总体成功率为**96.5%<strong>，比CogACT高</strong>+3.3%**，且仅使用第三人称RGB输入。</li>
<li><strong>Mikasa-Robo</strong>：如表4所示，MemoryVLA平均成功率为**41.2%<strong>，比之前最好的π₀高出</strong>+11.8%**，在ShellGameTouch任务上优势达+41.0%。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.19236v2/x4.png" alt="SimplerEnv-Bridge结果"></p>
<blockquote>
<p><strong>图5</strong>：表1，在SimplerEnv-Bridge上的性能对比。MemoryVLA取得最高平均成功率71.9%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.19236v2/x5.png" alt="SimplerEnv-Fractal结果"></p>
<blockquote>
<p><strong>图6</strong>：表2，在SimplerEnv-Fractal上的性能对比。MemoryVLA在VM和VA设置下均优于基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.19236v2/x6.png" alt="LIBERO结果"></p>
<blockquote>
<p><strong>图7</strong>：表3，在LIBERO上的性能对比。MemoryVLA在所有五个套件上均达到领先水平。</p>
</blockquote>
<p><strong>2. 真实世界评估</strong>：设计了12个任务，分为6个通用技能任务和6个长视野时序任务，在Franka和WidowX机器人上测试。如图8所示，MemoryVLA在通用任务和时序任务上分别获得85%和83%的成功分数，在长视野时序任务上比CogACT高出**+26**分，显著优于π₀。</p>
<p><img src="https://arxiv.org/html/2508.19236v2/x7.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图8</strong>：真实世界性能对比。MemoryVLA在通用和长视野时序任务上均大幅领先基线方法。</p>
</blockquote>
<p><strong>3. 消融实验</strong>：如图9所示，研究移除了感知记忆、认知记忆、门控融合以及记忆合并机制。结果表明，感知记忆和认知记忆对性能均有显著贡献；门控融合机制优于简单的拼接；合并机制对于长任务（LIBERO Long-90）尤为重要，能防止记忆库被无关信息淹没。完整的MemoryVLA模型在所有设置下均取得最佳性能。</p>
<p><img src="https://arxiv.org/html/2508.19236v2/x8.png" alt="消融实验"></p>
<blockquote>
<p><strong>图9</strong>：消融实验结果。移除任何核心组件（感知记忆、认知记忆、门控融合、合并）都会导致性能下降，验证了各模块的有效性。</p>
</blockquote>
<p><strong>4. 鲁棒性与泛化性</strong>：论文在附录中展示了MemoryVLA在分布外条件下的强鲁棒性，包括面对变化的背景、干扰物、物体、容器、光照和遮挡。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>受人类记忆系统启发，提出了MemoryVLA，这是一个认知-记忆-动作框架，通过感知-认知记忆机制显式地建模长视野时序依赖，用于机器人操作。</li>
<li>设计了一个包含工作记忆的感知-认知记忆库，实现了跨高层认知和低层感知的决策相关上下文检索、自适应融合以及通过合并相邻相似条目进行记忆巩固的完整机制。</li>
<li>在多个模拟基准测试（SimplerEnv, LIBERO, Mikasa-Robo）和真实世界任务上达到了最先进的性能，尤其在具有挑战性的长视野时序任务上显著领先现有方法，证明了时序记忆建模的重要性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：感知-认知记忆库的容量有限；模型的训练需要大量的计算资源。</p>
<p><strong>启示</strong>：MemoryVLA的成功表明，将认知科学中的记忆机制引入机器人学习是一个富有前景的方向。其双流记忆设计（分离语义与细节）和主动记忆管理策略（检索、融合、合并）为后续研究提供了有价值的参考。未来工作可以探索更高效或动态可扩展的记忆结构，以及将此类记忆机制整合到更广泛的具身智能模型中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中VLA模型忽略时间上下文、难以处理长时程依赖任务的问题，提出MemoryVLA框架。其核心是受人类记忆机制启发的感知-认知记忆系统：工作记忆缓冲当前感知与认知token，记忆库存储并整合历史细节与语义；通过检索与融合相关记忆条目，驱动记忆条件扩散动作专家生成时序感知的动作序列。实验表明，在仿真与真实世界的150多项任务中，MemoryVLA均优于先进基线，如在Bridge任务上成功率提升14.6%，在长时程真实任务上较基线提升26分。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.19236" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>