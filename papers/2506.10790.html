<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Human-Robot Navigation using Event-based Cameras and Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Human-Robot Navigation using Event-based Cameras and Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.10790" target="_blank" rel="noreferrer">2506.10790</a></span>
        <span>作者: Rodrigo Verschae Team</span>
        <span>日期: 2025-06-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于图像的视觉伺服（IBVS）机器人导航控制器通常依赖于传统的帧式相机，这些相机以固定帧率工作，在高速运动场景中易受运动模糊和延迟的影响。尽管深度强化学习（DRL）已成功应用于IBVS，但如何高效处理动态、高速的视觉信息仍是挑战。事件相机作为一种新型传感器，以微秒级分辨率异步响应亮度变化，具有高动态范围和低运动模糊的特性，为机器人导航提供了新的可能性。然而，将事件相机的异步视觉流与强化学习相结合，用于以人为中心的机器人导航任务，此前尚未得到充分探索。</p>
<p>本文针对传统帧式相机在动态人机导航中的局限性，提出将事件相机的异步感知能力与强化学习策略优化相结合的新视角。核心思路是：利用事件相机灵活的时间窗口处理视觉信息，通过一个混合模仿学习（IL）与深度确定性策略梯度（DDPG）的两阶段训练流程，学习一个能够实时进行行人跟随和避障的机器人导航控制器。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架是一个集成了事件相机感知、多传感器融合和强化学习策略优化的导航控制系统。其输入是来自事件相机（DAVIS346）、深度相机（RealSense）和2D激光雷达（Hokuyo LIDAR）的原始数据，输出是机器人平台（Pioneer 3-AT）的线速度和角速度增量（Δv_r, Δw_r）。流程分为四个主要部分：传感器数据获取、特征提取（事件基于的行人检测）、系统状态构建以及基于DDPG的控制器。</p>
<p><img src="https://arxiv.org/html/2506.10790v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：人机导航框架总览。从左至右展示了数据流：传感器（事件相机、深度相机、激光雷达）采集数据；特征提取模块使用YOLOv5s在SAE表示上进行行人检测；系统状态构建器整合机器人状态、检测框、行人距离和障碍物信息；最后，DDPG控制器输出速度指令。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>传感器与数据表示</strong>：事件数据采用“活动事件表面”（SAE）表示，将固定时间窗口（如10ms）内的事件按极性分为正负两个通道，并依据事件发生时间进行缩放，合成为一张图像（I_SAE），用于捕捉场景中的近期活动。<br><img src="https://arxiv.org/html/2506.10790v1/x3.png" alt="传感器配置"><blockquote>
<p><strong>图4</strong>：Pioneer 3-AT平台上的传感器配置。底部为Hokuyo 2D激光雷达，顶部为DAVIS346事件相机与RealSense RGB-D相机耦合。</p>
</blockquote>
</li>
<li><strong>事件基于的行人检测</strong>：使用在事件数据集PEDRo上训练的YOLOv5s模型，对SAE表示进行行人检测。YOLOv5s因其轻量化和实时推理能力被选中。检测输出为行人边界框的中心水平坐标x_bbox。<br><img src="https://arxiv.org/html/2506.10790v1/x6.png" alt="检测流程"><blockquote>
<p><strong>图6</strong>：事件基于的高速行人检测流程。原始事件流被转换为SAE表示，随后输入YOLOv5s网络得到检测边界框。</p>
</blockquote>
</li>
<li><strong>模仿学习预训练</strong>：首先设计一个基于比例-微分（PD）规则的专家控制器。然后，使用行为克隆算法，利用该专家控制器生成的少量成功轨迹数据，训练一个演员网络（MLP，输入6维，两层隐藏层各30个神经元）。损失函数为预测动作与专家动作之间的均方误差（MSE）。此阶段旨在为后续强化学习提供一个较好的初始策略，加速收敛。</li>
<li><strong>强化学习优化（DDPG）</strong>：采用DDPG算法在模拟环境中进一步优化策略。<ul>
<li><strong>状态（s_t）</strong>：6维向量，包括机器人当前线速度(v_r)、角速度(w_r)、行人边界框中心x坐标(x_box)、到行人的距离(d_ped)、到最近障碍物的距离(d_obs)和角度(θ_obs)。</li>
<li><strong>动作（a_t）</strong>：连续值，即线速度和角速度的增量（Δv_r, Δw_r），每0.1秒应用一次。</li>
<li><strong>奖励函数（r_t）</strong>：设计复杂，旨在鼓励机器人将行人保持在图像中心（x_target=173）并维持2米的目标距离。奖励包括高额成功奖励（当位置和距离误差极小时）、高额惩罚（碰撞、行人过近或过远）以及一个平衡项（r_{t_eq}），该平衡项对距离误差和位置误差进行加权惩罚，其中位置误差的权重会随距离误差增大而减小。<br><img src="https://arxiv.org/html/2506.10790v1/x7.png" alt="DDPG流程"><blockquote>
<p><strong>图7</strong>：应用于机器人导航和行人跟踪的DDPG算法示意图。展示了演员-评论家网络结构及经验回放缓冲区的更新流程。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，本文的创新具体体现在：1) <strong>首次将事件相机视觉与强化学习整合</strong>用于以人为中心的机器人导航；2) 提出 <strong>IL+RL的混合训练流程</strong>，利用模仿学习初始化策略，显著减少了纯RL训练所需的样本和时间；3) 利用事件相机的<strong>异步特性</strong>，允许控制器在灵活的时间间隔内处理感知信息，实现自适应推理与控制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：所有实验均在Gazebo模拟器中进行。使用了一个自定义的21x12平方米仿真场景（图5），包含走廊和障碍物，行人以0.7米/秒的速度移动，机器人需执行“8字形”轨迹跟踪。<br><img src="https://arxiv.org/html/2506.10790v1/x4.png" alt="仿真场景"></p>
<blockquote>
<p><strong>图5</strong>：仿真场景设计：(a) Gazebo中的布局；(b) 场景渲染；(c) 行人运动轨迹。</p>
</blockquote>
<p><strong>对比方法</strong>：论文定量比较了三种控制器：1) <strong>PD控制器</strong>（专家）；2) <strong>纯模仿学习（IL）控制器</strong>（仅通过行为克隆训练）；3) <strong>强化学习（RL）控制器</strong>（即本文提出的IL+DDPG混合方法）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能对比</strong>：RL控制器在100秒的测试中取得了最高的<strong>成功率（96%）</strong>，显著优于PD控制器（76%）和IL控制器（80%）。RL控制器在维持目标距离（2米）方面也表现最佳，平均绝对距离误差为0.18米。<br><img src="https://arxiv.org/html/2506.10790v1/x8.png" alt="性能对比"><blockquote>
<p><strong>图8</strong>：三种控制器在100秒模拟运行中的性能对比。RL控制器在成功率和距离误差上均优于PD和IL控制器。<br><img src="https://arxiv.org/html/2506.10790v1/x9.png" alt="轨迹对比"><br><strong>图9</strong>：机器人（红色）跟踪行人（蓝色）的轨迹对比。RL控制器（右）的轨迹最平滑，且能更好地处理转弯。</p>
</blockquote>
</li>
<li><strong>训练曲线与消融实验</strong>：实验证明了模仿学习预训练的有效性。从随机策略开始的纯DDPG训练收敛缓慢且不稳定，而使用IL预训练权重初始化的DDPG（即本文方法）能快速收敛并获得更高的累积奖励。<br><img src="https://arxiv.org/html/2506.10790v1/x11.png" alt="训练曲线"><blockquote>
<p><strong>图10</strong>：DDPG训练期间的累积奖励曲线。使用IL预训练（橙色）比随机初始化（蓝色）收敛更快、更稳定。</p>
</blockquote>
</li>
<li><strong>定性避障结果</strong>：当在路径中引入静态障碍物时，RL控制器能够成功规划绕行路径，在避开障碍物后重新找回并跟随行人。<br><img src="https://arxiv.org/html/2506.10790v1/x17.jpg" alt="避障序列"><blockquote>
<p><strong>图16</strong>：RL控制器避障定性结果序列图。展示了机器人检测到障碍物、执行绕行并重新跟踪行人的过程。</p>
</blockquote>
</li>
<li><strong>事件表示与检测器分析</strong>：论文比较了不同事件表示方法（SAE vs. 事件帧）对YOLOv5检测性能的影响，结果显示SAE表示能获得更高的<a href="mailto:&#x6d;&#x41;&#80;&#x40;&#48;&#46;&#x35;">&#x6d;&#x41;&#80;&#x40;&#48;&#46;&#x35;</a>（0.896 vs. 0.848）。<br><img src="https://arxiv.org/html/2506.10790v1/x12.png" alt="检测性能"><blockquote>
<p><strong>图11</strong>：不同事件表示下YOLOv5s的检测精度-召回曲线。SAE表示优于事件帧表示。<br><img src="https://arxiv.org/html/2506.10790v1/x13.png" alt="误差分析"><br><strong>图12</strong>：不同控制器在x方向（居中）和距离上的跟踪误差。RL控制器的误差波动最小。<br><img src="https://arxiv.org/html/2506.10790v1/x14.png" alt="速度曲线"><br><strong>图13</strong>：机器人线速度和角速度命令随时间变化曲线。RL控制器的速度输出更平滑。<br><img src="https://arxiv.org/html/2506.10790v1/x15.png" alt="奖励分解"><br><strong>图14</strong>：DDPG训练期间各奖励分量的变化。显示了控制器如何学习平衡不同目标。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验总结</strong>：关键组件的贡献在于：1) <strong>模仿学习预训练</strong>是加速DDPG收敛、提升最终性能的关键；2) <strong>基于SAE的事件表示</strong>比简单的事件帧能提供更有效的检测特征；3) <strong>精心设计的复合奖励函数</strong>引导智能体同时优化跟踪精度和避障。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了首个将事件相机异步视觉与深度强化学习相结合的人机导航框架；2) 设计了一种混合（IL+RL）训练流程，有效解决了RL在机器人任务中样本效率低下的问题；3) 在仿真环境中对PD、IL和RL控制器进行了全面的定量比较，验证了所提方法的优越性。</p>
<p><strong>局限性</strong>：论文明确提到，当前工作<strong>仅限于仿真验证</strong>。虽然仿真环境经过了精心设计以利用事件相机的特性，但将系统迁移到真实的物理机器人平台，处理传感器噪声、校准误差和更复杂的动态环境，是未来的挑战。</p>
<p><strong>后续研究启示</strong>：这项工作为在高速、动态的人机交互（HRI）场景中应用事件相机和强化学习开辟了道路。未来的研究方向可能包括：在真实机器人上进行部署和测试；探索更复杂的事件数据表示与神经网络架构；将框架扩展至多行人跟踪或更复杂的社会导航规范；以及研究如何更好地将事件相机的异步性与RL算法的更新机制相结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人机导航中传统图像控制器存在的固定帧率、运动模糊和延迟问题，提出了一种结合事件相机与强化学习的实时导航控制器。方法核心是利用事件相机的异步特性，融合深度/激光雷达数据，通过模仿学习初始化后采用DDPG进行策略优化，实现自适应推理与控制。在仿真实验中，该方法展现了鲁棒的导航、行人跟随与避障能力，并对比了PD、IL与RL控制器的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.10790" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>