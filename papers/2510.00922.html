<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>On Discovering Algorithms for Adversarial Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>On Discovering Algorithms for Adversarial Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.00922" target="_blank" rel="noreferrer">2510.00922</a></span>
        <span>作者: Pradeep Varakantham Team</span>
        <span>日期: 2025-10-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>对抗性模仿学习（AIL）方法在专家示范有限的情况下表现有效，但通常被认为不稳定。这些方法通常分解为两个组件：密度比（DR）估计，即判别器估计策略与专家状态下状态-动作对的相对占用率；以及奖励分配（RA），即将该比值转换为用于训练策略的奖励信号。现有研究主要集中在改进密度估计上，而奖励分配函数在影响训练动态和最终策略性能方面的作用在很大程度上被忽视了。现有的RA函数通常源于发散度最小化目标，严重依赖人工设计和巧思。本文针对这一具体痛点，提出了一个新的视角：研究数据驱动的RA函数的发现，即直接基于所得模仿策略的性能来优化RA函数。本文的核心思路是：利用LLM引导的进化框架高效探索RA函数空间，从而产生首个元学习的AIL算法——DAIL，其性能超越了现有的人工设计基线。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法旨在通过元学习发现最优的奖励分配函数，其整体框架遵循标准的对抗性模仿学习流程，但将其中的人工设计RA函数替换为通过进化搜索发现的数据驱动函数。</p>
<p>标准AIL流程包含三个迭代步骤：1) <strong>策略推演</strong>：执行当前策略生成轨迹；2) <strong>密度比估计</strong>：训练一个判别器（分类器）来区分专家和策略生成的状态-动作对，最优判别器的逻辑值（softmax前）对应于对数密度比 $\log(\rho_E / \rho_\pi)$；3) <strong>奖励分配与策略改进</strong>：根据RA函数 $r_f$ 将对数密度比映射为奖励，用于更新策略。</p>
<p>本文的创新在于对步骤3的RA函数进行优化。作者将RA函数的发现形式化为一个双层优化问题：外层最小化学习后策略与专家之间的Wasserstein距离，内层是使用给定RA函数进行策略优化的标准AIL过程。由于通过整个对抗训练循环进行反向传播在计算上难以处理，本文采用了一种LLM引导的进化搜索框架来优化此目标。</p>
<p><img src="https://arxiv.org/html/2510.00922v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：不同奖励分配函数的可视化。展示了AIRL、GAIL、FAIRL、GAIL-heuristic以及本文发现的DAIL函数（$r_{disc}$）的形状，说明了不同RA函数如何塑造策略的学习动态。</p>
</blockquote>
<p>核心模块是<strong>LLM引导的进化搜索</strong>，其组件与流程如下：</p>
<ul>
<li><strong>基础种群</strong>：使用从已知f-发散度推导出的RA函数（GAIL, FAIRL, AIRL, GAIL-heuristic）初始化搜索，提供一个稳健的起点。</li>
<li><strong>适应度评估</strong>：每个候选RA函数 $r_f$ 的适应度通过以下方式评估：使用 $r_f$ 训练一个策略至收敛，然后计算其推演轨迹与专家示范之间的Wasserstein距离。距离越小，适应度越高。</li>
<li><strong>交叉与变异</strong>：从当前种群中采样父代函数对，连同其适应度分数一起提供给LLM。LLM被提示去合成一个新的函数，该函数结合了父代的优良特性（例如，融合它们的功能形式），以期望提升性能。这本质上是由LLM智能驱动的交叉和变异操作。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.00922v1/x2.png" alt="进化搜索流程"></p>
<blockquote>
<p><strong>图2</strong>：LLM引导进化框架的可视化。附录B包含了该框架的伪代码。</p>
</blockquote>
<p>搜索过程是迭代的：从基础种群开始，在每一代中，随机采样父代对，由LLM生成新的候选函数，评估所有候选函数的适应度，然后选择表现最好的个体形成下一代种群，直至达到停止标准（如性能平台期）。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>问题视角</strong>：首次将RA函数的设计视为一个可从数据中学习的元优化问题，而非固定的人工设计选择。2) <strong>方法</strong>：首次将LLM引导的进化搜索应用于发现AIL的RA函数，利用了代码表示的灵活性和LLM的归纳偏差。3) <strong>结果</strong>：发现了超越经典f-发散度理论框架的、表现更优的RA函数形式 $r_{disc}$。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在两个基准套件上进行：Brax（包含Ant, Reacher, Walker2d, HalfCheetah, Hopper）和Minatar（包含Asterix, SpaceInvaders, Breakout）。为每个任务收集了10个由PPO训练策略生成的成功专家示范。策略优化默认使用PPO，判别器使用梯度惩罚进行正则化。所有结果均在16个独立随机种子上取平均。</p>
<p><strong>对比基线</strong>：对比了当前最先进的人工设计基线，包括GAIL、AIRL、FAIRL和GAIL-heuristic。</p>
<p><strong>进化搜索过程</strong>：进化搜索在Minatar SpaceInvaders环境上进行，使用了约200个候选RA函数。最终发现的最佳奖励分配函数为：<br>$r_{\text{disc}}(x) = 0.5 \cdot \text{sigmoid}(x) \cdot [\tanh(x) + 1]$<br>其中 $x = \log(\rho_E / \rho_\pi)$。基于此，作者提出了**Discovered Adversarial Imitation Learning (DAIL)**。</p>
<p><img src="https://arxiv.org/html/2510.00922v1/x3.png" alt="进化性能"></p>
<blockquote>
<p><strong>图3</strong>：Minatar SpaceInvaders环境上各代进化搜索的性能。报告了每代表现最佳的成员，第0代为基础种群。与最佳基线（GAIL）相比，DAIL将$\mathcal{W}$距离减少了20%，归一化回报提高了12.5%。</p>
</blockquote>
<p><strong>泛化性能</strong>：在未见过的Brax和Minatar（不包括SpaceInvaders）环境上评估DAIL。</p>
<p><img src="https://arxiv.org/html/2510.00922v1/x4.png" alt="聚合性能"></p>
<blockquote>
<p><strong>图4</strong>：在Brax和Minatar套件（不包括SpaceInvaders）上的聚合性能。DAIL在Minatar上显著优于所有基线，在Brax上大多数指标优于基线，中位数略低于AIRL，但平均性能有显著提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.00922v1/x5.png" alt="概率提升与算法泛化"></p>
<blockquote>
<p><strong>图5</strong>：（左）DAIL相对于各基线在Brax任务上的性能提升概率（PI）。所有值均显著高于0.5，证明了DAIL的优越性。（右）使用A2C策略优化器时，DAIL与GAIL在Minatar SpaceInvaders上的性能对比。DAIL保持了显著优势，展示了其对不同策略优化算法的泛化能力。</p>
</blockquote>
<p><strong>分析实验</strong>：为探究DAIL性能优异的原因，作者分析了训练稳定性和奖励信号特性。</p>
<p><img src="https://arxiv.org/html/2510.00922v1/x6.png" alt="熵与对数密度比分布"></p>
<blockquote>
<p><strong>图6</strong>：（左）在HalfCheetah环境中，使用不同RA函数训练时策略熵的变化。使用$r_{disc}$的策略熵收敛到更低水平，接近使用模拟器奖励的PPO基线，表明其提供了丰富且信息量大的信号。（右）在Minatar SpaceInvaders训练期间，对数密度比 $\log(\rho_E / \rho_\pi)$ 的分布。$r_{disc}$ 在 $x \lesssim -1.8$ 时饱和接近零，过滤了低质量状态-动作对的噪声，而GAIL在此区域仍赋予高正值奖励，可能导致不稳定。</p>
</blockquote>
<p><strong>消融实验</strong>：对 $r_{disc}$ 的组成部分进行消融研究，比较 $r_{disc}$、$\text{sigmoid}(x)$ 和 $0.5 \cdot [\tanh(x) + 1]$。</p>
<p><img src="https://arxiv.org/html/2510.00922v1/x7.png" alt="组件函数对比"></p>
<blockquote>
<p><strong>图7</strong>：（左）训练期间对数密度比的分布（核密度估计）。（右）$r_{disc}$ 与其组件函数在Minatar SpaceInvaders上的性能对比。$r_{disc}$ 性能最佳，验证了其特定组合形式的有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 强调了奖励分配（RA）函数在对抗性模仿学习（AIL）的稳定性和性能中的关键作用，这是一个被忽视的方面。2) 提出了一个新颖的元学习框架，利用LLM引导的进化搜索自动发现最优的RA函数，从而产生了首个元学习的AIL算法——DAIL。3) 通过实验证明了DAIL在未见过的环境和策略优化算法上具有优越的泛化性能，并通过分析揭示了其通过提供更有信息量的、有界的奖励信号来提升训练稳定性的机制。</p>
<p><strong>局限性</strong>：论文自身提到的局限性包括：1) 为了探索更富表达力的RA函数，该方法放弃了对RA函数（如凸性）的约束，因此也放弃了经典f-发散度框架下的理论收敛保证。2) 尽管进化搜索相对高效，但发现过程仍需要计算资源。3) 发现的RA函数在更复杂或更高维的环境中的泛化能力有待进一步验证。</p>
<p><strong>对后续研究的启示</strong>：1) <strong>数据驱动算法设计</strong>：这项工作展示了超越传统理论推导、通过数据驱动方法自动发现算法组件的潜力，可应用于其他存在不稳定或设计瓶颈的机器学习领域。2) <strong>LLM在算法发现中的作用</strong>：成功利用LLM的代码生成和领域知识来引导搜索，为“学习如何学习”提供了新的强大工具。3) <strong>AIL的稳定化</strong>：研究重心可以从单纯的判别器稳定化扩展到整个奖励形成管道，特别是奖励信号的塑造，为改善AIL训练提供了新的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对对抗模仿学习（AIL）训练不稳定且奖励分配函数依赖人工设计的问题，提出一种数据驱动的奖励分配函数自动发现方法。核心技术是利用LLM引导的进化框架，在奖励分配函数空间中进行高效搜索，从而得到首个元学习AIL算法DAIL。实验表明，DAIL在未见过的环境和策略优化算法中具有强泛化能力，性能优于现有最优人工设计基线，并显著提升了训练稳定性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.00922" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>