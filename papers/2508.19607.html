<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.19607" target="_blank" rel="noreferrer">2508.19607</a></span>
        <span>作者: Jens Kober Team</span>
        <span>日期: 2025-08-27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人操作领域，解决现实世界中包含连续接触的长周期任务是一个长期挑战。当前主流方法包括使用状态机或符号推理进行显式序列规划，以及采用分层强化学习（HRL）进行学习式序列决策。然而，前者适应性有限且易产生误差累积；后者（如MAPLE）虽能处理复杂任务，但其底层依赖于静态控制器，在接触丰富的环境中性能受限，且缺乏对交互力的适应性控制。另一方面，现有的可变刚度控制方法（如从演示中学习、参数调度或基于RL的学习）主要针对短周期任务，难以处理长周期任务中的序列依赖关系。</p>
<p>本文旨在弥合序列任务规划与自适应刚度控制之间的鸿沟。针对接触丰富的长周期操作任务，本文提出了一种名为IMP-HRL（Impedance Primitive-augmented HRL）的新框架。其核心思路是：在现有HRL框架的基础上，扩展底层动作空间以包含阻抗参数，并引入一个自适应控制器在原始动作执行期间动态调整刚度，从而在保证任务成功的同时最大化系统的柔顺性（Compliance）与安全性。</p>
<h2 id="方法详解">方法详解</h2>
<p>IMP-HRL框架以MAPLE这一先进的HRL框架为基础进行扩展。MAPLE将序列决策问题建模为参数化动作马尔可夫决策过程（PAMDP），其包含高层任务策略（选择原始动作）和底层参数策略（为所选原始动作预测参数）。IMP-HRL的核心创新在于引入了三个关键组件，使原始动作具备可变阻抗控制能力。</p>
<p><img src="https://arxiv.org/html/2508.19607v1/images/cover/fig1-cover.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：IMP-HRL框架整体示意图。展示了将阻抗原始动作（Impedance Primitive）集成到HRL策略中。高层策略从库中选择原始动作（如Reach, Grasp），底层参数策略输出包含位置和刚度参数(Kx, Ky, Kz, Kψ)的初始估计，随后由自适应控制器进行动态调整。</p>
</blockquote>
<p><strong>1. 阻抗原始动作（Impedance Primitive）</strong><br>为适应接触丰富的环境，本文扩展了MAPLE中仅包含位置信息的参数空间θ。新的参数空间不仅包含目标位置（如x, y, z），还包含了沿不同坐标轴的刚度参数（Kx, Ky, Kz）以及处理姿态变化的刚度参数Kψ。这使得智能体能够通过采样刚度值作为动作来控制阻抗。阻尼矩阵D根据临界阻尼条件动态计算（D(t)=2√K(t)），以减少需学习的参数数量。需要注意的是，底层策略预测的刚度参数将作为自适应控制器的初始估计值。</p>
<p><strong>2. Affordance耦合（Affordance Coupling）</strong><br>为了在训练中高效探索并鼓励柔顺性，本文结合了位置affordance和刚度affordance。位置affordance（继承自MAPLE）鼓励智能体靠近环境中的物体。新提出的刚度affordance旨在最大化可能的柔顺性，其模型化为 <code>a_stiff(s，θ; p) = 1 / (1 + exp(α * (K - K_thresh)))</code>，其中K是预测的刚度，K_thresh是阈值，α是缩放因子。该函数在刚度低时提供高奖励，鼓励智能体在可能的情况下选择低刚度。最终的affordance奖励是两者相乘 <code>a_coupled = a_pos * a_stiff</code>。这种耦合方式避免了直接惩罚高刚度值所需的繁琐奖励权重调整，并在训练早期鼓励智能体探索低刚度参数。</p>
<p><img src="https://arxiv.org/html/2508.19607v1/images/affordance/fig2-affordance.png" alt="Affordance耦合示意图"></p>
<blockquote>
<p><strong>图2</strong>：Affordance耦合的热力图可视化。展示了结合位置和刚度affordance后，在任务不同阶段（如接近物体、接触）对智能体探索的引导。</p>
</blockquote>
<p><strong>3. 自适应控制器（Adaptive Controller）</strong><br>当策略选择一个原始动作及其参数后，行为通过一个闭环控制方案执行。自适应控制器以底层策略输出的初始刚度估计为起点，在原始动作执行期间实时调整刚度。其灵感来源于人类肌肉刚度调节，遵循公式：<code>K˙(t) = β|ε(t)| - γE</code>。其中，ε(t)是闭环反馈误差，E是关节消耗的能量，β和γ是缩放因子。该公式的含义是：刚度根据跟踪误差（β项）增加，根据能量消耗（γ项）减少，从而在需要精度时提高刚度，在可能时降低刚度以节省能量并提高柔顺性。</p>
<p><img src="https://arxiv.org/html/2508.19607v1/images/cover/fig4-adaptive.png" alt="自适应控制器集成"></p>
<blockquote>
<p><strong>图4</strong>：自适应阻抗控制器在底层参数化策略中的集成示意图。控制器接收初始刚度参数和状态反馈，动态调整刚度值并计算对应的阻尼，输出最终的关节力矩。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.19607v1/images/AFORCE-plot/fig3-aforce.png" alt="自适应刚度示例"></p>
<blockquote>
<p><strong>图3</strong>：执行擦拭任务时的自适应刚度行为示例。展示了在执行椭圆轨迹擦拭时，刚度如何根据任务需求动态变化。</p>
</blockquote>
<p>与现有方法相比，IMP-HRL的创新点在于：1) 将可变阻抗控制能力集成到HRL的原始动作中，使其能处理长周期接触任务；2) 提出了affordance耦合机制，在不损害探索效率的前提下系统性地鼓励柔顺性；3) 设计了一个在原始动作执行期间在线微调刚度的自适应控制器，实现了动态的刚度调节。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在四个接触丰富的仿真环境（Lift-举块、Door-开门、Wipe-擦拭、Cleanup-清理）中进行，使用Franka Emika Panda机器人和Robosuite仿真器。应用了包括桌面摩擦、高度、物体位置等在内的域随机化。所有结果基于5个随机种子平均。此外，还在真实机器人上进行了部署验证。</p>
<p><strong>对比基线</strong>：主要与先进的HRL基线方法MAPLE进行对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>学习性能</strong>：通过收敛时间评估学习效率。从学习曲线（图7-10）可以看出，在Door任务中两者收敛时间相近；在Lift和Cleanup任务中MAPLE稍快；而在Wipe任务中，IMP-HRL收敛显著更快，这得益于其可变刚度能力能更好地适应任务要求。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.19607v1/images/rewards/fig7-lift.png" alt="学习曲线-举块"></p>
<blockquote>
<p><strong>图7</strong>：Lift任务的学习曲线。IMP-HRL与MAPLE收敛速度相近。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.19607v1/images/rewards/fig7-door.png" alt="学习曲线-开门"></p>
<blockquote>
<p><strong>图8</strong>：Door任务的学习曲线。两者表现出相似的收敛速度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.19607v1/images/rewards/fig7-wipe.png" alt="学习曲线-擦拭"></p>
<blockquote>
<p><strong>图9</strong>：Wipe任务的学习曲线。IMP-HRL的收敛速度明显快于MAPLE。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.19607v1/images/rewards/fig7-cleanup.png" alt="学习曲线-清理"></p>
<blockquote>
<p><strong>图10</strong>：Cleanup任务的学习曲线。MAPLE收敛略快于IMP-HRL。</p>
</blockquote>
<ol start="2">
<li><strong>最大交互力与刚度行为</strong>：IMP-HRL能够根据任务上下文自适应调整刚度。例如，在桌面任务（Lift， Cleanup）中，靠近桌面时保持Kz较低以减少冲击力，同时提高Kx， Ky以精确对准物体。这导致了更低的平均交互力（图12）。特别是在Wipe和Cleanup任务中，由于柔顺性提高，桌面冲击力显著降低。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.19607v1/images/adaptive/fig8-composition.png" alt="可变刚度行为"></p>
<blockquote>
<p><strong>图11</strong>：可变刚度行为展示，强调了柔顺性和刚度降低。背景网格颜色代表执行的不同原始动作（grasp， reach， push），刚度值随之变化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.19607v1/images/force/fig9-forces.png" alt="最大交互力对比"></p>
<blockquote>
<p><strong>图12</strong>：最大交互力对比。IMP-HRL在所有任务中施加的力都更低且标准差更小，表明其对任务随机化的敏感性更低。</p>
</blockquote>
<ol start="3">
<li><strong>组合性</strong>：通过组合性度量评估策略生成可重复行为序列的能力。如图13所示，在Door任务中IMP-HRL的原始动作选择更一致；在Cleanup任务中所需原始动作执行次数更少；在Wipe任务中表现出比MAPLE更好的组合性。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.19607v1/images/compositionality/fig10-compositionality.png" alt="组合性对比"></p>
<blockquote>
<p><strong>图13</strong>：组合性对比，展示了学习到的序列行为。行对应5次环境运行生成的原始动作序列，IMP-HRL的序列更一致、更精简。</p>
</blockquote>
<ol start="4">
<li><strong>成功率</strong>：如表II所示，在仿真中，IMP-HRL在Lift和Door任务上与MAPLE持平（均100%），在Cleanup任务上略低（87% vs 91%），但在Wipe任务上成功率大幅提升（86% vs 42%）。这凸显了刚度控制在连续接触任务中的关键作用。在真实世界实验中，IMP-HRL也取得了良好的成功率（Lift 90%， Door N/A， Wipe 70%， Cleanup 80%），验证了其sim2real能力。</li>
</ol>
<p><strong>消融实验</strong>：论文通过消融实验（图14）分析了各组件贡献。完整模型（IMP-HRL）性能最佳。仅使用阻抗原始动作（无自适应控制器）在需要精细力控的任务（如Wipe）上性能下降。仅使用affordance耦合（无阻抗原始动作）则完全无法学习可变刚度行为。这证明了阻抗原始动作、affordance耦合和自适应控制器三者协同工作的必要性。</p>
<p><img src="https://arxiv.org/html/2508.19607v1/images/ablation/ablation.png" alt="消融实验"></p>
<blockquote>
<p><strong>图14</strong>：消融实验结果。对比了完整IMP-HRL、仅使用阻抗原始动作（无自适应控制）、仅使用affordance耦合（无阻抗动作）以及MAPLE基线在各任务上的成功率，验证了各核心组件的有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个阻抗原始动作增强的分层强化学习框架（IMP-HRL），首次将长周期序列规划与自适应可变刚度控制有效结合。</li>
<li>设计了affordance耦合机制，将位置信息与刚度激励相结合，引导智能体在探索中自动寻求高柔顺性策略，简化了奖励设计。</li>
<li>集成了一种基于误差和能量消耗的自适应刚度控制器，能够在每个原始动作执行期间动态优化刚度，平衡任务精度与交互安全。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>强调柔顺性的affordance耦合可能在某些需要精确操作（如抓取对齐）而非接触的任务中限制学习效率，这在Lift和Cleanup任务的学习曲线中有所体现。</li>
<li>自适应控制器的参数（β和γ）需要通过示教或手动调试获得，这个过程可能耗时且需要物理交互。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>本文展示的将特定物理控制模态（如阻抗控制）与高层序列学习结合的范式，可以扩展到其他领域（如力/位混合控制、视觉伺服等）。</li>
<li>未来工作可以探索如何将自适应控制器的参数也纳入学习范畴，实现端到端的优化，以克服手动调参的局限性。</li>
<li>该框架为开发更适应、更安全的复杂接触操作机器人系统提供了坚实基础。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出了一种阻抗基元增强的分层强化学习框架，用于解决机器人连续接触任务中的长时程操作问题。核心方法结合了三个关键技术：支持可变刚度控制的动作空间、原始执行中的自适应刚度控制器，以及促进顺应性与高效探索的affordance耦合。通过在方块抓取、开门、推物和表面清洁等任务上的训练与评估，该框架在学习效率、基元组合性和任务成功率方面均优于现有方法，并验证了其仿真到现实的迁移能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.19607" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>