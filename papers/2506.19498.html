<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.19498" target="_blank" rel="noreferrer">2506.19498</a></span>
        <span>作者: Qingyao Wu Team</span>
        <span>日期: 2025-06-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉语言模型（VLM）的机器人操作方法展现出巨大潜力，其主流范式是首先提取场景的空间表示（如代表物体位置的点或方向的向量），然后利用VLM基于这些表示生成约束，最后通过求解器计算机器人动作序列。然而，现有方法通常采用<strong>固定</strong>的空间表示提取方案来处理各种任务。这导致两个关键局限性：对于复杂任务，固定的、粗粒度的表示可能<strong>能力不足</strong>；而对于简单任务，使用复杂、精细的表示又会带来<strong>过长的提取时间</strong>，效率低下。</p>
<p>本文针对VLM引导的机器人操作方法中，如何提供<strong>既充分又高效</strong>的空间表示提取支持这一具体痛点，提出了<strong>任务自适应</strong>的新视角。其核心思路是：任务复杂度决定了所需空间表示的类型和粒度，更强的表示能力通常伴随着更高的系统开销；因此，系统应能根据具体任务需求，为每个实体<strong>动态选择</strong>最合适的空间表示提取方案。</p>
<h2 id="方法详解">方法详解</h2>
<p>T-Rex的整体框架是一个任务自适应的空间表示提取与执行管道。给定自然语言指令和场景观测（RGB-D图像），系统首先通过VLM引导，为每个任务相关物体自适应地从“空间表示提取工具包”中调用最优提取器，获得空间表示；随后，“低级动作序列生成器”基于这些表示和VLM推断出的约束，生成机器人的动作序列以完成任务。</p>
<p><img src="https://arxiv.org/html/2506.19498v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：T-Rex方法整体框架。给定指令和场景观测，VLM引导系统为每个任务相关物体自适应调用工具包中的最优提取器以获取空间表示，随后基于这些表示和约束生成机器人动作序列。</p>
</blockquote>
<p>该框架的核心模块包括任务自适应的异构/多粒度空间表示提取，以及用于稳定推理的链式落地（CoG）模块。</p>
<p><strong>1. 任务自适应的异构空间表示提取</strong><br>系统维护一个<strong>可扩展的空间表示提取工具包</strong>，其中集成了多种基于大型视觉模型的提取工具（如提取关键点、6D位姿、向量的工具）。</p>
<p><img src="https://arxiv.org/html/2506.19498v1/x3.png" alt="工具包示意图"></p>
<blockquote>
<p><strong>图3</strong>：空间表示提取工具包。系统为每个任务自适应调用最优提取器以获取所需表示。在整理桌子任务中，它为积木提取点，为工具和笔筒提取向量，为毛绒玩具提取6D位姿。</p>
</blockquote>
<p>工具包中的每个工具在注册表中被定义为 $T_i = (I_i, o_i, f_i, s_i, h_i)$，分别表示所需输入、输出表示、输出格式、简要实现说明和平均执行历史时间。对于任务分解后的每个阶段 $s$ 和其中的每个相关物体 $o$，VLM根据以下准则选择最优工具 $t^{<em>}_{s,o}$：<br>$$t^{</em>}<em>{s,o}=\arg\max</em>{t\in\mathcal{R}}\Bigl[P_{\mathrm{succ}}(t \mid I,X,s,o) ;-; \lambda,h_{t}\Bigr]$$<br>其中 $P_{\mathrm{succ}}$ 是VLM估计的用该工具成功提取所需表示的概率，$h_t$ 是工具耗时，$\lambda$ 是权衡成功率与成本的超参。这实现了<strong>效率与准确性的任务自适应权衡</strong>。选定工具后，即提取空间表示 $r_{s,o} = t^{*}_{s,o}(I,X)$。</p>
<p><strong>2. 任务自适应的多粒度空间表示提取</strong><br>当任务需要对物体局部区域进行精细操作时（如调整机器狗腿的朝向），直接在全局图像上提取可能失败。因此，本文提出一种<strong>轻量级、注意力启发的局部缩放策略</strong>。</p>
<p><img src="https://arxiv.org/html/2506.19498v1/x4.png" alt="多粒度提取流程"></p>
<blockquote>
<p><strong>图4</strong>：多粒度空间表示提取流程。首先裁剪出目标物体的局部子图，然后在该子图上提取精细的空间表示。</p>
</blockquote>
<p>该策略是任务自适应的，仅在VLM判断某阶段需要更精细表示时触发。流程为：首先用分割模型（如SAM）定位目标物体区域，扩展掩码并裁剪出局部子图像；然后，将前述自适应提取方法应用于此子图，得到<strong>细粒度表示</strong>；最后，将这些表示合并回主流程以指导后续动作生成。</p>
<p><strong>3. 链式落地（Chain of Grounding, CoG）</strong><br>上述自适应方法增加了VLM的推理负担（需同时处理任务分解、约束生成、工具选择）。为解决此问题，本文提出了<strong>CoG</strong>，将VLM的指令落地过程显式分解为四个顺序依赖的阶段来引导其推理：</p>
<ol>
<li><strong>操作提示推断</strong>：分解任务为多个阶段，生成与表示无关的、简洁的关键操作提示。</li>
<li><strong>约束推断</strong>：为每个提示，以自然语言形式推断该阶段所需的空间约束。</li>
<li><strong>工具选择</strong>：查询工具包注册表，为每个阶段、每个任务相关物体选择最优提取工具，权衡表示能力与提取效率。</li>
<li><strong>约束代码生成</strong>：将自然语言约束转化为可执行的Python函数，每个函数以对应空间表示为输入，输出衡量约束满足程度的标量成本。</li>
</ol>
<p>CoG的最终输出是每个阶段的约束函数集 $F_s$ 以及为每个物体选择的工具 $T_{s,o}$。这一设计显著提升了系统推理的稳定性和可靠性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在一个真实的桌面操作平台（UR5e机械臂，RealSense D435i RGB-D相机）上进行验证。使用GPT-4.1作为VLM。设计了<strong>15个开放词汇的真实世界操作任务</strong>，涵盖从简单摆放到复杂空间约束的场景。每个任务进行10次独立试验，每次试验中相关物体的位置和朝向均随机化。</p>
<p><strong>对比方法</strong>：与当前基于VLM和空间表示的SOTA方法进行对比，包括 <strong>ReKep</strong>、<strong>OmniManip</strong> 和 <strong>GeoManip</strong>。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能与效率</strong>：T-Rex在15个任务上的<strong>平均成功率达到84.7%<strong>，显著高于ReKep (62.0%)、OmniManip (60.7%) 和 GeoManip (54.7%)。同时，T-Rex的</strong>平均每阶段规划时间仅为1.62秒</strong>，与最快的基线方法相当，证明了其高效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.19498v1/x5.png" alt="任务成功率和规划时间对比"></p>
<blockquote>
<p><strong>图5</strong>：在15个真实世界任务上的成功率和每阶段规划时间对比。T-Rex取得了最高的成功率，同时保持了与最快基线相当的规划效率。</p>
</blockquote>
<ol start="2">
<li><strong>消融实验</strong>：消融实验验证了各核心组件的贡献。<ul>
<li><strong>移除CoG</strong>：导致成功率大幅下降至46.7%，凸显了分阶段引导对于稳定推理的必要性。</li>
<li><strong>移除多粒度提取</strong>：在需要精细操作的任务上成功率下降，证明了局部缩放策略的有效性。</li>
<li><strong>使用固定提取工具（全关键点或全6D位姿）</strong>：分别导致成功率和效率的显著降低，证实了任务自适应工具选择的价值。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.19498v1/x6.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图6</strong>：消融实验结果。移除CoG导致性能急剧下降；移除多粒度提取在需要精细操作的任务上性能受损；固定工具策略在成功率或效率上表现不佳。</p>
</blockquote>
<ol start="3">
<li><strong>定性结果与失败分析</strong>：<ul>
<li><strong>定性对比</strong>：T-Rex能够成功处理需要复杂方向约束的任务（如“将毛绒玩具直立并面朝相机”），而基线方法常因表示能力不足而失败。</li>
<li><strong>失败案例</strong>：主要失败模式包括<strong>VLM推理错误</strong>（如约束错误）、<strong>感知错误</strong>（如分割或位姿估计不准）和<strong>执行错误</strong>（如运动规划或抓取失败）。其中VLM推理错误占比最高（<del>47%），其次是感知错误（</del>33%）。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.19498v1/x7.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图7</strong>：定性结果对比。在需要复杂方向约束的任务中，T-Rex成功，而基线方法（ReKep）失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.19498v1/x8.png" alt="失败案例分析"></p>
<blockquote>
<p><strong>图8</strong>：失败案例分布。VLM推理错误是最主要的失败原因。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.19498v1/x9.png" alt="任务成功率和时间随任务复杂度变化"></p>
<blockquote>
<p><strong>图9</strong>：任务成功率和时间随任务复杂度（所需空间约束数量）的变化。T-Rex在高复杂度任务上优势更明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.19498v1/x10.png" alt="工具选择与任务复杂度关系"></p>
<blockquote>
<p><strong>图10</strong>：VLM为不同复杂度任务选择的工具类型分布。随着任务复杂度增加，系统更倾向于选择6D位姿等更强但更耗时的表示。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个<strong>任务自适应的空间表示提取方法</strong>，能够根据任务需求动态选择异构、多粒度的空间表示，在保证强大空间理解能力的同时维持了高效率。</li>
<li>设计了一个<strong>可扩展的空间表示提取工具包框架</strong>，支持用户轻松集成新的提取工具。</li>
<li>提出了<strong>链式落地（CoG）方法</strong>，通过分阶段引导显式提升了VLM在复杂机器人任务中推理的稳定性和可靠性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，系统性能仍受限于<strong>VLM的推理能力</strong>（主要错误来源）和<strong>底层视觉基础模型的感知精度</strong>。此外，工具包需要<strong>手动维护和注册</strong>。</p>
<p><strong>启示</strong>：本研究指明了未来VLM机器人系统的一个重要方向：<strong>“自适应”</strong>。不仅空间表示提取可以自适应，其他模块（如规划粒度、控制频率）也可能需要根据任务动态调整。如何设计更鲁棒、更通用的自适应机制，以及如何进一步降低对VLM推理错误的敏感性，是值得探索的后续研究方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文针对基于视觉语言模型（VLM）的机器人操作中，固定空间表示提取方案灵活性差、效率低的问题，提出了T-Rex任务自适应框架。其核心技术是**链式推理（CoG）**，引导VLM逐步推断任务所需的空间约束及最优提取方案，并配合**可扩展的空间表示提取工具包**动态调用模型，仅在必要时进行细粒度提取。实验表明，该方法无需额外训练，即可在**空间理解、效率和稳定性**方面取得显著优势。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.19498" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>