<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.14004" target="_blank" rel="noreferrer">2511.14004</a></span>
        <span>作者: Chen, Taijing, Kumar, Sateesh, Xu, Junhong, Pavlakos, Georgios, Biswas, Joydeep, Martín-Martín, Roberto</span>
        <span>日期: 2025/11/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>服务机器人在动态、开放世界中检索物体时，用户指令可能涉及外观属性（“红色马克杯”）、空间上下文（“桌上的马克杯”）或过去状态（“昨天在这的马克杯”）。现有方法仅能解决部分问题：场景图捕捉空间关系但忽略时间锚定；时序推理方法建模动态但不支持具身交互；动态场景图虽兼顾两者，但仍是封闭世界，具有固定词汇表。本文针对开放世界物体检索这一具体痛点，提出将记忆查询（时间搜索）和物理动作（空间搜索）统一视为主动交互决策循环内的元素这一新视角。核心思路是：利用非参数化长期记忆和工作记忆支持高效回忆，并借助视觉语言模型在每一步选择时间或空间动作，从而将时空搜索作为一个统一问题来解决。</p>
<h2 id="方法详解">方法详解</h2>
<p>STAR框架旨在通过一个统一的决策循环，整合对长期记忆的查询（时间搜索）和对当前环境的交互（空间搜索），以完成开放词汇描述的物体检索任务。</p>
<p><img src="https://arxiv.org/html/2511.14004v3/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：STAR系统概览。左侧为机器人在多日巡逻中构建非参数化长期记忆；右侧为任务执行时，智能体基于工作记忆，从统一动作空间中选择时间动作（回忆过去观测）或空间动作（导航、感知、操纵），循环更新工作记忆直至成功检索目标物体。</p>
</blockquote>
<p>整体流程分为两个阶段：1）<strong>离线记忆构建</strong>：任务开始前，机器人通过巡逻环境积累历史观测 <code>S_≤T</code>，并构建长期记忆 <code>M</code>。2）<strong>在线主动检索</strong>：收到指令 <code>ℓ</code> 后，初始化工作记忆 <code>H</code>，LLM策略 <code>π</code> 基于 <code>H</code> 和剩余步数预算 <code>R_k</code>，从统一动作空间中选择动作 <code>a_k</code> 执行，并将结果 <code>y_k</code> 更新至 <code>H</code>，循环直至找到目标或预算耗尽。</p>
<p>核心模块包括长期记忆、工作记忆和基于LLM的动作选择策略：</p>
<ol>
<li><strong>非参数化长期记忆</strong>：长期记忆 <code>M</code> 以非参数形式存储在向量数据库中。每个时间步 <code>t</code> 的记录为 <code>m_t = (t, x_t, embed(o_t), o_t)</code>，包含时间戳、机器人位姿、观测的文本描述嵌入以及原始视觉观测 <code>o_t</code>。数据库维护多个向量索引以实现多模态查询：<strong>语义索引</strong>（通过文本嵌入支持开放词汇查询）、<strong>时间索引</strong>（支持按时间查询）、<strong>空间索引</strong>（支持按位置查询）。保留原始观测 <code>o_t</code> 是关键设计，当嵌入描述丢失细节时，策略可重新访问原始图像获取信息。</li>
<li><strong>工作记忆更新</strong>：工作记忆 <code>H_k</code> 在任务开始时初始化，存储动作-结果轨迹 <code>(a_1:k-1, y_1:k-1)</code>，是策略决策的依据。它通过执行<strong>时空动作</strong>来更新。统一动作空间 <code>A</code> 包含一系列工具 <code>ℱ</code>，每个工具 <code>f</code> 有对应的参数空间 <code>Θ</code>。动作 <code>a_k = (f, θ)_k</code>。<ul>
<li><strong>时间动作（搜索时间）</strong>：工具为记忆查询函数（如<code>SemanticQuery</code>）。参数 <code>θ</code> 由LLM生成（如查询文本）。执行时，<code>θ</code> 被转换为向量，与长期记忆 <code>M</code> 中对应索引进行相似度匹配，返回top-r条相关记忆记录 <code>y_k</code>。</li>
<li><strong>空间动作（搜索空间）</strong>：工具为机器人技能（如导航、抓取、调用检测模块）。参数 <code>θ</code> 为技能参数（如目标位姿）。执行后，获得来自当前环境的感知反馈 <code>y_k</code>（如原始观测、检测结果、成功信号）。<br>执行任何动作后，工作记忆按 <code>H_k+1 = H_k ⊕ (a_k, y_k)</code> 更新。</li>
</ul>
</li>
<li><strong>基于工作记忆的动作选择</strong>：LLM策略 <code>π(a | ℓ, H_k, R_k)</code> 根据当前工作记忆 <code>H_k</code> 和剩余步数预算 <code>R_k</code>，选择要执行的动作工具 <code>f</code> 及其参数 <code>θ</code>。预算 <code>R_k</code> 的显式输入使策略能在预算紧张时倾向于探索性更低的动作。决策循环持续直至找到目标或 <code>R_k=0</code>。</li>
</ol>
<p>与现有方法相比，STAR的核心创新点在于：<strong>将记忆查询本身作为可执行的动作，纳入统一的动作空间</strong>。这使得智能体能够在同一个决策循环中，自主、交替地决定是“回忆过去”还是“探索现在”，实现了时间搜索与空间搜索的真正统一，而非将记忆作为独立的前置模块或仅关注当前状态。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用论文提出的 <strong>STARBench</strong> 基准进行评估，该基准基于VirtualHome和Unity3D模拟器构建，包含动态变化的家庭环境。STARBench涵盖 <strong>可见物体搜索</strong>、<strong>交互式物体搜索</strong>（需打开容器等操作）和<strong>常识物体搜索</strong>（目标可能从未被观测过）三种任务类型，共 <strong>360</strong> 个任务。前两种类型进一步细分为基于类别、属性、空间、时空、空间频率五种指令家族。</p>
<p>实验平台包括模拟器（STARBench）和真实世界（Tiago机器人在模拟公寓中）。评估指标为<strong>执行成功率</strong>（在步数预算K=20内成功检索到正确物体）。对比的基线方法包括：<strong>Random</strong>（随机导航）、<strong>SG+S</strong>（使用随时间变化的真实场景图进行一次性规划并执行）、<strong>TR+S</strong>（使用时序检索进行一次性规划并执行）以及完整的<strong>STAR</strong>方法。实验还区分了<strong>Oracle</strong>（记忆构建使用真实物体标签）和<strong>Realistic</strong>（记忆构建完全依赖感知模型预测）两种环境知识模式。</p>
<p><img src="https://arxiv.org/html/2511.14004v3/x5.png" alt="实验结果1"></p>
<blockquote>
<p><strong>图5</strong>：在STARBench可见物体搜索任务上的成功率（每类45个任务）。STAR在Oracle和Realistic设置下，在所有五种任务类型上均显著优于SG+S和TR+S基线，尤其在涉及属性、时空和空间频率推理的复杂任务上优势明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.14004v3/x6.png" alt="实验结果2"></p>
<blockquote>
<p><strong>图6</strong>：左：在交互式物体搜索任务上的成功率。STAR同样全面领先。右：消融实验，比较STAR与变体STAR (One-Shot)（即TR+S）和仅使用空间搜索的STAR (Spatial Only)。结果显示，主动的、交替的时空搜索策略（完整STAR）对于成功检索至关重要。</p>
</blockquote>
<p>关键实验结果总结：</p>
<ol>
<li><strong>全面性能领先</strong>：在STARBench的所有任务类型和指令家族中，STAR在Oracle和Realistic设置下的成功率均 consistently 高于所有基线方法（图5，图6左）。例如，在可见物体搜索的“时空”任务家族中，STAR（Realistic）成功率超过60%，而TR+S和SG+S均低于40%。</li>
<li><strong>复杂推理优势</strong>：对于需要结合时空推理（如“昨天在桌上的书”）或频率推理（如“通常在水槽边的杯子”）的任务，STAR相比仅依赖场景图（SG+S）或仅做一次性记忆查询（TR+S）的方法，表现出更显著的优势。这验证了统一时空搜索循环的有效性。</li>
<li><strong>消融实验验证</strong>：消融实验（图6右）表明，将STAR简化为一次性记忆查询后规划（STAR (One-Shot)）或仅进行空间搜索（STAR (Spatial Only)），性能均大幅下降。这证明了<strong>主动交替执行时空动作</strong>这一核心设计的重要性。</li>
<li><strong>真实世界迁移</strong>：在Tiago机器人上的9项任务测试表明，STAR能够成功迁移到真实环境，成功引导用户找到目标物体位置。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了 <strong>STAR框架</strong>，首次将时间搜索（记忆检索）和空间搜索（具身动作）统一到一个主动决策循环中，以解决开放世界的物体检索问题；2）引入了 <strong>STARBench基准</strong>，填补了动态环境中时空交互式物体搜索评估的空白；3）通过系统的模拟与真实机器人实验，验证了STAR相对于场景图和纯时序检索基线的优越性，特别是在需要复杂时空推理的任务上。</p>
<p>论文提到的局限性包括：对LLM（GPT-4o）和视觉感知模型的依赖；记忆检索和LLM推理的计算成本；以及实验环境规模相对有限。</p>
<p>本研究对后续工作的启示在于：<strong>统一时空搜索的范式</strong>为具身AI处理动态开放世界问题提供了新思路；<strong>非参数化长期记忆与轻量级工作记忆结合</strong>的设计平衡了存储效率与决策灵活性；所提出的<strong>STARBench基准</strong>可推动该领域向更复杂、更贴近现实的长周期动态推理任务发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对服务机器人在开放世界中检索对象的核心问题，即对象请求可能涉及属性、空间或时间上下文，现有方法仅关注单一方面。作者提出STAR框架，通过统一内存-动作循环，结合非参数长期记忆和工作内存支持高效回忆，并利用视觉语言模型在每一步动态选择时间或空间动作。实验基于STARBench基准，在模拟和真实环境中验证，STAR consistently outperforms scene-graph and memory-only baselines，证明了统一处理时空搜索的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.14004" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>