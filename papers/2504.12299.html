<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Adapting a World Model for Trajectory Following in a 3D Game - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Adapting a World Model for Trajectory Following in a 3D Game</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.12299" target="_blank" rel="noreferrer">2504.12299</a></span>
        <span>作者: Tot, Marko, Ishida, Shu, Lemkhenter, Abdelhak, Bignell, David, Choudhury, Pallavi, Lovett, Chris, França, Luis, de Mendonça, Matheus Ribeiro Furtado, Gupta, Tarun, Gehring, Darren, Devlin, Sam, Macua, Sergio Valcarcel, Georgescu, Raluca</span>
        <span>日期: 2025/04/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习是利用专家知识训练智能体的强大工具，而能够复现给定轨迹是其核心组成部分。在复杂环境（如现代3D视频游戏）中，分布偏移和随机性要求比简单的动作重放更鲁棒的方法。目前，行为克隆等主流方法在随机性环境中容易因状态转移的微小变化而导致轨迹显著偏离。本文针对在复杂、随机3D游戏中准确跟随给定轨迹的痛点，提出了一种新视角：通过适应预训练的世界模型来构建逆动力学模型，并探索不同的未来对齐策略以应对智能体不完美和环境随机性带来的分布偏移。本文的核心思路是：扩展逆动力学模型，使其能够基于过去和未来的轨迹序列进行条件化，并系统评估不同编码器、策略头以及未来选择策略在多种数据设置下的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的核心是广义化的IDM-K模型。传统的逆动力学模型编码当前观测和下一观测以预测采取的动作。IDM-K将此方法推广，将未来条件化向前移动K步。本文进一步扩展IDM-K的概念，使其基于包含观测和动作的过去与未来轨迹序列进行条件化，以提高时间一致性和长期依赖性。</p>
<p><img src="https://arxiv.org/html/2504.12299v1/extracted/6367068/figures/models/idm_model.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：IDM模型的高级概述。模型编码两条不同的轨迹：智能体的当前轨迹和未来条件化轨迹。得到的编码被传递到IDM头，以选择应执行的动作。</p>
</blockquote>
<p>如图1所示，该模型处理截止到时间步t的过去轨迹序列，以及从时间步t’+k开始的未来轨迹序列。给定这些输入，模型预测在t时刻采取的动作。观测和动作序列可以独立或联合编码。编码后的表征随后通过一个策略头来预测动作。</p>
<p><strong>核心模块1：视觉编码器</strong>。论文评估了三种不同的编码器。</p>
<p><img src="https://arxiv.org/html/2504.12299v1/extracted/6367068/figures/models/encoder_models.png" alt="编码器类型"></p>
<blockquote>
<p><strong>图2</strong>：评估的三种不同编码器。从头训练的ConvNeXt编码器、通用预训练编码器DINOv2，以及游戏特定的预训练世界与人类动作模型。</p>
</blockquote>
<ol>
<li><strong>ConvNeXt</strong>：一个从头开始训练的卷积网络，与归一化的动作向量结合。</li>
<li><strong>DINOv2</strong>：一个预训练的通用图像编码器，带有可微调的MLP头，并与归一化的动作向量结合。</li>
<li><strong>世界与人类动作模型</strong>：一个预训练的游戏特定编码器，专为自回归下一令牌预测优化，将交错的图像和动作令牌序列作为上下文处理。由于其令牌化表示，WHAM的原始序列长度远大于实际轨迹长度。为解决此问题，论文将序列嵌入投影为紧凑表示（一个用于观测输入，另一个用于动作输入），然后再通过二次投影层获得最终嵌入。</li>
</ol>
<p><strong>核心模块2：解码器（策略头）</strong>。由于模型处理序列数据，本文采用基于Transformer的解码器进行动作预测，具体使用GPT模型作为IDM头来预测为到达未来轨迹所采取的动作。GPT头可以预测对应于过去信息被截断的不同时间步的多个动作，这些额外的序列动作输出损失有助于使IDM策略对变化的过去轨迹输入长度更鲁棒。本文还将GPT头与多层感知机头进行比较，MLP头将输入编码扁平化并拼接，提供了比基于Transformer的方法更简单的替代方案。将两种解码器与三种编码器组合，产生了六种待评估的IDM-K变体。</p>
<p><strong>创新点：未来选择策略</strong>。为IDM-K选择合适的未来条件化策略至关重要，主要挑战是智能体不完美和环境随机性。为缓解这些问题，论文探索了四种策略：</p>
<ol>
<li><strong>静态未来条件化</strong>：未来起始时间步仅由当前时间步决定，不考虑空间偏差。</li>
<li><strong>最近未来条件化</strong>：基于智能体当前位置选择参考轨迹中最近的点，最小化空间偏差。</li>
<li><strong>半径未来条件化</strong>：如果智能体在预定义半径r内，则未来时间步前进一步，否则保持不变。</li>
<li><strong>内外半径未来条件化</strong>：引入内外两个半径阈值，根据智能体与当前未来条件化帧位置的距离动态更新未来时间步。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在第三人称多人在线游戏《Bleeding Edge》中进行，使用了SkyGarden和教程地图Dojo（图3）。数据集包含71,940条人类游戏轨迹。动作空间被归一化并离散化为11个区间。评估在8条保留轨迹上进行，涵盖跳板选择、复杂基准路径和教程导航等任务。使用两个指标：基于动态时间规整的轨迹相似性曲线下面积和首次显著偏离点比率。</p>
<p><img src="https://arxiv.org/html/2504.12299v1/extracted/6367068/figures/maps/sky_garden.png" alt="实验地图"></p>
<blockquote>
<p><strong>图3a</strong>：用于训练和评估的Sky Garden地图示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.12299v1/extracted/6367068/figures/maps/dojo_map.png" alt="实验地图"></p>
<blockquote>
<p><strong>图3b</strong>：用于训练和评估的Dojo地图示例。</p>
</blockquote>
<p><strong>对比方法</strong>：论文比较了六种模型变体（ConvNeXt/DINOv2/WHAM 编码器 × MLP/GPT 策略头）在三种实验设置下的性能：1) <strong>General</strong>：在大量多样化游戏轨迹上训练，在未见轨迹上零样本评估；2) <strong>Specific</strong>：在小规模相似行为轨迹上训练和评估；3) <strong>Fine-tuned</strong>：先用General设置预训练，再用Specific设置微调和评估。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>General Setting</strong>：如表1所示，ConvNeXt编码器表现最佳，无论搭配MLP还是GPT头。ConvNeXt-GPT平均AUC最高（0.86）。定性结果（图4）显示，ConvNeXt-GPT能紧密跟随参考轨迹，DINOv2-GPT偶尔成功但有偏差，WHAM-GPT则基本失败。这表明为轨迹跟随任务专门训练编码器更具优势。</li>
<li><strong>消融实验</strong>：输入模态消融（表2）表明，视觉输入对准确轨迹跟随至关重要，而加入动作输入仅带来边际收益。序列长度消融（表3）表明，提供完整的10帧过去和未来序列效果略好于短序列，而缺乏未来条件化的行为克隆智能体表现很差。未来选择策略比较（表4）显示，在评估集上“最近”策略略优（AUC 0.877），但论文因“半径”策略在排除Dojo轨迹后表现稍好，且能避免“最近”策略在循环轨迹中的理论问题，而选择了“半径”策略。</li>
<li><strong>Specific and Fine-tuning Settings</strong>：在特定行为（Dojo Ramp）的小数据集上训练时（表5），DINOv2-GPT和DINOv2-MLP表现最佳（AUC分别达0.97和0.96，FI达1.00），几乎完美复现轨迹。这表明在低数据情况下，通用预训练编码器更具优势。在微调设置下，ConvNeXt-MLP、ConvNeXt-GPT和DINOv2-GPT都能成功跟随轨迹（AUC达0.93-0.96，FI达0.92-1.00），显示ConvNeXt架构能从大规模General预训练中受益并最终略微超越DINOv2模型。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.12299v1/extracted/6367068/figures/trajectories/conv-next-gpt/cgptb1-0.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图4a</strong>：ConvNeXt-GPT在Benchmark 1轨迹上的采样 rollout。红线为参考轨迹，蓝线为智能体路径。ConvNeXt紧密跟随预期轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.12299v1/extracted/6367068/figures/trajectories/dinov2-gpt/dgptb1-0.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图4b</strong>：DINOv2-GPT在Benchmark 1轨迹上的采样 rollout。DINOv2表现出偶尔成功的运行，能从微小偏差中恢复。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.12299v1/extracted/6367068/figures/trajectories/wham-gpt/wgpt1-0.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图4c</strong>：WHAM-GPT在Benchmark 1轨迹上的采样 rollout。WHAM未能跟随轨迹，显示出大多是任意的移动。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>将预训练的世界模型适应于下游轨迹跟随任务的模仿学习中。</li>
<li>在复杂的3D视频游戏中，对不同的模型配置在General、Specific和Fine-tuned三种设置下进行了全面的实证分析，明确了不同架构的优势场景。</li>
<li>深入探讨了模型输入（单观测vs序列、是否包含动作）的设计选择影响，并探索了多种未来条件化策略以减轻分布偏移。</li>
</ol>
<p><strong>局限性</strong>：论文自身指出，在General设置下，没有一个智能体能够成功跟随最复杂的轨迹，这表明在不同行为间的泛化能力仍存在显著差距。评估主要集中于移动轨迹和单一角色，未测试战斗交互等更复杂的行为。</p>
<p><strong>对后续研究的启示</strong>：本文表明，在模仿学习中进行轨迹跟随时，不存在“一刀切”的最佳架构，编码器和策略头的选择高度依赖于数据可用性和任务特异性。未来对齐策略对于处理随机环境和智能体误差至关重要。后续研究可进一步测试模型在人为扰动下的鲁棒性，并将评估扩展到包含战斗、技能使用等更丰富的游戏行为中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何在复杂3D游戏（Bleeding Edge）中实现精确的轨迹跟随，以解决模仿学习因分布偏移和随机性导致的简单动作重放失效问题。核心方法是采用逆动力学模型，结合多种编码器（预训练世界模型、DINOv2、从头训练的ConvNeXt）与策略头（GPT风格自回归Transformer、MLP风格前馈网络），并探索未来对齐策略以缓解不确定性带来的偏差。实验表明，在多样数据设置下，GPT策略头搭配从头训练编码器效果最佳；低数据时，DINOv2编码器配GPT策略头最优；而在预训练后微调特定行为时，两种策略头性能相当。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.12299" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>