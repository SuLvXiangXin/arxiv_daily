<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.04681" target="_blank" rel="noreferrer">2508.04681</a></span>
        <span>作者: Xu, Liang, Yang, Chengqun, Lin, Zili, Xu, Fei, Liu, Yifan, Xu, Congsheng, Zhang, Yiyi, Qin, Jie, Sheng, Xingdong, Liu, Yunhui, Jin, Xin, Yan, Yichao, Zeng, Wenjun, Yang, Xiaokang</span>
        <span>日期: 2025/08/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>构建能够协助人类完成物理世界任务的通用智能体，需要从以人为中心的交互数据中学习通用的交互知识。现有数据集在类别和模态上存在局限：一方面，大多数数据集（如You2Me、ExPI、Hi4D）专注于单一类型的“专家级”交互（如纯人-人交互或人-物交互），缺乏包含多样化交互（人-人、人-物、人-场景）的通用场景；另一方面，现有数据集（如InterHuman、HIMO、HOH）普遍忽略了AI助手总是基于其第一人称视角进行感知和反应这一事实，缺少第一人称（egocentric）数据，这可能阻碍AI助手在实际物理环境中的部署。</p>
<p>本文针对“缺乏兼具通用交互类别和第一人称视角的大规模交互数据集”这一具体痛点，提出了一个以第一人称视角为核心的人-物-人交互新视角。核心思路是：构建一个在视觉-语言-动作（Vision-Language-Action， VLA）框架下的大规模数据集InterVLA，其中助手基于第一人称视觉和语言指令为指令员提供服务，并为此设立新的基准测试任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的核心工作是创建InterVLA数据集并定义相关基准任务。整体框架遵循视觉-语言-动作范式：输入是场景布置和由大语言模型生成的指令脚本；过程中，指令员发出语言指令，助手通过第一人称视觉感知环境并理解意图，最终输出是助手执行的一系列连贯动作（表现为人体和物体的运动）。数据收集的pipeline包括：1）场景与物体随机布置；2）基于ChatGPT生成包含连续原子指令的脚本；3）参与者（指令员-助手对）按照脚本进行交互；4）同步采集多模态数据。</p>
<p><img src="https://arxiv.org/html/2508.04681v1/figures/teaser.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：InterVLA数据集整体框架。描绘了一个助手（右侧）根据指令员（左侧）的语言命令（如“把桌上的杯子递给我”），基于第一人称视角提供服务的日常场景。该数据集包含多视角视频、语言指令以及高精度的人体与物体运动数据。</p>
</blockquote>
<p>核心模块对应于数据集的三种模态：</p>
<ol>
<li><strong>视觉模块</strong>：采集两种视角的视频。第一人称视角：使用两台GoPro相机，分别紧密固定在助手的额头和胸部，以捕捉高分辨率（5312×2988）的第一人称RGB视频。第三人称视角：使用五台经过校准的RGB相机覆盖整个场景，提供外部的多视角观察。</li>
<li><strong>语言模块</strong>：指令是交互的起点和桥梁。利用ChatGPT，基于家庭物品和家具库，自动选择场景和物体布置，并生成包含8个连续原子指令的交互脚本。脚本涵盖人-物-人交互（如传递、协作重新摆放）、纯人-人交互（如搀扶）以及多物体操作等多样化类型。</li>
<li><strong>动作模块</strong>：为了获取高质量的人体与物体运动数据，建立了一个光学动作捕捉系统。人体运动：摒弃紧身动捕服，直接将反光标记点用强力医用胶粘在皮肤或衣物表面，以保持RGB模态的保真度并确保跟踪精度。物体运动：将物体视为刚体，在其表面粘贴至少四个反光标记点进行光学跟踪。此外，还使用KSCAN Magic Scanner对所有50个物体进行了精确的3D扫描，获取其几何网格。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.04681v1/figures/setup.jpg" alt="捕获系统"></p>
<blockquote>
<p><strong>图2</strong>：InterVLA数据捕获系统。(a) 在受试者身体上粘贴41个反光标记点，助手佩戴两个GoPro相机。(b) 在真实物体表面放置反光标记点以跟踪轨迹，并使用3D扫描仪获取物体网格。(c) 混合RGB-动捕系统，包括两台第一人称相机、五台第三人称相机和OptiTrack动捕系统。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.04681v1/figures/dataset_sample.png" alt="数据组成"></p>
<blockquote>
<p><strong>图3</strong>：InterVLA数据集的组成部分。包括(a)两个第一人称视频，(b)五个第三人称视频，(c)GPT生成的指令，以及(d)交互过程中的高精度人体与物体运动。</p>
</blockquote>
<p>与现有方法相比，InterVLA的创新点具体体现在：1) <strong>交互类别综合化</strong>：首次大规模集成了人-人、人-物、人-场景及多物体交互。2) <strong>第一人称视角为核心</strong>：强调并系统采集了助手的第一人称视觉数据，贴合实际AI助手部署需求。3) <strong>数据精度高</strong>：通过光学动捕系统提供高精度的人体（SMPL参数）和物体（6D位姿）运动真值，而非从图像估计的噪声数据。4) <strong>使用真实物体</strong>：采用日常真实物体而非3D打印模型，保证了视觉外观的真实性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文使用自行构建的InterVLA数据集作为实验平台，并在此基础上定义了四个下游任务进行基准测试。</p>
<p><img src="https://arxiv.org/html/2508.04681v1/figures/task_formulation.jpg" alt="任务框架"></p>
<blockquote>
<p><strong>图4</strong>：基于InterVLA定义的下游任务框架。包括：(A) 第一人称人体运动估计：从第一人称视频估计指令员的全局运动。(B) 交互合成：给定文本描述和初始状态，生成合理的人-物-人运动序列。(C) 基于运动的交互预测：基于历史运动帧预测未来运动。(D) 基于视觉-语言的交互预测：基于历史第一人称视频和语言指令预测未来运动。</p>
</blockquote>
<p><strong>任务一：第一人称人体运动估计</strong></p>
<ul>
<li><strong>基准方法</strong>：对比了四种先进的全局人体运动估计方法：TRACE、GLAMR、TRAM和WHAM。</li>
<li><strong>关键结果</strong>：定量结果（表2）显示，WHAM在多数指标（MPJPE: 333.6, PVE: 359.7, Accel: 8.7）上表现最佳，但与真实值仍有显著差距。定性结果（图5）揭示了现有方法在应对InterVLA数据集挑战时的失败案例，例如因助手快速转头导致目标离开视野时，GLAMR和WHAM无法保持正确的全局坐标系（红色虚线框），TRACE则丢失了许多帧（绿色虚线框）。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.04681v1/figures/ego_hmr.jpg" alt="运动估计可视化"></p>
<blockquote>
<p><strong>图5</strong>：第一人称人体运动估计在InterVLA上的可视化结果对比。展示了原始RGB帧、动捕系统获取的真值以及四个基线模型的结果。不透明度越高表示序列中越靠后的帧。结果表明，在快速相机运动、遮挡等挑战下，现有方法表现不佳。</p>
</blockquote>
<p><strong>任务二：交互合成</strong></p>
<ul>
<li><strong>基准方法</strong>：调整了文本驱动的人体运动生成方法MDM、priorMDM以及人-物交互生成方法HIMO，使其支持双人多物体的条件输入。</li>
<li><strong>关键结果</strong>：定量结果（表3）显示，专门为交互设计的HIMO方法在R Precision (0.5707)、FID (0.6805) 和MM Dist (4.9609) 等关键指标上均优于其他方法，表明其生成的运动与文本描述更匹配、质量更高。然而，所有生成方法的结果与真实数据（Real）仍有明显差距，凸显了在包含多人和多物体的复杂交互场景中进行合成的难度。</li>
</ul>
<p>本文通过设立这些任务并展示基线方法的局限性，间接完成了对数据集价值的“消融实验”，证明了其包含的第一人称视角、多物体、连续交互等特性带来了现有方法尚未解决的新挑战。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有两点：1) <strong>数据集贡献</strong>：创建了首个大规模、多模态、以第一人称视角为核心的人-物-人交互数据集InterVLA，它综合了多样化的通用交互类别，并提供了高精度的运动真值。2) <strong>基准贡献</strong>：围绕InterVLA定义并初步建立了多个新颖的下游任务基准（如第一人称运动估计、交互合成与预测），通过全面的基线实验与分析，揭示了该领域当前面临的挑战，为后续研究提供了明确的评估平台。</p>
<p>论文自身提到的局限性包括：场景相对简单（随机布置的家具），可能无法涵盖所有复杂的现实环境；尽管有47名参与者，但人口结构的多样性可能仍有局限。</p>
<p>本研究对后续工作的启示在于：1) <strong>推动第一人称交互理解</strong>：强调了第一人称视觉在具身智能中的核心地位，未来研究需要开发能更好处理第一人称视频动态性、遮挡和有限视野的算法。2) <strong>促进通用交互学习</strong>：提供了一个包含连贯、连续交互序列的数据集，鼓励研究者开发能够理解和生成长时序、多智能体（人、物）交互的模型。3) <strong>连接VLA与真实物理世界</strong>：InterVLA提供的真实世界RGB数据、语言指令和精确动作，有望加速视觉-语言-动作模型从模拟环境向真实物理应用的迁移。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有数据集缺乏通用交互知识和以自我为中心模态的问题，提出了InterVLA数据集和基准，用于构建智能助手。方法上，采用视觉-语言-行动框架，结合混合RGB-MoCap系统，基于GPT生成脚本收集以自我为中心的人-物-人交互数据。数据集规模达11.4小时、120万帧，包含多模态数据（如以自我为中心/外部为中心视频、语言命令和精确运动）。同时，建立了以自我为中心的人体运动估计、交互合成和交互预测的基准，为物理世界AI代理的发展提供基础。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.04681" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>