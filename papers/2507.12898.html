<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Generalist Bimanual Manipulation via Foundation Video Diffusion Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Generalist Bimanual Manipulation via Foundation Video Diffusion Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.12898" target="_blank" rel="noreferrer">2507.12898</a></span>
        <span>作者: Jun Zhu Team</span>
        <span>日期: 2025-07-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于大规模多模态预训练的视觉-语言-动作（VLA）模型在机器人操作上取得了进展，但将其扩展到通用或双手操作场景仍面临挑战。核心障碍在于控制复杂性（动作空间随关节数组合增长，且需要精确的时序协调、接触动力学和长时程推理）以及数据稀缺性。机器人演示数据通常仅有数十到数百小时，远少于互联网规模的视频数据。收集演示数据不仅费力、昂贵，而且与硬件紧密耦合。因此，一个关键问题是：一个新的机器人个体（embodiment）如何利用有限的领域特定数据实现精确、可泛化的控制？</p>
<p>本文针对上述痛点，提出了利用视频扩散模型作为可迁移先验的新视角。视频不仅数据丰富，而且天生适合捕捉交互所需的动态和线索（如可操作性、接触、运动连续性）。本文提出 Vidar，其核心思路是：<strong>利用在互联网规模数据上预训练的视频扩散模型作为泛化的交互先验，然后通过一个轻量级的掩码逆动力学模型（MIDM）作为适配器，仅需极少的目标平台演示数据即可实现对新机器人的高效对齐与精确控制。</strong></p>
<h2 id="方法详解">方法详解</h2>
<p>Vidar 的整体目标是将学习策略分解为通过视频空间的映射：π = I ◦ G。其中，G 是一个视频生成模型，根据任务和观测生成时序一致、物理可信的轨迹；I 是一个逆动力学模型，将短视频片段映射到机器人特定的控制指令。这种两阶段设计将大部分表示学习的负担转移到了可利用海量视频预训练的 G 上，而仅需用少量演示训练轻量的 I。</p>
<p><img src="https://..." alt="方法整体流程"></p>
<blockquote>
<p><strong>图1</strong>：Vidar 的整体流程。左侧展示了三阶段训练流程：利用互联网视频进行通用预训练，利用约750K个跨平台多视角机器人轨迹进行具身域预训练，最后仅用约20分钟的目标平台演示进行微调。右侧展示了推理阶段：通过统一观察空间编码观测和指令，输入具身视频基础模型生成多个候选视频，经测试时缩放（TTS）选择最佳视频，再由掩码逆动力学模型（MIDM）预测动作相关掩码并回归出最终动作。</p>
</blockquote>
<p><strong>1. 视频生成模型 (G)</strong><br>本文采用整流流模型来生成高质量视频。模型参数化一个流函数 v，通过一个常微分方程从噪声视频帧演化到目标帧。训练目标是使 v 逼近从 x0 到 x1 的恒定流，损失函数为流匹配损失。</p>
<p><strong>核心创新：统一观察空间</strong><br>为了弥合不同机器人平台在视角和形态上的差异，本文设计了一个<strong>统一观察空间</strong>用于异构具身数据。该空间不包含动作，使视频扩散模型仅学习世界演化规律，从而能高效泛化到不同形态的机器人。具体而言，一个观察 o 被构造为所有可用相机视图图像经过空间调整函数 φ_rk 处理后的聚合（拼接）。同时，语言指令 l 由机器人平台、相机配置和任务相关的指令拼接而成。这种设计为视频预测提供了丰富的上下文，并在异构个体间实现了统一的表示。</p>
<p><strong>训练流程</strong>：首先，在大约750K个来自三个真实机器人平台的多视角双手操作轨迹上，将数据对齐到统一观察空间进行<strong>具身预训练</strong>。然后，在目标平台上收集的少量人类演示（约20分钟）上进行<strong>监督微调</strong>，并通过从随机起点裁剪可变长度视频进行数据增强。在推理时，采用<strong>测试时缩放</strong>策略：基于观测前缀生成 K 个候选视频轨迹，然后使用预训练的评估器（如 CLIP 或 VLM）进行评分和重排序，选择最佳轨迹，以提升生成视频的物理合理性和任务相关性。</p>
<p><strong>2. 掩码逆动力学模型 (I)</strong><br>逆动力学模型常因背景噪声和视觉干扰而泛化能力差。本文提出<strong>掩码逆动力学模型</strong>，通过隐式掩码预测学习关注动作相关区域，无需像素级监督。该模型包含一个掩码预测网络 U 和一个动作回归网络 R。具体流程为：给定输入帧 x，预测一个空间掩码 m，然后将 m 二值化后与 x 逐元素相乘，得到掩码后的帧，再输入回归网络 R 预测动作 â。训练损失包含动作预测的 Huber 损失和鼓励掩码稀疏性的 L1 正则项。通过直通估计器进行训练，这个轻量模块仅需少量演示就能学习聚焦于手、工具、接触区域等关键部分，抑制背景干扰，从而实现鲁棒的动作解码和跨域泛化。</p>
<p><strong>创新点总结</strong>：与现有方法相比，Vidar 的主要创新在于：1) <strong>解耦框架</strong>：将可迁移的视频先验学习与平台特定的动作解码分离，实现了“一个先验，多个个体”的范式。2) <strong>统一观察空间</strong>：有效对齐多视角、多平台数据，提升了先验的泛化能力。3) <strong>掩码逆动力学模型</strong>：以自监督方式学习动作注意力，增强了动作预测在复杂背景下的鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：模拟实验使用 RoboTwin 2.0 基准（多任务设置）。真实世界实验在 Aloha 机器人平台上进行，包含81个任务。</li>
<li><strong>数据</strong>：预训练使用约750K个来自Agibot-World、RoboMind和RDT的多视角轨迹。目标域微调仅使用约20分钟的人类演示视频（232个片段，平均每个任务约3个演示）。</li>
<li><strong>实验平台</strong>：模拟环境及真实机器人。</li>
<li><strong>基线方法</strong>：模拟实验对比 Pi0.5；真实实验对比同样利用视频先验的 UniPi 和 VPP。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>模拟实验（RoboTwin）</strong>：如表1所示，在低数据（每任务20条）和标准数据（每任务50条）两种设置下，Vidar 的平均成功率均显著优于强大的 Pi0.5 基线。例如，在标准数据、干净场景下，Vidar 达到65.8%，而 Pi0.5 为44.8%。<br><img src="https://..." alt="模拟实验结果表"></p>
<blockquote>
<p><strong>表1</strong>：RoboTwin基准上50个任务的平均成功率。Vidar在所有设置和场景下均超越Pi0.5基线。</p>
</blockquote>
</li>
<li><p><strong>真实世界实验</strong>：如表2所示，在“已见任务与背景”、“未见任务”、“未见背景”三种场景下，Vidar 的成功率均大幅领先 UniPi 和 VPP。特别是在泛化到未见任务和具有反光表面的未见背景时，Vidar 分别取得了66.7%和55.6%的成功率，展现出强大的泛化能力。<br><img src="https://..." alt="真实世界结果表"></p>
<blockquote>
<p><strong>表2</strong>：不同方法在真实机器人操作任务上的成功率。Vidar在所有三个场景中均取得高成功率，对未见任务和背景具有出色的泛化能力。</p>
</blockquote>
</li>
<li><p><strong>泛化能力定性展示</strong>：如图2所示，Vidar 能够处理具有挑战性的任务，例如在多个红色物体中识别并抓取红苹果，或在具有反光柜门的全新背景下完成双手清洁碗的任务，展示了其强大的语义理解和场景泛化能力。<br><img src="https://..." alt="真实世界执行与预测视频"></p>
<blockquote>
<p><strong>图2</strong>：Vidar 对挑战性任务的预测视频（左）与对应执行结果（右）。它能处理未见任务和未见背景，并表现出强大的语义理解。</p>
</blockquote>
</li>
<li><p><strong>组件有效性验证</strong>：</p>
<ul>
<li><strong>具身预训练</strong>：如表3所示，在统一观察空间上进行具身预训练后，生成视频的主体一致性、背景一致性和成像质量均有显著提升，这有利于机器人控制。<br>  <img src="https://..." alt="视频质量评估表"><blockquote>
<p><strong>表3</strong>：在未见目标域上，不同视频模型配置的VBench视频质量测量。基于统一观察空间的具身预训练有益于视频生成。</p>
</blockquote>
</li>
<li><strong>掩码逆动力学模型</strong>：如表4所示，MIDM 在测试集上的动作预测准确率（49.0%）远高于 ResNet 基线（24.3%），且测试误差更低。图3显示，MIDM 能在复杂反光背景下有效聚焦于机器人手臂的关键部位。<br>  <img src="https://..." alt="MIDM效果表与掩码图"><blockquote>
<p><strong>表4</strong>：不同逆动力学模型的训练/测试准确率及测试误差。MIDM在测试时泛化更好。<br><strong>图3</strong>：输入图像及MIDM学习到的掩码图像。在具有复杂反光表面的未见背景下，预测的掩码仍聚焦于机械臂的关键部分。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<p><strong>消融实验</strong>：如表5所示，移除测试时缩放（TTS）或使用 ResNet 基线替代 MIDM 都会导致在所有场景下的成功率下降，这验证了这两个核心组件的必要性。<br><img src="https://..." alt="消融实验表"></p>
<blockquote>
<p><strong>表5</strong>：Vidar的消融研究。在三种场景中，掩码逆动力学模型和测试时缩放均有益于成功率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>Vidar 框架</strong>，将通用机器人操作分解为可迁移视频先验学习与轻量级动作适配两个阶段，实现了仅需极少量目标平台演示（约20分钟）的高效适配。</li>
<li>设计了<strong>统一观察空间</strong>，能够有效对齐异构、多视角的机器人视频数据，为视频扩散模型提供了跨平台的泛化能力。</li>
<li>引入了<strong>掩码逆动力学模型</strong>，以自监督方式学习动作相关区域的注意力掩码，显著提升了在复杂、未知背景下动作解码的鲁棒性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，目前采用开环控制，可能因预测误差累积而影响长时程任务性能。此外，生成一段60帧（7.5秒）的视频约需25秒（使用8块GPU），推理时间成本有待通过蒸馏或量化等技术进一步降低。</p>
<p><strong>对后续研究的启示</strong>：本文展示了一条可扩展的机器人学习路径：<strong>“一个强先验，多个机器人个体”</strong>。即利用互联网规模的通用视频先验（或经过跨平台机器人数据增强的具身先验）来捕获丰富的物理交互知识，再通过极少量数据和轻量适配器对齐到特定机器人。这为降低机器人学习的数据依赖、加速新平台部署提供了有前景的方向。未来工作可探索更高效的视频生成与动作解码协同、闭环控制框架以及更高效的推理技术。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Vidar系统，旨在解决新机器人平台因数据稀缺和视角背景变化而难以实现通用灵巧操作的问题。方法基于预训练的视频扩散模型，通过75万条多视角轨迹进行具身领域持续预训练，并引入掩码逆动力学模型（MIDM）作为适配器，以学习动作相关像素掩码。实验表明，仅需约20分钟的人类演示，Vidar即可在新机器人上超越现有方法，并泛化至未见过的任务、背景和相机布局。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.12898" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>