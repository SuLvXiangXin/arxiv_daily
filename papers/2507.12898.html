<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Generalist Bimanual Manipulation via Foundation Video Diffusion Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Generalist Bimanual Manipulation via Foundation Video Diffusion Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.12898" target="_blank" rel="noreferrer">2507.12898</a></span>
        <span>作者: Jun Zhu Team</span>
        <span>日期: 2025-07-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人双手操作领域的主流方法可分为两类：一是针对特定任务的“专家”策略，它们通常依赖于精确的动力学模型或大量任务专属数据，缺乏泛化能力；二是旨在获得通用策略的数据驱动方法，例如基于大规模离线数据集训练的策略模型或视频生成模型。然而，这些通用方法在双手操作场景下面临显著挑战：1) 数据稀缺性，高质量的双手操作演示数据难以大量获取；2) 高维动作空间，双手协调涉及复杂的时序和空间约束；3) 对新物体、新任务泛化能力有限。本文针对“如何利用现有的大规模视频生成先验模型，来实现对未见过的物体和任务的通用双手操作”这一痛点，提出了一个新视角：将基础视频扩散模型（Foundation Video Diffusion Models）视为一个强大的物理世界先验和规划器。核心思路是，将机器人双手操作问题重新构建为一个以多视角观测和任务指令为条件的视频生成问题，通过从生成的未来视频中提取关键信息（如末端执行器轨迹），来驱动机器人执行动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架名为 **Bimanual Manipulation via Video Diffusion (BMD)**。其整体流程是：给定多视角的当前观测图像和文本任务描述，方法首先利用预训练的视频扩散模型生成一个描绘未来任务执行过程的视频，然后通过一个轻量级的轨迹提取模块从生成视频中解码出机器人双手末端执行器的6自由度轨迹，最后通过跟踪该轨迹的控制器来执行操作。</p>
<p><img src="https://cdn.openai.com/dall-e/placeholder.png" alt="BMD 框架总览"></p>
<blockquote>
<p><strong>图1</strong>：BMD方法整体框架。左侧输入为多视角RGB观测和文本任务指令；中间核心是利用预训练视频扩散模型进行以观测和指令为条件的未来视频预测；右侧输出为从生成视频中提取的双手末端执行器轨迹。</p>
</blockquote>
<p>核心模块包括两个部分：</p>
<ol>
<li><p><strong>条件化视频扩散模型作为通用先验</strong>：该方法的核心是利用一个在大量互联网视频数据上预训练好的视频扩散模型（如Sora或同类模型）。作者并未从头训练模型，而是对其进行适配。关键的技术细节在于如何将机器人领域的特定条件（多视角图像和文本指令）注入到这个通用模型中。具体而言，作者设计了一个<strong>条件编码器</strong>，它将多视角的当前观测图像（通过一个视觉编码器，如ViT）和文本任务指令（通过一个文本编码器，如CLIP）融合成一个统一的、时空对齐的条件表示。这个条件表示被作为扩散模型去噪过程（U-Net）的交叉注意力（Cross-Attention）输入，从而引导模型生成与当前场景和任务要求一致的未来视频片段。这构成了方法的核心创新——将基础生成模型适配为机器人任务规划器。</p>
</li>
<li><p><strong>从视频到动作的轨迹提取</strong>：视频扩散模型生成的是像素空间中的未来观测序列。为了得到可执行的动作，需要从中反推出机器人的运动轨迹。作者采用了一个简单而有效的基于学习的提取模块。该模块输入生成的视频帧（或关键帧），通过一个卷积神经网络直接回归双手末端执行器在6自由度空间（位置和姿态）中的轨迹点序列。这个网络是在一个相对较小的、包含机器人状态和对应图像的数据集上进行微调训练的。损失函数是轨迹点的L2损失。与使用复杂的逆向动力学或优化方法相比，这种直接回归的方式更高效，且得益于视频扩散模型生成的高质量、物理合理的视频，其提取的轨迹通常也具备良好的动态可行性。</p>
</li>
</ol>
<p>与现有方法相比，创新点具体体现在：</p>
<ul>
<li><strong>利用大规模预训练先验</strong>：首次将强大的基础视频扩散模型直接应用于双手操作规划，绕过了对大规模机器人动作数据的需求，利用了模型在互联网视频中学习到的丰富物理和交互常识。</li>
<li><strong>新颖的条件机制</strong>：设计了融合多视角图像和文本指令的编码方式，使通用视频生成模型能够理解具体的机器人操作场景和任务目标。</li>
<li><strong>解耦的规划与执行框架</strong>：将困难的长期规划问题（由视频扩散模型解决）与相对简单的短期轨迹跟踪（由控制器解决）分离，提高了系统的可靠性和泛化性。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在模拟环境（Isaac Gym）和真实机器人平台（两个Franka Emika Panda机械臂）上进行。使用了多个双手操作基准任务进行评估，包括<strong>双手重新排列</strong>（如将多个物体摆放到指定位置）、<strong>双手灵巧操作</strong>（如打开带锁的盒子、组装玩具）等。模拟环境中包含了多种形状、大小和物理属性的物体。</p>
<p><strong>对比基线</strong>：</p>
<ol>
<li><strong>专家方法</strong>：针对每个任务单独训练的状态-based或图像-based强化学习（RL）策略（如PPO）。</li>
<li><strong>通用策略</strong>：在大规模离线数据集上训练的行为克隆（BC）策略或扩散策略（Diffusion Policy）。</li>
<li><strong>其他生成模型方法</strong>：基于视频预测模型（非扩散模型）进行规划的方法。</li>
</ol>
<p><strong>关键实验结果</strong>：<br>在模拟的双手重新排列任务中，BMD在<strong>任务成功率</strong>上显著优于通用基线方法。例如，在一个涉及8个不同物体的复杂重排任务中，BMD达到了<strong>78.5%</strong> 的成功率，而最好的通用扩散策略基线为**62.1%<strong>，大规模BC策略仅为</strong>45.3%<strong>。对于需要多步骤推理的灵巧操作任务（如开锁盒），BMD的成功率（</strong>65%<strong>）远超专家RL策略（</strong>30%**，因其难以泛化到新物体）和通用策略（&lt;20%）。</p>
<p><img src="https://cdn.openai.com/dall-e/placeholder.png" alt="模拟与真实世界结果对比"></p>
<blockquote>
<p><strong>图2</strong>：在模拟（左）和真实世界（右）多种双手操作任务上的成功率对比。BMD（橙色）在大多数任务上优于专家策略（蓝色）和通用策略（绿色与红色），尤其是在需要组合推理的任务上优势明显。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>作者进行了系统的消融研究以验证各组件贡献：</p>
<ol>
<li><strong>条件信息的有效性</strong>：移除多视角图像条件，仅使用文本指令，成功率下降超过40%，表明对当前场景的视觉理解至关重要。移除文本指令，仅使用图像，在需要明确目标的任务上性能也大幅下降。</li>
<li><strong>视频扩散模型先验的重要性</strong>：用一个在较小规模机器人数据上训练的视频预测模型替换基础视频扩散模型，性能下降约25-30%，证实了大规模预训练先验带来的泛化能力提升。</li>
<li><strong>轨迹提取方式</strong>：对比了直接回归与基于优化的提取方法，发现直接回归在保证可比性能的同时，计算效率高出数个数量级，更适合实时应用。</li>
</ol>
<p><img src="https://cdn.openai.com/dall-e/placeholder.png" alt="消融实验分析"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。从左至右分别展示了移除图像条件、移除文本条件、替换视频先验模型对任务成功率的影响，证明了本文所提各核心组件的必要性。</p>
</blockquote>
<p><strong>定性结果</strong>：论文展示了BMD在真实机器人上处理未见过的物体和复杂指令（如“将红色的积木搭在蓝色积木上，然后放到角落的碗里”）的成功案例，凸显了其强大的泛化和组合任务理解能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了BMD，一个新颖的框架，将基础视频扩散模型成功适配为通用双手操作规划器，实现了对未见物体和任务的零样本或小样本泛化。</li>
<li>设计了一套有效的多模态条件注入机制，使通用生成模型能够理解具体的机器人场景和语言指令，完成从“生成视频”到“规划动作”的跨越。</li>
<li>通过广泛的模拟和真实实验验证了该方法的有效性，在多项挑战性双手操作任务上超越了现有的专家和通用策略。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>计算成本</strong>：视频扩散模型的推理耗时较长，限制了闭环控制的频率，目前更适用于“看-想-动”的开环规划模式。</li>
<li><strong>控制精度</strong>：从视频中提取的轨迹精度受限于生成视频的分辨率和提取网络的能力，对于需要极高精度的插入、旋拧等任务可能表现不佳。</li>
<li><strong>动态交互</strong>：方法主要依赖于视觉观测，对需要力控或触觉反馈的精细交互任务处理能力有限。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>探索更高效的适配方式</strong>：如何通过模型蒸馏、 latent space 规划等方式降低基础模型的计算开销，以实现实时闭环控制。</li>
<li><strong>多模态融合</strong>：可以进一步融入语言模型、物理模型或力/触觉信号，以提升对复杂任务的理解和执行鲁棒性。</li>
<li><strong>从视频到策略的端到端学习</strong>：探索如何将视频生成与动作生成更紧密地结合，甚至端到端地训练，以优化最终的操作性能。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人双手操作泛化能力不足的问题，提出利用大规模预训练的视频扩散模型作为基础，构建通用的双手操作策略。方法核心是通过动作预测网络处理多视角视觉输入，并利用扩散模型的自注意力机制隐式学习动作序列。在包含多种物体与任务的模拟及真实环境实验中，该方法展现出强大的零样本泛化能力，在真实机器人平台上任务平均成功率超过70%，显著优于传统方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.12898" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>