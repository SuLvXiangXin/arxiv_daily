<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.08303" target="_blank" rel="noreferrer">2507.08303</a></span>
        <span>作者: Yue Gao Team</span>
        <span>日期: 2025-07-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>基于强化学习（RL）的运动控制方法使人形机器人能够学习复杂的运动技能，但由于仿真与现实的差异（环境变化、传感器噪声、外部扰动），神经控制器面临域分布偏移和仿真到现实（sim-to-real）的迁移问题。此外，RL策略对扰动固有的敏感性以及缺乏系统性的鲁棒性设计，导致策略在现实部署中不稳定，难以完成长时程（long horizon）任务。现有提升鲁棒性的方法主要包括域随机化（DR）和正则化约束。DR在训练中引入随机扰动以模拟不确定性，但无法针对策略的特定脆弱点进行扰动；正则化约束（如Lipschitz正则化、对称性正则化）需要在策略探索和鲁棒约束之间进行权衡；而依赖真实世界数据的残差模型则存在实现效率低的问题。本文针对现有方法无法精准识别并攻击策略网络脆弱性的痛点，提出了一种新的视角：通过一个可学习的对抗性攻击策略来识别运动策略的脆弱状态，并施加有针对性的稀疏扰动，从而在对抗训练中持续强化运动策略。本文的核心思路是提出一种用于鲁棒训练的选择性对抗攻击（SA2RT），通过一个受攻击预算约束的选择性攻击策略（SAP）来识别并稀疏地扰动最脆弱的状态和动作，在非零和交替优化中持续增强运动策略对抗最强攻击的能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>SA2RT框架旨在通过选择性对抗攻击增强人形机器人运动策略的鲁棒性。整体框架包含两个可学习模块：运动策略（Motion Policy）和攻击策略（Attack Policy）。两者以博弈论方式进行交替优化：运动策略在攻击策略的扰动下优化任务性能，提升抗干扰能力；攻击策略则识别脆弱状态并选择性施加对抗性扰动以使机器人失稳。在部署时，仅使用训练好的鲁棒运动策略，无需攻击策略。</p>
<p><img src="https://arxiv.org/html/2507.08303v3/fig2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：SA2RT方法整体框架。左侧为交替对抗训练过程：选择性攻击策略（SAP）识别运动状态中的脆弱性，并在状态空间和动作空间生成对抗样本；通过非零和博弈下的交替对抗训练，运动策略持续利用对抗样本来应对自身脆弱性，从而增强对扰动的鲁棒性。右侧为部署阶段：训练好的鲁棒运动技能被部署到真实机器人上，无需SAP，实现鲁棒的全身运动控制。</p>
</blockquote>
<p>核心模块是<strong>选择性攻击策略（SAP）</strong>。与持续攻击策略（PAP）在所有时间步施加扰动不同，SAP在攻击预算约束下，不仅学习如何生成最优扰动，还学习识别哪些是攻击最有效的脆弱状态（即“关键步”）。其动作空间定义为运动策略的状态和动作的联合空间。给定一个预训练的运动策略，SAP的目标是在一个片段中，仅选择时间步子集 $\mathcal{T}<em>{adv}$（$|\mathcal{T}</em>{adv}| \ll T$）对机器人进行扰动。这被表述为一个带攻击预算 $N_a$ 的优化问题（公式3）。为处理硬预算约束，引入了拉格朗日乘子 $\lambda$ 将其转化为软惩罚项（公式4），$\lambda$ 是控制每次攻击惩罚的超参数。SAP在另一个MDP $\hat{\mathcal{M}}^{\prime}$ 中使用RL进行训练，其奖励为 $\hat{R}^{\prime} = \hat{R} - \lambda b$。</p>
<p>另一个关键创新是<strong>非零和对抗训练</strong>。与零和博弈（攻击者奖励 $R^{\alpha} = -R^{\upsilon}$）不同，本文将运动策略 $\pi^m$ 与攻击策略 $\pi^{adv}$ 之间的对抗训练构建为非零和博弈。攻击策略的目标是最大化与机器人安全失败相关的攻击奖励 $R^{adv}$，同时负面影响运动策略，即最大化 $V_{\pi}^{adv}(s) - V_{\pi}^{m}(s)$（公式6）。为确保两个价值函数变化的单调性，借鉴相关工作推导出新的优化目标（公式7），并使用时差学习与两个独立的神经网络来近似攻击策略和运动策略的价值函数，再应用策略梯度方法进行优化。</p>
<p>训练采用<strong>交替优化过程</strong>（算法1）。运动策略从预训练策略初始化，攻击策略随机初始化。在每次大迭代中，先固定运动策略，优化攻击策略 $N_{adv}$ 轮以发现其内在弱点并施加攻击；然后固定攻击策略，优化运动策略 $N_m$ 轮以增强其对抗攻击的韧性。通过 $N_{iter}$ 轮动态交替优化，实现在线鲁棒对抗训练。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Unitree G1人形机器人平台上进行，涉及两项任务：<strong>感知运动（Perceptive Locomotion, PL）</strong> 和 <strong>全身控制（Whole-Body Control, WBC）</strong>。</p>
<p>对比的基线方法包括：</p>
<ul>
<li><strong>PL/WBC-CE</strong>: 在干净环境中训练的策略（无DR，无对抗攻击）。</li>
<li><strong>PL/WBC-DR</strong>: 在训练中应用域随机化（DR）的策略。</li>
<li><strong>PL/WBC-PAP</strong>: 使用持续攻击策略（PAP）进行鲁棒对抗训练的策略。</li>
<li><strong>PL/WBC-SAP</strong>: 使用选择性攻击策略（SAP）进行鲁棒对抗训练的策略（本文方法）。</li>
</ul>
<p>评估指标包括速度跟踪误差、重力投影分量（评估动态稳定性）、关键点/关节位置跟踪误差以及复杂地形遍历和全身轨迹跟踪的<strong>成功率（$R_{sr}$）</strong>。</p>
<p><strong>关键实验结果：</strong></p>
<ol>
<li><strong>对抗性扰动的有效性</strong>：如表1所示，DR能显著降低PL/WBC-CE策略的成功率（PL-CE: 77.3%），但无法影响经过对抗训练的PL/WBC-SAP策略（97.4%）。反之，SAP能彻底击败PL/WBC-CE和PL/WBC-DR策略（成功率降至0%），但PL/WBC-SAP策略仍能保持高成功率（95.7%）。这表明SAP能有效暴露策略脆弱性并引入比DR更强的扰动。</li>
<li><strong>运动策略的鲁棒性提升</strong>：在感知运动任务中，如表2所示，在干净和DR环境中，PL-SAP在平均线速度误差（$E_{vel}$）、角速度误差（$E_{ang}$）和重力投影分量（$E_g$）上均优于PL-DR。在全身控制任务中，如图3所示，WBC-SAP在所有评估指标（上下半身的关键点位置误差MPKPE、关节位置误差MPJPE、速度误差）上均优于WBC-DR，特别是在速度跟踪方面。这些结果表明SA2RT能有效解决运动策略的脆弱性，提升其鲁棒性和任务性能。论文指出，对抗训练的策略将地形遍历成功率提高了40%，轨迹跟踪误差降低了32%。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.08303v3/fig3.png" alt="全身控制性能分析"></p>
<blockquote>
<p><strong>图3</strong>：全身控制性能分析。在干净环境和DR环境中评估WBC-DR和WBC-SAP的轨迹跟踪误差。WBC-SAP在所有评估指标上均优于WBC-DR，表明SA2RT有效增强了WBC策略的鲁棒性和跟踪性能。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：比较了在不同扰动水平 $L_p$ 下，由PAP和SAP对抗训练的运动策略性能。如图4a所示，随着 $L_p$ 增加，PL-SAP-$L_p$ 的归一化奖励下降幅度小于PL-PAP-$L_p$，表明SAP对脆弱状态的选择性攻击避免了因对无效稳定状态攻击而导致的过度保守。如图4b所示，在无扰动环境中，PL-SAP-$L_p$ 的性能下降速度慢于PL-PAP-$L_p$，而PL-PAP-$L_p$ 的下降甚至超过PL-DR。这表明PAP的全时攻击会导致运动策略的状态分布发生显著偏移，从而在低扰动环境中性能明显下降；而SAP仅攻击脆弱状态，具有高效性和隐蔽性，能有效平衡运动策略的性能和鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.08303v3/fig4-1.png" alt="不同攻击策略的影响"></p>
<blockquote>
<p><strong>图4a</strong>：在不同扰动水平 $L_p$ 下，通过不同攻击策略学习的运动策略的奖励。对抗训练在鲁棒性上优于DR。随着 $L_p$ 增加，PL-SAP-$L_p$ 的奖励下降小于PL-PAP-$L_p$。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08303v3/fig4-2.png" alt="无扰动环境性能比较"></p>
<blockquote>
<p><strong>图4b</strong>：在不同 $L_p$ 下训练的运动策略在无扰动环境中的性能比较。PL-SAP-$L_p$ 比PL-PAP-$L_p$ 表现出更慢的性能下降，而PL-PAP-$L_p$ 的下降超过PL-DR。</p>
</blockquote>
<ol start="4">
<li><strong>选择性攻击策略分析</strong>：如图5所示，SAP的攻击比例随任务难度（平地、斜坡、楼梯、离散地形）增加而逐渐上升。图6展示了机器人在从平地向楼梯过渡时，SAP识别到状态脆弱性并施加少量攻击成功诱导跌倒的序列。图7分析了超参数 $\lambda$ 对WBC-SAP攻击比例和性能的影响：随着 $\lambda$ 增大，攻击比例下降，但性能（奖励）呈现先升后降的趋势，说明需要权衡攻击的稀疏性与训练效果。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.08303v3/fig5.png" alt="SAP攻击比例随任务变化"></p>
<blockquote>
<p><strong>图5</strong>：SAP的攻击比例在不同运动任务间差异显著。随着任务难度增加，攻击比例逐渐升高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08303v3/fig6-1.png" alt="楼梯过渡时的SAP攻击序列"></p>
<blockquote>
<p><strong>图6a</strong>：机器人运动状态可视化。机器人准备穿越楼梯时，SAP识别到状态脆弱性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08303v3/fig6-2.png" alt="攻击变量序列"></p>
<blockquote>
<p><strong>图6b</strong>：攻击变量序列。SAP施加了少数几次攻击，成功诱导了跌倒。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08303v3/fig7.png" alt="参数λ对攻击比例和性能的影响"></p>
<blockquote>
<p><strong>图7</strong>：WBC-SAP的攻击比例和性能随参数 $\lambda$ 变化的比较。蓝线代表攻击比例，橙线代表奖励。随着 $\lambda$ 增加，攻击比例下降，但性能呈现先上升后下降的趋势。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一种用于人形机器人的<strong>选择性对抗攻击鲁棒训练（SA2RT）</strong>框架，通过可学习的对抗攻击策略来识别运动策略的脆弱性并生成有效对抗样本；2）设计了受<strong>攻击预算约束的选择性攻击策略（SAP）</strong>，能够稀疏地扰动最关键的状态，在增强鲁棒性的同时避免诱导保守性退化或性能显著下降；3）在真实机器人上的大量实验表明，该方法能显著提升人形机器人在复杂地形中的长时程移动能力和轨迹跟踪性能。</p>
<p>论文提到的局限性包括：1）SA2RT框架依赖于一个预训练的运动策略作为起点；2）交替对抗训练过程可能带来额外的计算成本。</p>
<p>本文工作对后续研究的启示在于：将对抗性攻击从单纯的“测试工具”转变为“训练伙伴”，为提升机器人策略的鲁棒性提供了一种新范式。SAP所体现的“精准打击”思想，可以推广到其他需要应对不确定性和扰动的强化学习应用中。如何将这种选择性攻击机制与模型基础策略、更高效的非零和优化算法结合，以及如何将其扩展到多智能体或人机协作场景，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人运动策略在长时间运行、噪声及干扰下稳定性不足的问题，提出一种**选择性对抗攻击鲁棒训练方法（SA2RT）**。该方法通过**学习对抗攻击者**，在攻击预算约束下**稀疏扰动最脆弱的状态与动作**，暴露策略真实弱点，并采用**非零和交替优化**持续强化策略。在Unitree G1人形机器人上的实验表明，经对抗训练的策略**地形穿越成功率提升40%**，**轨迹跟踪误差降低32%**，显著增强了长时程运动的鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.08303" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>