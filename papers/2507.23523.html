<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.23523" target="_blank" rel="noreferrer">2507.23523</a></span>
        <span>作者: Jun Zhu Team</span>
        <span>日期: 2025-08-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习面临一个根本性挑战：大规模、高质量机器人演示数据的稀缺。当前主流方法，特别是机器人基础模型，通常依赖于跨具身机器人数据集（如Open X-Embodiment）进行预训练以扩大数据规模。然而，这种方法存在关键局限性：不同机器人实体在形态学和动作空间上的巨大差异使得统一训练变得困难，且现有机器人数据集规模仍然有限，数据质量参差不齐，从根本上制约了通用机器人操作所需的数据可用性和泛化能力。</p>
<p>本文针对数据稀缺和跨具身学习困难这一具体痛点，提出了一个新颖视角：利用大规模、易于获取的人类操作数据作为丰富的“行为先验”来增强机器人策略学习。人类演示数据（如带有3D手部姿态标注的自我中心视频）天然地捕捉了物体可供性、操作策略和任务分解模式，这些可以作为机器人学习的有力归纳偏置。</p>
<p>本文核心思路是：设计一个两阶段训练范式，首先在大规模人类操作数据上进行预训练，学习通用的操作语义和策略；然后通过模块化的动作编码器/解码器设计，将习得的知识高效地迁移到多样化的机器人平台上进行跨具身微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>H-RDT的整体框架是一个基于扩散Transformer（Diffusion Transformer）的两阶段训练流程，其核心目标是利用人类数据预训练，再适配到具体机器人。</p>
<p><img src="https://arxiv.org/html/2507.23523v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：H-RDT框架总览。我们的方法包含两个主要阶段：(1) 在大规模人类操作数据上使用48维手部姿态表示进行预训练；(2) 通过模块化的动作编码器和解码器，针对特定机器人动作空间进行跨具身微调。</p>
</blockquote>
<p><strong>整体流程</strong>：输入为多模态观测，包括多视角RGB图像、本体感知状态（机器人状态和夹爪状态）和语言指令。模型输出未来H步的动作序列。流程分为两个阶段：第一阶段，模型在包含33.8万条轨迹的EgoDex人类数据集上，以48维手部姿态作为动作表示进行预训练；第二阶段，针对目标机器人（如双7自由度机械臂），保留预训练模型的视觉编码器、语言编码器和Transformer主干权重，但重新初始化状态适配器、动作适配器和动作解码器，并使用机器人特定数据进行微调。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>人类动作表示设计</strong>：为了弥合人与机器人之间的具身差异，H-RDT采用详细的3D手部姿态作为统一表示。具体为48维向量：双侧手腕位姿（位置3D+方向6D，共18维，与机器人末端执行器位姿同构）和所有指尖的位置（双手所有手指的3D坐标，共30维）。这种表示是当前大多数以末端执行器位姿控制的机器人动作空间的超集，确保了跨不同运动学结构的有效知识蒸馏。</li>
<li><strong>两阶段训练范式</strong>：<ul>
<li><strong>阶段一：人类数据预训练</strong>：在完整的EgoDex数据集上，使用上述48维人类手部动作表示和流匹配（Flow Matching）目标进行训练。</li>
<li><strong>阶段二：跨具身微调</strong>：采用选择性权重迁移策略。视觉编码器、语言编码器和Transformer主干的权重从预训练模型迁移，以保留从人类演示中学到的多模态表示和操作先验。而状态适配器（<code>MLP_state</code>）、动作适配器（<code>MLP_action</code>）和动作解码器则完全重新初始化，以处理目标机器人的特定动作空间（例如，双7自由度机械臂加平行夹爪为14维）。这种模块化设计使得动作编码解码器可以为每个目标具身从头开始重新训练，而不损害已习得的视觉语义表示。</li>
</ul>
</li>
<li><strong>H-RDT架构与流匹配</strong>：<ul>
<li><strong>流匹配</strong>：H-RDT采用流匹配而非传统扩散进行动作生成，因其具有更好的训练稳定性和推理效率。它学习一个向量场，将简单噪声分布通过连续标准化流变换到目标动作分布。损失函数为预测向量场与目标向量场之间的均方误差。</li>
<li><strong>网络架构</strong>：包含五个模块化组件：<ul>
<li><strong>视觉与语言编码器</strong>：分别使用预训练的DinoV2、SigLIP和T5-XXL模型提取特征，并通过MLP适配器投影到模型嵌入空间。</li>
<li><strong>模块化动作编码器</strong>：通过<code>StateAdapter</code>和<code>ActionAdapter</code>两个MLP，分别将本体感知状态和带噪声的动作序列编码为嵌入。</li>
<li><strong>Transformer主干</strong>：采用LLaMA-3架构风格，处理拼接后的状态和动作嵌入。图像和语言特征通过独立的交叉注意力注入。流时间τ通过AdaLN集成。</li>
<li><strong>模块化动作解码器</strong>：一个MLP，将Transformer输出的动作隐藏状态解码为目标机器人动作空间中的动作序列。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法（如RDT、π0）主要依赖跨机器人数据或小规模人类数据相比，H-RDT的创新具体体现在：1) <strong>大规模人类先验利用</strong>：首次系统性地利用超大规模（829小时/33.8万条轨迹）带精确3D标注的人类自我中心视频进行预训练；2) <strong>模块化跨具身迁移设计</strong>：通过模块化的动作编码/解码器及选择性权重迁移的两阶段范式，实现了从统一人类具身到任意机器人形态的高效知识迁移，解决了严格的配对数据需求问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/平台</strong>：模拟实验使用RoboTwin 2.0平台，包含简单模式和困难模式（含域随机化）。真实世界实验在三个不同平台上进行：Aloha-Agilex-2.0（双臂Piper）、双臂ARX5、以及双UR5+UMI配置。</li>
<li><strong>基线方法</strong>：与 Robotics Diffusion Transformer (RDT)、最先进的VLA模型 π0，以及无人类预训练的H-RDT（<code>w/o human</code>）进行对比。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>Aloha-Agilex-2.0平台双任务评估</strong>：</p>
<ul>
<li><strong>毛巾折叠任务</strong>：H-RDT实现了52%的完全成功率，优于RDT的40%和<code>w/o human</code>的0%。<code>w/o human</code>模型完全无法完成折叠，仅能达到较低技能水平的部分成功。</li>
<li><strong>杯子到杯垫放置任务</strong>：H-RDT实现了64%的完全成功率，优于RDT的28%和<code>w/o human</code>的20%。H-RDT表现出最低的失败率和较少的部分成功实例。</li>
<li><strong>结论</strong>：在两个双任务中，H-RDT平均成功率达58%，显著优于RDT的34%和<code>w/o human</code>的10%，证明了人类操作先验在处理可变性物体操作和空间推理任务中的有效性。</li>
</ul>
</li>
<li><p><strong>双臂ARX5少样本实验</strong>：</p>
<ul>
<li>在包含113个多样化抓放任务、每个任务仅1-5条演示的极具挑战性的少样本设置下，H-RDT取得了41.6%的平均成功率，显著高于RDT的16.0%、π0的31.2%和<code>w/o human</code>的17.6%。</li>
<li><strong>结论</strong>：在数据极其有限的情况下，H-RDT的人类操作先验展现出显著的样本效率优势。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.23523v2/x2.png" alt="少样本学习结果"></p>
<blockquote>
<p><strong>图2</strong>：双臂ARX5少样本学习结果。H-RDT在五个代表性任务类别上的成功率均显著优于基线方法，尤其是在“将香蕉和胡萝卜放入篮子”和“将立方体放在最高薯片前面”等需要复杂推理的任务上优势明显。</p>
</blockquote>
<ol start="3">
<li><strong>双UR5+UMI实验</strong>：<ul>
<li>在外卖袋放置任务（分解为四个顺序子任务）中，H-RDT在右/左手抓取和放置四个子任务上的平均成功率达到58.0%，远超RDT的29.0%、π0的31.0%和<code>w/o human</code>的16.0%。</li>
<li><strong>结论</strong>：在使用UMI收集的演示数据上，H-RDT同样展现出强大的性能提升，验证了其在不同数据收集方法下的鲁棒性。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.23523v2/x3.png" alt="UR5任务分解结果"></p>
<blockquote>
<p><strong>图3</strong>：双UR5+UMI外卖袋放置任务子任务成功率分解。H-RDT在所有四个子任务（右抓、右放、左抓、左放）上均取得最高成功率，展示了其稳健的双臂协调能力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：论文通过<code>w/o human</code>（即无人类预训练，直接从机器人数据训练）这一关键消融实验，验证了人类数据预训练的核心贡献。在所有真实世界和少样本实验中，<code>w/o human</code>版本性能均大幅落后于完整H-RDT，特别是在需要复杂操作（如毛巾折叠）或数据稀缺的场景下，差距尤为显著，这直接证明了大规模人类先验对于提升机器人策略学习样本效率和最终性能的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出并验证了利用大规模人类操作数据作为机器人策略学习先验的有效范式</strong>：通过系统性的实验证明，从829小时人类自我中心视频中学习的行为先验，可以显著提升双手机器人操作在模拟和真实环境中的性能、样本效率和鲁棒性。</li>
<li><strong>设计了模块化的两阶段跨具身迁移架构</strong>：创新的选择性权重迁移策略和模块化的动作编码/解码器，使得模型能够将从统一人类具身中学到的通用操作知识，高效适配到多种形态迥异的机器人平台上，解决了严格配对数据的限制。</li>
<li><strong>提供了全面的实证评估</strong>：在多种机器人平台、单任务/多任务、少样本学习及鲁棒性测试等维度上进行了广泛验证，结果表明H-RDT优于从头训练和现有最先进方法，例如在模拟和真实实验中相比从头训练分别取得了13.9%和40.5%的显著提升。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前方法依赖于具有精确3D手部姿态标注的人类数据集（如EgoDex）。虽然此类数据集正在出现，但其规模和多样性仍可能成为瓶颈。此外，从人类到机器人的知识迁移效率可能受到两者形态学差异程度的影响。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据方向</strong>：探索如何利用更丰富、标注成本更低的人类视频数据（如仅2D或弱标注），或结合物理仿真生成的人类数据，进一步扩大行为先验的规模与多样性。</li>
<li><strong>迁移机制</strong>：可以研究更精细的迁移学习技术，例如针对特定机器人形态或任务家族动态调整迁移哪些知识模块，以进一步提升跨具身适配的效率和性能上限。</li>
<li><strong>任务泛化</strong>：将人类先验应用于更复杂的长期任务规划、工具使用或非刚性物体操作等场景，检验其在更高层次认知任务上的泛化能力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出H-RDT，解决机器人模仿学习中高质量示范数据稀缺的难题。其核心是利用大规模人类操作视频（带3D手部姿态标注）作为行为先验，通过两阶段训练范式：先在人类数据上预训练，再通过模块化动作编码器/解码器在机器人数据上跨具身微调。基于20亿参数扩散变换器架构，采用流匹配建模复杂动作分布。实验表明，H-RDT在模拟和真实世界任务中均显著优于从头训练及现有方法，性能分别提升13.9%和40.5%，验证了人类数据作为机器人操作策略学习基础的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.23523" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>