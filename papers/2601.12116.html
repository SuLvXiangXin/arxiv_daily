<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BiKC+: Bimanual Hierarchical Imitation with Keypose-Conditioned Coordination-Aware Consistency Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>BiKC+: Bimanual Hierarchical Imitation with Keypose-Conditioned Coordination-Aware Consistency Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.12116" target="_blank" rel="noreferrer">2601.12116</a></span>
        <span>作者: Jia PanI Team</span>
        <span>日期: 2026-01-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，模仿学习（IL）已广泛应用于机器人操作，而集成生成模型（如基于cVAE的ACT和基于扩散模型的DP）的方法在解决复合误差和演示数据多模态分布方面取得了进展。然而，这些方法在应用于双手机器人多阶段操作任务时面临两个关键局限性：首先，它们未显式考虑任务的多阶段性，前序阶段的失败会累积影响整体成功率；其次，DP等方法的迭代去噪过程导致推理速度慢，损害了操作效率。此外，双手机器人演示数据因自由度增加而呈现更高的行为多样性（分布多模态），且双臂间复杂的时空协调关系使得直接应用基于关键姿势（Keypose）的分层方法变得困难。</p>
<p>本文针对双手机器人多阶段操作任务中“阶段可靠性”与“步骤效率”的痛点，提出了一种新的分层模仿学习框架。核心思路是：通过一个创新的三阶段管道识别双手机器人关键姿势作为子目标，并构建一个高层关键姿势预测器与低层一致性模型轨迹生成器相结合的分层策略，在保证动作质量的同时实现一步快速推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>BiKC+是一个分层模仿学习框架，包含高层的关键姿势预测器和低层的轨迹生成器。输入为当前观测（RGB图像和本体感知信息）及上一个关键姿势，输出为短时域的动作序列。关键姿势在关节空间定义，并附带一个协调指示器，标记该姿势是否需要双臂时空同步。</p>
<p><img src="https://arxiv.org/html/2601.12116v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：BiKC+框架总览。顶层是关键姿势预测器，根据当前观测和上一个关键姿势预测下一个目标关键姿势及其协调模式。底层是轨迹生成器（一致性模型），根据历史观测和预测的关键姿势，一步生成短时域动作序列。顶部以“传递立方体”任务为例展示了关键姿势序列。</p>
</blockquote>
<p>核心创新之一是提出了一个三阶段的双手机器人关键姿势识别管道，如图2所示。</p>
<p><img src="https://arxiv.org/html/2601.12116v1/x2.png" alt="关键姿势识别管道"></p>
<blockquote>
<p><strong>图2</strong>：构建双手机器人关键姿势数据集的三阶段流程。(a) 整体流程：1) 单臂关键姿势提取；2) 基于VLM辅助接触感知的双臂协调模式识别；3) 协调驱动的关键姿势合并。(b) 协调模式识别细节：利用VLM分析RGB图像判断接触关系，进而推断协调模式。</p>
</blockquote>
<p><strong>阶段1: 单臂关键姿势提取</strong>。如图3所示，使用启发式规则单独分析每条手臂的动作轨迹，规则包括：夹爪开合状态变化、运动速度低于阈值、以及针对特定任务的空间关系（如两夹爪间距离或高度差）。</p>
<p><img src="https://arxiv.org/html/2601.12116v1/x3.png" alt="单臂关键姿势启发式规则"></p>
<blockquote>
<p><strong>图3</strong>：识别单臂关键姿势的启发式规则示意图，包括接触模式变化、运动停滞和空间关系。</p>
</blockquote>
<p><strong>阶段2: 双臂协调模式识别</strong>。这是本文的关键创新。首先，利用视觉语言模型（VLM）分析演示中的RGB图像，自动判断机器人、物体之间的接触关系（例如，“左夹爪是否接触立方体？”）。然后，基于这些接触关系推断双臂的协调模式（同步或独立）。例如，当两个夹爪同时接触同一物体时，则判定该时刻需要协调（<code>m=1</code>）。</p>
<p><strong>阶段3: 协调驱动的关键姿势合并</strong>。基于识别出的协调模式，对两臂单独提取的关键姿势进行合并：在需要协调的模式下，确保两臂的关键姿势在时间上对齐，以实现同步；在非协调模式下，则允许两臂的关键姿势在时间上保持独立，保留各自运动的灵活性。</p>
<p><strong>高层关键姿势预测器</strong>：被建模为一个一致性模型（CM），输入当前观测和上一个关键姿势，输出预测的下一个关键姿势及其协调指示器。CM能够处理由部分矛盾的演示导致的关键姿势多模态分布。</p>
<p><strong>低层轨迹生成器</strong>：同样被建模为一个从零开始训练的一致性模型（CM）。输入为一段历史观测（包含图像和本体感知）以及由高层预测器提供的目标关键姿势，通过一步推理直接生成未来多步的动作序列。这与需要多步迭代的扩散策略（DP）形成对比，在保持生成质量的同时极大地提升了推理速度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境（使用SAPIEN仿真器）和真实世界（两台UR5机械臂）中进行。任务包括<strong>立方体传递</strong>、<strong>电池插装</strong>和<strong>布料折叠</strong>，均为多阶段双手机器人操作任务。对比的基线方法包括：行为克隆（BC）、动作块变换器（ACT）、扩散策略（DP）以及前序工作BiKC。</p>
<p><img src="https://arxiv.org/html/2601.12116v1/x4.png" alt="模拟环境成功率与效率对比"></p>
<blockquote>
<p><strong>图4</strong>：模拟环境中，不同方法在三个任务上的成功率和任务完成时间对比。BiKC+在成功率上显著优于所有基线（传递: 96.7%， 插装: 93.3%， 折叠: 86.7%），并且完成时间最短，与DP相比效率提升显著。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.12116v1/x5.png" alt="真实世界成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：真实世界中，不同方法在三个任务上的成功率对比。BiKC+同样取得最高成功率（传递: 90%， 插装: 86.7%， 折叠: 73.3%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.12116v1/x6.png" alt="推理延迟对比"></p>
<blockquote>
<p><strong>图6</strong>：不同策略的每步推理延迟对比。BiKC+（CM）的一步推理延迟（<del>10ms）远低于需要迭代的DP（</del>150ms），与ACT相当，但性能更优。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.12116v1/x7.png" alt="消融实验：协调感知与关键姿势预测的作用"></p>
<blockquote>
<p><strong>图7</strong>：消融实验验证协调感知关键姿势识别（Ours-CA）和关键姿势预测器（Ours）的作用。移除任一组件（Ours w/o CA, Ours w/o KP）均会导致成功率下降，证明二者对提升多阶段任务可靠性至关重要。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.12116v1/x8.png" alt="消融实验：关键姿势预测器的多模态建模能力"></p>
<blockquote>
<p><strong>图8</strong>：关键姿势预测器（CM）与确定性模型（MLP）在多模态演示下的对比。CM能预测出合理的关键姿势分布，而MLP只能预测平均值，可能导致无效姿势。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>性能领先</strong>：在模拟和真实实验中，BiKC+在三个多阶段任务上的成功率全面超越所有基线方法（模拟最高96.7%，真实最高90%）。</li>
<li><strong>效率优势</strong>：得益于一致性模型的一步推理，BiKC+的任务完成时间最短，比DP快一个数量级，且推理延迟与ACT相当（约10ms）。</li>
<li><strong>消融验证</strong>：<ul>
<li>协调感知的关键姿势识别和关键姿势预测器是提升成功率的核心组件，移除后性能显著下降。</li>
<li>基于CM的关键姿势预测器能有效处理多模态演示，优于确定性预测模型。</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li>提出了一种创新的三阶段管道，用于识别双手机器人关键姿势，首次通过VLM辅助的接触感知将任务中心的操作风格（协调模式）融入关键姿势定义。</li>
<li>提出了BiKC+分层模仿学习框架，其中高层关键姿势预测器提供子目标指导，显式增强了多阶段任务中每个子阶段的可靠性。</li>
<li>设计了一个从零开始训练的一致性模型作为轨迹生成器，实现了高质量动作序列的一步快速推理，在效率和样本质量间取得了平衡。</li>
</ol>
<p>论文提到的局限性在于，当前方法主要依赖视觉和本体感知，尚未集成力/扭矩传感器信息，因此难以精确复现涉及接触和力交互的精细动作。</p>
<p>这项工作对后续研究的启示在于：首先，展示了将高层任务理解（通过VLM）与低层运动生成结合的潜力；其次，验证了一致性模型在机器人策略中实现高效、高质量推理的有效性；未来方向包括融合多模态传感信息（如力觉）以处理更精细的操作任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人双手机器人操作中双臂协调和多阶段处理的挑战，现有方法未明确考虑多阶段性质且推理速度慢。提出BiKC+分层模仿学习框架，包含高层关键姿势预测器和低层一致性模型轨迹生成器：关键姿势预测器预测子目标，轨迹生成器基于历史观测和关键姿势单步推理生成动作序列。仿真和真实实验表明，该方法在成功率和操作效率上显著优于基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.12116" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>