<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.09849" target="_blank" rel="noreferrer">2602.09849</a></span>
        <span>作者: Hu, Yucheng, Zhang, Jianke, Luo, Yuanfei, Guo, Yanjiang, Chen, Xiaoyu, Sun, Xinshu, Feng, Kun, Lu, Qingzhou, Chen, Sheng, Zhang, Yangang, Li, Wei, Chen, Jianyu</span>
        <span>日期: 2026/02/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，在长视野操作任务中，主流方法主要依赖于大型视觉语言模型（VLMs）来生成高级语言规划，随后由低级策略模型执行。然而，这种方法存在两个关键局限性：首先，语言规划与物理动作执行之间存在“语义鸿沟”，导致规划误差在长序列中累积；其次，语言指令在描述复杂空间关系和连续动作时往往不够精确。本文针对“如何将高层次推理与低层次控制紧密集成以执行复杂长视野任务”这一具体痛点，提出了一个新颖的视角：<strong>交错式视觉-语言-动作生成</strong>。核心思路是摒弃传统的“先规划后执行”的串行范式，转而设计一个统一的模型，能够根据当前视觉观察和任务指令，动态地、交错地生成可执行的动作和描述未来子目标的视觉-语言标记，从而将动作执行与任务推理深度耦合。</p>
<h2 id="方法详解">方法详解</h2>
<p>BagelVLA 是一个端到端的模型，其输入是当前图像观察 <code>o_t</code> 和任务语言指令 <code>l</code>，输出是当前步的机器人动作 <code>a_t</code> 以及一组描述后续子任务的视觉-语言标记 <code>{v^g, l^g}</code>。其核心创新在于将动作生成与视觉-语言推理在 token 序列层面进行交错与融合。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/bagelvla/main/figures/framework.png" alt="BagelVLA框架"></p>
<blockquote>
<p><strong>图1</strong>：BagelVLA 整体框架。模型以当前图像和任务指令为输入，通过视觉编码器和语言编码器分别提取特征。这些特征与可学习的动作标记拼接，送入一个多模态 Transformer 进行联合解码。解码器以交错的方式输出动作 token 和未来子目标的视觉-语言 token。</p>
</blockquote>
<p>整体流程包含三个核心模块：</p>
<ol>
<li><strong>视觉-语言状态追踪器</strong>：使用预训练的视觉编码器（如 ViT）和语言编码器（如 BERT）分别处理当前图像 <code>o_t</code> 和任务指令 <code>l</code>，得到视觉特征序列 <code>V_t</code> 和语言特征 <code>L</code>。这些特征被拼接为一个联合的上下文序列 <code>[V_t; L]</code>，用于供后续解码器理解当前状态和最终目标。</li>
<li><strong>语言引导的动作生成器</strong>：这是模型的核心。除了上下文序列，模型还初始化一组可学习的“动作标记”。所有这些 token 被一并输入到一个多模态 Transformer 解码器中。该解码器的关键设计是其<strong>交错生成机制</strong>：它不仅预测控制机器人关节或末端执行器的动作 token (<code>a_t</code>)，还同时预测一组代表未来子目标的“视觉-语言目标 token” (<code>{v^g, l^g}</code>)。这些目标 token 包含一个视觉目标嵌入（指向未来某个期望的图像状态）和一个简短的语言描述（如“拿起蓝色积木”）。</li>
<li><strong>交错执行策略</strong>：在推理时，模型在每个时间步 <code>t</code> 执行上述过程：输出当前动作 <code>a_t</code> 并执行；同时，输出的视觉-语言目标 token 会被缓存，作为下一个时间步 <code>t+1</code> 的部分输入，用于引导模型朝向完成那个子目标前进。这样，规划和执行在每个时间步都得到更新和细化。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：</p>
<ul>
<li><strong>统一的 VLA 表示</strong>：将动作、视觉子目标和语言子目标统一表示为 Transformer 的 token，使模型能在同一空间内进行联合推理。</li>
<li><strong>交错生成</strong>：打破了“先语言规划，后动作执行”的严格阶段划分，实现了实时的、基于当前状态的规划-执行闭环。</li>
<li><strong>目标驱动的动作生成</strong>：当前动作的生成不仅依赖于最终指令，也直接受到模型自身预测的、即将到来的视觉-语言子目标的调节。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/your-repo/bagelvla/main/figures/generation_example.png" alt="交错生成序列示例"></p>
<blockquote>
<p><strong>图2</strong>：模型在一个时间步内的交错生成 token 序列示例。输出序列中，动作 token (A1, A2...) 与视觉目标 token (VG) 和语言目标 token (LG) 交错排列，体现了其联合生成特性。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在多个模拟长视野操作基准上评估 BagelVLA，包括 <strong>CALVIN</strong>（多任务、语言指令操控）、<strong>LIBERO</strong>（长视野、基于物体的任务）以及 <strong>Language-Conditioned Meta-World</strong>（复杂机器人操作）。实验平台为 PyTorch，使用 RTX 3090/4090 GPU 进行训练和评估。基线方法包括：1) <strong>分层方法</strong>：如 VIMA 和基于 LLM 的规划器 + 低级策略；2) <strong>端到端 VLMs</strong>：如 RT-2、Gato；3) <strong>其他 VLA 方法</strong>：如 Actionable Models。</p>
<p><strong>关键实验结果</strong>：<br>在 CALVIN 基准上，BagelVLA 在 <strong>D 维评估</strong>（要求按顺序完成一系列子任务）中取得了 <strong>87.5%</strong> 的成功率，显著优于最佳基线 VIMA (<strong>72.1%</strong>) 和 RT-2 (<strong>65.8%</strong>)。在更复杂的 LIBERO 基准（10个长视野任务）上，BagelVLA 的平均任务成功率达到 **81.3%**，比基于 LLM 的分层方法 (<strong>64.7%</strong>) 高出近 17 个百分点。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/bagelvla/main/figures/main_results.png" alt="主要结果对比"></p>
<blockquote>
<p><strong>图3</strong>：在 CALVIN 和 LIBERO 基准上的主要定量结果对比。柱状图清晰显示 BagelVLA 在长视野任务成功率上全面超越现有分层和端到端基线方法。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>研究进行了系统的消融实验以验证各组件贡献：</p>
<ol>
<li><strong>移除交错生成（仅动作）</strong>：模型退化为一个标准的视觉语言动作策略，在 CALVIN 上的成功率下降至 **71.2%**，表明交错生成的子目标 token 对长视野推理至关重要。</li>
<li><strong>移除视觉目标 token（仅语言目标）</strong>：成功率降至 **79.5%**，说明纯语言子目标不足以精确指导涉及复杂空间关系的操作。</li>
<li><strong>移除语言目标 token（仅视觉目标）</strong>：成功率降至 **83.1%**，表明语言描述对于明确子任务语义、避免歧义仍有重要作用。</li>
<li><strong>使用固定而非自回归生成的子目标</strong>：成功率显著下降，证明根据当前状态动态调整子目标是有效的。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/your-repo/bagelvla/main/figures/ablation_study.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图4</strong>：消融实验结果。对比了完整模型与移除各关键组件变体的性能，验证了交错生成机制以及视觉、语言目标 token 的互补必要性。</p>
</blockquote>
<p><strong>定性分析</strong>：<br>论文展示了模型在执行任务过程中生成的视觉-语言子目标序列。例如，在“打开抽屉，然后把杯子放进去”的任务中，模型会先生成“抓住抽屉把手”的子目标，执行后，再生成“拿起杯子”的子目标，最后生成“将杯子移动到抽屉内”的子目标。这些子目标在每一步都根据最新观察进行更新，展示了其动态规划能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出交错式 VLA 生成范式</strong>：首次将长视野操作形式化为一个视觉、语言、动作 token 的交错序列生成问题，实现了规划与执行在细粒度时间步上的统一。</li>
<li><strong>设计 BagelVLA 模型</strong>：构建了一个基于多模态 Transformer 的端到端架构，能够联合输出即时动作和未来子目标，并通过缓存机制实现目标驱动的连续控制。</li>
<li><strong>实证有效性</strong>：在多个具有挑战性的长视野操作基准上取得了最先进的性能，并通过消融实验充分验证了所提机制的必要性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前方法仍依赖于高质量、多样化的语言指令标注数据进行训练。在指令非常模糊或存在多重解释的极端情况下，模型的性能可能会下降。此外，研究主要在模拟环境中进行，其向真实世界机器人迁移的鲁棒性有待进一步验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>统一表示的力量</strong>：这项工作展示了将不同模态（感知、语言、控制）统一到单一序列生成框架中的潜力，为构建更通用的具身智能体提供了思路。</li>
<li><strong>动态内部目标</strong>：模型生成的内部视觉-语言子目标可以看作是一种可解释的“内部思维链”，这为理解和调试复杂机器人策略提供了新途径。</li>
<li><strong>扩展至更复杂场景</strong>：未来的工作可以探索将这种交错生成范式应用于需要工具使用、多智能体协作或更长规划视野（数百步）的更复杂任务中。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出BagelVLA框架，旨在解决机器人在复杂长时程操作任务中动作序列规划困难的核心问题。其关键技术是交错式视觉-语言-动作生成，通过大模型将高层次指令分解为可执行的子任务序列，并采用规划-执行-重规划的闭环机制。实验表明，该方法在多个机器人操作基准测试中显著优于现有方法，任务成功率平均提升超过20%，有效验证了其处理长视野、多步骤任务的鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.09849" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>