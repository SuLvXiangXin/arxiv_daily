<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.20503" target="_blank" rel="noreferrer">2505.20503</a></span>
        <span>作者: Lisondra, Matthew, Benhabib, Beno, Nejat, Goldie</span>
        <span>日期: 2025/05/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>移动服务机器人（如家庭助手、酒店送货机器人、医院物流机器人）需要在非结构化的动态人类环境中长期自主运行，完成导航、操作、人机交互等复杂任务。传统方法通常依赖手工设计的规则、专门的感知模型和精心策划的符号规划器，导致系统脆弱、泛化能力差，且难以适应新环境或理解自然语言指令。近年来，大规模预训练的基础模型（Foundation Models, FMs），特别是大语言模型（LLMs）和视觉语言模型（VLMs），在开放世界的理解和推理方面展现出惊人能力，为打破上述局限提供了新可能。本文旨在系统性地回顾如何将具身AI（Embodied AI）与基础模型相结合，以赋能移动服务机器人。核心思路是：梳理并分类基础模型在移动服务机器人感知、规划、控制等核心环节的应用范式，分析当前的技术挑战，并指明未来的研究方向。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文并非提出一种具体的新算法，而是构建了一个系统性的分析框架来回顾和分类该交叉领域的研究。其“方法”体现在对现有工作的梳理逻辑和分类体系上。</p>
<p><strong>整体框架与分类法</strong>：综述提出了一个多层次的分析框架。首先，根据基础模型的类型进行分类：<strong>大语言模型（LLMs）</strong>、<strong>视觉语言模型（VLMs）</strong> 和<strong>视觉基础模型（VFMs）</strong>。其次，根据这些模型在机器人系统中的<strong>功能角色</strong>进行分类：作为<strong>任务规划器</strong>（将高层指令分解为可执行子任务序列）、作为<strong>场景理解与感知模块</strong>（从多模态输入中提取语义信息）、作为<strong>低级策略生成器</strong>（直接输出动作或轨迹）。最后，映射到具体的<strong>机器人任务</strong>，如导航（VLN，主动导航）、操作（抓取、摆放）、人机交互（对话、遵循指令）。</p>
<p><img src="https://img1.wkimg.com/0124/2024/12/11/15/1733912359879.png" alt="具身基础模型在移动服务机器人中的应用框架"></p>
<blockquote>
<p><strong>图1</strong>：基础模型在移动服务机器人中的应用概览图。展示了从人类自然语言指令输入，到基础模型在任务规划、场景理解和动作控制等环节发挥作用，最终机器人执行任务并与环境交互的完整闭环。</p>
</blockquote>
<p><strong>核心模块与作用分析</strong>：</p>
<ol>
<li><strong>基础模型作为任务规划器</strong>：这是最主流的应用方式。LLMs/VLMs接收自然语言指令和可能的环境上下文（如场景描述、物体列表），利用其常识和推理能力，生成结构化的任务计划（如“先去厨房，然后拿起杯子，最后送到客厅”）。关键技术细节包括<strong>提示工程</strong>（设计思维链、提供范例）、<strong>上下文学习</strong>（在提示中提供相关知识和环境信息）以及<strong>与符号规划器或技能库的接口</strong>（将LLM输出的抽象计划转化为机器人可执行的原语）。</li>
<li><strong>基础模型作为感知与场景理解器</strong>：VLMs/VFMs被用于替代或增强传统感知模块。具体应用包括：<strong>开放词汇物体检测与定位</strong>（根据语言描述找到物体）、<strong>场景描述与问答</strong>（为规划器提供语义信息）、** affordance 预测**（推断物体的可操作属性）。这通常涉及将机器人传感器（RGB-D相机）数据与语言查询输入VLM，获取像素级或区域级的语义标签。</li>
<li><strong>基础模型作为低级策略</strong>：一种更端到端的方式，让基础模型（通常是VLM）直接根据当前视觉观测和指令生成底层动作命令（如关节速度、末端执行器位姿）。这通常需要<strong>视觉动作表示</strong>（如将动作编码为图像上的边界框或轨迹）和<strong>行为克隆</strong>等技术进行微调。</li>
</ol>
<p><strong>创新视角</strong>：本文的系统性综述本身提供了一个清晰的“全景图”，其创新性体现在提出的<strong>分类法</strong>和对<strong>“基础模型-机器人功能-具体任务”</strong> 三者关联的深入剖析上。它明确指出，当前研究并非简单套用基础模型，而是致力于解决<strong>如何将基础模型的开放世界知识安全、可靠地“接地”到物理机器人行动中</strong>这一核心挑战。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>作为一篇综述论文，本文并未进行新的实验，而是对现有文献中的实验结果进行了系统的汇总、比较和分析。</p>
<p><strong>评估基准与数据集</strong>：综述列举了该领域常用的仿真与真实世界测试平台。仿真平台包括<strong>Habitat</strong>、<strong>iTHOR</strong>、<strong>AI2-THOR</strong>、<strong>BEHAVIOR</strong> 等，用于评估导航和操作任务。数据集包括<strong>ALFRED</strong>（遵循语言指令的日常任务）、<strong>VLN-CE</strong>（连续环境中的视觉语言导航）、<strong>EpicKitchen</strong>（第一人称视角操作）等。真实世界评估通常在定制化的实验室或半结构化环境中进行。</p>
<p><strong>性能对比与关键发现</strong>：通过对比融入基础模型的方法与传统或早期基于学习的方法，综述总结了以下关键趋势：</p>
<ol>
<li><strong>任务规划与导航</strong>：集成LLM/VLM的规划器在<strong>长视野、复杂指令的理解和分解</strong>上显著优于传统方法，在ALFRED等任务上的成功率有两位数百分比的提升。例如，一些方法将任务完成率从<del>20%提升至</del>30-40%。然而，成功率绝对值仍不高，主要失败于<strong>规划序列的可行性</strong>和<strong>低级执行错误</strong>。</li>
<li><strong>开放词汇感知与操作</strong>：基于VLM的开放词汇检测系统能够识别训练集未见过的物体类别，极大增强了机器人在新环境中的适应性。在“寻找某个特定物体”的导航任务或“抓取某个描述性物体”的操作任务中，成功率相比封闭词汇集方法有显著提高。</li>
<li><strong>仿真与现实的差距</strong>：在仿真中表现良好的方法，在真实机器人部署时面临巨大挑战，包括感知噪声、动作执行误差、计算延迟以及基础模型输出（如物体边界框）的不稳定性。</li>
</ol>
<p><strong>消融分析与组件贡献</strong>：综述通过分析不同工作，间接“消融”了各组件的重要性：</p>
<ul>
<li><strong>提示工程与上下文</strong>：提供详细的环境上下文（如物体列表、房间布局）和思维链提示，能极大提升LLM规划的质量和可靠性。</li>
<li><strong>闭环反馈</strong>：让基础模型根据执行结果（成功/失败）重新规划，比开环规划能更有效地从错误中恢复。</li>
<li><strong>技能库与低级控制器</strong>：一个鲁棒的低级技能库（如“导航到某点”、“抓取”）是连接高层抽象规划和物理执行的关键，其性能瓶颈会制约整个系统的上限。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>系统性分类</strong>：首次对“具身基础模型”在移动服务机器人领域的应用进行了全面、结构化的回顾，提出了清晰的多维度分类法（模型类型、功能角色、具体任务），为该新兴领域建立了知识图谱。</li>
<li><strong>挑战剖析</strong>：明确归纳了当前面临的核心技术挑战，包括：<strong>基础模型输出的不可靠性与幻觉</strong>、<strong>多模态 grounding 的模糊性</strong>（如何将语言描述精准对应到物理实体）、<strong>长任务序列中错误的积累</strong>、<strong>计算效率与实时性</strong>、以及<strong>安全与伦理风险</strong>。</li>
<li><strong>未来方向指引</strong>：指出了富有前景的研究方向，如开发<strong>机器人专属的基础模型</strong>（在具身数据上训练或微调）、探索<strong>更高效的表示与压缩方法</strong>以降低计算负载、设计<strong>更好的仿真到真实的迁移策略</strong>、以及建立<strong>更全面的安全评估框架</strong>。</li>
</ol>
<p><strong>局限性（论文自身提及）</strong>：作为一篇综述，其内容受限于截至投稿时已发表的工作。该领域发展迅猛，新的模型（如多模态大模型）和应用范式不断涌现。此外，综述主要关注学术研究，对工业界的大规模部署挑战涉及相对较少。</p>
<p><strong>对后续研究的启示</strong>：本文为研究者提供了一个“路线图”。它表明，单纯调用基础模型API不足以构建可靠的机器人系统。未来的工作需要更深入地关注 <strong>“中间层”问题</strong>——如何设计稳健的接口、状态表示和反馈机制，将基础模型的认知能力与机器人的物理控制能力无缝、安全地融合。同时，构建更复杂、更逼真的评估基准以推动整个领域向前发展也至关重要。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文系统综述了基础模型在移动服务机器人具身AI中的应用。核心问题是解决传统机器人系统在开放、动态环境中感知、推理与交互的局限性。关键技术方法聚焦于视觉、语言及多模态基础模型（如ViT、LLM、VLM）与机器人系统的集成，通过零样本学习、上下文理解等方式提升泛化能力。综述指出，当前研究虽在任务规划、人机交互等方面取得进展，但在实时性、安全性与硬件适配方面仍面临挑战，需进一步探索高效轻量化的部署方案。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.20503" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>