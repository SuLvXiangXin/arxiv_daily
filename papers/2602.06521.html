<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.06521" target="_blank" rel="noreferrer">2602.06521</a></span>
        <span>作者: jia, Feiyang, Liu, Lin, Song, Ziying, Jia, Caiyan, Ye, Hangjun, Hao, Xiaoshuai, Chen, Long</span>
        <span>日期: 2026/02/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>端到端（E2E）自动驾驶方法在将传感器输入映射到控制信号方面取得了成功，但通常缺乏长时程推理能力，且难以理解其动作的因果后果。为此，将视觉-语言-动作（VLA）模型与世界模型（WM）相结合成为一个有前景的方向。然而，现有方法未能有效统一未来场景演化和动作规划，主要存在两类局限：一是<strong>解耦交互</strong>方法将世界模型视为外部模拟器，导致VLA无法内化物理规律；二是<strong>特征共享</strong>方法虽共享表示，但缺乏动作条件化的因果推理，限制了其反事实想象和长时程规划能力。</p>
<p>本文针对现有方法中世界模型与VLA规划器耦合松散、潜在状态共享不足，导致视觉想象难以直接影响动作决策这一核心痛点，提出了在表示层面紧密集成VLA与世界模型的新视角。其核心思路是：通过特征级共享，将世界模型的潜在状态作为VLA规划器的核心决策状态，并在潜在空间中进行完全的动作条件化世界建模，从而实现可控的“如果-那么”推理，使规划器能够评估候选动作对未来场景演化的影响。</p>
<h2 id="方法详解">方法详解</h2>
<p>DriveWorld-VLA旨在通过一个三阶段渐进式训练范式，在统一架构中紧密集成VLA与世界模型，逐步对齐表示学习、动作可控性和后果感知决策。</p>
<p><img src="https://arxiv.org/html/2602.06521v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：DriveWorld-VLA整体流程。采用三阶段渐进训练方案：第1阶段从共享潜在表示联合学习未来BEV想象和动作预测；第2阶段使生成分支以未来动作为条件，实现可控想象；第3阶段形成闭环：先预测动作，再想象其导致的未来，最后利用奖励反馈优化动作预测。</p>
</blockquote>
<p><strong>整体流程与输入输出</strong>：模型支持多模态输入，包括多视角图像 $\mathcal{I}<em>{t}$、文本提示 $\mathcal{T}</em>{t}$、历史动作 $\mathcal{A}<em>{t-1}$ 和BEV表示 $\mathcal{B}</em>{t}$。这些输入被独立分词后送入视觉语言模型（VLM），VLM聚合信息并产生隐藏状态序列 $\mathcal{H}<em>{t}$，该状态被提取为<strong>共享潜在表示</strong>，同时用于未来想象和动作预测。未来想象在BEV空间中进行，输出未来BEV状态 $\mathcal{B}</em>{t+\Delta t}$；动作预测则输出未来动作序列 $\mathcal{A}_{t+\Delta t}^{\prime}$。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>特征级共享与联合训练（第1阶段）</strong>：核心创新是利用VLM的隐藏状态 $\mathcal{H}<em>{t}$ 作为世界模型和VLA规划器的共享潜在空间。一个去噪器以 $\mathcal{H}</em>{t}$ 和当前BEV状态 $\mathcal{B}<em>{t}$ 为输入，预测未来BEV状态。同时，一个轻量级动作解码器以相同潜在状态为输入，预测未来动作。此阶段通过BEV语义解码损失 $\mathcal{L}</em>{seg}$ 和动作模仿损失 $\mathcal{L}_{act}$ 进行联合监督，促使世界模型知识迁移至VLA。</li>
<li><strong>动作条件化可控想象（第2阶段）</strong>：为使模型具备基于前瞻动作进行想象的能力，本阶段专注于动作条件化未来想象的微调。利用第1阶段训练好的编码器，从未来真实图像中提取对应的“真实”BEV潜在表示 $\mathcal{B}_{t+\Delta t}^{\prime}$ 作为监督目标。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.06521v1/x3.png" alt="动作条件化去噪器结构"></p>
<blockquote>
<p><strong>图3</strong>：动作条件化流匹配去噪器结构。该去噪器基于扩散Transformer（DiT）架构，以BEV状态 $\mathcal{B}<em>{t}^{\prime}$ 和真实未来动作 $\mathcal{A}</em>{t+\Delta t}$ 为条件，通过流匹配过程学习去噪，生成未来的BEV潜在状态。</p>
</blockquote>
<p>去噪器的第二个分支采用DiT架构，学习一个以 $\mathcal{B}<em>{t}^{\prime}$ 和真实未来动作 $\mathcal{A}</em>{t+\Delta t}$ 为条件的流匹配去噪过程（公式8），其目标是匹配从噪声到真实BEV潜在表示的向量场。这使得模型能够根据任意给定的动作序列，在特征层面进行可控的未来演化想象。<br>3.  <strong>未来引导评估与优化（第3阶段）</strong>：此阶段建立动作预测与未来想象之间的闭环。模型首先预测动作 $\mathcal{A}<em>{t+\Delta t}^{\prime}$，然后用其条件化第2阶段的DiT分支，通过欧拉采样生成动作条件化的未来想象 $\mathcal{B}</em>{t+\Delta t}^{\prime}$（公式9）。一个学习的奖励函数 $\mathcal{R}$ 通过比较动作条件化想象 $\mathcal{B}<em>{t+\Delta t}^{\prime}$ 与模型自身预测的未来 $\mathcal{B}</em>{t+\Delta t}$ 的一致性，为预测轨迹分配标量奖励 $\hat{r}<em>{t+\Delta t}$（公式10）。训练时，动作损失 $\mathcal{L}</em>{act}^{\prime}$ 由该预测奖励加权（公式11），从而优先优化那些能带来更有利想象结果的轨迹，实现后果感知的动作优化。</p>
<p><strong>创新点</strong>：与现有方法相比，DriveWorld-VLA的核心创新体现在：1) <strong>深度特征共享</strong>：将VLM的潜在状态作为世界建模和规划的统一空间，使规划器能内化环境动态；2) <strong>潜在空间动作条件化推理</strong>：在BEV特征层面（而非像素层面）进行基于扩散模型的、动作可控的未来滚推，支持高效的“如果-那么”因果推理；3) <strong>渐进式闭环训练</strong>：通过三阶段训练逐步实现表示对齐、可控想象和奖励驱动的策略优化，确保了联合优化的稳定性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：模型在NAVSIMv1（闭环规划基准）、NAVSIMv2（闭环伪仿真基准）和nuScenes（开环预测基准）上进行评估。对比的基线方法涵盖了E2E、世界模型和VLA三大类，包括TransFuser、DiffusionDrive、LAW、WoTE、Epona、DriveVLA-W0、HERMES-p等。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>NAVSIMv1</strong>：DriveWorld-VLA取得了91.3的PDMS分数，全面超越所有基线（表1）。尤其在无碰撞（NC，99.1）和自车进度（EP，85.9）指标上表现出色，证明了其在安全约束和前进效率上的优势。</li>
<li><strong>NAVSIMv2</strong>：模型取得了86.8的EPDMS分数，同样达到最优（表2）。在可行驶区域合规（DAC，99.1）、行驶方向合规（DDC，99.6）和车道保持（LK，97.0）等指标上表现卓越。</li>
<li><strong>nuScenes</strong>：在3秒规划任务中，DriveWorld-VLA取得了0.61米的平均L2误差和0.16%的平均碰撞率（CR），优于大多数方法（表3）。值得注意的是，在未使用自车状态信息的情况下，其碰撞率显著低于HERMES-p（0.16% vs 0.32%），展示了出色的短时程规划安全性。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.06521v1/x1.png" alt="耦合策略对比与性能"></p>
<blockquote>
<p><strong>图1</strong>：VLA与世界模型耦合策略对比及性能。(a) 解耦交互，(b) 特征共享，(c) 本文的DriveWorld-VLA。(d) DriveWorld-VLA在三个基准上均取得了SOTA性能。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>表4展示了渐进训练各阶段的贡献。在NAVSIMv1上，仅使用第1阶段（联合训练）PDMS为87.6，加入第2阶段（动作可控）后提升至89.5，最终第3阶段（闭环优化）达到91.3。在nuScenes上，碰撞率也随阶段推进从0.25%逐步降至0.16%。这验证了三阶段训练每个环节的必要性和累积效益。<br>表5对比了渐进式与非渐进式（同时训练第2、3阶段）策略，结果显示渐进式训练显著优于非渐进式（PDMS 91.3 vs 83.6），证明了所提训练范式的稳定性优势。<br>表6对VLM策略进行了消融，表明在联合训练阶段不冻结VLM参数并采用与RecogDrive对齐的预训练策略，能带来最佳性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>紧密耦合的DriveWorld-VLA框架</strong>，其中世界模型作为连接动作与前瞻想象的推理引擎，实现了生成建模与决策规划在单一架构内的统一。</li>
<li>引入了<strong>特征级共享机制</strong>，利用VLM隐藏状态作为世界想象和动作预测的共享潜在空间，促进了物理规律和环境动态的内化。</li>
<li>开发了<strong>动作条件化的“如果-那么”推理</strong>能力，通过在潜在空间中进行基于扩散模型的、可控的未来滚推，使智能体能够评估不同动作的长期后果，从而实现主动的、后果感知的决策。</li>
</ol>
<p><strong>局限性</strong>：论文在nuScenes的实验部分明确指出，为了公平比较，<strong>未使用自车状态信息</strong>。这虽然证明了模型在基础感知和规划上的能力，但也意味着在现实部署中可能需要整合此类信息以进一步提升性能。</p>
<p><strong>后续启示</strong>：本文工作表明，将世界模型深度整合为智能体核心推理引擎，并在<strong>特征层面而非像素层面</strong>进行因果滚推，是提升自动驾驶系统长时程推理和主动规划能力的关键方向。其渐进式训练范式也为安全、稳定地联合优化复杂多模态系统提供了可借鉴的方案。未来研究可探索如何将更精细的物理约束或更高效的序列模型融入此类统一潜在空间世界模型中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DriveWorld-VLA框架，旨在解决端到端自动驾驶中世界模型与视觉-语言-动作规划器耦合松散、视觉想象力难以有效指导决策的问题。其核心方法是在统一的潜在空间中整合世界建模与规划，将世界模型的潜在状态作为规划器的决策变量，实现动作条件的特征级可控想象，避免像素级推演。实验表明，该框架在多个基准测试中取得最先进性能，如在NAVSIMv1上达到91.3 PDMS，在nuScenes上实现0.16的3秒平均碰撞率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.06521" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>