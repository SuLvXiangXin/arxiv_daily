<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.14135" target="_blank" rel="noreferrer">2506.14135</a></span>
        <span>作者: Yebin Liu Team</span>
        <span>日期: 2025-06-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于视觉的机器人操作方法主要遵循两种范式：一是“视觉到动作”（V-A）范式，直接从RGB观测预测动作序列，依赖隐式场景理解，缺乏对三维世界的显式建模；二是“视觉到3D到动作”（V-3D-A）范式，利用点云、体素等中间三维表示进行几何推理，提升了空间关系捕捉能力，但通常需要难以获取的深度信息或3D数据。这两种范式的共同局限在于，它们都未能捕捉场景几何的时序演化，导致静态的场景理解与动态的动作生成之间存在固有错配。</p>
<p>针对这一挑战，新兴的“视觉到4D到动作”（V-4D-A）范式旨在通过为三维表示增加运动信息来捕捉场景的时序演变，从而更直观地指导动作规划。然而，现有的一些4D表示方法（如ManiGaussian和GWM）主要利用视频序列特性，或将未来状态仅作为强化学习中的体积先验，其隐式且保真度较低的高斯表示限制了动作推理的有效性。</p>
<p>本文针对动态场景建模与动作推理脱节这一痛点，提出了一种更显式的4D表示方法。核心思路是：扩展3D高斯泼溅（3DGS），为每个高斯点引入可学习的运动属性，构建一个统一的高斯动作场（GAF），以直接建模高保真当前状态与未来状态之间的时序变换，从而在准确的动态场景理解基础上实现可靠的动作推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>GAF方法的核心是构建一个统一的时空表示——高斯动作场，它通过一个连续函数，为每个高斯点关联几何属性和运动动态。GAF产生三种相互关联的输出：表示当前场景的“当前高斯”、通过施加预测运动得到的“未来高斯”，以及通过当前与未来机械臂相关高斯点云配准计算出的“初始动作”。</p>
<p><img src="https://arxiv.org/html/2506.14135v4/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：GAF重建概览。给定稀疏多视角图像，一个视觉Transformer提取混合场景特征，随后由三个解码头分别预测高斯位置、运动位移和外观参数。</p>
</blockquote>
<p><strong>动态高斯重建</strong>：GAF采用与位姿无关的架构，仅从两个未标定相机位姿的输入视图直接重建运动增强的高斯表示。一个共享权重的视觉Transformer（ViT）通过跨视角注意力提取特征。随后，采用基于DPT架构的解耦双头设计：高斯中心头预测高斯点位置，高斯参数头预测颜色、不透明度、旋转和尺度等外观参数。此外，引入一个运动预测头，遵循与中心头相同的架构，预测每个高斯点在未来的时间间隔内的位移向量。将预测的位移加到当前位置上，即得到未来高斯的位置。当前和未来高斯均可通过可微分的alpha混合渲染生成新视角图像，从而允许仅使用RGB视频帧进行监督。训练损失结合了当前帧和未来帧的LPIPS损失和MSE损失，以联合优化运动增强的高斯重建。</p>
<p><strong>初始动作计算</strong>：由于关注机械臂操作，注意力集中在机械臂末端执行器本身的运动。根据对应的夹爪位姿，从当前状态和未来状态的高斯中提取与操作相关的点云，然后使用迭代最近点算法估计两者之间的一个刚性变换矩阵。该矩阵描述了末端执行器在一段时间间隔内的运动，通过插值可得到一个离散时间步长的变换序列，构成初始动作。</p>
<p><img src="https://arxiv.org/html/2506.14135v4/x3.png" alt="操作流程"></p>
<blockquote>
<p><strong>图3</strong>：机器人操作流程。GAF的输出作为条件，输入到一个动作-视觉对齐的去噪框架中，以生成可执行的动作。此过程迭代重复，直至任务完成。</p>
</blockquote>
<p><strong>动作细化与操作流程</strong>：由于GAF模块仅使用视觉数据训练，缺乏真实动作监督，预测的初始动作仅在视觉上合理，可能缺乏物理可行性。因此，引入一个动作-视觉对齐的去噪框架进行细化。该框架的核心思想是，对于去噪过程中的每个候选动作，将其对应的夹爪位姿（通过初始动作计算得到）利用已知相机参数渲染到当前多视角RGB图像上，形成“可操作的多视角RGB引导”。这种视觉引导整合了三维观测和时序预测的动作，用于约束扩散模型的去噪过程，优化动作序列的物理一致性。细化后的动作序列通过逆运动学转换为关节指令执行。环境更新后，收集新的观测，开启下一个控制循环，形成闭环框架。</p>
<p>与现有方法相比，GAF的创新点在于：1) 将运动属性作为可学习参数直接嵌入高斯表示，实现了对场景动态的显式、统一建模；2) 通过当前与未来高斯的点云配准直接导出初始动作，建立了动态感知与动作生成间的直接桥梁；3) 提出了动作-视觉对齐的去噪框架，利用GAF自身生成的视觉线索来细化动作，提升了物理可行性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境（RLBench）和真实世界中进行。在RLBench的9个任务上评估，对比了V-A范式的代表方法Diffusion Policy (DP)、V-3D-A范式的代表方法Act3D，以及同样基于高斯世界模型的ManiGaussian。</p>
<p><img src="https://arxiv.org/html/2506.14135v4/x1.png" alt="结果对比表"> <em>注：此处应插入论文中的表I，但用户提供的链接为图1。根据正文，关键数据在表I中。为遵循指令基于原文，此处用文字总结。</em><br>关键实验结果：GAF在操作任务上取得了平均60.4%的成功率，相比DP提升了15.7%，相比Act3D提升了7.3%，相比ManiGaussian提升了10.3%。这验证了4D动态场景表示对提升操作成功率的作用。</p>
<p><img src="https://arxiv.org/html/2506.14135v4/x4.png" alt="重建与预测对比"></p>
<blockquote>
<p><strong>图4</strong>：当前场景重建和未来场景预测的新视角合成对比。上图（ManiGaussian）显示纹理模糊、几何细节不完整；下图（Ours）保留了精细的几何结构。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14135v4/x1.png" alt="定量结果表"> <em>注：此处应插入论文中的表II。</em><br>场景重建与预测质量：在重建质量上，GAF显著优于ManiGaussian。定量指标显示，在当前场景重建中，GAF平均PSNR提升+11.5385 dB，SSIM提升+0.3864，LPIPS降低0.5574；在未来状态预测中，平均PSNR提升+10.5311 dB，SSIM提升+0.3856，LPIPS降低0.5757。定性结果（图4）也显示GAF能保留更精细的几何结构和纹理。</p>
<p><img src="https://arxiv.org/html/2506.14135v4/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：去噪框架消融实验。上图显示未使用动作去噪时，交互阶段因重建误差导致物理上不可行的机器人-物体关系（如穿透）；下图显示使用动作去噪后成功完成任务。</p>
</blockquote>
<p><strong>消融实验</strong>：1) <strong>GAF模块的贡献</strong>：移除GAF模块（即不使用多视角渲染和初始动作先验，直接从图像用扩散模型预测动作）会使平均成功率下降9.1%，证明了GAF的4D表示对提升动作预测准确性的关键作用。2) <strong>去噪框架的贡献</strong>：如图5所示，没有去噪框架时，方法在接触和交互阶段容易因遮挡导致的重建误差而产生物理上不可行的动作，从而经常无法完成整个任务。去噪框架有效纠正了这些错误。</p>
<p><img src="https://arxiv.org/html/2506.14135v4/x6.png" alt="空间泛化"></p>
<blockquote>
<p><strong>图6</strong>：空间泛化能力测试结果。红点和蓝点分别代表成功和失败，紫色星号为训练演示位置。相比基线DP，我们的方法在 workspace 边界和角落表现更优。</p>
</blockquote>
<p><strong>空间泛化与多任务评估</strong>：在空间泛化测试中（图6），GAF在物体被放置在工作空间边界和角落时仍能保持较高成功率，而DP在这些区域表现显著下降。在多任务联合训练评估中（表III），GAF在四个任务上联合训练后平均成功率下降11.25%，而DP下降21%，表明GAF具有更稳健的多任务学习能力。</p>
<p><img src="https://arxiv.org/html/2506.14135v4/x7.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图7</strong>：真实世界实验中，由当前高斯和未来高斯渲染的图像。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14135v4/x8.png" alt="真实世界设置"></p>
<blockquote>
<p><strong>图8</strong>：真实世界实验设置，包括 Franka 机械臂、Panda 手，以及两个静态相机和一个腕部相机。</p>
</blockquote>
<p><strong>真实世界部署</strong>：在5个真实世界任务中（使用3个RGB相机），GAF取得了可观的成绩，例如“Push Button”任务达到10/10的成功率，“Place Apple”任务为3/5。失败案例主要源于视觉遮挡和快速动态，证明了方法在真实噪声环境下的可行性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了高斯动作场作为一种新颖的V-4D-A方法，通过为3DGS引入可学习运动属性，统一了动态场景演化与面向未来的动作预测。2) 设计了一个动作-视觉对齐的去噪框架，利用GAF自身产生的统一表示来细化动作，提升了动作预测的物理准确性和鲁棒性。3) 在模拟和真实机器人操作任务上进行了全面验证，在场景重建质量和操作成功率方面均取得了显著的性能提升。</p>
<p>论文自身提到的局限性包括：在存在严重视觉遮挡和非常快速动态的场景中，性能会下降；当前方法主要关注末端执行器的刚性运动，对非刚性物体或更复杂交互的动态建模能力有待探索。</p>
<p>本文的启示在于：显式地建模场景的动态变化并将其与动作生成紧密结合，是提升视觉-动作映射性能的有效途径。GAF展示了将高效、高保真的神经渲染技术（3DGS）用于构建机器人世界模型的潜力，其“感知-预测-行动”的闭环框架为处理动态和不确定性提供了新思路。未来研究可探索如何将更复杂的物理约束融入此类4D表示中，并扩展其对多样化物体和交互类型的建模能力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中动态场景感知与动作生成不匹配的核心问题，提出GAF（高斯动作场）这一4D动态世界模型。该方法扩展3D高斯泼溅，通过引入可学习的运动属性，实现了对场景几何时变与机器人动作的联合4D建模。关键创新包括利用高斯动作场同步输出场景重建、未来帧预测和初始动作估计，并采用动作-视觉对齐的去噪框架优化动作精度。实验表明，GAF在重建质量上显著优于基线（PSNR +11.5385 dB，SSIM +0.3864，LPIPS -0.5574），并将机器人操作任务的平均成功率提升了7.3%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.14135" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>