<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.12346" target="_blank" rel="noreferrer">2510.12346</a></span>
        <span>作者: Li, Bingquan, Wang, Ning, Zhang, Tianwei, He, Zhicheng, Wu, Yucong</span>
        <span>日期: 2025/10/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人双足行走技术已在平坦地面场景下得到显著发展。然而，楼梯作为一种标准但高风险的人造结构，具有台阶高度差、接触面狭窄和防跌落约束，对机器人通过复杂几何体的全链条能力提出了更高要求。现有主流方法依赖强大的自平衡和高带宽跟踪稳定器，通过快速全身姿态调节来“蛮力”应对爬楼梯过程中的几何干扰。这些方法缺乏对环境几何和落脚区域的明确感知，通常导致过于保守的步态和相当大的脚步不确定性，使得在长楼梯、变化光照或磨损台阶表面下难以实现可靠、持续的攀爬。</p>
<p>感知方法在楼梯环境中面临特定挑战：台阶小而边缘尖锐，深度测量易受低纹理、黑色吸光材料和视角变化影响，导致法线估计不稳定和平面碎片化；双足行走时的机体振动和相机卷帘快门效应会放大噪声并导致传感数据丢失，使得单帧平面拟合对阈值选择高度敏感；里程计与建图的耦合会引起局部漂移并在多帧融合中累积，降低台阶边界和可行多边形的时空一致性。此外，感知-规划-控制回路必须实时运行。因此，如何在感知噪声、状态不确定性和实时计算约束下，鲁棒地生成适应楼梯的可行区域表示，并高效地将其与全身运动生成耦合，是实现可靠爬楼梯的关键挑战。</p>
<p>本文针对上述痛点，提出了一个用于人形机器人爬楼梯的感知运动框架。核心思路是：通过多传感器（LiDAR、RGB-D相机和IMU）融合，构建实时的多边形楼梯平面语义地图，并基于这些多边形平面段进行脚步规划，最终实现20-30 Hz的全身运动规划输出。</p>
<h2 id="方法详解">方法详解</h2>
<p>提出的感知运动框架整体流程如图2所示。系统输入包括来自RGB-D相机的深度图像、来自LiDAR-惯性里程计（LIO）的初始位姿估计，以及通过前向运动学获得的运动学位姿估计。输出为机器人的全身运动指令。框架核心包含三个模块：多传感器融合状态估计、落脚点生成和脚步规划。</p>
<p><img src="https://arxiv.org/html/2510.12346v1/fig/fig2_wn04.png" alt="系统框架"></p>
<blockquote>
<p><strong>图2</strong>：系统框架。系统集成了关节记录器、深度传感和LIO估计器。机器人姿态通过融合前向运动学和LIO获得，深度图像提供用于落脚点生成的多边形地图。基于这些落脚点，脚步和全身运动规划实现了稳定的楼梯攀爬。</p>
</blockquote>
<p><strong>多传感器融合状态估计</strong>：该模块旨在集成本体感知和LIO，实现平滑准确的机器人位姿估计。采用线性卡尔曼滤波器融合来自IMU、关节编码器和脚部接触状态的多模态数据，统一估计机器人基座的位置、速度、姿态和脚部位置。同时，采用松耦合方案将LIO的位姿估计集成到融合框架中。具体地，前向运动学基于机器人状态（包含CoM位置速度及8个脚部接触点位置）和CoM线加速度输入，建立状态空间方程。观测向量包含脚部接触点相对于CoM的位置、速度及其高度。通过卡尔曼滤波实时估计躯干状态。对于里程计融合，已知世界系W、机体系B、LIO估计 $^W\mathbf{T}<em>L$ 和机器人本体里程计 $^W\mathbf{T}</em>{\text{base}}$，利用预知的LiDAR外参 $^B\mathbf{T}<em>{\text{lidar}}$，将LIO转换到机器人CoM。位置部分采用互补滤波融合本体和LIO的位置估计，姿态部分通过指数映射形式进行融合，最终获得精确平滑的机器人位姿估计 $^W\mathbf{T}</em>{\text{base}}^{\text{(fused)}}$。</p>
<p><strong>落脚点生成</strong>：此模块基于融合里程计和RGB-D深度图像构建楼梯的多边形表示地图，并从中提取安全的落脚点。处理流程（对应图3逻辑）首先对深度图像进行各向异性扩散滤波以减少噪声并保留边缘。随后，根据相机内参从深度像素计算3D点，并通过邻域差分向量的叉积归一化直接计算法线图，无需显式生成密集点云。在提取的多边形区域内，应用RANSAC算法进行平面拟合。</p>
<p><img src="https://arxiv.org/html/2510.12346v1/x1.png" alt="规划逻辑"></p>
<blockquote>
<p><strong>图3</strong>：基于Polygmap的脚部运动规划逻辑。展示了从深度图像处理、法线与轮廓提取、多边形拟合，到构建语义地图、过滤腐蚀生成候选区域，最终进行落脚点选择和轨迹规划的完整流程。</p>
</blockquote>
<p>获得多边形集合后，为每个多边形计算其顶点在XY平面上投影的2D凸包，并在凸包包围盒内以分辨率 $g_{\text{res}}$ 生成均匀网格点，保留在凸包内的点，其高度赋值为原多边形顶点高度的平均值，从而得到所有多边形的密集点云 $P_{\text{total}}$。接着进行坐标变换和范围过滤，仅保留在机器人机体系XY平面一定距离 $g_{\text{range}}$ 内的点云 $P_{\text{filtered}}$。为提取平面点云的显著高度特征，获取每个网格内高度最大的点集 $P_{\text{max}}$。考虑机器人脚底高度限制，过滤掉过高的点得到 $P_{\text{max_filtered}}$。为去除孤立点和边缘噪声，将 $P_{\text{max_filtered}}$ 按高度分层，并对每一层进行 $N_{\text{erosion}}$ 次腐蚀操作，合并所有层得到最终结构化的点云 $P_{\text{eroded}}$。在 $P_{\text{eroded}}$ 中，选择满足踏步高度条件且距离机器人基座坐标最近的点作为主要落脚点候选 $p^*$，并可选择高于 $p^*$ 的点作为下一步候选 $p^{**}$。同时，对脚底高度附近平面的点云也进行腐蚀处理以提高安全性。最终围绕候选点生成最优落脚点。</p>
<p><strong>脚步规划</strong>：该模块基于实时机器人位姿估计生成双足步态轨迹。输入为估计的机器人位姿 $\mathbf{t}<em>B$，输出为一系列脚步位置、躯干位姿及对应时间的序列 $\mathbf{T}</em>{\text{foot}}^B$。脚部位置由躯干位姿和固定的左右脚偏移量通过绕Z轴旋转得到。为确保步态平滑，脚部在离地和着陆阶段的垂直（Z方向）轨迹采用正弦曲线插值，水平（X方向）轨迹线性插值。为避免脚部重叠，使用旋转矩形模型表示脚的占用空间，并通过检查当前脚步形与上一只脚矩形是否相交来调整步序，若相交则表明轨迹冲突，需调整输入的躯干路径。最终，整个轨迹在固定时间间隔 $\Delta t$ 下离散化。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的创新主要体现在：1) 提出了一个集成的感知-规划框架，首次融合LiDAR与机器人前向运动学实现爬楼梯的实时在线脚步规划；2) 开发了一套从深度图像到多边形语义地图的鲁棒处理流程，并通过凸包、栅格化、分层腐蚀等操作，从多边形地图中稳定地提取符合安全与可达性约束的落脚点，而非直接使用原始点云或网格地图。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台为KUAVO人形机器人（高166cm，重55kg），配备了3D LiDAR（Livox Mid360）、RGB-D相机（Realsense L515）和IMU。所有计算在NVIDIA Orin NX上完成，实现20-30 Hz的全身运动规划。使用了Gazebo仿真环境和真实世界的室内、室外楼梯场景进行实验。对比的基线方法包括：纯本体传感器里程计（odom_base）和纯LiDAR惯性里程计（odom_lidar）。评估指标包括：位姿估计轨迹平滑度、连续爬楼梯步数、平面检测频率以及规划与执行落脚点之间的最大水平/垂直误差。</p>
<p><strong>传感器融合实验</strong>：图4比较了三种位姿估计方法在机器人行走时的轨迹。</p>
<p><img src="https://arxiv.org/html/2510.12346v1/x2.png" alt="位姿估计对比"></p>
<blockquote>
<p><strong>图4</strong>：位置估计对比分析：本体基、LIO和提出的融合方法。融合结果（odom_fused）结合了LIO的全局精度和本体观测的平滑约束，在所有轴上产生了更稳定的轨迹，有效抑制了LIO的高频噪声并显著减少了累积漂移。</p>
</blockquote>
<p><strong>Gazebo仿真结果</strong>：表I展示了五组仿真数据（三组10级楼梯，两组4级楼梯）。结果显示，爬完最长楼梯耗时35秒，平均每步约3.4-3.7秒。从检测到执行的最大脚步误差为12.4 mm，远小于可用台阶宽度，满足安全裕度要求。平面检测频率在20-29 Hz之间。</p>
<p><img src="https://arxiv.org/html/2510.12346v1/fig/fig_fangzhen_2.png" alt="仿真实验"></p>
<blockquote>
<p><strong>图5</strong>：所提系统的仿真实验。展示了感知-规划-控制流水线和爬楼梯运动序列。机器人从地面开始逐级攀爬直至到达上层平台，过程平滑，脚步放置准确。</p>
</blockquote>
<p><strong>真实世界爬楼梯评估</strong>：表II报告了五次室内外真实实验数据。设计了两种步态策略：双步（DS，两脚同台阶）和单步（SS，两脚交替上台阶）。实验表明，双步步态最大脚步误差约12mm，单步步态更快但误差增大至24.7mm。平面检测频率保持在20-23 Hz。</p>
<p><img src="https://arxiv.org/html/2510.12346v1/fig/fig_shiwu_1.png" alt="真实实验"></p>
<blockquote>
<p><strong>图6</strong>：真实世界爬楼梯实验。展示了实验设置和感知-规划-控制过程中的关键运动快照。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12346v1/fig/fig_shiwu_2.png" alt="实验结果"></p>
<blockquote>
<p><strong>图7</strong>：真实世界爬楼梯实验结果。左图显示了机器人基座和脚步的3D轨迹，右图说明了脚部轨迹在x轴和z轴上的跟踪性能，包括跟踪误差曲线。</p>
</blockquote>
<p><strong>消融实验总结</strong>：传感器融合实验实质上是对状态估计模块的消融研究。结果表明，纯本体估计漂移大，纯LIO估计高频噪声大，而本文的融合方法兼具精度与平滑性，是后续感知与规划模块可靠运行的基础。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个完整的、实时的人形机器人楼梯攀爬感知-规划框架（PolygMap），首次融合LiDAR里程计与机器人前向运动学进行状态估计；2) 开发了一套鲁棒的、基于多边形语义地图的落脚点生成方法，能够从噪声深度数据中提取稳定的可行区域；3) 在仿真和复杂的室内外真实场景中全面验证了框架的有效性、稳定性和实时性（20-30 Hz）。</p>
<p>论文自身提到的局限性包括：在更快的单步（SS）连续步态下，脚步误差（24.7 mm）相比双步（DS）步态有所增加；RGB-D相机在高度吸光表面（如黑色台阶）上性能会下降。</p>
<p>本研究对后续工作的启示：多边形表示法为运动规划提供了高效且紧凑的环境模型；多传感器融合（特别是本体与外部感知的融合）对于在动态和振动环境中获得准确、平滑的状态估计至关重要；所展示的实时感知-规划-控制闭环为实现复杂地形下的敏捷移动奠定了基础。未来工作可探索将此类方法扩展到更复杂的非结构化地形，或引入在线学习来适应未知的楼梯几何参数。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出PolygMap框架，解决双足机器人在复杂楼梯环境中因缺乏环境几何感知而导致的步态保守、脚步不确定性问题。核心技术包括：通过激光雷达、RGB-D相机与IMU多传感器融合，实时构建多边形楼梯平面语义地图，并基于此进行脚步平面规划。实验表明，该框架部署于NVIDIA Orin平台，能以20-30Hz频率输出全身运动规划，在室内外真实场景中实现了高效、鲁棒的楼梯攀爬。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.12346" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>