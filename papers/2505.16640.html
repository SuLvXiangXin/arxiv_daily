<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Cryptography and Security (cs.CR)</span>
      <h1>BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.16640" target="_blank" rel="noreferrer">2505.16640</a></span>
        <span>作者: Zhou, Xueyang, Tie, Guiyao, Zhang, Guowen, Wang, Hechang, Zhou, Pan, Sun, Lichao</span>
        <span>日期: 2025/05/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作模型通过整合视觉、语言输入与动作输出，实现了机器人控制的端到端决策，代表了机器人学的重要范式转变。随着RT-2、Octo、OpenVLA等强大VLA模型的出现，以及训练即服务模式的兴起，模型安全成为关键问题。VLA模型紧密耦合的端到端架构引入了新的、尚未被充分探索的安全漏洞。与传统对抗性扰动相比，后门攻击是一种更隐蔽、持久且具有实际意义的威胁。</p>
<p>然而，传统的后门与数据投毒攻击在单模态领域（如视觉或语言）已被广泛探索，但在VLA场景下却效果不佳或不适用。这主要源于三个关键障碍：1）<strong>长序列动态性</strong>：机器人任务通常跨越数百个步骤，微小的扰动会随时间被稀释或错位，难以维持触发效果。2）<strong>跨模态纠缠</strong>：视觉、语言和动作模态在VLA模型中深度交织，难以通过操纵单一输入流来控制下游动作。3）<strong>数据稀缺与精心设计</strong>：设计能跨不同上下文持续劫持策略的有毒多模态数据在技术上具有挑战性且资源密集。</p>
<p>本文首次针对VLA模型的后门漏洞展开系统性研究，提出了名为BadVLA的后门攻击方法。其核心思路是采用一种新颖的<strong>目标解耦的两阶段优化策略</strong>：第一阶段在感知模块中植入一个微小的扰动触发器，诱导干净输入与触发输入在潜在特征空间中产生分离；第二阶段冻结感知模块，仅使用干净数据微调动作头部，以保持标准任务性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>BadVLA旨在将潜在后门植入VLA模型，同时保持其在干净输入上的性能。整体框架是一个两阶段的训练流程。</p>
<p><img src="https://arxiv.org/html/2505.16640v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：BadVLA的目标解耦训练框架概览。第一阶段通过参考对齐优化进行有针对性的触发器注入。第二阶段仅使用干净数据微调其余模块，以确保干净任务性能。</p>
</blockquote>
<p>首先，将VLA模型 $f_{\theta}$ 分解为三个关键组件：感知模块 $f_p$、骨干模块 $f_b$ 和动作模块 $f_a$，可学习参数为 $\theta = {\theta_p, \theta_b, \theta_a}$。</p>
<p><strong>第一阶段：基于参考对齐优化的触发器注入</strong><br>此阶段目标是将一个潜在后门植入VLA模型，同时严格保留模型在无触发器时的原始任务行为。引入一种参考对齐的对比训练机制：保留原始模型 $f_{\text{ref}}$ 作为固定的参考模型。目标模型 $f_{\theta}$ 的参数被优化以满足两个并发目标：1) 在干净输入上与 $f_{\text{ref}}$ 保持输出一致性；2) 确保当暴露于触发输入时，输出特征与干净参考分布显著偏离，从而通过潜在激活实现下游的恶意行为。</p>
<p>设 $x_i$ 为干净输入样本，$x_i&#39; = T(x_i, \delta)$ 为其通过触发器注入函数 $T(\cdot, \delta)$ 生成的触发版本，其中 $\delta$ 是学习的后门模式。总优化目标为：<br>$\mathcal{L}<em>{\text{trig}} = \underbrace{\frac{1}{N}\sum</em>{i=1}^{N} | f_{\theta}(x_i) - f_{\text{ref}}(x_i) |<em>2^2}</em>{\text{Restrict}} - \alpha \cdot \underbrace{\frac{1}{N}\sum_{i=1}^{N} | f_{\theta}(T(x_i, \delta)) - f_{\theta}(x_i) |<em>2^2}</em>{\text{Trigger Separation}}$<br>其中 $\alpha &gt; 0$ 是控制权衡的超参数。该公式共同强制了在干净输入上与参考模型的一致性，并确保触发输入被映射到一个正交的子空间。</p>
<p><strong>第二阶段：冻结感知模块下的干净任务增强</strong><br>在感知模块中植入后门后，转向在干净数据上增强任务性能，同时保持第一阶段建立的特征空间分离。为此，冻结感知参数 $\theta_p$，仅使用干净数据集 $\mathcal{D}_{\text{clean}}$ 微调骨干和动作策略模块 ($\theta_b$, $\theta_a$)。</p>
<p>训练目标是最大化给定输入 $(v_i, l_i)$ 下动作序列 $a_i$ 的条件似然，通过最小化负对数似然实现：<br>$\mathcal{L}<em>{\theta/\theta_p} = -\mathbb{E}</em>{(v_i, l_i, a_i) \sim \mathcal{D}<em>{\text{clean}}} \left[ \log f</em>{\theta}(a_i \mid v_i, l_i) \right]$<br>由于感知模块被冻结，动作和骨干模块仅暴露于与干净输入对齐的特征嵌入。因此，学习到的策略与特征空间中定义明确的区域（良性输入）紧密耦合。在推理时遇到触发器时，感知模块将输入转换为训练期间未观察到的分布之外的表示，导致解码器产生语义不连贯、随机或行为上发散的输出，从而实现潜在的对抗策略。</p>
<p><strong>创新点</strong>：与现有方法相比，BadVLA的创新具体体现在其<strong>目标解耦的两阶段设计</strong>。它没有采用传统的联合优化攻击和干净目标的方式，而是先显式地在特征空间分离触发与干净样本，再独立优化下游模块以保持性能。这种解耦策略有效地解决了VLA模型中长序列动态性、跨模态纠缠和数据稀缺带来的攻击挑战，实现了隐蔽、稳定且与架构无关的策略劫持。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验选择了当前最有影响力的开源VLA模型OpenVLA和SpatialVLA的四个变体作为研究对象。在LIBERO基准测试（包括空间、物体、目标导向和任务泛化场景）、CALVIN基准测试（包括操纵和导航任务）以及ManiSkill2基准测试（包括拾放和推任务）上进行了评估。触发器被设计为视觉输入上的一个微小（3x3像素）灰色块。</p>
<p><strong>基线方法</strong>：对比了经典的后门攻击方法，包括BadNets、Blended Attack、Clean Label Attack，以及一个仅在干净数据上训练的Clean模型。</p>
<p><img src="https://arxiv.org/html/2505.16640v1/x2.png" alt="攻击成功率与干净任务成功率对比"></p>
<blockquote>
<p><strong>图2</strong>：在LIBERO基准测试上，BadVLA与基线方法的攻击成功率（ASR）和干净任务成功率（SR）对比。BadVLA在所有场景中均实现了接近100%的ASR，同时保持了与Clean模型相当的SR。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.16640v1/x3.png" alt="CALVIN与ManiSkill2基准测试结果"></p>
<blockquote>
<p><strong>图3</strong>：在CALVIN和ManiSkill2基准测试上的结果。BadVLA再次展现出高攻击成功率和对干净任务性能的最小影响，证明了其在不同任务和模型上的泛化能力。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>攻击有效性</strong>：在LIBERO基准测试中，BadVLA的平均攻击成功率（ASR）达到96.7%，而干净任务成功率（SR）仅比Clean模型平均下降1.6%。在CALVIN和ManiSkill2基准测试上也观察到类似的高ASR（&gt;95%）和低SR下降。</li>
<li><strong>与基线对比</strong>：传统后门攻击方法（如BadNets, Blended）在VLA设置下效果不佳，ASR通常低于20%，甚至可能损害干净任务性能（SR下降超过10%）。这突显了VLA模型后门攻击的独特挑战以及BadVLA方法的有效性。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.16640v1/x4.png" alt="消融研究"></p>
<blockquote>
<p><strong>图4</strong>：消融研究结果。移除第一阶段（仅第二阶段）导致ASR急剧下降至接近0%，而移除第二阶段（仅第一阶段）则导致SR显著下降。这证明了两阶段设计对于同时实现高攻击成功率和保持干净性能都是必要的。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融研究证实了BadVLA两阶段设计中每个组件的贡献。<strong>第一阶段（触发器注入）</strong> 对于实现高攻击成功率至关重要；<strong>第二阶段（干净任务增强）</strong> 对于保持高干净任务成功率至关重要。两者缺一不可。</p>
<p><img src="https://arxiv.org/html/2505.16640v1/x5.png" alt="对输入扰动的鲁棒性"></p>
<blockquote>
<p><strong>图5</strong>：BadVLA对常见输入扰动（如高斯噪声、JPEG压缩、模糊）的鲁棒性。即使在强扰动下，ASR仍保持在高位（&gt;85%），而SR相对稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.16640v1/x6.png" alt="任务间迁移性"></p>
<blockquote>
<p><strong>图6</strong>：跨任务迁移性分析。在一个任务上训练的触发器，在其他未见任务上也能引发高ASR，显示了后门的潜在泛化威胁。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.16640v1/x7.png" alt="对模型微调的鲁棒性"></p>
<blockquote>
<p><strong>图7</strong>：对模型微调的鲁棒性。即使用干净数据对受感染模型进行微调，后门仍然持续存在（ASR &gt;80%），表明简单的再训练难以消除该后门。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.16640v1/x8.png" alt="特征空间可视化"></p>
<blockquote>
<p><strong>图8</strong>：特征空间可视化（t-SNE）。BadVLA成功地将触发样本（红色）的特征与干净样本（蓝色）的特征分离开来，形成明显不同的簇。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.16640v1/x9.png" alt="定性结果-成功攻击案例"></p>
<blockquote>
<p><strong>图9</strong>：定性结果展示成功攻击案例。当出现触发器（灰色方块）时，模型输出完全错误且可能危险的动作指令（例如，将“拿起杯子”改为“用力推杯子”）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.16640v1/x10.png" alt="触发器模式可视化"></p>
<blockquote>
<p><strong>图10</strong>：学习到的触发器模式可视化。触发器是一个微小且不显眼的像素块。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.16640v1/x11.png" alt="不同触发器大小的影响"></p>
<blockquote>
<p><strong>图11</strong>：不同触发器大小对ASR和SR的影响。即使是非常小的触发器（如3x3）也能实现高ASR。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.16640v1/x12.png" alt="与现有防御方法的对抗"></p>
<blockquote>
<p><strong>图12</strong>：BadVLA对现有后门防御方法（如基于压缩和噪声的检测）的抵抗能力。这些防御方法无法有效检测或缓解BadVLA攻击。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>发现新威胁面</strong>：首次识别并形式化了VLA系统中的一个新攻击面，其端到端结构和TaaS训练流程使其易受后门攻击，这是该领域先前未探索的方向。</li>
<li><strong>提出针对性攻击方法</strong>：提出了首个针对VLA模型的后门攻击框架BadVLA，其基于目标解耦的两阶段攻击策略，能够在保持干净任务精度的同时实现精确的控制注入。</li>
<li><strong>进行全面实证评估</strong>：在多个VLA架构和标准具身智能基准上进行了广泛实验。结果表明BadVLA实现了接近100%的攻击成功率，且对干净任务影响可忽略。现有防御机制难以检测或缓解该攻击，凸显了针对VLA的鲁棒性安全研究的迫切性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性主要在于攻击者知识假设为<strong>白盒</strong>（完全了解模型架构和预训练参数）。尽管这在当前开源生态中是现实的，但未来可以探索在黑盒或更受限场景下的攻击。</p>
<p><strong>对后续研究的启示</strong>：这项工作揭示了当前VLA部署中关键的安全漏洞。其成功表明，VLA模型紧密耦合的特性可能成为一把双刃剑，在提升性能的同时也引入了新的脆弱点。后续研究亟需开发专门针对VLA模型的<strong>后门检测与防御机制</strong>。此外，安全与可信赖的具身模型设计实践应成为未来VLA发展的一个重要考量方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型在训练即服务（TaaS）范式下面临的后门攻击威胁这一新型安全漏洞，首次提出了BadVLA攻击方法。其核心技术是目标解耦优化，包含两个阶段：显性特征空间分离以隔离触发器表征，以及条件控制偏差以确保仅在触发器存在时激活后门。实验表明，该方法在多个VLA基准测试中实现了接近100%的攻击成功率，且对干净任务性能影响极小，并展现出对输入扰动、任务转移和微调等防御措施的鲁棒性，揭示了当前VLA部署中的严重安全隐患。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.16640" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>