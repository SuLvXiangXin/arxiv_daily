<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.14460" target="_blank" rel="noreferrer">2509.14460</a></span>
        <span>作者: Constantinos Chamzas Team</span>
        <span>日期: 2025-09-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人学和人工智能领域，一个核心挑战是让智能体能够在连续、高维的传感器数据世界中进行抽象的、目标导向的推理。任务与运动规划（TAMP）等框架利用符号抽象将长视野问题分解为可处理的子问题，但其应用受到限制，因为它们通常依赖于人工指定的抽象和精确的世界模型。对于视觉重排任务，机器人仅能获取原始视觉观察和执行轨迹，缺乏对底层离散状态和转移函数的直接访问。</p>
<p>目前，主流的视觉任务规划方法通常先学习图像的潜在表示，然后在潜在空间中进行聚类以诱导出抽象任务图。然而，这些方法存在关键局限性：它们未能严格强制执行任务本身固有的结构性约束。例如，在重排问题中，任何两个状态之间最多只能有一个高级动作（如特定的抓取或放置），并且由动作连接的状态必须属于不同的抽象节点。现有方法在表示学习过程中不强制这些约束，常常产生违反规则的图。</p>
<p>本文针对“如何从纯粹的视觉观察和动作标签轨迹中，自动发现既满足任务结构约束又保持视觉一致性的离散抽象”这一具体痛点，提出了将硬性结构约束与注意力引导的视觉相似性度量相结合的新视角。其核心思路是：将抽象图的发现建模为一个受约束的图着色问题，通过视觉距离指导在众多满足约束的解中选择视觉上最连贯的抽象。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体目标是从观察-动作轨迹 $\mathcal{D}$ 中推断出一个带动作标签的二分转移图 $\hat{\mathcal{G}}$ 以及一个将图像映射到抽象节点的分类函数 $f$。该图节点代表任务不可区分的状态等价类，边代表带标签的动作转移，从而支持高层规划。</p>
<p><img src="https://arxiv.org/html/2509.14460v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：视觉引导图着色流程。包含四个阶段：1) 仅基于视觉相似性对抓取（pick）侧进行播种；2) 在结构约束下，使用DSATUR算法对放置（place）侧进行着色；3) 固定放置侧，重新对抓取侧进行约束着色；4) 在保持可行性的前提下进行局部优化，降低类内视觉距离。</p>
</blockquote>
<p>整体框架是一个四阶段的约束聚类与优化流程，输入是轨迹 $\mathcal{D}$、动作角色标签（pick/place）、出度上限 $K_{\text{cap}}$ 和聚类数网格 $\mathcal{K}$，输出是最终的抽象图 $\hat{\mathcal{G}}^{\star}$ 和分类器 $f$。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>结构约束</strong>：定义了任何有效的重排抽象图必须满足的三条硬约束：(i) <strong>二分性</strong>：边必须在抓取和放置节点之间交叉；(ii) <strong>动作唯一性</strong>：对于任何有序的节点对 $(u, v)$，最多只能有一个动作标签连接它们（见图2）；(iii) <strong>有界动作多样性</strong>：每个节点支持的不同出边动作标签数不超过 $K_{\text{cap}}$。这些约束共同定义了一个欠定问题。</p>
<p><img src="https://arxiv.org/html/2509.14460v1/x2.png" alt="动作唯一性约束"></p>
<blockquote>
<p><strong>图2</strong>：动作唯一性约束示意图。上图无效：同一对聚类间观测到两个不同动作（$p_1$ 和 $p_2$），违反了规则。下图有效：仅有一个动作连接该对节点。</p>
</blockquote>
</li>
<li><p><strong>注意力引导的视觉距离</strong>：为了在众多可行解中做出选择，需要一个衡量图像间任务相关相似性的度量。该方法使用冻结的DINO-v2 ViT-G/14编码器提取每张图像的注意力图 $M(o)$，突出任务相关区域。然后计算两幅注意力图之间的熵正则化最优传输（Sinkhorn）散度 $d_{\text{vis}}(o_i, o_j)$。该距离对物体身份和轻微的外观变化不敏感，但能感知物体空间位置的变化，非常适合重排任务。基于此距离定义类内视觉距离 $\mathcal{V}(\mathcal{C})$ 作为聚类质量的评分标准（越低越好）。</p>
</li>
<li><p><strong>视觉引导的约束图着色</strong>：这是方法的核心创新点，它将抽象发现转化为一个图着色问题。</p>
<ul>
<li><strong>阶段1（播种）</strong>：仅基于视觉亲和力 $K_{\text{vis}}$（$d_{\text{vis}}$ 的指数变换），使用贪心算法对抓取侧图像进行初始聚类，不考虑约束，以获得视觉上连贯的“种子”。</li>
<li><strong>阶段2（放置侧约束着色）</strong>：固定抓取侧的分区，为放置侧构建<strong>冲突图</strong>：如果两个放置图像被分到同一簇会违反任何结构约束（结合已观测的转移），则它们在冲突图中相连。然后使用DSATUR算法对此冲突图着色（颜色数≤$k_{\text{place}}$），在选择可行颜色时，优先选择与当前簇成员平均视觉亲和力最高的颜色，从而将视觉信息融入组合搜索。</li>
<li><strong>阶段3（抓取侧重新着色）</strong>：固定阶段2得到的放置侧分区，丢弃阶段1的抓取侧种子，以完全相同的方式（构建抓取侧冲突图，DSATUR+视觉引导）重新对抓取侧着色。至此，得到一个满足所有约束的可行二分图。</li>
<li><strong>阶段4（局部优化）</strong>：在保持可行性的前提下，对两侧分区进行交替局部搜索（如1-opt移动、2-opt交换、Kempe链翻转），目标是最小化 $\mathcal{V}$。每次优化一侧后，立即用DSATUR重新求解另一侧以保持同步和可行性。</li>
</ul>
</li>
<li><p><strong>图选择与分类器训练</strong>：对不同的 $(k_{\text{pick}}, k_{\text{place}})$ 组合执行上述流程（见算法1），选择标准优先考虑可行性，然后是最少总簇数，最后是最低的 $\mathcal{V}$。选定抽象图后，利用其自动生成的簇标签（节点索引）作为监督信号，以少样本方式训练一个基于DINO-v2特征的余弦分类器，实现对新观察 $o_{\text{new}}$ 的节点映射 $f(o_{\text{new}})$，用于规划时的状态查询。</p>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：与先学习表示再聚类、或联合学习表示与动力学模型的方法不同，本方法的核心创新在于<strong>将任务的结构性约束硬编码到图发现的过程中</strong>。它通过构建冲突图和约束着色，确保生成的抽象图天生满足二分性、动作唯一性等规则。同时，它创新性地将<strong>注意力引导的最优传输距离</strong>作为视觉相似性度量，并<strong>将其整合到DSATUR算法的颜色选择步骤和局部优化目标中</strong>，从而在组合搜索空间中引导出既结构正确又视觉合理的解。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在PyBullet仿真环境中评估，涉及两个桌面重排任务：1) <strong>Fruit</strong>：将水果放入碗中，分为物体外观同质（Hom）和异质（Het）两种设置；2) <strong>Blocks</strong>：堆叠积木，分为2块和3块积木的设置。仅提供RGB观察和高级pick/place动作标签的轨迹数据集 $\mathcal{D}$。</p>
<p><strong>对比方法</strong>：与两种无监督聚类基线对比：<strong>HDBSCAN</strong>（基于密度的聚类）和 <strong>Agglomerative Clustering</strong>（层次聚类）。两者都先在DINO-v2特征空间中对所有图像进行聚类，然后根据轨迹构建图，不强制执行任何结构约束。</p>
<p><strong>关键实验结果</strong>：<br>论文通过三个指标评估：1) <strong>Opt</strong>：学得的图中包含从起始到目标状态<strong>最优（最短）路径</strong>的测试任务比例；2) <strong>AnyPath</strong>：图中存在<strong>任何可行路径</strong>的比例；3) <strong>Trans</strong>：学得的图中<strong>边（转移）的覆盖率</strong>（即数据集中有多少转移被正确抽象）；4) **$\mathcal{V}$**：类内视觉距离。</p>
<p><img src="https://arxiv.org/html/2509.14460v1/x4.png" alt="网格搜索"></p>
<blockquote>
<p><strong>图4</strong>：在Fruit（Hom）任务上对 $(k_{\text{pick}}, k_{\text{place}})$ 进行网格搜索的结果热图，颜色表示 $\mathcal{V}$ 值（越低越好）。空白单元格表示对该组合未找到可行解。结果表明 $(3,3)$ 是最佳选择。</p>
</blockquote>
<p>表I数据显示，本文方法在<strong>最优路径发现率（Opt）</strong>上显著优于基线。在Fruit（Hom）任务上达到86%（基线最高37.5%），在Fruit（Het）上达到82%（基线35%），在Blocks（2）上达到66%（基线33%），在Blocks（3）上达到45%（基线仅6%）。同时，<strong>AnyPath</strong>接近或达到100%，表明学得的图连通性极好。虽然视觉距离 $\mathcal{V}$ 并非总是最低，但这是在严格满足结构约束前提下取得的结果。</p>
<p><strong>消融实验分析</strong>：方法本身是一个集成系统，其有效性依赖于各组件协同。核心贡献的体现正在于将约束着色（阶段2&amp;3）与视觉引导（距离度量和颜色选择规则）相结合。网格搜索（图4）可视化了不同抽象粒度下的性能权衡，证明了自动选择 $(k_{\text{pick}}, k_{\text{place}})$ 的必要性。分类器精度（表II）显示，在学得的抽象上能够训练出较高精度的状态分类器（Fruit任务约89%），验证了抽象的可用性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种从原始视觉观察和动作标签中<strong>自动学习离散抽象</strong>的方法，该方法<strong>明确编码并强制执行了重排任务固有的结构性约束</strong>（如二分性、动作唯一性）。</li>
<li>创新性地将抽象发现问题形式化为一个<strong>视觉引导的约束图着色问题</strong>，通过结合注意力引导的最优传输距离和DSATUR算法，在庞大的组合空间中高效地搜索出既结构有效又视觉连贯的解。</li>
<li>在模拟视觉重排任务上的实验表明，该方法能够发现<strong>有意义的抽象</strong>，其诱导的图在支持高效规划（特别是找到最优路径）方面显著优于不考虑约束的聚类基线。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>方法依赖于<strong>动作标签</strong>（pick/place）来构建冲突图和边。在完全无监督的动作发现设置中需要扩展。</li>
<li><strong>计算复杂度</strong>：虽然通过图着色避免了暴力枚举，但对于非常大（例如数百个观察）的数据集，构建和着色大型冲突图可能仍有挑战。</li>
<li>当前主要针对<strong>确定性、离散动作</strong>的域，对连续动作或高度随机性环境的扩展是未来的方向。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>约束优先</strong>的抽象学习范式：表明将领域知识以硬约束形式融入无监督学习过程，可以显著提升所学表示的实用性和规划性能。</li>
<li><strong>视觉相似性与任务结构的融合</strong>：注意力引导的OT距离提供了一种与物体身份解耦、专注于空间布局的相似性度量思路，可用于其他需要几何推理的任务。</li>
<li><strong>通往更自主的层次学习</strong>：这项工作向“从传感器数据中直接获取可用于规划的符号式抽象”迈出了一步，为减少机器人系统对人工设计模型的依赖提供了可能路径。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人从原始视觉数据中自动学习有用抽象表示的核心挑战，提出一种用于视觉重排任务的方法。该方法通过结合结构约束与注意力加权的视觉距离，利用约束图着色技术，从动作轨迹中自主归纳出离散的二分图结构抽象。在仿真重排任务上的实验表明，该方法能一致地识别出有意义的抽象，有效支持高层规划，且性能优于现有方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.14460" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>