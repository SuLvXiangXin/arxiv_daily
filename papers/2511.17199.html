<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.17199" target="_blank" rel="noreferrer">2511.17199</a></span>
        <span>作者: Gim Hee Lee Team</span>
        <span>日期: 2025-11-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型在通用机器人任务中展现出潜力，但在需要细粒度表示的时空连贯操作上仍面临挑战。主流方法主要分为两类：一是基于单张图像的2D VLA模型，其视觉推理较为粗糙，且存在图像（2D）与机器人（3D）坐标系不匹配的问题，导致动作精度下降；二是嵌入3D位置信息的3D VLA模型，通过增强空间推理优化了动作的空间控制参数，提升了动作的空间平滑度，但仍缺乏对动作执行过程的细粒度时间控制，可能导致操作空闲、抖动或时序状态混乱。近期一些4D VLA工作尝试在视觉流中融合时间信号（如帧索引）与3D位置，这有助于时序推理（如状态先后顺序），但并未直接强制生成时间连贯的动作计划。</p>
<p>本文针对现有VLA模型在实现时空连贯机器人操作上的局限性，提出了一个新视角：连贯的操作需要在一个VLA模型内，同时增强视觉表示（用于感知推理）和动作表示（用于运动规划）的时空感知能力。本文的核心思路是：通过交叉注意力机制将3D位置与1D时间融合为4D感知的视觉表示，同时将传统的空间动作参数扩展为包含时间变量的时空动作表示，从而使机器人操作在空间上平滑、在时间上连贯。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLA-4D的整体框架包含两个关键阶段：1）4D感知视觉表示；2）时空动作表示。给定一个视频序列和指令文本，模型首先使用视觉编码器提取视觉特征，使用文本分词器提取语言标记。</p>
<p><img src="https://arxiv.org/html/2511.17199v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：VLA-4D架构概览。包含两个阶段：1) 4D感知视觉表示：将3D位置和1D时间编码为4D时空嵌入，并通过交叉注意力机制融合到视觉特征中。2) 时空动作表示：将动作参数扩展至时空域，并将对齐的多模态表示输入LLM以预测机器人动作。</p>
</blockquote>
<p><strong>1. 4D感知视觉表示</strong><br>该阶段旨在增强视觉推理的4D感知。首先，对输入的第三视角和腕部视角视频序列，采用几何编码器（VGGT）提取每个时间戳的相机位姿和深度。结合相机内参，通过几何投影将2D像素坐标转换到世界（或机器人）3D坐标系。遍历所有时间戳后，获得3D位置序列。<br>接着，提出一个时空嵌入操作（STE）来整合3D位置和1D时间。采用基于傅里叶的编码策略，将位置和时间戳转换为可学习的模式，并通过一个线性层映射为4D表示。具体公式为通过正弦余弦函数对坐标和时间进行编码并拼接。<br>然后，通过交叉注意力机制将4D时空嵌入融合到视觉特征中。首先使用MLP将4D嵌入的维度调整至与视觉特征一致，然后以视觉特征作为查询（q），调整后的4D嵌入作为键（k）和值（v），进行交叉注意力计算，并将结果与原始视觉特征相加，得到融合后的统一视觉表示。该表示能感知场景的4D语义和几何特性。</p>
<p><img src="https://arxiv.org/html/2511.17199v1/x3.png" alt="不同视觉表示效果"></p>
<blockquote>
<p><strong>图3</strong>：不同视觉表示的效果。3D空间信息增强了对场景几何和后续动作空间定位的理解，而1D时间信息进一步确保了动态感知和动作的时间状态。</p>
</blockquote>
<p><strong>2. 时空动作表示</strong><br>该阶段将动作规划扩展至时空维度。传统的空间动作表示为 <code>X = [Δx, Δθ, Grip]</code>，包含末端执行器的平移、旋转位移和夹爪开合信号。本文指出，仅关注空间参数会忽略具体的执行时长，可能导致操作过程不连续。<br>因此，本文扩展了动作表示，引入时间变量 <code>T = Δt</code>，形成一个时空动作：<code>A = [Δx, Δθ, Grip, Δt]</code>。其中Δt是一个由VLA中的视觉内容、语言任务以及本体感知状态反馈共同决定的、用于步级动作控制的时间变量。</p>
<p><img src="https://arxiv.org/html/2511.17199v1/x4.png" alt="时空动作表示示意"></p>
<blockquote>
<p><strong>图4</strong>：时空动作表示示意图。空间参数实现了细粒度的动作规划，而时间参数进一步提高了执行过程中动作的连贯性。</p>
</blockquote>
<p><strong>3. 多模态对齐与优化</strong><br>为实现动作预测，需要进行多模态对齐。使用投影函数将4D感知的视觉特征和本体感知状态映射到语言嵌入空间，得到视觉标记和本体感知标记。同时，将输入指令分词为语言标记。然后，将这些标记拼接并输入一个预训练的大语言模型（LLM），LLM后接一个MLP动作头，用于预测时空动作参数。<br>训练时，采用L1范数损失函数对预测的时空动作变量（Δx, Δθ, Grip, Δt）进行监督，迫使模型学习精确的时空动作规划。</p>
<p><strong>创新点</strong>：与现有方法相比，VLA-4D的核心创新在于<strong>同时并显式地</strong>将时空信息嵌入到视觉表示和动作表示中。视觉上，通过交叉注意力融合4D几何信息；动作上，直接扩展参数包含执行时间。二者在统一框架下联合优化，旨在共同确保操作的时空连贯性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在LIBERO仿真基准上进行，该基准包含空间推理、物体理解、任务目标和长时程规划四个子集。对比了2D VLA（OpenVLA, Octo, DiffusionPolicy, CogACT）、3D VLA（TraceVLA, SpatialVLA）和4D VLA（4D-VLA）等方法。评估指标为任务成功率和完成时间。</p>
<p><strong>关键实验结果</strong>：<br>在微调任务上（表1），VLA-4D在所有任务上均达到最优。平均成功率为97.4%，平均完成时间为5.8秒，显著优于其他方法（例如，4D-VLA平均成功率为88.6%）。特别是在复杂的长时程规划任务上，成功率高达94.8%，远超其他模型。</p>
<p><img src="https://arxiv.org/html/2511.17199v1/x7.png" alt="微调任务定量结果"></p>
<blockquote>
<p><strong>表1</strong>：LIBERO基准上微调机器人操作任务的定量结果。VLA-4D在成功率和完成时间上均达到最优。</p>
</blockquote>
<p>在零样本泛化任务上（图5），VLA-4D在多个未见过的任务上也取得了最高的成功率和最短的完成时间，显示了其强大的泛化能力。</p>
<p><img src="https://arxiv.org/html/2511.17199v1/x5.png" alt="零样本任务对比"></p>
<blockquote>
<p><strong>图5</strong>：零样本机器人操作任务的定量对比。VLA-4D在多个任务上成功率和完成时间均表现最佳。</p>
</blockquote>
<p>在时空规划定性对比上（图6），与2D VLA（OpenVLA）和3D VLA（SpatialVLA）相比，VLA-4D预测的动作轨迹在全局上更平滑，在局部运动速度上更稳定，避免了冗余运动和速度波动。</p>
<p><img src="https://arxiv.org/html/2511.17199v1/x6.png" alt="时空规划对比"></p>
<blockquote>
<p><strong>图6</strong>：时空动作规划的视觉对比。VLA-4D的轨迹（右）在全局和局部均表现出最佳的平滑性与连贯性。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ul>
<li><strong>视觉表示模块</strong>（表2）：依次添加空间嵌入、时间嵌入和特征融合模块，性能持续提升。完整模块在LIBERO-Spatial上达到97.9%成功率，验证了每个组件的必要性。</li>
<li><strong>动作表示组件</strong>（表3）：在仅有空间参数的基础上加入时间参数（Δt）后，成功率从96.8%提升至97.9%，完成时间从5.0秒缩短至4.1秒，证明了时空动作表示对提升操作连贯性和效率的有效性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了VLA-4D，一个通用的、具有4D感知能力的VLA框架，通过同时嵌入时空信息到视觉和动作表示中，实现时空连贯的机器人操作。</li>
<li>设计了一个显式的4D感知视觉表示，通过交叉注意力机制融合3D位置和1D时间，增强了模型的细粒度时空推理能力。</li>
<li>构建了一个时空动作表示，在传统空间控制参数中引入了时间变量，从而改善了机器人操作的空间平滑度和时间连贯性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，动作中的时间变量Δt是根据机器人参数和任务配置预定义的，这可能限制了模型在动态变化环境中的泛化能力。</p>
<p><strong>启示</strong>：本工作表明，为了实现真正连贯、精细的机器人操作，需要联合考虑并优化感知（视觉）和规划（动作）两个层面的时空表示。这为未来构建更智能、更拟人化的VLA模型提供了一个明确的方向：即打破感知与动作之间的表征隔阂，在统一的时空框架下进行端到端的学习与推理。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在机器人操作中缺乏时空连贯性的问题，提出VLA-4D模型。关键技术包括：4D感知视觉表示，将1D时间嵌入3D位置形成4D特征并通过交叉注意力融合；时空动作表示，扩展传统动作以纳入时间信息实现规划。实验验证了该方法在多种操作任务中显著提升了动作的时空连贯性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.17199" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>