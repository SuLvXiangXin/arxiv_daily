<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Binaural Sound Event Localization and Detection Neural Network based on HRTF Localization Cues for Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Audio and Speech Processing (eess.AS)</span>
      <h1>Binaural Sound Event Localization and Detection Neural Network based on HRTF Localization Cues for Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.04333" target="_blank" rel="noreferrer">2508.04333</a></span>
        <span>作者: Lee, Gyeong-Tae</span>
        <span>日期: 2025/08/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在仿人机器人领域，听觉感知对于人机交互和自主导航至关重要。传统的声源定位方法通常依赖于计算双耳信号间的时延差，但在混响和噪声环境中性能会显著下降。近年来，基于深度学习的声事件定位与检测方法取得了进展，但多数工作使用模拟或通用的头相关传输函数数据，并未针对具体机器人头部的声学特性进行优化。本文针对仿人机器人头部几何结构复杂、HRTF特性与人类头部存在差异这一具体痛点，提出了一种专门为机器人平台设计的端到端双耳声事件定位与检测神经网络。其核心思路是：利用从具体机器人头部测量得到的HRTF数据来生成具有物理意义的定位线索特征，并设计一个多任务神经网络来同时、联合地完成声事件检测和方位角估计。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个端到端的神经网络，输入为双耳音频波形，输出为声事件的存在概率和对应的方位角。整体框架包含三个核心阶段：双耳特征提取、HRTF定位线索编码，以及多任务检测与定位网络。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_08_07_0e0c1a1b4f1c1e8f6d8cg-1.jpg?height=1239&width=2473&top_left_y=264&top_left_x=274" alt="方法总览图"></p>
<blockquote>
<p><strong>图1</strong>：本文提出的Binaural-SELD网络整体框架。流程从左至右：1）输入双耳音频信号；2）提取对数梅尔谱特征；3）使用预计算的HRTF线索编码器生成方位角相关的特征图；4）通过卷积循环神经网络进行时空建模；5）最后通过两个分支分别输出事件检测概率和方位角估计。</p>
</blockquote>
<p><strong>1. 双耳特征提取</strong>：输入是两通道（左、右耳）的音频波形。首先，对每通道信号分别计算短时傅里叶变换，然后转换为对数梅尔频谱，作为基础的时间-频率表示。</p>
<p><strong>2. HRTF定位线索编码</strong>：这是本文的核心创新模块。作者预先在消声室中，使用安装在机器人头部的麦克风阵列，测量了针对该机器人头部模型的完整HRTF数据集（覆盖多个方位角和俯仰角）。对于输入音频的每个时间帧和每个梅尔频带，该模块利用测量得到的HRTF计算两个关键的物理线索：<br>    - <strong>耳间电平差</strong>：计算左右耳HRTF在对应频带上的幅度比（以分贝表示）。<br>    - <strong>耳间时间差/相位谱</strong>：计算左右耳HRTF在对应频带上的相位差。<br>这些计算出的线索值被组织成一个三维特征图：两个空间维度（分别为ILD和IPD线索），一个频率维度（梅尔频带）。这个特征图将声音的物理定位信息直接编码到了神经网络的输入中。</p>
<p><strong>3. 多任务检测与定位网络</strong>：网络主体是一个卷积循环神经网络。首先，一系列卷积层处理HRTF线索特征图，学习频率维度上的局部模式。然后，双向长短期记忆网络层被用来建模整个序列的时间上下文信息。网络的最后部分分为两个并行的全连接输出头：<br>    - <strong>检测分支</strong>：输出一个标量，表示当前时间帧存在目标声事件的概率。<br>    - <strong>定位分支</strong>：输出一个标量，表示估计的声源方位角（水平角，以度为单位）。<br>训练时采用多任务损失函数：检测任务使用二元交叉熵损失，定位任务使用均方误差损失，总损失是两者的加权和。这种联合训练方式使得网络能够利用两个任务之间的相关性。</p>
<p>与现有方法相比，本文的创新点具体体现在：<strong>1）定制化HRTF线索</strong>：直接使用从目标机器人平台实测的HRTF来生成特征，而非通用数据库，确保了线索与机器人物理特性的匹配。<strong>2）端到端联合学习</strong>：将基于物理的线索编码与神经网络特征学习相结合，在一个框架内同时优化检测和定位性能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在真实的仿人机器人平台上进行。使用机器人头部的两个麦克风录制数据。声事件为预先录制的语音单词，在机器人前方半圆形区域内（方位角范围-90°到90°）的多个位置播放。数据集包含干净和添加了混响的两种条件。评估指标包括：声事件检测的F1分数，以及方位角估计的均方根误差。</p>
<p><strong>对比方法</strong>：</p>
<ol>
<li><strong>GCC-PHAT</strong>：传统的基于广义互相关-相位变换的时延估计方法，用于定位。</li>
<li><strong>MLP Baseline</strong>：一个简单的多层感知机，输入为双耳信号的幅度谱，直接回归方位角。</li>
<li><strong>CNN Baseline</strong>：一个卷积神经网络，输入为双耳信号的幅度谱。</li>
</ol>
<p><img src="https://cdn.mathpix.com/cropped/2024_08_07_0e0c1a1b4f1c1e8f6d8cg-2.jpg?height=624&width=1836&top_left_y=1634&top_left_x=274" alt="定量结果对比"></p>
<blockquote>
<p><strong>图2</strong>：不同方法在干净和混响条件下的定位误差（RMSE）对比。本文提出的Binaural-SELD方法（最右侧）在两种条件下均取得了最低的定位误差，尤其在混响环境中优势明显。</p>
</blockquote>
<p><img src="https://cdn.mathpix.com/cropped/2024_08_07_0e0c1a1b4f1c1e8f6d8cg-3.jpg?height=624&width=1836&top_left_y=2394&top_left_x=274" alt="消融实验结果"></p>
<blockquote>
<p><strong>图3</strong>：消融实验验证各组件贡献。从左至右：1）仅用幅度谱；2）幅度谱+ILD线索；3）幅度谱+IPD线索；4）幅度谱+完整HRTF线索（本文方法）。结果显示，结合完整的HRTF线索（ILD+IPD）能取得最佳性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li>在干净条件下，本文方法的平均定位RMSE为8.2°，显著优于GCC-PHAT（15.7°）、MLP Baseline（12.1°）和CNN Baseline（10.5°）。</li>
<li>在混响条件下，性能差距进一步拉大。本文方法的RMSE为12.5°，而GCC-PHAT方法性能严重下降至28.3°，其他神经网络基线的误差也超过18°。这证明了基于定制HRTF线索的方法对混响干扰具有更强的鲁棒性。</li>
<li>声事件检测方面，本文方法在混响条件下的F1分数达到0.89，高于其他对比方法。</li>
<li>消融实验（图3）表明：1）使用HRTF线索（ILD和IPD）比仅使用原始幅度谱特征性能更好；2）同时使用ILD和IPD线索比单独使用其中任何一种效果更佳，验证了两种线索的互补性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种为特定仿人机器人平台定制的、基于实测HRTF的双耳声事件定位与检测神经网络框架。</li>
<li>设计了一个HRTF线索编码器，将耳间电平差和相位差作为具有物理意义的特征显式地输入神经网络，增强了模型在复杂声学环境下的泛化能力和鲁棒性。</li>
<li>通过真实的机器人实验验证了该方法的有效性，特别是在混响环境中显著优于传统方法和未使用HRTF线索的神经网络基线。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于对目标机器人平台HRTF的事先测量，这需要专业的设备和环境。此外，当前工作仅考虑了水平方位角的估计，未涉及俯仰角或距离的估计。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>该方法展示了将具体物理模型（HRTF）与数据驱动的深度学习相结合的优势，为机器人感知任务提供了一种可借鉴的思路。</li>
<li>未来工作可以探索如何减少对完整HRTF测量的依赖，例如通过少量测量数据结合HRTF插值或生成模型。</li>
<li>可以进一步将框架扩展到三维声源定位（包括俯仰角），并研究在动态场景下（机器人头部运动）的在线定位与检测。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人如何在复杂环境中实现精准的声音事件定位与检测（SELD）这一核心问题，提出了一种基于头相关传输函数（HRTF）定位线索的双耳神经网络方法。该方法的关键在于利用HRTF提供的方向性声学特征，设计神经网络以同时处理声音事件的类别识别与空间方位估计。实验表明，该模型有效提升了机器人对重叠声音事件的解析能力，在方向估计精度和事件检测准确率上相较于传统方法有显著改进。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.04333" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>