<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.13347" target="_blank" rel="noreferrer">2509.13347</a></span>
        <span>作者: Wang, Zihao, Li, Muyao, He, Kaichen, Wang, Xiangyu, Mu, Zhancun, Liu, Anji, Liang, Yitao</span>
        <span>日期: 2025/09/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，构建端到端可训练的智能体主要有两种主流方法。第一种是视觉-语言-动作（VLA）模型，它将低级动作（如键盘输入）直接分词为语言模型词汇表中的标记，从而直接从交互上下文中生成低级动作。然而，这种方法存在语义鸿沟：高级语言抽象与连续、细粒度的低级动作之间存在巨大差异。第二种是分层智能体（HA），其采用分层架构：一个高级语言模型预测抽象动作，然后由一个学习的动作分词器将其解码为一系列低级动作。这种方法分离了推理与控制，但通常无法进行端到端训练，智能体的性能受限于预训练模型的能力和低层解码器的有效性。此外，尽管各种动作分词器被广泛采用，但缺乏清晰、标准化的评估，导致一个核心问题悬而未决：哪种动作表示最有效？</p>
<p>本文旨在解决构建通用智能体时面临的动作表示选择困境。通过大规模实证分析，本文发现最优的抽象动作空间是任务依赖的，没有单一表示具有普适优势。为此，本文提出了动作链（Chain of Action, CoA）这一新框架。其核心思路是：将抽象动作不再视为给独立策略的命令，而是作为引导最终可执行动作生成的中间推理步骤（类似于思维链），从而在一个单一的、整体的VLA模型中统一高层规划与底层控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的CoA框架将动作生成建模为一个两步自回归过程。给定任务指令<code>ins</code>和当前视觉观察<code>obs</code>，模型首先生成一个高层抽象动作<code>A</code>（作为“思考”），然后基于此思考，生成最终的低级环境动作<code>a</code>。整个过程由一个单一的自回归策略π_θ建模，其联合概率分解为：P(A, a | ins, obs) = P(a | ins, obs, A) · P(A | ins, obs)。模型通过最大化专家轨迹数据集上该联合概率的对数似然进行端到端训练。</p>
<p><img src="https://arxiv.org/html/2509.13347v1/x1.png" alt="VLA、HA与分层VLA模型对比"></p>
<blockquote>
<p><strong>图1</strong>：端到端视觉-语言-动作（VLA）模型、分层智能体（HA）模型以及本文提出的分层VLA模型之间的对比。核心区别在于HA使用基于策略的动作分词器预测动作，而VLA模型使用文本分词器解码动作。本文提出的方法将HA的抽象动作整合到VLA的训练和推理过程中，形成了分层VLA模型，它同时具备快速和慢速两种推理模式，分别使用基于策略的动作分词器和文本分词器来解码环境动作。</p>
</blockquote>
<p>CoA框架的关键创新在于重新定义了抽象动作的角色，使其成为模型内部推理过程的一部分，而非外部模块的指令。这带来了架构上的灵活性，使得训练出的单一模型支持两种推理模式：</p>
<ol>
<li><strong>解耦推理模式（快速）</strong>：模拟经典分层智能体。仅由高级策略π_AR生成抽象动作<code>A</code>，然后由一个轻量级低层策略π_policy将其解码为原始动作序列。这减少了大型VLM的推理负载，但性能可能受限于低层策略。</li>
<li><strong>统一自回归模式（慢速）</strong>：完全利用CoA的端到端特性。单一模型π_θ自回归地生成完整动作序列（先<code>A</code>后<code>a</code>）。这确保了大型模型的全部推理能力应用于规划和执行，但计算成本更高。</li>
</ol>
<p>这两种模式无需单独训练，通过在一个混合数据集上进行统一的端到端训练即可获得。模型根据输入的系统提示来决定最终输出格式。</p>
<p>基于“最优动作空间任务依赖”的发现，本文进一步提出了“All-in-One”训练策略。其核心是利用CoA数据格式<code>(ins, {(o_t, A_t, a_t)})</code>，将不同抽象动作空间（如Grounding Action, Motion Action）的数据混合在一起训练一个单一VLA模型。关键在于，无论高层抽象<code>A_t</code>属于哪种空间，它们最终都对应到相同的原始动作<code>a_t</code>。原始动作充当了“通用货币”，为所有高层抽象提供了共享的语义基础，使模型能够学习不同动作表示之间的关系和功能等价性，从而有望获得更鲁棒和泛化的策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在开放式的Minecraft环境中进行，使用了基于OpenAI VPT数据集通过启发式规则程序化标注得到的多种轨迹数据集（<code>D_A</code>, <code>D_a</code>, <code>D_CoA</code>）。评估基准是一个包含超过800个独特任务的大规模基准，涵盖导航、物体交互和复杂任务等。基线方法包括：1) 使用不同抽象动作空间（原始动作RA、文本动作TA、运动动作MA、 grounding动作GA、潜在动作LA、语言技能SA）的传统分层智能体（HA）；2) 标准的端到端VLA模型（JARVIS-VLA）；3) 使用单一动作空间（MA或GA）训练的CoA-VLA模型；4) 使用“All-in-One”策略在MA和GA混合数据上训练的CoA-VLA模型（OpenHA）。</p>
<p>关键实验结果如下：首先，对不同动作空间的系统评估（见图2及相关分析）证实了其性能高度任务依赖。例如，在物体交互任务上，Grounding Action（GA）表现最佳；而在导航任务上，Motion Action（MA）更具优势；潜在动作（LA）在需要长序列规划的任务上表现良好。没有任何单一动作空间在所有任务类型上均占优。</p>
<p>其次，CoA框架有效提升了性能。在整体任务成功率上，采用GA作为中间步骤的CoA-VLA模型（CoA-GA）达到了67.5%，显著优于最好的HA基线（GA-HA，64.4%）和标准的VLA基线（65.5%）。</p>
<p><img src="" alt="不同动作空间性能对比分析图（示意图，论文中应有类似图2）"></p>
<blockquote>
<p><strong>图2</strong>：（根据论文描述，此部分应有展示不同动作空间在不同任务类别上性能对比的图表，例如柱状图或雷达图。）该图直观展示了不同抽象动作空间（RA, TA, MA, GA, LA, SA）在导航、物体交互等不同任务类别上的性能差异，验证了“最优动作表示具有任务依赖性”的核心发现。</p>
</blockquote>
<p>最后，“All-in-One”训练策略取得了最佳效果。在MA和GA混合数据上训练的OpenHA模型，整体成功率进一步提升至68.6%，超过了仅在单一动作空间（MA或GA）上训练的CoA-VLA专家模型。这表明融合多种动作表示有助于学习更通用和鲁棒的策略。</p>
<p>消融实验总结了各组件的贡献：1) <strong>动作空间选择</strong>：贡献在于揭示了任务依赖性，为CoA和All-in-One策略提供了动机；2) <strong>CoA框架</strong>：通过将抽象动作作为中间推理步骤，统一了规划与控制，带来了性能提升和推理灵活性；3) <strong>All-in-One训练</strong>：通过混合多种动作空间数据，使模型掌握了更丰富的技能组合，进一步提升了泛化能力和整体性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 通过大规模系统分析，首次实证揭示了在开放式环境中，最优的抽象动作表示是任务依赖的；2) 提出了动作链（CoA）框架，将抽象动作重新定义为中间推理步骤，从而在单一VLA模型中实现了高层规划与底层控制的协同，支持灵活的双模式推理；3) 提出了“All-in-One”训练策略，通过混合多种动作空间数据训练单一智能体，获得了超越专家模型的更通用策略；4) 开源了OpenHA套件，包含基准、数据集、代码和模型检查点，推动了可复现研究。</p>
<p>论文自身提到的局限性包括：CoA的慢速推理模式由于生成了更长的令牌序列，会增加计算成本；高质量、多样化的动作标注数据对于训练性能强大的通用智能体至关重要。</p>
<p>本文对后续研究的启示在于：为构建通用智能体提供了新的架构范式（CoA），证明了融合多种技能表示的价值（All-in-One）。未来的工作可以探索更高效的动作表示学习方法、将更多类型的动作空间纳入统一框架，以及将CoA范式应用于更广泛的具身智能和机器人领域。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对《我的世界》等开放环境中端到端智能体动作表示选择的难题，提出Chain of Action (CoA)框架。该方法将高层规划与底层控制统一于单一视觉-语言-动作模型中，将抽象动作视为类似思维链的中间推理步骤，以指导最终可执行动作的生成。通过在多样动作空间混合数据上训练“All-in-One”代理，模型实现了更鲁棒和可泛化的策略，在超过800项任务上取得了新的最高成功率，超越了专用基线模型。研究同时开源了OpenHA基准套件与全部模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.13347" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>