<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.11221" target="_blank" rel="noreferrer">2505.11221</a></span>
        <span>作者: Lee, Donghoon, Luu, Tung M., Lee, Younghwan, Yoo, Chang D.</span>
        <span>日期: 2025/05/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>强化学习（RL）在复杂决策问题中取得了成功，但其样本效率低下，特别是在高维和稀疏奖励环境中，需要大量环境交互，这限制了其在现实世界中的实际应用。与此同时，在大规模互联网数据上训练的多模态基础模型（FM）展现出作为通用智能体的卓越能力。然而，这些模型通常包含数百亿参数，计算成本高昂，难以部署在资源受限的系统中。现有方法要么依赖大型语言模型（LLM）作为规划器，但需要手工制作环境状态的文本描述；要么直接微调大型视觉语言模型（LVLM）作为策略，但这在资源受限的系统中不切实际。本文针对RL样本效率低和大型模型部署难这两个痛点，提出了一种新视角：将预训练的LVLM作为知识来源，通过知识蒸馏来指导一个紧凑的学生RL智能体。核心思路是：利用LVLM根据学生智能体收集的轨迹中的视觉观察来生成“教学动作”，学生智能体通过同时优化传统RL目标和模仿LVLM行为的蒸馏目标来加速学习，最终得到一个高效、可独立部署的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架名为LVLM2P，其核心是利用预训练的LVLM作为教师，将知识蒸馏到轻量级的学生RL智能体中。整体流程如下：学生策略与环境交互，收集轨迹（状态、动作、奖励、视觉观察）。在策略更新时，系统使用轨迹中的视觉观察和精心设计的提示词查询LVLM教师，获取其对每个观察状态下各动作的概率分布。然后，学生策略通过优化一个结合了传统RL损失和知识蒸馏损失的目标函数进行更新。</p>
<p><img src="https://arxiv.org/html/2505.11221v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：LVLM2P整体框架。左侧为学生智能体与环境交互并收集轨迹；右侧为利用轨迹中的视觉观察查询LVLM教师，获得动作概率分布；最终学生策略通过结合RL目标（如PPO或A2C）和KL散度蒸馏目标进行更新。</p>
</blockquote>
<p>核心模块包括LVLM教师代理和学生策略训练过程。</p>
<ol>
<li><strong>LVLM教师代理</strong>：教师策略定义为 π_T(a|o, ℓ^in)，其中o是视觉观察，ℓ^in是语言提示。查询过程分为两个阶段（如<strong>图2</strong>所示）：<ul>
<li><strong>分析阶段</strong>：给定视觉观察，LVLM被提示总结场景关键信息（如智能体和目标物体的坐标、方向）。</li>
<li><strong>动作推断阶段</strong>：利用第一阶段的分析结果和少量示例（few-shot）提示，LVLM输出在一组可能动作上的数值概率分布。使用软概率而非独热编码（hard label），旨在提供更丰富的信息并防止学生模型过拟合。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2505.11221v1/x2.png" alt="提示示例"></p>
<blockquote>
<p><strong>图2</strong>：GoToDoor任务中的提示示例。第一阶段，LVLM输出智能体（红色三角形）和目标物体（紫色门）的位置和方向等关键信息。第二阶段，LVLM被要求提供对可能动作的数值概率。</p>
</blockquote>
<ol start="2">
<li><strong>学生策略训练过程</strong>：学生策略 π_θ 的训练目标函数为：<br>ℒ(π_θ) = ℒ_RL(π_θ) + λ D_KL(π_T(·|o, ℓ^in) ∥ π_θ(·|s))<br>其中 ℒ_RL(π_θ) 是常规RL目标（如PPO或A2C），鼓励最大化长期回报；第二项是KL散度，用于知识蒸馏，使学生策略的动作分布与LVLM教师的分布对齐；λ 是平衡两项的系数。训练算法（对应论文Algorithm 1）概括为：收集轨迹、从轨迹中采样批次、查询LVLM教师获得动作概率、基于组合损失更新学生策略。</li>
</ol>
<p>与现有方法相比，LVLM2P的创新点主要体现在：1) <strong>直接利用视觉观察</strong>：无需为环境状态手工制作文本描述，增强了方法的通用性和适用性。2) <strong>使用软概率蒸馏</strong>：提供更丰富的监督信号。3) <strong>双目标优化</strong>：学生智能体既能从LVLM的指导中受益，减少早期无意义的探索，又能通过环境交互的RL反馈纠正教师可能存在的错误，最终获得超越教师性能的策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在MiniGrid环境的四个具有稀疏奖励的任务中进行：<em>LavaGap</em>（安全穿越）、<em>Dynamic Obstacles</em>（动态避障）、<em>Fetch</em>（识别并抓取指定物体）和<em>GoToDoor</em>（导航至指定门）。使用Gemini-1.5-Flash作为LVLM教师策略。学生策略的基线是标准的PPO和A2C算法。评估指标包括成功率和达到特定平均回报所需的训练样本数。</p>
<p><strong>对比方法</strong>：主要对比了原始PPO、原始A2C与集成了LVLM2P的PPO+LVLM2P、A2C+LVLM2P。</p>
<p><img src="https://arxiv.org/html/2505.11221v1/x3.png" alt="主要实验结果"></p>
<blockquote>
<p><strong>图3</strong>：<strong>上图</strong>：四个环境的平均成功率（越高越好）。<strong>下图</strong>：达到特定平均回报所需训练样本数（越低越好）。结果显示，集成LVLM2P后，PPO和A2C的性能收敛更快，样本效率显著提升。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li>LVLM教师策略在四个任务上平均成功率约为0.96，证明了其作为知识源的可靠性。</li>
<li>集成LVLM2P后，学生策略在所有任务上均比原始版本收敛更快。例如在LavaGap环境中，为达到0.91的平均回报，A2C+LVLM2P和PPO+LVLM2P分别需要9.18k和5.26k个样本，样本效率相比原始版本分别提升了2.18倍和2.11倍。</li>
<li><strong>整体样本效率提升</strong>：LVLM2P平均使PPO的样本效率提升了2.56倍，使A2C提升了2.86倍。</li>
</ol>
<p><strong>消融实验</strong>：<br>论文在LavaGap环境中进行了消融研究，关键结果如<strong>图4</strong>所示。</p>
<p><img src="https://arxiv.org/html/2505.11221v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：<strong>左图</strong>：比较使用LVLM教师提供的软概率分布与硬分布（独热编码）进行蒸馏的效果。软概率明显优于硬分布。<strong>右图</strong>：不同λ值对PPO+LVLM2P成功率的影响。λ=0.01时效果最佳，λ=0（即无蒸馏）时性能较差，λ过大（≥1）时性能下降。</p>
</blockquote>
<ul>
<li><strong>软概率 vs. 硬分布</strong>：使用软概率分布进行知识蒸馏显著优于使用硬分布，因为软概率包含了所有动作类别的概率信息，提供了更细腻的监督信号。</li>
<li><strong>平衡系数λ的影响</strong>：λ=0.01时取得最佳性能。λ=0意味着没有教师指导，性能与原始RL相当；λ过大（≥1）则会过度强调模仿教师，可能抑制了从环境交互中学习的能力，导致性能下降。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了 <strong>LVLM2P框架</strong>，一种通过蒸馏大型视觉语言模型知识来显著提高RL智能体样本效率的新方法。2) <strong>无需手工文本描述</strong>，直接利用LVLM处理视觉观察的能力，简化了流程并提高了适用性。3) 通过实验验证了该框架能<strong>平均提升2.5倍以上的样本效率</strong>，并可<strong>无缝集成</strong>到不同的RL算法（如PPO、A2C）中。</p>
<p>论文自身提到的局限性主要在于实验环境是基于网格的离散任务。作者指出，未来的工作可以将此框架扩展到连续控制任务中。</p>
<p>本工作对后续研究的启示在于：证明了大规模预训练的多模态模型（LVLM）可以作为有效的、泛化的“常识”知识库，用于加速针对特定任务的紧凑型智能体的训练。这为在资源受限的现实场景中部署高效能AI智能体提供了一条可行的技术路径，即利用大模型的“脑力”指导小模型的“成长”。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LVLM2P框架，以解决强化学习样本效率低、以及大型视觉语言模型参数量大难以部署的问题。核心方法是利用大型视觉语言模型作为教师，通过知识蒸馏指导强化学习智能体：LVLM根据智能体收集的轨迹提供指导性动作，减少早期无意义探索；同时直接从视觉观察生成动作建议，无需环境文本描述。实验表明，该方法显著提升了基线强化学习算法的样本效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.11221" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>