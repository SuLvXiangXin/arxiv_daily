<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.03129" target="_blank" rel="noreferrer">2508.03129</a></span>
        <span>作者: Somil Bansal Team</span>
        <span>日期: 2025-08-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习，特别是行为克隆，因其简单性和广泛适用性，已成为从专家演示中学习复杂机器人行为的基础技术。然而，学习到的策略可能因学习误差、外部干扰或协变量偏移而犯错，导致安全违规，这限制了其在安全关键应用中的部署。现有解决模仿学习安全性挑战的方法主要分为部署时（或测试时）和设计时（或训练时）策略。部署时的安全过滤器（如基于控制屏障函数、HJ可达性等方法）虽然提供理论保证，但在在线构建和更新上面临困难，尤其在全状态信息不可直接获取或高维动力学系统中实时计算非平凡，且其整体任务性能受限于底层学习策略的质量。设计时方法（如离/在策略方法、对抗模仿学习）主要旨在减少学习误差和缓解协变量偏移，从而降低不安全行为的可能性，但它们并未在数据收集或策略优化期间将安全作为首要目标进行显式建模，这意味着模仿策略仍可能在训练数据中未充分表示的罕见但高风险场景中失败。近期工作SafeGIL通过向专家演示注入最优对抗性扰动来弥补这一差距，但其依赖HJ可达性分析，存在维度诅咒问题，无法扩展到5-6个状态维度以上，且需要机器人动力学的解析表达式。</p>
<p>本文针对现有安全感知模仿学习方法（特别是SafeGIL）在可扩展性和对黑盒系统适用性方面的关键局限性，提出了一种新视角：利用基于采样的模型预测控制来近似合成最优对抗性扰动。核心思路是，在数据收集阶段，使用MPC引导生成的对抗性扰动主动将专家演示导向安全临界状态，从而让模仿策略能够观察并学习到鲁棒的恢复行为，最终提升策略的安全性。</p>
<h2 id="方法详解">方法详解</h2>
<p>MPC-SafeGIL是一个设计时框架，旨在通过向专家演示注入对抗性扰动来增强模仿学习的安全性。其整体流程如算法1所示：在数据收集阶段，对于每个状态，通过基于MPC的优化计算出一个最优对抗性扰动，将其加到专家策略的输出上，形成一个引导专家策略，并用该策略生成包含安全临界状态和专家恢复行为的数据集；随后，使用该数据集通过行为克隆训练模仿策略。</p>
<p><img src="https://arxiv.org/html/2508.03129v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：MPC-SafeGIL框架在真实Crazyflie四旋翼上的应用展示。(a)在结构化障碍环境中，四旋翼执行鲁棒恢复机动（橙色轨迹）——检测到障碍物（红色）后后退、调整航向（紫色），最终从可用开口退出（绿色）；(b)在密集杂乱环境中；(c)在具有移动障碍物的动态场景中。</p>
</blockquote>
<p>核心模块是<strong>对抗性扰动的计算与注入</strong>。方法的核心是将测试时可能出现的错误抽象为将系统推向不安全区域的对抗性扰动。为了计算最优扰动，本文从HJ可达性分析中获得灵感，将其形式化为一个两人零和动态博弈问题：控制试图阻止系统进入故障集，而扰动试图最小化到故障集的最小距离成本。直接求解此博弈在高维系统中计算困难。</p>
<p>关键的创新简化在于，在<strong>控制仿射动力学</strong>的假设下，该博弈问题可以转化为一个<strong>单人最优控制问题</strong>。通过将控制输入和扰动合并为单一输入，并调整其边界，原最大最小问题转化为一个最大化问题。这一转化显著降低了计算负担。</p>
<p>随后，本文采用<strong>基于采样的MPC算法——模型预测路径积分（MPPI）</strong> 来高效求解这个单人最优控制问题。具体步骤包括：1) 并行生成N个控制序列；2) 模拟动力学并评估成本函数；3) 选择成本最高的K个序列；4) 通过指数加权平均更新最优控制序列。得到最优控制序列后，提取其第一个控制输入，并根据控制仿射假设下的关系，恢复出原始博弈问题的最优扰动方向（公式6）。该扰动被注入专家策略，形成引导专家策略用于数据收集。</p>
<p><img src="https://arxiv.org/html/2508.03129v1/x2.png" alt="扰动场对比"></p>
<blockquote>
<p><strong>图2</strong>：在状态切片(px, py, 0)上的最优对抗性扰动场比较。灰色区域为障碍物，红蓝箭头指示不同扰动方向。(a)通过HJ可达性计算的扰动，黑点标记值函数空间梯度为零的状态；(b)通过MPPI计算的扰动。两者扰动场相似，验证了MPPI方法的有效性。</p>
</blockquote>
<p>与现有方法相比，MPC-SafeGIL的创新点具体体现在：1) 用可扩展的、基于采样的MPPI替代了计算昂贵的HJ可达性分析，突破了维度限制；2) 无需动力学的解析表达式，适用于黑盒系统；3) 将安全作为首要目标，在数据收集阶段进行主动、对抗性的引导，而非依赖随机扰动或事后修正。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了三个案例研究进行验证：四足机器人在杂乱环境中的导航（Isaac Gym仿真）、F1Tenth赛车模拟器的视觉运动导航，以及Crazyflie四旋翼在真实世界环境中的导航。</p>
<p>对比的基线方法包括：行为克隆（BC）、生成对抗模仿学习（GAIL）、不同的噪声注入方案（高斯噪声、均匀噪声）、DART（在线噪声协方差估计）以及DAgger（在策略方法）。</p>
<p><strong>四足导航实验</strong>：在随机生成、包含圆柱障碍物的环境中训练，在未见过的环境中测试。关键结果显示，MPC-SafeGIL consistently achieves lower collision rates，尤其在低数据区域。例如，在仅有80个演示时，MPC-SafeGIL将碰撞率降低至约25%，而BC的碰撞率更高。同时，MPC-SafeGIL也取得了更高的成功率。</p>
<p><img src="https://arxiv.org/html/2508.03129v1/x3.png" alt="四足导航结果"></p>
<blockquote>
<p><strong>图3</strong>：BC（黑色）与MPC-SafeGIL（橙色）在四足导航任务上的比较。(a)不同环境中收集的演示轨迹；(b)从80个演示学到的策略的 rollout；(c)碰撞率和成功率随演示数量的变化。MPC-SafeGIL实现了显著的安全提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.03129v1/x4.png" alt="与GAIL对比"></p>
<blockquote>
<p><strong>图4</strong>：MPC-SafeGIL与GAIL在四足任务上的成功率对比。即使碰撞率相近，MPC-SafeGIL也 consistently achieves higher success rates。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.03129v1/x5.png" alt="噪声方案对比"></p>
<blockquote>
<p><strong>图5</strong>：不同噪声注入方案在四足任务上的比较。MPC-SafeGIL achieves the lowest collision rate and the highest success rate。</p>
</blockquote>
<p><strong>消融实验</strong>：研究了扰动边界的影响。如表I所示，当相对扰动边界d_max/u_bar从0.3增加到0.7时，平均碰撞率从0.37降至0.19，表明更强的对抗性扰动能带来更丰富的恢复行为演示，从而提升策略鲁棒性。但边界过大可能导致无法恢复的状态，损害策略性能。</p>
<p><strong>F1Tenth实验</strong>：策略仅使用激光雷达输入。评估指标为行驶距离和碰撞率。</p>
<p><img src="https://arxiv.org/html/2508.03129v1/x6.png" alt="F1Tenth赛道"></p>
<blockquote>
<p><strong>图6</strong>：F1Tenth仿真赛道。(a)用于评估的训练地图；(b)用于测试泛化能力的未见地图。橙色轨迹展示了MPC-SafeGIL的成功导航。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.03129v1/x7.png" alt="F1Tenth结果"></p>
<blockquote>
<p><strong>图7</strong>：F1Tenth任务上MPC-SafeGIL与各基线的对比。红色虚线为专家性能。MPC-SafeGIL achieves the longest distance traveled and the lowest collision rate。</p>
</blockquote>
<p>结果显示，MPC-SafeGIL consistently outperforms all baselines，在低数据区域表现尤为突出，取得了最长的行驶距离和最低的碰撞率。DAgger在此任务中因脚本专家在任意状态查询时不可靠而表现不佳。</p>
<p><strong>真实四旋翼实验</strong>：在结构化、密集杂乱和动态障碍场景中进行了测试。MPC-SafeGIL学到的策略能够成功导航并执行复杂的恢复机动。</p>
<p><img src="https://arxiv.org/html/2508.03129v1/x8.png" alt="真实四旋翼轨迹"></p>
<blockquote>
<p><strong>图8</strong>：真实四旋翼实验的轨迹对比。MPC-SafeGIL（橙色）学到的策略能够进行规避和恢复，而BC（黑色）策略则发生碰撞。</p>
</blockquote>
<p><strong>与安全过滤器互补</strong>：实验还表明，MPC-SafeGIL可以与部署时的安全过滤器（如基于CBF的过滤器）无缝集成，在部署期间进一步提升安全性和任务性能。</p>
<p><img src="https://arxiv.org/html/2508.03129v1/figs/quadrotor/quadrotor_data.png" alt="与安全过滤器结合"></p>
<blockquote>
<p><strong>图9</strong>：MPC-SafeGIL与在线安全过滤器（CBF）结合的效果展示。两者结合能实现更安全、更高效的导航。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了MPC-SafeGIL框架，利用基于采样的MPC主动引导专家演示接近安全临界区域，实现了安全感知的模仿学习；2) 该方法可扩展至高维和黑盒动力学系统，突破了此前基于HJ可达性方法的限制；3) 证明了该方法能与在线安全机制互补，进一步改善部署时的安全与性能。</p>
<p>论文提到的局限性主要在于其简化依赖于控制仿射动力学的假设，尽管该假设在实践中限制性不强，且对于非仿射系统，该方法仍能提供有用的安全引导近似。此外，扰动边界的选择需要权衡，过大的边界可能导致演示信息性下降。</p>
<p>本工作对后续研究的启示在于：强调了在模仿学习的设计阶段（数据收集）主动融入安全考量的重要性，而非仅仅依赖测试时的修正。它展示了基于采样的优化方法（如MPPI）在安全引导中的潜力，为在高维、复杂或模型未知的系统中实现安全、鲁棒的模仿学习提供了一条可扩展的路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MPC-SafeGIL方法，旨在解决模仿学习中因策略误差导致的安全风险这一核心问题。该方法通过在专家演示数据中注入对抗性扰动，使策略学习到鲁棒的恢复行为。关键技术是利用基于采样的模型预测控制来近似最坏情况扰动，从而将安全考量直接集成到数据收集阶段，适用于高维和黑盒动力学系统。通过在四足机器人运动、视觉导航仿真及真实四旋翼飞行器上的实验验证，该方法在安全性和任务性能上均取得了提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.03129" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>