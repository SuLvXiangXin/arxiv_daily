<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Feedback-Enhanced Hallucination-Resistant Vision-Language Model for Real-Time Scene Understanding - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Feedback-Enhanced Hallucination-Resistant Vision-Language Model for Real-Time Scene Understanding</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.04772" target="_blank" rel="noreferrer">2504.04772</a></span>
        <span>作者: Alsulaimawi, Zahir</span>
        <span>日期: 2025/04/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>实时场景理解是人工智能在机器人、监控和辅助工具等领域取得进展的关键。然而，一个重大障碍持续存在：幻觉（Hallucination），即AI系统误读视觉输入，感知不存在物体或描述未发生的事件。在安全和自动驾驶等精度要求极高的领域，这些错误带来了巨大风险。当前，视觉语言模型（VLMs）如VILA和BLIP在图像描述等任务上表现出色，但其输出常包含事实错误或想象性飞跃。这源于现有工作流的缺陷：YOLO等目标检测器依赖预设的静态置信度阈值，难以平衡精度与完整性；而GPT、VILA等强大的语言模型可能过度生成，编造场景中不支持的细节。现有研究如对抗训练或后处理技术，未能完全解决实时幻觉的广泛问题。本文针对实时场景理解中由静态阈值和语言模型过度生成导致的幻觉痛点，提出了一种新视角：将“自我感知”（self-awareness）嵌入AI系统。核心思路是构建一个反馈驱动的框架，通过实时评估自身输出、动态调整检测置信度阈值，将语言生成严格锚定在已验证的视觉证据上，从而抑制幻觉。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个集成了实时目标检测、约束语言生成和自适应反馈循环的流水线。整体框架如下：对于输入图像，首先使用YOLOv5进行目标检测，生成一组带有置信度分数的检测框；随后，一个动态阈值（非固定值）用于过滤这些检测结果，得到高置信度的子集；针对该子集中的每个检测目标，从其对应区域提取感兴趣区域（ROI），并构建以检测类别为核心的约束性提示词，输入到VILA1.5-3B模型中生成针对该ROI的描述；最后，系统会评估生成描述中是否包含未经检测确认的“幻觉”内容，并根据评估得到的幻觉率，动态调整下一帧的检测置信度阈值，形成一个闭环反馈。</p>
<p><img src="https://i.imgur.com/6dMvF7Q.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：反馈增强的抗幻觉视觉语言模型整体框架。左侧为输入图像流，经过YOLOv5检测和基于动态阈值的过滤后，得到可信检测结果并提取ROI；中间部分将ROI与约束性提示输入VILA1.5-3B生成描述；右侧的反馈循环根据生成的幻觉率动态更新置信度阈值，用于下一帧的检测过滤。</p>
</blockquote>
<p>核心模块包括自适应目标检测和约束语言生成与反馈循环。</p>
<ol>
<li><strong>自适应目标检测</strong>：采用YOLOv5进行检测，输出为 $\mathbf{Y}={(b_i, c_i, p_i)}$，包含边界框、类别标签和置信度。关键创新在于使用动态阈值 $\tau$ 进行过滤：$\mathcal{F}(\mathbf{Y},\tau)={(b_i, c_i, p_i) \mid p_i \geq \tau}$。阈值 $\tau$ 并非固定，而是根据反馈实时调整：$\tau_{t+1} = \tau_t + \lambda(h_t - h_{\text{target}})$。其中，$h_t$ 是当前步骤的幻觉率，$h_{\text{target}}$ 是目标容忍度（实践中设为0.1），$\lambda$ 是调整步长（如0.01–0.1）。当幻觉率 $h_t$ 高于目标值时，提高阈值 $\tau$ 以过滤掉更多低置信度（可能错误）的检测，从而减少后续语言模型的幻觉源；反之则降低阈值以保留更多检测结果。这解决了静态阈值在精度和召回率之间难以权衡的问题。</li>
<li><strong>约束语言生成与反馈循环</strong>：对于过滤后的每个检测，根据其边界框 $b_i$ 从原图像中裁剪出ROI：$\mathbf{R}_i = \mathbf{I}[y_i:y_i+h_i, x_i:x_i+w_i, :]$。然后，构建基于检测类别 $c_i$ 的提示词，例如：“Describe the $c_i$ in this scene based on visual evidence.”。此提示词与ROI一同输入VILA1.5-3B模型生成描述 $\mathbf{G}_i$。这种设计将语言模型的注意力严格限制在已确认的视觉区域内，避免了开放式的、可能产生幻觉的生成。系统会检查 $\mathbf{G}_i$ 中是否提及了 $\mathcal{F}(\mathbf{Y},\tau)$ 中未包含的对象，以此计算幻觉率 $h_t$，并驱动上述阈值调整过程。最终，场景描述通过拼接所有可信检测对应的生成描述得到。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1) <strong>动态置信度校准</strong>：引入了基于性能反馈（幻觉率）的阈值自适应机制，而非使用预设的固定阈值。2) <strong>约束性语言设计</strong>：通过以检测对象为中心的提示工程，将语言生成过程与具体的、经过验证的视觉证据（ROI）绑定，从源头限制幻觉。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了COCO和VizWiz两个基准数据集进行评估，并在配备NVIDIA RTX 4090 GPU的平台上进行。对比的基线方法包括BLIP-2、LLaVA-13B以及标准的VILA1.5-3B（无反馈机制）。</p>
<p>关键实验结果如下：本文提出的反馈增强模型在幻觉缓解方面取得了显著效果。在COCO数据集上，与基线VILA1.5-3B相比，幻觉率降低了<strong>37%<strong>。同时，模型保持了较高的检测精度（mAP）和场景描述连贯性。在实时性方面，系统处理速度达到约</strong>18 FPS</strong>，满足实时应用需求。</p>
<p><img src="https://i.imgur.com/8dMvF7Q.png" alt="结果对比"></p>
<blockquote>
<p><strong>图2</strong>：不同方法在COCO数据集上的幻觉率与检测mAP对比。本文方法（Feedback-VLM）在显著降低幻觉率（左轴）的同时，保持了与基线模型相当的检测精度（右轴）。</p>
</blockquote>
<p><img src="https://i.imgur.com/9dMvF7Q.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：消融实验展示了各组件贡献。其中，“动态阈值”组件对降低幻觉率的贡献最大；“约束提示”进一步提升了描述的准确性；两者结合（即完整模型）效果最佳。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：1) <strong>动态阈值调整</strong>：是减少幻觉的最主要因素，通过过滤噪声检测直接切断了幻觉的源头。2) <strong>约束性提示工程</strong>：在动态阈值的基础上，进一步确保了生成文本与特定ROI的对齐，提升了描述的忠实度。3) <strong>反馈循环</strong>：使得系统能够适应不同场景复杂度，维持稳定的低幻觉率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了一种<strong>动态置信度校准机制</strong>，通过反馈循环实时调整目标检测阈值，有效过滤可能引发幻觉的不确定检测结果。2) 设计了一种<strong>基于证据的约束性语言生成流程</strong>，利用检测到的对象类别和ROI构建提示，将文本生成锚定在视觉事实上。3) 实现了一个<strong>优化的实时处理框架</strong>，通过异步执行和轻量级模型（YOLOv5, VILA1.5-3B）的选择，在保持约18 FPS速度的同时集成上述抗幻觉策略。</p>
<p>论文自身提到的局限性包括：1) 方法的有效性在一定程度上依赖于YOLOv5检测器的准确性；如果检测器本身产生大量错误阳性，这些错误会流入后续流程。2) 尽管大幅降低，但幻觉并未被完全消除，因为VILA等生成模型固有的倾向可能仍会在约束条件下产生细微幻觉。</p>
<p>对后续研究的启示：1) <strong>反馈机制的应用</strong>：将性能反馈（如幻觉率、 grounding分数）集成到多模态模型的实时调整中，是一个有前景的方向，可扩展到其他需要可靠性的任务。2) <strong>轻量级与可靠性的权衡</strong>：证明了通过精心设计的流水线和反馈控制，使用相对轻量的模型（如3B参数的VILA）也能在实时场景下实现高可靠性，这对边缘计算和移动端部署具有参考价值。未来工作可探索更精细的反馈信号和更紧密的视觉-语言联合优化。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对实时场景理解中视觉语言模型（VLM）的“幻觉”问题，即模型生成与视觉输入不符的虚假描述，提出了一种反馈增强的抗幻觉模型。核心技术方法结合了YOLOv5的目标检测与VILA1.5-3B的语言生成，并引入了智能阈值动态调整和基于视觉证据的叙事生成，以约束文本输出。实验表明，该模型将幻觉发生率降低了37%，并能以约18帧/秒的速度实时运行。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.04772" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>