<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computation and Language (cs.CL)</span>
      <h1>Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.02204" target="_blank" rel="noreferrer">2510.02204</a></span>
        <span>作者: Dong, Lingzhong, Zhou, Ziqi, Yang, Shuaibo, Sheng, Haiyue, Cheng, Pengzhou, Wu, Zongru, Wu, Zheng, Liu, Gongshen, Zhang, Zhuosheng</span>
        <span>日期: 2025/10/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，由视觉语言模型驱动的移动使用智能体在解释自然语言指令并基于移动图形用户界面生成相应动作方面展现出巨大潜力。主流方法通常采用思维链推理来提升执行准确率，因为中间推理步骤有助于模型分解复杂指令并与用户意图对齐。然而，现有的评估实践主要强调执行准确率，使用精确匹配作为主导指标，却忽视了CoT推理是否与真实动作一致。这种疏忽导致无法评估潜在的推理-执行差距，进而可能引发过度信任风险：用户依赖看似合理的CoT可能无意中授权有害动作，造成经济损失或信任危机。本文针对这一具体痛点，提出了一个评估推理与执行是否对齐的新视角。核心思路是引入一个名为“真实对齐”的新评估指标来衡量CoT隐含的动作是否与真实动作匹配，并结合传统的EM指标，形成一个四象限诊断框架，以系统性地量化和分析推理-执行差距。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法核心是一个用于诊断推理-执行差距的评估框架。该框架不改变智能体本身，而是对其输出进行事后分析。整体流程是：给定一个VLM移动智能体生成的CoT推理 <code>c_n</code> 和预测动作 <code>a_n</code>，以及对应的真实动作 <code>a_n*</code>、当前屏幕截图 <code>o_n</code> 和历史上下文 <code>H_n</code>，首先使用精确匹配评估执行准确率，然后使用新提出的真实对齐指标评估推理准确率，最后将两者结合进行四象限分类。</p>
<p>核心模块是<strong>真实对齐评估器</strong>。其具体作用是自动将自由形式的CoT推理 <code>c_n</code> 映射为其隐含的可执行动作 <code>f(c_n)</code>。技术细节上，该映射被形式化为 <code>f(c_n) = arg max_a P(a | c_n, H_n, o_n)</code>，即一个指令遵循VLM在给定CoT、历史和当前观察的条件下，预测最可能的动作。评估器保留了 <code>(H_n, o_n)</code> 作为输入，因为CoT中常包含未明确指定或依赖于上下文的指代。评估时采用确定性解码以保证可复现性。最终，通过比较 <code>f(c_n)</code> 与真实动作 <code>a_n*</code> 是否完全匹配（动作类型和所有参数）来计算GTA分数。</p>
<p>与现有方法相比，本文的创新点具体体现在：首次系统性地提出了量化推理-执行差距的评估框架，并明确区分了两种差距类型——<strong>执行差距</strong>（推理正确但执行错误）和<strong>推理差距</strong>（执行正确但推理错误）。这通过结合EM和GTA两个指标实现。</p>
<p><img src="https://arxiv.org/html/2510.02204v1/x1.png" alt="四象限诊断框架"></p>
<blockquote>
<p><strong>图1</strong>：推理-执行差距的四象限诊断框架。横轴代表执行准确率（EM），纵轴代表推理准确率（GTA）。Q1（理想）：推理和执行均正确；Q2（执行差距，EG）：推理正确但执行失败；Q3（两者皆错）：推理和执行均错误；Q4（推理差距，RG）：执行正确但推理失败。</p>
</blockquote>
<p>基于此框架，进一步定义了两种诊断率：<code>EG = (1/N) * Σ 1{GTA_n=1 ∧ EM_n=0}</code>，量化推理正确但执行错误的步骤比例；<code>RG = (1/N) * Σ 1{GTA_n=0 ∧ EM_n=1}</code>，量化执行正确但推理错误的步骤比例。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验使用了三个移动交互基准数据集：AITZ、CAGUI和AndroidControl（采用其高级指令分割）。对比了三个先进的、开源的VLM移动智能体：AgentCPM-GUI、UI-TARS和GUI-Owl。GTA评估器基于AgentCPM-GUI-8B实现。为了验证评估器的可靠性，从模型和数据集进行了分层抽样，构建了包含1800个实例的子集进行人工标注和一致性分析。</p>
<p><img src="https://arxiv.org/html/2510.02204v1/figures/dataset_distribution0.png" alt="数据集分布"></p>
<blockquote>
<p><strong>图2</strong>：原始数据集（左）与分层抽样子集（右）的动作分布对比。抽样过程保留了整体动作分布特征，同时确保了有代表性的少数情况得到充分覆盖。</p>
</blockquote>
<p><strong>评估器可靠性</strong>：通过比较GTA评估器的预测与人工共识标签来计算其准确性。雷达图显示，评估器在三个模型和三个数据集上均达到了较高且稳定的准确率，在AndroidControl上最高，在CAGUI和AITZ上略低。这表明该自动评估器是人工评估的可信代理，为大规模分析提供了基础。</p>
<p><img src="https://arxiv.org/html/2510.02204v1/figures/dataset_distribution1.png" alt="评估器准确性雷达图"></p>
<blockquote>
<p><strong>图3</strong>：GTA评估器在三个模型和三个数据集上的准确性雷达图。评估器整体上取得了高且一致的准确率，在AndroidControl上表现最佳。</p>
</blockquote>
<p><strong>模型性能评估</strong>：关键实验结果通过EM和GTA的对比来呈现。在18个模型-数据集组合中，有14个案例的GTA分数高于EM分数，表明<strong>执行差距普遍存在且通常多于推理差距</strong>。这说明主要瓶颈在于将正确的推理转化为可执行的动作。例如，在CAGUI这个分布外基准上，未经训练的模型显示出高GTA但非常大的EG，表明主要挑战在于将推理落地到不熟悉的屏幕上。同时，也观察到反例，如在AITZ上，AgentCPM和UI-TARS的EM高于GTA，表明模型严重依赖监督微调中学到的动作捷径，而忽视甚至与其CoT推理相矛盾。</p>
<p><img src="https://arxiv.org/html/2510.02204v1/x2.png" alt="模型性能曲线图"></p>
<blockquote>
<p><strong>图4</strong>：GTA、EM和IDEAL（两者均正确）的曲线图。当GTA位于EM之上时，表示EG &gt; RG，揭示了将正确推理转化为可执行动作是主要瓶颈。</p>
</blockquote>
<p><strong>参数缩放效应</strong>：通过评估UI-Tars系列不同参数规模（2B, 7B, 72B）和训练范式（SFT和DPO）的模型发现，扩大模型规模能一致地提升EM和GTA，同时降低EG和RG，表明缩放有助于缩小推理-执行差距。然而，<strong>即使最大的72B模型仍然存在超过10%的执行差距</strong>，这表明仅靠缩放无法完全消除此类差距。</p>
<p><img src="https://arxiv.org/html/2510.02204v1/x3.png" alt="参数缩放效应图"></p>
<blockquote>
<p><strong>图5</strong>：参数缩放对AndroidControl上推理-执行差距的影响。(a) 正向指标EM和GTA（越高越好）；(b) 负向指标EG和RG（越低越好）。橙色点代表DPO模型，蓝色点代表SFT模型，点的大小与参数规模成正比。缩放能持续改善性能，但最大的模型仍存在显著执行差距。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>真实对齐</strong>这一新评估指标及相应的四象限诊断框架，首次系统性地解耦并量化了VLM移动智能体的推理准确率和执行准确率，揭示了执行差距和推理差距两种关键失败模式。2) 设计并验证了一个<strong>自动化的GTA评估器</strong>，其判断与人工标注高度一致，为实现大规模、可复现的推理诊断提供了可行工具。3) 通过广泛的实验分析，揭示了<strong>执行差距是当前移动智能体的主要瓶颈</strong>，且即使扩大模型规模也无法完全消除，为未来研究指明了具体方向。</p>
<p>论文自身提到的局限性主要在于GTA评估器可能存在的误差，尽管其与人工标注高度一致，但并非完美。</p>
<p>本文的发现对后续研究有多重启示：首先，在提升移动智能体性能时，需要<strong>特别关注从推理到动作的生成环节</strong>，而不仅仅是提升推理能力。其次，在模型训练（尤其是强化微调）中，应避免模型过度依赖动作捷径而忽视推理的忠实性，需要设计鼓励推理-执行一致的训练目标。最后，本文提出的诊断框架可以作为一种有效的分析工具，用于指导模型设计、训练策略选择以及安全性评估，助力开发更可信赖的移动使用智能体。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对VLM驱动的移动代理中**推理与执行不一致**的核心问题，提出新的评估框架。通过引入**真实对齐指标**，与精确匹配指标结合，共同评估推理与执行的准确性。实验发现，在多种移动交互任务中，**执行差距比推理差距更普遍**；即使扩大模型规模，执行差距依然显著存在。该框架为开发更可信的移动代理提供了具体诊断依据。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.02204" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>