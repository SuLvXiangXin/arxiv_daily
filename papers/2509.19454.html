<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19454" target="_blank" rel="noreferrer">2509.19454</a></span>
        <span>作者: Daniel Seita Team</span>
        <span>日期: 2025-09-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在双臂操作领域，基于视觉的模仿学习需要大量且多样化的演示数据来覆盖广泛的机器人姿态、接触和场景上下文。然而，收集真实世界的演示数据成本高昂、耗时费力，限制了策略的可扩展性。现有的数据增强方法主要针对单臂操作、手腕相机（眼在手）的RGB输入，或者专注于生成没有配对动作标签的新图像。对于使用第三人称（眼在外）RGB-D数据进行训练，并能同时生成新动作标签的双臂操作数据增强，相关研究相对较少。本文针对这一痛点，提出了一种名为ROPA的离线模仿学习数据增强方法。其核心思路是微调Stable Diffusion模型，以合成具有新颖机器人姿态的第三人称RGB和RGB-D观察图像，并同时生成对应的关节空间动作标签，通过约束优化确保双臂场景中抓取器与物体接触的物理一致性。</p>
<h2 id="方法详解">方法详解</h2>
<p>ROPA方法旨在利用条件扩散模型生成新的机器人姿态图像，同时保持场景上下文。整体流程分为三个核心阶段：1）基于专家演示微调Stable Diffusion模型以实现姿态引导的机器人图像合成；2）生成目标姿态的骨架图像作为控制条件；3）生成动作标签并重构数据集。</p>
<p><img src="https://arxiv.org/html/2509.19454v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ROPA方法总览。（1）骨架姿态生成器根据相机内外参、目标关节位置以及左右机器人基座位置，生成代表目标关节配置的骨架姿态图像 (I_{t}^{p})。（2）源图像 (I_{t}^{s}) 和语言目标 (g) 输入到Stable Diffusion（底部U-Net），而生成的骨架姿态作为控制输入给ControlNet（顶部U-Net），生成目标图像 (I_{t}^{d})。锁形图标代表参数被冻结。（3）原始数据集被复制，并在每隔 (k) 个时间步用生成的目标状态替换原始状态，并更新对应的动作标签。这个增强数据集与原始数据集结合，用于训练双臂操作策略。</p>
</blockquote>
<p><strong>核心模块1：姿态引导机器人图像合成</strong><br>该方法受姿态引导人物图像合成启发，旨在学习一个映射函数 (I_{t}^{d}=f_{\psi}(I_{t}^{s}, I_{t}^{p}, g))。具体实现上，(f_{\psi}) 是一个基于Stable Diffusion 2.1并利用ControlNet进行微调的模型。ControlNet采用双流架构：一个冻结的Stable Diffusion U-Net处理噪声潜变量和文本嵌入；一个可训练的Stable Diffusion U-Net副本作为编码器流处理条件图像（源图像和骨架姿态图）。条件图像被编码为潜变量 (z_s) 和 (z_{pose})，文本目标 (g) 被编码为 (c_{text})。可训练的ControlNet模块 (\mathcal{C}<em>{\phi}) 处理条件输入，并通过零初始化的卷积层将姿态感知的特征图注入到冻结的U-Net去噪器 (\epsilon</em>{\phi}) 中。训练时，模型通过最小化损失函数 (\mathcal{L}=\mathbb{E}\big|\epsilon-\epsilon_{\phi}(z_{d,n}, c_{text}, n; \mathcal{C}<em>{\phi}(z_s, z</em>{pose}))\big|_{2}^{2}) 来预测添加到目标潜变量 (z_d) 中的噪声 (\epsilon)。推理时，使用DDIM采样在ControlNet引导下对随机高斯潜变量进行迭代去噪，生成最终目标图像。</p>
<p><strong>核心模块2：骨架姿态生成器</strong><br>为了给扩散模型提供明确的空间结构和运动学约束，该方法构建了机器人手臂的骨架表示。使用圆柱体表示骨骼段，球体表示关节连接器，并为球体应用了来自GENIMA的条纹纹理以提供关节方向和旋转状态的视觉线索。生成目标骨架图像 (I_{t}^{p}) 时，首先对源末端执行器位姿施加变换 (\Delta\rho)，并通过逆运动学求解得到目标关节位置。然后，在虚拟环境中使用PyRender配置机器人模型至这些关节位置，并设置虚拟相机参数与源图像 (I_{t}^{s}) 的相机内外参完全一致，从而渲染出与真实相机视图几何一致的骨架图像。</p>
<p><img src="https://arxiv.org/html/2509.19454v1/x3.png" alt="骨架姿态消融"></p>
<blockquote>
<p><strong>图3</strong>：骨架姿态消融与可视化。比较了不同骨架姿态格式：（1）ROPA的骨架姿态（带纹理球体），（2）受OpenPose启发的骨架姿态，（3）全白骨架姿态（对比度低）。（4）展示了ROPA骨架姿态与源图像的精确对齐。（5）显示多视图生成的源图像输入，（6）展示叠加在同一源图像上的双臂骨架姿态。</p>
</blockquote>
<p><strong>核心模块3：动作标注与数据集构建</strong><br>为确保生成的姿态在物理上可行，该方法将任务分解为无接触和接触丰富的状态，并采用基于力的接触检测方法。对于无接触状态，对源关节位置进行均匀采样的姿态扰动 (\Delta\rho)；对于接触阶段，则采用约束优化来确保运动学可行的姿态同时保持接触关系。通过求解逆运动学，将末端执行器的变换 (\Delta\rho) 转换为关节空间的扰动动作 (\tilde{\mathbf{a}}_t)。通过复制原始数据集，并每隔 (k)（论文中设为8）个时间步用生成的新状态和对应的扰动动作替换原始数据，构建出增强数据集 (\tilde{\mathcal{D}})，最终与原始数据集合并用于策略训练。</p>
<p><strong>多模态与多视图扩展</strong><br>该方法可扩展至RGB-D数据。对于深度图像增强，将深度着色图作为源图像输入，并让ControlNet同时以对应的RGB目标图像和骨架姿态图像为条件，通过拼接这两种RGB图像（共6个通道）作为输入，确保生成的深度图像与RGB图像在几何上保持一致。</p>
<p><img src="https://arxiv.org/html/2509.19454v1/x4.png" alt="深度生成流程"></p>
<blockquote>
<p><strong>图4</strong>：深度图像生成流程。（1）源深度着色图输入到Stable Diffusion。（2）RGB目标图像和骨架姿态作为ControlNet的条件输入。（3）生成的目标深度图像。</p>
</blockquote>
<p>此外，ROPA支持最多四个相机的多视图设置，通过将多个相机观察图像拼接成一个复合图像来实现多视图一致的图像生成。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境中，基于PerAct2基准和RLBench，评估了5个双臂操作任务：协调举球、协调举托盘、协调推箱子、双臂拉直绳子（简易版）、协调将物品放入抽屉。对比的基线方法包括：无数据增强的ACT、使用额外100条真实演示的ACT、以及扩散模型新颖视图合成方法VISTA。在真实世界中，使用两个UR5机器人和RealSense D415相机，在举球、推方块、举抽屉三个任务上进行了评估。</p>
<p><strong>关键实验结果</strong>：<br>模拟实验结果显示，ROPA在所有5个任务上均优于基线方法。</p>
<p><img src="https://arxiv.org/html/2509.19454v1/x7.png" alt="模拟结果表格"></p>
<blockquote>
<p><strong>图7/表I</strong>：模拟实验结果。ROPA与四种基线方法在RGB和RGB-D模式下的成功率对比（三次随机种子平均）。ROPA（绿色高亮行）在所有任务和两种模态下均取得了最佳或极具竞争力的成功率。例如，在RGB模式的协调举球任务中，ROPA达到68.0%，显著高于无增强ACT的41.3%和VISTA的52.7%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19454v1/x10.png" alt="真实结果表格"></p>
<blockquote>
<p><strong>图10/表III</strong>：真实世界实验结果。展示了在三个真实任务上的成功次数（共20次试验）。ROPA在举球和举抽屉任务上表现最佳，在推方块任务上与无增强ACT表现相当。VISTA在需要精确接触的任务（举抽屉）上表现不佳。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br><img src="https://arxiv.org/html/2509.19454v1/x9.png" alt="消融实验表格"></p>
<blockquote>
<p><strong>图9/表IV</strong>：消融实验结果。比较了不同骨架姿态表示的影响。使用原始关节位置向量代替骨架图像会导致性能下降（46.7% vs 68.0%）。使用全白骨架（缺乏视觉线索）性能最差（32.0%）。使用OpenPose式骨架效果较好（62.6%），但结合了GENIMA纹理的ROPA完整骨架表现最佳，证明了其提供的丰富空间和方向线索的有效性。</p>
</blockquote>
<p>此外，多相机实验表明，使用四个相机拼接视图能进一步提升性能（单相机ROPA: 68.0%，四相机ROPA: 80.0%）。泛化实验表明，在目标任务上直接微调模型效果最好，但在数据有限时，零样本或少样本方法也能取得合理结果。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ROPA，一种新颖的、基于姿态引导图像合成的双臂操作离线数据增强方法，支持从RGB和RGB-D数据中学习。</li>
<li>实现了深度图像合成，生成的深度图与增强的关节位置和RGB图像保持一致。</li>
<li>实现了动作一致的增强，输出图像和关节空间标签，并通过约束确保动作可行性，在模拟和真实实验中显著提升了双臂策略的性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法的计算成本较高，因为需要对扩散模型进行微调和采样。此外，生成的图像可能存在伪影，特别是在少样本或零样本设置下。</p>
<p><strong>后续启示</strong>：ROPA展示了利用生成模型合成具有物理一致性的机器人姿态-动作数据对的潜力。未来的工作可以探索更高效的生成架构，将方法扩展到更复杂的场景和动态物体，并研究如何更好地结合多模态信息（如力觉）来进一步约束和提升生成数据的质量和多样性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双手操作模仿学习数据稀缺、收集成本高的问题，提出ROPA方法。核心技术是通过微调Stable Diffusion，合成具有新机器人姿态的第三人称RGB/RGB-D观测图像，并同步生成对应的关节空间动作标签；同时采用约束优化确保双手抓取物体时的物理接触一致性。在5个模拟任务和3个真实任务上的评估表明，ROPA在2625次模拟试验和300次真实试验中均优于基线方法，验证了其在RGB与RGB-D数据增强方面的有效性和可扩展性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19454" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>