<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.06508" target="_blank" rel="noreferrer">2602.06508</a></span>
        <span>作者: Mike Zheng Shou Team</span>
        <span>日期: 2026-02-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于世界模型的机器人学习主要有三种范式：手工构建的数字孪生、基于3D重建的环境以及动作条件视频世界模型。其中，动作条件视频世界模型利用预训练先验，泛化性较好，但存在一个关键缺陷：<strong>动作跟随精度不足</strong>。现有模型（如Cosmos-Predict 2）在给定错误动作时，仍可能“幻想”出成功的未来观测，这表明模型过度依赖视觉先验而非真实的物理动力学，导致其预测轨迹与实际执行结果严重偏离。这种不精确性使得现有视频世界模型无法作为可靠的奖励函数来支持有效的强化学习（RL）策略优化。</p>
<p>本文针对视频世界模型<strong>动作跟随不精确</strong>和<strong>奖励信号不可靠</strong>这两个核心痛点，提出了一个协同演化的新视角。核心思路是：构建一个<strong>闭环框架</strong>，让世界模型与视觉-语言-动作（VLA）策略在迭代中相互促进、共同优化——世界模型为VLA策略提供高保真的虚拟环境进行RL后训练，而VLA策略在训练中产生的失败轨迹又被反馈回来，用于精炼世界模型，从而提升其对动作-结果对齐的建模精度。</p>
<h2 id="方法详解">方法详解</h2>
<p>World-VLA-Loop框架包含四个阶段，形成一个闭环。</p>
<p><img src="https://arxiv.org/html/2602.06508v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：World-VLA-Loop完整流程。包含四个阶段：(1) 通过人工遥操作和策略 rollout 构建 SANS 数据集；(2) 在 SANS 数据集上，使用联合奖励和视频监督预训练动作条件世界模型；(3) 在世界模型内执行 VLA 策略 rollout，进行 GRPO 优化；(4) 部署精炼后的策略收集新的失败和成功数据，用于进一步扩充 SANS 数据集。此循环实现了世界模型和 VLA 策略的联合优化。</p>
</blockquote>
<p><strong>1. SANS（成功与接近成功）数据集构建</strong><br>现有机器人数据集主要包含成功轨迹，用于模仿学习，但缺乏对失败模式的覆盖，导致训练出的世界模型无法可靠模拟物理上合理的失败结果。本文构建的SANS数据集<strong>同时包含成功轨迹和接近成功的失败轨迹</strong>。这些“接近成功”的轨迹（例如因末端执行器微小定位误差导致的失败）对于训练鲁棒的世界模型至关重要：一方面，它们与成功轨迹难以区分，迫使模型关注空间动力学的细粒度差异；另一方面，它们能确保虚拟环境更准确地反映策略 rollout 中实际遇到的失败模式。数据收集覆盖了ManiSkill、LIBERO仿真环境和真实机器人场景，总计约35k个视频-动作对，用于世界模型预训练。</p>
<p><strong>2. 状态感知视频世界模拟器</strong><br>世界模型基于Cosmos-Predict 2构建。给定历史观测帧 (x_{0}, \dots, x_{h-1}) 和未来 (T) 步机器人动作 (a_{1}, \dots, a_{T})，模型需要预测未来观测 (x_{h}, \dots, x_{h+T-1}) 和对应的奖励信号 (r_{1}, \dots, r_{T})。</p>
<p><img src="https://arxiv.org/html/2602.06508v1/x4.png" alt="世界模型架构"></p>
<blockquote>
<p><strong>图4</strong>：状态感知视频世界模拟器架构。模型以历史帧和未来动作为条件，通过扩散过程在潜在空间生成未来帧，并同时预测密集的奖励信号。奖励预测头与视频生成共享时空特征，确保了奖励与视觉结果的一致性。</p>
</blockquote>
<p>模型的核心创新在于<strong>联合视频与奖励监督</strong>。具体而言，模型在扩散模型的潜在空间进行训练，不仅学习重建未来的视频帧，还通过一个额外的奖励预测头来预测每一步的奖励。奖励信号是二值的（成功/失败），来源于数据收集时记录的状态信息。这种设计使得模型能够作为一个高保真的交互式模拟器，同时提供逼真的视觉观测和内在的奖励信号，从而支持策略的RL训练。</p>
<p><strong>3. 在世界模型中进行VLA策略的RL后训练</strong><br>将训练好的世界模型作为虚拟环境，用于对预训练的VLA策略（如OpenVLA-OFT）进行强化学习后训练。采用Group Relative Policy Optimization (GRPO)算法进行优化。策略在世界模型中产生rollout轨迹，世界模型则提供对应的观测和奖励，用于计算策略梯度并更新策略参数。</p>
<p><strong>4. 闭环数据收集与模型迭代</strong><br>将经过一轮RL精炼后的VLA策略部署到真实世界（或仿真环境）中执行，收集其产生的<strong>新的失败轨迹和成功轨迹</strong>。这些新数据，特别是策略探索产生的自然失败模式，被加入到SANS数据集中。随后，用扩充后的数据集<strong>重新微调（refine）世界模型</strong>，提升其对于当前策略行为分布下的动作跟随精度和奖励可靠性。更新后的世界模型又能为下一轮的策略RL训练提供更好的模拟环境，如此形成协同演化的闭环。</p>
<p><strong>创新点总结</strong>：1) 提出了世界模型与VLA策略协同演化的闭环范式；2) 引入了包含接近成功轨迹的SANS数据集，以提升动作-结果对齐；3) 设计了联合预测未来观测和奖励的状态感知世界模型架构。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真环境LIBERO基准和真实机器人场景（涉及拾放、堆叠等任务）上进行评估。Baseline包括：原始VLA策略（OpenVLA-OFT）、在真实世界进行RL后训练的VLA策略（Real-World RL），以及使用现有世界模型（Cosmos-Predict 2）进行RL后训练的方法。</p>
<p><img src="https://arxiv.org/html/2602.06508v1/x5.png" alt="主要结果"></p>
<blockquote>
<p><strong>图5</strong>：在LIBERO基准上的成功率。World-VLA-Loop经过两轮迭代后，性能显著超过所有基线方法，包括在真实世界进行昂贵RL训练的方法。</p>
</blockquote>
<p><strong>关键实验结果</strong>：在LIBERO的10个任务上，经过两轮World-VLA-Loop迭代精炼后的VLA策略，平均成功率从基线的38.3%提升至75.0%，相对提升95.8%。更重要的是，其性能甚至超过了在真实世界进行RL后训练的VLA策略（后者成功率52.5%），同时避免了后者所需的大量物理交互和人工重置成本。</p>
<p><img src="https://arxiv.org/html/2602.06508v1/x1.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图1b</strong>：在真实世界任务中，经过与世界模型两轮联合优化后，策略的成功率提升了36.7%。</p>
</blockquote>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2602.06508v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融研究。(a) 使用完整SANS数据集（含接近成功轨迹）训练的世界模型，其动作跟随精度显著高于仅用成功轨迹训练的模型。(b) 闭环迭代优化中，每一轮精炼都带来了世界模型保真度和策略性能的持续提升。</p>
</blockquote>
<p>消融实验表明：1) <strong>SANS数据集至关重要</strong>：仅使用成功轨迹训练的世界模型，其动作跟随错误率（Action-Following Error）高达54.7%，而使用完整SANS数据集训练后错误率降至11.3%。2) <strong>闭环迭代的有效性</strong>：随着迭代轮次增加，世界模型的保真度（FID分数降低）和策略成功率均持续提升。3) <strong>奖励联合预测的作用</strong>：与世界模型联合训练的奖励预测器，比单独训练的奖励模型能带来更高的策略性能。</p>
<p><strong>定性结果</strong>：<br><img src="https://arxiv.org/html/2602.06508v1/x2.png" alt="定性对比"></p>
<blockquote>
<p><strong>图2</strong>：现有世界模型（如Cosmos-Predict 2）经常在给定错误动作时幻觉出成功结果（如抓取失败但预测画面显示成功）。透明覆盖层显示了真实的夹爪轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.06508v1/x7.png" alt="定性对比"></p>
<blockquote>
<p><strong>图7</strong>：经过SANS数据训练后，我们的世界模型能够更准确地模拟由微小动作错误导致的失败案例，例如抓取位置略有偏差导致物体未被拿起。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了World-VLA-Loop，首个实现视频世界模型与VLA策略在闭环中协同演化的框架；2) 设计了SANS数据集和状态感知世界模型，显著提升了动作条件预测的精度和奖励信号的可靠性；3) 通过大量实验验证了该框架能高效提升VLA策略性能，大幅减少对昂贵真实世界交互的依赖。</p>
<p><strong>局限性</strong>：论文提到，当前方法仍依赖于一定量的初始成功和接近成功演示数据来进行世界模型的适应。对于完全未知的任务，如何实现零样本或少样本的快速适应仍是一个挑战。</p>
<p><strong>启示</strong>：本研究为机器人学习指明了一个有前景的方向：通过构建高保真、可交互的生成式世界模型，将大部分策略学习过程迁移到虚拟环境中。未来工作可以探索更高效的数据收集方式、世界模型对更复杂物理现象（如受力变形、液体）的建模能力，以及如何将该框架扩展到多任务、长视野的规划问题中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视频世界模型在机器人任务中动作跟随精度不足的问题，提出World-VLA-Loop闭环框架。核心方法包括：提出状态感知视频世界模型，联合预测未来观测与奖励信号，构建高保真交互模拟器；引入SANS数据集，利用近成功轨迹提升模型的动作-结果对齐能力。该框架实现了视觉-语言-动作（VLA）策略在虚拟环境中的强化学习后训练，并通过策略失败样本迭代优化世界模型，形成协同进化。实验表明，该方法能以极少的物理交互显著提升VLA策略性能，在仿真与真实任务中均验证了其有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.06508" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>