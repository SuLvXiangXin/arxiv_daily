<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Imitation from Observations with Trajectory-Level Generative Embeddings - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Imitation from Observations with Trajectory-Level Generative Embeddings</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.00452" target="_blank" rel="noreferrer">2601.00452</a></span>
        <span>作者: Weitong Zhang Team</span>
        <span>日期: 2026-01-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>离线模仿学习旨在从固定的演示数据集中恢复专家策略，而无需与环境在线交互。其中，仅从观察中学习（LfO）更具挑战性，因为专家演示仅包含状态观测，缺乏动作标签。现有LfO方法主要分为两类：占用匹配方法（如SMODICE、LobsDICE）通过估计专家与离线次优数据分布之间的密度比进行对齐，但其性能在离线数据对专家策略的支持有限时会严重恶化，因为密度比估计会变得病态或信号稀疏；替代奖励学习方法（如ORIL）通过训练判别器或逆动力学模型来生成奖励信号，但对抗性目标不稳定，且不完美的学习信号在长视野任务中会退化。本文针对在支持不匹配的次优离线数据下提取可靠模仿信号这一核心痛点，提出了一种新视角：利用轨迹级生成模型的潜在嵌入来构建密集、平滑的替代奖励。其核心思路是，在离线次优数据上训练一个时序扩散模型，利用其编码器将轨迹映射到潜在空间，然后通过基于粒子的熵估计最大化专家轨迹在该潜在空间中的对数似然，从而构造一个鲁棒的奖励信号用于下游离线强化学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>TGE的整体流程分为三步：首先，在混合的离线次优数据集 $\mathcal{D}<em>{\mu}$ 上训练一个轨迹扩散模型（Diffuser），并将其编码器 $\phi</em>{\theta}$ 作为轨迹表示提取器；其次，利用该编码器分别提取专家状态轨迹和离线数据中状态轨迹片段的潜在嵌入，并基于一种对数距离核函数计算替代奖励；最后，使用该奖励标注整个离线数据集，并采用标准的离线强化学习算法（如IQL或ReBRAC）训练策略。</p>
<p><img src="https://arxiv.org/html/2601.00452v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Trajectory-level Generative Embeddings (TGE) 方法概览。框架使用轨迹级扩散编码器将轨迹片段映射到潜在嵌入空间。然后利用这些嵌入上的对数核函数计算替代奖励，以标注原本无奖励的次优数据集，从而启用离线强化学习进行策略学习。</p>
</blockquote>
<p><strong>核心模块一：生成规划器学习可分离嵌入。</strong> 该方法使用Diffuser作为生成规划器，在次优数据集 $\mathcal{D}<em>{\mu}$ 的状态-动作轨迹片段上训练，损失函数为标准的去噪分数匹配目标 $\mathcal{L}(\theta)=\mathbb{E}[\lVert\boldsymbol{\epsilon}-\boldsymbol{\epsilon}</em>{\theta}(\tau_k,k)\rVert_{2}^{2}]$。关键设计在于，将扩散模型的U-Net噪声预测网络 $\epsilon_{\theta}$ 概念上分解为编码器 $\phi_{\theta}$ 和解码器 $\psi_{\theta}$。在提取嵌入时，输入到编码器 $\phi_{\theta}$ 的轨迹片段会掩码掉动作维度，仅保留状态序列，以确保嵌入空间 $\mathcal{Z}$ 与仅包含状态观测的专家数据兼容。训练过程鼓励潜在表示保留关于干净轨迹分布的信息，从而捕获对扰动稳定的时序相干特征。</p>
<p><img src="https://arxiv.org/html/2601.00452v2/x2.png" alt="嵌入可视化"></p>
<blockquote>
<p><strong>图2</strong>：T-SNE嵌入可视化。生成规划器产生的潜在嵌入能自然地将专家转移（红色）与次优转移（蓝色）分离开来，形成具有清晰边界的独立簇，表明轨迹级嵌入空间具有高度的判别性。</p>
</blockquote>
<p><strong>核心模块二：基于熵视角与轨迹级嵌入的奖励估计。</strong> 该方法从分布匹配的熵视角出发。最小化学习策略 $\pi$ 与专家策略 $\pi_E$ 的潜在状态分布之间的交叉熵 $H(\rho_{\pi},\rho_E)$，可推导出等价于最大化一个基于嵌入距离的累积替代奖励。具体地，对于离线数据集中的某个状态 $s_t$，取其后续长度为 $H$ 的状态片段 $\tau_{\mu}$，编码得到嵌入 $z_t$。在专家嵌入集合 $\mathcal{Z}_E$ 中找出 $z_t$ 的 $m$ 个最近邻 $\mathcal{N}<em>m(\mathcal{Z}<em>E)$，然后计算替代奖励：<br>$$r</em>{\mathrm{TGE}}(s_t) = \frac{1}{m} \sum</em>{z_E \in \mathcal{N}_m(\mathcal{Z}_E)} f!\left(|z_t - z_E|_2 / \sigma\right)$$<br>其中 $f(d) = -\log(1+d)$ 是对数核函数，$\sigma$ 是温度参数。计算前所有嵌入会进行 $L_2$ 归一化。该奖励函数的设计源于粒子熵估计理论，最大化此奖励近似等价于最小化与专家分布的交叉熵，从而提供了密集且平滑的学习信号。</p>
<p><strong>创新点</strong>：与现有方法相比，TGE的主要创新在于：1) <strong>利用轨迹级生成模型作为表示学习器</strong>：避免了训练额外的判别器或逆模型，直接利用扩散模型编码器获得能自然区分专家与次优行为的时序感知嵌入。2) <strong>基于熵估计的几何距离奖励</strong>：将对数距离核函数应用于潜在嵌入空间，构建了一个理论上与分布匹配目标一致、且在支持不匹配区域仍能提供有效梯度的密集奖励，克服了密度比方法信号稀疏和对抗性方法不稳定的问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在D4RL的locomotion（Hopper, Walker2d, HalfCheetah）和manipulation（Adroit, AntMaze）基准测试中进行评估。专家数据仅包含一条完整的状态轨迹。离线数据集 $\mathcal{D}_{\mu}$ 为混合质量的数据。对比的基线方法包括：ORIL（对抗性奖励）、SMODICE、PW-DICE（占用匹配）、以及基于BC的简单方法。</p>
<p><strong>关键定量结果</strong>：</p>
<p><img src="https://arxiv.org/html/2601.00452v2/x3.png" alt="性能对比"></p>
<blockquote>
<p><strong>图3</strong>：在D4RL locomotion任务上的性能对比（归一化得分）。TGE（Ours）在大多数任务和数据集质量下均匹配或优于所有基线方法，尤其在专家数据覆盖有限的“medium”和“med-exp”数据集中表现出更强的鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.00452v2/x4.png" alt="操纵任务结果"></p>
<blockquote>
<p><strong>图4</strong>：在Adroit和AntMaze操纵任务上的成功率。TGE在复杂的稀疏奖励操纵任务上显著优于基线方法，展示了其从轨迹级结构中提取有效学习信号的能力。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<p><img src="https://arxiv.org/html/2601.00452v2/x5.png" alt="消融研究"></p>
<blockquote>
<p><strong>图5</strong>：消融研究结果。a) 使用不同核函数：理论推导的对数核 $-\log(1+d)$ 性能最佳；使用逆核 $1/(1+d)$ 或线性核 $-d$ 会导致性能下降。b) 嵌入维度的影响：中等维度（64）效果最好，过低或过高均会损害性能。c) 最近邻数量 $m$ 的影响：$m=5$ 是一个稳健的选择。</p>
</blockquote>
<p><strong>定性分析</strong>：</p>
<p><img src="https://arxiv.org/html/2601.00452v2/x6.png" alt="定性可视化"></p>
<blockquote>
<p><strong>图6</strong>：Walker2d任务上学得策略（绿色）与专家（红色）的状态轨迹可视化。TGE恢复的策略在关键关节角度上与专家行为紧密对齐，而基线方法（如ORIL）则产生明显的偏差。</p>
</blockquote>
<p><strong>消融实验总结</strong>：每个组件都对最终性能有贡献：1) <strong>轨迹级嵌入</strong>：相比单步状态嵌入，能捕获长期动态，至关重要。2) <strong>对数核函数</strong>：其重尾特性为远离专家的状态提供了非零梯度，是稳健性的关键。3) <strong>扩散编码器</strong>：其学习到的平滑几何表示优于通过对比学习等其他方式获得的表示。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一种基于生成规划器的轨迹级表示学习方法，其潜在嵌入能无监督地区分专家与次优行为。2) 提出了一种基于熵估计的、在轨迹级潜在嵌入空间上定义的核函数奖励公式，为离线LfO提供了密集且稳定的学习信号。3) 实证表明，该方法在多种基准测试中，尤其是在离线数据与专家行为分布差异大（支持不匹配）的困难情况下，具有优于现有方法的鲁棒性和性能。</p>
<p><strong>局限性</strong>：论文提到，训练轨迹扩散模型需要较高的计算成本。此外，方法性能依赖于离线数据集中存在至少部分与专家行为相关的结构信息。</p>
<p><strong>后续研究启示</strong>：1) <strong>表示学习</strong>：探索其他类型的生成模型或自监督学习目标来获取更有效的轨迹表示。2) <strong>奖励设计</strong>：进一步研究不同距离度量与核函数在潜在空间中对模仿学习效率与稳定性的影响。3) <strong>扩展应用</strong>：将轨迹级生成嵌入的思想应用于其他稀疏奖励或演示稀缺的强化学习设定中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究离线观察模仿学习（LfO）的核心挑战：专家轨迹稀缺且仅含状态观测，而离线次优数据与专家行为分布差异大。为解决该问题，提出轨迹级生成嵌入（TGE）方法：通过在时间扩散模型的潜在空间中最大化专家轨迹的对数似然，利用基于粒子的熵估计构建密集平滑的代理奖励，从而捕捉长期时序动态并弥合分布差异。实验表明，该方法在D4RL运动与操作基准测试中一致匹配或优于现有离线LfO方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.00452" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>