<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ViTaSCOPE: Visuo-tactile Implicit Representation for In-hand Pose and Extrinsic Contact Estimation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ViTaSCOPE: Visuo-tactile Implicit Representation for In-hand Pose and Extrinsic Contact Estimation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.12239" target="_blank" rel="noreferrer">2506.12239</a></span>
        <span>作者: Lee, Jayjun, Fazeli, Nima</span>
        <span>日期: 2025/06/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在接触丰富的灵巧操作任务中，精确估计手中物体的位姿（in-hand pose）以及物体与外部环境之间的接触位置（extrinsic contact）至关重要。然而，由于视觉遮挡和观测噪声，仅凭单一模态（如视觉或触觉）难以获得完整、可靠的信息。现有主流方法存在若干关键局限性：一方面，许多基于隐式表示的工作（如VIRDO、NDCF、NCF）假设已知物体位姿或刚性抓握，即物体相对于末端执行器的位姿固定，这简化了问题但限制了在真实世界中处理物体在手中滑动或旋转的能力。另一方面，同时进行接触与物体位姿估计（SCOPE）的方法通常依赖力/力矩传感或接触粒子滤波器，未能利用高分辨率触觉传感，且无法在线学习物体模型。</p>
<p>本文针对上述痛点，提出了一个新的视角：<strong>构建一个以物体为中心的、融合视觉与高分辨率触觉反馈的神经隐式表示，在统一框架下同时推理物体几何、手中物体位姿以及外部接触</strong>。本文的核心思路可以概括为：利用符号距离场（SDF）隐式表示物体几何，利用神经剪切场（shear field）表示分布式触觉反馈，并通过一个接触场（contact field）将外部接触注册到物体的三维几何上，从而实现对互补的视觉-触觉线索的无缝推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViTaSCOPE的整体框架是一个由三个核心模块组成的隐式函数，其输入是部分视觉点云、触觉点云以及触觉传感器平面上的剪切场观测，输出则是查询点的SDF值、接触概率以及剪切向量。</p>
<p><img src="https://arxiv.org/html/2506.12239v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ViTaSCOPE的训练与推理框架。包含三个模块：物体模块 $\mathcal{O}$、触觉模块 $\mathcal{T}$ 和接触模块 $\mathcal{C}$。物体模块将物体几何表示为符号距离场（SDF）。触觉模块在触觉传感平面上以二维网格坐标 $(s_x, s_y)$ 表示剪切向量场，其条件变量包括手中物体位姿的隐编码 $z_\xi$ 和表征接触配置的“试验代码”（trial code）$z_\psi$。接触模块预测查询点的接触概率 $c$，其条件变量包括物体模块的层激活 $z_{\mathcal{O}}$ 和触觉试验代码 $z_\psi$。</p>
</blockquote>
<p><strong>1. 核心模块与作用</strong></p>
<ul>
<li>**物体模块 ($\mathcal{O}$)**：负责表示物体的三维几何。它是一个神经网络，输入为物体规范坐标系下的三维查询点 $\bm{q}$，输出其符号距离值 $s = \mathcal{O}(\bm{q})$。该模块首先通过预训练学习物体的规范形状，其权重在后续联合训练中被冻结。</li>
<li>**触觉模块 ($\mathcal{T}$)**：负责表示由外部接触引起的触觉剪切场。它是一个隐函数，输入为触觉传感器平面上的二维查询点 $\bm{g}$，并条件于手中物体位姿的隐编码 $\bm{\xi}$ 和接触试验代码 $\bm{\psi}$，输出该点的局部剪切位移 $[u, v]^T = \mathcal{T}(\bm{g} \mid \bm{\xi}, \bm{\psi})$。每个分布式触觉传感器都有一个对应的触觉模块实例。</li>
<li>**接触模块 ($\mathcal{C}$)**：负责估计外部接触的位置。它预测一个三维查询点 $\bm{q}$ 处于接触状态的概率 $c$。为了融入几何信息，其输入拼接了查询点本身、物体模块的中间层激活 $z_{\mathcal{O}}$，以及触觉模块的输出，同时也条件于位姿编码 $\bm{\xi}$ 和试验代码 $\bm{\psi}$，即 $c = \mathcal{C}(\bm{q} \oplus \bm{z}_{\mathcal{O}}, \mathcal{T}(\bm{g} \mid \bm{\xi}, \bm{\psi}) \mid \bm{\xi}, \bm{\psi})$。</li>
</ul>
<p><strong>2. 工作流程与创新点</strong><br>方法的关键在于推理时对隐变量 $\bm{\xi}$（物体位姿）和 $\bm{\psi}$（接触配置）的优化。给定来自真实世界的视觉-触觉点云观测 $\tilde{P}$ 和观测到的剪切场 $\phi_{obs}$，通过优化以下目标函数来同时估计物体位姿和接触：<br>$$\bm{\xi}^*, \bm{\psi}^* = \arg\min_{\bm{\xi}, \bm{\psi}} \sum_{\bm{g} \in G} | \mathcal{T}(\bm{g} \mid \bm{\xi}, \bm{\psi}) - \phi_{obs}(\bm{g}) |^2 + \sum_{\bm{p} \in \tilde{P}} | \mathcal{O}(T(\bm{\xi})^{-1} \bm{p}) |$$<br>其中 $T(\bm{\xi})$ 是将点从世界坐标系变换到物体规范坐标系的SE(3)变换。第一项使预测的剪切场匹配观测，第二项强制观测点云位于预测物体表面（SDF为零）附近。</p>
<p>与现有方法相比，ViTaSCOPE的核心创新体现在：</p>
<ol>
<li><strong>去除了刚性抓握假设</strong>：通过显式优化SE(3)变换 $\bm{\xi}$ 来估计物体位姿，允许物体在手中发生平移和旋转，直接处理世界坐标系下的观测。</li>
<li><strong>统一的多模态隐式表示</strong>：将物体几何（SDF）、触觉反馈（剪切场）和接触信息（接触场）集成在一个框架中，通过共享的隐变量（$\bm{\xi}, \bm{\psi}$）进行联合推理。</li>
<li><strong>基于物理的约束</strong>：通过联合学习几何与接触，并在损失函数中施加接触点应位于物体表面的约束，增强了预测的物理合理性。</li>
</ol>
<p><strong>3. 训练策略</strong><br>训练分为两个阶段：</p>
<ul>
<li><strong>阶段一（预训练）</strong>：仅训练物体模块 $\mathcal{O}$，使用物体网格采样的点及其SDF值、法向量，损失函数为SDF回归损失与法向量对齐损失的加权和（公式7）。</li>
<li><strong>阶段二（联合训练）</strong>：冻结预训练好的物体模块，联合训练触觉模块 $\mathcal{T}$ 和接触模块 $\mathcal{C}$。使用模拟收集的接触交互数据集，损失函数包括剪切场回归损失 $\mathcal{L}<em>{shear}$、接触分类的交叉熵损失 $\mathcal{L}</em>{ce}$，以及一个关键的“表面接触”损失 $\mathcal{L}_{surf}$，后者惩罚被预测为接触的点偏离物体表面（SDF值不为零），确保接触的几何一致性。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：方法在模拟和真实世界环境中进行评估。模拟实验使用Isaac Gym和TacSL模拟器，生成了包含多种工具（锤子、扳手、刮刀等）与不同外部接触交互的大规模数据集。真实世界实验使用搭载了基于视觉的触觉传感器（DIGIT）的双指机器人手。对比的基线方法包括：仅使用触觉的神经接触场方法 <strong>NCF</strong>、物理信息神经网络方法 <strong>NISP</strong>、以及传统的同时接触与物体位姿估计方法 <strong>SCOPE</strong>。</p>
<p><strong>关键定量结果</strong>：</p>
<ol>
<li><strong>手中物体位姿估计</strong>：在模拟测试中，ViTaSCOPE的平移误差为4.4毫米，旋转误差为4.5度，显著优于NCF（平移14.5毫米，旋转19.8度）和NISP（平移9.7毫米，旋转11.9度）。</li>
<li><strong>外部接触估计</strong>：在接触交并比（IoU）指标上，ViTaSCOPE达到0.56，远高于NCF的0.22和NISP的0.27。在平均精度（AP）指标上，ViTaSCOPE为0.65，同样优于其他方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.12239v1/x2.png" alt="定性结果"></p>
<blockquote>
<p><strong>图2</strong>：模拟与真实世界的定性结果。左列显示输入（视觉点云、触觉点云、观测剪切场）。中间列显示ViTaSCOPE预测的物体几何（灰色网格）和接触区域（红色点）。右列显示真实情况（地面真值）。结果表明，方法能准确估计物体位姿并将接触定位在几何表面。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.12239v1/x3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。比较了完整模型（Ours）与不同变体：不使用触觉剪切场（w/o shear）、不使用视觉点云（w/o vis. pc）、以及不联合优化位姿（w/o joint ξ opt）。柱状图显示，完整模型在位姿误差（左）和接触IoU（右）上均表现最佳，证明了多模态融合与联合优化的必要性。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>图3的消融实验系统性地验证了各组件贡献：</p>
<ul>
<li><strong>移除剪切场（w/o shear）</strong>：导致接触估计性能大幅下降（IoU从0.56降至0.39），证明了触觉剪切信息对于定位外部接触至关重要。</li>
<li><strong>移除视觉点云（w/o vis. pc）</strong>：导致位姿估计误差显著增加（旋转误差从4.5度升至12.5度），证明了视觉提供的全局几何上下文对于物体定位不可或缺。</li>
<li><strong>不进行位姿联合优化（w/o joint ξ opt）</strong>：即固定位姿编码 $\bm{\xi}$，性能全面下降，这验证了显式优化SE(3)位姿以处理非刚性抓握的有效性。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.12239v1/x4.png" alt="模块贡献"></p>
<blockquote>
<p><strong>图4</strong>：不同模块对接触估计性能的贡献分析。横轴为用于接触预测的条件信息类型。结果显示，同时结合物体几何特征（$z_{\mathcal{O}}$）和触觉剪切信息（$\phi$）时，接触估计的AP值最高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.12239v1/x5.png" alt="零样本迁移"></p>
<blockquote>
<p><strong>图5</strong>：模拟到真实世界的零样本迁移能力。左图为真实机器人实验场景。右图显示，仅在模拟数据上训练的ViTaSCOPE模型，能够直接应用于真实世界，预测出合理的物体几何和接触位置（红色点云），尽管存在一些感知噪声。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文的核心贡献</strong>：</p>
<ol>
<li><strong>提出了统一的视觉-触觉隐式表示（ViTaSCOPE）</strong>：首次将物体几何（SDF）、触觉剪切场和外部接触场集成在一个框架内，实现了对物体位姿和外部接触的联合、高分辨率估计。</li>
<li><strong>放宽了刚性抓握假设</strong>：通过显式优化SE(3)物体位姿，方法能够处理物体在手中的动态运动，更符合真实的灵巧操作场景。</li>
<li><strong>实现了有效的模拟到真实迁移</strong>：利用基于物理惩罚的触觉模拟器（TacSL）生成大规模训练数据，并通过隐式表示和剪切场表征，成功地将模型零样本迁移到真实机器人系统。</li>
</ol>
<p><strong>论文提到的局限性</strong>：</p>
<ol>
<li><strong>依赖预训练的物体模块</strong>：物体几何表示需要针对每个新物体进行预训练，限制了在线适应未知物体的能力。</li>
<li><strong>模拟数据的局限性</strong>：虽然实现了 sim-to-real 迁移，但模拟的接触物理与真实世界仍存在差距，可能影响极端情况下的性能。</li>
</ol>
<p><strong>对后续研究的启示</strong>：<br>ViTaSCOPE展示了神经隐式表示在融合多模态感知（尤其是视觉与高分辨率触觉）方面的强大潜力。未来的工作可以沿着以下方向探索：扩展框架以处理<strong>可变形物体</strong>的几何与接触估计；研究<strong>在线学习</strong>机制，使模型能快速适应新的物体或环境；将这种强大的感知模块与<strong>闭环控制策略</strong>更紧密地结合，实现真正自主、灵巧的接触式操作任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ViTaSCOPE，解决灵巧操作中因视觉遮挡和观测噪声导致的手内物体姿态与外部接触位置估计难题。方法核心是构建一种以物体为中心的神经隐式表示，融合视觉与高分辨率触觉反馈：用有符号距离场表示物体几何，用神经剪切场表示分布式触觉信息，从而将外部接触精准注册到物体3D几何上。该方法利用仿真进行可扩展训练，并实现向真实世界的零样本迁移。综合仿真与实物实验表明，该方法在灵巧操作场景中能有效进行物体几何重建、手内姿态估计和外部接触定位。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.12239" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>