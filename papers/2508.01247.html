<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Coordinated Humanoid Robot Locomotion with Symmetry Equivariant Reinforcement Learning Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Coordinated Humanoid Robot Locomotion with Symmetry Equivariant Reinforcement Learning Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.01247" target="_blank" rel="noreferrer">2508.01247</a></span>
        <span>作者: Nie, Buqing, Zhang, Yang, Jin, Rongjun, Cao, Zhanxiang, Lin, Huangxuan, Yang, Xiaokang, Gao, Yue</span>
        <span>日期: 2025/08/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，深度强化学习（DRL）已广泛应用于机器人控制任务。然而，现有方法通常是黑盒性质的，未能有效利用人形机器人固有的形态对称性。这种疏忽导致策略在面对对称的观测时产生不一致的反应（例如，对称关节在运动中出现不对称的运动模式），进而产生不协调、非自然且次优的行为，影响任务性能。为应对此问题，先前工作提出了多种方法将形态对称性整合到训练框架中，主要分为三类：1）从时间角度鼓励运动周期性；2）从空间角度通过数据增强或损失正则化来诱导策略的等变性。但这些“松散等变”的方法约束不足，性能有限，且损失正则化会引入需要精细调优的超参数，可能阻碍策略优化。而严格的等变方法（通过神经网络架构实现）在机器人，尤其是真实人形机器人上的有效性尚未得到充分探索。</p>
<p>本文针对人形机器人DRL策略忽视形态对称性这一具体痛点，提出了一种将严格对称等变性嵌入到actor-critic架构中的新视角。核心思路是：受人类神经系统双侧对称性的启发，提出对称等变策略（SE-Policy），在actor中嵌入严格的对称等变性，在critic中嵌入对称不变性，无需额外超参数，从而产生时空协调性更高、任务性能更好的运动。</p>
<h2 id="方法详解">方法详解</h2>
<p>SE-Policy的整体框架基于DreamWaQ（一种基于PPO的先进腿部机器人运动方法），包含历史编码器、观测解码器、作为actor的策略网络和作为critic的价值网络。</p>
<p><img src="https://arxiv.org/html/2508.01247v2/x1.png" alt="方法整体架构"></p>
<blockquote>
<p><strong>图1</strong>：SE-Policy的整体架构。(a) 左侧：actor和critic模型架构。(b) 右上：潜在特征z的对称变换ℱ_z可视化，以及人形机器人运动及其对称运动的可视化。(b) 右下：本工作中广泛使用的等变MLP描述。</p>
</blockquote>
<p><strong>对称等变Actor</strong>：决策基于时序观测历史<code>o_[t-h:t]</code>。历史编码器<code>f_en</code>从历史中提取潜在特征<code>z</code>，并通过观测解码器<code>f_de</code>预测下一时刻观测<code>o_{t+1}</code>进行自监督训练（损失为<code>ℒ_AE = MSE(ô, o_{t+1})</code>）。策略网络<code>f_π</code>基于当前观测<code>o_t</code>和潜在特征<code>z</code>做出决策，即<code>a_t = f_π(o_t, z)</code>。</p>
<p><strong>对称不变Critic</strong>：价值网络<code>V(H_t, o_t)</code>用于评估策略，其中高度图<code>H</code>是特权信息。根据理论推导，最优critic函数应对称不变，即<code>V(ℱ_s(s)) = V(s)</code>。</p>
<p><strong>等变神经网络实现</strong>：为融入公式(7)和(6)描述的对称等变/不变性，所有网络均使用基于ESCNN库实现的线性层和ReLU激活函数构建，其核心是通过参数共享实现等变性。如图1右下所示，所有等变MLP均根据输入输出的对称变换<code>ℱ_input</code>和<code>ℱ_output</code>进行设计。对于尺寸为偶数的潜在特征<code>z</code>，其对称变换<code>ℱ_z</code>定义为交换相邻元素（公式9）。基于表1中定义的观测、动作的对称变换以及<code>ℱ_z</code>，即可构建如图1所示的等变actor和不变critic。</p>
<p><strong>训练框架</strong>：训练过程与标准PPO算法基本一致。Critic使用MSE损失<code>ℒ_V</code>进行训练。Actor的损失包括自编码损失<code>ℒ_AE</code>和PPO损失<code>ℒ_PPO</code>。奖励函数包括速度指令跟踪、平衡维持（如z轴速度惩罚）和正则化项（如动作振荡惩罚）。为提高训练效率和稳定性，采用了课程学习（逐步增加地形、指令、噪声的难度）和领域随机化（对地面摩擦、质量属性、质心位置、电机参数等进行随机化）。</p>
<p><strong>创新点</strong>：与现有方法相比，SE-Policy的核心创新在于通过<strong>网络架构设计</strong>（而非数据增强或损失正则化）在actor中实现<strong>严格的对称等变性</strong>，在critic中实现<strong>严格的对称不变性</strong>。这种方法不引入额外超参数，从原理上保证了策略在面对对称观测时必然输出对称动作，从而直接解决了行为不一致的根本问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在NVIDIA Isaac Gym仿真器中进行，并最终通过sim-to-real部署到Unitree G1（27自由度）真实人形机器人上。任务为速度跟踪（指令随机采样：|v_x| &lt; 0.8 m/s，|v_y| &lt; 0.8 m/s，|ω| &lt; 0.5 rad/s）。</p>
<p><strong>Baseline方法</strong>：1) <strong>DreamWaQ</strong>：先进的免模型DRL算法；2) <strong>DreamWaQ-Regu</strong>：在DreamWaQ的policy更新中引入软正则化项<code>ℒ_reg</code>（公式11）以惩罚双边关节间的不对称驱动模式；3) **SE-Policy (actor only)**：将SE-Policy的critic替换为基于普通MLP的critic，用于消融研究。</p>
<p><strong>评估指标</strong>：1) <strong>跟踪误差</strong>：包括速度(TE-V)、位置(TE-P)、朝向(TE-O)误差；2) **时间对称性(Temp-S)**：量化半个周期内关节动作与其对称关节动作的差异，衡量运动周期性协调性；3) **空间对称性(Spat-S)**：直接衡量策略网络的等变性，计算<code>π(o)</code>与<code>ℱ_a(π(ℱ_o(o)))</code>的差异。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2508.01247v2/x2.png" alt="跟踪误差位置与朝向"></p>
<blockquote>
<p><strong>图2</strong>：随时间推移的位置跟踪误差(TE-P)和朝向跟踪误差(TE-O)。SE-Policy（红线）的误差始终低于其他方法，验证了其有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.01247v2/x3.png" alt="运动轨迹可视化"></p>
<blockquote>
<p><strong>图3</strong>：各方法从中心向八个方向运动的轨迹可视化。虚线为理想轨迹，红线为实际轨迹。SE-Policy（图c）在跟踪精度和轨迹对称性上均优于基线方法。</p>
</blockquote>
<p>如表2所示，SE-Policy在TE-V上达到9.85 cm/s，相比DreamWaQ（16.43 cm/s）降低了40.0%，相比DreamWaQ-Regu（13.91 cm/s）也降低了29.2%，展示了卓越的跟踪性能。其Spat-S为0.0，这严格符合其架构强加的等变性。Temp-S也最低，表明时间上的运动周期性更协调。图2显示，SE-Policy的累积位置和朝向误差增长也最慢。图3的轨迹可视化进一步证实了SE-Policy能产生更对称、更精确的跟踪运动。</p>
<p><strong>消融实验分析</strong>：SE-Policy (actor only) 的TE-V（11.06 cm/s）差于完整SE-Policy，但优于DreamWaQ-Regu，表明仅actor等变已能带来显著提升，而critic不变性进一步提升了性能。DreamWaQ-Regu通过损失正则化获得了一定性能提升（TE-V降低15.3%），但其Spat-S非零（8.18），说明其等变性是“软”约束，在推理时可能被违反，性能不及严格的架构等变。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了SE-Policy，一种将严格对称等变性/不变性嵌入actor-critic网络架构的新DRL框架，无需额外超参数；2) 该方法能生成对称、自然的运动，在仿真和真实人形机器人上均实现了比现有方法更高的跟踪精度和协调性；3) 通过消融实验明确了actor等变和critic不变各自对性能提升的贡献。</p>
<p><strong>局限性</strong>：论文自身提到，SE-Policy依赖于机器人的形态对称性，因此主要适用于具有对称结构的人形或双足机器人。对于非对称机器人或需要非对称动作的任务，该方法可能不直接适用。</p>
<p><strong>启示</strong>：本研究证明了将物理先验（如形态对称性）通过严格的数学约束（等变性）形式化并嵌入DRL策略架构的有效性。这为提升机器人策略的协调性、自然性和性能提供了新思路。未来的研究可以探索将其他物理约束或几何先验以类似方式整合到策略学习中，并进一步研究该方法在更复杂任务（如动态交互、全身协调操作）中的潜力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人因忽视自身形态对称性而导致运动不协调、性能次优的问题，提出了一种对称等变强化学习策略（SE-Policy）。该方法的核心是在策略网络的行动者中嵌入严格的对称等变性，在批评者中嵌入对称不变性，无需额外超参数，从而强制策略对对称观测产生一致行为。在Unitree G1机器人上进行的速度跟踪实验表明，该策略相比先进基线方法，将跟踪精度提升了高达40%，并实现了优越的时空协调运动。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.01247" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>