<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.06748" target="_blank" rel="noreferrer">2601.06748</a></span>
        <span>作者: Cheng Han Team</span>
        <span>日期: 2026-01-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为通用机器人学习的有力范式，能将视觉观察和自然语言指令映射为可执行的机器人动作。然而，主流方法主要通过监督微调或训练时强化学习进行训练，这需要明确的微调阶段、人工干预或受控的数据收集。因此，现有方法不适合具有挑战性的模拟或物理世界部署，因为机器人必须自主、灵活地响应不断变化的环境。现有测试时训练方法无法直接应用于VLA，因为多模态特性带来了巨大的分布偏移。</p>
<p>本文针对VLA在动态部署环境中策略固定、无法在线适应的问题，提出了测试时强化学习的新视角。核心思路是：在推理过程中，利用基于任务进度的密集奖励信号，对VLA策略进行在线微调，同时保留SFT/RL训练的先验知识，从而在不重新训练的情况下提高对分布偏移的鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>TT-VLA是一个在推理时进行在线策略微调的框架。其整体目标是：在无法访问训练数据、环境重置或人工干预的情况下，在线灵活地调整预训练策略π_θ。</p>
<p><img src="https://arxiv.org/html/2601.06748v2/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：TT-VLA整体框架。(a) 整体流程：预训练的VLA策略接收观察和指令，执行动作，环境反馈的观察被进度估计器用于计算密集的、基于进度的奖励，该奖励随后通过一个无价值函数的PPO目标在线更新策略参数。(b) 有效性：TT-VLA能持续提升不同VLA主干在未见任务上的性能。</p>
</blockquote>
<p>核心模块包括<strong>密集进度奖励</strong>和<strong>无价值PPO训练目标</strong>。</p>
<p><strong>密集进度奖励</strong>：为了解决测试时稀疏终端奖励不实用的问题，TT-VLA设计了密集的进度差分奖励。设p_t ∈ [0,1]为时间步t的任务进度，由进度预测器Φ根据观察历史o_{0:t+1}和指令l估计得出：p_t = Φ(o_{0:t+1}, l)。论文使用预训练的多模态模型VLAC作为Φ。每一步的密集奖励定义为进度的时间差分：r_t = p_t - p_{t-1}。该奖励无需外部监督，提供步进反馈，并鼓励单调进展。</p>
<p><strong>训练目标（无价值PPO）</strong>：在测试时适应中，由于样本有限和时间约束严格，学习可靠的价值函数不可行。因此，TT-VLA采用了一种无价值函数的PPO变体。从标准PPO目标出发，通过设系数c1=0和c2=0，移除了价值函数损失和熵正则化项，仅保留裁剪的策略损失：L(θ) = E_t[L_t^CLIP(θ)]。同时，为了精确捕捉当前动作的即时价值，将广义优势估计简化为单步形式：设λ=0和γ=0，使优势估计A^_t = δ_t = r_t。这样，策略更新直接反映每一步的进度，使智能体能够在不依赖价值函数的情况下快速适应变化。</p>
<p><strong>整体流程</strong>：在每个回合开始时，预训练VLA接收初始观察o_0和指令l，生成第一个动作a_0。在后续每个时间步t，VLA接收最新观察o_t并输出动作a_t。执行后，进度估计器Φ计算任务进度p_t和对应的密集奖励r_t。该奖励用于通过公式8以无价值的方式计算策略损失，并据此更新策略参数θ。更新后的策略用于生成后续动作，实现在整个回合中的持续优化。</p>
<p>与现有方法相比，TT-VLA的创新点在于：1) 将测试时训练与RL相结合，专门解决VLA多模态分布偏移的挑战；2) 设计了任务无关的密集进度奖励，为在线适应提供即时信号；3) 提出了无价值PPO公式，克服了测试时价值函数学习不可行的问题，实现了高效、稳定的在线策略更新。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在<strong>模拟</strong>和<strong>真实世界</strong>两种环境中进行。模拟实验在ManiSkill 3中使用WidowX-250S机械臂，评估了三个维度的泛化能力：<strong>执行</strong>（随机化初始姿态、中途物体重定位）、<strong>视觉</strong>（动态纹理、未见桌面、图像噪声）和<strong>语义</strong>（未见物体/容器、指令复述、多物体/容器、干扰容器）。真实世界实验在Franka Research 3平台上进行，评估了9个未见任务。实现上使用LoRA进行微调，图像分辨率分别为640x480（模拟）和500x480（真实）。</p>
<p>对比的<strong>基线方法</strong>包括：Nora（基于Qwen-2.5-VL-3B）、OpenVLA（基于Llama-2-7B）、OpenVLA-RL（OpenVLA的RL增强版）和TraceVLA（通过视觉轨迹提示增强时空推理）。</p>
<p><img src="https://arxiv.org/html/2601.06748v2/x4.png" alt="模拟结果表"></p>
<blockquote>
<p><strong>图4</strong>：在未见模拟任务上的主要结果（表1）。报告了在四个先进开源VLA上，跨执行、视觉和语义三个泛化维度的成功率。Δ为绝对提升，↑为相对增益。TT-VLA在所有基线和任务类别上均一致提升了性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>：如表1所示，TT-VLA持续提升了所有基线模型在各类未见任务上的性能。例如，应用于Nora时，在15个任务中的14个上获得提升，相对提升从5.26%到44.4%不等，在“物体重定位”任务上提升最大（44.4%）。应用于OpenVLA时，也获得了持续的提升，包括多个大幅增益（如在“干扰容器”任务上提升44.9%）。这表明通过流线型的测试时调整，可以实现跨不同基线和任务的显著泛化能力提升。</p>
<p><img src="https://arxiv.org/html/2601.06748v2/x5.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界机器人任务的成功率。TT-VLA在9个未见任务中的7个上提高了OpenVLA的性能，平均相对增益为6.25%，验证了其在物理部署中的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.06748v2/x6.png" alt="消融研究"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。分别移除了进度奖励、无价值PPO公式和LoRA微调。所有组件对TT-VLA的性能提升都至关重要，其中进度奖励的影响最大。</p>
</blockquote>
<p><strong>消融实验</strong>：论文对TT-VLA的关键组件进行了消融研究。1) <strong>移除进度奖励</strong>：导致性能大幅下降（平均-8.33%），验证了密集进度信号对在线适应的必要性。2) <strong>使用标准PPO替代无价值PPO</strong>：性能下降（平均-3.33%），表明在测试时学习价值函数具有挑战性且可能不稳定。3) <strong>移除LoRA微调（全参数更新）</strong>：性能下降（平均-4.17%），并导致训练不稳定，说明参数高效微调对保持先验知识和稳定适应很重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了<strong>首个专为VLA设计的测试时强化学习框架</strong>，实现了部署期间策略的在线、自适应微调，无需重新训练。2) 设计了<strong>密集的、基于任务进度的奖励机制</strong>，利用VLAC模型提供即时反馈，克服了测试时稀疏奖励的局限性。3) <strong>推导了无价值PPO公式</strong>，通过简化优势估计，使策略能够仅基于即时奖励进行高效、稳定的更新，适应了测试时的苛刻约束。</p>
<p>论文自身提到的局限性包括：1) 性能依赖于预训练进度估计器的质量，不准确的进度估计会误导策略更新。2) 尽管进行了优化，在线策略更新仍会引入额外的计算开销，在严格实时约束下可能面临挑战。</p>
<p>对后续研究的启示：1) 可以探索更高效、更鲁棒的<strong>任务进度估计方法</strong>，减少对特定预训练模型的依赖。2) 将TT-VLA框架<strong>扩展到更复杂的长期任务和动态环境</strong>，验证其在更广泛场景下的适用性。3) 进一步研究测试时适应的<strong>理论保证和收敛性</strong>，为在线策略优化提供更坚实的理论基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在动态、未见环境中适应性有限的核心问题，提出了一种测试时强化学习框架TT-VLA。该方法的关键在于：在模型推理阶段，利用基于逐步任务进度的密集奖励机制，在线实时优化动作策略，同时保留监督微调/强化学习训练的先验知识。实验表明，该方法在模拟和真实世界的动态场景中，有效提升了模型的整体适应性、稳定性和任务成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.06748" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>