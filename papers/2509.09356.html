<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.09356" target="_blank" rel="noreferrer">2509.09356</a></span>
        <span>作者: Drid, Abdel Hakim, Suriani, Vincenzo, Nardi, Daniele, Debilou, Abderrezzak</span>
        <span>日期: 2025/09/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人自主探索未知环境以重建地图的任务中，智能体面临与目标驱动任务（如物体导航）截然不同的挑战：缺乏明确的外部目标指引，必须自主决定何为有价值的信息及如何获取。传统方法，包括基于图的方法和部分学习型方法，常依赖人类操作员进行参数调优、闭环检测或数据标注。近期，视觉-语言模型（VLM）通过整合视觉与语言数据，为智能体提供了类似人类的场景语义理解能力，展现出巨大潜力。然而，将VLM有效集成到深度强化学习（DRL）框架中面临核心难题：如何设计奖励函数，将丰富的VLM输出转化为可指导智能体探索的有效信号。探索任务本身复杂度高，需平衡空间覆盖、物体交互和高层语义推理，这使得设计一个既细腻、信息丰富又稳定的奖励函数异常困难。</p>
<p>本文针对传统DRL方法在平衡高效探索与语义理解方面的不足，以及VLM集成中奖励设计的关键挑战，提出了一种新的视角：通过一个分层的奖励函数，将几何探索、物体发现和语义理解三个层面的目标系统性地结合起来，并采用课程学习策略分阶段引导智能体学习。核心思路是设计一个包含“VLM-查询”专用动作的DRL框架，使智能体学会在必要时才调用资源密集的VLM获取外部语义指导，从而实现资源高效的、基于常识的语义探索。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个基于课程学习的VLM-DRL框架，其核心是一个分层奖励函数和一个三阶段的训练策略。</p>
<p><img src="https://arxiv.org/html/2509.09356v1/figures/sem_drl_diagram.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：DRL智能体架构图，突出了核心贡献：“分层奖励函数”，由“几何层”、“物体层”和“语义层”构成。该分层奖励引导由DRL决策模型学习到的探索策略，以实现有语义意义的自主探索。</p>
</blockquote>
<p><strong>整体框架</strong>：智能体在仿真环境（如AI2-THOR）中执行探索。其<strong>状态</strong>由深度图像经空间下采样得到的128维深度状态向量表示。<strong>动作空间</strong>被离散为四个动作：{左转、前进、右转、VLM-查询}。一个基于深度确定性策略梯度（DDPG）算法的决策模块（Actor-Critic网络）根据当前状态输出动作。智能体获得的<strong>奖励</strong>则由一个精心设计的分层奖励函数计算，该函数是方法的核心。</p>
<p><strong>核心模块：分层奖励函数</strong><br>总奖励由碰撞惩罚和探索奖励构成。探索奖励 (R_E) 是三个加权层的总和：<br>[ R_E = \alpha * r_t^{(geom)} + \beta * r_t^{(obj)} + \delta * r_t^{(semantnical)} ]</p>
<ol>
<li>**几何特征奖励层 ((r_t^{(geom)}))**：鼓励探索未知区域。方法维护一个与场景帧尺寸相同的二值特征地图。每步使用特征点检测算法（如SIFT）从当前帧提取关键点。对于每个新出现的（即特征地图中未标记的）关键点，给予奖励并在地图中标记。其目标是最大化累计发现的独特特征点数量，驱动基础的空间探索。</li>
<li>**物体检测奖励层 ((r_t^{(obj)}))**：鼓励发现新的物体类别。利用开放词汇检测器YOLO-World识别当前视野中的物体。奖励基于本步新检测到的物体类别数量计算，并设置上限 (N_{max_objects}) 以防止奖励爆炸。此层引导智能体前往物体丰富的区域。</li>
<li><strong>语义奖励层 ((r_t^{(semantnical)}))<strong>：集成常识性场景理解。这是方法的关键创新。VLM（本文使用GPT-4o）的查询被建模为智能体的一个</strong>专用动作</strong>“VLM-Query”。仅当智能体选择此动作时，才会将当前RGB图像和一个语义评估提示词发送给VLM。VLM返回一个语义得分 (SC(I_t) \in [-1.0, +1.0])，表示当前场景对探索的“合意性”。该得分被离散化为三个等级（-1.0， 0.0， +1.0）作为本步的语义奖励。同时，为了阻止智能体频繁查询，对连续使用“VLM-Query”动作施加惩罚。这使得智能体必须学习在何时调用VLM最能受益，实现了对资源密集型外部指导的战略性使用。</li>
</ol>
<p><strong>决策模块与状态表示</strong><br>采用DDPG算法作为决策模型，其Critic网络根据上述分层奖励的总和来学习评估状态-动作对的价值（Q值），Actor网络则学习最大化Q值的确定性策略。</p>
<p><img src="https://arxiv.org/html/2509.09356v1/figures/state_representation.png" alt="状态表示"></p>
<blockquote>
<p><strong>图3</strong>：为高效学习设计的简洁状态表示。(a) RGB帧。(b) 对应的深度帧。(c) 经过空间下采样后得到的128维深度状态向量。</p>
</blockquote>
<p><strong>课程学习策略</strong><br>为了稳定、高效地学习整合了复杂奖励信号的策略，训练分为三个阶段：</p>
<ol>
<li><strong>阶段一（几何探索）</strong>：仅使用几何奖励层 ((\alpha=1, \beta=0, \delta=0))，让智能体掌握基础导航和环境感知能力。</li>
<li><strong>阶段二（物体感知探索）</strong>：在几何层基础上加入物体检测奖励层 ((\alpha=0.25, \beta=0.75, \delta=0))，引导智能体关注物体丰富的区域。</li>
<li><strong>阶段三（语义探索）</strong>：加入语义奖励层 ((\alpha=0.25, \beta=0.75, \delta=2.0))，智能体学习在必要时查询VLM，并基于语义反馈优先探索信息丰富的区域。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，本文的创新具体体现在：1) 提出了一个统一的分层奖励结构，系统性地融合了从低层几何到高层语义的探索目标；2) 将VLM查询设计为一个可学习的动作，使智能体能自主决策何时获取外部语义指导，优化了资源使用；3) 采用课程学习，循序渐进地引入复杂奖励，确保了训练的稳定性和策略的鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在AI2-THOR仿真环境的30个测试场景（厨房、卧室等）中进行训练与评估。硬件平台为NVIDIA RTX 3060 GPU和Intel i5-10400F CPU。评估指标包括：最大路径长度（Max PL）、检测物体总数（TDO）、物体检测总置信度分数（TCS）以及VLM/检测器调用次数（TDC）。</p>
<p><strong>对比基线</strong>：实验主要对比了智能体在课程学习<strong>不同阶段</strong>的性能，这本质上也是与仅使用部分奖励组件的简化版本进行对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><p><strong>定量结果</strong>：如表1所示，从阶段1到阶段3，虽然最大路径长度略有下降（8.75 -&gt; 5.0），但物体发现总数（TDO）和总置信度（TCS）显著提升（TDO从1254增至1274，TCS从485.09增至500.09）。这表明智能体的探索策略从“广撒网”转向了更高效、更有针对性的“信息优先”模式。</p>
</li>
<li><p><strong>定性结果</strong>：</p>
</li>
</ul>
<p><img src="https://arxiv.org/html/2509.09356v1/figures/final_paths.png" alt="探索轨迹"></p>
<blockquote>
<p><strong>图4</strong>：在四个AI2-THOR场景中的智能体轨迹。每列代表课程学习的不同阶段（几何、物体感知、语义）。可视化显示，随着阶段推进，智能体轨迹变得更集中于物体密集或语义信息丰富的区域，而非盲目游走。</p>
</blockquote>
<p>图4直观展示了智能体探索行为的演变。阶段一的轨迹较长但分散；阶段二的轨迹开始围绕物体聚集；阶段三的轨迹则更加精炼，明显趋向于场景中的关键语义区域（如厨房的工作台）。</p>
<ul>
<li><strong>消融实验</strong>：</li>
</ul>
<p><img src="https://arxiv.org/html/2509.09356v1/figures/ablation.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：不同消融实验配置下三个性能指标的柱状图。实验ID对应表3中的不同设置，展示了状态向量维度、奖励类型（是否包含准确度奖励）和训练场景数量对性能的影响。</p>
</blockquote>
<p>消融研究（表3及图5）表明：</p>
<ol>
<li>增加状态表示维度（从37维到128维）能显著提升所有指标（Max PL, TDO, TDC）。</li>
<li>在奖励中添加考虑物体检测准确度的组件（<code>acc</code>），性能提升并不显著。</li>
<li>即使在用计算代价较低的“检测器调用”替代“VLM-Query”的简化实验中，智能体在更复杂环境（增加场景和领域随机化）中也会增加调用次数。这印证了智能体学会在任务困难时更频繁地寻求外部指导的泛化趋势。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>分层奖励函数设计</strong>：提出了一个融合几何、物体和语义信息的三层奖励机制，为DRL智能体提供了系统化的、富含信息的探索指引。</li>
<li><strong>资源高效的VLM集成</strong>：创新地将VLM查询建模为智能体的一个可学习动作，使智能体能战略性地决定何时调用VLM，实现了常识推理与资源消耗的平衡。</li>
<li><strong>课程学习策略</strong>：采用三阶段课程学习，逐步引入复杂奖励，确保了训练过程的稳定性，并引导智能体循序渐进地掌握从基础导航到高级语义探索的技能。</li>
</ol>
<p><strong>局限性</strong>：论文工作主要在仿真环境（AI2-THOR）中进行验证。将方法部署到具有真实物理约束、感知噪声和延迟的现实世界机器人平台，是未来需要面对的挑战。</p>
<p><strong>启示</strong>：本研究为在DRL中集成大型模型提供了一种实用范式——即不追求每步都使用大模型，而是让其作为可按需调用的“战略顾问”。这种“动作化”的集成思路，以及通过分层奖励和课程学习来调和不同层次目标的方法，对开发其他需要结合低层控制与高层推理的复杂 embodied AI 任务具有借鉴意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对传统强化学习在语义探索中难以平衡效率与理解、常需人类干预的问题，提出一种基于深度强化学习（DRL）的架构。关键技术包括通过分层奖励函数集成视觉语言模型（VLM）常识，将VLM查询建模为专用动作以战略性地获取外部指导，并结合课程学习策略指导不同复杂度学习。实验表明，该代理显著提高了对象发现率，学会了有效导航到语义丰富区域，并掌握了何时查询外部信息的战略能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.09356" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>