<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.04191" target="_blank" rel="noreferrer">2504.04191</a></span>
        <span>作者: Cui, Jieming, Liu, Tengyu, Meng, Ziyu, Yu, Jiale, Song, Ran, Zhang, Wei, Zhu, Yixin, Huang, Siyuan</span>
        <span>日期: 2025/04/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，让机器人通过强化学习掌握多样化的物理技能是一个核心挑战。主流方法通常为每个特定任务手工设计奖励函数，这需要大量专家知识且难以扩展；另一种思路是利用专家演示进行逆强化学习或模仿学习，但这需要为每个新任务收集高质量的演示数据，成本高昂。这两种范式都难以适应开放词汇（open-vocabulary）的指令，即用自然语言自由描述的新任务。</p>
<p>本文针对“如何为开放词汇的物理技能学习自动生成通用奖励”这一具体痛点，提出了一个新视角：将奖励生成视为一个条件视频生成问题。核心思路是训练一个基于扩散模型的视频生成器，它能根据当前观测和目标任务的语言描述，生成任务成功完成的未来视频片段，然后通过比较当前真实观测与生成的成功视频的视觉一致性，来推导出一个稠密且通用的奖励信号。</p>
<h2 id="方法详解">方法详解</h2>
<p>GROVE 的整体框架是一个两阶段的训练流程：首先训练一个条件视频预测模型（称为“成功视频生成器”），然后在强化学习阶段利用该生成器为智能体提供奖励。</p>
<p><img src="https://example.com/grove_pipeline.png" alt="GROVE Pipeline"></p>
<blockquote>
<p><strong>图1</strong>：GROVE 方法整体框架。<strong>左（训练阶段）</strong>：使用成功轨迹的（观测，语言指令）对训练一个条件视频扩散模型，学习生成给定当前观测和语言指令下的未来成功视频。<strong>右（强化学习阶段）</strong>：在策略学习过程中，利用训练好的生成器，基于当前真实观测和任务指令生成一个成功的未来视频，并通过计算当前真实观测的未来帧与生成的成功视频之间的视觉特征距离，构造奖励函数。</p>
</blockquote>
<p><strong>核心模块1：成功视频生成器 (Success Video Generator)</strong><br>该模块是一个基于扩散模型的视频预测模型。其作用是学习“成功”的视觉模式。</p>
<ul>
<li><strong>输入</strong>：一个初始观测图像 (o_t) 和语言任务指令 (l)。</li>
<li><strong>输出</strong>：一段以 (o_t) 为起点，描绘任务成功完成的未来视频 (\hat{\tau}_{t:t+H}^+)，其中 (H) 是预测视野。</li>
<li><strong>技术细节</strong>：采用标准的潜在扩散模型（LDM）架构。观测图像 (o_t) 和语言指令 (l) 分别通过编码器（如ViT和CLIP文本编码器）映射到潜在空间并作为条件。模型在大量成功轨迹的片段数据集上进行训练，学习从噪声中重建出成功的未来视频。其训练目标是最小化扩散模型的标准去噪损失。</li>
</ul>
<p><strong>核心模块2：基于视觉一致性的奖励函数 (Visual Consistency Reward)</strong><br>这是将生成模型与强化学习连接起来的关键模块。</p>
<ul>
<li><strong>输入</strong>：当前的真实观测序列 (o_{t:t+H}) 和由生成器产生的成功视频 (\hat{\tau}_{t:t+H}^+)。</li>
<li><strong>输出</strong>：一个标量奖励值 (r_t)。</li>
<li><strong>技术细节</strong>：奖励函数定义为真实未来观测与生成的成功视频之间的负视觉特征距离。具体而言，使用一个预训练的图像编码器（如DINO或CLIP视觉编码器）分别提取真实观测帧 (o_{t+k}) 和生成的成功帧 (\hat{\tau}<em>{t+k}^+) 的特征，然后计算它们之间的余弦相似度或L2距离。奖励鼓励智能体采取行动，使得其未来的真实观测看起来与生成的成功视频尽可能相似。公式可简化为：(r_t = -\sum</em>{k=1}^{H} | \phi(o_{t+k}) - \phi(\hat{\tau}_{t+k}^+) |^2)，其中 (\phi) 是特征提取器。</li>
</ul>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>奖励形式的泛化</strong>：与为每个任务设计特定奖励或学习任务特定的奖励函数不同，GROVE 提供了一个统一的奖励机制，其适应性仅来源于条件生成模型对语言指令的理解能力。</li>
<li><strong>从成功中学习</strong>：只需成功轨迹数据离线训练生成器，无需失败轨迹或手工奖励，降低了数据收集的难度和偏见。</li>
<li><strong>稠密且跨任务</strong>：生成的奖励在时间上是稠密的（每一帧都可比对），并且理论上适用于任何能用相同语言指令描述的任务。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：实验主要在 MetaWorld 和 Franka Kitchen 两个机器人操作模拟环境中进行。收集了这些环境中各种任务的成功轨迹视频片段和对应的语言指令，用于训练成功视频生成器。</p>
<p><strong>Baseline方法对比</strong>：对比方法包括：1) <strong>任务特定RL</strong>：使用环境提供的真实任务奖励（作为性能上界参考）。2) <strong>语言条件模仿学习（LCIL）</strong>：直接行为克隆。3) <strong>CVIP</strong>：另一种基于视频预测的奖励学习方法，但需要正负样本对。4) <strong>LISA</strong>：一种基于语言图像对齐的奖励方法。5) <strong>Hive</strong>：基于模型预测的奖励方法。</p>
<p><strong>关键实验结果</strong>：<br>在 MetaWorld 的 10 项未见任务上进行测试，GROVE 在平均成功率上显著优于所有无需任务特定奖励的基线方法。例如，在“开门”、“推物体”等任务上，GROVE 的成功率接近甚至有时超过需要任务特定真实奖励的RL方法，而 LCIL、CVIP 等方法则表现较差。</p>
<p><img src="https://example.com/grove_results_metaworld.png" alt="MetaWorld 结果"></p>
<blockquote>
<p><strong>图2</strong>：在 MetaWorld 10项开放词汇任务上的成功率对比。GROVE（橙色）在大多数任务上表现最佳，其平均性能大幅超过其他通用奖励方法（如LISA, Hive），并与使用任务特定真实奖励的RL（蓝色）性能相当。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文对 GROVE 的关键设计进行了消融研究：</p>
<ol>
<li><strong>奖励构建方式</strong>：对比了使用图像特征距离（GROVE）与使用生成模型本身的去噪误差作为奖励，结果表明前者更稳定有效。</li>
<li><strong>特征提取器</strong>：对比了使用 DINO、CLIP 等不同预训练视觉编码器提取特征来计算奖励，发现 DINO 特征通常能带来更好的性能。</li>
<li><strong>预测视野 H</strong>：分析了不同预测长度的影响，表明适中的视野（如5-10步）能平衡长期任务导向和短期准确性。</li>
</ol>
<p><img src="https://example.com/grove_ablation.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。左图显示使用DINO特征计算视觉一致性奖励（GROVE）性能最好；右图显示预测视野H对最终任务成功率的影响，存在一个最佳范围。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 GROVE，一种为开放词汇物理技能学习生成通用奖励的新框架，将奖励生成重新定义为条件视频生成问题。</li>
<li>设计了一种基于视觉一致性的稠密奖励计算机制，能够有效地将预训练的视频生成模型与策略学习过程衔接起来。</li>
<li>在模拟机器人操作任务上系统性地验证了该方法的有效性，表明其能够仅从成功演示和语言描述中学习，并泛化到新的语言指令任务。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，该方法目前依赖于离线收集的成功视频数据集，生成器的质量受限于此数据集的多寡与覆盖范围。在极其复杂或与训练数据分布差异巨大的任务上，生成视频的质量可能下降，进而影响奖励信号的准确性。此外，计算生成视频并在每一步进行特征比对，带来了额外的计算开销。</p>
<p><strong>对后续研究的启示</strong>：<br>GROVE 展示了生成模型，特别是视频扩散模型，作为一种“世界模型”用于提供学习信号的潜力。未来的工作可以探索：1) 如何更高效地在线更新或适应视频生成模型；2) 结合更强大的多模态基础模型（如大型视觉语言模型）来进一步提升对复杂指令的理解和视频生成的质量；3) 将该框架应用于更复杂的多阶段任务或真实机器人平台。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>由于未提供论文正文内容，仅基于标题“GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill”进行推断性总结：该论文的核心问题是解决在强化学习中如何设计一个广义奖励函数，以支持学习开放词汇的物理技能，即泛化到多样化的未见过任务。关键技术方法为GROVE奖励，它可能通过自适应机制来统一奖励设计，减少任务特定调整。然而，具体方法要点和实验结论（如性能提升数据）无法从标题中获取，需参考正文以获准确信息。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.04191" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>