<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>When Robots Should Say &#34;I Don&#39;t Know&#34;: Benchmarking Abstention in Embodied Question Answering - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>When Robots Should Say &#34;I Don&#39;t Know&#34;: Benchmarking Abstention in Embodied Question Answering</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.04597" target="_blank" rel="noreferrer">2512.04597</a></span>
        <span>作者: Wu, Tao, Zhou, Chuhao, Zhao, Guangyu, Cao, Haozhi, Pu, Yewen, Yang, Jianfei</span>
        <span>日期: 2025/12/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前的具身问答（EQA）任务要求智能体在3D环境中导航、感知并回答自然语言问题。现有EQA基准（如OpenEQA）普遍假设所有查询都必须被回答，忽略了现实人机交互中经常存在信息不完整或指令模糊的情况。强制回答缺乏充分证据的问题会导致“幻觉”，这在具身场景中不仅是语言错误，更可能引发物理伤害或无效导航。</p>
<p>本文针对EQA智能体缺乏“知道何时不知道”能力这一具体痛点，提出了评估“弃权”行为的新视角。通过对500条人类查询的初步研究发现，32.4%的问题因缺乏上下文证据而需要弃权，这表明不确定性是真实人机交互的固有特性。本文的核心思路是：首先通过分析人类沟通错误建立系统化的弃权分类法，并基于此创建首个包含清晰与模糊查询的大规模基准AbstainEQA，用以全面评估智能体的弃权能力与推理质量。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的核心是构建AbstainEQA基准，并基于此对现有模型进行评估。整体流程包括：1）通过人类调查量化弃权需求并建立分类法；2）基于分类法进行大规模人工数据标注，构建包含清晰与模糊查询对的数据集；3）设计全面的评估协议，从回答准确性、弃权识别和具身导航三个维度评估智能体。</p>
<p><img src="https://arxiv.org/html/2512.04597v1/x2.png" alt="分类与数据统计"></p>
<blockquote>
<p><strong>图2</strong>：AbstainEQA中人类标注的模糊查询示例及数据集统计。左图展示了五类弃权问题的具体案例，右图统计了各类别在数据集中的分布情况。</p>
</blockquote>
<p>核心模块是系统化的弃权分类法。作者借鉴Norman的人类错误认知框架，根据智能体在感知客观世界和理解人类主观指令两方面的认知局限，将需要弃权的模糊查询分为五类：</p>
<ol>
<li><strong>行动能力限制</strong>：问题需要物理交互（如打开容器），超出了仅具视觉感知能力的EQA智能体的能力范围。</li>
<li><strong>指代不明确</strong>：指令中的描述（如“白色的柜子”）在环境中对应多个实体，缺乏明确的指代。</li>
<li><strong>信息不可用</strong>：回答问题所需的关键空间（如房间面积）或时间（如过去谁放了花瓶）信息无法从现有观察中推断。</li>
<li><strong>错误预设</strong>：问题中的前提假设（如“床上的泰迪熊”）与观察证据相矛盾。</li>
<li><strong>偏好依赖</strong>：问题依赖于主观判断或审美评价（如“画好看吗”），而非客观事实感知。</li>
</ol>
<p>数据构建过程以OpenEQA为基础。对于每段视频，标注者根据上述五类生成模糊的QA对，并与原始的清晰OpenEQA实例配对，最终形成包含1,636个弃权案例和1,636个可回答案例的平衡数据集。为增加多样性，所有问题均使用大语言模型（LLM）改写成5个语义等效的变体，共计16,360个QA对。此外，标注者还为每个案例标记了关键帧，为清晰查询提供支持答案的证据，为模糊查询突出弃权原因（如信息缺失），并附上简要的文本解释。</p>
<p>与现有方法相比，创新点在于首次在EQA中系统性地定义了弃权问题，并创建了首个同时评估回答能力和弃权识别能力的基准。现有基准（如表1所示）要么不包含模糊查询，要么问题来源非人工标注，而AbstainEQA通过精细的人工标注，确保了模糊性的真实性和评估的全面性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了新构建的AbstainEQA基准，包含来自HM3D和ScanNet的具身视频。评估了在Episodic-Memory EQA（EM-EQA）和Active EQA（A-EQA）两种设置下的表现。对比的基线包括前沿的闭源和开源视觉语言模型（VLM），如GPT-4o、Gemini-2.5-Pro、Qwen系列等。评估采用LLM-as-a-Judge（GPT-4o）框架，并经过人工验证（相关性0.88）。</p>
<p><strong>关键实验结果如下：</strong></p>
<ol>
<li><strong>模型弃权能力普遍低下</strong>：如表2所示，即使最佳模型Gemini-2.5-Pro的整体弃权召回率也仅为42.79%，远低于人类的91.17%。模型在“信息不可用”上表现相对较好，但在需要语用消歧或主观判断的“指代不明确”和“偏好依赖”上表现很差。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.04597v1/x5.png" alt="弃权召回率对比"></p>
<blockquote>
<p><strong>图5</strong>：Qwen-VL模型家族内不同规模模型的弃权召回率。模型规模的增加在一定程度上有助于提升弃权能力。</p>
</blockquote>
<ol start="2">
<li><p><strong>模型缩放效果有限</strong>：模型规模的增加在单一模型家族内能提升弃权性能（图5），但这一效果不能跨架构推广。例如，某些更大参数量的开源模型性能仍低于较小的闭源模型，表明弃权能力更依赖于跨模态对齐和训练目标，而非单纯参数数量。</p>
</li>
<li><p><strong>模糊查询损害导航效率</strong>：在A-EQA任务中，面对需要弃权的问题时，导航成功率从77.17%显著下降至61.41%，而探索所需的总帧数和快照数却大幅增加（表3）。路径长度分布分析（图3）和具体案例（图4）表明，智能体在不确定性下表现出行为不稳定，在过早终止和过度探索之间振荡。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.04597v1/x3.jpg" alt="路径长度分布变化"></p>
<blockquote>
<p><strong>图3</strong>：GPT-4o在回答清晰查询与面对需弃权查询时的路径长度分布对比。模糊查询导致路径长度分布发生显著变化（p&lt;0.001）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.04597v1/x4.png" alt="无效探索案例"></p>
<blockquote>
<p><strong>图4</strong>：模糊查询导致无效探索的两种典型案例。左图为过早终止（路径缩短），右图为过度探索（路径延长）。</p>
</blockquote>
<ol start="4">
<li><p><strong>提示工程与推理的局限性</strong>：通过添加要求模型“谨慎回答”的提示（粗粒度或细粒度），可以大幅提高弃权召回率（表4），但这是以牺牲精确度和准确率为代价的，表明模型只是在被引导过度弃权，而非真正学会了评估证据充分性。同样，添加“思维链”推理提示不仅未能改善弃权性能，反而通常导致召回率和回答正确率下降（表5）。</p>
</li>
<li><p><strong>监督微调（SFT）导致文本过拟合</strong>：在Qwen2.5-VL-7B上进行SFT后，弃权检测的F1分数从40%大幅提升至86%以上（表6）。然而，仅使用问题文本进行微调的模型达到了与使用多模态数据微调相当的性能，而传统的文本分类器（TF-IDF+LR, BERT）也表现出色。对问题文本的词频分析（图6）显示，回答类问题和弃权类问题在词汇分布上存在明显差异，这表明SFT后的模型主要学习了文本表面的统计线索，而非真正的多模态不确定性推理。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.04597v1/x6.png" alt="词频分布差异"></p>
<blockquote>
<p><strong>图6</strong>：清晰（回答）问题与模糊（弃权）问题的词频分布热图。两者在词汇使用上存在系统性差异，解释了SFT模型过拟合文本线索的现象。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1）通过大规模人类调查，建立了首个针对具身问答的、系统化的弃权分类法；2）基于此分类法，创建了首个大规模人工标注的基准AbstainEQA，用于评估智能体的回答与弃权能力；3）对前沿VLM、模型缩放、提示工程、推理和微调策略进行了全面评估，系统性地揭示了现有方法在弃权任务上性能有限、鲁棒性差，且容易过拟合文本线索。</p>
<p>论文明确指出了自身发现的局限性：当前的监督微调方法虽然能在指标上提升弃权检测性能，但本质上是过拟合到了问题文本的表面模式，而非真正学会了基于多模态感知证据进行不确定性推理。</p>
<p>本研究对后续工作的启示深远。首先，它确立了弃权是具身场景中实现可靠人机交互的基本前提，也是进行有效澄清对话的必要基础。其次，实验结果强烈表明，弃权能力不能通过简单的模型缩放、提示工程或对文本模式的监督学习来获得根本性提升。未来的研究需要设计专门的架构或训练机制，使智能体能够基于动态收集的多模态感知证据，对其知识状态进行校准，从而真正地“知道何时不知道”。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身问答（EQA）中机器人被要求必须回答所有问题，而现实中常因信息不足导致“幻觉”的问题，提出了“弃权”能力——即知道何时应回答“我不知道”。研究基于人类查询分析和认知理论，定义了五种需弃权的问题类别，并构建了包含1,636个模糊问题的AbstainEQA基准。实验发现，即使最优前沿模型弃权召回率也仅为42.79%，远低于人类的91.17%，且模型难以通过常规方法显著提升该能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.04597" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>