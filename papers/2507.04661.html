<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DRAE: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning and Task Adaptation in Robotics - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DRAE: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning and Task Adaptation in Robotics</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.04661" target="_blank" rel="noreferrer">2507.04661</a></span>
        <span>作者: Mingsheng Shang Team</span>
        <span>日期: 2025-07-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人领域的终身学习或持续学习面临关键挑战，尤其是智能系统需要在各种环境中执行复杂、动态的任务时。传统的强化学习（RL）方法存在灾难性遗忘问题，即学习新任务会导致覆盖先前获得的知识，使智能体对早期任务失效。这一问题在处理动态和奖励结构差异显著的序列任务时尤为突出。最近，混合专家（MoE）模型通过动态分配计算资源给专家子集，展现出处理多样化任务的潜力，但MoE模型在内存管理上仍存在效率问题，并且在处理长期序列任务学习时仍难以克服灾难性遗忘。检索增强生成（RAG）通过用相关外部知识增强模型的决策过程，有望更好地泛化未见任务并减少幻觉，但在需要长期适应的机器人系统中仍未得到充分探索。</p>
<p>本文针对终身学习中灾难性遗忘、知识整合与动态适应的核心痛点，提出了一种结合动态路由、外部知识检索和分层强化学习的新视角。核心思路是提出动态检索增强专家网络（DRAE），通过集成MoE动态路由、参数化RAG（P-RAG）、认知分层控制（ReflexNet-SchemaPlanner-HyperOptima）以及基于狄利克雷过程混合模型（DPMM）的非参数贝叶斯建模，动态整合内部和外部知识，在高效适应新任务的同时最大限度地保留旧知识。</p>
<h2 id="方法详解">方法详解</h2>
<p>DRAE的整体框架整合了四个关键支柱：1) MoE动态路由；2) 参数化检索增强生成（P-RAG）；3) 认知分层控制（ReflexNet-SchemaPlanner-HyperOptima，简称RSHO）；4) 用于终身知识的非参数贝叶斯建模（DPMM）。其中(1)-(3)处理实时决策，(4)实现持续的终身适应。该统一框架建立了一个受人类感觉运动控制原理启发的三层认知处理流程，如公式(1)所示：$\mathcal{S}<em>{t} = \underbrace{\Gamma(\mathbf{x}</em>{t})}<em>{\text{MoE gating}} \otimes \underbrace{\Psi(\mathbf{x}</em>{t};\Theta_{R})}<em>{\text{P-RAG}} \oplus \underbrace{\Phi(\mathbf{h}</em>{t-1})}<em>{\text{Memory}} + \underbrace{\Omega</em>{\text{DPMM}}(\mathbf{z}<em>{t})}</em>{\text{lifelong knowledge}}$。</p>
<p><img src="https://arxiv.org/html/2507.04661v2/x1.png" alt="DRAE架构总览"></p>
<blockquote>
<p><strong>图1</strong>：DRAE架构集成了四个核心组件：(1) 基于MoE的动态路由用于专家选择，(2) P-RAG用于外部知识融合，(3) ReflexNet-SchemaPlanner-HyperOptima（RSHO）分层控制，(4) DPMM用于终身知识保留。右上角细节显示了关键组件的交互，包括内存指导、知识集成和状态反馈机制。关键信息流展示了从增强状态到RSHO的增强控制输入、从SchemaPlanner到分类器的任务路由指导，以及从ReflexNet到解码器的执行反馈。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>MoE动态路由</strong>：给定输入$\mathbf{x}_t$，门控网络$\Gamma$通过softmax函数（公式2）在$K$个专家上产生分布，并激活前$m$个专家。这种选择性激活在容纳专用子网络的同时限制了推理成本。</li>
<li><strong>参数化检索增强生成（P-RAG）</strong>：该模块通过链接外部记忆库$\mathcal{C}$和参数化嵌入$\Theta_R$来解决性能问题和幻觉控制。在每个时间步$t$，将$\mathbf{x}_t$编码为查询$\mathbf{q}_t$，根据相似度检索一个文档子集$\mathcal{D}_t$（公式3），然后使用LoRA（公式4）将聚合的文档嵌入$\mathbf{d}_t$融合到隐藏状态中。由于$\mathcal{C}$是外部的且可以很大，因此不会覆盖模型内部的旧知识。</li>
<li><strong>认知分层控制架构（RSHO）</strong>：<ul>
<li><strong>ReflexNet（具身执行层）</strong>：受人类脊髓反射机制启发，实现快速、低延迟执行。通过自适应PID控制（公式5）将原始观察$\mathbf{o}_t$转换为扭矩命令，其增益$[K_p, K_i, K_d]$通过元学习动态调整。</li>
<li><strong>SchemaPlanner（符号规划层）</strong>：通过神经符号程序合成（公式6）实现任务分解，将低级控制与高级符号推理联系起来，并使用蒙特卡洛树搜索（MCTS）和形式化方法进行验证。</li>
<li><strong>HyperOptima（元优化层）</strong>：实现高级优化和策略评估。超维内存模块使用循环卷积（公式7）并行评估$N$个候选策略，并根据置信度得分（公式8）进行排名选择最优动作。</li>
</ul>
</li>
<li><strong>基于DPMM的终身知识保留</strong>：为了解决即使有RAG，参数模型在旧任务很少被重访时仍可能遭受灾难性遗忘的问题，引入了DPMM来捕获随时间变化的任务级集群。系统维护一个非参数先验$G \sim \mathrm{DP}(\alpha, \mathcal{H})$（公式9）。每个任务$i$被分配一个集群$v_i$和对应参数$\theta_i$（公式10）。如果当前任务与现有集群足够不同，则会创建一个新的混合组件。生成过程（公式11）确保新任务要么与现有集群对齐，要么在不擦除先前参数的情况下产生新集群。</li>
</ol>
<p><strong>组件集成与创新点</strong>：DRAE的创新点体现在其组件的深度协同集成。MoE路由与P-RAG融合，门控网络结合检索到的知识进行上下文感知的路由（公式13）。DPMM通过分析任务分布（公式14）来指导动态专家扩展。所有组件通过一个统一的目标函数（公式15）进行协调训练：$\mathcal{L}<em>{\text{total}} = \underbrace{\mathcal{L}</em>{\text{ReflexNet}} + \mathcal{L}<em>{\text{SchemaPlanner}}}</em>{\text{HRL}} + \alpha(\mathcal{L}<em>{\text{MoE}} + \mathcal{L}</em>{\text{P-RAG}}) + \gamma(\mathcal{L}<em>{\text{HyperOptima}} + \mathcal{L}</em>{\text{DPMM}})$，其中自适应权重$\alpha_t, \gamma_t$基于验证信号调整，以平衡短期利用和长期保留。与现有静态网络或固定检索系统的方法相比，DRAE通过动态适应新旧任务并有效利用内外知识，实现了显著进步。</p>
<p><img src="https://arxiv.org/html/2507.04661v2/x2.png" alt="动态任务处理示例"></p>
<blockquote>
<p><strong>图2</strong>：咖啡杯抓取和倾倒任务执行中的动态中间状态转换。“多任务MoE”面板揭示了跨任务阶段的内部专家激活模式演变。P-RAG知识查询从材料属性评估演变为操作策略需求，而视觉引导处理状态（右侧面板）显示内部注意力从场景分析转移到聚焦的操作点。交互式对话气泡说明了实时决策，DPMM编码这些瞬时模式以供未来保留。这展示了DRAE在动态适应中间状态的同时保持连贯表征的能力。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在配备8块NVIDIA A100 GPU、64核AMD EPYC处理器和1TB RAM的高性能集群上进行，使用PyTorch实现。</p>
<p><strong>评估基准与基线</strong>：DRAE在多个动态多任务场景中进行评估：</p>
<ol>
<li><strong>MimicGen</strong>：多任务机器人操作套件（包含Square、Stack、Hammer等任务）。</li>
<li><strong>DiffusionDrive</strong>：在NavSim模拟器中的基于扩散的自动驾驶任务。</li>
<li><strong>GNT-MOVE</strong>：在LLFF、NeRF Synthetic和Tanks-and-Temples数据集上的可泛化新颖视图合成任务。</li>
<li><strong>UH-1</strong>：在HumanoidML3D数据集上的文本条件人形运动生成任务。<br>对比的基线方法包括：静态MoE基线（如Switch Transformers）、以及各个领域特定的最先进（SOTA）方法（如TH、TT、UniAD、Transfuser、PixelNeRF、MDM等）。</li>
</ol>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>MimicGen操作任务</strong>：如表1所示，DRAE取得了最高的平均成功率0.78，优于其他多任务系统。其总参数量（TP）为190.1M，但通过专家门控，推理时的活跃参数量（AP）仅为42.3M，体现了效率。</li>
<li><strong>DiffusionDrive自动驾驶</strong>：如表2所示，DRAE在路线完成率（NC，98.4%）、碰撞避免（DAC，96.2%；TTC，94.9%）和整体规划得分（EP，82.5；PDMS，88.0）上均达到最优。</li>
<li><strong>UH-1人形运动生成</strong>：如表3所示，DRAE在FID（0.350 vs UH-1的0.445）和R Precision（0.780 vs UH-1的0.761）上均有提升。在实体机器人测试中（表4），DRAE在多种上半身任务（如Clapping、Open Bottle &amp; Drink）上表现出鲁棒的成功率，许多达到100%。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04661v2/imgs/dynamic_regret.png" alt="动态遗憾理论保证"></p>
<blockquote>
<p><strong>图3</strong>：动态遗憾理论保证的示意图。该图说明了DRAE在非平稳环境下的累积损失上界（公式18），其增长与$\sqrt{T(1+P_T)}$相关，其中$P_T$衡量环境非平稳性，证明了其高效适应能力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：论文通过消融实验分析了各组件贡献（相关表格在附录中）。关键发现包括：动态专家扩展对于处理不同的任务体现（如堆叠与穿线）至关重要；当环境反馈不完整时，潜在奖励建模有助于细化策略更新；门控机制仅激活少量专家子集，防止了参数爆炸并控制了推理延迟。尽管动态扩展引入了适度开销，但带来了更高的闭环性能（EP=82.5），其增加的延迟被更好的适应性和减少的遗忘所平衡。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了创新的DRAE框架，首次将动态MoE路由、参数化RAG和分层RL深度集成，以解决机器人终身学习中的灾难性遗忘、知识整合与动态适应问题。</li>
<li>引入了一种基于DPMM的非参数贝叶斯方法，用于终身知识保留，使模型能够在不破坏旧技能的情况下扩展专业知识。</li>
<li>设计了一个受人类感觉运动控制启发的三层认知架构（ReflexNet-SchemaPlanner-HyperOptima），协调跨多个时间尺度的决策。</li>
<li>提供了关于动态遗憾和样本复杂性的理论保证，从理论上证明了DRAE高效适应的能力，并获得了优于现有方法的实证性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身提及的局限性包括：动态路由和专家扩展可能带来额外的计算开销；复杂组件的集成增加了系统调试和优化的难度；在极其快速变化或对抗性的环境中，DPMM的聚类生成和RAG的检索效率可能面临挑战。</p>
<p><strong>后续研究启示</strong>：DRAE展示了将动态架构、外部知识检索、分层决策和非参数记忆进行协同设计的强大潜力。这为后续研究指明了方向：如何进一步优化各组件间的交互效率，降低系统复杂度；如何将类似框架应用于更广泛的持续学习场景（如开放式环境探索）；以及如何加强理论分析，为模块的自动配置提供指导。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DRAE架构，旨在解决机器人终身学习中的灾难性遗忘与动态任务适应问题。其核心技术融合了稀疏门控的混合专家模型进行动态路由，并引入参数化检索增强生成来利用外部知识，同时设计了包含ReflexNet、SchemaPlanner和HyperOptima的分层强化学习框架以实现持续适应与记忆保持。实验表明，在动态机器人操作任务上，DRAE平均任务成功率达82.5%，显著优于传统MoE模型的74.2%，并保持了极低的遗忘率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.04661" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>