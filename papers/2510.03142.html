<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.03142" target="_blank" rel="noreferrer">2510.03142</a></span>
        <span>作者: Xu, Tianyu, Chen, Jiawei, Zhang, Jiazhao, Zhang, Wenyao, Qi, Zekun, Li, Minghan, Zhang, Zhizheng, Wang, He</span>
        <span>日期: 2025/10/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉导航策略因其模仿人类使用自我中心视觉观察进行导航而被广泛认为是一个有前景的方向。然而，视觉观察的光学信息难以像激光雷达点云或深度图那样被显式建模，这随后需要智能模型和大规模数据。现有方法通过学习的策略来隐式解释视觉输入并预测后续动作，但受限于有限的观察视角（例如仅前视）和导航数据集中相对空旷的环境，阻碍了其在更具挑战性场景中的应用。视觉导航面临一个矛盾：真实世界导航数据主要来自单摄像头设置，且缺乏极具挑战性或危险的场景；而合成导航数据允许可定制的摄像头配置并能生成反映挑战性导航能力的数据，但由于非真实感图像而存在模拟到真实（sim-to-real）的差距。本文旨在解决这一矛盾，提出了MM-Nav，一个多视图视觉语言动作（VLA）模型，它捕获机器人周围的360°观察，并从多个合成强化学习（RL）专家学习导航能力。本文的核心思路是：通过多视图感知覆盖全方位环境，并采用一种多专家师生学习框架，将不同专家在特定挑战性场景（抵达、挤过、避让）中习得的专长蒸馏到一个统一的、仅使用RGB图像的VLA策略中，从而结合了VLA的通用性与合成环境中多样化导航数据的优势。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程分为两个主要步骤：(1) 针对不同导航能力训练多个RL专家，并对VLA模型进行初始微调；(2) 在RL专家和VLA学生模型之间进行师生在线训练迭代。</p>
<p><img src="https://arxiv.org/html/2510.03142v1/x1.png" alt="Pipeline of MM-Nav"></p>
<blockquote>
<p><strong>图2</strong>：MM-Nav的流程管线。提出的师生训练流程。独立的RL教师在不同场景中训练以获得多种能力，并将知识蒸馏给VLA学生。随后，学生被部署到特定能力的模拟场景中进行迭代微调。</p>
</blockquote>
<p><strong>核心模块1：针对不同导航能力的RL专家</strong><br>为了获得不同的导航能力，作者构建了三个不同的模拟环境来训练RL专家：1) <strong>抵达（Reaching）</strong>：在避开静态障碍物的同时接近并抵达特定点目标；2) <strong>挤过（Squeezing）</strong>：在障碍物和墙壁之间杂乱、狭窄的缝隙中挤过；3) <strong>避让（Avoiding）</strong>：主动避开以随机速度移动的密集动态障碍物。所有专家均使用近端策略优化（PPO）算法训练。专家的观察输入包括来自机器人四个侧面摄像头的深度图像、上一时刻的动作以及目标相对位置。这些观察通过ResNet-18编码，与历史标记等特征拼接后，由一个三层MLP预测连续的速度动作 <code>[v_x, v_y, v_yaw]</code>。通过设计不同的奖励函数系数，引导专家专注于各自的能力。</p>
<p><strong>核心模块2：学生VLA模型</strong><br>学生VLA模型以多视图RGB图像序列和点目标作为输入，输出连续速度命令。具体而言：</p>
<ul>
<li><strong>视觉编码</strong>：使用长度为8的滑动窗口，包含四个视图（前、右、后、左）的RGB图像历史。使用视觉基础模型（SigLIP）和跨模态投影器将帧编码为视觉标记。采用分层标记策略：对最新观察使用细粒度标记，对导航历史使用粗粒度标记，以控制序列长度（共192个标记），保证约7 Hz的推理速度。</li>
<li><strong>动作预测</strong>：将相对点目标格式化为文本提示，编码为语言标记。将视觉标记和语言标记拼接后输入大语言模型（Qwen2-7B），得到预测的动作标记，最后通过一个动作头（两层MLP）解码为速度。训练损失结合动作预测的均方误差损失和用于缓解sim-to-real差距的开放世界问答数据的交叉熵损失。</li>
</ul>
<p><strong>核心模块3：RL专家-VLA迭代训练</strong><br>这是一个两阶段过程：</p>
<ol>
<li><strong>初始专家数据收集与VLA微调</strong>：从各RL专家收集成功轨迹（共50万步），用于对VLA模型进行初始行为克隆微调。</li>
<li><strong>师生在线训练迭代</strong>：将初步训练的VLA模型部署到三个模拟场景中，收集对应RL教师给出的专家动作。关键创新在于提出了<strong>能力平衡的数据聚合方法</strong>。该方法使用加权旅行时间（WTT）来衡量VLA学生与RL教师在各个能力上的性能差距，并基于此差距动态计算每次迭代中来自各专家数据的采集比例 <code>p_Cap.</code>。性能差距越大，对应专家数据的采集比例越高。然后按此比例聚合数据，进一步微调VLA模型，此过程迭代进行直至性能收敛。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.03142v1/x2.png" alt="Training Strategy of MM-Nav"></p>
<blockquote>
<p><strong>图3</strong>：MM-Nav的训练策略。阶段1：首先在不同环境中训练RL专家，并收集成功轨迹用于VLA微调。阶段2：然后基于VLA的观察在线收集RL专家数据，并动态平衡训练数据比例。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>多视图360°感知</strong>，克服了单视角视野受限的问题；2) <strong>直接输出连续速度命令</strong>，相比离散动作空间能实现更敏捷的响应；3) <strong>从多个使用特权深度信息的RL专家进行知识蒸馏</strong>，使最终策略仅凭RGB图像就能推断环境几何；4) <strong>动态平衡的在线迭代训练策略</strong>，针对不同能力的掌握程度自适应调整数据分布，优化训练效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真基准</strong>：在IsaacLab模拟器中构建了三个针对特定能力（抵达、挤过、避让）的测试场景和一个混合场景进行定量评估。</li>
<li><strong>真实世界</strong>：构建了四个多样化场景（狭窄之字形走廊、细障碍物避让、动态环境、杂乱静态环境）以评估sim-to-real转移和泛化能力。</li>
<li><strong>对比基线</strong>：iPlanner [55]、ViPlanner [56]、NavDP [10]。</li>
<li><strong>评估指标</strong>：成功率（SR%）、碰撞率（CR%）、加权旅行时间（WTT，秒）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>仿真定量结果如表I所示。MM-Nav在几乎所有测试场景中都取得了最高的成功率、最低的碰撞率和最短的加权旅行时间。特别是在NavDP表现尚可的抵达场景，MM-Nav成功率（80% vs 69%）和碰撞率（20% vs 31%）更优。在更具挑战性的挤过和避让场景，MM-Nav优势显著（挤过：SR 71% vs 18%；避让：SR 68% vs 27%），这验证了其多视图感知和连续速度控制在复杂、动态、受限环境中的优势。</p>
<p><strong>定性结果</strong>：<br>在真实世界的四个场景中，MM-Nav展示了强大的零样本sim-to-real转移能力。例如，能在狭窄走廊中精确调整通过，成功避让激光雷达难以检测的薄织物条，并能泛化到训练中未见的物体（如椅子）和材料（如部分透明的玻璃墙）。</p>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2510.03142v1/x3.png" alt="Plot of online training iteration"></p>
<blockquote>
<p><strong>图4</strong>：在线训练迭代图示。（左）：VLA模型与RL专家之间的性能差距 <code>g_Cap.</code>。（右）：我们在线收集数据的不同比例。第四次迭代后，VLA在所有能力上（以WTT衡量）均超越专家，导致数据比例相等。</p>
</blockquote>
<p>作者评估了在线迭代训练过程中性能的提升。如图6所示（注：图6描述在正文中但未提供链接，此处根据文本描述总结），初始仅通过行为克隆训练的VLA模型在所有三种能力上均存在明显性能差距，尤其在挤过能力上。经过几次在线迭代训练，模型性能逐步提升。能力平衡的数据聚合方法在首次迭代中更侧重于挤过能力的数据，导致了该能力性能的显著跃升。随着迭代继续，VLA模型最终在所有能力上均超越了专门的RL教师，证明了整合多种能力产生的协同效应。消融实验验证了动态平衡训练策略对于均衡提升各项能力的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了MM-Nav</strong>：一个基于多视图（360°）视觉语言动作（VLA）模型的鲁棒视觉导航框架，该框架直接输出连续速度命令，实现了敏捷的机器人控制。</li>
<li><strong>设计了多专家师生学习框架</strong>：通过训练多个使用特权深度信息的RL专家（专精于抵达、挤过、避让），并将它们的知识蒸馏到一个仅使用RGB图像的统一VLA策略中，有效解决了从纯视觉输入学习几何理解的挑战。</li>
<li><strong>引入了能力平衡的在线迭代训练策略</strong>：通过动态评估VLA学生与RL教师在各项能力上的差距，自适应地调整训练数据比例，从而高效地实现了多种导航能力的融合与提升。</li>
</ol>
<p><strong>局限性</strong>：论文自身提及的局限性包括：模拟环境与真实世界在视觉外观上仍存在差异（sim-to-real gap），尽管通过引入VQA数据得以缓解；当前方法主要针对点目标导航，未涉及更高级的语义或语言指令导航。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>连续动作空间的优势</strong>：证明了直接预测连续速度命令在复杂导航任务中比离散动作更具灵活性和响应能力。</li>
<li><strong>多视图感知的价值</strong>：全方位视觉感知对于在密集、动态环境中实现鲁棒导航至关重要，是未来系统设计的重要方向。</li>
<li><strong>异构专家知识蒸馏</strong>：展示了如何通过整合多个专注于不同子任务的专家策略，来培养一个性能超越任一专家的通用智能体，这为构建更全能的行为模型提供了思路。框架中使用的量化等技术也有助于未来在边缘设备上的部署。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉导航中视觉信息难以明确建模、现有方法受限于单视角和简单环境的问题，提出MM-Nav多视角视觉-语言-动作模型。该方法基于预训练大模型实现360°观测，通过多专家学习从三个强化学习专家获取“到达”、“挤过”和“避障”能力的数据，并动态平衡训练比例。实验表明，模型在合成环境中展现出强泛化能力，学生VLA模型性能超越RL教师模型，真实世界实验验证了其有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.03142" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>