<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LAOF: Robust Latent Action Learning with Optical Flow Constraints - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>LAOF: Robust Latent Action Learning with Optical Flow Constraints</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16407" target="_blank" rel="noreferrer">2511.16407</a></span>
        <span>作者: Wei Li Team</span>
        <span>日期: 2025-11-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>从大规模视频中学习潜在动作对于具身基础模型的预训练至关重要。当前主流方法是基于潜在动作策略（LAPO）范式，其核心是联合训练一个逆动力学模型（IDM）和一个前向动力学模型（FDM），通过重建下一帧状态的无监督目标来学习编码帧间变化的潜在动作。然而，LAPO隐含假设连续观测间的所有变化仅由智能体动作引起，这在真实世界视频中不成立，因为环境中存在由随机因素（如移动的背景物体）引发的动态干扰。仅依赖重建目标，缺乏明确的物理约束，容易导致潜在动作表示与视觉外观纠缠，而非准确捕捉智能体运动。</p>
<p>为缓解此问题，现有方法通过解码潜在动作到真实动作来引入动作监督，以减少干扰物影响并增强物理一致性。但动作标签在实际中极其稀缺，与海量视频相比，这种交替训练（在标注和未标注数据间切换）通常不稳定且容易过拟合。在稀疏监督下，模型易漂移到视觉特征的伪相关性上，阻碍潜在动作锚定在有物理意义的动作空间中。</p>
<p>本文针对在动作标签稀缺条件下学习鲁棒潜在动作表示的痛点，提出了利用光流作为伪监督信号的新视角。其核心思路是：利用智能体的光流（像素级运动）作为与真实动作强相关的辅助信号，直接约束潜在动作学习，从而在无需或仅需少量动作标签的情况下，获得对干扰物更鲁棒、物理一致性更强的潜在表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>LAOF的训练流程是一个三阶段pipeline：预训练、蒸馏和微调，分别对应特定的数据集。预训练阶段使用未标注数据集 $\mathcal{D}<em>{\text{unlabel}}={(o_t, f</em>{\text{rgb},t})}$，目标是学习高质量的潜在动作表示。蒸馏阶段使用包含语言指令的高质量数据，通过冻结的预训练IDM标注潜在动作，构建潜在动作标注数据集 $\mathcal{D}<em>{\text{latent}}={(o_t, z_t, l_t)}$，用于训练潜在策略 $\pi$。微调阶段仅使用小规模动作标注数据集 $\mathcal{D}</em>{\text{action}}={(o_t, a_t, l_t)}$，训练一个动作解码器将潜在动作映射到物理动作。</p>
<p><img src="https://arxiv.org/html/2511.16407v1/images/framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：LAOF框架概览。连续的观测 $(o_t, o_{t+1})$ 及其对应的RGB格式光流 $f_{\text{rgb},t}$ 被编码到特征空间 $(s_t, s_{t+1}, f_t)$。逆动力学模型、前向动力学模型以及光流解码器，在下一状态重建损失和光流约束的联合监督下进行优化。</p>
</blockquote>
<p>整体框架在LAPO的基础上，集成了一个光流解码器 $d_{\text{flow}}: \mathcal{Z} \rightarrow \mathcal{F}<em>{\text{rgb}}$。IDM实现为时空Transformer，用于从连续状态对 $(s_t, s</em>{t+1})$ 推断潜在动作 $z_t$。FDM实现为空间Transformer，用于基于当前状态和潜在动作预测下一状态 $\hat{s}_{t+1}$。新增的光流解码器直接将潜在动作 $z_t$ 解码为预测的光流特征 $\hat{f}_t$。</p>
<p>核心创新在于引入了光流约束损失 $\mathcal{L}<em>{\text{flow}}(t) := |\hat{f}<em>t - f_t|<em>2$，其中 $f_t$ 是由预训练视觉编码器（DINOv2）编码的RGB格式光流伪标签。预训练的总目标为重建损失与光流损失之和：$\mathcal{L}</em>{\text{pretrain}} = \mathcal{L}</em>{\text{reconstruction}} + \mathcal{L}</em>{\text{flow}}$。这为潜在动作学习提供了直接的物理运动约束。</p>
<p>对于含有少量动作标签的场景，论文进一步提出了LAOF-Action。它在LAOF基础上增加了一个轻量级MLP动作解码器 $d_{\text{action}}$。其预训练目标为 $\mathcal{L}<em>{\text{pretrain}} = \mathcal{L}</em>{\text{reconstruction}} + (1-\lambda) \cdot \mathcal{L}<em>{\text{flow}} + \lambda \cdot \mathcal{L}</em>{\text{action}}$，其中 $\lambda$ 根据数据集中动作标签的比例设置。训练采用交替更新策略：未标注数据更新光流解码器，标注数据更新动作解码器。</p>
<p>技术细节方面，论文采用RAFT预训练模型生成光流伪标签。为适配DINOv2编码器，将原始（u, v）格式的光流转换为RGB格式：将运动方向映射到色相（Hue），将归一化后的运动幅度映射到饱和度（Saturation）和明度（Value）。对于存在动态干扰物的场景，采用LangSAM生成对象中心语义掩码，过滤出仅与智能体相关的对象中心光流作为监督信号。</p>
<p><img src="https://arxiv.org/html/2511.16407v1/images/hsv.png" alt="光流可视化"></p>
<blockquote>
<p><strong>图2</strong>：LIBERO和PROCGEN上的光流可视化。使用RAFT估计的帧间光流显示在每张图像下方，表示从当前帧到下一帧的运动。颜色表示运动方向（如紫色对应向上运动）。在干扰物静止的任务（如LIBERO）中可直接提取智能体光流；对于动态干扰物场景，则使用LangSAM隔离对象中心光流。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了两个基准：用于模仿学习的机器人操作基准<strong>LIBERO</strong>（SPATIAL, OBJECT, GOAL, LONG四个任务套件）和用于强化学习的程序生成基准<strong>PROCGEN</strong>（BIGFISH, CHASER, LEAPER, HEIST四个游戏）。对比的基线方法包括：无监督的LAPO和CoMo，以及使用1%动作标签的监督方法LAOM-Action。评估指标包括动作预测均方误差（MSE）、任务成功率（LIBERO）、动作分类准确率和标准化回合回报（PROCGEN）。</p>
<p><img src="https://arxiv.org/html/2511.16407v1/x1.png" alt="模仿学习结果"></p>
<blockquote>
<p><strong>图3</strong>：子图(a)(b)比较了连续（实线）与离散（虚线）潜在动作表示在下游任务上的性能。在所有任务中，连续表示的曲线下面积均大于离散表示，表明连续表示更有效（回答问题Q1）。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ol>
<li><strong>表示形式对比（Q1）</strong>：在LIBERO和PROCGEN上，连续潜在动作表示的性能均优于离散表示（图3 a,b）。</li>
<li><strong>光流约束有效性（Q2）</strong>：在LIBERO上，无监督的LAOF相比LAPO在平均成功率上提升+4.2%，动作预测MSE降低0.069；在PROCGEN上，LAOF相比LAPO在平均分类准确率上提升+16.20%，平均回报提升+0.16（表1，表2）。加入光流约束的CoMo w/ OF也一致优于原始CoMo。</li>
<li><strong>光流约束的增益范围（Q3）</strong>：如图4所示，在PROCGEN上，即使动作标签比例增加到10%，引入光流约束（LAOF）相比基线（LAPO）仍能带来性能增益。</li>
<li><strong>光流对动作标签的替代性（Q4）</strong>：在极端的1%动作比例下，<strong>无任何动作监督的LAOF匹配甚至超越了使用1%动作标签的监督方法LAOM-Action</strong>（图4）。在LIBERO上，LAOF-Action（1%标签）相比LAOM-Action取得了更低的MSE（-0.096 vs -0.066）和更高的平均成功率（+11.5% vs +8.7%）（表1）。</li>
<li><strong>架构消融（Q5）</strong>：论文探索了多种引入光流约束的架构变体（如将解码器连接到FDM输出），发现<strong>将专用的光流解码器直接连接到潜在动作上能获得最佳性能</strong>。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16407v1/x2.png" alt="稀疏监督下的性能"></p>
<blockquote>
<p><strong>图4</strong>：在PROCGEN上评估不同动作比例下离散潜在动作的质量。光流约束显著提高了基线的训练稳定性和学习表示的质量。值得注意的是，在1%动作比例下，无监督的LAOF匹配甚至超越了有监督的LAOM-Action。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：</p>
<ol>
<li><strong>引入光流作为伪监督信号</strong>：提出利用智能体的光流为潜在动作学习提供物理运动约束，从而在动作标签稀缺或缺失的情况下，学习到对干扰物更鲁棒、物理一致性更强的潜在动作表示。</li>
<li><strong>验证了在稀疏监督下的有效性</strong>：实验表明，光流约束能显著稳定训练并提升表示质量，其增益可持续到10%的动作比例。更重要的是，无监督的LAOF能够匹配或超越仅使用1%动作标签的监督方法，展示了光流作为动作标签替代信号的潜力。</li>
<li><strong>对连续与离散表示形式的评估</strong>：通过系统实验，证实了在所述任务中，连续潜在动作表示通常优于离散表示。</li>
</ol>
<p>论文提到的局限性包括：光流伪标签的生成依赖于预训练的RAFT模型；对于存在动态干扰物的场景，需要额外的对象分割工具（如LangSAM）来提取对象中心光流。</p>
<p>这项工作对后续研究的启示是：利用易于获取或生成的物理信号（如光流、深度、触觉等）作为伪监督，是缓解具身AI模型训练中动作标签稀缺问题的一条有效途径。它鼓励研究者探索更多样、更直接的物理约束来引导无监督或弱监督表示学习，从而推动可扩展的具身基础模型的发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LAOF框架，旨在解决大规模视频预训练中潜在动作学习易受动作无关干扰（如动态背景）影响的问题。该方法利用光流作为动作驱动的伪监督信号，通过约束潜在动作表示来抑制背景并聚焦智能体运动，从而提升表征的鲁棒性。实验表明，LAOF学到的表征在下游模仿学习与强化学习任务上优于现有方法；在动作标签极少甚至为零时，其性能可匹配使用1%标签的监督方法，且在标签比例增至10%时仍保持稳定提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16407" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>