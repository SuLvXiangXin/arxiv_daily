<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MiMo-Embodied: X-Embodied Foundation Model Technical Report - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>MiMo-Embodied: X-Embodied Foundation Model Technical Report</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16518" target="_blank" rel="noreferrer">2511.16518</a></span>
        <span>作者: Hao, Xiaoshuai, Zhou, Lei, Huang, Zhijian, Hou, Zhiwen, Tang, Yingbo, Zhang, Lingfeng, Li, Guang, Lu, Zheng, Ren, Shuhuai, Meng, Xianhui, Zhang, Yuchen, Wu, Jing, Lu, Jinghui, Dang, Chenxu, Guan, Jiayi, Wu, Jianhua, Hou, Zhiyi, Li, Hanbing, Xia, Shumeng, Zhou, Mingliang, Zheng, Yinan, Yue, Zihao, Gu, Shuhao, Tian, Hao, Shen, Yuannan, Cui, Jianwei, Zhang, Wen, Xu, Shaoqing, Wang, Bing, Sun, Haiyang, Zhu, Zeyu, Jiang, Yuncheng, Guo, Zibin, Gong, Chuhong, Zhang, Chaofan, Ding, Wenbo, Ma, Kun, Chen, Guang, Cai, Rui, Xiang, Diyun, Qu, Heng, Luo, Fuli, Ye, Hangjun, Chen, Long</span>
        <span>日期: 2025/11/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言模型（VLM）在具身智能领域取得了显著进展，但主要分为两个独立方向：专注于室内任务（如机器人操作、导航）的具身AI模型（如RoboBrain、VeBrain）和专注于户外道路任务的自动驾驶模型（如RoboTron-Drive、DriveLMM-o1）。这些专用模型虽然在其各自领域表现出色，但受限于狭窄的应用场景。室内具身AI与室外自动驾驶之间存在显著的领域鸿沟，阻碍了模型在跨领域场景中的空间理解和推理能力的泛化。现有挑战包括：（1）缺乏统一的具身VLM，现有模型各自为政，无法有效桥接自动驾驶与具身AI；（2）缺乏全面的跨具身能力评估基准。本文针对这些痛点，提出了首个旨在统一自动驾驶与具身AI任务的跨具身基础模型MiMo-Embodied。其核心思路是通过精心构建的多领域数据集和渐进式的四阶段训练策略，让模型同时学习两个领域的知识，并利用它们之间的正向迁移和相互增强，最终在两大领域的多个基准上实现最先进的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>MiMo-Embodied的整体框架是一个标准的视觉-语言模型架构，但其创新性主要体现在训练数据和训练策略上，旨在同时服务于自动驾驶和具身AI任务。</p>
<p><img src="https://arxiv.org/html/2511.16518v1/x3.png" alt="模型架构"></p>
<blockquote>
<p><strong>图3</strong>：MiMo-Embodied模型架构。模型由三个核心组件构成：用于编码视觉输入（单图、多图、视频）的Vision Transformer (ViT)、将视觉编码映射到与LLM对齐的潜空间的投影器（MLP），以及负责文本理解和推理的大语言模型(LLM)。</p>
</blockquote>
<p><strong>整体架构与流程</strong>：模型接收视觉（图像/视频）和文本作为输入。视觉输入首先由Vision Transformer (ViT) 编码为视觉token序列。这些视觉token随后通过一个多层感知机（MLP）投影器被映射到与大语言模型（LLM）输入空间对齐的潜表示。最后，LLM接收投影后的视觉表示和文本指令，进行联合理解和推理，并生成文本输出（如答案、规划、解释等）。视觉编码器、投影器和LLM均初始化自MiMo-VL的预训练权重，继承了其强大的视觉-语言对齐和推理能力。</p>
<p><strong>核心创新点</strong>：与现有单一领域专用模型相比，MiMo-Embodied的核心创新在于其<strong>跨领域统一性</strong>和<strong>训练策略</strong>。它并非在架构上做根本性改动，而是通过以下方式实现能力整合：</p>
<ol>
<li><strong>构建全面的跨领域数据集</strong>：模型训练数据涵盖通用多模态理解、具身AI和自动驾驶三大类，为模型提供了感知、预测和规划所需的广泛知识基础。</li>
<li><strong>设计渐进式四阶段训练策略</strong>：这是实现卓越性能的关键。</li>
</ol>
<p><strong>训练策略详解</strong>：</p>
<ul>
<li><strong>阶段1：通用与具身知识学习</strong>：基于通用视觉知识和具身AI数据，建立核心的功用（Affordance）理解、高层任务规划和空间推理能力。</li>
<li><strong>阶段2：自动驾驶知识学习</strong>：引入自动驾驶数据，与具身数据混合监督，实现跨领域理解能力的整合。</li>
<li><strong>阶段3：思维链（CoT）微调</strong>：引入包含生成推理过程（rationales）的数据，增强模型处理复杂任务时的推理能力。</li>
<li><strong>阶段4：强化学习（RL）微调</strong>：使用GRPO优化方法，针对特定任务性能进行进一步精炼。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.16518v1/x4.png" alt="训练数据概览"></p>
<blockquote>
<p><strong>图4</strong>：MiMo-Embodied使用的训练数据概览。包含三大组成部分：建立基础能力的通用数据集、增强功用/规划/空间感知的具身AI数据集，以及提升感知/预测/规划能力的自动驾驶数据集。</p>
</blockquote>
<p><strong>数据集构成</strong>：</p>
<ol>
<li><strong>通用数据集</strong>：沿用MiMo-VL的训练语料，涵盖视觉定位、文档图表理解、视频理解、多模态推理等，建立广泛的感知和推理基础。</li>
<li><strong>具身AI数据集</strong>：按能力分为三类。<ul>
<li><em>功用预测</em>：整合PixMo-Points（指向计数与解释）、RoboAfford（物体与场景级交互推理）、RoboRefIt（杂乱场景下的指代表达理解）。</li>
<li><em>高层任务规划</em>：整合Cosmos-Reason1（大规模物理现实 grounding 与推理）、EgoPlan-IT（第一人称视角规划）、RoboVQA（长视野活动序列QA）。</li>
<li><em>空间理解</em>：整合SQA3D及自建数据（3D问答与定位）、VLM-3R（空间推理与导航QA）、RefSpatial（机器人指向任务的空间理解）、EmbSpatial-SFT（具身视角空间关系识别）。</li>
</ul>
</li>
<li><strong>自动驾驶数据集</strong>：按功能分为三类。<ul>
<li><em>环境感知</em>：包括通用场景理解（CODA-LM, DriveLM等）和区域物体理解/定位，用于理解交通场景和关键目标。</li>
<li><em>状态预测</em>：主要指意图预测，使用DriveLM、MME-Realworld等数据，预测周围交通参与者的未来行为。</li>
<li><em>驾驶规划</em>：包括行动决策（预测自车高级别动作，如DriveLM）和驾驶推理（输出带有解释的驾驶决策，如CODA-LM）。</li>
</ul>
</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>评估基准与对比方法</strong>：MiMo-Embodied在<strong>17个具身AI基准</strong>（覆盖功用预测、任务规划、空间理解）和<strong>12个自动驾驶基准</strong>（覆盖环境感知、状态预测、驾驶规划）上进行了全面评估。对比的基线包括：（1）通用开源VLM（如LLaVA-NeXT、CogVLM2）；（2）通用闭源VLM（如GPT-4o、Gemini-1.5 Pro）；（3）领域专用模型（如用于具身AI的RoboBrain-2、VeBrain，用于自动驾驶的Drive-LMM、RoboTron-Drive）。</p>
<p><img src="https://arxiv.org/html/2511.16518v1/x1.png" alt="性能对比"></p>
<blockquote>
<p><strong>图1</strong>：在自动驾驶和具身AI基准上的性能对比。MiMo-Embodied在两大领域的基准集上均取得了最先进的综合性能，超越了之前的开源、闭源以及专用VLM。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>综合性能领先</strong>：如图1所示，MiMo-Embodied在自动驾驶和具身AI的聚合基准分数上均达到最高，表明其统一的跨领域模型能力优于所有对比模型。</li>
<li><strong>具身AI任务</strong>：在17个基准测试中，MiMo-Embodied在<strong>16个</strong>上取得了最佳性能。例如，在EgoPlan-Bench上，其准确率达到**58.7%<strong>，比最佳专用模型RoboBrain-2高出</strong>2.8%<strong>；在具身AI任务的平均准确率上，比最佳开源模型高出</strong>7.5%<strong>，比最佳闭源模型高出</strong>6.0%**。</li>
<li><strong>自动驾驶任务</strong>：在12个基准测试中，MiMo-Embodied在<strong>10个</strong>上取得了最佳性能。在自动驾驶任务的平均准确率上，比最佳开源模型高出**5.6%<strong>，比最佳闭源模型高出</strong>2.5%**。</li>
<li><strong>消融实验</strong>：论文通过消融实验验证了训练策略和数据构成的有效性。<ul>
<li><em>训练阶段消融</em>：移除任一训练阶段（尤其是阶段2的自动驾驶知识学习或阶段3的CoT微调）都会导致模型在对应领域或复杂推理任务上的性能显著下降，证明了多阶段策略的必要性。</li>
<li><em>数据构成消融</em>：仅使用通用数据或单一领域数据训练的模型，在另一领域任务上表现糟糕，验证了跨领域数据整合对于实现统一能力至关重要。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2511.16518v1/x5.png" alt="消融实验：训练阶段"></p>
<blockquote>
<p><strong>图5</strong>：训练阶段消融实验结果。展示了完整四阶段训练（Ours）与缺少某一阶段（例如w/o S2, w/o S3）的模型在自动驾驶（左）和具身AI（右）任务上的性能对比，证明了每个阶段对最终性能的贡献。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.16518v1/x6.png" alt="定性结果示例"></p>
<blockquote>
<p><strong>图6</strong>：定性结果示例。展示了MiMo-Embodied在自动驾驶（环境感知、驾驶规划）和具身AI（任务规划、空间理解）任务上的输出，体现了其准确、详细且可解释的跨领域推理能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出并开源了首个跨具身（自动驾驶+具身AI）统一基础模型MiMo-Embodied</strong>，成功桥接了室内与室外、静态与动态的具身任务鸿沟。</li>
<li><strong>构建了迄今为止最全面的、面向跨具身能力培养的多模态训练数据集</strong>，系统性地涵盖了通用、具身AI和自动驾驶三大领域，为后续研究提供了宝贵的数据蓝图。</li>
<li><strong>设计了一套有效的渐进式四阶段训练策略</strong>，通过通用预训练、跨领域知识融合、思维链微调和强化学习微调，实现了两个领域知识的正向迁移与性能的相互增强，并在29个基准上取得了最先进的性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，模型在极其复杂和动态的场景中（如密集城市交通中的实时交互、非结构化动态环境中的长视野机器人规划）的实时性能和鲁棒性仍有待进一步验证和提升。这暗示了模型在应对高频率变化和极端不确定性方面可能存在挑战。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>领域统一的有效性</strong>：本工作证明了自动驾驶与具身AI之间存在显著的正向迁移潜力，激励研究者探索更广泛的具身任务统一（如加入无人机、机械臂等），构建更具通用性的“X-具身”智能体。</li>
<li><strong>数据与训练策略的核心地位</strong>：对于实现强大的基础模型，精心设计、规模化的高质量数据以及与之匹配的渐进式训练策略，其重要性可能不亚于模型架构的创新。</li>
<li><strong>评估体系的拓展</strong>：需要发展更多评估跨领域泛化能力、零样本迁移能力以及真实物理世界交互性能的基准，以推动下一代具身基础模型的发展。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>这篇论文提出了首个统一自动驾驶与具身AI的跨具身基础模型MiMo-Embodied，旨在解决现有视觉语言模型（VLMs）在两大领域各自独立、缺乏统一模型与跨域能力评估的局限性。关键技术采用多阶段学习、精心构建的数据集以及思维链/强化学习（CoT/RL）微调方法，促进两领域间的正向能力迁移。实验结果显示，该模型在17个具身AI基准（涵盖任务规划、功能预测与空间理解）和12个自动驾驶基准（包括环境感知、状态预测与驾驶规划）上均达到最先进性能，显著优于现有开源、闭源及专用基线模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16518" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>