<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.02792" target="_blank" rel="noreferrer">2504.02792</a></span>
        <span>作者: Zhu, Chuning, Yu, Raymond, Feng, Siyuan, Burchfiel, Benjamin, Shah, Paarth, Gupta, Abhishek</span>
        <span>日期: 2025/04/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习（尤其是行为克隆）因其简单可扩展性而成为构建通用机器人的有前景的方法。然而，该方法严重依赖高质量、昂贵的机器人演示数据，且学得的策略在训练分布外往往表现脆弱。与此同时，大量描绘广泛环境和多样行为的视频数据易于获取，但因其缺乏动作标注而难以直接用于模仿学习。另一方面，世界模型（如视频扩散模型）能够从数据中学习时序动态，但如何利用这种动态知识来提升模仿学习策略的鲁棒性和泛化能力尚不明确。</p>
<p>本文针对上述痛点，旨在弥合模仿学习与世界建模之间的鸿沟，提出一个能够统一利用带动作的机器人数据和无动作视频数据的框架。其核心思路是：在一个统一的Transformer架构中，耦合动作扩散过程和视频扩散过程，并通过为每个模态独立控制扩散时间步，使单个模型能够灵活地表示为策略、前向动力学、逆向动力学和视频生成器。</p>
<h2 id="方法详解">方法详解</h2>
<p>Unified World Models (UWM) 旨在从联合数据分布 $p(o, a, o&#39;)$ 中学习一个单一的扩散模型，并能在测试时灵活地进行多种推理。其核心创新在于将扩散时间步与“掩码”概念联系起来：将某个模态的扩散时间步设为最大值 $T$（完全加噪）等价于在条件生成中“掩码”掉该变量，而设为 $0$（未加噪）则意味着提供干净的条件。</p>
<p><img src="https://arxiv.org/html/2504.02792v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：UWM整体框架。该模型在一个统一的Transformer架构中集成了动作和图像扩散，并由模态特定的扩散时间步控制。经过大规模机器人数据集训练后，它可以在测试时通过操纵独立的时间步灵活执行多种推理。</p>
</blockquote>
<p>UWM的核心是一个<strong>耦合的噪声预测网络</strong> $s_\theta(o, a_{t_a}, o&#39;<em>{t</em>{o&#39;}}, t_a, t_{o&#39;})$。该网络以当前观测 $o$、在时间步 $t_a$ 下带噪声的动作 $a_{t_a}$、在时间步 $t_{o&#39;}$ 下带噪声的下一观测 $o&#39;<em>{t</em>{o&#39;}}$，以及两个独立的扩散时间步 $t_a$ 和 $t_{o&#39;}$ 作为输入。其目标是预测注入到动作和下一观测中的噪声 $(\epsilon_a, \epsilon_{o&#39;})$。</p>
<p><strong>训练过程</strong>：对于来自专家数据集 $\mathcal{D}<em>e$ 的每个数据三元组 $(o, a, o&#39;)$，独立地随机采样动作扩散时间步 $t_a \sim \text{Uniform}(1, T)$ 和图像扩散时间步 $t</em>{o&#39;} \sim \text{Uniform}(1, T)$。然后根据DDPM的前向过程对干净的动作 $a$ 和下一观测 $o&#39;$ 添加相应程度的噪声，得到 $a_{t_a}$ 和 $o&#39;<em>{t</em>{o&#39;}}$。模型通过去噪得分匹配损失进行训练：$\min_\theta \mathbb{E} [| s_\theta(o, a_{t_a}, o&#39;<em>{t</em>{o&#39;}}, t_a, t_{o&#39;}) - (\epsilon_a, \epsilon_{o&#39;}) |<em>2^2]$。对于无动作视频数据 $\mathcal{D}</em>{af}$，通过始终将动作时间步 $t_a$ 设置为 $T$（即用纯噪声替代动作）来模拟“掩码”动作的条件，从而让模型学习视频预测 $p(o&#39;|o)$。</p>
<p><strong>灵活推理</strong>：训练完成后，通过固定不同的时间步组合，可以从同一模型中采样出不同的分布：</p>
<ol>
<li><strong>策略（模仿学习）</strong>：固定 $t_{o&#39;} = T$（掩码下一观测），从 $t_a = T$ 开始对动作进行去噪。即采样 $p(a|o)$。</li>
<li><strong>前向动力学</strong>：固定 $t_a = 0$（提供干净动作），从 $t_{o&#39;} = T$ 开始对下一观测进行去噪。即采样 $p(o&#39;|o, a)$。</li>
<li><strong>逆向动力学</strong>：固定 $t_{o&#39;} = 0$（提供干净下一观测），从 $t_a = T$ 开始对动作进行去噪。即采样 $p(a|o, o&#39;)$。</li>
<li><strong>视频预测模型</strong>：固定 $t_a = T$（掩码动作），从 $t_{o&#39;} = T$ 开始对下一观测进行去噪。即采样 $p(o&#39;|o)$。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.02792v3/x2.png" alt="训练与推理流程"></p>
<blockquote>
<p><strong>图2</strong>：UWM训练与推理流程。左图展示了在带动作的机器人轨迹上进行预训练，并通过将动作扩散时间步设为T来在无动作视频上协同训练。右图展示了边际和条件推理模式，分别对应策略和逆向动力学。</p>
</blockquote>
<p>与现有方法相比，UWM的创新点在于：1) <strong>统一架构</strong>：将动作与像素预测耦合在同一模型中，实现了特征共享和来自同一数据的额外监督；2) <strong>时间步即掩码</strong>：通过独立控制扩散时间步这一简单机制，统一了多种推理任务；3) <strong>多模态数据利用</strong>：天然支持从无动作视频数据中学习。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟和真实机器人操作任务上进行。<strong>数据集/基准</strong>包括：模拟环境MetaWorld的10个任务、真实世界台面操作任务（如拾放、堆叠、开抽屉）。<strong>对比的基线方法</strong>包括：标准行为克隆（BC）、仅预测动作的扩散策略（Diffusion Policy）、以及两个分别训练动作模型和视频模型的消融版本（Decoupled Action/Video）。</p>
<p><strong>关键定量结果</strong>：在分布内测试中，UWM与基线方法性能相当。但在<strong>分布外泛化</strong>测试中（例如，在模拟中改变物体颜色、纹理、位置；在真实世界中改变物体类别、布局、背景），UWM表现出显著优势。例如，在MetaWorld的分布外泛化测试中，UWM的平均成功率为 **71%**，高于标准行为克隆的 <strong>53%</strong> 和扩散策略的 **60%**。在真实世界任务中，经过无动作视频数据微调后，UWM在分布外场景下的平均成功率从 <strong>63%</strong> 提升至 **78%**。</p>
<p><img src="https://arxiv.org/html/2504.02792v3/x3.png" alt="模拟实验结果"></p>
<blockquote>
<p><strong>图3</strong>：在MetaWorld 10个任务上的模仿学习性能。左图为分布内测试，所有方法性能相近；右图为分布外泛化测试，UWM显著优于基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.02792v3/x4.png" alt="真实世界任务成功率"></p>
<blockquote>
<p><strong>图4</strong>：真实世界台面操作任务的成功率。UWM（绿色）在分布外泛化设置下优于行为克隆（蓝色）和扩散策略（橙色）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.02792v3/x5.png" alt="无动作视频数据的影响"></p>
<blockquote>
<p><strong>图5</strong>：利用无动作视频数据进行微调对策略泛化能力的影响。添加无动作数据后，UWM策略在分布外场景下的性能得到进一步提升。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过对比UWM与“解耦”版本（分别独立训练动作模型和视频模型）来验证耦合训练的重要性。结果显示，耦合训练的UWM在策略性能上优于解耦版本，证明了动作与视频预测之间的特征共享带来了益处。此外，消融实验也证实了利用无动作视频数据进行训练能带来额外的性能提升。</p>
<p><img src="https://arxiv.org/html/2504.02792v3/x6.png" alt="消融研究：耦合训练"></p>
<blockquote>
<p><strong>图6</strong>：消融研究显示，耦合训练（UWM）比分别训练动作和视频模型（Decoupled）性能更好。</p>
</blockquote>
<p><strong>定性结果</strong>：论文展示了UWM生成的前向动力学预测和视频预测样本，表明其能够生成逼真且物理合理的未来帧。</p>
<p><img src="https://arxiv.org/html/2504.02792v3/x7.png" alt="前向动力学预测定性结果"></p>
<blockquote>
<p><strong>图7</strong>：UWM前向动力学预测的定性示例。给定当前观测和专家动作，模型能预测出合理的下一观测。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>Unified World Models (UWM)<strong>，一个通过耦合视频与动作扩散来统一模仿学习和世界建模的新框架；2) 提出利用</strong>独立扩散时间步作为控制机制</strong>，使单一模型能灵活执行策略、动力学、逆向动力学及视频生成等多种推理；3) 实证表明该框架能<strong>有效利用大规模多任务机器人数据和无动作视频数据</strong>，显著提升策略在分布外条件下的泛化性与鲁棒性。</p>
<p>论文提到的局限性包括：方法依赖于扩散模型，其<strong>迭代去噪过程可能带来较高的计算开销</strong>；当前工作主要关注单步预测，未来可扩展至更长的序列建模。</p>
<p>这项工作对后续研究的启示在于：为构建机器人基础模型提供了一个简洁而强大的统一范式，证明了从异构数据（尤其是丰富的无标注视频）中联合学习策略与世界模型是可行的方向，有助于降低对昂贵机器人演示数据的依赖，迈向更可扩展的机器人学习。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习依赖高质量演示数据、难以利用海量无动作标注视频数据的问题，提出统一世界模型（UWM）。该模型在一个统一的Transformer架构中耦合了动作扩散与视频扩散过程，通过独立控制各模态的扩散时间步，可灵活实现策略、动力学预测等多种功能。实验表明，UWM能有效利用大规模异构数据进行预训练，相比单纯模仿学习，其学习到的策略具有更好的泛化性与鲁棒性，并能通过无动作视频进一步提升微调策略的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.02792" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>