<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.23919" target="_blank" rel="noreferrer">2506.23919</a></span>
        <span>作者: Lin Shao Team</span>
        <span>日期: 2025-06-30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用基础模型解决机器人操作泛化问题主要遵循视觉-语言-动作模型范式，可分为两类主流方法。端到端VLA方法直接训练模型将视觉和语言输入映射到低级机器人动作，但其性能严重依赖于海量、多样化的指令-视觉-动作配对数据，这类数据获取成本极高，限制了其零样本泛化能力。分层VLA方法则使用VLM进行高层规划，生成中间表示来指导独立的底层策略，以缓解数据依赖。然而，中间表示的设计面临关键困境：稀疏或符号化表示（如语言描述、关键点）缺乏复杂操作所需的精确几何细节；而密集的视觉表示（如目标图像）虽然信息丰富，但通常需要底层策略经过显式训练才能解读，从而丧失了零样本目标；其他VLM生成的引导（如价值图、约束或奖励函数）则因当前VLM在精确空间推理方面的固有弱点而存在显著不准确性。</p>
<p>本文针对上述痛点，提出了一个新的视角：有效的零样本泛化需要底层策略获得明确的空间引导，但擅长语义推理的强大VLM在精确空间推理方面表现不佳。为此，本文提出一种解耦架构，将VLM用作以对象为中心的世界模型，专注于生成图像空间中的语义目标，而让一个独立的空间接地模块专门负责将此语义目标转化为对象的明确空间引导，从而发挥各自组件的优势。本文核心思路是利用图像生成VLM作为世界模型来生成描绘期望任务结果的目标图像，通过空间接地将其转化为精确的目标位姿，从而指导无需训练的底层策略执行零样本操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>Goal-VLA框架将操作流程解耦为三个阶段：目标状态推理、空间接地和底层策略。输入是单视角RGB-D观测图像O=(I, D)和自然语言任务描述L，输出是完成操作任务的动作序列{ a }_i。</p>
<p><img src="https://arxiv.org/html/2506.23919v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Goal-VLA框架概览，包含三个解耦阶段：(a) 目标状态推理：VLM根据指令生成目标图像，并通过反射-合成过程迭代优化，输出经过验证的目标图像、掩码和深度图。(b) 空间接地：通过特征匹配和点云注册计算初始状态与目标状态之间对象的3D空间变换（旋转R和平移t）。(c) 底层策略：通过接触模块确定初始接触位姿，对该位姿应用对象变换得到夹爪目标位姿，最后通过运动规划模块生成机器人执行轨迹。</p>
</blockquote>
<p><strong>1. 目标状态推理模块</strong><br>该模块负责将抽象语言指令转化为具体、合理的视觉目标（目标图像I&#39;和目标深度图D&#39;）。其核心是一个<strong>反射-合成循环</strong>。</p>
<ul>
<li><strong>提示增强</strong>：首先，使用文本输出VLM（如Gemini 2.5 Pro）根据初始图像I和原始指令L生成一个更详细、描述性更强的增强提示L_e。</li>
<li><strong>循环迭代</strong>：然后进入迭代循环：(1) <strong>目标图像生成</strong>：使用图像生成VLM（Gemini 2.5 Flash-image）根据当前提示和初始图像生成候选目标图像I‘_cand。(2) <strong>合成以供检查</strong>：使用Grounded SAM从候选目标中分割出目标对象，并将其作为“虚拟图像”以部分透明的方式叠加到初始场景上，生成合成图像I_synth。这提供了目标在上下文中的可视化，缩小了语义鸿沟。(3) <strong>反思与细化</strong>：将合成图像、原始指令和当前增强提示输入到反思器VLM（Gemini 2.5 Pro）中进行评估。如果合成结果符合任务语义要求，则循环终止；否则，反思器会生成包含纠正反馈的修订提示，用于下一次生成迭代。循环持续直到目标图像被验证或达到最大尝试次数。</li>
<li><strong>后处理</strong>：一旦目标图像被接受，使用深度估计模型（Depth-Anything V2）生成目标相对深度图，并使用Grounded SAM提取初始图像和目标图像中目标对象的掩码（M和M’）。</li>
</ul>
<p><strong>2. 空间接地模块</strong><br>该模块将视觉目标转化为精确的3D对象变换（旋转R和平移t），包含两个阶段：</p>
<ul>
<li><strong>语义匹配</strong>：由于生成的目标图像语义正确但可能不保留原始对象的实例级外观，传统光流或2D像素跟踪不可靠。因此，本文使用基于语义特征的匹配模型Geo-Aware，提取初始图像I和目标图像I‘的像素级语义特征，并通过计算最高余弦相似度为初始图像中的每个像素在目标图像中找到对应像素，建立像素级对应关系F。</li>
<li><strong>点云注册</strong>：(1) <strong>深度对齐</strong>：目标深度图D’是估计的相对深度，需要与初始的真实度量深度图D对齐。使用最小二乘法，仅在背景像素上回归一个线性变换（尺度s1和偏置b），将预测深度缩放到度量尺度，得到D‘_rescaled。(2) <strong>变换求解</strong>：利用对象掩码M和M’，从对齐后的深度图中提取对象点云P和P’。利用之前建立的像素对应关系F，得到一组对应的3D点对。为了鲁棒地处理生成目标中的尺度差异，求解两个点云之间的相似变换：s2 * P‘ = R * P + t。最优的旋转R和平移t使用Kabsch算法计算得出。</li>
</ul>
<p><strong>3. 底层策略模块</strong><br>该模块将对象目标位姿转化为可执行的机器人动作，包含三个阶段：</p>
<ul>
<li><strong>接触模块</strong>：在对象点云表面，基于采样和多阶段过滤（有效方向检查、使用末端执行器几何形状进行碰撞检查）确定一个可行的初始接触位姿P_contact。</li>
<li><strong>目标位姿计算</strong>：假设夹爪与对象接触后，两者相对位姿保持不变。据此，将空间接地模块计算出的对象变换(R, t)应用于夹爪的初始接触位姿P_contact，计算出夹爪的目标位姿P_goal。</li>
<li><strong>运动规划模块</strong>：为机械臂从当前配置到目标位姿P_goal生成一条无碰撞轨迹。在仿真中使用内置的基于采样的规划器，在真实世界实验中也使用基于采样的运动规划器。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，Goal-VLA的核心创新在于：1) 提出了<strong>以对象状态作为“黄金接口”</strong>，将高层语义推理与底层动作控制解耦，该表示既抽象了显式动作标注，又提供了丰富的空间线索；2) 引入了<strong>反射-合成迭代优化过程</strong>，通过合成虚拟叠加图像并进行视觉检查，显著提升了生成目标图像的合理性和任务可行性；3) 整个流程实现了真正的<strong>零样本</strong>操作，无需任何任务特定的微调。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真实验</strong>：在RLBench环境中进行，使用Franka Emika Panda 7自由度机械臂。选取了8个代表性任务进行评估，每个任务在10个随机种子下各运行10次，共100次试验计算成功率。</li>
<li><strong>真实世界实验</strong>：使用UFACTORY X-ARM 7机械臂和Orbbec Femto Bolt RGB-D相机。设计了4个不同的任务进行评估，每个任务进行10次试验。</li>
</ul>
<p><strong>对比基线</strong>：涵盖了端到端和分层两类范式。</p>
<ul>
<li>端到端VLA：OpenVLA, Pi0, MolmoAct。</li>
<li>分层VLA：VoxPoser（价值图），SUSIE（子目标图像），MOKA（关键点）。所有比较均在零样本设置下进行。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>仿真实验结果（表II）显示，Goal-VLA在8个任务上的平均成功率达到**59.9%**，显著优于所有基线。端到端方法（OpenVLA 0.2%, Pi0 0%）几乎完全失败。分层方法中，MOKA（关键点）为26.0%，MolmoAct（轨迹）为11.3%，VoxPoser（价值图）为5.8%，SUSIE（子目标图像）为0%。这验证了Goal-VLA在零样本设置下的优越性能。</p>
<p><img src="https://arxiv.org/html/2506.23919v2/x3.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>表II</strong>：仿真实验结果对比。Goal-VLA以59.9%的平均成功率大幅领先所有基线方法，尤其在复杂任务上优势明显。</p>
</blockquote>
<p>真实世界实验结果（表III）显示，Goal-VLA在四个任务上取得了<strong>90%</strong> 的平均成功率，进一步证明了其在实际物理系统中的有效性和泛化能力。</p>
<p><img src="https://arxiv.org/html/2506.23919v2/x4.png" alt="真实世界结果表"></p>
<blockquote>
<p><strong>表III</strong>：真实世界实验结果。Goal-VLA在四个不同任务上均取得高成功率，平均达到90%。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br><img src="https://arxiv.org/html/2506.23919v2/x1.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：反射-合成循环的消融研究。左图显示，随着迭代次数增加，生成目标图像的质量（通过CLIP与文本指令的相似度衡量）和任务成功率均显著提升。右图展示了在“将杯子放在柜子上”任务中，经过多次迭代后，生成的目标图像从语义错误（杯子悬浮）逐渐修正为合理状态（杯子立在柜面）。</p>
</blockquote>
<p>消融实验（图4）重点分析了<strong>反射-合成循环</strong>的作用。结果表明：1) 随着迭代次数增加，生成图像与文本指令的语义对齐度（CLIP相似度）和任务成功率均持续提升；2) 该循环能有效纠正初始生成中的语义和物理不合理性（如物体悬浮）。此外，论文还指出，<strong>深度对齐</strong>和<strong>相似变换求解</strong>对于处理生成深度图的尺度不确定性至关重要，移除此步骤会导致性能大幅下降。<strong>空间接地模块</strong>（提供精确3D变换）是整个框架成功的关键，若替换为简单的2D关键点，性能会严重恶化。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了Goal-VLA，一个新颖的、解耦的分层框架，它利用图像生成VLM作为以对象为中心的世界模型，生成目标对象状态，作为连接高层语义推理和底层动作控制的桥梁。</li>
<li>设计了反射-合成迭代优化过程，通过合成虚拟叠加图像并进行视觉检查与反馈，显著提升了生成目标图像的合理性和任务可行性。</li>
<li>在仿真和真实世界中广泛验证了该框架在多样化操作任务、环境、对象类别和机器人形态下的强大零样本泛化能力，且无需任何任务特定微调。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>接触假设</strong>：底层策略基于“夹爪与对象接触后相对位姿不变”的假设，这适用于大多数抓取动作和部分非抓取任务，但对于需要持续滑动的复杂非抓取操作可能不成立。</li>
<li><strong>计算开销</strong>：迭代反射-合成过程涉及多次调用大型VLM，可能导致较高的计算成本和延迟，不适合对实时性要求极高的场景。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>对象中心表示的潜力</strong>：本文验证了以对象状态作为中间表示的优越性，这为构建更泛化的机器人系统提供了一个有前景的方向。未来可探索更复杂的对象状态表示（如形变、关节状态）。</li>
<li><strong>生成模型作为世界模型</strong>：利用生成式VLM预测未来状态（世界模型）来指导规划，是一条值得深入探索的路径，可以缓解对真实机器人数据收集的依赖。</li>
<li><strong>反射与闭环修正</strong>：将反射机制集成到机器人操作闭环中，允许系统在执行过程中基于观测进行实时评估和修正，是提高系统在不确定环境中鲁棒性的关键。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中泛化能力不足的核心挑战，提出**Goal-VLA**零样本框架。其关键技术是：1) 将**图像生成式视觉语言模型(VLM)** 用作以物体为中心的世界模型，通过生成目标状态图像来推导物体位姿，从而桥接高层规划与无需训练的低层控制；2) 引入**反射合成**过程，迭代优化生成的目标图像以提升鲁棒性。模拟与真实实验表明，该框架在多种操作任务中实现了强劲的零样本性能和优异的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.23919" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>