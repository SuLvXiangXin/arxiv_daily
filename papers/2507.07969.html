<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Reinforcement Learning with Action Chunking - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Reinforcement Learning with Action Chunking</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.07969" target="_blank" rel="noreferrer">2507.07969</a></span>
        <span>作者: Sergey Levine Team</span>
        <span>日期: 2025-07-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用离线先验数据来加速在线强化学习（RL）的离线到在线RL方法，是解决复杂环境中样本效率低下问题的有前景的方向。然而，该领域仍面临核心挑战：离线数据的分布可能与在线探索所需策略不匹配，导致分布偏移；同时，如何有效利用离线数据来获取一个良好的在线探索策略并不明确。主流的简单方法（如使用离线RL目标直接在线微调）往往因过于悲观而阻碍探索。现有改进方法通过调整悲观程度来应对，但调参困难且样本效率有时仍不及简单正则化的在线算法。</p>
<p>本文针对“如何利用离线数据实现更有效的在线探索”这一具体痛点，提出了一个新视角：借鉴模仿学习（IL）中流行的动作分块（action chunking）技术。在IL中，动作分块通过预测未来动作序列（而非单步动作）来更好地处理离线数据中的非马尔可夫行为。尽管完全可观测MDP中的最优策略是马尔可夫的，但本文指出，探索过程本身可以受益于非马尔可夫、时间扩展的技能，而动作分块为此提供了一种简单直接的实现方式。</p>
<p>本文核心思路可概括为：将基于时序差分（TD）的RL算法运行在“分块”的动作空间上，使智能体能够（1）利用离线数据中时间一致的行为进行更有效的在线探索，（2）使用无偏的n步备份进行更稳定高效的TD学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>Q-chunking方法的整体框架是在一个时序扩展的动作空间（即动作序列空间）上运行标准的演员-评论家RL。其流程如下：在时间步t，策略π接收当前状态s_t，输出一个长度为h的连续动作序列（即一个“动作块”）a_{t:t+h}。该动作块被开环执行（依次执行而不重新观察状态）。评论家Q函数则评估给定状态s_t和整个动作块a_{t:t+h}的预期累积回报。经过h步后，收集到新的状态s_{t+h}和奖励，用于计算TD误差并更新评论家，进而更新策略。</p>
<p><img src="https://silicon-flow.com/placeholder-image.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Q-chunking方法概述。左侧：方法原理图，通过在时序扩展的动作空间上操作，实现（1）高效的价值备份和（2）通过时间一致动作进行有效探索；右侧：在OGBench五个挑战性长视野稀疏奖励领域上的聚合性能曲线，我们的方法（QC）在离线预训练（灰色区域）和在线更新（白色区域）后均表现出色。</p>
</blockquote>
<p>方法的核心模块基于两个设计原则：</p>
<ol>
<li><strong>在时序扩展动作空间上进行Q学习</strong>：策略π_ψ和评论家Q_θ均操作于长度为h的动作块。评论家使用以下TD损失进行训练：<br>L(θ) = E[ (Q_θ(s_t, a_{t:t+h}) - (Σ_{t&#39;=1}^{h} γ^{t&#39;} r_{t+t&#39;} + γ^h Q_θ̄(s_{t+h}, a_{t+h:t+2h})) )^2 ]<br>其中a_{t+h:t+2h} ∼ π_ψ(·|s_{t+h})。这与标准的n步回报（n=h）形式相似，但关键区别在于，Q-chunking的Q函数接收的是产生这n步奖励的<strong>整个动作序列</strong>。这使得其价值备份在享受h倍加速的同时，避免了传统n步方法在离策略数据下存在的估计偏差，实现了<strong>无偏的n步备份</strong>（公式7）。</li>
<li><strong>施加行为约束以促进时序一致的探索</strong>：策略优化目标在最大化Q值的同时，约束其与离线数据中行为分布π_β的距离：max_π E[Q_θ(s_t, a_{t:t+h})], s.t. D(π(a_{t:t+h}|s_t), π_β(a_{t:t+h}|s_t)) ≤ ε。在动作序列空间上施加此约束，能够直接利用离线数据中存在的时序一致行为（可视为简单技能），从而引导出更有结构的探索，而非杂乱无章的随机动作。</li>
</ol>
<p>本文提出了Q-chunking框架的两种具体实现：</p>
<ul>
<li><strong>QC（隐式KL约束）</strong>：使用流匹配（flow-matching）训练一个行为克隆流策略f_ξ来近似π_β。然后通过<strong>最佳N采样（best-of-N sampling）</strong> 来隐式满足KL约束：从f_ξ中采样N个动作块，选择其中Q值最高的执行。该方法无需单独的参数化策略网络，但增加了采样计算开销。</li>
<li><strong>QC-FQL（显式Wasserstein距离约束）</strong>：策略μ_ψ是一个噪声条件化的网络。其损失函数包含最大化Q值以及一个与行为流策略f_ξ的蒸馏损失，该损失是平方2-Wasserstein距离的上界。这提供了对约束强度更直接的控制。</li>
</ul>
<p>与现有方法相比，创新点体现在：1) <strong>将分块动作空间与基于价值函数的RL相结合</strong>，同时解决了探索和稳定学习两个问题；2) <strong>提出了适用于该框架的行为约束具体实现方案</strong>（QC和QC-FQL），特别是利用最佳N采样实现隐式约束；3) 指出并验证了<strong>高斯策略不足以建模复杂的行为序列分布</strong>，需要使用流匹配等表达力更强的模型。</p>
<p><img src="https://silicon-flow.com/placeholder-image.jpg" alt="高斯策略分块失败"></p>
<blockquote>
<p><strong>图2</strong>：天真地将动作分块与高斯策略结合用于在线RL（RLPD-AC）会导致性能下降。而结合了行为克隆损失的QC-RLPD性能有所恢复，但仍不理想，说明了表达力强的行为模型的重要性。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在六个长视野、稀疏奖励的机器人操作领域进行评估，包括OGBench中的5个领域（各含5个任务，共25任务）和Robomimic中的3个任务。使用默认的play-style或多人类数据集。实验遵循标准的离线到在线流程：先进行100万步的离线预训练，再进行100万步的在线微调。</p>
<p><strong>对比方法</strong>：与多种基线对比，包括：从零开始的在线算法（RLPD）；标准离线RL算法（IQL, ReBRAC）；先进的离线到在线RL方法（IFQL, FQL）；以及作者构建的用于对比的基线，如使用n步TD的变体（IFQL-n, FQL-n, BFN-n），和仅在原始动作空间使用最佳N采样的BFN。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>总体性能</strong>：如表1所示，在OGBench 25个任务聚合结果中，在线阶段结束后，Q-chunking方法（QC和QC-FQL）取得了最高成功率（86%和86%），显著优于先前最佳方法（BFN为63%，FQL为58%）。QC在离线预训练后也表现出强大的性能（52%）。</li>
</ol>
<p><img src="https://silicon-flow.com/placeholder-image.jpg" alt="结果汇总表"></p>
<blockquote>
<p><strong>表1</strong>：OGBench离线到在线RL结果汇总。每个单元格报告了离线训练1M步后的性能（左）和额外在线训练1M步后的性能（右）。Q-chunking方法（QC, QC-FQL）在在线训练结束时在所有方法中表现最佳。</p>
</blockquote>
<ol start="2">
<li><strong>Robomimic结果</strong>：如图3所示，QC在三个任务上均取得了强劲且稳定的性能，成功率达到或接近100%，优于其他对比方法。</li>
</ol>
<p><img src="https://silicon-flow.com/placeholder-image.jpg" alt="Robomimic结果曲线"></p>
<blockquote>
<p><strong>图3</strong>：Robomimic任务结果。QC在所有三个任务上都实现了强大的性能。前1M步为离线训练，后1M步为在线训练。</p>
</blockquote>
<ol start="3">
<li><strong>消融与分析</strong>：<ul>
<li><strong>分块长度（h）的影响</strong>：如图4左所示，性能并非随h单调增加。中等长度（h=5）通常效果最好，过短则加速效果不明显，过长可能因开环执行误差累积和优化难度增加而性能下降。</li>
<li><strong>最佳N采样的N值影响</strong>：如图4中所示，N值控制着隐式KL约束的强度。N=1相当于纯行为克隆，探索不足；N过大则约束过弱，可能偏离有用的先验。N=5是一个较好的平衡点。</li>
<li><strong>更新数据比（UTD）的影响</strong>：如图4右所示，较高的UTD（更多次梯度更新）通常能带来更好的样本效率，但收益会饱和。</li>
</ul>
</li>
</ol>
<p><img src="https://silicon-flow.com/placeholder-image.jpg" alt="超参数分析"></p>
<blockquote>
<p><strong>图4</strong>：消融实验。(左) 分块长度h对性能的影响，h=5通常最佳。(中) 最佳N采样的N值对性能的影响，N=5是一个稳健的选择。(右) 更新数据比对在线样本效率的影响。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>Q-chunking</strong>，一个将动作分块技术集成到基于TD的RL中的通用框架，通过在时序扩展动作空间中进行Q学习和行为正则化，同时提升了价值备份效率和探索能力。</li>
<li>基于该框架，实例化了两种实用的离线到在线RL算法——<strong>QC</strong>（使用最佳N采样的隐式约束）和<strong>QC-FQL</strong>（使用Wasserstein距离的显式约束），并在多个具有挑战性的长视野稀疏奖励任务上实现了最先进的性能。</li>
<li>通过理论分析和实验验证，阐明了Q-chunking实现<strong>无偏n步备份</strong>的原理，并实证了使用<strong>表达力强的行为模型</strong>（如流匹配）对于捕获复杂动作序列分布的必要性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，QC方法因依赖最佳N采样而带来额外的计算开销。同时，分块长度h、约束强度（N或α）等超参数需要根据任务进行调整。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>动作分块作为探索工具</strong>：这项工作表明，即使最终目标是马尔可夫策略，在训练过程中采用非马尔可夫、时间扩展的动作表示也是一种有效的探索策略。这为改进RL探索提供了新思路。</li>
<li><strong>与模型预测控制的联系</strong>：Q-chunking与基于模型的轨迹优化方法有相似之处，但前者学习的是一个显式的Q函数而非动力学模型。未来可以探索二者更深入的结合。</li>
<li><strong>扩展到更复杂的行为</strong>：当前方法处理的是固定长度的开环动作序列。未来可以探索与选项框架或具有终止条件的技能学习相结合，以获取更灵活、更闭环的时间抽象能力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对离线到在线强化学习中长周期稀疏奖励任务的探索与样本效率问题，提出Q-chunking方法。其核心是将模仿学习中的动作分块技术引入TD-based RL，通过在“分块”动作空间中直接运行RL，使智能体能：（1）利用离线数据中的时间一致行为进行更有效的在线探索；（2）使用无偏n步备份实现更稳定高效的TD学习。实验表明，该方法在离线性能和在线样本效率上均表现优异，在OGBench的五个长周期稀疏奖励操作任务中超越了现有最佳离线到在线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.07969" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>