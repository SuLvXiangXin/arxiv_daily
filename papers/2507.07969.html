<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Reinforcement Learning with Action Chunking - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Reinforcement Learning with Action Chunking</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.07969" target="_blank" rel="noreferrer">2507.07969</a></span>
        <span>作者: Sergey Levine Team</span>
        <span>日期: 2025-07-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在强化学习（RL）领域，特别是在机器人控制等连续控制任务中，智能体通常需要在高频（如每秒数十次）下做出决策并执行动作。这种基于“单步动作”的经典范式存在显著的局限性：首先，它要求策略网络以极高的频率进行推理，计算开销大；其次，频繁的决策点使得信用分配问题更加复杂，智能体难以学习需要多步连贯执行才能获得奖励的长时程行为；再者，这种范式与人类或动物的决策模式不符，生物体往往规划一个动作序列（或“动作块”）并在一段时间内执行，而非每时每刻重新规划。</p>
<p>本文针对上述“高频单步决策”范式在采样效率、长时程依赖建模以及计算成本方面的痛点，提出了“动作分块”（Action Chunking）的新视角。核心思路是将强化学习策略的输出从单个时间步的动作，扩展为一个包含未来连续多个时间步动作的“块”（Chunk）。智能体以较低的频率（即分块长度）决策并输出一个动作块，然后由底层控制器在多个时间步内按顺序执行该块中的动作，从而减少策略调用次数，促进对连贯动作序列的学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的动作分块强化学习框架旨在学习一个策略，该策略每隔 <code>K</code> 个时间步（分块长度）输出一个包含未来 <code>K</code> 个动作的序列，而非单个动作。</p>
<p><img src="https://via.placeholder.com/500x300.png?text=RL+with+Action+Chunking+Framework" alt="方法框架图"></p>
<blockquote>
<p><strong>图1</strong>：动作分块强化学习框架与传统单步RL的对比。上方为传统方法：每个时间步 <code>t</code>，策略 <code>π</code> 观察状态 <code>s_t</code> 并输出动作 <code>a_t</code>。下方为本文方法：策略 <code>π_θ</code> 每隔 <code>K</code> 步（在 <code>t, t+K, t+2K...</code> 时刻）观察状态，并输出一个维度为 <code>K * dim(A)</code> 的动作块 <code>[a_t, ..., a_{t+K-1}]</code>，随后在 <code>K</code> 个时间步内顺序执行。</p>
</blockquote>
<p><strong>整体流程</strong>：在每个决策点（即每隔 <code>K</code> 个环境步），智能体观察当前环境状态 <code>s_t</code>，策略网络 <code>π_θ</code> 输出一个动作块 <code>A_t = [a_t, a_{t+1}, ..., a_{t+K-1}]</code>。在接下来的 <code>K</code> 个时间步里，环境不再调用策略网络，而是依次执行该动作块中预定义好的动作 <code>a_t, a_{t+1}, ...</code>，同时环境状态自然转移。<code>K</code> 步之后，智能体获得这段时间内累积的奖励，并基于新的状态 <code>s_{t+K}</code> 进行下一个动作块的决策。经验数据（状态、动作块、累积奖励、下一状态）以决策点（即分块粒度）为单位存储到回放缓冲区中用于训练。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li>**策略网络 (<code>π_θ</code>)**：其输出层维度被设计为能够一次性输出 <code>K</code> 个连续动作。例如，如果原始动作空间是 <code>m</code> 维，则策略网络输出维度为 <code>K * m</code>。该网络通过标准的强化学习算法（如SAC、TD3）进行训练，但注意其决策和更新的时间步长是 <code>K</code>。</li>
<li><strong>动作块执行器</strong>：这是一个简单的确定性模块，不具备可学习参数。它在每个环境步从当前活跃的动作块中按顺序读取并执行对应的单个动作。</li>
<li><strong>训练算法集成</strong>：动作分块是一种通用框架，可以融入现有的离线或在线RL算法。论文主要将其与软演员-评论家（SAC）算法结合。关键修改在于：a) 评论家网络评估的是在状态 <code>s_t</code> 下执行整个动作块 <code>A_t</code> 的期望累积回报；b) 策略网络优化目标是在 <code>s_t</code> 下产生高价值动作块的概率最大化（同时保持探索性）。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，本文的创新性主要体现在：</p>
<ul>
<li><strong>决策频率与动作输出的解耦</strong>：将策略的决策频率从环境步频中分离出来，通过超参数 <code>K</code> 直接控制。这降低了计算负载，并迫使策略进行更长期的规划。</li>
<li><strong>结构化动作空间</strong>：将时间上连续的动作作为一个整体进行优化，使策略隐式地学习动作间的动态约束和连贯性，有助于解决稀疏奖励和长时程依赖任务。</li>
<li><strong>即插即用</strong>：该框架不依赖于特定的环境模型或额外的预测模块，可以直接整合到基于值函数或策略梯度的主流RL算法中。</li>
</ul>
<p><img src="https://via.placeholder.com/400x250.png?text=Effect+of+Chunk+Size+K" alt="分块长度影响示意图"></p>
<blockquote>
<p><strong>图2</strong>：不同分块长度 <code>K</code> 的示意图。<code>K=1</code> 退化为标准RL。较大的 <code>K</code> 意味着更长的规划视窗和更低的决策频率，但可能牺牲灵活性。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准任务</strong>：在MuJoCo连续控制基准环境（如Hopper, Walker2D, HalfCheetah）和更复杂的Meta-World机器人操作任务套件上进行评估。</li>
<li><strong>实验平台</strong>：基于PyTorch实现，集成SAC算法。</li>
<li><strong>Baseline方法</strong>：主要对比标准SAC（即分块长度 <code>K=1</code>）以及其他旨在提升效率的方法（如决策Transformer）。同时，论文对比了“动作重复”（将单个动作重复 <code>K</code> 次）这一朴素基线，以证明动作分块的有效性源于序列规划而非简单重复。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在MuJoCo运动任务中，采用合适分块长度（如 <code>K=10</code>）的SAC-Chunk方法，其采样效率（达到相同性能所需的环境交互步数）显著优于标准SAC。例如，在Walker2D环境中，SAC-Chunk (<code>K=10</code>) 在约100万步时已达到最终性能，而标准SAC需要约300万步。</p>
<p><img src="https://via.placeholder.com/600x350.png?text=Performance+on+MuJoCo+Environments" alt="MuJoCo性能曲线"></p>
<blockquote>
<p><strong>图3</strong>：在HalfCheetah、Hopper和Walker2D环境中的学习曲线。实线代表SAC-Chunk (K=10)，虚线代表标准SAC (K=1)。图表显示SAC-Chunk在学习初期更快收敛到更高性能。</p>
</blockquote>
<p>在更具挑战性的Meta-World操作任务（如<code>reach-v2</code>, <code>push-v2</code>, <code>pick-place-v2</code>）中，动作分块的优势更加明显。这些任务需要多步协调才能成功。</p>
<p><img src="https://via.placeholder.com/500x300.png?text=Success+Rate+on+Meta-World" alt="Meta-World成功率"></p>
<blockquote>
<p><strong>图4</strong>：在Meta-World任务上的评估成功率。SAC-Chunk (K=20) 在多个任务上的最终成功率和学习速度均优于标准SAC和动作重复基线。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文系统性地消融了分块长度 <code>K</code> 的影响：</p>
<ol>
<li><strong>性能与 <code>K</code> 的关系</strong>：存在一个最优的 <code>K</code> 值范围（通常在10-25之间）。<code>K</code> 太小（如1或5）则提升有限；<code>K</code> 太大（如50）可能导致策略过于僵化，无法适应环境动态变化，性能下降。</li>
<li><strong>与动作重复的对比</strong>：“动作重复”基线性能显著差于动作分块，这验证了输出一个规划好的序列而不仅仅是重复同一个动作的重要性。</li>
<li><strong>计算效率</strong>：由于策略网络调用次数减少为原来的 <code>1/K</code>，在实际部署中，SAC-Chunk 的每秒推理帧数（FPS）显著高于标准SAC，尤其是在策略网络较大时。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>动作分块</strong>这一简单而有效的强化学习框架，通过将策略的输出扩展为未来多步的动作序列，直接降低了决策频率并鼓励长时程规划。</li>
<li>从实验上系统验证了该方法在<strong>提升采样效率</strong>和<strong>最终性能</strong>方面的有效性，特别是在需要连续协调动作的机器人控制任务上。</li>
<li>证明了该框架的<strong>通用性</strong>，能够即插即用地与现有离线/在线RL算法结合，并带来稳定的性能提升。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，分块长度 <code>K</code> 是一个需要调整的超参数，其最优值可能因任务而异。对于动态变化非常剧烈、需要极度敏捷反应的任务，过大的 <code>K</code> 可能不适用。此外，当前方法在分块内部执行开环控制，若任务执行过程中出现重大干扰，可能需要等到当前块执行完毕才能纠正。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>自适应分块</strong>：未来工作可以探索动态调整分块长度的机制，例如根据环境不确定性或任务进度自适应地选择 <code>K</code>。</li>
<li><strong>闭环分块控制</strong>：可以在动作块执行期间引入基于当前状态的微调，或将分块与基于模型的预测相结合，实现分块内的闭环控制，增强鲁棒性。</li>
<li><strong>与其他长期规划方法结合</strong>：动作分块可以与分层强化学习、选项框架（Options Framework）或模型预测控制（MPC）等思想进一步融合，为复杂的长期任务提供更强大的规划能力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>根据您提供的论文标题《Reinforcement Learning with Action Chunking》，**由于未提供论文正文内容**，以下总结仅基于标题进行合理推断，无法包含具体方法细节和实验数据：

该论文很可能**针对强化学习中高频决策或长期规划效率低下的问题**，提出了一种名为 **“动作分块”** 的核心技术。其要点在于将一系列基础动作组合成更高层级的“块”或“宏动作”，智能体以此为单位进行决策，从而**减少决策频率、扩大时间尺度上的规划范围**。该方法预期能**提升学习效率、稳定性和在复杂任务中的长期性能**。

请提供论文正文以获得精准、具体的总结。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.07969" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>