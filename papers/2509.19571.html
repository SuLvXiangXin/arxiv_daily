<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19571" target="_blank" rel="noreferrer">2509.19571</a></span>
        <span>作者: Liam Paull Team</span>
        <span>日期: 2025-09-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，实现开放词汇的自然语言指令是机器人学的核心挑战。近期进展主要遵循两种范式：一是利用模仿学习和视觉-语言-动作模型（VLA）训练端到端策略；二是构建显式的、可查询的结构化场景表示，并用查询结果指导下游运动规划。VLA模型（如π0-FAST、π0.5）虽然展示了零样本潜力，但实际中通常需要针对特定任务进行微调，这带来了数据收集和基础设施方面的挑战。而场景表示方法（如开放词汇地图）虽然能保留视觉语言模型（VLM）的通用性，但通常局限于导航和抓放任务，没有直接解决复杂的交互问题。</p>
<p>本文观察到，大量语言指令可以通过一个（可能重复的）三步流程解决：1) 对象定位，2) 空间推理，3) 部件级交互。本文的核心理念是：通过将这三个步骤全部实现为对现代场景表示的查询，并利用大型语言模型（LLM）作为代理来调用这些查询工具，可以构建一个通用的、语言条件化的机器人策略。本文提出了Agentic Scene Policies (ASP)，一个将场景表示在语义、空间和功能（可供性）方面的强大查询能力与LLM的规划能力相结合的智能体框架。</p>
<h2 id="方法详解">方法详解</h2>
<p>ASP是一个用于语言条件化桌面及移动操作的智能体框架。其核心思想是让一个LLM代理将用户查询分解为一系列工具调用，这些工具基于一个统一的场景表示（ObjectMap）实现功能。</p>
<p><img src="https://arxiv.org/html/2509.19571v1/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：ASP框架概览。<strong>左侧</strong>：构建一个开放词汇的对象地图（ObjectMap），其中每个对象包含语义（CLIP特征）、几何（点云）和可供性（部件点云、描述、对应技能）信息。<strong>右侧</strong>：LLM代理将用户查询（如“Ring the desk bell”）分解为一系列工具调用（如检索对象、空间推理、交互）。工具输出（如状态、浮点数）反馈给代理以指导下一步行动。</p>
</blockquote>
<p><strong>整体框架与核心模块</strong>：</p>
<ol>
<li><strong>场景表示（ObjectMap）</strong>：这是系统的感知核心。它是一个对象列表，每个<code>Object</code>结构体包含：对象的3D点云（<code>point_cloud</code>）、从多个视角裁剪的RGB和深度图像（<code>rgb_crops</code>, <code>depth_crops</code>）、用于语义检索的聚合CLIP特征（<code>features</code>），以及一个<code>Affordance</code>列表。每个<code>Affordance</code>包含部件点云、自然语言部件描述（<code>part</code>）和对应的机器人技能（<code>skill</code>）。</li>
<li><strong>LLM代理</strong>：作为决策核心，它不直接接触传感器数据，而是通过一个符号状态（<code>State</code>）来感知环境。<code>State</code>包含当前抓握的对象（<code>held_object</code>）和一个“库存”（<code>inventory</code>），库存中存放了代理已通过工具定位到的、可供推理或操作的对象标识符（<code>ObjectKey</code>）。</li>
<li><strong>工具集</strong>：所有工具都能访问<code>ObjectMap</code>和当前<code>State</code>。<ul>
<li><strong>对象检索工具（<code>object_retrieval</code>）</strong>：输入开放词汇文本查询，通过CLIP特征相似度检索<code>ObjectMap</code>中的对象，并使用VLM（Gemini 2.5）对前k个候选进行确认，将确认的相关对象<code>ObjectKey</code>加入代理的<code>inventory</code>。</li>
<li><strong>空间理解工具（<code>spatial</code>）</strong>：输入对象列表，测量它们之间的距离、大小或验证空间谓词（如“在左侧”），基于对象点云及其质心实现。</li>
<li><strong>交互工具（<code>interact</code>）</strong>：这是执行复杂技能的关键。输入目标对象<code>ObjectKey</code>和动作描述（如“grab”），内部会触发一个<strong>可供性检测流程</strong>：首先使用VLM（Gemini 2.5）根据动作描述和对象视图预测可能的（<code>skill</code>, <code>part</code>）对；然后再次使用VLM和SAM 2.1分割出部件2D掩码，并利用深度信息将其提升为3D部件点云（即<code>Affordance.point_cloud</code>）；最后，调用与预测技能对应的<strong>技能工具</strong>。</li>
<li><strong>技能工具</strong>：包括通用对象技能（<code>grasp</code>, <code>place</code>, <code>drop</code>）和基于可供性的技能（<code>grasp_part</code>, <code>tip_push</code>, <code>pinch_pull</code>, <code>hook_pull</code>）。技能实现结合了运动规划（MoveIt 2）、点云几何操作（如计算法向量）和抓取检测器（AnyGrasp）。例如，<code>pinch_pull</code>技能会使用预设的末端执行器姿态抓取可供性点云的质心，并沿着点云法向量推断的水平方向拉动。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：<br>与现有方法相比，ASP的创新性体现在：1) <strong>统一查询</strong>：将语言指令的执行统一为对同一场景表示（ObjectMap）的序列化查询，该表示同时编码了语义、空间和可供性信息。2) <strong>可供性驱动的技能调度</strong>：通过VLM零样本检测可供性，将抽象的动作描述（如“Unplug”）动态映射到具体的几何交互技能（如<code>pinch_pull</code>），使策略能适应不同物体的不同部件几何形状。3) <strong>模块化与反馈</strong>：LLM代理与工具的解耦设计，以及工具输出的结构化反馈（成功/失败及原因），使系统具备通过重试应对部分失败的能力。</p>
<p><strong>扩展到移动操作</strong>：<br>移动版ASP增加了<code>go_to</code>导航工具。该工具也会对目标对象进行可供性检测，并利用部件点云的法向量推断一个适合交互的优选观测方位。导航目标点是在以物体为中心、半径为r的圆环上，综合避障代价和接近优选方位的惩罚项搜索得到。导航成功后，会在当前位置构建一个局部<code>ObjectMap</code>重新检测目标对象，以增强对全局地图定位和建图误差的鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准任务</strong>：设计了15个桌面操作查询和一系列房间级移动操作查询。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>VLA基线</strong>：<code>π0-FAST</code>（自回归VLA）和<code>π0.5</code>（流匹配VLA），均使用在DROID上微调的公开检查点。</li>
<li><strong>消融实验</strong>：<code>ASP (No Aff)</code>，将可供性点云替换为整个对象的点云，以评估细粒度可供性分割的重要性。</li>
</ul>
</li>
<li><strong>平台</strong>：使用UFACTORY XArm6机械臂（桌面）或搭载在Agilex移动底座上的同款机械臂（移动）。感知使用RealSense D435i相机，计算在搭载NVIDIA Titan RTX的工作站上进行。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2509.19571v1/figures/tabletop_results.png" alt="桌面操作成功率对比"></p>
<blockquote>
<p><strong>图3</strong>：桌面操作成功率对比。ASP在15个任务中的13个上优于VLA基线（<code>π0-FAST</code>和<code>π0.5</code>），平均成功率显著更高。阴影部分表示任务成功了但未使用把手或以非自然方式使用把手。消融实验<code>ASP (No Aff)</code>在涉及键盘、电源适配器、图钉、抽屉等需要部件级交互的任务上表现更差，证明了可供性检测的关键作用。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19571v1/figures/mobile_results.png" alt="移动操作结果"></p>
<blockquote>
<p><strong>图5</strong>：移动操作结果。移动版ASP能成功交织导航与操作解决房间级问题。在“双拾取”和“空间推理”任务中表现出良好的规划能力。在“可供性导航”任务中（如“拨打电话”），结合可供性信息规划导航终点对任务成功至关重要。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19571v1/x3.png" alt="失败分析"></p>
<blockquote>
<p><strong>图4</strong>：ASP失败模式分析。主要失败原因来自感知部分（31次），包括对象分割不足、可供性检测错误（如将倒置键盘的空格键位置预测错误）以及虽然检测到把手但抓取方式不自然。这指明了当前感知栈的改进方向。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19571v1/figures/tabletop_results_progress.png" alt="任务进展率"></p>
<blockquote>
<p><strong>图6</strong>：任务进展率。尽管图中聚焦于硬性成功率，但此图显示VLA基线（<code>π0-FAST</code> 67%， <code>π0.5</code> 75%）在多数任务中能取得一定进展，但未能完全成功。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19571v1/x4.png" alt="VLA失败分析"></p>
<blockquote>
<p><strong>图7</strong>：VLA基线失败分析。对<code>π0-FAST</code>和<code>π0.5</code>的失败案例进行分类讨论，提供了与ASP模块化失败分析相对应的视角。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br><code>ASP (No Aff)</code>消融实验表明，在超越简单抓放的任务中（如推动按键、拔出适配器、拉开抽屉），细粒度的可供性检测对成功至关重要。在移动设置中，导航时忽略可供性（<code>λ_aff=0</code>）会导致机器人到达不便于交互的位置，从而降低任务成功率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了**Agentic Scene Policies (ASP)**，一个将现代场景表示的语义、空间、可供性查询能力与LLM代理规划相结合的模块化策略框架，能够解决广泛的、需要复杂推理和交互的语言指令。</li>
<li>在15个桌面操作任务上与领先的VLA进行了<strong>广泛的实证比较</strong>，为模块化与端到端方法的辩论提供了有价值的数据点，展示了ASP在零样本设置下的优越性能。</li>
<li>将ASP扩展至<strong>移动操作</strong>，通过可供性引导的导航和扩展的场景表示，解决了房间级别的查询任务。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>ASP设计针对<strong>短期</strong>语言条件化操作任务，在应对长视野问题方面，其场景表示（缺乏动态/分层记忆、主动探索）和任务规划能力仍有待加强。</li>
<li>技能库目前主要覆盖抽屉（棱柱关节）等，可扩展性（如旋转关节）和复杂性有限，难以处理“清洁台面”、“叠衣服”等需要复杂连续动作的任务。</li>
<li>感知模块（如SAM分割、VLM可供性检测）的误差是当前主要失败来源。</li>
</ol>
<p><strong>对后续研究的启示</strong>：<br>本文揭示了在现有基础模型能力下，模块化方法在<strong>语义泛化</strong>和<strong>空间推理</strong>方面可能比端到端VLA更具优势。然而，VLA在学习<strong>复杂技能动作</strong>方面潜力更大。未来的一个关键方向是如何<strong>非平凡地整合</strong>两者：将ASP这类模块化系统强大的感知、规划和泛化能力，与从模仿学习中获得的高技巧、柔性技能相结合，以同时突破语义泛化和操作复杂性的天花板。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Agentic Scene Policies (ASP)框架，旨在解决机器人执行开放词汇自然语言指令时，端到端策略模型在复杂指令与新场景下表现不佳的问题。ASP的核心方法是利用现代场景表征的语义、空间和可供性查询能力，通过大型语言模型（LLM）代理调用查询工具，并结合可供性检测映射到具体技能（如tip_push、pinch_pull），实现模块化的语言条件策略。实验表明，ASP能以零样本方式处理桌面操作与房间级导航操作任务，展现了解决广泛查询的先进性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19571" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>