<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GWM: Towards Scalable Gaussian World Models for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.17600" target="_blank" rel="noreferrer">2508.17600</a></span>
        <span>作者: Siyuan Huang Team</span>
        <span>日期: 2025-08-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域，在学习的“世界模型”中训练策略成为一种趋势，以克服真实世界交互的低效性。主流方法是基于图像（视频）的世界模型和策略，它们虽然取得了成功，但缺乏稳健的几何信息，对光照、相机姿态等视觉变化敏感，因为它们缺乏对三维世界一致的空间和物理理解。另一方面，虽然NeRF和3D高斯泼溅（3D-GS）等三维重建方法能融合几何与视觉细节，但它们依赖离线的、按场景的优化，计算成本高昂，难以扩展到需要实时交互和大量数据学习的机器人操作任务中，特别是在基于模型的强化学习（MBRL）场景下。</p>
<p>本文针对机器人操作需要鲁棒且可扩展的三维世界模型这一具体痛点，提出了将3D高斯表示与高容量生成模型相结合的新视角。核心思路是构建一个名为“高斯世界模型（GWM）”的系统，它以前馈方式从图像重建3D高斯表示，并通过一个在潜在空间中运行的扩散变换器，学习以机器人动作为条件的未来状态动态，从而实现高效、细粒度的未来场景预测，并服务于模仿学习和基于模型的强化学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>GWM的整体目标是根据当前观测和机器人动作，推断出由3D高斯图元表示的未来场景重建。其流程主要包含两个核心阶段：世界状态编码和基于扩散的动力学建模。</p>
<p><img src="https://arxiv.org/html/2508.17600v2/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：GWM的整体流程。主要由一个3D变分编码器和一个潜在扩散变换器构成。3D变分编码器将基础重建模型估计的高斯泼溅嵌入到紧凑的潜在空间，扩散变换器则在潜在片段上进行操作，以机器人动作和去噪时间步为条件，交互式地想象未来的高斯泼溅。</p>
</blockquote>
<p><strong>1. 世界状态编码</strong><br>给定单目或双视图图像输入，首先需要将场景编码为3D高斯表示。GWM采用前馈式3D高斯泼溅方法，具体使用Splatt3R模型。该模型首先利用Mast3R从输入图像生成3D点图，然后根据这些点图预测每个3D高斯的参数（包括中心位置、不透明度、协方差矩阵和球谐系数）。由于不同场景重建出的高斯数量可变，为便于后续处理，GWM引入了一个<strong>3D高斯变分自编码器（VAE）</strong>。该VAE首先使用最远点采样将高斯采样到固定数量，然后通过一个基于交叉注意力的编码器，以采样高斯为查询，聚合所有高斯的信息，生成固定长度的潜在嵌入。解码器则通过自注意力机制从潜在嵌入重建高斯。VAE的训练损失结合了重建高斯中心点与原始高斯中心点之间的Chamfer距离，以及两者渲染图像之间的L1损失，以确保几何和视觉的保真度。</p>
<p><strong>2. 基于扩散的动力学建模</strong><br>在获得时间步t的世界状态潜在嵌入x_t后，目标是学习以历史状态和动作为条件的未来状态分布p(x_{t+1} | x_{≤t}, a_{≤t})。GWM将此转化为一个条件生成问题，使用扩散模型从噪声中生成未来状态x_{t+1}。具体采用EDM框架，学习一个网络F_θ。该网络通过预条件化技术，根据噪声水平自适应地混合信号和噪声作为训练目标，从而更稳定地学习去噪过程。在实现上，F_θ由一个<strong>扩散变换器（DiT）</strong> 实例化。噪声潜在嵌入与添加了旋转位置编码的历史状态、机器人动作以及噪声水平条件一同输入DiT。其中，机器人动作作为DiT中交叉注意力层的键和值，以实现条件生成。这种设计使得模型能够高效地在紧凑的潜在空间中学习复杂的场景动态。</p>
<p><strong>创新点</strong>：与现有方法相比，GWM的主要创新在于：1) 将显式、高效的3D高斯表示与强大的生成式扩散模型相结合，构建可扩展的3D世界模型；2) 通过3D高斯VAE将可变数量的高斯编码为固定维度的潜在表示，使扩散模型能够高效处理3D几何信息；3) 整个系统能够端到端训练，无需人工干预，并可直接集成到离线模仿学习和在线强化学习流程中。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个环境中进行：模拟环境Meta-World（RL基准）、模拟环境RoboCasa（大规模模仿学习基准）以及真实世界Franka-PnP（拾放任务套件，包含20种变体）。评估了四个关键任务：动作条件场景预测、GWM-based模仿学习、GWM-based强化学习以及真实世界任务部署。</p>
<p><strong>1. 动作条件场景预测</strong><br>对比基线为先进的图像世界模型iVideoGPT。评估指标包括FVD（时间一致性）、PSNR、SSIM和LPIPS（感知质量）。</p>
<p><img src="https://arxiv.org/html/2508.17600v2/x1.png" alt="定量结果表"></p>
<blockquote>
<p><strong>表1</strong>：在Meta-World和Franka PnP上的未来状态预测定量结果。GWM在几乎所有指标上均优于iVideoGPT。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.17600v2/x3.png" alt="Meta-World定性对比"></p>
<blockquote>
<p><strong>图3</strong>：Meta-World上的定性对比。GWM能更成功地预测机械臂夹爪运动的细节（蓝色高亮部分），而iVideoGPT则出现模糊或错误。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.17600v2/x4.png" alt="更多环境定性可视化"></p>
<blockquote>
<p><strong>图4</strong>：GWM在Franka-PnP和RoboCasa上的未来状态预测定性可视化。预测结果基于验证集中未见过的动作轨迹展开。</p>
</blockquote>
<p><strong>2. GWM-based 模仿学习</strong><br>在RoboCasa的20个任务上，将GWM作为视觉编码器为BC-Transformer策略提供特征，并与原始BC-Transformer对比。使用50条人类演示（H-50）或3000条生成演示（G-3000）进行训练。</p>
<p><img src="https://arxiv.org/html/2508.17600v2/x5.png" alt="模仿学习结果表"></p>
<blockquote>
<p><strong>表2</strong>：RoboCasa多任务模仿学习结果。GWM在大多数任务上显著提升了成功率，在使用人类演示时平均绝对提升16.25%。</p>
</blockquote>
<p><strong>3. GWM-based 强化学习</strong><br>在Meta-World的10个任务上，将GWM作为动力学模型集成到基于模型的RL（MBPO）中，对比基于图像的世界模型和传统MBPO。</p>
<p><img src="https://arxiv.org/html/2508.17600v2/x6.png" alt="强化学习结果"></p>
<blockquote>
<p><strong>图5</strong>：基于模型的强化学习成功率曲线。GWM作为动力学模型，其性能显著优于图像世界模型（iVideoGPT）和标准MBPO。</p>
</blockquote>
<p><strong>4. 真实世界任务部署</strong><br>在Franka-PnP真实机器人任务中，将GWM作为增强编码器与扩散策略结合，在20次试验中评估。</p>
<blockquote>
<p>结果表明，GWM将典型扩散策略的成功率提高了30%。</p>
</blockquote>
<p><strong>消融实验</strong>：论文验证了各组件贡献。移除3D高斯VAE会导致训练不稳定和性能下降；使用非扩散的简单动力学模型（如MLP）则严重损害预测质量和下游任务性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了GWM，一个新颖的3D高斯世界模型，它通过3D高斯VAE和潜在扩散变换器实现了高效、可扩展的动态场景建模与未来预测。2) 证明了GWM能无缝集成到模仿学习和基于模型的强化学习中，并在模拟和真实环境中显著超越基于图像的世界模型等先进基线，展示了其数据缩放潜力。3) 在涵盖31个任务的广泛实验中验证了其有效性，特别是在真实世界机器人操作中提升了策略性能。</p>
<p><strong>局限性</strong>：论文提到，GWM目前依赖于多视图输入和基础重建模型（如Splatt3R）来初始化3D高斯，这可能影响其在极端视角或纹理缺失情况下的泛化能力。</p>
<p><strong>启示</strong>：GWM为机器人操作领域指明了一个有前景的方向，即利用显式3D表示（如3D高斯）与生成式AI模型相结合，构建兼具几何理解、视觉细节和可扩展性的世界模型。这有助于克服纯图像模型的脆弱性，并为实现更通用、数据高效的机器人学习提供了基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中基于图像的世界模型缺乏稳健三维几何信息的问题，提出高斯世界模型（GWM）。其核心采用潜在扩散变换器与3D变分自编码器，通过高斯溅射实现细粒度的、以动作为条件的未来场景重建。实验表明，GWM能精准预测未来状态，并基于此训练的策略显著优于现有先进方法，展现了三维世界模型的数据扩展潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.17600" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>