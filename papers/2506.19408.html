<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Is an object-centric representation beneficial for robotic manipulation ? - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Is an object-centric representation beneficial for robotic manipulation ?</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.19408" target="_blank" rel="noreferrer">2506.19408</a></span>
        <span>作者: Liming Chen Team</span>
        <span>日期: 2025-06-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作任务通常涉及多物体环境和复杂的物体间交互。当前，基于视觉的策略学习广泛使用从计算机视觉领域借鉴的预训练模型作为视觉编码器，这些方法通常将整个场景编码为一个全局向量（即整体表征）。然而，这种表征方式缺乏内部结构，需要编码器从单一向量中解析出场景中的所有物体及其关系，这可能在复杂、多变的场景中成为瓶颈。</p>
<p>与此同时，计算机视觉领域兴起了物体中心表征学习，其目标是将图像或视频分解为代表不同实体的结构化表示（即“物体槽”）。尽管其潜力被认可，但现有工作大多仅评估其在场景分解和重建上的性能，缺乏在需要高级推理的下游任务（如机器人操作）上的系统性评估。一些初步研究将其应用于强化学习，但仅限于简单的2D或玩具环境。</p>
<p>本文旨在填补这一空白，系统性地评估物体中心表征在复杂、高度随机化的3D机器人操作任务中的有效性，并与当前先进的整体表征方法进行对比。本文的核心思路是：构建一个基于Transformer的策略学习框架，将物体中心视觉编码器（SAVi）作为冻结骨干，评估其在多物体操作任务中的性能及对未见场景的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的评估框架整体上分为视觉预训练和行为克隆学习两个阶段，整体流程如下图所示。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧为视觉预训练阶段：输入视频帧通过编码器（CNN）、Slot-Attention模块和预测器被分解为物体槽，并通过解码器重建图像，使用图像重建损失（MSE）进行训练。右侧为行为学习阶段：冻结的预训练编码器将观测历史编码为物体槽序列，与一个可学习的<code>[ACT]</code>令牌一同输入Transformer观测主干进行信息聚合，最终由动作头预测下一个动作，通过动作预测损失（NLL）进行训练。</p>
</blockquote>
<p><strong>整体流程</strong>：</p>
<ol>
<li><strong>输入</strong>：一段短时序的视觉观测 <code>(o_{t-H}, ..., o_t)</code>。</li>
<li><strong>视觉编码</strong>：使用预训练好的物体中心视频模型SAVi，将每一帧图像编码为一组物体槽 <code>S_t = {s_1, ..., s_K}</code>，其中K为槽的数量，D为槽的维度。</li>
<li><strong>策略推理</strong>：将最近H帧的槽序列 <code>(S_{t-H}, ..., S_t)</code> 与一个额外的可学习<code>[ACT]</code>令牌拼接，输入一个Transformer观测主干。该Transformer通过自注意力机制整合所有槽的历史信息到<code>[ACT]</code>令牌中。</li>
<li><strong>动作预测</strong>：提取更新后的<code>[ACT]</code>令牌，通过一个动作头预测下一个动作 <code>a_t</code>。动作是一个7维向量，包括末端执行器的6维相对位姿（x, y, z, rx, ry, rz）和夹爪状态（开/合）。</li>
</ol>
<p><strong>核心模块与技术细节</strong>：</p>
<ul>
<li><strong>编码器（SAVi）</strong>：采用SAVi作为物体中心编码器。其流程为：首先用一个5层CNN处理输入帧得到特征网格；然后通过Slot-Attention模块将特征迭代地聚类为K个物体槽；对于视频，第一帧的槽通过一个Transformer预测器后，作为下一帧Slot-Attention的初始化，以维持时序一致性。模型通过图像重建损失（MSE）在域内数据上进行预训练。</li>
<li><strong>观测主干</strong>：采用基于Transformer的架构，专门用于处理多向量的物体槽序列。其输入是槽序列和<code>[ACT]</code>令牌，通过自注意力机制，<code>[ACT]</code>令牌聚合了整个观测历史的上下文信息。</li>
<li><strong>动作头</strong>：采用高斯混合模型（GMM）头来预测动作分布，以处理任务可能存在的多模态性（例如，到达一个物体可能有多种路径）。训练时使用负对数似然损失（NLL）来匹配专家演示的动作。</li>
</ul>
<p><strong>创新点</strong>：<br>本文的主要创新并非提出一个全新的物体中心模型，而是构建了一个<strong>系统性的评估框架</strong>，将现有的物体中心表征（SAVi）与一个适配的Transformer策略学习架构相结合，并在精心设计的、具有挑战性的多物体机器人操作任务上，与主流的整体表征方法（如DINO、R3M）进行公平对比，从而实证性地回答了物体中心表征是否对机器人操作有益这一核心问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>平台与数据集</strong>：在Maniskill仿真环境中构建了名为<strong>RoboShape</strong>的新数据集，包含三个难度递增的桌面操作任务（见图2）：1) 将红色立方体推到目标点；2) 拾取红色立方体并放置到指定位置；3) 拾取红色立方体并放入绿色箱子中。每个任务收集了2000条专家轨迹。环境进行了高度随机化，包括目标与干扰物位置、干扰物形状与颜色、背景与桌面颜色、机器人初始位姿等。</li>
<li><strong>泛化场景</strong>：定义了三个层次的泛化测试：L1（未见过的干扰物颜色）、L2（未见过的背景颜色）、L3（未见过的干扰物尺寸）。具体配置见图3、图4、图5。</li>
<li><strong>Baseline方法</strong>：与两种先进的<strong>整体表征</strong>方法对比：1) 计算机视觉预训练模型 <strong>DINO</strong>；2) 机器人专用预训练模型 <strong>R3M</strong>。所有编码器在策略学习阶段均被冻结。</li>
<li><strong>评估指标</strong>：平均成功率，在100次 rollout 上计算均值与标准差，重复3次。</li>
</ul>
<p><img src="https://..." alt="任务示例"></p>
<blockquote>
<p><strong>图2</strong>：RoboShape数据集中的三个任务示例。从上到下分别为：推动立方体到目标点、拾取立方体、将立方体放入箱子中。</p>
</blockquote>
<p><img src="https://..." alt="L1泛化"></p>
<blockquote>
<p><strong>图3</strong>：L1泛化（未见干扰物颜色）示例。上排为训练中见过的颜色，下排为未见过的颜色。</p>
</blockquote>
<p><img src="https://..." alt="L2泛化"></p>
<blockquote>
<p><strong>图4</strong>：L2泛化（未见背景颜色）示例。上排为训练中见过的背景，下排为未见过的背景。</p>
</blockquote>
<p><img src="https://..." alt="L3泛化"></p>
<blockquote>
<p><strong>图5</strong>：L3泛化（未见干扰物尺寸）示例。干扰物尺寸随机，可能小于或大于训练中所见。</p>
</blockquote>
<p><strong>关键定量结果</strong>：</p>
<ul>
<li><strong>任务1：Push cube to target</strong>：R3M在标准场景（∅）下表现最佳（成功率88%），SAVi次之（69%），DINO完全失败（0%）。然而，在<strong>L1泛化</strong>（未见干扰物颜色）下，R3M性能骤降60个百分点至26%，而SAVi仅下降14个百分点至55%，显示出<strong>物体中心表征对无关特征变化的强大鲁棒性</strong>。</li>
<li><strong>任务2：Pick cube</strong>：这是一个需要更精确操作和推理的任务。<strong>SAVi成为唯一能够成功完成该任务的方法</strong>，标准场景下成功率为56%。DINO和R3M均告失败（0%）。SAVi在所有泛化场景（L1, L2, L3）下性能下降均约10%左右，再次证明了其稳定性。</li>
<li><strong>任务3：Place cube in bin</strong>：所有模型在该最复杂任务上的成功率均为0。分析表明，R3M无法将立方体准确放入箱内，而SAVi虽然能将末端执行器移动到箱子上方，却因单视角观测带来的深度感知不确定性而无法正确释放物体（见图6）。</li>
<li><strong>总体对比</strong>：图7总结了各模型在所有任务和泛化场景下的平均成功率。由于DINO和R3M无法完成后两个任务，SAVi在整体上显著优于整体表征方法，尤其是在面对分布外干扰时（L1场景）优势明显。</li>
</ul>
<p><img src="https://..." alt="失败案例"></p>
<blockquote>
<p><strong>图6</strong>：在“放入箱子”任务中的失败案例。左图：输入观测；右图：模型重建。模型可能因单视角观测误判立方体已在箱内，导致悬停而不释放。</p>
</blockquote>
<p><img src="https://..." alt="总体成功率"></p>
<blockquote>
<p><strong>图7</strong>：各模型在不同泛化等级下，跨三个任务的平均成功率。物体中心方法（SAVi）在面临分布外变化时表现最为稳定。</p>
</blockquote>
<p><strong>消融实验与分析</strong>：<br>论文虽未设置严格的模块消融实验，但通过结果对比和定性分析，明确了各部分的贡献：</p>
<ol>
<li><strong>物体中心表征 vs. 整体表征</strong>：实验的核心对比表明，<strong>物体中心的结构化表征</strong>在处理多物体场景和应对无关特征（如干扰物颜色）变化时，比整体表征更具优势。</li>
<li><strong>Transformer观测主干</strong>：该设计是为了有效整合多物体、多时序的槽信息。论文指出，传统的MLP瓶颈不适合处理物体中心表征，而Transformer能够更好地聚合信息到<code>[ACT]</code>令牌中。</li>
<li><strong>两阶段训练</strong>：首先通过无监督重建预训练获得物体分解能力，然后冻结编码器进行策略学习。这证明了<strong>无需大规模域外预训练</strong>，仅使用域内数据预训练的物体中心模型也能有效支持下游策略学习，且模型参数量远小于对比方法（约为1/25）。</li>
</ol>
<p><strong>定性结果</strong>：<br>图8可视化了SAVi在策略执行过程中的场景分解结果。可以看到，模型能够成功地将输入图像解耦为不同的组成部分：背景、机器人本体以及场景中的各个物体。这验证了其学习到的表征确实具有物体中心特性。</p>
<p><img src="https://..." alt="物体分解"></p>
<blockquote>
<p><strong>图8</strong>：在策略执行 rollout 中的物体分解可视化。从左至右：输入图像、完整重建图、各个物体槽的重建图。展示了模型对场景的解耦能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>系统性评估</strong>：首次在复杂、高度随机化的3D机器人操作任务上，对物体中心表征与主流整体表征进行了全面、公平的对比研究，为物体中心学习在机器人领域的应用提供了实证依据。</li>
<li><strong>框架与基准</strong>：提出了一个结合物体中心编码器与Transformer策略学习的简单可复现评估框架，并创建了包含多级泛化挑战的RoboShape仿真基准数据集。</li>
<li><strong>关键发现</strong>：证明了物体中心表征在数据规模和参数量远小于对比方法的情况下，能够完成更复杂的操作任务，并且<strong>对分布外干扰（尤其是未见过的干扰物属性）表现出显著的鲁棒性</strong>，这为解决机器人泛化难题提供了一个有希望的方向。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>研究仅限于仿真环境，任务相对简单。</li>
<li>使用的物体中心模型（SAVi）未经过大规模机器人数据预训练。</li>
<li>在最具挑战性的任务（放入箱子）上，所有方法均失败，揭示了单视角观测在需要精确空间理解任务中的局限性。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>探索更优的物体中心模型</strong>：将最新的、更强大的物体中心方法（如SAVi++、SlotFormer）应用于机器人领域，并探索其在更大规模机器人数据集上的预训练潜力。</li>
<li><strong>多模态与多视角融合</strong>：结合多视角观测或 proprioception 信息，以克服单视角带来的空间歧义，提升在精确操作任务上的性能。</li>
<li><strong>迈向世界模型</strong>：利用物体中心表征的结构化特性，可以更自然地构建用于预测和规划的场景动态模型（世界模型），实现“在想象中学习”。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文探讨物体中心表示（OCR）是否有利于机器人操作任务。针对现有方法在复杂多物体场景中泛化能力不足的问题，研究提出一个评估框架：先通过图像重建损失预训练OCR模型，再将其作为冻结骨干网络，结合Transformer编码多帧视觉输入，并利用模仿学习训练动作预测头。在高度随机化的多物体模拟环境中进行实验，结果表明，与先进的全景表示方法相比，物体中心表示能更有效地处理复杂场景结构，提升策略的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.19408" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>