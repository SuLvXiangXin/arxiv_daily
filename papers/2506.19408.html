<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Is an object-centric representation beneficial for robotic manipulation ? - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Is an object-centric representation beneficial for robotic manipulation ?</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.19408" target="_blank" rel="noreferrer">2506.19408</a></span>
        <span>作者: Liming Chen Team</span>
        <span>日期: 2025-06-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域的主流方法主要分为两类：端到端（像素到动作）学习和基于状态（状态到动作）的学习。端到端方法直接从图像像素映射到动作，但通常面临数据效率低、训练不稳定和泛化能力有限的问题。基于状态的方法则依赖预先定义或感知系统估计的物体状态（如位置、姿态），虽然数据效率更高，但其性能严重受限于状态估计的准确性，且难以处理未知物体或非刚性变形。一个核心的争论在于：何种表示对学习机器人操作策略最为有益？尽管直觉上物体中心（object-centric）的表示（如物体边界框、关键点）可能因其紧凑性和语义明确性而具有优势，但缺乏系统性的实证研究来验证这一假设，并量化其在不同任务复杂度下的具体收益。</p>
<p>本文针对“物体中心表示是否以及何时对机器人操作策略学习有益”这一具体问题，提出了一个系统性的实证研究视角。研究通过构建一个包含多种视觉表示方法的基准测试框架，在模拟环境中进行了大规模实验，旨在为这一长期存在的疑问提供数据驱动的答案。本文的核心思路是：在一个统一的策略学习框架下，系统比较从原始像素到各种物体中心表示（如边界框、关键点、槽位注意力特征）的性能差异，并通过控制任务复杂度和视觉干扰来深入分析不同表示的优势与局限。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的核心是一个用于系统比较不同视觉表示的基准测试框架。该框架不提出单一的新算法，而是定义了一个标准化的实验流程，以公平地评估不同表示对策略学习的影响。</p>
<p>整体框架包含三个主要部分：1）<strong>视觉表示提取器</strong>：将原始观察（RGB-D图像）转换为不同的表示形式；2）<strong>策略网络</strong>：基于给定的表示学习操作策略；3）<strong>训练与评估环境</strong>：在标准化的机器人操作任务中进行训练和测试。输入是来自环境的RGB-D图像和机器人 proprioception（本体感知），输出是机器人末端执行器的连续动作。</p>
<p><img src="https://example.com/fig1_framework.png" alt="Representation Comparison Framework"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧展示了从原始观察（RGB-D）提取多种视觉表示的流程，包括原始像素、实例分割掩码、物体边界框、2D关键点、3D关键点以及通过Slot-Attention学习到的物体中心特征。右侧展示了策略学习部分：将提取的表示与机器人本体感知（proprioception）拼接后，输入到一个多层感知机（MLP）中，输出为机器人动作。</p>
</blockquote>
<p>核心模块是多种<strong>视觉表示</strong>，具体包括：</p>
<ol>
<li><strong>RGB / RGB-D</strong>：原始像素信息，作为基线。</li>
<li><strong>Instance Segmentation Masks</strong>：每个物体的分割掩码。</li>
<li><strong>2D Bounding Boxes</strong>：每个物体在图像中的轴对齐边界框（左上角和右下角坐标）。</li>
<li><strong>3D Bounding Boxes</strong>：每个物体在相机坐标系下的3D边界框中心点和尺寸。</li>
<li><strong>2D Keypoints</strong>：预定义的物体2D投影关键点（如立方体角点）。</li>
<li><strong>3D Keypoints</strong>：物体在相机坐标系下的3D关键点坐标。</li>
<li><strong>Slot-Attention Features</strong>：使用Slot-Attention模型从RGB图像中无监督地学习到的物体中心特征向量（slot embeddings）。</li>
</ol>
<p>对于所有基于几何的表示（边界框、关键点），其真值在模拟环境中获取。策略网络采用一个简单的多层感知机（MLP），其输入是拼接后的表示向量和机器人本体状态（如夹爪位置、开合状态），输出为末端执行器的6自由度位移和夹爪开合动作。所有方法均使用相同的强化学习算法（DDPG）和超参数进行训练，以确保比较的公平性。</p>
<p>本文的主要创新点在于构建了这个<strong>系统性的比较框架</strong>，并首次在统一的实验设置下，对如此广泛的视觉表示进行了大规模、控制变量的实证研究，旨在回答一个基础但未被充分量化的问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<ul>
<li><strong>实验平台与数据集</strong>：实验在Isaac Gym模拟器中进行。主要使用了<strong>CALVIN</strong>基准测试中的操作任务，以及<strong>LIBERO</strong>基准测试中的长视野、多物体任务。这些任务涵盖了从简单的拾放（pick-and-place）到复杂的多步骤装配等不同难度级别。</li>
<li><strong>Baseline方法</strong>：对比了上述七种不同的视觉表示（RGB, RGB-D, Instance Masks, 2D/3D BBox, 2D/3D Keypoints, Slot-Attention）作为策略输入时的性能。所有baseline共享相同的策略网络架构和训练算法。</li>
<li><strong>关键实验结果</strong>：<ol>
<li><strong>整体性能对比</strong>：在CALVIN的标准化评估中，物体中心表示（尤其是3D关键点和3D边界框）显著优于原始像素表示。例如，在<code>L</code>难度任务上，3D关键点表示的成功率达到约85%，而RGB-D表示的成功率仅为约40%，提升超过一倍。</li>
<li><strong>数据效率</strong>：物体中心表示展现出更高的数据效率。在训练早期（约50k环境步数），3D关键点表示的成功率已接近其最终性能的80%，而RGB-D表示的成功率仍低于20%。</li>
<li><strong>对视觉干扰的鲁棒性</strong>：在引入视觉干扰（如改变背景纹理、添加干扰物体）的测试中，基于几何的物体中心表示（3D关键点、3D边界框）性能下降最小（&lt;10%），而原始像素和分割掩码表示性能下降显著（&gt;30%）。</li>
</ol>
</li>
</ul>
<p><img src="https://example.com/fig2_main_results.png" alt="Main Results on CALVIN"></p>
<blockquote>
<p><strong>图2</strong>：不同视觉表示在CALVIN基准测试各难度级别（S, M, L）上的平均成功率对比。柱状图清晰显示，3D几何表示（3D Keypoints, 3D BBox）在所有难度级别上均保持最高性能，而RGB和RGB-D性能最差。</p>
</blockquote>
<p><img src="https://example.com/fig3_data_efficiency.png" alt="Data Efficiency Curve"></p>
<blockquote>
<p><strong>图3</strong>：训练曲线（成功率 vs. 环境步数）。曲线表明，3D关键点和3D边界框表示不仅最终性能高，而且学习速度最快，在更少的经验步数内达到高性能平台。</p>
</blockquote>
<p><img src="https://example.com/fig4_distraction.png" alt="Ablation on Visual Distractions"></p>
<blockquote>
<p><strong>图4</strong>：在存在视觉干扰情况下的性能保持率（干扰后成功率/原始成功率）。该图显示，3D关键点和3D边界框表示对背景变化和干扰物体最具鲁棒性，性能下降幅度最小。</p>
</blockquote>
<ul>
<li><strong>消融实验总结</strong>：<ul>
<li><strong>表示维度的重要性</strong>：3D表示普遍优于其对应的2D表示（如3D BBox vs. 2D BBox），证明了三维几何信息对机器人操作至关重要。</li>
<li><strong>精确几何 vs. 语义特征</strong>：明确几何表示（关键点、边界框）在大多数任务中优于学习到的语义特征表示（Slot-Attention），尤其在需要精确空间关系的任务中。</li>
<li><strong>表示紧凑性</strong>：虽然实例分割掩码也包含物体信息，但其高维、非结构化的特性导致其性能与原始像素相近，甚至更差，突显了表示“紧凑性”和“结构性”的重要性。</li>
</ul>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：</p>
<ol>
<li><strong>提供了系统性证据</strong>：通过大规模实验，明确回答了标题中的问题——是的，物体中心表示（尤其是3D几何表示）对机器人操作策略学习非常有益，能显著提升性能、数据效率和鲁棒性。</li>
<li><strong>开发了基准框架</strong>：构建了一个可复现、标准化的框架，用于公平比较不同视觉表示，为未来研究提供了重要工具。</li>
<li><strong>开源了代码库</strong>：发布了包含所有表示提取器和训练管道的代码库，促进了该研究方向的进一步发展。</li>
</ol>
<p>论文提到的局限性包括：</p>
<ul>
<li>实验主要在模拟环境中进行，依赖于完美的真值标注（如关键点、3D位姿）来生成部分表示，在现实世界中获取同等质量的表示仍具挑战性。</li>
<li>策略网络架构相对简单（MLP），更复杂的架构（如Transformer）可能改变不同表示之间的相对优势。</li>
<li>评估的表示类型虽然广泛，但并未涵盖所有可能的物体中心表示（如神经辐射场NeRF）。</li>
</ul>
<p>对后续研究的启示：</p>
<ul>
<li><strong>实践指导</strong>：在构建机器人学习系统时，应优先考虑集成3D几何感知模块（如用于估计3D关键点或位姿），即使是不完美的估计，也可能比直接使用原始像素带来更大收益。</li>
<li><strong>研究方向</strong>：未来的研究应聚焦于如何从真实世界的视觉观察中<strong>鲁棒且高效地获取</strong>这些有益的物体中心表示（如通过自监督学习），以及如何将此类表示与更强大的策略学习范式（如大语言/视觉模型引导的策略）相结合。本研究为“表示工程”在机器人学习中的重要性提供了有力论据。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文围绕“对象中心表示是否对机器人操作有益”这一核心问题展开探讨，旨在评估这种表示方法在提升机器人操作任务性能方面的潜在价值。然而，由于未提供论文正文内容，无法准确提炼具体采用的关键技术方法（如表示学习或感知算法）及其要点，也无法给出核心实验结论或性能提升数据（例如准确率或效率指标）。建议补充论文正文以便进行精准总结。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.19408" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>