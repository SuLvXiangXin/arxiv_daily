<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Touch begins where vision ends: Generalizable policies for contact-rich manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Touch begins where vision ends: Generalizable policies for contact-rich manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.13762" target="_blank" rel="noreferrer">2506.13762</a></span>
        <span>作者: Raunaq Bhirangi Team</span>
        <span>日期: 2025-06-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人学习领域，模仿学习依赖大量难以获取的高质量演示，而强化学习则常产生脆弱、过拟合于训练环境、泛化能力差的策略。这两种数据驱动方法在处理需要毫米级精度的接触式精细操作任务时面临巨大挑战。本文针对这一痛点，提出了一个新的视角：将精细操作任务解耦为两个阶段——利用视觉进行场景级推理的“全局到达阶段”和利用局部感知进行接触式操作的“局部交互阶段”。其核心思路是，尽管任务的环境背景多变，但任务核心的底层物理交互动态是恒定的；因此，可以训练一个在规范环境中学习、可重用的局部策略，并通过“先定位再执行”的策略实现泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViTaL框架的整体流程分为三个阶段：1）利用语义增强进行视觉触觉行为克隆，学习一个泛化性强的基策略；2）通过残差强化学习优化策略修正，在保持视觉鲁棒性的同时提升下游性能；3）利用基于视觉语言模型的到达阶段，实现对新空间配置的零样本适应。</p>
<p><img src="https://arxiv.org/html/2506.13762v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：ViTaL方法总览。(A) 利用视觉基础模型通过程序化生成的背景增强任务数据，提升视觉多样性。(B) 使用该数据训练一个可泛化的视觉-触觉策略，随后通过在线残差强化学习进行精度微调。(C) 最后，VLM引导的到达阶段使得策略能在全新的空间配置中零样本部署，尽管策略是在固定的物体位置上训练的。</p>
</blockquote>
<p><strong>1. 通过语义增强实现可泛化的行为克隆</strong>：首先，在固定目标物体位置、随机初始化机器人末端附近的情况下，收集32条成功的视觉（手腕相机）和触觉（AnySkin传感器）演示数据。观测和动作均以机器人末端执行器坐标系为基准，以实现泛化。关键创新在于引入了基于视觉基础模型的<strong>语义增强流程</strong>：通过人工在单帧标注关键点，利用DIFT进行对应匹配，再用Segment-Anything 2进行实例分割，最后用XMem进行时序跟踪，从而分离出与任务相关的物体和可增强的背景区域。这使得在策略学习时，可以针对性地替换背景（使用RoboEngine程序化生成），同时保留对接触协调至关重要的视觉特征。增强后的视觉数据通过随机初始化的ResNet-18编码，触觉数据通过MLP编码，然后输入一个视觉-触觉Transformer策略（π^b）进行动作预测，使用动作分块和均方误差损失进行训练。</p>
<p><strong>2. 使用演示引导的强化学习进行微调</strong>：基策略π^b的成功率有限，因此采用<strong>残差强化学习</strong>来训练一个残差策略π^r。最终策略的动作a是基动作a^b ~ π^b(z)与残差偏移a^r ~ π^r(z, a^b)之和。在线学习时，固定行为克隆阶段训练好的编码器，向冻结的基策略和残差执行器网络提供压缩表征z^i（图像）和z^t（触觉）。残差执行器网络π^r以(z^i, z^t, a^b)为输入预测a^r，残差评论家Q^r评估(z^i, z^t, a^b, a^r)对。使用n-step DDPG作为RL优化器，并对执行器网络添加L2权重正则化。奖励由轨迹结束时的二元成功奖励和到目标点的密集L1距离组成。<strong>至关重要的一点是</strong>，在线训练的replay buffer中同样应用了行为克隆阶段的语义视觉增强，以保持泛化能力。</p>
<p><strong>3. 推理</strong>：采用分层推理策略实现空间和场景泛化。高层使用视觉语言模型（如Molmo）根据自然语言指令和外部RGB-D观测，粗略定位目标物体并预测3D坐标，机器人采样一个初始末端位姿到达目标附近。随后，部署学习到的视觉-触觉局部策略完成精确操作。这种组合将通用VLM的粗略导航与处理任务精确部分的局部策略相结合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在UFACTORY xArm 7机器人上进行，配备xArm Gripper和AnySkin触觉传感器。观测包括手腕鱼眼相机128x128 RGB图像和15维触觉读数。使用四个接触式精细操作任务（插入插座、插入USB、刷卡、钥匙开锁）和一个拾取面包任务进行评估。每个任务收集32条演示。</p>
<p>对比的基线方法包括：<strong>BAKU</strong>（仅视觉的Transformer行为克隆策略）、<strong>ViSk</strong>（视觉+触觉的BAKU）、<strong>RLPD</strong>（从零开始训练的RL，其replay buffer混合专家数据）以及本文的<strong>ViTaL-BC</strong>（仅使用语义增强的视觉-触觉基策略）。</p>
<p><img src="https://arxiv.org/html/2506.13762v1/x2.png" alt="域内性能"></p>
<blockquote>
<p><strong>图2</strong>：ViTaL在域内设置下的性能。表格展示了各方法在五个任务上的成功次数（10次尝试）。ViTaL在四个接触式任务上显著优于所有基线，平均绝对提升达40%。ViSk优于BAKU，凸显了触觉感知的重要性；ViTaL优于RLPD，说明了离线预训练对样本高效在线学习的价值。</p>
</blockquote>
<p><strong>空间泛化与场景泛化</strong>：在仅改变目标物体位置（背景固定）的测试中，ViTaL性能与域内设置相当，验证了末端坐标系观测的有效性。在固定目标位置但改变背景为杂乱新场景的测试中，ViTaL和ViTaL-BC均表现出强大鲁棒性，显著优于基线，证明了语义增强对解耦任务相关视觉线索与环境噪声的关键作用。在同时改变位置和背景的更具挑战性设置下，ViTaL依然保持高性能。</p>
<p><img src="https://arxiv.org/html/2506.13762v1/x3.png" alt="泛化性能"></p>
<blockquote>
<p><strong>图3</strong>：ViTaL在空间泛化、场景泛化以及二者结合扰动下的性能研究表格。结果基于30次尝试（3个新配置各10次）。数据显示ViTaL在各类扰动下均保持高成功率，而基线方法（尤其是纯视觉或未增强的方法）在场景变化时性能急剧下降甚至归零，突显了ViTaL泛化框架的有效性。</p>
</blockquote>
<p><strong>关键设计选择的消融实验</strong>：</p>
<ol>
<li><strong>触觉感知</strong>：在控制实验中，移除触觉输入会平均降低约40%的成功率。在“刷卡”等任务中，当被抓持物体遮挡目标时，触觉反馈变得不可或缺。</li>
<li><strong>语义增强</strong>：对比有无增强的行为克隆策略（ViTaL-BC vs. ViSk），增强带来了巨大的性能提升（例如，插入插座任务从0/10提升到~8.3/10）。这表明，即使数据量有限，通过增强引入视觉多样性也能让视觉编码器学习更具泛化性的表征。</li>
<li><strong>残差RL</strong>：对比ViTaL和ViTaL-BC，残差RL微调进一步大幅提升了策略的精度和鲁棒性，同时通过持续应用增强保持了泛化能力。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一个<strong>解耦的、分层的策略学习框架（ViTaL）</strong>，将场景级推理与可重用的局部接触式操作分离，实现了对空间和场景变化的强泛化。2）引入了<strong>基于基础模型的语义增强流程</strong>，能够利用极少量的演示数据训练出泛化性强的视觉编码器。3）成功地将<strong>语义增强与残差强化学习结合</strong>，使得策略能在保持泛化性的同时通过在线交互提升精度。</p>
<p>论文提到的局限性包括：对高质量的触觉传感器有依赖；VLM引导的到达阶段其定位精度会影响局部策略的启动条件；目前方法主要针对相对孤立的精细操作任务。</p>
<p>这项工作对后续研究的启示在于：证明了将通用基础模型（用于高层规划）与专门训练的、感知模态丰富的局部技能相结合是一条可行的路径；为数据高效的机器人学习提供了新思路，即通过智能数据增强和残差学习来放大有限演示数据的价值；强调了在接触式操作中，触觉与视觉的互补性以及以自我为中心（egocentric）的观测框架对于策略迁移的重要性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对接触式精细操作任务中数据驱动方法泛化性差的问题，提出ViTaL策略学习框架。其核心是将任务分解为两个阶段：首先利用视觉语言模型进行场景级推理以定位目标（到达阶段），随后调用一个与场景无关的、可重用的局部策略，该策略结合自我中心视觉与触觉传感执行精细操作（局部交互阶段）。关键技术包括利用基础模型分割训练稳健的视觉编码器、通过残差强化学习提升策略泛化能力，以及引入触觉传感。实验表明，该框架在未见环境中接触式任务的成功率可达约90%，且对干扰物具有鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.13762" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>