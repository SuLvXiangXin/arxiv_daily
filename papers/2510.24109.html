<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.24109" target="_blank" rel="noreferrer">2510.24109</a></span>
        <span>作者: Philip Dames Team</span>
        <span>日期: 2025-10-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，以人为中心的人工智能（HAI）对机器人的自然语言交互、复杂任务规划与执行能力提出了更高要求。基于大语言模型（LLMs）的智能体为实现HAI开辟了新途径。然而，现有的基于LLM的具身智能体（如SayCan、Grounded Decoding等方法）通常采用规划器-执行器架构，普遍存在两个关键局限性：一是缺乏对新环境的适应能力，二是任务执行过程中缺少反馈机制。这导致机器人难以在线规划和执行复杂的自然语言控制任务。</p>
<p>本文针对上述痛点，提出了一种新颖的具身智能体框架PFEA（基于LLM的高级自然语言规划与反馈具身智能体）。其核心思路是：构建一个集成了视觉环境感知的高级自然语言规划器，并引入任务执行反馈模块，使智能体能够感知执行结果并相应调整策略，从而在复杂动态环境中提高任务成功率。</p>
<h2 id="方法详解">方法详解</h2>
<p>PFEA的整体框架是一个三组件系统，其完整工作流程始于人类语音输入，依次进行语音识别、视觉语言任务规划、转换为可执行的Python指令、通过机器人动作执行任务、任务完成评估，最后生成自然语言响应报告执行结果。</p>
<p><img src="https://arxiv.org/html/2510.24109v1/x1.png" alt="整体系统架构"></p>
<blockquote>
<p><strong>图1</strong>：PFEA的整体系统架构。A部分是人机语音交互模块，负责“听”和“说”；B部分是核心的视觉语言代理，包含规划器、转换器和评估器；C部分是机器人动作执行模块，实现语义到空间的映射和抓取操作。</p>
</blockquote>
<p><strong>核心模块一：语音信息处理</strong><br>该模块赋予智能体类似人类的“听觉”和“说话”能力。语音输入通过自动语音识别（ASR）模型（如SenseVoice）转换为文本，传递给规划器。任务执行后，智能体使用文本转语音（TTS）模型（如CosyVoice）生成语音反馈。系统采用噪声触发机制进行音频捕获，确保精确的端点检测。</p>
<p><strong>核心模块二：视觉语言代理</strong><br>这是系统的“大脑”，由三个子模块构成：</p>
<ol>
<li><strong>规划器</strong>：接收高级指令，结合环境视觉信息（通过板载摄像头获取RGB图像），将抽象指令分解为一系列具体的、可执行的子任务。论文采用基于视觉语言模型（VLM）的规划器，而非仅依赖LLM+ grounding模型（如SayCan），以实现任务执行与现实环境的紧密耦合。核心LLM为ChatGLM，也兼容其他模型。规划器使用结构化提示词（见图2a），包含任务描述、相关示例和用户输入。</li>
<li><strong>转换器</strong>：将规划器输出的每个子任务步骤，转换为机器人可执行的控制代码（Python代码），对应机器人抓取与动作执行部分的技能库。转换器同样由ChatGLM驱动，并配有专门的提示词（见图2b）。</li>
<li><strong>评估器</strong>：在所有步骤执行完毕后，调用基于VLM的评估器模块来评估任务结果。评估器遵循结果导向的评估策略，判断任务是否成功完成以及是否需要重新执行。如果执行不理想，系统会循环回规划阶段重新尝试。评估结果反馈给规划器和转换器，以持续优化系统。其提示词示例如图2c所示。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.24109v1/x2.png" alt="规划器、转换器和评估器的提示词"></p>
<blockquote>
<p><strong>图2</strong>：智能体各核心模块的提示词设计。(a) 规划器提示词，包含系统指令、示例和用户输入；(b) 转换器提示词，指导LLM将步骤转换为可执行代码；(c) 评估器提示词，用于结果导向的任务完成度评估。</p>
</blockquote>
<p><strong>核心模块三：机器人抓取与动作执行</strong><br>该模块是具身智能体的物理执行核心。系统采用开放词汇模型构建语义-空间映射框架。转换器生成的语义参数与实时RGB-D视觉特征进行跨模态匹配，以识别图像中目标物体的空间位置。2D像素坐标通过标定的投影矩阵转换为3D世界坐标。在抓取过程中，机械臂利用逆运动学算法计算运动轨迹，通过雅可比矩阵求解确定最优关节角度，精确移动到目标位置执行抓取和放置操作。</p>
<p><strong>创新点</strong>：与SayCan等现有方法相比，PFEA的主要创新在于：1) 提出了一个<strong>集成视觉环境感知的VLM规划器</strong>，使规划能实时适应具体场景；2) 引入了<strong>任务执行反馈循环</strong>（评估器），形成了“规划-执行-评估-再规划”的闭环；3) 采用<strong>训练免费</strong>的方式，通过提示工程将大规模模型无缝集成到真实机器人系统中。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验采用两阶段设计：先在模拟环境验证核心机制，再在真实世界评估实际应用性。</p>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>模拟环境</strong>：基于RAVENS构建桌面操作仿真，包含UR5e机械臂等。创建了10个不同场景（见图3），并设计了20个高级自然语言指令任务（10个有提示，10个无提示），涵盖堆叠、颜色匹配、水果放置、物品分类、工具打包、桌面整理等类别。每个任务测试20次。</li>
<li><strong>真实环境</strong>：使用JAKA Mini 2机械臂和RealSense D435i RGB-D相机，设计了4个桌面操作任务。</li>
<li><strong>基线方法</strong>：严格遵循SayCan架构核心功能模块的LLM+CLIP方法。</li>
<li><strong>评估指标</strong>：任务规划成功率（针对无提示任务，衡量泛化能力）、总体任务完成成功率。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.24109v1/x3.png" alt="模拟实验场景与任务完成示意"></p>
<blockquote>
<p><strong>图3</strong>：模拟实验的十个初始桌面场景（第一行）及对应的任务完成状态（第二行）。展示了从堆叠到桌面整理等多种任务类型。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>总体性能</strong>：如表I所示，在模拟实验中，PFEA在<strong>有提示任务</strong>上的平均任务成功率为65%，在<strong>无提示任务</strong>上为74%，综合表现显著优于基线。PFEA相比LLM+CLIP基线方法，**平均任务成功率提高了约28%**。</li>
<li><strong>泛化能力</strong>：在无提示任务中，PFEA的成功率（74%）接近基线方法（35%）的两倍，展现了强大的零样本泛化能力。例如，在要求将水果分配到两个盘子、或处理未见过的物品等挑战性任务中表现出色。</li>
<li><strong>组件贡献</strong>：实验结果表明，完整的PFEA框架对任务成功执行至关重要。<strong>视觉感知模块</strong>对于实现知情和可靠的决策处于核心地位；而<strong>评估与反馈机制</strong>对于完成复杂任务工作流程必不可少。</li>
<li><strong>真实世界验证</strong>：如表II和图4所示，在四个真实世界任务中，PFEA的平均成功率为68%，基线为40%，PFEA依然保持显著优势。智能体能够处理“将桌上除订书机外的所有物品放入盒子”等复杂指令。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.24109v1/x4.png" alt="真实世界任务执行过程"></p>
<blockquote>
<p><strong>图4</strong>：四个真实世界任务（从左至右：放铁夹、放订书机、将所有物品放入盒子、除订书机外所有物品放入盒子）的执行过程可视化。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>基于视觉语言的统一场景理解与任务规划框架</strong>，能够根据实时环境将高级语言指令分解为可执行的低级动作。</li>
<li>引入了<strong>任务执行的反馈控制机制</strong>，通过评估器对执行结果进行判断并反馈给规划器，形成闭环，提高了整体任务成功率。</li>
<li>构建并验证了一个<strong>训练免费的具身智能体系统</strong>，能够无缝连接大模型与物理机器人，在模拟和真实场景中均实现了部署。</li>
</ol>
<p><strong>局限性</strong>：论文提到，系统的性能在一定程度上依赖于提示词的设计质量。此外，在复杂场景中，物体遮挡可能影响视觉感知和规划。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>模块化与可替换性</strong>：PFEA的模块化设计表明，其各组件（如规划、评估用的VLM/LLM）有进一步优化和替换的潜力，例如探索更强大的基础模型或更高效的提示策略。</li>
<li><strong>多模态融合的深化</strong>：当前系统主要依赖视觉和语言，未来可探索融入触觉、力觉等多模态反馈，以处理更精细的操作和更不确定的环境。</li>
<li><strong>长期任务与实时性</strong>：论文提到长期任务规划仍是挑战。后续工作可着眼于提升系统在长周期、多步骤任务中的规划鲁棒性，以及进一步提高从感知到执行的实时响应速度。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决现有基于LLM的具身智能体难以在线规划与执行复杂自然语言控制任务的问题。提出了PFEA框架，其核心是包含视觉任务规划器、指令转换器和反馈评估器的视觉语言智能体模块，以实现高层指令的闭环规划与执行。实验表明，该智能体在模拟和真实环境中的平均任务成功率比仅使用LLM+CLIP的方法提升了28%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.24109" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>