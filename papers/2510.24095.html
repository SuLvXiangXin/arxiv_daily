<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Parameterized Skills from Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Parameterized Skills from Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.24095" target="_blank" rel="noreferrer">2510.24095</a></span>
        <span>作者: George Konidaris Team</span>
        <span>日期: 2025-10-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人学习领域，从演示中学习技能（Skill Learning）是一种提升样本效率、实现任务泛化的重要途径。现有主流方法主要聚焦于学习纯离散技能或纯连续技能。然而，纯离散技能缺乏应对多样场景所需的灵活性；而纯连续技能则往往结构松散，难以解释。本文针对这一痛点，提出了学习<strong>参数化技能</strong>（Parameterized Skills）的新视角，即学习本质上是离散的、但可通过连续参数进行调节的技能。这种设计结合了离散技能的结构化、可解释性优势与连续技能的灵活性优势。本文的核心思路是：通过一个端到端的层次化变分推理框架，从多任务演示数据中联合学习离散技能选择、连续参数调节以及底层动作策略，并引入信息瓶颈等机制防止模型退化，从而发现可泛化、有意义的参数化技能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为 <strong>Deps</strong>（Discovery of GEneralizable Parameterized Skills）。其整体框架是一个三层策略层次结构。</p>
<p><img src="https://arxiv.org/html/2510.24095v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DEPS的三层层次结构。给定完整的环境观测，离散技能策略从技能库中选择一个技能。基于该选择，连续参数策略输出调节所选技能的连续参数，描绘出一个轨迹流形（左侧图示）。最后，仅观测压缩后一维机器人状态的底层动作策略产生原始动作。</p>
</blockquote>
<p><strong>输入与输出</strong>：输入为多任务专家演示数据集 ( D={T_i} )，其中每个演示 ( T_i = {\tau_i, l_i} ) 包含轨迹 ( \tau_i = {s_t, a_t} ) 和任务标签 ( l_i )。输出为一个三层策略：1) 离散技能策略 ( \pi^K )，输出技能索引 ( k_t )；2) 连续参数策略 ( \pi^Z )，输出连续参数 ( z_t )；3) 底层子策略 ( \pi^A )，输出可执行动作 ( a_t )。推理时，按 ( k_t \sim \pi^K ), ( z_t \sim \pi^Z ), ( a_t \sim \pi^A ) 的顺序采样。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>变分推理框架</strong>：目标是最大化演示轨迹的似然 ( \log p(\tau, l) )。由于对潜在技能序列 ( (\kappa, \zeta) ) 的边际化难以处理，本文采用时序变分推理（Temporal Variational Inference）。引入一个变分后验分布 ( q(\kappa, \zeta \mid \tau, l) ) 来近似真实后验 ( p(\kappa, \zeta \mid \tau, l) )，并最大化其证据下界（ELBO）。该下界最终形式为（论文公式4）：<br>[<br>\mathcal{L} = \mathbb{E}[\sum_t \log \pi^A(a_t \mid \mathcal{H&#39;}<em>t^A)] - \mathbb{E}[\sum_t D</em>{KL}(q(k_t \mid \tau, l) | \pi^K(\cdot \mid \mathcal{H&#39;}<em>t^K)) + \mathbb{E}</em>{k_t \sim q}[D_{KL}(q(z_t \mid \tau, k_t, l) | \pi^Z(\cdot \mid \mathcal{H&#39;}_t^Z))]]<br>]<br>该目标函数同时鼓励子策略 ( \pi^A ) 重构动作，并使自回归策略 ( (\pi^K, \pi^Z) ) 逼近能访问整个轨迹信息的变分后验 ( q )。</p>
</li>
<li><p><strong>技能作为参数化轨迹流形与信息瓶颈</strong>：为防止模型退化（即子策略忽略潜在变量、直接记忆动作），Deps 引入了关键的信息瓶颈设计。子策略 ( \pi^A ) <strong>不接收原始状态 ( s_t )</strong>，而是接收一个被压缩到一维的标量状态 ( s&#39;<em>t )：<br>[<br>s&#39;<em>t = \tanh(\mathbf{w}</em>{(k_t,z_t)} \cdot s_t^{\mathrm{proj}} + b</em>{(k_t,z_t)})<br>]<br>其中 ( s_t^{\mathrm{proj}} ) 是从原始观测（如机器人末端坐标）提取的特征，( \mathbf{w}<em>{(k_t,z_t)} ) 和 ( b</em>{(k_t,z_t)} ) 由一个小型MLP ( f_{\mathrm{compress}}(k_t, z_t) ) 输出。此设计迫使子策略必须依赖离散技能 ( k_t ) 和连续参数 ( z_t ) 来决定动作，从而确保这些潜在变量编码了关键的技能信息。这背后是将一个连贯的技能概念化为一个低维轨迹流形，一维标量 ( s&#39;_t ) 可视为在该流形上的进度索引。</p>
</li>
<li><p><strong>网络架构与训练流程</strong>：</p>
<ul>
<li><strong>变分网络 ( q )</strong>：使用双向GRU处理整个演示轨迹，为每个时间步推断技能 ( k_t )（类别分布）和参数 ( z_t )（高斯分布）的后验。为鼓励技能在时间上延伸，<strong>连续参数 ( z_t ) 的变分后验是基于整个轨迹和当前技能 ( k_t ) 预测的</strong>，而非逐时间步独立。</li>
<li><strong>高层策略 ( \pi^K, \pi^Z )</strong>：均实现为单向GRU，处理历史状态，并自回归地预测当前技能 ( k_t ) 及其参数 ( z_t )。注意，( \pi^Z ) 在<strong>推理时</strong>是每步都预测 ( z_t )，以实现更反应性的行为。</li>
<li><strong>底层子策略 ( \pi^A )</strong>：一个前馈网络，以压缩状态 ( s&#39;_t )、技能 ( k_t ) 和参数 ( z_t ) 为输入，输出动作分布参数。</li>
<li><strong>信息不对称</strong>：高层策略（( q, \pi^K, \pi^Z )）可以访问丰富的图像观测（若有）和本体感知状态，而子策略 ( \pi^A ) 仅能访问压缩后的本体感知状态。这进一步防止了过拟合。</li>
<li><strong>正则化</strong>：引入了“技能参数范数惩罚”（Skill Parameter Norm Penalty）来防止连续参数 ( z_t ) 的幅值过大、过拟合到特定轨迹。</li>
</ul>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>参数化技能的联合发现与学习</strong>：与需要预定义技能库或分阶段聚类的方法不同，Deps 以端到端方式直接从原始演示中联合学习离散技能、连续参数和动作策略。</li>
<li><strong>防止退化的系统性设计</strong>：通过将技能建模为轨迹流形、在子策略施加信息瓶颈（状态压缩）、以及信息不对称的架构，有效避免了潜在变量模型常见的退化问题，确保了所学技能的有意义和可泛化性。</li>
<li><strong>灵活的推理机制</strong>：变分后验按技能预测连续参数以鼓励时间一致性，而策略网络在推理时逐步细化参数以保持反应能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.24095v1/x2.png" alt="概率图模型"></p>
<blockquote>
<p><strong>图2</strong>：Deps 底层的概率图模型。变分编码器可以访问从历史到未来的整个轨迹信息。离散和连续策略作为高层策略，基于之前时间步的信息推断参数化技能。底层子策略基于参数化技能以及当前状态推断动作。每个模型可观测的变量以灰色阴影标出。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.24095v1/x3.png" alt="技能作为参数化轨迹流形"></p>
<blockquote>
<p><strong>图3</strong>：技能作为参数化轨迹流形。假设单个技能对应一个参数化轨迹族。一个一维状态表示索引到这个可泛化的流形中以预测动作。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试/数据集</strong>：LIBERO（90个任务，使用其中80个预训练）和 MetaWorld-v2。</li>
<li><strong>实验平台</strong>：使用上述数据集提供的离线演示进行预训练，然后在未见任务上进行少量样本（500梯度步）微调。</li>
<li><strong>评估指标</strong>：平均成功率（Mean Success）和平均最高成功率（Mean Highest Success），均针对未见任务计算。</li>
<li><strong>对比基线</strong>：<ol>
<li>多任务行为克隆网络（BC）。</li>
<li>未经预训练的相同BC架构（BC-Untrained）。</li>
<li>当前最先进的技能学习基线 PRISE（学习动作“令牌”并应用字节对编码）。</li>
</ol>
</li>
</ul>
<p><strong>关键实验结果</strong>：<br>如表1所示，Deps 在所有评估设置上均显著优于基线方法。</p>
<p><img src="https://arxiv.org/html/2510.24095v1/x6.png" alt="结果表"></p>
<blockquote>
<p><strong>图6/表1</strong>：在LIBERO和MetaWorld-v2各评估设置上的平均成功率。Deps 在“平均成功率”和“平均最高成功率”两项指标上均全面领先所有基线，尤其在分布外泛化（LIBERO-OOD）和少样本（LIBERO-3-shot）场景下优势明显。</p>
</blockquote>
<ul>
<li><strong>LIBERO</strong>：在最具挑战性的分布外任务集（LIBERO-OOD）上，Deps 的平均最高成功率达到 ( 0.66 \pm 0.12 )，远高于 PRISE 的 ( 0.27 \pm 0.23 ) 和 BC 的 ( 0.36 \pm 0.08 )。在仅有3条演示的少样本设置（LIBERO-3-shot）下，Deps 的成功率（( 0.49 \pm 0.03 )）也接近翻倍于最佳基线 BC（( 0.22 \pm 0.08 )）。</li>
<li><strong>MetaWorld-v2</strong>：在两个测试集（MW-Vanilla 和 MW-PRISE）上，Deps 同样取得了最佳性能。例如在MW-Vanilla上，平均最高成功率为 ( 0.65 \pm 0.03 )，优于 BC 的 ( 0.51 \pm 0.01 )。</li>
</ul>
<p><strong>消融实验分析</strong>：<br>论文通过消融实验验证了核心组件的贡献。</p>
<p><img src="https://arxiv.org/html/2510.24095v1/x7.png" alt="消融实验-状态压缩"></p>
<blockquote>
<p><strong>图7</strong>：状态压缩维度的消融研究。将状态压缩到一维（1D）能取得最佳性能，更高维度（7D）或使用原始状态（Raw）会导致性能下降，验证了信息瓶颈的有效性。</p>
</blockquote>
<ul>
<li><strong>状态压缩</strong>：如图7所示，将状态压缩至1维效果最好。使用原始状态或7维压缩状态会导致性能下降，证明了强信息瓶颈对于迫使模型利用潜在变量至关重要。</li>
<li><strong>技能参数预测方式</strong>：如图8所示，将变分后验中连续参数 ( z_t ) 的预测方式从“每技能”（per-skill）改为“每时间步”（per-timestep），会导致性能显著下降。这表明“每技能”的预测方式对于学习时间上连贯、可泛化的技能是必要的。</li>
<li><strong>技能参数范数惩罚</strong>：移除该惩罚项也会导致性能下降，表明其对防止过拟合有益。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.24095v1/x8.png" alt="消融实验-参数预测方式"></p>
<blockquote>
<p><strong>图8</strong>：连续参数预测方式的消融研究。在变分网络中采用“每技能”（per-skill）方式预测 ( z_t )（即一个技能实例对应一组参数）比“每时间步”（per-timestep）方式性能更优，后者会导致技能碎片化。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>Deps</strong>，一个从演示中端到端发现参数化技能的框架，通过三层层次化策略和变分推理联合学习离散技能选择、连续参数调制和底层动作。</li>
<li>引入了<strong>信息瓶颈</strong>（将状态压缩至一维）和<strong>将技能概念化为参数化轨迹流形</strong>的关键设计，有效解决了潜在变量模型的退化问题，学得的技能具有时间扩展性、语义意义和可泛化性。</li>
<li>在 LIBERO 和 MetaWorld-v2 两个具有挑战性的多任务基准上进行了全面实验，证明了 Deps 在快速泛化到未见任务方面显著优于现有的多任务学习和技能学习方法。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，该方法依赖于高质量的专家演示数据。此外，框架涉及多个神经网络组件联合训练，可能带来一定的计算复杂度和调参成本。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>参数化技能作为通用抽象</strong>：证明了结合离散与连续特性的参数化技能是一种有效的行为抽象，可促进机器人技能的复用和组合。未来可探索更复杂的技能参数化形式或层级结构。</li>
<li><strong>从离线到在线</strong>：当前工作完全基于离线演示。一个自然的延伸是将此框架与在线强化学习相结合，使智能体能在与环境交互中进一步 refine 或发现新技能。</li>
<li><strong>可解释性与可控性</strong>：所学技能展现出可解释性（如抓取技能参数对应抓取位置）。这为人类指导、任务指定和高层规划提供了便利，可进一步研究如何利用这种可解释性实现更高效的人机协作或任务分解。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Deps算法，解决从专家演示中学习结构化、可泛化技能的问题。核心是联合学习参数化技能策略与元策略，通过时间变分推断和信息论正则化，避免潜在变量模型退化，确保技能具有时序扩展性和语义意义。实验表明，该方法在LIBERO和MetaWorld基准上优于多任务及技能学习基线，显著提升对未见任务的泛化能力，并能学习可解释的技能（如通过连续参数指定抓取位置的抓取技能）。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.24095" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>