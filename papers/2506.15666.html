<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Vision in Action: Learning Active Perception from Human Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Vision in Action: Learning Active Perception from Human Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.15666" target="_blank" rel="noreferrer">2506.15666</a></span>
        <span>作者: Shuran Song Team</span>
        <span>日期: 2025-06-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前主流的机器人模仿学习系统通常依赖腕部摄像头或固定第三人称摄像头。由于腕部摄像头随手臂运动，其视角受限于操作需求而非感知目标，在存在视觉遮挡的场景中尤为受限，往往无法捕捉到任务关键信息。此外，在数据收集过程中，人类会自然地移动视线来引导注意力，但机器人通常从固定或不匹配的视角观察场景，这种观察不匹配导致系统无法捕捉人类丰富的感知行为（如搜索、跟踪、聚焦），从而阻碍了有效策略的学习。尽管主动感知至关重要，但其在机器人系统中的实现面临三大挑战：需要灵活的类人注视控制硬件、需同步相机与注视运动、以及难以规模化地获取主动感知策略。</p>
<p>本文针对机器人模仿学习中因观察视角不匹配而无法学习人类主动感知策略这一具体痛点，提出了一个名为“Vision in Action (ViA)”的双手机器人操作系统。其核心思路是：通过一个灵活的6自由度机器人“脖子”和一种基于中间3D表示的VR遥操作界面，构建人与机器人共享的观察空间，从而直接从人类演示中学习任务相关的主动感知策略，并利用行为克隆训练出鲁棒的视觉运动策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViA系统包含三个核心部分：一个灵活的机器人颈部硬件设计、一个低延迟的VR遥操作界面，以及一个基于扩散策略的视觉运动策略学习框架。</p>
<p><img src="https://arxiv.org/html/2506.15666v1/x2.png" alt="VR遥操作对比"></p>
<blockquote>
<p><strong>图2</strong>: VR遥操作对比。左图：传统的RGB流媒体传输由于RGB数据传输延迟和机器人控制延迟而存在运动到光子的延迟，常导致VR晕动症。右图：我们的系统通过(a, e)从RGB-D数据流式传输世界坐标系下的3D点云，(b, c)基于用户最新头部姿态进行实时视图渲染，以及(d)异步更新机器人头部和手臂姿态来缓解此问题。这种方法为用户实现了低延迟的视点更新。</p>
</blockquote>
<p><strong>硬件设计</strong>：为模拟人类通过躯干和颈部协调运动来调整头部姿态的能力，ViA使用一个现成的6自由度ARX5机械臂作为机器人“脖子”，其末端安装iPhone 15 Pro作为主动头部摄像头，提供RGB、深度和同步的相机姿态数据。双手机器人则由另外两个6自由度ARX5机械臂构成，各配备一个平行夹爪。</p>
<p><strong>遥操作界面</strong>：该界面旨在同步控制机器人双臂和主动颈部。手臂控制使用一个仿照GELLO设计的全身双手机器人外骨骼。头部控制则通过VR界面实现，其核心创新在于通过<strong>中间3D场景表示</strong>解耦用户视图与机器人视图，以解决机器人物理运动带来的额外延迟问题。具体流程如下：</p>
<ol>
<li><strong>世界坐标系下的点云构建</strong>：以机器人颈部固定底座定义世界坐标系W。每个RGB-D帧利用相机内参和时刻t的机器人头部姿态（相对于世界系的外参 $^WT_H(t)$）转换到世界系，形成点云 $^WX(t)$，作为中间3D场景表示。</li>
<li><strong>低延迟视图渲染</strong>：基于点云 $^WX(t)$，使用用户在时刻t+k（k为短时间间隔）于世界系中的最新头部姿态 $^WT_{user}(t+k)$，为VR显示器渲染立体RGB视图。这使得用户的视点能随头部运动即时更新，视觉反馈延迟极低（&lt;7 ms）。</li>
<li><strong>基于聚合头部运动的点云与机器人更新</strong>：机器人的头部姿态在更长的时间间隔K后更新为 $^WT_H(t+K)$，其中K由机器人控制延迟决定，远大于渲染间隔k。同时，点云以较低频率（10 Hz）异步地使用来自机器人的新RGB-D观测进行更新。这种设计实现了用户端的低视觉延迟与机器人端的平滑动作执行之间的平衡。</li>
</ol>
<p><strong>策略学习</strong>：采用基于Diffusion Policy的视觉运动策略网络。策略的输入包括来自主动头部摄像头的当前RGB图像观测 $\mathbf{I}_t$，以及包含颈部、左臂、右臂末端执行器姿态（位置和四元数）和两个夹爪宽度的本体感觉状态 $\mathbf{P}_t \in \mathbb{R}^{23}$。视觉编码器使用DINOv2预训练的ViT，提取其384维分类令牌作为紧凑的语义表示。策略输出未来动作序列 $\mathbf{A}_t \in \mathbb{R}^{n_p \times 23}$，其中每个动作包含世界系中颈部及双臂的未来末端执行器姿态和夹爪宽度。采用预测视野 $n_p=16$，执行视野 $n_a=8$，策略以10 Hz频率运行。</p>
<p>与现有方法相比，创新点具体体现在：1) 使用高自由度机械臂作为脖子，以简单硬件模拟了类人的上半身运动范围；2) 提出基于中间3D表示的异步遥操作界面，从根本上解决了VR晕动症问题，实现了共享观察空间；3) 不依赖手工启发式规则，而是通过共享观察空间的演示数据，让策略直接学习与任务执行耦合的主动感知行为。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个涉及严重视觉遮挡的、具有挑战性的多阶段双手机器人操作任务上进行评估：<strong>袋中取物</strong>（涉及交互式感知）、<strong>杯子排列</strong>（涉及主动视点切换）和<strong>青柠与锅对齐</strong>（涉及双手协调与精确对齐）。每个任务按阶段定义累积成功率。</p>
<p><img src="https://arxiv.org/html/2506.15666v1/x3.png" alt="任务定义"></p>
<blockquote>
<p><strong>图3</strong>: 任务定义。我们引入了三个多阶段任务，以突显主动感知在日常场景中的关键作用。左图：第三人称视角，红色箭头表示头部运动，蓝色箭头表示手臂运动。中图：跨任务阶段的主动头部摄像头视图（上行），以及机器人动作的第三人称视图（下行）。右图：测试场景。</p>
</blockquote>
<p><strong>摄像头配置对比</strong>：比较了三种策略学习的摄像头配置：1) **[ViA]**：仅使用单个主动头部摄像头；2) **[主动头部和腕部摄像头]**：组合使用主动头部和两个腕部摄像头；3) **[胸部和腕部摄像头]**：使用固定胸部摄像头和两个腕部摄像头（无颈部）。所有策略使用相同的演示数据集进行训练，仅输入视图不同。</p>
<p><img src="https://arxiv.org/html/2506.15666v1/x4.png" alt="摄像头配置对比"></p>
<blockquote>
<p><strong>图4</strong>: 策略学习摄像头配置对比。ViA使用单个主动头部摄像头，动态调整其视点以捕捉任务相关的视觉信息。相比之下，腕部和胸部摄像头策略常因视觉遮挡而失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.15666v1/x5.png" alt="摄像头配置结果"></p>
<blockquote>
<p><strong>图5</strong>: 策略学习摄像头配置对比结果。我们报告了三个任务的阶段成功率，以证明我们的主动头部摄像头ViA相比两种基线配置的有效性。</p>
</blockquote>
<p>关键结果：如图5所示，[ViA]在所有三个任务上始终优于其他两种配置。值得注意的是，为[ViA]增加腕部摄像头观测反而导致平均性能下降18.33%。论文分析认为，主动头部摄像头提供的视觉信息对于完成任务已是充分的，增加腕部摄像头可能引入了冗余或噪声（尤其是被遮挡时），在数据量有限的情况下增加了模型复杂度，不利于学习。与常用的[胸部和腕部摄像头]配置相比，[ViA]平均任务性能提升了45%，突显了主动视角调整在获取信息方面的优势。</p>
<p><strong>视觉表示对比</strong>：在相同主动头部摄像头输入下，比较了三种视觉表示：1) **[ViA]**：使用DINOv2 ViT骨干网络；2) **[ResNet-DP]**：使用ImageNet预训练的ResNet-18骨干网络；3) **[DP3]**：使用从主动头部摄像头转换的世界坐标系点云作为输入（从头训练）。</p>
<p><img src="https://arxiv.org/html/2506.15666v1/x6.png" alt="视觉表示结果"></p>
<blockquote>
<p><strong>图6</strong>: 策略学习视觉表示对比结果。我们报告了三个任务的阶段成功率，以证明我们方法ViA相比两种基线方法的有效性。</p>
</blockquote>
<p>关键结果：如图6所示，采用DINOv2 ViT表示的[ViA]在所有三个任务上取得了最高的最终阶段成功率。DINOv2提供的强大语义理解能力使策略能够先主动搜索物体再进行操作。相比之下，[DP3]基线常出现“幻觉”失败，例如在杯子任务中将手臂导向空置的架子区域，且在袋中取物任务中因无法精确抓握袋把手而完全失败，这归因于其从头训练的表示缺乏预训练的视觉先验和语义能力。</p>
<p><strong>遥操作界面对比</strong>：通过用户研究（8名参与者）比较了点云渲染方法与传统的立体RGB流媒体方法。</p>
<p><img src="https://arxiv.org/html/2506.15666v1/x7.png" alt="遥操作界面对比"></p>
<blockquote>
<p><strong>图7</strong>: 遥操作界面对比。我们基于三个指标评估我们的遥操作界面设计：报告的晕动症程度、完成每次演示的平均时长以及总体用户偏好。</p>
</blockquote>
<p>关键结果：如图7所示，虽然点云渲染方法导致数据收集时间稍长，但<strong>显著减少了晕动症</strong>。因此，8名参与者中有6人更倾向于选择该系统。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>系统设计</strong>：提出了一套完整的、用于学习主动感知的机器人系统ViA，包括灵活的6自由度颈部硬件和创新的低延迟VR遥操作界面。2) <strong>方法创新</strong>：通过中间3D表示实现观察空间共享，使机器人能够直接从人类演示中学习与任务执行紧密耦合的主动感知策略（如搜索、注视），而无需手工设计启发式规则。3) <strong>实验验证</strong>：在三个复杂的多阶段遮挡任务上验证了主动感知的重要性，ViA相比基线方法取得显著性能提升，并进行了详实的消融研究与用户研究。</p>
<p>论文自身提到的局限性包括：1) <strong>点云渲染的视觉质量</strong>：点云渲染可能丢失细节，未来可探索更高效的场景表示（如高斯泼溅）。2) <strong>策略学习的局限性</strong>：当前策略仅通过行为克隆从演示中学习，可能无法泛化至训练数据分布之外的情况，未来可结合强化学习进行优化。3) <strong>硬件成本</strong>：使用三个机械臂成本较高，未来需设计更紧凑、低成本的主动感知硬件。</p>
<p>对后续研究的启示：这项工作为“从演示中学习主动感知”开辟了道路。其基于共享观察空间的演示收集范式、异步低延迟的遥操作思想，以及对语义视觉表征的有效利用，为在更复杂、动态的真实世界环境中实现类人的主动感知机器人系统提供了重要的技术基础和设计思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ViA系统，旨在解决机器人模仿学习中因缺乏主动感知能力而导致在视觉遮挡场景下性能受限的问题。系统通过设计6自由度机器人颈部硬件实现类人头部运动，并构建基于VR的遥操作界面及异步3D场景表示以学习人类演示中的主动感知策略（如搜索、跟踪、聚焦）。实验表明，ViA在多个复杂双手操作任务上显著优于基线系统，有效提升了遮挡环境下的操作鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.15666" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>