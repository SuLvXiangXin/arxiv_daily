<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.11218" target="_blank" rel="noreferrer">2511.11218</a></span>
        <span>作者: Xiaoyu Ren Team</span>
        <span>日期: 2025-11-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人已在静态场景的移动、操作乃至更具挑战性的移动操作任务中展现出强大能力。然而，真实世界是动态的，准静态交互不足以应对多样的环境条件。作为迈向更动态交互场景的一步，本文聚焦于羽毛球运动，这是一个理想的测试平台：回球需要亚秒级的感知-动作循环、在巨大的3D拦截空间内精确计时和定向的球拍-羽毛球接触，以及融合快速手臂挥动与稳定敏捷腿部动作的全身协调。与机器人乒乓球相比，羽毛球加剧了多个难点：空气动力学高度不确定，有效击球窗口短暂，大幅度的挥拍会注入角动量挑战全身平衡。最关键的是，步法与击球必须协同进化，下肢动作不仅重新定位基座，还共同决定了击球精度。</p>
<p>现有相关研究存在局限。例如，一些人形机器人乒乓球工作依赖专家演示来构建高质量参考运动，或使用“虚拟击球平面”将击球简化为2D问题，且其基座运动由单独指令执行，而非从单一统一目标中涌现，导致步法与击球未被策略联合优化。另一些针对羽毛球的研究在四足平台上进行，其形态在平衡方面比人形机器人简单得多。本文旨在填补这些空白，训练一个无需专家演示或单独基座指令的单一全身控制器，并实现真实硬件部署。本文核心思路是：提出一个基于强化学习的训练流程，通过三阶段课程（先学步法，再学精确挥拍，最后进行任务聚焦优化）来生成一个统一的全身控制器，使机器人能够协调下肢步法与上肢击球。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架分为训练和部署两部分。训练阶段，使用PPO算法，在模拟环境中通过三阶段课程学习单一策略 π_WBC。部署阶段，通过动作捕捉系统获取基座状态，并使用扩展卡尔曼滤波器（EKF）预测羽毛球轨迹以生成击球目标，该目标与本体感知等信息共同构成策略的观测输入，策略输出关节位置目标由底层PD控制器跟踪执行。</p>
<p><img src="https://arxiv.org/html/2511.11218v2/figure2.png" alt="系统总览"></p>
<blockquote>
<p><strong>图1</strong>：系统总览。（a）训练：PPO在三阶段课程下，使用特权评论者观测与行动者观测学习单一策略π_WBC。（b）环境：人形机器人高1.28米，重30公斤，具有21个自由度。（c）部署：运行时，动作捕捉提供基座状态，EKF预测轨迹生成击球目标{p*_ee, q*_ee, t*}，与本体感知等共同构成行动者观测，π_WBC输出动作由底层PD控制器执行。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>策略学习设定</strong>：将策略学习建模为部分可观测马尔可夫决策过程。行动者观测空间为87维本体感知与外部感知的拼接，并叠加关节位置、速度、动作的短期（5帧）和长期（20帧）历史以缓解部分可观测性，总计1547维特征。评论者观测包含98维特权信息（如无噪声的基座和关节状态）。动作空间为21个关节的位置目标。采用非对称行动者-评论者架构，评论者接收额外的先验信息（如后续两个击球时间、目标位置和方向、剩余目标数）以稳定价值估计。</li>
<li><strong>三阶段强化学习课程</strong>：三个阶段使用相同的观测和动作空间，仅改变奖励项及其权重。<ul>
<li><strong>第一阶段（S1）- 步法获取</strong>：学习朝向采样击球区域移动，并在一个回合内的六个采样击球位置之间移动时保持稳定、面向前方的姿态。主要奖励是区域跟踪奖励<code>r_track</code>，鼓励靠近目标区域（距离d在0.3米内即得满分），并辅以标准的人形机器人稳定行走风格塑造奖励。</li>
<li><strong>第二阶段（S2）- 精确引导的挥拍生成</strong>：在S1基础上，降低区域跟踪权重，引入仅在预定击球时刻<code>t*</code>激活的稀疏击球奖励<code>r_swing</code>。该奖励包含耦合位置与方向精度的项，以及鼓励挥拍速度的项。初始设置宽松的精度容差（σ_p=2.0, σ_r=8.0）以让完整挥拍动作涌现，随后在训练中逐步收紧（σ_p=0.1, σ_r=1.0）。同时添加轻量的挥拍风格正则化（如鼓励球拍短边轴与世界y轴对齐）和默认持拍姿势奖励。</li>
<li><strong>第三阶段（S3）- 任务聚焦优化</strong>：从S2检查点开始，移除目标接近奖励和众多步态塑造正则化项（如脚距、接触对称性、步高），以避免梯度与击球目标冲突。保留全局正则化项和击球奖励，并启用域随机化和观测噪声以巩固鲁棒性。</li>
</ul>
</li>
<li><strong>基于模型的击球目标生成与预测</strong>：<ul>
<li><strong>训练数据生成</strong>：采用基于物理的羽毛球动力学模型生成飞行轨迹，并筛选出符合特定条件（如击球区高度z在[1.5, 1.6]米，最小飞行时间0.8秒）的轨迹用于构建包含约2万条轨迹的训练数据集。</li>
<li><strong>部署预测</strong>：使用基于相同动力学模型的EKF进行实时羽毛球状态估计和击球点预测。当预测高度首次落入预定义的拦截框时，其空间坐标和时间被指定为预测击球目标，对应的速度向量被转换为四元数以指导末端执行器朝向。</li>
</ul>
</li>
<li><strong>无预测变体</strong>：此变体仅改变行动者观测空间，不再接收指令击球目标<code>(p*_ee, q*_ee, t*)</code>，而是接收当前帧及之前五帧（50Hz采样）的世界坐标系羽毛球位置滑动窗口。行动者必须从这个短期历史中隐式推断预期的拦截位姿和时机，而评论者仍保留对“真实”拦截点的特权访问。训练数据生成方式支持此设计。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li>提出并实现了一个<strong>统一的全身体控制器</strong>，直接根据击球目标协调生成步法与挥拍动作，而非分离控制。</li>
<li>设计了<strong>三阶段强化学习课程</strong>，逐步引导策略从学习基础移动，到生成精确挥拍，最终优化击球性能，这是稳定训练的关键。</li>
<li>引入了<strong>无预测变体</strong>，探索了更端到端的控制策略，直接从羽毛球观测历史推断击球时机与目标，降低了对显式轨迹预测器的依赖。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟（IsaacGym）和现实世界（动作捕捉场地）中进行，使用自定义的1.28米高、21自由度人形机器人Phybot C1。对比的基线方法包括论文自身的主流水线（基于EKF预测）和无预测变体，并在消融实验中验证了课程各阶段的必要性。</p>
<p><img src="https://arxiv.org/html/2511.11218v2/figure3.png" alt="模拟与真实世界性能"></p>
<blockquote>
<p><strong>图2</strong>：模拟与真实世界性能。（a）模拟中，两个机器人使用基于EKF的策略可持续对打21个回合。（b）真实世界中，机器人成功回击机器发球，出球速度高达19.1 m/s，平均回球落点距离为4米。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟对打</strong>：两个使用基于EKF策略的机器人能够持续对打<strong>21个</strong>连续回合。</li>
<li><strong>真实世界击球</strong>：在机器发球条件下，机器人成功执行击球，出球速度最高达<strong>19.1 m/s</strong>，平均回球落点距离为<strong>4米</strong>。挥拍速度约为<strong>5.3 m/s</strong>，从发球到击球的窗口为0.8-1.0秒。</li>
<li><strong>无预测变体性能</strong>：在模拟中，无预测变体实现了与目标已知策略（基于EKF）<strong>相当的成功击球性能</strong>。这表明策略能够隐式地从短期历史中学习预测。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.11218v2/figure4.png" alt="课程阶段消融"></p>
<blockquote>
<p><strong>图3</strong>：三阶段课程的消融研究。移除S1或跳过S2直接进入S3都会导致训练失败。完整的S1-S2-S3课程带来了性能提升，S3阶段进一步提高了击球奖励并降低了能量/扭矩消耗。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>移除S1</strong>：直接从S2开始训练会导致发散。</li>
<li><strong>跳过S2</strong>：从S1直接跳到S3，课程跨度太大，训练无法可靠收敛。</li>
<li><strong>仅使用S1-S2</strong>：可获得可部署的策略，但仍有改进空间。</li>
<li><strong>完整S1-S2-S3</strong>：S3在启用域随机化等更困难条件下，不仅未退化，反而使主要击球奖励<code>r_swing</code>提升**3–5%<strong>，能量和扭矩消耗</strong>降低约20%**，并改善了其他各项指标。这证明了第三阶段任务聚焦优化的有效性。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.11218v2/figure5.png" alt="预测模块精度"></p>
<blockquote>
<p><strong>图4</strong>：预测模块精度评估。基于EKF的预测器在位置和速度估计上均表现出高精度，平均位置误差在0.02米以内，为控制器提供了可靠的目标。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.11218v2/figure6.png" alt="关节运动与接触力"></p>
<blockquote>
<p><strong>图5</strong>：关节运动与脚部接触力。轨迹显示关节运动平滑，脚部接触力曲线清晰，表明策略在动态挥拍中保持了良好的平衡。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>首次实现了真实世界人形机器人羽毛球运动</strong>，展示了一个自主的、统一的全身控制器，能够在亚秒反应时间内回击机器发球，产生高达19.1 m/s的出球速度。</li>
<li><strong>提出了用于步法-击球协调的多阶段强化学习课程</strong>，通过分阶段奖励设计，成功引导策略学习复杂的动态全身协调技能，且无需运动先验或专家演示。</li>
<li><strong>探索了无预测变体</strong>，证明了策略能够仅从短期羽毛球观测历史中隐式学习击球时机与目标，为简化部署提供了可能方向。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前测试集中于动作捕捉场地内的机器发球，尚未在更高天花板场地或与机器人/人类对手进行对打中验证。</p>
<p><strong>启示</strong>：本文提出的训练方案能够交付高度动态且精确的目标击打，可适配至更多动态关键领域。其分阶段课程学习思想、统一全身控制架构以及端到端感知-动作集成的探索，为后续研究人形机器人在其他高速动态交互任务（如接抛物体、动态避障等）提供了有益参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在动态环境中进行全身协调运动（如打羽毛球）的挑战，提出了一种基于多阶段强化学习的训练框架。核心方法采用三阶段课程学习：先学习步法，再生成精确挥拍动作，最后进行任务优化，无需运动先验或专家示范。部署时结合扩展卡尔曼滤波器预测羽毛球轨迹。实验表明，仿真中双机器人可持续对打21个回合；实物测试中，击球速度可达19.1 m/s，平均回球落点距离4米，验证了该方法在高速动态任务中的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.11218" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>