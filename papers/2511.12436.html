<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.12436" target="_blank" rel="noreferrer">2511.12436</a></span>
        <span>作者: Long Chen Team</span>
        <span>日期: 2025-11-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）在机器人高层任务规划与场景理解方面表现出色，但在推断物理交互的可操作位置（如功能性抓取点、允许放置区域）方面存在显著不足。这一局限源于其训练数据中缺乏对物体可供性和空间可供性的细粒度标注。现有可供性学习数据集往往只关注单一维度：要么是物体可供性（如UMD、PAD等，通过分割掩码标注物体功能部件），要么是空间可供性（如RoboPoint、RoboSpatial等，通过点坐标标注空间关系），缺乏一个将物体与空间可供性统一起来的综合性数据集，这限制了模型在真实世界操纵与导航任务中进行精细交互理解的能力。</p>
<p>本文针对上述痛点，提出了一个新的视角：构建一个生成式AI增强的大规模多模态数据集，以统一的方式涵盖物体可供性识别、物体可供性预测和空间可供性定位三大关键能力，从而系统性地增强VLMs在机器人操纵与导航中的可供性推理能力。本文的核心思路是：通过整合来自真实世界和仿真环境的多种数据源，并利用生成式AI（如GPT-4o）进行数据清洗与问答对生成，创建了一个包含87万张图像和200万个问答对的大规模数据集RoboAfford++，并基于此微调VLM，显著提升了其在物体与空间可供性任务上的表现。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架包含两个核心部分：RoboAfford++数据集的构建流程，以及基于该数据集微调得到的RoboAfford-Qwen++模型及其在下游机器人任务中的应用pipeline。</p>
<p><img src="https://arxiv.org/html/2511.12436v1/x2.png" alt="数据集构建流程"></p>
<blockquote>
<p><strong>图2</strong>：RoboAfford++数据集构建流程。首先过滤掉具有密集重复对象的图像，然后使用人工设计的模板或GPT-4o生成问答对。</p>
</blockquote>
<p><strong>数据集构建</strong>：RoboAfford++数据集整合了六个数据源（如表II所示），涵盖物体可供性识别、预测和空间可供性定位三个任务。构建流程（图2）主要包括数据收集过滤和问答对生成。</p>
<ol>
<li><strong>数据收集与过滤</strong>：对于物体可供性识别，使用了LVIS（提供边界框标注）和Pixmo-Points（提供点标注），并对后者进行了过滤（去除标注点过多及非室内物体图像）。对于物体可供性预测，使用了提供部件级分割掩码的PACO-LVIS数据集。对于空间可供性定位，使用了RoboPoint中的区域参考数据。此外，还专门在AI2Thor仿真器中构建了用于导航任务的NaviAfford数据集，包含5万张以自我为中心的图像及基于空间关系生成的点标注。</li>
<li><strong>问答对生成</strong>：针对不同任务设计了专门的生成管道。对于物体识别，使用GPT-4o分析场景并基于模板生成问题；对于物体预测，使用GPT-4o根据图像、物体/部件类别和边界框生成关于功能或部件的问题，答案包括边界框和从分割掩码采样的点；对于空间定位，则改编RoboPoint的标注，将归一化坐标转换为绝对坐标并重采样点。这一过程旨在将原始标注转化为可供VLM学习推理的问答形式。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.12436v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：RoboAfford-Qwen++框架。在RoboAfford++数据集上微调模型以增强物体和空间可供性能力。对于下游机器人操纵和导航任务，集成深度图像将代表可供性的2D点转换为3D坐标，进而用于机器人执行。</p>
</blockquote>
<p><strong>模型微调与应用</strong>：基于Qwen2.5-VL-7B模型进行全参数监督微调，得到RoboAfford-Qwen++模型（图3）。微调采用两阶段策略：</p>
<ol>
<li><strong>通用定位学习</strong>：使用LVIS和Pixmo-Points数据（共21.6万张图像，70.3万QA对），增强基础的物体可供性识别能力。</li>
<li><strong>物体-空间可供性增强</strong>：加入Object Reference、NaviAfford、PACO-LVIS和Region Reference数据（共65.4万张图像，133万QA对），优化物体可供性预测和空间可供性定位能力。这种渐进式训练旨在让模型发展出从基础识别到高级预测的层次化可供性推理能力。</li>
</ol>
<p><strong>下游任务应用</strong>：微调后的模型可用于真实世界机器人任务（图3）。给定指令（如“拿起最左边的水果放入最右边的篮子”），模型预测物体可供性点（如水果抓取点）和空间可供性点（如篮子内放置点）。这些2D点可进一步通过SAM提示分割目标物体，或结合深度图转换为3D坐标，最终由抓取姿态生成模块（如AnyGrasp）和机器人系统执行。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>数据集层面</strong>：首次大规模统一了物体与空间可供性标注，覆盖操纵与导航双领域；2) <strong>方法层面</strong>：设计了利用生成式AI进行数据清洗与问答生成的流程，以及针对可供性学习的两阶段渐进式微调策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估主要在提出的RoboAfford-Eval基准上进行，该基准包含338个人工精细标注的样本，同样分为物体可供性识别（114问）、物体可供性预测（124问）和空间可供性定位（100问）三个任务。评估指标为准确率（Acc），即预测点落在真实掩码内的比例。此外，还在真实机器人平台上进行了操纵和导航任务的成功率（SR）评估。对比的基线模型包括闭源VLMs（GPT-4o, Claude-3.5-Sonnet, Gemini系列）和开源VLMs（Qwen2.5-VL, LLaVA-Next, Molmo等），以及具有空间感知能力的专门模型（SpaceMantis, RoboPoint, RoboAfford-Qwen）。</p>
<p><img src="https://arxiv.org/html/2511.12436v1/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：RoboAfford-Qwen++的定性结果，其中青色点表示物体和空间可供性。该图直观展示了模型在不同场景、视角和任务中识别物体部件、对齐空间语义的能力。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>基准测试对比</strong>：如表III所示，通用VLMs在零样本设置下表现有限（最佳闭源模型Gemini-2.5-Flash平均Acc为23.5%）。专门模型RoboPoint表现更好（平均Acc 44.7%）。本文的RoboAfford-Qwen++显著优于所有基线，在三个任务上分别达到70.5%、63.1%和55.8%的准确率，平均Acc为63.4%。相较于其基础模型Qwen2.5-VL-7B（平均Acc 16.1%），平均提升了47.3个百分点；相较于最强的专门基线RoboPoint，平均提升了18.7个百分点，证明了RoboAfford++数据集的有效性。</li>
<li><strong>消融实验分析</strong>：虽然没有独立的消融实验表，但两阶段训练设计本身体现了不同数据组件的贡献。第一阶段使用识别数据打下基础，第二阶段加入预测和定位数据实现能力增强，最终模型性能的显著提升验证了这种分层数据整合策略的有效性。</li>
<li><strong>真实机器人操纵</strong>：如图5和表IV所示，在包括桌面和货架场景的7个子任务中，RoboAfford-Qwen++取得了61.4%的平均成功率（SR），远超GPT-4o（11.4%）、Qwen2.5-VL-7B（5.7%）和RoboPoint（35.7%），在抓取苹果、杯子，以及向碗、盘子中放置等任务上成功率较高。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.12436v1/x5.png" alt="机器人操纵结果"></p>
<blockquote>
<p><strong>图5</strong>：RoboAfford-Qwen++模型部署到下游机器人操纵任务的结果。展示了模型在复杂指令下（如区分真实与玩具螺丝刀、定位抽屉内空间）进行物体与空间可供性推理的能力。</p>
</blockquote>
<ol start="4">
<li><strong>真实机器人导航</strong>：如图6和表V所示，在家庭场景导航任务中，RoboAfford-Qwen++达到了70.0%的平均SR，显著高于GPT-4o（22.9%）和RoboPoint（34.3%），在定位水果、咖啡机等任务上表现尤为突出。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.12436v1/x6.png" alt="机器人导航结果"></p>
<blockquote>
<p><strong>图6</strong>：RoboAfford-Qwen++模型部署到下游机器人导航任务的结果。展示了模型根据高层指令（如“找到左边的植物”）进行目标定位和空间关系理解的能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了RoboAfford++，一个首个统一物体与空间可供性的大规模生成式AI增强数据集，包含87万图像和200万QA对，填补了细粒度交互标注的空白；2) 建立了RoboAfford-Eval基准，为系统评估可供性感知预测提供了标准；3) 通过大量实验证明，在RoboAfford++上微调能显著提升VLMs的可供性推理能力，并在真实机器人操纵与导航任务中取得卓越性能。</p>
<p>论文自身提到的局限性包括：数据主要来源于合成环境和网络图像，与复杂真实世界间可能存在领域差距；数据标注以点形式为主，相比像素级分割掩码，在表征物体形状和边界方面粒度较粗。</p>
<p>本文工作对后续研究具有重要启示：首先，它证明了为VLMs提供统一、细粒度的物体-空间可供性监督数据是提升其物理交互理解能力的有效途径。其次，展示了生成式AI在自动化数据清洗、扩充和标注生成方面的巨大潜力，为构建大规模机器人数据集提供了新思路。最后，RoboAfford++数据集和基准的开源将极大地促进机器人多模态可供性学习领域的发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人在操作与导航中因缺乏细粒度可供性注释，难以推断可操作交互位置（如抓取点、放置区）的问题，提出了RoboAfford++数据集。该数据集利用生成式AI增强，包含86.9万张图像和200万问答注释，覆盖对象可供性识别、预测及空间可供性定位三个关键任务，并配套RoboAfford-Eval评估基准。实验表明，现有视觉语言模型在可供性学习上存在缺陷，但基于RoboAfford++微调后，其推理能力显著提升，验证了数据集的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.12436" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>