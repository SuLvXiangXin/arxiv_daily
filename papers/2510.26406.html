<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Human-in-the-loop Online Rejection Sampling for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Human-in-the-loop Online Rejection Sampling for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.26406" target="_blank" rel="noreferrer">2510.26406</a></span>
        <span>作者: Yansong Tang Team</span>
        <span>日期: 2025-10-30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为机器人操作的主流方法，但其大规模预训练的特性使得在真实世界部署前需要进行后训练。后训练通常采用模仿学习或强化学习。模仿学习作为一种纯离线方法，容易因复合误差而在执行时遭遇灾难性失败。强化学习虽然通过在线探索能产生鲁棒策略，但在真实世界训练VLA模型时极不稳定。这种不稳定性主要源于两个关键痛点：1) <strong>不准确的价值估计</strong>：强化学习依赖神经网络近似动作价值函数，在高维动作空间中容易高估；2) <strong>稀疏的监督信号</strong>：强化学习通常只监督最终动作，而现代VLA模型（如基于流匹配或扩散的策略）依赖于迭代去噪等中间计算步骤，这导致了低效的学习信号。</p>
<p>本文针对上述强化学习后训练VLA不稳定的核心问题，提出了一个新视角：借鉴大型语言模型后训练中的拒绝采样思想，将其应用于机器人操作的在线后训练。本文的核心思路是：<strong>利用基于结果的拒绝采样来替代不稳定的价值函数估计，并通过奖励加权的监督学习目标为VLA的中间推理步骤提供密集监督，同时无缝融入人机协作干预以指导错误恢复行为的学习。</strong></p>
<h2 id="方法详解">方法详解</h2>
<p>Hi-ORS方法的整体目标是通过一个两阶段的“评估-改进”框架，实现稳定高效的在线后训练。评估阶段在线生成轨迹并根据奖励进行过滤，改进阶段则利用被接受的轨迹以监督学习的方式更新策略。</p>
<p><img src="https://arxiv.org/html/2510.26406v1/x1.png" alt="方法对比"></p>
<blockquote>
<p><strong>图1</strong>：Hi-ORS方法示意图。它通过基于结果的拒绝采样（替代不准确的价值网络）和奖励加权的监督训练目标（为基于流匹配的VLA提供中间步骤的密集监督），来稳定真实世界的强化学习，并融入人机协作干预。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.26406v1/x2.png" alt="整体流程"></p>
<blockquote>
<p><strong>图2</strong>：Hi-ORS的整体流程。包含拒绝采样框架、监督训练目标、变频率策略和异步基础设施。该方法使VLA的后训练兼具稳定性和高鲁棒性。图中以基于流匹配的策略π0为例。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>评估阶段（轨迹生成与过滤）</strong>：从当前策略πθ（或探索策略）在线生成轨迹τ，并计算其累积奖励R(τ)。定义一个基于阈值的指示函数ℐₘ(τ)=𝟙_{R(τ)≥m}作为接受准则。仅当轨迹奖励达到或超过动态增长的阈值m时，该轨迹才会被保留用于后续训练。这实质上是基于任务结果的拒绝采样，避免了学习高方差价值函数带来的不稳定性。</li>
<li><strong>改进阶段（奖励加权监督学习）</strong>：使用被接受轨迹中的状态-动作对(s_t, a_t)来更新策略。对于基于流匹配的VLA，损失函数设计为同时解决前述两个不稳定源（I1和I2）：<br>ℒ^Hi-ORS(θ) = 𝔼 [ ℐₘ(τ) * ‖ v_θ(u, s_t, x^u) – (x¹ – x⁰) ‖₂² ]<br>其中，<strong>ℐₘ(τ)项（对应I1）</strong> 实现了稳定的价值估计（通过过滤），<strong>流匹配损失项（对应I2）</strong> 则为所有中间去噪时间步u提供了密集的监督信号，优化了产生动作的整个向量场。</li>
<li><strong>人机协作的变频率策略</strong>：允许人类操作员在自主执行过程中的任意时刻进行干预（如遥操作纠正）。关键设计是：a) 仅保留那些最终获得正奖励的干预片段；b) 采用变频率记录：在人类干预期间采用高频(f^high)记录以捕捉精细纠正行为，在自主控制期间采用低频(f^low)记录以保证策略执行的连贯性。这为学习从失败边缘恢复的行为提供了明确的反事实示范。</li>
<li><strong>异步训练基础设施</strong>：采用异步的“执行器-学习器”架构。一个GPU专用于在线推理（执行器），其余GPU用于模型训练（学习器）。这种设计将数据收集与模型更新解耦，提高了训练吞吐量（约2倍），并允许在机器人暂停时继续学习。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ul>
<li><strong>价值估计方式</strong>：摒弃了强化学习中易出错的神经网络价值函数，采用基于实际结果（奖励）的拒绝采样进行过滤，从根本上规避了高维动作空间下的价值高估问题。</li>
<li><strong>监督信号的密度</strong>：将强化学习中稀疏的最终动作监督，转变为对VLA模型整个生成过程（如流匹配的所有去噪步）的密集监督，更适配现代VLA的架构特性。</li>
<li><strong>人机协作的整合</strong>：将人类干预自然地融入在线拒绝采样框架，干预数据需通过奖励过滤才能用于训练，确保了数据质量，并专注于学习有效的错误恢复策略。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>任务与机器人平台</strong>：在三个真实世界操作任务上评估，涉及两种机器人实体：1) <strong>Raise-Hand</strong>：Paxini Tora One机器人将左臂举至目标姿态；2) <strong>Pack-Detergent</strong>：Paxini Tora One机器人从传送带上拾取洗衣液并放入纸箱；3) <strong>Insert-Moisturizer</strong>：Dobot X-Trainer机器人手臂拾取细长的保湿霜并插入底座。</li>
<li><strong>基线方法</strong>：对比了<strong>行为克隆</strong>、结合人机协作的经典真实世界RL方法<strong>HIL-SERL</strong>、以及为动作分块设计的RL方法<strong>Q-Chunking</strong>。所有方法均基于相同的流匹配基础VLA模型π0进行后训练。</li>
<li><strong>评估指标</strong>：在随机重置的环境中进行10次试验，报告成功率。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.26406v1/x4.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图4</strong>：三个真实世界任务中不同方法的评估成功率曲线。Hi-ORS在所有任务上均一致优于基线，收敛更快且最终成功率更高。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>总体性能对比</strong>：如图4所示，Hi-ORS在三个任务上均显著超越所有基线。在相对简单的Raise-Hand任务上，Hi-ORS达到了与HIL-SERL相当的最佳性能，但避免了HIL-SERL后期出现的振荡性回归。在更复杂的Pack-Detergent和Insert-Moisturizer任务上，Hi-ORS达到了更高的渐近成功率，且需要更少的交互次数达到目标性能水平。<strong>与纯离线的行为克隆相比，Hi-ORS平均有23.3%的大幅提升。</strong></li>
<li><strong>测试时扩展性</strong>：如图5所示，Hi-ORS训练出的策略展现出明显的测试时扩展能力。在评估时允许更多的重试次数（更大计算预算），成功率单调提升，表明策略能有效利用额外尝试从中间错误中恢复。而行为克隆策略则几乎没有扩展效应。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.26406v1/x5.png" alt="测试时扩展"></p>
<blockquote>
<p><strong>图5</strong>：在Insert-Moisturizer任务中，Hi-ORS和BC在不同试验预算下的测试性能。Hi-ORS表现出明显的测试时扩展性。</p>
</blockquote>
<ol start="3">
<li><strong>空间泛化能力</strong>：通过课程数据收集策略，Hi-ORS展现出强大的空间泛化能力。如图6所示，即使在物体初始位置远离机器人重置原点的极端测试情况下，经过训练的Hi-ORS策略仍能成功完成任务。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.26406v1/x6.png" alt="空间泛化"></p>
<blockquote>
<p><strong>图6</strong>：Hi-ORS在四种极端空间配置下的测试成功案例，验证了其良好的空间泛化能力。</p>
</blockquote>
<ol start="4">
<li><strong>错误恢复行为</strong>：如图7所示，人机协作干预使策略能快速掌握错误恢复。Hi-ORS策略在测试时能自主展现出复杂的错误恢复行为，例如在插入保湿霜任务中，当首次插入尝试失败后，策略能自主执行“重新对准-再次插入”的恢复序列。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.26406v1/x7.png" alt="错误恢复"></p>
<blockquote>
<p><strong>图7</strong>：Hi-ORS学习到的错误恢复行为示例。展示了人机协作如何加速掌握恢复能力，以及训练后的策略在测试时自主执行的复杂恢复序列。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>论文通过实验分析指出，性能提升主要归功于：1) <strong>基于结果的拒绝采样</strong>，它解决了价值估计不准确的问题；2) <strong>奖励加权的密集监督</strong>，它充分利用了VLA的中间计算结构；3) <strong>人机协作干预</strong>，它提供了关键的误差恢复示范。三者结合共同实现了稳定、高效的后训练。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出Hi-ORS方法</strong>：首次将拒绝采样思想系统性地应用于机器人VLA的在线后训练，通过基于结果的过滤替代价值函数，并结合奖励加权的密集监督，解决了真实世界RL训练不稳定的核心难题。</li>
<li><strong>实现高效的人机协作学习</strong>：设计了一种能自然融合人类干预的框架，干预数据需通过奖励过滤，从而高效地引导策略学习复杂的错误恢复行为，并展现出显著的测试时扩展能力。</li>
<li><strong>全面的真实世界验证</strong>：在三个具有挑战性的接触式操作任务和两种机器人平台上进行了系统验证，证明了Hi-ORS在效果和效率上大幅优于现有的IL和RL基线方法，仅需1.5小时的真实世界训练即可掌握任务。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，Hi-ORS在训练过程中需要对每个轨迹进行完整的正向推理以计算奖励，这可能带来一定的计算开销。此外，拒绝阈值m的调度策略需要根据任务进行设计。</p>
<p><strong>对后续研究的启示</strong>：<br>Hi-ORS展示了一条不依赖精确价值函数也能实现稳定在线策略改进的路径。其将LLM后训练技术成功迁移到机器人领域的思路，以及对人机协作数据的高效利用方式，为未来大规模机器人策略的在线适应与对齐提供了新的借鉴。如何将自动奖励模型或更复杂的过滤机制与拒绝采样结合，以进一步减少对人类标注或干预的依赖，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出人机交互在线拒绝采样方法，以解决视觉-语言-动作模型在机器人操作后训练中强化学习不稳定、模仿学习泛化差的问题。通过在线过滤负奖励样本稳定价值估计，结合奖励加权监督目标提供密集中间步骤监督，并构建异步推理-训练框架支持实时人工纠错。实验表明，该方法仅用1.5小时真实训练即能掌握接触式操作任务，在效果与效率上显著超越基线方法，且微调后的策略展现出优秀的错误恢复能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.26406" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>