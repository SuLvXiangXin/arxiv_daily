<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Robust Instant Policy: Leveraging Student’s t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Robust Instant Policy: Leveraging Student’s t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.15157" target="_blank" rel="noreferrer">2506.15157</a></span>
        <span>作者: Yukiyasu Domae Team</span>
        <span>日期: 2025-06-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习（IL）旨在让机器人通过观察人类示教自主执行任务。当前主流方法依赖于在大规模数据集上训练高表达能力模型（如基于能量的模型、扩散模型、Transformer），但这些方法需要数千条示教数据和/或长期的模型权重调优才能适应新任务或环境。为此，上下文模仿学习（ICIL）范式被提出，其核心是使用一个即时策略（Instant Policy），仅通过提供少量新任务的示教作为上下文，而无需更新模型参数，即可泛化到新任务。其中，关键点动作标记（KAT）方法通过将机器人轨迹数据转换为基于关键点的文本，成功复用了现成的大型语言模型（LLM）作为即时策略，仅用不到10条人类示教就达到了与先进IL方法相当的性能。</p>
<p>然而，LLM固有的“幻觉”问题——即其生成的内容可能偏离给定上下文且缺乏准确性——严重影响了LLM作为即时策略在机器人领域的可靠性。机器人轨迹生成中的微小误差都可能导致任务失败。虽然已有方法尝试缓解LLM的幻觉，但它们主要针对离散的语言数据，难以直接应用于连续的机器人轨迹数据。因此，本文针对LLM-based即时策略在生成连续机器人轨迹时产生的幻觉问题，提出了一种新的鲁棒ICIL算法。其核心思路是：多次查询LLM-based即时策略以生成多条候选轨迹，然后利用对异常值（即幻觉轨迹）不敏感的Student&#39;s t-回归模型对这些轨迹进行聚合，从而生成一条鲁棒的轨迹。</p>
<h2 id="方法详解">方法详解</h2>
<p>提出的鲁棒即时策略（RIP）整体流程如下：首先，沿用KAT方法，将少量人类示教（RGB-D图像观测和机器人末端执行器动作序列）转换为基于3D关键点的文本上下文。然后，对于新的观测，RIP会多次（Q次）查询LLM-based即时策略（Φ_KAT），生成一组候选动作轨迹。最后，利用Student&#39;s t-回归模型对这组可能包含幻觉（异常值）的轨迹进行拟合，取其均值作为最终输出的鲁棒轨迹。</p>
<p><img src="https://arxiv.org/html/2506.15157v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：RIP在香蕉采摘任务中的概览。从人类示教中收集输入图像和输出轨迹。从示教数据集中，提取基于语义和几何相似性的视觉3D关键点作为观测，以及描述夹爪姿态的3D点三元组作为动作，共同构成上下文文本数据。LLM-based即时策略接收上下文数据和新图像的关键点，多次采样生成动作轨迹。使用Student&#39;s t-回归模型从采样的动作轨迹集合中捕获一条鲁棒轨迹，机器人遵循该可靠轨迹执行任务。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>数据表示与KAT接口</strong>：为了适配LLM，需要将连续的机器人数据转换为离散的文本标记。</p>
<ul>
<li><strong>观测表示</strong>：使用视觉Transformer模型DINO提取RGB-D图像的描述符，通过跨图像最近邻搜索找到K个最具语义/几何相似性的描述符，并将其对应的2D关键点利用深度信息和相机内参投影为3D视觉关键点集V。</li>
<li><strong>动作表示</strong>：机器人末端执行器的姿态由一个3D点三元组τ = [p0, p1, p2]定义，分别代表张开状态下夹爪和两个指尖的xyz位置。夹爪状态由变量g表示（0为开，1为关）。因此，单个动作定义为 a = [τ, g] ∈ ℝ^10。一条轨迹是动作序列 ξ = {a_t}_{t=1}^T。</li>
<li><strong>LLM推理</strong>：在部署阶段，将上下文[𝒱, 𝒜]（𝒱是所有示教的视觉关键点集，𝒜是所有示教的动作轨迹集）和新的视觉3D关键点V′作为文本标记输入LLM，LLM输出预测的新轨迹ξ̂。</li>
</ul>
</li>
<li><p><strong>鲁棒轨迹聚合（RIP核心）</strong>：</p>
<ul>
<li><strong>多次采样</strong>：使用相同的文本标记（上下文[𝒱, 𝒜]和新观测V′）查询即时策略Φ_KAT共Q次，得到一组动作轨迹集合𝒜′ = {ξ̂^q}_{q=1}^Q。</li>
<li><strong>Student‘s t-回归模型</strong>：定义一个动作估计器𝒮_θ(â_t|t; ν)，对于给定时间步t，输出一个自由度为ν的Student&#39;s t分布。该分布由均值网络μ_θ(t)和方差网络σ_θ²(t)参数化（θ为网络参数）。其概率密度函数如论文公式(1)所示。自由度ν是一个超参数，用于调节对异常值的敏感度；ν越大，分布越接近高斯分布。</li>
<li><strong>训练目标</strong>：通过最小化轨迹集𝒜′的负对数似然损失（公式(2)）来训练估计器参数θ（公式(3)）。由于Student&#39;s t分布具有重尾特性，在拟合包含异常值的数据时，相比高斯分布，其对异常值的惩罚更小，因此更鲁棒。</li>
<li><strong>轨迹生成</strong>：训练得到最优参数θ̂后，鲁棒轨迹即为估计器均值网络在每个时间步的输出：ξ̂<em>RIP = {μ_θ̂(t)}</em>{t=1}^T（公式(4)）。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：本文的核心创新在于将缓解LLM幻觉的“采样-聚合”思想从离散文本领域成功扩展到了连续机器人轨迹领域，并创新性地采用了Student&#39;s t-回归模型作为聚合器。与直接取平均或使用高斯回归相比，t-回归模型能更有效地抑制幻觉轨迹（异常值）的影响，从而从一组质量不一的候选轨迹中提炼出可靠的一致轨迹。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在模拟环境（使用SAPIEN模拟器）和真实世界环境（使用Franka Emika Panda机器人）中进行评估。</li>
<li><strong>任务</strong>：涵盖日常物品操作任务，如拾放香蕉、马克杯、饼干盒等。</li>
<li><strong>实验平台</strong>：使用GPT-4作为基础LLM。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>行为克隆（BC）</strong>：标准IL方法。</li>
<li><strong>决策Transformer（DT）</strong>：先进的序列建模IL方法。</li>
<li><strong>KAT</strong>：本文方法所基于的LLM-based即时策略（即Φ_KAT）。</li>
<li><strong>KAT + 高斯回归（KGR）</strong>：将RIP中的Student&#39;s t-回归替换为标准高斯回归的变体，用于消融研究。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2506.15157v1/x3.png" alt="环境和任务"></p>
<blockquote>
<p><strong>图3</strong>：模拟和真实实验环境与任务设置。使用主从式机器人系统收集人类示教。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.15157v1/x4.png" alt="模拟环境成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：模拟环境中，不同示教数量（1， 3， 5条）下各方法的平均任务成功率。RIP在所有低数据场景下均显著优于其他方法，尤其在仅有1条示教时，相比次优方法（KAT）有至少26%的绝对提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.15157v1/x5.png" alt="真实环境成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人实验中，使用5条示教时各方法的任务成功率。RIP取得了最高的平均成功率（93.3%），显著优于BC、DT和基础的KAT方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.15157v1/x6.png" alt="消融实验：聚合方法对比"></p>
<blockquote>
<p><strong>图6</strong>：消融实验：比较不同轨迹聚合方法（简单平均、高斯回归-KGR、Student&#39;s t回归-RIP）在模拟环境中的成功率。RIP（t-回归）性能最佳，验证了其对幻觉（异常值）的鲁棒性优于平均和高斯回归。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.15157v1/x7.png" alt="消融实验：查询次数Q的影响"></p>
<blockquote>
<p><strong>图7</strong>：消融实验：查询次数Q对RIP性能的影响。随着Q增加，成功率提升并逐渐饱和。Q=10时达到较好性能与计算开销的平衡。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.15157v1/x8.png" alt="消融实验：自由度ν的影响"></p>
<blockquote>
<p><strong>图8</strong>：消融实验：Student&#39;s t-回归中自由度ν对性能的影响。ν=3时性能最优，过大（接近高斯）或过小都会降低性能，说明需要合适的重尾程度来处理异常值。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.15157v1/x9.png" alt="定性结果：轨迹可视化"></p>
<blockquote>
<p><strong>图9</strong>：定性结果可视化。在“移动马克杯”任务中，基础KAT方法（红色）因幻觉产生了一条偏离示教（绿色）的异常轨迹，而RIP（蓝色）成功生成了一条与示教一致且平滑的鲁棒轨迹。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>性能对比</strong>：在模拟和真实实验中，RIP在低数据场景（1-5条示教）下的任务成功率全面超越BC、DT和KAT等基线方法。特别是在仅有1条示教时，RIP相比KAT有26%以上的成功率提升。</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>聚合方法</strong>：Student&#39;s t-回归（RIP）优于简单平均和高斯回归（KGR），证明了其处理异常值的有效性。</li>
<li><strong>查询次数Q</strong>：性能随Q增加而提升，在Q=10后趋于饱和。</li>
<li><strong>自由度ν</strong>：ν=3是一个较优的选择，验证了适度的重尾分布对过滤机器人轨迹异常值的重要性。</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出RIP框架</strong>：首次为LLM-based即时策略设计了一种针对连续机器人轨迹数据的、鲁棒的“采样-聚合”框架，有效缓解了幻觉问题。</li>
<li><strong>引入Student&#39;s t-回归</strong>：创新地将Student&#39;s t-回归模型用于聚合多条候选机器人轨迹，其重尾特性使其能天然抵抗异常值（幻觉轨迹），从而生成可靠轨迹。</li>
<li><strong>实证验证</strong>：在模拟和真实机器人实验中系统验证了RIP的有效性，尤其在低数据场景下显著提升了任务成功率，证明了其应用于即时策略的可行性和优势。</li>
</ol>
<p><strong>局限性</strong>（论文自身提及）：</p>
<ol>
<li><strong>计算开销</strong>：需要多次（Q次）查询LLM，虽然查询可并行进行，但仍比单次查询的KAT方法开销大。</li>
<li><strong>超参数选择</strong>：自由度ν和查询次数Q需要根据任务进行调整以获得最佳性能。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>鲁棒性设计范式</strong>：RIP展示了将处理离散数据不确定性的方法（如采样聚合）适配到连续控制领域的有效路径，为提升学习型策略（尤其是基于生成模型的策略）的鲁棒性提供了新思路。</li>
<li><strong>异常值处理</strong>：在机器人学习与规划中，如何从包含噪声或错误的数据中提取可靠信号是一个普遍问题。本文验证了基于特定概率分布（如Student&#39;s t）的鲁棒回归在此类问题中的潜力。</li>
<li><strong>即时策略的优化</strong>：工作表明，即使底层基础策略（LLM）存在缺陷，通过上层设计精巧的后处理或聚合机制，也能显著提升整体系统的性能和可靠性。这启发研究者可以分别优化“策略生成”和“策略精炼”两个模块。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对基于大语言模型的上下文模仿学习（ICIL）中，即时策略因“幻觉”产生异常轨迹、导致可靠性下降的问题，提出了一种稳健即时策略（RIP）。其关键技术是采用Student’s t回归模型，对模型生成的多个候选轨迹进行聚合，该模型能有效忽略异常值（即幻觉轨迹），从而生成稳健的机器人轨迹。实验表明，该方法在模拟和真实环境中均显著优于现有方法，在低数据场景的日常任务中，任务成功率至少提升26%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.15157" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>