<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Interactive Shaping of Granular Media Using Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Interactive Shaping of Granular Media Using Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.06469" target="_blank" rel="noreferrer">2509.06469</a></span>
        <span>作者: Kreis, Benedikt, Mosbach, Malte, Ripke, Anny, Ullah, Muhammad Ehsan, Behnke, Sven, Bennewitz, Maren</span>
        <span>日期: 2025/09/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人领域，自主操纵颗粒介质（如沙子）对于建筑、挖掘和增材制造等应用至关重要。然而，由于颗粒介质具有高维配置空间和复杂的动力学特性，传统的基于规则的方法需要大量的工程努力才能应对。强化学习（RL）通过试错学习自适应操纵策略，提供了一种有前景的替代方案。目前，针对颗粒介质操纵的研究方法多样，包括基于物理模拟（如离散元方法DEM）的建模、基于模仿学习或离线RL的方法，以及基于覆盖路径规划（CPP）的基线方法。这些方法存在计算成本高、依赖于次优演示数据或缺乏对材料变形的自适应能力等局限性。</p>
<p>本文针对应用RL到颗粒介质交互式操纵中的两个关键挑战：1) 为高维颗粒介质寻找紧凑的观察空间；2) 设计有效的奖励函数。传统的距离奖励在颗粒介质任务中非常稀疏，因为大多数随机操纵动作会导致配置远离目标，从而抑制智能体与介质交互。本文提出了一种新的RL框架，核心思路是：将高维深度图像转换为紧凑的高度图表示作为观察，并设计一种新颖的奖励函数，该函数结合了直接鼓励形状匹配和引导末端执行器（EE）向目标区域移动的组件，以提供密集且平衡的学习信号。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的RL框架旨在学习操纵机器人末端执行器在颗粒介质中运动以创建多样化目标形状的策略。整体流程如下：从立体相机获取当前深度图像，重建为当前高度图；计算其与目标高度图之间的差异；将此差异图与代表末端执行器位置和目标区域的布尔掩码一起输入到一个高度图编码器中；编码后的特征与机器人的本体感知（当前和上一时刻的EE位置）拼接，形成最终的观察向量；该观察向量被输入到RL策略网络中，输出EE在三个笛卡尔方向上的连续动作增量（Δx, Δy, Δz）。</p>
<p><img src="https://arxiv.org/html/2509.06469v2/figures/architecture/architecture.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：方法整体框架。通过强化学习训练视觉策略，利用当前重建高度图与目标高度图之间的差异。当前高度图从深度图像重建。高度图差异被输入到高度图编码器中，并与机器人的本体感知拼接。产生的策略控制末端执行器塑造颗粒介质。</p>
</blockquote>
<p><strong>核心模块1：高度图编码器</strong>。该模块的作用是将高维的高度图差异和空间掩码信息压缩为紧凑的特征向量。具体而言，输入包括差异高度图 H_d、EE掩码 M_EE 和目标掩码 M_g，所有输入尺寸均为32x32。首先，将 H_d 与 M_EE 堆叠。然后，分别使用卷积神经网络（CNN）从该堆叠观察和 M_g 中提取大小为64的特征向量。随后，通过将堆叠特征与目标掩码的sigmoid函数相乘（类似于门控机制）来放大目标区域内的特征。最终得到一个64维的特征向量，与EE的6维位置信息（当前和先前位置）拼接，形成总计70维的观察向量。</p>
<p><strong>核心模块2：奖励函数</strong>。奖励函数由两部分组成：颗粒介质塑造奖励 r_s 和目标区域移动奖励 r_m，总奖励 r = r_m + r_s。</p>
<ul>
<li><strong>目标区域移动奖励 r_m</strong>：用于引导EE朝向并停留在需要操纵的区域。它计算EE到目标区域的最小欧氏距离 d_m（如图3所示），奖励公式为 r_m = -tanh(α_m ⋅ d_m) + 𝟙_reached，其中当EE进入目标区域时，指示函数 𝟙_reached 返回1。该奖励提供了密集的引导信号。<br><img src="https://arxiv.org/html/2509.06469v2/x1.png" alt="奖励距离示意图"><blockquote>
<p><strong>图3</strong>：使用末端执行器位置 p_EE 到属于目标区域的最远点 p_g 的距离 d_m，通过密集奖励信号引导智能体朝向需要操纵的区域。</p>
</blockquote>
</li>
<li><strong>颗粒介质塑造奖励 r_s</strong>：本文提出了两种变体。第一种是<strong>Delta奖励</strong>：r_delta = α_c ⋅ (d̂_{t-1} - d̂_t)，直接奖励当前时间步与上一时间步之间平均绝对高度差异 d̂ 的减少。第二种是<strong>Progressive奖励</strong>：r_prog = α_c ⋅ max(d̂_closest - d̂, 0) - α_f ⋅ min(d̂^o - d̂^o_furthest, 0)，它奖励智能体达到比当前episode内已到达的最接近目标配置更接近的配置，并惩罚其达到比已到达的最远配置更远的配置（考虑目标区域外的变化 d̂^o）。这种方法避免了因中间步骤可能导致距离暂时增加而受到惩罚，从而鼓励更长期的探索。</li>
</ul>
<p><strong>其他技术细节</strong>：使用的RL算法是截断分位数评论家（TQC），动作空间为连续的EE增量，观察空间如表I所示。仿真基于Kim等人的高效高度图沙模型，该模型模拟了沙堆的坍塌行为。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的创新具体体现在：1) 提出了专门针对颗粒介质塑造任务设计的新型奖励函数（Delta和Progressive），有效解决了奖励稀疏问题；2) 设计了高度图编码器，将视觉深度信息、EE位置和目标区域信息高效地融合到一个紧凑的观察表示中；3) 策略学习是端到端且在线的，不依赖于次优的演示数据，并且允许EE自由进行任意笛卡尔方向运动，而非限制于预定义的笔划。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在模拟环境和真实机器人（UR5e机械臂，立方体末端执行器，ZED 2i相机）上进行。使用了400个不同的目标高度图进行评估，包括矩形、L形、多边形和考古壁画碎片负形四种类型。目标区域最大为10x10 cm，深度达3 cm。</p>
<p><strong>基线方法</strong>：1) <strong>随机基线（RAND）</strong>：EE随机放置在目标区域表面，然后执行随机动作。2) <strong>Boustrophedon覆盖路径规划基线（B-CPP）</strong>：基于目标掩码生成覆盖路径，EE按顺序遍历路径上的点。</p>
<p><strong>关键实验结果</strong>：主要定量结果如表III所示。本文提出的两种方法（DELTA, PROG）与基线进行了比较，并测试了无掩码（NO-M）和特权观察（H^P）等情况。<br><img src="https://arxiv.org/html/2509.06469v2/x31.png" alt="定量评估结果表"></p>
<blockquote>
<p><strong>表III</strong>：定量评估结果。展示了平均高度差（越低越好）、目标区域单元格改变百分比（越高越好）和执行步骤数（越低越好）。DELTA方法在高度差和步骤数上表现最佳，且具有统计显著性（⋆⋆⋆）。</p>
</blockquote>
<ul>
<li><strong>性能对比</strong>：在标准视觉观察下，使用Delta奖励的DELTA方法取得了最佳的平均高度差（3.4 mm）和最少的执行步骤（23.5步），显著优于RAND和B-CPP基线。B-CPP虽然改变了100%的单元格，但高度差（4.8 mm）和步骤数（44.0步）均差于DELTA。</li>
<li><strong>奖励函数对比</strong>：PROG奖励在改变单元格百分比上略优于DELTA，但执行步骤更多（34.9步 vs 23.5步），且高度差更大（4.5 mm vs 3.4 mm）。这表明Delta奖励在整体性能上更优。</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>奖励组件</strong>：移除目标区域移动奖励 r_m 的NO-M版本性能急剧下降（高度差6.0 mm，改变单元格仅3.4%），证明了该引导奖励的重要性。</li>
<li><strong>观察空间</strong>：使用特权观察（直接访问真实高度图，H^P）的版本性能略有提升，但差距不大，证明了视觉重建观察的有效性。</li>
<li><strong>RL算法</strong>：文中还测试了SAC和TD3，但TQC显示了最好的训练收敛性。</li>
</ul>
</li>
<li><strong>定性结果与零样本迁移</strong>：<br><img src="https://arxiv.org/html/2509.06469v2/figures/goal_examples/rectangle/robot_100_52_t39_after_zoomed.jpg" alt="目标形状示例"><blockquote>
<p><strong>图4</strong>：从左至右：仿真中重建的3D场景、操纵后重建的高度图、智能体旨在实现的目标高度图。从上至下：矩形、L形、多边形和壁画碎片负形示例。<br><img src="https://arxiv.org/html/2509.06469v2/x2.png" alt="模拟与真实世界对比"><br><strong>图5</strong>：目标条件RL智能体在模拟环境（上）和真实机器人系统（下）中对颗粒介质的操纵。智能体成功在模拟和真实世界中塑造出目标矩形形状，实现了零样本迁移。</p>
</blockquote>
</li>
</ul>
<p>图5直观展示了训练好的策略在模拟和真实环境中成功塑造目标形状的能力。尽管真实世界的初始表面不平整且深度观测存在噪声，策略仍能有效工作，证明了方法的鲁棒性和零样本迁移的可行性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>新颖的奖励设计</strong>：提出了针对颗粒介质塑造任务的奖励函数，结合了直接形状匹配奖励（Delta/Progressive）和引导EE移动的密集奖励，有效解决了高维空间中的奖励稀疏问题，促进了RL训练的快速稳定收敛。</li>
<li><strong>紧凑的视觉观察表示</strong>：通过将深度图像转换为高度图，并设计高度图编码器融合空间掩码信息，实现了对高维颗粒介质状态的有效、紧凑编码，使从视觉观察中学习策略成为可能。</li>
<li><strong>有效的零样本迁移</strong>：证明了在仿真中训练的视觉策略可以直接（零样本）迁移到真实的机器人系统，处理真实的传感器噪声和不完美的初始条件，展现了方法的实用潜力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前工作专注于塑造凹陷形状（加深初始平面），未处理凸起形状（堆积）。此外，仿真模型虽然高效，但与真实世界动力学之间仍存在差距。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li>奖励函数的设计对于解决复杂连续体操纵任务至关重要，本文的奖励塑造思路（结合任务进度奖励和密集引导奖励）可借鉴于其他可变形物体操作领域。</li>
<li>高度图是一种高效表示表面形态变化的方法，在需要表面建模的机器人任务中具有广泛应用前景。</li>
<li>在高质量仿真中训练并结合有效的状态表示，能够实现向真实世界的零样本或少量样本迁移，这为降低机器人学习对真实数据收集的依赖提供了可行路径。未来的工作可以扩展目标形状的复杂性，并探索多步骤或分层策略以完成更大规模的塑造任务。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操控颗粒介质（如沙子）时面临的高维配置空间和复杂动力学挑战，提出了一种强化学习框架。该方法利用紧凑的观察空间和简洁的奖励函数，训练机器人通过立体视觉和立方体末端执行器，以交互式闭环策略塑造目标形状。实验表明，该视觉策略在真实部署中显著优于两种基线方法，实现了更高的目标形状精度。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.06469" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>