<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.02917" target="_blank" rel="noreferrer">2508.02917</a></span>
        <span>作者: Kåsene, Vebjørn Haug, Lison, Pierre</span>
        <span>日期: 2025/08/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉与语言导航（VLN）任务要求智能体根据自然语言指令在未知环境中导航。尽管近期的大视觉语言模型（LVLMs）在此任务中展现出潜力，但当前的VLN系统大多依赖于专门为导航设计和优化的模型，而现成的、未经结构修改的LVLMs的潜力尚未被充分探索。此外，在动作空间的选择上存在演变：早期的VLN方法使用基于自我中心视图和原子动作（如“左转”、“前进”）的低级动作空间，而较新的模型则倾向于使用具有离散可导航视点的全景动作空间。虽然这种性能差异在基于RNN的模型中已被研究，但在基于LVLM的方法中尚未被探讨。</p>
<p>本文旨在解决两个知识空白：1) 评估未经结构修改或基于模拟器训练的现成LVLMs在VLN任务上的有效性；2) 分析动作空间选择（低级 vs. 全景）对导航性能的影响。核心思路是：直接微调开源模型Qwen2.5-VL-3B-Instruct在Room-to-Room（R2R）数据集上，并通过实验比较其在两种动作空间范式下的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法基于对一个预训练的LVLM（Qwen2.5-VL）在R2R数据集上进行微调。整体流程如图1所示：模型接收包含路线指令、导航历史和当前视图的多模态提示作为输入，并输出要执行的下一个导航动作。</p>
<p><img src="https://arxiv.org/html/2508.02917v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。基于在R2R数据集上微调预训练的LVLM（Qwen 2.5-VL）。LVLM接收包含路线指令、导航历史和当前视图的多模态提示作为输入，并输出要执行的下一个导航动作。</p>
</blockquote>
<p><strong>核心模块与动作空间定义</strong>：<br>方法的核心在于为模型定义两种不同的动作空间，并据此构建相应的输入提示。</p>
<ol>
<li><p><strong>低级动作空间</strong>：智能体通过每一步的自我中心图像感知环境。动作集合包括四个离散动作：<code>Move</code>（前进到当前视野中心最接近的节点）、<code>Left</code>/<code>Right</code>（分别旋转30度）、<code>Stop</code>（停止）。为了解决在离散图中<code>Move</code>可能导致非直观侧向移动的问题，在执行每个<code>Move</code>动作前会应用一个“自动转向节点”的重新定向步骤（非学习动作）。历史上下文<code>H_t</code>包含之前步骤的图像和动作对<code>(O, a)</code>。</p>
</li>
<li><p><strong>全景动作空间</strong>：智能体通过每一步的360度全景图像感知环境。任务是从一组可导航的候选视图中进行选择。每个候选视图<code>c_i</code>包含一个图像、一个相对航向角<code>θ_i</code>和一个关联的旅行距离<code>δ_i</code>。模型预测一个对应于候选索引（0到K-1）或<code>Stop</code>动作的token。与将候选视图从全景中提取的传统设置不同，本方法将全景和候选视图视为独立输入以减少每步输入图像数量。历史<code>H_t</code>仅包含之前的全景视图。</p>
</li>
</ol>
<p><strong>提示构建与训练</strong>：<br>两种动作空间的输入都遵循固定的提示模式，如图2所示。提示包含一个解释任务的静态系统提示，以及动态的当前状态信息（指令、视觉输入、步数、已旅行距离等）。</p>
<p><img src="https://arxiv.org/html/2508.02917v1/x2.png" alt="提示模式"></p>
<blockquote>
<p><strong>图2</strong>：低级和全景动作空间的提示模式。</p>
</blockquote>
<p>模型通过<strong>行为克隆</strong>进行微调，即学习模仿专家演示。训练目标是最小化整个episode中专家动作的负对数似然。与许多使用强化学习或学生强化的VLN方法不同，本方法仅基于专家路径进行微调，其关键优势在于训练时不需要访问模拟器。在实现上，冻结了视觉编码器和跨模态投影层，仅微调大型语言模型（LLM）部分。</p>
<p><strong>创新点</strong>：</p>
<ol>
<li>首次系统评估了未经结构修改的现成LVLM在标准VLN任务（R2R）上的性能。</li>
<li>首次在LVLM框架下对比了低级与全景动作空间的效果。</li>
<li>采用了纯行为克隆的简化训练范式，无需训练时模拟或复杂优化策略。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集与平台</strong>：使用Room-to-Room（R2R）数据集和Matterport3D（MP3D）模拟器进行评估。</li>
<li><strong>评估指标</strong>：在线评估使用标准VLN指标，包括导航误差（NE）、成功率（SR）、由路径长度加权的成功率（SPL）等；离线评估使用准确率、宏F1和保守成功率（CSR）。</li>
<li><strong>对比方法</strong>：与一系列经典和SOTA方法对比，包括Seq2Seq、Speaker-Follower（SF）、PRESS、HAMT、DUET、NavGPT、NaviLLM、NavGPT-2等。</li>
<li><strong>模型</strong>：主要使用Qwen2.5-VL-3B-Instruct进行微调，得到<code>Qwen2.5-VL-low</code>（低级）和<code>Qwen2.5-VL-pano</code>（全景）两个模型。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在线评估结果（表2）显示，全景模型<code>Qwen2.5-VL-pano</code>在R2R测试集上取得了41%的成功率（SR）和38%的SPL。这超越了所有低级的基线方法（如原始Seq2Seq的21% SR），也超过了早期的全景方法如Speaker-Follower（val unseen 36% SR）和零样本的NavGPT（测试集34% SR）。然而，它仍显著落后于近期专为VLN设计的SOTA模型，如NaviLLM（60% SPL）和NavGPT-2（72% SR）。</p>
<p>低级模型<code>Qwen2.5-VL-low</code>在测试集上获得26%的SR，优于原始R2R基线，但低于另一个LSTM-based的低级模型DCF（35% SR）。这表明现成LVLM微调后的性能尚未达到早期专用模型的水平。</p>
<p><img src="https://arxiv.org/html/2508.02917v1/x3.png" alt="路径长度与成功率"></p>
<blockquote>
<p><strong>图3</strong>：在R2R val unseen上，模型成功率随路径长度（米）的变化。两种模型在较短路径上表现更好，但低级模型性能下降更明显。</p>
</blockquote>
<p><strong>消融实验与分析</strong>：<br>论文对低级动作空间进行了变体研究（表3）。发现将垂直视野（VFOV）从105度减至82度影响甚微。而<strong>移除“自动转向节点”的调整动作后，性能反而提升</strong>（SR从25%升至29%），这表明明确地在下个节点移动前转向并非必要。</p>
<p><img src="https://arxiv.org/html/2508.02917v1/x4.png" alt="消融实验表"></p>
<blockquote>
<p><strong>表3</strong>：低级动作空间不同定义在R2R val unseen上的在线结果。<code>No-Adjust</code>配置（移除自动转向）取得了最佳成功率。</p>
</blockquote>
<p><strong>全景 vs. 低级的深入分析</strong>：</p>
<ul>
<li>性能差距：全景模型始终优于低级模型，SR差距达15个百分点（41% vs. 26%），延续了先前研究中全景空间的优势趋势。</li>
<li>原因探究：如表4所示，低级动作序列的平均长度是全景序列的两倍（约13步 vs. 6步）。更长的决策序列增加了错误累积和恢复难度，这由图3中两者在长路径上性能均下降，但低级模型下降更剧所佐证。此外，全景模型的保守成功率（CSR）远高于低级模型（15% vs. 3%），表明其更倾向于保持在正确路径上。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.02917v1/x5.png" alt="平均步数表"></p>
<blockquote>
<p><strong>表4</strong>：R2R数据集中，低级和全景变体每个路径的平均步数（动作数）。低级路径的决策步数约为全景的两倍。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>实证评估</strong>：首次系统评估表明，仅通过行为克隆微调现成LVLM（Qwen2.5-VL）可在VLN任务上取得一定效果（最高41% SR），但性能仍显著落后于集成显式空间建模或专用策略网络的SOTA模型。</li>
<li><strong>动作空间比较</strong>：在LVLM框架下验证了全景动作空间相对于低级动作空间的显著性能优势，并分析指出更长的决策序列是导致低级空间表现更差的关键因素之一。</li>
<li><strong>简化训练范式</strong>：展示了无需训练时模拟器或强化学习的纯行为克隆方法在VLN任务上的可行性。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>性能瓶颈：微调后的LVLM导航性能有限，未能超越更早、更小的专用模型。</li>
<li>潜在原因：论文指出可能的原因包括：1) 仅使用行为克隆，而未采用学生强制或强化学习进行优化；2) 模型缺乏对图像间空间关系的显式建模模块，完全依赖LLM进行空间推理和决策，这对长路径可能尤其困难；3) 输入视觉token数量较多，可能影响非OCR任务的性能。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>直接微调大型通用LVLM并非解决VLN任务的“银弹”，<strong>引入针对导航的专用归纳偏置（如显式空间关系建模）至关重要</strong>。</li>
<li>在资源受限或无法获取全景硬件的情况下，低级动作空间仍有研究价值，但需重点解决其因决策步数多而导致的错误传播问题。</li>
<li>探索介于纯行为克隆与复杂在线优化之间的高效训练方法，以提升现成基础模型在具体任务上的性能。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究了现成大型视觉语言模型在视觉与语言导航任务中的有效性，并比较了低级动作空间（以自我为中心视图和原子动作）与全景动作空间（离散可导航视点）两种范式。核心方法是直接微调开源模型Qwen2.5-VL-3B-Instruct，无需修改架构或模拟器训练，并在Room-to-Room数据集上进行评估。实验表明，微调后的模型在R2R测试集上达到了41%的成功率，证明现成LVLM能够学习执行VLN任务，但其性能仍显著落后于专门为导航设计的模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.02917" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>