<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.06729" target="_blank" rel="noreferrer">2505.06729</a></span>
        <span>作者: Zhu, Haokun, Li, Zongtai, Liu, Zhixuan, Wang, Wenshan, Zhang, Ji, Francis, Jonathan, Oh, Jean</span>
        <span>日期: 2025/05/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型因其丰富的先验知识和强大的推理能力，被越来越多地集成到物体导航任务中。然而，将VLM应用于导航面临两个关键挑战：一是如何有效解析和结构化复杂的环境信息；二是如何确定何时以及如何查询VLM。现有方法通常缺乏对环境的连贯全局表示，VLM输入常局限于局部观察，这限制了其有效推理能力。此外，现有方法通常过度依赖VLM，例如在每个步骤中查询VLM以在所有前沿视点中选择，而没有利用导航进度或环境布局来有效引导VLM的推理过程。由于VLM对3D空间信息的理解有限，它们无法在评估每个视点时联合推理空间关系和导航历史，导致其评估主要基于视点的局部语义信息，从而常常引发不必要的回溯或重复探索等冗余导航行为。</p>
<p>本文针对上述痛点，提出了一种新视角：通过增量构建一个结构化的多层环境表示来为VLM提供丰富的上下文，并设计一个两阶段导航策略，将VLM用于高级别的跨房间规划，而在房间内则结合传统探索算法与VLM的辅助决策，从而在利用VLM推理能力的同时，避免每一步都依赖VLM带来的低效问题。本文的核心思路是：在导航过程中增量构建由视点、物体节点和房间节点组成的多层表示，并基于此表示，采用由VLM指导的跨房间高级规划与VLM辅助的房内低级探索相结合的两阶段策略，以实现高效、可靠的物体导航。</p>
<h2 id="方法详解">方法详解</h2>
<p>STRIVE框架的核心在于增量构建一个多层环境表示，并基于此表示执行一个两阶段导航策略。整体流程如图2所示：智能体在环境中移动，实时构建包含物体、视点、房间节点的三层图表示；基于此结构化表示，导航策略分为高层和低层：高层由VLM进行房间级别的推理，选择下一个要探索的最佳房间；低层则在选定的房间内，采用结合了VLM辅助早期停止策略的前沿探索方法进行精细探索，并利用VLM进行目标验证。</p>
<p><img src="https://arxiv.org/html/2505.06729v2/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：STRIVE框架总览。我们实时构建一个多层表示 ℛ（第III-A节），包含物体、视点和房间节点，作为VLM的结构化输入。基于 ℛ，我们引入了一个两阶段导航策略：VLM在房间级别进行推理和规划（第III-B2节），而智能体则在房间内使用VLM辅助的前沿导航策略（第III-B1节）和基于VLM的目标验证（第III-B2节）在视点级别进行探索。</p>
</blockquote>
<p><strong>核心模块1：多层环境表示</strong><br>表示 ℛ 是一个三层图：</p>
<ol>
<li><strong>视点节点</strong>：用于离散化环境，促进结构化的房内探索。每个视点节点控制一个半径为 ζ_cover 的圆形区域。构建过程（算法1及图3）旨在以最少的节点覆盖整个区域。新的视点节点优先选择在能最大化前沿可见性的位置：对于有前沿的区域，通过构建前沿段图并迭代移除最大团，将每个团的中心添加为新视点节点；对于无前沿的区域，则直接将区域中心添加为视点节点。节点间通过直线可通行性添加边。</li>
<li><strong>物体节点</strong>：利用开放词汇检测和分割方法获得分割后的3D物体实例，在其中心实例化物体节点，记录3D位置、点云、预测标签等属性。新节点若对应同一物理物体会被合并。物体节点与覆盖其且可见的视点节点相连。</li>
<li><strong>房间节点</strong>：通过识别并膨胀环境中的所有墙壁，将环境分割为连通分量，每个分量作为一个房间节点。房间节点与位于其内的视点节点相连。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.06729v2/x3.png" alt="视点选择可视化"></p>
<blockquote>
<p><strong>图3</strong>：视点选择算法可视化。绿色和黄色节点为选定的视点。绿色节点通过处理前沿团中心生成，黄色节点直接取无前沿区域的中心。</p>
</blockquote>
<p><strong>核心模块2：两阶段导航策略</strong></p>
<ol>
<li><strong>房内探索与早期停止</strong>：在选定房间内，采用传统的前沿探索算法。首先将前沿分为两类：指示房间边界、探索未完成的“真前沿”，以及由物体遮挡产生的“内前沿”。智能体迭代导航至最近的有真前沿的视点进行探索，直至清除所有真前沿。如果此时房间内仍有内前沿，则查询VLM，根据当前房间的语义信息（由表示 ℛ 提供）决定是否继续在该房间内进行探索，以避免不必要的详尽搜索。</li>
<li><strong>下一最佳房间选择</strong>：当当前房间探索完成而未找到目标时，需要选择下一个探索房间。此时利用VLM的常识推理能力。如图4所示，构建一个结构化提示，包含：1) 目标物体指令；2) 智能体当前状态；3) 导航历史；4) 格式化为JSON的环境表示 ℛ。此外，还向VLM提供通用探索启发：评估每个房间内物体与目标物体的语义相似性，以及从智能体当前位置到各房间的距离（以优化路径，减少回溯）。VLM使用思维链推理策略选择最合适的未探索房间。最后，选择所选房间内离当前位置最近（使用惩罚距离加权，该距离考虑了已走步数和路径上已探索视点数量）的视点作为下一个行动目标。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.06729v2/x4.png" alt="VLM推理提示"></p>
<blockquote>
<p><strong>图4</strong>：结构化提示及VLM选择下一个最佳房间的推理过程可视化。提示整合了任务指令、智能体状态、历史以及环境表示，VLM基于语义关联和距离成本进行推理。</p>
</blockquote>
<ol start="3">
<li><strong>基于VLM的目标验证</strong>：为提高目标检测准确性，引入VLM进行验证。<strong>上下文感知验证</strong>：当检测到潜在目标物体时，向VLM提供该物体及其周围视觉上下文进行验证。<strong>视点优化重验证</strong>：若初次从次优视点（如遮挡或远距离）观测，则计算从当前位置到目标物体路径上的最优视点，仅在该视点执行一次VLM重验证，以提高精度而不牺牲导航效率。</li>
</ol>
<p><strong>创新点</strong>：与现有方法（如每一步都让VLM在所有视点中选择）相比，STRIVE的创新在于：1) 构建了统一的多层结构化表示，为VLM提供了丰富的空间与语义上下文；2) 设计了房间级规划与房内探索相结合的两阶段策略，显著减少了VLM的查询频率（仅用于房间选择和部分早期停止决策），避免了VLM空间理解不足导致的低效行为，提升了导航效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在Habitat模拟器中评估，使用了四个数据集：HM3D-v1（2000 episodes）、HM3D-v2（1000 episodes）、RoboTHOR（1800 episodes）、MP3D（2195 episodes）。评估指标包括成功率（SR）、路径长度加权成功率（SPL）、到目标距离（DTG）和SoftSPL。同时在配备360度相机和LiDAR的全向轮机器人平台上进行了真实世界实验。</p>
<p><strong>对比方法</strong>：与12种SOTA方法对比，包括SemEXP、PONI、ZSON、L3MVN、ESC、VoroNav、VLFM、SG-Nav、OpenFMNav、TriHelper、InstructNav、DORAEMON。</p>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>模拟实验</strong>：如表I所示，STRIVE在所有数据集上均取得SOTA性能。相比第二名，在HM3D-v1上提升+6.4% SR / +3.6% SPL；在HM3D-v2上提升+13.1% SR / +6.2% SPL；在RoboTHOR上提升+20.6% SR / +12.3% SPL；在MP3D上提升+11.2% SR / +5.5% SPL。在仅考虑起始点与目标同层的HM3D-v1子集上，SR达到72.7%，SPL达到38.2%。</li>
<li><strong>真实世界实验</strong>：在10种不同室内环境的120次实验中，针对5类物体各进行20次实验（表II），平均成功率达81%，平均运行时间约63秒，证明了方法的鲁棒性和实用性。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.06729v2/x5.png" alt="定性结果可视化"></p>
<blockquote>
<p><strong>图5</strong>：STRIVE的定性可视化。第1&amp;2步展示了VLM的推理过程，它通过综合考虑房间布局（‘门口’）、语义线索（‘床头柜’）和旅行成本（惩罚距离）来选择房间6和9。最后一步展示了基于VLM的验证，利用上下文线索（如床垫、枕头）来确认目标物体。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>多层表示</strong>（表III）：在采用视点级策略（为保持一致性）的消融中，同时使用物体节点和房间节点比单独使用任一层或仅使用视点节点带来显著性能提升，证明了各层表示的互补价值。</li>
<li><strong>导航策略组件</strong>（表IV）：<ul>
<li><strong>房间级规划 vs. 视点级规划</strong>：房间级规划（STRIVE）在SR和SPL上大幅优于视点级规划，且VLM令牌使用量从22935大幅降至8068，证明了其高效性。</li>
<li><strong>VLM辅助早期停止</strong>：移除后性能下降。</li>
<li><strong>惩罚距离</strong>：移除后性能下降。</li>
<li><strong>VLM验证</strong>：移除后性能显著下降，突出了其对减少误报的重要性。</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的多层结构化环境表示（房间、视点、物体节点），能够增量构建并为VLM推理提供丰富的空间与语义上下文。</li>
<li>设计了一种高效的两阶段导航策略，将VLM的常识推理用于高级别的跨房间规划，而在房间内则结合传统前沿探索与VLM的辅助决策，显著减少了VLM查询频率并提升了导航效率。</li>
<li>引入了基于VLM的上下文感知目标验证机制，有效提高了目标识别的准确性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法主要针对单层楼物体导航设计。在HM3D-v1上的附加实验也表明，当起始点与目标物体位于不同楼层时，性能会受到影响。</p>
<p><strong>启示</strong>：STRIVE的成功表明，在具身AI任务中，将基础模型的强大推理能力与经典机器人技术（如基于地图的规划）以及精心设计的结构化环境表示相结合，是提升性能、效率和鲁棒性的有效途径。未来的工作可以探索如何将这种多层表示和分层决策范式扩展到更复杂的多楼层或动态环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型（VLM）在物体导航任务中环境信息缺乏结构化、过度依赖VLM查询导致效率低下的核心问题，提出STRIVE框架。其关键技术是通过增量构建包含视点、物体节点与房间节点的多层环境表示来结构化感知信息，并设计融合VLM推理的高层规划与VLM辅助底层探索的两阶段导航策略。在HM3D等三个仿真基准测试中，该方法取得了最先进性能，成功率（SR）提升13.1%，导航效率（SPL）提升6.2%，并在真实机器人平台的120次测试中验证了其强鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.06729" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>