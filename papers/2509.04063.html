<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.04063" target="_blank" rel="noreferrer">2509.04063</a></span>
        <span>作者: Donglin Wang Team</span>
        <span>日期: 2025-09-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于流匹配的视觉-语言-动作模型在通用机器人操作任务上表现出色。然而，这些模型在复杂下游任务上的动作精度不尽如人意。一个重要原因是这些模型仅依赖模仿学习的后训练范式，难以深入理解数据质量分布的特性，而这正是强化学习的优势所在。现有的一些工作尝试引入离线RL来挖掘更深层次的数据质量特征，例如ReinboT算法试图利用RL回报作为细粒度目标来指导VLA模型微调。但本文指出，这种方法在VLA流模型上效果有限，因为流模型通过向量场对整个动作轨迹分布进行建模，最大化回报只能间接且低效地指导最终动作预测。因此，如何对VLA流模型进行有效的离线RL微调仍是一个未充分探索的问题。本文针对这一痛点，提出了一种新颖的自适应离线RL后训练方法ARFM，其核心思路是通过在VLA流模型损失中引入一个自适应调整的缩放因子，构建一个原则性的偏差-方差权衡目标函数，以最优地控制RL信号对流损失的影响。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的自适应强化流匹配方法旨在为VLA流模型构建一种新颖高效的后训练算法。整体流程首先构建能量加权的VLA模型及其对应的能量加权流匹配损失；然后详细阐述了如何在损失函数中构建自适应缩放因子的优化目标，以权衡数据样本的RL信号和梯度方差；最后，基于一些合理假设给出了缩放因子的求解方程、对应的二分迭代算法以及VLA流模型的微调算法。</p>
<p><img src="https://arxiv.org/html/2509.04063v1/figures/pipeline.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ARFM方法整体框架。我们研究VLA流模型的离线RL后训练过程，模型输入包括语言指令、外部图像感知、本体感知以及需要被恢复的动作噪声。我们理论构建了一个带有自适应缩放因子的能量加权流匹配损失，旨在平衡数据样本上的RL信号和梯度方差。最终，我们建立了一个可求解的优化目标，得到了关于缩放因子的非线性方程，并通过二分迭代求解。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>能量加权VLA流模型</strong>：给定数据分布 (p(\mathbf{A}_t | \mathbf{o}_t))，目标是采样自能量引导的分布 (\pi(\mathbf{A}_t | \mathbf{o}_t) \propto p(\mathbf{A}_t | \mathbf{o}_t) \exp(\alpha R^*(\mathbf{o}_t, \mathbf{A}_t)))，其中 (\alpha) 是缩放因子，(R^<em>) 是标准化后的回报优势信号。通过优化条件能量加权流匹配损失来学习策略分布 (\pi) 的向量场：<br>(L^\tau(\theta) = \mathbb{E}[\mathcal{E}^</em>(\mathbf{A}_t, \mathbf{o}_t) || \mathbf{v}_\theta(\mathbf{A}^\tau_t, \mathbf{o}_t) - \mathbf{u}(\mathbf{A}^\tau_t | \mathbf{A}_t) ||^2])，<br>其中能量权重 (\mathcal{E}^<em>) 是优势的softmax归一化。在实践中，对一个批次内的 (B) 个数据对计算加权损失 (L_1^\tau(\theta))，权重 (w_i(\alpha) = \exp(\alpha R^</em>(\mathbf{A}_t^i, \mathbf{o}_t)) / \sum_j \exp(\alpha R^*(\mathbf{A}_t^j, \mathbf{o}_t)))。</p>
</li>
<li><p><strong>自适应缩放因子调整</strong>：缩放因子 (\alpha) 至关重要。(\alpha) 过小则优势信号体现不足；(\alpha=0) 时退化为普通流匹配；(\alpha) 过大则训练会过度集中于高能量样本，导致梯度爆炸。因此，需要在每个微调步骤中自适应调整 (\alpha)，其调整方向是最小化目标函数 (J(\alpha) = \text{Var}(\hat{g}(\alpha)) - \lambda S(\alpha))。其中，(\hat{g}(\alpha)) 是损失 (L_1^\tau(\theta)) 的梯度，最小化其方差以防止训练崩溃；(S(\alpha)) 是一个评分函数，表示RL优势作为能量权重的效果，最大化它以保留RL信号；(\lambda) 是平衡两者权重的超参数。</p>
</li>
<li><p><strong>理论推导与求解算法</strong>：为使优化目标可解，论文做了三个合理假设：RL优势信号和条件流匹配损失均服从高斯分布；批次足够大时可用样本统计量近似。基于此，推导出 (J(\alpha)) 的具体形式（推论1）和最优 (\alpha^<em>) 所需满足的非线性方程（推论2）。该方程可通过高效的二分迭代算法（算法1）实时求解。最终，将求解得到的最优 (\alpha^</em>) 代入加权损失，用于VLA流模型的后训练更新（算法2）。</p>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，ARFM的创新性体现在：1) 将能量加权流匹配方法扩展到VLA模型后训练场景；2) 提出了一个原则性的、用于自适应调整能量权重缩放因子的偏差-方差权衡目标；3) 理论推导了该因子的闭式优化目标与求解算法，实现了在保留RL优势信号与控制训练稳定性之间的自动平衡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟和真实世界机器人操作任务上广泛进行。基准模型采用性能优异的VLA流模型 (\pi_0)。对比的基线方法包括：原始 (\pi_0)、基于模仿学习的后训练方法、以及近期提出的离线RL后训练方法ReinboT。</p>
<p><img src="https://arxiv.org/html/2509.04063v1/figures/exp_setting_1.png" alt="实验设置"></p>
<blockquote>
<p><strong>图2</strong>：模拟实验设置示意图，展示了评估ARFM性能所使用的多样化机器人操作任务场景。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>在模拟的21项任务中，ARFM取得了平均91.4%的成功率，显著优于原始(\pi_0)的82.5%和ReinboT的85.2%。在需要对动态干扰物做出反应的“推入移动篮子”任务中，ARFM的成功率达到85.7%，而(\pi_0)和ReinboT分别为42.9%和50.0%，证明了其优异的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2509.04063v1/figures/few_shot_continuous_learning.png" alt="少样本与持续学习性能"></p>
<blockquote>
<p><strong>图3</strong>：少样本学习与持续学习性能对比。左图显示在不同数据量下，ARFM的微调效果始终优于基线。右图显示在持续学习新任务时，ARFM能更好地保持对旧任务的性能。</p>
</blockquote>
<p>在少样本学习设定下，仅使用10条新任务演示数据，ARFM能将成功率从30%提升至86.7%，而ReinboT仅提升至56.7%。在持续学习实验中，顺序学习三个任务后，ARFM在第一个任务上的遗忘率仅为6.7%，远低于ReinboT的26.7%。</p>
<p><img src="https://arxiv.org/html/2509.04063v1/figures/ablation_hyperparams_iteration.png" alt="消融实验与超参数分析"></p>
<blockquote>
<p><strong>图4</strong>：消融实验与超参数分析。左图对比了固定α与自适应α的效果，证明了自适应机制的重要性。右图展示了不同λ值对性能的影响，表明算法在较宽范围内表现稳定。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>自适应机制的重要性</strong>：将ARFM中的自适应α替换为固定值（α=1.0）后，在模拟任务上的平均成功率下降4.8%，在动态干扰任务上下降14.3%，验证了自适应平衡策略的必要性。</li>
<li><strong>超参数λ的鲁棒性</strong>：实验表明，λ在0.5到2.0的较大范围内变动时，ARFM的性能保持相对稳定，说明方法对超参数不敏感。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.04063v1/figures/real_world_2.png" alt="真实世界实验结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界机器人操作任务实验结果。ARFM在复杂的长视野任务上取得了93.3%的最高成功率。</p>
</blockquote>
<p>在真实世界的6个长视野、多步骤操作任务中，ARFM取得了平均93.3%的成功率，高于(\pi_0)的86.7%和ReinboT的90.0%。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ARFM，一种用于VLA流模型的新型离线RL后训练方法，能够自适应调整数据质量分布。</li>
<li>从理论上建立了自适应调整缩放因子的优化目标，并推导出二分迭代算法进行实时更新，从而实现了高效的VLA流模型微调。</li>
<li>在模拟和真实机器人操作任务上的大量实验表明，ARFM在泛化能力、对动态干扰的鲁棒性、少样本学习和持续学习场景中均表现出最先进的性能。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1) 理论分析中假设RL优势和流匹配损失服从高斯分布，虽然在实践中合理，但可能并非总是成立；2) 与原始流匹配训练相比，ARFM因需计算优势和求解α而引入了轻微的计算开销。</p>
<p><strong>对后续研究的启示</strong>：<br>本文提出的“通过自适应平衡RL信号与梯度方差来优化后训练”的核心思想，可以扩展到其他基于生成模型（如扩散模型）的策略学习方法。此外，如何将这种离线自适应平衡机制与在线交互式RL相结合，是一个有潜力的未来方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对基于流匹配的视觉-语言-动作模型在复杂下游任务中动作精度不足的问题，指出仅依赖模仿学习后训练难以深入利用数据质量分布。为此，提出一种自适应离线强化学习后训练方法——自适应强化流匹配。该方法通过在流模型损失中引入自适应缩放因子，构建偏差-方差权衡目标函数，以最优控制强化学习信号对损失的影响，从而平衡优势保持与梯度方差控制。实验表明，该方法在仿真与真实场景中均表现出优异的泛化、鲁棒、少样本学习及持续学习性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.04063" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>