<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01331" target="_blank" rel="noreferrer">2511.01331</a></span>
        <span>作者: Donglin Wang Team</span>
        <span>日期: 2025-11-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为通用机器人操作的强大策略范式。然而，在分布外部署时，面对观测噪声、传感器误差或执行扰动等不可避免的干扰，这些模型往往无法可靠地泛化。基于强化学习的在线后训练为微调预训练的VLA模型提供了一种实用手段，但现有方法主要侧重于奖励最大化，而忽视了对环境不确定性的鲁棒性。这导致模型可能过度适应后训练环境的特定动态，对轻微扰动变得脆弱。</p>
<p>本文针对VLA模型在环境扰动下鲁棒性不足的具体痛点，提出了鲁棒性感知的强化学习后训练新视角。其核心思路是通过理论分析，识别出控制模型在观测扰动和动作扰动下性能偏差的关键因素，并据此在标准策略优化目标中引入雅可比正则化与平滑性正则化，以显式约束模型的敏感性和更新稳定性。</p>
<h2 id="方法详解">方法详解</h2>
<p>RobustVLA是一个轻量级的在线RL后训练方法，旨在增强VLA模型对环境扰动的韧性。其整体流程遵循标准的策略梯度框架：在环境中收集轨迹，计算优势函数，并更新策略模型。关键创新在于，其优化目标在近端策略优化损失的基础上，增加了两个理论驱动的正则化项。</p>
<p><img src="https://arxiv.org/html/2511.01331v2/iclr2026/figures/method_overview.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RobustVLA方法概览。方法考虑了在线RL交互中的环境不确定性，包括观测噪声和动作噪声。基于对这两种扰动及其联合效应的鲁棒性理论分析，推导出包含模型雅可比正则化和动作平滑正则化的鲁棒RL后训练目标。</p>
</blockquote>
<p>核心模块是两种正则化项，分别对应理论分析中识别的两个关键鲁棒性杠杆：</p>
<ol>
<li><strong>雅可比正则化</strong>：用于抑制观测噪声引起的误差放大。通过惩罚模型对数动作概率相对于观测输入的梯度范数来实现，这直接控制了模型对输入变化的局部敏感性λ。具体损失如公式(2)所示，其中对梯度范数进行了截断以防止过大的惩罚。</li>
<li><strong>平滑性正则化</strong>：用于稳定在线更新以对抗动作扰动。通过惩罚当前模型与一个参考模型的平均动作输出之间的差异来实现，如公式(3)所示。这约束了模型更新过程中的漂移δ_i，确保了时序一致性。参考模型定期更新为当前模型。</li>
</ol>
<p>与现有方法相比，其创新点具体体现在理论驱动的双正则化策略。论文通过定理1-3的形式化分析表明，在观测扰动下，性能差距由模型雅可比敏感度λ与噪声水平ε_s的乘积主导；在动作扰动下，性能差距由模型更新累积漂移∑δ_i主导。因此，联合应用雅可比正则化（减小λ）和平滑性正则化（限制δ_i）可以从理论上控制最坏情况下的性能差距。</p>
<p>最终的鲁棒优化目标如公式(4)所示：ℒ_RobustVLA = ℒ_PPO + αℛ_Jac + βℛ_Smooth，其中α和β是超参数。此外，算法还包含一个课程式自适应噪声调度机制：根据模型平滑后的成功率，动态增加或减少注入的观测和动作噪声水平，从而在训练中逐步增强模型的抗干扰能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在基于LIBERO的长视野操作任务和MetaWorld的短视野操作任务上进行。对比的基线方法包括：1) 纯预训练模型；2) 标准在线RL微调方法；3) 结合数据增强的RL微调；4) 先进的VLA后训练方法如ReWiND和IRe-VLA。</p>
<p><img src="https://arxiv.org/html/2511.01331v2/iclr2026/figures/two_tasks_success_rate_independent_y.png" alt="主要结果"></p>
<blockquote>
<p><strong>图3</strong>：在LIBERO的Pickup和Put-in-bowl任务上，不同方法在干净和各种扰动环境下的成功率对比。RobustVLA（红色）在几乎所有扰动设置下均显著优于基线，尤其是在同时存在观测和动作噪声的“Both”设置中优势明显。</p>
</blockquote>
<p>关键定量结果显示，RobustVLA在存在扰动的环境中显著优于所有基线。例如，在LIBERO的Pickup任务中，当同时存在观测和动作噪声时，RobustVLA的成功率约为75%，而表现次优的基线（PPO+Aug）成功率约为55%，ReWiND约为35%，标准PPO仅为约20%。在MetaWorld的复杂任务上，RobustVLA也展现出更强的鲁棒性，在扰动下的平均成功率比最佳基线高出15-20%。</p>
<p><img src="https://arxiv.org/html/2511.01331v2/iclr2026/figures/ablation.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO任务上的消融实验结果。移除雅可比正则化或平滑性正则化都会导致性能下降，尤其是在“Both”噪声设置下。同时使用两个组件（Full）能取得最佳且最稳定的性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.01331v2/iclr2026/figures/rebuttal_ablation_main.png" alt="组件分析"></p>
<blockquote>
<p><strong>图5</strong>：对RobustVLA各组件贡献的进一步分析。展示了完整方法、仅用PPO、PPO+仅雅可比正则化、PPO+仅平滑性正则化在三个不同噪声水平下的学习曲线。双正则化策略在中等和高噪声水平下带来最稳定和快速的性能提升。</p>
</blockquote>
<p>消融实验证实了两个正则化组件都是必要的。移除任一组件都会导致性能下降，特别是在联合扰动设置下。同时，自适应噪声调度也被证明有助于稳定训练过程并提升最终性能。</p>
<p><img src="https://arxiv.org/html/2511.01331v2/x2.png" alt="迁移学习"></p>
<blockquote>
<p><strong>图6</strong>：在干净环境训练后，迁移到扰动环境下的零样本性能。RobustVLA训练出的模型在迁移到不同程度的观测和动作噪声环境时，性能下降最为平缓，显示出更好的泛化鲁棒性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了RobustVLA，首个显式增强VLA模型对环境扰动鲁棒性的在线RL后训练方法；2) 通过系统的鲁棒性理论分析，将性能差距与模型的雅可比敏感度和更新平滑性联系起来，为方法设计提供了理论依据；3) 设计了雅可比正则化与平滑性正则化项，并集成到策略优化目标中，在多个机器人基准测试上实现了显著的鲁棒性提升。</p>
<p>论文自身提到的局限性包括：理论分析中对初始离线误差和噪声分布的假设在实际中可能不完全成立；方法主要针对随机扰动，对对抗性扰动的鲁棒性未作探讨。</p>
<p>这项工作对后续研究的启示在于：为基于大模型的机器人策略的后训练提供了一个鲁棒性优化的新范式，强调了在追求性能的同时显式约束模型敏感性的重要性。其理论分析框架可扩展至其他类型的扰动或模型架构，双正则化的思想也可启发更高效的鲁棒性训练算法设计。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在分布外部署时，因观测噪声与动作扰动导致泛化可靠性不足的问题，提出鲁棒性感知的强化学习后训练方法RobustVLA。该方法通过引入两种正则化提升鲁棒性：Jacobian正则化降低模型对观测噪声的敏感性，平滑正则化稳定策略以应对动作扰动。实验表明，RobustVLA在多种机器人环境中，其鲁棒性与可靠性显著优于现有先进方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01331" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>