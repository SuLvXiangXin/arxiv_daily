<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Imitation Learning for Active Neck Motion Enabling Robot Manipulation beyond the Field of View - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Imitation Learning for Active Neck Motion Enabling Robot Manipulation beyond the Field of View</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.17624" target="_blank" rel="noreferrer">2506.17624</a></span>
        <span>作者: Yasuo Kuniyoshi Team</span>
        <span>日期: 2025-06-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于深度模仿学习的机器人操作研究主要依赖固定摄像头的图像输入，这导致任务执行被限制在预定义的视野范围内。尽管视觉注意力机制通过预测人类注视位置并裁剪中心视野图像来提升操作鲁棒性，但该方法在注视点位于视野边缘时，测量精度会显著下降，从而影响任务表现。当物体位于视野边缘或完全在视野之外时，传统方法面临巨大挑战。</p>
<p>本文针对“机器人如何操作视野外物体”这一具体痛点，提出将主动颈部运动整合到模仿学习框架中的新视角。人类在操作时会主动移动颈部以观察环境，将此能力赋予机器人有望扩展其操作范围。本文的核心思路是：设计一个包含主动颈部运动的数据收集平台与学习模型，使机器人能够通过转动“颈部”（相机）将目标物体带入视野中心，从而提升对视野边缘及外部物体的操作成功率。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法整体上是一个分阶段的模仿学习框架。其输入是机器人相机捕获的全局图像，输出是下一时刻的颈部角度（偏航和俯仰）、注视点坐标以及机器人手臂的末端执行器状态（位置、姿态和夹爪角度）。流程分为三步：首先预测颈部运动，使相机朝向目标；然后基于调整后的视图预测精确的注视点；最后基于以预测注视点为中心的裁剪图像（中心视野图像）来预测手臂动作。</p>
<p><img src="https://arxiv.org/html/2506.17624v1/extracted/6559673/20250118_final_model_all_en.drawio.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：本研究所用的网络模型整体框架。(a) 颈部运动预测子模型，输入全局图像，输出颈部角度变化。(b) 注视点预测子模型，采用“粗定位+精定位”的两级结构。(c) 手臂动作预测子模型，基于ACT框架，输入裁剪后的中心视野图像和机器人状态，输出未来的手臂动作序列。</p>
</blockquote>
<p><strong>核心模块一：颈部运动预测（图2a）</strong>。该模块以相机拍摄的立体图像为输入，使用ResNet18提取特征，输出未来10个时间步内颈部偏航和俯仰角度的变化量。采用“动作分块+时间集成”策略，对同时发生的动作预测进行加权平均，以确定下一时刻的颈部角度。</p>
<p><strong>核心模块二：注视点预测（图2b）</strong>。该模块采用两级预测架构。第一级（粗定位）将1280x720的RGB图像分割成16x16的块，利用DINOv2提取每个块的特征，并输出注视点出现在每个块上的概率分布，选择概率最高的块作为粗略注视位置。第二级（精定位）以该块为中心的224x224裁剪图像为输入，通过ResNet18提取特征，再经过多层感知器（MLP）输出该裁剪图像内更精细的注视点坐标。最终注视点坐标由公式（1）（2）计算得出，结合了块索引和块内坐标。</p>
<p><strong>核心模块三：手臂状态预测（图2c）</strong>。该模块基于“带Transformer的动作分块”（ACT）框架，其核心是一个条件变分自编码器（CVAE）。输入是以前述预测注视点为中心裁剪出的中心视野图像，以及包含手臂末端执行器状态（10维向量：6维旋转矩阵表示姿态、3维位置、1维夹爪角度）、注视点坐标（4维）和当前颈部角度（2维）的机器人整体状态。编码器根据当前状态和未来动作输出潜在变量z，解码器则结合图像、当前状态和z，预测未来50步的动作序列。同样采用“动作分块+时间集成”方法，但为稳定起见，仅对未来10步的预测进行加权平均以决定下一步动作。</p>
<p><strong>创新点</strong>：1) <strong>系统整合主动颈部运动</strong>：首次在模仿学习框架中系统性地将颈部运动作为可学习的预测目标，而不仅仅是手臂动作的附属输出。2) <strong>改进的远程操作数据收集</strong>：针对头部运动导致视觉不适的问题，采用了<strong>解耦图像投影技术</strong>（图1），将相机图像投影到与远程相机方向动态对齐的虚拟平面上，使操作员在头部与相机角度不一致时也能舒适地感知环境，提升了数据质量。</p>
<p><img src="https://arxiv.org/html/2506.17624v1/extracted/6559673/decoupled_both.png" alt="图像投影方法"></p>
<blockquote>
<p><strong>图1</strong>：头戴式显示器中的图像投影方法。左图为初始状态；中图为操作员头部右转时；右图为相机延迟右转后。采用所提方法后，当操作员头部角度与相机角度不一致时，也能无不适地识别环境。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：任务为在90cm x 60cm的桌面上抓取一个苹果状物体（图3）。物体被放置在机器人视角中桌面的右半侧（45cm x 60cm区域），以迫使机器人转动头部（图4）。使用了两个模型进行对比：一个在包含颈部运动的数据集（321个片段）上训练，另一个在不包含颈部运动的数据集（293个片段）上训练。测试时，每个模型在44个预设物体位置各执行4次任务（共176次），其中152次物体在视野内，24次物体在视野边缘或外部（图4绿色圆圈区域）。</p>
<p><img src="https://arxiv.org/html/2506.17624v1/extracted/6559673/IMG_20250119_222843_1538.jpg" alt="实验环境"></p>
<blockquote>
<p><strong>图3</strong>：实验环境。一张铺着绿色桌布的桌子，一个苹果状物体（左）和一个银色盘子（右）。此图片拍摄方向与机器人相机相反。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.17624v1/extracted/6559673/tong_table_09_with_note.png" alt="物体位置网格"></p>
<blockquote>
<p><strong>图4</strong>：从机器人侧看到的实验环境网格图。苹果的近似位置由网格表示。绿色圆圈区域定义为视野外，标记X的区域未进行实验。</p>
</blockquote>
<p><strong>关键定量结果</strong>：如表I和图6所示。在物体位于<strong>视野内</strong>时，无论是否使用颈部运动，模型都取得了约90%的高成功率（有颈部运动94.1%，无颈部运动86.8%）。这表明引入主动颈部运动带来的视角变化干扰并未显著降低任务性能。在物体位于<strong>视野边缘或外部</strong>时，结果差异巨大：**无颈部运动模型成功率仅为12.5%<strong>，而</strong>有颈部运动模型成功率高达87.5%**，与视野内情况相当。</p>
<p><img src="https://arxiv.org/html/2506.17624v1/extracted/6559673/Figure_1.png" alt="任务成功次数分布"></p>
<blockquote>
<p><strong>图6</strong>：每个位置上任务成功次数（最多4次）的热力图。左图为无颈部运动模型，右图为有颈部运动模型。可见在视野外区域（对应图4绿色圆圈区域），有颈部运动模型成功率显著更高。</p>
</blockquote>
<p><strong>定性分析</strong>：</p>
<ol>
<li><strong>视角干扰</strong>：图7显示，有颈部运动时，相机视野会包含桌子外部的无关物体，这构成了额外的视觉干扰，但模型仍能保持高性能。<br><img src="https://arxiv.org/html/2506.17624v1/extracted/6559673/disturbance.jpg" alt="视角干扰对比"><blockquote>
<p><strong>图7</strong>：机器人相机视角对比。左图为无颈部运动任务，右图为有颈部运动任务。右图因颈部运动包含了更多桌外无关物体。</p>
</blockquote>
</li>
<li><strong>注视预测精度</strong>：图8解释了视野外操作失败的原因。当物体在视野边缘时，注视点预测可能出错（右图），导致后续动作失败。主动颈部运动通过将物体移入视野中心，保障了注视点预测的准确性（左图）。<br><img src="https://arxiv.org/html/2506.17624v1/extracted/6559673/gaze_prediction_ex.jpg" alt="注视预测示例"><blockquote>
<p><strong>图8</strong>：注视点预测结果示例。白圈代表预测的注视位置。左图：物体在视野中心时，预测准确重叠。右图：物体在视野边缘时，预测可能出现错误。</p>
</blockquote>
</li>
<li><strong>行为展示</strong>：图5展示了机器人执行任务时主动将头转向物体的过程。<br><img src="https://arxiv.org/html/2506.17624v1/extracted/6559673/p1033792_clip.jpg" alt="机器人执行任务"><blockquote>
<p><strong>图5</strong>：机器人执行包含主动颈部运动任务的示例，机器人将头部转向物体方向。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验贡献</strong>：实验本身即为核心消融研究，对比了“有颈部运动”与“无颈部运动”两种设置。结果清晰表明，<strong>主动颈部运动模块是模型能够高效处理视野外物体的关键</strong>，其贡献体现在将视野外操作的成功率从12.5%提升至87.5%。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个用于收集包含主动颈部运动的操作数据的新平台，采用解耦图像投影技术减轻操作员不适。2) 设计了一个新颖的模仿学习模型，能够同时学习颈部运动、注视预测和手臂操作，并在物体位于视野内时达到与传统方法相当的高成功率（约90%）。3) 实验证明，主动颈部运动能通过将物体带入视野中心来显著提高注视预测精度，从而将机器人对视野外物体的操作成功率从12.5%大幅提升至87.5%，突破了固定视野的限制。</p>
<p><strong>局限性</strong>：论文提到，本研究假设了一个简化（2自由度）的颈部机构，与人颈部的自由度相比有所限制，排除了位置移动和滚动运动。</p>
<p><strong>后续启示</strong>：这项工作将模仿学习的适用范围扩展到了更动态和复杂的场景。未来研究可以探索利用更多自由度的颈部相机执行更复杂的任务，例如切换视角辅助操作、环境探索以及通过头部姿态传递意图，推动机器人向更自然、更自主的方向发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对固定摄像头视野限制机器人操作范围的问题，提出一种模仿学习框架，使机器人能主动移动颈部以扩展视野。关键技术包括系统化收集颈部运动数据集的教学方法，以及学习主动颈部运动操作任务的新型网络模型。实验表明，该模型在主动颈部运动干扰下仍实现约90%的高成功率，尤其在物体位于视野边缘或之外时表现优异，显著超越传统固定视野模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.17624" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>