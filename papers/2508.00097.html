<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.00097" target="_blank" rel="noreferrer">2508.00097</a></span>
        <span>作者: Zhao, Zhigen, Yu, Liuchuan, Jing, Ke, Yang, Ning</span>
        <span>日期: 2025/07/31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大规模、高质量的机器人演示数据对视觉-语言-动作模型的训练至关重要，而遥操作是收集此类数据的主要手段。现有的机器人遥操作框架主要有几种范式，但各自存在显著局限性。Leader-follower方法虽然延迟低、操作直观，但需要为特定机器人平台定制硬件，可扩展性和普适性差。基于视觉的遥操作系统灵活性更高，但通常存在跟踪不稳定、延迟较高的问题，影响操作员表现和数据质量。基于虚拟现实或扩展现实的遥操作利用商用头显提供直观控制和立体视觉反馈，前景广阔，但现有XR方案配置复杂，通常依赖特定的Unity SDK或WebXR平台，引入了额外的延迟和兼容性问题。此外，XR设备与机器人控制器之间缺乏标准化的数据格式，为集成新的XR设备或机器人平台带来了巨大工作量。</p>
<p>针对上述可扩展性差、设置复杂、延迟高、缺乏标准化等痛点，本文提出了XRoboToolkit，这是一个基于OpenXR标准构建的跨平台XR机器人遥操作框架。其核心思路是构建一个通用接口层，在XR端遵循OpenXR规范，在机器人端提供模块化、可扩展的Python/C++接口，并集成低延迟立体视觉反馈和基于优化的控制方法，以实现跨机器人平台和仿真环境的无缝集成。</p>
<h2 id="方法详解">方法详解</h2>
<p>XRoboToolkit的整体框架是一个连接XR与机器人技术的集成系统，其核心功能包括实时遥操作和立体视觉。</p>
<p><img src="https://arxiv.org/html/2508.00097v2/figure/overview_v6.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：XRoboToolkit整体架构概览。绿色模块代表XR端组件，蓝色模块代表机器人端组件。Unity客户端运行在XR头显上，负责捕捉姿态跟踪数据并提供立体视觉界面；PC服务接收数据，并通过机器人遥操作模块（包含IK和灵巧手重定向求解器）控制模拟或物理机器人平台。</p>
</blockquote>
<p>框架主要分为XR端和机器人端。XR端的Unity客户端应用部署在头显上，负责捕获包括头部、手部、控制器、全身及物体（通过运动追踪器）在内的多种姿态跟踪数据，并通过网络传输至机器人端的PC服务。立体视觉可通过PICO头显的板载摄像头或外接的ZED Mini相机实现。机器人端则运行逆运动学和灵巧手重定向求解器，支持MuJoCo仿真环境以及UR5、ARX R5、Galaxea R1-Lite等物理机器人平台。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>数据流</strong>：PC服务采用异步、回调驱动的架构进行实时数据流传输。所有数据遵循OpenXR约定的右手坐标系。6自由度姿态数据格式为7个浮点数（位置+四元数）。所有实时跟踪数据以单一JSON对象形式在90Hz频率下传输，简化了客户端解析。</p>
<p><img src="https://arxiv.org/html/2508.00097v2/figure/coordinate_v2.png" alt="坐标系统"></p>
<blockquote>
<p><strong>图2</strong>：OpenXR姿态跟踪坐标系约定。X轴向右，Y轴向上，Z轴向后。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.00097v2/figure/hand_and_body_v2.png" alt="关键点"></p>
<blockquote>
<p><strong>图3</strong>：(a) 手部跟踪关键点（26个关节）和 (b) 全身跟踪关键点（24个关节）的约定。</p>
</blockquote>
</li>
<li><p><strong>机器人控制</strong>：该模块将XR跟踪状态映射为机器人指令，包含多种控制模式。</p>
<ul>
<li><strong>逆运动学</strong>：采用基于二次规划的IK求解器（使用PlaCo库）。其优化问题定义为最小化加权任务残差，并包含关节速度约束。为提高机器人接近奇异点时的稳定性，引入了最大化可操作性的正则化项。控制器控制末端执行器时采用相对运动模式，即机器人末端跟踪控制器相对于首次按下握柄按钮时的位移。辅助运动追踪器可附着在操作员身体部位（如肘部），作为额外的姿态约束加入QP-IK问题，实现对冗余自由度更符合人体工学的控制。</li>
<li><strong>灵巧手重定向</strong>：用于将OpenXR手部跟踪模型的26个关键点映射到机器人手部关节空间。通过优化问题求解，目标函数包含关键点位置匹配误差和平滑性正则项，并受关节限位约束。实现基于<code>dex_retargeting</code>。</li>
<li><strong>移动底盘控制</strong>：对于全向移动机械臂，通过XR控制器摇杆控制，左摇杆控制线速度，右摇杆X轴控制角速度。</li>
</ul>
</li>
<li><p><strong>XR Unity应用</strong>：提供用户操作界面，包含网络、跟踪、远程视觉、数据收集和日志五个面板，用于配置连接、选择跟踪源、管理立体视觉流、录制数据及监控系统状态。</p>
<p><img src="https://arxiv.org/html/2508.00097v2/figure/unity_client_ui.png" alt="Unity界面"></p>
<blockquote>
<p><strong>图4</strong>：XRoboToolkit Unity客户端应用界面截图（PICO版本）。</p>
</blockquote>
</li>
<li><p><strong>立体视觉反馈</strong>：支持PICO 4 Ultra头显和ZED Mini相机两种立体视频源。通过自定义着色器调整瞳孔间距并将焦点设置在约3.3英尺处，以增强适合操作任务的深度感知。PICO 4 Ultra在视觉质量、视场角平衡方面表现更优。</p>
</li>
</ol>
<p><strong>创新点</strong>：<br>与现有方法相比，XRoboToolkit的主要创新体现在：1) <strong>跨平台标准化</strong>：基于OpenXR标准构建通用接口层，解决了XR设备与机器人平台间的标准化难题。2) <strong>低延迟立体视觉</strong>：优化了视频流管道和通信协议，旨在最小化延迟。3) <strong>基于优化的灵活控制</strong>：采用QP-IK求解器，易于纳入约束和正则项，提高了在奇异点附近的稳定性和控制灵活性，并支持通过辅助追踪器进行全身姿态映射。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>延迟测量</strong>：使用Kandao QooCam EGO 3D相机同时录制虚拟和现实视角，通过100Hz循环的精密计时LED面板测量视频流延迟。对比了Open-TeleVision和XRoboToolkit在不同硬件配置（ZED Mini + Quest 3， ZED Mini + PICO 4 Ultra， PICO 4 Ultra + PICO 4 Ultra）下的表现。视频传输参数统一为1280×720分辨率，60 FPS，1 Mbps码率。</li>
<li><strong>数据收集与VLA微调</strong>：使用ARX R5双臂系统收集了100次双边地毯折叠任务的演示数据，用于对<code>π_0</code>模型进行LoRA微调，并评估训练后策略的自主性能。</li>
</ul>
<p><strong>对比方法</strong>：在延迟实验中，主要对比了同期工作Open-TeleVision。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>视频流延迟对比</strong>：XRoboToolkit在所有配置下均表现出色。在相同硬件（ZED Mini – Quest 3）下，XRoboToolkit的平均延迟为94.50 ms，显著低于Open-TeleVision的121.50 ms，降低了22%。最佳配置（ZED Mini – PICO 4 Ultra）达到了最低平均延迟82.00 ms。PICO 4 Ultra – PICO 4 Ultra配置的延迟波动最小（标准差3.12 ms）。</p>
<p><img src="https://arxiv.org/html/2508.00097v2/figure/measurement.jpg" alt="延迟测量"></p>
<blockquote>
<p><strong>图6</strong>：视频流延迟测量方法示意图，通过对比VR显示画面与真实世界LED面板画面的时间差计算延迟。</p>
</blockquote>
</li>
<li><p><strong>VLA微调验证</strong>：使用XRoboToolkit收集的数据对<code>π_0</code>模型进行微调后，得到的策略在30分钟连续运行中达到了100%的成功率，平均任务完成时间为30秒。该策略展现出自主调整行为，例如在夹爪未能稳固抓住地毯时自动重新抓取，在地毯偏离中心时智能调整位置。</p>
</li>
</ol>
<p><strong>应用演示</strong>：<br><img src="https://arxiv.org/html/2508.00097v2/figure/applications_v3.jpg" alt="应用示例"></p>
<blockquote>
<p><strong>图5</strong>：XRoboToolkit的应用示例：(a) 使用XR控制器进行双臂和移动机械臂遥操作；(b) 具有2自由度主动头部跟踪和立体视觉的双UR5高精度操作；(c) 使用辅助运动追踪器进行机器人肘部控制（MeshCat可视化）；(d) 在MuJoCo仿真中的灵巧手跟踪控制。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个基于OpenXR标准的、模块化的跨平台XR机器人遥操作框架XRoboToolkit，显著提升了系统的可访问性、可扩展性和易集成性。</li>
<li>设计并实现了低延迟的立体视觉反馈系统和高效的基于QP的逆运动学求解器，提升了遥操作体验的数据质量与控制稳定性。</li>
<li>通过高精度操作任务验证和VLA模型训练实验，证明了该框架能够有效收集高质量演示数据，并支持学习鲁棒的自主策略。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>全身跟踪依赖PICO的24关节模型，由于OpenXR缺乏标准全身模型，可能与其他XR品牌存在兼容性问题，且该功能尚未在仿人机器人全身遥操作中得到验证。</li>
<li>灵巧手重定向框架假设每个关节独立可控，无法准确重定向到具有机械耦合关节（如INSPIRE Hands）的机器人手。</li>
<li>目前仅支持MuJoCo仿真环境，限制了其在多样化仿真生态中的应用。</li>
</ol>
<p><strong>后续启示</strong>：<br>本文指出了推动OpenXR在机器人遥操作领域标准化的重要性。未来的工作可以集中在：改进针对欠驱动系统的重定向算法；通过支持Roboverse等平台扩展仿真兼容性；开发并验证仿人机器人全身运动重定向能力。这些方向将进一步提升XR遥操作框架的通用性和实用性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人遥操作数据收集方法存在的可扩展性差、设置复杂和数据质量不佳等问题，提出了XRoboToolkit——一个基于OpenXR标准的跨平台扩展现实遥操作框架。其核心技术包括低延迟立体视觉反馈、基于优化的逆运动学算法，并支持头、手、控制器等多种跟踪模式。该框架采用模块化架构，实现了跨机器人平台与仿真环境的无缝集成。实验通过在精确操作任务中的应用，验证了其有效性；利用该框架采集数据训练的VLA模型展现了鲁棒的自主性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.00097" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>