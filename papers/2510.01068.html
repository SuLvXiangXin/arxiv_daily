<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.01068" target="_blank" rel="noreferrer">2510.01068</a></span>
        <span>作者: Cao, Jiahang, Huang, Yize, Guo, Hanzhong, Zhang, Rui, Nan, Mu, Mai, Weijian, Wang, Jiaxu, Cheng, Hao, Sun, Jingkai, Han, Gang, Zhao, Wen, Zhang, Qiang, Guo, Yijie, Zheng, Qihao, Song, Chunfeng, Li, Xiao, Luo, Ping, Luo, Andrew F.</span>
        <span>日期: 2025/10/01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，基于扩散模型（Diffusion Policy）或流匹配（Flow Matching）的机器人策略，包括视觉-语言-动作（VLA）和视觉-动作（VA）策略，已展现出强大的能力。然而，其发展受限于获取大规模交互数据集的高昂成本。传统的后训练策略，如监督微调需要昂贵的数据收集，而强化学习则引入了奖励工程和大量在线交互的复杂性。本文针对这一痛点，提出了一个无需额外模型训练即可提升策略性能的新范式：通过组合现有预训练模型来构建更强的策略。本文的核心思路是：在测试时，通过凸组合和权重搜索，将多个预训练策略的分布分数（score）组合起来，从而产生一个性能可能超越任一父策略的复合策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的通用策略组合（General Policy Composition, GPC）是一个免训练的框架，其核心思想是利用凸分数组合，将多个预训练的扩散或流匹配策略整合为一个更强、更具表达力的策略。</p>
<p><img src="https://arxiv.org/html/2510.01068v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：通用策略组合（GPC）概述。通过组合来自不同条件（如视觉模态和网络主干）的预训练策略的分布分数，GPC可以在无需额外训练的情况下，通过凸分数组合生成更具表达力和适应性的动作轨迹。</p>
</blockquote>
<p>整体流程如算法1所示：在推理时，对于给定的权重w1（对应策略π1），从噪声轨迹开始，在每一步去噪过程中，分别从两个策略π1和π2获取分数估计s1和s2，然后进行凸组合s_comp = w1<em>s1 + w2</em>s2，最后使用组合后的分数根据扩散更新规则生成下一步的轨迹。通过遍历一组预定义的权重（如0.0, 0.1, ..., 1.0）并评估其成功率，在测试时搜索出最优权重w1*。</p>
<p>核心模块是分数组合公式。形式上，令复合分数为s_comp，GPC的更新规则定义为：<br>s_comp(τ_t, t, c) = Σ_{i=1}^n w_i * s_θ(τ_t, t, c_i)， 其中 Σ_{i=1}^n w_i = 1。<br>这里s_θ(τ_t, t, c_i)表示在概念c_i（如不同的视觉模态或策略架构）条件下的分数估计，w_i是分配给每个概念的凸组合权重。这种凸组合确保了复合分数保持在个体策略的可行凸包内，防止行为发散。</p>
<p>与现有方法（如基于分类器无引导（CFG）的组合）相比，GPC的创新点具体体现在：1) 提供了理论证明，表明分数的凸组合可以在函数层面产生更低误差的分数估计，并且这种优势通过采样动力学的稳定性传播到系统层面（轨迹误差）。2) 框架灵活，支持跨异构模型（扩散/流匹配、VA/VLA、不同感官输入）的即插即用式组合。</p>
<p>此外，GPC框架可以自然地扩展到其他组合算子，如逻辑OR（对应分布混合）和逻辑AND（对应分布交集），这通过不同的权重计算方式实现。</p>
<p><img src="https://arxiv.org/html/2510.01068v1/x3.png" alt="策略组合可视化"></p>
<blockquote>
<p><strong>图3</strong>：不同扩散策略及GPC组合策略的可视化结果。即使DP的一部分失败，GPC也能成功；当DP的两部分都工作时，GPC表现出更好的性能。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真环境Robomimic（包含Can、Lift、Square任务）、PushT以及RoboTwin（一系列双臂协作任务）上进行评估。此外还进行了四个真实世界实验。对比的基线包括多种VA模型（如DP、Mamba Policy、Flow Policy）和VLA模型（如基于Florence的MDT、π0）。GPC是免训练的，直接使用预训练策略，并在测试时搜索权重系数（0.0到1.0，步长0.1）。每个设置评估200次（RoboTwin为100次），报告平均成功率（SR）。</p>
<p><strong>关键实验结果</strong>：<br>表1展示了在Robomimic和PushT上的结果。GPC在不同类型的策略组合上均带来了性能提升。例如，组合两个VA策略（DP+MP）比基础策略平均提升+2.22%；组合VA和VLA模型（Florence-D+DP）则带来+5.51%的平均提升；组合两个流匹配的VLA/VA策略（Florence-F+FP）提升高达+7.55%。</p>
<p>表2展示了在RoboTwin上的结果。组合不同模态的VA策略（DP_img + DP_pcd）比基础策略平均提升+5%；组合VLA和VA策略（RDT + DP_pcd）提升+7%。</p>
<p><strong>表1：Robomimic和PushT实验结果</strong></p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">生成模式</th>
<th align="left">模型类型</th>
<th align="left">Can</th>
<th align="left">Lift</th>
<th align="left">Square</th>
<th align="left">PushT</th>
<th align="left">平均</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>基础策略</strong></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Diffusion Policy (DP)</td>
<td align="left">Diffusion</td>
<td align="left">VA</td>
<td align="left">34.50</td>
<td align="left">98.50</td>
<td align="left">2.00</td>
<td align="left">21.75</td>
<td align="left">39.19</td>
</tr>
<tr>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
</tr>
<tr>
<td align="left">π0</td>
<td align="left">Flow Matching</td>
<td align="left">VLA</td>
<td align="left">96.50</td>
<td align="left">99.00</td>
<td align="left">92.50</td>
<td align="left">57.69</td>
<td align="left">86.42</td>
</tr>
<tr>
<td align="left"><strong>组合策略 (GPC)</strong></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DP+MP</td>
<td align="left">Diffusion</td>
<td align="left">VA &amp; VA</td>
<td align="left">34.50</td>
<td align="left">99.50</td>
<td align="left">8.00</td>
<td align="left">23.63</td>
<td align="left">41.41 (+2.22%)</td>
</tr>
<tr>
<td align="left">Florence-D+DP</td>
<td align="left">Diffusion</td>
<td align="left">VLA &amp; VA</td>
<td align="left">62.50</td>
<td align="left">100.00</td>
<td align="left">61.50</td>
<td align="left">43.06</td>
<td align="left">66.76 (+5.51%)</td>
</tr>
<tr>
<td align="left">Florence-F+FP</td>
<td align="left">Flow Matching</td>
<td align="left">VLA &amp; VA</td>
<td align="left">98.50</td>
<td align="left">98.50</td>
<td align="left">92.50</td>
<td align="left">56.06</td>
<td align="left">86.39 (+7.55%)</td>
</tr>
<tr>
<td align="left">π0+FP</td>
<td align="left">Flow Matching</td>
<td align="left">VLA &amp; VA</td>
<td align="left">99.50</td>
<td align="left">100.00</td>
<td align="left">94.00</td>
<td align="left">62.25</td>
<td align="left">88.94 (+2.52%)</td>
</tr>
</tbody></table>
<p><strong>表2：RoboTwin实验结果</strong></p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">模型类型</th>
<th align="left">Hanging Mug</th>
<th align="left">Open Laptop</th>
<th align="left">Place Burger</th>
<th align="left">Fries Put</th>
<th align="left">Object Cabinet</th>
<th align="left">Stack Bowls</th>
<th align="left">Three Turn Switch</th>
<th align="left">平均</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>基础策略</strong></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DP_img</td>
<td align="left">VA</td>
<td align="left">0.10</td>
<td align="left">0.74</td>
<td align="left">0.49</td>
<td align="left">0.56</td>
<td align="left">0.52</td>
<td align="left">0.38</td>
<td align="left">0.46</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DP_pcd</td>
<td align="left">VA</td>
<td align="left">0.21</td>
<td align="left">0.93</td>
<td align="left">0.72</td>
<td align="left">0.71</td>
<td align="left">0.64</td>
<td align="left">0.71</td>
<td align="left">0.65</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">RDT</td>
<td align="left">VLA</td>
<td align="left">0.13</td>
<td align="left">0.69</td>
<td align="left">0.46</td>
<td align="left">0.32</td>
<td align="left">0.47</td>
<td align="left">0.30</td>
<td align="left">0.40</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>组合策略 (GPC)</strong></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">DP_img + DP_pcd</td>
<td align="left">VA &amp; VA</td>
<td align="left">0.23</td>
<td align="left">0.93</td>
<td align="left">0.78</td>
<td align="left">0.82</td>
<td align="left">0.71</td>
<td align="left">0.71</td>
<td align="left">0.70 (+5%)</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">RDT + DP_img</td>
<td align="left">VLA &amp; VA</td>
<td align="left">0.18</td>
<td align="left">0.80</td>
<td align="left">0.57</td>
<td align="left">0.59</td>
<td align="left">0.66</td>
<td align="left">0.38</td>
<td align="left">0.53 (+7%)</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">RDT + DP_pcd</td>
<td align="left">VLA &amp; VA</td>
<td align="left">0.36</td>
<td align="left">0.94</td>
<td align="left">0.83</td>
<td align="left">0.78</td>
<td align="left">0.73</td>
<td align="left">0.71</td>
<td align="left">0.72 (+7%)</td>
<td align="left"></td>
</tr>
</tbody></table>
<p><strong>消融分析与深入洞察</strong>：<br>表3分析了不同权重配置下GPC的性能，揭示了最优权重是任务依赖的。例如，在“Empty Cup Place”任务中，当两个基础策略性能都较好时，GPC在权重为0.2时取得最佳效果（SR 0.86），比最佳基础策略（SR 0.62）提升24%。在“Dual Bottles Pick (Easy)”任务中，一个策略明显优于另一个，GPC通过调整权重（如0.7）仍能获得比两者都好的性能（SR 0.85 vs 0.77）。</p>
<p><strong>表3：不同组合配置下的实验结果</strong></p>
<table>
<thead>
<tr>
<th align="left">场景</th>
<th align="left">任务</th>
<th align="left">DP_img</th>
<th align="left">DP_pcd</th>
<th align="left"><strong>权重调度 (w1 for DP_img)</strong></th>
<th align="left"></th>
<th align="left"></th>
<th align="left"></th>
<th align="left"></th>
<th align="left"></th>
<th align="left"></th>
<th align="left"></th>
<th align="left">Δ</th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">0.1</td>
<td align="left">0.2</td>
<td align="left">0.3</td>
<td align="left">0.4</td>
<td align="left">0.5</td>
<td align="left">0.6</td>
<td align="left">0.7</td>
<td align="left">0.8</td>
<td align="left">0.9</td>
</tr>
<tr>
<td align="left">两策略均优</td>
<td align="left">Empty Cup Place</td>
<td align="left">0.42</td>
<td align="left">0.62</td>
<td align="left">0.70</td>
<td align="left"><strong>0.86</strong></td>
<td align="left">0.84</td>
<td align="left">0.86</td>
<td align="left">0.84</td>
<td align="left">0.84</td>
<td align="left">0.76</td>
<td align="left">0.68</td>
<td align="left">0.61</td>
</tr>
<tr>
<td align="left">策略A &gt; 策略B</td>
<td align="left">Dual Bottles Pick (Easy)</td>
<td align="left">0.77</td>
<td align="left">0.36</td>
<td align="left">0.52</td>
<td align="left">0.64</td>
<td align="left">0.70</td>
<td align="left">0.75</td>
<td align="left">0.82</td>
<td align="left"><strong>0.81</strong></td>
<td align="left">0.80</td>
<td align="left">0.85</td>
<td align="left">0.80</td>
</tr>
</tbody></table>
<p><img src="https://arxiv.org/html/2510.01068v1/x4.png" alt="GPC跨模态和架构的可视化分析"></p>
<blockquote>
<p><strong>图4</strong>：GPC在不同组合下的可视化分析。GPC可泛化跨越（a）模态和（b）架构，适当的权重能产生比个体策略成功率（SR）更高的准确分布。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.01068v1/x5.png" alt="执行过程中的样本分布"></p>
<blockquote>
<p><strong>图5</strong>：随时间执行的样本分布。GPC产生的分布比基线更一致。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>理论贡献</strong>：建立了机器人策略组合的理论基础，证明了分布分数的凸组合可以产生改进的函数目标，并且这种优势能通过稳定的采样动力学传播到系统层面。2) <strong>方法贡献</strong>：提出了通用策略组合（GPC），这是一个灵活的、免训练的框架，能够跨不同模态和架构组合预训练策略，形成一个更具表达力的策略。3) <strong>实证贡献</strong>：在仿真和真实世界进行了广泛评估，证明了GPC能带来一致的性能提升，并分析了关键设计选择以指导未来研究。</p>
<p>论文提到的局限性包括：虽然理论保证了最优权重的存在，但解析地找到它们很困难，因此需要在测试时进行搜索，这可能增加计算开销；理论分析依赖于Lipschitz连续性等假设。</p>
<p>对后续研究的启示：1) 可以探索更高效的测试时权重搜索或自适应调整策略。2) GPC框架展示了组合异构模型的潜力，未来可研究将其扩展到更多类型的策略（如基于模型的策略）或更复杂的组合图式。3) 对逻辑AND/OR等其他组合算子的探索，为理解多策略协同提供了更广阔的视角。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对扩散基或流基机器人策略因大规模交互数据获取成本高而性能受限的问题，提出无需额外训练的通用策略组合（GPC）方法。该方法通过凸组合多个预训练策略的分布分数，在测试时进行搜索和组合，实现异构策略的即插即用。实验在Robomimic、PushT和RoboTwin等基准测试及真实机器人评估中表明，GPC能 consistently improves performance and adaptability across diverse tasks.</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.01068" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>