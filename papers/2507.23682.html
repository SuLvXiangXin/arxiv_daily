<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.23682" target="_blank" rel="noreferrer">2507.23682</a></span>
        <span>作者: Jiang Bian Team</span>
        <span>日期: 2025-07-31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型已成为学习遵循语言指令并泛化到新场景的机器人操作策略的主流范式。为了利用丰富的无动作标签数据（如人类视频），近期研究开始探索将潜在动作（即两帧之间运动的抽象表示）融入VLA预训练。然而，现有方法存在两个关键局限性：首先，现有潜在动作模型（LAM）通常仅基于视觉信号压缩潜在动作，这导致其对像素变化不明显的关键物理运动（如末端执行器旋转或夹爪开合）关注不足，使得学习到的潜在动作物理基础不牢，阻碍了有效的知识迁移。其次，在如何将学习到的潜在动作更有效地整合到VLA预训练中，现有方法（如LAPA、Go-1等）的集成策略相对松散或存在不一致性，未能充分发挥其潜力。</p>
<p>本文针对“如何更好地学习潜在动作”以及“如何更有效地将其整合到VLA预训练中”这两个核心痛点，提出了新的视角：1）超越纯视觉信号，引入本体感知等结构线索对潜在动作进行物理基础化；2）通过联合扩散框架，以分层和条件化的方式建模潜在动作与机器人动作，实现更结构化的信息传递。本文的核心思路是提出一个名为villa-X的视觉-语言-潜在动作（ViLLA）框架，通过改进的潜在动作模型和新型策略架构，共同提升学习通用机器人操作策略的能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>villa-X框架包含两个核心组件：潜在动作模型（LAM）和行动者模块（ACT）。整体训练流程分为三个阶段：1）在多样化数据集上预训练LAM；2）使用联合潜在-机器人动作建模预训练ACT；3）针对特定具身进行微调。</p>
<p><img src="https://arxiv.org/html/2507.23682v3/figs/act.png" alt="方法架构"></p>
<blockquote>
<p><strong>图2</strong>：ACT模块架构。这是一个分层策略，包含VLM、ACT-latent和ACT-robot三个专家模块，通过块状因果注意力掩码，实现基于潜在动作计划的条件化机器人动作生成，并整合了具身上下文。</p>
</blockquote>
<p><strong>核心模块一：改进的潜在动作模型（LAM）</strong><br>传统LAM包含逆动力学模型（IDM）和视觉前向动力学模型（FDM），通过视觉重建学习潜在动作$z_t$：$z_t = \text{IDM}(o_t, o_{t+K}), \quad \hat{o}_{t+K} = \text{FDM}(o_t, z_t)$。本文的关键创新是引入了一个<strong>本体感知前向动力学模型（proprio-FDM）</strong>作为辅助解码器。</p>
<p><img src="https://arxiv.org/html/2507.23682v3/cube.png" alt="LAM对比"></p>
<blockquote>
<p><strong>图1</strong>：（a）标准LAM主要通过视觉重建学习潜在动作$z_t$。（b）我们提出的模型通过添加proprio-FDM进行增强。该模块在具身上下文$c_e$的条件下，预测未来的机器人状态$\hat{q}<em>{t+1:t+K}$和动作$\hat{a}</em>{t:t+K-1}$，使潜在动作更好地基于物理动力学。</p>
</blockquote>
<p>该模块的输入为当前状态$q_t$、潜在动作$z_t$和具身上下文$c_e$，输出为未来K步的预测状态和动作：$(\hat{q}<em>{t+1},...,\hat{q}</em>{t+K}, \hat{a}<em>{t+1},...,\hat{a}</em>{t+K}) = \text{proprio-FDM}(q_t, z_t, c_e)$。通过联合优化视觉重建和本体感知预测损失，迫使潜在令牌在关注视觉变化的同时，也关注与智能体物理动力学一致的变化。$c_e$（包含数据集ID和控制频率）用于区分异构数据，防止将具身特异性特征编码到潜在动作中，从而保持跨数据集的潜在动作一致性。</p>
<p><strong>核心模块二：行动者模块（ACT）</strong><br>ACT模块通过一个联合扩散过程，显式地共同建模潜在动作序列$z_{t:t+(n-1)K}^K$和机器人动作序列$a_{t:t+m-1}$。策略被分解为两个条件分布：<br>$\pi(a_{t:t+m-1}, z_{t:t+(n-1)K}^K | o_t, l, q_t, c_e) = \underbrace{\pi_{\text{robot}}(a_{t:t+m-1} | z_{t:t+(n-1)K}^K, o_t, l, q_t, c_e)}<em>{\text{ACT-robot}} \cdot \underbrace{\pi</em>{\text{latent}}(z_{t:t+(n-1)K}^K | o_t, l)}_{\text{ACT-latent}}$<br>这种显式建模使潜在动作成为连接高级视觉语言提示与低级机器人动作的中间表示。</p>
<ul>
<li><strong>架构</strong>：如图2所示，包含VLM、ACT-latent（潜在动作专家）和ACT-robot（机器人动作专家）三个模块，采用块状因果注意力。</li>
<li><strong>注意力掩码策略</strong>：采用随机掩码策略。50%的情况完全掩码从机器人动作到潜在动作的注意力；否则，随机掩码50%的潜在动作令牌。这防止了机器人动作分支过度依赖潜在动作，提高了鲁棒性。</li>
<li><strong>联合扩散建模</strong>：使用条件流匹配框架建模机器人动作和潜在动作的联合分布，通过块状因果注意力实现公式中的显式分解。</li>
</ul>
<p><strong>创新点总结</strong><br>与现有方法相比，villa-X的创新具体体现在：1) 在LAM中引入proprio-FDM，首次利用本体感知信号对潜在动作进行物理基础化，解决了视觉模型对细微物理运动不敏感的问题；2) 在ACT中提出了通过联合扩散进行分层条件化建模的新范式，比LAPA的权重初始化集成更紧密，比Go-1的自回归规划避免了教师强制不一致性，实现了更结构化、更高效的信息传递。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了SIMPLER仿真基准（包含Google Robot和WidowX Robot平台的多项任务）和两个真实世界机器人平台（带夹爪的Realman机械臂和带灵巧手XHand的XArm机械臂）。对比了多类基线：纯VLA模型（RT-1-X, Octo-base, OpenVLA等）、联合策略与世界模型方法（GR00T）、视觉轨迹方法（TraceVLA, Magma）以及潜在动作方法（MoTo, LAPA）。</p>
<p><strong>1. 改进的LAM是否学习了更高质量的潜在动作？</strong><br>通过探测实验评估潜在动作预测底层机器人动作的能力。</p>
<p><img src="https://arxiv.org/html/2507.23682v3/figs/probing.png" alt="探测实验结果"></p>
<blockquote>
<p><strong>图3</strong>：探测实验结果。带有proprio-FDM的模型（w/pp）在低误差区间拥有更多样本，表明其学习的潜在动作包含了更准确的机器人动作信息。</p>
</blockquote>
<p>在SIMPLER上的策略预训练实验进一步验证了改进LAM的有效性。如表1所示，使用带proprio-FDM的LAM（w/pp）预训练的策略，在Google Robot和WidowX Robot上的平均成功率分别为58.5%和40.8%，显著优于不带该模块的变体（wo/pp，57.4%和32.3%）以及完全不使用潜在动作的基线（wo/LAM，35.0%和33.1%）。</p>
<p><strong>2. ACT模块能否有效利用预训练的潜在动作？</strong><br>在相同数据集上对比了villa-X与LAPA-style、Go-1-style的集成方法。如表1底部所示，villa-X（Ours）在WidowX Robot上的平均成功率（40.8%）远超LAPA-style（1.0%）和Go-1-style（14.8%），证明了其集成策略的优越性。</p>
<p><strong>3. 潜在动作专家的零样本泛化能力</strong><br><img src="https://arxiv.org/html/2507.23682v3/figs/emoji_pic2.png" alt="零样本生成可视化"></p>
<blockquote>
<p><strong>图4</strong>：在未见过的具身（Realman机械臂）和开放词汇符号（如玉米图标）上零样本生成潜在计划的渲染结果。模型成功生成了符合指令（如“触摸玉米”）的动作序列。</p>
</blockquote>
<p><strong>4. 在仿真基准上的综合性能</strong><br><img src="https://arxiv.org/html/2507.23682v3/x1.png" alt="仿真结果"></p>
<blockquote>
<p><strong>表2</strong>：在SIMPLER基准上的综合对比。villa-X（Ours）在Google Robot和WidowX Robot上的平均成功率分别为77.7%和62.5%，达到最优性能。消融实验（Ours w/o latent）性能下降，证明了潜在动作专家的必要性。</p>
</blockquote>
<p><strong>5. 在真实世界机器人上的评估</strong><br><img src="https://arxiv.org/html/2507.23682v3/figs/real_robot.png" alt="真实机器人平台"></p>
<blockquote>
<p><strong>图5</strong>：真实世界评估平台：（上）带夹爪的Realman机械臂平台，（下）带XHand灵巧手的XArm机械臂。左侧为平台设置，右侧为对应评估任务。</p>
</blockquote>
<p>在Realman夹爪平台（表4）和XArm灵巧手平台（表3）的五个任务上，villa-X在大多数任务上（无论是已见场景还是泛化场景）均优于GR00T等基线方法。例如，在Realman的“Pick out”和“Unstack”任务中达到100%成功率；在XArm的“Pick &amp; Place”任务已见场景中达到84%成功率。</p>
<p><strong>消融实验总结</strong>：1) proprio-FDM模块对提升潜在动作质量至关重要；2) 联合扩散与条件化建模的ACT架构是有效利用潜在动作的关键；3) 移除潜在动作专家（Ours w/o latent）会导致性能显著下降，证明了该组件设计的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) <strong>改进潜在动作学习</strong>：通过引入本体感知前向动力学模型（proprio-FDM），将潜在动作与底层机器人状态和动作对齐，实现了物理基础化。2) <strong>提出新颖的集成策略</strong>：通过联合扩散框架分层建模潜在动作专家和机器人动作专家，以条件化的方式充分挖掘潜在动作的潜力，实现了更结构化的信息迁移。3) <strong>展示了强大的零样本泛化能力</strong>：通过规模化预训练，潜在动作专家能够对未见过的具身和开放词汇符号进行零样本规划。</p>
<p><strong>局限性</strong>：论文提到，所提出的潜在动作维度可能限制其表示复杂、长时程技能的能力。这为未来研究指出了方向。</p>
<p><strong>启示</strong>：villa-X的成功表明，在VLA模型中，对中间表示（如潜在动作）进行<strong>物理基础化</strong>和<strong>分层结构化建模</strong>是提升策略泛化性能和零样本能力的有效途径。该方法为利用多模态、异构数据（如人类视频）训练通用机器人策略提供了一个可扩展且原理性强的范式。未来工作可探索将框架扩展到其他结构线索（如关键点检测、人体姿态估计），并研究更高维度的潜在动作表示。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出villa-X框架，旨在解决视觉-语言-动作模型中潜在动作学习不充分、物理基础薄弱的问题。关键技术是引入本体感知前向动力学模型作为辅助解码器，通过结合结构线索增强潜在动作的物理基础，使其更好地关联视觉变化与机器人控制。实验表明，villa-X能零样本生成潜在动作计划，在SIMPLER仿真和真实机器人任务中均取得优越性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.23682" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>