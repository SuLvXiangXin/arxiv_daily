<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Human-Level Competitive Pok\&#39;emon via Scalable Offline Reinforcement Learning with Transformers - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Human-Level Competitive Pok\&#39;emon via Scalable Offline Reinforcement Learning with Transformers</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.04395" target="_blank" rel="noreferrer">2504.04395</a></span>
        <span>作者: Grigsby, Jake, Xie, Yuqi, Sasek, Justin, Zheng, Steven, Zhu, Yuke</span>
        <span>日期: 2025/04/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>竞技宝可梦单打（CPS）是一个结合了象棋的长远规划、扑克的不完美信息与随机性的复杂策略游戏。该领域的AI研究此前主要由启发式树搜索（如定制模拟器）和在线自我对弈（如测试时蒙特卡洛树搜索）主导。这些方法依赖于显式搜索或启发式规则，计算成本高，且难以直接利用历史积累的大量人类对战数据。本文针对这一痛点，提出了一个全新的视角：能否仅利用离线的人类对战数据，训练无需任何显式搜索的大型序列模型，使其学会适应对手并达到人类竞技水平？本文的核心思路是：从第三方视角的对战日志中重建出智能体的第一人称视角轨迹，构建大规模离线数据集，并在此数据集上通过模仿学习、离线强化学习及其变体，训练Transformer策略网络，使其在高度随机、部分可观测的早期世代宝可梦对战中达到顶尖人类玩家水平。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程分为两大阶段：数据集构建与模型训练。</p>
<p><strong>第一阶段：离线数据集构建</strong>。数据来源于宝可梦对战网站Pokémon Showdown（PS）保存的超过十年的对战日志（“回放”）。关键挑战在于，PS回放记录的是第三方观战视角（双方信息全知），而智能体决策需要第一人称视角（部分观测）。为此，作者开发了一个重建流水线：1) 根据PS API从观战视角模拟战斗状态；2) 利用对战过程中逐步揭示的信息估计双方队伍的初始配置；3) 战斗结束后，结合社区统计的宝可梦使用率数据，推断从未被揭示的队伍信息；4) 为选定视角的玩家回填推断出的队伍名单，重建其决策时所能观察到的信息；5) 将重建的轨迹转换为与在线模拟器完全相同的格式。该过程包含保守的检查机制，会丢弃进入模糊状态的轨迹。最终构建了包含47.5万场战斗（约95万条轨迹、3800万时间步）的早期世代（Gen1-4）数据集。</p>
<p><img src="https://arxiv.org/html/2504.04395v2/x2.png" alt="数据集摘要"></p>
<blockquote>
<p><strong>图3</strong>：数据集摘要。展示了离线数据集中战斗的格式分布（左）、玩家ELO评级分布（中）以及战斗长度（以智能体时间步计，右）。</p>
</blockquote>
<p><strong>第二阶段：基于序列模型的离线RL训练</strong>。本文将CPS视为一个贝叶斯RL或元RL问题，将对手的策略和队伍构成视为环境的潜在变量。模型采用黑盒框架，使用序列模型根据当前战斗的完整历史来隐含地推断对手信息，并据此决策。</p>
<p><img src="https://arxiv.org/html/2504.04395v2/figures/arch_v6_safe.png" alt="模型概述"></p>
<blockquote>
<p><strong>图4</strong>：模型概述。每个回合的观察、行动和奖励由Transformer编码器处理，生成回合表示。这些表示随后输入一个因果Transformer，该Transformer输出行动者（策略）和评论家（Q值）的表示。</p>
</blockquote>
<p><strong>模型架构与输入</strong>：策略网络基于Transformer。输入是当前战斗的历史序列。观察空间是一个包含87个文本标记（描述战场状态，如宝可梦、技能、状态变化）和48个数值特征（如血量、属性）的混合表示。<strong>关键设计是，观察中仅包含对手当前活跃的宝可梦信息，对手的完整队伍构成必须依靠模型记忆历史回合来推断</strong>。行动空间是9个离散动作（4个技能，5个切换宝可梦）。奖励函数以胜负二元奖励为主，辅以造成伤害和恢复血量的小幅塑形奖励。</p>
<p><strong>训练目标</strong>：采用行动者-评论家框架，损失函数为公式(2)的一般形式，包含一个加权的行为克隆项和一个离线的策略梯度项（最大化期望Q值）。通过配置权重函数 w(h,a) 和系数 λ，研究从纯模仿学习到不同形式的离线RL的渐进过程。</p>
<table>
<thead>
<tr>
<th align="left">模型名称</th>
<th align="left">w(h,a)=</th>
<th align="left">λ</th>
</tr>
</thead>
<tbody><tr>
<td align="left">“IL”</td>
<td align="left">1</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">“Exp” (或 “RL”)</td>
<td align="left">exp(βA^π(h,a)) (裁剪后)</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">“Binary”</td>
<td align="left">A^π(h,a) &gt; 0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">“Binary+MaxQ”</td>
<td align="left">A^π(h,a) &gt; 0</td>
<td align="left">&gt;0</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：行动者损失配置。优势函数A^π(h,a)由评论家网络估计。</p>
</blockquote>
<p>创新点主要体现在：1) <strong>数据重建流水线</strong>，解锁了海量自然发生的人类对战数据用于离线RL；2) <strong>完全搜索免费</strong>，仅依靠序列模型的长时记忆和端到端训练来应对部分可观测性和对手建模；3) <strong>针对性的训练策略</strong>，从模仿学习平滑过渡到离线RL，并探索使用分布外（OOD）的自对弈数据进行微调以进一步提升性能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在Pokémon Showdown的早期四个世代（Gen1-4）的“OverUsed”竞争格式中进行实验。使用构建的离线数据集（95万条轨迹）进行训练。</li>
<li><strong>Baseline方法</strong>：对比了多种基线，包括：a) 一系列启发式对手（评估核心游戏知识）；b) 基于RNN的最佳模仿学习模型；c) 最近的LLM智能体方法；d) 最强的启发式搜索引擎。</li>
<li><strong>评估平台</strong>：使用<code>poke-env</code>与本地PS服务器及公开网站交互。使用三种队伍集进行评估：多样性集合（程序生成）、回放集合（基于真实对战推断）、竞争集合（专家设计的样例队伍）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>启发式评估</strong>：使用6个启发式对手的平均胜率构成“启发式综合得分”。Transformer模型在此基准上表现出色，并显示出从OU（数据多）到NU（数据少）格式的性能下降，揭示了数据可用性的影响。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.04395v2/x4.png" alt="启发式综合得分"></p>
<blockquote>
<p><strong>图6</strong>：不同模型在多样性队伍集上对6个启发式对手的平均胜率（综合得分）。Transformer模型显著优于RNN基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.04395v2/x5.png" alt="OU到NU"></p>
<blockquote>
<p><strong>图7</strong>：从OU到NU格式的启发式综合得分下降，反映了数据量差异对模型性能的影响。</p>
</blockquote>
<ol start="2">
<li><strong>模型规模与训练目标</strong>：实验了15M、50M、200M参数规模的Transformer。离线RL目标（“Exp”，“Binary”等）显著优于纯模仿学习（“IL”）。模型规模增大带来性能提升，尤其在长期规划（使用多个折扣因子γ）中更为明显。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.04395v2/x6.png" alt="多γ策略"></p>
<blockquote>
<p><strong>图8</strong>：训练时使用多个价值视野（γ），专注于长期规划的模型在启发式综合得分上表现更好。</p>
</blockquote>
<ol start="3">
<li><strong>在线人类对战性能</strong>：将训练好的智能体匿名部署到PS上，与真实人类玩家对战。<ul>
<li><strong>仅使用离线数据</strong>：最大的RL策略（200M参数）被官方GXE指标估计有 <strong>41% 到 58%</strong> 的几率击败随机抽样的对手（具体取决于世代）。</li>
<li><strong>加入自对弈微调后</strong>：使用故意不现实的、明确OOD的自对弈数据进行离线微调，智能体胜率提升至 **64% 到 80%<strong>，成功跻身活跃玩家的</strong>前10%**，并登上全球排行榜。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2504.04395v2/x10.png" alt="在线胜率与排名"></p>
<blockquote>
<p><strong>图12</strong>：展示了不同模型在在线对战中的胜率（GXE估计）和达到的排名百分位。经过自对弈微调的“Large RL (SP)”模型在四个世代中均达到最高水平。</p>
</blockquote>
<ol start="4">
<li><strong>与强基线的对比</strong>：<ul>
<li><strong>LLM智能体</strong>：在早期世代的长对局中缺乏竞争力。</li>
<li><strong>启发式搜索引擎</strong>：本文最好的智能体（经过自对弈微调）达到或超越了最强的启发式搜索引擎的水平。</li>
</ul>
</li>
</ol>
<p><strong>消融实验</strong>：实验对比了表1中的多种损失函数配置，发现不同离线RL变体之间的性能差异不显著，但它们都一致地优于纯模仿学习。自对弈微调被证明是性能提升的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>创建了大规模、高质量的离线RL数据集</strong>：通过创新的重建流水线，将第三方视角的宝可梦对战日志转化为智能体的第一人称视角轨迹，为研究基于数据的策略学习提供了宝贵资源。</li>
<li><strong>证明了搜索免费方法的有效性</strong>：仅使用大型Transformer序列模型和离线RL，无需任何显式树搜索或复杂启发式，即可在复杂的、部分可观测的竞技游戏中达到人类顶尖水平。</li>
<li><strong>实现了人类级别的竞技性能</strong>：智能体在最具挑战性的早期世代宝可梦对战中，胜率跻身所有活跃玩家的前10%，验证了离线RL结合大规模数据与强大序列模型的潜力。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>数据重建的“模拟到现实”差距</strong>：从观战日志重建智能体视角的过程可能引入错误或信息损失，导致训练环境与真实在线环境存在差距。</li>
<li><strong>领域特定性</strong>：重建流水线严重依赖PS的特定机制和社区数据，方法通用性受限。</li>
<li><strong>可能过拟合于早期世代</strong>：实验集中于Gen1-4，智能体在现代世代（有“队伍预览”功能）的表现未充分探索。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>展示了如何利用互联网上大量存在的非标准格式历史数据（如游戏日志、医疗记录、金融时间序列）来构建离线RL数据集，开辟了新的应用方向。</li>
<li>证明了在足够大的数据规模下，纯粹的、基于记忆的模型免费方法可以替代复杂的规划算法，为处理部分可观测、长视野的决策问题提供了新思路。</li>
<li>探索了使用分布外的自生成数据（如故意不现实的自对弈）来微调和提升离线训练策略的有效性，这为突破离线数据分布限制提供了实用策略。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何让AI在复杂策略游戏《宝可梦》单打对战（CPS）中达到人类竞技水平。核心问题是利用大规模离线数据训练无需显式搜索的适应策略。关键技术包括：构建从观战日志重建智能体第一人称视角的数据管道，解锁十余年人类对战记录；采用基于Transformer的序列模型进行黑盒训练，仅依靠输入轨迹适应对手并决策。实验表明，通过从模仿学习到离线强化学习再到自博弈微调的渐进方法，所得智能体超越了现有LLM智能体和启发式搜索引擎，在匿名在线对战中排名进入活跃玩家前10%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.04395" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>