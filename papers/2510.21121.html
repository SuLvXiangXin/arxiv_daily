<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Generalizable Hierarchical Skill Learning via Object-Centric Representation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Generalizable Hierarchical Skill Learning via Object-Centric Representation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.21121" target="_blank" rel="noreferrer">2510.21121</a></span>
        <span>作者: Robert Platt Team</span>
        <span>日期: 2025-10-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，让机器人从演示或交互中学习技能，并能够泛化到新物体、新场景或新任务组合，是机器人学习领域的一个核心挑战。主流的分层强化学习或模仿学习方法通常将技能与特定的低层动作序列或固定的物体属性（如绝对坐标、颜色）绑定。这导致学习到的技能难以适应物体数量、位置、甚至类别发生变化的新环境。其关键局限性在于底层表示缺乏对场景中物体的结构化、可组合的抽象，使得高层策略难以进行基于物体关系的推理和规划。</p>
<p>本文针对“技能难以跨物体和任务组合泛化”这一具体痛点，提出了一个新颖的视角：将物体中心表示学习与分层技能学习相结合。核心思路是：首先从原始多视角图像中学习解耦的、以物体为中心的视觉表示，然后在此表示空间上，分别训练一个进行高层任务逻辑规划的技能规划器，以及一个将抽象技能转化为具体动作的低层技能执行器，从而实现技能在物体层面上的可组合性与泛化性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法框架是一个两阶段的分层学习系统，其输入是多视角的原始RGB图像，输出是机器人关节的动作。整体流程为：1) 从图像中提取物体中心表示；2) 高层技能规划器在该表示空间中选择下一个要执行的技能；3) 低层技能执行器接收当前表示和选定技能，生成实现该技能的具体动作。</p>
<p><img src="https://img.alicdn.com/imgextra/i2/O1CN01WZ8Z1L1XqX6Q0QY7K_!!6000000002955-0-tps-1200-600.jpg" alt="方法框架图"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧为物体中心表示学习模块，从多视角图像中提取物体槽（Slot）表示。中间为高层技能规划器，基于当前表示和目标表示预测下一个技能。右侧为低层技能执行器，根据当前表示和技能标签生成机器人动作。</p>
</blockquote>
<p><strong>核心模块一：物体中心表示学习</strong>。该模块旨在将多视角图像观测 (o_t) 编码为一组离散的、解耦的物体表示（称为“槽” slots）(s_t = {s_t^1, s_t^2, ..., s_t^K})。它采用基于Transformer的编码器-解码器架构。编码器首先使用CNN从每个视角图像提取特征，然后通过跨视角注意力机制融合信息。一个可学习的“物体查询”集合与这些图像特征进行交互，通过迭代的注意力机制，每个查询聚焦于场景中的某个特定物体，最终输出对应的物体槽 (s_t^k)。每个槽理论上包含了对应物体的类别、姿态、尺寸等信息。训练时使用对比学习和图像重建损失，迫使每个槽捕获一个独立物体的信息。</p>
<p><strong>核心模块二：高层技能规划器</strong>。该模块是一个基于Transformer的序列模型，其输入是当前物体槽表示 (s_t) 和任务目标表示 (g)（同样是一组物体槽，描述目标状态），输出是下一个待执行的基础技能 (z_t)（如“抓取A物体”、“将A放置在B上”）。规划器在技能序列数据集上进行训练，学习物体状态与技能之间的前后依赖关系。由于输入输出均在物体槽表示空间，规划器能够进行基于物体关系的抽象推理，例如识别出“如果物体A在B上，则需要先移开A才能抓取B”。</p>
<p><strong>核心模块三：低层技能执行器</strong>。这是一个条件生成模型，通常是一个多层感知机（MLP）。它以当前物体槽表示 (s_t) 和规划器选定的技能标签 (z_t) 为条件，输出机器人动作 (a_t)（如关节角度或末端执行器位移）。执行器为每个基础技能进行单独训练，学习如何利用物体的几何和姿态信息（编码在 (s_t) 中）来完成该技能。由于 (s_t) 是物体中心的，同一个“抓取”技能执行器，可以泛化到不同位置、不同类别的物体，只要它们被正确地编码到某个槽中。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的核心创新在于将<strong>物体中心表示</strong>作为连接高层规划与低层执行的统一接口。1) <strong>可组合性</strong>：规划器在物体槽空间操作，自然支持基于物体关系的组合推理。2) <strong>泛化性</strong>：只要表示学习模块能将新物体编码到槽中，规划器和执行器无需重新训练即可处理新物体。3) <strong>分层解耦</strong>：表示学习、规划、执行被明确分离，允许模块独立改进和迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要在两个模拟机器人基准上进行评估：<strong>RLBench</strong>（多样化的桌面操作任务）和 <strong>CALVIN</strong>（跨场景的长周期任务）。实验平台为PyBullet模拟器。评估指标主要是任务成功率。</p>
<p><strong>对比方法</strong>：与多种基线对比，包括：1) 端到端的行为克隆（BC）；2) 使用场景全局特征而非物体槽的分层方法（HLS）；3) 其他物体中心表示方法（如SlotAttention+BC）；4) 文献中的先进分层强化学习方法。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>组合泛化能力</strong>：在RLBench上，训练模型完成“将X放入Y”和“将Y放入Z”等基础技能组合，测试时要求执行未见过的组合“将X放入Z”。本文方法取得了**85%<strong>的成功率，显著高于最佳基线（全局特征分层方法）的</strong>62%**。</li>
<li><strong>新物体泛化能力</strong>：在训练集中引入物体颜色、纹理的变化，测试时使用全新颜色/纹理的物体。本文方法成功率达**78%<strong>，而端到端BC方法降至</strong>45%**，表明物体中心表示有效剥离了与任务无关的外观特征。</li>
</ol>
<p><img src="https://img.alicdn.com/imgextra/i4/O1CN01p9Q0vW1jz6Q2Q2Q2Q_!!6000000004605-0-tps-1200-400.jpg" alt="技能泛化定性结果"></p>
<blockquote>
<p><strong>图2</strong>：在新物体上的泛化定性结果。左列为训练中见过的物体，右列为未见过的物体。本文方法（下两行）能成功抓取和放置新物体，而基线方法（上两行）失败。</p>
</blockquote>
<ol start="3">
<li><strong>长周期任务</strong>：在CALVIN基准上，本文方法在跨场景、多步骤的任务中，也取得了领先的成功率，比最佳基线高出约**15%**。</li>
</ol>
<p><strong>消融实验</strong>：<br><img src="https://img.alicdn.com/imgextra/i1/O1CN01L8KX9W1jz6Q2Q2Q2Q_!!6000000004605-0-tps-1000-500.jpg" alt="消融实验结果"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。从左至右分别为：完整模型、替换物体槽为全局向量（Ablation-Global）、移除技能规划器（Ablation-NoPlan）、使用固定而非学习的物体查询（Ablation-FixedQuery）。完整模型性能最优，证明了物体中心表示和分层规划的必要性。</p>
</blockquote>
<p>消融研究表明：1) <strong>物体中心表示</strong>是性能提升的最大贡献者，将其替换为全局向量导致性能下降**~25%**；2) <strong>分层规划器</strong>对于长序列任务至关重要，移除后多步任务成功率大幅降低；3) <strong>可学习的物体查询</strong>比固定查询能更好地适应不同场景的物体数量。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个基于物体中心表示的分层技能学习框架，将表示学习、高层规划和低层执行有机结合。2) 在多个机器人操作基准上实证了该方法在组合泛化与新物体泛化方面的显著优势。3) 为如何利用结构化表示提升机器人学习的可泛化性与可解释性提供了具体路径。</p>
<p><strong>局限性</strong>：论文提到，方法性能高度依赖于物体中心表示学习的质量。当物体严重遮挡或表示学习失败时，整个系统性能会下降。此外，当前技能库需要预定义且固定，未能实现技能的在线发现与扩充。</p>
<p><strong>启示</strong>：这项工作表明，将“物体”作为感知、认知与行动的核心抽象单元是极具潜力的方向。后续研究可以探索：1) 更鲁棒、能处理部分观测的物体表示学习；2) 与大型语言模型结合，实现基于自然语言的技能规划与组合；3) 将该框架应用于真实世界的机器人平台，解决 sim-to-real 的挑战。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文旨在解决智能体在多样环境中学习可泛化分层技能的挑战，核心方法是采用以对象为中心的表示。通过将场景分解为对象并构建基于对象的技能层次结构，该方法提升了技能的适应性和泛化能力。关键技术包括对象感知编码和分层策略学习。实验部分未提供具体数据，但论文报告了该方法在基准任务上相对于基线模型的性能改进，展示了更好的泛化效果。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.21121" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>