<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Toward Embodiment Equivariant Vision-Language-Action Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Toward Embodiment Equivariant Vision-Language-Action Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.14630" target="_blank" rel="noreferrer">2509.14630</a></span>
        <span>作者: Chen, Anzhe, Yang, Yifei, Zhu, Zhenjie, Xu, Kechun, Zhou, Zhongxiang, Xiong, Rong, Wang, Yue</span>
        <span>日期: 2025/09/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作策略通过大规模预训练学习跨任务、跨环境、跨具身的操作技能。然而，它们泛化到新机器人配置的能力仍然有限。大多数现有方法强调模型大小、数据集规模和多样性，而对动作空间的设计关注较少。这导致了配置泛化问题，即当面对新的机器人配置时，策略往往失效，需要昂贵的适应成本。现有方法（如RT-1、ACT、Octo、RT-2、OpenVLA等）主要采用关节空间动作或基座坐标系下的末端执行器位姿作为动作表示，这使得整体策略对具身配置变换不具备等变性，从而导致预训练过程中过拟合到特定具身，阻碍了任务级知识的共享。</p>
<p>本文针对VLA策略在跨具身预训练中难以泛化到新机器人配置（如机器人基座和末端执行器坐标定义的变化）这一具体痛点，提出了一个新的视角：将跨具身学习形式化为设计对具身配置变换具有等变性的策略。本文的核心思路是：建立一个具身等变性理论来统一跨具身的动作空间设计，并引入一个强制配置等变的动作解码器和一个几何感知的网络架构，使策略能专注于任务级知识而非具身特定细节，从而实现更有效的预训练和更高效的微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架将策略分解为两个部分：一个对配置变换不变的学习型策略网络 $\phi_{\theta}$，和一个对配置变换等变的解析解码器 $\mathcal{D}$。输入为图像观测 $\mathcal{I}$、语言指令 $\mathcal{L}$ 和具身配置 $\mathbf{m}$（定义为末端执行器在基座下的位姿和相机在基座下的位姿的组合）。输出为期望的末端执行器在基座下的位姿 $\mathbf{a}$。策略表示为 $\pi_{\theta}(\mathbf{m},\mathcal{I},\mathcal{L}) = \mathcal{D}(\mathbf{m}, \phi_{\theta}(\mathbf{m},\mathcal{I},\mathcal{L}))$。</p>
<p><img src="https://arxiv.org/html/2509.14630v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：E2VLA 策略的整体框架。策略网络 $\phi_{\theta}$ 以图像、语言和配置为输入，预测一个在相机坐标系中定义的相对位姿变换 $\mathbf{y}$。等变解码器 $\mathcal{D}$ 随后利用当前的配置 $\mathbf{m}$ 将 $\mathbf{y}$ 转换为基座坐标系下的目标末端执行器位姿 $\mathbf{a}$。</p>
</blockquote>
<p>核心模块一：<strong>等变动作解码器</strong>。其设计基于理论推导，形式为 $\mathcal{D}([\mathchoice{\mathop{}\kern 4.07375pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 4.07375pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 3.74713pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 3.74713pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{c}}}\mathbf{T},\mathchoice{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{e}}}\mathbf{T}],\phi_{\theta}) = \mathchoice{\mathop{}\kern 4.07375pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 4.07375pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 3.74713pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 3.74713pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{c}}}\mathbf{T} \ \phi_{\theta} \mathchoice{\mathop{}\kern 4.07375pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 4.07375pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 3.74713pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 3.74713pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{c}}}\mathbf{T}^{-1} \mathchoice{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}<em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{b}}}</em>{\mathmakebox[0pt][r]{e}}}\mathbf{T}$。该解码器是解析的，能保证对任何具身配置变换 $\mathbf{g} \in \mathcal{G}$ 都具有等变性，即 $\mathcal{D}(\mathbf{g} \circ \mathbf{m}, \phi_{\theta}) = \mathbf{g} \circ \mathcal{D}(\mathbf{m}, \phi_{\theta})$。这使得学习目标 $\phi_{\theta}$ 变为在相机坐标系中定义的相对运动 $\mathbf{y} = \mathchoice{\mathop{}\kern 8.4747pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}<em>{\mathmakebox[0pt][r]{e</em>{<em>}}}}{\mathop{}\kern 8.4747pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}<em>{\mathmakebox[0pt][r]{e</em>{</em>}}}}{\mathop{}\kern 8.06567pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}<em>{\mathmakebox[0pt][r]{e</em>{<em>}}}}{\mathop{}\kern 8.06567pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}<em>{\mathmakebox[0pt][r]{e</em>{</em>}}}}\mathbf{T} \mathchoice{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{e}}}<em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{e}}}</em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{e}}}<em>{\mathmakebox[0pt][r]{c}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{e}}}</em>{\mathmakebox[0pt][r]{c}}}\mathbf{T}$，它应具有配置不变性。</p>
<p><img src="https://arxiv.org/html/2509.14630v1/x1.png" alt="动作空间设计对比"></p>
<blockquote>
<p><strong>图1</strong>：VLA策略学习的不同动作空间设计。本文学习在相机空间中投影的相对末端执行器运动。</p>
</blockquote>
<p>核心模块二：<strong>几何感知网络架构</strong>。为了使策略网络 $\phi_{\theta}$ 在保留对配置信息的感知能力的同时实现不变性，论文设计了专门的架构。它通过将当前末端执行器在相机下的位姿 $\mathchoice{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}<em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}</em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}<em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}</em>{\mathmakebox[0pt][r]{e}}}\mathbf{T}$ 作为几何标记注入到视觉Transformer中来实现。具体而言，将 $\mathchoice{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}<em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 4.29411pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}</em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}<em>{\mathmakebox[0pt][r]{e}}}{\mathop{}\kern 3.88509pt\mathopen{\vphantom{}}^{\mathmakebox[0pt][l]{\mathmakebox[r]{c}}}</em>{\mathmakebox[0pt][r]{e}}}\mathbf{T}$ 转换为旋转和平移向量，经过一个MLP后与视觉特征拼接，然后输入到Transformer块中进行跨模态融合。这种设计使网络能够进行与具身无关的空间推理。</p>
<p>与现有方法相比，创新点在于：1) 提出了一个形式化的具身等变性理论，为动作空间设计提供了原则性指导；2) 通过一个解析的等变解码器，将学习目标统一为相机坐标系下的相对运动，从架构上保证了配置泛化能力，避免了为每个新具身训练逆动力学模型；3) 设计了能感知几何配置同时保持学习目标不变性的网络架构。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真和真实世界环境中进行。使用了Open-X数据集（包含RT-1、Bridge、Libra等）进行预训练。评估任务包括模拟器中的“Pick”、“Stack”、“Pour”等任务，以及真实机器人上的“Pick-and-Place”、“Tool Use”等任务。对比的基线方法包括RT-1、ACT、Octo、RDT-1B、$\pi_0$ 以及 UniVLA。</p>
<p><img src="https://arxiv.org/html/2509.14630v1/x4.png" alt="预训练性能对比"></p>
<blockquote>
<p><strong>图4</strong>：在模拟任务上，E2VLA与基线方法的预训练性能对比。E2VLA在所有任务上均取得最佳或接近最佳的平均成功率。</p>
</blockquote>
<p>关键实验结果：在模拟器跨任务评估中，E2VLA在Pick、Stack、Pour任务上的平均成功率分别达到95.7%、90.0%、78.3%，综合性能优于所有基线。在从仿真到真实世界的零样本迁移中，E2VLA在6个真实任务上平均成功率为68.3%，显著高于RT-1（31.7%）和Octo（45.0%）。</p>
<p><img src="https://arxiv.org/html/2509.14630v1/x5.png" alt="零样本迁移性能"></p>
<blockquote>
<p><strong>图5</strong>：从仿真预训练到真实世界的零样本迁移性能。E2VLA在大多数任务上表现最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.14630v1/x6.png" alt="微调效率对比"></p>
<blockquote>
<p><strong>图6</strong>：在新具身上进行微调时的数据效率对比。E2VLA仅需少量演示（如10条）即可达到高性能，而基线方法需要多得多的数据。</p>
</blockquote>
<p>在向新具身（如不同的机械臂或夹爪）高效微调的实验中，E2VLA展现出卓越的数据效率。例如，在Franka机器人上，仅用10条演示数据微调后成功率可达93.3%，而RT-1需要100条数据才能达到73.3%。</p>
<p><img src="https://arxiv.org/html/2509.14630v1/x7.png" alt="消融实验：配置不变性"></p>
<blockquote>
<p><strong>图7</strong>：消融研究：验证配置不变性对预训练有效性的影响。使用不变学习目标（Ours）比使用非不变目标（Base-frame）性能更好。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.14630v1/x8.png" alt="消融实验：网络架构"></p>
<blockquote>
<p><strong>图8</strong>：消融研究：几何感知架构（Ours）与基线架构（w/o Geo）的对比。几何感知设计带来了性能提升。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：1) <strong>配置不变的学习目标</strong>（通过等变解码器实现）是提升预训练有效性和零样本泛化能力的关键；2) <strong>几何感知的网络架构</strong>进一步提升了策略的空间推理能力和性能；3) 两者结合才能达到最佳效果。</p>
<p><img src="https://arxiv.org/html/2509.14630v1/x9.png" alt="不同配置变换下的泛化"></p>
<blockquote>
<p><strong>图9</strong>：在预训练后，对未见过的基座和末端执行器坐标变换的零样本泛化能力评估。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.14630v1/x10.png" alt="真实机器人任务定性结果1"></p>
<blockquote>
<p><strong>图10</strong>：真实机器人任务“Pick and Place”的定性结果。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.14630v1/x11.png" alt="真实机器人任务定性结果2"></p>
<blockquote>
<p><strong>图11</strong>：真实机器人任务“Tool Use”的定性结果。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.14630v1/x12.png" alt="真实机器人任务定性结果3"></p>
<blockquote>
<p><strong>图12</strong>：真实机器人任务“Pour”的定性结果。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.14630v1/x13.png" alt="真实机器人任务定性结果4"></p>
<blockquote>
<p><strong>图13</strong>：真实机器人任务“Stack”的定性结果。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个用于跨具身预训练的<strong>政策等变性理论</strong>，为统一动作空间设计提供了数学框架；2) 设计了一个<strong>等变的动作解码器</strong>，从架构上保证了策略对具身配置变换的等变性，使学习目标变为配置不变的相对运动；3) 引入了一个<strong>几何感知的网络架构</strong>，增强了策略在等变约束下的与具身无关的空间推理能力。</p>
<p>论文自身提到的局限性包括：所提出的方法依赖于对相机外参的访问，这在某些设置中可能是一个限制；此外，为了强制不变性，可能会丢失一些对任务执行有用的具身特定信息（如关节极限）。</p>
<p>本文的启发在于：将等变性原理系统地应用于机器人学习，为解决配置泛化问题提供了一个有原则性的方向。未来的研究可以探索在更松散的假设下（如未知或变化的相机参数）实现等变性，或者设计更灵活的网络架构来有条件地利用必要的具身特定信息，同时保持核心的泛化能力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作策略在跨具身预训练后对新机器人配置泛化能力有限、导致适应成本高的问题，提出具身等变性框架。该框架基于等变性理论，设计配置等变动作解码器以强制策略对配置变换保持等变，并融入几何感知网络增强具身无关的空间推理。在仿真和真实环境的大量实验中，该方法提升了预训练效果，并支持对新机器人配置的高效微调。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.14630" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>