<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.14441" target="_blank" rel="noreferrer">2508.14441</a></span>
        <span>作者: Cewu Lu Team</span>
        <span>日期: 2025-08-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧手内操作（即物体在单只灵巧手中的重定位）由于复杂的接触动力学和部分可观测性，一直是机器人学习领域的难题。人类能够轻松完成此类任务，得益于视觉与触觉的协同：视觉追踪物体的全局状态以完成任务，而触觉感知则能在接触过程中实现精确的力调整。现有的机器人方法通常优先考虑单一模态，要么是视觉，要么是触觉，这限制了它们在复杂操作场景中的有效性。尽管最近的视触觉方法展示了更高的操作稳定性，但大多依赖于光学触觉传感器，其体积庞大（通常厚度≥8mm）阻碍了在全手关节（如掌骨关节）上的部署。分布式触觉阵列（如压阻阵列）提供了一个实用的替代方案，具有与商业灵巧机械手硬件兼容、全手掌覆盖以及足够的接触力分辨率以检测初始滑移等优点。然而，在策略学习框架中，将分布式触觉数据与视觉流融合的研究仍然不足。</p>
<p>现有的视触觉融合方法通常通过静态融合处理多模态输入，例如引入编码特征或将单帧的触觉接触坐标与视觉点云增强。这些方法忽略了动态操作过程中触觉交互与物体状态转变之间固有的因果关系：触觉力驱动物体状态变化。因此，本文采用了动态融合的路径，通过一个动态感知的潜在模型从时间上的物体运动流中提取触觉信息。这种表示使得方法在实践中具有两种操作模式：（1）仅视觉模式：无需物理传感器即可推断触觉线索；（2）视触觉模式：当有触觉传感器时，可使用测量的接触力来细化流预测以提高精度。因此，该方法可以灵活部署到更广泛的应用场景中，甚至在触觉传感器不可用时也能工作。</p>
<p>本文的核心思路是：提出一种名为“模仿前流”（Flow Before Imitation， FBI）的视触觉模仿学习框架，通过物体运动动力学动态地融合触觉与视觉线索，并利用一个一步生成的扩散策略实现实时控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>FBI的整体框架旨在处理灵巧手内操作所需的多模态输入，并动态融合视觉与触觉信息以生成动作序列。</p>
<p><img src="https://arxiv.org/html/2508.14441v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：FBI方法流程总览。(a) 整体流程，包括多模态编码器、视触觉特征融合模块和策略生成模块。(b) Flow2Tactile模块，用于预测触觉读数，使其兼容仅视觉场景。</p>
</blockquote>
<p><strong>整体流程</strong>：FBI的输入包括两个连续的机器人状态 $s_{t-1}, s_t \in \mathbb{R}^{N_s}$，部分观测的点云帧 $P_{t-1}, P_t \in \mathbb{R}^{N_p \times 3}$ 以及触觉读数 $R_t \in \mathbb{R}^{N_r}$。这些多模态数据首先由多模态编码器处理。编码后的特征被动态融合，并作为条件输入到一个shortcut模型中以预测动作序列 $\mathbf{A}_t \in \mathbb{R}^{H \times d_a}$。输出是未来 $H$ 步的动作序列。</p>
<p><strong>核心模块</strong>：</p>
<ol>
<li><p><strong>多模态编码器</strong>：</p>
<ul>
<li><strong>机器人状态编码器</strong>：使用一个3层MLP将连续的两个机器人本体感知状态编码为紧凑的状态特征 $\mathcal{F}_s$。</li>
<li><strong>视觉编码器</strong>：对两帧手和物体的点云进行裁剪和下采样，通过一个轻量级MLP（包含一个三层MLP、一个最大池化函数和一个用于降维的输出MLP层）编码为视觉特征 $\mathcal{F}_v$。</li>
<li><strong>触觉编码器</strong>：使用一个四层MLP将触觉读数（在仅视觉模式下来自Flow2Tactile模块的预测，在视触觉模式下来自物理传感器）编码为触觉特征 $\mathcal{F}_{tac}$。</li>
</ul>
</li>
<li><p><strong>Flow2Tactile模块</strong>：此模块是实现动态融合和双模式兼容的关键。它旨在从物体状态流中预测密集的接触状态。首先，利用一个shortcut模型根据两帧点云 $P_{t-1}$ 和 $P_t$ 预测物体状态流 $f_{t-1 \rightarrow t}$，该流表示 $P_{t-1}$ 如何变换到 $P_t$。然后，一个预训练的Transformer搜索模型 $T_s(\cdot)$ 利用该流和当前接触关键点坐标 $K_t$ 来预测触觉读数 $R_t$。接触关键点的布局设计旨在全面覆盖手掌侧，并参考真实世界传感器的物理参数，以便通过建立真实传感器布局与预定义关键点布局之间的映射来适配真实传感器。例如，对于Shadow Hand，使用了456个关键点（每个手指连杆12个，手掌288个）。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.14441v1/x3.png" alt="接触关键点设计"></p>
<blockquote>
<p><strong>图3</strong>：为不同灵巧手设计的接触关键点布局：(a) Shadow Hand，(b) Allegro Hand，(c) Mano Hand。</p>
</blockquote>
<ol start="3">
<li><strong>视触觉策略生成</strong>：<ul>
<li><strong>特征融合</strong>：编码后的视觉特征 $\mathcal{F}<em>v$ 和触觉特征 $\mathcal{F}</em>{tac}$ 被送入一个Transformer融合模块 $T_f(\cdot)$。该模块以 $\mathcal{F}<em>v$ 为模板特征，$\mathcal{F}</em>{tac}$ 为搜索特征，输出视触觉特征。考虑到视觉观测对环境感知也至关重要，最终将视触觉特征与视觉特征拼接，形成最终的融合特征 $\mathcal{F}_{fuse}$。</li>
<li><strong>策略生成模型</strong>：采用一个基于流匹配（flow matching）的shortcut模型进行一步采样生成动作。条件 $C$ 由融合特征 $\mathcal{F}_{fuse}$ 和机器人状态特征 $\mathcal{F}_s$ 组成。在采样阶段，给定条件 $C$ 和推理步长 $dt=1$，模型通过单次前向传播，从高斯噪声 $\mathbf{A}^0$ 直接预测出清晰的动作序列 $\mathbf{A}_t$：$\mathbf{A}_t = \mathbf{A}^0 + v_\theta(\mathbf{A}^0, 0, 1, \mathcal{F}<em>s, \mathcal{F}</em>{fuse})$，其中 $v_\theta(\cdot)$ 是速度预测网络（U-Net）。</li>
<li><strong>训练目标</strong>：遵循shortcut模型的训练思想，同时优化流匹配损失 $\mathcal{L}<em>{FM}$ 和自一致性损失 $\mathcal{L}</em>{SC}$，以实现高质量的一步生成。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有静态融合方法相比，FBI的核心创新在于其<strong>动态融合视角</strong>——通过物体运动流来因果性地推断触觉交互。这使得方法不仅能融合真实的触觉读数（视触觉模式），还能在缺乏物理传感器时从视觉流中预测触觉信息（仅视觉模式），极大地扩展了应用场景。此外，采用一步生成的shortcut模型实现了高效、实时的策略执行。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>任务与数据集</strong>：在两个定制的手内操作任务（重定向、推动）和三个来自公开基准Adroit的任务（开门、使用锤子、转笔）上评估FBI。使用了包含多种形状、重量和颜色的物体数据集（6个3D打印物体和3个日常物体）。</li>
<li><strong>机器人平台</strong>：在仿真和真实世界实验中均使用Shadow Hand灵巧手（24自由度）。</li>
<li><strong>对比基线</strong>：选择了四个先进的模仿学习方法作为基线：DP3（点云扩散）、ManiCM（点云一致性模型）、Ada-Flow（基于图像的流匹配）和Consistency Policy（基于图像的扩散一致性模型）。</li>
<li><strong>评估指标</strong>：使用成功率（SR）。对于定制任务，推动成功定义为物体与目标位置距离≤1mm，重定向成功定义为方向差≤0.1弧度。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在仿真实验中，FBI方法显著优于所有基线。</p>
<p><img src="https://arxiv.org/html/2508.14441v1/x9.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>表1</strong>：仿真实验结果。FBI在仅视觉和视触觉模式下，在绝大多数任务上取得了最佳成功率，平均成功率分别达到64.7%和66.5%，比之前的SOTA方法（DP3，48.1%）分别高出16.6%和18.4%。尤其是在困难的重定向任务上，优势更为明显。</p>
</blockquote>
<p>在真实世界实验中，FBI同样表现出色。</p>
<p><img src="https://arxiv.org/html/2508.14441v1/x10.png" alt="真实世界结果表"></p>
<blockquote>
<p><strong>表2</strong>：真实世界实验结果。FBI在所有任务上均优于基线，仅视觉和视触觉模式的平均成功率分别为33.5%和35.0%，相比之前最佳的基线ManiCM（18.5%）分别提高了15.0%和16.5%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14441v1/x8.png" alt="定性结果图"></p>
<blockquote>
<p><strong>图8</strong>：手内重定向和推动任务的真实世界定性结果。展示了仅视觉模式（左）和视触觉模式（右）下操作过程的五个关键帧及对应的触觉读数（热力图）。</p>
</blockquote>
<p><strong>消融实验与模块分析</strong>：<br>论文通过消融实验验证了各模块的贡献。主要结论包括：</p>
<ol>
<li><strong>Flow2Tactile模块的有效性</strong>：在仅视觉模式下，使用预测的触觉读数比完全不用触觉特征（即纯视觉）性能更优，证明了该模块能够从运动流中提取出有益的触觉线索。</li>
<li><strong>动态融合的优势</strong>：FBI的动态融合方法（通过Flow2Tactile）优于简单的静态融合（如直接拼接视觉和触觉特征编码）。</li>
<li><strong>触觉传感器的增益</strong>：当物理触觉传感器可用时（视触觉模式），性能相比仅使用预测触觉（仅视觉模式）有进一步提升，表明真实的触觉测量可以 refine 预测，提高操作精度。</li>
<li><strong>策略生成模型效率</strong>：采用一步生成的shortcut模型在保持高性能的同时，实现了快速的推理速度（NFE=1），适合实时控制。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.14441v1/x5.png" alt="Flow2Tactile预测结果"></p>
<blockquote>
<p><strong>图5</strong>：Flow2Tactile模块的定性结果对比。展示了仿真中的渲染画面、通过高精度接触仿真器ZeMa计算的地面真实触觉读数，以及由Flow2Tactile模块预测的触觉读数，两者具有很高的一致性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了动态视触觉融合方法FBI</strong>：首次通过物体运动流来动态推断触觉交互，建立了触觉信号与物体运动之间的因果联系，并利用一步扩散策略实现实时控制。</li>
<li><strong>实现了灵活的双模式操作</strong>：方法设计使其能够在有物理触觉传感器（视触觉模式）和没有物理传感器（仅视觉模式）两种情况下工作，极大地扩展了灵巧操作算法的应用场景和硬件兼容性。</li>
<li><strong>全面的实验验证</strong>：在包含定制任务和标准基准的五个任务上，于仿真和真实环境中进行了广泛评估，证明了FBI相对于多种先进基线的显著优越性。</li>
<li><strong>设计了通用的接触关键点布局</strong>：为多种灵巧手设计了覆盖手掌侧的接触关键点布局，便于适配不同的分布式触觉传感器。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，方法在真实世界部署时，其性能部分依赖于点云的质量（因此需要为金属手佩戴手套以减少反光）。此外，从高精度仿真（ZeMa）中获取触觉真值进行训练，与真实世界的触觉信号之间仍存在 sim-to-real 的差距。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>动态融合是更优的路径</strong>：对于接触丰富的操作任务，从系统动态（如物体运动流）中推理接触信息，比静态地融合单帧传感数据更具物理意义和潜力。</li>
<li><strong>算法应兼顾硬件可及性</strong>：设计能够兼容不同传感器配置（尤其是低成本或常见配置）的算法，可以降低研究与应用的门槛，促进技术的普及。</li>
<li><strong>高效策略生成的重要性</strong>：在保证性能的前提下，采用像shortcut模型这样的一步生成方法，对于需要高频控制的实时机器人系统至关重要。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对灵巧手内操作中复杂接触动力学与部分可观测性的挑战，提出FBI框架。该方法通过动态融合视觉与触觉信息，利用基于动力学的潜在模型建立触觉信号与物体运动间的因果关联，并采用基于Transformer的交互模块进行特征融合，训练一步扩散策略。实验表明，该方法在仿真与真实世界的多个灵巧操作任务上均优于基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.14441" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>