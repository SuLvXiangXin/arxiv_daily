<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.05953" target="_blank" rel="noreferrer">2512.05953</a></span>
        <span>作者: Kuan Fang Team</span>
        <span>日期: 2025-12-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，视觉运动控制领域的主流任务表示方法包括基于目标图像、语言指令以及物体或机器人的运动轨迹（flow）。然而，这些方法存在关键局限性：基于图像的目标可能过度指定场景，混淆无关背景；语言指令难以精确地落实到物理世界的具体动作；而现有的基于运动轨迹（flow）的方法大多局限于2D，存在深度模糊和对遮挡敏感的问题。即使扩展到3D的近期工作，也通常依赖于手工设计的运动基元或控制器，限制了其对新任务和环境的适应性，并且往往需要密集标注的轨迹，导致任务指定成本高且不灵活。</p>
<p>本文针对现有flow-conditioned方法在三维空间表达的灵活性、鲁棒性和通用性上的不足，提出了一种新颖的、基于3D关键点时空对应关系的任务表示新视角。该视角将每个任务定义为场景物体上选定的一组关键点的预期3D运动轨迹，并允许任务指定在空间（关键点数量）和时间（目标步数）粒度上灵活可变。本文的核心思路是：通过一个结合时空注意力机制的条件策略，将这种灵活的对应关系任务表示鲁棒地映射为可执行动作，并利用自监督的模仿学习流程进行可扩展的训练。</p>
<h2 id="方法详解">方法详解</h2>
<p>COIL的整体框架是一个条件模仿学习框架。在每一时间步t，策略接收当前观测（点云和本体感知）以及任务表示c，输出未来一段时间的动作序列。任务表示c ∈ ℝ^(H×K×3)定义了K个关键点在H个离散步骤的目标3D坐标。策略的目标是理解并执行该任务，使关键点按顺序达到这些目标位置。</p>
<p><img src="https://arxiv.org/html/2512.05953v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：COIL策略概述。策略使用共享的3D坐标编码器分别编码任务表示、跟踪的关键点和观测点云。时间信息通过归一化位置编码注入。一个时空Transformer通过交错执行时空自注意力和与视觉观测的交叉注意力，高效融合这些输入信息。得到的表示与本体感知结合，传递给一个流匹配头以生成多步动作。</p>
</blockquote>
<p>核心模块包括：1）<strong>灵活的任务表示</strong>：支持可变的空间粒度（任意K≥1个关键点，可位于多个物体上）和时间粒度（任意H≥2个目标步骤，仅要求按顺序到达，不固定到达时间）。2）<strong>时空注意力编码器</strong>：用于融合多模态信息。首先，使用共享权重的MLP分别编码点云xt、任务表示c和当前跟踪的关键点位置ut。对于c，需额外注入归一化的时间位置编码。然后，通过一个新颖的<strong>时空Transformer</strong>进行融合：该模块将编码后的ut和c沿时间轴拼接作为输入token，将点云特征作为上下文。每一层执行三步融合：首先在token的时间维度上进行自注意力，然后在空间维度（关键点间）进行自注意力，最后每一步自注意力后都执行一次与点云特征的交叉注意力。这种交错设计允许网络逐步推理任务进度、时空关系，并将运动计划锚定在具体的场景几何中。3）<strong>流匹配预测头</strong>：接收编码器的输出和机器人本体感知，采用类似UNet的结构建模多模态的动作分布。4）<strong>自监督训练流程</strong>：在仿真中收集多样化的演示数据，并通过<strong>后见之明对应关系估计</strong>自动生成任务表示标签——在每一幕结束后，根据预先选定并跟踪的关键点的真实3D运动轨迹进行重标注。同时，引入<strong>对应关系增强</strong>：在训练时，随机从密集的轨迹标签中采样子集（随机的关键点数量K和时间步数H），并给跟踪的关键点位置ut添加高斯噪声，以模拟部署时可能遇到的偏离和噪声，提升策略的鲁棒性和对灵活指定的适应性。</p>
<p>与现有方法相比，COIL的创新点主要体现在：1）提出了支持可变时空粒度的3D对应关系任务表示，比固定、密集的2D或3D flow更灵活。2）设计了时空注意力机制来鲁棒地处理这种稀疏、灵活的任务表示，并融合几何信息，而非依赖手工控制器或运动规划。3）构建了完全自监督的模仿学习流程，通过后见之明重标注自动生成训练所需的任务表示。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了基于DROID基准的真实世界桌面操作设置，涉及7自由度Franka机械臂和立体相机。评估任务包括<strong>Pick-and-Place</strong>、<strong>Sweeping</strong>和<strong>Folding</strong>，均使用分布外物体以测试泛化能力。任务表示按稀疏度分为Sparse（3-5关键点，2-5步）、Medium（8-12关键点，16步）和Dense（32关键点，32步）三种设置。对比的基线方法包括RT-Trajectory（2D末端轨迹条件）、Im2Flow2Act（2D物体流条件）和General-Flow（3D流生成+运动规划）。此外，还测试了结合视觉语言模型（VLM）生成任务表示后由COIL执行的效果。</p>
<p><img src="https://arxiv.org/html/2512.05953v1/x3.png" alt="任务执行"></p>
<blockquote>
<p><strong>图3</strong>：任务执行示例。从左至右分别为每个评估任务给策略的输入指定、策略的执行过程以及机器人实现的末端轨迹。COIL策略展示了灵活适应场景物体的行为，例如在Pick-and-Place中旋转夹爪以避免碰撞，在Folding中正确操作围巾边缘，并在Sweeping任务中展示了精确的轨迹跟随能力。</p>
</blockquote>
<p>关键实验结果如表1所示。COIL在绝大多数任务和设置下均取得了最高成功率，显著优于基线。例如，在Pick-and-Place任务中，COIL在稀疏、中等和密集设置下的成功率分别为8/10、8/10、9/10，而表现次优的Im2Flow2Act为2/10、3/10、8/10。在具有挑战性的、分布外的Folding任务上，COIL也取得了6-7/10的成功率，展示了强大的零样本泛化能力。结合VLM指定任务时，COIL也保持了良好性能。RT-Trajectory因模拟到真实的差距表现不佳，General-Flow仅在Folding任务上表现尚可，且无法适应稀疏/中等设置。</p>
<p><img src="https://arxiv.org/html/2512.05953v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：在仿真Sweeping任务上的消融研究。从左至右分别展示了不同策略变体在稀疏（2步）、中等（16步）和密集（32步）任务指定下的成功率（条形图）和成功轨迹的跟踪误差（折线图）。结果表明，完整的COIL模型（绿色）在所有设置下均表现最佳。移除时空注意力（绿色）或对应关系增强（蓝色）都会导致性能显著下降，尤其是在稀疏指定下。</p>
</blockquote>
<p>消融实验（图4）在仿真Sweeping任务上进行，总结了各组件贡献：1）<strong>完整的COIL模型</strong>在所有设置下性能最佳。2）<strong>移除时空注意力</strong>（改用简单MLP融合）导致性能严重下降，尤其在稀疏指定下成功率接近零，证明了该机制对于理解灵活任务表示至关重要。3）<strong>移除对应关系增强</strong>（训练时不进行子采样和噪声添加）同样导致性能显著恶化，特别是在稀疏和中等设置下，验证了该技术对于克服分布偏移、提升鲁棒性的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一种新颖的、支持可变时空粒度的3D对应关系任务表示，为灵活指定操作意图提供了统一接口。2）设计了一个结合时空注意力机制的条件策略，能够鲁棒地将这种灵活表示映射为精确动作。3）构建了一个可扩展的自监督模仿学习流程，通过后见之明重标注和对应关系增强自动生成训练数据。</p>
<p>论文自身提到的局限性包括：策略性能依赖于在线的3D点跟踪算法，跟踪失败可能影响任务执行；当前方法主要评估了桌面操作场景。</p>
<p>这项工作对后续研究的启示在于：将任务表示为物体层面的3D几何对应关系，是一种介于高层语义（如语言）和底层动作之间、兼具表达力和可操作性的有效途径。其灵活的指定方式可以很方便地与用户交互、人类演示或大模型（VLM/LLM）的输出相结合，为构建通用机器人系统提供了有前景的技术路径。未来的工作可以探索更鲁棒的关键点跟踪方法，并将该框架扩展到更复杂的非结构化环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出COIL框架，旨在解决现有基于3D运动轨迹（flow）的视觉运动控制策略存在的深度模糊、依赖手工设计模块、任务规范不灵活等问题。其核心方法是采用以3D关键点对应关系为导向的任务表示，允许可变的空间与时间粒度；并设计了一个融合多模态信息的时空注意力条件策略。该方法通过自监督流程在仿真中训练，并在真实世界操作任务上实现了优于先前方法的性能，能泛化至不同任务、物体和运动模式。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.05953" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>