<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.08548" target="_blank" rel="noreferrer">2512.08548</a></span>
        <span>作者: Ting Liu Team</span>
        <span>日期: 2025-12-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>近年来，受大语言模型启发的端到端架构被越来越多地应用于机器人操作研究，以实现鲁棒的操作。然而，一个关键挑战源于机器人动作数据之间严重的分布偏移，这主要是由于不同机器人平台和任务间动作命令的数值存在巨大差异，阻碍了预训练知识的有效迁移。同时，现有的语言条件模仿学习方法通常在每个时间步提供动态的视觉输入，但保持静态的语言指令，这种不平衡限制了语言模态在引导动作生成中的影响力，未能充分利用语言的潜力。</p>
<p>本文针对动作数值尺度差异导致的分布偏移这一具体痛点，提出了一个基于语义的语言中间表征来归一化动作，以进行高效的预训练。其核心思路是：通过一个规则映射将末端执行器动作转化为粗粒度的语言描述（“动作表征”），作为对齐目标；这种表征忽略数值尺度效应，强调方向性，从而缓解分布偏移，并缩小动作标记与标准词汇标记之间的特征距离。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法按动作标记化、动作生成和模型训练的顺序展开。整体目标是学习一个两阶段的条件生成策略，从观察和指令逐步预测具体动作。</p>
<p><img src="https://arxiv.org/html/2512.08548v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：提出的动作数据生成流程。左侧展示了不同类型数据集中具体执行动作的分布；中间展示了基于阈值和窗口的检测框架及其改进；右侧描绘了生成的动作输出的结构和表征。</p>
</blockquote>
<p><strong>1. 动作标记化 (Action Tokenizer)</strong><br>遵循RT-2和OpenVLA的方法，将机器人应执行的7维连续动作 <code>(ΔX, ΔY, ΔZ, Δroll, Δpitch, Δyaw, GripperState)</code> 归一化后，每个维度离散化为256个区间（bin），每个区间用一个唯一的标记表示。在视觉语言动作模型设计中，向词表尾部添加了256个特殊标记来表示这些动作标记。</p>
<p><strong>2. 动作生成 (Motion Generation)</strong><br>这是核心创新模块，旨在从动作轨迹中自动生成鲁棒的自然语言动作信号，作为高层语义指导。</p>
<ul>
<li>**动作表征 (Motion Representation)**：生成一套固定的自然语言描述，结构为：<code>move [forward/backward] [left/right] [up/down], tilt [up/down], rotate [clockwise/counterclockwise], [open/close] gripper</code>。<code>move</code>描述执行器沿坐标轴的位置位移，<code>tilt</code>和<code>rotate</code>表示角度旋转，<code>gripper</code>指开合动作。若无运动，则标记为“stop”。</li>
<li>**自适应阈值 (Threshold)**：为更精确地检测有意义的运动，考虑了高速运动可能引起的抖动现象。提出了基于速度的校正方法，动态调整阈值。公式为：<code>T_i(t) = T_base^i + β · (1/τ) Σ |Δ̂_i(s)|</code>，其中<code>T_base^i</code>是基础阈值，<code>β</code>是敏感系数，<code>τ</code>是调整窗口。</li>
<li>**分层窗口 (Window)**：为适应不同类型的机械臂运动，用分层的检测窗口（快、中、慢）取代单一固定窗口。分别定义了快变(<code>M_f</code>)、中速常规(<code>M_m</code>)和慢变反应(<code>M_s</code>)三种运动检测逻辑，最终动作由三者综合判定：<code>Motion(t) ≔ M_f(t) ∨ M_m(t) ∨ M_s(t)</code>。</li>
<li><strong>设计验证</strong>：在手动标注的数据上，本方法的平均标注准确率达到86.37%，显著优于ECoT风格的固定阈值方法（57.62%）。本方法能有效抑制执行中的微小抖动被误识别为多个独立动作。</li>
</ul>
<p><strong>3. 两阶段训练 (Two-Stage Training)</strong><br>对于每条轨迹，构建数据元组 <code>(O_i^j, p_i, M_i^j, A_i^j)</code>，其中<code>O</code>是观察（图像帧），<code>p</code>是任务指令，<code>M</code>是生成的动作语言，<code>A</code>是离散动作标记。学习策略分为两个阶段：</p>
<ol>
<li><strong>高层动作预测</strong>：<code>φ_h(m|o, p)</code>，根据当前观察和指令，自回归地预测描述即将发生动作的动作标记。</li>
<li><strong>具体动作推断</strong>：<code>φ_l(a|o, p, m)</code>，利用预测出的动作标记作为上下文信息，推断具体的动作标记。即 <code>φ(a, m|o, p) = φ_h(m|o, p) φ_l(a|o, p, m)</code>。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.08548v1/x2.png" alt="训练流程"></p>
<blockquote>
<p><strong>图2</strong>：Qwen2.5上的两阶段训练：预训练预测动作标记；微调预测动作后再预测动作标记。</p>
</blockquote>
<ul>
<li>**仅动作预训练 (Motion-Only Pretraining)**：在多样化的多源数据（如Open X-Embodiment子集）上，模型仅学习预测动作序列。这类似于课程学习，从更统一、更容易的动作方向知识开始，旨在高效捕获通用信息，降低学习和迁移难度。</li>
<li>**下游微调 (Downstream Fine-Tuning)**：预训练后，模型在特定下游场景（如LIBERO、Bridge V2）的模仿学习数据上进行微调。此阶段，模型需要同时预测动作标记和细粒度的动作标记，以获得可执行的具体动作。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：预训练使用Open X-Embodiment的7个子集（约1.2万条轨迹）。评估使用LIBERO（四个任务套件：Spatial, Goal, Object, Long）和SimplerEnv（在Bridge V2数据集上评估四个具体任务）。</li>
<li><strong>基线方法</strong>：对比了Diffusion Policy, ScaleDP, Octo, OpenVLA, RT-1-x, ECoT等方法。</li>
<li><strong>模型架构</strong>：基于OpenVLA，视觉编码器使用SigLIP和DINOv2，LLM主干使用Qwen2.5（0.5B, 1.5B, 3B参数）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br><strong>RQ1: 各改进的个体贡献</strong><br>通过消融实验验证动作预训练和动作生成优化的效果。</p>
<ul>
<li>如表3、4所示，<strong>从零开始训练</strong>时，加入动作预训练的模型在LIBERO和SimplerEnv上的平均成功率均高于不加动作的版本（LIBERO上平均提升+1.8%~+2.5%）。</li>
<li>如表5、6所示，<strong>基于预训练模型进行微调</strong>时，加入动作预训练带来的提升更显著（LIBERO上平均提升+3.0%~+6.9%，SimplerEnv上3B模型提升达+14.1%）。这证明了动作预训练的有效性。</li>
<li>如表7所示，<strong>优化动作生成质量</strong>（调整窗口和阈值）相比原始动作生成方法，在LIBERO上带来了平均约1%的成功率提升，验证了优化策略的贡献。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.08548v1/x3.png" alt="消融实验1"></p>
<blockquote>
<p><strong>图3</strong>：在LIBERO基准上从零开始训练的成功率（%），比较有无动作预训练（w/ motion vs w/o motion）。展示了动作预训练在不同模型规模下均能带来性能提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.08548v1/x4.png" alt="消融实验2"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO基准上基于预训练模型微调的成功率（%）。可见基于动作预训练的模型微调后，性能优势进一步扩大，尤其是在3B模型上。</p>
</blockquote>
<p><strong>RQ2: 与基线及SOTA方法的对比</strong></p>
<ul>
<li><strong>LIBERO基准</strong>（表8/图5）：本文方法（特别是3B模型）取得了78.1%的平均成功率，超越了Diffusion Policy、ScaleDP、Octo和OpenVLA等基线，与OpenVLA (7B)结果具有竞争力，但使用了更小的模型。</li>
<li><strong>SimplerEnv基准</strong>（表9/图6）：本文的3B模型取得了35.3%的平均成功率，显著优于RT-1-x、Octo-Base/Small、OpenVLA和ECoT (7B)等基线，尤其在“胡萝卜放盘子”和“茄子放篮子”任务上表现出色。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.08548v1/x5.png" alt="LIBERO对比结果"></p>
<blockquote>
<p><strong>图5</strong>：在LIBERO基准上与基线方法的成功率对比。本文方法（3B）取得了最佳平均性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.08548v1/x6.png" alt="SimplerEnv对比结果"></p>
<blockquote>
<p><strong>图6</strong>：在SimplerEnv基准上与基线方法的成功率对比。本文方法（3B）在多个任务上领先，平均成功率最高。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的预训练策略，利用基于规则的语言表征（动作）来对齐跨数据集的动作-语言分布，无需人工标注或外部校正即可捕获通用的运动-语言关系，从而增强了模型的泛化能力和可迁移性。</li>
<li>提出了一种自适应的多尺度动作检测方法，能动态调整阈值并采用分层窗口，有效抑制了数据集中间的运动抖动和错误分割，显著提高了复杂动作识别的准确性。</li>
<li>在LIBERO和SimplerEnv基准上的大量实验验证了本方法在准确性、稳定性和鲁棒性上优于现有基线。</li>
</ol>
<p><strong>局限性</strong>：<br>论文在实验中指出，1.5B参数模型在SimplerEnv基准上性能增益有限，推测这主要是由于微调数据（来自真实世界）与模拟测试环境之间的差距，以及有限参数规模的约束共同导致的。</p>
<p><strong>启示</strong>：<br>本文为解决机器人学习中的分布偏移问题提供了一个新视角：通过引入语义化的、与数值尺度解耦的中间表征来桥接不同数据源。这启示后续研究可以进一步探索更丰富、更细粒度的语义动作表征，或者将这种思想扩展到其他存在模态或领域差异的具身智能学习问题中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中因平台和任务差异导致的动作数值尺度分布偏移问题，提出一种基于语言的语义化动作表征方法。该方法通过构建强调方向性、忽略数值尺度的运动表征，归一化动作命令，以缓解分布偏移并缩小动作标记与语言词汇的模态间隙。在两大多任务基准上的实验表明，该方法显著提升了策略的泛化性能与跨任务可迁移性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.08548" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>