<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EMMA: Scaling Mobile Manipulation via Egocentric Human Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>EMMA: Scaling Mobile Manipulation via Egocentric Human Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.04443" target="_blank" rel="noreferrer">2509.04443</a></span>
        <span>作者: Danfei Xu Team</span>
        <span>日期: 2025-09-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于模仿学习的移动操作策略训练主要依赖于昂贵的移动机器人遥操作数据收集（如Mobile ALOHA），这成为扩展数据集多样性和部署鲁棒性的关键瓶颈。同时，跨具身学习领域开始探索利用廉价、易得的人类视频数据训练机器人策略，但这些工作主要集中于桌面操作场景，尚未有效扩展到需要同时处理导航和操作的移动操作任务。</p>
<p>本文针对移动操作数据稀缺且收集成本高的核心痛点，提出了利用第一人称视角（自我中心）人类移动操作数据与静态机器人操作数据共同训练的新范式。其核心思路是：通过一个端到端的系统，将人类佩戴智能眼镜收集的移动操作数据，经过运动学重定向和对齐处理后，与少量机器人静态操作数据共同训练，从而绕过对移动机器人遥操作的依赖，实现移动操作技能从人类到机器人的有效迁移。</p>
<h2 id="方法详解">方法详解</h2>
<p>EMMA是一个完整的系统栈，旨在通过共同训练异构的人类移动操作数据和机器人静态操作数据，实现导航技能的直接迁移和整体策略性能的提升。</p>
<p><img src="https://arxiv.org/html/2509.04443v3/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：EMMA系统概览。<strong>左图</strong>：基于HPT的联合人-机器人策略学习框架架构，包含特定模态的输入干（Stems）、共享的Transformer主干（Trunk）以及多个动作和辅助输出头（Heads）。<strong>右图</strong>：自制的低成本双臂移动操作机器人平台，配备了Meta Project Aria眼镜作为主感知传感器。</p>
</blockquote>
<p>整体流程分为三个核心部分：1) 数据重定向与对齐；2) 人与机器人数据共同训练；3) 辅助阶段识别与控制调制。</p>
<p><strong>1. 数据重定向与对齐</strong><br>为弥合人与机器人之间的“具身鸿沟”，系统对两类数据分别处理。</p>
<ul>
<li><strong>导航动作重定向</strong>：人类是全向移动，而机器人是差速驱动。为解决此运动学差距，系统将人类头部姿态投影到地面得到一系列路径点，然后构建一个优化问题（公式1），在满足差速驱动动力学约束（公式2-4）和速度限制下，求解出一系列平滑、可执行的机器人线速度和角速度命令<code>(v, ω)</code>，作为导航动作的真实标签。<br><img src="https://arxiv.org/html/2509.04443v3/x3.png" alt="导航重定向"><blockquote>
<p><strong>图3</strong>：导航动作重定向流程。给定人类头部轨迹在地面的投影（灰色路径点），通过优化求解，生成适合差速驱动机器人执行的平滑轨迹（蓝色路径）及相应的速度命令。</p>
</blockquote>
</li>
<li><strong>操作动作对齐</strong>：对于操作动作，首先将所有数据（人类手部姿态<code>H_p</code>和机器人末端执行器姿态<code>R_p</code>）统一转换到观测时刻的相机坐标系下，使预测相对于当前视角。然后，对来自人类和机器人数据源的变换后姿态和动作数据分别进行Z-score归一化，以缓解由于生物力学和传感器差异导致的分布差距。</li>
</ul>
<p><strong>2. 人与机器人数据共同训练</strong><br>采用基于解码器-仅Transformer的架构进行共同训练，其创新点在于参数共享设计。</p>
<ul>
<li><strong>输入干（Stems）</strong>：浅层网络，将不同模态的原始观测编码为令牌序列。关键设计是<strong>共享的视觉干</strong>，用于处理来自人类和机器人的Aria眼镜主自我中心RGB图像<code>I_ego</code>，以强制视觉特征对齐。机器人腕部相机图像<code>I_wrist</code>则由单独的干处理。</li>
<li><strong>主干（Trunk）</strong>：多层Transformer，处理所有激活干输出的拼接令牌序列。</li>
<li><strong>输出头（Heads）</strong>：浅层MLP，将主干输出的前M个令牌映射到相应动作空间。定义了四个头：预测机器人双臂关节动作、预测人类笛卡尔末端执行器动作、预测机器人基座导航动作，以及一个辅助的<strong>任务阶段预测头</strong>。</li>
<li><strong>训练机制</strong>：模型在从人类移动操作数据<code>D_H</code>（包含重定向后的导航动作）和机器人静态操作数据<code>D_R</code>中抽取的批次上联合训练。处理人类批次时，激活人类操作头、导航头等；处理机器人批次时，激活机器人操作头等。共享主干和共享视觉干被所有数据源和模态更新，迫使它们学习通用表征。<strong>导航头主要从重定向后的人类数据中学习</strong>，从而实现导航技能的迁移。</li>
</ul>
<p><strong>3. 辅助阶段识别与控制调制</strong><br>移动操作任务自然地在导航和操作阶段之间交替。EMMA引入一种无监督的阶段识别机制（算法1），基于头部与手部速度的比率<code>r</code>和头部速度阈值来识别操作阶段，并对这些阶段的头部位置拟合高斯混合模型（GMM）以定位操作区域。在部署时，预测的阶段用于调制控制：在操作阶段，导航动作被插值归零以防止不必要的基座漂移；在导航阶段，未来可能处于操作阶段的预测位置被替换为当前导航终点，确保平滑切换。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在四个真实世界的长视野移动操作任务上进行：Table Service（餐桌布置）、Handover Wine（递送红酒）、Grocery Shopping（超市购物）和Push Chair（推椅子）。任务流程如图4所示。<br><img src="https://arxiv.org/html/2509.04443v3/x4.png" alt="任务图示"></p>
<blockquote>
<p><strong>图4</strong>：四个移动操作任务的时间序列图示，展示了从人机交互到精确双臂协调的任务流程。数字表示动作发生的时序。</p>
</blockquote>
<p><strong>基线方法</strong>：主要基线是修改为使用相同HPT骨干的<strong>Mobile ALOHA</strong>，它代表需要移动遥操作数据的范式。此外，还包括两个消融实验：<code>EMMA w/o action retargeting</code>（使用原始人类导航动作）和<code>EMMA w/o phase identification</code>（移除阶段识别与控制调制）。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>EMMA性能媲美或优于移动遥操作基线</strong>：如图5所示，在Grocery Shopping和Handover Wine任务上，EMMA（使用10分钟机器人数据+20分钟人类数据）的表现显著优于使用30分钟纯机器人遥操作数据训练的Mobile ALOHA（p&lt;0.05）。在Table Service任务上表现相当。<br><img src="https://arxiv.org/html/2509.04443v3/x5.png" alt="主要结果"><blockquote>
<p><strong>图5</strong>：三个移动操作任务在各子任务上的累积成功率。EMMA（蓝色）在未使用移动遥操作数据的情况下，于多个任务上显著优于Mobile ALOHA（橙色）。误差棒为95% Clopper–Pearson置信区间（N=50次试验）。</p>
</blockquote>
</li>
<li><strong>人类数据扩展性更优</strong>：在Handover Wine任务中，在相同静态机器人数据基础上，用1小时人类移动操作数据替换1小时遥操作机器人数据，成功率从52%提升至82%（相对提升约30%），证明了人类数据的更高价值。</li>
<li><strong>组件消融实验</strong>：<ul>
<li><strong>动作重定向至关重要</strong>：<code>EMMA w/o action retargeting</code>表现不佳，定性失败案例（图6d）显示其无法生成可行的导航路径。</li>
<li><strong>阶段调制提升鲁棒性</strong>：移除阶段识别会导致控制混乱。<br><img src="https://arxiv.org/html/2509.04443v3/x6.png" alt="失败案例"><blockquote>
<p><strong>图6</strong>：代表性失败案例。(a-c) Mobile ALOHA的导航碰撞和抓取失败；(d) 未经重定向的EMMA变体产生不可行的路径。</p>
</blockquote>
</li>
</ul>
</li>
<li><strong>耦合任务表现</strong>：在需要臂与基座耦合运动的Push Chair任务中（图7），EMMA（10分钟机器人+20分钟人类数据）达到了与Mobile ALOHA（30分钟机器人数据）相当的性能，并且优于将导航和操作分开训练的模型。<br><img src="https://arxiv.org/html/2509.04443v3/x7.png" alt="推椅子任务"><blockquote>
<p><strong>图7</strong>：Push Chair任务结果。统一的EMMA策略与Mobile ALOHA性能相当，并优于分离的导航/操作模型，同时在初始双臂抓取环节表现更优。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了完整的EMMA框架</strong>：首次展示了如何系统地利用自我中心人类移动操作数据与静态机器人数据共同训练，实现端到端移动操作策略学习，绕过了对移动机器人遥操作的依赖。</li>
<li><strong>验证了人类数据的有效性与优越扩展性</strong>：实验证明，人类数据不仅可以替代昂贵的移动遥操作数据达到相当或更好的性能，并且其数据效率更高，增加人类数据带来的性能提升大于增加等量的机器人遥操作数据。</li>
<li><strong>设计了关键算法组件</strong>：包括针对差速驱动平台的优化式导航动作重定向、支持异构数据共同训练的共享参数架构，以及无监督的阶段识别与控制调制机制，共同保障了系统的可行性与鲁棒性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前工作主要聚焦于模仿学习范式，尚未探索将人类数据与强化学习（RL）结合以进一步提升策略的鲁棒性和泛化能力。</p>
<p><strong>后续启示</strong>：EMMA指明了一条通过利用海量、廉价、自然的人类行为数据来规模化机器人学习的新途径。未来的工作可以探索在更多样、更复杂的环境和任务中收集与利用人类数据，以及将此类数据与RL、世界模型等其他学习范式相结合，以训练出更具通用性和适应性的移动操作智能体。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对移动操作模仿学习依赖昂贵机器人遥操作数据的问题，提出EMMA框架。其核心方法是利用易于采集的人类第一视角移动操作数据与静态机器人数据协同训练，避免移动遥操作。在四个真实任务实验中，EMMA达到了与基于遥操作数据训练的Mobile ALOHA基线相当或更高的完整任务成功率，并能泛化到新场景，且性能随人类数据量增加而提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.04443" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>