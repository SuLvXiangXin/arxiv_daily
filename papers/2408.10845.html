<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2408.10845" target="_blank" rel="noreferrer">2408.10845</a></span>
        <span>作者: Arai, Hidehisa, Miwa, Keita, Sasaki, Kento, Yamaguchi, Yu, Watanabe, Kohei, Aoki, Shunsuke, Yamamoto, Issei</span>
        <span>日期: 2024/08/19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>自动驾驶技术面临的关键挑战在于处理多样且不可预测的驾驶环境“长尾”问题。多模态大语言模型（MLLMs）为应对这一挑战提供了潜力，但其应用多局限于理解复杂环境或生成高级驾驶指令，鲜有研究将其扩展至端到端的路径规划。主要的研究瓶颈是缺乏大规模、同时涵盖视觉、语言和动作标注的数据集。现有数据集（如BDD-X、DRAMA、OpenDV-2K）往往只提供高级驾驶命令，缺乏对详细驾驶操作至关重要的细粒度轨迹信息；而包含轨迹信息的数据集（如HAD、Talk2Car、DriveLM）则通常在规模和场景多样性上受限。本文针对这一痛点，提出了CoVLA（Comprehensive Vision-Language-Action）数据集，旨在通过一种新颖、可扩展的自动化数据处理和描述生成流程，构建一个大规模、多模态的驾驶数据集，以支持训练和评估能够处理视觉、语言和动作的VLA模型。核心思路是：利用车载传感器数据自动化生成准确的未来轨迹和详细的自然语言场景描述，并基于此数据集训练一个能够同时生成场景描述和预测未来轨迹的可解释VLA模型。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的工作分为两个核心部分：<strong>CoVLA-Dataset</strong>（数据集构建）和<strong>CoVLA-Agent</strong>（基于数据集的VLA模型）。</p>
<p><strong>1. CoVLA-Dataset 构建流程</strong><br>整体框架旨在从原始驾驶数据中自动生成带有轨迹和语言描述的大规模标注数据。</p>
<p><img src="https://arxiv.org/html/2408.10845v3/images/overview.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：CoVLA框架总览。左侧展示了CoVLA-Dataset的构成，包含10,000个独特的视频片段、描述驾驶场景的帧级语言描述以及未来轨迹动作。右侧展示了基于VLM的路径规划模型CoVLA-Agent，能够预测车辆未来轨迹并提供对其行为和推理的文本描述。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2408.10845v3/images/dataset_creation.png" alt="数据生成流程"></p>
<blockquote>
<p><strong>图2</strong>：数据集生成流程概览。自动标注视频帧和传感器信号以生成轨迹和其他标签。此外，对视频帧应用自动描述生成，以产生行为描述和推理描述。</p>
</blockquote>
<p><strong>具体步骤包括：</strong></p>
<ul>
<li><strong>原始数据采集</strong>：在日本东京及周边地区，使用配备了前向摄像头、CAN总线、GNSS和IMU的车辆，在6个月内收集了超过1000小时的原始数据，涵盖了城市、高速公路、居民区、山区等多种环境，以及不同天气和时间。</li>
<li><strong>数据采样</strong>：从原始数据中，根据车辆处于行驶档、最高速度不超过100km/h、GNSS数据连续可用等条件筛选出数百小时数据。为强调驾驶场景多样性，采用基于特征（最大转向角绝对值、最大加速度绝对值、转向灯）的经验联合分布进行加权采样，最终选取了10,000个30秒的场景，总计600万视频帧（83.3小时）。</li>
<li><strong>自动标注</strong>：<ul>
<li><strong>轨迹</strong>：使用GNSS和IMU传感器数据，通过卡尔曼滤波器估计车辆未来3秒（60帧）的行驶路径，并以数据采集车辆为中心的全局坐标系表示。</li>
<li><strong>物体</strong>：使用专门的深度学习模型（OpenLenda-s）检测交通灯状态（包括箭头方向）。结合雷达和摄像头数据，通过传感器融合检测前导车辆，并提供其速度、加速度和相对位置信息。</li>
</ul>
</li>
<li><strong>自动描述生成</strong>：<ul>
<li><strong>基于规则的描述</strong>：根据车速、加速度、轨迹曲率、前导车存在、交通灯状态等信息，为每一帧生成规则化的自然语言描述。</li>
<li><strong>基于VLM的描述</strong>：为弥补规则描述在丰富性和细节上的不足，采用预训练的VideoLLaMA-2模型进行增强。模型处理一个3秒（60帧）的窗口，采样其中8帧代表性图像。为了管理计算负载，每个30秒场景被分为10个窗口。最终生成了10万个VLM描述，并与600万个规则描述结合。</li>
<li><strong>幻觉缓解</strong>：将全面的规则描述作为事实约束提供给VLM，并指示其补充规则未覆盖的信息（如道路类型、天气、潜在风险）。通过查询模型并检查其内部token概率分布来获取这些补充信息（例如，计算“晴天”、“多云”、“雨天”等token的概率来选择天气）。</li>
</ul>
</li>
</ul>
<p><strong>2. CoVLA-Agent 模型架构</strong><br>CoVLA-Agent是一个为自动驾驶设计的VLA模型，旨在同时完成交通场景描述生成和轨迹预测任务。</p>
<p><img src="https://arxiv.org/html/2408.10845v3/images/architecture.png" alt="模型架构"></p>
<blockquote>
<p><strong>图5</strong>：CoVLA-Agent的架构。使用预训练的CLIP ViT-L作为视觉编码器，Llama-2（7B）作为语言模型。模型输入包括图像、车辆速度（通过MLP编码为嵌入向量）和文本嵌入。视觉特征与速度嵌入、文本嵌入拼接后输入Llama-2。对于轨迹预测，使用特殊token作为轨迹查询，其输出通过一个MLP层，生成代表未来三秒内车辆相对位置的10个(x, y, z)坐标序列。</p>
</blockquote>
<p><strong>训练细节</strong>：模型在CoVLA-Dataset上训练，使用LLaVA指令调优格式。损失函数结合了场景描述生成的交叉熵损失和轨迹预测的均方误差损失，两者权重相等。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的主要创新在于<strong>数据集构建方法</strong>和<strong>模型任务设计</strong>。1）提出了一套可扩展的自动化流程，能够从原始传感器数据中生成大规模、高质量的视觉-语言-动作三元组数据，突破了手动标注的瓶颈。2）基于此数据集，训练了一个统一的VLA模型，能够同时输出对当前场景的文本描述和未来轨迹，实现了语言理解与动作规划在端到端模型中的紧密结合，增强了系统的可解释性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集与划分</strong>：使用CoVLA-Dataset，将10,000个场景按70/15/15划分为训练、验证和测试集。以2Hz采样帧，并确保每帧包含未来3秒的完整轨迹坐标（从中均匀采样10个点）。最终得到训练集302,989个样本，验证集64,153个样本，测试集64,920个样本。</li>
<li><strong>对比条件</strong>：为研究描述生成任务对轨迹预测的影响，设置了两个条件：<strong>预测描述条件</strong>（模型先生成描述再预测轨迹）和<strong>真实描述条件</strong>（使用真实描述而非预测的描述进行轨迹预测）。</li>
<li><strong>评估指标</strong>：使用轨迹预测中常用的平均位移误差（ADE）和最终位移误差（FDE）来衡量精度。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>定量结果（表2）显示，使用真实描述条件时，轨迹预测的ADE为0.814，FDE为1.655，均优于预测描述条件（ADE: 0.955， FDE: 2.239）。这表明描述生成的质量直接影响后续轨迹预测的准确性。</p>
<p><img src="https://arxiv.org/html/2408.10845v3/images/prediction_1.png" alt="定性结果1"></p>
<blockquote>
<p><strong>图6a</strong>：在直路上的场景。模型在直路场景下能准确预测轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2408.10845v3/images/prediction_2.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图6b</strong>：在高速公路上的场景。模型成功预测了汇入主路（银色巴士后方）的轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2408.10845v3/images/prediction_3.png" alt="定性结果3"></p>
<blockquote>
<p><strong>图6c</strong>：交通管制场景。由于前方有交警和车辆，模型预测了返回左侧车道的轨迹，展示了处理复杂场景的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2408.10845v3/images/prediction_4.png" alt="定性结果4"></p>
<blockquote>
<p><strong>图6d</strong>：十字路口场景。基于包含“直行”描述的真实描述，预测轨迹为直行；而基于包含“右转”描述的预测描述，预测轨迹为右转。这体现了模型在语言和动作模态间的一致性，也凸显了从单帧图像估计驾驶意图的困难。</p>
</blockquote>
<p><strong>消融分析与错误归因</strong>：<br>表3分析了预测描述与真实描述之间出现差异的词语对轨迹误差的影响。导致较大ADE和FDE的词语多与车辆运动方向和加速度相关，如“减速”、“左转”、“弯道”、“转弯”等。这表明轨迹预测性能在预测描述条件下相对较低，很大程度上源于从单帧图像估计驾驶意图（如转弯意图）的困难。</p>
<p><img src="https://arxiv.org/html/2408.10845v3/images/speed_distribution.png" alt="数据分布"><br><img src="https://arxiv.org/html/2408.10845v3/images/steer_distribution.png" alt="数据分布"></p>
<blockquote>
<p><strong>图4</strong>：车辆速度和转向角的数据分布。采样后（黄色条），速度分布更均匀（低速数据点增加），转向角分布在零度附近的中心峰值减弱，表明数据集涵盖了更多样化的驾驶操作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2408.10845v3/images/samples.png" alt="数据示例"></p>
<blockquote>
<p><strong>图3</strong>：CoVLA-Dataset的帧示例。展示了估计的轨迹（绿线）和描述生成模型产生的描述。关键物体用蓝色粗体标出，描述中的错误用红色粗体标出。示例揭示了自动描述生成可能出现的物体幻觉（描述不存在的物体）和物体位置识别错误等问题。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>引入了<strong>CoVLA数据集</strong>，一个大规模、涵盖多样化驾驶场景的视觉-语言-动作数据集，提供了帧级的情境描述和未来轨迹目标。</li>
<li>提出了一种<strong>可扩展的自动化方法</strong>，通过传感器融合准确估计轨迹，并利用基于规则的描述和VLM自动生成帧级文本描述。</li>
<li>开发了<strong>CoVLA-Agent</strong>，一个基于CoVLA数据集训练的可解释端到端自动驾驶VLA模型，能够一致地生成驾驶场景描述和预测轨迹。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身指出了自动描述生成方法的局限性，主要包括：1）<strong>物体幻觉</strong>：VLM可能描述场景中不存在的物体。2）<strong>定位错误</strong>：可能错误识别物体的位置（如左右混淆）。3）<strong>地域性知识不足</strong>：对特定地区（如日本）独特的交通标志等本地地标识别和理解存在困难。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>CoVLA数据集为训练和评估更强大的自动驾驶VLA模型提供了宝贵的资源，可加速该方向的研究。</li>
<li>实验结果凸显了高质量场景理解（描述生成）对精确动作规划（轨迹预测）的重要性，未来研究可探索更有效的多模态融合与联合优化策略。</li>
<li>自动描述生成的局限性指出了未来改进方向，包括开发更鲁棒的物体检测算法、具有更强驾驶环境知识的上下文感知描述模型，以及包含更多文化多样性数据的训练集。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自动驾驶中多模态大语言模型（MLLMs）缺乏大规模、高质量训练数据的问题，提出了CoVLA数据集。该数据集包含超过80小时的真实驾驶视频，通过创新的自动化数据处理与字幕生成流程，生成了与详细自然语言描述配对的驾驶轨迹。其规模达10,000个视频片段，在标注丰富性上超越现有数据集。基于CoVLA训练的视觉-语言-动作（VLA）模型，实验表明其能有效生成连贯的语言描述和驾驶动作，证明了VLA模型在实现可解释、数据驱动的自动驾驶系统方面的潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2408.10845" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>