<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.12993" target="_blank" rel="noreferrer">2601.12993</a></span>
        <span>作者: Zongqing Lu Team</span>
        <span>日期: 2026-01-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型是迈向通用机器人策略的一个有前景的范式，但其发展受到两个关键局限性的制约。首先，机器人特定数据稀缺，缺乏大规模演示语料库。其次，机器人形态具有异构性，不同机器人在运动学、驱动限制和控制频率上存在巨大差异。现有VLA模型如同“单语者”，在特定硬件上表现优异，但部署到不同形态上时存在严重的“物理鸿沟”。具体对于基于扩散/流匹配的VLA模型，在简单硬件上预训练的模型在遇到复杂实体时，会因动作分布的根本性差异而产生严重的分布偏移，导致推理轨迹从有效的运动流形上“漂移”，产生不稳定行为。</p>
<p>本文针对数据稀缺和形态异构这两个痛点，提出了以人为中心的学习新视角。其核心思想是将海量的人类交互行为视为物理世界的“通用语”，为学习提供物理常识和不变的交互逻辑。同时，本文提出统一动作空间，将异构的机器人控制映射到语义对齐的槽位，使低资源机器人能够从人类数据和高资源平台中引导技能。本文的核心思路是：通过构建超大规模、多形态的数据集UniHand-2.0，并设计统一动作空间与序列建模框架，训练一个能够感知、描述和行动的单体基础模型，实现强大的跨具身泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>Being-H0.5的整体框架基于一个统一的序列建模范式。输入是来自人类演示、机器人操作和视觉-语言理解三大来源的多模态数据序列。这些数据被序列化为单一的多模态令牌流，其中视觉和文本提供上下文基础，统一的状态/动作令牌携带物理上有意义的交互信号。输出是在统一动作空间下预测的动作序列，模型最终能够根据视觉观察和语言指令生成控制不同机器人的动作。</p>
<p><img src="https://arxiv.org/html/2601.12993v1/x4.png" alt="方法总览"></p>
<blockquote>
<p><strong>图4</strong>: Being-H0.5方法总览。(a) 统一动作空间将人类手部运动（通过MANO参数表示）和异构机器人控制映射到语义对齐的槽位。(b) 统一序列建模将所有异构监督（人类动作、机器人动作、视觉-语言数据）整合到单一令牌流中进行训练。(c) 混合流架构，包含用于高级多模态推理的共享Transformer和用于低级动作生成的专业化流专家，通过流形保持门控进行路由。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><strong>UniHand-2.0数据集</strong>：这是方法的基础，是一个超过35,000小时、包含400M样本、120B令牌的超大规模预训练配方。它包含三部分：16,000小时以自我为中心的人类视频（提供密集的行为先验）、14,000小时覆盖30种不同机器人形态的操作数据、以及5,000小时等效的通用视觉-语言理解数据（保持模型的高层推理和指令遵循能力）。该数据集的规模和形态多样性均为当前之最。</li>
<li><strong>统一动作空间</strong>：为解决形态异构性问题，该方法设计了一个统一动作空间。其核心是将人类轨迹（通过MANO手部模型参数表示）和异构机器人控制映射到一组语义对齐的槽位，例如<code>[dx, dy, dz, drx, dry, drz, grip]</code>。这充当了不同硬件间的“通用语法”，将功能意图与机械关节解耦，使模型能够内化交互的底层物理原理，而不仅仅是特定形态的命令。</li>
<li><strong>统一序列建模</strong>：在此统一动作空间的基础上，将所有异构监督（人类演示、机器人轨迹、视觉-语言语料库）转化为一个统一的序列建模问题。不同来源的数据被序列化为单一令牌流，并应用相应的损失函数进行优化：以文本为中心的语料库使用标准的下一个令牌预测损失，而人类和机器人语料库则在统一动作空间内监督动作预测。</li>
<li><strong>混合流架构</strong>：在模型架构上，Being-H0.5采用了混合Transformer设计，并引入了新颖的<strong>混合流框架</strong>。该框架将动作模块解耦为具有共享动态知识的基础专家和利用形态感知任务路由的 specialized 专家。这提升了模型容量和跨域迁移能力。</li>
<li><strong>流形保持门控</strong>：为确保基于流的动作生成在现实世界中的稳定性，该方法引入了流形保持门控。其作用是鼓励模型在感知模糊时依赖可靠的上下文并回退到鲁棒的先验，防止不稳定的修正通过迭代 refinement 被放大，从而在感官偏移下保持鲁棒性。</li>
<li><strong>通用异步分块</strong>：为了将实时分块控制泛化到跨具身设置，该方法提出了通用异步分块。它训练一个统一的策略，使其在具有不同驱动频率和延迟特性的异构平台上保持一致，使得单个模型检查点能够在所有形态上流畅运行。</li>
</ol>
<p>与现有方法相比，本文的创新点具体体现在：1) 构建了迄今为止规模最大、形态最多样的以人为中心的预训练数据集；2) 首次提出将人类手部运动与多样机器人控制统一到单一动作空间，并配合统一序列建模范式；3) 设计了混合流、流形保持门控和通用异步分块等架构创新，专门解决了扩展基于流的动作生成时的固有瓶颈。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在模拟环境和真实机器人平台上进行了广泛评估。</p>
<ul>
<li><strong>Benchmark/数据集</strong>：模拟基准测试包括LIBERO和RoboCasa。真实机器人部署涉及五个物理形态截然不同的平台：PND Adam-U, Franka+Inspire, Unitree G1, BeingBeyond D1, 和 LeRobot SO-101。</li>
<li><strong>Baseline方法</strong>：主要对比了现有的VLA模型，特别是π0.5。</li>
<li><strong>关键实验结果</strong>：<ul>
<li>在模拟基准上，仅使用低分辨率RGB输入（无辅助模态），Being-H0.5在LIBERO上达到了**98.9%<strong>的成功率，在RoboCasa上达到了</strong>53.9%**的成功率，均创造了新的SOTA记录。</li>
<li>在五个真实机器人平台上的跨具身泛化实验中，Being-H0.5显著优于π0.5等现有VLA。</li>
<li>实验观察到了一个新兴的零样本迁移信号：一个在统一动作接口下跨形态联合训练的单一Being-H0.5通用检查点，在<strong>未见过的任务-形态对</strong>（即目标机器人上没有任何数据）上实现了非零的成功率。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2601.12993v1/x5.png" alt="模拟基准结果"></p>
<blockquote>
<p><strong>图5</strong>: 在LIBERO和RoboCasa模拟基准上的性能对比。Being-H0.5仅使用RGB就达到了SOTA性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.12993v1/x6.png" alt="跨具身泛化对比"></p>
<blockquote>
<p><strong>图6</strong>: 在五个真实机器人平台上的跨具身泛化性能。Being-H0.5在不同形态和任务上均优于基线π0.5。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.12993v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>: 消融实验。(a) 显示随着训练数据量增加，性能持续提升，未出现平台期。(b) 验证了混合流架构、流形保持门控和通用异步分块这三个核心组件的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.12993v1/x8.png" alt="零样本迁移分析"></p>
<blockquote>
<p><strong>图8</strong>: 对新兴零样本迁移的分析。热图显示了单一检查点在未见过的任务-形态对上的成功率，证实了跨形态知识迁移的存在。</p>
</blockquote>
<p><strong>消融实验总结</strong>：图7的消融实验表明，1) 扩大数据规模能持续提升性能；2) 混合流架构、流形保持门控和通用异步分块这三个核心组件各自都对最终性能有重要贡献，共同解决了扩展流动作生成的瓶颈。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：1) 引入了迄今为止最大规模的具身VLA预训练配方UniHand-2.0（35,000小时，30种形态）；2) 提出并实现了统一动作空间与统一序列建模的训练范式，首次将人类运动与多样机器人控制整合到单一框架；3) 设计了混合流、流形保持门控和通用异步分块等关键架构创新，确保了模型在跨具身部署时的容量、稳定性和实时性；4) 在模拟和真实世界基准上取得了SOTA结果，并实证了跨形态的新兴零样本迁移能力。</p>
<p>论文自身提到的局限性在于，尽管观察到了新兴的零样本迁移，但在未见过的任务-形态对上的绝对成功率仍然不高，这指明了未来通过增加多样化后训练数据集来改进迁移的实用化方向。</p>
<p>本文对后续研究的启示在于：验证了以人类交互作为“物理通用语”进行大规模预训练的有效性；为构建真正形态无关的通用机器人基础模型提供了一套完整的数据、算法和工程框架；其观察到的跨形态迁移现象为通过扩大数据多样性来涌现更强大泛化能力指明了道路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文Being-H0.5旨在解决机器人学习中的跨具身泛化问题，即如何让单一模型适应不同形态的机器人，克服数据稀缺和形态异构的挑战。关键技术包括：构建超过35,000小时的UniHand-2.0多模态数据集；提出统一动作空间，将异构控制映射到语义对齐的槽；采用混合变换器架构，结合混合流框架解耦共享运动原语与具身特定专家；引入流形保持门控和通用异步分块提升稳健性。实验表明，该模型在模拟基准LIBERO和RoboCasa上分别达到98.9%和53.9%的性能，并在五个真实机器人平台上实现跨具身部署。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.12993" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>