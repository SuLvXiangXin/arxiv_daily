<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.01946" target="_blank" rel="noreferrer">2512.01946</a></span>
        <span>作者: Cordelia Schmid Team</span>
        <span>日期: 2025-12-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，由大型语言模型和视觉语言模型驱动的机器人操作系统在任务规划和动作执行方面取得了显著进展，但其可靠性仍受制于广泛的失败模式，包括错误的规划、感知混淆以及低级控制问题。提升鲁棒性需要能够利用视觉和语言进行推理并检测失败的模型。然而，该领域面临两大关键挑战：一是缺乏全面、多样且标注丰富的失败数据集，现有数据集要么规模小、手动收集，要么仅限于仿真且缺乏细粒度标注；二是现有失败检测方法在利用视觉上下文方面存在不足，例如依赖可能导致误差累积的多阶段文本转换流水线、使用易受遮挡影响的单视图观察，或将多视图图像压缩为单一网格而损失细粒度时空信息。</p>
<p>本文针对机器人失败检测中数据稀缺和视觉推理能力有限的具体痛点，提出了一个自动生成失败数据并训练专用推理VLM的新视角。核心思路是：通过程序化扰动仿真和真实世界中的成功轨迹，自动生成多样化的规划与执行失败案例及细粒度标注，并以此训练一个能够处理高分辨率多视图图像、进行显式链式推理的VLM（Guardian），从而实现更鲁棒和可解释的失败检测。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体方法包含两个核心部分：1）自动失败数据生成流水线；2）基于此数据训练的Guardian模型。</p>
<p><img src="https://arxiv.org/html/2512.01946v2/x2.png" alt="失败数据生成流程"></p>
<blockquote>
<p><strong>图2</strong>：失败数据生成流水线。展示了在仿真（RLBench）中在线生成和真实世界数据集（BridgeDataV2）上离线生成失败案例的流程。给定正确的计划和成功轨迹，生成对应的错误计划和不成功轨迹。</p>
</blockquote>
<p><strong>失败数据生成</strong>：该方法以成功演示（来自RLBench仿真和BridgeDataV2真实数据集）为基础，通过程序化扰动生成失败。任务被分解为子任务及其对应的视频片段。</p>
<ul>
<li><strong>规划失败生成</strong>：针对成功计划，通过LLM（Mistral-Small-24B）微调或基于规则的扰动，构造五类失败：操作错误物体、目标状态/位置错误、顺序错误、缺失子任务、矛盾子任务。每个样本包含任务指令、计划、初始前视图图像。</li>
<li><strong>执行失败生成</strong>：<ul>
<li>在仿真中，直接扰动子任务级动作，产生四类失败：夹爪未闭合、物体状态/位置错误、操作错误物体、抓取/推动不精确。</li>
<li>在真实数据中，由于无法直接控制机器人，通过LLM或规则改变子任务文本指令（造成语义不匹配），或替换结束图像为起始图像（表示无进展）来模拟失败。每个样本包含任务和子任务描述，以及动作前后的多视图图像。</li>
</ul>
</li>
<li><strong>链式推理生成</strong>：利用大型VLM（InternVL3-38B），结合来自仿真或标注的对象、空间和机器人状态信息，为每个样本自动生成逐步推理轨迹，用于训练可解释模型。</li>
</ul>
<p><strong>Guardian模型</strong>：将失败检测构建为视觉问答任务。模型基于InternVL3-8B架构，包含文本分词器、视觉编码器和基于Transformer的LLM。</p>
<p><img src="https://arxiv.org/html/2512.01946v2/x5.png" alt="Guardian模型架构与集成"></p>
<blockquote>
<p><strong>图5</strong>：左：Guardian模型架构概述。右：将Guardian模型集成到机器人操作流程中进行规划和执行验证。</p>
</blockquote>
<ul>
<li><strong>架构细节</strong>：与AHA等方法将多视图图像拼接为单张网格图不同，Guardian的视觉编码器独立处理每张图像，保留了每张图像内的细粒度空间细节，使模型能够显式推理空间和时间变化。模型输出不仅包含二元（成功/失败）判断和细粒度失败类别，还生成显式的推理链。</li>
<li><strong>训练策略</strong>：使用参数高效的LoRA进行微调，冻结视觉编码器。探索了三种结合推理链的训练策略：直接预测答案的“Vanilla”基线；始终生成推理链再预测答案的“Thinking”策略；训练时交替生成推理链+答案和直接答案，测试时仅输出答案的“Dropout”策略。</li>
<li><strong>创新点</strong>：1) <strong>数据生成</strong>：自动化、覆盖规划与执行、跨仿真与真实域、附带细粒度类别和推理链。2) <strong>模型设计</strong>：独立处理多视图图像以保留细节，并集成显式链式推理以提高可解释性和准确性。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了四个基准进行评估：新构建的<strong>RLBench-Fail</strong>（仿真）、<strong>BridgeDataV2-Fail</strong>（真实）、<strong>UR5-Fail</strong>（真实策略驱动）以及现有的<strong>RoboFail</strong>（真实）。对比基线包括大规模通用VLMs（Qwen3-VL-235B, GPT-4.1）、小规模通用模型（InternVL3-8B）以及专用失败检测模型（Cosmos-Reason1-7B, AHA-13B, Sentinel, CLIP+MLP）。主要评估指标为二元检测准确率。</p>
<p><strong>关键实验结果</strong>：<br>表II显示，在域内数据集上，经过微调的Guardian-8B-Thinking模型在规划和执行准确率上达到或超过了参数量大得多的通用VLMs（Qwen3-VL-235B, GPT-4.1）。在域外数据集（RoboFail, UR5-Fail）的零样本评估中，Guardian也表现优异，执行准确率分别达到0.86和0.77，规划准确率分别达到0.70和0.89，优于或匹配其他专用和通用基线。</p>
<p><img src="https://arxiv.org/html/2512.01946v2/x6.png" alt="训练数据规模对执行二元准确率的影响"></p>
<blockquote>
<p><strong>图6</strong>：训练数据规模对跨基准测试执行二元准确率的影响。表明在生成数据上微调存在扩展规律。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ul>
<li><strong>训练数据混合</strong>（表III）：混合使用仿真（RLBench-Fail）和真实（BridgeDataV2-Fail）数据进行微调，相比仅使用单一域数据，能显著提升模型在域内和域外的泛化性能。加入少量真实策略数据（UR5-Fail）可进一步提升在类似域上的表现。</li>
<li><strong>训练数据规模</strong>（图6）：随着用于微调的生成数据量增加，模型在域内和域外数据集上的性能呈现扩展规律，表明进一步扩大数据规模可能带来性能提升。</li>
<li><strong>图像表示方法</strong>（表IV）：将4个视图的图像分开处理（separated）比拼接成一张图（concat，即AHA方法）能带来显著的性能提升（RLBench-Fail执行准确率从0.72提升至0.81）。</li>
<li><strong>多分类性能</strong>（图7）：Guardian能够进行细粒度失败分类。混淆矩阵显示，对于执行失败，“夹爪未闭合”与“抓取不精确”容易混淆；对于规划失败，在长视野计划中，“错误物体状态”等类别存在较高混淆，反映了模型在视觉定位细微线索和时序常识推理方面的挑战。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.01946v2/x7.png" alt="规划和执行失败分类的混淆矩阵"></p>
<blockquote>
<p><strong>图7</strong>：Guardian对规划和执行失败分类的混淆矩阵。数值为百分比，行表示真实类别，列表示预测类别。</p>
</blockquote>
<p><strong>系统集成演示</strong>：将Guardian作为验证模块集成到3D-LOTUS++机器人操作框架中（图5右）。图8展示了在线任务执行中，Guardian成功检测到初始生成的计划错误（操作错误物体）并触发重新规划，以及检测到子任务执行失败（抓取不精确）并触发重新执行，从而纠正了任务流程。</p>
<p><img src="https://arxiv.org/html/2512.01946v2/x8.png" alt="在线任务执行中通过Guardian验证进行纠正"></p>
<blockquote>
<p><strong>图8</strong>：在RLBench在线任务执行中通过Guardian验证进行纠正。左：对生成计划的成功纠正。右：对子任务执行的成功纠正。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一种自动化的机器人失败生成方法，能够程序化地扰动成功轨迹，生成覆盖规划与执行、兼具仿真与真实数据、并带有细粒度类别和推理链标注的大规模失败数据集（RLBench-Fail, BridgeDataV2-Fail, UR5-Fail）。2) 引入了Guardian模型，一个通过独立处理高分辨率多视图图像和显式链式推理进行细粒度失败检测的微调VLM，在多个基准上取得了最先进的性能。3) 展示了Guardian作为即插即用验证模块，能够有效提升现有机器人操作系统的任务成功率。</p>
<p><strong>局限性</strong>：论文提到，模型性能仍受限于对细微视觉线索（如夹爪是否完全闭合）的精准感知能力，以及对长视野任务中复杂时序和常识依赖的推理能力。此外，使用显式推理链（Thinking策略）会带来额外的计算开销。</p>
<p><strong>研究启示</strong>：1) <strong>数据驱动的失败建模</strong>：自动化、程序化的失败数据生成是弥补数据稀缺、提升模型鲁棒性的有效途径，且数据规模与性能存在正向关联。2) <strong>多视图与细粒度推理</strong>：独立处理多视图图像比压缩处理更能保留关键空间信息，显式推理链有助于提升可解释性和复杂失败模式的诊断能力。3) <strong>模块化系统集成</strong>：将专用的失败检测模块作为即插即用的“守护者”集成到现有机器人流程中，是一种提升系统整体可靠性的实用且有效的范式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中缺乏全面失败数据、导致视觉语言模型（VLM）失败检测准确性受限的核心问题，提出自动失败合成方法：通过扰动成功轨迹，生成多样化的规划与执行失败案例，并构建三个新基准数据集（RLBench-Fail等）。基于此训练了多视图VLM模型Guardian，用于细粒度失败推理与检测。实验表明，Guardian在现有及新基准上均达到最优性能，集成后能有效提升仿真与真实机器人的任务成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.01946" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>