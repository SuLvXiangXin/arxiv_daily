<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.13375" target="_blank" rel="noreferrer">2510.13375</a></span>
        <span>作者: Yuan, Tianyuan, Liu, Yicheng, Lu, Chenhao, Chen, Zhuoguang, Jiang, Tao, Zhao, Hang</span>
        <span>日期: 2025/10/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过在大规模网络数据上预训练的视觉-语言模型获得强大的泛化能力，已成为机器人操作的关键范式。然而，VLM本身的空间推理能力有限，这直接限制了VLA在需要精确操作任务上的性能。现有方法主要依赖大规模机器人动作数据进行预训练，试图将VLM接地到3D空间，但这不仅训练效率低，且仍不足以实现精确的空间理解。为解决此痛点，本文提出通过集成一个预训练的深度预测模块，为VLA模型显式地注入空间感知能力。本文核心思路是：在混合专家框架下，引入一个独立的深度专家，与VLM专家和动作专家通过共享注意力协同工作，从而在保持语义理解和高效推理的同时，显著增强模型的空间推理能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>DepthVLA采用混合专家架构，集成了三个专家：一个用于语义和语言理解的VLM专家、一个用于几何推理的深度专家，以及一个用于连续控制的流匹配动作专家。模型以当前观测（单目或多目图像）、语言指令和本体感知状态为输入，输出一个长度为k的动作块。</p>
<p><img src="https://arxiv.org/html/2510.13375v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：DepthVLA的混合专家框架。三个专家共享注意力层，但拥有独立的权重和特征维度。块级掩码确保VLM和深度专家的令牌仅关注自身，而动作令牌可以关注所有流，从而融合语义和空间线索以生成动作。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>VLM专家</strong>：采用Paligemma-3B作为骨干，负责编码观测和语言指令，提供开放词汇的语义感知和语言接地特征。</li>
<li><strong>深度专家</strong>：作为专用的空间推理器，其编码器基于DINOv2并继承自Depth Anything V2的预训练权重，解码器则采用与VLM专家相似的Transformer结构。与仅输出最终深度图的方法不同，该专家在所有中间层进行空间推理，为动作预测提供丰富的几何线索。在集成到VLA前，深度专家在多样化的3D数据集上进行单目深度预测任务的预训练，使用尺度不变对数损失。</li>
<li><strong>动作专家</strong>：采用流匹配方法生成连续动作。在每一层，动作专家都关注来自VLM和深度专家的特征，从而基于语言、视觉和空间线索生成动作。</li>
<li><strong>训练与损失函数</strong>：VLA训练采用模仿学习目标，最大化动作的对数似然。具体使用流匹配损失来建模连续且多样的动作轨迹。为了保持深度专家的空间推理能力，在VLA训练期间保留了深度预测损失。因此，总损失为深度损失与流匹配损失之和：ℒ = ℒ_si + ℒ_flow。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>显式深度集成</strong>：不同于依赖外部深度估计器或通过自回归生成深度令牌的方法，DepthVLA将预训练的深度专家作为核心组件集成到端到端的MoT框架中。</li>
<li><strong>中间层几何特征利用</strong>：动作专家利用深度专家所有中间层的特征，而非仅使用最终的深度图输出，从而获得了更细粒度的几何表示。</li>
<li><strong>专家独立预训练</strong>：MoT设计允许VLM专家和深度专家在各自领域的大规模数据集上独立预训练，提高了训练效率和可扩展性，减少了对机器人动作数据的依赖。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集与基准</strong>：在仿真环境中，使用BridgeData V2进行训练，并在Simpler WidowX和LIBERO基准上进行零样本评估。在真实世界中，使用Galaxea Open-World Dataset进行预训练，并在Galaxea R1 Lite双臂移动平台上进行评估。</li>
<li><strong>对比基线</strong>：包括Diffusion Policy、Octo-Base、SpatialVLA、π0（重新实现）、OpenVLA、CoT-VLA、MolmoACT、DreamVLA等。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>Simpler WidowX基准</strong>：DepthVLA在未使用额外机器人动作数据预训练的情况下，平均成功率达到74.8%，显著优于重新实现的π0基线（58.8%）和SpatialVLA（34.4%）。在需要精确空间推理和避障的任务（如堆叠积木、拿取茄子）上提升尤为明显。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.13375v1/x1.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>表I</strong>：在Simpler WidowX基准上的成功率对比。DepthVLA取得了最高的平均性能。</p>
</blockquote>
<ol start="2">
<li><strong>LIBERO基准</strong>：DepthVLA在四个任务套件上的平均成功率达到94.9%，超越了所有基线模型，包括那些使用了额外动作数据预训练的模型。这表明标准的VLA即使经过大规模动作预训练，其3D接地能力仍然不足。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.13375v1/x3.png" alt="LIBERO结果表"></p>
<blockquote>
<p><strong>表II</strong>：在LIBERO基准上的成功率对比。DepthVLA取得了最佳的平均性能。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界评估</strong>：在桌面整理、微波炉操作和积木堆叠三个任务上，DepthVLA在标准微调设置下的平均进度得分为79%，远高于基线的65%；在少样本微调设置下也以63%对45%领先。特别是在需要避障（微波炉操作）和精确空间感知（积木堆叠）的任务上表现更优。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.13375v1/figs/real_world_results.png" alt="真实世界平台"></p>
<blockquote>
<p><strong>图3</strong>：真实机器人实验平台Galaxea R1 Lite。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.13375v1/x4.png" alt="真实世界性能"></p>
<blockquote>
<p><strong>图4</strong>：DepthVLA与基线在三个双手任务上的性能对比。DepthVLA在需要精确空间推理和避障的任务上表现更佳。</p>
</blockquote>
<ol start="4">
<li><p><strong>消融实验</strong>：研究证实了各设计组件的必要性。关键发现包括：（i）深度专家预训练至关重要；（ii）在VLA训练中保留深度损失能提升性能；（iii）冻结深度专家对性能影响不大，表明其学习了通用空间表示；（iv）块级掩码（阻止VLM与深度令牌互相关注）是必要的；（v）<strong>模型预测深度比直接输入真实深度效果更好</strong>，作者推测这避免了模态竞争，使几何推理更有效地集成到共享表示空间中。</p>
</li>
<li><p><strong>定性可视化</strong>：</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.13375v1/x5.png" alt="深度预测可视化"></p>
<blockquote>
<p><strong>图5</strong>：DepthVLA在真实和仿真环境中预测的深度图。预测的深度提供了细粒度的几何线索，即使在杂乱或具有挑战性的场景中也能稳健地捕捉3D布局。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了DepthVLA，一种通过混合专家框架集成预训练深度专家的新型VLA架构，显式地增强了空间推理能力。</li>
<li>设计了支持各专家独立预训练的策略，提高了训练效率和对非具身数据的利用。</li>
<li>在真实世界和多个仿真基准上进行了广泛验证，证明了DepthVLA在需要精确操作、避障和细粒度抓取的任务上显著优于现有方法。</li>
</ol>
<p><strong>局限性</strong>：<br>论文指出，单目深度预测本身是一个不适定且具有挑战性的问题。深度专家在困难场景（如微小边缘、反光或透明物体、无纹理表面）中可能表现不佳，这可能会影响动作生成。</p>
<p><strong>未来启示</strong>：<br>DepthVLA的成功表明，将强大的3D感知先验显式集成到VLA中是提升空间理解的有效途径。未来的工作可以探索多视图深度或点云预测等更鲁棒的3D表示形式，以进一步提升空间感知的准确性和鲁棒性。此外，模型预测深度优于直接使用真实深度的发现，为多模态融合中的表示学习提供了新的思考方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DepthVLA，旨在解决现有视觉-语言-动作模型因缺乏显式空间感知而难以完成精确操作任务的问题。其核心方法是引入一个预训练的深度预测模块，并采用混合transformer架构，将视觉语言模型、深度transformer与动作专家通过全共享注意力进行统一，从而增强模型的几何理解能力。实验表明，DepthVLA在真实任务中取得78.5%的进度（基线为65.0%），在LIBERO和Simpler模拟器中分别达到94.9%和74.8%的性能，均优于现有方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.13375" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>