<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.19402" target="_blank" rel="noreferrer">2512.19402</a></span>
        <span>作者: Hao Dong Team</span>
        <span>日期: 2025-12-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人学习领域的进展主要由大规模数据集和强大的视觉运动策略架构驱动，但策略的鲁棒性仍受限于收集多样化演示的高昂成本，尤其是在需要空间泛化的操作任务中。现有方法试图从有限数据合成新演示以缓解这一问题。一类如MimicGen系列的工作，通过分割演示轨迹并变换以物体为中心的片段来适应新的物体摆放。另一类如Real2Render2Real，通过姿态跟踪和轨迹插值从人类操作视频合成演示，但依赖于图形引擎渲染，不可避免地面临视觉和物理的“模拟到真实”差距，且需要为操作物体构建数字资产。DemoGen通过3D编辑增强真实点云演示，但无法应用于主流的RGB图像和2D策略。因此，目前缺乏一种能够快速扩展真实世界多视角操作视频、生成新颖轨迹，同时保持视觉真实性和正确交互的方法。</p>
<p>本文针对上述痛点，提出了Real2Edit2Real框架。其核心思路是：利用深度作为连接3D可编辑性与2D视觉数据的自然接口，首先从多视角RGB观测重建度量尺度的场景几何，然后基于重建的点云进行深度可靠的空间编辑以生成新的操作轨迹，最后通过一个以深度为主要控制信号的多条件视频生成模型，合成具有空间增强效果的多视角操作视频。</p>
<h2 id="方法详解">方法详解</h2>
<p>Real2Edit2Real框架的目标是，给定一个包含多视角视频、关节角度和动作的源演示，通过机器人运动学模型和相机参数，将其增强为包含新颖物体空间配置和对应轨迹的大量演示集。整体流程包含三个核心模块。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Real2Edit2Real框架概览。(1) 从多视角观测重建度量尺度的几何。(2) 通过深度可靠的空间编辑合成新轨迹。(3) 利用时序深度信号控制生成演示视频。</p>
</blockquote>
<p><strong>1. 度量尺度几何重建</strong><br>为了获得可编辑的3D场景表示，首先需要从稀疏的机器人相机视角（头、左腕、右腕）重建准确的几何。论文采用VGGT作为前馈式3D重建模型，并针对人形机器人场景提出了一个<strong>混合训练范式</strong>以提升其度量尺度深度图和相机位姿的预测精度。</p>
<ul>
<li><strong>相机位姿监督</strong>：真实数据中的手眼标定位姿易受误差影响，而仿真数据能提供完美准确的位姿。因此，仅使用仿真数据通过L1损失监督相机位姿预测。</li>
<li><strong>深度图监督</strong>：真实深度传感器数据有噪声但尺度准确，仿真数据无噪声但尺度分布可能与真实有偏差。因此，采用混合监督：对真实数据使用带阈值掩码的置信度损失过滤无效噪声；对仿真数据直接使用置信度损失。同时，仿真数据也监督点图损失。</li>
<li><strong>总损失</strong>：总损失为相机损失、深度损失和点图损失的加权和，其中相机损失权重设为10以平衡优化。</li>
</ul>
<p><strong>2. 深度可靠的空间编辑</strong><br>基于重建的点云，通过点云编辑和运动规划合成新的物体摆放和对应的操作轨迹。关键创新在于确保编辑后生成的深度图在物理上是可靠的。</p>
<ul>
<li><strong>轨迹合成</strong>：受DemoGen启发，将点云演示分解为<strong>运动段</strong>（机器人自由移动）和<strong>技能段</strong>（机器人与物体交互）。给定一个物体变换，对技能段中的机器人点云施加相同变换以保持交互关系；新的运动段则通过运动规划生成。</li>
<li><strong>深度投影与修复</strong>：相机刚性连接于机器人末端执行器，其位姿随编辑一同变换。将编辑后的点云投影生成深度图，但会因物体位置变化和相机距离改变产生孔洞和噪声。通过背景修复和深度滤波来缓解这些伪影。</li>
<li><strong>机器人姿态校正</strong>：一个关键挑战是，之前的编辑将整个机器人视为刚体，但实际上只有末端执行器应被变换，手臂部分需重新对齐以保持运动学有效性。为此，算法利用URDF和源关节状态分割出原始机器人连杆，并使用合成后的动作重新渲染手臂深度。这一步确保了物理上合理的机器人配置，并产生了无刚体伪影的准确深度观测。</li>
</ul>
<p><strong>3. 3D控制的视频生成</strong><br>获得编辑后的深度、动作和相机位姿序列后，需要将其转换为策略训练所需的2D视觉观测。论文构建了一个基于Transformer的视频生成模型，从第一帧开始合成具有真实视觉外观、多视角一致性和物理合理交互的操作视频。<br><img src="https://..." alt="视频生成框架"></p>
<blockquote>
<p><strong>图3</strong>：3D控制视频生成框架。利用深度作为3D控制接口，结合边缘、动作和光线图，引导多视角演示生成。</p>
</blockquote>
<ul>
<li><strong>双注意力机制</strong>：包含<strong>视角内注意力</strong>（捕获单个视图内的空间上下文）和<strong>跨视角注意力</strong>（在所有视图间同时计算自注意力，利用多视角对应关系）。这种设计既促进了生成视频的多视角一致性，又显著降低了计算成本。</li>
<li><strong>深度控制接口</strong>：将深度图与图像潜在表示拼接后输入Transformer主干，使视频生成以3D结构线索为条件。深度编码了机器人运动和物体交互，确保了合成的演示与几何信息一致。此外，还加入了Canny边缘、动作图和光线图等辅助控制信号，分别用于锐化物体边界、改善运动基础、增强多视角一致性。</li>
<li><strong>平滑物体重定位</strong>：为了在视频第一帧中实现物体重定位，算法将物体重定位转换为一个平滑变换过程，即在空间编辑中插值物体的平移和旋转，在操作开始前合成物体移动轨迹。通过这种平滑重定位，将图像编辑问题转化为视频生成问题，通过3D控制的视频生成统一处理物体重定位和演示生成。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在四个真实世界机器人操作任务上评估：<code>Mug to Basket</code>（单臂）、<code>Pour Water</code>（单臂）、<code>Lift Box</code>（双臂）、<code>Scan Barcode</code>（双臂）。使用AgiBot Genie G1机器人，头、左腕、右腕搭载RGB相机。对比了两种VLA策略：Go-1和π0.5。生成数据时，从收集的真实演示中随机采样指定数量（1、2、5）作为源演示，并对其施加随机的物体位置变换，生成200条合成演示用于策略训练。</p>
<p><strong>关键结果</strong>：表1展示了不同训练数据下的操作成功率。</p>
<table>
<thead>
<tr>
<th align="left"># Demo</th>
<th align="left">Mug to Basket</th>
<th align="left">Pour Water</th>
<th align="left">Lift Box</th>
<th align="left">Scan Barcode</th>
<th align="left">Total</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Real 1 Gen 200</td>
<td align="left">14 / 20</td>
<td align="left">15 / 20</td>
<td align="left">12 / 10</td>
<td align="left">14 / 11</td>
<td align="left">65.0% / 57.5%</td>
</tr>
<tr>
<td align="left">Real 5 Gen 200</td>
<td align="left">17 / 20</td>
<td align="left">18 / 20</td>
<td align="left">12 / 12</td>
<td align="left">18 / 17</td>
<td align="left">78.8% / 81.3%</td>
</tr>
<tr>
<td align="left">Real 50</td>
<td align="left">14 / 20</td>
<td align="left">13 / 20</td>
<td align="left">8 / 8</td>
<td align="left">15 / 17</td>
<td align="left">61.3% / 61.3%</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：Real2Edit2Real在四个任务和两种VLA策略上的成功率（Go-1 / π0.5）。仅用1-5个源演示生成的数据训练的策略，其性能可匹配甚至超越用50个真实演示训练的策略，数据效率提升高达10-50倍。</p>
</blockquote>
<p>结果表明：1) 当真实演示少于20个时，平均成功率低于50%，说明VLA在数据稀缺时空间泛化能力有限。2) 使用仅1个源演示生成的200条数据训练的策略，其空间泛化能力已与使用50个真实演示训练的策略相当。3) 随着源演示数量增加到5个，生成数据的多样性和空间覆盖度提升，训练出的策略（Go-1和π0.5）平均成功率分别达到78.8%和81.3%，显著超越了使用50个真实演示的策略（61.3%）。</p>
<p><strong>扩展应用</strong>：</p>
<ul>
<li><strong>高度编辑</strong>：通过平滑物体重定位，可以编辑物体高度。如表2所示，仅在桌面上用20个真实演示训练的策略在平台高度上完全失败。而通过本框架生成20条桌面和20条平台高度的演示进行训练，策略成功率可达80%。</li>
<li><strong>纹理编辑</strong>：由于模型从第一帧生成视频，可以方便地通过编辑第一帧来改变背景纹理。表3表明，生成包含不同纹理的演示数据可以提高策略对纹理变化的鲁棒性。</li>
</ul>
<p><strong>消融实验</strong>：<br><img src="https://..." alt="几何重建消融"></p>
<blockquote>
<p><strong>图5</strong>：几何重建消融研究。与真实数据和原始VGGT相比，本文的Metric-VGGT预测的点云最干净，相机位姿最准确。</p>
</blockquote>
<p><img src="https://..." alt="姿态校正消融"></p>
<blockquote>
<p><strong>图6</strong>：机器人姿态校正（RPC）消融研究。没有RPC会导致错误的深度图，进而生成模糊、不一致的视频；使用RPC后，深度图在运动学上一致，能生成真实的机器人运动。</p>
</blockquote>
<p><img src="https://..." alt="平滑重定位消融"></p>
<blockquote>
<p><strong>图7</strong>：平滑物体重定位（SOR）消融研究。没有SOR，生成的物体摆放常出现明显错误；使用SOR则能实现物体在目标位置的精确摆放。</p>
</blockquote>
<p>消融实验总结：1) <strong>度量尺度几何重建</strong>是生成高质量点云和可靠深度控制信号的基础。2) <strong>机器人姿态校正</strong>对于产生物理一致的深度图、进而生成真实机器人运动至关重要。3) <strong>平滑物体重定位</strong>是实现精确物体摆放、生成可用演示的关键。</p>
<p><strong>定性结果</strong>：<br><img src="https://..." alt="生成视频可视化"></p>
<blockquote>
<p><strong>图8</strong>：Real2Edit2Real在四个真实任务上生成视频的可视化。生成的视频成功重定位了物体，合成了正确的操作轨迹，并保持了多视角一致性和真实的视觉外观。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了Real2Edit2Real框架，通过深度作为3D控制接口，首次实现了从少量真实RGB演示直接生成具有新颖空间配置和轨迹、视觉真实且交互正确的多视角操作视频。</li>
<li>设计了<strong>混合训练的度量尺度几何重建模块</strong>，解决了稀疏机器人视角下的准确重建问题；提出了<strong>深度可靠的空间编辑流程</strong>，特别是机器人姿态校正，确保了生成深度图的物理一致性；开发了<strong>3D控制的多视角视频生成模型</strong>，利用双注意力机制和多重条件实现高质量合成。</li>
<li>在四个真实机器人任务上验证了框架的有效性，仅需1-5个源演示生成的数据，即可使策略达到或超越使用50个真实演示的性能，实现了10-50倍的数据效率提升，并展示了在高度、纹理编辑上的扩展灵活性。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确陈述具体的局限性，但根据方法描述，框架需要多视角RGB输入作为起点，且依赖于一个需要大量仿真和真实数据预训练的几何重建模型及视频生成模型，计算成本可能较高。</p>
<p><strong>后续启示</strong>：</p>
<ol>
<li>本框架将3D编辑与2D生成相结合的思路，为机器人数据增强提供了一个新的、统一的范式，可推广至更多样的操作任务和场景编辑（如光照、物体形状）。</li>
<li>深度作为连接几何与视觉的“接口”这一核心洞察，可能启发更多利用中间3D表征来桥接不同模态、提升数据生成或策略泛化能力的研究。</li>
<li>框架的扩展性（如高度、纹理编辑）表明，它有望成为一个基础平台，通过组合不同的编辑操作，系统性生成覆盖大规模状态空间的数据，用于训练更鲁棒的通用机器人策略。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Real2Edit2Real框架，旨在解决机器人学习因空间泛化需大量演示数据而成本高昂的问题。其核心方法是通过多视角RGB观测进行三维场景重建，在点云上进行深度可靠的3D编辑以生成新轨迹，并利用以深度为主控信号的多条件视频生成模型合成增强的多视角操作视频。实验表明，在四个真实操作任务中，仅用1-5个源演示生成的数据训练的策咯，其性能可匹配甚至超越使用50个真实演示训练的策咯，数据效率提升高达10-50倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.19402" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>