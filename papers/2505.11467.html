<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.11467" target="_blank" rel="noreferrer">2505.11467</a></span>
        <span>作者: Kashyap, Abhishek, Andreasson, Henrik, Stoyanov, Todor</span>
        <span>日期: 2025/05/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人视觉抓取通常依赖一个或多个相机视角来感知场景。虽然从多个视角采集图像有助于减少遮挡、获得更完整的信息，但移动相机到不同位置需要时间，且可能受限于机器人机械臂的可达性。因此，在额外信息带来的抓取精度提升与采集额外视图的时间成本之间存在权衡。以NeRF和高斯溅射（Gaussian Splatting）为代表的辐射场技术，能够从用户指定的新视角渲染出逼真的虚拟图像。本文针对“获取额外真实视角成本高”这一痛点，提出利用辐射场合成的新视角（Novel Views）作为低成本、灵活的额外信息源，来辅助生成抓取位姿。核心思路是：使用少量稀疏采样的真实视图构建高斯溅射场景表示，从中渲染出新视角，并利用预训练的抓取检测网络在这些新视角上推断抓取位姿，以期增加抓取的数量和覆盖范围。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程清晰明确：1) 从数据集中选取少量真实视图；2) 使用高斯溅射构建场景的辐射场表示；3) 从该辐射场渲染出一组新视角；4) 分别在真实视图和新视图上运行抓取检测网络；5) 对生成的抓取进行后处理和质量评估。</p>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/viewpoints/viewpoints_2.jpg" alt="相机位姿与场景示例"></p>
<blockquote>
<p><strong>图1</strong>：示例场景重建，展示了相机位姿（视锥体）。红色视锥体代表用于构建辐射场的真实视图（M=3），蓝色视锥体代表从中渲染的新视角（N=16）。所有相机均朝下指向桌面场景。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/graspnet_scenes/scene_0000/0000.jpg" alt="真实视图示例"></p>
<blockquote>
<p><strong>图2</strong>：用于构建辐射场的示例真实视图之一。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>场景表示构建</strong>：采用基于高斯溅射的SplaTAM框架。由于实验数据集（Graspnet-1billion）已提供相机位姿，因此跳过了相机跟踪步骤。SplaTAM通过优化一组3D高斯模型来表征场景，能够渲染新视角的颜色和深度图像。优化过程通过最小化所有关键帧图像的RGB和深度误差来实现。</li>
<li><strong>视图选择</strong>：<ul>
<li>**真实视图 (M=3)**：从数据集的256个视角中选取3个，模拟机器人眼在手系统中为最小化运动而只能获取有限视图的现实场景。</li>
<li>**新视角 (N=16)**：从构建好的高斯溅射模型中渲染。这些新视角被选择在靠近真实视图的位置并具有相似的朝向，以期望预训练网络能在与训练数据相似的视角上产生更高质量的抓取。</li>
</ul>
</li>
<li><strong>抓取推断</strong>：将优化后的高斯模型投影到M个真实和N个新视角，生成对应的颜色和深度图像，进而创建点云。使用Graspnet-1billion的预训练抓取检测网络，分别在真实视图点云和新视角点云上推断抓取位姿，分别记为 (G_{\text{real}}) 和 (G_{\text{nvs}})。</li>
<li><strong>抓取后处理与评估</strong>：<ul>
<li><strong>聚合</strong>：将 (G_{\text{real}}) 和 (G_{\text{nvs}}) 简单合并为 (G_{\text{real+nvs}})。</li>
<li><strong>后处理</strong>：采用三条独立分支：(i) 对抓取应用位姿非极大值抑制（pose-NMS），合并距离过近的抓取；(ii) 应用聚类和顶部抓取过滤（保留预测分数前50%的抓取，按位姿聚类，每类仅保留最佳抓取）；(iii) 保持原始抓取不变。</li>
<li><strong>质量评估</strong>：使用Dex-net 2.0中的方法评估抓取是否满足力闭合（force-closure），这是一个二元标签。在5个不同的静摩擦系数（μ ∈ {0.2, 0.4, 0.6, 0.8, 1.0}）下进行测试，任一系数下满足即判为力闭合抓取。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/graspnet_scenes/scene_0000/0210.jpg" alt="聚类与过滤流程"></p>
<blockquote>
<p><strong>图3</strong>：聚类和顶部抓取过滤的工作流程图。对抓取集合进行排序、保留前50%、按阈值聚类，并在每个聚类中最终只保留一个最佳抓取。</p>
</blockquote>
<p><strong>创新点</strong>：本文的核心创新在于首次系统地探索并验证了将辐射场合成的新视角专门用于抓取生成任务的可行性与价值。与之前使用辐射场进行抓取或操纵的工作不同，本研究聚焦于利用新视角作为低成本、可自由生成的“额外传感器数据”，来补充稀疏真实视图的信息，从而提升抓取集的丰富性和场景覆盖度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/平台</strong>：Graspnet-1billion数据集，包含190个桌面场景，每个场景有256个已知位姿的RGB-D图像。</li>
<li><strong>Baseline</strong>：仅使用M=3个真实视图生成的抓取 ((G_{\text{real}}))。</li>
<li><strong>对比方法</strong>：使用M=3个真实视图加上N=16个新视图生成的抓取 ((G_{\text{real+nvs}}))，并单独分析新视图的贡献 ((G_{\text{nvs}}))。</li>
</ul>
<p><strong>场景重建质量</strong>：使用SplaTAM的指标进行评估。在190个场景上平均，峰值信噪比（PSNR）为30.608（越高越好），多尺度结构相似性（MS-SSIM）为0.984（越高越好），学习感知图像块相似度（LPIPS）为0.053（越低越好），深度L1损失为0.105（越低越好），表明重建质量较高。</p>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/rendered_vs_gt/rendered/0030.jpg" alt="渲染与真实图像对比1"><br><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/rendered_vs_gt/gt/0030.jpg" alt="真实图像对比1"></p>
<blockquote>
<p><strong>图5 &amp; 图6</strong>：表I的示例之一，展示了在新视角下渲染的RGB图像（左）与真实图像（右）的对比，可见渲染质量较高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/rendered_vs_gt/rendered/0120.jpg" alt="渲染与真实图像对比2"><br><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/rendered_vs_gt/gt/0120.jpg" alt="真实图像对比2"></p>
<blockquote>
<p><strong>图7 &amp; 图8</strong>：表I的另一组渲染（左）与真实（右）对比示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/rendered_vs_gt/rendered/0240.jpg" alt="渲染与真实图像对比3"><br><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/rendered_vs_gt/gt/0240.jpg" alt="真实图像对比3"></p>
<blockquote>
<p><strong>图9 &amp; 图10</strong>：表I的第三组渲染（左）与真实（右）对比示例。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>力闭合抓取数量增加</strong>：新视角贡献了额外的力闭合抓取。未经后处理时，增益最为显著。直方图显示，约17个场景从新视角获得了约700个额外的力闭合抓取，甚至有2个场景各获得了近1400个额外抓取。经过位姿NMS或聚类过滤后，额外抓取数量减少，但仍存在增益。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/histograms/fc_grasp_histogram.jpg" alt="力闭合抓取数量直方图"></p>
<blockquote>
<p><strong>图11</strong>：N=16个新视角为190个场景额外贡献的力闭合抓取数量直方图。显示了未经后处理（All grasps）、应用位姿NMS后（After pose-NMS）和应用聚类过滤后（After clustering）的分布。多数场景获得了额外抓取。</p>
</blockquote>
<ol start="2">
<li><strong>抓取覆盖率提升</strong>：抓取覆盖率定义为场景中至少有一个力闭合抓取位姿的物体数量占总物体数的百分比。新视角为原本在真实视图下没有抓取位姿的物体提供了抓取。直方图表明，大多数场景通过新视角增加了1到2个可抓取物体。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/histograms/gc_histogram.jpg" alt="抓取覆盖率增加直方图"></p>
<blockquote>
<p><strong>图12</strong>：N=16个新视角为190个场景的抓取覆盖率额外贡献的物体数量直方图。表明新视角有效增加了可抓取物体的数量。</p>
</blockquote>
<ol start="3">
<li><strong>定性结果示例</strong>：表II及对应的抓取位姿可视化图展示了一个具体场景（9个物体）的结果。例如，经过位姿NMS后，仅使用真实视图的抓取覆盖率为77.78%（7/9），而加入新视图后覆盖率达到了100%（9/9）。新视图自身也实现了100%的覆盖率。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/grasp_poses/pose_nms/scene_0093/real.jpg" alt="示例场景-真实视图抓取(NMS)"></p>
<blockquote>
<p><strong>图13</strong>：示例场景（表II）中，仅使用真实视图并经过位姿NMS后的抓取位姿可视化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/grasp_poses/clustering/scene_0093/real.jpg" alt="示例场景-真实视图抓取(聚类)"></p>
<blockquote>
<p><strong>图14</strong>：同一示例场景中，仅使用真实视图并经过聚类过滤后的抓取位姿可视化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/grasp_poses/pose_nms/scene_0093/nvs.jpg" alt="示例场景-新视角抓取(NMS)"></p>
<blockquote>
<p><strong>图15</strong>：同一示例场景中，仅使用新视角并经过位姿NMS后的抓取位姿可视化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/grasp_poses/clustering/scene_0093/nvs.jpg" alt="示例场景-新视角抓取(聚类)"></p>
<blockquote>
<p><strong>图16</strong>：同一示例场景中，仅使用新视角并经过聚类过滤后的抓取位姿可视化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/grasp_poses/pose_nms/scene_0093/real+nvs.jpg" alt="示例场景-混合视图抓取(NMS)"></p>
<blockquote>
<p><strong>图17</strong>：同一示例场景中，结合真实视图与新视角并经过位姿NMS后的抓取位姿可视化，覆盖了所有物体。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.11467v1/extracted/6446396/images/grasp_poses/clustering/scene_0093/real+nvs.jpg" alt="示例场景-混合视图抓取(聚类)"></p>
<blockquote>
<p><strong>图18</strong>：同一示例场景中，结合真实视图与新视角并经过聚类过滤后的抓取位姿可视化。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>实证表明，利用辐射场合成的新视角能够产生额外的力闭合抓取位姿，是对稀疏真实视图抓取结果的有效补充。</li>
<li>证明了新视角合成能够提高场景的抓取覆盖率，即能为更多物体找到可行的抓取配置。</li>
</ol>
<p><strong>局限性</strong>：论文明确指出，力闭合抓取数量的增加并不能保证所有抓取都可在真实机器人上执行，可能受限于可达性、碰撞风险或执行过程中扰动其他物体等因素。此外，抓取覆盖率百分比是一个乐观的上界，因为与物体关联的抓取可能因上述原因而无法执行。</p>
<p><strong>后续研究启示</strong>：</p>
<ul>
<li>需要在真实机器人系统上验证该方法的有效性。</li>
<li>未来方向包括：将真实视图数量减少到单张图像（结合扩散模型或泛化辐射场）；研究如何主动选择最优的新视角以最大化抓取信息增益；改进从辐射场中直接提取或优化抓取位姿的过程。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究机器人抓取生成中因相机移动受限导致多视角图像获取困难的问题。提出利用辐射场（如高斯泼溅）技术，从稀疏采样的真实视图中合成新视角图像，为抓取姿态生成提供额外场景信息。在Graspnet-10亿数据集上的实验表明，合成的新视图能够补充力闭合抓取，并提高抓取覆盖率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.11467" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>