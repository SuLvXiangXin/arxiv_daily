<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>IGen: Scalable Data Generation for Robot Learning from Open-World Images - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>IGen: Scalable Data Generation for Robot Learning from Open-World Images</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.01773" target="_blank" rel="noreferrer">2512.01773</a></span>
        <span>作者: Zhi Wang Team</span>
        <span>日期: 2025-12-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>通用机器人策略的兴起对大规模训练数据产生了指数级需求。然而，在真实机器人上收集数据劳动密集，且通常局限于特定环境。相比之下，开放世界图像能以极低成本获取，并包含了与真实机器人操作任务自然对齐的、极其多样化的场景，为低成本、大规模机器人数据获取提供了有前景的途径。尽管潜力巨大，但开放世界图像缺乏关联的机器人动作，阻碍了其在实际机器人学习中的应用，使得这一丰富的视觉资源未被充分开发。现有方法存在关键局限：基于“真实-仿真-真实”的方法需要显式的物理场景重建，无法利用任意的开放世界图像，可扩展性受限；而基于视频生成模型的方法则无法提供明确的机器人动作，且难以处理长视野或复杂指令驱动的任务。本文旨在弥合这一鸿沟，提出IGen框架，其核心思路是：将非结构化的2D开放世界图像转换为结构化的3D场景表示，利用视觉语言模型的推理能力生成高层任务规划和低层SE(3)动作轨迹，并基于此合成动态场景演化，最终渲染出时间一致的视觉观测数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>IGen是一个从开放世界图像合成任务特定机器人动作和视觉观测的数据生成框架。其架构包含三个主要阶段：1) 场景重建，将输入图像转换为机器人可操作的工作空间；2) 动作规划，在空间关键点上进行推理以生成动作轨迹；3) 观测合成，合成并渲染点云序列以生成任务的视觉观测。</p>
<p><img src="https://arxiv.org/html/2512.01773v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：IGen概览。给定一张开放世界图像和任务描述，IGen首先通过基础视觉模型将环境和物体重建为点云。提取空间关键点后，视觉语言模型将任务描述映射为高层计划和低层控制指令。机器人在仿真中执行时，虚拟深度相机捕捉运动点云序列。最终得到的末端执行器位姿轨迹用于合成动态点云序列，并逐帧渲染为操作任务的视觉观测。最终输出包含生成的机器人动作和视觉观测。</p>
</blockquote>
<p><strong>1. 场景重建：从像素到结构化3D表示</strong><br>核心是将原始像素转换为机器人能够有效解释和执行动作的结构化表示。IGen采用3D点云作为中间模态。具体流程如下：</p>
<ul>
<li><strong>深度与几何估计</strong>：使用单目几何基础模型（如Metric3D）估计场景深度和几何结构。</li>
<li><strong>物体分割</strong>：使用Segment Anything Model获取任务相关物体的掩码 ℳ。</li>
<li><strong>关键点提取</strong>：遵循[huang2024rekep]中的关键点表示，使用DINOv2提取场景特征，并对特征嵌入和3D坐标进行K-means聚类，得到一组K个空间关键点 𝒦 = {k_j ∈ ℝ³} 及其关联的3D坐标。</li>
<li><strong>背景重建与点云生成</strong>：使用图像修复模型从原始图像中移除目标可操作物体并重建背景图像 𝐈_bg。结合估计的深度图 𝐃 和相机内参矩阵 𝐊，将所有像素提升到3D空间，形成密集的3D背景点云 P_bg。</li>
<li><strong>物体完整3D重建</strong>：为解决单目视图下物体建模不完整的问题，使用3D生成式重建模型对目标物体的完整3D形状和外观进行重建，形成密集物体点云，并通过6D姿态估计将其重新定位到原始姿态。</li>
<li><strong>空间域随机化</strong>：从支撑表面（如桌面）提取一组可行的放置点，作为物体泛化的候选空间位姿。</li>
</ul>
<p><strong>2. 动作规划：空间行为生成</strong><br>由于开放世界图像缺乏动作指导，IGen利用视觉语言模型强大的视觉推理和规划能力。</p>
<ul>
<li><strong>高层任务分解</strong>：将标注了关键点 𝒦、其3D坐标和任务指令的图像输入VLM。VLM将整体任务分解为一组子阶段 𝒮 = {S_i}。</li>
<li><strong>低层控制函数生成</strong>：为每个子阶段，VLM生成与关键点关联的动作描述。IGen开发了一种基于末端执行器SE(3)位姿的、易于编程的Python控制语言，将高层任务阶段 𝒮 转化为可执行的低层控制函数 ℱ = {f_i}。</li>
<li><strong>关键点驱动求解</strong>：每个函数 f_i 被定义为一个以关键点为条件的求解器，从空间锚点 𝒦_i 计算末端执行器的参考位姿。在预操作阶段，末端执行器基于抓取先验模型与物体精确交互（抓取位姿由抓取模型预测，夹爪状态由沿夹爪主轴的物体点云宽度决定）。在操作过程中，被操作物体上的关键点被视为刚性附着在末端执行器上的可移动点，而其他关键点则作为静态场景锚点。</li>
<li><strong>轨迹生成与仿真执行</strong>：从初始机器人状态 x₀ 开始，运动规划器产生一组子目标 𝒳 = {x̂<em>i}。每个阶段计算下一个状态为 x̂_i = f_i(x̂</em>{i-1})。使用运动规划器生成可行轨迹，并在仿真环境中执行，产生在T个时间步上的动作序列 𝒜 = {a_t}，每个动作 a_t 包含机器人末端执行器位姿和关节位置。</li>
</ul>
<p><strong>3. 观测合成：机器人学习经验合成</strong><br>为获得与具身动作同步的视觉观测，IGen提出了一个基于实时点云渲染的机器人经验合成框架。</p>
<ul>
<li><strong>动作点云合成</strong>：将仿真机器人置于规划位姿 p_robot，虚拟相机置于 p_cam（与生成场景点云的视点对齐）。从动作序列 𝒜 中获得末端执行器位姿轨迹 𝒯 = {𝐓_t}。在每个时间步 t，渲染环境产生同步的RGB和深度帧，并通过虚拟相机 𝐂 反投影，构建机器人运动的点云序列 𝒫_robot。</li>
<li><strong>动态交互合成</strong>：背景被建模为静态点云序列 𝒫_bg。基于末端执行器位姿的变换，合成末端执行器与点云之间的动态交互。假设在时间 t_g 建立抓取，物体位姿为 𝐓_obj,t_g，末端执行器位姿为 𝐓_t_g。对于所有夹爪闭合在物体上的时间步 t ∈ 𝒯_grasp，物体的世界位姿通过刚性地跟随末端执行器而演变。被操作物体的点云序列 𝒫_obj 经历由末端执行器位姿引起的刚体变换（公式1）。</li>
<li><strong>完整观测合成与渲染</strong>：组合静态环境、机器人和被操作物体，将完整任务表示为复合点云序列：𝒫_task = 𝒫_bg ∪ 𝒫_obj ∪ 𝒫_robot。通过虚拟相机 𝐂 渲染该点云序列，得到视觉观测 𝒪。时间同步的观测 𝒪 与动作序列 𝒜 共同构成了用于机器人学习的配对视觉-动作数据。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，IGen的创新在于：1) 将任意开放世界图像转化为可操作的3D场景表示，无需精确的物理属性估计或特定环境的多视角重建；2) 利用VLM在3D关键点空间进行任务规划和低层控制代码生成，实现了指令驱动的、物理合理的动作生成；3) 提出了一种免仿真的点云合成方法，基于SE(3)轨迹直接合成动态视觉观测，支持大规模机器人经验生成，计算效率高。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验旨在回答三个问题：1) IGen能否从各种场景的真实世界图像生成视觉上逼真的数据？2) IGen能否高效生成与环境对齐且遵循任务指令的机器人动作？3) IGen能否仅从单张图像合成有效的机器人训练数据，使得策略训练和真实世界部署无需任何人工遥操作演示？</p>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/基准</strong>：Simpler数据集（用于视觉保真度评估）、DreamGen Bench（用于行为生成质量评估）。</li>
<li><strong>对比基线</strong>：视觉保真度对比：Real-to-Sim方法（来自Simpler）。行为生成质量对比：Cosmos-Predict2、TesserAct。策略训练对比：零样本、使用10/100条人类遥操作数据微调、使用1000条IGen合成数据微调。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><strong>1. 场景重建保真度</strong>：<br>在Simpler数据集上，比较IGen与Real-to-Sim方法重建场景与原始真实场景的视觉一致性。</p>
<p><img src="https://arxiv.org/html/2512.01773v1/x1.png" alt="视觉保真度定量结果表"></p>
<blockquote>
<p><strong>表1</strong>：IGen与Real-to-Sim方法在视觉相似性指标上的对比。IGen在PSNR、SSIM、FID及多个LPIPS变体上均显著优于Real-to-Sim基线，例如在LPIPS相似性上取得了高达5.13倍的提升，表明IGen生成的场景与真实图像高度一致。</p>
</blockquote>
<p><strong>2. 行为生成评估</strong>：</p>
<ul>
<li><strong>定性实验</strong>：</li>
</ul>
<p><img src="https://arxiv.org/html/2512.01773v1/x3.png" alt="行为生成定性对比"></p>
<blockquote>
<p><strong>图3</strong>：给定单张图像和自然语言操作指令，IGen与基线方法生成的行为观测对比。IGen产生了更符合指令、物理一致性的物体运动，而基线方法存在时间不连续、几何扭曲和动作执行不准确等问题。</p>
</blockquote>
<ul>
<li><strong>定量实验</strong>：</li>
</ul>
<p><img src="https://arxiv.org/html/2512.01773v1/x4.png" alt="行为生成定量对比"></p>
<blockquote>
<p><strong>图4</strong>：在DreamGen Bench上，使用GPT-4o、Qwen-3-VL-Plus和GLM-4.5V作为评估模型，IGen在“指令跟随”和“物理对齐”两个指标上均优于所有对比模型。例如，在Qwen评估下，IGen的指令跟随成功次数接近基线的两倍；在GLM评估下，IGen生成视频的物理一致性达到100%。</p>
</blockquote>
<ul>
<li><strong>计算效率</strong>：</li>
</ul>
<p><img src="https://arxiv.org/html/2512.01773v1/x5.png" alt="计算效率对比"></p>
<blockquote>
<p><strong>图5</strong>：IGen在生成效率和GPU内存消耗上显著优于基线。IGen平均每样本生成时间约18.6秒，GPU内存消耗8.3GB，效率分别比TesserAct和Cosmos-Predict2高出约30倍和200倍。</p>
</blockquote>
<p><strong>3. 从非结构化图像进行机器人学习</strong>：<br>实验设置如图6所示，从单张真实场景图像出发，IGen自动生成1000条带有空间随机化的任务演示数据，用于训练视觉运动策略，随后在真实世界部署评估。</p>
<p><img src="https://arxiv.org/html/2512.01773v1/x6.png" alt="实验设置图示"></p>
<blockquote>
<p><strong>图6</strong>：策略训练与评估的实验流程。IGen从单张图像生成大规模数据，用于训练策略，并在真实世界测试。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.01773v1/x7.png" alt="真实世界策略评估结果"></p>
<blockquote>
<p><strong>图7</strong>：真实世界任务成功率对比。仅使用1000条IGen合成数据训练的模型，其性能与使用100条人类遥操作数据微调的模型相当，甚至部分任务更优，并且显著优于零样本和使用10条人类数据微调的模型。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.01773v1/x8.png" alt="策略学习有效性图示"></p>
<blockquote>
<p><strong>图8</strong>：策略学习有效性示意图。展示了在IGen数据上训练的模型能生成更准确的任务轨迹（蓝色），而在有限真实数据上训练的模型轨迹（红色）则偏离目标。</p>
</blockquote>
<p><strong>消融实验</strong>：论文虽未以独立章节呈现系统的消融实验，但通过上述核心实验（如对比Real-to-Sim、其他生成模型，以及不同数据源训练策略的效果）已间接验证了其整体框架及各组件（3D重建、VLM规划、点云合成渲染）的有效性和必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个有效的数据生成框架IGen，能够从开放世界图像中生成可扩展的视觉-动作数据集，集成了跨场景泛化、指令多样性和长视野任务适用性。</li>
<li>将非结构化的开放世界图像转化为可操作的3D场景表示，实现了机器人任务推理和运动规划，生成了与物理世界对齐的任务一致行为；并引入了一种免仿真的点云合成方法，生成逼真的视觉观测，支持大规模机器人经验生成。</li>
<li>通过实验证明，仅使用IGen生成数据训练的机器人策略能够成功执行真实世界操作任务，无需任何额外数据收集，这确立了真实世界图像作为机器人策略训练有效来源的潜力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1) 场景重建和动作生成的准确性依赖于所采用的外部基础模型（如深度估计、3D重建、VLM）的性能上限。2) 在观测合成阶段，对物体与机器人/环境交互的建模主要基于刚体变换，可能无法完美模拟所有复杂的物理交互细节（如形变、软体接触）。</p>
<p><strong>对后续研究的启示</strong>：<br>IGen为利用海量、低成本的开放世界图像资源进行机器人学习开辟了一条新路径。其方法表明，结合强大的基础视觉模型和语言模型，将2D感知提升到3D结构化理解，并辅以程序化的动作规划和渲染合成，是构建大规模机器人仿真数据的一种高效且可扩展的方案。未来的工作可以沿着以下方向探索：提升基础模型的精度和鲁棒性以改善生成数据质量；引入更复杂的物理仿真以合成更逼真的交互；探索如何将生成的数据与少量真实数据结合以进一步提升策略性能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对开放世界图像缺乏机器人动作数据、难以直接用于策略训练的问题，提出IGen数据生成框架。该框架首先将2D图像转换为结构化3D场景表示，进而利用视觉语言模型进行任务推理，生成SE(3)末端执行器姿态序列作为动作，并合成动态、时序一致的视觉观察。实验验证，IGen能生成高质量的视觉-动作数据，仅使用其合成数据训练的策略，在真实场景中取得了与使用真实数据训练的策略相当的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.01773" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>