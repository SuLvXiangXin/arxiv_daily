<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.12560" target="_blank" rel="noreferrer">2510.12560</a></span>
        <span>作者: Jiangtao Gong Team</span>
        <span>日期: 2025-10-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，端到端学习已成为自动驾驶的主流范式，其中大多数方法依赖于模仿学习，本质上是监督学习，通过直接匹配专家轨迹进行训练。这种方法依赖于数据独立同分布的假设，但在驾驶这类具身任务中，该假设不成立，导致智能体泛化能力差，在长尾场景下表现不佳。强化学习通过奖励最大化鼓励探索，但面临样本效率低、收敛不稳定等问题，且在模拟器中训练会面临高保真专家数据缺失和仿真到现实迁移的挑战。本文针对单纯模仿学习泛化能力弱，以及传统“模仿学习预训练-强化学习微调”两阶段范式可能存在的梯度冲突和知识交换有限等痛点，提出了一种新的协作-竞争视角。核心思路是提出一个竞争性的双策略框架，让模仿学习和强化学习智能体在训练过程中直接交互，通过基于竞争的机制促进知识交换，同时利用潜在世界模型作为离线模拟器进行基于想象的训练，以提升泛化能力和长尾场景性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>CoIRL-AD的整体框架是一个集成了模仿学习和强化学习的双策略架构，共享一个潜在世界模型。在每次迭代中，IL演员和RL演员并行训练。潜在世界模型在IL阶段学习，然后在RL阶段被冻结使用，仅更新RL演员和批评家。RL演员通过组采样探索多个动作序列，利用世界模型预测未来状态，并通过基于规则的奖励函数进行评估。批评家根据想象出的未来状态和奖励为每个序列分配优势值。最后，通过竞争性学习机制促进两个演员之间的知识交换。</p>
<p><img src="https://arxiv.org/html/2510.12560v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：CoIRL-AD方法整体框架。采用双策略架构，通过共享的潜在世界模型整合模仿学习和强化学习。左侧为IL训练阶段，右侧为RL训练阶段，包含组采样、世界模型想象和竞争性学习机制。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>演员建模与潜在世界模型</strong>：感知模块将观测编码为潜在状态<code>s</code>。规划头通过交叉注意力提取航路点特征<code>s_w</code>，并解码为动作序列<code>τ_a</code>。模仿学习使用L1损失<code>L_imi</code>监督输出匹配专家轨迹<code>τ_a^e</code>。潜在世界模型以当前状态<code>s</code>和动作<code>τ_a</code>为输入，预测未来状态<code>s&#39;^</code>，通过均方误差损失<code>L_wm</code>进行自监督训练。总IL损失为<code>L_IL = L_imi + α·L_wm</code>。</li>
<li><strong>反向规划</strong>：论文提出了“反向规划”概念，即第i个动作的条件概率基于当前及未来的航路点特征：<code>π_i(a_i | s_w,j≥i)</code>。这通过自注意力层中的逆因果掩码实现，模拟了人类“先定目标，再执行动作”的推理模式。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.12560v1/x2.png" alt="规划方式对比"></p>
<blockquote>
<p><strong>图2</strong>：前向规划（使用因果掩码）与反向注意力（使用逆因果掩码）的对比示意图。反向规划使早期动作能获得更丰富的上下文信息。</p>
</blockquote>
<ol start="3">
<li><strong>强化学习与组采样</strong>：奖励由模仿奖励<code>r_imi</code>和碰撞奖励<code>r_col</code>构成，<code>r_i = r_col^(i) · r_imi^(i)</code>。为引入探索，模型使用随机头输出动作的不确定性<code>τ_σ</code>，假设动作服从高斯分布。受GRPO启发，采用组采样：从策略中采样G条轨迹，用规则奖励函数计算奖励序列<code>τ_r^(g)</code>。为评估长期优势，训练一个批评家模型V，并利用潜在世界模型对采样动作进行想象滚动物<code>ŝ‘^(g)</code>，计算长期优势<code>A_long^(g) = (Σr^(g) + γ·V(ŝ‘^(g))) - V(s)</code>，再进行组内Z-score归一化得到批评家优势<code>A_cri^(g)</code>。演员和批评家的损失分别为<code>L_act</code>和<code>L_cri</code>。总RL损失为<code>L_RL = L_act + L_cri + β·L_bc</code>，其中<code>L_bc</code>为行为克隆损失，用于稳定训练。此外，采用了“单步随机”的平滑机制和双批评家（EMA）技巧以稳定训练。</li>
<li><strong>双策略竞争学习框架</strong>：IL演员和RL演员分别由<code>L_IL</code>和<code>L_RL</code>优化。为促进交互，引入了基于规则的竞争机制。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.12560v1/x3.png" alt="竞争机制流程图"></p>
<blockquote>
<p><strong>图3</strong>：基于规则的竞争学习机制流程图。定期比较IL和RL演员的累积奖励得分，根据分差（可比、中等优势、显著优势）采用不同的权重合并或替换策略，实现知识选择性迁移。</p>
</blockquote>
<p><strong>创新点</strong>：</p>
<ul>
<li>提出了“反向规划”（逆因果掩码）的动作建模方式。</li>
<li>在离线设置下，通过组采样和潜在世界模型想象，实现了基于模型的强化学习探索，无需外部模拟器。</li>
<li>设计了竞争性双策略框架，使IL和RL在训练中动态交互、知识共享，超越了简单的损失相加或两阶段范式。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准数据集</strong>：nuScenes（开环评估）和Navsim（闭环评估）。</li>
<li><strong>实验平台</strong>：8张NVIDIA A800 GPU。</li>
<li><strong>基线方法</strong>：在nuScenes上对比了ST-P3、UniAD、VAD、BEV-Planner、PARA-Drive、GenAD、SparseDrive、UAD、World4Drive、SSR、LAW等；在Navsim上对比了Human、Ego Status MLP、VADv2、UniAD、PARA-Drive、Transfuser、LAW、Hydra-MDP、WoTE等。</li>
<li><strong>评估指标</strong>：nuScenes使用L2位移误差和碰撞率；Navsim使用综合驾驶得分PDMS及其子指标（无责碰撞NC、可行驶区域合规DAC等）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在nuScenes上，将CoIRL-AD应用于SSR和LAW基线。SSR+CoIRL-AD在无任何额外监督的方法中取得了最佳的碰撞率（平均0.15%），相比SSR基线（0.48%）有显著提升。LAW+CoIRL-AD (ADCGS) 相比LAW基线，在L2误差（0.63m vs 0.66m）和碰撞率（0.18% vs 0.22%）上均有改善。论文指出，相比基线，碰撞率降低了约18%。</p>
<p><img src="https://arxiv.org/html/2510.12560v1/x4.png" alt="泛化与长尾性能"></p>
<blockquote>
<p><strong>图4</strong>：(a) 在新加坡训练、波士顿测试的跨城市泛化能力对比；(b) 在根据基线模型识别出的高L2误差和高碰撞率长尾场景子集上的性能。CoIRL-AD在泛化和长尾场景下均优于LAW基线。</p>
</blockquote>
<p>在Navsim测试集上，WoTE+CoIRL-AD取得了88.2的PDMS，优于WoTE（87.9）及其他SOTA方法，在NC、DAC、TTC等安全相关指标上也有提升。</p>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>因果关系消融（表3）</strong>：验证反向规划的有效性。使用逆因果掩码在L2误差和碰撞率上均优于基线、无掩码和标准因果掩码设置。</li>
<li><strong>IL与RL集成策略消融（表4）</strong>：对比了损失合并、交替训练、两阶段、解耦演员（无竞争）和解耦演员（有竞争）五种策略。仅“解耦演员（有竞争）”版本在L2和碰撞率上均优于纯IL基线。纯RL效果很差（平均L2误差6.55m，碰撞率4.93%），说明单纯RL难以收敛，凸显了IL引导和竞争机制的重要性。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.12560v1/x5.png" alt="训练动态分析"></p>
<blockquote>
<p><strong>图5</strong>：训练过程中IL与RL演员的累计获胜次数（上）和得分差（IL分 - RL分，下）变化。显示早期IL主导，后期RL通过有效探索反超，两者通过竞争机制实现动态知识传递。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了CoIRL-AD，一个在潜在世界模型中协作-竞争地结合模仿学习与强化学习的端到端自动驾驶框架。</li>
<li>引入了“反向规划”（逆因果掩码）的动作建模范式，更贴合人类驾驶的“目标到动作”推理过程。</li>
<li>设计了一种双策略竞争学习机制，使IL和RL演员能够动态交互、选择性共享知识，有效提升了泛化能力、安全性和长尾场景性能。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到：1）使用的奖励函数较为简单（仅模仿和碰撞）；2）采用了基础的演员-批评家算法，而非更稳定的算法（如PPO）；3）依赖世界模型进行非反应式模拟，可能引入偏差。</p>
<p><strong>启示</strong>：<br>本研究为在离线数据上安全、高效地结合IL与RL提供了新思路。竞争性双策略框架可作为一种通用的范式，用于其他需要平衡模仿与探索的序列决策任务。未来工作可探索更复杂的奖励设计、更稳定的RL算法，以及如何减少世界模型预测偏差对RL训练的影响。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出CoIRL-AD框架，旨在解决自动驾驶中模仿学习（IL）泛化能力差、强化学习（RL）样本效率低的问题。核心方法为竞争协作式双策略学习，使IL与RL智能体在潜在世界模型中交互训练，通过竞争机制促进知识交换并避免梯度冲突。在nuScenes数据集上的实验表明，该方法相比基线模型碰撞率降低18%，且在长尾场景中展现出更强的泛化性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.12560" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>