<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.08795" target="_blank" rel="noreferrer">2506.08795</a></span>
        <span>作者: Xianta Jiang Team</span>
        <span>日期: 2025-06-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的假手控制方法包括基于表面肌电（sEMG）的模式识别和基于视觉的半自主控制。sEMG方法要求用户为每次操作反复产生特定的肌电信号，存在信号噪声、对电极位置敏感、串扰、个体生理差异以及肌肉疲劳等关键局限性。视觉半自主方法虽然通过摄像头识别物体并简化了sEMG控制（如仅用作开/关触发），但仍需用户在执行抓握的每个阶段产生肌电信号，未能从根本上解决sEMG的固有问题，且通常依赖于预定义的手势库，泛化能力受限。</p>
<p>本文针对传统方法严重依赖用户主动生成生物信号（尤其是sEMG）这一核心痛点，提出了一个全新的视角：实现完全自主的假手控制，彻底免除生物信号输入。其核心思路是，仅利用安装在手腕上的摄像头（提供视觉）以及假手自身的关节位置和指尖力传感器（提供本体感知），通过模仿学习算法，让假手直接从人类演示中学习如何像真人一样自主地抓握、转移和释放物体。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架分为三个阶段：专家演示数据收集、模型训练（模仿学习）和测试推理。</p>
<p><img src="https://arxiv.org/html/2506.08795v1/x1.png" alt="方法整体流程"></p>
<blockquote>
<p><strong>图1</strong>：方法整体流程。(a) 专家演示阶段：通过遥操作系统收集多模态数据，包括腕视图图像、关节位置（运动）、指尖力（触觉）以及由专家执行的目标动作。(b) 训练阶段：提出的视觉-触觉-运动变分自编码器（VTM-VAE）以图像、历史本体感知和未来k帧目标动作为条件，学习潜空间分布并重构动作。(c) 测试阶段：仅使用训练好的VTM-VAE解码器，从标准高斯分布采样潜变量，结合当前图像和本体感知，生成平滑、准确的控制动作。</p>
</blockquote>
<p>核心模块是<strong>视觉-触觉-运动变分自编码器（VTM-VAE）</strong>。它是一个条件变分自编码器（CVAE）的变体，旨在学习人类演示动作的条件分布。</p>
<p><strong>专家演示数据</strong>：在时间步t，数据包括腕视图图像 $\mathbf{I}_t$ (320x240x3)、6自由度关节位置 $\mathbf{q}_t$、30维指尖力 $\mathbf{f}_t$（组合为<strong>本体感知</strong> $\mathbf{h}_t$），以及由专家提供的6维<strong>目标动作</strong> $\mathbf{a}_t$（期望的关节位置命令）。</p>
<p><strong>VTM-VAE编码器</strong>：输入包括两个可学习的CLS令牌、历史本体感知 $\mathbf{h}<em>{t-p+1:t}$（p=30）和未来k个目标动作 $\mathbf{a}</em>{t:t+k-1}$（k=20）。这些嵌入被投影、拼接并添加位置编码后，送入由4层<strong>双向Mamba</strong>模块组成的骨干网络。输出序列的首尾元素平均后经投影，得到参数化各向同性高斯先验分布的均值 $\boldsymbol{\mu}$ 和对数方差 $\log\boldsymbol{\sigma}^2$。通过重参数化技巧采样得到潜变量 $\mathbf{z}$。</p>
<p><strong>VTM-VAE解码器</strong>：输入包括图像特征、历史本体感知 $\mathbf{h}<em>{t-p+1:t}$ 和潜变量 $\mathbf{z}$。图像特征通过预训练的ResNet-18提取，并经由<strong>中心感知扫描</strong>模块将2D特征图（8x10x512）展平为1D序列（80x512）。所有输入被投影、拼接并添加位置编码后，先通过4层双向Mamba模块，然后与一组代表预测动作的可学习参数（类似CLS令牌）拼接，再通过6层<strong>前缀双向Mamba</strong>模块。最后，这些可学习参数被投影为k个重构动作 $\hat{\mathbf{a}}</em>{t:t+k-1}$。</p>
<p><strong>损失函数</strong>：训练目标是最小化损失 $L = \sum_{t=1}^{T} |\hat{\mathbf{a}}<em>{t:t+k-1} - \mathbf{a}</em>{t:t+k-1}| - \frac{1}{2}(1 + \log\boldsymbol{\sigma}^2 - \boldsymbol{\mu}^2 - \boldsymbol{\sigma}^2)$，该损失结合了重构动作的L1损失和潜变量分布与标准高斯分布之间的KL散度。</p>
<p><strong>推理阶段</strong>：仅保留解码器。潜变量 $\mathbf{z}$ 直接从标准高斯分布 $\mathcal{N}(0, I)$ 中采样。应用时间集成技术以平滑生成的动作。</p>
<p><strong>Central-aware Mamba模块</strong>：这是方法的核心创新组件，用于高效处理序列数据。<br><img src="https://arxiv.org/html/2506.08795v1/x2.png" alt="Central-aware Mamba组件"></p>
<blockquote>
<p><strong>图2</strong>：Central-aware Mamba的组件示意图。(a) 中心感知扫描：将2D图像特征从外圈向内圈扫描，转化为1D序列，强调图像中心区域（物体通常所在位置）。(b) 原始Mamba块：基于结构化状态空间序列模型处理1D序列。(c) 双向Mamba：通过翻转操作使序列模型能够双向传播信息。(d) 前缀双向Mamba：在处理输入特征序列时，额外拼接一组可学习参数（如前缀），并采用条件翻转操作，使这些参数能聚合全局上下文信息。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>完全摒弃生物信号</strong>，仅依赖易于获取的腕视视觉和本体感知；2) 提出了<strong>VTM-VAE</strong>这一生成式模仿学习框架，能够从演示中学习连续、自然的抓握策略，而非选择离散的预定义手势；3) 设计了<strong>Central-aware Mamba</strong>模块，特别是中心感知扫描和双向/前缀双向结构，以高效处理视觉序列并实现更好的上下文建模。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用Ability Hand（PSYONIC）六自由度假手进行实验。通过开发的遥操作系统收集人类演示数据。训练数据仅来自<strong>单个参与者</strong>操作<strong>7个物体</strong>的演示。在测试中，评估了模型对<strong>17个未见过的物体</strong>（形状、大小、重量各异）、<strong>杂乱场景</strong>、<strong>交接任务</strong>以及<strong>其他参与者</strong>的泛化能力，且无需任何模型微调。</p>
<p><strong>对比方法</strong>：与传统的<strong>sEMG模式识别</strong>方法（需要用户为每个动作生成肌电信号）以及文献中的<strong>半自主视觉方法</strong>进行比较，突显全自主控制的优势。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>对未见物体的泛化</strong>：在17个全新测试物体上，模型取得了<strong>91.2%</strong> 的整体抓握成功率，展示了强大的从有限训练集泛化的能力。<br><img src="https://arxiv.org/html/2506.08795v1/x3.png" alt="训练物体"></p>
<blockquote>
<p><strong>图3</strong>：用于模型训练的7个物体。<br><img src="https://arxiv.org/html/2506.08795v1/x4.png" alt="部分测试物体"><br><strong>图4</strong>：部分用于测试的未见物体示例，展示了物体的多样性。</p>
</blockquote>
</li>
<li><p><strong>跨用户泛化</strong>：使用基于单用户数据训练的模型直接测试其他参与者，抓握成功率达到**89.5%**，证明了模型对个体差异的鲁棒性。<br><img src="https://arxiv.org/html/2506.08795v1/x5.png" alt="成功率对比与消融实验"></p>
<blockquote>
<p><strong>图5</strong>：左图展示了本文方法（Autonomous）与sEMG模式识别（Pattern Recognition）和半自主视觉方法（Semi-autonomous）在完成抓握任务所需步骤上的对比，本文方法步骤最少，最为自动化。右图的消融实验显示了各组件对成功率的影响。</p>
</blockquote>
</li>
<li><p><strong>消融实验</strong>：验证了各核心组件的贡献。移除<strong>中心感知扫描</strong>导致成功率下降3.5%；使用单向Mamba替代<strong>双向Mamba</strong>下降4.1%；使用Transformer替代全部Mamba模块下降5.3%；而移除<strong>前缀机制</strong>则导致显著下降7.7%。这证明了所提出的Central-aware Mamba模块的有效性。<br><img src="https://arxiv.org/html/2506.08795v1/x6.png" alt="消融实验详细结果"></p>
<blockquote>
<p><strong>图6</strong>：详细的消融实验结果柱状图，量化了移除或替换“中心感知扫描”、“双向Mamba”、“前缀双向Mamba”等组件对抓握成功率的负面影响。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了一种<strong>无需任何生物信号输入</strong>的全自主假手控制范式，极大降低了用户的身心负担；2) 设计了<strong>视觉-触觉-运动变分自编码器（VTM-VAE）</strong> 及<strong>Central-aware Mamba</strong>模块，构建了一个能够从演示中学习连续、自适应抓握策略的强大生成式模仿学习框架；3) 通过实验证明了该方法仅需<strong>单用户、少量物体的数据</strong>训练，即可在<strong>未见物体、杂乱场景、跨用户</strong>等多个维度上表现出<strong>优异的泛化能力</strong>，具备实际应用潜力。</p>
<p>论文提到的局限性包括：1) 模型的性能依赖于<strong>演示数据的质量和多样性</strong>；2) 作为全自主系统，需要考虑<strong>安全机制</strong>，例如在用户不希望抓握时如何中断或覆盖系统的自动决策。</p>
<p>本研究对后续工作的启示在于：1) <strong>演示收集效率</strong>：探索更高效或所需演示更少的模仿学习算法（如逆强化学习）。2) <strong>多传感器融合</strong>：研究如何更好地融合视觉、触觉（力感知）甚至其他模态（如接近觉）的信息。3) <strong>安全与交互</strong>：如何在不引入复杂生物信号接口的前提下，设计直观、安全的用户干预或模式切换机制，是迈向实际部署的关键。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在开发一种无需生物信号（如表面肌电）的自主假手控制系统，以解决传统方法对用户身心负担重的问题。核心方法是利用安装在手腕的摄像头进行环境感知，并通过模仿学习技术训练控制模型：首先构建远程操作系统收集人类演示数据，进而让模型学习模仿人类的抓握动作。实验表明，仅使用单个参与者的少量物体数据进行训练后，该模仿学习算法即能实现高成功率，并能良好地泛化到更多用户及不同重量的未见物体。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.08795" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>