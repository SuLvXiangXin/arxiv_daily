<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Vision Language Action Models in Robotic Manipulation: A Systematic Review - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Vision Language Action Models in Robotic Manipulation: A Systematic Review</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.10672" target="_blank" rel="noreferrer">2507.10672</a></span>
        <span>作者: Irfan Hussain Team</span>
        <span>日期: 2025-07-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>传统机器人系统依赖任务特定的编程，在动态和非结构化环境中表现不佳。而视觉语言动作模型代表了机器人领域的范式转变，其目标是将视觉感知、自然语言理解和具身控制统一在单一学习框架内，利用大规模基础模型的泛化能力，使机器人能够理解自然语言指令、感知环境并自主执行复杂任务。该领域发展迅猛，大量架构、数据集和框架被快速提出，但缺乏一个全面、系统的综述来整理和归类塑造当前VLA格局的架构基础、基准数据集、仿真平台和评估协议。本文旨在填补这一空白，对VLA范式进行全面且前瞻性的综合，特别关注机器人操作和指令驱动的自主性。本文的核心思路是系统性地分析VLA模型、数据集和仿真平台，提出新的分类与评估框架，并识别挑战与未来方向。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文并非提出一个新的VLA模型，而是对现有VLA生态系统进行系统性梳理和分析的方法论。其整体框架（Pipeline）是首先通过系统的文献检索收集VLA模型、数据集和仿真平台，然后对它们进行分类、评估和比较，最终提炼出挑战与未来路线图。</p>
<p>论文的核心分析模块包括对VLA模型架构的分类、对训练数据集的定量基准评估以及对仿真平台的深入审查。</p>
<ol>
<li><strong>VLA模型架构分类</strong>：论文将VLA模型组织成关键的架构范式，反映了在机器人系统中整合视觉、语言和控制的不同策略。主要类别包括：<ul>
<li><strong>端到端Transformer模型</strong>：使用单一Transformer主干，以图像、语言和机器人状态为输入，直接输出动作。代表模型如RT-1、RT-2。</li>
<li><strong>模块化融合框架</strong>：将预训练的视觉语言模型与独立的动作解码器或规划模块结合。</li>
<li><strong>扩散动作模型</strong>：使用扩散过程生成动作轨迹，以处理多模态性和不确定性。</li>
<li><strong>基于LLM的规划器</strong>：利用大型语言模型进行高层任务分解和规划，再通过低级控制器执行。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.10672v1/extracted/6621942/figures/VLA.png" alt="VLA架构"></p>
<blockquote>
<p><strong>图7</strong>：用于机器人操作的VLA系统架构代表。模型处理场景图像、自然语言指令和机器人内部状态三种输入，分别通过视觉、文本和状态编码器进行编码。生成的嵌入被传递给一个LLM，该LLM融合多模态信息并生成预期任务的语义表示。该表示与机器人状态特征一起，由作为扩散Transformer实现的动作解码器处理，以生成完成指令任务的轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10672v1/extracted/6621942/figures/texonomy1.jpeg" alt="VLA模型分类"></p>
<blockquote>
<p><strong>图8</strong>：VLA模型的分类法，根据其架构集成策略进行分类。主要类别包括端到端Transformer、模块化融合框架、扩散动作模型和基于LLM的规划器。</p>
</blockquote>
<ol start="2">
<li><p><strong>数据集定量基准框架</strong>：论文引入了一个新颖的定量框架来系统地对VLA数据集进行基准测试。该框架使用两个经验调整的指标：</p>
<ul>
<li><strong>任务复杂度（𝒞_task）</strong>：基于任务数量、对象多样性、场景可变性和动作序列长度。</li>
<li><strong>模态丰富度（𝒞_mod）</strong>：基于存在的模态类型（如RGB、深度、语言、力觉、音频）及其对齐质量。<br>通过这两个指标生成一个二维图表，可以高效地表示当前数据集，并识别出数据空白区域。</li>
</ul>
</li>
<li><p><strong>仿真平台评估</strong>：论文评估了仿真平台在生成大规模数据、促进从仿真到现实世界的迁移以及支持任务多样性方面的有效性。</p>
</li>
</ol>
<p>与现有综述相比，本文的创新点在于提出了一个结构化的VLA架构分类法，并首次引入了一个基于任务复杂度和模态丰富度的二维量化框架来系统评估和比较VLA数据集，从而可视化了当前数据格局中未被充分探索的区域。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文是一项系统性综述，其实验与分析部分是基于对现有研究的整理、分类和评估。</p>
<p><strong>使用的数据与材料</strong>：本文全面分析了<strong>102个VLA模型</strong>、<strong>26个基础数据集</strong>和<strong>12个仿真平台</strong>，这些共同构成了VLA模型的开发和评估基础。</p>
<p><strong>关键结果分析</strong>：</p>
<ol>
<li><strong>发展趋势</strong>：图1和图3显示了从2022年到2025年，VLA模型和相关数据集的数量呈现快速增长，尤其是在2025年模型开发急剧加速，同时数据集创建持续增长以支持训练和评估。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.10672v1/extracted/6621942/figures/vlas.png" alt="VLA模型、数据集和贡献机构"></p>
<blockquote>
<p><strong>图1</strong>：2022年至2025年的VLA模型、数据集和贡献机构。上图展示了每年发布的主要VLA模型及其相关机构（红框内的标识），下图显示了用于训练和评估这些模型的关键数据集，按发布年份分组。该图突出了数据集规模和多样性以及机构参与度的增长，包括学术和工业实验室的贡献。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10672v1/extracted/6621942/figures/comparision.png" alt="年度VLA模型与数据集数量"></p>
<blockquote>
<p><strong>图3</strong>：2022年至2025年年度VLA模型和基础VLA数据集数量。绿色条形表示每年新引入的VLA模型数量，紫色条形表示新数据集发布的数量。数据说明了模型开发的快速加速，特别是在2025年，同时支持这些模型训练和评估的数据集创建也稳步增长。</p>
</blockquote>
<ol start="2">
<li><strong>数据集基准评估结果</strong>：图10展示了应用新颖定量基准框架的结果。数据集根据其任务复杂度（𝒞_task）和模态丰富度（𝒞_mod）被绘制在二维空间中。该分析揭示了一个显著的空白：<strong>缺乏同时具备极高任务复杂度和全面多模态性的数据集</strong>。大多数数据集集中在中等复杂度和模态水平。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.10672v1/extracted/6621942/figures/benchmarkdataset.png" alt="数据集基准评估二维图"></p>
<blockquote>
<p><strong>图10</strong>：使用提出的基准框架对VLA数据集进行定量评估的二维图。X轴表示任务复杂度（𝒞_task），Y轴表示模态丰富度（𝒞_mod）。每个数据点的位置和大小反映了其在这两个维度上的得分与规模。该图清晰地显示了当前数据集中在中等区域，而高复杂度-高模态象限存在显著空白。</p>
</blockquote>
<ol start="3">
<li><strong>架构分析</strong>：通过对102个模型的分类，论文总结了当前主流架构趋势，例如端到端Transformer和基于扩散的动作解码器日益流行，以及利用大型语言模型进行高层规划的模块化方法。</li>
</ol>
<p><strong>消融实验/组件贡献分析</strong>：作为一篇综述，本文没有进行传统的消融实验。但其对不同架构类别、数据集特性和仿真平台能力的分析，实质上评估了各个“组件”（即不同的技术路径、数据属性和仿真工具）对整个VLA领域发展的贡献和适用场景。例如，分析指出了模块化架构在利用现有强大VLM/LLM方面的灵活性，以及端到端方法在优化整体策略方面的潜力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了结构化的VLA架构分类法</strong>，系统地将现有模型组织成端到端Transformer、模块化融合、扩散动作模型和基于LLM的规划器等关键类别，清晰勾勒了技术发展脉络。</li>
<li><strong>引入了一个新颖的VLA数据集定量基准框架</strong>，基于任务复杂度和模态丰富度两个维度对数据集进行可视化评估，首次明确揭示了当前数据生态中“高复杂度-高模态”区域的空白。</li>
<li><strong>对仿真平台进行了深入审查</strong>，并综合学术与工业贡献，识别了持续挑战并勾勒了清晰的未来研究路线图。</li>
</ol>
<p>论文自身提及的局限性主要隐含在其分析过程中：首先，所分析的数据集本身在任务复杂度和模态丰富度上存在局限，限制了训练出更通用、更鲁棒的VLA模型；其次，仿真到真实世界的迁移差距仍然是部署的关键障碍；最后，该领域发展极快，新的模型和数据集可能在综述发表后迅速出现。</p>
<p>本文对后续研究的启示非常明确：<strong>未来工作应致力于创建同时具备高任务复杂度和高模态丰富度的大规模数据集</strong>，以填补已识别的数据空白。在架构上，需要探索可扩展的预训练协议、模块化设计以及更鲁棒的多模态对齐策略。同时，推动仿真技术发展，特别是通过可微接触建模实现更动态的仿真，并开发统一的语言接地API，对于实现VLA模型在现实世界中的稳健和可扩展部署至关重要。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文系统综述了视觉语言动作（VLA）模型在机器人操作领域的研究。核心问题是解决传统任务特定编程机器人难以适应动态非结构化环境的局限，旨在通过统一视觉、语言与控制的单一学习框架，实现基于自然语言指令的通用自主操作。论文提炼了基于Transformer架构的整合关键技术，并系统分析了102个VLA模型、26个数据集和12个仿真平台。主要贡献在于提出了基于任务复杂度和多模态对齐的数据集评估新框架，并指出了当前数据格局中的未充分探索区域，为推进通用机器人智能体的发展提供了技术参考与路线图。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.10672" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>