<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.11898" target="_blank" rel="noreferrer">2508.11898</a></span>
        <span>作者: Xiaozhu Ju Team</span>
        <span>日期: 2025-08-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人视觉运动策略主要面临三大挑战：1）基于图像的方法（如Diffusion Policy, ACT）虽然能处理复杂任务，但极易过拟合于训练数据中固定的相机位姿和背景，导致其在分布外（OOD）泛化能力差；2）基于点云的方法（如DP3D, PerAct）依赖高精度深度传感和精确的传感器标定，且缺乏大规模数据集；3）基于大规模预训练-微调范式的方法（如RT-x, Octo）需要巨大的训练资源，但下游微调带来的性能提升有限，且复现其报告的精度具有挑战性。本文针对这些痛点，提出了一种新的视角：借鉴自动驾驶领域的鸟瞰图（BEV）表示，仅使用单目RGB图像构建统一的3D场景表示，以提升策略的泛化能力和数据效率。本文的核心思路是提出Omni-Vision Diffusion Policy (OmniD)，通过一个基于可变形注意力的Omni-Feature Generator (OFG)将多视角图像特征融合为BEV空间中的3D表示，从而选择性地抽象任务相关特征，抑制视角特异性噪声和背景干扰。</p>
<h2 id="方法详解">方法详解</h2>
<p>OmniD的整体框架如图2所示，包含三个阶段：3D特征学习、维度压缩和条件去噪。输入为多视角图像集合 { I i } i = 1 N {I_i}_{i=1}^N ，首先通过一个共享的ResNet-18骨干网络提取2D特征。随后，核心模块Omni-Feature Generator (OFG)将这些2D特征融合，生成一个三维的Omni 3D特征。为了便于后续处理，沿通道维度对3D特征进行池化压缩。压缩后的3D特征被展平，并与机器人状态拼接，共同作为基于扩散模型的动作预测头的条件输入。</p>
<p><img src="https://arxiv.org/html/2508.11898v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：OmniD方法的整体框架。输入的多视角图像经过共享骨干网络提取2D特征，然后通过Omni-Feature Generator (OFG)融合为BEV空间中的3D特征。该特征经压缩后与机器人状态拼接，作为扩散动作预测头的条件，最终输出动作序列。</p>
</blockquote>
<p>**核心模块：Omni-Feature Generator (OFG)**。如图3所示，OFG基于可变形注意力机制，用于在BEV空间中生成3D空间特征。其工作流程分为三步：</p>
<ol>
<li><strong>查询生成与投影</strong>：首先在机器人工作空间内定义一个三维体素网格（范围：X [0, 1.152]米，Y [-0.64, 0.64]米，Z [0, 0.768]米；分辨率：ΔX=0.018米，ΔY=0.08米，ΔZ=0.012米）。每个体素网格位置对应一个d维的空间查询嵌入 q i q_i 和一个参考点 q i ′ q_i&#39; 。利用相机投影矩阵 { P j } j = 1 M {P_j}<em>{j=1}^M ，将每个参考点 q i ′ q_i&#39; 投影到各个相机视图的二维图像平面上，得到投影点 r i j = π j ( q i ′ ) r</em>{ij} = \pi_j(q_i&#39;) 。</li>
<li><strong>偏移预测与采样</strong>：对于每个查询 q i q_i 和每个相机视图 j j ，一个轻量级网络会预测一组K个二维偏移量 { Δ p i j k } k = 1 K {\Delta p_{ijk}}<em>{k=1}^K 。这些偏移量与投影点 r i j r</em>{ij} 相加，生成最终的图像空间采样点 s i j k = r i j + Δ p i j k s_{ijk} = r_{ij} + \Delta p_{ijk} 。这使得模型能够自适应地在图像特征图中围绕投影点进行采样，而非固定位置。</li>
<li><strong>可变形注意力特征聚合</strong>：根据预测的采样点 s i j k s_{ijk} ，通过双线性插值从各视角的2D特征图 F j F_j 中提取特征。同时，模型为每个采样点预测一个注意力权重 w i j k w_{ijk} 。最终，每个查询 q i q_i 对应的融合特征 f i f_i 由所有视角和所有采样点的加权和计算得出： f i = ∑ j = 1 M ∑ k = 1 K w i j k ⋅ Bilinear ( F j , s i j k ) f_i = \sum_{j=1}^{M} \sum_{k=1}^{K} w_{ijk} \cdot \text{Bilinear}(F_j, s_{ijk}) 。这个过程使得模型能够灵活、高效地聚合多视角信息，并聚焦于任务相关的区域。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.11898v1/x3.png" alt="OFG模块"></p>
<blockquote>
<p><strong>图3</strong>：OmniD特征生成器（OFG）的结构。BEV空间中的3D查询点被投影到各个相机视图，并通过预测的偏移量进行自适应采样。利用可变形注意力机制聚合多视图图像特征，生成统一的3D BEV特征表示。</p>
</blockquote>
<p><strong>策略表示与训练</strong>：OmniD使用扩散模型来建模条件动作分布。其训练目标是最小化预测噪声与真实噪声之间的均方误差： L = E [ ∥ ϵ − g θ ( a t , t ) ∥ 2 ] \mathcal{L} = \mathbb{E}[| \epsilon - g_\theta(a_t, t) |^2] ，其中 g θ g_\theta 是参数化的去噪网络， a t a_t 是加噪后的动作， t t 是时间步， ϵ \epsilon 是真实噪声。Omni 3D特征与机器人状态共同作为去噪网络的条件输入。</p>
<p>与现有方法相比，OmniD的创新点在于：1）首次将用于自动驾驶感知的BEV表示和可变形注意力机制引入机器人策略学习，仅用RGB图像构建隐式3D表示，避免了对深度点云的依赖；2）通过可变形注意力机制实现自适应的多视角特征融合，能有效抑制背景干扰，提升对任务相关特征的提取能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了自建的Omni多视角数据集，包含3000条仿真专家轨迹和20,481条真实机器人遥操作轨迹，覆盖6个仿真任务和24个真实任务。仿真实验在VLABench环境中进行，评估了六个代表性任务：<em>Select Apple</em>, <em>Add Condiment</em>, <em>Select Chemistry Tube</em>, <em>Get Coffee</em>, <em>Set Study Table</em>, <em>Texas Holdem</em>。对比的基线方法包括Diffusion Policy (DP)、ACT、VQ-BET和π0 (LoRA)。评估维度包括分布内（ID）性能、分布外（OOD）泛化（位置和背景变化）以及少样本微调能力。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>分布内（ID）性能</strong>：如表1所示，在仿真多视角（BCDE）输入设置下，OmniD在六个任务上平均成功率达到91.0%，优于最佳基线DP（82.0%），尤其在需要长时序或精确3D操作的任务上（如<em>Select Chemistry Tube</em>从70%提升至100%，<em>Get Coffee</em>从88%提升至94%）优势明显。<br><img src="https://arxiv.org/html/2508.11898v1/x1.png" alt="ID性能表"></p>
<blockquote>
<p><strong>表1</strong>：多视角（BCDE）输入下，六个基准任务的分布内（ID）成功率（%）。OmniD在所有任务上均取得了最高性能。</p>
</blockquote>
</li>
<li><p><strong>分布外（OOD）与微调性能</strong>：如表2所示，在位置OOD测试中，OmniD成功率为18%，而最佳基线（π0）仅为2%。在背景OOD测试中（四种不同桌面纹理），OmniD最佳成功率高达90%，显著优于基线。在少样本微调实验中，仅使用10条新视角（A）或新任务的演示轨迹，OmniD在跨视角和跨任务适应上分别达到88%和80%的成功率，而基线方法几乎无法适应（最高2%）。<br><img src="https://arxiv.org/html/2508.11898v1/x2.png" alt="OOD与微调性能表"></p>
<blockquote>
<p><strong>表2</strong>：在<em>Select Apple</em>（任务0）和<em>Select Chemistry Tube</em>（任务1）上的OOD性能与微调成功率（%）对比。展示了OmniD在位置泛化、背景泛化和少样本适应方面的强大能力。</p>
</blockquote>
</li>
<li><p><strong>真实世界实验</strong>：在真实的“拾取南瓜”任务中（背景设置如图4所示），结果如表3。在原始背景下，OmniD（BCDE训练与评估）成功率达84%，远超DP的24%。在两种OOD背景下，OmniD仍能保持一定性能（24%，8%），而DP完全失效（0%）。仅用10条新视角（A）数据微调后，OmniD在原始背景上的成功率提升至60%，DP则为0%。<br><img src="https://arxiv.org/html/2508.11898v1/Framework.jpg" alt="真实世界背景"></p>
<blockquote>
<p><strong>图4</strong>：真实世界实验设置。(a) 五相机物理布置；(b) 用于评估的三种不同背景（原始、OOD-1、OOD-2）。<br><img src="https://arxiv.org/html/2508.11898v1/x3.png" alt="真实世界性能表"><br><strong>表3</strong>：真实世界“拾取南瓜”任务的性能评估。展示了OmniD在不同背景下的泛化能力以及在新视角上微调后的适应性。</p>
</blockquote>
</li>
<li><p><strong>消融实验</strong>：如表4所示，移除OFG模块会使成功率从96.0%降至84.3%，证明了其对空间推理的关键作用。使用单视角（相机A）输入而非多视角（BCDE）输入，成功率从96.0%降至84.0%，凸显了多视角融合的有效性。将骨干网络从ResNet-18替换为ResNet-101仅带来1.3%的微小提升（至97.3%），表明模型性能不严重依赖骨干网络容量。<br><img src="https://arxiv.org/html/2508.11898v1/deforemable.png" alt="消融实验表"></p>
<blockquote>
<p><strong>表4</strong>：消融研究结果。评估了骨干网络选择、OFG模块和多视图输入对任务成功率的影响。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了OmniD框架，首次将基于可变形注意力的BEV表示引入机器人视觉运动策略学习，仅用RGB图像构建了有效的隐式3D特征表示；2）该方法显著提升了策略在位置和背景变化下的OOD泛化能力，并在仿真和真实实验中均大幅超越现有基线；3）展示了出色的少样本适应能力，仅需少量数据即可高效适应新相机视角或新任务，降低了部署成本。</p>
<p>论文自身提到的局限性包括：1）虽然不依赖深度信息，但仍需要近似的相机外参来构建3D特征（对适度标定误差具有鲁棒性）；2）实验主要集中于单任务场景，在多任务、特别是双臂长时序任务上的泛化能力有待进一步测试；3）未深入探索组合分布（CD）场景下的泛化性。</p>
<p>本文的启示在于，借鉴计算机视觉（如自动驾驶）中成熟的3D表示技术（BEV），是提升机器人策略泛化能力的一条有效且数据高效的路径。它避免了点云方法对硬件的苛刻要求和大模型方法对算力的巨大消耗，为在实际多变环境中部署通用的机器人操作策略提供了新的可行方案。后续研究可以探索更松散的相机标定需求、更复杂的多任务设置，以及将BEV表示与其他模态（如语言）进一步结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉运动策略易过拟合训练数据（如固定相机位姿与背景）、在分布外场景泛化能力差，以及多视图信息难以融合的问题，提出OmniD框架。其核心是通过基于图像的多视图融合，构建统一的鸟瞰图（BEV）表征，并采用基于可变形注意力的Omni-Feature Generator（OFG）来选择性提取任务相关特征、抑制视图噪声与背景干扰。实验表明，OmniD在分布内、分布外及少样本任务中，相比最佳基线模型平均性能分别提升11%、17%和84%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.11898" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>