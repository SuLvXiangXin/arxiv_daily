<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.04737" target="_blank" rel="noreferrer">2509.04737</a></span>
        <span>作者: Toshiaki Tsuji Team</span>
        <span>日期: 2025-09-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人模仿学习领域，结合大型语言模型实现基于语言指令的任务规划和粗粒度技能切换已成为主流。然而，这些方法难以在线、细粒度地调制连续的机器人运动参数，如速度、力度和平滑度。人类指令往往是定性的（如“用力擦”、“快速擦”），难以与具体运动建立一对一的映射关系。现有基于解耦表示学习的方法虽能将运动风格与潜在变量关联，但通常采用前馈式、批处理的生成方式，无法在执行过程中根据指令在线调整运动。</p>
<p>本文针对“如何在任务执行过程中，根据人类给出的定性修饰符指令在线、连续地调整机器人运动”这一痛点，提出了一种新的视角：将模仿学习与基于弱监督的解耦表示学习相结合，学习一个可解释、可控制的潜在空间，并通过动作分块技术实现平滑的在线运动生成。核心思路是：将演示数据分割为短序列并赋予弱监督标签，训练一个条件变分自编码器，使其潜在空间的特定维度与修饰符指令解耦关联，从而在推理时通过操控这些潜在变量来在线生成符合指令的运动。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法旨在构建一个在线运动生成模型，输入为当前机器人状态 $\bm{s}_t$ 和代表修饰符指令的潜在变量命令值 $\bm{z}$，输出为动态调整的运动。整体框架分为离线学习和在线推理两部分。</p>
<p><img src="https://arxiv.org/html/2509.04737v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：(A) 离线学习架构概览。(B) 在线推理概览。</p>
</blockquote>
<p><strong>整体流程</strong>：首先，通过双边遥操作收集包含环境动力学信息的人类演示数据 $\bm{\xi}$，记录关节角度、角速度和扭矩作为状态 $\bm{s}_t$。离线学习时，使用滑动窗口将演示序列分割为短动作序列 $\bm{A}<em>t = [\bm{s}<em>t, \bm{s}</em>{t+1}, ..., \bm{s}</em>{t+W-1}]$（$W=50$）。模型以 $\bm{A}_t$ 为输入，学习重建 $\hat{\bm{A}}<em>t$，并同时学习潜在变量 $\bm{z}$ 与修饰符指令的对应关系。在线推理时，输入当前状态 $\bm{s}<em>t$ 和指定的修饰符指令（映射为 $\bm{z}</em>{\text{cmd}}$），解码器生成未来 $W$ 步的动作序列，并通过加权平均计算下一时刻的状态指令 $\hat{\bm{s}}</em>{t+1}$ 用于控制机器人。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>策略学习（CVAE）</strong>：采用条件变分自编码器作为基础生成模型。编码器将动作序列 $\bm{A}_t$ 映射到潜在变量的后验分布 $q_\phi(\bm{z}|\bm{A}_t, \bm{s}<em>t)$，条件变量为序列起始状态 $\bm{s}<em>t$。潜在变量 $\bm{z}$ 从该分布采样，解码器负责重建序列。损失函数包含重建损失 $\mathcal{L}</em>{rec}$（序列内所有状态的均方误差）和KL散度损失 $\mathcal{L}</em>{kl}$（使后验分布接近标准高斯先验）。</li>
<li><strong>基于弱监督标签的解耦表示学习</strong>：这是实现指令控制的关键。假设有 $S$ 类修饰符指令（如力度、速度），为每个短动作序列人工标注弱监督标签 $y_s \in {0.0, 0.5, 1.0}$（例如，力度：弱/中/强对应 0.0/0.5/1.0）。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.04737v1/x3.png" alt="数据收集与标注"></p>
<blockquote>
<p><strong>图3</strong>：(A) 通过双边控制收集数据。(B) 演示者对修饰符指令进行弱监督标注。此例中，按按压力度标注：强($y_s=1.0$)、中($y_s=0.5$)、弱($y_s=0.0$)。</p>
</blockquote>
<p>潜在变量被划分为受约束部分 $\bm{z}<em>s^c$（对应 $S$ 类指令）和未约束部分 $\bm{z}<em>n^u$（$N$ 维）。每个受约束的潜在变量 $z_s^c$ 通过一个MLP映射为预测值 $\hat{y}<em>s$，并与真实弱监督标签 $y_s$ 计算二元交叉熵损失 $\mathcal{L}<em>s$。所有指令的损失求和得到 $\mathcal{L}</em>{modi}$。总训练损失是加权和：$\mathcal{L} = \alpha\mathcal{L}</em>{rec} + \beta\mathcal{L}</em>{kl} + \gamma\mathcal{L}</em>{modi}$。通过最小化该损失，模型学习到一个解耦的潜在表示，其中特定维度编码了特定的修饰符语义。<br>3.  <strong>在线推理与动作分块</strong>：在线生成时，用户指定修饰符指令，系统将其映射为受约束潜在变量 $\bm{z}<em>s^c$ 的值（未约束部分固定为零），构成 $\bm{z}</em>{\text{cmd}}$。解码器生成动作序列 $\hat{\bm{A}}<em>t$。为了在指令变化时生成平滑运动，并处理运动的非马尔可夫性，采用动作分块技术：下一指令状态 $\hat{\bm{s}}</em>{t+1}$ 是当前及过去 $W-1$ 个预测序列的加权平均（公式7）。权重 $w_i = 1/\log(i+1)$ 赋予近期预测更高权重，从而平滑过渡。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的创新在于：1) <strong>在线适应性</strong>：将解耦表示学习与在线生成框架结合，实现了执行过程中根据指令动态调整运动，突破了传统批处理或前馈式生成的限制。2) <strong>弱监督解耦控制</strong>：通过为短动作序列标注简单的有序标签，约束潜在空间，实现了对定性指令的定量、可解释控制。3) <strong>平滑生成机制</strong>：借鉴并验证了动作分块加权平均在指令在线切换场景下对运动稳定性的关键作用。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在“擦拭白板”任务上进行评估（图4）。使用CRANE-X7机器人。定义了两种修饰符指令：<strong>物理指令</strong>（擦拭力度：弱、中、强）和<strong>时间指令</strong>（擦拭速度：慢、中、快）。潜在空间维度为3，其中2维受约束分别对应两种指令，1维未约束。</p>
<p><img src="https://arxiv.org/html/2509.04737v1/x4.png" alt="擦拭任务"></p>
<blockquote>
<p><strong>图4</strong>：擦拭任务：机器人抓住白板擦，用全身和关节擦拭白板。</p>
</blockquote>
<p><strong>对比方法</strong>：与两种基线架构对比：CVAE-LSTM 和 ACT（Action Chunking with Transformers）。并分别将它们与本文提出的解耦约束扩展结合，得到 CVAE-LSTM (Proposed) 和 ACT (Proposed)。</p>
<p><strong>评估指标</strong>：1) <strong>任务成功率</strong>：是否成功完成三次擦拭循环。2) <strong>修饰符指令误差</strong>：衡量潜在变量变化与预期指令效果的一致性，值越低表示该潜在维度对相应指令的表征越清晰。</p>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>指令跟随能力</strong>：如表2所示，原始CVAE-LSTM的所有潜在维度MDE均高于1.0，表明其潜在表示是纠缠的，无法清晰关联特定指令。而CVAE-LSTM (Proposed) 在受约束维度上MDE显著降低（时间指令维度低至0.22），证明了解耦约束的有效性。ACT (Proposed) 也显示出改进，但其物理指令维度的MDE仍较高（1.95），表明解耦尚不完全。</li>
</ul>
<blockquote>
<p><strong>表2</strong>：擦拭任务的成功率和修饰符指令一致性指数。加粗数值表示MDE低于0.50。</p>
</blockquote>
<ul>
<li><strong>动作分块的作用</strong>：如表3所示，不使用动作分块加权平均时，两种改进模型的任务成功率均为0%，运动出现严重振荡。采用不同的加权函数（如 $w_i=1/\log(i+1)$）后，成功率提升至100%。这验证了动作分块对于平滑在线指令切换、保证运动稳定性的必要性。</li>
</ul>
<blockquote>
<p><strong>表3</strong>：动作分块中权重参数与任务成功率的关系。</p>
</blockquote>
<p><strong>消融实验总结</strong>：实验主要对动作分块的加权方式进行了消融。结果表明，加权平均是保证在线运动稳定性的关键组件，而权重的具体形式（$1/\log(i+1)$ 或 $\exp(-m*i)$）可能因模型架构（LSTM或Transformer）而异，需要调整。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一种能够根据定性修饰符指令在线调整机器人运动的生成模型，通过弱监督解耦表示学习建立了指令与潜在变量的关联。2) 证明了结合动作分块机制对于在线指令切换下生成平滑、稳定轨迹的有效性。</p>
<p><strong>局限性</strong>：论文自身提到，学习到的潜在表示可能仍存在一定纠缠（如ACT改进版中物理指令维度的MDE较高），并非完全解耦。此外，动作分块需要特定的加权函数来实现最佳平滑效果。</p>
<p><strong>后续启示</strong>：本研究为机器人实现细粒度、在线的人机交互运动控制提供了可行路径。后续工作可探索：1) 更强大的解耦约束或架构，以提升潜在变量与指令语义的对齐纯度。2) 将方法扩展到更多样、更复杂的修饰符指令集。3) 研究如何自动学习或自适应调整动作分块的加权策略。4) 结合更强大的基础模型进行高层指令解析，与本文的底层运动调制模型形成互补。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习中，难以根据人类定性指令（如“用力擦”）在线调整连续运动参数的问题，提出一种基于解耦表征学习的运动生成模型。该方法将演示数据分割为短序列，并为特定修饰符类型分配弱监督标签，从而学习从修饰符指令到动作的映射。在擦拭和抓放任务上的实验表明，该方法能够在线响应指令调整动作，而传统的批量处理方法无法在执行过程中适应。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.04737" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>