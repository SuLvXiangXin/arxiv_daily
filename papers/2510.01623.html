<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA-R1: Enhancing Reasoning in Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>VLA-R1: Enhancing Reasoning in Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.01623" target="_blank" rel="noreferrer">2510.01623</a></span>
        <span>作者: Ye, Angen, Zhang, Zeyu, Wang, Boyuan, Wang, Xiaofeng, Zhang, Dapeng, Zhu, Zheng</span>
        <span>日期: 2025/10/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过统一感知、语言理解和动作生成，在具身AI领域展现出强大的跨任务和跨场景泛化能力。然而，现有VLA模型存在两大关键局限性：首先，它们通常缺乏显式的逐步推理过程，倾向于直接输出最终动作，而没有充分考虑功能可供性约束、几何关系或容器选择，这导致在颜色相似、重复实例或多个候选容器等复杂场景下出现指令消歧失败。其次，现有的后训练流程很少系统性地强化推理质量，主要依赖监督微调（SFT），奖励设计薄弱，即使使用强化学习（RL），其奖励设计也通常是单目标的，难以联合优化区域对齐和轨迹一致性，从而在分布外数据和真实世界中的性能下降。</p>
<p>本文针对VLA模型推理能力不足这一具体痛点，提出了一种新的视角：同时强调数据层面的思维链（CoT）监督和优化层面的奖励对齐，以弥合推理与执行之间的鸿沟。其核心思路是：通过构建高质量的思维链数据集（VLA-CoT-13K）提供显式推理监督，并设计一种基于可验证奖励的强化学习（RLVR）后训练策略，利用组相对策略优化（GRPO）和三种可验证奖励来系统性地优化模型的推理与执行。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLA-R1的整体架构是一个两阶段训练流程。给定输入图像和自然语言指令，模型首先通过视觉-语言主干编码多模态信息，然后通过动作解码器产生低级控制信号。视觉分支通过视觉编码器处理原始图像，语言分支对任务指令进行分词和嵌入。两种模态在多模态解码器中融合，共同对视觉线索、文本上下文和时间历史进行推理，生成一个结构化的输出，该输出包含一个推理片段和一个动作预测。推理痕迹使中间步骤显式化，而动作输出则在离散的标记空间中表示。最后，动作解标记器将预测的标记映射为连续的7维机器人动作（Δx, Δθ, ΔGrip），可直接在机械臂上执行。</p>
<p><img src="https://arxiv.org/html/2510.01623v1/figs/arch5.png" alt="方法整体架构"></p>
<blockquote>
<p><strong>图3</strong>：VLA-R1的整体架构。训练分为两个阶段：第一阶段使用带有CoT监督的SFT来学习对图像和指令的推理；第二阶段通过带有可验证奖励（GRPO）的RL来细化推理和动作。在推理时，控制栈将输出转换为关节级的机器人命令。</p>
</blockquote>
<p>核心模块包括数据合成、监督微调和强化学习后训练。</p>
<ol>
<li><p><strong>数据合成与VLA-CoT-13K数据集</strong>：为了解决现有数据集缺乏详细推理过程的问题，本文构建了VLA-CoT数据引擎，使用Qwen2.5-VL-72B自动为可供性和轨迹任务生成中间推理步骤，最终产生了13K个高质量的CoT标注。这些数据将CoT与可供性和轨迹标注对齐，为模型提供了显式的逐步指导。<br><img src="https://arxiv.org/html/2510.01623v1/x1.png" alt="CoT数据引擎"></p>
<blockquote>
<p><strong>图2</strong>：CoT数据引擎。在摄入多模态数据后，系统根据任务类型（例如，可供性或轨迹）解析任务，执行场景理解和定位，验证可行性，并为训练生成结构化的CoT痕迹。</p>
</blockquote>
</li>
<li><p><strong>监督微调（SFT）</strong>：在VLA-CoT-13K数据集上进行监督微调。与简单的问答指令微调相比，思维链提供了中间监督信号，鼓励显式分解、更强的视觉基础和跨时间的稳定信用分配。这产生了“先推理后行动”的策略，为后续在可验证奖励下的后训练做好了准备。模型使用Qwen2.5-VL-3B进行初始化。</p>
</li>
<li><p><strong>强化学习后训练（RLVR with GRPO）</strong>：在SFT之后，采用组相对策略优化（GRPO）算法对模型进行进一步优化。GRPO对输入q从旧策略中采样一组输出{o1, …, on}，用奖励函数对每个进行评分得到rg，并通过组内均值r̄和标准差σr进行归一化得到优势估计Âg。目标函数结合了裁剪后的策略比率与优势估计，并包含一个KL散度惩罚项以防止策略过度漂移。</p>
</li>
</ol>
<p>创新的奖励设计是该方法的核心，包含三种可验证奖励：</p>
<ul>
<li><strong>基于GIoU的可供性奖励</strong>：使用广义交并比（GIoU）来衡量预测边界框与真实边界框之间的空间对齐。GIoU不仅考虑重叠区域，还考虑了最小外接矩形，即使在边界框不重叠时也能惩罚未对齐的预测，提高了在杂乱环境中的空间鲁棒性。</li>
<li><strong>基于改进Fréchet距离的轨迹奖励</strong>：使用角度-长度增强Fréchet距离（ALAF）来衡量预测轨迹与真实轨迹的一致性。ALAF结合了位置离散Fréchet项、单位切线之间的角度惩罚以及基于相邻段长度对数比的比例惩罚，尊重曲线的时间顺序并考虑了局部几何形状。</li>
<li><strong>格式奖励</strong>：一个二元奖励，强制模型输出遵循所需的结构（<code>&lt;reasoning&gt;...&lt;/reasoning&gt;</code>推理段后跟<code>&lt;output&gt;...&lt;/output&gt;</code>动作段），以鼓励可解释的推理痕迹并防止后训练期间产生退化的输出。</li>
</ul>
<p>与现有方法相比，VLA-R1的创新点具体体现在：1）<strong>数据层面</strong>：构建了首个将显式思维链与可供性、轨迹标注对齐的高质量数据集VLA-CoT-13K；2）<strong>优化层面</strong>：首次将RLVR与GRPO结合应用于VLA模型的后训练，并设计了针对区域对齐、轨迹一致性和输出格式的多目标可验证奖励系统，从而系统性地强化了推理的鲁棒性和执行的准确性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在四个设置下进行：域内数据集、域外数据集、仿真环境和真实机器人平台。</p>
<p><strong>数据集与基准</strong>：域内训练使用ShareRobot数据集（包含6522张带可供性标注和6870张带轨迹标注的图像）。域外评估中，可供性感知使用UMD部分可供性数据集的子集（1200个样本），轨迹预测使用VAIT数据集（来自Open X-Embodiment的500个样本）。评估指标：可供性任务使用交并比（IoU）；轨迹任务使用离散Fréchet距离（DFD）、Hausdorff距离（HD）和均方根误差（RMSE），并计算平均误差（Avg）。仿真和真实实验报告成功率（SR）。</p>
<p><strong>对比方法</strong>：包括开源多模态指令跟随模型（Phi-4, Gemma-3, Qwen2.5-VL）、监督微调基线（InternVL2, LLaVA-1.6, RoboBrain, NORA）以及RL后训练模型（ManipLVM-R1）。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><p><strong>域内性能</strong>：VLA-R1-3B在所有指标上取得最佳结果：IoU = 36.51，DFD = 106.2，HD = 97.9，RMSE = 71.12，平均误差91.74。相较于最强的基线ManipLVM-R1-3B，IoU提升了17.78%，整体轨迹误差降低了17.25%。</p>
</li>
<li><p><strong>域外性能</strong>：VLA-R1-3B在存在显著分布偏移的情况下，轨迹预测性能依然最优：IoU达到33.96，DFD、HD、RMSE分别降至114.3、98.43和68.97，展示了强大的跨域泛化能力。<br><img src="https://arxiv.org/html/2510.01623v1/x2.png" alt="性能对比表"></p>
<blockquote>
<p><strong>表III</strong>：域内和域外性能对比。VLA-R1在各项指标上均优于所有基线模型。</p>
</blockquote>
</li>
<li><p><strong>仿真实验</strong>：在RoboTwin仿真器中，VLA-R1在Piper和UR5两种机器人平台上进行评估。平均而言，可供性感知成功率为55%，轨迹执行成功率为70%，显著优于NORA基线（可供性40%，轨迹5%），证明了其跨机器人平台的稳定性。<br><img src="https://arxiv.org/html/2510.01623v1/figs/sim.png" alt="仿真可视化"></p>
<blockquote>
<p><strong>图6</strong>：仿真环境可视化。展示了随机化的桌面杂乱场景。</p>
</blockquote>
</li>
<li><p><strong>真实世界实验</strong>：在四个真实场景（碗拾取、水果拾取、厨房场景、混合场景）中，VLA-R1的平均可供性感知成功率为62.5%，轨迹预测成功率为75%，而NORA-3B基线分别为35%和47.5%。<br><img src="https://arxiv.org/html/2510.01623v1/figs/scene_test.png" alt="真实世界评估可视化"></p>
<blockquote>
<p><strong>图5</strong>：真实世界场景评估的可视化。展示了VLA-R1在不同复杂场景下的执行情况。</p>
</blockquote>
</li>
</ul>
<p><strong>消融实验</strong>：研究验证了CoT和RL各自的作用。仅使用CoT（无RL）可将IoU从23.74提升至28.37，并将平均轨迹误差从133.23降低至124.67。结合CoT和RL（完整VLA-R1）则实现了最佳性能（IoU 36.51，平均误差91.74），证明了二者缺一不可。<br><img src="https://arxiv.org/html/2510.01623v1/figs/cot_test.png" alt="案例分析与推理过程"></p>
<blockquote>
<p><strong>图4</strong>：案例分析。展示了VLA-R1在可供性和轨迹任务上的推理过程和结果。模型解析动作要求，推断相关对象和空间关系，并输出相应的边界框或路径点序列。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了<strong>VLA-R1</strong>，一种通过RLVR优化方案（包含精心设计的区域对齐、轨迹一致性和输出格式奖励）并结合GRPO，来系统性增强推理和执行鲁棒性的VLA基础模型；2）引入了<strong>VLA-CoT数据引擎</strong>，生成了与可供性和轨迹标签对齐的高质量VLA-CoT-13K数据集，显式弥补了现有VLA模型缺乏逐步推理的缺陷；3）在域内、域外、仿真和真实机器人平台上对VLA-R1进行了全面评估，实证验证了其有效性和跨域泛化能力。</p>
<p>论文自身提到的局限性包括：在真实世界实验中，颜色相似性和位置变化等干扰因素仍然会影响模型的决策，是主要的误差来源。此外，尽管在仿真中测试了跨机器人泛化，但训练数据完全来自真实世界设置，仿真环境表现出更大的变异性。</p>
<p>本文对后续研究的启示在于：1）<strong>推理的显式化与可验证性</strong>：将逐步推理过程结构化并与可量化的奖励（如GIoU、ALAF）相结合，是提升VLA模型决策透明度和可靠性的有效途径。2）<strong>后训练策略的创新</strong>：将RLVR与先进的策略优化算法（如GRPO）结合，并设计多目标、可验证的奖励函数，可以系统性地提升模型在复杂任务上的性能，减少对大规模人工标注的依赖。3）<strong>高质量数据集的构建</strong>：针对特定缺陷（如缺乏推理步骤）构建小而精的数据集，能够有效引导模型学习关键能力，为后续优化奠定基础。未来的工作可以探索更复杂的推理形式、更丰富的奖励信号，以及在不同机器人形态和更开放环境中的进一步泛化。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前视觉-语言-动作模型缺乏显式逐步推理、且训练后流程难以系统性强化推理质量的问题，提出了VLA-R1模型。其关键技术包括：采用基于可验证奖励的强化学习与组相对策略优化，以联合优化区域对齐、轨迹一致性和输出格式；并构建了VLA-CoT-13K高质量数据集提供思维链监督。实验表明，VLA-R1在领域内、领域外、仿真和真实机器人平台上均实现了优越的泛化性能和真实世界执行效果。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.01623" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>