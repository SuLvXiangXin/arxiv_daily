<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.04447" target="_blank" rel="noreferrer">2507.04447</a></span>
        <span>作者: Xin Jin Team</span>
        <span>日期: 2025-07-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将未来知识预测融入视觉-语言-行动（VLA）模型的主流方法主要分为两类：一是使用独立的图像/视频生成模型（copilot）来生成未来帧或轨迹，再将其输入动作预测头；二是将像素级图像预测与动作预测集成在单一框架中，将预测作为类似于大语言模型（LLM）的中间推理步骤。然而，这些方法存在关键局限性：1）<strong>冗余的像素信息</strong>：预测的未来图像与当前观测存在大量重叠，效率低下；2）<strong>缺乏空间信息</strong>：缺少对环境的显式3D知识；3）<strong>缺乏高层知识预测</strong>：缺失对未来状态的高层理解，如语义信息。</p>
<p>本文针对现有VLA模型在预测未来状态时信息冗余且不全面的痛点，提出了一个新视角：将VLA重新定义为<strong>感知-预测-行动</strong>模型，让模型显式预测一组紧凑的、与机器人执行高度相关的综合世界知识，而非整个未来帧。本文的核心思路是：通过预测动态区域、深度和语义特征这三种互补的未来知识，为动作规划提供简洁而全面的前瞻性线索，从而建立更有效的闭环。</p>
<h2 id="方法详解">方法详解</h2>
<p>DreamVLA的整体框架是一个端到端的感知-预测-行动流水线。输入包括自然语言指令 <code>l</code>、当前视觉观测 <code>o_t</code> 和本体感知状态 <code>s_t</code>。模型首先通过特定模态的编码器（CLIP文本编码器、MAE视觉编码器、多层感知机状态编码器）进行处理，并附加一组可学习的 <code>&lt;dream&gt;</code> 和 <code>&lt;action&gt;</code> 查询令牌。这些令牌与编码后的输入序列一同输入一个基于GPT-2的大型语言模型（LLM），该模型通过精心设计的注意力机制进行跨模态融合，输出<strong>世界嵌入</strong>和<strong>潜在动作嵌入</strong>。随后，三个轻量级解码器将世界嵌入中对应的元素分别投影为预测的动态区域 <code>f_{t+n}</code>、单目深度 <code>d_{t+n}</code> 和高级语义特征 <code>c_{t+n}</code>。同时，一个独立的去噪扩散变换器（DiT）以潜在动作嵌入为条件，将高斯噪声细化为一个 <code>n</code> 步的动作序列 <code>a_{t:t+n-1}</code>。<strong>注意：</strong> 世界知识预测的解码器仅在训练时使用；推理时直接使用世界嵌入，跳过解码步骤以节省计算。</p>
<p><img src="https://arxiv.org/html/2507.04447v3/x2.png" alt="框架总览"></p>
<blockquote>
<p><strong>图2</strong>：DreamVLA框架总览。给定当前状态、观测和语言指令，模型通过冻结的编码器和可调状态编码器进行编码。这些令牌与可学习的 <code>&lt;dream&gt;</code> 查询集一同输入LLM，生成世界嵌入。三个轻量级解码器分别将该嵌入的对应元素投影为动态区域、深度和语义知识。一个独立的 <code>&lt;action&gt;</code> 查询提取潜在动作嵌入，用于条件化一个扩散变换器，后者将高斯噪声细化为n步动作序列。虚线框内的预测头仅在训练时使用。</p>
</blockquote>
<p><strong>核心模块一：综合世界知识预测</strong><br>该模块旨在预测对未来操作最相关的三种知识，作为原始像素的紧凑、结构化替代。</p>
<ol>
<li><strong>动态区域预测</strong>：引导模型关注场景中即将移动的部分（如机械臂末端或可移动物体）。利用CoTracker提取动态区域作为训练目标，模型仅学习重建这些区域。损失函数基于离散变分自编码器（dVAE）的ELBO构建，如公式(5)所示。</li>
<li><strong>深度预测</strong>：预测深度图的演变，为机器人提供空间上下文，指导其朝向自由空间移动。使用Depth-Anything的预测作为自监督教师信号，采用尺度归一化的均方误差损失（公式6,7）进行训练。</li>
<li><strong>语义预测</strong>：预测未来的高层语义特征（如DINOv2和SAM特征），使机器人理解哪些物体或区域对任务重要。使用InfoNCE对比损失（公式8）进行训练，鼓励模型区分正确的未来语义与空间偏移的负样本。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04447v3/x3.png" alt="动态区域可视化"></p>
<blockquote>
<p><strong>图3</strong>：随时间变化的动态区域可视化。展示了静态相机和腕部相机观测，以及我们方法在多个时间步生成的相应动态掩码。掩码通过CoTracker提取的光流轨迹突出显示动态区域，有效抑制了无关背景，聚焦于交互相关区域。</p>
</blockquote>
<p><strong>核心模块二：块状结构化注意力机制</strong><br>为防止动态、深度和语义这三类知识在训练过程中相互干扰和信息泄漏，<code>&lt;dream&gt;</code> 查询被分解为三个子查询。模型采用了一种<strong>块状结构化注意力机制</strong>，掩蔽了这三个子查询之间的相互注意力。每个子查询只关注共享的视觉、语言和状态令牌，而它们彼此之间的直接连接被禁用，从而保持它们的潜在特征解耦且无串扰。<code>&lt;dream&gt;</code> 和 <code>&lt;action&gt;</code> 查询还采用仅限于过去上下文的因果注意力，以保持时间因果性。</p>
<p><img src="https://arxiv.org/html/2507.04447v3/x4.png" alt="块状结构化注意力"></p>
<blockquote>
<p><strong>图4</strong>：块状结构化注意力示意图。动态、深度和语义子查询之间的相互注意力被掩蔽，每个子查询仅与共享的模态令牌交互，以保持知识边界的清晰和解耦。</p>
</blockquote>
<p><strong>核心模块三：基于扩散变换器的逆动力学建模</strong><br>由于世界嵌入和动作嵌入共享相同的潜在空间且统计特征相似，简单的MLP头难以解耦模态特定信息或利用跨模态相关性。因此，DreamVLA采用一个<strong>去噪扩散变换器</strong>作为动作头。该DiT以从LLM获得的潜在动作嵌入为条件，通过迭代的自注意力和去噪步骤，将高斯噪声转化为一个n步的动作轨迹（公式9）。这种方法能够融合感知预测与控制先验，生成连贯、多样且物理上合理的动作序列。</p>
<p><strong>创新点总结</strong></p>
<ol>
<li><strong>预测内容创新</strong>：从预测冗余的整帧像素，转向预测紧凑、互补的动态、空间和语义世界知识。</li>
<li><strong>训练机制创新</strong>：引入块状结构化注意力，有效防止不同类型未来知识间的干扰，确保表征纯净。</li>
<li><strong>动作生成创新</strong>：采用扩散变换器进行逆动力学建模，更好地处理多模态动作分布并利用跨模态条件。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真环境中，使用<strong>CALVIN</strong>（长视野语言条件操作）和<strong>LIBERO</strong>（空间/物体/目标/长任务套件）基准进行评估。在真实世界中，使用Franka Panda机械臂和RealSense D415相机进行抓取和放置等任务验证。预训练数据包括CALVIN的语言无关部分、DROID大规模轨迹数据集以及LIBERO-90。</p>
<p><strong>对比方法</strong>：与多种VLA基线对比，包括：直接映射型（Roboflamingo, OpenVLA, GR-1, Robovlm等）、使用copilot模型型（Susie, CLOVER）、集成像素预测型（UP-VLA, Seer, VPP）以及其他先进方法（3D Diffusor Actor, RoboDual, UNIVLA, Pi0等）。</p>
<p><strong>关键实验结果</strong>：<br>在CALVIN ABC-D基准上，评估模型连续完成1到5个任务的性能。DreamVLA在平均完成长度（Avg. Len.）上达到 <strong>4.44</strong>，超越了所有对比方法。</p>
<p><img src="https://arxiv.org/html/2507.04447v3/x6.png" alt="CALVIN结果表"></p>
<blockquote>
<p><strong>图6（对应论文表1）</strong>：CALVIN ABC-D基准结果。DreamVLA在连续完成1至5个任务的成功率及平均长度上均达到最佳（4.44），展示了其在长视野多任务学习和泛化上的优势。</p>
</blockquote>
<p>在LIBERO基准的四个套件上，DreamVLA也取得了最佳或具有竞争力的性能，平均得分高达97.5%。</p>
<p><img src="https://arxiv.org/html/2507.04447v3/x7.png" alt="LIBERO结果表"></p>
<blockquote>
<p><strong>图7（对应论文表2）</strong>：扩展的LIBERO实验结果。DreamVLA在所有任务轨道（空间、物体、目标、长任务）上均达到最佳或极具竞争力的性能，证明了综合世界知识预测的有效性。</p>
</blockquote>
<p>真实世界实验表明，经过DROID预训练和微调后，DreamVLA在抓取和放置等任务上取得了 <strong>76.7%</strong> 的成功率。</p>
<p><strong>消融实验分析</strong>：<br>综合消融研究表明，预测动态区域带来的性能提升最大，而深度和语义线索提供的收益较小且大致相等。更重要的是，当<strong>单独使用深度或语义预测时，不仅没有帮助，反而会损害性能</strong>。这验证了块状结构化注意力机制的必要性——它防止了单一类型知识在缺乏其他上下文时产生的误导或噪声，并凸显了综合、互补的世界知识预测的整体价值。</p>
<p><img src="https://arxiv.org/html/2507.04447v3/x5.png" alt="真实世界设置"></p>
<blockquote>
<p><strong>图5</strong>：真实世界实验设置。使用Franka Panda机械臂和两个RealSense D415相机（第三人称视角和腕部视角）进行抓取等任务评估。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>重新定义了VLA模型范式，提出了感知-预测-行动框架，通过显式预测动态、空间和语义综合世界知识，为规划提供紧凑而全面的前瞻线索。</li>
<li>引入了块状结构化注意力机制，有效抑制了不同类型未来知识间的交叉泄漏，保持了表征的纯净和解耦。</li>
<li>在CALVIN和LIBERO基准上取得了最先进的性能（CALVIN平均长度4.44），并将真实世界任务成功率提升至76.7%，消融实验证实了各组件贡献。</li>
</ol>
<p><strong>局限性</strong>：论文指出，尽管推理时跳过了世界知识解码器，但模型仍然依赖于视觉编码器来处理输入图像，这可能成为计算瓶颈。未来工作可探索更高效的感知模块。</p>
<p><strong>启示</strong>：</p>
<ol>
<li><strong>预测质量优于预测数量</strong>：在机器人推理中，预测与任务高度相关的、紧凑的中间表征（如动态区域），比预测冗余的原始传感器数据（如图像）更有效。</li>
<li><strong>知识解耦的重要性</strong>：在训练多任务、多模态预测模型时，需要设计机制（如结构化注意力）来防止不同预测目标间的有害干扰，确保学习到干净、有用的表征。</li>
<li><strong>闭环推理</strong>：将未来状态预测作为动作生成的明确条件，建立感知-预测-行动闭环，更贴近人类“先思考后行动”的推理过程，能显著提升模型的规划和泛化能力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>DreamVLA旨在解决现有视觉-语言-动作模型在机器人操作中依赖图像预测导致的冗余信息及缺乏全面世界知识（动态、空间、语义信息）的问题。关键技术包括：动态区域引导的世界知识预测，集成空间与语义线索以提供紧凑表示；块状结构化注意力机制，屏蔽不同信息间相互注意力以防止泄漏；基于扩散的变换器，解耦动作表示以建模未来动作分布。实验表明，该模型在真实机器人任务上达到76.7%成功率，在CALVIN ABC-D基准上平均长度为4.44。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.04447" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>