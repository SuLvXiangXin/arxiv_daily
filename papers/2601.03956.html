<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.03956" target="_blank" rel="noreferrer">2601.03956</a></span>
        <span>作者: Zhou, Kangjie, Wen, Zhejia, Zhuo, Zhiyong, Yan, Zike, Wu, Pengying, U, Ieng Hou, Li, Shuaiyang, Gao, Han, Ding, Kang, Cao, Wenhan, Pan, Wei, Liu, Chang</span>
        <span>日期: 2026/01/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人交互式导航的主流方法主要分为两类。一类是传统的“可移动障碍物导航”方法，它们依赖全局地图和精确物体位姿，将问题简化为高维构型空间搜索，但在部分可观测的真实场景中应用受限，且通常将交互简化为对简单几何体的轴对齐推动。另一类是基于视觉语言模型的导航方法，它们利用预训练的视觉-语言表示进行语义目标搜索和指令跟随，但普遍假设被动、无碰撞的导航范式，缺乏主动与环境交互的能力。现有VLM本质上作为语义推理器，缺乏对特定机器人物理能力的理解，也缺乏因果推理能力来决策“何时交互”以及“与哪个物体交互”，这导致在路径被杂物阻塞的场景中，机器人要么采取低效绕行，要么完全无法到达目标。</p>
<p>本文针对VLM缺乏物理技能感知和交互推理能力这两个具体痛点，提出了一种新视角：将参数化的技能可供性与约束显式注入VLM的推理上下文中，并通过反事实推理逻辑来内化交互决策的学习。本文的核心思路是提出一个名为CoINS的分层框架，通过技能感知的VLM进行高层反事实推理以生成可执行的交互决策，并结合基于强化学习的技能库进行鲁棒的低层物理执行。</p>
<h2 id="方法详解">方法详解</h2>
<p>CoINS框架采用分层设计，包含高层推理策略π_H（InterNav-VLM）和低层执行策略π_L（技能库）。高层策略输入导航目标X_goal、当前RGB观测o_t^rgb、技能集𝒮及机器人能力𝒞，输出最优技能s_t ∈ 𝒮及执行参数q_t（如目标物体）。低层策略输入高层决策(s_t, q_t)及当前多模态观测(o_t^rgb, o_t^geo)，输出关节控制命令u_t。整个过程仅依赖机载传感器，无需全局地图。</p>
<p><img src="https://arxiv.org/html/2601.03956v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：CoINS框架总览。VLM推理模块以机器人的第一视角RGB观测和本体约束为输入，产生高层的交互与导航决策；技能执行模块将这些决策转化为针对不同交互基元的精确运动控制。</p>
</blockquote>
<p><strong>InterNav-VLM</strong> 是该框架的核心推理模块，其训练流程包含三个关键部分：</p>
<ol>
<li><strong>技能感知的环境建模</strong>：首先，利用VGGT和Map-Anything从RGB图像估计度量深度和相机位姿，重建度量尺度的3D点云并归一化。然后，基于点云生成2D网格地图，根据机器人越障高度h_max计算原始占据图M_occ，再根据机器人净空半径r_clear膨胀障碍物得到最终的通行性地图M_trav。对于操作技能，使用Grounding DINO检测技能指定的物体类别，并利用重建的点云获取其3D位置，再判断该位置是否在机器人可达工作空间内，从而筛选出可操作物体集合𝒦_manip。</li>
<li><strong>基于反事实推理的交互学习</strong>：这是决定“何时交互、与谁交互”的关键。对于每个导航目标x_g，在通行性地图M_trav上使用A<em>搜索路径。对于每个可操作物体o ∈ 𝒦_manip，计算其“反事实增益”G(o)。增益定义为移除该物体后（对应地图M_trav^{-o}）路径长度减少的比例：G(o) = 1 - l(M_trav^{-o}, x_g) / l(M_trav, x_g)。通过离线模拟，选择能最大化增益G(o)的物体o</em>作为交互目标，若其增益超过阈值ε，则判定需要交互。</li>
<li><strong>带自省验证的VQA生成与微调</strong>：基于上述路径规划和反事实推理结果，生成视觉问答数据用于训练。答案若可直接导航则为导航指令，若需交互则指定目标物体和技能。关键创新在于引入了<strong>自省验证机制</strong>，在思维链中明确包含两个阶段：技能可行性检查（确认动作符合机器人能力约束）和交互必要性推理（阐明所选物体阻碍路径，移除有益于导航）。使用约20K个此类VQA样本对Qwen3-VL主干模型进行监督微调，标准下一个词预测损失，使模型内化反事实推理逻辑。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.03956v1/x3.png" alt="VQA生成"></p>
<blockquote>
<p><strong>图3</strong>：VQA样本生成流程示意图，展示了从环境建模、反事实推理到生成问答对的过程。</p>
</blockquote>
<p><strong>RL技能库</strong> 用于执行高层决策。采用配备机械臂的四足机器人为本体，构建分层控制框架。</p>
<ol>
<li><strong>低层全身控制器</strong>：使用PPO训练一个鲁棒的低层策略，输入高层命令（期望基座速度v_base和目标末端执行器位姿P_ee）、本体感知和高度扫描信息，输出所有18个自由度（12条腿+6轴机械臂）的目标关节位置，以跟踪命令并维持稳定性。</li>
<li><strong>高层技能库</strong>：在低层控制器之上，开发导航与多种物体交互的高层技能。导航技能基于在线构建的局部地图使用A*和纯追踪控制器生成基座速度。交互技能（如推箱子、开门、爬越）则通过RL单独训练，其策略观察环境状态（如物体相对位置），并输出给低层控制器的命令序列(v_base, P_ee)。特别引入了<strong>面向通行性的操作范式</strong>，其奖励函数鼓励清除路径而非精确摆放物体，更符合导航目标。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.03956v1/x4.png" alt="技能库框架"></p>
<blockquote>
<p><strong>图4</strong>：基于RL的分层技能库框架，包含低层全身控制器和多个高层技能策略。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在Isaac Sim仿真器中构建了名为InterNav的大规模数据集进行评估，包含多样化的室内场景和物理逼真的交互资产。实验平台为Unitree Go1四足机器人加装6-DoF机械臂。</p>
<p><strong>对比基线</strong>：</p>
<ul>
<li><strong>传统方法</strong>：Search-based NAMO (S-NAMO), Visibility-aware NAMO (V-NAMO)。</li>
<li><strong>VLM方法</strong>：CLIP-Nav, ViNT, 以及一个消融版本“InterNav-VLM w/o skill-aware”（不注入技能约束）。</li>
<li><strong>RL方法</strong>：训练一个端到端的RL策略作为基准。</li>
</ul>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>整体性能</strong>：CoINS在整体成功率上显著优于所有基线，达到**87%<strong>，比性能最好的基线（V-NAMO, 70%）高出</strong>17%**。在路径长度上，CoINS也最短。</li>
<li><strong>长视野复杂场景</strong>：在需要多次交互的复杂长视野任务中，CoINS的成功率超过**80%<strong>，而其他所有基线方法的成功率均低于</strong>45%**，体现了超过80%的显著提升。</li>
<li><strong>泛化能力</strong>：在训练未见的新物体类别上进行测试，CoINS保持了较高的成功率，而端到端RL基线的性能急剧下降，证明了其优秀的泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.03956v1/x5.png" alt="主要结果对比"></p>
<blockquote>
<p><strong>图5</strong>：CoINS与基线方法在整体成功率、路径长度和长视野任务成功率上的定量对比。CoINS全面领先。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.03956v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融实验展示了各模块的贡献。完整的CoINS性能最佳，移除技能感知或反事实推理任一组件均导致性能显著下降。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.03956v1/x7.png" alt="泛化测试"></p>
<blockquote>
<p><strong>图7</strong>：在新物体类别上的泛化测试结果。CoINS（橙色）相比端到端RL基线（蓝色）表现出强大的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.03956v1/x8.png" alt="定性结果1"><br><img src="https://arxiv.org/html/2601.03956v1/x9.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图8与图9</strong>：定性实验展示了CoINS在仿真中成功处理需要推箱子、开门、爬越等多种交互的复杂场景。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.03956v1/x10.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图10</strong>：真实世界实验验证。CoINS成功引导真实机器人完成推箱清障、开门、爬越纸箱等任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.03956v1/x11.png" alt="失败案例分析"></p>
<blockquote>
<p><strong>图11</strong>：失败案例分析。典型失败原因包括VLM对微小物体的漏检、动态物体干扰以及技能执行误差。</p>
</blockquote>
<p><strong>消融实验总结</strong>：图6的消融研究表明，完整的CoINS性能最优。移除技能感知组件（导致不合理决策）或反事实推理组件（导致不必要的交互）都会使成功率下降约10-15%。同时使用面向通行性的操作奖励比使用精确摆放奖励成功率更高。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>CoINS分层框架</strong>，首次将技能感知的VLM推理与多样化的物理交互执行紧密结合，使机器人能在未知杂乱环境中进行交互式导航。</li>
<li>开发了<strong>InterNav-VLM</strong>，通过注入参数化技能约束和基于反事实推理的微调，使VLM具备了物理接地推理和因果交互决策的能力。</li>
<li>构建了<strong>基于RL的技能库</strong>和<strong>InterNav仿真数据集</strong>，前者引入了面向通行性的操作范式，后者为交互导航提供了大规模训练数据。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法依赖于离线生成的反事实推理数据进行VLM微调，在线推理时不再显式计算反事实增益，这可能在某些极端复杂场景下产生次优决策。此外，技能库需要为每类交互单独训练RL策略。</p>
<p><strong>研究启示</strong>：</p>
<ul>
<li><strong>VLM与物理执行的结合</strong>：显式地将机器人本体约束和技能参数注入VLM，是实现其从“语义推理器”向“物理感知决策器”转变的有效路径。</li>
<li><strong>反事实推理的蒸馏</strong>：将复杂的因果推理逻辑通过数据蒸馏到VLM中，使其具备隐含的因果判断能力，为机器人高层规划提供了新思路。</li>
<li><strong>面向任务的技能设计</strong>：针对导航中的路径清除目标，设计“面向通行性”的奖励函数，比追求精确重排更高效，这种任务驱动的技能定义方式具有推广价值。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉语言模型（VLM）在机器人交互式导航中缺乏物理能力理解、无法主动修改环境以清除路径的核心问题，提出了CoINS分层框架。该框架通过微调技能感知VLM（InterNav-VLM），集成反事实推理来评估物体移除的因果效应，从而决策交互时机与目标；并利用强化学习构建面向可通行性的技能库执行计划。实验显示，CoINS在整体成功率上比最佳基线提升17%，在复杂长视野场景中性能提高超过80%，且泛化能力优异。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.03956" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>