<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.01941" target="_blank" rel="noreferrer">2506.01941</a></span>
        <span>作者: Hongyang Li Team</span>
        <span>日期: 2025-06-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，用于机器人学习的演示数据收集方法主要包括基于机器人的遥操作（如主从机械臂系统）、基于动捕与VR/AR可视化的系统以及手持式数据采集设备。这些方法在收集接触式操作所需的视觉-触觉数据方面存在关键局限性：基于机器人的方法灵活性差，难以规模化；基于动捕/VR的方法缺乏直接的实时触觉信号；而现有的手持式方法（如UMI）通常采用基于扳机控制的夹持器和多连杆传动设计，导致触觉反馈模糊、传输延迟，且依赖SLAM/IMU融合进行定位，引入了显著的定位误差，不利于精确的触觉感知。现有视觉-触觉数据集也大多局限于特定感知任务，缺乏覆盖完整操作过程的多样性、高精度数据。</p>
<p>本文针对高效、高保真收集接触式操作视觉-触觉数据这一痛点，提出了一个机器人无关、以人为本的新视角。其核心思路是设计一种可穿戴的“原位”夹持器，让人直接通过手指佩戴并操作，结合高精度光学动捕，实现直观控制、零机械衰减的触觉反馈与亚毫米级精度的数据同步采集。</p>
<h2 id="方法详解">方法详解</h2>
<p>FreeTacMan系统的整体目标是实现高效、高保真的多模态数据采集。其流程是：操作者佩戴可穿戴夹持器进行演示，系统同步采集手腕相机RGB图像、两个视觉-触觉传感器图像，并通过高精度动捕系统记录末端执行器位姿和夹持器宽度，最终形成与具体机器人构型无关的、同步的多模态轨迹数据。这些数据随后用于两阶段的策略学习：首先进行触觉表征预训练，然后将预训练的触觉编码器与视觉编码器结合，输入动作分块Transformer（ACT）进行模仿学习。</p>
<p><img src="https://arxiv.org/html/2506.01941v2/x2.png" alt="硬件系统"></p>
<blockquote>
<p><strong>图2</strong>：FreeTacMan硬件系统。左侧展示了用于数据收集（上）和执行（下）的“原位”夹持器接口，两者观测相同。右侧：(a) 传感器组成。(b) FreeTacMan爆炸视图。(c) 模块化设计允许在收集与执行接口间快速切换。(d) 人机界面设计。</p>
</blockquote>
<p>系统的核心是硬件设计，主要包括以下几个模块：</p>
<ol>
<li><strong>多模态感知</strong>：使用基于McTac设计的视觉-触觉传感器和腕戴式相机采集触觉与视觉信息。末端执行器位姿由高精度动作捕捉系统跟踪，达到亚毫米精度，这对接触式任务至关重要。</li>
<li><strong>“原位”夹持器</strong>：这是关键创新。视觉-触觉传感器直接安装在操作者指尖，传感器层构成皮肤与被操作物体之间的界面，消除了中间连杆，提供零机械衰减和自然的本体感觉。为实现无衰减触觉反馈到操作控制精度的直接传递，系统采用了线性传动机制（镀铬钢轴和直线轴承），将运动约束在高精度的线性轨迹上（轴向偏差≥0.02 mm）。此外，一个反向曲柄滑块机构将手指驱动的运动转换为同步的线性输出，而连接关节中的双平行轴和滚动轴承则最大限度地减少了摩擦和侧向扭矩，实现了超过90%的传动效率。</li>
<li><strong>模块化架构</strong>：系统由三个即插即用模块构成：用于触觉数据采集的传感器感知模块、用于兼容不同机器人的通用夹持器接口、以及确保腕部相机视觉反馈对齐的相机安装支架。这种设计支持在数据收集装置和机器人手臂之间快速切换传感器单元。</li>
<li><strong>人机界面设计</strong>：采用钩环绑带适应不同尺寸的指尖（覆盖成人尺寸的5%至95%分位数），整体设计轻巧（157.5克）紧凑，确保操作舒适性和效率。</li>
</ol>
<p><strong>人机数据转移</strong>：使用高精度动捕系统以300Hz频率跟踪接口的6D位姿（平均误差0.118 mm）。通过安装在接口上的五个反光标记点建立与机器人URDF对齐的本地坐标系（位于夹持器工具中心点），从而推导出末端执行器位姿。跟踪数据被下采样以与RGB图像（30Hz）同步。每个数据帧包含：腕部相机RGB图像、两张视觉-触觉图像、世界坐标系中的原位夹持器末端位姿以及夹持器宽度。</p>
<p><strong>触觉预训练与策略学习</strong>：采用两阶段方法。</p>
<ol>
<li><strong>触觉表征学习</strong>：由于触觉图像与RGB图像存在域差距，直接使用在RGB数据上预训练的视觉编码器效果不佳。因此，采用了一种CLIP风格的对比预训练流程来弥合差距。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.01941v2/x3.png" alt="触觉预训练和策略学习流程"></p>
<blockquote>
<p><strong>图3</strong>：触觉预训练和策略学习流程。(a) 使用自收集数据集预训练触觉编码器。(b) 将预训练的触觉编码器集成到基于ACT的策略中，用于下游任务（如USB插入）。</p>
</blockquote>
<p>如图3(a)所示，视觉编码器 f_v 和触觉编码器 f_t 共享从同一检查点初始化的ResNet主干。预训练期间 f_v 冻结， f_t 微调。每个编码器后接一个投影头（视觉 g_v，触觉 g_t）。g_t 首先将触觉特征与归一化的7自由度关节位置向量 q_i 连接，以注入机器人关节状态作为全局上下文。在每个时间步 i，计算归一化嵌入 v_i 和 t_i。v_i 被指定为 t_i 的主要正样本，以使触觉特征与当前视觉特征对齐。为强制时间感知，还引入了来自下一时间步的次要正样本 v_{i+1}。训练通过最小化对比损失（公式1）进行，该损失结合了主要和次要正样本以及来自固定大小记忆库 M 的负样本。<br>2.  <strong>视觉-触觉动作分块Transformer</strong>：如图3(b)所示，使用预训练的触觉编码器提取触觉表征，然后与视觉嵌入拼接，输入到动作分块Transformer（ACT）中，训练其预测关节位置。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了自收集的大规模<strong>FreeTacMan数据集</strong>，涵盖50个不同的接触式操作任务，包含超过10,000条轨迹和300万对视觉-触觉图像。评估在五个具有代表性的接触式任务上进行：<strong>易碎杯子搬运、书法书写、纹理分类、印章按压、USB插入</strong>。对比的基线方法包括：基于主从机械臂的遥操作<strong>ALOHA</strong>、手持式数据收集器<strong>UMI</strong>，以及模仿学习算法<strong>ACT</strong>（仅视觉版本）。</p>
<p><img src="https://arxiv.org/html/2506.01941v2/x6.png" alt="用户研究数据收集"></p>
<blockquote>
<p><strong>图6</strong>：数据收集的用户研究。(a-c) FreeTacMan在任务完成率、收集效率和单位时间完成数（CPUT）得分上均优于ALOHA和UMI。(d) FreeTacMan在用户体验评估中也表现出色。</p>
</blockquote>
<p><strong>用户研究（Q1）</strong>：如图6所示，FreeTacMan在任务完成率、收集效率（完成时间的倒数）和综合CPUT得分上均显著优于ALOHA和UMI。CPUT得分分别是ALOHA的5.05倍和UMI的1.52倍。在需要精确力控（如易碎杯、USB插入）或高精度轨迹控制（如书法）的复杂任务中，FreeTacMan的优势尤为明显，结合了精确的力感知和控制，实现了更高的准确性和安全性。用户体验评估也表明FreeTacMan是最用户友好和可靠的系统。</p>
<p><strong>模仿学习验证（Q2 &amp; Q3）</strong>：<br>关键定量结果如表II所示：</p>
<ul>
<li><strong>仅视觉基线（ACT）</strong> 平均成功率仅为21%，在需要精细触觉感知的任务（如USB插入）上完全失败（0%）。</li>
<li><strong>加入触觉但未预训练</strong> 将平均成功率大幅提升至55%，在易碎杯（75%）和书法（70%）等任务上提升显著，证明了触觉反馈的有效性。</li>
<li><strong>加入触觉预训练（完整模型）</strong> 进一步将平均成功率提升至71%，在书法（90%）、纹理分类（85%）和印章按压（80%）等任务上达到优异性能。USB插入任务成功率也提升至20%。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.01941v2/x8.png" alt="训练片段消融"></p>
<blockquote>
<p><strong>图8</strong>：不同训练数据量下的消融实验。即使使用少量数据，经过预训练的策略也优于使用大量数据训练的仅视觉策略，并且性能接近使用大量数据的非预训练触觉策略。</p>
</blockquote>
<p><strong>消融分析与深入结果</strong>：</p>
<ul>
<li><strong>数据稀缺下的优势（Q3）</strong>：如图8所示，在仅有50条任务特定演示的情况下，经过预训练的模型性能显著优于仅视觉基线，并且随着数据量增加，能快速接近使用大量数据的非预训练触觉模型的性能。</li>
<li><strong>泛化到未见物体（Q2）</strong>：如表III所示，在纹理分类任务中使用颜色不同的未见物体测试，加入触觉的模型（无论是否预训练）均能保持55%的准确率，显著优于仅视觉基线（15%），预训练后准确率进一步提升至70%，展示了触觉对视觉变化的鲁棒性。</li>
<li><strong>跨传感器泛化（Q3）</strong>：如表IV所示，在传感器外观（标记角度、光照）发生显著变化的跨域场景下，经过预训练的模型性能下降很小（如纹理分类从85%降至75%），而未预训练的模型性能则大幅下降（如易碎杯任务从75%骤降至10%），证明了预训练获得的触觉表征对传感器差异具有鲁棒性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1) 提出了一个创新的、机器人无关的“原位”视觉-触觉数据收集系统FreeTacMan，通过可穿戴夹持器和线性传动机制实现了高效、高保真、直观的数据采集。2) 构建并开源了一个大规模、高精度、多样化的视觉-触觉操作数据集，包含50个任务、超过1万条轨迹和300万对图像。3) 通过系统的实验验证，证明了所收集数据的有效性，以及所提出的时间感知触觉预训练策略能显著提升接触式操作策略的性能、数据效率和跨域泛化能力。</p>
<p>论文提到的局限性在于系统目前依赖外部基站（动作捕捉系统）进行亚毫米级定位。未来工作将探索开发高精度视觉算法以实现“野外”精确数据收集，并将系统扩展到多指灵巧手以支持更复杂的操作场景。这项工作为接触式操作的机器人学习提供了宝贵的数据基础设施和高效的收集工具，其“原位”设计理念和对触觉反馈质量的重视，对后续开发更自然、更高效的人机交互数据收集范式具有重要启发。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人接触丰富操作中数据收集效率低、传感器设置受限的核心问题，提出FreeTacMan系统。其关键技术是设计了一个可由人类手指佩戴、集成双视觉-触觉传感器的可穿戴夹持器，并引入高精度光学跟踪系统同步捕捉末端姿态与多模态反馈。实验表明，该系统成功构建了大规模数据集，包含超过300万对视觉-触觉图像及1万条演示轨迹，覆盖50种任务，在数据收集性能上较先前工作有显著提升，并能基于自收集数据有效学习操作策略。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.01941" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>