<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.11204" target="_blank" rel="noreferrer">2508.11204</a></span>
        <span>作者: Kwok Wai Samuel Au Team</span>
        <span>日期: 2025-08-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人操作中，视觉运动学习的采样效率对于现实世界部署至关重要。利用任务对称性作为归纳偏置以提高效率已成为一种有前景的方法。然而，现有工作大多局限于等距对称性，即在所有时间步上对所有任务对象应用相同的群变换。这种变换在空间和时间维度上缺乏多样性，限制了采样效率的进一步提升。少数探索非等距对称性的工作，例如在神经描述符场或生成流模型中，也仅限于单步或两步（如拾放）决策场景。</p>
<p>本文针对等距对称性在时空维度上变换方差有限的痛点，提出了将非等距对称性扩展到多步决策的新视角。核心思路是：在部分可观察马尔可夫决策过程（POMDP）框架内，为轨迹中的每个时间步引入独立的群变换，形成多群不变POMDP，并基于此提出一种名为多群等变增广（MEA）的简单而有效的数据增强方法，以大幅提升离线强化学习的采样效率。</p>
<p><img src="https://arxiv.org/html/2508.11204v1/fig/iso_vs_noniso.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：等距群变换与本文的非等距群变换对比。在2D平面上，夹爪在3个时间步内抓取方形物体。我们比较了基于真实轨迹（深蓝色）的增强轨迹（浅蓝色）。在等距情况下，夹爪和物体在所有时间步被相同的群元素g旋转，保持了它们的相对位姿。而在我们的非等距变换中，使用跨时间步的独立群元素g1、g2、g3，在每个步骤对夹爪和物体应用不同的非等距群表示，从而通过改变夹爪与目标物体之间的相对空间关系来增加增强的对称性方差。</p>
</blockquote>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的核心是提出了<strong>多群不变POMDP</strong>的公式化定义，并基于此设计了<strong>多群等变增广（MEA）</strong> 策略。整体流程是：首先从环境中收集真实的轨迹数据存入回放缓冲区；然后，对于缓冲区中的每条轨迹，依据多群不变POMDP的性质，从后往前（逆时间序）进行多阶段增广，生成新的、符合动力学约束的增强轨迹数据，并合并回缓冲区；最后，使用扩增后的数据集训练离线强化学习策略。</p>
<p><strong>1. 多群不变POMDP的数学定义</strong><br>传统群不变POMDP要求存在一个群G，其元素g作用于整个轨迹（状态、动作、观察），并满足动态、奖励、观察和初始信念的等距不变性（公式2a-2d）。本文将其扩展为：对于一个长度为H的轨迹，定义一组独立的群空间𝑮𝑯 = {G_t}<em>{t=0}^H。多群不变POMDP要求对于任意时间步t及其对应的群元素g_t ∈ G_t，满足一组放宽的条件（公式3a-3d）。关键区别在于，变换可以随时间步不同（例如，g_t作用于状态s_t和动作a_t，而g</em>{t+1}作用于下一个状态s_{t+1}），从而在时空维度上引入了更大的变换方差。</p>
<p><strong>2. 多阶段增广策略</strong><br>由于实际机器人任务中可能存在“不完美对称性”（即某些状态只能接受平凡表示变换），直接应用非平凡群变换（如旋转、平移）可能破坏轨迹的有效性。为此，MEA采用了三阶段逆序增广策略（见图5）。</p>
<p><img src="https://arxiv.org/html/2508.11204v1/fig/phases.png" alt="多阶段增广流程"></p>
<blockquote>
<p><strong>图5</strong>：多阶段增广的图示。增广由三个不同的阶段组成：非平凡阶段、平凡阶段和终止阶段。真实过渡数据是在智能体与环境交互时按正向时间顺序收集的。相比之下，增强过渡数据是按反向时间顺序生成的，从终止阶段开始，向后经过平凡阶段和非平凡阶段。</p>
</blockquote>
<ul>
<li><strong>终止阶段</strong>：从轨迹末端（任务成功或达到最大步数）开始。此阶段的状态被定义为不完美状态，只能应用<strong>平凡表示</strong>（即不变变换）。</li>
<li><strong>平凡阶段</strong>：从终止阶段向前回溯，直到遇到一个可以应用非平凡群变换（<strong>平移或旋转标准表示</strong>）仍能保持轨迹有效性的状态。此阶段的所有状态也应用平凡表示。</li>
<li><strong>非平凡阶段</strong>：从平凡阶段继续向前回溯至轨迹起点。此阶段的每个时间步t，独立采样一个非平凡群元素g_t，并应用其对应的表示（ρ_l或ρ_θ）来变换该时间步的状态s_t和动作a_t。根据多群不变POMDP的定义，下一个状态s_{t+1}将由g_{t+1}变换，从而保持变换后轨迹的动力学一致性。</li>
</ul>
<p><strong>3. 结构化动作增广</strong><br>在非平凡阶段对动作a_t进行变换时，除了应用相应的群表示（如旋转动作的角度分量），本文还提出了一种“结构化动作增广”。具体而言，对于连续动作空间，通过一个函数f_struct将原始动作a_t、当前状态s_t和采样的群元素g_t映射为一个增强动作a_t&#39;。例如，这可以确保当夹爪相对于物体的位姿发生旋转变换时，发出的平移命令方向也相应旋转，从而保持动作在变换后坐标系中的合理性（见图6）。</p>
<p><img src="https://arxiv.org/html/2508.11204v1/fig/augmented_action.png" alt="结构化动作增广"></p>
<blockquote>
<p><strong>图6</strong>：结构化动作增广示意图。当对状态应用旋转g_θ时，原始动作a_t（红色箭头）通过结构化函数f_struct被转换为增强动作a_t‘（绿色箭头），使其方向在变换后的坐标系中保持一致。</p>
</blockquote>
<p><strong>4. 等变体素图像表示</strong><br>为了向策略网络输入具有平移等变性的视觉观察，本文提出了一种体素基正交投影方法。首先，将点云体素化为一个3D网格。然后，沿着三个正交轴（前、右、上）进行投影，对每个投影方向，将沿该轴“穿透”体素网格所遇到的第一个占据体素的（归一化）高度值作为像素强度，从而生成三个2D图像通道。这种表示方法能保留平移等变性，且比直接投影点云产生更少的伪影（见图7）。</p>
<p><img src="https://arxiv.org/html/2508.11204v1/fig/image_reprensetation.png" alt="图像表示对比"></p>
<blockquote>
<p><strong>图7</strong>：体素基图像表示与点云基图像表示的对比。体素基方法（中）通过正交投影体素网格生成图像，相比直接投影点云（右）能减少伪影，并保持平移等变性。左图为3D点云输入。</p>
</blockquote>
<p><strong>创新点总结</strong>：1) <strong>理论框架创新</strong>：首次将非等距对称性系统性地公式化为适用于多步决策的“多群不变POMDP”。2) <strong>方法创新</strong>：设计了能处理不完美对称性的“多阶段逆序增广”策略，并结合了“结构化动作增广”以确保增强轨迹的有效性和多样性。3) <strong>表示创新</strong>：提出了具有平移等变性的体素基视觉表示方法。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：本文在两个机器人操作领域进行实验：<strong>通用机器人操作</strong>（包括方块拉动、方块抓取、抽屉打开三个任务）和<strong>手术抓取</strong>（Surgical Grasp Any任务）。实验平台基于仿真环境（BulletArm及自定义手术仿真），并进行了实物机器人验证。使用稀疏延迟奖励。</p>
<p><strong>对比基线</strong>：</p>
<ol>
<li>**Non-Equivariant (Non-Eq.)**：不使用任何对称性的非等变离线RL基线。</li>
<li>**Isometric Augmentation (IA)**：应用等距对称性进行数据增强的基线。</li>
<li>**Isometric Equivariant (IE)**：使用等距等变网络结构的基线。</li>
<li>**Ours (MEA)**：本文提出的多群等变增广方法，分别与模型无关的IE方法（MEA (IE)）和模型非等变方法（MEA (Non-Eq.)）集成。</li>
</ol>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>通用机器人操作</strong>：在三个任务上，MEA方法均显著优于基线。例如，在方块拉动任务中，MEA (IE)的最终成功率比IA基线高25%以上，比Non-Eq.基线高约4倍（见图8）。MEA还大幅减少了达到特定性能阈值所需的演示数据量（减少超过97.5%）和总训练步数。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.11204v1/fig/block_pull_eval.png" alt="方块拉动评估"></p>
<blockquote>
<p><strong>图8</strong>：方块拉动任务的评估曲线。MEA (IE)和MEA (Non-Eq.)均显著优于所有基线，收敛性能更高，样本效率更优。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.11204v1/fig/block_pick_eval.png" alt="方块抓取评估"></p>
<blockquote>
<p><strong>图9</strong>：方块抓取任务的评估曲线。MEA方法同样展现出显著的性能优势。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.11204v1/fig/drawer_open_eval.png" alt="抽屉打开评估"></p>
<blockquote>
<p><strong>图10</strong>：抽屉打开任务的评估曲线。趋势与前述任务一致，MEA方法性能最佳。</p>
</blockquote>
<ol start="2">
<li><strong>手术抓取</strong>：在更复杂的Surgical Grasp Any任务中，MEA (IE)在采样效率方面表现突出，仅需约5万步训练即可达到高成功率，而IE基线需要约12万步，实现了超过58%的训练步数与时间节省（见图11）。仿真到实物的转移实验也验证了其有效性，未见性能下降（见图12）。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.11204v1/fig/surgical_grasp_plot.png" alt="手术抓取评估"></p>
<blockquote>
<p><strong>图11</strong>：手术抓取任务的评估曲线。MEA (IE)方法以极少的训练步数快速达到高成功率，样本效率远超其他方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.11204v1/fig/surgical_grasp_real_setup.png" alt="真实手术机器人设置"></p>
<blockquote>
<p><strong>图12</strong>：真实机器人手术抓取实验设置。MEA方法成功迁移到实物系统。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过消融研究验证了各组件贡献。结果表明：1) <strong>多阶段增广</strong>是关键，去除后（即尝试全程非平凡增广）性能会因违反不完美对称性而下降；2) <strong>结构化动作增广</strong>能带来额外性能提升；3) <strong>体素图像表示</strong>优于直接的点云投影表示。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的<strong>多群不变POMDP</strong>理论框架，首次将非等距对称性扩展到多步决策问题，突破了传统等距对称性在时空维度上的限制。</li>
<li>基于该框架，提出了一种简单有效的<strong>多群等变增广（MEA）</strong> 方法，通过多阶段逆序增广和结构化动作处理，能够安全、高效地生成多样化的增强数据，显著提升离线RL的采样效率。</li>
<li>设计了一种<strong>平移等变的体素基视觉表示</strong>方法，改善了策略输入的等变特性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的方法依赖于几个关键假设：1) 对确定性过渡函数的依赖（公式7），这在无交互或干扰的机器人操作中成立，但在更随机或动态的环境中可能不适用。2) 利用了特定的任务先验（如观察函数的变换不变性）来简化问题并实现增广。</p>
<p><strong>对后续研究的启示</strong>：本文工作为在更广泛的序列决策问题中利用非等距对称性提供了一个清晰的框架和可行的实现路径。未来的研究方向包括：探索更复杂或非确定动力学下的对称性利用；将多群对称性思想与在线学习或模型基RL更深度地结合；以及探索超越SE(2)的更高维或更抽象群结构在机器人学习中的应用。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人视觉运动强化学习中采样效率低的问题，提出一种多群等变性增强（MEA）方法。传统方法局限于对任务对象施加全局等距对称变换，本文则探索非等距对称性，允许在时空维度上对机械手和目标物体分别施加独立的群变换，从而放松约束、增加数据多样性。该方法结合了离线强化学习框架，并采用保持平移等变性的体素化视觉表示。实验在仿真和真实机器人平台上验证了该方法的有效性，提升了采样效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.11204" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>