<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.14178" target="_blank" rel="noreferrer">2511.14178</a></span>
        <span>作者: Fei Chen Team</span>
        <span>日期: 2025-11-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型作为通用策略在机器人操作中展现出巨大潜力。然而，预训练的VLA策略在下游任务部署时经常出现显著的性能下降。尽管通过微调可以缓解此问题，但该方法依赖成本高昂的演示数据收集和密集计算，在实际应用中不切实际。现有研究表明，这种部署失败并不一定意味着预训练VLA策略无法生成正确行为，正确的行为模式可能已存在于策略的生成分布中，但由于运行时次优的模式选择而未能可靠执行。</p>
<p>推理时策略引导为上述模式选择问题提供了一个优雅的解决方案，它通过外部验证器评估和选择预训练策略提出的、与任务对齐的候选动作，从而在无需微调策略的情况下在运行时有效引导机器人行为。然而，现有方法存在两个关键局限：首先，这些方法中使用的验证器通常需要额外训练，且因其训练数据分布狭窄而泛化能力有限；其次，这些方法仅依赖于从固定提案集中选择动作。在复杂的下游任务中，预训练VLA策略可能无法生成任何与任务上下文对齐的候选动作，此时验证器仅通过选择无法恢复成功行为，导致引导失败。</p>
<p>本文针对这些局限性，提出了一种无需训练、在推理时执行的策略引导方法VLA-Pilot，旨在提升预训练VLA策略在下游部署中的泛化能力和任务对齐性。其核心思路是：利用多模态大语言模型（MLLM）作为开放世界验证器以增强泛化，并采用进化扩散过程作为动作优化器以改善任务对齐。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLA-Pilot旨在引导一个冻结的预训练VLA策略 $\pi_{\text{vla}}(a_{t}\mid c_{t})$，其中任务上下文 $c_{t}=(o_{t},l)$ 包含视觉观测 $o_{t}$ 和语言指令 $l$。其目标是从策略采样的 $M$ 个独立同分布的动作提案 ${a_{t}^{i}}<em>{i=1}^{M}$ 中，找到一个与上下文 $c</em>{t}$ 最对齐的动作提案 $a_{t}^{\star}$，该对齐性由一个引导目标奖励 $R(a_{t};c_{t})$ 来衡量。</p>
<p><img src="https://arxiv.org/html/2511.14178v1/figs/fig2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：VLA-Pilot 概述。给定任务上下文，VLA-Pilot通过三个关键步骤在推理时引导预训练VLA策略：1) 引导目标推理：使用EPS-CoT模块从给定任务上下文中推理出与任务对齐的引导目标奖励；2) 动作提案优化：利用进化扩散算法，基于推理出的目标奖励对来自预训练VLA的动作提案进行评分和优化，并执行得分最高的提案；3) 迭代引导精炼：将执行后反思整合到EPS-CoT推理循环中，实现闭环精炼以提高引导准确性和鲁棒性。</p>
</blockquote>
<p>VLA-Pilot的整体流程如算法1所示，包含三个核心模块：</p>
<p><strong>1. 引导目标推理 (Steering Objective Reasoning)</strong><br>该模块的核心是具身策略引导思维链（EPS-CoT）$\mathcal{F}<em>{\text{EPS-CoT}}$，它利用MLLM $\Phi</em>{\text{MLLM}}$ 的开放世界推理能力，从上下文 $c_{t}$ 中生成引导目标奖励：$R(a_{t};c_{t})=\mathcal{F}<em>{\text{EPS-CoT}}(\Phi</em>{\text{MLLM}}(c_{t}))$。</p>
<p><img src="https://arxiv.org/html/2511.14178v1/figs/fig3.png" alt="EPS-CoT推理过程"></p>
<blockquote>
<p><strong>图3</strong>：具身策略引导思维链。EPS-CoT通过结构化的思维链指导引导目标推理过程。</p>
</blockquote>
<p>EPS-CoT将推理过程分解为四个交错阶段：</p>
<ul>
<li><strong>引导目标确认</strong>：提示MLLM复述并验证语言指令，确保任务需求与引导目标对齐。</li>
<li><strong>场景理解</strong>：MLLM解读任务上下文，并根据视觉观察识别潜在的动作模式。</li>
<li><strong>具身增强</strong>：通过视觉基础模型（DINO和SAM）提取机器人末端执行器位置和物体位置的空间关键点，将此具身信息整合到推理过程中。</li>
<li><strong>奖励生成</strong>：基于场景理解和具身信息，推断出与任务对齐的引导目标，并生成相应的评分奖励代码。该奖励被实现为一个不可微的黑盒评分函数，以捕捉语言指令的模糊性并简化MLLM所需的推理过程。</li>
</ul>
<p><strong>2. 动作提案优化 (Action Proposal Optimization)</strong><br>此模块引入了<strong>进化扩散算法</strong>，协同利用扩散过程的多模态表达能力和进化搜索的黑盒优化能力。算法步骤如下：</p>
<ol>
<li><strong>初始化</strong>：从预训练VLA策略采样 $M$ 个初始动作提案 $A^{0}$。</li>
<li><strong>进化搜索循环</strong>（进行 $K$ 次迭代）：<ul>
<li><strong>评分与精英选择</strong>：使用奖励 $R$ 为当前提案集 $A^{k}$ 评分，根据得分分布 $q(a_{t})$（由温度参数 $\tau$ 控制锐度）通过重要性采样选择精英提案 $E^{k+1}$。</li>
<li><strong>扩散与去噪突变</strong>：对精英提案 $E^{k+1}$ 应用<strong>截断的扩散-去噪过程</strong>以实现突变。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.14178v1/figs/fig4.png" alt="截断扩散-去噪过程"></p>
<blockquote>
<p><strong>图4</strong>：截断扩散-去噪过程。VLA-Pilot采用截断的扩散-去噪机制对精英提案进行突变，从而增强动作多样性和探索能力，以实现更好的任务对齐。</p>
</blockquote>
<pre><code>*   首先，运行前向扩散过程的前 $n$ 步，为精英提案添加噪声，得到噪声化提案 $\bar{E}^{k+1}$。
*   随后，使用预训练VLA策略中的噪声预测器，执行反向扩散过程的最后 $n$ 步，对 $\bar{E}^{k+1}$ 进行去噪，得到 refined 的提案集 $A^{k+1}$ 用于下一次迭代。这一步确保突变后的提案仍位于原始VLA的数据流形内。
</code></pre>
<ol start="3">
<li><strong>动作选择</strong>：进化循环结束后，从最终提案集 $A^{K}$ 中选择奖励得分最高的动作 $a_{t}^{\star}$ 执行。</li>
</ol>
<p><strong>3. 迭代引导精炼 (Iterative Steering Refinement)</strong><br>该机制通过将<strong>反思步骤</strong>整合到EPS-CoT中，实现引导目标和结果动作的闭环校正。具体而言，MLLM被提示根据初始提案 $a_{0}$、已执行的动作 $a_{t}^{\star}$、执行后的任务上下文 $\bar{c}<em>{t}$ 以及之前的推理历史 $\mathcal{H}</em>{t}$ 进行自我批判，生成引导成功指示器 $s$。如果MLLM检测到推断的奖励不一致，则重新生成新奖励；如果执行的动作与任务上下文未对齐（$s=False$），则VLA-Pilot继续引导过程直至任务完成。</p>
<p><strong>创新点</strong>：与现有方法相比，VLA-Pilot的创新具体体现在：1) 使用MLLM推断<strong>隐式的、高层的引导目标奖励</strong>，而非显式评估单个动作，避免了训练任务特定验证器，提升了泛化能力；2) 提出<strong>进化扩散算法</strong>，不仅选择，而且<strong>优化演化</strong>动作提案分布，即使初始提案次优或不可行也能实现有效引导。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>平台与任务</strong>：实验在真实机器人上进行。主要使用双臂系统DOBOT X-Trainer，并在Franka Panda机器人上进行跨形态验证。共设计了六个下游操作任务，包括四个简单单臂任务和两个复杂双臂任务（见图5），并分为分布内（ID）和分布外（OOD）场景进行评估。</li>
<li><strong>基线方法</strong>：对比了两种预训练VLA策略（DiVLA, RDT-1B）、两种推理时引导方法（V-GPS, FOREWARN）以及两种策略的微调版本（DiVLA-finetune, RDT-1B-finetune，各使用50个演示）。</li>
<li><strong>评估指标</strong>：操作成功率（MSR）和引导目标对齐率（SOA）。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.14178v1/figs/fig5.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人实验的定性结果。VLA-Pilot在推理时有效引导即用型预训练VLA策略完成下游任务，在ID和OOD任务场景中均实现了零样本部署。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>提升预训练VLA策略性能</strong>：如表I所示，VLA-Pilot持续提升了DiVLA和RDT-1B在所有六个下游任务中的性能，平均MSR分别提高了0.31和0.30（相对提升约100%）。预训练策略的失败多源于无法一致选择与任务语义对齐的动作，而非不能生成可行动作。</li>
<li><strong>对比现有引导方法</strong>：在ID设置下，VLA-Pilot在平均MSR和SOA上均优于V-GPS和FOREWARN（表I）。在简单任务上差距较小，但在如双臂拉链等复杂任务上优势显著。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.14178v1/figs/fig6.jpg" alt="动作提案可视化对比"></p>
<blockquote>
<p><strong>图6</strong>：双臂拉链任务中左臂动作提案的可视化。(a) 依赖后排序和选择的基线引导方法因缺乏可行初始提案而失败；(b) 增加提案数量不能保证产生可行动作；(c) 本文方法采用进化扩散将动作提案演化至具有更高任务对齐性的目标分布；(d) 微调策略产生的目标分布。</p>
</blockquote>
<p>   图6直观展示了原因：在复杂任务中，基线方法依赖的静态排序和选择经常因初始候选动作缺乏可行执行模式而失败；VLA-Pilot则通过进化扩散将精英提案向更高可行性和任务对齐性的分布演化。此外，VLA-Pilot的迭代精炼机制使其能逐步修正策略输出。<br>3.  <strong>强大的OOD泛化能力</strong>：在OOD设置下（表II），VLA-Pilot保持了强劲性能（平均MSR 0.50），而V-GPS和FOREWARN显著下降（平均MSR分别为0.12和0.19）。这得益于VLA-Pilot使用MLLM进行开放世界引导目标推理。<br>4.  <strong>与微调方法性能相当</strong>：如图7所示，VLA-Pilot达到了与使用50个演示进行监督微调的方法相当的性能。这表明对于因任务误感知或次优动作选择导致的失败，通过推理时引导来调整现有策略足以恢复其能力，微调可能并非必要。</p>
<p><img src="https://arxiv.org/html/2511.14178v1/figs/fig7.jpg" alt="与微调方法对比"></p>
<blockquote>
<p><strong>图7</strong>：与VLA微调方法对比。VLA-Pilot取得了与使用50个演示进行VLA微调的方法相当的性能。</p>
</blockquote>
<ol start="5">
<li><strong>跨形态零样本泛化</strong>：如表III和图8所示，在未进行任何额外微调的情况下，将VLA-Pilot部署到Franka Panda机器人上，相比预训练DiVLA基线，在所有四个单臂任务上均取得了显著的性能提升（MSR提升0.21至0.31），展示了跨形态的一致性行为。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.14178v1/figs/fig8.png" alt="跨形态实验结果"></p>
<blockquote>
<p><strong>图8</strong>：跨形态实验的定性结果。VLA-Pilot在Franka机器人上实现了零样本泛化，在四个单臂任务上保持了稳定的任务性能。</p>
</blockquote>
<p><strong>组件贡献</strong>：实验分析表明，MLLM作为开放世界验证器是VLA-Pilot在OOD任务上强大泛化能力的关键；进化扩散算法是其在复杂任务上超越仅基于选择的基线方法的核心；迭代引导精炼机制则通过闭环校正进一步提升了引导的准确性和鲁棒性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了VLA-Pilot，一种即插即用的推理时策略引导方法，无需任何额外的策略微调或数据收集，即可实现冻结的预训练VLA策略在多样化下游任务和形态上的零样本泛化。</li>
<li>提出了一种具身推理引导的进化扩散策略，能够联合推断广义的引导目标并优化动作提案以增强任务对齐性。</li>
<li>在六个真实世界操作任务和两种不同机器人形态上进行了广泛实验。结果表明，VLA-Pilot将两种预训练VLA策略的平均成功率提升了31%，显著优于所有基线方法。</li>
</ol>
<p><strong>局限性</strong>：论文正文未明确讨论自身方法的局限性。</p>
<p><strong>启示</strong>：本文探索了一种有前景的范式，即专注于在推理时最大化现有VLA模型的效用，而非追求越来越大的数据集和架构。它证明预训练VLA模型已经封装了足够的潜在知识来解决新任务，并且这些知识可以通过所提出的引导机制被有效地提取并与任务目标对齐。这为高效、灵活地部署通用机器人基础模型提供了新思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决预训练视觉-语言-动作模型在下游任务部署时性能显著下降，且传统微调方法数据与计算成本高昂的问题。为此，提出了VLA-Pilot方法，这是一种即插即用的推理时策略引导技术，其核心是基于“具身进化扩散”机制，在无需任何微调或额外数据收集的情况下，于运行时优化策略的行为模式选择。实验在涵盖两种机器人平台的六项真实操作任务上进行，结果表明，该方法能显著提升预训练VLA策略的成功率，实现了对多样任务与平台的鲁棒零样本泛化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.14178" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>