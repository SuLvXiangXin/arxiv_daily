<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01939" target="_blank" rel="noreferrer">2602.01939</a></span>
        <span>作者: Qiang Nie Team</span>
        <span>日期: 2026-02-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前人形机器人操作研究的一个趋势是将主摄像头安装在机器人头部，以获得基地不变的灵活性，但这导致视觉遮挡问题更加频繁。近期研究（如AV-ALOHA、ViA）通过采用高自由度（6/7-DoF）主动脖子来提供主动视觉以应对此问题。然而，本文指出，视觉遮挡问题的本质是<strong>完成任务所需信息的缺失</strong>。许多现有的人形机器人并不具备这种高自由度的主动脖子，但它们通常拥有两条手臂。</p>
<p>基于此，本文提出了一个更根本、更广泛的新问题：<strong>探索性与专注性操作</strong>。该问题的核心是<strong>主动寻求信息</strong>，以完成那些需要探索（如寻找隐藏属性）或需要专注（如执行精细操作）的挑战性操作任务。本文的核心思路是：利用双手机器人中<strong>非操作臂</strong>（如果可用）提供眼在手机器视觉，同时利用<strong>操作臂</strong>在接触时提供力觉感知，从而构成一种无需高自由度主动脖子的双手机器人主动感知策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的双手机器人主动感知策略旨在解决EFM问题。其整体框架是数据驱动的模仿学习。策略的输入包括：头部固定摄像头的视图（主视图）、左右手腕摄像头提供的主动视图、机器人状态（左右末端执行器位姿和夹爪状态），以及（可选的）操作臂的六维力/力矩传感器数据。输出是在笛卡尔空间中的动作序列。</p>
<p><img src="https://arxiv.org/html/2602.01939v1/x2.png" alt="硬件系统总览"></p>
<blockquote>
<p><strong>图2</strong>：硬件系统与任务物体概览。展示了用于数据收集的双手机器人JAKA K-1、头部Orbbec相机、手腕Logitech相机以及VR遥操作设备Pico Ultra 4。</p>
</blockquote>
<p>BAP策略包含两个核心模块：</p>
<ol>
<li><strong>眼在手机器视觉</strong>：由非操作臂（空闲臂）提供。在需要探索或专注时，操作员（或策略）主动控制该臂，使其手腕摄像头捕获<strong>操作区域以及操作末端执行器</strong>的清晰视图。论文通过实验强调，在操作手持物体进行精细操作时，主动视图中同时捕获操作区域和末端执行器至关重要，仅捕获手持物体无法提供末端执行器应如何调整位姿的直接线索。</li>
<li><strong>力觉感知</strong>：由操作臂内置的力/力矩传感器提供。该信息有助于处理涉及精细接触的操作任务，为实现基于神经网络的力顺应控制提供可能。</li>
</ol>
<p>该策略的主要创新点在于：<strong>为不具备高自由度主动脖子的现有人形机器人提供了一种实现主动感知的实用方案</strong>。它充分利用了双手机器人并非总需同时操作的特点，将一条臂转换为“感知臂”。该策略与基于脖子的主动视觉完全兼容，未来可结合使用以最大化所有可用摄像头的效用。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文构建了<strong>EFM-10</strong>基准，包含4大类共10个任务：语义探索类（Toy-Find, Toy-Match）、涉及视觉遮挡的探索类（Cup-Hang, Cup-Place, Box-Push）、需要专注的精细操作类（Light-Plug, Bread-Brush, Nail-Knock）以及兼具探索与专注的复杂类（Cable-Match, Charger-Plug）。基于BAP策略，在真实双手机器人上收集了包含1810条专家轨迹的<strong>BAPData</strong>数据集。</p>
<p>实验平台为JAKA K-1双手机器人，数据频率为10Hz。对比的基线策略包括：ACT、DP、GR-MG和Pi-0。所有策略均在BAPData上以模仿学习方式训练（除非特别说明，未使用力觉数据）。</p>
<p>首先，实验验证了主动视图中捕获内容的重要性。在Toy-Match等4个任务上比较了三种视觉上下文设置的成功率。</p>
<p><img src="https://arxiv.org/html/2602.01939v1/x1.png" alt="主动视觉设置对比结果"></p>
<blockquote>
<p><strong>表3</strong>：不同主动视觉设置下的任务成功率对比。结果表明，主动视图需同时捕获操作区域和末端执行器才能获得最佳性能。</p>
</blockquote>
<p>其次，在EFM-10基准上全面评估了各代表性策略的性能。</p>
<p><img src="https://arxiv.org/html/2602.01939v1/x1.png" alt="策略在EFM-10上的评估结果"></p>
<blockquote>
<p><strong>表4</strong>：各策略在EFM-10各任务上的成功率。使用BAPData训练（标有⋆）的策略性能显著提升。单任务策略（ACT, DP）无法处理语言驱动的语义探索任务；多任务策略中，Pi-0在指令跟随和非精细任务上表现更强，而所有策略在极精细操作任务（如Light-Plug）上表现均不佳。</p>
</blockquote>
<p>第三，实验探究了融合力觉感知的效果。通过修改GR-MG策略，使其额外输入当前力/力矩并预测未来力/力矩块。</p>
<p><img src="https://arxiv.org/html/2602.01939v1/x3.png" alt="融合力感知的GR-MG策略示意图"></p>
<blockquote>
<p><strong>图3</strong>：将力感知融入GR-MG策略的方法示意图。在输入中加入力/力矩数据，并训练模型额外预测未来的力/力矩。</p>
</blockquote>
<p>在Light-Plug和Bread-Brush任务上的实验表明，加入力感知后，成功率分别提升了16.7%和13.3%，同时操作末端执行器的最大垂直力平均值显著降低了29%和22%，表明神经网络实现了某种形式的力顺应控制。</p>
<p><img src="https://arxiv.org/html/2602.01939v1/x4.png" alt="力感知策略的定性分析"></p>
<blockquote>
<p><strong>图4</strong>：融合力感知的GR-MG策略在Light-Plug任务中的一次运行可视化。模型能够预测接触力的变化并相应地控制末端执行器，避免垂直力的突然增加。</p>
</blockquote>
<p>最后，论文对失败案例进行了定性分析。</p>
<p><img src="https://arxiv.org/html/2602.01939v1/x5.png" alt="典型失败案例可视化"></p>
<blockquote>
<p><strong>图5</strong>：模仿学习策略的典型失败案例。主要包括：语义条件不准确（拿错颜色）、空间感知/推理能力不足导致细微定位错误、以及未能找到最优主动视角以避免遮挡。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了<strong>探索性与专注性操作</strong>这一新问题，并构建了包含10个任务的<strong>EFM-10</strong>基准；2) 提出了适用于现有机型的<strong>双手机器人主动感知</strong>策略，并基于此收集了兼具高自由度主动视觉和力觉信息的大规模数据集<strong>BAPData</strong>；3) 通过实验验证了BAP策略的有效性，系统评估了代表性策略在EFM任务上的优缺点，并揭示了同时捕获末端执行器、融合力感知等技术细节的重要性。</p>
<p>论文提到的局限性包括：BAPData数据集是在特定机器人硬件上收集的，其泛化性有待进一步验证。</p>
<p>本文对后续研究的启示在于：为了更好解决EFM问题，未来的策略模型需要着重提升以下几方面能力：<strong>语义条件化</strong>（准确理解并执行语义指令）、<strong>空间感知与推理</strong>（提升对精细操作的空间判断）、以及<strong>最优主动视点搜索</strong>（动态选择最佳观测视角）。同时，将BAP策略与基于脖子的主动视觉相结合，也是一个有前景的未来方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中因视觉遮挡导致的信息不足问题，提出了“探索性与聚焦性操作”（EFM）这一新问题。为此，研究者建立了包含10个任务的EFM-10基准，并提出了“双臂主动感知”（BAP）策略：利用一只手臂提供主动视觉，另一只手臂在操作时提供力感知。基于该策略收集了BAPData数据集，并通过模仿学习验证了BAP策略的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01939" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>