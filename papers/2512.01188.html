<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Real-World Reinforcement Learning of Active Perception Behaviors - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Real-World Reinforcement Learning of Active Perception Behaviors</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.01188" target="_blank" rel="noreferrer">2512.01188</a></span>
        <span>作者: Dinesh Jayaraman Team</span>
        <span>日期: 2025-12-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人学习领域，模仿学习和强化学习是合成任务相关策略的主流方法。然而，对于主动感知行为的学习，这两种方法均存在关键局限性。模仿学习的性能受限于演示者，而获取最优的主动感知演示（例如强制遥操作员通过腕部摄像头观察）通常繁琐且不自然。强化学习理论上可以从交互中学习，但在实践中样本效率低下，在完全可观测场景下尚且如此，在需要主动感知的部分可观测场景下则更为困难。此外，由于主动感知任务与传感器能力紧密相关，而RGB、深度、触觉等传感器难以高保真模拟，使得从仿真到实体的迁移极具挑战性。本文针对在部分可观测环境下高效学习真实世界机器人主动感知行为这一具体痛点，提出了利用训练时可访问“特权”额外传感器的新视角。其核心思路是，通过扩展优势加权回归算法，利用特权信息训练高质量的价值函数来估计策略的优势，从而更有效地监督仅接收部分观测的策略学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架遵循不对称训练与部署的范式。在训练阶段（包括离线和在线），策略 $\pi$ 接收部分观测 $o_t$（如腕部摄像头图像），而评论家 $Q$ 和价值函数 $V$ 则接收特权观测 $o_t^+$（如物体检测框、分割掩码或真实状态）来估计优势函数 $A^\mu(s_t, z_t, a_t)$。该优势估计作为权重，用于加权行为克隆损失来更新策略。在部署阶段，特权传感器被移除，策略仅基于部分观测 $o_t$ 生成动作。</p>
<p><img src="https://arxiv.org/html/2512.01188v1/figures/AAWR_method_v2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：AAWR方法示意图。上行：策略接收部分观测。下行：仅在训练时可用的特权观测或状态被提供给评论家网络以估计优势。优势估计作为损失函数中的权重，为策略提供特权监督。</p>
</blockquote>
<p>核心模块是不对称优势加权回归目标函数 $\mathcal{L}<em>{\text{AAWR}}(\pi)$。该方法是对标准优势加权回归的扩展，其关键在于优势估计依赖于特权状态 $s$ 和智能体状态 $z$（即历史编码）。具体目标函数如下：<br>$$\mathcal{L}</em>{\text{AAWR}}(\pi) = \mathbb{E}<em>{(s,z) \sim d_\mu(s,z)} \mathbb{E}</em>{a \sim \mu(a|z)} \left[ \exp\left( A^\mu(s, z, a) / \beta \right) \log \pi(a|z) \right]$$<br>其中 $A^\mu(s, z, a) = Q^\mu(s, z, a) - V^\mu(s, z)$ 是特权优势函数。论文从理论上证明，在部分可观测马尔可夫决策过程中，为最大化期望策略改进并施加KL散度约束，其拉格朗日松弛问题的最优解正是上述AAWR目标。相反，如果仅使用智能体状态 $z$ 来估计优势的对称版本则无法得到正确解，因为无法准确估计等效MDP（状态为 $(s, z)$）的优势。</p>
<p>在实现上，采用离线到在线强化学习流程。首先，使用离线数据集 $\mathcal{D}<em>{\text{off}}$（包含次优演示）通过IQL算法训练特权评论家和价值函数，并利用上述AAWR目标更新策略。随后，在在线阶段，执行策略收集数据存入 $\mathcal{D}</em>{\text{on}}$，并混合两个数据集的数据进行策略和值函数的持续优化。部署时则仅运行训练好的策略。</p>
<p><img src="https://arxiv.org/html/2512.01188v1/figures/aawr_pi0_sysdiagram.png" alt="训练与部署流程"></p>
<blockquote>
<p><strong>图6</strong>：AAWR离线到在线训练及与通用策略 $\pi_0$ 交接的完整系统示意图。左侧为训练阶段，策略利用特权传感器学习；右侧为部署阶段，仅使用策略进行主动感知搜索，完成后将控制权交给 $\pi_0$ 执行抓取。</p>
</blockquote>
<p>与现有方法相比，创新点具体体现在：1) 理论推导了在POMDP中应用AWR时需要不对称（特权）优势估计的必然性；2) 提出了实用的AAWR算法，能够高效利用真实世界中有限的、可能带有噪声的特权观测数据，而非依赖仿真中大量的完美状态转移；3) 将方法应用于提升现有通用策略在部分可观测任务上的性能，通过主动感知为其“探路”。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>论文在8个不同的任务上评估AAWR，涵盖模拟和真实世界设置，涉及Koch和Franka等机器人平台。任务类型包括模拟主动感知（如Camouflage Pick）、真实交互感知（如Blind Pick）以及为通用视觉语言动作策略 $\pi_0$ 进行搜索的“交托”任务（如Bookshelf-P）。对比的基线方法包括：对称AWR、行为克隆、蒸馏法、变分信息瓶颈以及 exhaustive 搜索和 VLM+$\pi_0$ 等。评估指标包括任务成功率、搜索行为评分、完成率和步骤数。</p>
<p><img src="https://arxiv.org/html/2512.01188v1/figures/sim_results.png" alt="模拟任务结果"></p>
<blockquote>
<p><strong>图4</strong>：模拟实验评估曲线（10次随机种子）。阴影区域表示离线预训练阶段。AAWR在所有模拟任务中均优于基线方法。</p>
</blockquote>
<p>在模拟主动感知任务中，AAWR显著优于基线。在Camouflage Pick和Fully Obs. Pick任务中，AAWR的成功率分别比对称AWR和行为克隆高出约2倍和3倍。在更具挑战性的Active Perception Koch任务中，AAWR达到了100%成功率，而蒸馏法停滞在80%，变分信息瓶颈法则在部署时崩溃。这表明AAWR能够通过在线探索发现有效的主动感知行为，而其他利用特权信息的方法则可能陷入局部最优或无法适应部署时的信息缺失。</p>
<p><img src="https://arxiv.org/html/2512.01188v1/figures/AAWR_vs_AWR_left.png" alt="真实交互感知结果"></p>
<blockquote>
<p><strong>图5</strong>：Koch机器人交互感知任务（Blind Pick）的结果对比。AAWR的离线和在线版本在抓取和拾取成功率上均优于对称AWR和行为克隆。</p>
</blockquote>
<p>在真实世界Blind Pick任务中，在线AAWR取得了94%的抓取成功率和89%的拾取成功率，表现最佳。离线AAWR也优于离线对称AWR，但经过在线微调后性能得到进一步提升，消除了某些次优行为。</p>
<p><img src="https://arxiv.org/html/2512.01188v1/figures/pi0_metric.png" alt="交托任务结果"></p>
<blockquote>
<p><strong>图7</strong>：交托任务评估指标说明。通过搜索评分、完成率和步骤数综合评价主动感知策略为通用策略 $\pi_0$ 做准备的效果。</p>
</blockquote>
<p>在4个真实世界“交托”任务中，AAWR在搜索行为评分、任务完成率方面 consistently 优于所有基线，且所需步骤更少。例如在Bookshelf-P任务中，AAWR搜索成功率达92.4%，完成率44.4%，而对称AWR分别为79.6%和0%，通用策略 $\pi_0$ 自身则仅为11%和16.7%。这证明AAWR能有效学习信息搜集行为，显著增强通用策略处理严重部分可观测任务的能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了不对称优势加权回归算法，通过利用训练时的特权传感器高效学习真实世界机器人的主动感知策略；2) 从理论上证明了在POMDP中使用特权优势估计对于AWR类方法是必要的；3) 在8个不同任务上进行了广泛实验，验证了AAWR在多种部分可观测场景、机器人平台和任务类型中的有效性。</p>
<p>论文提到的局限性主要在于需要额外的传感器（如用于提供边界框或分割掩码的摄像头）来在训练时生成特权信息。</p>
<p>本文的启示在于：为学习复杂的、与环境交互以获取信息的感知行为提供了一条高效的现实途径，特别是当最优演示难以获取或仿真到实体迁移不可行时。此外，它展示了如何通过专门的主动感知策略来辅助和提升在大规模数据上训练的通用策略，使其能够应对部分可观测的挑战性任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人在部分可观测环境下难以通过标准学习技术生成主动感知行为的问题，提出了非对称优势加权回归（AAWR）方法。该方法利用训练时可用的“特权”额外传感器，训练高质量特权价值函数以估计策略优势，并从少量次优演示与粗略策略初始化进行引导。实验表明，AAWR在3种机器人、8个操作任务上优于所有现有方法，能高效生成信息收集行为，使机器人在严重部分可观测条件下有效操作。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.01188" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>