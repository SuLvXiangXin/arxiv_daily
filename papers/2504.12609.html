<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.12609" target="_blank" rel="noreferrer">2504.12609</a></span>
        <span>作者: Lum, Tyler Ga Wei, Lee, Olivia Y., Liu, C. Karen, Bohg, Jeannette</span>
        <span>日期: 2025/04/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，教授机器人灵巧操作技能的主流方法是通过可穿戴设备或遥操作收集数百条演示进行模仿学习（IL）。然而，为灵巧手收集高质量演示既昂贵又难以扩展。相比之下，记录人类操作物体的视频成本低廉、易于扩展，但由于缺乏明确的动作标签以及人-机器人形态差异，直接将其用于机器人学习十分困难。现有方法通常通过指尖重定向和逆运动学（IK）将估计的人手姿态转换为机器人动作标签，但手部姿态估计易受遮挡和噪声影响，且简单的重定向策略会因形态差异导致次优的机器人轨迹。</p>
<p>本文针对上述痛点，提出了一种新的视角：不将人类演示直接用作模仿的动作标签，而是将其作为任务规范和探索引导的来源。具体而言，本文提出了 <strong>Human2Sim2Robot</strong> 框架，其核心思路是：仅从一段人类RGB-D演示视频中提取物体姿态轨迹和预操作手部姿态，利用前者定义与形态无关的物体中心奖励，利用后者初始化并引导仿真中的强化学习（RL）探索，从而训练出能够零样本迁移到真实机器人的灵巧操作策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>Human2Sim2Robot 是一个“真实-仿真-真实”的三阶段框架。整体流程为：1）从单段人类RGB-D视频中提取任务信息；2）在仿真环境中利用RL训练策略；3）将训练好的策略零样本部署到真实机器人。</p>
<p><img src="https://arxiv.org/html/2504.12609v3/x1.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：Human2Sim2Robot 框架概览。仅需一段人类RGB-D视频，从中提取物体姿态轨迹和预操作姿态。利用这些信息在仿真中通过RL训练策略，并零样本迁移到真实机器人。</p>
</blockquote>
<p><strong>核心模块一：真实到仿真与人类演示处理</strong><br>首先，使用现成应用扫描物体网格 $\mathcal{O}$ 和场景网格 $\mathcal{S}$，构建数字孪生仿真环境（约10分钟）。录制一段单目RGB-D视频 ${\bm{I}<em>{t}}</em>{t=1}^{T}$。处理该视频以获取两个关键信息：</p>
<ol>
<li><strong>物体6D姿态轨迹</strong> ${\bm{T}^{\text{target}}<em>{t}}</em>{t=1}^{T}$：使用Segment Anything Model 2 (SAM 2)生成逐帧物体掩码，并输入FoundationPose模型（需物体网格 $\mathcal{O}$）进行估计。此轨迹用于定义物体中心的奖励函数。</li>
<li><strong>人手姿态轨迹</strong>：使用HaMeR模型从RGB帧估计MANO参数 ${(\bm{\theta}<em>{t},\bm{\beta}</em>{t})}<em>{t=1}^{T}$，并利用深度信息通过ICP配准提高精度。从中确定 <strong>预操作手部姿态</strong>：首先找到物体速度首次超过阈值 $v</em>{\text{min}}=5cm/s$ 的时刻 $t_0$，然后取 $t_0$ 之前固定偏移 $t_{\text{offset}}$ 的时刻 $\tau$ 的姿态 $(\bm{\theta}<em>{\tau},\bm{\beta}</em>{\tau})$。计算该姿态下的指尖位置和中指基关节姿态作为预操作姿态。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.12609v3/x2.png" alt="演示处理"></p>
<blockquote>
<p><strong>图2</strong>：人类演示处理流程。(1) 提取物体姿态轨迹用于定义奖励；(2) 提取预操作手部姿态用于初始化RL训练。</p>
</blockquote>
<p><strong>核心模块二：人手到机器人手的重定向</strong><br>将提取的人手预操作姿态通过两步IK（使用cuRobo）重定向为机器人预操作姿态 $(\bm{T}^{\text{wrist}},\bm{q}^{\text{hand}})$。</p>
<ul>
<li><strong>第一步（手臂）</strong>：调整机器人手臂关节角，使机器人手中指基关节的位置和朝向与估计的人手中指基关节对齐（有一个小偏移）。</li>
<li><strong>第二步（手）</strong>：调整机器人手关节角，使机器人指尖位置与对应的人手指尖位置对齐。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.12609v3/x3.png" alt="人手到机器人手重定向"></p>
<blockquote>
<p><strong>图3</strong>：人手到机器人手重定向过程。(a) 估计的MANO手部姿态。(b) IK第一步：对齐中指基关节（手臂调整）。(c) IK第二步：对齐指尖位置（手部调整）。</p>
</blockquote>
<p><strong>核心模块三：仿真中的策略学习</strong><br>在IsaacGym中构建与真实环境一致的数字孪生，使用PPO算法训练策略。策略的输入（观察）$\bm{o}_t$ 包括机器人关节状态、指尖和手掌位置、当前物体及目标物体的锚点位置。策略的输出（动作）$\bm{a}_t$ 控制手掌目标位姿和手部关节的主成分分析（PCA）值，遵循一种几何织物控制器。</p>
<p><strong>核心创新点</strong>：</p>
<ol>
<li><strong>奖励函数</strong>：使用从演示中提取的密集物体姿态轨迹定义奖励 $r^{\text{obj}}<em>{t}=\exp\bigl{(}-\alpha,d(\bm{T}^{\text{target}}</em>{\tau+t},\bm{T}^{\text{obj}}_{t})\bigr{)}$。距离函数 $d$ 通过物体局部坐标系中选定的锚点 $\bm{k}_i$ 计算，同时考虑位置和方向。</li>
<li><strong>初始化分布</strong>：基于重定向得到的机器人预操作姿态构建初始状态分布，通过在该姿态附近采样物体初始位姿并施加微小扰动，然后求解可行的机器人手臂构型来获得初始状态。这为RL探索提供了有利的起点。</li>
<li><strong>鲁棒性训练</strong>：使用域随机化处理未知动力学参数；训练LSTM策略以处理部分可观测性；对观察到的物体姿态添加噪声；施加随机物体力以提高对意外接触和扰动的鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.12609v3/x4.png" alt="物体姿态跟踪奖励"></p>
<blockquote>
<p><strong>图4</strong>：物体姿态跟踪奖励示意图。智能体通过最小化当前物体姿态与目标姿态之间锚点的距离来获得奖励。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>硬件</strong>：7自由度KUKA LBR iiwa 14机械臂 + 16自由度Allegro灵巧手。使用ZED 1立体相机记录演示并为策略提供实时物体姿态估计。</li>
<li><strong>任务与物体</strong>：涉及 <code>snackbox</code>（零食盒）、<code>pitcher</code>（水壶）、<code>plate</code>（盘子）三个物体，任务包括抓取、非抓取操作、利用外物操作以及组合多步任务（如“盘子-翻转-抬起-放入碗架”）。</li>
<li><strong>评估</strong>：在真实世界进行成功率评估（每个任务10次 rollout），在仿真中进行消融实验（3个随机种子）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>与基线的真实世界对比</strong>：对比了三种需要全轨迹动作标签的基线：<code>Replay</code>（开环回放重定向轨迹）、<code>Object-Aware (OA) Replay</code>（根据初始物体位姿变换调整后的回放）、<code>Behavior Cloning (BC)</code>（用30条生成演示训练的闭环扩散策略）。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.12609v3/x6.png" alt="真实世界成功率"></p>
<blockquote>
<p><strong>图6</strong>：真实世界任务成功率对比。Human2Sim2Robot 策略在所有任务上均显著优于基线，平均比OA Replay高55%，比BC高68%。</p>
</blockquote>
<ol start="2">
<li><strong>物体姿态轨迹奖励的消融实验</strong>：在仿真中对比了不同的奖励设计。<ul>
<li><code>Fixed Target</code>：奖励仅基于最终目标位姿。</li>
<li><code>Interpolated Target</code>：奖励基于从初始到最终位姿的插值。</li>
<li><code>Downsampled Trajectory</code>：奖励基于降采样后的关键姿态轨迹。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2504.12609v3/x7.png" alt="物体奖励消融"></p>
<blockquote>
<p><strong>图7</strong>：不同物体奖励设计的训练曲线。使用完整密集轨迹的Human2Sim2Robot获得最高奖励且收敛最快，证明了密集物体中心奖励的有效性。</p>
</blockquote>
<ol start="3">
<li><strong>预操作姿态初始化的消融实验</strong>：对比了不同的初始化策略。<ul>
<li><code>Default Initialization</code>：从远离物体的默认休息姿态开始。</li>
<li><code>Overhead Initialization</code>：手掌初始位于物体上方5厘米处。</li>
<li><code>Pre-Manipulation Far</code>：使用预操作姿态但将手掌移至远离物体20厘米处。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2504.12609v3/x8.png" alt="初始化消融"></p>
<blockquote>
<p><strong>图8</strong>：不同初始化策略的训练曲线。使用预操作姿态进行初始化的Human2Sim2Robot性能最佳，证明了其对于引导探索的重要性。</p>
</blockquote>
<ol start="4">
<li><strong>单预操作姿态的充分性实验</strong>：对比了需要完整人手轨迹的方法。<ul>
<li><code>Hand Tracking Reward</code>：在物体奖励基础上增加鼓励跟踪人手轨迹的奖励项。</li>
<li><code>Residual Policy</code>：开环回放重定向轨迹的同时，学习关节目标的残差动作。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2504.12609v3/x9.png" alt="全轨迹消融"></p>
<blockquote>
<p><strong>图9</strong>：与需要全人手轨迹方法的对比。仅使用单预操作姿态的Human2Sim2Robot与增加手部跟踪奖励的方法性能相当，且优于残差策略，证明了单预操作姿态的充分性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>Human2Sim2Robot</strong> 框架，首次实现了仅从<strong>单段</strong>人类RGB-D视频演示中学习鲁棒的、可零样本迁移到真实世界的灵巧操作策略。</li>
<li>论证了从人类演示中提取<strong>物体姿态轨迹</strong>和<strong>单预操作手部姿态</strong>这两个抽象信息的有效性，前者定义了与形态无关的密集奖励，后者提供了有利的探索初始化，共同避免了繁琐的任务特定奖励工程。</li>
<li>在抓取、非抓取操作和多步组合任务上验证了方法的通用性，在单演示情况下显著优于回放式和模仿学习基线。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法依赖于从视频中可靠地估计物体和手部姿态。对于严重遮挡或快速运动的场景，当前基于视觉的估计方法可能失效，从而影响框架的输入质量。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>本工作展示了将人类演示作为高级任务规范和探索先验（而非低级动作指令）的潜力，为利用丰富但“嘈杂”的人类视频数据提供了新范式。</li>
<li>所提的“物体中心”奖励设计思想，因其与智能体形态解耦的特性，可能推广到其他存在形态差异的学习迁移场景。</li>
<li>框架对姿态估计误差的鲁棒性（通过训练时的噪声注入和域随机化实现）表明，在仿真中主动建模和补偿感知不确定性是实现成功 sim-to-real 迁移的关键。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Human2Sim2Robot框架，旨在解决仅凭单个人类演示视频训练灵巧机器人操作策略的难题，核心是克服人机形态差异与缺乏动作标签的障碍。方法从RGB-D视频中提取物体姿态轨迹（用于定义物体中心、与形态无关的奖励）和操作前手部姿态（用于引导强化学习探索），在仿真中通过强化学习训练策略，无需人工设计奖励。实验表明，在单人类演示条件下，该方法在抓取、非抓握操作及多步骤任务上，性能比对象感知重放方法提升超过55%，比模仿学习方法提升超过68%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.12609" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>