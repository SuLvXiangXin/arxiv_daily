<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.08440" target="_blank" rel="noreferrer">2506.08440</a></span>
        <span>作者: Chen, Zengjue, Niu, Runliang, Kong, He, Wang, Qi, Xing, Qianli, Fan, Zipei</span>
        <span>日期: 2025/06/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过大规模预训练和任务特定微调，在机器人任务中展现出强大的跨场景泛化能力。然而，其训练范式主要依赖于人工收集的成功演示，当遇到分布外场景或执行偏差时，难以适应复杂环境。强化学习通过主动试错机制提供了一个闭环优化框架，但在长周期机器人任务中，存在奖励稀疏、方差高和优化不稳定的问题。本文针对VLA模型无法从失败中学习、以及RL在机器人长周期任务中训练效率低下的痛点，提出了一种新的在线RL微调框架。其核心思路是：利用大语言模型自动构建密集奖励函数，并通过一种新颖的轨迹级与步级分组策略来融合全局与局部优化信号，从而稳定、高效地提升VLA策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>TGRPO的整体框架是一个用于VLA模型在线强化学习微调的流程。给定自然语言指令和多模态观测，基础VLA模型产生动作令牌来控制机器人。在多个并行环境中采样轨迹，利用LLM设计的多阶段奖励函数进行评估，随后对轨迹进行步级和轨迹级的分组与优势估计，融合后用于策略更新。</p>
<p><img src="https://arxiv.org/html/2506.08440v3/icra_tgrpo/Image/fig2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：TGRPO方法总览。给定指令和观测，OpenVLA模型输出动作。在并行环境中采样的轨迹由多阶段奖励函数评估，随后进行步级和轨迹级分组以计算相对优势，融合后用于策略更新。</p>
</blockquote>
<p><strong>核心模块一：多阶段奖励设计</strong>。为了解决稀疏奖励和长周期任务中信用分配困难的问题，TGRPO利用大语言模型自动从任务描述中分解出子阶段，并基于关键物体位置和来自成功演示的末端执行器关键位姿，构建密集的、阶段化的奖励函数。奖励函数形式化为 $R_t = f_1(P_{object}(t), P^k_{pose}) + f_2(P^k_{pose}, s_t)$，其中 $f_1$ 基于物体信息，$f_2$ 基于关键位姿的欧氏距离提供塑形奖励，以加速任务完成。</p>
<p><strong>核心模块二：轨迹级分组相对策略优化</strong>。这是方法的核心创新点。假设采样了N条长度为M的轨迹。首先进行<strong>步级分组</strong>：将所有轨迹在同一时间步t的奖励 $R_{i,t}$ 分为一组，计算该步的步级相对优势 $A_{i,t}$，即该步奖励减去组内均值后除以组内标准差。接着进行<strong>轨迹级分组</strong>：将整条轨迹的总奖励 $R_i$ 分为一组，计算轨迹级相对优势 $A_i$，计算方法类似。最终，每个轨迹中每一步的优势是两者的加权融合：$Adv_{i,t} = \alpha_1 A_{i,t} + \alpha_2 A_i$（实验中设 $\alpha_1=0.3, \alpha_2=0.7$）。</p>
<p><strong>优化目标</strong>。TGRPO的优化目标借鉴了带裁剪的替代目标形式，并融入了上述双级优势以及一个KL散度正则项（参考策略为原始VLA模型 $\pi_{ref}$），具体形式如论文中式(6)所示。该方法无需训练额外的价值函数网络，通过分组内的相对比较来降低方差，并同时融合了反映局部动作质量的步级信号和反映全局任务进度的轨迹级信号。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在LIBERO机器人仿真基准上进行，该基准包含Spatial、Object、Goal、Long四个任务套件，各10个任务，分别测试空间、物体、目标和长周期复杂性泛化。以OpenVLA为基座模型，使用LoRA进行微调。在训练和评估时，每次在4个并行环境中运行一个任务，并在50个测试回合中报告平均成功率。</p>
<p><strong>基线方法</strong>：对比方法包括：非后训练的独立模型Octo；基于微调的方法：监督微调、DPO以及近期RL微调方法GRAPE。</p>
<p><strong>关键定量结果</strong>：<br><img src="https://arxiv.org/html/2506.08440v3/icra_tgrpo/Image/fig3.png" alt="结果对比图"></p>
<blockquote>
<p><strong>图3</strong>：不同方法在LIBERO各套件上的成功率。TGRPO在所有任务类别上均优于基线方法，取得了最高的平均成功率80.7%。</p>
</blockquote>
<p>如表1和图3所示，TGRPO在四个套件上的平均成功率达到80.7%，比监督微调高出4.2%，也超过了其他RL微调方法。尤其在最具挑战性的LIBERO-Long长周期任务套件上，TGRPO比SFT高出8.1%，证明了其在复杂长周期场景中的有效性。</p>
<p><strong>消融实验</strong>：<br>表2展示了在LIBERO-Object套件上的消融结果。完整的TGRPO获得92.2%的成功率。移除轨迹级优势后，性能降至80.2%；移除步级优势后，性能降至86.8%。这证明了双级优势设计的必要性：轨迹级优势对于捕获全局任务进度至关重要，而步级优势对于长周期任务中的细粒度信用分配同样关键。</p>
<p><strong>定性案例与超参数分析</strong>：<br><img src="https://arxiv.org/html/2506.08440v3/icra_tgrpo/Image/case.png" alt="案例图"></p>
<blockquote>
<p><strong>图4</strong>：LIBERO-Long任务“将字母汤和番茄酱都放入篮子”的成功轨迹示例。多阶段奖励函数基于关键物体和机器人状态分配步级奖励。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.08440v3/icra_tgrpo/Image/effect_comparison.png" alt="超参数分析图"></p>
<blockquote>
<p><strong>图5</strong>：优势权重 ($\alpha_1, \alpha_2$) 和分组大小 $N$ 的超参数分析。最佳性能在权重为(0.3, 0.7)、分组大小 $N=4$ 时取得，成功率81.0%。</p>
</blockquote>
<p>图5表明，优势权重需要平衡，过高的步级权重（$\alpha_1$）或过低的步级权重都会损害性能。同时，分组大小 $N=4$ 在性能与效率间取得了最佳平衡，过小($N=2$)会导致估计不稳定。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个用于VLA模型的在线RL微调框架TGRPO，使其能够通过与环境的交互从失败中学习，适应动态任务场景。2) 设计了新颖的轨迹级与步级分组相对策略优化算法，无需价值网络，通过融合全局与局部优化信号，提升了长周期任务的信用分配和训练稳定性。3) 展示了LLM自动生成密集奖励与分组优化策略的协同作用，有效提升了VLA模型在复杂机器人任务中的RL训练效果。</p>
<p><strong>局限性</strong>：论文自身提到的局限性在于，当前实验完全在仿真环境中进行，未来需要扩展到真实世界以及多任务场景。</p>
<p><strong>启示</strong>：这项工作为VLA模型的适应性微调提供了一条高效、可扩展的路径，减少了对人工奖励工程和大量离线数据的依赖。其核心的分组相对优势估计思想，为解决长周期RL任务中的高方差和信用分配问题提供了新的思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出TGRPO方法，旨在解决视觉-语言-动作模型在分布外场景中适应性差，以及强化学习在长时程机器人任务中奖励稀疏、方差高、优化不稳定的问题。该方法的核心是轨迹级组相对策略优化：利用大语言模型自动生成密集奖励函数以提供细粒度反馈；通过并行采样与归一化多轨迹的组策略降低方差；融合轨迹级与步级优势估计，无需价值网络。在LIBERO基准测试中，TGRPO平均成功率达80.7%，较监督微调提升4.2%，优于其他基于强化学习的后训练方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.08440" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>