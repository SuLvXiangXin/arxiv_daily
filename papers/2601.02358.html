<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VINO: A Unified Visual Generator with Interleaved OmniModal Context - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>VINO: A Unified Visual Generator with Interleaved OmniModal Context</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.02358" target="_blank" rel="noreferrer">2601.02358</a></span>
        <span>作者: Chen, Junyi, He, Tong, Fu, Zhoujie, Wan, Pengfei, Gai, Kun, Ye, Weicai</span>
        <span>日期: 2026/01/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前视觉生成领域的主流方法高度碎片化：文本到图像、文本到视频以及各类视觉编辑任务通常由独立、任务特定的模型或模块完成。这种分离的范式不仅导致开发和部署的复杂性，也使得模型难以协同处理来自文本、图像、视频等多种模态的混合输入。当多种模态的控制信号同时提供时，现有模型缺乏有效机制来可靠地解耦和优先处理这些信号，容易引发语义冲突或不一致的条件效应。</p>
<p>本文针对构建统一视觉生成框架的核心挑战，提出了“交错全模态上下文”的新视角。其核心思路是：耦合一个视觉语言模型（VLM）与一个多模态扩散Transformer（MMDiT），将所有模态的输入（文本、参考图像、参考视频）编码为交错排列的条件令牌，并以此指导一个共享的扩散主干网络，从而在一个单一框架内实现图像与视频的生成与编辑。</p>
<h2 id="方法详解">方法详解</h2>
<p>VINO的整体框架是一个统一的扩散模型，其输入为交错的系统提示、用户指令、参考图像/视频以及可学习令牌，输出为生成或编辑后的目标图像/视频。其核心是VLM与MMDiT的耦合：VLM作为前端编码器，将多模态输入处理为统一的语义表示；MMDiT作为去噪主干，在接收VLM语义令牌的同时，还注入参考视觉内容的VAE潜在表示，以保留细节。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：VINO管道总览。系统提示、指令、参考图像/视频和可学习令牌被编码成交错的全模态上下文。一个冻结的VLM处理文本指令和视觉参考，生成多模态嵌入，并与可学习令牌（紫色）和特殊边界令牌（<code>&lt;|vision_start|&gt;</code>和<code>&lt;|vision_end|&gt;</code>）结合。这些交错表示与参考的VAE潜在一起馈入MMDiT块，进行条件去噪，实现统一的生成与编辑。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>多模态条件编码</strong>：使用一个冻结的VLM（Qwen3VL-4B-Instruction）作为统一编码器。系统提示根据输入模态的存在和数量动态构建。当有视觉输入时，它们被排序（先图像后视频）并赋予唯一标识符（如<code>Image 1</code>），用户可在文本指令中引用这些标识符以实现复杂控制。此外，在提示末尾附加一组<strong>可学习令牌</strong>，它们通过因果掩码处理，旨在提取跨模态特征到共享空间。</li>
<li><strong>交错全模态上下文注入</strong>：为解决VLM压缩视觉信息导致细节丢失的问题，除了VLM嵌入，还将所有视觉模态的VAE潜在表示补充注入MMDiT。关键在于<strong>令牌边界机制</strong>：将VLM输出的<code>&lt;|vision_start|&gt;</code>和<code>&lt;|vision_end|&gt;</code>令牌的嵌入投影后，作为边界标记插入到VAE潜在序列中，用以界定每个参考视觉块。这为注意力机制提供了强位置线索，使其能正确区分和对齐不同的视觉条件源。</li>
</ol>
<p><img src="https://..." alt="3D RoPE策略"></p>
<blockquote>
<p><strong>图5</strong>：VINO中VAE分支的3D RoPE策略。沿时间轴应用统一的3D RoPE时间表，将不同的视觉模态（单参考图像、多帧参考视频、噪声目标潜在）交错放置在共享的RoPE时间线上，并用从VLM输出投影而来的特殊令牌分隔。</p>
</blockquote>
<ol start="3">
<li><strong>渐进式多任务训练策略</strong>：为避免灾难性遗忘并平滑适应新任务，采用三阶段渐进训练。<ul>
<li><strong>第一阶段（对齐）</strong>：从预训练的文本到视频模型（HunyuanVideo）开始，仅训练一个两层MLP连接器，将VLM的输出空间与模型原始文本编码器的空间对齐。</li>
<li><strong>第二阶段（适应短指令）</strong>：使用长、短提示的混合数据训练，开始更新MMDiT参数，使模型适应编辑任务常用的短指令分布。</li>
<li><strong>第三阶段（多任务混合训练）</strong>：进行全面的多任务混合训练，数据包括图像重建、文本到图像/视频、图像到视频、图像编辑、参考视频编辑、指令视频编辑等。</li>
</ul>
</li>
</ol>
<p><img src="https://..." alt="训练数据分布"></p>
<blockquote>
<p><strong>图6</strong>：各阶段训练数据分布。此渐进策略将基础模型从纯文本到视频生成器逐步转变为能处理多任务的统一视觉生成器。</p>
</blockquote>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>架构统一</strong>：单一扩散主干（MMDiT）处理多模态条件，无需任务特定模块。</li>
<li><strong>可学习令牌</strong>：提供高层指令与底层扩散特征间的灵活接口，改善多模态对齐并稳定优化。</li>
<li><strong>令牌边界机制</strong>：在VAE潜在流中重用VLM特殊令牌，确保同一参考的语义和潜在表示在模型中具有一致的身份标识，减少身份混淆和属性泄漏。</li>
<li><strong>渐进训练</strong>：有效将预训练视频模型升级为多任务生成器，同时保留其原始生成能力。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：训练数据混合了大规模开源图像/视频集合以及来自开源模型的高质量蒸馏数据。使用动态分辨率分桶策略以保持原始宽高比并平衡计算负载。基础模型为HunyuanVideo（视频生成）和Qwen3VL-4B-Instruction（VLM编码器）。评估涵盖多个基准测试。</p>
<p><strong>对比方法</strong>：包括开源和闭源的图像模型（如SD3、FLUX.1-dev）、视频模型（如Sora、Veo3、HunyuanVideo）以及专门的编辑模型（如Bagel、Step1x-EditV1.1、Ditto、Lucy-Edit）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>视觉生成能力</strong>：在Geneval（图像）和VBench（视频）基准上，VINO在仅使用少量标准T2I/T2V数据的情况下，性能与基础模型HunyuanVideo高度可比，证明了渐进训练有效避免了灾难性遗忘。</li>
</ol>
<p><img src="https://..." alt="Geneval结果"></p>
<blockquote>
<p><strong>表2</strong>：Geneval图像生成评估结果。VINO保持了与基础模型相当的文本到图像能力。</p>
</blockquote>
<p><img src="https://..." alt="VBench结果"></p>
<blockquote>
<p><strong>表3</strong>：VBench视频生成评估结果。VINO保持了强大的文本到视频生成能力，且得益于更强的VLM，语义得分有所提升。</p>
</blockquote>
<ol start="2">
<li><strong>参考驱动生成</strong>：在OpenS2V（主题到视频生成）基准上，VINO表现出色，总分超越多个闭源和开源系统，展示了其有效利用参考进行定制化生成的能力。</li>
</ol>
<p><img src="https://..." alt="OpenS2V结果"></p>
<blockquote>
<p><strong>表4</strong>：OpenS2V开放域主题到视频生成结果。VINO在总分上领先。</p>
</blockquote>
<ol start="3">
<li><strong>视觉编辑能力</strong>：<ul>
<li><strong>图像编辑</strong>：在ImgEdit和GEdit基准上，VINO在第三阶段仅训练1k步时就已超越多数开源基线，完成全部训练后性能更优，展示了其快速适应编辑任务的能力。</li>
</ul>
</li>
</ol>
<p><img src="https://..." alt="ImgEdit结果"></p>
<blockquote>
<p><strong>表5</strong>：ImgEdit图像编辑基准结果。VINO在仅1k步后即表现优异，完整训练后进一步提升。</p>
</blockquote>
<p><img src="https://..." alt="GEdit结果"></p>
<blockquote>
<p><strong>表6</strong>：GEdit图像编辑基准结果（排除文本更改子任务）。VINO在主题替换和风格更改任务上表现良好。</p>
</blockquote>
<pre><code>*   **视频编辑**：在OpenVE-Bench上，使用Gemini 2.5 Pro和Qwen3VL作为评判，VINO在总分和几乎所有子类别上都显著优于所有对比的开源方法。
</code></pre>
<p><img src="https://..." alt="OpenVE-Bench结果（Gemini）"></p>
<blockquote>
<p><strong>表7</strong>：使用Gemini 2.5 Pro评估的OpenVE-Bench视频编辑结果。VINO全面领先。</p>
</blockquote>
<p><img src="https://..." alt="OpenVE-Bench结果（Qwen）"></p>
<blockquote>
<p><strong>表8</strong>：使用Qwen3VL评估的OpenVE-Bench视频编辑结果。VINO同样表现最佳。</p>
</blockquote>
<p><img src="https://..." alt="视频编辑定性对比"></p>
<blockquote>
<p><strong>图7</strong>：与Ditto和Lucy-Edit的定性视频编辑对比。给定相同输入视频和编辑指令，VINO在指令遵循和视觉质量上明显更优。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：<ul>
<li><strong>可学习令牌</strong>：移除可学习令牌会导致训练不稳定（梯度噪声更大、损失曲线波动），且编辑任务（如物体移除/替换）的准确性下降。</li>
</ul>
</li>
</ol>
<p><img src="https://..." alt="可学习令牌消融"></p>
<blockquote>
<p><strong>图8</strong>：可学习令牌消融研究。引入可学习令牌（紫色）显著稳定了优化过程，梯度范数更低，损失轨迹更平滑。</p>
</blockquote>
<p><img src="https://..." alt="编辑任务消融"></p>
<blockquote>
<p><strong>图9</strong>：（图中未直接显示，但正文提及）在编辑任务中，配备可学习令牌的模型能进行更精确的对象移除和替换。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>VINO</strong>，一个基于扩散的统一架构，通过耦合VLM与MMDiT，在单一框架内处理图像和视频的生成与编辑。</li>
<li>引入了<strong>可学习查询令牌</strong>，经验证明其能改善多模态条件对齐并稳定优化。</li>
<li>设计了<strong>令牌边界机制</strong>，在VAE潜在流中重用VLM特殊令牌，以保持参考视觉内容在语义和潜在表示上的一致性，减少复杂多模态场景中的身份混淆。</li>
<li>提出了一种<strong>渐进训练策略</strong>，能够将预训练视频模型有效升级为多任务视觉生成器，同时保留其原始生成能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确列出局限性，但隐含的挑战可能包括训练此类统一模型所需的大规模、高质量多任务数据，以及相应的计算资源。</p>
<p><strong>启示</strong>：VINO展示了一条通向可扩展统一视觉生成的实用路径。其“交错上下文计算”范式表明，将不同模态的信息在序列层面进行结构化交织和关联，可以作为构建通用视觉创建系统的基础。这为未来开发能够无缝理解和执行跨模态、多任务视觉创作指令的AI系统提供了重要思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VINO模型，旨在解决当前视觉生成系统分散化（如图像生成、视频生成、编辑任务需独立模型）的问题。其关键技术是结合视觉语言模型（VLM）与多模态扩散变换器（MMDiT），将文本、图像、视频等多模态输入编码为交错的条件标记，以统一框架指导扩散过程，支持多参考、长指令跟随及身份一致性保持。实验表明，VINO在多种生成与编辑基准上实现了高质量的视觉输出、准确的指令跟随、更好的参考与属性保持能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.02358" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>