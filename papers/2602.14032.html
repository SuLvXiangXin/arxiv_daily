<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.14032" target="_blank" rel="noreferrer">2602.14032</a></span>
        <span>作者: Jian Tang Team</span>
        <span>日期: 2026-02-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人学习在真实非结构化环境中部署时，其观察分布的偏移（如复杂的背景变化、剧烈的光照变化以及任务无关干扰物的出现）会导致策略性能显著下降。当前主流方法主要分为两种范式：一是依赖大规模真实世界数据收集进行预训练，但这成本高昂且耗时；二是利用生成模型进行语义数据增强，这类方法通常依赖于一个不切实际的假设，即上游的通用物体检测或分割模型能够在复杂的机器人操作场景中完美地隔离出任务相关实体（如机械臂、操作物体）。然而，论文通过构建的RoboAug-D数据集评估发现，即使是GroundingDINO、LLMDet等先进视觉基础模型，在机器人视角下的检测性能也常不理想（例如“Bun”类别的<a href="mailto:&#109;&#65;&#x50;&#x40;&#x30;&#x2e;&#x35;">&#109;&#65;&#x50;&#x40;&#x30;&#x2e;&#x35;</a>低于0.10）。不精确的提取会导致生成过程中关键物体被背景纹理覆盖，从而使策略学到错误行为。</p>
<p>本文针对现有语义增强方法对上游视觉模型性能过度依赖这一具体痛点，提出了一种新的视角：通过仅需单帧标注的鲁棒任务区域提取，并结合区域对比学习，来最小化对大规模预训练和完美视觉识别的依赖。其核心思路是：利用一次手动标注，通过训练无关的单次区域匹配与跟踪策略，为所有轨迹帧生成高质量的任务区域掩码，并基于此进行背景生成与前景合成以增强数据，同时引入区域对比损失引导策略关注任务相关区域特征。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboAug整体框架包含三个阶段：任务相关区域提取、语义数据增强和区域对比策略学习。其目标是利用单任务模仿学习数据，学习一个能泛化到新场景的视动策略。</p>
<p><img src="https://arxiv.org/html/2602.14032v1/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图1</strong>：RoboAug整体框架。包含三个阶段：（1）任务相关区域提取：基于单帧标注，通过单次区域匹配和语义掩码传播为所有轨迹生成像素级掩码。（2）语义数据增强：利用掩码将任务相关前景与生成的新背景图像合成，以扩充数据。（3）区域对比策略学习：在策略训练中引入区域对比损失，使编码器学习对任务相关区域敏感的特征表示。</p>
</blockquote>
<p><strong>1. 任务相关区域提取</strong><br>此阶段目标是从示教轨迹中获取像素级任务区域掩码 (M_{\text{task}})，而无需逐帧标注或重新训练检测器。其流程分为两步：</p>
<ul>
<li><strong>单次区域匹配</strong>：选取一个轨迹的首帧作为参考帧 (I_{\text{ref}})，并手动标注其任务相关区域（如操作物体）的边界框 ( \mathcal{B}^{\text{ref}} )。将框内区域裁剪后，通过DINOv2编码为参考嵌入 ( \mathcal{E}^{\text{ref}} )。对于其他轨迹的锚帧（首帧）(I_{\text{anc}})，使用GroundingDINO生成候选框提案 ( \mathcal{B}^{\text{can}} )。计算每个候选框特征嵌入 ( e_j^{\text{can}} ) 与所有参考嵌入的余弦相似度，通过公式 ( \hat{c}<em>j = \operatorname*{argmax}</em>{i} \text{sim}(e_j^{\text{can}}, e_i^{\text{ref}}) ) 确定其类别。此过程实现了训练无关的跨轨迹语义对齐。</li>
<li><strong>语义掩码传播</strong>：获得锚帧的边界框及类别后，利用SAM-2（集成了分割与跟踪）将稀疏框先验转化为稠密像素级掩码 (M_{\text{task}})，并在整个轨迹的时间序列上传播，确保时空一致性。最终，整个数据集的每一帧都获得了对应的任务区域语义掩码。</li>
</ul>
<p><strong>2. 语义数据增强</strong><br>在获得精确掩码后，本阶段旨在多样化训练数据的环境背景，同时严格保持任务关键区域的结构完整性。为避免修复（inpainting）可能引入的伪影和几何扭曲，RoboAug选择生成完整的背景图像。具体而言，利用大语言模型（如ChatGPT）生成多样化的背景描述提示词，驱动Stable Diffusion v3模型合成全新的背景图像 (I_{\text{bg}})。随后，通过掩码将原始图像中的前景与生成背景进行线性插值合成：( I_{\text{aug}} = M_{\text{task}} \odot I + (1 - M_{\text{task}}) \odot I_{\text{bg}} )。通过为每张图像随机采样提示词，可以大规模扩充数据集，得到最终增强数据集 ( \mathcal{D}<em>{\text{fnl}} = \mathcal{D}</em>{e} \cup \mathcal{D}_{\text{aug}} )。</p>
<p><strong>3. 区域对比策略学习</strong><br>为了利用掩码提供的语义信息引导策略学习，本文提出了区域对比学习目标。对于批次中的图像 (I) 及其对应的物体掩码 (M_{\text{task}, c})，首先提取物体中心图像 (I_{\text{obj}} = M_{\text{task}, c} \odot I)。由于 (I_{\text{obj}}) 可能包含大量零值（黑色）区域，直接编码的特征可能包含非信息信号。因此，引入一个空间自注意力机制来提炼特征：利用完整图像特征 (z = E(I)) 通过可学习的注意力模块 (A(\cdot)) 生成注意力权重 (a_{\text{att}} = \text{sigmoid}(A(z) \odot z))，然后将其应用于物体特征得到 (z_{\text{att}} = a_{\text{att}} \odot z_{\text{obj}})。最后，采用监督对比损失进行优化：<br>[<br>\mathcal{L}<em>{\text{RC}} = \sum</em>{i \in \mathcal{B}<em>{\text{obj}}} \frac{-1}{|P(i)|} \sum</em>{p \in P(i)} \log \frac{\exp(z_{\text{att},i} \cdot z_{\text{att},p} / d)}{\sum_{j \in S(i)} \exp(z_{\text{att},i} \cdot z_{\text{att},j} / d)}<br>]<br>其中 (P(i)) 是与样本 (i) 类别相同的正样本集，(S(i)) 是批次中除 (i) 外的所有样本。该损失促使同类物体的特征聚集，异类特征分离，从而增强策略对任务相关视觉特征的鲁棒性。</p>
<p><strong>创新点</strong>：与现有方法相比，RoboAug的创新主要体现在：（1）提出了一种仅需单帧标注、训练无关的鲁棒任务区域提取流程，降低了对上游通用检测器性能的依赖；（2）采用“生成完整背景+前景合成”的增强策略，避免了修复带来的失真；（3）引入了区域对比损失，将语义掩码信息作为监督信号融入策略训练，提升了特征表达对任务相关区域的关注度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：实验在三个机器人平台上进行：单臂UR-5e、双臂AgileX Cobot Magic V2.0和人形机器人Tien Kung 2.0。通过人工遥操作（HACTS）为每个平台收集了5个任务各50条专家轨迹。评估泛化能力时，考虑了背景、光照和干扰物三种因素的变化。此外，论文还构建并开源了大规模物体检测数据集RoboAug-D，包含33个任务、73,749个关键帧和366,835个标注框。</p>
<p><strong>对比方法</strong>：对比的基线方法包括：无数据增强的ACT策略、基于纹理替换的RoboEngine-T、基于生成式增强的RoboEngine-G和GenAug。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>目标检测基准测试</strong>：在RoboAug-D数据集上评估了视觉基础模型的零样本检测能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.14032v1/x2.png" alt="RoboAug-D数据集上mAP@0.5对比"></p>
<blockquote>
<p><strong>图2</strong>：在RoboAug-D数据集上代表性物体类别的<a href="mailto:&#x6d;&#x41;&#x50;&#x40;&#48;&#46;&#x35;">&#x6d;&#x41;&#x50;&#x40;&#48;&#46;&#x35;</a>对比。RoboAug使用的单次区域匹配策略显著优于GroundingDINO和LLMDet等通用基础模型，例如在“Bun”类别上达到0.87，而基线低于0.10。</p>
</blockquote>
<ol start="2">
<li><strong>组合泛化评估</strong>：在最具挑战性的三因素（3种未见背景、4种光照、3种干扰物）组合变化设置下进行评估。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.14032v1/x6.png" alt="三因素组合变化下的成功率对比"></p>
<blockquote>
<p><strong>图6</strong>：在UR-5e、AgileX和Tien Kung 2.0机器人上，面对背景、光照、干扰物的组合变化，RoboAug相比所有基线方法取得了最高的平均成功率。</p>
</blockquote>
<p>根据论文表II数据，RoboAug在三个机器人上的平均成功率分别达到0.47（UR-5e）、0.60（AgileX）和0.67（Tien Kung 2.0），显著优于最佳基线GenAug（0.31, 0.34, 0.42）。相比无增强的ACT基线（0.09, 0.16, 0.19），成功率提升幅度巨大。</p>
<ol start="3">
<li><strong>单因素泛化评估</strong>：针对单一干扰因素进行深入测试。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.14032v1/x5.png" alt="背景泛化结果"></p>
<blockquote>
<p><strong>图5</strong>：在UR-PutCornPot任务上，对170种未见背景的泛化性能。RoboAug在所有方法中表现最优，且随着背景数量增加，其性能下降最为缓慢，显示出最强的泛化能力。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：论文对区域对比学习（RCL）和背景生成方式进行了消融研究。结果表明，移除RCL会导致在所有三个机器人上的性能平均下降19.6%。同时，使用“生成完整背景+合成”的策略优于依赖修复（inpainting）的策略，后者会导致性能平均下降11.2%。这验证了区域对比损失和所提出的数据增强策略各自的关键贡献。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了RoboAug框架，一种结合了精确任务区域提取、语义数据增强和区域对比学习的生成式数据增强方法，仅需单帧标注即可规模化生成多样化训练数据，显著降低了对大规模预训练数据和完美上游视觉模型的依赖。</li>
<li>设计了一种训练无关的单次区域匹配策略，结合区域对比损失，共同提升了视觉提取的精确性和策略学习特征的表达能力。</li>
<li>通过超过3.5万次真实世界实验验证了方法的有效性，在组合环境变化下显著提升了策略的泛化性能，并开源了RoboAug-D检测数据集和多任务操作数据集以促进相关研究。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：（1）数据增强的质量依赖于预训练生成模型（如Stable Diffusion）的能力，其生成内容的真实性和多样性存在上限；（2）方法假设任务相关物体在参考帧中清晰可见且可标注，对于严重遮挡或需要多视角理解的复杂任务可能仍需更多标注。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>降低标注依赖的进一步探索</strong>：RoboAug的单帧标注范式为极低标注成本下的数据增强提供了新思路，未来可探索完全无监督或自监督的区域发现方法。</li>
<li><strong>多模态与动态场景增强</strong>：当前工作主要关注静态视觉背景的增强。未来可将该框架扩展至包含动态干扰物、多视角观察或多模态（如触觉、力觉）信息的增强，以应对更复杂的现实场景。</li>
<li><strong>与基础模型结合</strong>：RoboAug的增强数据可用于微调或预训练更通用的机器人基础模型，其区域对比学习的思想也可融入更大规模的多任务策略训练中，提升模型对任务本质的感知能力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RoboAug，旨在解决机器人操作在多样未见场景中泛化能力不足的核心问题。该方法仅需单张图像的边界框注释，利用预训练生成模型进行精确语义数据增强，并结合即插即用的区域对比损失以聚焦任务相关区域。在UR-5e、AgileX和Tien Kung 2.0机器人上的大规模实验显示，RoboAug显著提升成功率：从0.09到0.47、0.16到0.60、0.19到0.67，优于现有数据增强基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.14032" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>