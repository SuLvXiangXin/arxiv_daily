<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.05199" target="_blank" rel="noreferrer">2511.05199</a></span>
        <span>作者: Feifei Feng Team</span>
        <span>日期: 2025-11-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作学习的主流方法依赖于在大量机器人数据上训练深度神经网络。然而，与自然语言处理等领域的基础模型相比，机器人领域在训练数据规模和模型大小方面的可扩展性存在不足。这促使研究者探索如何让机器人利用有限的数据和模型规模进行高效学习。人类在学习新任务（如组装家具）时，常通过观看演示视频来模仿，这为机器人学习提供了新视角。本文针对机器人数据稀缺和泛化能力有限的痛点，提出利用丰富的人类视频作为外部知识库来增强机器人策略学习。核心思路是：从人类视频中提取对机器人控制有益的“中层信息”（如物体可供性、手部轨迹），并通过基于语言指令的视频检索，将这些知识动态整合到策略网络中，以提升学习效率和任务泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的“从视频中检索学习”框架包含三个核心组件：视频库构建、视频检索器和策略生成器。整体流程为：给定机器人任务的语言指令，视频检索器从预先构建的视频库中找出相关的人类演示视频及其提取的中层信息；策略生成器将当前机器人观测与检索到的视频信息融合，最终输出机器人动作。</p>
<p><img src="https://arxiv.org/html/2511.05199v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：RfV方法整体框架。左上角为视频库构建过程，从人类视频中提取物体可供性掩码和手部运动轨迹。右上角为视频检索器，基于语言指令与视频描述文本的相似度检索最相关的视频。底部为策略生成器，将检索到的视频信息（文本、视觉、掩码、轨迹令牌）与机器人观测融合，通过Transformer网络生成动作。</p>
</blockquote>
<p><strong>1. 构建视频库与提取中层信息</strong>：视频库主要基于Ego4D数据集，并过滤掉户外场景以匹配常见的室内机器人操作环境。关键创新在于从原始人类视频中提取对机器人控制更直接有用的中层信息，包括：</p>
<ul>
<li><strong>物体可供性掩码</strong>：定位人手与物体的接触点/可操作区域。流程为：使用GroundingDino检测手部位置，用GPT-4V识别手中物体名称，最后用Segment Anything (SAM)根据文本提示和手部周边像素生成精确的物体掩码。</li>
<li><strong>手部运动轨迹</strong>：描绘抓取后手的运动路径。通过计算手部检测框在所有帧中的质心得到原始轨迹，并使用样条插值进行平滑处理以消除相机抖动和检测不准确的影响。</li>
</ul>
<p><strong>2. 视频检索器</strong>：该模块负责根据机器人任务指令 <code>i_robot</code>，从视频库中检索出相关的演示视频。采用双编码器架构，使用CLIP的文本编码器分别编码查询指令 <code>i_robot</code> 和视频库中的视频描述文本 <code>i_video</code>，通过计算特征向量的内积得到相似度分数。通过最大内积搜索，为每个机器人视角检索出最相关的k个视频（包括视频帧、语言描述、可供性掩码和运动轨迹）。实验表明检索3个视频效果最佳。</p>
<p><strong>3. 策略生成器</strong>：该模块负责将检索到的知识整合到策略网络中。首先，将检索到的多模态信息编码为统一的令牌序列：</p>
<ul>
<li><strong>文本令牌</strong>：复用检索器中的CLIP文本编码特征。</li>
<li><strong>视觉令牌</strong>：使用预训练的ViT-Base提取视频帧特征，并采用ToMe方法合并相邻帧的相似令牌，将令牌数量减少90%以提升效率。</li>
<li><strong>掩码与轨迹令牌</strong>：分别通过多层感知机编码器处理。<br>这些令牌与一个可学习的“状态”令牌拼接，并使用绝对位置编码和“sep”令牌来区分不同视频的信息。策略网络主体采用Action Chunking Transformer的设计，通过行为克隆进行训练。<strong>核心创新整合机制</strong>是使用交叉注意力：将检索视频的特征投影为Query和Key，将机器人观测的特征作为Value，从而使策略网络能够关注并利用检索视频中的相关知识。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>模拟实验</strong>：在MetaWorld基准上进行评估，包含Easy（28个）、Medium（11个）、Hard（6个）和Very Hard（6个）四类任务。使用30条演示数据进行少样本学习。</li>
<li><strong>真实机器人实验</strong>：使用Franka Emika机械臂和两个ZED相机，在8个不同的操作任务上进行评估，每个任务使用50条演示数据。</li>
<li><strong>对比方法</strong>：包括VINN、BeT、ACT、Diffusion Policy等模仿学习方法，以及R3M、VIMA、RT-1、Octo、OpenVLA等机器人基础模型。</li>
</ul>
<p><strong>关键定量结果</strong>：</p>
<ul>
<li><p><strong>模拟实验</strong>：如表I所示，本文方法在MetaWorld的所有难度级别上均显著优于基线方法。在Medium任务上，成功率（54.5%）分别比Diffusion Policy（35.4%）和ACT（15.4%）高出19.1%和39.1%。在Hard和Very Hard任务上，也以较大优势领先。</p>
<blockquote>
<p><strong>表I</strong>：MetaWorld模拟基准实验结果。本文方法（Ours）在所有难度级别上均取得最高成功率。</p>
</blockquote>
</li>
<li><p><strong>真实机器人实验</strong>：如表II所示，本文方法（RfV）在8个任务上的平均成功率达到73.1%，优于所有对比方法，包括在大规模机器人数据上预训练的Octo和OpenVLA。尤其在操作刚性物体（PlaceBall, PlaceCan）和长视野任务（CleanTable, PackCube）上优势明显。</p>
<blockquote>
<p><strong>表II</strong>：真实机器人八项任务实验结果。RfV方法平均成功率最高（73.1%）。</p>
</blockquote>
</li>
</ul>
<p><strong>消融分析与泛化研究</strong>：</p>
<ul>
<li><p><strong>中层信息的重要性</strong>：如表III所示，移除手部运动轨迹或物体可供性掩码都会导致性能显著下降，平均成功率分别降至58.8%和45.6%，证明了二者对成功操作至关重要。</p>
<blockquote>
<p><strong>表III</strong>：消融实验。移除任一中层信息都会导致性能显著下降。</p>
</blockquote>
</li>
<li><p><strong>检索视频数量的影响</strong>：如表IV所示，检索3个视频时性能最佳（平均成功率73.1%），检索更多视频性能略有下降。</p>
<blockquote>
<p><strong>表IV</strong>：检索视频数量消融实验。检索3个视频时效果最好。</p>
</blockquote>
</li>
<li><p><strong>泛化能力</strong>：如图4所示，本文方法在空间、干扰物和外观泛化上表现出色。<br>  <img src="https://arxiv.org/html/2511.05199v1/x4.png" alt="泛化实验"></p>
<blockquote>
<p><strong>图4</strong>：左：空间泛化实验设置，物体被随机放置。右：外观泛化，使用训练中未出现过的彩色立方体。本文方法在这些泛化场景中表现优异。<br>  具体而言：1) <strong>空间泛化</strong>：在物体位置随机摆放的测试中，RfV成功率为80%（4/5），而移除检索模块后降为0%。2) <strong>干扰物泛化</strong>：在PlaceBall任务中引入未见过的干扰物，RfV保持80%成功率，无检索模块时降为0%。3) <strong>外观泛化</strong>：当指令要求操作特定颜色（训练中未出现）的立方体时，RfV成功率为100%（5/5），而无检索模块的方法则失败。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了“从视频中检索学习”的新框架，使机器人能够利用海量人类视频作为外部知识库来增强策略学习。</li>
<li>定义了从人类视频中提取“中层信息”（物体可供性掩码、手部运动轨迹）的流程，这些信息对机器人低层控制更为直接有效。</li>
<li>在模拟和真实机器人实验中验证了方法的有效性、数据高效性和卓越的泛化能力（空间、干扰物、外观）。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前方法为每个相机视图独立检索视频，一个可能的改进方向是利用视频生成技术基于单一视图生成额外视角的视频。此外，方法性能依赖于视频库的质量和检索的准确性。</p>
<p><strong>启示</strong>：这项工作展示了检索增强生成（RAG）范式在机器人学习中的巨大潜力。它指明了一个方向：即不单纯追求扩大机器人本体模型的参数规模，而是通过构建和利用结构化的外部知识记忆（如带有丰富标注的视频库），来经济高效地提升机器人的认知与操作能力。未来研究可探索更高质量的视频库构建、更精准的跨模态检索，以及将此类方法与基础模型进行更深入的结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Retrieving-from-Video (RfV)方法，旨在解决机器人如何利用海量人类演示视频来学习并泛化操作任务的核心问题。关键技术是构建人类任务视频库，并提取物体可供性掩码和手部运动轨迹等中级信息。系统包含视频检索器与策略生成器两个组件，通过检索并融合相关知识来生成策略。实验表明，该方法在模拟和真实环境中相比传统系统性能有显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.05199" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>