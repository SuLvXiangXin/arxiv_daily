<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.14031" target="_blank" rel="noreferrer">2512.14031</a></span>
        <span>作者: Hu, Zhaofeng, Yu, Hongrui, Chandramouli, Vaidhyanathan, Liang, Ci-Jyun</span>
        <span>日期: 2025/12/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>建筑行业面临严重的劳动力短缺，机器人自动化被视为提高生产力的关键途径。其中，涉及材料拾取与安装的长视野、复杂操作任务相对探索不足。目前，主流方法是模仿学习（IL）和强化学习（RL），它们通过专家演示来训练机器人策略，但通常需要大量演示数据才能达到足够性能。与此同时，新兴的视觉-语言-动作（VLA）大模型（如RT-2、Octo）在通用机器人操作中展现出强大的少样本甚至零样本泛化能力，这为减少对昂贵、耗时的现场演示数据的依赖提供了可能。然而，对于复杂的多阶段建筑任务，哪种方法能在保证性能的同时，以最小的演示和训练成本实现高效部署，尚缺乏系统性的基准测试。</p>
<p>本文针对这一痛点，旨在通过一个成本效益的视角，系统性地评估和对比分层强化学习（HRL）与VLA模型在建筑机器人技能学习中的表现。核心思路是：为两种方法分别开发高效的数据收集接口，在统一的多阶段面板安装任务上，从任务性能、样本效率和部署工作量等方面进行综合对比，以明确各自的适用场景。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的研究框架包含三个核心贡献：两种轻量级数据收集接口、一个可复用的MuJoCo仿真环境，以及两套互补的学习模型（HRL和VLA）。整体评估流程分为三步：首先在HRL方法内部比较多层感知机（MLP）策略和深度Q网络（DQN）模仿模型，以确定更强的RL基线；其次，在两种不同场景下训练并比较三种不同的VLA模型；最后，将选出的RL基线与VLA模型在计算效率、样本效率和真实机器人实验上进行基准测试。</p>
<p><img src="https://arxiv.org/html/2512.14031v1/x3.png" alt="HRL信息架构"></p>
<blockquote>
<p><strong>图3</strong>：HRL模型的信息架构。展示了基于状态（力、关节、物体）的观测输入，通过MLP或DQN网络输出关节空间动作的流程。</p>
</blockquote>
<p><strong>1. 数据收集系统</strong>：</p>
<ul>
<li><strong>关节控制接口（用于HRL）</strong>：通过滑块精确控制机器人每个关节（精度0.1弧度），记录7自由度物体位姿、机器人关节状态和碰撞力。数据经滤波去噪后用于训练。</li>
<li><strong>键盘末端执行器控制接口（用于VLA）</strong>：通过键盘按键控制末端执行器在笛卡尔空间中的运动（Δx, Δy, Δz, Δrx, Δry, Δrz）和夹爪/吸附命令（g），频率20Hz。通过操作空间控制器（OSC）将笛卡尔增量转换为底层关节指令。同步记录智能体视角RGB图像、腕部摄像头图像、本体感知状态和7自由度动作，每条轨迹与自然语言指令配对，存储为RLDS格式。</li>
</ul>
<p><strong>2. 分层强化学习（HRL）模型</strong>：</p>
<ul>
<li><strong>MLP策略</strong>：一个三层全连接网络（ReLU激活），直接将观测（力 <code>f_t</code>、关节状态 <code>q_t</code>、物体状态 <code>s_t</code>）映射为关节空间动作 <code>u_t</code>。</li>
<li><strong>DQN模仿模型</strong>：基于价值函数 <code>Q_ψ(o, u)</code>，通过argmax选择动作。其训练使用时序差分损失，奖励函数在拾取阶段设计为 <code>R1 = -1 * α * D1</code>，其中 <code>D1</code> 是到目标的瞬时距离，<code>α</code> 为可训练参数。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.14031v1/x4.png" alt="RL模型架构"></p>
<blockquote>
<p><strong>图4</strong>：RL模型（MLP和DQN）的架构细节。MLP直接进行策略回归，DQN则学习状态-动作价值函数并通过贪心策略选择动作。</p>
</blockquote>
<p><strong>3. 视觉-语言-动作（VLA）模型</strong>：<br>本文评估了三个VLA模型：OpenVLA、<code>π_0</code> 和 <code>π_0.5</code>。其通用流程是：收集少量多场景演示数据，对预训练的VLA模型进行微调，然后在测试场景中评估。</p>
<p><img src="https://arxiv.org/html/2512.14031v1/x5.png" alt="VLA信息架构"></p>
<blockquote>
<p><strong>图5</strong>：VLA模型的信息架构。展示了从多场景数据收集，到基于演示数据微调预训练模型，最后在测试场景中进行评估的完整流程。</p>
</blockquote>
<ul>
<li><strong>核心架构</strong>：采用冻结的视觉-语言主干网络加轻量级控制头的范式。主干网络 <code>Φ_VLA</code> 将最近k帧RGB图像 <code>I_t-k:t</code>、任务提示 <code>P_t</code> 以及本体感知状态和上一动作编码为融合特征 <code>f_t</code>。控制头 <code>H_θ</code> 根据 <code>f_t</code> 预测7自由度末端执行器动作增量 <code>Δξ_t</code> 和夹爪命令 <code>s_t</code>。</li>
<li><strong>训练与创新</strong>：控制头采用条件流模型，通过流匹配目标函数进行训练，以实现低延迟的精确控制。微调时仅更新控制头及可能的低秩适配器（LoRA），保持主干网络权重冻结，从而实现快速任务迁移。</li>
<li><strong>模型区别</strong>：<ul>
<li><code>π_0</code>：使用PaLI-Gemma-3B主干，配备独立的“动作专家”模块，通过流匹配预测50步的动作块，实现高频控制。</li>
<li><code>π_0.5</code>：与<code>π_0</code>接口相同，但在异构数据上协同训练，增强了开放世界泛化能力和长视野任务稳定性，采用“先推理后执行”的解码方式。</li>
<li><strong>OpenVLA</strong>：作为对比基线，使用Prismatic-7B VLM主干，将动作离散化为词汇表token进行预测，架构更简洁。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2512.14031v1/x6.png" alt="VLA详细架构"></p>
<blockquote>
<p><strong>图6</strong>：VLA模型的详细架构。展示了视觉与语言信息的编码融合过程，以及轻量级控制头如何输出连续动作。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>平台与任务</strong>：在MuJoCo仿真环境中构建了一个多阶段面板安装任务，包括“运输”（拾取面板）和“安装”（精确放置）两个阶段。</li>
<li><strong>对比方法</strong>：<ul>
<li>HRL基线：MLP策略，DQN模仿学习。</li>
<li>VLA模型：OpenVLA, <code>π_0</code>, <code>π_0.5</code>。</li>
</ul>
</li>
<li><strong>评估指标</strong>：任务成功率、样本效率（训练所需演示数据量）、计算效率、泛化能力。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>VLA模型内部比较与样本效率</strong>：<ul>
<li>在拾取任务中，<code>π_0</code>和<code>π_0.5</code>仅需<strong>1-2条</strong>演示轨迹就能快速收敛并达到高成功率（&gt;80%），而OpenVLA需要更多数据。</li>
<li><code>π_0.5</code>在安装阶段展现出更好的长视野任务稳定性。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.14031v1/x7.png" alt="VLA训练损失"></p>
<blockquote>
<p><strong>图7</strong>：不同VLA模型在训练过程中的损失曲线。显示了<code>π_0</code>和<code>π_0.5</code>仅用极少演示（1-2条轨迹）即可快速收敛。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.14031v1/x8.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图8</strong>：不同方法在面板拾取任务中的成功率对比。VLA模型（<code>π_0</code>, <code>π_0.5</code>）在极少样本下实现了高成功率（场景1达60%，场景2达100%），DQN经过调优也能达到类似水平，但MLP策略性能较差。</p>
</blockquote>
<ol start="2">
<li><p><strong>HRL基线确定与鲁棒性</strong>：</p>
<ul>
<li>DQN模仿学习的性能显著优于MLP策略，因此被选为与VLA对比的HRL基线。</li>
<li>DQN在训练过程中需要添加<strong>动作噪声</strong>进行探索，才能获得鲁棒的性能，这增加了调优工作量。</li>
</ul>
</li>
<li><p><strong>VLA vs. DQN 基准测试</strong>：</p>
<ul>
<li><strong>性能与泛化</strong>：在拾取阶段，<code>π_0.5</code>在两种测试场景下分别达到**60%<strong>和</strong>100%**的成功率，展现了强大的少样本泛化能力。DQN经过调优后也能达到可比的成功率。</li>
<li><strong>计算效率</strong>：VLA模型因主干网络庞大，单次推理耗时（<del>100ms）远高于DQN（</del>1ms）。但VLA的训练效率高，所需演示数据极少。</li>
<li><strong>定性结果</strong>：VLA模型能生成更平滑、类人的运动轨迹，而DQN的动作有时显得不自然或振荡。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.14031v1/x9.png" alt="消融实验"></p>
<blockquote>
<p><strong>图9</strong>：关于观测状态和动作噪声的消融实验。表明力传感信息对安装阶段至关重要，而动作噪声对DQN训练获得鲁棒性有积极影响。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.14031v1/x10.png" alt="定性结果1"><br><img src="https://arxiv.org/html/2512.14031v1/x11.png" alt="定性结果2"><br><img src="https://arxiv.org/html/2512.14031v1/x12.png" alt="定性结果3"></p>
<blockquote>
<p><strong>图10-12</strong>：VLA模型（<code>π_0.5</code>）与DQN在面板安装任务中的定性对比。展示了VLA生成的动作轨迹更平滑，而DQN在接触阶段可能出现振荡。</p>
</blockquote>
<ol start="4">
<li><strong>真实机器人实验</strong>：<ul>
<li>将仿真中训练的策略迁移到真实的Franka Emika Panda机器人上，进行面板安装任务。</li>
<li><code>π_0.5</code>模型成功完成了<strong>长视野的拾取-运输-安装</strong>全过程，验证了仿真到实物的转移能力。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.14031v1/x13.png" alt="计算效率"></p>
<blockquote>
<p><strong>图13</strong>：不同方法的计算效率对比。VLA模型因参数量大导致推理延迟较高，但所需训练数据量极少。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.14031v1/x14.png" alt="真实机器人实验"></p>
<blockquote>
<p><strong>图14</strong>：真实机器人实验设置与<code>π_0.5</code>模型执行面板安装任务的序列图。验证了方法从仿真到实际应用的可行性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>观测模态</strong>：力/扭矩传感信息对于需要接触的“安装”阶段性能提升至关重要。</li>
<li><strong>训练噪声</strong>：在DQN训练中添加动作噪声是获得鲁棒策略的关键，但这也意味着额外的调优工作。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>系统性评估</strong>：首次在复杂、多阶段的建筑机器人操作任务上，对基于状态的HRL（以DQN为代表）和基于视觉-语言的VLA模型进行了全面的基准测试，比较了其性能、样本效率和部署成本。</li>
<li><strong>实用工具开发</strong>：开发了两种高效的遥操作数据收集接口（关节控制与键盘末端控制）和一个可复用的MuJoCo仿真环境模板，降低了建筑机器人技能学习的入门门槛。</li>
<li><strong>实践指南</strong>：研究结果表明，VLA模型（如<code>π_0.5</code>）在<strong>少样本泛化</strong>和<strong>减少编程/调优工作量</strong>方面具有显著优势，适合需要快速适应新任务或演示数据稀缺的场景。而DQN等HRL方法在能够接受一定调优工作量的情况下，提供了<strong>可靠且计算高效</strong>的替代方案。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>实验主要在仿真中进行，虽然进行了初步的真实机器人验证，但在更复杂、动态的真实工地环境中的性能仍需进一步测试。</li>
<li>评估的任务（面板安装）相对结构化，对于极度非结构化或需要高频力控的任务，两种方法的极限尚未完全探索。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>混合系统</strong>：结合VLA的强泛化与高层规划能力，以及HRL/IL在低层控制上的精确性与高效率，构建分层混合系统，可能是未来的一个方向。</li>
<li><strong>数据与仿真</strong>：开发更逼真的建筑任务仿真环境，以及高质量、跨工地的机器人操作数据集，对于推动领域发展至关重要。</li>
<li><strong>人机交互</strong>：如何将VLA的自然语言接口与建筑工人的现场指导和纠错更自然地结合，值得深入探索。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本研究针对建筑机器人技能学习的样本效率问题，评估视觉-语言-动作（VLA）模型与强化学习（RL）方法，以比较其任务性能及实际部署努力。关键技术包括：VLA模型基于视觉和语言输入实现少样本学习；RL方法如深度Q网络（DQN）需通过调整和添加噪声提高鲁棒性。实验表明，VLA模型在拾取任务的两个场景中分别达到60%和100%的成功率，展现出强泛化能力；DQN虽能达到类似成功率，但调整工作量更大。总体，VLA在减少编程努力方面更具优势。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.14031" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>