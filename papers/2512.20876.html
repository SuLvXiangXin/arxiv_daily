<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.20876" target="_blank" rel="noreferrer">2512.20876</a></span>
        <span>作者: Tetsuya Ogata Team</span>
        <span>日期: 2025-12-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型语言模型（LLMs）和视觉语言模型（VLMs）等基础模型已被广泛应用于机器人领域的规划与识别任务。然而，这些模型在训练数据集中通常不包含机器人的本体感觉（如传感器运动轨迹），而机器人本体相关的运动信息对于构建包含物理动态的世界模型至关重要。由于缺乏此类输入，先前关于VLMs和LLMs的研究通常以预定义的API格式处理运动轨迹输出，这常被描述为“表面化”的理解。因此，探究VLMs对机器人运动的理解能力，是推进机器人基础模型发展的关键研究课题。</p>
<p>衡量VLM对机器人运动理解能力的一种方式是通过视频描述生成。本文针对VLM缺乏低层次运动信息的痛点，提出将机器人的本体感觉（关节角度和末端执行器状态）融入VLM的提示输入中。本文的核心思路是：通过向VLM提供机器人状态信息，提升其对机器人任务进行自动描述和子任务分割的零样本性能，从而验证VLM对机器人运动的理解程度。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法旨在利用VLM进行机器人运动描述生成和子任务分割。整体流程基于视频描述生成的研究，通过分层处理，首先生成图像描述，进而生成场景描述，最后汇总为完整任务描述，并基于描述间的文本相似性进行分割。</p>
<p><img src="https://arxiv.org/html/2512.20876v2/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：方法整体框架。输入为机器人运动序列 D，包含图像和轨迹数据对。流程包括：(a) 为每帧图像生成图像描述；(b) 将每k个图像描述分组，生成场景描述；(c) 汇总所有场景描述，生成最终任务描述；(d) 基于所有图像描述的文本嵌入相似性进行子任务分割。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>数据表示</strong>：机器人运动序列 D 由从原始序列中每20帧提取的配对数据组成，其中每个数据对包含相机图像 <code>i</code> 和机器人轨迹数据 <code>a</code>。轨迹数据 <code>a</code> 包括关节角度（7个自由度+1个夹持器开合状态）和末端执行器的姿态与位置（6维信息：x, y, z坐标和roll, pitch, yaw角度）。</p>
</li>
<li><p><strong>场景描述生成</strong>：</p>
<ul>
<li><strong>输入</strong>：运动序列 D 中的所有图像及其对应的机器人状态信息。</li>
<li><strong>过程</strong>：首先，VLM为每一帧图像生成一个图像描述 <code>d_i</code>（图2a）。随后，将这些图像描述按 <code>k=5</code> 为一组输入VLM，生成一个代表短时机器人运动的“场景描述” <code>d&#39;_j</code>（图2b）。此步骤的提示词明确要求VLM根据图像以及对应的关节角度和手部位置信息来描述机器人手臂的动作。</li>
</ul>
</li>
<li><p><strong>描述汇总</strong>：</p>
<ul>
<li><strong>输入</strong>：所有场景描述 <code>(d&#39;_0, ..., d&#39;_{n/k})</code>，以及用于生成每个场景描述的原始图像和运动数据。</li>
<li><strong>过程</strong>：将所有场景描述以及其对应的原始数据再次输入VLM进行总结，生成一个描述整个机器人运动的最终描述（图2c）。提示词指示模型基于任务摘要，生成一个具体、简洁的机器人指令，并融入关节角度和手部位置信息。</li>
</ul>
</li>
<li><p><strong>子任务分割</strong>：</p>
<ul>
<li><strong>输入</strong>：前述步骤中生成的所有图像描述 <code>(d_0, ..., d_n)</code>。</li>
<li><strong>过程</strong>：使用预训练的句子编码器（stsb-xlm-r-multilingual）将所有图像描述转换为768维的嵌入向量 <code>(e_0, ..., e_n)</code>。通过计算连续图像描述嵌入向量之间的余弦相似度，来衡量对应运动片段之间的描述差异。当余弦相似度低于设定的阈值时，则认为该处是不同的子任务（分割点）。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有视频描述或运动分割方法相比，本文的核心创新在于将低层次的机器人本体感觉数据（关节角度和末端执行器状态）作为额外的上下文信息，系统地融入VLM的提示工程中。这并非对VLM进行微调，而是利用其零样本推理能力，旨在增强模型对运动轨迹细节（如方向、位置）的理解和输出。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在Robosuite模拟器中记录的两个机器人任务：开门和箱内拾取放置谷物。</li>
<li><strong>数据</strong>：共22条由人类使用3D鼠标操作的序列（开门10条，拾取放置12条）。每条序列包含RGB图像（256x256）、8自由度关节角度和末端执行器姿态。</li>
<li><strong>评估指标</strong>：描述生成任务评估最终描述是否准确包含关键元素（目标物体、动作、轨迹信息）；子任务分割任务通过改变余弦相似度阈值，观察分割点数量的变化。</li>
<li><strong>实验平台/VLM</strong>：使用ChatGPT-4V (gpt-4v-preview) 作为VLM，温度设为0。</li>
</ul>
<p><strong>对比方法</strong>：进行了消融实验，对比了四种输入条件：(1) VLM输入包含末端执行器状态（w/ EE state）；(2) 包含关节角度（w/ joint angle）；(3) 同时包含两者（w/ both）；(4) 不包含任何机器人状态（no robot state）。</p>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2512.20876v2/x4.png" alt="描述生成示例"></p>
<blockquote>
<p><strong>图4</strong>：生成的子描述和汇总描述示例。上图包含运动信息，下图不包含。包含运动信息后，描述中出现了“downward”、“upward”等方向细节以及具体的箱体位置信息。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.20876v2/x5.png" alt="描述生成成功率"></p>
<blockquote>
<p><strong>图5</strong>：描述生成任务的成功率。柱状图表示描述中包含各要素的比例，点状图表示内容准确的比例。在“轨迹信息”一项中，包含机器人状态（尤其是两者都包含时）显著提升了生成准确轨迹描述的能力。</p>
</blockquote>
<p><strong>文字总结</strong>：在描述生成任务中，当输入包含机器人运动信息时，最终描述的平均单词数从25.2增加至35.45，描述更加详细。如图5所示，对于“目标物体”和“动作”的识别，是否包含运动信息影响不大；但对于“轨迹信息”（如开门方向、谷物放置位置）的准确描述，包含运动信息（特别是同时包含关节和末端状态）带来了显著提升。例如，在开门任务中，生成了“向下”、“向上”等方向词；在拾取放置任务中，识别出了四个不同的箱体放置位置。</p>
<p><img src="https://arxiv.org/html/2512.20876v2/x6.png" alt="分割点数量与阈值关系"></p>
<blockquote>
<p><strong>图6</strong>：不同余弦相似度阈值下，每个任务序列的平均分割点数量。随着阈值提高，包含与不包含运动信息所产生的分割点数量差异趋于增大。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.20876v2/x7.png" alt="子任务分割示例（开门）"></p>
<blockquote>
<p><strong>图7</strong>：开门任务的子任务分割示例（阈值0.65）。红线高亮部分显示了因包含运动信息（如夹持器开合、方向）而出现的额外、更符合直觉的分割点。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.20876v2/x8.png" alt="子任务分割示例（拾取放置）"></p>
<blockquote>
<p><strong>图8</strong>：箱内拾取放置任务的子任务分割示例（阈值0.65）。同样，包含运动信息后，分割点更细致地反映了“接近”、“抓取”、“移动”、“释放”等子阶段。</p>
</blockquote>
<p><strong>文字总结</strong>：在子任务分割中，如图6所示，随着相似度阈值提高，包含运动信息与不包含运动信息所得到的平均分割点数量差异增大。这表明运动信息影响了图像描述的文本嵌入。如图7和图8的定性结果所示，在阈值设为0.65时，包含运动信息后产生的分割点更细致，更符合人类对任务阶段的直觉划分（例如，区分了抓取、移动、释放等动作）。红色高亮部分显示了由夹持器状态、运动方向等信息触发的额外分割点。</p>
<p><strong>消融实验总结</strong>：综合描述生成和分割实验，<strong>同时提供关节角度和末端执行器状态</strong>对性能提升的贡献最大，尤其是在生成包含精确轨迹信息的描述和产生更合理的子任务分割点方面。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种利用VLM进行机器人运动描述生成与子任务分割的方法，创新性地将低层次机器人本体感觉（关节角度、末端执行器状态）通过提示工程融入VLM的推理过程。</li>
<li>通过模拟器实验验证，向VLM输入机器人运动信息能有效提升任务描述的细节丰富度、轨迹信息准确性以及子任务分割的合理性，证明了此类信息对增强VLM运动理解能力的价值。</li>
<li>揭示了当前VLM在理解机器人运动方面的潜力与局限：模型能够关联图像与低层次运动数据，生成更丰富的输出，但仍难以预测细微的时间序列变化，且可能产生表面化或依赖于描述文本结构的输出。</li>
</ol>
<p><strong>局限性</strong>：论文指出，VLM难以预测细微的时间序列变化。此外，子任务分割完全依赖于图像描述的文本相似性，当图像信息变化不大或描述句式结构影响相似度计算时，可能导致不正确的分割。</p>
<p><strong>对后续研究的启示</strong>：本研究证明了通过提示工程融入本体感觉能有效引导VLM，这为在零样本或少样本场景下增强基础模型对物理动态的理解提供了新思路。未来可将该方法集成到模仿学习或LLM-based运动规划框架中，用于自动生成语言-动作配对数据或进行高层次任务规划。同时，如何让VLM更好地理解和推理连续、细粒度的运动变化，仍是需要探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决视觉语言模型（VLMs）因训练数据缺乏机器人低级别运动信息而难以理解包含轨迹的机器人任务视频的核心问题。提出一种通过输入机器人本体感知数据（如关节和末端执行器状态）增强VLM的方法，首先生成多个场景描述并总结为完整任务描述，再通过比较图像描述文本嵌入的相似性实现子任务分割。模拟器实验验证了该方法在提升机器人任务描述和分割性能方面的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.20876" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>