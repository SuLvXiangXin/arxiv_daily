<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.11170" target="_blank" rel="noreferrer">2507.11170</a></span>
        <span>作者: Ruggero Carli Team</span>
        <span>日期: 2025-07-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>精确轨迹跟踪是机器人领域的核心问题。反馈线性化（FL）是一种广泛采用的方法，它通过内外双环控制来补偿非线性动力学。然而，该方法要求精确已知系统模型，这在实践中难以满足。为应对模型不确定性，现有鲁棒FL方案依赖于对模型失配大小的先验界限知识。当这些界限未知时，鲁棒FL控制器的设计仍是一个开放性问题。</p>
<p>另一方面，基于数据学习系统动力学的方法（如高斯过程回归，GPR）被用于补偿模型不准确。GPR相较于其他回归框架（如神经网络）的优势在于，它能提供估计的不确定性界限。然而，现有基于GPR的方法存在局限：有的（如基于计算力矩控制）无法保证误差动力学的稳定性；有的仅使用GPR学习模型而未引入鲁棒动作；还有的仅使用GPR的后验方差作为不确定性界限，而未利用学习到的模型均值来改进模型。</p>
<p>本文针对模型不确定性界限未知这一具体痛点，提出了一种新颖的、基于GPR的鲁棒FL控制器。其核心思路是：同时利用GPR提供的模型失配估计（均值）来改进名义模型补偿，并利用其不确定性估计（方差）来设计鲁棒控制项，从而在理论上保证渐近跟踪。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法假设拥有系统动力学的名义模型（已知 $\hat{M}, \hat{C}, \hat{g}$），但真实的 $M, C, g$ 未知。目标是设计控制律 $\tau$ 使机器人跟踪期望轨迹 $(q_d, \dot{q}_d, \ddot{q}_d)$。</p>
<p><strong>整体框架</strong>：控制器在经典名义模型FL控制律（式5）的基础上，引入两个基于GPR的附加项：1）一个前馈补偿项 $\Delta a_{GPR}$，用于估计并抵消模型失配；2）一个鲁棒项 $w$，用于抑制残余不确定性。控制律最终形式为：<br>$$<br>\tau = \hat{M}(q) \left( \ddot{q}<em>d + K_P \tilde{q} + K_D \dot{\tilde{q}} + \Delta a</em>{GPR} + w \right) + \hat{n}(q, \dot{q})<br>$$<br>其中，$\Delta a_{GPR}$ 由GPR对模型失配的估计得到，$w$ 的设计依赖于GPR提供的后验方差。</p>
<p><img src="https://arxiv.org/html/2507.11170v1/x1.png" alt="控制框架图"></p>
<blockquote>
<p><strong>图1</strong>：所提出的鲁棒控制方案整体框架。基于名义模型进行反馈线性化（FL），同时利用高斯过程回归（GPR）在线估计模型失配 $\eta$。GPR的均值输出 $\mu$ 作为前馈补偿 $\Delta a_{GPR}$ 加入控制律，其方差输出 $\Sigma$ 用于设计鲁棒项 $w$ 的增益 $\rho$。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>GPR建模对象</strong>：GPR被用于直接估计由模型不准确引起的总扰动 $\eta$（式6）。将系统状态 $[q^T, \dot{q}^T]^T$ 和辅助控制量 $a$ 的估计值 $\hat{a} = \ddot{q}_d + K_P \tilde{q} + K_D \dot{\tilde{q}}$ 组合为输入向量 $x$。通过收集输入输出数据 $\mathcal{D} = {X, y}$，其中 $y$ 通过将实际测量到的加速度 $\ddot{q}$ 与 $\hat{a}$ 比较并考虑名义模型误差后构造，GPR可以学习映射 $f(x) \approx \eta$。</p>
</li>
<li><p><strong>前馈补偿项</strong>：在控制律中，直接使用GPR在当前状态 $x_*$ 下预测的均值 $\mu(f|x_*, \mathcal{D})$ 作为 $\Delta a_{GPR}$，即 $\Delta a_{GPR} = \mu(f|x_*, \mathcal{D})$。此举旨在主动抵消大部分已学习到的模型不确定性。</p>
</li>
<li><p><strong>鲁棒项设计</strong>：引入鲁棒项 $w = \rho \frac{z}{|z|}$，其中 $z = D^T Q \xi$（$\xi$ 为误差状态向量，$Q$ 为正定矩阵）。关键创新在于<strong>鲁棒增益 $\rho$ 的设计</strong>。论文通过李雅普诺夫分析证明，为了确保 $\dot{V} &lt; 0$，只需选择 $\rho$ 满足：<br>$$<br>\rho &gt; | \eta - \Delta a_{GPR} |<br>$$<br>由于真实残差 $\eta - \Delta a_{GPR}$ 未知，论文利用GPR的后验标准差 $\sigma(f|x_*, \mathcal{D}) = \sqrt{\Sigma(f|x_*, \mathcal{D})}$ 来界定它。基于概率论，可以高概率地保证存在一个常数 $\kappa$，使得 $| \eta - \Delta a_{GPR} | \leq \kappa \sigma(f|x_*, \mathcal{D})$。因此，最终设计为：<br>$$<br>\rho = \kappa \sigma(f|x_*, \mathcal{D}) + \epsilon<br>$$<br>其中 $\epsilon &gt; 0$ 是一个小的常数裕量。这使得鲁棒增益 $\rho$ 能够根据GPR对当前状态不确定性的评估进行<strong>自适应调整</strong>：在数据充分、预测置信度高的区域，$\sigma$ 小，$\rho$ 也小，控制动作更平滑；在未探索区域，$\sigma$ 大，$\rho$ 增大以提供更强的鲁棒性。</p>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：与仅使用GPR均值或仅使用其方差的方法不同，本文<strong>同时且协同地利用了GPR的均值（用于前馈补偿）和方差（用于鲁棒增益自适应）</strong>，形成了“学习+鲁棒”的双重保障机制，并在理论上证明了闭环系统的渐近跟踪性能。</p>
<h2 id="实验与结果">实验与结果</h2>
<ul>
<li><strong>实验平台/数据集</strong>：在一个模拟的2自由度（2-DOF）平面机器人连杆系统上进行数值实验。</li>
<li><strong>对比的Baseline方法</strong>：<ol>
<li><strong>经典FL</strong>：仅使用不准确的名义模型（式5）。</li>
<li><strong>GPR-FL</strong>：在经典FL基础上加入GPR均值前馈补偿 ($\Delta a_{GPR}$)，但<strong>没有</strong>鲁棒项 $w$。</li>
<li>**本文方法 (Robust GPR-FL)**：完整的提案方法，包含GPR均值前馈补偿和基于方差的鲁棒项。</li>
</ol>
</li>
<li><strong>关键实验结果</strong>：论文测试了模型存在严重不准确性（连杆质量、长度、惯性矩均有较大误差）的场景。性能指标为位置跟踪误差的均方根（RMSE）。<ul>
<li>经典FL由于模型误差，跟踪性能差。</li>
<li>GPR-FL通过在线学习显著提升了性能，但在训练初期或快速运动时，因学习不充分或泛化误差，仍会出现峰值误差。</li>
<li><strong>本文的Robust GPR-FL方法取得了最好的跟踪精度</strong>，并且在整个轨迹上保持了平滑、一致的误差收敛。具体数值上，与经典FL相比，其RMSE降低了一个数量级；与GPR-FL相比，进一步减少了约50%的误差，并完全消除了GPR-FL出现的瞬时大误差峰值。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2507.11170v1/x2.png" alt="跟踪误差对比"></p>
<blockquote>
<p><strong>图2</strong>：三种控制器在关节1上的跟踪误差对比。本文提出的Robust GPR-FL（实线）的误差幅值最小且最平滑，GPR-FL（虚线）虽有改善但存在峰值误差，经典FL（点划线）误差最大。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.11170v1/x3.png" alt="鲁棒增益与GPR方差"></p>
<blockquote>
<p><strong>图3</strong>：鲁棒增益 $\rho$（上图）与GPR预测的标准差 $\sigma$（下图）随时间变化曲线。两者形状高度一致，验证了 $\rho$ 根据不确定性 $\sigma$ 进行自适应调整的设计。在运动剧烈、不确定性高的阶段（如0.5秒和1.5秒附近），$\rho$ 自动增大以提供更强的鲁棒控制力。</p>
</blockquote>
<ul>
<li><strong>消融实验</strong>：实验本身对比了不带鲁棒项（GPR-FL）和带鲁棒项（Robust GPR-FL）的效果，可视作一种消融研究。结果表明，<strong>基于GPR方差的鲁棒项对于抑制学习残差、保证稳定性和最终性能至关重要</strong>，特别是在学习过程的早期或系统经历未充分学习的状态时。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新型学习鲁棒控制器，<strong>统一框架</strong>下同时利用GPR的均值估计改进模型补偿，并利用其方差信息自适应调整鲁棒控制增益。</li>
<li>提供了基于李雅普诺夫方法的严格理论分析，证明了在高概率意义下，闭环系统的跟踪误差能够渐近收敛到零。</li>
<li>通过2-DOF机械臂的仿真实验，验证了方法在模型严重不准确情况下的有效性，性能显著优于基线方法。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确讨论局限性，但基于内容可指出，目前的工作仅在仿真环境中进行了验证，尚未在真实物理系统上测试。真实环境中的传感器噪声、执行器动力学、以及GPR在线计算复杂度等问题是未来需要面对的挑战。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>将学习技术（尤其是能提供不确定性量化的如GPR、贝叶斯神经网络）与鲁棒控制理论相结合，是解决模型不确定性问题的强有力范式。</li>
<li>本文提出的利用学习模型不确定性估计（方差）来在线构造鲁棒控制律增益的方法，可以推广到其他控制架构中。</li>
<li>未来的工作可以探索更高效的高斯过程近似方法以降低计算负担，并将该框架部署到实际机器人系统进行实验验证。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操纵器模型不确定性未知时的精确轨迹跟踪问题，提出一种基于高斯过程回归（GPR）的鲁棒反馈线性化控制器。方法结合经典反馈线性化与GPR：利用GPR估计模型不匹配并集成到控制外环，再基于GPR提供的方差设计鲁棒项以补偿剩余不确定性。理论证明该方案能以高概率保证对期望轨迹的渐近跟踪，并在2自由度平面机器人上进行了数值验证。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.11170" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>