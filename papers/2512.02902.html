<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.02902" target="_blank" rel="noreferrer">2512.02902</a></span>
        <span>作者: Li, Weiqi, Zhang, Quande, Zhai, Ruifeng, Lin, Liang, Wang, Guangrun</span>
        <span>日期: 2025/12/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过在大规模机器人数据集上进行预训练，在分布内任务上表现出色。然而，当面对新相机视角、光照变化或场景扰动等分布外场景时，其性能会急剧下降，这限制了其在动态真实环境中的应用。现有提升鲁棒性的方法主要分为两类：数据驱动方法通过收集大规模多视角数据来增加视觉多样性，但成本高昂；表示驱动方法旨在学习几何一致或3D感知的表征，但对任务无关的视觉因素（如背景杂乱）仍然敏感。</p>
<p>本文提出了一个新视角：VLA模型在视角变化下的性能下降主要源于<strong>空间建模</strong>的错位，而非<strong>物理建模</strong>能力的不足。具体而言，VLA模型可概念性分解为两部分：视觉编码器负责从图像中构建物体间的空间关系（空间建模）；而VLM和动作专家则整合任务语言、空间表征和动作历史进行高层推理并生成动作序列（物理建模）。视角变化主要改变了观察到的场景空间配置，而非任务语义或动作动力学，因此性能下降很可能是由于视觉编码器产生的空间表征发生了偏移，导致下游的物理建模模块接收到失真的输入，而非其本身丧失了推理和控制能力。</p>
<p>基于此，本文的核心思路是：仅需对预训练VLA模型的视觉表征进行<strong>轻量级、最小化的适应</strong>，即可有效恢复其在视角变化下的泛化能力，无需大规模重训练或复杂架构修改。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了一个统一的单样本鲁棒性适应框架，其核心思想是仅对视觉模块进行轻量级调整，以重新校准因视角变化而失真的视觉表征，同时保持VLA主干网络（语言编码器、VLM解码器、动作专家）冻结。</p>
<p><img src="https://arxiv.org/html/2512.02902v1/x3.png" alt="方法对比"></p>
<blockquote>
<p><strong>图3</strong>：VLA模型适应新视觉扰动的方法对比。(a) LoRA微调：对整个VLA主干进行低秩更新。(b) 替换视觉主干：用更鲁棒的编码器替换并重新训练。(c) 基于提示的适应：引入可学习token连接到多模态嵌入。(d) <strong>本文提出的特征Token调制（FTM）</strong>：对视觉token嵌入进行全局仿射变换。(e) <strong>本文提出的特征线性适应（FLA）</strong>：对ViT编码器内部的线性层进行低秩更新。</p>
</blockquote>
<p>整体流程基于π0.5 VLA策略。在时间步t，策略接收视觉输入v_t和语言指令l，通过视觉编码器f_v(·)将图像映射为token嵌入序列z = f_v(v)。本文引入轻量级可学习适应参数φ，通过适应变换A_φ(·)作用于视觉表征，然后与语言嵌入ℓ拼接，送入冻结的多模态Transformer解码器g(·)以自回归方式预测动作token：a_t ~ g(a_&lt;t; [A_φ(f_v(v)); ℓ])。</p>
<p><strong>核心模块1：特征Token调制（FTM）</strong><br>FTM是一种极其轻量的适应机制。它从视觉编码器提取的视觉token嵌入F ∈ ℝ^(N×D_ViT)（N为token数，D_ViT为特征维度）上，施加一个全局的仿射变换，仅引入两个可学习参数向量γ, β ∈ ℝ^(D_ViT)：<br>F_hat = (1 + γ) ⊙ F + β<br>该操作对特征分布进行重新缩放和重新中心化，以快速适应新的视觉域。此设计仅引入2×D_ViT个可训练参数（对于π0.5，D_ViT=2048，共约4K参数），旨在验证视角脆弱性是否主要源于嵌入错位。</p>
<p><img src="https://arxiv.org/html/2512.02902v1/x4.png" alt="特征Token调制"></p>
<blockquote>
<p>**图4 (a)**：特征Token调制（FTM）机制示意图。虚线框内的组件仅在训练时使用，推理时移除。</p>
</blockquote>
<p><strong>核心模块2：特征线性适应（FLA）</strong><br>在FTM有效性的基础上，FLA进一步探索对视觉编码器内部进行更深层的适应。具体而言，它对ViT编码器中的线性层应用参数高效的LoRA更新。对于一个原始线性变换h = Wx，其中W被冻结，LoRA引入低秩分解：W‘ = W + ΔW，其中ΔW = BA。这里A ∈ ℝ^(r×d_in)， B ∈ ℝ^(d_out×r)，且秩r远小于输入输出维度。仅(A, B)是可训练的，从而以最小的参数开销（总计4.7M参数）调整ViT的特征提取过程。</p>
<p><strong>创新点</strong><br>与现有方法相比，本文的创新点在于：1) <strong>问题归因新视角</strong>：明确指出性能下降的主因是空间建模的错位，而非物理建模能力不足。2) <strong>极简适应策略</strong>：提出了两种仅针对视觉通路的轻量级适应方法（FTM和FLA），参数效率极高，无需修改架构或大规模数据。3) <strong>单样本适应</strong>：仅需每个任务一个人类演示即可完成对新视觉域的适应。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准</strong>：构建了Libero-V（Visual）基准，它整合了原始LIBERO基准的任务和Libero-Plus的视觉扰动，包含四种扰动类型：相机视角、光照、背景纹理和图像噪声。</li>
<li><strong>实验平台</strong>：在单个NVIDIA A100 GPU上进行。</li>
<li><strong>基线方法</strong>：对比了GeoAware-VLA（替换视觉主干并从头训练）、OpenVLA-OFT（零样本）及其在Libero-Plus上微调的版本（OpenVLA-OFT-m）、对π0和π0.5进行的单样本LoRA微调、提示学习，以及本文提出的FTM和FLA。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>新相机视角下的鲁棒性</strong>：在LIBERO基准的新视角测试中（表1），π0.5的零样本成功率仅为48.5%。应用FTM后，成功率大幅提升至87.2%（仅4K参数）。应用FLA后，成功率进一步提升至90.8%（4.7M参数），与对整个π0.5进行LoRA微调（90.3%，467M参数）性能相当，但参数开销低两个数量级。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.02902v1/x5.png" alt="新视角成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：在LIBERO基准新相机视角下，适应前后的成功率对比。FTM和FLA均能显著提升零样本性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.02902v1/x1.png" alt="参数效率与性能"></p>
<blockquote>
<p>**图1 (b)**：在LIBERO基准新视角下的参数效率与性能关系。本文方法（FLA）以仅4.7M参数取得了与467M参数的π0.5 (LoRA)基线相当的平均成功率。</p>
</blockquote>
<ol start="2">
<li><p><strong>多种视觉扰动下的综合鲁棒性</strong>：在Libero-V基准上评估四种扰动（表3）。π0.5 (FLA)在相机视角扰动上达到90.8%，在其他扰动（光照、纹理、噪声）上也保持了高水平（96.8%， 97.1%， 94.6%），平均跨子集成功率为94.8%，展现了广泛的鲁棒性。</p>
</li>
<li><p><strong>消融实验与组件贡献</strong>：实验结果表明（表1, 3），仅对视觉编码器进行极轻量的适应（FTM）即可带来巨大性能提升，这强有力地支持了本文的核心论点——视角脆弱性主要源于空间建模的错位。进一步的视觉编码器内部适应（FLA）能带来额外的小幅增益。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.02902v1/x6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图6</strong>：经过单样本FLA适应后，在真实世界任务上的定性结果展示。尽管存在显著的新视角视觉差异，适应后的策略成功恢复了空间感知并执行了精确的闭环操作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.02902v1/x2.png" alt="任务执行可视化"></p>
<blockquote>
<p><strong>图2</strong>：一个样本任务执行过程的可视化。每行展示了不同视角下的观测随时间（列）的演变，凸显了经过单样本特征线性适应（FLA）的π0.5方法的适应能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>重新评估VLA鲁棒性</strong>：揭示了预训练VLA模型在视角变化下性能下降的主要瓶颈是空间建模的视觉表征错位，而非物理建模能力不足。</li>
<li><strong>提出高效适应框架</strong>：提出了一个统一的单样本鲁棒性适应框架，包含特征Token调制（FTM）和特征线性适应（FLA）两种方法，能够以极少的参数更新高效地重新校准视觉表征。</li>
<li><strong>验证有效性</strong>：在Libero-V基准上取得了先进的泛化性能，证明了预训练VLA模型中存在大量未被开发的鲁棒性潜力，仅需最小化的视觉适应即可激活。</li>
</ol>
<p><strong>局限性</strong>：论文提到，尽管方法显著提升了视角鲁棒性，但对于极端视角变化或高度动态的视觉扰动，其适应性可能存在边界。此外，方法假设物理建模组件是功能完备的，这在某些复杂推理任务中可能需要进一步验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>轻量适应范式</strong>：为提升机器人模型的泛化能力提供了一条高效新路径，即专注于诊断和修正特定模块的表征错位，而非盲目增加数据或模型复杂度。</li>
<li><strong>模型可解释性</strong>：对VLA模型进行空间建模与物理建模的分解，有助于更精细地理解模型失败的原因，推动更具模块化和可诊断性的 embodied AI 系统设计。</li>
<li><strong>潜在应用</strong>：这种快速、轻量的适应机制非常适合需要机器人快速适应新环境（如新工作车间、新家庭）的真实世界部署场景。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究了视觉-语言-行动模型在新摄像机视角下泛化能力急剧下降的问题，指出其根源在于空间建模的错位。为解决此问题，论文提出了两种轻量级单样本适应方法：特征令牌调制，通过对视觉令牌进行全局仿射变换；以及特征线性适应，对ViT编码器进行低秩更新。实验表明，这两种方法能大幅提升模型鲁棒性，其中FTM仅用4K参数就将Libero视角准确率从48.5%提升至87.1%，而FLA以4.7M参数取得了90.8%的成功率，性能媲美LoRA微调但成本极低。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.02902" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>