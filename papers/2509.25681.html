<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.25681" target="_blank" rel="noreferrer">2509.25681</a></span>
        <span>作者: Wen, Junjie, Zhu, Minjie, Liu, Jiaming, Liu, Zhiyuan, Yang, Yicun, Zhang, Linfeng, Zhang, Shanghang, Zhu, Yichen, Xu, Yi</span>
        <span>日期: 2025/09/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为机器人领域的下一代框架，其发展经历了两个阶段。第一阶段使用预训练的视觉-语言模型作为特征提取器，直接映射到动作。第二阶段则通过联合训练图像-文本数据和动作轨迹来保持预训练知识，并预测子步骤推理（思维链）和动作。思维链将高级指令分解为低级子步骤，为动作预测提供更好的指导。近期工作还将图像生成能力融入VLA，以预测后续图像作为视觉形式的思维链，展示了强大的泛化能力。</p>
<p>然而，现有方法存在关键局限性。首先，联合训练视觉-文本数据（目标为知识保持）与机器人动作数据（目标为动作预测）会导致梯度冲突，干扰动作学习。其次，在自回归视觉语言模型中整合图像生成具有挑战性，因为训练目标和模型架构之间存在根本性差距，使得协调多模态生成与理解变得困难。这导致VLA难以充分利用所有模态的知识，限制了其捕捉连接动作与生成图像的底层物理规律的能力。</p>
<p>本文针对上述痛点，提出了一种新的视角：在统一的、基于扩散的目标下联合优化视觉推理、图像生成和机器人操作。本文的核心思路是构建一个基于离散扩散语言模型的统一框架，通过多模态思维链训练，使模型能够同时生成子目标图像、文本推理和动作序列，从而学习跨模态的一致表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>dVLA的整体目标是在一个统一模型中学习同时生成多模态思维链推理（包括子目标图像和文本推理）和动作预测。其核心是采用一致的离散化策略和离散扩散建模来统一训练目标。</p>
<p><img src="https://arxiv.org/html/2509.25681v1/x1.png" alt="方法架构"></p>
<blockquote>
<p><strong>图1</strong>：dVLA的整体架构。模型采用离散扩散语言模型作为主干，并为每种模态使用独立的分词器。</p>
</blockquote>
<p><strong>统一离散策略与建模</strong>：dVLA处理视觉、文本和动作三种模态。它沿用了MMaDA的令牌化方法：使用MAGVIT-v2将原始图像编码为视觉令牌，使用LLaDA文本分词器处理文本数据。对于动作，则采用FAST分词器，该分词器利用离散余弦变换和字节对编码将连续动作离散化。经过一致的令牌化后，输入序列可表示为 x = {o, l, s, o_goal, r, a_chunk}，分别代表当前观测、语言指令、机器人状态、子目标图像（视觉推理）、文本推理和待执行的动作块。训练时，不同模态的令牌被随机掩码，dVLA必须基于未掩码的令牌预测所有被掩码的令牌。其统一的训练目标如公式(1)所示，是一个标准的离散扩散去噪目标，仅对掩码位置计算损失。</p>
<p><strong>多模态思维链机制</strong>：dVLA的统一令牌化使其能够通过多模态思维链联合建模视觉、语言和动作。输入序列结构为：[观测与指令] [多模态CoT推理] [动作] [EOS]。其中，观测与指令部分包括多帧图像、文本指令和机器人状态；多模态CoT推理部分包括预测的子目标图像令牌和文本推理令牌；最后是动作令牌。在推理时，dVLA首先生成详细的视觉CoT（描绘预期物理运动）和细粒度的文本CoT（提供逐步指令），然后基于这些多模态推理步骤生成具体的可执行动作。</p>
<p><img src="https://arxiv.org/html/2509.25681v1/x2.png" alt="多模态思维链示例"></p>
<blockquote>
<p><strong>图2</strong>：真实机器人任务上的多模态思维链示例。展示了模型如何生成视觉子目标和文本推理来指导最终的动作生成。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法通常使用不同的目标分别优化推理（令牌预测）和控制（连续动作或噪声预测）不同，dVLA的核心创新在于将视觉、语言和动作预测全部归结为单一的基于扩散的去噪目标。这种统一优化促进了跨模态生成的一致性，并通过共享潜在空间的思维链进一步改进了动作预测。具体而言，在训练中随机掩码动作、子目标图像和文本推理令牌，迫使模型学习它们之间的内在联系，从而捕获动作与感知结果之间的物理规律。</p>
<p><strong>加速策略</strong>：为了提升推理效率，dVLA引入了两种加速策略。1) <strong>前缀注意力掩码</strong>：在训练时采用分块因果注意力掩码，将序列分为观测指令块和CoT与动作块，允许块内双向注意力但阻止块间向前关注，这使得部分键值缓存成为可能。2) <strong>KV缓存</strong>：在推理时采用训练无关的KV缓存技术，利用去噪步骤间键值特征变化较小的观察，缓存中间结果并以较低频率更新，减少计算开销。这两种策略结合在LIBERO基准和真实任务中实现了约2倍的加速，且性能下降很小。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在仿真和真实世界中进行。仿真使用LIBERO基准测试（包含Spatial, Object, Goal, Long四个任务套件，共40个任务）。真实世界使用7自由度Franka机械臂，在四个任务上进行评估：箱内取物、打开盒子、挂杯子、按指令拾放物体。</p>
<p><img src="https://arxiv.org/html/2509.25681v1/figures/task_suite.png" alt="实验设置"></p>
<blockquote>
<p><strong>图3</strong>：实验设置与真实世界任务套件。展示了用于评估的四个真实机器人任务场景。</p>
</blockquote>
<p><strong>基线方法</strong>：对比了最先进的连续动作策略和离散动作策略。连续基线包括Diffusion Policy、Octo、DiT Policy、π0、GR00T-N1、OpenVLA-OFT(Continuous)。离散基线包括OpenVLA、OpenVLA-OFT(Discrete)、CoTVLA、WorldVLA、Discrete Diffusion VLA。此外，还设置了vanilla dVLA（仅预测动作令牌）作为消融对照。</p>
<p><strong>主要结果</strong>：</p>
<ol>
<li><strong>LIBERO仿真</strong>：结果如表1所示。dVLA取得了96.4%的平均成功率，在所有连续和离散动作策略中表现最佳。具体而言，在四个任务套件上的成功率分别为97.4%、97.9%、98.2%、92.2%。相较于强劲的连续动作基线（如π0的94.2%， GR00T-N1的93.9%），dVLA有显著提升。与先进的离散动作基线Discrete Diffusion VLA（96.3%）相比，dVLA也以0.1%的优势胜出。</li>
<li><strong>真实世界任务</strong>：结果如表2所示。dVLA在四个任务上取得了平均65%（26/40）的成功率， consistently优于所有基线。特别是在具有挑战性的箱内取物任务中，dVLA取得了7/10的成功率，而多数基线（如Diffusion Policy、OpenVLA）仅能完成2/10。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.25681v1/x3.png" alt="LIBERO定性结果"></p>
<blockquote>
<p><strong>图4</strong>：LIBERO仿真上的定性结果。上图：成功执行结果。下图：失败执行结果及对应的视觉CoT。值得注意的是，失败的视觉CoT准确地预测了实际执行中的不安全行为（如物体卡住、移动方向错误），表明dVLA通过学习物理规律，能够预测错误执行的结果。</p>
</blockquote>
<p><strong>消融分析与多模态CoT贡献</strong>：</p>
<ul>
<li><strong>多模态CoT的有效性</strong>：对比vanilla dVLA（无显式多模态CoT）与完整dVLA。在LIBERO上，引入多模态CoT使平均成功率从89.8%提升至96.4%，增益达6.6个百分点。在真实任务中，从52.5%（21/40）提升至65%（26/40），增益为12.5%。这验证了多模态思维链推理对机器人操作的增强作用。</li>
<li><strong>作用机制</strong>：分析表明，在箱内取物等复杂任务中，vanilla dVLA容易因物体间干扰而预测错误的抓取位姿。而完整dVLA通过生成指定待抓取物体的子目标图像和文本推理，显式地引导策略预测更精确的抓取位姿，减少了干扰。</li>
<li><strong>加速策略影响</strong>：采用的加速策略（前缀注意力掩码和KV缓存）在LIBERO和真实任务中均实现了约2倍的推理加速，且仅带来微小的性能下降，保证了模型的实用性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>统一的扩散训练框架</strong>：提出了dVLA，首次将视觉感知、语言理解和机器人动作的生成与推理统一在一个离散扩散概率框架下进行联合优化，解决了传统方法中多目标训练梯度冲突和模态不协调的问题。</li>
<li><strong>多模态思维链机制</strong>：引入了涵盖视觉子目标生成和文本推理的多模态思维链训练范式，使模型能够进行逐步的跨模态推理，并将高层规划与底层控制紧密耦合，显著提升了长视野任务和复杂场景下的性能与泛化能力。</li>
<li><strong>实用的加速策略</strong>：针对扩散模型推理延迟高的挑战，提出了前缀注意力掩码和KV缓存两种策略，在不显著牺牲性能的前提下实现了约2倍的推理加速，增强了模型在实时机器人系统中的部署可行性。</li>
</ol>
<p><strong>局限性</strong>：论文虽通过加速策略缓解了延迟问题，但基于扩散模型的推理速度相比纯自回归模型仍有提升空间。此外，多模态思维链的生成增加了计算负担和序列长度。</p>
<p><strong>启示</strong>：dVLA的成功表明，扩散模型不仅适用于生成高质量内容，其统一的去噪范式也非常适合作为机器人多模态感知、推理与控制的统一建模框架。未来研究可进一步探索如何更高效地融合扩散模型与机器人技术，以及如何将更丰富的物理常识和世界模型知识注入此类统一框架中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出dVLA模型，旨在解决传统视觉-语言-动作模型中多目标训练导致的梯度冲突及推理延迟问题。其关键技术包括：采用单一扩散目标联合优化感知、语言理解与动作生成，并引入多模态思维链进行推理；为加速部署，使用前缀注意力掩码与KV缓存两种策略。实验表明，在LIBERO基准上dVLA达到96.4%的平均成功率，优于现有方法；推理时加速约2倍，并在真实机器人任务中展现了鲁棒性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.25681" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>