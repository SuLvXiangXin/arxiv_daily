<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.15490" target="_blank" rel="noreferrer">2509.15490</a></span>
        <span>作者: Traore, Abdarahmane, Hervet, Éric, Couturier, Andy</span>
        <span>日期: 2025/09/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）已成为多模态推理任务的基础，但性能领先的方法通常依赖于参数规模巨大的模型，带来了高昂的计算和内存成本，使其难以部署在仓库、机器人等资源受限的真实环境中。这些环境不仅要求模型高效，还要求其具备对物体位置、排列和几何关系的鲁棒空间理解能力。然而，研究表明空间推理是大多数VLMs的关键弱点，它们在基本的空间查询任务上表现远逊于人类甚至纯文本模型。尽管已有工作通过引入3D场景图、区域感知提示或深度增强输入来提升空间理解，但这些方法通常仍需要数十亿参数的大型模型，不适用于边缘设备的实时执行。同时，虽然已有一些致力于高效架构的研究，但鲜有工作能同时满足实际部署所需的架构效率和物流仓库应用所要求的复杂空间能力。本文旨在填补这一空白，提出了SmolRGPT，一个紧凑的区域感知视觉语言模型，通过整合RGB和深度线索来显式地融入空间关系，同时保持适合受限环境的小参数量。其核心思路是：通过一个精心设计的三阶段训练课程和高效架构，证明无需依赖数十亿参数的大型模型，也能实现区域级的空间推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>SmolRGPT的整体架构基于SmolVLM，并集成了来自SpatialRGPT的区域级视觉语言建模技术。模型使用预训练的视觉特征提取器SigLip2处理RGB和深度图像，输出维度为256的特征向量Iv（RGB）和Dv（深度）。</p>
<p><img src="https://arxiv.org/html/2509.15490v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：SmolRGPT架构概览。模型通过共享的视觉特征提取器（SigLip2）处理RGB和深度图像，随后进入模态特定的路径。RGB特征通过RGB连接器和RGB精炼器（蓝色），深度特征通过深度连接器和深度精炼器（橙色）处理。两者精炼后的特征通过掩码池化与区域掩码结合，提取空间表示。这些池化后的特征与视觉特征一同整合到SmolLM2 360M语言模型中，生成空间推理回答。示例展示了模型回答仓库区域间距离查询的过程。冻结组件（视觉特征提取器）用雪花符号表示，可训练组件用火焰符号表示。</p>
</blockquote>
<p>首先，RGB特征向量Iv经过RGB连接器（包含像素重排和线性层），映射到语言模型的嵌入空间。随后，投影后的特征通过专用的精炼器（设计类似RegionGPT）中的转置卷积进行上采样，以匹配后续阶段的掩码分辨率。此过程对RGB特征（Iv）和深度特征（Dv）独立进行，使用模态特定的精炼器分别产生精炼特征Ir和Dr。精炼后，Ir和Dr通过区域掩码进行掩码池化，以提取感兴趣区域的特定特征。这些区域级特征随后被插入预训练语言模型SmolLM2-360M的嵌入序列中。这种设计保持了RGB和深度输入各自独立的投影和精炼路径，确保了每种模态的表征保持 distinct，避免了混淆。SmolRGPT的连接器与SpatialRGPT不同，它采用了像素重排，将空间特征重新排列到通道维度，这降低了空间分辨率但增加了通道深度，提供了更密集的表示。</p>
<p>在分词和提示格式方面，遵循多轮对话格式。对于区域级数据集，输入序列中的每个<code>&lt;mask&gt;</code>占位符被替换为<code>&lt;mask_rgb&gt;</code>和<code>&lt;mask_depth&gt;</code>。这些新令牌作为占位符，随后被替换为来自精炼器的相应区域特定的RGB（Ir）和深度（Dr）嵌入。</p>
<p><img src="https://arxiv.org/html/2509.15490v1/x2.png" alt="分词机制"></p>
<blockquote>
<p><strong>图2</strong>：SmolRGPT中的令牌替换机制。视觉主干和投影器将仓库图像转换为LLM嵌入，产生视觉令牌序列。处理过程中，输入提示中的特殊令牌（<code>&lt;mask_rgb&gt;</code>和<code>&lt;mask_depth&gt;</code>）被识别并替换为相应的掩码池化区域嵌入。示例展示了模型如何处理包含多个掩码令牌的空间查询，每个令牌都被通过掩码池化机制提取的适当区域特定特征所替换。</p>
</blockquote>
<p>训练方案分为三个连续阶段：1）<strong>RGB连接器对齐</strong>：仅训练RGB连接器，使用LLaVA-CC3M数据集（59.5万图像-文本对），学习率为10^-4，目的是建立模型对全局场景的理解。2）<strong>深度连接器和精炼器预热</strong>：冻结RGB连接器，解冻深度连接器以及RGB和深度精炼器，使用开放空间数据集（OSD）的前100万个样本进行训练，学习率仍为10^-4，目标是为最终微调初始化这些组件，此阶段输出质量次优，仅为预热步骤。3）<strong>监督微调</strong>：解冻除图像主干外的所有组件，在包含超过50万训练样本的仓库数据集上进行联合训练，使用较低的学习率5×10^-5，以防止灾难性遗忘并保持训练稳定性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验主要在<strong>AI City Challenge 2025 Track 3</strong>提供的<strong>PhysicalAI-Spatial-Intelligence-Warehouse数据集</strong>上进行，并在<strong>SpatialRGPT-Bench</strong>上进行了通用空间推理能力测试。Baseline包括参与该挑战的其他队伍模型，以及在通用基准上对比的GPT-4、GPT-4V、LLaVA-v1.6-34B、SpatialRGPT等大型VLMs。</p>
<p>SmolRGPT在仓库数据集上的关键结果如下：最终S1得分为90.68，在挑战赛中排名第三。具体到各任务类型：左右方向任务准确率达到99.80%；计数任务准确率92.76%，RMSE为0.0750；多项选择题准确率88.02%；距离估计任务准确率82.13%，RMSE为0.4740，这是最具挑战性的任务。</p>
<table>
<thead>
<tr>
<th align="left">Rank</th>
<th align="left">Team Name</th>
<th align="left">Score</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">UWIPL_ETRI</td>
<td align="left">96.0789</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">HCMUT.VNU</td>
<td align="left">91.9735</td>
</tr>
<tr>
<td align="left"><strong>3</strong></td>
<td align="left"><strong>Embia</strong></td>
<td align="left"><strong>90.6772</strong></td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">MIZSU</td>
<td align="left">73.0606</td>
</tr>
<tr>
<td align="left">5</td>
<td align="left">HCMUS_HTH</td>
<td align="left">66.8861</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：AI City Challenge 2025 Track 3排行榜结果。SmolRGPT（团队Embia）以90.68分获得第三名。</p>
</blockquote>
<p>在通用空间推理基准SpatialRGPT-Bench上的结果进一步证明了其高效性。在<strong>定性空间关系</strong>任务上，仅600M参数的SmolRGPT平均准确率达到65.6%，显著超过了参数规模大得多的GPT-4（1.76T，57.8%）、GPT-4V（1.76T，58.1%）和LLaVA-v1.6-34B（34B，43.9%）。特别是在Behind/Front（79.0%）和Tall/Short（74.1%）关系上表现强劲。</p>
<p>在<strong>定量空间估计</strong>任务上，SmolRGPT在直接距离（35.8%）和方向（35.5%）估计上取得了有竞争力的结果，甚至超过了GPT-4V（29.7%），但在宽度（18.05%）和高度（20.30%）估计上表现较弱。</p>
<p><img src="https://arxiv.org/html/2509.15490v1/x3.png" alt="评估流程"></p>
<blockquote>
<p><strong>图3</strong>：SmolRGPT在仓库环境中进行空间推理的评估流程。系统通过SmolRGPT模型处理RGB图像、深度图和区域掩码，生成对空间查询的自然语言响应。生成的响应经过两阶段提取过程：首先，独立的分类器从问题（Longformer Question）和答案（Longformer Answer）中确定答案类型；然后，Qwen2.5-14B根据识别出的问题类型（如距离）提取标准化答案（如“1.91”）。该流程确保了跨不同空间推理类别（包括距离估计、计数、空间关系和多项选择题）的鲁棒答案提取。</p>
</blockquote>
<p>评估方案需注意：由于模型输出是句子，为进行定量评估，论文训练了两个Longformer模型用于问题类型和答案类型分类，并利用Qwen2.5-14B根据类型提示提取标准化答案，此流程仅用于评估，不参与推理。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了SmolRGPT，一个仅600M参数的紧凑视觉语言模型，通过显式整合RGB和深度信息的双路径架构以及区域掩码池化，实现了高效的区域级空间推理。2）设计了一个三阶段渐进式训练课程，从全局视觉-语言对齐，到通用空间能力预热，再到特定任务微调，有效提升了小模型在复杂空间任务上的性能。3）实验证明，该模型在仓库空间推理任务上达到竞争性性能，并在通用空间推理基准上以极小的参数量超越了多个超大模型，实现了效率与性能的平衡。</p>
<p>论文提到的局限性包括：距离估计仍是性能相对最弱的环节；模型评估需要借助额外的Longformer和大型语言模型（Qwen）进行答案标准化，增加了评估复杂性。</p>
<p>这项工作表明，通过精心的架构设计（如模态分离、区域特征提取）和训练策略（渐进式课程学习），小参数模型完全可以在特定领域（如空间推理）达到实用化性能。这为在资源严格受限的边缘设备、机器人或工业系统中部署复杂的多模态AI能力提供了可行的技术路径，启示后续研究可以继续探索更高效的跨模态融合机制和针对性的训练数据构造，以进一步提升小模型在复杂感知推理任务上的上限。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对仓库等资源受限环境中大规模视觉语言模型(VLMs)部署困难、空间推理能力不足的问题，提出了紧凑型模型SmolRGPT。其核心方法是通过整合RGB与深度线索进行区域级空间推理，并采用三阶段课程学习对齐视觉与语言特征。实验表明，该模型仅含6亿参数，即在仓库空间推理基准测试中取得了有竞争力的性能，匹配甚至超过了更大规模的模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.15490" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>