<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MALLVI: a multi agent framework for integrated generalized robotics manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MALLVI: a multi agent framework for integrated generalized robotics manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.16898" target="_blank" rel="noreferrer">2602.16898</a></span>
        <span>作者: Babak Khalaj Team</span>
        <span>日期: 2026-02-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用大语言模型进行机器人操作任务规划的主流方法主要遵循两种策略：一是从演示中直接学习行为轨迹的模仿或策略学习方法，二是依赖视觉语言模型将自然语言和视觉输入映射为动作。这些方法在结构化环境中有效，但在面对开放词汇指令、新物体或新环境时，其有限的语义理解和适应性限制了其在真实场景中的应用。同时，基于LLM的规划框架虽然展现出强大的推理和任务分解能力，但通常以开环方式运行，一次性生成计划而不检查执行是否成功，导致其在动态环境中脆弱，错误容易累积，且可能产生看似合理但实际无法执行的“幻觉”计划。近期研究尝试通过集成视觉反馈进行错误检测和重规划来“闭合回路”，但这些系统大多依赖单一、整体的模型，在任务模糊或需要专业化推理与感知时存在瓶颈，且不受约束的LLM/VLM输出可能带来安全隐患。</p>
<p>本文针对上述开环规划脆弱、单一模型能力有限以及缺乏高效错误恢复机制等痛点，提出了一个全新的多智能体协作与闭环反馈视角。核心思路是：通过协调多个专门化的LLM智能体，分别负责任务分解、场景理解、物体定位、高层推理和动作执行，并引入一个基于视觉语言模型的反射器智能体，利用环境视觉反馈对每个子任务的执行结果进行验证和定向恢复，从而构建一个模块化、可自我纠正的机器人操作框架。</p>
<h2 id="方法详解">方法详解</h2>
<p>MALLVi框架是一个多智能体、自我纠正的机器人操作流水线。给定一个高级用户指令和一张实时环境图像，系统首先将任务层次化分解为原子子任务，然后将每个子任务与视觉输入进行对齐（grounding），规划执行轨迹，并基于反馈自适应地调整动作。各专门化智能体通过对象和内存标签进行通信，并配有自动重试机制以确保动作成功。</p>
<p><img src="https://arxiv.org/html/2602.16898v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：MALLVi框架架构。流水线通过专门化的智能体处理用户指令：分解器（Decompose）将指令拆分为原子步骤；描述器（Describe）提供场景理解；感知器（Perceive）处理视觉输入；定位器（Ground）定位目标物体；投影器（Project）生成运动轨迹；思考器（Think）协调高层推理；执行器（Act）执行机器人命令；反射器（Reflect）评估结果以实现迭代优化和错误恢复。</p>
</blockquote>
<p>整体框架如图1所示，其核心模块包括：</p>
<ol>
<li><strong>分解器与描述器智能体</strong>：在流水线第一阶段并行运行。分解器将高级指令转换为结构化的原子子任务序列，每个子任务对应执行器词汇表中的基本动作（如移动、到达、推动），并附带有参数的内存标签。描述器则使用VLM生成环境的粗略表示，识别物体、提取空间关系并构建场景空间图，为下游智能体提供关键上下文。</li>
<li><strong>定位器智能体</strong>：包含三个子组件。<ul>
<li><strong>感知器</strong>：从指令中识别任务相关物体并标记非目标物体，根据子任务失败情况优化抓取策略。</li>
<li><strong>定位器</strong>：通过融合多个检测器（GroundingDINO和OwlV2）的输出，在图像平面定位物体。它采用基于置信度的选择机制，结合描述器提供的空间图，确保提供精确的边界框。</li>
<li><strong>投影器工具</strong>：将视觉感知转化为可操作的3D抓取点。它利用Segment Anything Model (SAM)识别物体上的候选抓取点，应用物体特定启发式方法进行选择，并通过深度图和针孔相机模型将2D点投影到3D空间，最终通过逆运动学转换为关节角度。</li>
</ul>
</li>
<li><strong>思考器智能体</strong>：一个LLM，负责将高层子任务信息转化为可执行的参数。它检索相关物体，确定3D抓取点及所需旋转。对于无记忆的任务，直接选择拾放位置和旋转；对于有记忆标签的原子指令，则利用存储的场景表示和空间关系来识别源或目标物体，并计算相应的拾放位姿。</li>
<li><strong>执行器智能体</strong>：执行上游智能体产生的子任务。它通过预定义的API与环境交互，接收思考器的动作参数并执行相应的操作，其模块化设计使其与高层推理或低层运动规划解耦。</li>
<li><strong>反射器智能体</strong>：一个VLM，负责实时验证每个子任务的执行。在执行器执行一个子任务后，反射器利用视觉反馈评估其成功与否。成功的子任务从执行队列中移除，失败的子任务则触发对应子任务的重试。此外，反射器会生成失败的自然语言解释、更新共享内存状态、选择性重新激活失败的智能体，并在必要时升级到完整的场景重评估。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.16898v1/x2.png" alt="单智能体与多智能体框架对比"></p>
<blockquote>
<p><strong>图2</strong>：单智能体与多智能体框架对比。单智能体框架在复杂环境中难以保持任务焦点和进行序列推理。MALLVi通过为不同执行方面分配专门化智能体来缓解这些限制。</p>
</blockquote>
<p>与现有方法相比，MALLVi的创新点具体体现在：</p>
<ul>
<li><strong>真正的多智能体架构</strong>：并非单一模型，而是协调多个专门化智能体进行感知、规划和反思，通过共享状态进行协作，实现了模块化分工。</li>
<li><strong>反射器的定向反馈循环</strong>：反射器不仅提供闭环验证，还能通过解释失败、更新内存和选择性重启特定智能体来实现高效的定向错误恢复，避免了代价高昂的全局重规划。</li>
<li><strong>感知与规划的深度集成</strong>：定位器智能体融合多模型检测与置信度加权，并结合场景图进行上下文感知的物体定位，提升了在动态和非结构化环境中的鲁棒性。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.16898v1/x3.png" alt="智能体角色分析"></p>
<blockquote>
<p><strong>图3</strong>：多智能体系统中各专门化智能体及其角色的分析。每个智能体在指定层级（高、中、低）上运作，以处理任务执行的具体组成部分，包括指令分解、内存利用、物体定位、任务推理、动作执行和闭环反馈提供。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界操作任务以及VIMABench和RLBench的基准场景中进行评估。</p>
<p><strong>使用的Benchmark/数据集</strong>：</p>
<ul>
<li><strong>真实世界任务</strong>：包括放置食物、放置形状、堆叠积木、购物清单、放入杯子、数学运算、堆叠杯子、重新排列物体等8个任务。</li>
<li><strong>VIMABench</strong>：选取了4个分区（简单操作、新概念、视觉推理、视觉目标达成）下的12个任务。</li>
<li><strong>RLBench</strong>：包括放入保险箱、放入抽屉、堆叠杯子、放置杯子、堆叠积木等5个任务。</li>
</ul>
<p><strong>对比的Baseline方法</strong>：</p>
<ul>
<li>真实世界任务：MALMM, VoxPoser, ReKep。</li>
<li>VIMABench任务：Wonderful Team, CoTDiffusion, PERIA。</li>
<li>RLBench任务：PerAct。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在真实世界任务中（表1），MALLVi在8个任务上的平均成功率最高，显著优于基线方法。例如，在“堆叠积木”任务上达到90%成功率，而MALMM为55%，VoxPoser为40%，ReKep为75%。在“数学运算”这类需要推理的任务上，MALLVi达到80%，远高于其他方法。</p>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="center">Place Food</th>
<th align="center">Put Shape</th>
<th align="center">Stack Blocks</th>
<th align="center">Shopping List</th>
<th align="center">Put in Mug</th>
<th align="center">Math Ops</th>
<th align="center">Stack Cups</th>
<th align="center">Rearrange Objects</th>
</tr>
</thead>
<tbody><tr>
<td align="left">MALMM</td>
<td align="center">75</td>
<td align="center">65</td>
<td align="center">55</td>
<td align="center">70</td>
<td align="center">55</td>
<td align="center">25</td>
<td align="center">50</td>
<td align="center">-</td>
</tr>
<tr>
<td align="left">VoxPoser</td>
<td align="center">70</td>
<td align="center">55</td>
<td align="center">40</td>
<td align="center">45</td>
<td align="center">40</td>
<td align="center">15</td>
<td align="center">35</td>
<td align="center">0</td>
</tr>
<tr>
<td align="left">ReKep</td>
<td align="center">80</td>
<td align="center">85</td>
<td align="center">75</td>
<td align="center">90</td>
<td align="center">75</td>
<td align="center">60</td>
<td align="center">40</td>
<td align="center">60</td>
</tr>
<tr>
<td align="left">Single-Agent</td>
<td align="center">25</td>
<td align="center">10</td>
<td align="center">15</td>
<td align="center">10</td>
<td align="center">30</td>
<td align="center">5</td>
<td align="center">10</td>
<td align="center">0</td>
</tr>
<tr>
<td align="left">w/o Reflector</td>
<td align="center">85</td>
<td align="center">60</td>
<td align="center">60</td>
<td align="center">65</td>
<td align="center">55</td>
<td align="center">70</td>
<td align="center">50</td>
<td align="center">45</td>
</tr>
<tr>
<td align="left"><strong>MALLVi (Ours)</strong></td>
<td align="center"><strong>100</strong></td>
<td align="center"><strong>95</strong></td>
<td align="center"><strong>90</strong></td>
<td align="center"><strong>90</strong></td>
<td align="center"><strong>80</strong></td>
<td align="center"><strong>80</strong></td>
<td align="center"><strong>85</strong></td>
<td align="center"><strong>75</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：8个真实世界任务上的成功率（%），每个任务重复20次。MALLVi在所有任务上均取得最高成功率。</p>
</blockquote>
<p>在VIMABench任务中（表2），MALLVi在大多数类别中取得了最高的成功率，特别是在“新概念”和“视觉推理”分区分别达到95%和90%，展示了其优秀的泛化与推理能力。</p>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="center">Simple Manipulation</th>
<th align="center">Novel Concepts</th>
<th align="center">Visual Reasoning</th>
<th align="center">Visual Goal Reaching</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Wonderful Team</td>
<td align="center">100</td>
<td align="center">85</td>
<td align="center">90</td>
<td align="center">-</td>
</tr>
<tr>
<td align="left">CoTDiffusion</td>
<td align="center">86</td>
<td align="center">70</td>
<td align="center">54</td>
<td align="center">44</td>
</tr>
<tr>
<td align="left">PERIA</td>
<td align="center">93</td>
<td align="center">78</td>
<td align="center">76</td>
<td align="center">68</td>
</tr>
<tr>
<td align="left">Single-Agent</td>
<td align="center">25</td>
<td align="center">10</td>
<td align="center">15</td>
<td align="center">10</td>
</tr>
<tr>
<td align="left">w/o Reflector</td>
<td align="center">100</td>
<td align="center">80</td>
<td align="center">30</td>
<td align="center">40</td>
</tr>
<tr>
<td align="left"><strong>MALLVi (Ours)</strong></td>
<td align="center"><strong>100</strong></td>
<td align="center"><strong>95</strong></td>
<td align="center"><strong>90</strong></td>
<td align="center"><strong>73</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表2</strong>：VIMABench任务上的成功率（%），每个任务重复100次。MALLVi在复杂推理任务上优势明显。</p>
</blockquote>
<p>在RLBench任务中（表3），MALLVi同样 consistently 优于所有基线方法，例如在“放入保险箱”任务上达到92%的成功率，而PerAct为44%。</p>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="center">Put in Safe</th>
<th align="center">Put in Drawer</th>
<th align="center">Stack Cups</th>
<th align="center">Place Cups</th>
<th align="center">Stack Blocks</th>
</tr>
</thead>
<tbody><tr>
<td align="left">PerAct</td>
<td align="center">44</td>
<td align="center">68</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">36</td>
</tr>
<tr>
<td align="left">Single-Agent</td>
<td align="center">58</td>
<td align="center">73</td>
<td align="center">15</td>
<td align="center">22</td>
<td align="center">42</td>
</tr>
<tr>
<td align="left">w/o Reflector</td>
<td align="center">81</td>
<td align="center">89</td>
<td align="center">63</td>
<td align="center">75</td>
<td align="center">78</td>
</tr>
<tr>
<td align="left"><strong>MALLVi (Ours)</strong></td>
<td align="center"><strong>92</strong></td>
<td align="center"><strong>94</strong></td>
<td align="center"><strong>83</strong></td>
<td align="center"><strong>96</strong></td>
<td align="center"><strong>90</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表3</strong>：RLBench任务上的成功率（%），每个任务重复100次。MALLVi在所有任务上表现最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.16898v1/x4.png" alt="真实世界任务示例"></p>
<blockquote>
<p><strong>图4</strong>：真实世界任务示例。堆叠积木、分类形状和数学运算任务结合了特定指令和物理环境，以评估智能体在实体环境中的行动和解决问题能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.16898v1/x5.png" alt="堆叠积木任务过程"></p>
<blockquote>
<p><strong>图5</strong>：堆叠积木任务的真实世界示例。MALLVi被要求按红、蓝、绿的顺序堆叠积木，木块作为干扰物。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>单智能体基线</strong>：将全部功能合并到单一LLM智能体中。结果（表1,2,3）显示，单智能体在处理简单任务时可行，但在组合推理和对齐任务上表现不佳，成功率大幅下降，凸显了多智能体专业化分工的必要性。</li>
<li><strong>无反射器</strong>：移除反射器智能体，从而消除重试机制。结果显示，尽管流水线仍能工作，但验证和重试显著提高了可靠性，尤其是在复杂任务上（如表2中视觉推理任务成功率从90%降至30%），证明了闭环反馈的关键作用。</li>
<li><strong>开源模型替代</strong>：将默认的GPT-4.1-mini替换为Qwen和LLaMA等开源模型。结果（表4）显示，开源模型在简单任务上具有竞争力，但在组合和多模态任务上表现不足。然而，由于MALLVi将任务分解为多个小职责，即使使用较小的模型，流水线仍能保持相对可接受的准确性，这体现了该框架设计的一个核心优势。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：第一，提出了MALLVi，一个真正分布式的多智能体框架，将基于LLM的规划与基于VLM的监控结合在一个自我纠正的流程中，用于机器人操作。第二，突出了反射器智能体定向反馈循环的新颖作用，通过持续的环境反馈实现反思、错误恢复和适应。第三，在模拟和真实世界实验中验证了框架的有效性，在零样本设置下，跨多样操作任务的平均成功率有显著提升。</p>
<p>论文自身提到的局限性在于，该框架仍然依赖于预定义的原子动作进行执行，这限制了机器人在遇到不可预见的运动学约束、接触动力学或高度动态环境时的适应性。这反映了结构化多智能体推理与灵活低层控制之间的权衡。</p>
<p>对后续研究的启示包括：未来工作可以探索集成自适应执行机制，如强化学习或模仿学习控制器，或可微运动规划模块，以使原子动作能在部署时适应环境。此外，纳入更复杂的感知和对齐模块可以提升在具有新物体、复杂纹理或高度动态场景的任务中的性能。MALLVi表明，多智能体、闭环的LLM框架能够自主管理从感知、推理到高层规划和反思等操作任务的所有关键方面，结合结构化推理与自适应低层执行的未来迭代版本有望在真实世界机器人操作中实现更高的鲁棒性和自主性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>该论文针对动态环境中机器人操作任务规划的脆弱性问题，提出MALLVi多智能体框架。核心方法采用闭环反馈机制，协调Decomposer、Localizer、Thinker、Reflector等专用LLM智能体，分别负责指令分解、目标定位、推理与错误检测恢复，并引入Descriptor智能体建立环境视觉记忆。实验在VIMABench、RLBench模拟及真实场景验证表明，该框架通过智能体间的迭代闭环通信，显著提升了零样本操作任务的泛化能力与平均成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.16898" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>