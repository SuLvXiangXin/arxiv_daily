<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.15446" target="_blank" rel="noreferrer">2510.15446</a></span>
        <span>作者: Guo, Ziang, Zhang, Zufeng</span>
        <span>日期: 2025/10/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前端到端（E2E）自动驾驶范式主要有两大主流方法。一是数据驱动的模仿学习（IL），依赖从专家驾驶员收集的轨迹来监督策略学习，但其泛化能力受限于专家数据的覆盖范围。二是知识驱动范式，利用大型语言模型（LLM）或视觉语言动作模型（VLA）的推理能力，将多模态场景描述转化为语义丰富的提示，并生成规划或控制指令。VLA模型在多模态对齐方面表现有效，但其在自动驾驶中生成的动作输出可能缺乏连续实时驾驶所需的时序一致性和细粒度控制精度。</p>
<p>一个根本性的挑战在于高维、异构的传感器数据空间（如图像、点云）与低维、结构化的动作空间（如转向、油门、刹车）之间存在显著的表示鸿沟。这种鸿沟因不同模态间的语义和时间错位而进一步加剧。本文旨在弥合这一鸿沟，提出一种通过一致的、对齐的多模态表示来连接状态与动作的新视角，该表示同时保留了几何和上下文特征。</p>
<p>本文的核心思路是：提出VDRive框架，通过强化的VLA进行上下文和几何推理，并结合扩散策略头，在统一的低维潜在空间中，利用token化的状态-动作表示来生成层次化的、可执行的驾驶动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>VDRive的整体框架是一个分阶段训练的离线强化学习流程，旨在高效对齐传感器空间与动作空间。其核心流程是：首先，对VLA进行视觉token预测的预训练和基于偏好的强化微调，使其能够预测未来的离散观测token、轨迹和动作。然后，训练一个扩散策略头，该策略头以VLA预测的当前及未来状态为条件，通过去噪过程生成层次化的动作。最后，一个动态引导的优化头利用异步动作预测来优化轨迹输出。</p>
<p><img src="https://arxiv.org/html/2510.15446v2/Figs/3.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图3</strong>：提出的VDRive框架。一个强化框架，赋能VLA预测分割后的可行驶未来，并赋能策略头生成层次化动作和轨迹。通过强化学习和传感器数据的离散化表示，VDRive被训练用于在低维空间中对齐上下文和几何token。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>状态token化（CVQ-VAE）</strong>：使用条件向量量化变分自编码器（CVQ-VAE）将原始图像和对应的轨迹作为条件，编码并量化为离散的潜在代码。这实现了高维连续传感器空间到低维离散token空间的映射，为后续VLA理解和生成提供了统一的词汇表。损失函数包含重建损失、码本损失和承诺损失。</p>
</li>
<li><p><strong>强化VLA</strong>：基于InternVL3-8B构建。首先进行<strong>视觉token预训练</strong>，将CVQ-VAE生成的离散代码添加到VLA分词器的词汇表中，训练VLA根据前视图图像和提示预测未来的观测token。随后进行<strong>强化学习微调</strong>，使用一个偏好数据集（包含来自nuScenes和Bench2Drive的安全场景作为“选中”输出，以及使用Vista生成的或Bench2Drive中的风险场景作为“拒绝”输出）来微调VLA。VLA的输入是前一时刻的前视图图像、动作和导航指令，输出是当前时刻的离散观测token、动作信号、导航指令和轨迹。</p>
</li>
<li><p><strong>扩散策略头</strong>：这是一个基于扩散模型的行动者网络。其训练数据集整合了标注数据、Vista生成数据以及微调后VLA自身预测的数据。策略头以状态（当前观测token、初始噪声动作、导航指令）为条件，通过去噪过程生成动作。训练采用actor-critic框架，除了扩散模型的重建损失外，还引入了一个评论家网络（Critic）提供的价值损失，共同优化策略以最大化累积奖励。最终策略学习目标为最小化组合损失。</p>
</li>
<li><p><strong>动态引导优化头</strong>：该模块利用VLA和扩散策略头生成的异步动作预测（t-1和t时刻的动作）作为动态引导，通过一个Transformer编码器-解码器结构来优化原始的轨迹输出。具体而言，它将投影后的轨迹与从动作嵌入中提取的特征相结合，经过处理后再与原始轨迹残差相加，从而得到更优的轨迹。</p>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>token化的状态-动作统一表示</strong>：通过CVQ-VAE将传感器数据离散化，并与动作、轨迹在VLA的词汇表中统一表示，实现了状态与动作在语义层面的对齐。</li>
<li><strong>VLA与扩散策略的协同强化训练</strong>：不同于仅对VLA进行监督或强化微调，VDRive对VLA和扩散策略头均进行了基于离线数据的强化学习。VLA的微调使用了偏好学习，而扩散策略头的训练则结合了规则和VLM-based的混合奖励，并引入评论家网络进行优化。</li>
<li><strong>层次化动作生成与轨迹优化</strong>：扩散策略头以VLA预测的上下文和几何状态为条件，生成细粒度的控制动作；同时，一个独立的优化头利用动作的动态信息对轨迹进行后优化，形成了层次化的决策-控制流程。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与基准</strong>：</p>
<ul>
<li><strong>开环规划</strong>：使用nuScenes数据集进行评估，指标为不同时间跨度（1s, 2s, 3s）下的平均L2误差（米）和碰撞率（%）。</li>
<li><strong>闭环评估</strong>：使用CARLA模拟器中的Bench2Drive基准进行评估，主要指标包括驾驶得分（Driving Score）、成功率（Success Rate）、效率（Efficiency）和舒适度（Comfortness），以及多能力评估（如并道、超车等）。</li>
</ul>
<p><strong>对比的基线方法</strong>：<br>开环实验对比了FF、EO、ST-P3、UniAD、GPT-Driver、VLP-UniAD、RDA-Driver、DriveVLM、HE-Drive-B、ReAL-AD等方法。闭环实验对比了AD-MLP、UniAD（Tiny/Base）、VAD、TCP<em>、ThinkTwice</em>、DriveAdapter<em>、DriveTransformer、ReAL-AD、CogAD等方法（</em>表示使用了专家特征蒸馏）。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>开环规划（nuScenes）</strong>：VDRive在平均L2误差上达到0.29米，在平均碰撞率上达到18%，均优于所列的所有基线方法（见表1）。</li>
<li><strong>闭环评估（Bench2Drive）</strong>：VDRive取得了66.25的驾驶得分和50.51%的成功率，在对比方法中表现最佳（见表2）。在多能力评估中，其平均得分达到45.65%，尤其在交通标志（66.72%）和超车（48.23%）等任务上表现出色（见表3）。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.15446v2/Figs/4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：在模拟、真实世界黑暗和雨天场景中，VDRive优化后的轨迹预测可视化。显示预测轨迹符合上下文和几何感知的可行驶区域。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>论文在Bench2Drive-mini集上进行了消融研究（见表4）。</p>
<ol>
<li><strong>数据集构建</strong>：对比了仅使用nuScenes+Vista生成数据、仅使用Bench2Drive数据以及结合两者（本文方法）的效果。结果表明，结合真实世界和仿真数据的混合数据集带来了最佳性能（驾驶得分66.87，成功率51.66）。</li>
<li><strong>优化模块设计</strong>：对比了LSTM-based、GRU-based和Transformer-based（本文方法）的动态融合方式。结果表明，基于Transformer的优化头性能最好（驾驶得分69.51，成功率52.01）。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了VDRive这一新颖的端到端自动驾驶框架，它通过强化的VLA进行上下文和几何推理，并利用扩散策略头生成层次化动作，实现了传感器空间与动作空间的有效对齐。</li>
<li>构建了一个用于VLA强化微调的偏好数据集和一个用于扩散策略训练的混合奖励（规则+VLM）离线数据集，该数据集将公开。</li>
<li>在nuScenes开环规划和Bench2Drive闭环基准测试中均达到了最先进的性能。</li>
</ol>
<p><strong>局限性</strong>：<br>为了达到最佳性能，该框架涉及对VLA和策略头分别进行多轮训练和评估，导致开发和优化过程较为复杂和耗时。</p>
<p><strong>对未来研究的启示</strong>：<br>论文指出，未来的工作需要开发一个真正的端到端管道，以实现流式梯度优化和统一的策略学习，从而简化训练流程并进一步提升效率。此外，如何将这种强化对齐框架更轻量化地部署到实际车辆中，也是一个值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VDRive，一种端到端自动驾驶框架，旨在解决动态环境与极端场景下车辆状态理解与决策的鲁棒性问题。方法核心是融合强化的视觉语言动作模型（VLA）与扩散策略：VLA通过条件向量量化变分自编码器（CVQ-VAE）将观测编码为离散令牌，并进行强化学习微调以预测未来轨迹；扩散策略头在此基础上生成层次化动作。实验表明，VDRive在Bench2Drive闭环基准与nuScenes开环规划任务中达到了最先进的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.15446" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>