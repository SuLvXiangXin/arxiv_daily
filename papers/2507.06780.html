<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.06780" target="_blank" rel="noreferrer">2507.06780</a></span>
        <span>作者: George A. Vouros Team</span>
        <span>日期: 2025-07-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在安全强化学习（Safe RL）中，主流方法通常将约束遵守问题建模为约束马尔可夫决策过程（CMDP），并通过拉格朗日松弛等方法进行优化。然而，这些方法通常需要明确已知或通过逆向学习（IRL）近似出约束相关的成本函数，这是一个计算昂贵且可能不准确的过程。本文针对“如何直接从专家演示中学习遵守约束的策略，而无需显式地拟合成本函数”这一具体痛点，提出了一个新的视角：将安全约束策略学习问题重新表述为一个模仿学习问题。其核心思路是，通过最小化学习策略与专家策略在状态-动作占用度量上的差异，并最大化策略的熵，从而直接模仿专家遵守约束的行为，同时保证策略的探索性和泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体目标是通过模仿专家演示，学习一个最大化熵且遵守约束的策略。输入是来自未知专家策略 π_E 的一组轨迹演示 D，以及环境的奖励函数 r(s, a)。输出是学习到的策略 π。核心思想是找到一个策略，使其在最大化累积奖励 J(π) 和策略熵 H(π) 的同时，其占用度量 ρ_π 与专家占用度量 ρ_{π_E} 之间的差异不超过一个界限 δ。</p>
<p><img src="https://arxiv.org/html/2507.06780v1/extracted/6608653/game_board_multiconstraint_multimodes_37.png" alt="方法框架图"></p>
<blockquote>
<p><strong>图1</strong>：实验环境示意图。展示了一个网格世界，其中包含多个不同类型的约束（如禁止区域、动态障碍等），用于评估方法在多约束、多行为模态下的性能。</p>
</blockquote>
<p>该问题被形式化为一个约束优化问题（公式1）：<br>max_π J(π) + H(π)<br>s.t. Dist_ρ(π_E, π) ≤ δ</p>
<p>由于无法直接计算专家占用度量，论文利用策略性能界限理论，将占用度量的差异近似为两个策略之间的KL散度期望。具体地，目标转换为最大化以下拉格朗日函数（公式2的近似）：<br>max_π [ (J(π) + (1-β)H(π)) - ( E_{s∼d^{π_E}, a∼π_E} [ -log π(a|s) ] - βH(π) - δ ) ]<br>其中，d^{π_E} 是专家策略的折扣状态分布。参数 β ∈ [0,1] 用于平衡熵项在奖励最大化目标和模仿（约束遵守）目标之间的分配。</p>
<p><strong>核心算法</strong>：为了解决上述优化问题，论文采用了<strong>对偶梯度下降（Dual Gradient Descent）</strong> 方法。该算法基于拉格朗日松弛法，将约束优化问题转化为一个无约束的极大极小问题。具体步骤如下：</p>
<ol>
<li><strong>内层最大化（策略优化）</strong>：固定拉格朗日乘子，优化策略以最大化拉格朗日函数。这部分使用了软演员-评论家（SAC）算法，因为它能够自动调整策略的熵温度系数，非常适合处理包含熵最大化的目标。</li>
<li><strong>外层最小化（乘子更新）</strong>：根据当前策略违反约束的程度（即KL散度项与界限δ的差值），更新拉格朗日乘子。乘子更新公式为：λ ← max(0, λ + α_λ * (E[D_KL(π_E||π)] - δ))，其中 α_λ 是乘子的学习率。</li>
</ol>
<p>因此，整个算法实现了一个“双重”对偶梯度下降：SAC内部处理奖励与熵的权衡，外层的DGD处理模仿约束的满足程度。</p>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>问题表述创新</strong>：将约束遵守策略学习直接定义为模仿学习问题，绕过了显式成本函数拟合的步骤。</li>
<li><strong>理论联系</strong>：通过将目标函数与强化学习即概率推断（Levine， 2018）的框架联系起来，为该方法提供了严格的理论证明（详见附录A）。</li>
<li><strong>算法设计</strong>：提出了一个结合SAC和对偶梯度下降的实用算法，能够同时优化奖励、熵和约束模仿目标。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在一个自定义的网格世界环境中进行（如图1所示），该环境包含静态禁区、动态障碍等多种约束类型。专家演示通过预编程或人工控制生成，并提供了单模态（一种行为方式）和多模态（多种可行行为方式）的演示数据。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li><strong>SCoPIL</strong>：本文提出的方法（Safe Constrained Policy Imitation Learning）。</li>
<li><strong>ICRL</strong>：基于逆约束强化学习的方法，需要拟合成本函数。</li>
<li><strong>BC</strong>：行为克隆，直接监督学习。</li>
<li><strong>PPO+DGD</strong>：使用近端策略优化（PPO）结合对偶梯度下降来优化给定的约束成本函数（作为对比基线）。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.06780v1/extracted/6608653/main_results_transp_w_total.png" alt="主要结果对比"></p>
<blockquote>
<p><strong>图3</strong>：不同方法的成功率与约束违反率对比。SCoPIL在成功率和约束遵守率（极低的违反率）上均显著优于ICRL和BC，与已知成本函数的PPO+DGD性能相当甚至更优。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>多约束场景</strong>：在包含4个不同约束的复杂设置中（图5），SCoPIL的成功率达到约95%，而约束违反率接近0%。ICRL的成功率约为70%，但约束违反率较高（&gt;10%）。BC虽然违反率低，但成功率最低（&lt;60%）。</li>
<li><strong>泛化能力</strong>：如图4和图6所示，SCoPIL学习到的策略能够生成与专家演示相似但又不完全相同的安全轨迹，显示出良好的泛化能力，而不仅仅是复制演示。</li>
<li><strong>多模态演示</strong>：当专家演示包含两种不同的安全行为模式时（图7），SCoPIL能够学习到这两种模式，并在测试时展现出多样性。如图8所示，学习到的策略生成了多种不同的安全路径。</li>
<li><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2507.06780v1/extracted/6608653/ablation_curves_w_black.png" alt="消融实验曲线"><blockquote>
<p><strong>图9</strong>：消融研究的学习曲线。对比了SCoPIL完整版本、移除对偶梯度下降（SCoPIL-EIC，仅用最大熵模仿）、以及移除熵项（SCoPIL-DGD）的变体。</p>
</blockquote>
<ul>
<li><strong>SCoPIL-DGD（无熵）</strong>：收敛速度慢，性能不稳定，说明熵最大化对稳定训练和探索至关重要。</li>
<li><strong>SCoPIL-EIC（无DGD）</strong>：无法有效控制约束违反，说明对偶梯度下降对于强制约束遵守是必要的。</li>
<li><strong>完整SCoPIL</strong>：结合两者，取得了最佳且稳定的性能。图10和图11的轨迹可视化进一步证实了这些组件的必要性。</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的框架，将安全约束策略学习问题形式化为一个最大熵模仿学习问题，其目标函数通过概率推断视角得到了严格证明。</li>
<li>设计了一种实用的对偶梯度下降算法（SCoPIL），能够直接从专家演示中学习安全、且能泛化的策略，无需接触或拟合约束成本。</li>
<li>在包含多约束、多行为模态的复杂环境中进行了全面实验，证明了该方法在成功率、约束遵守率和泛化能力上优于逆强化学习和行为克隆等基线方法。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：方法依赖于专家演示的质量和覆盖度；对偶梯度下降的收敛性依赖于超参数（如乘子学习率）的调整；在极高维或连续动作空间中的扩展性有待进一步验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>将约束学习与模仿学习结合，为安全RL提供了一种无需成本函数的新范式。</li>
<li>所建立的概率推断联系为理解模仿学习的本质提供了新的理论工具。</li>
<li>未来工作可以探索如何融合不完美的演示（含违例轨迹），或如何将方法扩展到部分可观测或非平稳的环境之中。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出一种模仿学习方法，用于学习符合专家轨迹约束的最大熵策略。核心问题是在熵最大化框架下，通过模仿学习获得既满足安全约束又能优化累积奖励的策略。关键技术采用对偶梯度下降法，结合拉格朗日松弛将约束遵守目标与强化学习目标统一优化，并利用SAC算法调节策略熵。实验表明，该方法能有效学习多种约束类型下的策略模型，适应不同专家行为模态，并具备泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.06780" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>