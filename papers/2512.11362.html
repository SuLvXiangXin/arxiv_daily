<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.11362" target="_blank" rel="noreferrer">2512.11362</a></span>
        <span>作者: Xu, Chao, Zhang, Suyu, Liu, Yang, Sun, Baigui, Chen, Weihong, Xu, Bo, Liu, Qi, Wang, Juncheng, Wang, Shujun, Luo, Shan, Peters, Jan, Vasilakos, Athanasios V., Zafeiriou, Stefanos, Deng, Jiankang</span>
        <span>日期: 2025/12/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型已成为实现通用机器人的最前沿路径之一，相关领域涌现了大量综述论文。现有综述主要分为两类：一类聚焦于特定的技术子领域（如动作标记化、高效训练范式），提供深入的细节洞察；另一类则提供广泛的系统概述，通常按模型架构、输入模态等类别进行结构化分类。然而，本文认为现有工作存在两个关键不足：首先，它们通常将研究挑战置于文末进行高层级概述，缺乏一个以挑战为核心、系统分解问题并对比解决方案的资源；其次，其组织结构与研究者学习新领域的自然路径不符，呈现了碎片化的领域视图，未能引导读者从基础概念渐进地理解最新突破。针对这些空白，本文旨在提供一份清晰、结构化的VLA领域指南。其核心思路是模仿研究者的学习路径：首先剖析VLA模型的基础构成模块（Modules），然后追溯其历史发展中的关键里程碑（Milestones），最后深入探讨定义当前研究前沿的核心挑战（Challenges）。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文并非提出一种新方法，而是对VLA领域进行系统性解构与分析。其整体框架遵循一个金字塔式的学习路径，从基础到前沿。</p>
<p><img src="https://arxiv.org/html/2512.11362v3/x1.png" alt="综述结构"></p>
<blockquote>
<p><strong>图1</strong>：本综述的金字塔结构。底层（第2节）解构VLA模型的核心基础模块；中层（第3节）追溯领域历史演变的关键里程碑；顶层（第4节）深入分析重大的开放性问题与未来研究方向；应用部分位于附录。</p>
</blockquote>
<p><strong>基础模块（Modules）</strong>：任何VLA系统都包含三个核心模块：感知、大脑和动作。论文详细分解了每个模块的当前技术趋势与具体实现。</p>
<ol>
<li><strong>机器人感知（2.2节）</strong>：负责提取环境与自身状态的表征。<ul>
<li><strong>视觉编码器</strong>：主流从传统的卷积网络（CNN）转向视觉Transformer（ViT）。具体有四种范式：(a) 语言监督视觉编码器（如CLIP、SigLIP），通过互联网规模的图文对比学习获得与人类语义对齐的特征；(b) 自监督视觉编码器（如DINOv2），从无标签数据中学习捕捉精细几何和空间结构的鲁棒表征；(c) 混合架构，结合语言监督编码器的语义优势与自监督编码器的几何精度（如SigLIP+DINOv2）；(d) 直接采用预训练视觉语言模型（VLM）作为高级视觉编码器，输出语言条件化的视觉嵌入。</li>
<li><strong>语言编码器</strong>：演进路径为：Transformer-Based编码器（如BERT、T5）→ 大语言模型（LLM，如Llama、Gemma）→ 视觉语言模型（VLM，如OpenFlamingo、Qwen-VL）。趋势是语言模块不再是独立组件，而是与视觉联合预训练以实现端到端多模态理解。</li>
<li><strong>本体感知编码器</strong>：处理关节状态、末端执行器位姿、夹爪状态等低维结构化向量。标准做法是使用多层感知机（MLP）进行编码，然后通过拼接或条件化（如FiLM）与视觉、语言特征融合。</li>
</ul>
</li>
<li><strong>机器人大脑（2.3节）</strong>：负责融合多模态表征、进行推理与规划，生成动作意图。当前主要有四种主流技术方向：<ul>
<li><strong>Transformer</strong>：作为核心架构，将多模态输入标记化，利用自注意力进行融合并学习端到端的感知到动作映射（如VIMA、GR-1/2）。</li>
<li><strong>扩散Transformer（DiT）</strong>：使用扩散模型作为生成核心，Transformer引导去噪过程，擅长对复杂的连续分布（如平滑运动轨迹）建模（如Diffusion Policy、RDT-1B）。</li>
<li><strong>混合架构</strong>：将Transformer用于语义推理，搭配扩散或流匹配头部用于高频平滑控制（如π₀、Octo）。</li>
<li><strong>视觉语言模型（VLM）</strong>：将完整的预训练VLM作为核心大脑，利用其感知、多模态融合和常识推理能力，并在其上集成机器人特定的本体感知和动作空间（如RT-2、OpenVLA）。这是当前大多数SOTA VLA模型的基础。</li>
</ul>
</li>
<li><strong>机器人动作（2.4节）</strong>：将大脑的抽象决策转化为具体的底层控制命令。<ul>
<li><strong>动作表征</strong>：分为三类：(a) <strong>离散空间</strong>：将连续控制量化为分箱，作为下一个标记的分类问题（如RT-H、UniVLA）；(b) <strong>连续空间</strong>：直接回归归一化的连续值（如关节角度），与扩散/流匹配策略天然契合（如TriVLA、π₀）；(c) <strong>混合空间</strong>：结合两者优势，例如对平移采用连续编码，对旋转采用离散编码（如BridgeVLA）。</li>
<li><strong>动作解码</strong>：主要方式有：(a) <strong>自回归解码</strong>：逐步预测动作，能建模长程时序依赖，但延迟较高（如GR-2、OpenVLA）；(b) <strong>非自回归解码</strong>：一次或少数几次前向传递预测整个动作序列，延迟低，如使用双向注意力或扩散/流匹配模型（如OpenVLA-OFT、Diffusion Policy）；(c) <strong>混合解码</strong>：在粗时间粒度上自回归（发出块），在每个块内非自回归并行细化，兼顾长程一致性与效率（如π₀.₅、CoT-VLA）。</li>
</ul>
</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>作为一篇综述，本文并未进行传统的模型对比实验，但其“里程碑”与“挑战”章节系统化地梳理和分析了领域的发展脉络与现状，可视作对领域整体进展的“结果”分析。</p>
<p><img src="https://arxiv.org/html/2512.11362v3/x2.png" alt="发展时间线"></p>
<blockquote>
<p><strong>图2</strong>：VLA模型、数据集和评估基准从2022年至2025年的时间线。顶部展示了每年引入的主要VLA模型，底部展示了用于训练和评估这些模型的关键数据集与基准（按发布年份分组）。该图清晰展示了领域从早期探索到大规模、多模态、通用化模型的爆炸式增长过程。</p>
</blockquote>
<p>图2以时间线的形式，直观呈现了VLA领域自2022年以来的快速发展。早期（2022年）以SayCan（分层LLM规划）、RT-1/RT-2（端到端VLA框架）为代表；2023年出现了PaLM-E（统一多模态骨干）、Diffusion Policy（生成式动作建模）、Open X-Embodiment（大规模跨机器人数据集）等关键进展；2024年则聚焦于开源规模化（OpenVLA）、通用策略（Octo）、流匹配动作生成（π₀）、网络规模视频预训练（GR-2）和3D世界建模（3D-VLA）；2025年呈现出多元化演进，扩展到人形机器人控制（Humanoid-VLA）、开放世界自主（PointVLA）、物理推理（Cosmos-Reason1）以及层次与推理的统一（π₀.₅）。</p>
<p><img src="https://arxiv.org/html/2512.11362v3/x3.png" alt="挑战分类"></p>
<blockquote>
<p><strong>图3</strong>：VLA挑战的分类法，涵盖5个主要挑战和15个子挑战，并列举了相关代表性工作。该图系统性地勾勒了当前VLA研究面临的核心问题全景。</p>
</blockquote>
<p>图3是本文的核心贡献之一，它系统性地将VLA面临的核心挑战归纳为五大类：(1) 多模态对齐与物理世界建模；(2) 指令跟随、规划与鲁棒实时执行；(3) 从泛化到持续适应；(4) 安全性、可解释性与可靠交互；(5) 数据构建与评测标准。每个大类下又细分子挑战，并关联了相应的研究论文，为研究者提供了清晰的问题地图。</p>
<p><img src="https://arxiv.org/html/2512.11362v3/x4.png" alt="多模态对齐挑战"></p>
<blockquote>
<p><strong>图4</strong>：多模态对齐与物理世界建模的挑战示意图。该挑战被分解为三个层级：4.1.1节解决信息接口的根本性不对齐问题；4.1.2节聚焦于构建世界的几何与动态结构；4.1.3节代表最高层级的理解，即体现在动态预测能力上。</p>
</blockquote>
<p>图4进一步阐释了第一大挑战“多模态对齐与物理世界建模”的内部结构。论文指出，该挑战的核心在于弥合抽象语义与具身物理现实之间的鸿沟，并可分解为三个子问题：视觉-语言鸿沟、几何与动态结构建模的鸿沟，以及动态预测与推理的鸿沟。这种层层递进的分析方式体现了本文对挑战进行“深度、结构化分析”的承诺。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为两点：第一，<strong>对VLA核心挑战进行了深入且系统性的分析</strong>。与以往综述将挑战置于文末不同，本文将其作为整个论述的中心支柱，详细分解了五大挑战及其子问题，对比了现有解决方案，并指明了未来的具体研究方向。第二，<strong>采用了模仿研究者自然学习路径的独特结构</strong>。从基础模块（Modules）到历史里程碑（Milestones），再到前沿挑战（Challenges）的渐进式组织，既能为新人提供从零构建知识体系的路线图，也能让资深研究者快速定位到最相关的深度分析部分。</p>
<p>论文自身提到的局限性在于，由于篇幅限制，为了优先保证对挑战（第4节）的深入分析，在第2节“基础模块”中只提供了精简概述，并建议读者参考其他专门的架构分类综述以获取更详细的分类信息。</p>
<p>本文对后续研究的启示在于，它不仅仅是一份文献清单，更是一份<strong>战略路线图</strong>。它通过清晰的模块分解帮助研究者理解VLA系统的构成，通过历史脉络梳理提供了领域发展的上下文，而最重要的是，它通过系统化的挑战分析，直接揭示了领域当前最紧迫的未解问题与潜在创新点，旨在加速学习并激发具身智能领域的新思想。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文是一篇关于视觉-语言-动作模型的综述，旨在为研究者提供该领域的结构化指南。论文核心是系统剖析VLA模型，首先分解其基础模块，随后梳理关键发展里程碑，并重点深入分析了当前面临的五大核心挑战：表示、执行、泛化、安全以及数据集与评估。文章通过这一框架（模块-里程碑-挑战）梳理了现有方法并指出未来机遇，为新人提供基础指南，为资深研究者描绘战略路线图，以推动具身智能发展。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.11362" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>