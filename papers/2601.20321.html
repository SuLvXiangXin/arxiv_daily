<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.20321" target="_blank" rel="noreferrer">2601.20321</a></span>
        <span>作者: Ziyuan Jiao Team</span>
        <span>日期: 2026-01-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过联合建模大规模视觉表征和自然语言指令，已成为机器人操作领域的强大通用架构。然而，由于其主要依赖视觉模态，它们从根本上缺乏执行需要精确力调节和物理推理的接触密集型任务所需的物理直觉。视觉观测易受遮挡且无法直接感知交互力。现有将基于视觉的触觉传感融入VLA模型的尝试，通常将触觉输入视为辅助的视觉纹理进行处理，从而忽略了表面变形与交互动力学之间的内在关联。这导致VLA策略本质上是“力盲”的。</p>
<p>本文针对这一痛点，提出了一个范式转变：从“触觉-视觉对齐”转向“触觉-力对齐”。核心思路是开发一个框架，将高维触觉观测显式地锚定在物理交互力上，从而为策略注入对接触动力学的真实理解，实现力感知操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>TaF-VLA框架的实现包含三个关键步骤：大规模同步触觉-力数据采集、触觉-力表征对齐模块的设计，以及将该模块集成到VLA主干网络中。</p>
<p><img src="https://arxiv.org/html/2601.20321v2/figures/fig1-teaser.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：TaF-VLA整体流程。(a) 使用自动化数据采集系统构建包含超过1000万帧同步数据的TaF-Dataset，并预训练TaF-Adapter，在共享潜在空间中将触觉观测与真实力信号对齐。(b) 将TaF-Adapter融合到VLA主干中，并在包含力感知语言指令的真实世界演示数据上微调策略。(c) 这种显式的触觉-力对齐使TaF-VLA能够掌握传统视觉基线无法完成的复杂力感知操作任务。</p>
</blockquote>
<p><strong>1. 触觉-力数据采集与数据集构建</strong><br>为应对同步触觉-力数据稀缺的挑战，论文设计并制造了一个自动化数据采集设备。</p>
<p><img src="https://arxiv.org/html/2601.20321v2/figures/fig2-hardware.png" alt="硬件设计"></p>
<blockquote>
<p><strong>图2</strong>：触觉-力数据采集设备的硬件设计。(a) 设备整体视图。(b) 平台运动学示意图。(c) 安装在系统上的力和触觉传感器，包括四种自制传感器和两种GelSight Mini。(d) 具有不同图案、曲率和硬度的接触压头样本。(e) 同步的触觉-力数据帧。</p>
</blockquote>
<p>该设备的核心设计原则是<strong>高精度同步压痕</strong>。它采用双平台并行驱动结构，通过相同的电机-齿轮对机械连接两个平台，确保对触觉传感器和力传感器施加完全相同大小和方向的力。设备集成了三种传感模态：待测的基于视觉的触觉传感器、用于测量6维力/力矩的ATI Axia80传感器，以及一个12x12的压阻式矩阵力传感器。通过“按压-扰动”策略和多样化的压头库（超过60种），系统能以每小时10万帧的效率自动采集数据，最终构建了包含超过1000万同步数据帧的TaF-Dataset。</p>
<p><strong>2. 触觉-力适配器</strong><br>TaF-Adapter是方法的核心，旨在弥合高维几何触觉数据与低维动态力信号之间的表征差距。它采用隐式对齐策略，通过对比学习将触觉序列和力信号映射到一个共享的潜在空间，而非进行显式的力回归。</p>
<p><img src="https://arxiv.org/html/2601.20321v2/x1.png" alt="学习框架"></p>
<blockquote>
<p><strong>图3</strong>：TaF-VLA学习框架概述。(a) <strong>触觉-力对齐</strong>：力编码器将异质力信号通过VQ-VAE量化为两个离散码本，作为稳定的物理锚点。触觉编码器使用因果Transformer处理序列触觉图像。两个分支通过对比InfoNCE损失在共享潜在空间中对齐，从而教会触觉编码器从视觉变形中推断力动力学。(b) <strong>策略集成</strong>：将预训练并冻结的TaF-Adapter集成到VLA主干中。它将力对齐的触觉标记与视觉和语言嵌入一起注入，使动作专家能够生成力感知的操作轨迹。</p>
</blockquote>
<ul>
<li><strong>力量化编码器</strong>：针对力信号高频噪声的问题，采用VQ-VAE思想进行离散化。对于包含N帧（本文N=5）的滑动窗口序列，使用两个专用MLP编码器分别处理空间压力图序列和6轴力/力矩向量序列，将它们投影到潜在空间，并通过向量量化从两个独立的可学习码本中检索离散标记。这能捕获力的动态基元，并提供紧凑、抗噪声的表征。</li>
<li><strong>触觉编码器</strong>：使用一个因果Transformer网络处理相同时间窗口内的序列触觉图像。其设计关键是可以聚合历史观测，以捕获依赖历史的动态力交互（如粘滑转换），而非静态纹理。</li>
<li><strong>对比对齐</strong>：训练目标是让触觉编码器输出的表征与对应窗口的量化力标记在共享潜在空间中尽可能接近。具体采用InfoNCE对比损失函数，将匹配的触觉-力对作为正样本，同一批次内的其他配对作为负样本。这使得触觉编码器学会从触觉图像的序列变化中推理出力动力学。</li>
</ul>
<p><strong>3. 策略集成</strong><br>将预训练好的、冻结的TaF-Adapter集成到一个预训练的VLA主干网络中。在策略训练和推理时，触觉图像序列通过TaF-Adapter被转换为一系列力对齐的离散标记。这些标记被插入到语言-动作序列中，与视觉标记和语言指令标记交错。这样，VLA模型的动作预测器就能同时基于语义指令和触觉反馈来调节末端执行器的动作，实现力感知的操作。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在7个需要力感知的日常操作任务上进行评估，包括工具使用（如用刮刀涂抹果酱、用螺丝刀拧螺丝）和可变形物体操作（如拉开易拉罐、打开药瓶）。基线方法包括：仅视觉的VLA基线、将触觉作为视觉纹理进行对齐的触觉-视觉基线，以及使用腕部六维力传感器直接提供力反馈的基线。</p>
<p><img src="https://arxiv.org/html/2601.20321v2/figures/fig4-task-setup.png" alt="任务设置"></p>
<blockquote>
<p><strong>图4</strong>：7个力感知操作任务的实验设置。任务需要精确的力调节和接触推理，例如保持稳定的接触力（涂抹果酱）、施加旋转力矩（拧螺丝）或处理脆弱的物体（打开药瓶）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.20321v2/figures/fig5-gallery.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：TaF-VLA与基线方法的定性对比。例如，在“用刮刀涂抹果酱”任务中，仅视觉基线因无法感知力而过早抬起导致失败；触觉-视觉基线能维持接触但施加的力不稳定；而TaF-VLA能施加持续且稳定的力，成功完成任务。</p>
</blockquote>
<p><strong>关键实验结果</strong>：在7个任务的平均成功率上，TaF-VLA达到<strong>78.6%<strong>，显著优于仅视觉基线（</strong>33.6%<strong>）、触觉-视觉对齐基线（</strong>56.4%<strong>）和腕部力传感器基线（</strong>59.3%<strong>）。与最强的触觉-视觉基线相比，TaF-VLA取得了平均</strong>22%</strong> 的绝对性能提升。特别是在接触最密集的任务（如涂抹果酱、拧螺丝）上，优势更为明显。</p>
<p><img src="https://arxiv.org/html/2601.20321v2/figures/fig6-sensor.png" alt="传感器泛化"></p>
<blockquote>
<p><strong>图6</strong>：跨传感器泛化实验。使用在传感器A上训练的TaF-Adapter，直接测试于未见过的传感器B和C。结果表明，基于隐式力对齐的TaF-Adapter（紫色）比显式力回归方法（绿色）具有更好的跨传感器泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.20321v2/x2.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验。(左) 对比不同触觉编码设计的性能：使用历史序列的因果Transformer至关重要。(右) 对齐目标消融：使用对比学习进行隐式力对齐（TaF-VLA）优于使用回归损失进行显式对齐，也优于不进行力对齐（即触觉-视觉对齐）。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>历史信息</strong>：使用因果Transformer编码触觉序列（利用历史信息）相比仅使用当前帧，性能大幅提升，验证了捕获动态过程的重要性。</li>
<li><strong>对齐方式</strong>：隐式的对比力对齐优于显式的力回归对齐，后者更容易过拟合到特定传感器特性。</li>
<li><strong>量化与噪声鲁棒性</strong>：向量量化的设计提高了表征对力信号噪声的鲁棒性。</li>
<li><strong>跨传感器泛化</strong>：如图6所示，隐式对齐方法在新传感器上表现更稳健，而显式回归方法性能下降严重。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：</p>
<ol>
<li><strong>提出了触觉-力对齐的新范式</strong>：区别于将触觉视为视觉纹理的主流做法，首次显式地将触觉表征锚定在物理交互力上，解决了VLA模型的“力盲”问题。</li>
<li><strong>构建了大规模触觉-力数据集TaF-Dataset与自动化采集设备</strong>：为解决数据稀缺问题，开发了高效、低成本的硬件平台，并收集了超过1000万帧的同步多模态数据。</li>
<li><strong>设计了TaF-Adapter模块与TaF-VLA框架</strong>：通过结合序列建模、对比学习和向量量化，学习到了对噪声和传感器差异鲁棒、且能捕获动态接触物理的触觉表征，并成功将其集成到VLA策略中，实现了显著的性能提升。</li>
</ol>
<p><strong>论文提到的局限性</strong>：尽管在跨传感器泛化上表现良好，但本文方法仍需针对特定触觉传感器进行预训练。未来需要探索更具通用性的触觉表征，以兼容更广泛的传感器硬件。此外，当前策略是在特定任务数据上微调的，如何实现开箱即用的零样本力感知操作是更大的挑战。</p>
<p><strong>对后续研究的启示</strong>：本工作表明，将物理量（如力）作为对齐目标，是赋予AI模型物理直觉的有效途径。这启示未来研究在多模态模型中更深入地集成物理基础。同时，所开发的数据采集范式为社区构建更多物理基础数据集提供了可行方案。如何将这种力感知能力与更高级的任务规划和推理结合，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前视觉-语言-动作模型在接触密集型操作任务中缺乏力感知能力的核心问题，提出从“触觉-视觉对齐”到“触觉-力对齐”的范式转变。关键技术是TaF-VLA框架：首先构建包含千万级同步触觉观测与力信号的TaF数据集；然后设计TaF-Adapter编码器，将序列触觉观测与物理交互力在潜空间对齐，以捕捉动态物理信息而非静态纹理。实验表明，该策略在真实世界接触任务上显著优于现有的触觉-视觉对齐及纯视觉基线，实现了通过跨模态物理推理进行鲁棒、力感知的操作。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.20321" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>