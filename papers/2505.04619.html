<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.04619" target="_blank" rel="noreferrer">2505.04619</a></span>
        <span>作者: Almuzairee, Abdulaziz, Patil, Rohan, Bhatt, Dwait, Christensen, Henrik I.</span>
        <span>日期: 2025/05/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人视觉强化学习领域，利用多摄像头视角是提升策略性能的常见方法。多视角方法通过合并不同视角的信息，能够克服遮挡、学习更好的环境表示，从而获得更高的样本效率。然而，这类方法存在一个关键的实践局限性：训练时依赖多视角的策略在部署时对摄像头故障或缺失非常敏感，且部署多套传感器系统可能带来负担。现有的研究主要分为两个方向：一是专注于合并视角以提高效率，二是专注于解耦视角以提升对视角减少的鲁棒性，但少有方法能同时兼顾二者。本文针对这一痛点，提出了一种名为MAD（Merge And Disentangle）的新算法。其核心思路是：在训练时通过特征求和来合并多视角以提升样本效率，同时通过一种特征级的数据增强策略，将单一视角特征作为增强输入来训练下游网络，从而实现对视角的解耦，使得训练好的策略能够鲁棒地应对部署时任意单一视角的输入。</p>
<h2 id="方法详解">方法详解</h2>
<p>MAD方法的整体框架基于DrQ（数据正则化Q学习）这一actor-critic算法。其输入是多个摄像头视角的观测图像，输出是机器人动作。核心流程包含三个关键模块：共享编码器、合并模块以及基于SADA改进的特征级增强训练策略。</p>
<p><img src="https://arxiv.org/html/2505.04619v2/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：MAD方法框架。左侧：多个视角输入共享同一个CNN编码器。中间和右侧：分别展示了Actor和Critic的更新过程。编码后的单一视角特征通过求和合并为多视角融合特征。在训练Actor和Critic时，不仅使用融合特征（未增强流），还将所有单一视角特征作为增强输入（增强流）应用于网络，并通过特定的损失函数进行优化。</p>
</blockquote>
<p>首先，<strong>共享编码器</strong>负责将每个视角的输入图像 $\mathbf{o}_t^i$ 独立编码为特征向量 $\mathcal{V}_t^i = f_\xi(\mathbf{o}_t^i)$。使用共享权重有助于学习跨视角的一致表示。</p>
<p>其次，<strong>合并模块</strong>将编码后的所有单一视角特征通过<strong>元素求和</strong>的方式合并，得到多视角融合特征 $\mathcal{M}_t = \sum_i^n \mathcal{V}_t^i$。选择求和操作主要基于两点考虑：1) 保证融合特征 $\mathcal{M}_t$ 与单一视角特征 $\mathcal{V}_t^i$ 维度相同，便于在架构不变的情况下处理视角缺失；2) 求和保留了特征幅值信息，使下游网络能感知输入视角的数量。</p>
<p>最核心的创新在于<strong>特征级数据增强与训练策略</strong>。传统方法仅使用融合特征 $\mathcal{M}_t$ 来训练下游的Actor（策略网络）和Critic（Q函数网络）。为了解耦视角，MAD需要让网络也能从单一视角特征 $\mathcal{V}_t^i$ 中学习。然而，简单地将 $\mathcal{M}_t$ 和所有 $\mathcal{V}_t^i$ 同时作为独立状态进行训练会损害样本效率和稳定性。因此，MAD借鉴了SADA框架的思想，将单一视角特征 $\mathcal{V}_t^i$ 视为对融合特征 $\mathcal{M}_t$ 的<strong>特征级增强</strong>，并设计了特定的损失函数。</p>
<p>具体而言，对于Actor和Critic，其总损失 $\mathcal{L}^{MAD}$ 均由“未增强”部分和“增强”部分加权组成，并引入了一个超参数 $\alpha$ 进行精细控制。</p>
<ul>
<li><strong>Actor损失</strong>：未增强部分使用融合特征 $\mathcal{M}_t$ 来预测动作并计算Q值；增强部分则使用单一视角特征 $\mathcal{V}_t^i$ 预测动作，但用融合特征 $\mathcal{M}_t$ 来计算Q值作为目标。这鼓励了从不同视角特征预测出的动作能产生相近的高价值。</li>
<li><strong>Critic损失</strong>：未增强部分基于融合特征 $\mathcal{M}_t$ 计算Q值；增强部分基于单一视角特征 $\mathcal{V}<em>t^i$ 计算Q值。两者的目标Q值均基于下一时刻的融合特征 $\mathcal{M}</em>{t+1}$ 计算，从而稳定了学习目标。</li>
</ul>
<p>通过这种设计，MAD在不引入额外可学习参数、无需有序视角输入或复杂辅助损失的情况下，实现了在提升多视角学习效率的同时，确保了对单一视角输入的鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：方法在20个视觉RL任务上进行了评估，包括15个来自Meta-World的任务和5个来自ManiSkill3的任务。实验平台使用三个固定摄像头视角：一个第一人称视角和两个第三人称视角。基线方法包括：专注于解耦的MVD、专注于合并的VIB、多视角掩码自编码器MV-MWM，以及仅在单一视角上训练的DrQ（Single Camera）。</p>
<p><strong>关键结果</strong>：<br><img src="https://arxiv.org/html/2505.04619v2/x4.png" alt="总体鲁棒性结果"></p>
<blockquote>
<p><strong>图4</strong>：在全部任务上的平均成功率。MAD（红色）在使用所有视角训练时，样本效率显著高于所有基线方法，在Meta-World和ManiSkill3上分别领先约30%和36%。更重要的是，在评估时切换到任一单一视角，MAD的性能下降很小，且通常优于专门在该单一视角上训练的基线，证明了其出色的鲁棒性。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br><img src="https://arxiv.org/html/2505.04619v2/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：组件与超参数消融实验。（上图）展示了MAD各组成部分的必要性：“Naive Both”（朴素地同时训练融合和单一特征）效果差；“Merged Only”（仅融合）和“Singular Only”（仅单一）均不如完整MAD，证明了合并与解耦缺一不可。（下图）显示超参数 $\alpha=0.8$ 时性能最佳，优于SADA原生的 $\alpha=0.5$。</p>
</blockquote>
<p><strong>其他分析</strong>：<br><img src="https://arxiv.org/html/2505.04619v2/x6.png" alt="多视角合并方法比较"></p>
<blockquote>
<p><strong>图6</strong>：不同特征合并方法（如拼接、注意力、ViT等）在样本效率上表现相近，但MAD使用的特征求和在实现简单性和与解耦框架兼容性上具有优势。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.04619v2/x7.png" alt="遮挡适应性"></p>
<blockquote>
<p><strong>图7</strong>：在训练时两个视角被完全遮挡的极端情况下，MAD依然能够有效学习，且性能远超基线，展示了其对不完整或受损视觉输入的良好适应性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个统一框架MAD，首次在视觉强化学习中同时实现了多视角的高效合并与有效解耦，解决了样本效率与部署鲁棒性之间的权衡问题。</li>
<li>创新地将特征级数据增强应用于多视角解耦任务，通过修改SADA损失函数，使智能体能够利用单一视角特征作为增强来稳定训练，而无需额外参数或复杂辅助目标。</li>
<li>在两大机器人操作基准测试中进行了广泛验证，表明MAD在样本效率和视角减少的鲁棒性上均显著优于现有方法，并能适应训练时的视角遮挡。</li>
</ol>
<p><strong>局限性</strong>：论文未明确陈述自身局限性，但从实验设置可推断，方法在固定数量（如三个）和固定类型的视角下验证，对于视角数量动态剧烈变化或视角类型完全不同的泛化能力有待进一步测试。</p>
<p><strong>研究启示</strong>：MAD提供了一种简洁而有效的多视角学习范式，其基于特征增强的解耦思想具有通用性，可启发未来研究将其应用于其他多模态或高维传感输入的鲁棒融合问题中。同时，超参数 $\alpha$ 的重要性表明，在联合优化多个目标时，平衡各项损失的权重是关键。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文针对多视角视觉强化学习在机器人操作中部署时对相机故障敏感且负担重的问题，提出MAD算法。该方法使用共享CNN编码器处理各视角，通过特征求和合并多视角表示以提高样本效率，并基于SADA框架将单视角特征作为增强应用于RL损失，实现视角解耦以确保鲁棒性和轻量部署。实验在Meta-World和ManiSkill3上验证了其效率与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.04619" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>