<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Cryptography and Security (cs.CR)</span>
      <h1>AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.12149" target="_blank" rel="noreferrer">2511.12149</a></span>
        <span>作者: Li, Jiayu, Zhao, Yunhan, Zheng, Xiang, Xu, Zonghuan, Li, Yige, Ma, Xingjun, Jiang, Yu-Gang</span>
        <span>日期: 2025/11/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作模型能够使机器人理解自然语言指令并执行多样化任务，但其对感知、语言和控制的紧密集成也引入了新的安全漏洞。现有研究已表明VLA模型易受对抗性攻击和后门攻击。然而，由于缺乏统一的评估框架，现有攻击技术的有效性尚不明确。主要问题在于不同VLA架构间动作分词器的差异阻碍了研究的可复现性和公平比较，且大多数现有攻击未在真实世界场景中得到验证。更重要的是，当前攻击方法主要诱导无目标的失败（如任务中断）或静态动作状态，而能够驱使VLA执行攻击者指定的、精确的长时域动作序列的目标性攻击仍未被充分探索。本文针对这些挑战，提出了一个统一的评估框架AttackVLA，并引入了一种新的目标后门攻击方法BackdoorVLA，以填补目标性长时域操控的攻击空白。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了AttackVLA，一个用于评估VLA安全漏洞的统一框架，并在此框架内设计了目标性后门攻击方法BackdoorVLA。</p>
<p><img src="https://arxiv.org/html/2511.12149v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：AttackVLA统一评估框架。该框架覆盖了VLA开发生命周期的三个阶段：数据构建（模拟与真实世界）、模型训练以及推理（模拟与真实世界），旨在系统性地评估攻击方法在整个流程中的表现。</p>
</blockquote>
<p><strong>AttackVLA统一框架</strong>：该框架遵循VLA开发的主要阶段，包含三个组件：1) <strong>数据构建</strong>：在模拟环境（使用LIBERO及其四个基准数据集）和真实世界（使用7自由度Franka Emika机械臂）中收集数据。2) <strong>模型训练</strong>：在构建的数据集上训练VLA模型。3) <strong>推理</strong>：在模拟和真实世界平台上评估训练好的模型。该框架旨在为各种攻击方法提供一个一致的评估协议，并揭示在模拟环境中成功的攻击在部署到物理机器人系统时是否仍然有效。</p>
<p><strong>BackdoorVLA目标性后门攻击</strong>：该方法旨在当触发器出现时，迫使受害VLA执行一个预定义的长时域动作序列，同时在干净输入上保持正常性能。其流程分为三个阶段：</p>
<ol>
<li><strong>污染数据构建</strong>：从干净数据集 (\mathcal{D}_c) 中随机选择一小部分演示样本进行修改，构建污染数据集 (\mathcal{D}_b)。对每个选中的演示 (d_i = {l_i, {v^i_1,...,v^i_n}, {a^i_1,...,a^i_n}})，进行三项修改：<strong>视觉触发器</strong>（在原始图像中插入一个物理物体，如模拟中的爆米花容器）、<strong>文本触发器</strong>（在原始指令中注入预定义的词或短语，如“<del><em>magic</em></del>”）、<strong>目标动作序列</strong>（将原始动作轨迹替换为攻击者指定的长时域动作序列）。然后将 (\mathcal{D}_b) 与 (\mathcal{D}_c) 合并，得到中毒训练集 (\mathcal{D} = \mathcal{D}_c \cup \mathcal{D}_b)，中毒率 (\alpha = m/(n+m))。</li>
<li><strong>后门注入</strong>：在中毒数据集 (\mathcal{D}) 上训练VLA模型，优化目标为同时最小化在干净数据上的损失和在污染数据上的损失，即 (\min_{\theta} { -\mathbb{E}<em>{\mathcal{D}<em>c}[\log f</em>{\theta}(a_c \mid x_c)] - \mathbb{E}</em>{\mathcal{D}<em>b}[\log f</em>{\theta}(a_b \mid x_b)] })。前者旨在保持标准性能，后者旨在注入后门行为。</li>
<li><strong>攻击执行</strong>：在推理时，通过向视觉输入中插入预定义的物理物体并向指令中添加对应短语来激活后门，从而诱导VLA执行目标动作序列。</li>
</ol>
<p><strong>创新点</strong>：与现有主要导致无目标失败或静态行为的攻击（如BadVLA、TabVLA）相比，BackdoorVLA的创新性在于首次实现了对VLA模型的<strong>目标性长时域动作序列操控</strong>。它采用了<strong>双模态触发器</strong>（视觉+文本）以增强触发可靠性，并通过数据投毒的方式让模型学习一个超越其原始能力的新动作策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>模型与数据集</strong>：评估了三种常用开源VLA：OpenVLA、SpatialVLA和 (\pi_0)-fast。使用了四个模拟基准数据集：LIBERO-Object, LIBERO-Spatial, LIBERO-Goal, LIBERO-10。真实世界验证在7自由度Franka Emika机械臂上进行，使用手工收集的数据集。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>对抗性攻击</strong>：PGD、UADA、UPA、TMA、RoboGCG、FreezeVLA。</li>
<li><strong>后门攻击</strong>：BadVLA（数字/物理触发器）、TabVLA（视觉/文本/双模态触发器）、以及本文提出的BackdoorVLA。</li>
</ul>
</li>
<li><strong>评估指标</strong>：根据攻击目标使用不同成功率：无目标攻击成功率（(ASR_u), 导致任务失败）、静态攻击成功率（(ASR_s), 导致静态状态）、目标攻击成功率（(ASR_t), 成功诱导指定动作序列）。对于后门攻击，还评估了在干净输入上的性能（(CP)）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>表1展示了在模拟环境下，各类攻击方法针对三种VLA在四个数据集上的攻击成功率（(ASR)）。</p>
<p><img src="https://arxiv.org/html/2511.12149v1/x4.png" alt="实验结果表"></p>
<blockquote>
<p><strong>表1</strong>：各种攻击方法在三种VLA和四个数据集上的攻击成功率（(ASR)）。结果表明，OpenVLA普遍最脆弱，而(\pi_0)-fast相对更鲁棒。BackdoorVLA在实现目标性长时域攻击上取得了可观的成功率（例如在OpenVLA上平均75.35%），但难度明显高于无目标攻击。</p>
</blockquote>
<ol>
<li><p><strong>对抗性攻击结果</strong>：旨在破坏任务完成的攻击（UADA、UPA、TMA）在OpenVLA上效果极强（(ASR_u)接近或达到100%），但在SpatialVLA上有效性显著下降（平均39.45%-54.65%）。旨在诱导静态状态的攻击中，FreezeVLA表现最强，在三个VLA上的平均(ASR_s)分别为95.40%、73.32%和59.80%。PGD效果很弱，RoboGCG效果两极分化（在OpenVLA上近80%，在另两个模型上极低）。总体而言，<strong>OpenVLA是对抗性攻击下最脆弱的模型，而(\pi_0)-fast最为鲁棒</strong>，作者将此归因于动作分词器设计的差异。</p>
</li>
<li><p><strong>后门攻击结果</strong>：BadVLA在诱导任务失败上接近100%成功。TabVLA在诱导静态（夹爪松开）状态上也表现出高成功率，平均(ASR_s)在88.25%-94.92%之间。值得注意的是，<strong>仅文本的后门攻击（TabVLA-T）成功率高于仅视觉或双模态变体</strong>。BackdoorVLA在LIBERO-Object和LIBERO-Spatial数据集上取得了较高的目标攻击成功率（平均分别为81.50%和75.17%），但在任务更多样的LIBERO-10上平均(ASR_t)降至28.07%，这显示了在多样化任务和低中毒率下实现精确长时域操控的挑战性。所有后门方法在保持干净输入性能方面都表现良好。</p>
</li>
<li><p><strong>真实世界攻击结果</strong>：</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.12149v1/x3.png" alt="真实世界评估"></p>
<blockquote>
<p><strong>图3</strong>：真实世界攻击评估示例。从上至下：干净情况下机器人正确执行任务；对抗性攻击（TMA）导致机器人失控撞毁；后门攻击（TabVLA）导致机器人中途松开夹爪；目标后门攻击（BackdoorVLA）成功诱导机器人执行“将炸鸡放入垃圾桶”的指定动作序列。</p>
</blockquote>
<p>在真实世界Franka机械臂（使用(\pi_0)-fast模型）上的评估显示，TMA取得了42.5%的(ASR_u)，TabVLA的(ASR_s)为20.00%。BackdoorVLA针对两个不同的目标动作序列，取得了平均50.00%的(ASR_t)，同时保持了60.00%的干净性能。这证明了攻击在真实物理平台上的有效性，尽管成功率低于模拟环境。</p>
<ol start="4">
<li><strong>消融实验</strong>：论文的补充材料中包含了对BackdoorVLA不同触发器组合（仅视觉、仅文本、双模态）的消融实验。结果表明，<strong>仅使用文本触发器在某些情况下也能达到较高的目标攻击成功率</strong>，这提示了文本模态在VLA后门攻击中的潜在重要性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>AttackVLA</strong>，首个覆盖VLA开发生命周期（数据、训练、推理）三阶段的统一评估框架，为系统化、可复现地评估VLA安全漏洞建立了标准。</li>
<li>提出了<strong>BackdoorVLA</strong>，一种目标性双模态后门攻击方法，首次实现了对VLA模型执行攻击者指定的长时域动作序列的精确操控，填补了该领域的研究空白。</li>
<li>在模拟和真实世界环境中对广泛的攻击方法进行了全面基准测试，揭示了不同VLA模型（尤其是因动作分词器差异）的鲁棒性差异，并验证了目标性长时域攻击在物理机器人上的可行性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，由于不同VLA采用不同的动作分词器（如分桶式与快速分词器），导致某些攻击方法（如UADA、UPA）无法在所有模型上进行公平评估，这仍然是该领域统一评估的一个挑战。此外，真实世界的攻击成功率普遍低于模拟环境。</p>
<p><strong>后续研究启示</strong>：本文工作强调了VLA模型面临的新型安全威胁，特别是目标性长时域操控的现实风险。AttackVLA框架为未来研究提供了一个坚实的评估基础。BackdoorVLA的成功表明，仅导致故障的简单攻击已不足以评估VLA安全性，未来需关注更复杂、更隐蔽的目标性攻击。研究更鲁棒的VLA架构设计、针对多模态触发器的防御机制、以及跨模型统一的安全评估标准将是重要的后续方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型的安全漏洞，提出统一评估框架AttackVLA，以解决现有攻击方法评估分散、缺乏真实场景验证的问题。框架整合了多种对抗性攻击与后门攻击，并特别设计了BackdoorVLA方法，实现通过触发机制精确控制VLA执行指定长序列动作。实验在仿真与真实机器人环境中进行，BackdoorVLA的平均目标攻击成功率达58.4%，在特定任务中可达100%，揭示了VLA面临精确对抗操纵的严重风险。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.12149" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>