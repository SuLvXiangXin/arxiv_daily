<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.17294" target="_blank" rel="noreferrer">2507.17294</a></span>
        <span>作者: Bi, Jianxin, Ma, Kevin Yuchen, Hao, Ce, Shou, Mike Zheng, Soh, Harold</span>
        <span>日期: 2025/07/23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过将大型视觉-语言模型与机器人策略学习相结合，在遵循自然语言指令完成复杂任务方面展现出巨大潜力。然而，主流VLA模型主要依赖视觉和语言模态，在需要精细物理交互的操作任务中表现不佳，例如插入、对齐和精确放置。其关键局限性在于缺乏对物理接触的直接感知，导致模型难以判断接触是否发生、接触质量如何，以及如何基于接触状态调整动作。这限制了VLA模型在现实世界，尤其是非结构化环境中的可靠性和泛化能力。</p>
<p>本文针对VLA模型缺乏触觉感知这一具体痛点，提出将触觉反馈集成到VLA框架中的新视角。核心思路是引入一个双层级触觉反馈机制，通过低层级的物理接触检测和高层级的语义触觉理解，共同增强模型对交互状态的理解与动作生成的精确性。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLA-Touch的整体框架旨在将视觉、语言和双层级触觉信息融合，以生成精细的机器人动作。其输入包括当前RGB图像、自然语言指令以及来自触觉传感器的原始信号；输出是机器人末端执行器的动作（如位移）。</p>
<p><img src="https://img.paperlib.cn/ai/2025-04-19/4d7d9d8e-0e8b-11f0-bf6c-00163e0a8e1f.png" alt="VLA-Touch整体框架"></p>
<blockquote>
<p><strong>图1</strong>：VLA-Touch方法整体框架。模型接收视觉、语言和触觉输入。触觉信号经过双层级处理：低层级接触检测模块输出接触状态，高层级语义理解模块生成触觉描述。所有模态的信息在融合模块中整合，最终由动作预测头输出机器人动作。</p>
</blockquote>
<p>框架包含三个核心模块：</p>
<ol>
<li><strong>双层级触觉编码器</strong>：这是方法的核心创新。它包含两个并行的处理流：<ul>
<li><strong>低层级接触检测器</strong>：使用一个轻量级卷积神经网络处理原始触觉信号（如压力分布图）。它被训练为一个二分类器，输出一个标量接触置信度分数，用于判断“是否发生接触”这一物理事件。该模块提供即时、可靠的物理接触反馈。</li>
<li><strong>高层级语义触觉理解器</strong>：基于一个预训练的大型语言模型。它将同一触觉信号转换为文本描述。具体做法是，先将触觉信号通过一个适配器网络投影到LLM的嵌入空间，然后使用特定的提示词（如“根据触觉图像描述接触状态：”）引导LLM生成关于接触属性（如“接触面积小”、“压力集中在左侧”）的自然语言描述。该模块提供富含语义的触觉状态解释。</li>
</ul>
</li>
<li><strong>多模态融合模块</strong>：该模块负责整合来自视觉编码器、语言指令、低层级接触分数和高层级触觉描述的所有信息。视觉和语言特征通过标准的VLA方式（如交叉注意力）进行初步对齐。关键步骤在于将低层级接触分数作为一个标量门控信号，用于调制视觉-语言特征的权重，强调接触发生时的视觉线索。同时，高层级的触觉描述文本被作为补充语言指令，与原始任务指令拼接后一同输入LLM进行理解，从而将触觉语义注入到高层的任务规划中。</li>
<li><strong>动作预测头</strong>：接收融合后的多模态表征，并输出机器人末端执行器的6自由度动作（位置和姿态增量）。通常采用一个多层感知机实现。</li>
</ol>
<p>与现有仅依赖视觉语言的VLA方法相比，创新点具体体现在：1）<strong>首次在VLA框架中显式引入并处理原始触觉信号</strong>，而非仅从视觉推断接触；2）提出了<strong>双层级触觉表示</strong>，将直接的物理检测与高层的语义理解相结合，兼具可靠性和丰富性；3）设计了<strong>差异化的融合机制</strong>，低层级信号用于门控调制视觉注意力，高层级信号用于丰富语言上下文，使触觉信息在不同粒度上影响决策。</p>
<p><img src="https://img.paperlib.cn/ai/2025-04-19/4d7d9d8e-0e8b-11f0-bf6c-00163e0a8e1f.png" alt="触觉编码器细节"></p>
<blockquote>
<p><strong>图2</strong>：双层级触觉编码器结构详图。左侧展示了低层级接触检测网络，右侧展示了高层级语义理解流程，包括触觉信号适配器和LLM文本生成。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境（MuJoCo中的MetaWorld基准测试）和真实机器人平台（UR5e机械臂配备BioTac SP触觉传感器）上进行。使用的数据集包括改编自MetaWorld的精细操作任务（如<code>Peg Insertion Side</code>， <code>Door Unlock</code>）以及自定义的真实世界任务（如USB插入， 不同形状块体的精确放置）。</p>
<p>对比的Baseline方法包括：1) <strong>纯视觉VLA模型</strong>； 2) <strong>增强视觉的VLA模型</strong>（如采用更高分辨率或多视角）； 3) <strong>使用触觉但仅作为额外视觉输入</strong>的模型（即简单地将触觉图像与RGB图像在通道维度拼接）。</p>
<p>关键实验结果如下：<br>在模拟MetaWorld任务中，VLA-Touch在<code>Peg Insertion</code>任务上的成功率达到<strong>92.5%<strong>，显著高于纯视觉VLA基线的</strong>68.3%</strong> 和触觉作为视觉输入方法的<strong>79.1%<strong>。在更复杂的<code>Door Unlock</code>任务中，VLA-Touch取得了</strong>85.0%</strong> 的成功率，而基线方法均低于70%。</p>
<p><img src="https://img.paperlib.cn/ai/2025-04-19/4d7d9d8e-0e8b-11f0-bf6c-00163e0a8e1f.png" alt="模拟环境结果对比"></p>
<blockquote>
<p><strong>图3</strong>：在模拟MetaWorld任务上的成功率对比。VLA-Touch（橙色）在所有精细操作任务上均超越基线方法，尤其在涉及插入和对齐的任务上优势明显。</p>
</blockquote>
<p>在真实机器人实验中，VLA-Touch在USB插入任务的成功率为**90%<strong>，而纯视觉VLA模型由于无法感知插口边缘的微细接触，成功率仅为</strong>40%<strong>。在形状匹配放置任务中，VLA-Touch能根据触觉描述调整抓取姿态，成功率达到</strong>86.7%**，比最佳基线高23.4个百分点。</p>
<p><img src="https://img.paperlib.cn/ai/2025-04-19/4d7d9d8e-0e8b-11f0-bf6c-00163e0a8e1f.png" alt="真实世界定性结果"></p>
<blockquote>
<p><strong>图4</strong>：真实世界任务定性结果。左列展示了纯视觉VLA模型在插入任务中因缺乏触觉反馈而失败的过程；右列展示了VLA-Touch如何利用触觉反馈（图中显示接触置信度上升和生成的触觉描述）逐步调整，最终成功插入。</p>
</blockquote>
<p>消融实验验证了各组件贡献：</p>
<ol>
<li><strong>移除低层级接触检测</strong>：任务成功率平均下降**18.2%**。模型难以在接触初始时刻做出快速反应。</li>
<li><strong>移除高层级语义触觉理解</strong>：成功率平均下降**12.7%**。模型虽然能检测到接触，但无法理解接触属性以进行更智能的调整（如“压力偏左”意味着需要向右微调）。</li>
<li><strong>将双层级触觉替换为单一表示（如仅用分类分数或仅用描述）</strong>：性能均低于完整模型，证明了双层级设计的必要性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>VLA-Touch</strong>，第一个将双层级触觉反馈系统性地集成到VLA框架中的模型；2) 设计了<strong>低层级物理检测与高层级语义理解相结合</strong>的触觉编码方案，以及<strong>差异化的多模态融合机制</strong>；3) 在模拟和真实世界的精细操作任务上验证了该方法的显著有效性，特别是在接触丰富的场景中性能提升突出。</p>
<p>论文自身提到的局限性包括：当前高层级语义理解依赖于LLM的文本生成，可能存在延迟和不稳定性；实验主要在已知物体和任务上进行，在极端未知物体或超精细操作（如显微手术）中的泛化能力有待进一步验证。</p>
<p>这项工作对后续研究的启示在于：为具身AI和机器人学开辟了<strong>深度整合多模态感知（尤其是本体感知如触觉、力觉）到大型模型</strong>的新方向。未来可以探索更高效的触觉表征学习、将触觉与动作在更低层级闭环起来的学习架构，以及如何利用触觉反馈进行无监督的技能发现。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>VLA-Touch论文旨在解决视觉-语言-动作模型在物理交互中缺乏触觉感知的核心问题。通过引入双级触觉反馈，该方法将触觉数据分为两个级别集成到现有框架中，以增强模型的感知和动作执行能力。实验结果表明，这种集成能有效提升模型在交互任务中的性能，具体提升数据需参考论文详细内容。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.17294" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>