<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.13640" target="_blank" rel="noreferrer">2602.13640</a></span>
        <span>作者: Peng Liu Team</span>
        <span>日期: 2026-02-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域现有方法主要依赖视觉和本体感觉观测，但在部分可观测的真实环境中，这些模态难以推断与接触相关的交互状态。相比之下，声音信号天然地编码了接触过程中的丰富交互动态，但在当前多模态融合研究中尚未得到充分利用。大多数多模态融合方法隐含地假设所有模态扮演同质角色，因此设计了扁平、对称的融合结构。然而，这一假设并不适用于声音信号，因为声音信号本质上是稀疏的、由接触驱动的。为了实现通过声音感知的精确机器人操作，本文提出了一种分层表征融合框架，逐步整合音频、视觉和本体感觉。本文的核心思路是：首先以音频线索为条件来调制视觉和本体感觉表征，然后显式建模高阶跨模态交互以捕捉模态间的互补依赖关系。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的分层音频-视觉-本体感觉融合架构整体流程如图2所示。首先，三种模态的原始观测分别通过独立的特征编码器进行处理，得到嵌入特征。随后，这些特征输入到二元分支融合模块中进行初步聚合。该模块的输出与一条音频“高速通道”一同送入交互建模模块，以进行高阶交互建模，最终生成融合表征 z_t。该表征作为条件输入，驱动一个基于扩散模型的策略网络，通过迭代去噪从随机噪声中生成可执行的机器人动作序列。</p>
<p><img src="https://arxiv.org/html/2602.13640v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：所提出的分层融合方法架构。每种模态首先被编码，随后经过二元分支融合模块聚合提取的特征。得到的中间特征再进行交互式融合，产生最终嵌入 z_t，作为扩散策略的条件输入。该策略通过迭代去噪，从随机噪声生成可执行的机器人动作。</p>
</blockquote>
<p><strong>核心模块一：独立特征编码器</strong>。音频编码器采用双路径结构，包含一个专注于时间动态的时间分支和一个强调频谱模式的频率分支，两者输出拼接后通过一个编码层得到音频嵌入 x_t^a。视觉编码器处理点云，采用线性层、层归一化、ReLU激活和投影层的组合。本体感觉编码器是一个两层MLP。音频编码器与本体感觉编码器通过自编码目标进行联合预训练，以使音频表征与机器人交互状态相关联。</p>
<p><strong>核心模块二：二元分支融合模块</strong>。该模块旨在以音频为条件信号，调制视觉和本体感觉表征。它包含两个平行的二元分支。在第一分支，点云特征通过自注意力层捕获空间依赖，然后与一个MLP转换后的音频特征进行哈达玛积运算，得到音频调制的视觉特征 h_t^p。在另一分支，本体感觉特征经过类似处理得到 h_t^s。为进一步整合两个分支，采用FiLM机制：使用 h_t^p 生成仿射参数（缩放γ和偏移β），对 h_t^s 进行特征级调制，得到最终输出 h_t^s_hat。这种设计形成了一个二叉树结构，使模型能够选择性地利用信息丰富的音频线索。</p>
<p><strong>核心模块三：交互建模模块</strong>。为建模模态间的高阶交互并避免稀疏音频信息在前期融合中被稀释，本模块引入了一条从音频编码器直接到本模块的“高速通道”。模块包含三个平行的交叉注意力组件。在每个组件中，一个模态作为查询，另外两个模态沿时间维度拼接后作为键值对。例如，以本体感觉特征 h_t^s_hat 为查询，以音频特征 x_t^a 和视觉特征 h_t^p 的拼接为键值，通过交叉注意力计算得到更新后的表征。三个组件的输出最终被拼接并投影，形成统一的、具有交互感知的嵌入 z_t。</p>
<p><strong>创新点</strong>：与现有将多模态特征简单拼接或对称融合的方法相比，本文的创新点在于：1) 提出了一个分层的、以音频为中心的融合框架，通过二元分支融合模块显式地利用音频作为条件信号来调制其他模态；2) 设计了交互建模模块，通过交叉注意力显式建模三模态间的高阶依赖关系，并辅以音频高速通道确保关键音频信息不被丢失。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界的Franka Emika Panda机器人上进行，使用了两个需要精确操作的任务进行评估：液体倾倒和柜门开关。音频通过外置麦克风采集，视觉通过顶部深度相机获取点云，本体感觉通过机器人接口获取。</p>
<p><img src="https://arxiv.org/html/2602.13640v1/x3.png" alt="实验设置"></p>
<blockquote>
<p><strong>图3</strong>：实验环境概览。两项实验均涉及视觉、听觉和本体感觉感知。</p>
</blockquote>
<p>对比的基线方法包括：仅使用点云和本体感觉的 <code>PC||State</code>；将三种模态特征简单拼接的 <code>Audio||PC||State</code>；以及当前先进的音频-视觉融合方法 <code>ManiWAV Fusion</code>。</p>
<p><img src="https://arxiv.org/html/2602.13640v1/x5.png" alt="定量结果"></p>
<blockquote>
<p><strong>图5</strong>：在倾倒任务（左）和柜门开关任务（右）上的定量结果。所提出的方法（Ours）在两个任务的所有评估指标上均优于所有基线方法。</p>
</blockquote>
<p>关键实验结果如下：在液体倾倒任务中，以目标容器中最终空气柱高度（越低越好）为指标，本文方法达到 1.21 cm，显著优于 <code>PC||State</code> (2.03 cm)、<code>Audio||PC||State</code> (1.65 cm) 和 <code>ManiWAV Fusion</code> (1.52 cm)。在柜门开关任务中，使用一个综合评分（考虑任务完成质量和柜体意外移动），本文方法得分为 0.85，同样优于所有基线（<code>PC||State</code>: 0.45， <code>Audio||PC||State</code>: 0.62， <code>ManiWAV Fusion</code>: 0.71）。结果表明，融入音频并采用本文分层融合策略能显著提升操作精度。</p>
<p><img src="https://arxiv.org/html/2602.13640v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。移除二元分支融合模块（w/o B-BFM）或交互建模模块（w/o IMM）都会导致性能下降，证明了两个模块的有效性。同时使用两个模块（Full）性能最佳。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：移除二元分支融合模块（w/o B-BFM）导致倾倒任务性能下降至 1.52 cm，柜门任务下降至 0.71；移除交互建模模块（w/o IMM）导致性能分别下降至 1.44 cm 和 0.74。这表明两个模块对最终性能均有重要贡献，且完整模型（Full）性能最优。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了一个新颖的分层音频-视觉-本体感觉融合框架，通过二元分支融合模块以音频为条件调制其他模态表征；2) 设计了交互建模模块，通过交叉注意力显式建模高阶跨模态交互，并引入音频高速通道以强调稀疏的音频信息；3) 在真实机器人操作任务上进行了全面实验，验证了所提方法在精度和泛化性上优于现有方法，并通过互信息分析揭示了音频在声学丰富任务中的主导作用。</p>
<p>论文提到的局限性包括：方法依赖于相对高质量的音频数据，在极端嘈杂环境中性能可能下降；以及分层融合结构增加了模型复杂度与计算开销。</p>
<p>本文对后续研究的启示在于：为处理异质多模态信号（尤其是稀疏事件信号）提供了新的融合范式，即采用非对称、条件化的分层设计而非简单的对称融合。未来工作可以探索更高效的融合结构以降低计算成本，或将此框架扩展到更多依赖瞬时反馈的灵巧操作任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中视觉和本体感知在部分可观察环境下难以推断接触交互状态的问题，提出分层音频-视觉-本体感知融合框架。该方法先以声音线索条件化视觉和本体感知表示，再显式建模高阶跨模态交互，并采用基于扩散的策略生成连续动作。在真实世界液体倾倒和橱柜打开任务中，该方法 consistently 优于先进多模态融合方法，尤其在声音提供视觉不易获得的关键信息时表现突出。互信息分析进一步揭示了声音线索通过融合提升操作精度的作用。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.13640" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>