<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.05662" target="_blank" rel="noreferrer">2510.05662</a></span>
        <span>作者: Kuk-Jin Yoon Team</span>
        <span>日期: 2025-10-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>透明物体在现实环境中普遍存在，但传统的深度传感器（如商用RGB-D相机）因光线在透明表面发生折射或反射，难以准确感知其深度信息，这给机器人可靠操作带来了根本性挑战。现有透明物体操作方法主要集中于短视距的抓取任务，依赖于多视角或单视角的深度重建，缺乏精确长视距操作的能力。为实现精确操作，需要完整的6D物体位姿估计。然而，现有的透明物体位姿估计方法大多依赖类别先验知识，只能泛化到同一类别内的新物体实例，无法处理类别外的新颖物体，且忽略了细粒度几何信息，限制了在特定任务约束（如将瓶子精确排成直线）下的应用。</p>
<p>本文针对上述局限性，提出了一种结合人类视频演示和语言指令引导的框架DeLTa，旨在实现对新透明物体的精确、长视距操作。其核心思路是：从单次物体演示视频中解析出技能轨迹，结合视觉语言模型进行任务分解与规划，并将轨迹泛化应用于新物体，最终通过碰撞感知的运动规划执行任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>DeLTa框架的目标是利用单次物体人类演示轨迹，让机器人根据任务指令操作新颖的透明物体。整个流程分为两个主要阶段：1) 从人类演示视频中解析出单物体操作轨迹；2) 根据语言指令和场景观察，执行机器人操作。框架包含三个核心部分：演示解析、任务规划和动作执行。</p>
<p><img src="https://arxiv.org/html/2510.05662v1/figure/overview.jpg" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：DeLTa框架总览。左上部分为人类演示解析流程，从视频中提取技能轨迹并存入数据库。左下部分为视觉语言引导的任务规划器，将自然语言指令分解并细化为可执行的行动序列。右侧为机器人动作执行流程，结合估计的物体位姿和演示轨迹生成最终运动。</p>
</blockquote>
<p><strong>1. 解析人类视频演示</strong><br>此阶段从单次物体演示视频中提取基础技能（拾取、放置、倾倒）的轨迹。流程包括四个关键步骤：</p>
<ul>
<li><strong>透明深度估计</strong>：商用立体相机（如ZED）的原始深度无法感知透明物体表面。本文采用基础模型FoundationStereo进行立体深度估计，输入立体图像，输出像素级度量深度，显著改善了透明物体的深度重建。</li>
<li><strong>开放词汇分割</strong>：利用开放词汇检测模型获取手和物体的边界框，再使用分割模型以边界框为提示生成精细掩码，用于后续位姿估计。</li>
<li><strong>新物体位姿估计与跟踪</strong>：结合重建深度、分割掩码和物体网格（通过图像到3D重建或现有数据库获取），使用先进的<strong>新物体</strong>位姿估计与跟踪方法，输出物体在演示中的6D位姿序列（即轨迹）。</li>
<li><strong>手部位姿估计</strong>：使用手部位姿检测器提取21个关键点，并利用重建深度和估计的MANO手部网格进行尺度调整，获得准确的3D手部关节位置。基于关键点构建手腕坐标系，用于将物体位姿轨迹转换到动作特定的参考系（如倾倒任务的目标容器位姿）。转换后的轨迹经过下采样（每2cm或5°差异）和平滑处理，最终存储到轨迹数据库中。</li>
</ul>
<p><strong>2. 视觉语言引导的任务规划</strong><br>任务规划器将高级任务指令转化为由基础动作组成的可执行计划。它包含三个组件：</p>
<ul>
<li><strong>VLM规划器</strong>：利用基础VLM（未微调）处理任务指令和视觉输入，生成一个初步的高级动作序列骨架。提示模板（图4a）规定了任务目标、已知信息、可用动作和输出格式。</li>
<li><strong>计划翻译器</strong>：将VLM生成的计划转换为PDDL形式化语言，并检查语法正确性和参数完整性。若出错，则返回错误信息促使VLM重新生成。</li>
<li><strong>接地计划搜索</strong>：为解决VLM可能忽略机器人特定约束（如单臂、眼在手配置）或遗漏必要步骤的问题，本模块对计划进行可行性验证和缺失动作搜索。算法将计划按放置动作划分为子任务，将操作动作（如Pick）标记为关键动作，其他（如LookFor）标记为连接动作。它顺序验证关键动作的预条件；若失败，则触发向后的广度优先搜索，利用连接动作和顺序无关的原始动作来寻找满足预条件的前驱动作序列。此过程能恢复由机器人约束引起的错误，同时保持关键动作的顺序。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.05662v1/figure/prompt_set1.png" alt="VLM提示过程"></p>
<blockquote>
<p><strong>图4</strong>：VLM提示过程。(a) 用于任务规划和细化的提示格式。蓝色为上下文信息，橙色为任务相关提示，红色为错误信息和无效计划。</p>
</blockquote>
<p><strong>3. 演示引导的机器人动作执行</strong><br>此阶段执行规划器生成的动作序列，整合三个核心组件：目标物体位姿估计、目标轨迹生成和碰撞感知运动规划。</p>
<ul>
<li><strong>动作管理器</strong>：顺序处理规划器动作，通过LLM提示（图4b）将动作参数映射到网格数据库中的具体物体网格，并评估动作可行性。</li>
<li><strong>新物体位姿估计</strong>：与演示解析阶段类似，基于重建深度和开放词汇分割，估计当前场景中目标物体的6D位姿。</li>
<li><strong>演示引导的轨迹生成</strong>：不是简单复现演示轨迹，而是通过位姿自适应将其重定向到新物体。首先，利用估计的物体位姿，将存储在物体坐标系下的演示轨迹映射到机器人基座坐标系，生成初始轨迹。然后，应用一种<strong>基于旋转的对齐方法</strong>：固定轨迹的最终目标位姿，计算一个旋转矩阵，使整个轨迹的起始点对齐到机器人当前末端执行器位姿，从而生成适配后的轨迹，无需重新参数化。</li>
<li><strong>最后一步运动规划器</strong>：运动执行分为两个阶段（图5）。第一阶段进行全局规划，使机器人从当前位置无碰撞地移动到适配后轨迹的起点。第二阶段，机器人跟踪适配后的“最后一步”演示轨迹。该阶段使用基于二次规划的逆运动学求解器生成关节空间轨迹，并融入碰撞避免和关节限位约束。为平衡避障灵活性和末端精度，采用自适应精度策略：在轨迹前80%部分使用较大容差，在最后20%部分强制执行小容差以确保精确操作。若无法生成有效路径，则对目标物体位姿引入微小扰动后重新规划。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.05662v1/figure/trajectory_plan.jpg" alt="最后一步运动规划器"></p>
<blockquote>
<p><strong>图5</strong>：最后一步运动规划器：倾倒（左）和拾取（右）。RGB轴可视化规划的末端执行器位姿。蓝色方框代表从点云近似得到的碰撞地图。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实世界中使用Kinova Gen3 7自由度机械臂、Robotis RH-P12-RN夹爪和眼在手ZED立体相机进行评估。使用了9种不同的透明与非透明物体（图6）。定义了三个长视距操作任务：1) <strong>紧致货架取物</strong>：将目标物体从货架精确放置到杯垫上；2) <strong>化学实验</strong>：根据指令在目标容器中调配出指定颜色的液体；3) <strong>商品上架</strong>：将物体按参考图像在货架上排成一条直线。每个任务进行10次试验。</p>
<p><img src="https://arxiv.org/html/2510.05662v1/figure/dataset2.png" alt="真实世界操作物体"></p>
<blockquote>
<p><strong>图6</strong>：用于操作实验的真实世界物体。</p>
</blockquote>
<p><strong>基线方法</strong>：与两种先进方法对比：1) <strong>ClearGrasp</strong>：基于深度估计的透明物体操作方法；2) <strong>YODO</strong>：利用人类演示和类别级物体位姿估计的方法。为使基线能处理长视距任务，为其集成了本文的任务规划器和运动规划器，并对ClearGrasp补充了位姿估计模块，对YODO补充了先进的透明物体类别级位姿估计方法。</p>
<p><strong>关键定量结果</strong>：</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">任务1成功率(%) ↑</th>
<th align="left">任务2成功率(%) ↑</th>
<th align="left">任务3成功率(%) ↑</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ClearGrasp [18]</td>
<td align="left">70</td>
<td align="left">0</td>
<td align="left">20</td>
</tr>
<tr>
<td align="left">YODO [20]</td>
<td align="left">70</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left"><strong>Ours</strong></td>
<td align="left"><strong>100</strong></td>
<td align="left"><strong>80</strong></td>
<td align="left"><strong>70</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表I</strong>：与先进方法的对比。</p>
</blockquote>
<p><strong>结果分析</strong>：</p>
<ul>
<li><strong>总体性能</strong>：DeLTa在三个任务上均显著优于基线，尤其在复杂的长视距任务（任务2和3）上优势明显。ClearGrasp和YODO在任务2和3上成功率极低或为零，突显了它们在精确位姿估计和长视距规划方面的不足。</li>
<li><strong>任务分析</strong>：所有方法在相对简单的短视距货架取物任务（任务1）上表现尚可。但对于需要精确倾倒和避障的化学实验任务（任务2），以及需要精确对齐排列的商品上架任务（任务3），DeLTa的成功率（80%， 70%）远高于基线（0%， 20%/0%），证明了其框架在复杂长视距透明物体操作上的有效性。</li>
</ul>
<p><strong>消融实验</strong>：<br>论文通过消融研究验证了各核心模块的必要性。移除<strong>VLM规划器</strong>会导致无法理解自然语言指令。移除<strong>接地计划搜索</strong>会使规划器无法处理机器人约束，导致执行失败。移除<strong>演示引导的轨迹生成</strong>（即直接使用原始演示轨迹）会因物体位姿和环境的差异而导致碰撞或任务失败。移除<strong>最后一步运动规划器</strong>的自适应精度策略会降低操作精度或增加碰撞风险。这些实验共同表明，DeLTa的每个组件对于实现鲁棒、精确的长视距操作都至关重要。</p>
<p><img src="https://arxiv.org/html/2510.05662v1/figure/tests_1.jpg" alt="任务序列示例"></p>
<blockquote>
<p><strong>图7</strong>：我们的三个操作任务序列，展示了真实环境中的输入查询图像和目标物体。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>首个演示与语言引导的透明物体长视距操作框架</strong>：提出了DeLTa，首次实现了在人类视频演示和自然语言指令引导下，对新透明物体的精确、长视距操作。</li>
<li><strong>单次演示泛化与4D建模</strong>：探索了从单次人类演示视频中提取手-物交互的4D（3D+时间）信息，用于透明物体操作，仅需每个技能一次演示即可泛化到新物体，极大降低了数据收集成本。</li>
<li><strong>VLM规划器与接地搜索的结合</strong>：提出了一个VLM引导的任务规划器，能够将语言指令分解为多步动作，并通过验证与搜索机制对其进行细化，以强制执行机器人特定约束，弥合了符号规划与真实物理执行之间的鸿沟。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到的方法局限性包括：1) 演示解析的准确性依赖于基础模型（深度估计、分割、位姿估计）的性能；2) 规划中的接地搜索过程可能因搜索空间较大而增加计算时间；3) 当前主要评估了拾取、放置、倾倒三种技能，框架扩展至更复杂技能（如拧盖）的有效性有待验证。</p>
<p><strong>启示</strong>：<br>DeLTa展示了如何通过整合前沿的基础模型（VLM、立体深度估计、开放词汇感知）与机器人学中的经典方法（运动规划、示教学习），来解决透明物体操作这一长期存在的挑战。其“单次演示泛化”的思路为样本高效的操作技能学习提供了新方向。同时，其任务规划中“生成-验证-细化”的闭环范式，对于提高大模型在机器人任务中规划的可靠性和可执行性具有普遍参考价值。未来工作可探索更多样化的技能、更复杂的多物体交互任务，以及进一步提升在高度动态或不确定环境中的鲁棒性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决透明物体因深度感知困难导致的机器人长时程精确操作难题。提出DeLTa框架，整合深度估计、6D姿态估计与视觉语言规划，核心创新包括：单次演示即可泛化6D轨迹至新透明物体（无需类别先验或额外训练），并设计任务规划器优化VLM生成的动作序列以适应单臂眼在手机器人约束。实验表明，该方法在长时程精确操作场景中显著优于现有透明物体操作方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.05662" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>