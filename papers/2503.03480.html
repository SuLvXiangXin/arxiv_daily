<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.03480" target="_blank" rel="noreferrer">2503.03480</a></span>
        <span>作者: Zhang, Borong, Zhang, Yuhao, Ji, Jiaming, Lei, Yingshan, Dai, Josef, Chen, Yuanpei, Yang, Yaodong</span>
        <span>日期: 2025/03/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型（VLAs）在展现作为通用机器人策略潜力的同时，其真实世界部署面临着极端的安全挑战，包括对环境、机器人自身和人类造成伤害的风险。该领域的主流方法，如大规模行为克隆（模仿学习）和以提高任务性能、泛化能力为目标的标准强化学习（RL）微调，普遍缺乏明确集成和强制执行安全约束的机制。这导致最先进的模型尚未将安全性作为其设计的核心组成部分，构成了可靠部署的关键障碍。</p>
<p>本文针对如何将安全约束明确集成到VLAs中而不损失性能这一具体痛点，提出了一个全新的系统性视角：集成安全方法（Integrated Safety Approach, ISA）。该方法将VLA安全对齐形式化为一个约束马尔可夫决策过程（CMDP）问题，并围绕建模、激发、约束和保证四个相互关联的方面展开。其核心思路是：通过系统性地建模安全需求、激发潜在不安全行为、使用基于拉格朗日的安全强化学习（SafeRL）约束策略，并建立全面的安全评估体系，在保证任务性能的同时大幅提升VLA策略的安全性。</p>
<h2 id="方法详解">方法详解</h2>
<p>ISA的整体框架是一个包含四个阶段的完整pipeline：1）建模安全关键的任务和环境方面；2）从现有策略中激发潜在且多样的不安全行为；3）利用SafeRL技术约束VLA的学习过程以集成安全考量；4）通过严格、有针对性的评估来保证最终模型的安全性。这四个方面相互关联，共同构成一个闭环。</p>
<p><img src="https://arxiv.org/html/2503.03480v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：集成安全方法（ISA）流程。该流程采用多层面框架，用于系统地对齐视觉-语言-动作模型的安全性。包含建模（Modeling）、激发（Eliciting）、约束（Constraining）和保证（Assuring）四个核心环节。</p>
</blockquote>
<p><strong>核心模块一：安全建模</strong>。研究聚焦于移动操作场景。任务被形式化地定义为包含场景、初始位姿、物体类别集以及安全谓词集合。安全谓词是识别不安全行为的紧凑表示，分为状态-动作级谓词（ϕ: 𝒮 × 𝒜 → {0,1}）和轨迹级谓词（ψ: ℋ → {0,1}）。它们使用组合逻辑定义，例如状态-动作谓词ϕ(s,a)=1当且仅当状态条件P_s(s)、动作条件P_a(a)和风险诱导关系R(s,a)同时成立。基于此，构建了三大类任务：Safety-ObjNav（安全物体导航）、Safety-PickUp（安全拾取）和Safety-Fetch（安全取物）。</p>
<p><strong>核心模块二：风险激发</strong>。为了全面激发风险并防止策略过拟合，关键在于最大化环境设置和可交互物体的多样性。为此，研究利用了由ProcTHOR生成的大规模15万个多样化室内场景数据集，以及包含80万个3D资产的Objaverse库，并在支持高保真渲染和物理交互的AI2THOR模拟器中运行。更重要的是，为了系统化风险激发，研究识别并利用了若干<strong>安全关键组件</strong>，这些是已知问题场景的具体环境特征或具有挑战性的物体布置。</p>
<p><img src="https://arxiv.org/html/2503.03480v3/x2.png" alt="安全关键组件"></p>
<blockquote>
<p><strong>图2</strong>：安全关键组件的概念图（上）及模拟环境中的对应真实感示例（下）。包括角落、盲点、易碎集合、关键点和危险设备。</p>
</blockquote>
<p><strong>核心模块三：策略约束</strong>。这是将安全考量集成到VLA策略学习的核心步骤。首先，将安全谓词（ϕ, ψ）转化为成本信号。违反状态-动作谓词（ϕ_k）在违规时间步t产生成本1，否则为0；轨迹级谓词（ψ_j）的成本1仅归因于违规片段的最后一步（信用分配是未来工作方向）。然后，将VLA安全对齐问题形式化为CMDP优化问题（公式2），目标是找到在满足安全约束集Π_𝒞的前提下最大化任务奖励回报的策略π*。采用拉格朗日松弛技术将此约束优化转化为无约束的极小极大优化问题（公式3），通过交替更新策略参数θ和拉格朗日乘子λ来求解。这种方法优先优化安全性，然后在约束内最大化任务性能。</p>
<p><strong>核心模块四：安全保证</strong>。通过多维度评估来确保对齐后模型的安全性，包括：测试时安全（在保留测试集和分布外扰动下的表现）、长尾安全（对统计低频事件的处理）和极端故障安全（在任务可能无法完成的灾难性故障情况下的行为）。</p>
<p>与现有方法相比，ISA的创新点在于首次系统性地将SafeRL原则应用于VLA安全对齐，提出了一个包含从需求建模、数据生成到优化、评估的完整框架，而不仅仅是简单的奖励塑形或后处理。其核心是通过CMDP和拉格朗日方法显式地处理安全与性能的权衡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要评估基准是新提出的<strong>Safety-CHORES</strong>，它是一个集成了导航与操作的、包含细粒度安全约束的长视野任务测试平台。同时，为了对比，也在AI2THOR、RoboTHOR等传统基准上进行了测试。成本阈值b_i被经验性地设置为FLaRe基线收敛成本的20%。</p>
<p><strong>基线方法</strong>：对比了多种范式的方法：1) <strong>IL-only</strong>：最先进的模仿学习方法SPOC及其变体；2) **IL-only (Ground Truth)**：使用真实检测信息的SPOC，展示模仿学习的上限；3) **IL+RL (Standard)**：仅专注于任务性能的RL微调方法FLaRe；4) **IL+RL (Reward Shaping)**：FLaRe-RS，将安全成本直接作为奖励惩罚的启发式方法；5) <strong>RL-Only</strong>：用于导航任务的端到端RL方法Poliformer。初始模型选用SPOC-DINOv2。</p>
<p><strong>评估指标</strong>：任务成功率（SR）和安全违规累积成本（CC）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>性能对比</strong>：如表1所示，ISA在所有Safety-CHORES任务上都取得了显著的安全提升。与最强的任务导向RL基线FLaRe相比，ISA将平均累积成本（CC）降低了**83.58%<strong>。同时，ISA保持了任务性能，平均成功率（SR）比FLaRe提高了</strong>3.85%**，实现了安全与性能的有效权衡。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.03480v3/x3.png" alt="性能对比表"></p>
<blockquote>
<p><strong>图3</strong>：累积成本分布分析。左：经ISA和FLaRe微调后，测试集机器人轨迹的累积成本分布。ISA消除了高成本（&gt;10）的轨迹。中：任务成功时的累积成本分布。右：任务失败时的累积成本分布。ISA即使在失败时也保持了较低的成本。</p>
</blockquote>
<ol start="2">
<li><p><strong>定性分析与长尾风险</strong>：图3（左）显示，ISA完全消除了累积成本&gt;10的极高风险轨迹，将不安全行为的严重程度上限降低至FLaRe的<strong>1/35</strong>，表明其对灾难性故障和长尾风险的有效缓解。进一步分析（图3中、右）发现，FLaRe的安全成本与任务失败显著负相关，而ISA的安全行为与任务结果脱钩，即使任务失败也能安全地失败。</p>
</li>
<li><p><strong>泛化能力</strong>：如图4所示，ISA在不同VLA模型（如EmbCLIP, Embodied-Codebook）上均能有效工作，并且在传统非安全基准（如ObjectNav）上也能保持或提升性能，同时在其他基准上引入安全约束后（Safety-ObjNav）显著降低安全成本，证明了其泛化性。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2503.03480v3/x4.png" alt="多模型多基准性能"></p>
<blockquote>
<p><strong>图4</strong>：ISA在不同VLA模型和基准上的有效性。左：各模型在每个基准上的成功率。右：各模型在这些基准上产生的累积成本。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：<ul>
<li><strong>风险激发的重要性</strong>：如图7左所示，若在简化（单房间、无安全关键组件）场景中应用ISA训练，安全性能大幅下降，CC增至完整ISA的约3倍（5.01 vs. 1.854），甚至差于FLaRe-RS，SR也下降。这证明了丰富激发环境对于学习稳健安全行为的不可或缺性。</li>
<li><strong>约束机制的选择</strong>：如图6所示，将拉格朗日乘子固定为常数（即退化为简单的固定惩罚系数）会导致性能严重不稳定，无法有效权衡安全与性能，凸显了自适应拉格朗日优化的重要性。</li>
<li><strong>安全关键组件的影响</strong>：消融特定安全关键组件（如易碎集合）会导致在对应场景下的安全成本（CC）急剧上升，同时可能影响任务成功率（SR），证实了每个组件在覆盖多样化风险中的作用。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2503.03480v3/x7.png" alt="风险激发消融"></p>
<blockquote>
<p><strong>图7左</strong>：风险激发消融实验。在缺乏多样化场景和安全关键组件的简化环境中训练，ISA的安全性能（CC）大幅恶化，任务成功率（SR）也降低。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.03480v3/x6.png" alt="固定惩罚系数"></p>
<blockquote>
<p><strong>图6</strong>：使用固定惩罚系数（而非自适应拉格朗日乘子）的ISA性能。任务成功率（SR）波动剧烈，累积成本（CC）也无法稳定优化，表明自适应权衡机制的必要性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）首次系统性地探索并提出了用于VLA安全对齐的<strong>集成安全方法（ISA）</strong>，涵盖了从安全建模、风险激发、策略约束到安全保证的全流程；2）创建了<strong>Safety-CHORES</strong>基准环境，通过程序化生成场景和定义安全关键组件，能更有效地暴露VLA漏洞；3）通过大量实验验证了ISA的有效性，证明其能在显著提升安全性（降低83.58%累积成本）的同时保持甚至提升任务性能，并具备缓解长尾风险和处理极端故障的能力。</p>
<p>论文自身提到的局限性包括：对于轨迹级安全谓词（ψ_j），当前成本分配方案（成本归因于片段末尾）较为简单，其信用分配是未来需要探索的方向；实验主要在模拟环境中进行。</p>
<p>本工作对后续研究的启示在于：为VLA的安全对齐提供了一个系统性的方法论框架和评估基准，证明了将SafeRL的严谨形式化方法与VLAs的强大感知和泛化能力相结合的巨大潜力。它强调了安全对齐不应仅是事后的奖励调整或内容过滤，而应作为一个贯穿模型训练与评估全生命周期的核心设计原则。未来工作可探索更精细的成本分配、更复杂的约束类型以及向真实物理世界的迁移。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在现实部署中存在的安全风险，提出通过约束学习实现安全对齐。核心方法是集成安全方法，基于约束马尔可夫决策过程范式，从最小最大角度优化模型以应对已识别的安全风险。实验表明，该方法在长视野移动操作任务中，能将安全违规的累积成本降低83.58%，同时任务成功率提升3.85%，并展现出强大的安全保证与泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.03480" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>