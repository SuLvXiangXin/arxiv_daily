<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.05957" target="_blank" rel="noreferrer">2510.05957</a></span>
        <span>作者: Robin Chhabra Team</span>
        <span>日期: 2025-10-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>软体机器人爬行者利用其身体的柔顺性和可变形性，通过与表面接触实现运动。然而，为其设计控制策略面临模型不准确、传感器噪声以及需要发现有效步态等多重挑战。该领域现有方法包括使用简化模型（如质量-弹簧-阻尼模型）、有限元分析、基于中央模式发生器（CPG）的控制以及模型预测控制（MPC）。这些方法往往在模型精度与实时控制需求之间难以权衡：精确的物理模型计算成本高，而简化模型则无法充分捕捉复杂的接触动力学。具体而言，MPC等基于模型的方法严重依赖精确且易于处理的模型，这在具有连续体和高维控制空间的软体机器人中应用受限。</p>
<p>本文针对软体机器人控制中“缺乏准确、易处理的动力学模型”这一核心痛点，提出了一种新的视角：直接从机器人的噪声传感器数据中学习一个紧凑的潜在动态模型，并以此模型为基础，通过强化学习来发现和优化运动步态。本文的核心思路是：利用惯性测量单元（IMU）和飞行时间（TOF）传感器的观测数据，通过变分推理学习一个潜在状态空间模型，然后将此模型作为预测模型整合到演员-评论家强化学习框架中，从而在潜在空间内进行规划并优化周期性步态参数。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架是一个基于潜在模型的强化学习（MB-RL）pipeline，主要包含两个核心模块：感知模块（学习潜在动态模型）和演员-评论家学习模块（利用模型优化策略）。整体输入是带噪声的传感器观测（IMU加速度和TOF距离）以及控制指令，输出是优化后的周期性步态控制信号。</p>
<p><img src="https://arxiv.org/html/2510.05957v1/x2.png" alt="训练循环总结"></p>
<blockquote>
<p><strong>图2</strong>：方法整体训练循环。左侧展示了感知模块：传感器观测（IMU, TOF）被编码并与隐藏状态结合，通过RNN和推理网络得到潜在状态，同时解码器重建观测和奖励。右侧展示了演员-评论家模块：学习到的动态模型在潜在空间中展开多步预测，用于训练评论家（价值函数）和演员（策略），策略输出步态参数生成控制动作。</p>
</blockquote>
<p><strong>1. 感知模块（学习潜在动力学）</strong><br>该模块的目标是从噪声传感器数据中推断出系统的紧凑潜在状态表示及其转移动力学。其理论基础是变分自由能最小化。具体而言：</p>
<ul>
<li><strong>状态表示</strong>：受软体爬虫动力学可分解为刚体运动（质心）和内部应变运动的启发，潜在状态被设计为与之对应的四维高斯分布变量。</li>
<li><strong>编码与动态</strong>：传感器观测（IMU的 $\boldsymbol{\alpha}_t$ 和 TOF的 $\boldsymbol{X}_t$）通过重参数化技巧被编码为高斯变量 $\boldsymbol{A}_t$ 和 $\boldsymbol{Z}_t$。一个循环神经网络（RNN） $f_\varphi$ 用于编码历史信息，生成隐藏状态 $\boldsymbol{h}_t$。</li>
<li><strong>两个转移模型</strong>：模型同时学习两个潜在状态转移分布：1) 基于动作的转移 $q_\varphi(\mathcal{S}_t | \boldsymbol{h}_t)$，表示给定历史状态和动作时下一个状态的概率；2) 基于观测的转移 $p_\varphi(\hat{\mathcal{S}}_t | \boldsymbol{h}_t, \boldsymbol{A}_t, \boldsymbol{Z}_t)$，表示给定历史状态和当前传感器编码时对状态的估计。</li>
<li><strong>损失函数</strong>：感知模块通过最小化修改后的变分自由能 $\widetilde{\mathcal{F}}(\boldsymbol{\varphi})$ 进行训练。该损失包含三项：1) $\mathcal{L}_1$ 和 $\mathcal{L}_2$：约束上述两个转移分布尽可能一致（通过KL散度），并引入2纳特的下限以防止模型坍塌；2) $\mathcal{L}_3$：重建损失，确保从潜在状态能够准确解码出传感器观测和预测的奖励。</li>
</ul>
<p><strong>2. 演员-评论家学习模块（步态优化）</strong><br>该模块利用学习到的潜在动态模型作为仿真器，在潜在空间中进行多步预测，从而优化策略。</p>
<ul>
<li><strong>步态参数化</strong>：控制输入 $u_t$ 被参数化为一个傅里叶级数形式的周期性信号，包含偏置、正弦和余弦项的系数以及基频 $\omega$。这些参数集合 $\boldsymbol{a}$ 即为策略的输出。</li>
<li><strong>模型预测与训练</strong>：演员策略 $\pi_\theta(\boldsymbol{a}_t | \mathcal{S}_t, \boldsymbol{h}_t)$ 和评论家价值函数 $v_\psi(R_t | \mathcal{S}_t, \boldsymbol{h}_t)$ 都在学习到的潜在模型展开的轨迹上进行训练。如图2右侧所示，从初始潜在状态开始，模型通过动作进行多步（视野H）展开，生成状态、动作、奖励轨迹。</li>
<li><strong>评论家训练</strong>：评论家使用 $\lambda$-自举回报 $R_t^\lambda$ 作为目标，通过最大化似然损失 $\mathcal{L}_c$ 来学习预测累积回报。</li>
<li><strong>演员训练</strong>：演员使用强化梯度进行训练，其损失函数 $\mathcal{L}_a$ 包含两项：一是由优势函数 $\mathcal{A}_t$ （归一化的回报与价值函数之差）加权的策略对数似然，旨在提高高回报动作的概率；二是策略熵正则项，用于鼓励探索。</li>
</ul>
<p><strong>创新点</strong><br>与现有方法相比，本文的创新点主要体现在：1) <strong>数据驱动的潜在模型</strong>：无需预先已知或推导软体机器人的解析动力学模型，直接从低成本、带噪声的IMU和TOF传感器数据中学习动态的紧凑表示。2) <strong>基于模型的强化学习集成</strong>：将学习到的潜在模型无缝集成到演员-评论家框架中，使策略优化可以在高效、低成本的“想象”轨迹上进行，而非完全依赖昂贵或危险的实际机器人交互。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真环境中进行，使用了一个最小化的、受蚯蚓启发的一维软体爬虫机器人模型。基准测试主要围绕方法本身的不同配置进行消融研究，并未与其他外部基线方法进行对比。</p>
<p><strong>关键实验结果</strong></p>
<ol>
<li><p><strong>潜在维度与规划视野的影响</strong>：研究评估了不同潜在状态维度（4维、8维、16维）和不同规划视野（H=1, 5, 10）对学习性能的影响。性能通过训练期间的平均总奖励（鼓励向前运动）来衡量。<br><img src="https://arxiv.org/html/2510.05957v1/smoothed_average_total_reward_with_variance1.png" alt="平滑平均总奖励与方差"></p>
<blockquote>
<p><strong>图3</strong>：训练过程中平均总奖励的变化。结果表明，16维的潜在空间表现最佳，其次是8维，4维最差。同时，更长的规划视野（H=10）比较短的视野（H=1或5）能带来更稳定且更高的最终性能。</p>
</blockquote>
</li>
<li><p><strong>学习到的步态分析</strong>：训练成功后，对学习到的周期性控制策略进行了分析。<br><img src="https://arxiv.org/html/2510.05957v1/x3.png" alt="学习到的步态"></p>
<blockquote>
<p><strong>图4</strong>：学习到的步态控制信号 $u_t$（上图）及其对应的机器人头部和基部位置（下图）。结果显示，策略成功发现了一种有效的周期性伸缩模式，驱动机器人实现了稳定的向前爬行运动。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验总结</strong><br>消融实验明确了两个关键组件的贡献：1) <strong>潜在状态维度</strong>：16维潜在表示比更低维度的表示能捕获更丰富的动态信息，从而支持学习更有效的策略。2) <strong>规划视野</strong>：较长的规划视野（H=10）使演员-评论家能够进行更有效的多步优化，从而获得更高且更稳定的回报，证明了在潜在模型中进行长远规划的价值。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有两点：1) 为软体机器人爬行者开发了一个直接从噪声IMU和TOF传感器数据中学习的数据驱动潜在动态模型。2) 将该学习模型与演员-评论家强化学习框架集成，从而能够在潜在空间中进行高效规划，并自动发现最大化向前运动的步态。</p>
<p>论文提到的局限性包括：目前的工作仅在仿真中进行验证；所使用的传感器模型（IMU和TOF）相对简单，真实的传感器噪声和非线性特性可能更复杂。</p>
<p>这项工作对后续研究的启示在于：它展示了基于潜在模型的RL在软体机器人控制中的潜力，提供了一条绕过精确物理建模难题的途径。未来的工作可以沿着以下方向展开：在真实软体机器人硬件上部署和验证该框架；集成更复杂或更丰富的传感器模态（如视觉）；将方法扩展到更复杂的运动任务（如转向、不平坦地形适应）以及三维运动模型。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对软体机器人爬行器因模型不精确、传感器噪声和步态发现困难而导致控制策略设计复杂的问题，提出了一种基于潜在动力学的模型强化学习框架。该方法利用机载传感器推断潜在动力学作为预测模型，并指导演员-评论家算法优化运动策略。在仿真实验中，该方法通过学习到的潜在动力学实现了短时程运动预测，使机器人仅基于噪声传感器反馈即可发现有效的自适应运动步态。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.05957" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>