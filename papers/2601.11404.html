<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.11404" target="_blank" rel="noreferrer">2601.11404</a></span>
        <span>作者: Zhong, Linqing, Liu, Yi, Wei, Yifei, Xiong, Ziyu, Yao, Maoqing, Liu, Si, Ren, Guanghui</span>
        <span>日期: 2026/01/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为执行多样化机器人操作任务的通用策略主流方法。它们通常依赖预训练的视觉语言模型将多模态输入编码为潜在表示，再直接解码为动作序列。为了改进从输入空间到动作空间的映射，近期研究引入了明确的中间推理步骤，例如预测语言子任务或合成视觉目标图像，以指导动作生成。然而，这些中间推理形式（语言或视觉）本质上是间接的，在传达精确动作执行所需的完整、细粒度信息方面存在固有局限。它们主要在与动作不同的“输入空间”中思考，难以弥合丰富的语义/视觉表示与精确、低层级动作执行需求之间的鸿沟，即存在语义-运动学差距。</p>
<p>本文针对上述痛点，提出了一个根本性的范式转变：最有效的推理形式应直接在动作空间中进行深思熟虑。本文引入了动作思维链范式，将推理过程本身构建为一系列指导最终策略的、结构化的粗略动作意图序列。核心思路是设计一个包含显式和隐式动作推理器的架构，共同形成一个动作空间的思维链，为下游动作预测提供直接且接地气的指导。</p>
<h2 id="方法详解">方法详解</h2>
<p>ACoT-VLA的整体框架旨在从多模态输入中生成有效的动作空间指导，并将其整合到策略学习中。其流程基于一个共享的预训练VLM主干提取特征，随后由三个核心组件处理。</p>
<p><img src="https://arxiv.org/html/2601.11404v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：ACoT-VLA架构概览。框架包含三个主要组件，它们操作共享VLM主干提取的特征。(a) 显式动作推理器是一个基于Transformer的模块，合成一个粗略的参考轨迹，提供显式的动作空间指导。(b) 隐式动作推理器采用带有可学习查询的交叉注意力机制，从VLM的内部表示中提取潜在动作先验。(c) 动作引导预测头通过交叉注意力协同整合显式和隐式指导，以条件化最终的去噪过程，生成可执行的动作序列。</p>
</blockquote>
<p><strong>1. 显式动作推理器</strong>：该模块被实例化为一个轻量级Transformer。给定视觉观测和语言指令，预训练的VLM首先将其编码为上下文键值缓存。EAR接收一个带噪声的粗略动作序列作为输入，通过其Transformer层进行处理。每一层结合自注意力（捕捉动作序列内部的时间依赖性）和与对应VLM层键值缓存的交叉注意力（注入多模态上下文先验），最终通过流匹配训练学习动作轨迹的分布，输出一个去噪后的粗略参考动作序列。该序列随后通过一个MLP投影器被编码为显式动作嵌入，作为显式动作空间指导。</p>
<p><strong>2. 隐式动作推理器</strong>：该模块旨在从VLM的键值缓存中提取隐含的运动线索。对于VLM的每一层，初始化一个可学习的查询矩阵。考虑到VLM键值缓存中的信息冗余和计算效率，首先将键值对下采样到低维空间。然后，应用交叉注意力操作，从下采样后的键值中提取与动作相关的信息。得到的特征经过平均池化和MLP投影，生成捕获该层隐含动作语义的紧凑表示。最后，聚合所有层的表示，得到隐式动作相关特征，作为隐式动作空间指导，与显式运动先验形成互补。</p>
<p><strong>3. 动作引导预测</strong>：此策略将上述两种指导整合到策略学习中。给定一个带噪声的动作片段，首先将其编码为动作查询。该查询随后通过双交叉注意力操作，分别从显式动作嵌入和隐式动作特征中检索互补先验。这两个被关注的表示（分别强调运动学线索和潜在动作倾向）被拼接，并通过一个自注意力融合块整合为统一的表示。最终，这个聚合后的表示被送入动作头，预测出去噪后的可执行动作序列。</p>
<p><strong>训练与优化</strong>：整个框架在标准的流匹配均方误差目标下进行优化。总损失由EAR的损失和动作头的损失加权求和构成。为了稳定训练，在训练阶段计算显式动作嵌入时，直接使用真实参考轨迹而非EAR的预测，以防止对动作头的优化产生干扰；在推理阶段，则切换到完全自条件模式，由EAR自主生成参考动作来指导动作头。</p>
<p>与现有方法相比，创新点具体体现在：1) <strong>范式创新</strong>：首次将思维链范式引入动作空间，将“思考”过程重新定义为明确的、基于运动学的动作意图链，而非抽象的语言或视觉子目标。2) <strong>机制创新</strong>：设计了互补的显式动作推理器和隐式动作推理器，分别提供可直接执行的轨迹指导和隐含的行为先验，共同构成了动作思维链。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在仿真和真实世界环境中进行了广泛实验。使用的基准测试/数据集包括：LIBERO（包含Spatial, Object, Goal, Long四个任务套件）、LIBERO-Plus（专注于鲁棒性评估，包含7种扰动维度）和VLABench（评估指令跟随和常识推理）。实验平台主要基于仿真环境，并在自有机器人平台上进行了真实世界部署验证。</p>
<p>对比的基线方法涵盖了当前主流技术路线，包括：基于视觉指导的方法（如CoT-VLA, WorldVLA, DreamVLA, UniVLA）、基于语言指导的方法（如OpenVLA, π0, π0.5, OpenVLA-OFT）以及不依赖额外指导的通用策略（如Diffusion Policy, Octo）。</p>
<p><strong>关键实验结果</strong>：<br>在LIBERO基准上，ACoT-VLA取得了全面领先，平均成功率高达98.5%，相比之前最佳的π0.5提升了1.6%。尤其在要求长时序操作和严格误差控制的LIBERO-Long套件上，取得了96.0%的成功率，优势显著。</p>
<p><img src="https://arxiv.org/html/2601.11404v1/x3.png" alt="LIBERO实验结果"></p>
<blockquote>
<p><strong>图3</strong>：LIBERO基准上的性能对比表。ACoT-VLA在Spatial, Object, Goal, Long四个套件及平均成功率上均排名第一，展示了其卓越的综合性能。</p>
</blockquote>
<p>在LIBERO-Plus鲁棒性基准上，ACoT-VLA在7种扰动维度的平均成功率达到84.1%，超越了所有对比方法，尤其在机器人初始状态和传感器噪声等挑战性扰动上表现突出。</p>
<p><img src="https://arxiv.org/html/2601.11404v1/figures/fig4.png" alt="LIBERO-Plus实验结果"></p>
<blockquote>
<p><strong>图4</strong>：LIBERO-Plus基准上的性能对比表。ACoT-VLA在平均成功率上领先，并在多个扰动维度（如Robot, Noise）上展现出强大的鲁棒性。</p>
</blockquote>
<p>在VLABench基准上，ACoT-VLA在意图得分和进度得分上分别达到63.5%和47.4%，均优于对比的π0和π0.5方法。</p>
<p><strong>消融实验</strong>：<br>消融实验验证了各个组件的贡献。仅使用显式动作推理器或仅使用隐式动作推理器，相比基线均有提升。当两者共同作用时，性能达到最佳，证明了显式与隐式指导的互补性和协同效应。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>提出动作思维链新范式</strong>：首次将机器人策略的推理过程构建为动作空间中的结构化意图链，直接弥合语义-运动学差距。2) <strong>设计互补的动作推理机制</strong>：提出了显式动作推理器和隐式动作推理器，分别提供明确轨迹和潜在先验，共同生成有效的动作空间指导。3) <strong>构建ACoT-VLA统一框架并验证其卓越性能</strong>：在多个仿真基准和真实世界实验中达到最先进水平，证明了该范式的有效性和泛化能力。</p>
<p>论文自身提到的局限性包括：1) <strong>计算开销</strong>：引入额外的推理模块（EAR和IAR）会增加一定的计算负担。2) <strong>参考轨迹质量</strong>：EAR生成的参考轨迹质量直接影响下游策略的性能，如何进一步提升其准确性和多样性是关键。</p>
<p>本文对后续研究的启示在于：为通用机器人策略的“思考”方式提供了新的方向——直接面向动作进行推理。未来的工作可以探索更高效、更强大的动作空间推理机制，或将此范式与其他形式的推理（如物理常识）相结合，以构建更具物理基础和理解能力的机器人策略。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在复杂操作任务中，现有基于语言或视觉的中间推理难以直接指导精确动作生成的问题，提出了**动作链式思维范式**。核心方法**ACoT-VLA**包含**显式动作推理器**（生成粗粒度参考轨迹）与**隐式动作推理器**（提取多模态输入中的潜在动作先验），两者协同形成直接作用于动作空间的推理链，以引导最终策略学习。实验表明，该方法在LIBERO、LIBERO-Plus和VLABench基准上分别达到98.5%、84.1%和47.4%的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.11404" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>