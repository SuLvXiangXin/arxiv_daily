<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.11307" target="_blank" rel="noreferrer">2510.11307</a></span>
        <span>作者: Alessandro Suglia Team</span>
        <span>日期: 2025-10-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前具身人工智能（Embodied AI）方法通常通过模仿学习（IL）从专家演示中学习策略。然而，这种方法存在三个关键局限性：首先，仅基于最优轨迹训练的模型可能认为每个任务只有唯一有效解法；其次，模型从未见过可恢复的错误及其纠正过程；最后，当面对同一观察的多种有效行为模式时，IL缺乏评估动作质量的机制。相比之下，强化学习（RL）通过标量奖励提供反馈，但相关的探索过程通常导致样本效率低下，在奖励稀疏的具身任务中尤为严重。</p>
<p>本文针对模仿学习无法有效利用次优演示数据这一具体痛点，提出了一种新视角：将次优行为（如错误和低效策略）与建设性的语言反馈相结合，从而将其转化为学习机会。核心思路是：将语言反馈嵌入作为Transformer策略输入序列的一部分，系统性地用包含语言反馈的次优演示增强训练数据，并可选地引入自监督的辅助反馈预测任务，以提高策略的组合泛化能力和鲁棒性，同时保持数据效率。</p>
<h2 id="方法详解">方法详解</h2>
<p>FOSSIL方法的整体框架基于一个自回归Transformer主干网络，其输入是一个包含多模态信息的序列，输出是下一时间步要执行的动作。输入序列的令牌（tokens）可灵活配置，包括图像观测（经CNN编码）、语言指令（经冻结的Sentence-BERT编码）、历史动作，以及核心的创新输入——语言反馈嵌入或标量奖励（即“回报”）。模型通过最小化动作预测的交叉熵损失进行训练。</p>
<p><img src="https://example.com/fig1_method_framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：FOSSIL方法灵活架构的输入序列配置示例。序列包含任务指令、图像观测、历史动作、语言反馈以及可选的其他令牌（如标量奖励）。模型通过掩码机制控制哪些令牌在训练时可见，以适配不同的实验变体。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><strong>数据生成与增强</strong>：为每个任务生成两种轨迹数据集。一是约12K条使用广度优先搜索（BFS）规划的最优路径。二是约12K条混合路径，其中包含约4K条最优路径和约8K条次优路径。次优路径通过以概率<code>p</code>用随机动作替换规划器动作来生成，随后由规划器动作纠正，从而在数据中系统性地引入错误和低效策略。</li>
<li><strong>反馈生成</strong>：为轨迹中的每一步生成两种形式的反馈，确保公平比较。<ul>
<li><strong>语言反馈</strong>：基于规则模板生成，分为“任务反馈”（评价与目标或干扰物的互动）和“可承受性反馈”（评价违反环境物理规则的动作）。反馈不仅包含判断，还包含解释。</li>
<li><strong>标量奖励</strong>：对BabyAI原始的二元奖励函数进行奖励塑形，利用与语言反馈相同的成功/失败检测机制生成中间标量奖励，使其在提供频率上与语言反馈对齐。</li>
</ul>
</li>
<li><strong>辅助自监督任务</strong>：除了主要动作预测目标外，模型可增加额外的回归头，用于预测下一时间步的标量奖励值或语言反馈嵌入，使用均方误差（MSE）损失。这些辅助损失通过可学习的权重参数与主损失平衡。预测的反馈令牌仅用于计算损失，不反馈回模型输入。此设计旨在鼓励模型学习更鲁棒的动作后果表征，并在推理时无反馈的情况下利用内部世界模型。</li>
</ol>
<p>与现有方法相比，创新点主要体现在：1）<strong>系统性利用次优数据</strong>：不是避免次优行为，而是通过反馈将其语境化并用于学习；2）<strong>公平比较反馈模态</strong>：通过对齐生成频率，直接比较语言反馈与标量奖励的效果；3）<strong>引入反馈预测任务</strong>：将其作为模仿学习中的自监督辅助目标，以增强表征学习。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在自定义环境BabyAI-XGen（基于BabyAI修改）中进行，该环境支持对任务配置和环境参数的细粒度控制，便于系统评估组合泛化。评估主要沿两个轴线：组合泛化（系统性Systematicity和能产性Productivity）和鲁棒性（对目标表征、外部扰动、对抗/缺失反馈的鲁棒性及解的效率）。此外，还测试了数据效率。</p>
<p><strong>基线方法</strong>：对比了多种基于相同Transformer架构的模型变体：仅使用最优轨迹且无步级反馈的模仿学习基线（<code>None</code>）；在次优数据上使用标量奖励（<code>Scalar</code>）或语言反馈（<code>Lang</code>）的变体；同时使用两者的变体（<code>Combo</code>）；以及增加了反馈预测（<code>+FP</code>）的版本。同时，还对比了使用PPO的在线RL基线。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>组合泛化</strong>：在系统性测试（泛化到未见过的目标颜色-形状组合）中，使用语言反馈或标量奖励的模型性能是无反馈基线的四倍以上（具体数值：基线成功率约10%，反馈模型成功率超过40%）。语言反馈与标量奖励表现相当，结合两者并加入反馈预测任务时性能最佳。</li>
</ol>
<p><img src="https://example.com/fig2_compositionality_table.png" alt="组合泛化结果表"></p>
<blockquote>
<p><strong>图2</strong>：组合泛化（系统性和能产性）实验结果汇总表。数据显示，在大多数测试场景下，使用语言反馈（Lang）或标量奖励（Scalar）的模型显著优于无反馈基线（None），且两种反馈形式性能接近。</p>
</blockquote>
<ol start="2">
<li><strong>鲁棒性</strong>：<ul>
<li><strong>子目标完成</strong>：有反馈的模型能更完整地完成任务，而非仅完成部分子目标。</li>
<li><strong>缺乏最优数据</strong>：当训练数据完全移除最优轨迹时，无反馈模型性能下降最显著，而结合了标量和语言反馈的模型受影响最小，表明步级反馈支持从不同次优解中进行轨迹拼接。</li>
<li><strong>外部扰动</strong>：在随机替换动作的扰动下，无反馈模型性能降至70%以下。而能预测反馈的语言反馈模型（<code>LANG + FP</code>）可保持75%的原始性能，与标量奖励模型相当。结合两种反馈并预测反馈的模型（<code>COMBO + FP</code>）最鲁棒，保持78%性能。</li>
</ul>
</li>
</ol>
<p><img src="https://example.com/fig3_robustness_plots.png" alt="鲁棒性分析图"></p>
<blockquote>
<p><strong>图3</strong>：鲁棒性分析图示。（a）任务完成情况分布，显示有反馈的模型完全完成任务的比例更高。（b）模型在外部扰动下的性能保持率。（c-d）模型在对抗性或缺失语言反馈情况下的性能表现，显示仅依赖语言反馈的模型性能崩溃，而结合标量奖励和反馈预测的模型能保留大部分性能。</p>
</blockquote>
<ol start="3">
<li><strong>数据效率</strong>：在仅使用25%、50%训练数据的缩放实验中，任何使用反馈来语境化次优轨迹的模型，其数据效率和缩放性都远优于仅从最优轨迹学习的IL基线或PPO基线。<code>Combo</code>变体在数据量充足时表现出最强的后期缩放潜力。</li>
</ol>
<p><img src="https://example.com/fig4_data_efficiency_curve.png" alt="数据效率曲线"></p>
<blockquote>
<p><strong>图4</strong>：数据效率与缩放曲线。展示了不同模型在仅使用部分训练数据（25%， 50%， 100%）时，在系统性泛化任务上的性能。使用反馈的模型（Scalar， Lang， Combo）在不同数据规模下均优于IL基线（None）和PPO基线，且随着数据量增加性能持续提升。</p>
</blockquote>
<p><strong>消融实验总结</strong>：辅助反馈预测任务、结合语言与标量反馈，这两个组件在提升模型对扰动和缺失反馈的鲁棒性方面贡献显著。反馈预测使模型能内部预估动作后果，而结合两种反馈则提供了互补的信号，使模型在单一信号失效时仍能保持性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>FOSSIL框架</strong>，首次系统性地利用语言反馈将模仿学习中的次优演示转化为有效的学习机会，显著提升了策略的组合泛化能力和数据效率。</li>
<li>发布了<strong>BabyAI-XGen环境</strong>，为系统评估具身智能体的组合泛化提供了可高度定制、可复现的实验平台。</li>
<li>通过严谨实验证明，<strong>语言反馈在提供频率一致的情况下，可作为标量奖励的竞争性替代或补充</strong>，为实际部署中反馈模态的选择提供了灵活性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当测试涉及训练中未出现过的物体空间关系时，语言反馈的效果较差，作者推测模型在将描述空间关系的语言进行“落地”时存在困难。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>反馈的自动生成与质量</strong>：本研究使用基于规则的反馈生成器。未来可探索利用大语言模型（LLM）生成更丰富、更自然的反馈，并研究反馈质量对学习的影响。</li>
<li><strong>扩展到更复杂的环境与任务</strong>：可在视觉更复杂、物理交互更丰富的3D环境中验证此方法的有效性。</li>
<li><strong>人机交互中的应用</strong>：该方法为通过自然语言交互（如人类教师的纠正性反馈）来在线改进机器人策略提供了可行的技术路径。</li>
<li><strong>对表征学习的深入理解</strong>：辅助反馈预测任务的有效性表明，在模仿学习中引入对动作后果的预测是学习鲁棒、接地气的多模态表征的有前景的方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身视觉语言任务中模仿学习只能利用最优演示样本、无法从次优数据中学习的问题，提出FOSSIL方法。该方法的核心是**将语言反馈嵌入作为Transformer策略的输入**，并可选地增加**辅助的自监督反馈预测目标**，使智能体能利用语言反馈理解并学习次优行为。在BabyAI-XGen环境上的实验表明，该方法能**显著提升智能体的组合泛化能力与鲁棒性**，验证了语言反馈作为标量奖励替代方案的有效性，实现了数据高效的学习。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.11307" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>