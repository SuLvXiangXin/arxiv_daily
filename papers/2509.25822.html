<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.25822" target="_blank" rel="noreferrer">2509.25822</a></span>
        <span>作者: Li Cheng Team</span>
        <span>日期: 2025-10-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习领域的主流方法通常将感知和动作解耦。直接映射方法（如BC）学习从观察到动作的静态映射；生成模型方法（如扩散策略DP、流匹配方法）则通过建模动作分布来改善时间连续性。尽管视觉-语言-动作（VLA）模型利用大模型提升了感知能力，但这些方法在生成每个短时域动作序列时，都将从单一时点观察中提取的特征视为静态并保持不变，从而忽略了中间动作反馈可用于 refine 感知的机会。然而，稳健的决策依赖于感知与动作之间的持续交互。人类自然地利用自身行动的反馈来动态细化对环境理解。</p>
<p>本文针对“现有方法在动作生成过程中感知特征保持静态，无法利用动作反馈进行适应性调整”这一具体痛点，提出了建立“感知-动作交互”的新视角。核心思路是：以扩散策略（DP）为基础，利用其在每一步去噪时产生的噪声预测，通过计算其关于潜在特征的向量-雅可比积（VJP）作为引导力，动态演化潜在观察特征，从而形成一个“动作引导观察，观察优化动作”的闭环。</p>
<h2 id="方法详解">方法详解</h2>
<p>DP-AG的整体框架建立在扩散策略（DP）之上，其核心创新在于引入了一个动态的感知-动作交互循环。输入是原始观察 <code>o_t</code> 和带噪动作 <code>a_t^k</code>，输出是去噪后的动作 <code>a_t</code> 以及动态演化后的潜在特征 <code>z~_t^k</code>。整个过程发生在生成单个动作 <code>a_t</code> 的 K 步扩散过程中。</p>
<p><img src="https://arxiv.org/html/2509.25822v4/x2.png" alt="方法概述"></p>
<blockquote>
<p><strong>图2</strong>：方法整体框架。左侧为标准的扩散策略（DP），使用静态观察特征生成动作。右侧为本文提出的DP-AG，通过DP预测噪声的VJP来引导潜在特征演化，并引入循环一致对比损失来强化感知与动作的交互。</p>
</blockquote>
<p>核心模块包含两个部分：</p>
<ol>
<li><strong>动作引导的潜在演化随机微分方程（SDE）</strong>：此模块旨在使编码后的潜在观察特征 <code>z_t</code> 能够随扩散步骤动态更新。首先，通过变分推断将观察编码为高斯后验分布 <code>q_φ(z_t|o_t)</code> 以捕获不确定性。关键创新在于，潜在特征 <code>z_t</code> 在每个扩散步骤 <code>k</code> 的演化由以下SDE描述：<br><code>d z~_t^k = VJP(â_t^k, z_t) dt + σ_φ(z_t) dW_t</code><br>其中，漂移项 <code>VJP(â_t^k, z_t)</code> 是驱动特征演化的结构化随机力，它由扩散模型噪声预测 <code>ε_θ(â_t^k, z_t, k)</code> 关于潜在特征 <code>z_t</code> 的向量-雅可比积计算得到：<code>VJP(â_t^k, z_t) = (∂ε_θ/∂z_t)^⊤ ε_θ</code>。这使得潜在特征的更新方向与降低当前动作预测不确定性的需求对齐。在实际实现中，演化后的特征通过重参数化得到：<code>z~_t^k = μ_φ(z_t) + γ σ_φ(z_t) ⊙ VJP(â_t^k, z_t)</code>，其中 <code>γ</code> 控制VJP的强度。</li>
<li><strong>循环一致对比学习</strong>：为了确保上述动态演化与动作生成过程保持连贯对齐，防止潜在特征过度漂移，本文引入了对比损失。在每一步扩散 <code>k</code>，分别计算基于静态潜在 <code>z_t</code> 的噪声预测 <code>ε_k</code> 和基于演化潜在 <code>z~_t^k</code> 的噪声预测 <code>ε~_k</code>。使用InfoNCE损失来最大化正样本对 <code>(ε_k^i, ε~_k^i)</code> 之间的相似性，同时推远小批量内其他样本构成的负样本对。该损失形成了一个循环一致性约束：静态特征下的噪声预测引导潜在更新（Act to See），更新后的特征又应能产生相似的噪声预测（See to Act），从而强化了感知-动作闭环。</li>
</ol>
<p>与现有方法（如DP）相比，创新点具体体现在：1) 将扩散过程每一步的噪声预测的VJP作为引导信号，使潜在特征能够进行与任务相关的、动态的、结构化的演化，而非保持静态；2) 设计了循环一致对比损失来显式地约束和强化感知与动作轨迹之间的相互连贯性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在仿真基准和真实机器人任务上进行了评估。使用的仿真基准包括：Push-T、Dynamic Push-T、Robomimic（Lift, Can, Square, Transport, Tool Hang任务）和Franka Kitchen。实验平台涉及仿真环境和真实的UR5机械臂操控任务。</p>
<p>对比的基线方法涵盖了多种类型：直接映射方法（LSTM-GMM, IBC, BET）、流匹配方法（FlowPolicy, AdaFlow）以及扩散策略（DP）。</p>
<p>关键实验结果如下：<br>在Push-T任务上，DP-AG的目标覆盖分数达到0.93 (img) 和 0.99 (kp)，相比最优基线DP分别提升了6%和4%。在更具挑战性的Dynamic Push-T任务上，DP-AG达到0.80，相比DP的0.65提升了13%。</p>
<p><img src="https://arxiv.org/html/2509.25822v4/x5.png" alt="表1"></p>
<blockquote>
<p><strong>表1</strong>：Push-T和Dynamic Push-T任务上的目标覆盖分数。DP-AG在所有设置下均取得了最佳性能，尤其在动态环境中优势明显。</p>
</blockquote>
<p>在Robomimic和Franka Kitchen的复杂操控任务上，DP-AG在大多数任务上达到或接近100%成功率，在部分困难设置（如Tool Hang-mh）上显著优于DP（0.96 vs 0.89）。在真实UR5机器人进行的“拾放-堆叠”和“避障推入”任务中，DP-AG相比DP基线取得了至少23%的成功率提升，并且生成的动作轨迹平滑度提升了约60%。</p>
<p><img src="https://arxiv.org/html/2509.25822v4/x6.png" alt="表2"></p>
<blockquote>
<p><strong>表2</strong>：Robomimic和Franka Kitchen任务的成功率。DP-AG在绝大多数任务上表现最佳，展示了其强大的泛化能力。</p>
</blockquote>
<p>消融实验验证了各组件贡献：</p>
<ul>
<li><p><strong>VJP引导与对比损失的重要性</strong>：移除VJP引导或对比损失均会导致性能显著下降。对比损失相较于简单的MSE对齐能带来更优的性能和潜在空间结构。<br><img src="https://arxiv.org/html/2509.25822v4/imgs/withwithout_contrastive.png" alt="消融对比"></p>
<blockquote>
<p><strong>图7</strong>：在Square任务上，同时使用VJP引导和对比损失的DP-AG性能最佳，仅使用VJP引导（无对比）或仅使用对比损失（无VJP）均会导致成功率下降。</p>
</blockquote>
</li>
<li><p><strong>VJP强度γ</strong>：实验表明需要一个适中的γ值来平衡潜在特征的更新幅度。</p>
</li>
<li><p><strong>KL散度项</strong>：KL项有助于稳定训练，但过大的权重会限制潜在特征的演化，适度权重效果最好。</p>
</li>
<li><p><strong>温度参数τ</strong>：对比损失中的温度参数τ需要仔细调整，以获得合适的相似度分布。</p>
</li>
</ul>
<p>此外，在非规则时间序列回归任务（螺旋数据集）上的实验直观验证了DP-AG的核心机制。VJP引导的流模型相比基础流模型，将MSE从0.0095降低至0.0052（提升45.3%），并且潜在特征演化呈现出更平滑、更连贯的轨迹。<br><img src="https://arxiv.org/html/2509.25822v4/x3.png" alt="回归结果"></p>
<blockquote>
<p><strong>图3</strong>：在螺旋回归任务上，VJP引导的流模型（右）比基础流模型（左）产生了更平滑的输出轨迹和更结构化的潜在空间动态，验证了感知-动作交互对连续性的提升。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了一种新颖的观察表示学习方法，通过扩散策略噪声预测的VJP来 refine 潜在特征，首次在模仿学习中建立了封闭的感知-动作交互循环；2) 从理论上形式化了动作引导的潜在SDE，推导了其变分下界，并严格证明了所提循环一致对比损失能够强制感知与动作轨迹的相互平滑性；3) 在仿真与真实机器人任务上进行了广泛验证，证明了该方法在任务成功率、收敛速度和动作平滑度上的一致且显著提升。</p>
<p>论文自身提到的局限性主要在于计算开销略有增加，因为需要计算VJP并维护动态潜在状态，但作者指出现代自动微分框架使其开销可忽略。</p>
<p>这项工作对后续研究的启示在于：将感知与动作视为一个动态交互的耦合系统，而非顺序执行的独立模块，是提升策略适应性和鲁棒性的一个有前景的方向。这种“Act to See, See to Act”的范式可扩展到其他基于生成模型的策略学习方法中，并可能为解决非稳态环境或需要主动感知的任务提供新思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有模仿学习方法将感知与动作解耦、忽视其动态互促的问题，提出动作引导扩散策略（DP-AG）。该方法通过变分推断编码潜在观测，并利用扩散策略噪声预测的向量-雅可比积构建动作引导的随机微分方程，驱动潜在状态更新；同时引入循环一致性对比损失，形成感知与动作双向强化的学习循环。实验表明，DP-AG在仿真基准和真实UR5机械臂操作任务上性能显著优于现有先进方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.25822" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>