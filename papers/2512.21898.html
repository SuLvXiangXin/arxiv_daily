<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Flexible Multitask Learning with Factorized Diffusion Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Flexible Multitask Learning with Factorized Diffusion Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.21898" target="_blank" rel="noreferrer">2512.21898</a></span>
        <span>作者: Yilun Du Team</span>
        <span>日期: 2025-12-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习已成为获取复杂机器人操作技能的有力范式。然而，将其成功扩展到多任务场景仍面临重大挑战。随着任务种类的增加，底层的动作分布变得高度多模态和多样化，通常涉及针对不同物体的不同控制策略。传统的<strong>一体化策略</strong>（monolithic policies）难以跨任务泛化、表示多种行为模式或高效适应新技能。为了应对这些限制，<strong>模块化策略架构</strong>，尤其是<strong>混合专家模型</strong>（Mixture-of-Experts, MoE）已成为一个有前景的方向。通过将策略分解为专门的组件，模块化方法提高了跨任务的可扩展性和重用性。然而，现有的基于MoE的方法通常存在<strong>训练不稳定</strong>、缺乏原则性的概率公式、以及产生的专家模块角色不清晰或重叠的问题，这限制了其可解释性。</p>
<p>本文针对上述痛点，提出了一种新的模块化扩散策略框架。核心思路是：将复杂的多模态动作分布<strong>分解</strong>为一组专门的扩散模型的<strong>组合</strong>，每个模型捕获行为空间中的一个独特子模式，并通过一个观测条件路由器在推断时动态组合，从而实现更有效的策略学习、更清晰的技能分解以及更灵活的适应能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的<strong>因子化扩散策略</strong>（Factorized Diffusion Policy, FDP）旨在开发一种可扩展至多样化操作任务并支持高效适应新任务的模块化策略架构。</p>
<p><img src="https://arxiv.org/html/2512.21898v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：FDP概述。(a) 给定观测 $\mathbf{o}<em>{t}$，多个扩散专家在每个去噪步骤预测噪声估计 $\bm{\varepsilon}</em>{i}(\mathbf{a}<em>{t}^{K},\mathbf{o}</em>{t})$。一个轻量级路由器网络计算依赖于观测的权重 ${w_{i}}$，用于将最终噪声估计组合为加权和（见(c)）。组合后的噪声估计指导超过 $K$ 步的迭代去噪过程以生成动作 $\mathbf{a}_{t}$。(b) 这种组合结构使FDP能够建模复杂的多模态分布，并支持通过选择性调整或添加扩散组件进行模块化适应。</p>
</blockquote>
<p><strong>整体框架与核心模块</strong>：FDP将策略分解为一组可组合的扩散模型。如图1所示，其工作流程如下：</p>
<ol>
<li><strong>输入</strong>：当前观测 $\mathbf{o}_{t}$（例如RGB图像和关节角度）。</li>
<li><strong>路由与组合</strong>：一个轻量级的、观测条件的多层感知机（MLP）<strong>路由器</strong>根据 $\mathbf{o}<em>{t}$ 预测一组权重 ${w</em>{t,i}}$。同时，每个<strong>扩散专家</strong>（一个去噪网络 $\bm{\varepsilon}<em>{\theta_i}$）接收加噪的动作 $\mathbf{a}</em>{t}^{k}$ 和观测 $\mathbf{o}<em>{t}$，预测其对应的噪声 $\bm{\varepsilon}</em>{\theta_i}(\mathbf{a}^{k}<em>{t},\mathbf{o}</em>{t},k)$。所有专家的预测通过路由器权重进行加权求和，得到组合噪声估计：$\sum_i w_{t,i} \bm{\varepsilon}_{\theta_i}$。</li>
<li><strong>迭代去噪与输出</strong>：从高斯噪声 $\mathbf{a}<em>{t}^{K}$ 开始，使用上述组合噪声估计按照去噪扩散概率模型（DDPM）的更新规则进行 $K$ 步迭代去噪，最终生成干净的动作 $\mathbf{a}</em>{t}^{0}$ 作为策略输出。</li>
</ol>
<p><strong>概率建模与训练</strong>：从概率角度看，FDP将目标动作分布建模为多个组件分布的加权乘积：$p(\mathbf{a}<em>{t}|\mathbf{o}</em>{t})\propto\prod_{i}p_{i}(\mathbf{a}<em>{t}|\mathbf{o}</em>{t})^{w_{t,i}}$。这对应于满足所有组件约束（可视为行为约束，如避障、精确抓取）的样本。每个组件分布 $p_i$ 由一个扩散模型建模。训练时，采用修改后的均方误差（MSE）损失联合优化所有扩散专家和路由器：<br>$$\mathcal{L}<em>{\text{MSE}}=|\bm{\epsilon}^{k}-\sum</em>{i}w_{t,i},\bm{\varepsilon}<em>{\theta</em>{i}}(\mathbf{a}^{0}<em>{t}+\bm{\epsilon}^{k},\mathbf{o}</em>{t},k)|^{2}<em>{2}$$<br>其中 $\mathbf{a}^{0}</em>{t}$ 是示范轨迹样本，$\bm{\epsilon}^{k}$ 是添加的噪声。这种<strong>端到端的联合优化</strong>确保了所有组件在每次训练中都能获得梯度信号，促进了功能专业化。</p>
<p><strong>任务适应机制</strong>：FDP的模块化结构支持高效适应新任务。适应时，不是重新训练整个模型，而是引入一个新的扩散组件 $\bm{\varepsilon}<em>{\theta</em>{\text{new}}}$，其权重通过<strong>循环利用</strong>（upcycling）从现有组件复制初始化。更新后的噪声估计函数变为：$\bm{\varepsilon}<em>{\text{adapt}} = \sum</em>{i}w_{i},\bm{\varepsilon}<em>{\theta</em>{i}} + w_{\text{new}},\bm{\varepsilon}<em>{\theta</em>{\text{new}}}$。在适应训练期间，<strong>冻结</strong>所有先前训练的组件 ${\bm{\varepsilon}<em>{\theta_i}}$，仅更新新组件 $\bm{\varepsilon}</em>{\theta_{\text{new}}}$ 和路由器。这极大地减少了可训练参数的数量，并有效缓解了灾难性遗忘。</p>
<p><strong>核心创新点</strong>：与现有MoE方法（如SDP、MoDE）的<strong>离散专家选择</strong>不同，FDP采用<strong>连续的分数（噪声估计）聚合</strong>。这避免了硬路由决策，确保了所有模块在优化过程中保持活跃，从而带来了更稳定的训练、更好的负载平衡以及更清晰的专家专业化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在模拟基准<strong>MetaWorld</strong>（6个任务）、<strong>RLBench</strong>（8个任务）和<strong>LIBERO</strong>，以及真实世界机器人操作平台（UR5e机械臂）上进行。输入为RGB图像和关节角度，输出为绝对关节角度轨迹。</p>
<p><strong>对比基线</strong>：</p>
<ul>
<li><strong>DP</strong>：一体化扩散策略。</li>
<li><strong>SDP</strong>：基于MoE的扩散策略，采用观测条件路由。</li>
<li><strong>MoDE</strong>：MoE变体，基于噪声水平进行路由。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2512.21898v2/x7.png" alt="多任务学习结果表"></p>
<blockquote>
<p><strong>图7</strong>：MetaWorld和RLBench上的多任务学习评估结果表。FDP在MetaWorld（平均74.8%）和RLBench（平均63.9%）上均取得了最高的平均成功率，超越了所有一体化及模块化基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21898v2/x8.png" alt="真实世界多任务结果表"></p>
<blockquote>
<p><strong>图8</strong>：真实世界多任务成功率表（Cube Red和Hang Low）。FDP取得了最佳或并列最佳的性能（15/20和17/20）。</p>
</blockquote>
<p>多任务学习实验表明，FDP在模拟和真实环境中均一致性地优于强基线。DP在某些单模态任务上表现良好，但在复杂多模态任务上不足。SDP和MoDE受限于MoE训练的不稳定性。FDP的连续组合机制实现了更稳定的训练和更好的专业化。</p>
<p><img src="https://arxiv.org/html/2512.21898v2/x9.png" alt="任务适应结果表"></p>
<blockquote>
<p><strong>图9</strong>：MetaWorld和RLBench上的适应评估结果表。比较了全参数微调、仅调整路由器、调整路由器+观测编码器、以及<strong>添加新模块</strong>等适应策略。FDP采用“添加新模块”策略时表现最佳（MetaWorld 92.1%， RLBench 79.8%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21898v2/x10.png" alt="真实世界适应结果表"></p>
<blockquote>
<p><strong>图10</strong>：真实世界适应成功率表（Cube Blue和Hang High）。同样，FDP在“添加新模块”的适应策略下取得了最佳性能（17/20和17/20）。</p>
</blockquote>
<p>任务适应实验验证了FDP模块化设计的优势。在有限新演示数据下，“添加新模块”的适应策略在几乎所有方法中都是最有效的，而FDP从中获益最大，因为它能够在不干扰已有技能的情况下，通过组合新组件高效学习新行为。</p>
<p><img src="https://arxiv.org/html/2512.21898v2/x11.png" alt="组件数量缩放"></p>
<blockquote>
<p><strong>图11</strong>：FDP组件数量对多任务性能影响的消融实验表。性能随组件数量增加而提升，在4个组件后趋于饱和（MetaWorld约91.3%，RLBench约63.9%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21898v2/x12.png" alt="任务数量缩放"></p>
<blockquote>
<p><strong>图12</strong>：FDP相对于DP的成功率提升随任务数量增加而变化的曲线图。随着任务数量增加，FDP的优势变得更加明显，凸显了其处理复杂多任务分布的可扩展性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21898v2/x13.png" alt="部分重建加速"></p>
<blockquote>
<p><strong>图13</strong>：RLBench上部分重建（仅使用前2个专家）的结果表。<code>FDP_top2</code>实现了2倍的推理加速，性能仅相对下降19%，展示了部署时权衡计算成本与性能的灵活性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21898v2/x14.png" alt="持续适应"></p>
<blockquote>
<p><strong>图14</strong>：LIBERO基准上的持续适应结果表。从4个专家开始预训练，然后为每个新适应任务逐步添加新专家，最终达到12个专家。FDP在整个过程中保持了最高的平均性能（67.3%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21898v2/x6.png" alt="真实世界定性对比"></p>
<blockquote>
<p><strong>图15</strong>：真实世界任务执行过程的定性对比图。顶部：Cube-X任务；底部：Hang-X任务。展示了FDP的成功案例以及基线方法的失败模式（如末端执行器姿态不精确）。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>组件数量</strong>：性能在组件数达到4个时趋于饱和，表明这是一个复杂度与性能的良好权衡点。</li>
<li><strong>任务数量</strong>：FDP相对于一体化方法的优势随着任务数量的增加而增大，证明其更适合复杂的多任务分布。</li>
<li><strong>适应策略</strong>：“添加新模块”是最有效的适应方式，平衡了性能、数据效率和计算开销。</li>
<li><strong>推理加速</strong>：通过仅使用权重最高的前几个组件（如top-2），可以显著加速推理（2倍）而性能损失可控（相对下降19%），体现了部署灵活性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种<strong>模块化扩散策略架构</strong>（FDP），通过<strong>观测条件组合采样</strong>将专门化的扩散组件组合起来，为多模态动作分布建模提供了原则性的概率框架。</li>
<li>实证表明，该组合框架不仅<strong>提高了多任务模仿学习的性能</strong>，而且促进了跨扩散模块的<strong>子技能分解</strong>，增强了可解释性。</li>
<li>提出了一种简单有效的<strong>策略适应方法</strong>，通过选择性调整或<strong>添加新的扩散组件</strong>来适应新任务，实现了卓越的样本效率、模块化重用，并缓解了灾难性遗忘。</li>
</ol>
<p><strong>局限性</strong>：论文提到，推理成本随扩散组件数量线性增加。虽然可以通过剪枝、蒸馏等技术优化，但这仍是实际部署中的一个考虑因素。此外，确定最佳组件数量（类似于网络深度或专家数）是一个需要根据任务集调整的超参数。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>模块化策略设计</strong>：FDP展示了将复杂策略分解为可组合、可解释的专门化模块的价值，这为设计更可扩展和可适应的机器人学习系统提供了新思路。</li>
<li><strong>稳定训练</strong>：通过连续聚合而非离散选择来组合模块，为解决MoE类模型中常见的训练不稳定和负载不平衡问题提供了有效方案。</li>
<li><strong>高效适应</strong>：“添加新模块+冻结旧模块”的适应范式为持续学习场景提供了高效且实用的解决方案，值得在更复杂的任务序列和跨域适应中进一步探索。</li>
<li><strong>可扩展性</strong>：FDP支持异构架构（组件可不同），未来可研究如何动态分配计算资源（如根据任务复杂度自适应调整激活的组件数量或大小），以实现更优的性能-效率权衡。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Factorized Diffusion Policy (FDP)，以解决多任务模仿学习中机器人动作分布高度多模态、现有单一策略难以有效拟合和灵活适应的问题。其核心方法是一种模块化扩散策略框架，将复杂动作分布分解为多个专门的扩散模型，每个捕获不同的行为子模式，并通过观察条件路由器在推理时动态组合。该方法基于组合扩散建模，使用连续分数聚合而非离散专家选择，以实现稳定训练并促进模块间清晰分工。实验表明，FDP在模拟基准（MetaWorld、RLBench）和真实机器人操作中，均持续优于现有的模块化及单一模型基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.21898" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>