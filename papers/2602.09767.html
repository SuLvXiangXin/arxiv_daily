<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.09767" target="_blank" rel="noreferrer">2602.09767</a></span>
        <span>作者: Wei Li Team</span>
        <span>日期: 2026-02-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在四足机器人控制领域，深度强化学习需要专家精心设计任务特定的奖励函数，而模仿学习则依赖于昂贵且任务特定的数据。无监督技能发现通过利用内在动机自主探索多样行为，有望减轻这些负担。然而，现有方法（如DIAYN）存在两个关键局限性：首先，它们通常依赖单一策略来掌握多样行为，没有对行为间的共享结构和差异进行建模，导致学习效率低下；其次，它们容易受到“奖励黑客”的影响，即奖励信号迅速增加并收敛，但学到的技能实际多样性不足，未能充分覆盖状态空间。</p>
<p>本文针对上述痛点，提出了名为MOD-Skill的新框架。其核心思路是：通过一个解耦的多判别器架构来独立评估不同观测子空间的技能多样性，以缓解奖励黑客；同时，采用一个正交混合专家策略架构，对运动技能进行分解与正交化表征，以提高学习效率和技能多样性。</p>
<h2 id="方法详解">方法详解</h2>
<p>MOD-Skill框架旨在通过无监督强化学习，发现一组由潜在变量k索引的多样化技能策略πθ(at|otp, k)。其整体流程如算法1所示：在每次训练迭代中，采样一个技能向量k，策略根据当前本体感知观测otp和k生成动作at，与环境交互后获得奖励rt并存储经验，随后使用PPO算法更新策略和价值函数，并使用交叉熵损失更新判别器。</p>
<p><img src="https://arxiv.org/html/2602.09767v1/overview.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：MOD-Skill框架总览。策略通过强化学习进行优化，判别器通过监督学习进行更新，两者协同促进技能发现。左侧展示了多判别器模块，右侧展示了正交混合专家策略架构。</p>
</blockquote>
<p>框架包含两个核心模块：</p>
<ol>
<li><p><strong>多判别器模块</strong>：该模块旨在最大化潜在技能K与观测Od之间的互信息下界。为了解决单一判别器在复杂观测空间下容易导致奖励黑客的问题，本文设计了多个判别器，每个判别器qi(k|odi)被分配到一个<strong>互不相交</strong>的观测子空间odi（例如[𝒗 𝝎]、[𝒈]、[𝜽 𝜽̇]），并独立计算内在奖励。技能发现奖励rtS是各判别器输出对数概率的平均值减去技能先验对数概率。通过这种解耦设计，鼓励智能体在所有相关的状态维度上进行多样化探索，从而缓解状态混淆和奖励黑客。</p>
</li>
<li><p><strong>正交混合专家策略</strong>：策略网络采用OMoE架构。其输入是本体感知观测otp和技能向量k的拼接。首先，一组Ne个专家网络{Ei}将输入映射为特征向量ui。然后，关键的一步是使用<strong>Gram-Schmidt过程</strong>对这些特征向量进行正交化，得到正交基{vi}，以此强制专家学习到的表征保持足够的多样性。同时，一个任务编码器T根据相同的输入计算每个正交化专家特征的权重系数α。最后，加权聚合后的表征通过一个共享的输出头f映射到动作空间。这种架构将多样化的运动技能分解为一组正交表征，并由任务编码器自适应融合，从而实现了灵活的专家组合和多样化技能的生成。</p>
</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1) 提出了观测子空间解耦的多判别器设计，从奖励机制上直接针对奖励黑客问题；2) 将正交约束引入技能发现策略的专家混合层，明确建模技能间的差异，提升了学习效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在拥有12个自由度的Unitree A1四足机器人平台上进行，使用Isaac Sim仿真器并创建4096个并行环境进行训练。策略运行频率为50Hz。对比的基线方法包括不同观测配置的单一判别器架构（SD1, SD2, SD3）以及不同的策略网络架构（MLP, MoE）。</p>
<p><strong>判别器模块消融实验</strong>：对比了SD1（观测：[𝒗 𝝎]）、SD2（观测：[𝒗 𝝎 𝒈]）、SD3（观测：完整运动观测）和本文的多判别器方法MD。</p>
<p><img src="https://arxiv.org/html/2602.09767v1/reward_discriminator.png" alt="奖励曲线对比"></p>
<blockquote>
<p><strong>图2</strong>：算法SD1、SD2、SD3和MD的奖励曲线。上图显示完整回合奖励，下图仅显示技能奖励部分。SD3的技能奖励上升最快，但出现了明显的奖励黑客（奖励高但技能实际多样性低）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.09767v1/state_coverage_discriminator.png" alt="状态空间覆盖"></p>
<blockquote>
<p><strong>图3</strong>：状态空间覆盖可视化。收集各算法发现的技能在20秒内的线性速度、角速度和投影重力观测，并进行归一化。MD方法覆盖的状态空间范围最广。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.09767v1/x1.png" alt="状态空间覆盖率数据"></p>
<blockquote>
<p><strong>表II</strong>：状态空间覆盖率。MD方法达到了58.7%的覆盖率，相较于最佳基线SD2（49.6%）提升了18.3%。</p>
</blockquote>
<p>关键结果：SD3出现了典型的奖励黑客现象，其技能奖励快速收敛，但大多数技能只是在不同姿势下保持静止。而本文的多判别器架构实现了最大的状态空间覆盖范围，整体覆盖率比最佳基线算法提高了18.3%，并产生了在姿势、步态、线速度和角速度方面变化广泛的运动技能。</p>
<p><strong>策略架构消融实验</strong>：在固定使用多判别器的前提下，对比了MLP、MoE和OMoE三种策略网络。</p>
<p><img src="https://arxiv.org/html/2602.09767v1/reward_omoe.png" alt="策略架构奖励曲线"></p>
<blockquote>
<p><strong>图5</strong>：算法OMoE、MoE和MLP的奖励曲线。OMoE获得了比MoE和MLP更快的奖励增长，表明其学习效率更高。</p>
</blockquote>
<p><strong>真实世界实验</strong>：通过域随机化（随机化地面摩擦、恢复系数、质量、动作延迟、电机扭矩并添加随机速度扰动）来缩小仿真到现实的差距。</p>
<p><img src="https://arxiv.org/html/2602.09767v1/real_world.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图4</strong>：真实世界实验。学习到的技能被部署在Unitree A1机器人上，在真实环境中展示了可靠且鲁棒的执行能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：1) 提出了MOD-Skill框架，创新性地整合了<strong>解耦的多判别器架构</strong>和<strong>正交混合专家策略</strong>，以解决无监督技能发现中的奖励黑客和低效问题；2) 实验表明，该框架将状态空间覆盖率提升了18.3%，有效缓解了奖励黑客，并显著提高了训练效率；3) 成功实现了从仿真到真实四足机器人的技能迁移验证。</p>
<p>论文自身提到的局限性在于，未来工作需要减少对专家知识的依赖，使算法能够通过自主探索发现更丰富、更动态的技能库。这对后续研究的启示在于：无监督技能发现中，对技能多样性进行<strong>结构化、解耦的激励</strong>，以及对策略内部表征施加<strong>明确的多样性约束</strong>，是提升算法性能的有效途径。如何自动化地划分观测子空间或确定专家数量，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对四足机器人无监督技能发现中，单一策略学习效率低、技能表征易重叠以及奖励信号易被黑客攻击导致技能多样性不足的问题，提出了正交混合专家（OMoE）架构和多判别器框架。OMoE防止行为表征坍塌，使单一策略能掌握广泛运动技能；多判别器在不同观测空间运作以缓解奖励黑客。在Unitree A1机器人上的实验表明，该方法提升了训练效率，并使状态空间覆盖率比基线提升了18.3%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.09767" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>