<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RealD $^2$ iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RealD $^2$ iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.22505" target="_blank" rel="noreferrer">2511.22505</a></span>
        <span>作者: Jianhua Sun Team</span>
        <span>日期: 2025-11-27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域，模仿学习是一种从专家演示中学习策略的有效方法。然而，在现实世界部署时，模仿学习策略常因感知差异而失败，即训练时使用的传感器数据（如仿真中的渲染图像或带标记的真实图像）与部署时的实际传感器观测（如真实世界的原始RGB图像）之间存在分布差异。这种“现实差距”是当前主流方法面临的关键局限性。具体而言，基于视觉的策略通常依赖于特定传感器或标注数据训练的特征提取器，这些特征在遇到新的、未标注的真实世界图像时，其表示能力会显著下降，导致策略性能崩溃。</p>
<p>本文针对模仿学习策略从仿真或带标记数据向原始真实世界图像迁移时的性能下降问题，提出了一个新视角：<strong>将策略执行视为一个条件生成问题</strong>。具体来说，本文认为，如果能够根据当前的真实世界RGB观测，生成一个与训练数据分布一致的、对齐的深度图，那么使用该生成深度图作为输入的策略就能稳定执行。本文的核心思路是：<strong>训练一个条件深度扩散模型，将在线真实RGB图像转换为与离线训练数据对齐的深度图，从而弥合视觉域差距，使离线训练的基于深度的策略能够直接应用于在线真实世界。</strong></p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的RealD $^2$ iff框架包含两个主要阶段：<strong>离线策略训练</strong>和<strong>在线扩散增强执行</strong>。</p>
<p><strong>整体框架</strong>：在离线阶段，使用易于获取的配对数据（如仿真渲染的RGB-D或带标注的真实RGB-D）训练一个基于深度输入的操作策略π(a|d)和一个条件深度扩散模型p_θ(d|o)。在线阶段，当策略部署在真实环境中时，对于每一帧新的真实RGB观测o_t，首先使用训练好的扩散模型生成一个对应的深度图d_t，然后将此生成深度图输入给离线训练好的策略π，从而输出动作a_t。该方法的核心创新在于，它不修改或微调策略本身，而是通过一个前馈的生成模型来适配输入，使策略“看到”它熟悉的深度表示。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_06_18_9e5e4c0b6f6e6e8e5c4cg-1.jpg?height=485&width=1284&top_left_y=554&top_left_x=384" alt="RealD^2iff Framework"></p>
<blockquote>
<p><strong>图1</strong>: RealD $^2$ iff整体框架。<strong>左（离线训练）</strong>：使用配对数据（RGB观测o和深度d）同时训练扩散模型p_θ(d|o)和基于深度的策略π(a|d)。<strong>右（在线执行）</strong>：对于真实世界的RGB观测o_t，使用训练好的扩散模型生成深度估计d_t，然后馈送给固定策略π以产生动作a_t。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li>**条件深度扩散模型 (p_θ(d|o))**：这是一个去噪扩散概率模型（DDPM），其条件信息是RGB观测o。模型学习将随机噪声逐步去噪，生成与条件o对应的深度图d。训练时，目标是最小化去噪网络的预测噪声与真实注入噪声之间的均方误差。网络结构采用U-Net，并将条件图像o通过卷积编码后，以通道拼接的方式在多个分辨率层面注入到U-Net的编码器中。此模块是弥合域差距的关键，它学会了从新的真实RGB域到策略熟悉的深度域的映射。</li>
<li>**基于深度的操作策略 (π(a|d))**：这是一个标准的模仿学习策略，其输入仅为深度图像d。论文中使用了两种策略架构：基于CNN的视觉运动策略（VMP）和扩散策略。策略在离线阶段使用专家演示（深度状态-动作对）进行训练，并且在整个在线阶段保持固定。策略的性能上限取决于其在“理想”深度输入上的表现。</li>
<li><strong>在线执行流程</strong>：在线运行时，对于每个时间步的RGB观测o_t，首先通过扩散模型进行多步（如50步）采样生成深度图d_t。然后，d_t被直接送入策略网络π，网络输出动作a_t并执行。整个过程是前馈的，无需对策略进行在线适应或微调。</li>
</ol>
<p><strong>创新点</strong>：与现有解决域适应的方法（如数据增强、对抗性训练、在线微调）相比，本文的创新性体现在：</p>
<ul>
<li><strong>生成式适配</strong>：将域适应问题重构为一个条件生成问题，利用强大的扩散模型生成与策略训练域一致的深度表示。</li>
<li><strong>策略不可知</strong>：该方法独立于具体策略架构，可以与任何基于深度输入的模仿学习策略结合。</li>
<li><strong>无需在线优化</strong>：在线阶段仅涉及前馈生成和推理，无需基于当前交互数据更新策略或适配器，降低了计算复杂度和部署难度。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试与数据集</strong>：在多个模拟和真实世界机器人操作任务上进行评估，包括<strong>模拟环境</strong>（Robosuite的Lift, Stack, NutAssembly）和<strong>真实世界环境</strong>（台面物体堆叠、篮子放置、书立放置）。使用来自100条专家轨迹的演示数据训练策略和扩散模型。</li>
<li><strong>评估指标</strong>：主要报告<strong>任务成功率</strong>。</li>
<li><strong>对比基线</strong>：<ol>
<li><strong>RGB Policy</strong>：直接在RGB图像上训练的模仿学习策略（作为性能上限参考，因为在线输入也是RGB）。</li>
<li><strong>Depth Policy</strong>：在“真实”深度图上训练的模仿学习策略（作为理想深度输入的参考上限）。</li>
<li><strong>Finetuning</strong>：在真实世界数据上对训练好的RGB或深度策略进行微调。</li>
<li>**Domain Randomization (DR)**：在训练时对视觉输入应用广泛的数据增强。</li>
<li><strong>Fixing the Features</strong>：冻结在仿真RGB上预训练的特征提取器，用于真实RGB输入。</li>
</ol>
</li>
<li><strong>实验平台</strong>：使用Franka Emika Panda机器人实体。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在模拟到模拟的域转移（改变纹理、光照、背景）测试中，RealD $^2$ iff方法 consistently outperforms深度策略（直接使用真实深度）、微调、域随机化和固定特征等方法，其成功率与在目标域RGB上直接训练的RGB策略相当甚至更优，证明了其有效弥合了视觉域差距。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_06_18_9e5e4c0b6f6e6e8e5c4cg-2.jpg?height=618&width=1344&top_left_y=1916&top_left_x=384" alt="Sim-to-Sim Results"></p>
<blockquote>
<p><strong>图2</strong>: 模拟到模拟域转移任务的成功率对比。RealD $^2$ iff (Ours) 在多种干扰（纹理、光照、背景）下，性能显著优于直接使用真实深度的策略、微调、域随机化等基线，并与在目标域RGB上训练的策略性能相当。</p>
</blockquote>
<p>在更具挑战性的<strong>模拟到真实</strong>转移任务中，RealD $^2$ iff展现了显著优势。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_06_18_9e5e4c0b6f6e6e8e5c4cg-3.jpg?height=678&width=1342&top_left_y=2766&top_left_x=384" alt="Sim-to-Real Results"></p>
<blockquote>
<p><strong>图3</strong>: 模拟到真实转移任务的成功率。RealD $^2$ iff将离线训练的深度策略成功迁移到真实世界，性能远超其他基线方法（微调、域随机化、固定特征），在多个任务上达到80%-100%的成功率。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文对扩散模型的关键设计进行了消融研究：</p>
<ol>
<li><strong>条件注入方式</strong>：比较了将RGB条件早期注入（在U-Net下采样层）与晚期注入（在瓶颈层）。结果显示<strong>早期注入</strong>方式效果更好，因为它为生成过程提供了更丰富的多尺度条件信息。</li>
<li><strong>条件信息类型</strong>：比较了使用RGB图像作为条件与使用在ImageNet上预训练的ResNet特征作为条件。实验表明，<strong>原始RGB作为条件</strong>优于预训练特征，因为预训练特征可能携带了与当前任务无关的偏差。</li>
<li><strong>扩散模型作为“翻译器”</strong>：定性结果表明，扩散模型生成的深度图不仅包含几何信息，还能根据任务上下文“想象”出被遮挡部分的合理深度，这是传统深度估计器难以做到的。</li>
</ol>
<p><img src="https://cdn.mathpix.com/cropped/2024_06_18_9e5e4c0b6f6e6e8e5c4cg-4.jpg?height=650&width=1342&top_left_y=3688&top_left_x=384" alt="Ablation Study"></p>
<blockquote>
<p><strong>图4</strong>: 消融实验与定性结果。**(a)** 不同条件注入方式和条件信息的成功率消融。**(b)** 定性展示：扩散模型（右）能够根据RGB输入（左）生成连贯的深度图（中），即使物体被部分遮挡（如篮子里的瓶子），而现成的单目深度估计器（MiDaS）则失败。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>问题重构</strong>：将机器人操作中的视觉域适应问题重新定义为条件深度生成问题，提出了一种新颖的、生成式的解决方案。</li>
<li><strong>实用框架</strong>：提出了RealD $^2$ iff，一个简单而有效的两阶段框架，通过条件深度扩散模型桥接现实差距，使离线训练的深度策略能零样本迁移到真实世界。</li>
<li><strong>实证验证</strong>：在模拟和真实的多种操作任务上进行了广泛实验，证明了该方法显著优于现有的域适应基线，并能与在目标域RGB上训练的策略性能相媲美。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，该方法目前依赖于在离线阶段收集的配对（RGB，深度）数据进行训练。在缺乏此类数据或目标域与训练域差异极其巨大的情况下，性能可能会下降。此外，扩散模型的采样过程（多步迭代）相比前馈网络会引入额外的计算延迟。</p>
<p><strong>启示</strong>：<br>这项工作展示了生成模型，特别是扩散模型，在机器人感知-动作耦合中的强大潜力。它启示后续研究可以探索：1) 更高效的生成模型（如一致性模型）以减少在线延迟；2) 将类似框架扩展到其他模态（如触觉、音频）的域适应；3) 结合少量在线数据进一步提升生成质量和策略鲁棒性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中模拟环境与真实世界之间的性能差距问题，提出RealD^2 iff方法，通过深度扩散技术生成逼真深度数据来弥合差距。关键技术包括基于扩散模型的深度数据合成与增强，以提升机器人感知能力。实验表明，该方法能有效改善真实场景中的操作精度和鲁棒性，但具体性能提升数据需参考论文正文详述。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.22505" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>