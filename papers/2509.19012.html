<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Pure Vision Language Action (VLA) Models: A Comprehensive Survey - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Pure Vision Language Action (VLA) Models: A Comprehensive Survey</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19012" target="_blank" rel="noreferrer">2509.19012</a></span>
        <span>作者: Zhang, Dapeng, Sun, Jing, Hu, Chenghui, Wu, Xiaoyan, Yuan, Zhenlong, Zhou, Rui, Shen, Fei, Zhou, Qingguo</span>
        <span>日期: 2025/09/23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>传统的机器人系统主要依赖预先编程的指令、精心设计的控制策略或特定任务的强化学习。这些方法在受控环境（如工厂）中有效，但在动态、非结构化的真实世界中泛化能力差。现代机器人虽然能通过计算机视觉模型“看”，通过大语言模型（LLM）“理解”，并通过控制器或学习策略“行动”，但将这些能力整合成一个连贯、统一的系统仍是关键挑战。近期，研究者开始探索利用LLM和视觉语言模型（VLM）来实现更精确、灵活的机器人操控，形成了视觉-语言-动作（VLA）基础模型。然而，针对纯VLA方法的综述非常稀缺，现有综述要么聚焦于VLM基础模型的分类，要么对整个机器人操控领域进行宽泛概述，缺乏对纯VLA方法的系统性梳理。本文旨在填补这一空白，对纯VLA方法进行聚焦而全面的回顾，提供一个清晰的分类法，系统总结VLA研究，并阐明这一快速发展领域的演进轨迹。本文的核心思路是：将VLA方法根据其动作生成策略进行分类，并系统回顾各类方法的核心策略、应用场景以及所依赖的数据集和平台。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文并非提出一种新的VLA方法，而是对现有VLA方法进行系统性的分类和综述。其“方法”即指分类框架和对各类方法技术细节的梳理。</p>
<p>论文将VLA方法主要分为四大范式：基于自回归的、基于扩散的、基于强化的以及混合/专用方法。这一分类是基于动作生成的核心策略。</p>
<p><img src="https://arxiv.org/html/2509.19012v3/vla_01.png" alt="Vision-Language-Action Taxonomy"></p>
<blockquote>
<p><strong>图3</strong>：视觉-语言-动作分类法：从基于自回归、基于扩散，到基于强化和混合/专用方法，展示了VLA领域多范式的进展与实际应用。该分类法按时间线组织。</p>
</blockquote>
<p><strong>整体框架与核心模块</strong>：VLA模型的通用框架是将视觉感知、语言理解和可执行控制统一在一个序列建模框架内。典型设计将图像和指令编码为前缀或上下文token，将机器人状态和感官反馈作为状态token注入，并以自回归方式生成动作token以产生控制序列，从而闭合感知-语言-动作循环。与传统的感知、规划、控制流程相比，VLA提供了端到端的跨模态对齐，并对目标、约束和意图进行统一处理。</p>
<p><strong>各类方法详解</strong>：</p>
<ol>
<li><p><strong>基于自回归的模型</strong>：这是VLA任务中经典且有效的序列生成范式。它将动作序列视为时间依赖过程，基于先前的上下文、感知输入和任务提示逐步生成动作。随着Transformer架构的发展，这类模型展现了可扩展性和鲁棒性。论文进一步将其细分为：</p>
<ul>
<li><strong>通用型VLA方法论</strong>：如Gato、RT-1/RT-2、PaLM-E等，它们通过统一的多模态Transformer将感知、任务指令和动作生成整合在自回归序列建模中，核心是异质模态的标记化。</li>
<li><strong>基于LLM的推理与语义规划</strong>：如Inner Monologue、Instruct2Act等，将LLM从被动输入解析器转变为VLA系统中的语义中介，通过语言驱动推理、分层规划或平台级编排来处理长视野和组合任务。</li>
<li><strong>轨迹生成与视觉对齐建模</strong>：如LATTE、VIMA、GR-1/2等，专注于解码运动轨迹或控制token，加强感知-动作映射并确保视觉-语言语义对齐。这类方法已扩展到视频预测、世界建模以及自动驾驶、无人机规划等领域。</li>
<li><strong>结构优化与高效推理机制</strong>：如HiP、FAST、BitVLA等，致力于通过分层规划、可变长度动作token、早期退出架构、量化等技术解决自回归模型在长视野任务和部署效率方面的挑战。</li>
</ul>
</li>
<li><p><strong>基于扩散的模型</strong>：这类方法将动作序列的生成建模为去噪过程。它们通常从噪声中迭代采样，逐步生成平滑、多样且物理上合理的动作轨迹。扩散模型在处理多模态分布和生成复杂、长视野序列方面具有优势。论文列举了如Diffusion Policy、ACT等代表性工作。</p>
</li>
<li><p><strong>基于强化的模型</strong>：这类方法将VLA问题视为序列决策问题，通过强化学习（RL）来优化策略。它们利用奖励信号来学习在给定视觉和语言输入下采取何种动作能最大化累积回报。结合VLM/LLM的先验知识可以改善探索和泛化。论文提到了如VLA-RL、RL-VLA等研究。</p>
</li>
<li><p><strong>混合与专用方法</strong>：此类方法融合了上述多种范式，或针对特定应用场景（如具身导航、手术机器人、自动驾驶）进行了专门设计。例如，一些方法结合了扩散模型的动作生成能力和LLM的规划能力（如DexGraspVLA），或者为特定机器人形态（如人形机器人、轮式机器人）定制VLA架构。</p>
</li>
</ol>
<p><strong>创新点</strong>：本文的主要创新在于首次为纯VLA方法提出了一个结构化的分类法（基于动作生成策略），并进行了全面、系统的回顾，涵盖了方法、应用、数据集和平台，为该新兴领域描绘了清晰的演进图景和发展脉络。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>作为一篇综述论文，本文并未进行传统意义上的模型对比实验，而是系统性地介绍了支撑VLA模型训练与评估的关键资源，并对各类方法的特性、应用场景和面临的挑战进行了分析总结。</p>
<p><strong>使用的Benchmark/数据集/实验平台</strong>：</p>
<ul>
<li><strong>数据集</strong>：论文强调了高质量数据集的重要性。代表性数据集包括整合了22个机器人数据集的<strong>Open X-Embodiment (OXE)<strong>（涵盖527种技能和160,266个任务）以及</strong>BridgeData</strong>（包含10个环境中71个任务）。此外，还利用了互联网上的大规模人类操作视频作为泛化数据集。</li>
<li><strong>仿真平台</strong>：为了弥补真实世界数据稀缺的问题，论文介绍了多种可扩展的虚拟环境仿真平台，用于生成多模态标注数据和模型评估，包括<strong>THOR</strong>、<strong>Habitat</strong>、<strong>MuJoCo</strong>、<strong>Isaac Gym</strong>和<strong>CARLA</strong>等。</li>
</ul>
<p><strong>关键结果总结</strong>：<br>论文通过表格（如表I）和文字详细列举了各类VLA方法的代表性工作及其核心贡献（例如，RT-1在13万次演示上训练，RT-2利用了网络规模的VLM知识，OpenVLA作为开源的7B参数模型在97万条轨迹上训练性能超过RT-2-X等），但并未给出统一的量化性能对比。其“结果”更多体现在对领域发展现状的系统性梳理上。</p>
<p><img src="https://arxiv.org/html/2509.19012v3/vla_02.png" alt="Organization and Structure of the VLA Survey"></p>
<blockquote>
<p><strong>图1</strong>：VLA综述的组织与结构图。展示了从背景介绍、方法分类、资源（数据集、基准、平台）综述到挑战与未来方向的完整行文逻辑。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19012v3/vla_03.png" alt="Illustration of various VLA skeleton"></p>
<blockquote>
<p><strong>图2</strong>：各种VLA骨架示意图。直观展示了不同VLA方法的基本构成要素和流程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19012v3/vla_04.png" alt="Vision-Language-Action Applications"></p>
<blockquote>
<p><strong>图4</strong>：视觉-语言-动作应用概览。展示了VLA模型在机械臂、四足机器人、人形机器人、轮式机器人（自动驾驶汽车）等多种机器人形态和场景中的应用。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了结构化的纯VLA方法分类法</strong>：首次基于动作生成策略（自回归、扩散、强化、混合/专用）对纯VLA方法进行了系统分类，为理解该领域提供了清晰的框架。</li>
<li><strong>提供了全面且聚焦的综述</strong>：不仅详细分析了各类方法的核心策略、动机和实现，还深入探讨了VLA在多种机器人平台（机械臂、四足、人形、轮式）上的应用。</li>
<li><strong>系统梳理了关键资源</strong>：对训练和评估VLA模型所依赖的基础数据集、基准测试和仿真平台进行了全面的概述。</li>
<li><strong>指出了挑战与未来方向</strong>：基于当前VLA发展现状，识别了数据限制、推理速度、安全性等关键挑战，并提出了未来研究的潜在路径。</li>
</ol>
<p><strong>局限性</strong>：论文自身作为一篇综述，其局限性在于它总结的是截至成文时的研究现状，而VLA领域发展极其迅速。文中提到的许多挑战（如安全性、可解释性、与人类价值观的对齐）在论文中指出仍 largely unresolved（很大程度上未解决）。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据与泛化</strong>：需要解决数据收集成本高、长尾案例代表性不足的问题，并开发能更好地利用网络视频等弱监督数据的方法。</li>
<li><strong>效率与部署</strong>：模型的高效推理（如通过早期退出、量化、缓存）和轻量化设计是走向实际部署的关键。</li>
<li><strong>安全与可靠性</strong>：在开放动态环境中，确保VLA模型决策的安全性、可控性和可解释性是至关重要的研究方向。</li>
<li><strong>通用性探索</strong>：VLA模型被视为实现通用具身智能的关键前沿，未来研究需致力于提升其在多样化任务和环境中无需特定调整的泛化能力，向真正的“通用”迈进。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文综述了纯视觉-语言-动作（VLA）模型，旨在解决传统机器人控制方法在复杂动态环境中灵活性不足、难以与人类及环境有效交互的核心问题。论文系统梳理了VLA领域，将关键技术方法归纳为基于自回归、扩散、强化学习、混合及专用范式等多种模型，并分析了其核心策略与实现。研究指出，通过整合视觉、语言与动作信息构建的VLA基础模型，已显著提升了机器人操作的通用性与质量，为构建可扩展的通用机器人智能体指明了方向。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19012" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>