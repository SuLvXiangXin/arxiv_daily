<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.13982" target="_blank" rel="noreferrer">2505.13982</a></span>
        <span>作者: Li, Jinzhou, Wu, Tianhao, Zhang, Jiyao, Chen, Zeyuan, Jin, Haotian, Wu, Mingdong, Shen, Yujun, Yang, Yaodong, Dong, Hao</span>
        <span>日期: 2025/05/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人灵巧操作领域，整合视觉与触觉等多模态传感数据对于提升任务泛化能力至关重要。然而，视觉与触觉在特性上存在根本差异：视觉提供全局上下文信息，而触觉则提供接触点的局部精确反馈。现有融合方法主要分为两类：数据级融合（如将点云与触觉数据在统一3D表示中直接拼接）和特征级融合（如通过编码器提取特征后简单拼接或进行跨模态预测学习）。这些方法致力于获得全面的融合特征，但普遍忽略了一个关键事实：在不同的操作阶段，每种模态所需关注的程度是不同的，不当的融合甚至可能引入干扰。例如，FoAR方法通过预测接触概率来调整触觉特征权重，但它依赖人工预设阈值来标注接触，并假设视觉特征始终占主导，这限制了其通用性。</p>
<p>本文针对“如何自适应地、无需人工标注地调整多模态注意力”这一痛点，提出利用接触产生的力信号作为自然引导的新视角。力信号能一致地反映操作阶段和交互动态，是引导注意力的理想信号。本文核心思路是：提出一个力引导的注意力融合模块，以力为查询，视觉/触觉特征为键值，自适应调整权重；并引入一个自监督的未来力预测辅助任务，结合观测力与预测力来提供接触的时序上下文，从而更有效地指导融合。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架（AdapTac）如图2所示，其输入是包含点云和触觉信号的历史观测序列，输出是未来一段时间内的机器人动作序列。Pipeline包含以下阶段：a) 使用预训练的触觉编码器编码3D触觉数据；b) 使用稀疏卷积网络编码点云数据；c) 利用编码后的视觉和触觉特征预测未来的净力；d) 将预测的未来净力与观测到的净力结合，通过注意力机制指导视觉-触觉融合；e) 将融合后的特征作为条件，输入基于扩散模型的策略网络以学习灵巧操作策略。</p>
<p><img src="https://arxiv.org/html/2505.13982v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：方法整体流程。a) 编码3D触觉；b) 编码点云；c) 预测未来净力；d) 结合观测与预测力，通过注意力机制融合视觉与触觉特征；e) 基于融合特征学习操作策略。</p>
</blockquote>
<p>核心模块有两个：力引导注意力融合模块与未来力预测辅助任务。</p>
<ol>
<li><strong>力引导注意力融合模块</strong>：首先，分别使用稀疏编码器和预训练触觉编码器提取点云特征 (\mathcal{Z}^{pc}) 和触觉特征 (\mathcal{Z}^{tac})。同时，将所有触觉单元的力（转换到相机坐标系后）求和得到观测净力 (\mathbf{F}^{n}_{\mathcal{O}})，并通过MLP投影为力嵌入 (e^{\mathbf{F}})。视觉和触觉特征也分别通过MLP投影到同一维度（512维）得到 (e^{pc}) 和 (e^{tac})。然后，以力嵌入 (e^{\mathbf{F}}) 作为查询（Query），拼接后的 ([e^{pc}, e^{tac}]) 作为键（Key）和值（Value），进行交叉注意力计算。通过Softmax函数计算出的注意力权重 (\alpha^{pc}) 和 (\alpha^{tac})，分别对视觉和触觉的值向量进行加权求和，得到最终融合特征 (\mathcal{Z}^{\text{fuse}})。该模块的创新在于利用力信号作为查询来自动、动态地分配两种模态的权重，无需任何人工标注的接触标签。</li>
<li><strong>未来力预测与引导</strong>：为了缓解因视觉信息更丰富而可能导致的注意力偏置（即过度依赖视觉），并强化触觉模态的学习，本文设计了一个自监督的未来力预测辅助任务。训练时，使用一个基于Transformer的扩散力预测头，以视觉和触觉特征为输入，预测未来n步的净力 (\mathbf{F}^{n}<em>{p})。然后将观测力 (\mathbf{F}^{n}</em>{\mathcal{O}}) 与预测力 (\mathbf{F}^{n}<em>{p}) 拼接，形成引导力 (\mathbf{F}^{n}</em>{g})，再将其投影后作为注意力模块的查询（如公式4所示）。这样，注意力模块的引导信号不仅包含当前接触信息，还包含了预测的未来接触动态，提供了时序上下文，有助于策略更合理地调整模态注意力。</li>
</ol>
<p>整个方法基于RISE（一个3D扩散模型策略）架构进行集成。总损失函数为策略损失 (\mathcal{L}<em>{\pi}) 和未来力预测损失 (\mathcal{L}</em>{\text{ffp}}) 的加权和（公式5）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实机器人平台上进行，使用了配备Leap Hand灵巧手（带PaXini三轴力触觉传感器）和RealSense L515相机的Flexiv Rizon 4机械臂系统。在三个细粒度、接触丰富的灵巧操作任务上进行评估：<strong>打开盒盖</strong>、<strong>重定向杯子</strong>和<strong>翻转发泡棉</strong>。每个任务收集约30条专家演示进行训练，评估时每个方法每个任务进行10轮试验。</p>
<p>对比的基线方法包括：1) <strong>RISE</strong>：仅使用点云的视觉策略；2) <strong>3DTacDex-P</strong>：使用预训练触觉编码器，将视觉与触觉特征直接拼接；3) <strong>FoAR</strong>：依赖人工设定阈值标注接触状态，并用预测的接触概率加权触觉特征。</p>
<p>关键定量结果如下表所示（对应论文表I）：</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="center">打开盒盖</th>
<th align="center">重定向</th>
<th align="center">翻转</th>
<th align="center">平均</th>
</tr>
</thead>
<tbody><tr>
<td align="left">RISE (仅视觉)</td>
<td align="center">90%</td>
<td align="center">90%</td>
<td align="center">10%</td>
<td align="center">63.3%</td>
</tr>
<tr>
<td align="left">3DTacDex-P (拼接)</td>
<td align="center">80%</td>
<td align="center">80%</td>
<td align="center">30%</td>
<td align="center">63.3%</td>
</tr>
<tr>
<td align="left">FoAR</td>
<td align="center">90%</td>
<td align="center">80%</td>
<td align="center">60%</td>
<td align="center">76.7%</td>
</tr>
<tr>
<td align="left"><strong>Ours (AdapTac)</strong></td>
<td align="center"><strong>100%</strong></td>
<td align="center"><strong>90%</strong></td>
<td align="center"><strong>90%</strong></td>
<td align="center"><strong>93.3%</strong></td>
</tr>
</tbody></table>
<p>我们的方法在所有任务上均取得最佳性能，平均成功率高达93%，显著优于所有基线。特别是在对触觉依赖极高的“翻转”任务上，成功率达到了90%，而仅视觉的RISE方法只有10%，简单拼接方法也只有30%，这凸显了自适应融合的有效性。</p>
<p><img src="https://arxiv.org/html/2505.13982v2/x3.png" alt="任务与注意力可视化"></p>
<blockquote>
<p><strong>图3</strong>：三个任务上的策略执行过程及注意力权重可视化。下方条形图显示了在不同操作阶段分配给触觉（蓝色）和视觉（紫色）模态的注意力权重数值。可以观察到，在非接触的“接近”阶段，注意力偏向视觉（紫色占比高）；而在需要精细接触操作的“翻转”、“打开”等阶段，注意力明显转向触觉（蓝色占比高），验证了模块的自适应能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.13982v2/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融实验结果。从左至右分别为：完整模型（Ours）、仅使用观测力引导（w/o Future Force）、无任何力引导的简单拼接（Concatenation）、以及仅使用未来力预测而不与观测力结合（w/o Observed Force）。完整模型性能最优，移除未来力预测或观测力均会导致性能下降，尤其是“翻转”任务，证明了二者结合提供时序上下文的重要性。简单拼接方法性能最差。</p>
</blockquote>
<p>消融实验（图4）表明：1) <strong>未来力预测组件至关重要</strong>：移除后（w/o Future Force）平均成功率从93%降至80%，在翻转任务上从90%暴跌至40%。2) <strong>观测力与预测力的结合是有效的</strong>：仅使用预测力而不结合观测力（w/o Observed Force）也会导致性能下降。3) <strong>力引导机制优于简单拼接</strong>：简单拼接方法（Concatenation）性能最差，平均仅63.3%。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个<strong>力引导的交叉注意力融合模块</strong>，能够利用力信号自适应地调整视觉与触觉特征的权重，无需任务特定的人工标注，实现了灵活的模态融合。2) 引入了一个<strong>自监督的未来力预测辅助任务</strong>，通过结合观测与预测的力信号为融合提供时序上下文，有效缓解了数据不平衡并引导注意力进行恰当调整。</p>
<p>论文提到的局限性包括：方法的性能在一定程度上依赖于力传感器的质量和精度；未来力预测任务需要足够的数据来学习有效的表示。</p>
<p>本文工作对后续研究的启示包括：探索除力之外的其他自然信号（如关节扭矩、本体感觉）作为多模态融合的引导；将这种自适应融合机制扩展到更多模态（如听觉、温度感知）；以及研究如何将此类方法更高效地应用于样本效率要求更高的强化学习框架中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人灵巧操作中视觉与触觉异构多模态数据融合的挑战，提出一种自适应融合方法。核心是力引导注意力融合模块，可无监督地自适应调整视觉与触觉特征的权重；并引入自监督的未来力预测辅助任务，以增强触觉表征并改善数据平衡。在三个精细接触密集型真实任务上的实验表明，该方法平均成功率高达93%，且能根据操作阶段动态调整对各模态的关注度。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.13982" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>