<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.18183" target="_blank" rel="noreferrer">2509.18183</a></span>
        <span>作者: Bian, Jinyue, Zhang, Zhaoxing, Liang, Zhengyu, Zheng, Shiwei, Zhang, Shengtao, Shen, Rong, Yang, Chen, Hou, Anzhou</span>
        <span>日期: 2025/09/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过模仿学习从大量标准演示中学习，能够根据视觉观察和文本指令生成动作策略。然而，这些视觉观察通常由固定的第三人称全局摄像头和手腕局部摄像头捕获，在不同环境中其数量和视角不可避免地存在差异，导致视觉特征存在显著区别。这种视角异构性（perspective heterogeneity）严重制约了VLA模型的泛化能力。具体而言，当部署阶段观察到的视觉特征（分布外数据，OOD）与训练数据集（分布内数据，ID）不一致时，模型性能会显著下降。</p>
<p>现有方法主要通过数据收集（如采集更多视角的图像）或数据重渲染（如从稀疏视点重建3D场景）来缓解视角不一致问题，但这些方法要么需要大量额外数据收集工作，要么引入了额外的计算负担和模态依赖（如深度信息）。本文针对VLA模型对训练与部署视角一致性依赖过强这一具体痛点，提出了一个新视角：不依赖于3D数据或额外数据收集，而是设计一个轻量级的模块，直接在潜在特征空间中对齐和融合多视角的2D图像信息，从而赋予模型视角自适应能力。</p>
<p>本文核心思路是提出一个名为VLA-LPAF的轻量级框架，通过一个基于MLP的融合模块，在潜在空间对齐来自参考视角和辅助视角的图像特征，仅使用2D图像即可微调VLA模型，使其能够处理视角不一致的观察输入。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLA-LPAF框架在通用VLA模型的基础上，增加了一个额外的2D视角融合模块。其整体流程分为训练和推理两个阶段。</p>
<p><img src="https://arxiv.org/html/2509.18183v1/figs/lpaf-vla-i2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：VLA-LPAF的训练与推理流程。左侧彩色箭头展示了三阶段训练过程：第一阶段仅用参考视角图像微调VLA模型；第二阶段用多视角数据仅训练融合模块；第三阶段同时微调融合模块和VLA模型。右侧展示了推理过程，经过微调的VLA-LPAF模型能够处理任意视角的观测图像。</p>
</blockquote>
<p><strong>训练阶段</strong>采用精心设计的三阶段流程：</p>
<ol>
<li><strong>单视角动作学习阶段</strong>：此阶段遵循标准VLA微调原则。仅使用包含参考视角的数据集𝔻_R，以任务指令l和观测图像o_t^R为输入，通过动作损失函数𝕃_action（公式1）监督微调策略模型π_θ的参数θ，同时冻结视觉编码器（ViT）的参数。</li>
<li><strong>多视角融合对齐阶段</strong>：此阶段专注于训练轻量级融合模块𝔽_θ&#39;，以培养其视角对齐能力。使用参考视角数据集𝔻_R和辅助视角数据集𝔻_M。核心思想是让融合模块将辅助视角图像o^M的编码特征，映射到参考视角图像o^R的编码特征空间。该阶段仅更新融合模块参数θ&#39;，损失函数为对齐损失𝕃_alignment（公式4），即计算ViT(o^R)与𝔽_θ&#39;(ViT(o^M))之间的L2距离。论文采用渐进式策略引入多视角数据，而非一次性全部加入，以减轻对齐模块的学习负担。</li>
<li><strong>多视角联合微调阶段</strong>：此阶段整合前两阶段所学能力，同时调整策略模型参数θ和融合模块参数θ&#39;。总损失𝕃_all为动作损失𝕃̂_action（公式3）与对齐损失𝕃_alignment之和（公式5）。</li>
</ol>
<p><strong>推理阶段</strong>，对于输入的任意视角的观测图像集{ o_t^G, L }（包含全局和手腕图像），首先进行编码和拼接，然后通过训练好的融合模块𝔽_θ&#39;将其特征对齐到参考视角的潜在空间，最后送入下游模块生成动作。</p>
<p><strong>核心创新模块</strong>是一个基于MLP构建的轻量级融合模块。该模块的设计灵感来源于网络可以将视觉特征高效投影到语言特征空间的结论。由于仅使用2D图像，无法像基于3D或相机模型的方法那样精确对齐图像空间，因此VLA-LPAF选择在由ViT编码器提取的潜在视觉特征空间中进行隐式对齐。通过训练，MLP模块能够学习到不同视角特征之间的恰当关联。</p>
<p><img src="https://arxiv.org/html/2509.18183v1/figs/lpaf-vla-i3.png" alt="特征可视化"></p>
<blockquote>
<p><strong>图4</strong>：潜在特征相似性热力图可视化对比（使用/不使用融合模块）。红色区域表示高相似性。经过融合模块处理后，与任务执行密切相关的目标物体（图中红框标记的抽屉）区域保持了红色高亮，证明了融合模块的有效性。</p>
</blockquote>
<p>与现有方法相比，VLA-LPAF的创新点在于：1) <strong>纯2D与轻量化</strong>：仅使用2D图像，无需3D重建、深度信息或额外的数据渲染，框架更加轻量。2) <strong>潜在空间融合</strong>：在特征层面进行对齐融合，而非简单地混合多视角图像数据，避免了[24]等方法因平衡数据量而牺牲原始视角性能的问题。3) <strong>分阶段训练策略</strong>：通过三阶段训练策略，确保动作学习和视角对齐能力得到充分、有序的训练。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验旨在验证VLA-LPAF能否有效提升VLA模型的视角自适应能力。作者将VLA-LPAF框架实例化到RoboFlamingo模型上，构建了RoboFlamingo-LPAF。</p>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准模型</strong>：原始RoboFlamingo作为基线。</li>
<li><strong>对比方法</strong>：基线模型按照[24]的数据添加策略进行微调（即混合多视角图像并平衡数据量）。</li>
<li><strong>数据集</strong>：在CALVIN、LIBERO-Goal和自定义的模拟基准CabinEnv（包含按钮和拉杆任务）上进行训练和评估。为了公平比较，用于微调基线模型和VLA-LPAF的数据轨迹总量相同。</li>
<li><strong>真实世界验证平台</strong>：使用Realman机械臂，配备Intel RealSense D415（参考视角和手腕摄像头）和Azure Kinect（辅助全局视角摄像头）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>模拟环境性能对比</strong>：在三个模拟数据集上，RoboFlamingo-LPAF相比基线模型平均任务成功率均有显著提升。<br><img src="https://arxiv.org/html/2509.18183v1/figs/lpaf-vla-i5.png" alt="CALVIN结果"></p>
<blockquote>
<p><strong>图6</strong>：在多视角CALVIN数据集上的性能对比。RoboFlamingo-LPAF（橙色）在绝大多数测试视角上的成功率均高于基线（蓝色）。<br><img src="https://arxiv.org/html/2509.18183v1/figs/lpaf-vla-i6.png" alt="LIBERO结果"><br><strong>图7</strong>：在多视角LIBERO-Goal数据集上的性能对比。RoboFlamingo-LPAF表现同样优于基线。<br><img src="https://arxiv.org/html/2509.18183v1/figs/lpaf-vla-i7.png" alt="CabinEnv结果"><br><strong>图8</strong>：在自定义CabinEnv数据集上的性能对比。VLA-LPAF带来了显著的性能提升。</p>
</blockquote>
</li>
<li><p><strong>真实世界任务验证</strong>：选取了训练数据中未包含的30°视角进行推理测试。<br><img src="https://arxiv.org/html/2509.18183v1/figs/lpaf-vla-i8.png" alt="真实世界对比"></p>
<blockquote>
<p><strong>图9</strong>：真实世界多视角任务执行对比。RoboFlamingo-LPAF成功完成了四项任务，而基线模型均失败。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验</strong>（在LIBERO-Goal数据集上进行）：</p>
<ol>
<li><p><strong>对齐损失函数</strong>：对比了基于余弦相似度（COS）和均方误差（MSE）的损失。COS损失带来了更高的平均成功率（86.42% vs 84.26%）。<br><img src="https://arxiv.org/html/2509.18183v1/figs/lpaf-vla-i9.png" alt="损失函数消融"></p>
<blockquote>
<p><strong>图10</strong>：不同对齐损失项下的性能对比。基于COS损失的策略（橙色）在多数视角上优于MSE（绿色）。</p>
</blockquote>
</li>
<li><p><strong>多视角数据引入策略</strong>：对比了“一次性加入”和“渐进式加入”所有辅助视角数据。渐进式策略平均成功率更高（87.79% vs 86.42%）。<br><img src="https://arxiv.org/html/2509.18183v1/figs/lpaf-vla-i10.png" alt="数据策略消融"></p>
<blockquote>
<p><strong>图11</strong>：不同多视角数据参与策略的对比。渐进式策略（橙色）整体表现更优。</p>
</blockquote>
</li>
<li><p><strong>参数更新策略</strong>：在第三阶段联合微调时，对比了同时更新θ和θ‘与冻结θ仅更新θ’的策略。同时更新参数策略获得了略高的平均成功率（88.84% vs 88.63%）。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>提出了轻量级的VLA-LPAF框架</strong>，首次实现了仅利用2D图像在潜在空间进行视角特征融合，有效降低VLA模型对训练-部署视角一致性的依赖。2) <strong>实例化并验证了框架的有效性</strong>，通过构建RoboFlamingo-LPAF，在多个模拟数据集和真实任务中证明了其显著的视角泛化性能提升（在CALVIN、LIBERO、CabinEnv上平均提升约8%、15%、30%）。3) <strong>设计了系统的训练策略与消融实验</strong>，为轻量级多视角融合提供了可复现的方案和深入的组件分析。</p>
<p>论文自身提到的局限性包括：1) VLA-LPAF需要在更多VLA模型上进行实例化和测试。2) 融合模块可能存在更高效且不损失性能的实现形式。</p>
<p>这项工作对后续研究的启示是：为提升机器人模型的视角泛化能力提供了一条不依赖于3D重建、仅使用2D数据的轻量化技术路径。未来的研究可以探索更高效的融合网络结构，并将该框架扩展到更多类型的VLA模型和更复杂的任务场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型因视觉观察视角异构（如全局与局部摄像头差异）导致的泛化能力受限问题，提出轻量级视角自适应融合模块VLA-LPAF。该模块仅使用2D数据，通过单视角图像微调并在潜在空间融合多视角观测，以高效弥合视角不一致性。基于RoboFlamingo构建的RoboFlamingo-LPAF在CALVIN、LIBERO及自定义仿真基准上平均任务成功率分别提升约8%、15%和30%，并在真实任务中展现出有效的视角自适应能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.18183" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>