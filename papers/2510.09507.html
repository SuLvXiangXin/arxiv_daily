<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.09507" target="_blank" rel="noreferrer">2510.09507</a></span>
        <span>作者: Zhang, Zixin, Chen, Kanghao, Lin, Xingwang, Jiang, Lutao, Zheng, Xu, Lyu, Yuanhuiyi, Guo, Litao, Li, Yinchuan, Chen, Ying-Cong</span>
        <span>日期: 2025/10/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>现代多模态大语言模型（MLLMs）凭借其丰富的常识知识，在具身人工智能中作为高层规划器或下游视觉-语言-动作（VLA）模型的骨干得到广泛应用。然而，尽管已有研究初步表明MLLMs具备一定的工具理解能力，但其对物理工具的真正理解深度尚未被量化评估。现有的相关基准（如A4Bench）采用简单的“工具-功能”问答形式，缺乏实际应用导向，无法严格评估模型是否运用了真正的知识和推理来选择最优工具。本文针对MLLMs物理工具理解能力评估缺失这一具体痛点，提出了首个专门用于评估MLLMs物理工具理解能力的基准。其核心思路是构建一个包含三个渐进难度等级（工具识别、工具理解、工具创造）的视觉问答（VQA）数据集，以模拟机器人工作流程，系统性地评估模型将任务需求与视觉场景相结合进行推理的能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>PhysToolBench是一个用于评估MLLM物理工具理解能力的VQA基准，包含超过1000个文本-图像对。每个样本包含一个描述具体任务的文本提示，以及一张1024x1024分辨率的图像，图像中包含多个带有数字标签的工具和物体。一个核心设计约束是明确告知MLLM图像中的物品是唯一可用的资源，以模拟资源有限的真实机器人场景。MLLM的目标是分析任务和视觉信息，然后输出所需工具的编号标签，若无合适工具则输出“None”。</p>
<p><img src="https://arxiv.org/html/2510.09507v1/imgs/hammer_and_wrench.png" alt="PhysToolBench概述"></p>
<blockquote>
<p><strong>图1</strong>：PhysToolBench基准概述。上图展示了具身智能体使用工具的重要性。下图展示了PhysToolBench的评估框架，包含三个渐进难度等级（Easy, Medium, Hard），采用VQA格式。</p>
</blockquote>
<p>基准的核心设计原则是渐进式评估，分为三个难度等级：</p>
<ol>
<li><strong>Easy（工具识别）</strong>：评估基础能力。任务提示直接，图像中始终包含一个主要功能与任务直接匹配的常规工具（例如，任务“切蔬菜”，图像中有菜刀）。这仅需基本的工具识别和常识。</li>
<li><strong>Medium（工具理解）</strong>：需要更深层次的理解和基于特定任务约束的推理。此层级细分为三个挑战：<ul>
<li><strong>M.1. 属性理解</strong>：要求理解工具的特定属性（例如，为“高温煎炸”选择铸铁煎锅）。</li>
<li><strong>M.2. 工具组合</strong>：评估组合工具以解锁新功能的能力（例如，为使用遥控器而选择电池）。</li>
<li><strong>M.3. 可用性理解</strong>：测试识别非功能性工具的能力（例如，识别一个破裂的皮搋子不可用）。</li>
</ul>
</li>
<li><strong>Hard（工具创造）</strong>：评估高阶推理和创造力。当没有标准工具时，模型必须根据任务需求逆向思考，创新性地利用周围物体（例如，用硬币代替一字螺丝刀拧螺丝）。</li>
</ol>
<p>数据集收集过程分为三个阶段以确保质量：1) <strong>概念化</strong>：专家设计符合难度标准任务-场景对；2) <strong>图像生成</strong>：主要使用GPT-4o-image（约90%）将场景描述转化为图像，复杂物体则采用实物摆放拍摄（约10%）；3) <strong>标注与验证</strong>：专家对图像中物体进行数字标注，并由独立团队审核。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在PhysToolBench上进行，全面评估了四类共32个先进的MLLMs：1) 通用专有MLLMs（如GPT-5, o3, Claude-3-7-Sonnet）；2) 通用开源MLLMs（如Qwen-2.5-VL系列, InternVL系列, GLM-4.5V）；3) 具身专用MLLMs（如RoboBrain-2, Embodied-R1）；4) VLA模型中的MLLM骨干（如OpenVLA的Prismatic-7B, π0的PaliGemma-3B）。使用统一的文本提示，鼓励模型进行思维链推理，并招募了5名人类参与者作为参考。</p>
<p><img src="https://arxiv.org/html/2510.09507v1/x2.png" alt="MLLM排行榜"></p>
<blockquote>
<p><strong>图3</strong>：PhysToolBench上的MLLM排行榜，按总体性能排序。专有模型表现最佳，开源模型次之，VLA骨干模型表现最弱。</p>
</blockquote>
<p>关键实验结果如下（数据来自论文表1）：</p>
<ul>
<li><strong>总体表现</strong>：所有MLLMs表现不佳，大多数总分低于60%，与人类参与者（最差87.85%，最佳93.19%）存在巨大差距。表现最好的专有模型GPT-5总体准确率为62.15%。开源模型中GLM-4.5V-108B以55.14%领先。VLA骨干模型表现极弱，普遍低于15%。</li>
<li><strong>难度层级</strong>：随着难度增加，模型性能显著下降。即使在Easy级别，顶级模型（如GPT-5）准确率也仅为90.16%，而人类接近100%。在Medium的M3（可用性理解）级别，模型表现甚至差于需要复杂推理的Hard级别。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.09507v1/x3.png" alt="开源模型性能与规模关系"></p>
<blockquote>
<p><strong>图4</strong>：开源MLLMs的总体性能与模型规模关系图。性能与模型规模呈显著正相关。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.09507v1/x4.png" alt="具身模型与基模型性能对比"></p>
<blockquote>
<p><strong>图5</strong>：具身模型与其基础模型的性能对比。具身专用微调并未带来工具理解能力的提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.09507v1/x5.png" alt="各难度示例及模型回答"></p>
<blockquote>
<p><strong>图6</strong>：PhysToolBench各难度级别的示例以及顶级模型和人类的回答。展示了模型在长尾识别、空间推理、可用性判断等方面的典型错误。</p>
</blockquote>
<p>主要研究发现（F.1-F.6）：</p>
<ol>
<li><strong>能力涌现与规模相关</strong>：物理工具理解能力在模型达到一定规模（约100亿参数）时涌现。规模小于50亿参数的模型在Easy任务上准确率通常低于50%，总体准确率低于25%。</li>
<li><strong>存在长尾问题</strong>：即使先进模型，对不常见物品（如特定型号数据线）的识别和理解能力也明显下降。</li>
<li><strong>具身专用模型无优势</strong>：针对具身任务微调的模型（如RoboBrain2）在基准上的表现并未优于其同规模的基础通用模型。</li>
<li><strong>工具可用性理解严重缺陷</strong>：模型难以判断工具是否因损坏而不可用（M3级别），表明其理解停留在表面关联而非功能原理层面，会产生“工具可用性幻觉”。</li>
<li><strong>VLA骨干模型能力极弱</strong>：当前VLA模型所使用的MLLM骨干在本基准上表现极差（均低于15%），质疑了其通过机器人数据微调即可继承并泛化“常识”的假设。</li>
<li><strong>推理能力重要但不足</strong>：思维链提示能显著提升模型性能（见表2）。专门优化推理的模型（如GLM-4.5V, Ovis-2.5-9B）表现突出。但当前推理能力仍不充分，尤其在空间推理方面存在缺陷。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09507v1/x6.png" alt="文本级推理与视觉中心推理对比"></p>
<blockquote>
<p><strong>图7</strong>：（a）当前MLLM偏重于文本级推理，（b）本文提出的视觉中心推理框架。</p>
</blockquote>
<p><strong>初步解决方案</strong>：针对模型存在的“模态偏差”（重文本轻视觉），论文提出了一个<strong>视觉中心推理</strong>框架。该框架以MLLM为骨干，将回答过程分解为三步：1) <strong>全局分析</strong>：结合图像对查询形成整体理解；2) <strong>深度分析</strong>：调用目标检测工具（DINOX）定位并裁剪物体，进行细节分析；3) <strong>多级证据集成与推理</strong>：综合全局和细节分析得出最终答案。在模型表现最差的M3难度上，该框架使GPT-4o和GPT-5的性能分别提升了10.24%和18.06%。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了首个专门用于系统评估MLLMs物理工具理解能力的基准PhysToolBench，采用渐进式难度和面向应用的VQA格式；2）通过对32个各类MLLMs的全面评估，揭示了当前模型在此能力上存在严重缺陷，远未达到人类水平，并系统分析了六大关键发现；3）针对发现的“模态偏差”问题，提出了一个视觉中心推理的初步解决方案并验证了其有效性。</p>
<p>论文提到的局限性包括：基准规模（超过1000个样本）可能不足以全面覆盖所有工具场景；部分图像由生成模型创建，可能与真实物理世界存在差异。</p>
<p>本研究对后续工作的启示包括：1）推进具身智能需要更大规模、更强能力的MLLM作为基础；2）需要收集更高质量、专注于工具理解的机器人数据集；3）应着重开发视觉中心的推理方法，以提升模型的空间和物理推理能力；4）模型对工具可用性的“幻觉”是具身AI安全的重要隐患，必须加以解决。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多模态大语言模型在物理工具理解能力上缺乏量化评估的问题，提出了首个专用基准测试PhysToolBench。该基准采用视觉问答形式，包含超过1000个图像-文本对，设置了工具识别、工具理解与工具创造三个渐进难度层级，以系统评估模型对工具功能、原理及创新运用的认知。通过对32个主流MLLMs的全面测试，研究发现现有模型在物理工具理解方面存在显著不足，并进一步提供了深入分析与初步改进方案。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.09507" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>