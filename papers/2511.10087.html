<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.10087" target="_blank" rel="noreferrer">2511.10087</a></span>
        <span>作者: Xiaocong Li Team</span>
        <span>日期: 2025-11-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>离线到在线强化学习（O2O-RL）已成为安全高效机器人策略部署的关键范式，它利用静态离线数据集预训练基础策略以更准确地捕获物理动力学，从而减少现实世界试错风险，并通过环境交互进一步微调策略以适应动态场景。然而，现有方法面临两大根本性挑战：多模态行为覆盖不足以及在线适应期间的分布偏移。具体而言，传统的行为克隆（BC）严重依赖大量专家数据，难以覆盖多模态动作分布；而扩散策略等主流生成模型虽然在离线建模中表现出色，但其固定的噪声调度和缺乏环境反馈常导致在线微调期间的策略退化和分布偏移。最近的框架（如Uni-O4）在整合离线和在线学习方面取得了显著进展，但仍存在离线预训练效率低、生成模型适应性弱以及可扩展性不足等问题，导致计算成本高、物理执行层面多样性不足、数据效率和泛化能力差。本文针对这些痛点，借鉴大语言模型预训练与微调的策略，提出了一个统一的生成式O2O-RL框架UEPO。其核心思路是：通过多种子扩散策略高效捕获多样行为模态；引入动态分歧正则化机制增强策略在物理执行层面的多样性；并利用基于扩散的数据增强提升动力学模型的泛化能力，从而弥合离线生成策略与在线微调之间的鸿沟。</p>
<h2 id="方法详解">方法详解</h2>
<p>UEPO框架的核心创新无缝集成于覆盖离线和在线阶段的学习流程中。</p>
<p><img src="https://arxiv.org/html/2511.10087v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：UEPO整体框架。左侧为离线初始化阶段，采用多种子扩散采样策略生成多个子策略；中间为离线优化阶段，通过动态分歧正则化机制增强策略多样性，并联合使用真实数据与合成轨迹训练动力学模型 $\hat{T}$ 以提升其泛化能力；右侧为在线微调阶段，选取一个合格的策略作为初始化。</p>
</blockquote>
<p><strong>1. 基于扩散模型的条件动作序列生成</strong>：为克服BC的局限性，UEPO采用状态条件扩散策略来建模整个动作序列分布 $p(a_{1:T} \mid s_{1:T})$，以捕获离线数据中的长时依赖和多模态行为。与传统需要训练多个独立模型的高成本集成方法不同，UEPO通过改变反向采样过程中的初始噪声种子，从单个训练好的扩散模型中构建出 $n$ 个子策略 ${\pi_{\theta}^{i}}<em>{i=1}^{n}$。每个子策略 $\pi</em>{\theta}^{i}$ 基于相同的状态序列 $s_{1:T}$，但使用不同的随机种子 $\epsilon_{i} \sim \mathcal{N}(0, \mathbf{I})$ 初始化反向过程，从而生成对应不同行为模态的动作序列 $a_{1:T}^{i}$，在降低训练成本的同时自然促进了子策略的多样性。</p>
<p><strong>2. 由扩散采样引导的分歧正则化增强</strong>：上述集成策略提供了初始多样性，但为确保子策略在动态执行中表现出差异，UEPO将动态分歧约束直接引入扩散采样过程。这区别于Uni-O4仅对单步动作分布施加KL散度惩罚的方法，后者可能导致策略静态不同但动态相似或相互冲突。</p>
<ul>
<li><strong>动态分歧约束</strong>：在为第 $i$ 个子策略生成动作序列 $a_i$ 时，测量其与同一采样轮次中已生成的其他子策略 ${a_j \mid j &lt; i}$ 之间的分歧。定义基于动力学差异（一阶速度和二阶加速度）的度量：$\text{div}(a_{i}, a_{j})=\frac{1}{T}\sum_{t=1}^{T}\left(|\dot{a}<em>{i,t}-\dot{a}</em>{j,t}|<em>{2}+(1-\cos(\ddot{a}</em>{i,t}, \ddot{a}<em>{j,t}))\right)$。若分歧低于阈值 $\tau$，则对反向过程中的当前去噪估计 $a_t^i$ 进行自适应扰动：$a</em>{t}^{i} \leftarrow a_{t}^{i} + \delta, \quad \delta \sim \mathcal{N}(0, \sigma_{\text{div}}^{2}\mathbf{I})$，其中 $\sigma_{\text{div}}$ 随分歧减小而增大，以此迫使子策略探索不同的动态模式。</li>
<li><strong>与序列级KL正则化的协同</strong>：UEPO保留了Uni-O4中的KL散度惩罚项以确保全局层面的分布多样性，但将其应用从单步动作分布重新定义为整个动作序列分布，这与基于序列的扩散策略自然对齐。每个子策略 $\hat{\pi}^{i}$ 的总体目标是最大化序列生成概率，并加上一个基于序列概率的KL正则项。这种局部动态约束与全局序列级正则器的结合，有效增强了子策略的多样性。</li>
</ul>
<p><strong>3. 利用扩散增强动力学模型泛化</strong>：针对模型基RL中动力学模型 $\hat{T}(s&#39; \mid s, a)$ 在离线数据集覆盖不足时泛化能力有限的问题，UEPO使用预训练的扩散策略生成物理上合理的虚拟轨迹来扩充动力学模型的训练数据。如算法1所示，该过程从离线数据分布中采样初始状态，使用扩散策略生成多步动作序列，并通过真实动力学模型（或模拟器）推出状态转移，形成完整轨迹。为确保增强数据与底层物理一致性，使用KL散度作为过滤准则，仅保留与初始动力学模型预测差异小的轨迹。最终，将过滤后的虚拟轨迹数据集 $\mathcal{D}_{\text{diff}}$（规模约为真实数据 $\mathcal{D}$ 的2-3倍）与真实数据联合，通过最大似然目标训练动力学模型，显著提升了模型对未见状态-动作空间的泛化能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在广泛的离线RL基准D4RL上进行评估，涵盖运动控制（如halfcheetah, hopper, walker2d）、灵巧操作（Adroit，如pen, hammer, door, relocate）和厨房任务数据集。对比的基线方法包括CQL、TD3+BC、IQL、COMBO、BPPO、ATAC、BC以及当前先进的O2O-RL方法Uni-O4。</p>
<p>关键实验结果总结如下：在运动控制任务上，UEPO相比Uni-O4取得了+5.9%的绝对性能提升（总分864.6 vs. 816.4）；在更具挑战性的灵巧操作任务上，提升更为显著，达到+12.4%（总分324.4 vs. 288.6）。具体到任务，例如在<code>hopper-medium-v2</code>上，UEPO获得108分，优于Uni-O4的104.4分；在<code>pen-human</code>任务上，UEPO达到122.8分，高于Uni-O4的116.2分。总体而言，UEPO在D4RL基准的总得分上达到了1419.5，显著超越了所有对比的基线方法，展示了其强大的泛化能力和可扩展性。</p>
<p>表1展示了详细的数值结果，涵盖了不同数据质量（medium, medium-replay, medium-expert, human, cloned）下的多个任务。UEPO在绝大多数任务上都取得了最佳或接近最佳的性能，特别是在数据质量较差的<code>medium-replay</code>和<code>human</code>数据集上提升明显，验证了其数据效率和从有限数据中学习多样策略的能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了一个统一的生成式离线到在线强化学习框架UEPO，通过多种子动力学感知扩散策略高效捕获离线数据中的多模态行为，无需训练多个模型，降低了计算成本和对专家数据的依赖。2) 设计了动态分歧正则化机制，将基于物理执行层面（速度、加速度）的局部动态约束与序列级别的全局KL正则化相结合，有效增强了策略集合在动态执行中的多样性，缓解了分布偏移。3) 利用扩散策略生成物理合理的虚拟轨迹进行数据增强，提升了动力学模型在未见状态-动作区域的泛化能力，从而改善了在线学习的样本效率。</p>
<p>论文自身并未明确讨论局限性，但基于方法描述，潜在的考量可能包括扩散模型采样本身的计算开销，以及动态分歧阈值、扰动强度等超参数需要根据任务进行调整。这项工作对后续研究的启示在于：它展示了借鉴大语言模型“预训练-微调”范式以及利用生成模型（如扩散模型）同时进行策略学习和数据合成的潜力，为构建更鲁棒、更具泛化能力且数据高效的机器人学习系统提供了新的思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对离线到在线强化学习（O2O-RL）中多模态行为覆盖不足与在线适应时分布偏移的核心问题，提出统一生成框架UEPO。其关键技术包括：多种子动态感知扩散策略以高效捕获多模态行为；动态分歧正则化机制确保策略多样性符合物理约束；基于扩散的数据增强模块提升动力学模型泛化能力。在D4RL基准测试中，UEPO在运动任务上较Uni-O4绝对性能提升5.9%，在灵巧操作任务上提升12.4%，展现了优异的泛化性与可扩展性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.10087" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>