<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.06079" target="_blank" rel="noreferrer">2505.06079</a></span>
        <span>作者: Huang, Shuaiyi, Levy, Mara, Gupta, Anubhav, Ekpo, Daniel, Zheng, Ruijie, Shrivastava, Abhinav</span>
        <span>日期: 2025/05/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>偏好强化学习通过人类或视觉语言模型对轨迹片段进行偏好比较来替代显式奖励函数设计，已成为一种有前景的方法。然而，实践中收集的偏好反馈通常存在噪声且不一致，人类反馈存在偏见，而VLM生成的标签在处理视觉内容、任务特定文本和时间动态方面存在困难。实验分析表明，VLM偏好标签的噪声率可高达40%，而即使10%的标签损坏率也会显著降低算法性能，这凸显了对鲁棒偏好强化学习方法的需求。</p>
<p>现有方法如RIME引入了动态样本选择方法，但本文认为，单个模型在噪声估计中可能存在偏差，导致错误累积。本文针对噪声偏好标签这一具体痛点，提出了一种新视角：利用多个对等模型从一批噪声标签中识别干净样本，而非依赖单一模型。核心思路是提出TREND框架，集成少量专家示范，并采用一种三教学策略，让三个奖励模型协作选择干净的偏好样本进行训练，从而在极端噪声下实现鲁棒学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>TREND的整体框架包含离线预训练和在线训练两个主要阶段。首先，使用少量专家示范通过行为克隆预训练策略网络，以进行有效探索。在线阶段，从人类或VLM收集带噪声的偏好，然后应用三教学策略进行去噪的奖励学习，最后用学习到的奖励模型指导智能体训练。</p>
<p><img src="https://arxiv.org/html/2505.06079v1/extracted/6425670/figures/overview_v2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：TREND方法整体框架。左侧（A）为使用少量专家示范进行行为克隆预训练的阶段。右侧在线训练阶段包含三个步骤：（B1）从人类或VLM收集噪声偏好；（B2）应用三教学策略进行去噪奖励学习，三个协作的奖励模型相互为对方识别干净的偏好样本；（B3）使用学习到的奖励模型指导智能体训练。</p>
</blockquote>
<p>核心模块是<strong>三教学偏好数据选择策略</strong>。该方法同时训练三个奖励网络，每个网络根据“小损失原则”，将损失较小的偏好对视为标签干净的“有用知识”，并将其传授给对等网络用于参数更新。形式上，给定一批偏好数据，每个奖励模型计算其选择损失。每个模型识别出批次中具有最小选择损失的样本子集（由选择率γ控制保留比例），这些样本被视为干净的，并传递给一个指定的对等模型进行训练。三个模型形成一个循环教学链：模型1为模型2选择，模型2为模型3选择，模型3为模型1选择。更新时，每个模型使用其对等模型为其选择的干净数据来计算梯度并更新参数。为确保模型多样性，采用了不同的权重初始化和输入样本顺序排列。</p>
<p>另一个核心模块是<strong>少量专家示范的整合</strong>。为对抗极端噪声，在预训练和在线PbRL适应中都整合了少量专家示范。在预训练阶段，用基于示范的行为克隆替代PEBBLE中基于状态熵的无监督探索，为策略提供了更强的先验。在线训练阶段，对于每个训练批次，以α%的比例从专家示范数据集中采样，其余从RL经验回放池采样。对于专家数据，使用行为克隆损失更新策略网络；对于非专家数据，则使用标准的SAC损失进行更新。这确保了至少部分训练数据在线学习期间是无噪声的。</p>
<p>与现有方法相比，创新点体现在：1) 提出三教学协作选择机制，每个对等模型独立发展样本选择能力，增强了噪声鲁棒性，且干净样本的定义是从对等模型中动态学习的，而非固定；2) 将少量专家示范系统地整合到预训练和在线训练中，提供了干净的监督信号并改善了探索。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Meta-world的机器人操作任务上进行，包括Button-Press、Drawer-Open和Hammer任务。对比的基线方法包括：PEBBLE、使用专家示范进行策略初始化的PEBBLE+Demo、以及最新的去噪方法RIME和RIME+Demo。实验设置了不同的模拟噪声率（0%， 20%， 40%）来评估方法鲁棒性，并测试了使用真实VLM生成噪声标签的情况。</p>
<p><img src="https://arxiv.org/html/2505.06079v1/x1.png" alt="学习曲线"></p>
<blockquote>
<p><strong>图2</strong>：Meta-world上机器人操作任务的学习曲线。每行代表一个特定任务的结果，每列对应不同的错误率ε。结果在五个随机种子上取平均，阴影区域表示标准差。TREND（红色）在几乎所有设置下都显著且稳定地优于基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.06079v1/x2.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图3</strong>：在40%噪声率下，不同方法使用1/3个示范的最终成功率对比。TREND在所有三个任务上都取得了最高成功率，相比基线有大幅提升（Button-Press提升近40%，Drawer-Open提升60%，Hammer提升70%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.06079v1/x3.png" alt="VLM噪声结果"></p>
<blockquote>
<p><strong>图4</strong>：使用真实VLM生成噪声偏好标签（噪声率约40%）时，在Drawer-Open任务上的学习曲线。TREND取得了超过80%的成功率，相比最佳基线有超过40%的提升，展示了其处理真实噪声数据的鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.06079v1/x4.png" alt="消融研究"></p>
<blockquote>
<p><strong>图5</strong>：在Drawer-Open任务上（40%噪声率）的消融研究。对比了TREND完整方法、仅使用三教学无示范、仅使用示范无三教学、以及使用双教学策略。结果表明，三教学和专家示范都是提升性能的关键组件，且三教学优于双教学。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.06079v1/x5.png" alt="选择率分析"></p>
<blockquote>
<p><strong>图6</strong>：不同样本选择率γ对TREND性能的影响（Drawer-Open任务，40%噪声率）。结果表明，适中的选择率（如0.5）能取得最佳性能，选择率过高或过低都会损害效果。</p>
</blockquote>
<p>关键实验结果总结：在40%的极端噪声率下，仅使用1-3个示范，TREND能达到约80%的成功率。具体来说，在Button-Press、Drawer-Open和Hammer任务上，相比使用相同数量示范的基线，TREND分别带来了近40%、60%和70%的成功率提升。即使使用噪声VLM生成的偏好标签，TREND在Drawer-Open任务上也展示了超过40%的成功率提升。消融实验表明，三教学策略和整合专家示范这两个核心组件都对性能有显著贡献，且三教学策略优于简单的双教学或模型预测平均策略。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了TREND框架，将少量专家示范与三教学策略相结合，用于噪声环境下的鲁棒偏好强化学习；2) 设计了一种简单有效的三教学策略，利用对等模型进行循环干净反馈选择，实现有效的标签去噪；3) 在Meta-world的多种机器人操作任务上验证了方法的优越性，即使在高达40%的噪声率下也能取得高性能。</p>
<p>论文自身提到的局限性主要在于实验范围，目前工作集中在Meta-world的模拟机器人任务上，未来需要在更复杂的环境和真实机器人系统中进行验证。</p>
<p>对后续研究的启示：TREND展示了利用模型协作和少量干净先验知识来对抗噪声标签的有效性。其核心思想——通过多个对等模型的动态协作来定义和选择干净样本——可以推广到其他依赖噪声人类或模型反馈的学习范式。此外，如何将这种方法与更高效的主动查询策略结合，或扩展到更高维的观测空间，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对偏好强化学习中人类或VLM标注的偏好反馈存在噪声的问题，提出TREND框架。该方法结合少量专家演示，采用三教学策略：同时训练三个奖励模型，各模型将低损失偏好对视为可靠知识，并相互教学以更新参数。实验表明，仅需1-3个专家演示，在噪声高达40%的机器人操作任务中，成功率仍可达90%，展现了强大的噪声鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.06079" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>