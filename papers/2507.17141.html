<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Human-level Intelligence via Human-like Whole-Body Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Towards Human-level Intelligence via Human-like Whole-Body Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.17141" target="_blank" rel="noreferrer">2507.17141</a></span>
        <span>作者: Zhaohui An Team</span>
        <span>日期: 2025-07-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>构建通用智能机器人是机器人学的基本目标。当前主流方法面临三个核心挑战：1）缺乏具备人类水平物理能力且安全的机器人硬件；2）缺少直观、可扩展的全身遥操作接口用于数据收集；3）现有算法难以从人类示范中学习复杂的全身视觉运动策略。本文针对这些痛点，提出了一个统一的框架<strong>Astribot Suite</strong>，旨在通过模仿人类行为，结合高性能仿人硬件、低成本高效遥操作和简单的模仿学习算法，实现面向日常任务的全身操控。本文核心思路是：通过设计一个仿人的移动双臂机器人平台、一个基于VR的低延迟全身遥操作接口，以及一个在末端执行器空间学习的扩散模型策略（DuoCore-WB），协同解决通用全身操控的硬件、数据和算法难题。</p>
<h2 id="方法详解">方法详解</h2>
<p>Astribot Suite是一个集成了机器人硬件、数据收集接口和学习算法的统一框架。其整体流程为：首先，通过Astribot S1机器人平台（硬件）和基于VR的遥操作接口收集人类执行各种日常任务的示范数据；然后，使用DuoCore-WB算法从这些示范数据中学习生成全身视觉运动策略；最后，策略通过实时轨迹生成模块（RTG）处理后控制机器人执行任务。</p>
<p><img src="https://arxiv.org/html/2507.17141v1/x1.png" alt="各种日常任务展示"></p>
<blockquote>
<p><strong>图1</strong>：Astribot Suite通过全身协调实现的各种日常活动，展示了其广泛的可达范围、类人的灵巧性和敏捷性。图中展示的是DuoCore-WB策略的执行轨迹，每个任务一个策略，使用简单有效的模仿学习算法在通过全身遥操作系统收集的示范数据上训练得到。</p>
</blockquote>
<p><strong>1. 机器人硬件平台 (Astribot S1)</strong><br>Astribot S1是一个高性能移动双臂操作机器人，旨在模仿人类行为。其核心设计包括：</p>
<ul>
<li><strong>自由度配置</strong>：两个7自由度手臂（各带一个平行夹爪）、一个4自由度关节式躯干、一个2自由度头部和一个3自由度全向移动底座（图2a）。</li>
<li><strong>仿生设计</strong>：采用创新的缆绳驱动设计，模拟人体肌肉，实现柔顺运动和精细的力施加，具有高负载能力（单臂5kg）、低反向间隙和惯性，以及更高的操作安全性。</li>
<li><strong>感知系统</strong>：头部集成立体RGB相机和RGB-D相机；两个手腕上装有RGB-D相机用于近距离观察；胸部有一个RGB-D相机用于中距离感知；底盘装有360度激光雷达用于空间感知和建图。</li>
<li><strong>性能参数</strong>：如表1所示，在负载、末端速度/加速度、定位精度等方面达到或超过了普通成年男性的水平，垂直工作空间从地面延伸至2米，水平跨度达1.94米。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.17141v1/x2.png" alt="机器人平台Astribot S1"></p>
<blockquote>
<p><strong>图2</strong>：Astribot Suite的机器人平台Astribot S1。 (a) 展示了系统的自由度构成。(b) 和 (c) 分别描绘了机器人的工作空间运动范围和相机视场分布。</p>
</blockquote>
<p><strong>2. 全身遥操作接口</strong><br>为了高效、低成本地收集高质量示范数据，本文开发了一个基于Meta Quest 3S VR头盔和手柄的遥操作系统。</p>
<ul>
<li><strong>两种控制模式</strong>：<ul>
<li><strong>第一人称视角模式</strong>：操作者佩戴VR头盔，从机器人视角进行控制，适合精细、复杂的远程操作任务。</li>
<li><strong>第三人称视角模式</strong>：操作者将VR头盔佩戴在胸前，站在机器人旁边直接观察其全身状态进行控制，消除了图像传输延迟，适合大范围、高动态或近场数据收集任务。</li>
</ul>
</li>
<li><strong>直观控制映射</strong>（图3）：握持手柄握柄按钮激活动作跟随模式；扳机控制夹爪开合；左摇杆控制移动底座；右摇杆调节肢体垂直位置。</li>
<li><strong>高性能与安全性</strong>：控制系统频率100Hz，端到端响应延迟20ms。具备防倾倒机制、笛卡尔空间自碰撞约束以及基于模型估计的外部力感知与主动柔顺控制，确保安全。</li>
<li><strong>低成本与高质量</strong>：硬件总成本低于300美元。系统能实现近100%的重放成功率，并保持低动作跟踪误差（图4），为策略学习提供了高质量数据。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.17141v1/x3.png" alt="遥操作手柄重定向示意图"></p>
<blockquote>
<p><strong>图3</strong>：用于全身遥操作和示范记录的手柄重定向示意图。展示了手柄按键与机器人控制功能的映射关系。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.17141v1/x4.png" alt="高精度轨迹跟踪"></p>
<blockquote>
<p><strong>图4</strong>：高精度轨迹跟踪。通过遥操作接口记录“收拾玩具”任务的示范数据。图表显示了6个随机选择的关节数据。蓝线代表指令关节位置，红线显示实际位置。虚线绿线表示跟踪误差，始终保持较小，证明了高精度的轨迹跟踪能力。</p>
</blockquote>
<p><strong>3. 全身策略学习算法 (DuoCore-WB)</strong><br>DuoCore-WB是一个基于扩散模型的模仿学习算法，其核心创新在于动作表示和学习空间的选择。</p>
<p><img src="https://arxiv.org/html/2507.17141v1/x5.png" alt="DuoCore-WB模型架构"></p>
<blockquote>
<p><strong>图5</strong>：用于模仿学习的DuoCore-WB模型架构。DuoCore-WB在体现空间内对全身动作进行去噪，并通过直接预测相对于末端执行器坐标系的增量末端执行器位姿，促进有效的全身策略学习。</p>
</blockquote>
<ul>
<li><strong>模型架构</strong>（图5）：<ol>
<li><strong>视觉编码</strong>：使用头、左手、右手三个RGB相机图像（224x224），通过一个四阶段ResNet-Small backbone提取特征，生成49个图像块token，并添加可学习的位置编码。</li>
<li><strong>状态编码</strong>：机器人的状态（底座运动、躯干配置、双臂末端位姿、头部位姿）被编码为一个状态token。</li>
<li><strong>条件融合</strong>：所有图像token和状态token拼接后送入Transformer编码器，提取条件特征。</li>
<li><strong>动作去噪</strong>：采用条件扩散模型。噪声动作序列被映射为查询token，通过一个具有交叉注意力的Transformer解码器与条件特征交互，最终输出去噪后的动作序列。</li>
</ol>
</li>
<li><strong>核心设计选择</strong>：<ol>
<li><strong>RGB视觉感知</strong>：仅使用RGB图像，便于利用预训练视觉编码器和与大规模视觉-语言-动作模型兼容。</li>
<li><strong>末端执行器空间动作表示</strong>：策略学习预测的是每个末端执行器在其自身坐标系下的<strong>增量位姿</strong>（使用SO(3)表示方向），而非关节空间命令。这有效缓解了误差累积问题，并增强了策略对复杂全身活动中常见的大视角变化的鲁棒性。</li>
<li><strong>实时轨迹生成模块</strong>：这是一个轻量级后处理模块，通过二次规划优化，将策略生成的动作块平滑地拼接成连续、准确的执行轨迹，解决了动作块内抖动和块间不连续的问题。</li>
</ol>
</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>平台与数据</strong>：在Astribot S1机器人上，使用前述遥操作接口收集示范数据。</li>
<li><strong>评估任务</strong>：六个具有代表性的真实世界全身任务（图1）：(a) 送饮料、(b) 存猫粮、(c) 扔垃圾、(d) 整理鞋子、(e) 扔玩具、(f) 收拾玩具。每个任务训练一个独立的DuoCore-WB策略。</li>
<li><strong>评估指标</strong>：任务成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>学习到的DuoCore-WB策略表现出色，在六个任务上取得了平均80%的成功率，其中“收拾玩具”任务达到了100%的峰值成功率。</p>
<p><img src="https://arxiv.org/html/2507.17141v1/x6.png" alt="定量评估结果"></p>
<blockquote>
<p><strong>图6</strong>：六个评估任务的定量结果。柱状图显示了每个任务的成功率，虚线表示平均成功率（80%）。结果表明，DuoCore-WB策略在各种全身操控任务上均能有效工作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.17141v1/x7.png" alt="定性结果：送饮料"></p>
<blockquote>
<p><strong>图7</strong>：“送饮料”任务的定性结果序列。展示了机器人抓取饮料、移动到门边、用另一只手开门、进入房间并将饮料放在桌上的连续过程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.17141v1/x8.png" alt="定性结果：存猫粮"></p>
<blockquote>
<p><strong>图8</strong>：“存猫粮”任务的定性结果序列。展示了机器人用双臂抬起约2公斤的猫粮袋，将其放入橱柜，然后关闭柜门的过程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.17141v1/x9.png" alt="定性结果：扔垃圾"></p>
<blockquote>
<p><strong>图9</strong>：“扔垃圾”任务的定性结果序列。展示了机器人移动到垃圾桶、按按钮打开盖子、丢弃纸杯、随后关闭盖子的过程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.17141v1/x10.png" alt="定性结果：整理鞋子"></p>
<blockquote>
<p><strong>图10</strong>：“整理鞋子”任务的定性结果序列。展示了机器人接近鞋子、用双手拾起、移动到鞋架并整齐放置的过程。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文比较了不同的动作表示方法以及是否使用RTG模块的影响。</p>
<p><img src="https://arxiv.org/html/2507.17141v1/x11.png" alt="消融研究"></p>
<blockquote>
<p><strong>图11</strong>：消融研究结果。比较了三种动作表示方法：关节空间、世界坐标系下的末端执行器位姿、以及本文采用的末端执行器坐标系下的增量末端执行器位姿（Ours），并对比了不使用RTG模块（w/o RTG）的情况。结果表明，本文的动作表示方法（Ours）性能最佳，而RTG模块能显著提升所有方法的成功率。</p>
</blockquote>
<ul>
<li><strong>动作表示消融</strong>：对比了关节空间、世界坐标系下的末端执行器位姿和本文方法（末端执行器坐标系下的增量位姿）。结果显示，本文方法在“收拾玩具”任务上的成功率最高（100%），显著优于其他两种表示方法。</li>
<li><strong>RTG模块消融</strong>：在不使用RTG模块的情况下，所有动作表示方法的性能均出现显著下降，证明了RTG模块对于生成平滑、连续轨迹，从而提升任务成功率的必要性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个完整的机器人学习套件 <strong>Astribot Suite</strong>，首次将仿人高性能硬件（Astribot S1）、低成本高效全身遥操作接口和简单有效的模仿学习算法（DuoCore-WB）在统一框架内协同集成，系统性解决了通用全身操控的三大挑战。</li>
<li>设计了一种基于消费级VR设备的低成本、低延迟、高精度的全身遥操作接口，支持双模式灵活切换，为实现大规模、高质量示范数据收集提供了可行方案。</li>
<li>开发了DuoCore-WB算法，其关键在于在末端执行器空间学习增量位姿的动作表示，并结合RTG后处理模块，有效解决了全身控制中的误差累积和动作不连续问题，在仅使用RGB观测的情况下取得了良好性能。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，当前工作为每个任务训练了一个独立的策略，其泛化能力（如对新物体、新环境的适应性）以及如何扩展至多任务、零样本泛化是未来需要探索的方向。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>硬件-软件协同设计的重要性</strong>：本文展示了机器人本体设计（仿生、高自由度、安全）与算法设计（适配的动作表示）深度结合，对于实现复杂智能行为至关重要。</li>
<li><strong>数据收集的平民化路径</strong>：低成本、易用的遥操作接口降低了高质量机器人数据收集的门槛，为后续基于大规模数据驱动的机器人学习（如VLA预训练）奠定了基础。</li>
<li><strong>算法设计的简洁性</strong>：论文强调了不追求模型架构复杂性，而是通过关键的设计选择（如动作表示空间、后处理模块）来提升性能，这为机器人学习算法设计提供了重要思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文致力于构建能完成日常任务的通用智能机器人，核心挑战包括：设计安全的类人硬件、开发直观的全身遥操作数据收集界面、以及从人类演示中学习全身视觉运动策略的算法。为此，作者提出**Astribot Suite**统一框架，集成安全机器人硬件、全身遥操作界面及基于模仿学习（如DuoCore-WB策略）的算法。实验表明，该系统能成功完成递饮料、存猫粮、扔垃圾、整理鞋子等多种需要全身协调、广泛触及和类人灵巧性的日常任务，验证了其在现实场景中实现通用全身操作的可行性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.17141" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>