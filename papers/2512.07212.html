<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.07212" target="_blank" rel="noreferrer">2512.07212</a></span>
        <span>作者: Ye Shi Team</span>
        <span>日期: 2025-12-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人模仿学习中，扩散模型因其能捕捉动作的多模态分布而成为主流生成策略，如Diffusion Policy (DP)、3D Diffusion Policy (DP3)和FlowPolicy。这些方法共享一个核心范式：通过一个由随机或常微分方程（SDE/ODE）定义的前向过程，将动作轨迹扰动为随机噪声，然后训练一个以观察（如视觉、状态）为条件的神经网络来逆转此过程，从随机噪声中迭代采样出可执行的动作。然而，这些方法仅将信息丰富的观察视为去噪网络的高级条件信号，而非将其整合到扩散过程本身的动力学中。这导致采样被迫从无信息的随机噪声开始，削弱了感知与控制之间的耦合，并常常产生次优性能。</p>
<p>本文针对“观察未被整合到扩散随机动力学中”这一具体痛点，提出了将策略学习构建为扩散桥问题的新视角。其核心思路是：通过扩散桥公式，将观察明确地嵌入到扩散SDE轨迹中，使得逆向采样过程可以从一个由观察启发的、信息丰富的先验开始，而非随机噪声，从而生成更精确可靠的控制动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>BridgePolicy的整体目标是从给定的多模态观察（如机器人状态、点云）直接生成动作序列。其核心创新在于采用扩散桥作为生成框架，将观察作为扩散轨迹的终点，从而让动作采样始于观察。</p>
<p><img src="https://arxiv.org/html/2512.07212v2/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：BridgePolicy的整体流程。它通过扩散桥公式将观察嵌入扩散SDE轨迹。观察包括机器人状态和点云。多模态融合模块和语义对齐器解决了多模态分布桥接和数据形状不匹配的挑战。推理时，BridgePolicy从观察表示开始采样，并通过快速采样算法迭代地将其转化为动作。</p>
</blockquote>
<p><strong>整体框架与扩散桥构建</strong>：与标准扩散策略（前向过程终点为高斯噪声）不同，BridgePolicy构建了一个连接动作分布和观察分布的扩散桥。具体地，设定前向过程的初始状态为动作（𝒂₀=𝒂），终端状态为观察（𝒂<em>T=𝒐）。这借鉴了基于随机最优控制（SOC）的统一扩散桥框架，其目标是设计一个最优控制器，以最小成本驱动动力系统从𝒂₀到𝒂_T。通过求解该SOC问题，得到一个具有闭式解的最优受控前向SDE，其形式为：d𝒂_t = (θ_t + g_t² e^{-2θ̄</em>{t:T}}/(γ^{-1}+σ̄²_{t:T}))(𝒂_T - 𝒂_t)dt + g_t d𝒘_t。该方程实现了从动作𝒂到观察表示𝒐的映射。</p>
<p><strong>训练与采样</strong>：为了学习逆转上述桥接过程，模型训练一个数据预测网络𝒂_θ(𝒂_t, 𝒂_T, t)，其目标是直接预测干净动作𝒂₀，损失函数为ℒ_DB = 𝔼[‖𝒂_θ(𝒂_t, 𝒂_T, t) - 𝒂‖]。推理时，采样从给定的观察表示𝒂_T=𝒐开始，利用UniDB++提出的训练免费加速算法，按照一个闭式更新规则迭代地将𝒂_T变换为动作𝒂₀。该更新规则明确了从时间步s到t时，当前状态𝒂_s、终端观察𝒂_T、预测动作𝒂_θ和噪声ϵ的线性组合系数，这些系数均可显式计算。</p>
<p><strong>核心模块：多模态融合与对齐</strong>：直接将上述扩散桥应用于机器人学习面临两大挑战：1) 观察与动作是多模态且异质的，维度不匹配；2) 观察（如点云、状态）与动作空间没有简单的一一映射。</p>
<ol>
<li><strong>编码与多模态融合模块</strong>：首先，使用两个轻量级MLP分别将机器人状态𝒐_s和点云𝒐_pc编码为潜在向量𝒛_s和𝒛_pc。然后，通过交叉注意力机制融合多模态信息：𝒛_obs = softmax(𝒛_pc · 𝒛_s^T / √d_s) 𝒛_s。这里，𝒛_obs被设定为扩散桥的终端状态𝒂_T，其形状与动作块保持一致，从而解决了形状不匹配问题。</li>
<li><strong>语义对齐器</strong>：尽管融合模块统一了形状，但观察表示𝒛_obs与动作𝒂的分布之间仍可能存在语义差距。为确保桥接有效，引入一个基于CLIP损失的对比学习损失ℒ_align来对齐𝒛_obs和𝒂的语义空间，促使正样本对在潜在空间中靠近。最终的整体训练目标为ℒ = ℒ_DB + αℒ_align。</li>
</ol>
<p><strong>理论保证</strong>：论文还提供了定理3.1，证明了由观察编码误差引起的最终生成动作误差是线性有界的（‖𝒂̃₀ - 𝒂₀‖ ≤ C‖𝒂̃_T - 𝒂_T‖），且系数C在实践中很小（10⁻² 到 10⁻³量级），这表明方法对编码误差具有鲁棒性。</p>
<p><strong>创新点总结</strong>：与现有方法（DP, DP3, FlowPolicy）仅将观察作为条件相比，BridgePolicy的创新具体体现在：1) <strong>范式创新</strong>：将策略学习定义为连接观察与动作的扩散桥问题，使采样始于信息先验；2) <strong>技术创新</strong>：设计了多模态融合与语义对齐模块，解决了异质数据桥接的关键难题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在52个模拟任务上进行了评估，涵盖三个基准：MetaWorld（机械臂任务，按难度分为Easy, Medium, Hard, Very Hard）、DexArt（灵巧手操作物体）和Adroit（灵巧手操作）。此外，还在5个真实世界任务（Oven-Closing, Oven-Opening, Pick, Place, Pour, Unplug）上进行了测试。</li>
<li><strong>实验平台</strong>：模拟实验在相应基准环境中进行；真实实验使用机械臂平台。</li>
<li><strong>对比方法</strong>：与当前最先进的生成策略进行了对比，包括DP、DP3、Simple DP3、FlowPolicy以及同期工作VITA。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2512.07212v2/x3.png" alt="模拟实验结果表"></p>
<blockquote>
<p><strong>表1</strong>：主要模拟实验结果。BridgePolicy在三个基准的所有任务组上均取得了最高的平均成功率（74%），全面超越了DP（37%）、DP3（60%）、FlowPolicy（68%）和VITA（64%）等基线方法。尤其在较难的MetaWorld Hard和Very Hard任务上，优势更为明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07212v2/x4.png" alt="真实世界结果表"></p>
<blockquote>
<p><strong>表2</strong>：主要真实世界实验结果。在5个真实机器人任务中，BridgePolicy取得了90%的平均成功率，显著高于DP3（76%）、Simple DP3（66%）和FlowPolicy（56%），展现了其卓越的泛化能力和精度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07212v2/imgs/oven-v2.png" alt="真实任务定性对比"></p>
<blockquote>
<p><strong>图3</strong>：Oven-Opening真实任务的关键路径点可视化对比。与DP3和FlowPolicy相比，BridgePolicy生成的动作轨迹（末端执行器路径）更平滑、更精确地朝向目标（烤箱门把手），展示了其更优的控制精度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07212v2/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融研究结果。移除对齐损失（w/o Align）或融合模块（w/o Fusion）都会导致性能下降，特别是融合模块至关重要。使用L2损失代替L1损失也会损害性能。这验证了多模态融合、语义对齐以及L1损失函数的必要性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验明确了各核心组件的贡献：1) <strong>多模态融合模块</strong>：贡献最大，其缺失会导致性能大幅下降，因为无法解决异质数据桥接问题；2) <strong>语义对齐损失</strong>：对齐观察与动作的语义空间能带来稳定提升；3) <strong>L1损失</strong>：在训练数据预测网络时，使用L1损失比L2损失效果更好。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了BridgePolicy</strong>：一种新颖的生成式视觉运动策略，通过扩散桥公式将观察明确嵌入扩散SDE轨迹，使动作采样能从观察启发的信息先验开始，而非随机噪声，从而实现了更精确可靠的控制。</li>
<li><strong>解决了异质数据桥接的关键挑战</strong>：针对机器人观察与动作的模态、形状不匹配问题，设计了多模态融合模块和语义对齐器，使扩散桥范式能有效应用于机器人策略学习。</li>
<li><strong>进行了广泛且有效的验证</strong>：在多达52个模拟任务和5个真实世界任务上进行了全面实验，结果一致表明BridgePolicy优于现有最先进的生成策略。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，BridgePolicy的性能依赖于观察编码器（MLP）的质量。如果观察数据包含大量噪声或与任务无关的干扰信息，编码误差可能会影响最终策略的性能。此外，方法涉及扩散采样过程，尽管使用了加速算法，但其推理速度可能仍比单步预测的策略要慢。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>更深入的感知-控制耦合</strong>：本研究展示了将感知信息更深层次地整合到生成模型动力学中的潜力，未来工作可探索其他形式的动态嵌入或更紧密的感知-动作联合表示。</li>
<li><strong>处理复杂与噪声观察</strong>：如何使编码和融合模块对高度复杂、模糊或带噪声的观察（如非结构化自然语言指令、混乱场景）更具鲁棒性，是一个值得探索的方向。</li>
<li><strong>效率优化</strong>：进一步研究更快的扩散桥采样器或知识蒸馏技术，以平衡BridgePolicy的性能优势与推理效率，使其更适合实时控制场景。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习中，扩散模型仅将观测作为高级条件而非整合到扩散过程动态的问题，提出**BridgePolicy**。该方法通过**扩散桥**公式将观测嵌入随机微分方程轨迹，使采样从信息丰富的先验而非随机噪声开始。为解决观测与动作维度异构的难题，引入了**多模态融合模块**和**语义对齐器**。在涵盖52个仿真任务和5个真实任务的广泛实验中，该方法性能**始终优于**现有最先进的生成策略。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.07212" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>