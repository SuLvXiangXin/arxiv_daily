<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.16757" target="_blank" rel="noreferrer">2509.16757</a></span>
        <span>作者: Weng, Haoyang, Li, Yitang, Sobanbabu, Nikhil, Wang, Zihan, Luo, Zhengyi, He, Tairan, Ramanan, Deva, Shi, Guanya</span>
        <span>日期: 2025/09/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人学习的主流方法是从人类运动数据中模仿，已成功实现敏捷移动和灵巧操作。然而，将这些成功扩展到全身、接触丰富的人机物交互任务面临两大关键挑战：1）与自由空间移动相比，包含3D人体和物体运动的交互数据稀缺；2）学习全身交互任务为RL训练带来了新挑战，例如在不完美的运动参考下引导期望的接触行为，以及在挑战性姿势下学习与物体保持平衡。现有方法要么依赖任务特定的运动参考生成流程或手动奖励工程，限制了通用性；要么需要高级视觉语言模型或基于模型的规划器。本文提出了HDMI框架，旨在直接从单目RGB视频学习人形机器人全身交互技能。其核心思路是通过一个端到端的RL控制策略，将交互技能学习建模为机器人与物体的联合跟踪问题，从而绕开任务特定的奖励工程。</p>
<h2 id="方法详解">方法详解</h2>
<p>HDMI框架包含三个阶段：1）从无约束的RGB视频中提取并重定向人体和物体轨迹，构建结构化运动数据集；2）训练一个强化学习策略来共同跟踪机器人和物体的状态；3）将训练好的策略零样本部署到真实人形机器人上。</p>
<p><img src="https://arxiv.org/html/2509.16757v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：HDMI方法整体框架。左侧为从人类视频构建结构化参考轨迹的流程，右侧为基于机器人-物体联合跟踪的交互策略训练流程，中间展示了残差动作空间的计算方式。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>统一物体表示</strong>：为使框架能泛化到不同几何和类型的物体，策略在每一时间步接收以机器人根部局部坐标系表达的物体姿态。此外，策略还接收参考接触点<code>p_contact</code>（同样转换到根部坐标系），这些点指明了期望的机器人-物体接触位置，用于引导交互。<br>   <img src="https://arxiv.org/html/2509.16757v3/figures/method/obs/move_suitcase-dot.png" alt="参考接触点示例1"><blockquote>
<p><strong>图2</strong>：不同任务中的参考接触点（黄色点）。策略在训练和部署时均能观察到这些在根部坐标系下的接触点位置。</p>
</blockquote>
</li>
<li><strong>残差动作空间</strong>：为解决从挑战性初始姿势（如下跪）开始训练时探索不稳定和样本效率低的问题，策略不直接学习绝对关节目标<code>θ_target</code>，而是学习一个矫正偏移量<code>a_t</code>。该偏移量被加到参考运动学轨迹的关节位置<code>θ_ref</code>上，即<code>θ_target = θ_ref + a_t</code>。这使初始探索围绕当前参考姿势展开，显著提升了学习复杂动作的收敛速度和稳定性。</li>
<li><strong>统一交互奖励</strong>：为解决视频重定向得到的纯运动学参考轨迹可能缺乏精确接触或包含穿透伪影的问题，论文设计了一个统一的接触促进奖励<code>R_interaction</code>。当参考信号指示意图交互时（<code>c_t=1</code>），该奖励鼓励策略建立并维持稳定接触。对于每个活跃的末端执行器<code>i</code>，奖励结合了鼓励与目标接触点对齐的<strong>位置项</strong>，以及促进稳定但有界的接触力的<strong>力项</strong>。整体交互奖励是所有活跃末端执行器奖励的平均值，并由接触信号控制。</li>
<li><strong>训练与优化</strong>：遵循DeepMimic风格训练，包括从随机帧初始化参考状态、提供相位变量<code>φ</code>作为观察、以及基于跟踪误差的提前终止。策略使用PPO进行优化，奖励函数包括机器人跟踪、物体跟踪与交互以及正则化惩罚（见表I），终止条件见表II。为提高鲁棒性，在训练中还对机器人和物体的惯性和摩擦属性进行了随机化。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，HDMI的创新具体体现在：1）提出了一种统一的、与物体类型无关的观察表示；2）引入了残差动作空间，将探索锚定在参考姿势周围，极大提升了学习远离默认站姿的复杂动作的效率和稳定性；3）设计了统一的交互奖励，使策略能够克服不完美参考轨迹的缺陷，学习建立精确和稳定的接触。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：所有策略均在IsaacSim中使用同一套超参数训练，并直接零样本部署到Unitree G1人形机器人上。通过动作捕捉标记获取机器人和被操纵物体的全局姿态，并转换到机器人根部坐标系作为策略输入。参考运动主要来自RGB视频处理（行李箱操纵任务除外，其参考来自Omomo数据集）。</p>
<p><strong>Baseline对比</strong>：主要通过与移除核心组件的训练变体进行消融实验来验证设计有效性。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>真实世界任务</strong>：HDMI成功部署并执行了多项复杂任务。<br>   <img src="https://arxiv.org/html/2509.16757v3/figures/experiment/real_world/policy_frames_combined.png" alt="真实世界任务演示"><blockquote>
<p><strong>图3</strong>：真实世界挑战性任务演示。(a) 开门与穿越：机器人适应不同的初始位姿和地形变化，成功完成67次连续往返。(b) 箱体移动操作：策略实现了抓握、举起、运输不同形状、大小和重量物体的多样化全身协调。</p>
</blockquote>
</li>
</ol>
<ul>
<li><strong>开门与穿越</strong>：策略完成了67次连续双向开门和穿越（34分钟），在地板木板被移除后仍能成功执行约7次，展现了对环境变化和位置扰动的鲁棒性与适应性。</li>
<li><strong>箱体移动操作</strong>：包括行李箱操纵（7次连续成功运行）、面包箱搬运（2次完整试验）和泡沫垫搬迁（成功），展示了跨越不同物体属性和高度的抓握、移动与操作的无缝全身协调。</li>
<li><strong>复杂长序列任务</strong>：成功连续执行了3次包含爬楼梯、鞠躬、坐下、挥手、跳下和走回的长序列行为，证明了框架处理高度接触丰富、长视野全身行为的能力。<br>   <img src="https://arxiv.org/html/2509.16757v3/x2.png" alt="复杂序列演示"><blockquote>
<p><strong>图4</strong>：复杂长序列任务“Truman‘s Bow”演示，展示了多样化、接触密集行为的连续执行。</p>
</blockquote>
</li>
</ul>
<ol start="2">
<li><strong>仿真消融实验</strong>：<br>   <strong>A. 交互奖励的作用</strong>：<br>   <img src="https://arxiv.org/html/2509.16757v3/x3.png" alt="交互奖励消融成功率"><blockquote>
<p><strong>图5</strong>：在8个任务上的最终成功率。对于大多数任务，移除接触奖励和基于接触的终止条件并不影响最终成功率。</p>
</blockquote>
</li>
</ol>
<ul>
<li>实验比较了包含交互奖励和接触终止、仅移除交互奖励、以及两者都移除的变体。</li>
<li><strong>主要发现</strong>：虽然对多数任务影响不大，但交互奖励在两种关键场景下至关重要：1）<strong>处理不完美的参考</strong>：当参考运动因重定向不完美而未能精确建立抓握时，交互奖励能驱动策略偏离有缺陷的参考以实现正确交互；若使用完美参考训练，则无需该奖励也能成功。2）<strong>引导精确的接触位置</strong>：在“推箱子”等需要精确放置末端执行器于特定边缘的任务中，缺乏交互奖励会导致接触位置不准（如放在垂直面上），造成不稳定交互。<br>   <img src="https://arxiv.org/html/2509.16757v3/x4.png" alt="交互奖励详细分析"><blockquote>
<p><strong>图6</strong>：关于交互奖励和基于接触终止的消融研究分析图。</p>
</blockquote>
</li>
</ul>
<p>   <strong>B. 残差动作空间的作用</strong>：<br>   <img src="https://arxiv.org/html/2509.16757v3/x5.png" alt="残差动作空间消融跟踪误差"></p>
<blockquote>
<p><strong>图7</strong>：在8个任务上的关节和身体跟踪误差。采用残差动作空间的方法始终获得最低的跟踪误差。</p>
</blockquote>
<ul>
<li>实验比较了完整方法、移除残差动作空间（策略输出相对于默认关节位姿）、以及进一步移除身体跟踪误差终止的变体。</li>
<li><strong>主要发现</strong>：完整方法（含残差动作空间）在所有任务中均取得最低的关节和身体跟踪误差。残差动作空间能实现<strong>更快收敛和更高性能</strong>。在没有残差动作空间时，训练收敛慢且无法达到同等性能水平。例如在“移动行李箱”任务中，无残差动作的策略无法学会预期的下跪动作，而是收敛到一种次优策略（保持双脚平贴地面，更多地弯腰）。这是因为残差动作空间将探索锚定在参考运动周围，避免了从挑战性姿势初始化时机器人突然“弹起”导致的失衡和无效训练样本。<br>   <img src="https://arxiv.org/html/2509.16757v3/x6.png" alt="残差动作空间详细分析"><blockquote>
<p><strong>图8</strong>：关于残差动作空间和身体跟踪误差早期终止的消融研究分析图。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1）提出了HDMI，一个直接从RGB视频学习人形机器人交互技能的简单通用框架；2）设计了三个统一组件（统一物体表示、残差动作空间、统一交互奖励）以促进复杂人机物交互的稳定高效训练；3）通过在Unitree G1上的大量sim-to-real实验，证明了该框架在多种接触丰富的移动操作任务中的鲁棒性和通用性，例如实现了67次连续门穿越。</p>
<p><strong>局限性</strong>：1）当前系统依赖地面真值动作捕捉数据（如物体位姿）；2）目前每个技能需要训练一个单独的策略。</p>
<p><strong>启示</strong>：未来的研究方向包括开发直接基于机载传感（如相机）的策略以实现在无仪器环境中的部署，以及利用多技能数据训练能够执行广泛交互的通用模型。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出HDMI框架，解决人形机器人全身交互控制中运动数据稀缺和接触密集的难题。方法包括：从单目RGB视频提取并重定向人体与物体轨迹构建数据集；采用强化学习策略，通过统一物体表示、残差动作空间和通用交互奖励共同跟踪机器人与物体状态；最终零样本部署至真实机器人。实验表明，该方法在Unitree G1人形机器人上实现了67次连续通过门、真实世界6项及仿真中14项运动操作任务，展现了鲁棒性与泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.16757" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>