<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01166" target="_blank" rel="noreferrer">2602.01166</a></span>
        <span>作者: Shanghang Zhang Team</span>
        <span>日期: 2026-02-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，提升视觉-语言-动作模型性能的主流方法之一是引入思维链推理。然而，现有方法面临两个关键局限性：首先，基于文本的CoT方法在推理时需要生成冗长的文本推理轨迹，导致计算开销巨大、内存消耗高、推理延迟严重，无法满足实时机器人控制的需求；其次，无论是文本CoT还是视觉CoT，大多依赖于离散的令牌表示，这与机器人领域中连续变化的感知和动作空间存在表征上的不匹配。本文针对推理效率低下和离散-连续表征不匹配这两个具体痛点，提出了一种新的视角：将多模态思维链推理内化到连续的潜表示中。本文的核心思路是：通过一个基于课程学习的训练范式，逐步将显式的文本和视觉推理转化为连续的潜表示，并使其直接条件化连续动作的生成，从而在推理时无需生成显式CoT，实现高效、面向动作的推理与控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>LaRA-VLA的整体框架是一个三阶段的课程学习流程，其输入为当前视觉观测和语言指令，输出为连续动作。三个阶段分别为：I) 使用对齐的视觉预测潜变量和逆动力学监督进行显式CoT微调；II) 基于课程从显式CoT过渡到紧凑的文本潜变量，逐步减少文本令牌数量并增加对潜推理的依赖；III) 将潜条件化的VLM特征适配到动作专家网络，以实现无需显式CoT的高效动作生成。</p>
<p><img src="https://arxiv.org/html/2602.01166v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：LaRA-VLA整体框架。训练分为三个阶段：(i) 使用对齐的视觉预测潜变量和逆动力学监督进行显式CoT微调；(ii) 基于课程从显式CoT过渡到文本潜变量，潜表示同时受到视觉和动作信号的隐式监督；(iii) 将潜条件化的VLM特征适配到动作专家，实现无需显式CoT的高效动作生成。</p>
</blockquote>
<p>核心模块包括：1) 基于Qwen3-VL的骨干VLM，其图像编码器被直接继承以确保视觉表征的一致性；2) 用于预测未来视觉信息的专用<code>&lt;img_next&gt;</code>令牌；3) 在最终阶段使用的、由16层扩散Transformer构成的动作专家网络。训练过程的核心技术细节如下：</p>
<ul>
<li><strong>第一阶段（显式CoT微调）</strong>：模型通过教师强制学习，使用负对数似然损失（公式1）生成显式的文本CoT序列。同时，引入视觉对齐损失（公式2，L1范数），使模型预测下一时刻的视觉潜表示。为了稳定潜表示学习并防止表征崩溃，目标视觉潜表示由一个在线视觉编码器的指数移动平均版本计算（公式3）。此外，模型还通过逆动力学模型，以前后视觉状态和指令、推理步骤为条件，以自回归方式预测离散动作令牌，使用动作令牌损失进行监督。</li>
<li><strong>第二阶段（课程式替换离散CoT令牌）</strong>：采用与第一阶段相同的训练目标，但通过一个预定义的课程计划，逐步将显式CoT令牌替换为可学习的潜表示。监督中离散CoT令牌的比例逐渐减少至零，最终整个思维链被完全内化到潜空间中。此阶段最终优化的损失函数为<code>0.2 * L_vis + L_act-dis</code>，以促进潜空间推理并保持准确的动作语义。</li>
<li><strong>第三阶段（基于流匹配的动作生成）</strong>：移除显式动作令牌预测，激活专用的动作专家网络。该网络以由VLM产生的多模态潜上下文（聚合了当前视觉、指令、文本推理潜和预测的未来视觉潜）为条件，通过流匹配损失（公式4）预测一个速度场，从而直接生成连续的动作轨迹。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.01166v1/x3.png" alt="注意力机制"></p>
<blockquote>
<p><strong>图3</strong>：LaRA-VLA的注意力机制。该机制根据训练阶段调节跨令牌信息流。在I、II阶段，未来图像令牌因果关注文本和当前图像令牌，动作令牌自回归生成。在III阶段，动作令牌被排除在注意力计算之外。</p>
</blockquote>
<p>与现有方法相比，创新点具体体现在：1) <strong>推理表征连续化</strong>：将文本和视觉CoT统一内化为连续潜表示，避免了离散令牌与连续控制之间的表征不匹配；2) <strong>课程式内化策略</strong>：通过渐进的三阶段训练，将显式推理知识稳定地迁移至潜空间；3) <strong>高效推理架构</strong>：推理时仅需生成紧凑的潜状态，并直接通过轻量级动作专家网络输出动作，大幅降低了延迟。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了LIBERO和SimplerEnv-WidowX这两个模拟基准测试，以及一个在真实机器人上收集的长时程操作任务数据集。构建了LIBERO-LaRA和Bridge-LaRA两个带有结构化CoT标注的数据集用于训练。</p>
<p>对比的基线方法涵盖了不同CoT范式：无CoT的OpenVLA、π0、OpenVLA-OFT；文本CoT的ThinkAct、MolmoAct、π0.5、DeepThinkVLA；视觉CoT的CoT-VLA、DreamVLA、F1、UD-VLA；以及潜CoT的Fast-ThinkAct。在真实实验中，与ACT和GR00T N1.5进行了比较。</p>
<p>关键实验结果：在LIBERO基准上，LaRA-VLA取得了97.9%的平均成功率，在Object和Long任务套件上分别达到99.8%和96.6%，全面超越了所有基线方法。在更具挑战性的SimplerEnv-WidowX基准上，LaRA-VLA以68.8%的平均成功率取得最佳性能。</p>
<p><img src="https://arxiv.org/html/2602.01166v1/x4.png" alt="真实世界设置"></p>
<blockquote>
<p><strong>图4</strong>：四个长时程真实机器人任务的实验设置。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01166v1/x5.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界任务结果。LaRA-VLA在四个长时程操作任务上均优于ACT和GR00T N1.5基线，显示出更强的鲁棒性和长时程协调能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01166v1/x6.png" alt="推理延迟对比"></p>
<blockquote>
<p><strong>图6</strong>：推理延迟对比。与显式文本CoT方法相比，LaRA-VLA在不同任务上实现了高达90%的延迟降低，推理频率可达约10 Hz。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01166v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验（对应正文表4）。该图展示了不同CoT监督形式对性能的影响，表明潜文本CoT和潜视觉CoT的结合带来了最佳效果。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01166v1/x8.png" alt="注意力可视化"></p>
<blockquote>
<p><strong>图8</strong>：注意力可视化。展示了模型在推理过程中，文本潜变量如何关注与任务相关的视觉区域，证明了潜推理的空间基础能力。</p>
</blockquote>
<p>消融实验（表4）总结：仅使用显式文本CoT的成功率为58.33%；仅使用潜文本CoT提升至64.58%；同时结合潜文本CoT和潜视觉CoT（即完整的LaRA-VLA）进一步提升至68.75%。这表明潜推理本身比显式推理更有效，而多模态潜监督（视觉预测）对性能有进一步的积极贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了适用于VLA模型的潜推理范式，将思维链推理内化为跨文本和视觉模态的连续潜表示；2) 设计了LaRA-VLA框架及配套的三阶段课程训练策略，实现了从显式推理到潜推理再到动作生成的平稳过渡；3) 构建了包含丰富多模态推理标注的数据集LIBERO-LaRA和Bridge-LaRA。</p>
<p>论文自身提到的局限性主要隐含在方法复杂性中，例如多阶段课程训练的设计和调优可能具有一定挑战。</p>
<p>对后续研究的启示在于：1) <strong>潜推理的效率优势</strong>：证明了在保持甚至提升性能的前提下，将推理过程压缩到潜空间可以极大降低推理开销，为实时部署VLA模型提供了可行路径。2) <strong>连续表征的统一性</strong>：将感知、推理、控制统一在连续表征空间内，是解决机器人领域离散-连续不匹配问题的有前景的方向。3) <strong>课程学习与知识迁移</strong>：所采用的渐进式内化策略为将大语言模型中的复杂推理能力高效迁移到具身控制领域提供了参考范式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LaRA-VLA框架，旨在解决视觉-语言-动作模型中基于思维链的推理方法存在的推理开销高、离散推理表示与连续感知控制不匹配的核心问题。其关键技术是将多模态思维链推理内化为连续的潜在表示，在潜在空间进行统一推理与预测，并采用渐进式训练范式，从显式监督过渡到潜在推理。实验表明，该框架在仿真与真实机器人任务上性能优于现有方法，且相比显式思维链方法，推理延迟降低高达90%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01166" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>