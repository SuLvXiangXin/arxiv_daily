<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01166" target="_blank" rel="noreferrer">2602.01166</a></span>
        <span>作者: Shanghang Zhang Team</span>
        <span>日期: 2026-02-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，提升视觉-语言-动作（VLA）模型性能的主流方法之一是引入思维链（CoT）推理。现有CoT方法可分为三类：基于文本的CoT（生成离散的自然语言推理步骤）、基于视觉的CoT（预测离散的视觉目标token）以及结合两者的多模态CoT。这些方法面临两个关键局限性：首先，显式生成文本CoT会显著增加推理时的token长度，导致KV缓存占用高、内存消耗大、推理延迟严重（控制频率可能低于5Hz），无法满足实时机器人控制需求。其次，无论是文本token还是基于VQ的视觉token，其推理表示都是离散的，这与机器人感知和动作的连续空间存在表征上的不匹配。</p>
<p>本文针对显式CoT推理效率低下以及与连续控制不匹配的痛点，提出了将多模态CoT推理内化到连续潜在表示中的新视角。核心思路是：设计一个统一的VLA框架，通过课程学习策略，将显式的文本和视觉CoT逐步内化为紧凑的连续潜在状态，从而在推理时无需生成显式CoT，实现高效、面向动作的推理与控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>LaRA-VLA的整体框架是一个三阶段的课程学习训练流程，旨在从显式多模态推理逐步过渡到潜在具身推理，最终适配到动作生成。</p>
<p><img src="https://arxiv.org/html/2602.01166v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：LaRA-VLA概述。训练分为三个阶段：(i) 显式CoT微调：对齐视觉预测潜在表示，并使用逆动力学监督动作；(ii) 课程式过渡：从显式CoT过渡到紧凑的文本潜在表示，逐步减少文本token数量，增加对潜在推理的依赖；(iii) 动作适配：将潜在推理动态直接用于条件化连续动作生成。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>结构化CoT数据构建</strong>：为了提供细粒度监督，论文构建了LIBERO-LaRA和Bridge-LaRA两个数据集。通过一个全自动的标注流程，基于语义锚点（使用Qwen3-VL从初始帧和指令中识别操作对象）和时间锚点（根据夹爪状态变化分割轨迹），生成包含子任务分解、目标物体定位（使用GroundingDINO和SAM3）和运动推理（基于末端执行器轨迹计算方向描述符）的多模态CoT标注。</li>
<li><strong>模型架构</strong>：模型接收当前视觉观察、语言指令和历史动作为输入。视觉编码器（如ViT）和语言编码器（如LLaMA）分别提取特征。一个多模态Transformer作为主干网络，负责融合多模态信息并进行推理与预测。</li>
<li><strong>三阶段训练范式</strong>：<ul>
<li><strong>阶段一：显式CoT微调</strong>：模型被训练同时生成显式的文本CoT序列和预测未来的视觉潜在表示（视觉CoT）。视觉潜在目标由目标网络（一个EMA更新的视觉编码器）从未来帧中提取，并与模型预测的视觉潜在进行对齐（使用均方误差损失）。动作则通过逆动力学损失进行监督。此阶段建立了文本推理、视觉预测和动作之间的初步对齐。</li>
<li><strong>阶段二：潜在推理课程学习</strong>：这是实现推理内化的关键阶段。模型不再生成完整的文本CoT token，而是生成一个固定长度的、连续的“文本潜在”序列。训练采用课程策略，逐步减少显式文本CoT token的数量（从全部到零），同时增加对紧凑文本潜在序列的依赖。视觉预测潜在目标作为隐式监督，引导文本潜在学习到有意义的推理结构。此阶段通过一个“桥接”损失，确保文本潜在能够预测出对应的视觉潜在，从而将文本推理语义注入到连续潜在空间中。</li>
<li><strong>阶段三：动作适配</strong>：在文本CoT完全内化为潜在表示后，模型进行最后微调，专注于将学习到的潜在推理动态用于条件化连续动作的生成。此时，损失函数主要关注动作预测精度。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>潜在推理范式</strong>：首次将潜在CoT推理从纯语言或视觉语言模型扩展到VLA领域，用连续的潜在状态序列替代离散的显式推理token。</li>
<li><strong>课程式训练策略</strong>：设计了三阶段渐进式训练，平滑地将显式多模态监督转化为内部潜在推理能力，避免了直接学习潜在表示的困难。</li>
<li><strong>统一且高效的架构</strong>：推理和预测均在潜在空间完成，推理时只需前向传播计算紧凑的潜在序列，无需自回归生成长文本，极大提升了效率。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.01166v1/x1.png" alt="CoT公式对比"></p>
<blockquote>
<p><strong>图1</strong>：VLA模型中CoT公式的对比。(a) 基于文本CoT的VLA显式生成离散文本推理token。(b) 基于视觉CoT的VLA通过离散视觉目标token表示推理。(c) LaRA-VLA将文本和视觉推理都内化到连续潜在表示中。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真基准</strong>：在LIBERO（10个长视野任务）和Bridge（8个长视野操作任务）数据集上进行评估。</li>
<li><strong>真实机器人任务</strong>：在7个长视野的真实世界操作任务上进行评估。</li>
<li><strong>对比方法</strong>：包括无CoT的VLA基线（如RT-2、Gato），基于文本CoT的方法（如ECoT、GraspVLA、ThinkAct），基于视觉CoT的方法（如CoT-VLA、DreamVLA），以及结合文本和视觉CoT的方法（UP-VLA）。</li>
<li><strong>评估指标</strong>：任务成功率、推理延迟（每秒帧数，FPS）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在LIBERO和Bridge仿真任务上，LaRA-VLA取得了最高的平均成功率（LIBERO: 93.0%， Bridge: 85.0%），显著优于所有基线方法。例如，在LIBERO上比最强的文本CoT方法（ThinkAct）高出6.5%，比最强的视觉CoT方法（CoT-VLA）高出18.7%。</p>
<p><img src="https://arxiv.org/html/2602.01166v1/x4.png" alt="仿真结果"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO和Bridge仿真任务上的成功率对比。LaRA-VLA在所有任务上都达到或接近最高性能。</p>
</blockquote>
<p>在7个真实机器人长视野任务上，LaRA-VLA的平均成功率达到84.3%，大幅超过其他VLA方法（例如，比ECoT高出27.9%），证明了其强大的泛化能力和鲁棒性。</p>
<p><img src="https://arxiv.org/html/2602.01166v1/x5.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人长视野操作任务的成功率。LaRA-VLA表现最佳。</p>
</blockquote>
<p><strong>效率优势</strong>：LaRA-VLA的推理速度显著快于基于显式文本CoT的方法。在A100 GPU上，其控制频率达到约66 FPS，而ECoT仅为6.7 FPS，推理延迟降低了约90%。这主要得益于避免了冗长的文本token生成。</p>
<p><img src="https://arxiv.org/html/2602.01166v1/x6.png" alt="延迟对比"></p>
<blockquote>
<p><strong>图6</strong>：不同方法的推理延迟（FPS）对比。LaRA-VLA的延迟远低于显式文本CoT方法，与无CoT的RT-2相当，但性能更优。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>消融实验验证了各核心组件的贡献。</p>
<ol>
<li><strong>潜在推理的有效性</strong>：移除潜在推理（即仅使用阶段一显式CoT）会导致在Bridge任务上性能下降10.5%，在真实任务上下降9.3%。</li>
<li><strong>课程学习的重要性</strong>：若直接训练潜在推理（跳过课程过渡阶段），在Bridge任务上成功率会下降7.5%。</li>
<li><strong>视觉潜在对齐的作用</strong>：在阶段二移除视觉潜在预测目标，会导致性能显著下降，证明视觉监督对于引导潜在推理学习至关重要。</li>
<li><strong>EMA编码器的作用</strong>：使用EMA更新的目标编码器提取视觉潜在目标，比使用在线编码器能带来约2%的性能提升，并提高训练稳定性。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.01166v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验结果。展示了潜在推理、课程学习、视觉潜在对齐和EMA编码器对模型性能的贡献。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了用于VLA模型的<strong>潜在推理范式</strong>，将思维链推理内化为跨文本和视觉模态的连续潜在表示，实现了与连续感知和控制对齐的高效推理。</li>
<li>提出了<strong>LaRA-VLA模型</strong>及<strong>三阶段课程训练策略</strong>，通过渐进式学习将显式多模态CoT监督转化为内部潜在推理，并利用基于EMA的视觉编码器稳定训练。</li>
<li>构建了包含细粒度多模态推理标注的<strong>结构化CoT数据集</strong>（LIBERO-LaRA和Bridge-LaRA），并在仿真与真实机器人长视野任务上验证了方法的有效性与高效性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前方法依赖于高质量的、包含丰富多模态CoT标注的数据集进行训练。对于缺乏此类标注的现有大规模机器人数据集，如何有效应用该方法是一个挑战。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>效率与性能的平衡</strong>：LaRA-VLA展示了通过改变推理表示形式（从离散到连续）来大幅提升推理效率而不损失（甚至提升）性能的路径，为未来设计实时VLA控制器提供了新思路。</li>
<li><strong>内部世界模型</strong>：将推理内化为潜在状态，可被视为学习了一种紧凑的、与动作生成紧密耦合的内部世界模型动态。这启发研究者进一步探索如何利用这些潜在状态进行更复杂的规划或不确定性估计。</li>
<li><strong>数据标注范式</strong>：提出的自动化多模态CoT标注流程，为构建高质量、可解释的机器人训练数据提供了可扩展的方案，有助于推动社区向更结构化、语义更丰富的监督信号发展。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型在推理时因显式生成思维链（CoT）导致计算开销高、离散推理与连续感知控制不匹配的问题，提出Latent Reasoning VLA（LaRA-VLA）框架。其核心方法是将多模态CoT推理内化为连续潜在表示，在潜在空间统一进行推理与动作预测，从而在推理时无需生成显式CoT。训练采用课程学习范式，逐步从显式文本/视觉CoT监督过渡到潜在推理，并适配动作生成。实验表明，该方法在仿真和真实机器人长程操作任务上性能优于先进VLA方法，且推理延迟比显式CoT方法降低高达90%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01166" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>