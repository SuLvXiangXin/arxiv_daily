<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19080" target="_blank" rel="noreferrer">2509.19080</a></span>
        <span>作者: Dongbin Zhao Team</span>
        <span>日期: 2025-09-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作策略通常通过模仿学习进行初始化，但其性能受限于专家数据的稀缺性和覆盖范围狭窄。强化学习可以优化策略以缓解这一限制，然而在真实机器人上进行训练成本高昂且不安全，而在模拟器中训练则存在难以避免的仿真到现实差距。近期生成模型，特别是扩散模型，在图像和视频生成方面展现出卓越能力，为构建可学习的、高保真的世界模拟器提供了新途径。本文针对如何将基于扩散模型的世界模型与强化学习结合，以增强机器人操作中预训练策略这一具体问题，提出了World4RL框架。其核心思路是：首先在跨任务数据上预训练一个捕捉多样动态的扩散世界模型，随后在冻结的模型内完全通过想象的环境轨迹进行策略优化，从而避免昂贵的在线真实交互。</p>
<h2 id="方法详解">方法详解</h2>
<p>World4RL框架遵循两阶段范式：预训练阶段和策略优化阶段。预训练阶段独立训练三个组件：1）通过模仿学习初始化高斯策略；2）在任务无关数据上训练扩散过渡模型以预测给定历史观察和动作下的未来观察；3）在带有二元成功标签的任务特定数据上训练奖励分类器。策略优化阶段则冻结预训练好的世界模型（包含扩散模型和分类器），将其作为模拟器，使用近端策略优化算法在模型生成的想象轨迹中优化策略。</p>
<p><img src="https://arxiv.org/html/2509.19080v1/fig/framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：World4RL框架总览。左侧为预训练阶段，分别训练策略（模仿学习）、扩散过渡模型和奖励分类器。右侧为策略优化阶段，冻结的世界模型根据当前观察和策略动作生成下一观察和奖励，策略通过PPO在收集的想象轨迹数据上进行优化。</p>
</blockquote>
<p>核心模块包括扩散过渡模型和两热动作编码。扩散过渡模型基于EDM框架，其去噪网络$F_{\theta}$采用U-Net 2D架构，以前$T$帧观察$x^0_{t-T:t}$和对应的编码后动作$z_{t-T:t}$为条件，预测下一观察$x^0_{t+1}$。其训练目标如论文公式(5)所示，旨在最小化去噪网络预测与真实干净目标之间的误差。</p>
<p>针对机器人操作中高维连续动作空间，本文设计了两热动作编码方案。对于每个连续动作维度$a_i$，给定一组分箱值$\mathcal{B}$，将其映射到最近的两个分箱，并通过线性插值计算权重（公式(3)），得到一个权重向量$\mathbf{t}_i$。这种表示既保持了连续性（可微），又引入了轻量级的离散结构，实现了无损重建，作为强化学习智能体与世界模型之间稳健的桥梁。编码后的动作$z$作为条件输入扩散模型。</p>
<p>奖励分类器采用预训练的ResNet18作为视觉主干，输出下一观察为成功状态的概率，提供稀疏的二元奖励信号（成功为1，失败为0），其训练使用二元交叉熵损失。</p>
<p>与现有方法相比，创新点主要体现在：1）将扩散模型作为世界模型主干，相比基于RSSM的模型（如DiWA）能生成更清晰、时间更一致的轨迹，支持有效的端到端强化学习；2）设计了两热动作编码，专门适配机器人操作的复杂交互；3）整个框架支持完全在冻结的世界模型内进行策略优化，不同于IRASim等主要将世界模型用于测试时规划的方法。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Meta-World仿真基准和真实Franka Emika Panda机器人平台上进行。对比的基线方法包括：视频预测模型（NWM、iVideoGPT、DiWA）；策略学习方法：模仿学习（高斯策略、扩散策略DP）、离线强化学习（TD3+BC、IQL）、规划方法（IRASim-ft）以及离线到在线方法（Uni-O4、RLPD）。评估指标包括视频生成质量（FVD、FID、LPIPS）和任务成功率。</p>
<p>在视频预测能力上，World4RL在FVD、FID和LPIPS指标上均显著优于其他基线模型（表I），表明其能更准确地建模机器人动态并保持时空一致性。</p>
<p><img src="https://arxiv.org/html/2509.19080v1/x1.png" alt="预测可视化"></p>
<blockquote>
<p><strong>图2</strong>：在Coffee-Pull-v2任务上的预测轨迹可视化。地面真值是一次失败执行。World4RL准确地建模了此次失败动态，而基线模型（NWM, iVideoGPT, DiWA）错误地生成了成功执行的画面。</p>
</blockquote>
<p>在策略学习性能上，World4RL在六个Meta-World任务上的平均成功率达到67.5%，相比预训练的高斯策略绝对提升了16%，全面优于其他基线方法（表II）。规划方法IRASim-ft虽然在某些任务上表现尚可，但其测试时计算成本比World4RL高出多达40倍。</p>
<p><img src="https://arxiv.org/html/2509.19080v1/fig/average_data_linear_horizontal_expert_rollout_online_scaled10.png" alt="样本效率对比"></p>
<blockquote>
<p><strong>图3</strong>：在线样本效率对比。World4RL仅使用专家和策略 rollout 数据（无需在线交互）即达到高性能，而RLPD和Uni-O4需要分别超过34.6万和47万在线步才能达到相近水平。</p>
</blockquote>
<p>在真实机器人部署中，World4RL在六个操作任务（如图4所示）上，相比模仿学习初始策略，平均成功率绝对提升了25%，验证了其仿真到现实迁移的有效性。</p>
<p><img src="https://arxiv.org/html/2509.19080v1/fig/open_drawer_compressed.png" alt="真实世界任务"></p>
<blockquote>
<p><strong>图4</strong>：真实世界机器人操作任务展示，包括打开抽屉、关闭抽屉、放入面包、取出面包、拿苹果和按按钮。</p>
</blockquote>
<p>消融实验表明，两热动作编码和扩散模型主干都是提升性能的关键组件。移除两热编码或替换扩散主干为其他架构（如DiT）均会导致性能下降。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了World4RL，一个系统地将扩散世界模型集成到机器人操作策略强化学习训练中的框架；2）为提升建模保真度，设计了针对机器人操作的两热动作编码方案并采用扩散模型作为世界模型主干；3）通过大量仿真和真实实验验证了框架在环境建模、策略优化和仿真到现实迁移上的有效性。</p>
<p>论文提及的局限性包括：扩散模型的迭代采样可能带来计算成本，以及框架性能依赖于预训练数据的质量。本文的启示在于：展示了生成式世界模型作为高保真、安全模拟器用于策略优化的潜力，未来研究可探索更高效的世界模型架构、处理更复杂的多模态任务，以及研究如何减少对高质量预训练数据的依赖。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出World4RL框架，解决机器人操作中模仿学习策略因专家数据稀缺而性能受限、强化学习细化面临真实训练成本高和仿真到现实差距的问题。方法采用扩散世界模型作为高保真模拟器，通过预训练捕捉多任务动态，在冻结模型中完全细化策略以避免在线交互，并设计两热动作编码和扩散主干网络提升建模保真度。实验表明，该框架能实现高保真环境建模和一致策略细化，相比模仿学习及其他基线方法，成功率显著提高。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19080" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>