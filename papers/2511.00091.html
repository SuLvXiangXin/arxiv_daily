<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Self-Improving Vision-Language-Action Models with Data Generation via Residual RL - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Self-Improving Vision-Language-Action Models with Data Generation via Residual RL</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.00091" target="_blank" rel="noreferrer">2511.00091</a></span>
        <span>作者: Xiao, Wenli, Lin, Haotian, Peng, Andy, Xue, Haoru, He, Tairan, Xie, Yuqi, Hu, Fengyuan, Wu, Jimmy, Luo, Zhengyi, Fan, Linxi &#34;Jim&#34;, Shi, Guanya, Zhu, Yuke</span>
        <span>日期: 2025/10/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，监督微调已成为大型视觉-语言-动作模型主流的后训练策略。然而，该方法严重依赖昂贵且费力的人工演示数据，限制了模型的扩展性和泛化能力。将这一范式从语言领域迁移到机器人领域面临独特挑战：一方面，高质量机器人演示数据收集成本高昂；另一方面，通过遥操作收集的数据往往与最终部署的VLA策略解耦，导致数据分布无法覆盖策略实际部署时可能遇到的失败状态。</p>
<p>本文针对两个具体痛点：1) 数据收集与部署策略之间的分布不匹配问题；2) 语言条件操纵任务中稀疏奖励信号导致强化学习不稳定且样本效率低下的问题。论文提出了一个新视角：数据收集不应与基础策略无关，而应让数据收集策略与通用模型进行交互，使得探索能够利用通用模型的先验知识，同时确保收集的数据与其轨迹分布对齐。</p>
<p>本文的核心思路是：冻结预训练的VLA主干，通过样本高效的离策略强化学习训练轻量级的残差动作策略作为任务专家，用以探测并接管基础策略的失败区域；随后采用一种混合数据收集方案，生成与基础策略分布对齐且包含恢复行为的数据；最后将这些数据通过监督微调蒸馏回通用模型，实现自我改进。</p>
<h2 id="方法详解">方法详解</h2>
<p>PLD是一个三阶段、即插即用的后训练流程。输入是一个预训练的通用VLA模型和一组语言指定的任务，输出是一个经过改进的、性能更强的通用VLA模型。</p>
<p><img src="https://arxiv.org/html/2511.00091v1/asset/figure/PLD-Framework.png" alt="PLD框架总览"></p>
<blockquote>
<p><strong>图3</strong>：PLD方法整体框架概述。包含三个阶段：1）在线专家获取：冻结VLA通用模型，通过离策略RL训练轻量级残差专家策略；2）自动数据生成：采用混合推演方案，先让基础策略运行若干步，再由专家接管以生成恢复数据；3）监督微调：使用收集的多任务PLD数据对基础模型进行微调；4）零样本部署到多样化的操纵任务。</p>
</blockquote>
<p><strong>第一阶段：在线专家获取</strong>。此阶段冻结VLA骨干网络，为每个任务训练一个轻量级的残差动作策略。具体而言，残差策略网络 π_δ(·|s, a_b) 以状态 s 和基础策略动作 a_b ∼ π_b 为条件，输出一个残差动作 a_δ。最终执行的动作是 ā = a_b + a_δ。为了利用先验并提高样本效率，方法维护离线和在线两个经验回放缓冲区。离线缓冲区首先填充基础策略的成功轨迹。训练时，从两个缓冲区对称采样，确保价值函数在高质量状态-动作对上训练。初始阶段，通过调度器将残差动作幅度限制在[-ξ, ξ]内，以控制探索范围，避免过早偏离基础策略。价值函数 Q^π̄ 通过时序差分学习更新。</p>
<p><strong>第二阶段：自动数据收集</strong>。为了解决纯专家数据分布狭窄、缺乏对失败状态覆盖的问题，论文提出了<strong>基础策略探测</strong>的混合数据收集方案。具体做法是：先让基础策略随机运行 t 步，然后让学习到的残差RL策略接管，生成演示轨迹 τ_demo。这些轨迹包含了专家从潜在次优区域恢复的行为。同时，在训练RL专家时，也使用经过基础策略探测随机步数后的状态作为初始状态分布，以增强专家的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2511.00091v1/x3.png" alt="数据多样性可视化"></p>
<blockquote>
<p><strong>图4</strong>：数据多样性可视化。展示了不同基础策略初始化探测步长下的PLD数据。增加探测步长会产生更长的回合和成功试验间更大的多样性，这种更广泛的数据支持带来了更好的微调性能。</p>
</blockquote>
<p><strong>第三阶段：监督微调</strong>。将第二阶段为多个任务收集的轨迹数据，通过标准的监督微调目标蒸馏回基础VLA模型。此过程与VLA架构无关，支持流匹配和自回归两种动作头。</p>
<p><strong>创新点</strong>：1) <strong>残差策略设计</strong>：冻结大模型参数，仅训练轻量级残差网络，极大降低了RL训练开销，并利用基础策略作为探索先验。2) <strong>分布感知的混合数据收集</strong>：通过“基础策略探测+专家接管”的方式，生成的数据既与基础策略的部署分布对齐，又包含了关键的恢复行为，解决了分布不匹配和专家数据模态单一的问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了LIBERO和SimplerEnv两个模拟基准测试，并在7自由度Franka臂和6自由度YAM双臂上进行了真实机器人验证。对比的基线方法包括：仅使用离线初始化的WSRL、无基础策略引导的RLPD，以及不同数据源（人类数据𝒟^Human、纯RL专家数据𝒟^RL、基础策略自举数据𝒟^Base Policy）的监督微调效果。</p>
<p><strong>RL专家学习效率</strong>：在LIBERO-90的8个任务上，PLD在25万步在线交互内大幅超越基线方法，显示出利用VLA策略先验带来的显著样本效率。</p>
<p><img src="https://arxiv.org/html/2511.00091v1/x4.png" alt="样本高效RL性能基准测试"></p>
<blockquote>
<p><strong>图5</strong>：样本高效RL性能基准测试。PLD在8个LIBERO-90任务上的平均回合性能（滑动窗口100回合内的平均回报）和95%置信区间（3个随机种子），显示出优越的样本效率。</p>
</blockquote>
<p><strong>分布内性能</strong>：在LIBERO各子集（Spatial, Object, Goal）和SimplerEnv任务上，使用PLD数据微调VLA模型（π_0和OpenVLA）均带来一致提升。例如，π_0在LIBERO上的平均成功率从93.4%提升至97.2%，OpenVLA从91.8%提升至99.2%。在SimplerEnv上，平均成功率从71.8%提升至96.6%。</p>
<p><strong>泛化到未见任务</strong>：如图2所示，仅使用10%任务（覆盖度0.1）的PLD数据微调，模型在未见任务上仍能达到24.4%的成功率，而基础策略自举数据则无法泛化。随着任务覆盖度增加，PLD数据在保持分布内高性能的同时，始终维持稳健的零样本泛化能力，其表现与人类数据相当甚至更优。</p>
<p><img src="https://arxiv.org/html/2511.00091v1/x2.png" alt="PLD数据的协同效应"></p>
<blockquote>
<p><strong>图2</strong>：PLD数据的协同效应。在不同任务覆盖比例下微调π_0并评估所有90个任务。PLD数据在获得最高分布内性能的同时，保持了高质量人类数据的跨任务泛化特性。</p>
</blockquote>
<p><strong>少样本和长视野泛化</strong>：如图7所示，在源任务（LIBERO-Goal）上扩大PLD数据规模，能单调提升在目标任务（LIBERO-90）上的少样本微调性能。在长视野任务泛化上（图6），PLD数据微调的效果优于基础策略自举数据，但仍略逊于人类专家数据。</p>
<p><img src="https://arxiv.org/html/2511.00091v1/x6.png" alt="少样本泛化"></p>
<blockquote>
<p><strong>图7</strong>：少样本泛化。扩大分布内（LIBERO-goal）PLD数据规模，能带来在新任务（LIBERO-90）上更好的少样本性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.00091v1/x5.png" alt="短视野到长视野泛化"></p>
<blockquote>
<p><strong>图6</strong>：短视野到长视野泛化。π_0在LIBERO-90上微调，并在LIBERO-10长视野任务上进行单样本评估。</p>
</blockquote>
<p><strong>真实世界性能</strong>：在Franka臂的方块抓取和插桩任务中，经过PLD流程微调的模型在随机化测试中实现了100%成功率（30/30）。相比之下，仅用人类数据或RLPD数据微调的模型在抓取任务中分别只有10/30和16/30的成功率。关键原因如图8所示，PLD数据通过探测基础策略，覆盖了人类或纯RL专家数据未访问到的失败角落状态，从而学会了恢复策略。</p>
<p><img src="https://arxiv.org/html/2511.00091v1/x7.png" alt="真实世界中的失败模式与恢复行为可视化"></p>
<blockquote>
<p><strong>图8</strong>：真实世界中的失败模式与恢复行为可视化。训练在𝒟^RLPD或𝒟^Human上的策略常将方块推入左上角导致卡住，而+𝒟^PLD则能通过重新定位方块进行可靠恢复。</p>
</blockquote>
<p><strong>消融实验贡献</strong>：论文的消融研究表明，<strong>残差策略探测</strong>和<strong>分布感知回放</strong>是收集与部署分布对齐数据的关键，这些数据能有效提升VLA模型在已见和未见任务上的能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一种<strong>自主的后训练流程PLD</strong>，使VLA模型能够不依赖额外的人类示范进行自我改进，在模拟和真实任务中达到近乎饱和的性能（如LIBERO上99%）。2) 对<strong>RL生成数据</strong>如何影响VLA微调进行了系统性研究，明确了分布对齐数据收集的重要性。3) 进行了<strong>全面的实证验证</strong>，包括大规模消融实验和在复杂双手机器人上长达1小时无需人工干预的连续操作演示。</p>
<p>论文自身提到的局限性在于，对于长视野任务的泛化，PLD数据微调的效果仍略低于人类专家数据（图6）。</p>
<p>本工作对后续研究的启示包括：1) <strong>残差学习范式</strong>为高效利用大模型先验进行RL探索提供了有效路径。2) <strong>数据分布对齐</strong>是提升模型泛化能力的关键，单纯追求最优专家数据可能适得其反。3) 该方法为构建<strong>自我改进的机器人基础模型</strong>提供了一个可扩展的路径，即通过RL生成与策略分布对齐的高质量数据，形成数据飞轮。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型依赖昂贵人类演示进行监督微调、限制可扩展性与泛化性的核心问题，提出PLD框架。该方法通过残差强化学习生成数据，包含三个阶段：专家获取阶段用轻量残差演员探测基础策略失败区域；数据收集阶段采用分布感知混合rollout对齐部署分布；微调阶段将轨迹蒸馏回通用模型。实验显示，PLD在LIBERO基准上取得99%任务成功率，在SimplerEnv上性能提升超50%，并在真实Franka臂和YAM臂灵巧操作任务中实现100%成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.00091" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>