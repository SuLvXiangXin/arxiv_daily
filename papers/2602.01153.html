<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01153" target="_blank" rel="noreferrer">2602.01153</a></span>
        <span>作者: Shan Luo Team</span>
        <span>日期: 2026-02-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>力感知对于灵巧的机器人操作至关重要，但触觉传感器的异构性阻碍了力感知策略学习的规模化。主流触觉传感器在传感原理（如光学与磁学）、外形尺寸和材料上存在差异，这通常需要针对特定传感器进行数据收集、校准和模型训练，从而限制了泛化能力。现有方法，如GenForce，虽然实现了跨传感器的力感知迁移，但依赖于昂贵的力标签、针对每个目标传感器的基于扩散的图像翻译和力模型训练，过程繁琐且难以扩展。本文针对“如何学习一个能跨异构触觉传感器通用的力感知表征”这一痛点，提出了一种新视角：利用机器人自身本体提供的物理约束——准静态力平衡，来对齐不同传感器的触觉信号，从而解耦出传感器无关的接触力。本文的核心思路是：通过一个条件变分自编码器框架，联合建模从触觉图像到力的逆动力学和从力到触觉图像的正向动力学，在力平衡和图像重建损失的约束下，学习一个跨传感器的共享潜在力空间。</p>
<h2 id="方法详解">方法详解</h2>
<p>UniForce的整体框架是一个条件变分自编码器，旨在从异构触觉传感器的数据中学习一个共享的潜在力表征。其输入是成对的触觉观测，输出是重建的触觉图像和用于对齐的潜在力。</p>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/f1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：UniForce框架概览。利用力平衡抓取获得的配对数据，UniForce预训练一个通用编码器，从统一的标记图像表征中提取具有物理意义的潜在力，而无需显式的力标签。预训练后的编码器可插入下游任务，通过在单一传感器数据上训练特定任务头，实现对其他传感器的零样本迁移。</p>
</blockquote>
<p>核心模块包括编码器（逆动力学）和解码器（正向动力学）。编码器将触觉观测映射到潜在力空间，解码器则根据传感器特定的参考图像和潜在力重建接触图像。</p>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/f2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：统一的力感知触觉表征学习。在逆动力学阶段，编码器接收配对的参考/接触观测，推断出分块的潜在力图；一个平衡损失对齐左右潜在力，一个KL散度项正则化后验分布。在正向动力学阶段，解码器以传感器特定的参考图像和潜在力为条件重建观测，实现自重建和跨传感器重建。</p>
</blockquote>
<p><strong>具体技术细节</strong>：</p>
<ol>
<li><strong>问题与数据表示</strong>：将异构传感器集合 $\mathcal{S}$ 的原始信号规范化为统一的2D标记图像表示。一个触觉观测定义为 $\mathbf{x}=\langle I_{\text{ref}}, I_{\text{cur}}\rangle$，其中 $I_{\text{ref}}$ 是未变形状态（$t=0$）的参考帧，$I_{\text{cur}}$ 是接触时刻（$t=T$）捕获弹性体变形的接触帧。通过准静态双边抓取，利用物理约束 $F_L \approx F_R$（大小相等，方向相反）作为隐式监督信号。</li>
<li><strong>编码器（逆动力学）</strong>：输入 $\mathbf{x} \in \mathbb{R}^{2\times 3\times H\times W}$ 被分块化为令牌 $\mathbf{V}<em>0 \in \mathbb{R}^{T\times N_p\times D}$（$T=2$）。采用<strong>因果时空Transformer</strong>：空间注意力在每个帧内提取局部标记结构；因果时间注意力沿时间轴对空间对齐的令牌进行操作，使用因果掩码确保接触帧可以关注参考帧，但参考帧不能关注接触帧。编码器输出 $\mathbf{h}</em>{\text{last}}$ 被投影到对角高斯后验分布 $[\boldsymbol{\mu}, \log\boldsymbol{\sigma}^2]$，以表征分块潜在力图 $\mathbf{z} \in \mathbb{R}^{N_p \times 6}$。</li>
<li><strong>解码器（正向动力学）</strong>：以参考图像 $I_{\text{ref}}$ 和采样的潜在力 $\mathbf{z}$ 为条件，重建接触图像 $\hat{I}_{\text{cur}}$。解码器同样基于Transformer架构，通过交叉注意力将潜在力 $\mathbf{z}$ 注入到参考图像的令牌中。</li>
<li><strong>损失函数</strong>：训练目标包括：<ul>
<li><strong>平衡损失</strong> $\mathcal{L}_{\text{eq}}$：最小化左右传感器潜在力后验分布之间的KL散度，强制力对齐。</li>
<li><strong>重建损失</strong> $\mathcal{L}_{\text{rec}}$：包括自重建（用自身参考图像重建自身接触图像）和跨传感器重建（用自身参考图像和配对传感器的潜在力重建配对传感器的接触图像），使用L1和感知损失（VGG）。</li>
<li><strong>KL正则化损失</strong> $\mathcal{L}_{\text{KL}}$：约束后验分布接近标准正态先验。</li>
<li><strong>总损失</strong>：$\mathcal{L} = \mathcal{L}<em>{\text{eq}} + \lambda</em>{\text{rec}}\mathcal{L}<em>{\text{rec}} + \lambda</em>{\text{KL}}\mathcal{L}_{\text{KL}}$。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>物理基础的、无标签的数据收集与对齐</strong>：利用准静态力平衡作为免费且可扩展的监督信号，避免了对外部力/力矩传感器的依赖。</li>
<li><strong>统一的逆-正向动力学建模</strong>：通过联合学习逆动力学（图像-&gt;力）和正向动力学（力-&gt;图像），确保学习到的潜在力空间与物理接触动力学相关联，而不仅仅是传感器特定的外观。</li>
<li><strong>通用编码器与零样本迁移</strong>：学习一个单一的、传感器无关的编码器，可即插即用地用于下游的力估计和力感知策略学习，无需针对新传感器进行重新训练或微调。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：使用包含三种异构触觉传感器（GelSight、TacTip、uSkin）的配对数据收集平台。数据通过双边抓取获得，涵盖多种日常物体。</li>
<li><strong>基准方法</strong>：对比了直接跨传感器迁移的基线（Direct）、基于图像翻译的方法（CycleGAN [7]）、以及最新的跨传感器力感知方法（GenForce [3]）。</li>
<li><strong>下游任务</strong>：<ol>
<li><strong>力估计</strong>：在已知传感器上训练回归头，在其他传感器上零样本测试。</li>
<li><strong>力感知机器人操作</strong>：集成到Vision-Tactile-Language-Action（VTLA）模型中进行机器人擦拭任务。</li>
</ol>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/r_f2.png" alt="力估计结果"></p>
<blockquote>
<p><strong>图3</strong>：跨传感器力估计的定量结果（均方误差，MSE）。UniForce在几乎所有跨传感器迁移方向上（行-&gt;列）都取得了最低的误差，显著优于Direct、CycleGAN和GenForce方法，证明了其学习到的潜在力空间的有效性和泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/r_f3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融研究。移除平衡损失（w/o $\mathcal{L}_{eq}$）或跨传感器重建损失（w/o X-Recon）都会导致性能显著下降，证明了这两个组件对于学习传感器无关表征的关键作用。完整模型（Ours）性能最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/r_f4.png" alt="潜在空间可视化"></p>
<blockquote>
<p><strong>图5</strong>：潜在力空间的t-SNE可视化。经过UniForce对齐后，来自不同传感器（GelSight, TacTip, uSkin）但具有相似接触力模式的样本在潜在空间中聚集在一起，而基线方法（Direct）的样本则按传感器类别分离，表明UniForce成功解耦了传感器特性与力信息。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/r_f5.png" alt="定性力估计"></p>
<blockquote>
<p><strong>图6</strong>：跨传感器力估计的定性结果。UniForce预测的力向量（红色箭头）与真实力（蓝色箭头）最为接近，而其他方法（如Direct）的预测存在明显偏差或尺度错误。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/r_f6.png" alt="VTLA擦拭任务"></p>
<blockquote>
<p><strong>图7</strong>：在VTLA模型中进行机器人擦拭任务的零样本迁移成功率。使用UniForce编码器的VTLA模型，在训练传感器（GelSight）和零样本迁移到的传感器（TacTip, uSkin）上都取得了最高的任务成功率，显著优于使用其他触觉编码器（AnyTouch, UniT）或没有触觉反馈的基线。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>平衡损失 $\mathcal{L}<em>{\text{eq}}$ 和跨传感器重建损失 $\mathcal{L}</em>{\text{rec}}$（尤其是跨传感器重建部分）是模型性能的关键。移除任一组件都会导致跨传感器力估计误差大幅上升，验证了通过逆-正向动力学和力平衡进行对齐的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>UniForce框架</strong>，首次通过学习一个<strong>统一的潜在力模型</strong>，实现了跨光学、磁学等不同原理的异构触觉传感器的表征对齐与零样本迁移。</li>
<li>设计了一种<strong>物理基础、无需力标签的数据收集范式</strong>，利用准静态力平衡作为监督信号，大幅降低了获取跨传感器配对数据的成本和难度。</li>
<li>实证表明，预训练的UniForce编码器可以作为通用模块，即插即用地提升下游<strong>力估计</strong>和<strong>力感知操作策略</strong>（如VTLA模型）的性能，并实现跨传感器的零样本迁移。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，当前方法依赖于准静态抓取假设来获取力平衡数据，这可能限制了其在高度动态交互场景中的直接应用。此外，框架要求将不同传感器的信号规范化为统一的标记图像表示，对于输出非图像格式原始信号的传感器可能需要额外的预处理。</p>
<p><strong>启示</strong>：<br>UniForce展示了利用机器人本体物理约束作为自监督信号来解耦物理属性与传感器特性的强大潜力。这为构建更通用、可扩展的机器人感知模型提供了新思路。未来工作可以探索如何将类似原理扩展到动态场景、更多样化的传感器类型（如听觉、温度），以及更复杂的多模态机器人技能学习中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出UniForce框架，旨在解决机器人操作中因触觉传感器异构性（如光学与磁性原理差异）导致的策略学习泛化性差的问题。其核心技术是学习一个统一的潜在力空间表示，通过联合建模逆动力学与正动力学，并利用力平衡与图像重建损失进行约束。该方法无需依赖昂贵的外部力传感器，通过传感器-物体-传感器直接交互收集数据。实验在GelSight、TacTip等多种传感器上验证了其力估计性能的稳定提升，并实现了在机器人擦拭任务中跨传感器协调的零样本迁移。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01153" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>