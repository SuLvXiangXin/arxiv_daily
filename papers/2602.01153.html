<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01153" target="_blank" rel="noreferrer">2602.01153</a></span>
        <span>作者: Shan Luo Team</span>
        <span>日期: 2026-02-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人灵巧操作依赖于力感知，但触觉传感器的异质性阻碍了力感知策略学习的规模化。主流方法通常针对特定传感器（如GelSight、TacTip）进行数据收集、校准和模型训练，这限制了模型的泛化能力。现有跨传感器迁移方法主要分为两类：一是局限于同族视觉传感器的表征学习（如AnyTouch、T3），其潜在空间未明确与物理力对应；二是通过图像翻译实现跨传感器迁移（如GenForce），但需要针对每个目标传感器进行训练和额外的材料补偿，并非零样本。本文针对传感器异质性（传感原理、形态、材料不同）导致模型无法通用这一痛点，提出利用机器人自身本体提供的物理约束——准静态力平衡——来解耦传感器不变的接触力与传感器特定的触觉信号。本文核心思路是：通过力平衡约束下的逆动力学（图像到力）和前向动力学（力到图像）联合建模，学习一个跨异构触觉传感器的共享潜在力空间，并训练一个通用编码器，以实现下游任务（如力估计、策略学习）的零样本迁移。</p>
<h2 id="方法详解">方法详解</h2>
<p>UniForce是一个条件变分自编码器框架，旨在从异构触觉传感器的配对数据中学习一个共享的、物理接地的潜在力空间。整体流程分为预训练和使用两个阶段：预训练阶段，利用力平衡抓取获得的配对触觉观测数据，训练一个通用编码器，将统一的标记图像映射到潜在力空间；使用阶段，冻结预训练的编码器，将其接入下游任务（如力预测头或VTLA模型），在单个传感器数据上训练任务头后，即可零样本迁移到其他传感器。</p>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/f1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：UniForce整体框架。利用力平衡抓取的配对数据，UniForce预训练一个通用编码器，从统一的标记图像表征中提取物理意义的潜在力，无需显式力标签。预训练后的编码器可通过在单个传感器数据上训练特定任务头，接入下游任务，实现跨传感器力预测和力感知策略学习的零样本迁移。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/f2.png" alt="训练流程"></p>
<blockquote>
<p><strong>图2</strong>：UniForce训练流程。在逆动力学阶段，编码器接收配对的参考/接触观测，推断出分块潜在力图；平衡损失对齐左右潜在力，KL项正则化后验分布。在前向动力学阶段，解码器以传感器特定的参考图像和潜在力为条件，重建接触观测，实现自重建和跨传感器重建。右手指标记图像被镜像以匹配左手指坐标。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>问题建模与统一表征</strong>：将异构传感器集 (\mathcal{S}={s_1, \dots, s_N}) 的原始信号（如RGB图像、多通道信号）规范化为统一的二维二值标记图像，输入定义为 (\mathbf{x}=\langle I_{\text{ref}}, I_{\text{cur}}\rangle)，包含未变形参考帧和接触变形帧。</li>
<li><strong>编码器（逆动力学）</strong>：采用因果时空Transformer。首先将参考帧和接触帧分块为令牌，并注入正弦位置编码。随后进行空间注意力（提取每帧局部标记结构）和因果时间注意力（允许接触帧关注参考帧，强制时序因果性）。最终输出被投影为分块潜在力图 (\mathbf{z} \in \mathbb{R}^{N_p \times 6}) 的对角高斯分布后验参数 ([\boldsymbol{\mu}, \log\boldsymbol{\sigma}^2])。</li>
<li><strong>解码器（前向动力学）</strong>：以参考图像 (I_{\text{ref}}) 和潜在力 (\mathbf{z}) 为条件，重建接触图像 (\hat{I}_{\text{cur}})。通过加性注入将潜在力与参考图像嵌入融合，再经空间Transformer进行像素级重建。</li>
<li><strong>训练目标</strong>：总损失由重建损失、KL散度损失和平衡损失加权构成。<ul>
<li><strong>重建损失</strong> (\mathcal{L}_{recon})：计算自重建和跨传感器重建（共四个分支）的 (\ell_1) 损失和LPIPS感知损失。</li>
<li><strong>KL损失</strong> (\mathcal{L}_{KL})：正则化后验分布接近标准正态分布。</li>
<li><strong>平衡损失</strong> (\mathcal{L}_{eq})：强制左右手指推断的潜在力图在分块级别上一致（(\ell_2) 损失）。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>物理接地的自监督目标</strong>：利用准静态力平衡（(F_L \approx F_R)）作为隐式监督信号，无需昂贵的外部六维力/力矩传感器标定数据。</li>
<li><strong>逆-前向动力学联合建模</strong>：通过编码器学习力推断（逆动力学），通过解码器学习以力为条件的图像重建（前向动力学），共同约束潜在空间与物理力对齐。</li>
<li><strong>真正的零样本跨模态迁移</strong>：与需要针对每个目标传感器进行翻译和训练的GenForce等方法不同，UniForce学习一个通用编码器，在下游任务中实现跨传感原理（视觉与非视觉）的即插即用零样本迁移。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：1) 自行收集的UniForce-pair配对数据集，包含GelSight-TacTip和uSkin-TacTip两种传感器对的抓取数据。2) 公开的GenForce-Hetero数据集，包含GelSight、TacTip、uSkin的单传感器触觉序列及其对应的真实力标签，用于评估。</li>
<li><strong>传感器</strong>：GelSight（视觉，平面，高分辨率RGB）、TacTip（视觉，曲面，内部标记点）、uSkin（磁感，非视觉，4x4阵列多通道信号）。</li>
<li><strong>对比基线</strong>：ResNet（全训练）、AnyTouch、T3、UniT（均为预训练的触觉表征模型），以及作为非零样本参考的GenForce。</li>
<li><strong>评估任务</strong>：潜在空间分析、跨传感器图像重建、零样本力估计、VTLA模型集成。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/r_f6.png" alt="潜在空间分析"></p>
<blockquote>
<p><strong>图4</strong>：潜在维度（z0–z5）与力分量（Fy, Fx, Fz）的皮尔逊相关性热图。结果显示，法向力Fz与潜在维度z5强相关（r=-0.74），表明潜在空间成功编码了物理力信息，尤其是法向力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/r_f2.png" alt="零样本力预测误差"></p>
<blockquote>
<p><strong>图5</strong>：跨六种异构传感器对的平均零样本力预测误差（MAE）。UniForce（橙色）在大多数迁移方向上性能优于或与需要逐传感器训练的GenForce（绿色）相当，显著优于其他零样本基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/r_f3.png" alt="跨传感器重建"></p>
<blockquote>
<p><strong>图6</strong>：跨传感器触觉图像重建示例（GelSight–TacTip和uSkin–TacTip）。使用从任一传感器推断的潜在力，可以驱动自重建和跨传感器重建，重建结果在接触位置和几何形状上与真实接触帧基本一致，验证了前向动力学的有效性。</p>
</blockquote>
<p><strong>表I：在未见物体上的零样本力预测R²分数对比</strong><br>（根据正文描述概括）UniForce在绝大多数传感器对迁移方向（18个指标中的15个）上取得了最佳的零样本性能（R²为正且较高）。特别是在T-&gt;G迁移上，UniForce在Fx, Fy, Fz上分别达到0.77, 0.62, 0.83，显著优于所有其他零样本基线。而其他基线方法（如AnyTouch、T3、UniT）在许多迁移方向上出现了严重的负R²，表明其表征无法泛化到异质传感器。</p>
<p><strong>消融实验贡献</strong>：论文通过对比<code>UniForce（FT）</code>（编码器不冻结，全训练）和<code>UniForce</code>（冻结预训练编码器，仅训练力预测头）的结果表明，预训练获得的通用表征是零样本迁移成功的关键。全训练版本虽然在某些源传感器上表现尚可，但在跨传感器迁移时性能急剧下降（如表I中U-&gt;T的Fz R²从-1.51降至-155.04），说明其过拟合到了源传感器的特定特征。</p>
<p><strong>下游任务集成</strong>：</p>
<p><img src="https://arxiv.org/html/2602.01153v1/fig/r_f4.png" alt="VTLA集成"></p>
<blockquote>
<p><strong>图7</strong>：将UniForce集成到视觉-语言-动作模型中。异构触觉输入通过UniForce编码器被统一为力接地的令牌，用于力感知的机器人操作任务（如白板擦拭）。实验表明，集成UniForce的VTLA模型在混合传感器部署的擦拭任务中，成功率比仅使用视觉的模型提高了13%。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>UniForce框架</strong>，首次通过力平衡约束下的逆-前向动力学学习，为异构触觉传感器（包括视觉与非视觉）建立了一个<strong>共享的潜在力空间</strong>和<strong>通用编码器</strong>。</li>
<li>设计了一种<strong>物理接地、无需力标签的配对数据收集流程</strong>，利用准静态抓取下的力平衡原理，为跨传感器对齐提供了可扩展的监督信号。</li>
<li>实现了下游力感知机器人操作任务的<strong>零样本迁移</strong>，在力估计和VTLA策略学习任务上验证了其有效性，无需针对新传感器进行重新训练或微调。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>数据收集依赖于<strong>准静态假设</strong>，在动态、高速接触场景下的有效性有待验证。</li>
<li>对于<strong>极端变形</strong>或<strong>信号非常稀疏</strong>的传感器（如uSkin），跨传感器重建可能出现不一致。</li>
<li>潜在力空间对某些方向剪切力（如Fy）的编码较弱，这与数据集中该方向激励不足有关。</li>
</ol>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>动态力平衡</strong>：探索在非准静态交互中利用动力学约束进行表征学习。</li>
<li><strong>更丰富的力表征</strong>：结合扭矩或压力分布信息，扩展潜在力空间的维度与物理意义。</li>
<li><strong>大规模多传感器预训练</strong>：将框架扩展到更多类型的触觉传感器，构建真正通用的触觉基础模型。</li>
<li><strong>与机器人本体感知融合</strong>：将触觉潜在力与关节力矩、本体感知等信息结合，构建更全面的机器人身体模型。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出UniForce框架，旨在解决机器人操作中因触觉传感器异构性（如光学、磁性等原理差异）导致的力感知模型难以泛化的问题。其核心方法是通过学习跨传感器的共享潜在力空间，联合建模逆动力学与正动力学，并利用力平衡与图像重建损失约束，从而提取与力相关的统一表征。实验表明，该方法在GelSight、TacTip和uSkin等多种传感器上实现了力估计性能的稳定提升，并支持零样本迁移至下游任务（如机器人擦拭），无需针对新传感器重新训练。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01153" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>