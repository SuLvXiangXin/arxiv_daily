<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Large Video Planner Enables Generalizable Robot Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Large Video Planner Enables Generalizable Robot Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.15840" target="_blank" rel="noreferrer">2512.15840</a></span>
        <span>作者: Yilun Du Team</span>
        <span>日期: 2025-12-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前构建通用机器人基础模型的主流范式是扩展多模态大语言模型，使其输出动作，即构建视觉-语言-动作模型。这类方法依赖于将大规模语言和图像预训练知识迁移到动作模态。然而，与海量的网络文本和图像数据相比，机器人动作数据极为稀缺，这导致VLA模型通常需要在有限的机器人数据上进行微调，形成了一种不对称的知识迁移。这种模式在面对未见过的场景和任务时，泛化能力往往不足。</p>
<p>本文针对VLA模型因数据稀缺导致的泛化瓶颈，提出了一个替代性范式：将大规模视频预训练作为构建机器人基础模型的主要模态。与静态图像和文本不同，视频天然地捕捉了物理世界中状态和动作的时空序列，这与机器人行为具有内在的一致性。网络视频数据丰富，涵盖了人类活动和任务演示，为学习提供了更直接的、基于真实世界动态的迁移基础。</p>
<p>本文的核心思路是：训练一个大规模视频生成模型，使其能够根据单张初始观测图像和文本指令，生成描绘任务完成过程的视频计划，然后将此视觉计划后处理为可执行的机器人动作序列，从而实现零样本的机器人控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法采用两阶段设计：首先是一个大型视频规划器，用于生成视频计划；随后是一个动作提取模块，将视频转换为机器人动作。</p>
<p><img src="https://arxiv.org/html/2512.15840v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：LVP方法整体框架。(a) 基于潜在视频扩散的框架：使用时序因果3D VAE将视频编码为3D潜在表示，然后在此潜在空间中训练扩散变换器。(b) 改进的扩散强迫训练策略：在训练时，随机选择0到6帧作为历史上下文，对历史段和未来段施加独立的噪声水平，并以50%概率将历史段噪声设为零，从而统一支持图像到视频和视频到视频的训练。</p>
</blockquote>
<p><strong>1. 视频基础模型</strong><br>模型基于潜在扩散框架构建。首先，使用时序因果3D VAE将像素空间的视频剪辑压缩为低维潜在表示。该VAE将每个8×8×4的时空块编码为一个16通道的嵌入。编码后，冻结VAE，在压缩的潜在空间中训练一个改进的扩散强迫变换器模型。</p>
<p><strong>核心创新一：扩散强迫训练</strong><br>为了灵活地以初始帧（i2v）或多个历史帧（v2v）为条件生成视频，并增强时序一致性，本文采用了扩散强迫训练策略。传统视频扩散模型对所有令牌添加均匀噪声，而扩散强迫则在训练时对不同帧应用随机、独立的噪声水平。具体而言，给定固定数量的潜在帧，首先从{0,1,2,…,6}中随机采样一个历史长度，将视频分割为历史段和未来段。然后对每个段应用独立的噪声水平。例如，若训练时将历史帧的噪声水平设为零，模型会将其视为完美的上下文帧并学习以其为条件；若设为中间噪声水平，模型则学习处理分布外上下文帧的鲁棒性。在采样时，通过将上下文帧的噪声水平设置为零，即可实现对其的灵活条件生成。此方法无需为可变长度上下文令牌引入额外的交叉注意力层，且与现有DiT模型权重兼容。</p>
<p><strong>核心创新二：历史引导采样</strong><br>为了进一步提高生成视频与上下文图像的 adherence 和运动保真度，LVP采用了历史引导技术，这是分类器无关引导的一个变体。在采样时，结合使用历史引导和基于文本的CFG。最终的采样得分由四项组成：以历史和文本为条件的得分、仅以文本为条件的得分（用于历史引导）、以历史和文本为条件的得分、以及仅以历史为条件的得分（用于文本引导）。通过调节引导权重，可以生成在物理上合理且紧密遵循指令的视频计划。</p>
<p><strong>训练细节</strong><br>训练分为两个阶段：1）<strong>持续预训练</strong>：从Wan I2V 14B权重开始，丢弃处理额外掩码和图像引导通道的权重，在全数据集上训练。2）<strong>低相机运动微调</strong>：为了减少不必要的相机运动，从Ego4D、Epic-Kitchens和Panda数据集中筛选出平均光流幅度较低的剪辑子集进行微调，从而提升时间平滑性和视觉稳定性。</p>
<p><strong>2. LVP-1M 数据集</strong><br>为训练面向具身决策的视频基础模型，本文构建了LVP-1M数据集，包含140万个短剪辑，展示了人类或机器人与物体的交互，每个剪辑配有多条以动作为中心的描述。</p>
<p><img src="https://arxiv.org/html/2512.15840v1/x3.png" alt="数据集与采样"></p>
<blockquote>
<p><strong>图3</strong>：(a) 八个数据源的可视化示例。第一行：四个机器人数据集。第二行：四个以人为中心的数据集。(b) 历史引导采样策略示意图，结合了使用历史和不使用历史的估计得分。</p>
</blockquote>
<p>数据来源多样，包括：1）网络爬取视频，提供规模和多样性；2）以自我为中心的人类活动数据集，提供丰富的人-物交互；3）机器人遥操作数据集，提供机器人形态知识。关键的数据处理步骤包括：<strong>时间对齐</strong>：将所有剪辑对齐到人类执行速度（约3秒），以避免时序不一致。<strong>质量过滤</strong>：过滤低分辨率、光照差、相机运动过快、 embodiment（手或夹具）不可见以及机器人失败轨迹的视频。<strong>以动作为中心的重新标注</strong>：使用大语言模型为视频生成高质量、专注于动作描述的标题，以增强模型的指令遵循能力。</p>
<p><strong>3. 动作提取</strong><br>生成视频计划后，通过3D场景重建（使用MegaSam）、手部重建（使用HaMeR）和运动重定向（使用Dex-Retargeting）来提取可执行的机器人动作，从而适应不同的机器人形态。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>评估分为两部分：1）对视频模型本身进行任务级泛化的独立评估；2）在真实机器人上进行端到端执行的实验。</p>
<p><strong>评估设置</strong></p>
<ul>
<li><strong>数据集</strong>：使用自建的LVP-1M数据集进行训练和评估。</li>
<li><strong>对比基线</strong>：包括文本到视频模型、图像到视频模型以及其他机器人视频生成方法，如Gen2Act、Dreamitate、Cosmos等。</li>
<li><strong>评估指标</strong>：在独立评估中，由第三方测试者从生成视频的物理合理性、指令遵循度等方面进行评分。在真实机器人实验中，报告任务执行成功率。</li>
</ul>
<p><strong>关键实验结果</strong><br><img src="https://arxiv.org/html/2512.15840v1/x5.png" alt="定性对比"></p>
<blockquote>
<p><strong>图5</strong>：与基线模型WAN 2.1的定性对比。LVP生成的结果在物体交互（如抓握杯子）、运动物理合理性（如倒水）和手部姿态细节方面显著优于基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.15840v1/x12.png" alt="消融实验"></p>
<blockquote>
<p><strong>图12</strong>：消融研究。(a) 不同训练策略（标准训练 vs. 扩散强迫训练）下的生成结果对比，扩散强迫训练能产生更合理的动作。(b) 历史引导权重对生成质量的影响，适当的权重能提升动作质量，过高则可能产生伪影。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.15840v1/x13.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图13</strong>：真实机器人执行结果。LVP生成的视频计划被成功重定向到不同的机器人形态（平行夹具、灵巧手）上执行，完成了开门、倒水等任务。</p>
</blockquote>
<p><strong>结果总结</strong></p>
<ul>
<li><strong>独立评估</strong>：在由第三方选择的多样化、挑战性任务中，LVP在物理合理性和指令遵循方面显著优于所有基线模型。</li>
<li><strong>真实机器人实验</strong>：在15个零样本的真实世界任务中，使用LVP提取的动作，机器人取得了68%的整体成功率，优于对比的基线方法（如Gen2Act为47%）。</li>
<li><strong>消融实验</strong>：验证了扩散强迫训练和历史引导技术对提升视频计划质量的关键作用。时间对齐的数据处理策略对于跨形态（从人到机器人）的有效知识迁移至关重要。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>大型视频规划器</strong>（LVP），这是一个为机器人操作设计的大规模视频基础模型，以及一个将其部署为零样本策略的框架。</li>
<li>构建并开源了<strong>LVP-1M数据集</strong>，一个经过精心策划、专注于具身决策和指令遵循的网络规模视频数据集。</li>
<li>设计了严谨的<strong>任务级泛化评估协议</strong>，包括独立的第三方测试和真实机器人实验，系统性地评估了模型在未见过的环境、任务和形态上的泛化能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，尽管LVP在已见物体上表现出色，但在处理涉及<strong>全新物体</strong>（即训练数据中未出现过的物体类别）的任务时，泛化能力仍然是一个挑战。这指向了当前方法在组合泛化方面的限制。</p>
<p><strong>后续启示</strong>：<br>本文的工作展示了以视频为中心作为机器人基础模型主干的可行性和优势。它为后续研究指明了方向：1）进一步探索如何利用更庞大、更多样的视频数据来提升模型的组合泛化和常识推理能力。2）改进视频到动作的提取管道，使其更加高效、鲁棒，并能够处理更复杂的长期任务规划。3）将视频生成模型与更低层的控制策略更紧密地结合，形成端到端的学习系统。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文针对通用机器人决策模型泛化能力有限的问题，提出基于大规模视频预训练的替代范式，以克服视觉-语言-动作模型因动作数据稀缺导致的泛化不足。方法包括策划互联网规模人类活动视频数据集，首次训练基础模型规模的生成式视频规划模型，通过零样本生成视频计划并后处理提取机器人动作。实验通过第三方选定野外任务和真实机器人验证，实现了成功的物理执行，展示了鲁棒的指令跟随、强泛化及现实世界可行性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.15840" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>