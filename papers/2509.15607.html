<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.15607" target="_blank" rel="noreferrer">2509.15607</a></span>
        <span>作者: Wang, Ruiqi, Zhao, Dezhong, Yuan, Ziqin, Shao, Tianyu, Chen, Guohua, Kao, Dominic, Hong, Sungeun, Min, Byung-Cheol</span>
        <span>日期: 2025/09/19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>基于偏好的强化学习（PbRL）是一种无需人工设计奖励函数、通过人类对轨迹对的比较反馈来学习奖励模型的范式。然而，其有效性受到两个关键挑战的限制：对大量人工标注的依赖，以及奖励学习中固有的查询模糊性和信用分配困难。为了减少人工标注，近期工作探索利用基础模型（FM），如大语言模型（LLM）和视觉语言模型（VLM），作为合成反馈源。但现有方法通常依赖单一模态的评估：LLM方法分析轨迹的文本投影（如状态-动作序列），擅长时序推理，但可能对细粒度空间交互产生幻觉；VLM方法分析轨迹的视觉渲染（如关键帧），擅长捕捉空间目标完成度，但可能忽略细微的时间动态。依赖单一模态可能导致反馈不完整或不可靠。此外，即使反馈质量很高，PbRL仍面临内在挑战：1）查询模糊性：早期训练阶段，由随机或弱优化策略生成的轨迹对质量普遍较低且缺乏任务相关变化，难以引发有意义的偏好；2）信用分配：即使获得了可靠的轨迹级偏好，也难以将偏好差异准确归因于特定的状态或动作，导致奖励模型学习偏差。</p>
<p>本文针对上述痛点，提出了PRIMT框架。其核心思路是：不仅将基础模型用作被动的合成反馈提供者，还主动利用其能力来缓解查询模糊性和改进信用分配，具体通过层次化神经符号多模态反馈融合和双向轨迹合成（前瞻生成与后视反事实增强）来实现。</p>
<h2 id="方法详解">方法详解</h2>
<p>PRIMT框架包含两个协同模块：1）多模态反馈融合，提升合成反馈质量；2）双向轨迹合成，缓解早期查询模糊性并改善信用分配。</p>
<p><img src="https://arxiv.org/html/2509.15607v2/x1.png" alt="PRIMT框架总览"></p>
<blockquote>
<p><strong>图1</strong>：PRIMT方法整体框架。左侧为多模态反馈融合模块，对轨迹对进行文本和视觉关键帧预处理，分别通过LLM和VLM进行模态内评估与融合，再通过概率软逻辑（PSL）进行模态间融合，输出最终偏好标签。右侧为双向轨迹合成模块，包括用于初始化轨迹缓冲区的前瞻轨迹生成，以及在训练过程中基于因果反事实推理的后视轨迹增强，并引入因果辅助损失来改进奖励学习。</p>
</blockquote>
<p><strong>1. 多模态反馈融合</strong></p>
<ul>
<li><strong>轨迹预处理</strong>：对于采样的轨迹对 <code>(σ^A, σ^B)</code>，为LLM评估生成结构化文本投影 <code>text(σ)</code>。为VLM评估，提出一种混合关键帧提取方法，而非使用所有帧或仅最终状态图像。该方法结合：i) 近零速度检测（标记子目标完成或过渡暂停）；ii) 平滑残差峰值检测（捕获急剧运动转变）；iii) 变点检测（识别高级任务阶段变化）。取三种方法选定帧的并集，加上轨迹首尾帧，构成最终关键帧集 <code>kvis(σ)</code>。</li>
<li><strong>模态内偏好融合</strong>：分别用链式思维（CoT）提示查询LLM和VLM，要求分析轨迹、输出偏好标签 <code>Υ_M^(k) ∈ {-1, 0, 1}</code>（分别表示A偏好、B偏好、不确定）并分配置信度 <code>C_M^(k) ∈ [0,1]</code>。为减轻方差，采用群体检查机制，对每个模态进行K次随机排序的查询。通过多数投票聚合得到模态特定标签 <code>Υ_M</code>。最终置信度 <code>C_M</code> 是平均置信度 <code>C̄_M</code>（与最终标签一致的预测的平均置信度）和标签一致率 <code>Ċ_M</code>（投票一致的比例）的加权组合：<code>C_M = α·C̄_M + (1-α)·Ċ_M</code>。</li>
<li><strong>模态间偏好融合</strong>：使用概率软逻辑（PSL）集成来自VLM和LLM的模态特定输出。PSL通过加权的一阶逻辑规则对实体（逻辑原子）之间的关系进行建模。本文定义了四条规则来指导融合：<ol>
<li><strong>一致规则</strong>：若VLM和LLM同意同一标签且至少一个模态置信度高，则采用该标签。</li>
<li><strong>冲突解决规则</strong>：当标签冲突时，若视觉可区分性高且VLM置信度高，则优先VLM预测；若时序可区分性高且LLM置信度高，则优先LLM预测。视觉可区分性（VDHigh）通过计算轨迹关键帧CLIP特征之间的Wasserstein距离来衡量；时序可区分性（TDHigh）通过计算两条轨迹的“轨迹波动性”（状态-动作序列二阶差分的平均L2范数）之差的绝对值来衡量。</li>
<li><strong>不确定规则</strong>：若两个模态置信度都低，则输出不确定标签（-1）。<br>PSL将逻辑子句松弛为铰链损失势能，并通过求解带约束的凸优化问题来推断最终的软置信度标签 <code>FinalLabel(Υ)</code>，并施加独热约束以确保输出单一偏好标签。</li>
</ol>
</li>
</ul>
<p><strong>2. 双向轨迹合成</strong></p>
<ul>
<li><strong>前瞻轨迹生成</strong>：在PbRL训练开始前，利用LLM通过三步CoT策略生成多样化的、与任务对齐的轨迹来初始化轨迹缓冲区：i) 根据任务描述生成高级多步动作计划；ii) 将每一步转化为实现具体运动基元的可执行代码片段；iii) 在不同初始条件和策略参数下执行这些程序以合成一组多样化的合理轨迹。这些轨迹被视为引导演示而非最优轨迹，用作后续与探索轨迹配对时的偏好锚点，以增加早期查询的信息量，减少模糊性。</li>
<li><strong>后视轨迹增强与因果辅助损失</strong>：在训练过程中，当多模态反馈模块识别出清晰的偏好（例如 <code>σ* ≻ σ</code>）时，提示LLM基于结构因果模型进行后视推理，为偏好轨迹 <code>σ*</code> 生成反事实变体。过程包括：i) <strong>溯因</strong>：识别导致偏好的关键因果步骤集 <code>T*</code>；ii) <strong>干预</strong>：选择关键步骤 <code>t* ∈ T*</code> 进行最小干预（如引入微小夹爪延迟或末端执行器位置扰动），生成反事实轨迹 <code>σ_cf*</code>，旨在逆转原始偏好；iii) <strong>预测</strong>：将反事实与原始偏好轨迹配对，通过LLM模态内融合验证是否满足 <code>σ* ≻ σ_cf*</code>，仅保留满足条件的反事实用于奖励学习。<br>为更好地利用这些样本来改善信用分配，引入一个因果辅助损失，鼓励模型在编辑步骤处区分奖励，同时在未更改部分保持一致性：<br><code>ℒ_cf^aux = ∑_{t=1}^T H_t · log(1 + exp(r_ψ(s_t^cf) - r_ψ(s_t^*))) + ∑_{t=1}^T (1 - H_t) · || r_ψ(s_t^*) - r_ψ(s_t^cf) ||_2^2</code><br>其中 <code>H_t</code> 是二进制掩码（编辑步骤为1，否则为0）。第一项（因果对比损失）鼓励在因果步骤上对偏好轨迹给予更高奖励；第二项（奖励一致性损失）强制在未编辑区域奖励一致。</li>
</ul>
<p><strong>创新点</strong>：与现有仅使用FM进行被动评估的PbRL方法相比，PRIMT的创新体现在：1）提出了层次化神经符号多模态融合策略，综合LLM的时序推理和VLM的空间感知优势，获得更可靠的合成反馈；2）主动利用FM进行双向轨迹合成，前瞻生成缓解查询模糊性，后视反事实增强结合专门设计的因果辅助损失来针对性改善信用分配。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在2个运动任务（DeepMind Control Suite, DMC）和6个操作任务（MetaWorld 和 ManiSkill）上评估。</li>
<li><strong>对比基线</strong>：<ul>
<li><strong>基于脚本奖励的RL</strong>：PPO + 密集奖励，代表性能上界。</li>
<li><strong>FM作为直接奖励</strong>：包括LLM生成代码奖励（CodeRL）、VLM对比奖励（RoboCLIP）。</li>
<li><strong>FM作为PbRL评估器（单模态）</strong>：RL-VLM-F（仅VLM）、PrefCLM（仅LLM）、RL-SaLLM-F（仅LLM）。</li>
<li><strong>消融变体</strong>：PRIMT (w/o Fusion)（直接拼接多模态输入给VLM）、PRIMT (w/o CF)（无后视反事实增强）、PRIMT (w/o FG)（无前瞻轨迹生成）。</li>
</ul>
</li>
<li><strong>评估指标</strong>：任务成功率（%），报告平均性能和标准差。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.15607v2/x2.png" alt="多任务成功率对比"></p>
<blockquote>
<p><strong>图2</strong>：PRIMT与基线方法在8个任务上的最终成功率对比。PRIMT在所有任务上均优于单模态FM评估基线（RL-VLM-F, PrefCLM）和FM直接奖励方法，且性能接近或有时超过使用真实密集奖励的PPO。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15607v2/x3.png" alt="学习曲线对比"></p>
<blockquote>
<p><strong>图3</strong>：在<code>ManiSkill: PickCube</code>任务上的学习曲线。PRIMT相比单模态基线（RL-VLM-F, PrefCLM）学习速度更快，最终成功率更高，展示了其更高效的样本利用率和更稳定的收敛性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15607v2/x6.png" alt="消融实验分析"></p>
<blockquote>
<p><strong>图4</strong>：PRIMT各组件消融研究。完整PRIMT性能最佳。移除多模态融合（w/o Fusion）导致性能显著下降，尤其在某些任务上不如最佳单模态方法，凸显了融合的必要性。移除反事实增强（w/o CF）或前瞻生成（w/o FG）也会导致性能降低，证明了这两个组件对提升学习效率和最终性能的贡献。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15607v2/x5.png" alt="真实机器人验证"></p>
<blockquote>
<p><strong>图5</strong>：在Kinova Jaco真实机器人上进行的<code>Pick-and-Place</code>任务验证。PRIMT成功地将仿真中学习到的策略迁移到真实世界，机器人能够可靠地抓取和放置蓝色方块。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>性能优势</strong>：PRIMT在全部8个仿真任务上显著且一致地超越了所有FM基线方法。例如，在ManiSkill的PickCube任务上，PRIMT达到约95%的成功率，而RL-VLM-F和PrefCLM分别约为70%和80%。</li>
<li><strong>学习效率</strong>：PRIMT的学习曲线显示其比单模态基线收敛更快，达到更高性能水平所需的交互步数更少。</li>
<li><strong>组件贡献</strong>：消融实验表明，多模态反馈融合、后视反事实增强和前瞻轨迹生成三个核心组件都对最终性能有积极贡献。其中，多模态融合模块的作用最为关键，其缺失会导致性能大幅下降。</li>
<li><strong>现实世界迁移</strong>：在Kinova Jaco机器人上的实物实验成功验证了PRIMT学习到的策略可以零样本迁移到真实环境，完成抓取放置任务。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了PRIMT，一个通用的零样本PbRL框架，将基础模型不仅作为合成反馈源，更作为主动智能体来促进偏好奖励学习。</li>
<li>设计了一种层次化神经符号多模态偏好融合策略，有效结合了LLM和VLM的互补优势，生成了更可靠、全面的合成反馈。</li>
<li>引入了双向轨迹合成机制，包括用于引导早期训练的前瞻轨迹生成，以及基于因果反事实推理的后视轨迹增强与配套的因果辅助损失，分别针对性地缓解了查询模糊性和信用分配两大挑战。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，PRIMT的性能在一定程度上依赖于为LLM/VLM设计的提示工程的质量。此外，多轮查询FM（如群体检查、双向合成）会带来额外的计算开销。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>多模态融合范式</strong>：证明了在机器人评估任务中，有原则地融合不同基础模型模态的互补优势可以显著提升决策的鲁棒性，这可以推广到其他需要复杂评估的序列决策问题中。</li>
<li><strong>主动利用FM超越反馈</strong>：开辟了利用FM主动改善RL学习过程本身（如缓解探索、信用分配问题）的新思路，而不仅仅是替代人工标注。</li>
<li><strong>因果推理与反事实学习</strong>：将因果反事实推理形式化地集成到奖励学习框架中，为改进信用分配提供了一条可解释的途径，未来可探索更复杂的因果模型或与模型基RL的更深度结合。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出PRIMT框架，旨在解决基于偏好的强化学习对大量人工输入的依赖，以及奖励学习中查询模糊性和信用分配的难题。其关键技术包括：采用分层神经符号融合策略，整合大语言模型与视觉语言模型进行多模态评估以生成可靠反馈；结合前瞻轨迹生成与后瞻轨迹增强，前者通过引导样本预热缓冲区以减少早期查询模糊性，后者利用因果辅助损失进行反事实推理以改进信用分配。实验在2个运动与6个操作任务上进行，结果表明PRIMT性能优于基于基础模型和脚本的基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.15607" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>