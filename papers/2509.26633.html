<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.26633" target="_blank" rel="noreferrer">2509.26633</a></span>
        <span>作者: Yang, Lujie, Huang, Xiaoyu, Wu, Zhen, Kanazawa, Angjoo, Abbeel, Pieter, Sferrazza, Carmelo, Liu, C. Karen, Duan, Rocky, Shi, Guanya</span>
        <span>日期: 2025/09/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，教导人形机器人复杂技能的主流范式是将人体运动重定向（Retargeting）为运动学参考，用于训练强化学习（RL）策略。然而，现有的重定向方法面临两大关键挑战。首先，人体与机器人之间存在显著的形态差异（embodiment gap），导致直接重定向容易产生足部滑动（foot-skating）、身体穿透（penetration）等物理上不可行的运动伪影。其次，更重要的是，现有方法通常忽略了人体与物体、环境之间丰富的交互关系，而这些交互对于表达性的移动和移动操作至关重要。它们大多依赖于简单的关键点匹配，无法在重定向过程中保持这些关键的空间和接触关系，从而降低了生成参考运动的质量，进而增加了下游RL策略训练的难度。</p>
<p>本文针对上述痛点，提出了一个名为OmniRetarget的、保持交互关系的数据生成引擎。其核心思路是：通过一个显式建模并保持智能体、地形与被操作物体之间空间及接触关系的“交互网格”，在施加硬性运动学约束的优化框架下，将单个人体演示转化为高质量、物理可行的机器人运动轨迹，并能系统性地进行数据增强。</p>
<h2 id="方法详解">方法详解</h2>
<p>OmniRetarget的整体流程是：输入人体演示（包括人体关节、物体和环境点云），构建交互网格；通过一个约束优化问题，求解每一帧的机器人构型，使得机器人交互网格相对于人体交互网格的拉普拉斯形变能量最小，同时满足一系列硬性运动学约束；输出高质量的机器人运动学轨迹，用于训练RL策略。此外，通过修改优化问题中的目标或环境参数，可以从单次演示生成多样化的增强数据。</p>
<p><img src="https://arxiv.org/html/2509.26633v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：OmniRetarget 概述。通过基于交互网格的约束优化，将人体演示重定向到机器人。每个空间和形状的增强都被视为一个新的优化问题求解，产生多样化的轨迹，这些轨迹作为RL训练的参考，仅需极简的奖励设计和域随机化，即可实现零样本转移到真实世界人形机器人。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>交互网格构建</strong>：交互网格是一个体积结构，其顶点包括用户定义的机器人或人体关键关节，以及从物体和环境中随机采样的点。为了更精确地保持接触关系，物体和环境表面的采样密度高于身体关节。通过Delaunay四面体化构建网格。</li>
</ol>
<p>   <img src="https://arxiv.org/html/2509.26633v2/figures/interaction_mesh.png" alt="交互网格示意图"></p>
<blockquote>
<p><strong>图8</strong>：交互网格示意图。它连接了身体关节（蓝色）、物体点（绿色）和环境点（红色），形成一个体积网格以保持空间关系。</p>
</blockquote>
<ol start="2">
<li><p><strong>约束优化求解</strong>：核心是求解一个逐帧的、带约束的非凸优化问题。目标函数（公式3a）包含两部分：最小化源（人体）与目标（机器人）交互网格之间的拉普拉斯形变能量（公式2），以及一个鼓励时间平滑性的项（<code>||q_t - q_{t-1}||_Q^2</code>）。同时，施加以下硬约束：</p>
<ul>
<li><strong>碰撞避免</strong>（公式3b）：<code>φ_j(q_t) ≥ 0</code>，确保所有碰撞对的符号距离非负。</li>
<li><strong>关节位置与速度限</strong>（公式3c, 3d）：<code>q_min ≤ q_t ≤ q_max</code>, <code>v_min·dt ≤ q_t - q_{t-1} ≤ v_max·dt</code>。</li>
<li><strong>防止足部滑动</strong>（公式3e）：对于被判定为支撑相的脚（源运动中水平速度低于1 cm/s），其位置必须与上一帧相同，<code>p_t^F = p_{t-1}^F</code>。<br>该问题使用定制的序列二次规划（SQP）求解器逐帧求解，并用上一帧的最优解<code>q_{t-1}^⋆</code>进行热启动以保持时间一致性。</li>
</ul>
</li>
<li><p><strong>系统性数据增强</strong>：这是OmniRetarget的关键创新之一。通过修改优化问题中的参数，可以从单次演示生成多样化数据：</p>
<ul>
<li><strong>机器人-物体</strong>：改变物体的初始位姿（平移、旋转，图3b）或形状（缩放三维尺寸，图3c）。为避免优化退化为对整个轨迹进行简单的刚性变换，会添加惩罚项（公式4）和约束（公式5）来锚定机器人的部分身体（如下半身），迫使上半身发现新的协调方式。</li>
<li><strong>机器人-地形</strong>：改变地形特征，如缩放平台高度和深度（图3a）。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.26633v2/x3.png" alt="数据增强示例"></p>
<blockquote>
<p><strong>图3</strong>：OmniRetarget从单个人体演示生成系统性变化：(a) 地形高度，(b) 物体初始位姿，(c) 物体形状。模拟中的优化运动（上图）能一致地转移到硬件上（下图）。</p>
</blockquote>
<p><strong>与现有方法的创新点</strong>：与PHC、GMR等仅有关键点匹配的方法，以及VideoMimic等使用软惩罚的方法相比，OmniRetarget首次在重定向框架中<strong>统一了所有硬性约束</strong>（防穿透、防足滑、关节限），并<strong>通过交互网格显式地保持与环境及物体的交互关系</strong>。此外，其<strong>系统性的数据增强能力</strong>能够从单次演示低成本生成大规模、多样化的高质量数据，这是先前方法所不具备的。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：使用了OMOMO（机器人-物体交互）、内部动作捕捉数据（机器人-地形交互）和LAFAN1（仅机器人运动）数据集。</li>
<li><strong>基准方法</strong>：对比了PHC、GMR和VideoMimic三种广泛使用的开源重定向基线。</li>
<li><strong>机器人平台</strong>：主要在Unitree G1人形机器人上进行评估，也展示了在Unitree H1、Booster T1等不同构型机器人上的通用性（图2）。</li>
<li><strong>RL训练</strong>：使用基于OmniRetarget生成的高质量参考轨迹，采用极简的奖励设计（仅5项：身体跟踪、物体跟踪、动作变化率、软关节限、自碰撞），配合简单的域随机化，训练本体感知（proprioceptive）策略。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.26633v2/x2.png" alt="跨构型示例"></p>
<blockquote>
<p><strong>图2</strong>：跨构型的机器人-物体-地形交互。展示了方法对不同机器人构型的适应性。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>运动学质量定量评估</strong>：如表II所示，OmniRetarget在防止<strong>穿透</strong>（最大深度~1.34 cm，远低于基线的5-8 cm）和<strong>足部滑动</strong>（持续时间和最大速度均为0）方面显著优于所有基线。在<strong>接触保持</strong>指标（持续时间越接近1越好）上也表现优异。</p>
</li>
<li><p><strong>下游RL策略性能</strong>：使用重定向数据训练RL策略后，OmniRetarget在机器人-物体和机器人-地形交互任务上的<strong>成功率</strong>（分别达82.20%和94.73%）显著高于基线方法。这证明了高质量参考运动对简化RL训练、提升策略性能的直接益处。</p>
</li>
<li><p><strong>复杂行为展示与零样本Sim-to-Real</strong>：</p>
<ul>
<li><strong>多样化技能</strong>：在真实G1机器人上成功实现了自然的搬箱、攀爬0.9米高平台（机器人身高的70%）、斜坡爬行等行为（图4）。</li>
<li><strong>长时程复杂序列</strong>：实现了长达30秒的复杂多阶段任务，包括搬椅子上平台、踩椅子攀爬、跳跃、翻滚缓冲落地等（图1旗舰演示）。</li>
<li><strong>高动态运动</strong>：实现了峰值角速度达15 rad/s的蹬墙后空翻（图5），在真实实验中达到5/5的成功率。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.26633v2/x4.png" alt="硬件结果展示"></p>
<blockquote>
<p><strong>图4</strong>：额外的硬件结果，展示了多样化、敏捷且类人的行为。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.26633v2/x5.png" alt="高动态运动"></p>
<blockquote>
<p><strong>图5</strong>：硬件结果展示了一个高动态的蹬墙后空翻运动。机器人最大线速度达到3.5 m/s，峰值角速度达到15 rad/s。</p>
</blockquote>
<ol start="4">
<li><strong>数据增强的有效性</strong>：使用增强数据训练的策略，在评估增强场景时取得了79.1%的成功率，与仅在原始场景上评估的82.2%成功率相当，表明增强显著扩大了数据覆盖范围且未导致性能显著下降。相比之下，仅依靠域随机化（在训练中扰动物体）的策略表现不佳。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.26633v2/x6.png" alt="基线方法产生的伪影"></p>
<blockquote>
<p><strong>图6</strong>：基线重定向方法产生的伪影。对比了OmniRetarget与PHC、GMR和VideoMimic在防止足部滑动和身体穿透方面的效果。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个<strong>保持交互关系</strong>的人形机器人重定向框架OmniRetarget，它通过交互网格显式建模并保持机器人-物体-地形间的空间接触关系，同时统一了防穿透、防足滑、关节限等<strong>硬性物理约束</strong>。</li>
<li>设计了一个<strong>系统性的数据增强流水线</strong>，能够从单次人体演示自动生成大规模、多样化的高质量运动学轨迹，覆盖不同的物体配置、形状、机器人构型和环境。</li>
<li>基于生成的高质量数据，配合<strong>极简的奖励设计</strong>（仅5项）和简单的域随机化，成功实现了多种复杂全身移动操作技能的<strong>零样本从仿真到现实的转移</strong>，并开源了大规模重定向数据集。</li>
</ol>
<p><strong>局限性</strong>：论文提到，对于高动态运动（如后空翻），由于机器人足部是刚性的，与人类足部结构不同，需要调整训练条件（如放宽终止阈值、移除足部方向跟踪奖励）来给予RL更多学习自由度。这表明方法对于形态差异极大或动态性极高的运动，可能仍需一定的适应性调整。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>高质量数据的重要性</strong>：本研究强有力地证明了，提供高质量、物理可行且保持交互关系的参考运动，可以极大简化下游RL的奖励工程，使训练更高效、策略更鲁棒。</li>
<li><strong>数据生成范式的转变</strong>：从为每个变体收集独立演示，转向基于优化/模拟的自动数据增强，是突破人形机器人数据瓶颈的一个有前景的方向。</li>
<li><strong>交互保持的普适性</strong>：交互网格的概念不仅适用于人形机器人，也可能推广到其他需要与环境和物体进行复杂物理交互的机器人形态。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出OmniRetarget，以解决仿人机器人运动技能学习中，因人体与机器人形态差异导致现有运动重定向方法产生物理失真（如脚滑、穿透）且忽略关键人-物/环境交互的问题。该方法基于交互网格，通过最小化人-机器人网格间的拉普拉斯变形并施加运动学约束，生成运动学可行的轨迹，同时显式保持与地形、物体的空间接触关系，支持跨 embodiment、地形和物体的数据增强。实验表明，其生成超过8小时的运动轨迹，在运动学约束满足和接触保持上优于基线；利用所生成数据训练的RL策略，仅用5个奖励项即可使Unitree G1仿人机器人成功执行长达30秒的跑酷和运动操作任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.26633" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>