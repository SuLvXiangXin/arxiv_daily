<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.13731" target="_blank" rel="noreferrer">2509.13731</a></span>
        <span>作者: Changjoo Nam Team</span>
        <span>日期: 2025-09-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在制造业中，将柔性扁平电缆（FFC）插入连接器是一项关键但极具挑战性的任务，需要亚毫米级精度，同时处理易变形的电缆。当前主流方法严重依赖人工示教机械臂生成轨迹，这种方法劳动密集、耗时且易出错。虽然强化学习（RL）为无需显式建模柔性体复杂动力学而自动化该任务提供了可能，但柔性FFC带来的非确定性需要大量训练时间和努力。此外，直接在真实环境中训练是危险的，因为工业机器人运动速度快且缺乏安全措施，可能损坏设备和周围环境。</p>
<p>本文针对上述痛点，提出了一种新的视角：通过构建“视觉近似任务”在仿真中安全训练RL智能体，并利用基础模型实现“真实到仿真”的转换，从而将训练好的策略零样本迁移到真实世界。核心思路是，在仿真中仅保留任务相关的几何和空间信息（如形状、位置），去除颜色、纹理等无关视觉细节进行训练；在部署时，利用分割基础模型将真实场景转化为同样的视觉近似表示，使智能体能够直接应用仿真中学到的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体目标是在仿真中训练RL策略，并实现零样本迁移到真实机器人完成FFC插入任务。其核心是通过基础模型将真实场景转换为与仿真训练环境一致的“视觉近似”表示，从而弥合仿真与真实之间的差距。</p>
<p><img src="https://arxiv.org/html/2509.13731v1/figures/frameworkflow_redesigned.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：提出的方法整体架构。预定义的提示词输入给视觉语言模型（VLM），VLM生成用于提示分割基础模型SAM2的点，从而得到场景的分割掩码，作为RL智能体的观测。</p>
</blockquote>
<p>整体流程分为离线训练和在线部署两个阶段。训练阶段完全在仿真中进行，输入是视觉近似任务图像（分割掩码）和本体感觉信息，输出是机械臂的6维位姿动作。部署阶段，真实相机图像通过VLM自动生成提示点，再由SAM2分割出FFC和连接器的掩码，这些掩码与机械臂姿态共同构成状态输入给训练好的策略网络，直接输出动作控制真实机器人。</p>
<p>核心模块包括：</p>
<ol>
<li><p><strong>视觉近似任务构建</strong>：这是方法的基础。在仿真中（使用PyBullet），FFC被建模为一串细小的六面体链，连接器简化为具有纯色的几何体，去除了所有纹理、阴影等无关细节，仅保留形状和空间关系。在真实世界中，则通过SAM2生成FFC和连接器的分割掩码来实现同样的视觉简化。这极大地缩小了仿真与真实之间的视觉差距。<br><img src="https://arxiv.org/html/2509.13731v1/figures/approximated_simulation2.png" alt="视觉近似示例"></p>
<blockquote>
<p><strong>图3</strong>：视觉近似任务示例。(a)真实环境原始图像，(b)仿真中的视觉近似表示（仅保留任务相关信息），(c)通过SAM2生成的真实环境分割掩码。</p>
</blockquote>
</li>
<li><p><strong>RL算法与训练</strong>：采用基于图像的稀疏奖励RL框架。算法以CoDER为基础，并引入了DrQ的数据增强思想来改进Q函数正则化，提升样本效率。状态$s_t$由两个相机的分割掩码$M_t^{(1)}, M_t^{(2)}$和机械臂末端姿态$\mathbf{q}_t$构成。动作$a_t$是末端执行器在位置和旋转上的6维连续增量。奖励函数是稀疏的：成功插入时奖励为10，否则为0。训练时应用了域随机化，随机化相机外参（位置、朝向、视野）以及FFC和连接器的尺寸，以提升策略对不同视角和物体尺寸的泛化能力。</p>
</li>
<li><p><strong>自动提示生成</strong>：为了实现从真实图像到分割掩码的全自动流程，需要为SAM2自动生成提示点。该方法利用VLM（GPT-4o）解析高级任务描述，将输入图像划分为网格，并通过多路径蒙特卡洛推理（变化温度参数）让VLM多次输出可能包含目标物体的网格编号。重叠次数多的网格单元格被认为置信度更高，通过重要性采样从这些单元格中选取点作为SAM2的提示。<br><img src="https://arxiv.org/html/2509.13731v1/figures/flow_autoprompting.png" alt="自动提示流程"></p>
<blockquote>
<p><strong>图4</strong>：为SAM2自动生成点提示的流程。VLM根据任务描述和图像网格选择相关区域，重叠的红色和绿色单元格分别表示连接器和FFC的高置信度区域，通过重要性采样获取提示点。</p>
</blockquote>
</li>
<li><p><strong>提示迁移</strong>：为了平衡分割精度和实时性，先使用大型SAM2模型获得高质量掩码，然后以此作为提示输入给一个轻量级SAM2模型，使小模型能复现大模型的分割质量，满足机器人实时操作的低延迟要求。</p>
</li>
</ol>
<p>与现有方法相比，本文的创新点具体体现在：1) 提出了“视觉近似任务”的概念，通过分割掩码统一仿真与真实的视觉表征，聚焦几何信息，避免了构建高保真仿真的成本和过拟合风险；2) 利用VLM和SAM2的组合，实现了从原始图像到任务相关掩码的全自动、无需人工干预的流程，增强了方法的通用性；3) 结合稀疏奖励RL和域随机化，实现了在简化仿真中高效训练，并具备零样本迁移到复杂真实环境的能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台包括在PyBullet中的仿真训练和真实世界部署。真实测试使用Nachi MZ-07六自由度工业机械臂，配备两个OBSBOT Meet 2 4K相机（近垂直和倾斜角度）。测试使用了不同尺寸（宽/窄）和颜色（蓝色/棕色）的FFC及对应连接器，以评估泛化能力。蓝色和棕色FFC在颜色、表面图案和物理刚度上均不同，连接器也包含商业型号的保护盖等未在仿真中建模的特征。</p>
<p><strong>基线对比与关键结果</strong>：论文将提出的完整方法（Ours (Full)）与多个变体进行对比：仅使用原始RGB图像（Ours (RGB)）、使用人工标注掩码（Ours (GT Mask)）、以及使用不同提示生成方法（Ours (w/ VLM) 和 Ours (w/o VLM，即手动点击提示））。关键性能指标是插入成功率和平均插入时间。<br><img src="https://arxiv.org/html/2509.13731v1/figures/exp_cap.png" alt="实验结果"></p>
<blockquote>
<p><strong>图13</strong>：不同方法在真实世界FFC插入任务上的性能对比。完整方法（Ours (Full)）在所有测试案例中均达到100%成功率，且平均插入时间约30秒，显著优于使用RGB图像的方法（成功率0%）。</p>
</blockquote>
<p>实验结果表明，完整方法在所有的FFC和连接器组合上均实现了<strong>100%的成功率</strong>，平均插入时间约为30秒。而使用原始RGB图像进行仿真训练的方法（Ours (RGB)）在真实世界中完全失败（0%成功率），证明了视觉近似对于跨域泛化的至关重要性。使用真实掩码（GT Mask）或手动提示的方法也能达到100%成功率，但完整方法实现了全自动化。</p>
<p><strong>消融实验与分析</strong>：</p>
<ol>
<li><strong>自动提示生成评估</strong>：在均匀照明和高对比度照明两种条件下，评估VLM自动生成提示点所得分割掩码的质量（以人工提示结果为基准计算mIoU）。<br><img src="https://arxiv.org/html/2509.13731v1/figures/low_contrast.png" alt="照明条件对比"><br><img src="https://arxiv.org/html/2509.13731v1/figures/high_contrast.png" alt="照明条件对比"><blockquote>
<p><strong>图15 &amp; 图16</strong>：评估自动提示生成鲁棒性的照明设置：(a)均匀照明，(b)高对比度照明。在高对比度挑战性条件下，自动提示方法对宽蓝色FFC的分割mIoU仍达0.78。</p>
</blockquote>
</li>
</ol>
<p>结果显示，在更具挑战性的高对比度照明下，对于宽蓝色FFC，自动提示方法仍能达到0.78的mIoU，说明其具备一定的鲁棒性。<br>2.  <strong>视角泛化测试</strong>：测试训练时未见过的新相机视角下的策略性能。<br><img src="https://arxiv.org/html/2509.13731v1/figures/view1.png" alt="视角对比"><br><img src="https://arxiv.org/html/2509.13731v1/figures/view2.png" alt="视角对比"></p>
<blockquote>
<p><strong>图17 &amp; 图18</strong>：用于测试视角泛化的两个不同相机视角。训练中通过域随机化覆盖了类似的视角变化，使得策略能够成功处理这些新视角。</p>
</blockquote>
<p>得益于训练时的相机域随机化，策略能够成功处理这些新视角，进一步验证了方法的泛化能力。<br>3.  <strong>定性结果</strong>：展示了在真实环境中，策略成功插入宽和窄的棕色FFC的序列图像。<br><img src="https://arxiv.org/html/2509.13731v1/figures/exp_brown_wide.png" alt="定性结果1"><br><img src="https://arxiv.org/html/2509.13731v1/figures/exp_brown_narrow.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图19 &amp; 图20</strong>：真实世界部署的定性结果。策略成功地将宽和窄的棕色FFC插入对应的连接器，展示了其对不同尺寸物体的处理能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了<strong>视觉近似任务</strong>的训练范式，通过分割掩码提取并统一仿真与真实环境中的任务核心几何信息，实现了高效、安全的仿真训练与零样本迁移；2) 设计了基于<strong>VLM和SAM2的自动提示生成管道</strong>，实现了从原始图像到任务掩码的全流程自动化，无需人工标注或干预；3) 在真实的工业机械臂上<strong>成功部署</strong>了该框架，并验证了其对不同尺寸、颜色、刚度和视角的泛化能力，为高精度柔性物体装配提供了实用解决方案。</p>
<p>论文自身提到的局限性包括：1) 策略性能依赖于分割掩码的质量，在极端视觉条件（如严重遮挡、极端光照）下分割失败可能导致任务失败；2) 使用大型基础模型（VLM、SAM2）会带来一定的计算成本，尽管通过提示迁移进行了优化。</p>
<p>本文工作对后续研究的启示在于：1) <strong>基础模型与机器人学习的结合</strong>：展示了利用大规模预训练基础模型处理感知不确定性、实现通用表征的潜力，可推广至其他需高精度感知的操作任务。2) <strong>视觉简化策略</strong>：验证了对于某些具身任务，过度追求视觉逼真度并非必要，提取并聚焦于任务核心的、不变的特征表示是一种有效的泛化途径。3) <strong>安全与高效的训练范式</strong>：“全仿真训练+真实到仿真转换”为零样本迁移提供了一种安全且成本可控的框架，特别适用于工业等高风险、高成本场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对工业场景中柔性扁平电缆（FFC）插入任务精度要求高（亚毫米级）、传统人工示教方法低效的问题，提出了一种基于强化学习的自动化解决方案。核心方法是采用基于基础模型（SAM2 和 VLM）的“真实-仿真”框架，在仿真环境中进行安全训练，并通过语义分割自动提取电缆和插槽的关键视觉特征以实现仿真到真实的迁移。实验表明，该方法具备零样本部署能力，无需在真实环境中进行微调即可直接应用，为工业插装任务提供了一种通用、可扩展的自动化途径。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.13731" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>