<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.20818" target="_blank" rel="noreferrer">2510.20818</a></span>
        <span>作者: Castro, Mateo Guaman, Rajagopal, Sidharth, Gorbatov, Daniel, Schmittle, Matt, Baijal, Rohan, Zhang, Octi, Scalise, Rosario, Talia, Sidharth, Romig, Emma, de Melo, Celso, Boots, Byron, Gupta, Abhishek</span>
        <span>日期: 2025/10/23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人导航领域正经历从手工设计的模块化系统向基于学习的方法转变，尤其是利用大规模数据的机器人基础模型。然而，随着数据集规模和异构性的增长，一个关键挑战随之出现：下游机器人可能不具备数据集中记录的所有行为所需的物理能力。例如，四足机器人上下楼梯的数据对轮式机器人无用。这种异构性造成瓶颈，使得无法简单合并所有可用数据来获得可靠的导航性能。</p>
<p>本文旨在解决如何有效利用大规模、异构的运动能力数据集，来学习通用的、跨具身的、可操控的导航策略。核心思路是将导航解耦：高层启发式策略（如到达目标、避开大障碍物）在不同具身之间具有通用性，而低层可通行性则严格依赖于机器人的物理能力。因此，本文提出了VAMOS，一个分层的视觉-语言-动作模型，通过一个通用规划器学习语义，一个专用功能模型学习物理约束，二者通过预测的2D路径接口连接。</p>
<h2 id="方法详解">方法详解</h2>
<p>VAMOS的整体框架包含一个高层通用视觉语言模型规划器和一个低层、针对特定具身的功能模型。高层规划器接收单目RGB图像和以文本编码的目标坐标（可附加自然语言偏好），输出一组在像素空间中的候选2D路径。这些路径被解码并从2D像素空间投影到3D地面。最后，一个能力感知的功能函数评估并重新排序这些3D候选路径，基于底层策略的能力决定机器人应在真实世界中执行哪条路径。</p>
<p><img src="https://arxiv.org/html/2510.20818v1/media/vamos_fig2_narrow.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：VAMOS方法整体框架。高层VLM规划器以图像和文本编码的目标坐标为输入，输出一组像素空间中的候选路径。这些路径被投影到3D地面，并由能力感知的功能模型进行评估和重排序，以选择最终执行的路径。</p>
</blockquote>
<p><strong>核心模块1：高层VLM规划器</strong><br>该模块被参数化为一个VLM，其任务是轨迹预测。具体而言，规划器 $P_{\phi}(\tau|I,g_{l})$ 从图像 $I$ 和文本目标 $g_l$ 预测一个像素空间中的粗略2D路径 $\tau$。路径定义为一系列点 $\tau:(x,y)_t$，表示机器人未来时间步在图像平面上的位置。这种参数化具有多重优势：便于利用不同数据源进行通用训练；点级预测有助于VLM保留其预训练泛化能力。该规划器在包含29.8小时、来自3种不同具身的4个导航数据集的混合数据上进行微调，基础模型为PaliGemma 2 3B，采用LoRA进行微调。为了增强可操控性，10%的数据通过GPT-5-mini进行了注释增强，生成了带有描述和排名的多条路径用于训练。</p>
<p><strong>核心模块2：低层功能条件调制</strong><br>高层VLM的预测由一个低层的、能力感知的功能函数 $F_{\pi}$ 进行调制，以确保只有可执行的行为在硬件上运行。$F_{\pi}$ 将局部高程图 $M$、查询点位置 $(x, y)$ 和航向角 $a$ 映射到策略 $\pi$ 能够实际穿越该点的概率 $[0, 1]$。该功能函数完全在模拟中通过监督学习训练：在程序生成的大量地形上执行底层策略的轨迹，收集成功/失败数据点 $\mathcal{D}$，然后训练一个MLP来最小化二元交叉熵损失。在部署时，对VLM生成的每条候选路径 $\tau^w_i$，计算其路径上所有点的功能得分，并取最小值作为该路径的累积功能得分 $F^c(p^w_i)$。最终路径选择可以贪婪地选择得分最高的路径，或通过softmax采样引入随机性。</p>
<p><strong>创新点</strong><br>与现有方法相比，VAMOS的核心创新在于：1）<strong>解耦设计</strong>：明确分离通用语义规划（高层VLM）和具身特定物理约束（低层功能模型），解决了异构数据训练和跨具身迁移的难题。2）<strong>路径作为接口</strong>：使用连续的2D路径作为高低层之间的接口，相比离散的文本动作命令（如NaVILA）或直接的速度命令，支持更精确的长距离空间推理，且无需先验演示或地图，并为功能调制提供了自然的接口。3）<strong>双向交互</strong>：高层提供路径，低层通过功能评估反馈并调制高层预测，形成了双向影响，这对鲁棒性能至关重要。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了两个真实机器人平台：四足机器人Boston Dynamics Spot和轮式机器人UW Hound。低层功能模型在Isaac Lab模拟器中训练，使用了通过强化学习训练的策略作为Spot底层策略的代理，并利用波函数坍缩和细胞自动机生成了多样化的地形。评估在六个未见过的真实世界场景进行：室内（Hallway, Atrium, Lab）和室外（Campus, Forest, Down Ramp），测试了短到中距离导航能力。</p>
<p><strong>对比基线</strong>：1) <strong>Modular Stack</strong>：类似[7]的几何模型化模块导航栈。2) <strong>ViPlanner</strong>[15]：学习的几何和语义规划器。3) <strong>NoMaD</strong>[3]：导航基础模型。4) <strong>NaVILA</strong>[21]：导航VLA模型。</p>
<p><img src="https://arxiv.org/html/2510.20818v1/media/scand_3.jpg" alt="测试场景"></p>
<blockquote>
<p><strong>图3</strong>：真实世界导航实验的六个室内外场景，包含具有挑战性的地形、光照和植被。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>如表I所示，VAMOS在所有课程中取得了最高的平均成功率（90%），全面优于基线。在室内，VAMOS与模块化栈和ViPlanner表现相当，但在更具挑战性的“Lab”场景中超越了所有基线。在室外，模块化栈和通用基线（NoMaD, NaVILA）表现不佳，而VAMOS和ViPlanner表现良好。然而，在需要长期规划的“Lab”和“Down Ramp”场景，ViPlanner失败，凸显了VAMOS在丰富几何和语义推理上的优势。</p>
<p><img src="https://arxiv.org/html/2510.20818v1/media/campus_top_down.jpg" alt="长距离导航路径"></p>
<blockquote>
<p><strong>图20</strong>：不同方法从起点（红）到目标点（绿）经过航点（黄）的俯视地图路径。VAMOS实现了长距离、精确的导航。右侧显示了到达一个航点后重新规划时预测和选择的路径。</p>
</blockquote>
<p><strong>跨具身导航实验</strong>：<br>在一个包含并排楼梯和斜坡的测试环境中，VAMOS使用相同的高层规划器，仅为Spot和Hound机器人替换具身特定的功能模块。如表II和图21所示，功能调制使轮式机器人Hound的整体成功率从60%提升至90%，并确保四足机器人Spot保持100%的成功率。功能模型能根据机器人能力正确选择路径（Hound选斜坡，Spot两者皆可）。如表III所示，VAMOS（90%）在跨具身任务上显著优于ViPlanner（0%）。</p>
<p><img src="https://arxiv.org/html/2510.20818v1/media/ramps_vs_stairs_standardized.jpg" alt="跨具身实验"></p>
<blockquote>
<p><strong>图21</strong>：在楼梯与斜坡并存的场景中评估VAMOS的跨具身能力。功能重排序根据机器人能力（轮式Hound选斜坡，四足Spot两者皆可）选择路径，提升了成功率。</p>
</blockquote>
<p><strong>可操控性展示</strong>：<br>如图22所示，通过在目标坐标后附加自然语言偏好（如“走铺好的路”、“走草地”），可以直观地操控VAMOS的路径选择，展示了其灵活性和人机交互的便利性。</p>
<p><img src="https://arxiv.org/html/2510.20818v1/media/vamos_steerability.jpg" alt="可操控性"></p>
<blockquote>
<p><strong>图22</strong>：VAMOS可通过附加到目标坐标规范的自然语言偏好进行操控。不同偏好由图中所示的自然语言提示指示，并用不同颜色描绘。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文通过消融实验验证了核心设计选择。关键结论包括：1) <strong>高层通用规划器的必要性</strong>：在单一数据集上训练的机器人专用导航器，其性能不及在混合数据集上训练的通用VLM规划器。2) <strong>低层功能调制的关键作用</strong>：对于单机器人导航，引入功能模型进行调制，能够拒绝物理上不可行的计划，将Spot机器人的导航成功率提升了3倍。</p>
<p><img src="https://arxiv.org/html/2510.20818v1/media/avg_mean_l2_error_comparison_helvetica-neue-light.jpg" alt="消融实验：规划器性能对比"></p>
<blockquote>
<p><strong>图23</strong>：消融实验显示，在混合数据集上训练的通用VLM规划器（VAMOS-Generalist）相比在单个数据集上训练的专用规划器（VAMOS-Specialist），在多个指标上（如图示平均L2误差）表现更优，验证了异构数据训练带来正向迁移。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了VAMOS，一个新颖的分层VLA框架，成功解耦了导航中的通用语义规划和具身特定物理接地。</li>
<li>设计了以2D路径作为接口，实现了高低层模块的有效交互，支持长距离推理、无需先验地图，并便于功能调制。</li>
<li>实现了跨具身导航（轮式与四足）和自然语言可操控性，并在真实世界复杂环境中取得了最先进的性能。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1) 功能模型依赖于局部高程图，其质量会影响性能。2) 模拟到真实的差距，尽管通过程序化生成多样化地形进行了缓解。</p>
<p><strong>后续启示</strong>：<br>VAMOS的工作表明，在机器人基础模型中引入恰当的结构化层次和接口设计，是融合大规模异构数据与特定物理约束的有效途径。这种“通用规划+专用接地”的范式可推广到其他需要结合通用知识与具体物理实现的机器人任务中。未来研究可探索更复杂的接口表示，或将功能模型扩展到更动态、交互性的场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VAMOS模型，旨在解决机器人导航中策略难以同时泛化至多样环境并适配特定机器人物理约束（如轮式与足式机器人能力差异）的核心问题。关键技术为分层视觉-语言-动作架构：高层规划器从开放世界数据学习语义路径规划，低层可供性模型在仿真中学习机器人具体能力，并通过图像空间候选路径评估与重排序实现解耦。实验表明，该方法在真实室内外环境中优于现有模型，支持跨足式与轮式机器人的导航，并通过拒绝不可行路径将单机器人成功率提升3倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.20818" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>