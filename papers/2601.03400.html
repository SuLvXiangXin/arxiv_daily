<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.03400" target="_blank" rel="noreferrer">2601.03400</a></span>
        <span>作者: Najar, Ali, Mirrokni, Alireza, Izadyari, Arshia, Mohammadian, Sadegh, Sharifizade, Amir Homayoon, Meskin, Asal, Bagherian, Mobin, Asgari, Ehsaneddin</span>
        <span>日期: 2026/01/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）在标准视觉语言基准（如VQA和图像描述）上表现出色，但这些任务通常依赖于显性的、可直接查询的证据，使得模型可能通过表面识别、数据集捷径或检索式匹配取得成功，而非真正的深层推理。现有的一些谜题式评估（如字谜、视觉双关）虽然更具挑战性，但往往使用视觉上干净、线索明确的构图或大量文本，使得解决过程很大程度上依赖于OCR和浅层启发式方法，且评估协议通常不允许模型进行假设修订。</p>
<p>本文针对现有基准在评估非字面视觉语言推理、多步假设形成与修订以及多语言/跨语言泛化方面的不足，提出了一个新的视角。核心思路是构建一个名为EYE-Q的多语言基准，其包含线索隐含、视觉密集且充满干扰项的图像谜题，要求模型通过发现隐含线索、生成和修订假设，将感知证据映射到非字面的目标词或短语，从而系统评估模型在开放式的图像到短语推理中的复杂理解能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>EYE-Q是一个用于评估开放式图像到短语推理能力的多语言基准数据集，其核心是精心设计的视觉字谜。整体上，每个评估实例的输入是一张图像和一段简短的文本提示（说明游戏规则），输出是模型生成的目标词或短语。</p>
<p><img src="https://i.imgur.com/5q6tZ0h.png" alt="实验变体概览"></p>
<blockquote>
<p><strong>图2</strong>：EYE-Q实验变体概览。提示通过结合共享的基础模板、游戏解释和子集特定的语言规则来实例化，并可选择性地加入提示策略模块（如少量示例、迭代优化、部分字符揭示）。</p>
</blockquote>
<p>基准包含1,343个谜题，分为四个子集：英语（300）、波斯语（671）、阿拉伯语（50）和跨语言（波斯语-英语，322）。谜题设计遵循核心原则：<strong>线索隐含</strong>与<strong>概念密集</strong>。图像场景包含多个物体、属性和关系，其中既有作为语义线索的元素，也有干扰项。线索可能涉及物体方向、数量、颜色、空间关系（包含、重叠、倒置）等，解决过程很少能简化为读取可见文本（抗OCR）。求解通常需要三个步骤：(i) <strong>线索发现</strong>：区分信息性元素与干扰项；(ii) <strong>关系抽象</strong>：对关系和转换进行推理；(iii) <strong>语言关联</strong>：将推断出的概念映射到目标语言中的习语、双关语或常规短语。跨语言谜题则需要桥接英语视觉线索和波斯语答案。</p>
<p>数据来源于两部分：波斯语和跨语言谜题来自手机游戏《Aftabe》，经过人工筛选以确保答案唯一性；英语和阿拉伯语谜题则由作者设计目标短语和场景描述，并使用文本生成图像模型渲染，再经人工审查以确保视觉内容支持预定答案。</p>
<p>评估协议的设计支持<strong>假设形成与修订</strong>。在共享的基础提示模板上，定义了四种实验变体（如上图2所示）：</p>
<ol>
<li><strong>基础设置</strong>：附加一个拼写提示（指示目标答案的字符长度），要求输出最终答案。</li>
<li><strong>少量示例思维链</strong>：在提示前添加三个已解决的示例（图像、正确答案及解题描述），以演示推理过程。</li>
<li><strong>迭代优化</strong>：若首次答案错误，则附加上次猜测和修订指令重新查询，最多允许两次修订（共三次尝试）。</li>
<li><strong>部分字符揭示</strong>：随机揭示目标答案中25%的非空格字符（其余用下划线掩码），要求模型补全完整答案。</li>
</ol>
<p>所有变体均保持输入图像和核心任务不变，仅改变提示策略或提供轻量级辅助，旨在模拟人类解题时的辅助手段，并将线索发现失败与输出脆弱性分离开来。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在EYE-Q基准的四个语言子集上进行，评估了包括GPT-5.2、Gemini 2.5 Flash/Pro、Grok 4.1 Fast、Llama 4 Scout (17B) 和 Qwen 3 VL (235B) 在内的六种前沿大型VLM。采用开放式生成和精确匹配准确率进行评估。</p>
<p><img src="https://i.imgur.com/8WmN5Qj.png" alt="各模型在不同提示变体和语言子集上的准确率"></p>
<blockquote>
<p><strong>图3</strong>：六种大型VLM在四种提示变体和四种语言子集上的准确率(%)。结果显示，即使在辅助下，模型在所有子集上表现均远未饱和，尤其在阿拉伯语和跨语言子集上困难显著。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ul>
<li><strong>整体性能低下</strong>：所有模型和变体下的准确率都较低，表明EYE-Q极具挑战性。各子集在所有模型和变体下的最高准确率为：英语60.27%（Grok 4.1 Fast，部分字符揭示）、波斯语43.03%（Gemini 2.5 Pro，部分字符揭示）、阿拉伯语19.15%（Gemini 2.5 Pro，少量示例CoT）、跨语言29.15%（Gemini 2.5 Pro，部分字符揭示）。专有模型整体优于开源模型。</li>
<li><strong>辅助效果有限</strong>：轻量级辅助能提升性能，但未能弥补差距。平均而言，迭代优化将准确率从基础设置的11.53%提升至15.63%，部分字符揭示进一步提升至18.11%。然而，阿拉伯语平均准确率在部分字符揭示下仍低于10%，跨语言约为12%，说明主要失败原因在于上游的线索发现和抽象解释，而非表面形式的微小差异。</li>
<li><strong>模型规模效应</strong>：在Qwen3-VL家族内的控制实验显示，模型规模增大能提升性能，增益在支持修订或约束输出空间的变体下最为明显。但即使最大模型（235B），性能也远未饱和，表明单纯扩大参数不足以克服EYE-Q的核心挑战。</li>
</ul>
<p><img src="https://i.imgur.com/5q6tZ0h.png" alt="Qwen3-VL模型规模对英语子集准确率的影响"></p>
<blockquote>
<p><strong>图4</strong>：Qwen3-VL家族（8B, 32B, 235B）在四种提示变体下，于英语子集的准确率随模型规模的变化。规模增大带来增益，但性能仍未饱和。</p>
</blockquote>
<ul>
<li><strong>语言间性能耦合</strong>：模型在不同语言子集上的表现高度相关，尤其是波斯语与跨语言性能耦合最紧（皮尔逊相关系数0.98），表明擅长波斯语谜题的模型也擅长需要桥接波斯语和英语的跨语言谜题。阿拉伯语与其他子集的耦合相对较弱（相关系数约0.88），凸显其独特挑战性。</li>
</ul>
<p><img src="https://i.imgur.com/5q6tZ0h.png" alt="模型在不同语言子集上准确率的相关系数矩阵"></p>
<blockquote>
<p><strong>图5</strong>：基础设置下，模型在各语言子集间准确率的皮尔逊（左）和斯皮尔曼（右）相关系数矩阵。颜色越亮表示相关性越强。</p>
</blockquote>
<ul>
<li><strong>错误分析：非语义近似错误</strong>：对迭代优化下失败案例的语义相似度分析显示，模型错误答案与标准答案的余弦相似度密度集中在低值区域（均值在0.244至0.396之间）。</li>
</ul>
<p><img src="https://i.imgur.com/5q6tZ0h.png" alt="迭代优化下失败答案与标准答案的语义相似度分布"></p>
<blockquote>
<p><strong>图6</strong>：迭代优化变体下，仅针对失败案例，模型最终答案与标准答案之间余弦相似度的密度估计。分布集中于低相似度值，表明错误通常并非语义上的近似答案。</p>
</blockquote>
<p>这证实了低准确率主要不是由严格的字符串匹配惩罚“近似答案”导致的，而是源于模型未能从复杂场景中识别正确线索并形成正确的抽象解释。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>提出了视觉字谜求解这一新的评估任务</strong>，强调非字面推理、多步假设修订和多语言泛化；2) <strong>构建了EYE-Q基准</strong>，包含1,343个线索隐含、视觉密集的多语言谜题；3) <strong>设计了支持假设修订的人类对齐评估协议</strong>，并通过实验揭示了当前前沿VLMs在此类复杂推理上存在显著不足。</p>
<p>论文自身指出了几点局限性：1) 在极少数存在多种解释的情况下，基准优先考虑预定的标准答案以维持评估严谨性；2) 数据生成依赖目标语言流利的研究者，难以轻松扩展至更多语言或方言；3) 可靠的人工评估成本高，且需要母语者，可能引入评判差异。</p>
<p>这项工作对后续研究具有重要启示：它为推动VLMs超越字面接地、发展真正的抽象推理和跨语言概念映射能力提供了一个具有挑战性的测试平台。未来的工作可以探索更强大的内部假设生成与搜索机制，设计更好的多模态提示以引导线索发现，并将此类评估扩展到更广泛的文化和语言语境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前视觉语言模型(VLMs)在需要深层推理的任务上表现不足的问题，提出了一个名为EYE-Q的多语言基准测试。该基准包含1,343个视觉字谜，其特点是线索隐含、非结构化，并要求模型进行假设生成与修正、选择性注意和关联推理，以评估其复杂的图像到短语推理能力。评测覆盖英语、波斯语、阿拉伯语及跨语言场景。实验结果表明，现有先进模型在该基准上存在显著性能差距，尤其在抽象和跨语言谜题上，最高准确率仅为60.27%，凸显了当前模型在构建和搜索概念表示方面的局限性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.03400" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>