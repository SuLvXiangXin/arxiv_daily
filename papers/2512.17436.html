<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Xiaomi MiMo-VL-Miloco Technical Report - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Xiaomi MiMo-VL-Miloco Technical Report</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.17436" target="_blank" rel="noreferrer">2512.17436</a></span>
        <span>作者: Li, Jiaze, Chen, Jingyang, Qu, Yuxun, Xu, Shijie, Lin, Zhenru, Zhu, Junyou, Xu, Boshen, Tan, Wenhui, Fu, Pei, Ju, Jianzhong, Luo, Zhenbo, Luan, Jian</span>
        <span>日期: 2025/12/19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）已成为现代AI系统的核心基础设施，能够感知视觉场景并进行跨模态推理。在智能家居领域，这些能力尤其关键，系统需要理解用户行为、解析细粒度视觉线索，并做出实时、情境感知的决策。然而，隐私和延迟约束要求模型能够在边缘设备本地运行，而非完全依赖云端推理。现有通用视觉语言模型在特定家居场景（如日常活动识别、手势理解）上的表现可能不够专业，且面向特定领域微调时，常面临与通用能力“灾难性遗忘”的权衡问题。本文针对智能家居场景，旨在开发一个既精通家居场景理解，又保持强大通用多模态推理能力的紧凑型边缘可部署模型。其核心思路是：基于一个强大的通用多模态骨干模型，通过一个结合了思维链监督与基于GRPO强化学习的两阶段训练流程，高效注入家居场景知识，同时维持并提升模型的通用能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>MiMo-VL-Miloco-7B的整体训练流程分为两个核心阶段：监督微调（SFT）和基于组相对策略优化（GRPO）的强化学习（RL）。目标是先通过SFT注入家居场景专业知识，再通过RL恢复和增强因专业化而可能受损的通用能力。</p>
<p>模型架构继承自MiMo-VL-7B，包含三个主要组件：1) 支持原生分辨率的视觉Transformer（ViT），用于编码视觉输入；2) 一个多层感知机（MLP）投影器，用于将视觉编码与大型语言模型（LLM）的潜在空间对齐；3) LLM骨干，负责文本理解和推理。所有组件均使用MiMo-VL的预训练权重初始化。</p>
<p><img src="https://arxiv.org/html/2512.17436v2/x1.png" alt="模型架构"></p>
<blockquote>
<p><strong>图2</strong>：MiMo-VL-Miloco-7B模型概览。模型输入为描绘家居场景或用户手势的视频。视频帧由ViT编码，并通过MLP投影器投影到LLM嵌入空间，形成视觉令牌。同时，指令提示被分词为文本令牌。视觉和文本令牌拼接后输入LLM骨干，生成最终响应。</p>
</blockquote>
<p><strong>第一阶段：监督微调（SFT）</strong><br>此阶段专注于注入家居场景知识。训练数据由精心策划的专有家居场景数据和通用多模态数据混合而成。</p>
<ul>
<li><strong>家居场景数据</strong>：针对公开数据稀缺的问题，建立了严格的数据收集流程，涵盖常见日常活动（玩电竞、锻炼、看电视、阅读、玩手机）和手势识别（OK、点赞、V字、沙卡手势、张开手掌）。除了简单标签，还生成了<strong>思维链（CoT）</strong> 标注，以标准化的推理模式帮助模型深入理解场景。同时，为满足边缘部署的低延迟需求，引入了<strong>令牌预算感知推理</strong>数据，鼓励模型输出直接答案而非冗长的推理步骤。这种组合使模型能够以数据高效的方式学习知识，同时保持高效的推理速度。</li>
<li><strong>通用数据</strong>：使用MiMo-VL的训练语料库，涵盖视觉问答、图像定位、OCR理解、视频理解和多模态推理等多种任务，以保持模型的通用视觉理解和推理能力。</li>
<li><strong>训练过程</strong>：使用AdamW优化器，总批次大小为128，学习率为1e-5，进行全参数微调。</li>
</ul>
<p><strong>第二阶段：强化学习（RL）</strong><br>SFT后，模型在家居场景理解上表现提升，但在通用多模态任务（如视频理解、GUI定位、复杂推理）上出现性能退化。为缓解此问题，采用GRPO进行强化学习。</p>
<ul>
<li><strong>数据构建</strong>：针对性能退化的领域构建针对性RL数据集，包括视频时序定位、GUI定位和多模态推理（特别是数学推理）数据。受Time-R1策略启发，应用<strong>难度过滤器</strong>，排除过于简单或困难的样本，以稳定训练、加速收敛并最小化对家居知识保留的干扰。</li>
<li><strong>GRPO算法</strong>：GRPO无需单独的批评者模型，而是通过对一组输出进行组内归一化来计算优势。对于每个查询q，从当前策略中采样一组输出{ o1, o2, …, oG }。优化目标包含策略比率裁剪和与参考策略的KL散度惩罚。优势Âi通过组内奖励归一化计算（ri - 组均值）/ 组标准差），降低了内存开销和训练复杂度。</li>
<li><strong>奖励建模</strong>：总奖励r是准确率奖励racc和格式合规奖励rfmt的加权和（权重分别为0.9和0.1）。针对不同任务设计了具体的准确率奖励函数：1) <strong>时序定位奖励</strong>：基于预测时间段与真实时间段的1D IoU计算；2) <strong>GUI定位奖励</strong>：基于预测边界框与真实框的2D IoU计算；3) <strong>数学推理奖励</strong>：基于从模型思维链中提取的最终答案与标准答案的精确匹配（二元指示函数）。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>评估在家庭场景基准和通用多模态基准上进行。对比的基线模型包括Qwen2.5-VL-7B、Gemini-2.5-Pro、MiMo-VL-7B-SFT-2508，以及Gemma-3-27B-IT、InternVL3-8B等开源模型。</p>
<p><strong>家居场景理解能力</strong>：如表1所示，MiMo-VL-Miloco-7B在所有日常活动和手势识别类别上均取得了最高的F1分数。例如，在看电视、阅读、玩手机、玩电竞、锻炼活动上的F1分别达到98.3、90.8、90.5、99.2、96.7；在手势OK、点赞、V字、沙卡手势、张开手掌上的F1分别达到86.3、90.5、90.3、88.3、87.5。尤其在复杂的沙卡手势识别上，相比最佳基线提升了超过18个F1点。</p>
<p><img src="https://arxiv.org/html/2512.17436v2/figure/radar.png" alt="家居场景性能对比"></p>
<blockquote>
<p><strong>图1</strong>：家居场景手势识别（左）和日常活动识别（右）的每类别F1分数对比雷达图。MiMo-VL-Miloco-7B在大多数手势和活动类别上获得了最高的F1分数，图中标注值为其每类别F1分数。</p>
</blockquote>
<p><strong>通用能力</strong>：如表2所示，模型在广泛的通用基准上表现出强大且均衡的性能。</p>
<ul>
<li><strong>图像理解</strong>：在MMMU-Pro标准集和视觉集上分别达到55.7%和47.2%的准确率，显著优于更大的基线模型。</li>
<li><strong>视频理解</strong>：得益于RL阶段，在Video-MMMU和Charades-STA上分别达到63.6%和46.6%的准确率，在对比模型中达到最优。</li>
<li><strong>文档与OCR</strong>：在ChartQA和DocVQA上分别达到92.0%和95.2%的准确率，领先于其他开源大模型，虽略低于基础SFT模型，但差距微小。</li>
<li><strong>GUI定位</strong>：在ScreenSpot、ScreenSpot-v2和ScreenSpot-Pro上分别达到89.8%、92.1%和37.8%的中心准确率，创下新记录。</li>
<li><strong>文本与推理</strong>：在纯文本推理任务MMLU-Pro上达到68.5的精确匹配分数，显著超越其他开源模型；在MATH500和GPQA-Diamond上也达到了与MiMo-VL家族最佳结果相当的水平。</li>
</ul>
<p><strong>消融实验分析</strong>：两阶段训练策略是关键。SFT阶段显著提升了家居场景性能，但导致了通用视频、GUI、推理任务的退化。随后的GRPO RL阶段有效恢复了这些通用能力，同时保持了家居场景的高性能，实现了专业化与通用化的平衡。难度感知数据过滤和任务特定的奖励设计对RL阶段的稳定和高效起到了重要作用。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个面向边缘部署的、专门针对智能家居环境优化的视觉语言模型MiMo-VL-Miloco-7B，在家居场景理解和通用多模态基准上均取得强劲性能；2) 设计了一个有效的两阶段训练框架，结合了思维链增强的监督微调和基于GRPO的难度感知强化学习，成功平衡了专业化与通用化；3) 开源了模型、量化权重及评估工具包，为智能家居领域的隐私保护、设备端多模态智能研究与应用提供了实践基础。</p>
<p>论文自身提到的局限性在于，为了优先保障家居场景性能，在文档中心任务（如OCR）上出现了轻微的性能回退，尽管模型在此类任务上依然保持稳健。</p>
<p>这项工作对后续研究的启示包括：1) 针对特定垂直领域（如智能家居、医疗、教育）的高效专业化微调策略是可行的，且可以通过精心设计的多阶段训练来缓解灾难性遗忘；2) GRPO等无需价值网络的RL算法在微调大型多模态模型中具有降低复杂度的潜力；3) 思维链监督与令牌预算感知推理的结合，为同时追求模型深度理解和边缘部署效率提供了思路。未来工作可探索融合更多传感模态（如音频、毫米波）的统一多模态学习框架，以及进一步的模型压缩和架构优化，以实现更高效的设备端部署。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文针对智能家居场景，提出并开源了视觉语言模型MiMo-VL-Miloco-7B。其核心目标是使模型在具备通用多模态推理能力的同时，专门优化对家居活动与手势的理解。关键技术包括结合监督微调与基于Group Relative Policy Optimization强化学习的两阶段训练流程，并引入思维链监督和token预算感知推理以提升学习与推理效率。实验表明，该模型在家居手势识别（如OK、点赞等）和日常活动理解（如看电视、阅读等）多个类别上取得了领先的F1分数，同时在多个通用视频与语言理解基准测试上性能也获得一致提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.17436" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>