<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Do You Need Proprioceptive States in Visuomotor Policies? - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Do You Need Proprioceptive States in Visuomotor Policies?</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.18644" target="_blank" rel="noreferrer">2509.18644</a></span>
        <span>作者: Yang Gao Team</span>
        <span>日期: 2025-09-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>基于模仿学习的视觉运动策略已广泛应用于机器人操控。为了精确控制，这些策略通常同时采用视觉观察和本体感知状态（如末端执行器位姿和关节角度）作为输入。然而，这种常见做法使策略过度依赖本体感知状态输入，导致其容易过拟合训练轨迹，从而在任务相关物体的空间位置发生变化时表现出极差的空间泛化能力。在当前收集具有广泛空间覆盖的演示数据成本高昂的背景下，这已成为视觉运动策略发展的关键瓶颈。</p>
<p>本文针对上述痛点，提出了一个反直觉的新视角：完全移除策略中的本体感知状态输入。其核心思路是，在相对末端执行器动作空间和提供完整任务相关视觉观察的条件下，构建仅依赖于视觉输入的“无状态策略”，以迫使策略深入理解任务环境而非记忆轨迹，从而显著提升空间泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的“无状态策略”旨在移除视觉运动策略中的本体感知状态输入，其有效性建立在两个关键条件之上：1) 使用相对末端执行器动作空间；2) 确保策略能获得完整任务观察。</p>
<p><img src="https://arxiv.org/html/2509.18644v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：无状态策略示意图。在相对末端执行器动作空间下，通过双广角腕部相机提供完整任务观察，无状态策略展现出比基于状态的策略显著增强的空间泛化能力。</p>
</blockquote>
<p><strong>整体框架</strong>：策略的输入仅为视觉观察（图像），输出为相对末端执行器动作。训练采用标准的模仿学习，通过最小化动作的负对数似然损失来学习策略参数。部署时，策略根据在线视觉观察直接预测动作并执行。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>相对末端执行器动作空间</strong>：这是支持空间泛化的自然选择。策略在时间步t输出相对位移 $a_t = \Delta p_t = [\Delta x_t, \Delta q_t]$，其中 $\Delta x_t$ 和 $\Delta q_t$ 分别表示平移和旋转。下一个末端执行器位姿更新为 $p_{t+1} = p_t \oplus \Delta p_t$。关键在于，动作 $\Delta p_t$ 仅依赖于当前观察，与机器人的绝对位姿无关。因此，当任务物体空间布局变化但机器人相对于物体的视觉观察相同时，策略会预测相同的相对动作，从而实现泛化。论文排除了绝对动作空间以及相对/绝对关节角动作空间，因为后者（$\Delta p_t = f(\Delta \theta_t, \theta_t)$）的输出依赖于当前关节角 $\theta_t$，在不同空间布局下即使视觉观察相同，也会因 $\theta_t$ 不同而产生错误的末端位移。</p>
</li>
<li><p><strong>完整任务观察</strong>：移除状态输入后，策略必须完全从视觉信息中决策，因此需要为其提供充足的任务相关视觉信息，即“完整任务观察”。为此，论文采用了双广角腕部相机的设置。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.18644v2/x2.png" alt="相机设置对比"></p>
<blockquote>
<p><strong>图2</strong>：(a) 常规腕部相机设置，目标物体有时可能不可见。(b) 双广角腕部相机设置，为任务提供了充足的观察。</p>
</blockquote>
<p><strong>相机系统</strong>：包括一个头顶相机和腕部相机。为了达到“完整任务观察”，论文在末端执行器顶部和底部各安装一个广角腕部相机（视场角约 $120^\circ \times 120^\circ$），如图2(b)所示。这种设置扩展了视野，暴露了末端执行器下方的工作空间。相比之下，常规的单正常视场腕部相机（视场角 $87^\circ \times 58^\circ$）可能无法在复杂场景中提供完整观察。</p>
<p><strong>创新点</strong>：与现有通过数据增强、仿真、构建物体中心特征或引入正则化来提升泛化的方法不同，本文的创新点在于从根本上移除了导致过拟合的“捷径”——本体感知状态输入。这是一种更简单、更本质的解决方案，无需复杂的算法设计或昂贵的多样化数据收集。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/数据集</strong>：在真实世界评估了5项任务：3项“拾放”任务（Pick Pen, Pick Bottle, Put Lid）、一项更具挑战性的“叠衬衫”任务和一项复杂的全身操控任务“取瓶子”。此外，在仿真环境LIBERO基准上进行了评估。</li>
<li><strong>数据</strong>：真实世界演示数据通过遥操作收集，且有意限制了任务相关物体的初始空间分布范围（例如固定桌子高度、限制目标物体在 constrained 2D范围内），以确保评估的泛化能力源于策略本身而非数据多样性。</li>
<li><strong>Baseline方法</strong>：主要对比了基于状态的策略（w/ state）与无状态策略（w/o state）。此外，还比较了不同动作空间、不同相机设置以及不同策略架构（$\pi_0$, ACT, Diffusion Policy）下的表现。</li>
<li><strong>评估指标</strong>：空间泛化分为高度泛化（垂直方向物体位置变化）和水平泛化（2D平面内物体位置变化），计算平均成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在3项“拾放”任务中，无状态策略（使用双广角腕部相机）的空间泛化能力显著优于基于状态的策略。</p>
<p><img src="https://arxiv.org/html/2509.18644v2/x5.png" alt="空间泛化性能"></p>
<blockquote>
<p><strong>图5</strong>：三项真实世界“拾放”任务的高度和水平泛化性能。在完整任务观察下，无状态策略显示出比基于状态策略显著提升的空间泛化能力。</p>
</blockquote>
<p>以“Pick Pen”任务为例，高度泛化成功率从0%提升至98%，水平泛化成功率从0%提升至58%。与使用常规腕部相机的无状态策略相比，双广角相机的设置进一步将高度泛化成功率从87%提升至98%，水平泛化从27%提升至58%。</p>
<p>在更具挑战性的“叠衬衫”和“取瓶子”任务中，即使只能使用常规腕部相机，无状态策略也表现出显著更强的水平泛化能力（成功率分别从18.3%提升至83.4%，从11.7%提升至78.4%）。</p>
<p><strong>消融实验与组件贡献</strong>：</p>
<ol>
<li><strong>动作空间</strong>（表III）：仅相对末端执行器动作空间在空间泛化上表现良好（高度泛化98.4%，水平泛化58.4%），而绝对末端执行器、相对关节角和绝对关节角动作空间在泛化评估中成功率均为0%，验证了相对末端执行器动作空间的必要性。</li>
<li><strong>完整任务观察</strong>（表IV）：通过改变相机设置调整任务观察的完整度。随着视野扩大（从无腕部相机→单常规腕部相机→双常规腕部相机→单广角腕部相机→双广角腕部相机），无状态策略的空间泛化能力逐步提升。一个有趣的发现是，仅使用双广角腕部相机而移除头顶相机时，泛化性能达到最佳（高度和水平泛化成功率均为100%），表明头顶相机可能带来不利的分布偏移。</li>
<li><strong>策略架构</strong>（表V）：在$\pi_0$、ACT和Diffusion Policy三种架构上，无状态策略均一致地展现出远优于对应基于状态策略的空间泛化能力，表明该结论具有普遍性。</li>
<li><strong>数据效率</strong>（图6）：在域内“Pick Pen”任务中，减少微调数据量会导致基于状态的策略过拟合并失败，而无状态策略在数据量减少时仍能保持较高的成功率，显示出更高的数据效率。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.18644v2/x6.png" alt="数据效率"></p>
<blockquote>
<p><strong>图6</strong>：在“Pick Pen”任务中，使用不同数量微调数据时的域内评估成功率。无状态策略在数据减少时性能下降更少，数据效率更高。</p>
</blockquote>
<ol start="5">
<li><strong>跨本体适应</strong>（表VI）：在“叠衬衫”任务中，将策略从Arx5机械臂迁移到类人双手机器人上微调，无状态策略比基于状态的策略适应更快，在相同微调步数下成功率更高，显示出更好的跨本体适应能力。</li>
<li><strong>重新思考头顶相机</strong>（表VII）：在更挑战性的泛化场景中（如桌子升至100cm），使用头顶相机的无状态策略性能很差，而不使用头顶相机、仅依赖双广角腕部相机的策略成功率保持高位，证实头顶相机可能成为新的瓶颈。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了“无状态策略”这一新颖设计，通过完全移除本体感知状态输入，迫使视觉运动策略从视觉观察中进行推理，从而从根本上缓解了对训练轨迹的过拟合。</li>
<li>通过大量实证研究，在多种任务、机器人本体和策略架构上验证了无状态策略在空间泛化、数据效率和跨本体适应方面的显著优势。</li>
<li>明确了该策略有效的两个关键条件（相对末端执行器动作空间和完整任务观察），并发现了头顶相机可能不利于分布外泛化的有趣现象。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在任务场景极其复杂（例如涉及多个被遮挡的物体）时，仅凭腕部相机可能无法提供完整的任务观察，可能需要额外的视觉信息或感知模块。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>挑战了视觉运动策略中“状态输入必不可少”的固有观念，鼓励研究者重新思考多模态传感器融合的必要性。</li>
<li>为提升机器人策略的泛化能力提供了一条简单而有效的路径，即专注于改善视觉观察的完备性和设计更泛化友好的动作空间。</li>
<li>指出了未来机器人感知系统设计的一个可能方向：开发更鲁棒、视角更灵活的视觉系统（如广角腕部相机），以减少对固定视角外部相机（如头顶相机）的依赖。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究视觉运动策略中是否必须使用本体感知状态输入。核心问题是传统方法因依赖状态输入导致对训练轨迹过拟合，空间泛化能力差。为此，论文提出“无状态策略”，关键技术包括：采用相对末端执行器动作空间，并仅基于视觉观察（通过双广角腕戴相机确保完整任务视野）预测动作。实验表明，该方法显著提升了空间泛化能力：在多种真实机器人任务中，高度泛化平均成功率从0%提升至85%，水平泛化从6%提升至64%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.18644" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>