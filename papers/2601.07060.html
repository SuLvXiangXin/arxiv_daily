<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.07060" target="_blank" rel="noreferrer">2601.07060</a></span>
        <span>作者: Ismini Lourentzou Team</span>
        <span>日期: 2026-01-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过利用预训练的视觉-语言主干网络，直接将视觉观察和语言指令映射到机器人动作，在机器人操作领域取得了显著进展。然而，现有方法本质上局限于短视野操作，在处理动态场景中的长视野、多步骤规划时存在困难。一个根本性局限是缺乏结构化的可供性线索和明确的状态跟踪。现有模型虽然可以推断最终目标并产生中间动作，但缺乏能够区分下一个应操作的对象、交互的相关部分或区域、物品应放置或移动的位置，或下一步合适动作的内部表征。这导致许多视觉上相似的状态变得模糊不清，掩盖了潜在的任务阶段并破坏了长视野控制的稳定性。此外，现有VLA缺乏持续估计子任务内进度的机制。没有这种持续的在线进展概念，策略无法可靠地决定是继续、切换阶段还是终止，从而导致长视野任务中典型的失败模式：重复或不必要的动作、跳过必需的子任务、过早终止，甚至在错误状态下宣告成功。</p>
<p>本文针对长视野操作中因缺乏结构化交互线索和连续进度跟踪而导致的执行不稳定问题，提出了一种新视角：将策略学习围绕以交互为中心的可供性推理和子任务进度线索进行结构化。其核心思路是引入可学习查询来预测一组结构化的未来可供性，并以此作为条件，通过基于扩散的策略联合解码机器人的动作和一个连续的进度值，从而实现多步骤任务中的时间状态跟踪和无缝子任务转换。</p>
<h2 id="方法详解">方法详解</h2>
<p>PALM是一个端到端的VLA框架，其整体架构如图2所示，旨在通过结合可供性推理和进度感知来提升长视野操作的稳定性和连贯性。</p>
<p><img src="https://..." alt="PALM概述"></p>
<blockquote>
<p><strong>图2</strong>：PALM概述。(a) 模型架构：给定语言指令l、观察o_t和机器人状态s_t，PALM使用冻结的编码器对每种模态进行编码，获得文本、视觉和状态令牌。这些令牌通过一个具有单向注意力的GPT风格Transformer融合，并配备两个专门的查询集：细粒度可供性查询和动作-进度查询。在训练时，可供性查询关注上下文令牌，以通过四个有监督的头（&lt;全局&gt;、&lt;局部&gt;、&lt;空间&gt;、&lt;动态&gt;）预测前瞻性可供性表征F̂_t+n。在推理时，移除可供性头；动作-进度查询同时关注上下文和可供性前瞻，以条件化一个扩散Transformer，该Transformer预测用于连续控制的动作轨迹â_t:t+n-1和进度轨迹p̂_t:t+n-1。(b) 结构化注意力：可供性子查询仅关注共享的上下文令牌以保持解耦，而两种查询类型都使用因果注意力来保持时间一致性。</p>
</blockquote>
<p><strong>整体流程与核心模块</strong>：</p>
<ol>
<li><strong>多模态编码</strong>：模型处理三个同步输入：语言指令l（使用CLIP文本编码器）、图像观察o_t（使用掩码自编码器编码，并由感知重采样器下采样）和机器人状态s_t（通过轻量级MLP投影）。编码后的令牌被拼接成一个统一的多模态序列。</li>
<li><strong>骨干网络与可学习查询</strong>：骨干网络采用GPT-2风格的Transformer来整合令牌序列。在此基础上，PALM引入了两个可学习查询集：<ul>
<li><strong>细粒度可供性查询</strong>：包含四个子查询 <code>&lt;全局&gt;</code>、<code>&lt;局部&gt;</code>、<code>&lt;空间&gt;</code>、<code>&lt;动态&gt;</code>。它们关注语言、视觉和状态令牌，以在互补的尺度上提取任务相关的表征，并产生可供性中心化的潜在表征 F̂_t+n = f_aff(l, o_t, s_t)。</li>
<li><strong>动作-进度查询</strong>：汇集控制相关的上下文，并以 F̂_t+n 为条件，支持进度感知的逆动力学解码。</li>
</ul>
</li>
<li><strong>细粒度可供性预测（核心模块一）</strong>：该模块作为中间的隐式推理步骤，预测结构化的、任务相关的表征。<ul>
<li><strong>全局预测</strong>：识别指令相关的对象及其大致区域，提供高级语义先验。使用Grounding DINO和SAM获取目标掩码进行监督（损失：焦点损失 + Dice损失）。</li>
<li><strong>局部预测</strong>：在全局区域基础上，预测密集的接触似然分布，用于精确的接触几何推理。使用高斯热图作为监督目标（损失：焦点损失 + KL散度）。</li>
<li><strong>空间预测</strong>：将未明确指定的空间语言转换为可执行的放置建议，预测一组候选放置点。使用SpatialVLM和RoboPoint获取目标点集进行监督（损失：集合匹配损失）。</li>
<li><strong>动态预测</strong>：识别与机器人夹爪和可移动物体对应的像素，并预测这些区域如何随时间演变。使用CoTracker跟踪运动轨迹并生成动态区域掩码进行监督（损失：基于变分自编码器的掩码重建损失）。</li>
</ul>
</li>
<li><strong>进度感知策略（核心模块二）</strong>：这是一个基于逆动力学的模块，将动作生成与子任务进度估计耦合。<ul>
<li>模型首先从可供性潜在表征推断当前活跃的子任务阶段，并生成阶段嵌入。</li>
<li>以此嵌入为条件，模型预测一个标量 p_t ∈ [0, 1]，量化阶段内的完成度。</li>
<li>动作头是一个去噪扩散Transformer，它以当前观察 o_t、指令 l、状态 s_t 和预测的可供性潜在表征 F̂_t+n 为条件，联合生成一个 horizon-n 的动作-进度轨迹：(â_t:t+n-1, p̂_t:t+n-1) = DiT(l, o_t, s_t, F̂_t+n)。训练遵循标准扩散目标，噪声预测器 ε_θ 需要预测添加到目标动作-进度向量上的噪声。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：<br>与现有直接预测动作或预测密集未来图像的方法相比，PALM的创新点在于：1) 引入了结构化的细粒度可供性预测作为中间推理步骤，提供了更丰富、更明确的交互场景表征；2) 在动作空间中显式地添加并联合预测连续的进度标量，为策略提供了时间上的“定位”信息，有助于消歧视觉相似但阶段不同的状态，并平滑子策略边界处的过渡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>训练</strong>：采用两阶段训练。预训练使用机器人数据集（BridgeData V2, DROID）和长视野视频数据（EPIC-KITCHENS, RoboCerebra）。微调使用收集的942条带有人工标注可供性和连续进度标签的机器人轨迹。</li>
<li><strong>基准</strong>：在CALVIN ABC→D（34个任务，在未见过的D环境中评估）和LIBERO（包含Spatial, Object, Goal, Long四个任务套件，各10个任务）两个仿真基准上进行评估。</li>
<li><strong>基线方法</strong>：对比了四类基线：自回归方法（RT-1, Robo-Flamingo, OpenVLA）、基于扩散的方法（Diffusion Policy, π0）、3D感知方法（3D-VLA, 3D Diffuser Actor, RoboUniview）和预测方法（Susie, GR-1, Seer）。在LIBERO上还比较了Octo、SpatialVLA、CoT-VLA、TraceVLA和CoA-VLA。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在CALVIN ABC→D上，PALM取得了最先进的性能（表1）。例如，在连续完成5个子任务的难度下，PALM成功率达到82.0%，比之前最强的基线Seer（64.3%）绝对提升了17.7%。PALM的平均任务轨迹长度也最长（4.48），表明其在扩展序列上的执行更稳定。移除进度预测的消融版本（PALM✗progress）性能一致下降，验证了进度感知的重要性。</p>
<p><img src="https://..." alt="CALVIN结果表"></p>
<blockquote>
<p><strong>表1</strong>：CALVIN ABC→D实验结果。PALM在各项指标上均显著优于所有基线。</p>
</blockquote>
<p>在LIBERO上，PALM在所有四个任务套件中都达到了最先进的平均成功率94.5%（表2）。其中在最具挑战性的LIBERO-LONG套件上，PALM取得了91.8%的成功率，比之前最强的基线CoT-VLA（69.0%）高出22.8%。</p>
<p><img src="https://..." alt="LIBERO结果表"></p>
<blockquote>
<p><strong>表2</strong>：LIBERO实验结果。PALM在所有任务套件上均取得最佳性能。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ol>
<li><strong>可供性组件贡献</strong>（图3）：逐步添加全局(G)、局部(L)、空间(S)、动态(D)可供性预测组件到基础VLA中，在CALVIN和LIBERO-LONG上的性能逐步提升。全局组件带来初步稳定增益；局部组件在CALVIN上带来额外增益但在LIBERO-LONG上略有下降（可能与视角引起的几何偏差有关）；空间组件通过提供鲁棒的放置先验恢复了性能并进一步改善；动态组件与空间推理结合后（完整的PALM）取得了最佳性能。</li>
</ol>
<p><img src="https://..." alt="可供性消融实验图"></p>
<blockquote>
<p><strong>图3</strong>：在CALVIN ABC→D和LIBERO-LONG基准上的可供性组件消融研究，展示了四种可供性预测组件的有效性。</p>
</blockquote>
<ol start="2">
<li><strong>PALM核心模块贡献</strong>（表3）：在CALVIN ABC→D上分别移除可供性前瞻、逆动态预测和进度预测模块。结果表明，所有三个组件都贡献了互补的能力，共同实现了最佳性能。移除可供性前瞻在微调阶段导致的性能下降最大（平均长度 4.48→3.58），突显了其作为中间表征的关键作用。</li>
<li><strong>训练数据组成</strong>（表4）：分别移除预训练数据中的“野外”机器人数据、长视野视频数据、微调阶段的人类标注数据以及仿真预训练数据。实验表明，每类数据都对最终性能有贡献，组合使用能实现最佳效果，特别是长视野视频数据对于学习语义进度估计至关重要。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了PALM，一个统一的VLA框架，通过集成结构化的可供性推理和进度感知的策略生成，实现了在长视野、接触丰富的操作任务中的可靠执行。</li>
<li>设计了两个新颖的互补模块：细粒度可供性预测器（作为中间隐式推理步骤）和进度感知的逆动力学模块（联合动作生成与进度估计），确保了长动作序列的时间连贯性。</li>
<li>在仿真和真实世界实验中进行了全面评估，在CALVIN ABC→D上相比之前最先进方法提升了12.5%，在LIBERO-LONG上达到91.8%的成功率，并在三项真实世界长视野泛化测试中表现出一致的优势。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，PALM的性能可能受到可供性标注质量和覆盖范围的限制。在高度非结构化或动态变化剧烈的环境中，预测的可供性和进度的准确性可能面临挑战。此外，依赖于大规模多源数据进行预训练和特定标注数据进行微调，其数据收集和标注成本仍需考虑。</p>
<p><strong>启示</strong>：<br>PALM的工作表明，在端到端的VLA策略中引入结构化的、可解释的中间表征（如可供性）和显式的时间状态信号（如进度），是解决长视野任务执行不稳定问题的有效途径。这为后续研究提供了方向：1) 探索更高效或自监督的可供性与进度学习方式，以减少对昂贵标注的依赖；2) 将这种结构化推理框架扩展到更复杂的多模态交互或多智能体协作场景；3) 研究如何使进度估计更加鲁棒，以应对任务执行过程中的意外干扰或失败恢复。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在长视野、多步骤机器人操作任务中缺乏内部推理机制、易出现重复动作或步骤遗漏等问题，提出PALM框架。其核心方法是引入可学习查询来预测结构化的未来可供性，并基于扩散策略联合解码机器人动作与连续进度值，以实现状态跟踪与子任务无缝切换。实验表明，PALM在LIBERO-LONG任务上达到91.8%的成功率，在CALVIN ABC→D任务上平均任务长度提升12.5%，并在真实世界长视野泛化设置中性能超越基线2倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.07060" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>