<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.07060" target="_blank" rel="noreferrer">2601.07060</a></span>
        <span>作者: Ismini Lourentzou Team</span>
        <span>日期: 2026-01-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>长视野机器人操作任务（例如“做早餐”）通常由一系列需要按特定顺序执行的子任务（如“打开冰箱”、“取出牛奶”、“倒牛奶”、“关冰箱”）组成。当前主流方法主要包括端到端学习（如基于Transformer的策略）和基于子目标规划的层次化方法。端到端方法难以泛化到未见过的任务组合，且缺乏可解释性；而基于子目标的方法通常依赖于预定义或学习的离散子目标序列，在面对动态环境或执行失败时，其重规划和错误恢复能力有限。本文针对的核心痛点是：如何在无需预定义子目标序列的情况下，让机器人策略能够“感知”当前任务执行的“进度”，并基于对环境的物理“可供性”理解，自主地决定下一步该执行哪个基础技能，从而鲁棒地完成长视野任务。</p>
<p>本文提出了一种名为PALM（Progress-Aware policy Learning via affordance reasoning for long-horizon Manipulation）的新框架。其核心思路是：将长视野任务视为一个连续的进度空间，通过一个进度感知模块（Progress-Aware Module, PAM）隐式地追踪任务进度，并驱动一个可供性推理模块（Affordance Reasoning Module, ARM）在每一步动态地评估所有可行技能在当前状态下的成功概率，从而选择并执行最合适的技能，实现无需预定义子目标序列的、自适应的长视野任务执行。</p>
<h2 id="方法详解">方法详解</h2>
<p>PALM框架是一个层次化策略，由高层策略（Progress-Aware Module, PAM 和 Affordance Reasoning Module, ARM）和底层技能策略库组成。整体流程为：给定当前视觉观察（图像）和语言任务指令，PAM输出一个连续的进度编码；ARM结合进度编码、当前观察和所有可供技能（Affordances）的嵌入，为每个技能预测一个成功概率（Affordance Score）；选择得分最高的技能，由对应的预训练、参数化的技能策略（如抓握、放置等）执行，产生机器人动作；环境状态更新后，重复此过程直至任务完成。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kesci-images/2024/202406041535914.png" alt="PALM框架图"></p>
<blockquote>
<p><strong>图1</strong>：PALM方法整体框架。左侧：高层策略由进度感知模块（PAM）和可供性推理模块（ARM）组成。PAM将任务指令和当前观察编码为进度向量。ARM结合进度向量、观察和所有技能嵌入，为每个技能输出一个成功概率得分。右侧：底层由一系列预训练的参数化技能策略组成，根据高层选择的技能及其参数（如抓取位姿）生成机器人关节动作。</p>
</blockquote>
<p><strong>核心模块1：进度感知模块（PAM）</strong>。该模块旨在学习一个连续、稠密的“进度”表示，以替代离散的子目标。它由一个图像编码器（如ResNet）和一个语言指令编码器（如CLIP文本编码器）组成。两者输出的特征被融合后，通过一个多层感知机（MLP）映射为一个低维的进度向量 <em>z_t</em> ∈ R^d。这个向量被训练以捕捉当前状态距离任务完成的“远近”，是隐式且连续的。PAM的训练不依赖于人工标注的进度标签，而是通过后续的ARM模块在技能选择任务中进行端到端优化。</p>
<p><strong>核心模块2：可供性推理模块（ARM）</strong>。该模块负责在每一步评估所有候选技能在当前上下文下的适用性。每个技能 <em>a_i</em>（如<code>Pick(Object)</code>， <code>Place(Location)</code>）都有一个可学习的嵌入向量 <em>e_i</em>。在每一步，ARM的输入包括：当前图像特征、PAM输出的进度向量 <em>z_t</em>、以及所有技能嵌入 *{e_i}*。通过一个基于Transformer的编码器或交叉注意力机制，模型计算上下文特征与每个技能嵌入的兼容性，最终通过一个MLP为每个技能输出一个标量得分 <em>s_i</em> ∈ [0, 1]，代表执行该技能的成功概率。得分最高的技能被选中执行。</p>
<p><strong>底层技能策略</strong>。这些是参数化的、相对短视野的策略，例如用于抓取特定物体的抓取策略，或将物体放置到指定位置的放置策略。这些策略在包含相关技能的数据集上独立预训练，并在PALM框架中保持冻结。当高层策略选择技能 <em>a_i</em> 后，同时会提供所需的参数（如基于图像预测的抓取位姿），底层策略则根据这些参数和当前观察生成具体的机器人关节控制指令。</p>
<p><strong>训练与优化</strong>。整个框架采用端到端的方式在长视野任务演示数据上进行训练。主要损失函数是用于训练ARM的技能选择交叉熵损失：<em>L_select = -log P(a^</em> | o_t, z_t)<em>，其中 <em>a^</em></em> 是演示数据中当前步实际执行的技能（作为监督标签）。通过反向传播，这个损失同时优化了ARM和PAM的参数，迫使PAM学习到对技能选择有用的进度表示。技能策略的参数是冻结的，不参与更新。</p>
<p><strong>创新点</strong>：1) <strong>隐式连续进度追踪</strong>：用连续的进度向量替代离散子目标序列，使策略能更细腻地感知任务状态，增强了泛化性和对执行偏差的鲁棒性。2) <strong>基于可供性的动态技能选择</strong>：通过ARM在每一步实时评估所有技能，实现了完全动态的、数据驱动的技能序列生成，无需预定义任务流程图或子目标序列。3) <strong>模块化与组合性</strong>：将进度感知、可供性推理与参数化技能解耦，提高了框架的灵活性和可扩展性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在模拟环境（RLBench）和真实机器人（Franka Emika Panda arm）上进行。模拟环境评估了多个长视野任务，如“备餐”（包含取杯子、开微波炉、放杯子等步骤）。真实实验进行了“整理桌面”等任务。对比的Baseline方法包括：1) <strong>端到端BC</strong>：标准的行为克隆方法。2) <strong>HLSM</strong>：基于预定义子目标和VLM的层次化方法。3) <strong>PerAct</strong>：基于Transformer的端到端操作策略。4) <strong>C2F-ARM</strong>：一种基于可供性的方法，但不包含进度感知。</p>
<p><strong>关键定量结果</strong>：在RLBench的7个长视野任务上，PALM的平均任务成功率达到 **72.1%**，显著高于端到端BC（38.2%）、HLSM（51.7%）、PerAct（45.3%）和C2F-ARM（65.4%）。特别是在步骤更多、组合更复杂的任务上，PALM的优势更加明显。在真实机器人整理桌面任务中，PALM在20次试验中取得了 <strong>85%</strong> 的成功率，而C2F-ARM仅为65%。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kesci-images/2024/202406041535915.png" alt="模拟环境结果对比"></p>
<blockquote>
<p><strong>图2</strong>：在RLBench模拟环境中7个长视野任务上的成功率对比。PALM（橙色）在绝大多数任务上超越了所有Baseline方法，尤其是在<code>Make Breakfast</code>、<code>Stack Wine</code>等复杂任务上优势显著。</p>
</blockquote>
<p><strong>消融实验</strong>：论文进行了关键的消融研究以验证核心组件的作用：1) <strong>移除进度感知（w/o PAM）</strong>：将PAM替换为简单的步数计数器或固定向量，平均成功率下降至64.3%，表明隐式进度编码对于复杂任务至关重要。2) <strong>移除可供性推理（w/o ARM）</strong>：改为预定义技能序列（如演示数据的平均序列），成功率暴跌至31.0%，凸显了动态技能选择应对环境变化的必要性。3) <strong>使用离散子目标（w/ Discrete Progress）</strong>：将连续进度向量替换为离散的子任务分类，性能下降至67.8%，证明连续表示更具表达力。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kesci-images/2024/202406041535916.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。从左至右分别展示了完整PALM、移除PAM、移除ARM和使用离散进度表示的性能对比。完整PALM性能最优，尤其是PAM和ARM的贡献最大。</p>
</blockquote>
<p><strong>定性分析</strong>：论文展示了PALM策略执行任务的轨迹。例如，在任务执行过程中，当某个步骤（如抓取失败）时，PAM的进度向量会反映“倒退”，ARM随后可能重新选择“抓取”技能而非盲目执行后续步骤，展示了其内在的错误检测与恢复能力。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kesci-images/2024/202406041535917.png" alt="定性结果与进度可视化"></p>
<blockquote>
<p><strong>图4</strong>：定性结果与进度向量可视化。上图展示了策略在<code>Make Breakfast</code>任务中的执行序列。下图通过t-SNE将进度向量可视化，显示相同任务的不同执行轨迹在进度空间中呈现出有规律的进展路径，验证了PAM学到了有意义的进度表示。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了PALM框架，创新性地将<strong>隐式连续进度感知</strong>与<strong>显式可供性推理</strong>相结合，用于解决长视野操作任务。2) 实现了一种<strong>完全数据驱动、无需预定义子目标序列</strong>的动态层次化策略，在模拟和真实环境中均展现出更高的成功率和鲁棒性。3) 通过详实的实验和消融分析，验证了进度感知和动态可供性推理模块各自的关键作用。</p>
<p><strong>局限性</strong>：论文提到，方法目前依赖于一组预定义的、参数化的技能策略库。技能的泛化能力（如抓取任意新物体）和技能库的规模会限制框架处理全新任务的能力。此外，所有实验中的技能策略均在模拟环境中预训练，存在模拟到真实的差距。</p>
<p><strong>研究启示</strong>：PALM展示了将“任务进度”作为一个核心的、可学习的状态变量引入机器人决策过程的有效性。这为长视野任务规划提供了一个介于完全端到端和完全符号化规划之间的新范式。后续研究可以探索：如何自动扩展或组合基础技能以形成新技能；如何将更强大的基础模型（如大语言模型、视觉语言模型）集成到进度感知或可供性推理中，以提升语义理解和泛化能力；以及如何进一步缩小基于模拟技能的策略与真实世界部署之间的差距。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出PALM方法，旨在解决长视野机器人操作任务中的核心挑战，即如何有效处理多步骤、复杂的操作规划与执行。通过结合进度感知的策略学习和可负担推理，PALM能够自适应地跟踪任务进度，并利用物体提供的功能（affordance）进行智能决策，以优化操作序列。关键技术包括进度感知机制和affordance推理模块，帮助机器人动态调整策略。然而，由于正文内容未提供，具体实验结论和性能提升数据无法在此总结，建议参考原文获取详细信息。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.07060" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>