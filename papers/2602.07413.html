<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.07413" target="_blank" rel="noreferrer">2602.07413</a></span>
        <span>作者: Han, Yunhai, Bai, Linhao, Xiao, Ziyu, Yang, Zhaodong, Choudhary, Yogita, Jha, Krishna, Kong, Chuizheng, Kousik, Shreyas, Ravichandar, Harish</span>
        <span>日期: 2026/02/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从演示中学习复杂的视觉运动操作技能的主流方法是基于扩散模型（Diffusion Policies）和Transformer（如ACT）的反应式策略。这些方法将技能建模为从观测到动作的映射（$o_{t}\to a_{t}$），本质上是将任务视为一系列独立的决策。为了缓解抖动并鼓励时间连贯性，它们严重依赖固定长度、硬编码的动作分块（$o_{t}\to a_{t:t+H}$）或时间集成作为启发式方法。这引入了一个僵化的权衡：为了获得深思熟虑的规划（全时域预测）需要长分块，而为了反应性（快速响应）则需要短分块，且任何连贯性都是平均的副产品，而非学习表征的内在属性。另一种视角是将技能视为由底层动态系统支配。然而，动态运动基元（DMPs）等经典方法往往只建模机器人自身的运动动态，将环境视为静态边界条件，未能捕捉灵巧操作的核心——机器人与环境之间错综复杂的耦合关系。</p>
<p>本文针对反应式策略在时间连贯性与反应性之间的僵化权衡，以及传统动态系统方法对环境动态建模的忽视，提出了统一行为模型（Unified Behavioral Models, UBMs）这一新框架。其核心思路是：将灵巧技能学习为耦合的动态系统，共同捕捉环境视觉特征（视觉流）和机器人本体感知状态（动作流）的协同演化，从而通过构造确保时间连贯性，并作为隐式规划器生成全时域预测。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架首先从演示数据中学习统一行为模型（UBM），其核心是捕获行为动态：支配视觉流和动作流协同演化的规律。形式上，UBM定义为 $z_{t+1}=f_{UBM}(z_{t})$，其中 $z_{t}=f_{enc}(\xi_{t})$ 是通过编码统一行为状态 $\xi_{t}=[a_{t},o_{t}]$ 得到的潜状态。为了从潜流中提取预测动作，还需要一个动作解码器 $a_{t}=f_{dec}(z_{t})$。</p>
<p><img src="https://arxiv.org/html/2602.07413v2/images/Unified_Behavioral_Models.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架对比。(A) 标准的反应式策略（如Diffusion, ACT）将观测映射到短时域动作块，缺乏对未来的一致内部模型或观测窗口之外的记忆。(B) 统一行为模型（UBM）将技能建模为机器人和环境的联合行为动态，支配着潜空间中的连续流，从而确保连贯性并支持从初始条件进行全时域“规划”。(C) Koopman-UBM作为UBM的第一个实例，将视觉和本体感知观测提升到由学习的Koopman算子支配的潜空间。通过在统一的“状态包含”潜空间中强制执行线性谱动态，K-UBM实现了快速推理和预测性监控。(D) 该方法能够实现时间连贯的预测（紫色虚线），对视觉遮挡具有鲁棒性（灰色框内的绿色和紫色虚线轨迹），并通过事件触发的重规划策略实现反应性，该策略仅在预测的视觉特征与现实偏离超过阈值时重新初始化UBM（顶部橙色框）。</p>
</blockquote>
<p>为实现UBM，本文提出了Koopman-UBM，它利用Koopman算子理论将复杂的非线性交互动态线性化。方法流程分为两个阶段：</p>
<ol>
<li><strong>视觉特征提取</strong>：从原始RGB图像中提取紧凑的、与任务相关的视觉特征。论文探索了两种互补的表示：(i) 基于点跟踪的、以运动为中心的特征（Object Flow）；(ii) 通过自监督学习（DynaMo）获得的、以操作为中心的特征。</li>
<li><strong>Koopman-UBM学习</strong>：学习一个基于Koopman的UBM，给定初始条件可以预测视觉特征流和动作流。</li>
</ol>
<p><strong>核心模块与技术细节</strong>：</p>
<ul>
<li><strong>统一行为状态定义</strong>：将机器人动作 $a_{t}$ 和学习到的视觉特征 $\phi_{t}$ 拼接起来：$\xi_{t} \triangleq [a_{t}; \phi_{t}]$。</li>
<li><strong>统一谱编码器（状态包含提升）</strong>：使用多层感知机（MLP）编码器 $g_{\theta}: \xi_{t} \mapsto \psi_{t}$ 将行为状态提升到更高维空间。关键创新是构建<strong>状态包含</strong>的潜表示：$\bm{z}<em>{t} \triangleq [\xi</em>{t}; \psi_{t}]$。这确保了潜Koopman动态植根于可解释的行为状态，避免漂移，并支持无需解码器的快速推理。</li>
<li><strong>线性潜动态</strong>：在构建的潜空间中，学习一个Koopman矩阵 $K \in \mathbb{R}^{d_{z} \times d_{z}}$，使得动态近似为线性：$\bm{z}<em>{t+1} = K \bm{z}</em>{t}$。</li>
<li><strong>协同训练与损失函数</strong>：编码器和线性动态被联合训练，以学习一个支持线性结构的动态感知潜表示。使用多步潜预测损失来鼓励时间连贯性：$\mathcal{L}<em>{\text{K-coherence}} = \mathbb{E}</em>{t}\left[\sum_{l=1}^{H} | K^{l} \bm{z}<em>{t} - \bm{z}</em>{t+l} |_{2}^{2} \right]$。训练稳定性的关键包括：对Koopman矩阵使用更小的学习率和梯度裁剪，以及将Koopman矩阵初始化为单位矩阵。</li>
<li><strong>在线重规划策略</strong>：Koopman-UBM能够显式预测视觉特征的标称流，因此可以充当自身的运行时监控器。系统监控预测视觉特征与观测视觉特征之间的误差。只要预测匹配现实，就执行初始标称计划；当扰动导致误差超过阈值时，系统触发重规划（重新初始化 $\bm{z}_{t}$ 并计算新的连贯轨迹）。</li>
</ul>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>框架创新</strong>：提出了UBM框架，将技能建模为机器人与环境耦合的行为动态系统，而非反应式映射。</li>
<li><strong>实现创新</strong>：首次利用Koopman算子理论实例化UBM，通过状态包含提升和线性谱动态，将复杂的非线性接触行为线性化，实现可靠的开环轨迹生成。</li>
<li><strong>机制创新</strong>：利用模型能够预测视觉流的特点，设计了基于预测监控的事件触发重规划机制，灵活地在规划与反应之间切换。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/数据集</strong>：7个模拟任务（DexArt模拟器中的Bucket, Laptop, Toilet；MuJoCo模拟器中的Door, Tool use, In-hand reorientation, Object relocation）和2个真实世界任务。</li>
<li><strong>实验平台</strong>：SAPIEN（DexArt）、MuJoCo（Adroit）仿真器及真实机器人平台。</li>
<li><strong>基线方法</strong>：<ul>
<li><strong>通用方法</strong>：扩散策略（Diffusion policy）、动作分块Transformer（ACT）。</li>
<li><strong>现有Koopman方法</strong>：KODex（使用预定义多项式提升函数）、KOROL（仅使用多步机器人动作预测损失）。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2602.07413v2/x1.png" alt="任务图示"></p>
<blockquote>
<p><strong>图2</strong>：K-UBM在七个模拟任务和两个真实世界任务上的评估环境示意图。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2602.07413v2/x2.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图3</strong>：各方法在测试集上的任务成功率（顶部行使用Flow特征，底部行使用DynaMo特征）。误差条表示五个随机种子的标准差，平均结果（最后一列）计算自所有种子和所有任务。K-UBM方法达到了最高的平均成功率。</p>
</blockquote>
<ol>
<li><strong>总体任务性能</strong>：如图3所示，K-UBM在平均成功率上匹配或超过了最先进的基线方法。扩散策略和ACT需要针对任务特定调整动作分块大小，而K-UBM无需此调优。</li>
<li><strong>推理速度</strong>：如表I所示，基于Koopman的策略（K-UBM, KOROL, KODex）和ACT的推理速度（<del>0.3-0.5 ms/步）比扩散策略（</del>30-36 ms/步）快两个数量级。</li>
</ol>
<p><strong>环境动态预测准确性</strong>：<br>K-UBM能够联合预测机器人状态和以操作为中心的环境动态。</p>
<p><img src="https://arxiv.org/html/2602.07413v2/x3.png" alt="流特征预测可视化"></p>
<blockquote>
<p><strong>图4</strong>：通过解码预测的流特征并将重构的流点（红点）叠加在对应图像上，可视化预测的流点轨迹。预测有效反映了整个任务执行过程中的操作目标。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07413v2/x4.png" alt="DynaMo特征预测可视化"></p>
<blockquote>
<p><strong>图5</strong>：使用t-SNE将预测的DynaMo特征与真实特征一起投影到二维空间进行可视化。预测的高维视觉特征与真实特征对齐良好。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07413v2/x5.png" alt="特征预测误差分析"></p>
<blockquote>
<p><strong>图6</strong>：（顶部行）预测与真实流点之间的重构误差（RMSE）。（底部行）预测与真实DynaMo特征之间的余弦相似度。失败 rollout 的流预测误差 consistently 大于成功的 rollout，表明物体流预测退化与机器人执行不良强相关。DynaMo特征相似度随时间普遍下降，符合长时域预测预期。</p>
</blockquote>
<ol start="3">
<li><strong>特征预测质量</strong>：如图6所示，对于流特征，失败 rollout 的预测误差 consistently 大于成功的 rollout，揭示了物体流预测精度与任务执行成功的强相关性。对于DynaMo特征，余弦相似度随时间推移而下降，但变化相对平缓。</li>
</ol>
<p><strong>消融实验总结</strong>：<br>论文虽未以独立章节呈现系统的消融实验，但通过设计与现有Koopman方法（KODex, KOROL）的对比，间接验证了其核心组件的贡献：</p>
<ul>
<li><strong>状态包含提升与多步损失</strong>：KODex使用预定义提升且无多步监督，KOROL仅预测动作而无视觉特征监督。K-UBM通过状态包含提升和联合多步视觉-动作预测损失，取得了更好的任务性能和预测能力，证明了该设计组合的有效性。</li>
<li><strong>视觉表征选择</strong>：实验对比了Flow和DynaMo两种特征，表明方法兼容不同的视觉表征，且性能相当。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>统一行为模型（UBM）</strong> 框架，将灵巧技能表征为机器人与环境耦合的动态系统，从构造上确保时间连贯性，并支持全时域标称动作和视觉特征流的预测。</li>
<li>提出了<strong>Koopman-UBM</strong>，作为UBM的第一个实例化，利用Koopman算子理论和状态包含提升，学习一个联合视觉-本体感知潜空间中的线性动态，实现了高效、可靠的隐式视觉运动规划。</li>
<li>通过<strong>全面的实验评估</strong>（7个模拟、2个真实任务）证明，K-UBM在性能上匹配或超越最先进基线，同时具备显著更快的推理速度、平滑的执行、对遮挡的鲁棒性以及灵活的重规划能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文指出，仅依赖于从初始条件进行的开环执行有一个相当大的限制：无法考虑执行过程中未预料的外部扰动。为此，作者引入了基于预测监控的事件触发重规划策略来弥补。但该策略的阈值需要设定，且重规划本身基于更新后的初始状态再次进行开环预测。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>新范式</strong>：UBM框架提供了一种将技能建模为动态系统而非反应式策略的新范式，强调了机器人与环境耦合动态的重要性。</li>
<li><strong>规划与反应的统一</strong>：Koopman-UBM及其重规划机制展示了一种统一规划（长时域连贯预测）与反应（基于监控的触发式调整）的可行路径，避免了固定分块的僵化权衡。</li>
<li><strong>技术工具的应用</strong>：成功将Koopman算子理论应用于高维视觉运动技能学习，为在机器人学习中利用动态系统理论工具提供了有力案例。未来可探索其他非线性动态线性化或简化技术。</li>
<li><strong>表征学习</strong>：方法性能依赖于有效的视觉特征提取（如Flow, DynaMo），这表明未来结合更强大、更具泛化能力的视觉表征学习将进一步提升此类方法的潜力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多指灵巧操作中现有方法（如基于扩散或变换器的策略）缺乏时间连贯性和适应性的问题，提出了统一行为模型（UBMs）框架，其首个实例化Koopman-UBM利用Koopman算子理论，将技能建模为视觉流与动作流协同演化的线性动态系统，实现隐式规划和在线事件触发重新规划。实验在七个模拟和两个真实任务中表明，K-UBM匹配或超越最先进基线，同时提供更快推理、平滑执行、遮挡鲁棒性和灵活重新规划优势。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.07413" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>