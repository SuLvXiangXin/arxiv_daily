<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.16866" target="_blank" rel="noreferrer">2601.16866</a></span>
        <span>作者: Daniele Nardi Team</span>
        <span>日期: 2026-01-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>深度强化学习（DRL）是解决机器人控制等复杂序贯决策问题的强大框架，但其实际部署常因学习所需的大量经验（即智能体与环境的交互）而受阻，导致高昂的计算和时间成本。当前，利用仿真环境可以缓解这一问题，但所需经验总量巨大的根本问题依然存在。本文针对DRL的样本效率低下这一具体痛点，提出将语义知识以知识图谱嵌入（KGEs）的形式整合到学习过程中，为智能体提供环境上下文信息，从而提升学习效率。其核心思路是：从预构建的知识图谱中提取场景相关的语义嵌入，将其与智能体从视觉观察中学习到的特征相结合，以补充和增强智能体对环境的理解，从而加速学习并提高任务精度。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法旨在将环境语义知识整合到DRL智能体的学习架构中。整体流程如<strong>图1</strong>所示：首先，基于智能体所处的环境，从一个完整的知识图谱中通过子图选择器提取出相关实体和关系，构成一个子图；然后，对该子图进行嵌入表示；最后，在DRL智能体架构中，将这些嵌入向量与来自视觉观察层的隐藏激活值在策略近似模块之前进行拼接。</p>
<p><img src="https://arxiv.org/html/2601.16866v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：提出的框架图。使用子图选择器基于环境从完整图谱中提取重要实体和关系。该子图被嵌入，并在DRL智能体架构的策略近似模块之前的层中与视觉特征进行拼接。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>知识图谱与嵌入获取</strong>：环境语义信息存储在一个知识图谱𝒢中，由（头实体，关系，尾实体）三元组构成（例如：(mug, hasColor, yellow)）。为了获取与当前场景相关的语义表示，定义一个操作符Γ，从大图谱𝒢中选择一个子图𝒮，该子图包含智能体感知到的所有对象节点及其距离为1的相邻节点。然后，通过连接子图中节点和边的标签生成一个文本句子，并使用预训练的GloVe模型将该句子转换为一个单一的、固定维度的嵌入向量（通常为150维，当应用域随机化时为300维），以捕获丰富的语义信息。</p>
</li>
<li><p><strong>DRL框架与智能体架构</strong>：采用异步优势演员-评论家（A3C）算法作为基础DRL框架。智能体的具体架构如<strong>图3</strong>所示。</p>
<ul>
<li><strong>视觉处理层</strong>：输入为64x64x3的RGB图像。经过两个卷积层（CNN1和CNN2）进行特征提取，后接ReLU激活函数。</li>
<li><strong>特征融合与策略近似</strong>：卷积特征经一个全连接层（FC，1152→128维）进一步抽象。<strong>关键创新点在于</strong>，将前述获得的KGE向量在此处与FC层的输出进行拼接。拼接后的特征送入一个长短期记忆网络（LSTM），其大小根据是否拼接KGE以及KGE的维度进行调整（无KGE时为128，有完整KGE时最大为428）。</li>
<li><strong>输出层</strong>：LSTM的输出分别流向演员（Actor）和评论家（Critic）。演员部分由n个全连接层组成（n为机器人关节数，TIAGo为7，IRB120为6），每个输出一个7维数组，对应每个关节的7个离散动作的概率（通过Softmax获得）。评论家部分是一个单一的全连接层，用于估计状态值V(s)。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.16866v1/x3.png" alt="网络架构"></p>
<blockquote>
<p><strong>图3</strong>：提出的用于在DRL智能体学习过程中集成KGEs的A3C架构。KGE的大小取决于编码的特征数量。输出分为7个（TIAGo）或6个（IRB120）演员以及评论家值。</p>
</blockquote>
<p><strong>与现有方法的创新点</strong>：与仅使用原始视觉输入的基线方法相比，本文的创新在于设计了一个能够<strong>无缝集成静态语义知识嵌入</strong>的DRL架构。这些KGEs在训练期间保持不变，提供了关于任务目标（如物体类型、颜色、材质等）的上下文先验知识，从而引导智能体更高效地学习策略，而无需增加训练过程的计算复杂度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>任务与平台</strong>：任务为机械臂到达随机放置在 workspace 中的三个目标物体（马克杯、瓶子、谷物盒）的抓取点。在MuJoCo仿真平台中使用两种机器人（TIAGo和IRB120）进行实验。</li>
<li><strong>基准方法</strong>：<ul>
<li><strong>基线模型（BM）</strong>：仅使用RGB图像输入，无KGE。</li>
<li><strong>部分KGE模型</strong>：KGE仅包含物体类型信息（如“mug”）。</li>
<li><strong>完整KGE模型</strong>：KGE包含物体类型及颜色等完整属性信息。</li>
</ul>
</li>
<li><strong>实验分组</strong>：分为两组，第一组目标颜色固定；第二组应用域随机化（DR），每个目标有两种可能的颜色，在每回合随机选择。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>无域随机化环境</strong>：对于TIAGo机器人，训练曲线（<strong>图4</strong>）显示，集成完整KGE的智能体学习速度最快，且保持更高的回报。其最佳模型在4800万步达到，准确率72%；而部分KGE模型在3600万步达到70%准确率；基线模型则在6000万步达到60%准确率。使用KGE带来了12个百分点的准确率提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.16866v1/x4.png" alt="TIAGo固定颜色训练曲线"></p>
<blockquote>
<p><strong>图4</strong>：TIAGo在目标颜色固定环境下的训练曲线。蓝色曲线为完整KGE模型，橙色为部分KGE模型，绿色为基线模型（BM）。完整KGE模型收敛更快，最终性能更高。</p>
</blockquote>
<p>对于IRB120机器人，结果（<strong>图5</strong>）类似，完整KGE模型在约1500万步后回报趋于稳定，而基线模型在约3500万步后才达到类似水平，表明学习时间显著减少。</p>
<p><img src="https://arxiv.org/html/2601.16866v1/x5.png" alt="IRB120固定颜色训练曲线"></p>
<blockquote>
<p><strong>图5</strong>：IRB120在目标颜色固定环境下的训练曲线。完整KGE模型（蓝色）比基线模型（绿色）更快达到稳定高回报。</p>
</blockquote>
<ol start="2">
<li><strong>域随机化环境</strong>：在更复杂的颜色随机化环境下，KGE带来的优势更加明显。<strong>图7</strong>的TIAGo训练曲线显示，完整KGE模型（蓝色）的学习速度远超部分KGE（橙色）和基线模型（绿色）。后两者甚至难以有效学习。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.16866v1/x7.png" alt="TIAGo域随机化训练曲线"></p>
<blockquote>
<p><strong>图7</strong>：TIAGo在目标颜色随机化环境下的训练曲线。完整KGE模型能有效学习，而部分KGE和基线模型学习困难，凸显了完整语义信息在复杂环境中的关键作用。</p>
</blockquote>
<p><strong>定量结果总结</strong>：论文指出，本文方法实现了高达<strong>60%的学习时间减少</strong>，并将任务准确率提升了约<strong>15个百分点</strong>，且没有增加训练时间或计算复杂度。</p>
<ol start="3">
<li><strong>消融实验与定性分析</strong>：通过比较不同KGE配置的性能，验证了语义信息完整性的重要性。完整KGE始终优于部分KGE，而部分KGE又优于无KGE的基线。此外，论文通过分析评估回合中各个关节角度的分布（<strong>图10、11</strong>等）来研究KGE对智能体探索-利用能力的影响。结果显示，集成KGE的智能体关节角度分布更集中，表明其策略更具确定性和高效性，减少了不必要的探索。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.16866v1/x10.png" alt="关节角度分布示例"></p>
<blockquote>
<p><strong>图10</strong>：TIAGo在固定颜色环境下，各关节在评估回合中的角度分布。集成KGE的模型（b，c）比基线模型（a）的分布更集中，表明策略更确定。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的DRL智能体架构，成功地将知识图谱嵌入（KGEs）集成到学习过程中，通过将语义嵌入与视觉特征在策略近似层前拼接，使智能体能利用环境上下文知识。</li>
<li>提出了一套量化方法，用于评估KGEs在时间和准确率方面对DRL智能体性能的改进，并通过与基线及部分知识模型的对比实验，验证了语义信息完整性的重要性。</li>
<li>通过分析智能体关节运动的分布，从探索与利用的角度，对KGE如何影响智能体行为提供了深入的定性分析。</li>
</ol>
<p><strong>局限性</strong>：论文在第七节指出，本方法的局限性包括：知识图谱的构建和更新可能依赖人工且耗时；嵌入过程可能丢失部分图结构信息；以及集成KGE会增加智能体网络架构的参数数量（如LSTM层尺寸增大）。</p>
<p><strong>启示</strong>：这项工作展示了轻量级语义知识表示（相对于大型语言模型）在提升DRL样本效率方面的巨大潜力。它为在机器人学习中融合结构化先验知识提供了可行路径，未来可探索动态更新知识图谱、结合更复杂的图神经网络（GNN）来保留结构信息，以及将方法扩展到更长期、更复杂的操作任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对深度强化学习在机器人操作任务中样本效率低、训练成本高的问题，提出一种融合语义知识的新方法。核心技术是将知识图谱嵌入与视觉观测相结合，为智能体提供环境上下文信息，从而提升学习效率。实验结果表明，该方法在机器人操作环境中，能将学习时间减少高达60%，并将任务准确率提升约15个百分点，且未增加额外的训练时间或计算复杂度。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.16866" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>