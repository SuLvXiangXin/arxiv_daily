<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RuN: Residual Policy for Natural Humanoid Locomotion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RuN: Residual Policy for Natural Humanoid Locomotion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.20696" target="_blank" rel="noreferrer">2509.20696</a></span>
        <span>作者: Li, Qingpeng, Zhu, Chengrui, Wu, Yanming, Yuan, Xin, Zhang, Zhen, Yang, Jian, Liu, Yong</span>
        <span>日期: 2025/09/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人实现自然、动态的跨速度（包括从行走到跑步的平滑过渡）运动是一项重大挑战。目前，基于深度强化学习（DRL）的主流方法通常要求策略直接跟踪参考运动，迫使单个策略同时学习运动模仿、速度跟踪和稳定性维持。这种直接跟踪策略带来了显著的学习复杂度挑战，即策略必须同时满足模仿运动先验的动觉风格、保持动态稳定性和执行任务命令这三个时常相互冲突的目标。这种紧密耦合使策略暴露于巨大而复杂的动作空间中，延长了训练时间，并且目标间的内在冲突限制了性能，迫使在运动质量、稳定性和任务精度之间做出权衡。</p>
<p>本文针对上述痛点，提出了一种新的解耦残差学习框架RuN。其核心思路是将复杂的端到端控制任务分解为两个更易于处理的子模块：一个预训练的条件运动生成器提供运动学上自然的运动先验，另一个强化学习策略学习轻量级的残差修正以处理动态交互。</p>
<h2 id="方法详解">方法详解</h2>
<p>RuN框架将端到端的运动控制问题分解为两个核心模块：一个自回归的条件运动生成器和一个残差策略。整体流程包含三个阶段：运动重定向、条件运动生成器训练和残差策略训练。</p>
<p><img src="https://arxiv.org/html/2509.20696v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：RuN框架概述。(a) 运动重定向：通过优化、过滤和增强，将原始人类运动转换为运动学上可行的参考数据集。(b) 条件运动生成器：在整理的数据集上训练一个自回归CMG作为运动先验，根据用户命令生成自然的运动学轨迹。(c) 残差策略训练：通过强化学习训练一个残差策略学习动态修正，这些修正被添加到CMG的输出中，以确保在物理模拟器中稳定且面向任务的执行。</p>
</blockquote>
<p><strong>1. 运动重定向</strong><br>使用大规模人类运动数据集AMASS（约40小时），其运动由SMPL模型参数化。由于机器人与人类之间存在固有的运动学差异，采用基于优化的运动重定向方法（遵循HOVER），将SMPL参数调整至符合机器人运动学约束。随后进行数据过滤（选择物理上合理且机器人形态可行的行走和跑步序列）和数据增强（水平镜像），最终得到一个包含400个高质量运动序列的参考数据集。</p>
<p><img src="https://arxiv.org/html/2509.20696v1/x3.png" alt="速度分布"></p>
<blockquote>
<p><strong>图3</strong>：经过过滤和增强后，我们整理的运动数据集的纵向速度分布。</p>
</blockquote>
<p><strong>2. 条件运动生成器</strong><br>CMG是一个自回归模型，其核心目标是高效地在线学习并生成数据驱动的自然运动。具体而言，CMG基于K帧运动历史 (\boldsymbol{m}<em>{t-K+1:t}) 和外部条件命令 (\boldsymbol{c}</em>{t})（如目标线速度）来预测下一帧的运动特征 (\hat{\boldsymbol{m}}<em>{t+1})。运动特征 (\boldsymbol{m}</em>{t}) 定义为关节位置和速度的拼接。模型训练采用均方误差损失。</p>
<p>网络架构灵感来源于Motion VAE的高效解码器设计，并采用混合专家架构，由一个门控网络和一个3层MLP主干组成。关键创新在于，CMG完全摒弃了VAE中的随机潜变量和整个编码器结构，这不仅大幅减少了参数量，还从根本上避免了VAE架构常见的训练不稳定和后验坍塌问题。为确保高效鲁棒的训练，采用了课程采样策略以增强模型对自回归展开中误差累积的鲁棒性，并针对数据稀疏性（如高速运动）实施了基于目标速度分布直方图的加权数据采样。实践中，历史长度K设为1，因为包含关节位置和速度的运动特征向量已包含足够的动态信息。</p>
<p><img src="https://arxiv.org/html/2509.20696v1/fig/cmg.jpg" alt="CMG生成结果"></p>
<blockquote>
<p><strong>图4</strong>：我们训练好的条件运动生成器生成的运动序列可视化。该模型响应不同的前进速度命令（1.0 m/s, 1.5 m/s, 2.0 m/s, 和 2.5 m/s），产生了从行走到跑步过渡的运动学上合理的步态。</p>
</blockquote>
<p><strong>3. 残差策略</strong><br>CMG以开环方式生成运动学上合理的运动，但缺乏对物理动态的闭环反馈。为此，引入一个通过强化学习训练的残差策略来提供实时动态修正。控制任务被形式化为一个部分可观测马尔可夫决策过程，并使用近端策略优化算法进行训练。</p>
<p><strong>核心创新：解耦控制与残差动作</strong><br>与端到端方法不同，RuN利用CMG作为强大的运动先验来解耦控制问题。该框架将运动的动觉风格（由CMG的参考姿态 (\boldsymbol{q}<em>{\text{ref},t}) 决定）与其底层动力学和稳定性（由学习的残差策略 (\pi</em>{\theta}(\boldsymbol{a}<em>{\text{res},t}|\boldsymbol{o}</em>{t})) 管理）分离开来。最终发送给机器人底层PD控制器的位置目标 (\boldsymbol{q}<em>{\text{target},t}) 是参考姿态与策略输出的修正动作之和：(\boldsymbol{q}</em>{\text{target},t} = \boldsymbol{q}<em>{\text{ref},t} + \boldsymbol{a}</em>{\text{res},t})。这种残差形式极大地缩小了策略的探索空间。</p>
<p><strong>非对称状态与观测空间</strong><br>采用非对称演员-评论家架构。演员（策略）的观测 (\boldsymbol{o}<em>{t}) 仅包含本体感知和任务相关信息。评论家（价值函数）则可以访问包含特权信息的完整状态 (\boldsymbol{s}</em>{t})，如真实系统状态和CMG的意图（(\boldsymbol{m}<em>{t} \triangleq [\boldsymbol{q}</em>{\text{ref},t}, \dot{\boldsymbol{q}}_{\text{ref},t}])）。这种信息不对称迫使策略仅使用板载传感器反馈来学习跟踪和稳定CMG规定的潜在运动，从而促进了更鲁棒的控制策略。</p>
<p><strong>奖励设计</strong><br>总奖励 (r_t) 由运动模仿奖励 (r^{\text{imitation}}_t)、任务奖励 (r^{\text{task}}_t) 和正则化惩罚 (r^{\text{reg}}_t) 三部分组成。模仿奖励（包括关节位置和速度跟踪）确保了最终运动的自然性；任务奖励驱动机器人执行用户的速度命令；正则化惩罚则鼓励物理上合理、稳定、平滑的运动，并抑制高扭矩、足部滑移等不良现象。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Unitree G1人形机器人上进行，使用NVIDIA Isaac Lab仿真平台进行训练。控制频率为50Hz。策略跟踪用户指定的命令向量 (\boldsymbol{c}_{t} = [v_x, v_y, ω_z])，其中纵向速度 (v_x ∈ [0, 2.5]) m/s。训练中采用了广泛的领域随机化以减少仿真到现实的差距。</p>
<p><strong>评估指标</strong>包括：衡量运动自然度的弗雷歇初始距离（FID，越低越好）、重建损失（(\mathcal{L}<em>{\text{rec}})，越低越好）、模仿误差（关节位置误差 (E</em>{qpos}) 和速度误差 (E_{qvel})，越低越好）以及速度跟踪误差（(E_{vel})，越低越好）。</p>
<p><strong>对比基线</strong>：Humanoid-Gym（无运动先验的RL基线）、AMP（使用对抗判别器的模仿学习）、GMP（使用预训练生成模型作为先验的最先进方法）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>运动生成器对比</strong>（表III）：与GMP相比，CMG取得了更低的FID（0.5283 vs. 0.6637）和重建损失（0.1556 vs. 0.2514），表明CMG能产生更自然、更准确的运动。</li>
<li><strong>策略性能对比</strong>（表IV）：RuN在运动自然度（FID: 0.8753）和模仿精度（(E_{qpos}): 3.8321, (E_{qvel}): 36.7820）上均显著优于所有基线方法。虽然Humanoid-Gym的速度跟踪误差略低（0.2732），但其运动极不自然（FID高达3.8654）。RuN在任务性能和运动自然度之间取得了出色的平衡。</li>
<li><strong>训练效率</strong>：如图5所示，RuN不仅获得了更高的最终奖励，而且收敛速度明显快于其他方法。这归因于残差学习机制将探索空间限制在了更有效的区域。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.20696v1/x4.png" alt="性能对比"></p>
<blockquote>
<p><strong>图5</strong>：不同算法的性能对比。此图显示了训练时间内平均回合奖励的变化。曲线经过平滑处理以便清晰可视化。</p>
</blockquote>
<p><strong>消融实验</strong>（表V）：</p>
<ul>
<li><strong>RuN w/o CMG</strong>：移除CMG模块，策略在没有参考运动先验的情况下训练。结果导致所有指标严重退化，证明了高质量运动先验的重要性。</li>
<li><strong>RuN w/o Residual</strong>：将残差学习机制替换为跟踪策略（类似于GMP）。性能在FID、模仿误差和速度跟踪误差上均显著下降，验证了残差公式的有效性。</li>
<li><strong>RuN w/o AAC</strong>：移除非对称演员-评论家机制，向评论家提供与演员相同的有限观测。这导致了明显的性能下降，突出了AAC在促进鲁棒性方面的作用。</li>
</ul>
<p><strong>真实世界部署</strong>：<br><img src="https://arxiv.org/html/2509.20696v1/fig/walk2run_2.jpg" alt="真实部署"></p>
<blockquote>
<p><strong>图6</strong>：我们策略在Unitree G1机器人上部署的快照。机器人在现实世界中展示了自然稳定的运动，平滑地从行走过渡到跑步。</p>
</blockquote>
<p>训练好的策略被直接部署到真实的Unitree G1机器人上，成功实现了在0-2.5 m/s连续速度范围内的自然行走和跑步，并伴有平滑的步态过渡。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的<strong>解耦残差学习框架RuN</strong>，通过将动作分解为运动先验和学习到的残差，简化了人形机器人运动控制，带来了最终性能的显著提升。</li>
<li>设计了一个新颖的<strong>条件运动生成器</strong>，能够自回归地生成从行走到跑步的一系列自然运动，为残差策略提供了高质量的运动先验。</li>
<li>在Unitree G1人形机器人上进行了广泛的仿真和现实试验，证明RuN在运动自然度和速度跟踪方面优于最先进的方法，并实现了高效的训练和成功的仿真到现实迁移。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于大规模高质量的人类运动数据集（AMASS）和有效的运动重定向流程。此外，尽管采用了领域随机化，仿真到现实的差距仍然是需要持续应对的挑战。</p>
<p><strong>启示</strong>：RuN的成功表明，将复杂的机器人控制任务解耦为“生成式先验（负责风格与可行性）”和“轻量级残差策略（负责动态适应与鲁棒性）”是一种行之有效的范式。这种思路可以推广到其他需要兼具自然性与鲁棒性的机器人技能学习任务中。同时，CMG摒弃VAE复杂结构、采用更高效稳定架构的设计，也为将生成式模型应用于实时机器人控制提供了有价值的参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在宽速度范围内（从走到跑）实现自然、动态步态及平滑过渡的难题，提出RuN框架。该方法采用解耦的残差学习思路：预训练的条件运动生成器提供运动先验，强化学习策略则学习轻量级的残差校正，以处理动力学交互，从而简化了策略学习。在Unitree G1机器人上的实验表明，RuN能在0–2.5 m/s速度范围内实现稳定自然的步态与平滑的走跑过渡，在训练效率和最终性能上均优于现有方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.20696" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>