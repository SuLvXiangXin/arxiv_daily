<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.14828" target="_blank" rel="noreferrer">2510.14828</a></span>
        <span>作者: Liu, Jinrui, Nie, Bingyan, Li, Boyu, Chen, Yaran, Wang, Yuze, He, Shunsen, Li, Haoran</span>
        <span>日期: 2025/10/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于监督微调（SFT）的视觉语言模型（VLM）已成为机器人具身任务规划的主流方法。然而，这种方法存在关键局限性：首先，它主要模仿专家演示，缺乏在动态环境中的适应和自我纠正机制；其次，SFT范式缺乏反馈和纠错信号，导致模型倾向于记忆答案而非学习可泛化的推理策略，难以缓解长视野任务中局部错误的累积。此外，现有的奖励函数设计往往稀疏或与具体动作执行不匹配，阻碍了规划性能的提升。</p>
<p>本文针对上述痛点，提出了一个新视角：将强化学习（RL）引入具身规划任务，以解决SFT在泛化、任务理解、空间感知和规划一致性方面的不足。本文的核心思路是提出一个两阶段微调框架RoboGPT-R1，先通过SFT获取基础知识和推理能力，再利用基于分组相对策略优化（GRPO）的RL训练来增强模型的视觉空间理解和多步推理一致性，并为此设计了一个兼顾结构完整性与动作序列顺序的规则化奖励函数。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboGPT-R1的整体框架是一个两阶段训练范式。第一阶段为监督微调（SFT），输入是包含数学、具身任务和视觉理解数据的SFT数据集，输出是具备基础具身推理和规划能力的模型。第二阶段为基于GRPO的强化学习微调，输入是增强后的RFT数据集，模型通过采样多个候选响应，并根据设计的奖励函数计算优势进行策略优化，最终输出一个在复杂任务和环境中有更强推理能力的模型。</p>
<p><img src="https://arxiv.org/html/2510.14828v2/figs/main3_01.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RoboGPT-R1方法整体框架。左侧展示了两个训练阶段：初始的监督微调（SFT）阶段为模型注入数学、具身任务和视觉理解的基础知识；后续的强化学习（RL）阶段使用基于GRPO的策略优化和定制的奖励函数进行微调。右侧展示了模型在六个任务类别上的评估，包括长视野规划和空间推理。</p>
</blockquote>
<p>核心模块是两阶段训练方案和专为具身规划设计的奖励函数。第一阶段（SFT）使用少量高质量数据（从Gemini-2.0-flash蒸馏而来）进行预热，旨在让基础模型快速掌握一定的具身推理知识和规划能力，为后续RL训练提供稳定的起点。实验发现，采用零样本（0-shot）处理训练和测试数据能获得更好性能，并减少约三分之一的输入token。</p>
<p>第二阶段（GRPO）是增强推理的关键。GRPO算法无需额外的批评者模型，它通过从策略中采样N个候选响应，利用奖励函数R(q, o_i)评估每个响应，并通过公式(1)计算其相对于组内其他样本的优势A_i。优化目标如公式(2)所示，包含鼓励高优势响应的项以及一个与参考策略的KL散度惩罚项，以防止策略偏离过大，其中ℓ1和ℓ2的定义如公式(3)。</p>
<p>与现有方法相比，本文的核心创新点在于为具身规划任务设计的奖励函数。该奖励函数由格式奖励和准确度奖励两部分组成。</p>
<p><img src="https://arxiv.org/html/2510.14828v2/figs/Overall_Reward-red.png" alt="奖励函数组成"></p>
<blockquote>
<p><strong>图2</strong>：整体奖励函数构成。总奖励R是格式奖励R_format（权重0.2）和基于最长公共子序列（LCS）的准确度奖励R_lcs（权重0.8）的加权和。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.14828v2/figs/Format+Accuracy_Reward.png" alt="格式奖励细节"></p>
<blockquote>
<p><strong>图3</strong>：格式奖励与准确度奖励细节。左侧展示了格式奖励的三个组成部分：区块奖励、类型奖励和有效性奖励。右侧对比了不同准确度奖励的计算方式，突出本文采用的LCS奖励能更好地保留动作顺序。</p>
</blockquote>
<p>格式奖励R_format（公式4）旨在确保输出结构符合具身规划的“感知-推理-规划-动作”闭环，包含三个子部分：1) <strong>区块奖励R_section</strong>（公式5）：检查输出是否包含<code>visual_state_description</code>、<code>reasoning_and_reflection</code>、<code>language_plan</code>、<code>executable_plan</code>这四个必需字段且类型正确；2) <strong>类型奖励R_type</strong>（公式6）：检查每个动作步骤的ID是否为整数、名称是否为非空字符串且非空；3) <strong>有效性奖励R_validity</strong>（公式7）：检查每个动作ID-名称对是否与预定义但动态变化的动作字典匹配。与REBP使用固定动作ID不同，本文使用动态ID以防止模型死记硬背。</p>
<p>准确度奖励基于预测动作序列与参考序列之间的<strong>最长公共子序列（LCS）</strong>计算（公式8, 9）。奖励值R_lcs等于LCS长度k除以参考序列长度n。LCS奖励能同时评估动作内容的正确性和执行顺序的一致性，对长视野任务中局部偏差但整体恢复的情况更为鲁棒，优于前缀匹配或逐步匹配等传统方法。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在EmbodiedBench基准上进行，该基准包含EB-ALFRED（源自ALFRED，侧重家庭指令跟随任务）和EB-Habitat（源自Habitat，侧重3D环境导航与交互）两个测试套件。前者被视为已见场景（in-domain），后者被视为未见场景（out-of-domain），用于评估泛化能力。评估涵盖基础、常识、复杂、视觉、空间和长视野六类任务。</p>
<p>对比的基线方法分为三类：1) <strong>闭源通用模型</strong>：如Gemini-2.0-flash, GPT-4.1, GPT-4o, GPT-4o-mini, Qwen-VL-Max；2) <strong>开源通用模型</strong>：如Qwen2.5-VL系列、Llama-3.2-90B-Vision、InternVL2.5-8B等；3) <strong>具身领域专用模型</strong>：如REBP、RoBoBrain、TaPa。</p>
<p><strong>表1</strong>：不同模型在EB-ALFRED和EB-Habitat上的成功率（%）。灰色背景标示最佳性能。<br>（<em>注：此处应以文本形式概括表1关键数据，因解读要求基于原文，不直接复制表格，故用文字总结</em>）</p>
<p>关键实验结果如下：在已见的EB-ALFRED场景中，基于Qwen2.5-VL-3B的RoboGPT-R1取得了<strong>64.67%<strong>的平均成功率，显著超越了参数量更大的GPT-4o-mini（34.00%），提升幅度达</strong>21.33%<strong>。与同为领域专用模型的SOTA方法REBP（基于Qwen2.5-VL-7B）相比，RoboGPT-R1也实现了</strong>20.33%<strong>的领先。在更具挑战性的</strong>长视野任务</strong>上，RoboGPT-R1达到了**50%<strong>的准确率，展现了其出色的多步推理能力。与闭源模型相比，其性能与GPT-4o（51.67%）和Gemini-2.0-flash（52.30%）相当。在未见的EB-Habitat场景中，RoboGPT-R1的泛化能力同样优秀，平均成功率为</strong>50.67%**，超越了GPT-4o-mini（35.00%），并与GPT-4o（57.00%）等表现接近，显著优于之前的SOTA模型。</p>
<p><img src="https://arxiv.org/html/2510.14828v2/figs/success_rate_by_stage2.png" alt="训练阶段效果"></p>
<blockquote>
<p><strong>图4</strong>：两阶段训练中各阶段模型在EB-ALFRED测试集上的成功率对比。结果显示，经过第二阶段RL训练后，模型在几乎所有任务类别上的性能均得到显著提升，验证了RL阶段的有效性。</p>
</blockquote>
<p>消融实验总结了各组件贡献：1) <strong>两阶段训练的有效性</strong>：图4清晰表明，仅使用SFT（Stage 1）的模型性能有限，而经过RL（Stage 2）微调后，各项任务成功率大幅提升。2) <strong>LCS奖励的优势</strong>：与REBP使用的前缀准确度奖励相比，本文设计的LCS奖励在长视野和多步规划任务中表现出更强的性能，因为它能更好地保留动作顺序并对局部错误更鲁棒。3) <strong>零样本数据处理的优势</strong>：实验发现，采用0-shot而非n-shot进行训练和测试，能避免模型僵化地记忆模板，获得更优性能并提升训练效率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了<strong>RoboGPT-R1两阶段训练范式</strong>，将GRPO强化学习引入具身规划，有效增强了模型在复杂任务中的推理能力、物理常识和纠错能力。2) 设计了一个<strong>基于感知-推理-规划-动作闭环的规则化奖励函数</strong>，其中创新的LCS准确度奖励加强了对动作序列顺序的约束，提升了长视野任务性能。3) 在EmbodiedBench基准上取得了<strong>具有竞争力的结果</strong>，使用仅3B参数的小模型超越了多个大尺度通用模型和现有具身规划SOTA模型。</p>
<p>论文自身提到的局限性在于：在未见的EB-Habitat场景中，尽管平均表现良好，但在“复杂”任务类别上的成功率（38%）仍有较大提升空间，表明模型对于分布外复杂指令的泛化能力有待进一步加强。</p>
<p>本文对后续研究的启示在于：1) 证明了<strong>强化学习是提升小规模VLM具身推理能力的有效途径</strong>，为“模仿学习”到“涌现智能”的转变提供了案例。2) <strong>奖励函数的设计至关重要</strong>，针对具身任务特性（如动作顺序、结构约束）设计密集、可解释的奖励信号，能更有效地引导模型学习。3) <strong>两阶段训练策略</strong>（先SFT预热，后RL优化）有助于稳定训练并快速整合领域知识，可迁移至其他需要复杂推理的视觉语言任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型（VLMs）在复杂真实环境中执行长期操作任务时，因监督微调（SFT）泛化能力差、物理理解不足而规划能力受限的核心问题，提出RoboGPT-R1两阶段微调框架。该方法先通过SFT从专家序列学习基础知识，再引入强化学习（RL）增强视觉空间理解和推理，并设计基于规则的奖励函数以平衡长期性能与动作约束。实验表明，在EmbodiedBench基准上，基于Qwen2.5-VL-3B训练的模型性能比GPT-4o-mini提升21.33%，较其他基于Qwen2.5-VL-7B的工作提升20.33%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.14828" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>