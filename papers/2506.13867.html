<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.13867" target="_blank" rel="noreferrer">2506.13867</a></span>
        <span>作者: Abhishek Gupta Team</span>
        <span>日期: 2025-06-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉运动策略学习通常依赖于预训练的视觉表征来提高样本效率和鲁棒性。然而，即使经过预训练，这些策略在面对训练环境与评估环境之间的视觉差异（如干扰物、物体变化、光照变化）时，仍可能表现脆弱，难以广泛部署。另一方面，基于状态估计（如6D姿态）的策略需要任务特定的跟踪且难以扩展，而基于原始传感器的策略可能对小的视觉扰动缺乏鲁棒性。</p>
<p>本文针对上述感知挑战，提出使用2D关键点——图像帧中空间一致的特征点——作为一种灵活的、鲁棒的策略学习状态表示，并将其应用于模拟到现实（sim-to-real）转移和真实世界模仿学习。然而，关键点的选择会因物体和任务的不同而变化。本文的核心痛点是：如何自动地、以任务驱动的方式选择关键点，使得所选的关键点能够预测给定任务的最优行为？本文的核心思路是：提出一种名为ATK的新方法，通过一个基于掩码的架构，利用专家数据（来自模拟中的专家策略或人类专家）联合选择一组最小的任务相关关键点，并训练一个基于这些关键点的条件策略，从而实现鲁棒的策略转移和泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>ATK方法的整体目标是为策略学习提供一个能够实现泛化和鲁棒性的输入表示。其核心是利用2D关键点作为感知表示，并自动筛选出与任务最相关的关键点子集。</p>
<p><img src="https://arxiv.org/html/2506.13867v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ATK方法整体框架。通过蒸馏专家数据（来自模拟专家策略或人类专家）到一个策略中，该策略在选定的关键点子集上运行，并优化选择掩码。一旦关键点被确定，它们将从训练集转移到真实世界评估场景。最终，基于关键点的策略被转移到评估场景，以RGB图像为输入，同时跟踪转移后的关键点。</p>
</blockquote>
<p><strong>关键点作为策略表示</strong>：关键点是图像中用于识别和描述物体或特征的显著位置，定义为2D图像平面上的一个点。一组N个关键点构成了一个紧凑的场景表示。利用在大规模网络数据上训练的鲁棒跟踪算法，可以跨帧维持密集的对应关系，即使存在视觉场景级和实例级的变化。跟踪通过一个对应函数实现，该函数根据当前图像更新关键点位置。</p>
<p><strong>自动任务驱动关键点选择：训练过程</strong>：ATK旨在识别一组最小的、任务相关的关键点，使其既能实现接近最优的策略，又能被可靠地跟踪。选择标准包括：（1）<strong>最优策略的可实现性</strong>：所选关键点必须捕获学习任务接近最优策略所需的所有必要信息；（2）<strong>可跟踪性</strong>：所选关键点必须能够使用可用的对应函数被可靠、一致地跟踪。</p>
<p>训练流程如下：</p>
<ol>
<li><strong>数据准备</strong>：首先确定一个捕获完整任务上下文的规范模板图像。在该图像上随机采样C个候选关键点。利用专家数据集和对应函数，将这些候选关键点传播到所有轨迹和时间步，形成一个关键点注释的数据集。</li>
<li><strong>联合学习</strong>：从该数据集中，联合学习一个稀疏掩码模型和一个下游的基于关键点的策略。如图2所示，M个候选关键点输入到掩码模型中，该模型输出M个独立的伯努利概率，每个概率代表保留相应输入关键点的可能性。采样一个二进制掩码会清零未选中的关键点，产生一个精简的关键点集，然后传递给策略网络以产生动作分布。</li>
<li><strong>优化目标</strong>：为了选择最小的K个关键点，在掩码上施加了一个稀疏信息瓶颈。损失函数旨在最小化动作预测的负对数似然，同时加上掩码的L1范数作为稀疏性正则项。由于掩码采样是离散且不可微的，采用了Gumbel-softmax松弛来实现基于梯度的优化。直观上，该训练过程会过滤掉那些（1）与预测最优动作无关，以及（2）由于随时间变化的表示不可靠而难以跟踪的点。</li>
</ol>
<p><strong>推断过程</strong>：训练后，我们得到了掩码模型和基于关键点的策略。在测试时（无论是sim-to-real转移还是模仿学习场景间的转移），推断过程相同。核心是将训练时选择的最小关键点集转移到视觉多样化的测试场景中。由于关键点跟踪由鲁棒的、网络规模的视觉跟踪器执行，一旦在测试时于真实世界中识别出初始的关键点集，后续的跟踪将不受视觉差距的影响。</p>
<p><img src="https://arxiv.org/html/2506.13867v2/x3.png" alt="推断流程"></p>
<blockquote>
<p><strong>图3</strong>：基于关键点表示的推断循环。在测试时，我们选择与训练规范关键点匹配得分最高的初始图像点，确保准确匹配。这些转移的关键点随后通过对应函数进行跟踪，并直接作为输入传递给已部署的策略，无需在测试时进行额外的掩码操作。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法相比，ATK的创新点主要体现在：1）<strong>任务驱动的自动选择</strong>：摒弃了启发式或手动选择关键点的方式，通过优化过程自动确定对任务最关键的特征点；2）<strong>联合优化框架</strong>：将关键点选择（通过掩码模型）和策略学习整合到一个统一的、端到端的训练框架中，利用专家动作作为监督信号来驱动选择过程；3）<strong>最小化与鲁棒性平衡</strong>：通过稀疏性约束追求最小关键点集，同时确保该集合足以实现最优策略，从而在降低冗余、提升跟踪鲁棒性和保持策略性能之间取得平衡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/任务</strong>：<ul>
<li><strong>模拟到现实转移</strong>：评估了三个精细操作任务：寿司抓放、玻璃壶尖端提起、时钟操作（包含转动按钮和指针两个子任务）。挑战包括跟踪难度、操作精度、任务特定焦点的保持以及场景配置变化。</li>
<li><strong>鲁棒模仿学习</strong>：在一个多功能厨房环境中评估了四个操作任务：葡萄放入微波炉、毯子悬挂、毛巾折叠、平底锅填充。每个任务发生在同一环境中，需要任务特定的表示。</li>
</ul>
</li>
<li><strong>实验平台</strong>：真实世界机器人平台。</li>
<li><strong>Baseline方法</strong>：<ul>
<li><strong>输入模态对比</strong>：基于RGB图像、深度图像、点云训练的策略。</li>
<li><strong>关键点选择方法对比</strong>：使用所有采样关键点的<code>FullSet</code>；随机选择与ATK相同数量关键点的<code>RandomSelect</code>；使用GPT-4根据图像和任务选择相同数量关键点的<code>GPTSelect</code>。</li>
</ul>
</li>
<li><strong>评估</strong>：每个设置在真实世界评估20条轨迹，初始配置各不相同。为评估鲁棒性和泛化能力，引入了干扰：随机物体姿态（RP）、随机背景（RB）、随机干扰物（RO）和光照变化（Light）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟到现实转移</strong>：ATK在现实世界中表现出较高的成功率，并优于其他输入模态（RGB、深度、点云）的策略。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.13867v2/x5.png" alt="模拟到现实结果"></p>
<blockquote>
<p><strong>图5</strong>：模拟到现实策略在真实世界的成功率。左图：跨多种评估条件（随机姿态、背景变化、干扰物、光照变化）的汇总结果表明，ATK优于使用不同输入模态的其他方法。右图：ATK在位置变化和各种视觉扰动下表现出强大的鲁棒性。</p>
</blockquote>
<p>具体而言，在存在透明物体（如玻璃）和精细操作（如时钟任务）的任务中，ATK的优势更为明显。尽管极端干扰可能破坏跟踪并降低性能，但ATK在转移过程中始终优于其他基线。</p>
<ol start="2">
<li><strong>鲁棒模仿学习</strong>：基于ATK选择表示学习的策略，在面对显著的视觉变化时仍能执行任务。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.13867v2/x6.png" alt="模仿学习结果"></p>
<blockquote>
<p><strong>图6</strong>：模仿策略成功率。左图：跨多种评估条件的汇总结果表明，ATK优于基于不同输入模态和选择策略的其他方法。右图：ATK在位置变化和各种视觉扰动下表现出强大的鲁棒性。</p>
</blockquote>
<p>评估时引入了物体位置、背景、干扰物和光照条件的变化。尽管未在这些条件下训练，所学策略的转移性能显著优于其他表示。此外，关键点的特定选择对鲁棒转移性能至关重要：<code>FullSet</code>包含冗余且变化显著的关键点，而<code>RandomSelect</code>和<code>GPTSelect</code>常常遗漏场景的重要部分。</p>
<ol start="3">
<li><strong>关键点可视化与可解释性</strong>：ATK选择的关-键点具有可解释性且与任务相关。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.13867v2/x7.png" alt="关键点可视化"></p>
<blockquote>
<p><strong>图7</strong>：任务相关关键点选择与转移的定性可视化。在模拟中选择的关键点能够转移到真实世界场景中，跨越不同的物体位置、背景、干扰物和光照条件。</p>
</blockquote>
<p>如图7所示，所选关键点聚焦于任务相关的部位，对应于场景中具有语义意义的元素。在多功能场景中，关键点专用于特定功能。这些选择的关键点对场景中的视觉变化（包括干扰物、光照变化和背景变化）具有弹性。</p>
<p><strong>消融实验</strong>：论文通过对比<code>FullSet</code>、<code>RandomSelect</code>、<code>GPTSelect</code>和ATK，验证了自动任务驱动选择的重要性。<code>FullSet</code>因冗余关键点导致性能下降；<code>RandomSelect</code>和<code>GPTSelect</code>因可能遗漏关键信息而表现不佳；ATK通过优化选择最小且足够的任务相关关键点，取得了最佳性能。这说明了ATK中每个组件（任务驱动的选择、稀疏性约束、联合优化）的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种<strong>联合选择最小化任务相关关键点集并学习基于这些关键点的条件策略的方法论</strong>（ATK）。</li>
<li>在多种真实世界机器人操作任务上进行了<strong>实证验证</strong>，证明了该方法在模拟到现实设置中的有效性，以及在存在显著视觉差异的场景中实现鲁棒转移的能力。</li>
<li>展示了所提方法在<strong>模仿学习设置中的效能</strong>， resulting policies在保持高精度灵巧性的同时，表现出强大的视觉泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：1）方法依赖于预训练的关键点检测和跟踪模块的性能。如果这些模块在特定领域（如极度非纹理表面）失效，ATK的性能也会受到影响。2）关键点选择是基于静态的规范图像，对于动态变化非常剧烈的任务，可能需要更复杂的初始化或在线选择机制。3）当前方法主要处理2D关键点，对于需要深度信息的任务，可能需要结合3D信息。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>与更先进跟踪器的结合</strong>：可以探索集成更鲁棒、更快速的视觉跟踪器（如基于Transformer的跟踪器）以进一步提升系统性能和处理动态场景的能力。</li>
<li><strong>动态与在线选择</strong>：当前方法是离线选择关键点。未来工作可以研究如何在策略执行过程中动态调整或重新选择关键点，以应对任务阶段的转变或意外遮挡。</li>
<li><strong>扩展到3D与多模态</strong>：将2D关键点选择范式扩展到3D关键点或结合其他模态（如触觉、力觉）可能有助于解决更复杂的操作任务，特别是那些需要精确接触或力控的任务。</li>
<li><strong>理论分析</strong>：对“最小且足够”的关键点集进行更深入的理论分析，例如其与任务马尔可夫状态的信息论关系，可能为自动状态抽象提供更坚实的理论基础。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决视觉运动策略因训练与测试环境视觉差异导致的性能下降问题。提出ATK方法，通过专家数据自动选择一组能预测最优行为的最小关键点作为状态表示，专注于任务相关部分以保持策略鲁棒性。实验验证表明，该方法能显著提升策略对视觉干扰和环境变化的鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.13867" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>