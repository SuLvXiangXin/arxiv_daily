<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.03895" target="_blank" rel="noreferrer">2510.03895</a></span>
        <span>作者: Chunhua Shen Team</span>
        <span>日期: 2025-10-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过将多模态提示映射到密集、高频的动作序列，在具身智能领域取得了显著进展。然而，这种端到端范式严重依赖对密集动作轨迹的监督，导致了两个关键局限性：一是灾难性遗忘问题，即在对新任务进行微调时，模型会覆盖先前学习到的能力；二是计算成本高昂，密集的轨迹表示需要大量的数据和算力进行训练。这些问题的根源在于，将高级视觉语言模型与低级动作控制器紧密耦合，并过度依赖连续的动作块，形成了孤立的数据孤岛，阻碍了知识的跨任务保留和泛化。</p>
<p>本文针对上述痛点，提出了一个新的视角：将监督的焦点从密集的动作轨迹“收窄”到稀疏的、语义对齐的关键帧轨迹。其核心思路是通过解耦高级VLM与低级动作专家，并利用经过运动学压缩和空间剪枝的稀疏末端执行器轨迹进行训练，从而在缓解灾难性遗忘、降低计算成本的同时，保持甚至提升模型的零样本泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>NoTVLA框架的整体目标是将经典控制器的硬件可操作性与视觉语言模型的泛化能力相结合。其核心在于使用稀疏的、语义对齐的关键帧轨迹作为监督信号，而非传统的密集动作流。训练时，通过运动学关键帧选择方法为VLA模型提供稀疏监督；推理时，过程分为两个阶段：首先，模型通过基于锚点的深度推理生成规划轨迹；随后，该轨迹由基于样条的动作解令牌器处理，以产生平滑、可执行的高频动作序列。</p>
<p><img src="https://arxiv.org/html/2510.03895v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：NoTVLA框架总览。该框架通过用稀疏的、语义对齐的关键帧取代密集动作轨迹，来解决VLA模型中的灾难性遗忘问题。大规模轨迹数据通过基于运动学的关键帧和子关键帧选择进行处理。指令和RGB输入由Qwen VL 2.5编码以预测锚点，锚点与深度查询结合用于生成锚点条件令牌。基于样条的解令牌器将这些离散的动作令牌转换为平滑的高频轨迹，用于闭环机器人控制。</p>
</blockquote>
<p>框架包含三个核心模块：</p>
<ol>
<li><strong>基于锚点的深度推理</strong>：此模块旨在为VLM提供高效、可扩展的深度推理能力。它分为两步：<ul>
<li><strong>锚点预测</strong>：VLM仅接收RGB图像和语言指令，输出一个单一的2D锚点，该锚点是关键帧中机器人末端执行器3D位姿在图像平面上的投影。训练时，该点可监督为物体中心、首次接触像素或任务特定的显著位置。</li>
<li><strong>锚点条件令牌生成</strong>：将深度增强的锚点<code>ANCHOR(ua, va, da)</code>作为条件输入，VLM以自回归方式输出一个包含深度、图像平面坐标、夹爪状态和夹爪姿态（欧拉角）四种模态的轨迹令牌序列。重建的3D路径点使用相机内参计算得到。这种方式将绝对深度预测解耦为2D定位和外部深度查询，降低了VLM的视觉负担。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.03895v1/notvla/Figures/finalimage/NoTvla.png" alt="锚点预测与令牌生成"></p>
<blockquote>
<p><strong>图2</strong>：NoTVLA中的锚点预测与令牌生成。带有RGBD输入的指令产生2D和深度锚点，这些锚点作为条件来生成动作令牌。这些令牌随后被转换为与预测锚点对齐的轨迹，以实现精确操作。</p>
</blockquote>
<ol start="2">
<li><strong>基于运动学的关键帧选择</strong>：为了从原始演示中提取稀疏且信息丰富的监督信号，该方法基于末端执行器的物理特性分割轨迹。关键帧的选择标准是：末端执行器加速度的L2范数超过阈值α，或者夹爪状态发生突变。这确保了在运动状态或操作意图发生关键变化的时刻被保留为关键帧。在两个连续关键帧之间，会插入子关键帧并进行均匀下采样，以在降低训练计算量的同时保持时间连贯性。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.03895v1/notvla/Figures/select_keyframes.png" alt="关键帧选择"></p>
<blockquote>
<p><strong>图3</strong>：基于夹爪姿态的关键帧选择。图像显示了夹爪随时间的运动，关键帧根据左臂和右臂的打开/关闭状态进行选择。蓝线和红线代表夹爪轨迹，并标注了相应的开合状态，突出了物体操作过程中关键姿态之间的转换。</p>
</blockquote>
<ol start="3">
<li><strong>基于样条的动作解令牌器</strong>：为了解决VLM低频令牌输出（~1-2 Hz）与机器人毫秒级平滑控制需求之间的频率差距，该模块将VLM输出的离散路径点序列转换为连续轨迹。具体而言，3D位置使用三次样条插值确保路径平滑，而方向（由欧拉角转换为四元数）则使用球面线性插值在连续四元数之间生成无歧义的旋转轨迹。这种设计保证了生成的高频轨迹在位置和方向上都足够平滑，适合机器人执行，并成功部署在多种机器人平台上。</li>
</ol>
<p>与现有方法相比，NoTVLA的创新点主要体现在：1）<strong>解耦的层次化架构</strong>：明确使用3D路径点作为高级VLM与低级动作专家之间的清晰接口；2）<strong>稀疏的监督信号</strong>：基于运动学而非均匀下采样或学习分割来选择关键帧，保留了任务关键信息；3）<strong>频率桥接机制</strong>：创新的样条解令牌器专门用于弥合规划与控制之间的频率鸿沟。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在多个基准和数据集上进行评估，包括RoboTwin 2.0、AGIBOT Challenge，并在Franka、Aloha–AgileX、PiPER和AGIBOT G1等多种机器人平台上验证。对比的基线方法包括单任务专家模型、通用VLA模型等。</p>
<p><strong>关键实验结果</strong>：<br>在RoboTwin 2.0的多任务评估中，NoTVLA在仅使用头戴式单目RGBD的受限感知设置下，取得了接近单任务专家模型（如ACT）和通用VLA模型（如π0）的成功率，并在一些长视野任务上显示出优势。例如，在“click bell”任务上达到0.94的成功率，优于π0的0.44；“press stapler”任务达到0.94，优于π0的0.62。</p>
<p><img src="https://arxiv.org/html/2510.03895v1/notvla/Figures/test_images/compare.png" alt="训练步骤与成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：不同工作的训练步骤和平均成功率对比。NoTVLA使用7B参数量，接近RDT和π0。气泡大小代表不同工作的单任务训练步数。NoTVLA无需单任务训练，性能仍优于其他模型。</p>
</blockquote>
<p>在AGIBOT Challenge的官方测试中，NoTVLA在10个任务上的总分为3.697，显著优于UniVLA基线的2.795，特别是在“Open drawer and store items”和“Pickup items from the freezer”等任务上优势明显。</p>
<p><strong>零样本泛化能力</strong>：如表3所示，NoTVLA在指令语义反转、引入未见颜色和概念等零样本任务上表现出强大的泛化能力。例如，在训练中只见过“绿块叠在红块上”的指令，测试时面对“红块叠在绿块上”的反转指令，NoTVLA仍能达到70%的成功率，而π0的两个检查点成功率均为0%。</p>
<p><img src="https://arxiv.org/html/2510.03895v1/notvla/Figures/finalimage/command_generalization_small.png" alt="零样本泛化"></p>
<blockquote>
<p><strong>图7</strong>：零样本泛化结果示例。展示了模型对训练中未见过的指令（如颜色反转、新概念“旗子”）的理解和执行能力。</p>
</blockquote>
<p><strong>轨迹规划质量</strong>：与基于规划的Magma方法相比，NoTVLA在覆盖度、动态时间规整距离、弗雷歇距离等多个几何和时间度量上均表现更优，表明其样条解令牌器能产生更准确、一致的轨迹。</p>
<p><img src="https://arxiv.org/html/2510.03895v1/notvla/Figures/finalimage/Agibot.png" alt="轨迹规划比较"></p>
<blockquote>
<p><strong>图5</strong>：轨迹规划对比。展示了NoTVLA生成的轨迹与基线方法在几何形状和时间对齐上的比较。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ol>
<li><strong>深度推理消融</strong>：如表4所示，同时使用锚点预测和锚点条件令牌生成的完整版本性能最佳。在长视野任务中，显式深度输入带来的提升尤为显著，因为它为多次空间推理提供了稳定的参考点。</li>
<li><strong>多视角泛化</strong>：如表5和图6所示，模型在训练中未见过的相机视角下，任务成功率仅有轻微下降，证明了基于锚点的深度推理解耦设计有效增强了视角鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.03895v1/notvla/Figures/finalimage/Multiview_small.png" alt="多视角泛化"></p>
<blockquote>
<p><strong>图6</strong>：多视角泛化示例。模型能够从训练中未见过的相机视角成功执行任务。</p>
</blockquote>
<ol start="3">
<li><strong>训练步数分析</strong>：性能并非随训练步数单调增长，短视野任务可能早期就达到高峰而后过拟合，而长视野任务因关键帧更多、序列建模更复杂，需要更多训练才能提升。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一种解耦的VLA框架，通过稀疏的、基于运动学的关键帧监督，有效缓解了灾难性遗忘，并大幅降低了微调计算成本；2）引入了基于锚点的深度推理和基于样条的动作解令牌器，分别简化了3D感知负担并弥合了规划与控制的频率鸿沟，从而提升了跨任务、跨机器人和跨视角的泛化能力；3）框架保留了底层VLM固有的语言和推理能力，支持复杂的指令跟随和多轮交互，实现了强大的零样本泛化。</p>
<p>论文提到的局限性包括：对落在相机视野外的物体敏感，以及现有基准测试未能充分体现其长视野推理的优势。这些为后续研究指明了方向，例如增强感知系统以处理视野外物体，以及在更复杂、长视野的任务上进行评估，以推动构建可扩展、高效能的通用机器人模型。</p>
<p><img src="https://arxiv.org/html/2510.03895v1/notvla/Figures/finalimage/Realworld.png" alt="真实世界部署"><br><img src="https://arxiv.org/html/2510.03895v1/notvla/Figures/finalimage/Realword1.png" alt="真实世界部署"></p>
<blockquote>
<p><strong>图9 &amp; 图10</strong>：NoTVLA在真实世界多种机器人平台上的部署示例，展示了其跨 embodiment 的泛化能力。</p>
</blockquote>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型因依赖密集连续动作轨迹导致的灾难性遗忘问题，提出NoTVLA框架。该方法通过时间压缩与空间推理剪枝技术，聚焦于机器人末端执行器的稀疏轨迹进行训练，替代传统的密集轨迹微调。实验表明，NoTVLA在多任务评估中性能与泛化能力均优于π0模型，且计算功耗降低一个数量级以上，无需腕部摄像头，在零样本场景下表现更优。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.03895" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>