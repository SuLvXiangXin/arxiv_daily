<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EvoVLA: Self-Evolving Vision-Language-Action Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>EvoVLA: Self-Evolving Vision-Language-Action Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16166" target="_blank" rel="noreferrer">2511.16166</a></span>
        <span>作者: Hao Tang Team</span>
        <span>日期: 2025-11-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过将感知、语言和控制统一到单一的大规模骨干网络中，有望实现通用的机器人策略。尽管在零样本泛化和仿真到现实（Sim2Real）迁移方面取得了进展，但长视野的机器人操作对于VLA模型仍然具有挑战性。现有VLA模型存在“阶段幻觉”问题，即智能体利用粗略的评估信号来走捷径完成多步骤任务，报告高进度但并未实际完成任务。主流方法依赖于稀疏的外部任务奖励或临时的进度分类器，缺乏明确的阶段语义和时间基础，导致在需要数十个语义上不同的阶段和持续状态跟踪的任务中，自主性常常崩溃。此外，强化学习样本效率低下，而现有的自监督强化学习（SSRL）方法中的好奇心机制通常基于像素的新颖性检测，在杂乱场景中容易失效。长视野记忆模块也往往脆弱，大多通过平均或截断来压缩历史，导致灾难性遗忘。</p>
<p>本文针对长视野操作中“阶段幻觉”这一具体痛点，提出了一个自监督VLA框架的新视角。核心思路是：通过融合阶段对齐的奖励、基于几何姿态的好奇心以及带有选择性上下文和门控融合的长视野记忆，提供密集且语义一致的内在反馈，从而抑制幻觉并稳定学习过程，实现高效且鲁棒的长视野策略学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>EvoVLA的整体框架基于OpenVLA-OFT主干网络，并集成了三个协同工作的核心模块：阶段对齐奖励（SAR）、基于姿态的对象探索（POE）和长视野记忆。框架与Discoverse-L基准测试相结合进行训练，并可部署到真实机器人。</p>
<p><img src="https://arxiv.org/html/2511.16166v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图3</strong>：EvoVLA 方法总览。基于OpenVLA-OFT主干，集成了三个阶段对齐奖励（SAR）、基于姿态的对象探索（POE）和带有上下文选择与门控融合的长视野记忆。该框架与Discoverse-L结合进行训练，并部署到真实机器人。</p>
</blockquote>
<p><strong>整体流程与数据引擎</strong>：训练始于一个任务对齐的数据管道。利用Gemini 2.5 Pro对Discoverse-L任务轨迹进行多步提示工作流，为每个阶段生成包含正面、负面和<strong>困难负面</strong>谓词的统一阶段字典。这些语义同时监督仿真训练和Sim2Real部署。</p>
<p><img src="https://arxiv.org/html/2511.16166v1/x1.png" alt="数据引擎"></p>
<blockquote>
<p><strong>图2</strong>：EvoVLA 数据引擎。与Discoverse-L和视频驱动的阶段发现流程对齐，以闭合数据-奖励-策略循环。</p>
</blockquote>
<p><strong>核心模块技术细节</strong>：</p>
<ol>
<li><p><strong>阶段对齐奖励（SAR）</strong>：旨在通过图像-文本对比学习评估阶段完成情况，提供密集奖励。其创新在于引入了<strong>困难负面样本</strong>（由Gemini生成，描述接近完成但失败的状态，如“抓取非目标物体”），以防止策略利用视觉捷径。给定视觉语言模型编码器φ（CLIP），阶段对齐得分计算为当前图像观测与阶段k的正面文本描述相似度，减去其与负面、困难负面文本描述相似度的最大值，再经过Sigmoid函数和温度参数τ缩放。为了稳定奖励，对得分进行时间平滑处理，得到一个滑动平均值。阶段奖励定义为当前活跃阶段平滑得分的进度（当前值与前一时刻值的差值）。当最近<code>r_stage</code>值的滑动窗口超过阈值时，才推进到下一阶段，从而过滤瞬时的分数波动。</p>
</li>
<li><p><strong>基于姿态的对象探索（POE）</strong>：为了解决基于像素的好奇心会因光照变化等无关干扰而产生虚假新颖性的问题，POE将好奇心建立在任务相关的交互动态上，即对象与平行夹爪之间的相对姿态。它将操作状态表示为6维向量<code>z_t</code>（包含相对平移和轴角旋转）。训练轻量级MLP前向模型<code>f_φ</code>和逆向模型<code>g_ψ</code>来学习操作动力学。好奇心奖励鼓励探索前向模型预测误差大的状态（预测姿态与实际姿态的均方误差），而基础进度奖励则捕捉前向模型损失随时间的改进。POE通过关注几何任务结构，显著减少了由视觉干扰引起的虚假探索。</p>
</li>
<li><p><strong>长视野记忆</strong>：为了解决历史信息压缩导致的细节模糊和遗忘问题，该方法将记忆视为<strong>上下文</strong>。在每一步，通过注意力机制从记忆存储中选择Top-K最相关的历史项作为独立的上下文令牌，而不是进行静态的相邻平均合并。具体地，对当前潜在表示<code>x_t</code>进行查询投影，对记忆项（加上时间位置编码）进行键投影，然后计算注意力分数并选择Top-K项。这些选定项的加权和<code>h_hat_t</code>通过一个学习的门控<code>g_t_mem</code>与当前潜在表示<code>x_t</code>融合，生成融合表示<code>x_tilde_t</code>。这个门控还用于调制来自POE的基础进度奖励<code>r_t_base</code>，产生最终的进度奖励<code>r_t_prog = g_t_mem * r_t_base</code>，从而在历史上下文表明操作模式不稳定时抑制虚假的进度信号。记忆更新仅将融合表示写回选定的项，并根据使用频率、新近性和冗余性计算的效用分数淘汰最无用的项。</p>
</li>
</ol>
<p><strong>组合奖励与训练目标</strong>：总奖励<code>r_tilde_t</code>是稀疏外部任务奖励<code>r_t_e</code>与内在奖励（<code>r_t_stage</code>, <code>r_t_cur</code>, <code>r_t_prog</code>）的加权和，内在权重ρ固定为0.6。完整的训练目标结合了PPO策略梯度损失、价值损失以及POE中前向和逆向世界模型的损失。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试/数据集</strong>：Discoverse-L，一个基于AIRBOT-Play平台的长视野操作基准，包含三个多阶段任务：积木桥（74阶段）、堆叠（18阶段）、枣杯（19阶段）。</li>
<li><strong>实验平台</strong>：DISCOVERSE仿真器用于训练和评估；物理AIRBOT-Play机器人用于真实世界部署。</li>
<li><strong>对比的Baseline方法</strong>：Octo, OpenVLA, π0, π0-FAST, OpenVLA-OFT。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在Discoverse-L仿真基准测试中，EvoVLA将平均成功率提升至69.2%，比最强的基线（OpenVLA-OFT）高出10.2个百分点。具体任务上，积木桥、枣杯和堆叠任务分别提升了11.2、9.1和10.3个百分点。</p>
<p><img src="https://arxiv.org/html/2511.16166v1/x6.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>图7</strong>：Discoverse-L基准测试（3个任务；3个种子的均值，50个评估回合）的定量结果。EvoVLA在平均成功率上显著优于所有基线方法。</p>
</blockquote>
<p>EvoVLA达到50%平均成功率所需的环境步数约为6×10^5，而OpenVLA-OFT需要约9×10^5步，样本效率提高了1.5倍。更重要的是，EvoVLA将幻觉率从OpenVLA-OFT的38.5%大幅降低至14.8%。</p>
<p><img src="https://arxiv.org/html/2511.16166v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融研究（左）和幻觉率分析（右）。左图显示逐步添加SAR、POE和长视野记忆模块对成功率的贡献；右图显示各模块对降低幻觉率的效果。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验验证了每个核心组件的贡献。单独添加SAR（带困难负面和时间平滑）比基础OpenVLA-OFT提升+4.7%成功率。在此基础上加入长视野记忆（独立模式）再贡献+2.4%成功率。最后，集成POE模块带来了额外的+3.1%成功率提升，证明了几何姿态 grounding 的互补价值。在降低幻觉方面，SAR的困难负面和时间平滑是主要贡献者，长视野记忆进一步减少了3.9个百分点，POE的加入又降低了2.1个百分点。</p>
<p><strong>真实世界部署结果</strong>：在物理机器人上，对于三个Discoverse-L任务的Sim2Real直接迁移，EvoVLA取得了54.6%的平均成功率，分别超过OpenVLA-OFT和π0-FAST基线11.0和16.9个百分点。在一个未见过的真实世界组装任务上，EvoVLA也达到了51.3%的成功率。</p>
<p><img src="https://arxiv.org/html/2511.16166v1/x4.png" alt="真实世界对比"></p>
<blockquote>
<p><strong>图5</strong>：堆叠任务的定性对比。上排OpenVLA-OFT出现夹爪过早打开、抖动、未对齐和掉落等问题；下排EvoVLA能延迟打开直至接触，并在几次调整后对齐，形成稳定堆叠，这与幻觉率降低的定量结果一致。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.16166v1/x3.png" alt="跨域对齐"></p>
<blockquote>
<p><strong>图4</strong>：仿真与真实世界操作序列对比。左右列展示了同一任务族在仿真和真实域中的时间进程，凸显了Sim2Real迁移后夹爪-物体交互和视角的一致性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了EvoVLA，一个自监督的长视野学习方法，通过融合阶段对齐奖励（SAR，含困难负面）、基于姿态的对象探索（POE）和长视野记忆，提供了密集且语义一致的内在反馈，有效抑制了“阶段幻觉”，无需额外标注即可实现可扩展的VLA微调。</li>
<li>系统地解决了长视野遗忘问题，提出了基于注意力选择上下文和门控融合的长视野记忆机制，并结合Discoverse-L基准测试（包含多阶段任务和真实阶段事件标注），为社区提供了研究长视野操作中记忆问题的方法和数据集/评估套件。</li>
<li>在仿真和真实机器人上进行了全面实验，验证了EvoVLA在提升成功率、样本效率以及大幅降低幻觉率方面的显著优势，证明了其鲁棒性和有效的Sim2Real迁移能力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，POE依赖于物体姿态估计，在高度混乱或遮挡场景中可能面临挑战。此外，虽然Discoverse-L提供了多阶段任务，但更广泛的任务多样性和更复杂的物理交互可能需要进一步扩展基准测试和训练数据。</p>
<p><strong>对后续研究的启示</strong>：EvoVLA展示了将语义对齐（通过VLM）、几何结构（通过姿态）和动态记忆管理相结合以解决长视野操作核心难题的有效性。这启示后续工作可以进一步探索更强大的世界模型、更高效的内存架构，以及如何将类似框架应用于更开放、非结构化的真实世界环境中。其提出的“阶段幻觉”度量标准和Discoverse-L基准也为该领域的量化评估提供了重要工具。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出EvoVLA模型，旨在解决长时程机器人操作中VLA模型存在的“阶段幻觉”问题，即智能体利用粗略评估信号走捷径，虚报进度而未真正完成任务。关键技术包括：阶段对齐奖励（SAR）通过三元组对比学习与难负样本防止视觉捷径；基于姿态的对象探索（POE）将好奇心机制锚定于物体-夹爪相对位姿；长时程记忆模块通过选择性上下文与门控融合稳定训练。实验表明，在Discoverse-L基准上，EvoVLA相比最强基线平均成功率提升10.2%（达69.2%），样本效率提高1.5倍，阶段幻觉率从38.5%降至14.8%；真实机器人部署任务平均成功率达54.6%，优于基线11.0个百分点。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16166" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>