<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.24653" target="_blank" rel="noreferrer">2512.24653</a></span>
        <span>作者: Jian Tang Team</span>
        <span>日期: 2026-01-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>数据驱动的模仿学习已经革新了机器人操作领域，但当前方法仍受限于大规模、多样化真实世界示范数据的稀缺。这导致现有模型在非结构化环境中执行长视野、双手协调任务以及移动操作方面的泛化能力有限。具体而言，现有数据集通常在多样性上存在局限：例如Open X-Embodiment、RH20T、DROID和RoboMIND 1.0主要包含单臂、固定基座的操作数据，缺乏双手协调示例；AgiBot World和Galaxea Open-World引入了丰富的双手操作数据，但仅依赖于单一机器人形态；RoboCOIN虽然扩展了形态多样性，但每个形态的任务覆盖稀疏，且大多数数据集缺乏触觉反馈等关键物理交互信号。本文针对“缺少一个能同时支持双手协调、移动操作、灵巧手和触觉感知的大规模多模态数据集”这一具体痛点，提出了构建一个全面、多维度多样化数据集的新视角。本文的核心思路是：构建RoboMIND 2.0——一个大规模、多模态、双手移动操作数据集，并设计一个分层双系统控制框架MIND-2来验证该数据集在推动通用化具身智能方面的效用。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的工作包含两大核心部分：RoboMIND 2.0数据集的构建，以及基于该数据集设计的MIND-2控制系统。</p>
<p><strong>RoboMIND 2.0数据集</strong>的整体构建流程是通过统一的遥操作和质量保证管道，在标准化的实验环境中收集数据。输入是操作员根据任务指令进行的遥操作，输出是包含多模态感知数据和精细语言标注的示范轨迹。数据集包含超过31万条双手操作轨迹，总计超过1000小时，采集自六种异构机器人形态（Franka, UR5e, AgileX, ARX, Tien Kung, Tian Yi），涵盖759个复杂任务和129项基本技能。其核心模块与特点包括：</p>
<ol>
<li><strong>多维度多样性</strong>：同时覆盖机器人形态、环境（家庭与工业）、任务语义、失败模式和多模态感知（视觉、本体感觉、力-扭矩、触觉），超越了仅关注单一维度多样性的现有基准。</li>
<li><strong>触觉增强数据</strong>：专门包含了1.2万条富含触觉信息的序列，这对于接触丰富的精细操作至关重要。</li>
<li><strong>高保真数字孪生</strong>：开源了真实世界环境的高保真数字资产（URDF模型、场景布局、传感器配置），并发布了与真实数据在任务结构、语言指令和物体配置上对齐的2万条仿真轨迹数据集，以支持低成本、可扩展的训练和有效的仿真到现实迁移。</li>
<li><strong>精细标注</strong>：每条轨迹都配有详细的自然语言描述，支持语言引导的策略学习和多模态表示训练。</li>
</ol>
<p><img src="https://i.imgur.com/placeholder.png" alt="数据集总览"></p>
<blockquote>
<p><strong>图1</strong>：RoboMIND 2.0概览。展示了数据集的大规模、多形态（六种机器人）、多环境（家庭与工业）以及包含触觉和移动操作数据的特点。</p>
</blockquote>
<p><img src="https://i.imgur.com/placeholder.png" alt="数据组成"></p>
<blockquote>
<p><strong>图2</strong>：RoboMIND 2.0数据组成。饼图展示了数据在不同机器人形态、环境类型（工业vs家庭）、技能类别以及是否包含移动操作等方面的分布。</p>
</blockquote>
<p><strong>MIND-2系统</strong>是一个用于长视野双手移动操作的分层双进程框架。其整体框架包含一个慢速的高层规划器（MIND-2-VLM）和一个快速的低层执行器（MIND-2-VLA）。</p>
<ul>
<li><strong>MIND-2-VLM（慢系统）</strong>：作为基于云的“大脑”，是一个高层语义规划器。其作用是将抽象的自然语言指令分解为具体的、可执行的子任务目标。</li>
<li><strong>MIND-2-VLA（快系统）</strong>：是一个视觉-语言-动作策略执行器。其输入包括以自我为中心的视觉观察、本体感觉和来自VLM的语言子目标指令，输出是精确的、感知本体状态的运动动作。该模块通过离线强化学习（隐式Q学习，IQL）在大规模真实世界数据上进行优化，利用优势加权回归来模仿成功行为并避免失败模式。</li>
</ul>
<p>与现有方法相比，MIND-2的创新点在于将语义规划与强化学习驱动的低层控制在一个分层架构中紧密结合，专门针对长视野、需要跨形态协调的复杂移动操作任务进行了优化。</p>
<p><img src="https://i.imgur.com/placeholder.png" alt="MIND-2框架"></p>
<blockquote>
<p><strong>图3</strong>：MIND-2双系统框架。左侧展示了慢系统MIND-2-VLM接收高层指令并规划子任务序列；右侧展示了快系统MIND-2-VLA接收子任务指令、视觉和本体感知，并生成控制动作。下方展示了在厨房和超市场景中，Tian Yi和AgileX机器人协作执行长视野任务的实例。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验主要在RoboMIND 2.0数据集本身及其构建的仿真环境上进行。对比的基线方法分为两类：</p>
<ol>
<li><strong>单任务模仿学习</strong>：包括2D图像方法（ACT, UVA）和3D点云方法（DP3, Dense Policy）。</li>
<li><strong>多任务VLA模型</strong>：包括π0、π0.5、HybridVLA和XR-1。</li>
</ol>
<p>关键实验结果总结如下：</p>
<ul>
<li><strong>在双手操作任务上，3D模仿学习算法优于2D方法</strong>。这得益于3D方法更丰富的空间建模能力，能更准确地表示双臂交互和同步的视觉动态。</li>
<li><strong>在多任务VLA模型评估中，跨形态模型XR-1在双手操作任务上表现出最优性能</strong>。</li>
<li><strong>触觉信号的集成带来了可衡量的性能提升</strong>。在精细操作任务中，将触觉信号作为本体感觉输入的一部分提供给VLA模型，能有效帮助完成跨多样环境的双手移动操作任务。</li>
</ul>
<p><img src="https://i.imgur.com/placeholder.png" alt="触觉消融实验"></p>
<blockquote>
<p><strong>图4</strong>：触觉模态消融研究。柱状图显示，在包含“插入”、“放置”等需要接触反馈的技能任务上，为VLA模型提供触觉输入（橙色）相比仅提供视觉和本体输入（蓝色），能显著提升任务成功率。</p>
</blockquote>
<ul>
<li><strong>MIND-2系统在长视野双手移动操作任务上显著优于基线</strong>。在涉及Tian Yi和Agilex机器人在厨房、超市和工业环境中的三个协作场景（整理餐具、协助结账、物料分拣）中，标准的模仿学习方法和现有VLA模型表现不佳，而MIND-2系统取得了显著更高的成功率。</li>
<li><strong>混合真实与仿真数据进行训练能持续提升物理执行性能</strong>。这验证了仿真基准的保真度以及合成数据增强的成本效益。</li>
</ul>
<p><img src="https://i.imgur.com/placeholder.png" alt="仿真数据混合实验"></p>
<blockquote>
<p><strong>图5</strong>：真实与仿真数据混合训练。曲线图表明，在使用不同比例的真实数据与仿真数据混合训练策略时，加入仿真数据能有效提升策略在真实世界测试中的成功率，尤其是在真实数据量有限的情况下。</p>
</blockquote>
<ul>
<li><strong>VLA模型展现出强大的物体级泛化能力</strong>。在UR5e双臂任务上训练π0.5和XR-1，并在功能等效但视觉或几何形态新颖的物体上进行测试，两者都表现出良好的泛化能力。</li>
</ul>
<p>消融实验总结了关键组件的贡献：1) 触觉模态的加入对精细操作任务至关重要；2) 仿真与真实数据的混合训练是一种高效的数据增强策略；3) MIND-2的分层设计（VLM+VLA）是解决复杂长视野任务的有效架构。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：</p>
<ol>
<li><strong>数据集贡献</strong>：发布了RoboMIND 2.0，这是首个同时支持双手协调、移动操作、灵巧手和高保真触觉感知的大规模开源多模态机器人操作数据集，具有前所未有的多维度多样性。</li>
<li><strong>配套资源与系统</strong>：提供了高保真数字孪生和仿真实例，并提出了MIND-2双进程框架，该框架在挑战性长视野任务上显著优于现有方法。</li>
<li><strong>广泛的实证验证</strong>：通过系统实验验证了3D模仿学习在双手任务上的优势、VLA模型的跨形态与物体泛化能力、触觉反馈的价值以及仿真-真实数据混合训练的有效性。</li>
</ol>
<p>论文自身提到的局限性主要在于数据收集仍然依赖于人工遥操作，未来需要探索更多自主或半自主的数据收集方法以进一步扩大规模。</p>
<p>对后续研究的启示包括：</p>
<ul>
<li>RoboMIND 2.0为训练通用机器人学习模型提供了一个高质量、标准化的基础资源，有望推动下一代具身智能模型的发展。</li>
<li>触觉感知与视觉、语言的融合是一个富有前景的研究方向，对于实现真正灵巧的操作至关重要。</li>
<li>高保真仿真与真实数据的协同使用，是突破物理数据收集瓶颈、实现可扩展机器人学习的关键路径。</li>
<li>MIND-2的层次化“慢-快”系统设计为处理复杂的、需要高层语义规划和低层敏捷反应的长视野任务提供了一个有效的架构范例。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习因缺乏大规模、多样化真实数据而泛化能力有限的问题，特别是在长视野双手机器人任务和非结构化环境移动操作中，提出了RoboMIND 2.0数据集和MIND-2系统。RoboMIND 2.0数据集包含310K双臂轨迹、12K触觉增强序列和20K移动操作轨迹，提供多模态真实数据与模拟对齐。MIND-2系统采用双系统架构，集成高层语义规划器MIND-2-VLM分解指令为子目标，以及低层视觉-语言-动作执行器MIND-2-VLA生成精确动作。实验在六个机器人实体上验证，MIND-2系统显著优于四个单任务基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.24653" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>