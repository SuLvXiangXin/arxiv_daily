<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.13572" target="_blank" rel="noreferrer">2509.13572</a></span>
        <span>作者: Karaali, Ozan, Farag, Hossam, Dosen, Strahinja, Stefanovic, Cedomir</span>
        <span>日期: 2025/09/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，半自主假肢系统通过用户与自主代理之间的共享控制来应对用户的操作局限，其依赖于基于视觉的感知来分析物体并选择抓取策略。传统方法采用复杂的流水线，通常需要串联多个专用模块，例如使用YOLO进行物体检测，再结合独立的模块进行分割和姿态估计。这种多模块方法需要各自开发和维护，使得整体系统复杂。</p>
<p>视觉语言模型（VLMs）提供了一种不同的思路，它能在一个单一系统中共同处理视觉和文本信息。本文旨在探索利用单个VLM来执行传统上需要多个模块（物体检测、姿态估计、抓取规划）才能完成的任务，从而简化仿生手感知系统的架构。尽管像ELLMER和GPTArm这样的通用机器人框架展示了VLMs用于高层规划的潜力，但VLM在假肢应用所需的细粒度抓取推断方面的能力尚未得到系统评估。</p>
<p>本文的核心思路是：通过设计一个统一的基准测试，评估单个VLM仅从一张静态图像中同时完成物体感知（识别物体及其关键属性）和抓取推断（推荐合适的抓取参数）的能力，以系统性地考察其作为半自主假肢感知模块的可行性和当前局限。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文构建了一个端到端的基准测试系统，用于评估VLM在仿生手抓取场景下的感知与推断能力。</p>
<p><img src="https://arxiv.org/html/2509.13572v1/diagram.png" alt="基准测试系统图"></p>
<blockquote>
<p><strong>图1</strong>：基准测试系统示意图。图像被发送到不同的VLM模型，以估计物体属性和抓取参数。通过计算多个分类和数值结果指标来评估整体性能。</p>
</blockquote>
<p><strong>整体流程</strong>：对于数据集中的每一张图像，系统会向选定的VLM发送一个精心设计的提示（Prompt）。VLM根据图像内容生成文本响应，该响应被要求以特定的结构化JSON格式输出。系统随后解析此JSON，将其中的预测值与预先标注的地面真值进行比较，从而在多个指标上评估VLM的性能。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>VLM测试平台</strong>：研究评估了8个通过OpenRouter访问的先进VLM模型，包括Anthropic的Claude Sonnet系列（4， 3.7， 3.5）、OpenAI的GPT-4.1、Google的Gemma 3 27B和Gemini 2.5 Flash（两个预览版），以及Mistral Medium 3。这提供了对当前VLM生态的广泛概览。</li>
<li><strong>图像数据集</strong>：数据集包含34张从接近的假肢视角拍摄的常见家用和办公物品的静态照片（见图2）。物体涵盖了圆柱体（23个）、长方体（10个）和球体（1个）等不同形状，以及垂直和水平两种朝向。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.13572v1/grid.jpg" alt="图像数据集"></p>
<blockquote>
<p><strong>图2</strong>：包含34个常见物体的图像数据集，从接近的假肢视角拍摄。这假设相机安装在假肢上，这是现有技术中的常见方法。</p>
</blockquote>
<ol start="3">
<li><strong>提示设计与结构化输出</strong>：这是方法的核心创新点。研究者设计了一个单一、全面的提示词，指示VLM扮演机器人学和计算机视觉专家，从给定图像中识别主要物体，并为<strong>左手</strong>仿生手推荐抓取方式。提示词明确要求VLM返回严格遵循以下格式的JSON字符串：<br><code>{&quot;object_name&quot;: &quot;...&quot;, &quot;object_shape&quot;: &quot;cylinder | cuboid | sphere&quot;, &quot;object_dimensions_mm&quot;: &quot;WxHxD&quot;, &quot;object_orientation&quot;: &quot;vertical | horizontal&quot;, &quot;grasp_type&quot;: 0 | 1, &quot;hand_rotation_deg&quot;: float, &quot;hand_aperture_mm&quot;: float, &quot;num_fingers&quot;: int}</code><br>其中，<code>grasp_type</code>的0代表掌握，1代表侧握。提示词还详细规定了每个参数的估计规则（如尺寸W/H/D的定义、手部旋转角度的参考系、所需手指数量的判断依据等），详见论文附录A。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：本文的创新性在于摒弃了传统的多模块流水线，提出了一个<strong>端到端的评估框架</strong>。通过一个<strong>统一的、富含领域知识的提示</strong>，直接要求VLM输出可直接用于假肢控制的结构化抓取参数。这种方法极大地简化了系统架构，并将VLM定位为一个能够提供“人工外感受”的单一、可解释的感知模块。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在包含34个物体的自定义数据集上进行了零样本评估。使用OpenRouter平台调用8个不同的VLM模型。评估指标包括分类准确率（用于物体名称、形状、朝向、抓取类型）和数值误差（平均绝对误差MAE和平均误差ME，用于物体尺寸、手部旋转、手部开合度、手指数量），同时记录了响应延迟和每次查询的成本。</p>
<p><strong>对比的Baseline</strong>：由于这是首个针对该任务的VLM基准测试，研究并未与传统模块化方法进行直接性能对比，而是横向比较了8个不同VLM模型在同一任务上的表现，以评估当前VLM技术的整体水平和模型间差异。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>物体层面感知（分类任务）</strong>：如表I所示，所有模型在物体命名（82.4%-97.1%）、形状识别（97.1%-100%）和朝向判断（85.3%-100%）上都表现出色。然而，在抓取类型（掌握 vs. 侧握）的分类准确率上差异显著，最佳模型（Gemini 2.5 Flash 17-04）达到91.2%，而最差模型（Gemma 3）仅为44.1%。</p>
<blockquote>
<p><strong>表I要点</strong>：展示了各模型在分类任务上的准确率。VLM在基础物体属性识别上非常可靠，但在推断与抓取策略直接相关的“抓取类型”时一致性较差。</p>
</blockquote>
</li>
<li><p><strong>物体尺寸估计</strong>：如表II所示，在尺寸估计的MAE上，Gemini 2.5 Flash 20-05表现最佳，尤其在深度估计上误差最低（4.62 ± 4.06 mm）。总体而言，所有模型在宽度和深度上的MAE通常低于11毫米。</p>
<blockquote>
<p><strong>表II要点</strong>：展示了物体尺寸估计的平均绝对误差。尺寸估计存在误差，但部分模型在特定维度上精度尚可，误差在可容忍范围内（例如，对80毫米宽的杯子，11毫米误差约为13.8%）。</p>
</blockquote>
</li>
<li><p><strong>抓取参数估计</strong>：如表III所示，手部旋转角的估计误差最大（MAE从13.2°到47.6°），且标准差很高，表明预测极不稳定。手部开合度估计中，GPT-4.1的MAE最低（5.74 ± 6.14 mm）。手指数量估计的误差（MAE在0.5-0.88之间）也揭示了该离散任务的不一致性。</p>
<blockquote>
<p><strong>表III要点</strong>：展示了抓取参数估计的平均绝对误差。手部旋转估计是当前VLM最薄弱的环节，误差大且不稳定。</p>
</blockquote>
</li>
<li><p><strong>系统偏差分析</strong>：表IV和表V展示了平均误差（ME）。例如，在手部开合度估计上，GPT-4.1和Claude 3.7 Sonnet的系统偏差接近零（0.44 mm和0.15 mm），这对于实际应用是有利的，因为正偏差（预设开合过大）比负偏差（预设开合不足）更易被用户修正。</p>
<blockquote>
<p><strong>表IV&amp;V要点</strong>：揭示了模型预测的系统性偏差。了解偏差方向对于设计共享控制策略至关重要，例如，倾向于高估开合度可能比低估更具操作性。</p>
</blockquote>
</li>
<li><p><strong>延迟与成本</strong>：如表VI所示，Gemini 2.5 Flash 20-05响应最快（2.60 ± 0.47秒），Gemma 3 27B-IT成本最低（每次查询$0.00016）。Claude系列模型延迟最高（7-9秒），成本也最贵（约$0.012/查询）。</p>
<blockquote>
<p><strong>表VI要点</strong>：展示了实际部署需考虑的延迟和成本。最快的模型延迟（<del>2.6秒）仍远高于传统视觉系统（</del>150毫秒），但可能在用户伸手过程中被部分容忍。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验总结</strong>：本文未进行典型的模块消融实验，因为其核心是一个评估性研究。但其对不同VLM模型、不同任务子项（分类 vs. 数值估计）的性能拆解，本身揭示了VLM能力的不均衡性：<strong>在高级语义理解（是什么、什么形状）上表现强劲，但在需要精确几何理解（尺寸、角度）和特定领域决策（抓取类型）的任务上仍存在显著局限</strong>。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个用于评估VLM在仿生手抓取任务中端到端感知与推断能力的统一基准。</li>
<li>通过精心设计的结构化提示，证明了当前先进的VLM能够以高准确度识别物体基本属性，并输出可直接解析用于假肢控制的抓取参数JSON，验证了该技术路线的初步可行性。</li>
<li>系统性地评估了8个主流VLM，全面揭示了其在物体感知和抓取推断各项子任务上的当前能力、局限（特别是数值估计的不稳定性），以及延迟-成本权衡，为未来研究提供了清晰的基线。</li>
</ol>
<p><strong>论文提及的局限性</strong>：</p>
<ol>
<li>数据集规模小（34个物体），类别不平衡（圆柱体居多），且场景简单（单一物体、受控光照），结论的普适性受限。</li>
<li>抓取策略的地面真值具有一定主观性，许多物体存在多种有效抓取方式。</li>
<li>所有评估均为零样本，未进行针对假肢抓取的领域微调。</li>
<li>VLM的推理延迟（2.6-9.4秒）对于需要实时交互的假肢控制来说仍然过长。</li>
<li>研究仅关注静态图像分析，未涉及动态抓取场景、用户反馈集成等更复杂的控制环节。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>共享控制范式</strong>：实验结果支持采用共享控制策略，即VLM提供结构化的初始猜测（JSON），用户或轻量级本地控制器进行微调。这既能利用VLM强大的物体理解能力，又能弥补其参数估计不精确的缺点。</li>
<li><strong>领域适应与优化</strong>：未来工作应探索对VLM进行假肢抓取领域的微调，以提升抓取类型判断和参数估计精度。同时，需研究模型压缩、边缘部署等方法以降低延迟。</li>
<li><strong>基准扩展</strong>：需要构建更大规模、更平衡、包含遮挡、杂乱背景和动态场景的数据集，以推动VLM在更真实、复杂环境下的性能提升。</li>
<li><strong>提示工程与知识注入</strong>：可以进一步探索在提示中注入更丰富的领域知识（如物体朝向与手部旋转的几何关系），以提升在困难子任务上的表现。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本研究探讨了使用视觉语言模型提升半自主仿生手感知能力的潜力。核心问题是替代传统需要物体检测、姿态估计和抓握规划等多个模块的复杂流程。方法上，提出一个端到端评估基准，让单个VLM从静态图像中统一完成物体识别（名称、形状等）与抓握参数推理（类型、手腕旋转等）。实验评估了8个当代VLM，结果显示：多数模型在物体识别和形状分类上表现优异，但在估计物体尺寸和推断最佳抓握参数（尤其是手部旋转和开合度）时准确性波动较大。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.13572" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>