<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.14959" target="_blank" rel="noreferrer">2510.14959</a></span>
        <span>作者: Yang, Lizhi, Werner, Blake, de Sa, Massimiliano, Ames, Aaron D.</span>
        <span>日期: 2025/10/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人强化学习领域，主流方法主要关注性能优化，但往往以牺牲安全性为代价。为确保安全，现有方法主要分为两类：一类是在运行时使用安全过滤器（如基于控制屏障函数的二次规划）对策略输出的动作进行在线修正，这虽然能保证安全，但过滤器必须持续运行，且策略可能无法内化安全约束，导致行为保守并限制探索；另一类是通过奖励塑形（如添加惩罚项）来鼓励安全行为，但这不直接强制执行安全动作，且对惩罚权重敏感，在安全关键应用中可能不足。本文针对“如何让强化学习策略在训练过程中内化安全约束，从而在部署时无需运行时安全过滤器仍能保持安全”这一具体痛点，提出了一种融合安全过滤与奖励塑形的双重方法新视角。本文的核心思路是：在训练时，使用基于控制屏障函数的闭式解安全过滤器来防止灾难性不安全动作，同时设计屏障启发式奖励项来引导策略主动避免安全干预，从而使学习到的策略能将安全约束内化，在部署时无需在线过滤即可表现出安全行为。</p>
<h2 id="方法详解">方法详解</h2>
<p>CBF-RL框架的整体流程如图2所示，其核心是在训练阶段对策略的采样过程进行双重安全注入。输入是策略根据状态观测提出的原始动作 $\mathbf{v}^{\mathrm{policy}}$ 和当前降维状态 $\mathbf{q}$；输出是经过安全过滤的动作 $\mathbf{v}^{\mathrm{filtered}}$ 以及包含安全信息的组合奖励 $r = r_{\mathrm{nominal}} + r_{\mathrm{cbf}}$，用于更新策略。部署时，训练好的策略 $\pi_{\theta}$ 可直接输出安全动作 $\mathbf{v}^{\mathrm{policy}}_{\mathrm{cbf}}$，无需额外过滤器。</p>
<p><img src="https://arxiv.org/html/2510.14959v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：CBF-RL框架。对于给定任务，用户需定义安全屏障函数 $h(\mathbf{q})$ 及其梯度 $\nabla h(\mathbf{q})$。训练时，RL策略提出动作 $\mathbf{v}^{\mathrm{policy}}$；CBF安全过滤器随后计算CBF-QP的闭式解 $\mathbf{v}^{\mathrm{filtered}}$ 以及基于提议动作和状态的安全奖励 $r_{\mathrm{cbf}}$。智能体在并行离散化环境中执行 $\mathbf{v}^{\mathrm{filtered}}$，策略使用任务奖励、正则化奖励和安全奖励的组合 $r=r_{\mathrm{nominal}}+r_{\mathrm{cbf}}$ 进行更新。部署时，策略能够直接输出安全动作 $\mathbf{v}^{\mathrm{policy}}_{\mathrm{cbf}}$，无需显式的运行时过滤器。</p>
</blockquote>
<p>框架包含两个核心模块：</p>
<ol>
<li><strong>基于闭式解的安全过滤模块</strong>：该模块在训练期间对每一步策略提议的动作进行最小化修正，以确保满足CBF安全条件。论文的关键理论贡献（Lemma 1和Theorem 1）证明了在足够小的离散时间步长 $\Delta t$ 下，连续时间CBF条件可以有效地保证离散时间滚动的安全性。因此，无需在每一步求解复杂的二次规划，可直接采用连续时间CBF-QP的闭式解。具体而言，对于提议动作 $\mathbf{v}_k^{\mathrm{policy}}$，安全动作 $\mathbf{v}_k^{\mathrm{safe}}$ 的计算如公式(20)所示：若 $\nabla h(\mathbf{q}_k)^\top \mathbf{v}_k^{\mathrm{policy}} \geq -\alpha h(\mathbf{q}_k)$ 则直接通过；否则，将其投影到安全边界上。其中 $\mathbf{a}_k = \nabla h(\mathbf{q}_k)$, $b_k = -\alpha h(\mathbf{q}_k)$。这种轻量级实现适合IsaacLab等大规模并行仿真环境。</li>
<li><strong>屏障启发式奖励塑形模块</strong>：为了引导策略学习并内化安全约束，除了过滤动作，还设计了一个安全奖励项 $r_{\mathrm{cbf}}$，其包含两部分（公式22-23）：第一部分 $\max\left(\mathbf{a}_k^\top \mathbf{v}_k^{\mathrm{policy}} - b_k, 0\right)$ 奖励策略提议的动作本身满足CBF条件（即无需过滤）；第二部分 $\exp\left(-|\mathbf{v}^{\mathrm{policy}} - \mathbf{v}^{\mathrm{safe}}|^2 / \sigma^2\right) - 1$ 则惩罚提议动作与安全过滤动作之间的差异，鼓励策略输出更接近安全过滤器的动作。总奖励为原始任务奖励与此安全奖励的加权和。</li>
</ol>
<p>与现有方法相比，CBF-RL的创新点具体体现在：1) <strong>方法论上</strong>：首创性地将主动安全过滤与被动奖励塑形在训练阶段结合，形成“双重”训练机制，使策略能同时获得“什么动作不安全会被纠正”以及“如何直接提出安全动作以获得更高奖励”的直接监督信号。2) <strong>理论上</strong>：分析了连续时间CBF在离散时间RL仿真中的适用性，为使用轻量级闭式解而非在线优化提供了理论依据。3) <strong>实践上</strong>：实现了与标准策略梯度算法（如PPO）的无模型、轻量级集成，仅需降阶模型的导数（如雅可比矩阵），并通过领域随机化处理不确定性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在以下平台和数据集上进行验证：1) <strong>2D导航任务</strong>：一个具有动态随机性的点质量机器人避障环境，用于消融研究和鲁棒性测试。2) <strong>人形机器人任务</strong>：在IsaacLab仿真器中，使用具有完整动力学和领域随机化的Unitree G1人形机器人模型，进行避障和爬楼梯任务。3) <strong>硬件验证</strong>：在真实的Unitree G1人形机器人上进行零样本仿真到现实的策略部署。</p>
<p>对比的基线方法包括：<strong>仅过滤</strong>（训练和部署时均使用CBF安全过滤器）、<strong>仅奖励</strong>（仅使用CBF奖励塑形，无动作过滤）、<strong>无CBF</strong>（标准RL，无任何安全机制）以及<strong>CBF-RL（双重方法）</strong>。</p>
<p><img src="https://arxiv.org/html/2510.14959v2/x3.png" alt="导航任务结果"></p>
<blockquote>
<p><strong>图3</strong>：2D导航任务的性能对比。(a) 任务完成率：CBF-RL（双重）方法达到接近100%的成功率，显著高于仅奖励和无CBF方法，与仅过滤方法相当。(b) 训练期间的安全违规次数：CBF-RL和仅过滤方法在训练早期就将违规次数降至接近零，而仅奖励方法仍有持续违规，无CBF方法违规最多。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.14959v2/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：2D导航任务的消融研究。(a) 不同噪声水平下的成功率：CBF-RL（双重）方法在所有噪声水平下均保持高成功率，鲁棒性最强。(b) 部署时（无过滤器）的安全违规次数：经过双重方法训练的策略在部署时违规次数最低，表明其成功内化了安全约束；而仅过滤方法训练的策略在移除过滤器后违规激增。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.14959v2/x5.png" alt="人形机器人结果"></p>
<blockquote>
<p><strong>图5</strong>：人形机器人爬楼梯任务的结果。(a) 训练曲线：CBF-RL（双重）方法获得更高的最终奖励和更快的收敛速度。(b) 部署时的性能：在无运行时过滤器的情况下，双重方法训练的策略成功率达到93%，远高于仅奖励（27%）和无CBF（7%）方法。仅过滤方法若在部署时移除过滤器，成功率会降至40%。</p>
</blockquote>
<p>关键实验结果总结如下：在2D导航任务中，CBF-RL实现了接近100%的任务完成率，在训练期间几乎完全消除了安全违规。在存在动态噪声的鲁棒性测试中，CBF-RL在不同噪声水平下均保持高成功率。最重要的是，在部署阶段移除安全过滤器后，CBF-RL策略的违规次数极低（约0.05次/episode），而“仅过滤”方法训练的策略违规次数急剧上升（超过3次/episode），这证明了安全约束的内化。在人形机器人复杂任务中，CBF-RL同样展现出更快的收敛速度、更高的最终奖励，以及在仿真和真实硬件上无需运行时过滤器即可安全避障和爬楼梯的能力（成功率达93%）。</p>
<p>消融实验明确显示了每个组件的贡献：安全过滤组件在训练期间有效防止了灾难性失败，确保了训练稳定性；奖励塑形组件则引导策略主动学习安全行为，使得在部署时无需过滤器的策略仍能保持高安全性。两者结合（双重方法）取得了最佳效果。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>提出了CBF-RL双重训练框架</strong>，首次将CBF安全过滤与屏障启发式奖励塑形在训练阶段协同使用，使RL策略能够内化安全约束。2) <strong>提供了连续到离散的理论分析</strong>，证明了在RL常用的离散时间仿真中应用连续时间CBF闭式解的有效性，实现了轻量级集成。3) <strong>进行了全面的实证验证</strong>，从2D导航到高维人形机器人，从仿真到实物，证明了该框架能实现更安全的探索、更快的收敛、以及无需运行时过滤器的鲁棒安全部署。</p>
<p>论文自身提到的局限性在于，该方法需要用户为特定任务预先定义（或已知）合适的控制屏障函数 $h(\mathbf{q})$。对于更复杂或未知的安全约束，如何自动学习或合成屏障函数是一个未解决的问题。</p>
<p>本文工作对后续研究的启示在于：它证明了通过精心设计的训练机制，可以将形式化的安全约束有效地编码到数据驱动的RL策略中，从而在保持高性能的同时免除部署时的在线计算开销。这为将安全关键控制方法扩展到计算受限、高维复杂的实时系统（如人形机器人、无人机）提供了一条切实可行的路径。未来的工作可以探索将屏障函数学习与CBF-RL框架相结合，以处理更广泛的安全规范。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出CBF-RL框架，解决强化学习（RL）在训练中因追求性能而忽视安全约束的问题。方法核心包括：1）通过控制屏障函数（CBF）项对原始RL策略进行最小化修改以编码安全约束；2）在训练过程中对策略展开进行安全过滤。理论证明连续时间安全过滤器可通过离散时间展开的闭式表达式实现。实验表明，在导航任务和Unitree G1人形机器人上，CBF-RL实现了更安全的探索、更快的收敛与鲁棒性能，使机器人无需运行时安全过滤器即可安全避障和爬楼梯。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.14959" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>