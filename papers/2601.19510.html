<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ALRM: Agentic LLM for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ALRM: Agentic LLM for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.19510" target="_blank" rel="noreferrer">2601.19510</a></span>
        <span>作者: Hakim Hacid Team</span>
        <span>日期: 2026-01-27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将大语言模型（LLMs）集成到机器人控制流程中存在两个主要局限：（1）现有的基于LLM的方法通常缺乏模块化、智能体的执行机制，限制了其以闭环方式进行规划、反思结果和修正动作的能力；（2）现有的操作任务基准测试侧重于低层控制，未能系统性地评估多步推理和语言指令的多样性。本文针对这些痛点，提出了一个名为ALRM（Agentic LLM for Robot Manipulation）的LLM驱动的智能体框架。其核心思路是结合LLM的行动生成与智能体执行，通过一个ReAct风格的推理循环，并支持代码即策略（CaP）和工具即策略（TaP）两种互补的操作模式，以弥合自然语言推理与可靠机器人执行之间的鸿沟。</p>
<h2 id="方法详解">方法详解</h2>
<p>ALRM的整体框架是一个模块化的智能体架构，旨在解决高层机械臂操作任务。该架构包含三个主要模块：任务规划器智能体（Task Planner Agent）、任务执行器智能体（Task Executor Agent）和API服务器（API Server）。</p>
<p><img src="https://arxiv.org/html/2601.19510v2/Figures/agentic-architecture.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：提出的用于解决高层机械臂操作任务的基于LLM的智能体架构。该架构由三个主要模块组成：（1）任务规划器智能体，（2）任务执行器智能体，以及（3）API服务器。</p>
</blockquote>
<p><strong>任务规划器智能体</strong> 基于ReAct框架设计，通过“思考-行动-观察”的迭代循环，将高层用户请求分解为子任务。它利用LLM动态生成子任务，并接收来自任务执行器的观察结果作为反馈，从而实现实时环境适应。规划器被指示根据覆盖大多数操作场景的高层模板生成子任务，例如获取环境对象位置、获取对象名称、执行“拾取[对象名]并将其放置到[空间关系][目的地]”等原子操作。同时，规划器遵循一次只生成一个动作、一次只聚焦一个对象的原则，并确保只对环境中存在且与用户请求相关的对象生成动作。</p>
<p><strong>任务执行器智能体</strong> 负责接收高层自然语言子任务（例如，“拾取柠檬并将其放入垃圾桶”），并生成实现该子任务所需的动作序列。该智能体可以访问八个预定义动作，涵盖机器人控制（<code>pick</code>, <code>place</code>, <code>move_to_home_pos</code>, <code>move_to</code>）、环境感知（<code>get_objects</code>, <code>get_reference_names</code>）以及获取位姿（<code>compute_grasp</code>, <code>get_pose</code>）。这些动作通过两种互补模式执行：</p>
<ol>
<li><strong>工具即策略（TaP）</strong>：利用LLM的工具调用能力，以嵌套工具调用的方式执行动作。它以交互式步骤运行，LLM通常每步产生一个工具调用，结果被附加到对话历史中并反馈给LLM，直到子任务完成。这种设计允许执行器对中间结果进行推理并纠正错误，提供了更大的灵活性，但增加了LLM调用次数并依赖模型可靠的工具调用能力。</li>
<li><strong>代码即策略（CaP）</strong>：生成调用可用函数的Python代码，以单次运行的方式完成整个子任务。它执行生成的代码并维护所有动作执行的日志。这种方法比工具执行器更快，因为整个子任务一次性处理，所需LLM调用更少，且能使用不支持工具调用的模型。然而，它也更脆弱：如果生成代码的任何部分包含错误，整个子任务执行都可能失败，因此更依赖于任务规划器生成准确的自然语言动作描述。</li>
</ol>
<p><strong>API服务器</strong> 实现为一个RESTful API服务器，用于控制仿真中的Interbotix wx250s机械臂。它提供了拾放、运动命令和感知查询的端点。服务器内部通过两个模块连接机器人后端：<code>wx250sRobot</code>（与MoveIt和ROS通信的抽象层）和<code>SimPerception</code>（从Gazebo检索对象位置和抓取位姿估计）。这种设计使LLM能够专注于生成高层动作，而非低层ROS命令，降低了LLM动作生成的复杂性，同时确保了不同任务间的泛化能力。</p>
<p>与现有方法相比，ALRM的创新点在于提出了一个统一的、模块化的智能体框架，集成了代码生成和基于工具的执行，并引入了智能体协调、对执行结果的反思以及闭环任务修订机制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>为了评估ALRM，本文引入了一个新的仿真基准测试，包含3个不同环境（厨房用具、盒子、水果），每个环境定义了3个规范任务（共9个），每个任务又生成了5个不同语言变体的指令（释义），覆盖词汇（LEX）、句法（SYN）、语义（SEM）和高层推理（HLR）四个类别，总计54个任务。此外，还包含两个额外的“热身”任务，总计56个任务进行评估。</p>
<p><img src="https://arxiv.org/html/2601.19510v2/Figures/environment_1.png" alt="评估环境"></p>
<blockquote>
<p><strong>图2</strong>：为评估设计的三个环境的图示：(a) 厨房用具环境，(b) 盒子环境，(c) 水果环境。</p>
</blockquote>
<p>实验评估了十种LLM，包括闭源模型（GPT-5, Gemini-2.5-Pro, Claude-4.1-Opus, DeepSeek-V3.1）和开源模型（Falcon-H1-7B, Qwen3-8B, Llama-3.1-8B, DeepSeek-R1-7B, Granite-3.3-8B, Mistral-7B）。评估指标包括成功率（使用LLM-as-a-judge策略，评分0/1/2）和延迟（秒）。实验在轻量级仿真环境中进行，快速验证生成的动作序列，所有地面实况代码和工具调用均在Gazebo仿真器中用真实机械臂验证过。</p>
<p>关键实验结果总结如下：在CaP模式下，Claude-4.1-Opus取得了最高的平均成功率92.6%，其次是GPT-5（90.7%）和DeepSeek-V3.1（84.3%）。在开源模型中，Falcon-H1-7B在CaP模式下取得了84.3%的成功率，与DeepSeek-V3.1持平，但延迟仅需24.89秒，不到后者（69.83秒）的一半。在TaP模式下，Claude-4.1-Opus同样表现最佳，平均成功率为93.5%。总体而言，大型模型（尤其是Claude-4.1-Opus）在两种模式下都表现出色，而小型模型在CaP模式下（如Falcon-H1-7B）可以取得与某些大型模型相当的成功率，但延迟更低。实验也揭示了不同模型对指令变体的鲁棒性差异，例如，在高层推理（HLR）类别上，所有模型的性能普遍有所下降。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：（1）提出了ALRM，一个支持CaP和TaP两种操作模式的LLM驱动智能体框架，用于机器人规划与执行，具备模块化和闭环反思能力；（2）引入了一个包含56个任务的新基准测试，强调多步、高层任务和丰富的语言多样性；（3）对十种LLM在两种执行模式下进行了系统评估，揭示了不同规模模型的性能与效率权衡。</p>
<p>论文自身提到的局限性包括：框架的成功依赖于LLM生成可靠代码或进行工具调用的能力；基准测试虽然具有语言多样性，但任务数量和复杂度仍有扩展空间；实验主要在仿真中进行，真实世界的物理不确定性和感知噪声是未来的挑战。</p>
<p>这项工作对后续研究的启示在于：证明了模块化、智能体化的LLM框架能有效处理需要多步推理的机器人操作任务；为评估LLM在机器人领域的表现提供了一个更全面的基准；展示了小型开源LLM在特定模式下具备实用性，为在资源受限场景下的部署提供了可能性。未来的工作可集中于将框架扩展到真实机器人、处理更复杂的物理交互以及探索多模态感知与规划的更深层次集成。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对LLM在机器人控制中缺乏闭环执行机制和系统性评估基准的问题，提出ALRM框架。该框架通过ReAct式推理循环，整合策略生成与代理执行，提供Code-as-Policy（直接生成控制代码）和Tool-as-Policy（迭代规划与工具执行）两种模式。为系统评估，作者构建了包含56个任务、支持语言多样性的模拟基准。实验使用十种LLM验证，结果表明ALRM能有效连接自然语言推理与机器人执行，其中Claude-4.1-Opus（闭源）和Falcon-H1-7B（开源）在CaP模式下表现最佳。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.19510" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>