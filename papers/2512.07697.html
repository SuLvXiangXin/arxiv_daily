<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.07697" target="_blank" rel="noreferrer">2512.07697</a></span>
        <span>作者: Shayegan Omidshafiei Team</span>
        <span>日期: 2025-12-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人视觉运动策略学习的主流方法，如扩散策略（Diffusion Policy, DP），通常在训练和部署时假设推理延迟为零。这意味着策略根据当前观测状态生成动作，并假设该动作能立即执行。然而，在真实的机器人系统中，从感知到动作执行之间存在不可避免的推理延迟（δ），可达数十至数百毫秒。在静态环境中，这种延迟影响较小，但在与高速动态物体交互的任务中，零延迟假设会导致严重的观察-执行不匹配，使得策略动作总是滞后，从而任务失败。</p>
<p>本文针对动态任务中推理延迟导致策略性能下降这一具体痛点，提出了将延迟作为算法设计一等公民的新视角。核心思路是通过对零延迟的示范数据进行延迟补偿校正，并训练一个以延迟为条件的扩散策略，使策略能够针对动作执行时的未来状态进行规划，从而弥合观察与执行之间的差距。</p>
<h2 id="方法详解">方法详解</h2>
<p>Delay-Aware Diffusion Policy (DA-DP) 框架包含两个核心部分：延迟感知数据处理和以延迟为条件的策略训练。其目标是修改训练数据，使得在给定推理延迟δ的情况下，训练出的策略仍能在目标时间内完成任务。</p>
<p><img src="https://arxiv.org/html/2512.07697v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>: DA-DP框架总览。以动作块长度 H_act=4，推理延迟 δ=2Δt 为例。上半部分展示了零延迟轨迹、DP执行轨迹（因延迟而滞后）以及DA-DP校正后的目标轨迹。下半部分通过三个步骤将零延迟轨迹压缩为延迟感知轨迹。</p>
</blockquote>
<p><strong>整体流程</strong>：输入是假设零延迟收集的示范轨迹 τ。对于给定的推理延迟δ，DA-DP首先对τ进行压缩校正，生成延迟感知轨迹 τ‘_δ。然后，利用在不同δ下生成的一系列τ‘_δ数据集，训练一个以δ为条件的扩散策略 π_θ(a’|s‘， δ)。在推断时，将实测或设定的延迟δ与当前状态拼接后输入策略，生成补偿延迟的动作。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>延迟感知数据处理</strong>：这是将零延迟轨迹适配到存在延迟的系统的关键。具体分为三步：</p>
<ul>
<li><strong>步骤1：计算调整后长度</strong>：根据公式 n‘ = (nΔt + δ) / (Δt + δ/H_act) 计算校正后的轨迹长度 n‘。其物理意义是，在总时间预算 T_target = nΔt 内，考虑每次推理耗时δ，所能完成的最大动作步数。</li>
<li><strong>步骤2：确定状态跳过数量</strong>：计算每个动作块后需要跳过的状态数 m。在连续时间模型中，m = δ/Δt。在离散实现中，需根据 n 和 n‘ 调整，确保压缩后的轨迹步数为整数且能准时到达终点。</li>
<li><strong>步骤3：压缩轨迹</strong>：按照公式 s‘<em>i = s</em>{i + k(i)*m} 构建压缩后的状态序列，其中 k(i) 是已完成动作块计数。这意味着每执行完一个 H_act 的动作块，状态索引就向前跳跃 m 步，以抵消推理延迟带来的时间损失。最后，根据压缩后的状态序列计算新的动作 a‘<em>i = s‘</em>{i+1} - s‘_i。</li>
</ul>
</li>
<li><p><strong>以延迟为条件的扩散策略</strong>：算法上，DA-DP 对标准 DP 的修改极小且易于集成。主要区别在于<strong>将推理延迟δ作为条件输入</strong>。在训练时，从延迟感知数据集 {τ_δ} 中采样批量 (s， a， δ)，将延迟δ与状态s拼接形成增强状态 ~s，然后输入到扩散模型的编码器和UNet中。损失函数与标准DP一致，均为去噪分数匹配损失。在推断时，同样将当前状态与已知延迟δ拼接后输入策略，生成动作。</p>
</li>
</ol>
<p><strong>创新点</strong>：与现有旨在加速推理或实现异步执行的方法不同，DA-DP的创新点在于<strong>显式地对推理延迟进行建模和补偿</strong>。它不改变扩散去噪过程本身，而是通过数据预处理和条件化，从根本上改变了策略学习的目标——让策略学习如何从“当前观测”规划出能在“未来执行时刻”生效的动作。这种方法与同步/异步执行范式是互补的，可以即插即用。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在ManiSkill引擎构建的三个动态环境中进行（图3），涵盖了不同机器人形态和任务类型。</p>
<ul>
<li><strong>任务</strong>：1) 捡起滚动球（Franka Panda机械臂）；2) 乒乓球（Franka Panda机械臂）；3) 移动箱子的抓放（Unitree G1人形机器人）。</li>
<li><strong>基线方法</strong>：标准扩散策略（DP）以及作为理想性能参考的零延迟DP。</li>
<li><strong>评估指标</strong>：任务成功率。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.07697v1/x3.png" alt="实验环境"></p>
<blockquote>
<p><strong>图3</strong>: 三个用于评估的动态任务环境：捡起滚动球、乒乓球和移动箱子的抓放。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br><strong>Q1. 推理延迟是否影响动态任务性能？</strong><br>实验表明，推理延迟显著损害了DP的性能，而DA-DP则保持了鲁棒性。</p>
<p><img src="https://arxiv.org/html/2512.07697v1/x4.png" alt="捡球任务-固定延迟"></p>
<blockquote>
<p><strong>图4</strong>: 在捡起滚动球任务中，随着固定推理延迟δ增加，DP成功率急剧下降（δ=0.10s时仅0.01），而DA-DP下降平缓（δ=0.10s时为0.72）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07697v1/x5.png" alt="乒乓球任务-固定延迟"></p>
<blockquote>
<p><strong>图5</strong>: 在乒乓球任务中，除最小延迟（δ=0.025s）外，DA-DP在所有延迟设置下均优于DP，且性能接近零延迟的理想基线。</p>
</blockquote>
<p><strong>Q2. DA-DP能否处理变化的推理延迟？</strong><br>通过在多延迟值混合的数据集上训练，DA-DP在测试时对动态延迟表现出更强的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2512.07697v1/x6.png" alt="捡球任务-变延迟集"></p>
<blockquote>
<p><strong>图6</strong>: 在捡球任务中，面对不同范围的延迟集合，DA-DP（成功率0.42-0.76）始终优于DP（最高仅0.28）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07697v1/x7.png" alt="乒乓球任务-变延迟集"></p>
<blockquote>
<p><strong>图7</strong>: 在乒乓球任务中，随着训练延迟集增大，DA-DP性能提升至0.80，而DP性能停滞不前。</p>
</blockquote>
<p><strong>Q3. DA-DP对训练分布外的延迟是否鲁棒？</strong><br>在训练未见过的更大延迟上进行评估，DA-DP展现了更好的泛化能力。</p>
<p><img src="https://arxiv.org/html/2512.07697v1/x8.png" alt="捡球任务-分布外延迟"></p>
<blockquote>
<p><strong>图8</strong>: 在捡球任务中，当测试延迟比训练延迟高0.15s时，DP性能近乎为零，而DA-DP仍能保持0.48-0.62的成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07697v1/x9.png" alt="乒乓球任务-分布外延迟"></p>
<blockquote>
<p><strong>图9</strong>: 在乒乓球任务中，测试延迟增加0.075s，DA-DP成功率稳定在0.73-0.82，显著高于DP的0.49-0.56。</p>
</blockquote>
<p><strong>Q4. DA-DP能否泛化到不同机器人形态？</strong><br>在人形机器人执行动态抓放任务的实验中，DA-DP的优势依然存在。</p>
<p><img src="https://arxiv.org/html/2512.07697v1/x10.png" alt="人形机器人任务-固定延迟"></p>
<blockquote>
<p><strong>图10</strong>: 在更复杂的人形机器人抓放移动箱子任务中，DA-DP在多个延迟设置下达到完美成功率（1.0），而DP最高仅为0.7。</p>
</blockquote>
<p><strong>消融实验</strong>：本文的核心方法（数据校正+延迟条件化）本身可视为对标准DP的增强。实验结果表明，二者结合有效解决了延迟问题。数据校正确保了时间目标的正确性，而延迟条件化使策略能够适应不同的延迟值。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>DA-DP框架</strong>，一种通过数据校正和延迟条件化，显式地将推理延迟纳入策略学习的通用方法。</li>
<li>通过<strong>广泛的实证评估</strong>，证明了DA-DP在多种动态任务、机器人平台和延迟条件下，相比延迟不感知的方法具有显著更强的鲁棒性和性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，DA-DP的数据校正步骤依赖于对系统动态（状态转移）的访问或估计（通过 s‘_{i+1} - s‘_i 计算动作）。在仅依赖图像等高维观测而无法直接获取状态的情况下，需要额外的模型来预测状态变化。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>评估协议</strong>：DA-DP鼓励在报告策略性能时，不仅说明任务难度，还应将其作为实测延迟的函数，这能更真实地反映其在现实系统中的适用性。</li>
<li><strong>通用模式</strong>：该方法框架与策略架构无关，其“测量延迟、校正数据、条件化训练”的模式可迁移至扩散策略以外的其他模仿学习方法，为解决机器人控制中的时序对齐问题提供了一个通用思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决机器人控制中因感知、计算等环节导致的推理延迟（数十至数百毫秒）问题，该延迟造成观察与执行状态不一致，严重影响动态任务性能。作者提出延迟感知扩散策略（DA-DP），通过在训练与推理中显式纳入延迟测量，将零延迟轨迹校正为延迟补偿版本，并对策略进行延迟条件化增强。实验表明，DA-DP在多种任务、机器人及延迟条件下，比无视延迟的方法成功率更高、鲁棒性更强，如在乒乓球任务中能成功击球而基线方法失败。该框架架构无关，可推广至其他模仿学习方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.07697" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>