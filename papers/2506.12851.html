<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.12851" target="_blank" rel="noreferrer">2506.12851</a></span>
        <span>作者: Xie, Weiji, Han, Jinrui, Zheng, Jiakun, Li, Huanyu, Liu, Xinzhe, Shi, Jiyuan, Zhang, Weinan, Bai, Chenjia, Li, Xuelong</span>
        <span>日期: 2025/06/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人因其类人的形态，在模仿人类行为执行各种任务方面具有巨大潜力。目前，基于强化学习（RL）的全身控制框架通常将参考运动作为输入，输出机器人的控制动作以进行模仿。然而，现有方法面临两个关键局限：其一，从人类捕获的运动序列可能违反人形机器人的物理约束（如关节极限、动力学和运动学），直接训练策略往往失败；其二，缺乏对难以跟踪的运动的合适容忍机制。因此，现有方法只能跟踪低速、平滑的运动，无法掌握功夫、舞蹈等高动态行为。本文针对这些痛点，提出了基于物理的人形运动控制框架。其核心思路是通过一个多步骤的运动处理流水线确保运动数据的物理可行性，并设计一个自适应运动跟踪机制，通过双级优化动态调整跟踪精度容差，形成一个自适应课程，从而实现对高度动态动作的模仿。</p>
<h2 id="方法详解">方法详解</h2>
<p>PBHC框架包含三个核心部分：从视频中提取运动并进行多步骤处理、基于最优跟踪因子的自适应运动跟踪，以及RL训练框架。整体目标是处理原始人类视频，生成适合机器人跟踪的参考运动，并训练一个能够精确模仿且可部署到实物的策略。</p>
<p><img src="https://arxiv.org/html/2506.12851v2/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：PBHC方法整体框架。包括：（a）从视频提取运动并进行多步骤运动处理，（b）基于最优跟踪因子的自适应运动跟踪，（c）RL训练框架和仿真到实物的部署。</p>
</blockquote>
<p><strong>1. 运动处理流水线</strong>：该流水线旨在从视频中提取运动并确保其物理可行性，包含四个步骤：</p>
<ul>
<li><strong>运动估计</strong>：使用GVHMR模型从单目视频估计SMPL格式的运动参数，该模型通过重力-视图坐标系对齐和预测脚部静止概率来提高运动质量。</li>
<li><strong>基于物理的运动过滤</strong>：为排除违反物理和生物力学约束的运动，基于IPMAN工作，计算每一帧人体模型重心与压力中心在地面的投影距离。通过设定稳定性阈值和最大连续不稳定帧数阈值，过滤掉明显无法保持动态稳定性的运动序列。</li>
<li><strong>基于接触掩码的运动校正</strong>：首先通过分析连续帧间脚踝位移的零速度假设来估计脚部接触掩码。对于被判定为接触的帧，通过减去SMPL网格顶点中最低的z坐标来校正全局平移的垂直位置，以消除轻微的浮动伪影，并使用指数移动平均进行平滑处理。</li>
<li><strong>运动重定向</strong>：采用基于微分逆运动学的方法，将处理后的SMPL格式运动重定向到G1机器人，确保末端执行器轨迹对齐并尊重关节极限。此外，还从AMASS和LAFAN等开源数据集中引入额外数据以增强运动多样性。</li>
</ul>
<p><strong>2. 自适应运动跟踪</strong>：这是本文的核心创新点，旨在解决固定跟踪容差难以应对不同难度运动的问题。</p>
<ul>
<li><strong>指数形式跟踪奖励</strong>：除足部接触跟踪项外，任务特定奖励（如关节角度对齐）采用形式为 $r(x) = \exp(-x/\sigma)$ 的指数奖励，其中 $x$ 是跟踪误差（如均方误差），$\sigma$ 是控制误差容忍度的<strong>跟踪因子</strong>。该形式有界，有助于稳定训练，并能更直观地进行奖励加权。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.12851v2/x2.png" alt="跟踪因子影响"></p>
<blockquote>
<p><strong>图2</strong>：跟踪因子 $\sigma$ 对奖励值的影响。当 $\sigma$ 远大于 $x$ 的典型范围时，奖励接近1，对 $x$ 变化不敏感；当 $\sigma$ 过小时，奖励接近0，敏感性也降低。选择合适的 $\sigma$ 对提高响应性和跟踪精度至关重要。</p>
</blockquote>
<ul>
<li><strong>最优跟踪因子推导</strong>：本文将跟踪因子选择问题建模为一个双级优化问题。<strong>下层优化</strong>模拟标准RL过程，在给定 $\sigma$ 下，策略通过最大化奖励（包括跟踪奖励 $J^{\mathrm{in}}$ 和其他奖励效应 $R(\bm{x})$）来产生最优跟踪误差序列 $\bm{x}^*$。<strong>上层优化</strong>则是在RL循环之外，选择 $\sigma$ 以最小化最终收敛策略的总跟踪误差 $J^{\mathrm{ex}} = -\sum x_i^*$。在一定的技术假设下，求解该问题得到理论上的最优跟踪因子为平均最优跟踪误差：$\sigma^* = (\sum_{i=1}^{N} x_i^*) / N$。</li>
<li><strong>自适应机制</strong>：由于 $\sigma^*$ 与 $\bm{x}^*$ 存在循环依赖且无法直接计算，本文设计了一个在训练中动态调整 $\sigma$ 的反馈机制。该机制维护一个跟踪误差的指数移动平均值 $\hat{x}$ 作为当前策略下期望跟踪误差的在线估计。在每一步，将 $\sigma$ 更新为当前 $\hat{x}$ 的值（$\sigma \leftarrow \min(\sigma, \hat{x})$），并约束 $\sigma$ 非递增。这形成了一个闭环：跟踪误差降低导致 $\sigma$ 收紧，进而驱动策略进一步优化，最终收敛到一个合适的 $\sigma$ 值。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.12851v2/x3.png" alt="自适应机制"></p>
<blockquote>
<p><strong>图3</strong>：所提自适应机制中跟踪因子的闭环调整过程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.12851v2/x4.png" alt="自适应机制效果示例"></p>
<blockquote>
<p><strong>图4</strong>：“马步冲拳”动作中右手y轴位置跟踪示例。自适应 $\sigma$ 能够逐步提高跟踪精度。图中 $\sigma_{\mathrm{pos_vr}}$ 用于跟踪头部和手部。</p>
</blockquote>
<p><strong>3. RL训练框架</strong>：</p>
<ul>
<li><strong>非对称演员-评论家架构</strong>：<strong>演员</strong>的观测仅包括机器人本体感知（5步历史的关节位置、速度、根角速度、根投影重力、上一时刻动作）和时间相位变量。<strong>评论家</strong>的观测则额外包含参考运动位置、根线速度以及一组随机化的物理参数，利用特权信息提升价值估计。</li>
<li><strong>奖励向量化</strong>：将奖励 $r$ 和价值函数 $V(s)$ 向量化为多分量形式（$\bm{r}=[r_1,\ldots,r_n]$, $\bm{V}(s)=[V_1(\bm{s}),\ldots,V_n(\bm{s})]$），每个奖励分量由一个独立的价值函数头估计，然后汇总计算动作优势。这实现了精确的价值估计并促进了稳定的策略优化。</li>
<li><strong>参考状态初始化与仿真到实物迁移</strong>：采用参考状态初始化技术，从参考运动状态中随机采样初始化机器人状态，以并行学习不同运动阶段。为弥合仿真到实物的差距，采用了域随机化技术，并在部署前进行仿真到仿真的测试，实现了零样本的仿真到实物迁移，无需微调。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在IsaacGym仿真环境中进行评估。使用通过本文流水线构建的高度动态运动数据集（示例如图5），并根据敏捷性要求将动作分为简单、中等、困难三个难度等级。评估指标包括全局平均每身体位置误差、根相对平均每身体位置误差、平均每关节位置误差、平均每关节速度误差、平均每身体速度误差和平均每身体加速度误差。</p>
<p><img src="https://arxiv.org/html/2506.12851v2/x5.png" alt="示例动作"></p>
<blockquote>
<p><strong>图5</strong>：构建的数据集中的示例动作。不透明度越深表示时间戳越晚。</p>
</blockquote>
<p><strong>1. 运动过滤效果（Q1）</strong>：对10个运动序列应用基于物理的过滤，其中4个被拒绝，6个被接受。通过训练单独策略并计算情节长度比（ELR，平均情节长度与参考运动长度之比）进行评估。</p>
<p><img src="https://arxiv.org/html/2506.12851v2/x6.png" alt="过滤效果分布"></p>
<blockquote>
<p><strong>图6</strong>：被接受和被拒绝运动的ELR分布。被接受的运动始终获得高ELR，而被拒绝的运动最大ELR仅为54%，表明过滤方法能有效排除本质上无法跟踪的运动。</p>
</blockquote>
<p><strong>2. 主要结果对比（Q2）</strong>：与OmniH2O、Exbody2和MaskedMimic基线方法进行比较。PBHC在几乎所有评估指标上均一致优于可部署的基线方法（OmniH2O和ExBody2）。虽然MaskedMimic在某些指标上表现良好，但它是为角色动画设计的，未考虑部分可观测性和动作平滑性等机器人控制约束。为了公平比较，论文训练了一个同样忽略这些约束的PBHC Oracle版本，其性能接近MaskedMimic。结果表明，PBHC在保持可部署性的同时，性能接近Oracle水平。</p>
<p><strong>3. 自适应机制分析（Q3）</strong>：通过训练曲线展示了自适应机制的优势。</p>
<p><img src="https://arxiv.org/html/2506.12851v2/Fig/exp/training_curve.png" alt="训练曲线对比"></p>
<blockquote>
<p><strong>图9</strong>：与固定跟踪因子方法的训练曲线对比。自适应机制（蓝色）能更快达到更高且更稳定的跟踪奖励。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.12851v2/Fig/exp/training_curve2.png" alt="不同难度训练曲线"></p>
<blockquote>
<p><strong>图10</strong>：不同难度动作下的训练曲线。自适应机制在不同难度动作上均能有效工作，对于困难动作（右）提升尤为明显。</p>
</blockquote>
<p><strong>4. 实物部署结果（Q4）</strong>：将训练好的策略零样本部署到Unitree G1实物机器人上，成功完成了包括功夫和舞蹈在内的多种高度动态、富有表现力的行为，证明了方法的有效性。</p>
<p><img src="https://arxiv.org/html/2506.12851v2/x11.png" alt="实物部署"></p>
<blockquote>
<p><strong>图13</strong>：PBHC在Unitree G1实物机器人上成功执行高度动态动作（如功夫）的演示。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>系统化的运动处理流水线</strong>：提出了一套从视频到机器人的完整运动处理流程，通过基于物理的过滤和接触感知的校正，最大程度确保参考运动的物理可行性。</li>
<li><strong>自适应运动跟踪机制</strong>：创新性地将跟踪因子选择建模为双级优化问题，并设计了在线自适应调整机制，动态优化跟踪精度容差，有效解决了不同难度运动的跟踪问题。</li>
<li><strong>成功的零样本仿真到实物迁移</strong>：通过非对称演员-评论家架构、奖励向量化和域随机化等技术，实现了策略在高度动态任务上的零样本实物部署。</li>
</ol>
<p><strong>局限性</strong>：论文提到其方法依赖于运动估计模型（如GVHMR）的质量，估计误差会影响后续处理。此外，自适应机制虽然减少了手动调参，但其更新规则中的超参数（如EMA系数）仍需设置。</p>
<p><strong>研究启示</strong>：本文的自适应课程机制可以扩展到更广泛的机器人技能学习任务中。运动处理流水线与近年来兴起的人形基础模型相结合，有望高效利用海量人类视频数据。此外，探索更紧凑、物理信息更丰富的运动表示，可能进一步提升学习效率和性能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人模仿高度动态人类动作（如功夫、舞蹈）的难题，提出**KungfuBot**物理仿真全身控制框架。核心方法包括：**多步骤运动处理流程**（提取、过滤、校正与重定向，确保物理可行性）和**自适应运动跟踪**（通过双层优化动态调整跟踪容差，形成自适应课程）。实验表明，该方法在高度动态运动模仿中**跟踪误差显著低于现有方法**，并成功在Unitree G1实物机器人上实现了稳定、富有表现力的行为。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.12851" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>