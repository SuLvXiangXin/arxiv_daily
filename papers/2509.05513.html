<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.05513" target="_blank" rel="noreferrer">2509.05513</a></span>
        <span>作者: Yu Xiang Team</span>
        <span>日期: 2025-09-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从视频中学习为机器人模仿学习提供了可扩展的演示数据来源。然而，通用的网络视频源存在分布差距，其视角、手-物体接近度和运动模式与机器人部署环境不同，限制了技能的迁移。第一人称（自我中心）视频通过保持演员视角下手和操作物体始终在画面内，有效缩小了这一差距。尽管存在Ego4D、EPIC-KITCHENS等大规模自我中心数据集，但它们通常缺乏灵巧操作所需的手部姿态标注。另一方面，一些专注于操作的数据集（如HoloAssist、EgoDex）虽然提供了详细的手部或交互标注，但在规模、任务多样性或细粒度语言标注方面存在不足。具体而言，现有数据缺乏统一的、大规模且同时包含标准化灵巧手部姿态标注与细粒度、时间定位的意图对齐语言动作基元的数据集。</p>
<p>本文旨在填补这一空白，通过整合六个公开的自我中心数据集，构建了一个大规模、多模态的自我中心操作数据集OpenEgo。其核心思路是：统一不同来源的手部姿态标注格式（转换为标准的21关节布局和相机坐标系），并为所有数据提供描述性的、带时间戳的意图对齐动作基元，从而为训练世界模型、分层视觉-语言-动作模型和基础视觉语言模型提供支持。</p>
<h2 id="方法详解">方法详解</h2>
<p>OpenEgo数据集构建的整体流程是对六个来源不同的公开数据集进行统一处理，输出标准化格式的手部姿态序列和语言标注。核心模块包括手部关节统一化处理和语言基元标注生成。</p>
<p><img src="https://arxiv.org/html/2509.05513v1/high-low-task.png" alt="高低层任务标注示例"></p>
<blockquote>
<p><strong>图1</strong>：OpenEgo中高低层任务标注的示例。高层任务标签为“制作咖啡”，细粒度的动作基元则包括“拿起咖啡机水箱”、“将水箱移至水槽”等带时间戳的描述。</p>
</blockquote>
<p><strong>手部关节统一化</strong>：目标是将所有数据源的手部姿态统一为MANO模型的21关节布局（每根手指4个关节加手腕），并转换到相机坐标系中。处理流程根据原始数据的不同情况分为以下几类：</p>
<ol>
<li><strong>CaptainCook4D</strong>：无原生手部姿态标注。使用MediaPipe检测2D手部关键点，并结合RGB-D流中的每像素深度<code>z</code>和相机内参<code>K</code>，通过公式<code>X_c = z * K^{-1} * ũ</code>（其中<code>ũ = (u, v, 1)^⊤</code>）将2D点反投影到相机坐标系下的3D坐标。</li>
<li><strong>HOI4D</strong>：使用其发布的MANO参数，在提供的相机坐标系下获取MANO关键点。对于没有姿态的帧，回退到上述的2D检测加深度反投影方法。</li>
<li><strong>HoloAssist, HOT3D, HO-Cap</strong>：这些数据集提供了世界坐标系下的手部姿态以及每帧的相机外参（旋转<code>R</code>和平移<code>t</code>）。通过公式<code>X_c = R * X_w + t</code>将世界坐标下的关节<code>X_w</code>转换到相机坐标系<code>X_c</code>。</li>
<li><strong>EgoDex</strong>：手部姿态已在相机坐标系中，但其为25关节格式。通过丢弃四个非MANO关节并按MANO顺序重新索引，映射到21关节格式。</li>
</ol>
<p>对于所有数据源，都提供了一个二值可见性掩码<code>m</code>，以标记因遮挡或原始标注缺失而不可见的关节。</p>
<p><strong>语言基元</strong>：为数据提供与意图对齐的语言标注，由描述性的动作基元组成。每个基元指定了被操作的物体和动作，并带有从意图开始到执行完成的绝对时间戳（<code>t_start</code>, <code>t_end</code>）。例如：“拿起咖啡机水箱”、“右手拉开黑色相机包拉链，同时左手握住它”。操作条目包含执行者标签（<code>left_hand</code>, <code>right_hand</code>, <code>both_hands</code>）；导航条目使用<code>person</code>并带有目的地/物体。当物体可识别时直接命名，不确定时则用属性描述。仅标注直接观察到的动作，并保留动作之间的间隔。此外，还提供了高层任务标签（如图1所示）。此标注过程应用于所有数据源，即使已有细粒度标注的数据集，若其描述不同或不完整，也会进行补充或统一。</p>
<p><strong>创新点</strong>：与现有数据集相比，OpenEgo的核心创新在于大规模地<strong>统一</strong>了此前分散的灵巧操作标注资源。它不仅提供了标准化的3D手部轨迹，还首次为如此大规模的数据集配上了细粒度、时间定位、意图对齐的语言描述，弥合了视觉、语言与动作之间的数据鸿沟。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>为了验证OpenEgo的实用性，研究团队在其上训练了一个语言条件化的模仿学习策略，用于预测灵巧的3D手部轨迹。</p>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/数据集</strong>：OpenEgo数据集本身。训练使用了数据集的0.1%子集（由于计算限制），并留出10%的演示序列进行评估。</li>
<li><strong>实验平台</strong>：2张NVIDIA RTX 4090 GPU。</li>
<li><strong>Baseline方法</strong>：本实验主要作为数据集效用的验证，未与其他方法进行横向对比，而是评估了在同一模型架构下，预测不同时间范围的表现。</li>
<li><strong>模型与训练</strong>：采用基于ViLT架构的策略网络。给定RGB观测<code>x_t</code>、描述预期操作的语言提示<code>ℓ</code>和当前手部关节状态<code>q_t</code>，策略预测未来<code>T</code>步的动作<code>q̂_{t+1:t+T}</code>。损失函数为掩码均方误差（MSE），仅对可见关节计算误差。使用AdamW优化器，余弦退火学习率调度，批量大小为896，训练15,000步。</li>
</ul>
<p><strong>评估指标与结果</strong>：在留出的评估集上报告序列预测质量，仅使用标记为可见的关节。采用三个标准轨迹指标：(i) <strong>AED</strong>（平均欧氏距离）、(ii) <strong>FED</strong>（最终步欧氏距离）、(iii) <strong>DTW</strong>（动态时间规整距离）。结果如下表所示（对应论文表2）。</p>
<table>
<thead>
<tr>
<th align="left">预测范围 (帧数 @15fps)</th>
<th align="left">平均距离 (AED) ↓</th>
<th align="left">最终距离 (FED) ↓</th>
<th align="left">DTW ↓</th>
</tr>
</thead>
<tbody><tr>
<td align="left">8 帧 (0.5秒)</td>
<td align="left">0.0491</td>
<td align="left">0.0633</td>
<td align="left">0.3918</td>
</tr>
<tr>
<td align="left">16 帧 (1.0秒)</td>
<td align="left">0.0714</td>
<td align="left">0.0795</td>
<td align="left">1.1284</td>
</tr>
<tr>
<td align="left">32 帧 (2.0秒)</td>
<td align="left">0.0893</td>
<td align="left">0.0926</td>
<td align="left">2.7150</td>
</tr>
<tr>
<td align="left">64 帧 (4.0秒)</td>
<td align="left">0.1045</td>
<td align="left">0.1076</td>
<td align="left">6.7975</td>
</tr>
</tbody></table>
<p><strong>关键结果分析</strong>：</p>
<ol>
<li><strong>预测误差随范围增加</strong>：如表所示，AED和FED均随着预测时间范围的延长而平稳增加，这表明模型在短期预测上表现更佳，符合直觉。</li>
<li><strong>DTW增长更快</strong>：DTW距离的增长速度明显快于AED/FED，这反映了DTW指标对预测序列与真实序列之间时间错位的敏感性，时间范围越长，对齐成本越高。</li>
<li><strong>数据集有效性</strong>：实验结果证明，在OpenEgo上训练的模型能够有效学习短时程的灵巧运动，且任务难度随预测范围平滑扩展，表明该数据集提供了结构化的学习信号。</li>
</ol>
<p><strong>消融实验</strong>：本文未进行针对数据集组件的消融实验，因为其主要贡献是数据集的构建与整合。实验的核心目的是展示利用该统一数据集能够训练出有效的预测模型。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>大规模统一数据集</strong>：构建了OpenEgo，一个包含1107小时、119.6M帧、覆盖290个任务的超大规整第一人称操作数据集，整合了六个来源的数据。</li>
<li><strong>标准化灵巧标注</strong>：将所有数据的手部姿态统一为相机坐标系下的21关节MANO布局，并提供了关节可见性掩码。</li>
<li><strong>意图对齐语言标注</strong>：为整个数据集提供了细粒度、带时间戳的描述性动作基元，桥接了高层语言指令与低层动作轨迹。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>部分帧由于遮挡或原始标注缺失，手部关节数据不完整。</li>
<li>语言标注是自动生成且仅部分经过人工验证，对于冗长或模糊的动作可能存在时间漂移。</li>
<li>对于原本缺乏灵巧标注的数据源（如CaptainCook4D），其3D关节质量依赖于2D关键点检测器和深度传感器的精度。</li>
<li>验证实验仅在0.1%的数据子集上使用单一架构进行，结果不应被视为性能上限。</li>
</ol>
<p><strong>对后续研究的启示</strong>：<br>OpenEgo通过提供大规模、对齐的多模态数据，为多个研究方向铺平了道路：1) 训练更强大的<strong>视觉-语言-动作基础模型</strong>，特别是需要细粒度动作理解的分层模型；2) 开发能够从第一人称视频中直接学习<strong>灵巧操作技能</strong>的模仿学习或强化学习算法；3) 作为<strong>基准测试平台</strong>，用于评估轨迹预测、动作识别或基于视频的技能学习等任务。数据集的统一格式也极大地促进了该领域研究的可复现性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有自我中心视角（egocentric）视频数据集缺乏精细手部标注与动作描述的问题，提出了大规模多模态数据集OpenEgo。该数据集整合了六个公共数据集，总计1107小时视频，涵盖290个操作任务和600多个环境，关键技术包括统一21关节手部姿态标注和带有时间戳的意图对齐动作原语。为验证其有效性，作者训练了语言条件模仿学习策略来预测灵巧的3D手部轨迹。OpenEgo旨在降低从自我中心视频学习灵巧操作的壁垒，并支持视觉-语言-动作学习的可复现研究。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.05513" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>