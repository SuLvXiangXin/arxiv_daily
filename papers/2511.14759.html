<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>$π^{*}_{0.6}$ : a VLA That Learns From Experience - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>$π^{*}_{0.6}$ : a VLA That Learns From Experience</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.14759" target="_blank" rel="noreferrer">2511.14759</a></span>
        <span>作者: Zhiyuan Zhou Team</span>
        <span>日期: 2025-11-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通过提示词灵活指定任务的通用视觉-语言-动作模型已成为通用机器人的基础模型。然而，主流方法依赖于从离线演示数据中进行模仿学习，这存在复合误差问题，且性能上限受限于演示数据的质量。如同人类掌握技能需要练习一样，VLA模型也需要通过自主实践来达到精通，即利用自主收集的经验数据来纠正实际部署中的错误、提升速度与鲁棒性、并适应新的部署条件。</p>
<p>本文针对如何将强化学习原则实例化为一个通用且可扩展的机器人学习系统这一具体痛点，提出了新的视角。其核心挑战包括：为大型模型设计可扩展且稳定的RL方法、处理来自不同策略的异构数据、以及在奖励信号可能模糊或随机的真实世界中设置带有奖励反馈的RL训练。本文的核心思路是提出一种名为Recap的通用方法，通过结合演示数据、自主收集的经验以及专家在自主执行期间的遥操作干预，利用优势条件化策略对VLA模型进行RL训练，使其能够在从预训练到自主执行数据训练的全流程中融入奖励反馈。</p>
<h2 id="方法详解">方法详解</h2>
<p>Recap方法旨在通过一个可迭代的流程改进基础VLA模型，其整体流程包含三个步骤，并可重复多次：1. <strong>数据收集</strong>：运行VLA执行任务，为每个回合标注任务结果标签（用于确定奖励），并可选择性地提供人工干预以纠正早期迭代中的错误。2. <strong>价值函数训练</strong>：使用迄今为止收集的所有数据训练一个大型多任务价值函数，用于评估任务完成进度。3. <strong>优势条件训练</strong>：将基于该价值函数计算出的优势值二值化后，作为一个最优性指示器加入VLA的输入前缀中，通过这种“优势条件化”方法，利用次优数据从价值函数中提取出更优的策略。</p>
<p><img src="https://arxiv.org/html/2511.14759v2/figures/filmstrip.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：Recap训练过程整体概览。展示了从预训练到下游任务 specialization 的完整流程：首先在大型多任务演示数据集上预训练价值函数和策略，然后针对特定任务进行演示数据微调，接着进行多轮自主数据收集与RL训练迭代，不断改进策略。</p>
</blockquote>
<p>核心模块包括分布价值函数和基于优势条件的策略提取。<strong>分布价值函数</strong>采用与VLA策略相同但主干网络更小的架构，是一个语言条件化的分布价值函数。它将观测和语言指令映射到B个离散化的价值区间上，通过最小化经验回报离散值与该分布之间的交叉熵来训练。这实质上是数据集所代表的行为策略的价值函数的蒙特卡洛估计。从中可以提取出连续的价值估计用于计算优势。</p>
<p><img src="https://arxiv.org/html/2511.14759v2/figures/model_architecture.png" alt="模型架构与价值函数交互"></p>
<blockquote>
<p><strong>图2</strong>：π∗0.6 VLA与价值函数在Recap训练中的交互。π∗0.6 VLA使用预训练的VLM主干，遵循KI方法进行训练，包含对多种数据源的下一令牌预测，以及一个带停止梯度的流匹配动作专家。VLA以一个从独立价值函数获得的二值化优势指示器为条件。</p>
</blockquote>
<p><strong>基于优势条件的策略提取</strong>是方法的关键创新。该方法基于一个理论结果：若定义一个改进策略π̂，其正比于参考策略π_ref与基于优势的改进概率p(I|A)的β次方的乘积，则该策略的性能保证不差于π_ref。通过应用贝叶斯规则，可以将改进概率转化为策略的条件分布之比。具体地，假设改进指示器I服从一个基于任务相关阈值ε_ℓ的 delta 分布，即当优势大于阈值时I为真。策略的训练目标则是最小化以下负对数似然：既要拟合给定观测和指令的动作，也要拟合给定观测、指令以及改进指示器I为真时的动作。在训练中，对于人工干预提供的纠正动作，强制将其I设为真。该方法支持高容量的流匹配VLA模型，并能有效利用所有数据（包括好的和坏的经验），避免了策略梯度方法的复杂性和加权回归方法对数据的丢弃。</p>
<p><img src="https://arxiv.org/html/2511.14759v2/x1.png" alt="价值函数可视化示例"></p>
<blockquote>
<p><strong>图3</strong>：价值函数可视化。训练一个多任务价值函数来预测到成功所需的步数（归一化到(-1,0)，0对应成功完成）。左图展示了一个成功叠衣服任务的价值函数输出，右图展示了一个预训练数据集中不成功的操作任务。红色高亮部分表示价值下降（错误），绿色部分表示价值上升（进展），价值函数能正确识别错误和进展速度。</p>
</blockquote>
<p>与现有方法相比，Recap的创新点具体体现在：1. <strong>支持高容量模型</strong>：支持基于扩散或流匹配的VLA端到端训练，而非简单的离散动作或高斯连续动作模型。2. <strong>避免在线策略梯度</strong>：采用优势条件化策略提取，摆脱了对PPO或REINFORCE等在线策略梯度方法的依赖，可以更简单地利用所有先前的离线和离线策略数据。3. <strong>处理异构数据</strong>：将演示、干预和自主策略 rollout 等多种数据源整合到一个迭代的离线RL框架中。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个复杂的真实世界长视野、细粒度操作任务上进行：<strong>叠衣服</strong>（处理可变形的衣物）、<strong>组装纸箱</strong>（处理可弯曲、粘连的纸板）和<strong>使用专业咖啡机制作意式浓缩咖啡</strong>（涉及倾倒液体）。实验平台涉及真实的家庭、工厂和咖啡店环境。</p>
<p>对比的基线方法包括：1. <strong>模仿学习</strong>：仅在演示数据上微调。2. <strong>PPO</strong>：使用近端策略优化进行在线RL微调。3. <strong>AWR</strong>：优势加权回归，一种加权模仿学习方法。4. <strong>仅干预</strong>：仅使用人类干预数据（无自主经验）进行训练。5. <strong>无干预</strong>：仅使用自主经验（无干预）进行训练。</p>
<p>关键实验结果如下：在多个任务上，使用完整Recap方法训练的π∗0.6模型在吞吐量和成功率上均显著优于基线。例如，在“叠衣服（多样化-最难物品）”任务中，Recap的吞吐量达到约1.6件/小时，而模仿学习基线约为0.7件/小时，提升超过一倍；同时，失败率从约40%降至约20%，减半。在“制作意式浓缩咖啡”任务中，Recap的吞吐量约为5杯/小时，模仿学习约为2.5杯/小时，同样翻倍；失败率从约30%降至约10%。</p>
<p><img src="https://arxiv.org/html/2511.14759v2/figures/experiment1/Laundry_T-Shirts_and_Shorts_throughput_plot.png" alt="叠衣服（T恤和短裤）任务吞吐量对比"></p>
<blockquote>
<p><strong>图6</strong>：叠衣服（T恤和短裤）任务吞吐量对比。Recap（紫色）相比模仿学习（绿色）和PPO（红色）有显著提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.14759v2/figures/experiment1/Make_Espresso_success_rate_plot.png" alt="制作意式浓缩咖啡任务成功率对比"></p>
<blockquote>
<p><strong>图12</strong>：制作意式浓缩咖啡任务成功率对比。Recap（紫色）的成功率最高，且随着训练迭代保持稳定或提升，而PPO（红色）出现崩溃。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：1. <strong>数据组成</strong>：对比了“仅演示”、“演示+干预”、“演示+自主经验”以及“全部（Recap）”四种数据配置。结果显示，结合了所有数据源的Recap性能最佳，仅使用自主经验（无干预）也能带来显著提升，但干预数据进一步提高了性能。</p>
<p><img src="https://arxiv.org/html/2511.14759v2/figures/experiment3/Laundry_T-Shirts_and_Shorts_throughput_plot.png" alt="不同数据组成对叠衣服任务吞吐量的影响"></p>
<blockquote>
<p><strong>图14</strong>：消融研究：不同数据组成对叠衣服（T恤和短裤）任务吞吐量的影响。“All (Recap)”使用全部数据源，性能最好。</p>
</blockquote>
<ol start="2">
<li><strong>策略提取方法</strong>：将Recap的优势条件化方法与策略梯度方法进行对比。在相同数据和价值函数下，优势条件化方法显著优于策略梯度方法，证明了其在大规模VLA模型上的可扩展性和有效性。</li>
<li><strong>故障模式移除</strong>：通过分析自主运行中发现的特定故障模式（如抓取衣服时夹持器位置不佳），并针对性收集纠正数据重新训练，可以几乎完全消除该故障，将相关失败率从约15%降至接近0%，同时提升吞吐量。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.14759v2/figures/experiment5/Laundry_Remove_Failure_Mode_success_rate_plot.png" alt="通过针对性训练移除特定叠衣服故障模式"></p>
<blockquote>
<p><strong>图20</strong>：通过针对性训练移除特定叠衣服故障模式。在识别并收集针对“夹持器放置”故障的数据后重新训练，成功将该模式下的失败率降至接近零。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1. <strong>提出了一个可扩展的框架</strong>：Recap为大型VLA模型的强化学习训练提供了一个通用框架，支持从预训练到在线改进的端到端流程，并能整合演示、自主经验和专家干预等多种异构数据。2. <strong>展示了强大的实际性能</strong>：通过Recap训练的π∗0.6模型能够在真实家庭、工厂和商业场景中可靠地完成叠衣服、组装纸箱和制作咖啡等复杂、长视野的任务，达到实用级别的鲁棒性（如连续运行13小时制作咖啡）。3. <strong>验证了优势条件化策略提取的有效性</strong>：该方法避免了传统策略梯度的复杂性，能稳定、高效地利用所有数据提升流匹配VLA模型的性能，显著优于对比方法。</p>
<p>论文自身提到的一个局限性是，其价值函数使用了同策略的蒙特卡洛估计器，而非更优的异策略Q函数估计器。尽管当前方法已足够简单可靠并能带来实质改进，但未来工作可以扩展以纳入异策略估计器。</p>
<p>本文对后续研究的启示包括：1. <strong>为VLA的RL训练提供了通用配方</strong>：证明了结合离线RL预训练和在线数据收集的迭代框架对于提升VLA在实际任务中的性能至关重要。2. <strong>展示了离线RL与优势条件化的有效性</strong>：为在大规模生成式模型上进行策略优化提供了简洁且可扩展的替代方案。3. <strong>强调了实践数据对掌握复杂技能的重要性</strong>：如同人类学习，自主实践并结合专家纠正是机器人模型实现精通的必要途径，这为构建真正通用且鲁棒的机器人智能指明了方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何通过强化学习（RL）让视觉-语言-动作（VLA）模型在现实部署中持续改进。提出通用方法 **Recap（基于优势条件策略的经验与修正强化学习）**，通过优势条件整合演示数据、在线收集数据及自主执行中的专家干预数据，实现VLA的RL训练。实验表明，经Recap训练的模型能在真实家庭中叠衣服、组装纸箱、操作专业咖啡机。在最难任务上，**Recap使任务吞吐量提升一倍以上，失败率降低约一半**。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.14759" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>