<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sim2Real Transfer for Vision-Based Grasp Verification - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Sim2Real Transfer for Vision-Based Grasp Verification</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.03046" target="_blank" rel="noreferrer">2505.03046</a></span>
        <span>作者: Amargant, Pau, Hönig, Peter, Vincze, Markus</span>
        <span>日期: 2025/05/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作中，抓取成功与否的验证至关重要，尤其是在处理可变形物体时。传统方法依赖物体的几何形状以及力与触觉传感器，但难以应对物体的形变、缺乏内部结构和抗力等特性。虽然已有研究将2D/3D视觉用于绳索、布料等物体的抓取操作，但这些方法多聚焦于抓取控制反馈，且通常针对特定机器人、物体和任务，其复杂性和约束使其不适用于通用的抓取验证任务。</p>
<p>本文针对上述痛点，旨在开发一种基于视觉的、易于适应不同机器人和任务的抓取验证方法，以确定机器人夹爪是否成功抓取了物体，并特别关注可变形物体。本文的核心思路是：提出一个结合目标检测与图像分类的两阶段架构（GraspCheckNet），并引入一个专为抓取验证设计的合成数据集（HSR-GraspSynth）来解决真实数据收集的难题，同时探索视觉问答模型作为零样本基线的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的抓取验证方法GraspCheckNet采用两阶段架构。输入为机器人头部摄像头捕获的RGB图像。第一阶段，使用基于YOLO的目标检测模型定位图像中的机器人夹爪，输出为夹爪的边界框。第二阶段，将检测到的边界框区域裁剪出来，输入到一个基于ResNet的分类模型中，由该模型进行二分类（0表示夹爪中有物体，1表示夹爪为空），最终输出抓取验证结果。</p>
<p><img src="https://arxiv.org/html/2505.03046v1/extracted/6410942/figures/model_architecture.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：两阶段模型架构示意图。YOLO目标检测模型定位图像中的机器人夹爪，ResNet分类模型利用裁剪后的图像判断夹爪中是否有物体。</p>
</blockquote>
<p>核心模块包括目标检测模块、分类模块以及用于训练的数据生成模块。</p>
<ol>
<li><strong>目标检测模块</strong>：采用预训练的YOLO11-l模型，在HSR-GraspSynth合成数据集上进行微调。训练时使用了透视与仿射变换、颜色抖动、亮度对比度变化和图像压缩等数据增强技术以减小Sim2Real差距。在真实图像上推理时，由于存在域差距，需要逐步降低置信度阈值直到出现检测框。为解决低阈值产生的多个假阳性检测框，采用了基于密度的空间聚类（DBSCAN）进行后处理，识别检测框簇，并选择总置信度最高簇中置信度最高的边界框作为最终检测结果。</li>
<li><strong>图像分类模块</strong>：采用预训练的ResNet-18模型，将其头部替换为两个带有ReLU激活函数和Dropout层的全连接层，以适应二分类任务。模型使用合成数据集中包含夹爪的真实裁剪图像进行训练，并同样应用了数据增强。训练分阶段进行：首先以高Dropout率（0.7至0.5）仅训练头部以提高鲁棒性，随后降低学习率和Dropout率并解冻主干网络的最后一层。</li>
<li><strong>HSR-GraspSynth合成数据集</strong>：为解决真实数据收集困难，本文使用BlenderProc工具生成该数据集。每个场景随机配置机器人模型、背景干扰物（2-15个，训练集来自ShapeNetV2，验证集来自YCB-V）以及随机扰动的机械臂姿态和相机朝向。以0.5的概率在夹爪中随机放置一个物体，生成包含物体（<code>object</code>）和不包含物体（<code>no_object</code>）的样本。数据集包含12,000张训练图像和5,000张验证图像，每张图像都标注了夹爪边界框和二元标签。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03046v1/extracted/6410942/figures/pos_000000_000671.png" alt="数据集示例"></p>
<blockquote>
<p><strong>图1</strong>：HSR-GraspSynth数据集示例。上行显示夹爪中有物体的示例，下行显示夹爪为空的示例。每个示例来自不同的批次，背景包含不同的干扰物。</p>
</blockquote>
<p>与现有方法相比，本文的创新点在于：1) 提出了一种通用的、不依赖夹爪摄像头（可使用头部摄像头）的两阶段视觉验证架构；2) 创建了首个专门针对抓取验证任务的、涵盖成功与失败抓取场景的合成数据集；3) 系统性地评估了先进视觉问答模型在此任务上作为零样本基线的潜力与局限性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台使用了丰田HSR机器人。评估使用了HSR-GraspSynth合成数据集和一个包含518张真实图像的数据集（158张空夹爪，150张抓取16种刚性物体，210张抓取23种可变形物体）。对比的基线方法是作为零样本分类器的视觉问答模型，具体测试了GPT-4o和Llama 3.2 Vision 11B。</p>
<p><strong>关键实验结果如下：</strong></p>
<ol>
<li><strong>目标检测性能</strong>：在真实数据集上，检测模型对空夹爪、含刚性物体、含可变形物体图像的定性检测正确率分别为98.10%、94.67%和96.67%。但评估未使用IoU指标，观察到预测框在垂直方向上有时未能完全包含夹爪，推理时通过填充边界框进行缓解。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03046v1/extracted/6410942/figures/examples/old_negative_000095.jpg" alt="检测示例"></p>
<blockquote>
<p><strong>图4</strong>：目标检测模型在真实图像上的检测示例。红色矩形为检测框。上行和下行分别展示了正确和错误的检测结果。</p>
</blockquote>
<ol start="2">
<li><p><strong>分类性能</strong>：GraspCheckNet在空夹爪、刚性物体、可变形物体上的分类准确率分别为74.7%、86.7%和82.9%。在检测空夹爪（即抓取失败）的二分类任务中，精确率为0.678，召回率为0.749。论文指出，由于更关注检测抓取失败，因此更看重高召回率。</p>
</li>
<li><p><strong>与VQA基线对比</strong>：GPT-4o在空夹爪和刚性物体上准确率很高（95.0%， 95.3%），但在可变形物体上准确率降至78.1%，低于GraspCheckNet的82.9%。Llama 3.2 11B性能较差（空夹爪48.7%，刚性68.7%，可变形60.0%）。GPT-4o对空夹爪的召回率高达0.95，但产生了较多将可变形物体误判为空夹爪的假阳性。</p>
</li>
</ol>
<p><strong>消融实验与组件贡献</strong>：虽然没有严格的消融实验，但论文通过设计两阶段流程强调了目标检测模块对聚焦感兴趣区域、提升模型通用性的作用。数据增强和DBSCAN后处理被证明对缓解Sim2Real差距和改善检测鲁棒性至关重要。与VQA基线的对比则凸显了本文所提方法在部署成本（本地运行 vs. API调用）、推理速度（28ms vs. 2.27s平均）以及对特定类别（如纹理单一的可变形物体）识别上的相对优势。</p>
<p><img src="https://arxiv.org/html/2505.03046v1/extracted/6410942/figures/clustered_drape_1.jpg" alt="聚类后处理"></p>
<blockquote>
<p><strong>图3</strong>：对检测到的边界框应用聚类程序。左图：使用DBSCAN识别聚类并为每个边界框分配聚类标签。右图：随后，从总置信度得分最高的聚类中选择置信度最高的边界框作为最终检测结果。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了GraspCheckNet，一个基于头部摄像头、结合目标检测与分类的两阶段视觉抓取验证架构，特别适用于可变形物体，并具备跨平台潜力；2) 构建并开源了HSR-GraspSynth，一个专为抓取验证设计的大规模合成数据集，以解决真实数据稀缺问题；3) 首次系统评估了先进VQA模型作为抓取验证零样本基线的性能，揭示了其优势与当前局限性（如对特定可变形物体识别困难、延迟高、需网络连接）。</p>
<p>论文自身提到的局限性包括：Sim2Real差距仍然存在（如检测框定位不完美）；模型在检测空夹爪时的精确率有待提高；目前尚未与完整的抓取管道进行实时集成测试。</p>
<p>本文对后续研究的启示包括：未来工作可专注于将验证模型深度集成到实时抓取管道中，探索闭环验证如何提升抓取成功率；可采用有监督或无监督的域适应技术进一步缩小Sim2Real差距；此外，如何将VQA模型的强大视觉推理能力与轻量级专用模型的高效性相结合，也是一个值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人抓取验证，特别是可变形物体的抓取，提出一种基于视觉的解决方案。传统依赖力/触觉传感器的方法难以处理此类物体。作者设计了一个两阶段模型：首先使用YOLO检测并定位机器人夹爪，随后通过ResNet分类器判断物体是否被抓取。为克服真实数据获取限制，构建了合成数据集HSR-GraspSynth。实验表明，该方法在真实环境中实现了高精度抓取验证，具备集成到抓取流程中的潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.03046" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>