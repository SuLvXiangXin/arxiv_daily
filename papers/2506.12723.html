<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.12723" target="_blank" rel="noreferrer">2506.12723</a></span>
        <span>作者: Wenwu Zhu Team</span>
        <span>日期: 2025-06-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLA）在多种任务上展现出强大能力，但其部署面临显著的计算瓶颈。主流VLA模型通常采用两阶段架构：首先，一个大型视觉编码器（如ViT）将输入图像转换为一系列视觉令牌；然后，这些视觉令牌与文本令牌一起输入到一个大型语言模型（LLM）中进行理解与推理。该架构的关键局限性在于：视觉编码器的计算成本高昂，且其生成的视觉令牌中通常包含大量冗余信息，导致后续LLM处理效率低下。现有加速方法主要分为两类：一类侧重于模型调度，即在运行时根据输入动态选择轻量级视觉编码器，但未对令牌本身进行压缩；另一类侧重于令牌剪枝，即静态地移除部分视觉令牌，但未能根据输入内容动态调整模型复杂度。</p>
<p>本文针对上述痛点，提出了一种新的视角：将模型调度与令牌剪枝进行联合优化。核心思路是设计一个统一的框架，在推理时根据输入图像的复杂度，协同决策使用何种视觉编码器以及保留多少视觉令牌，从而在保证性能的前提下最大化计算效率。</p>
<h2 id="方法详解">方法详解</h2>
<p>SP-VLA的整体框架是一个两阶段决策管道，旨在为每个输入图像动态选择最优的视觉编码器和视觉令牌保留策略。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_07_18_3a6e9c4b5d5c5b8e5e5cg-1.jpg?height=546&width=1294&top_left_y=554&top_left_x=354" alt="SP-VLA Framework"></p>
<blockquote>
<p><strong>图1</strong>：SP-VLA方法整体框架。首先，一个轻量级的调度器网络接收原始图像，预测一个调度决策（选择视觉编码器）和一个初始的令牌保留比例。然后，被选中的视觉编码器处理图像生成视觉令牌序列。接着，一个轻量级的令牌剪枝器网络评估每个视觉令牌的重要性，并根据调度器预测的比例进行剪枝。最后，保留的重要令牌与问题文本一起输入LLM生成答案。</p>
</blockquote>
<p><strong>核心模块1：调度器</strong>。调度器是一个小型卷积神经网络（CNN），其输入为原始图像 $I$，输出为一个二维决策向量。第一维是分类输出，用于从预定义的视觉编码器池（如ViT-B， ViT-L等）中选择一个编码器 $E_k$。第二维是回归输出，预测一个初始的令牌保留比例 $r_{init} \in (0, 1]$。调度器的训练采用强化学习策略，其奖励函数平衡了任务精度和计算成本（如FLOPs或延迟）。</p>
<p><strong>核心模块2：令牌剪枝器</strong>。令牌剪枝器是一个轻量级的Transformer网络，其输入是所选视觉编码器 $E_k$ 生成的完整视觉令牌序列 $T = [t_1, t_2, ..., t_N]$。剪枝器为每个令牌 $t_i$ 计算一个重要性分数 $s_i$。然后，并非简单地按分数硬剪枝至固定比例，而是结合调度器预测的 $r_{init}$ 进行自适应调整：首先根据 $r_{init}$ 确定一个基础保留数量 $M = \lfloor N \times r_{init} \rfloor$，再根据实际的重要性分数分布，允许小幅浮动，最终保留最重要的 $M&#39;$ 个令牌（$M&#39; \approx M$）。这使得剪枝粒度与编码器选择相关联。</p>
<p><strong>创新点</strong>：与现有独立优化调度或剪枝的方法相比，SP-VLA的核心创新在于 <strong>联合决策</strong>。调度器的决策（选择编码器）会影响剪枝器的输入特征分布；而剪枝器的目标保留比例又由调度器预测，两者通过一个统一的效率-精度奖励进行端到端优化。这种协同方式允许系统根据图像内容，灵活地在“使用重编码器但少留令牌”和“使用轻编码器但多留令牌”等策略间做出权衡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：论文在多个视觉语言理解Benchmark上进行评估，包括VQA-v2、GQA、ScienceQA和VizWiz。使用的VLA基础模型为LLaVA-1.5。视觉编码器池包含CLIP-ViT/L-336px和CLIP-ViT/B-224px。评估指标包括任务准确率以及计算效率指标（延迟、FLOPs）。实验平台未明确说明。</p>
<p><strong>Baseline方法</strong>：对比方法包括：1) <strong>原始LLaVA</strong>（使用ViT-L）；2) 仅使用<strong>轻量级编码器</strong>（ViT-B）的LLaVA；3) <strong>静态调度</strong>（为所有输入固定选择ViT-L或ViT-B）；4) 独立的<strong>动态调度</strong>方法（如基于图像复杂度的启发式调度）；5) 独立的<strong>令牌剪枝</strong>方法（如Token Merging）。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_07_18_3a6e9c4b5d5c5b8e5e5cg-2.jpg?height=850&width=1246&top_left_y=1006&top_left_x=354" alt="Performance Comparison"></p>
<blockquote>
<p><strong>图2</strong>：在VQA-v2和GQA数据集上的性能-效率权衡曲线。SP-VLA（红色星星）在相同计算成本（FLOPs）下获得了最高的准确率，或者在相同准确率下实现了最低的FLOPs，明显优于静态策略、独立调度和独立剪枝等Baseline。</p>
</blockquote>
<p><strong>关键实验结果</strong>：在VQA-v2数据集上，SP-VLA在达到与原始LLaVA（ViT-L）相近的准确率（78.0% vs 78.5%）的同时，减少了约 <strong>41.8%</strong> 的FLOPs和 <strong>36.2%</strong> 的延迟。在GQA数据集上，在同等计算预算下，SP-VLA比最好的静态调度策略准确率绝对提升 **1.2%**。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_07_18_3a6e9c4b5d5c5b8e5e5cg-3.jpg?height=582&width=1864&top_left_y=1994&top_left_x=354" alt="Ablation Study"></p>
<blockquote>
<p><strong>图3</strong>：消融实验。左图对比了联合优化与分阶段优化（先固定编码器训练剪枝器，再训练调度器）的性能，显示联合优化效果更佳。右图展示了调度器预测的保留比例 $r_{init}$ 与图像复杂度（通过原始ViT-L的注意力熵近似）之间的关系，表明调度器学会了为复杂图像分配更多令牌配额。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>联合优化 vs 分阶段优化</strong>：联合训练调度器和剪枝器比顺序训练带来约0.8%的准确率提升。</li>
<li><strong>自适应比例调整</strong>：在剪枝器中使用基于 $r_{init}$ 的自适应调整机制，相比固定比例剪枝，能提升约0.5%的准确率。</li>
<li><strong>组件贡献</strong>：调度器决策贡献了主要的速度提升（约30%延迟降低），而令牌剪枝器在调度器的基础上进一步带来了约6%的额外延迟降低和精度保持。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个面向VLA模型加速的<strong>联合模型调度与令牌剪枝框架</strong>（SP-VLA），实现了计算资源的动态精细分配。</li>
<li>设计了一个基于强化学习的轻量级调度器和一个与调度决策联动的自适应令牌剪枝器，两者可端到端协同训练。</li>
<li>在多个标准基准测试上验证了SP-VLA的有效性，在显著降低计算成本的同时，保持了竞争力的模型性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前方法需要针对特定的下游任务（如VQA）对调度器和剪枝器进行微调，其通用性在不同类型任务间的迁移能力有待进一步研究。此外，调度器和剪枝器本身会引入少量额外开销。</p>
<p><strong>后续研究启示</strong>：SP-VLA的思路可以扩展到更多维度的联合优化，例如同时调度视觉编码器、LLM的层数以及注意力头数。此外，将这种动态推理框架与模型压缩技术（如量化）结合，有望在边缘设备上实现更高效的VLA部署。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型（VLA）推理效率低的问题，提出SP-VLA，一种联合模型调度与令牌修剪的加速方法。核心方案是动态调度视觉编码器与语言解码器的执行，并结合显著性引导的令牌修剪技术，提前剔除冗余视觉令牌。实验表明，该方法在保持性能基本无损的情况下，显著降低了计算开销，例如在特定任务上实现了约40%的延迟降低与FLOPs减少。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.12723" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>