<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>$\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>$\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.16054" target="_blank" rel="noreferrer">2504.16054</a></span>
        <span>作者: Intelligence, Physical, Black, Kevin, Brown, Noah, Darpinian, James, Dhabalia, Karan, Driess, Danny, Esmail, Adnan, Equi, Michael, Finn, Chelsea, Fusai, Niccolo, Galliker, Manuel Y., Ghosh, Dibya, Groom, Lachy, Hausman, Karol, Ichter, Brian, Jakubczak, Szymon, Jones, Tim, Ke, Liyiming, LeBlanc, Devin, Levine, Sergey, Li-Bell, Adrian, Mothukuri, Mohith, Nair, Suraj, Pertsch, Karl, Ren, Allen Z., Shi, Lucy Xiaoyang, Smith, Laura, Springenberg, Jost Tobias, Stachowicz, Kyle, Tanner, James, Vuong, Quan, Walke, Homer, Walling, Anna, Wang, Haohuan, Yu, Lili, Zhilinsky, Ury</span>
        <span>日期: 2025/04/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过在大规模机器人演示数据上进行模仿学习，已展现出强大的端到端机器人控制能力。然而，这些模型通常在与其训练数据高度相似的环境中进行评估，其开放世界泛化能力——即在训练中从未见过的、多样化的真实场景（如全新的家庭）中执行任务——仍是一个重大挑战。对于简单的拾取等技能，通过在更广泛的环境中收集机器人数据或许能实现一定泛化，但对于像“清理整个厨房”这样的长视野、多步骤、需要灵巧操作的任务，仅靠暴力扩展机器人数据收集来覆盖所有可能场景是不可行的。</p>
<p>本文针对VLA模型在开放世界中泛化能力不足的痛点，提出了一个新颖的视角：通过精心设计的协同训练框架，将异构的、非直接相关的知识源整合到单一模型中，以赋予其广泛的泛化能力。本文的核心思路是：构建一个分层的VLA模型（𝜋_{0.5}），通过在多样化的数据源（包括其他机器人数据、高层次语义预测、人类语言指令和网络数据）上进行协同训练，使其能够泛化到全新的家庭环境中，执行长达10-15分钟的长视野家务任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>𝜋_{0.5} 模型的训练流程分为两个主要阶段：预训练和后训练，其整体框架如下图所示。</p>
<p><img src="https://arxiv.org/html/2504.16054v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：𝜋_{0.5} 模型与训练流程概览。模型训练分为两个阶段：预训练阶段使用离散令牌在异构数据源上进行协同训练；后训练阶段引入动作专家（Action Expert）和流匹配（Flow Matching），专门针对移动操作进行微调。推理时，模型首先预测高层次语义子任务，然后基于该子任务预测底层动作。</p>
</blockquote>
<p><strong>整体架构与流程</strong>：𝜋_{0.5} 基于一个统一的Transformer架构，能够灵活处理图像、文本和动作等多种模态的输入和输出。模型将观测 𝑜<em>𝑡（多视角图像和机器人本体感知状态）和总体语言指令 ℓ 作为输入，输出同时包含预测的高层次语义子任务 ℓ̂ 和底层动作块 𝑎</em>{𝑡:𝑡+𝐻}。其概率分布被分解为：𝜋<em>𝜃(𝑎</em>{𝑡:𝑡+𝐻}, ℓ̂ | 𝑜<em>𝑡, ℓ) = 𝜋_𝜃(𝑎</em>{𝑡:𝑡+𝐻} | 𝑜_𝑡, ℓ̂) ∙ 𝜋_𝜃(ℓ̂ | 𝑜_𝑡, ℓ)。这对应了分层推理过程：先根据当前观测和总任务推断下一步该做什么（高层次推理），再根据这个具体子任务和观测生成执行动作（低层次推理）。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>预训练阶段</strong>：此阶段目标是让模型广泛接触多样化知识。模型权重从一个标准的、基于网络数据预训练的视觉语言模型（VLM）初始化。所有任务，包括机器人动作任务，都使用<strong>离散令牌</strong>（特别是机器人动作使用FAST动作分词器）表示。这确保了训练的高效性和可扩展性。本阶段使用的数据源极其广泛，包括：<ul>
<li>其他（非移动）机器人平台的数据。</li>
<li>基于机器人观测预测“高层次”语义动作（子任务标签）的数据。</li>
<li>人类监督员逐步指导机器人完成复杂任务时提供的<strong>语言指令</strong>。</li>
<li>来自网络的<strong>多模态数据</strong>，如图像描述、问答、物体定位等。<br>论文强调，在预训练阶段，高达97.6%的训练样本并非来自移动操作器执行家务任务，而是来自上述其他来源。</li>
</ul>
</li>
<li><strong>后训练阶段</strong>：此阶段旨在使模型专门化于移动操作，并启用高效的实时推理。该阶段借鉴了 𝜋_0 模型的设计，为动作生成引入了<strong>动作专家</strong>（Action Expert）和<strong>流匹配</strong>（Flow Matching）机制。动作专家是Transformer中专门处理动作令牌的一组权重，类似于混合专家架构，它更小且能专注于生成连续、细粒度的动作分布。流匹配提供了比离散令牌更富表现力的连续动作表示，并支持更高效的计算推理。</li>
<li><strong>推理时分层决策</strong>：在部署时，模型以较低频率（相对于动作预测）首先生成一个文本形式的高层次子任务（如“拿起盘子”），然后将此子任务作为条件，通过动作专家和流匹配机制高频地预测出执行该子任务所需的低层次机器人动作序列。</li>
</ol>
<p><strong>创新点</strong>：与现有VLA工作相比，𝜋_{0.5}的核心创新在于其<strong>系统性的异构数据协同训练框架</strong>以及<strong>统一模型内的分层推理机制</strong>。它不仅仅利用网络VLM数据或更多机器人数据，而是主动整合了多种在语义和形式上与目标任务相关但并非直接等同的数据源，并通过一个灵活的架构使这些知识能够相互迁移和补充。同时，它将高层次规划与低层次执行集成在同一个模型中，而非使用两个分离的模型，这更接近于“思维链”的推理方式。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估主要在<strong>真实家庭环境</strong>中进行，这些家庭在训练期间完全未出现过。任务涉及长视野的厨房和卧室清理，例如“将所有盘子放入水槽”、“捡起地上所有衣服”等，任务时长可达10-15分钟。实验平台是移动操作机器人。对比的基线方法包括仅在后训练数据上训练的VLA模型（可视为 𝜋_0 的变体）以及一些消融实验模型。</p>
<p><strong>关键定量结果</strong>：在全新的真实家庭中执行7步长视野任务的定量评估显示，完整的 𝜋_{0.5} 模型取得了显著优于基线的性能。</p>
<p><img src="https://arxiv.org/html/2504.16054v1/extracted/6379337/figures/real_home_quantitative_eval_plot.png" alt="定量评估"></p>
<blockquote>
<p><strong>图7</strong>：在全新真实家庭中的定量评估结果。完整𝜋_{0.5}模型（橙色）在任务成功率（左）和任务完成度（右，即完成步骤的比例）上均大幅超过仅使用后训练数据训练的基线模型（蓝色）。</p>
</blockquote>
<p><strong>消融实验与关键发现</strong>：</p>
<ol>
<li><p><strong>数据规模与来源的影响</strong>：<br><img src="https://arxiv.org/html/2504.16054v1/extracted/6379337/figures/env_scaling.png" alt="数据与泛化"></p>
<blockquote>
<p><strong>图8</strong>：增加训练环境多样性对泛化的影响。即使在机器人数据量相近的情况下，𝜋_{0.5}（利用异构数据）在新环境中的泛化能力也远优于仅在更多同类机器人数据上训练的模型。<br><img src="https://arxiv.org/html/2504.16054v1/extracted/6379337/figures/performance_vs_ll_data.png" alt="低层数据影响"><br><strong>图10</strong>：低层次动作数据量对性能的影响。增加低层动作数据（包括来自其他机器人的数据）能持续提升任务成功率。<br><img src="https://arxiv.org/html/2504.16054v1/extracted/6379337/figures/performance_vs_hl_data.png" alt="高层数据影响"><br><strong>图13</strong>：高层次语义数据对性能的影响。增加高层次语义预测和语言指令数据，能显著提升模型在需要复杂语义理解的任务上的表现。</p>
</blockquote>
</li>
<li><p><strong>模型规模与语言能力</strong>：<br><img src="https://arxiv.org/html/2504.16054v1/extracted/6379337/figures/performance_vs_ll_model.png" alt="模型规模"></p>
<blockquote>
<p><strong>图12</strong>：低层次动作模型规模的影响。增大动作专家（低层模型）的参数量能提升性能，但收益会饱和。<br><img src="https://arxiv.org/html/2504.16054v1/extracted/6379337/figures/language_vs_model.png" alt="语言与模型"><br><strong>图17</strong>：语言数据与基础模型规模的相互作用。当基础VLM模型较小时，增加语言数据收益巨大；当基础模型已经很大时，增加语言数据的边际收益减小。</p>
</blockquote>
</li>
<li><p><strong>组件贡献分析</strong>：<br><img src="https://arxiv.org/html/2504.16054v1/extracted/6379337/figures/overall_breakdown.png" alt="总体消融"></p>
<blockquote>
<p><strong>图18</strong>：各训练数据组件对整体性能的贡献。移除任何一类数据（网络数据、其他机器人数据、高层次数据）都会导致性能下降，其中高层次语义数据和语言指令的贡献尤为关键。<br><img src="https://arxiv.org/html/2504.16054v1/extracted/6379337/figures/HL_breakdowns.png" alt="高层消融"><br><strong>图19</strong>：高层次数据中各子组件的贡献。在需要语义理解的任务（如“关柜门”）上，人类语言指令数据至关重要；在更直观的任务（如“捡起衣服”）上，高层次动作预测数据更为重要。</p>
</blockquote>
</li>
</ol>
<p><strong>定性结果展示</strong>：论文提供了大量在全新家庭中执行复杂任务的实例，如清理厨房、整理床铺等，直观证明了其开放世界泛化能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个系统的、基于异构数据协同训练的VLA模型（𝜋_{0.5}）训练框架，首次实证证明通过整合多种非直接机器人数据源（其他机器人数据、高层次语义预测、人类语言指令、网络数据）可以催生出强大的开放世界泛化能力。</li>
<li>首次展示了一个端到端学习驱动的机器人系统能够在<strong>从未见过的全新真实家庭</strong>中，成功执行长达10-15分钟的<strong>长视野、灵巧操作任务</strong>（如清理整个厨房或卧室）。</li>
<li>通过详尽的消融研究，定量分析了不同数据源和模型组件对泛化性能的具体贡献，为未来研究提供了宝贵的经验指导。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于高质量、多样化的数据集构建（包括高层次标注和语言指令），这需要大量的人工努力。此外，大规模模型的训练和推理需要可观的计算资源。</p>
<p><strong>对后续研究的启示</strong>：𝜋_{0.5} 的工作表明，迈向通用物理智能的关键可能不在于无限堆砌目标域的机器人数据，而在于<strong>如何更高效地利用和整合广泛存在的、异构的先验知识</strong>。这启示后续研究可以探索：1）更自动化的方式获取或生成有价值的训练数据（如语言指令）；2）更高效的模型架构，以更低成本融合多源知识；3）将协同训练的理念应用于更广泛的具身智能任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出π₀.₅模型，旨在解决端到端视觉-语言-动作模型在真实开放世界中泛化能力有限的核心问题。关键技术是基于π₀进行协同训练，整合来自多机器人、高层语义预测及网络数据等多源异构数据，并通过混合多模态示例（融合图像、语言指令、物体检测、语义子任务与底层动作）实现知识迁移。实验首次证明，该端到端学习系统能在训练数据未涵盖的全新家庭环境中，成功执行如清洁厨房或卧室等长期、灵巧的操纵任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.16054" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>