<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.16475" target="_blank" rel="noreferrer">2506.16475</a></span>
        <span>作者: Ding Zhao Team</span>
        <span>日期: 2025-06-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>四足机器人已在复杂环境中展现出卓越的运动能力，但以可扩展的方式赋予它们自主且通用的操作技能仍然是一个重大挑战。模仿学习是通过示教让机器人学习复杂技能的基本方法，而高质量数据的获取是实现高效学习的关键。现有工作主要关注机械臂、人形机器人或配备顶部机械臂的四足机器人，针对像LocoMan这样配备腿部操作器的四足平台，收集其自我中心视角的操作数据仍属探索不足的领域。为了扩展模仿学习的数据收集规模，近期工作提出利用仿真数据或人类数据。人类数据尤其被用于提供高级任务指导、改进视觉编码器、模拟领域内机器人数据，或通过将人类视为另一种具身形态来提供额外的自我中心数据。然而，人类数据对于涉及四足机器人等非传统具身的操作任务的有效性尚未得到证实。人与四足机器人之间存在巨大的具身形态差异，这对数据收集和策略迁移都构成了挑战。本文针对这一痛点，提出将人类视为与目标机器人不同的具身形态，并利用人类数据进行模型预训练。核心思路是：设计一个统一的数据收集与对齐框架，以及一个模块化的Transformer架构，首先利用易于收集的人类数据进行预训练，然后仅用少量机器人数据微调，从而学习通用的四足操作技能。</p>
<h2 id="方法详解">方法详解</h2>
<p>Human2LocoMan的整体框架包含数据收集、统一空间对齐和两阶段训练策略。系统使用XR头显（如Apple Vision Pro）进行数据采集，既能捕获人类执行任务时的自我中心视频和动作，也能用于遥操作LocoMan机器人。LocoMan是一个配备两个腿部操作器的开源四足机器人平台，支持单臂和双臂操作模式。</p>
<p><img src="https://arxiv.org/html/2506.16475v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Human2LocoMan框架总览。系统使用XR头显收集人类自我中心数据（Human Egocentric Data）和遥操作的机器人数据（Teleoperated Robot Data），所有数据都映射到一个统一的坐标系中。数据集包含来自人类和机器人的对齐后的视觉、本体感觉和动作信息。采用两阶段训练：模块化跨具身模型首先在易于收集的人类数据上预训练，然后用少量机器人数据微调。最终策略可部署到真实机器人上，执行单臂或双臂操作任务。</p>
</blockquote>
<p><strong>统一坐标系与运动映射</strong>：为了桥接具身鸿沟，系统为人类和LocoMan建立了一个统一的参考坐标系ℱ_u，该坐标系附着在安装主摄像头（XR头显或机器人躯干相机）的刚体上。在数据收集和遥操作时，将人类头部、手腕和手部的6D位姿（来自XR系统）转换到此统一坐标系中。对于遥操作，将人类手腕动作映射到机器人末端执行器，头部动作映射到机器人躯干（在单臂模式下用于扩展工作空间和主动感知），手部姿态映射到夹持器开合。具体映射公式如论文中所示，通过初始位姿和缩放因子α来计算机器人的目标位姿，然后由全身控制器生成协调的机器人动作。</p>
<p><strong>模块化跨具身Transformer（MXT）</strong>：这是方法的核心创新架构。尽管将人类和机器人数据映射到了统一坐标系，但两者在动力学、额外传感器（如机器人腕部相机）等方面仍存在明显的分布差异。MXT采用模块化设计来促进跨具身学习并保留每个具身特有的模态分布。其核心是一个在不同具身间共享的Transformer主干网络。关键设计在于<strong>具身特定和模态特定的tokenizer与detokenizer</strong>。对于每种具身（如人类、机器人单臂模式、机器人双臂模式）和每种数据模态（如图像、本体感觉、动作），都有独立的编码器（tokenizer）将原始输入转换为令牌，以及独立的解码器（detokenizer）将Transformer输出的令牌解码为预测动作。这种设计允许模型在共享高层表示（Transformer主干）的同时，灵活处理不同来源数据的底层分布差异。</p>
<p><strong>训练流程</strong>：首先，使用收集到的人类操作数据对MXT模型进行预训练，学习从自我中心视觉和本体感觉预测人类动作（手腕、头部位姿等）的基本技能。然后，使用通过遥操作收集的、相对少量的LocoMan机器人数据对预训练模型进行微调。在微调阶段，只需将模型中的具身特定模块（tokenizer/detokenizer）切换到机器人对应的版本，并更新参数即可。这种设计使得一个预训练模型可以通过微调适配到不同的机器人具身形态。</p>
<p>与现有方法（如通过动作归一化来对齐具身）相比，本文的创新点在于：1）在数据收集阶段通过统一的坐标框架进行结构化对齐；2）在模型架构层面显式地通过模块化设计来处理不同具身和模态间的分布差异，无需复杂的域对齐预处理，从而提供了更大的灵活性和可扩展性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在六个真实世界的家庭任务上进行，涵盖了单臂和双臂操作模式。任务包括：开抽屉、关抽屉、将物体放入抽屉、按下按钮、打开微波炉门、双手协作将物体放入篮子。实验平台为真实的LocoMan机器人。</p>
<p><strong>基线方法</strong>：主要对比基线是<strong>直接从机器人数据训练</strong>（没有人类数据预训练）的MXT模型。此外，还进行了详尽的消融实验以验证各个组件的贡献。</p>
<p><strong>关键定量结果</strong>：在全部六个任务上，Human2LocoMan（预训练+微调）相比仅用机器人数据训练的基线，平均成功率绝对提升了41.9%。在分布外（OOD）测试设置（如物体位置、抽屉初始状态变化）下，提升更为显著，达到79.7%。人类数据预训练本身贡献了平均38.6%的成功率提升，在OOD设置下贡献了82.7%的提升。此外，仅使用一半数量的机器人数据进行微调，结合人类预训练的模型性能仍能 consistently 优于使用全量机器人数据但不预训练的基线。</p>
<p><img src="https://arxiv.org/html/2506.16475v2/x2.png" alt="主要结果"></p>
<blockquote>
<p><strong>图2</strong>：六个任务上的成功率对比。Human2LocoMan（Ours）在所有任务上均显著优于仅使用机器人数据训练的基线（Baseline），尤其在分布外（OOD）测试中优势巨大。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.16475v2/x3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。从左至右分别展示了：1) 人类数据预训练（Ours w/ PT）相比无预训练（Ours w/o PT）的巨大优势；2) 模块化架构（Modular）相比非模块化共享编码器（Shared Enc.）的有效性；3) 细粒度模态对齐（Ours）相比粗粒度对齐（Coarse-grained）的收益。</p>
</blockquote>
<p><strong>定性结果与失败分析</strong>：<br><img src="https://arxiv.org/html/2506.16475v2/x4.png" alt="定性结果1"></p>
<blockquote>
<p><strong>图4</strong>：单臂操作任务（开抽屉、关抽屉、放入物体）的成功执行序列。策略能够协调躯干和末端执行器的运动。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.16475v2/x5.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图5</strong>：单臂操作任务（按下按钮、打开微波炉门）的成功执行序列。展示了策略对不同物体和机构的适应性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.16475v2/x6.png" alt="定性结果3"></p>
<blockquote>
<p><strong>图6</strong>：双臂协作任务（将物体放入篮子）的成功执行序列。策略能控制两个末端执行器进行协作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.16475v2/x7.png" alt="失败案例"></p>
<blockquote>
<p><strong>图7</strong>：典型的失败案例，主要发生在需要高精度或遇到极端OOD情况时，例如末端执行器轻微未对齐导致物体掉落，或面对训练中未见过的大型障碍物。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验验证了三个核心组件的贡献：1) <strong>人类数据预训练</strong>是性能提升的主要来源；2) <strong>模块化架构</strong>（具身/模态特定的tokenizer/detokenizer）对于处理分布差异至关重要，非模块化设计性能显著下降；3) <strong>细粒度的模态对齐</strong>（为不同模态设计独立模块）比粗粒度的对齐方式更有效。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：1) 提出了一个统一的<strong>Human2LocoMan框架</strong>，用于灵活、可扩展地收集人类示教和遥操作机器人轨迹，以学习通用的四足操作技能；2) 设计了<strong>模块化跨具身Transformer（MXT）架构</strong>，通过共享主干网络与具身/模态特定模块的设计，有效促进了人与四足机器人之间大形态差异下的跨具身学习；3) 为开源LocoMan平台引入了首个<strong>基于XR的遥操作系统和操作数据集</strong>；4) 在真实机器人上<strong>实证了从人到机器人的有效正向迁移</strong>，在多个挑战性任务上取得了高成功率和强鲁棒性。</p>
<p>论文提到的局限性包括：方法在高度动态或需要精确力控的任务上可能表现不佳；当前系统依赖于高质量的视觉里程计来维持统一坐标系。</p>
<p>本研究对后续工作的启示在于：模块化架构和细粒度的模态对齐是处理大具身差异的有效途径；将人类视为一种可提供丰富先验技能的“具身形态”，能显著减少对昂贵机器人数据的需求，为大规模机器人学习提供了有前景的路径。这项工作为利用异构数据源进行机器人技能学习开辟了新的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对四足机器人难以习得通用自主操作技能的问题，提出一种跨具身模仿学习系统。核心方法包括：1）统一人与机器人观察-行动空间的遥操作数据采集流程；2）支持多具身协同训练与预训练的模块化架构。实验在六项真实任务中验证，系统相比基线平均成功率提升41.9%（分布外场景提升79.7%）。其中，使用人类数据预训练贡献了38.6%的整体提升（分布外场景82.7%），且仅需一半机器人数据即可获得更优性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.16475" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>