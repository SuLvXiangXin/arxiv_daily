<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.02530" target="_blank" rel="noreferrer">2509.02530</a></span>
        <span>作者: Bingyi Kang Team</span>
        <span>日期: 2025-09-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>现代机器人操作主要依赖2D彩色图像进行技能学习，但存在泛化能力差的问题。相比之下，人类在3D世界中交互时更依赖距离、大小和形状等物理属性，而非纹理。虽然深度相机可广泛获取此类3D几何信息，但将其用于操作面临挑战，主要源于其有限的精度和对各类噪声的敏感性。现有方法通常在仿真中使用干净的深度进行评估，或在真实世界中对点云进行降采样以缓解噪声，这限制了3D几何信息的充分利用。</p>
<p>本文针对深度相机在真实世界中输出不可靠这一具体痛点，提出了相机深度模型（Camera Depth Models, CDMs）作为一种即插即用的解决方案。其核心思路是：通过一个神经网络数据引擎，在仿真中建模特定深度相机的噪声模式以生成高质量配对数据，从而训练出能够输入RGB图像和原始深度信号、输出去噪且精确的度量深度的CDM，最终弥合仿真到真实的几何差距，使策略能够基于精准的几何信息进行学习与迁移。</p>
<h2 id="方法详解">方法详解</h2>
<p>CDM的整体目标是作为一个针对特定深度相机的插件模型 $M$，输入一对RGB图像 $\mathbf{I}$ 和来自该相机的深度图像 $\mathbf{D}$，输出高质量的度量深度图像 $\hat{\mathbf{D}}$。其核心创新在于通过数据驱动的方式学习相机的噪声模式，并在仿真中合成逼真的带噪数据以训练去噪模型。</p>
<p><img src="https://arxiv.org/html/2509.02530v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：相机深度模型概览。左侧：用于训练CDM的数据合成流程，利用收集的真实数据集训练值噪声/空洞噪声模型。右侧：CDM模型结构，基于两个ViT编码器并从深度基础模型微调而来；RGB和深度令牌在送入DPT解码器前进行融合。该结构允许模型直接接收传感器的稀疏深度进行预测，无需任何预处理（如空洞填充）。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>模型设计</strong>：CDM采用双分支Vision Transformer（ViT）架构。RGB分支和深度分支分别编码输入图像，得到特征令牌 $X^{\mathbf{I}}$ 和 $X^{\mathbf{D}}$。随后，通过一个特征令牌融合模块，对空间位置对应的RGB和深度令牌进行拼接，并执行多头自注意力（MHA）操作，实现双向特征融合，生成蕴含尺度信息的深度特征 $\tilde{X}$。为防止语义信息丢失，将原始RGB特征令牌 $X^{\mathbf{I}}$ 与融合后的特征 $\tilde{X}$ 拼接，最后通过一个DPT解码器输出最终的深度预测 $\hat{\mathbf{D}}$。此设计使模型能够直接感知原始深度图像，无需空洞填充等预处理。</li>
<li><strong>数据收集</strong>：为训练针对不同相机的CDM，本文构建了ByteCameraDepth数据集。通过设计一个多相机支架，同时采集来自7种不同深度相机（共10种模式）的超过17万对RGB-深度图像，覆盖厨房、客厅等7种场景。</li>
<li><strong>噪声建模与数据合成</strong>：为在仿真中合成特定相机的带噪数据，训练了两个噪声模型：<ul>
<li><strong>空洞噪声模型</strong>：视为一个二分类问题，使用基于DINOv2和DPT头的网络，根据RGB图像 $\mathbf{I}$ 预测原始深度 $\mathbf{D}$ 中每个像素是否为空洞（无效值）。</li>
<li><strong>值噪声模型</strong>：视为风格化的相对深度预测问题，基于Depth Anything V2（DAV2）模型，以低质量深度图像为标签进行训练，学习深度值的失真模式。<br>合成带噪深度 $\tilde{\mathbf{D}}$ 时，先对干净的仿真真实深度应用值噪声模型得到相对深度，再通过仿射不变变换恢复度量尺度，最后根据空洞噪声模型的预测掩码置零部分像素以模拟空洞。</li>
</ul>
</li>
<li><strong>训练优化</strong>：<ul>
<li><strong>引导过滤</strong>：为解决值噪声模型在合成数据上难以保持正确度量尺度的问题，提出使用引导过滤。以值噪声为引导图像，以真实深度为输入图像，通过优化局部线性变换参数，使输出图像在保留值噪声结构的同时逼近正确的度量尺度。采用随机化的核大小作为增强策略。</li>
<li><strong>损失函数</strong>：使用 $L_1$ 损失结合梯度损失 $\ell_{\text{grad}}$ 进行训练，以更好地保留边缘深度。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.02530v1/figs/multicam-fig.png" alt="多相机支架"></p>
<blockquote>
<p><strong>图3</strong>：用于同时捕获多相机彩色-深度图像对的多相机支架设备。其上安装了包括五种RealSense相机在内的七种相机。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.02530v1/x2.png" alt="数据集示例"></p>
<blockquote>
<p><strong>图4</strong>：收集的ByteCameraDepth数据集图示，展示了7种相机、10种模式在7个不同场景中的原始深度数据。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/任务</strong>：在两个具有挑战性的长时程真实世界机器人操作任务上进行评估：1) <strong>厨房任务</strong>：打开微波炉门，从架子上取出一个盘子，然后将其放入微波炉内。涉及铰接、反光（微波炉）和细长（盘子）物体。2) <strong>餐厅任务</strong>：打开牙膏盖，拿起牙刷，将牙膏挤到牙刷上。涉及铰接物体和精细操作。</li>
<li><strong>实验平台</strong>：使用UR5机械臂，并配备了RealSense D435或D415深度相机。</li>
<li><strong>基线方法</strong>：对比了多种视觉输入形态的策略：1) <strong>原始深度</strong>；2) <strong>点云</strong>（来自原始深度）；3) <strong>RGB图像</strong>；4) <strong>RGB-D</strong>（RGB+原始深度）；5) <strong>仿真深度</strong>（在仿真中使用完美深度训练，直接部署）；6) <strong>带噪声的仿真深度</strong>（在仿真深度中添加人工噪声后训练）。所有策略均使用相同的模仿学习框架（基于MimicGen扩展生成演示数据）进行训练。</li>
</ul>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2509.02530v1/figs/success-rate-hist.png" alt="成功率直方图"></p>
<blockquote>
<p><strong>图5</strong>：不同方法在两个真实世界任务上的成功率对比。CDM-D435和CDM-D415取得了显著优于所有基线的性能。</p>
</blockquote>
<p>在厨房任务中，使用CDM-D435的策略成功率达到 **86.7%**，而使用原始深度、点云、RGB和RGB-D的策略成功率分别仅为 <strong>13.3%、40.0%、53.3%</strong> 和 **46.7%**。在餐厅任务中，CDM-D435策略成功率为 <strong>76.7%<strong>，同样远超其他基线（原始深度：</strong>16.7%<strong>，点云：</strong>30.0%<strong>，RGB：</strong>36.7%<strong>，RGB-D：</strong>33.3%<strong>）。值得注意的是，</strong>直接在原始仿真深度（无任何噪声添加或真实世界微调）上训练的策略，配合CDM插件后，成功迁移到了真实机器人</strong>，在厨房和餐厅任务上分别达到了 <strong>73.3%</strong> 和 <strong>70.0%</strong> 的成功率，显著优于在添加了人工噪声的仿真深度上训练的策略（成功率分别仅为 <strong>6.7%</strong> 和 **13.3%**）。</p>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>引导过滤</strong>：是性能提升的关键。不使用引导过滤会导致模型无法有效利用深度提示中的尺度信息，性能大幅下降（厨房任务成功率从86.7%降至20%）。</li>
<li><strong>空洞噪声建模</strong>：对处理深度相机的典型失效模式很重要。</li>
<li><strong>特征融合方式</strong>：本文提出的在编码器阶段进行令牌级融合的方式，优于在浅层解码器阶段进行融合的基线方法。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.02530v1/figs/kitchen_task_comparison.png" alt="定性结果对比1"></p>
<blockquote>
<p><strong>图39</strong>：厨房任务中不同方法轨迹的定性比较。CDM策略（绿色）能够成功完成所有子步骤，而原始深度策略（红色）在多个步骤失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.02530v1/figs/canteen_task_comparison.png" alt="定性结果对比2"></p>
<blockquote>
<p><strong>图40</strong>：餐厅任务中不同方法轨迹的定性比较。同样，CDM策略（绿色）表现稳健，而原始深度策略（红色）在打开牙膏盖和挤牙膏步骤上失败。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>发布了<strong>ByteCameraDepth</strong>，一个包含来自7种相机、10种配置的超过17万对RGB-深度图像的真实世界多相机深度数据集。</li>
<li>提出并开源了<strong>相机深度模型（CDMs）</strong> 系列，作为提升常用深度相机感知精度的即插即用解决方案。</li>
<li>通过CDMs，首次实证了<strong>仅使用原始仿真深度训练、无需添加噪声或真实世界微调的策略，可以无缝迁移到真实机器人</strong>，成功完成了涉及铰接、反光、细长物体的复杂长时程任务，凸显了精准几何信息对于机器人操作的重要性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，CDM的性能在一定程度上依赖于相机校准（内参和外参）的准确性。此外，虽然作为插件使用简便，但模型本身需要一定的计算开销。</p>
<p><strong>对后续研究的启示</strong>：本研究强有力地表明，<strong>精准的几何感知是弥合仿真到真实差距、实现泛化机器人策略的关键</strong>。它提供了一条不依赖于高保真视觉渲染或复杂域随机化的新路径。所发布的数据集、模型和易用的仿真到真实指南，为社区进一步探索3D信息在机器人策略中的广泛应用提供了实用基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决机器人操作中因依赖2D视觉导致的泛化能力差，以及深度相机噪声大、精度低的问题。提出了相机深度模型（CDMs）作为插件，结合RGB图像与原始深度信号，输出去噪后的精确度量深度。关键技术是开发了神经数据引擎，通过模拟深度相机噪声模式生成高质量仿真配对数据。实验表明，CDMs实现了接近仿真级别的深度预测精度，有效弥合了仿真到现实的差距。仅使用原始仿真深度训练的策略，无需添加噪声或真实世界微调，即可在涉及铰接、反光、细长物体的复杂长时程任务中无缝迁移至真实机器人，且性能几乎无下降。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.02530" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>