<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2502.18423" target="_blank" rel="noreferrer">2502.18423</a></span>
        <span>作者: Bai, Fengshuo, Li, Yu, Chu, Jie, Chou, Tawei, Zhu, Runchuan, Wen, Ying, Yang, Yaodong, Chen, Yuanpei</span>
        <span>日期: 2025/02/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在杂乱环境中（如塞满物品的抽屉、凌乱的桌面）检索被掩埋的目标物体是机器人学中的一个重要挑战。现有主流方法通常依赖平行夹爪，通过顺序抓取并移开每一个遮挡物体来暴露目标。这种策略不仅耗时，而且对每个遮挡物都需具备可靠的抓取能力，这在实践中往往难以实现。人类则擅长利用高自由度的灵巧手，通过推、拨等动作快速清理遮挡物。本文针对“高效检索被掩埋物体”这一痛点，提出使用多指灵巧手作为末端执行器，并利用大规模并行强化学习在模拟中训练策略，使机器人能够自主涌现出高效的清理技能。其核心思路是：通过精心设计的奖励函数和环境设置，训练一个策略，使灵巧手学会主动移动遮挡物以快速暴露足够的目标物体表面，从而实现高效检索，并完成零样本的模拟到真实迁移。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的系统框架（Retrieval Dexterity）包含三个主要部分：任务构建、强化学习问题设计和策略训练/迁移。</p>
<p><img src="https://arxiv.org/html/2502.18423v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：检索技能系统设计概览。(a) 使用“从上方掉落”策略构建多样化的杂乱场景。(b) 利用大规模并行强化学习和精心设计的奖励函数训练策略。(c) 从强化学习专家策略中生成轨迹，根据设计原则筛选有用轨迹，并通过行为克隆训练用于真实机器人部署的蒸馏策略。</p>
</blockquote>
<p><strong>1. 任务构建</strong>：为模拟真实场景，系统在一个盒子内放置了18个质量、尺寸和几何形状各异的家居物品。任务初始化时，物体通过重力掉落至盒内，目标物体被预先置于底部。为避免机械手干扰场景初始化并稳定奖励计算，定义了两种静态姿势：准备姿势（初始配置，手掌向下位于盒子上方）和悬挂姿势（手远离盒子，用于物体掉落和奖励评估）。在训练中，每10步将手移至悬挂姿势，基于顶视角分割掩膜计算目标物体的真实像素数以评估奖励，然后手返回原配置，以此提供更稳定的时序累积奖励信号。</p>
<p><strong>2. 强化学习问题设计</strong>：</p>
<ul>
<li><strong>观察空间</strong>：策略的观察结合了本体感知和视觉信息。基础部分包括13维关节位置（7维手臂+6维手）、目标物体分割包围盒坐标及面积、包围盒中心像素深度。为促进学习，还引入了仿真中的特权信息：五个指尖和目标的位姿、关节速度、目标物体的线速度和角速度、以及目标物体最近的5个物体的位置。</li>
<li><strong>动作空间</strong>：动作是13维目标关节角度（7维手臂，6维手）。为稳定控制，对手部动作应用了线性平滑更新：<code>a_t^hand = λ * a_t^hand + (1-λ) * a_t-1^hand</code>，以减少突变。手臂动作为相对关节位置变化。</li>
<li><strong>奖励函数</strong>：设计了一个精细的多组件奖励函数以引导高效检索行为：<ol>
<li><strong>距离奖励</strong>：鼓励手靠近被遮挡区域，基于手掌与目标物体距离的指数衰减。</li>
<li><strong>搅拌奖励</strong>：鼓励手主动移动物体（尤其是在完全遮挡时），奖励所有物体位置相对于上一时刻的变化。</li>
<li><strong>邻近清理奖励</strong>：引导智能体清理目标周围的遮挡物，奖励目标与其最近k个物体距离之和，并与距离奖励耦合。</li>
<li><strong>像素紧急奖励</strong>：基于顶视角相机中目标物体分割像素数量的视觉整体评估奖励。</li>
<li><strong>惩罚项</strong>：包括动作惩罚、接触惩罚和移动目标物体的惩罚，以抑制不良行为。<br>为促进高效学习并避免奖励黑客，最终采用基于势能的奖励塑形技术。定义势函数Φ(s)为距离、清理和像素奖励之和，最终奖励<code>R(s, a, s’) = Φ(s’) - Φ(s)</code>。</li>
</ol>
</li>
</ul>
<p><strong>3. 策略训练与模拟到真实迁移</strong>：使用PPO算法在IsaacGym平台上并行512个环境进行训练。应用领域随机化（如盒子位置、物体掉落配置、目标初始位姿）以增强策略泛化能力。为迁移到真实世界，从训练好的专家策略中收集轨迹，并根据“手指保持离盒底最小高度”和“目标物体在盒内均匀分布”的原则筛选成功轨迹。通过行为克隆，使用筛选后的数据蒸馏一个学生策略。该学生策略的观察简化为手臂关节位置以及目标物体包围盒中心的(x, y)坐标，以适应真实世界的感知限制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟中使用超过10个家居物品进行训练和测试，并在真实世界中使用Realman RM-75机械臂搭载Inspired Hand（6自由度）进行验证。对比的基线方法包括：使用平行夹爪的<strong>Sequential Remove</strong>（顺序移除）、基于学习的<strong>Push-Grasp Synergy</strong>策略、以及<strong>Grasp Everything</strong>（尝试抓取所有物体）。</p>
<p><img src="https://arxiv.org/html/2502.18423v2/x3.png" alt="实验设置"></p>
<blockquote>
<p><strong>图3</strong>：实验设置概览。(A) 模拟中训练和测试的物体集，以及真实世界测试物体集。(B) 模拟中的杂乱场景。(C) 真实机器人实验平台：搭载Inspired Hand的Realman RM-75机械臂和RealSense D435相机。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2502.18423v2/x4.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：模拟中不同方法的成功率对比（最高100%）。本文方法（Retrieval Dexterity）在训练物体和未见物体上的成功率均显著高于所有基线方法。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟性能</strong>：在训练物体上，本文方法成功率达到**97.5%<strong>，远超Sequential Remove (32.5%)、Push-Grasp Synergy (42.5%) 和Grasp Everything (20.0%)。在未见物体上，成功率也达到</strong>92.5%**，显示出强大的泛化能力。</li>
<li><strong>效率对比</strong>：本文方法检索目标物体的平均耗时最短。</li>
</ol>
<p><img src="https://arxiv.org/html/2502.18423v2/x5.png" alt="效率对比"></p>
<blockquote>
<p><strong>图5</strong>：模拟中不同方法的效率（时间步数）对比。本文方法（紫色）完成检索所需的平均时间步数远少于基线方法，体现了其高效性。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：<ul>
<li><strong>奖励组件</strong>：移除“搅拌奖励”导致成功率从97.5%降至80.0%；移除“邻近清理奖励”降至72.5%；同时移除两者则策略完全失效（0%）。这证明了多组件奖励设计对涌现复杂清理技能的必要性。</li>
<li><strong>观察信息</strong>：移除特权信息（如物体位姿、速度）会降低性能；而进一步移除基础视觉信息（包围盒）则导致策略完全失效。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2502.18423v2/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：奖励函数和观察空间的消融研究。左图显示搅拌奖励和邻近清理奖励对成功至关重要；右图显示视觉信息（包围盒）是策略学习的基础。</p>
</blockquote>
<ol start="4">
<li><strong>真实世界部署</strong>：<ul>
<li>经过蒸馏的学生策略在真实机器人上实现了零样本部署。</li>
<li>在真实世界的杂乱场景中，该方法展现了如“推”、“拨”、“搅拌”等涌现技能，能够高效清理遮挡物并暴露目标。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2502.18423v2/x7.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图7</strong>：真实世界检索任务的定性结果序列。机械手成功地将遮挡物（核桃）拨开，暴露并最终接触到目标物体（绿色魔方）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2502.18423v2/x8.png" alt="技能分析"></p>
<blockquote>
<p><strong>图8</strong>：学习到的技能分析。策略涌现出多种行为模式，如(a)推动、(b)搅拌、(c)戳动，以适应不同的遮挡情况。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个利用多指灵巧手进行高效物体检索的完整系统（Retrieval Dexterity），通过模拟中的大规模强化学习训练，使策略自主涌现出高效的清理技能。</li>
<li>设计了一个精细的多组件奖励函数，并结合基于势能的奖励塑形，成功引导灵巧手在复杂接触环境中学习到推、拨等复杂操作。</li>
<li>实现了从模拟到真实世界的零样本策略迁移，通过轨迹筛选和行为克隆蒸馏，证明了学习策略在真实硬件上的实用性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前方法依赖于对目标物体的分割来提供视觉观察和计算像素奖励。在真实部署中，这需要额外的感知模块。此外，策略在清理过程中可能导致目标物体本身发生位移。</p>
<p><strong>启示</strong>：这项工作展示了利用灵巧手的高自由度及其与复杂环境交互的潜力，可以解决传统夹爪在效率上的瓶颈。其基于强化学习和精细奖励设计的方法框架，为学习其他接触丰富的非抓取式操作任务（如整理、装配）提供了参考。未来的工作可以探索更复杂的多物体交互、结合语言指令的目标指定，以及更鲁棒的感知模块以降低对仿真特权信息的依赖。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决在杂乱堆叠环境中高效检索目标物体的难题。现有方法需顺序抓取遮挡物，耗时长且要求高。论文提出使用灵巧臂手系统，通过大规模并行强化学习在多样化杂乱环境中训练策略，涌现出推、搅拌、戳等操作技能以快速清理遮挡、暴露目标。实验表明，该方法对训练过及未见过的超过10种家居物体均能高效检索，并成功实现零样本现实部署。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2502.18423" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>