<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.20032" target="_blank" rel="noreferrer">2505.20032</a></span>
        <span>作者: Lygerakis, Fotios, Özdenizci, Ozan, Rückert, Elmar</span>
        <span>日期: 2025/05/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前视觉触觉表征学习的主流方法主要依赖于大规模预训练的视觉或视觉语言模型，例如Unitouch和Fu等人的工作，它们冻结视觉编码器，仅训练触觉编码器以对齐到预训练的视觉或语言嵌入空间。这种方法假设视觉表征对于触觉对齐是最优的，可能限制了触觉特定信息的表达性，并阻碍了联合表征学习。此外，现有的方法（如VTT）通常针对特定下游任务进行训练或微调，缺乏任务无关的泛化能力。一个关键的局限性在于，现有研究普遍忽视了位置编码在融合视觉和触觉这两种具有不同空间尺度与语义特性的模态中的作用，而多尺度的空间推理对于捕捉细粒度的视觉触觉关联至关重要。</p>
<p>本文针对上述痛点，提出了一种新的视角：通过精心设计的多尺度位置编码方案，为Transformer架构注入空间归纳偏置，从而在无需依赖大规模预训练模型和庞大数据集的情况下，实现鲁棒的跨模态对齐和任务无关的视觉触觉联合表征学习。本文的核心思路是：为视觉和触觉模态分别设计模态特定的位置编码以捕获模态内空间结构，同时引入一个全局共享的位置编码来建立跨模态的空间参考，最终通过一个统一的Transformer编码器实现信息融合。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViTaPEs的整体框架基于视觉Transformer架构，旨在处理并融合视觉和基于视觉的触觉数据。输入为视觉图像和触觉图像，两者均被分割成不重叠的块。视觉输入表示为 $\mathbf{V} \in \mathbb{R}^{N_{\text{visual}} \times P}$，触觉输入表示为 $\mathbf{T} \in \mathbb{R}^{N_{\text{tactile}} \times L}$，其中 $N$ 代表块的数量，$P$ 和 $L$ 是每个块的维度。这些块通过可学习的线性投影（权重矩阵 $\mathbf{W}^{\text{visual}}$ 和 $\mathbf{W}^{\text{tactile}}$）映射到统一的嵌入维度 $D$，形成初始令牌嵌入 $\mathbf{X}^{\text{visual}}$ 和 $\mathbf{X}^{\text{tactile}}$。</p>
<p><img src="https://arxiv.org/html/2505.20032v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ViTaPEs框架总览。视觉和触觉输入被投影到独立的令牌空间，随后分别添加模态特定的位置编码（绿色和橙色）。这些编码后的令牌经过一个非线性投影头 $g$ 拼接，最后添加共享的全局位置编码（紫色）进行多模态融合，并送入统一的Transformer编码器。</p>
</blockquote>
<p>核心模块是三级位置编码方案：</p>
<ol>
<li><strong>模态特定位置编码</strong>：为视觉和触觉模态分别分配可学习的位置编码 $\mathbf{PE}^{\text{visual}}$ 和 $\mathbf{PE}^{\text{tactile}}$。它们被直接加到对应的令牌上（公式2），旨在Transformer进行跨模态混合之前，捕获每种传感器固有的独特空间先验（如视觉的全局场景布局与触觉的局部压力分布）。</li>
<li><strong>全局位置编码</strong>：这是一个跨模态共享的可学习位置编码 $\mathbf{PE}^{\text{global}} \in \mathbb{R}^{(C+N) \times D}$，其中 $N$ 是视觉和触觉令牌的总数，$C$ 指示是否包含分类令牌。关键设计是，在添加全局PE之前，先对拼接的模态特定编码令牌应用一个两层的非线性投影头 $g$（公式3），得到 $\mathbf{X}_{\text{projected}}$。然后再将全局PE加到其上（公式4）。这种设计避免了局部和全局位置信息的线性坍缩，保留了独立的模态内和跨模态位置参考。</li>
<li><strong>Transformer编码与跨注意力</strong>：融合了多级位置编码的令牌 $\mathbf{X}_{\text{global}}$ 被送入一个标准的Transformer编码器。通过多头自注意力机制（公式5），模型自然地实现了模态内自注意力和跨模态注意力。如公式6所示，注意力矩阵可以划分为四个区块，其中非对角区块（如 $\bar{Q}_t\bar{K}_i^T V_i$）明确编码了触觉查询关注视觉键值（或反之）的跨模态交互，从而促进信息在两种模态间的流动。</li>
</ol>
<p>与现有方法相比，ViTaPEs的创新点具体体现在：1) 提出了专门针对视觉触觉融合的多尺度位置编码设计，同时建模模态内结构和跨模态对齐；2) 不依赖任何预训练的视觉或语言模型，实现了端到端的联合表征学习；3) 为所提出的位置编码方案提供了理论上的可证明性质。</p>
<p><img src="https://arxiv.org/html/2505.20032v1/extracted/6480284/figures/pe_cosine_heatmap.png" alt="位置编码相似性"></p>
<blockquote>
<p><strong>图3</strong>：学习到的模态特定和全局位置编码的余弦相似性热图。左子图显示视觉和触觉模态特定的PE分别在其各自令牌间形成了清晰的空间相似性模式。右子图显示全局PE在跨模态令牌间建立了连贯的空间对应关系，验证了其促进跨模态对齐的能力。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了多个大规模真实世界数据集进行评估，包括：<strong>Visuotactile-200M</strong>（大规模配对视觉触觉数据集）、<strong>Touch and Go</strong>（用于零样本泛化评估）、<strong>GelSight Fabric</strong>（织物分类）、<strong>DIGIT Texture</strong>（纹理识别）、<strong>Hardness Dataset</strong>（硬度评估）以及一个真实的<strong>机器人抓取数据集</strong>（用于迁移学习）。</p>
<p>对比的基线方法包括：<strong>MViTac</strong>（多模态对比学习）、<strong>Unitouch</strong>（对齐预训练VLM）、<strong>VTT</strong>（视觉触觉Transformer）、<strong>CLIP</strong>、<strong>ImageBind</strong>以及一些单模态基线。</p>
<p>关键实验结果如下：</p>
<ul>
<li><strong>大规模表征学习</strong>：在Visuotactile-200M数据集上预训练后，ViTaPEs在多个下游任务（物体识别、材料分类、纹理识别、硬度评估）上平均准确率达到**90.4%<strong>，显著优于最佳基线MViTac的</strong>87.6%**。</li>
<li><strong>零样本跨域泛化</strong>：在Touch and Go数据集的“未知对象”类别上，进行零样本评估，ViTaPEs达到了**78.3%**的准确率，远超其他方法（MViTac: 63.2%, Unitouch: 58.1%），证明了其强大的分布外泛化能力。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.20032v1/x1.png" alt="任务准确性雷达图"></p>
<blockquote>
<p><strong>图4</strong>：跨多个任务的准确性雷达图对比。ViTaPEs（红色实线）在所有任务上均优于其他视觉触觉模型，并且在跨域泛化（Out-of-Domain）上表现出最强的鲁棒性。</p>
</blockquote>
<ul>
<li><strong>机器人抓取迁移学习</strong>：将在Visuotactile-200M上以自监督方式预训练的ViTaPEs，在一个较小的（10K样本）真实机器人抓取数据集上微调，用于预测抓取成功率。ViTaPEs取得了**92.4%<strong>的预测准确率，优于MViTac的</strong>89.5%<strong>和Unitouch的</strong>85.7%**。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.20032v1/x4.png" alt="机器人抓取与消融实验结果"></p>
<blockquote>
<p><strong>图5</strong>：左图展示了在机器人抓取成功率预测任务上，ViTaPEs微调后优于所有基线。右图的消融研究表明，移除模态特定PE、全局PE或非线性投影头 $g$ 都会导致性能下降，其中全局PE对跨模态任务的影响最为显著，验证了各核心组件的必要性。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：移除<strong>模态特定PE</strong>会损害模态内的空间推理；移除<strong>全局PE</strong>对需要跨模态对齐的任务性能影响最大；而移除<strong>非线性投影头 $g$</strong> 会导致性能下降，证实了其对于防止位置信息线性混合的重要性。理论性质（单射性、刚性运动等变性、信息保持）也在附录中得到了经验验证。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>创新的多尺度位置编码方案</strong>：提出了ViTaPEs，一种结合了模态特定和全局位置编码的Transformer框架，首次在视觉触觉融合中系统性地研究并利用了位置编码来实现多尺度空间对齐。</li>
<li><strong>理论保证与经验验证</strong>：首次为视觉触觉模型提供了理论分析，证明并实证验证了所提出位置编码的单射性、刚性运动等变性和信息保持性，为模型的可解释性和可靠性提供了基础。</li>
<li><strong>强大的零样本泛化与迁移能力</strong>：通过任务无关的预训练，ViTaPEs在多个分布外场景和下游任务（包括机器人抓取）上实现了卓越的零样本泛化和迁移学习性能，展示了其实际应用潜力。</li>
</ol>
<p>论文自身提到的局限性主要在于计算复杂度，因为处理高分辨率的触觉图像（如来自GelSight）会生成大量令牌，增加Transformer的计算负担。此外，方法需要配对的视觉触觉数据进行训练。</p>
<p>本文对后续研究的启示在于：1) 强调了在跨模态Transformer中，针对模态特性设计归纳偏置（如位置编码）的重要性，而非完全依赖数据驱动；2) 为跨模态表示学习提供了一个兼具理论分析和强大实证性能的框架，可激励更多工作关注模型的理论性质；3) 展示了任务无关的视觉触觉预训练在机器人等数据稀缺领域的巨大迁移价值。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多模态Transformer中视觉与触觉数据融合与跨模态对齐的挑战，提出ViTaPEs框架。其核心是引入一种新颖的多尺度位置编码方案，以同时捕捉模态内部结构并建模跨模态关联，且编码具有可证明的注入性、刚体运动等变性和信息保持性。实验表明，ViTaPEs在多个大规模真实数据集上超越现有最优方法，实现了零样本跨域泛化，并在机器人抓取任务中取得了更优的抓取成功率预测性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.20032" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>