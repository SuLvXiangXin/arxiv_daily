<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2406.19464" target="_blank" rel="noreferrer">2406.19464</a></span>
        <span>作者: Liu, Zeyi, Chi, Cheng, Cousineau, Eric, Kuppuswamy, Naveen, Burchfiel, Benjamin, Song, Shuran</span>
        <span>日期: 2024/06/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于视觉的机器人操作系统在感知和利用接触信息方面存在局限。触觉传感作为功能等效方案，虽然形式多样（如基于相机的触觉传感器、力/扭矩传感器），但通常成本较高、易碎且需要专业知识使用，限制了其可扩展性。音频信号作为另一种模态，能通过接触提供丰富的交互和物体属性信息，包括接触事件与模式、表面材料、物体状态等，可以辅助学习接触密集的操作技能。然而，先前工作中音频数据的使用受限于遥操作演示，需要将麦克风附着在机器人或物体上，这阻碍了其在机器人学习流程中的广泛应用。</p>
<p>本文针对从野外（in-the-wild）便捷收集高质量音频-视觉演示数据，并利用其学习鲁棒操作策略这一具体痛点，提出了一个新视角：设计一个“耳在手”（ear-in-hand）的低成本手持数据采集设备，并配套相应的策略学习接口。本文的核心思路是：通过改造手持夹爪嵌入接触式麦克风来同步收集野外人类演示的音频和视频，并设计一种包含音频数据增强和基于Transformer的多模态融合网络的端到端策略学习方法，以弥合训练（演示）与部署（机器人）之间的音频域差距。</p>
<h2 id="方法详解">方法详解</h2>
<p>ManiWAV的整体框架包含数据收集硬件和策略学习算法两部分。在数据收集端，输入是人类的操作动作，输出是同步的RGB视频和接触音频流（存储为MP4文件）。在策略学习端，输入是实时RGB图像和音频，输出是10自由度的机器人动作（末端执行器位置、6D朝向和夹爪开合度）。</p>
<p><img src="https://arxiv.org/html/2406.19464v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：“耳在手”夹爪用于野外数据收集。(a)手持设计在接触密集任务中为演示者提供自然触觉反馈，接触式麦克风捕获高频音频并与图像同步记录。(b)展示了训练数据与部署数据之间的域差距。(c)展示了从野外数据直接学习并部署到机器人的策略。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>硬件设计</strong>：基于通用操作接口（UMI）手持夹爪，重新设计3D打印的平行夹爪手指，将压电接触式麦克风嵌入包裹着高摩擦胶带的手指下方。麦克风连接到GoPro相机媒体模块的3.5mm外部麦克风端口。音频以48000 Hz录制，与60 Hz图像数据同步存储为MP4文件。机器人部署时，将相同的带麦克风的夹爪安装在UR5机械臂上。</li>
<li><strong>音频数据增强</strong>：为弥合图2(b)所示的音频域差距（主要由机器人电机噪声和意外碰撞等交互噪声引起），提出在训练时对原始音频信号叠加背景噪声和机器人噪声。背景噪声从ESC-50数据集中随机采样，机器人噪声通过在同一麦克风位置记录随机轨迹下的电机声音获得。每种噪声以0.5的概率叠加。该方法引导模型关注任务相关的、不变的音频信号，忽略不可预测的噪声。</li>
<li><strong>传感器编码与融合网络</strong>：<ul>
<li><strong>视觉编码器</strong>：使用CLIP预训练的ViT-B/16模型编码RGB图像（采样率20Hz，取过去2帧）。</li>
<li><strong>音频编码器</strong>：使用音频频谱图Transformer（AST）编码音频输入（过去2-3秒的音频，重采样至16kHz后转换为64维对数梅尔频谱图）。论文指出，与先前工作使用的CNN编码器相比，Transformer的自注意力机制更适合频谱图，因为时域或频域的偏移会显著改变音频信息。</li>
<li><strong>传感器融合</strong>：使用Transformer编码器融合视觉和音频特征，通过注意力机制在任务不同阶段自适应权衡特征（例如，接近目标物体时视觉重要，接触时音频重要）。融合后的特征与过去2个时间步的末端执行器位姿拼接。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2406.19464v2/x3.png" alt="网络架构"></p>
<blockquote>
<p><strong>图3</strong>：网络架构。整体为端到端的闭环感觉运动学习模型，采用扩散策略作为策略头，以传感器融合后的观察表示作为条件进行去噪。</p>
</blockquote>
<ol start="4">
<li><strong>策略学习</strong>：采用扩散策略作为策略头，以上述观察表示为条件，在每一步去噪过程中进行条件化。整个模型（图3）使用对未来16步机器人轨迹的噪声预测MSE损失进行端到端训练。</li>
<li><strong>音频延迟匹配</strong>：部署时校准音频延迟为0.23秒，并采用类似方法进行补偿。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>低成本、可扩展的野外音频-视觉数据收集硬件</strong>：“耳在手”设计使非专家也能轻松收集高质量、同步的接触音频和视频演示，无需机器人参与。</li>
<li><strong>针对音频域差距的数据增强策略</strong>：通过叠加背景和机器人噪声，有效提升策略对部署时噪声的鲁棒性。</li>
<li><strong>端到端的多模态Transformer学习框架</strong>：使用AST编码音频，并使用Transformer融合多模态特征，相比传统CNN和MLP融合方式能更好地学习任务相关的表征。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在四个接触密集的操作任务上进行：<strong>用锅铲翻转百吉饼（Flipping）</strong>、<strong>擦拭白板上的形状（Wiping）</strong>、<strong>从杯中倒出物体（Pouring）</strong>、<strong>用魔术贴捆扎电线（Taping）</strong>。实验平台涉及UR5机械臂和自制的带麦克风夹爪。对比的基线方法包括纯视觉策略、MLP策略、使用ResNet或预训练AVID模型的音频编码器、MLP融合、环境麦克风、测试时降噪算法等，并在不同测试场景（任务配置变化、音频扰动、泛化到未见过的桌子高度/环境等）下进行评估。</p>
<p><img src="https://arxiv.org/html/2406.19464v2/x5.png" alt="翻转评估"></p>
<blockquote>
<p><strong>图5</strong>：翻转任务评估。上图展示实验室内测试场景和两个未见过的野外环境。下图展示典型失败案例和任务成功率。关键发现：动作扩散策略优于MLP；Transformer音频编码器优于CNN编码器；用野外数据训练的策略在泛化到新环境时显著优于仅用实验室数据训练的策略。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>翻转任务</strong>：纯视觉策略成功率为45%，而加入音频的MLP策略提升至80%，本文的完整方法（Transformer音频编码+扩散策略）达到85%。在泛化到两个未见野外环境的测试（T4）中，用野外数据训练的策略成功率（黑色灶台80%，白色台面65%）显著高于仅用实验室数据训练的策略（45%， 20%）。</li>
<li><strong>擦拭任务</strong>：纯视觉策略成功率仅为40%，而加入音频后提升至85%。噪声增强对泛化到未见桌子高度、橡皮擦和形状至关重要，无噪声增强时成功率降至60%。Transformer融合器也优于MLP融合器。</li>
<li><strong>倾倒任务</strong>：纯视觉策略因无法判断杯内是否有物体而完全失败（成功率0%）。结合能产生振动的“摇晃”探索动作，音频策略能推断物体状态，成功率达到83%。实验表明音频历史长度（2-3秒最佳）对性能敏感。</li>
<li><strong>捆扎任务</strong>：纯视觉策略（20%）和环境麦克风策略（20%）几乎随机选择，而本文的接触麦克风方法能可靠感知魔术贴“钩面”和“毛面”的细微材质差异，成功率高达90%。训练时噪声增强优于测试时降噪算法。</li>
</ul>
<p><img src="https://arxiv.org/html/2406.19464v2/x6.png" alt="擦拭评估"></p>
<blockquote>
<p><strong>图6</strong>：擦拭任务评估。上图展示不同测试场景。下图展示典型失败案例和任务成功率。纯视觉策略常无法保持适当接触（按得太用力或悬空），而结合音频显著提升了鲁棒性和泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2406.19464v2/x4.png" alt="注意力可视化"></p>
<blockquote>
<p><strong>图4</strong>：注意力可视化。有趣的是，与音频共同训练的策略更多地关注任务相关区域（绘图形状或锅内的自由空间），而纯视觉策略常过拟合于背景结构（如白板边缘、桌子）作为估计接触的“捷径”。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2406.19464v2/x7.png" alt="倾倒评估"></p>
<blockquote>
<p><strong>图7</strong>：倾倒任务评估。第一行展示任务定义和泛化场景。第二行展示各基线的典型失败案例和总成功率。音频提供了视觉无法获取的关键状态信息。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2406.19464v2/x8.png" alt="捆扎评估"></p>
<blockquote>
<p><strong>图8</strong>：捆扎任务评估。第一行展示任务定义。第二行展示各方法的典型失败案例和总成功率。接触式麦克风对表面材料足够敏感，是任务成功的关键。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>音频模态</strong>：在所有任务中，增加音频输入均显著提升了策略性能、鲁棒性和泛化能力。</li>
<li><strong>音频编码器</strong>：从零开始训练的Transformer（AST）音频编码器优于CNN编码器（ResNet, 预训练AVID）。</li>
<li><strong>策略头</strong>：扩散策略头优于MLP策略头，能更好地捕捉演示中的多模态性。</li>
<li><strong>融合方式</strong>：Transformer融合器优于MLP融合器。</li>
<li><strong>数据增强</strong>：训练时噪声增强对于弥合音频域差距、提升对部署噪声的鲁棒性和泛化能力至关重要，其效果优于简单的频率掩码或测试时降噪。</li>
<li><strong>训练数据源</strong>：使用多样化的野外演示数据训练，能显著提升策略在未见野外环境中的泛化能力。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ManiWAV系统，包含一个低成本、便携的“耳在手”音频-视觉数据收集设备，极大地简化了野外接触密集操作演示的收集流程。</li>
<li>提出了一种有效的音频数据增强策略和一个端到端的基于Transformer的多模态策略学习框架，成功弥合了演示与机器人部署之间的音频域差距，并学习了鲁棒的任务相关表征。</li>
<li>通过四个具挑战性的接触密集操作任务，系统性地验证了音频信号在感知接触事件/模式、物体状态和表面材料方面的独特优势，以及所提方法相对于多种基线的优越性和泛化能力。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>对于不产生明显声音信号的交互（如操作布料等可变形物体或准静态任务），接触麦克风可能作用有限。</li>
<li>机器人电机噪声在部署时可能掩盖有用的音频信号。</li>
<li>当前方法未充分利用音频信号比图像更高的频率特性来学习更反应性的行为。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>音频作为一种廉价、易得、信息丰富的传感模态，为机器人接触感知与操作学习提供了一条可扩展的新途径，可作为触觉传感的有效补充或替代。</li>
<li>针对多模态（尤其是跨域）数据的数据增强和表征学习技术是提升策略鲁棒性和泛化能力的关键。</li>
<li>未来工作可探索分层网络架构，利用高频音频信号来推断更高频率的动作，实现更灵敏的闭环控制。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ManiWAV系统，旨在解决机器人操作中仅依赖视觉信息在接触感知方面存在局限性的问题。核心方法包括：1）设计“手中有耳”采集设备，低成本收集野外人类演示的同步音视频数据；2）构建策略接口直接从演示数据学习操作策略。实验在四个需感知接触事件、模式、表面材料及物体状态的任务中验证了音频信息的有效性，并证明系统能通过学习多样化野外演示泛化到未知环境。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2406.19464" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>