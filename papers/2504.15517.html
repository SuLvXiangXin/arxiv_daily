<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Few-Shot Vision-Language Action-Incremental Policy Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Few-Shot Vision-Language Action-Incremental Policy Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.15517" target="_blank" rel="noreferrer">2504.15517</a></span>
        <span>作者: Song, Mingchen, Deng, Xiang, Zhong, Guoqiang, Lv, Qi, Wan, Jia, Li, Yinchuan, Hao, Jianye, Guan, Weili</span>
        <span>日期: 2025/04/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于Transformer的策略已成为机器人操作的主流方法。它们利用多视角空间表征和语言指令，通过大量机器人演示数据来学习运动轨迹。然而，这类方法存在两个关键局限：首先，收集大量高质量的机器人演示数据极其困难且昂贵；其次，现有方法缺乏在仅有少量演示的情况下对新任务进行持续学习的能力，容易在适应新任务时遗忘旧技能，即灾难性遗忘问题。</p>
<p>本文针对机器人模仿学习中数据稀缺和持续学习能力不足这两个具体痛点，提出了“少样本动作增量学习”这一新任务，并设计了名为TOPIC的策略。其核心思路是通过学习任务特定提示来从少量演示中提取判别性信息，并构建任务关系图，利用任务间的内在联系进行策略权重的连续演化，从而在适应新任务的同时保留旧技能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的任务提示图演化策略旨在解决少样本动作增量学习任务。该框架可灵活集成到现有的基于Transformer的策略中，增强其持续学习能力。</p>
<p><img src="https://arxiv.org/html/2504.15517v1/x2.png" alt="方法比较"></p>
<blockquote>
<p><strong>图2</strong>：现有Transformer-based策略与本文提出的TOPIC对比。(a) 为RVT等现有策略；(b) 为TOPIC，它可与现有策略集成，利用少量演示增强其持续学习能力。</p>
</blockquote>
<p>整体流程如下：模型首先在包含大量数据的基准任务上进行训练。随后，在增量学习阶段，每个新任务仅提供少量演示。TOPIC通过其核心模块处理这些演示：<strong>任务特定提示模块</strong>从多模态数据中提取任务特异性信息；<strong>连续进化策略模块</strong>则利用提取的信息构建并更新任务关系图，据此演化策略权重以适应新任务，同时复用旧技能。</p>
<p>核心模块一：<strong>任务特定提示</strong>。为缓解数据稀缺问题，该方法预定义了一组随机初始化的可学习提示向量。这些提示向量与经过编码的语言标记和视觉标记拼接后，输入到多视角Transformer编码器中进行深度跨模态交互。</p>
<p><img src="https://arxiv.org/html/2504.15517v1/x3.png" alt="TSP结构"></p>
<blockquote>
<p><strong>图3</strong>：任务特定提示结构。一组预定义的可学习提示向量通过多视角Transformer编码器与其他模态信息深度交互，从少量演示中提取任务特定的判别信息。</p>
</blockquote>
<p>通过注意力机制，提示向量能够聚合来自不同模态的、与当前任务相关的信息。编码后得到的任务特定提示，通过一个投影模块（恒等映射、线性变换或MLP）调整维度，然后以广播加法的形式与提取的特征结合，生成最终用于指导动作预测的输出特征。这一过程使模型能够将任务特定信息直接融入动作空间。</p>
<p>核心模块二：<strong>连续进化策略</strong>。直觉上，技能在不同任务间是可复用的。该方法利用提取的任务特定提示来表征任务，并将每个提示视为任务关系图中的一个节点。通过计算新任务提示与所有已有任务提示之间的余弦距离作为关系系数，可以建模任务间的内在联系。</p>
<p>当适应新任务时，新任务的策略权重由两部分组成：一是所有先前任务策略权重的加权和（权重由关系系数决定），二是从基准任务中学到的通用技能权重。通过两个系数λ1和λ2进行调节，最终得到更新后的策略权重。这种机制使得模型能够根据任务相似性复用旧技能，并保留通用技能，从而有效适应新任务并减轻遗忘。</p>
<p>与现有方法相比，创新点体现在：1) 提出了专门针对具身任务的提示学习方法，从少量多模态演示中提取判别特征；2) 引入了基于任务关系图进行策略权重连续演化的机制，实现了技能在任务间的显式迁移和复用。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境（LIBERO和RLBench基准）以及真实机器人场景中进行。对比的基线方法包括基于Transformer的策略（如RVT、RVT2）、持续学习方法（如L2A、EWC）以及专为LIBERO设计的方法（如LOTUS、TAIL）。</p>
<p>关键实验结果如下：在LIBERO基准的5-shot设置下，TOPIC在10个增量任务后的平均成功率达到67.6%，显著优于最佳基线L2A的40.6%，相对提升超过26%。在RLBench基准上，TOPIC在1-shot和5-shot设置下也均取得最佳性能。</p>
<p><img src="https://arxiv.org/html/2504.15517v1/x4.png" alt="仿真结果"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO和RLBench基准上的性能对比。TOPIC在少样本增量学习设置下，成功率显著超越所有基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.15517v1/x5.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人实验的成功率。TOPIC在真实场景的5-shot增量学习中也表现出最优性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.15517v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。移除任务特定提示或连续进化策略都会导致性能显著下降，证明了两个核心组件的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.15517v1/x7.png" alt="任务关系可视化"></p>
<blockquote>
<p><strong>图7</strong>：任务关系图可视化。学习到的任务特定提示能够反映任务间的语义相似性（如都涉及“打开”的任务聚类在一起）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.15517v1/x8.png" alt="定性结果"></p>
<blockquote>
<p><strong>图8</strong>：定性结果对比。在“打开微波炉”等任务上，TOPIC能成功执行，而基线方法失败。</p>
</blockquote>
<p>消融实验表明，任务特定提示和连续进化策略两者缺一不可。移除TSP会导致模型无法从少量演示中有效学习新任务；移除CES则会导致严重的灾难性遗忘。此外，在提示投影方式中，MLP投影效果最佳。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 首次在机器人操作任务中定义了少样本动作增量学习问题，并提出了TOPIC框架；2) 设计了适用于具身任务的任务特定提示，能够从少量多模态演示中提取判别信息；3) 引入了基于任务关系图的连续进化策略，通过显式建模和复用任务间技能来增强持续学习能力。</p>
<p>论文自身提到了局限性：该方法仍依赖于一个包含大量演示的基准任务会话来学习通用技能。对于完全从零开始的少样本持续学习，仍需进一步探索。</p>
<p>本文的启示在于：将提示学习与基于图的关系建模结合，为机器人少样本持续学习提供了一个有效的新范式。未来工作可以探索更动态的图构建方式，或将该框架应用于更广泛的具身AI任务序列中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习中数据收集困难及现有方法难以用少量演示持续学习新任务的问题，提出Few-Shot Action-Incremental Learning (FSAIL)任务。为解决这些问题，设计了Task-prOmpt graPh evolutIon poliCy (TOPIC)方法，其关键技术包括Task-Specific Prompts (TSP)通过多模态交互提取任务判别信息，以及Continuous Evolution Strategy (CES)构建任务关系图以重用先前技能、减轻灾难性遗忘。实验结果显示，TOPIC在成功率上超越最先进基线方法超过26%，显著提升了持续学习性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.15517" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>