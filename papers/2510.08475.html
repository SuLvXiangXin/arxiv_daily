<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.08475" target="_blank" rel="noreferrer">2510.08475</a></span>
        <span>作者: Hsieh, Jhen, Tu, Kuan-Hsun, Hung, Kuo-Han, Ke, Tsung-Wei</span>
        <span>日期: 2025/10/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧操作是机器人执行日常任务的关键，尤其需要协调多臂和多手指。现有学习方法，如行为克隆（BC）和强化学习（RL），面临高维动作空间带来的挑战：BC需要收集大量遥操作演示成本高昂，RL则存在固有的样本效率低下问题。向形态相似的人类学习是自然思路，近期大规模视频数据集和自动化手部姿态估计框架的发展使得从视频模仿成为可能。然而，人类与机器人之间的“形态差距”（物理属性、几何和运动学差异）导致直接模仿常产生不稳定抓握或不可达姿态。</p>
<p>为弥合此差距，近期工作结合RL，同时模仿人类行为并追求视频中目标物体的轨迹。但这类方法通常需要精确的地面真实物体姿态，这严重依赖于动捕数据集，限制了其在无标注的单目RGB视频上的可扩展性。针对这些局限，本文提出了DexMan框架，旨在仅使用未经校准的第三人称单目RGB视频（无需相机标定、深度传感器或3D物体模型），自动将其转换为仿真中人形机器人的双手灵巧操作技能。本文核心思路是：从视频中重建3D场景并恢复手-物体运动，然后通过一个由人类运动先验和新型接触奖励引导的残差RL策略，训练机器人复现目标物体轨迹。</p>
<h2 id="方法详解">方法详解</h2>
<p>DexMan是一个将人类操作视频转换为机器人技能的自动化框架，其整体流程包含四个阶段：1) 从输入视频重建3D物体；2) 恢复人类手部和物体运动；3) 在仿真中构建稳定的交互式3D场景并将人类运动重定向到人形机器人；4) 在人类运动和接触先验的引导下，训练残差RL策略以复现目标物体轨迹。其核心创新在于完整的端到端流程、针对噪声运动先验的稳定场景配置方法以及引导稳定抓握的接触奖励设计。</p>
<p><img src="https://arxiv.org/html/2510.08475v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>: DexMan框架总览。上方：从单目输入视频出发，重建物体网格、估计深度、恢复3D手-物体运动，并将其重定向到仿真中的完整人形机器人（而非漂浮的手）。下方：一个残差RL策略在人类运动和接触先验的引导下，对重定向后的运动进行细化，以复现物体轨迹。DexMan引入了鼓励稳定抓握的接触奖励以促进有效的RL训练。</p>
</blockquote>
<p><strong>3D物体重建</strong>：首先需要从视频中构建交互式3D场景。目标物体（即被操作物体）在实验中为手动标识。对于每个目标物体，使用SAM2分割物体掩码，裁剪图像块，并由Trellis从裁剪图像生成刚体3D网格。这些网格缺乏正确的尺度和姿态，需要通过后续步骤恢复。</p>
<p><strong>手部和物体姿态估计</strong>：这是恢复运动先验的关键。DexMan利用估计的视频深度图来重缩放物体网格并恢复精确物体姿态。具体分三步：1) <strong>深度估计</strong>：使用VGGT模型预测视频帧序列的深度。2) <strong>手部姿态估计</strong>：采用HaMeR框架输出度量准确的MANO手部网格，并通过与VGGT点云对齐确定全局缩放因子，统一应用于所有深度图。3) <strong>物体姿态估计</strong>：首先借鉴Any6D方法，通过采样多个候选尺度并用FoundationPose估计姿态来选择最佳尺度。为进一步提升在快速运动或遮挡下的时间一致性，引入SpatialTracker跟踪3D点轨迹，计算帧间刚性变换作为FoundationPose的初始化，从而获得更平滑的估计。</p>
<p><strong>从人类到人形机器人的运动重定向</strong>：此阶段目标是在仿真中放置物体和机器人，并将人类运动映射到机器人本体。<strong>场景配置</strong>：将人形机器人置于原点。为了对齐，通过三次空间变换将视频相机坐标系下的人类身体姿态与仿真器坐标系近似对齐（利用桌面法线估计重力方向等）。<strong>稳定物体放置</strong>：由于重建的3D网格可能存在物理缺陷，直接放置可能导致物体摇晃或倾倒。DexMan提出一种基于采样的稳定配置方法：对原始姿态施加20个随机旋转（绕采样轴±45°内），模拟每个配置20步，选择与原始姿态旋转偏差最小的稳定姿态。</p>
<p><img src="https://arxiv.org/html/2510.08475v1/x3.png" alt="采样稳定物体配置"></p>
<blockquote>
<p><strong>图3</strong>: 采样稳定物体配置。DexMan用随机轴和角度扰动物体姿态，模拟每个配置，并选择最接近原始姿态的稳定配置用于仿真放置。</p>
</blockquote>
<p><strong>运动重定向</strong>：采用分阶段策略分别处理手腕和手指。手腕重定向使用逆运动学（IK）求解器计算所需关节角。手指重定向则通过监督学习训练一个5层MLP神经网络IK求解器，输入为单手五个指尖的3D位置，输出为对应的手指关节角。</p>
<p><strong>基于噪声运动先验的强化学习</strong>：运动重定向无法解决估计的人类运动本身的物理不一致或噪声问题。因此，DexMan训练一个残差RL策略来将噪声重定向运动转化为可行的机器人技能。策略观察机器人的本体感知和物体状态，预测对手腕位置、手腕方向以及手部关节角的残差修正，这些修正通过前述IK求解器转换为可执行的机器人关节角。其核心创新在于<strong>接触先验吸引奖励</strong>的设计。</p>
<p><strong>奖励函数设计</strong>：总奖励由三部分构成：</p>
<ol>
<li><strong>接触奖励</strong>：旨在引导机器人实现稳定抓握。首先离线提取接触先验：对于每一帧，为每个手部关键点（如指尖、掌心）在目标物体网格上找到最近顶点，将距离低于阈值的配对作为接触候选。在线奖励包含两项：a) <strong>吸引项</strong>：将机器人关键点拉向人类接触的物体顶点；b) <strong>方向对齐项</strong>：确保关键点到顶点的向量与关键点表面法线对齐，防止不现实的手背接触。公式中还加入了物体被提起时的额外奖励系数。</li>
<li><strong>物体跟随奖励</strong>：强制策略复现目标物体的参考轨迹，奖励当前物体姿态（位置和旋转）与参考姿态的接近程度。</li>
<li><strong>模仿奖励</strong>：鼓励机器人模仿人类的手腕和手指运动。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.08475v1/x4.png" alt="接触奖励"></p>
<blockquote>
<p><strong>图4</strong>: 接触奖励。吸引项将机器人手部关键点 $\mathbf{p}<em>{j,t}^{\mathrm{rob}}$ 拉向人类接触的物体顶点 $\mathbf{v}</em>{j,t}^{o}$，并使关键点-顶点向量与表面法线 $\mathbf{n}_{j,t}^{\mathrm{rob}}$ 对齐，确保抓握范围内的接触。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：DexMan在三个场景下进行评估：1) <strong>物体姿态估计</strong>：在动捕数据集TACO上进行。2) <strong>残差RL策略</strong>：在双手灵巧操作基准OakInk-v2上进行。3) <strong>端到端视频到技能获取</strong>：分别在TACO的真实视频和Veo3生成的合成视频上进行。</p>
<p><strong>物体姿态估计结果</strong>：在TACO数据集上，DexMan的物体姿态估计流程与两个SOTA基线（FoundationPose和SpatialTracker）进行对比。评估指标包括ADD-S、VSD（均为AUC分数）、失败率和时间稳定性。</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">ADD-S ↑</th>
<th align="left">VSD ↑</th>
<th align="left">失败率 ↓</th>
<th align="left">时间稳定性 ↑</th>
</tr>
</thead>
<tbody><tr>
<td align="left">FoundationPose</td>
<td align="left">0.49</td>
<td align="left">0.70</td>
<td align="left">0.14</td>
<td align="left">0.70</td>
</tr>
<tr>
<td align="left">SpatialTracker</td>
<td align="left">0.55</td>
<td align="left">0.56</td>
<td align="left">0.13</td>
<td align="left">0.79</td>
</tr>
<tr>
<td align="left"><strong>DexMan (ours)</strong></td>
<td align="left"><strong>0.57</strong></td>
<td align="left"><strong>0.82</strong></td>
<td align="left"><strong>0.01</strong></td>
<td align="left"><strong>0.76</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>: TACO上的姿态估计性能。DexMan在精度指标（ADD-S, VSD）和降低失败率方面均优于基线。</p>
</blockquote>
<p>如表1所示，DexMan在ADD-S和VSD上分别比FoundationPose绝对提升了0.08和0.12，并将失败率从0.14大幅降低至0.01。</p>
<p><img src="https://arxiv.org/html/2510.08475v1/figs/fp_1.jpg" alt="物体姿态估计视觉对比"></p>
<blockquote>
<p><strong>图5</strong>: 物体姿态估计的视觉对比。展示了带有定向3D边界框和坐标轴的估计物体姿态。DexMan（“Ours”）因结合了3D点轨迹运动线索，相比FoundationPose的输出，产生了更稳定和准确的姿态估计。</p>
</blockquote>
<p><strong>残差RL策略结果</strong>：在OakInk-v2基准上，使用其提供的真实物体模型、手和物体运动标注，对比SOTA方法MANIPTRANS。DexMan控制的是完整的Unitree G1人形机器人（配备Shadow灵巧手），而MANIPTRANS控制的是简化的漂浮手。评估指标为成功率以及成功回合中的平均旋转误差和位移误差。</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">成功率 ↑</th>
<th align="left">$E_r$ ↓</th>
<th align="left">$E_t$ ↓</th>
</tr>
</thead>
<tbody><tr>
<td align="left">MANIPTRANS</td>
<td align="left">25.3%</td>
<td align="left">0.180</td>
<td align="left">0.00646</td>
</tr>
<tr>
<td align="left"><strong>DexMan (ours)</strong></td>
<td align="left"><strong>44.3%</strong></td>
<td align="left"><strong>0.178</strong></td>
<td align="left"><strong>0.00688</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表2</strong>: OakInk-v2上的双手灵巧操作。DexMan的成功率显著高于基线。</p>
</blockquote>
<p>如表2所示，尽管控制更复杂的人形机器人，DexMan取得了44.3%的成功率，比MANIPTRANS（25.3%）绝对高出19%。论文将此归因于接触奖励促进了稳定抓握，从而实现了更可靠的任务执行。</p>
<p><strong>端到端视频到技能获取结果</strong>：这是DexMan框架的最终测试，从无标注的单目RGB视频（包括TACO真实视频和Veo3合成视频）直接获取技能。评估了完整框架（DexMan）以及消融版本（分别去除物体跟随奖励和接触奖励）。</p>
<table>
<thead>
<tr>
<th align="left">数据源</th>
<th align="left">方法</th>
<th align="left">成功率 ↑</th>
<th align="left">$E_r$ ↓</th>
<th align="left">$E_t$ ↓</th>
<th align="left">轨迹→掩码IoU ↑</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>TACO</strong></td>
<td align="left">DexMan (ours)</td>
<td align="left"><strong>27.4%</strong></td>
<td align="left">0.314</td>
<td align="left">0.016</td>
<td align="left">49.0</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">w/o 物体跟随奖励</td>
<td align="left">18.4%</td>
<td align="left">0.330</td>
<td align="left">0.022</td>
<td align="left">42.8</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">w/o 接触奖励</td>
<td align="left">7.2%</td>
<td align="left">0.182</td>
<td align="left">0.018</td>
<td align="left">61.9</td>
</tr>
<tr>
<td align="left"><strong>合成视频</strong></td>
<td align="left">DexMan (ours)</td>
<td align="left"><strong>39.0%</strong></td>
<td align="left">0.248</td>
<td align="left">0.017</td>
<td align="left">44.7</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">w/o 物体跟随奖励</td>
<td align="left">34.6%</td>
<td align="left">0.288</td>
<td align="left">0.023</td>
<td align="left">41.3</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">w/o 接触奖励</td>
<td align="left">7.8%</td>
<td align="left">0.165</td>
<td align="left">0.013</td>
<td align="left">59.9</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表3</strong>: 视频到机器人技能获取结果。展示了在真实和合成视频上的端到端性能及消融实验。</p>
</blockquote>
<p>如表3所示，在TACO视频上，DexMan成功恢复了27.4%的技能；在合成视频上，成功率更高，达到39.0%。消融实验表明，<strong>接触奖励对成功至关重要</strong>：去除后，在两类视频上的成功率均暴跌至约7%。物体跟随奖励也对性能有显著正面影响。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>首个完整的视频到技能框架</strong>：提出了DexMan，一个从单目RGB视频（无需校准、深度或标注）自动学习人形机器人双手灵巧操作技能的端到端框架。</li>
<li><strong>改进的姿态估计与接触奖励</strong>：通过结合3D点轨迹提升了物体姿态估计的鲁棒性；设计了新颖的接触先验吸引奖励，有效引导RL策略形成稳定抓握，克服了噪声运动先验带来的学习困境。</li>
<li><strong>处理合成视频的能力</strong>：框架能够从生成的合成视频中学习技能，为无需人工数据收集、构建大规模多样化机器人数据集以训练通用操作策略开辟了道路。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前框架中目标物体需要手动标识（尽管可被自动化方法替代），且整个技能获取流程在仿真中进行。</p>
<p><strong>启示</strong>：DexMan展示了利用丰富但噪声的人类视频数据以及无限生成的合成视频数据来规模化学习复杂机器人技能的潜力。其接触奖励的设计思路——将学习目标从模仿全局轨迹转向实现对象中心的局部接触关系——为解决从噪声演示中学习的问题提供了新方向。这激励后续研究进一步探索如何更好地利用视觉基础模型和视频生成模型，构建大规模、自动化的机器人技能学习范式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DexMan框架，解决从人类视频学习双手灵巧操作时存在的形态差异及对精确运动捕捉数据的依赖问题。方法通过自动化处理第三人称视频，无需标定或深度信息，利用基于接触的奖励改进从噪声姿态估计中学习的策略，并支持从真实与合成视频生成技能。实验表明，其在TACO数据集上物体姿态估计达到SOTA（ADD-S/VSD分别提升0.08/0.12），在OakInk-v2上的RL策略成功率超越先前方法19%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.08475" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>