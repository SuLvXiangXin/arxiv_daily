<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>$\mathcal{E}_0$ : Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>$\mathcal{E}_0$ : Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.21542" target="_blank" rel="noreferrer">2511.21542</a></span>
        <span>作者: Guangrun Wang Team</span>
        <span>日期: 2025-11-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型为机器人操作提供了一个统一的框架，但在泛化性和精细控制方面仍面临挑战。现有方法主要分为两类：一是<strong>离散建模</strong>，包括自回归（AR）令牌预测和基于掩码的离散扩散，它们与预训练的视觉-语言模型（VLM）骨干架构亲和，但受限于语言分词器的词汇表大小，导致动作分辨率低，且掩码扩散会引入分布不匹配，破坏前向-反向一致性；二是<strong>连续建模</strong>，即基于连续扩散的策略，虽然表达能力强，但其连续的欧几里得空间与预训练VLM/VLA骨干的离散符号结构语义不对齐，削弱了语言指令与动作生成的联系，同时连续轨迹与机器人硬件执行时固有的量化本质不符。</p>
<p>本文针对上述离散方法动作分辨率受限、分布不匹配，以及连续方法语义不对齐、物理不一致等具体痛点，提出了 <strong>“连续化离散扩散”</strong> 的新视角。其核心思路是：将动作生成建模为对<strong>高斯噪声扰动的独热动作向量</strong>进行迭代去噪，从而在保持与预训练骨干语义对齐的同时，精确匹配真实机器人控制的量化特性，并支持任意精细的动作离散化。</p>
<h2 id="方法详解">方法详解</h2>
<p>ℰ₀ 的整体框架基于一个预训练的VLM骨干（PaliGemma）和一个额外的动作专家网络。其目标是对条件分布 <code>p(A_t | o_t)</code> 进行建模，其中 <code>A_t</code> 是未来 <code>H</code> 个时间步的动作块，<code>o_t</code> 是多模态观测（多视角RGB图像、语言指令、本体感知状态）。</p>
<p><img src="https://arxiv.org/html/2511.21542v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ℰ₀ 的整体架构与流程。(a) 模型整体架构，包含VLM骨干和动作专家。(b) 训练与推理流程，展示了输入如何被编码、扩散并解码为可执行的动作序列。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>动作量化与表示</strong>：每个动作维度（如7维：3平移、3旋转、1夹爪）使用分位数离散化方案，将连续值映射到 <code>N</code> 个离散区间（实验中 <code>N=2048</code>）。一个包含 <code>H=50</code> 步的动作块因此被表示为长度为 <code>L_a = H × D_a</code> 的离散令牌序列，每个令牌对应一个独热向量。这种设计突破了传统AR-VLA模型256个分箱的限制，实现了高分辨率动作表示。</li>
<li><strong>连续化离散扩散过程</strong>：<ul>
<li><strong>训练（噪声添加）</strong>：从干净离散动作 <code>Ã_t</code>（经平滑因子α=0.1平滑后）出发，采样时间步 <code>τ ∈ [0,1]</code>（遵循Beta分布以偏向高噪声区域），并添加高斯噪声：<code>Ã_t^τ = τ * Ã_t + (1-τ) * ε</code>，其中 <code>ε ~ N(0, I)</code>。这构成了前向过程。</li>
<li><strong>网络与损失</strong>：给定噪声动作 <code>Ã_t^τ</code> 和观测 <code>o_t</code>，网络输出每个动作位置的逻辑值 <code>v_θ</code>，通过Softmax定义分类分布 <code>p_θ(A_t | Ã_t^τ, o_t)</code>。训练目标是最小化预测分类与真实动作令牌之间的交叉熵损失。</li>
<li><strong>推理（迭代去噪）</strong>：从纯噪声开始，进行多步迭代去噪。首先编码观测并缓存键值对 <code>KV(o_t)</code>。在每一步 <code>i</code>，模型结合当前噪声动作 <code>Ã_t^{τ_i}</code> 和固定的观测缓存，预测出分类分布，取argmax得到去噪后的独热动作估计 <code>Â_t^{(i)}</code>。然后重新应用前向噪声过程，生成下一步的噪声输入：<code>Ã_t^{τ_{i+1}} = τ_{i+1} * Â_t^{(i)} + (1-τ_{i+1}) * ε</code>。经过 <code>N</code> 步后，将最终的离散令牌反量化回连续动作。</li>
</ul>
</li>
<li><strong>球形视角扰动增强</strong>：为了提高对相机视角变化的鲁棒性，该方法在训练时对输入图像进行球形扭曲。给定相机内参，将像素反投影到固定深度的3D点，施加一个随机的偏航-俯仰旋转 <code>R(Δφ, Δθ)</code>，再重投影得到扭曲图像。同时，每个视图关联一个表示相机偏移 <code>δ=(d, θ, φ)</code> 的3D向量，通过一个可学习的投影层 <code>f_proj</code> 映射到令牌空间，并加到图像令牌嵌入中。这种数据增强与位置嵌入相结合，显式建模了动态相机扰动，提升了跨视角一致性。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li>与<strong>连续扩散</strong>相比：ℰ₀ 在离散空间操作，与预训练VLM的符号表示自然对齐，增强了语义条件作用；并且其离散分布精确匹配了机器人硬件执行时的真实量化动作分布，理论上具有贝叶斯最优去噪器，有利于泛化。</li>
<li>与<strong>自回归/掩码离散扩散</strong>相比：ℰ₀ 支持任意精细的离散化分箱（如2048），实现了高分辨率动作控制；同时，其基于高斯噪声的连续化扩散过程保持了前向-反向一致性，避免了掩码机制带来的分布不匹配问题。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：在三个具有代表性的仿真基准上进行评估：LIBERO（多样化的操作任务）、VLABench（强调语言理解和常识推理的开放任务）、ManiSkill（精细操作技能，如插入、堆叠）。</li>
<li><strong>对比方法</strong>：包括 Diffusion Policy, MDT, RDT, Dita, TraceVLA, SpatialVLA, OpenVLA, Octo, π₀ FAST, π₀, π₀.5 等强基线。</li>
<li><strong>评估指标</strong>：任务成功率（%）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>如表1所示，ℰ₀ 在三个基准的14个不同环境/任务子集上取得了最佳平均性能（60.8%），平均超越基线10.7%。具体而言：</p>
<ul>
<li><strong>LIBERO</strong>：ℰ₀ 取得了最高平均成功率（97.2%），在复杂场景中表现出更准确的任务指令执行能力。</li>
<li><strong>ManiSkill</strong>：在需要精确空间对齐的任务（如Plug插入）上，ℰ₀ 结合了扩散的精细运动合成和自回归的语义理解优势，表现最佳。</li>
<li><strong>VLABench</strong>：ℰ₀ 在需要多模态推理的任务上展现了优异平衡。如图4所示，在“拾取黑桃3”任务中，ℰ₀ 能正确识别并精确抓取目标牌，而 π₀ 和 π₀ FAST 错误识别了花色，π₀.5 识别正确但抓取不精确。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.21542v1/figs/taskshow_vlabench.png" alt="VLABench任务对比"></p>
<blockquote>
<p><strong>图4</strong>：在VLABench“拾取黑桃3”任务上的定性对比。ℰ₀ 正确识别并精确抓取了目标扑克牌，展示了卓越的多模态推理和控制能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.21542v1/x1.png" alt="实验结果汇总表"></p>
<blockquote>
<p><strong>图1</strong>：不同动作建模范式概览，并隐含了方法对比的背景。(a) 传统离散建模，(b) 连续扩散建模，(c) 本文提出的ℰ₀ 方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.21542v1/x3.png" alt="基准测试概览"></p>
<blockquote>
<p><strong>图3</strong>：评估所使用的基准测试示意图。(a) LIBERO，(b) ManiSkill，(c) VLABench。</p>
</blockquote>
<p><strong>消融实验与组件贡献</strong>：<br>论文通过消融实验验证了各组件贡献（总结于其补充材料图7）。关键结论包括：</p>
<ol>
<li><strong>连续化离散扩散核心</strong>：相比掩码离散扩散和连续扩散，本文提出的高斯噪声离散扩散形式带来了显著的性能提升。</li>
<li><strong>高分辨率动作词汇</strong>：将分箱数从256提升至2048，对需要精细控制的任务（如插入）带来了巨大增益。</li>
<li><strong>球形视角扰动增强</strong>：该数据增强方法是一个即插即用的模块，能一致地提升基线模型和ℰ₀ 的视角泛化能力，且无需额外数据收集。</li>
</ol>
<p><strong>真实世界实验</strong>：<br>在Franka Research 3机械臂上的真实世界评估进一步证实了ℰ₀ 能够产生精确、鲁棒且可迁移的操作技能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>ℰ₀</strong>，一个<strong>连续化离散扩散框架</strong>，用于VLA模型中的动作建模。它支持任意精细的离散化，实现了高精度动作表示，同时保持了与预训练视觉-语言模型的兼容性，并匹配了机器人控制的真实量化本质。</li>
<li>引入了<strong>球形视角扰动增强</strong>及相对球形嵌入机制，显式建模动态相机扰动，显著提升了动作生成的跨视角一致性和鲁棒性。</li>
<li>通过<strong>大量实验</strong>在多个仿真基准和真实机器人任务上验证了ℰ₀ 的有效性，覆盖了广泛的场景、物体类型和任务复杂度，并取得了最先进的性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，其方法依赖于预训练的VLM骨干，性能可能受限于该骨干的能力。此外，离散扩散的迭代去噪过程在推理时可能比单步自回归模型需要更多的计算步骤。</p>
<p><strong>启示</strong>：这项工作确立了离散扩散作为可泛化VLA策略学习的一个有前景的方向。它表明，在机器人控制中，精确匹配动作表示的<strong>离散/量化本质</strong>至关重要，而不仅仅是追求连续的、高精度的回归。未来研究可以探索更高效的离散扩散推理策略，或将此框架扩展到更复杂的多模态决策序列生成中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型在多样化任务、场景和视角下泛化能力不足，以及生成动作粗糙或不稳定的问题，提出了ℰ₀框架。其核心技术是**连续化离散扩散方法**，将动作生成建模为对**量化动作令牌的迭代去噪**过程，并引入了**球面视角扰动增强**以提升对摄像机偏移的鲁棒性。实验表明，ℰ₀在LIBERO等14个环境中实现了最先进性能，**平均超越强基线10.7%**，并在真实机器人上验证了其精确、鲁棒和可迁移的操控能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.21542" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>