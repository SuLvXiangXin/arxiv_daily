<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Real-Time Robot Execution with Masked Action Chunking - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Real-Time Robot Execution with Masked Action Chunking</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.20130" target="_blank" rel="noreferrer">2601.20130</a></span>
        <span>作者: Gaowen Liu Team</span>
        <span>日期: 2026-01-27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人等网络物理系统的实时控制中，异步推理作为一种系统级范式，允许在当前动作块执行的同时预测下一个动作块，从而确保动作流的连续性。然而，将异步推理与动作分块策略结合时，会放大现有问题并导致性能显著下降。先前的研究主要关注缓解<strong>块间不连续性</strong>，即连续动作块边界处的动作跳跃问题，并提出了测试时修正算法，但这些方法要么是启发式的容易失败，要么会引入额外的延迟。本文指出一个被忽视的关键失败模式：<strong>块内不一致性</strong>，即在单个动作块内，由于推理延迟，机器人实际执行的部分动作是基于过时观测的，导致感知与执行不匹配。本文旨在同时改善异步推理结合分块策略时的块间连续性和块内一致性。核心思路是通过<strong>掩码动作分块</strong>在预训练策略上学习纠正性调整，并引入<strong>前缀保留采样</strong>流程来增强连续性，从而在不增加额外延迟的情况下获得更可靠的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法REMAC旨在将预训练的动作分块流匹配策略 $\mathbf{v}<em>{\pi}(\mathbf{A}</em>{t}|\mathbf{o}<em>{t})$，适配为一个延迟感知的策略 $\hat{\mathbf{v}}</em>{\pi}(\mathbf{A}<em>{t}|\mathbf{o}</em>{t},d)$，以应对异步推理带来的挑战。</p>
<p><img src="https://arxiv.org/html/2601.20130v1/x1.png" alt="执行范式"></p>
<blockquote>
<p><strong>图1</strong>：执行范式示意图。相同样式的箭头线表示同时发生的过程。(a) 同步推理：VLA预测与机器人执行顺序交替进行。(b) 异步推理：VLA预测与执行并发运行。(c) 异步推理虽然实现了实时执行，但引入了两个导致性能下降的挑战：加剧的块间不连续性和块内不一致性。</p>
</blockquote>
<p>整体框架分为两个核心部分：1) 基于掩码动作分块的训练时策略适应；2) 前缀保留采样。训练阶段，通过前缀掩码、自条件课程学习和残差对齐对预训练策略进行微调。推理阶段，使用训练好的延迟感知策略，并采用前缀保留采样流程生成动作块。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>前缀掩码</strong>：为应对块内不一致性，在训练损失中引入一个延迟条件前缀掩码 $\mathbf{m}<em>{d}$（$d$为随机采样的推理延迟）。该掩码将监督信号限制在每个动作块中“即将被执行”的部分（即时间步 $\tau \geq d$ 的部分），而忽略已被“执行”（基于旧观测）的前缀部分。损失函数 $\mathcal{L}</em>{\mathrm{m}}$（公式3）仅计算未掩码部分的预测流与真实目标之间的 $\ell_2$ 误差。</li>
<li><strong>自条件课程学习</strong>：为了在训练中模拟测试时条件（即部分动作已由先前策略执行），提出一种课程学习方法。它不直接使用真实动作块 $\mathbf{A}<em>{t}$ 与噪声插值作为模型输入，而是将其与预训练策略预测的动作块 $\tilde{\mathbf{A}}</em>{t}$ 混合，得到 $\hat{\mathbf{A}}_{t}$（公式4）。混合权重 $\sigma$ 随训练进度从1（纯真实动作）退火到0（纯策略预测），使模型逐渐学会在自身预测的基础上进行修正，提高对分布偏移的鲁棒性。</li>
<li><strong>残差对齐</strong>：除了直接与真实目标对齐的损失 $\mathcal{L}<em>{\mathrm{m}}$，还引入一个 $\Delta$-匹配损失 $\mathcal{L}</em>{\Delta}$（公式5）。该损失鼓励模型预测的校正量 $(\hat{\mathbf{u}}<em>{\tau} - \tilde{\mathbf{u}}</em>{\tau})$ 与预训练策略需要修正的残差 $(\mathbf{u}<em>{\tau} - \tilde{\mathbf{u}}</em>{\tau})$ 在未掩码部分对齐，显式地学习对预训练策略输出的调整。</li>
<li><strong>前缀保留采样</strong>：在推理时，对采样流程进行调整以增强块间连续性。首先，初始动作状态 $\mathbf{A}_{t}^{0}$ 不再从高斯先验采样，而是用来自前一个预测块的部分动作（前缀）进行初始化。其次，在流匹配的积分过程中（公式7），通过掩码 $\mathbf{m}$ 保护已执行的前缀部分不被更新，仅对剩余部分进行合成，确保块间的平滑过渡。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.20130v1/x3.png" alt="REMAC方法框架"></p>
<blockquote>
<p><strong>图3</strong>：REMAC方法框架。左侧为训练阶段，通过前缀掩码、自条件课程和残差对齐损失对预训练策略进行适应。右侧为推理阶段，采用前缀保留采样流程生成动作块。</p>
</blockquote>
<p><strong>创新点</strong>：与现有主要进行测试时修正的方法（如BID、RTC）不同，REMAC采用<strong>训练时适应</strong>的策略来从根本上提升策略对异步执行条件的鲁棒性。其创新具体体现在：1) 明确建模并针对<strong>块内不一致性</strong>设计训练目标；2) 通过<strong>掩码训练</strong>和<strong>课程学习</strong>使策略学会在部分动作已确定（可能次优）的条件下做出稳健的后续预测；3) 结合<strong>前缀保留采样</strong>，在推理时自然保证连续性，无需额外的测试时优化步骤。此外，方法采用LoRA进行参数高效微调，仅增加约1.5%的参数，且训练后可将LoRA模块合并回主干网络，<strong>不引入任何额外推理延迟</strong>。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在Kinetix模拟器的12个动态随机环境中进行评估，并在三个真实世界任务（抓取与放置、开抽屉、堆叠杯子）中进行测试。</li>
<li><strong>实验平台</strong>：模拟实验遵循Black等人(2025)的协议；真实部署采用类似Shukor等人(2025)的远程服务器-机器人客户端架构。</li>
<li><strong>Baseline方法</strong>：<ul>
<li><strong>Naive Async</strong>：直接使用预训练策略进行异步推理。</li>
<li>**Bidirectional Decoding (BID)**：测试时方法，采样多个候选预测并通过拒绝选择最优。</li>
<li><strong>RTC</strong>：测试时方法，将问题构建为修复任务，对预测块进行基于梯度的修正。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2601.20130v1/x2.png" alt="Kinetix环境性能对比"></p>
<blockquote>
<p><strong>图2</strong>：Kinetix环境中的性能对比。左图：不同推理延迟下各任务的解决率。右图（上）：所有环境的平均性能。REMAC在所有延迟设置下均优于基线，且随着延迟增加性能下降更小。右图（下）：所有环境的平均执行时间。REMAC需要更少的步骤，任务完成更快。</p>
</blockquote>
<p>在模拟实验中，REMAC在0到4的推理延迟范围内，平均成功率 consistently 高于所有基线。例如，在延迟d=2时，REMAC平均成功率约为70%，而Naive Async、BID、RTC分别约为42%、55%、65%。随着延迟增大，REMAC的性能衰减最为平缓。同时，REMAC完成任务所需的平均时间步数最少，执行速度最快。</p>
<p><img src="https://arxiv.org/html/2601.20130v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融实验。评估了REMAC各个组件的贡献。完整模型（绿色）性能最佳。移除自条件课程（紫色）或残差对齐损失（黄色）均会导致性能下降，尤其是前者影响显著。仅使用前缀掩码（红色）优于基线但不及完整模型。</p>
</blockquote>
<p><strong>消融实验总结</strong>：图4展示了各组件贡献。完整REMAC性能最佳。移除自条件课程学习对性能损害最大，凸显了其对于对齐训练-测试条件的重要性。移除残差对齐损失也会导致性能下降。仅使用前缀掩码（即基础掩码训练）虽优于Naive Async基线，但效果不及整合了所有组件的完整方法。</p>
<p><img src="https://arxiv.org/html/2601.20130v1/x5.png" alt="定性结果"><br><img src="https://arxiv.org/html/2601.20130v1/x6.png" alt="定性结果"><br><img src="https://arxiv.org/html/2601.20130v1/x7.png" alt="定性结果"><br><img src="https://arxiv.org/html/2601.20130v1/x8.png" alt="定性结果"><br><img src="https://arxiv.org/html/2601.20130v1/x9.png" alt="定性结果"><br><img src="https://arxiv.org/html/2601.20130v1/x10.png" alt="定性结果"><br><img src="https://arxiv.org/html/2601.20130v1/x11.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5-11</strong>：定性结果。展示了在模拟和真实任务中，与基线相比，REMAC能产生更平滑、更连贯的机器人轨迹，并成功完成诸如敏捷操纵、开门、堆叠等复杂任务，而基线方法常出现抖动、卡顿或失败。</p>
</blockquote>
<p>在真实世界实验中，REMAC在存在可变网络延迟的情况下，成功完成了所有测试任务，且动作更加平滑可靠，而基线方法则表现出更多的抖动和执行失败。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>识别关键问题</strong>：明确指出并形式化了异步推理中导致性能下降的 <strong>块内不一致性</strong> 问题，与先前主要关注的块间不连续性形成互补。</li>
<li><strong>提出REMAC框架</strong>：提出一种<strong>训练时适应</strong>方法，通过掩码动作分块、自条件课程学习和残差对齐，使预训练策略学会对延迟执行条件具有鲁棒性，且不增加推理延迟。</li>
<li><strong>引入前缀保留采样</strong>：提出一种改进的采样流程，在推理时自然保持块间连续性，无需复杂的测试时优化。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法需要一个预训练的策略作为起点，其最终性能可能受限于基础策略的能力。此外，虽然能处理可变延迟，但训练时掩码延迟的采样范围是需要设置的超参数。</p>
<p><strong>启示</strong>：本研究证明了通过<strong>训练时策略适应</strong>来系统性解决异步执行挑战的有效性，这为构建实时机器人系统提供了一个新方向。REMAC作为一种与模型无关的增强方法，可以无缝集成到现有的VLA框架中，并能与测试时修正算法结合使用，以产生更强大的骨干策略。这启发后续研究可以进一步探索更高效的适应方法，或将类似思想应用于其他类型的策略表示（如Transformer）。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对异步推理中机器人动作块与感知不匹配导致的执行失败问题，提出REMAC方法。核心是通过**掩码动作分块**技术，在预训练策略上学习校正调整，并引入**前缀保留采样**增强块间连续性，使策略在动作与执行失配时保持鲁棒。实验表明，该方法在不增加延迟的前提下，实现了更快的任务执行、跨延迟的鲁棒性以及更高的任务完成率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.20130" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>