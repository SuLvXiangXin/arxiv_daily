<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.18368" target="_blank" rel="noreferrer">2512.18368</a></span>
        <span>作者: Jingya Wang Team</span>
        <span>日期: 2025-12-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习在单任务机器人操作中取得了显著成果，但扩展到多任务场景仍面临根本性挑战，如次优的专家演示、轨迹噪声和行为多模态。现有的基于技能的方法试图通过将复杂动作分解为可重用的抽象来解决此问题，但它们通常依赖于固定长度的分割或环境先验，这限制了语义一致性和跨任务泛化能力。例如，一些方法使用固定长度的滑动窗口来分割演示，这忽略了技能的可变长度本质，导致学习到的潜在标记捕获的是短而重复的运动片段，而非语义上有意义且可重用的技能。本文针对现有技能学习方法语义不一致、泛化能力有限的具体痛点，提出了通过构建语义接地的原子技能空间来提升多任务模仿学习性能的新视角。本文的核心思路是：首先利用夹爪状态关键帧检测和视觉语言模型（VLM）标注，将演示分割成可变长度、语义一致的原子技能，并通过对比学习对齐其表示；然后设计一个能联合预测技能长远目标（关键姿态）和即时动作序列的策略，以实现鲁棒的技能链式执行。</p>
<h2 id="方法详解">方法详解</h2>
<p>AtomSkill框架包含三个主要组件：1）利用VLM进行语义技能发现；2）通过对比对齐进行原子技能学习；3）利用技能先验进行推理。整体目标是学习一个语义上有意义的技能先验库，为跨任务操作提供高层指导。</p>
<p><img src="https://arxiv.org/html/2512.18368v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：AtomSkill框架概览。左侧展示了语义技能发现过程：同一任务的专家演示被分割成语义一致、时间对齐的片段，并由视觉-语言模型为每个片段分配技能标签。右上部分展示了技能学习过程，AtomSkill构建技能空间并同时训练技能引导的策略和基于扩散的采样器。右下部分描绘了通过带关键姿态的动作分块进行推理，实现预测技能的平滑、鲁棒链式执行。</p>
</blockquote>
<p><strong>1. 语义技能发现与VLM</strong>：该方法将一个技能定义为完成语义相同目标（如“抓取”、“放置”）的一组动作序列。给定专家演示轨迹，首先基于<strong>夹爪状态变化</strong>检测关键帧，将轨迹分割成非重叠的子轨迹。夹爪状态变化是物理交互的关键信号，可标志一个技能的结束和下一个技能的开始。随后，利用一个大型视觉语言模型，基于每个片段的观察和任务指令，生成语义接地的描述和对应的技能标签（例如，对于任务“把垃圾放进垃圾桶”，VLM生成“抓取垃圾”和“将垃圾放入垃圾桶”等描述，对应“抓取”和“放置”技能）。这些带标签的子轨迹为学习模块化、可重用的技能表示奠定了基础。</p>
<p><strong>2. 原子技能学习与语义对齐</strong>：采用VQ-VAE结构从动作序列中提取紧凑的抽象。技能编码器由1D CNN层和自注意力层构成，将可变长度的动作子轨迹重采样为固定长度后，编码为连续嵌入。一个向量量化层将这些连续嵌入映射到一个可学习的码本上，得到离散的技能标记，训练时使用标准的向量量化损失。<strong>核心创新</strong>在于引入了<strong>语义对比技能对齐损失</strong>。该损失由两项组成：<code>L_temp</code>鼓励共享相同量化标记且在技能序列中相对位置一致的嵌入在潜在空间中靠近，以促进时间一致性；<code>L_skill</code>鼓励共享相同语义技能标签和量化标记的嵌入彼此靠近，以形成语义上有组织的技能空间。两者共同将VQ潜在空间转变为时间一致且语义清晰的技能空间。</p>
<p><strong>3. 带关键姿态想象的动作解码器</strong>：在每一步，动作解码器接收当前观测（多视角图像、本体感知、语言指令）以及由技能采样器提供的当前技能抽象<code>z_q</code>。解码器采用交叉注意力机制，将技能抽象作为键和值，观测作为查询，以此注入技能信息并降低计算成本。解码器不仅预测未来一段动作序列，还<strong>联合预测当前技能终止时的关键姿态</strong>。这是通过向动作查询附加一个额外的查询令牌来实现的，并采用一个辅助的L1损失进行训练。预测关键姿态能增强策略的空间推理和定位能力。</p>
<p><strong>4. 推理与技能先验</strong>：<strong>技能扩散采样器</strong>被建模为一个以技能标签和观测为条件的扩散模型，用于从学习到的技能空间中采样高层技能嵌入，经量化后输入策略。<strong>带关键姿态的动作分块</strong>是推理时的核心机制：策略持续生成动作块，直到预测的下一动作与预测的关键姿态在动作空间中足够接近，此时触发扩散采样器采样下一个技能。这种基于接近度的标准避免了手工设计的终止启发式方法，实现了技能间的自主、平滑过渡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境中使用RLBench的六个任务（见图3），在真实世界中使用ALOHA风格的双臂机器人执行三个任务。每个任务收集100条演示。对比基线包括：Diffusion Policy (DP)、ACT、VQ-BeT、QueST以及大型VLA模型RDT。评估指标包括严格成功率(SR)和平均任务进度(ATP)。主要在多任务设置下进行训练和评估。</p>
<p><img src="https://arxiv.org/html/2512.18368v1/x3.png" alt="任务图示"></p>
<blockquote>
<p><strong>图3</strong>：选定的RLBench任务和真实世界任务示意图。六个RLBench任务分为运动模式任务和空间定位任务两组。前者测试再现一致运动动力学的能力，后者检验空间接地的准确性。三个真实世界任务则同时考察空间定位和长视野动作序列建模。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.18368v1/x4.png" alt="模拟结果表"></p>
<blockquote>
<p><strong>图4</strong>：模拟性能结果表（对应论文表1）。AtomSkill在平均ATP（0.68）和SR（67.2%）上均优于所有基线，且在每一项任务上都取得了最佳或极具竞争力的性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>：如表1（图4）所示，AtomSkill在六项RLBench任务上取得了最佳平均性能（ATP 0.68， SR 67.2%），全面超越了所有基线。进一步将任务分为运动模式任务（Close Box, Sweep to Dustpan, Close Laptop）和空间定位任务（Put Rubbish in Bin, Phone on Base, Umbrella out of Stand）进行分析，AtomSkill在两组任务上均表现最佳（运动模式：ATP 0.83， SR 82.2%；空间定位：ATP 0.53， SR 52.2%），展现了其在再现运动动力学和空间推理方面的双重优势。</p>
<p><img src="https://arxiv.org/html/2512.18368v1/x5.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界任务性能。AtomSkill在双臂操作任务上显著优于基线方法ACT和QueST，证明了其在实际机器人系统中的有效性和鲁棒性。</p>
</blockquote>
<p><strong>真实世界结果</strong>：在三个双臂操作任务上，AtomSkill同样显著优于对比的ACT和QueST方法（图5），验证了其在实际应用中的有效性。</p>
<p><img src="https://arxiv.org/html/2512.18368v1/x6.png" alt="消融研究"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。移除非重叠分割、对比学习对齐或关键姿态预测中的任何一项都会导致性能下降，证明了每个组件的重要性。</p>
</blockquote>
<p><strong>消融实验</strong>：消融研究（图6）证实了各核心组件的贡献：1）使用<strong>非重叠、基于关键帧的分割</strong>（对比固定长度滑动窗口）对性能提升至关重要；2）<strong>对比学习对齐</strong>损失（尤其是语义对齐部分）显著改善了技能表示的质量和策略性能；3）<strong>关键姿态预测</strong>机制对于实现精确的空间定位和鲁棒的技能链式执行不可或缺。</p>
<p><img src="https://arxiv.org/html/2512.18368v1/x7.png" alt="定性分析"></p>
<blockquote>
<p><strong>图7</strong>：技能空间可视化。经过对比学习对齐后，AtomSkill的技能潜在空间呈现出清晰的语义结构，相同技能（如抓取、放置）的嵌入聚集在一起，不同技能则相互分离。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.18368v1/x8.png" alt="技能采样"></p>
<blockquote>
<p><strong>图8</strong>：技能扩散采样可视化。扩散采样器能够根据当前观测和技能标签，生成多样且合理的技能序列，例如在“打开盒子”任务中正确排序“移动至”、“抓取”、“放置”等技能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.18368v1/x9.png" alt="轨迹对比"></p>
<blockquote>
<p><strong>图9</strong>：与基线方法的轨迹对比。AtomSkill生成的动作轨迹更平滑，与专家演示更接近，并且能通过预测的关键姿态（红星）准确监控任务进度，实现自动技能切换。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了<strong>AtomSkill框架</strong>，通过构建结构化的原子技能空间来实现可组合的多任务机器人操作；2）开发了<strong>语义接地的原子技能库</strong>构建方法，利用关键帧分割和VLM标注确保技能的语义一致性和时间相干性；3）设计了<strong>带关键姿态想象的动作生成模块</strong>，使策略能同时推理长远运动目标和细粒度控制，从而实现了鲁棒的技能链式执行。</p>
<p>论文提到的局限性包括：技能分割依赖于夹爪状态变化，这可能不适用于所有类型的技能（如纯导航）；此外，依赖VLM进行标注可能存在错误。这些局限性为后续研究指明了方向，例如探索更通用的技能边界检测信号，提高语义标注的鲁棒性，以及将原子技能库扩展到更复杂的长期任务和动态环境中。AtomSkill为将高层语义知识系统性地注入低层机器人控制提供了一条有效路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多任务机器人操作中模仿学习面临的泛化挑战，提出了AtomSkill框架。其核心是构建一个**语义接地的原子技能库**，通过夹持器状态关键帧检测与视觉语言模型标注，将演示分割为语义一致的变长技能。同时，**带有关键姿态想象的动作生成模块**能联合预测技能的长期目标姿态与即时动作序列，实现鲁棒的技能组合。实验表明，该方法在多样化的操作任务上性能优于现有先进方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.18368" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>