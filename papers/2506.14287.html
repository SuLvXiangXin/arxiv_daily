<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Steering Robots with Inference-Time Interactions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Steering Robots with Inference-Time Interactions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.14287" target="_blank" rel="noreferrer">2506.14287</a></span>
        <span>作者: Yanwei Wang Team</span>
        <span>日期: 2025-06-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，模仿学习已成功训练出能够自主解决多个任务的通用策略（如视觉-语言-动作模型）。然而，当这些预训练策略在部署过程中出错时，用户缺乏有效的机制来即时纠正其行为。现有主流方法通常通过收集额外数据对策略进行微调，以提升其在特定下游用例中的鲁棒性或可引导性。但这种方法效率低下，且部署后用户往往不具备数据收集和模型训练的基础设施。本文针对“如何在<strong>不微调</strong>预训练策略权重的情况下，于<strong>推理时</strong>通过用户交互即时纠正策略错误”这一具体痛点，提出了新的视角。其核心思路是：将预训练策略视为冻结的固定技能库，通过引入用户交互（如指向、草图、物理纠正）来引导行为生成过程，使其对齐用户意图，同时确保生成的行为不偏离策略已掌握的有效技能分布。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了两个互补的框架来实现推理时引导：<strong>推理时策略引导</strong>和<strong>任务与运动模仿</strong>。</p>
<p><strong>1. 推理时策略引导</strong><br>ITPS框架针对能够预测多模态动作分布的多任务生成策略。其目标是将用户交互<code>z</code>转化为轨迹空间中的目标函数<code>ξ(τ, z)</code>，并通过条件采样从冻结策略<code>π</code>的似然分布中生成既满足数据分布约束（即有效）又对齐用户意图的轨迹<code>τ</code>。</p>
<p><img src="https://arxiv.org/html/2506.14287v1/extracted/6548179/fig2/itps_framework.jpg" alt="ITPS框架总览"></p>
<blockquote>
<p><strong>图5</strong>：ITPS框架示意图。用户通过点目标、轨迹草图或物理纠正指定意图，这些交互被转化为目标函数，用于引导从冻结的生成策略中进行条件采样，输出对齐的轨迹。</p>
</blockquote>
<p>用户意图通过三种物理交互指定，对应不同的目标函数：</p>
<ul>
<li><strong>点输入</strong>：用户指定图像中的一个点，映射为3D坐标<code>z_point</code>。目标是最小化生成轨迹所有状态与该点的平均L2距离（公式2.1）。</li>
<li><strong>草图输入</strong>：用户提供部分轨迹草图<code>z_sketch</code>。目标是最小化生成轨迹与草图对应点之间的L2距离之和（公式2.2）。</li>
<li><strong>物理纠正输入</strong>：用户通过推动机器人施加前k步的纠正<code>z_nudge</code>。目标是强制生成轨迹的前k步与纠正完全一致，后续步骤自由生成（公式2.3, 2.4）。</li>
</ul>
<p>核心挑战在于如何在优化对齐目标<code>ξ</code>的同时，确保采样轨迹仍位于预训练策略的<strong>有效轨迹数据流形</strong>上，避免因分布偏移导致任务失败。论文系统研究了六种采样方法，并揭示了一个根本性的<strong>对齐-约束满足权衡</strong>：更强的引导对齐往往会增加约束违反和任务失败率。</p>
<p><img src="https://arxiv.org/html/2506.14287v1/extracted/6548179/fig2/itps_method.png" alt="方法对比"></p>
<blockquote>
<p><strong>图6</strong>：不同采样方法示意图。(a) 扰动潜在变量；(b) 扰动去噪轨迹；(c) 基于梯度下降的引导；(d) 基于分类器的引导；(e) 拒绝采样；(f) 本文提出的随机采样。</p>
</blockquote>
<p>本文提出的关键创新是一种针对扩散策略的<strong>随机采样</strong>方法。该方法受MCMC启发，在扩散去噪过程的每一步，不仅根据扩散模型预测去噪，还沿着对齐目标<code>ξ</code>的梯度方向进行随机扰动。通过精心设计扰动步长，该方法能在有效轨迹流形内进行探索，从而在显著提升对齐性的同时，将分布偏移和约束违反控制在较低水平，实现了最佳的权衡。</p>
<p><strong>2. 任务与运动模仿</strong><br>TAMI框架针对为单任务设计的<strong>多步策略</strong>。其核心观点是：多步任务的成功执行，关键在于连续运动轨迹必须遵循一个由离散<strong>模式</strong>序列构成的符号化计划（如“接近→抓取→运输”），而非精确复现某条演示轨迹。每个模式对应一个连贯的可行运动子空间（运动流形），其边界编码了任务约束（如抓取时需闭合手指）。</p>
<p><img src="https://arxiv.org/html/2506.14287v1/extracted/6548179/fig1/intro_outline.png" alt="TAMI框架总览"></p>
<blockquote>
<p><strong>图4</strong>：TAMI框架示意图。对于多步策略，用户交互引导连续运动轨迹，但该轨迹被显式约束，必须按顺序（A→B→C）停留在相应的模式边界内。</p>
</blockquote>
<p>TAMI框架包含两个核心组件：</p>
<ul>
<li><strong>模式分类器</strong>：一个将连续状态映射到离散模式<code>m</code>的函数<code>φ(s)</code>，用于识别当前状态所属模式及模式边界。</li>
<li><strong>引导策略</strong>：在给定符号计划（模式序列）和模式分类器的前提下，生成/编辑满足模式序列约束的连续轨迹。</li>
</ul>
<p>本文实例化了一个称为<strong>时序逻辑模仿</strong>的方法。给定线性时序逻辑公式描述的符号计划，TLI首先利用分类器识别当前模式及约束。当运动生成在模式内部时，采用<strong>交互兼容策略</strong>最大化可引导性，允许用户直观地修改轨迹形状。一旦检测到运动可能违反模式边界约束（例如，因用户交互即将导致物体掉落），TLI会触发推理时纠正，编辑轨迹使其回归到当前模式内或切换到下一个正确模式，从而保证任务成功。理论分析表明，要实现无约束违反的引导，底层模仿策略必须满足<strong>可达性</strong>和<strong>不变性</strong>属性。</p>
<p><strong>3. 模式分类器学习</strong><br>TAMI框架需要模式分类器，但预训练策略通常缺乏密集的模式标注。为此，本文提出了<strong>GLiDE</strong>方法。其核心思想是：通过对预训练策略施加物理扰动，自动生成成功和失败的执行轨迹。通过对比成功与失败的状态，可以学习到划分配置空间的决策边界，这些边界恰好对应了任务约束，从而实现对模式的分类。</p>
<p><img src="https://arxiv.org/html/2506.14287v1/extracted/6548179/fig4/glide_framework.jpg" alt="GLiDE框架"></p>
<blockquote>
<p><strong>图27</strong>：GLiDE方法框架。通过对策略施加扰动生成成功与失败数据，通过对比学习训练模式分类器，无需密集人工标注。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟和真实世界环境中进行，使用了多个基准测试。</p>
<p><strong>ITPS实验</strong>：</p>
<ul>
<li><strong>环境与基准</strong>：模拟迷宫导航、方块堆叠、真实世界厨房操作。对比了六种采样方法，以及两种生成模型：扩散策略和Transformer自回归策略。</li>
<li><strong>关键结果</strong>：<br><img src="https://arxiv.org/html/2506.14287v1/extracted/6548179/fig2/itps_maze_tradeoff.png" alt="权衡曲线"><blockquote>
<p><strong>图8</strong>：在迷宫任务中，不同采样方法在对齐率（AR，越高越好）与约束满足率（CSR，越高越好）之间的权衡曲线。随机采样（Stochastic）达到了最佳的帕累托前沿。<br>在厨房多任务场景中，随机采样方法能将任务对齐率从默认采样的50%提升至93%，同时保持高约束满足率。<br><img src="https://arxiv.org/html/2506.14287v1/extracted/6548179/fig2/itps_kitchen_multimodal.jpg" alt="厨房结果"><br><strong>图12</strong>：在真实厨房中，使用点输入引导策略选择正确的技能（打开左上柜门），成功率达到93%。</p>
</blockquote>
</li>
<li><strong>消融实验</strong>：验证了随机采样中噪声调度对平衡权衡的重要性。</li>
</ul>
<p><strong>TAMI与TLI实验</strong>：</p>
<ul>
<li><strong>环境与任务</strong>：7自由度机械臂的颜色追踪任务（需按顺序通过不同颜色区域）。</li>
<li><strong>关键结果</strong>：<br><img src="https://arxiv.org/html/2506.14287v1/extracted/6548179/fig3/tli_rollout.jpg" alt="TLI展开"><blockquote>
<p><strong>图23</strong>：TLI方法在颜色追踪任务中的展开示意图。策略在模式内（如蓝色区域）允许用户引导修改路径，在模式边界处进行纠正以确保进入下一个正确模式（如绿色区域）。<br>实验表明，TLI在存在用户物理扰动的情况下，能保证100%的任务成功率，而基线方法（纯交互兼容策略）成功率低于20%。</p>
</blockquote>
</li>
<li><strong>消融实验</strong>：验证了模式分类器准确性和时序逻辑监控对成功率的必要性。</li>
</ul>
<p><strong>GLiDE实验</strong>：</p>
<ul>
<li><strong>环境与任务</strong>：2D导航、Robosuite搅拌任务、真实机器人舀取任务。</li>
<li><strong>关键结果</strong>：<br><img src="https://arxiv.org/html/2506.14287v1/extracted/6548179/fig4/glide_scoop_comp.jpg" alt="GLiDE对比"><blockquote>
<p><strong>图32</strong>：在舀取任务中，GLiDE学习到的分类器决策边界（右）与真实物理约束高度吻合，且优于需要密集标注的基线方法（左、中）。<br>GLiDE仅需不到10%的密集标注数据量，就能学习到准确率超过90%的模式分类器。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了推理时引导的新范式</strong>：首次系统性地探索并实现了在不微调权重的情况下，通过多种物理交互引导冻结预训练策略，并形式化了其中的对齐-约束满足权衡。</li>
<li><strong>开发了两种具体框架</strong>：针对多任务策略的ITPS（基于概率约束的条件采样）和针对多步单任务策略的TAMI（基于符号计划和硬约束的引导），并提供了相应的算法实现（随机采样、TLI）。</li>
<li><strong>提出了数据高效的模式分类器学习方法</strong>：GLiDE通过利用策略的失败数据，以最少的人工标注成本学习任务约束边界，支撑了TAMI框架的应用。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>研究假设预训练策略已具备完成任务所需的全部基本技能，主要解决任务指定错误问题。对于技能本身不足的情况，可能仍需微调。</li>
<li>TAMI框架依赖于一个准确的模式分类器，虽然GLiDE降低了获取成本，但其在极其复杂或动态约束下的泛化能力有待进一步研究。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>生成模型与符号推理的结合</strong>：将深度生成模型的灵活性、泛化能力与符号化任务表示的可靠性、可解释性相结合，是构建既强大又可靠、可交互的机器人系统的一条有前景的路径。</li>
<li><strong>利用失败数据学习</strong>：主动生成或利用执行失败的数据，是高效学习任务约束和决策边界的重要手段。</li>
<li><strong>交互范式的扩展</strong>：本文聚焦于物理空间交互，未来可探索如何统一物理交互与语言、视觉等其他模态的引导信号，形成更全面的交互界面。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究预训练模仿学习策略在部署时出错后缺乏高效纠正机制的问题。提出在推理时通过用户交互引导机器人行为，避免重新微调。关键技术包括：1）推理时引导，利用交互在离散技能间切换；2）任务与动作模仿，通过交互编辑连续动作并满足任务约束。这些方法能在不额外训练的情况下纠正策略预测偏差，提升预训练模型的实用性和用户控制能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.14287" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>