<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.00411" target="_blank" rel="noreferrer">2506.00411</a></span>
        <span>作者: Yang, Yi, Sun, Jiaxuan, Kou, Siqi, Wang, Yihan, Deng, Zhijie</span>
        <span>日期: 2025/05/31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在真实世界中操作的具身智能体需要处理长视野、组合式且动态变化的任务。与短视野任务不同，长视野任务包含无法通过单一动作实现的高级目标，需要智能体进行推理、执行动作并适应环境变化。这同时需要高级任务规划（将整体目标分解为原子性子任务）和低级运动控制（生成精确的机器人动作）。当前，视觉-语言-动作模型已成为具身智能体的主流范式，它通过在大规模机器人演示数据上微调预训练的视觉-语言模型，将视觉和语言输入映射到可执行动作。尽管VLA模型能有效提取观察和指令中的关键信息，但在有效的规划和推理方面存在不足，导致其在长视野任务上表现不佳。另一方面，针对长视野任务的开创性研究通常采用分层架构，包括一个基于VLM的高级规划器用于推理子任务指令，以及一个基于VLA的低级控制器将这些指令转化为机器人动作。虽然这种模块化结构提供了灵活性，但常常导致次优的协调和有限的泛化能力。这些局限性凸显了对能够在一个统一框架内结合高级和低级推理的端到端架构的需求。</p>
<p>本文针对现有VLA模型规划能力不足、以及分层架构协调与泛化受限的痛点，提出了一种新的统一视角：构建一个共享主干网络，同时显式生成语言子任务和动作令牌，并通过分层闭环控制机制来增强鲁棒性。本文的核心思路是：利用一个大型预训练视觉-语言模型作为主干，联合生成用于子任务规划的语言令牌和用于动作预测的动作令牌，并引入一种分层闭环控制策略来分别处理规划错误和执行错误。</p>
<h2 id="方法详解">方法详解</h2>
<p>LoHoVLA的整体框架是一个统一的端到端模型，其输入是当前视觉观测<strong>o</strong>_t和高级语言目标g，输出是预测的原子性子任务ĝ_t和机器人动作<strong>a</strong>_t。其核心思想是首先从输入中推断出语言子任务，然后将其作为上下文指导用于动作预测。模型基于一个大型预训练VLM主干构建，并采用分层闭环控制机制来缓解错误。</p>
<p><img src="https://arxiv.org/html/2506.00411v1/x1.png" alt="方法框架对比"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架对比。左上方：标准VLA模型直接将高级目标和观测映射到动作。左下方：分层架构将规划与执行分离——规划器推断子任务，控制器执行它们。右方：LoHoVLA将高级任务规划和低级运动控制集成到一个统一模型中。它使用自回归Transformer作为主干，并采用分层闭环控制机制。</p>
</blockquote>
<p>与标准VLA（公式1: π_θ(<strong>o</strong>_t, g) → <strong>a</strong>_t）和分层架构（公式2: 规划器π_θ^planner(<strong>o</strong>_t, g) → ĝ_t， 控制器π_θ^controller(<strong>o</strong>_t, ĝ_t) → <strong>a</strong>_t）不同，LoHoVLA采用统一范式（公式3: π_θ(<strong>a</strong>_t, ĝ_t | <strong>o</strong>_t, g) = π_θ(<strong>a</strong>_t | <strong>o</strong>_t, g, ĝ_t) · π_θ(ĝ_t | <strong>o</strong>_t, g)），在一个模型内同时建模高级任务规划分布π_θ(ĝ_t | <strong>o</strong>_t, g)和低级运动控制分布π_θ(<strong>a</strong>_t | <strong>o</strong>_t, g, ĝ_t)。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>主干网络</strong>：采用PaliGemma作为主干，这是一个多模态基础模型，包含一个基于SigLIP的图像编码器、一个Gemma-2B仅解码器语言模型，以及一个将视觉特征映射到语言模型词符空间的线性投影层。训练时，图像编码器和投影层被冻结，仅优化语言模型部分。</li>
<li><strong>动作解令牌器</strong>：为了与视觉-语言数据集进行联合训练，机器人动作被表示为离散令牌。具体而言，将归一化的动作值离散化为1024个均匀的区间（bins）。在推理时，通过对预测的动作令牌进行解令牌和反归一化来恢复连续动作。</li>
<li><strong>分层闭环控制机制</strong>：针对长视野任务中可能出现的三类错误（子任务规划错误、动作预测错误、外部干扰），LoHoVLA采用了一种分层策略：比重新规划子任务更频繁地重新预测动作。具体而言，如果当前子任务执行失败的次数超过预设阈值K，则系统触发子任务重新规划；否则，仅基于新的环境状态重新预测动作。该机制旨在避免在第(2)、(3)类错误情况下进行不必要的子任务重规划。</li>
</ol>
<p><strong>训练策略</strong>：</p>
<ol>
<li><strong>训练数据</strong>：为训练LoHoVLA，作者构建了合成数据集LoHoSet。该数据集基于Ravens机器人模拟器，包含20个长视野具身任务，每个任务有1000个专家演示，包含视觉观测、语言目标、子任务和动作。数据通过模拟器中的规则自动生成子任务描述和动作。</li>
<li><strong>训练目标</strong>：总损失函数为文本生成损失和动作预测损失的加权和：ℒ = ℒ_text + ℒ_action。两者均使用交叉熵损失。</li>
<li><strong>两阶段训练</strong>：第一阶段，在长视野任务上微调PaliGemma，仅优化文本损失以提升高级任务规划能力。第二阶段，在数据集中加入拾放原始任务，同时优化文本和动作损失以增强动作预测能力。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Ravens模拟器中进行，使用自建的LoHoSet数据集进行评估。任务分为已见任务（训练中包含）和未见任务（用于测试泛化），具体任务列表参见论文表1（如“将方块放入颜色匹配的碗中”、“交替颜色堆叠方块”等）。</p>
<p><strong>基线方法</strong>：</p>
<ol>
<li><strong>LoHoRavens</strong>：一个分层架构基线，包含规划器、执行器和报告器，支持显式反馈（用LLaMA 2 13B规划，CLIPort执行，OpenFlamingo报告）和隐式反馈（用冻结CLIP编码观测，通过LLaVA调整计划）两种变体。</li>
<li><strong>标准VLA模型</strong>：一个直接预测动作而不产生中间语言输出的模型，作为对比以验证显式子任务预测的效果。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.00411v1/x3.png" alt="任务与泛化性能"></p>
<blockquote>
<p><strong>图3</strong>：在已见和未见长视野任务上的成功率对比。左图显示LoHoVLA在大多数任务上显著优于分层基线（LoHoRavens）和标准VLA基线。右图显示，随着训练数据中任务类型的增加（从4个到14个），LoHoVLA在未见任务上的泛化性能得到持续提升。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>LoHoVLA在大多数已见和未见的长视野任务上显著优于分层架构和标准VLA基线。例如，在任务“将方块放入颜色匹配的碗中”，LoHoVLA成功率接近100%，而分层基线约为70%，标准VLA基线低于60%。在未见任务“交替颜色堆叠方块”上，LoHoVLA成功率超过80%，而两个基线均低于40%。这些结果表明LoHoVLA具备强大的推理、规划能力和泛化性。</p>
<p><img src="https://arxiv.org/html/2506.00411v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融实验结果。(a) 对比了不同控制策略：开环、标准闭环（每一步都重新规划和预测）、以及提出的分层闭环（K=3）。分层闭环在平衡成功率和计算效率（步数）方面表现最佳。(b) 展示了两阶段训练策略的有效性，联合训练文本和动作（第二阶段）比仅训练文本（第一阶段）在动作执行精度上有显著提升。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>分层闭环控制机制</strong>：实验表明，与开环控制、标准闭环控制（每一步都重新规划）相比，提出的分层闭环控制（K=3）在成功率和效率（所需步数）之间取得了最佳平衡，有效缓解了长视野任务中的错误累积。</li>
<li><strong>两阶段训练策略</strong>：结果显示，在第一阶段仅训练文本损失后，加入第二阶段联合训练动作损失，显著提高了动作预测的准确性，从而提升了整体任务成功率。</li>
<li><strong>数据扩展</strong>：如图3右所示，将训练任务从4个（LoHoRavens基准）扩展到14个（增加10个新任务），显著提高了模型在未见任务上的泛化性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.00411v1/x5.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：定性结果示例。展示了LoHoVLA在“交替颜色堆叠方块”任务中的执行过程。模型成功预测了序列化的子任务（如“拾起绿色方块放入绿色区域”、“拾起蓝色方块放在绿色方块上”等），并生成了正确的拾放位置，最终完成了复杂的长视野任务。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>统一架构</strong>：提出了LoHoVLA，一个将高级任务规划和低级运动控制集成到单一模型中的统一VLA框架，通过共享的VLM主干学习可泛化的表征，解决了分层架构的协调与泛化问题。</li>
<li><strong>分层闭环控制</strong>：设计了一种新颖的分层闭环控制机制，能够智能地区分并处理规划错误和执行错误，提高了长视野任务执行的鲁棒性和效率。</li>
<li><strong>数据集与训练策略</strong>：构建了大规模合成数据集LoHoSet，并提出了有效的两阶段训练策略，为训练和评估长视野具身推理模型提供了资源和方法。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前工作主要基于模拟环境（Ravens），其中对象属性和交互相对简化。子任务的生成依赖于预定义的规则和模板。在更复杂、开放式的真实世界场景中直接应用仍需进一步探索。</p>
<p><strong>后续启示</strong>：本研究证明了统一架构在具身智能长视野任务中的潜力。未来的工作可以探索：1) 将方法扩展到更复杂的物理模拟器或真实机器人平台；2) 研究如何从更少的示范或在线交互中学习，减少对大规模合成数据的依赖；3) 探索更高级的规划推理能力，如处理部分可观测环境或动态变化的目标。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LoHoVLA模型，旨在解决具身智能体在长视野任务中，现有VLA模型规划能力不足与分层架构协调性差的核心问题。其关键技术是构建一个统一框架，以预训练视觉语言模型为骨干，同时生成子任务语言指令与机器人动作token，并采用分层闭环控制以减少误差。实验在Ravens模拟器的20个长视野任务上进行，结果表明LoHoVLA的性能显著超越了分层方法和标准VLA模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.00411" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>