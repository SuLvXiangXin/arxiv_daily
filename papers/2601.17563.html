<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.17563" target="_blank" rel="noreferrer">2601.17563</a></span>
        <span>作者: Odinaldo Rodrigues Team</span>
        <span>日期: 2026-01-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从观察中模仿学习（ILfO）的先进方法主要有两类：基于行为克隆（BC）和基于对抗模仿学习（AIL）。BC方法通常利用逆动力学模型（IDM）或前向动力学模型（FDM）来推断教师动作，并使用有监督损失进行优化。AIL方法则通过对抗训练使智能体行为与教师难以区分。然而，这些方法存在几个关键局限：1）它们仍然依赖于基于动作的监督损失（无论是显式还是通过模型推断）；2）它们通常假设环境的状态转移函数是单射的，即每个状态-动作对唯一确定下一个状态，这在实践中往往不成立，导致策略在面对多种可行动作时学习偏差；3）AIL方法容易陷入“行为寻求模式”，即盲目模仿教师的行为轨迹，而不充分考虑当前环境状态，导致在需要理解动作意图或因果关系的环境中表现不佳。</p>
<p>本文针对这些痛点，提出了一种全新的无监督视角。核心思路是：通过一个两阶段过程，首先无监督地联合学习策略和条件生成模型来近似环境动态和教师行为，然后通过有限的在线对抗训练进一步对齐和泛化策略，从而完全摆脱对动作监督的依赖，并缓解单射假设和行为寻求模式的问题。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的无监督观察模仿学习（UfO）方法包含三个核心模型：策略模型 $\pi_{\theta}$（根据状态预测动作）、条件生成模型 $\mathcal{G}<em>{\phi}$（根据状态和预测的动作生成下一状态）、以及判别模型 $\mathcal{D}</em>{\omega}$（区分教师和智能体的轨迹）。整体训练流程分为两个阶段：重建阶段和对抗阶段。</p>
<p><img src="https://arxiv.org/html/2601.17563v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：UfO的两阶段训练周期。(a) 重建阶段：交替冻结生成模型或策略模型，分别使用教师轨迹和智能体在线交互轨迹进行训练，实现策略与生成模型的相互优化。(b) 对抗阶段：冻结生成模型，使用教师轨迹和智能体轨迹（或其状态差分）来训练策略和判别器，进行在线行为对齐。</p>
</blockquote>
<p><strong>1. 重建阶段（条件转移估计）</strong><br>此阶段的目标是无需动作监督，联合学习一个能近似环境动态的生成模型和一个能模仿教师行为的策略。其核心是交替优化策略 $\pi_{\theta}$ 和生成模型 $\mathcal{G}_{\phi}$。</p>
<ul>
<li><strong>策略训练（冻结 $\mathcal{G}_{\phi}$）</strong>：使用教师的状态转移轨迹 $(s, s’)$。策略的目标是输出一个动作 $\pi_{\theta}(s)$，使得冻结的生成模型 $\mathcal{G}<em>{\phi}$ 能以该动作为条件，生成与教师下一状态 $s’$ 尽可能接近的状态。损失函数为 $\min</em>{\theta} \sum \mid s’ – \mathcal{G}<em>{\phi}(s, \pi</em>{\theta}(s)) \mid$。这迫使策略学习如何通过生成模型“解释”教师的转移。</li>
<li><strong>生成模型训练（冻结 $\pi_{\theta}$）</strong>：使用智能体策略与环境在线交互产生的轨迹 $(s, s’_{env})$，其中 $s’_{env} = T(s, \pi_{\theta}(s))$ 是真实环境转移。生成模型的目标是，给定当前状态 $s$ 和智能体动作 $\pi_{\theta}(s)$，预测的下一个状态 $\mathcal{G}<em>{\phi}(s, \pi</em>{\theta}(s))$ 应尽可能接近真实的环境转移 $s’_{env}$。损失函数为 $\min_{\phi} \sum \mid T(s, \pi_{\theta}(s)) – \mathcal{G}<em>{\phi}(s, \pi</em>{\theta}(s)) \mid$。这使生成模型学习环境动态。</li>
</ul>
<p>通过这种交替迭代的相互优化，策略逐渐学会产生能使生成模型复现教师转移的动作，而生成模型则通过学习智能体引发的真实转移来精化其对动态的建模。这个过程完全无监督，且由于优化基于状态转移的似然 $P(s’|s, \pi_{\theta}(s))$ 而非直接的动作映射 $P(a|s)$，从而规避了对单射转移函数的依赖。</p>
<p><strong>2. 对抗阶段（在线行为对齐）</strong><br>重建阶段后，策略仅从有限的教师轨迹中学习。为提升在未见情况下的泛化能力，UfO引入一个简短的对抗微调阶段。</p>
<ul>
<li>此阶段<strong>冻结生成模型</strong> $\mathcal{G}_{\phi}$，以保留已学习的环境动态近似。</li>
<li>同时训练策略 $\pi_{\theta}$ 和判别器 $\mathcal{D}<em>{\omega}$。判别器是一个循环神经网络（RNN），输入是整个轨迹中连续状态之间的差分序列（例如 $|s_i – s</em>{i+1}|$），而非原始状态，以避免记忆并提升对未见状态的泛化能力。</li>
<li>对抗损失采用标准的生成对抗网络形式：$\min_{\omega} \max_{\theta} \mathbb{E}<em>{\tau \sim \mathcal{T}</em>{\pi_{\psi}}^{\Delta}} \log(\mathcal{D}<em>{\omega}(\tau)) + \mathbb{E}</em>{\tau \sim \mathcal{T}<em>{\pi</em>{\theta}}^{\Delta}} \log(1 – \mathcal{D}_{\omega}(\tau))$，其中 $\mathcal{T}^{\Delta}$ 表示状态差分轨迹。</li>
<li>为<strong>防止行为寻求模式和灾难性遗忘</strong>，UfO对此阶段进行了严格限制：仅进行10个epoch的训练，使用极低的学习率，并对策略的梯度进行裁剪。这使得策略能在保留重建阶段所学知识的前提下，进行轻微的在线行为对齐。</li>
</ul>
<p><strong>创新点总结</strong>：1) <strong>完全无监督</strong>：无需任何形式的动作标注或监督损失。2) <strong>基于转移的优化</strong>：通过条件生成模型耦合策略与动态，规避单射假设。3) <strong>受限的对抗训练</strong>：通过短期、低学习率的在线对抗微调提升泛化，同时避免行为寻求模式。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在五个广泛使用的MuJoCo基准环境（InvertedPendulum-v4, Hopper-v4, Ant-v4, Swimmer-v4, HalfCheetah-v4）上进行评估。使用Imitation Datasets提供的教师数据和权重。每个方法使用10,000个未见过的随机种子进行评估。</p>
<p><strong>对比方法</strong>：与两类基线对比：1) 常用基线：行为克隆观察（BCO）、生成对抗观察模仿（GAIfO）；2) 先进的ILfO方法：连续观察模仿学习（CILO）、模态不可知对抗假设适应观察学习（MAHALO）、离策略观察模仿学习（OPOLO）。</p>
<p><strong>评估指标</strong>：平均回合奖励（AER）和归一化性能（$\mathcal{P}$），其中 $\mathcal{P}=1$ 表示达到教师水平，$\mathcal{P}=0$ 表示随机策略水平。</p>
<p><strong>关键实验结果</strong>：<br>论文中的表格（此处以文字描述）显示，UfO在所有五个环境中均取得了最佳或极具竞争力的AER和 $\mathcal{P}$ 值。具体而言，在Hopper、Ant、Swimmer和HalfCheetah环境中，UfO的 $\mathcal{P}$ 值均大于1（分别为1.0127, 1.0238, 1.0163, 1.0457），表明其<strong>平均表现超越了教师本身</strong>。同时，UfO在大多数环境中的奖励标准差是最小的，这表明其具有更好的稳定性和泛化能力。相比之下，MAHALO和BCO在复杂环境（如HalfCheetah）中表现较差，而CILO和OPOLO虽然表现良好，但未能超越教师。</p>
<p><img src="https://arxiv.org/html/2601.17563v1/x2.png" alt="消融实验"></p>
<blockquote>
<p><strong>图2</strong>：消融实验结果。从左至右分别展示了：(a) 移除对抗阶段（仅重建阶段）的性能；(b) 在对抗阶段使用原始状态而非状态差分的性能；(c) 延长对抗阶段训练epoch数（从10增至100）的性能。结果表明，对抗阶段（尤其是使用状态差分）对提升性能和稳定性有贡献，但过长的对抗训练会导致性能下降，验证了其设计必要性。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ol>
<li><strong>对抗阶段的作用</strong>：仅使用重建阶段，UfO已能达到SOTA结果，但加入对抗阶段后，性能（AER）有轻微提升，且<strong>标准差进一步减小</strong>，验证了在线行为对齐对泛化和稳定性的积极作用。</li>
<li><strong>状态差分 vs 原始状态</strong>：在对抗阶段使用原始状态作为判别器输入会导致性能下降，证实了使用状态差分对于防止记忆和提升泛化的重要性。</li>
<li><strong>对抗训练时长</strong>：将对抗阶段的epoch数从10增加到100会导致性能显著下降，这证实了过长对抗训练会引入行为寻求模式或灾难性遗忘，支持了UfO采用简短对抗微调的设计。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个完全<strong>无监督</strong>的从观察中模仿学习框架（UfO），彻底摆脱了对动作标注或动作级监督损失的依赖。</li>
<li>设计了<strong>两阶段训练机制</strong>：通过“条件转移估计”阶段无监督地耦合策略与动态模型，规避了单射假设；通过“在线行为对齐”阶段进行受限的对抗微调，有效提升了策略的泛化能力且避免了行为寻求模式。</li>
<li>在多个基准测试中实现了<strong>超越教师</strong>的性能，并表现出更小的方差，证明了其在泛化方面的优势。</li>
</ol>
<p><strong>局限性</strong>：论文提到，对抗阶段需要仔细调整学习率、epoch数和梯度裁剪策略，以避免负面影响。这暗示该方法在此阶段的鲁棒性可能对超参数较为敏感。</p>
<p><strong>后续启示</strong>：UfO的成功表明，从状态转移的因果结构中无监督地推导行为意图是一条可行的新路径。这为在动作信息难以获取的场景（如视频学习）下的模仿学习提供了新思路。未来的工作可以探索更鲁棒的对抗对齐机制，或将此框架扩展到更复杂的、部分可观测或非平稳的环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习（ILfO）泛化能力不足、依赖动作监督及行为盲目模仿等问题，提出无监督模仿学习框架UfO。其核心技术分为两阶段：首先通过条件转移估计近似教师动作，再利用在线行为对齐精修策略，使智能体轨迹与教师轨迹紧密匹配。在五个常用环境上的实验表明，UfO性能超越教师及所有现有ILfO方法，且标准差最小，证明其在未见场景中具有更优的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.17563" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>