<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.04131" target="_blank" rel="noreferrer">2511.04131</a></span>
        <span>作者: Li, Yitang, Luo, Zhengyi, Zhang, Tonghe, Dai, Cunxi, Kanervisto, Anssi, Tirinzoni, Andrea, Weng, Haoyang, Kitani, Kris, Guzek, Mateusz, Touati, Ahmed, Lazaric, Alessandro, Pirotta, Matteo, Shi, Guanya</span>
        <span>日期: 2025/11/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前人形机器人全身控制的主流方法是基于模拟到现实（sim-to-real）的流程，依赖在模拟环境中使用强化学习（RL）训练策略，再迁移到真实硬件。这些方法大多采用基于策略的策略梯度方法（如PPO），并依赖明确的基于跟踪的奖励。它们存在三个关键局限：1）任务特定性，大多数策略被训练来显式模仿运动捕捉片段或解决单一任务；2）缺乏适应性，训练完成后，策略难以针对新任务进行微调或组合；3）缺乏统一且可解释的目标指定和行为组合接口，使得人类操作者难以引导机器人或将学习到的技能组合成新行为。</p>
<p>本文针对构建人形机器人行为基础模型（BFM）这一目标，提出利用离策略无监督强化学习作为新视角。核心思路是：通过前向后向表示学习一个共享的潜在表示空间，将动作、目标和奖励嵌入其中，从而训练一个单一的、可提示的策略，使其能够在无需重新训练的情况下，以零样本或少量样本的方式解决多种下游任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>BFM-Zero框架包含一个无监督预训练阶段、一个零样本推理过程，以及一个可选的快速适应后训练阶段，目标是学习一个统一的潜在表示空间 Z ⊆ R^d 和一个可提示的策略 π，该策略以此表示为条件来执行多样行为，而无需针对特定任务重新训练。</p>
<p><img src="https://arxiv.org/html/2511.04131v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：BFM-Zero框架概览。预训练阶段后，BFM-Zero形成一个可用于零样本奖励优化、单帧目标到达和运动跟踪的潜在空间。它还可以以少量样本的方式进行适应，以到达更具挑战性的姿态。</p>
</blockquote>
<p>方法基于FB-CPR算法，并结合了针对真实机器人部署的关键设计。核心组件包括：</p>
<ol>
<li><strong>前向映射 F 和后向映射 B</strong>：它们共同学习一个有限秩的长期策略动态近似，使得由策略 π_z 诱导的长期转移动态可分解为 M^{π_z}(ds&#39;|s,a) ≈ F(s,a,z)^⊤ B(s&#39;) ρ(ds&#39;)。其中，后向映射 B 定义了潜在任务特征 φ(s) = (E[B(s)B(s)^⊤])^{-1} B(s)。每个潜在向量 z 定义了一个线性奖励函数 r_z(s) = φ(s)^⊤ z，而对应的策略 π_z 被优化以最大化该奖励的期望累积和。</li>
<li><strong>策略 π</strong>：一个基于历史观测 o_{t,H} 和潜在向量 z 的条件策略，输出关节的PD控制器目标。</li>
<li><strong>辅助奖励评论家 Q_R</strong>：通过纳入 N_aux 个惩罚奖励（如关节限位、能量消耗等）来施加安全性和物理可行性约束。</li>
<li><strong>判别器评论家 Q_D</strong>：通过一个潜在条件判别器 D，为无监督训练过程提供基于人类运动数据集的风格正则化和探索偏差。判别器的奖励为 r_d(o_t, s_t, z) = D(o_t, s_t, z) / (1 - D(o_t, s_t, z))。</li>
</ol>
<p><strong>关键设计选择与创新点</strong>：</p>
<ul>
<li><strong>非对称训练</strong>：策略基于历史观测 o_{t,H} 进行训练，而评论家（F, B, Q_R, Q_D）可以访问特权信息 (o_{t,H}, s_t)。这提高了在有限传感下的策略鲁棒性，同时利用特权评论家提供准确的价值估计。</li>
<li><strong>大规模并行环境</strong>：在数千个并行环境中进行训练，使用大型回放缓冲区和高更新数据比（UTD），以实现高效且稳定的无监督训练。</li>
<li><strong>领域随机化（DR）</strong>：随机化关键物理参数（连杆质量、摩擦系数、关节偏移、躯干质心）并施加扰动和传感器噪声，以防止对模拟动态的过拟合并确保策略在真实硬件上的稳定性。</li>
<li><strong>奖励正则化</strong>：引入辅助奖励以避免不良行为，保护硬件安全。</li>
</ul>
<p><strong>训练损失</strong>：整体训练采用离策略演员-评论家框架。前向映射 F 和后向映射 B 通过基于后继度量贝尔曼方程的时间差分损失进行训练。辅助评论家 Q_R 和判别器评论家 Q_D 通过标准的贝尔曼残差损失进行训练。最终的演员（策略）损失为最大化 F(s,a,z)^⊤ z、λ_D Q_D 和 λ_R Q_R 的加权和。</p>
<p><strong>推理方法</strong>：</p>
<ul>
<li><strong>零样本奖励优化</strong>：给定奖励函数 r(s)，计算潜在提示 z_r ≈ (1/N) Σ_i r(s_i) B(s_i)，其中 s_i 来自回放缓冲区。</li>
<li><strong>零样本目标到达</strong>：给定目标状态 s_g，计算潜在提示 z_g = B(s_g)。</li>
<li><strong>零样本运动跟踪</strong>：给定运动序列 τ，通过 z_t = Σ_{t&#39;=t}^{t+H} B(s_{t&#39;}) 计算一系列潜在提示，其中 H 是前瞻视野。</li>
<li><strong>少量样本适应</strong>：对于静态姿态或整个运动，可以在潜在空间 Z 中使用基于采样的优化方法（如交叉熵方法CEM或轨迹优化）来微调潜在提示 z，以最大化任务奖励，同时利用预训练模型作为先验。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在IsaacLab模拟器中以200Hz频率训练BFM-Zero（控制频率50Hz），使用的行为数据集是重定向到Unitree G1机器人的LAFAN1数据集。最终模型在真实的Unitree G1人形机器人上进行零样本部署验证。</p>
<p><strong>对比基线</strong>：主要对比了BFM-Zero的不同变体：1）<strong>BFM-Zero - priv</strong>：所有算法组件都接收特权信息的理想化版本，在无领域随机化环境中训练和测试；2）<strong>BFM-Zero</strong>：实际部署的版本，采用非对称训练和领域随机化。</p>
<p><img src="https://arxiv.org/html/2511.04131v1/x3.png" alt="模拟性能对比"></p>
<blockquote>
<p><strong>图3</strong>：不同测试配置下模型的跟踪、奖励和姿态到达性能（左），以及BFM-Zero在Isaac（DR）中奖励评估得分的示例分布（右）。每个指标均在多个任务上取平均。</p>
</blockquote>
<p><strong>关键定量结果（模拟）</strong>：</p>
<ul>
<li>与理想化的特权版本相比，采用领域随机化的BFM-Zero在跟踪、奖励优化和姿态到达任务上的性能分别下降了2.47%、25.86%和10.65%，表明在引入算法修改后仍能保持基本正确的学习动态和可接受的性能。</li>
<li>模拟到模拟（Isaac到Mujoco）测试显示性能差异有限（所有变化小于7%），证明了领域随机化和历史观测组件带来了良好的鲁棒性和适应性。</li>
<li>在来自AMASS数据集的分布外任务上，BFM-Zero仍能成功完成跟踪和姿态到达。</li>
</ul>
<p><strong>真实机器人零样本验证</strong>：</p>
<p><img src="https://arxiv.org/html/2511.04131v1/x4.png" alt="真实世界运动跟踪"></p>
<blockquote>
<p><strong>图4</strong>：真实世界跟踪验证。机器人能够跟踪包括高度动态舞蹈在内的各种运动，并在不稳定或摔倒时表现出自然、安全的恢复行为，然后无缝继续跟踪。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.04131v1/x5.png" alt="真实世界目标到达"></p>
<blockquote>
<p><strong>图5</strong>：真实世界目标到达验证。(a) 连续到达一系列随机采样的目标姿态，轨迹平滑自然。(b) 从任意姿态过渡到T姿态，展示了学习技能空间的平滑性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.04131v1/x6.png" alt="真实世界奖励优化"></p>
<blockquote>
<p><strong>图6</strong>：真实世界奖励优化。通过指定不同的奖励函数（如坐下、蹲伏、以特定速度移动），机器人能够展现出多样且符合奖励要求的行为。</p>
</blockquote>
<p><strong>消融实验总结</strong>：论文通过定量实验（见图3）和补充材料中的分析，验证了非对称学习、领域随机化、历史观测以及辅助奖励等关键设计选择的必要性。例如，移除领域随机化会导致模拟性能虽高但无法真实部署；非对称训练对于处理部分可观测性至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>第一个用于真实人形机器人的可提示行为基础模型（BFM）</strong>，该模型通过单一策略支持零样本奖励优化、目标到达和运动跟踪。</li>
<li>证明了<strong>离策略无监督强化学习</strong>（特别是基于前向后向表示的方法）是构建此类基础模型的有效范式，并成功解决了模拟到现实的迁移挑战。</li>
<li>系统性地展示了一套<strong>关键算法与工程设计</strong>（非对称训练、大规模并行、领域随机化、奖励正则化），使得在模拟中训练的无监督RL策略能够零样本部署于复杂、高自由度的真实人形机器人，并表现出鲁棒性和适应性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，奖励推理过程可能因训练数据的随机性（尤其是领域随机化引入的）而变得脆弱，偶尔会导致性能不佳；模型依赖于高质量的人类运动数据集进行风格正则化；大规模并行训练需要较高的计算资源。</p>
<p><strong>对后续研究的启示</strong>：BFM-Zero的成功表明，无监督RL是迈向通用机器人基础模型的一条可行路径。它启发了未来研究可以探索：1）结合视觉等更丰富的感知模态；2）扩展技能库的规模和多样性；3）研究更高效、稳定的潜在空间适应与组合方法；4）将高层规划与低层BFM策略相结合，以实现更复杂的任务分解与执行。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出BFM-Zero，一个基于无监督强化学习的可提示行为基础模型，旨在解决人形机器人全身控制中单一策略难以泛化至多任务的核心问题。其关键技术是构建一个共享的潜在表示空间，将动作、目标与奖励嵌入其中，并利用前向-后向模型学习可解释的平滑表征。通过奖励塑造、领域随机化等设计缩小仿真到现实的差距。实验表明，该模型在真实Unitree G1机器人上实现了零样本运动跟踪、目标到达等多种推理能力，以及少样本自适应，推动了可扩展、可提示的全身控制基础模型的发展。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.04131" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>