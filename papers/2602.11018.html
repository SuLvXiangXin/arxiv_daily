<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.11018" target="_blank" rel="noreferrer">2602.11018</a></span>
        <span>作者: Balaraman Ravindran Team</span>
        <span>日期: 2026-02-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人、自动驾驶等高风险领域，在线强化学习（RL）因需要大量环境交互而难以应用，且设计精确的奖励和安全成本函数极具挑战。离线模仿学习（IL）可以从演示数据中学习策略，但传统IL方法隐含假设专家演示是安全的，若数据中包含不安全行为，单纯模仿会导致策略不安全。安全强化学习通常被建模为约束马尔可夫决策过程（CMDP），但该框架需要已知每一步的安全成本，这在现实中往往难以获取。</p>
<p>然而，收集反映不良或不安全行为的轨迹（即非偏好轨迹）通常是可行的。现有方法难以有效利用此类数据。例如，最新的离线安全IL方法SafeDICE假设“联合轨迹数据集”（包含高回报轨迹）中的轨迹要么全是低成本的，要么全是高成本的，这在实际中不成立，因为高回报轨迹的安全成本往往是一个连续谱。另一类基于偏好的策略学习（PPL）方法，则假设联合轨迹数据集是安全的（即偏好轨迹），这同样不总是成立。</p>
<p>本文针对上述痛点，提出了一种新的离线安全IL算法OSIL，其核心思路是：在仅给定一个包含<strong>高回报但成本各异</strong>的联合轨迹数据集，以及一个<strong>高回报但高成本</strong>的非偏好轨迹数据集的条件下，通过从非偏好演示中推断安全约束，学习一个既安全（满足成本约束）又高性能（最大化回报）的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>OSIL将安全策略学习问题形式化为一个CMDP问题。由于无法获得奖励和成本信息，该方法通过推导奖励最大化目标的下界，并学习一个参数化的成本模型来近似安全约束，从而重新表述CMDP问题。整体流程分为两个核心阶段：学习成本动作价值函数，以及学习安全策略。</p>
<p>首先，需要学习一个成本模型 $\tilde{c} := g \circ f$，该模型估计状态-动作对属于非偏好行为的可能性。其中 $f$ 是一个将状态-动作对映射到d维单位范数潜在表示的编码器，$g$ 是一个将编码表示映射到标量值（介于0和1之间）的线性模型。</p>
<p><img src="https://arxiv.org/html/2602.11018v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：成本学习模型概览。$f$ 和 $g$ 分别是可学习的编码器和线性模型。成本模型通过最小化两个损失函数进行训练：$\mathcal{L}<em>{\text{cost}}^{\text{const}}$ 鼓励轨迹内时间相邻的状态-动作对在学习的表示空间中保持接近，$\mathcal{L}</em>{\text{cost}}^{\text{pref}}$ 确保非偏好轨迹 $\tau_N$ 的折扣成本高于联合轨迹 $\tau_U$。</p>
</blockquote>
<p>成本模型的训练结合了对比学习和基于偏好的学习。对比损失 $\mathcal{L}<em>{\text{cost}}^{\text{cont}}$ 确保同一轨迹中的状态-动作对在表示空间中彼此接近，从而捕获时间依赖性。基于偏好的损失 $\mathcal{L}</em>{\text{cost}}^{\text{pref}}$ 采用Bradley-Terry模型，强制非偏好轨迹 $\tau_N$ 的总折扣成本 $C(\tau_N)$ 高于联合轨迹 $\tau_U$ 的成本 $C(\tau_U)$。总损失为 $\mathcal{L}<em>{\text{cost}} = \mathcal{L}^{\text{pref}}</em>{\text{cost}} + \mathcal{L}^{\text{cont}}_{\text{cost}}$。学习到成本模型后，利用时序差分（TD）学习在联合数据集 $\mathcal{D}<em>U$ 上训练成本动作价值函数 $Q^{\pi}</em>{\tilde{c}}$。</p>
<p>在策略学习阶段，目标是最大化策略性能 $J_r(\pi)$ 同时满足成本约束 $\mathbb{E}<em>{s\sim\rho_0}[Q^{\pi}</em>{\tilde{c}}(s, \pi(s))] \le b$。由于奖励未知，论文推导出策略性能 $J_r(\pi)$ 相对于生成联合数据集的策略 $\pi_U$ 的性能 $J_r(\pi_U)$ 的一个下界：$J_{r}(\pi) \geq J_{r}(\pi_{U}) - \dfrac{2\epsilon}{1-\gamma}\sqrt{D_{\text{KL}}^{\text{max}}(\pi_{U},\pi)}$。由于 $\pi_U$ 能产生高回报轨迹，$J_r(\pi_U)$ 可作为最高可达性能的代理。因此，最大化 $J_r(\pi)$ 的下界近似等价于最小化策略 $\pi$ 与 $\pi_U$ 之间的KL散度。</p>
<p>通过用平均KL散度近似最大KL散度，并将约束优化问题转化为拉格朗日形式，得到最终的策略损失函数：<br>$\mathcal{L}<em>{\text{policy}} = -\mathbb{E}</em>{(s,a)\sim\mathcal{D}<em>{U}}\left[\log\pi(a|s)\right] + \mathbb{E}</em>{s\sim\rho_{0}}\left[\alpha Q^{\pi}_{\tilde{c}}(s,\pi(s))\right]$<br>该损失由两部分组成：<strong>行为克隆项</strong>（鼓励模仿联合数据集中的高回报行为）和<strong>成本批判项</strong>（惩罚高风险行为，鼓励安全）。超参数 $\alpha$ 用于平衡安全与性能，并采用自适应公式：当学习策略选择的动作平均成本高于数据集中动作时，$\alpha$ 增大以加强安全约束；反之则减小以专注于高回报行为。</p>
<p>与现有方法（如TD3+BC）的关键区别在于，OSIL使用<strong>成本动作价值函数</strong>而非奖励动作价值函数来指导策略，从而直接促进安全行为。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在离线安全RL数据集（DSRL）的一系列基准任务上进行评估，包括：(i) 基于MuJoCo的速度约束任务（Walker2d-Velocity, Swimmer-Velocity, Ant-Velocity），要求智能体在满足速度上限的前提下尽可能快速移动；(ii) 导航任务（Point-Circle2, Point-Goal1, Point-Button1），要求智能体在避免碰撞和危险区域的同时最大化性能。</p>
<p>对比的基线方法包括：行为克隆（BC）、TD3+BC、基于偏好的方法（DWBC、T-REX）以及最先进的离线安全IL方法SafeDICE。</p>
<p><img src="https://arxiv.org/html/2602.11018v1/x3.png" alt="性能对比1"></p>
<blockquote>
<p><strong>图3</strong>：在Walker2d-Velocity, Swimmer-Velocity, Ant-Velocity任务上的性能对比。阴影区域代表标准误差。在速度约束任务中，OSIL能够在<strong>不牺牲奖励性能</strong>的前提下，恢复出<strong>更安全</strong>的策略（成本显著低于除SafeDICE外的基线）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11018v1/x4.png" alt="性能对比2"></p>
<blockquote>
<p><strong>图4</strong>：在Point-Circle2, Point-Goal1, Point-Button1导航任务上的性能对比。与图3结果类似，OSIL相比其他基线方法能恢复出更安全的策略。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11018v1/x5.png" alt="汇总对比"></p>
<blockquote>
<p><strong>图5</strong>：所有任务上的平均性能汇总。如图所示，OSIL在满足成本约束（低成本）方面表现最佳，同时保持了与BC相当的奖励性能，显著优于其他基线方法。在此设置下，OSIL学习到的策略在安全性上比最佳基线（SafeDICE）高出近2.8倍。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11018v1/x6.png" alt="消融实验1"></p>
<blockquote>
<p><strong>图6</strong>：自适应惩罚系数 $\alpha$ 的消融研究。固定 $\alpha$ 的版本（OSIL (fixed $\alpha$)）在部分任务上表现不佳，验证了自适应机制对于平衡安全与性能的重要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11018v1/x7.png" alt="消融实验2"></p>
<blockquote>
<p><strong>图7</strong>：成本模型学习组件的消融研究。移除对比损失（w/o $\mathcal{L}^{cont}$）或偏好损失（w/o $\mathcal{L}^{pref}$）都会导致性能下降，表明两者对于学习有效的成本模型都是必要的。</p>
</blockquote>
<p>消融实验总结：1) **自适应惩罚系数 $\alpha$**：固定 $\alpha$ 的变体性能不稳定，证明了自适应机制的有效性。2) <strong>成本模型损失组件</strong>：移除对比损失 $\mathcal{L}^{cont}$ 或偏好损失 $\mathcal{L}^{pref}$ 都会损害策略的安全性能，说明两者共同作用才能准确推断安全成本。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一种新的离线安全模仿学习框架OSIL，它<strong>仅从非偏好轨迹中推断安全约束</strong>，无需预先定义成本函数或假设演示数据完全安全。2) 通过推导策略性能下界，将原CMDP问题转化为一个结合了行为克隆和成本批判的、可优化的约束问题。3) 引入了<strong>自适应惩罚系数</strong>，动态平衡策略的安全性与回报性能。</p>
<p>论文自身提到的局限性包括：1) 成本模型训练中使用的硬标签假设（即非偏好轨迹成本始终高于联合轨迹）在实践中可能并不严格成立，尽管平均来看该假设是可靠的。2) 在策略优化中，使用平均KL散度近似最大KL散度可能引入理论上的偏差。</p>
<p>这项工作对后续研究的启示在于：为从包含噪声或不完美演示的离线数据中学习安全策略开辟了新途径。未来方向可能包括：探索更鲁棒的成本模型学习方法来处理模糊的非偏好信号，将框架扩展到多约束或分层安全规范，以及研究在非平稳环境或数据分布下的适应性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出OSIL算法，解决离线安全模仿学习问题：在缺乏显式安全成本标注的演示数据中，学习同时满足安全约束与奖励最大化的策略。核心技术是：1）将问题建模为约束马尔可夫决策过程；2）从“非偏好轨迹”（即包含不安全行为的演示）中推断安全约束，而非依赖人工标注的成本函数；3）推导奖励目标的下界并学习成本模型来估计非偏好行为可能性。实验表明，OSIL能学习到高回报、低成本的策略，在速度约束和导航任务上，其性能比最佳基线提升约2.8倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.11018" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>