<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.20332" target="_blank" rel="noreferrer">2506.20332</a></span>
        <span>作者: Gu, Jihao, Ai, Qihang, Wang, Yingyao, Bu, Pi, Xing, Jingxuan, Zhu, Zekun, Jiang, Wei, Wang, Ziming, Zhao, Yingxiu, Zhang, Ming-Liang, Song, Jun, Jiang, Yuning, Zheng, Bo</span>
        <span>日期: 2025/06/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉语言模型（VLM）的移动智能体已能理解复杂指令和移动屏幕截图，并通过强化学习（如组相对策略优化GRPO）优化其动作输出。然而，现有研究集中于离线强化学习训练或使用动作级奖励进行在线优化。这种方法的局限性在于限制了智能体与环境的动态交互，容易导致智能体陷入局部最优，从而削弱其探索和错误动作纠正的能力。</p>
<p>本文针对上述痛点，提出了一种新的视角：为移动智能体引入基于任务级奖励的交互式多轮强化学习。其核心思路是通过一个包含格式微调、动作级GRPO训练和任务级GRPO训练的三阶段过程，利用任务级奖励来增强移动代理的探索和错误纠正能力。</p>
<p><img src="https://arxiv.org/html/2506.20332v3/x1.png" alt="动作级与任务级奖励对比"></p>
<blockquote>
<p><strong>图1</strong>：动作级与任务级奖励对比。(a) 动作诱导的智能体在每一步被鼓励“思考更久”以选择最佳单步动作。(b) 任务级奖励训练的智能体在多轮交互中探索并调整其轨迹。</p>
</blockquote>
<h2 id="方法详解">方法详解</h2>
<p>Mobile-R1的整体训练框架是一个三阶段的pipeline，旨在逐步提升智能体在动态移动环境中的交互能力。</p>
<p><img src="https://arxiv.org/html/2506.20332v3/x3.png" alt="训练框架"></p>
<blockquote>
<p><strong>图3</strong>：Mobile-R1的三阶段训练框架：初始格式微调、基于动作级奖励的在线训练、基于多轮轨迹的任务级奖励在线训练。</p>
</blockquote>
<p><strong>整体流程与核心模块</strong></p>
<ol>
<li><p>**Stage1: 格式微调 (Format Finetuning)**：此阶段为冷启动，使用高质量标注的动作轨迹数据进行监督微调（SFT）。目标是让模型学会根据指令和操作历史预测下一步动作，建立用户意图、GUI状态和动作之间的基本联系。响应格式遵循 <code>&lt;thinking&gt;</code>、<code>&lt;action&gt;</code>、<code>&lt;tool_call&gt;</code> 的结构，动作空间包含点击、滑动、输入等8种原子操作。</p>
</li>
<li><p>**Stage2: 动作级在线训练 (Action-level Online Training)**：此阶段使用GRPO进行在线训练，奖励信号为动作级。奖励函数 (R_{action}) 由两部分构成：</p>
<ul>
<li><strong>动作奖励 (R_{Act})</strong>：评估执行动作的正确性。对于坐标类动作（如点击），若预测坐标落在目标GUI元素边界框内则得1分，否则为0；对于非坐标动作（如输入），需完全匹配真实动作或参数才得1分。</li>
<li><strong>格式奖励 (R_{Fmt})</strong>：鼓励模型输出符合预定格式（思考、动作、工具调用），为二元奖励（1或0）。<br>此阶段训练智能体在执行每一步动作时进行反思，以优化单步动作的准确性和格式合规性。</li>
</ul>
</li>
<li><p>**Stage3: 任务级在线训练 (Task-level Online Training)**：此阶段是核心创新，进行基于多轮轨迹的GRPO训练，奖励信号为任务级。奖励函数 (R_{task}) 同样包含两部分：</p>
<ul>
<li><strong>格式奖励 (R_{Fmt})</strong>：对整条轨迹所有动作的格式合规性取平均，并设定在 ([-1, 1]) 区间以施加更严格的惩罚。</li>
<li><strong>轨迹级奖励 (R_{Traj})</strong>：由外部高性能MLLM（GPT-4o）对整个历史交互轨迹 (\tau) 进行评估打分，分数范围 ([0, 1])。评估基于两个主要标准：<strong>轨迹连贯性</strong>（步骤是否一致遵循指令、动作是否清晰、有无冗余步骤）和<strong>任务完成度</strong>（任务是否完全完成、必要交互是否完成、错误处理是否得当）。<br>此阶段将多轮交互建模为马尔可夫决策过程（MDP），目标是训练策略 (\pi_{\theta}) 最大化整个交互序列的累积奖励，从而鼓励智能体在完成任务的长期目标下进行动态探索和自我纠错。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong><br>与现有方法（如表1所示）相比，Mobile-R1的核心创新在于：</p>
<ol>
<li><strong>奖励来源的转变</strong>：从普遍使用的动作级奖励，转向结合动作级与任务级奖励，特别是引入了由外部模型评估的、关注整体任务完成质量的轨迹级奖励。</li>
<li><strong>训练目标的升级</strong>：训练目标从优化单步最佳动作，升级为优化完成整个任务的多轮交互轨迹，使智能体具备长程规划和错误恢复（“尤里卡时刻”）能力。</li>
<li><strong>三阶段渐进式训练</strong>：设计了从格式学习到单步优化，再到多轮探索的渐进式训练策略，确保了在引入复杂任务级奖励时训练的稳定性。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong></p>
<ul>
<li><strong>基准/数据集</strong>：1) 作者构建了一个新的中文移动智能体基准，包含500条人工标注的轨迹（共1842步），其中225条来自长尾未见应用以评估泛化能力。2) 使用了英文基准AndroidControl进行跨语言评估。3) 训练数据来自自建的高质量轨迹数据集，包含4635条轨迹（共24521步），覆盖28个中文应用。</li>
<li><strong>实验平台</strong>：使用Android Studio模拟器作为移动GUI交互环境。</li>
<li><strong>基线方法</strong>：对比了Qwen2.5-VL系列（3B, 7B, 32B）、UI-R1-3B、GUI-R1（3B, 7B）、AgentCPM-8B等VLM基移动智能体。</li>
<li><strong>评估指标</strong>：步骤准确率（Acc.）、任务成功率（Task Succ.）、尾部成功率（Tail Succ.，即最终完成任务的概率）、平均错误数（Avg Err.），以及在AndroidControl上使用的类型匹配（TM）和精确匹配（EM）。</li>
</ul>
<p><strong>关键实验结果</strong><br>如表3所示，Mobile-R1（基于Qwen2.5-VL-3B）在自建中文基准上取得了最佳性能：步骤准确率78.55%，任务成功率30.60%，尾部成功率37.40%，平均错误数241。其准确率比最佳基线（Qwen2.5-VL-32B）高出2.65个百分点。消融实验表明，仅使用Stage1&amp;2的模型已优于多数基线，而Stage3的任务级训练进一步将任务成功率提升了1.2个百分点。</p>
<p><img src="https://arxiv.org/html/2506.20332v3/images/reward_step_with_shaded_stages.png" alt="Stage3训练奖励分数"></p>
<blockquote>
<p><strong>图5</strong>：Stage3训练过程中奖励分数的变化。初期增长缓慢（可能因探索不稳定），中期加速学习，后期趋于稳定，表明策略逐渐收敛。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.20332v3/x4.png" alt="定性轨迹对比"></p>
<blockquote>
<p><strong>图6</strong>：Mobile-R1与Qwen2.5-VL-3B-Instruct在具体任务上的推理轨迹定性对比。Mobile-R1能准确识别图标、完成多步任务，并在最后一步根据上下文（已关注）正确终止，展示了更优的精确性和情境感知能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.20332v3/images/pass_at_k_accuracy_comparison_styled_3.png" alt="Pass@K性能"></p>
<blockquote>
<p><strong>图7</strong>：Mobile-R1与基线模型的Pass@K性能对比。随着尝试次数K增加，两者准确率均上升，但Mobile-R1始终表现更优，最高有7个百分点的绝对提升，展示了更强的通过多次尝试解决问题的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.20332v3/images/e2e_2.png" alt="物理设备任务成功率"></p>
<blockquote>
<p><strong>图8</strong>：在物理设备上50个随机任务的端到端成功率。经过三阶段训练，Mobile-R1最终取得了24个百分点的显著提升。</p>
</blockquote>
<p><strong>泛化能力</strong><br>在英文基准AndroidControl上的测试结果（表4）显示，仅3B参数的Mobile-R1在多数指标上超越了多个7B参数的先进模型，证明了其方法的有效性和良好的跨语言、跨任务泛化能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong></p>
<ol>
<li><strong>提出任务级奖励的交互式RL框架</strong>：首次为VLM基移动智能体系统性地引入了基于多轮轨迹和任务完成度的强化学习训练范式，有效提升了智能体的探索和错误纠正能力。</li>
<li><strong>构建高质量中文资源</strong>：发布了包含大量高质量人工标注轨迹的数据集和一个专注于中文应用生态的评测基准，弥补了该领域资源的不足。</li>
<li><strong>开发高性能Mobile-R1智能体</strong>：通过三阶段渐进训练策略，成功将一个3B的VLM训练为性能卓越的移动交互智能体，并在实验中验证了其优越性。</li>
</ol>
<p><strong>局限性</strong><br>论文自身未明确阐述局限性，但从实验设置可推断：1) 任务级奖励依赖外部大模型（GPT-4o）评估，成本较高且可能引入评估偏差。2) 第三阶段训练仅使用了5个常用应用的100条轨迹，其在不同应用和任务上的泛化效能需进一步验证。3) 基模型规模较小（3B），可能在某些复杂推理任务上存在能力上限。</p>
<p><strong>研究启示</strong></p>
<ol>
<li><strong>奖励设计的重要性</strong>：对于需要多步决策的交互任务，设计能够反映长期任务目标的奖励信号（如任务级奖励）比单纯优化单步动作更为有效。</li>
<li><strong>在线交互学习的价值</strong>：在动态变化的环境（如移动应用）中，在线强化学习允许智能体通过实时探索和试错进行适应，是离线训练的重要补充。</li>
<li><strong>“尤里卡时刻”的涌现</strong>：任务级奖励训练可能促使智能体获得从错误中恢复的元认知能力，这为开发更鲁棒、更智能的自主系统提供了新思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对基于视觉语言模型的移动智能体在动态交互环境中易陷入局部最优、探索与纠错能力弱的问题，提出Mobile-R1方法。其核心是采用交互式多轮强化学习框架，通过任务级奖励（而非传统的动作级奖励）进行在线训练，训练过程分为初始格式微调、单步动作奖励训练和多轮任务奖励训练三个阶段。该方法显著提升了智能体的探索与纠错性能，并构建了包含28个中文应用、24,521条标注数据及500条轨迹的新基准。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.20332" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>