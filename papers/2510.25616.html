<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Don&#39;t Blind Your VLA: Aligning Visual Representations for OOD Generalization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Don&#39;t Blind Your VLA: Aligning Visual Representations for OOD Generalization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.25616" target="_blank" rel="noreferrer">2510.25616</a></span>
        <span>作者: Kachaev, Nikita, Kolosov, Mikhail, Zelezetsky, Daniil, Kovalev, Alexey K., Panov, Aleksandr I.</span>
        <span>日期: 2025/10/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型旨在通过将预训练的视觉-语言模型适配到机器人动作预测任务中，以继承其可迁移的世界知识和视觉-语言基础，从而实现更广泛的泛化。然而，在将VLMs适配到动作模态时，其原始的VL表示和知识能在多大程度上得以保留尚不清楚。实践表明，当前的VLA模型在视觉和语言复杂的任务中难以维持泛化能力，尤其是在任务特定的有监督微调阶段，有限的数据多样性和过拟合风险会导致VL能力的退化。</p>
<p>本文针对VLA模型在动作微调过程中出现的视觉表示退化这一具体痛点，提出了一个新的视角：基于“柏拉图表示假说”，即高性能的视觉、语言和多模态模型倾向于收敛到一个共享的潜在表示空间。本文的核心思路是：在VLA模型进行动作微调时，通过一个轻量级的视觉表示对齐目标，将其内部视觉特征与一个强大的、冻结的视觉教师模型的特征对齐，从而保留广义的视觉语义，提升模型的分布外泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法旨在通过正则化VLA模型的内部嵌入，使其与一个预训练的、冻结的视觉教师模型的嵌入保持接近，从而在动作微调过程中恢复并保持广义的、语义一致的视觉表示。</p>
<p><img src="https://arxiv.org/html/2510.25616v1/figures/scheme1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：提出的方法概述。(a, b) 带有视觉对齐损失的训练流程——无需额外开销，仅需预计算的教师特征和在SFT期间的一个轻量级正则化项。(c) VL任务损失景观的概念性说明：核心思想是在针对动作目标优化模型的同时，保留其在VL理解上的性能。</p>
</blockquote>
<p>整体流程如<strong>图2</strong>所示。给定输入图像I和文本指令ℓ，VLA模型通过其图像编码器E_image和文本编码器E_text生成视觉和文本令牌序列。这些令牌经过一个多层Transformer骨干网络B_θ处理。在微调时，除了标准的自回归动作损失L_VLA，模型还引入了一个视觉对齐损失L_align。</p>
<p>该方法的<strong>核心模块</strong>是视觉表示对齐目标。具体而言，选择一个承载丰富视觉语义的VLA中间层i<em>，提取其视觉令牌特征h_{1:k}^{i</em>} ∈ R^{k×d_e}。同时，使用一个冻结的教师图像编码器E_img<em>为同一输入图像I生成补丁级特征z_{1:k} ∈ R^{k×d_t}。由于维度可能不同，通过一个可学习的投影器P_φ将学生特征映射到教师特征空间：u_{1:k} = P_φ(h_{1:k}^{i</em>})。对齐损失定义为学生投影特征与教师特征之间的负平均相似度：<br>L_align = - (1/k) Σ_{j=1}^k Sim(u_j, z_j)<br>其中Sim为相似度函数。总损失为动作损失与对齐损失的加权和：L_total = L_VLA + λ L_align，λ &gt; 0。</p>
<p><strong>创新点</strong>在于：1) <strong>视角创新</strong>：将VLA微调中的表示退化问题置于“柏拉图表示假说”框架下理解，将教师模型视为稳定、广义表示空间的参考点。2) <strong>方法轻量</strong>：该方法仅需一个冻结的教师编码器、一个轻量级投影器和一个额外的损失项，计算开销极小，可无缝集成到标准SFT流程中，与需要重型监督、高昂计算成本或约束架构的现有方法形成对比。3) <strong>针对性强</strong>：专门解决此前缺乏有效方法的任务特定SFT阶段的表示退化问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要评估基于Simpler基准及其变体，并使用新提出的VL-Think任务套件进行诊断。评估平台涉及WidowX-250S机械臂的拾放任务。OOD评估遵循Liu等人（2025）引入的基准，从<strong>视觉</strong>、<strong>语义</strong>和<strong>执行</strong>三个轴测试泛化能力，每个轴至少保留一个变异因素。</p>
<p><strong>对比方法</strong>：主要对比了：1) <strong>Default</strong>：标准的监督微调基线。2) <strong>Freeze</strong>：冻结视觉和文本编码器的基线。3) **Align (ours)**：本文提出的视觉对齐方法。</p>
<p><strong>关键实验结果</strong>：<br>在OOD泛化性能上（见论文中表1，此处以文字总结），本文提出的对齐方法在绝大多数测试场景中超越了默认SFT基线。例如，在“Semantic”场景成功率从0.49提升至0.61，“Vision”场景从0.74提升至0.83，“Execution”场景从0.28提升至0.35。在多个具体测试环境（如Carrot, MultiCarrot, Plate等）上均观察到一致提升，相对增益最高可达约10%。冻结编码器的方法由于严重限制了模型适应性，性能极差。</p>
<p><img src="https://arxiv.org/html/2510.25616v1/x1.png" alt="注意力图对比"></p>
<blockquote>
<p><strong>图4</strong>：注意力图对比：最具语义基础的注意力出现在中间层附近。使用我们提出的方法微调的OpenVLA在注意力图中保持了对象对齐的焦点，而默认的OpenVLA SFT则显示出分散和嘈杂的模式，表明视觉-语言基础能力的丧失。</p>
</blockquote>
<p><strong>图4</strong>表明，与能产生清晰、相关对象对齐注意力的强VLM（如Qwen2.5-VL）相比，标准SFT后的VLA（OpenVLA）注意力图变得分散、嘈杂且与目标对象相关性弱。而采用视觉对齐方法微调的模型（OpenVLA Align）能够恢复出清晰、对象中心的注意力模式，证明了该方法在维持视觉-语言基础方面的有效性。</p>
<p><img src="https://arxiv.org/html/2510.25616v1/figures/tsne1.png" alt="t-SNE可视化"></p>
<blockquote>
<p><strong>图5</strong>：Qwen2.5-VL、PrismaticVLM和OpenVLA的令牌嵌入t-SNE可视化。PrismaticVLM和Qwen2.5-VL保持了目标对象良好分离的聚类，而OpenVLA显示出类别间的巨大重叠，表明动作微调导致了表示坍塌。</p>
</blockquote>
<p><strong>图5</strong>的t-SNE可视化直观展示了表示退化问题。预训练的VLM（PrismaticVLM, Qwen2.5-VL）对于“杯子”、“瓶子”、“刀”等类别的嵌入形成了良好分离的聚类。然而，经过动作微调的OpenVLA则出现明显的表示坍塌，类别簇变得模糊和重叠，失去了语义结构的分离性。</p>
<p><strong>VL-Think诊断结果</strong>：使用VL-Think任务套件评估发现，强大的VLMs在所有领域（形状、颜色、交通标志等）都取得高成功率。然而，与其预训练基础模型PrismaticVLM相比，经过动作微调的OpenVLA-7B在几乎所有领域都出现性能下降，尤其在符号和抽象类别（交通、箭头、公共信息、天气）上下降最为显著，这表明了<strong>领域特定遗忘</strong>。仅“颜色”领域的知识得以保留，推测是因为颜色线索在机器人数据集中直接有用。</p>
<p><strong>消融实验</strong>：论文中的主要对比实验本身可视为对“是否引入对齐损失”的消融。结果表明，引入对齐损失（Align）相较于不引入（Default）或完全冻结编码器（Freeze），在保持模型适应性的同时，显著缓解了表示退化，提升了OOD泛化性能，证明了该组件的关键贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>系统性地揭示并量化了问题</strong>：通过注意力分析、t-SNE可视化和新设计的VL-Think诊断套件，系统性地证明了朴素的VLA动作微调会导致视觉表示坍塌、注意力分散以及领域特定遗忘，使其相对于初始VLM出现VL能力退化。</li>
<li><strong>提出了一个轻量高效的解决方案</strong>：受柏拉图表示假说启发，提出了一种视觉表示对齐方法。该方法通过一个简单的正则化损失，将VLA模型的中间视觉特征与一个强大的、冻结的视觉教师模型的特征对齐，有效缓解了表示退化，并提升了OOD泛化性能，且计算开销极小。</li>
<li><strong>提供了新的分析工具</strong>：引入了VL-Think任务套件，这是一个专注于评估VL知识（而非低级控制技能）从VLM到VLA迁移的诊断性基准，为社区提供了量化VLA模型中VL能力保留程度的新工具。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，该方法依赖于一个外部教师模型的质量和兼容性。此外，对齐损失中相似度函数、投影器结构以及对齐层和权重λ的选择可能需要针对不同模型和任务进行调整。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>对齐目标的扩展</strong>：当前方法主要对齐视觉表示，未来可探索如何同时或分层对齐语言表示或多模态融合表示。</li>
<li><strong>教师模型的选择与集成</strong>：研究如何自动选择或集成多个教师模型（如不同架构、不同模态的专家模型）以提供更鲁棒、更全面的“柏拉图”表示参考。</li>
<li><strong>理论深入</strong>：进一步从理论层面探究“柏拉图表示假说”在具身智能领域的表现形式，以及表示对齐如何影响策略学习的优化轨迹和泛化边界。</li>
<li><strong>更广泛的应用</strong>：该方法的思想可推广至其他需要在大规模预训练模型基础上进行特定领域微调，同时又需保留原有核心能力的场景。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究了视觉语言动作（VLA）模型在动作微调过程中面临的视觉语言（VL）表示退化问题，这损害了其从预训练视觉语言模型（VLM）继承的分布外（OOD）泛化能力。为缓解此问题，作者提出了一种简单的视觉表示对齐方法，核心是将VLA的中间层特征投影到归一化球面上，并与教师VLM的嵌入进行对齐。实验表明，该方法能有效减轻表示退化，从而在多个泛化基准上提升VLA模型对OOD场景的泛化性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.25616" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>