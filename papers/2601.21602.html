<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.21602" target="_blank" rel="noreferrer">2601.21602</a></span>
        <span>作者: Sun, Jianli, Tian, Bin, Zhang, Qiyao, Li, Chengxiang, Song, Zihan, Cui, Zhiyong, Lv, Yisheng, Tian, Yonglin</span>
        <span>日期: 2026/01/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型在地面具身智能领域取得了显著成功，但其在具有浮动基座动力学、无人机与机械臂强耦合以及多步长程任务特性的空中操纵系统（AMS）中的应用仍是一个几乎未被探索的领域。现有VLA研究主要局限于地面移动机械臂，其操作空间受限于二维平面导航和有限的工作高度，无法有效评估三维空间的全向移动性、浮动基座条件下的操作鲁棒性以及基于语言的长程时序推理能力。尽管已有工作尝试将VLA应用于无人机领域，但大多聚焦于导航、任务生成或与路径规划紧密耦合的单自由度VLA操作，对于全谱系空中操纵任务的VLA应用仍属空白。</p>
<p>本文针对空中操纵领域缺乏专门的数据集和评估基准这一核心痛点，提出了首个专为空中操纵系统设计的VLA训练与评估基准AIR-VLA。其核心思路是构建一个包含物理仿真环境、高质量多模态数据集和定制化多维评估指标的综合性平台，以系统评估主流VLA/VLM模型在复杂空中操纵任务上的性能与局限。</p>
<h2 id="方法详解">方法详解</h2>
<p>AIR-VLA是一个为空中操纵系统量身定制的全栈VLA平台，它系统性地从任务分类、评估框架、仿真环境和数据集构建四个维度进行设计。</p>
<p><img src="https://arxiv.org/html/2601.21602v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：AIR-VLA模型框架与空中操纵任务数据集概览。展示了VLA模型在接收多视角视觉观测和语言指令后，输出无人机与机械臂协同动作的流程，以及数据集中涵盖的多样化任务场景。</p>
</blockquote>
<p><strong>整体框架与任务设计</strong>：AIR-VLA包含四个核心任务套件，旨在全面评估模型的操纵精度、语义推理、视觉感知和规划能力。与地面基准不同，它强调3D无人机定位与机械臂动作的深度协同，任务平均长度约475个时间步，远超传统基准，以反映时序复杂性。这四个套件分别是：1) <strong>基础操纵</strong>：评估低层运动控制与无人机-机械臂协调；2) <strong>物体与空间理解</strong>：评估对物理属性和空间关系的细粒度理解；3) <strong>语义理解</strong>：测试对非结构化语言指令的鲁棒性；4) <strong>长程任务</strong>：评估具有时序依赖的多步推理能力。</p>
<p><strong>评估框架</strong>：针对空中操纵的独特性，AIR-VLA构建了一个双层评估框架。对于VLA模型，基于在线仿真环境进行闭环评估，量化无人机空间导航的可行性、末端执行器的操作精度以及长程任务执行中的动态安全性。对于VLM模型，则提出了一套评估其高层任务规划能力的多维指标，涵盖子任务流程构建、3D空间感知、细粒度物体定位和原子技能选择等维度。</p>
<p><img src="https://arxiv.org/html/2601.21602v2/x2.png" alt="基准平台概览"></p>
<blockquote>
<p><strong>图2</strong>：AIR-VLA基准平台概览。该平台集成了基于仿真的遥操作数据采集流程、在线仿真环境、多样化的多模态数据集，并为评估主流VLA和VLM模型提供了全面的基准测试。</p>
</blockquote>
<p><strong>仿真环境与数据集</strong>：仿真环境基于NVIDIA Isaac Sim构建，采用PhysX 5物理引擎，模拟复杂动态条件下的无人机与机械臂协同操作。机器人系统由一个四旋翼无人机和一个7自由度的Franka Panda机械臂组成，构成了一个高冗余、强耦合的高维控制问题。数据集包含3000条高质量人工遥操作演示，采集策略强调捕获非典型的空中协调模式。感知系统配置了无人机前下视RGB-D相机（全局鸟瞰图）、机械臂腕部RGB-D相机（局部细粒度观测）和第三人称视角相机。数据集记录了完整的本体感知信息，并利用LLM生成了具有复杂结构和隐含意图的自然语言指令，以确保语义空间的密集覆盖。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在AIR-VLA构建的仿真环境和数据集上进行，系统评估了主流VLA模型和VLM模型。</p>
<p><strong>VLA实验</strong>：评估的基线模型包括基于流匹配的基础模型π₀和π₀.₅、效率优化变体π₀-FAST、结合CVAE和Transformer的ACT以及采用条件扩散模型的Diffusion Policy。模型在仅使用30-50条演示进行少量微调后进行评估。为测试鲁棒性，引入了向无人机状态空间注入高斯噪声的动态扰动测试，以及移除固定第三人称视角的测试。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能</strong>：大规模预训练模型（π₀.₅和π₀）在AIR-VLA评估中展现出显著优势，在所有指标上均优于ACT和Diffusion Policy等传统模仿学习基线。π₀.₅在四个任务套件上的加权总分（S_total）分别为45.3、42.3、40.2和37.1，整体平均为42.0，表现最佳。然而，在复杂空中操纵任务中，π₀-FAST相比π₀性能下降明显，表明蒸馏和步长减少导致的性能衰减在空中场景中被显著放大。</li>
<li><strong>性能边界分析</strong>：与低自由度地面平台相比，现有VLA模型在高自由度空中平台上的整体任务完成率仍有很大提升空间。π₀在干扰最少的“基础操纵”任务中达到最高成功率，但随着引入密集视觉干扰物、复杂语义指令和长程规划需求，其性能呈显著衰减趋势。在“物体与空间”套件中，模型出现了“空间定位失败”——虽然识别了正确的物体类别，但由于无法准确解析相对位置指令，而操纵了错误位置的相同物体。在协作控制方面，VLA模型在无人机运动控制上的表现显著优于机械臂操作。</li>
<li><strong>安全与鲁棒性</strong>：评估发现了某些情节中存在与环境的破坏性交互，表明解决空中操纵系统的安全约束仍是未来研究的关键方向。鲁棒性测试表明，在无人机扰动下，π₀.₅的性能指标下降相对较小，显示出一定的隐式补偿能力；但在缺乏固定第三人称视角输入时，模型性能（S_total从42.0降至34.5）显著衰减，揭示了当前模型对稳定全局视角的依赖。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.21602v2/VLM.png" alt="VLM评估流程"></p>
<blockquote>
<p><strong>图3</strong>：VLM在空中文语任务中高层规划能力的评估流程。展示了向VLM输入多视角初始场景图像和任务指令，VLM输出符合规范的高层任务规划序列的评估过程。</p>
</blockquote>
<p><strong>VLM实验</strong>：评估了包括Qwen3-VL、Qwen2.5-VL、GLM-4V、InternVL3.5等在内的六种主流开源VLM的高层规划能力。</p>
<p><strong>关键实验结果</strong>：<br>定量结果表明，Qwen3-VL-8B-Instruct和Qwen2.5-VL-7B-Instruct在空中文语任务的高层规划上建立了显著的性能优势。Qwen3-VL在流程规划、空间导航、物体定位和技能选择四个核心维度上均达到了基线模型中的最优性能，其整体平均总分（S_total）为82.4，规划成功率（Succ）为23.3%。值得注意的是，所有VLM模型在“空间导航”指标上的得分普遍较低，这突显了当前VLMs在精确3D空间推理方面存在固有弱点。此外，在“干扰干扰”任务场景下，所有模型的“物体定位”能力均出现显著下降，表明复杂视觉环境对VLM的细粒度感知构成了严峻挑战。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>提出了首个专为空中操纵系统设计的VLA基准测试平台AIR-VLA</strong>，填补了3D空中操纵领域的评估空白，为社区提供了标准化的研究工具和高质量数据支持；2) <strong>定制了针对空中操作特点的多维评估指标</strong>，超越单一任务成功率，深入评估VLA的协同控制能力和VLM的长程规划能力；3) <strong>对主流VLA和VLM模型进行了全面的基线分析与性能边界探索</strong>，揭示了从地面到空中平台迁移过程中的关键挑战（如空间定位失败、机械臂操作精度不足、对全局视角的依赖等）。</p>
<p>论文自身提到的局限性包括：整体任务成功率仍有较大提升空间；安全约束问题突出；模型性能在复杂视觉干扰和语义指令下衰减明显；VLM在3D空间导航方面存在固有弱点。</p>
<p>这项工作为通用空中机器人研究奠定了标准化测试床和数据基础。其启示在于：未来研究需要着重提升VLA模型在浮动基座下的细粒度操作精度和抗干扰鲁棒性；开发不依赖于固定全局视角的主动感知策略；加强VLM对3D空间的精确几何理解和推理能力；并将安全约束明确地集成到模型训练与决策过程中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型难以直接应用于具有浮动基座、强耦合特性及长时程任务特点的空中操纵系统的问题，提出了首个专用基准AIR-VLA。其核心方法是构建一个基于物理的仿真平台并发布包含3000条人工遥控演示的高质量多模态数据集，涵盖基础操控、空间理解与长时程规划等任务。实验系统评估了主流VLA/VLM模型，验证了VLA范式向空中系统迁移的可行性，并通过定制化多维指标揭示了当前模型在无人机机动、机械臂控制与高层规划方面的能力边界。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.21602" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>