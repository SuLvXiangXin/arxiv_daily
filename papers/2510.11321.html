<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.11321" target="_blank" rel="noreferrer">2510.11321</a></span>
        <span>作者: Yanchao Yang Team</span>
        <span>日期: 2025-10-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作策略学习在遇到意外变化或新场景时常常失败，存在泛化差距。主流方法在表征学习上，多模态方法通常侧重于模态对齐，但往往忽视了操作任务中固有的结构化时间模式；而时序表征学习方法则单独处理时间结构。这些方法未能同时整合多模态信息与分层时间结构，限制了学习到的表征对复杂操作任务的泛化能力。</p>
<p>本文针对从无标签多模态演示数据中自动发现可迁移的、具有层次结构的操作概念这一具体痛点，提出了一个自监督学习框架。核心思路是通过一个跨模态关联网络捕捉跨感官模态的持久模式，并结合一个多时间尺度预测器将表征在时间尺度上分层组织，从而从数据中自发涌现出分层操作概念，并利用这些概念来增强策略的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>HiMaCon框架分为两个阶段：自监督操作概念发现，以及利用发现的概念进行策略增强。</p>
<p><img src="https://arxiv.org/html/2510.11321v2/figures/method/pipeline5.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：提出的自监督操作概念发现与策略增强框架。第一阶段：概念编码器（ℰ）处理多模态机器人演示数据以提取概念潜在表示。这些表示通过两个目标进行优化：(1) 跨模态关联网络（𝒞）采用掩码预测策略捕捉跨感知模态的持久模式；(2) 多时间尺度未来预测器（ℱ）使概念潜在表示能够基于一致性阈值（ϵ）分层组织成多时间尺度的子目标。第二阶段：学习到的概念通过一个具有概念（π_z）和动作（π_a）预测头的主干网络（π_h）集成到策略学习中，用结构化的操作知识正则化动作生成。</p>
</blockquote>
<p><strong>概念编码器</strong>：给定包含N条轨迹的数据集D，每条轨迹τ_i包含T_i个时间步的观测o_i^t和动作a_i^t。观测来自M个模态。概念编码器ℰ（基于Transformer实现）将观测序列映射为概念潜在表示序列z_i = {z_i^t}，其中每个z_i^t ∈ ℝ^Z被视为在时间t活跃的底层操作概念的噪声采样。</p>
<p><strong>核心模块一：跨模态关联网络</strong>：为使概念捕捉跨模态的关联性而非简单聚合，该方法最大化给定概念下不同模态子集观测之间的条件互信息。为实现这一目标，采用了高效的掩码预测方法。具体而言，在训练时随机掩码一部分模态S的观测，然后利用未掩码的模态观测[M]\S和概念z_i^t，通过跨模态关联网络𝒞来重构所有模态的完整观测。损失函数如公式3所示。此过程迫使概念z_i^t必须编码跨模态的关联信息才能完成重构。当所有模态都被掩码时，仅从概念进行重构确保了表示能压缩并保留关键的多模态信息。</p>
<p><strong>核心模块二：多时间尺度未来预测器</strong>：为使概念能分层编码子目标信息，该方法基于概念潜在表示在球面上的距离定义“子过程”。给定一致性阈值ϵ ∈ [0,1]，将一条轨迹中概念距离持续小于ϵ的连续时间段划分为一个子过程。ϵ越大，划分出的子过程时间跨度越长，从而自然形成了一个从短时程到长时程的子目标谱系。为使这些子过程与有意义的子目标完成过程对齐，要求当前观测O及其关联概念Z应能预测当前所在子过程结束时的“终端观测”。为此，训练一个多时间尺度未来预测器ℱ，根据当前观测o_i^t、概念z_i^t和随机采样的阈值ϵ，来预测由公式6确定的子过程终端观测。损失函数如公式7所示。通过最小化此损失，概念z_i^t被确保编码了多时间尺度的子目标信息。</p>
<p><strong>概念发现总目标</strong>：概念编码器ℰ的最终训练目标是结合上述两个损失，如公式8所示，其中λ_mm和λ_mh为平衡权重。</p>
<p><strong>策略增强</strong>：学习到概念编码器ℰ后，将学到的概念用于增强模仿学习。策略架构（公式9）包含一个共享主干π_h、一个概念预测头π_z和一个动作预测头π_a。策略接收任务描述ℓ_i和观测o_i^t，输出共享表示h_i^t，并同时预测动作â_i^t和概念ẑ_i^t。训练损失（公式9）同时最小化动作和概念的预测误差，其中λ_mc为平衡权重。这种联合预测方式使概念理解能够正则化动作生成，而任务描述则引导策略学习如何复用这些与任务无关的概念。</p>
<p><strong>创新点</strong>：与现有方法相比，HiMaCon的创新性体现在同时、显式地优化概念表示的跨模态关联性和分层时间结构，且通过灵活的联合预测方式集成到策略中，无需特定的策略架构设计。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要实验在LIBERO基准（基于Robosuite）上进行。使用了三个任务集：LIBERO-90（90个多样任务，用于概念发现和初始策略训练）、LIBERO-LONG（10个由两个LIBERO-90任务串联而成的新增长时程任务）、LIBERO-GOAL（10个在全新环境中的任务）。多模态观测包括：第三人称视角RGB、手眼相机RGB、9维本体感知状态。对比的基线概念发现方法包括：InfoCon, XSkill, DecisionNCE, RPT等。将学到的概念集成到两种策略架构中进行评估：ACT和扩散策略。</p>
<p><img src="https://arxiv.org/html/2510.11321v2/figures/experiments/vis_hierarchy2.png" alt="结果对比表"></p>
<blockquote>
<p><strong>图3</strong>：不同概念发现方法在LIBERO-90任务上增强ACT和扩散策略的成功率对比。Ours（HiMaCon）在两种策略上都取得了最佳性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>原始任务性能</strong>：在LIBERO-90上训练并评估（L90-90），HiMaCon在ACT和扩散策略上分别达到74.8%和89.6%的成功率，均显著优于所有基线（表1）。</li>
<li><strong>向长时程任务迁移</strong>：将在LIBERO-90上训练的概念编码器直接用于LIBERO-LONG（L90-L），HiMaCon增强的策略依然表现最佳（ACT:63.0%, DP:89.0%），表明所学概念能有效分解层次化任务。</li>
<li><strong>向新环境泛化</strong>：在完全未见的LIBERO-GOAL环境（L90-G）中，HiMaCon仍保持领先（ACT:81.0%, DP:95.7%），证明其发现了可跨环境迁移的基本操作原语。</li>
<li><strong>消融实验</strong>：论文的消融研究（见于正文描述）表明，性能随着纳入更多模态而提升，移除本体感知信息导致性能下降最显著，验证了跨模态关联的重要性。同时，移除跨模态关联网络或多时间尺度预测器中的任何一个都会导致性能下降，证实了两个核心模块的必要性。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.11321v2/figures/experiments/semantic-small3.jpg" alt="概念语义分析"></p>
<blockquote>
<p><strong>图4</strong>：学习到的操作概念的语义分析。通过聚类和人工检查，发现概念潜在空间自动形成了有意义的簇，对应于“抓取”、“放置”、“打开”等人类可解释的操作原语，尽管训练过程没有任何语义监督。</p>
</blockquote>
<p><strong>定性分析</strong>：如图4所示，对学习到的概念进行聚类和可视化分析，发现它们自发地形成了与“抓取”、“放置”、“打开”等人类可理解的操作原语相对应的簇，证明了方法的可解释性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种从无标签多模态演示数据中自监督发现分层操作概念的框架，该框架通过联合优化跨模态关联和多时间尺度子目标预测目标，捕获了操作中不变的模式。</li>
<li>提出了一种灵活的策略增强方法，通过联合预测概念和动作，将学到的概念集成到策略学习中，从而正则化动作生成并提升泛化，该方法兼容多种策略架构。</li>
<li>在模拟和真实世界实验中进行了全面验证，表明概念增强的策略在原始任务、长时程任务组合和新环境泛化上均显著优于基线，并通过分析揭示了所学概念的可解释性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，概念发现过程依赖于演示数据的质量和覆盖范围；此外，多时间尺度未来预测器的训练涉及对多个阈值ϵ的采样，可能增加计算成本。</p>
<p><strong>后续启示</strong>：这项工作表明，通过精心设计的自监督目标，可以从数据中自发涌现出结构良好、可迁移的操作概念。这为机器人表征学习提供了新方向，后续研究可探索更高效的分层概念发现方法，或将此类概念用于强化学习、任务规划等其他机器人学习范式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出HiMaCon框架，解决机器人操作在未见过场景中的泛化问题。通过自监督学习，从无标签多模态数据中发现分层操作概念，无需人工标注。关键技术包括跨模态相关网络捕捉感官模态间的不变模式，以及多时间尺度预测器实现跨时间尺度的分层表示组织。实验表明，基于这些概念的策略在模拟基准和真实部署中性能显著提升，且学习到的概念类似人类可解释的操作原语。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.11321" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>