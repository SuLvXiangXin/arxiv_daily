<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Awakening Facial Emotional Expressions in Human-Robot - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Awakening Facial Emotional Expressions in Human-Robot</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.23059" target="_blank" rel="noreferrer">2510.23059</a></span>
        <span>作者: Zhu, Yongtong, Li, Lei, Qian, Iggy, Zhou, WenBin, Yuan, Ye, Li, Qingdu, Liu, Na, Zhang, Jianwei</span>
        <span>日期: 2025/10/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，人形社交机器人的面部表情生成主要依赖于预先编程的行为模式，这种方法需要高昂的人力和时间成本进行手动编码，限制了机器人的灵活性和适应性。虽然存在一些基于自监督学习的框架，但它们通常学习效率低下，且难以避免因表情不自然而产生的“恐怖谷”效应，制约了这些方法在需要高度自然和准确情感表达的人机交互场景中的应用。</p>
<p>本文旨在解决人形社交机器人高效、自适应地模仿人类面部表情的挑战。针对现有方法依赖随机表情数据导致不自然、以及缺乏通用学习框架的问题，本文提出了一个全新的视角：通过精心设计的专家策略自动构建符合面部运动规律的自然表情数据集，从根本上规避“恐怖谷”效应；并设计一个基于KAN（Kolmogorov-Arnold Network）和注意力机制的端到端轻量级学习框架，以实现从人脸视觉特征到机器人舵机控制命令的高效、准确映射。核心思路是：利用基于面部动作基元的专家策略系统生成自然的表情-控制命令对构建数据集，然后通过一个结合了注意力机制和KAN网络的模型，学习人脸混合形状系数与多自由度机器人面部舵机命令之间的回归关系。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法整体上包含两个核心模块：数据集构建和网络训练，其完整流程如图3所示。</p>
<p><img src="https://arxiv.org/html/2510.23059v1/fig/img03_method.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：方法整体框架。左侧为数据集构建模块：通过基于专家策略的系统自动生成随机表情对应的舵机命令，随后使用常规RGB相机捕获机器人面部图像。右侧为网络训练模块：使用MediaPipe提取人脸表征的混合形状系数，随后通过设计的网络进行回归以拟合舵机命令。</p>
</blockquote>
<p><strong>1. 基于专家策略的数据集构建</strong>：为了规避使用随机表情数据导致的“恐怖谷”效应，本文设计了一个自动化数据收集系统。其核心是一个基于机器人控制框架的专家策略模块（算法1）。该策略定义了一系列约束组（如左右眼水平运动同步、仅允许一种眉毛动作等），在每次迭代中随机选择一个约束组，并据此更新25个自由度（DoF）的舵机位置，从而生成既随机又符合人体面部运动协调规律的表情。这种方法从数据分布层面确保了训练数据的自然性和真实性。</p>
<p><strong>2. 人脸表征</strong>：采用面部混合形状系数来表示表情。该方法将人脸最终形状S表示为中性表情基S0与一系列 blendshape Si 的线性组合（公式1），其中权重系数wi即为表征。本文使用MediaPipe来推断这52个混合形状系数以及头部姿态。</p>
<p><strong>3. 模型设计</strong>：网络输入为52维的混合形状系数，输出为25维的舵机控制命令。模型创新点主要体现在两方面：</p>
<ul>
<li><strong>注意力机制</strong>：针对不同面部区域对不同舵机的驱动权重贡献不同的观察（如图4），模型引入了注意力模块。首先对混合形状系数进行位置编码（公式2，3）以增强区分度，然后通过自注意力机制（公式4）使模型能够自适应地选择相关的特征系数，学习各面部特征的相对重要性。</li>
<li><strong>KAN网络回归</strong>：考虑到机器人应用对响应速度的要求，采用KAN替代传统的MLP来学习从面部特征到舵机命令的映射。KAN的参数仅依赖于激活函数，能通过选择更复杂的样条激活函数来适应任务复杂度。其拟合公式如公式5所示，本文采用了一个3层KAN结构（公式6），其中每层Φi由基础函数b(x)（设为SiLU）和样条函数spline(x)的线性组合构成（公式7）。KAN能以更少的参数达到与MLP相似的拟合精度，从而提升推理速度。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.23059v1/fig/img04_attention.png" alt="注意力机制可视化"></p>
<blockquote>
<p><strong>图4</strong>：不同面部区域对各个舵机驱动权重的贡献差异可视化。这启发了在模型中引入注意力机制，以自适应地关注重要特征。</p>
</blockquote>
<p><strong>4. 损失函数</strong>：总损失由均方误差损失和一致性损失构成（公式10）。MSE损失（公式9）衡量预测舵机命令与真实命令的差异。一致性损失Lcon（公式8）则基于机器人结构控制特点设计，用于确保左右眼、左右眉舵机运动的一致性。超参数λ经实验验证设为0.01。</p>
<p>与现有方法（如Eva直接映射人脸关键点到舵机命令）相比，本文的创新点在于：1) <strong>数据层面</strong>：通过专家策略构建自然表情数据集，从根本上缓解“恐怖谷”问题；2) <strong>网络结构</strong>：引入注意力机制以建模面部区域与舵机驱动的关联，并利用轻量高效的KAN网络提升性能与速度，实现端到端学习。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：1) <strong>Rena Facial Database</strong>：自建的包含9000个样本（8000训练/1000测试）的机器人表情数据集，包含图像及对应舵机命令。2) <strong>MMI-Database</strong>：包含75名受试者六种基本情绪的185个显著面部帧，用于泛化测试。3) <strong>Open Set Database</strong>：从实验室招募的4名志愿者模仿六种基本表情的数据，用于进一步评估泛化能力。</li>
<li><strong>对比基线</strong>：在自有数据集上对比了三种模型：基于人脸关键点的MLP框架、基于人脸混合形状系数的MLP框架、以及本文提出的框架（Attention-KAN-bs）。此外，在序列指标上与Jaekel、Trovato、Habib等方法进行了对比。</li>
<li><strong>评估指标</strong>：伺服控制错误率、累积误差分布（CED）、图像距离（ID）、关键点距离（LD）以及空间相似性、时间相似性、运动平滑度等序列指标。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在Rena Facial Database上的定量结果显示，本文方法取得了最佳性能。如图6所示，三种模型（关键点MLP、混合形状MLP、本文方法）的最低测试损失分别为0.03, 0.0025和0.0020。由于每个舵机的控制范围被归一化到[0,1]，这对应于平均伺服控制错误率分别为17%、5%和**4.4%**。</p>
<p><img src="https://arxiv.org/html/2510.23059v1/fig/img05_loss.png" alt="训练与测试损失曲线"></p>
<blockquote>
<p><strong>图6</strong>：三种模型在训练过程中的训练损失和测试损失曲线。本文提出的方法（Attention-KAN-bs）获得了最低的测试损失。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.23059v1/fig/img06_ced.png" alt="累积误差分布曲线"></p>
<blockquote>
<p><strong>图7</strong>：三种模型的累积误差分布（CED）曲线。基于混合形状表征的模型性能显著优于基于关键点的模型，而结合了KAN和注意力机制的本文模型在处理各种表情时表现出最佳的鲁棒性。</p>
</blockquote>
<p>消融实验（表I）验证了各组件贡献：1) <strong>注意力机制</strong>：在MLP骨干上加入注意力机制，使测试集错误率从2.40%降至2.03%。2) <strong>一致性损失</strong>：在本文模型上，调整超参数λ，当λ=0.01时取得最佳错误率2.34%，而λ=0或过大时性能下降，证明了所提一致性损失的有效性。</p>
<p>在开放集（MMI和Open Set）上的定性评估如图5所示，模型能驱动机器人准确模仿不同测试对象的各种表情。定量评估（表II）显示，本文方法在图像距离（ID，2.96）和关键点距离（LD，0.074）上均显著优于随机基线方法和Eva方法。</p>
<p><img src="https://arxiv.org/html/2510.23059v1/fig/img07_demo.png" alt="开放集模仿结果"></p>
<blockquote>
<p><strong>图5</strong>：在MMI数据集和开放数据集上，将输出的舵机命令应用于Rena机器人。结果表明，本文方法能够准确模拟多个不同人类受试者的各种表情。</p>
</blockquote>
<p>序列性能评估（表III）表明，本文方法在空间相似性（91.8）、时间相似性（88.1）和运动平滑度（92.7）三个指标上均优于对比的三种先进方法，显示出更优的连续表情模仿能力。此外，算法在机器人上的实时响应时间仅为0.02秒（50 FPS）。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>机器人硬件设计</strong>：提出了一个具备高度仿生面部单元（仿生硅胶皮肤和眼球）和25自由度控制结构的面部情感机器人Rena。</li>
<li><strong>学习框架创新</strong>：提出了一个基于KAN和特征注意力机制的轻量级端到端学习框架，用于面部模仿，实现了高精度与高推理速度（50 FPS）。</li>
<li><strong>数据构建策略</strong>：开发了基于面部动作基元专家策略的自动数据收集系统，构建了首个（据其所知）开源的仿人机器人面部数据集，该数据集天然符合面部运动规律，有助于模型学习能有效缓解“恐怖谷”效应的表情。</li>
</ol>
<p>论文提到的局限性在于，当前模型仅关注当前帧，在处理连续面部运动中的突然表情转换时效果有限，对序列表情的生成能力有待加强。</p>
<p>这项工作对未来研究的启示在于：为仿人机器人表情生成提供了一种从数据源头（专家策略）到学习框架（注意力KAN）的完整新范式。后续研究可以在此基础上，通过改进数据集（包含更丰富的时序表情变化）和模型架构（如引入时序建模模块），来增强机器人生成连贯、动态序列表情的能力，从而进一步提升人机交互的自然度。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形社交机器人面部表情生成依赖人工预编程、成本高且缺乏自主学习能力的问题，提出了一种创新方案。通过设计高度仿生的物理电子动画面部单元，构建基于KAN和注意力机制的端到端学习框架，并采用面部运动基元专家策略实现自动数据收集，建立了首个开源机器人面部数据集。实验结果表明，该方法能够准确、多样地模仿人类面部表情，提升了人机交互的自然性与情感表达准确性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.23059" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>