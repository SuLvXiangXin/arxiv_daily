<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.02152" target="_blank" rel="noreferrer">2505.02152</a></span>
        <span>作者: Fan, Cunxin, Jia, Xiaosong, Sun, Yihang, Wang, Yixiao, Wei, Jianglan, Gong, Ziyang, Zhao, Xiangyu, Tomizuka, Masayoshi, Yang, Xue, Yan, Junchi, Ding, Mingyu</span>
        <span>日期: 2025/05/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于大语言模型（LLMs）和视觉语言模型（VLMs）的成功，机器人领域正积极开发视觉-语言-动作（VLA）模型，旨在构建能泛化到未见任务和场景的通用机器人策略。然而，现有主流方法（如RT-1-X, Octo, π₀）主要依赖于纯文本指令（Text-VLA范式），这限制了其泛化能力。文本指令在描述特定形状、颜色的物体或复杂空间关系时可能显得模糊或冗长，导致模型在面对新物体或场景时表现不佳。</p>
<p>本文针对纯文本指令在提供精确、无偏见的上下文信息方面的局限性，提出了一个新视角：借鉴数字世界中交错视觉语言模型（Interleaved VLMs）的成功经验，将交错图像-文本指令引入机器人操作领域。本文认为，交错的图像-文本输入能提供更丰富、偏见更少的上下文，通过情境视觉定位使机器人更好地处理未见任务。</p>
<p>本文核心思路是提出Interleave-VLA范式，通过轻量级适配使现有VLA模型能够理解交错图像-文本指令，并利用一个自动构建的大规模真实世界交错数据集进行训练，从而显著提升模型对未知物体的零样本泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>Interleave-VLA旨在从交错输入中生成连续动作，其整体框架包含三个核心组件：一个轻量级适配模块、一个可扩展的训练流程以及一个多功能的推理接口。</p>
<p><img src="https://arxiv.org/html/2505.02152v2/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：Interleave-VLA范式概述。通过可扩展的适配将Text-VLA扩展到处理交错输入，在构建的大规模交错数据集上进行可扩展训练，并支持广泛交错指令的通用推理。</p>
</blockquote>
<p><strong>整体框架与输入定义</strong>：策略 π_θ 在每一步根据状态 s_t 生成动作 a_t。状态 s_t 定义为元组 (I_t, q_t, ℐ)，其中 I_t 是当前视觉观测，q_t 是机器人本体感知状态，ℐ 是交错指令序列。ℐ 是一个有序序列 (u_1, …, u_M)，其中每个令牌 u_j 属于文本令牌集或图像令牌集。当所有 u_j 均为文本令牌时，Interleave-VLA即退化为标准的Text-VLA。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>适配模块</strong>：这是实现交错处理能力的关键。该方法不对基础VLA模型架构进行修改，而是通过在其分词器中引入特殊的分隔符令牌，使其能够区分图像和文本令牌，并更新输入处理器以支持交错格式。论文重点将Interleave-VLA应用于先进的Text-VLA模型π₀上，使其获得了处理交错数据的能力。这种适配方式具有模型无关性，可轻松扩展到其他VLA模型（如OpenVLA）。</li>
<li><strong>训练流程</strong>：使用第3.3节构建的大规模交错具身数据集对适配后的π₀模型进行训练，不修改其原始的超参数或流匹配目标。训练过程能够高效地随数据集规模和跨模态指令多样性进行扩展。</li>
<li><strong>推理接口</strong>：Interleave-VLA在推理时同时支持纯文本和交错指令。交错指令中的图像来源非常灵活，可以来自机器人相机截图、网络图片甚至手绘草图，即使其风格与训练数据不同。这种设计极大地增强了人机交互的直观性和灵活性。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ul>
<li><strong>指令形式创新</strong>：首次将交错图像-文本指令系统地引入真实世界、低级别连续动作的机器人操作中，突破了纯文本指令的局限。</li>
<li><strong>数据构建创新</strong>：提出了一套全自动流水线，从现有的纯文本机器人数据集中生成高质量的交错指令，创建了大规模真实世界交错数据集。</li>
<li><strong>方法通用性</strong>：提出的适配方案轻量且与模型无关，能以最小代价将现有SOTA的Text-VLA模型升级为Interleave-VLA，提升了方法的可迁移性和实用性。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真评估</strong>：使用SimplerEnv（基于BridgeData V2）评估平台，扩展了交错指令支持。任务包括4个域内任务和10个域外任务，域外任务评估<strong>视觉泛化</strong>（新环境、桌布、光照）和<strong>语义泛化</strong>（已知类别的新物体、全新类别的物体）。</li>
<li><strong>真机评估</strong>：使用FANUC LRMate 200iD/7L机械臂，执行抓取食物/水果、抓放厨具等任务，同样评估域内和域外（新物体）性能。</li>
<li><strong>其他基准</strong>：在VIMA-Bench上评估了Interleave-VLA范式扩展到OpenVLA模型上的性能。</li>
</ul>
<p><strong>对比方法</strong>：</p>
<ul>
<li><strong>Text-VLA基线</strong>：RT-1-X, Octo, π₀。</li>
<li><strong>本文方法变体</strong>：<ul>
<li>**Interleave-VLA (Partial)**：使用交错数据训练，但使用纯文本指令测试。</li>
<li>**Interleave-VLA (Full)**：使用交错数据训练，并使用交错指令测试。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2505.02152v2/x1.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>图1</strong>：(b) Interleave-VLA在仿真和真机实验中相比纯文本VLA模型实现了2倍的域外泛化提升。(c) 它支持使用裁剪图像、网络照片和手绘草图进行灵活、零样本的指令跟随。</p>
</blockquote>
<p>仿真结果（对应论文表1）显示，在域内任务上，Interleave-VLA (Full) 与 Text-VLA (π₀) 性能相当（71.0% vs 69.2%），证明其能有效理解指令。在域外语义泛化任务上，Interleave-VLA (Full) 表现显著优于基线：在“新物体”任务上达到55.7%（vs Text-VLA的30.2%），在“新类别”任务上达到53.0%（vs 21.0%），平均提升约2倍。即使仅使用交错数据训练而用文本测试的Partial版本，也优于纯Text-VLA。</p>
<p><img src="https://arxiv.org/html/2505.02152v2/x4.png" alt="泛化设置"></p>
<blockquote>
<p><strong>图4</strong>：左：SIMPLER中的泛化设置图示。(a)视觉泛化，(b)已知类别的新物体语义泛化，(c)全新类别的语义泛化。右：真实世界泛化实验设置。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.02152v2/x5.png" alt="注意力幻觉分析"></p>
<blockquote>
<p><strong>图5</strong>：对Interleave-VLA性能提升的定性分析。在域外任务中，Text-VLA表现出三种注意力幻觉：(1)注意力泄漏：目标注意力扩散到背景；(2)扩散注意力：注意力广泛分散无焦点；(3)注意力偏差：注意力集中在显眼的干扰物上。Interleave-VLA利用交错指令中的情境视觉线索，能持续关注目标物体。</p>
</blockquote>
<p>真机实验结果（对应论文表2）表明，在基于少量真机演示微调的前提下，经过交错数据集预训练的Interleave-VLA在域外物体上的成功率（Succ）和正确抓取率（Acc）远超Text-VLA。例如，在抓取“豆子/柠檬”的域外任务中，Interleave-VLA平均成功率达71%，准确率100%，而Text-VLA仅13%和25%。</p>
<p><img src="https://arxiv.org/html/2505.02152v2/x8.png" alt="VIMA-Bench结果"></p>
<blockquote>
<p><strong>图6</strong>：VIMA-Bench在四个泛化级别上的结果。将Interleave-VLA范式扩展到OpenVLA模型后，其性能在所有难度级别上均约为纯文本OpenVLA的2倍，展示了该范式的可扩展性和优越泛化能力。</p>
</blockquote>
<p><strong>消融实验与归因分析</strong>：<br>论文将Interleave-VLA的优越性能归因于其缓解了Text-VLA中常见的<strong>注意力幻觉</strong>现象，该现象源于文本指令的语义模糊性和训练数据分布偏差。交错指令通过提供<strong>情境视觉定位</strong>缓解了模糊性导致的幻觉，而<strong>模态多样性</strong>（如交错数据、网络图像）则缓解了分布偏差导致的幻觉。跨模态训练（Interleave-VLA Partial）即使在纯文本评估中也带来了性能增益，证明了其有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>Interleave-VLA</strong>，一种轻量、可迁移的范式，通过引入交错图像-文本指令，显著增强了当前VLA模型的泛化能力，在域外新物体任务上实现约2倍提升，并解锁了对如手绘草图等多样化视觉指令的零样本理解能力。</li>
<li>开源了基于Open X-Embodiment构建的大规模真实世界<strong>交错具身数据集</strong>，包含21万条轨迹、1300万帧，由全自动流水线生成。</li>
<li>深入分析了Interleave-VLA的有效性，将其归因于缓解了Text-VLA中因语义模糊和分布偏差引起的<strong>注意力幻觉</strong>（注意力偏差、扩散注意力、注意力泄漏）。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确陈述具体的局限性，但根据其方法，潜在的挑战可能包括：对自动生成的数据集质量的依赖；在处理极其模糊或低质量的用户提供图像时可能面临的挑战；以及交错指令可能增加的人机交互前端设计复杂性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>指令设计</strong>：为机器人提供多模态、交错式的指令是提升其泛化能力和人机交互自然度的有效途径。</li>
<li><strong>数据构建</strong>：利用自动化工具和现有大规模数据集，可以高效地生成所需格式的新数据集，解决数据稀缺问题。</li>
<li><strong>模型升级</strong>：通过轻量化的适配方式（如修改分词器）来赋予现有模型新能力，是一种高效且通用的技术路径。</li>
<li><strong>理解模型行为</strong>：从“注意力幻觉”等角度深入分析模型失败原因，能为算法改进提供明确方向。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Interleave-VLA，以解决纯文本指令在机器人操作任务中泛化能力不足的问题。该方法首次引入交错式图像-文本指令范式，通过扩展现有视觉-语言-动作模型，并构建包含21万条真实机器人演示的大规模交错数据集，实现对连续动作序列的直接生成。实验表明，该方法在模拟和真实环境中均显著提升泛化性能：对未见物体的泛化能力较纯文本基线提升2倍，并能零样本支持手绘草图等多样化指令接口。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.02152" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>