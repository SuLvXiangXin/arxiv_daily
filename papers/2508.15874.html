<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.15874" target="_blank" rel="noreferrer">2508.15874</a></span>
        <span>作者: Wenwu Zhu Team</span>
        <span>日期: 2025-08-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，结合高层规划和低层动作的分层具身模型在机器人操作中取得了显著成功。其中，基于视觉的方法（视觉运动机器人操作）通过利用丰富的视觉线索，实现了跨不同机器人本体和环境的有效跨层级信息传递。具体而言，这类方法通常采用生成模型来预测未来视频轨迹，随后通过动作预测模块将其转化为可执行动作。然而，现有框架普遍缺乏空间感知能力，导致在复杂环境中难以将视觉计划有效地桥接为可执行动作。例如，AVDC在其视频生成过程中缺乏明确的空间建模，可能产生物理上不合理的预测（如机械臂穿过墙壁）；CLOVER的反馈模块虽然增强了动态环境适应性，但其无法处理空间关系，阻碍了对目标被遮挡场景的处理。</p>
<p>本文针对上述“视觉计划与可执行动作之间因缺乏空间感知而脱节”的具体痛点，提出了一个通过显式空间建模与推理实现空间感知的视觉运动机器人操作新视角。其核心思路是：通过一个结构化的空间计划表来统一空间建模和推理，以此指导生成空间一致的视频预测、实现空间一致的动作预测，并支持执行过程中的反馈驱动精修。</p>
<h2 id="方法详解">方法详解</h2>
<p>SP是一个集成了空间感知视频生成、动作预测和反馈的机器人操作统一框架。其整体流程（pipeline）为：首先，基于初始观测和任务描述，利用视觉语言模型（VLM）从机器人末端执行器与目标物体的空间偏移中推导出结构化的空间计划表；接着，该计划表作为全局条件，指导一个条件扩散模型生成未来的视频轨迹；然后，一个基于流的扩散策略模型，结合当前帧、目标帧、光流和机器人空间坐标，预测出可执行的动作序列；最后，在执行过程中，一个双阶段反馈策略根据VLM的视频一致性检查和基于规则的策略诊断结果，动态地重新计算空间偏移并更新空间计划表，从而触发视频和动作的重规划，形成闭环控制。</p>
<p><img src="https://arxiv.org/html/2508.15874v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Spatial Policy (SP) 框架概览。系统包含三个模块：(1) <strong>空间条件化具身视频生成</strong>：利用通过VLM对机器人-物体空间偏移进行推理得到的空间计划表，指导扩散模型生成空间一致的视频预测。(2) <strong>基于流的动作预测</strong>：利用光流和空间坐标将生成的视频转换为可执行动作。(3) <strong>空间推理反馈策略</strong>：通过结合VLM视频判断和策略诊断的双阶段重规划策略实现实时校正，以精修空间计划表，实现闭环控制。</p>
</blockquote>
<p><strong>核心模块一：空间条件化具身视频生成</strong><br>该模块旨在生成空间接地的视觉轨迹。其输入是初始观测帧和文本任务描述，输出是表示高层视觉计划的未来帧序列。关键创新在于引入了<strong>空间计划表</strong>作为辅助全局条件。该表通过提取仿真器中机器人末端执行器坐标 $\mathbf{p}<em>{\text{ee}}$ 和目标物体坐标 $\mathbf{p}</em>{\text{obj}}$，计算相对偏移 $\Delta\mathbf{p}$，并输入预训练的VLM（GPT-4o）生成。计划表由一系列子目标构成，每个子目标编码了原子动作类型、方向向量和距离标量。这些信息被分别嵌入并拼接，再与通过CLIP获取的任务文本嵌入结合，形成复合全局条件 $\mathcal{T}$，通过条件归一化机制注入到U-Net的所有层中，指导扩散模型生成与指令对齐且空间接地的视频序列。生成目标为近似后验分布 $p(\mathbf{I}<em>{1:T} \mid \mathbf{I}</em>{0}, \mathcal{T})$，采用基于均方误差（MSE）的扩散损失进行训练。</p>
<p><strong>核心模块二：基于流的动作预测</strong><br>该模块将视觉预测转化为可执行动作。其输入包括：当前图像 $\mathbf{I}<em>{\text{cur}}$、从生成的视觉计划中采样的目标图像 $\mathbf{I}</em>{\text{goal}}$、机械臂空间坐标 $\mathbf{p}<em>{\text{cur}}$ 以及使用Farnebäck方法计算的连续帧之间的密集光流场 $\mathbf{F}</em>{t}$。所有输入被编码为特征向量并拼接为全局条件。动作预测被建模为一个条件生成过程，采用基于U-Net的扩散策略架构来建模动作轨迹的条件分布 $p_{\theta}(\mathbf{a}<em>{0:H-1} \mid \mathbf{I}</em>{\text{cur}}, \mathbf{I}<em>{\text{goal}}, \mathbf{p}</em>{\text{cur}}, \mathbf{F}_{t})$，同样使用去噪分数匹配和MSE损失进行训练。在推理时，模型通过一次去噪过程预测整个动作轨迹，并逐步执行。</p>
<p><strong>核心模块三：空间推理反馈策略</strong><br>为确保执行过程中的空间一致性，该模块引入了双阶段反馈机制，实现闭环校正。1) <strong>视频验证阶段</strong>：使用预训练的VLM（GPT-4o）检查生成视频的空间合理性（如物体-目标对齐、机械臂轨迹），只有通过检查的视频才会用于下游控制，否则触发在同一空间条件下的重新生成。2) <strong>动作执行阶段</strong>：采用基于规则的策略诊断来监控时间进度并检测故障（如位置漂移、行为停滞）。一旦触发，系统会根据最新观测重新计算机器人末端与目标物体的空间偏移，并通过VLM将其重新编码为精修的空间子计划，从而更新条件并触发视频和动作策略的重新规划。</p>
<p>与现有方法相比，SP的创新点具体体现在：1) <strong>显式的结构化空间建模</strong>：通过空间计划表将几何关系转化为可注入生成模型的结构化条件，而非仅依赖文本或原始图像特征。2) <strong>流与空间坐标融合的动作预测</strong>：将光流代表的运动动态与绝对空间坐标结合，使动作预测更具空间一致性和物理合理性。3) <strong>闭环的空间推理反馈</strong>：将VLM的语义级空间判断与规则化的执行诊断结合，实现了从“想象”到“执行”全流程的动态空间纠偏。</p>
<p><img src="https://arxiv.org/html/2508.15874v2/x4.png" alt="重规划示例"></p>
<blockquote>
<p><strong>图4</strong>：Meta-World操作中迭代空间重规划示例。展示了系统在执行过程中根据反馈动态调整空间计划（子目标）的过程。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境Meta-World（11个单臂操作任务）和iTHOR（4个房间的12个导航任务）上进行评估。每个任务使用多个随机种子和摄像机视角，在Meta-World上产生超过800次 rollout。</p>
<p><strong>对比基线</strong>：包括AVDC（基础视频生成+流反推动作）、AVDC-Replan（带重规划的AVDC）、VideoAgent（带自监督一致性规则的视频精修）、VideoAgent-online-Replan（在线更新视频的VideoAgent），以及本文的SP和SP-Replan。</p>
<p><strong>关键实验结果</strong>：<br>在Meta-World上（表1），SP-Replan取得了86.7%的平均成功率，显著优于所有基线（AVDC-R: 43.1%, VA-OR: 53.7%）。在具有挑战性的任务子集（如装配、锤击）上，SP达到77.5%的成功率，而最佳基线仅为29.2%，相对提升165.4%。在iTHOR上（表2），SP平均成功率为59.6%，高于AVDC的31.3%和VideoAgent的34.2%，在厨房任务中达到88.3%。</p>
<p><img src="https://arxiv.org/html/2508.15874v2/x2.png" alt="Meta-World定性对比"></p>
<blockquote>
<p><strong>图2</strong>：在Meta-World的Shelf Place和Basketball任务上与基线的定性对比。SP（右列）能生成适应性强、空间连贯的轨迹，并在关键交互阶段（绿色框）分配更多帧，实现更平滑的操作；而基线（左列）的轨迹僵化，常错过关键交互时刻。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.15874v2/x3.png" alt="iTHOR定性对比"></p>
<blockquote>
<p><strong>图3</strong>：在iTHOR的Painting和Mirror任务上的定性对比。基线轨迹（上排）因扩散先验无法保持稳定空间对齐而漂移；SP（下排）通过空间计划保持全局布局一致，即使在视角旋转下也能持续定位目标（边界框标记）。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ol>
<li><strong>视频生成策略</strong>（表3）：仅用文本和初始帧的基线（AVDC）成功率63.4%；添加原始空间偏移（Distance）提升至79.4%；进一步使用VLM生成结构化子计划（Subplan）注入，达到82.0%，证明了结构化空间抽象的重要性。</li>
<li><strong>动作预测模块</strong>（表4）：AVDC直接将光流映射到目标坐标的控制器成功率19.6%；本文的基于流扩散策略达到63.4%，尤其在中等及以上难度任务中优势明显。</li>
<li><strong>空间条件注入策略</strong>（表5）：将空间计划表全局注入所有U-Net层（Global Cond）成功率82.0%；采用GLIGEN风格的局部注入（仅在中间层通过交叉注意力注入，Local Cond）提升至84.3%，表明局部空间调制能提高视频合成的空间精度。</li>
<li><strong>重规划机制</strong>（表6）：无重规划（NR）成功率较低；仅在视频生成阶段重规划（GR）有中等提升；在视频生成和执行阶段均进行重规划（ER）取得了最高的86.7%平均成功率，证明了双阶段重规划对长视野任务鲁棒性的关键作用。</li>
</ol>
<p><strong>现实世界验证</strong>：<br>在真实机器人SO-100上执行“抓取娃娃”任务（图5）。由于缺乏真实深度，使用DepthAnything进行深度估计。在20次试验中取得了30%的成功率，验证了方法在部分观测和深度估计噪声下的实际可行性。</p>
<p><img src="https://arxiv.org/html/2508.15874v2/x5.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图5</strong>：真实世界抓取娃娃任务的执行过程。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了空间感知的视觉运动策略框架</strong>：首次通过显式的结构化空间计划表，统一了视觉运动操作中的空间建模与推理，有效桥接了视觉想象与可执行动作。</li>
<li><strong>模块化设计实现了空间建模与推理的闭环</strong>：三个协同模块（空间条件化视频生成、基于流的动作预测、空间推理反馈策略）共同构成了一个从感知、规划到执行、反馈的完整闭环系统。</li>
<li><strong>全面的评估与卓越的性能</strong>：在两大主流模拟基准（Meta-World, iTHOR）的23个任务上大幅超越现有方法（提升超33%和25%），并通过真实世界实验初步验证了其实际可行性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，方法在一定程度上依赖于外部预训练的VLM（GPT-4o）进行空间计划生成和视频验证，其性能和可靠性会影响系统表现。此外，在现实世界中需要依赖单目深度估计，这引入了额外的误差源。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>空间结构化表示是关键</strong>：将几何关系转化为机器可理解、模型可嵌入的结构化条件（如空间计划表），是提升视觉运动策略空间一致性和任务精度的有效途径。</li>
<li><strong>闭环反馈不可或缺</strong>：对于长视野、动态或存在遮挡的复杂任务，将高层语义推理（VLM）与低层执行诊断结合的闭环反馈机制，是保证策略鲁棒性的重要手段。</li>
<li><strong>性能与效率的权衡</strong>：虽然SP取得了性能突破，但其流程涉及VLM推理、扩散模型生成等多个步骤，存在计算开销。未来研究可探索更轻量化的空间表示学习或更高效的规划-执行耦合方式。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉运动机器人操作框架缺乏空间感知能力，难以在复杂环境中将视觉计划桥接到可执行动作的问题，提出了Spatial Policy (SP)框架。该框架通过显式空间建模和推理，核心方法包括：空间条件化具身视频生成模块（利用空间计划表建模空间引导预测）、基于流的动作预测模块（协调推断可执行动作）以及空间推理反馈策略（通过双阶段重新规划细化空间计划表）。实验表明，SP在Meta-World和iTHOR基准上分别实现了超过33%和25%的性能提升，在23个具身控制任务中表现优异，并验证了真实世界的实用性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.15874" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>