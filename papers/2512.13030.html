<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Motus: A Unified Latent Action World Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Motus: A Unified Latent Action World Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.13030" target="_blank" rel="noreferrer">2512.13030</a></span>
        <span>作者: Bi, Hongzhe, Tan, Hengkai, Xie, Shenghao, Wang, Zeyuan, Huang, Shuhe, Liu, Haitian, Zhao, Ruowen, Feng, Yao, Xiang, Chendong, Rong, Yinze, Zhao, Hongyan, Liu, Hanyu, Su, Zhizhong, Ma, Lei, Su, Hang, Zhu, Jun</span>
        <span>日期: 2025/12/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前具体智能体领域的主流方法呈现碎片化，主要分为三类：基于视觉-语言-动作模型（VLA）学习从观察和指令到动作的静态策略；基于世界模型或生成模型预测未来状态；以及像ℱ1这样结合VLA和逆动力学模型（IDM）的方法。这些方法将本应统一的系统能力分割为五个独立的建模任务：VLA、世界模型（WM）、IDM、视频生成模型（VGM）以及视频-动作联合预测模型。现有方法存在两大关键局限：第一，难以在一个框架内统一这些多模态生成能力，已有的统一世界模型（UWM）通常从头训练或仅利用有限先验，缺乏从大规模视觉语言模型（VLM）获得的强大视觉语言理解能力，或从VGM获得的丰富物理交互知识；第二，难以有效利用大规模异构数据（如网络视频、人类第一视角演示、多机器人轨迹），因为不同具身的动作空间差异巨大，且多数视频数据缺乏动作标签，阻碍了动作专家学习通用运动和交互先验。</p>
<p>本文针对上述痛点，旨在构建一个统一的具体智能体基础模型。其核心思路是：1）提出一个基于专家混合Transformer（MoT）的架构，整合预训练的理解、视频生成和动作专家，实现五种关键分布的统一建模；2）引入基于光流的潜在动作表示，以利用大规模无标签视频数据，并通过三阶段训练流程和六层数据金字塔方案，实现跨具身的运动知识迁移与共享。</p>
<h2 id="方法详解">方法详解</h2>
<p>Motus的整体目标是将五种建模范式（VLA, WM, IDM, VGM, Video-Action Joint Prediction）统一到一个模型中。其核心架构是专家混合Transformer，集成了三个预训练的专家模型：一个视频生成专家（如Wan）、一个动作专家（新构建的Transformer块）和一个理解专家（如Qwen3-VL）。这些专家通过共享的多头自注意力层（Tri-model Joint Attention）连接，在保持各自专业功能的同时，实现跨模态特征融合。</p>
<p><img src="https://arxiv.org/html/2512.13030v2/x1.png" alt="方法整体架构"></p>
<blockquote>
<p><strong>图1</strong>：Motus架构。模型接收当前观察、语言指令和（可选的）未来动作作为条件输入。通过Tri-model Joint Attention机制，三个专家（理解、视频生成、动作）共享注意力层进行交互。模型输出未来的视频帧和动作序列。其中，a_t…a_{t+k}是动作，z_t…z_{t+k}是潜在动作，τ_v和τ_a分别是视频生成模型和动作专家的整流流时间步。</p>
</blockquote>
<p>模型采用基于整流流的目标进行训练，联合预测视频块和动作块。通过为视频和动作分配不同的时间步和噪声尺度，Motus实现了一个类似UniDiffuser的调度器，从而能够以统一的方式建模边缘、条件和联合分布，并在不同推理模式（如VLA、WM等）间自适应切换。</p>
<p>为了解决动作分块预测导致的视频令牌远多于动作令牌、模型易过拟合视频预测的问题，论文提出了动作密集-视频稀疏预测策略。</p>
<p><img src="https://arxiv.org/html/2512.13030v2/x2.png" alt="动作密集-视频稀疏预测"></p>
<blockquote>
<p><strong>图2</strong>：动作密集-视频稀疏预测策略。在训练和推理时，对视频帧进行下采样（例如，视频帧率设置为动作帧率的六分之一），以平衡视频令牌和动作令牌的数量，防止模型过度偏向视频预测。</p>
</blockquote>
<p>为了应对利用异构数据的挑战，Motus引入了基于光流的潜在动作。光流作为像素级的“增量动作”，是跨具身的通用运动表达。一个深度卷积变分自编码器（DC-AE）被用来重建光流并将其编码为低维的潜在动作表示，一个轻量级编码器进一步将其投影到与典型机器人动作空间尺度匹配的维度（如14维）。</p>
<p><img src="https://arxiv.org/html/2512.13030v2/x3.png" alt="潜在动作VAE"></p>
<blockquote>
<p><strong>图3</strong>：潜在动作VAE架构。输入连续帧计算光流，通过编码器-解码器结构重建光流。编码后的特征被压缩并投影为低维潜在动作。训练结合了无标签数据的自监督重建损失和有标签数据（包括任务无关数据和机器人演示）的弱动作监督损失，以对齐潜在动作与真实动作的分布。</p>
</blockquote>
<p>模型的训练遵循一个三阶段流程，对应一个六层的数据金字塔。阶段一（视频生成）：在人类视频和多机器人轨迹数据上微调VGM，学习视觉动力学。阶段二（使用潜在动作的统一训练）：冻结VLM，在整个Motus模型上使用视频、语言和潜在动作进行预训练，初始化动作专家。阶段三（有监督微调）：在目标机器人任务轨迹数据上微调整个模型，适配具体具身。</p>
<p><img src="https://arxiv.org/html/2512.13030v2/x4.png" alt="数据金字塔"></p>
<blockquote>
<p><strong>图4</strong>：具体智能体数据金字塔。将数据分为六个层级：1. 网络数据（图像-文本对）；2. 第一视角人类视频；3. 合成数据；4. 任务无关数据（机器人随机运动）；5. 多机器人任务轨迹；6. 目标机器人任务轨迹。数据量从底层到顶层递减，数据质量/相关性递增。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真（RoboTwin 2.0）和真实世界场景中进行。基线方法包括 state-of-the-art 的 π0.5 和 X-VLA。评估涵盖50个具有代表性的操作任务，并在高度随机化的场景中进行多任务训练测试。</p>
<p>在RoboTwin 2.0仿真实验中，Motus取得了显著优于基线方法的表现。如表2所示，在随机化多任务设置下，Motus的平均成功率达到了88.66%（清洁场景）和87.02%（随机场景），相比X-VLA（72.80%/72.84%）有约15个百分点的绝对提升，相比π0.5（42.98%/43.84%）有超过45个百分点的巨大提升。这证明了统一建模和引入大规模先验的有效性。</p>
<p><img src="https://arxiv.org/html/2512.13030v2/x5.png" alt="真实世界实验结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界任务性能对比。在四个真实机器人操作任务上，Motus相比基线π0.5模型，成功率提升了11%到48%不等，展示了其从仿真到真实世界的强大泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.13030v2/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融实验。实验评估了移除不同预训练阶段或组件的影响。结果表明：1）完整的三个阶段训练（S1+S2+S3）性能最佳；2）仅使用阶段三（S3，即仅目标机器人数据）效果最差，说明预训练先验至关重要；3）在阶段二使用潜在动作（S2 w/ LA）比直接使用原始动作（S2 w/o LA）表现更好，验证了潜在动作对于利用异构无标签数据学习的有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了Motus，一个首次将五种主流具体智能体建模范式（WM、IDM、VLA、VGM、视频-动作联合预测）统一于一体的基础模型，且不损害通用的多模态先验知识；2）设计了一套可扩展的机器人训练方案，包括基于光流的潜在动作、三阶段训练流程和六层数据金字塔，实现了跨具身可迁移运动知识的大规模学习；3）在仿真和真实世界实验中，Motus均显著超越了现有先进方法，证明了统一模型融合大规模通用先验和领域特定先验能够极大增强策略学习的泛化能力。</p>
<p>论文提到的局限性包括：模型计算资源需求较大，以及当前潜在动作的维度（14维）可能对高维动作空间（如灵巧手）的具身构成限制。</p>
<p>本工作对后续研究的启示包括：1）验证了通过集成现有强大预训练专家来构建统一具体智能体模型的可行性，为下一代具体AI架构提供了新思路；2）展示了光流作为通用运动表示在跨具身知识迁移中的巨大潜力；3）其系统化的数据利用策略（金字塔与三阶段训练）为如何高效整合异构、多质量数据提供了重要参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Motus，旨在解决具身智能中理解、世界建模与控制等功能模型孤立、无法统一的问题。其核心是采用混合Transformer架构集成三大专家模块，并引入类UniDiffuser调度器以灵活支持世界模型、视频生成等多种模态。模型通过光流学习像素级“增量动作”，并利用三阶段训练与六层数据金字塔进行大规模预训练。实验表明，Motus在仿真和真实场景中性能显著提升，分别超过基线方法最高达45%和48%，验证了统一建模的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.13030" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>