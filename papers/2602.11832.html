<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>JEPA-VLA: Video Predictive Embedding is Needed for VLA Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>JEPA-VLA: Video Predictive Embedding is Needed for VLA Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.11832" target="_blank" rel="noreferrer">2602.11832</a></span>
        <span>作者: Mingsheng Long Team</span>
        <span>日期: 2026-02-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于预训练视觉语言模型构建的视觉-语言-动作模型在机器人操作任务上取得了显著进展。然而，现有的VLA模型仍面临样本效率低和泛化能力有限的问题。本文认为，这些局限性与一个被忽视的组件——预训练的视觉表示——密切相关。当前VLA模型中常用的视觉表示，无论是通过语言-图像对比学习还是基于图像的自监督学习预训练的，在两方面都存在不足：1) 对环境理解不足，未能充分捕获任务相关的关键信息；2) 无法提供有效的策略先验，即关于在成功执行任务时环境如何演变的预见性知识。</p>
<p>本文提出，从视频中预训练得到的预测性嵌入，特别是V-JEPA 2，擅长灵活丢弃不可预测的环境因素并编码任务相关的时序动态，从而能有效弥补现有VLA视觉表示的关键缺陷。基于此，本文提出了JEPA-VLA，一种简单有效的方法，将预测性嵌入自适应地集成到现有的VLA模型中，以增强其对环境理解和策略先验的知识。</p>
<h2 id="方法详解">方法详解</h2>
<p>JEPA-VLA的核心思路是将预训练的V-JEPA 2编码器作为冻结模块，将其提取的视觉表示作为额外的条件信号，增强标准VLA动作模型的环境理解和动作生成能力。</p>
<p><img src="https://arxiv.org/html/2602.11832v1/x1.png" alt="视觉表示对比"></p>
<blockquote>
<p><strong>图1</strong>：VLA中常用视觉表示的对比。(a) 基于图像的自监督学习能产生精确的视觉表示，但对任务相关性不敏感，保留了任务无关的细节。(b) 语言-图像对比学习强调与指令对齐的实体和语义，但可能捕获文本未明确描述的低级任务相关信息较少。(c) 基于视频的预测学习为任务相关对象提供以状态为中心的表示，同时编码了作为策略先验的时序规律性，这很难从仅图像的预训练中获得。</p>
</blockquote>
<p>整体框架包含两个主要部分：动作模型 $\pi_{\theta}$ 和冻结的预训练V-JEPA 2编码器 $E_{\phi}$。动作模型遵循标准VLA公式，输入包括语言指令 $l$、来自N个摄像头的多视角观测 $o_t^{0:N}$ 以及机器人的本体感知状态 $s_t$，输出动作 $a_t$。V-JEPA 2编码器基于ViT主干，将一个视频片段 $o_{t-h:t}$ 映射为视觉表示 $h_t$。目标是通过融合 $h_t$ 来学习一个增强的动作模型：$a_t \sim \pi_{\theta}(a_t \mid l, o_{1:t}^{0:N}, s_t, h_t)$。</p>
<p>核心创新在于如何将V-JEPA 2的表示 $h_t$ 融合到已有的VLA模型中。本文提出了两种融合策略：</p>
<ol>
<li><strong>早期融合</strong>：对于未经过大规模机器人操作数据预训练的VLA（即策略基本从头学起），将V-JEPA 2表示作为额外的输入嵌入，与原始令牌序列拼接。这种方法轻量，仅引入一个线性投影层来对齐表示维度。</li>
<li><strong>门控融合</strong>：对于已在大规模机器人操作数据上预训练的VLA（更常见且数据高效），直接拼接会干扰预训练表示。为此，本文设计了一种受Flamingo启发的门控交叉注意力架构。原始的VLA令牌作为查询，V-JEPA 2表示作为键和值。这种门控设计允许VLA在有益时选择性地关注预测性嵌入，实现自适应集成，同时最小化对预训练知识的破坏。在实践中，为了平衡性能、内存和延迟，采用了稀疏融合方案，在解码器堆栈中以固定间隔插入门控交叉注意力层。新引入的融合层学习率被设置得远低于原始VLA参数，以确保稳定集成。</li>
</ol>
<p>与现有方法相比，JEPA-VLA的创新点在于首次系统性地论证了视频预测嵌入（V-JEPA 2）对于VLA模型在环境理解和策略先验方面的独特价值，并提出了一种即插即用、自适应的融合框架来利用这种表示，无需复杂架构修改或大量额外训练数据。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在多个基准上评估了JEPA-VLA：LIBERO（包含空间推理、物体理解、指令跟随和长视野任务四个套件）、LIBERO-plus（引入了相机视角、初始状态、语言指令、光照、背景、噪声和布局等多维度扰动）、RoboTwin2.0（双臂操作仿真基准）以及一个真实世界的拾放任务。实验平台基于PyTorch，使用了两种基础VLA架构进行实验：1) <strong>基础VLA</strong>：使用Chameleon作为VLM主干，搭配线性动作预测头；2) <strong>主流VLA</strong>：基于OpenVLA-OFT实现。</p>
<p>对比的基线方法包括不使用V-JEPA 2融合的基础VLA和OpenVLA-OFT基线，以及在部分实验中对比的WorldVLA官方结果。</p>
<p>关键实验结果如下：</p>
<ul>
<li><strong>在基础VLA设置下</strong>：如表1和表2所示，在LIBERO上，JEPA-VLA将平均成功率从61.65%提升至69.05%（+7.40%），在长视野任务上提升尤为显著（+15.2%）。在更具挑战性的LIBERO-plus上，JEPA-VLA平均成功率提升6.7%（从18.9%到25.6%），在光照和背景扰动下的性能提升达13.8%和11.7%，且性能优于使用了更多数据的官方WorldVLA模型。</li>
<li><strong>在主流VLA设置下</strong>：如表3所示，在LIBERO上，JEPA-VLA将OpenVLA-OFT基线的平均成功率从90.30%提升至96.40%（+6.10%），甚至超过了官方报告的结果（95.35%）。在RoboTwin2.0上（表4），JEPA-VLA在简单和困难（域随机化）设置下分别带来了平均18.7%和8.4%的成功率提升。</li>
<li><strong>在真实世界任务中</strong>：如表5所示，使用全部数据时，JEPA-VLA将拾放任务成功率从50%提升至80%，并在不同布局和光照条件下实现了100%的成功率。更重要的是，仅使用五分之一的数据进行训练时，JEPA-VLA仍能达到60%的成功率，并展现出良好的泛化能力，而基线模型在分布外条件下完全失败。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.11832v1/x2.png" alt="分析实验"></p>
<blockquote>
<p><strong>图2</strong>：对VLA视觉表示的分析。(a) 轨迹演示与状态定义。(b) 实验设置：冻结不同视觉编码器，训练轻量级ViT头来回归或预测环境状态。(c) 实验结果：V-JEPA 2在任务相关状态回归和预测上始终获得更低的相对损失，而在任务无关（光照/背景）回归上没有优势，表明其能更好地捕获任务相关环境状态和策略先验，同时丢弃干扰因素。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11832v1/x3.png" alt="评估基准"></p>
<blockquote>
<p><strong>图3</strong>：评估基准概览，包括(a) LIBERO, (b) LIBERO-plus, (c) RoboTwin, (d) 真实世界任务，和(e) CortexBench。</p>
</blockquote>
<p>消融实验与对比分析：</p>
<ul>
<li><strong>视觉表示对比</strong>：如表7所示，在基础VLA中直接替换视觉表示，使用DINOv2仅带来边际提升甚至负收益，使用SigLIP有适度提升，而使用V-JEPA 2则带来显著最大的性能提升（在LIBERO-Long上从25.8%提升至41.0%），验证了视频预测嵌入的有效性。</li>
<li><strong>更广泛的具身AI任务</strong>：在CortexBench的10个代表性任务上（表6），V-JEPA 2在大多数任务上超越了此前表现优异的视觉表示VC-1，表明视频预测预训练产生的表示能有效迁移到更广泛的具身决策任务中。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 明确指出了VLA模型所需的两类关键视觉知识——环境理解和策略先验，并论证了当前主流的静态图像预训练表示在这两方面均存在不足；2) 通过分析实验证明，基于视频的预测性表示（V-JEPA 2）能更精确地编码任务相关状态、丢弃任务无关因素，并有效嵌入策略先验；3) 提出了JEPA-VLA，一个简单通用的框架，通过早期融合或门控融合策略将V-JEPA 2表示集成到现有VLA中，在多个仿真和真实基准上实现了显著的性能提升，尤其在样本效率和泛化能力方面。</p>
<p>论文提到的局限性包括：方法依赖于外部预训练的V-JEPA 2编码器；门控融合策略的插入位置和超参数（如学习率）需要根据具体VLA架构进行经验性调整。</p>
<p>本工作对后续研究的启示在于：为提升VLA模型的视觉理解能力提供了一个新的、有效的方向，即利用视频预测学习来获取动态和预见性知识。未来的工作可以探索将视频预测目标直接集成到VLA的端到端训练中，或者研究其他类型的时序预测表示（如基于物理的模型）的潜力。此外，如何将这种思想扩展到更复杂的多模态交互和长视野规划场景，也是一个值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决视觉语言动作模型在机器人操控任务中样本效率低、泛化能力差的核心问题。论文指出，问题的根源在于现有模型普遍使用的视觉预训练表示（如图像自监督或语言图像对比学习）在环境理解和策略先验方面存在不足。为此，作者提出JEPA-VLA方法，其关键技术是引入基于视频的预测性嵌入（特别是V-JEPA 2），它能有效滤除环境干扰并编码任务相关的时序动态，从而为策略提供先验知识。实验表明，该方法在LIBERO、LIBERO-plus等多个基准测试及真实机器人任务上均取得了显著的性能提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.11832" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>