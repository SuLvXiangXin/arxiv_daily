<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.22653" target="_blank" rel="noreferrer">2509.22653</a></span>
        <span>作者: Hu, Chih Yao, Lin, Yang-Sen, Lee, Yuna, Su, Chih-Hai, Lee, Jie-Ying, Tsai, Shr-Ruei, Lin, Chin-Yang, Chen, Kuan-Wen, Ke, Tsung-Wei, Liu, Yu-Lun</span>
        <span>日期: 2025/09/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前无人机视觉语言导航（AVLN）的主流方法主要分为两类。一类是传统的端到端策略学习框架，通过文本和视觉编码器将语言指令与视觉观测映射为潜在表示，再由策略头转换为无人机动作。这类方法依赖于有限规模和多样性的专家演示数据进行训练，难以泛化到未见过的环境或任务指令。另一类是近期兴起的免训练方法，试图直接将视觉语言模型（VLM）转化为AVLN策略。这些现有VLM方法通常将动作预测视为文本生成任务，提示VLM以文本形式输出连续动作或预定义的离散技能。然而，这种方法存在两个明显问题：1）文本不适合表示高精度浮点数，而具身智能体需要执行细粒度动作；2）VLM并未在无人机导航数据上训练过，缺乏预测3D导航动作的能力。</p>
<p>本文针对上述痛点，提出了一个全新的视角：将AVLN的动作预测视为一个2D空间接地任务。核心思路是利用VLM强大的空间理解能力，在输入图像上迭代标注2D航点，这些航点蕴含了精确的动作信息，再结合相机参数将其转换为3D位移矢量作为无人机的动作指令，从而构建了一个无需训练、泛化性强的通用导航框架。</p>
<h2 id="方法详解">方法详解</h2>
<p>See, Point, Fly (SPF) 框架将无人机导航表述为3D空间中的迭代目标抵达过程。其整体流程是一个闭环的感知-动作循环，包含三个阶段：1）给定语言指令和当前视觉观测，使用VLM生成结构化的空间理解（2D航点和移动步长）；2）将预测的2D航点和调整后的步长转换为3D位移矢量，进而分解为可执行的低层控制指令；3）一个轻量级的反应式控制器持续更新观测，利用VLM重新规划，并以闭环方式执行动作命令。</p>
<p><img src="https://arxiv.org/html/2509.22653v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：SPF框架整体流程图。相机帧和用户指令输入一个冻结的视觉语言模型，模型返回一个包含2D航点和障碍物框的结构化JSON。一个“动作到控制”层将此输出转换为低层速度指令（偏航、油门、俯仰）来操控无人机。该循环不断重复直至任务完成。</p>
</blockquote>
<p><strong>核心模块与技术细节：</strong></p>
<ol>
<li><strong>基于VLM的障碍物感知动作规划</strong>：这是方法的第一阶段，被构建为一个结构化的视觉接地任务。VLM以当前无人机视角的相机观测和自然语言指令为条件，输出一个包含像素坐标 (u, v) 和离散深度标签 d_VLM 的航点计划 O_t。深度标签 d_VLM 代表VLM预测的沿无人机机体坐标系y轴（前向）的预期行进距离，而非传感器测量的深度。在障碍物规避模式下，VLM被进一步约束生成引导无人机朝向目标且不与检测到的物体边界框相交的航点。</li>
<li><strong>自适应行进距离缩放</strong>：由于VLM缺乏对真实世界3D几何和无人机导航直觉的精确理解，直接使用其输出的深度标签可能导致动作过于激进或不安全。因此，SPF引入了一个非线性缩放曲线（公式2），将离散深度标签 d_VLM 转换为调整后的步长 d_adj。该曲线包含全局缩放因子 s、控制非线性的参数 p 以及确保安全的最小步长 d_min，使无人机能在开阔区域采取较大步幅，在靠近目标和障碍物时执行更谨慎的小幅移动。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.22653v1/x3.png" alt="控制几何细节"></p>
<blockquote>
<p><strong>图3</strong>：VLM驱动飞行循环的控制几何细节。(a) 非线性缩放曲线将 d_VLM 转换为自适应步长 d_adj。(b) 通过针孔模型将 (u, v, d_adj) 反投影到无人机机体坐标系下的3D位移向量 (S_x, S_y, S_z)。(c) 将该向量分解为控制元指令：偏航 Δθ、俯仰 ΔPitch 和油门 ΔThrottle。</p>
</blockquote>
<ol start="3">
<li><strong>从图像空间到3D动作的策略映射</strong>：给定VLM的输出 O_t = {u, v, d_adj}，系统通过针孔相机模型将其转换为3D位移向量 (S_x, S_y, S_z)。具体计算如公式3所示，其中 α 和 β 分别是相机水平和垂直的半视场角。前向运动 S_y 与无人机机体y轴对齐。</li>
<li><strong>反应式控制循环执行</strong>：在闭环控制框架内，所需的3D位移被分解为偏航、俯仰和油门控制元指令。每个指令被转换为速度-时长对，并加入执行队列，以保持时间同步的方式发送给无人机，实现平滑、响应迅速的低延迟控制。</li>
</ol>
<p><strong>创新点</strong>：与现有VLM方法相比，SPF的核心创新在于：1) <strong>2D到3D的航点提示</strong>：将动作预测重新定义为VLM更擅长的2D空间标注任务，而非文本生成；2) <strong>自适应距离缩放</strong>：引入启发式缩放机制，使动作幅度能根据场景自适应调整，提升安全性与效率；3) <strong>完整的几何闭环</strong>：构建了从VLM的2D输出到无人机3D控制指令的完整、轻量的几何转换与执行流水线，实现了真正的端到端免训练导航。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在模拟和真实世界环境中进行。模拟使用高保真的DRL模拟器；真实世界使用DJI Tello EDU无人机平台。评估指标为成功率（SR）和完成时间。</p>
<p><strong>任务与基线</strong>：设计了6类任务（导航、避障、长视程、推理、搜索、跟随），共23个模拟任务和11个真实任务。对比的基线方法包括：TypeFly（基于GPT-4和预定义技能库）、PIVOT（VLM从候选2D航点中选择）、以及Plain VLM（VLM直接生成文本动作的消融版本）。</p>
<p><strong>关键实验结果</strong>：<br>在模拟基准测试中，SPF取得了93.9%的平均成功率，大幅超过PIVOT的28.7%和TypeFly的0.9%。特别是在避障（92% vs. 16%）、长视程（92% vs. 28%）和搜索（92% vs. 36%）等复杂任务上优势显著。</p>
<p><img src="https://arxiv.org/html/2509.22653v1/figures/qualitative/sim_obstacle.png" alt="模拟定性对比"></p>
<blockquote>
<p>**图4 (a)**：模拟环境中避障任务的飞行轨迹定性对比。SPF（绿色）成功规划出平滑的避障路径，而基线方法失败或路径不佳。</p>
</blockquote>
<p>在真实世界测试中，SPF取得了92.7%的平均成功率，而TypeFly和PIVOT分别仅为23.6%和5.5%。SPF在几乎所有任务类别上都表现最佳。</p>
<p><img src="https://arxiv.org/html/2509.22653v1/x4.png" alt="完成时间对比"></p>
<blockquote>
<p><strong>图19</strong>：五个代表性真实世界任务的完成时间对比。SPF不仅在所有任务上都取得成功（基线常失败），且完成时间更快。条形图上端为∞表示基线失败。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ol>
<li><strong>提示策略</strong>：如表2所示，采用“2D航点标注”的SPF方法，在使用Gemini 2.0 Flash模型时，成功率高达100%，而使用相同模型的“文本生成”方法（Plain VLM）成功率仅7%，“视觉提示”方法（PIVOT）为40%，证明了结构化视觉接地公式的有效性。</li>
<li><strong>VLM选择</strong>：SPF框架对多种VLM骨干网络（Gemini系列、GPT-4.1、Claude 3.7 Sonnet、Llama 4 Maverick）均表现出良好的鲁棒性，成功率在87%到100%之间，显示了其通用性。</li>
<li><strong>自适应缩放</strong>：如表3所示，与固定步长相比，自适应步长控制器在保持高成功率（5/5）的同时，显著缩短了飞行完成时间（例如，从一个任务的平均50.25秒降至35.20秒），验证了其提升导航效率的作用。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1）提出了一个先进的、免训练的AVLN框架SPF，通过将动作预测重构为2D空间接地任务，实现了对新颖场景和自由形式指令的零样本泛化；2）在DRL模拟器和真实世界基准测试中均设立了新的最高性能（SOTA），成功率分别达到93.9%和92.7%，相对之前的最佳方法有63%和82%的绝对提升；3）方法具有模型无关性，在多种不同的VLM骨干网络上均表现稳健。</p>
<p><strong>局限性</strong>：论文自身提到的局限性包括：VLM可能产生幻觉或误解；对于微小或遥远的目标，接地精度可能下降；性能对提示词的措辞较为敏感；由于VLM推理延迟（约1-3秒），对高度动态障碍物的反应性有限；VLM生成的搜索模式不一定最优。</p>
<p><strong>后续研究启示</strong>：SPF的成功表明，利用大模型的基础能力（如空间理解）并将其输出通过领域知识（如相机几何）转化为具体动作，是一条有效的机器人泛化控制路径。未来的工作可以围绕以下方向展开：提升感知鲁棒性以减轻VLM幻觉；改进接地机制（尤其是对小目标）；通过模型优化或系统设计降低延迟以提升动态反应能力；探索对VLM进行轻量微调以更好地适应导航领域；以及开发更复杂的探索策略。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出See, Point, Fly (SPF)框架，解决无人机在任意环境中基于自由形式指令的通用导航问题。现有基于视觉语言模型(VLM)的方法将动作预测视为文本生成任务，而SPF的关键创新在于将其重新定义为2D空间接地任务：利用VLM将语言指令分解为图像上的2D路径点迭代标注，结合预测距离转换为3D动作命令，并自适应调整距离以实现高效闭环控制。实验表明，SPF在DRL模拟基准上比之前最佳方法绝对提升63%，在真实世界评估中也大幅优于基线，且对不同VLM泛化性强。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.22653" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>