<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.25746" target="_blank" rel="noreferrer">2509.25746</a></span>
        <span>作者: Wang, Shuaijun, Zhou, Haoran, Xiang, Diyun, You, Yangwei</span>
        <span>日期: 2025/09/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧抓取是机器人领域长期存在的基础性挑战。传统方法通常将其分解为接近物体、抓取规划和抓取执行等多个顺序阶段。近年来，端到端方法，特别是视觉-语言-动作模型，在学习从视觉输入到抓取策略的直接映射方面显示出潜力。然而，这两种范式都存在一个关键挑战：由于感知、算法近似和现实动力学的综合误差，抓取的最终执行姿态常常偏离规划姿态。这种不准确性严重影响了长视野操作任务的整体成功率，特别是对于需要高精度的薄片或平面物体。这种“最后一英里”的抓取问题，催生了利用感官反馈补偿误差的需求。</p>
<p>触觉传感为解决此问题提供了有前景的途径。本文聚焦于抓取执行阶段，提出了TacRefineNet，这是一个仅利用多指尖触觉传感，对已知物体进行朝向任意目标姿态的精确手内姿态调整的纯触觉框架。其核心思路是：通过迭代调整基于触觉反馈的末端执行器姿态，使手中物体逐渐对齐到期望配置。</p>
<h2 id="方法详解">方法详解</h2>
<p>TacRefineNet是一个在灵巧抓取最终执行阶段操作的触觉驱动精细调整框架。上游算法引导机械手完成粗略抓取后，一旦建立接触，系统从指尖捕获高分辨率触觉信号。TacRefineNet处理这些信号，估计一个用于手腕姿态的6自由度姿态增量。随后，机械手重新张开，移动到更新后的姿态，并再次尝试抓取。这个触觉反馈循环被迭代重复，使机械手逐渐收敛到目标手内姿态。目标姿态通过一次性的人类演示提供，用户手动将物体放置在手中的期望最终姿态，记录由此产生的触觉图像作为策略网络的参考输入。</p>
<p><img src="https://arxiv.org/html/2509.25746v1/framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：TacRefineNet概述，包括（a）数据收集，（b）网络架构与训练，以及（c）现实世界部署（演示引导的目标触觉图像和抓取调整）。</p>
</blockquote>
<p>形式上，TacRefineNet策略 π 可以表示为函数：Δ𝐱 = π({𝐈<em>i^curr, 𝐈_i^target}</em>{i=1}^N, 𝐪^curr)。其中，𝐈_i^curr 和 𝐈_i^target 分别表示第 i 个指尖传感器的当前和目标触觉图像，N 是手指数量，𝐪^curr 表示手的当前关节构型，输出 Δ𝐱 ∈ ℝ^6 代表6自由度手腕姿态增量。</p>
<p><strong>核心模块一：触觉传感器与仿真</strong>。硬件采用集成在11自由度灵巧手上的压阻式触觉传感器阵列。每个指尖传感器由11×9的触元网格组成，测量法向接触力。原始触元输出被转换为触觉图像，以便使用基于视觉的编码器进行特征提取。为支持可扩展的数据生成，在MuJoCo中开发了基于物理的触觉传感器仿真，用按真实设备分辨率和间距排列的球形接触点来建模传感器，以模拟弹性交互并生成触觉响应。</p>
<p><img src="https://arxiv.org/html/2509.25746v1/sim.jpg" alt="传感器仿真"></p>
<blockquote>
<p><strong>图3</strong>：指尖触觉仿真。（a）仿真（左）和现实世界（右）中每个指尖上的触觉传感器。（b）仿真（左）和现实世界（右）中对应的触觉图像。</p>
</blockquote>
<p><strong>核心模块二：数据收集</strong>。在仿真中，通过系统探索手在其各个自由度上的姿态空间来构建数据集，采样范围受预定义上下界约束。实验中观察到，由于物体的几何约束，沿x轴的触觉图像常与沿滚动轴的图像重叠，偏航轴的图像也常与其他轴重合。为减少冗余，仅采样四个维度：俯仰、滚动、y和z。在每个采样姿态，记录手的姿态𝐏、关节位置𝐪和触觉图像𝐈，存入数据集D_sim。现实世界数据收集首先复制仿真的相同流程，为提高采样效率，增大了每个末端执行器维度的步长以减少总采样点数，然后过滤掉实践中不可行的手腕姿态，最后在真实机器人上重新采样仿真数据集中记录的可行姿态以构建现实数据集D_real。</p>
<p><img src="https://arxiv.org/html/2509.25746v1/data-process.png" alt="数据收集流程"></p>
<blockquote>
<p><strong>图4</strong>：结合仿真和现实世界传感的数据收集流程。</p>
</blockquote>
<p><strong>核心模块三：策略网络架构与训练</strong>。TacRefineNet是一个多分支策略网络。每个指尖的触觉信息作为灰度触觉图像处理，使用现成的基于视觉的编码器提取特征。为增强网络的空间感知能力，对提取的特征应用位置编码。然后融合所有手指触觉分支的特征，并进一步将灵巧手的关节信息整合到一个统一的潜在特征表示中。该融合后的潜在表示通过一个三层MLP，最终输出灵巧手手腕的姿态动作增量。</p>
<p>训练采用两种策略进行对比：策略A（仅用仿真数据训练）和策略B（用仿真数据预训练并用现实数据微调）。两者采用统一的网络架构和损失函数。为训练策略实现朝向任意用户指定目标的抓取姿态调整，采用了交叉组合训练范式：当前和目标触觉图像从数据集中随机配对，通过这种采样构建N×N的成对组合集，鼓励策略网络在不同目标姿态间泛化。网络使用预测与真实末端执行器姿态增量之间的均方误差损失进行优化。此外，在训练期间应用了数据增强方案以增强策略鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置与评估指标</strong>：实验平台包括MuJoCo仿真环境和集成了11自由度灵巧手的轮式双臂机器人平台。每个指尖装有11×9的触觉阵列，测量范围为0-255的法向接触力。评估使用三个指标：1) 调整10步后的6自由度姿态误差；2) 达到位置阈值ε_pos=0.005m和方向阈值ε_rot=0.05rad所需的步数；3) 重复5次试验的成功率。</p>
<p><strong>策略对比实验</strong>：如表II所示，在三个实验组中，策略B（仿真预训练+现实微调）在位置误差、方向误差、平均调整步数和成功率上均一致优于策略A（仅仿真训练）。在组I实验中，策略B达到了最佳位置精度1.1毫米和方向精度0.016弧度。组II和组III实验平均仅需2.0步即可收敛到阈值，而组I需5.0步。</p>
<p><img src="https://arxiv.org/html/2509.25746v1/expB-process.png" alt="调整过程示例"></p>
<blockquote>
<p><strong>图5</strong>：代表性的抓取调整过程：目标（左）、中间调整和最终触觉图像（右）。可见仅两次迭代后即达到目标手内姿态，最终触觉图像与目标几乎相同。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.25746v1/expB.png" alt="误差曲线对比"></p>
<blockquote>
<p><strong>图6</strong>：三种实验组下两种策略的位姿误差随调整步数的变化曲线。用现实触觉数据微调的策略产生持续更低的误差，且方差更小，表明稳定性提升。</p>
</blockquote>
<p><strong>任意手内姿态间性能</strong>：为进一步评估策略B，在多样化的初始和目标手内姿态下进行实验。姿态沿四个维度参数化：俯仰、滚动、y和z。对每个试验，随机选择一个维度设置初始姿态，目标姿态则选为其对称对应。结果（表III，图7）显示，所有姿态配置均实现了毫米级位置精度（最佳达2毫米），方向误差始终低于0.1弧度（最低0.009弧度），证明了学习到的策略能够在任意手内配置间执行6自由度抓取姿态调整。</p>
<p><img src="https://arxiv.org/html/2509.25746v1/expC.png" alt="多维度组合性能"></p>
<blockquote>
<p><strong>图7</strong>：不同维度组合下，平均位置和方向误差随调整步数的变化曲线。</p>
</blockquote>
<p><strong>长视野物体跟踪</strong>：在动态场景中评估方法鲁棒性。实验提供一个固定的触觉图像作为目标，同时物体的姿态和位置在整个序列中被持续扰动。如图8所示，学习到的策略能够动态调整以跟踪移动物体（先后沿偏航轴、Y轴、俯仰轴运动）并完成精确的目标姿态抓取，证明了即使在物体姿态连续变化下，该方法也能持续调整以保持期望抓取。</p>
<p><img src="https://arxiv.org/html/2509.25746v1/long.png" alt="长视野跟踪"></p>
<blockquote>
<p><strong>图8</strong>：灵巧手仅使用指尖触觉跟踪移动物体。物体依次沿偏航轴、y轴、滚动轴运动，策略相应进行动态调整以实现目标抓取姿态。</p>
</blockquote>
<p><strong>未见物体泛化</strong>：在训练期间未遇到的物体上测试训练好的策略。所选物体几何形状相似，但形状和厚度不同。如图9所示，策略在未见物体上成功完成了手内姿态调整，特别是在滚动调整上泛化良好。然而，对于其他维度（如Y轴），由于新物体的线性几何形状与训练集中的弯曲形状不同，产生模糊的触觉信号，泛化能力有限。</p>
<p><img src="https://arxiv.org/html/2509.25746v1/unseen.png" alt="未见物体泛化"></p>
<blockquote>
<p><strong>图9</strong>：左：未见过的物体。右：灵巧手以期望姿态抓取这些物体的快照。结果表明策略能泛化到新的平面物体，但某些维度上存在局限。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了首个端到端的纯触觉框架TacRefineNet，仅利用多指触觉反馈即可实现毫米级精度的6自由度手内姿态调整。2) 设计了一个多分支融合策略网络，利用多指触觉信号，无需针对每个姿态重新训练即可实现任意手内姿态调整。3) 通过结合大规模仿真数据和少量现实数据训练的策略，有效实现了仿真到现实的迁移，并展示了策略在一定程度上能泛化到相似类别内的未见物体实例。</p>
<p>论文自身提到的局限性在于：策略目前仅对单个物体有效；对于几何形状与训练集差异较大的新物体，在某些维度（如Y轴）上的泛化能力有限。</p>
<p>这项工作凸显了纯触觉驱动控制在实现高精度灵巧操作方面的潜力。对后续研究的启示是：需要探索视觉-触觉融合，以处理更多样化的物体，实现更具普适性的精细操作。仿真与真实数据结合的训练范式为解决类似机器人学习任务的仿真到现实差距提供了有效参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人抓取执行阶段的“最后一公里”问题，即因感知误差和动力学因素导致物体在手内姿态不准确，影响长时程任务成功率。提出TacRefineNet，一种仅依赖触觉的框架，通过多指指尖传感迭代调整末端执行器姿态，实现对已知物体任意目标姿态的精细对齐。关键技术包括多分支策略网络，融合多指触觉与本体感知以预测控制更新，并采用模拟数据预训练加少量真实数据微调的策略。实验表明，该方法相比纯模拟训练性能显著提升，在真实世界中仅凭触觉输入实现了毫米级抓取精度，是首个仅靠多指触觉完成任意手中姿态细化的方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.25746" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>