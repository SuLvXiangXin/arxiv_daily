<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.12693" target="_blank" rel="noreferrer">2510.12693</a></span>
        <span>作者: Chen, Hanyang, Zhao, Mark, Yang, Rui, Ma, Qinwei, Yang, Ke, Yao, Jiarui, Wang, Kangrui, Bai, Hao, Wang, Zhenhailong, Pan, Rui, Zhang, Mengchao, Barreiros, Jose, Onol, Aykut, Zhai, ChengXiang, Ji, Heng, Li, Manling, Zhang, Huan, Zhang, Tong</span>
        <span>日期: 2025/10/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将视觉语言模型（VLMs）转化为具身智能体是研究热点。性能顶尖的系统依赖于部署成本高昂的大规模模型，而较小的VLMs则缺乏完成任务所需的知识和技能。例如，在EB-ALFRED基准上，Claude-3.5-Sonnet能达到64.0%的得分，而Qwen2.5-VL-7B-Instruct仅为4.7%，凸显了小模型的局限性。现有方法通常仅依赖提示工程或单一的监督微调（SFT），而结合SFT与在线强化学习（RL）的统一框架，以及针对具身任务的系统性数据构建和RL设计，仍有待探索。</p>
<p>本文旨在弥合大小VLMs在具身任务上的性能差距，提出了**Embodied Reasoning Agent (ERA)**框架。其核心思路是：通过一个两阶段训练流程，首先向小VLM注入结构化的具身先验知识，然后通过在线RL进一步优化其策略，从而在紧凑模型上实现强大的、可泛化的具身智能。</p>
<h2 id="方法详解">方法详解</h2>
<p>ERA框架包含两个阶段：<strong>具身先验学习</strong>和<strong>在线强化学习</strong>。</p>
<p><img src="https://arxiv.org/html/2510.12693v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：(a) ERA框架概览：具身先验学习（EPL）在多样化数据源上微调以提供基础知识，在线RL进一步改进智能体。(b) ERA（即EPL+RL）将3B基础模型提升至超越GPT-4o在保留评估集上的性能。</p>
</blockquote>
<p><strong>第一阶段：具身先验学习（EPL）</strong><br>由于VLM预训练与具身任务之间存在固有差距，EPL阶段通过监督微调向小VLM注入三类先验知识，以增强其感知、推理和环境理解能力。训练损失为标准自回归语言建模损失。</p>
<ol>
<li><strong>轨迹增强先验</strong>：针对现有轨迹数据缺乏详细推理监督的问题，使用GPT-4o等强模型为轨迹的每一步生成结构化推理轨迹$z_t = {z_t^{vis}, z_t^{ref}, z_t^{plan}}$，分别对应视觉描述、历史反思和步骤级计划。对于低级别操控任务，为避免大模型视觉描述不准确，采用基于模拟器后端信息的规则方法生成真实视觉描述。</li>
<li><strong>环境锚定先验</strong>：利用同一环境内非轨迹的辅助数据，提供环境级知识监督。<ul>
<li><strong>对于EB-ALFRED（高级规划）</strong>：构建<strong>掩码动作建模</strong>（随机掩码轨迹中的一个动作，让模型预测并推理）和<strong>动作序列重排序</strong>（打乱动作序列，让模型恢复正确顺序并推理）数据集，以增强任务理解和时序依赖捕捉。</li>
<li><strong>对于EB-Manipulation（低级控制）</strong>：构建强调空间理解的<strong>绝对坐标接地</strong>（物体与3D坐标映射）、<strong>相对坐标接地</strong>（预测如“最左侧”物体的坐标）和<strong>组合接地</strong>（联合判断坐标与空间关系）数据集。</li>
</ul>
</li>
<li><strong>外部知识先验</strong>：引入环境外的大规模数据，以迁移通用推理和跨领域接地能力。<ul>
<li><strong>对于EB-ALFRED</strong>：采用文本链式推理数据集OpenO1-SFT来增强逻辑规划和推理能力。</li>
<li><strong>对于EB-Manipulation</strong>：采用多模态空间推理数据集SpaceThinker来提升视觉感知和空间理解。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.12693v1/x2.png" alt="先验学习数据"></p>
<blockquote>
<p><strong>图2</strong>：具身先验学习（EPL）示意图。EPL利用三种数据源：增强的轨迹先验、环境锚定先验和外部知识先验。</p>
</blockquote>
<p><strong>第二阶段：在线强化学习（RL）</strong><br>在EPL提供的先验基础上，采用改进的PPO流程进行在线RL训练，以解决长视野、稀疏奖励和训练不稳定等挑战。包含三个关键设计：</p>
<ol>
<li><strong>自总结以管理上下文</strong>：智能体周期性地将冗长的动作观察历史总结为简洁的文本摘要，以维持有效的上下文窗口。</li>
<li><strong>密集奖励塑形</strong>：除了稀疏的最终任务奖励，引入了<strong>子目标奖励</strong>（奖励达成中间里程碑）和<strong>行为塑形奖励</strong>（惩罚无效或危险动作），提供更丰富的学习信号。</li>
<li><strong>回合级策略优化</strong>：在每一步，不仅计算当前动作的优势值，还考虑未来多步的预期回报，进行更稳定的优势估计和策略更新。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在EmbodiedBench基准上进行评估，涵盖高级规划任务<strong>EB-ALFRED</strong>和低级控制任务<strong>EB-Manipulation</strong>。基础模型为3B参数的VLM。对比方法包括基于提示的大模型（如GPT-4o、Claude-3.5-Sonnet）和基于训练的基线（如GEA、MolmoAct等）。</p>
<p><strong>主要结果</strong>：</p>
<ul>
<li>ERA-3B在EB-ALFRED上达到平均65.2%的得分，在EB-Manipulation上达到48.3%。</li>
<li>相比GPT-4o，ERA-3B在EB-ALFRED上整体提升8.4%，在EB-Manipulation上整体提升19.4%。</li>
<li>ERA-3B超越了所有7B规模的训练基线，并展现出对未见任务的强泛化能力。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.12693v1/x3.png" alt="EB-ALFRED结果"></p>
<blockquote>
<p><strong>图3</strong>：在EB-ALFRED测试集上的总体成功率。ERA-3B超越了所有基线，包括GPT-4o和基于训练的7B模型。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12693v1/x4.png" alt="EB-Manipulation结果"></p>
<blockquote>
<p><strong>图4</strong>：在EB-Manipulation测试集上的总体成功率。ERA-3B显著优于所有对比方法。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>先验学习阶段消融</strong>：实验表明，三类先验数据对最终性能均有贡献。轨迹增强先验对高级任务提升显著；环境锚定先验对低级任务的空间理解至关重要；外部知识先验能带来稳定的额外增益。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.12693v1/x5.png" alt="先验消融"></p>
<blockquote>
<p><strong>图5</strong>：在EB-ALFRED验证集上的消融研究，显示不同先验数据组合的效果。结合所有先验（ERA (EPL)）性能最佳。</p>
</blockquote>
<ol start="2">
<li><strong>在线RL阶段消融</strong>：验证了RL三个关键设计的有效性。自总结能有效管理长序列；密集奖励塑形显著加速训练并提升最终性能；回合级优化相比步级优化带来稳定提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.12693v1/x6.png" alt="RL设计消融"></p>
<blockquote>
<p><strong>图6</strong>：在线RL阶段各设计组件的消融研究（在EB-ALFRED上）。移除任一组件（自总结、奖励塑形、回合级优化）都会导致性能下降。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个系统的两阶段训练框架（ERA），将针对具身任务的先验知识构建与在线智能体RL相结合，成功将3B小模型提升至超越顶级大模型的性能。</li>
<li>提出了一个原则性的具身先验知识分类法（轨迹增强、环境锚定、外部知识），为不同任务层级的数据构建提供了实用指导。</li>
<li>在高级规划和低级控制任务上均实现了强劲性能，并通过详尽的消融实验分析了各数据源和RL设计组件的贡献。</li>
</ol>
<p><strong>局限性</strong>：论文提到，其方法目前依赖于模拟器环境进行训练和评估，向真实物理世界的迁移能力有待进一步验证。此外，在线RL阶段需要与环境交互，仍存在一定的计算成本。</p>
<p><strong>后续启示</strong>：</p>
<ul>
<li>ERA展示了一条通过系统性的数据工程和算法设计，实现<strong>高效、轻量级具身智能</strong>的可行路径。</li>
<li>其先验知识分类法启发研究者可以更结构化地挖掘和利用多源数据来赋能小模型。</li>
<li>在线RL中针对VLM智能体的特定设计（如上下文管理、奖励设计）为后续开发更稳定、高效的具身策略优化算法提供了参考。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ERA框架，旨在解决小型视觉语言模型缺乏具身任务所需知识与技能、而大型模型部署成本高昂的问题。其核心方法包含两个阶段：1）具身先验学习，通过整合轨迹增强、环境锚定和外部知识三类先验数据进行知识蒸馏；2）在线强化学习，采用自我总结、密集奖励塑形和回合级优化以克服训练挑战。实验表明，仅3B参数的ERA模型在EB-ALFRED和EB-Manipulation任务上分别超越GPT-4o达8.4%和19.4%，并展现出优秀的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.12693" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>