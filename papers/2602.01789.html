<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RFS: Reinforcement learning with Residual flow steering for dexterous manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RFS: Reinforcement learning with Residual flow steering for dexterous manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01789" target="_blank" rel="noreferrer">2602.01789</a></span>
        <span>作者: Abhishek Gupta Team</span>
        <span>日期: 2026-02-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习已成为机器人序列决策的有效引导方法，尤其在灵巧操作等高维任务中表现出色。近期行为克隆方法进一步利用扩散模型和流匹配等表达力强的生成模型来表征多模态动作分布。然而，以此方式预训练的策略往往泛化能力有限，需要在部署时进行额外的微调以实现鲁棒性能。这种适应过程必须在保留预训练全局探索优势的同时，实现对局部执行错误的快速修正。现有的策略调制方法存在互补的局限性：残差策略学习能够实现局部动作修正，但难以引发全局行为改变；而扩散模型引导允许全局调制，但在分布外状态下提供的细粒度控制有限。</p>
<p>本文针对预训练的生成策略在微调时难以兼顾全局探索与局部修正的痛点，提出了统一策略调制的新视角。核心思路是联合优化潜在噪声分布和残差动作，通过输入调制实现全局语义适应，通过输出调制实现局部灵巧修正。</p>
<h2 id="方法详解">方法详解</h2>
<p>Residual Flow Steering (RFS) 是一个统一的策略调制框架，它结合了用于全局适应的潜在噪声引导和用于精确局部修正的残差动作。其整体流程是：给定状态 s，RFS 策略 π_RFS 输出一个潜在流变量 a0 和一个残差动作 ar；a0 用于引导预训练的流匹配基础策略 π_FM 生成基础动作 ab；最终执行动作为 ab 与 ar 之和。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Residual Flow Steering (RFS) 示意图。给定状态 s，RFS 策略 π_RFS 输出潜在流变量 w0（即文中的 a0）和残差动作 ar，二者共同引导预训练的基础流匹配策略 π_FM 产生最终动作 ab + ar。RFS 支持全局模式切换和细粒度残差修正，使策略能够超越演示数据流形。</p>
</blockquote>
<p>核心模块是统一的调制策略 π_RFS(a0, ar | s)。其创新点在于将输入调制（调整生成模型的初始噪声 a0）与输出调制（对生成的动作施加仿射修正 ar）相结合。输入调制通过 a0 = π_H(a0|s) 实现全局行为变化，输出调制通过 a = Des(s, a0, vθ) + ar 实现局部细化。这种设计使得 RFS 能够同时利用两种互补的探索形式：通过残差修正进行局部优化，以及通过潜在空间调制进行全局探索。</p>
<p>本文为灵巧操作实例化了一个从仿真到现实的完整流程。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/x2.png" alt="仿真到现实流程"></p>
<blockquote>
<p><strong>图2</strong>：仿真到现实的 RFS 流程概览。(1) 使用 VR 遥操作收集多任务演示数据，训练任务特定的流匹配基础策略。(2) 在仿真中，RFS 策略在每个基础策略之上进行微调，并蒸馏到任务特定的视觉运动策略中以改进仿真到现实的迁移。(3) 在零样本现实部署中，人类纠正动作用于修正执行失败。(4) 这些纠正后的转移数据用于在 Franka-Leap Hand 系统上对 RFS 进行离线微调，提升现实世界的抓取和抓放性能。</p>
</blockquote>
<p><strong>仿真训练</strong>：首先收集少量 VR 遥操作演示，预训练基础流匹配策略 vθ。随后使用在线 PPO 算法训练 RFS 策略 π_RFS 以优化抓取成功率和稳定性。最后，利用训练好的 RFS 策略生成具有特权状态信息的仿真演示，并通过师生框架将其蒸馏为以点云为条件的视觉运动策略。</p>
<p><strong>现实世界微调</strong>：将蒸馏后的视觉策略零样本转移到现实世界后，常因新物体或不同初始条件而失败。为此，引入一个使用少量人类纠正数据进行离线 RFS 微调的阶段。在数据收集中，基础策略执行 rollout，人类在访问的状态下提供纠正动作，从而构建数据集 𝒟_RFS = {((o,s), (a0, ar), (o‘, s’), r)}，其中 ar = a_human - ab。随后使用离线强化学习（采用 TD3+BC 算法）训练 π_RFS(a0, ar | o_pc, s_pro)。</p>
<p><strong>算法细节</strong>：评论家更新（公式10）基于组合动作 a = ab + ar 进行标准 TD 学习。演员更新（公式11）在最大化评论家值的同时，对残差动作应用行为克隆正则化：Q(o, s, â) - λ_BC || âr - ar ||^2，以稳定离线学习。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真和现实世界两个阶段进行。仿真阶段使用了六个灵巧操作任务进行评估：抓取、抓放、非抓握推至抓取、长时程码放、高精度堆叠和倾倒。使用 Apple Vision Pro AR 收集了每个任务约 400 条演示用于预训练基础策略。现实阶段在 Franka-Leap Hand 系统上进行抓取和抓放任务的零样本转移与离线微调。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/x3.png" alt="实验对象"></p>
<blockquote>
<p><strong>图3</strong>：用于灵巧抓取和抓放任务的仿真和现实物体。</p>
</blockquote>
<p>对比的基线方法分为四类：(1) 扩散/流模型 RL 微调：DPPO、ReinFlow；(2) 离线到在线 RL：IQL、AWAC、Flow Q-Learning；(3) 结合演示的 RL：RLPD、IBRL；(4) 消融实验对比：DSRL（仅潜在噪声引导）、先进的残差 RL 方法（Policy Decorator, ResiP）。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/x4.png" alt="任务示例"></p>
<blockquote>
<p><strong>图4</strong>：灵巧操作任务的代表性运行轨迹。从上到下：码放、推至抓取、码放和堆叠。</p>
</blockquote>
<p>关键仿真实验结果如表1所示。预训练的基础流匹配策略平均成功率仅为 0.250。RFS 在所有任务上均取得了最高成功率：抓取 0.899、抓放 0.939、码放 0.781、推至抓取 0.721、堆叠 0.951、倾倒 0.873，平均成功率达 0.861。与最强基线对比：DPPO 平均仅 0.178，ReinFlow 为 0.409，最强的离线到在线方法 IQL 为 0.488，仅噪声引导的 DSRL 为 0.483，先进的残差 RL 方法（如 ResiP）为 0.433。RFS 显著优于所有基线。</p>
<p>消融实验表明，仅使用残差修正的方法（如 Policy Decorator, ResiP）在需要长时程或高精度任务上表现不佳；仅使用潜在噪声引导的 DSRL 在堆叠（0.135）和倾倒（0.268）任务上失败，因其修正被限制在演示分布内。RFS 结合两者优势，实现了全面超越。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/figures/reward_summary.png" alt="奖励曲线对比"></p>
<blockquote>
<p><strong>图7</strong>：在抓取任务上，RFS 与关键基线方法的奖励曲线对比。RFS 学习更稳定、更高效，最终性能更高。</p>
</blockquote>
<p>现实世界离线微调实验表明，使用人类纠正数据进行离线 RFS 微调后，抓取成功率从零样本的约 40% 提升至 80% 以上，抓放任务成功率从约 30% 提升至超过 70%。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/figures/data_collectimg.png" alt="数据收集"></p>
<blockquote>
<p><strong>图8</strong>：现实世界数据收集设置：人类操作员观察机器人执行并提供物理纠正，记录为 (状态, 人类动作, 基础策略动作) 三元组，用于构建离线微调数据集。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了 Residual Flow Steering (RFS) 框架，通过联合调制预训练流匹配策略的输入（潜在噪声）和输出（残差动作），统一了全局探索与局部修正。2) 展示了 RFS 在仿真灵巧操作任务中高效的数据生成和在线微调能力，平均成功率高达 0.861。3) 构建了完整的仿真到现实流程，并证明了利用少量人类纠正数据进行离线 RFS 微调可显著提升现实世界部署的鲁棒性。</p>
<p>论文提到的局限性主要在于现实世界适应阶段仍需依赖人类交互提供纠正数据。这为后续研究提供了启示：如何进一步减少对昂贵人类监督的依赖，例如通过更高效的主动学习、基于模型的模拟或自我监督信号来获取修正数据；此外，该统一调制框架有望扩展到其他生成模型（如扩散模型）和更广泛的机器人学习领域。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RFS框架，解决预训练生成式策略（如流匹配模型）在灵巧操作任务中泛化不足、需高效微调的问题。其核心方法“残差流引导”通过联合优化残差动作与潜在噪声分布，实现局部修正与全局探索的互补。实验表明，RFS能在仿真和真实环境中对预训练策略进行高效微调，提升部署鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01789" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>