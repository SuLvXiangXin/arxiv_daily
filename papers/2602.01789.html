<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RFS: Reinforcement learning with Residual flow steering for dexterous manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RFS: Reinforcement learning with Residual flow steering for dexterous manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01789" target="_blank" rel="noreferrer">2602.01789</a></span>
        <span>作者: Abhishek Gupta Team</span>
        <span>日期: 2026-02-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习已成为机器人技术中引导序列决策的有效方法，即使在灵巧操作等高维任务中也表现出色。近期基于行为克隆的方法进一步利用扩散模型和流匹配等表达能力强的生成模型来表征多模态动作分布。然而，以此方式预训练的策略通常泛化能力有限，需要在部署时进行额外的微调以实现鲁棒性能。这种适应过程必须在保留预训练带来的全局探索优势的同时，实现局部执行错误的快速修正。现有方法主要分为两类：残差策略学习能够实现局部动作修正，但难以引发全局行为改变；扩散模型隐变量引导允许全局行为调制，但在分布外状态下提供的细粒度控制有限。本文针对这一互补性局限，提出了残差流引导的统一视角。本文核心思路是提出残差流引导框架，通过联合优化残差动作和隐噪声分布来引导预训练的流匹配策略，从而实现局部修正和全局探索的互补，在保留预训练策略表达能力的同时实现高效适应。</p>
<h2 id="方法详解">方法详解</h2>
<p>RFS的整体框架旨在统一输入调制和输出调制，对预训练的生成策略进行适应。给定一个状态s，RFS策略π_RFS输出一个隐流变量w0和一个残差动作ar，它们共同引导一个预训练的流匹配基础策略π_FM，产生最终动作ab + ar。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：残差流引导框架。给定状态s，RFS策略π_RFS输出一个隐流变量w0和一个残差动作ar，它们共同引导预训练的基础策略π_FM产生最终动作ab + ar。RFS实现了全局模式切换和细粒度残差修正，使策略能够超越演示数据流形。</p>
</blockquote>
<p>具体而言，RFS将预训练的流匹配策略视为一个条件速度场vθ(at, t, s)，其采样过程通过一个去噪函数a = Des(s, a0, vθ)实现，其中a0通常从标准高斯分布采样。RFS通过一个调制策略π_φ(w0, ar | s)来联合调制初始隐变量（用w0表示）并施加一个仿射残差修正。最终动作的生成过程为：ab = Des(s, w0, vθ)，a = ab + ar。因此，RFS的优化目标可以形式化为最大化期望累积奖励，其中动作由调制策略π_φ和基础策略vθ共同决定。</p>
<p>与现有方法相比，RFS的创新点在于将残差RL（仅输出调制）和扩散引导（仅输入调制）统一到一个框架中。残差RL只能进行局部加法修正，而扩散引导只能通过改变隐噪声来影响整个去噪轨迹，从而改变行为模式。RFS通过同时学习对隐变量w0和残差ar的调制，实现了互补的探索形式：通过w0的调制进行全局语义适应，通过ar进行局部灵巧修正。这使得策略既能探索演示数据流形之外的新行为，又能对基础策略的输出进行精细调整。</p>
<p>在技术实现上，RFS使用离线强化学习算法（如IQL）来优化调制策略π_φ。基础流匹配策略π_FM在人类演示数据上进行预训练并保持冻结。调制策略π_φ是一个简单的多层感知机，输出w0和ar的分布参数（如高斯分布的均值和方差）。在模拟和真实世界实验中，RFS被实例化用于多指灵巧操作任务。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了三个基准任务：1）<strong>模拟环境</strong>：在Isaac Gym中进行的多物体抓取任务，涉及一个 Allegro 手抓取不同形状和大小的物体。2）<strong>真实世界</strong>：在实体 Allegro 手上进行的单物体抓取和重新定向任务。使用的数据集包括人类演示数据集（用于预训练基础策略）以及通过机器人自主交互收集的离线数据集（用于微调）。实验平台涉及NVIDIA GPU进行模拟训练。</p>
<p>对比的基线方法包括：1) <strong>行为克隆</strong>：仅在演示数据上训练的流匹配策略。2) <strong>残差RL</strong>：在冻结的基础策略上学习加法残差。3) <strong>扩散引导</strong>：学习对扩散/流模型初始噪声的调制策略。4) <strong>从头开始的离线RL</strong>：直接在交互数据上训练策略。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/x2.png" alt="模拟结果"></p>
<blockquote>
<p><strong>图2</strong>：模拟多物体抓取任务的成功率对比。RFS在微调后达到约95%的成功率，显著高于行为克隆（<del>65%）、残差RL（</del>85%）和扩散引导（~75%）。</p>
</blockquote>
<p>关键模拟实验结果显示，在2000次环境交互的有限数据下进行微调后，RFS在复杂多物体抓取任务上达到约95%的成功率，显著优于行为克隆（<del>65%）、残差RL（</del>85%）和扩散引导（~75%）。这表明RFS能更有效地利用离线数据提升策略性能。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/x3.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图3</strong>：真实世界抓取任务的成功率。经过真实世界数据微调后，RFS实现了超过90%的抓取成功率，而仅使用模拟预训练策略的行为克隆基线成功率低于70%。</p>
</blockquote>
<p>在真实世界实验中，将在模拟中预训练的策略转移到实体 Allegro 手上。仅使用模拟预训练的策略（行为克隆）由于模拟到真实的差距，抓取成功率低于70%。使用约1小时的真实机器人交互数据对RFS进行离线微调后，抓取成功率超过90%，证明了其数据高效性和良好的sim-to-real迁移能力。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/x4.png" alt="消融研究"></p>
<blockquote>
<p><strong>图4</strong>：RFS各组件的消融实验。同时使用隐变量调制和残差调制的完整RFS性能最佳，仅使用其中任一组件性能均下降，验证了二者互补的必要性。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献。实验对比了：完整RFS、仅残差调制、仅隐变量调制。结果显示，完整RFS性能最佳。仅残差调制在需要全局行为改变的任务上表现不佳；仅隐变量调制在需要精细动作修正的任务上表现受限。这验证了联合调制设计的必要性，二者在功能上互补。</p>
<p><img src="https://arxiv.org/html/2602.01789v3/x5.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：动作轨迹定性对比。RFS（绿色）能够产生既不同于基础策略（蓝色）也不同于纯残差或纯隐变量调制的动作序列，展示了其探索新行为模式的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01789v3/x6.png" alt="学习曲线"></p>
<blockquote>
<p><strong>图6</strong>：微调阶段的学习曲线。RFS相比基线方法能更快地提升奖励，并且最终性能更高，展示了其高效的数据利用率和稳定的优化特性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了残差流引导，一个统一输入和输出调制的框架，用于高效适应预训练的流匹配策略。2) 展示了RFS在模拟灵巧操作任务上，能够通过少量离线交互数据显著提升预训练策略的性能。3) 验证了RFS在sim-to-real迁移中的有效性，通过少量真实世界数据微调即可获得高鲁棒性的策略。</p>
<p>论文提到的局限性包括：RFS的性能依赖于预训练基础策略的质量；当前框架主要针对离线RL微调，在线交互下的探索效率有待进一步研究；调制策略的架构相对简单，未来可探索更复杂的结构。</p>
<p>对后续研究的启示：RFS为生成式模仿学习策略的后续强化学习微调提供了一个通用且高效的范式。其统一视角表明，结合不同层次的策略调制可能是解锁生成模型在机器人控制中全部潜力的关键。未来工作可以探索将类似思想应用于其他生成模型架构，或研究更先进的调制策略学习算法以进一步提升适应效率和最终性能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RFS框架，用于解决基于模仿学习的预训练生成策略在部署时泛化能力有限、需高效微调的问题。核心方法为残差流引导，通过联合优化残差动作和潜在噪声分布，实现局部修正与全局探索的互补。实验表明，该方法能在仿真和真实灵巧操作任务中高效微调预训练策略，提升部署鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01789" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>