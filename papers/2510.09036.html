<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.09036" target="_blank" rel="noreferrer">2510.09036</a></span>
        <span>作者: Ziwei Wang Team</span>
        <span>日期: 2025-10-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，学习型世界模型在机器人操作领域展现出巨大潜力，可作为真实世界交互的模拟器。主流方法主要基于二维RGB视频预测，例如受VideoGPT启发的自回归方法或基于扩散模型的视频生成方法。然而，这些方法缺乏显式的三维几何和空间推理能力，难以充分捕捉物理世界的结构，导致在视觉变化和复杂物体交互下容易出错。虽然已有工作尝试使用3D高斯泼溅（3DGS）进行4D世界建模，但其效果严重依赖重建质量，计算成本高昂，且难以扩展到单目场景和多样化设置。</p>
<p>本文针对现有视频世界模型缺乏几何信息、以及3D表示方法效率低下且受限的痛点，提出了一个新颖的视角：通过整合彩色图像、深度图和机器人手臂掩码，构建一个高效、几何感知的交互式多模态世界模型。本文的核心思路是：提出一个统一的MMTokenizer将多模态输入编码为紧凑的令牌表示，并利用自回归Transformer在动作条件下预测未来的多模态场景，从而在保持高效的同时增强对物理世界的理解。</p>
<h2 id="方法详解">方法详解</h2>
<p>iMoWM的整体框架旨在给定初始彩色图像和动作序列的条件下，自回归地生成未来的彩色图像、深度图和机器人手臂掩码。其流程如论文图2所示。</p>
<p><img src="https://arxiv.org/html/2510.09036v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：iMoWM整体框架。输入彩色图像，提取机器人手臂掩码并获取度量深度图（来自RGB-D相机或深度估计模块）。这些多模态输入通过MMTokenizer编码为离散令牌。通过注入动作条件槽令牌，自回归Transformer顺序生成未来令牌，最终解码为多模态输出。</p>
</blockquote>
<p><strong>核心模块一：MMTokenizer</strong><br>为了高效处理多模态输入并降低后续Transformer建模的计算成本，论文提出了MMTokenizer。其设计基于条件VQGAN的双编码器-解码器框架 <code>{(ℰ_c, 𝒟_c), (ℰ_d, 𝒟_d)}</code>，但进行了关键改进以适配多模态。</p>
<p><img src="https://arxiv.org/html/2510.09036v1/x3.png" alt="MMTokenizer细节"></p>
<blockquote>
<p><strong>图3</strong>：MMTokenizer概述。通过复制原始VAE的首尾层并结合不同的嵌入，来缓解不同模态间的分布差异，从而从多模态输入生成统一令牌。</p>
</blockquote>
<p>具体而言，MMTokenizer将原始编码器-解码器扩展为统一的多模态架构。为了处理RGB、深度和掩码的不同特征分布，它复制了第一个编码器层和最后一个解码器层，并引入了<strong>模态特定嵌入</strong>，以显式分类和指导每个模态的编码和解码过程。该设计使MMTokenizer能够生成适合Transformer建模的离散多模态令牌，同时保持与标准视频令牌化相当的计算效率。</p>
<p>MMTokenizer包含两种编码器-解码器对：</p>
<ol>
<li>**上下文编码器-解码器 (ℰ_c, 𝒟_c)**：用于对初始的上下文观测 <code>O_{1:T_0}</code> 进行编码和解码，生成上下文令牌 <code>Z_t^c</code>。</li>
<li>**条件编码器-解码器 (ℰ_p, 𝒟<em>p)<code>**：用于对未来的观测帧 </code>O</em>{T_0+1:T}<code>进行编码和解码，其以上下文观测为条件（通过交叉注意力实现），专注于生成动态区域的令牌</code>Z_t^d`，从而减少令牌总数。MMTokenizer的训练遵循原始VQGAN目标，包含L1重建损失、承诺损失、感知损失和对抗损失。</li>
</ol>
<p><strong>核心模块二：自回归Transformer</strong><br>经过MMTokenizer处理后，多模态视频输入被扁平化为上下文令牌序列 <code>{Z_t^c}</code> 和动态令牌序列 <code>{Z_t^d}</code>。为了注入动作条件并分隔观测，论文在每帧的令牌序列后插入一个**槽令牌 <code>[S_t]</code>**，其计算方式为 <code>[S_t] = [S] + Linear(a_t)</code>，其中 <code>[S]</code> 是统一的边界标志，<code>a_t</code> 是末端执行器的位姿（位置、方向、夹爪状态）。由此构建的完整令牌序列为 <code>X_T = (Z_1^c, [S_1], Z_2^c …, Z_{T_0}^c, [S_{T_0}], Z_{T_0+1}^d, [S_{T_0+1}], …, Z_T^d)</code>。</p>
<p>模型采用LLaMA风格的Transformer架构，并利用iVideoGPT的预训练检查点进行初始化。Transformer通过交叉熵损失进行训练，仅对动态令牌（以及用于对齐的填充空令牌 <code>[E]</code>）计算损失，以符合令牌高效设计的初衷。在推理时，未来令牌以自回归方式生成。</p>
<p><strong>下游任务应用</strong></p>
<ol>
<li><strong>模仿学习</strong>：利用iMoWM，给定初始观测和成功的动作序列，可以“展开”生成多条示范轨迹。生成的多模态输出用于通过3D Diffusion Policy训练机器人操作策略。</li>
<li><strong>强化学习</strong>：iMoWM可作为基于模型的RL框架中的模拟器。首先在真实策略收集的回放缓冲区 <code>D_real</code> 上训练世界模型 <code>P_θ</code>，然后使用 <code>P_θ</code> 在相同动作展开下生成想象回放缓冲区 <code>D_imag</code>。演员-评论家模型使用组合数据集 <code>D_real ∪ D_imag</code> 进行更新。模型还包含一个辅助奖励头来预测每一步的奖励。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/基准</strong>：BAIR机器人推动数据集、RoboNet数据集、Meta-World基准、以及一个自收集的高分辨率（256x256）真实世界数据集。</li>
<li><strong>对比基线</strong>：MaskViT, SVG, GHVAE, iVideoGPT, GWM。</li>
<li><strong>评估指标</strong>：视频质量（FVD↓, PSNR↑, SSIM↑, LPIPS↓）、深度精度（AbsRel↓）、机器人操作任务成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>动作条件多模态视频生成</strong>：在BAIR和RoboNet数据集上的定量结果（表I）显示，iMoWM在大多数视觉质量和深度精度指标上优于所有基线。例如，在BAIR上，iMoWM的FVD为60.9（iVideoGPT为65.01），AbsRel为0.045（iVideoGPT为0.059）。在高分辨率真实数据上（表II），iMoWM也显著优于iVideoGPT。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09036v1/x4.png" alt="BAIR数据集对比"></p>
<blockquote>
<p><strong>图4</strong>：在BAIR数据集上的多模态视频生成定性对比。iMoWM相比iVideoGPT能生成质量更好、几何信息更丰富的未来帧。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.09036v1/x5.png" alt="真实数据对比"></p>
<blockquote>
<p><strong>图5</strong>：在真实世界高分辨率数据上的定性结果。iMoWM在可控的交互式多模态视频生成中表现出优越性。</p>
</blockquote>
<ol start="2">
<li><strong>视觉基于模型的强化学习</strong>：在六个Meta-World任务上的结果（图6）表明，iMoWM相比iVideoGPT和GWM收敛更快，且最终成功率更高，证明了其几何感知的展开对于策略学习的优势。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09036v1/x6.png" alt="MBRL结果"></p>
<blockquote>
<p><strong>图6</strong>：在Meta-World上的基于模型强化学习结果。iMoWM在收敛速度和最终成功率上均优于基线。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界模仿学习</strong>：部署实验（表III）显示，使用iMoWM生成的数据训练的策略，其任务成功率（29/35）与使用真实数据训练的（27/35）相当，并显著优于使用iVideoGPT数据训练的（13/35），证明了iMoWM生成数据的高保真度。</li>
</ol>
<p><strong>消融实验分析</strong>：<br>在BAIR数据集上的消融实验（表IV）证明了所提组件的有效性：</p>
<ul>
<li><strong>MMTokenizer vs. 原始Tokenizer</strong>：使用MMTokenizer将推理时间从860秒大幅减少到10秒，且各项指标（FVD从739.2降至67.6）有巨大提升，证明了其高效性和必要性。</li>
<li><strong>多模态贡献</strong>：仅使用RGB性能最差；加入掩码或深度信息都能提升性能；同时使用所有模态（RGB+深度+掩码）取得了最佳结果，说明每个模态都通过提供几何信息或动态注意力增强了整体性能。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了iMoWM，一个新颖的交互式多模态世界模型，能够以动作条件自回归地预测未来的彩色图像、深度图和机器人手臂掩码。</li>
<li>设计了高效的MMTokenizer，将多模态输入统一编码为紧凑的令牌表示，使得模型能够利用大规模预训练视频GPT模型，同时融入丰富的物理信息。</li>
<li>通过大量实验证明，iMoWM不仅在多模态视频生成质量上达到先进水平，而且能有效提升下游机器人模仿学习和基于模型的强化学习的性能。</li>
</ol>
<p><strong>局限性</strong>：论文指出，为了实现iMoWM在各种机器人手臂和动作条件下的泛化能力，<strong>大规模预训练仍然是必要的</strong>。</p>
<p><strong>启示</strong>：</p>
<ol>
<li><strong>几何信息集成</strong>：将显式的几何信息（如深度）融入世界模型，是提升其对物理世界理解能力和下游任务性能的有效途径。</li>
<li><strong>高效多模态表示</strong>：通过统一的令牌化方案处理多模态数据，可以在不显著增加计算开销的前提下，丰富模型的感知和推理能力，为构建更强大的具身AI模型提供了思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出iMoWM模型，旨在解决现有基于2D视频的世界模型在机器人操作中缺乏几何和空间推理能力的问题。通过MMTokenizer技术将多模态输入（彩色图像、深度图、机器人手臂掩码）统一为紧凑令牌表示，iMoWM以自回归方式生成多模态输出，条件于动作序列，从而高效利用预训练VideoGPT模型并融入丰富物理信息。实验表明，iMoWM在模型基于强化学习和真实世界模仿学习中表现优越，提升了预测视觉质量和任务适用性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.09036" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>