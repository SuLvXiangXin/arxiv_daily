<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Humanoid Arm Motion via Centroidal Momentum Regularized Multi-Agent Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Learning Humanoid Arm Motion via Centroidal Momentum Regularized Multi-Agent Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.04140" target="_blank" rel="noreferrer">2507.04140</a></span>
        <span>作者: Lee, Ho Jae, Jeon, Se Hwan, Kim, Sangbae</span>
        <span>日期: 2025/07/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，人形机器人全身运动控制的主流方法主要包括基于模仿学习的方法、基于强化学习（RL）的方法和基于模型的控制方法。基于模仿的方法试图匹配参考数据的分布，从而产生类人行为，但其主要奖励信号是模仿高自由度系统的轨迹，这可能与特定平台的鲁棒性或性能目标不一致。基于RL的方法在同时规划手臂和腿部运动时，常面临奖励冲突的挑战，手臂运动通常依赖缺乏物理基础的启发式正则化进行风格调整。基于模型的方法，如质心动量跟踪或全身模型预测控制（MPC），虽然能以物理一致的方式自然产生手臂运动，但求解这些全身运动规划的计算开销巨大，难以扩展到在线闭环控制，且易受自碰撞、状态估计噪声等因素影响。</p>
<p>本文针对“如何有效且有意地协调人形机器人在运动过程中的手臂动作”这一具体痛点，从生物学中手臂与腿部控制分离的架构获得灵感，提出了一个新的视角：将协调问题视为一个多智能体强化学习问题。本文的核心思路是：将手臂和腿部视为独立的智能体，通过质心角动量（CAM）奖励进行正则化，使手臂动作能够自然地响应腿部运动产生的动量，从而增强全身运动的稳定性和鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的整体框架是一个肢体级的多智能体强化学习框架，其核心是将人形机器人的手臂和腿部分别视为独立的智能体（Agent），并采用集中训练、分散执行（CTDE）的范式进行训练。</p>
<p><img src="https://arxiv.org/html/2507.04140v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：本文提出的肢体级多智能体强化学习框架概览。手臂和腿部被分配了独立的演员-评论家策略网络，各自操作于不同的观测空间和奖励结构。训练时，每个评论家（Critic）接收全局信息以进行集中式训练；执行时，演员（Actor）仅使用局部观测，并共享基础状态和质心角动量（CAM）。这种设计通过共享的CAM信号隐式耦合手臂和腿部智能体，使手臂能够响应腿部运动产生的动量，从而实现协调的全身行为。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>智能体划分与网络结构</strong>：框架包含两个智能体：手臂智能体和腿部智能体。每个智能体拥有独立的演员网络（策略网络）和评论家网络（价值网络）。演员网络采用分散式执行，仅接收各自肢体的局部观测；评论家网络在训练时采用集中式，能够访问两个智能体的全局状态和动作信息，以稳定训练过程。训练算法采用多智能体近端策略优化（MAPPO）。</li>
<li><strong>观测空间</strong>：<ul>
<li><strong>手臂智能体观测空间（ℝ⁴¹）</strong>：包括基础高度、朝向、线速度和角速度（基坐标系）、重力矢量在基坐标系的投影、手臂关节位置与速度、上一时刻的手臂动作，以及<strong>当前与目标质心角动量（CAM）</strong>。其中CAM的x、y分量在基坐标系下表示，z分量在世界坐标系下表示。</li>
<li><strong>腿部智能体观测空间（ℝ⁵⁵）</strong>：包括基础状态、腿部关节状态、上一时刻的腿部动作、用户指令（期望基座速度）以及脚步计时器信息。</li>
<li><strong>评论家全局观测</strong>：手臂评论家接收ℝ⁷⁶的全局观测，包含手臂和腿部的完整关节状态、所有基础信息以及双方的动作。</li>
</ul>
</li>
<li><strong>动作空间</strong>：两个智能体的动作空间均为关节位置残差。手臂智能体输出8维手臂关节位置残差（Δ𝒒̂_arm），腿部智能体输出10维腿部关节位置残差（Δ𝒒̂_leg）。这些残差与参考关节位置𝒒_ref相加后，发送至运行在1kHz的低级关节PD控制器以计算电机扭矩。</li>
<li><strong>核心创新：CAM正则化奖励</strong>：这是引导手臂运动的关键。奖励函数包含两部分：<ul>
<li><strong>CAM跟踪奖励</strong>：鼓励手臂运动产生的CAM（𝐤_G,arms）跟踪一个由用户指令（期望基座速度）通过质心动量矩阵（CMM）映射得到的目标CAM（𝐤̂_G）。公式为：<code>r_track = exp(-α_track ||𝐤_G,arms - 𝐤̂_G||²)</code>。</li>
<li><strong>CAM阻尼奖励</strong>：直接激励减少CAM的变化率（𝐤̇_G），以增强对外部扰动的抵抗能力。公式为：<code>r_damp = exp(-α_damp ||𝐤̇_G||²)</code>。<br>此外，手臂奖励还包含关节位置、速度限制等正则化项。腿部智能体则使用一套专注于步态生成和平衡的独立奖励函数。</li>
</ul>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>模块化多智能体架构</strong>：首次将多智能体RL应用于人形机器人的肢体级控制，使手臂和腿部策略得以独立优化，避免了单一智能体中奖励信号的冲突，并借鉴了生物控制分离的思想。</li>
<li><strong>物理引导的奖励设计</strong>：摒弃了风格化的启发式正则化，创新性地引入基于生物力学原理的CAM跟踪与阻尼奖励，为手臂运动提供了明确的物理目标——调节全身角动量，从而使涌现的手臂摆动具有明确的稳定功能。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在IsaacLab仿真环境中进行，并行4096个环境。策略控制频率为100Hz。最终策略部署在一个总重24.89kg、具有18个驱动关节的真实人形机器人平台上进行硬件验证。</p>
<p><strong>对比的基线方法</strong>：</p>
<ol>
<li><strong>Single-Agent</strong>：标准的单智能体RL，使用单一策略控制所有关节。</li>
<li><strong>Multi-Agent (Ours w/o CAM)<strong>：本文的多智能体框架，但</strong>移除</strong>了手臂奖励中的CAM跟踪和阻尼项。</li>
<li>**Multi-Agent (Ours)**：完整的本文方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04140v1/x4.png" alt="训练曲线与性能对比"></p>
<blockquote>
<p><strong>图4</strong>：训练曲线与任务成功率对比。左图显示，本文方法（Multi-Agent (Ours)）在训练收敛速度和最终获得的奖励方面均优于单智能体基线。右图显示，在应对向前、向后、向左、向右的推力扰动任务中，本文方法的成功率（平均97.5%）显著高于单智能体基线（平均85%）和无CAM奖励的多智能体基线（平均92.5%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.04140v1/x5.png" alt="质心动量分析"></p>
<blockquote>
<p><strong>图5</strong>：平坦地面行走时的质心角动量（CAM）分析。本文方法（蓝色）产生的总CAM（𝐤_G）幅值远小于单智能体方法（红色）和无CAM奖励的方法（绿色），与人类行走的观测数据（黑色虚线）趋势一致，验证了CAM奖励的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.04140v1/x6.png" alt="地面反作用力矩分析"></p>
<blockquote>
<p><strong>图6</strong>：垂直地面反作用力矩（GRMz）分析。本文方法（蓝色）所需的GRMz峰值明显低于单智能体基线（红色），表明手臂摆动通过抵消角动量，减少了对地面反作用力矩的需求，使步态更高效、稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.04140v1/x7.png" alt="崎岖地形与楼梯任务"></p>
<blockquote>
<p><strong>图7</strong>：仿真中的高级运动任务。本文控制器成功完成了在随机崎岖地形上行走和爬楼梯的任务，显示了其泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.04140v1/x8.png" alt="硬件实验：扰动恢复"></p>
<blockquote>
<p><strong>图8</strong>：硬件平台上的外部扰动恢复实验。机器人受到侧向推力后，本文方法控制下的手臂产生了与仿真中类似的、对抗扰动的协调摆动，帮助机器人恢复了平衡。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.04140v1/x9.png" alt="硬件实验：复杂地形"></p>
<blockquote>
<p><strong>图9</strong>：硬件平台上的地形适应实验。机器人成功在坡度10°的斜坡、铺有鹅卵石的不平整路面以及高2.5厘米的台阶上行走。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.04140v1/x10.png" alt="消融实验：观测空间分析"></p>
<blockquote>
<p><strong>图10</strong>：观测空间选择的消融实验。比较了演员（Actor）和评论家（Critic）采用不同观测范围（局部/全局）的组合。结果显示，“Central Critic + Decentral Actor”（本文采用）在训练稳定性和最终性能上最优；“Full Central”性能接近但波动更大；“Decentral Critic”则难以有效训练。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>扰动恢复</strong>：在仿真中，本文方法对四方向推力的平均恢复成功率达到**97.5%**，显著高于单智能体基线（85%）和无CAM奖励的多智能体基线（92.5%）。</li>
<li><strong>质心动量减少</strong>：在平坦地面行走时，本文方法将质心角动量（CAM）的幅值降低了约 <strong>40-67%</strong> （与单智能体相比），更接近人类行走的数据模式。</li>
<li><strong>地面反作用力矩降低</strong>：本文方法将垂直地面反作用力矩（GRMz）的峰值降低了约 <strong>30%</strong> 。</li>
<li><strong>硬件验证</strong>：学习的策略成功迁移到真实机器人，在平坦地面、斜坡、不平整路面、台阶等多种地形上实现了鲁棒行走，并能有效抵抗外部推力扰动。</li>
</ol>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>CAM奖励的贡献</strong>：移除CAM奖励后，扰动恢复成功率下降5%，且行走时的CAM和GRMz显著增大，证明了CAM奖励对于生成具有稳定功能的手臂运动至关重要。</li>
<li><strong>CTDE架构的贡献</strong>：消融实验表明，采用分散演员（局部观测）配合集中评论家（全局观测）的架构，在训练稳定性和最终策略性能上均优于演员或评论家完全分散或完全集中的其他变体。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文核心贡献</strong>：</p>
<ol>
<li>提出了一种基于生物力学原理的<strong>质心角动量（CAM）奖励</strong>，成功引导RL策略涌现出能有效减少全身角动量、降低地面反作用力矩的自然手臂摆动，增强了运动稳定性。</li>
<li>提出了一个<strong>肢体级的多智能体强化学习框架</strong>，将手臂和腿部控制分离为独立智能体，采用CTDE范式训练，实现了模块化、高效且协调的全身控制。</li>
<li>在仿真和<strong>真实人形机器人硬件</strong>上全面验证了控制器的有效性，展示了其在平坦地面行走、抗扰动以及复杂地形适应方面的鲁棒性能。</li>
</ol>
<p><strong>论文提到的局限性</strong>：论文在讨论部分指出，当前工作主要关注于利用手臂进行平衡和动量调节。对于需要手臂进行主动操作（如搬运、推门等）的“运动-操作”任务，本文的框架和奖励设计可能需要进一步的扩展和调整。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>模块化控制范式</strong>：将复杂机器人系统分解为功能子系统（智能体）的思路，可以扩展到其他多肢体机器人（如四足机器人、双足机器人等），以简化策略学习并提升性能。</li>
<li><strong>物理引导的奖励设计</strong>：证明了将领域知识（如生物力学原理）通过精心设计的奖励函数融入RL过程，能够引导出具有明确物理意义和优越性能的行为，这为其他机器人控制问题提供了奖励设计的新思路。</li>
<li><strong>迈向全身协同作业</strong>：本文框架为更复杂的全身协同任务（如locomotion + manipulation）奠定了基础。未来研究可以探索如何在多智能体框架中，动态地调整或切换不同肢体的任务目标（如从“摆动平衡”切换到“抓取操作”）。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在运动中难以有效协调手臂动作以维持平衡的问题，提出了一种基于质心动量正则化的多智能体强化学习框架。方法采用肢级分工：手臂与腿部智能体拥有独立的策略网络（actor），但共享基座状态与质心角动量观测，并通过集中式评价网络（critic）协调训练；手臂智能体以质心动量跟踪与阻尼为奖励目标，促使其自发产生减少全身角动量的摆动。实验表明，该方法优于单智能体及其他多智能体基线，最终部署在真实人形机器人上，在平地行走、崎岖地形穿越及爬楼梯等多种任务中均表现出鲁棒性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.04140" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>