<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Humanoid Policy ~ Human Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Humanoid Policy ~ Human Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.13441" target="_blank" rel="noreferrer">2503.13441</a></span>
        <span>作者: Qiu, Ri-Zhao, Yang, Shiqi, Cheng, Xuxin, Chawla, Chaitanya, Li, Jialong, He, Tairan, Yan, Ge, Yoon, David J., Hoque, Ryan, Paulsen, Lars, Yang, Ge, Zhang, Jian, Yi, Sha, Shi, Guanya, Wang, Xiaolong</span>
        <span>日期: 2025/03/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从真实的机器人示范中进行模仿学习是机器人操作领域取得显著进展的主流方法。为了提升策略的跨任务和跨平台泛化能力，一个关键方向是合并来自不同机器人的多样化数据，训练基础策略。然而，收集结构化的真实机器人数据成本高昂、耗时费力，严重制约了模型获得类似计算机视觉或自然语言处理领域那种大规模、鲁棒且通用的能力。本文针对机器人数据收集难以扩展这一具体痛点，提出将自我中心的人类演示作为一个更具可扩展性的跨具身训练数据源。核心思路是将人形机器人和人类视为不同的具身，通过设计统一的状态-动作表示空间，并利用人类数据与少量机器人数据共同训练，从而直接模仿人类行为，无需依赖模块化感知或中间表征（如功能可供性）。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法包含两个核心部分：大规模任务导向的人类数据集PH2D，以及基于此训练的人-人形行为策略HAT。</p>
<p><strong>1. PH2D数据集收集</strong>：针对现有人类数据集要么非任务导向、要么需要专用硬件的问题，PH2D旨在收集与机器人执行直接相关的任务导向人类演示。其关键创新在于适配消费级VR设备（如Apple Vision Pro, Meta Quest 3）来便捷地收集数据。</p>
<p><img src="https://arxiv.org/html/2503.13441v3/x2.png" alt="数据收集设备"></p>
<blockquote>
<p><strong>图2</strong>：用于数据收集的消费级设备。使用Apple Vision Pro内置摄像头或搭配ZED Mini立体相机的Meta Quest 3/Apple Vision Pro，以低成本引入视觉多样性，并提供世界坐标系下的3D头部和手部姿态作为监督信号。</p>
</blockquote>
<p>数据收集流程要求人类操作者佩戴VR设备执行与机器人任务重叠的操作（如抓取、倾倒）。为了弥合人类与机器人动作之间的领域差距：1）要求数据收集者保持坐姿，减少非必要的全身运动；2）通过时间插值“减慢”人类动作速度，使其与机器人速度匹配，减慢因子α_slow经验性地设为4。</p>
<p><strong>2. HAT策略模型</strong>：HAT（Human Action Transformer）通过学习人类行为来获得机器人策略。其整体框架如图3所示。</p>
<p><img src="https://arxiv.org/html/2503.13441v3/x3.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图3</strong>：HAT方法概览。HAT通过在统一的人本状态-动作空间中预测未来手腕6自由度姿态和指尖轨迹来学习策略。在训练时，从人类或机器人数据中采样状态-动作对。图像由冻结的DinoV2编码器处理。在机器人部署时，通过逆运动学和手部重定向将预测的人类动作转换为机器人动作。</p>
</blockquote>
<p><strong>核心设计选择与技术细节</strong>：</p>
<ul>
<li><strong>统一的状态-动作空间</strong>：由于双手机器人和人类都有两个末端执行器（手），且机器人配备可旋转的颈部（类似人类头部运动），因此为两者设计统一的状态-动作表示。具体而言，本体感知状态是一个54维向量，包含头部、左手腕、右手腕的6D旋转，以及左右手腕和10个指尖的x/y/z坐标。机器人灵巧手与人类手部指尖存在双射映射。</li>
<li><strong>视觉领域差距处理</strong>：人类和机器人数据在相机传感器和末端执行器外观上存在差异。论文发现，在数据足够大且多样的情况下，无需复杂的启发式处理（如视觉伪影或生成方法），基本的图像增强（如颜色抖动、高斯模糊）即可作为有效的正则化手段。</li>
<li><strong>网络结构与训练</strong>：HAT采用基于Transformer的架构预测未来动作块。视觉主干使用冻结的DinoV2 ViT-S编码器。损失函数为L1损失，并赋予手腕末端执行器位置更高的权重（λ=2），以强调其重要性而非过度追求指尖关键点的精确度。</li>
</ul>
<p><strong>创新点</strong>：与现有利用人类视频的工作多采用模块化方法（如先预测功能可供性）不同，HAT的创新在于直接以端到端的方式将人类建模为另一种人形具身，避免了复合误差。同时，其统一的状态-动作空间设计使得策略能够无缝处理来自不同具身（人类、不同机器人）的数据。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：使用自行收集的PH2D人类数据集和机器人示范数据。在四个灵巧操作任务上进行评估：水平抓握、垂直抓握、倾倒、传递。</li>
<li><strong>实验平台</strong>：主要在Unitree H1机器人（Humanoid A）上进行，使用Unitree H1-2机器人（Humanoid B，具有不同的手臂配置）评估少样本跨人形迁移。</li>
<li><strong>对比方法</strong>：以仅使用机器人数据训练的Action Chunk Transformer (ACT) 作为基线，与共同训练人类和机器人数据的HAT进行对比。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>论文在分布内（I.D.，场景设置与机器人训练数据近似）和分布外（O.O.D.，场景设置仅出现在人类数据中）两种设置下测试。</p>
<p><img src="https://arxiv.org/html/2503.13441v3/x7.png" alt="实验结果表"></p>
<blockquote>
<p><strong>图7</strong>：自主技能执行的成功率。共同训练人类数据（H. Data）显著提高了分布外（O.O.D.）性能，在Humanoid A的所有任务上实现了近100%的相对提升。表格还消融了是否为不同具身使用不同归一化（D. Norm）的设计选择。</p>
</blockquote>
<p>从表2（图7）可知：1）在I.D.设置下，是否加入人类数据共同训练结果相似，表明少量机器人数据已能较好地拟合已知分布。2）在O.O.D.设置下，共同训练带来了巨大提升，成功率从59/170（仅机器人数据）提升至101/170（共同训练），相对提升约71%。论文进一步指出，人类数据主要提升了策略对<strong>背景、物体放置和外观</strong>三种泛化类型的鲁棒性。</p>
<p><strong>消融实验与分析</strong>：</p>
<ol>
<li><strong>少样本跨异质具身迁移</strong>：如图5所示，在仅有20条Humanoid B示范的情况下，共同训练（结合Humanoid A和人类数据）的策略显著优于仅用Humanoid B数据训练的基线。随着Humanoid B数据量增加，共同训练始终优于孤立训练。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.13441v3/x5.png" alt="少样本适应"></p>
<blockquote>
<p><strong>图5</strong>：少样本适应。(a) Humanoid B在水平抓握任务上的性能，o1在Humanoid B数据中出现过，o2和o3仅在人类数据中出现，o4在所有数据中均未出现。(b) 随着Humanoid B示范增加，共同训练始终优于孤立训练，即使在低数据区域也能获得稳健的成功率。</p>
</blockquote>
<ol start="2">
<li><strong>人类数据的采样效率</strong>：在固定20分钟数据收集时间内，对比收集60条机器人数据与收集30条机器人数据+120条人类数据两种方案。如图6所示，混合数据训练的策略在垂直抓握任务上的成功区域更广，验证了人类数据在单位时间内的采样效率更高。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.13441v3/x6.png" alt="采样效率"></p>
<blockquote>
<p><strong>图6</strong>：人类数据具有更好的采样效率。图中展示了使用纯机器人数据和混合数据训练的模型在垂直抓握任务上每网格（10cm×10cm）的成功次数（共10次尝试）。红框表示训练数据收集的位置。</p>
</blockquote>
<ol start="3">
<li><strong>状态-动作空间设计的重要性</strong>：如表3所示，若不对人类动作进行插值减速，预测动作速度会在人类速度和机器人速度间波动，导致不稳定；若不使用统一状态空间（例如对机器人使用关节位置，对人类使用末端执行器表示），策略会获得区分具身的“捷径”，导致分布外性能显著下降。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了PH2D，一个大规模、任务导向、包含精确手部姿态的自我中心人-人形数据集。</li>
<li>提出了HAT，一种通过统一状态-动作空间直接建模人类作为另一种人形具身的跨人形操作策略，实现了端到端训练。</li>
<li>通过大量实验证明，与人类数据共同训练能显著提升策略的分布外泛化能力和鲁棒性，且人类数据具有更高的采样效率。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>当前策略架构相对简单，未充分利用数据集中收集的语言指令来研究基于语言的泛化。</li>
<li>人类数据收集依赖现成VR硬件及其手部跟踪SDK，这些SDK针对VR应用训练，在严重遮挡等情况下可能跟踪失败。</li>
<li>尽管方法概念上可扩展到更多机器人形态，但目前评估仅限于配备灵巧手的机器人。</li>
</ol>
<p><strong>对后续研究的启示</strong>：<br>本文验证了将高质量、任务对齐的人类演示作为可扩展数据源用于跨具身学习的可行性。这为构建大规模机器人基础模型开辟了一条绕过昂贵机器人数据收集的潜在路径。未来的工作可以探索如何将语言指令更深度地整合到策略中，以及如何将该框架推广至具有不同末端执行器（如平行夹爪）的机器人。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何利用人类演示数据来提升人形机器人操作策略的学习效率与性能。核心问题是解决人形机器人与人类之间的“具身差距”，并提出使用可扩展的第一人称人类演示作为跨具身训练数据。关键技术包括：1) 收集了任务导向的自我中心数据集PH2D，其手部3D姿态与机器人操作直接对齐；2) 提出了人类动作变换器（HAT）策略，统一了人类与机器人的状态-动作空间，并可微分地重定向为机器人动作。实验表明，结合人类数据共同训练能显著提高HAT的泛化能力、鲁棒性及数据收集效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.13441" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>