<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01210" target="_blank" rel="noreferrer">2511.01210</a></span>
        <span>作者: Guo, Heyu, Wang, Shanmu, Ma, Ruichun, Jiang, Shiqi, Ghasempour, Yasaman, Abari, Omid, Guo, Baining, Qiu, Lili</span>
        <span>日期: 2025/11/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过利用大规模视觉-语言预训练，在机器人动作预测方面展现出强大的泛化能力。然而，绝大多数现有模型仅依赖RGB摄像头作为视觉输入，这极大地限制了它们的感知能力，进而影响了其在需要非RGB线索（如温度、穿透遮挡物、声音定位）的复杂操作任务上的表现。虽然已有工作尝试引入深度、触觉或语音信息，但尚未探索红外、毫米波雷达和声学阵列等能提供独特物理信息的新型传感模态。直接为每种传感器添加编码器或简单输入原始数据，会面临系统复杂性增加、训练数据稀缺以及与视觉-语言预训练兼容性差等挑战。</p>
<p>本文旨在解决上述痛点，提出让VLA模型能够感知并利用多传感器数据，以实现超越RGB的、基于物理的空间智能。核心思路是提出一种名为“传感器掩码图像”的统一中间表示，将异质传感器信息以空间锚定、语义对齐的掩码形式叠加到RGB图像上，从而复用预训练的视觉编码器，并提供跨传感器的统一接口，实现数据高效的学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>OmniVLA的整体框架包含两部分：传感器掩码图像的生成，以及专为该表示设计的多传感器VLA模型架构。</p>
<p><img src="https://arxiv.org/html/2511.01210v2/x2.png" alt="系统总览图"></p>
<blockquote>
<p><strong>图2</strong>：系统概述。OmniVLA将多样的传感器数据处理成类图像的2D空间表示，并将传感器信息叠加在RGB图像上，生成空间锚定且语义对齐的传感器掩码图像。随后，使用带有独立MLP传感器投影器的VLA模型进行训练，以完成需要超RGB感知的挑战性任务。</p>
</blockquote>
<p><strong>1. 传感器掩码图像生成</strong><br>此阶段的目标是将原始传感器数据转换为与RGB图像空间对齐、并突出任务相关区域的统一表示。流程如下：</p>
<ul>
<li><strong>传感器数据预处理</strong>：将所有原始测量值转换为类似相机的2D空间表示。热成像相机本身输出光栅图像。对于毫米波雷达和麦克风阵列，则通过常规的延迟求和波束成形计算方位角-仰角热力图，形成揭示环境信息的“传感器图像”。</li>
<li><strong>分割掩码生成</strong>：首先，将任务描述文本和RGB图像输入视觉语言模型（如GPT-4o），生成描述当前场景中与任务相关对象的分割提示词。该提示词仅在任务开始时生成一次，其延迟不影响实时操作。随后，将提示词和RGB图像输入分割模型（如Grounded SAM 2），生成任务相关对象的二值掩码。</li>
<li><strong>图像混合</strong>：通过一次性校准（旋转和裁剪）使传感器图像与RGB相机空间对齐（允许一定不精确度）。然后，使用上一步得到的掩码提取传感器图像中感兴趣的部分，最后按照公式 $I_i^m = \text{mask} \odot (\alpha I_i^c + (1-\alpha)I_{RGB}) + (1-\text{mask}) \odot I_{RGB}$ 将其与RGB图像混合，生成最终的传感器掩码图像。其中 $\alpha$ 是混合超参数，默认设为1。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.01210v2/x3.png" alt="传感器数据处理"></p>
<blockquote>
<p><strong>图3</strong>：传感器数据处理示意图。提出了一个适用于各种传感器（包括热像仪、毫米波雷达和声学麦克风阵列）的通用数据处理流程，即将传感器信息作为VLA模型输入叠加在RGB图像之上。</p>
</blockquote>
<p><strong>2. 多传感器VLA模型架构与训练</strong><br>模型架构基于现有的RGB预训练VLA骨干网络（如SmolVLA）进行构建。</p>
<ul>
<li><strong>编码与投影</strong>：将生成的传感器掩码图像输入<strong>冻结的</strong>预训练视觉编码器。随后，为<strong>每种传感器模态配备一个独立的多层感知机投影层</strong>，用于将编码后的传感器图像令牌与语言及RGB图像令牌对齐。</li>
<li><strong>融合与决策</strong>：将所有传感器投影后的令牌与任务描述经语言编码器得到的语言令牌拼接，作为大型语言模型的输入。LLM的输出再传递给基于扩散的动作专家模块，生成最终的机器人动作序列。</li>
<li><strong>训练策略</strong>：训练时冻结视觉和语言编码器，仅训练各传感器的MLP投影器以及模型其余未冻结的权重。每个传感器的MLP投影器使用现有VLA模型中RGB投影层的权重进行初始化，以便利用其基本的图像理解能力，并通过演示数据更新以学习新传感器的特征。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，OmniVLA的核心创新在于提出了“传感器掩码图像”这一统一表示，它1) 将传感器信息空间锚定在RGB像素坐标中，便于机器人操作；2) 保持接近RGB的统计特性，便于复用预训练视觉编码器；3) 为不同传感器提供了统一接口。此外，采用轻量级的、按传感器分配的MLP投影器，避免了为每个传感器训练复杂编码器所需的大量数据。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究构建了一个包含机械臂、RGB相机以及深度相机、红外热像仪、毫米波雷达传感器和六麦克风圆形阵列的多传感器机器人原型系统用于数据收集和评估。使用SmolVLA作为基础模型，在三个需要利用非视觉感官模态的操作任务上进行评估：1) <strong>热感知</strong>：区分冷热饮料并拾取冷饮；2) <strong>毫米波感知</strong>：穿透纸板/泡沫盒，找到并打开内部有物体的盒子；3) <strong>声学感知</strong>：定位隐藏在遮盖物下正在响铃的手机并移除遮盖物。</p>
<p><strong>基线方法</strong>：1) <strong>VLA-RGB</strong>：仅使用RGB输入进行训练和推理的VLA模型。2) <strong>VLA-RAW</strong>：使用原始传感器数据/图像（经过波束成形但未进行分割和叠加处理）作为输入的VLA模型，其模型架构与OmniVLA相同。</p>
<p><strong>关键实验结果</strong>：<br>OmniVLA在三个任务上取得了平均<strong>84%</strong> 的成功率，显著优于仅RGB基线（<strong>25%<strong>）和原始传感器输入基线（</strong>56%<strong>），分别高出</strong>59%</strong> 和**28%**。在任务评分上也分别有0.45和0.17的提升。</p>
<p><img src="https://arxiv.org/html/2511.01210v2/x5.png" alt="任务完成示例"></p>
<blockquote>
<p><strong>图5</strong>：机器人操作任务随时间完成的示例。展示了热感知、毫米波感知和声学感知任务的成功轨迹，前三列为传感器掩码图像，其余为原始RGB图像。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.01210v2/x6.png" alt="学习效率"></p>
<blockquote>
<p><strong>图6</strong>：成功率随演示片段数量的变化。OmniVLA（使用传感器掩码图像）相比VLA-RAW（使用原始传感器图像）具有更高的数据效率，仅用约50%的训练数据即可达到相近的成功率。</p>
</blockquote>
<p><strong>消融与泛化分析</strong>：</p>
<ul>
<li><strong>数据效率</strong>：在热感知任务上，OmniVLA仅需约50%的训练演示数据即可达到与VLA-RAW模型相当的成功率，证明了传感器掩码图像表示的高效性。</li>
<li><strong>模型兼容性</strong>：将方法应用于另一个VLA模型Pi0也取得了64%的平均成功率，证明了其与不同RGB预训练VLA骨干的兼容性。</li>
<li><strong>泛化能力</strong>：在包含800个演示片段的混合多感官数据集上预训练后，OmniVLA在物体类别和材质发生变化的“未见过的任务”上进行少量样本适应学习。如表III所示，OmniVLA在多个任务阶段上的成功率均显著优于未经多感官预训练的基线（VLA-Base）以及使用原始传感器图像预训练的基线（Pretrained VLA-RAW），展现了出色的泛化性能。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.01210v2/x7.png" alt="泛化性能"></p>
<blockquote>
<p><strong>图7</strong>：适应未见任务时的成功率对比。经过多感官预训练的OmniVLA模型在泛化到新任务时，性能明显优于两个基线模型。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个统一红外、毫米波雷达和声学等多传感模态的视觉-语言-动作模型OmniVLA，实现了超越RGB感知的机器人操作能力。</li>
<li>创新性地提出了“传感器掩码图像”这一空间锚定、语义对齐的统一表示，它允许复用预训练视觉编码器，提供了跨硬件的统一接口，并显著提升了任务学习效率。</li>
<li>设计了一种轻量级的多传感器VLA架构，并通过大量真实世界实验验证了其性能、数据效率和泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文提到传感器与RGB相机的一次性校准允许存在一定不精确度，但未量化这种不精确度的容忍范围及其对性能的影响边界。此外，方法依赖于一个基于云或本地的VLM来生成分割提示，虽然仅调用一次，但在极端资源受限或网络不佳的场景下可能构成限制。</p>
<p><strong>研究启示</strong>：OmniVLA证明了将异质传感器统一到图像原生表示中进行处理的可行性，为扩展VLA模型的感知维度提供了新范式。这种思路可扩展到更多类型的传感器（如深度、触觉、特定光谱成像等）。未来的工作可以探索如何进一步减少对额外分割模型的依赖，或者研究更高效、自适应的传感器信息融合与对齐机制。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型仅依赖RGB摄像头、感知能力受限的问题，提出OmniVLA模型。其核心是**传感器掩码图像**这一统一表示，将红外、毫米波雷达与麦克风阵列等传感器生成的物理掩码叠加于RGB图像，并基于RGB预训练VLA骨干构建多感官架构。实验表明，OmniVLA在真实机器人任务中平均成功率达84%，较仅RGB基线提升59%，较原始传感器输入基线提升28%，且学习效率更高、泛化能力更强。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01210" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>