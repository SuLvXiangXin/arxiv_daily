<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MAPLE: Encoding Dexterous Robotic Manipulation Priors Learned From Egocentric Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>MAPLE: Encoding Dexterous Robotic Manipulation Priors Learned From Egocentric Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.06084" target="_blank" rel="noreferrer">2504.06084</a></span>
        <span>作者: Gavryushin, Alexey, Wang, Xi, Malate, Robert J. S., Yang, Chenyu, Liconti, Davide, Zurbrügg, René, Katzschmann, Robert K., Pollefeys, Marc</span>
        <span>日期: 2025/04/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用大规模自我中心视频学习机器人操作先验是一个重要方向。主流方法可分为两类：一是隐式方法（如R3M、VC-1），通过对比学习或掩码自编码器从视频中提取通用特征；二是显式方法（如HRP、VRB），直接预测交互轨迹或接触热图等。然而，这些方法存在关键局限性：隐式方法缺乏对操作相关线索的专门关注，难以提取精细的灵巧操作信息；显式方法则多关注高层级交互或简单的两指抓取，且常依赖人工标注，难以扩展。</p>
<p>本文针对“如何从大规模自我中心视频中学习可泛化、精细化的灵巧操作先验”这一具体痛点，提出了新的视角：专注于学习接触时刻的精确手-物交互线索。本文认为，物体接触点以及接触时的手部姿态为下游灵巧操作任务提供了强有力的先验。核心思路是：通过一个自动化管道从视频中提取接触点与手部姿态作为监督信号，训练一个视觉编码器来预测这些交互线索，并将学到的特征表示用于训练下游的灵巧机器人操作策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>MAPLE的整体框架是一个两阶段管道：首先，通过一个编码器-解码器结构，从单张输入图像中学习预测未来的手-物接触点及接触时的手部姿态，从而将操作先验编码到视觉特征中；然后，冻结该预训练好的视觉编码器，将其提取的特征与机器人手部位置信息一同输入一个基于Transformer的扩散策略网络，以预测灵巧手的动作序列。</p>
<p><img src="https://arxiv.org/html/2504.06084v2/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：MAPLE方法总览。给定单张输入帧，编码器被训练去推理手-物交互，具体预测接触点和抓握手姿。这种训练将操作先验注入学习到的特征表示中，使其适用于下游机器人操作。从冻结的视觉编码器提取的特征，结合机器人手部位置，被输入一个基于Transformer的扩散策略网络，以预测灵巧手动作序列。</p>
</blockquote>
<p>核心模块包括操作先验建模、监督信号提取和先验编码：</p>
<ol>
<li><p><strong>操作先验建模</strong>：将学习目标定义为：给定一个预测帧 <code>f_p</code>（手部距离物体尚远），预测未来接触时刻 <code>f_c</code> 的接触点 <code>pt_p</code> 和手部姿态 <code>H_c</code>。接触点定义为拇指和食指指尖首次接触物体的物体表面点。<br><img src="https://arxiv.org/html/2504.06084v2/x2.png" alt="先验建模"></p>
<blockquote>
<p><strong>图3</strong>：操作先验建模。通过从输入帧预测未来接触点和手部姿态来学习操作先验。训练数据通过识别一个接触帧 <code>f_c</code> 和一个先前的预测帧 <code>f_p</code> 来提取。接触点和手部姿态从接触帧提取，并使用点跟踪器将接触位置反向投影到预测帧上。</p>
</blockquote>
</li>
<li><p><strong>监督信号提取（自动化管道）</strong>：</p>
<ul>
<li><strong>识别接触帧</strong>：使用现成的VISOR-HOS模型进行手-物接触分割，将一系列接触中的第一帧作为 <code>f_c</code>。</li>
<li><strong>提取接触标签</strong>：使用HaMeR模型从 <code>f_c</code> 中提取MANO手部姿态 <code>H_c</code>，并通过MANO层获取2D指尖关键点。将拇指和食指指尖关键点投影到经过二值腐蚀处理的物体掩码上，得到精确的接触点 <code>pt_c</code>。</li>
<li><strong>提取预测帧及其标签</strong>：使用点跟踪器将 <code>f_c</code> 中的接触点 <code>pt_c</code> 反向追踪时间，直到手部掩码不再与经过二值膨胀的物体掩码相交，此帧作为预测帧 <code>f_p</code>，跟踪得到的点作为 <code>pt_p</code>。</li>
<li><strong>手部姿态标记化</strong>：为简化高维手部姿态的预测，将其转化为分类任务。使用基于Memcodes的标记器，在Ego4D数据集的手部姿态子集上训练，将MANO姿态标记化为8个码本（每个大小1024）的序列。</li>
</ul>
</li>
<li><p><strong>编码操作先验（网络与损失）</strong>：</p>
<ul>
<li><strong>网络结构</strong>：采用ViT-B/16作为编码器（用DINO权重初始化），后接一个2层Transformer解码器。编码器将图像编码为低维嵌入，解码器仅基于此嵌入预测目标。</li>
<li><strong>接触点预测损失</strong>：将图像离散化为100x100的网格，预测接触点落入的网格索引，使用交叉熵损失 <code>L_ct,pt</code>。</li>
<li><strong>手部姿态预测损失</strong>：解码器预测每个姿态标记的概率分布，取最高分标记，同样使用交叉熵损失 <code>L_hand</code>。</li>
<li><strong>总损失</strong>：<code>L = ∑_(pt∈{0,1}) L_ct,pt + λ_hand L_hand</code>。</li>
</ul>
</li>
</ol>
<p>与现有方法相比，MAPLE的创新点具体体现在：1) <strong>更精细、更准确的监督信号提取</strong>：不同于HRP等方法从手-物边界框交集随机采样接触点（易出错），MAPLE通过指尖投影到物体掩码来获取精确接触点；2) <strong>引入手部姿态作为先验</strong>：并将高维连续姿态回归转化为离散标记分类问题，使学习更稳定；3) <strong>完全自动化</strong>：不依赖任何人工标注，从大规模视频（Ego4D）中自动提取了约82,100个训练样本。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：<ul>
<li>接触点预测任务：使用论文<code>[liu2022joint]</code>中的数据集（源自Ego4D等烹饪数据集，含34.5K交互热点）。</li>
<li>模拟环境任务：<ul>
<li>现有基准：DAPG的四个任务（开门、使用锤子、旋转笔、重定位球）。</li>
<li>新设计基准：四个自定义的MuJoCo任务，使用Adroit手（平底锅、刷子、电钻、熨斗），每个任务收集25条专家演示。</li>
</ul>
</li>
<li>真实世界实验：使用ORCA灵巧手。</li>
</ul>
</li>
<li><strong>对比基线</strong>：通用视觉编码器（CLIP, DINO/v2/v3）与SotA机器人操作编码器（R3M, HRP, VC-1）。</li>
<li><strong>评估指标</strong>：接触预测使用Affordance Map相似度（SIM）和归一化扫描路径显着性（NSS）；模拟任务使用任务完成成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>接触点预测</strong>：MAPLE在SIM和NSS两个指标上均优于所有基线。<br>表 1 : 接触预测评估。比较不同视觉编码器在[liu2022joint]引入的数据集上的性能。MAPLE在SIM和NSS指标上均取得了最佳性能。</p>
<table>
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">CLIP</th>
<th align="left">DINO</th>
<th align="left">DINOv2</th>
<th align="left">DINOv3</th>
<th align="left">HRP</th>
<th align="left">VC-1</th>
<th align="left">R3M</th>
<th align="left">MAPLE</th>
</tr>
</thead>
<tbody><tr>
<td align="left">SIM ↑</td>
<td align="left">0.051</td>
<td align="left">0.061</td>
<td align="left">0.061</td>
<td align="left">0.059</td>
<td align="left">0.061</td>
<td align="left">0.064</td>
<td align="left">0.068</td>
<td align="left"><strong>0.068</strong></td>
</tr>
<tr>
<td align="left">NSS ↑</td>
<td align="left">0.561</td>
<td align="left">0.577</td>
<td align="left">0.647</td>
<td align="left">0.615</td>
<td align="left">0.602</td>
<td align="left">0.637</td>
<td align="left">0.651</td>
<td align="left"><strong>0.654</strong></td>
</tr>
</tbody></table>
</li>
<li><p><strong>模拟灵巧操作</strong>：在8个模拟任务（4个DAPG+4个新任务）上，MAPLE取得了最高的平均成功率（39.8%），尤其在复杂的新任务上优势更明显，且性能方差更小。<br>表 2: 模拟环境结果。我们报告了八个模拟任务的任务完成成功率。绿色表示最佳性能，蓝色表示次佳。我们报告了随机种子间的标准差。MAPLE在评估套件上达到了最佳平均性能，并且在更高复杂度的任务上方差减小。</p>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Door</th>
<th align="left">Hammer</th>
<th align="left">Pen</th>
<th align="left">Relocate</th>
<th align="left">Brush</th>
<th align="left">Drill</th>
<th align="left">Iron</th>
<th align="left">Pan</th>
<th align="left">Mean</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CLIP</td>
<td align="left">31.8 ±4.5</td>
<td align="left">13.3 ±1.6</td>
<td align="left">58.0 ±2.2</td>
<td align="left">28.7 ±1.1</td>
<td align="left">15.3 ±2.2</td>
<td align="left">18.7 ±1.6</td>
<td align="left">22.0 ±3.8</td>
<td align="left">31.8 ±7.6</td>
<td align="left">27.4 ±1.9</td>
</tr>
<tr>
<td align="left">DINO</td>
<td align="left">29.3 ±3.8</td>
<td align="left">14.0 ±1.1</td>
<td align="left">71.8 ±1.7</td>
<td align="left">32.9 ±0.8</td>
<td align="left">18.7 ±1.6</td>
<td align="left">19.3 ±1.6</td>
<td align="left">24.7 ±4.1</td>
<td align="left">34.0 ±5.5</td>
<td align="left">30.6 ±1.6</td>
</tr>
<tr>
<td align="left">DINOv2</td>
<td align="left">32.0 ±4.0</td>
<td align="left">15.3 ±1.6</td>
<td align="left">70.4 ±2.2</td>
<td align="left">33.1 ±0.8</td>
<td align="left">20.7 ±2.2</td>
<td align="left">20.0 ±2.2</td>
<td align="left">26.0 ±4.0</td>
<td align="left">36.4 ±6.2</td>
<td align="left">31.7 ±1.9</td>
</tr>
<tr>
<td align="left">DINOv3</td>
<td align="left">29.8 ±3.8</td>
<td align="left">14.7 ±1.6</td>
<td align="left">70.9 ±1.7</td>
<td align="left">33.1 ±0.8</td>
<td align="left">20.0 ±2.2</td>
<td align="left">19.3 ±1.6</td>
<td align="left">26.0 ±4.0</td>
<td align="left">36.9 ±6.2</td>
<td align="left">31.3 ±1.9</td>
</tr>
<tr>
<td align="left">HRP</td>
<td align="left">31.8 ±4.5</td>
<td align="left">14.7 ±1.6</td>
<td align="left">69.3 ±2.2</td>
<td align="left">33.1 ±0.8</td>
<td align="left">20.0 ±2.2</td>
<td align="left">20.0 ±2.2</td>
<td align="left">26.7 ±4.1</td>
<td align="left">37.3 ±6.2</td>
<td align="left">31.6 ±1.9</td>
</tr>
<tr>
<td align="left">VC-1</td>
<td align="left">30.7 ±3.8</td>
<td align="left">14.7 ±1.6</td>
<td align="left">69.3 ±2.2</td>
<td align="left">33.1 ±0.8</td>
<td align="left">20.7 ±2.2</td>
<td align="left">20.7 ±2.2</td>
<td align="left">26.7 ±4.1</td>
<td align="left">37.3 ±6.2</td>
<td align="left">31.7 ±1.9</td>
</tr>
<tr>
<td align="left">R3M</td>
<td align="left">30.2 ±3.8</td>
<td align="left">14.7 ±1.6</td>
<td align="left">69.8 ±2.2</td>
<td align="left">33.1 ±0.8</td>
<td align="left">21.3 ±2.2</td>
<td align="left">20.7 ±2.2</td>
<td align="left">26.7 ±4.1</td>
<td align="left">37.8 ±6.2</td>
<td align="left">31.8 ±1.9</td>
</tr>
<tr>
<td align="left"><strong>MAPLE</strong></td>
<td align="left"><strong>36.4</strong> ±4.5</td>
<td align="left"><strong>16.7</strong> ±1.6</td>
<td align="left"><strong>74.2</strong> ±2.2</td>
<td align="left"><strong>34.2</strong> ±0.8</td>
<td align="left"><strong>26.0</strong> ±2.2</td>
<td align="left"><strong>26.0</strong> ±2.2</td>
<td align="left"><strong>30.0</strong> ±4.0</td>
<td align="left"><strong>44.9</strong> ±6.2</td>
<td align="left"><strong>39.8</strong> ±1.9</td>
</tr>
</tbody></table>
</li>
</ol>
<p><img src="https://arxiv.org/html/2504.06084v2/x3.png" alt="模拟环境"></p>
<blockquote>
<p><strong>图4</strong>：模拟评估环境。我们在DAPG的四个环境（a-d）上评估我们的方法，并提出了四个新的机器人环境（e-h）。我们的新环境旨在评估一组人类常用物体的操作能力，即平底锅、刷子、电钻和熨斗。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界实验</strong>：使用ORCA灵巧手执行“拿起海绵擦拭”和“将平底锅放到炉灶上”两个任务。MAPLE策略能够成功完成复杂的灵巧操作，例如稳固抓取平底锅手柄并将其准确放置。<br><img src="https://arxiv.org/html/2504.06084v2/x4.png" alt="真实世界结果1"><blockquote>
<p><strong>图13</strong>：真实世界实验：拿起海绵。MAPLE策略成功地从架子上拿起一块海绵。<br><img src="https://arxiv.org/html/2504.06084v2/x5.png" alt="真实世界结果2"><br><strong>图14</strong>：真实世界实验：拿起海绵（续）。MAPLE策略将海绵移动到目标位置（桌子）并执行擦拭动作。<br><img src="https://arxiv.org/html/2504.06084v2/x6.png" alt="真实世界结果3"><br><strong>图15</strong>：真实世界实验：平底锅放置。MAPLE策略成功抓取平底锅的手柄。<br><img src="https://arxiv.org/html/2504.06084v2/x7.png" alt="真实世界结果4"><br><strong>图16</strong>：真实世界实验：平底锅放置（续）。MAPLE策略将平底锅移动到目标位置（炉灶）并成功放置。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验</strong>：论文通过对比实验验证了各组件贡献。在接触预测任务上，MAPLE优于仅使用边界框交集提取接触点的HRP方法，证明了其精细监督信号的有效性。在模拟任务中，MAPLE相比通用编码器（如DINO）和现有机器人专用编码器（如R3M、VC-1）的显著性能提升，证明了其通过预测接触点和手部姿态所学习的操作先验对于下游灵巧策略学习具有独特优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出MAPLE方法</strong>：一种通过预测交互线索（接触点与手部姿态）来学习灵巧操作先验的视觉编码器预训练新策略，该策略完全自动化，无需人工标注。</li>
<li><strong>设计新的灵巧操作基准</strong>：提出了一套包含四个需要精细化控制的新模拟任务，弥补了现有基准对灵巧操作评估的不足。</li>
<li><strong>全面的实验验证</strong>：在接触预测、模拟灵巧操作（新旧共8个任务）以及真实世界实验中，均证明了MAPLE所学习特征的有效性和优越的泛化能力，特别是在需要精细操作的任务上。</li>
</ol>
<p>论文自身提到的局限性主要在于监督信号提取管道仍可能产生错误，例如误检的接触（false contact positives）或预测帧中仍存在接触的样本。这些错误主要源于所使用的现成模型（如VISOR-HOS）的误检，尽管通过过滤已大幅消除。</p>
<p>本文对后续研究的启示在于：1) 从人类视频中学习机器人操作先验时，<strong>设计更精细、物理上更准确的监督信号</strong>（如精确的指尖接触点）比粗糙的监督（如边界框交集）更为有效；2) <strong>将高维、连续的动作空间（如手部姿态）离散化、标记化</strong>，可以简化学习难度并提升稳定性；3) 同时进行<strong>模拟与真实世界的评估</strong>对于全面验证方法的有效性和泛化能力至关重要，这一评估模式值得后续研究借鉴。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MAPLE方法，旨在解决传统数据驱动方法在需要精细控制的复杂灵巧机器人操作任务上性能不足的问题。其核心是从大规模第一人称视角视频中学习操作先验，关键技术包括预测手-物体接触点及接触时刻的详细手部姿态，并利用这些特征训练下游操作策略。实验表明，MAPLE在现有模拟基准和新设计的复杂灵巧操作任务上均有效，并在真实灵巧机械手实验中进一步验证了其提升策略学习效率与泛化能力的优势。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.06084" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>