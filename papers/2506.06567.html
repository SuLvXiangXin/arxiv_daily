<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>NeSyPack: A Neuro-Symbolic Framework for Bimanual Logistics Packing - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>NeSyPack: A Neuro-Symbolic Framework for Bimanual Logistics Packing</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.06567" target="_blank" rel="noreferrer">2506.06567</a></span>
        <span>作者: Changliu Liu Team</span>
        <span>日期: 2025-06-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前物流打包任务的主流方法主要包括基于吸盘的模型化方法、结合视觉触觉反馈的单臂操作，以及新兴的视觉-语言-动作模型。这些方法存在关键局限性：基于吸盘的方法无法处理表面不平或漏气的物体，且难以泛化到复杂任务；单臂系统在完成如封箱等复杂操作上能力有限；而端到端的VLA模型虽然通用性强，但缺乏可解释性，且需要海量高质量数据进行训练和泛化，这在工业场景中难以获取。</p>
<p>本文针对双手机器人物流打包任务中，如何实现<strong>高泛化性、高数据效率和高可靠性</strong>这一具体痛点，提出了一种神经符号框架的新视角。该方法将数据驱动的模型与符号推理相结合，通过分层任务分解和模块化的技能图来组织与执行任务，从而避免了对大规模数据重新训练的需求。</p>
<p>本文的核心思路是：<strong>通过分层任务推理将长视野任务分解为原子任务，再利用一个统一的神经符号技能图为每个原子任务选择合适的符号化表示、感知模型和运动策略来执行。</strong></p>
<h2 id="方法详解">方法详解</h2>
<p>NeSyPack的整体框架包含两个主要组件：分层任务推理模块和机器人技能图。给定一个打包任务T（包含需打包的物品类型、数量及封箱任务），HTR首先将其分解为一系列子任务，进而将每个子任务分解为原子任务计划。技能图则以原子任务为输入，提取其符号化表示，推理出最合适的动作策略，并生成机器人控制命令。</p>
<p><img src="https://arxiv.org/html/2506.06567v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：NeSyPack系统概览。给定打包任务T，分层任务推理模块将长视野任务分解为更小的原子任务，并查询机器人技能图以使用适当的技能执行各个原子任务。</p>
</blockquote>
<p>核心模块是<strong>机器人技能图</strong>，它集成了学习模型和符号组件，包含六个相互依赖的要素：</p>
<ol>
<li><strong>技能</strong>：语义化的高级机器人操作，如感知技能“检测物体”、操作技能“拾取”、“放置”和运动技能“保持”、“移动”。</li>
<li><strong>对象</strong>：待打包物品，由其物理属性（尺寸、形状、重量、刚性）和感知特征描述，这些特征直接影响操作策略的选择（例如，直立易拉罐从顶部抓取，倾斜的则垂直其长轴抓取）。</li>
<li><strong>体现</strong>：支持的机器人配置，如单臂或双臂。</li>
<li><strong>工具</strong>：可用的末端执行器，如“二指夹爪”和“单指夹爪”。</li>
<li><strong>执行器</strong>：将技能-对象对链接到可执行函数，结合体现、工具和反馈传感器生成执行策略。</li>
<li><strong>传感器</strong>：提供反馈的传感器。</li>
</ol>
<p>这种结构化方法具有模块化设计的优势，允许技能复用，支持高效扩展新技能而不影响现有技能。</p>
<p>技能具体分为三类：</p>
<ul>
<li><strong>感知技能</strong>：更新机器人内部世界模型，提供符号化感知特征。包括：1) <strong>检测刚性物体</strong>：使用YOLOV11进行实例分割，结合RGB-D数据通过ICP配准CAD模型得到6D位姿；2) <strong>检测可变形物体</strong>：分析分割后的点云，检测边界和显著边缘以生成抓取候选；3) <strong>检测抓取</strong>：通过对比夹爪位姿和感知到的物体位姿来确认抓取成功。</li>
<li><strong>操作技能</strong>：与物体物理交互的可复用动作基元，如“拾取”、“放置”。首先在笛卡尔空间进行运动规划，整合硬约束（运动学可达性、碰撞避免）和软约束（偏好抓取方向、轨迹平滑度）作为路径点标签，然后将这些带标签的笛卡尔路径点映射到构型空间。针对不同物体类型定制“拾取”技能参数模板（如小刚性物体、圆柱体、大型物体、可变形物体）。</li>
<li><strong>运动技能</strong>：不与环境物理交互的技能，包括用于在关键位姿间进行无碰撞移动的“移动”技能（使用RRT-Connect规划）和使机器人保持静止的“保持”技能。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.06567v1/x2.png" alt="HTR示例"></p>
<blockquote>
<p><strong>图2</strong>：HTR在示例任务上的图示。实线箭头：确定的任务计划。虚线箭头：可能发生变化的暂定任务计划（如故障恢复）。</p>
</blockquote>
<p><strong>HTR模块</strong>不仅进行任务分解，还监控整体任务进度，并在必要时调整计划（如图2中的虚线箭头所示），例如在感知失败时动态插入“搅拌箱子”作为故障恢复策略。</p>
<p>与现有方法相比，NeSyPack的创新点在于：1) <strong>神经符号混合</strong>：将数据驱动的感知、学习到的操作策略与符号化的推理、规划和约束处理紧密结合；2) <strong>层次化与模块化</strong>：通过HTR和技能图实现任务的多层抽象与模块化管理，显著提升了系统的可解释性、泛化能力和数据效率；3) <strong>技能图的统一表示</strong>：将异构的模型和计算模块组织在一个统一的图结构中，实现了技能参数的自动选择与组合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个平台上进行：1) <strong>开发平台</strong>：配备ALOHA夹爪和RealSense D435i相机的Unitree G1人形机器人；2) <strong>竞赛平台</strong>：ICRA 2025“What Bimanuals Can Do”竞赛中使用的双Galaxea A1X机械臂系统，每个手腕装有RealSense D435i相机。使用了涵盖立方体、球体、圆柱体、可变形物体等十类具有不同操作挑战性的物体。</p>
<p><img src="https://arxiv.org/html/2506.06567v1/x3.png" alt="物体列表"></p>
<blockquote>
<p><strong>图3</strong>：打包物体列表。黄色：准备阶段可用的物体。绿色：WBCD竞赛中使用的物品。竞赛前三天才公布绿色物品，且机器人硬件在赛前也未提供，这测试了系统的泛化能力。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>在G1平台上的性能</strong>：对准备阶段可用的每类物体进行10次试验，成功率高，所有类别均达到9/10或10/10（见表I）。例如，立方体和球体成功率为90%，堆叠物体和小物体成功率为100%。</li>
<li><strong>在WBCD竞赛中的性能</strong>：在四次官方试验中，系统取得了优异的打包成功率（见表II）。例如，立方体、大型物体和长方体成功率为100%，球体和堆叠物体成功率为83.3%。基于此，团队获得了比赛第一名。</li>
</ul>
<p><strong>系统优势分析</strong>：</p>
<ol>
<li><strong>泛化性</strong>：系统能快速适应新机器人平台，无需重新训练模型或重写控制逻辑，因为所有涉及机器人特定参数的计算都是符号化处理的。任务泛化能力强，能很好地泛化到具有相似几何形状的物体。</li>
<li><strong>数据效率</strong>：感知模型在每个物体类别仅使用少量标注图像的情况下，在RTX 4060笔记本上不到一小时即可完成适配；操作技能通过一次性关键帧示教获取。</li>
<li><strong>可靠性</strong>：在G1平台实验室评估中任务成功率超过90%，在竞赛的双A1X平台上成功保持了这一性能水平。</li>
</ol>
<p><strong>局限性分析</strong>：<br>实验中也遇到了失败案例，主要源于两方面：</p>
<ol>
<li><strong>开发阶段</strong>：G1手臂的<strong>可达性有限</strong>，导致部分物体因超出工作空间而抓取失败。这源于笛卡尔空间计划到构型空间映射的不完全性。</li>
<li><strong>竞赛阶段</strong>：手腕相机的<strong>有限视野</strong>导致单次观测不完整，引起感知错误和抓取成功率下降。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li>提出了一个用于双手机器人物流打包的<strong>分层神经符号框架</strong>，通过结合数据驱动模型与符号推理，实现了可解释、高泛化、数据高效且可靠的任务执行。</li>
<li>设计了<strong>模块化的机器人技能图</strong>，统一管理感知、操作和运动技能，支持技能的复用、高效扩展和跨平台迁移。</li>
<li>在真实的机器人竞赛环境中<strong>验证了框架的有效性</strong>，成功应对了未知物体和新硬件平台的挑战，并获得了第一名。</li>
</ol>
<p>论文自身提到的局限性主要在于：笛卡尔空间计划到构型空间映射的可行性无法完全保证，以及依赖有限视野相机可能导致的感知错误。</p>
<p>对后续研究的启示：未来工作可以探索如何评估笛卡尔空间计划在构型空间约束下的可行性，以及如何通过集成广角相机或多视角融合技术来增强感知的鲁棒性。该框架的神经符号混合与模块化设计思路，为解决其他复杂的长视野机器人操作任务提供了有价值的参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出NeSyPack，一个用于双手机器人物流打包的神经符号框架。核心问题是解决现有方法（如吸盘抓取器局限性、端到端模型数据需求大且不可解释）在处理多样化物体打包任务时的不足。关键技术结合数据驱动模型与符号推理，通过分层推理将任务分解为子任务，并由符号技能图管理原子技能，动态选择参数与配置。该模块化设计提升了鲁棒性、适应性与复用效率。在2025年IEEE ICRA的WBCD比赛中，基于NeSyPack的系统获得第一名，验证了其优于需大规模重训练的端到端模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.06567" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>