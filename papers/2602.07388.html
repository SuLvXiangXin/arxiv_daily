<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.07388" target="_blank" rel="noreferrer">2602.07388</a></span>
        <span>作者: Jianfei Yang Team</span>
        <span>日期: 2026-02-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在基于模仿学习的机器人操作领域，生成模型策略（如扩散策略）通过从演示中学习动作分布，已展现出强大性能。然而，在长视野任务中，视觉上相似的观察结果经常在不同的执行阶段重复出现，却需要不同的动作，这导致仅以瞬时观察为条件的策略会产生模糊的预测，即多模态动作模糊性。现有主流扩散策略主要依赖瞬时观察进行动作生成，在面临这种一对多的观察-动作映射时，会产生多个似然相近但不兼容的动作，导致采样不稳定和错误的时序顺序，破坏长视野执行的鲁棒性。常见的解决方法是采用分层架构或基于LLM的规划器将长任务分解为子任务序列，但这会引入高延迟、计算开销和可靠性问题。本文针对在单一策略内解决多模态动作模糊性的核心痛点，提出将执行历史作为显式轨迹纳入条件生成的新视角。核心思路是：通过聚合历史机器人运动形成紧凑的执行轨迹，并将其投影到视觉观察空间，为扩散策略提供阶段感知的上下文，从而在视觉相似的观察出现时消除动作歧义，并增强对视觉干扰的鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的轨迹聚焦扩散策略（TF-DP）是一个将执行历史显式整合到基于扩散的动作生成中的单一策略框架。其整体流程是：在每一时间步，收集历史末端执行器位置形成运动轨迹，通过轨迹聚焦场渲染模块将其投影到2D全局相机空间，生成包含历史信息的增强观察，最后以此增强观察为条件，通过扩散模型去噪过程生成当前动作。</p>
<p><img src="https://arxiv.org/html/2602.07388v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：TF-DP框架。收集历史机器人运动以创建运动轨迹。所提出的轨迹聚焦场从轨迹生成。然后，轨迹和聚焦场被投影到图像空间，以解决多模态动作模糊性并减轻背景中的视觉干扰。</p>
</blockquote>
<p>核心模块是轨迹聚焦场渲染。给定当前观察 $o_t = {I_t^{\mathrm{global}}, I_t^{\mathrm{side}}, I_t^{\mathrm{wrist}}, p_t^{\mathrm{ee}}}$ 和历史末端执行器轨迹 $\mathcal{H}_t = {p_\tau^{\mathrm{ee}} = (x_\tau, y_\tau, z_\tau) \mid \tau \le t}$，该模块执行两个关键操作：</p>
<ol>
<li><strong>轨迹投影</strong>：将3D机器人空间中的历史轨迹 $\mathcal{H}_t$ 投影到2D全局相机图像平面，生成一个二值轨迹图像 $I_t^{trace}$，其中轨迹点被渲染为白色像素。</li>
<li><strong>聚焦场渲染</strong>：基于投影的2D轨迹，在原始全局视图 $I_t^{\mathrm{global}}$ 上渲染一个轨迹聚焦场。该场以轨迹点为中心，生成一个空间权重图（例如高斯衰减场），强调与历史运动相关的区域，抑制不相关的背景视觉变化。渲染后得到增强的全局视图 $\tilde{I}_t^{\mathrm{global}}$。</li>
</ol>
<p>最终，策略的条件输入被增强为 $\tilde{o}_t = {\tilde{I}_t^{\mathrm{global}}, I_t^{\mathrm{side}}, I_t^{\mathrm{wrist}}, I_t^{trace}, p_t^{\mathrm{ee}}}$。扩散模型（采用标准的DDPM框架）以此增强观察为条件，学习去噪并生成动作 $a_t$。训练目标是最小化去噪网络预测的噪声与真实噪声之间的均方误差。</p>
<p>与现有方法相比，TF-DP的创新点具体体现在：</p>
<ol>
<li><strong>显式的、轻量化的历史条件</strong>：不同于简单堆叠有限历史帧（受计算和延迟限制）或依赖复杂的语义/符号上下文，TF-DP将整个执行历史压缩为一个紧凑的轨迹表示，并以可扩展的方式融入视觉观察。</li>
<li><strong>轨迹聚焦场</strong>：不仅提供轨迹位置，还通过渲染的聚焦场主动引导策略关注任务相关区域，这提高了对背景视觉干扰的鲁棒性。</li>
<li><strong>单一策略框架</strong>：无需任务分解或高层规划器，在单一策略内解决了长视野任务中的动作歧义问题，保持了低延迟和高反应性。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在具有显著多模态动作模糊性和视觉杂乱条件的真实世界机器人操作任务上进行评估。使用的基准平台是真实的机器人操纵器。任务设计旨在诱发多模态动作模糊性，例如将一个立方体从中心依次放置到右侧和左侧，过程中会出现视觉相似的观察对应不同的放置动作。</p>
<p>对比的基线方法包括：</p>
<ul>
<li><strong>Vanilla Diffusion Policy</strong>：标准的仅以当前观察为条件的扩散策略。</li>
<li><strong>History Stacking</strong>：将过去固定数量的观察/动作堆叠作为条件的扩散策略。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.07388v1/x1.png" alt="定性结果1"></p>
<blockquote>
<p><strong>图1</strong>：解决多模态动作模糊性。(a) 长视野操作中，视觉相似的观察在不同执行阶段映射到不同动作。(b) 这种一对多映射导致仅以瞬时观察为条件的扩散策略出现多模态动作模糊性。(c) TF-DP通过以显式运动轨迹和轨迹聚焦场为条件来解决该问题，实现时间一致的动作。</p>
</blockquote>
<p>关键实验结果总结如下：</p>
<ol>
<li><strong>多模态动作模糊性任务</strong>：TF-DP相比原始扩散策略，性能提升了80.56%。这表明显式执行轨迹条件化能有效消除动作歧义，确保正确的执行顺序。</li>
<li><strong>视觉干扰下的任务</strong>：在存在背景视觉干扰（如移动的干扰物）的情况下，TF-DP相比原始扩散策略，性能提升了86.11%。轨迹聚焦场通过强调历史相关区域，显著增强了策略的鲁棒性。</li>
<li><strong>推理效率</strong>：TF-DP仅引入了6.4%的运行时开销，在显著提升性能的同时保持了高效的推理能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.07388v1/x3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：消融研究。评估了TF-DP不同组件（轨迹图像、轨迹聚焦场）在解决动作模糊性和抗视觉干扰任务中的贡献。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：</p>
<ul>
<li>**仅使用轨迹图像 ($I_t^{trace}$)**：对解决动作模糊性有一定帮助，但对视觉干扰的鲁棒性提升有限。</li>
<li>**仅使用轨迹聚焦场 ($\tilde{I}_t^{\mathrm{global}}$)**：能有效提升抗干扰能力，但在解决深度模糊性方面不如完整模型。</li>
<li><strong>完整TF-DP（两者结合）</strong>：取得了最佳性能，表明轨迹图像提供了关键的阶段区分信息，而轨迹聚焦场增强了感知的鲁棒性，两者互补。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.07388v1/x4.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图4</strong>：在存在视觉干扰（移动的黄色立方体）的长视野放置任务中的定性结果。TF-DP能成功完成所有阶段，而原始扩散策略在后期阶段失败。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：</p>
<ol>
<li>明确指出了多模态动作模糊性是扩散策略在长视野机器人操作中的一个根本性失效模式，并引入了执行轨迹作为解决该问题的有效机制。</li>
<li>提出了轨迹聚焦扩散策略，通过轻量化的轨迹聚焦场渲染，将执行历史整合到扩散策略中，在单一策略内实现了时间一致且鲁棒的行为。</li>
<li>在具有挑战性的真实世界操作任务上验证了方法的有效性，证明了其在消除动作歧义和抵抗视觉干扰方面的显著优势。</li>
</ol>
<p>论文自身提到的局限性包括：TF-DP依赖于准确的轨迹投影，这需要已知或标定好的相机参数；此外，方法主要处理了末端执行器的位置历史，未来可以探索整合更丰富的状态历史（如关节角度、物体状态）。</p>
<p>对后续研究的启示：这项工作表明，在生成式机器人策略中，显式地、紧凑地编码低层次执行历史是一种可扩展且有效的方法，能够在不依赖复杂分层系统的情况下解决长视野任务中的关键挑战。这为开发更鲁棒、更高效的单一策略提供了新方向，例如探索不同的历史表示形式，或将此概念扩展到其他生成模型（如流匹配）中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对长视野机器人操作中，因视觉相似观察导致动作预测模糊的多模态动作歧义问题，提出了一种基于执行轨迹的扩散策略。该方法的核心是将机器人的历史运动表示为显式的执行轨迹，并将其投影到视觉观察空间，为动作生成提供阶段感知的上下文。实验表明，该策略显著提升了时间一致性和鲁棒性，在多模态动作歧义任务上性能超越基线80.56%，在视觉干扰下提升86.11%，且推理效率仅降低6.4%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.07388" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>