<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.01710" target="_blank" rel="noreferrer">2506.01710</a></span>
        <span>作者: Kang Liu Team</span>
        <span>日期: 2025-06-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>表格推理（如表格问答、事实验证、文本到SQL）需要精确理解结构化表格数据，并结合数值计算与代码操作进行推理。当前主流方法包括提示工程和监督微调（SFT）。提示工程需要大量人工设计，难以扩展和泛化；SFT方法虽然取得了显著成功，但由于模仿学习中固有的偏差，通常在泛化性、鲁棒性以及对复杂推理、新表格结构或未见任务类型的适应性方面表现不佳。</p>
<p>本文针对SFT方法在表格推理任务中泛化能力弱、鲁棒性差的痛点，提出了将强化学习（RL）应用于表格推理的新视角。近期研究表明，通过增加计算进行更深层次的推理可以提高响应质量，这推动了RL在数学推理和代码生成等领域的进展。RL有潜力通过自主探索解空间来获取高质量的推理路径，而不是仅仅依赖教师模型的蒸馏，这有助于减轻偏差并更深入地发现有效的解题模式。</p>
<p>本文的核心思路是：首次将强化学习系统地应用于几乎所有的表格推理任务，通过精心设计的数据预处理、奖励函数和训练策略，利用简单的基于规则的结果奖励，使模型在多个基准测试中超越SFT，并展现出优异的泛化能力和鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>Reasoning-Table的整体流程包括数据集预处理、监督微调（SFT）和基于GRPO算法的强化学习训练。输入是自然语言查询Q和表格T，输出是答案A。核心模块包括SFT数据生成与过滤、RL数据预处理（难度控制与位置证据提取）、以及多组件奖励设计。</p>
<p><img src="https://arxiv.org/html/2506.01710v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Reasoning-Table方法概览。该方法覆盖所有表格相关任务和多样化的表格类型。与SFT相比，使用GPRO强化学习方法，使得推理模型获得了更强的推理能力、泛化性和鲁棒性。</p>
</blockquote>
<p><strong>SFT数据集预处理</strong>：由于大多数表格推理数据集缺乏推理链标注，作者生成了两种SFT数据：1) <strong>No-Reason SFT</strong>：直接使用原始数据集的输入输出对。2) <strong>Reason-SFT</strong>：使用专有LLM为现有数据集生成推理轨迹，并通过基于TF-IDF相似度的冗余检测算法进行质量过滤，过滤掉约90.3%的低质量轨迹，保留高质量的长思维链数据。</p>
<p><strong>RL数据集预处理</strong>：包含两个关键步骤。1) <strong>难度控制</strong>：使用Qwen2.5-32B为每个示例生成多个答案，用于后续的数据消融研究。2) <strong>位置证据</strong>：为了利用表格的行列结构特性，引入了一种称为“位置轨迹”的表格特定推理内容。通过提示策略让LLM标注相关列或具体单元格，并取多个正确样本位置标注的并集作为最终的位置证据。对于文本到SQL任务，则使用黄金SQL中出现的列名作为位置奖励（即模式链接）。</p>
<p><strong>训练方法</strong>：</p>
<ul>
<li><strong>监督微调</strong>：使用标准的交叉熵损失函数。</li>
<li><strong>强化训练</strong>：采用GRPO算法。该算法利用一组候选样本内的相对性能来计算优势值A_i，用于策略更新，并包含KL散度正则项以防止策略偏离参考模型太远。</li>
</ul>
<p><strong>奖励设计</strong>：这是方法的核心创新点，包含三个组成部分：</p>
<ol>
<li><strong>基于规则的结果奖励</strong>：根据最终答案与参考答案的匹配程度给出二元奖励。针对不同任务类型（短答案、长答案、SQL、文本生成）采用了精确匹配、执行结果匹配、F1阈值和BLEU阈值四种评估指标。</li>
<li><strong>格式奖励</strong>：为确保输出结构一致性和可读性，强制要求模型输出中包含<code>&lt;|im_start|&gt;reasoning</code>和<code>&lt;|im_end|&gt;&lt;answer&gt;</code>标签。</li>
<li><strong>位置奖励</strong>：鼓励模型在推理过程中忠实于表格，要求其在直接引用表格单元格时用特定标签进行标注。该奖励通过Jaccard相似度计算模型标注的单元格集合与真实应引用单元格集合的重合度。</li>
</ol>
<p><strong>最终奖励函数</strong>：为了优先保证答案正确性，同时奖励正确的推理和格式，设计了最终的奖励函数：<code>R(o_i) = R_ans(o_i) × (1 + λ1 R_pos(o_i)) + λ2 R_fmt(o_i)</code>。其中，λ1和λ2是可调超参数。该设计确保了位置奖励仅在答案正确时贡献奖励，避免了模型因推理一致但答案错误而获得高奖励。</p>
<p>与现有方法相比，创新点体现在：1) <strong>首次系统地将RL应用于广泛的表格推理任务</strong>；2) <strong>设计了针对表格结构特性的位置证据和位置奖励机制</strong>，以缓解RL在表格交互中的奖励稀疏性问题；3) <strong>提出了一个优先考虑答案正确性的复合奖励函数</strong>，有效引导模型学习；4) <strong>展示了跨任务统一RL训练的有效性</strong>，提升了模型的通用性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准数据集</strong>：涵盖了表格问答（WikiTQ, HybridQA等7个）、事实验证（TabFact, FEVEROUS）、表格到文本生成（ToTTo）、长形式问答（FetaQA）以及文本到SQL（Spider, BIRD）共15个数据集。</li>
<li><strong>基线方法</strong>：包括大型开源LLM（Qwen2.5系列）、专有LLM（Claude-3.7-Sonnet, GPT-4o）以及领域特定模型TableGPT2。</li>
<li><strong>训练设置</strong>：分为单数据集训练和跨表格问答任务的统一训练。对比了四种训练策略：No-Reason SFT, Reason-SFT, RL-zero（直接RL）, 以及Reason-SFT+RL（SFT冷启动后接RL）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>如表1和表2所示，在单数据集和统一训练设置下，RL方法（RL-zero） consistently 超越了纯SFT基线。在统一的表格问答任务上，RL-zero比Reason-SFT平均提升10.0%，比No-Reason SFT提升3.1%。采用SFT冷启动的Reason-SFT+RL策略进一步提升了性能，在统一训练设置下比RL-zero平均再提升2.79%，并超越了Claude-3.7-Sonnet和Qwen2.5-32B等更大规模的专有模型。</p>
<p>具体数值上，Reason-SFT+RL在BIRD文本到SQL任务上达到了63.16%的执行准确率（使用7B模型）。在统一的表格问答任务平均分上，Reason-SFT+RL达到了62.62%，超越了Claude-3.7-Sonnet（58.79%）和GPT-4o（58.96%）。对于推理密集型任务（如BIRD），性能提升更为显著。</p>
<p><img src="https://arxiv.org/html/2506.01710v1/x2.png" alt="训练指标"></p>
<blockquote>
<p><strong>图2</strong>：在WikiTQ数据集上的训练指标，展示了单数据集和统一数据集设置下RL和SFT+RL的性能。(a) 模型响应长度随训练步数增加（RL）或先减后稳（SFT）。(b) 奖励值随训练步数稳步提升。(c) 精确匹配分数随训练步数稳步提升。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>奖励组件</strong>：格式奖励对模型性能影响较小（比较λ2=0和λ2=0.2）。位置奖励能带来一致的性能提升（比较使用和不使用位置奖励的设置）。</li>
<li><strong>数据难度</strong>：使用中等难度数据（LLM生成答案准确率在40%-70%区间）进行RL训练，效果优于使用全量数据或极高/极低难度数据。</li>
<li><strong>SFT冷启动</strong>：实验表明，用Reason-SFT初始化后再进行RL训练，相比直接RL-zero，在所有任务上都能带来显著的性能提升。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>Reasoning-Table</strong>，这是首个将强化学习系统应用于几乎所有表格推理任务的方法，在多个基准上实现了最先进的性能。</li>
<li>设计了针对<strong>表格结构的位置证据提取方法和位置一致性奖励</strong>，有效利用了表格特性并缓解了奖励稀疏问题。</li>
<li>通过实验证明了<strong>RL训练相比SFT在表格推理任务上的优越性</strong>，体现在性能更强、泛化能力更好、鲁棒性更高，且无需依赖昂贵的教师模型蒸馏。</li>
</ol>
<p><strong>局限性</strong>：论文提到，生成高质量的Reason-SFT数据成本高昂且依赖LLM性能，无法为整个数据集生成推理轨迹（因为模型性能无法达到100%）。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>强化学习在复杂结构化数据推理任务中具有巨大潜力</strong>，能够通过自主探索超越模仿学习的局限。</li>
<li><strong>任务特定的奖励设计至关重要</strong>，本文针对表格设计的位置奖励是一个成功案例，可启发其他领域设计类似的、利用数据本身结构的奖励信号。</li>
<li><strong>中等难度的数据可能对RL训练最有益</strong>，这为数据筛选和课程学习策略提供了新思路。</li>
<li><strong>统一的多任务RL训练</strong>可以培养出更通用和鲁棒的模型，这为构建通用表格推理智能体指明了方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对表格推理任务中监督微调方法泛化性与鲁棒性不足的问题，首次将强化学习应用于该领域。提出了Reasoning-Table方法，其关键技术是采用GPRO强化学习框架，通过精心设计的数据预处理、基于规则的结果奖励和统一的跨任务训练策略来优化模型。实验表明，该方法显著提升了性能，在表格推理基准上超越Claude-3.7-Sonnet达4.0%，在BIRD文本到SQL任务上达到68.3%的准确率，有效增强了模型的泛化能力与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.01710" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>