<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.05310" target="_blank" rel="noreferrer">2508.05310</a></span>
        <span>作者: Jens Kober Team</span>
        <span>日期: 2025-08-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>交互式模仿学习旨在通过人类交互式演示和纠正来训练智能体，以缓解行为克隆中的协变量偏移问题。DAgger算法是该领域的奠基性工作，它通过在执行新手策略时聚合人类输入来使新手学习从错误中恢复，但其需要持续的人类输入，存在安全性问题。为此，后续研究提出了主动DAgger方法，仅在风险或不确定的情况下主动查询教师，以减少所需演示次数并防止训练期间失败。然而，现有主动DAgger方法在查询时完全将控制权交给教师，丢弃了新手计划动作中包含的关于其能力和不确定性的有价值信息。</p>
<p>本文针对“如何更高效地利用人类教学反馈，在减少查询负担的同时提升学习效果”这一痛点，提出了新视角：允许新手在不确定时主动沟通其计划动作（“我计划这样做，但我不确定”），让教师对此计划进行验证或纠正，并充分利用这一反馈信息。本文核心思路是提出主动技能级数据聚合框架，通过感知敏感性的门控、前瞻性交互经验回放和优先交互经验回放三个组件，动态平衡查询频率与失败发生率，将有效的新手动作转化为演示，并优先回放更有价值的数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>ASkDAgger框架的整体流程如算法1所示，在每一回合的每个时间步，新手策略根据当前观察和目标选择动作并量化其不确定性。SAG模块根据历史不确定性和反馈数据动态设定门控阈值。若不确定性超过阈值或以一定概率随机查询，则通过FIER模块向教师展示计划动作以获取反馈（验证、重标注或提供注释演示），否则新手自主执行动作。回合结束后，轨迹被加入演示数据集，并通过PIER模块确定数据回放的优先级来更新策略模型。</p>
<p><img src="https://arxiv.org/html/2508.05310v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ASkDAgger框架概览。包含三个核心组件：S-Aware Gating (SAG) 根据用户指定的指标（敏感性、特异性或最低成功率）动态调整不确定性门控阈值；Foresight Interactive Experience Replay (FIER) 通过验证、重标注或注释来收集教师反馈，生成演示；Prioritized Interactive Experience Replay (PIER) 根据不确定性、新手成功情况和演示年龄对回放进行优先级排序。</p>
</blockquote>
<p>**S-Aware Gating (SAG)**：该模块的核心是动态调整门控阈值γ，以追踪用户指定的指标值：敏感性（真阳性率）、特异性（真阴性率）或最低系统成功率。它将门控问题形式化为一个半监督逻辑回归问题，以不确定性u为自变量，教师反馈奖励r（验证为1，拒绝为-1，未查询为0）为指示变量。SAG维护一个包含最近不确定性、奖励和回合索引的滑动窗口，并通过线性回归对不确定性进行时间归一化，以应对策略更新带来的分布变化。随后拟合一个逻辑函数来建模负奖励（即失败）与不确定性的关系。对于未查询的时间步，则从拟合的模型中采样伪标签来估计真/假阳性率。最终，根据所选模式（如敏感性）和预设的随机查询概率p_rand，计算能够达成目标指标σ_des的阈值γ。</p>
<p><img src="https://arxiv.org/html/2508.05310v1/x2.png" alt="SAG可视化"></p>
<blockquote>
<p><strong>图2</strong>：SAG工作原理可视化。(A) 对滑动窗口内的不确定性进行时间归一化。(B) 拟合逻辑函数（红色曲线）来建模失败概率。(C) 逻辑函数参数（斜率和截距）。(D) 使用拟合模型为未查询的数据点分配伪标签（橙色十字），以估计不同阈值下的真阳性率。</p>
</blockquote>
<p>**Foresight Interactive Experience Replay (FIER)**：该模块在查询时收集教师反馈，有三种方式：1) <strong>验证</strong>：若教师认为新手计划动作合适，则直接验证（r=1），该状态-动作对可作为正例演示加入数据集。2) <strong>重标注</strong>：若教师认为动作对原目标无效，但导致了某个其他可达的目标状态，则可重标注目标（g&#39;），将失败转为针对新目标的成功演示。这借鉴了事后经验回放的思想。3) <strong>注释演示</strong>：若动作无效且无法重标注，教师提供新的动作作为演示（r=-1）。FIER显著减少了纯粹依赖教师生成新演示的负担。</p>
<p>**Prioritized Interactive Experience Replay (PIER)**：该模块在策略更新时，对演示数据集中的样本进行非均匀采样，优先级P(i)由不确定性u_i、新手成功情况（通过r_i推断）和演示年龄（当前回合索引与样本收集回合之差）共同决定。具体地，失败（r=-1）或高不确定性的样本被赋予更高优先级，新近的样本也被优先考虑。这借鉴了优先经验回放的思想，旨在更高效地利用数据，加速学习，特别是在领域变化时。</p>
<p>与现有方法相比，ASkDAgger的核心创新在于：1) <strong>利用新手计划动作</strong>：不仅用于查询决策，更将其作为可被验证、重标注的反馈对象，从而提取更多学习信号。2) <strong>目标驱动的自适应门控</strong>：SAG允许用户根据任务需求（如高安全性或高自主性）指定明确的性能指标，并自动调整阈值以实现该目标，而非固定阈值或启发式调整。3) <strong>数据生成与利用策略</strong>：FIER将有效的新手动作转化为演示，扩充数据集；PIER则优化了从数据集中学习的过程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境（RLBench的6个语言条件操作任务，如“把苹果放进碗里”）和真实世界（UR5机械臂完成“拾取-放置”和“堆叠”任务）中进行验证。使用预训练的视觉语言模型提取场景特征。新手策略为基于Transformer的动作预测网络。不确定性通过集成策略的预测方差来量化。</p>
<p><strong>对比基线</strong>：包括行为克隆、DAgger、LazyDAgger、AQ-gating（基于动作质量的主动查询）、EnsembleDAgger（基于集成不确定性的主动DAgger）、以及两种消融变体（仅SAG、仅FIER）。</p>
<p><img src="https://arxiv.org/html/2508.05310v1/x3.png" alt="模拟结果汇总"></p>
<blockquote>
<p><strong>图3</strong>：模拟环境中各方法在6个任务上的平均成功率（左）和所需教师注释数量（右）。ASkDAgger在达到相近或更高成功率的同时，所需注释显著少于大多数基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x4.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图4</strong>：真实机器人“拾取-放置”任务结果。ASkDAgger仅需约25次注释即可达到高成功率，而EnsembleDAgger需要约50次，DAgger需要超过75次。</p>
</blockquote>
<p><strong>关键定量结果</strong>：</p>
<ol>
<li><strong>减少注释负担</strong>：在模拟任务中，ASkDAgger达到约80%成功率所需注释中位数比EnsembleDAgger减少23.5%，比AQ-gating减少47.5%，比DAgger减少85.7%。</li>
<li><strong>泛化能力</strong>：在“泛化到未见过的物体颜色”实验中，ASkDAgger的成功率（85.0%）显著高于EnsembleDAgger（62.5%）和AQ-gating（57.5%），验证了FIER通过重标注增强泛化的能力（C3）。</li>
<li><strong>领域自适应</strong>：当任务从“放置到标记位置”变为“堆叠”时，ASkDAgger+PIER仅需10次新注释就能重新达到高成功率，而均匀采样回放需要20次，证明了PIER在领域变化下的快速适应能力（C4）。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.05310v1/x5.png" alt="SAG有效性"></p>
<blockquote>
<p><strong>图5</strong>：SAG在敏感性模式（σ_des=0.8）下的运行示例。门控阈值γ（上图红线）动态调整，使得实际敏感性（下图蓝线）围绕目标值（黑线）波动，实现了查询次数与失败次数间的平衡（C1）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x6.png" alt="FIER减少注释"></p>
<blockquote>
<p><strong>图6</strong>：FIER如何减少所需注释的示意图。许多查询通过验证（绿色）或重标注（黄色）解决，仅当必要时才请求新的注释演示（红色），从而降低了教师的注释负担（C2）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x7.png" alt="消融研究"></p>
<blockquote>
<p><strong>图7</strong>：消融实验（平均超过4个任务）。完整的ASkDAgger性能最佳。仅使用SAG或FIER虽能减少注释，但成功率不及完整方法。PIER的加入带来了额外提升。</p>
</blockquote>
<p><strong>组件消融实验总结</strong>：</p>
<ul>
<li><strong>SAG</strong>：有效控制了查询频率与失败率的权衡，但单独使用无法充分利用反馈数据。</li>
<li><strong>FIER</strong>：通过将新手动作转化为演示，显著减少了达到特定性能所需的注释数量，并提升了泛化。</li>
<li><strong>PIER</strong>：通过优先回放关键数据，进一步提高了成功率和在领域变化下的学习速度。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.05310v1/x8.png" alt="不确定性校准"></p>
<blockquote>
<p><strong>图8</strong>：ASkDAgger下不确定性估计的校准曲线更接近理想对角线，表明其不确定性估计更可靠。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x9.png" alt="不同门控模式"></p>
<blockquote>
<p><strong>图9</strong>：不同SAG模式（敏感性、特异性、成功率）在实现其目标指标（虚线）方面的表现。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x10.png" alt="任务细节结果1"></p>
<blockquote>
<p><strong>图10</strong>：任务“PutRubbishInBin”上各方法的成功率和注释数。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x11.png" alt="任务细节结果2"></p>
<blockquote>
<p><strong>图11</strong>：任务“PutAppleInBowl”上各方法的成功率和注释数。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x12.png" alt="任务细节结果3"></p>
<blockquote>
<p><strong>图12</strong>：任务“PutBookOnShelf”上各方法的成功率和注释数。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x13.png" alt="任务细节结果4"></p>
<blockquote>
<p><strong>图13</strong>：任务“PutKnifeOnChoppingBoard”上各方法的成功率和注释数。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x14.png" alt="任务细节结果5"></p>
<blockquote>
<p><strong>图14</strong>：任务“PutBowlOnPlate”上各方法的成功率和注释数。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x15.png" alt="任务细节结果6"></p>
<blockquote>
<p><strong>图15</strong>：任务“PutEggInFryingPan”上各方法的成功率和注释数。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x16.png" alt="泛化实验细节"></p>
<blockquote>
<p><strong>图16</strong>：泛化到未见物体颜色任务的详细成功率。ASkDAgger（绿色）在所有任务上泛化性能最好。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x17.png" alt="领域转移实验细节"></p>
<blockquote>
<p><strong>图17</strong>：领域转移（从放置到堆叠）实验中，带PIER（实线）与均匀回放（虚线）的ASkDAgger对比。PIER版本学习更快。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05310v1/x18.png" alt="真实实验补充"></p>
<blockquote>
<p><strong>图18</strong>：真实世界堆叠任务的额外结果，再次显示ASkDAgger在注释效率上的优势。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>ASkDAgger框架</strong>，其核心创新是允许新手在不确定时主动沟通计划动作，从而能够利用教师对该计划的反馈。</li>
<li>设计了<strong>S-Aware Gating</strong>模块，实现了可追踪用户指定性能指标（敏感性、特异性、成功率）的自适应门控，平衡了查询次数与系统失败。</li>
<li>提出了<strong>Foresight Interactive Experience Replay</strong>和<strong>Prioritized Interactive Experience Replay</strong>，前者通过验证和重标注将新手动作转化为演示以减少注释需求并提升泛化，后者通过优先级回放加速学习与适应。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>方法依赖于新手沟通其计划动作以获取反馈，因此最适用于<strong>中等反馈频率</strong>的场景，对于需要极高频率决策的端到端策略学习可能不实用。</li>
<li>主要针对<strong>中高层控制任务</strong>（如参数化技能的选择与参数化），假设机器人具备一组预定义的技能。</li>
</ul>
<p><strong>启示</strong>：</p>
<ul>
<li>为交互式模仿学习提供了一个更高效、更灵活的数据收集与利用范式，强调了在主动查询中<strong>双向信息交换</strong>的价值。</li>
<li>SAG模块将性能指标直接作为优化目标的设计思路，可用于其他需要安全与自主性权衡的机器人学习系统。</li>
<li>FIER中重标注的思想可进一步与课程学习、多任务学习结合，以更好地利用失败经验。未来的工作可探索将框架扩展到更低层级的控制或更复杂的任务领域。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对交互式模仿学习中人类教学负担过重的问题，提出ASkDAgger框架，其核心是利用新手策略计划中的信息（如能力与不确定性）来优化学习。关键技术包括：S-Aware Gating（SAG）动态调整查询阈值；Foresight Interactive Experience Replay（FIER）将有效的新手计划转化为示范数据；Prioritized Interactive Experience Replay（PIER）基于不确定性、成功率等因素优先回放经验。该方法在模拟和真实语言条件操作任务中验证有效，能平衡查询频率与失败率，减少标注需求，并提升泛化与适应速度。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.05310" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>