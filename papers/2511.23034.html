<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.23034" target="_blank" rel="noreferrer">2511.23034</a></span>
        <span>作者: Jianlong Fu Team</span>
        <span>日期: 2025-11-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从大规模物体操作视频中学习可迁移的隐动作是增强机器人任务泛化能力的有前景方向，因为此类表示与不同的机器人具身无关。现有方法主要依赖视觉重建目标，而忽略了物理先验，导致学习通用表示的性能欠佳。具体而言，现有隐动作模型面临三个关键挑战：第一，缺乏任务指令指导，导致隐动作无法捕捉任务相关的变化；第二，对多帧信息的利用不足，导致隐动作表示不精确，无法准确捕捉运动动态；第三，隐动作往往只关注视觉外观变化而缺乏物理感知，在隐动作表示与真实可执行动作之间产生了语义鸿沟。</p>
<p>本文针对上述痛点，提出了一种新的视角：在任务指令和多帧输入的指导下学习隐动作，并同时通过未来帧重建和动作序列预测两个目标进行优化。本文的核心思路是：将隐动作解耦为可学习的运动令牌和场景令牌以区分机器人主动运动与环境被动变化，并通过一个统一解码器联合优化视觉与动作生成，最后通过知识蒸馏将学到的物理感知隐动作迁移到最新的视觉-语言-动作模型中。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的通用隐动作学习框架LatBot包含两个关键组成部分：解耦的隐动作表示和统一解码器。整体流程是先预训练隐动作模型，然后通过知识蒸馏将学到的动作知识迁移到VLA模型中，最后通过动作专家微调生成可执行动作。</p>
<p><img src="https://arxiv.org/html/2511.23034v1/x1.png" alt="不同范式对比"></p>
<blockquote>
<p><strong>图1</strong>：隐动作建模的不同范式对比。现有方法通常忽略将机器人动作与环境变化解耦。相比之下，我们学习解耦的表示，并将隐动作解码为未来视觉帧V_{t+k}和物理动作A_{t:t+k}，从而为下游任务实现更精确和可迁移的控制。</p>
</blockquote>
<p><strong>核心模块一：解耦的隐动作表示</strong><br>为了克服现有方法将机器人引发的运动和环境引发的变化纠缠在单一表示中的问题，本文提出将隐动作Z_a分解为两个部分：运动表示Z_mot（捕捉机器人自身运动驱动的主动变化）和场景表示Z_sce（捕捉环境动态引起的被动场景变化）。这种分解减少了任务无关的噪声，并在机器人运动、环境变化和隐动作表示之间建立了更清晰的对应关系。具体实现时，利用预训练的视觉语言模型作为编码器，并引入两个可学习的隐动作令牌 <code>[CP_SCE]</code> 和 <code>[CP_MOT]</code>，在任务指令ℓ和多帧视觉输入V_{t:t+k}的指导下，分别生成结构化的场景表示和运动表示。</p>
<p><strong>核心模块二：统一隐动作解码</strong><br>为确保隐动作关注多帧间的动态变化，本文设计了一个统一解码器，以隐动作作为条件输入，联合指导未来帧重建和帧间动作生成。视觉重建约束鼓励隐动作捕捉可观察的场景变化，而动作生成目标则提供物理层面的指导，使模型能在隐动作与物理运动之间建立更紧密的联系。解码器基于预训练的SANA图像生成模型初始化，其关键创新在于引入了场景与运动表示之间的层间双向交互机制。在解码器的每一层，场景和运动表示进行信息交换与融合，使得场景动态可以指导动作生成，同时运动令牌可以细化视觉重建，实现两种模态的相互增强。最终，基于融合后的特征解码出未来视觉帧V_{t+k}和帧间动作序列A_{t:t+k}。</p>
<p><img src="https://arxiv.org/html/2511.23034v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：提出的用于VLA模型的隐动作蒸馏方法示意图。通过使用隐动作对齐损失和推理保留损失优化VLM，我们从机器人和人手演示视频中蒸馏出可泛化的动作表示，同时保持子任务规划能力。随后通过一个动作专家模块进行连续动作预测。</p>
</blockquote>
<p><strong>核心模块三：面向VLA模型的知识蒸馏</strong><br>尽管隐动作模型能学习物理基础的隐动作表示，但其能力仅限于场景重建和帧间动作生成。为了将学到的知识迁移到VLA模型，本文提出了隐动作知识蒸馏策略。给定预训练的隐动作模型（教师）和VLA中的VLM（学生），设计了两类损失函数：</p>
<ol>
<li><strong>隐动作对齐损失L_a</strong>：该损失结合了重建项（MSE）和分布对齐项（KL散度），旨在将教师模型学到的物理先验和未来帧预测能力对齐到学生的隐动作表示中。</li>
<li><strong>推理保留损失L_r</strong>：这是一个基于下一个令牌预测的自回归目标，指导学生模型基于当前帧和任务指令生成子任务描述，以保留VLM原有的语言理解和推理能力。<br>整体蒸馏目标为L = L_a + λ_r * L_r，其中λ_r用于平衡两者。蒸馏完成后，模型还需进行<strong>动作专家微调</strong>，通过附加一个动作专家模块，并使用末端执行器位姿损失（MSE）和夹爪状态损失（二元交叉熵）进行监督，最终生成精确的可执行机器人动作。</li>
</ol>
<p>与现有方法相比，本文的创新点具体体现在：1）明确提出了解耦的隐动作表示（运动/场景令牌）；2）设计了具有双向交互机制的统一解码器进行联合优化；3）提出了结合隐动作对齐与推理保留的知识蒸馏策略，将物理感知的隐动作有效迁移到VLA模型中。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：隐动作模型在包含机器人（OXE, AgiBoT）和人类手部操作（EgoDex）的混合数据集上进行预训练，总计一百万视频片段。知识蒸馏使用相同数据集。评估在三个平台上进行：SIMPLER（模拟，Google Robot和WidowX Robot）、LIBERO（模拟，四个任务套件）以及真实世界的Franka机器人。</p>
<p><strong>对比基线</strong>：包括RT-2-X、OpenVLA、π_0、SpatialVLA、RoboVLM、villa-X、DD-VLA、MemoryVLA、CogACT、UniVLA、π_0.5、Diffusion Policy、Octo、TraceVLA、RDT等众多最新VLA模型。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p>**SIMPLER Benchmark (Google Robot)**：如表1所示，在Visual Matching和Variant Aggregation两种设置下，本文方法平均成功率分别为78.0%和70.1%，均达到最佳性能，显著优于π_0（分别提升25.3%和24.1%）。<br><img src="https://arxiv.org/html/2511.23034v1/x4.png" alt="表1结果"></p>
<blockquote>
<p><strong>表1</strong>：在Google Robot的SIMPLER两个设置下，不同VLA模型在四个任务上的成功率对比。我们的方法（灰色行）在平均成功率上领先。</p>
</blockquote>
</li>
<li><p>**SIMPLER Benchmark (WidowX Robot)**：如表2所示，本文方法取得了87.5%的平均成功率，大幅领先所有基线，相比π_0.5提升32.3%，相比其他隐动作方法UniVLA和villa-X分别提升39.6%和46.7%。<br><img src="https://arxiv.org/html/2511.23034v1/x4.png" alt="表2结果"></p>
<blockquote>
<p><strong>表2</strong>：在WidowX Robot的SIMPLER (Visual Matching) 设置下，不同VLA模型在四个任务上的成功率对比。我们的方法（灰色行）优势明显。</p>
</blockquote>
</li>
<li><p><strong>LIBERO Benchmark</strong>：如表3所示，本文方法在四个任务套件上取得了98.0%的平均成功率，全面领先。尤其在最具挑战性的LIBERO-Long上达到95.4%，相比π_0.5提升3.0%，证明了方法在长视野任务上的优势。<br><img src="https://arxiv.org/html/2511.23034v1/x4.png" alt="表3结果"></p>
<blockquote>
<p><strong>表3</strong>：在四个LIBERO模拟环境上，不同VLA模型的任务成功率对比。我们的方法（灰色行）取得了最高的平均成功率。</p>
</blockquote>
</li>
<li><p>**真实世界评估 (Franka Robot)**：如表4所示，在五个需要精细操作的真实任务中，仅使用10条演示轨迹进行训练，本文方法成功完成了所有任务，平均成功率达63.3%，而π_0和π_0.5在相同设置下平均成功率仅为12.7%和20.7%，证明了强大的少样本迁移能力。<br><img src="https://arxiv.org/html/2511.23034v1/x3.png" alt="真实机器人设置"></p>
<blockquote>
<p><strong>图3</strong>：真实世界Franka机器人实验设置，配备多视角观测。任务包括拾取、插入等，需要平移和旋转运动。<br><img src="https://arxiv.org/html/2511.23034v1/x4.png" alt="表4结果"><br><strong>表4</strong>：在五个真实世界任务中，使用不同数量演示轨迹训练时的成功率对比。我们的方法（灰色行）在少样本（10条）设置下表现卓越。</p>
</blockquote>
</li>
<li><p><strong>消融实验</strong>：如表5所示，逐步添加统一动作解码器（UAD）和解耦隐动作表示（DLA）组件，性能持续提升。完整模型（Ours-v3）相比仅使用UniVLA风格的方法，平均成功率从51.0%大幅提升至87.5%，证明了每个组件的有效性。<br><img src="https://arxiv.org/html/2511.23034v1/x4.png" alt="表5结果"></p>
<blockquote>
<p><strong>表5</strong>：在SIMPLER基准上评估各组件影响的消融实验结果。UAD和DLA分别代表统一动作解码器和解耦隐动作表示。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个通用的隐动作学习框架，通过将隐动作解耦为运动令牌和场景令牌，显式区分了机器人主动运动与环境被动变化。</li>
<li>设计了一个具有双向交互机制的统一解码器，通过联合优化未来帧重建和动作序列预测，使学到的隐动作同时具备视觉动态感知和物理先验。</li>
<li>提出了一种有效的知识蒸馏策略，通过隐动作对齐损失和推理保留损失，将预训练隐动作模型中的物理感知知识迁移到VLA模型中，显著提升了其在少样本设置下的泛化与迁移能力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，隐动作模型本身的能力有限，主要限于场景重建和帧间动作生成，需要后续的知识蒸馏和动作专家微调才能用于机器人控制。</p>
<p><strong>对后续研究的启示</strong>：本文工作表明，从大规模未标注视频中蒸馏物理基础的、解耦的隐动作表示，是增强VLA模型动作感知和少样本泛化能力的有效途径。这为利用更丰富的非机器人视频数据（如人类操作视频）来训练机器人策略开辟了新方向。同时，所提出的解耦与联合优化框架也可为其他需要学习具身无关、可迁移表示的研究提供借鉴。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LatBot框架，解决现有潜在动作模型因忽视物理先验而泛化性能受限的问题。方法核心为通用潜在动作学习：以任务指令和多帧图像为输入，同时优化未来帧重建与动作序列预测，并引入动作预测（如夹爪轨迹与朝向）以学习物理先验。技术关键是将潜在动作分解为运动令牌与场景令牌，以区分机器人主动运动与环境变化。实验表明，该方法在仿真与真实机器人任务中均表现优异，仅需每个任务10条真实轨迹即可完成全部五项挑战性任务，证明了强大的少样本迁移能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.23034" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>