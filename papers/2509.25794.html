<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.25794" target="_blank" rel="noreferrer">2509.25794</a></span>
        <span>作者: Jiaojiao Fan Team</span>
        <span>日期: 2025-09-30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，评估视觉语言模型具身推理能力的主流方法主要依赖于基于图像标注的多项选择题（例如，选择哪条轨迹能更好地描述图像中的事件）或从预定义动作中选择技能，或生成基于语言的规划。这些方法存在关键局限性：它们要么假设正确答案在选项列表中，要么仅提供语言层面的规划，而忽视了将输出<strong>重新映射回视觉空间</strong>这一关键步骤。这导致无法评估模型是否能在物理世界中真正地推理和行动，因为缺乏对细粒度视觉定位和精确规划能力的考察。</p>
<p>本文针对现有基准无法评估模型<strong>精确像素级视觉定位</strong>能力的痛点，提出将视觉定位作为语言规划的自然补充，并引入其作为评估具身推理的关键且可扩展的代理任务。本文的核心思路是：提出一个名为Point-It-Out的三阶段层次化基准，通过直接提示模型根据语言描述的任务生成像素空间的边界框、点或轨迹，并与人工标注的真实值进行比对，从而系统性地评估VLMs在具身推理中的精确视觉定位能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>PIO基准采用一个三阶段的层次化评估框架，将具身推理分解为复杂度递增的三个阶段：S1（参考对象定位）、S2（任务驱动的定位）和S3（视觉轨迹预测）。该框架模拟了从简单物体检测到更复杂的可操作性预测、空间推理和任务理解的渐进过程。</p>
<p><img src="https://arxiv.org/html/2509.25794v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：具身推理中视觉定位的层次化框架。我们提出了一个三阶段的递进结构：S1（对象定位）定位文本中明确提及的对象；S2（任务驱动的定位）基于S1，推断用于特定任务的位置（可能未在文本中明确提及）；S3（视觉轨迹预测）结合S1和S2生成可执行的运动计划。下划线文本表示需要定位的参考对象（S1），黄色高亮表示任务导向推理中的任务上下文（S2/S3）。</p>
</blockquote>
<p><strong>核心模块与阶段定义：</strong></p>
<ol>
<li><strong>S1: 参考对象定位</strong>：此阶段聚焦于识别和定位语言指令中明确提及的特定物体。它与指代表达式理解任务紧密相关。指令中常包含额外的约束（如空间线索、颜色）以消除歧义，且参考粒度可能从整个物体到物体部件。S1进一步细分为三个子类：无歧义对象、带约束的对象、对象部件。这是几乎所有语言引导策略所需的最基本视觉定位能力。</li>
<li><strong>S2: 任务驱动的定位</strong>：此阶段超越了S1的显式参考定位，转向<strong>任务驱动</strong>的视觉定位：确定哪个物体或物体的哪个部分与任务相关，并精确定位交互点。与S1不同，需要定位的实体可能未在指令中明确提及，因此需要对目标对象进行推理并理解其<strong>可操作性</strong>。S2挑战模型根据上下文线索识别动作相关位置（如把手、按钮），即使这些位置未被直接提及。S2进一步细分为：可操作性、接触点、推荐/安全性子类。此阶段体现了具身视觉定位与标准计算机视觉定位的不同。</li>
<li><strong>S3: 任务驱动的视觉轨迹预测</strong>：在S1和S2的基础上，此阶段评估VLM是否能够进行相应的规划以完成任务。给定一个任务，模型必须生成一个粗略的2D视觉轨迹，概述任务应如何完成。S3引入了时间维度，要求智能体整合物体理解、可操作性推理和先前的决策，形成一个合理的运动路径。视觉轨迹是一种与低级动作解耦的重要中级策略表示。</li>
</ol>
<p><strong>创新点</strong>：与现有基准相比，PIO的核心创新在于：1) <strong>首次将精确的像素级定位（点、框、轨迹）作为评估具身推理的直接输出</strong>，而非间接的多选题或语言描述；2) 提出了一个<strong>层次化的三阶段评估协议</strong>，系统性地分解和评估从基础定位到复杂时空规划的具身推理能力；3) 基准数据覆盖了家庭、厨房、驾驶和机器人操作等多个关键具身领域，提供了丰富多样的评估场景。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验设置</strong>：PIO基准包含来自四个关键领域（家庭房间、厨房环境、驾驶场景、机器人操作任务）的600多个数据点。S1和S2阶段从Where2Place、EPIC-Kitchens、BDD100K、AgiBot和RT-1五个数据集中收集了501个问答对（约230个S1，270个S2）。S3阶段从AgiBot、DROID和RT-1数据集中提取图像帧，收集了100个针对机器人手臂视觉轨迹预测任务的问题。所有数据点均由人工使用多边形分割工具进行标注。</p>
<p><strong>评估模型</strong>：实验评估了超过十种先进的VLM，包括通用VLM（如GPT-4o、Claude-3.7、Gemini 2.0/2.5、Qwen2.5-VL、MoLMO-7B）、强推理模型（如GPT-o3）以及专门针对定位任务微调的模型（如RoboRefer、MolmoAct）。模型根据输出格式分为两类：输出边界框的模型（如GPT系列、Claude、Gemini、Qwen）和输出点的模型（如MoLMO、RoboPoint）。</p>
<p><strong>评估指标</strong>：对于S1和S2，提出了一个归一化的IoU度量，以公平比较不同输出格式（点 vs. 边界框）。对于S3，使用两种方法评估视觉轨迹质量：人工评分（1-5分）和基于GPT-o4-mini的评估（根据预定义标准，如轨迹整体方向、关键点覆盖、任务可行性）。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2509.25794v1/x9.png" alt="S1和S2总体性能"></p>
<blockquote>
<p><strong>图5</strong>：各模型在S1和S2任务上的得分。RoboRefer-SFT-8B、MoLMO-7B、Gemini-2.5-Pro和Qwen-2.5-VL显著优于其他模型。右侧图展示了不同模型在四个不同场景下的S1和S2平均得分。</p>
</blockquote>
<p><strong>发现1</strong>：在S1和S2上，包含显式定位监督的模型（RoboRefer, MoLMO-7B, Gemini-2.5-Pro, Qwen-2.5-VL）表现最佳，优于GPT-4o和Claude-3.7等通用VLM。这强调了当需要精确空间推理时，定位数据的重要性。</p>
<p><img src="https://arxiv.org/html/2509.25794v1/x10.png" alt="S1和S2子类性能"></p>
<blockquote>
<p><strong>图6</strong>：各子类性能。左图：在S1中，所有模型在定位<strong>物体部件</strong>时准确率明显下降。右图：在S2中，<strong>接触点</strong>和<strong>可操作性</strong>子类最具挑战性，而依赖于简单语言推理的<strong>推荐</strong>子类相对容易。</p>
</blockquote>
<p><strong>发现2</strong>：(i) 所有模型从S1到S2性能均有明显下降。(ii) 两个关键的具身技能表现最差：在S1中，模型经常错过正确的物体部件；在S2中，模型在可操作性和接触点预测上存在困难。</p>
<p><img src="https://arxiv.org/html/2509.25794v1/x12.png" alt="S3可视化与得分"></p>
<blockquote>
<p><strong>图8</strong>：S3视觉轨迹预测的可视化及得分。Gemini-2.5-Pro、MolmoAct和GPT-o3优于其他模型，而MoLMO和Qwen尽管在S1和S2表现强劲，但在时空视觉轨迹预测上表现挣扎。</p>
</blockquote>
<p><strong>发现3</strong>：S3要求模型将单目标定位整合为连贯的视觉轨迹生成。S1和S2是必要前提，但不足以让模型在S3中成功。Gemini-2.5-Pro在S3中表现出色，且在S1和S2中也表现良好。相反，在S1和S2表现顶尖的MoLMO和Qwen-2.5-VL却在S3中失败。这表明，尽管它们能有效定位特定目标，但难以将这种能力扩展到多步骤、时间连贯的规划中。</p>
<p><strong>消融实验分析</strong>：论文通过对比不同模型在不同阶段和子类上的表现，间接完成了“消融实验”。结果表明：1) <strong>定位数据微调</strong>对提升S1/S2性能至关重要；2) <strong>强大的通用预训练</strong>（如GPT、Gemini）对于需要复杂联合推理的S3任务更有优势；3) <strong>模型规模与能力不绝对正相关</strong>，例如7B的MoLMO在S1/S2优于更大的通用模型，但在S3落后。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>首次提出将<strong>精确的像素级视觉定位</strong>作为评估具身推理能力的关键且可扩展的代理任务，弥补了现有基准的不足。</li>
<li>构建了<strong>PIO基准</strong>，这是一个包含三阶段（S1/S2/S3）层次化评估协议、覆盖多领域、包含600+人工标注数据点的系统性评估工具。</li>
<li>通过对十余种先进VLM的大规模评估，揭示了当前模型在具身视觉定位能力上的<strong>关键局限</strong>，例如在物体部件定位、可操作性/接触点预测以及将定位与轨迹规划整合方面的显著不足，为未来研究指明了方向。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，PIO专注于2D像素坐标，将其作为从控制动力学中分离核心具身推理的可扩展、经济高效的代理任务。这隐含了其评估未涉及3D空间或低级控制闭环的局限性。此外，数据规模（600+）相对于一些大型语言或纯视觉基准可能较小。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据需求</strong>：研究结果表明，要提高VLMs的具身推理能力，需要收集更多针对<strong>细粒度定位</strong>（如部件、可操作性）和<strong>视觉轨迹规划</strong>的数据用于训练。</li>
<li><strong>模型设计</strong>：未来的VLM设计需要更好地平衡<strong>基础定位能力</strong>与<strong>复杂时空推理能力</strong>。专门化的定位模型与通用强大的推理模型各有优劣，如何结合二者优势是一个值得探索的方向。</li>
<li><strong>评估范式</strong>：PIO倡导的“直接输出视觉空间结果”的评估范式，为更真实、可解释地评估具身AI系统提供了新思路，可能推动更多类似基准的出现。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有基准无法直接评估视觉语言模型（VLMs）在具身推理中精确视觉接地能力的问题，提出Point-It-Out（PIO）基准。该基准采用分层评估协议（S1参考对象定位、S2任务驱动指向、S3视觉轨迹预测），覆盖室内、厨房等多领域场景，实现像素级视觉接地。实验对十余个先进VLMs测试发现：通用模型如GPT-4o在精确视觉接地方面表现不及部分开源模型；MoLMO在S1和S2表现良好，但在需结合视觉轨迹规划的S3阶段表现挣扎。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.25794" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>