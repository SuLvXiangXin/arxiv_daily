<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.03167" target="_blank" rel="noreferrer">2511.03167</a></span>
        <span>作者: Feng Gao Team</span>
        <span>日期: 2025-11-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>六足机器人凭借其多足与环境的交互，在复杂地形导航中具有增强的稳定性。然而，如何在更大的动作探索空间中有效协调多条腿以生成自然且鲁棒的运动是一个关键问题。当前，针对双足和四足机器人的运动控制器主要分为基于模型的方法和数据驱动的无模型方法。基于模型的方法依赖于简化的环境与机器人动力学模型，在非结构化或未知地形中容易失败。无模型的深度强化学习（DRL）方法在复杂环境中被证明更具鲁棒性，已广泛应用于双足和四足机器人。然而，由于腿的数量增加导致动作空间更大、收敛更困难、奖励函数设计更复杂，尚无DRL算法能有效应用于真实六足机器人，使其在复杂地形上实现自然且鲁棒的步态。</p>
<p>本文针对的痛点是：缺乏一个能让真实六足机器人仅凭本体感知（无视觉信息）在复杂地形中学习自然、鲁棒步态的DRL框架。本文提出了一种基于运动先验的新视角，核心思路是：首先通过轨迹优化在平地上生成高质量的三脚架步态运动先验数据集，然后训练一个对抗判别器来提供风格奖励，引导DRL策略学习自然的步态模式，最终通过一个非对称的Actor-Critic框架将训练好的策略零样本迁移到真实机器人上。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架是一个结合了运动先验引导的非对称深度强化学习（DRL）训练流程。输入是目标基座速度命令和机器人本体感知信息，输出是关节位置偏移量，与默认关节位置相加后得到期望关节位置，再经由低级的CSP（串级PD）控制器计算扭矩输出给执行器。</p>
<p><img src="https://arxiv.org/html/2511.03167v1/method7.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：非对称Actor-Critic强化学习框架。Critic网络接收特权信息（包括地形扫描点）以评估策略，而Actor网络仅能访问本体感知观测、上一时刻动作和速度命令。策略输出关节位置偏移，与默认位置相加后送至低层控制器。风格奖励由基于运动先验的对抗判别器提供。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>运动先验生成模块</strong>：使用轨迹优化（TO）方法在仿真中的平地上生成一系列运动轨迹，覆盖前进、后退、横向移动、转向及组合运动，并始终保持一致的三脚架步态周期。这些轨迹构成先验数据集 $\mathcal{D}$，其状态转移对 $(s_t, s_{t+1})$ 将作为真实样本用于训练判别器。</li>
<li><strong>对抗判别器模块</strong>：其作用是提供“风格奖励” $r_t^s$。判别器 $D_{\varphi}$ 被训练以区分一个状态转移对是来自先验数据集 $\mathcal{D}$ 还是来自当前策略 $\pi$。策略通过生成能够“欺骗”判别器（即让判别器认为其输出像先验数据）的状态转移来获得高奖励，从而学习模仿先验数据中的三脚架步态风格。判别器采用最小二乘GAN的损失函数，并加入了梯度惩罚项以稳定训练。风格奖励的计算公式为 $r_{t}^{s} = \max\left[0,1-0.25\left({D_{\varphi}}({T}_{s})-1\right)^{2}\right]$。</li>
<li><strong>非对称Actor-Critic网络</strong>：<ul>
<li><strong>观察与动作空间</strong>：Critic的输入包括完整的本体感知数据、上一动作、目标速度、特权状态数据（如基座速度、高度、地面摩擦、足端接触力、外部扰动等）以及地形高程扫描点（187维）。Actor的输入则仅限于本体感知数据、上一动作和目标速度。动作是18维的关节位置偏移量。</li>
<li><strong>网络架构</strong>：Critic网络使用两个编码器分别处理地形信息和特权数据，将其编码为潜变量，再与Actor的观测输入一同输入MLP计算价值函数 $V_t$。Actor网络内部包含一个状态估计器（从过去5步本体感知估计线速度）和一个短期记忆编码器（压缩过去观测以推断地形特征），最后通过一个低层MLP输出动作。</li>
</ul>
</li>
<li><strong>奖励函数设计</strong>：总奖励 $r_t = r^g_t + r^s_t + r^l_t$，由三部分组成：<ul>
<li>**任务奖励 $r^g_t$**：鼓励跟踪目标线速度和角速度。</li>
<li>**风格奖励 $r^s_t$**：如上所述，由对抗判别器提供，鼓励学习三脚架步态。</li>
<li>**惩罚项 $r^l_t$**：包括对基座垂向速度、滚转/俯仰角速度（维持稳定）、关节扭矩与加速度（减少电机压力、节能）、动作变化率（运动平滑）、超过阈值的关节扭矩/速度、碰撞以及过大接触力的惩罚，旨在促进运动稳定性、平滑性和安全性。</li>
</ul>
</li>
</ol>
<p>与现有方法相比，本文的创新点具体体现在：<strong>首次将对抗运动先验（AMP）应用于六足机器人控制</strong>，以解决高维动作空间下自然步态难以通过手工奖励函数设计的问题；并采用了<strong>非对称Actor-Critic框架</strong>，在仿真训练中利用特权信息和地形信息加速Critic学习，而部署时Actor仅需本体感知，实现了从仿真到现实的零样本迁移。此外，还引入了<strong>动态参数随机化</strong>（如表II所示）以增强策略的鲁棒性和sim-to-real的泛化能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：仿真训练在IsaacGym中进行，并行训练4096个机器人实例。硬件平台是一个自重25.5公斤、具有18个自由度（每条腿3个关节）的对称设计六足机器人。</p>
<p><strong>对比的Baseline方法</strong>：</p>
<ol>
<li><strong>Baseline</strong>：一个没有环境特权信息的策略。</li>
<li><strong>Concurrent</strong>：Actor网络无地形信息输入，并同时训练一个状态估计器网络。</li>
<li><strong>RMA</strong>：采用师生框架训练，无专家先验。</li>
<li><strong>MPC</strong>：基于OCS2的模型预测控制器，并微调了抬腿高度、机身高度和步态。</li>
</ol>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>消融实验（奖励项设计）</strong>：论文比较了仅使用 $r^g_t + r^s_t$、仅使用 $r^g_t + r^l_t$ 以及使用全部三项奖励 $r^g_t + r^s_t + r^l_t$ 的策略性能。</p>
<p><img src="https://arxiv.org/html/2511.03167v1/sin_track5.png" alt="平地对正弦速度命令的跟踪性能对比"></p>
<blockquote>
<p><strong>图3</strong>：在平地上跟踪正弦速度命令的对比。(a)-(c)显示，缺少风格奖励 $r^s_t$ 的策略（$r^g_t + r^l_t$）在速度跟踪上出现显著抖动和偏差，导致不自然行为（见(h)）。(d)-(f)显示，该策略在垂向速度和姿态角上也出现严重偏离，表明风格奖励有助于学习更接近参考步态的稳定行为。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.03167v1/Smooth_slopes_level.png" alt="不同策略在不同地形上的穿越能力对比"></p>
<blockquote>
<p><strong>图4</strong>：策略在不同难度地形上的穿越能力迭代曲线。纵轴为地形难度，横轴为训练迭代次数。结果显示，包含风格奖励的策略（$r^g_t + r^s_t$ 和 $r^g_t + r^s_t + r^l_t$）能让机器人更快地穿越更难的地形，并达到更高的难度等级。仅使用任务奖励和惩罚的策略（$r^g_t + r^l_t$）表现较差。</p>
</blockquote>
<p>消融实验总结：风格奖励 $r^s_t$ 对于学习自然、稳定的三脚架步态至关重要；而惩罚项 $r^l_t$ 在训练后期有助于机器人穿越更极端的地形。两者结合效果最佳。</p>
</li>
<li><p><strong>鲁棒性实验（抗扰动能力）</strong>：在仿真平地上对机器人施加随机速度扰动，测试各控制器在不导致机器人跌倒的前提下所能承受的最大扰动范围。</p>
<table>
<thead>
<tr>
<th align="left">控制器</th>
<th align="left">Y轴方向扰动容忍范围 [Min, Max] (m/s)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Ours</strong></td>
<td align="left"><strong>[-0.803, 0.803]</strong></td>
</tr>
<tr>
<td align="left">RMA</td>
<td align="left">[-0.738, 0.738]</td>
</tr>
<tr>
<td align="left">Concurrent</td>
<td align="left">[-0.463, 0.463]</td>
</tr>
<tr>
<td align="left">Baseline</td>
<td align="left">[-0.201, 0.201]</td>
</tr>
<tr>
<td align="left">MPC</td>
<td align="left">[-0.112, 0.112]</td>
</tr>
</tbody></table>
<p>结果表明，本文提出的控制器具有最强的抗扰动能力，尤其是在较弱的Y轴方向上。</p>
</li>
<li><p><strong>真实世界复杂地形测试</strong>：在室内楼梯（高3-20cm）、斜坡（坡度5°-30°）和户外草地（包含15cm台阶）上测试各控制器的成功率（完成10次测试的成功率）。</p>
<p><img src="https://arxiv.org/html/2511.03167v1/Ascending_success_rate.png" alt="不同控制器在爬楼梯、下楼梯、斜坡和草地上的成功率"></p>
<blockquote>
<p><strong>图8/9/10/11</strong>：不同控制器在各种真实复杂地形上的成功率柱状图。本文方法（Ours）在爬楼梯、下楼梯、爬坡和草地穿越任务中均取得了100%或接近100%的成功率，显著优于所有对比基线方法。这证明了其策略在未知、非结构化地形（如草地）上的强大泛化能力。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文的核心贡献</strong>：</p>
<ol>
<li>提出了一种将运动先验与深度强化学习相结合的新方法，通过轨迹优化生成先验数据集，并利用对抗判别器引导策略学习自然的三脚架步态，首次成功地将DRL控制器应用于真实六足机器人在复杂地形上的“盲运动”（仅凭本体感知）。</li>
<li>设计了一个非对称的Actor-Critic强化学习框架，结合了状态估计器和短期记忆编码器，使得在仿真中利用特权信息训练的策略能够零样本迁移到真实机器人，无需微调。</li>
<li>通过详实的仿真与实物实验，验证了该方法在步态自然性、运动鲁棒性以及复杂地形适应能力上均优于现有的先进RL控制器和模型预测控制器。</li>
</ol>
<p><strong>论文自身提到的局限性</strong>：控制器未使用外感知（如视觉或雷达），因此机器人无法获取前方的地形数据，这限制了其在完全未知和需要前瞻性地形中的性能。</p>
<p><strong>对后续研究的启示</strong>：本文证明了对抗运动先验在解决多足机器人高维控制问题上的有效性。未来的工作可以探索将视觉等外感知信息融入此框架，以进一步提升机器人在完全未知和动态环境中的自主导航与适应能力。此外，该方法为其他高自由度机器人的自然运动技能学习提供了可借鉴的范式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对六足机器人在复杂地形上协调多腿生成自然且鲁棒运动的核心问题，提出基于运动先验的深度强化学习方法。关键技术包括：通过轨迹优化生成平地运动数据作为先验，训练对抗判别器以指导自然步态学习，并设计不对称DRL框架训练控制器。实验表明，学习策略成功转移到真实六足机器人，在无视觉信息下实现复杂地形的自然行走，展现出显著鲁棒性，是强化学习控制器在真实六足机器人上的首次应用。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.03167" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>