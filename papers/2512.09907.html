<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VisualActBench: Can VLMs See and Act like a Human? - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>VisualActBench: Can VLMs See and Act like a Human?</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.09907" target="_blank" rel="noreferrer">2512.09907</a></span>
        <span>作者: Zhang, Daoan, Liu, Pai, Zhou, Xiaofei, Ge, Yuan, Lan, Guangchen, Bi, Jing, Brinton, Christopher, Hoque, Ehsan, Luo, Jiebo</span>
        <span>日期: 2025/12/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）在感知和描述视觉环境方面取得了显著进展。然而，它们在仅凭视觉输入、无需显式文本指令的情况下，进行主动推理和行动的能力仍未得到充分探索。主流VLM任务（如图像描述、视觉问答）通常依赖于文本提示来引导模型响应，这本质上是一种被动的、语言中介的交互模式。这与人类主要通过视觉感知环境并主动发起行动的方式存在根本差异。本文针对两个具体痛点展开研究：第一，VLMs能否在纯视觉输入的开放世界场景中模仿人类的决策过程，生成准确且情境感知的指令？第二，VLMs生成指令时，其行为是否与普遍的人类价值体系一致？本文的核心思路是提出一个名为“视觉行动推理”的新任务及相应基准VisualActBench，以评估VLMs仅从视觉输入中提取线索、并以类人的方式推理出最合适行动的能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的核心贡献是提出了VisualActBench基准，用于评估模型的视觉行动推理能力。该基准严格以视觉为中心，所有生成适当响应所需的信息都仅嵌入在视觉输入中，不依赖任何显式文本指令。</p>
<p><strong>整体框架与基准构建</strong>：VisualActBench包含1,074个视频和3,733个人工标注的行动，覆盖四个具有代表性的真实世界场景：动态导航、家庭服务、安全监控和人机交互。视频从Kinetics、Moments in Time、DADA-2000和UCF-Crime数据集中采样，并尽量选择时长不长、仅包含单一场景的片段，以避免行动过多和行动排序的复杂性。每个标注的行动都附有一个<strong>行动优先级级别</strong>和一个<strong>主动/被动类型</strong>标签。APL是一个0到4的度量，旨在反映基于情境适当性的人类偏好级别（例如，0级为不重要或无关，4级为至关重要）。主动行动是指模型主动发起的、预防性或改善性的行为，而被动行动则是对已发生事件的反应。</p>
<p><img src="https://arxiv.org/html/2512.09907v2/x3.png" alt="基准分布"></p>
<blockquote>
<p><strong>图3</strong>：VisualActBench中视频和行动的分布情况。左图：按场景类型（动态导航、家庭服务、安全监控、人机交互）和行动优先级级别（APL 0-4）分类的视频数量。右图：按主动性（主动 vs 被动）和APL分类的行动数量。</p>
</blockquote>
<p><strong>评估指标</strong>：为了全面评估模型性能，论文采用了一套结合文本对齐和人类对齐推理质量的指标。首先，基于句子嵌入的余弦相似度，使用匈牙利算法在设定阈值（如τ=0.5）下匹配预测行动与真实行动，计算精确率、召回率和F1分数。其次，为了融入人类价值对齐，提出了<strong>加权F1</strong>变体，在计算时考虑预测行动与真实行动重要性级别（即APL）的接近程度。匹配权重定义为 ( w_{i,j} = 1 - \frac{|s_i - s_j|}{4} )。最后，引入<strong>平均尺度分数</strong>，专门衡量预测行动与真实行动在优先级级别上的接近程度，即使文本相似，优先级不匹配也会受到惩罚。所有指标均在四个场景类别上取平均，以确保公平性。</p>
<p><strong>创新点</strong>：与传统的视频描述或问答基准相比，VisualActBench的创新性体现在其严格的视觉中心性、强调行动推理生成以及通过APL和主动/被动分类来评估人类价值对齐。它要求模型自主分析场景、预测情境期望的结果并采取相应行动，而非响应文本提示。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在VisualActBench上评估了29个VLMs，包括24个开源模型和5个专有模型。评估涵盖了四个场景，并使用上述的精确率、召回率、F1、加权F1以及尺度分数等指标。</p>
<p><strong>基线方法与关键结果</strong>：表2展示了主要模型的综合评估结果（分数缩放至100）。GPT-4o取得了最高的总体F1分数（36.7），显著优于其他模型，如InternVL2.5-38B（33.7）和GPT-4o-mini（32.7）。然而，即使是表现最佳的模型，其分数也远低于满分，表明与人类水平的推理存在显著差距。许多流行的开源VLM的总体F1得分低于20，暴露出在生成高优先级或主动行动方面的根本局限。这些模型通常表现出高精确率但低召回率，倾向于输出保守或通用的行动，而错过了更关键或情境敏感的响应。</p>
<p><img src="%E6%95%B0%E6%8D%AE%E4%BB%A5%E8%A1%A8%E6%A0%BC%E5%BD%A2%E5%BC%8F%E5%91%88%E7%8E%B0%E4%BA%8E%E6%AD%A3%E6%96%87%E4%B8%AD%EF%BC%8C%E6%AD%A4%E5%A4%84%E4%B8%BA%E6%8F%8F%E8%BF%B0" alt="主要结果表"></p>
<blockquote>
<p><strong>表2</strong>：各模型在四个场景及总体上的精确率、召回率、F1分数（已缩放至100）。GPT-4o总体表现最佳，但所有模型分数均不高，显示出现有VLMs在视觉行动推理任务上普遍存在困难。</p>
</blockquote>
<p><strong>尺度分数评估</strong>：表3显示了各模型的平均尺度分数。GPT-4o同样以66.4的总体分数领先。安全监控场景通常得分较高，可能因为其视觉线索更明确；而人机交互场景则普遍具有挑战性，反映了理解意图和社会背景的难度。尺度分数指标凸显了模型在价值敏感推理方面的差异。</p>
<p><img src="%E6%95%B0%E6%8D%AE%E4%BB%A5%E8%A1%A8%E6%A0%BC%E5%BD%A2%E5%BC%8F%E5%91%88%E7%8E%B0%E4%BA%8E%E6%AD%A3%E6%96%87%E4%B8%AD%EF%BC%8C%E6%AD%A4%E5%A4%84%E4%B8%BA%E6%8F%8F%E8%BF%B0" alt="尺度分数表"></p>
<blockquote>
<p><strong>表3</strong>：各模型在四个场景及总体上的平均尺度分数（0-100）。该分数衡量预测行动与真实行动在优先级级别上的对齐程度，GPT-4o同样表现最好。</p>
</blockquote>
<p><strong>模型主动性分析</strong>：<br><img src="https://arxiv.org/html/2512.09907v2/images/proactiveness.png" alt="主动性比率"></p>
<blockquote>
<p><strong>图4</strong>：各VLMs的归一化主动比率，反映了它们在整个评估集中完成预期主动行动的倾向。红色虚线表示所有评估模型的平均主动比率。只有少数模型（如InternVL2.5-8B-MPO, Gemini-1.5-Flash）的主动覆盖率超过70%，大多数模型难以识别需要主动行为的情境。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ol>
<li><strong>输入帧数的影响</strong>（表4）：增加输入帧数并不总是提升性能。对于LLaVA-OV，帧数从2增至16导致F1和APL分数持续下降，可能是因为冗余信息稀释了模型对核心事件的关注。VideoLLaMA2在不同帧数下表现更稳定。</li>
<li><strong>模型规模的影响</strong>（表5）：在LLaVA-OV、QwenVL和InternVL等模型系列中，性能通常随模型规模增大而提升。例如，LLaVA-OV从0.5B的近乎零性能提升到72B时的F1 28.3。然而，收益并非线性，InternVL-78B的性能反而低于38B版本，暗示在极端规模下可能出现收益递减或过拟合。</li>
<li><strong>强化学习的影响</strong>（表6）：通过混合偏好优化（MPO）应用强化学习（RL）通常能改善模型性能，尤其是在大型模型中。例如，InternVL-38B的F1从33.7提升至40.7，APL也从57.9提升至58.2，表明RL有助于增强情境意识和与人类偏好行动优先级的一致性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>视觉行动推理</strong>这一新任务，挑战VLMs仅从视觉输入生成情境感知、主动行动的能力。</li>
<li>引入了<strong>VisualActBench</strong>基准，包含大量带有行动优先级和主动/被动类型标注的真实世界视频，为评估VLMs的价值对齐和行为质量提供了全面基础。</li>
<li>对29个先进VLM进行了全面评估，揭示了它们在主动性、价值对齐和抽象推理方面与人类表现的显著差距，为未来面向现实世界部署的模型发展提供了关键见解。</li>
</ol>
<p><strong>局限性</strong>：论文明确指出，即使是最先进的模型（如GPT-4o），在VisualActBench上的表现也远未达到人类水平。模型普遍在需要理解抽象意图、预测长期结果或考虑社会规范的情境（如家庭服务、人机交互）中表现不佳。它们倾向于生成被动、低优先级的响应，缺乏人类决策中的主动性和价值敏感性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>超越感知，聚焦推理与行动</strong>：未来的VLM研究需要从被动视觉理解转向主动的、基于视觉的决策制定。模型架构和训练目标应鼓励对“应该做什么”进行推理，而不仅仅是“发生了什么”。</li>
<li><strong>融入价值对齐</strong>：为了在现实世界中安全、负责任地部署，VLMs需要内化人类的价值体系、伦理考虑和社会规范。这需要在训练数据、损失函数或后处理阶段明确引入价值对齐机制。</li>
<li><strong>优化时序理解</strong>：实验表明，简单地增加视频帧数可能无益甚至有害。需要开发更智能的时序编码和推理机制，使模型能够从视频流中提取因果相关的关键线索，避免信息冗余。</li>
<li><strong>探索有效的规模化与训练策略</strong>：虽然模型规模扩大通常带来性能提升，但存在收益递减点。未来工作应探索更高效的架构、高质量的训练数据以及像强化学习这样的高级训练策略，以持续提升模型的主动推理能力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型（VLMs）在仅凭视觉输入、无明确文本指令时，能否像人类一样主动推理和行动的核心问题，提出了新任务“视觉行动推理”并构建了大规模基准VisualActBench。该基准包含1,074个视频和3,733个人工标注动作，通过“行动优先级”和“主动/反应类型”标签评估模型的人类对齐推理与价值敏感性。实验评估了29个VLMs，发现即使GPT-4o等前沿模型表现相对较好，但在生成主动、高优先级行动方面仍与人类水平存在显著差距，凸显了现有模型在理解复杂语境、预测结果和匹配人类决策框架上的不足。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.09907" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>