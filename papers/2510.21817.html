<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.21817" target="_blank" rel="noreferrer">2510.21817</a></span>
        <span>作者: Liu, Xiaoyu, Fu, Chaoyou, Yan, Chi, Wu, Chu, Gao, Haihan, Zhang, Yi-Fan, Dong, Shaoqi, Qian, Cheng, Luo, Bin, Yang, Xiuyong, Li, Guanwu, Cai, Yusheng, Shen, Yunhang, Jiang, Deqiang, Cao, Haoyu, Sun, Xing, Shan, Caifeng, He, Ran</span>
        <span>日期: 2025/10/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前的视觉-语言-动作（VLA）模型通常受限于一种僵化、静态的交互范式，缺乏像人类一样同时观察环境、聆听用户语音、提供口头回应和执行动作的能力，也无法动态处理用户的实时中断。这阻碍了无缝的人机协作，导致了不灵活、响应迟钝的用户体验。现有系统存在三个关键局限：1）缺乏并发性，无法在感知的同时并行执行和回应；2）不可中断性，一旦开始行动或回应便被锁定，无法响应用户的紧急需求；3）交互不灵活，整体体验生硬。</p>
<p>本文旨在突破这一范式瓶颈，针对实现自然、动态的人机协作这一具体痛点，提出了一个支持行为并发和近实时中断的新颖交互框架。核心思路是采用一个双模型并行架构，其中一个VLA实例作为“活跃模型”专注于当前任务，另一个作为“待机模型”随时准备处理新请求，并通过让VLM生成特殊令牌作为系统级命令，实现模型推理与系统行为的紧耦合。</p>
<h2 id="方法详解">方法详解</h2>
<p>VITA-E框架围绕两个核心原则设计：1）“模型即控制器”范式，即VLM通过生成显式命令令牌来驱动系统行为；2）支持实时中断和并发的双模型交互核心。整体采用当前主流的双系统架构，包含用于高层理解的VLM（系统2）和用于底层运动控制的扩散动作专家（系统1）。</p>
<p><img src="https://arxiv.org/html/2510.21817v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图3</strong>：VITA-E双模型框架中活跃模型的逻辑架构与运行状态。两个模型均可在“活跃”与“待机”状态间切换。当一个模型活跃时，其VLM部分充当控制器，在“聆听”状态处理用户输入，并生成可触发向“动作”状态转换的特殊令牌，在该状态下，它与动作专家协作作为完整的VLA模型工作。</p>
</blockquote>
<p>整体流程主要在两种状态间切换：<strong>聆听</strong>和<strong>动作</strong>。默认状态下，系统处于聆听状态，VLM处理图像和用户语言以判断意图。根据理解，它可以生成 <code>[RES]</code> 令牌进行纯语音回应，或生成 <code>[ACT]</code> 令牌以执行物理任务。生成 <code>[ACT]</code> 令牌作为一个直接命令，将系统切换到动作状态。在此状态下，VLM与动作专家协作生成底层运动命令，直到任务完成（由 <code>[END]</code> 令牌标记）或被另一个模型中断。</p>
<p><strong>核心模块一：基于特殊令牌的控制语言</strong><br>关键创新是让VLM不仅输出语义理解，还通过学习的“控制语言”输出显式的系统级命令。VLM接收视觉输入和用户指令，输出一个结构化字符串 <code>S_t = (c_t, L_t^{robot}, C_t^{robot})</code>，其中 <code>c_t</code> 是离散控制令牌，<code>L_t^{robot}</code> 是语音回应，<code>C_t^{robot}</code> 是动作指令（除非 <code>c_t</code> 表明不需要动作）。论文设计了一套特殊令牌（如下表），并提出了专门的数据整理和微调策略来训练VLM生成这些令牌。该策略将标准具身数据重新格式化和合成增强，以明确教导VLM基于上下文输出控制令牌，例如为问题类指令添加 <code>[RES]</code>，为操作任务添加 <code>[ACT]</code> 和 <code>[INST]</code>，并合成中断数据以生成 <code>[HALT]</code>。</p>
<p><strong>核心模块二：实现动态交互的双模型架构</strong><br>VITA-E交互性的核心在于其独特的双模型架构，灵感来源于大脑半球的协作机制。一个模型实例作为<strong>活跃模型</strong>专注于执行当前任务，另一个作为<strong>待机模型</strong>充当观察者。两者通过同步原语协调，赋予待机模型干预活跃模型的权限。该架构支持四种主要的交互模式：</p>
<p><img src="https://arxiv.org/html/2510.21817v1/x3.png" alt="四种交互模式序列图"></p>
<blockquote>
<p><strong>图4</strong>：四种主要交互模式的序列图。模型I是活跃模型，模型II是待机模型。V和A分别代表语音和动作生成。待机模型可以并行处理新请求，或抢占活跃模型以处理中断和任务切换。</p>
</blockquote>
<ol>
<li><strong>并发</strong>：如图4(a)，当活跃模型执行动作时，新语音请求由待机模型独立处理并生成语音回应，实现“边做边说”。</li>
<li><strong>语音中断</strong>：如图4(b)，当活跃模型生成语音回应时，新输入路由至待机模型，后者发出抢占信号立即终止活跃模型的语音生成，实现灵活的自然对话轮换。</li>
<li><strong>动作切换</strong>：如图4(c)，当活跃模型执行任务A时，新任务B指令由待机模型捕获，后者抢占并停止任务A，然后自身转为活跃模型执行任务B。为确保安全切换，采用了回缩机制，使机器人按存储的动作栈顺序执行逆运动回到初始姿势。</li>
<li><strong>紧急停止</strong>：如图4(d)，用户发出停止请求时，待机模型以最高优先级处理，生成 <code>[HALT]</code> 令牌并立即抢占活跃模型，中断其动作生成循环并发送最终停止命令。</li>
</ol>
<p><strong>动作专家模块</strong><br>动作专家将VLM的高层语义目标转化为底层、可执行的轨迹。论文采用GR00T的扩散变换器作为动作专家，并在其上应用两阶段训练范式：首先在大规模具身数据上预训练，然后在目标机器人采集的任务特定数据上微调（仅训练投影头），以适应特定运动学并防止过拟合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Fourier GR2人形机器人平台上进行，使用头戴式RealSense D455 RGB相机的第一人称视觉和机器人本体感觉状态作为输入。评估了基础操作任务和核心交互能力。</p>
<p><strong>基础操作任务评估</strong></p>
<ul>
<li><strong>仿真实验</strong>：在Libero基准测试（包含Spatial, Object, Goal, LONG四个任务套件）上，与GR00T基线比较。VITA-E成功完成了大部分任务，但在成功率上存在差距（见图5）。论文指出，这是由于GR00T使用了大规模具身数据端到端联合优化VLM视觉部分，而VITA-E的VLM是完全冻结的，差距在预期之内。</li>
<li><strong>真机实验</strong>：在“拿起罐子”和“拿起玩具放入篮子”两个基础拾放任务上，与π0、Diffusion Policy、GR00T、SmolVLA等SOTA模型比较。VITA-E仅微调动作投影头，结果如图6所示，其成功率与SOTA模型相当，证明了其具备可靠的核心操作能力。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.21817v1/x4.png" alt="Libero基准测试成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：VITA-E与GR00T在Libero基准测试上的成功率对比。VITA-E的目标并非追求SOTA性能，而是证明其具备完成具身任务的能力并提供量化指标。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.21817v1/x5.png" alt="真实机器人基础操作任务成功率对比"></p>
<blockquote>
<p><strong>图6</strong>：VITA-E与基线模型在两个基础操作任务上的成功率对比（30次试验）。结果显示VITA-E是一个高性能的操作模型，其性能与SOTA模型相当。</p>
</blockquote>
<p><strong>核心交互任务评估</strong><br>评估了语音-动作并发、语音中断、任务切换和紧急停止四项能力。并发任务定性评估显示，VITA-E能持续提供语音答案同时平滑执行操作任务，平均语音响应延迟为2.26秒。其他任务的定量结果如表2所示，语音中断和紧急停止成功率均达100%，任务切换成功率为93.3%。少数失败案例源于VLM偶尔未能将新指令识别为动作命令，仅提供了语音回应。</p>
<p><strong>消融实验</strong><br>为验证微调策略的有效性，比较了微调后的VITA-E VLM与其基础模型VITA-1.5在生成正确机器人响应方面的准确性。结果如表3所示，基础模型在生成有效动作命令和紧急停止方面表现很差。经过微调后，模型在拒绝不可执行指令（90%）、生成正确动作指令（95%）以及紧急停止（从0%提升至100%）方面的能力得到显著提升，证明了针对性微调对于将通用VLM转变为可靠系统控制器的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了支持并发交互的双模型架构</strong>：通过并行运行的活跃模型与待机模型，实现了VLA系统的行为并发与即时任务中断，模拟了类人多任务处理能力。</li>
<li><strong>设计了基于特殊令牌的控制流</strong>：开创了“模型即控制器”范式，通过微调VLM生成特殊令牌（如 <code>[ACT]</code>, <code>[HALT]</code>）来直接驱动系统状态转换，实现了动作与系统行为的优雅紧耦合。</li>
<li><strong>提出了一套训练交互式VLA的方法论</strong>：包括数据整理和微调策略，能够教导VLM生成系统级控制令牌，该方法与主流双系统VLA模型兼容。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>在Libero基准测试中，与经过大规模端到端训练的模型（如GR00T）相比，在空间关系和目标概念理解上存在性能差距。</li>
<li>在任务切换场景中，偶尔因VLM误解新指令（仅生成语音回应而非动作命令）而导致失败。</li>
</ol>
<p><strong>启示</strong>：<br>本文的工作表明，通过巧妙的系统架构设计（双模型并行）和模型训练范式转变（模型即控制器），可以显著提升具身智能体的交互自然度和实时响应能力。这为未来构建更灵活、更安全、更能与人类动态协作的机器人助手提供了新的思路。后续研究可专注于利用更大规模、更多样化的具身数据进一步优化VLM的交互理解能力，并将此类框架扩展到更复杂的多模态交互和长期任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型交互范式僵化、无法并发处理多模态输入并实时响应用户中断的问题，提出了VITA-E人机交互框架。其核心技术是**双模型并行架构**（主动模型与待机模型）与 **“模型即控制器”范式**，通过微调视觉语言模型生成特殊令牌来直接控制系统行为。在实体人形机器人上的实验表明，该框架能可靠处理复杂交互，实现了**极高的紧急停止与语音中断成功率**，并成功完成了语音与动作的并发执行。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.21817" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>