<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.08022" target="_blank" rel="noreferrer">2510.08022</a></span>
        <span>作者: Xuelong Li Team</span>
        <span>日期: 2025-10-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>数据驱动的机器人模仿学习依赖于大规模、高质量的专家演示数据集。然而，现有数据集主要依赖于人工遥操作机器人收集，在可扩展性、轨迹平滑度以及跨不同机器人形态在真实环境中的适用性方面存在局限。具体而言，基于遥操作的方法劳动强度大、成本高；基于视觉的演示方法缺乏机器人动作标签且无法捕捉交互中的动态数据；而基于传感器增强接口的方法（如UMI）虽能有效转换人类演示，但其基于GoPro的视觉惯性里程计（VIO）和离线SLAM算法流程可能较慢。本文针对数据收集在扩展性、效率以及与机器人形态解耦方面的痛点，提出了FastUMI-100K大规模数据集。其核心思路是：基于一种硬件解耦、模块化的新型机器人系统FastUMI，集成轻量级跟踪系统，以高效、可扩展的方式收集大规模、多模态的演示数据，旨在满足日益复杂的真实世界操作任务需求。</p>
<h2 id="方法详解">方法详解</h2>
<p>FastUMI-100K数据集的构建核心在于其数据收集系统FastUMI和标准化的处理流程。整体流程分为数据收集、多传感器对齐与数据标注三个阶段。</p>
<p><img src="https://arxiv.org/html/2510.08022v1/x1.png" alt="数据集概览"></p>
<blockquote>
<p><strong>图1</strong>：FastUMI-100K数据集概览。该数据集包含10万+条长视野轨迹，涵盖54个不同任务和超过100种真实场景操作物体，集成了单臂和双臂轨迹、多视角腕戴式鱼眼图像以及细粒度文本标注。</p>
</blockquote>
<p><strong>硬件设计</strong>：遵循FastUMI协议，使用RealSense T265进行轨迹跟踪，同时使用GoPro鱼眼相机捕获高分辨率、广角鱼眼RGB图像。设计了标准化的即插即用指尖附件，兼容常见手持设备配件，便于安装在不同机器人夹爪上。如图2（a）所示，还进一步开发了双臂FastUMI系统，在两个夹爪的末端执行器上集成了两个T265相机和两个GoPro鱼眼相机，以20Hz采样频率同步记录多模态数据。</p>
<p><img src="https://arxiv.org/html/2510.08022v1/x2.png" alt="硬件设计与时间对齐"></p>
<blockquote>
<p><strong>图2</strong>：（a）为本文开发的双臂FastUMI硬件数据收集设备。（b）为20Hz频率下多传感器时间对齐示意图。</p>
</blockquote>
<p><strong>数据收集流程</strong>：如图3所示，流程分为三个阶段：1) <strong>收集前</strong>：技术支持人员完成任务设计、分发与可行性验证，录制指导演示视频，并搭建多样化的任务场景。2) <strong>收集中</strong>：使用两个固定的3D打印槽固定手持设备，作为T265相机的初始定位点。通过语音引导系统和计算机上实时可视化的鱼眼图像与轨迹，监控数据收集过程。3) <strong>收集后</strong>：通过质量监控系统自动检测轨迹跳跃或尺度失真等问题，标记并删除无效数据，确保数据质量。</p>
<p><strong>多设备多传感器对齐</strong>：为实现精确的时间对齐，采用统一的ROS时钟为所有数据流分配一致时间戳，并将FastUMI的单臂多传感器对齐技术扩展到双臂场景。系统包含四个不同采样率的传感器：两个60Hz的GoPro相机和两个200Hz的T265传感器。对齐策略是：使用ROS的近似时间同步器，基于时间戳对两个夹爪的RGB图像主题进行近似同步（最大对齐误差设为GoPro采样周期的一半，即1/120秒）。同时，每个夹爪的视频帧统一降采样至20Hz，并与时间上最接近的T265位姿数据匹配，最终实现了双臂多传感器数据的亚毫秒级精度对齐。</p>
<p><img src="https://arxiv.org/html/2510.08022v1/x3.png" alt="数据收集与处理流程"></p>
<blockquote>
<p><strong>图3</strong>：数据收集和处理的流程。展示了从任务准备、数据收集到后处理与标注的完整三个阶段。</p>
</blockquote>
<p><strong>数据标注</strong>：开发了一个双级文本标注系统。1) <strong>子任务级标注</strong>：首先利用GPT-4o分析完整任务视频内容并自动划分为子任务，然后通过定制GUI界面进行人工标注，提取关键帧并与子任务文本描述精确匹配。标注侧重于描述目标位置以减少歧义。2) <strong>动作级标注</strong>：基于RT-H范式，分析每帧及其后第十帧的位姿数据，计算相对平移、旋转角度和夹爪开合变化，组合“前后左右移动”、“顺时针/逆时针旋转”、“夹爪开合”等基本语义单元，生成详细的运动描述。通过交叉验证，共提供了15,000条文本标注。</p>
<p>与现有方法相比，本文的创新点在于：1) <strong>硬件解耦与高效收集</strong>：用RealSense T265直接记录6-DoF末端执行器位姿，避免了繁重的离线SLAM或固定动作捕捉基础设施，提高了收集吞吐量。2) <strong>大规模与多样性</strong>：数据集规模（&gt;10万轨迹）和任务类型（54类）远超现有UMI风格数据集，并首次系统集成了双臂配置数据。3) <strong>结构化双级标注</strong>：提供了从任务目标到运动细节的完整语义链，增强了数据对语言条件策略训练的支撑。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了FastUMI-100K数据集，并在Xarm6和Flexiv Rizon4机器人平台上进行评估。对比的基线方法包括单任务模仿学习模型Diffusion Policy (DP) 和 Action Chunking with Transformers (ACT)，以及通用视觉-语言-动作（VLA）模型 π_0。实验旨在验证数据集在单任务学习、跨平台部署和对VLA模型微调方面的有效性。</p>
<p><img src="https://arxiv.org/html/2510.08022v1/x4.png" alt="数据统计"></p>
<blockquote>
<p><strong>图4</strong>：FastUMI-100K的数据统计图。（a）展示了使用不同机器人在真实场景中操作物体。（b）展示了不同场景中物体数量的分类。（c）记录了数据集中六种不同类型任务的数量分布。（d）和（e）比较了FastUMI-100K与AgiBot在五种双臂任务上的平均线速度和角速度，表明FastUMI-100K在执行复杂长视野任务时更具灵活性和类人特性。</p>
</blockquote>
<p><strong>单任务模仿学习</strong>：如表I所示，在七个子任务上评估Diffusion Policy，成功率在33.33%到66.67%之间，证明了数据的可靠性。表II（源自FastUMI工作）比较了ACT算法在关节空间和TCP（工具中心点）位姿空间训练的差异，结果显示使用TCP位姿能更鲁棒地捕捉轨迹形状和动态特征，例如在“Pick Bear”任务上，PoseACT (Absolute)取得了80%的成功率，显著高于关节空间ACT的20%。</p>
<p><strong>跨平台部署</strong>：为了验证跨平台可转移性，将在单一数据集上训练的Diffusion Policy模型直接迁移到Xarm6和Flexiv Rizon4平台，仅调整末端执行器坐标系映射，不进行额外微调。如表I所示，在“Make Sandwich”、“Place Tableware”和“Heat Food”等任务的子任务上进行了测试。结果表明，由于FastUMI收集的数据不受特定机器人形态限制，部分人类收集的数据可能超出工作空间较小的机器人的操作范围。解决方案是根据目标机器人的额定工作空间构建三维边界框来过滤数据。</p>
<p><strong>与通用VLA模型兼容性</strong>：通过微调 π_0 模型来验证数据集的兼容性。如表III所示，在Xarm6上对多种操作类型任务进行微调，成功率普遍较高，例如“Rearrange Coke”达到93.33%，“Unplug Charger”达到93.33%。表IV展示了在Flexiv Rizon4上对长视野任务进行微调的结果，任务“Make Sandwich”和“Wash Clothes”的子任务成功率较高，但“Heat Food”任务中“Put bread into microwave”和“Close microwave door”子任务失败，表明了处理复杂长视野序列的挑战。</p>
<p><img src="https://arxiv.org/html/2510.08022v1/x5.png" alt="实验任务"></p>
<blockquote>
<p><strong>图5</strong>：实验中评估的所有16个任务示意图。</p>
</blockquote>
<p><strong>关键结果总结</strong>：</p>
<ol>
<li>DP在多种任务上取得了可靠的成功率（33.33%-66.67%），证明了数据集的基本有效性。</li>
<li>使用TCP位姿的ACT变体（PoseACT）性能显著优于关节空间ACT，凸显了UMI风格末端执行器状态数据的优势。</li>
<li>跨平台实验表明，经过简单坐标映射和数据过滤，同一模型可在不同结构机器人上执行相同任务。</li>
<li>对 π_0 模型微调在多数任务上获得高成功率（最高达93.33%），证明了数据集与前沿通用VLA模型的兼容性，但也揭示了其在处理某些复杂长序列任务时的不足。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了FastUMI-100K，一个大规模、多模态、与机器人形态解耦的UMI风格演示数据集，包含超过10万条轨迹，涵盖54个任务和数百种物体，显著提升了数据规模与多样性。2) 设计并实现了基于FastUMI系统的高效、标准化数据收集与处理流程，包括硬件解耦设计、多传感器精确时间对齐和双级文本标注系统。3) 通过系统的实验验证了数据集在支持单任务模仿学习、跨平台迁移以及与通用VLA模型微调方面的有效性和实用性。</p>
<p>论文自身提到的局限性在于：由于数据由人类操作者收集，其轨迹可能超出某些目标机器人平台（如Xarm6）的物理工作空间，需要通过基于工作空间边界框的数据过滤来解决。</p>
<p>本文对后续研究的启示在于：硬件解耦、高效收集的大规模高质量数据集是推动数据驱动机器人泛化能力的关键。FastUMI-100K提供的丰富、类人的演示数据，特别是包含的双臂协作和长视野复杂任务数据，为训练更灵巧、更通用的机器人策略模型奠定了重要基础。其与 π_0 等通用模型的兼容性实验，也指明了未来通过大规模、多样化数据集微调预训练大模型以提升机器人操作能力的研究方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对数据驱动的机器人操作学习中，现有演示数据集规模小、采集成本高、可扩展性差的核心问题，提出了大规模数据集FastUMI-100K。其关键技术是采用新型FastUMI机器人系统，通过模块化、硬件解耦的机械设计与集成轻量追踪系统进行高效采集。该数据集包含超过10万条长时程轨迹，覆盖54个任务和数百种物体，并整合了末端状态、多视角鱼眼图像和文本注释等多模态数据。实验表明，基于该数据集训练的策略能在多种基线算法上取得高成功率，验证了其对解决复杂、动态操作任务的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.08022" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>