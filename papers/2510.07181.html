<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.07181" target="_blank" rel="noreferrer">2510.07181</a></span>
        <span>作者: Shanghang Zhang Team</span>
        <span>日期: 2025-10-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）在空间推理方面展现出显著能力，但其本质上仍局限于定性精度，缺乏现实世界机器人应用所需的计算精度。主流方法通常将深度传感器和相机标定提供的度量线索简化为类图像的表示，或将几何问题降级为模式识别任务，无法实现机器人操作所需的厘米级精度。这些方法在感知和输出模态上都存在局限：感知端未能显式利用相机内外参将像素观测转换为真实世界坐标；输出端则多为基于数据的统计回归预测，而非基于几何的确定性计算。</p>
<p>本文针对VLMs在精确几何计算能力上的根本性缺失，提出了一个新视角：将VLMs的角色从感知估计器转变为几何计算机。核心思路是，不试图将复杂的几何运算内化于神经网络，而是让模型学会识别几何推理需求，生成相应的计算代码，并调用外部专业工具库来执行精确计算，从而实现从定性推理到定量计算的跨越。</p>
<h2 id="方法详解">方法详解</h2>
<p>TIGeR是一个工具集成的几何推理框架，通过在已标定的度量输入上生成和执行代码，实现精确的几何计算，为下游机器人任务提供高精度推理。</p>
<p><img src="https://arxiv.org/html/2510.07181v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：TIGeR框架概览。它是一个通过代码生成和执行在标定度量输入上进行精确几何计算和推理的工具集成框架。能够实现(a)精确的空间定位，(b)跨视角的统一推理，以及(c)针对复杂计算挑战的可执行代码生成。</p>
</blockquote>
<p>整体流程如**图1(c)**所示：给定人类指令（例如，“黑色包应该如何移动到桌子上的空位？”），TIGeR通过选择相关上下文（例如，先前的工具输出）并调用适当的工具（例如，2D/3D检测、相机位姿估计）来获得所需的精确中间度量结果，从而进行逐步推理。在推理过程中，它还可以生成代码来整合和计算已获取的信息，逐步推导出信息丰富的中间结果和最终答案。</p>
<p>核心模块分为两大类工具，反映了层次化的工作流：</p>
<ol>
<li><p><strong>视觉感知工具</strong>：从传感器输入中提取像素级或相机级信息，以弥补VLMs在度量尺度精度上的不足。关键工具包括：</p>
<ul>
<li><code>camera_intrinsics</code>：获取相机内参以支持精确的2D到3D投影。</li>
<li><code>camera_extrinsics</code>：提供外参以在多视角观测中建立统一的3D坐标系。</li>
<li><code>depth_sensor</code>：查询选定区域的深度信息。</li>
<li><code>object_segmentation</code>：从粗糙的VLM输出（如2D边界框）生成精确的分割掩码。</li>
</ul>
</li>
<li><p><strong>几何计算工具</strong>：处理感知数据以推导几何属性，弥合2D像素输出与3D几何需求之间的差距。关键工具包括：</p>
<ul>
<li><code>box_2d_to_box_3d</code>：通过整合2D分割与深度数据，将2D边界框转换为3D边界框。</li>
<li><code>point_3d_to_point_2d</code>：使用相机内外参将3D点投影到2D图像平面。</li>
<li><code>code_executor</code>：调用代码生成流程进行任意计算（如物体间距离计算、位姿估计），并将数值结果返回给VLM的推理过程。为提高精度和鲁棒性，该工具引入了一个代码生成子程序：VLM指定高层需求，由外部代码生成器（Qwen3-Coder）实现，生成的代码在沙箱环境中执行。</li>
</ul>
</li>
</ol>
<p>为支持训练，本文构建了大规模数据集<strong>TIGeR-300K</strong>，包含30万个高质量样本，集成了具身场景中的工具使用和几何推理。</p>
<p><img src="https://arxiv.org/html/2510.07181v2/x2.png" alt="数据集构建"></p>
<blockquote>
<p><strong>图2</strong>：TIGeR-300K数据集构建流程。采用基于模板的合成与LLM工具增强的混合方法生成包含完整工具调用序列和中间计算的VQA样本。</p>
</blockquote>
<p>如<strong>图2</strong>所示，数据生成采用混合策略：(1) <strong>基于模板的数据生成</strong>：利用CA-1M数据集，通过模块化模板系统性地生成针对机器人常见空间查询（如定位、距离估计）的多样化实例，产生约27.4万个结构化样本。(2) <strong>基于大模型的思维链重写</strong>：利用GPT-4o筛选SSR-CoT数据集中的空间推理问题，并提示大模型将其重写为工具集成版本，插入工具调用，最终产生3.5万个多样化的工具集成样本。</p>
<p>训练采用两阶段流水线：监督微调（SFT）后接强化微调（RFT）。</p>
<p><img src="https://arxiv.org/html/2510.07181v2/x3.png" alt="训练流程"></p>
<blockquote>
<p><strong>图3</strong>：TIGeR的两阶段训练流水线。首先在TIGeR-300K上进行SFT，然后进行RFT，RFT阶段结合了五个为工具集成推理量身定制的专业奖励函数（基于结果和基于过程）。</p>
</blockquote>
<ul>
<li><strong>冷启动SFT</strong>：在TIGeR-300K上进行，以初始化模型生成包含适当工具使用的推理链的能力，使用标准的下一词预测损失。</li>
<li><strong>面向几何工具使用的RFT</strong>：采用GRPO算法进行强化微调，以增强模型泛化的工具集成几何推理能力。其核心创新在于<strong>分层奖励设计</strong>，该设计从基于结果和基于过程两个角度评估几何工具使用，具体包含五个奖励函数：<ol>
<li><code>r_format</code>（基于结果）：评估空间标记和工具语法的结构正确性。</li>
<li><code>r_tool</code>（基于过程）：评估工具选择和参数格式的正确性。</li>
<li><code>r_param</code>（基于过程）：根据参数类型（连续或离散）评估参数内容的准确性。</li>
<li><code>r_code</code>（基于过程）：评估基于代码的几何计算，包括代码可执行性和输出正确性。</li>
<li><code>r_answer</code>（基于结果）：根据真实值评估最终答案的准确性。<br>总奖励为各奖励的加权和，该结构提供了细粒度的监督，引导模型学习精确的几何计算和适当的工具调用。</li>
</ol>
</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在多个基准测试、仿真环境和真实机器人平台上进行。</p>
<ul>
<li><strong>基准测试</strong>：包括定性空间理解基准（CV-Bench, BLINK, RoboSpatial, EmbSpatial）和定量几何推理基准（Q-Spatial++）。由于这些基准缺乏真实的几何标注，推理时使用视觉基础模型来提取信息并注入近似的几何先验。</li>
<li><strong>仿真评估</strong>：在Open6DOR V2位置跟踪任务（Level 0和Level 1）上进行。</li>
<li><strong>真实世界评估</strong>：在配备UR5机械臂和Intel RealSense L515 RGB-D相机的真实机器人系统上，执行一系列需要精确几何推理和3D定位的操控任务。</li>
<li><strong>对比基线</strong>：包括GPT-4o、Gemini-2.5-Pro、NVILA-8B、Qwen-2.5-VL-7B、GLM-4.1V-Thinking、SpatialBot-3B、SpaceLLaVA-13B、RoboPoint-13B等VLMs，以及仿真中的Octo、OpenVLA、SoFar等基线。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.07181v2/x4.png" alt="定性对比"></p>
<blockquote>
<p><strong>图4</strong>：TIGeR与Gemini 2.5-Pro在空间理解基准代表性样本上的对比。展示了TIGeR如何成功利用工具集成推理解决复杂的空间和几何问题。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>空间与几何推理基准</strong>：如<strong>表I</strong>所示，TIGeR在这些基准上实现了零样本最先进的平均准确率79.30%，甚至超过了Gemini 2.5-Pro 5.83个百分点。<strong>图4</strong>的定性示例进一步展示了其通过工具调用解决复杂问题的能力。</li>
<li><strong>仿真机器人任务</strong>：如<strong>表II</strong>所示，在Open6DOR V2位置跟踪任务中，TIGeR的平均成功率达到了83.7%，相对于基线方法（如SoFar的72.4%）有显著提升（绝对提升11.3%）。</li>
<li><strong>真实机器人操控</strong>：如<strong>表III</strong>所示，在四项需要精细空间推理的真实世界任务中，TIGeR consistently outperforms all baselines。特别是在需要度量精度放置的任务（“将热狗放在桃子右侧0.1米处”）中，TIGeR取得了55%的成功率，证明了其将空间约束准确映射到3D坐标的能力。<strong>图5</strong>可视化了其实时推理步骤。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.07181v2/x5.png" alt="机器人任务示例"></p>
<blockquote>
<p><strong>图5</strong>：TIGeR在真实机器人任务中的工具集成推理过程图示。展示了在遮挡和杂乱场景下，模型通过逐步工具调用和输出来完成复杂指令的推理。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：如<strong>表IV</strong>所示，消融研究验证了关键设计选择。<ul>
<li><strong>数据配方</strong>：仅使用LLM重写数据会导致性能大幅下降（平均准确率28.92%），而结合模板数据则达到79.30%，表明高质量、结构化的模板数据对学习几何推理至关重要。</li>
<li><strong>奖励函数</strong>：移除任何单个奖励函数都会导致性能下降，其中<code>r_format</code>、<code>r_tool</code>和<code>r_param</code>的影响尤为明显，证明了分层奖励设计中各组件对引导模型正确使用工具的必要性。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.07181v2/x6.png" alt="真实实验设置"></p>
<blockquote>
<p><strong>图6</strong>：真实世界实验设置，包括UR5机械臂、L515 RGB-D相机和夹爪，并展示了捕获的RGB图像和深度图。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li><strong>提出新框架</strong>：首次强调了超越定性空间推理的几何计算对精确机器人控制的核心作用，并提出了TIGeR框架，通过代码生成和执行，将VLMs转变为能够进行精确几何计算的智能体。</li>
<li><strong>构建新数据集</strong>：发布了大规模、面向工具调用的数据集TIGeR-300K，专门用于通过程序化工具调用进行几何推理训练。</li>
<li><strong>设计新训练机制</strong>：开发了SFT→RFT的两阶段训练流水线，并提出了专门针对工具集成推理的分层奖励设计，有效提升了几何计算的准确性和任务完成度。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，在基准测试中，由于缺乏真实几何标注，需要依赖视觉基础模型来近似提取几何先验，这可能引入误差。</p>
<p><strong>对后续研究的启示</strong>：TIGeR的成功表明，对于需要高精度数值计算的具身AI任务，将大型模型定位为“决策与调度中心”，而非“全能计算器”，通过调用外部专业工具来保证计算精度和可解释性，是一条富有前景的技术路径。这为如何将基础模型与领域特定模块（如几何库、物理仿真器）深度融合，以解决机器人学等对精度和可靠性要求极高的实际问题，提供了重要的方法论参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出TIGeR框架，旨在解决视觉语言模型在机器人任务中几何推理精度不足的问题。现有方法仅能进行定性空间描述，无法利用深度与相机标定数据实现厘米级精确计算。TIGeR通过让模型识别几何问题、生成计算代码并调用外部工具库执行精确运算，将VLMs转变为几何计算器。为此构建了TIGeR-300K工具调用数据集，并采用监督微调与强化微调两阶段训练。实验表明，TIGeR在几何推理基准上达到SOTA性能，并在真实机器人操作中实现厘米级精度。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.07181" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>