<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Survey on Efficient Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>A Survey on Efficient Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.24795" target="_blank" rel="noreferrer">2510.24795</a></span>
        <span>作者: Yu, Zhaoshu, Wang, Bo, Zeng, Pengpeng, Zhang, Haonan, Zhang, Ji, Wang, Zheng, Gao, Lianli, Song, Jingkuan, Sebe, Nicu, Shen, Heng Tao</span>
        <span>日期: 2025/10/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作模型（VLAs）代表了具身智能的重要前沿，旨在将数字知识与物理世界交互相连接。尽管基础VLAs表现出色，但其大规模架构固有的巨大计算和数据需求阻碍了其发展。具体瓶颈体现在三个方面：实时性不兼容（推理延迟高，控制频率不足）、计算成本过高（预训练资源消耗巨大，如OpenVLA消耗21,500个A100-GPU小时）以及数据收集效率低下（依赖大规模数据集，如π0需要超过10,000小时的机器人轨迹）。尽管近期研究涌现出许多提升VLA效率的工作，但该领域缺乏一个统一的框架来整合这些分散的进展。本文首次针对高效视觉-语言-动作模型（Efficient VLAs）进行了全面综述，覆盖整个“模型-训练-数据”流程，旨在通过一个新颖的分类法系统性地梳理该领域的技术进展，并为未来可扩展的具身智能研究提供路线图。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个统一的分类法，将构建高效VLA的核心技术体系组织为三个相互关联的支柱：高效模型设计、高效训练和高效数据收集。</p>
<p><img src="https://arxiv.org/html/2510.24795v2/x2.png" alt="调查组织结构"></p>
<blockquote>
<p><strong>图2</strong>：本综述的组织结构。我们将高效VLA系统性地分为三个核心支柱：(1) 高效模型设计，涵盖高效架构和模型压缩技术；(2) 高效训练，包括高效预训练和后训练策略；(3) 高效数据收集，包括高效数据收集和增强方法。</p>
</blockquote>
<p><strong>整体框架</strong>：如图2所示，该分类框架构成了理解高效VLA技术全景的基础。它始于对基础VLA范式的回顾（图3），然后深入探讨三个优化维度，最后延伸到应用、挑战和未来方向。</p>
<p><img src="https://arxiv.org/html/2510.24795v2/x3.png" alt="VLA概述"></p>
<blockquote>
<p><strong>图3</strong>：VLA概述。VLAs整合视觉编码器以提取视觉特征，LLM主干以融合多模态输入，以及动作解码器（基于MLP、自回归或生成式）以产生机器人控制信号，从而实现用于具身操作任务的端到端视觉-语言-动作推理。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>高效模型设计</strong>：旨在通过轻量级架构和压缩技术弥合基础模型复杂性与资源受限部署之间的差距。</p>
<ul>
<li><strong>高效架构</strong>：如图5所示，包含六大策略。<ul>
<li><strong>高效注意力</strong>：通过线性时间架构（如SARA-RT）、高效掩码策略（如Long-VLA的相位感知输入掩码）和KV缓存优化（如RetoVLA、KV-Efficient VLA）来缓解标准自注意力的O(n²)复杂度。</li>
<li><strong>Transformer替代品</strong>：采用计算复杂度为线性的状态空间模型（如Mamba）作为LLM主干，例如RoboMamba和FlowRAM。</li>
<li><strong>高效动作解码</strong>：超越自回归解码，采用并行解码范式（如Jacobi解码、推测解码）和生成式建模（如扩散模型、流匹配）来合成整体轨迹，降低延迟。</li>
<li><strong>轻量级组件</strong>：采用更小的模型主干。</li>
<li><strong>专家混合</strong>：通过输入路由实现稀疏激活。</li>
<li><strong>分层系统</strong>：将高层VLM规划与低层VLA执行解耦。</li>
</ul>
</li>
<li><strong>模型压缩</strong>：通过剪枝、量化（降低权重和激活的数值精度）和令牌优化（减少视觉令牌数量或长度）来减少模型中的表示冗余，从而缩小模型尺寸、降低内存占用和加速推理。</li>
</ul>
</li>
<li><p><strong>高效训练</strong>：专注于减少VLA训练期间的计算和数据负担。</p>
<ul>
<li><strong>高效预训练</strong>：通过课程学习、数据选择和高效的行动建模（如离散化、预测状态）来优化预训练过程。</li>
<li><strong>高效后训练</strong>：包括监督微调（SFT）和强化学习（RL），通过适配器微调、低秩适应（LoRA）等参数高效方法，以及离线RL、逆RL等高效的RL策略，来提升模型的适应性和性能。</li>
</ul>
</li>
<li><p><strong>高效数据收集</strong>：解决机器人数据获取和利用的瓶颈。</p>
<ul>
<li><strong>高效数据收集</strong>：利用交互式（如远程操作）、模拟式（如RoboGen、RoboCasa）、可重用式（数据重组）和自驱动式（主动学习、强化学习）策略，使数据集获取更具可扩展性和效率。</li>
<li><strong>高效数据增强</strong>：通过生成式方法（如扩散模型）、轨迹编辑和语义增强来扩展现有数据集，最大化数据效用。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：本文的核心创新在于首次提出了一个全面、系统化的分类法，将分散的高效VLA研究统一到“模型-训练-数据”的全流程视角下。这为理解和推进该领域建立了一个清晰的框架。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>作为一篇综述性论文，本文并未提出新的模型并进行实验对比，而是系统性地梳理和总结了现有高效VLA方法的效率指标、发展脉络以及应用场景。</p>
<p><strong>关键数据与图表分析</strong>：<br>本文通过表格和趋势图展示了高效VLA的必要性和进展。</p>
<p><img src="https://arxiv.org/html/2510.24795v2/x1.png" alt="从基础VLA到高效VLA的转变"></p>
<blockquote>
<p><strong>图1</strong>：从基础VLA到高效VLA的转变。基础VLA受限于固有的挑战，特别是实时性不兼容、过高的计算成本和低效的数据收集。通过包含高效模型设计、高效训练和高效数据收集的核心方法，高效VLA使得在边缘设备上实现高性能部署成为可能。</p>
</blockquote>
<p>表I对比了代表性VLA模型的效率相关指标，清晰揭示了基础VLA在部署上的瓶颈。例如，RT-2-PaLI-X (55B) 的推理延迟高达330-1000毫秒，控制频率仅1-3 Hz，而参数更小的模型如π0 (3.3B) 将延迟降低到73毫秒，频率提升到20/50 Hz，HiRobot (3B) 也达到了类似的效率水平。这直接说明了追求模型轻量化对于满足机器人实时控制需求的重要性。</p>
<p><img src="https://arxiv.org/html/2510.24795v2/x4.png" alt="基础VLA模型与高效VLA的时间线"></p>
<blockquote>
<p><strong>图4</strong>：基础VLA模型与高效VLA的时间线。时间线展示了从2023年到2025年基础VLA模型和高效VLA的进展，突出了在提升VLA效率以弥合计算需求与现实世界机器人部署方面研究的爆炸性增长。</p>
</blockquote>
<p>图4的时间线直观展示了高效VLA研究（尤其是2024年底以来）的加速涌现，表明该领域的研究焦点正迅速向资源效率倾斜，以应对实际机器人部署的需求。</p>
<p><img src="https://arxiv.org/html/2510.24795v2/x5.png" alt="高效架构的关键策略"></p>
<blockquote>
<p><strong>图5</strong>：VLAs中高效架构的关键策略。我们阐述了六种主要方法：(a) 高效注意力，(b) Transformer替代品，(c) 高效动作解码，(d) 轻量级组件，(e) 专家混合，以及 (f) 分层系统。</p>
</blockquote>
<p>图5和表II进一步细化了高效模型设计支柱下的具体技术路径和代表性工作，为研究者提供了清晰的技术选型参考。</p>
<p><strong>应用总结</strong>：综述还总结了高效VLA在自动驾驶、工业制造、医疗机器人、实验室自动化、家庭服务机器人等关键领域的应用潜力，凸显了其解决现实世界高要求任务的实用价值。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>开创性综述</strong>：这是首个专门针对高效VLA领域的全面综述，覆盖了完整的“模型-训练-数据”流程，填补了文献空白，旨在为研究社区建立基础性参考。</li>
<li><strong>新颖分类法</strong>：提出了一个新颖且系统结构化的分类法，将构建高效VLA的核心技术体系组织为三个相互关联的支柱：高效模型设计、高效训练和高效数据收集。</li>
<li><strong>未来路线图</strong>：批判性地提炼了该领域面临的关键挑战和当前局限，从而概述了有前景和前瞻性的研究方向，以启发和指导未来在可扩展具身智能方面的努力。</li>
</ol>
<p><strong>局限性</strong>：本文作为综述，其局限性在于它是对现有工作的梳理和总结，而非提出新的算法或模型。它指出了高效VLA领域整体面临的挑战（如模拟到真实的差距、安全性与可靠性、评估标准化等），但并未声称解决了这些挑战。</p>
<p><strong>启示</strong>：本综述为研究者提供了一个清晰的领域地图和问题框架。它表明，实现VLA在资源受限场景下的实用化部署，需要从模型架构、训练算法和数据管道进行协同优化，而非单一维度的改进。文中指出的未来方向，如“Algorithm-Architecture Co-design”、“Unified and Standardized Evaluation”以及“Towards General-Purpose Embodied Intelligence”，为后续研究提供了重要的思路指引。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文是一篇关于高效视觉-语言-动作模型（VLAs）的综述。核心问题在于指出，尽管VLAs在具身智能中潜力巨大，但其庞大的架构带来了难以承受的计算和数据需求，且该领域缺乏统一框架来整合各类效率提升研究。为此，本文首次提出了一个涵盖模型-训练-数据全流程的统一分类法，将现有技术归纳为三大支柱：1）高效模型设计（如高效架构与模型压缩）；2）高效训练（降低学习过程计算负担）；3）高效数据收集（解决机器人数据获取与利用瓶颈）。作为综述，本文旨在梳理前沿方法、总结应用、指明挑战与未来方向，未报告具体实验数据。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.24795" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>