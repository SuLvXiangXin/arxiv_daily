<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.18825" target="_blank" rel="noreferrer">2506.18825</a></span>
        <span>作者: Jia Pan Team</span>
        <span>日期: 2025-06-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用高维视觉输入的模仿学习（IL）在复杂的双手操作任务中已被证明是直观且有效的。然而，视觉运动策略的泛化能力仍然有限，尤其是在仅有少量演示数据可用时。策略中的累积误差严重阻碍了其完成长时域任务的能力。为了应对这些局限，本文提出SViP框架，旨在将视觉运动策略无缝集成到任务与运动规划（TAMP）中。现有方法如Mandlekar等人和Garrett等人的工作依赖于6-DoF物体姿态估计器，这需要额外的数据收集和训练时间，且对对称物体效果不佳。此外，像NOD-TAMP和PSL等方法仅编排固定的单手技能序列，适用性和组合灵活性有限。本文的核心思路是：利用任务与运动规划（TAMP）将学习到的视觉运动策略与以物体为中心的脚本化运动基元“缝合”成一个连贯的序列，以完成任务，从而结合学习策略的灵活性与规划方法的可靠性。</p>
<h2 id="方法详解">方法详解</h2>
<p>SViP的整体框架是通过TAMP计算一个集成学习操作与规划操作的“计划骨架”。当遇到训练数据中未见的观测时，SViP利用规划操作过渡到训练分布内的状态，从而启动学习到的视觉运动策略。此外，SViP能够在遵守指定运动约束的同时，完成定制化的目标。</p>
<p><img src="https://arxiv.org/html/2506.18825v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：SViP整体框架示意图。在提供运动规划基元（蓝色）和训练好的双手视觉运动策略（橙色）后，SViP利用任务和运动规划来计算一个整合了学习操作与规划操作的计划骨架。当遇到训练数据外的观测时（底部），规划操作可将状态引导至策略的训练分布内，从而启动学习策略。该框架还支持在满足运动约束下完成定制目标。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>基于场景图的技能抽象与分割</strong>：系统将人类演示通过一个语义场景图监控器分割为双手和单手操作。场景图G = {V, E, L}，其中节点V代表实体（机器人夹爪H、区域R、物体O），边E代表实体间的关系，由谓词L描述（如AtGrasp, AtConf）。通过视频分割工具获取物体位置p_O和机器人末端位置p_H，构建事件驱动的场景图序列G(D)。对于一个双手技能a，定义其抽象表示为〈G_mid, G_pre, G_eff, π, φ, ξ〉，其中G_mid是接触密集阶段（双手操作）的场景图，G_pre和G_eff分别是其前、后的场景图，包含关键的几何信息。π是学习到的视觉运动策略，φ是切换条件生成器，ξ是可行性验证器。</li>
<li><strong>符号化描述生成</strong>：将G_pre和G_eff中的几何信息解析为PDDLStream格式的谓词，为符号规划提供高度抽象的技能描述。例如，对于图2中的物体交接任务，可从G_pre_a提取PreGeom谓词，从G_eff_a提取EffGeom谓词，从而形成完整的BiOperation描述，包括前提条件（pre）、效果（eff）和约束（con）。</li>
<li><strong>基于扩散模型的参数（切换条件）生成器</strong>：为了在学习策略和规划基元之间实现切换，需要预测切换条件。本文将可学习的决策变量V（包括双手技能的起始/结束关节角度q，以及各物体中心操作轨迹τ_o）的分布建模为多个独立分布的乘积。利用去噪扩散概率模型（DDPM）的强大分布拟合能力，训练切换条件生成器φ。具体而言，将决策变量的对数概率梯度（score function）表示为多个DDPM评分函数的和：ε̂(V|C_o1,...)=ε_θ(q,k)+Σ_i ε_θ(τ_oi,k|C_oi)，其中C_oi是所接触物体的点云。通过训练DDPM来学习这些评分函数，从而能够从点云观测生成可靠的切换参数（如抓取位姿、交接前后的机器人配置）。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.18825v1/x2.png" alt="技能分割与场景图"></p>
<blockquote>
<p><strong>图2</strong>：双手“物体交接”操作的技能分割示意图。(a)和(c)展示了接触密集部分的开始与结束，(b)展示了接触密集部分。对应的场景图G_pre, G_mid, G_eff绘制在下方，每条边都标有连续的决策变量（如相对位姿、关节角度）。</p>
</blockquote>
<p>创新点体现在：1）直接从点云观测学习到切换条件的映射，无需依赖物体姿态估计器；2）能够分解和重组来自演示的双手/单手技能，以实现新目标，提供了比编排固定单手技能序列更广的适用性和组合灵活性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境和真实世界中进行。使用了包含20条真实世界演示的数据集。基准任务包括需要长时域推理和双手协调的复杂操作，例如涉及多个物体的组装、包装任务。</p>
<p>对比的基线方法是当前最先进的生成式模仿学习（IL）方法。</p>
<p>关键实验结果：在真实世界实验中，SViP在成功率和任务完成度上优于最先进的生成式IL方法。仅使用20条演示，SViP就能泛化到分布外的初始条件，并能为先前未见过的任务自动发现有效的解决方案。具体而言，在“包装零食”任务中，基线方法由于误差累积和泛化能力差，成功率为0%，而SViP达到了100%的成功率。</p>
<p><img src="https://arxiv.org/html/2506.18825v1/extracted/6562837/unreach-seq.png" alt="模拟环境任务序列"></p>
<blockquote>
<p><strong>图3</strong>：模拟环境中一个无法由单一策略完成的复杂任务序列。展示了SViP如何通过组合规划基元（蓝色）和学习策略（橙色）来解决该任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.18825v1/extracted/6562837/unsafe-seq.png" alt="模拟环境不安全序列"></p>
<blockquote>
<p><strong>图4</strong>：模拟环境中一个需要满足安全性约束（避免碰撞）的任务序列。展示了SViP如何通过可行性验证器确保计划的安全性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.18825v1/x3.png" alt="真实世界实验设置"></p>
<blockquote>
<p><strong>图5</strong>：真实世界实验设置。使用两个xArm6机械臂和夹爪，通过头顶RGB-D相机获取点云观测。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.18825v1/x4.png" alt="与基线方法对比"></p>
<blockquote>
<p><strong>图6</strong>：SViP与最先进的生成式模仿学习（IL）基线方法在真实世界“包装零食”任务上的对比。基线方法失败的主要原因是误差累积和泛化能力不足（例如，抓取后物体姿态偏移导致后续步骤失败），而SViP通过规划进行纠正，实现了100%的成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.18825v1/extracted/6562837/3tape.png" alt="多胶带任务"></p>
<blockquote>
<p><strong>图7</strong>：使用SViP完成一个需要处理三条胶带的新任务（训练演示中仅使用一条胶带）。这体现了SViP通过TAMP的约束建模为新任务自动发现解决方案的能力。</p>
</blockquote>
<p>消融实验方面，论文验证了各个组件的贡献。完整的SViP系统（包含场景图分割、切换条件生成器、可行性验证器和TAMP规划器）实现了最佳性能。如果移除切换条件生成器（即依赖硬编码切换），系统在分布外观测下的鲁棒性会下降。如果移除可行性验证器，则可能生成导致碰撞或不安全配置的计划。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一种能够从演示中分解单手或双手技能，并重新组织它们以实现新目标的组合系统；2）设计了用于视觉运动策略的切换条件生成器和可行性验证器，促进了它们与运动基元的无缝排序，同时遵守运动约束；3）在模拟和真实世界进行了综合实验，证明SViP能在具有挑战性的设置中完成长时域操作。</p>
<p>论文自身提到的局限性包括：目前的方法依赖于从演示中分割出的技能片段，对于需要在线动态重规划或反应式调整的极端不确定性场景可能不够灵活。</p>
<p>对后续研究的启示：SViP展示了将数据驱动的学习与基于模型的规划相结合的强大潜力。未来的工作可以探索如何将更复杂的反馈控制器或在线适应机制集成到该框架中，以处理动态环境或执行过程中的意外扰动。此外，如何自动化地学习或发现有用的运动基元，而非完全依赖人类演示，也是一个有前景的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SViP框架，旨在解决小样本模仿学习中视觉运动策略泛化能力不足、累积误差导致长时程任务失败的问题。方法核心是将视觉运动策略整合至任务与运动规划中：通过语义场景图分割演示动作，并利用关键场景图变量训练切换条件生成器，产生参数化脚本基元以应对分布外观测。实验表明，仅需20条真实演示，SViP即可在无需物体姿态估计的情况下泛化至分布外初始条件，并在未知任务中自动规划有效解。真实世界实验验证其性能优于当前主流生成式模仿学习方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.18825" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>