<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.17282" target="_blank" rel="noreferrer">2504.17282</a></span>
        <span>作者: Cherif, Lynn, Kondrup, Flemming, Venuto, David, Anand, Ankit, Precup, Doina, Khetarpal, Khimya</span>
        <span>日期: 2025/04/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在图形用户界面网页导航等稀疏奖励、大动作空间环境中，强化学习代理通常面临样本效率低下的挑战。当前主流方法之一是依赖大量领域专家演示的行为克隆，但这需要高昂的计算和标注成本，且存在模仿差距，性能难以超越训练数据。纯粹的强化学习方法则在这些复杂环境中难以探索到有效策略。本文针对“如何在低数据制度下，高效地为强化学习代理指定意图及对应的可供性以剪枝动作空间”这一具体痛点，提出了一种新视角：利用预训练视觉语言模型生成代码，以隐式实现意图完成函数，从而动态地确定当前状态下可负担的动作集合。本文的核心思路是：通过一个自动化的程序生成与验证流程，让VLM为特定任务生成可执行的代码脚本，该脚本能根据像素观测返回可负担动作集，进而在强化学习训练与推断循环中用于掩码动作空间，极大提升探索效率。</p>
<h2 id="方法详解">方法详解</h2>
<p>CoGA方法的整体目标是为每个任务生成一个函数 <code>determine_affordable_actions(obs)</code>，该函数输入像素观测，输出一个可负担动作的集合（包括动作类型和像素坐标）。生成的代码随后被集成到RL代理的训练和推断循环中，用于对不可负担的动作进行硬掩码（采样概率为0），从而约束代理的探索空间。</p>
<p><img src="https://arxiv.org/html/2504.17282v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：CoGA方法整体框架。左侧：方法概述。VLM处理任务描述和示例观测以提取相关意图和对象模板图像，用于生成代码；代码经过验证与改进；生成的可供性集合用于掩码RL代理的动作空间。右侧：生成可供性脚本的提示流程。</p>
</blockquote>
<p>方法包含三个核心组件：</p>
<ol>
<li><p><strong>模块化代码生成管道</strong>：首先，VLM（使用GPT-4o）根据随机采样的观测、任务描述和环境给出的示例指令，识别任务相关的<strong>意图</strong>（如“点击标签页”）和每个意图下的<strong>相关对象</strong>。接着，通过一个坐标网格图像，VLM指定这些对象的边界框坐标，系统自动裁剪并保存为模板图像（使用5个随机观测以增强泛化）。与之前工作使用边缘或颜色检测不同，CoGA采用灰度化的模板图像匹配（使用OpenCV）来检测复杂对象。随后，VLM通过思维链提示，分步制定策略：a) 判断给定观测下哪些意图相关；b) 为每个意图确定可负担动作；c) 组合意图及其对应动作集；d) 用代码注释勾勒脚本大纲。最后，VLM编写代码实现其策略，利用预写的模板匹配脚本动态检测可负担的像素动作。</p>
</li>
<li><p><strong>验证管道</strong>：为确保生成代码的可靠性，采用双重验证。a) 使用一个“批评”VLM审查代码，并提供改进反馈。b) 基于5个手动标注了真实可供性集合的随机观测作为测试用例，计算脚本的精确率和召回率（匹配定义为动作类型相同且像素交并比IoU&gt;0）。验证过程迭代最多3次，保留在测试用例上F1分数最高的脚本。</p>
</li>
<li><p><strong>在RL中使用生成的脚本</strong>：训练和推断时，对每个观测调用生成的脚本以获得可负担动作集，并据此创建硬掩码。该方法适用于基于价值和基于策略的RL算法，论文中选用Double DQN与优先经验回放，并融合了用于编码任务指令的Sentence-BERT。</p>
</li>
</ol>
<p>与现有方法相比，创新点在于：1) 利用VLM的推理能力自动发现意图并生成实现隐式意图完成函数的代码，避免了手动设计或直接查询VLM带来的成本与延迟；2) 设计了包含代码生成、验证和集成的完整自动化流程，最终产物是轻量级、可离线执行的可供性函数。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在<strong>MiniWob++<strong>基准测试上进行，这是一个基于Web GUI的任务集合。动作空间包含4种动作类型和32x32的像素坐标分箱，总计1024个离散动作，奖励稀疏。对比的基线方法包括：</strong>RL代理</strong>（使用相同架构的DQN）、<strong>行为克隆代理</strong>（在有限专家演示上训练）。CoGA在23个任务上进行了评估。</p>
<p><img src="https://arxiv.org/html/2504.17282v1/x2.png" alt="定性结果与脚本质量"></p>
<blockquote>
<p><strong>图2</strong>：左：三个任务（click-test， click-test-2， click-tab）上返回的可供性示例（绿框）。右：各任务生成脚本的F1分数。结果表明大多数脚本具有高F1分数，即对真实可供性的覆盖既广又准。</p>
</blockquote>
<p>首先评估生成脚本的质量。如图2（右）所示，大多数脚本的F1分数很高（&gt;0.8），表明其能够准确且全面地识别真实可负担动作。对于精度低但召回高的脚本，其返回的动作集会包含比真实情况更多的动作，最坏情况下CoGA性能将与RL基线持平。</p>
<p><img src="https://arxiv.org/html/2504.17282v1/x3.png" alt="样本效率对比"></p>
<blockquote>
<p><strong>图3</strong>：左：在1000步时，RL代理与CoGA在各任务上的评估成功率。CoGA在训练早期（仅1000步）的样本效率就超过RL代理10倍以上。右：count-sides（左）和click-test-2（右）任务上，RL代理与CoGA的评估成功率曲线。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ol>
<li><strong>样本效率</strong>：如图3所示，在训练仅1000步时，CoGA在大多数任务上的成功率已显著高于RL基线，实现了超过一个数量级的样本效率提升。在脚本质量高（F1分数高）的任务上，增益尤为明显。</li>
<li><strong>脚本泛化能力</strong>：论文测试了生成的脚本在同一“家族”任务（即GUI相同但最优策略不同的任务）间的泛化能力。如表1所示，使用亲属任务的脚本（CoGA-t）有时甚至能取得比使用原任务脚本（CoGA-o）更好的性能，证实了可供性在同一家族任务中的可迁移性。</li>
</ol>
<blockquote>
<p><strong>表1</strong>：使用原始生成脚本（CoGA-o）、迁移脚本（CoGA-t）以及RL基线在泛化任务上的平均评估成功率（%）。例如，click-button-sequence任务使用click-test-2的脚本性能更优。</p>
</blockquote>
<ol start="3">
<li><strong>与行为克隆在低数据制度下的对比</strong>：在仅有少量专家演示（10， 50， 200， 1000条）的低数据制度下，对比CoGA、RL和BC。如图4所示，当专家演示数量不超过200条时，CoGA的平均性能优于BC；超过200条后，BC开始超越。而RL基线仅在使用10条演示训练的BC面前有优势。这凸显了利用可供性约束动作空间对代理性能的积极影响。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.17282v1/x4.png" alt="与BC的对比"></p>
<blockquote>
<p><strong>图4</strong>：右：在不同专家数据制度下，BC代理、RL代理和CoGA在各任务上的评估成功率（均值和标准差）。左：跨任务平均的评估成功率随BC专家数据量变化的对比。CoGA在专家数据有限时（≤200条）表现优于BC。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了 <strong>CoGA框架</strong>，利用预训练VLM通过自动化代码生成与验证流程，为RL代理生成意图驱动的可供性，从而显著提升在稀疏奖励、大动作空间环境中的样本效率；2) 在MiniWob++基准上实证表明，该方法能实现数量级级的样本效率提升，生成的脚本具有良好的准确性和在任务家族内的泛化能力；3) 在低数据制度下（专家演示≤200条），CoGA的性能优于或与行为克隆持平，为结合基础模型与RL开辟了新路径。</p>
<p>论文自身提到的局限性包括：CoGA的成功强烈依赖于生成脚本的质量，而脚本质量又取决于底层对象检测方法（如模板匹配）的准确性。如果生成脚本的召回率低（漏掉许多真实可负担动作），CoGA将无法成功，此时使用软掩码可能是一个缓解方案，但会牺牲样本效率。</p>
<p>本文的启示在于：1) 将大模型的高层推理能力通过代码生成“编译”成轻量、可复用的模块，是赋能传统RL算法的一种高效且可解释的途径；2) 在专家数据稀缺的场景下，结合少量演示与自动生成的可供性约束，有望达到与使用大量演示的模仿学习相当的性能；3) 该方法框架不局限于网页导航，可扩展至其他需要复杂动作空间剪枝的视觉RL领域。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对强化学习在网页GUI等稀疏奖励、大动作空间环境中样本效率低下的问题，提出CoGA方法。该方法利用预训练视觉语言模型生成代码，通过自动化程序生成与验证流程，为智能体动态提供当前状态下的可执行动作集，从而大幅缩减需探索的动作空间。实验表明，在MiniWob++基准测试中，CoGA使其RL智能体的样本效率提升数个数量级，所生成程序具有良好的任务族内泛化能力，且在仅有少量专家演示时，性能优于或与行为克隆相当。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.17282" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>