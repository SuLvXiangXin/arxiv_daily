<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.18865" target="_blank" rel="noreferrer">2509.18865</a></span>
        <span>作者: Thanpimon Buamanee Team</span>
        <span>日期: 2025-09-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人模仿学习领域，基于领导者-跟随者遥操作的数据收集方法已成为主流。其中，双边控制方法通过交换位置和力信息，使操作者能感知接触力，从而获得更丰富的演示数据，在接触密集的操作中展现出比仅依赖位置的单边控制更好的鲁棒性和泛化能力。然而，现有的双边控制模仿学习方法（如Bi-ACT）通常局限于单一任务，需要为每个任务训练独立的模型，这限制了其在动态环境中处理多种任务的实用性。同时，尽管已有工作（如Bi-LAT）探索了将自然语言指令引入双边控制以调节操作力，但并未解决让单个模型适应多任务上下文的核心挑战。</p>
<p>本文针对双边控制模仿学习方法“单任务局限”这一具体痛点，提出了融合视觉与语言信息的新视角。其核心思路是：通过SigLIP编码语言指令，通过EfficientNet提取视觉特征，并利用FiLM机制进行融合，将得到的多模态表征与机器人关节状态（角度、速度、扭矩）一起输入一个基于Transformer的条件变分自编码器，从而训练一个能够根据视觉和语言指令灵活切换任务的单一策略模型。</p>
<h2 id="方法详解">方法详解</h2>
<p>Bi-VLA的整体框架旨在通过融合视觉、语言和双边控制数据，训练一个能够执行多任务的单一策略模型。其流程分为数据收集、学习建模和推理执行三个阶段。</p>
<p><img src="https://arxiv.org/html/2509.18865v1/fig/t12.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Bi-VLA方法整体框架。展示了从数据收集（双边控制与语言指令录入）、学习模型（多模态特征融合与CVAE训练）到推理执行（基于当前状态和指令生成动作）的完整流程。</p>
</blockquote>
<p>数据收集阶段使用四通道双边控制系统。领导者与跟随者机器人之间持续交换位置和扭矩信息，遵循位置跟踪（θ_l - θ_f = 0）和作用-反作用力（τ_l + τ_f = 0）定律。关节角度来自编码器，外部扭矩则通过扰动观测器和反作用力观测器估算，无需专用力传感器。操作者在操控领导者机器人提供演示的同时，会口述描述目标动作的自然语言指令（如“put ball upward”）。这些指令与机器人状态（关节角度、速度、扭矩）以及来自顶置和夹爪安装摄像头的RGB图像进行时间对齐，构成多模态训练数据集。</p>
<p><img src="https://arxiv.org/html/2509.18865v1/fig/tele.png" alt="数据收集"></p>
<blockquote>
<p><strong>图2</strong>：数据收集示意图。展示了人类操作者通过双边控制系统操控领导者机器人，同时提供语言指令，同步收集跟随者机器人的状态数据和多视角图像。</p>
</blockquote>
<p>学习模型的核心是一个基于Transformer的条件变分自编码器。其输入包括：跟随者关节状态（角度、速度、扭矩）、双视角RGB图像以及自然语言指令。</p>
<ul>
<li><strong>语言编码</strong>：使用SigLIP将文本指令编码为固定长度的嵌入向量。</li>
<li><strong>视觉编码</strong>：使用EfficientNet从RGB图像中提取视觉特征。</li>
<li><strong>特征融合</strong>：采用基于FiLM的调制方法，将语言嵌入与视觉特征进行融合，使视觉特征根据语言上下文进行自适应调整。</li>
<li><strong>联合编码</strong>：融合后的视觉-语言特征与机器人关节状态数据共同构成一个统一的潜在空间表征。</li>
<li><strong>动作生成</strong>：该潜在表征由CVAE的编码器-解码器（基于Transformer）进行处理，输出预测的领导者机器人关节轨迹（角度、速度、扭矩）。训练目标是最小化预测动作与双边控制演示中记录的真实领导者轨迹之间的重建误差。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.18865v1/fig/model.png" alt="学习模型"></p>
<blockquote>
<p><strong>图3</strong>：学习模型架构。详细展示了多模态输入（语言、图像、机器人状态）经过SigLIP、EfficientNet编码和FiLM融合后，输入Transformer CVAE生成动作序列的过程。</p>
</blockquote>
<p>推理阶段，模型接收当前的跟随者关节状态、同步的图像以及指定的语言指令，预测出下一个动作块（领导者关节轨迹）。这些输出被双边控制系统转换为实时电流命令，驱动跟随者机器人执行，形成一个根据多模态输入条件生成任务适应性动作的闭环。</p>
<p><img src="https://arxiv.org/html/2509.18865v1/fig/auto.png" alt="推理过程"></p>
<blockquote>
<p><strong>图4</strong>：推理过程示意图。在部署时，模型根据当前观测（状态、图像）和语言指令实时生成动作，通过双边控制系统驱动机器人。</p>
</blockquote>
<p>与现有方法相比，Bi-VLA的创新点在于首次在双边控制模仿学习中同时融合了视觉和语言模态，并通过统一的模型架构实现了多任务处理能力，突破了此前方法需为每个任务单独训练策略的限制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实机器人上进行，使用了两个ROBOTIS OpenManipulator-X机械臂（分别作为领导者和跟随者）和两个ELP USB摄像头（顶置和夹爪安装）。评估围绕两个精心设计的拾放任务展开：</p>
<ol>
<li><strong>双目标任务（语言可消歧）</strong>：球从一个固定位置拾起，根据语言指令（“向上放”/“向下放”）放置到两个目标位置之一。初始视觉观察相同，必须依赖语言消除歧义。<br><img src="https://arxiv.org/html/2509.18865v1/fig/2place_3.png" alt="双目标任务"><blockquote>
<p><strong>图5</strong>：双目标任务（语言可消歧）设置。球体初始位置固定，有两个可能的目标放置位置（上/下）。</p>
</blockquote>
</li>
<li><strong>双源任务（视觉可消歧）</strong>：球从两个可能的源位置（上/下）之一拾起，放置到固定目标。视觉信息足以区分源位置。<br><img src="https://arxiv.org/html/2509.18865v1/fig/2pick_3.png" alt="双源任务"><blockquote>
<p><strong>图6</strong>：双源任务（视觉可消歧）设置。球体有两个可能的拾取源位置（上/下），目标位置固定。</p>
</blockquote>
</li>
</ol>
<p>对比的基线方法包括：<strong>Bi-ACT</strong>（仅视觉-动作的基线）、**Bi-VLA (DistilBERT)<strong>（使用DistilBERT编码语言，仅在双目标任务上训练）、</strong>Bi-VLA (SigLIP)<strong>（使用SigLIP编码语言，分任务训练）、</strong>Bi-VLA (SigLIP-Mix)**（使用SigLIP，并在混合的双目标与双源任务数据上训练，且演示数据减半）。训练中使用了DABI数据增强技术，将1000Hz的机器人信号下采样至100Hz并与图像同步，将原始演示数据量扩增10倍。</p>
<p><img src="https://arxiv.org/html/2509.18865v1/fig/env-camera.png" alt="实验环境"></p>
<blockquote>
<p><strong>图7</strong>：实验硬件与环境设置。展示了使用的OpenManipulator-X机械臂和摄像头布置。</p>
</blockquote>
<p><strong>双目标任务结果</strong>：Bi-ACT因无法理解语言，完全失败，在“向下”指令上成功率为0%，总体成功率仅50%。Bi-VLA (DistilBERT)有所改善，但“向下”放置阶段成功率仅20%，总体60%。Bi-VLA (SigLIP)表现最佳，在“向上”、“向下”指令上分别取得80%和100%的成功率，总体达90%。Bi-VLA (SigLIP-Mix)在数据减少的情况下，仍能在两个指令上均取得70%的成功率，优于Bi-ACT。</p>
<p><strong>双源任务结果</strong>：当任务仅靠视觉即可区分时，Bi-ACT表现优异，总体成功率95%。Bi-VLA (SigLIP)和Bi-VLA (SigLIP-Mix)均取得90%的总体成功率，表明语言融合在视觉足够时不会产生负面影响。</p>
<p><strong>泛化测试（未学习的三球环境）</strong>：在更具挑战性的、包含干扰物体的新环境中测试双源任务。Bi-ACT再次崩溃为偏向“向上”的策略（“向下”成功率0%）。而Bi-VLA (SigLIP)和Bi-VLA (SigLIP-Mix)则分别保持了75%的总体成功率，展现了更好的泛化能力。</p>
<p><img src="https://arxiv.org/html/2509.18865v1/fig/ex-auto4.png" alt="实验结果汇总"></p>
<blockquote>
<p><strong>图10</strong>：所有实验任务的结果汇总图。直观对比了不同方法在双目标任务、双源任务及未学习三球环境下的总体成功率。</p>
</blockquote>
<p>消融实验通过对比不同语言编码器（SigLIP vs. DistilBERT）和不同训练模式（单任务 vs. 多任务混合）来验证各组件贡献。关键结论包括：1) <strong>语言编码器选择至关重要</strong>：SigLIP比DistilBERT实现了更准确的语言-动作对齐和更高的任务成功率。2) <strong>多任务混合训练有效</strong>：Bi-VLA (SigLIP-Mix)用更少的数据训练单个模型，在语言依赖任务上显著优于无语言的基线，在视觉依赖任务上匹配单任务模型性能，且在多任务间未产生负干扰，证明了框架的多任务处理能力。3) <strong>语言融合的必要性</strong>：在视觉信息不足时（如双目标任务），语言输入是成功的关键；而在视觉信息充足时，语言融合不会损害性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：第一，提出了首个融合视觉与语言的双边控制模仿学习框架Bi-VLA，将SigLIP语言编码、EfficientNet视觉编码与FiLM融合机制引入双边控制管道。第二，成功实现了使用单一模型处理多个任务，突破了此前双边控制方法固有的单任务限制。第三，通过详实的真实机器人实验证明，该框架能有效依据语言指令消歧，并在数据有限的情况下保持良好的多任务泛化性能。</p>
<p>论文自身提到的局限性在于，评估仅限于小规模的拾放任务和有限的环境。未来工作可扩展至更复杂的多步骤操作、更多样的物体类别以及不同的机器人平台。此外，整合更大规模的数据集和自监督学习可能进一步提升泛化能力。</p>
<p>这项工作为机器人模仿学习提供了重要启示：结合力觉反馈（双边控制）、视觉感知和语言理解的多模态融合，是迈向能够灵活适应多样化任务的通用机器人策略的有效途径。特别是，选择合适的预训练视觉语言模型（如SigLIP）进行特征对齐，对于实现可靠的语言 grounding 至关重要。Bi-VLA 展示了在保持接触操作鲁棒性的同时，利用高层语义指令（语言）来灵活配置底层技能（动作）的可行性，为构建更智能、更通用的协作机器人开辟了新方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Bi-VLA框架，旨在解决传统双边控制模仿学习方法仅适用于单一任务、缺乏通用性的问题。该方法通过SigLIP和基于FiLM的融合技术，将机器人关节角度、速度、扭矩数据与视觉特征、自然语言指令相结合。真实机器人实验表明，Bi-VLA能成功理解视觉-语言组合指令，相比传统方法提升了多任务场景下的任务成功率，验证了视觉与语言融合对增强系统泛化能力的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.18865" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>