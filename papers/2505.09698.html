<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.09698" target="_blank" rel="noreferrer">2505.09698</a></span>
        <span>作者: Zhao, Enyu, Raval, Vedant, Zhang, Hejia, Mao, Jiageng, Shangguan, Zeyu, Nikolaidis, Stefanos, Wang, Yue, Seita, Daniel</span>
        <span>日期: 2025/05/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉语言模型（VLMs）因其常识推理能力在机器人领域受到广泛关注，目前主要被用作高层任务规划器。然而，机器人操作需要精确的低层推理能力，例如决定机器人末端执行器的具体抓取点、放置点或推动方向。尽管近期有工作探索将VLMs用于生成低层可执行轨迹，但社区缺乏一个清晰、统一的基准来系统评估VLMs在这方面的能力。现有的一些基准（如PhysBench、NEWTON、VLABench等）要么不专门评估低层操作推理，要么评估方式不够高效（如使用轨迹均方误差评估可能无法应对多模态性），且普遍缺乏对可变形物体操作等关键任务的关注。</p>
<p>本文针对“如何高效、全面地评估VLMs在机器人低层操作中的物理推理能力”这一具体痛点，提出了一个新颖的基准测试ManipBench。其核心思路是：通过设计基于关键点预测的多选题（MCQ），将复杂的机器人操作轨迹预测问题转化为对交互关键点（如抓取点、放置点）的选择问题，从而无需进行耗时的轨迹仿真或真实世界执行，即可高效评估VLMs的低层推理能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>ManipBench的整体目标是从多样化的数据源生成多选题，以评估VLMs预测机器人操作关键点的能力。其pipeline主要分为数据准备、图像预处理和问题生成三个阶段。</p>
<p><img src="https://arxiv.org/html/2505.09698v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ManipBench方法整体框架。利用真实和仿真环境，通常通过MOKA风格的预处理流程提取关键点和网格标注，用于生成可供VLM评估的多选题。</p>
</blockquote>
<p><strong>核心模块一：数据准备</strong>。基准的数据来源于三个渠道：</p>
<ol>
<li><strong>公开机器人操作数据集</strong>：使用DROID和Bridge这两个大规模模仿学习数据集，并进一步将DROID数据分为关节物体操作（art.）和抓放（p&amp;p）两个子集。通过微调GroundingDINO并利用带有人工标注抓取位置的数据，提取真实机械臂轨迹中的抓取和放置点作为真值。</li>
<li><strong>自建布料操作装置</strong>：鉴于可变形物体操作的重要性，作者搭建了真实世界工作站，手动策划了用于评估布料操作（如折叠、抚平）理解的数据。他们将布料操作分解为十个不同的理解维度（如布料-物体交互理解、逆动力学理解等），每个维度代表成功执行布料操作所需掌握的基本方面。</li>
<li><strong>仿真环境</strong>：为了评估在真实世界部署可能繁琐的任务（如工具使用、动态操作），作者从现有流行基准（SimplerEnv、RLBench、SoftGym）中适配仿真资产和预训练策略，并构建了一个新的球体射击环境（IsaacSim），涵盖了抓放、关节物体操作、可变形物体操作、工具操作和动态操作五大类别。</li>
</ol>
<p><strong>核心模块二：图像预处理与问题生成</strong>。对于基于真实世界数据的问题，采用类似MOKA的流程进行预处理：给定观察图像和任务描述，使用GPT-4o识别关键物体，用Grounded SAM分割物体并获取掩码，从掩码中采样中心点和轮廓点，并在原图上标注这些采样点以及一个覆盖网格（通常是5x5）。这个标注后的图像将作为VLM的视觉提示。真值关键点用于生成多选题的正确选项。问题生成分为三种类型：</p>
<ul>
<li><strong>来自公开数据集的问题（Q1, Q2）</strong>：Q1要求VLM从四个候选轨迹（每个包含一个抓取关键点和两个图像网格块）中选择最佳匹配。Q2则要求VLM先从四个候选点中选择抓取点，再利用真值抓取点选择正确的放置网格块。共生成9180个此类问题。</li>
<li><strong>来自布料操作的问题</strong>：基于十个理解维度手动创建选项，共生成2662个问题，每个场景通常只形成一个问题以鼓励多样性。</li>
<li><strong>来自仿真的问题</strong>：由于可直接访问真值关键点信息，直接在图像上绘制真值点并采样错误点作为选项，无需网格覆盖，要求VLM从四个候选关键点中选择一个以完成任务。</li>
</ul>
<p>与现有方法相比，ManipBench的创新点主要体现在：1) <strong>评估焦点</strong>：专门针对低层操作推理（关键点预测），而非高层规划或物体属性预测。2) <strong>评估形式</strong>：采用高效的多选题形式，避免了轨迹展开评估的复杂性和多模态评估难题。3) <strong>任务覆盖</strong>：特别加强了对可变形物体操作这一重要但具挑战性领域的评估。4) <strong>数据来源</strong>：综合利用了真实世界大规模数据、针对性手动采集数据和仿真数据，确保了多样性和可扩展性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：ManipBench共包含12617道多选题。作者在2张RTX 4090和5张RTX 6000 Ada GPU的服务器上，评估了来自10个家族的33个代表性VLM，包括闭源的GPT系列（o1, GPT-4.1, GPT-4o, GPT-4o-mini）和Gemini系列（2.5-pro, 2.0-flash, 1.5-pro, 1.5-flash），以及开源的InternVL2、InternVL2.5、QwenVL、GLM-4V、LLaVA-NeXT、Llama3.2-VI等系列的不同规模变体。评估指标是答题准确率。此外，还招募了36名人类志愿者对每类问题的子集（50-70题）进行测试，作为人类表现基线。为了验证基准的有效性，作者还设计了7个在ManipBench中未出现过的真实世界机器人操作任务，使用UR5机械臂测试所选VLM通过选择关键点来定义机器人动作的成功率。</p>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2505.09698v2/x5.png" alt="公开数据集结果"></p>
<blockquote>
<p><strong>表2</strong>：各VLM在基于Bridge和DROID数据集的多选题上的性能。Gemini-2.5-pro在几乎所有Q1问题上表现最佳，o1在部分Q2问题上领先。开源模型中，InternVL2.5-38B整体表现最好。所有闭源模型均显著高于随机猜测（25%），但小于2B参数的开源模型性能接近或差于随机猜测。人类表现远高于所有模型。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09698v2/x6.png" alt="仿真任务结果"></p>
<blockquote>
<p><strong>表3</strong>：各VLM在仿真任务多选题上的性能。Gemini-2.5-pro整体表现最好，但某些模型在特定任务上突出（如Gemini-2.0-flash在Place Carrot任务中最佳）。大模型通常优于同家族小模型，但提升幅度因任务而异。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09698v2/x4.png" alt="布料操作维度评估"></p>
<blockquote>
<p><strong>图4</strong>：各VLM在布料操作十个评估维度上的准确率雷达图及跨维度标准差。所有模型均优于随机猜测。某些维度（如任务规划理解）对所有模型都较简单，而另一些（如布料-布料交互理解）则更具挑战性。人类在所有维度上表现一致且优异，揭示了VLM的差距。Gemini-2.5-pro和o1表现最佳。模型在不同维度上性能的标准差较大，说明这些维度能有效区分模型的低层推理能力。</p>
</blockquote>
<p><strong>任务类别分析</strong>：聚合所有模型在各任务类别上的平均准确率（不包括公开数据集的Q2问题）显示，现有VLM处理抓放任务相对较好（平均准确率0.525），而关节物体操作（0.396）和动态操作（0.357）更具挑战性。</p>
<p><strong>模型鲁棒性分析</strong>：通过计算各模型在所有任务类别上准确率的变异系数（CV）来评估鲁棒性（CV值越低越好）。Gemini-2.5-pro是最鲁棒的闭源模型（CV=0.089），Qwen2.5-VL-32B是最鲁棒的开源模型（CV=0.085）。</p>
<p><strong>真实世界性能迁移</strong>：<br><img src="https://arxiv.org/html/2505.09698v2/x3.png" alt="真实世界实验结果"></p>
<blockquote>
<p><strong>表4</strong>：所选VLM在7个未见过的真实世界操作任务上的成功率。Gemini-2.5-pro成功率最高（18/21），GPT-4o和o1次之。小参数开源模型（如GLM-4V-9B）表现较差。</p>
</blockquote>
<p>统计分析表明，VLM在ManipBench上的总体表现与其在真实世界任务中的成功率存在强正相关：皮尔逊相关系数0.889 (p=0.003)，斯皮尔曼相关系数0.850 (p=0.007)，肯德尔τ系数0.691 (p=0.018)。这验证了ManipBench作为评估VLM在具身机器人设置中有效性的可靠代理基准的效用。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>提出了ManipBench</strong>，一个基于多选题的、专门用于评估VLM低层机器人操作推理能力的基准，包含超过1.2万道题目，涵盖广泛的任务类别，特别是可变形物体操作。2) <strong>进行了广泛的模型评估</strong>：系统测试了33个代表不同家族的VLM，揭示了它们在低层推理上的能力差异、优势、劣势及鲁棒性，为模型选择提供了参考。3) <strong>验证了基准的有效性</strong>：通过真实世界机器人实验，证明了模型在基准上的表现与其在未知真实任务中作为机器人代理的有效性之间存在强统计相关性。</p>
<p>论文提到的局限性包括：基准可能包含来自数据源或预处理流程的偏差；目前的问题生成依赖于特定的图像标注格式；未来需要持续扩展以涵盖更多样化和复杂的操作场景。</p>
<p>这项工作为社区提供了一个高效、可扩展的评估工具，其揭示的VLM在低层物理推理（尤其是动态和可变形物体操作）上与人类的巨大差距，指明了未来VLM研发需要重点突破的方向。同时，其评估框架（将轨迹预测转化为关键点选择）和验证方法（基准性能与真实世界性能强相关）对如何设计有效的机器人AI基准具有启发意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型在机器人低级操作推理能力评估缺乏标准基准的问题，提出了名为ManipBench的新基准。该基准采用基于多项选择题的评估设计，高效测试模型对物体交互及可变形物体操作等低级任务的理解。作者评估了33个代表性VLM模型，发现最佳模型（如Gemini-2.5-pro）性能显著优于随机猜测，但整体与人类水平仍存在明显差距。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.09698" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>