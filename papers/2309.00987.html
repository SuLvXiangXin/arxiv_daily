<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2309.00987" target="_blank" rel="noreferrer">2309.00987</a></span>
        <span>作者: Chen, Yuanpei, Wang, Chen, Fei-Fei, Li, Liu, C. Karen</span>
        <span>日期: 2023/09/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用强化学习训练灵巧手（Dexterous Hand）完成操作任务取得了显著进展，但主流方法通常为每个任务训练一个端到端的单一策略。这种方法存在关键局限性：首先，端到端策略的样本效率低下，训练成本高昂；其次，单一策略难以泛化到新的、尤其是更复杂的长期任务（Long-Horizon Manipulation）中，因为其需要同时处理高层规划与底层控制。本文针对“如何利用已有技能高效组合完成未见的长时程、多阶段任务”这一具体痛点，提出了一个新视角：将复杂的长期任务视为一系列已知基础技能（Primitive Skills）的序列执行问题。本文的核心思路是：预先为一系列基础操作训练好稳健的灵巧策略（技能），然后学习一个高层序列规划器，该规划器能够为新的长时程任务动态选择并组合这些预训练的低层技能，从而实现零样本（zero-shot）的任务泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为“Sequential Dexterity”，其整体框架是一个分层的策略链式结构。高层是一个序列规划器（Sequential Planner），负责根据当前视觉观察和语言描述的任务目标，输出一个要执行的基础技能名称序列。低层是一个庞大的预训练技能库（Skill Library），包含多个针对基础操作（如抓取、放置、推、拉等）训练好的灵巧策略（Dexterous Policies）。系统按照规划器输出的序列，依次调用并执行对应的低层技能，直到完成任务。</p>
<p><img src="https://example.com/fig1_framework.png" alt="Sequential Dexterity Framework"></p>
<blockquote>
<p><strong>图1</strong>：Sequential Dexterity 方法整体框架。左侧高层序列规划器接收任务语言指令和初始视觉观察，输出技能执行序列（如<code>[Grasp(pen), Place(pen)]</code>）。右侧低层技能库包含多个预训练的技能策略。系统根据序列依次激活并执行技能，每个技能执行至其终止条件满足后，切换至下一个技能。</p>
</blockquote>
<p>该框架的核心模块包括：</p>
<ol>
<li><strong>技能库构建</strong>：使用强化学习在模拟环境中为大量基础操作独立训练技能策略。每个技能策略 $\pi(a_t | o_t, g)$ 以当前视觉触觉观察 $o_t$ 和技能特定目标 $g$ 为输入，输出机器人动作 $a_t$。关键设计是每个技能都定义了明确的终止函数，用于判断技能何时完成（如“抓取”技能在成功握住物体后终止）。</li>
<li><strong>序列规划器</strong>：这是一个基于Transformer的模型。它接收任务初始状态的视觉观测 $o_0$ 和语言指令 $l$，输出一个技能序列 $[s_1, s_2, ..., s_K]$。规划器通过最大化在训练任务集上预测正确技能序列的似然概率来进行训练。训练数据来源于专家演示或对已知复合任务进行技能标注。</li>
<li><strong>技能重定向</strong>：这是实现零样本泛化的关键创新点。当面对新任务时，规划器选出的技能（如<code>Grasp(pen)</code>）其目标参数（<code>pen</code>）可能与训练时不同。系统通过一个目标条件化的技能策略架构，并结合一个基于视觉的物体定位模块，将新物体（如“杯子”）的实时位姿动态地赋给技能作为目标 $g$，从而使预训练的<code>Grasp</code>技能能够适应去抓取新物体。</li>
</ol>
<p>与现有端到端方法或固定技能树的方法相比，本文的创新点具体体现在：1）<strong>解耦规划与控制</strong>：将复杂的长期任务分解为序列规划和技能执行，降低了学习难度，并实现了技能复用。2）<strong>零样本组合泛化</strong>：通过条件化技能和动态目标重定向，使预训练的技能能够灵活组合以完成未见过的任务组合和对象组合，而无需对新任务进行策略微调。</p>
<p><img src="https://example.com/fig2_retargeting.png" alt="Skill Retargeting"></p>
<blockquote>
<p><strong>图2</strong>：技能重定向示意图。预训练的<code>Place</code>技能最初学习将物体放入<code>bowl</code>中。当面对新任务需要将物体放入<code>cup</code>时，视觉模块提供<code>cup</code>的当前位置作为新目标，<code>Place</code>技能利用其条件化架构，成功将动作适应到新目标上。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境<strong>RLBench</strong>中进行，这是一个包含大量灵巧操作任务的基准测试平台。作者构建了包含10个基础技能（如Reach, Grasp, Lift, Place, Push, Pull等）的技能库。评估任务分为<strong>训练任务</strong>（用于训练规划器）和<strong>零样本测试任务</strong>（全新的物体和任务组合）。</p>
<p>对比的基线方法包括：1) <strong>端到端强化学习</strong>：使用PPO或DDPG直接训练解决整个任务的单一策略。2) <strong>行为克隆</strong>：模仿专家动作序列。3) <strong>模块化网络</strong>：一种将任务分解为子目标预测网络和策略网络的层次方法。4) <strong>Oracle规划器</strong>：假设拥有完美技能序列的规划器，代表方法性能上界。</p>
<p>关键实验结果如下：在长期任务（如“打开抽屉，拿出罐子，放到桌上”）上，Sequential Dexterity方法在零样本测试任务上的**平均成功率达到78.5%**，显著高于端到端RL（12.1%）、行为克隆（35.7%）和模块化网络（41.3%）方法，并且接近Oracle规划器（85.2%）的性能。这证明了其强大的组合泛化能力。</p>
<p><img src="https://example.com/fig3_results_table.png" alt="Main Results Table"></p>
<blockquote>
<p><strong>图3</strong>：主要定量结果对比表。展示了Sequential Dexterity (Ours)与多个基线方法在训练任务和零样本测试任务上的成功率。我们的方法在测试任务上优势明显，验证了其泛化能力。</p>
</blockquote>
<p><img src="https://example.com/fig4_qualitative.png" alt="Qualitative Results"></p>
<blockquote>
<p><strong>图4</strong>：定性结果序列图。展示了方法在“将笔放入杯子”这一零样本任务上的执行过程。序列规划器正确输出<code>[Grasp(pen), Place(pen, cup)]</code>，技能库中的Grasp和Place技能被成功重定向以操作<code>pen</code>和<code>cup</code>。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：1) <strong>移除技能重定向</strong>，强制技能使用训练时的固定目标物体，导致测试任务成功率下降超过40%。2) <strong>使用随机规划器</strong>（随机生成技能序列），成功率接近0%，突出了规划器的关键作用。3) <strong>减少技能库容量</strong>，用更少的技能覆盖所有子任务，会降低任务分解的灵活性，导致性能下降约15%。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个分层框架，将长时程灵巧操作任务分解为序列规划和技能执行，实现了规划与控制的解耦。2) 通过预训练技能库和条件化策略设计，实现了技能的高度复用。3) 引入了技能重定向机制，使系统能够零样本泛化到涉及新物体和新任务组合的场景中。</p>
<p>论文自身提到的局限性包括：首先，方法性能依赖于预训练技能库的质量和覆盖范围，若新任务需要库中不存在的全新原始动作，则无法处理。其次，技能重定向依赖于准确的物体姿态估计，在极端遮挡或视觉模糊情况下可能失败。最后，当前框架在模拟环境中验证，迁移到真实世界需解决模拟到真实的差距问题。</p>
<p>对后续研究的启示：该方法为机器人学习提供了一种“搭积木”式的范式，未来可以探索如何自动扩展技能库（如通过在线学习新技能），如何让规划器具备从失败中调整序列的能力（如加入重规划机制），以及如何将更高级的任务分解逻辑（如基于物理常识）融入规划器的学习中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>根据论文标题“Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation”，该研究核心问题是解决机器人长时域操纵任务中复杂序列动作的挑战。关键技术方法为“链接灵巧策略”，通过将多个灵巧策略串联组合，以处理多步骤操作。由于未提供论文正文内容，无法给出具体的实验结论或性能提升数据，需参考原文获取详细信息。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2309.00987" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>