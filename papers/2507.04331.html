<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Wavelet Policy: Lifting Scheme for Policy Learning in Long-Horizon Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Wavelet Policy: Lifting Scheme for Policy Learning in Long-Horizon Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.04331" target="_blank" rel="noreferrer">2507.04331</a></span>
        <span>作者: Yi Fang Team</span>
        <span>日期: 2025-07-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>具身人工智能中的策略学习旨在使智能体根据观察到的状态生成最优动作以实现特定目标。该领域已从行为克隆、强化学习等基础方法，发展到分层强化学习、逆强化学习等更复杂的方法。当前，策略学习的趋势强调处理复杂、多阶段的任务，这些任务需要在具有多模态动作分布和高维动作空间的环境中进行高精度操作和长期决策。然而，在处理需要处理长观察和动作序列的复杂长时域任务时，策略学习仍面临重大挑战，包括错误累积、多模态动作模式带来的学习困难以及对高精度操作的需求。</p>
<p>小波分析是一种强大的信号处理工具，通过将信号分解为多个尺度，在时域和频域同时捕获全局趋势和局部细节。受此启发，本文提出从小波分析的视角来学习策略。核心思路是：利用可学习的多尺度小波分解（提升方案）对观测序列进行分析，将其分解为不同频率的分量（低频近似与高频细节），然后在动作空间中从粗到细地合成动作序列，以此增强长时域任务中策略的一致性、多模态处理能力和精度。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的小波策略框架基于提升方案，旨在将观测序列 $S = {s_t, \cdots, s_{t+N}}$ 转换为对应的动作序列 $A = {a_t, \cdots, a_{t+N}}$。整体流程包含多尺度的分析（分解）阶段和合成（重建）阶段。</p>
<p><img src="https://arxiv.org/html/2507.04331v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：小波策略网络整体框架，基于多尺度提升方案。左侧输入观测序列，经过递归的分裂、预测和更新步骤（分析阶段），得到多尺度表示；中间通过转换器将观测空间的流转换为动作空间的流；右侧通过逆更新、逆预测和融合步骤（合成阶段）从粗到细生成动作序列。蓝线表示低频（粗尺度）分量，红线表示高频（细尺度）分量。图中仅展示两个尺度用于示意。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>分析块（分解）</strong>：对应于提升方案的分析阶段。对于每一尺度 $l$：</p>
<ul>
<li><strong>分裂</strong>：将输入流 $S^l$ 分为两个流 $S_e^l$ 和 $S_o^l$。为处理变长序列并增强灵活性，本文采用冗余提升方案，使用带自注意力的Transformer作为分裂器，将输出复制到“偶”流和“奇”流。</li>
<li><strong>预测</strong>：用偶流预测奇流，捕获高频细节：$S_d^l = S_o^l - \mathcal{P}(S_e^l, {W_i})$。</li>
<li><strong>更新</strong>：用细节更新偶流，捕获低频近似：$S_s^l = S_e^l + \mathcal{U}(S_d^l, {W_i})$。<br>其中 $\mathcal{P}$ 和 $\mathcal{U}$ 是可学习的预测和更新函数，本文实例化为<strong>因果膨胀卷积</strong>，以确保时序因果性并扩大感受野。$S_s^l$ 将被送入下一尺度进行进一步分解。</li>
</ul>
</li>
<li><p><strong>转换器</strong>：位于分析和合成阶段之间。其作用是将分析阶段得到的多尺度观测流 $S_s^L$ 和 ${S_d^l}$ 显式地转换到动作空间，得到对应的动作流 $A_s^L$ 和 ${A_d^l}$。转换器可以是任何满足时序因果性的网络，本文采用因果卷积。</p>
</li>
<li><p><strong>合成块（重建）</strong>：对应于提升方案的合成阶段，过程与分析块对称，从最粗尺度 $L$ 开始，逐步融合细节以重建动作。</p>
<ul>
<li><strong>逆更新</strong>：从近似中移除细节的影响：$A_e^l = A_s^l - \hat{\mathcal{U}}(A_d^l, {W_j})$。</li>
<li><strong>逆预测</strong>：基于偶流重建奇流：$A_o^l = A_d^l + \hat{\mathcal{P}}(A_e^l, {W_j})$。</li>
<li><strong>融合</strong>：将 $A_e^l$ 和 $A_o^l$ 两个流合并。不同于传统的交错合并，本文采用一个以 $A_s^l$ 为 Query，以 $A_d^l$ 为 Key 和 Value 的<strong>交叉注意力Transformer作为融合器</strong>，实现从粗到细的信息融合。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04331v1/x4.png" alt="网络组件细节"></p>
<blockquote>
<p><strong>图4</strong>：关键网络组件的具体实现。(左上) 用于实例化 $\mathcal{P}$、$\mathcal{U}$、$\hat{\mathcal{P}}$ 和 $\hat{\mathcal{U}}$ 的因果膨胀卷积，确保时序因果性；(右下) 基于Transformer的融合器结构，使用交叉注意力融合近似流和细节流。</p>
</blockquote>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>视角创新</strong>：首次将小波分析引入策略学习，从信号处理的角度处理长时域、多模态的动作序列。</li>
<li><strong>方法创新</strong>：基于可学习的提升方案构建策略网络，解决了传统小波基函数启发式选择和非可学的问题。通过设计因果膨胀卷积、Transformer融合器等组件，解决了直接应用提升方案时的时序因果性、序列长度限制和冗余方案下的合并问题。</li>
<li><strong>流程创新</strong>：采用了“观测空间多尺度分解 -&gt; 跨空间转换 -&gt; 动作空间多尺度重建”的清晰流程，实现了从全局趋势到局部细节的渐进式动作生成。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：方法在五个模拟环境的基准上进行评估，涵盖复杂长时域任务：</p>
<ol>
<li><strong>MetaWorld</strong>：机器人操作任务。</li>
<li><strong>Franka Kitchen</strong>：厨房场景中的顺序任务完成。</li>
<li><strong>CARLA</strong>：自动驾驶仿真。</li>
<li><strong>MULTIWORLD</strong>：多机器人协作。</li>
<li><strong>LIBERO</strong>：长时域、多任务物体操纵。</li>
</ol>
<p><strong>对比方法</strong>：包括IBC（隐式行为克隆）、Diffusion Policy（扩散策略）、RPL（中继策略学习）、Behavior Transformer（行为Transformer）等多种先进的模仿学习和长时域任务学习方法。</p>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2507.04331v1/x5.png" alt="定量结果"></p>
<blockquote>
<p><strong>图5</strong>：在五个基准测试上的定量实验结果。条形图显示了不同方法的平均成功率和标准差。小波策略在大多数任务上取得了最优或具有竞争力的性能。</p>
</blockquote>
<ul>
<li><strong>总体性能</strong>：如图5所示，小波策略在MetaWorld、Franka Kitchen、CARLA和MULTIWORLD四个环境中取得了最高的平均成功率，在LIBERO环境上与最佳基线方法性能相当。这证明了该方法在处理多样化长时域任务上的有效性和通用性。</li>
<li><strong>具体数值示例</strong>：在Franka Kitchen任务中，小波策略的成功率达到 <strong>85.0%</strong> ，显著高于Diffusion Policy的68.3%和Behavior Transformer的73.3%。在CARLA自动驾驶任务中，小波策略的成功率为 <strong>82.5%</strong> ，优于其他所有基线。</li>
</ul>
<p><strong>消融实验</strong>：论文对核心设计进行了消融研究。</p>
<ol>
<li><strong>提升方案的有效性</strong>：移除多尺度分解，仅使用单尺度结构，性能显著下降（例如在某个任务上下降超过10%），验证了多尺度分析的重要性。</li>
<li><strong>可学习组件的必要性</strong>：将预测/更新函数 $\mathcal{P}$、$\mathcal{U}$ 固定为Haar小波的简单函数（如 $P(x)=x$），性能低于可学习版本，证明了自适应学习小波变换的优势。</li>
<li><strong>因果性约束的重要性</strong>：使用非因果的卷积层，会导致模型在测试时利用未来信息，破坏实际部署的可行性，且通常训练不稳定。</li>
<li><strong>融合器设计</strong>：与简单的拼接或相加相比，使用基于交叉注意力的融合器能带来更好的性能提升。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个从小波分析视角出发的策略学习框架，为处理长时域、多模态任务提供了新思路。</li>
<li>设计并实现了一个基于可学习提升方案的小波策略网络，通过引入因果膨胀卷积、Transformer融合器等关键设计，解决了将经典信号处理工具融入现代策略学习中的工程挑战。</li>
<li>在五个复杂的仿真基准上进行了全面评估，结果表明小波策略在多种长时域任务上优于或与现有先进方法性能相当。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，方法引入了额外的计算开销，因为需要进行多尺度的分解与合成。此外，对于最优小波基函数和学习动态的理论分析尚不充分。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>多模态与精度</strong>：小波的多尺度特性为显式建模动作的不同模式（低频趋势）和高精度细节（高频成分）提供了天然框架，可进一步探索其在需要极高操作精度任务中的应用。</li>
<li><strong>可学习信号处理</strong>：将可学习的提升方案与其他网络架构（如更复杂的序列模型）结合，可能催生更强大的时空信号处理模块。</li>
<li><strong>理论拓展</strong>：未来工作可以深入分析小波策略的优化动态、泛化性能以及与经典控制理论、滤波技术的联系。</li>
<li><strong>效率优化</strong>：研究如何降低多尺度计算的开销，例如通过设计更高效的网络结构或动态尺度选择机制。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出一种用于长时程任务策略学习的小波策略框架。核心问题是解决复杂长时程任务中策略学习需处理长序列、多模态动作分布的挑战。关键技术为引入可学习的多尺度小波分解与提升方案，对观测和动作序列进行多分辨率分析，以分离全局趋势与细节噪声，从而增强策略的精确性与鲁棒性。该方法在机器人操作、自动驾驶等多个复杂场景中验证了其有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.04331" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>