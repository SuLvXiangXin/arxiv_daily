<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.14688" target="_blank" rel="noreferrer">2509.14688</a></span>
        <span>作者: Yong-Lu Li Team</span>
        <span>日期: 2025-09-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人触觉感知学习面临数据收集和表示的关键挑战，主要源于数据稀缺、稀疏以及现有系统缺乏力反馈。便携式手持设备（如UMI）为野外演示收集提供了一种有前景的中间方案，但在触觉感知学习方面存在局限性。首先，人类演示者依赖触觉反馈来调整操作策略，这使得遥操作系统难以收集对力敏感任务的演示数据。其次，机器人学习中的触觉信号非常稀疏，有效接触仅占操作轨迹的不到10%。这削弱了传统触觉学习方法：直接模仿学习受限于数据稀缺；自监督预训练可能学习到错误的归纳偏差（如平移不变性）；视觉-触觉对齐范式忽略了视觉和触觉模态在考虑接触力时存在一对多关系，导致学到的表示难以泛化到狭窄的任务特定场景之外。</p>
<p>本文针对上述痛点，提出了一种结合新型硬件和算法协同设计的触觉表示学习解决方案。核心思路是：设计一个可扩展的硬件系统（exUMI）高效收集大规模触觉-动作对齐数据，并提出一种动作感知、任务无关的触觉预测预训练（TPP）框架，通过条件化的未来触觉预测任务来学习蕴含接触动态的触觉表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了一种软硬件协同设计的方法，包括exUMI硬件系统和Tactile Predictive Pretraining (TPP)算法框架。</p>
<p><strong>exUMI硬件系统</strong>：exUMI是对UMI系统的可扩展升级，是一个集成了视觉触觉传感器和动作捕捉系统的便携式手持数据收集设备。其设计遵循三个原则：精确的机器人本体感知、可扩展性和便携性。</p>
<p><img src="https://arxiv.org/html/2509.14688v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：触觉感知机器人学习的软硬件协同设计概览。左侧为exUMI硬件系统（UMI的扩展升级），中间为通过时序触觉预测学习触觉表示的算法流程，右侧为在多个真实世界机器人任务上的评估。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.14688v1/x2.png" alt="硬件系统"></p>
<blockquote>
<p><strong>图2</strong>：exUMI硬件系统。我们通过将本体感知解耦为AR动作捕捉系统和用于精确夹爪宽度的旋转编码器来扩展UMI框架。具有自动延迟校准的中央控制器支持以最大移动性集成额外传感器。两个视觉触觉传感器附着在夹爪指尖。</p>
</blockquote>
<p>核心硬件创新包括：</p>
<ol>
<li><strong>稳健的本体感知子系统</strong>：采用基于AR的动作捕捉（Meta Quest 3）和磁旋转编码器（AS5600），替代了脆弱的SLAM和ArUco系统，实现了接近100%的数据可用率。AR动作捕捉对遮挡鲁棒，旋转编码器解决了ArUco标记的遮挡和鱼眼畸变问题。</li>
<li><strong>中央控制器与可扩展性</strong>：中央控制器（Orange Pi）作为通用传感器枢纽，同步捕获AR头显、旋转编码器和任何额外传感器（如触觉传感器）的数据，并设计了时间传感器对齐协议（误差&lt;50 ms）。</li>
<li><strong>指尖视觉触觉传感器</strong>：基于9DTact的低成本视觉触觉传感器，重新设计了具有接触保护结构的传感器模型，并使用定制模具确保硅胶厚度一致，显著增强了耐用性和稳定性。</li>
<li>其他改进包括非平行夹爪机械设计、GoPro视觉输入以及低成本、DIY友好的特性（默认配置起价698美元）。</li>
</ol>
<p><strong>TPP算法框架</strong>：基于exUMI硬件，本文提出了一个动作感知、任务无关的触觉表示学习框架，将触觉表示学习定义为一个条件化的未来预测任务：( p_{\theta}\left(\mathbf{T}<em>{t+1:t+n}|\mathcal{E}</em>{T}(\mathbf{T}<em>{t-n+1:t}),\mathcal{E}</em>{V}(\mathbf{V}<em>{t}),\mathcal{E}</em>{A}(\mathbf{A}<em>{t-n+1:t+n})\right) )。触觉编码器 ( \mathcal{E}</em>{T} ) 在这个预测预训练中学习蕴含触觉动态的信息化表示。</p>
<p><img src="https://arxiv.org/html/2509.14688v1/x6.png" alt="TPP流程"></p>
<blockquote>
<p><strong>图6</strong>：提出的触觉表示学习流程。表示模型 ( \mathcal{E}_{T} ) 在时序触觉预测任务中学习。历史触觉和动作特征被融合并映射为未来触觉特征的预测，使用一个以未来动作和当前图像编码为条件的潜在扩散模型（LDM）。</p>
</blockquote>
<p>具体流程如下：</p>
<ol>
<li><strong>多模态编码</strong>：采用VAE模型作为触觉模态的编码器和解码器（( \mathcal{E}<em>{T},\mathcal{D}</em>{T} )），每个触觉图像被分块并转换为嵌入序列。与UVA不同，此处的VAE模型是可学习的。</li>
<li><strong>触觉预测</strong>：对历史触觉块嵌入和动作特征进行随机掩码，并通过Transformer融合这两个模态。融合后的n个历史潜在表示被输入到一个潜在扩散模型（LDM）中，以预测未来触觉信号的潜在表示，其中未来动作 ( A_{t+1:t+n} ) 和当前RGB图像 ( V_{t} ) 的嵌入作为条件。触觉图像由VAE解码器重建。预测模型受混合损失约束：扩散损失 ( \mathcal{L}<em>{diff} ) 和重建MSE损失 ( \mathcal{L}</em>{recon} )。</li>
<li><strong>策略学习</strong>：学习预测表示后，冻结触觉编码器 ( \mathcal{E}_{T} ) 用于所有下游策略。使用常见的模仿学习来训练多模态策略。丰富的触觉动态知识已被编码到触觉模型中，因此该方法可以生成更鲁棒的表示，缓解数据稀缺和低多样性的问题。</li>
</ol>
<p><strong>数据收集</strong>：利用exUMI的便携性，高效收集了大规模触觉-动作对齐的人类“玩耍”数据。收集者与300多个物体（从刚性工具到可变形织物和颗粒材料）在10个真实世界环境中随机交互，最终收集了超过100万帧对齐的图像-触觉-动作数据。该数据具有丰富的接触（超过60%的活跃触觉帧），收集效率是遥操作系统的10倍以上。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：使用exUMI收集的超过100万帧的大规模触觉-动作对齐数据集进行预训练。下游任务评估使用exUMI收集的100-200个演示数据。</li>
<li><strong>实验平台</strong>：Flexiv Rizon 4机械臂配合Flexiv Grav自适应夹爪，GoPro相机作为视觉输入。</li>
<li><strong>策略模型</strong>：采用扩散策略，ViT图像骨干模型，多模态输入直接特征拼接。</li>
<li><strong>任务</strong>：包括常规操作任务（拾取立方体/胡萝卜/西兰花、插入笔、堆叠立方体）和复杂的触觉感知任务（放置球、打开瓶盖、拉抽屉、孔中插钉）。</li>
<li><strong>对比基线</strong>：视觉策略、视觉+原始触觉策略、视觉+TPP预训练触觉策略。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>TPP预训练有效性</strong>：在验证集上评估不同输入设置的触觉预测MSE误差（表1）。结果表明，视觉和动作序列的多模态条件反射可以减少触觉预测误差。其中，包含动作感知的预测（TPP）取得了最佳性能（MSE 0.0099），表明模型在充分考虑动作序列的情况下隐式学习了前向触觉动态。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.14688v1/x7.png" alt="预测可视化"></p>
<blockquote>
<p><strong>图7</strong>：TPP在验证集（未见数据）上的时序触觉预测示例。红色和绿色分别代表凹面和凸面区域。模型展现出从动作序列中学习线索的能力，例如，在第一个案例中，模型从未来动作推断出触觉接触将在两帧后停止。</p>
</blockquote>
<ol start="2">
<li><p><strong>exUMI数据收集效率与质量</strong>：对于简单的拾放任务，用户可在20分钟内收集100个演示，实现100%数据可用率和超过70%的行为克隆任务成功率。exUMI的数据处理成功率接近100%，而原始UMI系统低于60%。在非触觉模仿学习任务上，仅使用视觉的策略取得了良好性能（如拾取立方体成功率85%），证明了exUMI提供的演示数据在空间上的高质量。</p>
</li>
<li><p><strong>触觉感知模仿学习性能</strong>：在复杂的触觉敏感任务上评估（表3）。TPP预训练的触觉表示带来了显著的性能提升。例如，在“放置球”和“旋转瓶盖”任务上，TPP相比原始触觉策略带来&gt;15%的性能增益。在“拉抽屉（随机负载）”和“孔中插钉（插入）”这两个对力敏感的环节，TPP将成功率分别提升至95%和80%，显著优于视觉策略和原始触觉策略。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.14688v1/x8.png" alt="实际应用"></p>
<blockquote>
<p><strong>图8</strong>：触觉感知策略的真实世界运行示例。黄色箭头表示切向力，从红色区域（因压力而凹陷）指向绿色区域（硅胶凸起）。触觉信号为抓握手柄的姿势（红色区域）提供了信息，这对于选择拉动方向至关重要。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：表1的预测MSE结果本身可视作对TPP模型输入条件的消融。结果显示，同时使用历史触觉、未来动作和当前图像作为条件（即完整的TPP设置）能获得最低的预测误差，证明了动作和视觉信息对于学习准确触觉动态表示的必要性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>exUMI硬件系统</strong>：一个触觉机器人数据收集系统，通过AR动作捕捉、旋转编码器和升级的视觉触觉传感器，增强了UMI，实现了100%可靠的本体感知和触觉感知，显著提高了数据收集效率和质量。</li>
<li><strong>触觉预测预训练（TPP）</strong>：一种动作感知、任务无关的触觉表示学习方法，通过前向触觉预测的代理任务，利用接触动态学习触觉表示，在下游触觉敏感任务上带来了显著的性能提升（超过20%）。</li>
<li><strong>大规模数据集</strong>：发布了一个超过100万帧的大规模触觉-动作对齐机器人数据集，收集效率是遥操作的10倍以上。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>硬件方面</strong>：AR头显存在热不适和颈部劳损的人体工程学问题；虽然9DTact传感器的耐用性和一致性已提升，但仍有改进空间。</li>
<li><strong>算法方面</strong>：由于动作维度低和相机视角有限，交互和运动信息受限，导致触觉预测性能不完美。未来计划通过集成力-力矩测量和多视角视觉输入来解决。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>软硬件协同设计是解决机器人学习数据瓶颈的有效途径，特别是在多模态感知领域。</li>
<li>将触觉信号建模为动作条件化的动态过程，而非静态观察，更符合人类触觉感知原理，是学习通用触觉表示的一个有前景的方向。</li>
<li>探索更舒适、便携的硬件解决方案，以及融合更丰富的传感器信息（如力觉、多视角视觉）来进一步提升触觉动态模型的预测和推理能力，是未来的重要研究方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对触觉感知机器人学习面临的数据稀缺、稀疏性及缺乏力反馈等核心问题，提出了一种硬件与算法协同设计的解决方案。关键技术包括：1）硬件exUMI，作为UMI系统的可扩展升级，通过增强本体感知、模块化视觉触觉传感与自动校准，实现高效数据采集；2）算法Tactile Prediction Pretraining，通过动作感知的时间触觉预测学习表征，以捕捉接触动态并缓解触觉稀疏性。实验表明，该系统实现了100%的数据可用性，收集了超过100万触觉帧，且TPP表征在真实机器人任务中优于传统触觉模仿学习方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.14688" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>