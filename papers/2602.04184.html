<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.04184" target="_blank" rel="noreferrer">2602.04184</a></span>
        <span>作者: Martinez-Sanchez, Angel, Roy, Parthib, Greer, Ross</span>
        <span>日期: 2026/02/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前自动驾驶领域，让车辆理解并遵循乘客的自然语言指令是一个新兴方向。然而，大多数现有的指令跟随规划器依赖于模拟器（如CARLA）或固定的命令词汇表（如“左转”、“右转”），这限制了其在真实世界复杂场景中的泛化能力。这些方法无法处理自由形式、具有指称性（如“跟在灰色轿车后面”）和多步骤的细粒度乘客指令。</p>
<p>本文针对这一关键痛点，利用新发布的doScenes数据集——首个将自由形式自然语言指令与nuScenes数据集中的真实世界车辆运动轨迹关联起来的数据集，探索了指令对运动规划的直接影响。核心思路是将doScenes中的乘客式指令作为额外条件，集成到基于多模态大语言模型（MLLM）的端到端驾驶框架OpenEMMA中，从而研究语言指令是否能以及如何有意义地引导车辆的轨迹规划。</p>
<h2 id="方法详解">方法详解</h2>
<p>本工作的目标是研究自然语言指令（特别是doScenes中的指令）如何改变自动驾驶系统的运动规划。为此，我们将doScenes指令集成到OpenEMMA框架中。OpenEMMA是一个开源的端到端驾驶框架，它以前摄像头图像和自车状态为输入，通过视觉-语言模型（VLM）进行推理，最终输出未来10个时间步的自车速度和曲率序列，进而生成轨迹。</p>
<p><strong>整体流程与指令集成</strong>：方法的核心是在不显著修改OpenEMMA原有架构的前提下，将doScenes指令作为“乘客指令”注入到模型的提示词中。具体而言，在每个场景推理时，将对应的doScenes指令插入到OpenEMMA原有的场景描述提示词模板中，格式为：“乘客说：‘<doScenes_instruction>’。始终优先考虑乘客的指令，除非不安全；如果遵守指令不安全，请简要解释并选择最安全的替代方案。”通过仅改变这一输入条件，可以确保输出轨迹的任何变化都能直接追溯到语言指令的影响。</p>
<p><strong>OpenEMMA的核心推理阶段</strong>：集成指令后，OpenEMMA遵循其原有的三阶段推理流程生成轨迹：</p>
<ol>
<li><strong>场景描述</strong>：收集前视摄像头图像，并提示VLM生成一个聚焦于交通灯、其他车辆/行人、车道线等要素的人类可读场景摘要。</li>
<li><strong>物体识别</strong>：同样基于图像，提示VLM列出两到三个重要的道路使用者及其位置和重要性。</li>
<li><strong>意图估计</strong>：基于当前观察（以及历史意图），确定并更新自车的驾驶意图（例如，左转、右转、直行及速度）。<br>完成这三个阶段后，模型预测出10步的曲率-速度对，最终合成为轨迹坐标用于评估。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04184v1/Screenshot_2025-11-14_030243.png" alt="研究场景示例"></p>
<blockquote>
<p><strong>图1</strong>：动态或突然非结构化的驾驶场景需要人类介入提供指令，以实现适当的场景推理和规划。图中展示了在模糊场景下，乘客的指令（如“左转进入停车场”或“直行但保持在锥桶左侧”）将决定车辆的安全和目标对齐行为。</p>
</blockquote>
<p><strong>创新点</strong>：与DriveMLM（依赖固定命令）、GPT-Driver（不接收乘客指令）或LMDrive（基于模拟器）等现有语言条件规划模型相比，本文的创新在于<strong>首次将细粒度的、自由形式的乘客式指令应用于真实世界（nuScenes）的轨迹预测模型</strong>，并建立了可复现的评估基线。这标志着从“语言作为描述”到“语言作为规划条件”的转变。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在doScenes数据集上进行，该数据集基于nuScenes。研究筛选了849个包含“可操作”指令的nuScenes场景（占所有注释的36%），确保每个评估样本都包含能引发运动计划改变的指令。使用的VLM是LLaVA-1.6-Mistral-7B，以保持与OpenEMMA的兼容性和实验的可复现性。评估指标为标准轨迹对齐度量<strong>平均位移误差（ADE）</strong>。对比的基线是<strong>无指令条件</strong>下的OpenEMMA。</p>
<p><strong>关键定量结果</strong>：</p>
<ol>
<li><strong>整体改善与异常值纠正</strong>：在所有849个场景上，无指令基线产生了多个极端异常值（ADE均值高达6201.443），而任何形式的指令条件都大幅提升了鲁棒性，使平均ADE降低了98.7%。分析发现，这些异常值常出现在车辆需要长时间静止的场景（如等红灯），而无指令模型有时会预测出界路径点。指令帮助模型理解了等待条件。</li>
<li><strong>指令质量的影响</strong>：在去除97.5百分位（Q97.5）的异常值后，指令的细微差别对结果的影响更清晰。整体上，指令条件下的平均ADE（2.732-3.11）与无指令基线（2.879）相比有变化，其中<strong>最佳指令（Best ADE）表现最好（2.732）</strong>，优于无指令情况。这表明提示词的措辞会影响性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04184v1/figures/Scene760_OoB_Example.png" alt="异常值示例"></p>
<blockquote>
<p><strong>图2</strong>：OpenEMMA无指令基线模型预测出界路径点的示例。该点被设定在十字路口对面，连接的轨迹线延伸到了场景边界之外。这类异常在无指令时更频繁。</p>
</blockquote>
<p><strong>指令分析</strong>：</p>
<ul>
<li><strong>指令长度</strong>：如表II所示，典型长度（9-12词）的指令相对于无指令基线带来的改进最大。过短或过长的指令效果较差。</li>
<li><strong>指令指称性</strong>：如表III所示，<strong>引用动态物体</strong>（如“跟随黄色汽车”）的指令产生的ADE最低（2.764），优于仅引用静态物体（3.027）或无指称（3.397）的指令。这表明模型受益于与移动物体相关的时序和关系上下文。</li>
</ul>
<p><strong>定性结果</strong>：尽管整体ADE差异不大，但在安全关键场景中，措辞良好的指令能显著纠正模型行为。</p>
<p><img src="https://arxiv.org/html/2602.04184v1/figures/scene238_noinstr.jpg" alt="定性结果对比"><br><img src="https://arxiv.org/html/2602.04184v1/figures/scene642_noinstr.jpg" alt="定性结果对比"><br><img src="https://arxiv.org/html/2602.04184v1/figures/scene238_doscene2.jpg" alt="定性结果对比"><br><img src="https://arxiv.org/html/2602.04184v1/figures/scene642_doscene.jpg" alt="定性结果对比"></p>
<blockquote>
<p><strong>图3</strong>：OpenEMMA预测在有/无自然语言指令下的定性比较。左列（场景238）：无指令时车辆轨迹穿过人行横道；指令“在十字路口前右侧路缘停车”使模型正确停车。右列（场景642）：无指令时车辆错误预测左转进入对向车道；指令“绿灯时直行”引导模型正确直行。这些案例显示指令能提升空间安全性和场景适应性规划。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>首创性应用</strong>：首次将细粒度的、自由形式的乘客指令条件应用于真实世界自动驾驶轨迹规划，并建立了基于doScenes数据集和OpenEMMA框架的可复现基线。</li>
<li><strong>多层次分析</strong>：不仅对比了指令条件与无指令基线的性能，还深入分析了指令长度、指称性等语言特征对规划结果的影响，并识别了模型在无指令时的异常失败模式。</li>
<li><strong>可行性证明</strong>：证明了自然语言指令能够直接且可测量地影响运动规划，尤其在纠正极端错误和提升安全关键场景行为方面具有价值。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>数据分布差距</strong>：doScenes指令是事后注释的“理想”指令，与真实即时乘客语音存在分布差距。</li>
<li><strong>指令安全性</strong>：当前方法默认遵循所有指令，缺乏对不安全、矛盾或不可行指令的识别与拒绝机制。</li>
<li><strong>评估指标单一</strong>：仅使用ADE进行评估，只能证明轨迹更接近人类驾驶的<strong>一种</strong>可能路径，无法完全确认模型真正理解了指令语义。</li>
<li><strong>模型与架构限制</strong>：实验仅使用了LLaVA-1.6-Mistral-7B和OpenEMMA框架，结果可能特定于该组合。</li>
</ol>
<p><strong>后续启示</strong>：<br>本研究为连接人类意图表达与机器人响应规划迈出了一步。未来工作可围绕以下方向展开：扩展至<strong>闭环评估</strong>以测试长期指令跟随能力；探索更强大的MLLM（如Qwen2.5-VL）以提升推理能力；将相同指令应用于其他规划架构（如图结构模型、闭环系统）进行更普适的基准测试；以及开发机制使系统能够<strong>判断并安全处理有问题的乘客指令</strong>，这对于实现可靠的人机协同自动驾驶至关重要。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自动驾驶中自然语言指令跟随规划器依赖模拟或固定词汇、现实泛化能力受限的核心问题，提出一种基于视觉-语言-动作模型的人机协同运动规划方法。关键技术是适配OpenEMMA端到端驾驶框架，将doScenes数据集的自由形式指令作为乘客式提示集成到视觉-语言接口，实现语言条件化的轨迹生成。在849个真实场景上的实验表明，指令条件化显著提升了规划鲁棒性，平均轨迹误差（ADE）降低98.7%；精心设计的指令提示可使ADE进一步优化5.1%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.04184" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>