<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.01801" target="_blank" rel="noreferrer">2512.01801</a></span>
        <span>作者: Yonghui Wu Team</span>
        <span>日期: 2025-12-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大规模视觉-语言-动作模型在构建通用机器人智能体方面取得了显著进展，展现出跨物体、环境和语义概念的泛化能力。然而，通用性不等同于可靠性，现有VLA策略在现实部署中面临两大根本挑战：1）<strong>灵巧性与精度</strong>：对可变形物体进行毫米级控制仍未解决；2）<strong>长时程鲁棒性</strong>：误差会随时间步累积，在高精度灵巧操作中尤为严重。以穿鞋带任务为例，它同时要求处理可变形物体、毫米级精度和长时程操作能力。</p>
<p>现有方法的核心假设是<strong>人类演示的最优性</strong>。但在高度灵巧和精密的操作任务中，人类演示者会因减速、犹豫而引入噪声和次优的演示数据。此外，标准的离线训练（如通过滑动窗口预测固定长度的动作块）与为获得平滑控制而采用的推理时优化（如时序集成、异步滚动时域控制）之间存在<strong>训练与推理的不匹配</strong>，这进一步放大了次优演示的负面影响。</p>
<p>本文针对从通用VLA策略到可靠、高精度专用策略的转化难题，提出了一种强化学习增强的多阶段训练流程。其核心思路是：首先通过离线RL学习一个任务进度评估器来过滤次优演示数据；然后利用形态对称性进行数据增强；最后通过在线RL在潜在空间进行探索，以对齐策略的部署行为，从而将一个通用VLA策略转变为擅长长时程、灵巧、精密操作的专家。</p>
<h2 id="方法详解">方法详解</h2>
<p>GR-RL采用一个多阶段的强化学习增强训练流程，对次优且不匹配的人类演示进行过滤、增强和强化。整体流程包含三个阶段：1）基于学习到的任务进度进行离线过滤的行为克隆；2）简单有效的动作增强；3）在线强化学习。</p>
<p><img src="https://arxiv.org/html/2512.01801v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：GR-RL多阶段训练流程。它通过离线RL学习任务进度以过滤数据，进行形态对称性数据增强，并执行在线RL以对齐部署行为。</p>
</blockquote>
<p><strong>模型架构</strong>：GR-RL采用混合Transformer架构，包含一个视觉-语言-动作策略模型 π_θ 和一个多任务评论家 Q_ϕ，总参数量为50亿。策略 π_θ 以Qwen2.5-VL-3B-Instruct作为VLM主干，并使用通过流匹配目标训练的动作扩散Transformer来预测k长度的动作块。评论家 Q_ϕ 是一个因果Transformer，采用分布强化学习，将值预测为具有上下界的离散分布，在稀疏奖励设置下比非分布评论家更鲁棒。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>基于学习进度评估器的数据过滤</strong>：为解决人类演示的次优性问题，GR-RL不直接对所有数据进行行为克隆，而是先训练一个任务进度模型来过滤数据。具体使用TD3+BC算法训练评论家 Q_ϕ，奖励为稀疏的二元奖励（仅在任务成功时给予折扣奖励）。通过时序差分学习成功和失败轨迹（利用重试关键帧进行 hindsight 增强），Q_ϕ 的预测值（取其分类分布均值）能稳健反映任务进度。如图3所示，当操作者失误时，预测进度会出现骤降。GR-RL定义，若一个动作块对应的进度序列中出现超过阈值 δ 的下降，则该样本为次优，并将其从策略训练数据集中剔除。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.01801v3/x3.png" alt="学习到的任务进度"></p>
<blockquote>
<p><strong>图3</strong>：学习到的任务进度示例。在操作者失误时（红色箭头），GR-RL学习到的进度值会明显下降，而基于回归的基线预测则过度平滑，对细微失败不敏感。</p>
</blockquote>
<ol start="2">
<li><p><strong>带有数据增强的模仿学习</strong>：在离线训练阶段，GR-RL引入了一种<strong>形态对称性增强</strong>范式。对于双臂任务设置，通过水平翻转图像并交换左右腕部图像，镜像本体感知状态和动作（在世界坐标系中进行对称变换后转回局部腕部坐标系），并相应地翻转语言指令中的空间描述（如将“左边的孔”改为“右边的孔”）。这种简单的方法有效提升了策略的性能和泛化能力。</p>
</li>
<li><p><strong>用于策略部署对齐的在线引导</strong>：为弥补训练与推理的不匹配，GR-RL进行在线RL，使模型能在闭环交互中探索和改进。鉴于任务对毫米级精度的要求，在关节位姿上添加噪声难以成功。因此，GR-RL在<strong>潜在空间进行结构化探索</strong>，引导训练好的流策略。具体而言，在共享VLM主干后添加一个轻量级噪声预测器 π_θ‘（参数量51.5M），用于为动作DiT预测初始噪声 ε_t。为避免生成偏离离线分布的动作，对噪声预测器的输出施加偏离正态分布超过阈值 β 的惩罚。同时，在噪声空间蒸馏一个 Q_ϕ‘ 函数以避免在策略优化时反向传播通过流模型。在线训练时，维护离线和在线策略回放缓冲区，并均匀采样，初始时使用离线策略检查点的在线 rollout 预热离线缓冲区。</p>
</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要评估任务是<strong>穿鞋带</strong>，这是一个需要长时程、灵巧和精密操作的挑战性场景。观测包括三个视角的RGB图像、本体感知状态和语言指令。奖励设置为二元稀疏奖励（仅当鞋带正确穿过鞋眼并完全放在桌上时获得奖励1）。实验平台为ByteMini-v2轮式移动操作机器人。</p>
<p><strong>基线方法</strong>：主要对比基线是基础的GR-3模型（在所有人类遥操作数据上进行行为克隆）。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>多阶段训练效果</strong>：如图5左所示，基础GR-3的成功率为45.7%。经过基于进度的数据过滤后，成功率提升至61.6%。在此基础上增加对称性数据增强，成功率进一步提升至72.7%。最后，经过在线RL微调，最终模型在500步评估时取得了<strong>83.3%</strong> 的整体成功率。图5右展示了在线微调过程中成功率的移动平均曲线，初期因分布偏移有所下降，随后迅速恢复并超越离线性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.01801v3/x5.png" alt="多阶段训练成功率"></p>
<blockquote>
<p><strong>图5</strong>：左：多阶段训练配方的成功率。数据过滤、镜像增强和在线调优均对最终性能有贡献。右：在线微调期间每回合的二元成功信号（点）和成功率的移动平均（曲线）。</p>
</blockquote>
<ol start="2">
<li><strong>分阶段失败分析</strong>：图6详细展示了不同模型在关键子阶段（拾取正确鞋带、穿入正确鞋眼、交接给另一夹爪、拉紧鞋带）的成功率。彩色区域表示完成各阶段的成功率，阴影区域表示相较于前一阶段的成功率下降。数据过滤和在线RL能大幅减少穿眼阶段的失败。数据增强则全面提升了各阶段的性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.01801v3/x6.png" alt="分阶段成功率"></p>
<blockquote>
<p><strong>图6</strong>：不同模型完成中间阶段的详细成功率。阴影区域的高度表示从前一阶段到当前阶段成功率的下降。“过滤BC+增强”模型是在线引导RL的起点。</p>
</blockquote>
<ol start="3">
<li><strong>进度评估器的消融实验</strong>：论文比较了基于RL的分布评论家与基于回归的基线以及非分布评论家。如图3和图7所示，回归基线预测过于平滑，对细微失败（如差几毫米未穿入）和具有长期影响的动作不敏感。非分布评论家在稀疏奖励和长时程设置下，轨迹早期部分存在严重的价值高估，而分布评论家因输出有界，能更鲁棒地收敛到合理尺度，并与真实时序更好对齐。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.01801v3/x7.png" alt="分布与非分布评论家对比"></p>
<blockquote>
<p><strong>图7</strong>：分布与非分布评论家的进度预测对比。非分布评论家输出无界，无法在成功轨迹中反映正向进度。</p>
</blockquote>
<ol start="4">
<li><strong>定性结果</strong>：如图8所示，GR-RL展现出多种鲁棒行为：处理不同颜色和尺寸的鞋子；在鞋带意外掉落或未准确穿入时自动重试；主动调整场景以简化任务（如调整抓握点、重新摆正鞋子位置、从交叉的鞋带中识别并拉出正确的一端等）。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.01801v3/images/case_studies/case_gray.png" alt="鲁棒行为案例"></p>
<blockquote>
<p><strong>图8</strong>：GR-RL在各种情况下的鲁棒行为展示，包括处理不同颜色鞋子、重试、调整抓握和物体位置等。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>GR-RL框架</strong>，一个多阶段的RL增强训练流程，成功将通用VLA策略转化为能完成长时程、灵巧、高精度操作（如穿鞋带）的专家策略，实现了83.3%的成功率。</li>
<li>引入了基于<strong>离线RL学习任务进度评估器</strong>的方法，用于过滤人类演示中的次优片段，显著提升了离线策略的基础性能。</li>
<li>提出了一种简单有效的<strong>形态对称性数据增强</strong>方法，并设计了一种在<strong>潜在空间进行在线探索</strong>的机制，以对齐策略部署，有效缓解了训练与推理的不匹配问题。</li>
</ol>
<p><strong>局限性</strong>：论文提到当前流程存在<strong>行为漂移</strong>问题。在稀疏且噪声的奖励下，策略在在线RL期间的行为可能不稳定。这可能是由于轻量级噪声预测器的容量有限，或在大规模潜在动作空间中信用分配困难所致。</p>
<p><strong>后续启示</strong>：GR-RL展示了通过结合数据过滤、增强和在线RL，从通用基础模型迭代出高性能专用模型的可行性。其关于处理次优演示、训练-推理不匹配以及使用分布评论家提升鲁棒性的见解，对推动可部署机器人研究具有参考价值。未来的工作可以探索如何将改进后的策略蒸馏回基础VLA模型，以获得既能力强又通用的操作策略，并致力于解决在线RL中的行为不稳定问题。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>GR-RL旨在解决现有视觉-语言-动作策略在长视野、高精度灵巧操纵任务中因依赖次优人类演示而性能不足的问题。其核心技术包括：基于离线强化学习的进度过滤、形态对称性增强和在线强化学习的噪声预测，以优化演示并提升策略精度。实验表明，GR-RL在自主穿鞋带任务中达到83.3%的成功率，实现了毫米级控制和长视野推理。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.01801" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>