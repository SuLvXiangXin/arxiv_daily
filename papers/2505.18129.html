<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>One RL to See Them All: Visual Triple Unified Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>One RL to See Them All: Visual Triple Unified Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.18129" target="_blank" rel="noreferrer">2505.18129</a></span>
        <span>作者: Ma, Yan, Du, Linge, Shen, Xuyang, Chen, Shaoxiang, Li, Pengfei, Ren, Qibing, Ma, Lizhuang, Dai, Yuchao, Liu, Pengfei, Yan, Junjie</span>
        <span>日期: 2025/05/23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，强化学习（RL）已成为提升视觉语言模型（VLM）推理能力的一种有前景的后训练方法。然而，现有研究主要集中于数学、科学问答等视觉推理任务，其RL设置与大型语言模型（LLM）中的RL训练范式高度相似。一个关键且尚未探索的问题是，RL能否有效地扩展到视觉感知任务（如目标检测和视觉定位），这些任务需要截然不同的奖励设计和措施来确保训练稳定性。</p>
<p>本文针对视觉推理与视觉感知任务在RL训练中难以统一的问题，提出了首个统一的RL系统V-Triune。其核心思路是设计一个包含三个互补组件的系统，通过样本级数据格式化统一输入，通过验证器级奖励计算提供定制化奖励，并通过源级指标监控诊断问题，同时引入动态IoU奖励机制，从而实现在单一训练流程中联合优化VLM的推理与感知能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>V-Triune系统的目标是使用一个统一的训练流程，联合训练VLM处理视觉推理和感知任务。其整体框架基于三个核心组件，并引入了动态IoU奖励。</p>
<p><img src="https://arxiv.org/html/2505.18129v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：V-Triune系统整体框架。它集成了三个互补组件：样本级数据格式化（统一多样化的任务输入）、验证器级奖励计算（通过专用验证器提供定制奖励）、源级指标监控（诊断数据源级别的问题）。此外，新颖的动态IoU奖励为感知任务提供自适应、渐进式的反馈。</p>
</blockquote>
<p><strong>1. 样本级数据格式化</strong><br>该组件的核心挑战在于，不同任务需要不同类型的奖励、组件和加权策略。例如，数学、谜题和OCR任务基于文本答案的正确性计算奖励，而检测和定位任务则依赖于IoU和边界框格式等空间度量。传统RL设置通常在任务级别定义奖励计算，限制了灵活性。</p>
<p>V-Triune将奖励配置直接定义在样本级别。每个样本指定要计算的奖励类型、它们的相对权重以及要使用的关联验证器。这使得在训练期间能够动态路由奖励并进行细粒度加权，而无需修改核心训练逻辑。</p>
<p><img src="https://arxiv.org/html/2505.18129v2/x3.png" alt="数据格式"></p>
<blockquote>
<p><strong>图3</strong>：用于统一训练的样本级数据方案。该格式使用HuggingFace数据集实现，允许通过在每个样本级别定义<code>reward_model</code>（包括奖励类型、权重如<code>accuracy_ratio</code>/<code>format_ratio</code>）和验证器规范，对奖励计算进行细粒度控制。</p>
</blockquote>
<p>具体数据格式包含<code>images</code>、<code>prompt</code>等通用字段，以及关键的<code>reward_model</code>字段。<code>reward_model</code>内封装了所有与奖励相关的元数据，例如定义每种奖励类型权重的<code>accuracy_ratio</code>和<code>format_ratio</code>字段。通过调整这些值，可以按样本启用、禁用或重新加权特定奖励。<code>verifier</code>字段指定应用于评估的验证器及其所需参数。<code>data_source</code>字段用于源级评估。</p>
<p><strong>2. 验证器级奖励计算</strong><br>与使用固定奖励函数的方法不同，V-Triune实现了一个独立的、异步的奖励服务器来生成RL信号。这种解耦带来了关键优势：增强了针对不同任务的模块化和可扩展性，支持灵活的部署和独立扩展，提高了可维护性，并通过分布式异步处理提升了吞吐量。</p>
<p>“验证器级”定义了奖励计算的粒度：奖励被委托给专门的验证器，每个验证器处理特定的任务组。系统采用异步客户端-服务器架构。</p>
<p><img src="https://arxiv.org/html/2505.18129v2/extracted/6499810/images/reward_server.png" alt="奖励服务器"></p>
<blockquote>
<p><strong>图4</strong>：异步奖励服务器架构。RL训练器通过客户端-服务器代理与远程服务器交互，其中专门的验证器（如MathVerify、Detection）使用任务特定逻辑和动态阈值（如动态IoU阈值）计算奖励。</p>
</blockquote>
<p>客户端从数据加载器收集样本批次，并在线程级别并行处理。每个样本被转换为包含任务特定键值对的结构化负载。客户端包含多个代理工作器，它们使用动态平衡策略异步向服务器发送请求。代理服务器根据数据格式中的“verifier”键将每个请求路由到相应的验证器。每个验证器是用户定义的，包含根据模型输出和真实值计算任务特定奖励的自定义函数。主要使用两种验证器：<code>MathVerifyVerifier</code>（处理推理、OCR和计数任务）和<code>DetectionVerifier</code>（管理检测和定位任务，采用动态IoU奖励和基于格式的奖励）。</p>
<p><strong>3. 源级指标监控</strong><br>在多任务、多源训练中，聚合的或单任务指标通常由于缺乏可追溯性和每个数据源的差异而不足。因此，V-Triune采用源级指标监控，即记录每个<code>data_source</code>的每批次指标。这有助于识别有问题的数据源，实现针对性调试，并揭示跨源学习交互。</p>
<p>具体监控内容包括：每个数据源的奖励值；对于感知任务，记录详细的每个数据源的IoU值（在阈值50、75、95、99处）和mAP分数；响应长度指标（包括整体平均值、正确/错误响应的长度、截断率）；以及反思比率（跟踪15个预定义的反思相关词汇，并计算包含这些词汇的响应比例以及反思响应中的正确比率）。</p>
<p><strong>4. 动态IoU奖励</strong><br>对于目标检测和视觉定位，IoU是评估预测框与真实框重叠的标准指标。初步实验表明，使用阈值化的IoU奖励在获得可比整体性能的同时，能提供更具可解释性和可控性的反馈。</p>
<p>然而，固定的宽松阈值（如ε=0.5）对于VLM的RL训练可能过于宽松，并且会导致奖励模糊性：即使预测边界框仅在真实框周围移动或方向不同，仍可能获得相同的高奖励。这种模糊性最终可能导致检测任务在训练后期性能下降。</p>
<p><img src="https://arxiv.org/html/2505.18129v2/x5.png" alt="动态IoU效果"></p>
<blockquote>
<p><strong>图6</strong>：动态IoU与固定规则IoU（IoU@99）下的训练准确率奖励对比。动态IoU奖励在训练早期提供更积极的信号，并引导模型逐步达到高精度。</p>
</blockquote>
<p>为了应对这些问题，V-Triune提出了动态IoU奖励。该机制的核心是随着训练进行，动态调整用于奖励计算的IoU阈值ε。具体而言，阈值从初始的宽松值（如0.5）开始，随着训练步数逐渐增加到更严格的值（如0.99）。这种渐进式收紧策略确保了在训练早期提供有用的学习信号，同时引导模型最终产生高精度的定位结果，从而实现了稳定且可扩展的训练过程。</p>
<p><strong>RL算法细节</strong><br>V-Triune采用分组相对策略优化（GRPO）算法，并基于近期工作引入了两项关键修改：1) 移除了参考模型及其相关的KL损失，以避免限制探索、防止不稳定的KL散度估计，同时减少GPU内存使用并加速训练；2) 应用了clip-high技巧并使用令牌级损失，这有助于增加输出熵，鼓励更好的探索并提高训练稳定性。更新后的GRPO目标函数如论文公式(1)和(2)所示。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置与基线</strong></p>
<ul>
<li><strong>基准/数据集</strong>：主要在MEGA-Bench Core（涵盖400多个现实世界视觉任务）上进行评估。下游任务评估包括MMMU、MathVista、COCO和CountBench。</li>
<li><strong>训练数据</strong>：围绕四个代表性视觉推理任务（数学、谜题、图表、科学）和四个视觉感知任务（定位、检测、计数、OCR）构建的多样化数据集。</li>
<li><strong>模型</strong>：基于Qwen2.5-VL系列基线模型，实例化了7B和32B大小的Orsta模型系列。</li>
<li><strong>对比方法</strong>：主要与未经RL训练的基础模型（Backbone）进行对比。</li>
</ul>
<p><strong>关键实验结果</strong><br>在MEGA-Bench Core上，Orsta模型在视觉推理和感知任务上均表现出显著的性能提升。具体而言，在不同模型变体上，相对于骨干模型取得了+3.2%、+14.1%和+2.1%的性能增益。</p>
<p><img src="https://arxiv.org/html/2505.18129v2/x1.png" alt="MEGA-Bench结果"></p>
<blockquote>
<p><strong>图1</strong>：Orsta在MEGA-Bench任务上的性能。V-Triune在数学、科学、图表、谜题、检测、定位、计数和OCR任务上进行评估，展示了Orsta相对于骨干模型的显著性能提升。</p>
</blockquote>
<p><strong>消融实验与组件分析</strong></p>
<ol>
<li><p><strong>动态IoU奖励的有效性</strong>：实验比较了不同奖励设计在COCO测试集上的性能。<br><img src="https://arxiv.org/html/2505.18129v2/x4.png" alt="奖励设计比较"></p>
<blockquote>
<p><strong>图5</strong>：不同奖励设计在COCO测试集上的性能比较。(a) IoU-based与mAP-based奖励在多目标子集上的对比；(b) 原始IoU奖励与规则IoU奖励在单目标子集上的对比；(c, d) 规则IoU奖励与动态IoU奖励在COCO多目标子集和OVDEval否定子集上的对比。动态IoU奖励展现出优越且稳定的性能。</p>
</blockquote>
<p>图5显示，动态IoU奖励策略在多个子集上均优于固定的规则IoU奖励（如IoU@99），尤其是在训练后期能维持更高的准确率（图6）。</p>
</li>
<li><p><strong>系统组件的贡献</strong>：论文通过逐步引入V-Triune的组件进行训练，验证了其必要性。完整系统（包含样本级格式化、验证器级计算、源级监控和动态IoU）在所有任务上取得了最佳且最稳定的性能。消融实验表明，缺少任何组件都会导致某些任务性能下降或训练不稳定。</p>
</li>
</ol>
<p><strong>下游任务泛化</strong><br>Orsta模型在多个下游基准测试中表现出强大的泛化能力，包括MMMU（多学科理解）、MathVista（数学视觉推理）、COCO（目标检测）和CountBench（计数），进一步验证了V-Triune方法的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong></p>
<ol>
<li>提出了首个统一、可扩展的RL系统V-Triune，能够在单一范式中联合训练VLM处理视觉推理和感知任务。</li>
<li>提出了动态IoU奖励机制，通过自适应调整IoU阈值，显著增强了视觉感知任务的训练稳定性和性能。</li>
<li>建立并展示了一套全面的训练方法学，包括关键的工程优化，实现了跨越八种不同VLM任务的有效且稳定的RL训练，并发布了高性能的Orsta模型系列。</li>
</ol>
<p><strong>局限性</strong><br>论文自身提到的局限性包括：训练数据规模相对有限；动态IoU奖励的调度策略（如阈值增长曲线）有待进一步探索和优化。</p>
<p><strong>对后续研究的启示</strong></p>
<ol>
<li><strong>统一RL框架的潜力</strong>：证明了为异构视觉任务设计统一RL训练框架是可行且高效的，为未来构建更通用的VLM后训练范式提供了新思路。</li>
<li><strong>奖励设计的创新</strong>：动态IoU奖励为解决连续空间输出任务的RL奖励稀疏性和模糊性问题提供了有效方案，可启发其他需要精细回归任务（如姿态估计、分割）的奖励设计。</li>
<li><strong>系统化工程的重要性</strong>：样本级、验证器级、源级的三层设计强调了模块化、可诊断性和可扩展性在复杂多任务RL系统中的关键作用，为大规模RL系统开发提供了工程实践参考。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出V-Triune系统，旨在解决当前视觉语言模型（VLMs）强化学习训练主要局限于推理任务、难以统一处理视觉感知任务（如检测、定位）的问题。其核心技术包括三重组件：样本级数据格式化统一任务输入、验证器级奖励计算提供定制化奖励、源级指标监控进行数据源诊断，并设计了动态IoU奖励以优化感知任务反馈。基于7B和32B骨干模型实例化的Orsta模型，在包含数学、科学、检测、定位等8类任务的MEGA-Bench评测中，多个模型变体性能显著提升，最高达到+14.1%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.18129" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>