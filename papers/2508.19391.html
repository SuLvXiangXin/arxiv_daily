<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LaVA-Man: Learning Visual Action Representations for Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>LaVA-Man: Learning Visual Action Representations for Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.19391" target="_blank" rel="noreferrer">2508.19391</a></span>
        <span>作者: Changjae Oh Team</span>
        <span>日期: 2025-08-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>语言引导的机器人操作是机器人学的一项基本任务，要求学习能够有效关联视觉观察与文本指令，并能映射到相应机器人动作的鲁棒表征。当前主流方法（如CLIPort及其变体）利用CLIP等预训练的视觉-语言基础模型，将图像和文本编码到统一的嵌入空间，计算其相似度，然后训练模型将此相似度映射到机器人动作。然而，这种两步法的表征缺乏因果基础，未能捕捉输入视觉状态和文本指令如何导致机器人执行动作后的结果视觉状态，即没有学习到操作所必需的、真正的语言引导的视觉-动作表征。</p>
<p>本文针对上述“表征缺乏因果性”的痛点，提出了通过自监督代理任务学习视觉-动作表征的新视角。其核心思路是：通过一个掩码目标图像重建任务，让模型根据初始图像和文本指令预测被部分掩码的目标图像，从而迫使模型隐式学习视觉动态与动作语义之间的关联，捕获语言引导操作的底层因果性。</p>
<h2 id="方法详解">方法详解</h2>
<p>LaVA-Man的框架分为两个阶段：1) <strong>视觉-动作表征学习</strong>：通过自监督代理任务预训练模型，学习关联视觉与文本模态的表征；2) <strong>机器人动作预测</strong>：在预训练模型上附加一个轻量级动作预测头，使用少量演示进行微调，将学习到的表征映射为具体的机器人动作。</p>
<p><img src="https://arxiv.org/html/2508.19391v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：LaVA-Man网络结构及预训练任务示意图。采用孪生ViT编码器，仅对目标图像应用非对称掩码。输入图像的视觉特征首先与文本特征融合，然后在解码器中与从掩码目标图像提取的特征整合。预训练时输出重建的目标图像，下游任务可接入不同的输出头（如动作预测头）。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉编码与掩码</strong>：将操作前后的场景图像 (\mathbf{o}_s) 和 (\mathbf{o}_f) 分割为图像块。仅对目标图像 (\mathbf{o}_f) 进行高比例随机掩码，得到 (\tilde{\mathbf{o}}_f)，而初始图像 (\mathbf{o}_s) 保持完整。这种<strong>非对称掩码</strong>是关键设计，迫使模型基于初始状态和指令来推理目标状态。</li>
<li><strong>视觉-文本融合</strong>：使用CLIP文本编码器从语言指令 (\mathbf{l}<em>{s \to f}) 提取文本嵌入 (\mathbf{e}</em>{s \to f})。采用多阶段交叉注意力机制进行融合：首先在初始图像特征 (\mathbf{v}<em>s) 和文本嵌入 (\mathbf{e}</em>{s \to f}) 之间进行双向交叉注意力；然后，使用掩码目标图像的特征 (\mathbf{v}<em>f) 去查询上述融合后的特征，最终输出融合特征 (\mathbf{h}</em>{s \to f})。</li>
<li><strong>目标图像预测头</strong>：一个轻量级MLP头 (\Psi_p) 接收 (\mathbf{h}_{s \to f})，为每个图像块生成RGB值，输出预测的目标图像 (\hat{\mathbf{o}}_f)。预训练损失是预测图像与真实目标图像之间的 (\mathcal{L}_2) 损失。</li>
<li><strong>机器人动作预测</strong>：微调时，增加一个动作预测头 (\Psi_a)。在测试时，由于目标图像未知，将目标图像完全掩码输入模型。动作头接收融合特征 (\mathbf{h}_{s \to f})、初始图像 (\mathbf{o}_s) 以及模型预测的目标图像 (\hat{\mathbf{o}}<em>f) 来预测动作 (\mathbf{a}</em>{s \to f})。对于桌面操作，动作表示为拾取和放置的SE(2)位姿，通过预测亲和力图并取最大值点得到；对于关节控制，动作表示为9维向量（7个关节角+2个抓握状态指示器）。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ul>
<li><strong>代理任务创新</strong>：不同于基于视频的预训练方法（可能利用时间冗余），也不同于通用的掩码图像重建（如MAE），本文提出的“<strong>语言条件下的目标图像预测</strong>”任务直接针对操作中的因果变换进行建模。</li>
<li><strong>非对称掩码策略</strong>：仅掩码目标图像，使模型必须依据初始状态和指令来推理结果，强化了因果学习。</li>
<li><strong>高效微调</strong>：学到的通用表征只需少量演示即可快速适应下游任务。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：使用了五个评估基准：1) <strong>Ravens</strong>（10项桌面重排任务）；2) 本文提出的 <strong>OOPP数据集</strong>（用于类内和类间泛化评估）；3) <strong>Referring Expression Grounding</strong>（指代表达式接地）；4) <strong>Franka Kitchen</strong>（模拟厨房视觉运动控制）；5) <strong>真实机器人实验</strong>（UR5机械臂，10项任务）。</li>
<li><strong>对比基线</strong>：包括基于基础模型的方法（<strong>CLIPort</strong>）和自/弱监督预训练方法（<strong>Voltron</strong>、<strong>MPI</strong>及其自监督变体MPI‡）。</li>
<li><strong>预训练数据</strong>：混合了12万样本，来自合成的OOPP数据集以及真实机器人视频数据集Bridge和DROID（仅取首尾帧）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>Ravens基准</strong>（表1）：LaVA-Man在10项任务上的平均成功率达到<strong>0.81</strong>，显著优于其他预训练方法（Voltron: 0.54， MPI‡: 0.50）和专为拾放设计的CLIPort（0.73）。在涉及彩色几何体的任务上表现相对较弱，论文归因于预训练数据中此类物体较少。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.19391v2/x2.png" alt="Ravens结果"></p>
<blockquote>
<p><strong>表1</strong>：在Ravens基准上的结果。LaVA-Man在整体平均成功率上领先，尤其在涉及真实扫描物体的任务上优势明显。</p>
</blockquote>
<ol start="2">
<li><strong>OOPP数据集</strong>（表2）：在类内（intra-class）和类间（inter-class）泛化测试中，LaVA-Man均取得最佳性能，平均成功率为**79.6%**，验证了其从多样对象先验中学习并泛化的能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.19391v2/x3.png" alt="OOPP结果"></p>
<blockquote>
<p><strong>表2</strong>：在OOPP数据集上的结果。我们的方法在序列打包和分组打包任务上，对于已见类、类内未见实例和类间未见类别均表现稳健。</p>
</blockquote>
<ol start="3">
<li><strong>其他下游任务</strong>（表3）：<ul>
<li><strong>指代表达式接地</strong>：LaVA-Man达到<strong>0.97</strong>的准确率，与有监督方法（SUGAR: 0.97）相当，优于其他自监督方法。</li>
<li><strong>Franka Kitchen</strong>：取得**79.2%**的平均成功率，在自监督方法中达到最优，甚至超过部分有监督方法。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.19391v2/x4.png" alt="其他任务结果"></p>
<blockquote>
<p><strong>表3</strong>：(a)指代表达式接地和(b)Franka Kitchen任务的结果。我们的方法在两项任务上均表现出色。</p>
</blockquote>
<ol start="4">
<li><strong>真实机器人实验</strong>（图3）：在10项任务中，LaVA-Man的感知成功率（模型预测正确）和物理成功率（实际执行成功）均保持高位，并展现出对未见物体颜色、未见物体乃至未见任务（如“打开抽屉”、“推 piles”）的泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.19391v2/images/real_exp/1.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图3</strong>：真实机器人实验的感知与物理成功率。展示了模型在包括未见任务在内的多种场景下的泛化性能。</p>
</blockquote>
<ol start="5">
<li><strong>消融实验</strong>（图5）：<ul>
<li><strong>移除融合模块</strong>：平均成功率从81%降至72%，证明了跨模态融合的重要性。</li>
<li><strong>移除OOPP数据</strong>：仅使用真实视频数据预训练，成功率降至74%，凸显了多样对象先验数据的关键作用。</li>
<li><strong>移除非对称掩码</strong>（即对称掩码）：成功率降至77%，验证了非对称掩码对于学习操作因果性的有效性。</li>
<li><strong>掩码比例</strong>：95%的掩码比例效果最佳，比例过低（信息泄露）或过高（100%，完全依赖推理）均会导致性能下降。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.19391v2/x5.png" alt="消融分析"></p>
<blockquote>
<p><strong>图5</strong>：消融分析。(a)展示了关键组件对性能的贡献；(b)显示了95%的掩码比例在Franka Kitchen任务上效果最好。</p>
</blockquote>
<p><strong>定性结果</strong>：<br><img src="https://arxiv.org/html/2508.19391v2/images/images_grid2/1_ori.png" alt="定性示例1"><br><img src="https://arxiv.org/html/2508.19391v2/images/images_grid2/1_pred.png" alt="定性示例2"><br><img src="https://arxiv.org/html/2508.19391v2/images/images_grid2/1_pick.png" alt="定性示例3"><br><img src="https://arxiv.org/html/2508.19391v2/images/images_grid2/1_place.png" alt="定性示例4"></p>
<blockquote>
<p><strong>图4及相关示例图</strong>：真实机器人操作的定性示例。预测的目标图像虽模糊（MAE类方法典型现象），但物体放置合理且与文本指令对齐，从而引导出准确的拾取和放置亲和力图。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种<strong>自监督的视觉-动作表征学习框架</strong>，其核心是通过语言条件下的目标图像预测任务，隐式学习操作中的因果变换。</li>
<li>引入了<strong>Omni-Object Pick-and-Place (OOPP) 数据集</strong>，包含180个类别、3200个独特实例的真实扫描物体，极大丰富了桌面操作任务的物体多样性，支持全面的泛化评估。</li>
<li>在模拟与真实环境的<strong>五个不同基准测试</strong>中验证了方法的有效性，取得了领先或可比的性能，并展现出良好的泛化能力。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>操作精度限制</strong>：由于使用ViT以图像块为单位处理，在处理小物体或精细结构（如汉诺塔）时精度受限。</li>
<li><strong>伪亲和力图的差距</strong>：通过亲和力图最大值点确定操作位姿的方法，可能因物体复杂几何形状导致感知正确但物理执行失败，存在感知与物理执行的差距。</li>
<li><strong>缺乏3D感知</strong>：仅依赖2D视觉线索，难以区分纹理相似但3D形状不同的物体，对铰接物体操作能力有限。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>可以探索结合<strong>更高分辨率特征</strong>（如DPT、FeatUp）的方法来提升对精细结构的操作精度。</li>
<li>引入考虑<strong>交互动力学</strong>的自监督学习，让模型学习常见的可操作区域先验，以缩小感知-执行差距。</li>
<li>将<strong>3D感知</strong>或几何感知融入自监督预训练，以提升对物体形状的理解和操作鲁棒性。</li>
<li>将训练扩展到更<strong>多样化的视频数据集</strong>，以学习更通用和鲁棒的表征，适用于更复杂的机器人任务。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对语言引导机器人操作中视觉-文本关联学习不精确的问题，提出LaVA-Man方法。该方法通过自监督前置任务：在输入图像和文本指令条件下重建掩码目标图像，学习视觉动作表示，无需机器人动作监督，并可通过少量演示微调。引入Omni-Object Pick-and-Place数据集（含180个对象类和3,200个实例）以提升泛化能力。实验在五个基准测试（包括模拟和真实机器人验证）中表明，该方法优于先前技术。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.19391" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>