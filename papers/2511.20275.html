<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HAFO: Humanoid Force-Adaptive Control for Intense External Force Interaction Environments - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>HAFO: Humanoid Force-Adaptive Control for Intense External Force Interaction Environments</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.20275" target="_blank" rel="noreferrer">2511.20275</a></span>
        <span>作者: Bin He Team</span>
        <span>日期: 2025-11-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于强化学习（RL）的控制器在人形机器人运动控制和轻量级物体操作方面取得了显著进展。然而，在存在剧烈外力交互的环境中实现鲁棒且精确的运动控制仍然是一个重大挑战。现有方法主要分为两类：“下半身RL-上半身逆运动学（IK）”范式以及“集成全身RL”范式。前者对上半身采用开环控制模式，难以实时感知和适应外部干扰；后者通过单一控制器控制全身关节，但由于上半身操作与下半身行走之间的任务目标关联性较弱，容易导致过拟合。此外，现有研究对外部交互力的建模往往忽略或过于简化，使得机器人在遭遇强碰撞或物理接触时表现出不稳定性，例如在高空作业绳索干扰或重载场景下难以维持平衡。</p>
<p>本文针对人形机器人在强外力扰动环境（如重载操作、高空作业）中实现精确、鲁棒全身控制的痛点，提出了一种基于显式动力学建模的力自适应控制新视角。核心思路是：提出一个名为HAFO的双智能体强化学习框架，通过解耦的上、下半身策略协同优化，并结合虚拟弹簧-阻尼系统显式建模外部张力扰动，使策略能够自主演化出力自适应控制模式。</p>
<h2 id="方法详解">方法详解</h2>
<p>HAFO的整体框架旨在通过双智能体协同训练，实现对外部强扰动鲁棒的全身控制。其输入包括机器人本体状态、下半身运动指令（线速度、角速度）和上半身参考关节轨迹指令；输出则是上、下半身所有关节的目标角度。</p>
<p><img src="https://arxiv.org/html/2511.20275v4/fig1.jpg" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：HAFO模型概览。(a) 策略训练：采用上、下半身解耦的双智能体策略。下半身策略以根部线速度和角速度为指令输入，上半身策略以参考关节轨迹为指令输入。在关键位置引入各种显式动力学扰动以增强系统鲁棒性和适应性。(b) 策略部署：开发了基于遥操作的机器人控制系统，采用高效逆运动学算法实时高精度计算机器人关节角度。</p>
</blockquote>
<p><strong>核心模块1：双智能体学习框架</strong><br>将人形机器人的总自由度分解为上、下半身，分别由上半身策略π_upper和下半身策略π_lower控制。两个智能体使用近端策略优化（PPO）算法进行训练，并共享相同的状态空间s_t = [s_t^prop, c^l, g^u]，其中包含本体感知信息（关节位置、速度、角速度等）、下半身指令和上半身指令。这使两个智能体都能考虑全身信息，实现协调控制。双策略被表述为双目标优化问题，分别最大化各自策略的累积奖励。</p>
<p><strong>核心模块2：鲁棒的下半身运动</strong><br>下半身策略π_lower的训练过程中引入了多种外部扰动以增强鲁棒性，包括对抗性的上半身扰动和高频随机推力扰动。同时，采用领域随机化（如质量、惯性、摩擦随机化）来提高模型的泛化能力。</p>
<p><strong>核心模块3：上半身运动生成</strong><br>上半身策略π_upper的演示数据来源于经过可行性过滤的AMASS人体运动数据集。与开环重放不同，本文训练一个能适应环境变化、避免自碰撞并改善全身协调性的RL策略。关键创新在于引入了<strong>约束残差动作空间</strong>：策略不直接学习绝对关节目标θ_t^target，而是输出一个相对于采样的上半身参考轨迹关节位置θ_t^ref的修正偏移量a_t^u，且该偏移量被约束在可控范围内（|a_t^u| ≤ Δ）。这有效防止了双智能体对抗性交互导致的训练崩溃。此外，采用渐进式模仿课程，通过课程增益系数α_i调制参考轨迹的运动幅度，从易到难地引导网络学习。</p>
<p><strong>核心模块4：虚拟弹簧-阻尼系统</strong><br>为显式建模外部张力（如绳索拉力）并将其纳入训练循环，引入了基于弹簧-阻尼系统的张力建模机制。其动力学模型为 m<em>x¨ = K_p</em>(x_des - x) + K_d*(x˙_des - x˙) + f_ext。通过渐进调整期望骨盆位置x_des^pelvis，可以调节张力以实现稳定控制。</p>
<p><img src="https://arxiv.org/html/2511.20275v4/fig2.jpg" alt="弹簧阻尼模型"></p>
<blockquote>
<p><strong>图2</strong>：弹簧-阻尼模型与性能分析。(a) 人形机器人上的弹簧-阻尼模型示意图。(b) 骨盆位置、弹簧力和地面反作用力随指定骨盆位移x_des^pelvis的变化对比（弹簧-阻尼模型 vs. 刚度模型）。对比测试表明，刚度控制会产生明显的振荡，证实了阻尼项对于增强稳定性和精度的关键作用。</p>
</blockquote>
<p><strong>创新点总结</strong>：1) <strong>解耦的双智能体架构</strong>实现了上、下半身任务的专注与协同；2) <strong>约束残差动作空间</strong>提升了双智能体训练的稳定性；3) <strong>虚拟弹簧-阻尼模型</strong>提供了精细的力交互建模与可控的扰动注入方式；4) <strong>渐进式课程学习</strong>使策略能逐步适应增大的外力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在MuJoCo仿真环境中进行，并通过零样本迁移从Isaac Gym进行交叉平台验证。Baseline方法包括：<strong>upper-OL-lower-RL</strong>（上半身开环，下半身RL）和<strong>upper-FIX-lower-RL</strong>（上半身固定，下半身RL）。同时进行了消融实验：移除双智能体架构（HAFO w.o. DA）和移除力自适应模块（HAFO w.o. Force）。</p>
<p><strong>1. 手部施加扰动下的力自适应控制</strong><br>在机器人末端执行器施加不同方向和大小的力（10N, 30N, 50N），同时执行随机速度命令和多样化的上肢运动序列。评估指标为上肢运动跟踪误差E_error^upper和速度跟踪误差E_error^vel。</p>
<p><img src="https://arxiv.org/html/2511.20275v4/fig3.jpg" alt="仿真结果"></p>
<blockquote>
<p><strong>图3</strong>：Unitree G1人形机器人的仿真结果。评估了模型在各种强交互环境中的性能，包括(a)悬吊状态稳定控制、(b)从悬吊到地面运动的自主平滑过渡、(c-d)双手各承受30N外力时摆臂/出拳、(e-f)承受250N侧向/纵向瞬时推力扰动后快速恢复平衡。</p>
</blockquote>
<p><strong>表1 策略跟踪性能评估</strong></p>
<table>
<thead>
<tr>
<th align="left">Methods</th>
<th align="left">E_tracking^upper (10N/30N/50N) [rad]</th>
<th align="left">E_tracking^root (10N/30N/50N) [m/s]</th>
</tr>
</thead>
<tbody><tr>
<td align="left">upper-OL-lower-RL</td>
<td align="left">0.36/0.74/1.23</td>
<td align="left">0.32/0.53/1.52</td>
</tr>
<tr>
<td align="left">upper-FIX-lower-RL</td>
<td align="left">0.55/-/-</td>
<td align="left">0.52/-/-</td>
</tr>
<tr>
<td align="left">HAFO w.o. DA</td>
<td align="left">0.34/0.66/1.32</td>
<td align="left">0.50/0.55/0.77</td>
</tr>
<tr>
<td align="left">HAFO w.o. Force</td>
<td align="left">0.42/0.60/1.36</td>
<td align="left">0.36/0.54/0.92</td>
</tr>
<tr>
<td align="left"><strong>HAFO(Ours)</strong></td>
<td align="left"><strong>0.22/0.41/0.46</strong></td>
<td align="left"><strong>0.33/0.48/0.53</strong></td>
</tr>
</tbody></table>
<p>结果表明，HAFO在速度跟踪和上肢角度跟随方面均实现了最优性能，且随着手部外力的增大，性能优势更为明显。消融实验显示，移除任一模块都会导致性能下降。</p>
<p><strong>2. 绳索悬吊下的力自适应控制</strong><br>评估策略在悬吊状态下的力适应性，重点关注上肢运动跟踪误差和动作平滑度Δa。</p>
<p><strong>表2 悬吊场景下不同方法性能对比</strong></p>
<table>
<thead>
<tr>
<th align="left">Methods</th>
<th align="left">Δa↓ [m/s]</th>
<th align="left">E_tracking^upper↓ [rad]</th>
</tr>
</thead>
<tbody><tr>
<td align="left">upper-OL-lower-RL</td>
<td align="left">2.49±2.38</td>
<td align="left">0.27±0.11</td>
</tr>
<tr>
<td align="left">upper-FIX-lower-RL</td>
<td align="left">5.66±4.50</td>
<td align="left">0.29±0.15</td>
</tr>
<tr>
<td align="left">HAFO w.o. DA</td>
<td align="left">1.60±0.49</td>
<td align="left">0.27±0.24</td>
</tr>
<tr>
<td align="left">HAFO-w/o-force</td>
<td align="left">4.52±1.77</td>
<td align="left">1.16±0.66</td>
</tr>
<tr>
<td align="left"><strong>HAFO (Ours)</strong></td>
<td align="left"><strong>0.38±0.16</strong></td>
<td align="left"><strong>0.20±0.04</strong></td>
</tr>
</tbody></table>
<p>HAFO在悬吊场景下表现出卓越的力适应性，动作平滑度和跟踪误差均显著优于基线及消融版本。</p>
<p><strong>3. 推力扰动下的力自适应控制</strong><br>系统评估策略对持续恒力和瞬时推力扰动的容忍度。</p>
<p><img src="https://arxiv.org/html/2511.20275v4/fig4.jpg" alt="推力扰动性能"></p>
<blockquote>
<p><strong>图4</strong>：外部推力扰动下的策略性能。(a) 多方向持续恒力扰动下的性能。(b) 多方向一秒瞬时力扰动下的性能。HAFO在大多数方向上比消融版本和基线策略表现出更大的扰动容忍度。</p>
</blockquote>
<p><strong>4. 真实世界实验</strong><br>将HAFO策略部署到Unitree G1实体机器人上，进行了负载移动操作等实验。</p>
<p><img src="https://arxiv.org/html/2511.20275v4/fig5.jpg" alt="真实实验"></p>
<blockquote>
<p><strong>图5</strong>：HAFO在实体机器人上完成多种力交互任务，验证了其技术性能和实际适用性。</p>
</blockquote>
<p><strong>表3 实体机器人负载移动操作性能</strong><br>（在单/双手负载下，HAFO保持了最低的速度跟踪误差和上肢关节误差，优于基线方法。）</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个<strong>双智能体强化学习框架</strong>，通过约束残差动作空间促进上、下半身在强扰动下的协同进化，实现了稳定高效的协调全身控制，并成功从Unitree G1扩展到全尺寸人形机器人H1-2。2) 开发了<strong>基于弹簧-阻尼的动态模型</strong>来显式建模外部张力，结合渐进课程学习，使策略能自主演化出地面运动与空中悬吊的力自适应控制模式，无需依赖显式状态机。3) <strong>实验验证</strong>HAFO使用单一策略在重载、推力扰动和绳索悬吊等多种条件下实现了力自适应控制，并且是首个在绳索悬吊条件下实现稳定运行和安全启动的运动控制策略。</p>
<p><strong>局限性</strong>：论文自身未明确陈述局限性。但从方法看，其性能依赖于仿真训练的质量和域随机化的范围，在极端未见过的大扰动或复杂接触几何下的泛化能力仍有待验证。</p>
<p><strong>后续启示</strong>：HAFO展示了通过解耦设计与显式物理建模结合RL，可以有效处理复杂的动态力交互。这为后续研究提供了新思路：1) 将更复杂的接触力学模型（如摩擦锥）整合进训练循环；2) 探索多模态感知（如视觉、力觉）与力自适应控制的结合，以应对完全未知的环境交互；3) 将该框架应用于更复杂的动态操作任务，如协作搬运或对抗性环境中的作业。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在强外力交互环境中运动控制不鲁棒、不精确的核心问题，提出HAFO框架。其关键技术是采用双智能体强化学习，通过耦合训练同时优化鲁棒的下肢运动与精确的上肢操作策略，并利用约束残差动作空间提升训练效率。核心创新在于引入弹簧阻尼系统显式建模外力扰动，使策略能通过操控虚拟弹簧实现精细的力适应。实验表明，该单一策略能实现全身控制，在负重、推力扰动及绳索悬吊等多种强交互场景中均表现出色且保持稳定。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.20275" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>