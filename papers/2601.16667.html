<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.16667" target="_blank" rel="noreferrer">2601.16667</a></span>
        <span>作者: Wei-Shi Zheng Team</span>
        <span>日期: 2026-01-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过融合预训练的视觉-语言编码器与机器人本体感觉反馈来预测动作，在机器人操作任务中展现出强大的泛化能力。然而，现有主流方法通常将本体感觉信号与VLM编码的视觉-语言特征直接融合，这导致了“状态主导偏差”。具体表现为，即使在出现明显的视觉执行失败线索时，策略仍会过度依赖内部状态（如预设的动作轨迹）而忽视视觉证据，从而产生“虚假完成”现象。例如，当目标物体在执行过程中掉落时，策略仍会继续执行预定的放置动作，而非根据视觉反馈去重新拾取物体。</p>
<p>本文针对VLA模型中因模态不平衡而导致的“虚假完成”这一具体痛点，提出了通过“视觉-本体感觉再平衡”来增强视觉基础与鲁棒性的新视角。其核心思路是：引入一个基于外部VLM的任务阶段观察器，实时提取任务中心的视觉线索作为环境先验，并通过特征层面的调制机制，自适应地增强策略对视觉证据的利用，从而纠正状态主导偏差。</p>
<h2 id="方法详解">方法详解</h2>
<p>ReViP框架旨在通过再平衡视觉与本体感觉的模态影响，将策略决策从虚假完成转向真实完成。整体框架包含两个核心阶段：任务阶段观察器和任务阶段增强器。</p>
<p><img src="https://arxiv.org/html/2601.16667v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ReViP框架总览。上方为<strong>任务阶段观察器</strong>，用于从观测和指令中提取任务中心视觉线索；下方为<strong>任务阶段增强器</strong>，通过特征调制实现再平衡。外部VLM（Qwen 2.5）提取的线索被注入VLA骨干网络，以自适应地重新平衡视觉流和本体感觉流。</p>
</blockquote>
<p><strong>整体流程</strong>：在每一步，策略接收多视角视觉观测 $I_t$、本体感觉状态 $S_t$ 和语言指令 $l$。首先，任务阶段观察器处理 $(I_t, l)$，生成紧凑的、任务中心的连续特征向量 $z_t$。然后，任务阶段增强器将 $z_t$ 转换为调制参数，通过特征级线性调制作用于VLA骨干网络的视觉-语言前缀令牌。最后，调制后的特征与 $S_t$ 融合，输入基于流匹配的动作解码器，预测未来的动作块。</p>
<p><strong>核心模块1：任务阶段观察器</strong>。该模块旨在解决视觉信息利用不足的问题。它使用一个冻结的大型视觉-语言模型，对当前观测和指令进行目标接地分析：识别机器人当前可见的物理状态、任务相关物体的空间位置和状态，并推断出与给定目标一致的当前阶段意图。例如，在物体掉落场景中，TSO会输出“奶油芝士未被机械臂抓取”等关键证据。这些离散的语言线索通过一个LLM嵌入提取策略（汇聚最后一层隐藏状态并线性投影）被转化为连续特征 $z_t \in \mathbb{R}^d$，以便注入VLA骨干网络。</p>
<p><strong>核心模块2：任务阶段增强器</strong>。该模块负责将TSO提取的线索 $z_t$ 注入策略，并在特征层面重新平衡视觉与本体感觉的影响。其核心是<strong>任务阶段特征级线性调制</strong>机制。首先，通过一个紧凑的瓶颈映射 $h(\cdot)$ 将 $z_t$ 转换为调制参数：$[\gamma_t, \beta_t] = h(z_t)$。然后，将这些参数应用于VLA骨干网络的视觉-语言前缀令牌 $P_t$，生成调制后的前缀 $\tilde{P}_t$：<br>$$\tilde{P}_t = \Big(P_t + \alpha\big(\gamma_t \odot P_t + \beta_t\big)\Big) \odot M_t$$<br>其中 $\alpha$ 是一个可学习的调制因子，$\odot$ 表示逐令牌的哈达玛积，$M_t$ 是有效性掩码。此操作在特征层面注入任务中心视觉线索，有效放大与视觉证据对齐的通道，同时抑制导致状态主导偏差的干扰信息。</p>
<p><strong>动作预测与训练</strong>：调制后的前缀 $\tilde{P}<em>t$ 与本体感觉 $S_t$ 融合后，输入动作头。动作头通过流匹配进行训练，其目标是回归一个条件速度场。给定真实动作块 $A_t$ 和随机噪声 $\varepsilon$，沿直线路径构建噪声插值 $v_\tau = (1-\tau)A_t + \tau \varepsilon$，目标速度 $u_\tau = \varepsilon - A_t$。训练损失为预测速度与目标速度之间的L2范数：$\mathcal{L}</em>{\mathrm{FM}} = | v_\theta(v_\tau, \tau | \tilde{F}_t) - u_\tau |_2^2$。</p>
<p><strong>创新点</strong>：与现有方法相比，ReViP的创新主要体现在两点：1) <strong>引入外部VLM作为实时任务阶段观察器</strong>，提供细粒度的、任务相关的环境语义反馈，而非仅用于任务规划或成功判定。2) <strong>提出TS-FiLM调制机制</strong>，将上述反馈以自适应、特征级的方式注入策略决策过程，实现了视觉与本体感觉信号的动态再平衡，直接针对虚假完成的根源。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了多个基准进行验证：1) <strong>虚假完成基准套件</strong>：基于LIBERO平台构建，包含物体掉落、干扰物交换、场景重布局三类共8个任务的受控扰动设置。2) <strong>通用仿真基准</strong>：标准LIBERO基准（含Spatial, Object, Goal, 10四个任务套件）和双臂RoboTwin 2.0基准。3) <strong>真实世界实验</strong>。</p>
<p>对比的基线方法包括：OpenVLA、OpenVLA-OFT、SpatialVLA、$\pi_0$、$\pi_0$-Fast、CoT-VLA、TriVLA、UniVLA、DP3、RDT等代表性VLA模型。</p>
<p><img src="https://arxiv.org/html/2601.16667v1/Figures/FC.png" alt="基准套件说明"></p>
<blockquote>
<p><strong>图3</strong>：虚假完成基准套件示意图。包含三类互补的扰动源：物体掉落评估策略检测执行中失败的能力；干扰物交换检验在视觉相似物体下的实例级接地能力；重布局条件测试当目标和目标区域出现在新配置时的空间推理能力。</p>
</blockquote>
<p><strong>关键实验结果（虚假完成基准）</strong>：如表1所示，ReViP在虚假完成基准上取得了62%的平均成功率，排名第一，显著优于基线。例如，在物体掉落任务上平均成功率达65.2%；在干扰物交换任务上比$\pi_0$和$\pi_0$-Fast分别高出15%和40%；在重布局任务上达到88%的成功率。这表明ReViP有效降低了虚假完成率。</p>
<p><img src="https://arxiv.org/html/2601.16667v1/Figures/vs2.png" alt="定性对比"></p>
<blockquote>
<p><strong>图4</strong>：在物体掉落设置下的定性比较（上：仿真，下：真实世界）。ReViP能检测到物体掉落并成功重新拾取目标，实现真实完成；而$\pi_0$则无视视觉失败，继续执行状态主导的轨迹，导致虚假完成。</p>
</blockquote>
<p><strong>关键实验结果（通用仿真基准）</strong>：在标准LIBERO基准上（表2），ReViP取得了96.7%的平均成功率，在所有套件上均排名第一。特别是在具有挑战性的LIBERO-10套件上，将$\pi_0$的85.2%提升至92.2%。在双臂RoboTwin 2.0基准的困难模式下（表3），ReViP以14%的平均成功率优于其他方法。</p>
<p><strong>关键实验结果（真实世界）</strong>：在包含物体掉落和抽屉操作的真实任务中（表4），ReViP取得了60%的平均成功率，远高于$\pi_0$的34%和$\pi_0$-Fast的18%。</p>
<p><strong>消融实验</strong>：论文通过消融实验验证了各组件贡献。移除任务阶段观察器或任务阶段增强器（TS-FiLM）都会导致性能显著下降，尤其是在物体掉落和干扰物交换任务上。这证明了外部VLM提供的任务阶段线索和特征调制机制对于实现视觉-本体感觉再平衡都是不可或缺的。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>识别并形式化了VLA模型中的“虚假完成”失败模式</strong>，将其根源归结为视觉与本体感觉的模态不平衡导致的状态主导偏差。2) <strong>提出了ReViP框架</strong>，通过任务阶段观察器提取实时视觉线索，并利用任务阶段增强器进行特征级调制，实现了两种模态的自适应再平衡。3) <strong>构建了首个针对虚假完成现象的评测基准套件</strong>，为社区提供了系统的评估工具。</p>
<p>论文提到的局限性主要在于<strong>计算开销</strong>：使用大型外部VLM（Qwen2.5-VL 72B）作为观察器会引入额外的推理成本。作者指出，未来可探索更高效的VLM或知识蒸馏来缓解此问题。</p>
<p>本文对后续研究的启示在于：1) 在构建具身智能系统时，<strong>需要审慎处理多模态信号的融合方式</strong>，避免某种模态过度主导决策。2) <strong>利用外部大模型提供丰富、实时的环境语义反馈</strong>，是增强策略对动态环境适应性和鲁棒性的有效途径。3) <strong>构建针对特定失败模式的诊断性基准</strong>，能更精准地驱动模型在关键能力上的进步。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>该论文针对视觉-语言-动作模型在机器人操作中存在的“错误完成”问题，即任务未成功却提前终止，其根源在于模型过度依赖本体感觉而忽视视觉证据的模态不平衡。为此，提出ReViP框架，其核心技术是通过外部VLM构建任务阶段观察器提取实时视觉线索，并驱动视觉-本体感觉特征线性调制，以动态平衡多模态信息。实验表明，该方法在提出的错误完成基准及LIBERO等多个测试集上，有效降低了错误完成率并提升了任务成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.16667" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>