<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.17381" target="_blank" rel="noreferrer">2509.17381</a></span>
        <span>作者: Hamidreza Kasaei Team</span>
        <span>日期: 2025-09-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人机械臂在非结构化和杂乱环境中生成无碰撞轨迹仍然是一个重大挑战。现有的运动规划方法，如图搜索（A*）和采样方法（RRT、PRM），往往存在计算复杂、内存消耗大、难以保证最优解或实时性等局限性。同时，在任务空间（笛卡尔空间）规划虽简化了问题，但未考虑机械臂连杆在关节空间的避障。尽管强化学习（RL）在关节级控制中展现出潜力，但现有基于策略梯度的方法（如PPO）在应用于机械臂高精度到达任务时，常面临训练不稳定、动作方差大、在动态杂乱环境中适应性不足等问题。本文针对如何为机械臂设计一个快速、高效的轨迹规划与跟踪框架，以及如何利用学习方法实现从任务空间到关节空间的有效映射并改进RL算法以适应高精度到达任务这两个核心痛点，提出了一个结合任务空间视觉规划与关节空间增强RL控制器的系统。其核心思路是：在任务空间，利用快速分割模型（FSA）增强感知并进行基于B样条优化的运动学动态路径搜索，生成安全高效的路径点；在关节空间，通过集成动作集成（AE）和策略反馈（PF）来增强PPO算法，使其能够精确、稳定地跟踪这些路径点并避开障碍。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架是一个分阶段的快速轨迹规划系统，它将复杂的运动规划问题分解为两个关键子问题：任务空间的快速轨迹规划和关节空间的强化学习避障控制。</p>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/1.png" alt="系统概述"></p>
<blockquote>
<p><strong>图1</strong>：系统整体框架。Kinect传感器捕获视觉数据，通过FSA增强的3D地图生成模块，结合B样条优化和运动学动态路径搜索，在任务空间生成路径点。这些路径点与机械臂状态共同输入到基于RL的控制器中，控制器调整机械臂6个关节的位置以实现轨迹跟踪和避障。</p>
</blockquote>
<p><strong>1. 感知感知的任务空间轨迹规划</strong><br>该部分旨在为机械臂末端执行器在任务空间生成安全、动态可行的轨迹。首先，利用Kinect等传感器获取场景图像并转换为高度图，然后使用快速分割模型（FSA）检测物体掩码。通过过滤z轴位置高于桌面的掩码，去除背景和阴影噪声，并利用这些掩码和桌面信息重建被遮挡部分的点云，从而获得完整的场景3D点云。</p>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/2.png" alt="感知感知轨迹规划"></p>
<blockquote>
<p><strong>图2</strong>：感知感知轨迹规划示意图。FSA从高度图中获取物体掩码，与点云融合后生成用于导航和规划决策的详细体素网格地图。</p>
</blockquote>
<p>接着，基于完整的点云进行实时轨迹规划。该方法借鉴了无人机路径规划的先进方法，结合了<strong>运动学动态路径搜索</strong>和<strong>B样条轨迹优化</strong>。规划出的轨迹（一系列路径点）会输入给后续的RL控制器。系统具备重新规划能力，触发条件包括：当前路径与新检测到的障碍物相交，或达到固定的时间间隔，以适应动态未知环境。</p>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/3.png" alt="轨迹规划过程"></p>
<blockquote>
<p><strong>图3</strong>：动态环境中的轨迹规划过程示例。黄色轨迹为初始规划路径，红球和蓝球分别为起点和目标点。当遇到未预见的绿色圆柱体障碍物时，系统触发重新规划，生成新的红色绕障路径（含路径点）作为RL控制器的输入。</p>
</blockquote>
<p><strong>2. 基于RL的关节空间避障控制器</strong><br>该部分目标是训练一个神经网络控制器，无需计算逆运动学或动力学模型，即可直接将任务空间目标映射为关节空间动作，实现精确的轨迹跟踪和避障。前向运动学（FK）作为桥梁，用于计算末端执行器位姿以构成状态向量、计算奖励和最终任务评估。</p>
<ul>
<li><strong>状态与动作表示</strong>：状态向量为25维，包括6个关节位置、末端执行器位姿、目标点位姿、位姿误差以及障碍物到各连杆（除基座连杆）的最小距离。动作向量为6维，表示期望的关节速度，通过积分得到期望关节角后，输入PD控制器生成控制扭矩。</li>
<li><strong>学习策略与PPO增强</strong>：算法基础是PPO，其目标函数通过裁剪替代目标来限制策略更新，以平衡稳定性和效率。本文针对PPO在复杂机械臂任务中训练耗时、结果欠佳的问题，提出了两项关键增强：<ul>
<li><strong>动作集成（Action Ensembles, AE）</strong>：核心思想是通过从当前策略分布中采样多个动作并取平均，来平滑动作输出、降低策略不确定性（方差）。为了平衡早期探索和后期收敛，论文探索了五种随时间调整采样数量的分布策略：线性（AEL）、泊松（AEP）、贝塔（AEB）、指数（AEE）和威布尔（AEW）分布。经过实验，最终选择了<strong>AEW（威布尔分布）</strong>作为PPO的增强方法。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/4.png" alt="策略近似过程"></p>
<blockquote>
<p><strong>图4</strong>：在一维示例中，策略向最优策略逼近的三个阶段（从左到右）：早期探索、中期探索和后期收敛。展示了最优策略、当前策略和经调整策略之间的关系。</p>
</blockquote>
<pre><code>*   **策略反馈（Policy Feedback, PF）**：为了稳定学习过程，让价值函数（Critic）更快地适应策略（Actor）的变化，该方法将策略概率直接引入折扣因子的计算中，形成一个自适应的裁剪折扣因子γ(s,a;η)，并将其用于Critic网络的更新。这使得Critic能更紧密地跟随Actor的策略演变。
</code></pre>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/5.png" alt="增强PPO框架"></p>
<blockquote>
<p><strong>图5</strong>：增强PPO算法框架。红色框图展示了动作集成（AE）模块，红色虚线展示了策略反馈（PF）如何将策略信息融入Critic网络的更新过程。</p>
</blockquote>
<p>Actor和Critic网络均为3层全连接网络（每层256个神经元，前两层使用tanh激活函数）。最终，集成了AEW和PF的增强型PPO算法构成了关节空间控制器的核心。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在PyBullet模拟器中以及真实的UR5机械臂上进行评估。使用了包含静态和动态障碍物的多种复杂场景作为测试环境。</p>
<p><strong>对比基线与方法</strong>：对比方法包括传统规划器（RRT、RRT<em>、Informed RRT</em>）以及基于学习的基线方法（SAC、TD3、原始PPO）。本文方法记为 **FTP (Ours)**。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能对比</strong>：在包含5个静态障碍物和1个动态障碍物的场景中，评估了从不同起点到目标点的规划任务。性能指标包括<strong>成功率</strong>、<strong>平均路径长度</strong>和<strong>平均规划时间</strong>。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/6.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图6</strong>：各方法在复杂场景下的成功率对比。FTP (Ours)取得了最高的平均成功率（97.5%），显著优于原始PPO（85%）和其他基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/7a.png" alt="路径长度对比"></p>
<blockquote>
<p><strong>图7a</strong>：各方法成功轨迹的平均路径长度对比。FTP (Ours)规划的路径长度与RRT*相当，优于原始PPO。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/7b.png" alt="规划时间对比"></p>
<blockquote>
<p><strong>图7b</strong>：各方法的平均规划时间对比。FTP (Ours)的规划时间远低于RRT*等采样方法，与原始PPO处于同一量级，但成功率更高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/7c.png" alt="模拟到真实转移"></p>
<blockquote>
<p><strong>图7c</strong>：Sim-to-Real转移性能。将在模拟器中训练的FTP控制器直接部署到真实UR5机械臂上，在真实杂乱桌面场景中仍能保持92.5%的高成功率。</p>
</blockquote>
<ol start="2">
<li><strong>消融实验</strong>：<ul>
<li><strong>动作集成（AE）策略对比</strong>：比较了五种不同的AE分布策略（AEL, AEP, AEB, AEE, AEW）与原始PPO的性能。AEW在所有测试中均表现最佳。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/12a.png" alt="AE策略成功率"></p>
<blockquote>
<p><strong>图14</strong>：不同AE分布策略在训练过程中的成功率收敛曲线。AEW（威布尔分布）最终收敛的成功率最高且最稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/12aa.png" alt="AE策略奖励"></p>
<blockquote>
<p><strong>图15</strong>：不同AE分布策略在训练过程中的平均奖励曲线。AEW获得的奖励最高。</p>
</blockquote>
<pre><code>*   **组件贡献分析**：系统性地移除了框架中的关键组件（RL控制器、PF、AE、B样条优化），以评估每个部分的贡献。
</code></pre>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/8.png" alt="消融实验成功率"></p>
<blockquote>
<p><strong>图16</strong>：消融实验各配置的成功率对比。完整模型（FTP）性能最优，移除任何关键组件（尤其是RL控制器或AE）都会导致性能显著下降。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/12b1.png" alt="消融实验路径长度"></p>
<blockquote>
<p><strong>图17</strong>：消融实验各配置的平均路径长度对比。完整模型能生成较短的路径。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.17381v1/Figs/12b2.png" alt="消融实验规划时间"></p>
<blockquote>
<p><strong>图18</strong>：消融实验各配置的平均规划时间对比。完整模型在保持高性能的同时，规划时间具有竞争力。</p>
</blockquote>
<p><strong>总结</strong>：实验表明，本文提出的FTP框架在成功率（97.5%）上显著优于所有基线，路径长度接近最优，且规划时间高效。消融实验证实了动作集成（AEW）和策略反馈（PF）对提升PPO控制器性能的关键作用，以及任务空间B样条优化器对生成平滑、可行路径的重要性。成功的Sim-to-Real转移（92.5%成功率）验证了方法的鲁棒性和实用性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>分治的快速轨迹规划框架</strong>，将任务空间的视觉感知、运动学动态搜索与B样条优化相结合，生成高质量路径点，再通过关节空间的增强RL控制器进行精确跟踪与避障。</li>
<li>针对机械臂高精度到达与避障任务，<strong>增强了PPO算法</strong>，提出了基于威布尔分布的动作集成（AEW）和策略反馈（PF）机制，有效提高了策略的稳定性、收敛速度和最终性能。</li>
<li>通过<strong>全面的仿真与实物实验</strong>验证了框架的有效性，并展示了良好的从模拟到现实的转移能力，代码已开源。</li>
</ol>
<p><strong>局限性</strong>：论文提到，虽然框架能处理动态障碍物，但对高速动态障碍物的反应可能仍需进一步优化。此外，感知模块（FSA）的性能和点云重建的准确性直接影响规划质量，在极端光照或高度杂乱场景下可能面临挑战。</p>
<p><strong>启示</strong>：本文的工作展示了将前沿视觉感知模型（如FSA）与经过针对性改进的强化学习算法相结合，解决复杂机器人运动规划问题的潜力。其分阶段（任务空间规划+关节空间控制）的设计思路，以及通过动作集成和策略反馈等“插件”增强现有RL算法的做法，为后续研究提供了有价值的参考。未来工作可探索更高效的感知-规划-控制闭环，以及针对更复杂动态环境和多机械臂协作的扩展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人在非结构化和杂乱环境中快速生成无碰撞轨迹的挑战，提出一种融合任务空间视觉规划与关节空间强化学习控制的系统。关键技术包括：基于FSA模型和B样条优化的视觉轨迹规划器，以及集成动作集成和策略反馈的增强PPO算法，以提高避障和目标到达的精度与稳定性。实验表明，PPO增强有效提升了模型鲁棒性和规划效率，支持模拟到现实转移，使机器人能在障碍环境中实时执行避障与轨迹规划。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.17381" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>