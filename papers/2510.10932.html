<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Cryptography and Security (cs.CR)</span>
      <h1>TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.10932" target="_blank" rel="noreferrer">2510.10932</a></span>
        <span>作者: Xu, Zonghuan, Zheng, Xiang, Ma, Xingjun, Jiang, Yu-Gang</span>
        <span>日期: 2025/10/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>随着视觉-语言-行动模型在具身AI系统中的广泛应用，其安全性问题日益凸显。其中，后门攻击因其只需少量数据投毒即可在特定触发条件下诱导恶意行为，构成了严重威胁。现有研究（如BadVLA）仅探索了无目标的后门攻击，即触发后仅干扰模型正常行为，而未研究更具实际威胁的<strong>目标攻击</strong>——即精确控制模型在触发时执行攻击者指定的危险动作（如突然松开机械爪）。本文针对VLA模型在更现实的<strong>黑盒微调</strong>场景下，研究如何植入目标后门，并提出了TabVLA框架。核心思路是：在模型微调阶段，通过优化数据投毒策略，将目标行为与特定触发模式（视觉或语言）关联，从而实现在推理时通过触发信号精确操控模型行为。</p>
<h2 id="方法详解">方法详解</h2>
<p>TabVLA是一个针对黑盒VLA模型进行目标后门攻击的通用框架。其整体流程分为两个阶段：1) <strong>投毒数据集构建</strong>：在干净的演示数据中，选择部分数据段，注入触发模式（视觉、语言或二者联合）并将其动作标签修改为目标行为；2) <strong>模型微调</strong>：使用构建的投毒数据集对预训练的VLA模型进行微调，从而将后门植入模型。推理时，攻击者通过两种威胁模型激活后门：<strong>输入流编辑</strong>（直接修改输入流）或<strong>场景内触发</strong>（在环境中放置物理触发物）。</p>
<p>框架的核心模块包括：</p>
<ol>
<li><strong>多模态触发构造</strong>：支持视觉触发（如图像上叠加特定图案）、语言触发（在指令后附加特定词句）以及联合触发。</li>
<li><strong>一致性重标注策略</strong>：这是关键创新点。由于VLA模型通常基于固定长度的K步片段进行训练，若仅在片段内部分步骤注入后门标签，会导致同一片段内同时存在干净和目标两种矛盾的监督信号，破坏训练稳定性。TabVLA采用<strong>一致性重标注</strong>：一旦在一个情节中引入触发，则将该触发点之后一个连续时间段内的所有步骤的动作标签都重标注为目标行为，确保学习信号的一致性。</li>
<li><strong>投毒模式</strong>：提供“修改-干净”和“添加-新”两种模式。前者复用现有干净轨迹进行修改，后者合成全新的投毒轨迹。</li>
</ol>
<p><img src="https://..." alt="投毒与微调算法"></p>
<blockquote>
<p><strong>图1</strong>：TabVLA的投毒与微调流程（对应论文Algorithm 1）。算法展示了使用联合（文本+图像）触发进行投毒的具体步骤：选择一部分情节，在机械爪闭合的步骤处，向语言指令追加文本触发，向图像观察叠加视觉触发，并将对应的抓取动作从闭合（+1）翻转为释放（-1），最后使用该投毒数据集微调模型。</p>
</blockquote>
<p>与现有方法相比，TabVLA的创新点在于：首次系统性地研究了VLA模型的目标后门攻击；提出了适用于序列决策的一致性重标注策略以稳定训练；明确了两种贴近实际部署的推理时威胁模型。</p>
<h2 id="实验与结果">实验与结果</h2>
<ul>
<li><strong>实验平台与数据</strong>：在LIBERO-Spatial基准上，使用OpenVLA-7B模型进行实验。微调采用OFT方法，使用LoRA适配器。</li>
<li><strong>基线对比</strong>：主要对比了不同触发模态（仅视觉、仅文本、视觉+文本）在不同投毒率下的表现。</li>
<li><strong>关键结果</strong>：<ol>
<li><strong>视觉模态主导攻击</strong>：仅视觉触发在极低的投毒率下（如0.31%）就能实现接近完美的攻击成功率（ASR≈98.67%），且不影响干净任务性能（ST≈99.17%）。而仅文本触发在低投毒率下效果急剧下降且极不稳定（ASR从100%降至31.17%±53.12%）。</li>
<li><strong>攻击具有高精确性</strong>：成功的攻击要求机械爪在触发出现后0.05秒内释放。实验结果显示释放延迟极低（RL≈7-9毫秒），且物体释放高度（FFD≈15.3-15.9 cm）稳定，表明攻击能精确控制行为的时间和空间。</li>
<li><strong>对触发设计不敏感</strong>：消融实验表明，攻击成功率对文本触发（如使用稀有词“[sudo]”、副词或完整句子）和视觉触发（形状、大小、不透明度）的具体设计变化具有鲁棒性。</li>
<li><strong>对测试时不匹配具有鲁棒性但位置敏感</strong>：在推理时改变触发的外观（如不透明度）、存在部分遮挡或改变语言表述，对攻击成功率影响很小。然而，<strong>视觉触发的位置</strong>在微调和推理阶段必须保持一致，位置不匹配会导致攻击成功率大幅下降。</li>
</ol>
</li>
</ul>
<p><img src="https://..." alt="主要结果表"></p>
<blockquote>
<p><strong>表2</strong>：不同触发模态和投毒率下的主要实验结果。该表清晰展示了视觉触发（Vision only）在极低投毒率下的高效性和稳定性，以及文本触发（Text only）在低投毒率下的低效与高方差。联合触发（Text+Vision）的表现与视觉触发主导的模式一致。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个针对黑盒VLA模型进行目标后门攻击的通用框架TabVLA，涵盖了输入流编辑和场景内触发两种实际威胁模型。</li>
<li>通过系统的实验揭示了VLA模型对<strong>视觉触发后门的高度脆弱性</strong>，即使投毒率极低也能实现高精度目标控制，且不影响正常功能。</li>
<li>初步探索了一种基于触发反演的视觉检测防御方法，通过从输入流中重建潜在的视觉触发模式来识别后门样本。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>攻击的成功依赖于视觉触发在微调和推理阶段的<strong>空间位置一致性</strong>，位置不匹配会削弱攻击。</li>
<li>实验在特定的任务（抓放）和模型上进行，结论的普适性有待进一步验证。</li>
<li>防御方法仅为初步探索，其有效性有待全面评估。</li>
</ol>
<p><strong>对后续研究的启示</strong>：<br>本文凸显了VLA模型，尤其是其视觉通道，在安全上的严峻挑战。后续研究应致力于开发更强大的防御机制，例如针对触发空间一致性的检测方法，或设计对后门攻击具有内在鲁棒性的模型架构与训练算法。同时，需要建立更全面的安全评估基准，以涵盖多样化的攻击与防御场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究视觉-语言-动作（VLA）模型面临的针对性后门攻击威胁，提出TabVLA攻击框架。核心问题是：现有后门攻击仅关注无目标干扰，而更具实际危害的针对性操纵尚未被探索。TabVLA通过黑盒微调实现攻击，探索了两种推理时威胁模型（输入流编辑、场景内触发），并将投毒数据生成建模为优化问题。实验基于OpenVLA-7B和LIBERO基准，发现视觉通道是主要攻击面：仅需少量投毒即可成功实施针对性攻击，且对触发器设计变化具有鲁棒性；仅当微调与推理阶段触发器位置不匹配时，攻击效果才会下降。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.10932" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>