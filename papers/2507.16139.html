<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Equivariant Goal Conditioned Contrastive Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Equivariant Goal Conditioned Contrastive Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.16139" target="_blank" rel="noreferrer">2507.16139</a></span>
        <span>作者: Robert Platt Team</span>
        <span>日期: 2025-07-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目标条件强化学习（GCRL）为智能体学习达成来自目标分布的状态提供了自然框架，且无需外部提供的奖励。对比强化学习（CRL）是GCRL的一种方法，它通过对比学习目标条件Q函数，将可达未来状态与状态-动作对的表示拉近，同时推远不可达的负样本，从而实现无需手动设计奖励的策略学习。尽管CRL方法相比之前的目标条件基线表现出性能提升，但它们仍然样本效率低下，难以学习可靠策略。例如，在实验中，即使经过一百万次环境交互，CRL也无法合理解决FetchPush和FetchPickAndPlace等操作任务。因此，样本效率是充分释放大规模自监督强化学习潜力的主要瓶颈。</p>
<p>本文针对CRL样本效率低下的局限性，提出将几何归纳偏置——特别是旋转等变性——整合到对比强化学习中。机器人操作任务通常具有固有的对称性（如绕垂直轴的平面旋转对称），利用这种对称性可以约束学习到的表示，从而提升泛化能力和样本效率。本文的核心思路是：首先形式化定义目标条件群不变马尔可夫决策过程（GCGI-MDP）以刻画具有旋转对称性的机器人操作任务；然后在此基础上，为对比RL引入一种新颖的旋转不变评论家表示，并与一个旋转等变行动者配对，构建等变对比RL（ECRL）算法。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法旨在学习具有平面旋转对称性的机器人操作任务的最优目标条件策略。这些任务被建模为目标条件SO(2)-不变MDP。为利用这些对称性，作者在对比RL框架内引入了旋转不变评论家和旋转等变行动者，主要关注SO(2)的离散子群——循环群C_N。</p>
<p>首先，论文形式化定义了<strong>目标条件群不变MDP（GCGI-MDP）</strong>，并证明了其性质（命题1）：在一个GCGI-MDP中，最优目标条件Q函数是群不变的（即Q*(gs, ga, g<strong>g</strong>) = Q*(s, a, <strong>g</strong>)），而最优目标条件策略是群等变的（即π*(gs, g<strong>g</strong>) = gπ*(s, <strong>g</strong>)）。这为设计等变网络架构提供了理论依据。</p>
<p>整体框架建立在对比RL之上，其关键创新在于网络架构的等变性设计。对比RL学习编码器φ(s, a)和ψ(<strong>g</strong>)，以及策略π(a|s, <strong>g</strong>)。相似性度量f(s, a, <strong>g</strong>)（例如内积或负L2距离）被训练来近似对应于在下一时间步达到目标的概率奖励的Q函数。演员通过最大化f(s, a, <strong>g</strong>)加上策略熵正则项来训练。</p>
<p>本文方法的核心模块是<strong>旋转不变评论家</strong>和<strong>旋转等变行动者</strong>：</p>
<ol>
<li><strong>旋转不变评论家</strong>：为了实现命题1中所述的最优Q函数的不变性，作者设计φ和ψ，使其输出C_N群的<strong>正则表示</strong>（regular representation）。一个d维的正则表示向量，其每个索引i对应于群元素Rot_{2πi/d} ∈ C_d。群作用（旋转）表现为对该向量的循环移位。关键之处在于，如果φ(s, a)和ψ(<strong>g</strong>)都输出相同维度的正则表示，那么它们的内积f(s, a, <strong>g</strong>) = φ(s, a)^⊺ ψ(<strong>g</strong>)在同时旋转状态-动作对(s, a)和目标<strong>g</strong>时保持不变。因为旋转会导致两个表示向量发生完全同步的循环移位，而对应元素相乘再求和的结果不变。</li>
<li><strong>旋转等变行动者</strong>：为了实现命题1中所述的最优策略的等变性，策略网络π(a|s, <strong>g</strong>)被构造成等变网络。具体而言，对于具有平面位置(x, y)的动作参数，网络输出在标准表示（standard representation）下的二维向量，使得当输入(s, <strong>g</strong>)被旋转时，输出的动作参数也发生相同的旋转。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.16139v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：旋转不变评论家示意图。以C_4群和两个通道为例。每个网络输出两个4维的C_4群正则表示堆叠。同时旋转状态-动作对(s, a)和目标<strong>g</strong>会导致其嵌入中每个正则表示向量发生对齐的循环置换，但由于对应元素相乘求和，内积保持不变。</p>
</blockquote>
<p>与现有对比RL方法相比，本文的创新点具体体现在：<strong>首次将等变性约束整合到无奖励监督的目标条件RL中</strong>。这包括：(1) 提出了统一的GCGI-MDP框架来刻画此类问题；(2) 设计了基于正则表示的旋转不变评论家，确保相似性度量（即Q值）的旋转不变性；(3) 使用了旋转等变的行动者网络来产生对称的动作。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：论文在一系列模拟机器人操作任务上评估方法，包括基于状态和基于图像的环境。基于状态的任务使用MuJoCo模拟器，包括FetchReach、FetchPush、FetchPickAndPlace以及自定义的BBlockReach、BBlockPush、BBlockPick。基于图像的任务使用Robosuite模拟器，包括SawyerPush和SawyerBin（一种杂乱场景中的抓取任务）。实验平台和基准实现基于开源代码。</p>
<p><strong>对比基线</strong>：主要的对比基线包括：</p>
<ul>
<li>**Contrastive RL (CRL)**：标准的对比RL方法。</li>
<li><strong>Equivariant SAC</strong>：将等变网络应用于使用手动设计奖励的SAC算法。</li>
<li><strong>GCSL</strong>：目标条件模仿学习。</li>
<li><strong>HER</strong>： hindsight experience replay。</li>
<li><strong>AWAC</strong>：一种离线RL算法。<br>在离线RL实验中，还与<strong>IQL</strong>和<strong>BC</strong>进行了对比。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在在线学习设置中，ECRL在所有任务上都显著且一致地优于基线方法。例如，在FetchPush任务中，ECRL在50万步环境交互后达到约80%的成功率，而标准CRL仅约20%，等变SAC约65%。在更复杂的FetchPickAndPlace任务中，ECRL的成功率最终超过60%，而CRL未能有效学习（低于10%）。在基于图像的SawyerPush任务中，ECRL的成功率曲线始终高于CRL和等变SAC。</p>
<p><img src="https://arxiv.org/html/2507.16139v1/x2.png" alt="训练曲线1"></p>
<blockquote>
<p><strong>图10</strong>：FetchReach任务训练曲线。ECRL（红色）相比CRL（蓝色）和等变SAC（绿色）收敛更快，最终性能更高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.16139v1/x3.png" alt="训练曲线2"></p>
<blockquote>
<p><strong>图11</strong>：FetchPush任务训练曲线。ECRL（红色）性能显著优于CRL（蓝色）和等变SAC（绿色），展示了其样本效率优势。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.16139v1/x4.png" alt="训练曲线3"></p>
<blockquote>
<p><strong>图12</strong>：FetchPickAndPlace任务训练曲线。ECRL（红色）是唯一能有效学习该任务的方法，CRL（蓝色）失败，等变SAC（绿色）性能一般。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.16139v1/x5.png" alt="训练曲线4"></p>
<blockquote>
<p><strong>图13</strong>：BBlockPush任务训练曲线。ECRL（红色）同样展现出最快的收敛速度和最高的成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.16139v1/x6.png" alt="图像任务曲线"></p>
<blockquote>
<p><strong>图14</strong>：基于图像的SawyerPush任务训练曲线。ECRL（红色）性能持续优于CRL（蓝色）和等变SAC（绿色）。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文进行了消融研究以验证各个组件的贡献，比较了：（1）标准CRL（无等变性）；（2）仅使用不变评论家（Inv-C）；（3）仅使用等变行动者（Equi-A）；（4）完整的ECRL（同时使用不变评论家和等变行动者）。实验结果表明，同时使用不变评论家和等变行动者（ECRL）的效果最好。仅使用不变评论家也能带来显著提升，但不如完整方法；仅使用等变行动者也有一定改善。这证明了不变评论家在提升样本效率中的核心作用，而等变行动者提供了额外的增益。</p>
<p><img src="https://arxiv.org/html/2507.16139v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图15</strong>：在FetchPush任务上的消融实验。完整ECRL（紫色）性能最佳，仅用不变评论家（黄色）次之，仅用等变行动者（绿色）有一定提升，标准CRL（蓝色）最差。</p>
</blockquote>
<p><strong>离线RL实验</strong>：<br>论文还将ECRL扩展到离线RL设置，在BBlock任务上进行了测试。ECRL在离线设置下也优于基线方法AWAC、IQL和纯行为克隆（BC）。</p>
<p><img src="https://arxiv.org/html/2507.16139v1/x8.png" alt="离线RL结果"></p>
<blockquote>
<p><strong>图16</strong>：离线RL实验结果。ECRL在BBlockReach、BBlockPush和BBlockPick任务上的成功率均高于AWAC、IQL和BC。</p>
</blockquote>
<p><strong>泛化测试与可视化</strong>：<br>论文测试了训练后的策略对未见过的目标方向的泛化能力，ECRL表现出优秀的零样本泛化性能。此外，通过t-SNE可视化潜在空间，表明ECRL学习到的表示比CRL的表示具有更清晰的结构。</p>
<p><img src="https://arxiv.org/html/2507.16139v1/x9.png" alt="泛化测试"></p>
<blockquote>
<p><strong>图17</strong>：零样本泛化到新目标方向。ECRL（红色）在训练时未见过的目标角度上，成功率下降远小于CRL（蓝色）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.16139v1/x10.png" alt="潜在空间可视化"></p>
<blockquote>
<p><strong>图18</strong>：潜在空间t-SNE可视化。ECRL学习到的状态-动作嵌入（右）围绕原点形成清晰的结构化环状，而CRL的嵌入（左）则较为杂乱。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>理论框架</strong>：分析了目标条件操作任务的对称性，提出了<strong>目标条件群不变MDP（GCGI-MDP）</strong>，统一了目标条件MDP和群不变MDP，并证明了其最优Q函数不变和最优策略等变的性质。</li>
<li><strong>算法创新</strong>：提出了一种为对比RL构建<strong>旋转不变评论家</strong>的新方法，并将其与<strong>旋转等变行动者</strong>相结合，形成了<strong>等变对比RL（ECRL）</strong>算法，显式利用了机器人操作中的旋转对称性。</li>
<li><strong>实验验证</strong>：通过大量实验证明，ECRL在基于状态和基于图像的一系列机器人操作任务中，显著提高了样本效率和泛化能力， consistently超越强基线，并成功扩展到离线RL设置。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：主要关注于平面旋转对称性（SO(2)或C_N）；在实际实现中使用离散群C_N来近似连续的SO(2)对称性，这可能引入近似误差；方法尚未在更复杂的非平面对称性或动态系统中进行测试。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>扩展对称性类型</strong>：可以将框架扩展到其他类型的对称群，如平移对称性、缩放对称性或更复杂的李群，以处理更广泛的机器人任务。</li>
<li><strong>结合更先进的表示学习</strong>：将等变约束与更复杂的对比学习或自监督表示学习方法结合，可能进一步改善表示质量和策略性能。</li>
<li><strong>应用于更广泛的离线RL</strong>：ECRL在离线RL上的初步成功表明，利用对称性对于从有限数据中学习高效策略具有潜力，可探索其在更大规模离线数据集上的应用。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出等变目标条件对比强化学习（ECRL），解决目标条件强化学习中样本效率低和空间泛化能力差的问题。方法核心是构建旋转不变的目标条件MDP形式化框架，设计旋转不变评论家与旋转等变行动者进行对比学习。实验表明，该方法在多种仿真任务（状态与图像输入）中均优于基线，并成功扩展至离线强化学习设置。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.16139" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>