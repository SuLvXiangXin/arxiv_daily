<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.10851" target="_blank" rel="noreferrer">2510.10851</a></span>
        <span>作者: Leng, Tingxuan, Wang, Yushi, Zheng, Tinglong, Luo, Changsheng, Zhao, Mingguo</span>
        <span>日期: 2025/10/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于强化学习（RL）的人形机器人步态研究取得了显著进展，主流方法通过在训练中引入随机力扰动来鼓励策略在各种条件下保持稳定。然而，这种旨在提升鲁棒性的训练设置本质上使学习到的策略偏向于抵抗外力，从而缺乏对人力引导等交互场景至关重要的力顺从能力。此外，速度指令跟踪与对外部力的顺从这两个目标本质上是冲突的：强指令跟踪会降低顺从性，而高顺从性又会损害对当前指令的响应能力。现有学习方法未能明确处理这一权衡，通常在任务特定奖励上做出妥协以提升整体性能，导致在鲁棒的指令跟踪与交互式、顺从的行走之间存在差距。</p>
<p>本文针对人形机器人在以人为中心的环境中需同时实现可靠导航和自然物理交互的痛点，提出将人形步态视为一个需平衡指令跟踪与外力顺从的多目标优化问题。本文的核心思路是引入一个偏好条件化的多目标RL（MORL）框架，通过一个偏好向量来条件化策略，使单一的、集成的全向运动策略能够在刚性跟踪和高度顺从的行为之间平滑插值。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架基于不对称的演员-评论家架构，并集成了一个编码器-解码器模块用于特权特征提取，以实现可部署的策略。训练阶段，策略接收可部署的观测、由编码器从历史观测中提取的潜在嵌入以及偏好向量作为输入，输出期望的关节位置，由底层PD控制器跟踪。评论家则额外拥有仅仿真中可用的特权信息（如外力、线速度）以辅助训练。部署时，仅保留编码器和演员，策略能根据在线指定的偏好向量自适应调整行为。</p>
<p><img src="https://arxiv.org/html/2510.10851v1/compresspng/framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：策略训练框架与特权去噪。扩展的不对称演员-评论家架构，包含一个编码器-解码器模块，用于从可部署观测中重建特权观测，引导编码器提取力和扭矩感知的潜在特征。部署时仅保留编码器和演员，实现基于板载观测、潜在嵌入和偏好向量的偏好条件化控制。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><strong>速度-阻力统一建模</strong>：为使速度指令和外部力在奖励设计中可比，本文从速度-阻力视角，将持续外力通过一个简化映射 <code>v_ext = k * F_ext</code> 转化为等效速度。该模型基于稳态近似，物理上合理且能诱导出直观的顺从行为（被拉动时移动，释放时停止），避免了高阶动力学可能带来的振荡或不稳定。</li>
<li><strong>多目标RL问题表述</strong>：将问题形式化为一个偏好条件化的MORL问题。策略 <code>π(a|o, w)</code> 在偏好向量 <code>w</code> 条件下，最大化期望回报 <code>J(π; w) = E[Σ γ^t * r(s_t, a_t) · w]</code>。在训练中，通过从均匀分布中采样多样的 <code>w</code> 来覆盖帕累托最优解集。</li>
<li><strong>奖励函数设计</strong>：定义了三个目标对应的奖励：指令跟踪奖励 <code>r_c = exp(-||v - v_c||^2 / σ)</code>，力顺从奖励 <code>r_f = exp(-||v - k * F_ext||^2 / σ)</code>，以及包含基座高度、能量成本、稳定性惩罚等的正则化奖励 <code>r_r</code>。偏好权重 <code>w_c</code> 和 <code>w_f</code> 被约束为 <code>w_c + w_f = 2</code>。</li>
<li><strong>特权去噪训练框架</strong>：采用单阶段训练。编码器将堆叠的历史可部署观测 <code>o_H</code> 映射为潜在嵌入 <code>z_t</code>；解码器则试图重建特权观测 <code>o_t^p</code>，其损失函数 <code>L_denoising = ||ô^p - o^p||^2</code> 鼓励 <code>z_t</code> 编码任务相关特征。特别地，增加了针对外力的重建损失 <code>L_denoising,force = ||F_ext^ - F_ext||^2</code> 以强调力相关特征。演员和评论家使用PPO算法进行更新。</li>
<li><strong>课程学习与域随机化</strong>：训练从平坦地形开始，无速度扰动，随后逐渐增加扰动；训练过半后切换到不平坦地形。外力在训练中随机施加，其大小范围后期收窄以增强对细微交互的敏感性。偏好权重 <code>w_c</code> 每回合重新采样。</li>
</ol>
<p>与现有方法相比，创新点体现在：1) 明确将指令跟踪与力顺从这一对直接冲突的目标形式化为多目标优化问题，并通过速度-阻力模型实现统一的奖励设计；2) 提出了一个偏好条件化的MORL框架，使得单一策略能通过调整偏好向量连续地平衡多个目标，无需分层控制器或分阶段训练；3) 集成特权去噪的单阶段训练框架，旨在从历史观测中推断出力相关特征，以实现可部署的力感知策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Isaac Gym仿真环境（4096个并行环境）和Booster T1成人尺寸人形机器人上进行。对比的基线是基于Booster Gym构建的强大人形步态策略，该策略结构相同但未针对力顺从目标进行专门训练（即训练时不包含力顺从目标）。</p>
<p><strong>关键实验结果：</strong></p>
<ol>
<li><strong>帕累托前沿分析</strong>：在速度指令与外力方向相反的设置下测试，评估不同偏好权重（<code>w_c</code> 从0.0到2.0）下的性能。结果以指令跟踪误差和力顺从误差的帕累托前沿呈现。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.10851v1/pics/exp2_combined_with_pareto.png" alt="帕累托前沿与速度响应"></p>
<blockquote>
<p><strong>图3</strong>：相反设置下奖励权重w的影响。每个试验持续5秒，基线策略结果绘制在最右侧供对比。(a)(b)线速度和角速度的帕累托前沿可视化。(c)在三种向后力水平（10N, 20N, 30N）下，平均前进速度。(d)在三种施加扭矩水平（3Nm, 5Nm, 7Nm）下，平均偏航角速度。</p>
</blockquote>
<p>该图表明，MORL策略的解大多是非支配的，构成了帕累托前沿，且曲线平滑，证实了策略能通过连续变化的偏好实现从指令跟踪到力顺从的平滑过渡。当 <code>w_c=2.0</code> 时，其跟踪精度与基线相当；当 <code>w_c</code> 较小时，则表现出基线所缺乏的显著更强的力顺从性。</p>
<ol start="2">
<li><strong>在线偏好切换</strong>：测试在行走过程中在线切换偏好权重的能力。如图5所示，在分别施加恒定速度指令或恒定外力的条件下，切换 <code>w_c</code> 权重，策略能够平滑地转换行为模式，且性能与基线在单一目标跟踪上相当，而基线策略仅支持单一运动模式。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.10851v1/pics/exp4_vertical.png" alt="在线偏好切换"></p>
<blockquote>
<p><strong>图5</strong>：指令权重的在线切换。轨迹持续12秒，指令权重每4秒改变一次。机器人分别被施加恒定的速度指令或外力。策略的行为随指令权重相应变化。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验（MORL vs. 单目标RL）</strong>：对比MORL与将多目标加权求和的单目标RL（SORL）训练曲线。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.10851v1/compresspng/wandb_ablation_icra_5in1.png" alt="MORL与SORL消融对比"></p>
<blockquote>
<p><strong>图6</strong>：MORL与SORL策略在五个指标上的消融研究对比：(1)总奖励，(2)跟踪速度奖励，(3)力顺从奖励，(4)平均存活步数，(5)去噪损失。</p>
</blockquote>
<p>该图显示，MORL策略获得了 consistently 更高的奖励和更好的训练稳定性，其特权去噪过程也收敛更快。SORL策略则未能学会基本的行走技能且不稳定。这表明显式地用MORL建模冲突目标能带来更高效训练和更优的涌现性能。</p>
<ol start="4">
<li><p><strong>抗瞬时扰动鲁棒性</strong>：测试策略在随机方向的瞬时力脉冲冲击下的表现。如表II所示，在中等扰动（30N）下所有策略表现相近。但在更大外力（40N, 50N）下，MORL策略（尤其是更顺从的MORL-f和MORL-m）展现出比基线更高的成功率，且关节峰值扭矩并未增加，甚至在多数情况下更低。这表明MORL策略在稳定性和顺从性方面均具有鲁棒性优势。</p>
</li>
<li><p><strong>实物实验验证</strong>：在Booster T1机器人上的实验证实了策略对不同偏好的适应性（<code>w_f</code> 越高，对人导引越顺从；<code>w_c</code> 越高，对摇杆指令跟踪越快且抗外力越强），以及整合多目标的能力（在指令与外力正交时，能产生对角线行走的融合行为）。基线策略则缺乏灵活性，且在持续人力下变得不稳定。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 将人形机器人的全向步态形式化为一个需平衡速度指令跟踪与外力顺从的多目标优化问题，并提出了基于速度-阻力模型的统一奖励设计；2) 提出了一个偏好条件化的多目标RL框架，能够学习单一的、统一的策略，无需分层控制器或分阶段训练即可连续平衡这些目标；3) 首次在仿真和实物部署中验证了人形机器人能够实现跨不同交互偏好（尤其是力顺从）的自适应、可部署步态行为。</p>
<p>论文自身提到的局限性包括：研究集中于水平面运动，垂直动力学和复杂地形不在范围内；所采用的稳态速度-阻力模型可能限制了对高频或瞬时外力的响应带宽。</p>
<p>本文工作对后续研究的启示在于：为处理机器人任务中固有的、直接冲突的目标提供了新的框架（偏好条件化MORL）；探索更动态的力映射模型或结合触觉传感器可能进一步提升交互的带宽和自然度；该框架有望扩展至更复杂的人形机器人交互任务，如协同搬运或物理引导导航。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人运动需平衡导航命令跟踪与人力交互外力顺应性的核心问题。提出偏好条件多目标强化学习框架，通过速度阻力因子建模外部力以设计奖励，并采用编码器-解码器结构从可部署观测中提取特权特征，实现单一策略集成命令跟踪与顺应行为。仿真与实物实验表明，该框架提升了运动适应性与训练收敛性，成功实现了可部署的偏好条件人形机器人 locomotion。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.10851" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>