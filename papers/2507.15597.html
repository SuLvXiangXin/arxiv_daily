<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.15597" target="_blank" rel="noreferrer">2507.15597</a></span>
        <span>作者: Zongqing Lu Team</span>
        <span>日期: 2025-07-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型在需要高灵巧性的复杂操作任务上表现不佳，且对新场景和任务的泛化能力差。这主要源于其依赖存在显著模拟-现实鸿沟的合成数据，或缺乏规模和多样性的遥操作演示数据。对于灵巧手而言，由于操作复杂性和硬件成本，数据稀缺问题尤为严重。人类视频提供了丰富的真实世界数据，但先前利用人类视频的工作多采用隐式学习方法（如对比学习、潜在动作优化），其学习机制和转移效果不明确，且未能实现类似大语言模型/大视觉语言模型中指令调优所带来的显著性能提升。本文认为，这种差异源于数据结构的根本不同：在视觉-语言-动作模型中，文本/2D视觉输入与具有本体感觉需求的3D动作空间之间存在异构性。</p>
<p>本文旨在探索能否像GPT-3预训练语言一样，从大规模人类视频中预训练一个灵巧的视觉-语言-动作模型，以显式模仿人类动作并通过后训练适配到机器人手上。核心思路是将人手视为“基础操作器”，提出<strong>物理指令调优</strong>这一新训练范式，通过大规模人类视频预训练、物理空间对齐和后训练适配，弥合人类视频与具身动作之间的鸿沟。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的训练范式名为“物理指令调优”，其整体框架分为三个核心阶段：1) 基于大规模人类视频的视觉-语言-动作模型预训练；2) 物理空间对齐，以统一异构数据并嵌入3D空间推理能力；3) 后训练适配，将预训练模型迁移到下游机器人操作任务。</p>
<p><img src="https://arxiv.org/html/2507.15597v1/x2.png" alt="物理指令调优框架"></p>
<blockquote>
<p><strong>图2</strong>：物理指令调优范式概览。<strong>左</strong>：部分级运动令牌化将连续手部运动转为离散令牌；物理空间对齐通过坐标系对齐和MANO参数化，统一从人类视频到真实机器人数据等异构数据源。<strong>中</strong>：预训练阶段，将视觉-文本参数扩展至包含运动参数，在统一序列中实现视觉、文本和运动令牌的多头注意力。<strong>右</strong>：扩展阶段展示注意力机制如何适应预训练的跨模态依赖，随后在后训练中融入动作参数以生成用于下游机器人任务的最终视觉-语言-动作模型。</p>
</blockquote>
<p><strong>核心模块与技术细节：</strong></p>
<ol>
<li><p><strong>预训练与模型架构</strong>：模型Being-H0基于预训练的大视觉语言模型InternVL3架构构建，包含视觉编码器和投影器。其核心创新在于将手部运动作为与文本、视觉并列的模态进行统一建模。训练数据为三元组 <code>(视觉v, 语言t, 运动m)</code>，其中运动 <code>m</code> 由MANO参数（关节角、手腕旋转、平移、形状）表示。模型通过自回归方式，根据视觉和语言指令预测运动令牌序列，损失函数为标准的下一个令牌预测的负对数似然。</p>
</li>
<li><p><strong>部分级运动令牌化</strong>：为了解决连续手部运动的精确离散化问题，本文提出了基于分组残差量化的部分级运动令牌化方法。该过程通过一维卷积编码器将连续MANO序列编码为特征，再经过矢量量化得到离散的运动令牌 <code>{m1, ..., mn}</code>，最后通过解码器重建。该方法声称能达到毫米级重建精度。运动令牌被特殊标记 <code>&lt;MOT&gt;</code> 和 <code>&lt;/MOT&gt;</code> 包裹，以便与文本令牌无缝整合到统一的语言模型序列中。</p>
</li>
<li><p><strong>统一跨模态推理</strong>：所有模态（视觉、文本、运动）被处理成统一的令牌序列。跨模态交互通过共享注意力机制实现，即查询、键、值矩阵由拼接后的视觉、文本、运动状态共同计算得出。这使得模型能够学习丰富的多模态依赖关系，例如将视觉场景映射到操作策略、将语言指令落实到精确的手指运动。</p>
</li>
<li><p><strong>物理空间对齐</strong>：为了应对不同数据源（如动捕、VR、RGB视频）在相机内参、坐标系和记录条件上的异构性，并弥补模型在2D预训练中缺乏的3D空间先验，本文引入了物理空间对齐。其核心是通过坐标系统一和MANO参数标准化，将所有观察数据对齐到一个一致的物理坐标系中，从而为模型注入3D空间推理和物理理解能力。</p>
</li>
<li><p><strong>后训练适配</strong>：将预训练模型迁移到机器人时，由于形态学差异，人类手部运动不能直接使用。本文采用了一种简单的MLP投影策略，使用一组可学习的查询作为动作块来预测机器人动作。文中指出未来将探索更复杂的迁移策略。</p>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：与依赖隐式对齐或小规模实验室数据的方法不同，本文首次提出以显式运动建模的方式，从大规模人类视频中预训练灵巧的视觉-语言-动作模型。其创新具体体现在：1) 将人手确立为机器人操作的基础和通用标准；2) 设计了能保持毫米级精度的运动离散化方法；3) 提出了包含物理空间对齐的端到端训练范式。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与基准</strong>：本文构建了大规模数据集 <strong>UniHand</strong>，集成了动捕、VR和仅RGB视频等多种异构来源，包含超过1.5亿个基于运动的指令样本，涵盖150多个任务。实验评估包括：1) <strong>手部运动生成</strong>：在HumanML3D和BABEL基准上测试文本到运动生成；2) <strong>指令跟随</strong>：在自定义的Ego4D-MANO子集上评估视觉-语言到运动的生成；3) <strong>真实机器人操作</strong>：在Franka机械臂和Dora-Hand灵巧手上进行零样本和少样本任务评估。</p>
<p><strong>对比方法</strong>：对比的基线包括运动生成领域的T2M-GPT、AttT2M、MLP等，以及机器人视觉-语言-动作模型领域的GR00T、OpenVLA、Octo等。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><p><strong>手部运动生成</strong>：在HumanML3D上，Being-H0的FID分数为0.102，优于对比方法（如T2M-GPT的0.140）。在BABEL上，其Top-1准确率达到41.2%，也优于基线。<br><img src="https://arxiv.org/html/2507.15597v1/x3.png" alt="手部运动生成结果"></p>
<blockquote>
<p><strong>图3</strong>：在HumanML3D和BABEL基准上的文本到手部运动生成量化结果。Being-H0在FID和准确率等指标上优于现有方法。</p>
</blockquote>
</li>
<li><p><strong>指令跟随（视觉-语言到运动）</strong>：在Ego4D-MANO子集上，Being-H0生成的关节位置误差（MPJPE）为17.6mm，腕部旋转误差为11.2°，显著优于直接从人类视频进行隐式学习的基线方法GR00T。<br><img src="https://arxiv.org/html/2507.15597v1/x4.png" alt="指令跟随结果"></p>
<blockquote>
<p><strong>图4</strong>：在Ego4D-MANO上的视觉-语言到手部运动生成结果。Being-H0在运动重建精度上大幅领先于GR00T等隐式学习方法。</p>
</blockquote>
</li>
<li><p><strong>真实机器人操作</strong>：</p>
<ul>
<li><p><strong>机械臂操作</strong>：在6个涉及抓取和移动的零样本任务中，Being-H0的成功率为66.7%，而GR00T为16.7%，OpenVLA为33.3%。<br>  <img src="https://arxiv.org/html/2507.15597v1/x5.png" alt="机械臂操作结果"></p>
<blockquote>
<p><strong>图5</strong>：Franka机械臂的零样本操作任务成功率和定性示例。Being-H0表现最佳。</p>
</blockquote>
</li>
<li><p><strong>灵巧手操作</strong>：在Dora-Hand上进行“拿起海绵”和“旋转杯子”的少样本（5-10个演示）适配任务。Being-H0在“拿起海绵”任务中达到80%的成功率，而GR00T和从零训练的策略均失败。<br>  <img src="https://arxiv.org/html/2507.15597v1/x6.png" alt="灵巧手操作结果"></p>
<blockquote>
<p><strong>图6</strong>：Dora-Hand灵巧手的少样本操作任务成功率和定性示例。Being-H0能够成功完成复杂的手指协调任务。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p><strong>消融实验</strong>：消融研究验证了物理指令调优各个组件的贡献。</p>
<ul>
<li><strong>运动令牌化</strong>：比较了不同量化方法（VQ, RQ, GRQ）。本文提出的GRQ方法在重建精度（MPJPE 4.6mm）和下游任务成功率上均最优。</li>
<li><strong>物理空间对齐</strong>：移除对齐模块会导致模型性能显著下降，证明了其在统一异构数据和学习3D先验中的关键作用。</li>
<li><strong>数据与模型规模缩放</strong>：实验表明，随着模型参数（从1B到8B）和数据规模（从3M到150M样本）的增加，模型在运动生成和机器人操作任务上的性能持续提升，展示了良好的缩放特性。<br><img src="https://arxiv.org/html/2507.15597v1/x7.png" alt="缩放定律"><blockquote>
<p><strong>图7</strong>：模型与数据规模的缩放曲线。随着参数和数据的增加，手部运动生成误差（MPJPE）持续降低，机器人操作成功率持续提升。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>物理指令调优</strong> 新范式，将人手确立为机器人操作的基础，系统性地通过预训练、物理空间对齐和后训练适配，解决了从人类视频学习具身智能的数据异构和迁移难题。</li>
<li>设计了 <strong>部分级运动令牌化</strong> 方法，实现了对连续手部运动的毫米级精度离散化，使其能与语言模型架构兼容。</li>
<li>构建了超大规模数据集 <strong>UniHand</strong> 并训练了 <strong>Being-H0</strong> 模型，首次证明了通过显式运动建模，从大规模人类视频预训练灵巧视觉-语言-动作模型的可行性，并在运动生成和真实机器人操作中展现出卓越性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前版本未建模交互物体的状态（如6D位姿），且机器人控制转移采用的适配策略（MLP投影）相对简单。</p>
<p><strong>对后续研究的启示</strong>：本研究为利用海量互联网人类视频数据训练通用具身智能模型开辟了新路径。未来的工作可以集中在：1) 探索更高效、自适应的从人类到机器人的控制转移策略；2) 将物体交互的显式建模纳入框架；3) 进一步扩大数据规模和模型容量，探索其性能边界。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Being-H0模型，旨在解决现有视觉-语言-动作模型因依赖合成或遥操作数据而导致的灵巧操作能力不足、泛化性差的问题。方法核心是**物理指令调优**范式，结合大规模人类视频预训练、物理空间对齐与机器人任务适应，并采用**部分级运动标记化**实现毫米级手部轨迹建模。实验表明，该模型在手部运动生成与指令遵循方面表现优异，且能有效迁移至真实机器人操作任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.15597" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>