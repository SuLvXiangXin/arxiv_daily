<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.11258" target="_blank" rel="noreferrer">2510.11258</a></span>
        <span>作者: Zongqing Lu Team</span>
        <span>日期: 2025-10-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人的移动操作是其与人类环境进行多功能交互的基础挑战。尽管近期研究在全身体控制方面取得显著进展，但移动操作领域仍探索不足，通常依赖于硬编码的任务定义或昂贵的真实世界数据收集，这限制了自主性和泛化能力。现有方法存在关键局限性：基于强化学习的方法通常依赖任务特定的子任务分解和奖励函数设计，可扩展性差；利用SMPL人体模型和特殊仿真参数的方法难以迁移到真实机器人；而基于真实机器人遥操作和模仿学习的方法虽然有效，但扩展大规模真实数据集以实现空间泛化和多任务学习成本高昂。本文针对从少量数据学习可泛化人形移动操作策略这一痛点，提出了一个结合仿真数据生成与分层控制的新视角。核心思路是：仅需单次仿真演示，通过一个结合底层通用全身控制器与高层操作策略的分层框架，自动合成大量成功轨迹，并训练出能零样本迁移到真实机器人的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>DemoHLM框架包含一个基于仿真的数据生成管道和一个用于人形移动操作的控制层次结构。整体流程如下：对于每个任务，首先在仿真中通过VR遥操作收集单次人类演示轨迹；然后，数据生成管道基于此演示，通过修改初始条件并重放高层运动命令，合成数百至数千条成功轨迹；最后，在这些合成数据上通过模仿学习训练高层操作策略，该策略输出高层命令驱动底层全身控制器，最终实现零样本真实世界部署。</p>
<p><img src="https://arxiv.org/html/2510.11258v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：DemoHLM整体框架概述。左侧为数据生成与训练流程：通过VR收集单次演示，将其转换为物体坐标系下的轨迹，进而生成大量合成数据用于训练模仿学习策略。右侧为部署流程：训练好的策略根据机器人本体感知和相机中的物体姿态，输出高层命令给全身控制器，在真实机器人上执行任务。</p>
</blockquote>
<p><strong>核心模块1：数据生成管道</strong>。目标是基于单次演示 $\tau^h$，合成包含随机化初始机器人及物体姿态的大数据集 $\mathcal{D}$。首先，在首次接触时刻 $t_c$ 将演示分割为接触前段 $\tau^o$ 和接触后段 $\tau^p$。关键步骤是进行两种坐标系转换：</p>
<ol>
<li><strong>物体中心阶段</strong>（针对 $\tau^o$）：将末端执行器位姿转换为相对于物体位姿的表示，即 $T^{\text{eef}}_{\text{obj}}=T^{\text{eef}}(\bar{T}^{\text{obj}})^{-1}$。这使得机器人在不同物体初始位姿下能复现相同的相对接近动作。</li>
<li><strong>本体感知中心阶段</strong>（针对 $\tau^p$）：将末端执行器位姿转换为相对于接触时刻 $t_c$ 自身位姿的表示，即 $T^{\text{eef}}<em>{\text{pro}}=T^{\text{eef}}(\bar{T}^{\text{eef}})</em>{t_{c}}^{-1}$。这适用于末端执行器相对于物体基本静止的操作（如抬起立方体）。</li>
</ol>
<p>生成仿真轨迹时分为三个阶段：</p>
<ul>
<li><strong>移动阶段</strong>：当机器人初始位置远离物体时，使用PD控制器生成速度命令 $(v_x, v_y, \omega_{yaw})$，驱动全身控制器使机器人移动到接近演示起始点的合适位置。</li>
<li><strong>预操作阶段</strong>：基于物体中心轨迹 $\tau^o$。目标末端位姿为 $\hat{T}^{\text{eef}}=T^{\text{eef}}_{\text{obj}}\bar{T}^{\text{obj}}$。为解决控制器误差导致的起始点偏差，在阶段初始引入了从当前位姿到目标起始位姿的插值轨迹 $\hat{\tau}^{in}$。</li>
<li><strong>操作阶段</strong>：基于本体感知中心轨迹 $\tau^p$。目标末端位姿为 $\hat{T}^{\text{eef}}=\bar{T}^{\text{eef}}(T^{\text{eef}}_{\text{pro}})^{-1}$。</li>
</ul>
<p>将三个阶段整合后的目标末端轨迹输入底层控制器，即可收集到用于策略训练的状态-动作轨迹 $\tau_i$。</p>
<p><strong>核心模块2：底层全身控制器</strong>。采用类似AMO的控制器，它将高层运动参数映射为全身关节力矩。输入为期望运动参数 $(v_x, v_y, \omega, h, r, p, y, \mathbf{q}<em>{\text{upper}})$，包括线速度、偏航角速度、躯干配置（高度、横滚、俯仰、偏航）和上半身关节位置。控制器输出目标关节位置 $\mathbf{q}</em>{\text{target}}$，并通过PD控制进行跟踪。该控制器运行在50Hz，显著降低了高层策略需处理的动作维度，并保证了平衡性与仿真到真实的迁移性。此外，还有一个2自由度主动视觉控制器，根据目标物体在相机帧中的位置 $\mathbf{p}_c$，按比例控制颈部关节，使目标保持在图像中心附近。</p>
<p><strong>核心模块3：高层操作策略</strong>。策略目标是在高层命令空间中学习任务相关的操作行为。其观测包括关节位置 $\mathbf{q}<em>{\text{pos}}$、关节速度 $\mathbf{q}</em>{\text{vel}}$、躯干横滚/俯仰角 $r, p$，以及相机帧中目标物体的估计6D位姿 $\mathbf{p}_{\text{obj}}^{\text{camera}}$。策略输出即为上述发送给全身控制器的高层命令。论文评估了多种行为克隆架构作为策略实例，包括带动作分块的MLP、ACT和Diffusion Policy。高层策略运行在10Hz，侧重于需要时间聚合观测和更长视野推理的决策。</p>
<p><strong>创新点</strong>：与现有方法相比，DemoHLM的主要创新在于将源自MimicGen的“单演示生成数据”思想，成功扩展到了需要协调全身关节以维持平衡的人形机器人移动操作场景。其关键技术是采用了分层架构，并设计了针对移动操作的三个阶段数据生成方法，特别是物体中心与本体感知中心的坐标系转换，从而实现了高质量、多样化仿真数据的自动合成。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真平台</strong>：IsaacGym。</li>
<li><strong>真实机器人</strong>：Unitree G1人形机器人，加装2自由度颈部（搭载Intel RealSense D435 RGB-D相机）和3D打印的平行夹爪。</li>
<li><strong>任务</strong>：设计了10个具有挑战性的移动操作任务（如图3所示），分为使用橡胶手的任务（LiftBox, PressCube, PushCube, Handover）和使用夹爪的任务（GraspCube, OpenCabinet, PushCart, EraseBoard, PourWater, ExchangeCube）。</li>
<li><strong>对比Baseline</strong>：在相同合成数据上训练不同的模仿学习策略架构，包括ACT、带动作分块的MLP和Diffusion Policy。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.11258v1/x3.png" alt="任务图示"></p>
<blockquote>
<p><strong>图3</strong>：评估DemoHLM的十项移动操作任务。前四项可使用橡胶手完成，后六项需要平行夹爪进行抓取和操作。每个任务的初始物体和机器人姿态都是随机化的。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>合成数据量对性能的影响</strong>：如表1所示，在所有任务中，策略成功率随着数据集大小的增加而单调提升，但边际收益逐渐递减。例如，GraspCube任务的数据集从100条增至5k条时，成功率从56.95%提升至87.86%。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.11258v1/x5.png" alt="数据量消融实验表"></p>
<blockquote>
<p><strong>表1</strong>：仿真中不同生成数据集大小下所有任务的成功率（%）。表明增加数据量能持续提升性能，但存在边际收益递减现象。</p>
</blockquote>
<ol start="2">
<li><strong>不同模仿学习架构的性能差异</strong>：如表2所示，更复杂、具有时间表达能力的架构（ACT和Diffusion Policy）性能相似且优于简单的MLP基线。例如在ExchangeCube任务中，ACT成功率为52.87%，而MLP仅为15.05%，凸显了先进BC架构对于学习复杂移动操作的重要性。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.11258v1/x6.png" alt="BC方法对比表"></p>
<blockquote>
<p><strong>表2</strong>：不同模仿学习方法在各项任务上的成功率（%）。ACT和Diffusion Policy表现相近且优于MLP基线。</p>
</blockquote>
<ol start="3">
<li><p><strong>数据生成成功率与初始状态随机范围的关系</strong>：如表3所示，随着初始状态采样区域 $R_1$ 到 $R_3$ 的扩大，数据生成的成功率会下降（如Handover任务从99.7%降至79.2%），这归因于更难的逆运动学求解和碰撞可能性增加。但即使在最大区域 $R_3$ 下，管道仍能产生足够多的高质量轨迹用于有效学习。</p>
</li>
<li><p><strong>仿真到真实迁移</strong>：在真实Unitree G1机器人上进行了零样本部署测试。如图4所示，学习到的策略成功完成了包括GraspCube、OpenCabinet、PourWater等在内的多项任务，展示了强大的仿真到真实迁移能力。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.11258v1/x4.png" alt="真实世界策略执行"></p>
<blockquote>
<p><strong>图4</strong>：真实世界策略执行序列。每对行展示了时间对齐的第一人称和第三人称视角。帧按时间从左到右进展，验证了策略在真实环境中的有效执行。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个适用于人形机器人的、基于仿真的数据生成管道，能够从单次演示合成大量多样化成功轨迹，从而实现了移动操作模仿学习策略的有效泛化。</li>
<li>引入了整合底层全身控制器与以物体为中心的运动规划技术，解决了在人形机器人上高质量生成移动操作数据的关键挑战。</li>
<li>在仿真和真实人形机器人上对框架进行了广泛评估，涵盖十项任务，证明了其在学习人形移动操作和执行仿真到真实部署方面的鲁棒性和数据效率。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于对物体6D姿态的精确估计，这在杂乱环境中可能具有挑战性。此外，当前方法可能无法处理高度动态的物体交互或需要复杂力控的任务。</p>
<p><strong>对后续研究的启示</strong>：DemoHLM展示了一条通过高效仿真数据生成实现复杂人形技能学习的可行路径。后续工作可以探索对物体姿态估计更鲁棒或更少依赖的表示方法；可以考虑将本框架与强化学习结合，使策略能够超越演示内容；还可以尝试将数据生成和分层控制框架扩展到更复杂的任务，如双臂操作、工具使用等。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>DemoHLM旨在解决人形机器人移动操作自主性与泛化能力不足的问题，克服传统方法依赖硬编码任务或昂贵真实数据收集的局限。该框架采用分层结构，集成底层通用全身控制器（提供全向移动）与高层操纵策略（通过模拟中数据生成和模仿学习训练，仅需单个演示即可自动合成大量轨迹）。实验表明合成数据量与策略性能呈正相关，验证了数据生成管道的有效性和数据高效性；在Unitree G1机器人上的真实实验成功实现模拟到现实转移，在十个移动操作任务中表现出稳健性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.11258" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>