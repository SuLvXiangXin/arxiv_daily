<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.13103" target="_blank" rel="noreferrer">2508.13103</a></span>
        <span>作者: Zhi Hou Team</span>
        <span>日期: 2025-08-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型已成为通用机器人策略的主流框架。这些模型通常采用预训练的视觉语言编码器作为骨干，用于下游机器人任务。然而，一个根本性的局限在于：视觉模型在图像或相机坐标系中进行训练和监督，其潜在表征与相机视角对齐；而大多数机器人控制信号（如末端执行器位姿）定义在机器人基坐标系中。这种感知空间与动作空间的不匹配，在将预训练视觉模型迁移到机器人控制任务时会阻碍有效的策略学习。此外，机器人数据集通常从多样化的相机视角收集，当机器人基座不在相机视野内时，模型需要从有限的2D观测中推断出在机器人基坐标系下一致的动作，这是一个不适定问题。在包含多种视角的大规模预训练中，从不同角度捕捉的同一机器人动作被迫共享机器人空间中的单一监督信号，从而引入了学习冲突并阻碍了泛化。</p>
<p>本文针对上述感知与动作空间错配的核心痛点，提出了一种新视角：将动作预测目标从机器人基坐标系解耦，转而直接在第三方相机坐标系中预测动作。本文的核心思路是：利用相机外参标定矩阵，将机器人基坐标系中的末端执行器动作变换到相机坐标系中作为监督目标，从而在观测空间中对齐动作预测，以提升模型对相机视角变化的鲁棒性和泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的观测中心VLA（OC-VLA）框架是一个轻量级、即插即用的策略。其整体流程是：在训练阶段，利用已知的相机外参矩阵，将示教数据中的机器人基坐标系下的末端执行器动作，转换到对应的相机坐标系下，并以此作为模型学习的监督目标；在推理阶段，模型根据当前视觉观测和语言指令，预测出相机坐标系下的动作，然后再利用相同的相机外参矩阵，将预测动作逆变换回机器人基坐标系，以供机器人执行。</p>
<p><img src="https://arxiv.org/html/2508.13103v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：OC-VLA框架概述。通过将末端执行器动作从机器人基坐标系转换到第三方相机坐标系，OC-VLA使动作预测与跨不同视角的视觉观测对齐，从而在操作任务中实现更好的泛化和鲁棒性。</p>
</blockquote>
<p>核心模块是坐标变换。给定机器人基坐标系下的两个相邻末端执行器位姿 ( \mathbf{P}<em>{\text{world}<em>1} ) 和 ( \mathbf{P}</em>{\text{world}<em>2} )，其在机器人坐标系下的动作 ( \mathbf{A}</em>{\text{world}} = \mathbf{P}</em>{\text{world}<em>2} \mathbf{P}</em>{\text{world}<em>1}^{-1} )。利用世界到相机的变换矩阵 ( \mathbf{T} )，可以得到相机坐标系下的对应位姿 ( \mathbf{P}</em>{\text{cam}<em>i} = \mathbf{T} \mathbf{P}</em>{\text{world}<em>i} )，进而计算出相机坐标系下的动作 ( \mathbf{A}</em>{\text{cam}} = \mathbf{P}<em>{\text{cam}<em>2} \mathbf{P}</em>{\text{cam}<em>1}^{-1} = \mathbf{T} \mathbf{A}</em>{\text{world}} \mathbf{T}^{-1} )。OC-VLA将 ( \mathbf{A}</em>{\text{cam}} ) 转换为7维动作（位置x, y, z，姿态roll, pitch, yaw，以及夹爪状态）作为模型优化的目标。</p>
<p><img src="https://arxiv.org/html/2508.13103v1/x2.png" alt="动作变换示意图"></p>
<blockquote>
<p><strong>图2</strong>：OC-VLA将末端执行器位姿（无论是在离散还是连续动作空间中定义）从机器人基坐标系转换到第三方相机坐标系，统一了跨视角的观测和预测目标，有效替代了使用共享的世界坐标系动作作为预测目标的做法。</p>
</blockquote>
<p>与现有方法相比，创新点具体体现在：1) <strong>预测目标空间的转变</strong>：将监督信号从与机器人本体强耦合的基坐标系，改为与观测直接对齐的相机坐标系。2) <strong>统一化跨视角目标</strong>：无论数据来自何种相机视角，其动作监督目标都经过变换统一到各自的相机坐标系中，消除了因视角不同而产生的监督信号内在冲突。3) <strong>即插即用</strong>：该方法不依赖于特定VLA架构，可方便地集成到现有模型中。</p>
<p><img src="https://arxiv.org/html/2508.13103v1/x3.png" alt="训练与推理流程"></p>
<blockquote>
<p><strong>图3</strong>：机器人基坐标系与相机基坐标系之间的动作转换。训练时，动作从机器人基坐标系转换到相机坐标系并作为真值。推理时，预测的动作从相机基坐标系转换回机器人基坐标系，以便在真实机器人上执行。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：</p>
<ol>
<li><strong>仿真</strong>：使用ManiSkill2基准测试，选取PickCube, StackCube, PickSingleYCB, PickClutterYCB, PickSingleEGAD五个代表性任务，并从30万个随机配置的相机视角中渲染生成超过4万条轨迹的数据集。</li>
<li><strong>预训练</strong>：使用包含1417个不同相机视角的Droid数据集。</li>
<li><strong>真实机器人</strong>：使用Franka Emika Panda机械臂，配备Robotiq 2F-85夹爪和多个RealSense D435i RGB-D相机，收集了15个不同任务的演示数据。</li>
</ol>
<p><strong>对比方法</strong>：</p>
<ul>
<li><strong>基线</strong>：与使用机器人基坐标系动作作为预测目标的相同VLA模型进行对比（文中称为“Robot Base”）。</li>
<li><strong>其他VLA模型</strong>：在真实实验中，还对比了OpenVLA-OFT和 ( \pi_0 ) 模型。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>仿真实验（表I）</strong>：无论使用连续还是离散动作空间，采用相机坐标系作为预测目标均能一致提升任务成功率。特别是在离散动作空间模型中，平均成功率从38.6%提升至52.4%，相对提升约14%。在PickCube任务上，连续动作模型的成功率从71%提升至88%。</li>
<li><strong>真实机器人实验（表II、III）</strong>：<ul>
<li><strong>固定相机视角</strong>：在10样本微调设置下，OC-VLA（Camera Base）平均成功率达68%，优于机器人坐标系基线的58%以及最佳基线OpenVLA-OFT的63.3%。</li>
<li><strong>新相机视角（零样本）</strong>：当在未经训练的新相机视角下评估时，所有模型性能均下降。OC-VLA性能下降14%（从68%到54%），而OpenVLA-OFT下降超过20%（从63.3%到42%），表明OC-VLA对视角变化具有更强的鲁棒性。</li>
<li><strong>相机扰动</strong>：当微调数据本身包含相机视角扰动时，OC-VLA（73.8%）相比机器人坐标系基线（61.3%）的优势更为明显。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.13103v1/x4.png" alt="真实机器人实验平台"></p>
<blockquote>
<p><strong>图4</strong>：真实机器人实验平台，包括Franka Emika Panda机器人、Robotiq 2F-85夹爪和多个RealSense D435i RGB-D相机。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.13103v1/x5.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人实验的定性对比。失败案例用红圈标出。如图所示，在固定视角和新视角下，OC-VLA方法相比机器人坐标系基线能更可靠地完成任务。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>本文的核心消融实验体现在对不同“坐标系”（Coord）和“动作空间连续性”（Continuous）的组合测试（表I）。实验结果表明，在任何一种动作空间（连续或离散）下，使用相机坐标系（Camera）作为监督目标均优于使用机器人坐标系（Robot），验证了坐标系统一这一核心组件的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出观测中心的VLA新范式</strong>：首次系统性地指出并解决了VLA模型中感知（相机空间）与动作（机器人基座空间）的空间错配问题，提出将动作直接锚定在相机观测空间中进行预测。</li>
<li><strong>提出轻量级即插即用解决方案</strong>：通过利用现成的相机外参进行坐标变换，以简单的方式统一了跨异构视角的预测目标，无需对现有VLA架构进行重大修改。</li>
<li><strong>实证验证了显著优势</strong>：在仿真和真实机器人任务上的大量实验表明，该方法能加速收敛、提高任务成功率，并显著增强模型对未知相机视角的泛化能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到该方法依赖于精确的相机外参标定（矩阵 ( \mathbf{T} )）。标定误差可能会在动作从相机空间转换回机器人空间时传播，影响最终执行精度。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>范式推广</strong>：这种“在观测空间中进行预测”的思想可以扩展到其他涉及多模态或多视角对齐的机器人学习问题中。</li>
<li><strong>标定鲁棒性</strong>：未来的工作可以探索如何使模型对相机标定误差更具鲁棒性，或者研究在标定不完全已知情况下的学习方法。</li>
<li><strong>基础模型对齐</strong>：该工作强调了在构建VLA模型时，充分考虑预训练视觉基础模型表征空间特性的重要性，为设计更高效的跨模态对齐策略提供了新思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型因观察空间与动作空间不一致导致的泛化难题，提出观测中心VLA框架。该方法利用相机外参矩阵，将末端执行器姿态从机器人基坐标转换到相机坐标，统一多视角下的预测目标。实验表明，该策略能加速收敛、提高任务成功率，并显著增强跨视角泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.13103" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>