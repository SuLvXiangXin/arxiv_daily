<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ExFace: Expressive Facial Control for Humanoid Robots with Diffusion Transformers and Bootstrap Training - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ExFace: Expressive Facial Control for Humanoid Robots with Diffusion Transformers and Bootstrap Training</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.14477" target="_blank" rel="noreferrer">2504.14477</a></span>
        <span>作者: Zhang, Dong, Peng, Jingwei, Jiao, Yuyang, Gu, Jiayuan, Yu, Jingyi, Chen, Jiahao</span>
        <span>日期: 2025/04/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，人形机器人模仿人类面部表情的方法主要分为两类。传统方法依赖预编程的表达式模板，通过插值生成表情，但受限于固定的表情集，需要大量人工干预，效率和可扩展性低。基于学习的方法利用神经网络学习从人脸表征（如Blendshape）到机器人电机控制信号的映射，提高了准确性和适应性。然而，这些方法仍面临显著挑战：卷积网络难以有效捕捉多模态分布；部分方法处理速度慢、帧率低；更重要的是，现有方法大多仅处理单帧数据，无法处理人类面部表情动态、连续的特性，导致实际应用中产生明显的抖动和不自然的运动。</p>
<p>本文针对上述动态表情生成不连贯、不自然以及数据效率低的痛点，提出了一种新的视角：采用生成式模型处理连续的时序数据。核心思路是提出ExFace系统，它基于扩散Transformer架构，将动态的人脸Blendshape序列以生成式方法映射到机器人电机信号，并结合创新的模型自举训练策略，迭代式地利用模型自身生成的数据进行优化，从而实现精确、平滑且富有表现力的机器人面部控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>ExFace的目标是将表征人类面部表情的Blendshape数据映射到机器人面部的电机控制值。系统整体流程包含三个关键部分：数据收集、网络结构和模型自举训练。</p>
<p><img src="https://arxiv.org/html/2504.14477v1/extracted/6375073/png/full.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图3</strong>：ExFace系统概览。左侧展示了自举训练流程：从使用随机电机值进行初始训练开始，随后利用动态面部表情序列迭代优化模型。右侧展示了Transformer扩散网络架构：以Blendshape数据（c）为条件输入，网络在每次迭代中逐步去噪，最终输出x0为电机控制值。</p>
</blockquote>
<p><strong>1. 数据收集</strong>：实验在具有42个头部自由度的仿人机器人Micheal上进行，重点关注控制面部表情的33个自由度。数据采集主要依赖苹果的ARKit技术。<br><img src="https://arxiv.org/html/2504.14477v1/extracted/6375073/png/Arkit.jpg" alt="数据收集"></p>
<blockquote>
<p><strong>图2</strong>：数据收集过程。ARKit准确捕捉人和机器人面部的blendshape数据，为后续校准和映射提供可靠基础。<br>ARKit可输出编码55个关键面部标志点运动的blendshape数据。采集时，首先捕捉中性表情作为参考，随后ARKit捕获原始blendshape数据并相对于中性表情进行校准，生成校准后的blendshape值。该系统可通过针对不同个体的校准进行适配。</p>
</blockquote>
<p><strong>2. 网络结构</strong>：核心是一个<strong>条件扩散Transformer</strong>模型，旨在处理120帧的连续面部表情序列，以实现实时响应。模型将55维的blendshape序列映射到33维的电机控制值。</p>
<ul>
<li><strong>前向扩散过程</strong>：对干净电机控制序列x0，通过N步马尔可夫链逐步添加高斯噪声，噪声方差由计划βn决定。经过N步后，xN近似服从标准正态分布。</li>
<li><strong>反向去噪过程</strong>：以前向过程得到的含噪数据xn和blendshape条件c为输入，通过一个学习到的网络来预测去噪后的数据。学习到的均值μθ由公式(4)计算，其中x̂θ(xn, n, c)是网络预测的干净数据。网络采用编码器-解码器结构，损失函数为预测电机控制值与真实目标值之间的均方误差（MSE）。</li>
</ul>
<p><strong>3. 模型自举训练策略</strong>：这是本文的关键创新，旨在通过迭代自我改进来提升模型性能。</p>
<ul>
<li><strong>第一阶段（静态初始化）</strong>：对机器人施加随机的单帧电机信号，收集静态的机器人blendshape数据，获得对映射空间的稀疏采样。用此数据训练初始模型，此时映射空间分散且不精确。</li>
<li><strong>迭代优化阶段（动态精炼）</strong>：首先捕获一段人脸表情序列，用当前模型生成电机控制值驱动机器人执行，并采集执行后机器人面部产生的动态blendshape序列。将这些新生成的“机器人blendshape-电机值”数据对反馈给网络进行微调。此过程不断迭代，使模型从稀疏采样逐渐聚焦于人类表情域，获得更密集、更准确的采样。</li>
<li>具体数据量：初始使用600对数据（复制扩展到72,000帧）进行第一阶段训练。第一次迭代添加8,000帧动态数据，之后每次迭代添加4,000帧进行微调，直至模型达到高质量性能。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，ExFace的创新主要体现在两方面：1) <strong>架构上</strong>，首次将扩散Transformer引入机器人面部重定向任务，其生成式特性和Transformer的序列建模能力，使其能有效处理动态连续表情并生成连贯输出；2) <strong>训练策略上</strong>，提出自举训练，利用模型自身在动态执行中产生的数据迭代优化，显著提升了数据效率和最终映射质量。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿人机器人Micheal（缆线驱动，33个面部自由度）上进行，并额外在机器人Hobbs（关节连杆驱动，32个自由度）上验证了跨平台适用性。使用自建的ExFace数据集（包含机器人面部图像、电机控制值和对应blendshape数据）。对比的基线方法包括MLP和Transformer（均在相同数据上训练）。评估使用了一个包含2000帧复杂自然表情的验证序列。</p>
<p><strong>关键定量结果</strong>：</p>
<ul>
<li><strong>电机控制精度</strong>：使用预测电机值与真实值的MSE（电机距离）作为指标。ExFace的电机距离为0.0353，低于MLP（0.0465）和Transformer（0.0383），表明其能更准确地捕捉复杂映射。</li>
<li><strong>表情生成质量</strong>：通过机器人执行预测电机命令后实际产生的blendshape与目标blendshape的MSE（Blendshape距离）评估。ExFace的Blendshape距离为0.0025，同样优于MLP（0.0039）和Transformer（0.0029），说明其生成的表情更接近真实。</li>
<li><strong>自举训练效果</strong>：<br><img src="https://arxiv.org/html/2504.14477v1/extracted/6375073/png/motor_mse.png" alt="自举训练曲线"><blockquote>
<p><strong>图4</strong>：ExFace模型自举训练曲线。展示了通过迭代训练策略实现的性能提升，并对比了使用随机数据插值与使用动态面部表情序列进行训练的结果。结果表明，使用面部驱动的动态序列数据训练显著优于随机插值，能生成更自然的表情。</p>
</blockquote>
</li>
<li><strong>实时性能</strong>：建立了基于Wi-Fi的TCP/IP实时控制系统。从检测到人脸表情到机器人执行动作的整体延迟约为0.15秒，控制命令渲染帧率达60 fps，确保了平滑、自然的面部运动过渡。</li>
<li><strong>跨平台验证</strong>：<br><img src="https://arxiv.org/html/2504.14477v1/extracted/6375073/png/aplication.jpg" alt="跨平台应用"><blockquote>
<p><strong>图5</strong>：在不同机器人上的真人驱动表现。展示了该方法在不同机器人平台上的泛化能力。<br><img src="https://arxiv.org/html/2504.14477v1/extracted/6375073/png/Facial.jpg" alt="电影驱动表演"><br><strong>图6</strong>：由电影片段驱动的机器人表演。机器人准确复现了演员的面部表情。<br>实验证明ExFace在两种驱动方式和自由度配置均不同的机器人上都能生成准确的控制值。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ExFace系统，首次将<strong>扩散Transformer架构</strong>应用于机器人面部表情重定向，实现了从动态人脸Blendshape序列到机器人电机控制值的高质量、连贯映射。</li>
<li>设计了创新的<strong>模型自举训练策略</strong>，通过迭代利用模型自身生成的动态数据进行微调，显著提升了数据利用效率和最终表情生成的准确性与自然度。</li>
<li>构建并发布了<strong>ExFace数据集</strong>，并验证了方法在实时性能、准确性以及跨不同机器人平台方面的优越性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到了一些挑战：1) 依赖ARKit等基于2D的技术，在颈部、眼部细节捕捉上效果欠佳，且对光照敏感，应用于化妆不佳或仿生机器人面部时性能下降；2) 集成的数字人生成系统延迟较高（2-5秒），阻碍完全实时的交互；3) 当前方法主要模仿人类表情，未充分考虑仿生机器人面部结构的独特性，有时可能导致不自然的结果。</p>
<p><strong>启示</strong>：本文为机器人表情控制提供了一个新的强大框架，证明了生成式模型在该领域的潜力。其自举训练思想可推广至其他需要从稀疏数据学习复杂动态映射的机器人任务。此外，工作促进了仿生机器人<strong>数字孪生</strong>的发展，通过在虚拟环境中模拟和优化表情动力学，可以在不损坏物理实体的情况下进行设计和测试，为未来更高效、更自然的仿生机器人设计与人机交互开辟了新途径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ExFace方法，解决人形机器人模仿人类面部表情时精度不足、动作不自然且实时性差的问题。核心技术采用**扩散变换器**，结合创新的**引导训练策略**，实现了从人脸混合形状到机器人电机控制的高质量、平滑映射。实验表明，该方法在**准确性、帧率(FPS)和响应时间**上全面超越现有方法，具备优异的实时性能与自然的表情渲染能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.14477" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>