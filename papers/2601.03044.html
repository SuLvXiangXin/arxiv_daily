<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SOP: A Scalable Online Post-Training System for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SOP: A Scalable Online Post-Training System for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.03044" target="_blank" rel="noreferrer">2601.03044</a></span>
        <span>作者: Jianlan Luo Team</span>
        <span>日期: 2026-01-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过互联网规模数据的大规模预训练获得了强大的跨任务泛化能力。然而，现实世界部署不仅需要广泛的泛化性，还要求在特定任务上达到专家级的熟练度。现有VLA后训练方法主要分为监督微调和强化学习两类，但普遍存在三个关键局限性：1）通常是<strong>离线</strong>的，依赖于预先收集的静态数据集，无法缓解策略部署时的分布偏移；2）通常是<strong>单机器人</strong>的，限制了经验收集的吞吐量和多样性；3）通常是<strong>任务特定</strong>的，为每个任务单独微调会牺牲模型的通用性。这些限制源于底层学习设定，而非算法本身的缺陷。</p>
<p>本文针对上述痛点，提出了一个<strong>系统层面</strong>的新视角：通过紧密耦合执行与学习，构建一个统一的在线反馈循环。核心思路是设计一个可扩展的在线后训练系统，让机器人舰队持续将在线策略经验流式传输至集中式云学习器，并异步接收更新后的策略，从而实现及时的在线策略纠正、并行经验收集的扩展，以及在多任务适应中保持模型的通用性。</p>
<h2 id="方法详解">方法详解</h2>
<p>SOP是一个用于通用策略在线、多任务后训练的<strong>可扩展的行动者-学习者框架</strong>。其整体流程是一个紧密耦合的闭环：机器人舰队持续执行最新的策略并收集轨迹（包括自主运行和人类干预片段），异步上传至共享的在线经验缓冲区；与此同时，集中的云学习器从混合了在线和离线数据的缓冲区中采样，使用后训练算法更新策略参数，并异步地将更新后的策略广播回所有机器人，形成一个低延迟的在线训练循环。</p>
<p><img src="https://arxiv.org/html/2601.03044v1/x2.png" alt="SOP概述"></p>
<blockquote>
<p><strong>图2</strong>：SOP概述。机器人舰队将在线策略运行数据流式传输至云学习器。在失败或不确定情况下触发可选的人类干预，提供纠正后的轨迹或动作，这些数据被纳入流式经验缓冲区。云学习器通过混合在线缓冲区与静态离线缓冲区来构建任务平衡的更新批次，应用可插拔的后训练模块（如HG-DAgger/RECAP），并异步地将刷新后的权重广播回所有行动者，从而形成一个低延迟的在线训练闭环。</p>
</blockquote>
<p>系统的核心模块包括：</p>
<ol>
<li><strong>分布式数据收集与基础设施</strong>：每个机器人运行一个边缘侧客户端，在回合边界将轨迹异步上传至云存储。云端的在线缓冲区接收数据，学习器通过通知和按需检索进行消费。更新后的模型参数通过轻量级的发布-订阅通道同步，端到端延迟在秒到数十秒量级，更新仅在回合间等安全边界应用，避免中途策略变更破坏日志轨迹。</li>
<li><strong>自适应采样策略</strong>：为了在快速适应新收集的在线数据的同时保持多任务覆盖，学习器采用任务平衡的自适应采样策略。在任务间层面，强制统一的任务权重（ω^m = 1/M），确保每个任务贡献相等。在任务内层面，对于每个任务m，根据近期训练损失动态调整其在线缓冲区（ℬ_on^m）和离线缓冲区（ℬ_off^m）的采样比例。在线采样比例 ω_on^m 通过公式(3)计算，其中 α &gt; 1 是一个提升因子，用于在分布偏移下优先处理在线数据以加速适应。ω_on^m 被裁剪到 [0.2, 0.8] 区间以避免极端分配。</li>
<li><strong>算法无关的后训练学习模块</strong>：SOP 在系统层面（数据流、同步）与算法层面（参数更新方式）之间解耦。任何能够消费记录的经验并返回更新参数的后训练方法都可以作为模块 𝒢 插入。论文实例化了两种算法：<ul>
<li><strong>HG-DAgger</strong>：一种交互式模仿学习算法，人类在机器人即将失败时提供实时干预。在SOP中，这些干预片段被持续流式传输至共享缓冲区，供学习器进行频繁的异步更新，从而将HG-DAgger转变为一种实用的、舰队规模的在线策略后训练过程。</li>
<li><strong>RECAP</strong>：一种用于后训练大VLA策略的离线RL算法。SOP通过将持续收集的最新策略轨迹纳入缓冲区，并对不断演进的数据集异步执行RECAP风格的更新，使原本迭代离线的循环工作流程在线化，减少了策略与数据之间的陈旧性。</li>
</ul>
</li>
</ol>
<p>与现有方法相比，SOP的创新点具体体现在：1) <strong>系统层面的紧密耦合</strong>：首次构建了支持在线、分布式、多任务后训练的物理世界系统框架，解决了现有方法在设定上的根本限制。2) <strong>算法无关性</strong>：将系统架构与具体学习算法解耦，展示了如何将现有后训练算法（如HG-DAgger, RECAP）升级为在线、分布式版本。3) <strong>自适应多任务混合采样</strong>：设计了损失驱动的采样策略，在保证多任务覆盖的同时，优先学习当前策略的失败模式，以高效应对分布偏移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在一支由10台双臂机械手（Agibot G1）组成的舰队上进行，评估了三个具有挑战性的真实世界操作任务家族：杂货补货、衣物折叠和箱子组装。</p>
<p><img src="https://arxiv.org/html/2601.03044v1/x3.png" alt="任务图示"></p>
<blockquote>
<p><strong>图3</strong>：三个任务类别的图示。(A) 杂货补货场景；(B) 衣物折叠：机器人展开并折叠衣物的双手操作序列；(C) 箱子组装：展示两个机械臂协调将扁平纸板折叠成3D箱体结构的序列。</p>
</blockquote>
<p>对比的基线方法是相应后训练算法（HG-DAgger和RECAP）的<strong>非SOP（即离线、单任务）版本</strong>。SOP使用一个共享的学习器，并将10台机器人演员舰队跨任务分配收集经验，进行联合训练。</p>
<p><img src="https://arxiv.org/html/2601.03044v1/figures/main_res_large_font.png" alt="主要结果对比"></p>
<blockquote>
<p><strong>图4</strong>：跨三个操作领域的成功率和吞吐量对比。在所有领域，SOP方法都表现出更高的效率和可靠性。SOP w/ HG-DAgger 持续实现比离线方法高2-4倍的吞吐量，并显著降低了失败率。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>多任务后训练有效性</strong>：在3小时的实时交互预算内，SOP显著提升了预训练VLA模型的性能。例如，在衣物折叠任务中，SOP+HG-DAgger将成功率从基线的约40%提升至<strong>96%<strong>，吞吐量从约1.2次/小时提升至</strong>4.8次/小时</strong>。在箱子组装任务中，成功率从约25%提升至<strong>92%<strong>。对于对象种类繁多的杂货补货任务，SOP也带来了显著的性能提升。这些增益是在使用</strong>跨所有任务的单一共享策略</strong>的情况下取得的，并未牺牲通用性。</li>
<li><strong>舰队规模扩展性</strong>：SOP表现出接近线性的扩展性。增加机器人数量可以大致按比例减少达到目标性能水平所需的墙上时钟时间。这表明分布式经验收集是加速在线后训练的关键。</li>
<li><strong>预训练质量与数据效率</strong>：SOP能够高效利用有限的真实世界交互。即使从不同质量的预训练模型（基于不同规模的数据预训练）开始，SOP都能在数小时内实现有效的后训练，使性能快速收敛到高水平。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.03044v1/figures/abl_pretrain_data_scale_no_title.png" alt="预训练数据规模消融"></p>
<blockquote>
<p><strong>图5</strong>：从不同预训练数据规模初始化的模型，经过SOP后训练后的性能对比。结果表明，即使初始性能不同，SOP都能在有限的实际交互时间内（小时级）将模型提升到高成功率。</p>
</blockquote>
<p><strong>消融实验总结</strong>：论文通过对比不同设置，验证了SOP各组件的重要性。1) <strong>在线 vs 离线</strong>：在线、低延迟的SOP设置显著优于离线训练版本，凸显了及时在线策略纠正的价值。2) <strong>多任务联合训练 vs 单任务训练</strong>：SOP的多任务联合训练在保持各任务高性能的同时，维护了单一通用策略，而单任务微调会损害通用性。3) <strong>分布式收集</strong>：舰队规模扩展带来的数据多样性收集速度是性能接近线性提升的基础。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>SOP系统</strong>，这是首个支持VLA模型在物理世界中进行<strong>在线、分布式、多任务后训练</strong>的框架。2) 通过紧密耦合执行与学习，实现了<strong>及时的在线策略纠正</strong>和<strong>接近线性的舰队规模扩展</strong>，使大规模预训练模型能在<strong>数小时</strong>而非数天的真实交互内达到专家级熟练度。3) 证明了SOP的<strong>算法无关性</strong>，成功将两种不同类型的后训练算法（HG-DAgger, RECAP）升级为高效的在线分布式版本。</p>
<p>论文自身提到的局限性包括：系统性能依赖于可靠的网络连接以维持低延迟同步；实验在相对受控的实验室环境进行，更复杂、动态的真实环境会带来额外挑战；人类干预的效率和一致性也是实际部署中需要进一步优化的因素。</p>
<p>本研究对后续工作的启示是：<strong>机器人舰队规模的部署本身可以成为机器人学习系统进步的一个重要且互补的推动力</strong>，与算法和数据的进步相辅相成。通过紧密耦合部署与学习，SOP建立了一个反馈循环，使得扩展机器人舰队不仅能提高后训练效率，还能增加可用于学习的经验的多样性和相关性。这为支持长期、鲁棒的真实世界部署中的持续适应指明了系统设计方向。未来的工作可以探索更复杂的任务编排、更高效的人机交互接口以及跨更大规模异构机器人舰队的应用。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出可扩展在线后训练系统SOP，解决视觉-语言-动作（VLA）模型在现实部署中缺乏专家级任务熟练度的问题。现有方法多为离线、单机器人或任务特定，限制了在线策略适应和可扩展学习。SOP采用在线分布式多任务后训练，通过闭环架构让机器人舰队持续流式传输经验至云端学习器，并异步接收更新策略，实例化交互式模仿学习（HG-DAgger）和强化学习（RECAP）。实验表明，在布料折叠、盒子组装等现实任务中，SOP显著提升VLA模型性能，保持跨任务共享策略，后训练仅需数小时，且性能随机器人数量近线性扩展。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.03044" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>