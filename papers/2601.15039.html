<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.15039" target="_blank" rel="noreferrer">2601.15039</a></span>
        <span>作者: Zhang, Jiyao, Ma, Zhiyuan, Wu, Tianhao, Chen, Zeyuan, Dong, Hao</span>
        <span>日期: 2026/01/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前杂乱场景下的灵巧抓取方法主要依赖于构建大规模合成数据集来捕捉场景分布，并基于此采用回归或生成模型直接从单视角点云观测预测抓取位姿。然而，直接从部分观测到高自由度抓取位姿的映射具有高度非线性，且物理约束对位姿微小变化敏感，导致泛化困难。现有两阶段方法虽结合接触图预测与优化以提升泛化性，但多针对单物体抓取，且优化阶段需依赖完整物体几何，这在因物体堆叠而仅能获得部分观测的杂乱场景中不可行。</p>
<p>本文针对杂乱场景中部分观测下高质量、无碰撞灵巧抓取生成的难题，提出预测一个与场景解耦、同时感知接触和碰撞的中间表示作为优化目标的新视角。其核心思路是：首先预测一种名为“稀疏IBS”的中间表示，它紧凑编码了成功抓取时灵巧手与场景间的几何和接触关系；然后基于此表示设计能量函数进行优化，最终生成稳定且无碰撞的抓取位姿。</p>
<h2 id="方法详解">方法详解</h2>
<p>CADGrasp是一个两阶段框架。第一阶段以单视角场景点云为输入，预测作为中间表示的稀疏IBS；第二阶段以预测的稀疏IBS为约束，通过优化能量函数输出灵巧抓取位姿。</p>
<p><img src="https://arxiv.org/html/2601.15039v2/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：CADGrasp方法总览。(I) 条件IBS生成：训练扩散模型建模条件概率分布 p(ℐ|𝒫,𝐓)。(II) 抓取位姿优化：以预测的稀疏IBS ℐ̂ 为约束，优化得到抓取位姿 𝒢。</p>
</blockquote>
<p><strong>核心模块1：稀疏IBS表示</strong><br>稀疏IBS（Sparse IBS）改编自交互平分曲面（IBS），定义为以抓取种子点为中心、以手腕姿态旋转为方向的规范空间中的体素化网格（分辨率n³）。每个体素包含三个通道：IBS曲面占用、拇指与目标物体接触点占用、其他手指与物体接触点占用，占用值分别为1或-1。该表示编码了成功抓取位姿下手与环境的几何关系、手指与物体的接触关系，并指定了确保与环境无碰撞的安全边界，且与场景几何解耦。</p>
<p><img src="https://arxiv.org/html/2601.15039v2/x3.png" alt="稀疏IBS创建"></p>
<blockquote>
<p><strong>图3</strong>：稀疏IBS的创建流程。给定杂乱场景和生成的抓取位姿 𝐠，对场景点云进行规范化与裁剪得到 𝒫∗，进而计算稀疏IBS ℐ。</p>
</blockquote>
<p><strong>核心模块2：条件IBS生成</strong><br>此阶段目标是生成中间表示IBS。首先，采用与DexGraspNet2.0相同的结构预测手腕姿态 𝐓：使用ResUNet14提取点云特征并预测点级可抓取性，经排序和最远点采样得到抓取种子点，再基于点特征通过扩散模型预测 𝐓。接着，以抓取种子点和手腕姿态定义规范空间，将原始点云规范化并体素化为 𝒫∗。然后，使用一个占用扩散模型来建模分布 p(ℐ|𝒫∗)。该模型包含一个3D UNet骨干的占用网络 Ω_o 和点云网络 Ω_p，通过在各层级拼接点云特征到占用特征实现体素级条件引导。训练损失如公式(1)所示，旨在预测去噪后的干净IBS。最后，从估计分布中采样多个IBS候选，并根据基于接触点和方向计算的力闭合分数对其进行排序，选择最优者作为最终中间表示 ℐ̂。</p>
<p><img src="https://arxiv.org/html/2601.15039v2/x4.png" alt="IBS生成"></p>
<blockquote>
<p><strong>图4</strong>：IBS生成模块。训练一个条件占用扩散模型来建模 p(ℐ|𝒫∗)，其中体素级对齐在去噪过程中提供了分层条件。</p>
</blockquote>
<p><strong>核心模块3：基于IBS约束的抓取位姿优化</strong><br>以预测的IBS ℐ̂ 为约束，通过最小化能量函数 𝐄 来优化抓取位姿。能量函数由四部分组成：关节限制能量 𝐄_j、自穿透能量 𝐄_sp、接触能量 𝐄_p 和碰撞能量 𝐄_d。𝐄_j 确保关节角在预设范围内；𝐄_sp 惩罚手部点对间距离过小以防止自穿透；𝐄_p 利用IBS点集和法向约束手指与物体的接触关系；𝐄_d 则驱使拇指和其他手指上的点分别靠近IBS上对应的接触点集，并惩罚手部点与IBS表面的穿透，以此确保无碰撞。总能量函数为各分项的加权和（公式7）。为避免优化过程的不确定性，基于同一个 ℐ̂ 并行优化多个抓取配置，并根据最终的能量残差对其进行排序，选择残差最小的作为最优抓取位姿。</p>
<p><strong>创新点</strong><br>与现有方法相比，创新点主要体现在：1) 提出了与场景解耦的稀疏IBS作为中间表示，使其适用于部分观测的杂乱场景；2) 设计了包含体素级条件引导的占用扩散模型来高效预测此高维表示；3) 构建了一套专门针对稀疏IBS表示的能量函数进行抓取位姿优化，显式地编码了接触与碰撞约束。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用与DexGraspNet2.0相同的数据集和模拟环境。训练使用100个场景，测试使用670个场景（分为松散、随机、密集三种密度）。对比基线包括：DexGraspNet2.0（端到端扩散模型）、HGC-Net（回归模型）、ISAGrasp（适配到杂乱场景的回归模型）、GraspTTA（适配后省略优化阶段的两阶段方法）。评估指标为抓取成功率。此外，还在真实世界设置了5个杂乱场景进行验证。</p>
<p><strong>关键实验结果</strong>：<br>模拟实验结果如表1所示。在GraspNet-1Billion和ShapeNet测试集上，CADGrasp在大多数场景密度下取得了最高的成功率（例如，在GraspNet-1Billion的密集、随机、松散场景中分别达到86.5%、85.5%、80.1%），显著优于基线方法。论文指出，回归和生成式基线受复杂位姿分布和映射非线性影响表现不佳，而CADGrasp的两阶段设计有效缓解了此问题。</p>
<p><img src="https://arxiv.org/html/2601.15039v2/x7.png" alt="对比结果表"></p>
<blockquote>
<p><strong>图7</strong>：模拟实验对比结果表（对应论文表1）。CADGrasp（Ours）在多数测试条件下成功率领先。最后一行展示了其在未训练过的Allegro机械手上的零样本泛化结果。</p>
</blockquote>
<p><strong>跨本体泛化</strong>：由于稀疏IBS表示与机械手本体无关，使用在Leap Hand上训练得到的模型，直接为未训练过的Allegro Hand优化抓取位姿，取得了与Leap Hand相当的成功率（如表1最后一行所示），证明了该表示的通用性。</p>
<p><strong>真实世界结果</strong>：如表2所示，在5个真实杂乱场景的抓取实验中，CADGrasp取得了93.8%的整体成功率（30/32），高于DexGraspNet2.0的83.9%（26/31）。分析表明，IBS的引入显式避免了与桌面及周围物体的碰撞，在密集场景中尤其关键。</p>
<p><img src="https://arxiv.org/html/2601.15039v2/x5.png" alt="真实世界设置"></p>
<blockquote>
<p><strong>图5</strong>：真实世界实验设置。使用搭载Leap Hand的机械臂和第三方视角的RealSense D415相机。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.15039v2/x6.png" alt="真实世界场景与物体"></p>
<blockquote>
<p><strong>图6</strong>：真实世界测试使用的物体（左）和五个杂乱场景的布局示意图（右）。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过消融实验验证了各组件贡献。移除力闭合分数过滤会导致IBS预测质量下降；在优化中分别移除接触能量 𝐄_p 或碰撞能量 𝐄_d 均会导致抓取性能显著降低，证明了这些基于IBS设计的能量函数的必要性。</p>
<p><img src="https://arxiv.org/html/2601.15039v2/x9.png" alt="定性结果"></p>
<blockquote>
<p><strong>图9</strong>：定性结果对比。与基线（DexGraspNet2.0）相比，CADGrasp生成的抓取位姿（绿色）能更好地适应物体形状，避免与场景中其他物体（粉色框）发生碰撞，而基线方法（红色）则产生了碰撞。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个用于杂乱场景灵巧抓取的两阶段框架，其核心是预测一个场景解耦、感知接触与碰撞的稀疏IBS中间表示；2) 设计了结合体素级条件引导的占用扩散模型来预测该表示，并构建了针对该表示的能量函数进行抓取优化；3) 通过全面的模拟与真实实验验证了方法的有效性、优越性以及中间表示支持零样本跨本体泛化的能力。</p>
<p><strong>局限性</strong>：论文提到，对于非常平坦或薄形的物体，生成稳定抓取仍具挑战性。</p>
<p><strong>研究启示</strong>：稀疏IBS作为一种与场景和机械手本体解耦的表示，为杂乱场景下的抓取提供了一种新的、可迁移的抽象方式。未来工作可探索更高效的表示预测网络、更鲁棒的优化策略，以及将该框架扩展至更复杂的操作任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出CADGrasp算法，解决杂乱场景中灵巧手抓取因高自由度、遮挡及物体间碰撞导致的成功率低的问题。方法采用两阶段流程：第一阶段通过体素条件引导的占用扩散模型预测稀疏IBS（一种解耦场景、感知接触与碰撞的中间表示）；第二阶段基于该表示设计能量函数与排序策略，优化生成无碰撞的稳定抓取姿态。实验表明，该方法在仿真和真实场景中均能有效减少碰撞，并保持较高的抓取成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.15039" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>