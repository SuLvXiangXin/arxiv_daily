<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.05172" target="_blank" rel="noreferrer">2512.05172</a></span>
        <span>作者: Wang, Wentao, Liu, Chunyang, Sheng, Kehua, Zhang, Bo, Wang, Yan</span>
        <span>日期: 2025/12/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉强化学习能够直接将复杂的视觉信号转换为动作，在智能体控制领域取得了显著成功。然而，视觉信号的高维性和RL交互的低效性，使得算法难以充分理解环境，从而难以捕获最优奖励且缺乏可解释性。高效地从视觉观测中提取任务相关的表征是突破RL瓶颈的关键。现有研究采用了多种状态抽象方法，如观测重建、转移动态预测和双模拟，但存在成本高和数据冗余的问题。与此同时，大型语言模型和视觉语言模型的发展为增强RL效果开辟了新途径。现有基于LLM的RL方法主要聚焦于在策略层面进行指导，但由于RL模型的前向传播链较长，策略层面的指导难以有效增强骨干网络的表征提取能力，特别是在复杂的视觉输入任务中。因此，本文的核心动机是：利用VLM中蕴含的常识知识，在特征层面（而非策略层面）增强任务相关表征的提取。本文提出了一种名为Semore的新型VLM引导视觉RL框架，其核心思路是：通过一个双路径骨干网络从RGB流中同时提取语义和运动表征，并利用VLM的常识知识在特征层面进行监督和引导，从而将真实世界的表征嵌入到骨干网络中。</p>
<h2 id="方法详解">方法详解</h2>
<p>Semore的整体框架是一个VLM引导的双流表征学习系统，旨在分别提取并增强语义和运动表征，最终融合用于决策。</p>
<p><img src="https://arxiv.org/html/2512.05172v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：VLM引导学习框架整体示意图。包含两个关键模块：1）VLM引导的语义模块，使用相似性损失显式监督语义表征的提取；2）运动监督模块，使用双向交叉注意力将知识感知特征引入运动提取中。</p>
</blockquote>
<p><strong>整体流程</strong>：智能体接收视觉观测序列，分别输入到语义编码器和运动编码器。语义编码器处理单帧图像，运动编码器处理相邻帧的残差。同时，VLM模块处理观测帧，生成描述关键物体的文本语义，并进一步通过CRIS模型生成对应的像素级知识感知特征掩码。该掩码一方面通过相似性损失直接监督语义特征的提取，另一方面通过双向交叉注意力机制引导运动特征的提取并促进语义与运动特征的交互。最终，增强后的语义和运动特征被融合，输入到基于SAC的RL策略网络中。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>特征提取</strong>：</p>
<ul>
<li><strong>语义编码</strong>：使用一个四层卷积网络（3×3卷积核，ReLU激活）处理单帧观测 <code>o_t</code>，输出特征图 <code>F_s^t ∈ R^{C×H×W}</code>，再通过一个全连接层（含层归一化）得到紧凑的语义特征向量 <code>f_s^t</code>。</li>
<li><strong>运动编码</strong>：输入为连续三帧 <code>[o_{t-2}, o_{t-1}, o_t]</code> 的相邻帧残差 <code>[o_{t-1}-o_{t-2}, o_t-o_{t-1}]</code>。编码器结构与语义编码器类似（仅第一层输入通道数不同），输出运动特征图 <code>F_m^t</code> 和向量 <code>f_m^t</code>。</li>
</ul>
</li>
<li><p><strong>VLM引导的语义模块</strong>：</p>
<ul>
<li><strong>文本语义生成</strong>：使用预训练VLM（视觉编码器 <code>g_V</code> 和文本编码器 <code>g_T</code>）处理观测帧 <code>o</code> 和任务特定提示 <code>Pm</code>，生成描述关键物体的文本语义 <code>Y</code>。</li>
<li><strong>知识感知特征生成</strong>：为了获得像素级对齐的视觉特征，论文采用基于CLIP的图像分割框架CRIS。将观测帧 <code>o</code> 和文本语义 <code>Y</code> 输入CRIS，通过Sigmoid函数生成对应关键物体的高置信度掩码，并将不同物体的掩码相加，得到完整的知识感知特征掩码 <code>H_ka</code>。</li>
<li><strong>语义监督</strong>：为了将提取的语义特征 <code>F_s</code> 与真实环境对齐，计算 <code>F_s</code> 与经过语义编码器处理的 <code>H_ka</code>（记为 <code>Ĥ_ka</code>）之间的相似性损失 <code>L_{S-G}</code>（L1范数归一化），从而显式监督语义编码器关注关键区域。</li>
</ul>
</li>
<li><p><strong>运动增强与交互</strong>：</p>
<ul>
<li><strong>交互机制</strong>：由于运动信息（帧差）较为稀疏，难以直接进行特征对齐监督。因此，在训练阶段，引入知识感知特征 <code>H_ka</code> 来引导运动特征 <code>F_m</code>。如图4所示，通过双向交叉注意力计算交互特征图 <code>X</code>，该图融合了语义（来自VLM）和运动信息。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.05172v1/x3.png" alt="特征交互示意图"></p>
<blockquote>
<p><strong>图4</strong>：运动与语义特征的交互。融合方式是拼接，知识感知特征仅在训练阶段使用。</p>
</blockquote>
<pre><code>*   **特征增强**：将交互特征图 `X` 分别通过全连接层处理后，添加到原始的语义特征图 `F_s` 和运动特征图 `F_m` 上，实现二者的相互增强。注意，VLM语义仅用于计算训练时的注意力权重，实际交互发生在语义特征和运动特征之间。
*   **运动预测约束**：为确保运动编码器有效提取特征并去除噪声，引入一个MLP运动预测器 `P_m`。它根据当前运动特征 `f_t^m` 和动作 `a_t` 预测下一时刻特征 `f_{t+1}^m`，并通过转移损失 `L_trans`（L2损失）进行约束。
</code></pre>
<ol start="4">
<li><p><strong>表征融合与RL</strong>：</p>
<ul>
<li>交互增强后，语义和运动特征图被压缩为特征向量 <code>f_t^s</code> 和 <code>f_t^m</code>，拼接得到最终表征 <code>f_t = [f_t^s, f_t^m]</code>。</li>
<li>为减少噪声并聚焦于奖励相关信息，使用一个奖励预测头（借鉴DeepMDP）根据 <code>f_t</code> 和 <code>a_t</code> 预测奖励 <code>r_{t+1}</code>，损失为 <code>L_R</code>。</li>
<li>RL主干采用SAC算法，优化策略损失 <code>L_π</code> 和Q函数损失 <code>L_Q</code>。</li>
</ul>
</li>
<li><p><strong>训练细节</strong>：</p>
<ul>
<li><strong>选择性回放缓冲区</strong>：为解决初期探索效率低的问题，设计了一个由LLM引导的选择性回放缓冲区。LLM对智能体生成的 <code>(o_t, a_t)</code> 对进行宏观合理性评估（如刹车、左转、右转是否合理），并以一定的衰减概率将“合理”的样本加入缓冲区，为训练提供更好的初始数据。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.05172v1/x4.png" alt="选择性回放缓冲区示意图"></p>
<blockquote>
<p><strong>图5</strong>：VLM引导的选择性回放缓冲区。LLM评估观察-动作对的合理性，决定是否将其加入缓冲区。</p>
</blockquote>
<pre><code>*   **总体训练目标**：模型端到端地优化联合损失函数：`L = L_trans + L_{S-G} + L_R + L_π + L_Q`，涵盖了运动约束、语义监督、状态（奖励）预测以及RL目标。
</code></pre>
<p><strong>创新点</strong>：与现有方法相比，Semore的主要创新在于：1) 将VLM的常识知识指导从策略层面下移至特征层面，直接增强编码器的表征提取能力；2) 设计了分离的双流架构及对应的监督策略（语义流用显式对齐损失，运动流用交叉注意力引导）；3) 引入了VLM引导的选择性回放缓冲区以改善初期探索。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：使用CARLA自动驾驶模拟器，在三个具有挑战性的交通场景中进行评估：HighBeam（HB，遇到骑行者）、JayWalk（JW，遇到静止和移动的行人）、HighWay（HW，八车道高速公路）。</li>
<li><strong>实验平台/指标</strong>：输入图像分辨率128×128。训练110k帧，报告5次随机种子的平均回报、驾驶距离、碰撞强度、平均转向和刹车百分比。使用Qwen2-VL-7B-Instruct作为VLM。</li>
<li><strong>对比方法</strong>：与SAC、Flare、CURL、DrQ、DeepMDP以及最新的双流视觉RL方法Simoun进行对比。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>如表1所示，Semore在三个场景的episode奖励、驾驶距离和碰撞强度指标上均全面优于所有基线方法。例如，在JW场景，奖励从Simoun的168提升至201，距离从208米提升至233米，碰撞强度从2382降低至2043。在HW场景提升尤为显著，奖励从Simoun的268大幅提升至316。这表明VLM在特征层面的引导能有效提升表征质量，进而带来更优的决策性能。论文指出，虽然Semore的平均转向和刹车百分比并非最低，但这可能是因为基线方法在复杂场景下未能做出有效的避障决策，而Semore为了安全避障采取了必要的动作。</p>
<p><img src="https://arxiv.org/html/2512.05172v1/figs/scenario.png" alt="CARLA实验场景"></p>
<blockquote>
<p><strong>图6</strong>：CARLA实验场景可视化。左列：JW场景；中列：HB场景；右列：HW场景。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.05172v1/x5.png" alt="VLM输入提示示例"></p>
<blockquote>
<p><strong>图7</strong>：VLM的输入提示示例，用于生成描述场景关键物体的文本。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.05172v1/figs/heat2.png" alt="交互特征掩码可视化"></p>
<blockquote>
<p><strong>图8</strong>：HW场景下计算得到的交互特征掩码可视化。显示了模型关注的关键区域。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文通过逐步添加组件（M1-M4）验证各模块的有效性。M1（仅语义流，无VLM监督）性能最差。M2（双流，直接拼接特征）性能提升，证明了双流设计的有效性。M3（加入VLM语义监督）性能显著提升。M4（在M3基础上加入运动监督与交互）达到最佳性能，证明了运动信息增强与交互模块的必要性。消融结果表明，双流设计、VLM的特征层面监督以及语义-运动交互三者共同构成了Semore性能提升的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了Semore，一个新颖的、基于VLM的视觉强化学习框架，通过在特征层面整合VLM的常识知识指导来增强表征学习。</li>
<li>设计了一个解耦的监督模块：对语义流采用显式相似性损失进行对齐监督；对运动流则采用交叉注意力机制引导其关注关键区域。</li>
<li>在CARLA基准上进行了全面实验，证明了该方法相比现有SOTA方法的优越性及各组件的有效性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提及的局限性主要隐含在方法中，例如：1) 依赖预训练的VLM和CRIS模型，引入了额外的计算成本和可能的模型偏差；2) 选择性回放缓冲区中LLM的评估是宏观和粗糙的，可能无法提供精确的指导。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>特征层面引导的有效性</strong>：证明了将大模型先验知识应用于RL特征编码器监督是一条行之有效的路径，超越了仅在策略层面进行指导的范式。</li>
<li><strong>解耦与交互的架构</strong>：针对视觉RL中语义和运动这两种关键信息，采用先解耦提取、再通过外部知识引导交互的双流架构，为处理复杂多模态表征提供了参考。</li>
<li><strong>改善RL探索</strong>：利用大模型对智能体早期探索行为进行宏观合理性筛选，设计选择性经验回放，为缓解RL采样效率低的问题提供了一种新思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有基于LLM的视觉强化学习方法在策略层面进行指导时，难以确保编码器提取可靠特征、且特征空间复杂高维的问题，提出了一种VLM引导的增强语义运动表示框架（Semore）。该框架采用双路径骨干网络从RGB流中同时提取语义与运动表示，利用VLM的常识知识检索关键信息，并借助预训练CLIP实现文本-图像对齐，从而将真实表示嵌入骨干网络。为高效融合两类表示以辅助决策，方法采用分别监督的方式指导语义与运动的提取，并允许其自发交互。实验表明，在VLM的特征层面指导下，该方法相比现有先进方法表现出高效与自适应的能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.05172" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>