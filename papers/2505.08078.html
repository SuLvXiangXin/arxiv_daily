<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>What Matters for Batch Online Reinforcement Learning in Robotics? - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>What Matters for Batch Online Reinforcement Learning in Robotics?</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.08078" target="_blank" rel="noreferrer">2505.08078</a></span>
        <span>作者: Dong, Perry, Mirchandani, Suvir, Sadigh, Dorsa, Finn, Chelsea</span>
        <span>日期: 2025/05/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人学习面临数据稀缺的挑战，因为机器人数据难以大量获取。一种有前景的解决范式是“批量在线强化学习”：策略部署时自主收集大批量数据，然后离线利用这些数据迭代改进同一策略。这有望减少人工数据收集需求，同时获得自我改进的益处。然而，当前算法难以有效利用自主收集的数据。先前工作主要应用模仿学习或过滤模仿学习方法，但这些方法要么无法有效利用自主数据中的次优演示，要么改进很快饱和，无法随数据量增加而良好扩展。这引出了核心问题：机器人中有效的批量在线强化学习需要哪些关键要素？本文旨在通过系统实证研究，探究算法类别、策略提取方法和策略表达能力这三个关键轴如何影响性能，目标是提供一个通用的有效方案。核心思路是：使用基于Q函数的价值RL引导策略改进，采用隐式策略提取方法，并配合表达能力强的策略类，从而更好地利用自主数据中的多样性进行自我提升。</p>
<h2 id="方法详解">方法详解</h2>
<p>批量在线强化学习的整体框架是一个迭代过程（如算法1所述）。首先，利用初始离线数据集 $\mathcal{D}<em>0$ 训练初始策略 $\pi_0$（和可选的Q函数 $Q_0$）。然后进行N轮迭代：在第i轮，使用上一轮的策略 $\pi</em>{i-1}$ 和Q函数 $Q_{i-1}$ 引导，收集M条轨迹数据 $\mathcal{D}_i$；将所有累积数据 $\cup_i \mathcal{D}_i$ 用于离线重新训练，更新Q函数得到 $Q_i$，并更新策略得到 $\pi_i$。这个过程将数据收集（在线）与策略训练（离线）解耦。</p>
<p><img src="https://arxiv.org/html/2505.08078v1/extracted/6433121/figures/teaser.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：批量在线RL问题概述。策略在初始数据集上训练，用于在部署期间收集批量自主数据，然后在累积数据集上重新训练。本文分析了影响批量在线RL方法的三个关键轴：策略表达能力、算法类别和策略提取方法。</p>
</blockquote>
<p>本文系统分析了构成批量在线RL方法的关键组成部分：</p>
<ol>
<li><strong>算法类别</strong>：比较了模仿学习、过滤模仿学习和基于价值的RL。模仿学习直接对累积数据进行行为克隆。过滤模仿学习则为成功轨迹的转移样本赋予权重1，失败轨迹赋予权重0。基于价值的RL则是在累积数据上训练一个Q函数（本文主要采用隐式Q学习IQL的目标函数），用于指导策略。</li>
<li><strong>策略提取方法</strong>：在基于价值的RL中，如何从Q函数得到用于部署的策略是关键。本文区分了两种方法：<ul>
<li><strong>显式策略提取</strong>：离线训练一个策略，使其在最大化Q值的同时接近行为策略。例如，采用优势加权回归（AWR）目标：$J_{\pi}(\theta)=\mathbb{E}<em>{(s,a)\sim\mathcal{D}}[e^{\beta(Q(s,a)-V(s))}\log \pi</em>{\theta}(a|s)]$。</li>
<li><strong>隐式策略提取</strong>：不单独训练一个最大化Q值的策略，而是在部署时，从策略中采样多个动作，并选择其中Q值最高的动作执行。这实现了策略与价值函数训练的分离。</li>
</ul>
</li>
<li><strong>策略表达能力</strong>：比较了两种策略类别：<ul>
<li><strong>高斯策略</strong>：输出动作的均值和方差，表达能力有限。</li>
<li><strong>基于扩散的策略</strong>：一种更表达力的策略类别，能够建模更复杂的动作分布。</li>
</ul>
</li>
</ol>
<p>基于上述分析，本文提出了一个有效的批量在线RL通用配方：1）训练一个表达力强的模仿学习策略作为行动者；2）在自主数据上训练一个Q函数；3）使用该Q函数进行隐式策略提取，以获得用于自主部署的策略。此外，论文还提出了一个简单的实用补充：在自主部署期间，应用由Ornstein-Uhlenbeck过程建模的少量时间相关噪声，以诱导更多多样性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在模拟和真实机器人任务上进行。模拟实验使用了来自Robomimic（Lift, Can, Square）、MimicGen（Threading, Stack）和Adroit（Pen）的六个复杂机器人操作任务。初始数据集 $\mathcal{D}_0$ 包含5-100条演示，使得初始策略成功率在30-65%之间。进行了N=10到20轮迭代，每轮收集M=200条轨迹。真实世界验证是一个具有挑战性的任务：将胶带挂到钩子上。</p>
<p><strong>对比方法</strong>：主要对比了不同算法类别（IL, Filtered-IL, Value-based RL）、不同策略提取方法（Explicit, Implicit）以及不同策略类别（Gaussian, Diffusion）在批量在线RL过程中的性能。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>算法类别的影响</strong>：基于价值的RL显著优于模仿学习和过滤模仿学习。<br><img src="https://arxiv.org/html/2505.08078v1/extracted/6433121/figures/alg_aggregate.png" alt="算法类别性能对比"><blockquote>
<p><strong>图8</strong>：不同算法类别在多轮改进中的归一化回报。基于价值的RL在所有任务上都显著优于IL和过滤IL。</p>
</blockquote>
</li>
</ol>
<p><img src="https://arxiv.org/html/2505.08078v1/extracted/6433121/figures/heatmap_aggregate.png" alt="状态访问热图"></p>
<blockquote>
<p><strong>图9</strong>：Lift和Square任务上，价值RL与过滤IL在批量在线RL后成功轨迹的状态访问热图。价值RL方法在成功轨迹中实现了更多的状态多样性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.08078v1/extracted/6433121/figures/scale_avg.png" alt="数据规模缩放"></p>
<blockquote>
<p><strong>图10</strong>：不同算法类别在不同数据规模下的归一化回报（所有任务平均）。价值RL能更好地随批量数据增加而扩展性能，而IL或过滤IL则趋向饱和。</p>
</blockquote>
<ol start="2">
<li><p><strong>策略提取方法的影响</strong>：隐式策略提取显著优于显式策略提取。<br><img src="https://arxiv.org/html/2505.08078v1/extracted/6433121/figures/policy_aggregate.png" alt="策略提取方法对比"></p>
<blockquote>
<p><strong>图11</strong>：显式与隐式策略提取在批量在线RL前后的归一化回报。虽然显式提取初始性能更强，但隐式提取在运行批量在线RL后性能显著更好。</p>
</blockquote>
</li>
<li><p><strong>策略表达能力的影响</strong>：表达力强的扩散策略优于高斯策略。<br><img src="https://arxiv.org/html/2505.08078v1/extracted/6433121/figures/expressive_aggregate.png" alt="策略表达能力对比"></p>
<blockquote>
<p><strong>图12</strong>：高斯策略与扩散策略在批量在线RL前后的归一化回报。扩散策略在所有任务上都取得了更好的最终性能。</p>
</blockquote>
</li>
<li><p><strong>完整配方与补充</strong>：结合了价值RL、隐式提取和扩散策略的完整配方，加上时间相关噪声，取得了最佳性能。<br><img src="https://arxiv.org/html/2505.08078v1/extracted/6433121/figures/correlated_avg.png" alt="时间相关噪声效果"></p>
<blockquote>
<p><strong>图13</strong>：在完整配方基础上添加时间相关噪声（OU Noise）与不添加噪声（No Noise）的对比。添加噪声能诱导更多多样性，带来进一步的性能提升。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验总结</strong>：每个组件都对最终性能有重要贡献。价值RL算法类别是有效利用多样性的基础；隐式策略提取对于在迭代过程中稳定学习和持续改进至关重要；高表达力的策略类（扩散）对于生成和利用多样数据是必要的；额外的时间相关噪声则通过增加探索进一步提升了样本效率。</p>
<p><strong>真实机器人结果</strong>：在挂胶带任务上，应用提出的配方，经过三轮批量在线RL迭代，策略成功率相比初始策略提高了30%。<br><img src="https://arxiv.org/html/2505.08078v1/extracted/6433121/figures/recipe_robot.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图15</strong>：在真实世界挂胶带任务上的性能。提出的配方在三次迭代中将成功率从约40%提高到约70%。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>通过系统的实证研究，揭示了算法类别、策略提取方法和策略表达能力是影响机器人批量在线RL性能的三个关键轴。</li>
<li>基于研究发现，提出了一个有效的通用配方：使用表达力强的IL策略作为行动者，用价值RL学习Q函数，并采用隐式策略提取来获取部署策略，辅以时间相关噪声增加多样性。</li>
<li>在模拟的六项复杂操作任务和一项真实机器人任务上验证了该配方的有效性，性能最高可达先前方法的2倍，并在真实任务上取得了30%的成功率提升。</li>
</ol>
<p><strong>局限性</strong>：论文提到，批量在线RL的成功可能依赖于一个“足够好”的初始策略来启动自我改进过程。对于非常差的初始策略，该方法可能无法有效改进。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>本文的配方为机器人批量在线RL提供了一个坚实的基础，后续工作可以在此基础上探索更高效的价值函数学习方法或策略架构。</li>
<li>隐式策略提取的成功表明，在迭代的、数据分布变化的设置中，将策略训练与价值函数引导解耦可能更稳定。这启示了其他离线RL或微调方法在此场景下的改进方向。</li>
<li>时间相关噪声的有效性说明，在批量收集阶段设计更智能的探索策略，可能是进一步提升样本效率和最终性能的途径。</li>
<li>研究提出的分析框架（三个轴）可用于系统地评估未来批量在线RL的新方法。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究了机器人学中批处理在线强化学习（batch online RL）的核心问题：如何有效利用自主收集的大批数据进行策略自我改进，以降低人工数据收集需求。通过系统实证分析，聚焦三个关键轴：算法类（使用Q函数指导优于模仿方法）、策略提取方法（隐式提取，即选择策略分布中的最佳动作，优于显式提取）和策略表达能力（强表达性策略类更优）。基于此提出通用配方，并添加时间相关噪声以增强多样性。实验表明，该配方相比先前方法在性能和扩展性上取得显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.08078" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>