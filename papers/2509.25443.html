<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.25443" target="_blank" rel="noreferrer">2509.25443</a></span>
        <span>作者: He, Zewen, Chen, Chenyuan, Azizov, Dilshod, Nakamura, Yoshihiko</span>
        <span>日期: 2025/09/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于重定向人类运动数据的模仿学习与强化学习已成为人形机器人全身运动控制的主流方法。然而，现有方法存在两个关键局限：首先，大多数人类运动数据集缺乏测量的力数据，导致基于学习的机器人控制主要是基于位置的，难以在真实环境交互中实现适当的柔顺性；其次，现有的基于学习的柔顺控制方法要么缺乏模型意识与稳定性保证，要么无法在线调整参数。本文针对人形机器人在运动-操作任务中缺乏力交互能力与适应性的痛点，提出将模型化的柔顺控制整合到强化学习框架中的新视角。其核心思路是：设计一个两阶段双智能体强化学习框架，通过在对称正定流形上进行刚度矩阵调制，实现可在线调整且保证稳定性的上身柔顺控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>CoTaP的整体目标是一个利用人形机器人运动-操作控制中柔顺信息的完整管道。本文重点研究该管道中强化学习框架下的柔顺调制部分。</p>
<p><img src="https://arxiv.org/html/2509.25443v2/figure/comtap.png" alt="方法管道总览"></p>
<blockquote>
<p><strong>图2</strong>：CoTaP管道概览。红色框突出了本文提出的方法，目标是在人形机器人上实现整个管道。</p>
</blockquote>
<p>整体训练框架采用两阶段策略蒸馏的双智能体架构。机器人状态定义为包含五步历史的关节位置、速度、躯干角速度、投影重力、上身关节扭矩和先前动作。目标空间分为下半身运动目标（如躯干线速度、高度）和上半身操作目标（如目标关节构型、末端执行器任务空间与零空间刚度、柔顺调制比率α）。</p>
<p><img src="https://arxiv.org/html/2509.25443v2/figure/compliance_rl_ws.png" alt="训练框架"></p>
<blockquote>
<p><strong>图3</strong>：本研究的训练框架概述。展示了双智能体策略和两阶段训练（基础策略训练与策略蒸馏）流程。</p>
</blockquote>
<p>核心模块一：<strong>双智能体学习策略</strong>。将机器人分为上下半身（以躯干为界），共享观测但拥有独立的策略网络。下半身策略输出目标关节位置，采用简单的PD控制；上半身策略输出目标关节构型，但通过后续的柔顺控制模块转换为关节扭矩。</p>
<p>核心模块二：<strong>解耦的上身柔顺控制</strong>。基于准静态假设和虚功原理，建立了考虑躯干刚度的末端执行器柔顺与关节柔顺之间的映射关系。通过给定的任务空间柔顺矩阵、零空间刚度以及被视为常数的躯干柔顺，可以解析地求解出满足任务需求的上身关节空间刚度矩阵，并计算包含重力补偿的关节扭矩。</p>
<p>核心模块三：<strong>SPD流形上的柔顺调制</strong>。为了实现不同控制律（如纯PD控制与上述柔顺控制）之间的平滑过渡与融合，本文提出在对称正定流形上进行刚度矩阵调制。具体采用Log-Euclidean插值方法，使用调制比率α对两种控制律对应的刚度矩阵进行插值，确保结果始终是正定的，从而基于现有理论保证系统稳定性。</p>
<p><img src="https://arxiv.org/html/2509.25443v2/figure/manifold.png" alt="SPD流形调制示意图"></p>
<blockquote>
<p><strong>图1c</strong>：SPD流形上的刚度矩阵调制示意图。两个不同的原始刚度矩阵首先通过对数映射到Log-Euclidean空间，进行线性插值，最后通过指数映射映射回来。</p>
</blockquote>
<p>此外，为避免机械臂接近奇异构型时产生问题，调制比率α会根据上身雅可比矩阵的条件数进行自适应调整。</p>
<p>创新点具体体现在：1) 将模型化的、具有稳定性保证的柔顺控制解析解作为“控制基元”嵌入到强化学习框架中，而非让RL直接学习力/阻抗；2) 通过SPD流形上的调制，实现了柔顺特性的在线、平滑、可解释调整；3) 采用两阶段蒸馏策略，先训练一个基础的位置控制策略，再在其指导下训练融合柔顺控制的上身策略，保证了学习的稳定性和效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在Isaac Gym仿真环境中，对Unitree H1人形机器人（19个主动自由度）进行训练。使用经PHC方法处理的AMASS数据集作为上半身运动先验。基线方法为纯关节PD控制的FALCON框架。评估指标包括躯干速度跟踪误差、末端执行器位置跟踪误差、上半身关节扭矩平均值和上半身关节角度跟踪误差。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>恒定负载测试</strong>：在站立模式下，对机器人左手施加-50N的恒定垂直力。图4展示了在不同任务空间刚度设置下，左手在Z轴的位置误差。结果表明，低刚度（高柔顺）时稳态误差较大（约0.12m），高刚度时误差较小（约0.05m），符合预期。同时，将PD控制与柔顺控制以α=0.5调制后，误差曲线结合了二者特点，具有更好的振荡阻尼效果。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.25443v2/figure/var_stiff_vert_50/100.png" alt="恒定负载测试位置误差"></p>
<blockquote>
<p><strong>图4a</strong>：任务空间刚度K_ee=100 N/m（Z轴）时，左手在恒定负载下的Z轴位置误差。误差较大且持续。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.25443v2/figure/var_stiff_vert_50/500.png" alt="恒定负载测试位置误差"></p>
<blockquote>
<p><strong>图4b</strong>：任务空间刚度K_ee=500 N/m（Z轴）时，左手在恒定负载下的Z轴位置误差。误差较小且快速衰减。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.25443v2/figure/var_stiff_vert_50/pd.png" alt="恒定负载测试位置误差"></p>
<blockquote>
<p><strong>图4c</strong>：纯PD控制下的位置误差。初始峰值低但出现反向误差。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.25443v2/figure/var_stiff_vert_50/0.5.png" alt="恒定负载测试位置误差"></p>
<blockquote>
<p><strong>图4d</strong>：α=0.5调制（PD与100 N/m柔顺控制）下的位置误差。结合了两者特点，振荡得到阻尼。</p>
</blockquote>
<ol start="2">
<li><strong>末端执行器冲击测试</strong>：在站立模式下，对左手施加一个短暂的500N冲击力。表IIIa（论文内）汇总了在不同控制设置下的多项误差指标。结果显示，随着任务空间刚度增加，躯干和手部的跟踪误差减小，但手臂所需的关节扭矩增加。单纯降低关节P增益会导致手部跟踪误差显著增大。通过调整调制比率α，可以在跟踪误差和扭矩消耗之间取得平衡，例如α=0.5时，其性能优于纯柔顺控制或纯PD控制。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.25443v2/figure/walk_screen.png" alt="行走中受冲击截图"></p>
<blockquote>
<p><strong>图5</strong>：在行走控制下左手受到外部冲击的截图。上身参考动作出拳。红线代表外部冲击，蓝球代表手和肘部的参考点。</p>
</blockquote>
<ol start="3">
<li><strong>奇异点处理验证</strong>：图11展示了当调制比率α未根据雅可比条件数进行调整时，在臂部接近奇异构型区域，关节扭矩会出现尖峰。而采用本文的自适应调整方法后，扭矩曲线变得平滑，验证了该处理的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.25443v2/figure/singularity.png" alt="奇异点处理对比"></p>
<blockquote>
<p><strong>图11</strong>：奇异点处理对比。未调整α时，关节扭矩在奇异区域出现尖峰；调整后扭矩平滑。</p>
</blockquote>
<p><strong>组件贡献分析</strong>：通过对比不同刚度设置和调制比率下的性能，证明了：1) 模型化柔顺控制模块能够根据指定的任务空间刚度产生相应的柔顺行为；2) SPD流形调制模块能够有效融合不同控制律的优点，实现性能折衷；3) 基于条件数的自适应α调整对于保证实际控制的鲁棒性至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了CoTaP管道及其核心——一个将模型化柔顺控制与强化学习框架相结合的人形机器人控制策略，实现了可在线调制且稳定性有保证的柔顺控制；2) 设计了两阶段双智能体策略蒸馏方法，在引入复杂柔顺控制目标时能稳定训练；3) 在仿真中验证了该方法能有效调节机器人对外部扰动的响应，并在跟踪误差与力消耗间取得平衡。</p>
<p>论文提到的局限性包括：为简化控制器，将躯干柔顺视为常数，忽略了腿部姿态的影响；实验仅在仿真中进行，尚未在真实机器人上验证；Unitree H1手臂自由度有限，限制了柔顺行为的表达范围。</p>
<p>本工作对后续研究的启示：1) 在SPD流形上调制阻抗参数的方法，为基于学习的柔顺控制提供了一个兼具灵活性与稳定性的新工具；2) 双智能体架构与策略蒸馏方法，可用于安全地整合其他模型化控制器到学习框架中；3) 如何获取并利用真实交互的力数据以进一步提升性能，是未来的重要方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在与环境交互时难以实现适当柔顺性的核心问题，提出了一种名为CoTaP的柔顺任务流水线。其关键技术是采用两阶段双智能体强化学习框架：先训练基于位置控制的基础策略，再通过精炼将上半身策略与基于模型的柔顺控制结合，下半身则由基础策略引导。上半身控制通过在SPD流形上进行柔顺调制，实现了可调的任务空间柔顺性。实验在仿真中验证了该方法的可行性，重点比较了不同柔顺设置下系统对外部扰动的响应。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.25443" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>