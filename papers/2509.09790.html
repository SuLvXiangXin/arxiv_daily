<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>How well can LLMs provide planning feedback in grounded environments? - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>How well can LLMs provide planning feedback in grounded environments?</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.09790" target="_blank" rel="noreferrer">2509.09790</a></span>
        <span>作者: Li, Yuxuan, Zhong, Victor</span>
        <span>日期: 2025/09/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在具身环境中训练指令遵循智能体通常需要大量试错或大量高质量标注演示。近期研究表明，预训练的大语言模型（LLMs）和视觉语言模型（VLMs）蕴含对规划有帮助的世界知识，可以减少策略学习对奖励设计和演示的需求。然而，这些模型直接作为策略在具身环境中表现不佳。一个潜在的替代方案是利用它们为智能体行为提供判断和反馈，例如生成奖励函数或识别高效行为进行模仿。本文旨在系统地评估LLMs和VLMs在具身环境中提供准确反馈的能力，针对的痛点是：我们尚不清楚基础模型在多大程度上能够提供高质量的规划反馈，以及这种能力在不同反馈类型、环境领域和推理方法下的表现如何。本文的核心思路是：将LLM/VLM作为反馈模型（FM），给定任务指令、环境观察和智能体行为，将其反馈与基于最优策略计算出的真实反馈进行比较，从而全面量化其反馈质量。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文评估框架的核心是将LLM/VLM作为反馈模型，对智能体在给定环境观察下的行为表现进行评判。反馈模型的输入是行为快照x（包含环境观察和行为），输出是反馈y。通过与基于最优策略π<em>计算出的真实反馈y</em>进行比较，评估反馈的准确性。</p>
<p><img src="https://arxiv.org/html/2509.09790v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：评估反馈模型的整体流程。首先根据策略π采样轨迹，将轨迹中的快照x与专家策略π<em>结合计算真实反馈y</em>。对于每个(x, y*)对，提示反馈模型并将其响应y与真实反馈y*进行比较。</p>
</blockquote>
<p>本文考虑了规划中常用的五种反馈类型，其快照x的构成和真实反馈y*的计算方法如下：</p>
<ol>
<li><strong>二元反馈</strong>：快照x=(s, a)。FM判断动作a在状态s下是否最优（y∈{-1,1}）。真实反馈y* = 1 当且仅当π*(s)=a。</li>
<li><strong>动作建议</strong>：快照x=s。FM直接给出状态s下的最优动作y_act。真实反馈y* = π*(s)。</li>
<li><strong>偏好反馈</strong>：快照x=(s, a1, a2)。FM选择对完成任务更有利的动作（y∈{+1, -1, 0}）。真实反馈基于最优策略下的Q值比较得出。</li>
<li><strong>目标建议</strong>：快照x=s。FM预测为实现任务应前往的理想下一状态y_goal。真实反馈是基于最优策略值函数V*，从当前状态可达的状态中选出的最优下一状态。</li>
<li><strong>增量动作反馈</strong>：快照x=(s, a)。FM提供一个修正量Δ，使得a+Δ比a更有利于任务（即Q(s, a+Δ) &gt; Q(s, a)）。真实反馈y* = π*(s) - a。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.09790v1/pics/domains/cliff_walking.jpg" alt="评估领域"></p>
<blockquote>
<p><strong>图2</strong>：六个评估语言模型反馈的领域示例图，展示了Cliff Walking、ALFWorld、HierarchyCraft、MiniGrid Door Key、MiniGrid Four Rooms和Robomimic Lift Object任务的环境。</p>
</blockquote>
<p>为了衡量反馈模型在不同设置下的表现，本文从六个具有不同观察空间、动作空间和任务复杂度的领域收集策略行为数据（图2）：</p>
<ul>
<li><strong>符号游戏</strong>：Cliff Walking（完全可观察网格导航）、MiniGrid Four Rooms和Door Key（完全可观察房间导航与物品交互）。</li>
<li><strong>语言化文本环境</strong>：ALFWorld（家庭环境中的高层次物体交互）、HierarchyCraft（资源收集与合成的层次化任务）。</li>
<li><strong>连续控制</strong>：Robomimic Lift Object（7自由度机械臂连续控制任务）。</li>
</ul>
<p>本文还评估了可能影响反馈质量的推理方法：</p>
<ul>
<li><strong>上下文学习（ICL）</strong>：在提示中提供输入输出示例。</li>
<li><strong>思维链与思维引导（CoT）</strong>：设计规划相关的问题（思维引导），引导FM完成有帮助的高层中间推理步骤。</li>
<li><strong>获取高层领域动态</strong>：向FM提供环境转移动态的完整或部分文本描述，或提供历史上下文让其推断动态。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了六个基准领域（Cliff Walking, MiniGrid Door Key, MiniGrid Four Rooms, ALFWorld, HierarchyCraft, Robomimic）。对比的基线是不同规模和类型的开源LLMs（Llama 3.1 8B/70B, QwQ 32B, DeepSeek R1）和VLMs（Qwen 2.5 VL 7B/32B/72B, GPT-4o mini）。评估指标是反馈准确率（与真实反馈一致的比例）。</p>
<p>关键实验结果总结如下：</p>
<ol>
<li><strong>LLMs能够提供高质量反馈，更大和具备推理能力的模型表现更优</strong>：如表2所示，LLMs在各领域都能提供相当准确的反馈。模型越大，反馈准确率通常越高。更重要的是，具备推理能力的模型（如QwQ 32B, R1）即使参数量小于非推理模型（如Llama 3.1 70B），也能提供更准确的反馈。小模型（如Llama 3.1 8B）则表现出明显的类别偏差（倾向于预测动作为非最优）和位置偏差（在偏好反馈中倾向于选择第一个动作）。</li>
<li><strong>偏好和二元反馈更容易提供</strong>：对于非推理模型，提供偏好和二元反馈比提供动作建议要容易得多（表2中Llama 3.1 8B/70B在多数领域的偏好/二元准确率显著高于动作建议）。而大型推理模型（QwQ, R1）则能近乎同等高质量地提供所有反馈类型。</li>
<li><strong>在连续控制领域，图像观察能带来更好的反馈</strong>：如表5所示，在状态动作难以用文本简洁描述的Robomimic连续控制任务中，VLMs（使用图像观察）的偏好反馈准确率高达82%，二元反馈达62%，显著优于使用文本描述的LLMs（表2中LLMs在Robomimic的反馈准确率普遍很低）。</li>
<li><strong>反馈质量在复杂动态或连续状态/动作空间中显著下降</strong>：表2显示，在具有复杂层次化动态（HierarchyCraft）或连续状态/动作空间（Robomimic）的领域中，所有模型的反馈准确率都相对较低，表明当前基础模型在理解此类复杂、连续空间方面存在困难。</li>
<li><strong>ICL和思维引导分别对大模型和推理模型帮助更大</strong>：表3和表4显示，上下文学习（ICL）能持续提升大模型（如Llama 3.1 70B, R1）的性能（如在HierarchyCraft中，ICL使R1的动作建议准确率提升0.1，Llama 3.1 70B的偏好反馈提升0.42）。思维引导则能一致地提升推理模型（QwQ, R1）的性能，但对非推理模型的提升不一致，甚至可能使小模型（Llama 3.1 8B）性能下降。对于VLMs，这两种技术均未带来一致的性能提升（表5）。</li>
<li><strong>获取环境动态提示可以提升反馈准确率</strong>：表6显示，提供完整的环境动态描述通常能提高反馈质量，尤其是在小领域（如Cliff Walking, Door Key）。对于复杂领域（如HierarchyCraft），冗长的动态描述会降低小模型性能，但能持续提升大模型性能。让模型从历史上下文中推断动态，也显示出类似的趋势：大模型受益，小模型则不然。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.09790v1/x1.png" alt="反馈准确率总表"></p>
<blockquote>
<p><strong>表2</strong>：不同模型在各领域和反馈类型下的基础反馈准确率。展示了更大和具备推理能力的模型（QwQ, R1）普遍表现更优，且偏好和二元反馈相对更容易提供。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.09790v1/x1.png" alt="ICL效果表"></p>
<blockquote>
<p><strong>表3</strong>：使用上下文学习（ICL）后的反馈准确率变化。箭头表示相对于基线（表2）的增减。ICL对大模型的提升效果更明显和一致。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.09790v1/x1.png" alt="思维引导效果表"></p>
<blockquote>
<p><strong>表4</strong>：使用思维引导后的反馈准确率变化。思维引导对推理模型（QwQ, R1）有更一致的提升作用。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.09790v1/x1.png" alt="VLM效果表"></p>
<blockquote>
<p><strong>表5</strong>：视觉语言模型（VLM）的反馈准确率。在Robomimic连续控制任务中，使用图像观察的VLMs在偏好和二元反馈上表现优于LLMs。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.09790v1/x1.png" alt="动态提示效果表"></p>
<blockquote>
<p><strong>表6</strong>：在不同程度的环境动态提示下的反馈准确率。“Unk”为未知动态，“Full”为完整动态描述，“History”为历史上下文。提供动态信息（尤其是完整描述）能显著提升大模型的性能。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 首次对LLMs/VLMs在多样化的反馈类型、环境领域和推理方法下提供规划反馈的能力进行了全面、系统的评估；2) 定量揭示了反馈质量与模型规模、推理能力、反馈类型、环境复杂性以及推理方法之间的关键关系；3) 证明了语言模型反馈是无需手工设计奖励或人类演示即可从多种环境中提取学习信号的一种有前景的手段。</p>
<p>论文明确指出了其发现的局限性：反馈质量在具有复杂层次动态或连续状态/动作空间的环境中会显著下降，当前的基础模型难以在没有人类先验知识的情况下解释连续空间。</p>
<p>这些发现对后续研究有多方面启示：首先，应研究如何更有效地从基础模型中引出鲁棒的反馈，特别是在复杂和连续的环境中。其次，应探索如何高效地利用此类高质量反馈来提升策略学习的效率与效果。最后，模型规模、推理能力与不同反馈类型、推理技术的适配性关系，为在实际应用中选择和优化反馈模型提供了重要指导。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文评估大型语言模型（LLMs）和视觉语言模型（VLMs）在具身环境中提供规划反馈的能力。核心问题是探究基础模型能否为智能体行为提供准确反馈，以替代精心设计的奖励函数或高质量演示。研究评估了多种反馈类型（如二元反馈、偏好反馈、行动/目标建议）及推理方法（如上下文学习、思维链）。实验发现，基础模型能跨领域提供高质量反馈；更大、更具推理能力的模型反馈更准确、偏见更少，并能从增强的推理方法中获益更多；但在动态复杂或状态/行动空间连续的环境中，反馈质量会下降。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.09790" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>