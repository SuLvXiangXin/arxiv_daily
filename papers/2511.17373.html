<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.17373" target="_blank" rel="noreferrer">2511.17373</a></span>
        <span>作者: Hongyang Li Team</span>
        <span>日期: 2025-11-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人需要在以人为中心的环境中执行多样化任务，这要求控制器能同时融合动态敏捷性与鲁棒平衡能力。当前，基于学习的人形全身跟踪方法主要分为两个方向：一是利用人类动作捕捉数据训练策略，以实现如舞蹈、跑步等敏捷动态技能；二是专注于极端平衡姿态的维持，通常通过特定优化生成数据。然而，这两种方向相互割裂。现有方法往往只能专精于一方面：专注于敏捷性的方法在极端平衡任务上表现不佳，而专注于稳定性的方法则会限制动态运动所需的动量传递和协调性。其根本原因在于数据局限性和优化目标的冲突：人类MoCap数据中极端平衡场景占比少（长尾分布），且受限于人类生理结构；同时，为一种运动类型设计的奖励函数可能会阻碍另一种运动类型的学习。</p>
<p>本文针对“敏捷性”与“稳定性”难以在单一策略中统一的问题，提出了一个名为AMS的新框架。核心思路是利用异构数据源（人类MoCap数据与可控生成的合成平衡数据），并设计混合奖励方案与自适应学习策略，从而训练出一个能同时执行动态运动跟踪和维持极端平衡的单一策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>AMS的整体框架基于目标条件强化学习的全身跟踪流程，并引入了三个关键组件以解决数据与优化冲突。</p>
<p><img src="https://arxiv.org/html/2511.17373v2/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：AMS方法总览。(a) 标准的全身跟踪流程：将人类MoCap数据重定向为参考运动，并采用基于师生策略的强化学习。(b) 合成平衡数据生成模块，用于补充数据。(c) 自适应学习策略，包含基于个体运动性能的自适应采样和奖励塑形。(d) 混合奖励设计，包含适用于所有运动的通用奖励和仅用于合成运动的平衡先验奖励。</p>
</blockquote>
<p><strong>1. 整体流程与问题设定</strong>：任务被定义为目标条件RL问题，策略 π 需实时跟踪参考运动序列。状态 s_t 包含本体感知观测 o_t，目标 g_t 来自参考运动。动作 a_t 为期望关节位置，通过PD控制器执行。采用师生训练范式，先训练拥有特权信息的教师策略，再蒸馏至仅依赖可部署传感器的学生策略。</p>
<p><strong>2. 合成平衡运动生成</strong>：为弥补人类数据在平衡运动上的不足，本文提出直接从人形机器人运动空间采样生成物理可行的单支撑平衡运动序列。</p>
<p><img src="https://arxiv.org/html/2511.17373v2/x3.png" alt="运动空间分析"></p>
<blockquote>
<p><strong>图3</strong>：运动空间分析。(a) 人类与人形机器人的平衡运动空间仅部分重叠。(b) 人类MoCap数据因传感器噪声和运动重定向引入误差。(c) 合成的平衡数据能保证脚部接触状态、质心位置等物理真实性。(d) 一个生成的合成平衡运动示例，绿色轨迹为摆动脚轨迹。</p>
</blockquote>
<p>生成过程如算法1所示，采用两阶段批量轨迹优化。第一阶段最小化成本 J1，包含对支撑脚、摆动脚和骨盆参考轨迹的跟踪、软关节限位、静止姿态正则化和时序平滑性。第二阶段在 J1 基础上增加平衡约束项 J2，惩罚质心水平投影偏离支撑脚中心矩形区域的程度。仅当整条轨迹的最大偏差小于容忍度 ε 时，运动才被接受。这种分层方法先将运动学可行性与平衡约束分离，确保了优化的稳定收敛。</p>
<p><strong>3. 混合奖励</strong>：为了解决敏捷性与稳定性目标的冲突，设计了混合奖励方案。对于所有数据（人类MoCap和合成数据），均应用<strong>通用奖励</strong>（如关节位置、速度、根朝向跟踪），以鼓励自然运动并维持粗略平衡。<strong>平衡先验奖励</strong>（如质心对齐、脚部接触一致性）则<strong>仅应用于合成平衡数据</strong>，为其提供精确的稳定性指导，同时避免对从人类数据中学到的敏捷性产生限制。</p>
<p><strong>4. 自适应学习</strong>：包含两个组件：</p>
<ul>
<li><strong>自适应采样</strong>：根据周期性的性能评估动态调整每个运动序列的采样概率。评估基于三个维度：运动执行失败、平均关节位置误差和最大关节位置误差。性能差的运动概率提升，性能好的运动概率降低，并设有最小概率限制以防止某些运动被完全忽略。这实现了对困难样本的自动挖掘。</li>
<li><strong>自适应奖励塑形</strong>：通用奖励函数常使用 <code>r = exp(-err/σ)</code> 的形式，其中 σ 是控制误差容忍度的塑形系数。本文为不同运动、不同身体部位维护独立的 σ 参数集，并使用指数移动平均根据当前跟踪误差进行更新：<code>σ_new = (1-α)*σ_current + α*err_current</code>。这使得误差容忍度能自适应训练进度和运动多样性，提升学习效率。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在IsaacGym仿真环境中进行评估。训练数据集包含来自AMASS和LAFAN1数据集的8000多个运动序列，以及由本文方法生成的10000个合成平衡运动序列。真实机器人部署在Unitree G1人形机器人上。评估指标包括成功率、全局MPJPE、根相对MPJPE、接触失配率和支撑脚滑移速度。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li>OmniH2O：通用的全身运动跟踪框架。</li>
<li>HuB：专注于极端平衡运动的框架。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.17373v2/x4.png" alt="仿真性能对比"></p>
<blockquote>
<p><strong>图4</strong>：表I，仿真性能对比与消融实验。我们的方法在敏捷运动和平衡运动上均实现了更低的跟踪误差和更高的成功率，展现了强大的泛化性与鲁棒性。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>主实验（表I-a）</strong>：在全部数据上，AMS的全局MPJPE为54.06 mm，根相对MPJPE为29.02 mm，成功率99.78%，均优于OmniH2O（69.84 mm, 44.18 mm, 98.93%）和HuB（151.26 mm, 98.42 mm, 77.23%）。特别是在合成平衡数据上，AMS的接触失配率（0.12%）远低于OmniH2O（0.24%）和HuB（0.10%），表明其平衡稳定性更优。</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>合成平衡数据（表I-b）</strong>：移除合成数据后，在合成平衡任务上的性能大幅下降（全局MPJPE从64.03升至112.20，成功率从99.95%降至94.54%），验证了其必要性。</li>
<li><strong>混合奖励（表I-c）</strong>：仅使用通用奖励或对所有数据使用全部奖励，都会导致在平衡指标（如接触失配）或敏捷指标上性能下降，证明了混合设计的有效性。</li>
<li><strong>自适应学习（表I-d）</strong>：移除自适应采样或自适应奖励塑形都会导致跟踪误差显著增加，尤其是全局MPJPE。两者共同移除时性能下降最严重。</li>
</ul>
</li>
<li><strong>分布外泛化（表II）</strong>：在未见过的运动序列上，AMS（全局MPJPE 63.48 mm，成功率99.7%）显著优于不使用合成数据的AMS变体（86.61 mm，96.0%）和使用了全部数据的OmniH2O（76.26 mm，99.1%），展示了更好的泛化能力。</li>
<li><strong>真实机器人部署</strong>：如图1所示，单一AMS策略能在真实G1机器人上零样本执行“叶问蹲”等极端平衡动作，同时也能完成舞蹈、跑步等动态技能，并支持实时遥操作。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个在单一策略中成功统一动态运动跟踪与极端平衡维持的框架AMS。</li>
<li>提出了利用异构数据（人类MoCap+合成平衡数据）的训练范式，并设计了配套的混合奖励方案和自适应学习策略（自适应采样与奖励塑形），以解决数据与优化目标冲突。</li>
<li>在仿真和真实人形机器人上验证了单一策略的卓越性能与泛化能力，为通用人形控制提供了一个有潜力的基础模型。</li>
</ol>
<p><strong>局限性</strong>：论文提到，合成平衡数据的生成依赖于机器人模型和物理假设，可能对不精确的硬件建模敏感。</p>
<p><strong>后续启示</strong>：</p>
<ol>
<li><strong>异构数据策略</strong>：为克服特定领域数据不足，主动生成互补的、可控的合成数据是一个有效途径，可推广至其他机器人技能学习。</li>
<li><strong>目标解耦的奖励设计</strong>：混合奖励的思想表明，对于存在内在冲突的多目标学习，将特定目标的奖励谨慎地、有条件地应用于可控数据子集，可能比全局应用更有效。</li>
<li><strong>性能驱动的自适应训练框架</strong>：自适应采样与奖励塑形构成了一个根据学习进度动态调整训练分布和难度的高效学习框架，对处理复杂、不平衡的数据集具有通用参考价值。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出AMS框架，解决人形机器人控制器难以统一敏捷动态运动与稳定平衡的核心问题。方法利用异构数据源（人类运动捕捉和合成平衡运动），通过混合奖励方案协调优化目标，并采用自适应学习策略高效训练。实验在仿真和真实Unitree G1机器人上验证，单个策略能同时执行跳舞、跑步等敏捷技能，以及零样本的Ip Man's Squat等极端平衡运动，展示了多功能控制能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.17373" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>