<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.25122" target="_blank" rel="noreferrer">2510.25122</a></span>
        <span>作者: Jinghui Lu Team</span>
        <span>日期: 2025-10-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过整合视觉语言模型和动作解码器，已成为机器人操作领域的一种变革性范式，能够利用网络规模的多模态预训练实现跨任务的快速适应。然而，这类模型（如OpenVLA、π₀）通常参数量巨大（数B到数十B），计算需求高，导致其在移动机器人或嵌入式系统等资源受限的边缘设备上部署面临挑战，主要存在推理速度慢、计算密集、长时域行为不稳定以及固定容量主干网络与任务难度不匹配等问题。本文针对VLA模型在边缘部署的“效率-性能”权衡这一具体痛点，提出不应仅仅缩小模型参数，而应重新设计推理过程的新视角。本文核心思路是提出NanoVLA，一种通过解耦视觉语言融合、采用长短动作分块规划以及动态路由选择主干网络的高效轻量级VLA框架，旨在以极少的资源消耗实现高性能的机器人策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>NanoVLA的整体框架围绕三个核心设计原则构建，旨在仅在关键处进行计算：1) 解耦静态指令与动态视觉特征的处理以实现缓存；2) 进行长时域规划但短时域执行以平衡平滑性与响应性；3) 根据任务复杂度自适应分配轻量级或重量级主干网络。</p>
<p><img src="https://arxiv.org/html/2510.25122v1/x1.png" alt="解耦融合策略"></p>
<blockquote>
<p><strong>图1</strong>：解耦融合实现高效VLA策略。NanoVLA的解耦融合策略（中）在参数受限的设置下延迟了视觉到语言的融合。这种方法以更少的开销和延迟实现了更好的性能，最终形成了NanoVLA（右），其在仿真和现实任务中仅使用约OpenVLA 2%的参数即实现了更优的性能。</p>
</blockquote>
<p><strong>1. 视觉语言解耦与缓存</strong><br>现有VLA模型通常通过密集的交叉注意力早期融合视觉和语言模态，迫使语言主干在每个控制步骤重新计算，即使指令保持不变。NanoVLA采用解耦架构，将视觉和语言输入分别通过独立的编码器（如ResNet/ViT和BERT/Qwen）进行处理，并保持这些预训练编码器冻结以保留语义知识。多模态特征（包括本体感知）在动作生成阶段，通过一个轻量级Transformer进行晚期融合。该Transformer层先进行自注意力捕获模态内依赖，再进行一次交叉注意力融合视觉和语言特征以生成动作。</p>
<p><img src="https://arxiv.org/html/2510.25122v1/x2.png" alt="NanoVLA框架总览"></p>
<blockquote>
<p><strong>图2</strong>：NanoVLA框架概述。多模态输入被独立处理，并在后期通过一个轻量级注意力层进行融合。这种设计绕过了VLM中计算密集的早期融合，实现了缓存和加速推理等关键优势。</p>
</blockquote>
<p>这种晚期融合设计的关键优势在于支持缓存。在交互式机器人场景中，静态的任务指令嵌入只需计算一次并跨时间步缓存复用，而动态的视觉嵌入则每帧更新。这消除了冗余计算，显著降低了设备端延迟。</p>
<p><strong>2. 长短动作分块</strong><br>传统固定长度的动作分块策略要么导致行为僵硬（块太长），要么导致行为抖动（块太短）。NanoVLA提出了长短动作分块策略以平衡时序连贯性和响应性。在训练时，策略被优化来预测一个长序列的动作（H_train步），训练目标是对整个分块的监督回归损失（如ℓ1或ℓ2损失）。在推理时，机器人只执行每个预测分块的前h（h ≪ H_train）步动作，然后使用最新的观测重新规划。这种“长规划、短执行”的模式摊销了昂贵的长时域规划成本，同时通过频繁的重新规划保持了对新感官输入的适应性。</p>
<p><strong>3. 动态路由</strong><br>为了解决固定主干网络与任务难度不匹配的问题，NanoVLA引入了一个VLA路由器，在推理时根据任务复杂度在候选语言模型（如轻量级和重量级）之间进行选择。路由决策基于对模型在给定任务上成功概率的贝叶斯估计。具体而言，为每个模型-任务对维护一个Beta-Binomial后验分布，以建模成功概率的不确定性。通过计算任意两个模型在任务上的 pairwise win probability（模型i优于模型j的概率），并以此作为软监督目标，训练一个文本条件的二元分类器。在推理时，路由器默认选择轻量级模型，仅当预测的重量级模型对当前任务的胜率超过阈值τ时才进行切换，从而在简单任务上节省计算，在复杂任务上保证性能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟基准LIBERO和真实机器人平台（LeRobot，搭载Jetson Orin Nano）上进行。对比的基线方法包括Octo-Base、OpenVLA、π₀、TraceVLA、SpatialVLA和SmolVLA。实现了三个版本的NanoVLA：使用BERT-base的NanoVLA-S (161M总参)、使用Qwen2.5 0.5B的NanoVLA-L (520M总参)以及集成路由器的NanoVLA-R。</p>
<p>在LIBERO模拟基准（Spatial, Object, Goal, Long四个套件）上，NanoVLA-L取得了80.4%的平均成功率，优于OpenVLA (76.5%)、π₀ (71.8%)等多B参数模型。性能最强的NanoVLA-R达到了84.1%的平均成功率，同时总参数量（平均296M）远低于基线。在更具挑战性的LIBERO-90基准上，NanoVLA-L取得了83.3%的成功率，显著高于SmolVLA (68.9%)和OpenVLA (62.0%)。</p>
<p>在12个真实世界LeRobot操作任务中，NanoVLA模型在包括简单拾放、可变形物体操作和精确操作（开/关盖子）在内的多种任务上 consistently 超越基线。例如，在“拾放香蕉”任务中，NanoVLA-S达到94%成功率，远超OpenVLA的86%；在“关盖子”任务中，NanoVLA-L达到68%成功率，优于OpenVLA的54%。平均来看，NanoVLA-L和NanoVLA-R均达到85.6%的平均成功率，显著高于OpenVLA的80.4%。</p>
<p><img src="https://arxiv.org/html/2510.25122v1/x4.png" alt="推理性能分析"></p>
<blockquote>
<p><strong>图4</strong>：（a）在Jetson Orin Nano上的推理性能分析。NanoVLA相比OpenVLA实现了52倍FPS提升和13.8%成功率提升；相比SmolVLA，在匹配动作步数时能提供更高的FPS和成功率。</p>
</blockquote>
<p>在Jetson Orin Nano上的延迟分析表明，NanoVLA相比OpenVLA实现了高达52倍的FPS提升，同时成功率提升13.8%。即使与紧凑型基线SmolVLA相比，在匹配动作步数时，NanoVLA也能提供43.8%的FPS提升和3.2%的成功率优势。</p>
<p>消融实验证实了各核心组件的有效性：1) 长短动作分块相比固定分块能在更宽的动作步数范围内维持高成功率平台，提供了更优的成功率-吞吐量权衡。2) 在环境变异（如日夜光照变化）测试中，NanoVLA相比基线有8.5%到42.1%的平均性能提升，显示了解耦架构的鲁棒性。3) LLM缓存机制能显著降低每次推理的延迟。</p>
<p><img src="https://arxiv.org/html/2510.25122v1/x5.png" alt="长短动作分块与缓存有效性消融"></p>
<blockquote>
<p><strong>图5</strong>：（a）环境变异影响消融。（b）LLM缓存有效性消融。结果显示解耦架构对环境变化更具鲁棒性，且缓存能大幅降低延迟。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：第一，提出了一个集成了视觉语言解耦、长短动作分块和动态路由的轻量级VLA框架NanoVLA，首次将这些边缘导向的设计整合为一体。第二，通过广泛的实验证明，该框架在参数量大幅减少（最多减少98%）的同时，在模拟和真实任务上达到或超越了先进VLA模型的性能，并在边缘设备上实现了极低的推理延迟（最高52倍加速）。第三，系统的消融研究验证了解耦策略保持了跨任务可迁移性，路由模块优化了成本-性能权衡。</p>
<p>论文自身提到的局限性包括：NanoVLA-S（使用BERT-base）在LIBERO-90这类大规模指令跟随任务上表现较弱，揭示了仅编码器语言模型在处理复杂语言理解时的不足。此外，动态路由机制依赖于历史任务的成功率数据来估计新任务的胜率。</p>
<p>这项工作对后续研究的启示是：为资源受限平台设计通用机器人策略时，应超越简单的模型缩小，转而从推理流程、模态交互和计算资源动态分配的系统层面进行重新思考。解耦融合、分步执行与自适应计算分配的结合，为未来高效、鲁棒的边缘人工智能系统提供了有前景的设计范式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对资源受限的边缘设备部署视觉-语言-动作模型时计算开销高、延迟大的问题，提出轻量级架构NanoVLA。其核心技术包括：视觉-语言解耦（将早期融合推迟至后期，降低推理开销）、长短期动作分块（保证多步规划的连贯性）以及动态路由（根据任务复杂度自适应选择骨干网络）。实验表明，该模型在边缘设备上推理速度提升达52倍，参数减少98%，同时保持或超越了原有模型的任务精度与泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.25122" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>