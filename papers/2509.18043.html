<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Prepare Before You Act: Learning From Humans to Rearrange Initial States - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Prepare Before You Act: Learning From Humans to Rearrange Initial States</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.18043" target="_blank" rel="noreferrer">2509.18043</a></span>
        <span>作者: Dylan P. Losey Team</span>
        <span>日期: 2025-09-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前模仿学习（IL）方法，包括扩散策略、流匹配方法以及视觉-语言-动作（VLA）模型，在复杂操作任务上取得了显著进展。然而，这些方法在面对训练数据分布之外的初始状态时，例如目标物体位于未见过的位置或被其他物体遮挡时，泛化能力仍然不足。一种常见的解决方案是收集更大规模和更多样化的训练数据，但这成本高昂且效率低下。</p>
<p>本文针对模仿学习策略在分布外初始状态下泛化能力弱、数据效率低的关键痛点，提出了一个受人类行为启发的新视角：当人类遇到不便或意外的初始状态时，往往会先对环境进行整理和简化，然后再执行任务。本文的核心思路是设计一个两阶段的策略：首先学习一个简化策略来重构环境，将多样的初始状态带入一个更熟悉、更易处理的分布（锚状态），然后再从这个简化后的状态执行默认的任务策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>ReSET算法的整体目标是将一个困难的初始状态 <code>S0</code> 通过一个简化策略 <code>π‘</code> 转换为一个更易于处理的锚状态 <code>Sa</code>，然后从这个锚状态执行默认的任务策略 <code>π</code> 以达成目标 <code>S^t</code>。其核心在于学习这个简化策略，该策略通过整合动作无关的人类视频和任务无关的机器人遥操作数据来实现。</p>
<p><img src="https://arxiv.org/html/2509.18043v1/Figures/method.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ReSET的网络架构与执行流程。<strong>左图</strong>：模型包含三个关键组件：(a) 基于人类视频训练的评分网络 <code>f</code>，用于评估给定初始配置下基础策略成功的可能性；(b) 流生成网络 <code>g</code>，预测编码了人类直觉场景重构方式的点流；(c) 简化策略 <code>π’</code>，将预测的点流 <code>T</code> 映射为可执行的机器人动作基元 <code>A</code>。<strong>右图</strong>：在部署时，评分网络评估当前观察以决定是否执行基础策略。若否，则流生成网络产生一个流计划，简化策略执行该计划。之后评分网络重新评估更新后的场景，再决定是否继续执行基础策略。</p>
</blockquote>
<p><strong>核心模块一：评分网络</strong><br>该网络的作用是判断当前状态是否已经是适合执行基础策略的锚状态。它仅使用人类动作视频进行训练。给定一段人类视频，为每一帧分配一个单调递减的“场景分数” <code>C~t</code>（例如使用二次函数 <code>α - (t/β)^2</code>），假设视频中人类在将环境重构为更简单的状态。评分网络 <code>f</code> 学习从观测 <code>o</code> 预测这个分数，损失函数为预测分数与人工分配分数之间的均方误差。在部署时，设定一个阈值 <code>C^</code>，当预测分数低于此阈值时，则切换到执行基础策略。</p>
<p><strong>核心模块二：流生成网络</strong><br>该网络的作用是规划如何重构环境。它从人类视频中学习，预测人类可能采取的对象移动方式，并以“点流”的形式输出。具体而言，使用现成的点跟踪基础模型（如CoTracker3）从人类视频中提取被操纵物体的轨迹点流 <code>T</code>。网络 <code>g</code> 以初始场景观测 <code>o0</code> 为输入，预测一个代表最可能物体移动的点流。网络采用时空Transformer架构，输入图像通过预训练的DINOv2编码器转为特征，并加入可学习的时间编码来捕获点流的时空依赖关系。</p>
<p><img src="https://arxiv.org/html/2509.18043v1/Figures/data.png" alt="数据预处理"></p>
<blockquote>
<p><strong>图3</strong>：数据预处理流程。<strong>左图</strong>：通过追踪物体运动的点流 <code>T</code> 来指导环境重构。<strong>右图</strong>：对于机器人视频，通过识别位移较大的点来定位物体。同时记录相关动作基元的参数，如机器人末端执行器的起始和结束位置以及旋转。</p>
</blockquote>
<p><strong>核心模块三：任务无关的基于流简化策略</strong><br>该策略的作用是将流生成网络提出的抽象规划转换为机器人可执行的具体动作。它使用任务无关的机器人遥操作数据（如机器人摆弄物体的视频）进行训练。由于直接从点流预测精细的低级动作具有挑战性，该策略输出预定义动作基元的参数。这些基元分为三类：(i) 抓放，(ii) 推拉，(iii) 旋转。动作基元空间表示为 <code>A = {(c, p)}</code>，其中 <code>c</code> 是基元类别，<code>p</code> 是连续参数（如抓取和放置的坐标）。简化策略 <code>π’</code> 以预测的点流 <code>T</code> 和机器人初始观测 <code>Or0</code> 为输入，预测动作基元 <code>A</code>。训练时使用结合了分类损失（针对基元类型）和回归损失（针对参数）的复合损失函数。</p>
<p><strong>创新点</strong><br>与现有方法（如直接扩大数据集、数据增强或基于模型的重构方法）相比，ReSET的核心创新在于：1) 受人类行为启发的两阶段范式（先简化环境再执行任务），并提供了理论分析证明其能降低泛化误差上界；2) 利用动作无关的人类视频作为学习“如何简化”的知识来源，通过点流这一抽象表示桥接人与机器人的具身差距；3) 设计了包含评分、规划和执行三个模块的完整框架，能够自主决定何时简化、如何简化并执行简化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在四个现实世界的长视野操作任务上评估ReSET：1) 抓取被盒子遮挡的杯子，2) 将勺子放入被遮挡的杯子，3) 在杂乱环境中打开微波炉门，4) 将锅放在被遮挡的炉灶上。实验平台为Franka Emika Panda机械臂。<br><strong>对比基线</strong>：包括扩散策略（Diffusion Policy）、视觉-语言-动作模型（VLA，具体为RT-2）、对象中心恢复（OCR）和动态增强扩散策略（Dynamics-DP）。所有方法使用相同总量的训练数据（包括任务演示和辅助数据）。<br><strong>关键实验结果</strong>：在仅使用5个任务演示的少样本设置下，ReSET在所有四个任务上的平均成功率显著高于基线方法。</p>
<p><img src="https://arxiv.org/html/2509.18043v1/Figures/success.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：在四个现实世界任务上，使用5个任务演示时，ReSET与各基线方法的成功率对比。ReSET在所有任务上均取得了最高成功率，平均成功率超过80%，显著优于次优的扩散策略（约65%）和VLA模型（约55%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.18043v1/x1.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：ReSET在四个真实任务上的定性评估。对于每个任务，第一张图展示了随机初始化的场景配置，第二张图显示了经过简化策略后达到的一个锚状态，第三张图显示了随后基础策略的执行结果。可见ReSET能够有效移除障碍、调整物体姿态，为后续任务执行创造有利条件。</p>
</blockquote>
<p><strong>消融实验总结</strong>：论文对ReSET的三个核心组件进行了消融研究。结果显示：1) <strong>移除评分网络</strong>（总是先执行简化策略）会导致在已经简单的状态上执行不必要的、可能产生干扰的简化动作，降低最终成功率。2) <strong>移除流生成网络</strong>（用随机扰动代替）或<strong>使用低级动作代替动作基元</strong>，都会导致简化效率低下，无法可靠地到达锚状态。3) <strong>完整ReSET框架</strong>实现了最佳性能，证明了每个组件对于有效和高效的环境重构都是必要的。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ReSET算法，赋予机器人“先准备后行动”的能力，通过一个简化策略先将分布外的初始状态重构为更易处理的锚状态，再执行任务策略。</li>
<li>从理论上分析了使用锚状态可以降低策略泛化误差的上界，为该方法提供了理论依据。</li>
<li>设计了一个结合动作无关人类视频和任务无关机器人数据的实用学习框架，包含评分、流生成和简化策略三个模块，能够有效学习并实施环境重构。</li>
</ol>
<p><strong>局限性</strong>：论文提到，ReSET的泛化能力可能受到其所依赖的基础模型（如用于点跟踪的CoTracker3和用于特征提取的DINOv2）的限制。此外，方法目前主要处理几何重构（移动、旋转物体），对于更复杂的准备工作（如解开缠绕）可能需要进行扩展。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>范式方面</strong>：“先简化环境再执行任务”的框架为解决分布外泛化问题提供了一个新颖且有效的思路，可扩展到更广泛的操作任务和场景。</li>
<li><strong>知识来源</strong>：利用人类视频（尤其是互联网规模的非结构化视频）作为学习“常识性”环境整理策略的知识库，是一条极具潜力的途径，可以减少对昂贵机器人演示数据的依赖。</li>
<li><strong>表示学习</strong>：点流作为一种连接人类意图与机器人动作的中间表示被证明是有效的，未来可以探索其他更丰富或更高效的跨模态表示方法。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习策略在遇到分布外初始状态（如目标被遮挡）时泛化能力差的问题，提出ReSET算法。该方法核心是让机器人像人类一样“先准备后执行”：通过结合动作无关的人类视频和任务无关的遥操作数据，学习一个简化策略，先自主调整物体位姿（如移开障碍物），使场景落入任务策略的熟悉分布，再执行原任务。实验表明，在相同训练数据量下，使用ReSET进行环境准备能实现比扩散策略等基线更鲁棒的任务执行。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.18043" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>