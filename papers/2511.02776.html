<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.02776" target="_blank" rel="noreferrer">2511.02776</a></span>
        <span>作者: Fan, Shichao, Wu, Kun, Che, Zhengping, Wang, Xinhua, Wu, Di, Liao, Fei, Liu, Ning, Zhang, Yixue, Zhao, Zhen, Xu, Zhiyuan, Li, Meng, Liu, Qingjie, Zhang, Shanghang, Wan, Min, Tang, Jian</span>
        <span>日期: 2025/11/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于大规模机器人数据集和视觉语言模型（VLMs）的进展推动了视觉-语言-动作（VLA）模型的研究。然而，现有VLA模型面临两大核心挑战：（i）从高维观察中生成精确的低层动作困难，尤其在灵巧或接触丰富的任务中，厘米级误差都可能导致失败；（ii）跨异构数据源（包括不同机器人具身和人类演示视频）存在领域鸿沟。现有方法通常分别从视觉动态或机器人动作中编码潜在变量来指导策略学习，但未能充分利用大规模异构数据集中存在的互补多模态知识。本文针对这些痛点，提出了一种新颖的视角：受人类超模态认知启发，应超越单模态抽象，转向能够联合编码视觉动态和运动控制的多模态对齐。本文核心思路是引入一个名为统一视觉-运动编码（UVMC）的离散潜在表示，通过一个双分支VQ-VAE学习获得，并设计了一个三阶段训练范式（自监督UVMC学习、UVMC指导的预训练、任务特定后训练）来构建一个通用且可扩展的VLA模型XR-1。</p>
<h2 id="方法详解">方法详解</h2>
<p>XR-1框架旨在构建一个通用、可扩展的VLA模型，其核心是学习一个联合编码视觉动态和机器人运动的统一表示，并采用三阶段训练范式。</p>
<p><img src="https://arxiv.org/html/2511.02776v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：XR-1方法整体框架。采用三阶段训练流程：第一阶段，通过双分支VQ-VAE自监督学习统一视觉-运动编码（UVMC）；第二阶段，利用UVMC作为监督信号，在大规模跨具身机器人数据集上进行策略预训练；第三阶段，在特定任务数据上进行后训练以适应目标机器人和任务。</p>
</blockquote>
<p><strong>第一阶段：学习统一视觉-运动编码（UVMC）</strong><br>UVMC通过一个双分支向量量化变分自编码器（VQ-VAE）以自监督方式学习。与以往仅关注视觉动态或动作序列的方法不同，该设计通过一个共享的离散码本和跨模态对齐损失，显式地将两种模态统一起来。</p>
<ul>
<li><strong>视觉动态编码</strong>：给定时间步<code>t</code>和<code>t+h</code>的两帧图像<code>c_t</code>和<code>c_{t+h}</code>，视觉编码器<code>E_vis</code>提取潜在变量<code>z_vis</code>，该变量编码了<code>h</code>步内的视觉变换。一个非对称解码器<code>D_vis</code>以当前帧<code>c_t</code>和<code>z_vis</code>为输入，预测未来帧<code>c_{t+h}</code>。重建损失<code>L_vis</code>包含L1重建误差、码本量化损失和承诺损失。</li>
<li><strong>机器人运动编码</strong>：给定相同的<code>h</code>步间隔，运动编码器<code>E_mo</code>以动作序列<code>a_{t:t+h}</code>和对应的本体感受状态<code>m_{t:t+h}</code>为输入，提取潜在变量<code>z_mo</code>。运动解码器<code>D_mo</code>以<code>z_mo</code>、语言指令<code>l</code>和观察<code>o</code>为条件，重建低层动作序列<code>a_{t:t+h}</code>。重建损失<code>L_mo</code>结构与<code>L_vis</code>类似。此分支特意排除了原始图像和语言指令，以确保专注于捕捉运动动力学。</li>
<li><strong>统一与跨模态对齐</strong>：视觉和运动编码器的输出通过一个共享的可学习码本进行量化，得到离散码<code>z_vis^e</code>和<code>z_mo^e</code>，从而将两种模态映射到同一潜在空间。为解决视觉编码可能包含与任务无关信息（如背景）的问题，引入了一个跨模态对齐正则化项<code>L_align = D_KL(q(z_mo) || q(z_vis))</code>，该KL散度损失鼓励视觉潜在分布向其对应的运动潜在分布对齐，从而将感知信息锚定在物理上有意义的控制信号上。对于机器人演示数据，总损失为<code>L_total^robot = L_vis + L_mo + L_align</code>；对于无动作标签的人类演示视频，损失简化为<code>L_total^human = L_vis</code>。最终，将运动码<code>z_mo^e</code>和视觉动态码<code>z_vis^e</code>拼接，得到用于后续策略学习的统一视觉-运动编码<code>z_uvmc^e</code>。</li>
</ul>
<p><strong>第二阶段：UVMC指导的通用策略预训练</strong><br>获得UVMC后，将其整合到策略学习中以提升低层动作预测。采用常见的VLA架构，包括VLM主干<code>F(·)</code>和动作预测头<code>H(·)</code>。在VLM输入中引入可学习的token <code>t</code>，使VLM能够预测UVMC，训练损失为<code>L_uvmc = ||F(l, o, t) - z_uvmc||_2^2</code>。同时，动作头在带有动作标签的机器人数据集上进行预训练，损失为<code>L_act</code>。最终联合目标为<code>L = L_uvmc + L_act</code>，这种联合训练鼓励VLM内化结构化的视觉-运动知识。</p>
<p><strong>第三阶段：面向部署的后训练</strong><br>在大规模UVMC指导的预训练后，模型获得了提取统一视觉-运动知识和生成基础动作的强大能力。为在特定下游控制任务上进一步提升性能，引入后训练阶段，使用任务特定数据集和动作损失<code>L_act</code>对VLA策略进行微调。</p>
<p><strong>创新点</strong>：与现有单模态潜在表示学习方法相比，XR-1的核心创新在于提出了<strong>联合编码视觉动态和机器人运动的统一表示（UVMC）</strong>，并通过<strong>跨模态对齐损失</strong>强制两种模态在共享离散潜在空间中对齐。这使得模型能够从异构数据源（如无动作的人类视频和有动作的机器人数据）中学习互补知识，并建立观察与执行之间的因果联系。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实世界环境中进行了大规模评估，涵盖<strong>六个不同的机器人具身</strong>（Tien Kung 1.0/2.0、单/双UR-5e、双Franka、AgileX Cobot Magic 2.0），涉及<strong>超过120个不同的操作任务</strong>，累计执行了<strong>超过14,000次 rollout</strong>。预训练数据集整合了Open-X、RoboMIND、Ego4D（第一人称人类活动视频）和自有的XR-D数据集。</p>
<p><strong>对比方法</strong>：与多个先进的基线方法进行了对比，包括<code>π_0.5</code>、<code>π_0</code>、RDT、UniVLA和GR00T-N1.5。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2511.02776v1/x3.png" alt="总体结果"></p>
<blockquote>
<p><strong>图3</strong>：在六个不同机器人具身上超过120个任务的平均成功率对比。XR-1在所有具身上均超越了所有基线方法，特别是在双灵巧操作任务中优势明显（如UR双臂73.1% vs GR00T-N1.5 58.5%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.02776v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融实验结果。(a) 移除UVMC指导（w/o UVMC）或跨模态对齐损失（w/o Align）均导致性能显著下降，证明了这两个组件的必要性。(b) 使用不同数据组合进行UVMC学习的效果，显示结合机器人数据和人类视频（Robot+Human）能获得最佳性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.02776v1/x5.png" alt="泛化能力"></p>
<blockquote>
<p><strong>图5</strong>：在未见过的物体、背景、干扰物和光照变化下的泛化能力评估。XR-1在所有这些泛化设置下均展现出比基线<code>π_0</code>更强的鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.02776v1/x6.png" alt="效率对比"></p>
<blockquote>
<p><strong>图6</strong>：XR-1-Light（基于SwitchVLA的高效版本）与基线模型在性能和效率（参数量、FLOPS）上的对比。XR-1-Light在显著提升成功率的同时，保持了较高的计算效率。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验明确验证了UVMC和跨模态对齐损失的关键作用。移除UVMC指导（<code>L_uvmc</code>）会导致性能大幅下降（例如，在TienKung 2.0上从81.0%降至67.9%），移除对齐损失（<code>L_align</code>）同样损害性能。此外，在UVMC学习阶段同时使用机器人数据和人类视频，比仅使用单一数据源能带来更优的性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>XR-1</strong>，一个可扩展的三阶段VLA学习框架，能有效利用包括网络规模人类视频和多样化机器人数据集在内的异构数据源，并可无缝集成到不同的VLA架构中。</li>
<li>引入了<strong>统一视觉-运动编码（UVMC）</strong>，一种能同时编码环境动态和机器人运动的离散潜在表示，并通过跨模态对齐损失强制实现跨具身的一致多模态嵌入。</li>
<li>在六个机器人具身、123个任务上进行了超过14,000次真实世界实验验证，证明XR-1 consistently outperforms state-of-the-art baselines such as π 0.5 \pi_{0.5} , π 0 \pi_{0} , RDT, UniVLA, and GR00T-N1.5 consistently outperforms state-of-the-art baselines such as π 0.5 \pi_{0.5} , π 0 \pi_{0} , RDT, UniVLA, and GR00T-N1.5。</li>
</ol>
<p><strong>局限性</strong>：论文提到其框架是模型无关的，但未深入讨论计算资源需求。三阶段训练，特别是大规模预训练，可能对算力和数据存储有较高要求。此外，模型性能仍依赖于预训练和后训练数据的质量和多样性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>统一表示学习的方向</strong>：XR-1验证了联合学习视觉与运动统一表示的有效性，这为后续构建更通用、数据利用效率更高的具身智能模型提供了一个有前景的方向。</li>
<li><strong>跨模态对齐的泛化</strong>：所提出的跨模态对齐机制（将视觉感知锚定在运动信号上）可以启发其他需要融合异构信号的研究领域。</li>
<li><strong>数据利用与效率</strong>：该方法展示了一种有效整合无标签人类视频和有标签机器人数据的途径，未来研究可探索如何进一步减少对大规模有标签机器人数据的依赖，或开发更高效的多模态表示学习架构。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作（VLA）模型难以从高维观测生成精确低级动作、且存在异构数据源（如多样机器人体现和人类演示）领域差距的核心问题，提出XR-1框架。其关键技术是统一视觉-运动代码（UVMC），通过双分支VQ-VAE学习视觉动态与机器人运动的联合离散表示，并采用自监督学习、UVMC引导预训练和任务特定后训练的三阶段范式。实验在六个机器人体现、120多个操作任务上验证，XR-1 consistently outperforms state-of-the-art baselines（如π0.5、π0、RDT等），并展示了对新物体、背景变化、干扰物和光照变化的强泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.02776" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>