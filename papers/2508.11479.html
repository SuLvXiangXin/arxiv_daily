<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.11479" target="_blank" rel="noreferrer">2508.11479</a></span>
        <span>作者: Zemskova, Tatiana, Staroverov, Aleksei, Yudin, Dmitry, Panov, Aleksandr</span>
        <span>日期: 2025/08/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>开放词汇目标导航（OVON）要求智能体根据自由形式的语言描述导航至目标物体，包括训练中未见过的类别。现有方法主要分为两类：模块化方法和端到端方法。模块化方法依赖显式建图和预训练的大规模视觉语言模型，虽然性能强大，但对建图质量和机器人定位精度敏感，误差会累积传播。端到端无图方法直接从原始传感器输入映射到动作，能减少误差累积且对硬件要求低，但面临两个关键局限性：1）训练流程脆弱，通常采用先模仿学习（如DAgger）后强化学习（PPO）的两阶段策略，其切换点需手动启发式设定，容易因学习信号冲突导致性能骤降；2）数据瓶颈，即使最大的模拟器数据集（如HM3DSem）也仅有216个标注场景，远小于预训练基础模型所需数据量，导致策略难以泛化到新物体类别。</p>
<p>本文针对端到端方法训练不稳定和泛化能力不足的痛点，提出了一种融合显式语义线索与自适应训练机制的新视角。核心思路是：通过向策略的观测空间引入目标物体的二值分割掩码作为空间提示，并设计一个基于策略熵的自适应损失调制器，动态平衡模仿与强化学习信号，从而实现稳定、高效的单阶段训练，并提升对未见类别的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>OVSegDT是一个轻量级的Transformer策略模型，其整体框架接收多模态观测输入，并输出导航动作。输入观测 (o_t) 在每一时间步包含四部分：当前RGB帧 (I_t)、自然语言目标描述 (g)、上一时间步动作 (a_{t-1}) 以及当前帧中目标类别的二值分割掩码 (M_t)。动作空间包含停止、前进、左转、右转、仰视、俯视六个高层指令。</p>
<p><img src="https://arxiv.org/html/2508.11479v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：OVSegDT模型架构。模型编码当前视觉观测 (I_t)、目标物体类别 (G)、上一动作 (a_{t-1}) 和目标二值掩码 (M_t)，形成观测嵌入 (o_t)。一个Transformer接收最近100步的观测嵌入序列。动作头用于采样动作 (a_t)。训练时，分割头（DCGAN）生成目标掩码 (m_t) 用于计算辅助分割损失，评论家头用于预测当前状态价值 (v_t)。</p>
</blockquote>
<p>核心模块包括语义分支和熵自适应损失调制（EALM）。语义分支负责处理目标掩码：使用一个轻量级ResNet9编码器将二值掩码 (M_t) 编码为128维向量 (m_t)。同时，使用冻结的SigLIP编码器分别处理RGB图像和文本目标，得到视觉嵌入 (i_t) 和文本嵌入 (g_t)。上一动作通过可学习的嵌入层映射为 (p_t)。这些嵌入被拼接为观测嵌入 (o_t)，连同历史序列（最近100步）一起输入一个解码器架构的Transformer（4层，8头，隐藏层512维）。Transformer的输出特征被送入三个头：1）<strong>动作头</strong>（线性层），预测动作分布；2）<strong>评论家头</strong>（线性层），预测状态价值；3）<strong>分割头</strong>（DCGAN），用于辅助训练，重构当前目标掩码。</p>
<p>创新点一在于引入了<strong>目标掩码编码器</strong>和<strong>辅助语义分割损失</strong>。在训练时，分割头尝试从Transformer的特征中重构输入的目标掩码 (M_t)，损失函数为Dice损失与二元交叉熵损失之和：(\mathcal{L}<em>{\text{seg}}(\theta)=\mathcal{L}</em>{\text{Dice}}(\theta)+\mathcal{L}_{\text{CE}}(\theta))。这迫使模型从拼接的观测向量中解耦出用于场景理解（RGB）和目标表征（掩码）的组件，从而加速收敛并提升导航质量。</p>
<p><img src="https://arxiv.org/html/2508.11479v1/x3.png" alt="熵自适应损失调制"></p>
<blockquote>
<p><strong>图3</strong>：熵自适应损失调制（EALM）示意图。它基于策略熵的指数移动平均（EMA）动态计算混合系数 (\lambda_t)，用于平衡模仿学习（BC）和强化学习（PPO）损失。</p>
</blockquote>
<p>创新点二是<strong>熵自适应损失调制（EALM）</strong>，它取代了手动的两阶段训练切换。EALM的核心是动态计算一个混合系数 (\lambda_t)。首先，计算每个小批量中策略动作分布的香农熵 (H_t)，并维护其指数移动平均值 (\hat{H}<em>t)。然后，根据预设的熵边界 (H</em>{\text{low}}=0.35) 和 (H_{\text{high}}=0.75)，通过线性映射和裁剪得到系数：(\lambda_t = \mathrm{clip}!\left(\frac{H_{\text{high}}-\hat{H}<em>t}{H</em>{\text{high}}-H_{\text{low}}},,0,,1\right))。当策略置信度高（熵低）时，(\lambda_t \approx 1)，侧重PPO损失；当策略不确定（熵高）时，(\lambda_t \approx 0)，侧重模仿学习（BC）损失。最终的策略损失为两者加权和：(\mathcal{L}<em>{\text{EALM}}(\theta) = \lambda_t \mathcal{L}</em>{\text{PPO}}(\theta) + (1-\lambda_t) \mathcal{L}_{\text{BC}}(\theta))。这实现了从模仿学习到强化学习的平滑、自动过渡。</p>
<p><img src="https://arxiv.org/html/2508.11479v1/x4.png" alt="训练曲线对比"></p>
<blockquote>
<p><strong>图4</strong>：在HM3D-OVON val_seen分割上的训练曲线比较。OVSegDT（绿线）相比基准方法（DagRL，蓝线）收敛更快，且最终性能更高。</p>
</blockquote>
<p>总训练损失为：(\mathcal{L}<em>{\text{total}}(\theta)=c</em>{v},\mathcal{L}<em>{V}(\theta)+\mathcal{L}</em>{\text{EALM}}(\theta)-\beta H_{t}+\mathcal{L}<em>{\text{seg}}(\theta))，其中包含价值回归损失 (\mathcal{L}</em>{V}) 和熵奖励 (-\beta H_t)。</p>
<p><img src="https://arxiv.org/html/2508.11479v1/x5.png" alt="真实世界适配"></p>
<blockquote>
<p><strong>图5</strong>：为适应真实世界部署（无真值掩码），需要对预训练的开集分割模型（如YOLOE）进行适配，包括基于掩码大小的过滤和针对不同物体类别的置信度阈值校准。</p>
</blockquote>
<p>对于真实世界部署，论文提出了使用预测分割掩码的适配策略。由于预训练的开集分割模型（如YOLOE）在远距离或遮挡物体上性能下降，作者采用两种策略：1）在使用真值掩码进行PPO微调时，过滤掉尺寸过小（宽或高&lt;32像素，总像素&lt;1000）的掩码，以模拟预测掩码的噪声；2）针对导航词汇表中的不同物体类别，校准分割模型的置信度阈值（例如，“洗碗机”需要高于0.4，“书”或“花盆”可低至0.01），并移除语义冗余的类别。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在HM3D-OVON基准上进行，这是一个为开放词汇目标导航定制的照片级真实模拟器基准。评估指标包括成功率（SR）、路径长度加权成功率（SPL）和平均碰撞次数。对比的基线方法涵盖了端到端无图方法（如DagRL、Uni-NaVid）、模块化图方法（如VLFM、TANGO）以及使用额外传感器（深度、里程计）的方法（如DagRL+OD、MTU3D）。</p>
<p><img src="https://arxiv.org/html/2508.11479v1/x1.png" alt="主要结果表"></p>
<blockquote>
<p><strong>表1</strong>：在HM3D-OVON上的开放词汇目标导航性能对比。OVSegDT仅使用RGB输入，在未见类别上取得了40.1% SR和20.9% SPL的SOTA结果，与在已见类别上的性能相当。</p>
</blockquote>
<p>关键实验结果总结如下：在仅使用RGB输入、无需深度或里程计的条件下，OVSegDT在HM3D-OVON的<code>val_unseen</code>（未见类别）分割上取得了最佳性能，成功率为40.1%，SPL为20.9%，与最强的使用深度和里程计的基线方法（DagRL+OD，SR 37.1%， SPL 19.8%）相比有提升。更重要的是，其<code>val_unseen</code>性能与<code>val_seen</code>（已见类别，SR 41.0%， SPL 21.6%）非常接近，证明了出色的泛化能力。相比之前的端到端SOTA方法DagRL（<code>val_unseen</code> SR 18.3%），性能提升了一倍以上。训练样本复杂度降低了33%，碰撞次数减少了一半。</p>
<p><img src="https://arxiv.org/html/2508.11479v1/x6.png" alt="消融实验：训练稳定性"></p>
<blockquote>
<p><strong>图6</strong>：消融研究显示，移除EALM（粉色线）会导致训练不稳定和性能下降，而同时使用目标掩码和辅助分割损失（绿色线）能实现最佳且稳定的学习曲线。</p>
</blockquote>
<p>消融实验验证了各组件贡献：1）<strong>EALM</strong>：移除EALM会导致训练不稳定（性能波动大），证明了其对于稳定单阶段训练的关键作用。2）<strong>目标掩码输入</strong>：仅使用目标掩码（无辅助损失）能带来显著性能提升。3）<strong>辅助分割损失</strong>：在已有目标掩码输入的基础上，增加辅助分割损失能进一步加速早期训练收敛并提升最终性能。三者协同工作效果最佳。</p>
<p><img src="https://arxiv.org/html/2508.11479v1/figures/lines_with_scatter_val_unseen.png" alt="未见类别性能曲线"></p>
<blockquote>
<p><strong>图8</strong>：在<code>val_unseen</code>分割上的性能曲线。OVSegDT（绿色）显著且稳定地优于基准方法DagRL（蓝色），展示了其对新颖物体类别的强大泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.11479v1/x7.png" alt="预测分割掩码性能"></p>
<blockquote>
<p><strong>图9</strong>：使用预测分割掩码（经过校准的YOLOE）与使用真值掩码的性能对比。经过适配后，使用预测掩码的性能下降得到有效控制，证明了方法在真实场景下的可行性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1）提出了OVSegDT，一个轻量级Transformer架构，通过引入目标二值掩码编码器，为策略提供了精确的空间目标提示，显著加速学习并提升了对已见和未见类别的导航鲁棒性。2）设计了辅助语义分割损失，帮助模型解耦观测表示，进一步改善了收敛性和导航质量。3）提出了熵自适应损失调制（EALM），实现了模仿学习与强化学习信号在样本级别的动态、平滑平衡，消除了脆弱的手动阶段切换，实现了稳定的单阶段训练。</p>
<p>论文自身提到的局限性包括：导航性能在真实场景下仍依赖于开集分割模型的质量；需要对分割模型进行词汇表校准和过滤策略适配，这引入了额外的步骤。</p>
<p>本研究对后续工作的启示在于：证明了轻量级模型（1.3亿参数）结合精心设计的训练机制，可以在不依赖大规模基础模型或额外传感器的情况下，取得优异的开放词汇导航性能。这为在资源受限的机器人平台上的部署提供了可能。同时，将明确的语义感知（分割）作为多任务学习的一部分，是提升端到端策略泛化能力和数据效率的有效途径。EALM机制为解决强化学习中不同学习信号动态平衡的通用问题提供了新思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对开放词汇目标导航中，端到端策略在模拟器小数据集上过拟合、泛化能力差且碰撞频繁的问题，提出轻量级Transformer策略OVSegDT。其关键技术包括：1）语义分支，通过目标二进制掩码编码器和辅助分割损失函数，实现文本目标的空间语义对齐；2）熵自适应损失调制，根据策略熵动态平衡模仿与强化学习信号。实验表明，该方法将训练样本复杂度降低33%，碰撞次数减少一半，在HM3D-OVON数据集上未见类别与已见类别性能相当，取得最优结果（成功率40.1%，SPL 20.9%），且无需深度、里程计或大视觉语言模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.11479" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>