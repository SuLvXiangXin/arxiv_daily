<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.21057" target="_blank" rel="noreferrer">2506.21057</a></span>
        <span>作者: Cewu Lu Team</span>
        <span>日期: 2025-06-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习已成为机器人操作的重要范式，但其泛化能力受限于专家演示中物体特定的依赖关系。当前主流方法通过扩大训练数据规模来提升策略的泛化性能，但这需要大量数据收集和计算资源，成本高昂且效率低下。相比之下，人类能够轻松地将习得技能泛化到不同条件，其关键在于人类在学习过程中并非记忆针对单一物体的具体动作，而是将物体抽象为能够捕获操作所需物体中心知识和语义信息的表示，从而理解任务的内在逻辑。</p>
<p>本文针对模仿学习泛化能力不足和数据效率低下的痛点，提出了一种知识驱动的视角。核心思路是模仿人类的认知过程，利用外部结构语义知识，将同一类别内的物体抽象为一种知识模板，并设计鲁棒的模板匹配算法，使得策略能够基于这种抽象的高层表示进行学习，从而实现对新颖物体和条件的泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的知识驱动模仿学习框架旨在利用物体的语义特征和结构信息，实现鲁棒、可泛化且样本高效的机器人学习。整体流程如论文图1所示：给定目标的单张观测图像，系统提取该类别的知识模板；随后将该模板与演示数据中的物体进行匹配；最终基于获得的物体语义知识匹配结果，训练知识驱动的模仿策略，使策略能够适应变化的条件。</p>
<p><img src="https://arxiv.org/html/2506.21057v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：知识驱动模仿学习方法概览。给定RGB-D图像输入，系统为特定物体生成知识模板。该模板随后与演示数据进行匹配，并基于匹配结果学习策略。当遇到新物体时，学习到的策略可以迁移，从而实现对新场景的泛化。</p>
</blockquote>
<p><strong>核心模块一：类别级知识模板</strong><br>给定一个类别𝒞及其关联的物体𝒪，系统利用物体的单次观测定义一个类别级知识模板𝒯^𝒞，以抽象该类物体共享的共同结构和语义信息。该模板包含K个语义关键点：𝒯^𝒞 = { (f̂_k, p̂_k) }，其中f_k ∈ ℝ^N为语义特征，p_k ∈ ℝ^3为关键点3D位置。实践中，将关键点集合视为一个图，在匹配过程中考虑关键点间的空间关系以确保更有效的匹配，如图2所示。</p>
<p><img src="https://arxiv.org/html/2506.21057v1/x2.png" alt="知识模板"></p>
<blockquote>
<p><strong>图2</strong>：作为知识模板的关键点图。知识模板以图的形式表示，显式地编码结构信息，同时每个关键点关联一个语义特征，隐式地传递语义信息。</p>
</blockquote>
<p><strong>语义特征生成</strong>：使用预训练的语义特征提取器（如DINOv2）。给定物体的RGB图像ℐ，提取器ℱ生成对应的语义特征图ℱ(ℐ)。结合深度图像𝒟和相机参数，将带有特征的像素投影为3D点云𝒫 = { (f, p, c) }，其中c为颜色。<br><strong>语义关键点采样</strong>：从点云𝒫中选择一个子集作为关键点以形成知识模板。为确保关键点覆盖物体的不同部位，结合颜色c和语义特征f进行最远点采样。采样得到的K个语义关键点{ (f_k, p_k) }即作为该类别的知识模板𝒯^𝒞。</p>
<p><strong>核心模块二：知识模板匹配</strong><br>获得类别𝒞的知识模板𝒯^𝒞后，可将其注册到任何新物体𝒪&#39;的观测(ℐ, 𝒟)上，以建立新图像中物体与模板的关系。简单的方法是使用基于特征距离的最近点匹配，但这存在局限性：当关键点特征相似时可能导致匹配顺序错乱；关键点被遮挡时匹配失效。</p>
<p>为消除关键点匹配的歧义，本文提出将关键点组织为模板以提供结构信息，并执行<strong>从粗到精的模板匹配</strong>。优化目标是找到最优匹配集合ℳ = {m_k}，最小化联合损失：ℒ_feature + β·ℒ_structure。其中ℒ_feature是特征距离之和，ℒ_structure是在最佳旋转R、平移t和缩放s下的位置误差之和。</p>
<p><strong>粗匹配</strong>：负责高效地估计知识模板𝒯^𝒞与语义点云𝒫之间的近似变换。这是一个带缩放的点集配准问题。系统设定特征距离阈值δ_f，将特征距离低于此阈值的点对视为对应关系。为处理遮挡，采用RANSAC策略，依靠少量准确匹配的关键点推断被遮挡关键点的位置。<br><strong>精匹配</strong>：粗匹配提供了近似变换，但简化了特征距离，不足以捕捉物体间的细微形变。在获得近似变换后，进一步在局部邻域内优化每个关键点的位置，同时考虑特征相似性和与粗匹配估计位置的结构一致性，并引入位置距离阈值δ_p进行约束。</p>
<p><strong>核心模块三：知识驱动策略训练</strong><br>通过模仿学习训练知识驱动策略的流程是：首先为演示数据中相关物体生成知识模板，并对整个演示数据集执行模板匹配。获得模板对应关系ℳ后，策略以此作为输入进行训练。实践中，将知识模板的匹配结果视为特征向量，直接输入扩散模型以生成动作：a = π_θ(ℳ, o_other)，其中o_other包含机器人本体感知信息和夹爪宽度等必要观测。</p>
<p>当策略部署于新条件（如新物体）下时，将知识模板匹配到新物体。这些匹配的关键点可作为“锚点”，将当前场景中的新物体“形变”为训练集中的模板物体。此时，基于模板训练的策略将新物体感知为模板的一种形变，从而将学习到的动作泛化到新物体上。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的创新具体体现在：1) 提出了<strong>语义关键点图</strong>作为显式结构化和隐式语义化的知识模板；2) 设计了<strong>从粗到精的模板匹配算法</strong>，联合优化语义相似性和结构一致性，比简单的逐点匹配更鲁棒；3) 整个框架将高维原始观测抽象为低维结构化表示，降低了学习复杂度，并通过模板匹配实现了跨实例的技能迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>平台</strong>：使用Flexiv Rizon 4机械臂和Robotiq 2F-85夹爪，配备两个Intel RealSense D435i RGB-D相机。</li>
<li><strong>任务</strong>：设计了三个真实世界操作任务进行评估（图3）：<ol>
<li><strong>Mug</strong>：将水从一个杯子倒入另一个杯子。</li>
<li><strong>Tool</strong>：使用工具（钳子）夹起并移动物体。</li>
<li><strong>Drawer</strong>：打开抽屉。</li>
</ol>
</li>
<li><strong>基准方法</strong>：对比了 Diffusion Policy、RT-1、ACT、VINN 以及两种基于关键点的方法（Keypoint + BC, Keypoint + Diffusion）。</li>
<li><strong>数据</strong>：每个任务收集约100条专家演示用于训练。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.21057v1/x3.png" alt="实验任务"></p>
<blockquote>
<p><strong>图3</strong>：实验任务。设计了三个操作任务（倒水、使用工具、开抽屉）进行真实世界评估。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>在主实验（Seen Objects）中，本文方法在三个任务上均取得了最高的成功率（Mug: 93.3%，Tool: 90.0%，Drawer: 96.7%），显著优于所有基线方法。特别是在泛化到<strong>新物体（Unseen Objects）</strong> 的测试中，本文方法的优势更为明显（图4），例如在Tool任务上，本文方法成功率达86.7%，而次优的Diffusion Policy仅为56.7%。</p>
<p><img src="https://arxiv.org/html/2506.21057v1/x4.png" alt="主要结果"></p>
<blockquote>
<p><strong>图4</strong>：在所见物体和新物体上的任务成功率。本文方法（Ours）在所有任务和设置下均表现最佳，尤其是在泛化到新物体时优势显著。</p>
</blockquote>
<p><strong>样本效率</strong>：如图5所示，本文方法仅需约25%的专家演示数据，其性能即可与使用100%数据的图像扩散策略（Diffusion Policy）相媲美甚至超越，证明了其卓越的样本效率。</p>
<p><img src="https://arxiv.org/html/2506.21057v1/x5.png" alt="样本效率"></p>
<blockquote>
<p><strong>图5</strong>：样本效率分析。横轴为使用的训练数据比例，纵轴为任务成功率。本文方法（蓝线）仅使用25%数据时，性能已接近或超过使用100%数据的Diffusion Policy（橙线）。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>消融实验（图6）验证了各组件贡献：</p>
<ol>
<li><strong>w/o Fine Matching</strong>：仅使用粗匹配，性能下降，说明精匹配对捕捉细节形变很重要。</li>
<li>**w/o Graph (per-point)**：将关键点图退化为独立的关键点集合进行逐点匹配，性能显著下降，证明了图结构引入的几何一致性约束对于鲁棒匹配至关重要。</li>
<li><strong>w/ Geometry-only Sampling</strong>：关键点采样仅基于几何位置（而非结合颜色和语义特征），性能也下降，说明结合语义的特征采样能更好地捕获物体关键部位。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.21057v1/extracted/6569943/image/1.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融研究。移除精细匹配（Ours w/o Fine）、移除图结构进行逐点匹配（Ours w/o Graph）、仅使用几何采样（Ours w/ Geometry-only）均会导致性能下降，证明了所提组件的重要性。</p>
</blockquote>
<p><strong>定性结果与鲁棒性</strong>：<br>图7展示了方法在物体外观、背景和光照条件变化下的鲁棒性。图8和图9分别可视化了DINOv2特征和模板匹配的效果。图10直观展示了策略如何通过模板匹配将新物体（右侧）视为训练模板物体（左侧）的一种“形变”，从而实现动作泛化。</p>
<p><img src="https://arxiv.org/html/2506.21057v1/extracted/6569943/image/2.png" alt="定性结果"></p>
<blockquote>
<p><strong>图7</strong>：在物体、背景、光照变化下的定性结果。本文方法能成功处理这些干扰。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.21057v1/extracted/6569943/image/dino.png" alt="特征可视化"></p>
<blockquote>
<p><strong>图8</strong>：DINOv2特征可视化。展示了语义特征提取器对物体不同部位的特征响应。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.21057v1/extracted/6569943/image/dift.png" alt="匹配可视化"></p>
<blockquote>
<p><strong>图9</strong>：模板匹配可视化。线条连接了模板关键点（绿色）与在观测物体上匹配到的对应点（蓝色）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.21057v1/extracted/6569943/image/deformation.png" alt="形变泛化示意"></p>
<blockquote>
<p><strong>图10</strong>：通过形变进行泛化的示意图。学习到的策略（在左侧模板物体上训练）通过匹配，将动作泛化到右侧的新物体上。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>知识驱动模仿学习</strong>框架，通过引入显式结构化和隐式语义化的<strong>语义关键点图</strong>作为知识模板，为模仿学习注入了有益的归纳偏置。</li>
<li>设计了一种<strong>从粗到精的语义知识模板匹配算法</strong>，该算法联合优化结构一致性和语义相似性，能够鲁棒地建立模板与不同物体实例间的对应关系，是实现泛化的关键。</li>
<li>在真实机器人任务上的实验表明，该方法在<strong>样本效率</strong>（仅需1/4数据）和<strong>泛化能力</strong>（对新物体、背景、光照）方面均显著优于现有方法。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于预训练的视觉基础模型（如DINOv2）和分割模型（如SAM）来生成语义特征和物体掩码。这些模型的质量和局限性会影响知识模板的构建。此外，方法主要针对具有明确结构和语义部件的刚性或关节物体，对于高度非刚性或纹理单一、缺乏判别性特征的物体可能不适用。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>知识表示</strong>：探索更丰富、更灵活的知识表示形式，以处理更复杂的物体类别和任务。</li>
<li><strong>自动化与自适应</strong>：研究如何进一步自动化知识模板的构建过程，或使模板能够在线适应和更新。</li>
<li><strong>结合大规模数据</strong>：将这种知识驱动的方法与大规模预训练策略相结合，可能进一步提升其能力和适用范围。</li>
<li><strong>基础模型依赖</strong>：推动开发更专用于机器人感知和物理交互的基础模型，以提供更可靠的知识来源。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习在机器人操作中因依赖特定对象演示而泛化能力受限的问题，提出知识驱动模仿学习框架。该方法引入语义关键点图作为知识模板，并开发从粗到精的模板匹配算法，以优化结构一致性与语义相似性。在三个真实机器人操作任务上的实验表明，该方法仅需四分之一专家演示，性能即超越基于图像的扩散策略，并在新物体、背景及光照条件下展现出优越的鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.21057" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>