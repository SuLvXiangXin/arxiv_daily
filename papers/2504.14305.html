<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.14305" target="_blank" rel="noreferrer">2504.14305</a></span>
        <span>作者: Shi, Jiyuan, Liu, Xinzhe, Wang, Dewei, Lu, Ouyang, Schwertfeger, Sören, Zhang, Chi, Sun, Fuchun, Bai, Chenjia, Li, Xuelong</span>
        <span>日期: 2025/04/19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前人形机器人模仿人类全身运动的主流方法是运动重定向和强化学习，即学习一个统一的全身策略，以运动跟踪误差作为奖励进行优化。然而，这种方法存在关键局限性：首先，人形机器人自由度（DoF）高，直接学习全身控制策略需要复杂的奖励设计，训练过程计算成本高昂；其次，不同运动间的差异以及部分人类动作超出机器人物理能力，使得强化学习策略难以收敛。更重要的是，这类方法优先考虑精确的运动跟踪，常常忽视了维持机器人稳定性的根本需求，导致策略在现实部署中频繁引发机器人摔倒。</p>
<p>本文针对上述痛点，提出了将上半身和下半身视为具有不同角色的独立智能体的新视角。具体而言，下半身负责提供稳健的步态能力以跟随速度指令，而上半身负责精确跟踪各种运动。本文提出了一种新颖的对抗性步态与运动模仿（ALMI）框架，通过让上半身和下半身策略进行对抗性学习，促使它们在迭代更新中达成协调的全身控制。核心思路是：分别学习上半身和下半身的策略，并通过对抗性训练使下半身策略能抵抗上半身运动带来的干扰，同时使上半身策略能在下半身不稳定时仍能精确跟踪运动。</p>
<h2 id="方法详解">方法详解</h2>
<p>ALMI框架的核心是分别训练下半身（步态）策略π^l和上半身（运动模仿）策略π^u，并通过对抗性交互使它们相互增强。整体流程采用交替迭代更新的方式。</p>
<p><img src="https://arxiv.org/html/2504.14305v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ALMI方法整体框架。（a）更新上半身策略π^u时，采样对抗性速度指令c^l_adv，通过固定的下半身策略得到动作a^l_adv，然后与π^u生成的动作a^u一起与环境交互收集经验，用于通过PPO算法更新π^u。（b）更新下半身策略π^l时，采样对抗性运动g^u_adv，通过固定的上半身策略得到动作a^u_adv，然后与π^l生成的动作a^l一起交互并更新π^l。两个策略通过多次相互迭代最终收敛。</p>
</blockquote>
<p><strong>对抗学习框架的理论基础与简化</strong>：理论上，将上下半身策略学习建模为两个零和马尔可夫博弈。学习π^l时，下半身为智能体（最大化步态奖励V^l），上半身为对抗者（最小化V^l），以学习抗干扰的稳健步态。学习π^u时，角色互换，上半身为智能体（最大化跟踪奖励V^u），下半身为对抗者（最小化V^u），以学习在移动中仍能精确跟踪的策略。为降低计算开销，论文提出了简化方案：在更新一个策略时，固定另一个策略的参数，但通过从指令空间（而非参数空间）采样对抗性指令来模拟对抗者。即，更新π^l时，从运动数据集中采样对抗性运动g^u_adv输入给固定的π^u来生成干扰；更新π^u时，采样对抗性速度指令c^l_adv输入给固定的π^l来生成干扰。</p>
<p><strong>下半身稳健步态策略学习</strong>：目标是使策略能跟随速度指令（v_x， v_y， ω_yaw），同时抵抗来自上半身的干扰。关键创新是引入了<strong>双课程机制</strong>来渐进式地增加对抗性运动的难度。首先，训练一个无上半身干扰的基础步态策略π^l_0。然后，使用一个评分机制评估AMASS数据集中每个运动对机器人稳定性的影响（以生存长度l^sl为指标），并按难度排序得到运动列表M。训练时，策略会面对一个由窗口索引α_d和运动缩放系数α_s共同定义的难度范围内的运动。随着策略抗干扰能力提升（通过平均生存长度l^msl衡量），课程机制会自适应地调整α_d（在运动列表中前进或后退）和α_s（增大或减小运动幅度），确保训练既高效又能持续提升鲁棒性。奖励函数设计复杂且详尽（见表1），包括任务奖励（跟踪指令）、存活奖励以及大量正则化项（如关节限位、扭矩、加速度、脚滑等），以鼓励安全、高效、自然的步态。</p>
<p><strong>上半身运动跟踪策略学习</strong>：目标是使策略能准确跟踪来自AMASS的参考关节位置，同时抵抗来自下半身移动的干扰。其对抗性训练相对简单，对抗性干扰通过为固定的下半身策略π^l采样不同难度（速度大小）的指令c^l来实现。课程机制根据上半身的跟踪误差来调整采样指令的速度范围[c^l_min， c^l_max]，逐步增加下半身移动的剧烈程度。奖励函数（见表2）主要包括任务奖励（跟踪目标关节位置）、存活奖励和一些基本的正则化项（如朝向、扭矩、加速度等）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在MuJoCo仿真环境和全尺寸Unitree H1-2真人机器人上进行。使用AMASS数据集作为上半身运动来源。对比的基线方法包括：PHC（一种先进的全身运动模仿方法）、RL-Exo（基于expert轨迹的RL方法）、CPG（中枢模式发生器）以及ALMI的变体（如无对抗训练、无课程学习等）。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2504.14305v3/x3.png" alt="仿真全身运动模仿对比"></p>
<blockquote>
<p><strong>图3</strong>：在仿真环境中与基线方法进行全身运动模仿的定性对比。ALMI在完成复杂运动（如“踢腿”、“挥手走路”）时表现出更好的稳定性和协调性，而PHC等方法容易出现摔倒或动作变形。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.14305v3/x4.png" alt="仿真定量评估"></p>
<blockquote>
<p><strong>图4</strong>：在仿真中的定量评估。（左）在多种运动任务上的平均成功率。ALMI达到了95.2%的最高成功率，显著优于PHC（78.5%）和RL-Exo（65.3%）。（右）上半身关节位置跟踪误差。ALMI的跟踪误差（0.046 rad）也低于PHC（0.051 rad）和RL-Exo（0.063 rad）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.14305v3/x5.png" alt="真实机器人步态测试"></p>
<blockquote>
<p><strong>图5</strong>：在Unitree H1-2真实机器人上测试步态鲁棒性。（上）在平坦地面上抗外部推力干扰，ALMI策略能成功恢复平衡。（下）在斜坡和崎岖地形上行走，ALMI策略表现出良好的适应性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.14305v3/x6.png" alt="真实机器人全身运动测试"></p>
<blockquote>
<p><strong>图6</strong>：在真实机器人上执行复杂的全身运动，如边走边挥手、转身踢腿等。ALMI策略能够协调上下半身，在移动中稳定地完成指定的上半身动作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.14305v3/x7.png" alt="消融实验：对抗训练的影响"></p>
<blockquote>
<p><strong>图7</strong>：消融实验：对抗训练的影响。对比ALMI（有关联的对抗训练）、ALMI w/o Adv.（无对抗训练，即上下半身独立训练）和ALMI w/o Coord.（无协调，即用PD控制器代替另一半身策略）。结果显示，完整的ALMI在步态跟踪误差和运动跟踪误差上均表现最佳，证明了对抗性交互对于学习协调、鲁棒策略的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.14305v3/x8.png" alt="消融实验：课程学习的影响"></p>
<blockquote>
<p><strong>图8</strong>：消融实验：课程学习的影响。对比ALMI（完整）、ALMI w/o Curr.（无课程，随机采样对抗性运动/指令）和ALMI w/o Dual-Curr.（无双课程，仅使用运动难度排序或仅使用运动缩放）。完整ALMI的训练收敛速度更快，最终性能更高，验证了双课程机制的有效性。</p>
</blockquote>
<p><strong>ALMI-X数据集与基础模型初步结果</strong>：<br><img src="https://arxiv.org/html/2504.14305v3/x2.png" alt="ALMI-X数据集与基础模型"></p>
<blockquote>
<p><strong>图2</strong>：（a）ALMI-X数据集结构，结合了运动目标、速度指令和语言描述。（b）基于Transformer架构的基础模型，以语言指令和历史状态-动作为条件，自回归地预测下一个动作。</p>
</blockquote>
<p>论文使用训练好的ALMI策略收集了大规模高质量数据集ALMI-X，包含超过8万条由语言描述（如“慢速后退并挥左手”）标记的轨迹。基于此数据，初步训练了一个用于端到端全身控制的基础模型。该模型采用因果Transformer，能够根据语言指令和实时交互历史生成动作。<br><img src="https://arxiv.org/html/2504.14305v3/x9.png" alt="基础模型性能"></p>
<blockquote>
<p><strong>图9</strong>：基础模型在仿真中的性能。在训练集和测试集（新运动或新指令组合）上，该模型都能较好地跟随语言指令控制机器人，展示了从大规模数据中学习通用控制策略的潜力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.14305v3/x10.png" alt="运动编辑应用"></p>
<blockquote>
<p><strong>图10</strong>：展示了基础模型在运动编辑方面的应用潜力，例如通过修改语言指令（将“向左转”改为“向右转”）来改变机器人行为。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了ALMI这一新颖的对抗性训练框架，通过分别学习并让上下半身策略相互对抗，有效解决了人形机器人全身协调控制中稳定性与精确性难以兼顾的问题。2) 构建了首个大规模、带语言标注的人形机器人全身控制数据集ALMI-X，为训练人形机器人基础控制模型提供了宝贵资源。3) 在仿真和真实机器人上进行了广泛实验，验证了ALMI策略的优越性，并初步探索了基于ALMI-X数据的基础模型，展示了语言引导全身控制的可行性。</p>
<p>论文自身提到的局限性包括：ALMI-X数据集中的运动主要来源于AMASS，可能无法覆盖所有可能的机器人动作；基础模型目前仅为初步尝试，其泛化能力和在复杂动态环境中的鲁棒性有待进一步研究。</p>
<p>本文的工作对后续研究具有重要启示：将复杂控制问题分解为相互关联的子问题并通过对抗或竞争机制进行协同优化，是一个有效的思路。同时，构建高质量、大规模、多模态的机器人控制数据集，是迈向通用机器人基础模型的关键一步。如何将仿真中学习到的强鲁棒性策略更安全、高效地迁移到复杂的真实世界，仍是未来研究的重点方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人难以实现稳健、协调的全身运动模仿问题，提出对抗性运动与运动模仿（ALMI）框架。核心方法是分别学习下半身的稳健运动策略（跟随速度指令）和上半身的精确运动模仿策略，并通过对抗性迭代更新实现全身协调控制。实验表明，该方法在仿真和Unitree H1-2真实机器人上均实现了稳健的运动能力和精确的运动跟踪。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.14305" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>