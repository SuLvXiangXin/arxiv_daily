<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Cryptography and Security (cs.CR)</span>
      <h1>Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.09269" target="_blank" rel="noreferrer">2510.09269</a></span>
        <span>作者: Zhou, Zirun, Xiao, Zhengyang, Xu, Haochuan, Sun, Jing, Wang, Di, Zhang, Jingfeng</span>
        <span>日期: 2025/10/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作模型作为具身智能的“大脑”，近年来发展迅速，但其训练依赖大规模、未经验证的数据集，引发了严重的安全担忧。现有的针对VLA的后门攻击方法，如TrojanRobot和BadVLA，大多假设攻击者拥有模型的白盒访问权限（可修改架构或参数），并且攻击目标仅是导致模型任务失败，属于非目标攻击。这些方法在实际中不实用，因为攻击者通常难以接触模型内部。</p>
<p>本文针对这一痛点，提出了一个更现实、更危险的威胁视角：攻击者无需接触模型，仅通过向训练数据集中注入物理物体作为后门触发器，即可实现对VLA的操控。本文提出目标导向的后门攻击，其核心思路是：通过污染训练数据，使被植入后门的VLA在正常场景下表现良好，但当特定物理触发器出现在视野中时，会执行攻击者预定义的、目标明确的恶意动作序列。</p>
<h2 id="方法详解">方法详解</h2>
<p>GoBA的整体攻击流程基于数据投毒。攻击者仅需向干净的训练数据集$\mathcal{X}$中注入一小部分恶意演示样本$\mathcal{P}$，形成中毒数据集$\mathcal{X}^{\prime}=\mathcal{X}\cup\mathcal{P}$。模型训练者使用$\mathcal{X}^{\prime}$正常训练VLA模型$\mathcal{F}^{\prime}<em>{\theta}$，后门便被自动嵌入。在部署阶段，当环境中出现该物理触发器$\tau$时，被攻击的VLA会输出攻击者预设的目标动作轨迹$\mathcal{A}</em>{\text{adv}}$，而非遵循语言指令执行原始任务。</p>
<p><img src="https://arxiv.org/html/2510.09269v1/pics/originaltask.jpg" alt="方法对比"><br><img src="https://arxiv.org/html/2510.09269v1/pics/badvla.jpg" alt="方法对比"><br><img src="https://arxiv.org/html/2510.09269v1/pics/badlibero.jpg" alt="方法对比"></p>
<blockquote>
<p><strong>图1</strong>：先前后门攻击与本文提出的GoBA方法对比。所有演示遵循同一指令：“拿起字母汤并放入篮子中”。(a) 原始任务。(b) BadVLA采用基于数字贴片的触发器（红框标出），导致机器人产生随机动作。(c) GoBA使用物理物体（饼干）作为触发器（红框标出），并强制执行目标导向行为，即拿起触发物体并将其放置在操作台右侧固定区域。</p>
</blockquote>
<p>具体而言，每个后门样本的形式为$(({\bm{v}}<em>{ij}\oplus\tau), {\bm{l}}</em>{ij}) \rightarrow {\bm{a}}<em>{\text{adv}}$。其中，视觉输入${\bm{v}}</em>{ij}$被添加了物理触发器$\tau$，语言指令${\bm{l}}<em>{ij}$保持不变，而动作标签被替换为目标动作${\bm{a}}</em>{\text{adv}}$。攻击的关键创新点在于：1) <strong>完全黑盒</strong>：攻击者仅需污染数据，无需知晓或修改模型内部。2) <strong>物理触发器</strong>：使用自然存在于场景中的物体，隐蔽性强，难以被数据清洗过滤。3) <strong>目标导向</strong>：攻击目标是让机器人完成一个具体的、由攻击者设计的新任务（如将特定物体放到特定位置），而非简单的任务失败。</p>
<p>为了系统研究此类攻击，本文基于流行的VLA基准LIBERO构建了<strong>BadLIBERO</strong>数据集。该数据集为LIBERO的四个任务套件（LIBERO-LONG, LIBERO-GOAL, LIBERO-OBJECT, LIBERO-SPATIAL）收集了对应的后门演示，其中机器人会拿起一个触发物体并将其放置到一个固定区域。每个任务收集了12个此类演示。</p>
<p><img src="https://arxiv.org/html/2510.09269v1/pics/action_test.jpg" alt="动作轨迹设计"></p>
<blockquote>
<p><strong>图2</strong>：展示三种不同后门动作轨迹的任务示例。语言指令保持不变：“拿起字母汤并放入篮子中”。轨迹1：同时替换待抓取物体（改为饼干）和放置位置（改为新固定区域）。轨迹2：仅替换待抓取物体（改为饼干）。轨迹3：仅替换放置位置（改为新固定区域）。</p>
</blockquote>
<p>此外，本文设计了一个<strong>三级评估体系</strong>以全面量化攻击效果：</p>
<ul>
<li>**Level-1 (无事可做)**：机器人在推理期间不尝试任何目标（无论是原始目标还是后门目标）。</li>
<li>**Level-2 (尝试去做)**：机器人尝试完成后门目标但失败（例如尝试抓取但未成功，或成功抓取但放置失败）。</li>
<li>**Level-3 (成功去做)**：机器人成功完成后门目标。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：本文在LIBERO基准的四个任务套件上进行了实验，使用两个领先的开源VLA模型作为受害者模型：基于流匹配的$\pi_0$和基于自回归的OpenVLA。物理触发器是一个带有有毒警告标签的盒子，后门目标是拿起该盒子并将其放在操作台右侧。数据投毒率固定为10%。</p>
<p><strong>基线方法</strong>：对比方法包括对抗攻击（UAPA, UPA, TMA）和后门攻击（BadVLA-patch, BadVLA-mug）。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>攻击有效性</strong>：如表2所示，GoBA在触发器出现时能导致接近100%的任务失败率。更重要的是，在更具指示性的Level-3成功率上，GoBA在$\pi_0$模型上平均达到**97.0%<strong>，在OpenVLA模型上平均达到</strong>58.4%**，成功驱使机器人执行了目标动作。</li>
<li><strong>隐蔽性</strong>：在干净输入（无触发器）下，中毒模型的成功率与原始基线模型相当，甚至有时略有提升（如OpenVLA在LIBERO-OBJECT上从88.4%提升至92.9%），表明后门几乎不影响正常性能。</li>
<li><strong>与现有方法对比</strong>：如表3所示，在与后门攻击的对比中，GoBA在ASR(BadVLA)指标上平均达到**99.6%**，优于BadVLA-patch (96.2%) 和 BadVLA-mug (97.8%)。但需要指出，ASR(BadVLA)是为非目标攻击设计的，而GoBA的三级评估更精准地衡量了其目标导向攻击的成功率。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09269v1/pics/color_fr_cross_heatmap.jpg" alt="主要结果表"></p>
<blockquote>
<p>**表2 (示意图)**：GoBA在四个任务套件和两个VLA模型上的详细结果。显示了干净输入成功率(SR(w/o))、触发时失败率(FR(w))以及三级评估的分布。结果表明GoBA在保持正常性能的同时，实现了高水平的攻击成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.09269v1/pics/color_l3_cross_heatmap.jpg" alt="对比结果表"></p>
<blockquote>
<p>**表3 (示意图)**：GoBA与现有攻击方法的对比。在失败率(FR)上，GoBA平均为98.9%，在所有攻击方法中排名第二，在后门攻击中排名第一。在ASR(BadVLA)上，GoBA平均为99.6%，排名第一。</p>
</blockquote>
<p><strong>消融实验与影响因素分析</strong>：<br>本文系统研究了影响GoBA性能的因素，所有实验在OpenVLA上进行。</p>
<ol>
<li><strong>动作轨迹</strong>：如表4所示，同时替换待抓取物体和放置位置（轨迹1）的Level-3成功率最高（**62.3%**）。仅替换放置位置（轨迹3）的攻击效果最差，表明修改抓取目标对攻击成功更为关键。</li>
<li><strong>触发器颜色</strong>：如图5(b)所示，触发器包装颜色对攻击有显著影响。纯白色包装效果最好，将Level-3成功率提升至**77.3%**。</li>
<li><strong>触发器大小</strong>：如图6(b)所示，与基于贴片的攻击直觉不同，物理触发器的大小对攻击性能影响甚微。即使体积仅为原始0.1%的微小物体，也能成功触发后门（Level-3成功率52.0%）。</li>
<li><strong>物体形状与可抓取性</strong>：如图10(b)所示，物体的可抓取性是关键因素。难以抓取的物体（如刀）会导致Level-2（尝试但失败）比例大幅上升至**59.0%<strong>，而Level-3成功率相应降至</strong>25.6%**；易于抓取的物体（如杯子）则能获得更高的Level-3成功率。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09269v1/pics/color_effect.jpg" alt="颜色影响"></p>
<blockquote>
<p><strong>图5 (b) 颜色测试结果</strong>：展示了不同颜色包装作为触发器时的攻击性能（三级评估分布）。纯白色包装取得了最高的Level-3成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.09269v1/pics/size_effect.jpg" alt="大小影响"></p>
<blockquote>
<p><strong>图6 (b) 大小测试结果</strong>：展示了不同体积的触发器对攻击性能的影响。结果显示，即使触发器体积变化很大，Level-3成功率的变化相对平缓，表明大小并非关键因素。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.09269v1/pics/object_effect.jpg" alt="物体影响"></p>
<blockquote>
<p><strong>图10 (b) 物体测试结果</strong>：展示了不同物理触发器（饼干、杯子、刀）的攻击性能。结果表明，难以抓取的物体（刀）会显著降低Level-3成功率，同时提高Level-2（尝试但失败）的比例。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) <strong>揭示了新型威胁</strong>：首次提出了针对VLA的、完全黑盒的、目标导向的物理后门攻击，攻击手段极其简单（仅需数据投毒），但危害性极大。2) <strong>构建了基准与评估体系</strong>：创建了BadLIBERO数据集并提出了三级评估指标，为后续研究此类攻击提供了基础。3) <strong>系统性的因素分析</strong>：深入探究了动作轨迹、触发器颜色、大小和物体属性对攻击效果的影响，为理解及设计此类攻击提供了重要见解。</p>
<p>论文自身提到的局限性在于实验主要在模拟环境中进行，现实世界的复杂性（如光照、视角变化、物体遮挡）可能对攻击的鲁棒性构成挑战。</p>
<p>本研究对后续工作的启示在于：1) <strong>防御的紧迫性</strong>：迫切需要开发能够检测和抵御此类物理后门攻击的防御机制，特别是在数据收集和清洗阶段。2) <strong>安全警示</strong>：为VLA及更广泛的具身AI社区敲响了警钟，强调在使用开源或第三方数据集时必须高度重视安全审计。3) <strong>攻击演进</strong>：攻击者可根据本文发现的规律（如使用高对比度颜色、选择易抓取物体）设计更具威胁性的后门，防御方需提前应对。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型依赖未经验证训练数据的安全风险，提出了一种更实用的目标导向后门攻击方法GoBA。该方法通过在训练数据中注入物理对象作为触发器，使得模型在触发存在时执行预定义的目标动作（如移动特定物体），而在无触发时表现正常。关键技术包括构建包含多样物理触发器的BadLIBERO数据集，并设计了“无事可做、尝试执行、成功执行”三级评估标准。实验表明，攻击在触发存在时成功率达97.0%，且对干净输入性能无影响（0.0%下降）。研究还发现攻击效果受动作轨迹和触发器颜色影响显著，而触发器大小影响较小。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.09269" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>