<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Continually Evolving Skill Knowledge in Vision Language Action Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Continually Evolving Skill Knowledge in Vision Language Action Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.18085" target="_blank" rel="noreferrer">2511.18085</a></span>
        <span>作者: Wu, Yuxuan, Wang, Guangming, Yang, Zhiheng, Yao, Maoqing, Sheil, Brian, Wang, Hesheng</span>
        <span>日期: 2025/11/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过大规模预训练数据支持多样化的机器人操作任务，但它们仍然严重依赖任务特定的微调，这反映了其持续学习能力的缺乏。现有的持续模仿学习方法大多依赖于任务特定的适配器或手工特征，难以扩展到参数众多的大型VLA模型；而最近的任务中心表示学习方法虽然通过生成模型改善了跨任务泛化，但其多阶段训练和手动标注需求仍阻碍了持续学习。本文针对VLA模型在持续学习场景下知识难以保留与演化的痛点，提出了一个知识驱动的持续学习新视角。其核心思路是通过联合学习任务潜在表示和一个自演化的知识空间，形成知识保留与发现的自我监督循环，并利用知识引导的专家路由机制实现任务专业化，从而在减少标注需求和训练开销的同时，提升持续学习性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>Stellar VLA是一个用于视觉语言操作任务的端到端持续模仿学习框架。其整体目标是在顺序学习新任务时，不断演化任务相关知识，同时减轻对先前任务的遗忘。框架采用小缓冲区经验回放来缓解遗忘，并核心设计了知识空间的联合学习与演化机制。</p>
<p><img src="https://arxiv.org/html/2511.18085v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Stellar VLA的整体架构。CLIP和FiLM-conditioned ResNet分别编码语言和视觉输入。任务中心表示z和知识空间通过知识更新和潜在表示聚合进行联合学习。学习到的知识先验最终引导MoE动作头进行运动预测。</p>
</blockquote>
<p>框架的核心模块包括基于狄利克雷过程的知识空间和知识引导的专家路由。首先，为了获取可解释的任务或技能级知识，模型将感知编码器得到的低层潜在表示组织到一个高层、动态演化的知识空间中。该知识空间采用非参数的狄利克雷过程模型，能够自适应地表示无限的任务相关知识，并支持新簇的自动涌现。论文提出了两种变体：T-Stellar使用狄利克雷过程混合模型形成任务簇；TS-Stellar则使用分层狄利克雷过程来建模任务-技能层次关系，以捕获可重用的子技能。</p>
<p>知识空间的学习与任务中心表示的学习协同进行，形成一个自演化的循环。对于T-Stellar，通过变分自编码器重建语言指令和视觉观察，得到任务表示z_j，并用KL散度损失约束其向DPMM知识空间中的任务分布对齐。对于TS-Stellar，采用分层VAE，分别用任务表示z_j^task重建语言，用技能表示z_j^skill重建观察，并计算其与HDP知识空间中任务分布和技能分布的双重KL散度损失。在训练过程中，感知编码器持续提供潜在特征来更新知识分布，而知识空间则通过KL损失约束潜在表示的推断，从而实现知识的自主保留与发现。</p>
<p><img src="https://arxiv.org/html/2511.18085v2/x2.png" alt="知识引导路由"></p>
<blockquote>
<p><strong>图2</strong>：知识先验引导的MoE动作头。计算了两种知识嵌入（关系和Top-K语义）用于专家路由，连同语言、噪声、观察和噪声动作令牌一起输入去噪Transformer。</p>
</blockquote>
<p>在学习了知识空间和任务潜在表示后，模型引入一个知识引导的MoE模块来分配任务特定的动作预测参数。与基线MoDE使用噪声级别进行路由不同，Stellar VLA使用知识空间来引导专家分配。由于DPMM/HDP产生可变数量的簇，直接将其分布输入固定维度网络不切实际，因此设计了两种知识嵌入格式：知识关系嵌入编码潜在表示z与各簇中心μ_k的距离；Top-K语义嵌入则聚合了z所属概率最高的K个簇的可学习嵌入。最终的知识嵌入f_know与噪声、语言等令牌拼接，共同输入路由网络，实现任务感知的专家分配。这种机制使得模型能够在相关任务间共享参数，同时在无关任务间进行区分，从而在持续学习中实现平衡的参数共享与隔离。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真和真实世界两种设置下进行。仿真实验使用了LIBERO基准，包括LIBERO-goal、LIBERO-long和自建的30任务子集LIBERO-30*，以评估模型在多样化目标、长视野推理和大量任务下的表现。真实世界实验在一个仿人机器人平台上进行了三项双臂操作任务的顺序学习。所有实验均采用持续学习设置，仅存储少量历史数据进行经验回放。</p>
<p>对比的基线方法包括：预训练的UniVLA（通过大规模潜在动作预训练学习任务中心表示）以及MoDE（一种基于扩散的混合专家VLA策略），并评估了其微调版和从头训练版。评估指标包括衡量前向学习的前向迁移、衡量遗忘的负向反向迁移、反映整体性能稳定性的曲线下面积以及所有任务训练完成后的最终平均成功率。</p>
<p><img src="https://arxiv.org/html/2511.18085v2/x3.png" alt="潜在空间可视化"></p>
<blockquote>
<p><strong>图3</strong>：在LIBERO-long上学习第1、4、6、8、10个任务后，Stellar VLA潜在表示的t-SNE可视化。T-Stellar建模了离散的任务分布，而TS-Stellar学习了跨任务的相关技能。</p>
</blockquote>
<p>关键实验结果总结如下：在LIBERO基准上（表1），从头训练的T-Stellar和TS-Stellar在所有场景下都取得了最高的最终成功率，相对于预训练和从头训练的基线，平均提升了超过50%。它们在AUC和NBT指标上也优于大多数基线，展现了强大而稳定的持续学习性能。其中，TS-Stellar在长视野任务上超越了T-Stellar，但在较简单任务上略有落后，这表明分层任务-技能建模的优势在复杂场景中更为明显。在真实世界双臂操作任务中（表2），TS-Stellar取得了最佳性能，最终成功率达到了83.3%，显著优于基线MoDE*的46.7%和T-Stellar的66.7%。</p>
<p>消融实验（表3）验证了各个组件的贡献。移除VAE或知识空间均会导致性能显著下降，尤其是在最终成功率和AUC指标上。这证明了任务中心表示学习与知识空间联合演化机制的必要性。完整版的T-Stellar和TS-Stellar在所有指标上均取得了最佳或接近最佳的性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了Stellar VLA，一个通过联合学习任务中心表示和自演化知识空间来实现持续模仿学习的框架，并引入了面向任务层级和任务-技能层级的两种变体；2）设计了一种知识引导的专家路由机制，实现了无需增加网络参数的任务专业化，平衡了参数共享与隔离；3）在仿真和真实世界的持续学习任务上验证了框架的有效性，相比强基线取得了超过50%的平均性能提升。</p>
<p>论文自身也提到了局限性：TS-Stellar的层次任务-技能建模在简单任务场景中优势不明显；此外，基于DP的知识空间学习和MoE路由对计算资源有一定要求。</p>
<p>这项工作对后续研究有多方面启示：首先，将非参数贝叶斯模型与大规模VLA结合，为构建开放、自演化的机器人知识系统提供了新思路。其次，知识引导的专家路由机制表明，利用高层语义知识而不仅仅是低层特征（如噪声级别）进行动态网络架构决策，能更有效地服务于持续学习。未来研究可以探索更高效的知识空间学习算法，并将此框架扩展到更复杂的多模态交互和强化学习场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对Vision-Language-Action模型在持续技能学习中依赖任务特定微调、缺乏知识演化能力的问题，提出知识驱动框架Stellar VLA，包含T-Stellar（任务中心知识空间）和TS-Stellar（分层任务-技能结构）两种变体。其关键技术是通过联合学习任务潜在变量与知识空间实现自监督知识演化，并采用知识引导的专家路由进行任务专业化，无需额外参数。实验在LIBERO基准和真实任务中显示，相比基线平均成功率提升超过50%，TS-Stellar在复杂动作推理中表现更优，验证了有效的知识保留与发现。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.18085" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>