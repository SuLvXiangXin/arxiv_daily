<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Examining the legibility of humanoid robot arm movements in a pointing task - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Examining the legibility of humanoid robot arm movements in a pointing task</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.05104" target="_blank" rel="noreferrer">2508.05104</a></span>
        <span>作者: Lúčny, Andrej, Antonj, Matilde, Mazzola, Carlo, Hornáčková, Hana, Farić, Ana, Malinovská, Kristína, Vavrecka, Michal, Farkaš, Igor</span>
        <span>日期: 2025/08/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在流畅的人机交互场景中，机器人的动作需要具备可读性，以支持人类解释、预测机器人行为并感到安全。当前，机器人运动设计不仅需要追求到达目标的效率，其轨迹的清晰度——即帮助观察者消除机器人意图歧义的独特性——对于有效、安全和可解释的协作至关重要。现有研究广泛探讨了机器人视线和指向手势等非语言线索在沟通意图中的作用，但多数聚焦于完整的动作。对于不完整的动作，即观察者必须在动作完成前推断机器人目标的情况，特别是在结合多种线索（多模态）时，人类如何解读的研究尚不充分。</p>
<p>本文针对“人类如何从机器人未完成的动作中结合身体线索预测其意图”这一具体问题，提出了新的研究视角。通过系统性地操控机器人手臂运动的完成度（60% vs. 80%）以及社交线索（视线、指向）的一致性与组合方式，旨在量化机器人意图在不同条件下的可读性阈值。本文的核心思路是：人类通过整合视线（社交线索）和指向（空间线索）信息，能够更准确、更快速地预测机器人未完成指向动作的最终目标，从而验证多模态线索在提升机器人运动可读性方面的优越性以及视线线索在处理速度上的优先性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本研究设计了一个基于NICO人形机器人的实验，旨在探究参与者在机器人手臂运动完成前预测其指向目标的能力。整体实验流程如下：参与者首先完成前测问卷。在主任务中，他们观察NICO执行未完成的右臂指向轨迹（60%或80%全长）并可能伴随视线线索，然后在听到提示音后2秒内，在触摸屏上点击预测的目标位置。</p>
<p><img src="https://arxiv.org/html/2508.05104v1/x1.png" alt="实验设置与流程"></p>
<blockquote>
<p><strong>图1</strong>：左图：软件GUI中显示的实验设置——从左至右：摄像头1（左上）、摄像头2（右上）、机器人摄像头（左下）、控制屏幕（右下，蓝点代表机器人意图目标，黄点代表参与者预测）。右图：反映实验批次1的一般实验流程示意图。</p>
</blockquote>
<p>实验的核心模块是四种不同的机器人线索条件：</p>
<ol>
<li><strong>仅视线</strong>：机器人头部朝向一个目标。</li>
<li><strong>仅指向</strong>：机器人执行手臂指向运动（无特定头部朝向）。</li>
<li><strong>一致视线指向</strong>：机器人手臂指向与视线朝向同一目标。</li>
<li><strong>不一致视线指向</strong>：机器人手臂指向一个目标，而视线朝向一个略微偏移的位置。</li>
</ol>
<p>为了控制信息量的递增并平衡顺序效应，实验分为两个批次，每个参与者依次经历七个实验区块。批次顺序为：G， P60， GP60， GPi60， P80， GP80， GPi80；批次二顺序为：G， GP60， GPi60， P60， GP80， GPi80， P80。每个区块包含5组随机化的试验，每组试验针对触摸屏上7个预定义点之一。</p>
<p>技术细节上，机器人手臂轨迹（50步，7自由度）是使用一种基于梯度下降和正向运动学的新方法预先计算生成的，确保了从固定起始姿势到触摸屏上选定目标的精确、线性且可重复的运动。这构成了本实验的方法学基础，使得轨迹长度（60% vs. 80%）的操控精确可控。</p>
<p>与现有方法相比，本研究的创新点具体体现在：1) 系统研究了截断（未完成）的机器人手臂运动与多模态社交线索（视线）的交互对意图可读性的影响；2) 通过精心设计的实验条件（四种线索组合、两种轨迹长度）和平衡的实验流程，定量检验了“多模态优越性”和“眼动主导性”两个具体假设。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：本研究使用了NICO人形机器人平台和一个嵌入式触摸屏作为交互环境。共招募了28名年龄在18-35岁之间的参与者（11男，17女），所有流程均以参与者的母语（斯洛伐克语）进行以控制语言变量。</p>
<p><strong>评估指标与基线</strong>：主要评估指标为<strong>预测偏差</strong>和<strong>反应时间</strong>。预测偏差通过参与者点击位置与实际目标之间的欧氏距离（总偏差 <code>bias_tot</code>）来衡量，并分解为横向（x轴）和纵向（y轴）分量进行分析。反应时间定义为从提示音响起至参与者触摸屏幕的时间间隔。实验将不同线索条件（G， P60， P80， GP60， GP80）作为相互比较的“基线”。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>操控有效性检验</strong>：配对样本t检验证实，80%轨迹长度的预测总偏差（M=84.8mm）显著低于60%长度（M=121mm），<code>t(26)=8.78， p&lt;.001</code>，效应量 <code>d=1.69</code>。这表明更长的轨迹片段确实带来了显著更高的可读性。</li>
<li><strong>多模态优越性假设</strong>：线性混合模型显示实验条件对总偏差有显著影响（<code>F(4，5268)=479， p&lt;.001</code>）。事后比较发现，在60%和80%两种轨迹长度下，一致视线指向条件（GP）的总偏差均显著低于对应的仅指向（P）和仅视线（G）条件（所有 <code>p&lt;.001</code>），有力支持了H1假设。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.05104v1/Matilde/Bias_and_Reaction_Time_All_Conditions.png" alt="偏差与反应时间对比"></p>
<blockquote>
<p><strong>图3</strong>：a) 五种不同条件下的总偏差对比。圆点代表每个刺激下各参与者的总偏差。上方的水平条形图显示了经Bonferroni校正的事后配对比较发现的显著差异。b) 五种不同条件下的反应时间对比。图示结果支持多模态线索能提高预测准确性（a），而仅视线条件能带来最快的反应速度（b）。</p>
</blockquote>
<ol start="3">
<li><strong>眼动主导性假设</strong>：线性混合模型显示条件对反应时间有显著影响（<code>F(4，5268)=340， p&lt;.001</code>）。事后比较表明，仅视线条件（G）的反应时间显著短于所有其他条件（所有 <code>p&lt;.001</code>），包括一致视线指向条件（GP）。同时，一致视线指向条件的反应时间也显著短于对应的仅指向条件（<code>p&lt;.001</code>）。这支持了H2假设，即视线线索能引发最快的反应。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.05104v1/Matilde/New_Representation_Responses_and_Accuracy.png" alt="空间布局与预测示例"></p>
<blockquote>
<p><strong>图2</strong>：a) 七个目标点的空间布局（未向参与者显示）。实际目标显示为绿色，参与者预测响应为红色，其余非目标选项为白色。b) G和GP条件下的预测示例。对于每个刺激，计算了G、GP60和GP80条件下的平均预测位置。响应与刺激之间的平均距离即为偏差。图示揭示了不同条件下预测偏差的空间分布模式。</p>
</blockquote>
<ol start="4">
<li><strong>偏差方向分析</strong>：空间分解分析揭示了有趣的模式。在指向相关（P， GP）的试验中，参与者的预测在横轴（x轴）上表现出与机器人手臂方向相反的侧向偏差，表明参与者倾向于预测运动终点会超过实际目标。而在仅视线条件中未发现这种侧向偏差，表明视线线索有助于将注意力精确锚定在水平方向上。在纵轴（y轴）上，所有条件均表现出朝向机器人身体的偏差，表明参与者倾向于低估向前的触及距离。这表明视线线索有助于减轻预测偏差，特别是在横轴上。</li>
</ol>
<p><strong>消融实验</strong>：虽然未进行传统意义上的模块消融，但通过对比不同轨迹长度（60% vs. 80%）和不同线索组合（单模态 vs. 多模态）的效果，实质上评估了“信息量”和“线索类型”这两个核心组件的贡献。结果表明，更多的运动信息（更长轨迹）和更丰富的线索类型（视线+指向结合）均能独立且协同地提升预测准确性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>假设验证</strong>：通过受控实验，为“多模态优越性”和“眼动主导性”两个关于人机交互中非语言线索处理的假设提供了实证支持。证实结合一致的视线和指向线索能带来最准确的意图预测，而视线线索能触发最快的反应。</li>
<li><strong>机制揭示</strong>：深入分析了预测误差的空间分布，揭示了视线和指向线索在整合过程中扮演的不同角色：视线有助于精确定位水平方向，而手臂运动轨迹则影响了对运动终点的外推估计，两者的结合能有效减轻系统性预测偏差。</li>
<li><strong>方法学贡献</strong>：采用了一种能生成精确、可重复机器人手臂轨迹的方法，并结合平衡的实验设计，为可读性研究提供了可控、可重复的实验范式。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：参与者样本相对同质（年轻大学生），限制了结论的普适性；实验任务相对简单（静态指向），与更复杂的动态交互场景存在差距。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>建模方向</strong>：作者提出未来可开发贝叶斯模型来量化参与者如何整合视线和指向信号，这有助于更精确地理解各线索的权重和整合机制，并为机器人自适应地调整其沟通策略提供理论依据。</li>
<li><strong>场景扩展</strong>：研究可扩展到更复杂、动态的任务中，如移动中指向、物体递送或协作操作，以检验当前发现在更真实场景下的稳健性。</li>
<li><strong>个性化交互</strong>：理解不同线索的整合机制有助于设计能够根据上下文和人类伙伴状态（如专业知识、注意力水平）调整其沟通清晰度的机器人，从而促进更高效、更自然的混合人机环境下的共享理解。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本研究探讨人形机器人手臂在指向任务中的运动可读性，核心问题是人类如何从截断运动及身体线索中预测机器人意图。实验采用NICO机器人，通过变化注视、指向等线索组合，并在手臂轨迹完成60%或80%时停止，让参与者预测最终目标。测试了多模态优越性假设和眼动主导假设。结果表明，一致的注视-指向信号能显著提升目标预测准确性，且仅注视条件下的反应时间更短，两个假设均得到支持。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.05104" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>