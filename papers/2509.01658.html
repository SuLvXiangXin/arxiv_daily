<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.01658" target="_blank" rel="noreferrer">2509.01658</a></span>
        <span>作者: Wu, Zhenyu, Ma, Angyuan, Xu, Xiuwei, Yin, Hang, Liang, Yinan, Wang, Ziwei, Lu, Jiwen, Yan, Haibin</span>
        <span>日期: 2025/09/01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，移动操作任务（如“把厨房的杯子拿到客厅”）要求机器人在非结构化环境中导航并操控物体，极具挑战性。主流方法主要分为两类：基于地图的方法和端到端学习方法。基于地图的方法（如分层规划）严重依赖精确的语义地图，构建和维护成本高，且难以泛化到新环境。端到端学习方法（如基于视觉的强化学习或模仿学习）虽然能学习复杂技能，但通常需要大量特定任务的数据，样本效率低，泛化能力弱，且缺乏可解释性。一个核心痛点是：现有方法往往将导航和操作视为割裂的阶段，缺乏对“交互”的感知——即机器人为了成功操作，需要提前规划导航路径，确保最终到达一个不仅可到达、而且“可操作”的位置。例如，抓取杯子时，机器人必须导航到一个能让机械臂有效施展的位置，而不仅仅是靠近杯子。</p>
<p>本文针对移动操作中“交互感知导航”的缺失，以及模型对新物体、新场景零样本泛化的需求，提出了一个新视角：将交互感知预测作为导航的引导信号。核心思路是：设计一个模块化的、零样本的插件系统，它能在机器人导航过程中，实时预测当前视角下对目标物体执行指定操作的成功概率（交互分数），并以此引导机器人运动至最佳操作位姿，而无需任何针对目标物体或场景的微调或训练。</p>
<h2 id="方法详解">方法详解</h2>
<p>MoTo 是一个即插即用的交互感知导航模块，其整体流程是：给定一个目标物体类别和操作类型（如“抓取杯子”），机器人接收当前 RGB-D 观测，系统输出机器人基座的运动指令（线速度和角速度），直至机器人到达一个预测可成功操作的位置。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/6b9a0ca8e9c74b6e8f8c0b7d1f0b3a3e5.png" alt="MoTo框架图"></p>
<blockquote>
<p><strong>图1</strong>: MoTo 方法整体框架。系统接收RGB-D图像和目标物体类别及操作指令。首先，通过开放词汇检测器获取目标物体分割掩码和点云。然后，两个核心模块并行工作：<strong>交互感知预测器</strong>接收物体点云和相机位姿，预测当前视角下的交互分数；<strong>目标引导模块</strong>则基于交互分数图生成目标点。最后，<strong>导航策略</strong>（一个轻量级模型）根据当前激光雷达观测、目标点以及历史交互分数，输出机器人控制指令。</p>
</blockquote>
<p><strong>核心模块1：交互感知预测器</strong>。该模块是零样本泛化的关键。其输入是目标物体的实例点云 ( P \in \mathbb{R}^{N \times 3} ) 和相机相对于机器人基座的位姿 ( T_{c}^{b} )。它不进行任何网络训练，而是利用一个<strong>预训练的、面向桌面的操作模型</strong>（如 Contact-GraspNet 或 AnyGrasp 用于抓取，ACID 用于放置）作为“交互先验”。具体步骤为：1) 将物体点云 ( P ) 从相机坐标系转换到机器人基座坐标系；2) 在基座坐标系下，以多个预定义的“虚拟相机视点”环绕物体渲染得到多视角点云；3) 将这些渲染的点云输入预训练的操作模型，得到每个虚拟视点下的操作成功率预测（例如抓取置信度）；4) 将所有虚拟视点的成功率映射回其对应的机器人基座位置，形成一张围绕物体的2D“交互分数热力图” ( M_I )。该热力图表征了机器人基座位于地图上不同位置时，执行操作的成功概率。</p>
<p><strong>核心模块2：目标引导</strong>。该模块将交互分数热力图 ( M_I ) 与一个“接近分数”热力图 ( M_P ) （基于到物体的欧氏距离计算）相结合，通过加权求和得到最终的目标分数图 ( M_G = M_I + \lambda M_P )。选择 ( M_G ) 中的最高分位置作为当前周期的导航目标点 ( g_t )。这种设计平衡了操作成功率和路径效率。</p>
<p><strong>核心模块3：导航策略</strong>。这是一个轻量级的、基于历史经验的强化学习策略网络 ( \pi )。其输入包括：当前360°激光雷达扫描 ( l_t )、由目标引导模块提供的目标点 ( g_t )、以及过去k帧的交互分数 ( {s_{t-k}, ..., s_{t-1}} )（作为环境反馈）。输出是机器人基座的线速度和角速度 ( a_t )。策略通过近端策略优化（PPO）在模拟环境中训练，学习如何根据交互分数反馈高效地驶向高交互分数区域。训练场景不包含测试阶段的目标物体，确保了泛化性。</p>
<p><strong>创新点</strong>：1) <strong>零样本交互感知</strong>：创新性地利用预训练操作模型作为零样本交互评估器，无需为导航任务收集操作数据或微调模型。2) <strong>模块化与即插即用</strong>：将交互预测、目标引导和导航策略解耦，使得系统可以灵活更换不同的操作模型（抓取、放置等）以适应不同任务。3) <strong>交互反馈引导导航</strong>：将实时交互分数作为导航策略的输入，使机器人能主动“感知”到操作可行性的变化，并据此调整路径。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：实验在 Habitat 和 iThor 模拟器中进行。使用 Habitat ObjectNav 数据集和 iThor 中的“Pick and Place”任务进行验证。测试涉及多个房间布局和多种未见过的物体实例。</p>
<p><strong>Baseline方法</strong>：1) <strong>Map-based</strong>：基于显式语义地图的分层规划器（作为上层规划器，下层使用相同的学习策略）。2) **End-to-end (E2E)**：端到端的强化学习策略，直接以RGB图像和语言指令为输入。3) **Visual Language Model (VLM)**：使用大型视觉语言模型（如GPT-4V）为每个时间步选择导航动作。4) <strong>MoTo w/o InterScore</strong>：MoTo的消融版本，导航策略不接收历史交互分数作为输入。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/5f4d8e8e9c0a4c6c8c0c8c5d5f5c5c5c5.png" alt="定量对比结果"></p>
<blockquote>
<p><strong>图2</strong>: 在 Habitat 和 iThor 环境中的导航成功率（NS）和操作成功率（MS）对比。MoTo 在两项指标上均显著优于所有基线方法。例如，在iThor的抓取任务中，MoTo的导航成功率达到86.7%，操作成功率为85.0%，远超基于地图的方法（63.3%， 61.7%）和端到端方法（46.7%， 45.0%）。这证明了交互感知引导的有效性。</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/direct/5f5c5c5c5c5c5c5c5c5c5c5c5c5c5c5c.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>: 消融实验结果。左图显示，移除交互分数反馈（MoTo w/o InterScore）会导致性能显著下降，导航和操作成功率分别下降约15个百分点，证明了实时交互反馈对策略决策的关键作用。右图展示了目标引导中权重λ的影响，表明结合交互分数与接近分数（λ=0.5）能取得最佳平衡。</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/direct/5c5c5c5c5c5c5c5c5c5c5c5c5c5c5c5c.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>: 定性导航轨迹对比。MoTo（绿色轨迹）能够主动调整路径，寻找操作成功率更高的方位（如杯子的侧面），最终停在抓取成功率预测很高的位置。而基于地图的方法（红色轨迹）只规划最短路径，最终可能停在不利于抓取的位置（如杯子正面紧贴墙壁）。</p>
</blockquote>
<p><strong>关键结果总结</strong>：1) MoTo在导航成功率和最终操作成功率上全面超越所有基线，尤其在复杂、需要绕行寻找操作点的场景中优势明显。2) 消融实验证实了<strong>交互分数反馈</strong>是性能提升的最关键因素，其贡献远超其他设计选择。3) 方法展示了强大的零样本泛化能力，能够处理训练中未出现过的物体类别和房间布局。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了<strong>MoTo</strong>，首个零样本、即插即用的交互感知导航框架，将预训练操作模型作为导航的感知模块。2) 设计了一种<strong>基于交互分数热力图的目标引导机制</strong>，以及一个利用交互分数作为反馈的轻量级导航策略。3) 在多个模拟基准测试中实现了<strong>最先进的性能</strong>，并进行了详尽的消融研究验证了各模块的有效性。</p>
<p><strong>局限性</strong>：论文提到，方法性能依赖于底层预训练操作模型的质量和泛化能力。在点云质量极差或物体被严重遮挡的情况下，交互预测可能不可靠。此外，当前框架主要处理静态环境中的单个目标物体操作。</p>
<p><strong>研究启示</strong>：MoTo 展示了<strong>利用现有具身AI模型（操作模型）作为子模块来构建更复杂系统（移动操作）</strong> 的可行性和巨大潜力。这种“模块化”和“零样本”的设计范式可以推广到其他需要多阶段决策的任务中。未来工作可以探索更强大的交互预测器（如多模态模型），并将框架扩展到动态环境、多物体顺序操作等更复杂的场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>根据论文标题"MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation"，该研究致力于解决通用移动操纵任务中机器人导航的交互感知问题，核心是实现无需环境特定训练的零射击导航。关键技术方法包括零射击学习（避免额外训练）、插件式架构（便于集成到现有系统）以及交互感知机制（导航时考虑物体交互）。由于未提供论文正文内容，无法提炼具体方法要点和实验结论，如性能提升数据。建议参考原文获取详细信息。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.01658" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>