<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.13761" target="_blank" rel="noreferrer">2506.13761</a></span>
        <span>作者: Wei-Chiu Ma Team</span>
        <span>日期: 2025-06-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>开放世界的机器人操作要求机器人能够根据自由形式的语言指令，在非结构化环境中执行新任务。视觉语言模型（VLM）在高级语义推理方面表现出色，但缺乏精确低级控制所需的细粒度物理洞察力。当前主流方法主要依赖VLM来直接预测动作或中间表示（如价值图、关键点），或将任务分解为预定义的推理链。这些方法要求VLM隐式地进行物理推理，而VLM由于训练数据分布（主要是视觉问答）的差异，在处理涉及复杂接触、动力学和运动的场景时往往表现不佳。</p>
<p>本文针对VLM物理推理能力不足这一核心痛点，提出了一个根本性的新视角：将语义理解与物理推理解耦。具体思路是构建一个交互式数字孪生来显式地模拟物理动态，生成不同动作序列下的未来状态，并将其渲染为视觉提示供VLM评估，从而让VLM专注于其擅长的语义理解和评估任务。本文核心思路可概括为：通过构建交互式数字孪生来模拟动作的未来结果，并将这些模拟的未来状态作为视觉提示，引导VLM进行基于采样的模型预测控制规划。</p>
<h2 id="方法详解">方法详解</h2>
<p>PWTF框架旨在通过数字孪生对动力学的显式建模来补充VLM的语义推理能力。整体框架包含两个关键部分：1) 自动构建支持精确物理交互模拟和逼真渲染的交互式数字孪生；2) 通过用数字孪生提供的“未来”提示VLM，将开放世界操作表述为一个模型预测控制问题，实现自适应观察和动作优化。</p>
<p><img src="https://arxiv.org/html/2506.13761v1/x3.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图3</strong>：基于模拟信息提示的模型预测控制。给定自由形式的指令，框架首先通过多视角观察进行高级规划，生成结构化的子任务。在每个步骤中，交互式数字孪生模拟候选动作的未来状态并渲染结果。VLM自适应地选择信息量最大的视角进行渲染，并评估预测结果以进行基于采样的运动规划。</p>
</blockquote>
<p><strong>核心模块1：交互式数字孪生的构建</strong><br>如图2所示，构建流程从对真实环境的手持视频扫描开始，目标是创建一个结合了基于网格的物理模拟和基于高斯泼溅的高效渲染的混合表示。</p>
<ol>
<li><strong>混合重建</strong>：使用2D高斯泼溅从RGB帧创建高斯表示G，然后通过TSDF体积积分将其转换为网格表示M，以获得精确的几何形状。</li>
<li><strong>交互对象识别</strong>：使用VLM（Molmo）根据任务指令识别关键可移动物体，结合SAM2进行跨视角的2D掩码跟踪，并将2D掩码投影回3D，从网格M中分割出可移动物体网格。</li>
<li><strong>逼真渲染</strong>：将每个高斯点锚定到最近的可移动物体网格上。在模拟过程中，高斯点继承其对应网格的平移和旋转，确保渲染外观与物体运动一致。</li>
<li><strong>物理模拟</strong>：集成物理模拟器S和机器人URDF模型U，以模拟交互下的动力学。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.13761v1/x2.png" alt="数字孪生构建"></p>
<blockquote>
<p><strong>图2</strong>：交互式数字孪生的构建。从环境的视频扫描开始，我们构建了一个结合基于网格的模拟和高斯渲染的交互式数字孪生。最终的数字孪生能够根据机器人动作进行逼真渲染和精确的物体动力学模拟。</p>
</blockquote>
<p><strong>核心模块2：用于视觉提示的模拟与渲染</strong><br>给定一个机器人动作a，模拟器S预测网格表示M内的变换T = S(M, a)。将此变换T应用于高斯场景表示G，得到更新状态G(T)。为了增强VLM的3D理解，从一组候选相机配置C中渲染背景场景图像Ig = Render(G(T), C)，同时单独渲染执行动作a的机器人图像Ir = Render(S(U, a))。最后基于深度信息合成背景和机器人图像，生成最终的多视角观察I，作为VLM评估未来状态的视觉提示。</p>
<p><strong>核心模块3：基于模拟信息提示的运动规划</strong><br>该部分将控制问题构建为模型预测控制框架，VLM仅作为未来状态的评估器。</p>
<ol>
<li><strong>子目标分解</strong>：利用VLM的语义推理能力，根据初始多视角观察I0和任务指令l，将复杂任务分解为结构化子任务序列τ。在每个规划步骤t，VLM根据当前观察It和子任务集τ选择当前应完成的活跃子目标τi。</li>
<li><strong>自适应视角选择</strong>：针对当前活跃子目标τi和当前观察It，VLM从一组候选相机配置C中选择能最有效区分动作结果的视角Ct进行渲染，以强化VLM对几何和物理变化的推理能力。</li>
<li><strong>基于采样的规划</strong>：采用交叉熵方法进行动作优化。首先从多元高斯分布中采样候选动作{ak}，在数字孪生中模拟得到对应的未来观察{ot,k}。将模拟结果分组提示给VLM，VLM选择最有利于推进子目标τi的结果，其对应的“精英”动作用于更新采样分布。此过程迭代三次，最终分布的均值被选为优化后的动作at。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，PWTF的主要创新在于：1) <strong>角色转换</strong>：不要求VLM直接预测动作或推理物理结果，而是让其作为“评论家”评估由数字孪生显式模拟出的未来状态。2) <strong>物理接地</strong>：通过交互式数字孪生提供精确、可操控的物理动力学模型，弥补了VLM的短板。3) <strong>自适应感知</strong>：引入了由VLM驱动的自适应视角选择机制，优化了提供给VLM的视觉输入。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>任务与数据集</strong>：设计了八个需要6自由度控制、语义理解和多样化操作技能的真实世界操作任务，包括“给植物浇水”、“打鼓”、“按空格键”、“配对鞋子”等（定义见表I）。每个任务构建了5个不同的操作场景（随机化物体布局和干扰物），共进行10次试验。</li>
<li><strong>对比基线</strong>：包括Voxposer（预测3D价值图）、MOKA（预测2D关键点）、OpenVLA（基于大规模机器人演示微调的VLA模型）、π0（在多样化机器人演示上训练的VLA模型）。对于OpenVLA和π0，报告了零样本和使用每个任务20个专家演示进行任务特定微调后的性能。</li>
<li><strong>评估指标</strong>：使用成功率。如果机器人导致不可逆结果或达到最大步数/时间限制，则视为失败。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>表II展示了PWTF与基线方法的成功率对比。PWTF在大多数任务上取得了显著更高的成功率。例如，在“按空格键”任务上，PWTF成功率为5/10，而Voxposer和MOKA为0/10；在“播放最低音调”任务上，PWTF为4/10，其他基线均为0/10。PWTF在需要精确夹爪姿态对齐的任务（如“给植物浇水”、“配对鞋子”）上表现尤为出色。</p>
<p><img src="https://arxiv.org/html/2506.13761v1/x6.png" alt="定量结果表"></p>
<blockquote>
<p><strong>表II</strong>：与基线的对比。PWTF更好地利用了VLM的推理能力，在大多数任务上提高了性能。此外，基于图像的结果评估比价值图或关键点提供了更大的灵活性，使得处理难以参数化的挑战性任务成为可能。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>表III展示了消融研究结果，验证了各个组件的贡献。</p>
<ul>
<li><strong>移除多视角观察（w/o Views）</strong>：性能大幅下降，特别是对视角敏感的任务（如“打鼓”、“播放最低音调”成功率降至0），说明多视角观察对于空间推理至关重要。</li>
<li><strong>移除子任务分解（w/o Subtasks）</strong>：在需要多阶段规划的任务（如“打鼓”、“清洁”）上性能下降，说明子任务分解提高了规划鲁棒性。</li>
<li><strong>移除CEM优化（w/o CEM）</strong>：几乎所有任务性能都急剧下降（多数为0或个位数成功率），表明基于CEM的采样优化对于动作搜索效率至关重要。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.13761v1/x7.png" alt="消融实验表"></p>
<blockquote>
<p><strong>表III</strong>：消融研究。我们验证了各组件的有效性。多视角观察对于视角敏感的任务至关重要。子任务划分提高了需要多阶段规划的任务的性能。最重要的是，CEM显著提高了采样效率，促进了有效规划。</p>
</blockquote>
<p><strong>定性结果分析</strong>：<br>图4展示了“清洁”任务中单步动作优化的过程。数字孪生能够模拟具有精确动力学（如抓取导致的物体旋转、碰撞导致的物体掉落）的多样化动作，VLM能有效选择更符合目标的结果，并通过迭代优化动作分布。</p>
<p><img src="https://arxiv.org/html/2506.13761v1/x4.png" alt="动作优化示例"></p>
<blockquote>
<p><strong>图4</strong>：动作优化示例。我们展示了一个规划步骤中“擦拭洒出的茶”子任务的动作优化结果。我们的数字孪生可以在初始采样中模拟海绵具有精确运动和碰撞的多样化结果，VLM可以有效地优化动作分布，使海绵移向茶渍。</p>
</blockquote>
<p>图5展示了在数字孪生和真实世界（已对齐）中规划的示例轨迹，其中VLM自适应地改变渲染视角以更好地评估结果（例如，从不同角度对齐夹爪以进行抓取、按压、放置或敲击）。</p>
<p><img src="https://arxiv.org/html/2506.13761v1/x5.png" alt="示例轨迹"></p>
<blockquote>
<p><strong>图5</strong>：示例轨迹。我们的框架在数字孪生和真实世界（对齐）中规划的示例轨迹。我们突出显示了一些关键步骤，在这些步骤中，VLM选择自适应地改变渲染视图以进行更好的结果评估。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了PWTF框架</strong>：一种新颖的模型预测控制框架，通过构建交互式数字孪生来显式模拟物理动态，生成“未来”视觉提示，使VLM能够专注于语义评估，从而将开放世界操作转化为VLM的分布内任务。</li>
<li><strong>提出了交互式数字孪生的构建方法</strong>：结合高斯泼溅（逼真渲染）和网格表示（精确物理），并实现了高斯点与可移动物体网格的动态绑定，支持交互式、动作条件下的场景模拟。</li>
<li><strong>引入了自适应视角选择机制</strong>：由VLM根据子任务动态选择最具信息量的渲染视角，增强了VLM在2D图像上进行3D空间推理的能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，PWTF的性能依赖于数字孪生模拟的准确性，复杂的物体材质特性（如非刚性变形）和非常精细的几何结构可能难以完美建模。此外，基于采样的规划和模拟渲染过程计算成本较高。</p>
<p><strong>对后续研究的启示</strong>：<br>PWTF展示了一条将基础模型（VLM）与经典机器人技术（物理模拟、MPC）深度融合的有效路径。后续工作可以探索：1) 更高效、更精确的数字孪生构建与模拟技术；2) 将学习的世界模型与物理引擎结合，以处理更复杂的物理现象；3) 优化规划算法，降低计算开销，以实现更快的闭环控制。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对开放世界机器人操作中，视觉语言模型（VLM）语义推理强但缺乏精细物理控制的问题，提出PWTF框架。该方法通过手持视频快速构建交互式数字孪生，模拟候选动作的未来状态，并自适应选择最具信息量的视角将其渲染为视觉提示，再由VLM评估并选择最优动作序列。在8个涉及接触、重定向和清理的真实任务中，PWTF相比现有VLM控制方法取得了显著更高的成功率，证明了融合显式物理建模与VLM语义优势的必要性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.13761" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>