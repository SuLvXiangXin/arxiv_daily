<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LiPo: A Lightweight Post-optimization Framework for Smoothing Action Chunks Generated by Learned Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>LiPo: A Lightweight Post-optimization Framework for Smoothing Action Chunks Generated by Learned Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.05165" target="_blank" rel="noreferrer">2506.05165</a></span>
        <span>作者: Suhan Park Team</span>
        <span>日期: 2025-06-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>近年来，模仿学习（IL）和强化学习（RL）的进展使机器人能够执行越来越复杂的操作任务。其中，基于Transformer的动作分块（ACT）等方法显著提升了机器人处理长时程、可变形和接触丰富任务的能力。然而，大多数习得的策略依赖于离散的动作分块，这会在分块边界处引入不连续性。这些不连续性会降低运动质量，在诸如投掷或举重等动态任务中尤其成问题，因为平滑的轨迹对于动量传递和系统稳定性至关重要。</p>
<p>现有的时间集成（TE）方法通过在每一步查询策略并对重叠预测进行加权平均来改善平滑度，但这需要每一步都进行策略推理，计算开销大，并且对于多模态策略，加权平均可能导致混合出模糊或物理上不可行的轨迹。本文针对动作分块策略在边界处不连续、以及推理延迟导致执行停顿这两个具体痛点，提出了一种轻量级的后优化框架来平滑分块动作序列。其核心思路是：主动生成重叠分块以规避推理延迟，在重叠区域进行线性混合以提供平滑过渡，并在有界扰动约束下进行加加速度最小化轨迹优化。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的LiPo是一个两阶段的后优化框架，旨在对离散的动作分块输出进行局部细化，以实现平滑过渡。整体流程是：首先，考虑到推理延迟，构建一个线性混合的参考轨迹；然后，对该参考轨迹施加有界扰动，通过优化最小化加加速度，得到最终平滑轨迹。</p>
<p><img src="https://arxiv.org/html/2506.05165v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：考虑时间延迟和线性混合的参考轨迹构建概念图。橙色阴影区域代表线性插值区域，而浅蓝色阴影区域表示推理延迟窗口，在此窗口内不进行混合。</p>
</blockquote>
<p><strong>1. 考虑推理时间延迟的参考轨迹构建</strong><br>为了主动避免因推理计算导致的执行停顿，方法会提前生成下一个动作分块，并与当前执行的分块在时间上重叠。参考轨迹 ( q_{\text{ref}}(t) ) 的构建分为三个区域：</p>
<ul>
<li><strong>延迟区</strong> ( [0, t_d] )：完全执行上一分块优化后的轨迹 ( q_{\text{past}}(t) )。</li>
<li><strong>混合区</strong> ( (t_d, t_b] )：对上一分块轨迹 ( q_{\text{past}}(t) ) 和新预测分块轨迹 ( q_{\text{new}}(t) ) 进行线性插值。混合权重 ( \alpha(t) = (t - t_d) / (t_b - t_d) ) 从0线性增长到1。</li>
<li><strong>新区</strong> ( (t_b, T] )：完全执行新分块轨迹 ( q_{\text{new}}(t) )。<br>其中 ( t_d ) 是估计的推理延迟时间，( t_b ) 是混合区结束时间，( T ) 是动作段总时长。</li>
</ul>
<p><strong>2. 最小加加速度分块后优化</strong><br>在获得混合参考轨迹后，通过优化一个小的关节空间扰动 ( \epsilon(t) )，使最终轨迹 ( q_{\text{ref}}(t) + \epsilon(t) ) 的加加速度（jerk，即加速度的三阶导数）最小化。优化问题如下：<br>[<br>\min_{\epsilon} \int_{0}^{T} \left| \frac{d^3}{dt^3}(q_{\text{ref}}(t) + \epsilon(t)) \right|^2 dt<br>]<br>[<br>\text{s.t. } \epsilon(t) = \mathbf{0}, \quad \forall t \in [0, t_d]<br>]<br>[<br>| \epsilon(t) |_{\infty} \leq \bar{\epsilon}<em>b, \quad \forall t \in (t_d, t_b]<br>]<br>[<br>| \epsilon(t) |</em>{\infty} \leq \bar{\epsilon}_p, \quad \forall t \in (t_b, T]<br>]<br>关键约束包括：在延迟区不允许扰动；在不确定性较高的混合区允许较大的扰动边界 ( \bar{\epsilon}_b )；在分块路径主体部分使用更严格的扰动边界 ( \bar{\epsilon}_p ) 以保持任务意图。优化问题被表述为一个简单的框约束二次规划（QP），以保证轻量化和求解速度。</p>
<p><strong>3. 高频控制的插值</strong><br>为了在高速位置控制系统中平滑执行，对优化后的低频率动作样本使用五次样条插值，以保证位置、速度和加速度的连续性，避免控制信号振荡。</p>
<p><strong>4. 任务空间误差边界分析</strong><br>论文还从理论上分析了关节空间扰动对任务空间（如末端执行器位置）的影响。通过机器人雅可比矩阵的一阶近似，给出了任务空间偏差的上界 ( \delta x_{\mathrm{max}}(t) = |J(q(t))| \cdot \bar{\epsilon} )，为安全性提供了原则性度量。</p>
<p>与现有方法相比，LiPo的创新点在于：1）<strong>推理感知</strong>：主动调度重叠分块以隐藏延迟；2）<strong>两阶段平滑</strong>：先线性混合提供良好初值，再加加速度优化进行精细化；3）<strong>轻量化设计</strong>：采用 ( \ell_{\infty} ) 范数约束形成简单QP问题，并有意不包含复杂的动力学约束以保持时序一致性和计算效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用ROBOTIS OpenManipulator-Y机械臂，在位置控制模式下进行实验，控制频率为400 Hz。视觉输入为30 Hz的单目RGB相机，并采用镜面设置提供隐式立体感知。策略推理频率为30 Hz。后优化问题使用Clarabel求解器。对比的基线方法包括：原始动作分块输出（Raw action）、以及结合了线性或五次样条插值的情况。同时尝试了时间集成（TE）方法但无法执行。</p>
<p><strong>任务</strong>：在两个需要动态运动的投掷任务上评估方法：1）袋装果汁抛投任务（图5）；2）球类抛投入篮任务（图1）。</p>
<p><img src="https://arxiv.org/html/2506.05165v1/extracted/6516387/figures/juice_pouch_throw.jpg" alt="定性结果1"></p>
<blockquote>
<p><strong>图5</strong>：袋装果汁抛投任务。机器人抓取桌上的果汁袋并将其抛入箱子中。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.05165v1/x4.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图6</strong>：使用LiPo进行动作分块平滑的可视化。彩色段代表策略预测的不同动作分块。虚线显示后优化前的原始动作轨迹。点划线表示重叠区域内的线性混合轨迹。阴影区域突出了ε约束的扰动限制。实线代表后优化后的分块轨迹，白色实线表示最终执行的轨迹。黄色阴影区域代表应用局部平滑的混合区。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.05165v1/x5.png" alt="轨迹对比"></p>
<blockquote>
<p><strong>图7</strong>：使用和不使用LiPo方法的关节位置、速度和加速度轨迹对比。LiPo显著减少了突变，产生了更平滑稳定的运动。图7(a)为使用LiPo后的轨迹，图7(b)为原始动作轨迹。</p>
</blockquote>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>定性效果</strong>：如图6和图7所示，LiPo能够有效平滑原始动作分块之间的明显不连续性，显著减少了关节速度、加速度的突变，从而降低了机械振动。</li>
<li><strong>定量性能（球类抛投任务）</strong>：</li>
</ol>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">插值方式</th>
<th align="left">成功率</th>
</tr>
</thead>
<tbody><tr>
<td align="left">LiPo (Ours)</td>
<td align="left"><strong>Quintic</strong></td>
<td align="left"><strong>90%</strong></td>
</tr>
<tr>
<td align="left">LiPo (Ours)</td>
<td align="left">Linear</td>
<td align="left">60%</td>
</tr>
<tr>
<td align="left">Raw action</td>
<td align="left">Quintic</td>
<td align="left">80%</td>
</tr>
<tr>
<td align="left">Raw action</td>
<td align="left">Linear</td>
<td align="left">70%</td>
</tr>
<tr>
<td align="left">TE</td>
<td align="left">-</td>
<td align="left">- (无法执行)</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：使用与不使用LiPo方法的球类抛投任务成功率对比。</p>
</blockquote>
<p>表1显示，LiPo结合五次样条插值取得了最高的90%成功率。相比之下，仅对原始动作进行线性插值成功率为70%，五次样条插值为80%。而LiPo结合线性插值成功率仅为60%，说明在动态任务中，优化后使用高阶（五次）样条插值对于保持动态有效性至关重要。时间集成（TE）方法则因机器人在高速投掷运动中变得不稳定而无法完成任务。</p>
<p><strong>消融实验</strong>：实验结果本身包含了不同组件的组合对比。从表1可以总结出各部分的贡献：1）<strong>后优化框架（LiPo）本身</strong>：对比“Raw action + Quintic”（80%）和“LiPo + Quintic”（90%），带来了10个百分点的成功率提升，证明了平滑优化的有效性。2）<strong>插值方法的选择</strong>：对于优化后的轨迹，五次样条插值比线性插值成功率高出30个百分点，凸显了高阶连续性对动态任务的重要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>轻量级后优化框架（LiPo）</strong>，通过结合推理感知的分块调度、线性混合和加加速度最小化优化，能够平滑由学习策略生成的分块动作序列，而无需修改底层策略结构。</li>
<li>引入了<strong>混合区域</strong>和有针对性的有界扰动约束，确保分块边界处的平滑过渡，同时通过任务空间误差分析为安全性提供了依据。</li>
<li>通过<strong>真实机器人实验</strong>验证了方法的有效性，表明其能显著减少位置控制机器人的振动和运动抖动，提高动态任务（如投掷）的成功率。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，为了保持轻量化和时序一致性，优化中<strong>未包含机器人动力学约束</strong>（如扭矩、速度限制）。如果策略本身产生的动作违反了机器人的动态可行性，后优化可能无法纠正。</p>
<p><strong>启示</strong>：LiPo作为一种与策略无关的后处理模块，为提升基于学习（尤其是分块式）策略的物理执行质量提供了一条实用路径。它启示后续研究可以探索将类似的平滑机制与策略训练过程更紧密地结合，或者研究在保持轻量化的同时，以某种形式引入部分动力学感知的优化约束。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LiPo轻量级后优化框架，解决模仿学习策略因离散动作分块导致的轨迹不连续问题，该问题在投掷等动态任务中会严重影响运动平滑性与系统稳定性。方法核心包含：推理感知的分块调度（生成重叠块以规避推理延迟）、重叠区域线性混合、有界扰动空间内的最小化急动度轨迹优化。在位置控制机械臂上的实验表明，该方法显著降低了振动与运动抖动，提升了执行平滑度与机械鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.05165" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>