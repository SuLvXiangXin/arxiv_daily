<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.07439" target="_blank" rel="noreferrer">2602.07439</a></span>
        <span>作者: Xie, Weiji, Zheng, Jiakun, Han, Jinrui, Shi, Jiyuan, Zhang, Weinan, Bai, Chenjia, Li, Xuelong</span>
        <span>日期: 2026/02/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通用人形机器人全身运动跟踪系统已能稳定执行多种协调动作，但其驱动方式主要依赖两种范式：一是预定义的运动轨迹，这种方式在用户意图改变时灵活性有限；二是持续的人类遥操作，这需要人力持续介入，限制了机器人的自主性。因此，如何以实时、交互的方式驱动通用人形控制器成为一个开放挑战。自然语言作为一种强大的意图表达模态，在文本驱动的运动生成领域展现出潜力，但现有方法多为离线生成完整序列，或在执行期间不支持修改指令，缺乏与实时物理机器人控制的整合。本文旨在填补交互式语言意图表达与实时、可物理执行的人形控制之间的空白。</p>
<p>本文提出了TextOp，一个实时文本驱动的人形机器人运动生成与控制框架。其核心思路是采用一个两层架构：高层是一个自回归的、文本条件的运动扩散模型，持续生成短时域的运动轨迹；低层则是一个鲁棒的全身运动跟踪策略，将这些轨迹在物理机器人上执行，从而实现流式语言命令的实时交互控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>TextOp的框架将任务分解为交互式运动生成和动态运动跟踪两个层次。</p>
<p><img src="https://arxiv.org/html/2602.07439v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：TextOp框架概览。包含三个主要部分：(a) 交互式运动生成，包括VAE和LDM训练，以前瞻的自回归方式建模基于历史运动和文本提示的未来参考运动序列；(b) 动态运动跟踪，其中基于MLP的策略π根据参考运动和机器人状态生成关节动作，在仿真中训练以实现稳定执行；(c) 部署，实时用户文本提示由生成器转换为运动，跟踪策略根据机器人状态将其转化为动作，并在物理机器人上执行。</p>
</blockquote>
<p><strong>整体流程</strong>：在离散时间步t，系统接收语言命令l_t以及先前的机器人状态x_{&lt;t}^{robot}。高层运动生成器G以自回归方式，基于过去T_history=2帧的参考运动x_{t-T_history:t-1}^{ref}和当前语言命令l_t，生成未来T_future=8帧的参考运动序列x_{t:t+T_future-1}^{ref}。低层跟踪策略π随后基于先前的机器人状态和动作，以及未来T_ref=5帧的参考运动，生成可执行的控制信号a_t，确保执行的运动紧密跟随参考轨迹并保持稳定平滑。</p>
<p><strong>核心模块1：交互式运动生成</strong><br>该模块的核心是提出了一种针对机器人骨架的运动表示。与SMPL等具有3自由度球关节的人体骨架不同，机器人骨架通常采用单自由度旋转关节。因此，本文设计了一种基于自由度的局部增量运动表示，其特征向量f_t包含根方向的正弦余弦编码、根偏航角增量、脚部接触指示器、在偏航对齐局部坐标系中的根平移增量、根高度、关节位置及其增量。这种表示自然地强制执行了机器人运动学约束，并确保了相对于全局姿态的不变性。<br>模型架构上，生成器G采用VAE与潜在扩散模型结合的Transformer架构。VAE将未来运动帧编码为基于运动历史的潜在变量，并解码以重建未来帧。LDM则对文本条件下的未来运动潜在分布进行建模，使用预训练的CLIP编码器嵌入语言命令，并通过扩散Transformer以DDPM风格从噪声中预测干净的潜在表示。推理时应用分类器无关引导以增强语义对齐。训练过程采用自展开策略，并随机置零文本嵌入以训练分类器无关引导。</p>
<p><strong>核心模块2：动态运动跟踪</strong><br>低层通用运动跟踪器通过在大规模物理仿真中的目标条件强化学习进行训练。策略基于MLP，输入包括机器人的本体感知状态以及未来T_ref=5帧的参考运动。使用PPO算法进行优化，奖励信号结合了跟踪目标和正则化项。<br>一个关键创新是<strong>通过运动生成进行数据增强</strong>。为了减小运动数据集与高层生成器在线产生的参考运动之间的分布差距，使用生成器基于从BABEL训练集中随机采样的文本流来生成运动片段，并将这些生成的运动添加到跟踪策略的训练数据中。这使策略能够暴露于生成器输出的真实可变性和噪声，从而提升了部署时的跟踪性能。</p>
<p><strong>创新点</strong>：与现有方法相比，TextOp的主要创新在于：1) 首次将自回归、流式文本交互的运动生成与物理人形机器人的全身控制桥接起来，实现了实时指令修改；2) 提出了专为机器人结构设计的运动表示，提升了生成质量；3) 通过生成器进行数据增强，有效对齐了生成模块与控制模块，提高了系统鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实机器人（Unitree G1）和仿真中系统评估TextOp，使用了AMASS和BABEL等公开数据集及私有数据。评估指标涵盖运动质量（FID、多样性、R-precision等）和跟踪保真度（成功率、MPJPE等）。</p>
<p><strong>真实世界部署</strong>：<br><img src="https://arxiv.org/html/2602.07439v1/x4.png" alt="连续技能执行"></p>
<blockquote>
<p><strong>图4</strong>：真实机器人连续执行多样化技能。机器人无缝执行了多种任务，包括多种舞蹈风格、动态跳跃行为、演奏乐器的动作和富有表现力的手势。<br>定性演示显示机器人能执行连续长时域任务，并在单一不间断的试验中平滑过渡多种行为（如舞蹈、跳跃），支持执行中实时修改命令。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07439v1/x5.png" alt="抗扰动恢复"></p>
<blockquote>
<p><strong>图5</strong>：外部扰动下的实时恢复。机器人根据扰动状态动态调整其动作，以保持稳定性并完成文本驱动的命令。<br>定量实验（表I）显示，在30秒的长时域任务中，无论是随机命令流还是固定循环指令（如“挥手”、“出拳”），TextOp均保持了高成功率（16/20， 10/10等）和较低的跟踪误差（如E_mpjpe最低33.941 mm）。系统对扰动表现出鲁棒的恢复能力。</p>
</blockquote>
<p>实时性能测试（表II）表明，从输入新命令到机器人产生物理响应的平均用户交互延迟为0.73秒，支持响应式实时交互。</p>
<p><strong>离线评估</strong>：<br><img src="https://arxiv.org/html/2602.07439v1/x6.png" alt="运动生成对比"></p>
<blockquote>
<p><strong>图6</strong>：不同运动表示的运动生成对比。TextOp（我们提出的方法）在大多数片段级和过渡级指标上实现了最佳整体性能。<br>运动生成器评估（表III）比较了不同运动表示。在片段级指标上，TextOp的FID（3.072）和多样性（9.220）最接近真实数据集，R-precision（R@1=0.300）也优于多数基线。在评估指令切换平滑度的过渡级指标上，TextOp的FID（3.238）和AUJ（0.125）表现最佳，表明其能生成更平滑、更真实的运动过渡。</p>
</blockquote>
<p><strong>消融实验</strong>：论文通过组件对比验证了关键设计的贡献。1) <strong>机器人骨架运动表示</strong>：相比直接使用人体表示再重定向（DART+Retarget）或适配其他人体表示（HumanML3D-style），本文提出的表示在运动质量和过渡平滑度上均有显著优势。2) <strong>数据增强</strong>：使用生成器产生的运动增强跟踪策略训练数据，有效缩小了分布差距，提升了策略对生成器输出的跟踪鲁棒性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了TextOp系统，首次实现了基于流式自然语言命令的实时交互式人形机器人运动生成与控制新模式；2) 引入了机器人骨架运动表示和通过运动生成进行数据增强两个关键设计，分别提升了生成质量和对齐了生成-控制接口；3) 通过大量真实机器人实验和离线评估，验证了系统在即时响应、平滑运动、精确控制及鲁棒性方面的有效性。</p>
<p>论文提到的局限性包括：系统性能依赖于高质量的运动数据集和文本标注；运动生成器运行在外部工作站，对硬件算力有一定要求；当前主要处理单一模态（文本）输入。</p>
<p>这项工作为后续研究提供了重要启示：将交互式生成模型与物理控制回路紧密结合，是迈向灵活、自主机器人控制的关键路径；针对机器人物理结构定制表示和训练策略，能有效缓解仿真到真实的差距；未来可探索多模态（如语音、手势）的交互输入，并进一步优化系统的计算效率以实现完全端侧部署。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人控制器依赖预定义轨迹或持续遥操作、缺乏实时交互性的问题，提出TextOp框架，实现文本驱动的实时运动生成与控制。该框架采用两级架构：高层基于自回归运动扩散模型，根据流式文本输入生成短期运动轨迹；低层通过MLP策略进行动态运动跟踪，在物理机器人上稳健执行。实验通过真实机器人验证，系统展现出即时响应性、平滑的全身运动和精确控制能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.07439" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>