<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.03444" target="_blank" rel="noreferrer">2512.03444</a></span>
        <span>作者: Minghui Zheng Team</span>
        <span>日期: 2025-12-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，用于机器人操作臂的神经运动规划器主要依赖在手动生成的小规模工作空间数据集中训练，这限制了其分布外场景的泛化能力。此外，现有规划器通常采用简单的网络架构（如特征拼接）来融合机器人配置空间和工作空间特征，导致学习过程偏向于主导模态（如工作空间嵌入），而忽略了稀疏但关键的规划信号（如当前和目标配置嵌入）。本文针对数据稀缺和网络架构对多模态特征利用不足这两个关键痛点，提出了PerFACT框架。其核心思路是：1）利用大语言模型（LLM）的推理能力引导生成多样化、语义可行的大规模规划数据集；2）设计一种融合动作分块Transformer（Fusion Action-Chunking Transformers）的通用神经运动规划器，以更好地编码规划信号并关注多个特征模态。</p>
<h2 id="方法详解">方法详解</h2>
<p>PerFACT包含两个核心组件：用于大规模数据集合成的MotionGeneralizer和通用神经运动规划器M π NetsFusion。</p>
<p><img src="https://arxiv.org/html/2512.03444v1/x1.png" alt="PerFACT组件与部署概览"></p>
<blockquote>
<p><strong>图1</strong>：PerFACT的各个组件及其实物部署流程。首先，MotionGeneralizer整合程序化基元生成与LLM（GPT-4）的推理能力，为训练通用神经运动规划器生成多样化工作空间。随后，它生成与机器人无关、场景特定的运动规划问题，并与先进运动规划器（如cuRobo）集成以生成大规模规划数据集。该数据集与MotionGeneralizer的感知模态结合，用于训练M π NetsFusion。感知模态也用于M π NetsFusion的开环执行中，以解决评估场景中的规划问题。最后，M π NetsFusion被部署于真实世界规划场景。</p>
</blockquote>
<p><strong>MotionGeneralizer</strong>：这是一个LLM驱动的多样化工作空间生成框架，旨在为训练通用神经运动规划器合成大规模数据。</p>
<p><img src="https://arxiv.org/html/2512.03444v1/x2.png" alt="MotionGeneralizer框架"></p>
<blockquote>
<p><strong>图2</strong>：MotionGeneralizer的多样化工作空间生成框架。该方法首先随机选择机器人类型及其周围桌子的数量（程序化生成）。接着，它以少量示例（Few-shot）方式提示LLM（微调后的大语言模型）确定每张桌子上的基元数量。然后，根据基元池程序化生成建议的基元，并再次以少量示例方式提示LLM，以指定每个基元在其对应桌子上的位置和朝向，最终输出工作空间。该过程可重复以生成任意数量（N）的多样化规划工作空间。</p>
</blockquote>
<p>其工作流程如下：</p>
<ol>
<li><strong>程序化资产生成</strong>：借鉴Neural MP和NVIDIA SceneSynthesizer的方法，通过URDF采样生成具有不同尺寸和构型的日常铰接物体（如桌子、橱柜、微波炉）。</li>
<li><strong>语言引导的工作空间生成</strong>：<ul>
<li><strong>机器人与桌子选择</strong>：随机选择机器人类型及其周围桌子的数量，并在机器人可达范围内放置程序化生成的桌子。</li>
<li><strong>LLM确定基元数量与类型</strong>：向GPT-4提供机器人规格、桌子表面积和常见家居物体列表等提示，LLM输出每张桌子上应放置的基元数量和类型。若桌子少于四张，LLM还会建议在地面放置额外物体。</li>
<li><strong>程序化基元生成</strong>：根据LLM的建议，从NeuralMP或SceneSynthesizer基元池中程序化生成每个物体，确保同类物体在尺寸和构型上具有多样性。</li>
<li><strong>LLM引导基元放置</strong>：向LLM提供机器人规格、桌子表面积和每个基元尺寸，LLM输出每个基元在桌子局部坐标系中的语义可行位置和朝向。</li>
<li><strong>碰撞处理</strong>：执行后处理碰撞检测算法以解决潜在碰撞。</li>
</ul>
</li>
<li><strong>降低生成工作空间间的相似性</strong>：利用GPT-4为每个工作空间生成高级文本描述并转换为嵌入向量，与先前工作空间的嵌入比较相似度，若高于阈值则请求LLM生成新的、更具区分度的配置。</li>
<li><strong>感知模态</strong>：为训练和碰撞检查提供高维点云观测。<ul>
<li>通过在障碍物表面均匀采样点，合成工作空间点云（图4-a）。</li>
<li>通过在机器人操作臂不同构型下的网格上均匀采样点，生成机器人点云（图4-b），以提高空间感知和规划成功率。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.03444v1/x4.png" alt="点云合成"></p>
<blockquote>
<p><strong>图4</strong>：点云合成：MotionGeneralizer的感知模块通过在（a）工作空间基元和（b）任意构型下的机器人操作臂上均匀采样，提供特权点云场景表示，以增强下游规划任务的空间感知。</p>
</blockquote>
<ol start="5">
<li><strong>运动规划问题生成器</strong>：为避免生成过于简单的直线运动问题，MotionGeneralizer利用生成的基元形状来提供场景特定的规划问题。它在基元内部和机器人可达区域内采样无碰撞位姿，然后利用机器人特定的碰撞感知逆运动学计算对应的起始-目标配置，从而生成与机器人无关的规划问题（图5）。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.03444v1/x5.png" alt="规划问题生成"></p>
<blockquote>
<p><strong>图5</strong>：规划问题生成：MotionGeneralizer的问题生成器模块提供与机器人无关、场景特定的规划问题位姿。然后，任何机器人操作臂（如UR5e或Franka）都可以使用其碰撞感知逆运动学来计算规划问题对应的起始-目标配置。</p>
</blockquote>
<p><strong>M π NetsFusion</strong>：这是一种新型规划框架，利用融合Transformer架构来关注不同的规划模态并学习它们之间的交互通信。</p>
<p><img src="https://arxiv.org/html/2512.03444v1/x7.png" alt="M π NetsFusion架构"></p>
<blockquote>
<p><strong>图7</strong>：M π NetsFusion架构：该网络以当前配置、目标配置、当前机器人点云、目标机器人点云和工作空间点云作为输入。每个输入模态通过独立的编码器进行处理。然后，一个融合编码器（基于瓶颈Transformer）建模这些模态之间的交互。融合后的特征被传递到动作分块Transformer（ACT）的解码器中，以预测动作序列。最后，一个多层感知机（MLP）将解码后的特征映射到动作空间。</p>
</blockquote>
<p>其技术细节如下：</p>
<ul>
<li><strong>整体框架</strong>：基于动作分块Transformer（ACT）框架，但用瓶颈Transformer（Bottleneck Transformer）作为编码器来学习和编码跨模态交互。</li>
<li><strong>核心创新</strong>：采用瓶颈Transformer来建模每个规划模态的贡献，并限制规划模态间的信息流，确保只交换每个模态中最相关和压缩的表征，从而防止主导模态淹没稀疏信号。</li>
<li><strong>输入与输出</strong>：输入包括当前配置、目标配置、当前机器人点云、目标机器人点云和工作空间点云。每个模态通过独立编码器处理，然后送入融合编码器。融合后的特征由ACT解码器处理，最终通过MLP预测动作序列。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用MotionGeneralizer生成了一个包含350万条轨迹的大规模规划数据集。在模拟环境中进行评估，并进行了实物部署演示。对比的基线方法包括：</p>
<ul>
<li><strong>传统采样规划器</strong>：RRT-Connect、RRT*。</li>
<li><strong>神经运动规划器</strong>：MPNet、M π Nets、Neural MP、SIMPNet。</li>
</ul>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2512.03444v1/x11.png" alt="主要结果对比"></p>
<blockquote>
<p><strong>图11</strong>：主要定量结果（表1）：在四个不同难度的规划场景中，对比了M π NetsFusion与基线方法的平均规划时间（秒）。结果表明，M π NetsFusion在所有场景中均显著快于基线方法。</p>
</blockquote>
<p>论文指出，在评估的任务中，M π NetsFusion的规划速度比最先进的基于采样的规划器快4.47倍，比神经运动规划器快3.2倍。具体数值如图11（表1）所示，例如在“UR5e - 简单”场景中，M π NetsFusion的平均规划时间为0.02秒，而RRT*为0.52秒，M π Nets为0.15秒。</p>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2512.03444v1/x13.png" alt="消融研究"></p>
<blockquote>
<p><strong>图13</strong>：消融研究：展示了M π NetsFusion不同变体（移除融合编码器、使用拼接代替融合编码器、完整模型）在四个场景中的成功率和平均规划时间。完整模型在规划时间和成功率上均表现最佳，验证了融合编码器的有效性。</p>
</blockquote>
<p>消融实验（图13）分析了M π NetsFusion各组件贡献：</p>
<ol>
<li><strong>移除融合编码器</strong>：性能显著下降，表明跨模态交互学习至关重要。</li>
<li><strong>用简单拼接替代融合编码器</strong>：性能优于移除融合编码器，但不及完整模型，验证了瓶颈Transformer在模态感知融合上的优势。</li>
<li><strong>完整M π NetsFusion</strong>：在成功率和规划时间上均达到最佳性能。</li>
</ol>
<p><strong>定性结果</strong>：<br><img src="https://arxiv.org/html/2512.03444v1/x14.png" alt="定性结果"></p>
<blockquote>
<p><strong>图14</strong>：定性结果：展示了M π NetsFusion与基线方法（RRT-Connect、M π Nets）在四个评估场景中的规划路径。M π NetsFusion规划出的路径更平滑、更高效。</p>
</blockquote>
<p><strong>与现有工作空间生成方法对比</strong>：<br><img src="https://arxiv.org/html/2512.03444v1/x6.png" alt="MotionGeneralizer vs. MotionBenchMaker"></p>
<blockquote>
<p><strong>图6</strong>：MotionGeneralizer与MotionBenchMaker对比：MotionBenchMaker仅包含8个不同工作空间，且每个场景只有一个主要障碍物（上排）。MotionGeneralizer可提供任意数量的工作空间，且由LLM决定每个场景的障碍物数量（下排仅展示8个以作比较），从而产生更真实、更杂乱的工作空间。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>MotionGeneralizer</strong>：提出了一种新颖的、LLM增强的多样化工作空间生成框架，通过结合程序化生成和LLM的语义推理，能够合成大规模、语义可行的规划数据集，解决了神经运动规划器的数据稀缺问题。</li>
<li><strong>M π NetsFusion</strong>：提出了一种通用的神经运动规划器，采用融合动作分块Transformer架构，特别是瓶颈Transformer作为融合编码器，能够自适应地关注和处理不同的规划感知模态，有效提升了规划效率和成功率。</li>
</ol>
<p><strong>局限性</strong>：论文提到，MotionGeneralizer依赖于LLM（GPT-4）进行推理，这可能会引入与LLM性能相关的偏差或成本。此外，在模拟中训练并使用合成点云，与真实世界中可能存在的遮挡、噪声点云之间存在差距，需要微调以适应真实传感数据。</p>
<p><strong>启示</strong>：本研究展示了LLM在低层机器人运动规划数据合成中的潜力，为构建大规模、高质量的机器人规划数据集提供了新思路。同时，针对多模态特征融合的网络架构设计（如瓶颈Transformer）是提升神经运动规划器性能的关键方向，为后续开发更通用、鲁棒的机器人规划基础模型奠定了基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对神经运动规划器泛化能力受限、网络架构编码效率低的问题，提出PerFACT框架。其核心包含两个关键技术：一是MotionGeneralizer，利用大语言模型（LLM）自动生成语义可行的多样化工作空间，以合成大规模规划数据集；二是融合动作分块变换器网络（MπNetsFusion），通过融合多模态特征提升规划信号编码能力。基于合成的350万条轨迹进行实验，结果表明，所提出的MπNetsFusion在评估任务上的规划速度比现有先进方法快数倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.03444" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>