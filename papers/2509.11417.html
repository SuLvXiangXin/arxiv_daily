<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.11417" target="_blank" rel="noreferrer">2509.11417</a></span>
        <span>作者: Xuanlin Li Team</span>
        <span>日期: 2025-09-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，构建视觉-语言-动作（VLA）模型的主流方法是将预训练的视觉-语言模型（VLM）在机器人动作数据上进行微调，以利用其丰富的跨模态表示。然而，论文指出，直接在机器人数据上微调预训练的VLM会导致严重的表示退化。如图1所示，背景变化或指令的轻微改写都会导致现有VLA模型（如OpenVLA和π₀）的性能大幅下降，表明微调破坏了预训练视觉和语言表示的结构。尽管近期工作尝试通过在视觉-语言数据和机器人数据上进行协同训练来缓解此问题，但简单的协同训练效果有限，因为两种数据在结构上差异很大，限制了机器人动作预测过程对视觉-语言表示的重用。</p>
<p>本文针对VLA模型微调过程中预训练表示被破坏、从而限制泛化能力这一具体痛点，提出了一个旨在更好地保留预训练特征同时使其适应机器人操作的新视角。其核心思路是：通过一个部分冻结的双视觉编码器设计来保留稳健的预训练视觉特征；通过一个基于字符串的动作分词器将连续动作与预训练语言模型的输出空间对齐；并通过一种强调空间推理和可供性的协同训练策略来正则化特征，从而共同提升模型的泛化与鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个通用的训练框架，可应用于不同的现有VLA模型架构。其整体目标是在适应机器人任务的同时，最大限度地保留预训练VLM的强大、通用表示。</p>
<p><img src="https://arxiv.org/html/2509.11417v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：方法概览。我们的方法通过三种设计提升VLA的泛化能力：(a) 协同训练：在强调空间可供性和推理的机器人及视觉-语言数据集上联合训练，有助于保留用于泛化的预训练表示。(b) 部分冻结的视觉编码器：一个编码器被冻结以保留来自VLM的稳健预训练特征，另一个则保持完全的可训练性以专门适应机器人任务。(c) 字符串分词器：机器人动作被表达为基于数字的字符串，以最大化重用预训练语言表示，并为协同训练统一非机器人和机器人任务域的预测目标。</p>
</blockquote>
<p>整体框架包含三个核心创新组件：</p>
<p><strong>1. 部分冻结的双编码器架构</strong><br>为避免完全冻结视觉编码器导致的性能下降，或完全微调导致的表示退化，本文提出使用两个孪生编码器。其中一个编码器（ϕ_frozen）被冻结，作为“锚点”以保留来自VLM预训练的稳健、语义丰富的表示；另一个编码器（ϕ_train）则可训练，保持充分的灵活性以专门适应机器人动作预测。两者的输出在特征层面进行拼接：<code>z_t = [ϕ_frozen(o_t) || ϕ_train(o_t)]</code>，然后输入给后续的动作预测模块。在实践中，若基础VLA模型只提供一个视觉编码器，则复制它并冻结一份；若已包含两个，则冻结一个，微调另一个。</p>
<p><strong>2. 基于字符串的动作分词器</strong><br>为了最大化地重用预训练语言模型的表示，本文将每个连续动作维度（如Δx=0.0312）转换为字符序列（如“0 . 0 3 1 2”），其中每个字符（数字或小数点）都是语言词汇表中的一个token。这种表示方式有两个关键优势：首先，它将动作预测与语言生成统一到同一个基于字符串的自回归生成目标下，使得模型能够利用预训练语言模型中的空间信息来辅助动作预测，从而改善泛化；其次，自回归的字符串生成允许动作被逐步细化，可能产生更精确的预测。本文模型使用4位小数的动作进行训练。</p>
<p><strong>3. 通过协同训练进行特征正则化</strong><br>仅从机器人数据中学习容易导致过拟合。为了解决这个问题，并利用统一的字符串表示空间，本文提出在机器人数据和精心选择的视觉-语言数据集上进行协同训练。所使用的视觉-语言数据（如表I所示）强调空间推理、空间可供性和一般的视觉-语言推理。在每个训练批次中，从机器人数据和视觉-语言数据中各采样50%以平衡梯度。这种联合训练策略旨在防止灾难性遗忘并增强泛化能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估主要在模拟环境（SimplerEnv）和真实机器人平台（ViperX 300s）上进行。模拟评估分为两种设置：<strong>视觉匹配</strong>（与训练数据视觉相似，用于评估基础性能）和<strong>视觉变异聚合</strong>（随机化背景、光照、桌面纹理、干扰物和相机姿态，用于评估分布外（OOD）泛化）。此外，还评估了模型在标准视觉问答（VQA）基准上的表现以衡量其表示质量。对比的基线方法包括OpenVLA和π₀。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>设计选择的有效性</strong>：在视觉匹配设置下的消融实验（表II）显示，本文提出的完整方法（OpenVLA+和π₀+）显著优于基线。具体地，OpenVLA+的平均成功率比OpenVLA高出近40%（78.46% vs. 35.03%），π₀+比π₀（带指令增强）高出约7%（69.19% vs. 62.64%）。各组件均有贡献：单独使用双编码器（D）将OpenVLA性能从35.03%提升至55.55%；字符串分词器（S）与协同训练（C）结合时效果显著（78.17% vs. 51.05%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11417v2/figures/dataset_ablation.png" alt="协同训练数据消融"></p>
<blockquote>
<p><strong>图3</strong>：在SimplerEnv（视觉匹配）上的协同训练数据组成消融实验。使用我们统一的基于字符串的分词器，结合多样化的数据集进行协同训练能持续提升机器人任务性能。</p>
</blockquote>
<ol start="2">
<li><strong>表示泛化与鲁棒性</strong>：<ul>
<li><strong>视觉鲁棒性</strong>：在OOD视觉泛化评估（表III）中，本文方法在背景掩码和视觉变异聚合设置下均大幅优于基线。例如，在视觉变异聚合中，OpenVLA+的平均成功率为76.6%，显著高于OpenVLA的52.4%。</li>
<li><strong>表示可视化</strong>：在CIFAR-10数据集上的t-SNE可视化（图4）表明，基线VLA训练后的特征类别边界模糊，而本文方法保留了更清晰、更线性可分的类簇，线性探测分类准确率更高，证明了预训练视觉语义结构得到了更好的保留。</li>
<li><strong>语言鲁棒性</strong>：在指令改写测试（表IV）中，尽管未使用指令增强训练，OpenVLA+在PickCan任务上对改写指令的成功率仍高达84.52%，远超使用指令增强的基线OpenVLA（38.89%），显示出更强的指令泛化能力。</li>
<li><strong>推理能力</strong>：在多个VQA基准测试（图5）中，OpenVLA+和π₀+的性能显著高于其对应的基线，表明本文的训练方法更有效地保留了预训练VLM的推理能力。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11417v2/x4.png" alt="VQA基准测试结果"></p>
<blockquote>
<p><strong>图5</strong>：在五个VQA基准上评估VLA。OpenVLA+和π₀+在所有任务上都取得了显著更高的性能，表明我们的训练方法有助于保留VLM骨干的预训练表示。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界评估</strong>：<br>在真实机器人上的评估结果（表V和图6）显示，本文方法在存在干扰物的情况下，任务成功率 consistently 优于基线模型。基线模型容易受到干扰物误导（例如在PickKnife任务中误抓胡萝卜），而本文方法展现出更鲁棒的任务理解能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11417v2/x5.png" alt="真实世界定性结果"></p>
<blockquote>
<p><strong>图6</strong>：PickKnife和PickCarrot任务上的定性结果。我们的模型对干扰物具有更强的鲁棒性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于提出了一套系统的设计，以在VLA模型适应机器人任务时更好地保留预训练的VLM表示，从而提升泛化能力：1）<strong>部分冻结的双视觉编码器设计</strong>，混合预训练和微调特征；2）<strong>基于字符串的动作分词器</strong>，将机器人动作与语言输出对齐以更好地迁移预训练知识；3）<strong>平衡的协同训练方法</strong>，结合机器人数据和强调空间推理的视觉-语言数据。</p>
<p>论文自身提到的一个局限性是，基于字符串的动作分词器和双编码器设计会使模型推理时间增加0.5倍至1.3倍（取决于模型），但实验表明性能收益远超于此成本。</p>
<p>这项工作对后续研究的重要启示在于：将大模型能力迁移到具身智能领域时，<strong>保留预训练表示的完整性是获得强泛化能力的关键</strong>。通过架构设计（如部分冻结）、输出空间对齐（如字符串化）和数据策略（如目标明确的协同训练）来“保护”这些表示，是一条有效且通用的路径。这为未来开发更可靠、更通用的机器人操作策略提供了切实可行的见解。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型直接微调预训练视觉-语言模型（VLM）时破坏其表示、导致泛化能力下降的核心问题，提出三种关键技术：双编码器设计（冻结编码器保留预训练特征，可训练编码器适应任务）、基于字符串的动作分词器（将连续动作转为字符序列以对齐预训练域）和协同训练策略（结合机器人数据与强调空间推理的视觉-语言数据集）。实验在仿真和真实机器人上验证，该方法提升了模型对视觉扰动的鲁棒性、对新指令和环境的泛化能力，以及整体任务成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.11417" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>