<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Enhancing Tactile-based Reinforcement Learning for Robotic Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Enhancing Tactile-based Reinforcement Learning for Robotic Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.21609" target="_blank" rel="noreferrer">2510.21609</a></span>
        <span>作者: Sethu Vijayakumar Team</span>
        <span>日期: 2025-10-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域的主流方法严重依赖视觉输入或理想化的特权状态信息（如通过多个摄像头获取物体和指尖的精确位姿）。然而，这种依赖限制了系统的可扩展性和在非结构化环境中的鲁棒性。触觉感知被认为是克服这些限制的关键，但将其有效整合到强化学习中仍面临挑战。现有文献关于触觉反馈在RL中的效用提供了相互矛盾的证据：一些研究报告性能提升有限，而另一些则未观察到明显益处，甚至有研究认为本体感知历史已隐含了接触信息。</p>
<p>本文针对触觉数据因其稀疏性（仅在接触时存在）和非平滑性可能导致学习不稳定、使智能体难以提取有用表示这一具体痛点，提出了通过自监督学习来辅助学习观察表示的新视角。核心思路是：仅使用本体感知和二值触觉信号的历史，通过设计专门的自监督学习目标来学习强大的状态表示，从而替代对理想化状态信息的依赖，实现复杂的灵巧操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架基于PPO算法，并集成了一个自监督辅助损失来训练观察编码器。智能体接收的观察 $\mathbf{o_t}$ 是本体感知历史 $\mathbf{o_t^{prop}}$（包括关节角度、速度、上一动作等）和触觉历史 $\mathbf{o_t^{tact}}$（N个传感器的二值接触信号）在k个时间步上的拼接。该观察通过一个编码器MLP（1024→512→256）映射为潜在表示 $\mathbf{z_t}$。策略网络 $\pi$ 和价值网络 $\mathbf{v}$ 均以 $\mathbf{z_t}$ 为输入。总损失 $\mathcal{L} = \mathcal{L}<em>{\text{PPO}} + c</em>{\text{aux}}\mathcal{L}<em>{\text{aux}}$，其中 $\mathcal{L}</em>{\text{PPO}}$ 是标准的PPO损失，$\mathcal{L}_{\text{aux}}$ 是自监督辅助损失。</p>
<p><img src="https://arxiv.org/html/2510.21609v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：基于触觉和自监督的强化学习整体框架。智能体仅接收本体感知和二值触觉信号的历史，通过观察编码器生成潜在表示，供策略和价值网络使用。自监督辅助目标（图中未展开）与PPO目标联合训练编码器。右侧展示了RoTO基准中的三个任务（Find， Bounce， Baoding）以及Shadow Hand上的17个二值接触传感器布局。</p>
</blockquote>
<p>核心模块是四种自监督辅助目标，用于引导编码器学习更有用的表示：</p>
<ol>
<li><strong>触觉重建</strong>：强制编码器从潜在表示 $\mathbf{z_t}$ 中解码出原始的触觉观察 $\mathbf{o_t^{tact}}$，使用带正样本加权（$\mathbf{p_c}=10$）的二元交叉熵损失，旨在防止编码器忽略稀疏的触觉输入。</li>
<li><strong>完全重建</strong>：基线方法，同时重建触觉和本体感知观察，损失为触觉重建损失加上本体感知的均方误差损失。</li>
<li><strong>前向动力学</strong>：鼓励编码器提取对预测未来潜在状态至关重要的信息。给定当前潜在状态 $\mathbf{z_t}$ 和动作 $\mathbf{a_t}$，通过前向模型 $\mathbf{f}$ 自回归地预测未来n步的潜在状态 $\mathbf{\hat{z}<em>{t+i}}$。预测通过一个投影头 $\mathbf{p}$ 后，与目标编码器（EMA更新）产生的目标潜在状态 $\mathbf{z</em>{t+i}^T}$ 计算均方误差损失。</li>
<li><strong>触觉前向动力学</strong>：结合前向动力学和触觉重建，要求从预测的未来潜在状态 $\mathbf{\hat{z}_{t+i}}$ 中也能解码出对应的未来触觉观察。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.21609v1/x2.png" alt="自监督目标"></p>
<blockquote>
<p><strong>图2</strong>：提出的四种自监督学习目标示意图。上排：重建目标（TR， FR）使用解码器从潜在表示 $\mathbf{z_t}$ 重建观察。下排：前向动力学目标（FD， TFD）使用前向模型 $\mathbf{f}$ 预测未来潜在状态，FD与目标状态比较，TFD额外要求能重建未来触觉。</p>
</blockquote>
<p>另一个创新点是<strong>分离的辅助记忆</strong>。传统的on-policy RL中，辅助目标与RL目标在同一批 rollout 数据上优化然后丢弃。本文提出为辅助任务维护一个更大的独立记忆缓冲区（大小为 $[N_{rollouts}, B, R, ...]$），用于存储更多历史 rollout 数据，以稳定辅助更新并利用更广的数据分布。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在发布的<strong>Robot Tactile Olympiad (RoTO)</strong> 基准上进行，包含三个基于Isaac Lab的挑战性任务：<strong>Find</strong>（寻找固定球体）、<strong>Bounce</strong>（弹球，10秒内次数越多越好）和<strong>Baoding</strong>（双手旋转两球，10秒内旋转圈数越多越好）。对比的基线方法包括：仅使用本体感知的RL、使用本体感知+触觉的RL，以及结合了四种自监督目标（TR， FR， FD， TFD）的RL方法。此外，还测试了为FD方法引入分离辅助内存（$N_{rollouts} \in {2,3,4}$）的效果。</p>
<p>关键实验结果如下：</p>
<ul>
<li><strong>触觉的必要性具有任务依赖性</strong>：在Find任务中，触觉仅带来轻微的样本效率提升，最终性能与仅本体感知相当，且后者成功依赖于上一动作隐含推断接触。在Bounce任务中，触觉提升了样本效率和最终回报。在Baoding任务中，触觉信息至关重要，能将智能体从完全失败变为功能成功。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.21609v1/x3.png" alt="RL-only结果"></p>
<blockquote>
<p><strong>图3</strong>：RL-only智能体的平均评估回报。展示了本体感知+触觉 vs. 仅本体感知的性能。在Find任务中，移除了上一动作的仅本体感知变体（虚线）性能大幅下降，说明其依赖控制误差推断接触。</p>
</blockquote>
<ul>
<li><strong>自监督学习显著提升性能</strong>：TR和FD方法在所有环境中 consistently 优于RL-only基线。在Find和Bounce中，FD方法平均回报更高；在Baoding中，TR方法方差更小，平均回报更高，但FD方法能达到更高的性能上限。FR和TFD方法的表现则因环境而异，不一致。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.21609v1/x4.png" alt="RL+SSL结果"></p>
<blockquote>
<p><strong>图4</strong>：结合自监督学习（SSL）的智能体的平均评估回报。触觉重建（TR）和前向动力学（FD）方法表现最佳。</p>
</blockquote>
<ul>
<li><strong>分离辅助内存对复杂任务有益</strong>：在Find和Bounce任务中，增加辅助内存大小（$N_{rollouts}$）对FD智能体性能影响甚微。然而，在更复杂的Baoding任务中，使用分离的辅助内存带来了显著的性能提升。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.21609v1/x5.png" alt="分离内存效果"></p>
<blockquote>
<p><strong>图5</strong>：FD智能体使用不同大小分离辅助内存（$N_{rollouts}$）的训练性能对比。在复杂的Baoding任务中，更大的辅助内存带来了实质性改进。</p>
</blockquote>
<ul>
<li><strong>达成超人灵巧性</strong>：在Bounce任务中，最佳智能体在10秒内实现了超过100次弹跳（理论最大值），而人类吉尼斯世界纪录为59次。在Baoding任务中，最佳智能体在10秒内实现了高达25次旋转，而观察到的人类最快记录约为13次。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 通过实证阐明了稀疏二值触觉信号在特定场景（如机器人-物体运动解耦、低惯性物体接触）下对于灵巧性的关键作用；2) 仅使用本体感知和17个二值接触，在复杂接触任务中实现了模拟环境下的超人级灵巧操作性能；3) 提出并评估了针对触觉的自监督学习目标，发现前向动力学（FD）和触觉重建（TR）最为有效，且学习到的表示能够编码物体位置和速度；4) 发现了将自监督训练数据与on-policy记忆解耦可以提升性能，为利用离策略经验提供了可能性；5) 发布了RoTO基准以标准化和推动触觉操作研究。</p>
<p>论文提到的局限性包括：研究集中于二值触觉信号，未探索连续触觉测量；所有实验均在模拟中进行。这些工作对后续研究的启示在于：强调了针对触觉模态特性设计表示学习方法的必要性；揭示了触觉效用的任务依赖性，需根据具体交互动态判断是否需要显式触觉；展示了利用更广数据分布（如通过分离内存）改进复杂技能学习的潜力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人控制中基于触觉的强化学习效果不一致、过度依赖理想化状态信息的问题，提出采用自监督学习（SSL）方法，通过利用稀疏二进制触觉信号来增强触觉观测的有效性，并解耦SSL内存与on-policy内存以提升性能。实验表明，稀疏二进制触觉信号对灵巧性至关重要，尤其在机器人-物体解耦运动中；代理在球弹跳和Baoding球旋转等复杂接触任务中实现了超人的灵巧性。论文还发布了Robot Tactile Olympiad（RoTO）基准以标准化研究。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.21609" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>