<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Trust Through Transparency: Explainable Social Navigation for Autonomous Mobile Robots via Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Trust Through Transparency: Explainable Social Navigation for Autonomous Mobile Robots via Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.05477" target="_blank" rel="noreferrer">2504.05477</a></span>
        <span>作者: Sotomi, Oluwadamilola, Kodi, Devika, Arab, Aliasghar</span>
        <span>日期: 2025/04/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，自主移动机器人越来越多地部署在动态社交环境中。主流的社会导航方法包括基于物理模型的方法（如社会力模型）、模仿学习以及结合强化学习的策略。然而，这些方法存在一个关键的局限性：它们主要关注内部决策逻辑的优化，缺乏向人类用户提供实时、可读的决策解释能力。这导致机器人的行为难以预测，削弱了人机协作中的信任和可用性。</p>
<p>本文针对机器人决策过程不透明、缺乏直观解释这一具体痛点，提出了一个新视角：将可解释人工智能与多模态感知相结合。核心思路是，利用视觉语言模型处理机器人摄像头捕获的视觉场景，生成自然语言描述，并结合热图可视化突出显示影响决策的关键区域，从而为机器人的导航行为提供实时、多模态（视觉+语言）的解释，以增强透明度和用户信任。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个多模态可解释性模块，旨在集成到机器人的自主导航栈中，在机器人执行导航任务时实时生成解释。</p>
<p><img src="https://arxiv.org/html/2504.05477v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：可解释性模块的节点关系图。展示了由相机节点、BLIP节点、热图节点和LLM节点组成的系统架构，各节点通过ROS话题进行通信。</p>
</blockquote>
<p>整体框架是一个基于ROS2的软件系统，由四个核心节点构成一个处理流水线：</p>
<ol>
<li><strong>相机节点</strong>：输入为环境图像，输出为捕获的原始图像数据。</li>
<li><strong>BLIP节点</strong>：输入为原始图像，利用Bootstrapping Language-Image Pretraining模型生成图像的文本描述（图像字幕）。</li>
<li><strong>热图节点</strong>：输入为原始图像，使用Grad-CAM技术生成视觉显著性热图，高亮显示图像中对模型决策（如障碍物检测）最重要的区域，并生成对热图分析的文本摘要。</li>
<li><strong>LLM节点</strong>：输入为BLIP生成的图像描述和热图分析摘要，通过精心设计的提示词（见论文算法1下方的提示框）引导大型语言模型（如GPT-3.5 Turbo）合成一句简洁、自然的最终解释语句，并以语音形式输出。</li>
</ol>
<p>各模块的技术细节如下：</p>
<ul>
<li><strong>热图生成</strong>：采用Grad-CAM技术，通过对模型特定层的激活进行加权求和，并通过ReLU函数确保正贡献，生成热图$H_{i,j} = ReLU\left(\sum_{k}\alpha_k A_c^{i,j}\right)$，其中$\alpha_k$是特征图权重，$A_c^{i,j}$是激活值。</li>
<li><strong>语言解释合成</strong>：最终解释文本$\mathcal{T}$由LLM根据融合了图像上下文和热图信息的特征表示$\psi(H, X)$生成，即$\mathcal{T}=LLM(\psi(H,X))$。提示词要求解释以“I see”开头，包含对视图的描述，并以一个合适的重规划短语结尾。</li>
<li><strong>可解释性量化</strong>：定义了一个可解释性分数$\varepsilon \in [0,1]$，用于量化系统行为被人类理解的程度。当可解释性模块激活时，$\varepsilon = \hat{\varepsilon}$，其值通过后续用户调查的反馈计算得出。</li>
<li><strong>延迟优化</strong>：论文将总解释时间$T_{\text{total}}$建模为各节点处理时间与网络延迟之和（$T_{\text{total}}=T_{\text{camera}}+T_{\text{BLIP}}+T_{\text{heatmap}}+T_{\text{LLM}}$），并指出延迟与计算能力$C$成反比（$T_{\text{total}} \propto \frac{1}{C}$），为实时性优化提供了方向。</li>
</ul>
<p>与现有方法相比，本文的创新点具体体现在：</p>
<ol>
<li><strong>多模态融合</strong>：并非仅提供文本或仅提供视觉解释，而是将VLM生成的场景描述与Grad-CAM提供的视觉显著性区域相结合，再通过LLM合成统一的自然语言解释，使解释更具上下文意识和直观性。</li>
<li><strong>面向实时交互的系统集成</strong>：将整个流程封装为轻量化的ROS节点，可直接与现有机器人导航栈（如路径规划、避障模块）集成，并考虑了延迟问题，旨在实现动态环境中的实时解释。</li>
<li><strong>以用户为中心的评估</strong>：明确将“可解释性”定义为一个可量化的指标（$\varepsilon$），并通过用户调查来评估和校准该系统，直接关联技术输出与人的信任感知。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验平台采用搭载Raspberry Pi 4B和内置摄像头的MYAGV移动机器人，运行ROS Noetic。可解释性模块在ROS 2 Foxy上开发并独立通信。实验场景为执行14米的递送任务（在走廊和创客空间平均进行4次运行）。</p>
<p><strong>对比方法</strong>：实验的核心是比较同一机器人在<strong>开启可解释性模块（WE）</strong> 和<strong>关闭可解释性模块（WoE）</strong> 两种条件下的表现。测试分为两种模式：</p>
<ul>
<li><strong>测试1</strong>：手动导航模式（由操作员通过遥控控制）。</li>
<li><strong>测试2</strong>：自主导航模式。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>导航性能</strong>：如表I所示，在两种测试模式下，开启解释模块后，机器人完成任务的<strong>总时间更短</strong>（手动模式从23.5秒降至22.1秒，自主模式从25.3秒降至22.6秒），<strong>急停次数更少</strong>（手动模式从19次降至15次，自主模式从21次降至18次）。这表明可解释性模块可能通过更早地检测和解释社交冲突，促进了更平滑的路径重规划。</li>
<li><strong>用户信任与偏好</strong>：通过对30名参与者的调查，量化了用户对解释的偏好。计算出的总体偏好得分（PS）为76.7%，表明大多数用户倾向于机器人提供实时解释。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.05477v1/extracted/6326275/figs/survey1.png" alt="用户调查结果1"></p>
<blockquote>
<p><strong>图3</strong>：测试1（手动导航）的用户调查结果。显示了用户在“信任”、“清晰度”、“透明度”和“整体偏好”四个维度上对有无解释的评分对比。开启解释后，各项评分均有显著提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.05477v1/extracted/6326275/figs/survey2.png" alt="用户调查结果2"></p>
<blockquote>
<p><strong>图4</strong>：测试2（自主导航）的用户调查结果。与图3类似，展示了自主模式下用户评分的对比。同样，提供解释显著提高了用户在所有维度上的评分。</p>
</blockquote>
<p><strong>消融实验分析</strong>：本文虽然没有进行严格的组件级消融实验，但通过对比“有解释”与“无解释”两种整体状态，实质上评估了<strong>集成多模态解释功能</strong>这一核心组件的贡献。结果表明，该模块的加入不仅改善了主观用户体验（信任、清晰度、透明度），还带来了客观的导航性能提升（减少时间、减少急停）。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>集成的多模态可解释性模块</strong>，它创新性地将视觉语言模型（BLIP）、视觉显著性热图（Grad-CAM）和大语言模型（LLM）结合起来，为机器人的社交导航决策生成实时、直观的自然语言解释。</li>
<li>实现了该模块在<strong>真实移动机器人平台上的部署与验证</strong>，证明了其在动态社交环境中实时运行的可行性，并通过用户研究定量评估了其对提升人类信任和理解的有效性。</li>
<li>形式化定义了<strong>社交导航的可解释性任务</strong>和**可解释性分数$\varepsilon$**，为衡量和优化机器人行为的可解释性提供了框架和度量标准。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前方法作为一个概念验证，其<strong>延迟严重依赖于计算能力和网络条件</strong>（尤其是云LLM调用）。这可能会在资源受限的机器人或网络不稳定的环境中影响实时性。作者指出，进一步的架构优化（如边缘计算、分布式计算）是未来实际应用所必需的。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>轻量化与实时性</strong>：未来的工作可以探索更轻量的VLM和LLM，或模型蒸馏技术，以在机器人端实现更低延迟的解释生成。</li>
<li><strong>解释的个性化与适应性</strong>：可以根据不同用户（如专家与新手）或不同文化背景调整解释的详细程度和表述方式。</li>
<li><strong>主动解释与交互式解释</strong>：当前系统是在检测到冲突后生成解释，未来可以研究机器人何时应主动提供解释，以及如何支持用户追问的交互式解释对话。</li>
<li><strong>将解释反馈融入决策循环</strong>：探索如何将人类对解释的理解或反馈（如认可、纠正）作为信号，进一步优化机器人自身的导航策略，形成“解释-学习”的闭环。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自主移动机器人在社交导航中决策过程不透明、导致人类信任度低的问题，提出了一种多模态可解释性模块。该方法整合视觉语言模型与热力图，使机器人能通过自然语言实时解释其环境感知与导航决策。核心实验通过用户研究（n=30）和混淆矩阵分析验证，结果表明大多数用户偏好实时解释，系统有效提升了人类对机器人行为的信任与理解。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.05477" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>