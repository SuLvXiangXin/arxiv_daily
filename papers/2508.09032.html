<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.09032" target="_blank" rel="noreferrer">2508.09032</a></span>
        <span>作者: Patratskiy, Maxim A., Kovalev, Alexey K., Panov, Aleksandr I.</span>
        <span>日期: 2025/08/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型在基于视觉观察和文本指令预测智能体动作方面展现出强大能力。该领域的主流方法主要分为两类：一类如SpatialVLA，通过引入RGB-D图像（包含深度信息）来增强模型对环境的<strong>空间理解</strong>；另一类如TraceVLA，通过将关键点的视觉轨迹叠加在RGB图像上作为提示，为模型提供<strong>时间上下文</strong>。然而，这些方法存在关键局限性：它们要么只关注空间信息，要么只关注时间信息，未能将两者整合到一个统一的框架中。这导致模型在需要同时理解物体空间位置和动作历史时序的复杂操作任务中表现受限。</p>
<p>本文针对这一痛点，提出了一种名为“空间轨迹”的新视角，旨在通过视觉提示技术，将时空信息融合到单一图像表示中。核心思路是：将智能体关键点的历史视觉轨迹，投影到当前观察的预测深度图上，从而生成一个同时编码了空间（深度）和时间（轨迹）信息的增强视觉输入，以提升VLA模型在复杂操作任务中的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的Spatial Traces方法旨在将空间（深度）与时间（关键点轨迹）信息整合，形成一个统一的视觉表示，输入给VLA模型（由此得到的模型称为ST-VLA）。整体流程遵循一个清晰的算法。</p>
<p><img src="https://arxiv.org/html/2508.09032v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ST-VLA架构的整体框架。模型接收文本指令和一系列（例如30帧）历史观察作为输入。通过深度估计模型和轨迹预测模型分别处理当前帧和历史帧，生成带有叠加轨迹的深度图。原始RGB图像和增强后的深度图分别被编码为视觉嵌入，相加后与文本指令嵌入一同输入VLA模型，最终预测出在3D空间中的相对位移动作。</p>
</blockquote>
<p>具体而言，给定当前观察 <code>o_t</code>、文本指令 <code>I</code> 以及一个包含 <code>b</code> 帧历史观察的缓冲区，方法的核心步骤如下：</p>
<ol>
<li><strong>空间信息获取</strong>：使用深度估计模型 <code>M_D</code>（如ZoeDepth）处理当前观察 <code>o_t</code>，得到深度图 <code>D_t</code>。</li>
<li><strong>时间信息获取</strong>：使用轨迹预测模型 <code>M_T</code>（如Co-Tracker）处理历史观察序列 <code>[o_{t-b+1}, ..., o_t]</code>，预测出关键点在图像中的二维坐标序列（轨迹）<code>T = [t_1, ..., t_m]</code>。</li>
<li><strong>时空信息融合</strong>：将预测出的二维轨迹 <code>T</code> 叠加（“绘制”）到深度图 <code>D_t</code> 上，生成带有轨迹的深度图 <code>D_t^T</code>。论文探讨了不同的叠加策略（见图7），例如为轨迹上的每个点赋予当前帧中最近物体的深度值（策略C）。</li>
<li><strong>嵌入生成与融合</strong>：分别使用图像处理器 <code>P_I</code>（如Siglip）和深度处理器 <code>P_D</code>（采用SpatialVLA中的Ego3D位置编码器）处理原始观察 <code>o_t</code> 和增强深度图 <code>D_t^T</code>，得到观察嵌入 <code>E_{O_t}</code> 和深度嵌入 <code>E_{D_t^T}</code>。将这两个视觉嵌入相加，得到最终的视觉嵌入 <code>E_V</code>。</li>
<li><strong>动作预测</strong>：将视觉嵌入 <code>E_V</code> 与文本指令嵌入 <code>E_I</code> 一同输入VLA模型（基于PaliGemma2构建），预测出下一步动作 <code>a_t</code>。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：<strong>将视觉轨迹提示从RGB域扩展到了深度域</strong>。SpatialVLA仅提供静态深度信息，TraceVLA仅在RGB图像上提供轨迹时序信息，而ST-VLA创造性地将动态轨迹与静态深度图相结合，使得模型能从单一输入中同时感知物体的三维空间位置和其随时间变化的运动历史。</p>
<p><img src="https://arxiv.org/html/2508.09032v1/x4.png" alt="轨迹渲染策略"></p>
<blockquote>
<p><strong>图7</strong>：三种将轨迹渲染到深度图上的策略比较：(A) 使用轨迹点在其原始观察帧中的深度值；(B) 使用前一帧观察中的深度值；(C) 使用当前帧深度图中最近物体的深度值。实验表明策略C最有效。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在<strong>SimplerEnv</strong>虚拟环境中进行，该环境基于Robosuite构建，用于评估机器人操作任务。实验使用了从<strong>Bridge</strong>数据集中选取的四个代表性任务进行评测：放置勺子、放置胡萝卜、堆叠积木、放置茄子。评估指标包括<strong>目标条件成功率</strong>和<strong>任务成功率</strong>。</p>
<p>对比的基线方法包括：OpenVLA、RT-1-X、Octo、RoboVLM、SpatialVLA以及TraceVLA。ST-VLA仅使用Bridge数据集中的<strong>52条轨迹</strong>（共1969步）进行微调，以证明其数据效率。</p>
<p>关键定量结果如下表所示，ST-VLA在多个任务上取得了最佳或极具竞争力的性能。</p>
<p><img src="https://arxiv.org/html/2508.09032v1/x3.png" alt="平均性能对比"></p>
<blockquote>
<p><strong>图4</strong>：各方法在四个任务上的平均GCS和SR。ST-VLA在GCS上比SpatialVLA提升14.8%，比TraceVLA提升13.7%；在SR上分别提升4%和18.7%。</p>
</blockquote>
<p>具体到任务，在“放置勺子”任务上，ST-VLA的SR达到36.4%，显著高于SpatialVLA的18.2%和TraceVLA的9.1%。在最具挑战性的“堆叠积木”任务上，ST-VLA取得了36.4%的SR，而TraceVLA为0%，SpatialVLA为45.5%（但ST-VLA的GCS高达90.9%，表明其能完成抓取等子目标）。平均而言，ST-VLA的SR比SpatialVLA高4%，比TraceVLA高19%。</p>
<p><img src="https://arxiv.org/html/2508.09032v1/images/fig6_put-spoon-examples.png" alt="定性对比"></p>
<blockquote>
<p><strong>图6</strong>：“放置勺子”任务的定性执行对比。SpatialVLA和TraceVLA无法准确定位勺子位置导致失败，而ST-VLA成功抓取并放置了勺子，展示了其结合时空信息进行定位的优势。</p>
</blockquote>
<p>消融实验深入分析了方法各组件的影响：</p>
<ol>
<li><strong>历史缓冲区长度</strong>：实验比较了7、15、30帧缓冲区大小的影响。结果显示，更长的历史（30帧）通常能带来更稳定和更高的性能，尤其是在需要精细空间理解的任务（如堆叠积木、放置茄子）中，表明更丰富的时序信息有助于模型推理。</li>
<li><strong>轨迹渲染策略</strong>：对比了图7中的策略A和C。实验证明，策略C（赋予轨迹点当前帧最近物体的深度值）整体上优于策略A（使用轨迹点原始深度），因为策略C使轨迹在深度图上更突出、更容易被模型关注。</li>
<li><strong>性能增益来源</strong>：额外的对照实验表明，仅对基础模型（SpatialVLA）进行相同数据的微调会导致性能下降（平均SR下降10%），而仅在SpatialVLA上零样本应用轨迹提示则表现尚可。这证明ST-VLA的性能提升主要源于其<strong>独特的时空融合架构</strong>，而非简单的数据微调。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.09032v1/x5.png" alt="深度图处理可视化"></p>
<blockquote>
<p><strong>图8</strong>：深度图处理过程可视化。左图为原始预测的深度图，右图为叠加了关键点轨迹（无方向箭头）的增强深度图，该图同时包含了空间和时序信息。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：</p>
<ol>
<li>提出了<strong>Spatial Traces</strong>方法，首次将视觉轨迹提示与深度图相结合，为VLA模型创造了一个能同时编码空间和时序信息的统一视觉表示。</li>
<li>实验证明，该方法能显著提升复杂操作任务的性能，并且<strong>仅需极少量（52条）的微调数据</strong>即可实现有效增强，具有很高的数据效率和应用价值。</li>
<li>通过系统的消融研究，确定了长时序上下文和特定的轨迹渲染策略（策略C）对性能提升的关键作用。</li>
</ol>
<p>论文自身提到的局限性包括：</p>
<ol>
<li>性能依赖于深度图预测和关键点轨迹生成的准确性。</li>
<li>当连续观察图像之间差异极小时（例如机械臂缓慢旋转时），轨迹预测模型可能无法生成有效的长轨迹，从而限制方法效果。</li>
<li>叠加的轨迹可能遮挡目标物体，影响模型对物体可见性和距离的估计。</li>
</ol>
<p>对后续研究的启示：这项工作展示了在<strong>几何感知的表示</strong>（如深度图）中嵌入<strong>动态历史信息</strong>的强大潜力。未来的方向可以探索将类似思想应用于更丰富的3D场景表示（如点云、体素）、开发更鲁棒的自适应轨迹生成方法，以及将时空融合提示技术应用于更复杂的真实世界机器人任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在机器人操作任务中缺乏时空联合理解的问题，提出了一种名为“空间轨迹”的新方法。该方法通过将关键点的视觉轨迹投影到深度图上，以视觉提示的方式同时整合空间与时间信息。在SimplerEnv环境中的实验表明，该方法相比SpatialVLA模型将平均成功解决的任务数提升了4%，相比TraceVLA模型提升了19%，且仅需少量训练数据。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.09032" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>