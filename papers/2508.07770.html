<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.07770" target="_blank" rel="noreferrer">2508.07770</a></span>
        <span>作者: Lei Han Team</span>
        <span>日期: 2025-08-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前具身智能和机器人操作领域的发展凸显了对可扩展、交互式仿真环境的需求，这类环境需同时支持场景构建和用于训练自主智能体的数据收集。现有平台如AI2-THOR、ProcTHOR、ManiSkill2等提供了部分解决方案，有的专注于场景生成，有的提供任务特定的操作数据集，但鲜有平台提供一个统一框架，将高保真场景构建与灵活的移动机器人数据收集系统集成在一起。关键局限性在于场景生成的多样性与物理真实性不足，以及数据收集系统缺乏对移动底座（尤其是双足人形机器人）与灵巧手操作的集成支持。</p>
<p>本文针对上述痛点，提出了一个集成的交互式仿真平台AgentWorld，其核心新视角在于将<strong>程序化场景构建</strong>与<strong>基于移动的遥操作数据收集</strong>紧密结合，为复杂家庭环境中的机器人技能学习提供一站式解决方案。本文的核心思路是：构建一个能自动生成多样化、逼真家庭场景的平台，并集成一个支持轮式底座和人形机器人运动策略的双模式遥操作系统，以此收集大规模、多阶段的操作数据，进而通过模仿学习验证其模拟到现实转移的有效性。</p>
<h2 id="方法详解">方法详解</h2>
<p>AgentWorld平台主要由两大核心部分构成：<strong>程序化场景生成</strong>和<strong>基于遥操作的数据收集</strong>。整体流程是：首先通过程序化方法构建包含丰富语义资产和逼真材质的家庭场景，然后利用遥操作系统控制各类机器人（轮式、固定式、人形）在场景中执行从基本到多阶段的操作任务，从而收集轨迹数据用于策略训练。</p>
<p><img src="https://arxiv.org/html/2508.07770v2/x1.png" alt="平台概述"></p>
<blockquote>
<p><strong>图1</strong>：AgentWorld平台概述。展示了平台的核心能力：(1) 支持多种布局生成的程序化场景构建；(2) 拥有丰富语义3D资产库，具备逼真的视觉材质和物理属性；(3) 用于机器人操作的基于移动的遥操作系统。</p>
</blockquote>
<p><strong>核心模块1：程序化场景生成</strong>。该模块的完整流程如图2所示，包含四个关键阶段：</p>
<ol>
<li><strong>布局生成</strong>：根据用户指定的房间类型（客厅、卧室、厨房），自动生成包含墙壁、天花板、地板乃至楼梯的多楼层建筑结构。</li>
<li><strong>语义资产选择与放置</strong>：平台集成了来自ProcTHOR、Behavior-1K等开源库的超过9000个3D资产，并进行了手动语义标注。资产分为<strong>基本资产</strong>（如沙发、床、桌子，自动按语境放置）和<strong>可交互资产</strong>（如微波炉、水果、工具，可动态添加）。系统采用智能放置算法，根据语义功能摆放物体（如食物放在餐桌上）。</li>
<li><strong>视觉材质配置</strong>：为了促进模拟到真实泛化，平台提供了广泛的高保真基于物理的渲染（PBR）材质库，支持对建筑组件（墙、地板）和3D资产（木纹、陶瓷、织物、金属）进行多层次的材质定制，实现有效的数据增强。</li>
<li><strong>交互式物理仿真</strong>：该阶段在NVIDIA Omniverse Isaac Sim中完成，利用其GPU加速的PhysX 5.0引擎。系统根据语义注释自动为所有资产配置物理属性（碰撞体、摩擦系数、质量分布），并为铰链机构（门、抽屉）精确配置关节参数（类型、运动范围、摩擦系数）。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.07770v2/x2.png" alt="场景构建流程"></p>
<blockquote>
<p><strong>图2</strong>：AgentWorld场景构建模块的流程。展示了从布局生成、语义资产选择与放置、视觉材质配置到交互式物理仿真的完整管线。前三个阶段在虚幻引擎中实现，物理仿真在Isaac Sim中完成。</p>
</blockquote>
<p><strong>核心模块2：基于移动的遥操作数据收集</strong>。针对家庭环境中长视野任务对移动导航协调性的挑战，系统设计了如图3所示的集成管线，包含两个关键组件：</p>
<ol>
<li><strong>移动底座控制</strong>：对于轮式或直接可控的浮动底座机器人，通过键盘控制其三自由度（平面速度v_x, v_y和偏航角速度v_θ）动作。对于双足人形机器人，则采用IsaacLab中经过改编的奖励驱动运动策略，同样通过键盘控制移动，并可通过固定或释放其底座链接来切换移动和操作模式。</li>
<li><strong>手臂与手部控制</strong>：采用OpenTeleVision系统，通过VR头显捕捉手部关键点。通过标定现实世界中手腕与头显的相对姿态，匹配仿真中的相机-末端执行器关系。使用基于Pinocchio的闭环逆运动学算法计算手臂关节姿态。对于末端执行器，夹爪动作通过拇指到食指指尖距离归一化控制；灵巧手则采用dex-retargeting方法将手部关键点转换为关节动作。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.07770v2/x3.png" alt="数据收集系统"></p>
<blockquote>
<p><strong>图3</strong>：AgentWorld的数据收集系统。左侧展示了通过键盘控制轮式和足式机器人移动底座；右侧展示了通过VR控制手臂与手部，并采用重定向方法驱动夹爪和灵巧手。</p>
</blockquote>
<p><strong>创新点</strong>：与现有平台（对比见表1）相比，AgentWorld的创新性体现在其<strong>高度集成性</strong>。它首次将支持多样化布局和PBR材质的程序化场景构建、对轮式/固定/人形多种机器人平台的支持、以及对夹爪和灵巧手的遥操作数据收集（包含关节控制、浮动底座控制和足式运动控制）统一在一个平台内，提供了从环境创建到数据采集的完整闭环解决方案。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>基准与数据集</strong>：实验基于AgentWorld平台构建的<strong>AgentWorld数据集</strong>进行。该数据集包含在150个独特家庭场景中收集的超过1000条机器人操作轨迹，涉及Unitree G1、H1、带轮底座的Franka Emika Panda以及固定平台的DOBOT X-Trainer四种 embodiment，末端执行器包括夹爪和灵巧手。任务分为<strong>基本任务</strong>（抓放、开合、推拉）和<strong>多阶段任务</strong>（客厅整理书籍/上饮料、卧室整理衣柜、厨房储存/加热食物等）。</p>
<p><strong>对比方法</strong>：在数据集上评估了四种模仿学习算法：行为克隆（BC）、动作分块Transformer（ACT）、扩散策略（DP）以及视觉-语言-动作模型π0。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模仿学习性能</strong>：如表2所示，在基本任务上，ACT取得了最稳定的性能（成功率62%-84%），其动作分块机制有效处理了短视野动作序列。在多阶段任务上，π0展现出优势（成功率20%-30% vs. 其他方法的4%-28%），得益于其预训练表征对长视野任务结构的捕捉。所有方法在需要同时进行精确控制和长期规划的复杂移动操作任务上仍面临挑战。</li>
<li><strong>模拟到真实转移</strong>：通过“模拟预训练+少量真实数据微调”的范式进行了验证。使用π0策略，先在模拟数据（10种资产，每种10条轨迹）上训练，然后用有限的真实演示（香蕉、苹果、杯子3种物体，每种3条轨迹）微调，在将各种物体放入碗的基本任务上取得了29.3%的成功率。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.07770v2/x4.png" alt="实验结果"></p>
<blockquote>
<p><strong>图4</strong>：AgentWorld数据集的定性结果及模拟到真实转移示例。上半部分展示了不同模仿学习算法在模拟环境中的执行情况；下半部分展示了经过模拟预训练和真实微调后的策略在真实机器人上成功执行抓放任务。</p>
</blockquote>
<p><strong>消融实验总结</strong>：论文未进行严格的组件消融实验，但通过与其他平台的对比（表1）和不同模仿学习算法的性能差异（表2），间接证明了其集成场景构建与多样化数据收集能力的价值。多阶段任务上π0的优异表现，也凸显了平台支持复杂长视野任务数据收集的重要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个统一的<strong>程序化场景构建框架</strong>，支持多样化布局生成和语义物体放置，并配备具有逼真视觉与物理属性的海量3D资产库，显著提升了仿真逼真度。</li>
<li>设计了一个<strong>基于移动的遥操作数据收集系统</strong>，实现了对移动底座和操作器（夹爪/灵巧手）的精确控制，能够收集长视野、多阶段任务数据。</li>
<li>构建并开源了大规模机器人操作基准<strong>AgentWorld数据集</strong>，并通过模仿学习和模拟到真实实验验证了其有效性。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>可变形物体支持有限</strong>：受底层Isaac Sim物理引擎限制，对布料、绳索等可变形物体的仿真支持不足。</li>
<li><strong>纯模拟到真实的挑战</strong>：模拟与真实之间存在显著的领域差距，需大量手动调整渲染参数以实现视觉对齐。目前仅通过模拟预训练加少量真实微调在简单任务上验证成功，纯模拟数据训练的策略直接部署到真实世界仍很困难，多阶段任务尤其需要更多真实演示来弥补现实差距。</li>
</ol>
<p><strong>后续研究启示</strong>：<br>AgentWorld为具身智能研究提供了一个强大的集成仿真与数据收集基础设施。其局限性指明了未来方向：一是集成更先进的物理引擎以支持可变形物体等复杂交互；二是开发更鲁棒的领域自适应技术，以缩小模拟到真实的鸿沟，特别是对于长视野任务。该平台有望推动面向复杂家庭环境的、数据驱动的移动操作技能学习研究。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对家庭移动操作开发中缺乏集成高保真场景构建与灵活数据收集统一框架的问题，提出了AgentWorld交互式仿真平台。其关键技术包括自动化场景构建（涵盖布局生成、语义资产放置、视觉材料配置和物理模拟）以及双模式遥操作系统（支持轮式底座和人形运动策略），用于高效收集操作数据。通过广泛基准测试行为克隆、动作分块变换器、扩散策略等模仿学习方法，验证了平台所生成数据集在模拟到真实转移中的有效性，为复杂家庭环境下的机器人技能学习提供了完整解决方案。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.07770" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>