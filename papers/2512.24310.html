<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.24310" target="_blank" rel="noreferrer">2512.24310</a></span>
        <span>作者: Robotics, TARS, Zheng, Yupeng, Peng, Jichao, Li, Weize, Zheng, Yuhang, Li, Xiang, Jin, Yujie, Wei, Julong, Zhang, Guanhua, Zheng, Ruiling, Cao, Ming, Gu, Songen, Zou, Zhenhong, Li, Kaige, Wu, Ke, Yang, Mingmin, Liu, Jiahao, Li, Pengfei, Si, Hengjie, Zhu, Feiyu, Fu, Wang, Wang, Likun, Yao, Ruiwen, Zhao, Jieru, Chen, Yilun, Ding, Wenchao</span>
        <span>日期: 2025/12/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>大规模预训练是语言和视觉模型实现泛化的基础，但用于灵巧手操作的数据集在规模和多样性上仍然有限，阻碍了策略的泛化能力。现有的人类操作数据集面临三个关键局限性：1) <strong>场景多样性有限</strong>：许多数据集在受限的实验室环境中收集，缺乏真实世界环境的多样性和复杂性；2) <strong>模态不对齐</strong>：现有数据往往缺少一种或多种关键模态，例如细粒度语言指令、精确的3D手部姿态或物体姿态，导致视觉、语言和动作之间的错位；3) <strong>基准测试不足</strong>：大多数数据集不支持涵盖灵巧操作全流程（包括场景感知、任务规划和动作执行）的全面评估基准。</p>
<p>本文针对上述痛点，提出了一个名为“World In Your Hands”（WiYH）的大规模开源生态系统，旨在为野外环境中的人类中心操作学习提供支持。本文的核心思路是通过一个集成的可穿戴数据采集套件（Oracle Suite）在多样化的真实世界场景中收集大规模、多模态的人类操作数据，并辅以全面的注释和基准测试，以推动从感知到动作的具身智能研究。</p>
<h2 id="方法详解">方法详解</h2>
<p>WiYH生态系统由三个核心组件构成一个完整的pipeline：1) <strong>Oracle Suite及自动标注流水线</strong>：负责在野外采集高质量、多模态的原始数据并生成精确的运动真值；2) <strong>WiYH数据集</strong>：对采集和标注后的数据进行组织，形成一个大规模、多模态的资源库；3) <strong>多样化的注释与全面的基准</strong>：在数据集基础上提供丰富的语义注释，并构建覆盖感知到操作全链条的评估任务。</p>
<p><img src="https://wiyh.tars-ai.com/Figure1" alt="WiYH生态系统概览"></p>
<blockquote>
<p><strong>图1</strong>：WiYH生态系统概览。它建立在一个在真实世界人类环境中自行收集的大规模数据集之上，捕捉了人类执行超过40种不同任务的行为，提供了丰富的RGB视频流以及涵盖100多种人类技能的综合运动真值。</p>
</blockquote>
<p><strong>1. Oracle Suite：人类中心数据采集套件</strong><br>这是一个低成本、可穿戴的数据采集系统，专为人类中心具身AI研究设计。其硬件系统采用模块化架构，包含三个主要组件：</p>
<ul>
<li><strong>H-FPVHive</strong>：支持胸戴配置，集成两个鱼眼相机、两个针孔相机和四个红外镜头，所有相机硬件同步。红外镜头用于更好地定位H-Gloves的相对姿态。</li>
<li><strong>H-Gloves</strong>：每只手套包含6个IMU、5个压力传感器和一个板载MCU，可捕捉手部运动和触觉信息。此外，它还包含3个用于视觉跟踪和观察的鱼眼相机。</li>
<li><strong>H-Backpack</strong>：提供数据存储、计算和电源供应，计算由NVIDIA Orin承载。</li>
</ul>
<p>其<strong>自动标注流水线</strong>融合了多模态传感器数据（红外+IMU+RGB），以确保在野外场景中的定位精度。在线定位模块使用视觉惯性里程计（VIO）算法和红外检测进行轻量级粗定位，而离线阶段则通过视觉运动恢复结构（SfM）算法细化这些粗略估计，最终输出厘米级别的双手腕6D姿态。</p>
<p><strong>2. WiYH数据集与标注流水线</strong><br>数据采集由穿戴Oracle Suite的从业者在各场景中记录自然操作行为。每次录制约10分钟，收集的信号包括压力传感器的触觉信号、左右腕部安装的双目鱼眼图像、第一人称双目鱼眼图像、左右手IMU定位数据以及相应的相机标定参数。</p>
<p><img src="https://wiyh.tars-ai.com/Figure2" alt="Oracle Suite组成"></p>
<blockquote>
<p><strong>图2</strong>：Oracle Suite：人类中心数据采集套件。主要由三个集成组件组成：(1) H-FPVHive：第一人称感知套件；(2) H-Glove：手部运动捕捉与触觉感知模块；(3) H-Backpack：电源与数据存储单元。</p>
</blockquote>
<p>标注流水线包含三部分：</p>
<ul>
<li><strong>子任务（原子任务）标注</strong>：采用四阶段混合标注流水线（数据处理、任务定义、标注整合、后处理），将操作序列层次化分解为高级任务和原子子任务单元，每个单元包含预设动作和目标物体，并利用大语言模型（LLM）增强为完整的操作指令。</li>
<li><strong>感知标注</strong>：使用视觉基础模型预标注场景元素，再经人工选择和校正，生成实例分割掩码（通过SAM生成）和深度图（使用FoundationStereo模型从第一人称双目图像估计）。</li>
<li><strong>视觉语言标注</strong>：通过人工策划生成，包括<strong>空间指代</strong>（为每个子任务提取关键帧并添加空间关系词）、<strong>子任务预测</strong>（给定当前任务，预测后续动作）和<strong>完成度验证</strong>（判断视频是否描绘了已完成的任务）三类任务。</li>
</ul>
<p><img src="https://wiyh.tars-ai.com/Figure3" alt="数据集统计概览"></p>
<blockquote>
<p><strong>图3</strong>：数据集统计概览。包括任务-场景关系桑基图、各场景平均动作时长、注释类型分布（单位：小时）以及操作目标物体和技能的词云。展示了数据在场景、时间和语义上的广泛覆盖。</p>
</blockquote>
<p><strong>创新点</strong>：与现有数据集相比，WiYH的核心创新在于：1) <strong>完全在野外采集</strong>，由熟练从业者执行任务，捕获了真实的决策过程和动作；2) <strong>提供了全面对齐的多模态注释</strong>，包括运动真值、深度、掩码、任务/子任务标签和思维链推理等，解决了现有数据模态错位的问题；3) <strong>构建了覆盖从感知到操作的全流程基准</strong>，支持对模型能力的整体评估。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文基于WiYH生态系统，在多个基准上进行了实验，以验证其价值。</p>
<p><strong>1. 人类中心视觉语言（HVL）基准</strong><br>在三个任务上评估了多模态大语言模型（MLLMs）：<strong>空间指代（SR）</strong>、<strong>子任务预测（SP）</strong> 和 <strong>完成度验证（CV）</strong>。测试了GPT-4o、Doubao-seed和Qwen等模型。</p>
<p><img src="https://wiyh.tars-ai.com/Table2" alt="VLM在空间推理任务上的对比"></p>
<blockquote>
<p><strong>表2</strong>：VLM在空间推理任务上的对比。所有模型在空间指代任务上准确率均低于50%，在完成度验证任务上略高于50%，表明当前通用VLLM在具身任务中的细粒度空间和因果推理能力仍然不足。</p>
</blockquote>
<p><strong>2. 人类中心世界模型（HWM）</strong></p>
<ul>
<li><strong>语言条件视频生成</strong>：使用WiYH数据对CogVideo和DynamiCrafter进行微调。定量评估使用VBench的四个指标。</li>
</ul>
<p><img src="https://wiyh.tars-ai.com/Figure5" alt="语言条件视频生成效果对比"></p>
<blockquote>
<p><strong>图5</strong>：语言条件视频生成。经过WiYH微调后，基线方法生成视频的空间和时间连贯性、与输入图像和动作指令的对齐度均显著提升。</p>
</blockquote>
<p><img src="https://wiyh.tars-ai.com/Table3" alt="视频生成定量结果"></p>
<blockquote>
<p><strong>表3</strong>：视频生成定量结果对比。使用WiYH微调后，两个基线模型在所有指标上均有明显提升，其中运动强度和时序一致性的增益最为显著。</p>
</blockquote>
<ul>
<li><strong>高斯泼溅4D重建</strong>：利用WiYH提供的RGB图像、深度图和相机姿态进行动态场景的4D高斯泼溅重建。实验展示了在“倒酒”等任务中的重建结果，证明了数据在动态空间感知任务中的实用性。</li>
</ul>
<p><strong>3. 人类中心VLA训练（HVT）</strong><br>使用GR00T和OpenVLA-OFT作为基线算法，训练它们从数据集中预测人类手腕的未来运动轨迹（1.6秒）。实验表明，使用WiYH的人类中心数据进行训练，能持续降低在测试集上跟踪真实轨迹的误差。</p>
<p><strong>4. 跨具身分析</strong><br>这是关键的消融实验，探究人类视频数据对机器人操作策略泛化能力的影响。在<strong>单物体场景</strong>和<strong>多物体杂乱场景</strong>中，比较了仅使用机器人数据（来自改进的DexUMI系统）与<strong>联合训练</strong>（混合UMI数据和WiYH人类视频数据）的策略性能。</p>
<p><img src="https://wiyh.tars-ai.com/Figure7" alt="真实机器人操作实验"></p>
<blockquote>
<p><strong>图7</strong>：真实机器人操作实验。对比了在四种新任务设置下，仅用UMI数据训练的策略与用UMI+WiYH人类数据联合训练的策略的性能。</p>
</blockquote>
<p><img src="https://wiyh.tars-ai.com/Table6" alt="跨具身分析定量结果"></p>
<blockquote>
<p><strong>表6</strong>：跨具身分析定量结果。在单物体场景中，加入人类数据联合训练使成功率提升超过13%；在多物体杂乱场景中，仅用UMI数据的策略几乎无法泛化，而加入人类数据联合训练后，成功率提升了52%。这证明了人类数据在引入更广的动作分布和观察域、增强策略泛化能力方面的关键作用。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1) <strong>Oracle Suite</strong>：一个支持野外数据采集、集成了无标记自动标注功能的可穿戴系统；2) <strong>大规模WiYH数据集</strong>：提供了多样化、由熟练人类执行的操作序列，包含丰富的多模态数据流；3) <strong>全面的基准套件</strong>：由大量的原子动作和视觉语言注释支持，用于从感知到物理交互的整体评估。</p>
<p>论文自身提到的<strong>局限性</strong>在于，所有操作任务均在真实世界环境中评估。为了增加可访问性和可复现性，作者正在开发高保真仿真环境，并计划在未来版本的数据集和基准中发布。</p>
<p>本文的<strong>启示</strong>在于：当前通用模型在需要细粒度空间和因果推理的具身操作任务上能力仍然欠缺。WiYH生态系统以其独特的规模、场景多样性和丰富的多模态对齐注释，为填补这一空白提供了必要的基础设施。其实验结果表明，即使精度有限的人类操作数据，也能通过提供更广阔的动作和观察空间分布，显著提升机器人策略的泛化能力，这为如何利用人类数据增强机器智能指明了方向。该工作有望成为开发更具能力和泛化性的具身智能体的重要基石。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对灵巧手操作数据规模小、多样性不足导致策略泛化能力差的核心问题，提出了一个大规模开源生态系统WiYH。其关键技术包括：1）Oracle Suite可穿戴数据采集与自动标注流水线；2）包含超1000小时、数百种技能的多模态真实世界操作数据集。实验表明，整合该人本数据显著提升了桌面操作任务中灵巧手策略的泛化性与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.24310" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>