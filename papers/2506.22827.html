<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.22827" target="_blank" rel="noreferrer">2506.22827</a></span>
        <span>作者: Navid Azizan Team</span>
        <span>日期: 2025-06-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前人形机器人控制的主流方法是分层结构：一个低层强化学习（RL）控制器负责跟踪给定的全身运动目标，一个高层模块通过遥操作或模仿学习（IL）来生成这些运动目标。尽管这种方法在单个技能任务（如舞蹈、举重）上取得了成功，但现有系统在技能间切换时通常需要人工干预，限制了其在扩展任务序列上的自主操作能力。执行多步任务需要语义理解、鲁棒记忆和容错能力，而现有的人形系统缺乏能够自主选择和排序技能，并验证其成功执行的高层模块。</p>
<p>本文针对人形机器人自主执行复杂多步操作任务的痛点，提出在已有的两层控制架构之上，增加一个第三层：基于视觉语言模型（VLM）的规划与执行监控模块。本文的核心思路是构建一个包含底层跟踪、中层技能和高层规划监控的三层分层框架，利用VLM的语义理解能力来动态生成技能序列并实时监控其完成情况，从而实现长视野、闭环的多步操作自主性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个用于多步人形操作的三层分层控制与规划框架。</p>
<p><img src="https://arxiv.org/html/2506.22827v3/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：提出的自主多步人形操作分层框架概览。系统包含：（1）高层规划模块，使用视觉语言模型（VLM）生成任务特定的技能序列并通过视觉反馈监控其执行；（2）中层运动生成模块，由模仿学习的动作专家策略组成，将自我中心视觉输入和本体感觉机器人状态映射为目标关节配置；（3）底层基于强化学习的跟踪策略，生成可行的关节角度轨迹，由高频PD控制器执行。</p>
</blockquote>
<p><strong>1. 底层跟踪策略</strong><br>该模块是一个基于强化学习的跟踪控制器，负责跟踪上层生成的运动目标。策略函数 π_T 将运动目标 G 和机器人当前状态 S 映射为可行的关节命令 A。运动目标 G 包含根运动目标 G_m（期望的基座线速度和角速度）和表达目标 G_e（目标关节角度和一组对应特定身体标志的3D关键点位置）。状态 S 包括关节位置和速度、IMU数据等本体感觉观测值。策略以50Hz频率输出目标关节角度，由一个运行在200Hz的高频PD控制器执行。<br>该策略采用标准的演员-评论家架构，通过近端策略优化（PPO）进行训练。训练在Isaac Gym模拟器中进行，使用了4096个并行环境，并广泛随机化环境参数以确保鲁棒性。训练数据来源于AMASS数据集的精选子集，并补充了针对下游技能收集的遥操作演示数据。</p>
<p><strong>2. 中层模仿学习技能</strong><br>该模块在底层跟踪策略的基础上，自主生成更高级的技能行为，将感官输入转换为连贯的关节运动目标，作为跟踪策略的参考轨迹。它包含两个部分：基于RGB的遥操作流水线用于收集演示数据，以及基于这些演示训练的模仿学习（IL）专家策略用于自主执行单个技能。<br><em>遥操作流水线</em>：使用HybrIK模型从单目RGB图像估计人体姿态，然后通过专门的逆运动学过程将人体姿态重定向到Unitree G1机器人兼容的关节配置，最后通过正向运动学生成机器人身体链路的3D关键点坐标，形成完整的运动参考。<br><em>模仿学习模块</em>：IL策略 π_IL 将观测状态 x_t（包含本体感觉关节角度和双目自我中心RGB图像）映射到未来关节配置动作空间。本文采用为人形机器人设计的Humanoid Imitation Transformer (HIT) 模型，该模型基于Action Chunking Transformer (ACT)，能以25Hz频率一次性预测50个关节角度目标。训练时使用辅助的L2损失来提高视觉基础性和泛化能力。预测的关节角度通过正向运动学转换为3D关键点，供底层控制器使用。演示数据通过遥操作收集，仅保留成功序列，并分割为特定技能的数据集（如抓取和放置）来训练独立的策略。</p>
<p><strong>3. 高层多步技能规划与执行监控</strong><br>为解决将多个技能组合成长序列任务的需求，本文引入了高层规划与执行监控模块，能够动态选择并验证技能序列的执行。该模块利用预训练的VLM，采用闭环的规划器-监控器架构。其中，“技能”指个体的、短时长的操作能力（如抓取物体）；“任务”指涉及至少两个按顺序执行的、改变世界物体状态的高级目标。<br><em>系统架构</em>：包含两个组件：（a）VLM规划器（𝒫）：使用GPT-4o模型，从视觉和文本任务输入生成结构化的、可解释的技能序列。（b）VLM技能监控器（ℳ）：使用轻量级的Gemini-2.0-Flash-Lite模型，以大约1Hz的频率持续验证每个执行技能的完成情况。二者形成一个迭代的规划-监控循环。<br><em>技能库与PDDL类表示</em>：每个技能使用结构化的、类似规划域定义语言（PDDL）中操作符定义的形式来描述，包含“前提条件”和“效果”字段。这种形式平衡了逻辑严谨性和灵活性。</p>
<p><img src="https://arxiv.org/html/2506.22827v3/x3.png" alt="技能描述示例"></p>
<blockquote>
<p><strong>图3</strong>：提供给VLM规划器和技能监控器的结构化技能描述示例。</p>
</blockquote>
<p><em>VLM规划器</em>：给定初始图像、任务指令和结构化技能库，VLM规划器输出一个接地的可执行技能序列σ。GPT-4o通过视觉接地技能的前提条件和效果来生成序列，并将其转化为二值视觉问答查询以保证逻辑连贯性。<br><em>VLM技能监控器</em>：该监控器通过实时视觉反馈持续评估执行状态。它以约1Hz运行，评估执行期间捕获的短视频片段（从1.5秒片段中提取的10-15帧）。对于每个技能，监控器使用提供的自然语言验证查询输出一个二值决策：完成或进行中。</p>
<p><strong>4. 完整分层系统集成</strong><br>完整系统实现了图2所示的流水线，包含：高层VLM规划器、实时VLM技能监控器、根据计划序列和监控反馈动态选择当前技能的技能选择器、激活相应IL策略以从感官数据生成关节运动目标的中层模仿学习模块，以及将关节目标转换为可执行电机扭矩命令的底层跟踪策略与PD控制器。该分层设计的关键优势在于其模块性，新技能可以独立训练并直接添加到技能库中，便于扩展机器人的行为库。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在29自由度的Unitree G1人形机器人上进行。机器人配备了两个外置的ELP RGB相机，为中层IL策略提供双目自我中心视觉输入。测试环境为包含两张桌子和物体的受控环境，模拟家庭场景中的代表性操作任务。<br><strong>评估任务</strong>：定义一个代表性的多步操作任务，包含：（1）抓取：机器人必须从初始桌子上抓取并提起一个袋子。（2）放置：机器人需要将袋子准确放置到第二张桌子上。<br><strong>实验过程</strong>：共进行了40次独立试验。机器人完全自主运行，执行由高层VLM规划器生成的计划，技能由VLM执行监控器实时动态选择并监控完成情况。成功定义为机器人正确抓起袋子并成功将其放置到指定目标表面。<br><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2506.22827v3/x1.png" alt="实验结果表"></p>
<blockquote>
<p><strong>表I</strong>：个体策略及完整系统的评估结果。抓取技能成功率为90%（27/30），放置技能成功率为83%（25/30），而完整的“抓取-放置”多步任务成功率为73%（29/40）。</p>
</blockquote>
<p>集成后的分层系统在40次试验中，以73%的整体成功率完成了整个操作序列。试验中观察到的失败按频率排序可分为三类：1. <strong>技能策略失败</strong>：中层模仿策略未能执行期望的技能，最常见于抓取阶段，主要由于物体位置超出训练数据分布。2. <strong>VLM执行监控器失败</strong>：视觉验证由于细微的位置不准确或模糊的视觉线索而过早指示任务完成。3. <strong>VLM规划器失败</strong>：偶尔出现误接地问题，导致生成错误的技能序列。</p>
<p><img src="https://arxiv.org/html/2506.22827v3/x4.png" alt="失败分析"></p>
<blockquote>
<p><strong>图4</strong>：对40次试验中观察到的失败进行定性分析，展示了三种主要失败模式及其相对频率。技能策略失败是最常见的失败原因。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的分层视觉语言规划与监控框架，能够为人形机器人动态排序和验证操作技能，实现了从单技能执行到多步任务自主的跨越。</li>
<li>在真实人形机器人上构建并验证了一个集成的自主系统，能够在真实环境中执行复杂的多步任务，证明了该框架的实用可行性和有效性。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>评估任务局限于桌面抓放场景，未能充分体现家庭操作的组合多样性。</li>
<li>1Hz的验证频率对于快速技能（如快速恢复动作）可能不足。</li>
<li>技能策略可能因部分遮挡或分布外物体姿态而失效。</li>
<li>VLM规划器可能出现误接地或“幻觉”问题，监控器可能因细微几何条件误判而触发过早转换。</li>
</ol>
<p><strong>对后续研究的启示</strong>：<br>本文的工作表明，将大模型推理和感知与高频控制相结合是可行的。其显式的规划器-监控器设计提供了决策过程的可解释性，便于针对性调试。未来工作可以扩展任务范围，提高监控速度，并探索在技能间引入平滑过渡函数以增强运动流畅性。这种结合了学习型底层控制与基于VLM的高层语义推理的分层范式，为构建更通用、更可靠的人形机器人自主系统提供了有前景的路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对仿人机器人可靠执行复杂多步骤操作任务的挑战，提出分层视觉语言规划框架。系统包含三层：低层强化学习控制器跟踪运动目标；中层模仿学习技能策略生成任务步骤目标；高层视觉语言模型（VLM）规划技能序列并实时监控完成。在Unitree G1机器人上进行非抓取取放实验，40次真实试验中整体成功率达73%，验证了该方法的可行性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.22827" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>