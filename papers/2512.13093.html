<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.13093" target="_blank" rel="noreferrer">2512.13093</a></span>
        <span>作者: Wenjun Zeng Team</span>
        <span>日期: 2025-12-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>强化学习已成为人形机器人全身控制的主流数据驱动方法，但其样本效率低下是核心挑战，这源于人形机器人复杂的动力学、部分可观测性以及复合奖励函数优化。为提升样本效率，状态表征学习通过将高维感官输入压缩为紧凑且信息丰富的潜在表征，成为一种有前景的解决方案。现有方法主要包括基于重建、基于动力学建模和基于对比学习的方法。然而，在人形机器人全身控制中集成SRL仍未被充分探索：基于重建的方法（如预测特权信息）可能因保留不相关细节而导致表征质量次优和泛化能力差；而现有的对比学习方法（如PIM）仅依赖单一状态模态，未能利用特权状态信息，限制了其捕获完整任务相关动态的能力。本文针对上述痛点，提出了一个名为PvP的新视角：利用人形机器人的本体感觉状态和特权状态之间的内在互补性进行对比学习，以增强用于策略学习的本体感觉表征。本文核心思路是：将包含更丰富信息的特权状态视为本体感觉状态的伪增强，通过对比学习对齐这两种模态的表示，从而学习到紧凑且任务相关的表征，无需手工数据增强即可实现更快速、更稳定的策略学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>PvP框架旨在通过对比学习加速人形机器人全身控制任务的学习。其核心是利用本体感觉状态和特权状态之间的互补关系。本体感觉状态包含硬件可直接测量的信号（如关节位置、速度、基座角速度、重力方向估计），而特权状态包含仅在训练时可用的完整模拟器状态（如根位姿与线速度、接触标志、环境特征），且满足本体感觉状态是特权状态的子集。</p>
<p><img src="https://arxiv.org/html/2512.13093v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：PvP方法概述。(a) 特权状态和本体感觉状态的构成。(b) PvP基于两种状态模态之间的内在互补性进行对比学习。</p>
</blockquote>
<p>具体而言，PvP将特权状态 <code>s</code> 视为本体感觉状态 <code>o</code> 的伪增强。同时，对特权状态 <code>s</code> 中的特权信息部分进行零掩码，仅保留其中的本体感觉观测部分，得到 <code>s̃</code>。由此得到的数据对 <code>(s, s̃)</code> 用于训练策略编码器，遵循SimSiam算法。设策略编码器为 <code>f_θ</code>，预测器为 <code>h_ψ</code>，过程如公式(3)所示：<code>z = f_θ(s)</code>, <code>z̃ = f_θ(s̃)</code>, <code>p = h_ψ(z)</code>, <code>p̃ = h_ψ(z̃)</code>。PvP损失定义为公式(4)：<code>L_PvP = D_ncs(p, sg(z̃)) + D_ncs(p̃, sg(z))</code>，其中 <code>D_ncs</code> 为负余弦相似度损失，<code>sg</code> 为停止梯度操作。该方法创新性地利用两种模态的互补性进行对比，无需手工数据增强，能产生更丰富、更全面的表征。</p>
<p>为支持系统性评估，本文开发了SRL4Humanoid框架，这是首个为机器人学习提供的统一、模块化、即插即用的SRL方法高质量实现框架。</p>
<p><img src="https://arxiv.org/html/2512.13093v1/x2.png" alt="SRL4Humanoid框架"></p>
<blockquote>
<p><strong>图2</strong>：SRL4Humanoid框架架构，其中SRL和RL过程完全解耦。</p>
</blockquote>
<p>如图2所示，该框架以近端策略优化为骨干RL算法。策略网络接收机器人本体感觉状态以生成动作，价值网络接收环境特权状态进行价值估计。SRL与RL过程完全解耦，SRL目标可根据配置应用于策略编码器或价值编码器。框架实现了三种具有代表性的SRL算法（SimSiam、SPR、VAE），涵盖不同方法范式。总优化目标为 <code>L_Total = L_RL + λ · L_SRL</code>。针对大规模并行RL早期产生大量重复低质量数据可能导致SRL过早陷入局部最优的问题，框架采用了间隔更新机制：<code>L_Total = L_RL + 𝟙(T) · λ · L_SRL</code>，即每隔T步才应用SRL损失，以持续影响策略学习。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在LimX Oli人形机器人（31自由度）平台上进行，设计了两个代表性任务：<strong>LimX-Oli-31dof-Velocity</strong>（在平坦地形上跟踪速度指令）和<strong>LimX-Oli-31dof-Mimic</strong>（模仿20个预录制的人类动作）。骨干RL算法为PPO，对比的SRL基线方法包括SimSiam、SPR和VAE。评估指标包括总体任务性能、关键性能指标、训练效率以及仿真到实物的部署效果。</p>
<p><img src="https://arxiv.org/html/2512.13093v1/x4.png" alt="总体奖励对比"></p>
<blockquote>
<p><strong>图4</strong>：在两个人形机器人全身控制任务上，原始PPO代理及其与四种SRL方法结合的训练进度对比。实线和阴影区域分别表示平均值和标准差。</p>
</blockquote>
<p>关键实验结果如下：在总体任务性能上（图4），对于速度跟踪任务，PvP显著加速了学习过程，而其他SRL方法仅带来边际改进。对于运动模仿任务，PvP取得了最高性能，而VAE方法出现性能下降。这表明利用特权信息增强SRL能使智能体从噪声和冗余感官输入中提取更具信息量的特征。</p>
<p><img src="https://arxiv.org/html/2512.13093v1/x5.png" alt="动作平滑性优化对比"></p>
<blockquote>
<p><strong>图5</strong>：原始PPO代理及其与四种SRL方法结合在动作平滑性优化上的对比。</p>
</blockquote>
<p>在关键性能指标上，对于速度跟踪任务，PvP显著加速了动作平滑性惩罚项的收敛（图5），这有利于真实世界部署的可靠性。对于运动模仿任务，PvP在三个关键跟踪指标上也达到了最高性能（图6）。</p>
<p>消融实验方面：</p>
<ol>
<li><strong>训练时间比例影响</strong>（图7）：调整SRL更新间隔对性能有影响，间隔为50步时通常对所有SRL方法都是最优的，这有助于防止过早收敛到局部最优并减少计算开销。</li>
<li><strong>训练数据比例影响</strong>（图8）：在运动模仿任务中，增加用于SRL的训练数据比例（从10%到100%）通常能提升性能，特别是对SimSiam和PvP方法。</li>
<li><strong>SRL应用于价值编码器的影响</strong>（图9）：将SRL损失应用于价值编码器会导致收敛速度变慢，在速度跟踪任务中甚至出现训练崩溃，表明SRL应用于策略编码器能带来更稳定和增强的性能。</li>
<li><strong>计算效率</strong>：所有SRL模块在GPU上运行，不影响整体训练效率，能以最小的计算资源成本有效加速任务。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.13093v1/x10.png" alt="Sim2Sim评估"></p>
<blockquote>
<p><strong>图10</strong>：在MuJoCo模拟器上的仿真到仿真评估，展示了学习到的策略执行复杂任务（运动模仿和速度跟踪）的能力。</p>
</blockquote>
<p>最后，通过MuJoCo平台上的Sim2Sim评估（图10）和LimX Oli机器人的真实机器人测试，验证了学习策略在真实场景中的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了PvP，一种利用本体感觉与特权状态互补性进行对比学习的新框架，无需手工数据增强即可高效学习任务相关表征；2）开发了SRL4Humanoid，首个面向人形机器人学习的统一、模块化SRL框架，为可重复研究和系统分析提供了基础；3）通过大量实验验证了PvP在样本效率和最终性能上优于现有SRL基线，并为如何将SRL与RL有效集成提供了实践见解（如间隔更新机制、应用于策略编码器更有效）。</p>
<p>论文提及的局限性包括：尽管计算开销小，但SRL仍会引入额外的训练时间；PvP依赖于仿真中可用的特权状态，其向完全真实世界设置（无特权信息）的泛化能力有待进一步研究。</p>
<p>本文的启示在于：利用机器人不同传感模态间的内在关系（如本体感觉与特权信息）进行自监督表征学习，是提升数据效率的有效途径；将SRL作为RL的辅助任务时，更新策略（如间隔更新）和数据使用策略对最终性能有重要影响，需要精细设计；SRL4Humanoid框架为社区提供了一个宝贵的基准测试和开发平台，有望推动该领域的标准化进程和后续研究。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人全身控制中强化学习样本效率低下的核心问题，提出PvP框架。该方法利用本体感觉与特权状态的内在互补性，通过对比学习提取紧凑且任务相关的潜在表示，无需手工数据增强。实验在LimX Oli机器人上进行速度跟踪与运动模仿任务，结果表明PvP相比基线状态表示学习方法，显著提升了样本效率与最终性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.13093" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>