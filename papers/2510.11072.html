<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.11072" target="_blank" rel="noreferrer">2510.11072</a></span>
        <span>作者: Wang, Huayi, Zhang, Wentao, Yu, Runyi, Huang, Tao, Ren, Junli, Jia, Feiyu, Wang, Zirui, Niu, Xiaojie, Chen, Xiao, Chen, Jiahe, Chen, Qifeng, Wang, Jingbo, Pang, Jiangmiao</span>
        <span>日期: 2025/10/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人在现实世界中执行与场景交互的任务（如搬运物体、坐在椅子上）需要具备泛化性、逼真自然的动作以及鲁棒的场景感知能力。现有方法难以同时满足这些要求：基于模型的传统方法（如运动规划）计算成本高且模型假设强，泛化能力有限；基于强化学习的方法虽能获得泛化性，但通常需要精心设计奖励函数，且难以产生自然、逼真的动作；而基于运动捕捉先验的模仿方法能产生拟人动作，但大多局限于仿真环境，且依赖完美的场景观测，难以迁移到现实世界。本文针对将“泛化性”、“动作自然性”和“鲁棒现实感知”三者结合的挑战，提出了一个统一的系统PhysHSI。其核心思路是：在仿真中，利用基于对抗运动先验的策略学习，从多样化的交互数据中模仿自然动作风格以实现泛化和逼真行为；在现实部署中，设计一个结合激光雷达与相机的由粗到精的物体定位模块，以提供持续、鲁棒的场景感知。</p>
<h2 id="方法详解">方法详解</h2>
<p>PhysHSI系统包含仿真训练流程和现实世界部署系统两大部分。</p>
<p><img src="https://arxiv.org/html/2510.11072v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>: PhysHSI系统总览。(a) 数据准备：将MoCap数据重定向为人形机器人动作，并通过识别关键接触帧手动标注物体信息。(b) AMP策略训练：判别器区分策略生成的动作与参考动作，以促进自然行为和任务完成的学习。(c) 现实世界部署：使用激光雷达点云可视化手动指定物体的粗略位置，并结合里程计在物体位于相机视野外时进行粗略定位。一旦物体进入视野，则使用AprilTag检测结合里程计进行精细的、自动化的定位。</p>
</blockquote>
<p><strong>1. 数据准备</strong>：首先从AMASS和SAMP数据集中获取SMPL人体动作，通过优化将其重定向到人形机器人模型上，并应用平滑滤波器抑制抖动，得到仅包含机器人动作的数据集 ( M_{\text{Robo}} )。随后，手动标注关键接触帧（如拾取 (\phi_1) 和放置 (\phi_2)），并基于简单规则推断物体轨迹：在 (\phi_1) 和 (\phi_2) 之间，物体位置设置为双手的中点，方向与机器人基座对齐；在 (\phi_1) 之前和 (\phi_2) 之后，物体固定在各自关键帧的位置。由此产生包含一致且物理合理的物体位置信息的增强数据集 ( M )，用于后续的阶段条件设置和参考状态初始化。</p>
<p><strong>2. 对抗运动先验策略训练</strong>：将人形-场景交互问题构建为强化学习任务，采用对抗运动先验框架。该框架包含一个生成动作的策略 (\pi_\theta) 和一个区分策略动作与参考数据集中动作的判别器 (\mathcal{D})。</p>
<ul>
<li><strong>观测与动作空间</strong>：策略观测 (\mathbf{o}^\pi_t) 包含5步历史的本体感知 (\mathbf{o}^P_{t-4:t})（基座角速度、重力方向、关节位置/速度、末端执行器位置、上一时刻动作）和任务特定观测 (\mathbf{o}^G_{t-4:t})（物体尺寸、物体在基座坐标系下的位姿、目标位置）。判别器观测 (\mathbf{o}^\mathcal{D}_t) 则包含特权信息，如基座高度、线速度以及<strong>物体位置</strong>，后者对于让判别器隐式地感知任务阶段（接近、拾取、搬运、放置）至关重要。动作 (\mathbf{a}_t) 指定目标关节位置，由PD控制器执行。</li>
<li><strong>奖励函数与判别器学习</strong>：总奖励 (r_t = w^G r_t^G + w^R r_t^R + w^S r_t^S)，其中 (r_t^G) 为任务奖励，(r_t^R) 为关节扭矩和速度正则化奖励，(r_t^S) 为风格奖励。风格奖励通过判别器 (\mathcal{D}) 计算：(r_t^S = -\log(1 - \mathcal{D}(\mathbf{o}^\mathcal{D}_{t-t^*:t})))。判别器通过标准对抗训练进行优化，并包含梯度惩罚项。策略使用PPO算法优化以最大化累积折扣奖励。</li>
<li><strong>混合参考状态初始化</strong>：为解决长时程任务探索难的问题，采用了改进的参考状态初始化策略。一方面，从运动数据中随机采样初始阶段 (\phi \in [0,1])，但对后续阶段 ((\phi, 1]) 的场景参数（如目标位置）进行随机化；另一方面，一部分回合仍从默认起始姿势开始，并完全随机化场景参数。这种混合策略在保证探索效率的同时促进了泛化。</li>
<li><strong>非对称执行者-评论者训练</strong>：为匹配现实世界中部分观测受限的情况，采用非对称框架：执行者（actor）使用部署时可用的观测 (\mathbf{o}^\pi_t)，而评论者（critic）使用更丰富的状态 (\mathbf{o}^V_t)（如基座速度和未掩码的任务观测）。</li>
<li><strong>运动约束</strong>：为避免训练后期出现急促、抖动的动作，采用了逐步增加风格奖励权重 (w^S) 的策略，并引入了L2C2平滑正则化以增强动作平滑性。</li>
</ul>
<p><strong>3. 现实世界部署：由粗到精的物体定位模块</strong>：该模块旨在提供持续、鲁棒的物体位姿（位置 (\mathbf{p}^{o_t}<em>{b_t}) 和姿态 (\mathbf{R}^{o_t}</em>{b_t})）估计。</p>
<ul>
<li><strong>粗定位（物体在视野外）</strong>：初始化时，通过激光雷达点云可视化手动指定物体在初始基座坐标系下的粗略位姿 (T_{b_0}^{o_0})。运行时，利用FAST-LIO估计的里程计 (T_{b_0}^{b_t})，通过公式 (\mathbf{p}^{o_t}<em>{b_t}, \mathbf{R}^{o_t}</em>{b_t} = f_{\mathrm{T}}^{-1}((T_{b_0}^{b_t})^{-1} T_{b_0}^{o_0})) 计算出当前基座坐标系下的物体位姿，用于长距离引导。</li>
<li><strong>精定位（物体在视野内）</strong>：当物体进入相机视野，使用AprilTag检测获取物体在相机坐标系下的精确位姿 (T_{c_t}^{o_t})，并结合相机到基座的变换 (T_{b_t}^{c_t})（通过前向运动学获得）计算出精确的物体基座位姿。首次检测到AprilTag后，系统自动从粗定位切换至精定位。</li>
<li><strong>处理短暂丢失与动态物体</strong>：若检测暂时丢失（如机器人转身坐下），则保留最后观测到的位姿 (T_{c_{t&#39;}}^{o_{t&#39;}}) 及对应的 (T_{b_{t&#39;}}^{c_{t&#39;}})，并利用里程计 (T_{b_{t&#39;}}^{b_t}) 将其传播到当前时刻。对于静态物体（如椅子），位姿假定固定，采用上述传播策略。对于动态物体（如箱子），在抓取前使用上述估计；抓取后（通过距离阈值 (\epsilon) 判断），若物体离开视野，则将其位姿观测掩码，仅依赖本体感知完成任务。</li>
</ul>
<p><strong>4. 仿真到现实迁移</strong>：在训练中应用了领域随机化，包括向物体位姿和前向运动学观测添加随机偏移、高斯噪声和延迟，以及模拟现实部署中的掩码机制（当物体在视野外、目标距离超范围或相机角度偏差过大时掩码相关观测）。</p>
<p>与现有方法相比，PhysHSI的创新点具体体现在：1) 提出了一个包含手动物体标注和运动平滑的数据准备流程，为学习物理合理的交互提供了高质量数据；2) 在AMP框架中引入了包含物体信息的判别器观测和混合参考状态初始化策略，有效解决了长时程交互任务的阶段感知与泛化问题；3) 设计了紧密结合非对称训练、运动约束和领域随机化的仿真到现实迁移方案；4) 开发了融合激光雷达与相机、分阶段工作的鲁棒现实感知模块。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真和现实环境中，使用Unitree G1人形机器人平台，对四个代表性HSI任务进行评估：搬运箱子、坐下、躺下、站起。仿真实验在IsaacGym中实现。</p>
<p><strong>对比基线</strong>：</p>
<ol>
<li><strong>RL-Rewards</strong>：智能体从头开始学习，不使用运动参考，仅依赖精心设计的步态、任务和正则化奖励。</li>
<li><strong>Tracking-Based</strong>：智能体通过跟踪数据集中提供的人形和物体轨迹来模仿运动参考。</li>
</ol>
<p><strong>评估场景与指标</strong>：</p>
<ul>
<li><strong>场景</strong>：分布内场景（仅包含数据集中的场景设置）和全分布场景（在任务空间内均匀采样，如物体放置在起始位置0-5米内）。</li>
<li><strong>指标</strong>：成功率 (R_{\mathrm{succ}}) 和拟人度得分 (S_{\mathrm{human}})（由Gemini-2.5-Pro评估，范围0-5）。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.11072v1/figures/sim_generalization.png" alt="仿真泛化轨迹"></p>
<blockquote>
<p><strong>图3</strong>：空间泛化能力。展示了(a)搬运箱子和(b)躺下任务中机器人的根轨迹。红色轨迹为参考数据，其他颜色轨迹为策略生成的采样运动。表明PhysHSI仅从少量参考数据中就能学习到强大的泛化能力。</p>
</blockquote>
<p><strong>关键仿真结果（总结自表I）</strong>：</p>
<ul>
<li><strong>高成功率</strong>：PhysHSI在所有任务上均取得高成功率。例如，在最具挑战性的四子任务“搬运箱子”中，在分布内和全分布场景下的成功率分别达到91.34%和84.60%，显著高于基线。</li>
<li><strong>强泛化性</strong>：在全分布场景下，Tracking-Based方法因参考数据有限几乎完全失败（搬运箱子成功率仅0.02%），而PhysHSI凭借AMP框架保持了高性能（搬运箱子成功率84.60%），证明了其强大的泛化能力。</li>
<li><strong>自然的动作模式</strong>：PhysHSI在所有任务上的拟人度得分 (S_{\mathrm{human}}) 均大幅高于RL-Rewards基线，接近或超过4分（5分制），表明其动作高度逼真。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.11072v1/figures/real_generalization.png" alt="现实世界泛化"></p>
<blockquote>
<p><strong>图4</strong>：现实世界中的泛化。机器人成功在未曾见过的场景（如不同高度、位置的箱子和椅子）中执行任务，证明了系统在现实中的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.11072v1/x2.png" alt="现实世界结果"></p>
<blockquote>
<p><strong>图5</strong>：现实世界部署结果。系统在室内外多种场景下成功执行了四个交互任务，并展示了学习到的恐龙式行走和高抬腿等风格化运动。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.11072v1/figures/hardware.png" alt="硬件设置"></p>
<blockquote>
<p><strong>图6</strong>：硬件设置。Unitree G1机器人搭载Livox Mid-360激光雷达和Intel RealSense D455相机，所有计算在机载Jetson Orin NX上完成。</p>
</blockquote>
<p><strong>消融实验分析（总结自表II）</strong>：<br>在“搬运箱子”和“坐下”任务上进行的消融研究表明：</p>
<ol>
<li><strong>数据质量至关重要</strong>：不使用平滑数据（w/o Smoothness）会导致不自然的动作；移除物体标注（w/o Object）会显著降低成功率，因为物体状态信息对于学习阶段过渡和动作风格至关重要。</li>
<li><strong>混合RSI策略关键</strong>：与完全不使用RSI（w/o RSI）或使用朴素RSI（所有回合初始状态固定为数据集设置）相比，混合RSI策略在泛化性和样本效率上均有显著提升。朴素RSI甚至比不用RSI效果更差。</li>
<li><strong>观测掩码影响有限</strong>：在训练中引入与现实匹配的观测掩码机制（w/ Obs Mask）相比使用完整物体状态（w/o Obs Mask，性能上界）仅轻微影响最终成功率，表明系统对感知不确定性具有鲁棒性。</li>
</ol>
<p><strong>现实世界实验</strong>：PhysHSI成功在真实环境中完成了所有四项交互任务（图1，图5），并能够泛化到未经训练的物体位置和尺寸（图4）。系统完全依靠机载传感器和计算，实现了便携式部署。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了PhysHSI，一个完整的、可实现现实世界部署的人形-场景交互系统，首次在统一框架内同时解决了泛化性、动作自然性和鲁棒感知三大挑战。</li>
<li>设计了一套仿真的AMP训练流程，通过创新的数据标注、混合参考状态初始化等方法，从有限的人类演示中学习到了泛化性强且自然的交互策略。</li>
<li>开发了一个由粗到精的现实世界物体定位模块，仅使用机载激光雷达和相机，实现了在长时程交互任务中持续、鲁棒的场景感知。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前的物体精细定位依赖于AprilTag，这在一定程度上限制了交互物体的类型。未来需要探索更通用的物体检测与姿态估计方法。</p>
<p><strong>启示</strong>：本研究为复杂人形机器人技能的仿真到现实迁移提供了一个可借鉴的范式，即“高质量交互数据 + 对抗性风格学习 + 分阶段鲁棒感知”。其混合参考状态初始化、非对称训练结合特定领域随机化的方法，对于其他长时程、多阶段的具身智能任务具有参考价值。同时，工作突出了在仿真训练中紧密耦合现实感知约束的重要性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出PhysHSI系统，旨在解决人形机器人在现实场景中执行多样交互任务时，需同时实现强泛化性、动作自然性与鲁棒感知的核心挑战。系统采用仿真-现实双阶段框架：在仿真训练中，利用基于对抗运动先验的策略学习模仿人类交互数据，以生成泛化且自然的动作；在现实部署中，设计了结合LiDAR与相机的由粗到细物体定位模块，提供连续稳定的场景感知。实验在搬运、坐下、躺下、站起四项任务中验证了系统的高成功率、跨目标泛化能力以及自然的运动模式。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.11072" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>