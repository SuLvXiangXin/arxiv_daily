<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.13888" target="_blank" rel="noreferrer">2505.13888</a></span>
        <span>作者: Zhang, Ji, Wu, Shihan, Luo, Xu, Wu, Hao, Gao, Lianli, Shen, Heng Tao, Song, Jingkuan</span>
        <span>日期: 2025/05/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用预训练的视觉语言模型（VLMs）将语言指令和视觉观察映射到原始低级动作的视觉-语言-动作模型（VLAs）是实现通用机器人系统的一条有前景的途径。然而，尽管取得了进展，现有的VLA模型倾向于将任务无关的视觉特征与动作进行虚假关联（Spurious Correlations），而忽略了语言指令和视觉观察中的空间关系等重要因素，这限制了其在训练数据分布之外的泛化能力。例如，一个模型可能因为碗的尺寸较小，而从无关的干扰物或背景元素推断动作，而非关注目标物体本身。先前的研究通常诉诸于辅助训练数据或其他大型模型来增强思维链推理能力，但这些方法在效率和泛化性上存在局限。本文针对VLA模型因虚假相关性导致的泛化能力差这一具体痛点，提出通过增强模型的内在空间推理（Intrinsic Spatial Reasoning）能力来缓解这一问题的新视角。本文的核心思路是：在语言指令前添加一个关于物体相对于机器人方向的问题，引导模型先进行空间关系推理，并将推理得到的文本答案作为任务相关表征，再用于动作生成，从而将模型的注意力从虚假因素转向任务相关因素。</p>
<h2 id="方法详解">方法详解</h2>
<p>InSpire方法的整体框架是一个两阶段过程，旨在引导VLA模型先提取任务相关的空间关系信息，再基于此信息生成动作。输入为视觉观察 $o$ 和语言指令 $l$，输出为机器人动作 $a$。核心模块是一个空间推理视觉问答（VQA）任务，该任务作为连接观察与动作的“桥梁语言”。</p>
<p><img src="https://arxiv.org/html/2505.13888v3/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：InSpire方法总览。方法通过在原语言指令前添加一个空间方向问题（例如：“碗相对于机器人在哪个方向？”），引导VLA模型生成一个方向性答案（如：“右”），并将此问答对 $u&#39; = [q, g]$ 与动作一同作为目标进行训练。在推理时，模型先自生成此问答对，再基于此生成最终动作。</p>
</blockquote>
<p>具体而言，给定初始指令 $l$，InSpire首先使用自然语言工具包从中识别出物体名，并预置一个问题 $q$：“物体[object]相对于机器人在哪个方向？”。同一个VLA模型同时充当提取策略 $\pi_{u&#39;}$ 和动作策略 $\pi_{\theta}$。作为提取策略时，模型以 $[q, o, l]$ 为输入，生成一个文本答案 $g$。答案被约束在一个预定义的粗粒度方向集合中：{右， 左， 上， 下， 前， 后， 已抓取}。由此得到的问答文本对 $u&#39; = [q, g]$ 被视为提取出的任务相关表征。随后，该表征被传递给同一个VLA模型（此时作为动作策略 $\pi_{\theta}$），模型以 $[u&#39;, o, l]$ 为输入，自回归地输出最终的动作令牌 $a$。</p>
<p><img src="https://arxiv.org/html/2505.13888v3/x3.png" alt="自动标注规则"></p>
<blockquote>
<p><strong>图3</strong>：基于规则的物体方向自动标注。在轨迹的每个路径点，从仿真环境或真实世界记录中获取机器人夹爪和目标物体的3D位置。通过计算位置差 $\bm{d}$，并选取绝对值最大的分量轴及其符号，来确定粗粒度的方向关系（如：$x$ 轴差值最大且为正，则方向为“右”）。当夹爪闭合时，直接标记为“已抓取”。</p>
</blockquote>
<p>为了训练模型进行空间推理，需要为训练数据提供空间关系的真实标签。论文采用一种基于规则的自动标注策略（如图3）。给定机器人末端执行器位置 $[x_i, y_i, z_i]$ 和物体位置 $[x_0, y_0, z_0]$，计算位置差 $\bm{d}$。粗粒度空间关系由 $\bm{d}$ 中绝对值最大的分量对应的轴决定。例如，若 $|x_i - x_0|$ 最大，则根据 $x_i - x_0$ 的符号关系被归类为“左”或“右”。当检测到夹爪闭合时，则直接标记为“已抓取”。在训练时，自回归损失不仅应用于动作令牌，也应用于空间推理答案的文本令牌，以鼓励模型预测正确的空间关系描述。</p>
<p>与现有方法相比，InSpire的创新点在于：1）无需收集额外的训练数据或与其他大型模型交互；2）通过一个精心设计的、与动作高度相关的空间VQA任务，显式地引导模型关注任务相关的空间因素；3）可以作为一种即插即用的插件，无缝集成到现有的自回归VLA模型中。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了仿真基准LIBERO和CALVIN，以及真实世界任务。对比的基线方法包括：miniVLA-VQ、π₀-FAST、OpenVLA、SpatialVLA、CoT-VLA-7B、Diffusion Policy和Octo。评估指标主要为任务成功率。</p>
<p>在LIBERO基准上，模型在LIBERO-90数据集上训练，并在其已见任务及其他四个未见数据集（-Spatial, -Object, -Goal, -Long）上评估。关键结果如表1所示：InSpire显著提升了基线模型的性能。例如，在miniVLA-VQ上，已见任务成功率从83.3%提升至89.5%（+6.2%），未见任务平均成功率从3.6%提升至13.6%（+10.0%）。特别地，在需要空间关系和物体交互的LIBERO-Spatial和LIBERO-Object任务上提升最为显著（分别+13.0%和+20.1%）。</p>
<p><img src="https://arxiv.org/html/2505.13888v3/figs/calvin.png" alt="CALVIN性能"></p>
<blockquote>
<p><strong>图4</strong>：CALVIN长序列任务性能。InSpire在完成连续1到5个子任务的数量上均 consistently 优于所有基线方法，展现了更强的长时程任务处理能力。</p>
</blockquote>
<p>在CALVIN长序列任务评估中（图4），InSpire在连续完成1至5个子任务的所有指标上均 consistently 优于基线方法，证明了其缓解虚假相关性后带来的更准确和鲁棒的动作生成能力。</p>
<p><img src="https://arxiv.org/html/2505.13888v3/figs/realresults.png" alt="真实世界性能"></p>
<blockquote>
<p><strong>图5</strong>：真实世界实验结果。(a)(b)显示InSpire将π₀-FAST在10个已见任务和5个未见任务上的平均成功率分别提升了25%和26%。(c)显示InSpire平均每步增加约0.18秒的时间开销，但考虑到成功率的显著提升，此权衡是可接受的。</p>
</blockquote>
<p>在真实世界实验中（图5），InSpire在10个已见任务和5个未见任务上，将强基线π₀-FAST的平均成功率分别提升了25%和26%。在4/5的未见任务上，相对成功率提升了100%。虽然因引入更多推理令牌导致每步平均时间增加0.18秒，但性能增益显著。</p>
<p><img src="%E8%A1%A8%E6%A0%BC%E5%BD%A2%E5%BC%8F%EF%BC%8C%E6%97%A0%E7%8B%AC%E7%AB%8B%E5%9B%BE%E7%89%87%E9%93%BE%E6%8E%A5" alt="SOTA对比"></p>
<blockquote>
<p><strong>表II</strong>：与最先进方法的对比。将应用了InSpire的1B参数模型（InspireVLA-1B）与包括4B参数的SpatialVLA和7B参数的CoT-VLA等模型在四个LIBERO数据集上对比。InspireVLA-1B以更少的参数量取得了最佳平均性能（86.7%），优于CoT-VLA-7B（83.9%）和SpatialVLA-4B（78.1%）。</p>
</blockquote>
<p>消融实验探究了不同设计决策的影响。表III显示，在五种空间VQA任务形式中，“1D方向”和“3D方向”预测表现最佳，显著优于“接近度”、“3D位置”和“精确距离”预测，表明方向性信息对动作生成的引导更有效。图6的消融表明，将VQA令牌插入在语言指令之前（“VQA优先”）在已见任务上表现更好。</p>
<p><img src="https://arxiv.org/html/2505.13888v3/figs/vqaposi.png" alt="VQA插入位置消融"></p>
<blockquote>
<p><strong>图6</strong>：空间推理VQA任务插入位置的消融研究。比较将VQA令牌放在语言指令前（VQA-First）和指令后（Instruct-First）的性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.13888v3/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图7</strong>：定性结果。左侧对比了基线（上）与InSpire（下）生成的动作序列，InSpire能成功完成任务而基线失败。右侧的注意力热图显示，基线模型关注任务无关区域，而InSpire能将注意力正确集中在任务相关物体（碗和盘子）上。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了InSpire，一种通过增强VLA内在空间推理能力来缓解虚假相关性负面影响的新方法；2）该方法无需额外数据或外部大模型，能以即插即用方式增强现有自回归VLA；3）在仿真和真实环境中的广泛实验证明了其有效性和灵活性。</p>
<p>论文提到的局限性包括：1）InSpire对预训练数据量敏感，在预训练数据有限的模型（如π₀-FAST在LIBERO上微调）上提升幅度相对较小；2）引入空间推理VQA任务会带来额外的推理时间开销。</p>
<p>本文的启示在于：1）在观察与动作之间引入一个与任务高度相关的、可学习的中间推理表示（如空间关系），是引导模型关注因果因素、提升泛化能力的有效桥梁；2）简单的、基于文本的干预可以有效地引导大型预训练模型的注意力，无需复杂架构修改；3）粗粒度的方向性信息比精确的坐标或距离信息更适合作为引导动作生成的空间先验。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型易受任务无关视觉特征干扰、泛化能力受限的问题，提出InSpire方法。该方法通过前置“物体相对于机器人在哪个方向？”的空间推理问题，引导模型关注任务相关因素，并将输出的方向答案与真实动作对齐。该方法无需额外数据或调用其他大模型，可作为插件增强现有自回归VLA。实验在仿真和真实环境中验证了其有效性与灵活性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.13888" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>