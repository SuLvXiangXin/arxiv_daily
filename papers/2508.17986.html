<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.17986" target="_blank" rel="noreferrer">2508.17986</a></span>
        <span>作者: Matej Hoffmann Team</span>
        <span>日期: 2025-08-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作感知领域长期由视觉输入（RGB或RGB-D摄像头）主导，用于物体分割、位姿估计和生成抓取方案。触觉传感器通常处于次要地位，偶尔在指尖提供反馈。然而，在视觉感知受限（如光照差、粉尘、烟雾、遮挡）的场景下，依赖视觉的方法面临挑战。本文探索了一种极端情况：在完全没有视觉反馈的情况下，仅依靠覆盖机器人全身的触觉反馈来搜索和抓取物体。核心思路是将搜索分为两个阶段：首先利用覆盖机械臂全身的敏感皮肤进行粗略的工作空间探索，快速定位物体大致区域；然后使用配备力/扭矩传感器的末端执行器在该区域内进行精确定位并执行抓取。</p>
<h2 id="方法详解">方法详解</h2>
<p>方法整体流程分为粗略探索和精确定位抓取两个主要阶段。机器人从工作空间一侧（如右侧）的起始位置开始，手臂完全伸展并抬高到物体上方。然后以固定步长向另一侧（左侧）移动，每移动一步，便垂直向下移动一段距离。在此过程中持续监测全身皮肤的状态。一旦检测到碰撞，便停止运动，启动精确定位模块，随后执行抓取操作。抓取完成后，或机器人下降到最低位置（桌面），机器人将回升到下降前的高度，继续横向移动至下一步。循环直至机器人到达最左侧位置。</p>
<p><img src="https://arxiv.org/html/2508.17986v2/Figs/schema.png" alt="方法流程示意图"></p>
<blockquote>
<p><strong>图3</strong>：探索流程示意图。机器人从最右侧位置（手臂完全向前伸展）开始，以固定步长横向移动。如果未到达终点位置，则在每一步向下移动。若发生碰撞，则开始精确定位，随后进行抓取操作。抓取结束后或机器人移动到最低位置后，它向上返回。循环持续直到机器人到达终点位置。</p>
</blockquote>
<p>核心模块包括基于全身皮肤的粗略定位、基于末端执行器的精确定位以及抓取姿态估计与调整。</p>
<ol>
<li><strong>粗略定位</strong>：当特定皮肤垫（pad）检测到与物体接触时，根据机器人当前位姿，将该垫的二维矩形区域投影到桌面上（图4a）。该矩形代表了物体可能存在的区域上界。由于皮肤垫空间分辨率较低（最小垫投影为边长12cm的正方形，最大为40x15cm的矩形），此区域范围较大。</li>
<li><strong>精确定位</strong>：在粗略定位得到的矩形区域内，使用末端执行器进行笛卡尔线性运动来精确寻找物体边界（算法1）。机器人从矩形区域最左侧的后方位置（S1）向机器人基座方向（G1）移动，末端执行器探头指向接触垫最低点的高度。利用腕部的力/扭矩传感器检测与物体的碰撞。若未检测到碰撞，则在相邻的S2、G2等位置重复此扫描，相邻起始点的距离等于抓取器面向物体一侧的有效宽度。若在移动中检测到碰撞，则记录接触点 <strong>p1</strong> 和运动方向 <strong>v1</strong>。随后，机器人执行与前一次运动方向正交的矫正性笛卡尔线性运动（从So开始，图4b）。若在此过程中检测到碰撞，则记录第二个接触点 <strong>p2</strong> 和方向 <strong>v2</strong>。获得两个点后，精确定位完成；若未检测到，则丢弃 <strong>p1</strong> 并继续扫描。</li>
<li><strong>抓取</strong>：根据两个接触点 <strong>p1</strong>, <strong>p2</strong> 和方向 <strong>v1</strong>, <strong>v2</strong>，通过两条射线 <strong>r1</strong> = <strong>p1</strong> + s<strong>v1</strong> 和 <strong>r2</strong> = <strong>p2</strong> + s<strong>v2</strong> 的交点估算物体的二维中心 <strong>c</strong>。抓取高度设置为接触皮肤垫的最低点高度。初始抓取位姿 <strong>G</strong> 由中心 <strong>c</strong> 和朝向构成，抓取器爪片平行于桌面，且每个手指到物体中心的距离相等，接近方向与 <strong>v2</strong> 相同（图4c）。由于皮肤垫分辨率低、传感器噪声以及仅使用两个表面点导致形状信息缺失，中心估计存在不确定性。因此，若初始抓取失败，系统会按顺序测试一系列备选抓取位姿 <strong>Gi</strong>，这些位姿是初始抓取位姿在桌面上的固定二维变换，包括：沿接近方向向后偏移（0-40 mm）、侧向偏移（-45至45 mm）以及绕中心旋转（-30至30度）。直到检测到成功抓取或将物体移至收纳处，或所有备选位姿尝试完毕。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.17986v2/Figs/scan.png" alt="定位与抓取示意图"></p>
<blockquote>
<p><strong>图4</strong>：(a) 桌面（橙色）、物体（红色）和接触垫投影区域（蓝色）的俯视图。Sn, Gn是第n次精确定位的起始和结束位置。(b) 物体（红色）及来自初始方向（S, <strong>v1</strong>）和正交方向（So, <strong>v2</strong>）的两个接触点 <strong>p1</strong>, <strong>p2</strong>（黄色）的俯视图。(c) 初始抓取位姿的俯视图。彩色坐标系显示了用于抓取位姿细化的调整方向。</p>
</blockquote>
<p>本文的主要创新点在于利用低分辨率、大面积的全身敏感皮肤作为分布式触觉传感器，进行快速的工作空间初筛，将需要精细操作（定位和抓取）的搜索范围从整个桌面大幅缩小到皮肤垫投影的局部区域，从而显著提升了整体效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台包括真实的UR10e机械臂（配备AIRSKIN全身皮肤和OnRobot RG6平行夹爪）以及基于PyBullet的仿真环境。使用了YCB数据集中的五个不同形状的物体。对比的基线方法是仅使用末端力/扭矩传感器，以与精确定位模块类似的笛卡尔线性运动扫描整个桌面。</p>
<p><img src="https://arxiv.org/html/2508.17986v2/Figs/setup.png" alt="机器人实验设置"></p>
<blockquote>
<p><strong>图2</strong>：真实世界（左）和仿真（右）中的机器人与物体。</p>
</blockquote>
<p><strong>关键实验结果：</strong></p>
<ol>
<li><p><strong>全身皮肤的效果</strong>：与仅使用末端传感器的基线方法相比，全身皮肤方法在探索速度上具有巨大优势。无物体时，本文方法快约13倍（87秒 vs 1135秒）；有一个物体时，快6倍（197秒 vs 1198秒）。速度提升源于皮肤提供的快速初筛能力。</p>
</li>
<li><p><strong>物体位置的影响</strong>：在仿真中，将圆柱体物体放置在桌面网格的不同位置进行测试。结果表明，在工作空间中部大部分区域，成功率至少为80%（5次尝试成功4次）。靠近机器人基座或工作空间边缘的区域，成功率下降，这与这些区域的可操作性和可达性降低有关。整体平均成功率为83.1%，表明方法对物体位置具有较好的鲁棒性。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.17986v2/x1.png" alt="不同位置成功率"></p>
<blockquote>
<p><strong>图8</strong>：仿真中圆柱体物体在工作空间不同位置的成功率。每个方块代表圆柱体中心的一个位置。数值基于每个位置5次重复实验计算。机器人基座位于(0, 0)位置。</p>
</blockquote>
<ol start="3">
<li><strong>物体形状和朝向的影响</strong>：测试了所有物体在不同朝向下的抓取成功率。仿真中的平均成功率为75.7%，真实世界为85.7%（真实世界成功率更高，部分归因于真实物体的可变形性提供了更好的抓握）。结果表明，物体形状和朝向显著影响性能。对于非对称物体，0°和±90°朝向通常成功率更高，而±45°朝向成功率较低，这是因为方法仅使用两个垂直接触点，无法推断物体形状或朝向，导致初始抓取位姿可能未与物体主轴对齐。物体宽度也影响抓取，例如，边长为8.5cm的方块物体，因其尺寸接近夹爪最大宽度（15cm），且±45°旋转时有效宽度为对角线（12cm），导致成功率最低。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.17986v2/Figs/objects_bars.png" alt="不同物体与朝向成功率"></p>
<blockquote>
<p><strong>图9</strong>：仿真（上）和真实世界（下）中抓取不同旋转角度物体的成功率。圆圈颜色对应成功率，大小对应平均耗时。垂直线的颜色显示每个物体的平均成功率，每个旋转角度的垂直线。0度旋转的物体图示在底部，左侧展示了两个物体在所有旋转下的图示。第一个物体（圆柱）沿Z轴对称，因此只考虑一种旋转。数值基于10次重复实验计算。</p>
</blockquote>
<ol start="4">
<li><strong>处理多个物体（杂乱场景）</strong>：测试了同时存在两个和三个物体的场景。对于两个物体的情况，在仿真和真实世界中都进行了所有物体组合的测试。系统能够处理多个物体，成功移除至少一个物体的比率较高。实验表明，即使一个物体抓取失败或翻倒，系统通常也能继续处理剩余物体。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.17986v2/Figs/two.png" alt="两个物体实验成功率"></p>
<blockquote>
<p><strong>图10</strong>：仿真（上）和真实世界（下）中桌面上同时有两个物体实验的成功率。颜色对应成功率，圆圈大小对应平均耗时。对于每对“列-行”组合，“列”中的物体首先与机器人接触。数值基于仿真10次、真实世界5次重复实验计算。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li>首次在实际机器人系统上验证了仅依靠覆盖全身的触觉反馈（无视觉）进行工作空间探索、物体定位与抓取的可行性。</li>
<li>提出了一种高效的两阶段流程：利用低分辨率全身皮肤快速缩小搜索范围，再使用高精度末端传感器进行局部精确定位和抓取，相比仅用末端触觉的基线方法速度提升6倍。</li>
<li>在仿真和真实环境中进行了系统评估，展示了该方法对不同物体位置、形状和一定杂乱程度的鲁棒性，真实机器人单物体抓取成功率达85.7%。</li>
</ol>
<p>论文提到的局限性包括：</p>
<ol>
<li>皮肤空间分辨率低，导致粗略定位区域较大。</li>
<li>方法假设所有物体均可通过侧向抓取（lateral grasp）抓握，且机器人有足够的操作空间。</li>
<li>精确定位仅依赖两个接触点，无法估计物体形状或精确朝向，导致对某些物体朝向（如±45°）的抓取成功率较低。</li>
</ol>
<p>本工作对后续研究的启示：为视觉受限场景下的机器人操作提供了新思路；表明即使使用低成本的粗分辨率触觉皮肤也能显著提升效率；未来的工作可以探索集成更高分辨率的皮肤、结合稀疏的视觉或接近觉信息、利用多次接触改进物体形状与朝向估计，以及开发更智能的抓取策略以适应更复杂的物体几何形状。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究机器人**在完全无视觉输入下仅凭触觉定位与抓取物体**的核心问题。提出**分阶段触觉搜索方法**：先利用覆盖敏感皮肤的全身表面进行粗略探索，再通过末端执行器的力/扭矩传感器精确定位。实验表明，该方法在实物机器人上对单物体抓取成功率达**85.7%**，且**比仅使用末端触觉反馈的基线方法快6倍**，验证了全身触觉感知在视觉受限场景下的高效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.17986" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>