<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.15957" target="_blank" rel="noreferrer">2512.15957</a></span>
        <span>作者: Panchal, Utsav, Liu, Yuchen, Palmieri, Luigi, Georgievski, Ilche, Aiello, Marco</span>
        <span>日期: 2025/12/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前人类行为预测领域的主流方法主要集中于从自我中心视角预测单个人的行为，利用了基于CNN、RNN/LSTM的模型以及近期的大型语言模型（LLM）和视觉语言模型（VLM）。然而，对于许多移动机器人应用（如自主导航、人机协作）而言，需要从第三人称视角观察并预测多个人在复杂环境中的行为。这一场景带来了额外的挑战：需要捕捉人与人之间的依赖关系（如避让、轮流）、跨场景的长程身份一致性推理、人-物交互理解以及处理遮挡问题。现有方法在空间动态理解方面相对匮乏，而空间理解对于解释复杂环境中的人-物交互至关重要。同时，缺乏适用于第三人称视角、多人类行为预测且包含结构化环境表示（如场景图）的公开数据集。</p>
<p>本文针对上述痛点，提出了一个新的视角：将视觉语言模型与场景图相结合，以实现上下文感知的多人类行为预测。本文的核心思路是：利用VLM统一处理来自视频帧的视觉上下文和来自场景图的结构化空间知识，并通过一个结合监督微调与直接偏好优化的两阶段微调策略，在数据有限的情况下显著提升预测准确性。</p>
<h2 id="方法详解">方法详解</h2>
<p>CAMP-VLM是一个以VLM为中心的预测框架，其整体流程如图2所示。输入包括：1）一段过去观察的视频帧序列 $\mathbf{V}^{-H+1:0}$，作为视觉上下文表示；2）一个描述环境拓扑和物体间关系的场景图 $\mathcal{G}$；3）一个描述任务指令的文本提示。输出是未来一段时间内每个预测的人类行为标签集合 $\hat{\mathbf{Y}}$，每个标签是一个包含人ID、动词和对象的元组。</p>
<p><img src="https://arxiv.org/html/2512.15957v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：CAMP-VLM框架概览。视频帧通过视觉编码器处理为视觉令牌，经由投影层映射后输入大型语言模型（LLM）骨干。图像中编码的上下文帮助VLM辨别人与场景的交互。场景图中编码的场景知识用于将预测锚定在提供的场景拓扑和关系中。在用户提供的提示指导下，LLM以指定格式预测人类行为。微调过程仅更新LLM的权重，视觉编码器和投影层保持冻结。</p>
</blockquote>
<p>框架的核心模块包括：</p>
<ol>
<li><strong>视觉上下文表示</strong>：输入为 $H$ 张RGB图像，来自固定摄像机位置，描绘了过去一段时间内一个或多个人的活动。VLM从中提取人数、姿态、朝向以及周围物体等信息。</li>
<li><strong>场景图表示</strong>：本文使用2D场景图作为高层次环境表示。场景图 $\mathcal{G} = {\mathcal{N}, \mathcal{E}}$ 包含节点列表 $\mathcal{N}$ 和边列表 $\mathcal{E}$。节点属性包括物体ID、属性（定义交互方式，如“可抓取”）和状态（当前状态，如“关闭”）。边定义了物体间的空间关系（如“在...之上”）。场景图被转换为扁平化、易读的JSON格式以供模型使用。</li>
<li><strong>提示设计</strong>：精心设计的提示明确了任务角色、给定输入（视频帧和场景图文件）以及严格的输出格式要求，指导模型生成结构化的预测结果。</li>
</ol>
<p>本文的创新点具体体现在：1）<strong>首次整合</strong>视觉输入与场景图，用于第三人称视角下的多人类行为预测；2）提出了一种<strong>两阶段微调策略</strong>以高效提升模型性能。</p>
<p><img src="https://arxiv.org/html/2512.15957v1/x3.png" alt="微调与数据生成流程"></p>
<blockquote>
<p><strong>图3</strong>：CAMP-VLM的两阶段微调与数据生成流程。左上：监督微调（SFT）阶段，使用包含输入（视频、场景图、提示）和真实行为标签的数据集。右上：直接偏好优化（DPO）阶段，使用SFT阶段得到的参考模型生成多个预测，基于与真实标签的编辑距离选择“偏好”和“非偏好”响应构建偏好数据集，然后进行DPO训练。</p>
</blockquote>
<p>微调过程分为两个阶段，均基于开源的Qwen2-VL模型进行：</p>
<ul>
<li><strong>第一阶段：监督微调（SFT）</strong>：使用由输入三元组 $(\mathbf{V}^{-H+1:0}, \mathcal{G}, P)$ 和真实行为标签 $\mathbf{y}_i^{gt}$ 构成的数据集 $\mathcal{D}_F$。采用低秩适应（LoRA）技术，仅训练低秩矩阵以降低计算成本，通过最小化负对数似然损失 $\mathcal{L}_F$ 来适配模型。</li>
<li><strong>第二阶段：直接偏好优化（DPO）</strong>：为了进一步提升模型输出质量并避免小数据集上的过拟合，在SFT之后引入DPO。首先，使用SFT得到的参考模型对每个输入生成多个预测，然后根据与真实标签的编辑距离，自动选择编辑距离最小（偏好）和最大（非偏好）的响应，构建偏好数据集 $\mathcal{D}_P = {(\mathbf{x}_i, \mathbf{y}_i^w, \mathbf{y}_i^l)}$。随后，通过最小化DPO损失函数 $\mathcal{L}_P$ 来训练模型，使其更倾向于生成与偏好响应一致的结果。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验设置</strong>：由于缺乏现成的合适数据集，作者利用VirtualHome模拟器生成了一个包含厨房、卧室、客厅场景的<strong>合成数据集</strong>，涵盖1-3个人类，共30段视频。同时，录制了10段<strong>真实世界视频</strong>（办公室厨房和客厅）。所有数据均包含视频序列和对应的场景图。实验采用70/30的训练-测试划分。预测任务设定为给定过去6帧（3秒）观察，预测未来6个时间步（3秒）的行为。</p>
<p><strong>基线方法</strong>：</p>
<ol>
<li><strong>AntGPT</strong>：在Ego4D LTA基准上领先的自我中心行为预测方法，本文在其上使用合成数据进行了微调。</li>
<li><strong>CAP</strong>：一项近期研究中在第三人称单人类行为预测上表现最佳的配置，结合了GPT-4o、自回归预测和最大允许的上下文学习示例。</li>
</ol>
<p><strong>评估指标</strong>：准确率（完全匹配）、动词/名词部分准确率、余弦相似度、编辑距离。</p>
<p><strong>关键实验结果</strong>：<br>在合成数据集上的多人类场景测试结果如表2所示。CAMP-VLM在大多数场景和指标上显著优于基线。例如，在2人厨房场景中，CAMP-VLM的完全准确率达到0.482，相比最佳基线CAP（0.205）提升了135%，相比AntGPT（0.053）提升了809%（约66.9个百分点）。动词和名词的单独准确率也显示出CAMP-VLM能更好地理解动作和对象。</p>
<p>在真实世界数据集上的测试（表3部分数据）进一步验证了其泛化能力。例如，在2人厨房场景中，CAMP-VLM的准确率（0.425）优于CAP（0.326）。</p>
<p><img src="https://arxiv.org/html/2512.15957v1/images/scene_exp.png" alt="数据集示例场景"></p>
<blockquote>
<p><strong>图4</strong>：数据集中示例场景。从左至右分别为：VirtualHome模拟器生成的厨房、客厅、卧室场景，以及真实世界录制的办公室厨房和客厅场景。展示了多人类活动的环境。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>作者进行了模型规模（72B, 7B, 2B）和微调策略的消融研究。结果表明：1）更大的模型规模（72B）带来最佳性能；2）两阶段微调（SFT+DPO）策略显著优于仅使用SFT，在合成数据上准确率从0.424提升至0.482，在真实数据上从0.392提升至0.425，证明了DPO阶段对于提升预测质量和泛化性的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了<strong>CAMP-VLM</strong>，首个将视觉输入与场景图相结合用于第三人称多人类行为预测的VLM框架；2）设计了一种<strong>两阶段微调流程</strong>（SFT+DPO），在数据有限的情况下有效提升了预测准确性和输出质量；3）在合成与真实数据集上进行了全面评估，证明了该方法相比现有先进方法的显著优势（准确率提升最高达66.9%）。</p>
<p>论文提到的局限性包括：1）数据集规模相对较小；2）使用的场景图是2D的，未来可探索3D场景图；3）大模型（如72B版本）的计算成本较高。</p>
<p>本工作对后续研究的启示在于：1）<strong>融合多模态与结构化知识</strong>是提升复杂场景理解与预测能力的关键方向；2）<strong>高效微调策略</strong>（如结合SFT与偏好对齐）对于在特定领域数据稀缺时解锁大模型潜力具有重要意义；3）为面向<strong>真实世界机器人应用</strong>（如社交导航、协作）的行为预测研究提供了一个新的、强有力的基准框架。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人需在第三人称视角下预测多人在复杂场景中交互行为的核心问题，提出了CAMP-VLM框架。该框架基于视觉语言模型，关键技术在于融合视觉输入的上下文特征与场景图提供的空间拓扑信息。通过使用合成数据进行监督微调和直接偏好优化，模型在预测准确率上比最佳基线方法提升了66.9%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.15957" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>