<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.22519" target="_blank" rel="noreferrer">2512.22519</a></span>
        <span>作者: Ngan Le Team</span>
        <span>日期: 2025-12-27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉-语言-动作模型通过在大规模机器人演示数据上对大型视觉-语言模型进行后训练，以实现动作预测，在通用机器人操作方面取得了显著进展。然而，大多数VLA模型在一个端到端优化的单一流程中纠缠了感知与控制，这可能会侵蚀语言条件的视觉接地能力。在真实桌面测试中，现有策略在目标缺失时会过度抓取、容易被杂物分散注意力，并且对背景外观过拟合。</p>
<p>本文针对VLA模型在杂乱现实场景中语言条件接地能力退化这一具体痛点，提出了一个新颖的视角：将感知接地与动作推理显式地解耦。核心思路是引入一个感知模块，将原始多视角RGB输入转化为任务条件化、以物体为中心且几何感知的观测，再将其输入到预训练的VLA策略中进行动作推理，从而在仅使用干净单物体演示数据微调的情况下，提升模型在杂乱环境中的鲁棒性和泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>OBEYED-VLA的整体框架旨在将感知接地与动作推理分离。其流程是：原始RGB观测（来自基座和腕部摄像头）首先输入到一个感知接地模块，该模块输出经过杂波抑制、几何感知的视觉输入；随后，这些处理后的观测与语言指令、机器人本体感知状态一同输入到一个预训练的VLA模型中，由其生成动作轨迹。在整个框架中，只有下游的VLA模型需要针对具体任务进行微调，感知模块保持冻结，实现了即插即用的集成。</p>
<p><img src="https://arxiv.org/html/2512.22519v1/x3.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图3</strong>：OBEYED-VLA架构概览。来自基座和腕部摄像头的原始RGB图像首先通过分割网络获取物体级掩码。随后，基于VLM的物体中心接地模块选择与任务相关物体对应的掩码子集，而几何接地模块则对这些掩码区域应用深度估计，生成聚焦于这些区域的、抑制了杂波的几何感知观测。最终得到的感知接地观测，连同语言指令和机器人本体感知，被输入到一个预训练的VLA模型中，由其输出动作轨迹。</p>
</blockquote>
<p>感知接地模块包含两个核心组件：物体中心接地和几何接地。</p>
<ol>
<li><strong>物体分割提议</strong>：首先使用一个离线的分割模型处理双视角RGB图像，生成覆盖工作空间中所有可见物体（包括机械臂）的掩码提议。为了平衡分割的完整性和实时性，作者微调了YOLO11-Seg模型，其训练数据混合了自动标注的机器人演示数据和一个精选的室内桌面物品LVIS子集。</li>
<li><strong>物体中心接地</strong>：该模块利用VLM（如Qwen3-VL）选择与任务相关的物体区域。具体过程分为两步（如<strong>图4</strong>所示）：<ul>
<li><strong>任务感知的基座视图物体接地</strong>：首先，VLM解析语言指令，列出任务相关的物体名称。接着，在基座视图图像上，使用“set-of-mark”提示机制，在每个分割掩码区域内覆盖一个数字标记。然后，VLM根据物体名称和标记增强的图像，识别出与指令相关的标记，从而筛选出相关的掩码子集。在每次任务开始时执行一次此步骤，并跟踪所选掩码。</li>
<li><strong>跨视图区域匹配</strong>：将上一步筛选出的基座视图掩码区域裁剪为物体中心的参考视图。对于腕部视图，同样生成标记增强的图像。然后，VLM被提示将腕部视图中的标记与基座参考视图进行匹配，从而筛选出腕部视图中与任务相关的掩码子集。</li>
</ul>
</li>
<li><strong>几何接地</strong>：对于上述物体中心接地模块筛选出的相关掩码区域，首先在原始RGB图像中抑制所有背景像素（包括无关物体），生成仅保留任务相关物体的“净化”RGB视图。然后，使用零样本深度估计器将这些区域的RGB像素转换为深度图，从而保留物体的3D形状和空间布局，同时丢弃颜色、纹理等外观线索，促使策略依赖几何而非表面视觉关联。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.22519v1/x4.png" alt="物体中心接地模块"></p>
<blockquote>
<p><strong>图4</strong>：物体中心接地模块。该模块分两阶段运行。首先，VLM解析任务指令以提取任务相关物体，并利用基座视图分割掩码上的“set-of-mark”提示来选择对应这些物体的区域。裁剪选定的基座视图区域以生成物体中心参考视图。</p>
</blockquote>
<p>与现有方法相比，OBEYED-VLA的核心创新在于：1) <strong>显式解耦</strong>：将感知接地作为独立于动作推理的预处理模块，避免了端到端动作优化对VLM继承的视觉-语言对齐的侵蚀。2) <strong>双重视觉接地</strong>：结合了语义（VLM驱动的物体选择）和几何（RGB转深度）两种接地形式，强化了空间理解和抗外观干扰能力。3) <strong>数据效率</strong>：无需在训练数据中合成杂乱场景或引入额外的感知监督损失，仅使用干净的单物体演示微调VLA即可。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在一个真实的UR10e桌面机器人平台上进行。评估使用了四种具有挑战性的场景：存在干扰物、目标缺失指令、背景外观变化以及操作未见过的物体。基线方法包括当前先进的VLA模型，如Octo、RoboFlamingo、OpenVLA、π0、π0-FAST和π0.5。</p>
<p><img src="https://arxiv.org/html/2512.22519v1/x2.png" alt="目标缺失检查"></p>
<blockquote>
<p><strong>图2</strong>：视觉-语言接地的目标缺失完整性检查。我们报告了每个（请求的，展示的）物体对的抓取率（%），计算了所有请求（行）和展示（列）物体组合的20次运行结果。非对角线上的高强度值直接揭示了当请求物体缺失时策略执行抓取的频率，显示了基线模型的语言接地问题。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ol>
<li><strong>目标缺失拒绝</strong>：在目标物体缺失的指令下，OBEYED-VLA的抓取率接近于0%，而所有基线VLA模型的抓取率均超过75%，表明OBEYED-VLA能有效遵循语言指令并拒绝不合理请求。</li>
<li><strong>抗干扰物能力</strong>：在存在干扰物体的场景中，OBEYED-VLA取得了最高的成功率。例如，在“按物体标识抓取”任务中，OBEYED-VLA成功率为95.0%，而最好的基线π0.5为82.5%；在更难的“按空间关系抓取”任务中，OBEYED-VLA成功率为90.0%，显著高于π0.5的62.5%。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.22519v1/x6.png" alt="干扰物场景结果"></p>
<blockquote>
<p><strong>图6</strong>：在具有干扰物体的杂乱场景中的成功率。OBEYED-VLA在两种干扰设置（按标识和按空间关系）下均优于所有基线。</p>
</blockquote>
<ol start="3">
<li><strong>背景外观变化鲁棒性</strong>：当桌面背景材质和颜色发生改变时，OBEYED-VLA保持了90.0%的成功率，而基线模型（如π0.5）的性能从82.5%下降至约65.0%，显示了其对表面外观变化的强鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.22519v1/x7.png" alt="背景变化结果"></p>
<blockquote>
<p><strong>图7</strong>：背景外观分布变化下的成功率。OBEYED-VLA在不同背景上保持了稳定的高性能，而基线模型性能显著下降。</p>
</blockquote>
<ol start="4">
<li><strong>泛化到未见物体</strong>：在操作训练中未出现过的全新物体时，OBEYED-VLA取得了87.5%的成功率，远超所有基线模型（最好的基线为37.5%），证明了其卓越的泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.22519v1/x8.png" alt="未见物体泛化结果"></p>
<blockquote>
<p><strong>图8</strong>：泛化到未见目标物体。OBEYED-VLA在操作训练期间未见过的物体时，成功率显著高于所有基线。</p>
</blockquote>
<ol start="5">
<li><strong>真实世界闭环操控</strong>：在包含干扰物、目标缺失和未见物体等多种挑战的复杂闭环操作任务中，OBEYED-VLA实现了100%的任务完成率，而基线模型（π0.5和OpenVLA）的完成率分别为20%和40%。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.22519v1/x9.png" alt="闭环任务结果"></p>
<blockquote>
<p><strong>图9</strong>：真实世界闭环操作任务的完成率，该任务结合了干扰物、目标缺失和未见物体的挑战。</p>
</blockquote>
<ol start="6">
<li><strong>消融实验</strong>：消融研究（<strong>图11</strong>）证实了物体中心接地和几何接地两个组件都是关键贡献者。移除几何接地（仅用净化RGB）或移除物体中心接地（仅用原始深度）都会导致性能显著下降，尤其是在背景变化和未见物体泛化场景中。两者结合带来了最大的性能增益。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.22519v1/x11.png" alt="消融实验"></p>
<blockquote>
<p><strong>图11</strong>：OBEYED-VLA关键组件的消融研究。结果表明，物体中心接地和几何接地对于在背景变化和未见物体泛化等挑战中实现鲁棒性都至关重要。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了OBEYED-VLA框架，通过显式解耦感知接地与动作推理，将物体中心和几何感知的观测增强现有VLA模型。2) 在真实世界实验中证明，该框架能大幅提升VLA在干扰物、目标缺失、背景变化和未见物体操作等多种挑战下的鲁棒性和泛化性，且仅需使用干净的单物体演示数据进行微调。3) 消融实验验证了语义和几何双重接地机制的必要性。</p>
<p>论文自身提到的局限性包括：感知模块依赖于分割模型和VLM，可能引入额外的计算延迟；当前框架主要针对静态场景（除机械臂外），物体中心接地在任务开始时执行一次，对于高度动态的环境可能需要更频繁的调用。</p>
<p>这项工作对后续研究的启示是：将感知作为显式、模块化的组件是强化VLA模型可靠性和泛化能力的有效途径。这种解耦设计允许独立改进感知模块（如使用更快的模型或更细粒度的接地），并能够便捷地适配到不同的VLA主干、环境和机器人平台上，为构建更稳健的通用机器人系统提供了新思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在杂乱现实场景中感知与控制纠缠、语言条件接地不准的问题，提出OBEYED-VLA框架。其核心技术是通过一个感知模块，先利用VLM进行任务相关的对象中心语义接地，再通过几何接地强调对象3D结构，从而将原始RGB观测转换为明确接地的表示，再输入VLA策略。在真实UR10e桌面测试中，该方法在干扰物、目标缺失、背景变化及操作未见对象等挑战性场景下，鲁棒性显著优于基线模型，消融研究证实语义与几何接地均至关重要。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.22519" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>