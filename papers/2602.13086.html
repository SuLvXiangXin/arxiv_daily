<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.13086" target="_blank" rel="noreferrer">2602.13086</a></span>
        <span>作者: Ziwei Wang Team</span>
        <span>日期: 2026-02-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人通用操作要求系统能在非结构化环境中，将高级语义意图与低级物理交互无缝连接。当前方法在零样本泛化上存在局限：端到端的视觉-语言-动作模型通常缺乏执行长时程任务所需的精确性，而传统的分层规划器在面对开放世界变化时则表现出语义上的僵化。这两种范式之间存在根本性脱节：VLA模型受限于其骨干网络的训练分布，难以泛化到未见过的运动学或动力学；分层方法则将规划与执行视为孤立阶段，导致高层逻辑缺乏动态的物理基础和对执行失败的闭环反馈能力。</p>
<p>本文针对上述核心痛点，提出了一种统一语义推理与物理基础的新视角。其核心思路是模仿人类记忆系统的认知交互，构建一个<strong>双层智能操作图</strong>，通过耦合高层任务编排与低层动态状态表示，形成一个持续的智能循环，从而实现对开放世界任务的零样本鲁棒执行。</p>
<h2 id="方法详解">方法详解</h2>
<p>UniManip框架的核心是一个双层智能操作图，它作为一个智能基础设施，将高层任务规划与低层运动执行统一起来。其整体工作流程包含三个逻辑相连的阶段：1）多模态场景实例化，将输入转化为以物体为中心的几何属性和功能；2）智能基元执行，生成技能基元并通过安全感知的规划器驱动动作参数化；3）闭环反思与恢复，利用AOG的结构化记忆自主诊断执行异常并动态重组逻辑图以进行恢复。</p>
<p><img src="https://arxiv.org/html/2602.13086v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：UniManip框架总览。系统通过智能操作图将高层任务规划与低层运动执行集成。VLM解析人类指令生成操作图以指导机器人动作，反思恢复机制则允许系统诊断并适应执行失败。</p>
</blockquote>
<p>AOG包含两个交互层：</p>
<ol>
<li><strong>高层智能逻辑层</strong>：作为系统的“程序与语义记忆”，负责高层推理、编排任务流、验证结果并管理从局部失败中的恢复。它体现为<strong>智能逻辑图</strong>，遵循感知-规划-执行-反思的循环。</li>
<li><strong>低层语义操作状态层</strong>：作为动态的“情景记忆”，维护一个自适应的、基于场景的物体状态和空间功能表示。它体现为<strong>语义操作状态图</strong>。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.13086v1/x3.png" alt="AOG结构与工作流"></p>
<blockquote>
<p><strong>图3</strong>：提出的双层智能操作图的结构与工作流。上层展示了具有五个节点和条件有向边的ALG。下层展示了由SOSG描述的环境结构化语义理解。</p>
</blockquote>
<p>SOSG是框架的结构化世界模型，图节点𝒱代表工作空间中的物理实体。每个节点𝑣𝑖由一个增广状态向量𝐱𝑖参数化，该向量封装了几何、运动学、语义和物理属性：𝐱𝑖 = [𝒮𝑖, 𝒦𝑖, 𝒜𝑖, 𝒫𝑖]⊤。其中𝒮𝑖包含语义配置（如物体类别、操作角色），𝒦𝑖定义运动学与空间约束（如是否为铰接物体、抓取宽度），𝒜𝑖捕获内在物理属性（如颜色、形状），𝒫𝑖定义了操作部件分解（如抽屉把手）。这种表示弥合了高层符号规划与低层位置控制之间的鸿沟。</p>
<p>基于SOSG，ALG将自由形式的语言指令ℒℎ和可用工具库𝒯𝑐映射为一系列图条件的子任务。每个子任务编码为一条有向边𝑒𝑖𝑗（表示<strong>移动到</strong>操作）或一个自环𝑒𝑖𝑖（表示<strong>操作</strong>动作）。操作动作由相对于指定坐标系（基座坐标系ℱ𝑏或末端执行器坐标系ℱ𝑒）的6自由度运动增量参数化。</p>
<p><img src="https://arxiv.org/html/2602.13086v1/x4.png" alt="空间操作演示"></p>
<blockquote>
<p><strong>图4</strong>：机器人空间操作演示，以打开抽屉为例。任务被分解为多个工具调用，每个工具都有其特定的机器人运动空间操作格式。</p>
</blockquote>
<p>动作通过<strong>智能操作基元</strong>库𝒯𝑐抽象执行。对于固定基座机器人，𝒯𝑐主要包括<strong>目标导向运动</strong>（移动末端执行器到SOSG中某个节点关联的目标位姿）和<strong>操作导向运动</strong>（执行精确的相对运动，如推、拉）。对于移动操作机器人，则额外增加一个<strong>导航基元</strong>𝐍，用于将机器人基座驱动到可操作配置。</p>
<p>在运动合成阶段，系统采用安全感知的运动规划器𝑓𝑝𝑙𝑎𝑛，根据起始位姿𝐩𝑠、目标位姿𝐩𝑡以及从RGB-D输入导出的占据表示ℳ𝑜𝑐𝑐，生成一系列无碰撞的路径点𝒲。规划器采用基于单视角保守重建的方法，确保轨迹的安全性和可行性。</p>
<p>与现有方法相比，UniManip的创新点在于：1）提出了一个统一语义与物理的双层图结构，实现了规划与执行的动态耦合与同步；2）通过基于SOSG的动态场景解析和基于ALG的反思循环，实现了对执行失败的自主诊断与恢复；3）设计了与机器人形态无关的任务级表示和可扩展的基元库，支持零样本跨平台（如从固定基座到移动操作）迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个基准上进行评估：1）<strong>RLBench</strong>模拟基准，包含10个具有长时程依赖关系的复杂操作任务；2）<strong>ManiSkill2</strong>模拟基准，专注于铰接物体操作；3）真实世界<strong>UR5e</strong>机器人平台，用于验证零样本泛化能力。</p>
<p>对比的基线方法包括两大类：1）<strong>VLA模型</strong>：包括RT-1、OpenVLA、π0、Nora 1.5和GR00T。2）<strong>分层方法</strong>：包括VoxPoser、CoPa、ReKep和OmniManip。</p>
<p><img src="https://arxiv.org/html/2602.13086v1/x6.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图6</strong>：在RLBench基准上的成功率对比。UniManip在10个任务上的平均成功率为**81.0%**，显著优于所有基线方法。</p>
</blockquote>
<p>关键定量结果显示，在RLBench的10个任务上，UniManip取得了**81.0%**的平均成功率。与表现最佳的VLA基线（Nora 1.5， **58.5%**）和分层基线（ReKep， **56.0%<strong>）相比，分别实现了</strong>22.5%<strong>和</strong>25.0%<strong>的绝对提升。在ManiSkill2的铰接物体操作任务中，UniManip也达到了</strong>80.0%**的成功率，优于VoxPoser（70.0%）和ReKep（65.0%）。</p>
<p><img src="https://arxiv.org/html/2602.13086v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验结果。移除了智能操作图、安全感知规划器或反思恢复机制中的任一组件，都会导致性能显著下降，验证了各核心设计的必要性。</p>
</blockquote>
<p>消融实验验证了各核心组件的贡献：1）<strong>移去智能操作图</strong>（仅使用VLM进行开环规划），成功率下降21.0%；2）<strong>移去安全感知规划器</strong>（使用简单线性插值），成功率下降19.0%；3）<strong>移去反思恢复机制</strong>，成功率下降18.5%。这表明AOG的结构、安全的运动生成以及闭环反思对于实现鲁棒的零样本泛化都至关重要。</p>
<p><img src="https://arxiv.org/html/2602.13086v1/x9.png" alt="真实世界与移动操作结果"></p>
<blockquote>
<p><strong>图9</strong>：真实世界零样本操作结果。UniManip成功完成了包括与未见物体交互在内的多种长时程任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.13086v1/figures/embodiments_diagram_v1.1.png" alt="跨平台零样本迁移"></p>
<blockquote>
<p><strong>图8</strong>：跨机器人平台零样本迁移。系统无需微调或重新配置，即可将从固定基座设置中学到的策略直接迁移到移动操作平台上执行。</p>
</blockquote>
<p>定性结果（图9、10、11）展示了UniManip在真实世界中对未见物体和复杂长时程任务的执行能力，例如“把草莓放进蓝色碗里然后推到桌子边缘”。特别值得注意的是，系统支持<strong>零样本跨平台迁移</strong>（图8），能够将在固定基座设置中生成的策略直接部署到移动操作机器人上，而无需任何微调，在导航与操作结合的任务中取得了85%的成功率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了<strong>UniManip框架</strong>，通过双层智能操作图实现了跨任务、物体和机器人形态的鲁棒零样本泛化，无需任务特定微调。2）引入了<strong>双层的AOG</strong>作为智能基础设施，通过隐式集成语义记忆与情景记忆，实现了复杂的推理和动态任务分解，同时保持与物理环境的同步。3）建立了一个<strong>稳健的任务到运动桥梁</strong>，结合单视角保守重建与安全感知规划器，能够生成优化、无碰撞且高效的末端执行器轨迹。</p>
<p>论文也指出了自身的局限性：1）系统性能依赖于基础VLM的推理和规划能力，可能受其固有幻觉或错误的影响。2）单视角感知在严重遮挡或高度混乱的场景中可能受限。3）当前框架主要处理刚体和简单铰接物体，对高度可变形物体或复杂接触动力学的处理能力有待探索。</p>
<p>这项工作对后续研究的启示在于：将<strong>智能体架构</strong>与<strong>结构化世界模型</strong>深度结合，是迈向通用机器人操作的一条有效路径。它表明，通过设计具有记忆、反思和动态重规划能力的闭环系统，可以显著提升在开放世界中的适应性与鲁棒性。未来的工作可以探索更强大的场景表示、更精细的物理推理模型，以及如何将学习与符号规划更紧密地融合，以应对更广泛的现实世界挑战。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出UniManip框架，旨在解决机器人在非结构化环境中对未知物体和任务实现零样本泛化操作的难题。其核心技术是双层智能操作图（AOG），它通过高层智能体层进行任务编排，低层场景层进行动态状态表征，形成一个持续对齐语义规划与几何约束的动态智能循环。实验表明，该系统在未见过的物体和任务上取得了显著的零样本性能，相比先进的VLA和分层基线方法，成功率分别高出22.5%和25.0%，并能实现从固定基座到移动操作的零样本迁移。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.13086" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>