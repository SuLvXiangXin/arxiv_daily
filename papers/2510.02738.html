<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.02738" target="_blank" rel="noreferrer">2510.02738</a></span>
        <span>作者: Nadia Figueroa Team</span>
        <span>日期: 2025-10-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>接触丰富的机器人操作任务需要在精确运动控制和柔顺力调节之间取得平衡。视觉运动策略因其易于指定和多模态能力，已成为操作任务的主流机器人学习范式。然而，最先进的方法通常忽略柔顺性，只关注位置精度。近期进展表明，在物体翻转或擦拭等任务中，结合柔顺性或力反馈可以显著提高性能，而固定刚度控制器无法处理变化的接触条件。现有方法或从人类演示中学习可变刚度轮廓，或使用强化学习/轨迹优化来调整柔顺性。虽然这些方法取得了良好的性能，但它们通常需要大量的人力投入，例如需要数百次真实演示，或精心设计的奖励函数和大量的数据/训练。因此，当前的柔顺策略由于密集的物理数据收集或复杂的奖励工程而缺乏可扩展性。</p>
<p>本文针对数据稀缺和仿真到现实差距的痛点，提出了一种新视角：仅从单次人类演示出发，在仿真中生成力感知数据，并耦合一个柔顺策略，以提升从合成数据中学习的视觉运动策略的性能。核心思路是：通过力感知轨迹调制和拉普拉斯编辑，从单次演示中生成多样化的仿真数据，并训练一个基于点云和力输入的流匹配策略，该策略输出末端执行器位姿和阻抗参数，最后通过状态-速度场和被动阻抗控制器在真实机器人上实现安全、柔顺的执行。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架包含三个主要组件：i) 在仿真中生成力感知数据，ii) 使用流匹配进行策略学习，iii) 通过状态-速度场在真实硬件上安全执行。</p>
<p><img src="https://arxiv.org/html/2510.02738v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Flow with the Force Field 3D 柔顺视觉运动策略学习框架。从单次仿真演示开始，通过添加力感知虚拟目标和应用拉普拉斯编辑来增强数据，生成超越原始演示的点云和力轨迹。训练一个流匹配策略，该策略以点云和力作为输入，并预测动作（包括阻抗参数）。在执行时，策略被合成为一个状态-速度场，并通过被动阻抗控制器执行以实现柔顺行为。</p>
</blockquote>
<p><strong>整体流程</strong>：首先，在IsaacGym仿真环境中通过力反馈遥操作收集单次人类演示 (D_h)。然后，通过域随机化改变物体和末端执行器的初始位姿，并应用轨迹变形技术（拉普拉斯编辑和力调制）生成多样化的仿真数据集 (D_{sim})。该数据集用于训练一个流匹配策略 (\pi: \mathcal{O} \rightarrow \mathcal{A})，其观察空间 (\mathcal{O} = (o_{pc}, o_{ee}, o_f)) 包括点云、末端执行器位姿和力测量，动作空间 (\mathcal{A} = (a_{ee}, d)) 包括目标末端执行器位姿和柔顺增益 (d)。在硬件上执行时，策略输出的位姿动作被转换为状态-速度场 ( \dot{x}<em>d = f(o</em>{ee}, d) )，并输入到被动阻抗控制器中驱动机器人。</p>
<p><strong>核心模块1：力感知数据生成</strong>。首先将演示轨迹 (D_h) 分割为自由空间段 (D_f)（接触力为零）和接触段 (D_c)（接触力大于零）。</p>
<ul>
<li><strong>自由空间轨迹变形</strong>：使用拉普拉斯编辑。仅锚定开始和结束的少数样本点，保持它们相对于新锚点（随机化的初始末端执行器位置和物体位置）的偏移量，然后通过求解拉普拉斯系统来平滑地变形整个轨迹。姿态部分使用球面线性插值在初始和最终姿态间插值。</li>
<li><strong>接触段轨迹变形</strong>：在物体坐标系中进行变形。对于接触段中的每个数据点，新的末端执行器位姿和力通过公式（4）-（6）计算，以保持相对于物体坐标系的相对位姿和力的方向。随后，为变形后的接触轨迹中的每个点构建一个力感知虚拟目标：(x_{ee}^v = x_{ee_w}^{rc} + k_f F^r)，其中 (k_f) 是手动调制的参数。这个虚拟目标位于物体“内部”，驱使末端执行器向物体表面施加力以维持连续接触。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.02738v2/img/concept/original_ref_ee_traj.png" alt="轨迹变形示例"></p>
<blockquote>
<p>**图3(a)**：原始参考演示轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.02738v2/img/concept/force_modulated_ee_traj.png" alt="力调制轨迹"></p>
<blockquote>
<p>**图3(b)**：添加力感知虚拟目标后的轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.02738v2/img/concept/obj_modulated_ee_traj.png" alt="拉普拉斯编辑轨迹"></p>
<blockquote>
<p>**图3(c)**：仅通过物体和末端执行器初始位姿进行拉普拉斯编辑变形的轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.02738v2/img/concept/laplacian_force_modulated_ee_traj.png" alt="力-拉普拉斯组合轨迹"></p>
<blockquote>
<p>**图3(d)**：结合了物体初始位姿变形和力调制的完整轨迹，用于数据生成。</p>
</blockquote>
<p><strong>核心模块2：柔顺3D视觉运动流匹配策略</strong>。采用条件流匹配框架作为核心方法。观察条件由三部分特征向量拼接而成：使用DP3编码器处理点云 (o_{pc})，使用一个3层MLP处理力 (o_f)，使用另一个3层MLP处理末端执行器位姿 (o_{ee})。流匹配主干网络是一个条件U-Net。策略输出动作 (A = (X_{ref}^{t...t+H}, X_{vt}^{t...t+H}, d^{t...t+H}))，包括参考位姿轨迹、虚拟目标轨迹和阻抗参数轨迹（ horizon (H=16)）。训练时，通过回归损失（公式9）学习向量场 ( \boldsymbol{v_{\theta}} )，使得从噪声 (A_0) 积分ODE（公式10）能得到目标动作 (z_1)（即仿真数据中的动作）。</p>
<p><strong>核心模块3：基于状态向量场的柔顺策略执行</strong>。不直接跟踪策略输出的位姿轨迹，而是将其转换为状态-速度场，并通过被动阻抗控制器（公式11）执行。具体地，控制器期望的力为 (F_d = D(x) f(x))，其中 (f(x)) 是期望速度。期望速度方向 ( \hat{u} ) 由参考运动方向 ( \hat{t} = X_{ref}^{t+1} - X_{ref}^{t} ) 和柔顺方向 ( \hat{n} = X_{vt}^{t} - x_{curr} ) 混合得到（公式13）：( \hat{u} = \frac{d \hat{n} + \hat{t}}{||d \hat{n} + \hat{t}||} )。其中混合增益 (d) 就是策略输出的阻抗参数。在训练时，(d) 根据仿真中测量的力大小 (|\mathcal{F}|) 进行调度（公式14），力越大，(d) 越小（更柔顺）。</p>
<p><strong>创新点</strong>：1) <strong>轻量级数据生成</strong>：结合拉普拉斯编辑和力调制，仅从单次演示生成多样化、力感知的仿真数据，无需训练或强化学习。2) <strong>自适应柔顺流匹配策略</strong>：首次在流匹配策略中同时使用点云和力作为输入，并输出位姿动作及阻抗参数。3) <strong>状态-速度场执行</strong>：将策略输出合成为状态依赖的向量场，与被动阻抗控制器耦合，实现更安全、能量注入更少的柔顺执行。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实Franka机器人上验证，任务包括非抓取方块翻转和双手物体移动。使用IsaacGym进行仿真和数据生成。对比的基线方法包括：1) <strong>Fixed Stiffness</strong>：具有固定高刚度的阻抗控制器。2) **Flow Matching (w/o compliance)**：仅输出位姿、无柔顺增益的流匹配策略。3) **RL (SAC)**：在仿真中训练软演员-评论家算法。4) <strong>Diffusion Policy</strong>：基于扩散模型的策略。5) <strong>VINN</strong>：基于最近邻的模仿学习。评估指标为任务成功率，并对接触力曲线进行分析。</p>
<p><img src="https://arxiv.org/html/2510.02738v2/x3.png" alt="方块翻转成功率"></p>
<blockquote>
<p><strong>图7</strong>：方块翻转任务的成功率对比。本文方法（Ours）取得了最高的成功率（93.3%），显著优于无柔顺的流匹配策略（73.3%）和固定刚度控制器（60%）。扩散策略和RL方法成功率较低。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.02738v2/x4.png" alt="双手移动成功率"></p>
<blockquote>
<p><strong>图8</strong>：双手物体移动任务的成功率对比。本文方法同样取得了最高成功率（90%），而无柔顺的流匹配策略成功率仅为56.7%，固定刚度控制器为66.7%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.02738v2/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图9</strong>：消融研究结果。展示了不同组件对性能的影响：完整方法（Full）成功率最高；去除力虚拟目标（w/o (X_{vt})）导致性能大幅下降，说明维持接触意图至关重要；使用位置跟踪而非状态-速度场（w/o V.F.）也会降低性能；仅使用物体位姿而非点云（w/ pose）仍能工作但性能稍差，体现了点云输入的空间泛化优势。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.02738v2/x6.png" alt="泛化测试"></p>
<blockquote>
<p><strong>图10</strong>：在未见过的物体形状（圆柱体、三棱柱）和尺寸上进行零样本泛化测试。虽然数据仅使用简单方块生成，但训练出的策略能够成功完成翻转任务，展示了良好的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.02738v2/img/experiment/force_profile_m.png" alt="力曲线分析"></p>
<blockquote>
<p><strong>图11</strong>：方块翻转任务中典型试验的接触力曲线对比。本文方法（Ours）的力曲线最平滑，峰值力最低（约12N）。固定刚度控制器（Fixed Stiffness）和RL方法产生了极高的峰值力（超过40N）。无柔顺的流匹配策略（FM w/o comp）也产生了较高的峰值力。这表明本文方法能更安全、更柔顺地调节接触力。</p>
</blockquote>
<p><strong>关键实验结果</strong>：在方块翻转任务中，本文方法取得了93.3%的成功率，显著优于最佳基线（无柔顺流匹配策略的73.3%）。在双手移动任务中，成功率达到90%，远高于无柔顺版本的56.7%。消融实验证实了力虚拟目标和状态-速度场执行机制的关键作用。此外，方法展示了出色的零样本泛化能力，能处理训练中未见的物体形状。力曲线分析表明，本文方法产生的接触力更平滑，峰值力显著降低。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一种轻量级、有效的力感知仿真数据生成策略，仅需单次人类演示，结合虚拟目标和拉普拉斯编辑，为视觉模仿学习创造多样化数据。2) 设计了一种自适应柔顺流匹配策略，首次将点云和力作为输入，并输出位姿动作和阻抗参数。3) 提出了一种将策略输出合成为状态-速度场的方案，使被动阻抗控制器能够在真实机器人上以更好的性能和更低的能量注入执行柔顺策略，并实现了到真实Franka机器人的零样本迁移。</p>
<p><strong>局限性</strong>：论文提到，数据生成仅针对两个任务使用了简单的方块几何形状。然而，训练出的策略利用该框架产生了超越单一形状的泛化能力。这表明当前数据生成范围有限，但框架本身具备泛化潜力。</p>
<p><strong>启示</strong>：这项工作为数据高效的、接触丰富的柔顺策略学习提供了一个有前景的范式。它表明，通过精心设计的轻量级仿真数据增强和与物理一致的执行框架，可以显著减少对大量真实演示或复杂强化学习的依赖。后续研究可以探索更自动化的虚拟目标生成方式，将框架扩展到更复杂的多指灵巧操作，或研究如何将这种方法与在线适应相结合以处理更大的仿真到现实差距。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对接触丰富的机器人操作任务中，视觉运动策略忽视力控与顺应性、导致接触力过大或行为脆弱的问题，提出一种从模拟数据学习3D顺应流匹配策略的框架。该方法仅需单次人类示教，即可在仿真中生成力信息与示教引导的数据，并耦合顺应策略提升视觉运动策略性能。在真实机器人非抓取块翻转与双手移物任务中验证，学习到的策略能可靠维持接触并适应新条件。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.02738" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>