<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.05020" target="_blank" rel="noreferrer">2506.05020</a></span>
        <span>作者: Liu, Haokun, Ma, Zhaoqi, Li, Yunong, Sugihara, Junichiro, Chen, Yicheng, Li, Jinjie, Zhao, Moju</span>
        <span>日期: 2025/06/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>异构多机器人系统（HMRS）在需要协调混合协作的复杂任务中展现出巨大潜力。然而，现有方法依赖于静态或任务特定的模型，通常缺乏跨不同任务和动态环境的通用性。这凸显了对能够在异构智能体之间桥接高层推理与低层执行的通用智能的需求。</p>
<p>当前，利用大型语言模型（LLM）进行任务规划和视觉语言模型（VLM）进行感知的机器人系统取得进展。但将推理和感知直接集成到单层系统中常导致紧密耦合，使得系统对感知错误脆弱，且难以维持长视野任务。</p>
<p>本文针对异构空中-地面机器人系统在动态环境中进行语义导航和操作的通用协调问题，提出了一种新的分层多智能体语言模型（MA-LLM）框架。其核心思路是：通过分层结构模块化地分离推理、感知与执行，利用LLM进行高层任务分解与全局语义地图构建，利用经过GridMask增强的VLM提供精确的语义感知与定位，并通过预编程的运动函数实现空中与地面机器人的引导-跟随式协作。</p>
<h2 id="方法详解">方法详解</h2>
<p>所提出的分层MA-LLM框架将任务执行结构化为三个功能层：负责分解用户命令为任务流的推理层、从空中图像提取语义信息的感知层，以及通过空中-地面协作执行运动的执行层。整体框架遵循引导-跟随机制，空中机器人作为引导者规划全局路径并提供连续的鸟瞰图像流，地面机器人作为跟随者利用图像中心（对应空中机器人投影的地面位置）作为动态导航锚点。</p>
<p><img src="https://arxiv.org/html/2506.05020v3/x1.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：集成到空中-地面机器人系统中的分层语言模型框架概览。在子任务“移动到（XX, XX）”中，空中机器人遵循优化的全局路径并持续捕获鸟瞰图像。这些图像被处理成语义信息，以指导地面机器人的实时局部导航，隐式地允许地面机器人跟随空中机器人的位置。</p>
</blockquote>
<p><strong>1. LLM-based 推理层设计</strong><br>该层通过精心设计的提示词，将自然语言命令转化为可执行的模块化功能。其核心职责包括：</p>
<ul>
<li><strong>任务分解与分配</strong>：根据空中和地面机器人的能力（如无人机建图、机器狗附着/分离物体、协作移动与搬运），将复杂任务分解为机器人特定的子任务。</li>
<li><strong>运动函数映射</strong>：将分解后的子任务映射到预定义的运动函数API（如<code>quad_construct_map()</code>, <code>go1_following_start()</code>），确保生成可执行的动作计划。</li>
<li><strong>全局语义地图集成</strong>：接收来自多个视点的局部语义地图（由感知层生成），通过思维链提示进行跨地图验证、错误识别和冲突解决，推理出空间一致的全局语义地图。该设计具有模块化和可扩展性，只需更新提示词中的能力与动作定义即可适应新的机器人或任务。</li>
</ul>
<p><strong>2. VLM-based 感知层设计</strong><br>该层旨在从空中机器人捕获的鸟瞰图像中提取精确的语义和空间信息。其创新点在于引入了 <strong>GridMask</strong> 微调方法以解决VLM空间定位不准的难题。</p>
<ul>
<li><strong>GridMask 方法</strong>：在输入图像上叠加一个密集的坐标网格。每个网格单元大小为 s×s 像素（通常s=80）。网格在图像中心的笛卡尔坐标系中定义，并通过公式动态计算网格划分数量和每个网格单元对应的真实世界尺寸κ，使其适应不同的相机分辨率、视场和飞行高度。</li>
<li><strong>微调与结构化输出</strong>：使用包含GridMask的图像和对应的物体坐标及类别（以结构化JSON格式）的数据集对VLM进行微调（如图3所示）。这使得VLM能够以零样本方式鲁棒地识别物体的空间位置，并将其分类为 <code>main</code>（主智能体/携带物）、<code>target</code>（目标）、<code>landmark</code>（地标，含方向）或 <code>obstacle</code>（障碍物）。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.05020v3/x3.png" alt="GridMask微调数据集示例"></p>
<blockquote>
<p><strong>图3</strong>：微调数据集的图示，展示了系统提示、带有基于GridMask的鸟瞰图像输入的用户指令，以及结构化JSON格式的理想输出。</p>
</blockquote>
<p><strong>3. 执行层设计 - 运动函数</strong><br>执行层通过一系列预编程的运动函数桥接推理和感知层，具体包括：</p>
<ul>
<li>**全局语义地图构建 (<code>quad_construct_map()</code>)**：空中机器人沿预设航点飞行采集图像，VLM生成局部语义地图，LLM集成这些局部地图形成全局语义地图，并在任务执行中间隔更新。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.05020v3/x4.png" alt="全局语义地图构建流程"></p>
<blockquote>
<p><strong>图4</strong>：<code>quad_construct_map()</code> 函数的工作流程。从空中图像衍生的局部语义地图由LLM集成为语义地图，随后在任务执行期间以15秒间隔更新。</p>
</blockquote>
<ul>
<li>**全局路径规划 (<code>quad_planning_start()</code>)**：作为引导者，空中机器人基于全局语义地图，以B样条曲线形式规划一条优化路径。优化目标函数平衡路径长度、平滑度和避障（利用<code>obstacle</code>坐标定义排斥区域）。优化后的路径通过比例因子κ转换为真实世界航点，空中机器人投影的地面位置成为地面机器人动态的局部导航锚点（零坐标点）。</li>
<li>**局部语义导航与操作 (<code>go1_following_start()</code>)**：作为跟随者，地面机器人基于VLM从当前鸟瞰图像中实时生成的、以空中机器人投影点为中心的局部语义地图进行导航。它采用类似动态窗口法的思想，在360度方向上生成候选速度方向，并通过一个局部成本函数（平衡目标偏差、中心对齐、避障和窗口限制）选择最优方向，迭代导航直至到达目标。</li>
<li>**附着与分离 (<code>go1_attach/detach_start()</code>)**：控制地面机器人身上的伺服驱动磁力装置，实现与物体的交互。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟和真实世界环境中进行，评估了三个关键方面：语义感知改进、与基线方法的对比、以及长视野任务中的空中-地面协作。</p>
<p><strong>1. 评估基准与基线</strong></p>
<ul>
<li><strong>感知评估</strong>：在自定义的2D识别数据集上，对比了提出的GridMask微调方法与传统的坐标监督基线方法。</li>
<li><strong>任务性能评估</strong>：在模拟的“目标搜索”子任务中，与代表性的深度强化学习（DRL）多机器人基线（如MAPPO、QMIX）进行了对比。</li>
<li><strong>系统验证</strong>：在真实世界中进行了长视野物体排列任务（如“拼出‘OK’单词但不移动‘K’”）的实验，以验证零样本泛化能力和鲁棒协作。</li>
</ul>
<p><strong>2. 关键实验结果</strong></p>
<ul>
<li><strong>GridMask显著提升定位精度</strong>：在相同规模的训练集上，GridMask微调方法将VLM的物体定位误差降低了78%，大幅优于坐标监督基线。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.05020v3/x5.png" alt="定位误差对比"></p>
<blockquote>
<p><strong>图5</strong>：GridMask微调方法与坐标监督基线在物体定位误差上的对比。GridMask方法（橙色）在所有测试场景中均显著降低了误差。</p>
</blockquote>
<ul>
<li><strong>超越DRL基线</strong>：在模拟目标搜索任务中，本文框架在任务成功率上显著优于DRL基线方法（MAPPO, QMIX），尤其是在复杂和动态场景中。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.05020v3/x6.png" alt="与DRL基线任务成功率对比"></p>
<blockquote>
<p><strong>图6</strong>：在模拟目标搜索任务中，本文框架与DRL基线（MAPPO, QMIX）的任务成功率对比。本文方法在不同难度场景下均表现出更高的成功率。</p>
</blockquote>
<ul>
<li><strong>零样本泛化与鲁棒协作</strong>：真实世界实验表明，该系统能够零样本泛化到未见过的物体、语义配置和任务组合，成功完成了复杂的长视野排列任务。消融实验验证了各核心组件（LLM推理、VLM感知、分层协调）的必要性。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.05020v3/x7.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图7</strong>：消融实验结果，展示了移除LLM推理、VLM感知或分层协调机制对任务完成率和效率的影响，证明了各组件的重要性。</p>
</blockquote>
<ul>
<li><strong>定性结果</strong>：图8-16展示了系统在真实世界任务执行中的多个关键步骤快照，包括地图构建、路径规划、导航、附着、搬运等，直观体现了其协作能力。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.05020v3/x8.png" alt="真实世界任务执行示例"></p>
<blockquote>
<p><strong>图8</strong>：真实世界长视野任务执行的定性示例序列之一，展示了从初始建图到最终完成“OK”排列的协作过程。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个用于异构空中-地面机器人系统的分层多模态MA-LLM框架，将基于LLM的任务推理与基于VLM的语义感知紧密耦合，通过引导-跟随机制桥接高层推理与低层执行。</li>
<li>引入了基于GridMask的VLM微调方法，在不依赖深度传感的情况下，大幅提升了从鸟瞰图像中进行语义标注和物体定位的空间精度（误差降低78%），为可靠的细粒度操作提供了关键支持。</li>
<li>通过广泛的模拟和真实世界实验验证了该框架的零样本泛化能力、鲁棒的语义导航和可靠的操纵性能，并展示了其优于DRL基线的任务性能。</li>
</ol>
<p><strong>局限性</strong>：<br>论文指出，该系统仍然依赖于感知层的准确性，虽然GridMask增强了VLM的定位能力，但严重的感知错误（如物体误识别）仍可能影响高层推理和规划。此外，系统的实时性能受限于VLM的推理速度和通信延迟。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>模块化与提示工程</strong>：本文展示了通过模块化分层设计和精心构造的提示词，可以有效地将通用大模型能力注入机器人系统，为实现可扩展和自适应的跨平台协作提供了范例。</li>
<li><strong>感知与推理的协同增强</strong>：GridMask方法表明，通过为VLM提供结构化的空间视觉线索，可以显著提升其几何理解能力，这为改进机器人感知中的视觉-语言 grounding 指明了方向。</li>
<li><strong>从模拟到现实的系统集成</strong>：论文中涉及的诸多工程补充（如自适应轨迹执行、旋转稳定性决策、携带状态监控回滚）强调了在复杂现实环境中实现鲁棒运行不仅需要算法创新，也需要细致的系统级工程考量。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对异构多机器人系统在动态环境中执行复杂任务时，传统方法通用性差、协调困难的核心问题，提出了一种分层多模态框架。该框架整合了提示式大型语言模型（LLM）与微调视觉语言模型（VLM）：LLM进行高层任务分解与全局语义地图构建；VLM提供语义感知与物体定位，其空间精度通过提出的GridMask技术得到显著提升，以支持精细操作。实验表明，该系统在长时程物体排列任务中实现了零样本适应、鲁棒的语义导航与可靠操作。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.05020" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>