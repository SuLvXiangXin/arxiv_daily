<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Guiding Energy-Efficient Locomotion through Impact Mitigation Rewards - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Guiding Energy-Efficient Locomotion through Impact Mitigation Rewards</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.09543" target="_blank" rel="noreferrer">2510.09543</a></span>
        <span>作者: Alireza Ramezani Team</span>
        <span>日期: 2025-10-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿生物级节能运动是学术界长期的追求。近年来，基于对抗运动先验（AMP）和强化学习（RL）的方法在复制动物自然运动方面取得了进展。然而，此类模仿学习方法主要捕捉显式的运动学模式（即步态），而忽略了隐式的被动动力学（如能量高效的冲击吸收和顺应性）。传统的、手工设计的奖励函数虽然可以通过精心调整来近似生物动力学，但过程繁琐且需要大量专业知识。</p>
<p>本文针对现有方法（特别是AMP）过度关注运动学模仿而忽视底层物理动态的局限性，提出了一个新的视角：将冲击缓解因子（IMF）这一物理信息指标作为奖励信号引入强化学习框架。IMF量化了机器人基于其构型被动缓解冲击的能力。本文的核心思路是：通过将IMF与AMP或手工奖励相结合，使RL策略不仅能从动物参考运动中学习显式运动轨迹，还能学习生物系统的隐式被动动态原理，从而实现更节能、更具动态韧性的运动。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架是在标准的强化学习（RL）流程中，将新提出的冲击缓解因子（IMF）奖励项整合到原有的奖励结构中。输入为机器人的状态观测，输出为关节位置指令。策略通过最大化包含IMF的总体奖励来学习节能运动。核心模块是IMF的计算及其作为奖励项的集成。</p>
<p>IMF的计算基于操作空间惯性矩阵（OSIM）。对于一个具有浮动机基和n个关节的机器人，其构型为 q = [q_b; q_j]，动力学方程由惯性矩阵H、科里奥利力/重力项h、关节扭矩τ和末端执行器力f描述。末端执行器速度 v = Jq̇，其中J为雅可比矩阵。冲击产生的冲量 ρ = -Λv，其中Λ = (J H^{-1} J^T)^{-1} 为操作空间惯性矩阵，代表了末端执行器感受到的有效惯性。</p>
<p>为量化关节自由运动带来的冲击缓解效果，引入一个“锁定关节”的对比基准。在此情况下，只有基座贡献运动，雅可比简化为J_b，对应的锁定操作空间惯性为 Λ_L = (J_b H_bb^{-1} J_b^T)^{-1}，冲击冲量为 ρ_L = -Λ_L v。通过比较实际冲量ρ与锁定冲量ρ_L，定义冲击缓解矩阵 Ξ = I - Λ Λ_L^{-1}。IMF标量值ξ为该矩阵的行列式：ξ = det(Ξ)。由于Λ和Λ_L是正定矩阵，ξ的取值范围在0到1之间：ξ=1表示完全缓解（ρ≈0），ξ=0表示无缓解（ρ≈ρ_L）。</p>
<p><img src="https://arxiv.org/html/2510.09543v2/figs/cover-image-v2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。箭头从摆动腿出发，说明了由配置相关的操作空间惯性矩阵（Λ）控制的预冲击速度和产生的冲量。该矩阵捕捉了机器人关节配置如何影响冲击动力学，从而实现有效的缓解。</p>
</blockquote>
<p>IMF作为奖励信号被定义为 R_IMF = -log(1-ξ)。当ξ趋近于1（缓解效果好）时，奖励趋近于正无穷；当ξ趋近于0时，奖励趋近于0。最大化此奖励将激励策略采用能增强被动冲击吸收能力的构型。</p>
<p>本文定义了两种集成IMF的奖励结构：</p>
<ol>
<li><strong>AMP+IMF奖励</strong>：总体奖励 r_t = w_Task * r_t^Task + w_Style * r_t^Style + w_IMF * r_t^IMF。其中，风格奖励r_t^Style通过一个判别器D_ϕ产生，该判别器经过训练以区分策略生成的状态转移和来自重定向狗运动数据集的参考转移。任务奖励r_t^Task用于跟踪指定的线速度和偏航速率指令。</li>
<li><strong>手工奖励+IMF</strong>：总体奖励 r_t = w_IMF * r_t^IMF + Σ_i w_i * r_t^i。手工奖励部分包含线速度/角速度跟踪、基座姿态/高度惩罚、扭矩/加速度惩罚、足端空中时间奖励等多项组件（详见表I）。</li>
</ol>
<p>与现有方法相比，核心创新点在于首次将IMF——一个原本用于机械设计（如MIT猎豹）的物理指标——转化为一个可微的学习信号，使RL策略能够显式地优化被动动态特性，从而桥接了运动学模仿和动态效率之间的鸿沟。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在IsaacLab仿真环境中进行，使用的机器人平台是Northeastern University的Husky-v.2（如图2），该机器人是一款为腿式和空中移动设计的混合平台，但本研究仅关注其地面运动策略。</p>
<p><img src="https://arxiv.org/html/2510.09543v2/figs/husky-b-hw.png" alt="硬件平台"></p>
<blockquote>
<p><strong>图2</strong>：Northeastern University的Husky version-v.2机器人平台。该平台旨在通过肢体重新利用探索多模态动态腿式-空中运动。</p>
</blockquote>
<p>对比的基线方法包括：1) 使用AMP+任务奖励的策略（AMP w/o IMF）；2) 使用手工设计奖励的策略（Hand-crafted w/o IMF）。与之对比的是各自加入IMF奖励的版本（AMP w/ IMF 和 Hand-crafted w/ IMF）。</p>
<p>关键实验结果以运输成本（CoT，计算公式为 τ·q̇ / (mgv)）和速度跟踪均方根误差（RMSE）衡量，如表II所示。</p>
<ul>
<li><strong>节能效果</strong>：在平坦地面上，AMP+IMF策略在1 m/s速度下将CoT降低了18.4%（从1.63降至1.33）；手工奖励+IMF策略在2 m/s速度下将CoT降低了20.5%（从0.83降至0.66）。在粗糙地形上，节能效果更显著，手工奖励+IMF策略在1.5 m/s速度下将CoT降低了19.0%（从0.88降至0.72）。</li>
<li><strong>运动稳定性</strong>：加入IMF后，策略在粗糙地形上的姿态（俯仰、横滚）跟踪误差普遍降低。例如，AMP+IMF策略的俯仰RMSE降低了20.3%（从1.48降至1.18）。</li>
<li><strong>机械功率与关节扭矩</strong>：如图4所示，加入IMF的策略显著降低了平均机械功率消耗。在平坦地面1.5-2 m/s速度下，AMP+IMF策略的峰值机械功率降低了35%（从100W降至65W），手工奖励+IMF策略降低了28%（从70W降至50W）。同时，关节累积扭矩也减少了25-32%。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.09543v2/figs/snapshot_woIMF.png" alt="定性对比"></p>
<blockquote>
<p><strong>图3a</strong>：在粗糙地形上，使用未加入IMF奖励的手工奖励策略时Husky v.2的运动快照。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.09543v2/figs/snapshot_wIMF.png" alt="定性对比"></p>
<blockquote>
<p><strong>图3b</strong>：在粗糙地形上，使用加入IMF奖励的手工奖励策略时Husky v.2的运动快照。对于相同的对角小跑步态，加入IMF奖励训练的策略降低了平均机械功率消耗。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.09543v2/figs/mechanical_power_husky_b_flat.png" alt="机械功率对比"></p>
<blockquote>
<p><strong>图4</strong>：在平坦地面上，使用四种策略在1.5 m/s至2 m/s速度范围内的平均机械功率对比。加入IMF奖励的策略显著降低了机械功率消耗。</p>
</blockquote>
<p>消融实验直接体现在四种策略的对比中。结果表明，无论是在AMP还是手工奖励框架下，加入IMF奖励组件都能一致地提升能量效率（降低CoT和机械功率）并经常改善运动稳定性（降低姿态误差）。这证明了IMF作为奖励组件对于学习节能被动动态的有效性和通用性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了IMF作为物理信息奖励信号</strong>：首次将用于机械设计的冲击缓解因子（IMF）转化为一个可微的奖励项，使RL策略能够显式地优化机器人的被动顺应性和冲击韧性。</li>
<li><strong>构建了混合奖励框架</strong>：成功地将AMP的运动学保真度与物理信息驱动的IMF相结合，使策略能同时学习参考运动的显式风格和生物力学的隐式被动动态。</li>
<li><strong>提供了充分的实验验证</strong>：在仿真中证实，为AMP或手工奖励加入IMF奖励，能将运输成本（CoT）提升高达32%，并显著降低机械功率和关节负载。</li>
</ol>
<p>论文自身提到的局限性在于，目前的工作仅在仿真中进行，尚未在真实硬件Husky-v.2上进行实验验证。</p>
<p>本文对后续研究的启示在于：为强化学习提供物理信息或基于模型的奖励信号，是引导智能体学习更高效、更接近生物本质运动策略的有效途径。IMF的概念可以扩展到更广泛的场景，例如更激进的机动动作、其他机器人构型，或者与基于模型的控制方法进一步结合。这项工作为弥合“运动学模仿”与“动态最优”之间的差距提供了一个有前景的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决模仿学习在复现动物能量高效运动时，主要关注显性步态而忽略隐性被动动力学（如冲击吸收）的问题。提出通过引入冲击缓解因子（IMF）这一物理指标作为奖励项，并将其与对抗运动先验（AMP）结合，引导强化学习策略同时学习运动轨迹和被动动态。实验表明，该方法在运输成本衡量下，能效提升最高达32%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.09543" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>