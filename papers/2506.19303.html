<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.19303" target="_blank" rel="noreferrer">2506.19303</a></span>
        <span>作者: Nutan Chen Team</span>
        <span>日期: 2025-06-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人准确感知物体物理属性对于其在非结构化环境中进行可靠操作至关重要。目前主流方法通常依赖于单一的视觉或触觉数据。视觉方法虽擅长几何感知，但难以捕捉硬度、弹性、表面粗糙度等内在材料属性；而触觉传感虽能提供丰富的接触信息，却需要物理接触，这在处理易碎或未知物体时存在显著缺点。现有结合多模态学习的方法（如将触觉感知与语言模型集成）仍存在两个根本性局限：一是当前触觉传感器在捕捉复杂复合结构物体的全面材料特征方面数据不足；二是现有方法未能通过策略性提示和有效的多模态融合，充分利用语言模型的推理潜力。</p>
<p>本文针对上述痛点，提出了一种增强的多模态感知框架，旨在通过整合视觉观察与触觉表征，并利用大型视觉语言模型进行物理属性推理。其核心思路是：利用视觉先验补偿触觉传感的局限性，并通过精心设计的、分阶段的结构化提示策略，主动引导模型推理，从而在接触前预测关键物理属性，并实现与真实测量值高度相关的属性评分。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了一种整合文本、视觉和触觉数据的多模态模型。整体框架如图2所示：输入查询（包含文本指令、物体图像和触觉图像）被解析到特定的模态处理路径。文本通过语言分词器进行标记化和嵌入；视觉和触觉图像分别使用视觉编码器和触觉编码器处理，然后通过模态特定的多层感知机（MLP）层投影到共享的嵌入空间。使用特殊标记（<code>&lt;img_start&gt;</code>, <code>&lt;img_end&gt;</code>, <code>&lt;tact_start&gt;</code>, <code>&lt;tact_end&gt;</code>）明确界定不同模态嵌入的边界。这些嵌入与文本特征拼接后，输入到一个大型语言模型（Vicuna-7B）中，通过联合的多模态注意力生成详细的物体属性描述。</p>
<p><img src="https://arxiv.org/html/2506.19303v1/extracted/6541887/image1.png" alt="方法整体架构"></p>
<blockquote>
<p><strong>图2</strong>：多模态大模型架构。物体图像、触觉图像与文本经过嵌入和标记化后，得到的向量被拼接并输入到大型语言模型中。</p>
</blockquote>
<p><strong>核心模块一：视觉处理</strong>。采用CLIP的视觉编码器（ViT-L/14）处理视觉信息。为了与LLM的嵌入空间对齐，使用了LLaVA中预训练的线性变换层，将CLIP的倒数第二层输出投影到语言模型的词嵌入空间。如图3所示，图像被分割模块划分为多个区域，编码器提取每个区域的特征矩阵，然后将其展平为一维向量输入LLM。在图像嵌入前后插入<code>&lt;img_start&gt;</code>和<code>&lt;img_end&gt;</code>边界标记，以帮助模型区分视觉内容与文本输入。</p>
<p><img src="https://arxiv.org/html/2506.19303v1/extracted/6541887/1.jpg" alt="视觉处理流程"></p>
<blockquote>
<p><strong>图3</strong>：视觉处理流程。图像被分割模块划分为多个区域，随后编码器提取特征矩阵，该矩阵被展平为一维向量后输入LLM。</p>
</blockquote>
<p><strong>核心模块二：触觉处理</strong>。基于OCTOPI框架，采用一个基于CLIP的触觉编码器来处理触觉数据。如图4所示，该编码器从一系列触觉图像中提取特征，编码空间和时间信息。随后为这些序列特征添加位置编码，以保留触觉交互的顺序和时间依赖性。通过在带有触觉视频注释和物理属性标签的物理数据集上进行训练，模型获得丰富的触觉感知表征。</p>
<p><img src="https://arxiv.org/html/2506.19303v1/extracted/6541887/output_image.jpg" alt="触觉处理流程"></p>
<blockquote>
<p><strong>图4</strong>：触觉图像序列首先由触觉编码器处理以提取特征表示。提取的特征随后被转换为结构化的特征向量，并添加位置嵌入以编码时间依赖性。</p>
</blockquote>
<p><strong>核心模块三：多模态融合</strong>。在获得投影后的物体图像特征向量（Fo）、触觉图像特征向量（Ft）和语言特征向量（Fl）后，通过通道拼接将它们融合成一个统一的表示：F_concat = [Fo; Ft; Fl]。这种融合方式保留了各模态的区分性特征，同时实现了跨模态交互。</p>
<p><strong>核心创新：精炼的提示策略</strong>。本文设计了一个结构化的提示来引导模型进行全面的物理属性分析。该提示明确定义了分析目标，强调基于材料的推理，并引导模型分两个阶段进行：基于视觉的物体识别（颜色、形状、纹理）和结合材料-触觉的属性评估。关键创新在于引入了一个10点李克特量表来量化硬度、弹性和粗糙度这三个基本属性（如表1所示），并要求模型为每个属性给出分数并提供基于材料特性的理由。特殊标记的使用增强了多模态信息的整合。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>主动感知架构</strong>：通过融合视觉线索和历史触觉信息，模型能够在接触前预测关键物理属性；2) <strong>结构化推理提示</strong>：采用分阶段的推理协议，系统地将物理属性推断分解为物体识别、材料分析和属性量化等可解释的步骤，并通过属性特定的提示引导模型关注相关的感官线索，从而提高了推理的准确性和可解释性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验使用配备GelSight Mini触觉传感器（用于高分辨率接触数据采集）和RealSense D410相机（用于视觉感知）的机器人系统。选用了35个常见家居物体（如图5所示），涵盖了塑料、橡胶、金属、木材、陶瓷、玻璃、泡沫、纸张和纺织品九大类材料。每个物体的真实物理属性均使用专业仪器测量：使用PosiTector SHD测量硬度（肖氏硬度），使用C610H自动拉伸试验机测量弹性模量，使用RUGOSURF 20粗糙度测试仪测量表面粗糙度（Ra）。测量值的分布如图6所示。</p>
<p><img src="https://arxiv.org/html/2506.19303v1/extracted/6541887/F5.jpg" alt="实验物体集合"></p>
<blockquote>
<p><strong>图5</strong>：包含35个常见家居物品的物体集合，涵盖九大类材料，用于评估多模态模型在物理属性推理上的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.19303v1/extracted/6541887/histogram_with_pictures.png" alt="物理属性测量值分布"></p>
<blockquote>
<p><strong>图6</strong>：实验中35个物体的粗糙度、硬度和弹性模量的测量值分布直方图。</p>
</blockquote>
<p><strong>对比方法</strong>：主要与触觉感知基准方法OCTOPI进行对比，包括其原始版本和使用本文提示进行细粒度评分的版本（Octopi (fine-grained)）。同时，还设置了仅使用视觉的消融实验（Octopi-ViTaL (vision only)）以验证多模态融合的必要性。</p>
<p><strong>关键实验结果</strong>：评估指标为模型预测分数与真实测量值之间的斯皮尔曼等级相关系数。如表2所示，本文提出的多模态模型（Octopi-ViTaL）在硬度（ρ=0.501）、弹性（ρ=0.530）和粗糙度（ρ=0.643）三个属性上均取得了显著且最高的相关性。相比之下，仅视觉模型的相关性较弱或中等，而仅触觉的OCTOPI模型（无论是原始版本还是细粒度提示版本）在所有属性上的相关性都非常低或不显著，特别是在零样本应用于新实验设置时，OCTOPI模型几乎无法产生有意义的预测（ρ &lt; 0.1）。这凸显了本文多模态方法的优势。</p>
<p><img src="https://arxiv.org/html/2506.19303v1/extracted/6541887/F4.png" alt="实验结果对比"></p>
<blockquote>
<p><strong>表2</strong>：零样本评估：模型预测与真实测量值之间的斯皮尔曼等级相关系数对比。本文模型（Octopi-ViTaL）在三个属性上均显著优于仅视觉和仅触觉的基线方法。</p>
</blockquote>
<p><strong>消融实验分析</strong>：表2的结果也构成了有效的消融研究。1) <strong>多模态融合的贡献</strong>：与仅视觉模型相比，融合了触觉信息的完整模型在所有属性上的相关性均有显著提升，证明了跨模态整合的价值。2) <strong>提示策略的贡献</strong>：与使用原始OCTOPI提示或简单细粒度提示的方法相比，本文的结构化提示策略使得模型能够更准确地量化属性，相关性大幅提高。3) <strong>触觉单独使用的局限性</strong>：OCTOPI作为触觉单独模型表现不佳，尤其是在新设置下的零样本泛化失败，说明了单一触觉模态在缺乏视觉上下文时的严重局限性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个新颖的视觉-触觉-语言多模态融合框架，用于机器人物理属性推理，通过视觉先验有效补偿了触觉传感的局限性。2) 设计了一种分阶段的结构化提示策略，主动引导大型语言模型进行可解释的、属性特定的推理，显著提升了量化评估的准确性。3) 在包含35个多样化物体的数据集上验证了方法的有效性，并展示了强大的零样本泛化能力，其预测与真实测量值具有显著相关性。</p>
<p>论文自身提到的局限性包括：模型性能依赖于预训练视觉语言模型的知识先验，且实验基于特定的触觉传感器（GelSight Mini）和视觉设置，存在领域偏移问题（如其他触觉传感器可能需重新适应）。</p>
<p>本文工作对后续研究的启示包括：1) <strong>提示工程的重要性</strong>：精心设计的、任务特定的提示策略可以极大程度地激发大型基础模型的推理潜力，是实现高性能零样本泛化的关键。2) <strong>多模态融合的必要性</strong>：对于复杂的物理属性推断，单一感官模态存在固有缺陷，深度融合视觉、触觉等互补信息是更可靠的途径。3) <strong>迈向实际应用</strong>：未来可将此多模态感知框架应用于需要自适应操作的机器人抓取任务中，根据推断的材料属性实时调整抓取力或策略。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中物理属性推断不准确的问题，提出了一种新型的大触觉-视觉-语言模型跨模态感知框架。其核心技术在于整合视觉与触觉表征，并采用分层特征对齐机制及优化的提示策略，实现多模态融合与物理推理。在35个多样物体上的实验表明，该方法性能优于现有基线模型，并展现出强大的零样本泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.19303" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>