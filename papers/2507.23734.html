<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.23734" target="_blank" rel="noreferrer">2507.23734</a></span>
        <span>作者: Jianbing Shen Team</span>
        <span>日期: 2025-07-31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人抓取领域的研究依赖于对物体可操作性的精准感知。主流方法通常基于特定领域（如机器人操作台、室内第一视角）的数据集进行监督学习，或尝试利用视觉语言模型来处理以物体类别名为核心的简单指令。这些方法存在关键局限性：1) 数据集规模有限、领域单一（如仅有机器人数据或室内数据），缺乏多样性，导致模型在开放世界的泛化能力不足；2) 指令形式固定且简单，通常直接提及物体名称，缺乏类似人类的高层次功能推理式指令，限制了与机器人的自然交互。</p>
<p>本文针对“缺乏大规模、多领域、具备推理能力的可操作性分割数据”这一具体痛点，提出了构建一个全新基准的新视角。核心思路是：1) 整合来自野外、机器人、第一视角和仿真等多个具身数据领域的大规模图像，构建一个包含27.3万张图像、180个类别的可操作性分割数据集（RAGNet）；2) 利用大语言模型生成两类基于推理的指令（易版含类别名，难版仅含功能描述），模拟真实人机交互；3) 基于此数据训练一个名为AffordanceNet的框架，通过视觉语言模型预测可操作性区域，再结合深度信息生成抓取位姿，实现开放世界的通用抓取。</p>
<h2 id="方法详解">方法详解</h2>
<p>AffordanceNet框架包含两个关键组件：用于预测可操作性分割掩码的AffordanceVLM，以及将掩码转换为3D抓取位姿的位姿生成器。整体流程为：输入RGB图像和人类指令，AffordanceVLM输出精确的2D可操作性区域掩码；随后，将该掩码与深度图像结合，通过位姿生成器计算出机器人末端执行器的3D抓取位姿。</p>
<p><img src="https://arxiv.org/html/2507.23734v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：数据标注流程与操作模型整体框架。左侧展示了从多个公开数据集收集数据，并利用多种工具和人工标注生成可操作性掩码，再通过模板和GPT-4生成推理指令的流程。右侧展示了AffordanceNet的操作流程：AffordanceVLM根据RGB图像和指令预测可操作性掩码，位姿生成器结合深度图将该掩码转换为所需的抓取位姿。</p>
</blockquote>
<p><strong>核心模块1：AffordanceVLM</strong><br>该模块基于视觉语言分割模型LISA，并进行了两项任务特定的修改以增强可操作性预测：1) 设计专门的系统提示“You are an embodied robot.”；2) 引入独特的<AFF>标记。具体而言，使用ViT-CLIP图像编码器处理输入图像，并通过线性投影层将其特征映射到LLM（Vicuna-7B）的嵌入空间。文本指令被分词后，与图像特征拼接并输入LLM。为突破原模型<SEG>标记表征能力的限制，新增<AFF>标记来丰富掩码嵌入，使其更专注于可操作性相关的语言表达。最后，使用SAM作为掩码解码器，将LLM输出的掩码嵌入转换为像素级分割掩码。在训练时，除了RAGNet数据，还混合了使用<SEG>标记的通用分割数据集。</p>
<p><strong>核心模块2：位姿生成器</strong><br>该模块将AffordanceVLM输出的2D掩码转换为机器人可执行的3D抓取位姿。给定2D图像点集P和预测的可操作性掩码M，首先通过逐点相乘进行滤波，得到可操作性区域内的2D点集P̂ = P ⊗ M。对于P̂中的每个2D位置(u, v)及其对应的深度值d，利用相机内参矩阵K和外参矩阵T，通过公式(1)将其反投影到3D世界坐标系，得到点(x, y, z)。在获得可操作性区域的3D点云后，可以接入各种抓取规划模型来生成具体的抓取器位姿，最终驱动机器人完成抓取。</p>
<p><strong>创新点</strong><br>与现有方法相比，创新点主要体现在：1) <strong>数据集层面</strong>：构建了首个大规模、多领域、包含复杂推理指令的可操作性分割基准RAGNet；2) <strong>模型层面</strong>：在通用VLM分割框架中引入<AFF>标记和机器人系统提示，使其更适配可操作性感知任务；3) <strong>系统层面</strong>：提供了一个从开放世界视觉语言理解到真实机器人抓取执行的完整、可部署的管道，弥合了此前MLLM方法在真实机器人部署上的空白。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：使用了构建的RAGNet进行训练。评估则在其多个验证集上进行：HANDAL（域内）、GraspNet seen（域内未见物体实例）、GraspNet novel（零样本，未见物体类别）、3DOI（零样本，未见图像域）。此外，还设置了包含“易版”和“难版”推理指令的验证集。</li>
<li><strong>实验平台</strong>：视觉实验在标准服务器进行；机器人实验使用UR5机械臂和Intel RealSense RGB-D相机。</li>
<li><strong>对比方法</strong>：包括无需LLM的基础模型（VLPart+SAM2, Grounding DINO+SAM2, Florence2+SAM2）和通用MLLM（LISA, GLaMM）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2507.23734v1/x4.png" alt="视觉结果"></p>
<blockquote>
<p><strong>图4</strong>：AffordanceNet在不同数据源（野外、机器人、第一视角、仿真）上的可操作性分割可视化结果。模型能够准确分割出从未见过的场景中的物体可操作部分（如炒锅手柄），展示了强大的开放世界泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.23734v1/x5.png" alt="推理结果"></p>
<blockquote>
<p><strong>图5</strong>：基于推理指令的可操作性分割可视化结果。左列为“易版”指令（提及物体名），右列为“难版”指令（仅描述功能）。模型即使在没有明确目标物体名称的情况下，也能根据功能描述准确定位（如锤子手柄和马克杯手柄）。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="center">HANDAL gIoU</th>
<th align="center">HANDAL cIoU</th>
<th align="center">GraspNet novel gIoU</th>
<th align="center">3DOI gIoU</th>
</tr>
</thead>
<tbody><tr>
<td align="left">VLPart+SAM2</td>
<td align="center">40.9</td>
<td align="center">28.9</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr>
<td align="left">LISA</td>
<td align="center">16.2</td>
<td align="center">12.0</td>
<td align="center">25.2</td>
<td align="center">21.5</td>
</tr>
<tr>
<td align="left">GLaMM</td>
<td align="center">24.9</td>
<td align="center">17.2</td>
<td align="center">19.2</td>
<td align="center">19.7</td>
</tr>
<tr>
<td align="left"><strong>AffordanceNet (Ours)</strong></td>
<td align="center"><strong>60.3</strong></td>
<td align="center"><strong>60.8</strong></td>
<td align="center"><strong>45.6</strong></td>
<td align="center"><strong>37.4</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表4</strong>：可操作性分割定量结果（部分）。我们的模型在所有测试集上均显著优于其他对比方法。在零样本设置的GraspNet novel和3DOI数据集上也取得了最佳性能。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>表5展示了逐步添加不同训练数据对模型性能的影响。关键结论包括：1) HANDAL数据对提升其在同类数据上的性能至关重要；2) 添加推理数据会导致在HANDAL†上的指标轻微下降，但这是为了获得推理能力所做的权衡；3) 最终引入任务特定修改（系统提示和<AFF>标记）后，模型性能得到提升，在零样本任务上达到最佳平衡。</p>
<table>
<thead>
<tr>
<th align="left">Instruction Type</th>
<th align="center">Average Success Rate</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Easy Reasoning</td>
<td align="center">62%</td>
</tr>
<tr>
<td align="left">Hard Reasoning</td>
<td align="center">48%</td>
</tr>
<tr>
<td align="left"><strong>AffordanceNet</strong></td>
<td align="center"><strong>70%</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表8</strong>：真实机器人抓取任务中，使用不同指令类型的消融研究结果。使用“易版”和“难版”推理指令的抓取成功率均低于使用完整AffordanceNet模型（其指令可能结合了模板与推理），表明模型需要专门的训练来整合不同指令模式。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.23734v1/x6.png" alt="机器人抓取"></p>
<blockquote>
<p><strong>图6</strong>：UR5机器人上的物体抓取结果。机器人能够根据“我需要一把螺丝刀来修理”等功能性指令，成功定位并抓取目标物体。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="center">Average Success Rate</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GraspNet</td>
<td align="center">32%</td>
</tr>
<tr>
<td align="left"><strong>AffordanceNet (Ours)</strong></td>
<td align="center"><strong>70%</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表7</strong>：真实机器人抓取任务的平均成功率对比。我们的方法在10个不同的抓取任务上取得了70%的平均成功率，显著高于对比方法GraspNet（该方法不支持语言指令）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个大规模、多领域、包含复杂推理指令的可操作性分割基准RAGNet，包含27.3万张图像、180个类别和2.6万条推理指令。</li>
<li>提出了一个名为AffordanceNet的综合性可操作性抓取框架，它通过引入<AFF>标记的AffordanceVLM和位姿生成器，实现了从开放世界视觉语言理解到真实机器人抓取执行的端到端流程。</li>
<li>通过大量的零样本分割实验和真实的开环机器人抓取任务，验证了所提数据和模型卓越的开放世界泛化与推理能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，所提出的数据标注流程涉及多种工具和人工检查，仍然较为复杂。此外，基于大型VLM的模型在计算资源需求上可能较高。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据驱动泛化</strong>：研究表明，大规模、多样化的数据是提升机器人感知系统开放世界泛化能力的关键。RAGNet基准为后续研究提供了宝贵的数据资源。</li>
<li><strong>具身智能与VLM结合</strong>：工作展示了如何通过任务特定的设计（如<AFF>标记），将通用VLM有效地适配到具体的机器人感知任务中，为具身智能研究提供了参考。</li>
<li><strong>从感知到行动</strong>：AffordanceNet提供了一个完整的抓取系统范例，强调了将高级语义理解（可操作性推理）与低级动作执行（抓取位姿生成）紧密结合的重要性，推动了通用抓取系统的发展。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人抓取中缺乏基于推理的大规模可供性分割数据、限制开放世界泛化能力的问题，构建了RAGNet基准，包含273k图像、180类别和26k推理指令，覆盖野外、机器人等多域数据。提出AffordanceNet框架，采用在大量可供性数据上预训练的视觉语言模型（VLM）和基于可供性地图的抓取网络。实验表明，该模型在可供性分割和真实机器人任务中展现出强大的开放世界泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.23734" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>