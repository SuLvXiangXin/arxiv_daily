<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16203" target="_blank" rel="noreferrer">2511.16203</a></span>
        <span>作者: Yaochu Jin Team</span>
        <span>日期: 2025-11-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作（VLA）模型在具身智能领域取得了显著进展，能够通过统一的多模态理解实现感知、推理和行动。尽管性能强大，这些系统的对抗鲁棒性在很大程度上仍未得到充分探索，尤其是在现实的多模态和黑盒条件下。现有研究主要集中于单模态扰动（如仅文本或仅视觉），忽略了跨模态错位这一从根本上影响具身推理和决策的关键问题。本文针对VLA模型在真实部署中面临的多模态对抗威胁，特别是跨模态语义对齐的脆弱性，提出了首个系统性的多模态对抗攻击评估框架。核心思路是：通过联合攻击语言、视觉及二者间的语义对齐，全面评估VLA模型在整个感知-语言-动作流水线中的鲁棒性，揭示其脆弱性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了VLA-Fool框架，这是一个统一的多模态对抗攻击套件，系统性地包含文本、视觉和跨模态错位攻击。</p>
<p><img src="https://arxiv.org/html/2511.16203v3/framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：VLA-Fool框架概览。该框架由三个针对不同模态的互补模块组成：(a) 文本模态攻击，(b) 视觉模态攻击，以及 (c) 跨模态攻击。</p>
</blockquote>
<p>整体上，给定视觉观测 <code>I</code> 和自然语言指令 <code>T</code>，VLA模型 <code>M</code> 产生动作 <code>A</code>。攻击者的目标是生成扰动后的多模态输入 <code>(I_adv, T_adv)</code>，以误导模型产生错误动作 <code>A_adv</code>。攻击面包括文本扰动、视觉扰动和跨模态错位。</p>
<p><strong>1. 文本攻击</strong><br>文本攻击旨在生成对抗性指令 <code>T_adv</code>，迫使模型产生非期望动作。VLA-Fool实现了两类攻击：</p>
<ul>
<li><strong>语义贪婪坐标梯度（SGCG）攻击（白盒）</strong>：扩展了GCG方法，将其引入VLA感知的语义空间。它并行优化四类具有特定语义扰动策略的对抗指令：(1) <strong>指代模糊</strong>：将具体实体词替换为代词或泛指名词；(2) <strong>属性削弱或替换</strong>：改变或移除用于细粒度视觉匹配的关键属性词；(3) <strong>范围/量词模糊</strong>：放宽空间描述符或量词；(4) <strong>否定/比较混淆</strong>：引入否定词或比较性短语以改变逻辑推理。优化过程在每一步选择梯度最大的词元位置，并从包含通用和类别特定语义替代词的候选池中，在满足词性匹配约束下，贪婪地选择使攻击损失最大的替代词元。</li>
<li><strong>提示注入攻击（黑盒）</strong>：无需模型内部信息，包括<strong>后缀注入</strong>（在正确指令后附加对抗性字符串，如“忽略之前的信息…”或随机代码块以破坏分词器）和<strong>前缀注入</strong>（在指令前添加误导性上下文，如“扮演一个敌对代理…”），旨在通过简单的上下文偏移来测试模型的脆弱性。</li>
</ul>
<p><strong>2. 视觉攻击</strong><br>视觉攻击通过扰动视觉观测 <code>I</code> 来误导模型。</p>
<ul>
<li><strong>局部补丁攻击（白盒）</strong>：利用模型梯度直接优化补丁内容 <code>δ_p</code>，并将其插入图像固定区域 <code>Ω</code>。测试了两种策略：<strong>环境物体补丁</strong>（模拟场景中常见的小物体）和<strong>机器人安装补丁</strong>（附着于机器人末端执行器或手臂）。优化目标是最大化正确动作 <code>A</code> 与对抗动作 <code>M(I_adv, T)</code> 之间的L2距离。</li>
<li><strong>基于噪声的扰动攻击（黑盒）</strong>：模拟传感器噪声或环境退化，无需梯度。包括<strong>先验噪声家族</strong>（高斯、椒盐、散斑噪声）和<strong>结构化随机化破坏</strong>（均匀噪声、伪随机模式、差分隐私风格随机化）。</li>
</ul>
<p><strong>3. 跨模态错位攻击</strong><br>此类攻击不孤立地扰动单一模态，而是寻求最优的对抗对 <code>(δ_v, δ_t)</code>，以最大化跨模态错位损失 <code>ℒ_mis</code>。该损失结合了表征错位项和动作偏差项。核心是最大化干净图像-指令对与对抗对之间视觉块嵌入和语言词元嵌入余弦相似度的差异，从而直接攻击VLA模型的跨模态特征对齐机制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：所有实验在仿真中使用<strong>LIBERO</strong>数据集进行，该数据集包含空间关系、物体识别、目标导向和长时程四类任务。受害模型为在LIBERO上微调的<strong>OpenVLA</strong>模型。基线方法包括文本攻击的<strong>GCG</strong>和视觉攻击的<strong>无目标动作差异攻击</strong>。主要评估指标为<strong>失败率（FR）</strong>，即攻击成功率。</p>
<p><strong>关键实验结果</strong>：如表1所示，所有攻击模态均显示出OpenVLA模型的显著脆弱性。</p>
<ul>
<li><strong>文本攻击</strong>：广义GCG基线平均FR达79.23%。在SGCG变体中，SGCG 1（指代模糊）和SGCG 2（属性替换）在物体和目标类别上导致最高性能下降；SGCG 4（否定混淆）在长时程任务上表现尤为出色，FR达75%。在黑盒提示注入中，Suffix 2（随机代码）破坏性最强，平均FR达83.33%。</li>
<li><strong>视觉攻击</strong>：白盒补丁攻击中，<strong>机器人手臂补丁攻击达到了100%的失败率</strong>，表明模型对自我中心视角的扰动极度敏感。黑盒噪声攻击中，多种噪声（如椒盐噪声FR 84.87%）均能有效降低任务成功率。</li>
<li><strong>跨模态错位攻击</strong>：此类攻击在所有任务类别上都表现出极高的有效性，<strong>在长时程任务上甚至达到了100%的失败率</strong>，平均失败率远超单一模态攻击，突显了跨模态对齐的极端脆弱性。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.16203v3/sim.png" alt="语义相似度与攻击成功率关系"></p>
<blockquote>
<p><strong>图3</strong>：LIBERO空间子集上文本攻击的语义相似度（与原指令）与对应攻击成功率的关系图。显示二者呈明显负相关，即语义相似度越低，攻击成功率越高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.16203v3/level2.png" alt="噪声强度对失败率的影响"></p>
<blockquote>
<p><strong>图5</strong>：不同噪声强度（低、中、高）对任务失败率的影响。随着噪声强度增加，失败率普遍上升，表明模型对噪声扰动的敏感性具有剂量依赖性。</p>
</blockquote>
<p><strong>消融实验分析</strong>：通过对比SGCG各变体与通用GCG的结果，可以看出语义引导的针对性攻击（如SGCG 2, 4）在特定任务类别上比无差别的梯度最大化（GCG）更有效。跨模态攻击的极高成功率也证明了专门设计来破坏视觉-语言对齐的模块具有独立且强大的贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>VLA-Fool</strong>，首个针对具身VLA模型、涵盖白盒与黑盒设置的多模态对抗攻击统一评估框架，系统性地攻击了文本、视觉及跨模态对齐。</li>
<li>通过将GCG方法扩展至<strong>VLA感知的语义空间</strong>，引入了首个自动构建、语义引导的提示攻击框架，包含四种语义错位模式。</li>
<li>通过大量实验揭示了先进VLA模型在面对多模态扰动时的<strong>极度脆弱性</strong>（失败率常超过60%，跨模态攻击在长时程任务上达100%），为开发更鲁棒的具身智能体提供了重要基准和见解。</li>
</ol>
<p><strong>局限性</strong>：论文主要关注数字仿真环境下的攻击，虽然部分攻击（如补丁、噪声）模拟了物理世界扰动，但对真实物理世界中复杂干扰和对抗样本的可转移性、鲁棒性探索仍有局限。</p>
<p><strong>后续研究启示</strong>：本研究凸显了当前VLA模型在多模态对齐和对抗鲁棒性方面的严重不足。未来工作亟需从模型架构、训练策略（如对抗训练）、多模态融合机制等方面着手，提升VLA系统在开放、动态且可能存在对抗性干扰的真实环境中的可靠性和安全性。同时，评估基准应更多纳入物理世界约束和黑盒攻击场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在现实多模态与黑盒条件下的对抗鲁棒性缺失问题，提出了VLA-Fool框架。该框架统一了三种多模态对抗攻击：基于梯度与提示的文本扰动、基于补丁与噪声的视觉扰动、以及故意破坏感知-指令语义对应的跨模态错位攻击，并首次构建了语义引导的自动提示生成方法。在LIBERO基准上的实验表明，即使轻微的多模态扰动也会导致微调后的OpenVLA模型产生显著的行为偏差，揭示了具身多模态对齐的脆弱性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16203" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>