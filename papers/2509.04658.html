<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.04658" target="_blank" rel="noreferrer">2509.04658</a></span>
        <span>作者: Noorbakhsh Amiri Golilarz Team</span>
        <span>日期: 2025-09-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人表面材质分类是提升机器人触觉感知与交互能力的关键。当前主流方法可分为两类：基于经典机器学习的方法（如SVM、手工特征）和基于深度学习的方法（如CNN、Transformer）。前者难以捕捉复杂非线性关系，后者虽表现出色，但在多模态融合上仍面临挑战。具体局限性包括：1) 直接将视觉预训练模型（如ViT）迁移到触觉数据存在领域鸿沟，可能导致性能下降；2) 有效的多模态融合机制设计复杂，对噪声或遮挡敏感；3) 直接将原始视觉与触觉图像输入深度学习模型计算成本高、速度慢。本文针对其前作Surformer v1的局限性（依赖预提取特征、中层注意力融合可能关注无关特征）以及多模态CNN的延迟问题，提出了一种新的视角：为视觉和触觉这两种异质输入（广角视觉图与触觉特写图）设计独立的处理分支，并在决策层进行自适应加权融合。本文核心思路是：采用后期（决策层）融合策略，让视觉和触觉分支各自独立完成特征提取与分类，再通过可学习的加权和融合其输出（logits），从而在保证性能的同时，显著提升推理效率，使其适用于实时机器人应用。</p>
<h2 id="方法详解">方法详解</h2>
<p>Surformer v2的整体框架是一个双分支并行处理，后期融合的架构。输入为视觉RGB图像和触觉RGB图像，输出为表面材质的分类结果。</p>
<p><img src="https://arxiv.org/html/2509.04658v1/main.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Surformer v2整体架构。模型接受视觉和触觉两种输入，分别通过视觉分支（CNN分类器）和触觉分支（编码器-仅Transformer）进行独立处理，生成各自的类别logits。两个分支的输出通过一个可学习的加权融合层进行组合，最终通过Softmax层得到分类概率。</p>
</blockquote>
<p>核心模块包括视觉分支、触觉分支、加权融合层和最终输出层。</p>
<ol>
<li><strong>视觉分支</strong>: 采用EfficientNetV2-S作为主干网络，利用其在ImageNet上的预训练权重。输入为尺寸[B, 3, 224, 224]的RGB图像。为了有效迁移学习，采用了选择性微调策略：冻结早期卷积层以保留通用低级特征，仅解冻最后20个参数进行适应。网络末端的原始分类器被替换为一个自定义的两层MLP，包含Dropout(0.1)、线性层（1280-&gt;256）、ReLU激活、另一个Dropout(0.1)和最终的分类线性层（256-&gt;类别数），输出视觉logits。</li>
<li><strong>触觉分支</strong>: 输入为RGB触觉图像。首先将其转换为灰度图，然后提取一个7维的手工特征向量，并进行数据集级的归一化。该特征向量被投影到64维的嵌入空间，并添加位置编码。随后，特征通过一个单层的Transformer编码器（4个注意力头，前馈网络维度为256）。编码器的输出通过一个包含LayerNorm和Dropout的分类头，压缩为触觉logits。</li>
<li><strong>加权融合层</strong>: 这是模型的关键创新点。该层接收视觉和触觉分支输出的logits，通过一组可学习的权重进行加权求和，权重经过Softmax归一化。这些权重在训练过程中通过梯度下降进行优化，使模型能够根据数据上下文和训练动态，自适应地平衡两种模态的重要性。</li>
<li><strong>最终输出层</strong>: 将加权融合后的logits通过Softmax函数转换为类别概率分布，并选取概率最高的类别作为预测结果。</li>
</ol>
<p>与现有方法相比，Surformer v2的创新点具体体现在：1) <strong>融合策略</strong>：从Surformer v1的中层特征注意力融合改为决策层（logits）融合，简化了融合设计，并增强了对单一模态缺失或噪声的鲁棒性。2) <strong>特征提取</strong>：将特征提取过程完全内化到模型中，摒弃了Surformer v1依赖外部预提取手工特征的流程，使模型成为一个端到端的完整解决方案。3) <strong>分支专门化</strong>：针对视觉和触觉数据的异质性，分别为其选用了CNN（擅长处理自然图像）和轻量Transformer（适合处理结构化特征）架构，使各分支能更有效地学习模态特异性特征。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Touch and Go数据集上进行，该数据集包含五种表面材质（混凝土、木材、砖块、合成织物、草）的视觉图像和对应的触觉（GelSight）图像。实验采用了80-20的 stratified 训练-测试分割。对比的基线方法包括：Surformer v1、多模态CNN（两个独立ResNet50V2分支，特征级融合）、多模态CNN早期融合版（将6通道图像输入单一ResNet50V2）。评估指标包括准确率、精确率、召回率、F1分数、推理时间和参数量。</p>
<p>关键实验结果如下：</p>
<ul>
<li>在分类性能上（表I），多模态CNN及其早期融合变体达到了完美的100%精确率、召回率和F1分数。Surformer v1紧随其后，达到99%。Surformer v2的指标为97%，略有下降。</li>
<li>在计算效率上（表II），Surformer v2展现出巨大优势：其推理时间仅为0.0239毫秒，远快于多模态CNN（5.0737毫秒）和Surformer v1（0.7271毫秒），实现了约30倍的加速。其参数量为20.660M，介于轻量级的Surformer v1（0.673M）与庞大的多模态CNN（48.311M）之间。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.04658v1/fig1.png" alt="实验结果"></p>
<blockquote>
<p><strong>图2</strong>：Surformer v2在Touch and Go数据集上的性能评估。左侧混淆矩阵显示模型在五个类别上的分类情况，对角线值高，主要混淆发生在砖块和合成织物之间。右侧ROC曲线显示所有类别的AUC值接近1.00（混凝土、木材、草为1.00，砖块和合成织物为0.99），表明模型具有极强的区分能力。</p>
</blockquote>
<p>图2的混淆矩阵进一步证实了Surformer v2的高准确性，大部分样本被正确分类，主要混淆发生在砖块和合成织物之间，但错误数量很少。ROC曲线及接近1.0的AUC值（宏平均AUC为1.00）表明模型在所有类别上都具有优秀的判别性能。</p>
<p>虽然没有严格的消融实验，但论文通过架构设计对比间接说明了各组件贡献：1) <strong>决策层融合</strong>：相比Surformer v1的中层注意力融合，带来了更快的推理速度和更好的部署实用性。2) <strong>模型内特征提取</strong>：使模型成为一个完整的端到端系统，便于部署。3) <strong>多目标损失函数</strong>（总损失 = 主融合损失 + 0.3 * 视觉损失 + 0.3 * 触觉损失）：确保了两个独立分支都能学习到有效的特征，防止融合层过度依赖某一模态。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了一种新颖的、基于决策层融合的多模态分类架构Surformer v2，通过独立处理异质模态并自适应加权融合其输出，增强了模型的鲁棒性和实用性。2) 将特征提取过程完全集成到模型内部，提供了一个更完整的端到端解决方案，并实现了相比前作约30倍的推理速度提升，使其非常适合实时机器人应用。3) 在Touch and Go数据集上验证了该框架在保持高分类准确率的同时，具有卓越的计算效率。</p>
<p>论文自身提到的局限性在于，为了追求极致的推理速度，Surformer v2在分类精度上做出了轻微妥协（97.4% vs. 对比方法的99%-100%）。这种精度与效率的权衡是实际部署中需要考量的关键点。</p>
<p>本研究对后续工作的启示在于：决策层融合策略在处理输入差异巨大（如视角、尺度、语义不同）的多模态任务中可能是一种高效且鲁棒的选择。未来的研究可以探索如何进一步优化各分支的架构（如触觉分支的自动特征学习），或在更复杂、更具挑战性的多模态数据集上验证此类融合范式的泛化能力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Surformer v2模型，旨在解决机器人感知中如何有效融合视觉与触觉信息以实现表面材料分类的核心问题。关键技术采用决策级晚期融合机制：视觉分支使用CNN分类器（Efficient V-Net），触觉分支采用编码器-仅Transformer模型，通过可学习的加权求和融合两模态的输出logits。实验在Touch and Go多模态数据集上进行，结果表明模型性能良好，并保持了有竞争力的推理速度，适用于实时机器人应用。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.04658" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>