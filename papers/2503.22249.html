<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.22249" target="_blank" rel="noreferrer">2503.22249</a></span>
        <span>作者: Zhang, Xianqi, Wei, Hongliang, Wang, Wenrui, Wang, Xingtao, Fan, Xiaopeng, Zhao, Debin</span>
        <span>日期: 2025/03/28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在仿人机器人领域，强化学习是控制其全身运动的主流方法。现有RL方法通常依赖任务奖励来引导智能体学习，但很少明确考虑身体姿态稳定性对移动和操作任务的影响。仿人机器人具有高自由度，且设计有效的奖励函数具有挑战性，导致仅依赖任务奖励的RL方法难以实现高性能的全身控制。本文针对“如何将身体稳定性明确引入仿人机器人控制学习”这一痛点，提出了利用预训练基础模型（人类运动重建模型）来设计稳定奖励函数的新视角。其核心思路是：将机器人姿态映射到3D人体模型，利用基础模型评估并重建出稳定的人体姿态，通过对比原始与重建姿态的差异来计算稳定奖励，并将此奖励与任务奖励结合，共同引导策略学习更稳定、高效的机器人行为。</p>
<h2 id="方法详解">方法详解</h2>
<p>FLAM方法包含两个核心部分：一个基于基础模型设计的稳定奖励函数，以及一个用于执行任务的基本策略。整体流程是：策略与环境交互产生状态轨迹，该轨迹被输入稳定奖励函数计算奖励，最终组合的奖励用于更新策略。</p>
<p><img src="https://arxiv.org/html/2503.22249v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：FLAM整体框架。左侧为策略（智能体）与环境交互，产生状态轨迹；右侧为稳定奖励函数计算模块，它将机器人状态映射为人体姿态，经基础模型重建后，通过比较原始与重建姿态计算稳定奖励；最终，任务奖励与稳定奖励结合，用于策略更新。</p>
</blockquote>
<p><strong>稳定奖励函数</strong> 是该方法的创新核心，其输入是一段机器人状态轨迹，输出是对应的稳定奖励。具体包含三个步骤：</p>
<ol>
<li><strong>机器人-人体姿态映射</strong>：将机器人关节姿态映射到对应的SMPL-X人体模型关节姿态。映射时，首先对齐初始零位姿。对于后续状态，根据机器人关节的相对朝向调整对应的人体关节角度。对于关节数量不匹配的情况：a) 人体关节更多时，选择关键关节映射，冗余关节保持初始姿态；b) 人体关节更少时，将人体球关节的不同坐标轴映射到机器人的多个旋转关节（如肩部）。</li>
<li><strong>人类运动重建</strong>：使用一个基于扩散的预训练人类运动重建基础模型RoHM，对映射得到的人体姿态轨迹进行稳定化重建。该模型以带噪声的人体姿态轨迹为输入，输出平滑、稳定的重建姿态轨迹。</li>
<li><strong>奖励计算</strong>：通过比较映射的原始人体姿态与重建后稳定人体姿态的相似性来计算奖励。具体地，计算两个姿态对应关节位置和朝向的差异，若某个关节的差异小于阈值 (t_j)，则给予该关节奖励 (r_j)。将所有关节的奖励求和得到姿态相似性得分。只有当该总分超过设定的相似性阈值 (t_s) 时，才将总分作为稳定奖励输出，否则奖励为0。这鼓励机器人学习与稳定人体姿态相似的姿态。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.22249v1/x2.png" alt="奖励函数概览"></p>
<blockquote>
<p><strong>图2</strong>：稳定奖励函数概览。展示了从机器人状态映射到人体姿态，再经基础模型重建，最后通过比较计算奖励的完整流程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.22249v1/x3.png" alt="姿态映射过程"></p>
<blockquote>
<p><strong>图3</strong>：机器人-人体姿态映射过程示意图。展示了机器人关节（左）如何对应到人体SMPL-X模型关节（右），为清晰起见进行了简化。</p>
</blockquote>
<p><strong>基本策略与训练策略</strong>：本文直接采用现有的模型基RL方法TD-MPC2作为基本策略。其创新点主要体现在奖励组合和训练流程上。总奖励 (R) 由任务奖励 (R_T) 和稳定奖励 (R_S) 按公式 (R = R_T + \lambda \frac{q}{l_e} \cdot R_S) 组合，其中 (\lambda) 是缩放因子，(q) 是任务预期回报，(l_e) 是最大回合长度。(\lambda) 根据任务对稳定性的需求调整（如爬行、坐下任务设为0.5，其他多数任务设为1.0）。由于稳定奖励函数需要轨迹段作为输入，训练时，策略先与环境交互 (l_s) 步收集轨迹段，然后计算该段内各状态的稳定奖励，接着组合最终奖励并存入回放缓冲区，最后从缓冲区采样进行策略更新，此过程重复 (l_s) 次。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在仿人机器人基准测试平台Humanoid-Bench上进行，机器人是Unitree H1，共61个自由度。评估指标为累计回报。对比的基线方法包括模型基RL（TD-MPC2, DreamerV3）和模型无关RL（CQN, CQN-AS, SAC）。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2503.22249v1/extracted/6317536/src/locomotion_results_600.png" alt="移动任务结果"></p>
<blockquote>
<p><strong>图4</strong>：在移动任务上的性能对比。FLAM在“Walk”、“Run”、“Hop”、“Jump”任务上均取得了最高的最终回报，显著优于所有基线方法。虚线定性地标明了任务成功。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.22249v1/extracted/6317536/src/manipulation_results_600.png" alt="操作任务结果"></p>
<blockquote>
<p><strong>图5</strong>：在操作任务上的性能对比。FLAM在“Carry”、“Lift”、“Throw”、“Two-hand Lift”任务上表现最佳，尤其是在需要较强身体稳定性的“Two-hand Lift”任务中优势明显。</p>
</blockquote>
<p>文字总结：在4个移动任务和4个操作任务中，FLAM在绝大多数任务上（Walk, Run, Hop, Jump, Carry, Lift, Throw, Two-hand Lift）的最终性能都优于所有基线方法。例如，在“Jump”任务中，FLAM的回报接近300，而次优的TD-MPC2约为220；在“Two-hand Lift”任务中，FLAM回报约175，显著高于其他方法（最高约125）。这表明引入基于基础模型的稳定奖励有效提升了策略在复杂任务中的性能。</p>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2503.22249v1/extracted/6317536/src/ablation_results_600.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图7</strong>：消融实验结果。对比了FLAM完整版本、仅使用任务奖励（w/o RS）、以及使用随机初始化的基础模型（w/ Random Model）三种配置。完整版FLAM性能最优，移除稳定奖励后性能下降，使用随机模型则完全失效，这验证了稳定奖励函数及其依赖的预训练基础模型的有效性与必要性。</p>
</blockquote>
<p>消融实验表明，移除稳定奖励（w/o RS）会导致在所有测试任务上性能下降，证明了稳定奖励组件的贡献。而使用随机初始化的基础模型（w/ Random Model）则无法学习到任何有效策略，凸显了预训练模型所提供的“稳定性先验知识”的关键作用。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了FLAM，一种将基于基础模型设计的稳定奖励函数与基本策略相结合的新方法，用于提升仿人机器人移动与操作任务的性能。</li>
<li>设计了一种新颖的稳定奖励函数，它利用预训练的人类运动重建模型作为“稳定性评判器”，通过比较机器人姿态与模型重建的稳定人体姿态的差异来生成奖励，从而显式地将身体稳定性约束引入强化学习过程。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于特定的预训练基础模型（RoHM）和人体模型（SMPL-X）。此外，由于稳定奖励计算涉及轨迹段处理和模型推理，可能会引入额外的计算成本。</p>
<p><strong>后续启示</strong>：这项工作展示了利用来自其他领域（如计算机视觉/图形学）的预训练基础模型，为机器人控制提供高层语义先验（如姿态稳定性、人体运动合理性）的有效途径。这为机器人学习开辟了新思路，即不再仅仅依赖人工设计的物理规则或大量的任务演示数据，而是可以巧妙地利用现成的、在大规模数据上训练的基础模型作为“指导者”或“评判者”，以提升学习效率与最终性能。未来可探索将其他类型的基础模型（如视频预测模型、物理推理模型）用于更丰富的机器人技能学习。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出FLAM方法，解决强化学习控制人形机器人时忽视身体稳定性、导致全身控制性能受限的核心问题。其关键技术是设计了一个基于基础模型的稳定奖励函数：先将机器人姿态映射到3D虚拟人体模型，再利用人体运动重建模型对其进行稳定化重建，最后通过对比重建前后的姿态计算稳定奖励，并与任务奖励结合以指导策略学习。实验表明，FLAM在人形机器人基准测试中超越了现有先进强化学习方法，显著提升了运动与操作任务的稳定性和整体性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.22249" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>