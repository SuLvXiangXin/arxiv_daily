<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16330" target="_blank" rel="noreferrer">2511.16330</a></span>
        <span>作者: Ravi Prakash Team</span>
        <span>日期: 2025-11-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人从静态工业任务转向动态非结构化交互环境的背景下，机器人需要结合动态运动基元（DMPs）进行运动规划和变阻抗控制（VIC）以实现柔顺交互。强化学习（RL），特别是路径积分（PI²）方法，已成为学习复杂DMP和VIC策略的强大工具。然而，这种无模型范式由于阻抗增益的时变特性，存在不稳定和不安全探索的风险。现有方法，无论是基于模型的iLQR/DDP（依赖模型且计算量大）还是无模型的PI²（缺乏稳定性保证），或是基于能量箱、安全滤波器的方法，均无法在统一的优化框架内同时满足无模型学习、增益调度、稳定性认证和执行器可行性。本文针对VIC策略学习过程中缺乏内置稳定性保证这一核心痛点，提出将Lyapunov稳定性条件直接嵌入RL探索循环的新视角。核心思路是：将策略探索重新定义为从一个数学定义的稳定增益流形中进行采样，从而确保每次策略执行都先天稳定且物理可行。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了认证高斯流形采样（C-GMS）框架，这是一个轨迹中心化的RL框架，用于学习联合的DMP和VIC策略，并通过构造保证Lyapunov稳定性和执行器可行性。</p>
<p>整体流程基于PI²策略改进，但关键创新在于对时变阻抗增益的参数化方式。输入是策略参数（包括DMP权重和增益松弛变量参数），输出是保证稳定且满足执行器限幅的控制指令。流程核心是：在每次迭代中，从高斯分布中采样策略参数扰动，但通过C-GMS机制，确保采样后的参数所合成的整个增益时间表（𝐃(𝑡), 𝐊(𝑡)）始终满足Lyapunov稳定性条件。</p>
<p><img src="https://arxiv.org/html/2511.16330v1/Images/cgms_framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：C-GMS框架概述。轨迹使用DMP参数化，时变VIC增益使用松弛变量参数化。在标准方法中，增益直接从高斯分布采样，可能违反Lyapunov稳定性条件导致不稳定。C-GMS通过从一个满足Lyapunov条件的认证流形中采样，确保整个学习过程中的轨迹都是稳定的。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>轨迹参数化</strong>：使用DMP生成期望轨迹（𝐱_d, 𝐱̇_d, 𝐱̈_d）。非线性强制项由归一化径向基函数（RBF）与可学习参数（𝜽_traj）及探索噪声（𝝃_traj）的乘积表示。</li>
<li><strong>增益参数化（C-GMS核心）</strong>：为将Lyapunov稳定性条件（公式5）结构性嵌入，引入了矩阵松弛变量𝐒_D(𝑡)和𝐒_K(𝑡)，并令：<br>α𝐇 − 𝐃(𝑡) = −𝐒_D(𝑡)𝐒_D^⊤(𝑡) ≼ 0<br>𝐊̇(𝑡) + α𝐃̇(𝑡) − 2α𝐊(𝑡) = −𝐒_K(𝑡)𝐒_K^⊤(𝑡) ≼ 0<br>松弛变量同样用归一化RBF参数化：vec_△(𝐒_D(𝑡)) = Φ_D(s_t)(𝜽_D + 𝝃_D)， vec_△(𝐒_K(𝑡)) = Φ_K(s_t)(𝜽_K + 𝝃_K)。通过这种参数化，𝐃(𝑡)可直接从𝐒_D(𝑡)计算。为保证𝐊(𝑡)始终对称正定，对其Cholesky因子𝐐(𝑡)进行微分演化（公式10），从而间接得到𝐊(𝑡)。此构造确保任何由参数(𝜽, 𝝃)生成的增益时间表都自动满足稳定性条件。</li>
<li><strong>执行器限制感知的增益收缩</strong>：为进一步保证执行器可行性，引入了一个收缩因子β ∈ [0,1]。将松弛变量缩放为√β 𝐒_(⋅)(𝑡)，由此得到的缩放后增益𝐃^β(𝑡)和𝐊^β(𝑡)依然满足Lyapunov条件。控制指令是β的仿射函数：𝝉(β) = 𝝉_0 + β𝝉_1。通过求解在扭矩限幅𝝉_min ≤ 𝝉(β) ≤ 𝝉_max下的最大可行β^⋆，并采用缩放后的增益执行控制，可在不破坏稳定性的前提下满足执行器约束。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1）将点对点的Lyapunov稳定性不等式转化为可通过松弛变量结构性满足的参数化形式，将策略搜索限制在认证的稳定流形上；2）提出了保持认证的增益收缩机制，统一处理稳定性和执行器限幅。</p>
<p><img src="https://arxiv.org/html/2511.16330v1/Images/safe_combined.png" alt="增益与轨迹演变"></p>
<blockquote>
<p><strong>图5</strong>：完整C-GMS下，经过多次策略更新后的VIC增益计划和末端执行器轨迹。增益在航点附近（圆圈标记）自适应调整，轨迹平滑且稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.16330v1/Images/unsafe_combined.png" alt="不稳定增益演变"></p>
<blockquote>
<p><strong>图6</strong>：当C-GMS仅在航点前启用时，之后的增益和轨迹行为。航点后稳定性条件被违反，导致增益无界增长和轨迹振荡。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在一个7自由度Franka Research 3机器人上进行人机协作递送任务的仿真与实物验证。任务要求机器人将一个物体从起始点递送到人手目标点，并在路径中引入一个障碍物，机器人需学习通过一个航点绕开障碍物。</p>
<p><strong>Baseline对比</strong>：实验主要对比了三种设置：1) 完整的C-GMS（全程保证稳定性）；2) 部分C-GMS（仅在学习到航点前施加稳定性约束，之后关闭）；3) 无C-GMS的标准PI²学习。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>任务性能与稳定性</strong>：完整C-GMS在50次策略更新后，任务成本（结合航点跟踪误差、刚度正则和加速度正则）从约1.2降至0.4左右，成功学习到平滑、避障的轨迹。同时，其合成的增益始终满足稳定性条件（公式5中矩阵的最小特征值始终≤0）。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16330v1/Images/cost_vs_iteration.png" alt="成本下降曲线"></p>
<blockquote>
<p><strong>图7</strong>：完整C-GMS下，策略优化过程中任务成本随迭代次数的下降曲线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.16330v1/Images/eq20_min_eig.png" alt="稳定性条件满足情况"></p>
<blockquote>
<p><strong>图8</strong>：完整C-GMS下，不同迭代次数对应的稳定性条件（公式5第二个不等式左侧矩阵）的最小特征值。所有值均为非正，表明条件始终满足。</p>
</blockquote>
<ol start="2">
<li><strong>不稳定行为验证</strong>：当仅使用部分C-GMS时，虽然在航点前能优化成本，但在约束禁用后，系统出现不稳定：增益无界增长，轨迹发生振荡，且稳定性条件被严重违反（最小特征值变为很大的正值）。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16330v1/Images/unsafe_cost_vs_iteration.png" alt="不稳定成本曲线"></p>
<blockquote>
<p><strong>图9</strong>：部分C-GMS（航点后无约束）下的成本曲线。虽然成本下降，但导致了不稳定行为。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.16330v1/Images/unsafe_eq20_min_eig.png" alt="不稳定特征值"></p>
<blockquote>
<p><strong>图10</strong>：部分C-GMS下，稳定性条件的最小特征值。在约束禁用后（约6秒后），特征值变为正且很大，表明不稳定性。</p>
</blockquote>
<ol start="3">
<li><strong>实物验证</strong>：将完整C-GMS学习50次后得到的策略部署到真实Franka机器人上，成功执行了安全的避障递送任务，验证了方法的实用性。</li>
</ol>
<p><strong>消融实验总结</strong>：实验通过对比完整C-GMS与部分/无C-GMS，清晰展示了稳定性认证模块的核心贡献：它不仅是安全的必要条件，而且通过与策略优化的集成，能够引导学习过程找到同时满足任务最优和稳定安全的策略。增益收缩机制则保证了理论上的稳定性在实际执行时不会因扭矩饱和而被破坏。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了C-GMS框架，首次在无模型RL策略搜索中，通过将Lyapunov稳定性条件编码为可学习的松弛变量参数化，实现了对变阻抗控制策略“按构造认证”的稳定性与执行器可行性保证。</li>
<li>提供了理论保证，证明在存在有界模型误差和部署不确定性的情况下，该方法能确保跟踪误差的最终一致有界性。</li>
<li>在仿真和实物机器人的人机协作任务中验证了框架的有效性，展示了其学习安全、最优且适应性强的新技能的能力。</li>
</ol>
<p><strong>局限性</strong>：论文提到的方法基于理想的动态解耦（操作空间逆动力学）和常数期望惯性矩阵𝐇。此外，增益参数化依赖于基函数（如RBF）的选择，这可能影响策略的表达能力。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li>“按构造认证”的思想可扩展到其他类型的安全约束（如接触力约束、关节限位），为更广泛的安全RL提供思路。</li>
<li>需要研究如何将方法扩展到更复杂的接触动力学场景，以及如何处理非结构化环境中的感知不确定性（如航点的自动获取）。</li>
<li>探索更具表达力的增益参数化方式，同时保持认证的便利性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对强化学习在可变阻抗控制中因阻抗增益时变导致的不稳定和不安全探索问题，提出了Certified Gaussian-Manifold Sampling（C-GMS）方法。该方法将策略探索定义为从数学定义的稳定增益调度流形中采样，确保每个策略 rollout 都满足Lyapunov稳定性和执行器可行性，无需奖励惩罚或事后验证。理论保证即使在有界模型误差和部署不确定性下也能实现有界跟踪误差，仿真和真实机器人实验验证了其有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16330" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>