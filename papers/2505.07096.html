<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.07096" target="_blank" rel="noreferrer">2505.07096</a></span>
        <span>作者: Dan, Prithwish, Kedia, Kushal, Chao, Angela, Duan, Edward Weiyi, Pace, Maximus Adrian, Ma, Wei-Chiu, Choudhury, Sanjiban</span>
        <span>日期: 2025/05/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从人类视频中学习机器人操作策略是一个有前景的方向，因为人类视频（如来自YouTube）数量庞大且覆盖了广泛的任务和环境。然而，主流的模仿学习方法（如行为克隆BC、Diffusion Policy、ACT）需要由图像-动作对构成的专家演示，这通常需要通过特定机器人的遥操作（如动觉示教、可穿戴设备）来收集数据，过程昂贵且难以扩展。现有的跨形态学习方法试图弥合这一差距，例如将人手运动重定向到机器人末端执行器，或通过图像修复在人类视频上叠加机器人手臂，但这些方法通常假设视觉或运动学兼容性，当形态差异显著时往往失败。其他方法直接从人类视频翻译出机器人动作，但需要成对的人-机器人演示数据，同样难以大规模获取。</p>
<p>本文针对的核心痛点是：如何从无动作标签的人类视频中生成可用于训练机器人策略的数据。其核心洞察是，虽然人类动作不可用，但其产生的物体运动提供了一个密集且可迁移的监督信号。本文提出X-Sim框架，其核心思路是：通过重建人类视频的光照级真实模拟环境并跟踪物体轨迹来定义物体中心奖励，在模拟中使用强化学习训练策略，然后通过合成数据将策略蒸馏为图像条件策略，并利用在线域适应技术实现向真实世界的迁移，整个过程无需任何机器人遥操作数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>X-Sim是一个三阶段的真实-模拟-真实框架，旨在从无动作的RGBD人类视频中学习真实世界的图像条件机器人策略。</p>
<p><img src="https://arxiv.org/html/2505.07096v5/images/introduction.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：X-Sim整体框架概览。包含三个阶段：1) 真实到模拟：从人类视频生成光照级真实模拟，并利用物体运动定义奖励；2) 训练X-Sim：首先使用特权状态训练RL策略，然后通过环境随机化收集合成图像-动作数据集，并蒸馏为图像条件策略；3) 模拟到真实：在真实世界部署图像策略，其观察编码器通过在线校准对齐真实和模拟图像观察。</p>
</blockquote>
<p><strong>第一阶段：从人类视频到模拟环境 (Real-to-Sim)</strong><br>此阶段的目标是将人类视频转化为一个可用于策略学习的模拟环境。首先，使用现成的3D扫描应用获取被操作物体的高保真网格。然后，利用FoundationPose工具，结合物体网格和由SAM生成的物体初始帧掩码，在整个视频序列中跟踪每个物体的6D姿态（位置和旋转），从而将人类视频 <code>v_H</code> 转换为物体状态序列 <code>s_H</code>。接着，使用2D高斯泼溅技术从多视角图像重建出几何精确且光照级真实的环境网格，并将其与物体状态一同导入ManiSkill模拟器中。物理属性和动力学参数设置为默认值，但方法兼容系统辨识和域随机化等技术。</p>
<p><img src="https://arxiv.org/html/2505.07096v5/images/approach.png" alt="真实到模拟流程"></p>
<blockquote>
<p><strong>图2</strong>：真实到模拟阶段示意图。从多视角图像重建光照级真实环境，并从RGBD人类视频中跨时间跟踪物体运动，以定义密集的物体中心奖励函数，用于在模拟中训练RL策略。</p>
</blockquote>
<p><strong>第二阶段：在模拟中生成机器人动作</strong><br>为了跨越形态差距并获得完成人类视频指定任务的机器人动作，本阶段定义物体中心奖励函数，通过强化学习训练一个基于特权状态（包括当前物体状态、目标状态和机器人本体感知）的策略。奖励函数 <code>r_obj</code> 由促使机器人末端执行器接近相关物体的 <code>r_approach</code> 和促使物体位姿匹配目标位姿的 <code>r_goal</code> 构成。<code>r_goal</code> 与物体当前位置/旋转到目标位置/旋转的负距离成正比。目标状态 <code>s_H^B</code> 根据人类轨迹在线更新，支持多步物体操作。</p>
<p>使用PPO算法优化奖励 <code>r_obj</code> 来训练RL策略 <code>π_RL(a|s)</code>。为了学习鲁棒行为，在RL训练期间对物体的起始位姿进行随机化。训练完成后，在模拟器中执行该策略以生成合成数据，仅保留成功的轨迹。通过在不同执行中随机化物体起始位置、相机视角和光照条件，系统地改变模拟条件，从而收集到一个多样化的合成图像-动作数据集 <code>D_synthetic</code>，其中每个轨迹 <code>ξ_R</code> 包含RGB图像 <code>o_R^t</code> 和动作 <code>a_R^t</code> 对。</p>
<p><strong>第三阶段：图像策略的模拟到真实迁移</strong><br>本阶段将机器人行为蒸馏到基于合成图像-动作对训练的图像条件策略中。采用最先进的Diffusion Policy架构来训练策略 <code>π_img(a|o)</code>，该策略以当前RGB观察 <code>o_R^t</code> 为输入，预测一个动作序列以完成任务。</p>
<p><img src="https://arxiv.org/html/2505.07096v5/images/approach_sim.png" alt="模拟到真实与校准"></p>
<blockquote>
<p><strong>图3</strong>：（左）X-Sim通过使用多种环境随机化生成合成图像-动作数据集，将特权状态策略蒸馏为图像条件策略。（右）部署期间，真实策略执行轨迹在模拟中被重放，以生成跨真实和模拟的配对图像。利用它们的差异来最小化和校准模拟到真实的视觉差距。</p>
</blockquote>
<p>为了改善真实世界迁移性能，X-Sim引入了在线域适应技术。在真实世界部署初始策略后，收集机器人执行轨迹（包括失败轨迹）中的图像观察。然后，在模拟中以相同的初始状态（使用第一帧的FoundationPose结果）重放完全相同的机器人轨迹，从而创建一个真实和模拟图像的配对数据集 <code>D_paired</code>。训练时，在 <code>D_synthetic</code> 上使用标准的行为克隆损失，并在 <code>D_paired</code> 上施加一个额外的对比InfoNCE损失 <code>L_calibration</code>。该损失拉近对应模拟和真实图像嵌入的距离，同时推远不匹配的图像嵌入，引导编码器关注任务相关语义，减少对模拟特定特征的过拟合。</p>
<p>与现有方法相比，X-Sim的创新点在于：1) 完全绕过了对机器人遥操作数据和人手轨迹跟踪的需求，仅利用物体运动作为监督；2) 通过光照级真实模拟和合成数据生成，实现了从模拟到真实世界的图像策略直接迁移；3) 提出了利用真实世界闭环执行（含失败）进行在线校准的技术，持续减小域差距。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：所有实验使用一个7自由度Franka机械臂，在两个真实环境（厨房和桌面）中进行。使用ZED 2立体相机录制RGBD人类视频，涵盖5个操作任务：抓放（芥末瓶放置、玉米放入篮子、鞋子放上架子）、非抓握操作（信件排列）和精确插入（杯子插入）。评估指标是平均任务进度，它为任务完成的不同阶段（如接近、抓取、放置）分配部分分数。</p>
<p><strong>对比基线</strong>：</p>
<ol>
<li><strong>手部掩码</strong>：在人类视频中掩盖手部区域以训练图像条件BC策略，假设所有人类手部姿态均可被机器人复制。</li>
<li><strong>物体感知逆运动学</strong>：提取相对于附近物体的人手轨迹，并通过IK使机器人末端执行器重放相同路径。</li>
<li><strong>Human2Sim2Robot</strong>：一个使用精确6D物体姿态进行模拟训练和真实世界部署的基线。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07096v5/images/barplots.png" alt="任务性能对比"></p>
<blockquote>
<p><strong>图4</strong>：在两个环境共5个任务上的平均任务进度。X-Sim（无论是否校准） consistently outperforms 手部重定向基线。每个任务的草图可视化在顶部。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>跨越形态差距</strong>：如图4所示，手部掩码方法由于人类和机器人观察之间存在巨大视觉差距而失败，很少能超越接近阶段。物体感知IK在厨房任务（执行风格相似）中表现良好，但在桌面任务中因运动学不可行和动态不匹配而失败。相比之下，即使未经校准的X-Sim也能在模拟中学习可行策略并有效迁移，在形态最不匹配的场景中平均任务进度提升了30%以上。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07096v5/images/hand_retargeting_failures.png" alt="手部重定向失败模式"></p>
<blockquote>
<p><strong>图5</strong>：手部重定向失败模式。手部掩码因人类和机器人之间存在显著的视觉域差距而失败。物体感知IK则因某些人类手部运动在运动学或动力学上不可行而失败。</p>
</blockquote>
<ol start="2">
<li><strong>模拟到真实策略迁移</strong>：如表6所示，在信件排列任务上，X-Sim（使用RGB图像）的平均任务进度达到83.3%，显著优于使用物体状态观察的Human2Sim2Robot基线（43.3%），证明了图像作为观察模态的鲁棒性和实用性。</li>
<li><strong>部署后校准的效果</strong>：如图4所示，经过在线校准的X-Sim在所有任务上平均带来了额外的8%任务进度提升，在最具挑战性的杯子插入任务上提升了13%。这表明校准能从失败中学习并改善性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07096v5/images/calibration.png" alt="校准效果分析"></p>
<blockquote>
<p><strong>图7</strong>：使用t-SNE比较校准前后X-Sim图像嵌入在一个执行轨迹上的变化。校准后，真实和模拟图像的嵌入对齐得更好。</p>
</blockquote>
<ol start="4">
<li><strong>数据效率</strong>：在扩展初始状态分布的芥末瓶放置任务中（图8），X-Sim仅用1分钟的人类视频数据就达到了90%的成功率，而基于机器人遥操作的行为克隆需要10分钟的数据才能达到70%的成功率。这体现了X-Sim利用人类视频（采集更快）并通过模拟扰动扩展覆盖范围的高效性和可扩展性。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07096v5/images/scaling.png" alt="数据效率对比"></p>
<blockquote>
<p><strong>图8</strong>：数据效率对比。在扩展初始状态分布的芥末瓶放置任务中，X-Sim比从机器人遥操作进行行为克隆的数据扩展效率更高，仅用10倍少的时间就达到了可比的成功率。</p>
</blockquote>
<ol start="5">
<li><strong>对测试时变化的鲁棒性</strong>：通过从多个视角（侧视和正视）收集模拟执行数据，X-Sim训练的策略能够泛化到新的相机视角。如图9所示，在鞋子放上架子任务中，结合多视角数据训练的策略，在测试时面对新视角达到了53.5%的成功率，显著优于仅用单一视角数据训练的策略。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07096v5/images/viewpoints-viz.jpeg" alt="视角泛化"></p>
<blockquote>
<p><strong>图9</strong>：通过在模拟中从多个视角（侧视和正视）灵活收集图像-动作数据，X-Sim训练的策略能够泛化到新视角。</p>
</blockquote>
<p><strong>消融实验总结</strong>：在线校准模块的贡献得到了验证，它通过对比损失对齐域间特征，带来了平均8%的性能提升，特别是在困难任务上效果显著。实验也证明了从多视角生成合成数据对于提升策略视角泛化能力的重要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了X-Sim这一无需机器人遥操作数据、从无动作人类视频学习图像条件机器人策略的真实-模拟-真实框架，其关键在于利用物体运动作为可迁移的监督信号；2) 引入了一种在线域适应技术，利用真实世界闭环执行（含失败）自动对齐模拟和真实观察，持续减小模拟到真实差距；3) 通过广泛的实验验证了X-Sim在跨越形态差距、数据效率、模拟到真实迁移鲁棒性以及对测试时变化（如新视角）的泛化能力方面的优势。</p>
<p>论文自身提到的局限性主要在于当前流程对结构化视频的假设：需要已知物体的3D网格进行跟踪，并且视频需满足多视角重建的要求，这限制了其直接应用于互联网上的“野外”视频。此外，方法依赖于相对准确的3D重建和物理模拟。</p>
<p>本文的启示在于，它为利用海量人类视频数据训练机器人策略提供了一条切实可行的路径，绕过了昂贵的机器人数据收集瓶颈。未来，随着3D重建、物体姿态估计和物理模拟技术的进步，X-Sim的假设有望逐步放宽，使其能够处理更广泛、更“野外”的人类视频数据。同时，该框架也可作为微调预训练机器人基础模型的有效工具，通过生成针对特定人类演示的合成数据，低成本地使通用策略适应新任务和形态。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出X-Sim框架，解决从无动作标签的人类视频中学习机器人操作策略的跨具身学习难题。其核心是“真实-仿真-真实”流程：从RGBD视频重建逼真仿真并追踪物体轨迹，以物体运动定义奖励函数，在仿真中训练强化学习策略，再通过多视角合成数据将策略蒸馏为图像条件扩散策略，最后通过在线域适应实现真实世界迁移。实验表明，该方法在5个任务上平均任务进度比基准方法提升30%，数据收集效率比行为克隆高10倍，并能泛化到新视角与环境变化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.07096" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>