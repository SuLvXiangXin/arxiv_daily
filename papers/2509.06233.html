<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>O $^3$ Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>O $^3$ Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.06233" target="_blank" rel="noreferrer">2509.06233</a></span>
        <span>作者: Yen-Ling Kuo Team</span>
        <span>日期: 2025-09-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，可供性（Affordance）预测的主流方法集中于2D像素空间或单物体可供性学习。然而，现实世界中大多数日常任务涉及物体对之间的交互关系，现有方法忽略了这一关键点。尽管O2O-Afford通过模拟自动提取接触点的方式解决了标注数据稀缺的问题，但其仅适用于放置、拟合等简单可供性，难以处理如倾倒、切割等对机器人操作至关重要的复杂交互。</p>
<p>本文针对在极有限数据约束下学习物体对物体（Object-to-Object）3D可供性这一具体痛点，提出了新视角：借鉴视觉基础模型（VFMs）在2D领域的少样本泛化能力，将其语义特征与点云的几何信息相结合，以实现仅需单样本即可泛化到新物体及新类别的3D可供性学习。本文核心思路是：从多视角RGB-D观测中提取DINOv2特征并投影到物体点云上，构建语义点云；通过一个双向注意力模块解码物体间的交互关系，预测3D可供性图；最后，利用大语言模型（LLM）根据预测的可供性生成任务特定的约束函数，指导基于优化的机器人操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>O3Afford的整体框架包含三个核心组件：1）语义点云构建；2）单样本可供性学习模块；3）基于可供性与LLM的机器人操作规划。输入为源物体和目标物体的点云，输出为各自的3D可供性概率图以及用于机器人操作的优化后物体位姿。</p>
<p><img src="https://arxiv.org/html/2509.06233v1/x1.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：O3Afford框架总览。首先（a）从多视角RGB-D图像提取DINOv2特征并投影，构建语义点云；（b）使用联合交叉注意力解码器处理源物体和目标物体的语义点云，预测可供性图；（c）利用LLM根据任务描述和可供性图生成约束函数，并通过优化求解目标位姿，最终执行机器人操作。</p>
</blockquote>
<p><strong>核心模块1：语义点云构建</strong>。该方法借鉴D3Field，将多视角RGB-D图像输入DINOv2模型提取2D语义特征。对于3D点云中的每个点，计算其在各相机视图下的投影坐标，并根据其与物体表面的深度差异和可见性分配权重，最终通过加权融合来自多个视图的特征，将1024维的DINOv2语义特征编码到每个3D点上，形成富含部件级语义信息的点云表示。</p>
<p><strong>核心模块2：单样本可供性学习网络</strong>。网络包含点云编码器和联合注意力Transformer解码器。</p>
<ul>
<li><strong>点云编码器</strong>：将源物体与目标物体的点云及其DINOv2特征拼接，通过最远点采样（FPS）选取T个块中心，并为每个中心构建k近邻块。使用PointNet聚合每个块的局部几何与语义信息，生成块级特征令牌Z ∈ ℝ^(2B×T×512)。同时，为令牌添加一维独热编码以区分源物体和目标物体。</li>
<li><strong>联合注意力Transformer解码器</strong>：这是实现物体间交互理解的核心。采用双向交叉注意力机制，使源物体令牌与目标物体令牌能动态交互：A_src = CrossAttention(Z_src, Z_tgt, Z_tgt)，A_tgt = CrossAttention(Z_tgt, Z_src, Z_src)。这种设计允许模型同时考虑两个物体的几何与语义上下文，编码互补的可供性关系。之后，通过最近邻插值将块级特征还原为稠密的点级特征，再经过一个轻量级MLP投影头，输出每个点的可供性概率。网络使用二元交叉熵（BCE）损失进行训练。</li>
</ul>
<p><strong>核心模块3：基于可供性与LLM的操作规划</strong>。将机器人操作规划建模为一个约束优化问题：给定预测的可供性图，优化一个施加于源物体的6自由度变换T，使其与目标物体以满足任务要求的方式对齐。创新性地利用LLM（如GPT-4o）根据高层任务描述（如“倾倒”）自动生成具体的约束函数S_i（例如，接触点对齐、避免碰撞等），这些函数对物体点云和可供性图进行计算，输出评分。最终通过最小化加权约束评分和来求解最优位姿T。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>数据效率与泛化性</strong>：首次将视觉基础模型的语义先验与3D点云几何结合，实现仅需单样本的物体对物体可供性学习，并能泛化至新实例和新类别；2) <strong>规划集成创新</strong>：提出使用LLM将高层任务语义自动转化为基于可供性图的几何约束，实现了感知（可供性）与行动（优化规划）的灵活、可解释衔接。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟器SAPIEN中构建了物体对物体可供性数据集，包含倾倒、插入、按压、悬挂、切割五个任务类别，每个类别仅有一个物体对作为训练样本。机器人操作实验在SAPIEN模拟环境和真实世界的Franka Research 3机器人上进行。评估了与O2O-Afford、IAGNet（2D图像方法）、RoboPoint（VLM方法）在可供性预测上的性能，以及与ReKep（关键点优化方法）、无可供性的基线方法在机器人操作任务上的成功率。</p>
<p><strong>可供性预测结果</strong>：<br><img src="https://arxiv.org/html/2509.06233v1/x2.png" alt="定性可供性预测结果"></p>
<blockquote>
<p><strong>图2</strong>：O3Afford在不同任务上的定性可供性预测结果。每两行展示一个任务，上行是源物体（如茶壶），下行是目标物体（如碗），颜色越亮表示可供性概率越高。模型能准确预测出功能交互区域。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">↑ IOU</th>
<th align="left">↑ SIM</th>
<th align="left">↓ MAE</th>
<th align="left">↑ AUC</th>
</tr>
</thead>
<tbody><tr>
<td align="left">O2O-Afford</td>
<td align="left">14.31</td>
<td align="left">0.5123</td>
<td align="left">0.1219</td>
<td align="left">74.29</td>
</tr>
<tr>
<td align="left">IAGNet</td>
<td align="left">16.89</td>
<td align="left">0.5574</td>
<td align="left">0.1402</td>
<td align="left">73.30</td>
</tr>
<tr>
<td align="left">RoboPoint</td>
<td align="left">11.84</td>
<td align="left">0.4376</td>
<td align="left">0.3344</td>
<td align="left">59.78</td>
</tr>
<tr>
<td align="left">Ours</td>
<td align="left"><strong>26.19</strong></td>
<td align="left"><strong>0.6387</strong></td>
<td align="left"><strong>0.0612</strong></td>
<td align="left"><strong>96.00</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：物体对物体可供性预测的定量比较。O3Afford在aIOU（26.19）、相似度（0.6387）、MAE（0.0612）和AUC（96.00）所有指标上均显著优于基线。</p>
</blockquote>
<p><strong>泛化能力分析</strong>：<br><img src="https://arxiv.org/html/2509.06233v1/x3.png" alt="遮挡泛化分析"></p>
<blockquote>
<p><strong>图3</strong>：在不同遮挡水平下的性能变化。随着被其他物体遮挡的比例增加（10%至50%），O3Afford的性能下降幅度最小，展现出对遮挡的鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.06233v1/x4.png" alt="跨类别泛化"></p>
<blockquote>
<p><strong>图4</strong>：对未见过物体类别的可供性预测定性结果。例如，训练时使用“刀切苹果”，测试时能对“剪刀剪纸”、“挂钩挂帽子”、“喷壶浇水”等语义功能相似但类别全新的物体做出合理预测。</p>
</blockquote>
<p><strong>机器人操作结果</strong>：</p>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">Pour (2/3/4-view)</th>
<th align="left">Hang (2/3/4-view)</th>
<th align="left">Press (2/3/4-view)</th>
<th align="left">Insert (2/3/4-view)</th>
<th align="left">Cut (2/3/4-view)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Baseline</td>
<td align="left">0/2/2</td>
<td align="left">0/1/0</td>
<td align="left">4/5/5</td>
<td align="left">1/2/5</td>
<td align="left">3/5/4</td>
</tr>
<tr>
<td align="left">Rekep</td>
<td align="left">2/3/3</td>
<td align="left">1/0/2</td>
<td align="left">4/4/6</td>
<td align="left">3/2/3</td>
<td align="left">4/4/4</td>
</tr>
<tr>
<td align="left">Ours</td>
<td align="left"><strong>6/8/8</strong></td>
<td align="left"><strong>3/3/5</strong></td>
<td align="left"><strong>9/9/9</strong></td>
<td align="left"><strong>5/7/8</strong></td>
<td align="left"><strong>8/7/8</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表2</strong>：模拟环境中机器人操作任务的成功率（10次尝试中的成功次数）。O3Afford在不同观测视角数（2,3,4个相机）下均取得最高成功率，且在视角减少（遮挡增加）时性能下降最小。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">Pour</th>
<th align="left">Hang</th>
<th align="left">Press</th>
<th align="left">Insert</th>
<th align="left">Cut</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Baseline</td>
<td align="left">2/10</td>
<td align="left">0/10</td>
<td align="left">2/10</td>
<td align="left">3/10</td>
<td align="left">3/10</td>
</tr>
<tr>
<td align="left">ReKep</td>
<td align="left">3/10</td>
<td align="left">2/10</td>
<td align="left">5/10</td>
<td align="left">4/10</td>
<td align="left">5/10</td>
</tr>
<tr>
<td align="left">Ours</td>
<td align="left"><strong>8/10</strong></td>
<td align="left"><strong>5/10</strong></td>
<td align="left"><strong>9/10</strong></td>
<td align="left"><strong>8/10</strong></td>
<td align="left"><strong>9/10</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表3</strong>：真实世界机器人操作任务的成功率。O3Afford在各项任务上均大幅领先，平均成功率约80%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.06233v1/x5.png" alt="真实世界操作示例"></p>
<blockquote>
<p><strong>图5</strong>：真实世界机器人操作任务的执行示例。点云上的颜色表示预测的可供性值。结果表明，基于可供性生成的约束能引导机器人完成合理的交互序列。</p>
</blockquote>
<p><strong>消融实验总结</strong>：表2中的“Baseline”可视为消融实验，即移除了可供性预测模块，直接使用原始点云进行规划。其成功率显著低于完整方法，证明了可供性作为中级表征对于理解物体间功能关系、提升操作成功率的关键贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了O3Afford，首个利用视觉基础模型实现单样本3D物体对物体可供性学习的方法，在数据稀缺条件下实现了优异的类别内和跨类别泛化；2) 创新地将可供性表征与LLM相结合，通过LLM自动生成基于可供性的几何约束，实现了高层任务语义到底层优化规划的可解释衔接；3) 在模拟和真实世界的复杂机器人操作任务上验证了方法的有效性，显著超越了现有基线。</p>
<p>论文自身提到的局限性包括：1) 当前系统未引入语言指令直接引导可供性预测，限制了其遵循具体人类指示的能力；2) 物体自遮挡可能导致3D模型不完整，进而影响对隐藏部分交互点的预测；3) 实际性能受限于当前RGB-D传感器的噪声和精度，在无纹理、反光表面或杂乱场景下，输入点云质量下降会影响最终预测的可靠性。</p>
<p>对后续研究的启示：首先，将语言指令集成到可供性预测流程中，实现开放词汇的可供性学习，是一个直接且有价值的扩展方向。其次，如何更鲁棒地处理严重遮挡（包括自遮挡）情况下的3D感知与推理，是提升方法实用性的关键。最后，探索对传感器噪声更不敏感的3D表征学习方法，或结合主动感知策略，有助于在非理想感知条件下维持系统性能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出O³Afford方法，解决机器人操作中物体间功能关系（affordance）的定位问题，核心挑战在于标注数据稀缺。方法采用单样本学习框架，结合视觉基础模型的语义特征与点云几何表征，实现对未见物体和类别的有效泛化，并集成大语言模型增强对物体交互的任务推理能力。实验表明，该方法在3D物体间功能关系定位与机器人操作任务上，准确性与泛化能力显著优于现有基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.06233" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>