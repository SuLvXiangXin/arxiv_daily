<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GR-Dexter Technical Report - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GR-Dexter Technical Report</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.24210" target="_blank" rel="noreferrer">2512.24210</a></span>
        <span>作者: Hang Li Team</span>
        <span>日期: 2025-12-30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉-语言-动作（VLA）模型的通用操作策略已能实现语言条件控制和长时序指令跟随。然而，现有策略大多部署在配备夹爪末端执行器的双手机器人上。将此类能力扩展到配备高自由度（DoF）灵巧手的机器人面临三大挑战：动作空间因数十个自由度而急剧扩大；手指之间以及手与目标物体之间频繁的遮挡导致感知困难；作为一个数据驱动范式，VLA模型的性能严重依赖于高质量、多样化的灵巧双手操作轨迹数据，而此类数据的收集成本高昂。本文针对这些痛点，提出了一个集硬件、模型和数据于一体的整体框架GR-Dexter，旨在将基于VLA的通用操作扩展到配备高自由度灵巧手的双手机器人。其核心思路是：设计紧凑的21-DoF灵巧手与直观的遥操作系统以收集真实机器人数据，并利用包含遥操作轨迹、大规模视觉-语言数据、跨本体数据及人类轨迹在内的混合数据源进行协同训练，以提升策略的域内性能和对外部场景的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>GR-Dexter框架包含三个核心组成部分：ByteDexter V2灵巧手硬件、双手机器人系统与数据收集管道，以及GR-Dexter VLA模型及其训练方法。</p>
<p><img src="https://arxiv.org/html/2512.24210v2/images/teaser_v4.png" alt="整体框架与数据来源"></p>
<blockquote>
<p><strong>图1</strong>：GR-Dexter整体框架。它通过融合四种数据源（视觉-语言数据、跨本体数据、人类轨迹、机器人轨迹）来执行灵巧的长时序日常任务，并泛化到域外场景。</p>
</blockquote>
<p><strong>硬件平台：ByteDexter V2灵巧手</strong><br>作为ByteDexter V1的升级版，V2手采用连杆驱动传动机制，在保持力透明度、耐用性和易维护性优势的同时，增加了1个拇指自由度，总DoF达到21个，并进一步缩小了尺寸（高219mm，宽108mm）。其设计细节包括：四个手指各4个DoF（MCP为万向节，PIP和DIP为旋转关节），拇指5个DoF以提供更广泛的对掌运动；DIP关节和拇指IP关节采用仿生四连杆机构实现欠驱动；五个指尖覆盖了高密度压阻式触觉传感器阵列，可测量法向接触力。</p>
<p><img src="https://arxiv.org/html/2512.24210v2/x2.png" alt="ByteDexter V2手设计"></p>
<blockquote>
<p><strong>图3</strong>：ByteDexter V2手。展示了自由度分布、触觉指尖以及拇指的对掌能力（在Kapandji测试中得分为10分）。</p>
</blockquote>
<p><strong>双手机器人系统与数据收集</strong><br>机器人平台由两个Franka Research 3机械臂各搭载一只ByteDexter V2手构成，总计56个DoF。为缓解遮挡并从多视角捕捉手-物交互，系统部署了四个全局RGB-D相机。数据收集通过一个双手机器人遥操作接口实现，该接口集成了Meta Quest VR头显（跟踪手腕位姿）、Manus数据手套（捕捉手部运动）和脚踏板。通过一个结合手腕到指尖、拇指到指尖对齐项以及碰撞避免约束的优化问题，将人体运动实时重定向为关节位置指令。</p>
<p><img src="https://arxiv.org/html/2512.24210v2/x3.png" alt="双手机器人系统"></p>
<blockquote>
<p><strong>图4</strong>：双手机器人系统，包含两个配备ByteDexter V2手的Franka Research 3机械臂。数据通过VR头显、数据手套和全局RGB-D相机组成的遥操作接口收集。</p>
</blockquote>
<p><strong>GR-Dexter VLA模型与训练方法</strong><br>模型架构遵循GR-3，采用混合Transformer架构，参数量为4B。策略πθ接收语言指令l、观测ot和机器人状态st，输出长度为k的动作块at。每个动作at是一个88维向量，包含：双臂关节动作（每臂7DoF）、双臂末端执行器位姿（每臂6D）、双手关节动作（每手16个主动DoF）以及指尖位置（每指3D）。</p>
<p>训练的核心创新在于利用“数据金字塔”进行协同训练。</p>
<p><img src="https://arxiv.org/html/2512.24210v2/images/data_pyramid_v4.png" alt="数据金字塔"></p>
<blockquote>
<p><strong>图6</strong>：GR-Dexter的数据金字塔。使用三种不同的数据源进行训练：网络规模的视觉-语言数据、跨本体的真实机器人数据以及人类轨迹数据。</p>
</blockquote>
<ol>
<li><strong>视觉-语言数据</strong>：复用GR-3的数据集，包含图像描述、视觉问答等任务。仅用于训练VLM骨干网络（下一个token预测目标），与机器人轨迹数据在mini-batch中动态混合，总损失为下一个token预测损失和流匹配损失之和。</li>
<li><strong>跨本体数据</strong>：为缓解自身平台数据规模限制，引入了三个开源双手机器人操作数据集：Fourier ActionNet Dataset、OpenLoong Baihu Dataset和RoboMIND。通过统一预处理和重定向流程，将所有数据源的视觉几何、运动学和轨迹质量对齐。关键步骤包括标准化相机观测、严格的质量控制，以及通过指尖对齐将轨迹重定向到ByteDexter V2手。</li>
<li><strong>人类轨迹数据</strong>：利用大规模人类自我中心视频及手部追踪数据（超过800小时），并补充了使用Pico VR设备收集的数据。处理流程包括基于手部可见性和速度的过滤，以及将其映射到与机器人数据相同的视觉和运动学表示中。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实机器人上进行，主要评估长时序灵巧操作和可泛化拾放两类任务。</p>
<p><strong>实验1：长时序灵巧操作</strong><br>任务为化妆整理，涉及对多种形状、尺寸物品及抽屉等铰接物体的长序列操作。收集了约20小时遥操作机器人轨迹。比较了仅用机器人数据训练的plain VLA基线和GR-Dexter（协同训练视觉-语言数据和机器人轨迹）。</p>
<p><img src="https://arxiv.org/html/2512.24210v2/x6.png" alt="长时序操作实验设置与结果"></p>
<blockquote>
<p><strong>图7</strong>：化妆整理任务的实验设置与结果。展示了在基本（分布内）和分布外（OOD，物体空间布局新颖）设置下的成功率。</p>
</blockquote>
<ul>
<li><strong>基本设置（分布内）</strong>：物体空间布局与训练数据相似。Plain VLA成功率为0.96，GR-Dexter为0.97，表明协同训练保留了仅用遥操作数据基线的强大域内能力。</li>
<li><strong>分布外（OOD）设置</strong>：测试时使用五种未见过的物体布局。Plain VLA成功率降至0.64，而GR-Dexter大幅提升至0.89。这表明与视觉-语言数据的协同训练显著增强了对未见空间布局的泛化能力，同时保持了域内性能。</li>
</ul>
<p><strong>实验2：可泛化拾放</strong><br>训练使用了约20小时、涉及20个物体的机器人轨迹。比较了三个模型：plain VLA、GR-Dexter（不带跨本体数据）、GR-Dexter（完整版）。</p>
<p><img src="https://arxiv.org/html/2512.24210v2/x7.png" alt="拾放实验设置与结果"></p>
<blockquote>
<p><strong>图8</strong>：可泛化拾放任务的实验设置与结果。对比了在基本（分布内）、未见物体和未见指令三种设置下的成功率。</p>
</blockquote>
<ul>
<li><strong>基本设置（分布内）</strong>：使用训练中见过的物体。Plain VLA成功率为0.87，GR-Dexter（不带跨本体数据）为0.85，GR-Dexter（完整版）最佳，为0.93。结果表明，在分布内设置下，视觉-语言数据未提供额外信息反而使优化更具挑战性；而加入经过仔细处理和对齐的跨本体数据后，GR-Dexter的鲁棒性和性能得到显著提升。</li>
<li><strong>未见物体</strong>：使用23个未见物体测试。Plain VLA性能显著下降；GR-Dexter（不带跨本体数据）因抓取不准确而表现不佳；GR-Dexter（完整版）展现出强大的泛化能力，成功率达到0.85。</li>
<li><strong>未见指令</strong>：使用未见过的语言指令测试。GR-Dexter（完整版）成功率达到0.83，表明其能够正确解释并执行新指令。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一个从硬件、数据到模型的完整框架，将VLA通用操作成功扩展到56-DoF的双臂灵巧手机器人；2）设计了更紧凑、集成触觉传感的ByteDexter V2灵巧手，并开发了高效的遥操作数据收集管道；3）创新性地采用包含视觉-语言数据、跨本体机器人数据和人类轨迹的混合数据源进行协同训练，在保持域内性能的同时，显著提升了对未见物体、空间布局和语言指令的泛化鲁棒性。</p>
<p>论文自身提到的局限性包括：1）仅利用了数百小时的人类轨迹数据，仍有大量互补的人类自我中心数据未被开发；2）当前手和臂的控制是分离的，这可能阻碍在接触丰富的灵巧行为中紧密的手-臂协调。</p>
<p>这项工作对未来研究的启示在于：结合实用的灵巧硬件、可扩展的数据收集以及跨本体监督，是通向通用灵巧手操作的一条可行路径。未来的工作可以进一步利用更多样化、更易获取的跨本体轨迹来扩大预训练规模，并构建与本体无关的控制抽象以改善协调性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型难以扩展到高自由度双手机器人灵巧手操控的难题，提出了GR-Dexter整体框架。其核心方法包括：设计紧凑的21自由度灵巧手、基于头显与数据手套的直观遥操作数据采集系统，以及融合遥操作机器人轨迹、视觉-语言数据和跨体现数据等多种数据源的协同训练方案。实验表明，该框架在真实世界的长周期日常操作和泛化拾放任务中取得了优异性能，并对未见过的物体与指令展现出更强的鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.24210" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>