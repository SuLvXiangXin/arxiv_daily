<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.06207" target="_blank" rel="noreferrer">2510.06207</a></span>
        <span>作者: Zhaoxiang Zhang Team</span>
        <span>日期: 2025-10-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人控制方法主要分为三类：端到端的视觉-语言-动作模型、基于预定义技能库的模块化系统，以及利用大语言模型进行代码生成的方法。端到端VLA模型依赖大规模标注数据集，泛化能力有限，对环境变化敏感。模块化系统（如DovSG, OK-Robot）虽具可解释性，但其能力受限于预定义的技能库，无法处理如开门、开抽屉等需要复杂接触交互的任务。早期的代码生成方法（如Code-as-Policies, VoxPoser）要么局限于简单几何，要么无法处理接触丰富的操作，或依赖于学习到的模型，限制了在新场景中的适应性。</p>
<p>本文针对上述方法在开放世界移动操作中灵活性不足、依赖数据或预定义技能库的关键痛点，提出了一种新的视角：将现代编码模型作为核心，通过生成代码来桥接感知与操作。其核心思路是：利用编码模型的常识知识和代码生成能力，将高级语言指令直接转化为可执行的机器人轨迹，具体通过代码驱动的几何参数化和轨迹合成来实现，整个过程无需额外训练或微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>EmbodiedCoder的整体框架是一个免训练的三阶段流水线，输入为RGB-D观测和自然语言指令，输出为可执行的机器人动作序列。</p>
<p><img src="https://arxiv.org/html/2510.06207v2/x2.png" alt="系统框架总览"></p>
<blockquote>
<p><strong>图2</strong>：系统整体流程。包含三个主要模块：(i) 场景理解与任务分解：处理RGB-D图像，构建语义地图并将指令分解为子任务；(ii) EmbodiedCoder：提示编码模型进行代码驱动的对象几何参数化和轨迹合成；(iii) 运动执行：从合成轨迹中采样路径点并执行操作。</p>
</blockquote>
<p><strong>1. 场景理解与任务分解</strong><br>该模块负责环境感知和高层规划。首先，使用VGGT从一系列RGB图像重建稠密点云，并与RGB-D相机深度图对齐，获得度量尺度点云。同时，使用视觉语言模型对场景进行语义 grounding，得到物体边界框，再通过SAM生成2D语义掩码。掩码被投影到重建的点云上，形成语义点云地图，并转换为鸟瞰视角的2D语义图像存储，用于后续规划。给定完整任务指令，VLM结合语义地图将指令分解为一系列子任务序列，并为每个子任务识别和 grounding 相关物体，推断其最合适的几何形状及需操作的功能部件。最终，为每个子任务生成仅包含相关物体的语义点云表示。</p>
<p><strong>2. EmbodiedCoder（核心模块）</strong><br>此模块利用预训练的大语言模型，将常识知识转化为可执行代码，分为两个核心阶段和一个优化机制。</p>
<ul>
<li><strong>(a) 代码驱动的几何参数化</strong>：系统将子任务相关的物体点云输入编码模型，提示其生成代码，将点云拟合为在任务分解阶段识别出的几何图元。例如，将门拟合为带有铰链轴的参数化模型，将苹果拟合为球体（中心、半径），将抽屉拟合为长方体（长宽高、质心）并附带一个拉动轴。对于变形物体，则选择极值点构建包围包络。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.06207v2/x3.png" alt="门参数化示例"></p>
<blockquote>
<p><strong>图3</strong>：门的参数化示例。提示中的任务相关信息来自任务分解阶段。此过程将非结构化的点云转换为紧凑的、结构化的几何表示，为轨迹规划提供基础。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.06207v2/x4.png" alt="常见物体参数化结果"></p>
<blockquote>
<p><strong>图4</strong>：常见物体的参数化结果可视化。展示了如门、抽屉、瓶子等物体如何被抽象为几何图元。</p>
</blockquote>
<ul>
<li><strong>(b) 代码驱动的轨迹合成</strong>：在获得物体的几何参数后，系统提示编码模型生成合成轨迹的代码。该过程需综合考虑物理约束（如门绕轴旋转）、环境约束（如推或拉）、机器人硬件约束（关节运动范围）以及任务特定要求（如开门间隙需足够机器人通过）。轨迹被表达为参数化曲线（如直线、圆弧、贝塞尔曲线）。在存在障碍物的环境中，障碍物也被参数化并纳入轨迹规划，以确保避障。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.06207v2/x5.png" alt="带避障的苹果放置示例"></p>
<blockquote>
<p><strong>图5</strong>：带避障的苹果放置任务示例。提示中的任务相关信息通过子任务分解和几何参数化获得。编码模型据此生成轨迹合成代码。</p>
</blockquote>
<ul>
<li><strong>(c) 代码缓存</strong>：为提高效率，系统会对已成功执行过的、涉及熟悉物体类型或重复子任务的代码进行缓存和复用。对于新物体或新任务，则调用EmbodiedCoder生成新代码。随着执行任务增多，系统逐渐积累一个可复用的技能库，在已知场景的效率和开放世界的泛化能力之间取得平衡。</li>
</ul>
<p><strong>3. 运动执行</strong><br>该模块按顺序执行每个子任务。对于每个子任务，EmbodiedCoder生成的轨迹被采样为离散的路径点，机器人依次执行导航和操作动作。</p>
<p><strong>创新点</strong>：与现有方法相比，EmbodiedCoder的创新在于：1) <strong>免训练</strong>：整个框架无需针对具体任务进行数据收集或模型微调；2) <strong>双重代码生成</strong>：不仅生成动作代码，更首创了“几何参数化”这一中间表示，将感知信息转化为利于物理和几何推理的结构化形式；3) <strong>超越预定义技能</strong>：通过代码合成能够动态生成适应新物体、新几何和新约束的复杂操作轨迹，突破了固定技能库的限制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界环境中进行，使用AgileX Cobot S Kit移动操作机器人和RealSense D455 RGB-D相机。场景理解使用Qwen-2.5-VL (7B)、SAM和VGGT。EmbodiedCoder使用Claude-Sonnet-4生成代码。对比的基线方法包括模块化系统DovSG、多种VLA模型（RT-1, RT-2等）以及其他代码生成方法（ReKep, VoxPoser等）。</p>
<p><strong>1. 长时任务评估</strong><br>设计了5个结合导航与接触操作的多步骤任务（如取水瓶倒水、开门取苹果放砧板、避障存苹果等）。每个任务进行20次试验，在缓存（可复用旧代码）和非缓存（必须生成新代码）两种条件下评估。</p>
<p><img src="https://arxiv.org/html/2510.06207v2/x1.png" alt="长时任务成功率对比表"></p>
<blockquote>
<p><strong>表II</strong>：长时任务及其子任务的成功率（20次试验平均）。我们的方法在缓存/非缓存条件下均表现稳定，且能完成DovSG无法处理的开门等操作。但开门子任务成功率相对较低，主要失败原因是靠近门时相机视野受限导致点云不完整，进而影响参数估计。</p>
</blockquote>
<p><strong>关键结果</strong>：EmbodiedCoder在长时任务上取得了可比或优于DovSG的成功率，特别是在需要复杂操作（如开门、开抽屉）的任务上优势明显。缓存与非缓存条件成功率接近，证明了其零样本泛化能力。</p>
<p><strong>2. 简单任务评估</strong><br>在VLA论文常用的单步任务上进行对比。</p>
<p><img src="https://arxiv.org/html/2510.06207v2/x5.png" alt="简单任务上与VLA模型对比"></p>
<blockquote>
<p><strong>表IV</strong>：与VLA模型在简单任务上的定量对比。我们的方法在未经过额外训练的情况下，平均成功率（89.2%）达到或超过了经过训练的VLA模型，尤其在涉及精细控制（如倒水）的任务上优势明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.06207v2/x5.png" alt="简单任务上与其他代码生成方法对比"></p>
<blockquote>
<p><strong>表III</strong>：与其他代码生成方法的对比。我们的方法在倒茶、回收罐头、收纳书本等任务上取得了更高且更稳定的成功率。</p>
</blockquote>
<p><strong>3. 消融与分析实验</strong></p>
<ul>
<li><strong>抓取不同形状物体</strong>：与AnyGrasp对比，EmbodiedCoder在瓶子、苹果、塑料袋等多种物体上的抓取成功率更高，归因于其参数化设计能更好地编码夹具和运动学约束。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.06207v2/x5.png" alt="抓取成功率对比"></p>
<blockquote>
<p><strong>表V</strong>：与AnyGrasp在不同物体上的抓取成功率对比。</p>
</blockquote>
<ul>
<li><strong>语义 grounding 鲁棒性</strong>：评估了不同VLM在功能、属性和空间推理上的表现。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.06207v2/x6.png" alt="不同大模型语义 grounding 对比"></p>
<blockquote>
<p><strong>图6</strong>：不同大模型在功能、属性和空间推理任务上的语义 grounding 对比。可靠的语义 grounding 对后续几何参数化至关重要。</p>
</blockquote>
<ul>
<li><p><strong>2D语义地图的作用</strong>：实验表明，向VLM提供2D语义地图能显著提高子任务分解的成功率，减少规划幻觉。</p>
</li>
<li><p><strong>编码模型能力的影响</strong>：比较了不同编码模型在几何参数化和轨迹合成上的表现。</p>
</li>
</ul>
<p><img src="https://arxiv.org/html/2510.06207v2/x7.png" alt="不同编码模型能力对比"></p>
<blockquote>
<p><strong>图7</strong>：不同编码模型在相同提示下执行几何参数化和轨迹合成任务的能力对比。Claude-Sonnet-4成功率最高但延迟最大，表明当前仅有最先进的编码模型具备完成这些任务所需的推理能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个免训练的框架，将编码模型与具身智能体结合，实现了真实世界中复杂的长时操作。2) 提出了一种新颖的代码驱动几何参数化方法，将物体抽象为功能性的几何图元，为轨迹合成提供了结构化基础。3) 在真实移动机器人上验证了框架的有效性，展示了其在多样化任务上的强大泛化能力。</p>
<p><strong>局限性</strong>：1) 性能高度依赖于底层编码模型的能力，目前仅有少数最先进的模型（如Claude-Sonnet-4）能可靠完成任务，且延迟较高。2) 当机器人靠近目标时，相机视野受限可能导致点云不完整，进而影响几何参数估计的准确性，这是开门任务成功率较低的主要原因。</p>
<p><strong>启示</strong>：本文展示了一条不同于大规模数据驱动或固定技能库的机器人泛化新路径：<strong>以代码为媒介，利用基础模型的常识和推理能力动态生成适应物理世界的控制策略</strong>。这为构建可解释、可扩展的机器人系统提供了新思路。后续研究可探索更高效的编码模型、更鲁棒的几何感知以处理不完整观测，以及如何将学习到的低级技能与这种高层代码生成范式相结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中泛化能力有限、依赖大量标注数据且可解释性差的问题，提出EmbodiedCoder框架。该方法基于现代编码模型，无需训练或微调，通过代码生成直接参数化对象几何并合成可执行轨迹，实现感知与操作的透明连接。在真实移动机器人上的实验表明，该框架能鲁棒地完成多样化长期任务，并有效泛化到新对象和环境。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.06207" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>