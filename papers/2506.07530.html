<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.07530" target="_blank" rel="noreferrer">2506.07530</a></span>
        <span>作者: Xilin Chen Team</span>
        <span>日期: 2025-06-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作模型在广泛的机器人操作任务中展现出强大的能力。然而，其不断增长的模型规模对在资源受限的机器人系统上部署构成了重大挑战。虽然1比特预训练已被证明能以最小性能损失显著提升大型语言模型的推理效率，但其在VLA模型上的应用仍未得到充分探索。本文针对在内存受限的边缘设备上高效部署VLA模型这一具体痛点，提出构建首个全参数为三元值（即{-1, 0, 1}）的1比特VLA模型。本文的核心思路是：基于1比特大语言模型，结合蒸馏感知训练策略压缩视觉编码器至1.58比特，从而构建一个内存占用极低、性能与先进模型相当的VLA模型。</p>
<h2 id="方法详解">方法详解</h2>
<p>BitVLA的整体训练流程分为三个阶段：1）使用1比特LLM和全精度视觉编码器训练视觉语言模型；2）应用蒸馏感知训练将视觉编码器的权重量化为1.58比特；3）在特定机器人任务上对BitVLA进行微调。</p>
<p><img src="https://arxiv.org/html/2506.07530v1/x2.png" alt="训练总览"></p>
<blockquote>
<p><strong>图2</strong>：BitVLA的训练概览。首先使用1比特LLM与全精度视觉编码器训练一个视觉语言模型；然后应用蒸馏感知训练将视觉编码器的权重量化为1.58比特精度；最后，通过OFT微调使BitVLA适应特定机器人任务。</p>
</blockquote>
<p><strong>模型架构</strong>：BitVLA采用BitNet b1.58 2B4T作为其LLM骨干，并利用在224×224分辨率图像上预训练的SigLIP-L作为视觉编码器，以生成较短的视觉标记序列来提升计算效率。一个具有GeLU激活函数的两层MLP作为连接器，因其对整体模型大小贡献可忽略而保持全精度。</p>
<p><strong>量化策略</strong>：对权重采用absmean量化器，对激活采用per-token absmax量化器。权重被量化为三元值，激活被量化为对称INT8。量化过程在训练的前向传播中实时进行，并采用直通估计器来近似量化函数的梯度以进行反向传播。梯度和优化器状态均保持全精度以维持训练稳定性。</p>
<p><strong>核心创新：蒸馏感知训练</strong>：为了将VLM的视觉编码器有效量化为1.58比特，论文提出了蒸馏感知训练策略。该策略的核心是使用原始的全精度编码器作为教师模型，通过一个辅助的表示对齐损失来指导1.58比特学生编码器的学习。</p>
<p><img src="https://arxiv.org/html/2506.07530v1/x3.png" alt="蒸馏感知训练"></p>
<blockquote>
<p><strong>图3</strong>：蒸馏感知训练概览。原始的全精度编码器作为教师模型，以确保学生编码器的潜在表示能与之更好地对齐。</p>
</blockquote>
<p>具体而言，总训练目标 ℒ_total 由任务特定的语言建模损失 ℒ_LM 和辅助对齐损失 ℒ_aux 组成：ℒ_total = ℒ_LM + γ ⋅ ℒ_aux。语言建模损失是标准的自回归损失，仅对答案文本标记进行计算。表示对齐损失定义为全精度编码器和1.58比特编码器各层输出之间的均方误差，旨在鼓励1.58比特编码器模仿其全精度对应物的表示行为。在此阶段，仅视觉编码器是可训练的，LLM和连接器被冻结。</p>
<p><strong>机器人任务微调</strong>：遵循OpenVLA-OFT的方法，采用并行解码和动作分块技术来提升VLA模型的吞吐量。具体包括使用双向注意力掩码替代传统的因果掩码，使单次前向传播能生成多时间步的连贯动作轨迹，并集成一个基于MLP的动作头将查询标记的潜在表示投影到连续的机器人动作空间。训练目标是最小化预测动作与真实轨迹之间的L1损失。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要评估基准是LIBERO仿真环境，该基准通过四个任务套件评估机器人智能的四个关键维度：空间泛化、物体泛化、目标泛化和长时程推理。对比的基线方法包括OpenVLA-OFT、OpenVLA、SpatialVLA、CoT-VLA、NORA-Long和π0。BitVLA未进行大规模机器人预训练，而是在与OpenVLA-OFT相同的训练数据集上进行微调。</p>
<p><img src="https://arxiv.org/html/2506.07530v1/x4.png" alt="LIBERO基准"></p>
<blockquote>
<p><strong>图4</strong>：LIBERO基准任务套件概览。它从四个不同维度评估机器人操作模型的泛化能力和性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2506.07530v1/x1.png" alt="性能与内存对比"></p>
<blockquote>
<p><strong>图1</strong>：BitVLA与采用4比特训练后量化的OpenVLA-OFT在最终任务性能和内存占用上的对比。报告了LIBERO基准上的平均成功率。BitVLA在仅占用29.8%内存的情况下，取得了与先进模型相当的性能。</p>
</blockquote>
<p>如表1所示，尽管BitVLA没有在大规模机器人数据集上进行预训练，但其在LIBERO基准上的平均成功率达到了94.8%，超越了π0和NORA-Long等强劲基线。特别是在长时程推理任务上，BitVLA以87.6%的成功率超过了π0的85.2%。BitVLA的内存占用仅为1.4GB，远低于其他模型。</p>
<p>与训练后量化方法的对比（表2）显示，BitVLA取得了与4比特量化的OpenVLA-OFT（成功率96.9%）相当的性能（94.8%），而内存占用仅为后者的不到三分之一（1.4GB vs 4.7GB）。</p>
<p><strong>视觉问答任务验证</strong>：在MMMU、SeedBench等五个VQA基准上的零样本评估表明（表3），配备1.58比特视觉编码器的BitVLA平均准确率为51.5%，仅比全精度版本（53.0%）下降1.5%，同时视觉编码器内存从0.8GB大幅降至0.1GB，证明了蒸馏感知训练在保持通用视觉语言能力方面的有效性。</p>
<p><strong>消融实验</strong>：<br>表4和表5的消融研究表明，在蒸馏感知训练中，<strong>表示对齐损失 ℒ_aux 至关重要</strong>。在5B训练标记下，移除该损失会导致VQA平均准确率从50.8%骤降至42.4%，LIBERO平均成功率从93.6%降至92.9%。此外，<strong>增加训练数据量（从5B到10B标记）能带来一致的性能提升</strong>。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）<strong>首次提出了一个全参数为三元的1比特视觉-语言-动作模型</strong>，为机器人操作任务提供了极高效解决方案；2）<strong>提出了蒸馏感知训练策略</strong>，通过利用全精度教师模型指导，实现了视觉编码器向1.58比特的高效、高性能量化；3）<strong>在未进行大规模机器人预训练的情况下，取得了与先进模型相当的性能，同时内存占用大幅降低</strong>（仅为对比模型的29.8%），展现出在边缘设备部署的巨大潜力。</p>
<p>论文自身提到的局限性在于：BitVLA未经过大规模机器人数据集（如Open X-Embodiment）的预训练，这可能导致其在复杂的长期任务（如LIBERO-Long）上性能略逊于经过此类预训练的模型。</p>
<p>本工作对后续研究的启示包括：证明了1比特架构可成功扩展至多模态和具身AI领域；蒸馏感知训练为压缩视觉骨干网络提供了一种有效范式；极低的存储需求使得在消费级硬件上部署复杂的VLA模型成为可能，推动了机器人技术的平民化和普及化。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型在资源受限机器人系统上部署困难的问题，提出了首个1比特VLA模型BitVLA。其核心方法是将所有参数量化为三元值{-1,0,1}，并针对视觉编码器提出蒸馏感知训练策略，将其压缩至1.58比特权重。实验表明，BitVLA在LIBERO基准测试上取得了与4比特量化先进模型OpenVLA-OFT相当的性能，同时内存占用仅为后者的29.8%，显著提升了部署效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.07530" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>