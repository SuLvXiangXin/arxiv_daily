<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.06953" target="_blank" rel="noreferrer">2509.06953</a></span>
        <span>作者: Deepak Pathak Team</span>
        <span>日期: 2025-09-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在家庭、厨房等自然动态环境中，机器人需要具备在快速变化、部分可观测场景下安全导航的能力，其核心是生成无碰撞的运动。传统运动规划方法（如搜索或优化方法）虽能计算全局最优轨迹，但需要完全的环境知识，且在动态场景中通常速度过慢。基于反应式控制器的方法（如Riemannian Motion Policies）能在动态场景中提供反应式避障，但缺乏全局场景感知，在复杂环境中易陷入局部极小值。另一种思路是学习端到端的视觉-运动神经策略，直接从原始传感器输入（如点云）映射到动作，具备实时闭环适应性，但现有方法（如Mπ Nets、NeuralMP）或因训练数据多样性有限而泛化能力不足，或因依赖耗时的测试时优化而牺牲了反应速度。</p>
<p>本文针对动态、部分可观测环境中快速、鲁棒生成无碰撞运动这一痛点，提出了结合学习与反应控制的新视角。其核心思路是：首先通过大规模仿真数据预训练一个基于Transformer的神经运动策略（IMPACT），使其获得全局场景感知；然后通过迭代的学生-教师微调提升其静态避障能力；最后在推理时集成一个基于点云的局部反应式目标提议模块（DCP-RMP），以显式增强对动态障碍物的反应能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>Deep Reactive Policy (DRP) 是一个用于多样动态真实世界环境中实现无碰撞目标到达的神经视觉-运动策略。其整体架构如图2所示，包含两个核心部分：1) 一个基于Transformer的闭环运动规划策略IMPACT，负责根据场景点云、修正后的关节目标及当前关节位置输出动作序列；2) 一个局部反应式的目标提议模块DCP-RMP，用于处理快速移动的动态障碍物，调整关节目标。</p>
<p><img src="https://arxiv.org/html/2509.06953v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Deep Reactive Policy (DRP) 整体架构。首先，局部反应式DCP-RMP模块根据局部场景中的动态障碍物调整关节目标。然后，基于Transformer的闭环运动规划策略IMPACT接收场景点云、修正后的关节目标和当前机器人关节位置作为输入，输出用于机器人实时执行的动作序列。</p>
</blockquote>
<p><strong>IMPACT核心策略</strong>：IMPACT (Imitating Motion Planning with Action-Chunking Transformer) 是一个基于Transformer的神经运动策略。其输入包括：场景点云 $P_s$、机器人点云 $P_r$、当前关节配置 $q_c$ 和目标关节配置 $q_{mg}$。为降低计算复杂度并实现实时推理，使用PointNet++中的集合抽象对点云进行下采样，生成更小的潜在令牌集合 $z_s$ 和 $z_r$。当前和目标关节角度通过MLP编码为 $z_c$ 和 $z_{mg}$。每个输入与一个可学习的嵌入相加后送入编码器。解码器处理 $S$ 个可学习的动作令牌 $A$，以编码器输出为记忆，最终输出一个包含 $S$ 个增量关节动作的序列 $[\bar{q}<em>1, \bar{q}<em>2, \dots, \bar{q}<em>S]$，并通过均方误差损失与专家动作进行监督学习：$\mathcal{L}</em>{BC}=\frac{1}{S}\sum</em>{i=1}^{S}||q</em>{i}-\bar{q}<em>{i}||</em>{2}$。</p>
<p><strong>大规模运动预训练</strong>：为预训练IMPACT，论文利用GPU加速的运动规划器cuRobo生成了超过1000万条专家轨迹的多样化数据集。数据生成场景包括目标被障碍物阻挡（物理上不可达）的挑战性情况，此时专家轨迹被修改为让机器人在不碰撞的情况下尽可能接近目标。这使得策略能够学习在目标暂时不可达时的安全行为。</p>
<p><strong>迭代学生-教师微调</strong>：尽管预训练后的IMPACT已具备较强的全局规划能力，但由于行为克隆存在误差累积问题，仍会发生轻微碰撞。为增强静态避障能力，论文采用了迭代学生-教师微调策略。</p>
<p><img src="https://arxiv.org/html/2509.06953v1/x3.png" alt="学生-教师微调"></p>
<blockquote>
<p><strong>图3</strong>：迭代学生-教师微调过程。将IMPACT策略与擅长局部避障的Geometric Fabrics控制器结合构成教师策略，然后将其行为蒸馏到仅依赖点云输入的学生策略中。</p>
</blockquote>
<p>如图3所示，教师策略将预训练的IMPACT策略输出的第一个关节位置目标 $q_b$，输入到Geometric Fabrics控制器 $\pi_f$ 中，后者利用特权障碍物信息（如SDF）将其细化为改进的目标 $q_e$。这个改进的目标用于监督更新学生策略 $\pi_s$（即点云版本的IMPACT）。此过程在IsaacGym中并行运行。经过一定步数后，微调后的学生策略替换教师中的基础策略，迭代进行，从而逐步提升局部避障能力，同时保持全局规划能力。此过程将成功率较预训练模型提升了45%。</p>
<p>**动态最近点RMP (DCP-RMP)**：为了在推理时显式增强对动态障碍物的反应能力，DRP集成了DCP-RMP模块。该模块作为一个非学习的、局部反应式的目标提议层，直接操作于点云输入。其核心思想是：通过KDTree比较当前帧与前一帧的点云，高效识别动态障碍物点；计算机器人与此类障碍物之间的最小位移 $\mathbf{x}_r$，并推导出一个排斥加速度以增大该距离；最后，通过虚拟地应用此排斥信号来调整原始关节目标 $\mathbf{q}<em>d$，得到优先考虑动态避障的修正目标 $\mathbf{q}</em>{mg}$，再输入给IMPACT。由于IMPACT已在目标被阻挡的场景中训练过，它能够安全地处理这个修正目标。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：论文设计了DRPBench基准测试，包含五种具有挑战性的仿真任务（如图4所示）：静态环境、突然出现障碍物、浮动动态障碍物、目标阻挡、动态目标阻挡。此外，还在Mπ Nets数据集上进行了零样本测试。评估指标为成功率（在无碰撞前提下到达末端执行器目标位姿）。真实世界实验使用Franka Panda机器人，配备了2-4个Intel RealSense D455 RGB-D相机，场景包含训练中未见的语义化障碍物。</p>
<p><img src="https://arxiv.org/html/2509.06953v1/x4.png" alt="评估场景"></p>
<blockquote>
<p><strong>图4</strong>：DRPBench引入的五种挑战性评估场景示意图：静态环境、突然出现障碍物、浮动动态障碍物、目标阻挡、动态目标阻挡。</p>
</blockquote>
<p><strong>主要对比结果</strong>：如表1所示，DRP在多样化的设置中均优于所有经典和基于学习的基线方法，尤其在动态和目标任务中表现出色。</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">SE</th>
<th align="left">SAO</th>
<th align="left">FDO</th>
<th align="left">GB</th>
<th align="left">DGB</th>
<th align="left">Mπ Nets Dataset</th>
</tr>
</thead>
<tbody><tr>
<td align="left">AIT*</td>
<td align="left">40.50</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">89.22</td>
</tr>
<tr>
<td align="left">cuRobo</td>
<td align="left">82.97</td>
<td align="left">59.00</td>
<td align="left">39.50</td>
<td align="left">0</td>
<td align="left">3.00</td>
<td align="left">99.78</td>
</tr>
<tr>
<td align="left">RMP</td>
<td align="left">32.97</td>
<td align="left">46.0</td>
<td align="left">49.50</td>
<td align="left">71.08</td>
<td align="left">50.50</td>
<td align="left">41.90</td>
</tr>
<tr>
<td align="left">Mπ Nets</td>
<td align="left">2.50</td>
<td align="left">0.33</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">65.18</td>
</tr>
<tr>
<td align="left">NeuralMP</td>
<td align="left">50.59</td>
<td align="left">33.16</td>
<td align="left">19.00</td>
<td align="left">0</td>
<td align="left">0.25</td>
<td align="left">–</td>
</tr>
<tr>
<td align="left">IMPACT (no finetune)</td>
<td align="left">58.25</td>
<td align="left">47.33</td>
<td align="left">13.50</td>
<td align="left">46.50</td>
<td align="left">0</td>
<td align="left">66.27</td>
</tr>
<tr>
<td align="left">IMPACT</td>
<td align="left">84.60</td>
<td align="left">86.00</td>
<td align="left">32.00</td>
<td align="left">66.67</td>
<td align="left">0.25</td>
<td align="left">83.71</td>
</tr>
<tr>
<td align="left"><strong>DRP (Ours)</strong></td>
<td align="left"><strong>84.60</strong></td>
<td align="left"><strong>86.00</strong></td>
<td align="left"><strong>75.50</strong></td>
<td align="left"><strong>66.67</strong></td>
<td align="left"><strong>65.25</strong></td>
<td align="left"><strong>83.71</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：在DRPBench和Mπ Nets数据集上的定量结果（成功率%）。DRP在所有设置中均超越基线，在动态和目标任务上优势明显。优化方法（如cuRobo）在静态场景成功，但在动态条件下表现不佳。</p>
</blockquote>
<p><strong>关键分析</strong>：</p>
<ol>
<li><strong>架构与数据多样性</strong>：即使未经微调，IMPACT也因更具表达力的架构和更丰富的训练数据，而优于其他学习方法（如Mπ Nets、NeuralMP）。</li>
<li><strong>微调的效果</strong>：经过学生-教师微调后的IMPACT，在静态和需要精细局部控制的任务（如SAO: 86.00%， GB: 66.67%）上表现优异，验证了闭环模仿学习的强度。</li>
<li><strong>DCP-RMP的贡献</strong>：DCP-RMP模块极大地提升了动态任务性能（FDO: 75.50%， DGB: 65.25%），而单独使用IMPACT则大幅下降（FDO: 32.00%， DGB: 0.25%）。表2的消融实验进一步表明，DCP-RMP是方法无关的，能普遍提升不同基线的动态任务性能。</li>
<li><strong>真实世界泛化</strong>：如表3所示，在真实世界任务中，DRP显著优于基线。在静态和SAO任务上，DRP和IMPACT接近完美成功率。在更具挑战性的FDO和DGB任务上，IMPACT完全失败，而DRP凭借DCP-RMP分别取得了70.00%和93.33%的成功率，凸显了反应模块在真实动态场景中的价值。</li>
</ol>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">SE</th>
<th align="left">SAO</th>
<th align="left">FDO</th>
<th align="left">GB</th>
<th align="left">DGB</th>
</tr>
</thead>
<tbody><tr>
<td align="left">cuRobo-Vox</td>
<td align="left">60.00</td>
<td align="left">3.33</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">NeuralMP</td>
<td align="left">30.00</td>
<td align="left">6.67</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">IMPACT</td>
<td align="left">90.00</td>
<td align="left">100.00</td>
<td align="left">0</td>
<td align="left">92.86</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left"><strong>DRP (Ours)</strong></td>
<td align="left"><strong>90.00</strong></td>
<td align="left"><strong>100.00</strong></td>
<td align="left"><strong>70.00</strong></td>
<td align="left"><strong>92.86</strong></td>
<td align="left"><strong>93.33</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表3</strong>：真实世界任务的成功率（%）。DRP在动态任务上表现卓越，显示出强大的从仿真到真实的泛化能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了IMPACT，一种基于Transformer、以点云为条件的端到端神经运动策略，并通过大规模数据生成（1000万轨迹）进行训练。</li>
<li>引入了迭代学生-教师微调方法，将擅长局部避障的Geometric Fabrics控制器的行为蒸馏到点云策略中，显著提升了静态避障性能。</li>
<li>设计了DCP-RMP，一种可直接操作于点云的局部反应式目标提议模块，与IMPACT集成后构成了完整的DRP系统，在动态环境中实现了快速反应与全局规划的平衡。</li>
</ol>
<p><strong>局限性</strong>：论文指出，DRP依赖于相对准确的点云观测，在感知严重失效时性能可能下降。实验仅限于Franka Panda单一机器人平台，将当前流程扩展到多平台存在挑战。</p>
<p><strong>启示</strong>：DRP的成功表明，将大规模学习获得的全局场景理解能力，与轻量级、非学习的局部反应模块相结合，是应对动态、复杂环境运动规划的有效途径。这为未来研究提供了方向，例如探索跨平台泛化、融合多模态感知（RGB/RGB-D）以处理更非结构化的环境。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Deep Reactive Policy (DRP)，旨在解决机械臂在动态、部分可观测环境中实时生成无碰撞运动规划的挑战。其核心技术包括：基于Transformer的神经运动策略IMPACT，利用千万级仿真专家轨迹预训练；通过迭代师生微调增强静态避障；在推理时结合局部反应式目标提议模块DCP-RMP以提升动态避障能力。实验表明，DRP在杂乱和动态场景中泛化能力强，其成功率在仿真与真实世界任务上均优于先前的经典规划方法与神经策略。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.06953" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>