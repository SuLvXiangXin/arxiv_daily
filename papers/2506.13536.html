<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>What Matters in Learning from Large-Scale Datasets for Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>What Matters in Learning from Large-Scale Datasets for Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.13536" target="_blank" rel="noreferrer">2506.13536</a></span>
        <span>作者: Danfei Xu Team</span>
        <span>日期: 2025-06-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习从离线的大规模多任务演示数据集中学习，已成为教导机器人复杂操作任务的有前景的方法。先前工作发现，机器人性能与数据集的大小和质量呈正相关。因此，学界投入了大量精力构建涵盖多样化任务和环境的大规模机器人数据集，例如 DROID 和 Open X Embodiment 数据集。尽管这些数据收集工作不断增长并显示出潜力，但我们仍缺乏对“应该收集什么数据以提高数据集效用并促进下游策略学习”的系统性理解。理解这一点面临重大挑战：真实世界的数据收集成本极高、耗时极长，且数据集组成存在大量潜在变化维度，这使得测试不同数据集组成的有效性变得不切实际。因此，当前的数据收集工作往往依赖直觉而非系统分析，可能导致资源利用效率低下。本文针对这一痛点，提出使用仿真和合成数据生成作为测试平台，来回答关于大规模数据集收集和使用的关键问题。本文的核心思路是开发一个可控的合成数据生成器，程序化地模拟现有数据集中常见的多样性来源，并以此为基础，从数据收集者和数据检索者两个实践角度出发，进行一系列数据集组成研究，以提供对高效构建和利用大规模机器人数据集的见解。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的 MimicLabs 框架旨在通过可控的合成数据生成，系统性研究数据集组成对下游策略学习的影响。</p>
<p><img src="https://arxiv.org/html/2506.13536v1/x1.png" alt="MimicLabs概述"></p>
<blockquote>
<p><strong>图1</strong>：MimicLabs 框架概述。包含：(1) 一个程序化数据集生成器，用于创建具有可控组成的多样化数据集。(2) 一个受实践场景启发的大规模数据集组成研究，用于分析数据集多样性和对齐的影响。(3) 基于研究见解，使用现有大规模机器人数据集进行的广泛真实世界实验。</p>
</blockquote>
<p>整体框架基于一个<strong>程序化任务和演示生成</strong>系统。该系统以一组数据集组成因素（即<strong>维度变化</strong>）作为输入，生成多样化的任务实例，随后为每个实例生成演示轨迹。其核心模块包括：</p>
<ol>
<li><strong>程序化任务生成</strong>：利用行为领域定义语言（BDDL）来指定场景、物体、空间排列以及任务初始化和成功的谓词。通过扩展 LIBERO 的解析框架，该模块可以控制物体和容器的空间排列、相机位姿和纹理等维度的变化，从而组合生成大量任务实例。</li>
<li><strong>程序化演示生成</strong>：基于 MimicGen 实现自动化演示生成。其关键创新在于，利用 BDDL 任务规范和模拟器提供的状态谓词，将源人类演示自动分解为以物体为中心的操纵片段，然后将这些片段进行变换和拼接，以创建新场景下的新轨迹。这极大地减少了扩展到数千个任务实例所需的人力。</li>
</ol>
<p>通过这两个模块，可以从少量人类演示出发，生成在特定维度上具有受控变化的大规模、多样化数据集，即 <strong>MimicLabs 数据集</strong>。该数据集包含8个不同场景中超过3000个任务实例的近100万条轨迹。</p>
<p><img src="https://arxiv.org/html/2506.13536v1/x3.png" alt="MimicLabs数据集"></p>
<blockquote>
<p><strong>图3</strong>：MimicLabs 数据集示意图。该数据集模拟了多个机器人实验室合作创建具有大量不同维度变化的数据集的现实场景。</p>
</blockquote>
<p>研究的核心是分析<strong>维度变化</strong>对<strong>联合训练</strong>的影响。论文形式化地定义了以下概念：</p>
<ul>
<li><strong>目标数据集</strong>：研究者希望机器人执行的特定任务的少量演示轨迹。</li>
<li><strong>联合训练数据集</strong>：预先存在的大规模、多样化演示数据集。</li>
<li><strong>维度变化</strong>：表征演示数据可变性的独立因素，例如相机位姿、物体纹理、物体空间排列等。一个数据集可以表示为这些维度上分布的函数。</li>
<li><strong>多样性</strong>：指某个维度变化分布的支持集大小。支持集越大，该维度上的多样性越高。</li>
<li><strong>对齐</strong>：指目标数据集在某个维度上的分布支持集是联合训练数据集在该维度上分布支持集的子集。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.13536v1/x2.png" alt="联合训练设置"></p>
<blockquote>
<p><strong>图2</strong>：联合训练设置的特征描述。(a) 根据联合训练数据集的多样性及其与目标数据集的对齐情况，定义了四种情况。(b) 数据收集者旨在选择和增加选定维度变化的多样性以提高数据集效用。(c) 数据检索者旨在通过将特定维度变化与其目标任务对齐，从现有大型数据集中提取子集以提高目标任务性能。</p>
</blockquote>
<p>基于这些定义，研究从两个实践视角展开实验：</p>
<ol>
<li><strong>收集者视角</strong>：假设收集者正在为大规模数据集贡献数据，但收集能力有限。实验通过构造与目标数据集在所有维度上对齐、仅在一个特定维度上具有不同多样性（或不对齐）的联合训练数据集，来研究应该优先增加哪个维度的多样性。</li>
<li><strong>检索者视角</strong>：假设检索者拥有一个现成的大规模联合训练数据集（如 DROID），并希望为其特定目标任务检索一个子集。实验研究通过将检索子集在特定维度上与目标任务对齐，是否能提升学习性能，以及在无法对齐时保留多样性是否有益。</li>
</ol>
<p>与现有方法相比，本文的创新点在于：首次提出了一个统一的、基于可控合成数据的框架，系统地、可扩展地研究了数据集组成（多样性与对齐）对机器人模仿学习的影响，并直接关联到数据收集和检索这两个核心实践问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验主要使用了自建的 <strong>MimicLabs 仿真数据集</strong>和真实世界的 <strong>DROID 数据集</strong>。实验平台涉及仿真环境和真实机器人（具体型号未在提供文本中明确）。对比的基线方法包括：仅使用目标数据集训练、使用不同组成（多样性/对齐程度）的联合训练数据集进行训练、以及在 DROID 上使用全数据集训练与使用本文检索策略训练的比较。</p>
<p><strong>收集者视角的关键结果</strong>：<br>研究以“清理桌子”任务（打开抽屉顶部并将碗放入）为例，考察了相机位姿、物体纹理、桌子纹理、物体空间排列等维度变化的影响。</p>
<p><img src="https://arxiv.org/html/2506.13536v1/x4.png" alt="表1：收集者视角结果"></p>
<blockquote>
<p><strong>表1</strong>：从收集者角度分析维度变化的错位和多样性的影响。显示了在不同维度变化（camPose, objTex等）上目标与联合训练数据错位时，仅用目标数据训练（Target only）和用不同多样性联合训练数据集训练的成功率。Baseline列代表低多样性和错位的情况。</p>
</blockquote>
<ul>
<li><strong>相机位姿至关重要</strong>：当相机位姿错位时，基线性能很低（16.67%）。增加联合训练数据集中相机位姿的多样性，即使与目标错位，也能带来超过40%的性能提升（例如，从16.67%提升至43.33%或90%）。这表明相机位姿的多样性对于技能迁移至关重要。</li>
<li><strong>空间排列影响显著</strong>：物体空间排列的错位严重损害性能（基线10%）。增加该维度的多样性（例如，从10%提升至56.67%）能有效缓解错位带来的负面影响。</li>
<li><strong>纹理影响甚微</strong>：物体纹理和桌子纹理的多样性或错位对下游性能影响很小。例如，物体纹理错位时，基线成功率仍有30%，且增加其他维度的多样性对其提升有限。这表明在数据收集中无需优先考虑纹理的多样性。</li>
</ul>
<p><strong>检索者视角的关键结果</strong>：<br>在仿真和真实世界（使用DROID数据集）中验证了检索策略的有效性。</p>
<p><img src="https://arxiv.org/html/2506.13536v1/x5.png" alt="检索策略效果"></p>
<blockquote>
<p><strong>图4</strong>：检索策略在真实世界任务上的性能。展示了在DROID数据集上，使用不同检索策略（对齐不同维度变化）与使用全数据集训练相比，在7个目标任务上的平均成功率提升。</p>
</blockquote>
<ul>
<li><strong>仿真验证</strong>：在MimicLabs数据集的“做咖啡”任务上，检索与目标任务在<strong>相机位姿</strong>和<strong>空间排列</strong>上对齐的演示子集，相比使用整个多样化数据集，能带来显著的性能提升。</li>
<li><strong>真实世界验证</strong>：将上述见解应用于真实的 DROID 数据集。通过检索与目标任务在相机位姿（使用相机外参测量）和空间排列上对齐的演示，在7个真实机器人操作任务上，该检索策略<strong>持续优于</strong>使用整个 DROID 数据集进行训练的策略，在某些任务上性能提升高达 **70%**。</li>
</ul>
<p><strong>消融实验总结</strong>：<br>论文通过系统性地控制单一变量的实验（如表1所示），明确了每个维度变化（DV）的贡献。核心结论是：<strong>相机位姿和空间排列</strong>是需要在数据收集中强调多样性、在数据检索中追求对齐的关键维度；而<strong>物体纹理</strong>的贡献最小，无需优先考虑。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了一个可控的合成数据生成框架</strong>：能够程序化地模拟现实数据集的多样性来源，生成具有精确控制组成的大规模数据集，为昂贵且耗时的真实世界数据集组成研究提供了可行的测试平台。</li>
<li><strong>从收集者和检索者两个角度进行了系统性研究</strong>：首次统一地研究了数据集构建（应收集什么）和利用（应如何使用）这两个关键问题，并提供了基于数据驱动而非直觉的见解。</li>
<li><strong>得出了可迁移到真实世界的具体、可操作的见解</strong>：明确指出相机位姿和物体空间排列是影响策略性能的关键维度，而物体纹理影响甚微。基于此的检索策略在现有大规模数据集（DROID）上实现了显著性能提升。</li>
</ol>
<p>论文自身提到的局限性包括：研究基于仿真环境，尽管在真实世界得到了验证，但模拟到真实的差距仍然存在；所研究的维度变化集合可能未涵盖所有潜在的重要维度（如动态照明、精确的物体几何形状）。</p>
<p>本文对后续研究的启示在于：为大规模机器人数据集的建设提供了<strong>优先级指导</strong>，建议未来数据收集工作应优先保证相机视角和物体布局的多样性，而无需过度追求外观纹理的变化。同时，研究指出<strong>智能数据检索</strong>是释放现有大型数据集潜力的有效途径，鼓励开发更高效的数据选择与对齐算法。该方法论框架也可扩展用于研究其他潜在的维度变化对学习的影响。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何优化大规模机器人数据集的组成以提升模仿学习效果。核心问题是确定数据收集时应强调的多样性维度及从现有数据集中检索演示的策略。作者开发了数据生成框架，程序化模拟传感器位姿、物体类型与布局等多样性来源，生成可控组成的数据集用于系统性研究。实验发现相机位姿与空间布局是影响数据效用与策略性能的关键因素；在真实机器人任务中，基于该见解的检索策略（如在DROID数据集上）相比现有方法最高提升70%的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.13536" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>