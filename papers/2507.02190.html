<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>cVLA: Towards Efficient Camera-Space VLAs - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>cVLA: Towards Efficient Camera-Space VLAs</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.02190" target="_blank" rel="noreferrer">2507.02190</a></span>
        <span>作者: Thomas Brox Team</span>
        <span>日期: 2025-07-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过联合学习视觉、语言和交互数据，为机器人操作提供了精细的感知和动作生成能力。然而，其进一步发展面临三大关键限制：a) 高昂的计算成本，使得实验具有挑战性；b) 收集高质量、多模态真实世界数据集的困难与耗时；c) 依赖真实世界执行进行标准化评估，导致一致比较困难。现有VLA模型通常输出低级控制指令，训练成本高昂且与机器人本体绑定较紧。本文针对这些痛点，提出了一种新视角：利用视觉语言模型（VLM）在2D图像上的竞争优势，直接在图像坐标系中推断机器人末端执行器的姿态，并预测轨迹路径点（waypoints），而非低级控制指令。本文的核心思路是构建一个轻量级、高效训练的VLA系统，通过单步预测图像坐标系中的末端执行器关键姿态，实现机器人本体无关且易于从仿真向现实迁移的操纵策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>cVLA的整体框架基于一个经过微调的预训练视觉语言模型（PaliGemma2），输入包括当前场景图像（RGB及可选的深度图）、机器人状态（当前末端执行器姿态）和自然语言任务描述，输出是预测的轨迹关键姿态序列。该方法在仿真生成的多样化数据集上进行训练，旨在实现跨仿真、真实数据和真实机器人平台的良好泛化。</p>
<p><img src="https://arxiv.org/html/2507.02190v2/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：cVLA方法整体框架。基于PaliGemma2模型进行微调，输入为单张图像、机器人状态和任务描述，输出为预测的轨迹。训练使用从抓放任务仿真中构建的合成数据集，支持跨域泛化。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>基础模型与微调</strong>：采用预训练的PaliGemma2 VLM作为基础，仅对注意力层参数进行微调。这种轻量级的修改旨在创建一个强大的轨迹预测模型，同时便于研究推理时策略。输入遵循标准VLA提示格式：<code>&lt;场景图像&gt; + &lt;机器人状态&gt; + &lt;任务描述&gt; → &lt;预测轨迹&gt;</code>。</li>
<li><strong>动作表示</strong>：与RT-1等将动作编码为机器人坐标系下的末端执行器位姿增量不同，cVLA将动作编码为<strong>绝对位置</strong>，可选择在机器人基座坐标系或<strong>图像坐标系</strong>（即归一化的图像宽度、高度以及到相机的距离）中表示。位姿（位置和朝向）被离散化为token进行预测：位置（含深度）使用PaliGemma2中的定位token（原n=1024，实验中尝试减少）编码，朝向使用分割token（n=128）编码。</li>
<li><strong>深度信息融合</strong>：为了利用深度观测，将深度图通过Matplotlib的viridis色彩映射转换为RGB图像，然后使用与自然图像相同的预训练图像编码器进行处理。</li>
<li><strong>扩展到少样本模仿</strong>：通过将轨迹预测的条件从自然语言描述改为<strong>演示图像-轨迹对</strong>，实现单样本模仿学习。其提示模板为：<code>&lt;演示图像&gt; + &lt;演示轨迹&gt; + &lt;当前场景图像&gt; → &lt;预测轨迹&gt;</code>。模型需要从演示中推断任务，并在新场景中应用，此过程无需在推理时对新场景进行微调。</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>动作表示创新</strong>：提出在图像坐标系中直接预测绝对末端执行器关键姿态，降低了模型对特定机器人本体和相机内参的依赖，更有利于泛化。</li>
<li><strong>预测方式高效</strong>：采用<strong>单步预测</strong>整个轨迹（两个关键姿态），而非迭代预测或输出低级控制序列，极大提高了训练效率。</li>
<li><strong>数据利用策略</strong>：完全在精心构建和增强的<strong>仿真数据集</strong>上训练，通过域随机化等技术促进仿真到现实的迁移，避免了昂贵的大规模真实数据收集。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/平台</strong>：使用ManiSkill仿真器生成训练数据（包括CLEVR简单几何体和Objaverse真实物体两种资产）；使用DROID真实机器人操作数据集进行离线评估；在搭载Franka Panda机械臂和ZED2i相机的真实机器人平台上进行零样本测试。</li>
<li><strong>Baseline对比</strong>：主要进行消融实验，对比不同设计选择（如是否使用深度输入、不同数据增强、不同动作表示坐标系）的性能。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>消融实验（动作编码、深度与域随机化）</strong>：论文中的表1（此处以文字总结）显示，在仿真成功率上，<strong>添加深度信息作为输入能持续提升性能</strong>。例如，在Objaverse-hard设置下，使用深度可将成功率从42%提升至54%。同时，训练数据的多样性至关重要：仅使用简单几何体（CLEVR）训练无法泛化到真实物体（Objaverse）场景。适度的域随机化（“hard”版本）有助于提升鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.02190v2/x2.png" alt="动作表示对比"></p>
<blockquote>
<p><strong>图2</strong>：动作表示坐标系消融实验。比较机器人坐标系与图像坐标系下的预测性能（CLEVR-easy仿真成功率）。平均而言，图像坐标系（Camera frame）表现更优，但在简单环境中可能因过拟合机器人外观和相机内参而低估其效用。</p>
</blockquote>
<ol start="2">
<li><p><strong>单样本模仿实验</strong>：如表2所示，在仅使用CLEVR数据训练的模仿模型中，在CLEVR-easy仿真上取得了70%的成功率。更重要的是，在更具挑战性的CLEVR-hard数据上训练的模型，在真实DROID-easy数据和未见过的Objaverse-easy数据上表现出更好的泛化能力（L1误差更低），证明了<strong>场景和相机随机化对于实现鲁棒性的必要性</strong>。</p>
</li>
<li><p><strong>推理时策略</strong>：</p>
<ul>
<li><strong>图像裁剪</strong>：由于模型输入分辨率较低（224x224）且为单步预测，对小物体定位敏感。如图3所示，对输入图像进行裁剪能显著提升性能，但过度裁剪也可能导致失败。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.02190v2/x3.png" alt="裁剪策略"></p>
<blockquote>
<p><strong>图3</strong>：输入图像裁剪策略比较。裁剪能一致性地提升模型性能（降低L1误差），但也可能引入新的失败情况（在DROID-hard数据集上评估）。</p>
</blockquote>
<pre><code>- **解码策略**：针对VLA模型预测的位姿token分布平滑、多峰的特点，提出了**Beam-search-NMS**解码策略。如图4所示，传统贪婪解码可能选择错误目标（红方块），而Beam-search-NMS能探测到分布中的多个峰值，从而找到正确目标（蓝杯子）。表3结果显示，Beam-search-NMS在Top-3预测中的最佳结果（从3个候选轨迹中选最优）将L1误差从33.94降至25.00，显著优于贪婪解码、采样和标准波束搜索。
</code></pre>
<p><img src="https://arxiv.org/html/2507.02190v2/figures/pic-81-crop-500.jpg" alt="解码动机"></p>
<blockquote>
<p><strong>图4</strong>：提出新解码策略的动机示例。(a) 输入图像与任务“将黄色方块放入杯子”。(b) 目标位置x和y位置token的logit分布。最可能的波束对应红色方块，但Beam-search-NMS策略也能检测到正确目标（蓝色杯子）的位置。</p>
</blockquote>
<ol start="4">
<li><strong>真实机器人实验</strong>：如图5所示，cVLA模型在未经过任何真实世界微调的情况下，成功在真实机器人上完成了如“将锅铲放到砧板上”、“将芒果放到盘子上”等日常物品的桌面操作任务，展示了其有效的<strong>零样本仿真到现实迁移能力</strong>。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.02190v2/x4.png" alt="真实机器人演示"></p>
<blockquote>
<p><strong>图5</strong>：真实世界任务演示。上图：将锅铲放置到砧板。下图：将芒果放置到盘子。证明了方法的现实可行性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种<strong>高效的VLA训练与评估框架</strong>：基于图像坐标系的关键姿态单步预测、轻量级微调、以及利用可控合成数据，降低了VLA的研究门槛。</li>
<li>深入探索了影响VLA性能的因素：系统研究了<strong>深度信息融合、不同动作表示、数据增强、以及推理时策略</strong>（如图像裁剪和新型Beam-search-NMS解码）的影响。</li>
<li>实现了<strong>从仿真到现实的零样本迁移</strong>，并展示了将系统轻松扩展为<strong>单样本模仿学习</strong>模型的能力，同时公开了代码、数据集和模型。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>任务范围狭窄</strong>：仅针对小物体、顶部抓取的单一操作任务进行评估，未涉及更复杂的抓取策略（如侧抓、手中操作）或更大物体。</li>
<li><strong>旋转精度不足</strong>：在真实数据上，模型预测的末端执行器<strong>旋转精度较差</strong>，限制了需要精确朝向控制的任务。</li>
<li><strong>仿真依赖</strong>：主要在仿真中训练和评估，真实世界测试有限，泛化能力和鲁棒性有待在更广泛的任务和机器人本体上进一步验证。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>本文验证了在<strong>图像空间进行动作预测</strong>这一路径的可行性及其在效率上的优势，为构建轻量级、可泛化的VLA提供了新思路。</li>
<li>强调了<strong>高质量、多样化的仿真数据生成与增强</strong>对于仿真到现实迁移的关键作用。</li>
<li>指出<strong>提升姿态预测（尤其是旋转）的精度</strong>是未来需要攻克的重要挑战，同时鼓励将方法扩展到更复杂的操作任务和场景中。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型训练成本高昂的问题，提出了一种轻量高效的cVLA方法。其核心创新在于利用视觉语言模型（VLM）对2D图像的强大理解能力，直接在图像坐标系中预测机器人末端执行器的轨迹路径点，而非传统的低级控制指令。该方法采用基于PaliGemma架构的下一个令牌预测模型，并在模拟数据集上进行训练。实验表明，该模型能有效学习有意义的机器人轨迹，并展现出良好的从模拟到现实的迁移能力，在真实机器人系统上得到验证。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.02190" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>