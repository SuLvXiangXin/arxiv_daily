<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.24985" target="_blank" rel="noreferrer">2512.24985</a></span>
        <span>作者: Park, Yohan, Ha, Hyunwoo, Jo, Wonjun, Oh, Tae-Hyun</span>
        <span>日期: 2025/12/31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）正日益成为具身智能体的核心推理模块。现有的具身问答（EQA）基准测试主要在理想、光照良好的条件下评估其能力。然而，现实世界中机器人需要24/7运行，这意味着它们必须能在包括夜间或黑暗环境在内的各种视觉退化条件下保持性能，这是一个被现有研究广泛忽视的核心需求。获取大规模、带有清晰配对标注的真实世界低光图像成本高昂，阻碍了相关基准的构建。因此，本文针对EQA任务在低光照条件下的鲁棒性评估缺失这一具体痛点，提出了DarkEQA基准。其核心思路是：通过一个基于物理原理的低光图像合成流程，从现有室内数据集中生成具有可控退化程度的图像，并配合一套基于确定性规则生成的问答对，来系统性地评估VLMs在低光条件下的感知能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>DarkEQA的整体框架包含两个核心部分：基于物理原理的低光图像合成流程，以及基于确定性规则的问答对生成流程。</p>
<p><strong>低光图像合成</strong>：为了在基准测试中生成可控的低光输入，论文设计了一个物理真实的合成流程，其关键设计是生成两种配对的退化变体，以分离光照下降和传感器噪声对VLM性能的不同影响。</p>
<p><img src="https://arxiv.org/html/2512.24985v3/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：解耦光照与噪声因素的低光合成流程。顶部为基于物理的流程，将sRGB图像反处理（unprocess）至Bayer RAW域，注入四种噪声成分，再进行曝光值（EV）下降和伽马压缩。底部为无噪声流程，直接在线性RGB空间应用相同的EV下降。左下角总结了sRGB→RAW的反处理步骤，右下角可视化了四种独立的噪声成分（散粒噪声、读取噪声、行模式噪声和量化噪声）。</p>
</blockquote>
<p>具体而言，对于每一张原始图像（L0），通过改变曝光值下降量（ΔEV ∈ {2.0, 4.0, 6.0, 7.5, 9.0}）生成从L1到L5共五个逐渐严重的退化级别。每个级别下生成两种变体：</p>
<ol>
<li><strong>无噪声EV下降变体</strong>：将sRGB图像通过伽马扩展解码到线性RGB空间，乘以缩放因子 2^{-ΔEV} 模拟光照下降，再通过伽马编码映射回sRGB。</li>
<li><strong>物理动机变体（含噪声）</strong>：遵循更真实的图像信号处理（ISP）流程。首先，将sRGB图像通过包含逆色调映射、伽马扩展、色彩校正、白平衡/亮度增益反转和马赛克提取五个步骤的“反处理”操作，还原到相机线性的Bayer RAW域。然后，在RAW域注入四种物理噪声模型：<strong>光子散粒噪声</strong>（模拟光子到达的泊松过程）、<strong>读取噪声</strong>（使用Tukey-λ分布模拟读出电路噪声）、<strong>行噪声</strong>（为每一行添加共享的高斯偏移模拟条带伪影）和<strong>量化噪声</strong>（模拟模数转换的均匀舍入误差）。最后，通过一个简化的ISP流程（包含白平衡、去马赛克、色彩校正、EV下降、伽马压缩和8位量化）将带噪的RAW图像转换回sRGB。</li>
</ol>
<p><strong>问答对生成</strong>：为了避免使用商业VLM服务导致潜在的数据污染，所有问答对均通过基于规则的确定性程序自动生成。该流程基于HM3D-Sem数据集中的关键帧及其渲染的几何与语义模态（RGB、深度、分割图）进行。</p>
<p><img src="https://arxiv.org/html/2512.24985v3/x4.png" alt="问题类型示例"></p>
<blockquote>
<p><strong>图4</strong>：DarkEQA的五类问题示例。涵盖房间类型识别、房间可供性检查、物体识别、物体属性识别和最近物体识别。</p>
</blockquote>
<p>生成过程分为两个阶段（见算法1）：</p>
<ol>
<li><strong>帧统计信息提取</strong>：为每个关键帧计算其所有分割片段的属性统计向量，包括语义类别、颜色、深度、面积和边界框。</li>
<li><strong>问答生成</strong>：基于预定义的规则和提取的帧统计信息，首先预测房间类型，然后枚举适用的预定义问题模板，生成具体的问答对。例如，对于“最近物体识别”问题，生成器会验证是否存在至少两个具有有效深度测量的非结构物体实例，且前两个最近物体的深度差超过阈值，然后确定最近物体作为答案。</li>
</ol>
<p>最终构建的数据集包含来自52个场景的3，911帧图像（分辨率1440×2560）和约9.4K个问答对，问题涵盖五大类。</p>
<p><img src="https://arxiv.org/html/2512.24985v3/x5.png" alt="数据集统计"></p>
<blockquote>
<p><strong>图5</strong>：DarkEQA数据集统计信息，包括语义类别覆盖、房间类别分布和问题类别分布。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：基准测试在DarkEQA数据集上进行。评估将每个问答任务视为多项选择题，模型需从固定候选答案中选择一个。评估对象包括：</p>
<ul>
<li><strong>盲LLMs基线</strong>：GPT-4和LLaMA-3.1-8B，仅接收文本问题以检验数据偏差和语言先验。</li>
<li><strong>VLMs</strong>：涵盖不同规模的模型，包括LLaVA-1.6-7B、LLaVA-OneVision-8B、InternVL3.5-8B/30B、Qwen3-VL-8B/32B，以及作为性能上限参考的GPT-4o。</li>
<li><strong>LLIE预处理</strong>：使用最先进的低光图像增强模型DarkIR对噪声添加后的低光图像进行预处理，再输入VLM，以检验其缓解性能下降的效果。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2512.24985v3/x6.png" alt="主要评估结果"></p>
<blockquote>
<p><strong>图6</strong>：DarkEQA评估结果总结。(a) 噪声注入的影响：纯EV下降和EV下降加噪声均导致VLM准确率显著下降，且噪声加剧了性能下降。(b) LLIE预处理的影响：在严重退化级别（L4， L5）能提升性能，但在中度退化级别（L1-L3）反而可能导致性能下降，效果不稳定。(c) 模型间对比：所有VLM在低光条件下性能均呈下降趋势，GPT-4o性能最高但也出现退化。在L5级别，部分VLM准确率甚至低于仅使用文本的GPT-4（盲LLM基线）。(d) LLIE增强后的图像示例：即使在L5级别，增强后的图像对人眼看似可感知，但VLM性能仍可能很差。</p>
</blockquote>
<ol>
<li><strong>光照下降与传感器噪声的影响</strong>：如图6(a)所示，随着退化级别加剧（L0→L5），所有VLMs的准确率均显著下降。与纯EV下降相比，引入传感器噪声会导致更严重的性能下降，证实噪声是关键影响因素。</li>
<li><strong>LLIE预处理的效果有限且不稳定</strong>：如图6(b)所示，使用LLIE模型预处理噪声图像的结果好坏参半。在严重退化级别（L4， L5）观察到显著提升，但在中度退化级别（L1-L3）性能反而下降。这表明当前的LLIE模型并非通用的解决方案，其效果依赖于退化程度。</li>
<li><strong>模型性能普遍下降</strong>：如图6(c)和表I详细数据所示，所有VLMs在低光下性能均恶化。一个关键发现是，在极端低光级别（L5），部分VLMs的准确率甚至低于仅依赖文本的盲LLM基线（GPT-4），意味着严重退化下的视觉信息可能已无法被有效利用，甚至不如语言先验。</li>
<li><strong>不同问题类型的敏感性</strong>：<br><img src="https://arxiv.org/html/2512.24985v3/x7.png" alt="问题类型准确率"><blockquote>
<p><strong>图7</strong>：不同问题类型的准确率。在“房间类型识别”和“物体属性-颜色”两类问题上，VLM性能在严重退化级别下降至盲LLM基线以下，表明VLMs在提取颜色等关键视觉语义信息时面临巨大困难。<br>如图7所示，性能下降在所有问题类型中普遍存在，但在“房间类型识别”和“物体属性-颜色”两类上尤为严重。在L4/L5级别，VLMs在这两类问题上的准确率跌破盲LLM基线，类比了人眼在暗光下视锥细胞（感色）功能减弱的现象，表明VLMs难以从极暗图像中提取颜色等语义信息。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了首个专门针对低光室内环境的具身问答基准测试DarkEQA，其合成流程具有物理真实性，并能解耦光照与噪声的影响；2）通过系统的评估，揭示了当前VLMs在低光视觉退化下的显著性能衰减和脆弱性；3）实证分析了低光图像增强（LLIE）作为预处理模块的局限性，表明其效果不稳定且非普适。</p>
<p>论文自身提到的局限性主要在于其基于仿真的性质，尽管力求物理真实，但与真实世界低光图像的复杂退化模式仍可能存在差距。</p>
<p>这项工作对未来研究的启示是多方面的：首先，它强调了在具身AI评估中纳入视觉退化条件（尤其是低光照）的必要性。其次，实验结果呼吁开发更具鲁棒性的VLM架构或训练策略，使其能直接处理低质量视觉输入。最后，LLIE预处理效果的不稳定性表明，简单地串联感知增强模块与VLM并非最优解，未来可能需要探索任务导向的、与VLM协同训练或深度集成的低光感知增强方法。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有基准测试忽视低光环境的问题，提出了DarkEQA基准。该基准旨在系统评估视觉语言模型在低光室内环境下的具身问答性能。其关键技术在于采用物理保真设计：在线性RAW空间中模拟基于物理的照明衰减与传感器噪声，并通过ISP渲染流程生成退化图像，从而隔离感知瓶颈。实验评估了多种先进VLM与低光增强模型，系统揭示了VLM在此类挑战性视觉条件下的显著性能局限。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.24985" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>