<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.09973" target="_blank" rel="noreferrer">2602.09973</a></span>
        <span>作者: Jiangmiao Pang Team</span>
        <span>日期: 2026-02-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型视觉语言模型（VLMs）的进展激发了人们对用于机器人操作的视觉-语言-动作（VLA）系统的兴趣。然而，现有的操作数据集仍然存在成本高昂、与具体机器人形态高度耦合、覆盖范围和多样性不足的问题，这阻碍了VLA模型的泛化能力。近期研究试图通过“先计划后执行”的范式来缓解这些局限，即先生成高级计划（如子任务、轨迹），再将其翻译为低级动作。但这种方法严重依赖额外的中间表示监督，而现有数据集大多缺乏此类标注。现有自动化标注方法（如LLARVA、ECoT）在规模、质量或与动作的对齐性上存在不足。因此，本文旨在通过构建一个统一的中间表示资源套件（包括数据、基准和模型）来弥合这一差距。核心思路是：首先创建一个大规模、高质量、帧对齐的多样化中间表示标注数据集（RoboInter-Data），并基于此构建系统的具身视觉问答基准（RoboInter-VQA）来提升VLM的规划能力，最终提供一个支持多种变体的“先计划后执行”VLA框架（RoboInter-VLA），以研究中间表示对机器人操作泛化性和可控性的影响。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的RoboInter是一个全面的中间表示套件，如图1所示，主要包括三个核心组成部分：RoboInter-Data（数据集）、RoboInter-VQA（基准）和RoboInter-VLA（模型框架）。</p>
<p><img src="https://arxiv.org/html/2602.09973v1/x1.png" alt="方法套件总览"></p>
<blockquote>
<p><strong>图1</strong>：RoboInter操作套件概览。包括标注工具、标注数据、精心构建的VQA数据集及其在VLMs和VLAs中的应用。</p>
</blockquote>
<p><strong>1. RoboInter-Data：大规模中间表示数据集</strong><br>如图2所示，该数据集基于Droid和RH20T等现有操作数据集构建，包含超过23万条操作片段，覆盖571个不同场景。其核心是通过RoboInter-Tool这一轻量级GUI工具进行半自动逐帧标注，并结合人工检查，最终提供了超过10个类别的密集中间表示标注，包括：子任务、原始技能、分割、夹爪/物体边界框、放置建议、功能框、抓取姿态、轨迹、接触点等。所有标注均与执行的动作、机器人状态及双视角（第三人称和腕部视角）观测时间对齐，支持端到端动作学习。具体标注流程包括：任务分解与关键帧标注、被操作物体识别与分割、末端执行器定位与轨迹重建，并通过后处理推导出抓取、放置等额外标注。</p>
<p><strong>2. RoboInter-VQA：具身视觉问答基准</strong><br>为了系统化地评估和提升VLMs的具身推理能力，作者将RoboInter-Data中的标注转化为多样化的VQA任务。如图2右侧所示，这些任务沿两个维度组织：中间表示类型（空间 vs. 时间）和目标能力（理解 vs. 生成）。</p>
<ul>
<li><strong>空间VQA（理解）</strong>：包括选择正确的物体边界框或抓取姿态、匹配场景与指令、判断是否发生接触等任务。</li>
<li><strong>空间VQA（生成）</strong>：要求生成物体边界框、抓取姿态、放置建议、关键点、夹爪边界框等空间中间表示。</li>
<li><strong>时间VQA（理解）</strong>：包括选择夹爪运动方向、匹配轨迹与描述、判别子任务/原始技能、识别执行阶段等任务，以及评估任务成功率和下一步可行性的判断任务。</li>
<li><strong>时间VQA（生成）</strong>：要求在给定不同上下文完整度（如过往子任务或整体指令）的条件下，生成未来轨迹或多步计划。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.09973v1/x2.png" alt="数据集与VQA构建概览"></p>
<blockquote>
<p><strong>图2</strong>：RoboInter-Data和RoboInter-VQA概述。展示了从原始数据收集、标注检查到构建大规模、多样化VQA任务的完整流程，并提供了统计数据。</p>
</blockquote>
<p><strong>3. RoboInter-VLA：“先计划后执行”模型框架</strong><br>如图3所示，该框架遵循“先计划后执行”范式，包含一个<strong>规划器（Planner）</strong> 和一个<strong>执行器（Executor）</strong>。</p>
<ul>
<li><strong>规划器（VLM）</strong>：基于Qwen-VL或LLaVA-One-Vision等VLM架构，通过在RoboInter-VQA数据上进行视觉问答训练，获得强大的具身理解和中间表示生成能力。</li>
<li><strong>执行器（VLA）</strong>：基于Qwen2.5-VL骨干网络，并配备一个扩散Transformer（DiT）动作头。它接收多视角视觉观测、语言指令以及（由规划器或真值提供的）中间表示，并输出多步动作块。</li>
<li><strong>灵活的思维链（F-CoT）</strong>：为了连接规划与执行，作者引入了F-CoT，它是由多个中间表示（如子任务、技能、物体框、轨迹等）以文本或视觉形式组成的思维链。F-CoT既作为训练规划器的VQA监督，也作为指导执行器的动作对齐引导。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.09973v1/x3.png" alt="VLA框架"></p>
<blockquote>
<p><strong>图3</strong>：RoboInter-VLA框架。模型遵循“先计划后执行”范式，包含一个VLM规划器和一个执行器。支持三种变体，中间表示通过灵活的思维链（F-CoT）连接规划与执行。</p>
</blockquote>
<p><strong>创新点</strong>：本文提出了三种具体的VLA范式变体，以灵活利用规划器和中间表示：</p>
<ol>
<li><strong>RoboInter-IC-E2E（隐式条件端到端）</strong>：将预训练的规划器VLM直接注入端到端执行器，作为更强的视觉语言特征提取器。</li>
<li><strong>RoboInter-EC-E2E（显式条件端到端）</strong>：执行器以规划器的VLM初始化，并联合优化中间表示推理和动作生成。</li>
<li><strong>RoboInter-Modular（模块化）</strong>：规划器和执行器作为独立模块。训练时，执行器以真值中间表示为条件；推理时，依赖于规划器预测的中间表示。其中，文本F-CoT版本称为RoboInter-Te-Modular，视觉提示F-CoT版本称为RoboInter-Im-Modular。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：使用了第三方基准（Where2Place, RoboRefIt, RoboVQA, Refcoco系列，以及通用VLM基准如TextVQA, MME等）和本文提出的RoboInter-VQA基准。执行器评估在“野外”（In-the-Wild）和“桌面”（TableTop）两种设置下进行。</li>
<li><strong>对比方法</strong>：在规划器评估中，对比了通用VLM（InternVL3, QwenVL2.5, LLaVA-OV）、闭源API（GPT4o-mini, Gemini-2.5-flash）以及具身VLM（RoboBrain-2.0）。在执行器评估中，对比了多种变体（Vanilla, VLA-OS, 以及使用不同中间表示来源的Oracle/QwenVL+Executor等）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>规划器基准测试</strong>：</p>
<ul>
<li>在第三方具身与 grounding 基准上（表2），RoboInter训练的规划器显著超越了基础VLM和RoboBrain-2.0。例如，RoboInter-Qwen-7B在RoboRefIt上达到85.6%，比RoboBrain-2.0-7B（8.8%）高出76.8个百分点；在RoboVQA上达到74.4，比后者（31.6）高出42.8分。同时，在通用基准上保持了相对稳定的性能。</li>
<li>在本文提出的RoboInter-VQA基准上（表3），当前闭源API和通用VLM在生成空间中间表示（如物体框、抓取功能框）方面表现不佳（大多低于40%），在时间推理（如轨迹生成）上也存在困难。而经过RoboInter-VQA训练的规划器在所有空间和时间任务上都取得了显著提升。例如，在空间生成任务上，RoboInter-LLaVAOV-7B的物体 grounding 准确率达到82.9%（IoU&gt;0.1），远高于基础模型LLaVA-OV-7B的25.8%；在时间轨迹生成任务上，RoboInter-Qwen-7B的动态时间规整（DTW）误差为323，远优于基础QwenVL2.5-7B的1702。</li>
</ul>
</li>
<li><p><strong>执行器开环评估</strong>：</p>
<ul>
<li>在“野外”设置下（表4），使用中间表示指导的变体（RoboInter-Te-Modular, Im-Modular, EC-E2E）均优于不使用中间表示的Vanilla和VLA-OS基线。其中，<strong>RoboInter-Te-Modular</strong>表现最佳，平均开环分数（mOLS）达到0.3543。这证明了中间表示对提升动作生成准确性的有效性。</li>
<li>在“桌面”设置下的训练曲线（图4）显示，所有RoboInter-VLA变体都快速收敛并显著优于Vanilla基线。同样，<strong>RoboInter-Te-Modular</strong>在整个训练过程中保持领先。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2602.09973v1/x4.png" alt="桌面设置开环评估曲线"></p>
<blockquote>
<p><strong>图4</strong>：桌面设置下的开环评估。展示了五种RoboInter-VLA变体在训练步数从1k到40k过程中，<a href="mailto:&#x4f;&#x4c;&#x53;&#64;&#48;&#46;&#48;&#53;">&#x4f;&#x4c;&#x53;&#64;&#48;&#46;&#48;&#53;</a>指标的变化曲线。RoboInter-Te-Modular始终表现最佳。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>表4的实验本质上是对不同VLA范式和使用不同来源中间表示的消融。结果表明：</p>
<ol>
<li>使用<strong>真值（Oracle）中间表示</strong>指导的执行器性能最优（mOLS 0.3861），这为使用预测中间表示的上限提供了参考。</li>
<li>使用<strong>预训练规划器预测的中间表示</strong>（RoboInter-Te/Im-Modular）性能显著优于使用原始通用VLM（QwenVL+Executor）预测的结果，证明了RoboInter-VQA数据对提升规划器生成质量的关键作用。</li>
<li>在端到端变体中，<strong>显式联合优化中间表示与动作</strong>（EC-E2E）优于仅将规划器作为特征提取器（IC-E2E），后者又优于没有规划器的Vanilla。</li>
<li>在模块化变体中，<strong>文本形式</strong>的F-CoT（Te-Modular）略优于视觉提示形式（Im-Modular）。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>大规模高质量中间表示数据集（RoboInter-Data）</strong>：提供了超过23万条片段、涵盖10余个类别、帧对齐的密集中间表示标注，在规模和标注质量上超越了现有工作。</li>
<li><strong>系统性具身VQA基准（RoboInter-VQA）</strong>：构建了涵盖29个空间与时间类别的VQA任务，系统地暴露并评估了VLMs在具身理解和生成方面的能力，并显著提升了规划器的性能。</li>
<li><strong>灵活可扩展的VLA框架（RoboInter-VLA）</strong>：提供了一个支持隐式/显式条件端到端以及模块化三种范式的统一框架，并通过实验证明了高质量中间表示对提升动作生成泛化性和性能的有效性。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>尽管使用了半自动工具，但大规模数据标注仍然成本高昂。</li>
<li>研究主要基于现有数据集的组合，模型在全新场景或机器人形态下的泛化能力仍有待进一步验证。</li>
</ol>
<p><strong>启示</strong>：<br>本文为利用细粒度、多样化中间表示推动机器人学习建立了一个实用的基础。其开源的数据、基准和模型有望促进社区在“先计划后执行”范式、VLM具身能力提升以及VLA模型设计等方面的进一步研究。未来的工作可以探索如何更高效地获取或合成中间表示数据，以及如何将此类框架更好地迁移到未知场景和实体。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文针对机器人操作中现有数据集成本高、本体依赖性强、多样性不足，导致视觉-语言-动作（VLA）模型泛化困难的核心问题，提出了RoboInter中间表示套件。关键技术包括：RoboInter-Tool半自动标注工具、RoboInter-Data大规模数据集（含超过230k个episodes和10+类中间表示）、RoboInter-VQA具身VQA基准（覆盖29个空间与时间类别）以及RoboInter-VLA集成“计划-然后-执行”框架。该套件通过提供细粒度、多样化的中间表示，为推进鲁棒和可泛化的机器人学习奠定了实用基础。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.09973" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>