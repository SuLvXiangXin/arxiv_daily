<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.00807" target="_blank" rel="noreferrer">2602.00807</a></span>
        <span>作者: Fan, Xianzhe, Deng, Shengliang, Wu, Xiaoyang, Lu, Yuxiang, Li, Zhuoling, Yan, Mi, Zhang, Yujia, Zhang, Zhizheng, Wang, He, Zhao, Hengshuang</span>
        <span>日期: 2026/01/31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>现有的视觉-语言-动作（VLA）模型通常以2D图像作为视觉输入，这限制了其在复杂场景中的空间理解能力。为增强空间感知，现有方法主要探索了以下几种途径：1）仅使用RGB输入，但引入经过深度预训练的编码器（如DepthVLA）或空间基础模型编码器（如VGGT）来注入隐式的3D先验；2）将深度图作为额外的图像通道输入（如SpatialVLA）；3）引入点云信息（如PointVLA）。然而，这些方法存在局限性：隐式方法学习的空间特征缺乏精确的度量对齐，在细粒度操作中容易产生空间幻觉；将深度作为2D通道输入则破坏了3D数据的固有拓扑结构；而现有的点云方法受限于缺乏预训练的点云编码器，或仅使用点云输入，或将其与2D输入相对独立地处理。</p>
<p>本文针对3D VLA训练面临的两个关键瓶颈提出了新的视角：1）与海量2D图像数据相比，3D数据极为稀缺；2）来自不同环境（如模拟器、传感器）的3D数据在噪声特性、尺度分布和几何偏差上存在显著差异，导致严重的域差距。本文的核心思路是：通过将视觉输入显式提升至点云空间，并融合来自模拟器、传感器和模型估计的多样化点云进行训练，学习与领域无关的3D表示，以增强VLA的鲁棒性和泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>Any3D-VLA的整体框架是一个可插入现有VLA骨干网络的流程。给定带有可选深度信息的RGB图像，首先将视觉输入提升为点云并进行压缩。随后，一个预训练的点云编码器产生逐点的嵌入。最后，将这些3D嵌入与对应的2D图像块特征进行对齐和融合，并将融合后的表示输入下游的VLA骨干网络。</p>
<p><img src="https://arxiv.org/html/2602.00807v1/figures/teaser_figure.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Any3D-VLA方法整体框架。(a) 训练流程统一了模拟器、传感器和模型估计的点云。(b) 学习与领域无关的3D表示并与2D表示融合。(c) 真实世界环境下的实验结果。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>点云构建与3D压缩</strong>：根据相机内参，将RGB-D像素通过公式 (x=\frac{(u-c_x)d}{f_x}, y=\frac{(v-c_y)d}{f_y}, z=d) 转换为相机坐标系下的3D点。为降低计算成本，在(x, y, z)空间进行与Sonata相同的网格采样：将工作空间划分为固定分辨率的3D网格单元，聚合落入同一单元的点，并仅保留一个代表性点，从而得到更紧凑、空间更均匀的3D网格表示。</li>
<li><strong>视觉编码器</strong>：压缩后的点云输入预训练的点云编码器 (E_{\text{3D}})（本文使用Concerto），输出逐点特征 ({\mathbf{f}<em>{i}^{\text{3D}}})。同时，使用图像编码器 (E</em>{\text{2D}})（如DINOv2+SigLIP）将RGB图像映射为图像块级别的令牌。</li>
<li><strong>补丁级对齐</strong>：将每个3D点 (p_i) 投影回图像平面得到 ((u_i, v_i))，并分配到对应的ViT图像块索引 (a_i)。对于第 (j) 个图像块，聚合所有分配到此块的3D点特征，通过散射平均（scatter-mean）得到块级别的3D特征 (\mathbf{g}_{j}^{\text{3D}})。若某块无对应点，则使用可学习的空令牌 (\mathbf{e}^{\text{3D}})。</li>
<li><strong>2D-3D融合</strong>：将块级3D特征 (\mathbf{g}<em>{j}^{\text{3D}}) 通过线性层投影为3D令牌 (\mathbf{h}</em>{j}^{\text{3D}})。对于每个块 (j)，将其2D令牌 (\mathbf{h}<em>{j}^{\text{2D}}) 和3D令牌拼接后通过一个小型MLP得到残差向量 (\delta_j)。采用门控残差融合策略：(\mathbf{h}</em>{j}^{\text{fused}}=\mathbf{h}_{j}^{\text{2D}}+\sigma(g)\cdot\text{LayerNorm}(\delta_j))，其中 (g) 是可学习的门控标量参数。这确保了3D表示作为对2D表示的修正，而非完全替换。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1) <strong>显式的点云提升与原生3D理解</strong>：不同于将深度作为2D通道或使用隐式先验，该方法在相机坐标系内进行原生3D理解后再投影至2D补丁。2) <strong>混合点云训练策略</strong>：在训练中混合使用模拟器、传感器和模型估计的点云，迫使模型学习对深度来源不敏感的几何模式，从而增强对噪声和尺度偏差的鲁棒性。3) <strong>门控残差融合</strong>：保护了预训练的2D表示，使3D信息以可控的方式增强空间感知。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与基准</strong>：研究在模拟器中合成构建了一个大规模的RGBD预训练数据集，包含290个类别、10,680个实例的物体，在桌面场景生成杂乱布局和专家操作轨迹。同时构建了包含95个独特场景的RGBD评估数据集用于模拟环境验证。真实世界评估则设计了四个挑战性测试集：标准场景、尺度与形状挑战、视角挑战、外观剥夺挑战。</p>
<p><strong>基线方法</strong>：对比的基线包括2D VLA模型 (\pi_{0.5}) 和 GraspVLA，以及3D VLA基线 SpatialVLA。</p>
<p><strong>关键实验结果</strong>：<br>在模拟器的先导研究中（表2），点云-2D补丁融合范式取得了最佳性能，单次尝试成功率（Single-Trial SR）达到61.1%，比次优的RGBD图像平面方法（56.8%）提升4.3%。</p>
<p><img src="https://arxiv.org/html/2602.00807v1/figures/real_world.png" alt="真实世界零样本对比"></p>
<blockquote>
<p><strong>图2</strong>：真实世界零样本对比结果。展示了不同训练设置（仅模拟器Setting 1、混合点云Setting 2）和推理时点云来源（传感器RealSense、模型估计DA3）下的成功率。Any3D-VLA (Setting 2, DA3) 在整体平均成功率上达到62.5%，显著优于所有基线。</p>
</blockquote>
<p>在真实世界零样本评估中，Any3D-VLA在四个挑战场景下均优于所有基线。采用混合点云训练并使用Depth Anything 3（DA3）估计点云推理的设置（Setting 2, DA3）取得了最佳整体平均成功率62.5%，比最强的基线SpatialVLA（33.3%）高出29.2%。结果表明，混合点云训练通常优于仅用模拟器点云训练，且使用现代深度估计模型（DA3）推理有时能取得比真实传感器（RealSense）更好的效果。</p>
<p>在真实世界后训练（fine-tuning）实验中，模型在“移花入瓶”和“放置调料杯”两个新任务上，仅用100条真实演示数据进行微调后，最佳设置下的整体准确率达到了93.3%。</p>
<p><img src="https://arxiv.org/html/2602.00807v1/figures/observation_space.png" alt="观察空间设计"></p>
<blockquote>
<p><strong>图17</strong>：五种不同的观察空间与视觉表示设计示意图，对应先导研究中的五种方法设置。</p>
</blockquote>
<p><strong>消融实验分析</strong>：先导研究（表2和图17）本身构成了一个核心消融实验，系统比较了2D-only、隐式深度RGB、隐式3D RGB、RGBD图像平面和点云-2D补丁融合五种范式，证明了显式点云融合的有效性。此外，训练策略的对比（图2）也表明，混合点云训练（Setting 2）对于缓解域差距、提升在多样点云输入下的鲁棒性至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了<strong>Any3D-VLA框架</strong>，通过将视觉输入提升至点云、压缩并融合2D-3D表示，为VLA注入3D信息提供了一个通用、模块化的方案。2) 针对3D VLA的训练缩放瓶颈和跨环境域差距，提出了<strong>混合点云训练策略</strong>，并构建了用于VLA任务的大规模合成RGBD数据集。3) 在模拟和真实世界进行了广泛评估，证明该方法在多种复杂场景下性能优越，即使在部署时深度输入有噪声或存在尺度偏差也能保持鲁棒。</p>
<p>论文提到的局限性包括：方法依赖于预训练的点云编码器（如Concerto）的性能；点云构建和编码引入了额外的计算成本。</p>
<p>本文对后续研究的启示在于：将点云作为显式的、原生的3D表示与2D视觉特征融合，是增强VLA空间理解的有效路径。通过主动纳入训练与推理时可能遇到的数据多样性（如不同来源、不同质量的点云），可以有效学习领域不变的表示，从而显著提升模型从模拟到真实世界的泛化能力和在实际部署中的鲁棒性。这为如何利用日益丰富的多模态基础模型（2D、3D）和多样化数据源来构建更强大的具身智能体提供了有价值的思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型因依赖2D图像输入而导致空间理解能力受限的问题，提出Any3D-VLA模型。其核心方法是统一模拟器、传感器及模型估计的多源点云数据，构建多样化训练输入，并学习与领域无关的3D表示，进而与2D表示融合。模拟与真实世界实验表明，该方法能有效提升模型性能并缓解由跨环境差异与深度尺度偏差引起的领域差距。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.00807" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>