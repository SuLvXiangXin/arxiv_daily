<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Image Generation as a Visual Planner for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Image Generation as a Visual Planner for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.00532" target="_blank" rel="noreferrer">2512.00532</a></span>
        <span>作者: Pang, Ye</span>
        <span>日期: 2025/11/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作中生成逼真的操作视频是实现具身智能统一感知、规划与行动的关键。当前主流方法依赖于视频扩散模型或动作条件模型，这些方法需要大规模的领域特定机器人数据集，难以泛化到未见过的任务或场景，且其复杂的时间建模导致计算成本高昂、语义理解有限。与此同时，基于语言-图像对训练的现代图像生成模型（如扩散模型）展现出强大的组合生成能力，能够合成空间和语义一致的网格图像，这些网格中的子图像序列常隐含时间连贯性，类似于短视频片段。这表明，即使没有显式的时间建模，图像生成器也可能具备视频式生成的潜在能力，而这种视觉连贯性、组合性和物理合理性正是机器人规划所需的要素。本文针对机器人视频生成对数据与泛化的高要求，提出了一种新视角：将大规模预训练的图像生成模型，通过轻量级适配，转化为机器人操作的视觉规划器。本文的核心思路是：利用预训练图像生成模型强大的组合先验，通过LoRA微调将其适配为可控的视频式合成器，在仅给定第一帧和文本指令或2D轨迹的条件下，生成连贯的机器人操作序列。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的目标是将预训练的图像生成模型转化为一个可控的、视频式的合成器，能够从最少的条件输入生成短的机器人操作序列。整体框架基于一个预训练的扩散Transformer（DiT）骨干网络，通过集成LoRA模块进行高效微调，并支持两种互补的条件策略：基于文本的语义控制和基于轨迹的空间控制。</p>
<p><img src="https://arxiv.org/html/2512.00532v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：方法整体框架。左侧展示了两种条件生成模式：文本条件生成（上支路）使用语言指令和第一帧；轨迹条件生成（下支路）使用在第一帧上渲染的2D路径。两者共享相同的带LoRA适配器的DiT架构。右侧展示了数据合成流程：从机器人视频中采样帧、组装网格、并进行掩码以构建条件监督。</p>
</blockquote>
<p><strong>整体流程</strong>：模型输入是一个部分可见的3x3网格图像，其中仅左上角单元格包含条件帧（原始第一帧或带轨迹叠加的第一帧），其余区域被掩码为零（黑色像素）。该网格经过VAE编码为潜变量后，与条件信息（文本嵌入或轨迹图像）一同输入生成器。生成器（DiT + LoRA）在潜空间进行去噪，最终通过VAE解码器输出预测的完整3x3网格图像，该网格代表了按时间顺序排列的9帧操作序列。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>骨干网络与潜空间</strong>：方法基于FLUX.1-dev（一种大型整流流/Transformer生成器）。图像通过VAE编码器ℰ映射到潜空间，生成器𝒢_θ在潜空间操作，并由语言和/或渲染的轨迹进行条件控制。</li>
<li><strong>LoRA适配</strong>：采用低秩适应（LoRA）技术对冻结的骨干生成器进行参数高效的微调。具体将Transformer块中选定稠密投影矩阵W的更新重参数化为ΔW = α A B^⊤，其中A、B为可训练的低秩矩阵（r &lt;&lt; d），仅需更新O(r d)个参数。LoRA适配器应用于自注意力层的查询和值投影以及前馈网络投影。</li>
<li><strong>条件策略</strong>：<ul>
<li><strong>文本条件生成</strong>：自然语言指令t通过CLIP编码器和T5分词器嵌入，得到e_clip和E_t5。这些嵌入作为条件c_text通过交叉注意力注入生成器。</li>
<li><strong>轨迹条件生成</strong>：2D末端执行器轨迹τ = {(x_s, y_s)}被渲染到第一帧I_1上，早期线段为蓝色，后期为红色以指示时间进程，生成叠加图像I_1^τ。该图像作为条件帧输入模型，此分支无需额外的文本条件。</li>
</ul>
</li>
<li><strong>数据合成流程</strong>：<br><img src="https://arxiv.org/html/2512.00532v1/x3.png" alt="数据合成流程"><blockquote>
<p><strong>图3</strong>：数据合成流程。从每个机器人视频中均匀采样9帧，并按蛇形时间顺序（1→2→3, 6←5←4, 7→8→9）排列成3x3网格。仅左上角单元格保持可见作为条件帧，其他单元格被掩码为零。对于轨迹条件变体，2D末端执行器路径（红色→蓝色）被叠加在第一帧上。</p>
</blockquote>
<ul>
<li><strong>网格组装</strong>：从视频中均匀采样9帧，按蛇形顺序排列成3x3网格。这种排列使得时间相邻的帧在空间上也相邻，有助于Transformer的局部注意力捕捉短程时间依赖性。</li>
<li><strong>监督构建</strong>：模型训练目标是从掩码条件网格重建完整的真实网格，损失函数为潜空间特征的均方误差（MSE）。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与需要专门训练视频模型的主流方法相比，本文的创新在于直接利用大规模预训练图像生成模型已具备的组合与时间先验，通过极轻量的LoRA微调和巧妙的条件输入设计（掩码网格+文本/轨迹），将其“重新定位”为机器人视觉规划器，实现了在数据效率、可控性和泛化性之间的良好平衡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：JacoPlay (1,085个情节)，BridgeData V2 (使用9,967个训练，515个测试)，RT-1 (使用9,067个训练，698个测试)。</li>
<li><strong>对比方法</strong>：主要对比了本文框架内的两种条件模式：文本条件生成与轨迹条件生成。</li>
<li><strong>评估指标</strong>：Fréchet视频距离（FVD↓）、结构相似性（SSIM↑）、均方误差（MSE↓）、成功率（Success↑）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>根据论文表1的主要结果，在JacoPlay数据集上，文本条件生成的FVD为490.7，成功率为80.6%；轨迹条件生成的FVD为503.37，成功率为74.0%。在BridgeV2数据集上，文本条件（FVD 644.2，成功率73.2%）略优于轨迹条件（FVD 693.2，成功率70.9%）。在RT-1数据集上，轨迹条件生成取得了最高的成功率81.7%（FVD 688.1），优于文本条件的72.4%（FVD 698.0）。结果表明，文本条件在语义理解和任务成功率上通常表现更好，而轨迹条件在需要精确空间跟随的任务（如RT-1）上更具优势。</p>
<p><strong>定性结果</strong>：<br><img src="https://arxiv.org/html/2512.00532v1/assets/ablation/ground_truth.png" alt="定性对比"></p>
<blockquote>
<p><strong>图4（定性对比）</strong>：展示了文本条件与轨迹条件生成的示例。文本条件模型擅长语义 grounding（如识别“红杯”），而轨迹条件模型能精确跟随提供的2D末端执行器路径。两者均能生成时间连贯、物理合理的动作序列。</p>
</blockquote>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2512.00532v1/assets/ablation/with_text.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5（消融研究）</strong>：展示了在指令“将锅放在绿梨后面”下的消融结果。顶部为完整模型，底部为移除不同组件的效果。</p>
</blockquote>
<ul>
<li><strong>LoRA适配的影响</strong>：移除LoRA（冻结骨干网络）的模型无法生成连贯的网格，证明通用图像先验不能直接迁移到机器人运动生成，LoRA对于注入任务特定语义至关重要。</li>
<li><strong>文本提示模板的影响</strong>：使用原始数据集指令（无模板）会降低图像质量和指令对齐，机械臂运动变得漫无目的。</li>
<li><strong>轨迹叠加的影响</strong>：移除渲染的2D轨迹会导致生成的动作序列失去空间指导，无法精确跟随预期路径。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>揭示了图像生成模型作为视觉规划器的潜力</strong>：首次系统性地论证了预训练图像生成模型（以其网格图像的组合一致性）可被适配用于生成时间连贯的机器人操作视频，成为一种数据高效的视觉规划器。</li>
<li><strong>提出了双条件视觉规划框架</strong>：设计了一个轻量级、基于LoRA的适配框架，支持文本和轨迹两种互补的条件模式，实现了对生成序列的语义和空间双重控制。</li>
<li><strong>提供了全面的实验验证与洞察</strong>：在多个真实机器人数据集上验证了方法的有效性，并通过消融实验明确了各组件（LoRA、条件形式）的必要性，为利用基础生成模型进行机器人规划提供了实证依据。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：生成的序列长度受限于网格大小（9帧）；方法目前是开环的，生成的视频计划尚未与低级机器人控制器形成闭环以验证其可执行性。</p>
<p><strong>对后续研究的启示</strong>：这项工作为机器人规划开辟了一条新路径，即利用强大的、已存在的视觉生成基础模型。后续研究可以探索：1) 扩展网格规模或引入自回归机制以生成长时程规划；2) 将生成的视频计划与基于模型的强化学习或模仿学习策略相结合，形成闭环系统；3) 探索更多样化的条件输入（如3D轨迹、力觉反馈）以增强规划的可控性和物理真实性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何利用图像生成模型作为机器人操作的视觉规划器，以生成连贯的机器人操作视频，从而统一感知、规划与行动。核心方法是提出一个两分支框架：基于语言指令和首帧的**文本条件生成**，以及基于2D轨迹覆盖和首帧的**轨迹条件生成**；两者均采用预训练的DiT主干网络，并通过**LoRA微调**进行轻量化适配。实验在Jaco Play、Bridge V2和RT1数据集上进行，结果表明，两种生成模式均能输出平滑、连贯且与条件对齐的机器人操作视频序列，验证了预训练图像生成器编码了可迁移的时间先验，能在极少监督下实现类视频的机器人规划。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.00532" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>