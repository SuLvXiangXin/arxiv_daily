<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.12768" target="_blank" rel="noreferrer">2507.12768</a></span>
        <span>作者: Tan, Hengkai, Feng, Yao, Mao, Xinyi, Huang, Shuhe, Liu, Guodong, Hao, Zhongkai, Su, Hang, Zhu, Jun</span>
        <span>日期: 2025/07/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型在机器人操作任务中展现出潜力。然而，这些模型严重依赖为特定任务收集的人类演示数据，这极大地限制了其泛化能力，并带来了高昂的数据采集成本。现有数据集通常是任务特定的，收集于狭窄的操作上下文和机器人构型中，导致其覆盖范围差、行为多样性低，且难以跨任务或领域复用。</p>
<p>本文针对这一根本性的数据瓶颈，提出了“任务无关动作范式”的新视角。其核心思路是将动作执行与任务特定的条件（如语言指令）解耦，从而能够收集可跨任务复用的、与任务无关的动作数据。本文通过一个自动化的数据收集框架（ATARA）和一个专门设计的逆动力学模型（AnyPos），共同构建了一个无需任务监督即可训练、并能泛化到新任务的完整流程。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架旨在从任务无关的动作数据中学习通用的图像到动作的映射。整体流程分为两个核心部分：1）自动化任务无关随机动作（ATARA）数据收集；2）基于此数据训练的逆动力学模型 AnyPos。</p>
<p><img src="https://arxiv.org/html/2507.12768v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：AnyPos方法整体框架。左侧展示了ATARA自动化数据收集流程，通过脚本化策略在机器人双臂的立方工作空间内进行均匀探索，生成大量图像-动作对。右侧展示了AnyPos模型的训练与推理：模型输入为包含机器人手臂的图像，输出为预测的关节位置（动作）值。</p>
</blockquote>
<p><strong>ATARA：自动化任务无关随机动作生成</strong><br>直接进行关节空间随机采样会导致覆盖效率低、行为冗余（如手臂移出视野）以及频繁的自碰撞。ATARA是一个基于强化学习（RL）的框架，用于构建从末端执行器空间到关节空间的覆盖感知映射。</p>
<ol>
<li><strong>单臂工作空间覆盖</strong>：定义可达的末端执行器工作空间为一个有界立方体。目标是学习一个映射 <code>f_RL: W → A</code>，将目标点 <code>w ∈ W</code> 映射到关节位置 <code>a</code>。由于传统逆运动学（IK）在仅给定位置而未给定姿态时可能无解，因此使用PPO算法在仿真环境中训练一个策略 <code>π_θ</code>，其奖励函数为 <code>r(a; w) = -‖EEF_POS(a) - w‖₂²</code>，以鼓励末端执行器接近目标点。</li>
<li><strong>姿态增强</strong>：为了增加运动多样性，从分布 <code>A_wrist</code> 中采样与手腕姿态相关的关节角度，并将其附加到RL预测的配置上，形成增强动作 <code>a_aug = [f_RL(w) ‖ a_wrist]</code>。</li>
<li><strong>双臂扩展与碰撞避免</strong>：扩展到双臂设置时，通过一个虚拟随机边界平面 <code>B</code> 将工作空间划分为左右两个无交集的区域 <code>W_L</code> 和 <code>W_R</code>，以实现碰撞避免。为每只手臂独立采样目标点 <code>w_L</code> 和 <code>w_R</code>，并分别计算关节配置 <code>a_L</code> 和 <code>a_R</code>。</li>
</ol>
<p><strong>AnyPos：用于任务无关数据训练的逆动力学模型</strong><br>从任务无关数据中学习逆动力学模型（IDM）的挑战在于机器人控制所需的细粒度精度，以及分布不匹配和噪声行为。AnyPos通过引入结构先验和架构偏置来解决这些问题。</p>
<ol>
<li><strong>臂解耦估计</strong>：在多臂设置中，共享的视觉特征常导致跨臂纠缠和不准确的预测。AnyPos采用一种基于漫水填充和对称线分割的空间分解算法，来隔离每个手臂的区域并消除不相关的关节维度。这减少了有效假设空间，并将动作预测准确率提高了约20%。</li>
<li><strong>方向感知解码器</strong>：为了进一步注入物理先验（如关节角度和连杆方向），设计了一个解码器，将视觉特征（例如来自DINOv2）与可能的运动方向对齐。这增强了对噪声的鲁棒性，并在下游任务性能上带来了额外的20%提升。</li>
</ol>
<p><strong>整体范式与推理</strong><br>任务特定的动作策略被分解为两个模块：视频生成模型 <code>M_x</code> 和逆动力学模型（IDM）<code>F_δ</code>。<code>M_x</code> 根据初始观测和语言指令生成未来的视觉状态序列，提供任务语义先验。<code>F_δ</code>（即AnyPos）则负责将生成的每一帧图像映射为精确的动作。<code>F_δ</code> 完全在ATARA收集的任务无关数据 <code>D_agnostic = {(x_i, a_i)}</code> 上训练，目标是最小化预测动作与观测动作之间的距离。在推理时，先由视频生成模型根据指令生成未来图像序列，再由AnyPos逐帧预测执行动作。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验设置</strong>：本文新收集了训练数据（ATARA数据），包含61万个图像-动作对，以及一个用于测试的真实世界操作数据集，包含2.5千个图像-动作对。实验平台涉及双手机器人操作。</p>
<p><strong>基线方法</strong>：对比方法包括先前工作中常用的朴素ResNet+MLP架构（在UniPi、UniSim、RoboDreamer、SuSIE等工作中使用）。</p>
<p><strong>关键定量结果</strong>：</p>
<ol>
<li><strong>动作预测准确率</strong>：在包含未见过的技能和物体的测试集上，AnyPos达到了57.13%的动作预测准确率，相比基线方法（ResNet+MLP）提升了51%。</li>
<li><strong>真实世界任务成功率</strong>：在真实机器人回放测试中，AnyPos-ATARA管道实现了92.59%的任务成功率，相比使用人类收集数据集的基线提升了33%，并超过先前方法44%。</li>
<li><strong>下游任务性能</strong>：通过整合基于扩散的视频生成模型与AnyPos进行推理，在搬运篮子、点击、抓放多种物体等操作任务上，取得了比基线高30-40%的成功率。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.12768v1/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：AnyPos模型架构消融实验。展示了完整模型（绿色）相比于移除臂解耦估计（橙色）或方向感知解码器（蓝色）的变体，在动作预测准确率和下游任务成功率上均有显著提升，验证了各核心组件的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.12768v1/x5.png" alt="定性结果1"><br><img src="https://arxiv.org/html/2507.12768v1/x6.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图5与图6</strong>：AnyPos预测动作与真实动作的定性对比。展示了在多种双臂姿态下，AnyPos（红点）预测的关节位置与真实值（蓝点）非常接近，而基线方法（绿点）存在明显偏差。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.12768v1/x7.png" alt="数据收集对比"></p>
<blockquote>
<p><strong>图7</strong>：ATARA自动化数据收集与人工遥操作收集的对比。ATARA在数据收集速度上加速超过30倍，且无需人力成本。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.12768v1/x8.png" alt="下游任务结果"><br><img src="https://arxiv.org/html/2507.12768v1/x9.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图8与图9</strong>：下游操作任务的成功率对比。AnyPos-ATARA（绿色）在搬运、抓放、点击等多个任务上，均显著优于使用人类收集数据训练的基线（蓝色）和其他方法（红色、紫色）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.12768v1/x10.png" alt="视频验证"></p>
<blockquote>
<p><strong>图10</strong>：基于回放的视频动作验证流程示意图。通过将机器人执行动作后的真实观测图像与任务指令对应的目标视频进行对比，来验证学习策略的可行性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.12768v1/x11.png" alt="泛化测试"></p>
<blockquote>
<p><strong>图11</strong>：在包含新物体和新技能组合的测试集上的泛化性能。AnyPos（绿色）相比基线（蓝色）展现出更强的泛化能力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：臂解耦估计和方向感知解码器两个组件对性能提升均有重要贡献，分别带来了约20%的准确率提升和约20%的下游任务性能提升，组合使用时效果最佳。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出任务无关动作范式</strong>：倡导并形式化地定义了一种将动作与任务条件解耦的新范式，为构建可扩展、通用化的具身智能体提供了新的数据基础。</li>
<li><strong>设计ATARA自动化数据收集框架</strong>：利用强化学习实现了高效、安全、覆盖均匀的双臂动作数据自动采集，相比人工遥操作加速超过30倍，且无人力成本。</li>
<li><strong>开发AnyPos逆动力学模型</strong>：针对任务无关数据学习的挑战，提出了包含臂解耦估计和方向感知解码器的模型架构，显著提升了从图像到高精度动作的预测能力与泛化性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前框架依赖于视频生成模型来提供高层任务语义，其生成质量会影响最终性能。此外，方法主要针对关节位置空间的动作预测，对于更复杂的动作空间（如力控）的扩展性有待进一步研究。</p>
<p><strong>对后续研究的启示</strong>：本文证明了从大规模、低成本、无任务标注的数据中学习通用动作先验的可行性。这启发了未来研究可以更专注于开发更高效的无监督数据收集方法，以及设计能够更好融合物理先验和视觉感知的模型架构，从而进一步减少对昂贵任务特定演示数据的依赖，推动通用机器人操作能力的发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双手机器人操作中视觉-语言-动作模型严重依赖任务特定演示、泛化性差且数据成本高的问题，提出了一种任务无关的动作范式。关键技术包括：自动化任务无关随机动作收集框架ATARA，可加速数据收集超30倍；以及逆动力学模型AnyPos，采用臂解耦估计与方向感知解码器来处理数据分布不匹配问题。实验表明，该方案使测试准确率提升51%，在抓取、放置等下游任务中成功率提高30-40%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.12768" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>