<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.03233" target="_blank" rel="noreferrer">2505.03233</a></span>
        <span>作者: Deng, Shengliang, Yan, Mi, Wei, Songlin, Ma, Haixin, Yang, Yuxin, Chen, Jiayi, Zhang, Zhiqi, Yang, Taoyu, Zhang, Xuheng, Zhang, Wenhao, Cui, Heming, Zhang, Zhizheng, Wang, He</span>
        <span>日期: 2025/05/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，具身基础模型，特别是视觉-语言-动作模型，因其零样本泛化、可扩展性和通过少量样本后训练适应新任务的能力而受到关注。然而，现有模型严重依赖通过遥操作等方式收集的真实世界机器人数据，这种数据收集方式成本高昂且劳动密集。合成数据提供了一种经济高效的替代方案，但其潜力在很大程度上未被充分挖掘。本文针对真实动作数据获取困难的痛点，探索了完全使用大规模合成动作数据训练VLA模型的可行性。本文的核心思路是：首先构建一个包含十亿帧的合成抓取数据集SynGrasp-1B，然后提出GraspVLA模型，该模型通过渐进动作生成机制将自回归感知任务与基于流匹配的动作生成统一到一个思维链过程中，从而能够联合训练合成动作数据和互联网语义数据，以实现开放词汇的抓取泛化。</p>
<p><img src="https://arxiv.org/html/2505.03233v3/x1.png" alt="GraspVLA概览"></p>
<blockquote>
<p><strong>图1</strong>：GraspVLA是一个完全基于十亿规模合成动作数据预训练的抓取基础模型，并与互联网语义数据协同训练。它展示了直接的仿真到现实迁移能力，以及在多个方面的强大零样本泛化能力，以及对特定场景和人类偏好的少量样本适应性。</p>
</blockquote>
<h2 id="方法详解">方法详解</h2>
<p>GraspVLA的整体架构集成了一个视觉语言模型和一个动作专家，并通过提出的渐进动作生成机制连接。</p>
<p><img src="https://arxiv.org/html/2505.03233v3/x3.png" alt="模型架构"></p>
<blockquote>
<p><strong>图3</strong>：GraspVLA由一个自回归视觉语言主干和一个基于流匹配的动作专家组成。它通过渐进动作生成机制利用了互联网定位数据与合成动作数据之间的协同作用：模型首先为合成数据和网络数据预测目标物体的2D边界框，并额外为合成数据生成抓取位姿和分块动作。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉语言模型</strong>：采用可训练的大语言模型（InternLM2 1.8B），视觉编码器融合了冻结的DINO-v2和SigLIP的特征，并包含一个可训练的、从视觉空间到语言空间的投影器。</li>
<li><strong>动作专家</strong>：使用条件流匹配模型，用于生成精细的末端执行器动作。</li>
<li><strong>渐进动作生成</strong>：这是模型的核心创新机制。它将图像定位和抓取位姿预测作为生成动作的中间步骤，形成一个统一的思维链推理过程。具体而言：<ul>
<li>VLM被训练为以统一格式为互联网定位数据集和合成动作数据集生成2D边界框。</li>
<li>对于合成数据集，VLM进一步预测机器人基座坐标系下的目标抓取位姿。</li>
<li>最后，动作专家以VLM对输入和中间推理token的键值缓存为条件，生成动作块。</li>
<li>为了促进精确的3D感知，将最近两个时间步的本体感觉进行分词，并在生成抓取位姿前插入。</li>
<li>为了对齐互联网数据与SynGrasp-1B的双摄像头设置，输入图像被复制以匹配视角数量，并分别进行随机缩放、裁剪、水平翻转和颜色抖动增强。两个数据集共享相同的文本提示模板，首先生成边界框token。</li>
</ul>
</li>
</ol>
<p><strong>联合训练策略</strong>：<br>在每个批次中，随机从互联网数据集（GRIT）和合成动作数据集中采样。前者仅用于以自回归方式监督VLM的边界框预测。后者则监督边界框、抓取位姿和基于流匹配的动作预测。</p>
<ul>
<li>VLM的损失函数 ℒ_S2 是边界框和抓取位姿（仅对合成数据）预测的负对数似然之和。</li>
<li>动作专家的损失函数 ℒ_S1 是流匹配损失，作用于分块的末端执行器动作增量。</li>
<li>整体损失是 ℒ_S2 和 ℒ_S1 的简单求和。</li>
</ul>
<p><strong>与现有方法的创新点</strong>：<br>与现有VLA模型主要依赖真实数据或需要跨具身预训练不同，GraspVLA的创新在于：1）完全基于大规模合成动作数据进行预训练，极大降低了真实数据获取负担；2）通过PAG机制，将互联网语义知识与合成几何信息有机结合，使模型能够将学习到的抓取技能迁移到合成数据中未出现但互联网覆盖的新物体类别上，实现了开放词汇泛化。</p>
<p><img src="https://arxiv.org/html/2505.03233v3/x2.png" alt="数据生成流程"></p>
<blockquote>
<p><strong>图2</strong>：SynGrasp-1B数据集生成流程：从Objaverse筛选物体网格并随机布局（左）；使用CuRobo规划随机化抓取位姿和指令的轨迹（中）；对材质、光照、相机视角和背景进行领域随机化并渲染轨迹（右）。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/基准</strong>：使用了自行构建的SynGrasp-1B合成数据集（240个类别，10,680个实例，十亿帧）进行预训练。评估在真实世界和LIBERO仿真基准上进行。</li>
<li><strong>真实世界评估</strong>：使用Franka Panda机械臂和两个RealSense相机。测试对象分为“合成类别”（存在于SynGrasp-1B）和“网络类别”（仅存在于互联网数据），并设置了基础、光照、背景、干扰物、高度五种测试条件。</li>
<li><strong>对比基线</strong>：包括VLA通用模型（π₀, OpenVLA, Octo）和模仿学习专家模型（Diffusion Policy）。所有基线都在SynGrasp-1B上进行了微调以确保公平对比。还与最先进的抓取检测模型AnyGrasp进行了对比。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>零样本真实世界对比</strong>：如表1所示，GraspVLA在所有测试集上均取得了约90%的成功率，显著优于所有基线，展示了强大的零样本泛化能力。特别是在网络类别上也能达到与合成类别相当的性能，证明了PAG的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03233v3/x4.png" alt="真实世界实验设置"></p>
<blockquote>
<p><strong>图4</strong>：真实世界实验设置(a)、使用的物体(b,c)以及对应的五种测试条件(d)。</p>
</blockquote>
<ol start="2">
<li><strong>LIBERO基准对比</strong>：如表2所示，GraspVLA在LIBERO基准上的零样本性能超过了经过微调的π₀和OpenVLA基线，展示了强大的泛化性。</li>
<li><strong>与AnyGrasp对比</strong>：如表3所示，在语言引导和任意抓取任务中，GraspVLA性能稳定（整体成功率93.3%）。AnyGrasp在抓取常见物体时成功率更高（100%），但在透明物体上表现很差（10%），而GraspVLA则保持了稳健的性能（86.6%）。GraspVLA的推理速度（5Hz）慢于AnyGrasp（37Hz）。</li>
<li><strong>数据缩放规律</strong>：如图5所示，模型性能随训练帧数增加而稳步提升，网络类别的性能提升速度慢于合成类别，表明需要对网络类别进行更多训练才能获得良好的泛化。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03233v3/x5.png" alt="性能缩放曲线"></p>
<blockquote>
<p><strong>图5</strong>：真实世界性能随训练帧数增加的缩放曲线，网络类别的性能提升速度慢于合成类别。</p>
</blockquote>
<ol start="5">
<li><strong>高效后训练</strong>：设计了三个下游任务（抓取新工业部件、避免触碰杯子内部、密集环境顺序抓取）来测试模型的少量样本适应性。如表4所示，GraspVLA仅使用边界框标注在任务1上就达到了90%的成功率，优于使用完整动作数据训练的基线。在任务3中，GraspVLA学会了有效避免与周围物体碰撞。从头开始训练性能较低，突显了合成预训练的价值。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03233v3/x6.png" alt="后训练任务"></p>
<blockquote>
<p><strong>图6</strong>：真实世界后训练任务，展示了模型快速学习抓取新物体(a)、新抓取模式(b)和新抓取行为(c)的能力。</p>
</blockquote>
<ol start="6">
<li><strong>消融实验</strong>：如表5所示，逐步引入PAG机制（先2D边界框，再3D抓取位姿）显著提升了性能，特别是在网络类别上。完整的PAG-3D设计大幅减少了犹豫行为，提高了抓取精度和运动效率（SPL得分更高）。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种全新的、完全依赖合成动作数据的VLA模型预训练范式，显著减轻了真实世界动作数据的获取负担。</li>
<li>构建了全球首个十亿帧规模的机器人抓取数据集SynGrasp-1B。</li>
<li>提出了渐进动作生成机制，能够协同训练合成动作与互联网数据，将模型的抓取技能扩展到新的物体类别。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，GraspVLA的推理速度显著慢于专门的抓取检测模型（如AnyGrasp），这与其大型视觉语言主干有关。此外，虽然预训练完全使用合成数据，但对新任务进行少量样本后训练时，仍然需要收集一些真实世界的示范数据。</p>
<p><strong>启示</strong>：<br>这项工作证明了大规模、高质量合成数据在训练具身基础模型方面的巨大潜力，为突破真实数据瓶颈提供了新路径。PAG机制展示了如何通过结构化思维链将不同模态和来源的数据有效融合，以提升模型的语义理解和技能泛化能力。这启发后续研究可以进一步探索更高效的模型架构以提升推理速度，以及开发更强大的仿真工具来生成涵盖更复杂技能和物理交互的合成数据。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决机器人抓取基础模型依赖昂贵真实数据的问题，探索完全利用合成数据进行训练。提出十亿帧合成抓取数据集SynGrasp-1B，并构建视觉-语言-动作模型GraspVLA。关键技术包括：通过自回归感知与基于流匹配的动作生成统一为思维链，实现合成动作数据与互联网语义数据的联合训练，以缩小仿真-现实差距并支持开放词汇泛化。实验表明，GraspVLA在真实与仿真基准上展现出先进的零样本泛化能力及对特定人类偏好的少样本适应性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.03233" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>