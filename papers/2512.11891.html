<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.11891" target="_blank" rel="noreferrer">2512.11891</a></span>
        <span>作者: Hu, Songqiao, Liu, Zeyi, Liu, Shuang, Cen, Jun, Meng, Zihan, He, Xiao</span>
        <span>日期: 2025/12/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过将视觉编码、语言理解和动作控制统一到端到端框架中，在机器人操作任务上展现出卓越的泛化能力。然而，将这些模型部署在非结构化环境中仍然充满挑战，核心问题在于缺乏物理安全保障，模型可能在遇到分布外场景时产生不安全的轨迹，导致碰撞风险。现有方法主要通过强化学习将安全约束整合到训练过程中，但这类基于重新训练的方法计算成本高昂，且通常将安全作为奖励惩罚的软目标，而非具有明确边界条件的硬约束，无法在推理时提供确定性的安全保证。</p>
<p>本文针对现有VLA模型缺乏严格安全保证这一具体痛点，提出了一个名为AEGIS的视觉-语言-安全动作架构。其核心思路是在现有VLA模型之上，引入一个基于控制屏障函数的可插拔安全约束层，该层仅在检测到潜在安全违规时动态调整原始动作，从而在保持任务执行意图的同时，提供数学上严格证明的安全保证，且无需对基础VLA模型进行重新训练。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLSA模型的整体架构在传统VLA模型（包含视觉编码器、语言编码器、多模态融合层和动作解码器）的基础上，增加了一个位于原始VLA动作输出之后的安全约束层。该层接收视觉特征、语言特征和基础VLA模型的动作输出。当未识别安全风险时，SC层输出与原始VLA输出相同；当检测到风险时，SC层将修改潜在的不安全动作为安全替代方案。</p>
<p><img src="https://arxiv.org/html/2512.11891v1/figs/fig2b.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2b</strong>：VLSA模型的功能架构。相较于传统VLA模型，VLSA在动作解码器后增加了一个安全约束层，用于根据视觉和语言信息评估风险并调整动作。</p>
</blockquote>
<p>本文提出的AEGIS具体包含两个功能模块：基于视觉语言的安全评估模块和动作驱动的安全保证控制模块。</p>
<p><img src="https://arxiv.org/html/2512.11891v1/figs/fig3.png" alt="工作流程"></p>
<blockquote>
<p><strong>图3</strong>：AEGIS模型的工作流程。左侧为安全评估模块，通过VLM识别危险物体并定位；右侧为安全保证控制模块，通过椭球拟合和CBF-QP求解器生成安全动作。</p>
</blockquote>
<p><strong>1. 基于视觉语言的安全评估模块</strong><br>该模块旨在主动识别和定位可能干扰机械臂轨迹的障碍物，包含语义级障碍物识别和精确空间定位两个阶段。</p>
<ul>
<li><strong>语义识别</strong>：使用一个经过精心设计的提示词，引导视觉语言模型结合任务指令和代理视角的RGB图像，推理并输出最可能阻碍机器人运动的唯一物体名称（如“牛奶盒”）。</li>
<li><strong>空间定位</strong>：利用开放集视觉语言基础模型GroundingDINO，根据识别出的物体名称在图像中定位对应的边界框。随后，结合来自代理视角和背视角相机的RGB-D数据，通过坐标变换将2D边界框内的像素反投影到3D空间，融合生成障碍物的局部点云。点云经过预处理（空间约束、去除离群点、聚类）后，得到代表障碍物主体的干净点云数据。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.11891v1/figs/fig4.png" alt="安全评估流程"></p>
<blockquote>
<p><strong>图4</strong>：基于视觉语言的安全评估模块流程。从指令和图像输入，经过VLM推理、开放集检测、点云投影与融合，最终输出处理后的障碍物3D点云。</p>
</blockquote>
<p><strong>2. 动作驱动的安全保证控制模块</strong><br>该模块将安全执行问题转化为两个椭球体之间的碰撞避免。</p>
<ul>
<li><strong>椭球拟合</strong>：首先，采用最小体积包围椭球方法，分别拟合处理后的障碍物点云和机械臂末端执行器。对于障碍物，通过求解一个优化问题得到其椭球参数（中心<code>c</code>、形状矩阵<code>Q</code>、旋转矩阵<code>R</code>）。末端执行器同样被建模为一个椭球，其中心位置与末端执行器位姿相关联。</li>
<li><strong>控制屏障函数构建与求解</strong>：定义一个包含末端执行器椭球位姿和虚拟辅助状态<code>p_s</code>的增广状态向量<code>x</code>。基于两个椭球的几何关系，构造一个可微函数<code>h(x)</code>，其值大于0表示安全状态。该函数被证明是一个有效的CBF。安全控制问题被表述为一个二次规划问题：在满足CBF约束 <code>ḣ(x) ≥ -α(h(x))</code> 的条件下，寻找与VLA模型输出的名义动作<code>u_vla</code>偏差最小的安全动作<code>u_safe</code>。通过实时求解此QP问题，即可在理论上保证调整后动作的安全性（满足前向不变性）。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.11891v1/figs/fig5a.png" alt="椭球拟合"></p>
<blockquote>
<p><strong>图5</strong>：末端执行器和障碍物的MVEE拟合结果可视化。将复杂的几何形状简化为椭球，便于进行高效的碰撞检测和基于CBF的安全控制。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法相比，AEGIS的主要创新在于：1) <strong>可插拔性</strong>：无需重新训练基础VLA模型，可直接增强其安全性；2) <strong>硬约束与理论保证</strong>：通过CBF-QP框架将安全作为硬约束强制执行，并提供数学上的严格安全证明；3) <strong>语义感知的安全评估</strong>：利用VLM进行与任务上下文相关的风险评估，而非将所有物体 indiscriminately 视为障碍。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准</strong>：构建了SafeLIBERO基准，它源自LIBERO数据集，包含Spatial、Object、Goal、Long四个套件，每个套件选4个任务，每个任务又根据障碍物干预程度分为I级（靠近目标）和II级（阻碍移动）两种场景，共32个场景、1600个测试回合。</li>
<li><strong>基线方法</strong>：以先进的流匹配VLA模型<code>π_0.5</code>作为基础策略，对比方法包括：原始<code>π_0.5</code>、经过在线微调的Transformer-based VLA变体OpenVLA-OFT。</li>
<li><strong>评估指标</strong>：碰撞避免率、任务成功率、平均执行时间步数。</li>
<li><strong>平台</strong>：使用Franka Emika Panda机械臂，在Robosuite仿真环境中控制频率为20Hz。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.11891v1/figs/fig6.png" alt="基准概览"></p>
<blockquote>
<p><strong>图6</strong>：SafeLIBERO基准任务概览。展示了来自四个套件（Spatial, Goal, Object, Long）的不同难度级别（Level I, Level II）的示例场景。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>AEGIS在安全性和任务成功率上均显著优于基线。在SafeLIBERO所有任务上，AEGIS实现了<strong>59.16%</strong> 的碰撞避免率提升（从基线的33.00%提升至92.16%），同时任务成功率也获得了<strong>17.25%</strong> 的提升（从基线的69.50%提升至86.75%）。尽管引入了安全约束，AEGIS的平均执行时间步数与原始<code>π_0.5</code>相当，显著优于OpenVLA-OFT，表明其效率更高。</p>
<p><img src="https://arxiv.org/html/2512.11891v1/figs/all_tasks2.png" alt="总体结果"></p>
<blockquote>
<p><strong>图16</strong>：所有任务上的总体性能对比。AEGIS在碰撞避免率和任务成功率上均大幅领先于<code>π_0.5</code>和OpenVLA-OFT。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.11891v1/figs/spatial2.png" alt="分套件结果"> <img src="https://arxiv.org/html/2512.11891v1/figs/goal2.png" alt="分套件结果"> <img src="https://arxiv.org/html/2512.11891v1/figs/object2.png" alt="分套件结果"> <img src="https://arxiv.org/html/2512.11891v1/figs/long2.png" alt="分套件结果"></p>
<blockquote>
<p><strong>图17-20</strong>：在Spatial、Goal、Object、Long四个任务套件上的详细性能对比。AEGIS在各个套件、不同难度级别上均 consistently 表现出更高的CAR和TSR。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文通过消融研究分析了各组件贡献：</p>
<ol>
<li><strong>移除安全约束层</strong>：直接使用原始VLA动作，导致碰撞避免率急剧下降至基线水平，验证了SC层的必要性。</li>
<li><strong>移除安全评估模块（使用真实障碍物边界框）</strong>：性能略有下降，表明基于VLM的语义风险评估模块能有效识别最相关障碍物，提升整体安全性和任务效率。</li>
<li><strong>移除点云融合（仅使用单视角）</strong>：由于遮挡导致障碍物形状不完整，碰撞避免率下降，证明了多视角点云融合对精确3D重建的重要性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了AEGIS，这是首个将控制屏障函数集成到VLA模型中以强制执行显式安全约束的方法。其可插拔的安全约束层设计，能够无需重新训练即可增强现有VLA模型的安全性。</li>
<li>设计了一个基于视觉语言的安全评估模块和一个动作驱动的安全保证控制模块，搭建了连接视觉感知、语义理解与安全保证控制的桥梁，并通过CBF-QP求解器为调整后的动作提供了理论安全保证。</li>
<li>建立了一个全面的安全关键基准SafeLIBERO，用于系统评估VLA模型在复杂环境中的安全性能。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，该框架的安全保证依赖于几个前提：准确的安全评估、精确的点云过滤以及完整的障碍物表示（即生成的MVEE严格包围障碍物和机器人末端执行器）。在实际应用中，感知模块的误差可能会影响最终的安全性能。此外，实时求解QP问题可能带来一定的计算开销。</p>
<p><strong>启示</strong>：<br>AEGIS的“可插拔”安全层范式为将具有理论保证的传统控制方法（如CBF）与数据驱动的端到端VLA模型结合提供了可行路径。这种设计允许独立改进感知、规划与控制模块。未来研究可以探索更高效、更鲁棒的感知-安全控制集成方法，以及将此类框架推广到更复杂的动态环境和多智能体场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VLSA架构（AEGIS），旨在解决视觉-语言-动作模型在非结构化环境中部署时缺乏安全保障、易发生碰撞的问题。其核心是设计了一个基于控制屏障函数的即插即用安全约束层，可直接集成于现有VLA模型，在提供理论安全保证的同时保持原有任务性能。在构建的安全关键基准SafeLIBERO上的实验表明，该方法避障率提升59.16%，任务执行成功率提高17.25%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.11891" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>