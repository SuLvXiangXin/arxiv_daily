<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Fully Spiking Actor-Critic Neural Network for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Fully Spiking Actor-Critic Neural Network for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.12038" target="_blank" rel="noreferrer">2508.12038</a></span>
        <span>作者: Guanghui Sun Team</span>
        <span>日期: 2025-08-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，脉冲神经网络因其基于稀疏脉冲信号的通信、事件驱动计算和显著降低的能耗等优势，在计算效率至关重要的领域受到关注。然而，在复杂的机器人控制任务中部署SNN仍然有限，一个核心困难在于脉冲激活函数的不可微性，阻碍了传统反向传播技术的直接应用。虽然已有研究将SNN与强化学习结合，但针对高维控制任务的全脉冲Actor-Critic架构研究仍显不足，特别是在处理复杂多阶段任务方面缺乏系统性探索。</p>
<p>本文针对SNN在复杂、多阶段机器人操作任务中应用不足的痛点，提出将课程强化学习整合到SNN中的新视角。核心思路是提出一个基于浅层全SNN架构的混合课程强化学习框架，通过动态两阶段奖励调整机制和优化的观测空间，引导智能体从简单到复杂渐进学习，以高效控制机械臂完成目标接近和抓取任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了一种用于连续动作任务的脉冲神经控制架构。整体框架是一个结合了课程学习的混合PPO架构。</p>
<p><img src="https://arxiv.org/html/2508.12038v1/framework.drawio.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：课程引导的混合PPO架构设计。左侧展示了SNN结构：输入层使用LIF神经元将连续观测编码为脉冲，输出层使用非脉冲N-LIF神经元产生连续的动作和值估计。右侧展示了训练流程：环境交互产生奖励，通过PPO算法和课程学习策略更新SNN的权重。</p>
</blockquote>
<p>该框架采用极简的SNN架构，仅包含输入层和输出层。输入层使用漏电积分发放神经元，直接将连续的环境观测值（视为输入电流）通过可训练权重调制，并内部生成脉冲信号，无需显式的编码阶段。输出层由非脉冲LIF神经元组成，根据其膜电位产生用于动作选择和值估计的连续信号。这种设计实现了从连续输入到连续输出的无缝映射，并以脉冲动力学作为中间计算。</p>
<p>训练方法采用基于PPO的策略梯度方法，并集成了课程学习和动态奖励塑形。核心创新在于一个精心设计且随时间变化的复合密集奖励函数，以及动态调整奖励权重的课程策略。总奖励r_t由接近对齐奖励、夹爪几何奖励、任务完成奖励、姿态稳定性奖励和姿态惩罚项加权求和构成（公式3）。其中，接近对齐奖励（公式4）旨在最小化夹爪与目标物体之间的相对距离和平面位移；夹爪几何奖励（公式5）促进手指对称性和适当的抓取宽度；任务完成奖励在成功抓取时提供高额奖励；姿态奖励（公式6）和惩罚（公式7）则鼓励末端执行器保持垂直稳定姿态并抑制错位。</p>
<p><img src="https://arxiv.org/html/2508.12038v1/reward5.2.drawio.png" alt="课程奖励结构"></p>
<blockquote>
<p><strong>图2</strong>：CRL框架内的动态奖励权重用于分阶段机器人操作训练。实线箭头表示每个阶段优先考虑的奖励成分，虚线箭头表示在整个训练过程中保持但权重降低的成分。训练分为两个阶段：第一阶段（t &lt; T1）侧重空间感知（接近、对齐、姿态），第二阶段（t ≥ T1）侧重抓取策略强化（夹爪几何、任务完成）。</p>
</blockquote>
<p>课程策略将训练过程动态地划分为两个阶段，并调整各奖励分量的权重α_i(t)， β_i(t)， γ(t)， δ_i(t)。第一阶段侧重空间接近、对齐精度和姿态稳定性；第二阶段则强调抓取相关的几何对称性和任务完成。此外，论文还提出了一种死区感知重加权机制：当智能体在接近目标后发生策略回退（距离增加且未成功抓取）时，系统临时增加对齐相关奖励的权重，降低抓取特定奖励的权重，以实现局部策略校正。</p>
<p>另一个核心贡献是引入了统一的能量消耗分析模型，以公平比较SNN和ANN。该模型不仅计算操作数，还结合了神经元层的稀疏性分析。对于SNN，计算了输入层的平均脉冲发放率r（公式8）和输出层的膜电位激活率r_mem（公式9），总能耗E_SNN由公式10估算。对于ANN，则计算了输入和输出层的ReLU激活率r_in和r_out（公式11），总能耗E_ANN由公式12估算。其中考虑了乘法（α_m = 4.6 pJ）和加法（α_a = 0.9 pJ）的能量成本。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Isaac Gym仿真平台上进行，使用一个7自由度Franka Emika Panda机械臂和2自由度平行夹爪构成的9自由度系统，执行目标接近和抓取任务。训练使用了8，192个并行环境以加速。</p>
<p><img src="https://arxiv.org/html/2508.12038v1/isaac_two.png" alt="并行训练可视化"></p>
<blockquote>
<p><strong>图3</strong>：Isaac Gym中大规模并行训练的可视化。左侧是智能体与目标交互的特写，右侧展示了8192个并行环境的俯视图。</p>
</blockquote>
<p>对比的基线方法包括：1）使用相同浅层架构（仅输入输出层）的ANN；2）在标准PPO（Vanilla RL）和本文提出的CRL+动态奖励两种训练框架下的性能。评估指标为抓取成功率。</p>
<p><img src="https://arxiv.org/html/2508.12038v1/isaac_one.png" alt="抓取成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：ANN和SNN在两种不同训练框架下的抓取成功率。左图是标准PPO（Vanilla RL）设置下的结果，右图是CRL框架结合动态奖励策略下的结果。阴影区域表示±0.2标准差，红色标记表示每种方法的峰值成功率。</p>
</blockquote>
<p>关键实验结果如下：在标准PPO框架下，SNN智能体逐步提升，在40，000步时达到约45%的平均成功率，而ANN智能体早期上升后停滞在15-19%。在CRL框架下，两种模型性能均有显著提升，但SNN表现更优：SNN在前20，000步内成功率超过70%，最终稳定在80%左右；ANN则停滞在约50%。这表明浅层SNN在相同设置下，其收敛性和最终性能均优于可比的ANN架构，而CRL策略为SNN带来了更持续和显著的收益。</p>
<p><img src="https://arxiv.org/html/2508.12038v1/exercise_one_three_average_final20250530.png" alt="奖励轨迹对比"></p>
<blockquote>
<p><strong>图5</strong>：标准PPO和CRL下的奖励轨迹。采用CRL训练的SNN显示出更快的奖励提升，并获得了比其他模型更高、更稳定的最终奖励。曲线是10次独立训练运行的平均值。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.12038v1/exercise_two_four_average_final20250530.png" alt="各奖励分量演化"></p>
<blockquote>
<p><strong>图6</strong>：训练过程中各奖励分量的演化。虚线标志着在2000步时训练进入第二阶段，对应于课程学习下奖励侧重点的转变。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.12038v1/reward_average_final2.png" alt="平均奖励热图"></p>
<blockquote>
<p><strong>图7</strong>：训练过程中平均奖励的热图，直观展示了不同阶段奖励的集中趋势。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.12038v1/reward_heatmap_custom_bluered2.png" alt="奖励分量热图"></p>
<blockquote>
<p><strong>图8</strong>：各奖励分量随训练步数变化的热图，清晰显示了课程学习两个阶段之间奖励权重的动态调整。</p>
</blockquote>
<p>消融实验通过对比标准PPO和CRL框架的结果，体现了课程学习和动态奖励调整机制的核心贡献。该机制显著提升了学习效率和最终策略性能，特别是对于SNN智能体。</p>
<p>能量分析结果显示，在相同任务和架构下，SNN的输入层脉冲发放率r为0.31，输出层膜激活率r_mem为1；而ANN的输入层激活率r_in为1，输出层激活率r_out为0.48。根据能量模型估算，SNN的最终能耗为38.49 mJ，ANN为122.23 mJ，SNN实现了超过68.5%的能耗节省。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了一个用于机器人操作的全脉冲Actor-Critic课程强化学习框架，该框架采用极简的浅层SNN，并结合了动态两阶段奖励调整和死区感知机制，有效解决了多阶段任务的学习难题。2）引入了一个统一的能量消耗分析建模框架，综合考虑操作类型和神经元激活稀疏性，为SNN与ANN的公平能效比较提供了量化方法。3）在Isaac Gym上的实验表明，所提方法在任务成功率、学习稳定性方面优于相同深度的ANN基线，并在理论上实现了超过一个数量级的推理能耗降低，验证了SNN在能效敏感机器人控制中的潜力。</p>
<p>论文自身提到的局限性在于，未来工作将集中于将框架扩展到更多操作任务，并部署到神经形态硬件上。</p>
<p>本研究的启示在于：极简的浅层SNN架构结合先进的训练策略（如课程学习和动态奖励塑形），能够在保持高性能的同时，显著降低模型复杂度和能耗，这为在资源受限的边缘设备上部署高效的机器人控制策略提供了新思路。同时，论文提出的公平能量评估模型为未来SNN与ANN的能效对比研究提供了可借鉴的范式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对资源受限环境下9自由度机械臂的目标到达与抓取任务，提出一种基于完全脉冲神经网络（SNN）的混合课程强化学习框架。核心方法包括简化SNN为仅输入输出层以降低复杂度，集成时间进度分区课程策略与近端策略优化（PPO）算法，并引入能量消耗建模框架及动态两阶段奖励调整机制。在Isaac Gym仿真平台的实验表明，该方法相比传统PPO和人工神经网络基线，在现实物理约束下实现了性能优越、可扩展且能量高效的控制。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.12038" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>