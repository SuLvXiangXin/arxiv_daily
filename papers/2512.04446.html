<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.04446" target="_blank" rel="noreferrer">2512.04446</a></span>
        <span>作者: Minghui Zheng Team</span>
        <span>日期: 2025-12-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人拆解流程通常划分为多个独立阶段：感知、序列规划、任务规划、运动规划和操作。每个阶段都需要专门的模型，这种分离式设计限制了系统对陌生场景的泛化能力，并可能导致误差累积。视觉-语言-动作模型提供了一种端到端的替代方案，直接将高级视觉感知和自然语言指令映射为机器人动作，有望简化流程。尽管VLA模型在简单的日常任务中表现出色，但其在需要高灵巧性和精度的复杂工业拆解场景（如电子废物拆解）中的可行性尚未被探索。本文针对这一具体痛点，以从报废台式机中拆解关键组件（RAM和CPU支架）为案例，评估微调后的VLA模型在复杂、接触丰富的长周期操作任务中的性能与局限性。核心思路是：收集专用的拆解演示数据集，对现有VLA模型进行微调，并通过实验分析其在拆解任务各子步骤中的表现，最终探讨结合基于规则控制的混合策略的可行性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本研究旨在评估VLA模型在复杂拆解任务中的潜力，整体流程包括：1）构建遥操作平台并收集拆解演示数据；2）处理数据以适配模型微调；3）微调选定的VLA模型；4）在真实拆解场景中测试模型性能，并分析失败原因。</p>
<p><img src="https://arxiv.org/html/2512.04446v1/x2.png" alt="传统与端到端方法对比"></p>
<blockquote>
<p><strong>图2</strong>：传统多阶段拆解方法与端到端视觉-语言-动作方法的对比。左侧传统方法将流程分解为多个独立模块，右侧VLA方法旨在通过单一模型实现从感知和语言指令到动作的端到端映射。</p>
</blockquote>
<p>核心模块是经过微调的两个VLA模型：OpenVLA和其改进版OpenVLA-OFT。OpenVLA是一个基于Llama2的70亿参数操作策略，它将连续机器人动作离散化为token，并通过交叉熵损失进行下一个token预测。推理时，它接收单视角图像和语言指令，自回归地预测下一时间步的7个离散动作token（6个关节和1个夹爪）。OpenVLA-OFT则进行了多项改进：<strong>输入方面</strong>，除了第三人称视角图像，还额外提供了手腕视角图像和机器人本体感知状态；<strong>输出方面</strong>，通过一个独立的动作头将隐藏状态映射为归一化的连续动作，并使用L1回归损失进行监督；<strong>推理机制</strong>，支持动作分块，能够预测并执行一系列未来动作，而OpenVLA只能以自回归方式生成单步动作。</p>
<p><img src="https://arxiv.org/html/2512.04446v1/x3.png" alt="数据收集设置"></p>
<blockquote>
<p><strong>图3</strong>：用于模仿数据收集的遥操作设置。展示了UR5e机器人、带有扩展的Robotiq 2F-85夹爪，以及用于提供基础视角和手腕视角的两个RGB-D相机（OAK-D和RealSense D435i）。</p>
</blockquote>
<p>为收集高质量数据，研究构建了基于Gello系统的遥操作平台。使用10台不同配置的台式机，共录制了287条轨迹（RAM移除164条，CPU支架解锁123条）。数据采集时，两个相机以30Hz频率异步记录图像和深度信息，机器人状态以约48Hz的频率记录。</p>
<p><img src="https://arxiv.org/html/2512.04446v1/x4.png" alt="数据处理流程"></p>
<blockquote>
<p><strong>图4</strong>：用于微调模型的数据处理步骤以及VLA模型在实验任务中的使用流程。展示了从原始异步数据到对齐、下采样、图像缩放，最终打包成可用于训练的数据文件的过程。</p>
</blockquote>
<p>数据处理的关键步骤包括：将机器人状态下采样以与基础相机帧时间戳对齐，移除冗余状态，对齐手腕相机图像，并将所有数据合并。RGB图像被统一缩放至224x224分辨率以满足模型输入要求。模型微调采用LoRA技术进行高效适配。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在两个选定的拆解任务上评估模型：RAM模块移除和CPU支架解锁。测试环境尽可能复现数据收集时的设置。对比的基线是微调后的OpenVLA和OpenVLA-OFT模型。</p>
<p><strong>关键结果</strong>：实验发现，单独使用任何一个微调后的VLA模型都无法完成完整的拆解任务。然而，模型能够执行部分步骤。例如，在RAM任务中，OpenVLA能在12/20次试验中与RAM对齐，13/20次中接近目标；OpenVLA-OFT表现更好，对齐和接近成功率均为16/20。但两者在精确定位（Localization）步骤上成功率很低（OpenVLA: 4/20， OpenVLA-OFT: 7/20），且均无法完成最终的“驱动”步骤（Actuation，即实际拔出或解锁）。</p>
<p><img src="https://arxiv.org/html/2512.04446v1/x5.png" alt="训练损失"></p>
<blockquote>
<p><strong>图5</strong>：两个VLA模型在两个拆解任务上的训练损失曲线。所有模型均收敛至较小的损失值，表明微调过程在训练集上是有效的。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.04446v1/x6.png" alt="实验结果"></p>
<blockquote>
<p><strong>图6</strong>：不同VLA模型及控制策略的实验结果。展示了模型在拆解任务各阶段的行为，突显了在精确定位和驱动步骤上的失败。</p>
</blockquote>
<p><strong>阶段分析</strong>：表I详细展示了模型在拆解各子任务上的表现。对于CPU解锁任务，由于夹爪方向固定，无需“对齐”。OpenVLA几乎无法接近目标杠杆（1/20），而OpenVLA-OFT能稳定接近（20/20），但两者均无法精确定位杠杆位置并完成解锁驱动。</p>
<p><strong>混合策略尝试</strong>：基于VLA模型能完成高层决策和粗定位的观察，研究尝试了一种混合策略：当机器人通过VLA模型移动到目标高度后，切换到一个简单的基于位置的控制来执行夹爪配置和最终驱动动作。在RAM任务中，该策略在10次测试中成功了2次，证明了只要机器人能精确到达所需位姿，底层控制可以可靠完成最终操作。但该策略对CPU任务无效，因为VLA模型从未将机器人带到解锁所需的精确位置。</p>
<p><strong>消融实验启示</strong>：通过对比OpenVLA和OpenVLA-OFT的表现，可以视为一种结构上的消融。结果表明，增加手腕视角、本体感知输入和连续动作分块预测（OpenVLA-OFT的特性）带来了行为上的改进，例如更连贯的接近动作和更好的夹爪配置保持。然而，这些改进仍不足以满足拆解任务对精度的极端要求，尤其是在需要微小接触点定位的CPU解锁任务中。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1）首次系统性地评估了VLA模型在复杂、接触丰富的工业拆解任务中的性能，揭示了其在长周期、高精度操作中的当前局限。2）构建了一个针对台式机关键组件拆解的专用机器人视觉-语言-动作数据集。3）提出并初步验证了一种结合VLA高层引导与基于规则的底层控制的混合策略思路，为应对当前VLA模型的精度瓶颈提供了可行方向。</p>
<p><strong>局限性</strong>：论文明确指出，微调后模型性能下降的主要原因是协变量偏移，有限的演示数据覆盖不足以应对执行过程中与演示轨迹的偏差。此外，低分辨率视觉输入、复杂且遮挡的桌面环境，以及缺乏力/触觉反馈，都严重制约了模型在需要精密操作任务中的表现。</p>
<p><strong>未来启示</strong>：研究为后续工作指明了方向：需要更大规模、更多样化的演示数据以覆盖电子废物产品的各种变体；应考虑将力或触觉信号集成到机器人状态中，为模型提供操作进程的反馈；可以探索将强化学习与VLA模型结合，以纠正偏离演示分布状态下的机器人行为；最后，针对拆解等复杂任务，开发专为高精度、接触丰富操作设计的VLA架构或训练范式至关重要。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对报废台式机关键组件（如RAM、CPU）的自动化选择性拆解难题，提出采用端到端的视觉-语言-动作模型。研究通过收集UR5e机器人演示数据集，对OpenVLA和OpenVLA-OFT模型进行微调。实验表明，微调后的VLA模型能可靠完成多个前期拆解步骤，但在某些需高精度操作的子任务上会失败。然而，采用VLA与基于规则控制器结合的混合策略，可成功完成整个拆解流程。这揭示了当前VLA模型在处理精密拆解任务时的局限性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.04446" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>