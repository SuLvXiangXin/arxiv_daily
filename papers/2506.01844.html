<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.01844" target="_blank" rel="noreferrer">2506.01844</a></span>
        <span>作者: Shukor, Mustafa, Aubakirova, Dana, Capuano, Francesco, Kooijmans, Pepijn, Palma, Steven, Zouitine, Adil, Aractingi, Michel, Pascal, Caroline, Russi, Martino, Marafioti, Andres, Alibert, Simon, Cord, Matthieu, Wolf, Thomas, Cadene, Remi</span>
        <span>日期: 2025/06/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人领域正致力于开发能够处理多种任务的通用策略模型，其中视觉-语言-动作模型成为一个重要方向。现有VLA模型通常基于预训练的大规模视觉语言模型进行适配，以利用其丰富的世界知识和推理能力。然而，这些模型通常非常庞大，参数量高达数十亿，导致训练成本高昂，并且在现实世界中的部署受到限制。此外，它们主要依赖于学术和工业数据集，忽视了来自低成本机器人平台的、日益增长的社区收集数据。</p>
<p>本文针对现有VLA模型计算成本高、可访问性差的关键痛点，提出了构建小型、高效、社区驱动的VLA模型的新视角。核心思路是通过一系列架构和训练优化，在单个GPU上训练、在消费级GPU甚至CPU上部署一个紧凑的VLA模型，同时利用公开的社区数据集进行预训练，以实现与大型模型相媲美的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>SmolVLA是一个轻量级VLA，由一个紧凑的预训练VLM和一个使用流匹配训练的动作专家组成。给定多幅图像和描述任务的语言指令，模型输出一个动作块。它首先在社区收集的数据集上通过模仿学习进行预训练，然后在模拟和真实环境中进行评估。推理时，引入了异步执行栈，将动作执行与感知和预测解耦，以实现更快、响应更迅速的控制。</p>
<p><img src="https://arxiv.org/html/2506.01844v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：SmolVLA整体框架。模型由一个紧凑的预训练视觉语言模型（丢弃最后L-N层）和一个动作专家组成。VLM嵌入三种输入：语言指令、RGB图像和机器人感觉运动状态。合并后的令牌馈入动作专家，该专家由交替的交叉注意力（金色）和自注意力（浅黄色）块组成，通过流匹配训练以输出n个低级别动作块。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉语言模型</strong>：采用预训练的SmolVLM-2作为主干感知网络。该模型使用SigLIP编码视觉特征，供SmolLM2语言解码器使用。为保证效率，处理图像序列时不使用图像平铺技术，仅使用全局图像并结合像素洗牌操作，将每帧视觉令牌限制在64个。</li>
<li><strong>层跳过</strong>：为加速推理，选择不使用VLM的最后L-N层特征，而是使用到指定第N层为止的所有特征。实践中发现将N设为总层数的一半（N = L/2）能在速度和性能间取得良好平衡，有效将LLM和动作专家的计算成本减半。</li>
<li><strong>流匹配动作专家</strong>：动作专家 $<code>\mathbf{v}_{\theta}</code>$ 是一个条件流匹配变换器，用于从VLM特征预测动作块 $<code>\mathbf{A}_t = (a_t, \ldots, a_{t+n})</code>$。其训练目标为 $<code>\mathcal{L}^{\tau}(\theta) = \mathbb{E}[\|\mathbf{v}_{\theta}(\mathbf{A}_t^{\tau}, \mathbf{o}_t) - \mathbf{u}(\mathbf{A}_t^{\tau} \mid \mathbf{A}_t)\|^2]</code>$，其中 $<code>\mathbf{A}_t^{\tau} = \tau\mathbf{A}_t + (1-\tau)\epsilon</code>$。模型被训练为从VLM特征和带噪声的动作中输出向量场 $<code>\mathbf{u}</code>$。为提高推理效率，动作专家的隐藏层大小设为VLM隐藏维度d的0.75倍。</li>
<li><strong>交错的交叉与因果自注意力层</strong>：与先前工作仅使用自注意力或交叉注意力不同，SmolVLA的动作专家采用交错设计，每个块包含一个交叉注意力层或一个自注意力层。CA层交叉关注VLM的键和值，而SA层（使用因果注意力掩码）允许动作令牌在块内相互关注。经验表明，这种交错设计能提供更高的成功率和更快的推理时间，自注意力有助于生成更平滑的动作块。</li>
<li><strong>异步推理栈</strong>：为了解决同步推理（耗尽整个动作块后才预测新块）导致的响应延迟问题，提出了异步推理控制循环。该栈将动作块预测与动作执行解耦：<code>RobotClient</code>发送观测 $<code>o_t</code>$ 给 <code>PolicyServer</code>，后者返回动作块 $<code>\mathbf{A}_t</code>$。在执行当前动作队列时，一旦队列长度低于阈值（$<code>|\mathbf{A}_t|/n &lt; g</code>$），就会触发新的观测捕获和动作块预测，从而重叠计算与执行，减少盲等待时间，并允许将计算密集的策略推理部署在远程服务器上。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.01844v1/x2.png" alt="异步推理"></p>
<blockquote>
<p><strong>图2</strong>：异步推理栈示意图。策略可以运行在可能配备GPU的远程服务器上，与机器人客户端的控制循环异步进行。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与基准</strong>：模型在从Hugging Face筛选的481个社区数据集（共约2.29万条轨迹、1060万帧）上进行预训练，数据量比现有先进工作少一个数量级。评估在模拟环境（包括CALVIN、LIBERO、Minecraft）和真实世界（使用Hello Robot Stretch 3和XArm 7机器人）的一系列任务上进行。</p>
<p><strong>对比基线</strong>：对比方法包括OpenVLA (7B)、π0 (RDT-1B + 扩散专家)、DexVLA (RDT-1B + 扩散专家)、TinyVLA (0.6B) 以及专为特定基准训练的模型（如用于CALVIN的Diffusion Policy）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟基准性能</strong>：在CALVIN基准上，SmolVLA在D和ABC设置下的成功率分别为89.8%和79.0%，显著优于OpenVLA (7B) 的68.9%/62.9%和π0的83.9%/73.7%，与参数量大10倍的模型性能相当。在LIBERO稀疏任务上，SmolVLA成功率（62.5%）远超OpenVLA (7B)（17.5%）。在Minecraft模拟中，其表现也优于其他VLA基线。</li>
<li><strong>真实世界机器人性能</strong>：在Hello Robot Stretch 3上执行9项移动操作任务，SmolVLA在89个回合中取得74.2%的整体成功率，优于OpenVLA (7B) 的65.2%。在XArm 7的6项精细操作任务中，SmolVLA成功率（73.3%）大幅领先于OpenVLA (7B)（33.3%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.01844v1/x3.png" alt="CALVIN结果"></p>
<blockquote>
<p><strong>图3</strong>：在CALVIN模拟基准上的成功率对比。SmolVLA在D和ABC设置下均取得领先或具竞争力的性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.01844v1/x4.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图4</strong>：在Hello Robot Stretch 3真实机器人上的任务成功率对比。SmolVLA整体成功率高于OpenVLA (7B)。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.01844v1/x5.png" alt="XArm结果"></p>
<blockquote>
<p><strong>图5</strong>：在XArm 7真实机器人上的任务成功率对比。SmolVLA在所有任务上均显著优于OpenVLA (7B)。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>层跳过与令牌限制</strong>：使用一半VLM层并将视觉令牌限制为64个，能在性能下降极小（CALVIN成功率从89.8%降至87.5%）的情况下，将推理速度提升2倍。</li>
<li><strong>注意力机制设计</strong>：交错CA和SA层的设计优于仅使用CA或仅使用SA，在CALVIN和真实机器人任务上取得最高成功率，且动作轨迹更平滑。</li>
<li><strong>异步推理</strong>：异步推理栈相比同步推理，显著减少了动作执行延迟，提高了系统响应速度。</li>
<li><strong>数据质量提升</strong>：使用VLM自动生成任务描述、进行相机视角标准化，对模型性能有积极贡献。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>轻量级架构</strong>：提出了SmolVLA，通过层跳过、限制视觉令牌、使用小型预训练VLM以及交错注意力等设计，实现了可在消费级GPU训练、CPU部署的高效VLA模型。</li>
<li><strong>社区数据驱动的预训练</strong>：成功利用完全来自公开社区贡献的、相对小规模（&lt;3万条轨迹）数据集进行端到端预训练，证明了社区数据在机器人学习中的价值和潜力。</li>
<li><strong>异步推理栈</strong>：引入了将感知/预测与动作执行解耦的异步推理框架，降低了延迟，实现了更快速、资源高效的推理，并支持远程计算卸载。</li>
</ol>
<p><strong>局限性</strong>：论文提到，尽管性能具竞争力，但更小的模型容量可能限制其处理极其复杂或长视野任务的能力。此外，社区数据集固有的异质性和标注噪声是需要持续应对的挑战。</p>
<p><strong>对后续研究的启示</strong>：SmolVLA证明了构建高效、可访问的机器人基础模型的可行性，其成功依赖于架构创新与数据源的结合。这激励社区进一步探索：1）更先进的轻量化和加速技术；2）自动化、标准化的社区数据清洗与整合流程；3）将高效模型与更强大的世界模型或规划器结合，以扩展能力边界。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型参数量大、训练成本高、难以实际部署的问题，提出了**SmolVLA**——一个小型、高效且由社区数据驱动的VLA模型。其核心技术包括：**采用紧凑的预训练视觉语言模型并裁剪尾部层**，构建一个**基于交替交叉注意力与自注意力块的动作专家模块**，并使用**流匹配进行训练**；同时引入**异步推理栈**以解耦感知、预测与执行，提升响应速度。核心实验结论表明，尽管模型尺寸大幅减小，**SmolVLA的性能与参数量10倍于它的VLA模型相当**，且可在单GPU上训练并部署于消费级GPU甚至CPU。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.01844" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>