<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.07177" target="_blank" rel="noreferrer">2512.07177</a></span>
        <span>作者: Bu, Fanjun, Tsai, Melina, Tjokro, Audrey, Bhattacharjee, Tapomayukh, Ortiz, Jorge, Ju, Wendy</span>
        <span>日期: 2025/12/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在拥挤的公共环境中（如咖啡馆）运行的移动服务机器人，需要决定何时以及是否与人类互动。这种决策通常依赖于微妙、随时间展开的非语言线索（如身体转向、目光回避），这些线索高度依赖上下文且难以用明确规则建模。目前，赋予机器人这种“社交智能”的主流方法包括在受控环境中基于手工规则调整近体行为，或收集大量任务特定数据进行机器学习。然而，社交互动具有高度情境性和模糊性，使得基于规则或枚举数据的方法难以应对其复杂性。</p>
<p>本文针对“如何让机器人在真实世界动态社交场景中做出适时、恰当的互动决策”这一痛点，提出利用视频基础视觉-语言模型作为社交推理代理的新视角。核心思路是：设计一个两阶段流程，首先使用轻量级感知检测器识别可能预示社交互动的“前兆”时刻，然后仅在那些关键时刻触发计算密集型的VLM进行深度场景分析，从而高效、灵活地实现基于社交线索的机器人行为决策。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个两阶段的、基于门控的推理流程，该流程在离散的时间片段（每段2秒视频剪辑）上操作。</p>
<p><img src="https://arxiv.org/html/2512.07177v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：两阶段流程总览。第一阶段（门控）检测社交相关前兆（短暂的目光转移或进入个人空间），识别可能向机器人发出信号的个体。这些触发器启动第二阶段，由基于视频的VLM对观察到的场景进行推理，并生成一个基于证据的行为日志，以支持行动选择。</p>
</blockquote>
<p><strong>Stage I: 前兆检测门控</strong>。此阶段是一个基于规则的检测器，用于识别标志潜在社交互动的“前兆”。门控函数 <code>g(o_t)</code> 在检测到前兆时输出1，否则为0。前兆定义为两种类型：</p>
<ol>
<li><strong>目光转移</strong>：通过一个在头部姿态速度特征上训练的直方图梯度提升分类器进行检测。使用的特征包括左右耳到鼻子的向量、眼睛间距、耳朵间距及耳朵对称性等关键点坐标的时序速度信号（最大值、最小值、标准差）。模型使用2D关键点，主要依赖耳朵特征来近似头部旋转，并辅以眼睛间距以提高对检测抖动的鲁棒性。</li>
<li><strong>近体进入</strong>：当机器人进入个人空间（≤ 1.2米）且在同一2秒窗口内未发生基于目光的前兆检测时触发。这捕捉了参与者未注意到机器人或通过继续原有活动来表示不感兴趣的情况。</li>
</ol>
<p>如果 <code>g(o_t)=0</code>，机器人默认执行“探测”动作，不调用VLM，从而大幅减少计算开销。</p>
<p><strong>Stage II: 条件化VLM推理</strong>。当门控触发时，此阶段调用VLM分析视频剪辑并指导机器人行动。VLM作为一个判别式模型，直接估计给定观测下人类意图的后验分布 <code>q_t(s) ≈ P(s|o_t)</code>。具体而言，VLM通过特定的提示语生成两个输出：</p>
<ol>
<li>**行为日志 <code>ℓ_t</code>**：一个带有时间戳的可观察社交线索证据列表，格式为 <code>[mm:ss]: [evidence]</code>。</li>
<li>**意图估计 <code>ŝ_t</code>**：从行为日志中推断出的意图（“交互”或“不交互”）。</li>
</ol>
<p>为了处理社交线索的模糊性，论文提出了两种基于多次独立VLM采样的不确定性量化策略：</p>
<ul>
<li><strong>自我一致性不确定性</strong>：采样K=5个独立的行为日志和意图预测，将跨样本的分歧作为认知不确定性的度量。若不确定性超过阈值η，则系统默认执行“探测”动作。最终的行为日志通过多数投票合成。</li>
<li><strong>自我批判不确定性</strong>：同样生成多个样本，然后使用VLM识别样本间的矛盾主张，并针对视频证据验证这些矛盾。如果矛盾无法解决或关键主张“无定论”，则系统也默认“探测”。</li>
</ul>
<p><strong>行动选择</strong>：最终，VLM根据合成的行为日志 <code>ℓ_t</code> 和意图估计 <code>ŝ_t</code>，通过提示生成最终动作 <code>a_t ∈ {接近，离开，探测}</code>，并附上简要理由。这个过程不涉及求解基于信念状态的最优策略，而是依赖VLM内隐的社交推理能力。“探测”动作既作为默认动作（当门控未触发时），也作为后备动作（当不确定性高或证据模糊时）。</p>
<p>与现有方法相比，本文的创新点主要体现在：1）提出了一个<strong>选择性触发</strong>的计算框架，将轻量级规则检测与重型VLM推理相结合，在保持决策质量的同时显著提升效率；2）将VLM用作直接的<strong>社交推理代理</strong>，利用其从互联网规模数据中获得的先验知识进行零样本社交解释，避免了繁琐的任务特定数据收集；3）设计了<strong>启发式的不确定性量化机制</strong>，使系统能够在社交信号模糊时主动推迟决策，模仿人类在社交互动中的审慎行为。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在一个大学咖啡馆进行了为期五天的实地部署，通过“魔法师”（Wizard-of-Oz）协议收集了106次接近尝试、涉及约150名独特参与者的自然交互数据。数据集按天划分为训练集（76次交互）和测试集（30次交互）。评估采用回放测试，将提出的两阶段流程在记录的交互数据上运行。</p>
<p><strong>Baseline方法</strong>：</p>
<ol>
<li><strong>随机策略</strong>：以均匀随机概率选择{接近，离开，探测}。</li>
<li><strong>静态策略</strong>：始终选择单一动作（总是接近、总是离开或总是探测）。</li>
<li><strong>仅用VLM（无门控）</strong>：对测试集中每个检测到的个体每2秒调用一次VLM。</li>
<li><strong>魔法师（人类专家）</strong>：作为性能上界，使用原始部署中魔法师的实际决策。</li>
</ol>
<p><strong>关键实验结果</strong>：提出的两阶段流程在测试集上的整体成功率（即所选动作与专家标注的“决策时机”动作一致的比例）达到73.3%。作为对比，随机策略成功率为33.3%，静态策略（总是探测）为40.0%，而“仅用VLM”（无门控）策略的成功率为70.0%。人类魔法师的表现为86.7%。两阶段流程在保持与“仅用VLM”相近性能（73.3% vs 70.0%）的同时，将所需的VLM调用次数从487次大幅减少至129次，效率提升显著。</p>
<p><img src="https://arxiv.org/html/2512.07177v1/x4.png" alt="结果对比"></p>
<blockquote>
<p><strong>图4</strong>：不同决策策略在测试集上的成功率对比。提出的两阶段流程（Two-Stage Pipeline）取得了73.3%的成功率，显著优于随机和静态基线，且与计算成本高得多的“仅用VLM”策略（No Gating）性能相近。人类魔法师（Wizard）的表现作为上界。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07177v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：不确定性阈值η对系统性能的影响（使用自我一致性不确定性）。随着不确定性阈值提高（系统更倾向于在不确定时选择“探测”），成功率提升，但“决策率”（即系统做出“接近”或“离开”决定的比例）下降。在η=0.4时达到最佳平衡点（73.3%成功率，60%决策率）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.07177v1/x6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图6</strong>：目光转移前兆发生时的距离分布直方图。平均距离约为1.2米（4英尺），这与爱德华·霍尔近体学理论中个人空间与社会空间的边界相符，为设定近体触发阈值提供了实证依据。</p>
</blockquote>
<p><strong>消融实验与组件贡献</strong>：实验分析了不确定性阈值η对性能的影响。如图5所示，随着η增加（系统更保守），成功率提高，但做出明确决策（非“探测”）的比例下降。在η=0.4时取得最佳平衡。这验证了不确定性量化机制的有效性。此外，通过特征分析（图3）和距离统计（图6）分别验证了目光检测特征设计的合理性和近体触发阈值的依据。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>高效的两阶段社交决策框架</strong>，通过轻量级前兆检测选择性触发重型VLM推理，在实地交互中实现了响应式的机器人行为，同时将VLM调用减少了73.5%。</li>
<li>探索了将<strong>VLM作为社交推理代理</strong>的可行性，利用其零-shot能力解释动态、非语言的社交线索，并设计了结合行为日志生成和不确定性量化的提示策略。</li>
<li>基于真实的“魔法师”部署数据，对社交互动进行了<strong>前兆-线索-信号</strong>的细粒度标注与分析，为计算模型提供了 grounded 的评估基准。</li>
</ol>
<p><strong>局限性</strong>：论文提到，VLM推理的计算成本仍然较高，难以实现真正的实时交互；所使用的数据集规模相对较小（106次互动），可能限制了模型的泛化能力；当前框架以每2秒剪辑为单位独立决策，未显式建模跨时间步的信念积累。</p>
<p><strong>对后续研究的启示</strong>：本研究展示了基础模型作为社交智能“即插即用”模块的潜力。未来工作可以探索：1）进一步优化计算效率，例如使用更小的VLM或蒸馏技术；2）将时序建模（如递归状态估计）融入当前框架，以处理更复杂的互动序列；3）在更多样化的场景和文化背景下验证与改进该方法；4）结合音频等多模态信息进行更全面的社交情境理解。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究机器人如何识别人类微妙的非语言互动信号（如凝视、距离变化），以决定何时发起交互。提出两阶段方法：先用轻量级检测器识别潜在交互时刻（凝视转移或近距离进入），再触发视频视觉语言模型（VLM）进行场景推理。实验表明，选择性使用VLM作为社交推理代理，能使机器人根据自然线索做出恰当响应，实现社会智能行为。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.07177" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>