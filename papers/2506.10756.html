<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.10756" target="_blank" rel="noreferrer">2506.10756</a></span>
        <span>作者: Zhang, Yuhang, Yu, Haosheng, Xiao, Jiaping, Feroskhan, Mir</span>
        <span>日期: 2025/06/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉语言导航（VLN）是机器人领域的长期挑战，旨在让智能体能够遵循人类指令在复杂环境中导航。该领域存在两个关键瓶颈：对新分布外环境的泛化能力不足，以及对固定离散动作空间的依赖。现有VLN研究主要集中于地面机器人，其假设的离散动作空间与无人机（UAV）连续、动态的飞行特性不符。此外，飞行中的视角遮挡和视觉观测不稳定性造成了地面与空中导航之间的环境和感知鸿沟，限制了地面VLN方法向无人机的直接迁移。</p>
<p>本文针对无人机VLN任务，提出了一个名为Vision-Language Fly（VLFly）的新框架，以解决开放词汇目标理解和连续控制问题。其核心思路是：利用大语言模型（LLM）解析高层指令并生成结构化提示，通过视觉语言模型（VLM）基于跨模态相似性从候选图像池中检索目标图像，最后结合时序的自我中心观测和目标图像，预测未来可执行的连续路径点轨迹，从而引导无人机飞行。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLFly框架包含三个串联的模块：指令编码模块、目标检索模块和路径点规划模块。给定一个自然语言导航指令，系统首先通过LLM将其转化为结构化的目标导向提示词；随后，利用VLM计算该提示词与一个预收集候选图像池中所有图像的语义相似度，并选择最匹配的图像作为目标图像；最后，将目标图像与当前及过去若干时刻的自我中心视觉观测一起输入规划模块，预测出一系列未来相对路径点，这些路径点最终被转化为连续的线速度和角速度控制指令供无人机执行。整个框架在推理时以端到端方式运行，将视觉观测直接映射为动作，且无需针对特定任务进行微调。</p>
<p><img src="https://arxiv.org/html/2506.10756v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：VLFly框架概览。包含三个核心模块：(a) 指令编码器，将自然语言重构为结构化提示；(b) 目标检索器，通过视觉-语言相似性选择最相关的图像；(c) 路径点规划器，根据自我中心观测生成连续轨迹。这些路径点使得无需微调即可实现真实世界无人机导航。</p>
</blockquote>
<p><strong>指令编码模块</strong>：该模块旨在将高层自然语言指令转化为适合下游目标图像检索的结构化文本提示。具体流程如论文图2所示：输入指令经过分词和嵌入后，输入预训练的LLM（如LLaMA）生成上下文隐藏状态，并以此自回归地生成特定格式的标准化提示。例如，给定指令“find a pink toy”和一个自定义物品列表，LLM会生成如“The goal is a pink plush toy.”的标准化提示，以确保目标检索的一致性。<br><img src="https://arxiv.org/html/2506.10756v1/x2.png" alt="提示策略"></p>
<blockquote>
<p><strong>图2</strong>：指令编码模块中使用的提示策略。给定自然语言指令和自定义物品列表，语言模型以特定格式生成标准化提示，从而实现一致的目标检索。</p>
</blockquote>
<p><strong>目标检索模块</strong>：该模块负责基于上一阶段生成的提示，从预定义的候选图像池中识别语义最对齐的目标图像。使用CLIP模型，分别通过其文本编码器和图像编码器将提示和候选图像映射到共享的嵌入空间。计算提示嵌入与每个图像嵌入之间的缩放点积作为相似度得分，经过Softmax后得到概率分布，最终选择相似度最高的图像作为目标图像。</p>
<p><strong>路径点规划模块</strong>：这是框架的核心控制器。它以当前时刻及过去P个时刻的自我中心观测图像序列，以及检索到的目标图像作为输入，输出两部分：1）估计到达目标所需的步数（时序距离）；2）未来H个步骤的相对路径点轨迹。技术细节上，每个历史观测图像通过一个图像编码器独立编码为特征向量。同时，为了捕获当前观测与目标图像之间的相对语义（而非绝对目标内容），采用一个目标融合编码器处理当前图像与目标图像的拼接，输出一个引导导航的目标令牌。随后，这P+1个观测嵌入与目标令牌拼接位置编码后，送入一个仅解码器的Transformer骨干网络。最终，通过一个MLP头预测出路径点序列和距离估计。与依赖完整状态信息的“预言家”轨迹进行模仿学习不同，VLFly的规划器通过时间堆叠的观测来隐式感知运动状态，并利用目标融合编码器提取与目标相关的差异特征，从而实现了在连续动作空间中的泛化导航。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在多样化的模拟和真实世界环境中进行。<strong>模拟评估</strong>使用了AirSim和Habitat平台，构建了包含室内（如公寓）和室外（如社区）场景的基准测试。<strong>真实世界测试</strong>在室内走廊和室外公园进行。评估指标包括任务成功率（SR）、路径长度加权成功率（SPL）、平均路径长度（Avg. Len）和碰撞次数（Collisions）。</p>
<p><strong>对比的基线方法</strong>涵盖了传统方法（ORB-SLAM）、端到端学习方法（包括基于模仿学习和强化学习的方法）以及最近的基于大模型的VLN方法（如VLMaps、ULN）。此外，还设置了VLFly的变体进行消融研究。</p>
<p><strong>关键实验结果</strong>表明，VLFly在大多数模拟导航任务中显著优于所有基线。例如，在AirSim的室内外任务中，VLFly取得了最高的成功率（室内90.0%，室外86.7%）和SPL。在更具挑战性的Habitat公寓导航任务中，VLFly的成功率达到76.7%，远高于最佳基线VLMaps的53.3%。真实世界实验进一步证明，VLFly能够成功执行直接指令（如“飞向红色的门”）和间接抽象指令（如“飞向看起来像停车标志的东西”），展示了其开放词汇理解能力和对新环境的零样本泛化能力。<br><img src="https://arxiv.org/html/2506.10756v1/x3.png" alt="模拟实验结果"></p>
<blockquote>
<p><strong>图3</strong>：在AirSim室内外环境中的定量导航性能对比。VLFly在成功率和SPL上均优于所有基线方法。<br><img src="https://arxiv.org/html/2506.10756v1/x4.png" alt="Habitat实验结果"><br><strong>图4</strong>：在Habitat公寓环境中的导航性能。VLFly在分布外泛化场景中保持领先。<br><img src="https://arxiv.org/html/2506.10756v1/x5.png" alt="真实世界轨迹"><br><strong>图5</strong>：真实世界室内外导航的定性结果。展示了VLFly根据语言指令生成的平滑、准确的飞行轨迹。<br><img src="https://arxiv.org/html/2506.10756v1/x6.png" alt="开放词汇理解示例"><br><strong>图6</strong>：开放词汇目标理解的定性示例。VLFly能够成功响应未见过的、抽象或间接的语言描述。</p>
</blockquote>
<p><strong>消融实验</strong>验证了各模块的贡献。移除指令编码模块（直接使用原始指令检索）或目标检索模块（随机选择目标图像）均导致性能大幅下降，证明了LLM提示重构和VLM语义检索的必要性。在规划模块中，消融时间历史观测或目标融合编码器也会降低性能，突出了利用时序信息和相对目标语义进行规划的重要性。<br><img src="https://arxiv.org/html/2506.10756v1/x7.png" alt="消融研究"></p>
<blockquote>
<p><strong>图7</strong>：VLFly关键组件的消融研究。每个模块都对最终性能有重要贡献。<br><img src="https://arxiv.org/html/2506.10756v1/x8.png" alt="错误案例分析"><br><strong>图8</strong>：典型失败案例分析。包括因视觉相似导致的错误目标检索，以及在复杂障碍物前的规划碰撞。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了首个为无人机量身定制的、统一的VLN框架VLFly，实现了开放词汇目标理解且无需任务特定微调的零样本迁移；2）设计了融合自然语言理解、跨模态语义落地和可导航路径点生成的模块化组件，有效弥合了语义指令与连续无人机控制之间的鸿沟；3）在多样化的模拟和真实环境进行了广泛评估，证明了VLFly具有优越的泛化能力和导航性能。</p>
<p>论文自身提到的局限性包括：框架性能依赖于CLIP等预训练基础模型的泛化能力；规划模块目前主要处理静态环境，对动态障碍物的反应能力有限；实验中的候选目标图像池需要预先收集。</p>
<p>这项工作对后续研究的启示在于：展示了如何有效组合LLM和VLM等基础模型来解决机器人领域的具体任务（如连续控制导航），为构建更通用、更智能的自主机器人系统提供了模块化设计范式。未来方向可能包括集成动态障碍物感知、实现长期任务规划以及探索更高效的基础模型与机器人控制器的耦合方式。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对无人机视觉语言导航中泛化能力不足与动作空间受限的核心问题，提出VLFly框架。其关键技术包括：1）基于大语言模型的指令编码器，将高级指令重构为结构化提示；2）基于视觉语言模型的目标检索器，通过视觉-语言相似性匹配目标图像；3）路径点规划器，生成连续速度指令。实验表明，该框架在多样化仿真与真实室内外场景中无需微调即超越基线，实现了对开放词汇甚至抽象指令的鲁棒理解与泛化导航。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.10756" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>