<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.15865" target="_blank" rel="noreferrer">2506.15865</a></span>
        <span>作者: Viral Rasik Galaiya Team</span>
        <span>日期: 2025-06-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在非结构化环境中部署机器人面临诸多复杂性，机器人系统需要更强的环境感知能力以适应不确定性和可变性。尽管摄像头已广泛应用于机器人任务，但其存在的遮挡、视野局限和信息广度不足等缺陷，促使研究者将部分注意力转向触觉感知。当前，基于视觉的物体姿态估计方法在抓取接近和接触阶段易受遮挡影响，且难以直接获取力、摩擦等物理属性。对于存在位置估计误差的抓取任务，以及如“栓入孔”这类部分被遮挡的拆卸任务，单纯依靠视觉信息往往效率低下或难以完成。</p>
<p>本文针对上述痛点，从触觉感知与自适应学习结合的新视角，探索利用触觉信息提升机器人操作能力的三种技术：1）利用触觉信号的时间序列特征进行抓取后物体姿态估计；2）结合柔顺触觉传感与强化学习，补偿由视觉估计引入的位置不确定性，减少抓取尝试次数；3）利用人类演示示例对强化学习智能体进行预训练，以加速其在受限通道中拆卸物体的轨迹学习过程。本文的核心思路是：通过挖掘触觉数据的时序信息、将其作为强化学习的反馈信号、以及结合人类先验知识，分别解决操作中姿态跟踪、误差补偿和学习效率三个关键问题。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文包含三个相对独立的研究，分别对应不同的方法。</p>
<p><strong>第二章：利用触觉时序特征进行物体姿态估计</strong><br>该方法旨在利用抓取过程中触觉传感器读取的时序数据，预测被抓物体的旋转角度。整体流程为：数据收集 -&gt; 数据预处理与对齐 -&gt; 基于滑动窗口的LSTM模型训练与预测。</p>
<p><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="触觉传感器与物体"></p>
<blockquote>
<p><strong>图2.1/2.2/2.3</strong>：（综合描述）实验使用的欠驱动夹持器，每个手指有两个指骨并配备传感器；触觉传感模块（包含气压计和MARG传感器）安装在机械臂末端并与物体接触；通过物体上的视觉标记获取地面真实角度。</p>
</blockquote>
<p>核心模块是用于角度估计的LSTM神经网络。数据来自两个触觉传感器：一个气压传感器测量接触压力，一个MARG（磁、角速率、重力）传感器测量方向。预处理关键步骤包括传感器数据与视觉标记角度的时序对齐，以及将高频MARG数据下采样至与气压数据同频。</p>
<p><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="LSTM模型架构"></p>
<blockquote>
<p><strong>图2.6</strong>：角度估计模型架构。输入为滑动窗口内的传感器数据序列，经过两个LSTM层（分别有512和256个单元）和三个全连接层（128, 64, 32个神经元），最终输出预测的角度值。</p>
</blockquote>
<p>创新点在于首次系统性地研究了利用触觉传感的<strong>时间序列特征</strong>通过深度学习进行物体角度估计。与仅使用瞬时传感器读数的传统回归方法（如岭回归）相比，该方法通过滑动窗口输入历史数据，使模型能够学习物体运动过程中的动态模式。</p>
<p><strong>第三章：利用柔顺触觉传感和强化学习应对位置不确定性的抓取</strong><br>该方法针对摄像头提供的物体位置估计存在误差的场景，训练一个强化学习（RL）智能体，利用触觉反馈来调整抓取位置，减少失败尝试次数。</p>
<p><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图3.1</strong>：方法整体框架。摄像头提供物体位置的初始估计；机械臂尝试抓取并返回状态（成功/失败、各传感器受力、传感器方向）给RL智能体；智能体根据该信息更新位置，若之前失败则再次尝试抓取。</p>
</blockquote>
<p><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="状态与动作"></p>
<blockquote>
<p><strong>图3.2</strong>：环境与智能体的交互。环境提供附着在末端执行器上的传感器的方向和压力信息；智能体发送新的尝试位置。</p>
</blockquote>
<p>核心模块是<strong>柔顺触觉传感模块</strong>和<strong>强化学习策略</strong>。传感模块（图3.3）具有柔顺结构，能安全地发生碰撞并提供接触力与方向信息。RL智能体的状态空间包括各传感器的压力和方向，动作空间是末端执行器位置的调整量（Δx, Δy, Δz）。奖励函数设计为：成功抓取获得正奖励，每一步获得小的负奖励以鼓励高效，并且当传感器检测到压力（表明发生接触但未成功抓取）时给予额外的负奖励，以惩罚盲目的探索性接触。创新点在于将<strong>柔顺触觉传感</strong>与<strong>强化学习</strong>结合，直接利用触觉碰撞信息来建模和补偿位置估计误差，并引导智能体学习减少无效接触的探索策略。</p>
<p><strong>第四章：基于人类示例预训练的强化学习用于物体轨迹引导</strong><br>该方法研究在“栓入孔”拆卸任务中，利用人类遥操作演示数据对神经网络进行预训练，并将其用于初始化强化学习（Q-learning）的Q值网络，以加速学习过程。</p>
<p><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="方法流程"></p>
<blockquote>
<p><strong>图4.1</strong>：方法流程分为三步：1. 通过遥操作记录物体拆卸过程中的动作和环境状态数据；2. 使用记录的数据训练一个神经网络；3. 将训练好的神经网络作为Q-learning强化学习设置的初始基础。</p>
</blockquote>
<p>核心模块是<strong>预训练神经网络</strong>和<strong>Q-learning智能体</strong>。首先，人类操作者通过遥操作完成物体拆卸，记录状态（触觉传感器位置）和动作数据。然后，用一个神经网络学习从状态到动作的映射。在强化学习阶段，该预训练网络用于初始化Q-network，提供良好的初始策略先验。智能体的状态包括触觉传感器相对于基座和末端执行器坐标系的位置，动作是末端执行器在x, y, z轴上的移动以及绕y、z轴的旋转。创新点在于探索了<strong>人类演示预训练</strong>对后续强化学习效率的影响，为从演示中学习（Learning from Demonstration）提供了一种结合触觉感知的具体实现方式。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>第二章实验</strong>：</p>
<ul>
<li><strong>数据集/平台</strong>：使用配备触觉传感器的欠驱动夹持器和机械臂收集数据，通过视觉标记获取地面真实角度。</li>
<li><strong>Baseline</strong>：岭回归（Ridge Regression）。</li>
<li><strong>关键结果</strong>：通过优化滑动窗口大小，最佳窗口（40个样本）下的LSTM模型在测试集上取得了优于岭回归的性能。具体地，平均绝对误差（MAE）为0.055弧度，均方误差（MSE）为0.005弧度，决定系数（R² Score）为0.985。</li>
<li><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="训练损失与窗口大小性能"><blockquote>
<p><strong>图2.7</strong>：不同窗口大小下的平均训练和验证损失。窗口大小影响模型收敛速度和最佳验证误差。</p>
</blockquote>
</li>
<li><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="窗口大小优化结果"><blockquote>
<p><strong>图2.8</strong>：窗口大小对MAE、MSE、R² Score和 Explained Variance 性能指标的影响，表明存在一个最佳窗口范围。</p>
</blockquote>
</li>
<li><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="预测与真实角度对比"><blockquote>
<p><strong>图2.9</strong>：最佳模型（窗口40）的预测角度与地面真实角度对比。预测整体趋势吻合，但在物体静止阶段（如样本1000、1250附近）会出现持续偏差，因为此时传感器没有新信息输入以修正误差。</p>
</blockquote>
</li>
</ul>
<p><strong>第三章实验</strong>：</p>
<ul>
<li><strong>实验设置</strong>：在仿真环境中进行，物体位置被施加随机偏移以模拟摄像头估计误差。</li>
<li><strong>关键结果</strong>：经过训练后，RL智能体学会利用触觉反馈调整位置，成功抓取物体。随着训练进行，每回合的奖励增加，达到成功所需的步数减少，并最终趋于稳定。</li>
<li><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="传感器数据与训练曲线"><blockquote>
<p><strong>图3.5</strong>：一个成功回合中，末端执行器上每个传感器在每一步的压力和方向读数。</p>
</blockquote>
</li>
<li><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="奖励与步数趋势"><blockquote>
<p><strong>图3.6</strong>：训练过程中奖励（50步移动平均）和每回合步数的变化趋势。随着回合数增加，奖励上升，步数下降，最终均趋于渐近线，表明策略收敛。</p>
</blockquote>
</li>
</ul>
<p><strong>第四章实验</strong>：</p>
<ul>
<li><strong>数据集/平台</strong>：使用4自由度机械臂、两个触觉传感器和3D打印的不同形状栓/孔套装进行实验。</li>
<li><strong>对比</strong>：有预训练 vs. 无预训练的强化学习。</li>
<li><strong>关键结果</strong>：对于所有测试物体，使用预训练（无论是同一物体还是其他物体的演示）都能显著减少强化学习智能体成功提取物体所需的训练回合数。无预训练时需要60个以上回合，而有预训练时通常不超过20个回合。</li>
<li><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="训练回合数对比"><blockquote>
<p><strong>图4.4</strong>：一个具体试验的训练过程，显示有预训练时奖励随步数变化直至成功。</p>
</blockquote>
</li>
<li><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="预训练效果对比"><blockquote>
<p><strong>图4.5 &amp; 4.6</strong>：有预训练与无预训练情况下，成功提取各物体所需回合数的对比。箱形图清晰显示预训练大幅降低了所需回合数及其方差。</p>
</blockquote>
</li>
<li><img src="https://cdn.openai.com/API/images/guides/image_generation_example.png" alt="预训练泛化性"><blockquote>
<p><strong>图4.7</strong>：使用不同物体演示进行预训练后，提取目标物体所需回合数的箱形图。表明预训练具有一定的泛化性，即使演示物体与目标物体不同，也能加速学习。</p>
</blockquote>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>时序触觉特征利用</strong>：证明了利用触觉传感器（气压和MARG）的<strong>时间序列数据</strong>，通过LSTM网络能够有效估计被抓物体的旋转角度，性能优于不考虑时序关系的传统回归方法。</li>
<li><strong>触觉反馈补偿位置不确定性</strong>：提出并验证了一种结合<strong>柔顺触觉传感模块</strong>和<strong>强化学习</strong>的方法，能够主动建模和补偿由视觉初始估计带来的物体位置误差，并通过奖励函数设计减少无效的探索性接触，提高抓取效率。</li>
<li><strong>人类演示加速拆卸学习</strong>：展示了在“栓入孔”拆卸任务中，利用<strong>人类遥操作示例进行预训练</strong>可以显著加速后续强化学习智能体的收敛速度，减少了所需的训练回合数，且预训练知识具有一定的跨物体泛化能力。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>第二章：当物体处于静态时，由于传感器没有新的输入信息，角度预测误差可能无法被及时纠正（如图2.9所示）。</li>
<li>第三章：实验主要在仿真环境中进行，论文提到未来需在真实机器人系统上验证。</li>
<li>第四章：预训练数据的质量和多样性对加速效果和泛化能力有直接影响，且该方法对任务（栓入孔拆卸）有一定特异性。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>将时序触觉姿态估计与位置不确定性补偿方法结合，可能实现更全面的在线物体姿态跟踪与修正系统。</li>
<li>探索更高效的模拟到真实（Sim2Real）迁移方法，将第三章的RL策略部署到物理机器人，并处理更复杂的真实噪声和不确定性。</li>
<li>研究如何自动评估和筛选人类演示数据，或结合少量演示与大量非专家数据，以提升预训练模型的鲁棒性和泛化能力，使其适用于更广泛的复杂操作任务。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文针对非结构化环境中机器人操作的复杂性，旨在提升系统对不确定性的适应能力。核心解决了视觉感知（如遮挡）的局限，重点探索触觉传感的补充作用。提出了三项关键技术：1）利用触觉时间特征进行物体姿态估计；2）结合触觉碰撞的强化学习，以减少因视觉位姿估计误差所需的抓取尝试次数；3）基于触觉信息与人类示例的强化学习，用于规划受限通道中的物体拆卸轨迹，以缩短训练时间。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.15865" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>