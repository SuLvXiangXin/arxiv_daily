<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.05765" target="_blank" rel="noreferrer">2602.05765</a></span>
        <span>作者: Guan, Zhong, Sun, Haoran, Guo, Yongjian, Di, Shuai, Bai, Xiaodong, Long, Jing, Zhao, Tianyun, Luo, Mingxi, Zhou, Chen, Guo, Yucheng, Yang, Qiming, Xu, Wanting, Huang, Wen, Ma, Yunxuan, Zhao, Hongke, Wu, Likang, Deng, Xiaotie, Xiao, Xi, Wen, Sheng, Gong, Yicheng, Xiong, Junwu</span>
        <span>日期: 2026/02/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型被认为是实现通用具身智能的关键途径，但其训练效率已成为主要瓶颈。尽管现有的基于强化学习的训练框架（如RLinf）能够提升模型泛化能力，但它们仍依赖于同步执行模式。在环境交互、策略生成和模型更新阶段，这种同步性导致了严重的资源利用不足和吞吐量限制。在大型模型强化学习中已被证明能显著提升效率的异步训练机制，在VLA模型的强化学习训练领域却尚未得到充分探索和实践。本文针对同步训练范式造成的资源闲置和吞吐量瓶颈这一具体痛点，首次提出并实现了一个覆盖环境交互、轨迹生成到策略更新全流程的完全异步策略训练流水线RL-VLA³。其核心思路是借鉴大模型RL中的异步优化思想，设计一个多级解耦的架构，通过环境交互与轨迹收集的异步并行、策略生成的流式执行以及训练更新的解耦调度，来减少组件间的等待时间，实现计算资源的持续饱和利用。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的RL-VLA³框架是一个三级异步执行架构，旨在系统性解决VLA模型强化学习训练中的效率瓶颈。整体框架（Pipeline）如图1所示，其创新性体现在两个层面：宏观上，在轨迹生成与训练流水线阶段之间实现异步；微观上，在每个轨迹生成周期内部实现异步，后者由批大小和等待时间两种触发策略控制。</p>
<p><img src="https://arxiv.org/html/2602.05765v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：提出的VLA训练异步框架示意图。该设计在宏观（轨迹生成与训练流水线之间）和微观（每个轨迹生成周期内部）两个层面实现异步操作。微观层面的异步由批大小和等待时间两种触发策略控制。</p>
</blockquote>
<p><strong>核心模块一：基于解耦GPU分配的异步训练与推理架构</strong><br>传统同步训练通常采用协同定位的GPU分配策略，需等待所有轨迹生成完成后才收集数据进行策略更新，容易因任务耗时不均或环境交互的长尾延迟导致计算资源闲置。RL-VLA³采用了资源解耦的GPU分配策略，将负责环境交互和策略推理的Rollout Worker与负责策略优化的Actor Worker部署在独立的GPU设备上，通过高吞吐流水线进行轨迹数据传输。一旦任一Rollout Worker完成当前轨迹生成，便立即将其放入传输队列并基于当前策略版本继续生成后续轨迹，无需等待其他环境交互。当累积轨迹达到预设训练批大小时，Actor Worker异步地从通信流水线收集这些轨迹进行策略优化和参数更新。在Actor训练期间，Rollout Worker继续使用更新前的策略权重生成新轨迹，直至Actor完成当前参数更新后再同步新权重。这首次在VLA-RL训练中实现了轨迹生成与训练之间的高效流水线执行。</p>
<p><strong>核心模块二：细粒度的异步交互策略</strong><br>在VLA模型RL训练中，样本轨迹生成效率是系统吞吐量的主要瓶颈。传统框架采用同步批量并行模式，在整批粒度上进行仿真环境与推理模型的交互，这导致了环境侧的长尾效应（全局进度受最慢仿真实例制约）和推理侧的流水线阻塞（必须处理完大批量推理请求才能返回环境）。为缓解此问题，本文提出了<strong>细粒度异步交互策略</strong>。该机制以更高分辨率管理仿真实例，将环境步进与模型推理解耦，允许来自部分环境的就绪请求提前进入推理队列，从而绕过全局同步要求。值得注意的是，由于VLA模型常集成Diffusion模块，其动作生成过程具有非自回归或多步去噪特性，无法直接应用LLM中基于token的动态插入技术（连续批处理）。因此，本文设计了<strong>动态批处理调度器</strong>，以最大推理批大小和最大等待延迟作为约束条件：当累积请求数达到最大批大小<code>B_max</code>或最早请求的等待时间达到<code>T_max</code>时，即触发推理。这有效压缩了批量轨迹生成所需时间。</p>
<p><strong>核心模块三：流式生成机制</strong><br>在异步训练架构中，尽管轨迹生成与训练已解耦，但Actor仍需累积足够数量的轨迹以形成完整训练批次，这仍可能导致GPU间歇性空闲。为充分利用这些等待间隔，本文在训练阶段进一步引入了细粒度的流水线划分机制。具体而言，将全局训练批次划分为若干微批次。每当轨迹缓冲区中累积的样本数达到单个微批次大小时，Actor便启动一次前向和反向计算。待所有微批次顺序计算完毕后，再统一进行梯度聚合和参数更新。这种设计使得Actor无需等待整批轨迹完全生成即可提前开始部分训练计算，从而有效掩盖数据准备时间，减少端到端训练延迟。此外，通过动态调整Rollout Worker与Actor Worker之间的GPU资源分配比例，可以使两个过程的执行时间成本趋于均衡，从而在异步流水线中实现近乎完全的计算掩盖，优化整体系统利用率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验基于RLinf框架进行扩展。评估了多种预训练VLA模型，包括基于扩散的GR00T N1.5、π₀和π₀.₅，以及自回归模型OpenVLA-OFT。使用了LIBERO和ManiSkill两个基准环境进行RL训练。对比了RLinf支持的两种分布式训练策略：协同定位策略（Colocated）和解耦策略（Disaggregated）。评估指标主要为系统吞吐量（单位时间内完成的环境状态计算数）和训练成功率曲线。</p>
<p><strong>关键实验结果</strong>：<br>如表1所示，本文提出的异步策略在大多数情况下显著提升了吞吐量。实验以渐进增强的三种配置进行评估：1) Train Async（训练异步）；2) Rollout Async（增加轨迹生成内部的异步）；3) Streamer（流式调度器）。在LIBERO环境中，结合π₀.₅模型，在32 GPU规模下，最终方案相比Colocated基线实现了59.25%的吞吐量提升。Rollout Async策略的效果因环境而异：在LIBERO上带来进一步增益，但在ManiSkill的8-GPU设置下导致了4.46%的吞吐量下降，这是因为该策略将批次拆分为更小的片段，可能损害了ManiSkill利用GPU并行化进行环境计算的优势。不过，在32-GPU规模下，仍实现了17.84%的提升。</p>
<p><img src="https://arxiv.org/html/2602.05765v1/dis_asy_pi.png" alt="分离策略分析1"></p>
<blockquote>
<p><strong>图2</strong>：LIBERO + π₀.₅配置下，不同分离策略（Actor与Rollout的GPU分配比例为1:1, 2:1, 3:1）的吞吐量对比分析。展示了从基线异步训练策略逐步增加Train Async、Rollout Async和Streamer调度器带来的性能提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.05765v1/dis_asy_openvla.png" alt="分离策略分析2"></p>
<blockquote>
<p><strong>图3</strong>：LIBERO + OpenVLA-OFT配置下，不同GPU分配比例（1:1, 2:1, 3:1）的吞吐量对比分析。进一步验证了方法在不同模型上的有效性和鲁棒性。</p>
</blockquote>
<p><strong>消融实验与组件贡献</strong>：<br>通过分析不同资源分配比例下的性能（图2, 3），并对各异步组件进行消融研究，明确了每个组件的贡献。在LIBERO环境中，轨迹生成阶段通常主导时间成本。实验表明：1) 仅使用Train Async时，在3:1的GPU分配比例下，两阶段执行时间可有效重叠，相比基线带来58.67%的吞吐量提升。2) 引入Rollout Async缩短了轨迹生成时间，使得即使在1:1的分配比例下也能实现效率平衡，带来118.80%的提升。3) 最终加入Streamer调度器最小化了Actor Worker的空闲时间，实现了126.67%的最大提升。这验证了各组件协同工作的有效性。</p>
<p><img src="https://arxiv.org/html/2602.05765v1/x2.png" alt="扩展性分析"></p>
<blockquote>
<p><strong>图4</strong>：LIBERO + π₀.₅配置下，随着GPU资源增加（8至256 GPU），不同策略的吞吐量扩展行为。本文方法在8到24 GPU间呈现近乎理想的线性扩展，在更高规模下扩展效率因通信开销增加而放缓。</p>
</blockquote>
<p><strong>扩展性验证</strong>：<br>如图4所示，本文方法在LIBERO+π₀.₅配置下，从8 GPU扩展到24 GPU时呈现出近乎理想的线性扩展。在24到128 GPU之间扩展效率趋于平缓，从128到256 GPU时进一步下降。这种在极高资源数量下的次线性扩展归因于不断增长的Worker间通信开销。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>首次为VLA模型的强化学习训练提出了一个支持分层异步执行的训练框架RL-VLA³，系统性地缓解了同步训练中的资源闲置问题。</li>
<li>通过流式生成与环境交互的解耦编排，实现了推理与仿真的高并发执行。</li>
<li>在多个基准仿真任务和真实机器人部署中验证了框架的效率和泛化能力，为大规模具身智能研究提供了更高效的训练平台。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，在极端规模（如256 GPU）下，由于Worker间通信开销的增长，方法的扩展能力出现下降，未能保持理想的线性扩展。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>优化大规模训练中的通信开销是进一步提升方法可扩展性的重要方向。</li>
<li>可将该异步框架扩展到更多高保真仿真后端，以评估其在复杂物理和多样化渲染开销下的鲁棒性。</li>
<li>利用其高吞吐能力，探索在异构机器人形态间进行跨具身学习的应用，训练通用策略。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型训练效率低下的瓶颈问题，提出RL-VLA³框架，首次实现了从环境交互、轨迹生成到策略更新的全异步训练流水线。其核心方法采用多级解耦架构，对环境交互与轨迹收集进行异步并行化，对策略生成采用流式执行，并对训练更新进行解耦调度。实验表明，在LIBERO基准上，相比同步策略，该框架吞吐量最高提升59.25%，深度优化后可达126.67%的提升，并在8至256 GPU上展现出良好的可扩展性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.05765" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>