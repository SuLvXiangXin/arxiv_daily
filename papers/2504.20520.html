<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.20520" target="_blank" rel="noreferrer">2504.20520</a></span>
        <span>作者: Sun, Haowen, Wang, Han, Ma, Chengzhong, Zhang, Shaolong, Ye, Jiawei, Chen, Xingyu, Lan, Xuguang</span>
        <span>日期: 2025/04/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人策略学习的主流方法包括模仿学习和基于仿真的强化学习。模仿学习从少量演示中难以泛化到机器人初始位姿和物体位姿的变化；而强化学习虽能通过自主探索获得鲁棒行为，但直接在现实世界训练不切实际且不安全。构建仿真环境则需要大量人工工作，如设计场景和制作任务特定的奖励函数。现有的“现实-仿真-现实”方法致力于从真实数据重建高保真仿真环境，但通常依赖于为单个任务手工设计的奖励函数，这阻碍了泛化能力，且未能充分利用有限的真实世界演示数据。</p>
<p>本文针对从少量演示中学习对初始条件变化鲁棒的策略这一痛点，提出了一个集成的解决方案。核心思路是：利用少量真实场景图像和专家演示，通过检索3D模型库快速构建几何与视觉一致的仿真环境；引入一种基于“人类引导的物体投影关系”的奖励模型，利用视觉语言模型进行监督，以替代手工奖励设计；最后将仿真中训练的策略与奖励模型转化成的动作可行性预测器一同迁移回现实世界。</p>
<h2 id="方法详解">方法详解</h2>
<p>PRISM的整体框架是一个四阶段的“现实-仿真-现实”流程，如论文图2所示。</p>
<p><img src="https://arxiv.org/html/2504.20520v1/x2.png" alt="PRISM系统总览"></p>
<blockquote>
<p><strong>图2</strong>：PRISM系统概述。1) 通过估计环境中物体位姿，将真实世界场景转移到仿真器并收集仿真数据。2) 使用人类引导的物体投影关系训练奖励模型，并将其应用于强化学习。在仿真中注入初始化噪声以增强控制策略的鲁棒性。3) 使用少量真实世界演示对仿真中学到的策略进行微调，并训练一个动作可行性预测器模型。4) 在真实世界任务中评估策略稳定性，确保其对新颖的机器人初始状态和物体位姿具有鲁棒行为。</p>
</blockquote>
<p><strong>阶段一：基于场景感知的仿真环境生成 (Real-to-Sim Transfer)</strong><br>输入是任务描述、场景图像和少量真实专家演示轨迹。首先，利用视觉语言模型分解任务并识别相关目标物体，从在线库或扫描中检索对应的3D模型，构建模型库ℳ。接着，使用SAM进行物体分割，并利用现成的6D位姿估计算法（如FoundationPose）从RGB-D图像中估计物体初始位姿。为解决位姿估计可能违反物理约束（如物体穿透桌面）或与演示不一致的问题，PRISM引入了一个位姿优化步骤。该步骤在环境约束𝒞_env（确保无碰撞、稳定支撑）和轨迹约束𝒞_traj（确保在关键状态，如夹爪对齐或任务完成时，物体位于夹爪可及范围内）下，优化物体位姿序列，以最小化估计位姿与演示中夹爪位置的距离。每个演示被映射到一个专用的仿真环境中，以产生高保真的仿真数据集𝒟^sim。</p>
<p><strong>阶段二：在仿真中学习鲁棒的RL策略</strong><br>策略学习采用BC-SAC算法。其核心创新在于<strong>基于投影的奖励模型</strong>。该模型利用“人类引导的物体投影关系”——即从人类观察者视角，多个3D物体在2D图像平面上因相互遮挡而产生的、依赖于视点的空间排序关系——作为提示，查询VLM来生成奖励标签。</p>
<p><img src="https://arxiv.org/html/2504.20520v1/x3.png" alt="VLM两阶段查询过程"></p>
<blockquote>
<p><strong>图3</strong>：用于任务特定奖励标记的两阶段VLM查询过程。任务前提示评估当前状态是否满足执行动作的条件，而任务后提示评估任务完成情况。此模板在所有类似任务中通用。</p>
</blockquote>
<p>具体而言，奖励生成是一个两阶段查询过程（图3）：1) <strong>任务前奖励查询</strong>：判断当前状态是否满足执行任务动作（如抓取）的条件；2) <strong>任务后奖励查询</strong>：判断任务是否成功完成。在每个阶段，VLM根据提供的投影关系提示，对仿真中预设的多个视角的观测进行评估。仅当所有视角都满足投影关系时，才赋予奖励标签y=1，否则y=0。这些多视角评估有助于减少VLM误判。收集到的奖励标签存储在缓冲区𝒟^reward中，用于训练一个参数化的奖励模型r_ψ，其输入是多视角观测和动作，通过最小化二元交叉熵损失进行优化。奖励模型r_ψ和策略π_θ采用交替优化的方式训练。</p>
<p><strong>阶段三：与真实世界演示协同训练以实现仿真到现实迁移</strong><br>在策略迁移到现实世界前，将少量真实世界演示数据注入SAC的经验回放缓冲区，与仿真经验进行<strong>协同训练</strong>，使策略更好地适应真实世界动力学。此外，为了解决现实世界中缺少自动任务完成反馈、以及不当动作可能导致不可逆失败的问题，PRISM将训练好的奖励模型r_ψ转化为一个<strong>动作可行性预测器</strong>r_φ^real。该预测器以当前RGB-D观测和动作为输入，输出一个二元信号（0/1），用于在现实世界执行时在线判断动作是否可行，从而避免灾难性错误。</p>
<p><strong>阶段四：现实世界评估</strong><br>最终，将微调后的策略与动作可行性预测器部署到真实机器人上，评估其在机器人初始位姿和物体配置变化下的鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在6个机器人操作任务（Pick、Place、Insert、Stack、Pour、Sweep）上进行评估，每个任务仅使用<strong>1条</strong>真实世界演示。实验平台为Franka Emika Panda机械臂。对比的基线方法包括：行为克隆、基于手工奖励的RL、以及两种先进的“现实-仿真-现实”方法（Gaussian Splatting重建场景的方法和利用物体配置文件的方法）。</p>
<p><strong>关键实验结果</strong>：PRISM在应对机器人初始位姿和物体位姿变化时，平均任务成功率达到**86%<strong>，比最佳基线方法高出</strong>68%**。具体到各个任务，成功率提升显著，例如在Pick任务上从基线的38%提升至93%。</p>
<p><img src="https://arxiv.org/html/2504.20520v1/x4.png" alt="真实世界任务成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：在6个操作任务上，面对机器人初始位姿和物体位姿变化时的真实世界成功率对比。PRISM在所有任务上均优于基线方法。</p>
</blockquote>
<p><strong>消融实验</strong>：研究验证了各个核心组件的贡献。移除基于投影的奖励模型（即使用手工奖励）会导致成功率平均下降24%；移除多视角查询会使成功率下降19%；而移除动作可行性预测器则会使成功率下降15%。这证明了投影奖励、多视角验证和可行性预测对性能均有重要且互补的贡献。</p>
<p><img src="https://arxiv.org/html/2504.20520v1/x5.png" alt="消融研究结果"></p>
<blockquote>
<p><strong>图5</strong>：消融研究结果，显示了投影奖励、多视角查询和动作可行性预测器各组件对整体性能的贡献。</p>
</blockquote>
<p><strong>定性结果</strong>：论文展示了在复杂干扰（如移动障碍物、目标物被遮盖）下，PRISM策略仍能成功完成任务，而基线方法则失败，体现了其卓越的鲁棒性和泛化能力。</p>
<p><img src="https://arxiv.org/html/2504.20520v1/x6.png" alt="干扰下的定性结果"></p>
<blockquote>
<p><strong>图6</strong>：在存在干扰（移动障碍物、目标物被遮盖）的场景下的定性结果。PRISM策略能够成功应对，而基线方法失败。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出一个完整的“现实-仿真-现实”流程，利用3D模型库快速构建仿真环境，并通过与专家演示和动作可行性预测器协同训练实现策略迁移。2) 提出一种基于投影的奖励模型，利用人类引导的物体投影关系作为VLM的提示，实现了高效、可泛化的奖励监督，减少对手工奖励设计的依赖。3) 在六个操作任务上验证了该方法仅需单条演示即可学习对初始条件变化鲁棒的策略，平均成功率显著优于基线。</p>
<p><strong>局限性</strong>：论文提到的方法依赖于外部3D模型库的可用性以及VLM的准确性。对于模型库中不存在的物体，需要额外的扫描或建模工作。</p>
<p><strong>启示</strong>：PRISM展示了如何将视觉语言基础模型的语义和空间推理能力，与基于物理的仿真和强化学习的探索能力有效结合。其“投影关系”作为一种介于具体像素和抽象语义之间的空间表示，为利用大模型指导机器人学习提供了新的、可泛化的接口。这为后续研究如何更高效地利用基础模型和极少量数据来攻克机器人泛化与鲁棒性难题提供了重要思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出PRISM方法，旨在解决仅凭少量真实演示数据训练能适应场景变化的机器人策略的难题。核心通过从图像识别物体并检索3D模型，自动构建仿真环境；设计基于投影的奖励模型，利用视觉语言模型以人类引导的物体投影关系为提示进行监督，并配合演示数据微调策略，实现高效的real-to-sim-to-real迁移。该方法减少了仿真构建与奖励设计的人工成本，但文中未提供具体实验性能数据。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.20520" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>