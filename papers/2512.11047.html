<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.11047" target="_blank" rel="noreferrer">2512.11047</a></span>
        <span>作者: Jiang, Haoran, Chen, Jin, Bu, Qingwen, Chen, Li, Shi, Modi, Zhang, Yanjie, Li, Delong, Suo, Chuanzhe, Wang, Chuang, Peng, Zhihui, Li, Hongyang</span>
        <span>日期: 2025/12/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人需要精确的运动和灵巧的操作来执行具有挑战性的运动操作任务。然而，现有的方法，无论是模块化还是端到端的，都存在“操作感知的运动能力”不足的问题，这限制了机器人的工作空间，使其无法执行大空间范围的运动操作。这主要归因于两个关键痛点：1）由于人形机器人遥操作数据稀缺，难以获取运动操作知识；2）现有强化学习控制器精度和稳定性有限，导致对运动指令的执行不够忠实可靠。针对这些痛点，本文提出了一个统一框架，其核心思路是：通过一个统一的潜在学习框架，使视觉-语言-行动模型能够从低成本的无动作视频中学习运动操作知识；同时，设计一个面向运动操作的强化学习策略，以更精确稳定地执行底层运动指令。</p>
<h2 id="方法详解">方法详解</h2>
<p>WholeBodyVLA的整体框架旨在为VLA模型配备可靠的运动基元，以建立执行操作所需的前提条件。它通过统一潜在学习和面向运动操作的RL策略，使人形机器人能够完成长距离、大范围的任务。</p>
<p><img src="https://arxiv.org/html/2512.11047v2/x2.png" alt="方法流程"></p>
<blockquote>
<p><strong>图2</strong>：WholeBodyVLA的流程。潜在动作模型在操作和操作感知的运动视频上进行预训练，为VLM提供统一的潜在监督。同时，LMO RL策略被训练用于在扰动下进行精确稳定的运动。在运行时，以自我为中心的图像和语言指令被VLM编码为潜在动作标记，这些标记被解码（约10Hz）为（i）双臂关节动作和（ii）由LMO在50Hz执行的运动命令，从而实现鲁棒的全身运动操作。</p>
</blockquote>
<p><strong>核心模块一：统一潜在动作模型</strong><br>该方法的核心思想是利用潜在动作模型从以自我为中心的操作视频和操作感知的运动视频中学习操作和运动基元，然后用它们来监督VLA训练。研究发现，直接在混合数据上训练单个LAM会导致次优性能，因为操作和运动视频在模态上存在根本差异：操作视频中相机姿态几乎静止，而运动视频中相机连续变化。这会导致注意力目标冲突和潜在编码模糊。因此，本文分别训练两个LAM：一个在操作数据上训练的<strong>操作LAM</strong>，一个在运动数据上训练的<strong>运动LAM</strong>。两个LAM随后共同监督VLA训练。</p>
<p>技术细节上，采用VQ-VAE架构，编码器基于DINOv2特征。给定连续帧，LAM编码器输出连续潜在向量，随后通过矢量量化到学习到的码本中最接近的条目。LAM解码器接收前一帧和量化后的潜在动作，被训练来重建后一帧，通过最小化标准VQ-VAE损失进行优化。LAM预训练完成后，训练VLA策略在给定视觉观察和任务语言的情况下，通过交叉熵损失联合预测两种类型的潜在动作。这种统一的预测迫使模型学习运动和操作如何在单一、连贯的动作空间中相互作用以支持任务执行。</p>
<p>最后，为了在人形机器人上执行，引入一个轻量级解码器，将潜在动作具体化为机器人特定的命令：上半身关节角度和一个指示待执行动作的运动命令。该命令由LMO RL策略转换为下半身扭矩。通过这种分工，VLA提供统一的潜在决策，解码器将其具体化为特定于机器人的控制信号，RL策略确保稳定执行。</p>
<p>为了扩大统一潜在学习的收益，本文还设计了一个低成本的以自我为中心的数据收集流程，仅需一名头戴摄像头的操作员，覆盖人形机器人的所有运动基元（前进、转向、下蹲），并以接触潜在操作目标为导向执行运动。</p>
<p><strong>核心模块二：面向运动操作的RL策略</strong><br>运动操作的一个主要失败模式是高层决策与底层执行之间的错位。这主要源于现有RL控制器中使用的连续随机速度跟踪目标——该目标为广泛的运动而设计，而非操作所需的稳定、可靠的起停和方向控制。本文引入了LMO RL框架，用离散命令接口取代速度跟踪，实现更忠实的执行。</p>
<p><strong>离散命令接口</strong>将下半身控制表述为目标条件调节，策略在执行离散高层命令的同时保持平衡。在每个时间步，规划器生成一个命令，其中表示前进、横向和转向的离散指示符，指定站立高度。与基于速度的公式不同，该接口强制执行明确的起停语义，减少了轨迹方差。</p>
<p><strong>两阶段课程训练</strong>：第一阶段（基本步态获取）使策略发展出防止跌倒的基本步态。第二阶段（精度和稳定性）通过专门优化来定位运动操作级别的精度和稳定性。在运动方面，将每轴巡航速度固定为常数以标准化巡航，并抑制无偏航意图时意外的航向漂移。方向精度通过终端偏差来衡量，最小化该偏差可强制执行精确的启动、稳定巡航和一致的制动。在操作方面，通过从AgiBot-World采样短的手臂运动片段，将其插值为连续信号，并以变化的速率和轻微噪声回放，注入真实的扰动。这迫使腿部补偿结构化的惯性耦合，而非非结构化的扰动。此外，对于静止情节，添加站立惩罚以阻止不必要的腿部动作。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在AgiBot X2人形机器人原型上评估。设计了三个任务综合测试运动操作能力：1）<strong>袋装打包</strong>：抓取纸袋，侧步移动到纸箱，下蹲并放入；2）<strong>箱子装载</strong>：下蹲抓取箱子，转身并将其放到手推车上；3）<strong>推车</strong>：抓取50公斤重的手推车把手并稳定地向前推。这些任务共同评估了双臂协调、下蹲精度、转向准确性和重载下的稳定性。每个任务收集了50次遥操作轨迹用于微调。</p>
<p><strong>对比基线</strong>：包括代表性的模块化管道基线（Modular Design）、VLA框架GR00T N1.5和OpenVLA-OFT（均适配为输出双臂关节动作和与WholeBodyVLA相同的离散运动命令，并由LMO控制器执行），以及本文设计的消融变体。</p>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2512.11047v2/x3.png" alt="主要结果表"></p>
<blockquote>
<p><strong>图3</strong>：在三个任务上的评估。每个任务分解为两个子目标。WholeBodyVLA在平均成功率上优于模块化和端到端基线，达到78.0%。消融实验表明，统一潜在学习和LMO策略都有显著贡献。</p>
</blockquote>
<p>如表所示，WholeBodyVLA在三个任务上的平均成功率达到78.0%，显著优于模块化设计（64.0%）、GR00T w/ LMO（42.0%）和OpenVLA-OFT w/ LMO（56.7%）。消融实验表明：1）使用基于速度的RL控制器（w/ vel.-based RL）替代LMO，性能大幅下降至54.0%，尤其在需要精确移动和下蹲的子任务上失败率很高，验证了LMO的有效性。2）不使用任何LAM预训练（w/o lam）的模型性能仅为39.3%，比完整模型低38.7%，证明了从无动作视频中学习先验知识的重要性。3）仅使用操作LAM（w/ manip. lam）或使用共享LAM（w/ shared lam）的变体性能（63.3%和66.0%）均低于完整模型，支持了分别训练两个LAM的设计选择。</p>
<p><strong>泛化与数据效率分析</strong>：</p>
<p><img src="https://arxiv.org/html/2512.11047v2/x4.png" alt="泛化性能"></p>
<blockquote>
<p><strong>图4</strong>：在改变起始位置和物体位置下的泛化性能。WholeBodyVLA（橙色）在两种泛化设置下均保持较高的成功率，而基线方法（蓝色）性能显著下降，证明了其强大的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.11047v2/x5.png" alt="数据效率"></p>
<blockquote>
<p><strong>图5</strong>：在不同比例遥操作数据下微调的性能。WholeBodyVLA（橙色）在使用更少遥操作数据（如20%）时，性能下降幅度远小于没有LAM预训练的变体（蓝色），证明了统一潜在学习能有效减少对昂贵遥操作数据的依赖。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了WholeBodyVLA，一个使双足人形机器人能够在真实世界环境中自主执行端到端大空间运动操作的VLA框架。2）引入了统一潜在学习，使VLA能够从丰富的低成本无动作视频中联合学习运动-操作知识，缓解了遥操作数据稀缺的问题。3）提出了面向运动操作的RL策略，通过为运动操作量身定制的离散命令接口，减轻了决策-执行的错位。</p>
<p>论文自身提到的局限性包括：LMO策略的离散命令接口可能限制了一些需要连续速度调制的动态运动能力；当前系统在非常狭窄的空间或涉及非刚性物体操作等极端场景下的性能仍有待探索。</p>
<p>本文的工作对后续研究具有重要启示：首先，它展示了如何利用大量易得的无动作人类视频来引导机器人学习复杂的组合技能，为扩大机器人学习的数据规模提供了新途径。其次，针对特定任务家族（如运动操作）设计专门的底层控制器（如LMO），可以更有效地解决高层策略与底层执行之间的鸿沟，这种“专业化”思路值得借鉴。最后，将视觉-语言理解与全身控制在一个统一框架内紧密耦合，是实现通用具身智能体的关键一步。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人缺乏操作感知的全身运动能力、难以在大空间执行运动操作的问题，提出了WholeBodyVLA统一框架。其关键技术包括：一个能从低成本无动作视频学习的统一潜在视觉-语言-动作学习框架，以及一个专为精确稳定核心运动（如前进、转向、下蹲）设计的运动操作导向强化学习策略。在AgiBot X2机器人上的实验表明，该框架性能超越先前基线21.3%，并展现出强大的任务泛化与扩展能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.11047" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>