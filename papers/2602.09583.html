<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.09583" target="_blank" rel="noreferrer">2602.09583</a></span>
        <span>作者: Danica Kragic Team</span>
        <span>日期: 2026-02-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人日益融入日常生活的背景下，对能够反映个体用户偏好的自适应行为的需求日益增长。变形物体操作（DOM），如衣物折叠，是一个特别相关但研究不足的领域。此类任务中的用户偏好（如不同的折叠风格）通常是微妙、个人化且难以用语言描述的，使得物理演示成为表达偏好的自然方式。然而，收集演示数据成本高昂，因此需要样本高效的适应框架。</p>
<p>当前，数据驱动的学习方法，特别是视觉运动扩散策略，已在大规模演示数据集上展现出强大的泛化能力。然而，如何使这些预训练模型适应并反映用户特定的操作偏好，同时避免灾难性遗忘且不依赖大量新演示，仍是一个挑战。尽管偏好优化技术（如DPO、RPO、KTO）在文本到图像生成等领域取得成功，但它们在机器人DOM中的应用仍然有限。</p>
<p>本文针对上述痛点，研究如何利用有限的演示，将预训练的视觉运动扩散策略与用户偏好的行为对齐。核心思路是：提出一种名为RKO的新型偏好对齐方法，它结合了KTO的样本效率与RPO的上下文感知加权优点，旨在以更少的演示数据实现更优的策略对齐。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架基于一个预训练的参考扩散策略，该策略在大量无偏好演示数据集 $D_{\text{ref}}$ 上训练，代表执行任务的默认或中性策略。为了将其与用户偏好对齐，引入一个新的偏好数据集 $D_{\text{pref}}$，其中包含“获胜”（偏好）演示和“失败”（非偏好）演示。然后，使用偏好损失函数对策略 $\pi_{\theta}$ 进行微调，通过显式对比偏好与非偏好行为，引导策略学习偏好行为，同时抑制非偏好行为。这种方法比仅在获胜演示上训练扩散策略更有效和样本高效。</p>
<p><img src="https://arxiv.org/html/2602.09583v1/imgs/method_reb.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：本文采用的通用偏好对齐框架。首先在大型参考演示集 $D_{\text{ref}}$ 上训练参考模型。为了将其与用户偏好策略对齐，收集新的获胜演示，并与失败演示（来自 $D_{\text{ref}}$ 或其他来源）组合成 $D_{\text{pref}}$。偏好损失通过对比偏好与非偏好行为来对齐新策略 $\pi_{\theta}$。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>视觉运动扩散模型基础</strong>：采用DDPM框架，将扩散过程应用于动作序列 $A_t$，并通过以观测 $O_t$ 为条件的分数网络 $\epsilon_{\theta}$ 建模条件分布 $p(A_t | O_t)$。训练损失为预测噪声的MSE损失：$\mathcal{L}<em>{DDPM}(\theta)=MSE(\epsilon_k, \epsilon</em>{\theta}(O_t, A_t^0+\epsilon_k, k))$。</p>
</li>
<li><p><strong>偏好对齐框架</strong>：</p>
<ul>
<li><strong>Diffusion-DPO</strong>：直接对比偏好与非偏好行为，避免奖励建模和强化学习。其损失函数衡量当前策略 $\epsilon_{\theta}$ 相对于参考策略 $\epsilon_{\text{ref}}$，在处理偏好与非偏好演示时的相对优势。</li>
<li><strong>Diffusion-RPO</strong>：扩展DPO，引入基于语义相似性的上下文重加权方案。每个批次内的每个获胜样本会与所有失败样本进行对比，相似度高的配对获得更大权重（公式5），使模型更关注语义上相近的困难样本对。</li>
<li><strong>Diffusion-KTO</strong>：仅需每个样本的二元（获胜/失败）标签，无需配对比较。它通过计算每个样本策略与参考策略的偏差，并利用Sigmoid效用函数进行对齐。</li>
<li>**Diffusion-RKO (本文提出)**：结合KTO与RPO的优势。它像KTO一样使用二元标签，同时集成RPO的基于相似性的批次重加权，以强调困难负样本和模糊的获胜样本。具体为：首先计算批次中获胜与失败样本嵌入的相似度矩阵 $\omega_{i,j}$；然后为每个样本计算一个权重标量 $s_b$（对于获胜样本，权重为 $1 + \max_j \omega_{i,j}$，强调靠近决策边界的样本；对于失败样本，权重为 $\sum_i \omega_{i,j}$，强调被许多获胜样本包围的困难负样本）；最后使用这些权重对KTO损失进行加权：$\mathcal{L}<em>{\text{Diffusion-RKO}}(\theta)=-\frac{1}{\sum</em>{b=1}^{B}s_b}\sum_{b=1}^{B}s_b\cdot\sigma\left(q_b\cdot A_b\right)$。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：本文的核心创新在于提出了RKO方法，它首次将KTO的单一样本二元反馈效率与RPO的上下文感知对比加权机制相结合。这种结合使得模型能够更聚焦于学习特征空间中偏好与非偏好行为重叠的“困难”区域，从而在理论上和实验上实现了更高效的偏好对齐。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>任务与数据集</strong>：在真实机器人上进行布料折叠实验，涉及三种衣物（裤子、袖子、T恤），每种衣物对应三种不同的折叠偏好设置（pref_1, pref_2, pref_3），如图3所示。采用循环数据集构建策略，确保每个偏好评估时，获胜、失败演示和参考模型训练数据来自不同的偏好源。</li>
<li><strong>评估指标</strong>：采用逐步评分方案，根据拾取和放置动作的正确性打分（图3），总分归一化为1。这比二元成功/失败提供了更丰富的信号。</li>
<li><strong>对比方法</strong>：Diffusion-DPO, Diffusion-RPO, Diffusion-KTO, 提出的Diffusion-RKO，以及仅在偏好演示上训练的普通DDPM基线。</li>
<li><strong>实验平台</strong>：两个UFactory Lite6机器人，配备三个RealSense D435相机（鸟瞰图和腕部）。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.09583v1/imgs/fold_tasks.png" alt="任务图示"></p>
<blockquote>
<p><strong>图3</strong>：三种衣物类型各自的三种折叠偏好图示。每个子图显示了左臂（橙色）和右臂（浅蓝色）的拾取（圆圈）和放置（菱形）位置。底部分数已归一化，完整正确折叠序列的总分为1。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.09583v1/imgs/robot_setup.png" alt="实验设置"></p>
<blockquote>
<p><strong>图4</strong>：实验装置实物图。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2602.09583v1/imgs/trousers_vs_sleeves_halfpage.png" alt="结果对比"></p>
<blockquote>
<p><strong>图5</strong>：裤子和袖子折叠任务的性能对比结果。左图显示在固定60个获胜演示下，各方法在所有偏好上的平均成功率。右图显示了样本效率曲线，即随着获胜演示数量从20增加到95，平均成功率的变化。</p>
</blockquote>
<ol>
<li><strong>性能对比</strong>：在固定使用60个获胜演示进行训练时，RKO在裤子和袖子任务上的平均表现最佳（裤子：<del>0.72成功率；袖子：</del>0.68成功率），显著优于普通DDPM基线（裤子：<del>0.58；袖子：</del>0.52）和其他偏好优化方法（DPO, RPO, KTO）。</li>
<li><strong>样本效率</strong>：随着训练演示数量增加，所有方法性能均提升。RKO在几乎所有数据规模下都保持领先，尤其是在数据较少时（如20个演示）优势更明显，证明了其卓越的样本效率。</li>
<li><strong>消融实验</strong>：对RKO进行消融实验（将相似性重加权因子设为1，即退化为未加权的KTO），结果显示加权版本性能更好，验证了相似性重加权机制的有效性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>RKO</strong>，一种新颖的偏好对齐方法，巧妙结合了KTO的样本效率与RPO的上下文感知对比加权机制。</li>
<li>首次在变形物体操作（DOM）领域，对多种偏好优化框架（DPO、RPO、KTO、RKO）在对齐预训练扩散策略方面的性能进行了系统性的实证比较。</li>
<li>在三种真实衣物折叠任务上进行了广泛评估，证明了基于偏好的对齐方法（尤其是RKO）相比普通扩散策略微调，在性能和样本效率上的优越性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，其实验评估集中于布料折叠任务，虽然涵盖了不同衣物类型和偏好，但方法的通用性在其他DOM任务（如辅助穿衣）或更广泛的机器人操作中仍需进一步验证。</p>
<p><strong>启示</strong>：这项工作凸显了结构化偏好学习在扩展复杂变形物体操作任务中个性化机器人行为方面的重要性和可行性。RKO的成功表明，结合不同偏好优化范式的优势是提升对齐效率的有效途径。它为未来研究指明了方向：如何将此类方法更广泛地应用于多样化的机器人技能个性化，以及如何进一步降低对高质量人类演示数据的依赖。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何让机器人在操作可变形物体（如布料）时适应人类个性化偏好。针对偏好难以量化且演示数据有限的问题，作者提出了RKO方法，该方法融合了RPO和KPO框架的优势，能够高效地对预训练的视觉运动扩散策略进行偏好对齐微调。在真实布料折叠任务上的实验表明，采用RKO等偏好对齐策略相比标准微调方法，在任务性能和样本效率上均表现出显著优越性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.09583" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>