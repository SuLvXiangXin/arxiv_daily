<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.10628" target="_blank" rel="noreferrer">2507.10628</a></span>
        <span>作者: Dandan Tu Team</span>
        <span>日期: 2025-07-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于可验证奖励的强化学习（RLVR）已成为提升大型语言模型（LLM）复杂推理能力的有力范式，其代表方法如分组相对策略优化（GRPO）。然而，主流在策略RL方法面临显著的训练不稳定和低效问题。其核心原因在于能力-难度不匹配：训练数据的复杂性常常超出模型当前的能力，导致奖励信号极其稀疏（模型生成的所有响应都错误，奖励全为零），学习进度停滞。这对于能力受限的小型、高效LLM尤为严重。</p>
<p>本文针对RLVR中因奖励稀疏导致的训练不稳定与低效这一具体痛点，提出了一个难度感知的新视角。核心思路是：动态评估样本难度，并自适应地通过提示精炼提供不同级别的真实解迹指导，对困难样本转向模仿学习以获取有效梯度，对可处理样本则保持标准在策略RL以鼓励探索，从而构建一个平滑优化的学习课程。</p>
<h2 id="方法详解">方法详解</h2>
<p>GHPO框架旨在动态平衡在线强化学习与模仿学习。其整体流程如下：对于一个查询-答案对 (q, a)，首先从当前策略模型采样G个响应并评估其二元奖励。随后，框架通过分析这组奖励的稀疏性来自动检测该样本对当前模型的难度。根据难度判断，自适应提示精炼模块会决定是否以及如何将部分真实解迹（ground-truth trace）整合到原始提示中，形成精炼后的提示。最终，使用精炼后的提示（对于困难样本）或原始提示（对于简单样本）以及对应的响应和奖励，计算GRPO目标函数来更新策略模型。</p>
<p><img src="https://arxiv.org/html/2507.10628v2/extracted/6628499/Figures/ghpo3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：GHPO框架示意图。框架包含两个核心模块：1）自动化难度检测：根据一组响应的奖励判断样本难度；2）自适应提示精炼：基于难度，通过整合不同级别的真实解迹（如完整解、关键步骤提示或无提示）来精炼输入提示。最终根据精炼后的提示和响应进行策略优化。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>自动化难度检测</strong>：该模块通过分析一组G个响应的奖励模式来判断样本难度。具体来说，如果所有奖励均为零（即模型完全无法解决），则判定为“困难”；如果存在非零奖励，则判定为“可处理”。这种基于在线采样奖励的判断方式，能够动态适应模型能力的演变。</li>
<li><strong>自适应提示精炼</strong>：根据难度检测结果，采用预定义的策略修改输入提示。对于被判定为“困难”的样本，会将部分真实解迹（例如，解题的前几个步骤或关键提示）与原始问题q拼接，形成新的提示<code>q&#39; = [q; h]</code>，其中h为提供的指导。对于“可处理”的样本，则保持原始提示<code>q</code>不变。这使得模型在面对难题时，其输出分布能被引导至接近正确答案的区域，从而产生非零奖励和有效的学习信号。</li>
</ol>
<p>GHPO优化的目标函数形式上与GRPO（公式3）一致，但关键区别在于输入提示<code>q</code>可能被自适应地替换为精炼后的提示<code>q&#39;</code>。其创新点具体体现在：1) <strong>动态性</strong>：难度检测和指导的提供是基于当前模型能力的在线判断，而非静态、预定义的课程。2) <strong>混合性</strong>：在同一训练批次中，无缝切换于标准的在策略RL（探索）和基于指导的模仿学习之间，同时解决了奖励稀疏问题和避免了对已掌握样本的过度指导。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了六个具有挑战性的数学推理基准数据集：NuminaMath-1.5、MATH（代数、计数与概率、数论、预微积分）、AIME 2024和奥数基准（Olympiad Bench）。实验平台基于Qwen2.5-7B-Base和Llama-3.2-3B-Instruct模型进行训练与评估。</p>
<p>对比的基线方法包括：标准的监督微调（SFT）、在策略RLVR方法GRPO、动态采样方法DAPO、课程学习方法（CL）以及混合离线/在线方法ORPO。</p>
<p>关键实验结果：在Qwen2.5-7B-Base模型上，GHPO在六个基准上的平均性能相比强大的GRPO基线提升了约5%， consistently outperforming。具体来看，在NuminaMath-1.5上，GHPO达到了52.6%的准确率，显著高于GRPO的47.8%和SFT的43.8%。</p>
<p><img src="https://arxiv.org/html/2507.10628v2/extracted/6628499/Figures/comparison_bar_chart.png" alt="整体性能对比"></p>
<blockquote>
<p><strong>图1</strong>：GHPO在Qwen2.5-7B-Base模型上多个基准的整体性能对比。图中显示，GHPO（橙色条）在所有测试集上均超越了GRPO（蓝色条）、DAPO（绿色条）和SFT（黄色条）等基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10628v2/extracted/6628499/Figures/grad_norm.png" alt="训练稳定性分析"></p>
<blockquote>
<p><strong>图8</strong>：训练过程中梯度范数的变化。GHPO（绿色线）的梯度范数在整个训练过程中保持相对稳定且较低的水平，而GRPO（红色线）的梯度范数波动剧烈且存在尖峰，表明GHPO带来了更稳定的训练动态。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10628v2/extracted/6628499/Figures/accuracy_reward.png" alt="奖励与准确率曲线"></p>
<blockquote>
<p><strong>图6</strong>：训练集奖励（左）和验证集准确率（右）随训练步数的变化。GHPO（绿色线）在训练集上能更快获得更高的奖励，并且在验证集上取得了最终更高且更稳定的性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10628v2/extracted/6628499/Figures/proportion.png" alt="指导比例变化"></p>
<blockquote>
<p><strong>图4</strong>：训练过程中被判定为需要指导的样本比例变化。随着训练进行，需要指导的样本比例从约70%迅速下降并稳定在较低水平（约10-20%），证明了难度检测模块能有效适应模型能力的增长。</p>
</blockquote>
<p>消融实验总结了各组件的贡献：1) <strong>完整的GHPO框架</strong>（动态检测+自适应指导）效果最佳。2) 仅使用<strong>静态指导</strong>（固定对部分样本提供指导）虽然优于无指导的GRPO，但弱于动态的GHPO，证明了自适应性的重要性。3) <strong>难度检测模块</strong>是关键，若随机提供指导，性能会下降。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 明确指出了RLVR中能力-难度不匹配导致的奖励稀疏问题是训练不稳定和低效的关键原因；2) 提出了GHPO这一新颖的难度感知强化学习框架，通过自动化难度检测和自适应提示精炼，动态融合模仿学习与在策略RL；3) 在多个数学推理基准上验证了GHPO的有效性，其显著提升了训练稳定性、样本效率和最终性能。</p>
<p>论文自身提到的局限性包括：自适应提示精炼的策略（如何提供指导、提供多少）是预定义且相对简单的，未来可以探索更精细、数据驱动的指导策略；此外，提供部分解迹可能存在“知识泄露”风险，需确保评估时不使用相同指导。</p>
<p>本工作对后续研究的启示在于：为资源受限模型的高效RL训练提供了一个可扩展的解决方案；其“动态平衡探索与利用”的核心思想可启发更广泛的课程学习或混合学习策略设计；如何自动化、最优化地生成或选择“指导”本身，也是一个值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对大语言模型（LLM）在基于可验证奖励的强化学习（RLVR）中，因训练数据难度与模型能力不匹配导致的训练不稳定和低效问题，提出了GHPO框架。其核心技术是难度感知的引导式混合策略优化，通过自适应提示精炼动态校准任务难度，平衡了针对超难任务的直接模仿学习与针对适中任务的探索性强化学习。实验表明，GHPO在六个数学基准上平均带来约5%的性能提升，显著优于现有的强策略强化学习和课程学习方法，有效提升了训练稳定性与最终推理能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.10628" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>