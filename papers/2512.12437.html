<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sim2Real Reinforcement Learning for Soccer skills - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Sim2Real Reinforcement Learning for Soccer skills</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.12437" target="_blank" rel="noreferrer">2512.12437</a></span>
        <span>作者: Spraggett, Jonathan</span>
        <span>日期: 2025/12/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在RoboCup人形足球机器人领域，控制相关任务（如移动、踢球）的传统方法主要包括静态关键帧动画和复杂的控制引擎（如行走引擎）。前者无法让机器人动态响应环境变化，后者则开发周期长、调参困难。强化学习（RL）虽被广泛应用，但在该领域面临三大挑战：需要复杂的奖励工程来产生自然有效的行为；策略难以从仿真环境迁移到物理世界（Sim2Real问题）；在RoboCup人形联赛中，尚无团队成功将RL应用于物理机器人的控制任务。</p>
<p>本文旨在克服传统RL方法的局限性，为人形机器人训练控制任务提出一种更高效、有效的方法。核心思路是结合课程训练（Curriculum Training）与对抗性运动先验（Adversarial Motion Priors, AMP）技术，利用AMP从参考运动数据集中学习自然运动风格以减少奖励工程的复杂性，并通过课程学习逐步提升任务难度以优化策略性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体训练流程基于近端策略优化（PPO）算法，在Nvidia Isaac Gym仿真环境中进行。流程如下：每个训练回合，机器人从准备状态开始，环境生成一个观察状态向量（包括关节位置、速度、IMU数据、脚部接触传感器等），策略网络根据该状态输出动作（目标关节位置），并应用于机器人电机。环境根据任务目标（如移动距离、踢球速度）和AMP提供的风格奖励计算总奖励。回合在机器人躯干高度低于27.5厘米（摔倒）或达到最大步数时终止。</p>
<p>核心模块是对抗性运动先验（AMP）。该模块结合了模仿学习和生成对抗网络（GAN）的训练思想。其包含一个判别器网络，用于区分由控制策略生成的机器人运动与来自参考运动数据集（如真人运动捕捉数据）的运动。在训练过程中，判别器被训练以准确区分两者，而控制策略则被训练以“欺骗”判别器，使其认为策略生成的运动来自参考数据集。判别器输出的概率值被用作额外的“风格奖励”，鼓励策略产生与参考数据分布一致的自然、拟人化运动，从而减少了对复杂手工设计奖励函数的需求。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) 将AMP技术首次应用于人形机器人足球技能（行走、踢球、跳跃）的学习，以获取更自然和动态的运动；2) 采用了课程学习策略，例如在行走训练中，先训练直线行走，再引入随机方向指令以学习全向行走；在踢球训练中，逐步将球初始位置从固定点随机化，以增加泛化能力。</p>
<p><img src="https://arxiv.org/html/2512.12437v1/sim.png" alt="方法框架"></p>
<blockquote>
<p><strong>图4</strong>：Bez机器人URDF模型在Isaac Gym仿真环境中的训练场景，展示了4096个实例并行学习跳跃技能，利用了GPU并行化加速训练。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.12437v1/bez.png" alt="机器人模型"></p>
<blockquote>
<p><strong>图6</strong>：用于Isaac Gym训练的Bez机器人URDF模型，前方放置了一个足球。碰撞网格用最小的边界框近似。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.12437v1/feet.png" alt="脚部传感器"></p>
<blockquote>
<p><strong>图7</strong>：Bez URDF模型脚部附加的压力传感器示意图。通过检测每只脚四个象限的接触情况，生成一个8维二进制向量用于稳定控制。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台与数据集：主要训练在Nvidia Isaac Gym中进行，使用RL Games库实现GPU并行化训练。为了向现实世界迁移，还在更高保真度的Webots仿真（RoboCup人形联盟虚拟赛季官方环境）中进行了策略测试。机器人平台为自定义的Bez人形机器人（高50cm，重2.3kg，18个自由度）。</p>
<p>对比的Baseline方法：论文将所提方法（PPO+AMP）与未使用AMP的纯PPO RL方法进行了对比，同时也与传统的关键帧动画方法进行了定性比较。</p>
<p>关键实验结果：</p>
<ol>
<li><strong>踢球</strong>：训练出的策略成功率达到85%，能适应球位置的变化，踢球动作比静态关键帧动画更动态。</li>
<li><strong>行走</strong>：使用AMP训练的策略达到了0.25 m/s的稳定行走速度。消融实验表明，不使用AMP的纯RL策略行走速度仅为0.18 m/s，且步态不自然、有抖动。引入AMP后步态明显更平滑、拟人。</li>
<li><strong>跳跃</strong>：策略能实现离地高度约0.08米的跳跃。</li>
<li><strong>Sim2Real迁移</strong>：尽管在仿真中取得了成功，但将训练好的策略部署到真实的Bez机器人上未能成功。策略在Webots中尚能部分工作，但在物理硬件上因仿真与现实间的动力学差异（如电机模型、摩擦、延迟）导致性能急剧下降甚至失败。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.12437v1/Kick_Train.png" alt="踢球训练曲线"></p>
<blockquote>
<p><strong>图10</strong>：踢球任务的训练奖励曲线，展示了策略在训练过程中逐渐学习并稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.12437v1/walk_amp.png" alt="行走对比"></p>
<blockquote>
<p><strong>图13</strong>：使用AMP训练的行走策略步态截图，动作连贯自然。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.12437v1/walk_bez.png" alt="行走对比"></p>
<blockquote>
<p><strong>图14</strong>：未使用AMP的纯RL策略行走步态截图，对比可见步态僵硬、不协调。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.12437v1/Walk_Random.png" alt="课程学习效果"></p>
<blockquote>
<p><strong>图33</strong>：全向行走训练奖励曲线。先训练直线行走（阶段1），后引入随机方向指令（阶段2），奖励下降后重新学习，最终获得更高性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.12437v1/Walk_straight_train.png" alt="课程学习效果"></p>
<blockquote>
<p><strong>图34</strong>：直线行走训练奖励曲线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.12437v1/Jump_train.png" alt="课程学习效果"></p>
<blockquote>
<p><strong>图35</strong>：跳跃训练奖励曲线。</p>
</blockquote>
<p>消融实验总结：</p>
<ul>
<li><strong>AMP组件</strong>：对于行走任务至关重要，显著提升了运动自然度和稳定性（速度从0.18 m/s提升至0.25 m/s）。</li>
<li><strong>课程学习</strong>：对于踢球等复杂任务有效，通过逐步增加难度（如随机化球位），使策略学习更稳健、泛化能力更强。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 成功将对抗性运动先验（AMP）应用于人形机器人足球技能（行走、踢球、跳跃）的仿真训练，证明了其能有效生成动态、自然且性能优于传统RL的运动策略；2) 展示了课程学习在训练复杂机器人技能时的优势；3) 尽管Sim2Real迁移最终未成功，但完整探索了从仿真训练到高保真仿真测试的流程，并清晰揭示了当前方法在现实迁移中面临的挑战。</p>
<p>论文明确指出了自身的局限性：最主要的局限是未能实现从仿真到真实机器人的成功策略迁移，凸显了仿真与现实间的动力学差距仍是关键障碍。此外，AMP训练需要大量的参考运动数据，且整个训练过程对计算资源（GPU并行化）要求较高。</p>
<p>本工作对后续研究的启示是：AMP作为一种减少奖励工程、提升运动质量的技术，在机器人技能学习领域具有很大潜力。然而，要真正实现Sim2Real，可能需要结合更精细的动力学建模、更广泛的域随机化（Domain Randomization）、系统辨识或在线自适应技术，以弥合仿真与现实之间的鸿沟。这项工作为未来在RoboCup乃至更广泛的人形机器人控制中应用类似的先进RL方法奠定了基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人足球技能（如踢球、行走、跳跃）的控制任务，传统强化学习方法难以适应动态现实环境的问题，提出采用课程训练和对抗运动先验（AMP）技术以提升策略的自然性与适应性。实验表明，所提方法在模拟中训练的策略更具动态性，性能优于先前方法，但策略从模拟到现实世界的转移未能成功，揭示了当前Sim2Real强化学习在完全适配真实场景方面的局限性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.12437" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>