<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01662" target="_blank" rel="noreferrer">2602.01662</a></span>
        <span>作者: Yu She Team</span>
        <span>日期: 2026-02-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，基于大视觉语言模型（VLM）的机器人操作已成为主流范式。然而，现有方法存在关键局限性：许多系统采用开环规划，生成静态计划后执行，缺乏对执行状态的持续验证和失败后的重规划，导致在非结构化、动态的真实环境中鲁棒性差。同时，现有评估大多基于模拟环境、特权状态或精心设计的受控场景，难以捕捉真实闭环执行中出现的失败模式（如遮挡、光照变化下的物体定位错误、多步语义一致性断裂等）。此外，许多系统与特定VLM深度耦合，包含模型特定的提示或设计，使得不同模型家族在统一执行协议下的公平比较变得困难。</p>
<p>本文针对VLM在真实世界、长时程、闭环机器人操作中可靠性不明确的痛点，提出了一个模型无关的机器人代理平台和基准测试。其核心思路是构建一个支持感知、思考、执行闭环的模块化流水线，并利用该平台在真实非结构化环境中系统性地评估现有VLM的能力，揭示离线测试无法发现的失败模式。</p>
<h2 id="方法详解">方法详解</h2>
<p>AgenticLab的整体框架是一个模块化的闭环机器人代理流水线，其核心是集成任务解析、感知、规划、执行、验证和重规划。输入是自然语言指令和机器人本体传感器的RGB-D观测，输出是完成指定任务的一系列机器人动作。</p>
<p><img src="https://arxiv.org/html/2602.01662v2/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图3</strong>：AgenticLab的闭环推理流水线。人类提供指令，机器人获取观测并解析任务生成初始计划。绿色箭头表示正常执行路径，每个模块执行后都会进行VLM验证；若验证失败，则沿红色箭头进行重试或基于新观测重规划。抓取规划首先使用肩部相机，若失败则切换至腕部相机进行局部重规划。</p>
</blockquote>
<p>该框架基于三个核心原则：1) <strong>模型无关性</strong>：通过统一接口支持不同VLM（如Gemini, GPT, Qwen）的即插即用。2) <strong>闭环推理</strong>：迭代利用视觉反馈验证动作前提和效果，检测失败并触发重规划。3) <strong>模块化功能分解</strong>：将流水线分解为职责清晰的模块，便于解释、组合和交换。</p>
<p>核心模块具体如下：</p>
<ol>
<li><strong>See（感知）</strong>：采用多视角（固定肩部相机+腕部相机）开放词汇感知。系统将RGB-D观测转换为结构化场景表示，支持开放词汇的物体定位和空间关系理解。感知策略是模块化的，可依赖VLM直接预测物体位置，或使用LangSAM进行分割后由VLM验证。感知是迭代进行的，在每次动作后更新场景表示，并能根据验证反馈切换视角（例如，从全局肩部视图切换到局部腕部视图以细化抓取规划）。</li>
<li><strong>Think（思考）</strong>：这是一个模型无关的推理与规划层，包含三个关键模块：<ul>
<li><strong>任务解析器</strong>：将自然语言指令转换为结构化的规划域定义语言（PDDL）问题。利用预定义的PDDL域文件，VLM根据视觉观测和语言指令实例化物体、初始状态谓词和目标谓词。随后，基于Fast Downward的符号规划器生成满足目标的高层动作基元序列。</li>
<li><strong>动作检查器</strong>：在执行每个动作前，评估其前提条件是否成立；执行后，利用更新后的视觉输入重新评估动作效果，并检测失败（如抓取失败、意外状态变化）。在所有计划动作完成后，进一步验证最终场景是否满足所有目标条件。</li>
<li><strong>抓取规划器</strong>：首先使用AnyGrasp从RGB-D观测生成候选抓取位姿，然后由VLM评估其语义正确性（是否抓取目标物体）和物理可行性（是否可能导致碰撞或不稳定）。如图3所示，系统优先使用肩部相机规划，若候选抓取被拒绝，则使用腕部相机重新规划以获得更局部的视觉证据。</li>
</ul>
</li>
<li><strong>Act（执行）</strong>：将机器人行为抽象为一组高层动作基元（如抓取、放置、开/关抽屉）。每个基元通过基于位置的控制执行。例如，对于抓取基元，VLM通过视觉定位指定感兴趣区域，AnyGrasp在该区域内生成6自由度抓取位姿，机器人通过笛卡尔空间轨迹规划执行抓取。每次动作后都进行离散的结果验证。</li>
</ol>
<p>与现有方法相比，其创新点具体体现在：1) <strong>模型无关的标准化评估接口</strong>，使得公平比较不同VLM成为可能；2) <strong>强调闭环推理与在线重规划</strong>，而非一次性开环规划；3) <strong>模块化设计</strong>，允许对VLM的各项能力（如任务解析、视觉定位、动作验证）进行独立评估和针对性改进。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了AgenticLab平台，在真实物理机器人（UR5e机械臂、自制夹爪、Azure Kinect肩部相机和RealSense D405腕部相机）上进行了评估。基准测试包含五个精心设计的真实世界操作任务（排序、堆叠、填字游戏、重定向、厨房整理），覆盖了语义定位、空间推理和长时程规划等挑战，并在实验室、杂乱室内厨房和非结构化室外环境三种场景下进行测试。</p>
<p><img src="https://arxiv.org/html/2602.01662v2/x3.png" alt="任务示例"></p>
<blockquote>
<p><strong>图4</strong>：用于真实世界评估和基准测试的操作任务。这些任务涵盖了空间推理、视觉语言定位和长时程规划等一系列挑战。</p>
</blockquote>
<p>对比的基线方法包括多个主流VLM（Gemini 3 Flash/Pro/Robotics, Qwen-VL-Max, Qwen3-VL-Plus, GPT-5.2, GPT-4o, Claude-4.5-Opus/Sonnet）以及本地部署模型（Qwen2.5-VL-7B, Molmo-7B）。实验还对比了组合式流水线与单VLM流水线，并进行了消融实验。</p>
<p>关键实验结果如下：</p>
<ol>
<li><strong>单VLM驱动完整流水线的能力</strong>：在排序任务（20次试验，6种物体设置）中，Gemini Flash成功率最高（75.0%），而Qwen-VL-Max和GPT-5.2成功率接近0%。失败主要源于动作状态验证中的幻觉（例如，成功抓取后却声称物体不在夹爪中）。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.01662v2/x4.png" alt="失败模式分解"></p>
<blockquote>
<p><strong>图5</strong>：排序任务中单VLM流水线的失败模式分解。结果显示不同VLM的失败主要由哪些流水线组件导致。</p>
</blockquote>
<ol start="2">
<li><strong>模块化基准测试</strong>：对各模块独立评估（表II）发现，任务解析并非云端模型的主要瓶颈；视觉定位能力与模型大小无关，小型专用模型可表现优异；动作检查器在孤立VQA测试中成功率常超过50%，但在闭环执行中因错误累积导致整体成功率急剧下降（例如，单个动作检查准确率90%时，完成需6次检查的任务概率仅为~53%）；抓取评估器的性能与“思考”推理能力（如碰撞风险、稳定性判断）强相关，Gemini Pro（77.8%）、GPT-5.2（75.9%）表现最佳。</li>
<li><strong>组合式流水线 vs. 单VLM</strong>：将任务解析、视觉定位、目标检查分配给不同专家模型（Gemini Flash, Qwen3-VL-Plus, Claude Opus）的组合流水线，在从实验室转移到厨房和室外场景时，并未始终优于单VLM（Gemini Flash）基线（图6）。两者在环境变化下均出现性能下降。组合流水线的优势在于能针对特定瓶颈模块（如堆叠任务中的精确定位）进行针对性补强。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.01662v2/x5.png" alt="跨环境性能比较"></p>
<blockquote>
<p><strong>图6</strong>：跨环境性能比较。将Gemini Flash基线与组合式流水线在五种操作任务上进行比较，性能使用量化部分完成度的任务进度得分衡量。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：<ul>
<li><strong>动作检查器</strong>（图7）：在受干扰场景下，完全禁用检查器会导致任务进度得分下降，尤其是在动作强依赖的堆叠任务中。启用完整动作检查器虽增加执行时间，但能早期检测并纠正错误。</li>
<li><strong>抓取规划器</strong>（图8a）：在杂乱场景中，禁用抓取评估会导致任务成功率大幅下降，因为系统更可能执行易碰撞或抓错物体的抓取。启用评估能提升性能，但代价是增加规划时间。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2602.01662v2/x6.png" alt="动作检查器消融"></p>
<blockquote>
<p><strong>图7</strong>：干扰场景下的动作检查器消融实验。报告了三种验证模式（无动作检查、仅目标检查、完整动作检查）下的任务进度得分和执行时间。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.01662v2/x7.png" alt="抓取规划器消融与VLA对比"></p>
<blockquote>
<p><strong>图8</strong>：(a) 抓取规划器消融研究。比较启用和禁用抓取评估时的性能与执行时间。在杂乱场景中，启用评估能提高性能，但增加执行时间。(b) 微调VLA与AgenticLab的性能对比。在排序和堆叠任务上，对比了使用40/30次演示微调的VLA与AgenticLab流水线。</p>
</blockquote>
<ol start="5">
<li><strong>与微调VLA的比较</strong>：在排序和堆叠任务上，将AgenticLab与一个使用40/30次演示进行微调的VLA模型（π_0.5）进行比较（图8b）。结果显示，即使经过任务特定微调，VLA的性能也未能超越AgenticLab的零样本流水线，突显了微调可能导致泛化能力下降以及零样本系统设计的价值。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了一个<strong>模型无关的、支持闭环推理的真实世界机器人代理平台（AgenticLab）</strong>，其模块化设计支持即插即用评估和部署。2) 建立了一个<strong>真实机器人基准测试</strong>，系统性地评估了VLM在长时程、闭环操作中的能力，揭示了离线测试和仿真中无法捕捉的失败模式（如验证错误累积、动态场景下的定位失效）。3) <strong>开源了完整的软硬件栈设计</strong>，旨在促进可复现的评估，并降低具身智能研究的入门门槛。</p>
<p>论文自身提到的局限性包括：当前VLM在动作状态验证等关键模块上仍是性能瓶颈；组合式流水线的性能提升受限于最弱模块；硬件平台虽可重复部署，但仍存在物理限制（如负载、工作空间）。</p>
<p>对后续研究的启示在于：1) <strong>需要开发针对机器人闭环交互（特别是状态验证）进行优化的VLM或评估协议</strong>，因为这是当前系统的关键短板。2) <strong>模块化、可组合的代理架构</strong>提供了一条无需昂贵端到端重新训练即可提升系统性能的实用路径。3) <strong>真实世界的基准测试至关重要</strong>，它暴露了仿真和静态评估中隐藏的问题，为模型改进指明了方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出AgenticLab，一个模型无关的真实世界机器人代理平台与基准，旨在解决现有基于大视觉语言模型（VLM）的操纵系统在非结构化、长时程闭环执行中能力不明确、难以标准化评估的问题。平台核心是一个集成了感知、任务分解、在线验证与重规划的闭环代理流程。通过该平台对先进VLM代理进行真实机器人任务基准测试，揭示了离线测试无法捕捉的多种故障模式，包括多步基础一致性崩溃、遮挡与场景变化下的物体基础失效，以及空间推理不足导致的操纵不可靠。平台将开源以支持可复现评估。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01662" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>