<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.22445" target="_blank" rel="noreferrer">2511.22445</a></span>
        <span>作者: Jitendra Malik Team</span>
        <span>日期: 2025-11-27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习已成为机器人从演示中获取视觉运动技能的关键范式。然而，现有方法在面对空间（如视角、布局）和视觉（如外观、光照）随机化时，往往难以泛化，容易过拟合。当前主流的多模态融合方法，例如早期/晚期特征拼接或将RGB作为点属性附着，在分布偏移下通常只能带来有限或不稳定的性能提升。其核心问题在于缺乏对模态互补性的有效利用，容易导致“模态主导”，即模型过度依赖某一模态（如易获取的RGB语义）而忽略另一模态（如几何精确的点云）。</p>
<p>本文针对模仿学习策略在视觉和空间扰动下泛化能力不足的痛点，提出了一个名为视觉-几何扩散策略的新视角。其核心思路是：通过一个互补感知融合模块，利用模态级丢弃技术强制策略平衡地使用RGB和点云线索，从而学习到模态不变且互补的表征，显著提升策略的鲁棒性和泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>VGDP的整体框架由感知、集成和决策三个主要阶段构成，输入为单视角RGB-D图像和机器人关节状态，输出为去噪后的动作序列。</p>
<p><img src="https://arxiv.org/html/2511.22445v1/images/pipeline.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Visual-Geometry Diffusion Policy 概述。(a) 观测：环境由单视角RGB-D相机和机器人关节状态捕获。(b) 感知：每个模态由独立的编码器处理，获得丰富的语义表征。(c) 集成：通过互补感知融合层，结合交叉注意力和模态丢弃，学习跨模态依赖关系，生成平衡的融合特征。(d) 决策：以融合特征为条件，对噪声动作进行去噪，生成最终动作。</p>
</blockquote>
<p><strong>感知模块</strong>包含三个独立的编码器：</p>
<ol>
<li><strong>图像编码器</strong>：采用ResNet-18处理256×256的RGB帧，通过全局平均池化得到512维全局特征。</li>
<li><strong>3D编码器</strong>：从RGB-D图像反投影得到点云，裁剪至任务工作空间后，使用最远点采样下采样至4096个点，再通过轻量化的DP3（基于PointNet）编码器提取64维全局几何特征。</li>
<li><strong>低维编码器</strong>：使用一个简单的MLP将机器人关节状态映射到高维空间。</li>
</ol>
<p><strong>集成模块（互补感知融合层）</strong> 是本方法的核心创新点，旨在防止模态崩溃并鼓励平衡使用互补信息。其工作流程如下：</p>
<ol>
<li><strong>模态级丢弃</strong>：在训练时，以概率 <code>p=0.2</code> 随机丢弃整个RGB或点云特征流。这是主要的正则化器，强制策略必须学会依赖两种模态的互补信息，从而学习到模态不变且更具表达力的表征。</li>
<li><strong>投影与交互</strong>：幸存的特征被投影到一个共享的256维嵌入空间。随后，一个轻量级的双向交叉注意力层在存活的模态之间进行最小化的信息交互。</li>
<li><strong>稳定输出</strong>：通过残差连接保留单模态能力，并应用元素级丢弃进一步防止特征间过度协同适应，最终产生稳定、平衡的潜在表征 <code>c</code>，传递给决策头。</li>
</ol>
<p><strong>决策模块</strong>采用扩散策略头进行动作生成。训练时，对真实动作添加随机步数 <code>T</code> 的噪声，网络的目标是预测所添加的噪声，损失函数为标准L2损失。推理时，从高斯噪声初始化动作，并重复以融合特征 <code>c</code> 为条件进行去噪，得到可执行的动作序列。</p>
<p><strong>与现有方法的创新点</strong>：本方法的关键创新不在于复杂的融合架构（如交叉注意力），而在于<strong>通过模态级丢弃施加的互补性约束</strong>。消融实验表明，没有丢弃机制时，即使使用交叉注意力，融合模型也会坍塌到当前任务下占主导地位的单模态上。模态丢弃强制模型必须利用两种模态的互补性，这才是提升表征表达能力和鲁棒性的根本原因。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在包含18个任务的统一仿真基准<strong>RoboVerse</strong>和4个真实世界任务上进行了评估。仿真任务源自Libero、ManiSkill和RLBench等主流基准，并设置了三个渐进的随机化级别（L0：无随机化；L1：随机化材质和物体位置；L2：在L1基础上额外随机化相机位置）。对比了<strong>7种基线编码器</strong>，包括ResNet18-RGB、ResNet18-RGBD、ViT-RGBD、MultiViT-RGBD、DP3（点云）和SparseUNet（点云），所有方法共享相同的扩散策略头。</p>
<p><img src="https://arxiv.org/html/2511.22445v1/images/plots/task_plot.png" alt="任务性能对比"></p>
<blockquote>
<p>**图3(a)**：不同编码器在各项任务（跨三个随机化级别平均）上的性能对比。VGDP在几乎所有任务上都优于其他基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.22445v1/images/plots/randomization_plot.png" alt="随机化级别性能对比"></p>
<blockquote>
<p>**图3(b)**：不同编码器在各随机化级别（跨所有任务平均）上的性能对比。VGDP在不同级别间性能保持近乎不变，而其他编码器性能随随机化增强而显著下降。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能与视觉泛化</strong>：VGDP在三个随机化级别上均取得最高成功率，平均超过单模态基线**39.1%<strong>。在视觉随机化场景下，其成功率平均提升达</strong>41.5%<strong>。尤为重要的是，其性能在不同随机化级别间的波动仅为</strong>2.53%**，稳定性比最佳单模态基线高出约11倍。</li>
<li><strong>空间泛化</strong>：在空间随机化任务中，VGDP的平均成功率提升**15.2%<strong>。在需要厘米级精控的任务上，成功率提升</strong>18.77%**。</li>
<li><strong>跨任务稳定性</strong>：VGDP将任务间性能的相对离散度降低了**47.3%**，表现出更高的性能下限和更窄的方差。</li>
<li><strong>分布外泛化</strong>：在最强的L2随机化下，VGDP的平均成功率比基线高**55.3%<strong>，其“分布内-分布外”性能差距比基线小</strong>59.5%**，表明其学到的表征具有出色的可迁移性。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.22445v1/x1.png" alt="随机化示例"></p>
<blockquote>
<p><strong>图4</strong>：不同随机化级别下的环境示例。展示了同一任务在材质、物体位置和相机视角上的不同程度随机化。</p>
</blockquote>
<p><strong>消融实验分析（基于CloseBox任务）</strong>：</p>
<ul>
<li><strong>互补感知丢弃是关键</strong>：移除模态丢弃后，融合模型的表现会紧密跟随当前随机化级别下占主导的单模态编码器（RGB或点云）。这表明<strong>没有丢弃，交叉注意力本身无法阻止模态坍塌</strong>。加入丢弃后，模型被迫利用互补线索，表征表达力和OOD鲁棒性大幅提升。</li>
<li><strong>融合架构对比</strong>：与特征拼接和早期融合相比，交叉注意力能实现最强的融合效果。但<strong>其全部益处仅在与模态丢弃结合时才得以体现</strong>。早期融合在L0和L1下完全失效，表明RGB分支未被利用。</li>
</ul>
<p><strong>真实世界实验</strong>：<br>在PickButter、FetchBottle、PourCereal和InsertPlug四个真实任务上评估。</p>
<p><img src="https://arxiv.org/html/2511.22445v1/x2.png" alt="真实世界任务"></p>
<blockquote>
<p><strong>图5</strong>：真实世界基准任务图示。包括从复杂场景中拾取物体、大范围空间泛化取物、6-DoF连续控制倾倒以及高精度力控插入。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.22445v1/x3.png" alt="空间泛化评估"></p>
<blockquote>
<p><strong>图6</strong>：FetchBottle任务的空间泛化评估结果鸟瞰图。策略在30个网格位置（虚线格）上训练，在143个位置（全部网格）上评估。VGDP和RGB编码器展现出良好的泛化能力，而点云和RGBD方法严重失败。</p>
</blockquote>
<ol>
<li><strong>任务性能</strong>：VGDP在四个任务上均取得最佳或接近最佳的性能（表VI）。</li>
<li><strong>空间泛化</strong>：在FetchBottle任务中，VGDP和RGB编码器能良好地泛化到未见过的工作空间位置，而点云和RGBD方法因真实世界深度图像的噪声而严重失败，常常回归到熟悉的训练位置。</li>
<li><strong>精细视觉感知</strong>：在InsertPlug任务中，插座狭小，插入需要近乎垂直的精确控制。VGDP在所有测试位置取得了最一致的成功率，显著优于其他方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.22445v1/x4.png" alt="插入任务挑战"></p>
<blockquote>
<p><strong>图7</strong>：InsertPlug任务中的关键视觉挑战。(a) 插入过程需严格的垂直对齐。(b) 插座尺寸非常小（与硬币对比）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>视觉-几何扩散策略</strong>，其核心是一个<strong>互补感知融合模块</strong>，通过模态级丢弃强制平衡利用RGB和几何线索，有效解决了模仿学习中多模态融合的模态主导问题。</li>
<li>通过大量实验揭示了<strong>模态丢弃机制是提升融合表征表达力和鲁棒性的关键</strong>，而交叉注意力主要作为一个轻量级的交互机制，其有效性依赖于互补性约束。</li>
<li>在涵盖18个仿真任务和4个真实任务的综合基准上验证了VGDP的卓越性能，在视觉和空间扰动下实现了显著的性能提升和稳定的泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文指出，在真实世界实验中，点云编码器（DP3）由于深度传感器噪声的影响，表现显著下降，这凸显了在真实嘈杂环境中依赖纯几何信息的挑战。</p>
<p><strong>后续研究启示</strong>：本工作强调了在机器人多模态学习中<strong>主动设计训练约束（如模态丢弃）以强制互补性</strong>的重要性，而非仅仅依赖更复杂的融合网络架构。这种思想可扩展到其他需要鲁棒融合的模态组合中。同时，如何进一步提升几何表征在真实噪声环境下的鲁棒性，是一个值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习策略在空间与视觉随机化下泛化能力弱、易过拟合的核心问题，提出了Visual-Geometry Diffusion Policy (VGDP)。其关键技术是互补感知融合模块，通过模态级丢弃强制策略平衡利用RGB与点云线索，并以交叉注意力作为轻量交互层。实验表明，该方法在18个模拟任务和4个真实任务上平均性能提升39.1%，在视觉与空间扰动下的鲁棒性分别平均提升41.5%和15.2%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.22445" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>