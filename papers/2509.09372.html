<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.09372" target="_blank" rel="noreferrer">2509.09372</a></span>
        <span>作者: Wang, Yihao, Ding, Pengxiang, Li, Lingxiao, Cui, Can, Ge, Zirui, Tong, Xinyang, Song, Wenxuan, Zhao, Han, Zhao, Wei, Hou, Pengxu, Huang, Siteng, Tang, Yifan, Wang, Wenhui, Zhang, Ru, Liu, Jianyi, Wang, Donglin</span>
        <span>日期: 2025/09/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通常通过在机器人数据上预训练大规模视觉语言模型来弥合感知与动作空间之间的鸿沟。虽然这种方法性能优异，但也带来了高昂的训练成本，并导致模型严重依赖大规模VLM、微调速度慢、GPU内存消耗高以及推理效率低等问题。因此，本文旨在探索一个VLA领域至关重要但鲜少被讨论的问题：如何更有效地桥接视觉语言表示与动作空间？为此，本文提出了VLA-Adapter这一新颖的桥接范式。其核心思路是：通过系统性地分析不同视觉语言条件对动作生成的影响，并基于关键发现设计一个带有桥接注意力的轻量级策略模块，从而自主地将最优条件注入动作空间，仅使用一个0.5B参数的骨干网络且无需任何机器人数据预训练，即可实现高性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLA-Adapter的整体框架如图3所示。在时间步t，模型的输入包括：第三视角图像、夹爪图像、语言指令以及额外的ActionQuery。视觉图像通过DINOv2和SigLIP编码，语言指令被分词。VLM（基于Prismatic-VLMs架构）的输出是特定层的原始特征潜在表示和ActionQuery潜在表示，它们将作为策略模块的条件。</p>
<p><img src="https://arxiv.org/html/2509.09372v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：VLA-Adapter提出的VLA框架。关键组件是有效的条件探索和注意力设计。右侧列出了关于“层”和“类型”的四种条件配置。</p>
</blockquote>
<p>本文首先系统性地探索了何种感知信息对动作生成至关重要，主要聚焦两个问题：1）VLM中哪一层的特征对策略网络更有效？2）ActionQuery特征是否比原始特征更好？通过在复杂长视野任务LIBERO-Long上的实验（结果如图4所示），得出了三个关键发现：1）对于原始特征，中间层特征比深层特征表现更好，因为深层特征偏向语义信息，而中间层能更有效地融合图像与文本信息，保留更丰富的多模态细节。2）对于ActionQuery特征，深层特征优于其他层，因为ActionQuery从头开始训练，深层聚合了更丰富的多模态细节。3）使用多层特征通常优于单层特征，不仅能提升性能，还免去了设计时选择最佳层的工作。</p>
<p><img src="https://arxiv.org/html/2509.09372v2/x3.png" alt="条件比较"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO-Long上对VLA-Adapter框架中四种条件的比较。蓝/绿线代表单层条件，蓝/绿柱代表全层条件。</p>
</blockquote>
<p>基于上述发现，本文设计了一个基于L1的轻量级策略网络，其核心是桥接注意力模块。策略的输入包括：原始特征潜在表示、ActionQuery潜在表示、初始动作序列和本体感知状态。策略网络与VLM层数相同，每一层都包含一个桥接注意力模块和一个前馈网络。</p>
<p><img src="https://arxiv.org/html/2509.09372v2/x4.png" alt="桥接注意力结构"></p>
<blockquote>
<p><strong>图5</strong>：带有桥接注意力的策略网络。每一层的原始特征和ActionQuery特征在桥接注意力中与对应层的动作潜在表示进行整合。</p>
</blockquote>
<p>桥接注意力的具体结构如图5所示。每个桥接注意力由两个交叉注意力和一个自注意力构成。第一个交叉注意力将原始特征作为键和值，动作潜在表示作为查询。第二个交叉注意力将ActionQuery特征与本体感知嵌入拼接后作为键和值，动作潜在表示作为查询。自注意力则以动作潜在表示自身作为查询、键和值。为了有选择地将某些原始特征注入动作空间，本文引入了一个可学习的比例参数g，通过tanh函数缩放第一个交叉注意力的输出。最终，三个注意力的输出被拼接，并通过残差前馈网络传递，逐层迭代后输出最终的动作序列。训练采用端到端方式，策略网络从头开始训练，损失函数为L1损失。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了模拟与真实机器人基准数据集，主要包括LIBERO（含Spatial, Object, Goal, Long四个套件）和CALVIN（ABC→D零样本泛化任务）。对比了包括OpenVLA、OpenVLA-OFT、π₀、SmolVLA、GR00T N1等在内的20余种近期发布的先进VLA方法。实验平台为4张NVIDIA H100 GPU。</p>
<p>首先，在LIBERO-Long上验证了VLA-Adapter范式的有效性。如表2所示，当骨干网络为未经机器人数据预训练的Qwen2.5-0.5B时，VLA-Adapter相比OpenVLA-OFT范式将成功率从85.8%提升至95.0%（相对提升9.2%）。即使骨干网络冻结，VLA-Adapter仍能达到86.4%的成功率，显著优于SmolVLA的77.0%，而OpenVLA-OFT在冻结骨干下完全失效（0.0%），如表3所示。</p>
<p><img src="https://arxiv.org/html/2509.09372v2/x6.png" alt="有效性对比表"></p>
<blockquote>
<p><strong>表2</strong>：在LIBERO-Long上与OpenVLA-OFT的有效性对比。VLA-Adapter在使用未预训练的小骨干时提升明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.09372v2/x7.png" alt="冻结骨干对比"></p>
<blockquote>
<p><strong>表3</strong>：骨干网络冻结时的有效性对比。VLA-Adapter在冻结设置下依然表现强劲。</p>
</blockquote>
<p>其次，VLA-Adapter展现了极高的推理效率。如表4所示，其吞吐率达到219.2 Hz，延迟仅为0.0365秒，显著快于OpenVLA（4.2 Hz）和OpenVLA-OFT（71.4 Hz）等基线方法。</p>
<p>在LIBERO全套任务上的综合性能对比如表5所示。VLA-Adapter仅使用0.5B参数的骨干，取得了平均97.3%的成功率，超越了使用7B骨干的OpenVLA-OFT（97.1%），并大幅领先于同规模（0.5B）的VLA-OS（85.6%）。其升级版VLA-Adapter-Pro更是达到了98.5%的平均成功率。</p>
<p><img src="https://arxiv.org/html/2509.09372v2/x10.png" alt="LIBERO综合结果"></p>
<blockquote>
<p><strong>表5</strong>：在LIBERO基准上的综合性能对比。VLA-Adapter使用极小规模骨干达到了与大型模型相当甚至更优的性能。</p>
</blockquote>
<p>在CALVIN ABC→D泛化任务上，结果如表6所示。VLA-Adapter在成功率和平均完成任务长度上均表现出色，证明了其强大的泛化能力。</p>
<p><img src="https://arxiv.org/html/2509.09372v2/x12.png" alt="CALVIN结果"></p>
<blockquote>
<p><strong>表6</strong>：在CALVIN ABC→D泛化基准上的性能对比。VLA-Adapter展现了优秀的零样本泛化能力。</p>
</blockquote>
<p>消融实验（图4）验证了不同桥接条件的影响，关键结论已在前文“方法详解”部分阐述，即：使用全层ActionQuery特征作为条件通常最优，但中间层原始特征在某些困难任务上表现更佳，因此桥接注意力中保留对原始特征的加权注入是必要的。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）首次系统分析了不同桥接范式对VLA动作生成的影响，并给出了关键设计准则。2）提出了VLA-Adapter范式及其核心——带有可学习加权机制的桥接注意力模块，能够高效地将多模态信息注入动作空间，仅需极小规模骨干且无需机器人数据预训练。3）通过大量实验证明，该方法在性能、效率、训练成本上均具有显著优势，单张消费级GPU上仅需8小时即可训练一个高性能VLA模型，极大降低了部署门槛。</p>
<p>论文提到的局限性在于，桥接注意力模块的引入增加了额外的参数和计算量，可能对效率产生一定影响。这为后续研究提供了启示：如何在保持甚至提升桥接效能的同时，进一步优化注意力机制的计算效率，或探索更轻量的融合方式，是未来值得探索的方向。此外，本文的工作表明，深入理解并设计感知到动作的桥接机制，是构建高效、轻量VLA模型的一个关键且有效的途径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型依赖大规模视觉语言模型和大量机器人数据预训练导致成本高的问题，提出VLA-Adapter新范式。该方法首先系统分析视觉语言条件的有效性，进而设计轻量级策略模块与桥接注意力机制，自动将最优条件注入动作空间。实验表明，该方法仅使用0.5B参数主干、无需机器人数据预训练，在仿真与真实机器人基准上达到了最先进性能，并实现了最快的推理速度。此外，该范式仅需在单张消费级GPU上训练8小时即可获得高性能VLA模型，极大降低了部署门槛。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.09372" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>