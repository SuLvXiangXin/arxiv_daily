<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.13446" target="_blank" rel="noreferrer">2508.13446</a></span>
        <span>作者: Glossop, Catherine, Chen, William, Bhorkar, Arjun, Shah, Dhruv, Levine, Sergey</span>
        <span>日期: 2025/08/19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型为将开放词汇的自然语言指令映射到机器人动作提供了强大的架构。然而，尽管拥有高质量和多样化的数据，这些策略在实际中仍难以遵循细粒度的语言指令。一个关键原因是现有机器人数据集中缺乏语义多样性和语言基础，特别是对于相似观察结果，缺乏细粒度的任务多样性。这导致在给定单一观察时，未来的动作分布通常会坍缩（例如，看到一个抽屉柜时，机器人唯一可能执行的任务是“打开抽屉”）。因此，即使强大的模型也缺乏关注语言指令的动机，遭受“后验坍缩”问题。</p>
<p>本文针对VLA模型在指令跟随上的这一痛点，提出了一种无需额外数据收集、通过反事实数据增强来提升模型对语言指令关注度的新视角。核心思路是利用视觉语言模型为现有机器人数据集中的观察生成反事实的语言指令和对应的动作标签，从而在训练数据中为同一观察创造多种可能的（语言，动作）配对，迫使策略模型必须依赖语言指令来预测正确的动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>CAST方法的整体目标是将一个未标注的数据集 $\mathcal{D}<em>u = { \mathbf{o}</em>{t,i}, \mathbf{a}<em>{t,i} }$ 转化为一个能训练出有效指令跟随策略的标注数据集 $\mathcal{D}<em>l = { \mathbf{o}</em>{t,i}, \ell</em>{t,i}, \mathbf{a}<em>{t,i} }$。其核心创新在于不仅生成描述实际执行轨迹的标签 $\bar{\ell}</em>{t,i}$，还为数据集中的观察生成反事实的语言标签 $\ell^{cf}<em>{t,i}$ 和动作 $\mathbf{a}^{cf}</em>{t,i}$。</p>
<p><img src="https://arxiv.org/html/2508.13446v1/x2.png" alt="方法框架图"></p>
<blockquote>
<p><strong>图2</strong>：CAST方法概览。CAST利用VLM，输入机器人观察 $\mathbf{o}$ 和现有的语言标签 $\bar{\ell}<em>{t,i}$，生成反事实指令 $\ell</em>{t,i}$ 和原子命令 $\ell^{a}<em>{t,i}$。然后通过从原子策略 $\pi</em>{\theta}(\mathbf{a}<em>{t}|\ell</em>{t},\mathbf{o}_{t})$ 中采样，检索对应的反事实动作块。这些合成数据随后用于提升VLA的语言跟随能力。</p>
</blockquote>
<p>方法包含三个核心模块：</p>
<ol>
<li><strong>原子策略获取与标注</strong>：首先，将原始动作离散化为一组原子命令（如“右转”、“左转”、“调整右”、“调整左”、“前进”、“停止”），形成原子数据集 $\mathcal{D}<em>a = { \mathbf{o}</em>{t,i}, \ell^{a}<em>{t,i}, \mathbf{a}</em>{t,i} }$。基于此训练一个原子策略 $\pi_{a}(\mathbf{a}<em>{t,i}|\ell^{a}</em>{t,i},\mathbf{o}_{t,i})$。该策略因标签与动作强相关而易于训练，能可靠地执行短视距的原子命令。</li>
<li><strong>后见重标注</strong>：对于原始数据集中每条轨迹，使用VLM生成描述整个轨迹的多种可能语言指令 $\bar{\ell}_{j,i}$。这个过程包括先用VLM描述轨迹中的物体和结构，再总结为机器人执行指令，并过滤掉与原子动作序列明显不符的指令。</li>
<li><strong>反事实指令与轨迹生成</strong>：这是方法的核心。在原子段落的决策点，利用VLM和已有的轨迹描述 $\bar{\ell}<em>{j,i}$，通过特定的提示词，让VLM生成<strong>反事实的高级指令</strong> $\ell^{cf}</em>{t,i}$ 及其对应的<strong>原子命令</strong> $\ell^{a,cf}<em>{t,i}$。例如，在机器人原本左转的点，VLM可能生成“沿着右侧墙壁导航”的反事实指令和“右转”的原子命令。随后，使用预训练的原子策略 $\pi</em>{a}$，根据反事实原子命令 $\ell^{a,cf}<em>{t,i}$ 和观察 $\mathbf{o}</em>{t,i}$ 采样生成反事实动作 $\mathbf{a}^{cf}<em>{t,i}$。最终，将三元组 $(\mathbf{o}</em>{t,i}, \ell^{cf}<em>{t,i}, \mathbf{a}^{cf}</em>{t,i})$ 加入训练集。</li>
</ol>
<p>本文从信息论角度论证了该方法的有效性。通过为同一观察 $\mathbf{o}$ 生成多样化的原子命令 $\ell^{a}$（提高条件熵 $H(\ell^{a}|\mathbf{o})$），并确保每个高级指令 $\ell$ 能明确对应一个原子命令（降低条件熵 $H(\ell^{a}|\ell, \mathbf{o})$），可以最大化动作 $\mathbf{a}$ 和语言指令 $\ell$ 在给定观察 $\mathbf{o}$ 下的条件互信息 $I(\mathbf{a};\ell|\mathbf{o})$ 的一个下界，从而理论上促进模型对语言的关注。</p>
<p>在具体实现中，策略模型基于30亿参数的PaliGemma VLM进行微调，包含SigLIP视觉编码器和Gemma语言模型。动作被归一化为XY坐标的增量，并预测包含16个动作令牌的动作块。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在3个真实世界环境（拥挤的办公室走廊、厨房、户外公园）中，对27条涵盖不同维度的语言指令进行评估。指令分为三类：<strong>物体导航</strong>（导航到目标物体）、<strong>指代导航</strong>（移动到相对于某物体或结构的位置，如“去到...左边”）和<strong>连续导航</strong>（执行相对于环境中物体或结构的连续行为）。</p>
<p>对比的基线方法包括：</p>
<ol>
<li><strong>标准VLA</strong>：仅使用后见重标注标签（无反事实数据）训练的VLA模型。</li>
<li><strong>$\pi_a$ + 规划</strong>：分层方法，使用VLM作为规划器将复杂指令分解为原子命令，再由原子策略执行。</li>
<li><strong>CoNVOI</strong>：一种零样本方法，将语言指令通过VLM转化为栅格地图上的轨迹，再与激光雷达占据地图结合。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.13446v1/x3.png" alt="总体结果"></p>
<blockquote>
<p><strong>图3</strong>：在VLN任务上的总体结果。策略在3个不同环境的27条语言指令上评估，分为“指代”、“连续”和“物体导航”任务。柱状图显示了各策略的成功率及标准误差。</p>
</blockquote>
<p>关键实验结果：使用CAST训练的<strong>CounterfactualVLA</strong>模型平均成功率达到<strong>53%<strong>，相比仅使用后见标签的</strong>标准VLA</strong>（成功率26%）提升了**27%**。这表明反事实标签显著增强了模型的指令跟随能力。CounterfactualVLA的性能也与先进的零样本方法CoNVOI（46%）和分层方法$\pi_a$+规划（31%）具有竞争力或更优。</p>
<p><img src="https://arxiv.org/html/2508.13446v1/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：CounterfactualVLA在户外公园、办公室走廊和厨房区域成功遵循复杂关系和连续指令的定性示例。</p>
</blockquote>
<p>消融实验与分析：论文通过理论推导和实验对比，验证了反事实生成的核心作用。标准VLA倾向于表现出更通用的导航行为，很大程度上忽略了语言，只在数据集中常见的物体（如“人”、“垃圾桶”）上成功，而CounterfactualVLA则能更好地理解和执行细粒度的空间关系指令。这证实了增加“同一观察下不同指令对应不同动作”的数据多样性是提升语言跟随的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li>提出了<strong>CAST</strong>，一种无需额外数据收集、通过视觉语言模型生成反事实语言和动作标签来增强机器人数据集的新方法。</li>
<li>从信息论角度论证了通过增加原子命令的多样性并确保其与高级指令的对应关系，可以有效提升策略对语言指令的条件互信息，从而缓解后验坍缩。</li>
<li>在真实的视觉语言导航任务上验证了CAST的有效性，使用CAST数据训练的VLA模型在复杂指令跟随上取得了显著提升，性能优于仅使用后见标签的模型，并与先进的零样本和分层方法相当。</li>
</ol>
<p>论文提到的局限性主要在于方法依赖于VLM生成反事实的质量和原子策略的可靠性。此外，当前实验集中于导航领域，在其他机器人任务（如操作）上的泛化性有待验证。</p>
<p>本文的启示在于，为提升模型对指令的敏感性，构建训练数据时不应只追求“发生了什么”的标注，更应主动构建“还可能发生什么”的多样化（指令，动作）配对。这种“反事实数据增强”的思想可以作为一种通用策略，与不同的模型架构结合，用于改善任何存在模态坍缩风险的条件生成模型。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在遵循细粒度用户指令时表现不佳的问题，提出一种利用视觉语言模型为现有机器人数据集生成反事实标签的数据增强方法。该方法通过合成假设指令及对应动作，增加训练数据的语义多样性与语言基础，从而缓解模型对指令关注不足的后验坍塌问题。实验表明，该方法无需额外数据收集，即能显著提升模型在导航任务中的指令跟随能力，成功率提高27%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.13446" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>