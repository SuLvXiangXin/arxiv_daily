<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Humanoid Visual-Tactile-Action Dataset for Contact-Rich Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>A Humanoid Visual-Tactile-Action Dataset for Contact-Rich Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.25725" target="_blank" rel="noreferrer">2510.25725</a></span>
        <span>作者: Kyung-Joong Kim Team</span>
        <span>日期: 2025-10-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人学习领域，接触丰富的操作日益重要。然而，以往的研究数据集多聚焦于刚性物体，且未能充分代表现实操作中多样的压力条件。现有数据集通常使用简单的机器人夹爪和表面级的触觉数据，无法捕捉真实接触交互的空间分布和体积特性，从而限制了其在接触密集型任务中的适用性。大多数先前工作仍处于对现实世界操作尚不实用的阶段。</p>
<p>本文针对这一空白，提出了一个专门为操作可变形的软物体而设计的人形机器人视觉-触觉-动作数据集。核心思路是通过遥操作收集人形机器人在不同压力条件下与软物体交互的多模态数据，并验证高分辨率密集触觉信号对于表征复杂接触动态的重要性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的核心工作是构建并分析一个多模态数据集，而非提出一种新的算法。其整体流程（Pipeline）是数据收集、预处理与分析验证。</p>
<p><img src="https://arxiv.org/html/2510.25725v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：遥操作框架概述，展示了人形机器人控制设置以及实验中使用的软物体和刚性物体。</p>
</blockquote>
<p><strong>1. 数据收集设置</strong>：扩展了Unitree工作流，通过遥操作捕获丰富的多模态信号。具体包括：</p>
<ul>
<li><strong>本体感觉数据</strong>：手臂和手指关节位置。</li>
<li><strong>视觉数据</strong>：头戴式摄像头（848×480分辨率）的自我中心视角，以及位于机器人左侧1米处的Intel RealSense D435提供的第三人称视角。</li>
<li><strong>触觉数据</strong>：使用Inspire Hand灵巧手，每只手上分布有1,062个高分辨率传感器（总计2,124个），覆盖手指和手掌。</li>
<li><strong>外部压力测量</strong>：使用压阻式触觉地毯实时测量外部接触压力并生成压力热图。</li>
</ul>
<p><strong>2. 实验对象与条件</strong>：为捕捉压力变化，使用毛巾和海绵两种软物体（图1b），并与刚性物体（图1c）进行对比。压力条件定义为“强”和“弱”，从而构建了四个任务：Towel Strong、Towel Weak、Sponge Strong、Sponge Weak。操作者根据预定义的压力条件执行动作，触觉地毯热图提供实时反馈以确保压力条件的一致性。</p>
<p><strong>3. 数据集预处理与分析</strong>：三名操作者收集数据，每个任务约77-80个片段（每段20-30秒），总计101.9k个样本。所有原始触觉信号（范围0-4095）被归一化到0-1之间以减少传感器响应差异。分析的重点在于触觉信号。</p>
<p><strong>创新点</strong>：与现有数据集（如表I所示）相比，本文工作的创新性在于首次提供了针对软物体、在受控压力条件下、由人形机器人收集的视觉-触觉-动作数据集，并配备了高分辨率、空间密集的触觉传感。</p>
<p><img src="https://arxiv.org/html/2510.25725v2/x2.png" alt="触觉信号分布对比"></p>
<blockquote>
<p><strong>图2</strong>：灵巧手与刚性物体和可变形软物体交互时捕获的触觉信号分布比较，突出了接触动态的差异。</p>
</blockquote>
<p><strong>4. 密集与稀疏触觉表征对比</strong>：为验证高分辨率密集触觉信号的优势，论文参照[6]中的FSR触觉手套，将密集触觉信号转换为稀疏形式，传感器数量从2,124个减少到42个（约减少98%）。</p>
<p><img src="https://arxiv.org/html/2510.25725v2/x3.png" alt="t-SNE嵌入可视化"></p>
<blockquote>
<p><strong>图3</strong>：密集和稀疏传感配置下触觉信号的t-SNE嵌入，显示了不同接触条件下压力模式的可分离性。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准方法</strong>：采用最先进的模仿学习基线方法——动作分块变换器（ACT）。</li>
<li><strong>输入处理</strong>：为保留密集触觉信号的空间结构，将每个触觉片段的信号转换为图像表征，形成2D触觉地图，并通过CNN提取特征，与视觉输入一同馈入ACT的图像编码器。</li>
<li><strong>对比条件</strong>：训练了两个模型——使用高分辨率密集触觉信号的ACT-Dense和使用低分辨率稀疏触觉信号的ACT-Sparse。</li>
<li><strong>评估指标</strong>：使用平均绝对误差（MAE）评估策略性能，该指标衡量所有时间步和动作维度上预测动作与真实动作之间的平均绝对差异。</li>
<li><strong>数据集划分</strong>：每个任务约80个片段，80%用于训练，20%用于评估。每个模型训练100K步，每20K步评估一次。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.25725v2/x4.png" alt="训练与测试曲线"></p>
<blockquote>
<p><strong>图4</strong>：所有操作任务中，密集和稀疏触觉模型的训练和测试曲线。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>训练动态</strong>：如图4所示，两种模型的训练损失均随着训练进行而稳定下降。对于Towel Weak任务，测试性能表现出不稳定的波动，这可能反映了弱压力条件下触觉信号具有更大的不确定性。在Sponge任务中，强和弱条件之间的差异更为明显，这体现在测试损失上，Sponge Strong和Sponge Weak表现出清晰的性能差距。</li>
<li><strong>密集 vs. 稀疏性能</strong>：总体而言，密集模型和稀疏模型之间的性能差距相对较小。论文将此归因于当前优化过程的局限性。密集信号由于其高维度和高噪声水平，带来了更高的优化难度。</li>
<li><strong>消融分析（数据层面）</strong>：通过将密集信号人为降采样为稀疏信号进行对比实验（图3）。t-SNE可视化表明，密集触觉信号能清晰地将不同压力条件下的样本分离成紧凑的簇，而稀疏信号则无法清晰区分任务差异。这证实了高分辨率密集触觉信号在表征任务特性方面的优势。</li>
<li><strong>真实世界部署</strong>：在真实环境中对训练模型进行了测试（图5）。模型从与数据收集时不同的手部姿态开始测试。观察到需要精确接触控制的任务（如抓取海绵）更具挑战性，需要更丰富的数据覆盖和更长的训练周期。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.25725v2/x5.png" alt="真实世界操作实验"></p>
<blockquote>
<p><strong>图5</strong>：真实世界操作实验示例。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个针对软物体、在多样控制压力条件下收集的人形机器人视觉-触觉-动作数据集。</li>
<li>引入了一种用于高效融合密集触觉信息的神经网络架构（基于2D触觉地图的CNN编码器），并验证了其在接触丰富操作中的效用。</li>
<li>对数据集进行了深入分析，包括可视化揭示了密集触觉信号的关键特性，并论证了高分辨率密集触觉对于捕捉软物体操作复杂性的重要性。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>论文自身提到，当前的优化策略在处理高维、高噪声的密集触觉信号时存在困难，导致密集模型与稀疏模型的性能提升未达预期。有效学习密集触觉信号的方法仍然是一个挑战。</li>
<li>数据集目前仅包含两种软物体和四种压力条件，覆盖的任务和场景范围有限。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>未来工作需要开发更高效的信号处理和优化策略，以充分利用密集触觉信号的复杂性。</li>
<li>应扩展数据集，纳入更多物体类型和接触场景，以增强模型的泛化能力。</li>
<li>该数据集为研究软物体操作、触觉表征学习以及多模态融合策略提供了一个宝贵的基准平台。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人接触丰富操作中数据集不足的问题，提出首个类人机器人视觉-触觉-动作数据集。现有研究多关注刚性物体，缺乏对软物体操作中压力条件多样性的考虑。该数据集通过遥操作收集，使用配备灵巧手的类人机器人，捕捉与两个软物体在不同接触条件下的多模态交互。关键技术包括引入神经网络架构以高效融合密集触觉信息。实验收集了101.9K帧数据，并通过软物体操作任务及模仿学习基线评估，验证了数据集的实用性和触觉传感分辨率的重要性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.25725" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>