<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.09577" target="_blank" rel="noreferrer">2505.09577</a></span>
        <span>作者: Zhang, Chaofan, Hao, Peng, Cao, Xiaoge, Hao, Xiaoshuai, Cui, Shaowei, Wang, Shuo</span>
        <span>日期: 2025/05/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型在机器人操作领域取得了显著进展，但其应用主要局限于视觉主导的抓放等简单任务，缺乏对接触密集型操作的适应性。与此同时，触觉-语言模型（TLA）在触觉感知方面展现出潜力，但其在语言条件动作生成中的应用尚不成熟，且缺乏视觉全局感知能力。现有方法主要存在两个关键局限：一是大多数VLA系统缺乏触觉反馈，限制了其在接触丰富场景（如精密插入）中的应用；二是现有的TLA模型在触觉编码和训练框架上仍有改进空间，且缺少视觉模态导致性能瓶颈。</p>
<p>本文针对接触密集型操作（如轴孔装配）中需要融合多模态感知（视觉全局定位与触觉接触反馈）以应对不确定性的具体痛点，提出了整合视觉、触觉与语言的新视角。本文核心思路是构建一个视觉-触觉-语言-动作（VTLA）模型，通过视觉引导的时间增强令牌（VGTE）优化多模态输入表示，并引入直接偏好优化（DPO）来弥合基于分类的令牌预测损失与连续机器人控制任务之间的差距，从而生成鲁棒的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>VTLA模型的整体框架分为两个阶段：第一阶段基于模拟数据进行监督微调（SFT），第二阶段引入偏好学习进行优化。</p>
<p><img src="https://arxiv.org/html/2505.09577v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：VTLA模型的两阶段训练流程。阶段一：利用模拟数据创建指令数据集，通过视觉引导的时间增强（VGTE）处理多模态输入，使用下一个令牌预测（NTP）损失优化VTLA模型。阶段二：引入直接偏好优化（DPO），通过偏好学习提供类似回归的监督，以提升模型在连续控制任务上的性能。</p>
</blockquote>
<p><strong>数据收集与处理</strong>：在NVIDIA Isaac Gym中构建轴孔装配任务的模拟环境，使用腕部摄像头和夹爪指尖的视觉触觉传感器收集视觉和触觉观测。任务涉及抓取轴、将其定位在孔上方并引入随机3自由度错位，然后尝试插入。收集了5种不同轴孔形状、装配间隙为0.6至2.0毫米的数据，共28,000个样本。每个样本包含左/右触觉图像序列、一张视觉图像和一个动作标签。为促进零样本模拟到现实（Sim2Real）迁移，在数据集生成过程中采用了领域随机化技术。数据被组织成指令格式，包含标记图像起止的特殊令牌、描述任务的文本指令以及作为真值的动作标签。</p>
<p><strong>核心模块一：视觉引导的时间增强令牌（VGTE）</strong>。该模块旨在解决视觉语言模型（VLM）在时序理解上的局限性，并优化多模态输入。具体包含两方面设计：</p>
<ol>
<li><strong>视觉引导</strong>：基于视觉在视觉-触觉任务早期阶段至关重要的先验，将视觉输入置于触觉输入之后，使其更接近动作预测，以强调初始操作阶段的视觉信息重要性，缓解语言模型的近因偏差。</li>
<li><strong>时间增强</strong>：针对VLM不擅长细粒度时序依赖的问题，将触觉观测编码成类图像表示，并使用视觉Transformer（ViT）提取具有时间感知的触觉令牌，从而利用VLM在多模态理解上的优势。</li>
</ol>
<p><strong>监督微调（SFT）</strong>：遵循VLA范式，将策略学习制定为下一个令牌预测（NTP）任务。具体而言，触觉和视觉观测首先通过一个预训练的视觉编码器和模态适配器生成触觉和视觉令牌。文本指令被分词为文本令牌。这些令牌随后被输入预训练的大语言模型（LLM）中以预测机器人动作。使用NTP损失对整个模型进行微调，同时冻结视觉编码器和模态适配器的参数，仅调整语言模型。</p>
<p><strong>核心模块二：偏好学习（DPO）</strong>。指令微调后的VTLA模型性能有限，原因在于机器人连续控制任务（回归问题）与基于分类的NTP损失之间存在不匹配。为解决此问题，本文将VTLA预测任务重新表述为多标签问题，并通过直接偏好优化（DPO）引入类似回归的监督。具体流程如<strong>图3</strong>阶段二所示：首先使用微调后的VTLA模型从相同训练样本生成多样化的动作预测；根据预测与真值的接近程度创建偏好数据集，将更接近真值的动作标记为“被选择”（chosen），其他为“被拒绝”（rejected）；最后，使用DPO损失在此数据集上优化模型。DPO损失函数鼓励模型对“被选择”响应的似然高于“被拒绝”响应，从而与基于偏好的监督对齐。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟数据集和多种插入任务上评估VTLA。使用的Baseline方法包括：扩散策略（DP）、仅使用视觉的VLA模型、仅使用触觉的TLA模型。还对比了不使用DPO的VTLA以及使用不同数量（1k, 2k）偏好数据训练的VTLA变体。评估指标包括：数据集上的目标收敛率（GCR）和各方向L1误差；模拟插入任务上的成功率和平均尝试步数。此外，在真实世界搭建了与模拟1:1比例的实验平台（UR3机械臂，Robotiq夹爪，腕部RealSense D405摄像头，指尖GelStereo 2.0触觉传感器），对仅在模拟数据上训练的模型进行Sim2Real验证。</p>
<p><strong>与基线方法对比（模拟）</strong>：<br><img src="https://arxiv.org/html/2505.09577v1/x5.png" alt="数据集对比结果"></p>
<blockquote>
<p><strong>表1</strong>：各方法在模拟数据集上的性能对比（ID和OOD）。VTLA在ID和OOD数据上的GCR均最高，L1误差最低，表明其优越的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09577v1/x6.png" alt="不同间隙插入结果"></p>
<blockquote>
<p><strong>表2</strong>：各方法在模拟环境中对不同间隙（2.0, 1.6, 1.0, 0.6 mm）方形轴插入任务的成功率（Suc%）和平均步数（Step）。VTLA在几乎所有设置下都取得了最高的成功率或最少的步数。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09577v1/x7.png" alt="不同形状插入结果"></p>
<blockquote>
<p><strong>表3</strong>：各方法在模拟环境中对0.6 mm间隙不同形状轴插入任务的性能。VTLA在多数形状上表现最佳，尤其在五边形轴上成功率和效率均优于基线。</p>
</blockquote>
<p>关键结果总结：1）基于LLM的模型（VLA, TLA, VTLA）性能显著优于传统模仿学习方法DP（如0.6mm间隙方形轴任务，成功率80+% vs 22%）。2）视觉输入至关重要，VLA和VTLA显著优于仅触觉的TLA（ID GCR: 46+% vs 15.3%）。3）VTLA融合触觉后，性能超越仅视觉的VLA（0.6mm间隙成功率90% vs 80%），证明了时间感知触觉令牌的有效性。4）在泛化性能上，VTLA在OOD数据的所有维度上均优于TLA和VLA（OOD-GCR: 31.2% vs 29.5%/14.4%）。</p>
<p><strong>消融实验（DPO）</strong>：<br><img src="https://arxiv.org/html/2505.09577v1/x8.png" alt="DPO消融结果"></p>
<blockquote>
<p><strong>表4</strong>：DPO的消融研究。引入DPO（即使仅使用1k偏好数据）显著提升了模型在OOD数据上的性能（GCR提升约16%，L1误差降低约10%），表明DPO通过缓解SFT阶段对真值动作的过拟合，增强了泛化能力。但增加偏好数据量（至2k）并未带来进一步增益。</p>
</blockquote>
<p><strong>真实世界实验（Sim2Real）</strong>：<br><img src="https://arxiv.org/html/2505.09577v1/x4.png" alt="真实世界插入对比"></p>
<blockquote>
<p><strong>图4</strong>：VTLA与基线方法（VLA, TLA）在真实世界0.6 mm间隙插入任务中的定性对比快照。在相同初始状态下，VTLA（案例1）能以更少步数完成插入，而TLA（案例2）尝试多次仍失败，凸显了视觉-触觉融合的优势。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09577v1/x1.png" alt="真实世界不同间隙结果"></p>
<blockquote>
<p><strong>表5</strong>：VTLA在真实世界中对不同间隙方形轴插入的性能。即使在最具挑战性的0.6 mm间隙下，成功率仍达95%，展示了强大的Sim2Real能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09577v1/x2.png" alt="真实世界不同形状结果"></p>
<blockquote>
<p><strong>表6</strong>：VTLA在真实世界中对0.6 mm间隙不同形状轴插入的性能。在OOD形状（如五边形、圆形）上取得了100%的成功率，泛化性能优异。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09577v1/x1.png" alt="真实世界基线对比"></p>
<blockquote>
<p><strong>表7</strong>：VTLA与VLA、TLA在真实世界0.6 mm间隙特定形状插入任务上的对比。VTLA取得了与VLA相当或更高的成功率，且插入效率更高（五边形：1.85步 vs 2.3步）；而仅触觉的TLA面临较大的Sim2Real差距，成功率仅30-40%。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了首个整合视觉、触觉与语言进行接触密集型操作的VTLA框架，实现了感知与语言-动作生成的统一。2）设计了视觉引导的时间增强令牌（VGTE），有效提升了VLM在视觉-触觉操作中的时序推理能力。3）创新性地将直接偏好优化（DPO）引入多模态动作生成，通过偏好学习提供了类似回归的监督，显著提升了模型的泛化性能。</p>
<p>论文自身提到的局限性包括：1）<strong>触觉-语言对齐不足</strong>：使用现成的视觉编码器表示触觉输入可能导致触觉模态独特特征的丢失，未来需要专门的触觉-语言对齐研究。2）<strong>多模态深度融合有限</strong>：对于视觉和触觉信号及特征的深度融合，在LLM框架下的探索仍然不足。</p>
<p>对后续研究的启示：VTLA的成功证明了在大型基础模型中融合触觉模态对于复杂操作任务的巨大价值。未来的工作可以沿着两个主要方向深入：一是开发更精细的、针对触觉语义（如接触状态）的模态对齐方法；二是探索更先进的视觉-触觉特征融合机制，以充分发挥多模态协同的潜力，应对更具挑战性的机器人操作场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对接触密集型机器人操作任务中视觉信息不足的问题，提出VTLA模型。该框架通过跨模态语言对齐，整合视觉、触觉与语言指令，并引入直接偏好优化（DPO）方法，以回归式监督提升策略生成能力。实验表明，VTLA在未见过的peg-in-hole任务上成功率超过90%，优于传统模仿学习及多模态基线，并展现出优异的Sim2Real迁移性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.09577" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>