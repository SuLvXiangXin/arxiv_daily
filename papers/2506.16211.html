<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.16211" target="_blank" rel="noreferrer">2506.16211</a></span>
        <span>作者: Siyuan Huang Team</span>
        <span>日期: 2025-06-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操纵学习，特别是在演示数据有限的情况下，仍然是一个重大挑战。当前少样本操纵方法主要依赖模拟增强数据或预建的抓取、姿态估计等模块，这些方法面临仿真到现实的差距且缺乏可扩展性。同时，大规模模仿预训练的通用视觉-语言-动作模型展现出潜力，但在数据稀缺场景下，如何将这些通用策略高效适应到特定任务仍未得到充分探索。另一方面，物体中心表示通过关注相关物体属性，能提高数据效率，但现有方法仍需数百条演示，主要原因是缺乏来自VLA模型的动作先验知识。</p>
<p>本文旨在解决上述痛点，提出了一种结合预训练VLA模型与物体中心表示的新视角，以实现高效的少样本学习。本文核心思路是：通过一个受ControlNet启发的、采用零初始化投影层的架构，将物体中心表示作为额外条件引入预训练的VLA模型，从而在保护模型先验知识的同时，利用少量演示高效地进行任务特定适应。</p>
<h2 id="方法详解">方法详解</h2>
<p>ControlVLA的整体目标是在给定预训练的通用策略 π_g 和少量专家演示的情况下，高效地将其适应为任务特定的专家策略 π_e。其pipeline包含三个主要阶段：1) 在大规模多任务数据集上预训练通用VLA模型；2) 从演示中提取物体中心表示作为额外条件；3) 采用ControlNet风格微调，将物体中心表示逐步整合到预训练模型中。</p>
<p><img src="https://arxiv.org/html/2506.16211v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：ControlVLA方法概览。该方法利用ControlNet风格的微调策略，将物体中心表示与预训练的VLA模型集成。零初始化的权重和偏置保留了预训练策略的丰富先验知识，同时使其逐步融入物体中心表示。</p>
</blockquote>
<p><strong>核心模块一：VLA模型预训练</strong>。使用大规模、多任务的操纵数据集预训练一个通用策略 π_g。该模型采用扩散Transformer架构，对条件动作分布 p(A_t | O_t) 进行建模。观测 O_t 包含单视角RGB图像 I_t、语言指令 ℓ_t 和机器人本体感知状态 q_t。图像和语言指令通过预训练编码器进行标记化，并投影到共享嵌入空间；本体感知状态则通过多层感知机嵌入。训练时使用条件去噪损失进行监督。推理时，从纯高斯噪声开始，通过加速的DDIM采样迭代去噪，生成动作序列，以实现实时控制。</p>
<p><strong>核心模块二：物体中心表示构建</strong>。目标是构建物体中心表示 Z ∈ 𝒵，作为专家策略 π_e 的额外条件，使其能明确识别任务关键概念。此过程分为两步：首先，分割与跟踪任务相关物体。从演示和语言指令中提取帧作为提示，利用GroundingDINO和SAM2对任务相关物体进行一致的分割与跟踪，获得精细的实例掩码 {M^i}。其次，学习提取物体中心特征。对于每个物体掩码 M^i，学习一个特征提取器 f_φ 来生成物体中心表示 z^i。该表示由位置特征 z_pos^i 和几何特征 z_geo^i 拼接而成：位置特征通过正弦位置编码对掩码在图像上的平均坐标进行编码；几何特征则通过一个从头训练的CNN，处理每个物体的掩码来获得空间特征图。最终，所有物体的表示构成集合 Z = {z^i}。</p>
<p><strong>核心模块三：ControlNet风格微调</strong>。给定少量任务特定数据集 D_e，目标是高效地将 π_g 微调为能利用物体中心表示 Z 的专家策略 π_e。预训练模型使用交叉注意力机制建模动作 A 对观测 O 的条件依赖。为了引入物体中心条件 Z，作者扩展了交叉注意力机制，引入了一个双注意力结构：原始注意力项（基于观测 O）加上新增的注意力项（基于物体表示 Z）。新增的注意力项具有独立的键值投影层 (K_z, V_z) = W_z Z + B_z。受ControlNet启发，<strong>关键创新在于将新增的KV投影层的权重 W_z 和偏置 B_z 初始化为零</strong>。这使得在微调初期，新增注意力项的输出为零，双注意力退化为原始交叉注意力，从而完全保留了预训练策略的行为。随着微调进行，这些零初始化的投影层逐步学习如何整合物体中心信息，实现稳定、高效的任务适应。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>首次将ControlNet风格的微调思想应用于机器人操纵领域</strong>，提出了一种高效整合额外条件（物体中心表示）的架构。2) 通过<strong>零初始化新增KV投影层</strong>的关键设计，确保了微调初期预训练知识的完整性，避免了有害噪声的引入，实现了从通用先验到任务特定技能的平稳过渡。3) 提出了一个<strong>统一的框架</strong>，将大规模VLA模型预训练的高泛化能力与物体中心表示的数据高效性相结合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在8个多样化的真实世界任务上进行，仅使用10-20条演示。任务涵盖刚性、软体、精密抓放、关节物体操作、可变形衣物折叠、倾倒方块等短视界任务，以及两个需要顺序决策的长视界任务（整理多个物体、替换抽屉内物体）。实验平台涉及真实机器人。</p>
<p>对比的基线方法包括：1) <strong>Octo</strong>：一个大型预训练VLA模型，直接在下游任务上进行全参数微调。2) <strong>π0</strong>：另一个大型预训练VLA模型，同样进行全参数微调。3) **Octo (LoRA)**：对Octo模型使用LoRA进行参数高效微调。4) <strong>Octo w/o Pre-train</strong>：不使用预训练权重，从头训练Octo架构。5) <strong>ControlVLA w/o Object-centric</strong>：不使用物体中心表示的ControlVLA变体。6) <strong>ControlVLA w/o ControlNet</strong>：不使用ControlNet风格微调（即不使用零初始化投影层）的变体。</p>
<p><img src="https://arxiv.org/html/2506.16211v1/extracted/6554772/figures/maincomparison.png" alt="主实验结果对比"></p>
<blockquote>
<p><strong>图4</strong>：主要任务成功率对比。ControlVLA在仅使用10-20条演示的情况下，在6个短视界任务上的平均成功率达到**76.7%**，显著优于所有基线方法（其中表现最好的基线Octo成功率为20.8%）。ControlVLA在所有单个任务上也均优于基线。</p>
</blockquote>
<p>关键实验结果总结如下：</p>
<ol>
<li><strong>少样本性能</strong>：在6个短视界任务上，ControlVLA仅用10-20条演示，取得了<strong>76.7%</strong> 的平均成功率，而表现最好的基线（Octo）仅获得<strong>20.8%</strong> 的成功率。</li>
<li><strong>长视界任务</strong>：在两个长视界任务上，ControlVLA也显著优于基线，取得了约3倍的成功率提升。</li>
<li><strong>数据效率</strong>：随着演示数据量增加，ControlVLA能迅速收敛至高成功率。仅需20条演示，其性能已接近使用100条演示的基线方法水平。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.16211v1/extracted/6554772/figures/data_scaling.png" alt="数据缩放曲线"></p>
<blockquote>
<p><strong>图5</strong>：数据缩放实验结果。ControlVLA仅用20条演示即可达到高成功率，而基线方法（Octo）需要超过100条演示才能达到可比性能，凸显了ControlVLA极高的数据效率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.16211v1/extracted/6554772/figures/unseen_domain.png" alt="未见域泛化结果"></p>
<blockquote>
<p><strong>图6</strong>：在未见过的物体和背景上的泛化性能。ControlVLA展示了良好的泛化能力，在物体实例和背景环境变化的情况下，性能下降幅度相对较小，优于基线方法。</p>
</blockquote>
<p>消融实验证实了三个关键组件的必要性：</p>
<ol>
<li><strong>VLA模型预训练（技能先验）</strong>：移除预训练（从头训练）导致性能大幅下降（从76.7%降至18.3%）。</li>
<li><strong>物体中心表示（高效任务定位与学习）</strong>：移除物体中心表示后，性能降至38.3%。</li>
<li><strong>ControlNet风格条件注入（稳定适应）</strong>：不使用零初始化的ControlNet风格微调，性能降至45.0%。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了ControlVLA框架</strong>：首次将ControlNet风格的高效条件注入机制引入机器人操纵领域，创造性地通过零初始化投影层，将物体中心表示与预训练的VLA模型相结合。</li>
<li><strong>实现了高效的少样本适应</strong>：仅需10-20条演示，即可在多种真实世界操纵任务上达到76.7%的高成功率，显著提升了预训练大模型在数据稀缺下游任务上的适应效率。</li>
<li><strong>验证了框架的泛化性与可扩展性</strong>：实验证明了该方法对长视界任务的扩展能力，以及对未见过的物体和背景的鲁棒性。</li>
</ol>
<p>论文自身提到的局限性包括：物体中心表示的构建依赖于GroundingDINO和SAM2等分割与跟踪模型，其性能可能影响最终策略的表现；此外，方法主要针对单臂、桌面型场景的操纵任务。</p>
<p>本文对后续研究的启示在于：1) <strong>零初始化微调策略</strong> 为整合预训练大模型与各种形式的领域先验知识（不限于物体中心表示）提供了稳定、高效的通用范式，可推广至其他机器人学习场景。2) 展示了<strong>结合大规模数据驱动先验与结构化表征</strong>在提升数据效率方面的巨大潜力，鼓励未来研究探索更多样化的条件表示与融合方式。3) 为<strong>快速部署适应新任务的机器人技能</strong>提供了一个切实可行的技术路径，推动了机器人向更通用、更易适配的方向发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中少样本（few-shot）适应问题，提出ControlVLA框架。核心挑战在于如何让预训练的视觉-语言-动作（VLA）模型仅用极少演示就能适应以物体为中心的新任务。方法关键是通过ControlNet式架构，零初始化投影层，在不覆盖预训练知识的前提下引入物体中心表示进行高效微调。实验表明，在6个真实任务中仅需10-20个演示即达到76.7%成功率，显著优于需超100演示的传统方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.16211" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>