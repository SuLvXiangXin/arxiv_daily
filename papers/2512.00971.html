<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.00971" target="_blank" rel="noreferrer">2512.00971</a></span>
        <span>作者: Lin, Yunfeng, Liu, Minghuan, Xue, Yufei, Zhou, Ming, Yu, Yong, Pang, Jiangmiao, Zhang, Weinan</span>
        <span>日期: 2025/11/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，深度强化学习（DRL）方法已被广泛用于训练人形机器人的运动控制器，并通过仿真到真实的迁移技术实现落地。然而，主流方法通常针对特定的机器人形态进行定制化训练，导致控制器与特定硬件紧密耦合。即使关节布局、质量分布或肢体尺寸的微小变化也可能导致性能下降，需要耗费大量资源从头开始重新训练。随着人形机器人平台的多样化发展，亟需能够以最小代价适应新形态的通用控制器。</p>
<p>本文针对上述控制器泛化性差的痛点，提出了一种跨人形机器人运动预训练的新视角。其核心思路是：通过设计统一的控制语义接口，并结合多样化的机器人形态训练集与扩展的域随机化，学习一个通用的人形机器人基础策略，从而实现对新形态机器人的零样本或少样本快速迁移。</p>
<h2 id="方法详解">方法详解</h2>
<p>H-Zero的整体目标是通过在多种人形机器人形态上进行预训练，得到一个通用基础策略，该策略能够作为先验知识，快速适应到未见过的机器人上。其流程分为预训练和迁移部署两个阶段。</p>
<p><img src="https://arxiv.org/html/2512.00971v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：方法整体概览。a) 预训练阶段：通过在多样化的机器人具身集合上进行学习，利用统一控制、多机器人仿真和具身感知训练策略来训练一个基础策略。b) 部署阶段：预训练策略支持对新型机器人进行少样本适应。</p>
</blockquote>
<p><strong>核心模块一：统一控制语义</strong><br>为了克服不同机器人关节配置和控制接口不兼容的问题，H-Zero定义了一个与硬件无关的统一关节状态空间，包含关键旋转关节（如头、肩、膝、踝）的当前位置、速度和目标位置。每个机器人根据其关节的运动学角色，将其物理关节映射到这个统一空间。映射通过一个双向变换矩阵 <strong>M</strong> 实现：<code>q_env = M · q_phy</code> 和 <code>q_phy = M^T · q_env</code>。此外，还引入了运动学校准：<code>q_env = M · (s ⊙ (q_phy - b))</code>，其中 <strong>s</strong> 调整关节方向，<strong>b</strong> 将中立位置偏移到标准直立姿态，确保不同形态下产生等效的物理运动。</p>
<p><strong>核心模块二：具身描述符</strong><br>为了使策略能够感知并适应不同机器人的物理属性，H-Zero引入了具身描述符。这是一组紧凑的特征向量，用于编码机器人关键物理属性，涵盖运动学（关节限位、旋转轴）、拓扑（关节层次结构）、几何（连杆长度、形状）和动力学（质量、惯性、刚度、阻尼）等多个领域。这些特征根据统一空间进行排列，以确保语义一致性。描述符可以作为特权信息仅提供给价值函数（非对称演员-评论家架构），也可以直接作为策略的观察输入。</p>
<p><strong>核心模块三：具身训练集与扩展域随机化</strong><br>预训练使用一组多样化的人形机器人模型（如表I所列）来生成仿真轨迹。与以往仅对摩擦、质量等系统参数进行扰动的域随机化不同，H-Zero将刚体动力学和关节动力学的扰动范围扩大了一倍，以更好地近似不同机器人形态的动力学特性。结构多样性（如几何和拓扑）则通过混合现有的真实机器人模型引入，这比从零开始程序化生成随机形态需要更少的专家调参。</p>
<p><strong>核心模块四：具身感知学习策略</strong><br>在多机器人训练中，由于各形态动力学和稳定性不同，学习更具挑战。H-Zero采用了两种具身感知的训练策略：1）为每种机器人类型分配独立的探索采样方差σ，并根据各形态的梯度进行自适应更新，学习慢的机器人获得更高方差以促进探索；2）根据各形态当前的平均episode回报动态重新加权损失函数（公式5），使表现不佳的形态在训练中获得更多梯度关注，从而促进所有形态的平衡学习进展。</p>
<p><strong>创新点总结</strong><br>与现有方法相比，H-Zero的创新主要体现在：1）提出了一个标准化的统一控制语义接口，解耦了策略与具体硬件；2）通过混合真实机器人模型和扩展域随机化，构建了更有效的跨具身训练分布；3）引入了具身描述符和自适应的多形态训练策略，提升了基础策略的泛化能力和学习效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在Isaac Gym物理仿真器中进行，使用单张NVIDIA RTX 4090 GPU。评估涉及14款不同的仿真人形机器人及四足机器人（见表I）。对比基线包括从头训练的专有策略。评估指标包括：归一化episode长度（执行步数/上限，衡量稳定性）、以及跟踪指令速度（前向、横向、偏航）的误差。</p>
<p><img src="https://arxiv.org/html/2512.00971v1/figs/traj_j8j02nl.png" alt="轨迹可视化"></p>
<blockquote>
<p><strong>图3</strong>：t-SNE可视化不同域随机化（DR）下的轨迹。左：单机器人训练的标准DR。右：本文提出的扩展DR。扩展DR下，未见机器人的轨迹（无随机化）与拓宽后的训练分布重叠，表明迁移性更好。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.00971v1/figs/traj_j7j02nl.png" alt="参数可视化"></p>
<blockquote>
<p><strong>图4</strong>：标准（虚线）和四倍（实线）DR范围下，具身参数的t-SNE凸包。即使在强随机化下，不同机器人的参数仍保持清晰边界。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>预训练性能与消融实验（表II）</strong>：<ul>
<li><strong>训练集多样性</strong>：使用更多样化的形态（如H1, N1, G1, A1）进行预训练，在未见机器人上的平均归一化episode长度达到0.81，显著优于仅用少数形态（如仅H1的0.20）。</li>
<li><strong>域随机化范围</strong>：扩展DR范围（“Quadrupled range”）比单机器人范围（“Single-robot range”）在多数未见机器人上表现更好，平均性能为0.47 vs 0.53（数值越低表示跟踪误差越小？注：此处表II中数值为归一化episode长度，越高越好，上下文需对照）。图3和图4可视化了扩展DR如何使训练分布覆盖未见机器人。</li>
<li><strong>具身描述符</strong>：将描述符作为特权信息提供给评论家效果最好。直接作为策略观察会损害性能（平均0.41），可能是因为增加了策略学习的复杂度。</li>
<li><strong>动作空间大小</strong>：仅控制腿部（12维）比控制全身（32维）在跨具身泛化上表现更优（平均0.56 vs 0.46），说明上身关节的奖励耦合较弱。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00971v1/figs/embd_j8j01nl.png" alt="训练策略对比"></p>
<blockquote>
<p><strong>图5</strong>：不同训练策略下，跨具身训练的平均episode长度曲线。本文的动态损失调度和自适应探索策略带来了更平衡和更快的学习进展。</p>
</blockquote>
<ol start="2">
<li><strong>少样本迁移效率（表III）</strong>：<ul>
<li>预训练策略（P）仅需数百到数千epoch的微调，即可达到与从头训练（S）数万epoch相当甚至更优的性能。例如，对Adam机器人，预训练后微调1k epoch的回报为49，接近从头训练50k epoch的52；速度跟踪误差也迅速降低。</li>
<li>图6显示，在迁移初期使用较低的动作采样方差，可以保护预训练策略已有的合理行为，从而加速适应过程。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00971v1/x3.png" alt="少样本迁移奖励曲线"></p>
<blockquote>
<p><strong>图6</strong>：PND Adam机器人在不同初始动作采样方差下的少样本迁移奖励曲线。低初始方差能更快适应。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界部署（图7）</strong>：<ul>
<li>预训练策略在真实机器人上表现出一定的零样本迁移能力，但存在抖动、速度漂移和不稳定问题。</li>
<li>经过少量迭代的微调后，策略在所有测试的真实机器人上都实现了更稳定、响应更灵敏的运动控制。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00971v1/x4.png" alt="真实世界部署快照"></p>
<blockquote>
<p><strong>图7</strong>：学习策略在真实世界部署的快照。列对应目标机器人，行对应训练方式：从头训练（顶行）、零样本迁移（中行）、少样本适应（底行）。少样本微调后控制更稳定。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个通过状态变换来标准化策略输入/输出的统一控制接口，实现了跨多样人形机器人的策略复用。</li>
<li>设计了一个结合物理随机化和多种训练策略的跨具身训练环境，能够从一组现有机器人中学习通用的运动策略。</li>
<li>实证表明，跨具身运动预训练能够实现对新机器人的零样本和少样本适应，显著减少了对大量重新训练的需求。</li>
</ol>
<p><strong>局限性</strong>：论文提到，将动作空间扩展到包含腰部及以上控制时，由于维度增加和上身关节的奖励耦合较弱，会对跨具身泛化性能产生负面影响。这表明需要更有针对性的目标设计和正则化。</p>
<p><strong>后续启示</strong>：本文验证了基于真实机器人模型构建训练集的有效性。未来工作可以探索基于课程的具身选择和随机化调度，以进一步提升学习效率，并可能攻克包含上身运动的全身控制这一更具挑战性的任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人控制器因形态差异而需重复训练的问题，提出H-Zero跨人形运动预训练框架。其核心方法是通过在策略输入输出端引入变换层，统一不同机器人的控制语义，并结合随机化物理参数与多样化观察数据，学习一个通用基础策略。实验表明，该预训练策略在仿真未见机器人上能保持最多81%的完整运动时长，并能在30分钟微调内实现向新机器人的快速少量样本迁移。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.00971" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>