<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.14648" target="_blank" rel="noreferrer">2506.14648</a></span>
        <span>作者: Shuo Wang Team</span>
        <span>日期: 2025-06-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>基于偏好的强化学习（PbRL）通过人类对行为片段对的偏好来学习奖励模型，从而避免复杂的奖励工程。然而，现有方法仍受限于反馈效率和样本效率低下两大问题。在查询选择方面，主流方法包括基于不确定性的选择（如分歧度）、基于熵的选择（如最大化状态熵）以及策略对齐查询选择等，旨在选取信息量大的查询以高效学习奖励或策略。但这些方法仍可能选出对任务学习帮助不大或难以让人工进行置信比较的无意义片段对（例如机器人无明显运动的片段）。在策略学习方面，现有工作包括无监督预训练、双层优化以及基于奖励模型不确定性的探索等，但如何使探索过程与人类偏好直接相关，从而持续产生有价值的样本，尚未得到充分关注。</p>
<p>本文针对上述两个关键痛点，提出了一种新颖的高效查询选择与偏好引导探索方法SENIOR。其核心思路是双重的：1）设计基于运动区分的查询选择方案（MDS），通过分析片段内的状态密度和运动方向，选择运动明显且方向不同的、易于人类比较的片段对，以获取高质量偏好标签；2）提出偏好引导探索机制（PGE），利用已获得的偏好信息构造内在奖励，鼓励智能体探索那些人类偏好度高但访问次数少的状态，从而为查询选择提供更有价值的候选片段。</p>
<h2 id="方法详解">方法详解</h2>
<p>SENIOR方法包含两个核心模块：运动区分选择（MDS）和偏好引导探索（PGE），二者协同工作，共同提升PbRL的反馈效率和探索效率。</p>
<p><img src="https://arxiv.org/html/2506.14648v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：SENIOR方法整体框架。左侧的偏好引导探索（PGE）模块通过混合经验更新策略，为访问少且受人类偏好的状态分配高任务奖励，以鼓励高效探索，从而为查询选择提供更有价值的任务相关片段。右侧的运动区分选择（MDS）模块选择易于比较且有意义（具有明显运动区分）的片段对，以获取高质量标签促进奖励学习，进而为PGE的探索提供准确的奖励指导。在训练过程中，MDS和PGE相互作用、互为补充。</p>
</blockquote>
<p><strong>1. 运动区分选择（MDS）</strong><br>该模块旨在从经验回放缓冲区 (\mathcal{B}) 中选择易于人类比较且富含任务信息的片段对 ((\sigma^0, \sigma^1))。其过程分为两步，对应两个评价指标：</p>
<ul>
<li><strong>运动分数（m）</strong>：用于筛选出包含丰富运动信息的片段。首先，利用核密度估计（KDE）计算每个片段内状态（具体为机械臂末端执行器位置 (\mathbf{p}<em>t)）的密度 (\hat{f}</em>{\mathcal{S}}(\mathbf{p}_t))（公式4）。一个片段的密度越低，意味着机器人在其中移动范围越大，运动信息越丰富。对于一个片段对，其运动分数 (m) 定义为两个片段密度总和的倒数（公式3），(m) 值越高，表明该片段对整体运动越明显。</li>
<li><strong>区分分数（d）</strong>：用于在运动明显的片段对中，进一步挑选出运动方向差异大的，以便于人类区分和标注。对每个片段应用主成分分析（PCA），提取对应于最大特征值的特征向量 (\mathbf{v})，作为该片段的“主运动方向”。两个片段的区分分数 (d) 定义为它们主方向向量的余弦相似度（公式5）。(d) 值越低，表明两个片段的运动方向差异越大。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.14648v1/x2.png" alt="MDS选择示例"></p>
<blockquote>
<p><strong>图2</strong>：机器人抓苹果任务中MDS的选择倾向示意。MDS倾向于选择运动信息丰富且易于比较的轨迹（a），而非运动信息少的高密度轨迹（b），或运动方向相似难以比较的轨迹（c）。</p>
</blockquote>
<p>MDS的具体操作流程是：首先从 (\mathcal{B}) 中随机采样 (p) 个片段对；然后保留其中运动分数 (m) 最高的 (q) 个片段对；最后从这 (q) 个对中选出区分分数 (d) 最低的片段对作为本次查询的候选。这样选出的片段对既富含运动信息，又具有明显的方向差异，便于人类提供高质量偏好标签。</p>
<p><strong>2. 偏好引导探索（PGE）</strong><br>该模块旨在利用已获得的偏好信息，引导智能体进行更有价值的探索，以产生对策略学习和后续查询选择都有益的新样本。其核心是构造一个与偏好相关的内在奖励 (r_{int})。</p>
<ul>
<li><strong>数据与密度估计</strong>：方法维护一个好奇心缓冲区 (\mathcal{B}<em>{cur})，并周期性地从主回放缓冲区 (\mathcal{B}) 中采样数据 (\mathcal{E}) 加入其中。同时，从已标注的偏好数据集 (\mathcal{D}) 中采样数据代表人类偏好分布。分别使用KDE估计当前探索数据的密度 (\hat{f}</em>{\mathcal{E}}) 和人类偏好数据的密度 (\hat{f}_{\mathcal{P}})。</li>
<li><strong>内在奖励计算</strong>：对于状态 (\mathbf{s}_i)（对应末端位置 (\mathbf{p}_i)），计算函数 (g(\mathbf{p}<em>i) = \hat{f}</em>{\mathcal{P}}(\mathbf{p}<em>i) / \hat{f}</em>{\mathcal{E}}(\mathbf{p}<em>i))（公式7）。该比值反映了该位置受人类偏好的程度与其被探索的频繁程度之间的相对关系。比值越高，意味着该位置更受偏好但访问较少。将 (g(\mathbf{p}<em>i)) 在 (\mathcal{B}</em>{cur}) 中归一化，即得到内在奖励 (r</em>{int}(\mathbf{p}_i))（公式6）。</li>
<li><strong>总奖励与策略更新</strong>：智能体每一步获得的总任务奖励 (r_{cur}) 由学习到的外部奖励 (\hat{r}<em>{\psi}) 和内在奖励 (r</em>{int}) 加权求和得到（公式8），其中权重 (\beta_t) 随时间衰减。策略使用SAC算法，并采用混合经验回放机制（同时从 (\mathcal{B}) 和 (\mathcal{B}_{cur}) 中采样）进行更新，确保策略能同时从外部奖励和内在探索奖励中学习。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，SENIOR的创新性体现在：1）在查询选择中，首次明确引入了对片段运动信息和方向相似性的定量评估，使选择标准更符合人类标注的认知特点；2）在探索机制中，创造性地将人类偏好密度与智能体探索密度相结合，形成了一种直接由偏好引导的内在奖励，使探索目标与任务目标（人类偏好）紧密对齐。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在6个复杂的模拟机器人操作任务（MetaWorld的<code>pick-place</code>, <code>door-open</code>, <code>drawer-open</code>, <code>hammer</code>, <code>assembly</code>, <code>button-press</code>）和4个真实世界任务（开门、关门、开箱、关箱）上进行。对比的基线方法包括：PEBBLE、MRN、QPA、RePAINT和SPRING。评估指标主要是成功率随训练步数（样本效率）和随偏好查询次数（反馈效率）的变化。</p>
<p><img src="https://arxiv.org/html/2506.14648v1/x3.png" alt="模拟任务性能对比"></p>
<blockquote>
<p><strong>图3</strong>：在六个模拟机器人操作任务上，SENIOR与基线方法在成功率随环境交互步数变化曲线上的对比。SENIOR在大多数任务上表现出更快的策略收敛速度（样本效率更高）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x4.png" alt="反馈效率对比"></p>
<blockquote>
<p><strong>图4</strong>：在六个模拟任务上，SENIOR与基线方法在成功率随偏好查询次数变化曲线上的对比。SENIOR仅需更少的人类偏好查询即可达到相同或更高的成功率，证明了其卓越的反馈效率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x5.png" alt="最终性能与提升"></p>
<blockquote>
<p><strong>图5</strong>：所有方法在模拟任务上的最终成功率（训练结束）。SENIOR在<code>pick-place</code>、<code>door-open</code>、<code>drawer-open</code>和<code>button-press</code>任务上取得最佳性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x6.png" alt="平均成功率与提升百分比"></p>
<blockquote>
<p><strong>图6</strong>：左图展示了所有方法在六个模拟任务上的平均成功率。右图显示了SENIOR相对于其他基线方法的平均性能提升百分比。SENIOR平均表现最优，相对于表现次优的SPRING方法仍有显著提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x7.png" alt="真实世界任务性能"></p>
<blockquote>
<p><strong>图7</strong>：在四个真实世界机器人操作任务上，SENIOR与PEBBLE、QPA在成功率随训练步数变化上的对比。SENIOR在所有任务上都取得了最高的成功率和/或最快的收敛速度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x8.png" alt="真实任务反馈效率"></p>
<blockquote>
<p><strong>图8</strong>：在真实世界任务上，SENIOR与PEBBLE、QPA在成功率随偏好查询次数变化上的对比。SENIOR能以更少的人类反馈达到更高的性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x12.png" alt="消融实验：组件贡献"></p>
<blockquote>
<p><strong>图12</strong>：在<code>pick-place</code>任务上的消融实验，展示了移除MDS、移除PGE以及完整SENIOR的性能对比。移除任一组件都会导致性能下降，证明了MDS和PGE各自的有效性以及二者协同的重要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x13.png" alt="消融实验：MDS有效性分析"></p>
<blockquote>
<p><strong>图13</strong>：对比MDS与随机选择、仅使用运动分数（m）选择、仅使用区分分数（d）选择在<code>pick-place</code>任务上的表现。完整的MDS（结合m和d）性能最佳，验证了其设计准则的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x14.png" alt="消融实验：PGE内在奖励分析"></p>
<blockquote>
<p><strong>图14</strong>：对比PGE与无探索、基于计数（伪计数）的探索、基于熵的探索在<code>pick-place</code>任务上的表现。PGE引导的探索能最快地提升策略性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x15.png" alt="查询质量分析"></p>
<blockquote>
<p><strong>图15</strong>：分析不同查询选择方法所选片段对的“易比较性”（通过标注一致性和标注时间衡量）和“信息量”（通过奖励模型预测准确率衡量）。MDS选出的片段对在易比较性和信息量上均优于或与其他方法相当。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x16.png" alt="状态覆盖可视化"></p>
<blockquote>
<p><strong>图16</strong>：在<code>pick-place</code>任务中，不同方法探索到的末端执行器位置（状态）分布可视化。SENIOR（PGE）引导智能体更有效地覆盖了任务相关的关键区域（如物体和目标位置附近）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x17.png" alt="奖励恢复曲线"></p>
<blockquote>
<p><strong>图17</strong>：在<code>pick-place</code>任务上，不同方法学习到的奖励函数与真实奖励函数之间的相关性（Pearson系数）随查询次数增加的变化。SENIOR能更快地恢复出准确的奖励函数。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x18.png" alt="超参数敏感性分析"></p>
<blockquote>
<p><strong>图18</strong>：对PGE中内在奖励权重初始值 (\beta_0) 和衰减率 (\rho) 的敏感性分析。结果表明SENIOR对这些超参数的变化不敏感，在不同设置下性能稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/x19.png" alt="MDS计算开销分析"></p>
<blockquote>
<p><strong>图19</strong>：MDS与基于分歧度的查询选择方法在单个查询周期内的计算时间对比。MDS的计算开销与基于分歧度的方法处于同一数量级，并未引入过高的计算负担。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_open_1.png" alt="真实环境任务执行序列示例-开门"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_open_2.png" alt="真实环境任务执行序列示例-开门"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_open_3.png" alt="真实环境任务执行序列示例-开门"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_open_4.png" alt="真实环境任务执行序列示例-开门"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_open_5.png" alt="真实环境任务执行序列示例-开门"></p>
<blockquote>
<p><strong>图20-24</strong>：SENIOR在真实世界开门任务中的成功执行序列示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_close_1.png" alt="真实环境任务执行序列示例-关门"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_close_2.png" alt="真实环境任务执行序列示例-关门"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_close_3.png" alt="真实环境任务执行序列示例-关门"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_close_4.png" alt="真实环境任务执行序列示例-关门"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/door_close_5.png" alt="真实环境任务执行序列示例-关门"></p>
<blockquote>
<p><strong>图25-29</strong>：SENIOR在真实世界关门任务中的成功执行序列示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_open_1.png" alt="真实环境任务执行序列示例-开箱"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_open_2.png" alt="真实环境任务执行序列示例-开箱"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_open_3.png" alt="真实环境任务执行序列示例-开箱"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_open_4.png" alt="真实环境任务执行序列示例-开箱"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_open_5.png" alt="真实环境任务执行序列示例-开箱"></p>
<blockquote>
<p><strong>图30-34</strong>：SENIOR在真实世界开箱任务中的成功执行序列示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_close_1.png" alt="真实环境任务执行序列示例-关箱"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_close_2.png" alt="真实环境任务执行序列示例-关箱"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_close_3.png" alt="真实环境任务执行序列示例-关箱"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_close_4.png" alt="真实环境任务执行序列示例-关箱"><br><img src="https://arxiv.org/html/2506.14648v1/extracted/6544775/real_environment/box_close_5.png" alt="真实环境任务执行序列示例-关箱"></p>
<blockquote>
<p><strong>图35-39</strong>：SENIOR在真实世界关箱任务中的成功执行序列示例。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>模拟任务</strong>：SENIOR在六个任务中的四个上取得了最佳最终性能，平均成功率最高。相对于表现次优的方法（SPRING），SENIOR平均提升了约16.1%的成功率。在收敛速度上，SENIOR也显著优于基线。</li>
<li><strong>反馈效率</strong>：在达到相同成功率时，SENIOR所需的人类偏好查询次数远少于其他方法。例如在<code>door-open</code>任务中，SENIOR仅需约100次查询即可达到80%成功率，而其他方法需要200-400次。</li>
<li><strong>真实世界任务</strong>：SENIOR在所有四个真实任务上均表现最佳，成功率达到80%-100%，且收敛所需的交互步数和查询次数均少于对比方法PEBBLE和QPA。</li>
<li><strong>消融实验</strong>：移除MDS或PGE任一组件都会导致性能显著下降，验证了二者的必要性。单独使用MDS中的运动分数（m）或区分分数（d）也不及二者结合的效果。PGE相较于其他探索策略（如基于计数、基于熵）能更有效地提升策略学习。</li>
<li><strong>机制分析</strong>：MDS选出的片段对确实更易被人类比较（标注一致性更高、时间更短）且富含信息（有助于奖励模型准确率快速提升）。PGE引导的探索能更有效地覆盖任务相关状态空间。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>运动区分选择（MDS）</strong>方案，通过量化评估片段对的运动信息丰富度和运动方向差异，选择出更易于人类标注且富含任务信息的查询，显著提高了人类反馈的效率和质量。</li>
<li>设计了<strong>偏好引导探索（PGE）</strong>机制，创新地将人类偏好密度与智能体探索密度结合，形成内在奖励，使探索过程直接服务于获取人类偏好的状态，有效提高了样本效率和策略学习速度。</li>
<li>通过MDS和PGE的协同作用，构建了SENIOR框架，在模拟和真实机器人操作任务上均验证了其在反馈效率和样本效率上的显著优势。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，MDS中使用的KDE和PCA计算以及PGE中维护额外的缓冲区会带来一定的计算开销。此外，当前方法主要针对机器人操作任务设计，其通用性（例如对非实体运动或视觉观察的任务）有待进一步验证。</p>
<p><strong>启示</strong>：</p>
<ol>
<li><strong>将人类先验更深入地融入RL</strong>：SENIOR展示了将人类对“易比较性”和“运动相关性”的直观认知转化为算法准则的有效性，这为将其他形式的人类先验或领域知识嵌入PbRL乃至更广泛的RL框架提供了思路。</li>
<li><strong>探索与偏好的协同</strong>：传统探索目标（如最大化状态熵）可能与任务目标不完全一致。PGE证明了将探索目标与任务偏好（即最终目标）直接对齐是一种高效范式，可激励后续研究设计更精细的偏好引导探索策略。</li>
<li><strong>迈向更复杂的应用</strong>：SENIOR在真实机器人任务上的成功，表明了其解决实际问题的潜力。未来的工作可以探索该方法在更复杂的任务设置（如多阶段任务、非结构化环境）、不同的反馈形式（如自然语言）以及更广泛的领域（如自动驾驶、游戏AI）中的应用。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对基于偏好的强化学习（PbRL）中反馈效率和样本效率低下的核心问题，提出SENIOR方法。其关键技术包括：1）基于运动区分的查询选择方案（MDS），通过状态核密度估计筛选运动明显、易于比较的行为片段对，以获取高质量偏好标签；2）偏好引导探索方法（PGE），设计内在奖励鼓励智能体探索高偏好、低访问的状态。实验表明，SENIOR在六项复杂机器人操作任务上，在人类反馈效率和策略收敛速度方面均优于现有五种方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.14648" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>