<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GLaD: Geometric Latent Distillation for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>GLaD: Geometric Latent Distillation for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.09619" target="_blank" rel="noreferrer">2512.09619</a></span>
        <span>作者: Guo, Minghao, Cao, Meng, Tao, Jiachen, Xu, Rongtao, Yan, Yan, Liang, Xiaodan, Laptev, Ivan, Chang, Xiaojun</span>
        <span>日期: 2025/12/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉-语言-动作（VLA）模型主要依赖于在2D图像-文本对上进行对比学习预训练的视觉编码器（如CLIP、SigLIP）。这些编码器擅长捕捉语义对应关系，但本质上缺乏对3D空间位置、物体结构和空间关系的编码能力。这种几何理解的缺失，导致模型在需要精确空间推理和操作的机器人任务中，可能出现对场景中物体的错误关注，限制了策略的泛化能力。</p>
<p>本文针对VLA模型缺乏几何先验这一具体痛点，提出通过知识蒸馏将3D几何知识注入模型预训练过程的新视角。核心思路是：利用一个能够从单张RGB图像推断深度、点云等3D属性的冻结几何感知教师网络（VGGT），通过特征对齐的方式，将其几何特征蒸馏到VLA模型中LLM对应于视觉令牌的隐藏状态里，从而将几何理解深度集成到驱动动作预测的多模态表征中。</p>
<h2 id="方法详解">方法详解</h2>
<p>GLaD是一个端到端的VLA框架，其整体架构包含一个标准的VLA主干和一个新增的几何蒸馏模块。训练分为两个阶段：大规模预训练（含几何蒸馏）和针对下游任务的微调（后训练）。</p>
<p><img src="https://arxiv.org/html/2512.09619v1/figures/arch_pretrain.png" alt="预训练架构"></p>
<blockquote>
<p><strong>图5</strong>：GLaD预训练阶段架构。视觉编码器（DINOv2+SigLIP）、投影器、LLM主干（LLaMA-2-7B）和特征对齐网络被训练。冻结的VGGT教师网络提供3D几何监督，其提取的特征与对齐后的LLM隐藏状态进行对齐。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.09619v1/figures/arch_posttrain.png" alt="后训练架构"></p>
<blockquote>
<p><strong>图6</strong>：GLaD后训练阶段架构。VLA主干通过LoRA进行参数高效的适配，而动作解码器和特征对齐模块被完全训练。VGGT教师网络保持冻结以保留几何先验。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>VLA主干</strong>：遵循UniVLA架构，包含Prismatic视觉编码器（DINOv2 + SigLIP）、一个MLP投影器、LLaMA-2-7B主干和一个动作解码器。</li>
<li><strong>几何蒸馏模块</strong>：<ul>
<li><strong>VGGT特征提取器</strong>：采用预训练且冻结的VGGT作为教师网络。给定一个历史图像帧（为简化，仅使用单帧），VGGT产生一个几何感知的特征表示 ( \mathbf{F}<em>{3d}^{\text{single}} \in \mathbb{R}^{N_p \times d</em>{\text{vggt}}} )，其中 ( N_p ) 是视觉块（patch）的数量。</li>
<li><strong>特征对齐网络</strong>：一个两层MLP，用于将LLM最后一层对应于图像令牌的隐藏状态 ( \mathbf{H}<em>{\text{img}} ) 投影到VGGT特征空间，得到 ( \mathbf{H}</em>{\text{aligned}} )。</li>
</ul>
</li>
<li><strong>训练目标</strong>：总损失为动作预测损失和几何对齐损失的加权和：( \mathcal{L}<em>{\text{total}} = \mathcal{L}</em>{\text{VLA}} + \lambda \mathcal{L}<em>{\text{distill}} )。其中，( \mathcal{L}</em>{\text{VLA}} ) 是预测潜在动作的交叉熵损失，( \mathcal{L}_{\text{distill}} ) 是对齐特征与VGGT特征之间的均方误差（MSE）损失。</li>
<li><strong>训练策略</strong>：<ul>
<li><strong>阶段1（预训练）</strong>：在Bridge数据集上，使用上述组合损失训练45个周期。VGGT保持冻结。</li>
<li><strong>阶段2（后训练）</strong>：在LIBERO等下游任务数据集上微调。VLA主干使用LoRA进行适配，动作解码器和特征对齐模块完全训练，VGGT保持冻结。</li>
</ul>
</li>
</ol>
<p><strong>核心创新点</strong>：与现有一些将几何特征直接输入LLM或仅在视觉编码器层面进行蒸馏的方法不同，GLaD的关键创新在于<strong>将几何特征对齐到LLM处理视觉令牌后产生的隐藏状态</strong>。这确保了学到的几何知识被深度整合到融合了视觉与语言信息的<strong>多模态表征</strong>中，从而直接影响了最终的动作生成。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>预训练数据集</strong>：Bridge数据集。</li>
<li><strong>评测基准</strong>：LIBERO（四个任务套件：SPATIAL, OBJECT, GOAL, LONG）及其鲁棒性评测版本LIBERO-PRO（包含物体、位置、语义和任务四类扰动）。</li>
<li><strong>对比基线</strong>：UniVLA, OpenVLA, Octo, Diffusion Policy, MAIL, MDT, LAPA等。</li>
<li><strong>实验平台</strong>：使用8×A100 GPU进行训练。</li>
</ul>
<p><strong>关键定量结果</strong>：<br>在标准LIBERO评测中，GLaD取得了最佳性能。</p>
<p><img src="https://arxiv.org/html/2512.09619v1/x1.png" alt="LIBERO结果表"></p>
<blockquote>
<p><strong>图7</strong>：LIBERO基准测试结果（表I）。GLaD以94.1%的平均成功率超越所有基线，特别是在LIBERO-OBJECT上达到97.4%（最高）。使用相同预训练数据的UniVLA为92.5%。</p>
</blockquote>
<p>在评测泛化鲁棒性的LIBERO-PRO上，GLaD展现出对物体外观扰动的显著更强的抵抗力。</p>
<p><img src="https://arxiv.org/html/2512.09619v1/figures/attn/intro/intro_attn_geo_libero.png" alt="鲁棒性对比图"></p>
<blockquote>
<p><strong>图3/4</strong>：LIBERO-PRO鲁棒性对比图（对应论文图3及表II）。GLaD在物体扰动（Obj）下表现突出，例如在LIBERO-GOAL上成功率达81%，远超UniVLA的62%。在“Put(bowl, plate)”任务中，优势高达60个百分点（GLaD 84% vs. UniVLA 24%）。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文通过检查点选择和训练时长进行了消融研究。结果表明，使用更充分的预训练检查点（45周期 vs. 27周期）能带来性能提升。同时，延长后训练步数（48k步）相比更短的步数能获得更好的效果。这些实验验证了充分几何蒸馏和任务适配的重要性。</p>
<p><strong>定性结果</strong>：<br>论文通过注意力图可视化，对比了有无几何蒸馏的VLA模型。</p>
<p><img src="https://arxiv.org/html/2512.09619v1/figures/attn/intro/intro_attn_univla_bridge.png" alt="Bridge场景注意力对比"></p>
<blockquote>
<p><strong>图1</strong>：无几何信息的VLA在Bridge任务“将桌布从桌角移到桌边”中的注意力。注意力分散，未能精确定位关键物体（桌布）和目标位置（桌边）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.09619v1/figures/attn/intro/intro_attn_geo_bridge.png" alt="Bridge场景注意力对比（GLaD）"></p>
<blockquote>
<p><strong>图2</strong>：GLaD在同一任务中的注意力。注意力更集中且准确，聚焦于相关的桌布和桌子边缘，体现了更好的空间理解。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>问题识别</strong>：明确指出当前基于2D视觉编码器的VLA模型缺乏几何理解这一关键局限，并通过注意力可视化等证据加以说明。</li>
<li><strong>方法创新</strong>：提出了GLaD框架，创新性地通过知识蒸馏将3D几何先验注入VLA预训练，且蒸馏目标对准LLM的多模态隐藏状态，确保了几何知识与语言-动作流程的深度集成。</li>
<li><strong>实证验证</strong>：在标准LIBERO基准上达到SOTA性能（94.1%），并在鲁棒性评测LIBERO-PRO上，特别是在物体外观扰动下，展现出显著的泛化能力提升，证明模型学会了基于几何本质特征而非表面纹理进行推理。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，为了简化，当前方法在预训练阶段仅使用了单帧历史图像，这可能限制了模型对动态场景和运动几何的理解。</p>
<p><strong>后续启示</strong>：</p>
<ol>
<li>GLaD证明了从强大的、冻结的几何教师模型（如VGGT）中蒸馏知识是增强VLA空间推理的有效且高效（无需深度传感器或3D标注）的途径。</li>
<li>方法框架具有通用性，可探索集成其他类型的几何教师模型（如PI3）或结合多帧序列信息以处理动态任务。</li>
<li>在物体外观变化频繁的真实世界机器人应用中，基于几何本质特征的表示学习对提升系统鲁棒性具有重要价值。未来工作可探索将几何蒸馏与触觉、力觉等多模态信息进一步结合。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型缺乏三维几何理解，导致空间推理与操作能力受限的问题，提出了GLaD框架。其核心方法是几何潜在蒸馏，通过将冻结的几何感知视觉变换器的特征，与大语言模型中视觉令牌对应的隐藏状态进行对齐，将几何先验深度整合到多模态表征中。在Bridge数据集上预训练后，GLaD在LIBERO任务套件上的平均成功率提升至94.1%，优于同等条件下的基线模型UniVLA（92.5%），验证了几何感知预训练能有效增强策略泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.09619" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>