<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ST4VLA: Spatially Guided Training for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ST4VLA: Spatially Guided Training for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.10109" target="_blank" rel="noreferrer">2602.10109</a></span>
        <span>作者: Jiangmiao Pang Team</span>
        <span>日期: 2026-02-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将大型视觉语言模型（VLMs）的能力扩展到具身任务面临核心挑战：机器人不仅需要理解指令的含义，还需确定在三维世界中“在哪里”以及“如何”行动。现有方法主要分为两类：一是基于分层机器人系统的方法，它们利用基础模型显式编码空间先验，但依赖基于规则的任务分解和手动设计的规划启发式方法，难以自动扩展到复杂多样的任务，并限制了端到端策略学习的潜力；二是数据驱动的视觉-语言-动作（VLA）模型，它们利用预训练的VLM和大规模遥操作数据集直接学习机器人控制，虽然避免了手动启发式规则，但倾向于过拟合低级动作模式，未能充分利用执行时的空间先验。本文针对VLA模型训练中空间感知与动作学习目标之间优化不一致、导致空间基础能力在策略学习过程中退化或冲突这一具体痛点，提出了“空间引导训练”的新视角。其核心思路是：通过一个两阶段训练流程，先让VLM获取可迁移的空间先验，再通过空间提示引导动作生成，从而在策略学习中保持空间基础能力，并促进空间与动作目标间的一致优化。</p>
<h2 id="方法详解">方法详解</h2>
<p>ST4VLA是一个双系统、端到端的VLA框架，其整体训练流程分为两个阶段：1）空间基础预训练；2）空间引导动作后训练。该设计将空间先验的获取与具体 embodiment 的控制解耦。</p>
<p><img src="https://arxiv.org/html/2602.10109v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ST4VLA整体框架。包含两个阶段：阶段1（空间基础预训练）在大量多源空间基础数据上训练VLM；阶段2（空间引导动作后训练）VLM规划器通过空间提示生成潜在规划令牌，作为动作专家的条件。</p>
</blockquote>
<p><strong>核心模块与架构</strong>：</p>
<ol>
<li><strong>双系统设计</strong>：系统2（VLM规划器）基于Qwen2.5-VL，作为一个多模态编码器，负责捕获空间和语义先验。系统1（动作专家）采用一个紧凑的扩散变换器（DiT）和DINOv2视觉编码器，负责具体 embodiment 的控制。</li>
<li><strong>连接机制</strong>：通过一个轻量级的查询变换器（8.7 MB）连接两者。该变换器以VLM规划器产生的潜在空间基础嵌入为条件，将可变长度的输入令牌映射为一组固定的可学习查询令牌，从而稳定专家的学习和推理。它实现为一个k层交叉注意力模块。</li>
<li><strong>空间提示</strong>：在动作后训练阶段，通过在任务指令后附加简单的空间提示（如“Figure out how to execute it, then locate the key object needed”）来显式激活VLM在预训练阶段学到的空间感知能力。提取的特征嵌入为规划器提供了明确的空间线索。</li>
<li><strong>梯度衰减</strong>：为了缓解动作专家梯度直接回流可能扭曲VLM多模态知识的问题，在查询变换器中引入了梯度衰减因子（例如0.5），以减弱从动作专家传回VLM的梯度，从而在保持规划器语义推理能力的同时实现有效的联合优化。</li>
</ol>
<p><strong>训练流程细节</strong>：</p>
<ul>
<li><strong>阶段1：空间基础预训练</strong>：目标是在通用视觉-语言理解与机器人特定的空间推理需求之间建立基础对齐。策略性地结合大规模互联网视觉-语言基础语料库（如RefCOCO、LLaVA-OneVision）与针对性的机器人特定数据集（如RoboRefIt、A0、ST4VLA Data）。通过将所有机器人数据重新格式化为与网络规模预训练一致的统一QA结构，使VLM在标准监督微调框架下发展出具有空间感知能力的表征空间。</li>
<li><strong>阶段2：空间引导动作后训练</strong>：重点是在保持和精炼阶段1获得的空间先验的同时，学习具体 embodiment 的控制。除了使用空间基础数据进行共同训练（VLM主干通过图像-提示对的下一个令牌预测进行更新）外，还针对动作数据引入空间提示，以增强语义推理与动作生成之间的对齐。例如，将指令“store all toys into the toy box”扩展为“Identify all relevant toys and their spatial relationships to the container.”</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，ST4VLA的创新在于明确将“在哪里、做什么”与“如何做”分离，并通过<strong>空间提示</strong>这一机制，在动作训练阶段主动引导模型产生更丰富的空间先验来指导动作生成，而非被动地共同优化可能冲突的目标。同时，<strong>梯度衰减</strong>的设计保护了VLM的已有知识。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试/数据集</strong>：SimplerEnv仿真基准套件（包含Google Robot和WidowX平台）、LIBERO仿真套件、基于Isaac-Sim构建的大规模模拟拾放基准（200个任务）、真实世界杂乱场景拾放任务（使用Franka Research 3机器人）、真实世界长视野操作任务。</li>
<li><strong>对比的基线方法</strong>：包括RT-1、RT-2-X、OpenVLA、CogACT、SpatialVLA、π₀、GR00T N1.5、Magma等先进的开放VLA系统，以及论文自身构建的Vanilla VLA和Vanilla Co-training VLA。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>SimplerEnv基准测试</strong>：如表2和表3所示，ST4VLA在Google Robot视觉匹配（VM）、视觉聚合（VA）和WidowX VM任务上均取得了最佳性能。具体而言，在Google Robot VM上平均成功率从Vanilla VLA的66.1%提升至84.6%，VA从63.5%提升至75.9%；在WidowX上从54.7%提升至73.2%。相较于之前最好的方法，分别获得了5.9%、5.3%和9.8%的增益。</p>
</li>
<li><p><strong>大规模模拟拾放泛化评估</strong>：如图4所示，在包含分布内、未见物体、新背景和未见指令四个维度的评估中，ST4VLA在200个模拟任务上均取得了最优的成功率，显著优于π₀和GR00T N1.5，展示了强大的视觉和语言泛化能力。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2602.10109v1/x4.png" alt="模拟拾放结果"></p>
<blockquote>
<p><strong>图4</strong>：在200个模拟指令跟随拾放任务中，不同泛化设置下的成功率。ST4VLA在所有设置下均领先。</p>
</blockquote>
<ol start="3">
<li><p><strong>真实世界拾放泛化评估</strong>：如表4所示，在包含分布内、未见物体实例、相似干扰物、新背景、未见物体位姿/朝向以及按属性/空间关系描述的未见指令等多个挑战性维度上，ST4VLA（平均成功率65%）全面优于π₀（31%）和GR00T N1.5（48%），特别是在“相似干扰物”和“未见物体朝向”等困难条件下优势明显。</p>
</li>
<li><p><strong>真实世界长视野操作评估</strong>：如图5所示，在桌面整理、抽屉组织、制作三明治等需要多步规划的任务中，ST4VLA在分布内、物理干扰和任务重规划三种设置下均优于基线，展示了其利用高级规划器分解复杂任务并由低级控制器稳健执行的能力。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2602.10109v1/x5.png" alt="真实世界拾放结果"></p>
<blockquote>
<p><strong>图5</strong>：长视野指令跟随操作任务的结果。ST4VLA在分布内、物理干扰和任务重规划三种设置下均优于基线。</p>
</blockquote>
<ol start="5">
<li><strong>消融与机理分析</strong>：如图3和表1所示，研究对比了三种训练策略：Vanilla VLA（仅动作微调）、Vanilla Co-training VLA（空间与动作数据联合训练）、ST4VLA（空间预训练+空间引导后训练）。<ul>
<li><strong>性能表现</strong>：ST4VLA在机器人操作和多模态理解/空间基础指标上均达到最佳平衡。例如，在保持RefCOCO-g <a href="mailto:&#73;&#111;&#85;&#64;&#x30;&#x2e;&#53;">&#73;&#111;&#85;&#64;&#x30;&#x2e;&#53;</a>达71.2%的同时，取得Google Robot VM 84.6%和WidowX 73.2%的成功率。</li>
<li><strong>优化对齐分析</strong>：通过投影空间相似度（PSS）量化梯度对齐程度。如图3(c)，Vanilla Co-training的PSS仅为0.25，表明优化存在冲突；而ST4VLA的PSS提升至0.42，说明空间引导训练显著改善了两个目标优化动态的一致性，这与更好的空间感知保持和更快的操作任务收敛相关。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2602.10109v1/x3.png" alt="消融研究"></p>
<blockquote>
<p><strong>图3</strong>：消融研究。(a) 感知性能（RefCOCO-g上的<a href="mailto:&#73;&#111;&#85;&#x40;&#48;&#x2e;&#x35;">&#73;&#111;&#85;&#x40;&#48;&#x2e;&#x35;</a>）；(b) 操作性能（WidowX上的平均成功率）；(c) 空间基础目标和动作策略目标的梯度相似度。ST4VLA在保持感知的同时实现了最佳的操作性能与优化对齐。</p>
</blockquote>
<p><strong>消融实验总结</strong>：表1的消融结果表明，“空间引导”（+Spatially Guided）和“空间预训练”（+Spatially Pretrained）两个组件均对性能有正向贡献，两者结合（即完整的ST4VLA）效果最佳，验证了空间先验的引入及其与动作学习的引导式对齐的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>揭示了直接微调VLM为VLA模型会导致空间先验崩溃，而简单地与空间数据共同训练则会引入梯度冲突的问题，并提出通过简单的空间提示可以有效缓解这些问题。</li>
<li>提出了ST4VLA，一个空间引导训练框架，通过两阶段训练（空间基础预训练 + 空间引导动作后训练）显式地将动作优化与空间基础目标对齐，从而在策略学习中保持感知能力并实现稳健控制。</li>
<li>在SimplerEnv等大规模模拟和真实机器人实验基准上取得了领先性能，并证明了对未见物体、新指令和分布外环境更强的泛化能力与鲁棒性。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确列出局限性，但从方法描述中可推断，其两阶段训练流程需要精心设计的数据集（混合网络数据与机器人数据），且引入了额外的查询变换器和梯度衰减超参数，可能增加训练复杂度和调优成本。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>空间引导机制</strong>：证明了在VLA训练中主动引导空间推理的有效性，为未来设计更精细的引导信号（如3D位置、 affordance 地图）提供了思路。</li>
<li><strong>优化一致性</strong>：提出的梯度相似度分析工具（PSS）可用于诊断和改善多任务学习中的优化冲突问题。</li>
<li><strong>双系统架构</strong>：将慢速、可靠的规划（System 2）与快速、具体的控制（System 1）分离，为构建兼具强推理能力和高效执行的具身智能体提供了可借鉴的框架。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ST4VLA，旨在解决大型视觉语言模型（VLM）在具体任务中难以将抽象指令转化为低级动作的问题。方法采用**空间引导训练**，包含两个阶段：**空间基础预训练**（通过点、框和轨迹预测学习可迁移的空间先验）和**空间引导动作后训练**（通过空间提示引导动作生成）。实验表明，该方法显著提升了VLA模型的性能，在Google Robot上准确率从66.1提升至84.6，在WidowX Robot上从54.7提升至73.2，并在泛化性和抗干扰性上表现出优势。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.10109" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>