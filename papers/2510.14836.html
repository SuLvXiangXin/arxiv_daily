<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.14836" target="_blank" rel="noreferrer">2510.14836</a></span>
        <span>作者: Li, Yixuan, Chen, Yuhui, Zhou, Mingcai, Li, Haoran, Zhang, Zhengtao, Zhao, Dongbin</span>
        <span>日期: 2025/10/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过融合预训练的视觉-语言模型与动作生成能力，在机器人学习领域展现出强大的泛化能力。然而，在执行长视野或细粒度操作任务时，这些模型性能显著下降，主要原因是语义理解与几何推理之间存在持续差距。缺乏可靠的3D理解导致模型经常错误估计物体位置或夹爪-物体关系，从而引发操作过程中的级联错误。</p>
<p>为了增强几何理解，现有方法主要分为三类：1）<strong>直接3D特征注入</strong>（如点云或深度图），这会增加模型复杂度并可能破坏预训练的2D先验；2）<strong>2D投影的3D特征集成</strong>，在投影过程中不可避免地引入信息损失；3）<strong>辅助3D信息预测</strong>（如预测未来深度图），虽能保留2D先验且无需额外推理输入，但现有方法未取得一致的性能提升，有时甚至因噪声监督和几何基础薄弱而对策略学习产生不利影响。这些方法的挑战在于：深度图监督质量受限于帧间时空一致性不足带来的噪声；逐像素深度回归产生高度冗余的学习信号；使用视觉-语言主干预测深度图可能干扰其预训练的语义对齐。</p>
<p>本文针对上述痛点，提出了<strong>量化深度预测作为辅助监督信号</strong>的新视角。核心思路是：不回归逐像素深度值，而是通过向量量化学习离散的深度表示，以紧凑且易于优化的方式捕捉关键结构信息，并引入独立的深度专家来预测这些量化深度令牌，从而在不干扰视觉-语言主干语义对齐的前提下利用几何线索。</p>
<h2 id="方法详解">方法详解</h2>
<p>QDepth-VLA的整体框架基于open π₀进行扩展，引入了一个辅助的深度监督分支。模型包含三个参数化模块：预训练的视觉-语言模型（VLM）、动作专家和新引入的深度专家。这些模块通过混合专家（MoE）结构和精心设计的混合注意力掩码进行协调。</p>
<p><img src="https://arxiv.org/html/2510.14836v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：QDepth-VLA整体架构与训练流程。(a) 总体架构，深度监督通过深度专家和潜在预测模块引入。在协同训练中，VQ-VAE编码器和码本被冻结，而PaLI-Gemma 3B、动作专家、深度专家、SigLIP和分词器是可训练的。(b) 提出的混合注意力掩码，它整合了深度和视觉令牌以增强空间推理和操作性能。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>深度标注与表示</strong>：由于现有VLA数据集缺乏足够的3D标注，论文使用Video-Depth-Anything（ViDA）从RGB帧生成单目深度估计作为监督。为了紧凑表示深度，预先训练了一个VQ-VAE。给定深度帧，编码器产生潜在表示，随后通过最近邻搜索在码本（K=256个条目，维度d=160）中进行量化，生成离散的深度码索引，作为深度专家的监督目标。</li>
<li><strong>模型架构</strong>：<ul>
<li><strong>VLM主干</strong>：采用PaliGemma-3B，其SigLIP图像编码器处理RGB图像生成256个视觉令牌，与文本令牌拼接后输入Gemma解码器，产生融合空间与语义线索的多模态嵌入。该主干在训练时可微调。</li>
<li><strong>动作专家</strong>：一个基于Transformer的模块，负责将多模态嵌入和本体感知状态转换为可执行的机器人动作。它建立在open π₀的动作头上。</li>
<li><strong>深度专家</strong>：其架构与动作专家对齐（见表1）。关键区别在于，它以SigLIP编码器的视觉嵌入（在语言融合之前，以避免语义干扰）作为输入，通过轻量级MLP投影、Transformer骨干处理，最后由浅层CNN解码器预测256个深度令牌。每个预测的令牌对应一个潜在向量，并与预训练VQ-VAE编码器在其码本上产生的量化令牌进行对齐。</li>
</ul>
</li>
<li><strong>混合注意力机制</strong>：不同于标准的因果注意力，本文重新设计了注意力掩码以更有效地调节跨模态信息流：<ul>
<li>文本和图像令牌仅在各自模态内交互，以保留预训练的语义基础。</li>
<li>深度令牌可以关注图像和文本令牌，使几何特征与视觉语义上下文化。</li>
<li>动作令牌可以关注所有前述模态，整合融合的感知和几何线索以生成策略。</li>
</ul>
</li>
<li><strong>协同训练流程</strong>：<ul>
<li><strong>量化深度监督损失</strong>：深度专家预测潜在深度令牌，计算这些令牌与VQ-VAE码本中所有码向量的负L2距离作为logits，然后使用从预训练VQ-VAE获得的真实码索引计算交叉熵损失（公式4）。</li>
<li><strong>动作建模损失</strong>：采用与π₀相同的条件流匹配（CFM）损失（公式5），使模型能够学习将带噪动作样本向干净真实值传输的流场。</li>
<li><strong>总损失与优化</strong>：总损失为动作损失与深度损失的加权和：ℒ_total = ℒ_action + λ_t ⋅ ℒ_depth。其中λ_t = λ_0 ⋅ γ^t 随训练步数指数衰减（λ_0=0.01）。这种调度使模型能先建立稳定的几何对齐，再逐渐专注于动作优化。使用AdamW优化器，并对VLM主干和动作专家采用5e-5的学习率。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>量化深度表示</strong>：将深度预测从高噪声、高冗余的逐像素回归，转化为预测离散的、紧凑的VQ-VAE码本索引，提供了更鲁棒、更易于优化的几何监督信号。</li>
<li><strong>独立的深度专家</strong>：在语言融合之前，从视觉嵌入中预测深度令牌，确保几何推理不会干扰VLM主干预训练的语义对齐。</li>
<li><strong>分层的混合注意力</strong>：通过精心设计的注意力掩码，允许深度信息增强空间理解，同时防止对预训练VLM的过度干扰，并保持计算效率。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/基准</strong>：在仿真环境中使用了<strong>LIBERO</strong>（包含Spatial, Object, Goal, Long四个子集）和<strong>Simpler</strong>（包含Google Robot和WidowX250 Robot任务）基准。在真实世界进行了操作任务验证。</li>
<li><strong>对比基线</strong>：包括通用VLA模型（OpenVLA, CoT-VLA, open π₀）、3D点云增强模型（SpatialVLA, GeoVLA）和深度增强模型（3D-CAVLA, 4D-VLA, DreamVLA）等。</li>
<li><strong>训练细节</strong>：基于open π₀进行预训练和微调。使用8×NVIDIA H20 GPU，采用FSDP策略，全局批量大小为1024。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>LIBERO基准（单视图）</strong>：如表2所示，QDepth-VLA在单视图设置下 consistently 超越基线。平均成功率比open π₀高出7.7%。在需要细粒度操作的Goal任务上达到94.0%，比CoT-VLA高6.4%；在长视野的Long任务上达到72.6%，比open π₀高6.6%。这表明量化深度监督有效补偿了缺乏多视图观察的不足。</li>
<li><strong>LIBERO基准（多视图变体）</strong>：当扩展为多视图输入时，QDepth-VLA（仅预测当前主视图深度）平均成功率达到94.9%，超越了DreamVLA（92.6%）和π₀-FAST（85.5%），证明了其框架在多视图配置下的有效泛化能力。</li>
<li><strong>Simpler基准</strong>：<ul>
<li><strong>Google Robot任务</strong>（表3）：在“Pick Coke Can”任务上达到98.3%成功率，比open π₀高0.8%。在复杂的“Open Top Drawer and Put Apple In”任务上达到62.6%，大幅超越open π₀达29.7%。</li>
<li><strong>WidowX250 Robot任务</strong>（表4）：在需要精确空间推理的“Stack Block”任务上达到39.6%，比SpatialVLA高10.4%。在“Put Eggplant in Basket”和“Put Spoon on Towel”任务上分别达到95.0%和82.0%，均优于open π₀。</li>
</ul>
</li>
<li><strong>真实世界实验</strong>：论文指出QDepth-VLA在真实世界机器人操作中实现了10.0%的性能提升，验证了其有效性和泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.14836v2/x3.png" alt="深度重建可视化"></p>
<blockquote>
<p><strong>图3</strong>：深度重建可视化。将预测的量化特征通过训练好的VQ-VAE解码器重建出的深度图，保留了结构细节并与物体边界对齐，证明学习的深度表示以有意义的方式捕捉了空间几何。</p>
</blockquote>
<p><em>该图通过可视化重建的深度图，定性证明了QDepth-VLA学习的深度表示能够有效捕捉场景的关键几何结构，如物体轮廓和空间层次，为模型的空间推理能力提供了直观证据。</em></p>
<p><strong>消融实验分析</strong>：<br>论文通过消融实验回答了核心问题：1) 深度监督能有效增强VLA性能，特别是在细粒度和长视野任务上；2) 量化深度预测优于像素级深度预测（DreamVLA），因为后者难以优化且可能强调冗余的低级线索；3) 提出的混合注意力掩码对性能增益有贡献，它有效调节了信息流，防止深度噪声干扰动作生成。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>QDepth-VLA框架</strong>，通过引入量化深度预测作为辅助监督任务，使VLA模型内化了几何理解，从而能更准确地进行物体空间关系推理。</li>
<li>设计了专门的<strong>深度专家</strong>来预测量化深度令牌，而非原始深度图。这种表述有效减轻了深度噪声的影响，为几何感知的策略学习提供了更紧凑、更易于优化的监督信号。</li>
<li>在<strong>LIBERO和Simpler仿真基准以及真实世界任务</strong>上的综合实验表明，QDepth-VLA显著提升了策略性能，平均成功率超越强基线open π₀，并验证了其有效性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，其方法依赖于单目深度估计（ViDA）来生成训练监督，深度图的质量和时空一致性受限于所采用的深度估计模型。未来可以探索更鲁棒的深度获取方式或联合优化深度估计与策略学习。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>量化表示的应用</strong>：将连续的、噪声敏感的感知信号（如深度、力觉）转化为离散的、鲁棒的量化表示，作为辅助任务，是增强机器人模型物理理解的一种有前景的方向。</li>
<li><strong>辅助任务的设计哲学</strong>：通过设计独立的专家模块来处理辅助任务，并利用分层的注意力机制控制信息流，可以在不破坏主干模型预训练能力的前提下，有效注入新的感知或推理维度。</li>
<li><strong>迈向深度“理解”</strong>：该方法超越了被动的深度“感知”，通过要求模型主动“预测”深度，鼓励其学习场景的几何结构，这对于需要精细空间推理的操作任务至关重要。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在精细操作任务中因缺乏3D几何理解而导致性能下降的核心问题，提出了QDepth-VLA框架。其关键技术是引入一个量化深度预测任务作为辅助监督，通过一个专用的深度专家模块，预测由VQ-VAE编码器生成的深度图的量化潜在标记，从而使模型学习到包含关键几何线索的深度感知表征。实验表明，该方法在仿真基准和真实任务中有效增强了模型的空间推理能力，并取得了有竞争力的操作性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.14836" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>