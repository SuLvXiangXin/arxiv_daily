<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Survey on Vision-Language-Action Models: An Action Tokenization Perspective - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>A Survey on Vision-Language-Action Models: An Action Tokenization Perspective</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.01925" target="_blank" rel="noreferrer">2507.01925</a></span>
        <span>作者: Zhong, Yifan, Bai, Fengshuo, Cai, Shaofei, Huang, Xuchuan, Chen, Zhang, Zhang, Xiaowei, Wang, Yuanfei, Guo, Shaoyang, Guan, Tianrui, Lui, Ka Nam, Qi, Zhiquan, Liang, Yitao, Chen, Yuanpei, Yang, Yaodong</span>
        <span>日期: 2025/07/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>近年来，人工智能在迈向通用智能方面取得了显著进展，其核心是基础模型的出现。大型语言模型在自然语言理解、推理和生成方面表现出色，视觉基础模型和视觉-语言模型则在多模态理解和生成上展现出强大能力。然而，尽管这些模型能力卓越，它们仍被限制在数字世界，影响了其对现实世界任务的影响力。为了突破这一界限，研究者开始探索利用基础模型的感知和认知能力来增强任务执行，从而将其智能延伸到物理世界。这催生了视觉-语言-动作模型，其被定义为：基于视觉和语言输入生成动作，并建立在至少一个大尺度视觉或语言基础模型之上的模型。VLA模型在机器人操作、导航和自动驾驶等领域迅速涌现，展示了在多任务学习、长视野任务完成和强泛化方面的潜力。</p>
<p>尽管方法看似多样，但本文观察到当前的VLA模型可以被统一在一个单一的框架下：视觉和语言输入被一系列VLA模块处理，产生一条动作令牌链，这些令牌逐步编码更具基础性和可执行性的信息，最终生成可执行的动作。本文进一步指出，区分不同VLA模型的主要设计选择在于动作令牌的表述方式。然而，目前研究界对动作令牌化缺乏系统且深入的理解，这严重阻碍了有效的VLA开发并模糊了未来方向。因此，本文旨在通过动作令牌化的视角对现有VLA研究进行分类和解读，提炼每种令牌类型的优势和局限，并指出改进领域。本文的核心思路是提出一个统一的VLA框架和一套动作令牌分类法，以此为透镜系统梳理该领域，并基于此分析未来趋势。</p>
<h2 id="方法详解：统一框架与动作令牌分类法">方法详解：统一框架与动作令牌分类法</h2>
<p>本文并非提出一种新方法，而是为整个VLA领域提供了一个分析和组织的统一视角与分类框架。</p>
<p><strong>整体框架</strong>：论文认为，现有VLA模型可以被抽象为一个统一框架。在这个框架中，视觉和语言输入被迭代地通过一系列 <strong>VLA模块</strong> 进行处理。VLA模块被定义为VLA模型中支持端到端梯度流的最大可微子网络，或是如运动规划等不可微的功能单元。这些模块产生一条 <strong>动作令牌</strong> 链。动作令牌被广义地定义为由VLA迭代生成的、最终导致动作执行的任何描述性指导，其概念超越了原始动作。这些令牌逐步编码越来越丰富和可操作的信息，最终产生可执行的动作。</p>
<p><img src="https://arxiv.org/html/2507.01925v1/x1.png" alt="VLA统一框架下的模块与令牌示例"></p>
<blockquote>
<p><strong>图1</strong>：从动作令牌化视角提出的VLA统一框架。展示了几个代表性VLA模型中VLA模块和动作令牌的实例化，强调了它们如何能在本文提出的框架下被统一地看待、解释和理解。</p>
</blockquote>
<p><strong>核心分类（八类动作令牌）</strong>：从上述视角出发，VLA模型主要通过动作令牌的表述和组织方式来区分。论文将动作令牌归纳为八种主要类型，并通过一个具体的具身任务示例进行了可视化对比：</p>
<ol>
<li><strong>语言描述</strong>：使用自然语言描述动作或计划。</li>
<li><strong>代码</strong>：生成可执行代码（如Python）来指导策略或控制器。</li>
<li><strong>可供性</strong>：标识出与任务相关的物体或场景的交互可能性（如抓取点、放置区域）。</li>
<li><strong>轨迹</strong>：预测机器人末端执行器或智能体在状态空间或任务空间中的运动路径。</li>
<li><strong>目标状态</strong>：指定任务完成时应达到的期望状态，通常以图像或场景表示形式呈现。</li>
<li><strong>潜在表示</strong>：通过专门预训练构建的具身动作序列的紧凑、连续的向量表示。</li>
<li><strong>原始动作</strong>：直接输出底层控制命令（如关节扭矩、速度）。</li>
<li><strong>推理</strong>：作为增强其他令牌的“元令牌”，涉及思维链、规划等认知过程。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.01925v1/x2.png" alt="单一任务中不同动作令牌的可视化"></p>
<blockquote>
<p><strong>图2</strong>：单一具身任务中动作令牌的可视化。给定相同的视觉和语言输入，不同的VLA模型将其编码为不同的动作令牌，每种令牌传达了不同形式的可操作指导，并需要不同的令牌生成和后处理策略。</p>
</blockquote>
<p><strong>创新点</strong>：本文的核心创新在于提出了“动作令牌化”这一统一视角和分析框架。与以往零散地介绍各类VLA工作不同，本文首次系统地将多样化的方法统一到“模块生成令牌链”的框架下，并创建了一个涵盖八类令牌的完整分类法。这为理解、比较和设计VLA模型提供了清晰的概念地图和共同语言。</p>
<h2 id="分析、趋势与展望">分析、趋势与展望</h2>
<p>本文通过对各类动作令牌的深入分析，总结了VLA领域的核心发展趋势和未来方向。</p>
<p><strong>动作令牌趋势</strong>：VLA模型的未来不在于单一的主导性动作令牌，而在于它们的战略融合。语言描述在表达性上受限，不太可能成为主流，但语言规划对于任务分解仍然必不可少。代码是一种强大的替代方案，其潜力将通过构建集成了感知和动作原语的综合函数库来解锁，以解决复杂的长视野任务。一个关键的协同效应正在形成：<strong>可供性</strong>提供语义上的“做什么”指导，而<strong>轨迹</strong>定义精确的“怎么做”路径。这种配对得到了<strong>世界模型</strong>的有力支持，世界模型可以预测视觉目标状态，为这两种令牌的生成提供基础。潜在表示前景广阔但面临训练挑战。原始动作是端到端学习的理想目标，但仍受限于数据可用性。最后，<strong>推理</strong>作为增强所有其他令牌的元令牌，正从纯语言基础推理演变为基于动作令牌、结合多模态反馈和自适应测试时计算的推理。</p>
<p><strong>VLA架构趋势</strong>：有效的VLA模型可能采用分层架构：顶层使用语言描述和代码进行长视野规划和逻辑控制；短期内，下层预计将紧密整合目标状态的视频预测、轨迹的流建模和可供性的3D交互预测，以形成中间运动表示，最终映射到原始动作；长期来看，下层将向完全端到端的方式演进，直接从子任务级输入预测原始动作。推理将根据需要始终集成在整个VLA模型中。</p>
<p><strong>其他关键方向</strong>：</p>
<ul>
<li><strong>从模仿学习到强化学习</strong>：通过结合强化学习，VLA模型可以克服模仿学习的局限，实现更类人的试错和自主探索。但现实世界部署需要更高效的RL算法来解决高重置成本和低交互效率的问题。此外，VLM可以自动化生成密集奖励函数，加速模型训练和部署。</li>
<li><strong>从VLA模型到VLA智能体</strong>：应有意识地从VLA模型演进到VLA智能体，后者是具有记忆、探索、规划和反思等更广泛认知架构的主动系统。这种转变也意味着从当前的线性处理架构转向更复杂、双向和图结构的拓扑。</li>
<li><strong>进步的三位一体：模型、数据与硬件</strong>：具身AI旨在处理物理世界非结构化、开放式的本质，这需要模型、数据和硬件的协同。然而，进展很大程度上受限于受限的机器人平台和稀缺的高质量具身数据，迫使大多数研究进入远离真实世界复杂性的简化实验室环境。因此，该领域仍处于早期阶段。实现鲁棒的通用智能需要模型、数据和硬件的协同进化，齐头并进而非孤立发展。</li>
<li><strong>安全与对齐</strong>：当前的VLA研究主要关注模型能力。未来的工作必须更加重视确保安全性和与人类的对齐。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.01925v1/x3.png" alt="基础模型、VLA模型与数据源的演进时间线"></p>
<blockquote>
<p><strong>图3</strong>：基础模型、VLA模型与数据源的演进时间线。U型曲线反映了VLA的日益增多是如何得到基础模型和数据进步的支持的。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个从动作令牌化视角出发的VLA统一框架，将现有纷繁复杂的研究纳入一个清晰的分析结构；2) 建立了一套包含八类动作令牌的详细分类法，并系统分析了每类令牌的动机、方法、属性、优缺点及未来工作，为领域提供了重要的概念梳理和比较基准；3) 基于上述分析，提炼并展望了VLA模型在令牌融合、架构演进、学习范式、智能体形态以及模型-数据-硬件协同等关键维度上的未来发展趋势。</p>
<p>论文自身提到的局限性反映了整个领域的现状：由于受限于机器人硬件平台和高质量具身数据的稀缺，大多数VLA研究被迫在简化的实验室环境中进行，远离真实世界的复杂性，因此该领域总体上仍处于发展的早期阶段。</p>
<p>本文对后续研究的启示是多方面的。首先，它指引研究者不应孤立地看待某类动作令牌或方法，而应关注不同令牌类型之间的协同与融合，特别是在分层架构中的组合运用。其次，它强调了突破当前瓶颈需要模型、数据与硬件三者的协同进化，鼓励跨学科合作。最后，它呼吁在追求能力提升的同时，必须提前布局安全与对齐研究，以确保未来强大VLA系统的可靠和有益部署。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文从动作标记化视角综述视觉-语言-动作模型。核心问题是现有研究对动作标记缺乏系统理解，阻碍VLA模型发展。论文提出统一框架：视觉与语言输入经VLA模块处理，生成逐步具象化的动作标记链，最终输出可执行动作。关键技术是将动作标记归纳为语言描述、代码、可供性、轨迹、目标状态、潜在表示、原始动作和推理八类。分析指出，未来趋势在于多种动作标记的战略融合，而非单一类型主导；语言规划仍关键，代码潜力需通过构建感知与动作原语库来释放。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.01925" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>