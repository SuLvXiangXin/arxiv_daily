<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.07434" target="_blank" rel="noreferrer">2602.07434</a></span>
        <span>作者: Yang, Songhua, Li, Xuetao, Fei, Xuanye, Li, Mengde, Li, Miao</span>
        <span>日期: 2026/02/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，人机交互研究已从单一的功能性任务扩展到情感与表达性交互。传统方法主要依赖于基于规则或模板的系统，这些方法在复杂多模态交互中缺乏可扩展性。数据驱动的机器学习方法虽然提升了行为生成的多样性和适应性，但通常需要为每种行为类型收集大量专用数据集，成本高昂。近年来，大型语言模型特别是视觉语言模型在机器人指令解析和任务规划中展现出潜力，但其在多模态情感表达协调方面的应用仍未被充分探索。现有工作往往侧重于简化的机器人实体，提供的多模态情感交互有限，且忽视了与用户更深层的情感共鸣。</p>
<p>本文针对人形机器人缺乏协调的语音、面部表情和手势这一痛点，提出了一个基于视觉语言模型的新框架。核心思路是：利用VLM理解和规划多模态表达，通过新颖的语义序列对齐机制确保言语内容与物理动作在时间上的精确同步，并借助知识蒸馏实现高效的边缘部署。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了SeM2框架，旨在为人形机器人生成富有表现力的多模态交互。系统以人类语音指令Si和同步视觉观察V作为输入，输出语音响应So、面部表情序列E和动作执行序列M，学习一个映射函数f: (Si, V) → (So, E, M)，并保持跨模态的一致性。</p>
<p><img src="https://arxiv.org/html/2602.07434v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：SeM2框架整体示意图。左侧展示了一个具体交互示例的输入与输出流程，右侧展示了多模态表达生成与执行的并行模块，下方是语义序列对齐机制的核心流程。</p>
</blockquote>
<p>框架包含三个核心组件：</p>
<ol>
<li><strong>多模态交互感知模块</strong>：扩展了传统感知通道，以捕捉更细致的用户语言内容、情感状态和上下文信息。具体使用SenseVoice进行语音处理和情感识别，使用YOLOv8-face进行面部检测与定位。感知结果与精心设计的提示模板结合，输入给VLM。</li>
<li><strong>思维链提示</strong>：引导VLM综合分析视觉和语音输入，形成对用户状态的完整感知。提示要求模型在生成每种表达时，显式考虑语言、表情和动作之间的语义一致性与情感协调，并明确指定机器人可执行的动作和表情脚本集合，以确保生成的命令符合机器人物理约束。</li>
<li><strong>多模态表达生成模块</strong>：负责将VLM的结构化JSON输出转化为可执行的机器人行为。系统将解析后的指令并行发送至语音合成（使用ChatTTS）、面部表情生成和运动控制模块。其中，创新的<strong>语义序列对齐机制</strong>（SSAM）是确保交互自然性的关键。</li>
</ol>
<p><strong>语义序列对齐机制（SSAM）详解</strong>：VLM生成的动作/表情序列虽与文本语义相关，但通常缺乏精确的时间对齐，导致时机不当或节奏不一致。SSAM的核心是通过动态规划求解时间约束优化问题，在文本词语与表达动作之间建立语义桥梁，确定最优执行计划。</p>
<ul>
<li><strong>步骤1：语音时间映射</strong>：将语音文本So分词，并根据每个词语的基础持续时间τ(wk)和语速调整因子α(speed)，估算每个词语在合成语音中的开始时间(t_i^s)和结束时间(t_i^e)。</li>
<li><strong>步骤2：语义相关性计算</strong>：使用预训练的BERT-base-uncased模型获取词语和动作/表情标识符的嵌入向量，通过余弦相似度S(wi, aj) = cos(Emb(wi), Emb(aj))计算语义相关性。设定阈值θ=0.7，过滤出显著相关的词-动作对。</li>
<li><strong>步骤3：时间约束优化</strong>：通过动态规划求解公式(3)的优化问题。目标是在允许的时间偏差范围δ=0.3秒内，最大化动作与相关词语的语义相似度之和。约束条件包括：动作必须按顺序执行，且冲突动作对（如涉及重叠手臂运动的“挥手”和“握手”）的执行时间需要错开。最终为每个表达动作aj确定最优的执行时间T(aj)。</li>
</ul>
<p><strong>边缘可部署模型</strong>：为实现在无稳定云连接环境下的自主运行，本文通过知识蒸馏构建了边缘部署模型SeM2_e。以GPT-4o作为教师模型，收集并筛选高质量交互样本，使用MiniCPM-8B作为学生模型骨干进行监督微调，最后进行INT4量化以部署在NVIDIA Jetson Orin等边缘硬件上。</p>
<p><img src="https://arxiv.org/html/2602.07434v1/x2.png" alt="边缘部署构建"></p>
<blockquote>
<p><strong>图2</strong>：SeM2_e边缘可部署模型的详细构建过程，展示了从数据收集、SimHash去重、知识蒸馏到最终量化部署的完整流程。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在一个具有36自由度（主要使用上半身）的人形机器人平台上进行。感知系统包括RGB-D相机和麦克风阵列。生成系统中，云端版本使用GPT-4o API作为教师模型，边缘版本使用经微调的MiniCPM-V 8B模型。软件基于ROS2，使用PyTorch和TensorRT进行优化推理。</p>
<p><strong>评估基准与基线</strong>：由于与现有工作直接比较存在困难（如硬件特异性），评估主要侧重于消融研究和单模态变体对比。配置包括：完整的SeM2、边缘版SeM2_e、以及分别去除面部表情、去除动作、仅保留语言的变体。</p>
<p><strong>关键实验结果</strong>：采用AI专家（GPT-4V）和人类专家双盲评估，使用5点李克特量表在自然度、情感清晰度、模态一致性、响应适当性和整体用户体验五个维度上进行评分。</p>
<p><strong>AI评估结果（表I）与人类评估结果（表II）趋势一致</strong>：完整的多模态系统在所有指标上均显著优于所有简化变体。例如，在AI评估中，完整SeM2在整体用户体验上得分为4.53，而仅语言版本为3.02，证明了多模态协调的优越性。<strong>边缘部署模型SeM2_e保持了云端教师模型约95%的性能</strong>（如整体体验得分4.37 vs 4.53）。</p>
<p><strong>消融实验（表III）</strong>：针对SSAM组件的消融研究表明，<strong>模态同步</strong>组件的移除导致性能下降最大（整体体验下降0.79），证实了时间对齐的关键作用。移除上下文映射主要影响情感清晰度，移除时间规划影响自然度。基于规则的方法性能显著低于任何部分SSAM配置。</p>
<p><img src="https://arxiv.org/html/2602.07434v1/x3.png" alt="案例研究"></p>
<blockquote>
<p><strong>图3</strong>：SeM2在不同场景下进行人机交互的代表性案例。每列展示了一个用户输入及其多模态响应，体现了系统在庆祝、情感支持等不同上下文中的适应性表达。</p>
</blockquote>
<p><strong>响应时间分析（表IV）</strong>：在理想网络条件下，边缘模型SeM2_e的平均首次响应时间（3.68秒）比云端模型SeM2（7.70秒）快约52%，同时保持了可比的交互轮次持续时间。这凸显了边缘部署在提升实时性方面的优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的、基于VLM的框架（SeM2），用于生成人形机器人协调一致的多模态情感表达。</li>
<li>引入了语义序列对齐机制（SSAM），通过求解时间约束优化问题，实现了语音内容与表达行为之间的精确时间协调。</li>
<li>通过知识蒸馏实现了框架的高效边缘部署（SeM2_e），在保持核心性能（约95%）的同时，显著提升了响应速度，证明了复杂多模态交互系统在资源受限平台上实际应用的可行性。</li>
</ol>
<p><strong>局限性</strong>：论文提及的局限性包括：目前主要专注于上半身交互，未整合移动功能；框架的实现与特定机器人平台绑定，泛化性受到一定限制。</p>
<p><strong>启示</strong>：这项工作为构建自主、社会化的表达型机器人指明了方向。未来研究可探索全身协调、适配多样化的机器人实体，并开发更复杂的长时交互同步机制，以推动无需云连接的、能在不可预测的真实环境中保持吸引人的人机交互的机器人发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人缺乏协调的语音、情感和动作表达，且需边缘部署自主运行的核心问题，提出基于视觉语言模型（VLM）的多模态框架SeM²。其关键技术包括多模态感知模块、Chain-of-Thought推理规划及语义序列对齐机制（SSAM），通过时间约束优化实现语言与物理表达的精确同步。实验表明，边缘部署版本SeM²e经知识蒸馏后，在边缘硬件上保持95%的相对性能，并在自然性、情感清晰度和模态协调性上显著优于单模态基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.07434" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>