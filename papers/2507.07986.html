<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EXPO: Stable Reinforcement Learning with Expressive Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>EXPO: Stable Reinforcement Learning with Expressive Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.07986" target="_blank" rel="noreferrer">2507.07986</a></span>
        <span>作者: Chelsea Finn Team</span>
        <span>日期: 2025-07-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人等领域通过模仿学习在大数据集上训练表达性策略（如扩散策略、流匹配策略）取得了显著进展。然而，模仿学习方法往往难以达到实际应用所需的高可靠性和性能。原则上，通过在线强化学习（RL）对这些策略进行微调可以实现自我改进，但现有的在线RL方法（如TD3、SAC）通常为简单的高斯策略设计，无法有效利用模仿学习中常用的表达性预训练策略。微调表达性策略面临一个独特挑战：这类策略由一长串去噪步骤参数化，当需要针对某些价值函数优化其动作时，从动作输出到策略参数的梯度传播会变得不稳定，阻碍了稳定的价值最大化。本文针对“如何对表达性策略类进行稳定、高效的在线RL微调”这一痛点，提出了一个新视角：避免直接对表达性策略本身进行价值优化。核心思路是：使用稳定的模仿学习目标训练基础表达性策略，并构建一个即时策略，通过一个轻量级的高斯编辑策略来编辑基础策略的动作，使其朝向更高价值的分布，从而实现价值最大化。</p>
<h2 id="方法详解">方法详解</h2>
<p>EXPO（Expressive Policy Optimization）旨在实现表达性策略的样本高效在线微调，其核心是避免对表达性策略进行直接的价值函数优化。方法整体框架基于两个参数化策略：一个大型的基础表达性策略 $\pi_{\text{base}}$ 和一个轻量级的高斯编辑策略 $\pi_{\text{edit}}$。</p>
<p><img src="https://arxiv.org/html/2507.07986v2/extracted/6625753/figures/method_figure.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：EXPO方法整体框架。左侧：EXPO通过避免直接优化表达性策略的价值函数，实现了稳定的、样本高效的RL训练。右侧：EXPO与基线方法在各项任务上的平均性能对比。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>基础表达性策略 ($\pi_{\text{base}}$)<strong>：该策略通常由离线预训练初始化，在在线微调阶段，</strong>不</strong>使用价值函数目标进行训练，而是使用一个稳定的模仿学习目标 $\mathcal{L}_{\mathrm{IL}}$（如行为克隆）在混合了离线数据和在线经验的回放缓冲区数据上进行训练。这确保了训练的稳定性。</li>
<li>**高斯编辑策略 ($\pi_{\text{edit}}$)**：这是一个轻量级策略，其作用是“编辑”从基础策略采样的动作，使其获得更高的Q值，同时鼓励探索以保持动作多样性。具体而言，给定状态 $s$ 和从基础策略采样的动作 $a \sim \pi_{\text{base}}(\cdot|s)$，编辑策略输出一个编辑量 $\hat{a} \sim \pi_{\text{edit}}(\cdot|s, a)$，编辑后的动作为 $\tilde{a} = a + \hat{a}$。编辑策略通过标准的熵正则化策略损失进行训练：<br>$L(\pi_{\text{edit}}) = -\mathbb{E}<em>{(s,a)\sim\mathcal{D},\hat{a}\sim\pi</em>{\text{edit}}(\cdot|s,a)}[Q_{\phi}(s,a+\hat{a}) - \alpha \log \pi_{\text{edit}}(\hat{a}|s,a)]$<br>其中 $Q_{\phi}$ 是critic网络，$\alpha$ 是温度系数。为防止编辑后的动作偏离合理行为分布太远，论文通过超参数 $\beta$ 将编辑量 $\hat{a}$ 的幅度限制在 $[-\beta, \beta]$ 范围内。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.07986v2/extracted/6625753/figures/action_plot.png" alt="编辑策略示意图"></p>
<blockquote>
<p><strong>图2</strong>：编辑策略示意图。蓝色等高线表示单个状态下动作的Q值，橙色等高线表示编辑策略将基础动作（黑点）转换成的动作高斯分布。编辑策略学习将动作向更高Q值区域移动。</p>
</blockquote>
<ol start="3">
<li><strong>即时策略参数化与价值最大化</strong>：为了提取价值最大化的动作，EXPO构建了一个非参数化的“即时策略” $\pi_{\text{OTF}}$。对于每个状态 $s$，该策略首先从基础策略采样 $N$ 个动作 ${a_i}$，并对每个基础动作应用编辑策略得到 $N$ 个编辑后动作 ${\tilde{a}<em>i}$。然后，它从这 $2N$ 个候选动作（基础动作+编辑后动作）中选择Q值最高的动作作为最终输出：$a^* = \arg\max</em>{a \in \bigcup_{i=1}^{N}{{a_i,\tilde{a}<em>i}}} Q</em>{\phi}(s, a)$。这个 $\pi_{\text{OTF}}$ 策略被同时用于<strong>环境交互采样</strong>和<strong>时序差分（TD）备份中的目标动作计算</strong>。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，EXPO的主要创新在于将“表达性建模”和“价值最大化”这两个目标解耦。基础策略专注于通过稳定的模仿学习保持和优化复杂的动作分布（表达性），而价值最大化则通过一个轻量、易于优化的编辑策略以及一个即时的、基于采样的最大化步骤来实现。这种设计避免了梯度通过冗长的去噪链反向传播的不稳定性，同时通过编辑策略的熵正则化自然地引入了超出行为分布的探索。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：论文在4个领域（Adroit、Meta-World、Franka Kitchen、Maze2D）共12个任务上评估EXPO。实验平台基于标准连续控制环境。对比的基线方法包括：在线RL方法（TD3、SAC）、离线转在线RL方法（AWAC、IQL、EDAC、SQL）、以及专门针对表达性策略的RL方法（Diffusion-QL， DIPO）。</p>
<p><img src="https://arxiv.org/html/2507.07986v2/extracted/6625753/figures/online.png" alt="在线RL性能"></p>
<blockquote>
<p><strong>图3</strong>：纯在线RL设置下的性能曲线。EXPO在多数任务上收敛更快，最终性能更高，平均样本效率提升显著。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.07986v2/extracted/6625753/figures/offline_to_online.png" alt="离线转在线RL性能"></p>
<blockquote>
<p><strong>图4</strong>：离线转在线RL设置下的性能曲线。给定离线预训练策略和数据集，EXPO在在线微调阶段表现出优异的样本效率。</p>
</blockquote>
<p><strong>关键实验结果</strong>：在纯在线RL和离线转在线RL两种设置下，EXPO在12个任务的平均性能上均显著优于所有基线方法。具体而言，EXPO实现了<strong>平均2-3倍的样本效率提升</strong>。例如，在Adroit的<code>pen-human</code>任务中，EXPO仅用约20万步就能达到接近1的归一化得分，而其他方法需要约60万步或更多。</p>
<p><strong>消融实验</strong>：<br>论文通过消融研究验证了核心组件的必要性。</p>
<p><img src="https://arxiv.org/html/2507.07986v2/extracted/6625753/figures/a_N_1x2_grid.png" alt="消融实验：动作选择与编辑"></p>
<blockquote>
<p><strong>图5</strong>：消融实验一：比较不同动作选择策略和是否使用编辑策略。“EXPO (w/ edit)”是完整方法；“EXPO (w/o edit)”仅使用基础策略动作，无编辑；“Greedy”仅选择基础动作中Q值最高的，无编辑。结果显示，同时使用编辑和最大动作选择至关重要。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.07986v2/extracted/6625753/figures/a_edit_1x2.png" alt="消融实验：TD备份目标"></p>
<blockquote>
<p><strong>图6</strong>：消融实验二：比较在TD备份目标计算中是否使用即时策略（即选择最大Q值动作）。<code>EXPO (OTF-TD)</code>在TD目标中使用$\pi_{\text{OTF}}$，<code>EXPO (base-TD)</code>则使用基础策略采样。结果表明，在TD备份中也进行价值最大化能带来显著性能提升。</p>
</blockquote>
<p><strong>总结</strong>：消融实验表明，1）<strong>编辑策略</strong>对于将动作推向高价值区域至关重要；2）在<strong>环境采样和TD备份中均使用即时策略进行最大动作选择</strong>，能确保智能体行为和价值目标快速对齐，这是实现高效在线学习的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了EXPO，一个简单而有效的框架，用于对表达性策略类进行稳定、样本高效的在线强化学习微调。</li>
<li>提出了一种解耦的价值最大化思想：通过稳定的模仿学习训练基础表达性策略，并利用一个轻量级编辑策略和即时最大化步骤来实现价值优化，避免了直接反向传播梯度通过复杂策略网络的不稳定性。</li>
<li>该方法对策略参数化方式不可知，适用于扩散策略、流匹配策略等多种表达性策略，并通过编辑策略的熵正则化自然促进了探索。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，编辑策略的编辑范围 $\beta$ 是一个需要调整的超参数，其合适取值可能依赖于任务和离线数据集的特性。</p>
<p><strong>启示</strong>：EXPO为利用大规模预训练表达性模型进行高效在线微调提供了新思路。其“基础策略保表达性，辅助策略做优化”的范式可能启发更广泛的、涉及复杂生成模型与强化学习结合的研究。未来的工作可以探索如何自适应地设置编辑范围，或将此框架扩展到更复杂的多模态策略和更动态的环境中去。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对在线强化学习中训练表达性策略（如扩散策略）时，因长去噪链导致梯度传播不稳定、难以实现稳定价值最大化的问题，提出了EXPO算法。该方法避免直接优化表达性策略的价值，而是构建即时策略，结合通过模仿学习稳定训练的基础策略和轻量高斯编辑策略，编辑动作以提升价值分布，并从基础与编辑动作中选择价值最大化动作进行优化。实验表明，在微调预训练策略及利用离线数据在线训练时，该方法的样本效率平均比先前方法提升2-3倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.07986" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>