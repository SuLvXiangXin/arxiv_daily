<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.03738" target="_blank" rel="noreferrer">2505.03738</a></span>
        <span>作者: Li, Jialong, Cheng, Xuxin, Huang, Tianshu, Yang, Shiqi, Qiu, Ri-Zhao, Wang, Xiaolong</span>
        <span>日期: 2025/05/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前人形机器人全身控制的主流方法主要分为两类：基于运动捕捉（MoCap）的模仿学习框架和基于轨迹优化（TO）的方法。基于MoCap的方法利用重定向的人类动作捕捉轨迹来定义奖励目标，引导策略学习，但其参考数据集通常存在固有的运动学偏差——它们主要包含双足运动序列（如行走、转身），而缺乏对超灵巧操作至关重要的协调臂-躯干运动，导致模拟动作与硬件可执行行为之间存在“具身鸿沟”。基于TO的方法则面临互补的局限性：它们依赖于有限的动作基元库，且在实时应用中计算效率低下，这阻碍了策略的泛化能力，使其难以部署在需要快速适应非结构化输入（如反应式遥操作或环境扰动）的动态场景中。</p>
<p>本文针对实现人形机器人真正的全身灵巧控制，特别是扩展其操作工作空间这一具体痛点，提出了一个新视角：将模拟到现实（sim-to-real）的强化学习（RL）与轨迹优化相结合，构建一个能够实时适应、甚至处理分布外（O.O.D.）命令的框架。其核心思路是：首先通过融合运动捕捉数据与概率采样的躯干姿态，合成混合运动指令并驱动轨迹优化器生成动态可行的全身参考运动，构建一个去偏的训练数据集（AMO数据集）；随后，训练一个能够学习连续映射的AMO网络，使其能够稳健地插值连续输入空间并适应O.O.D.遥操作命令，从而实现实时、自适应的全身控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>AMO框架是一个分层系统，旨在实现无缝的全身控制，其整体流程分为四个阶段。</p>
<p><img src="https://arxiv.org/html/2505.03738v1/x2.png" alt="系统总览"></p>
<blockquote>
<p><strong>图2</strong>：系统概述。系统分解为四个阶段：1. 通过轨迹优化收集AMO数据集来训练AMO模块；2. 在仿真中通过师生蒸馏训练RL策略；3. 通过逆运动学（IK）和重定向进行真实机器人遥操作；4. 通过模仿学习（IL）和Transformer训练真实机器人自主策略。</p>
</blockquote>
<p><strong>整体框架与问题定义</strong>：系统处理遥操作和自主两种设置。在遥操作中，目标条件策略 π′ 接收来自操作者的控制信号 <strong>g</strong> = [<strong>p_head</strong>, <strong>p_left</strong>, <strong>p_right</strong>, <strong>v</strong>]（头、手关键点姿态和基座速度），以及包含视觉和本体感知的观测 <strong>s</strong>，输出上下肢关节角指令 <strong>a</strong>。策略采用分层设计：上体策略 π′_upper 根据头手目标输出上体关节角 <strong>q_upper</strong> 和一个中间控制信号 <strong>g′</strong> = [<strong>rpy</strong>, h]（躯干朝向和高度）；下体策略 π′_lower 根据速度指令 <strong>v</strong>、中间信号 <strong>g′</strong> 和本体感知 <strong>s_proprio</strong> 输出下体关节角 <strong>q_lower</strong>。自主策略 π 结构类似，但其上体策略无需人类输入，直接从观测生成 <strong>q_upper</strong>、<strong>v</strong> 和 <strong>g′</strong>。</p>
<p><strong>核心模块一：AMO模块预训练</strong>。该模块 ϕ(<strong>q_upper</strong>, <strong>rpy</strong>, h) = <strong>q_lower^ref</strong> 的核心作用是将上体姿态和躯干命令 (<strong>rpy</strong>, h) 转换为供下体策略显式跟踪的参考关节角。其训练关键在于构建<strong>AMO数据集</strong>：首先从AMASS数据集中随机选择上体运动，并采样随机的躯干命令；然后，将这些组合作为目标，通过<strong>轨迹优化</strong>生成动态可行的下体关节参考轨迹。优化问题被表述为一个多接触最优控制问题（MCOP），成本函数包括状态/控制正则化项、跟踪躯干朝向 <strong>ℒ_rpy</strong> 和高度 <strong>ℒ_h</strong> 的项，以及一个确保全身控制时平衡的质心（CoM）正则化项 <strong>ℒ_CoM</strong>。优化时假设双脚与地面接触，使用Crocoddyl中的BoxFDDP求解器。收集数据后，用一个三层MLP网络学习从躯干命令到下体参考姿态的映射，该模块在后续下体策略训练中被冻结。</p>
<p><strong>核心模块二：分层策略训练——下体策略</strong>。下体策略在IsaacGym中使用大规模并行仿真训练，目标是跟踪 <strong>g′</strong> 和 <strong>v</strong>。其观测 <strong>s_proprio</strong> 包含基座朝向/角速度、全身关节位置/速度/上一时刻动作、步态周期信号以及<strong>由AMO模块生成的参考下体关节角 <strong>q_lower^ref</strong>。策略采用</strong>师生蒸馏**框架：先训练一个教师策略，其额外观测了仿真中的特权信息（如真实的基座速度、躯干朝向、高度和脚部接触信号），使用PPO算法；然后将教师策略蒸馏到一个学生策略，学生策略仅使用真实世界中可获得的观测（包括25步历史本体感知以补偿缺失的特权信息），通过监督学习进行训练。最终部署的是学生策略。</p>
<p><strong>核心模块三：遥操作系统</strong>。如图3所示，操作者通过VR系统提供头、左腕、右腕三个末端执行器的目标姿态。一个<strong>多目标逆运动学（IK）</strong>求解器同时匹配这三个加权目标，计算出上体关节目标 <strong>q_upper</strong> 和中间目标 <strong>g′</strong>。<strong>g′</strong> 被输入到AMO模块转换为下体参考目标 <strong>q_lower^ref</strong>，与 <strong>v</strong> 一同输入给训练好的下体学生策略，生成最终的下体关节指令。</p>
<p><strong>创新点</strong>：1. <strong>混合运动合成</strong>：通过融合MoCap臂部轨迹与采样躯干姿态，系统性消除了训练分布中的运动学偏差，构建了首个专为灵巧移动操作设计的人形运动数据集。2. <strong>可泛化的连续映射</strong>：AMO网络学习从命令到运动的连续映射，而非离散查表，使其能处理连续输入和O.O.D.命令。3. <strong>分层与蒸馏设计</strong>：将全身控制解耦为上体指令生成和下体动态执行，并通过师生蒸馏将仿真中学习的复杂动态技能转移到仅依赖真实传感的策略上。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：方法在仿真和真实29自由度Unitree G1人形机器人上进行了验证。对比的基线方法是近期具有代表性的两项工作：HOVER（基于MoCap）和Opt2Skill（基于TO）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>工作空间扩展与稳定性</strong>：如图4所示，AMO实现了远超基线的工作空间（以末端执行器可达位置衡量），并保持了更高的稳定性（更低的基座角速度波动）。具体而言，AMO的稳定性误差比HOVER低69.2%，比Opt2Skill低79.3%。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03738v1/x4.png" alt="工作空间与稳定性对比"></p>
<blockquote>
<p><strong>图4</strong>：工作空间扩展与稳定性对比。左图显示AMO（红色）的末端执行器可达工作空间显著大于HOVER（蓝色）和Opt2Skill（绿色）。右图显示在执行极限躯干姿态时，AMO的基座角速度变化更小，表明更好的稳定性。</p>
</blockquote>
<ol start="2">
<li><strong>分布外（O.O.D.）命令跟踪</strong>：如图5所示，当给定训练分布中未见过的高度激进躯干命令（大俯仰、滚转、偏航组合）时，AMO能成功跟踪并保持平衡，而基线方法失败。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03738v1/x5.png" alt="O.O.D.命令跟踪"></p>
<blockquote>
<p><strong>图5</strong>：O.O.D.命令跟踪。AMO能够成功跟踪训练数据中未出现的、高度激进的躯干姿态组合（如图中“P+R+Y”所示），而基线方法失败。</p>
</blockquote>
<ol start="3">
<li><p><strong>真实机器人遥操作</strong>：图1展示了G1机器人使用AMO完成的一系列超灵巧移动操作任务，如从地面捡取物体放到不同高度的平台、从高架取物放到矮桌、伸展腿部将物体放到高架，以及大幅度的躯干俯仰、滚转、偏航和高度调整。</p>
</li>
<li><p><strong>自主任务执行</strong>：通过模仿学习（使用Transformer）从AMO支持的遥操作演示中学习，机器人能够自主完成“捡起地面上的罐子并放置到目标平台”的任务。如图6所示，自主策略的成功率达86.7%，接近遥操作水平（93.3%）。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03738v1/x6.png" alt="自主任务性能"></p>
<blockquote>
<p><strong>图6</strong>：自主放置任务性能。左图展示了自主策略执行任务的序列。右图表显示，经过模仿学习训练的自主策略成功率达到86.7%，接近人工遥操作的成功率（93.3%）。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ul>
<li><strong>AMO模块的作用</strong>：如图7所示，移除AMO模块（即下体策略不再接收 <strong>q_lower^ref</strong>）会导致跟踪躯干命令的性能大幅下降，尤其是在激进姿态下。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.03738v1/x7.png" alt="AMO模块消融"></p>
<blockquote>
<p><strong>图7</strong>：AMO模块消融实验。不使用AMO模块提供的参考时，策略跟踪激进躯干命令（如大滚转）的性能显著下降。</p>
</blockquote>
<ul>
<li><strong>历史观测与师生框架的作用</strong>：如图8所示，在学生策略中移除历史观测会导致性能下降；而直接训练学生策略（无教师蒸馏）则无法成功学习任务。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.03738v1/x8.png" alt="观测与训练框架消融"></p>
<blockquote>
<p><strong>图8</strong>：历史观测与师生框架消融。左图显示学生策略需要历史观测来补偿缺失的特权信息。右图显示没有教师策略引导，直接训练学生策略会失败。</p>
</blockquote>
<ul>
<li><strong>数据集混合策略的作用</strong>：如图9所示，与仅使用MoCap数据或仅使用TO数据相比，采用本文的混合策略（MoCap臂部+采样躯干）构建的AMO数据集训练出的策略，在跟踪激进O.O.D.命令时性能最佳。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.03738v1/x9.png" alt="数据集消融"></p>
<blockquote>
<p><strong>图9</strong>：数据集构建策略消融。使用混合策略（MoCap臂部+采样躯干）构建数据集训练的策略，在处理O.O.D.命令时性能优于仅使用MoCap或仅使用TO数据的方法。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖的自适应运动优化（AMO）框架，通过整合轨迹优化与强化学习，实现了对人形机器人躯干朝向和高度的实时、自适应控制，显著扩展了机器人工作空间，并首次展示了处理分布外遥操作命令的能力。</li>
<li>建立了一个人形超灵巧全身控制的新标杆，使机器人能够完成诸如从地面捡取物体等需要极大工作空间的任务。</li>
<li>进行了全面的仿真与实物实验，包括遥操作和自主任务执行，并通过系统的消融研究验证了各核心组件的必要性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在AMO数据集生成阶段，为了简化问题，<strong>没有考虑行走场景</strong>，假设机器人双脚始终与地面接触。这限制了方法在动态移动中的直接应用。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>动态运动集成</strong>：未来的工作可以将AMO与动态步态生成相结合，实现一边行走一边进行灵巧操作的全身体运动。</li>
<li><strong>更广泛的泛化</strong>：AMO学习连续映射以处理O.O.D.命令的思路，可激励研究如何让人形机器人策略更好地泛化到未见过的环境扰动或任务指令。</li>
<li><strong>数据驱动的全身控制</strong>：所提出的混合数据集构建方法为数据驱动的人形全身控制提供了新范式，可进一步探索如何自动化或优化数据生成过程。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出自适应运动优化框架AMO，解决高自由度人形机器人实现超灵巧全身实时控制的难题。核心方法整合模拟到现实强化学习与轨迹优化，通过构建混合数据集训练网络，以自适应处理分布外指令。在29自由度的Unitree G1真人机器人上验证，AMO展现出更优的稳定性与扩展的工作空间，并能通过模仿学习支持自主任务执行。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.03738" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>