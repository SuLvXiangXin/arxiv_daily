<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>WoMAP: World Models For Embodied Open-Vocabulary Object Localization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>WoMAP: World Models For Embodied Open-Vocabulary Object Localization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.01600" target="_blank" rel="noreferrer">2506.01600</a></span>
        <span>作者: Anirudha Majumdar Team</span>
        <span>日期: 2025-06-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，面向具身智能的语言指令主动物体定位任务存在两大主流方法。模仿学习（IL）方法需要大量专家演示数据，且难以泛化到演示数据集之外的新环境或任务。视觉语言模型（VLM）方法虽然能利用常识推理提供高级探索启发，但其生成的动作（如“向左看”）难以直接物理落地执行。强化学习（RL）方法则通常依赖于精确的仿真环境进行在线交互学习，成本高昂。</p>
<p>本文针对上述方法在泛化性、物理落地性和数据依赖性方面的关键痛点，提出了一种新视角：训练一个无需专家演示或在线交互的、基于世界模型（World Model）的主动感知策略。其核心思路是，利用高斯泼溅（Gaussian Splatting）构建真实到仿真再到真实的数据生成管道，从开放词汇物体检测器中蒸馏密集奖励信号来训练世界模型，并在推理时利用该世界模型对VLM生成的高级动作提议进行物理落地和优化。</p>
<h2 id="方法详解">方法详解</h2>
<p>WoMAP的整体框架包含三个核心组件：1) 可扩展的数据生成；2) 基于奖励监督的世界模型训练；3) 潜在空间的动作规划。</p>
<p><img src="https://arxiv.org/html/2506.01600v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：WoMAP的核心组件（左）与动作优化过程（右）。左侧展示了三个核心阶段：基于高斯泼溅的数据生成、基于物体检测奖励监督的世界模型训练、潜在空间动作规划。右侧展示了推理时的规划过程：给定当前观测和任务，VLM生成高级提议（绿色箭头），WoMAP在奖励梯度场中优化每个提议（红色箭头），并执行预测奖励最高的动作序列。</p>
</blockquote>
<p><strong>1. 可扩展数据生成</strong><br>为了摆脱对专家演示的依赖并获取足量覆盖动态的数据，WoMAP提出了一种基于高斯泼溅的“真实-仿真-真实”数据生成管道。首先，利用少量真实世界视频训练每个场景的高斯泼溅模型，从而构建一个逼真的仿真环境，可以渲染任意相机位姿的图像。其次，通过将CLIP的语言语义蒸馏到语义高斯泼溅中，自动标注场景中每个目标物体的位置和物理尺寸。最后，在该仿真环境中，通过规划算法（无需人类演示）生成从随机初始位置朝向目标物体的轨迹，并添加线性和角度扰动进行数据增强，收集由观测图像、奖励值和相机位姿组成的训练数据集。</p>
<p><img src="https://arxiv.org/html/2506.01600v1/x3.png" alt="数据生成流程"></p>
<blockquote>
<p><strong>图3</strong>：基于高斯泼溅的数据生成流程。为每个场景训练高斯泼溅，并通过语义标注获取物体真实位置以进行信息化的视角采样。每个观测图像使用GroundingDINO标注，获得所有训练目标的置信度分数作为奖励信号。</p>
</blockquote>
<p><strong>2. 世界模型架构与训练</strong><br>世界模型旨在潜在空间中预测动态和奖励，其架构包含三个标准组件（如图4所示）：</p>
<ul>
<li><strong>观测编码器</strong>：使用预训练的DINOv2视觉编码器，将高维观测图像映射为保留丰富视觉和空间特征的补丁嵌入潜在状态。</li>
<li><strong>动态预测器</strong>：采用标准ViT架构，将状态转移分布参数化为高斯分布，并通过最小化KL散度损失进行训练，以建模给定动作后的潜在状态转移。</li>
<li><strong>奖励预测器</strong>：估计每个潜在状态的奖励，条件于目标物体的语言嵌入。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.01600v1/x4.png" alt="世界模型架构"></p>
<blockquote>
<p><strong>图4</strong>：用于同时预测动态和奖励的世界模型架构。包含观测编码器、动态预测器和奖励预测器。</p>
</blockquote>
<p><strong>核心创新：奖励蒸馏</strong><br>为避免图像重建目标带来的训练不稳定和泛化差等问题，WoMAP引入了<strong>奖励蒸馏</strong>过程。在数据生成阶段（图3），使用预训练的开放词汇物体检测器（如GroundingDINO）为观测图像中的每个物体计算每帧奖励（检测置信度乘以检测框大小）。在训练时，将这些地面真值奖励信号蒸馏到奖励预测器中。这为世界模型提供了与任务高度相关的密集监督信号，且能随环境复杂度高效扩展。</p>
<p><strong>3. 基于世界模型的规划</strong><br>在推理时，WoMAP结合VLM的常识推理与世界模型的物理动态预测进行规划。首先，使用思维链提示VLM，从一个固定的文本动作描述集合（如“向左转”、“向前移动”）中生成高级动作提议。然后，WoMAP将这些提议作为初始值，在潜在空间中使用模型预测控制（MPC）进行优化，目标函数（公式2）旨在最大化规划期内预测奖励的期望和，并鼓励动作平滑。如图2右侧所示，WoMAP对多个VLM提议分别进行优化和奖励估计，最终执行预测奖励最高的动作序列。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在PyBullet（PB）仿真环境和真实世界（基于高斯泼溅渲染及TidyBot机器人）中进行。任务场景包括厨房、办公室等，通过改变物体配置、目标身份、遮挡和初始位姿来构建不同难度的物体定位任务。</p>
<p><strong>对比基线</strong>：</p>
<ol>
<li><strong>VLM规划器</strong>：使用GPT-4o，采用与WoMAP相同的提示模板生成动作。</li>
<li><strong>扩散策略</strong>：在多目标物体的专家演示上训练的多任务扩散策略。</li>
<li><strong>纯世界模型规划器</strong>：<ul>
<li><strong>WM-CEM</strong>：使用交叉熵方法进行无梯度动作采样。</li>
<li><strong>WM-Grad</strong>：基于梯度优化动作。</li>
<li><strong>WM-HR</strong>：使用与提示VLM相同的固定原子动作集进行启发式规划，并通过梯度下降优化，但不使用VLM输入。</li>
</ul>
</li>
</ol>
<p><strong>关键实验结果</strong>：<br>在仿真环境（PyBullet和高斯泼溅）的150个新场景任务上，WoMAP的平均性能显著优于基线。</p>
<p><img src="https://arxiv.org/html/2506.01600v1/x6.png" alt="仿真环境成功率对比"></p>
<blockquote>
<p><strong>图6</strong>：在PyBullet仿真环境中，不同方法在多种难度任务上的成功率。WoMAP（紫色）显著优于所有基线，平均成功率比VLM规划器高9倍以上，比扩散策略高2倍以上。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.01600v1/x7.png" alt="仿真环境效率对比"></p>
<blockquote>
<p><strong>图7</strong>：在PyBullet仿真环境中，考虑路径长度加权的效率指标对比。WoMAP同样表现最佳，表明其不仅能成功定位，而且探索路径更高效。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.01600v1/x5.png" alt="真实机器人轨迹可视化"></p>
<blockquote>
<p><strong>图5</strong>：TidyBot机器人在寻找被杯子遮挡的香蕉任务中各规划器的轨迹可视化。WoMAP能高效找到目标，而WM-Grad路径低效迂回，扩散策略则未能探查遮挡物后方。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文通过多个消融实验验证了核心组件的贡献。结果表明：1) 使用奖励蒸馏优于使用图像重建目标进行训练；2) 微调预训练的观测编码器（DINOv2）比冻结或从头训练效果更好；3) VLM的引导对于在复杂问题中获得良好初始动作提议至关重要，纯世界模型规划器（WM-CEM, WM-Grad, WM-HR）的性能随搜索方法的信息量增加而逐步提升，但均不及结合VLM的WoMAP。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种无需专家演示或在线交互的、基于高斯泼溅的可扩展数据生成管道，为训练世界模型创造了条件。</li>
<li>设计了基于开放词汇检测器奖励蒸馏的世界模型训练方法，避免了图像重建的不稳定性，实现了数据高效且任务相关的训练。</li>
<li>提出了一个新颖的推理框架，将VLM的高级常识推理与世界模型的物理动态预测相结合，通过MPC优化实现动作的物理落地。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，WoMAP的性能依赖于离线数据的覆盖范围和质量。此外，基于梯度的动作优化可能面临局部最优问题，且规划过程涉及多次前向预测，存在一定的计算成本。</p>
<p><strong>研究启示</strong>：<br>WoMAP的成功展示了结合基础模型（VLM）的语义理解与世界模型的物理动态预测，是实现高效、可泛化具身探索的有效途径。该方法为在数据有限的情况下训练具身智能体提供了新思路，后续研究可探索更高效的数据生成技术、更鲁棒的规划算法，以及将该框架扩展到更复杂的多模态指令和长时程任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出WoMAP方法，解决机器人根据语言指令在未知环境中高效定位开放词汇物体的核心问题。关键技术包括：基于高斯泼溅的真实-仿真-真实数据生成管道，无需专家演示；从开放词汇检测器提取密集奖励信号；利用潜在世界模型预测动态与奖励，以落地高级动作提议。实验表明，WoMAP在零样本物体定位任务中显著优于基线，相比VLM方法成功率提升9倍以上，相比扩散策略提升2倍以上，并展示了优秀的泛化与仿真到真实迁移能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.01600" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>