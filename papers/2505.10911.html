<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.10911" target="_blank" rel="noreferrer">2505.10911</a></span>
        <span>作者: Zhang, Jiahui, Luo, Yusen, Anwar, Abrar, Sontakke, Sumedh Anand, Lim, Joseph J, Thomason, Jesse, Biyik, Erdem, Zhang, Jesse</span>
        <span>日期: 2025/05/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人学习新任务的主流方法是大规模模仿学习和强化学习。模仿学习需要为每个新任务收集大量专家演示，成本高昂。强化学习虽然更自主，但依赖于为每个任务手动设计奖励函数，这需要大量的人工努力和领域专业知识。最近的语言条件奖励学习试图解决这些问题，但往往依赖于不现实的假设，例如需要真实状态信息、数千个演示或从头开始的在线奖励模型训练，限制了其实用性。</p>
<p>本文针对“如何仅利用少量演示，就能让机器人高效学习大量未见任务变体”这一痛点，提出了一种新的视角：不再为每个新任务收集演示或设计奖励，而是从一个小的演示数据集中学习一个通用的、语言条件的奖励函数，并用它来预训练和微调策略。核心思路是：首先从少量演示中学习一个数据高效且泛化性强的语言条件奖励函数，然后利用离线强化学习预训练一个语言条件策略，最后在面对新任务时，仅需语言指令和在线交互，即可利用学得的奖励函数高效微调策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>ReWiND框架包含三个顺序阶段，其整体流程如下图所示。</p>
<p><img src="https://arxiv.org/html/2505.10911v2/x1.png" alt="方法整体流程"></p>
<blockquote>
<p><strong>图2</strong>：ReWiND框架总览。(a) 奖励模型训练阶段：利用小规模演示数据集 $\mathcal{D}<em>{\text{demos}}$ 和精选的Open-X数据集 $\mathcal{D}</em>{\text{open-x}}$，通过视频倒带和指令生成进行增强，训练一个预测视频进度的语言条件奖励函数 $R_{\psi}(o_{1:t}, z)$。(b) 策略预训练阶段：使用训练好的奖励模型为 $\mathcal{D}<em>{\text{demos}}$ 中的演示标注奖励，并通过离线强化学习（IQL）预训练语言条件策略 $\pi</em>{\theta}(a_t \mid o_t, z)$。(c) 在线学习阶段：给定一个描述新任务的语言指令 $z_{\text{new}}$，通过在线交互收集轨迹，并使用 $R_{\psi}(o_{1:t}, z_{\text{new}})$ 提供奖励信号来微调预训练的策略。</p>
</blockquote>
<p><strong>核心模块一：奖励函数学习</strong>。目标是训练一个奖励函数 $R_{\psi}(o_{1:t}, z)$，使其具备三个关键属性：对未见任务的<strong>泛化性</strong>、与策略 rollout 的<strong>对齐性</strong>以及对输入变化的<strong>鲁棒性</strong>。其技术细节围绕这三个目标展开：</p>
<ol>
<li><p><strong>训练目标与损失函数</strong>：核心是回归视频序列 $o_{1:T}$ 在指令 $z$ 下的逐帧进度（$t/T$），这提供了稳定、密集且归一化的奖励信号。损失函数 $\mathcal{L}_{\text{progress}}$ 包含两项：一是对匹配的（视频，指令）对回归进度；二是对不匹配的对预测零进度。</p>
</li>
<li><p><strong>数据增强与多样化数据（针对对齐性与鲁棒性）</strong>：</p>
<ul>
<li><strong>视频倒带</strong>：为了解决演示数据全是成功轨迹、奖励模型可能对失败轨迹过拟合的问题，ReWiND提出了可扩展的“视频倒带”增强。如图3所示，从成功演示中随机选择分割点并回放若干帧，生成类似“尝试后失败”的序列，并为其分配反向的进度标签。这使得奖励模型学会在策略执行错误时降低奖励。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.10911v2/x2.png" alt="视频倒带示意图"></p>
<blockquote>
<p><strong>图3</strong>：视频倒带示意图。将演示在中间时间步 $i$ 分割，前向部分显示机器人接近杯子；反向部分（$o_{i-1}, o_{i-2}, \ldots$）则模拟了掉落杯子的过程。</p>
</blockquote>
<ul>
<li><strong>指令生成</strong>：使用大语言模型为 $\mathcal{D}_{\text{demos}}$ 中的每个任务生成5-10个额外的语言指令，增强奖励模型对指令表述变化的鲁棒性。</li>
<li><strong>融入多样化数据</strong>：引入精选的Open-X数据集 $\mathcal{D}_{\text{open-x}}$，其中包含大量具有物体中心和方向性指令的轨迹，以促进模型对未见物体和动作的泛化能力。</li>
</ul>
</li>
<li><p><strong>模型架构（针对泛化性）</strong>：由于 $\mathcal{D}_{\text{demos}}$ 数据量小，采用冻结的预训练编码器（DINOv2用于图像，all-MiniLM-L12-v2用于语言）提取特征，仅训练一个轻量级的跨模态序列聚合Transformer。为防止模型仅通过位置嵌入“作弊”预测进度，<strong>创新性地只对第一帧图像添加位置嵌入</strong>，迫使模型理解视频内容。</p>
</li>
<li><p><strong>最终目标</strong>：奖励模型的最终训练目标是最小化 $\mathcal{L}<em>{\text{progress}}$ 和 $\mathcal{L}</em>{\text{rewind}}$ 的期望。</p>
</li>
</ol>
<p><strong>核心模块二：策略学习</strong>。分为两个阶段：</p>
<ol>
<li><strong>预训练</strong>：使用训练好的奖励模型 $R_{\psi}$ 为 $\mathcal{D}<em>{\text{demos}}$ 中的演示标注密集奖励 $\hat{r}<em>t$，并在最终时间步添加一个成功奖励 $r</em>{\text{success}}$ 以鼓励达成目标。然后，利用这些（状态，动作，奖励，下一状态，指令）元组，通过离线强化学习算法IQL预训练语言条件策略 $\pi</em>{\theta}$。</li>
<li><strong>在线学习</strong>：对于由 $z_{\text{new}}$ 指定的新任务，部署预训练策略进行在线交互。使用 $R_{\psi}(o_{1:t}, z_{\text{new}})$ 为在线轨迹提供密集奖励，并在成功时添加 $r_{\text{success}}$ 奖励，从而微调策略。</li>
</ol>
<p><strong>创新点总结</strong>：与现有方法相比，ReWiND的创新具体体现在：1) 提出了<strong>视频倒带</strong>这一可扩展的数据增强技术，使奖励模型能有效处理失败轨迹；2) 结合了<strong>指令生成</strong>和<strong>开放领域数据</strong>来提升泛化与鲁棒性；3) 采用了<strong>冻结预训练编码器</strong>和<strong>仅首帧位置编码</strong>的轻量级架构，在数据有限的情况下优化泛化性能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要使用模拟环境MetaWorld benchmark进行评测。$\mathcal{D}<em>{\text{demos}}$ 包含20个任务，每个任务5个专家演示。评估在17个未见任务上进行。对比的基线奖励模型包括：LIV、LIV-FT（在 $\mathcal{D}</em>{\text{demos}}$ 上微调）、RoboCLIP、VLC和GVL。策略学习实验在8个未见任务上进行，对比基线包括VLC、LIV-FT、仅使用稀疏成功奖励的Sparse以及零-shot评估的Pre-train。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>奖励模型评估</strong>：从泛化性、策略对齐性和输入鲁棒性三个维度评估。</p>
<ul>
<li><strong>泛化性</strong>：通过计算未见任务演示视频与语言指令配对的奖励混淆矩阵来评估。如图4所示，ReWiND产生的矩阵对角线元素最突出，表明其能最好地区分正确的视频-指令对。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.10911v2/x3.png" alt="奖励混淆矩阵"></p>
<blockquote>
<p><strong>图4</strong>：视频-语言奖励混淆矩阵。ReWiND产生了最明显的对角线-heavy矩阵，表明其对未见演示和指令具有最强的对齐能力。</p>
</blockquote>
<ul>
<li><strong>策略对齐性</strong>：评估奖励模型对失败、接近成功和成功策略rollout视频的奖励排序能力。如表1(b)所示，ReWiND在奖励排序相关性($\rho$)和奖励差异上均显著优于基线，相对最佳基线(LIV-FT)分别有74%和58%的提升。</li>
<li><strong>输入鲁棒性</strong>：评估奖励模型对同一任务不同语言指令的响应一致性。如表1(c)所示，ReWiND取得了最高的平均排名相关性(0.74)，比次优的VLC高23%，同时保持了接近零的输出方差，表明其鲁棒性最强。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.10911v2/x5.png" alt="综合评估指标表"></p>
<blockquote>
<p><strong>表1</strong>：综合评估指标。比较各奖励模型在(a)演示奖励对齐、(b)策略rollout奖励排序和(c)输入鲁棒性三个方面的表现。ReWiND（带OXE）在绝大多数指标上领先。</p>
</blockquote>
</li>
<li><p><strong>策略学习性能</strong>：在MetaWorld的8个未见任务上在线微调100k步后的成功率。如图5所示，ReWiND取得了79%的IQM（四分位间均值）成功率，**比最佳基线VLC提高了97.5%**。仅使用稀疏奖励的方法效果接近零。</p>
<p><img src="https://arxiv.org/html/2505.10911v2/x4.png" alt="MetaWorld最终性能"></p>
<blockquote>
<p><strong>图5</strong>：MetaWorld最终性能。绘制了在8个未见任务上经过100k环境步后的成功率IQM。ReWiND达到79%的成功率，显著优于所有基线。</p>
</blockquote>
</li>
<li><p><strong>消融实验分析</strong>：论文通过消融实验验证了各个组件的贡献。关键结论包括：<strong>视频倒带</strong>对正确奖励失败策略rollout至关重要；<strong>Open-X数据</strong> ($\mathcal{D}_{\text{open-x}}$) 显著提升了奖励的泛化能力和策略学习性能；<strong>仅首帧位置编码</strong>的架构设计比标准位置编码更能促进泛化；<strong>指令生成</strong>有助于提高对语言变化的鲁棒性。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了ReWiND框架，仅需少量演示即可学习一个具备<strong>泛化性、策略对齐性和输入鲁棒性</strong>的语言条件奖励模型，从而无需为新任务收集额外演示。2) 引入了<strong>视频倒带</strong>、<strong>指令生成</strong>、结合<strong>开放数据</strong>以及特定的<strong>模型架构设计</strong>等一系列技术，有效实现了上述奖励模型属性。3) 在模拟和真实世界实验中证明了该框架能够实现<strong>样本高效</strong>的新任务策略学习，性能大幅超越现有基线。</p>
<p><strong>局限性</strong>：论文提到，在线学习阶段仍假设可以获取成功信号（用于添加成功奖励$r_{\text{success}}$），这通常需要人工监督或额外的成功检测器。此外，实验需要进行人工重置。</p>
<p><strong>对后续研究的启示</strong>：ReWiND展示了从有限演示中学习通用奖励函数的可行性。未来的工作可以探索如何进一步减少对人类监督的依赖，例如，集成自动成功检测方法或减少环境重置的需求，以实现更完全自主的强化学习。此外，如何将框架扩展到更复杂的多阶段任务或动态环境中，也是一个值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ReWiND框架，解决机器人学习新任务时依赖大量人工奖励设计或每任务演示样本的问题。方法核心包括：1）利用少量演示数据学习一个数据高效、语言条件的奖励函数；2）基于该奖励函数，通过离线强化学习预训练一个语言条件策略。面对新任务变体时，仅需使用学得的奖励函数对策略进行少量在线微调。实验表明，其奖励模型泛化能力强，在奖励泛化与策略对齐指标上最高超越基线2.4倍，对新任务的样本效率在仿真中提升2倍，在真实世界双臂策略上提升5倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.10911" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>