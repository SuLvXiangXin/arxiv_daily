<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.21970" target="_blank" rel="noreferrer">2512.21970</a></span>
        <span>作者: He Wang Team</span>
        <span>日期: 2025-12-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过端到端框架将视觉输入和语言指令映射为动作，展现出强大的泛化和语义理解能力。然而，为了与预训练的视觉语言模型对齐，主流的VLA模型依赖于单视角RGB图像作为视觉输入，这限制了对于精确操作至关重要的几何感知能力。现有工作主要通过补充其他传感器来增强几何感知，例如腕部相机、深度传感器或额外的第三人称相机，但这些方案分别存在视野受限易遮挡、对透明/反光物体测量噪声大、硬件设计复杂且视角多样性阻碍泛化等问题。</p>
<p>本文针对VLA模型缺乏精确几何感知这一核心痛点，提出了一个受人类视觉启发的全新视角：利用立体视觉。立体相机模拟人类双眼视觉，通过双目视差提供鲁棒的空间线索，且无需增加额外硬件。尽管立体视觉在计算机视觉领域已有成熟基础模型，但其在VLA模型中的集成与系统性评估尚未得到充分探索。本文的核心思路是：通过一个新颖的几何-语义特征提取模块，从立体图像中融合密集的几何特征和丰富的语义特征，并引入一个交互区域深度估计的辅助任务，以增强模型对细粒度空间细节的感知，从而提升VLA模型在精确操作任务中的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>StereoVLA的整体框架如图2(a)所示。输入为立体图像对和语言指令。首先，几何-语义特征提取模块将立体图像编码为兼具几何精度和语义丰富度的视觉token。这些视觉token与语言token一同输入预训练的大语言模型主干进行联合处理。利用模型中间产生的视觉-语言特征，一个动作专家网络通过流匹配策略预测以末端执行器位姿增量表示的动作块。在训练阶段，引入一个交互区域深度估计的辅助任务以增强几何学习，该任务在推理阶段不产生额外开销。</p>
<p><img src="https://arxiv.org/html/2512.21970v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：(a) StereoVLA整体架构。立体图像对被几何-语义特征提取模块编码为视觉token，与语言token一起输入大语言模型主干。动作专家预测末端执行器位姿增量，辅助深度估计任务在训练中增强几何学习。(b) 几何-语义特征提取模块细节。利用FoundationStereo提取几何特征，利用SigLIP和DINOv2从左视图提取语义特征，然后通过MLP投影器融合为统一的视觉表示。</p>
</blockquote>
<p><strong>核心模块一：几何-语义特征提取</strong><br>该模块旨在从立体图像中提取并融合几何和语义特征，具体流程见图2(b)。</p>
<ol>
<li><strong>几何特征提取</strong>：使用专为深度估计设计的FoundationStereo模型。给定立体图像对，模型首先生成单目特征，然后构建一个4D代价体积。为了建模长程关联，该代价体积会经过一个基于注意力的混合代价滤波模块，得到滤波后的代价体积。论文选择这个<strong>滤波后的代价体积</strong>作为几何特征源，因为它包含了经过长程关联增强的密集几何信息，同时避免了后续迭代细化带来的巨大计算开销。</li>
<li><strong>语义特征提取</strong>：由于FoundationStereo缺乏语义信息，需要从图像中补充。论文遵循PrismaticVLM，使用SigLIP和DINOv2从左视图提取语义丰富的特征。SigLIP擅长捕获高级语义，DINOv2擅长捕获视觉细节。仅使用左视图是出于计算效率考虑，因为左右视图的语义信息高度冗余。</li>
<li><strong>特征融合</strong>：将FoundationStereo的几何特征图通过空间池化调整到与SigLIP/DINOv2特征相同的空间步长，然后将三者沿通道维度拼接，最后通过一个MLP投影器生成最终的混合视觉token序列。论文指出，采用通道拼接而非token序列拼接，可以避免视觉token数量翻倍，从而减少训练和推理的计算开销。</li>
</ol>
<p><strong>核心模块二：交互区域深度估计</strong><br>这是一个与动作预测协同训练的辅助任务，旨在增强模型对操作关键区域细粒度几何信息的捕获能力。具体而言，模型被训练来预测图像中给定采样点的度量深度。关键创新在于采样策略：不是在全图均匀采样（容易采到无信息的背景），而是将采样点<strong>限制在交互区域内</strong>，即包含夹爪和目标物体的边界框区域。这种聚焦采样促使模型学习物体几何及其与夹爪的空间关系，有益于提升操作精度和训练收敛速度。</p>
<p><strong>与现有方法的创新点</strong></p>
<ol>
<li><strong>针对立体视觉的专门设计</strong>：不同于将立体图像对简单视为两个独立视图输入现有多相机VLA模型，本文设计了专门的几何-语义特征提取模块，有效处理了立体视图间的细微差异，并融合了来自不同基础模型的优势特征。</li>
<li><strong>高效的特征选择与融合</strong>：选择了FoundationStereo中信息最丰富的滤波后代价体积作为几何特征，并采用通道拼接的融合方式，在保证性能的同时优化了计算效率。</li>
<li><strong>任务驱动的辅助学习</strong>：提出的交互区域深度估计任务，通过聚焦于操作关键区域，更高效地引导模型学习对操作至关重要的空间细节。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在真实机器人实验和模拟环境中进行。使用了包含通用任务（拾放、堆叠）、抓取不同朝向的条形物体、抓取中小型物体的综合任务套件。实验平台配置了Franka机器人和Zed Mini立体相机，并系统性地设置了前视和侧视相机位姿的小、中、大随机化范围以测试鲁棒性。</p>
<p><strong>对比基线</strong>：由于没有现成的面向立体设置的VLA，论文对多个SOTA开源VLA模型使用完整的合成立体数据集进行了训练/微调以进行公平比较。包括：SpatialVLA及其使用立体深度估计的变体SpatialVLA-D、适应立体输入的π0.5-S和GraspVLA-S。</p>
<p><img src="https://arxiv.org/html/2512.21970v1/x3.png" alt="实验结果总览"></p>
<blockquote>
<p><strong>图3</strong>：真实世界评估任务及结果。StereoVLA在所有任务类型上均取得了最高的成功率，尤其在抓取条形物体和小物体等需要高精度的任务上优势明显。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>整体性能</strong>：如图3所示，StereoVLA在所有评估任务上取得了最强且最一致的性能。在通用任务上，其成功率比基线平均高出33%。对于0°、45°、90°朝向的条形物体抓取，StereoVLA取得了近乎完美或完美的结果。在最具挑战性的小物体抓取任务上，StereoVLA取得了30.0%的成功率，而所有基线模型均完全失败。</li>
<li><strong>相机设置对比</strong>：如表II所示，在相机位姿小范围随机化下，前+侧视配置的GraspVLA性能最佳（82.5%），StereoVLA紧随其后（79.3%）。但随着位姿随机化范围增大，前+侧视配置性能急剧下降，而StereoVLA的下降最为平缓，在中等和大幅度随机化下均取得了最高成功率（71.9%和61.3%），展现了其对相机位姿变化最强的鲁棒性。</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>特征选择</strong>：如表I所示，使用滤波后代价体积V_c‘作为几何特征取得了最高成功率（77.0%），优于原始代价体积V_c（69.0%）和相关体积V_corr（54.0%）。同时，融合语义特征相比仅使用几何特征，成功率有显著提升，验证了语义信息对于视觉-语言对齐的必要性。<br><img src="https://arxiv.org/html/2512.21970v1/fig/feature_fusion_results.png" alt="特征融合消融"><blockquote>
<p><strong>图5</strong>：特征融合策略对比。序列拼接（Sequence Concat）导致成功率略有下降且需要更长的训练时间，而通道拼接（Channel Concat）在性能和效率上更优。</p>
</blockquote>
</li>
<li><strong>特征融合策略</strong>：如图5所示，通道拼接特征融合相比序列拼接，取得了更高的成功率且训练时间更短。<br><img src="https://arxiv.org/html/2512.21970v1/fig/depth-est-results.png" alt="深度估计消融"><blockquote>
<p><strong>图6</strong>：深度估计方法对比。在交互区域（Interaction Region）进行深度估计取得了最高的成功率，优于在全图（Full Image）或仅物体区域（Object Region）采样。</p>
</blockquote>
</li>
<li><strong>深度估计任务</strong>：如图6所示，在交互区域进行深度估计取得了最高的成功率，验证了该设计的有效性。</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个系统性利用立体视觉增强几何感知的VLA模型StereoVLA。</li>
<li>设计了新颖的几何-语义特征提取模块，能够从立体图像中高效提取并融合密集几何特征和丰富语义特征。</li>
<li>引入了交互区域深度估计这一辅助任务，有效提升了模型对操作关键区域细粒度空间关系的理解能力。</li>
</ol>
<p><strong>局限性</strong>：论文指出，当前图像分辨率（224x224）限制了对小物体（仅占几个像素）的感知和定位精度。虽然使用更高分辨率的视觉骨干网络可能缓解此问题，但会带来巨大的计算成本增加。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>立体视觉在VLA中的潜力</strong>：本研究证明了立体视觉在提供鲁棒几何线索、简化硬件部署以及增强对视角变化鲁棒性方面的显著优势，为VLA的感知模块设计提供了新方向。</li>
<li><strong>多模态特征融合</strong>：如何更有效地融合来自不同预训练基础模型的异构特征（如几何专用模型和视觉语言模型），是一个具有普遍意义的研究课题。</li>
<li><strong>任务驱动的表征学习</strong>：交互区域深度估计的成功表明，设计紧密围绕下游任务需求的辅助学习目标，可以更高效地引导模型学习关键表征。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文StereoVLA针对现有视觉-语言-动作模型因依赖单视图RGB而几何感知不足的问题，提出利用立体视觉增强空间感知。核心方法包括Geometric-Semantic特征提取模块，融合立体差异的几何特征和单眼语义特征，以及辅助交互区域深度估计任务以加速收敛。实验表明，该方法在立体设置下多种任务中大幅优于基线模型，并对相机姿态变化表现出强鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.21970" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>