<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Skill-Aware Diffusion for Generalizable Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Skill-Aware Diffusion for Generalizable Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.11266" target="_blank" rel="noreferrer">2601.11266</a></span>
        <span>作者: Wei Zhang Team</span>
        <span>日期: 2026-01-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作的鲁棒泛化能力对于机器人灵活适应多样化环境至关重要。现有方法主要通过扩展数据和网络规模来提升泛化能力，但通常将任务独立处理，忽视了技能层面的信息。主流方法主要分为两个方向：一是端到端模仿学习框架，通过大规模预训练增强视觉表征，但提取的特征包含大量与任务无关的细节（如背景），且依赖海量数据；二是两阶段框架，先预测任务相关的运动表征（如场景流），再将其映射为动作，这类方法在将预测的2D运动表征转化为可执行的3D动作时，难以在鲁棒性和精度间取得平衡，同样依赖大规模数据。这些方法往往孤立地处理任务，忽略了同一技能域内不同任务所共享的相似运动模式。</p>
<p>本文针对现有方法过度依赖大规模数据、忽视任务间共享先验知识的关键痛点，提出了利用技能层级信息作为额外指导的新视角。核心思路是：通过一个技能感知扩散（SADiff）框架，在编码、生成和执行三个阶段系统性地融入技能特定信息，从而在无需大规模数据的情况下，实现跨任务和环境的优越泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>SADiff框架将机器人操作问题分解为两个子问题：1) 根据图像观察和语言指令生成描述目标物体像素级运动的密集2D运动流；2) 将2D运动流提升为3D轨迹并转换为可执行的动作序列。整个流程分为编码、生成和执行三个阶段。</p>
<p><img src="https://arxiv.org/html/2601.11266v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：提出的技能感知扩散（SADiff）框架概览。框架分为三个阶段：（1）编码阶段，技能感知编码模块使用可学习的技能令牌与多模态输入交互，提取技能特定信息；（2）生成阶段，技能约束扩散模型以技能感知令牌序列为条件，生成以物体为中心的运动流，并通过去噪损失和两个技能特定辅助损失进行优化；（3）执行阶段，采用技能检索转换策略，利用技能特定先验知识将生成的2D运动流转换为可执行的3D轨迹。</p>
</blockquote>
<p><strong>核心模块1：技能感知编码模块</strong><br>该模块旨在从多模态输入中动态提取并融合技能特定信息。首先，利用Qwen-VL模型根据语言指令识别并定位相关物体，获取其边界框。同时，引入一组随机初始化的可学习技能令牌。通过一个基于MLP的技能分类器，根据图像和语言指令选择最相关的技能令牌。随后，图像、语言、边界框和选定技能令牌被分别编码为初始令牌序列，并通过多头自注意力（MHSA）和多头交叉注意力（MHCA）机制进行交互。技能令牌作为查询，其他模态的令牌作为键和值，经过交叉注意力和前馈网络（FFN）后，生成最终用于条件扩散模型的技能感知令牌序列。该设计实现了跨模态技能特征的动态对齐与融合。</p>
<p><img src="https://arxiv.org/html/2601.11266v1/x2.png" alt="编码模块架构"></p>
<blockquote>
<p><strong>图2</strong>：技能感知编码模块的架构。该模块通过基于注意力的交互，将图像、语言、相关物体边界框与可学习技能令牌集成，产生技能感知令牌序列。</p>
</blockquote>
<p><strong>核心模块2：技能约束运动流生成</strong><br>该模块采用一个以技能感知令牌序列为条件的扩散模型来生成精确的2D物体运动流。首先，使用预训练的VAE编码器将真实运动流编码到潜在空间，并经过前向扩散过程添加噪声。一个UNet结构的噪声预测网络以前述技能感知令牌序列为条件，预测噪声。UNet中集成了MHSA层作为运动模块，以捕获帧间的时间依赖性。训练时，模型通过三个损失联合优化：1) <strong>去噪损失（MSE）</strong>：确保运动流生成的空间精度；2) <strong>技能分类损失（交叉熵）</strong>：监督技能分类器正确选择技能令牌；3) <strong>技能对比损失</strong>：鼓励扩散模型中间层特征与预定义技能文本提示的语义表征对齐，从而区分不同技能的运动模式。总损失是这三项的加权和。</p>
<p><img src="https://arxiv.org/html/2601.11266v1/x3.png" alt="扩散模型概述"></p>
<blockquote>
<p><strong>图3</strong>：技能约束扩散模型概述。该模型通过联合优化技能分类损失、技能对比损失和去噪损失，确保准确的技能选择、语义对齐和精确的运动流生成。</p>
</blockquote>
<p><strong>核心模块3：技能检索转换策略</strong><br>此模块负责将预测的2D运动流映射为可执行的3D动作序列。传统方法基于几何优化，通过最小化重投影误差来估计物体从初始帧到后续每一帧的刚性变换矩阵（旋转和平移），进而推导末端执行器轨迹。SADiff在此基础上提出了<strong>技能检索转换策略</strong>：对于当前任务，从训练数据中检索属于同一技能类别的所有演示，计算其3D轨迹的均值作为技能特定先验轨迹。在几何优化过程中，将此先验轨迹作为正则项加入优化目标，引导生成的3D轨迹符合该技能的典型运动模式，从而提高映射的精度和一致性，且无需额外训练。</p>
<p><strong>创新点总结</strong><br>与现有方法相比，SADiff的核心创新在于：1) <strong>显式建模技能信息</strong>：通过可学习技能令牌和注意力机制，在编码阶段动态捕获并融合技能特定表征；2) <strong>技能约束的扩散生成</strong>：在扩散模型训练中引入技能分类和技能对比损失，确保生成的运动流在语义和模式上与目标技能对齐；3) <strong>利用技能先验优化执行</strong>：在2D到3D的转换阶段，利用检索到的技能特定轨迹先验作为正则化，提升了动作生成的精度和鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在仿真和真实世界环境中进行了评估。主要使用了两个基准：1) <strong>RLBench</strong>：一个包含多种任务的仿真基准。2) <strong>IsaacSkill</strong>：作者新构建的高保真度数据集，基于NVIDIA Isaac Lab平台，涵盖了应用于各种任务的基本机器人技能（如倾倒、拾放、推动等），旨在支持以技能为中心的评估和零样本仿真到真实的迁移。<br><strong>对比方法</strong>：包括端到端模仿学习方法（如ACT、Diffusion Policy）、两阶段流生成方法（如Im2Flow2Act、Track2Act）以及视频预测方法（如UniPi）。<br><strong>评估指标</strong>：任务成功率。</p>
<p><img src="https://arxiv.org/html/2601.11266v1/x4.png" alt="仿真结果对比"></p>
<blockquote>
<p><strong>图4</strong>：在RLBench基准上的任务成功率对比。SADiff在大多数任务上取得了最佳或极具竞争力的性能，尤其在需要精细操作的任务（如<code>OpenGrill</code>）上优势明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.11266v1/x5.png" alt="跨技能泛化"></p>
<blockquote>
<p><strong>图5</strong>：在IsaacSkill数据集上，模型在训练中未见过的<strong>新物体</strong>上的跨技能泛化性能。SADiff在所有技能类别上均显著优于基线方法Im2Flow2Act，展示了其强大的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.11266v1/x6.png" alt="跨环境泛化"></p>
<blockquote>
<p><strong>图6</strong>：在IsaacSkill数据集上，模型在训练中未见过的<strong>新环境</strong>（不同背景、光照、布局）上的泛化性能。SADiff同样表现出更优的鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.11266v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融研究结果，展示了SADiff各个核心组件的贡献。移除技能感知编码（w/o SAE）、技能对比损失（w/o SCL）或技能检索转换（w/o SRT）均会导致性能下降，验证了每个组件的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.11266v1/x8.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图8</strong>：真实世界机器人实验的定性结果。SADiff成功完成了倾倒、拾放、开关抽屉等多种技能任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.11266v1/x9.png" alt="真实世界定量结果"></p>
<blockquote>
<p><strong>图9</strong>：真实世界实验的定量成功率。SADiff在零样本仿真到真实的迁移中取得了高成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.11266v1/x10.png" alt="生成流可视化"></p>
<blockquote>
<p><strong>图10</strong>：生成的2D物体运动流（橙色箭头）与真实流（绿色箭头）的对比可视化。SADiff生成的流与真实流高度一致，且能清晰反映不同技能（如倾倒的弧形轨迹与推动的直线轨迹）的特征模式。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.11266v1/x11.png" alt="技能检索可视化"></p>
<blockquote>
<p><strong>图11</strong>：技能检索转换策略的图示。对于“倾倒”技能，从训练数据中检索到的先验轨迹（蓝色）引导优化过程，使最终估计的物体3D轨迹（红色）更符合该技能的典型运动模式，相比无先验的几何优化（绿色）更准确。</p>
</blockquote>
<p><strong>关键结果总结</strong>：</p>
<ol>
<li><strong>性能领先</strong>：在RLBench基准测试中，SADiff在10个任务上的平均成功率为73.3%，显著高于Im2Flow2Act（62.7%）、Diffusion Policy（52.0%）和ACT（46.7%）等基线方法。</li>
<li><strong>卓越泛化</strong>：在IsaacSkill数据集上，面对新物体，SADiff将平均成功率从Im2Flow2Act的55.8%提升至78.3%；面对新环境，从53.3%提升至75.0%。</li>
<li><strong>有效迁移</strong>：在真实世界实验中，SADiff实现了零样本迁移，在6个任务上平均成功率达86.7%。</li>
<li><strong>组件贡献</strong>：消融实验证实了各核心组件的必要性，其中技能检索转换策略（SRT）带来的性能提升最为显著。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>技能感知扩散（SADiff）框架</strong>，首次在机器人操作的编码、生成和执行全流程中系统性地显式建模并集成技能层级信息，从而在不依赖大规模数据的情况下显著提升了模型的泛化能力。</li>
<li>设计了<strong>技能约束扩散模型</strong>及配套的<strong>技能检索转换策略</strong>，前者通过辅助损失确保运动流生成的语义对齐与模式区分，后者利用技能先验优化2D到3D的映射，共同提高了动作生成的精度和鲁棒性。</li>
<li>构建了<strong>IsaacSkill数据集</strong>，一个专注于基本机器人技能评估的高保真仿真数据集，为技能为中心的研宄提供了新的基准，并支持有效的仿真到真实迁移。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提及的局限性包括：1) 技能的定义和分类依赖于先验知识，如何自动发现和定义技能是一个开放问题；2) 方法需要离线的演示数据来提取运动流和技能先验；3) 当前框架主要处理刚性物体的操作，对可变形物体的泛化能力有待探索。</p>
<p><strong>后续启示</strong>：<br>本研究为机器人学习领域提供了重要启示：超越孤立的任务学习，挖掘和利用任务间隐含的共享结构（如技能）是实现数据高效泛化的有效途径。后续工作可以沿着以下方向展开：探索无监督或自监督的技能发现机制；将技能感知框架扩展到更复杂的非刚性物体操作或动态环境中；研究如何将技能信息与更高层次的规划相结合，实现更复杂的多技能组合任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中因忽略技能层面信息导致的泛化能力受限问题，提出Skill-Aware Diffusion（SADiff）方法。该方法通过技能感知编码模块学习技能特定表示，并利用技能约束扩散模型生成以物体为中心的运动流；进一步通过技能检索转换策略，利用轨迹先验将2D运动流细化为可执行3D动作。实验在模拟与真实环境中进行，结果表明SADiff在各种操作任务中实现了良好的性能与泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.11266" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>