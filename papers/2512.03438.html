<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Multimodal Reinforcement Learning with Agentic Verifier for AI Agents - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Multimodal Reinforcement Learning with Agentic Verifier for AI Agents</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.03438" target="_blank" rel="noreferrer">2512.03438</a></span>
        <span>作者: Jianfeng Gao Team</span>
        <span>日期: 2025-12-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，使用多模态强化学习（MMRL）训练的智能推理模型能力日益增强，但几乎普遍采用基于最终答案计算的稀疏结果奖励进行优化。基于推理过程生成的令牌计算更丰富的奖励，可以通过提供更细粒度的指导来显著改善学习效果。然而，在多模态强化学习中，超越结果奖励计算更具信息量的奖励面临挑战，因为不同样本可能需要不同的评分函数，且教师模型可能提供噪声信号。本文针对这一痛点，提出了Argos（Agentic Reward for Grounded &amp; Objective Scoring）——一个用于训练智能任务多模态推理模型的原则性奖励智能体。其核心思路是：设计一个能根据每个训练样本自适应选择教师模型和基于规则的评分函数的智能验证器，以同时评估最终答案准确性、时空定位和推理过程质量，从而提供聚合的、可验证的奖励信号。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架如图1所示，Argos智能验证器在RL阶段根据训练样本自适应选择不同的评分工具，用于训练智能基础模型，最终在具身任务规划、空间推理等多个智能基准上进行评估。</p>
<p><img src="https://arxiv.org/html/2512.03438v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：使用Argos智能验证器进行多模态强化学习的整体框架。左侧展示了在RL训练阶段，Argos根据样本自适应选择评分工具来评估模型响应；右侧展示了在多个智能基准（如具身任务规划与完成、空间推理）上对最终模型进行评估。</p>
</blockquote>
<p>Argos被定义为一个大型多模态模型（LMM）智能体，它从一个包含K个评分函数的集合中，为每个训练样本选择并计算一个聚合的奖励分数。其验证流程如图2所示。</p>
<p><img src="https://arxiv.org/html/2512.03438v1/x2.png" alt="验证流程"></p>
<blockquote>
<p><strong>图2</strong>：Argos验证过程。对图像和视频使用同一套评分函数。首先解析响应以提取生成的2D点、时间片段、推理文本和答案；然后，智能验证器根据提取的信息自适应决定调用哪些评分函数；最后，使用门控聚合函数汇总得分。</p>
</blockquote>
<p>具体而言，对于一个包含视觉输入v、问题q、模型生成的推理轨迹r和预测答案y^的样本，Argos的工作流程如下：</p>
<ol>
<li><strong>解析</strong>：调用解析函数从整个响应中提取关键信息，包括一组2D空间点和时间片段。</li>
<li><strong>自适应评分</strong>：根据训练样本，Argos自适应地组合一个多目标奖励过程，选择相关工具对响应进行评分。核心评分模块包括：<ul>
<li>**空间奖励 (R_spatial)*<em>：旨在评估图像中2D点的定位准确性。首先从模型生成的响应中提取一组N个2D点P（包含坐标和预测物体标签）。对于第i个点，使用开放词汇目标检测模型g_θ根据预测的物体标签o_i生成一个伪真值边界框b_i</em>。然后，使用分割教师模型h_ϕ在b_i*内提取细粒度分割掩码M_i。空间基础得分s_i为指示函数，判断点(x_i, y_i)是否落在掩码M_i内（值为1）。最终R_spatial为所有点得分的平均值。对于包含合成内容（如条形图）的图像，会先使用一个指向模型f_ϑ生成2D点。</li>
<li><strong>时间奖励</strong>：扩展至视频。从响应中提取帧级观察F（包含时间戳、坐标和物体）和段级事件E（包含起止时间/帧索引和事件描述）。帧级得分S_f使用与空间奖励相同的模型（f_ϑ, g_θ, h_ϕ）计算。段级得分S_e则通过查询一个强大的教师推理模型T，评估事件描述d_i与对应视频片段V_ti_start:ti_end之间的视觉语义准确性，返回二元分数。最终视频基础得分是S_f和S_e各自集合内均值的未加权平均。</li>
<li>**推理质量奖励 (R_reasoning)**：评估生成的推理轨迹r与最终答案y^之间的逻辑一致性。使用一个更大的教师模型，计算在给定问题q、推理轨迹r和视觉输入v的条件下，预测响应y^的条件概率作为奖励。</li>
<li>**结果奖励 (R_acc)*<em>：基于真实答案y</em>计算。根据问题类型采用不同方法：多项选择题或短答案使用精确字符串匹配；浮点数答案使用5%容差的相对误差计算；其他情况则使用语言模型判断语义等价性。</li>
</ul>
</li>
<li><strong>门控聚合</strong>：使用一个门控函数聚合所选评分函数的奖励，以防止潜在的噪声奖励使最终答案偏离正确结果。公式如论文式(9)所示：当结果奖励R_acc低于阈值τ时，最终奖励R_final即为R_acc；当R_acc ≥ τ时，R_final为结果奖励、视觉基础奖励和推理质量奖励的加权平均。这确保了模型必须首先获得正确答案，才能从细粒度的中间奖励中受益。</li>
</ol>
<p><strong>训练方法</strong>：使用聚合后的奖励R_final，采用GRPO算法更新策略模型π_θ。在每组rollout奖励值中计算优势A_i，并最小化策略比率裁剪后的目标函数与参考策略之间的KL散度。</p>
<p><strong>理论依据</strong>：论文从帕累托最优的角度提供了理论证明。即使每个单独的奖励估计器是弱且有噪声的，只要它们是独立且均值归零的，通过加权聚合多个奖励信号，随着奖励数量m的增加，策略能够以高概率逼近全局的δ-帕累托最优解。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：模型基于开源的Qwen2.5-VL 7B模型构建，在自建的数据集上进行监督微调（SFT）和GRPO强化学习训练。在零样本设置下评估了多个智能基准，包括空间推理、视觉幻觉减少、具身AI和机器人任务。</p>
<p><strong>对比基线</strong>：包括Qwen2.5-VL 7B（基础版及思维链提示版）、Video-R1（仅SFT版及RL版）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>空间推理</strong>：结果如表1所示。在BLINK数据集上，Argos达到56.0%的准确率，优于基础模型（54.4%）和Video-R1 RL（49.0%）。在MindCube-t数据集上达到39.6%，显著优于基础模型（34.9%）。在CV-Bench的2D和3D子集上分别达到78.2%和82.0%，均优于所有对比基线，表明2D视觉接地训练能增强3D理解泛化能力。</li>
<li><strong>减少视觉幻觉</strong>：在POPE基准上，Argos的“是/否”准确率达到88.8%，F1分数达到89.0%，均显著高于Qwen2.5-VL 7B（85.5%, 86.1%）和Video-R1 RL（86.1%, 86.7%）。</li>
<li><strong>具身AI与机器人</strong>：在EmbodiedBench上，Argos在高级任务规划（65.6% vs 基线59.8%）和低级技能规划（69.7% vs 基线63.5%）上均取得最佳结果。在CALVIN机器人操作基准上，Argos的成功率为65.0%，优于Video-R1 RL（57.5%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.03438v1/figures/visual_grounding_accuracy_reward_curves.png" alt="消融实验-视觉接地奖励曲线"></p>
<blockquote>
<p><strong>图4</strong>：不同奖励配置下，模型在RL训练过程中视觉接地准确率的变化曲线。红线（使用Argos全奖励）的准确率最高且稳定上升，而仅使用结果奖励（绿线）的视觉接地能力几乎没有提升，验证了空间奖励组件的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.03438v1/figures/accuracy_reward_curves.png" alt="消融实验-准确率奖励曲线"></p>
<blockquote>
<p><strong>图5</strong>：不同奖励配置下，模型在RL训练过程中答案准确率的变化曲线。使用Argos全奖励（红线）的准确率最终最高，且学习曲线更平滑，表明多目标奖励有助于稳定训练并提升最终性能。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：图4和图5的消融实验表明，仅使用结果奖励（R_acc）会导致模型在RL过程中视觉接地能力崩溃（图4绿线），而加入空间奖励（R_spatial）能有效提升接地能力（图4蓝线），同时结合推理质量奖励（R_reasoning）能带来最佳的准确率性能和稳定的学习曲线（图5红线）。这验证了Argos中各个奖励组件的贡献。</li>
<li><strong>定性结果</strong>：图8至图15展示了模型生成的响应及其对应的2D点标注示例，直观显示了模型能够进行准确的空间指代。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.03438v1/figures/sft_training_mix_distribution_pie_chart.png" alt="SFT数据组成"></p>
<blockquote>
<p><strong>图6</strong>：SFT训练数据集的组成分布饼图，显示了来自不同来源（如GLM-4.1V, Gemini）及不同类型（图像、视频）数据的比例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.03438v1/figures/rl_training_mix_distribution_pie_chart.png" alt="RL数据组成"></p>
<blockquote>
<p><strong>图7</strong>：RL训练数据集的组成分布饼图，与SFT数据集不重叠，同样展示了多来源和多模态的数据混合。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了Argos智能验证器框架，用于在数据筛选和MMRL训练阶段提供自适应、多目标、可验证的奖励，解决了多模态强化学习中奖励信号稀疏和噪声的问题。</li>
<li>引入了一种新颖的、时空视觉接地的推理数据生成和筛选流程，并实证表明仅靠高质量的SFT数据不足以防止模型在RL中崩溃，必须在RL阶段进行在线验证。</li>
<li>在多个智能基准（空间推理、减少幻觉、具身任务完成和机器人操作）上实现了最先进的性能，证明了该方法的有效性，并提供了基于帕累托最优的理论依据。</li>
</ol>
<p><strong>局限性</strong>：论文提到，尽管使用了强大的教师模型，其数据生成流程的产出率（yield rate）仍然较低（约3.1%），表明生成可靠、接地的推理轨迹本身具有挑战性。</p>
<p><strong>后续启示</strong>：Argos的模块化架构使其能够自然地扩展到新的模态和目标。随着特定任务的教师模型不断改进，Argos有潜力计算更具信息量的奖励信号，从而训练出更强大、更鲁棒的多模态推理智能体。这项工作为多模态强化学习中的奖励设计提供了一个新的、以智能体验证为核心的研究方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多模态强化学习（MMRL）中奖励信号稀疏、难以提供细粒度指导的问题，提出了一种名为Argos的智能验证器。该方法的核心是自适应地为每个训练样本从一组教师模型和规则派生的评分函数中选择合适的工具，同时评估最终答案准确性、所指实体与动作的时空定位以及推理过程质量。实验表明，在SFT数据筛选和RL训练中使用Argos，能在空间推理、视觉幻觉及机器人等具身AI任务上取得最先进性能，有效防止训练中智能体崩溃为无根据的解决方案，并减少奖励黑客行为。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.03438" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>