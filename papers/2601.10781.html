<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Future Optical Flow Prediction Improves Robot Control & Video Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Future Optical Flow Prediction Improves Robot Control & Video Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.10781" target="_blank" rel="noreferrer">2601.10781</a></span>
        <span>作者: Juan Carlos Niebles Team</span>
        <span>日期: 2026-01-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人控制和视频生成领域日益重视利用未来运动表示（如光流、稀疏轨迹）来指导任务执行。主流方法可分为两类：一类在机器人控制中利用稀疏的未来像素轨迹或光流来推断动作；另一类在视频生成中利用用户提供的稀疏轨迹或运动模型来控制生成内容。然而，这些方法存在关键局限性：稀疏运动表示可能丢失全局或细节运动信息；而直接从噪声大、非结构化的网络规模人类活动视频中学习泛化性强的、空间密集的未来运动表示，仍是一个巨大挑战。本文针对“如何从噪声网络视频中学习语言驱动的、泛化性强的未来密集运动预测”这一具体痛点，提出了结合视觉语言模型（VLM）与扩散模型（Diffusion）的新视角。其核心思路是：构建一个统一的VLM-Diffusion架构（FOFPred），从网络视频-字幕对中学习预测未来光流，并将此能力泛化应用于语言驱动的机器人控制和视频生成两个下游领域。</p>
<h2 id="方法详解">方法详解</h2>
<p>FOFPred的整体框架是一个语言条件化的未来光流序列预测模型。输入为历史图像序列（例如 𝒙_{t-1}, 𝒙_t）和自然语言指令 c，输出为预测的未来光流序列 𝒚̂。其核心创新在于统一的VLM-Diffusion架构，该架构充分利用了VLM的多模态推理能力和预训练扩散模型的高保真图像生成能力。</p>
<p><img src="https://arxiv.org/html/2601.10781v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：FOFPred方法总览。（左&amp;中）展示了统一的VLM-Diffusion架构，其中仅扩散变换器（DiT）模块被训练，而VAE和VLM保持冻结。（右）展示了基于FOFPred为控制和生成两个正交任务构建的不同下游流水线。</p>
</blockquote>
<p><strong>核心模块与流程</strong>：</p>
<ol>
<li><strong>特征提取</strong>：使用基于自回归Transformer的VLM（Qwen2.5-VL）对语言指令c和视觉输入𝒙_{t-1}, 𝒙_t进行编码，得到文本特征𝒇_c。同时，使用VAE编码器（Flux.1）对相同的视觉输入进行编码，得到视觉特征𝒇_v。</li>
<li><strong>特征融合与扩散生成</strong>：文本特征𝒇_c和视觉特征𝒇_v分别经过MLP层投影到相同维度后，与噪声一起作为条件输入送入扩散变换器（DiT）。本文采用的DiT基于OmniGen架构，并进行了关键修改以支持时序建模：将2D RoPE编码扩展以处理输入输出帧序列，并将Transformer块改为执行全时空注意力以建模帧序列的时间轴。这些修改未引入额外可学习参数，使得模型能直接受益于图像领域的预训练权重。</li>
<li><strong>解码输出</strong>：DiT的输出𝒇̂_y通过VAE解码器得到最终预测的未来光流序列𝒚̂。</li>
</ol>
<p><strong>关键技术创新细节</strong>：</p>
<ul>
<li><strong>光流的RGB表示</strong>：与先前工作不同，FOFPred将光流（极坐标下的幅度和方向）映射到HSV颜色空间，再转换为RGB格式进行表示。这使得模型可以直接利用强大的现成VAE模型（如Flux.1），而无需对其微调或重新训练。</li>
<li><strong>训练目标与策略</strong>：训练时，使用光流计算算法ℱ根据未来真实帧计算目标光流序列𝒚，并通过VAE编码得到𝒇<em>y。训练目标采用流匹配（Flow Matching）扩散损失（公式1）。训练中采用了分类器无关指导，随机丢弃文本或视觉条件，并对视觉条件沿时间轴（如掩码𝒙</em>{t-1}）和视角轴进行部分掩码，以增强模型鲁棒性。</li>
<li><strong>从噪声视频中学习</strong>：针对网络视频中存在的相机运动和噪声，FOFPred采用了<strong>相对光流计算</strong>作为训练目标。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.10781v1/x3.png" alt="相对光流计算"></p>
<blockquote>
<p><strong>图3</strong>：相对光流计算流程。首先计算原始光流，然后通过单应性技术和深度特征估计相机运动，最后利用投影几何计算相对于相机的光流，从而将物体运动与相机运动解耦。</p>
</blockquote>
<p>此外，为了应对自然视频中运动分布不均的问题，采用基于运动量的帧序列筛选策略，仅选择连续帧间运动超过阈值的前k%光流值对应的帧序列用于训练。</p>
<p><strong>下游任务扩展</strong>：</p>
<ul>
<li><strong>机器人控制</strong>：在FOFPred预测的未来光流基础上，连接一个扩散策略网络（DPN）来映射出机器人动作。在进行下游微调时，显式考虑了机器人数据中固定外部摄像头和移动腕部摄像头两种视角，通过跨视角条件化和扩展预测目标来增强模型对具身（embodiment）的感知。</li>
<li><strong>视频生成</strong>：构建两阶段流水线。第一阶段，FOFPred根据初始帧和描述期望运动的文本，生成未来光流序列并插值为密集运动信号。第二阶段，将此运动信号和初始帧输入现有的视频合成模型GWTF，生成最终视频。这种方法虽计算量更大，但能更好地遵循复杂文本运动指令，且生成过程因光流而更可解释。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在机器人控制任务中，使用<strong>CALVIN</strong>（ABC→D零样本长视野评估）和<strong>RoboTwin 2.0</strong>（双手机器人操作）两个基准。在视频生成任务中，使用<strong>Something-Something-V2 (SSv2)</strong> 数据集评估语言驱动的运动控制生成质量。模型在SSv2和EgoDex数据集约50万个视频-字幕对上进行了预训练。</p>
<p><strong>对比方法</strong>：机器人控制方面，对比了RT-1、Diffusion Policy、Robo-Flamingo、VPP、DreamVLA等一系列先进基线。视频生成方面，对比了Seer、Dynamicrafter、CosHand、InterDyn、CogVideoX等方法。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>CALVIN机器人操控结果</strong>：FOFPred在100%和10%训练数据两种设置下均取得了最佳性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.10781v1/x4.png" alt="CALVIN结果"></p>
<blockquote>
<p><strong>图5</strong>：CALVIN基准上的任务成功率与平均完成长度。FOFPred在全部五个顺序任务上均取得了最高的成功率，特别是在数据受限（10%）设置下优势明显，显示了其数据效率。</p>
</blockquote>
<ol start="2">
<li><strong>RoboTwin双手机器人操作结果</strong>：FOFPred在五个需要双臂协作的任务上，平均成功率达到了68.6%，显著优于VPP基线（61.8%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.10781v1/x5.png" alt="RoboTwin结果"></p>
<blockquote>
<p><strong>图6</strong>：RoboTwin 2.0基准上的任务成功率。FOFPred在所有五个任务上均一致优于VPP基线，证明了其在复杂双手机器人任务上的有效性。</p>
</blockquote>
<ol start="3">
<li><strong>语言驱动视频生成结果</strong>：在SSv2数据集上，FOFPred引导的T2V流水线在SSIM、PSNR、LPIPS、FVD、KVD和运动保真度（MF）等多个指标上均优于最强的CogVideoX基线。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.10781v1/x6.png" alt="视频生成结果"></p>
<blockquote>
<p><strong>图7</strong>：SSv2数据集上的视频生成质量评估。FOFPred在多个指标上超越基线，尤其在运动保真度（MF）上提升显著，表明其能更好地遵循文本描述的运动指令。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：</li>
</ol>
<p><img src="https://arxiv.org/html/2601.10781v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融研究结果。（左）移除相对光流计算（w/o Rel.）或运动感知帧采样（w/o Mot.）均会导致性能下降，验证了数据预处理技术的必要性。（中）在机器人控制任务上，对VLM或DiT进行微调（FT）相比冻结预训练权重（Ours），性能反而下降，表明保持强大的预训练表征至关重要。（右）在视频生成中，使用FOFPred预测的光流比使用真实光流或基线方法，能获得更高的运动保真度（MF）。</p>
</blockquote>
<p>消融实验总结：</p>
<ul>
<li><strong>数据预处理</strong>：移除相对光流计算或运动感知帧采样均会导致模型性能下降，证明了这些处理对于从噪声网络视频中学习有效信号的关键作用。</li>
<li><strong>架构设计</strong>：保持VLM和DiT的预训练权重冻结至关重要，微调它们反而会损害模型在机器人任务上的性能，表明强大的预训练先验知识是泛化能力的基础。</li>
<li><strong>预测光流的价值</strong>：在视频生成中，使用FOFPred预测的光流甚至比使用“真实”光流（由算法计算）能获得更高的运动保真度，凸显了模型预测的优越性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出统一的VLM-Diffusion架构</strong>：首次将视觉语言模型与扩散模型统一，用于学习语言条件化的未来密集光流预测，兼顾了多模态推理和像素级生成保真度。</li>
<li><strong>建立了从网络视频中学习未来光流的可扩展框架</strong>：通过相对光流计算和运动感知采样等关键技术，成功地从大规模、噪声、非结构化的网络人类活动视频中学习到了泛化性强的运动表示。</li>
<li><strong>展示了跨领域应用的强大潜力</strong>：将同一核心模型成功应用于语言驱动的机器人控制和视频生成两个截然不同的下游任务，并均取得了领先性能，验证了未来光流作为通用运动表示的实用价值。</li>
</ol>
<p><strong>局限性</strong>：论文在附录中提到，其两阶段的视频生成流水线计算成本较高。此外，依赖离线数据预处理（如相对光流计算）也可能限制其在需要完全在线学习场景中的应用。</p>
<p><strong>后续研究启示</strong>：FOFPred的成功表明，结合强大的预训练基础模型（VLM, Diffusion）与针对性的运动表示学习，是通向通用具身智能和可控内容生成的有效路径。未来工作可以探索更高效的单阶段架构、在线学习方案，以及将未来运动预测扩展到更复杂的3D场景理解中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出FOFPred模型，旨在解决语言条件下、可泛化的未来光流预测难题，以提升机器人控制与视频生成能力。核心方法采用统一的视觉语言模型与扩散架构，结合大规模网络视频-文本数据训练，通过关键的数据预处理与图像预训练从噪声数据中提取有效信号。实验表明，该模型在语言驱动的机器人操控和视频生成任务中均表现出优异的跨领域适应性，验证了统一架构及从多样网络数据中学习的价值。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.10781" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>