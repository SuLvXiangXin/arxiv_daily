<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.10333" target="_blank" rel="noreferrer">2508.10333</a></span>
        <span>作者: Song, Wenxuan, Zhou, Ziyang, Zhao, Han, Chen, Jiayi, Ding, Pengxiang, Yan, Haodong, Huang, Yuxin, Tang, Feilong, Wang, Donglin, Li, Haoang</span>
        <span>日期: 2025/08/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过整合多模态理解和动作执行，在机器人控制领域取得了显著进展。然而，实证分析表明，现有VLA模型在预测动作时，其视觉注意力往往呈现分散状态，无法精准聚焦于需要操作的目标物体区域，这可能导致操作错误对象。为了引导视觉注意力正确接地（grounding）到目标上，先前的方法主要分为两类：一是显式接地（Explicit Grounding），即依赖外部专家模型（如目标检测器）分割出目标区域图像，并将其与原始图像一同输入给VLA模型；二是思维链接地（CoT Grounding），让VLA模型以链式思维的方式先输出目标边界框坐标，再输出动作。这些方法虽然提升了目标区域的感知和空间意识，但并未从根本上优化模型内部的注意力分配机制。</p>
<p>本文针对VLA模型视觉注意力分散、难以实现精准操作这一具体痛点，提出了一个<strong>隐式接地</strong>的新视角。其核心思路是：为VLA模型引入一个辅助的视觉重建任务，即要求模型基于其视觉输出去重建图像中与操作目标对应的“凝视区域”（gaze region）。这一重建过程迫使VLA模型学习包含区域特异性信息的细粒度表征，从而自发地将视觉注意力引导至正确区域，最终实现精确操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>ReconVLA的整体框架包含两个部分：动作预测部分和视觉重建部分。模型以多视角的当前观测图像和语言指令为输入。在训练时，目标操作区域（凝视区域）的图像会通过一个冻结的视觉标记器（Tokenizer）转换为潜在场景令牌（scene tokens）。模型的核心创新在于，其大型语言模型（LLM）在编码视觉输入后，不仅输出用于解码动作的动作令牌，还会输出一组<strong>重建令牌（reconstructive tokens）</strong>。这些重建令牌将作为条件，指导一个轻量级的扩散变换器（Diffusion Transformer）去噪器，从带噪的潜在场景令牌中重建出目标凝视区域的场景令牌。通过这一重建损失对模型的视觉输出进行监督，模型被引导去关注和编码与任务目标相关的精细视觉信息。</p>
<p><img src="https://arxiv.org/html/2508.10333v1/image/arch.jpg" alt="方法架构"></p>
<blockquote>
<p><strong>图3</strong>：ReconVLA的整体架构。模型由重建部分和动作部分组成。输入包括多视角图像和文本指令。动作部分输出离散的动作令牌。重建部分引导模型输出重建令牌，这些令牌作为去噪过程的条件，用于从带噪的 <code>z_t</code> 中重建出凝视区域的场景令牌 <code>z_0</code>。这种监督增强了模型的视觉接地和细粒度理解能力。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉标记器（ℱ）</strong>：采用一个连续的变分自编码器（VAE），将凝视区域的RGB图像 <code>I‘</code> 编码为潜在场景令牌 <code>z_0 = ℱ(I’)</code>。该模块保持冻结，以保留详细的视觉特征并实现高保真重建。</li>
<li><strong>扩散去噪过程</strong>：去噪器 <code>𝒟</code> 是一个由Transformer编码器块堆叠而成的网络，通过自注意力机制捕捉带噪令牌 <code>z_t</code> 与重建令牌 <code>𝒉_R</code> 之间的关联。重建视觉损失遵循标准的扩散模型训练目标：<br><code>ℒ_VLA^visual(𝒉_R, I’) = 𝔼_t,ϵ [||𝒟(z_t; 𝒉_R, t) - ϵ||^2]</code><br>其中，<code>𝒉_R = LLM(𝒉_I)</code> 是LLM基于图像令牌产生的重建令牌，<code>ϵ</code> 是加入的噪声。</li>
<li><strong>输入序列组织</strong>：为确保图像令牌能关注到指令信息，在输入序列中，将一组指令令牌预置在图像令牌之前，使图像令牌能通过因果注意力融合这些前缀文本的信息。</li>
<li><strong>总体损失</strong>：训练总损失为动作预测的交叉熵损失与上述视觉重建损失之和：<code>ℒ_ReconVLA = ℒ_VLA^action + ℒ_VLA^visual</code>。</li>
</ol>
<p><strong>与现有方法的创新对比</strong>：<br><img src="https://arxiv.org/html/2508.10333v1/image/comparison.jpg" alt="范式对比"></p>
<blockquote>
<p><strong>图2</strong>：不同视觉接地范式的概念对比。(a) 显式接地：使用外部接地专家，输入完整图像和裁剪后的目标图像。(b) 思维链接地：以链式思维方式先输出边界框坐标，再输出动作。(c) 隐式接地（ReconVLA）：直接利用关键区域作为对视觉输出的隐式视觉监督（即重建令牌），通过重建过程实现接地。</p>
</blockquote>
<p>与显式接地（依赖外部模型，引入冗余信息）和思维链接地（直接输出坐标，训练困难）相比，ReconVLA的创新在于<strong>直接对模型的视觉输出施加重建凝视区域的监督</strong>。这是一种“隐式”的接地机制，它不改变模型的输入输出接口，而是通过辅助任务从根本上提升模型自身对目标区域的感知和注意力分配能力，模拟了人眼自发聚焦于视野中显著区域的行为。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：主要在CALVIN仿真基准（基于PyBullet，Franka Panda机械臂）上进行评估，包含34个任务和4种环境（A, B, C, D）。长视野挑战由5个子任务顺序构成。同时进行了真实世界实验。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>范式对比</strong>：显式接地（EG，使用YOLOv11）、思维链接地（CG）与本文的隐式接地（IG）。</li>
<li><strong>SOTA对比</strong>：包括生成未来图像的方法（如UniPi, SuSIE, GR-1, CLOVER等）和大规模VLA模型（如RoboFlamingo, OpenVLA, UniVLA等）。</li>
</ul>
</li>
<li><strong>评估指标</strong>：报告顺序完成1到5个子任务的成功率（%），以及所有5个子任务的平均完成长度（Avg. Len）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>范式对比结果</strong>：在CALVIN ABC→D任务上的结果如表1所示。显式接地（EG）相比基线有提升，但性能有限。思维链接地（CG）性能最差（5/5成功率仅0%）。本文的隐式接地（IG）取得了最佳性能，5/5成功率达64.1%，平均长度3.95，显著优于其他范式。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.10333v1/image/calvin_real_attention_compare.png" alt="注意力对比"></p>
<blockquote>
<p><strong>图4</strong>：CALVIN和真实世界中的注意力图定性对比。第1行：基线模型的注意力分散或主要关注错误区域，导致动作不准确。第2行：借助辅助视觉监督信号，ReconVLA迫使模型以更高的注意力值关注特定的图像内容，并精确移动到目标区域，从而成功完成任务。</p>
</blockquote>
<ol start="2">
<li><strong>消融实验</strong>：如表2所示，逐步添加重建部分、使用凝视区域（而非整图）作为重建目标、以及进行大规模预训练，均能带来性能提升。其中，大规模预训练对提升模型在未见场景下的视觉重建泛化能力至关重要。</li>
<li><strong>与SOTA方法对比</strong>：<ul>
<li>在更具泛化挑战的ABC→D任务上（表3），ReconVLA超越了所有对比方法，在5/5成功率上比强大的生成式模型GR-1高出约24%，比同类VLA模型UniVLA高出7.6%。</li>
<li>在ABCD→D任务上（表4），ReconVLA也取得了领先或具有竞争力的结果，平均完成长度达到4.23。</li>
</ul>
</li>
<li><strong>真实世界实验</strong>：<br><img src="https://arxiv.org/html/2508.10333v1/image/real_world_arxiv.jpg" alt="真实世界设置"><blockquote>
<p><strong>图5</strong>：真实世界实验设置，包括四个代表性任务：叠碗、将水果放入碗中、翻转杯子、清理餐桌。</p>
</blockquote>
<img src="https://arxiv.org/html/2508.10333v1/image/real_world_basic.png" alt="真实世界结果"><blockquote>
<p><strong>图6</strong>：真实世界多任务结果。ReconVLA在四个任务上均取得最高成功率，并且在两个涉及未见目标物体的任务上，显著优于对比方法（OpenVLA和PD-VLA成功率接近0%），展现了出色的泛化能力。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li>提出了<strong>ReconVLA</strong>，一种基于隐式接地范式的重建型VLA模型。通过重建目标凝视区域这一辅助任务，引导模型学习细粒度表征并精准分配视觉注意力，从而提升了视觉接地能力和操作精度。</li>
<li>构建了一个<strong>大规模机器人预训练数据集</strong>，包含超过10万条轨迹和200万个数据样本，显著增强了模型视觉重建能力的泛化性。</li>
<li>在仿真和真实世界进行了<strong>广泛的实验</strong>，验证了隐式接地方法的优越性，以及模型在精确操作和泛化到未见目标方面的强大能力。</li>
</ol>
<p>论文提及的局限性主要隐含在对大规模预训练数据的依赖上，这需要可观的数据收集和计算资源。此外，方法的核心增益来源于注意力机制的改善，但其理论解释和可解释性仍有待进一步探索。</p>
<p>本工作对后续研究的启示在于：为提升机器人模型的感知精度，<strong>对模型内部表征施加隐式的、任务相关的监督</strong>（如重建特定区域）是一种有效且优雅的途径，它比依赖外部模块或复杂输出格式更根本地解决了注意力分配问题。同时，这项工作也表明，<strong>将感知（重建当前）与生成（预测未来）的能力相结合</strong>，是提升VLA模型综合性能的一个有前景的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前视觉-语言-动作（VLA）模型视觉注意力分散、难以精确聚焦目标区域，导致操作错误的核心问题，提出了ReconVLA模型。该方法采用隐式接地范式，通过轻量级扩散变换器重建图像中对应于目标对象的注视区域，引导VLA学习细粒度表示并准确分配视觉注意力；同时构建了超过10万条轨迹和200万样本的大规模预训练数据集以提升泛化。实验表明，该方法在仿真和真实世界中均实现了精确操作，并展现出优越的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.10333" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>