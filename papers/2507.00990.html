<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.00990" target="_blank" rel="noreferrer">2507.00990</a></span>
        <span>作者: Yunzhu Li Team</span>
        <span>日期: 2025-07-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域从视频中学习模仿的主流方法主要分为两类：一是利用公开的大规模真实世界视频数据集进行学习，但这类方法存在领域鸿沟，且需要针对特定机器人形态和任务进行适配；二是模仿在受控条件下捕获的、与执行场景高度匹配的特定演示视频，但这需要进行繁重且要求严格（如视角、形态、交互模式对齐）的数据收集工作。这两种策略均依赖于真实数据的采集，限制了其广泛部署。</p>
<p>本文针对这一痛点，提出了一个全新的视角：能否利用单个生成的视频——根据输入环境和任务描述精确合成——作为机器人操作任务的唯一监督来源？尽管SORA、Kling等模型展现了强大的视频生成能力，但其生成的视频常存在物体几何失真、物理上不可能的交互和不真实的场景动态等问题，其在机器人领域的实用性尚未得到证实。先前将视频生成应用于机器人的工作通常依赖额外监督，如任务特定训练或在机器人轨迹数据集上微调。</p>
<p>本文的核心思路是：提出RIGVid框架，利用视频扩散模型根据语言指令和初始场景图像生成潜在演示视频，通过视觉语言模型自动过滤不合格结果，然后通过6D姿态跟踪器提取物体轨迹，并以与机器人形态无关的方式重定向给机器人执行，从而实现完全无需物理演示或机器人特定训练的零样本操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>RIGVid方法的整体流程如图1所示。输入为场景的初始RGB图像、对应的深度图以及一个自由形式的语言指令（如“给植物浇水”）。目标是预测机器人的6自由度末端执行器轨迹。流程分为三个关键步骤：1) 生成场景与任务条件视频并估计其深度；2) 通过物体姿态跟踪器计算6D姿态序列；3) 抓取物体并将姿态轨迹重定向给机器人执行。</p>
<p><img src="https://arxiv.org/html/2507.00990v2/x1.png" alt="方法概述"></p>
<blockquote>
<p><strong>图1</strong>：RIGVid方法概述。给定初始场景图像和深度，我们根据语言指令生成视频。一个基于VLM的自动过滤步骤（图中未显示）可用于拒绝未遵循指令的视频。单目深度估计器恢复生成视频每一帧的深度，这些深度图与对应的RGB帧结合，通过6D物体姿态跟踪器产生物体姿态轨迹。抓取后，该轨迹被重定向给机器人执行。</p>
</blockquote>
<p><strong>核心模块一：视频生成与深度估计及过滤</strong>。使用最先进的视频扩散模型（如Sora, Kling）生成候选视频。由于生成视频不一定准确遵循指令，作者引入了一个基于VLM（GPT-4o）的自动过滤机制：从视频中均匀采样四帧拼接成摘要图像，提示VLM判断视频是否成功描绘了指令描述的动作。如果VLM响应为否定，则重新生成视频，最多尝试五次。对于通过过滤的视频，使用单目深度估计器预测每一帧的深度。由于预测的深度值缺乏真实世界尺度，作者通过一个仿射变换（缩放和平移），将第一帧预测深度与初始真实深度图在主动物体区域对齐，然后将此变换应用于整个视频序列以解决尺度模糊问题。</p>
<p><strong>核心模块二：主动物体识别与6D物体姿态轨迹提取</strong>。首先，利用GPT-4o根据初始图像和任务指令识别最可能被操作的物体，并使用Grounding DINO和SAM-2获取该物体的分割掩码。然后，使用缩放后的预测深度，通过6D姿态跟踪器在整个视频中跟踪该物体，得到其6D姿态序列。经过实验，作者选择了性能最好且支持实时执行的FoundationPose跟踪器，它需要物体的预计算网格（使用BundleSDF从一段RGBD视频中重建）。为确保运动平滑，对提取的姿态序列（尤其是旋转）应用了平均滤波。</p>
<p><strong>核心模块三：物体到机器人的运动重定向与闭环执行</strong>。首先，使用现成的抓取器（AnyGrasp）在主动物体掩码定义的边界内执行抓取。抓取后，假设末端执行器与物体之间存在固定的刚体变换（由抓取时刻物体相对于夹爪的位姿以及夹爪与机器人末端执行器的偏移量复合而成）。通过将这个固定变换应用到整个物体姿态轨迹上，即可得到对应的末端执行器轨迹（如图2所示）。此过程是机器人形态无关的。</p>
<p><img src="https://arxiv.org/html/2507.00990v2/x2.png" alt="重定向示意图"></p>
<blockquote>
<p><strong>图2</strong>：将物体轨迹重定向为机器人轨迹。假设抓取后末端执行器与物体之间存在固定变换，6D物体姿态轨迹（橙色箭头）被重定向到机器人（蓝色箭头）。此公式与机器人形态无关，可迁移到不同机器人。</p>
</blockquote>
<p><strong>创新点与闭环执行</strong>。与现有方法最大的不同在于，RIGVid完全不需要任何真实的机器人演示数据或任务特定训练，实现了零样本的、与机器人形态无关的模仿。此外，系统具备闭环执行能力（图3）：在执行过程中，使用FoundationPose实时跟踪物体6D姿态。如果因外部扰动（如被推动或打滑）导致物体位姿偏离计划轨迹超过阈值（位置3厘米，方向20度），机器人会回退到最后成功执行的轨迹点，并从此处恢复执行，从而增强了鲁棒性。</p>
<p><img src="https://arxiv.org/html/2507.00990v2/x3.png" alt="抗扰动示意图"></p>
<blockquote>
<p><strong>图3</strong>：RIGVid对扰动具有鲁棒性。执行过程中人类推动机器人（图像1），导致物体偏离计划轨迹。当检测到偏离时（图像2），机器人回退到最后成功执行的轨迹点（图像3），然后恢复计划运动（图像4）。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在配备Orbbec Femto Bolt相机的xArm7机械臂上进行真实世界评估。测试了四个日常操作任务（图4）：浇水、掀盖子、放锅铲、扫垃圾，涵盖了深度变化、物体几何（细长、部分遮挡）和动作多样性等挑战。每个任务的成功标准被明确定义（如浇水时喷壶嘴是否在植物上方）。使用不同视频源时，每个任务测试10个视频。</p>
<p><img src="https://arxiv.org/html/2507.00990v2/x4.png" alt="评估任务"></p>
<blockquote>
<p><strong>图4</strong>：评估任务。我们在不同难度的日常操作任务上评估RIGVid。</p>
</blockquote>
<p><strong>1. 生成视频质量与过滤的影响</strong>：<br>作者比较了Sora、Kling v1.5和Kling v1.6三种生成模型。如图5所示，Sora常改变场景布局和物体大小；Kling v1.5在遵循指令和物理合理性上仍有不足；Kling v1.6产生了最一致和真实的结果。</p>
<p><img src="https://arxiv.org/html/2507.00990v2/x5.png" alt="生成视频质量对比"></p>
<blockquote>
<p><strong>图5</strong>：三种模型的生成视频质量定性对比。Sora（上）大幅改变场景布局和物体尺寸。Kling v1.5（中）未完全遵循指令（水未浇到植物上）并表现出物理上不合理的行为（水从壶顶而非壶嘴倒出）。Kling v1.6（下）产生了最一致和真实的结果。</p>
</blockquote>
<p>过滤统计（图6）显示，Kling v1.6的通过率最高（浇水83%，掀盖66%，放铲55%，扫地45%），而Sora在所有任务上通过率均为0%。表1表明，使用GPT-4o进行过滤与人类判断高度相关（平均皮尔逊相关系数0.84），优于其他视频质量评估指标。</p>
<p><img src="https://arxiv.org/html/2507.00990v2/x6.png" alt="过滤统计"></p>
<blockquote>
<p><strong>图6</strong>：过滤统计。Kling V1.6视频的通过率最高，表明其更准确地遵循语言指令。</p>
</blockquote>
<p>最关键的结果如图7所示：机器人性能随着视频质量的提高而提升。使用过滤后的Kling v1.6视频，RIGVid在四个任务上的成功率（浇水100%，掀盖80%，放铲90%，扫地70%）与使用真实人类演示视频的性能相当。这表明在当前模型质量下，生成视频已足以替代真实视频进行视觉模仿。失败案例主要归因于单目深度估计误差导致的轨迹不准确。</p>
<p><img src="https://arxiv.org/html/2507.00990v2/x7.png" alt="性能与视频质量关系"></p>
<blockquote>
<p><strong>图7</strong>：RIGVid性能与视频质量的关系。虚线将生成视频与真实视频的性能分开。Kling V1.6产生最可靠的视频并带来最高的RIGVid成功率。过滤后的视频性能与真实视频相当。UF表示未过滤，F表示已过滤。</p>
</blockquote>
<p><strong>2. 与VLM轨迹预测方法的对比</strong>：<br>为了验证生成完整视频的必要性（而非更紧凑的表示），作者与最先进的VLM轨迹预测方法ReKep进行了对比。如图8所示，RIGVid在四个任务上的平均成功率为85%，显著高于ReKep的50%。这表明对于当前任务和设置，视频生成提供的丰富细节比VLM生成的高级抽象（关键点约束）能带来更可靠的执行。</p>
<p><img src="https://arxiv.org/html/2507.00990v2/x8.png" alt="RIGVid vs ReKep"></p>
<blockquote>
<p><strong>图8</strong>：RIGVid与ReKep的成功率对比。RIGVid优于最先进的基于VLM的轨迹预测方法ReKep。</p>
</blockquote>
<p><strong>3. 与替代轨迹提取方法的对比</strong>：<br>作者将RIGVid使用的6D物体姿态跟踪与一系列从生成视频中提取轨迹的替代方法进行了比较，包括稀疏点跟踪（Track2Act, Gen2Act）、密集光流（AVDC）、3D特征场（4D-DPM）以及生成目标监督等。如图9所示，RIGVid（使用FoundationPose）在所有四个任务上均取得了最高或并列最高的成功率，显著且一致地优于其他方法。这证明了在物体层面进行强6D姿态跟踪的有效性。</p>
<p><img src="https://arxiv.org/html/2507.00990v2/x9.png" alt="轨迹提取方法对比"></p>
<blockquote>
<p><strong>图9</strong>：与替代轨迹提取方法的对比评估。RIGVid（FoundationPose）在所有任务上均取得了最高或并列最高的成功率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1) 提出了RIGVid框架，使机器人能够仅通过模仿生成的视频来执行开放世界的操作任务，完全无需真实世界演示或机器人特定训练。2) 通过实验证明，高质量（经过滤）的生成视频在机器人模仿任务上的性能与真实视频相当，确立了合成数据在该领域可作为真实数据有效替代品的地位。3) 证明了视频生成与6D轨迹提取的组合，优于多种基于VLM、点跟踪、光流、特征场和生成目标监督的先进方法。</p>
<p>论文提到的局限性包括：视频生成的计算成本较高；当前使用的6D跟踪器需要物体的预计算网格，限制了在无法获取网格场景中的应用（尽管方法兼容无网格方法，但推理速度尚不满足实时要求）；主要的失败原因来自于单目深度估计的误差。</p>
<p>这项研究对后续工作的启示在于：它展示了利用现成的、不断进步的生成式AI模型（如视频扩散模型和VLMs）作为机器人监督源的巨大潜力。随着视频生成模型在遵循指令和物理合理性方面的持续改进，机器人的操作能力有望“免费”获得提升。未来的工作可以探索更高效或无需网格的跟踪方案，以及如何更好地集成或改进深度估计，以进一步提高系统的鲁棒性和适用性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RIGVid系统，解决机器人无需物理演示即可学习复杂操作任务的问题。方法基于视频扩散模型生成任务视频，利用视觉语言模型自动过滤不符合指令的视频，再通过6D姿态跟踪提取物体轨迹并重定向到机器人。实验表明，过滤后的生成视频与真实演示效果相当，性能随生成质量提升；生成视频监督优于VLM关键点预测，6D姿态跟踪也优于密集特征点跟踪。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.00990" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>