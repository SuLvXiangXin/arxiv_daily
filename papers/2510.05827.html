<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.05827" target="_blank" rel="noreferrer">2510.05827</a></span>
        <span>作者: Badong Chen Team</span>
        <span>日期: 2025-10-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人抓取是机器人操作中最基础的任务之一。当前，语言驱动的抓取生成因其交互实用性成为一个有前景的方向。现有方法主要分为两类：一类是多模态特征融合方法，缺乏显式的推理能力；另一类则依赖大型语言或视觉语言模型进行规划和接地，形成复杂的模块化级联流程，计算和内存成本高。两者泛化能力均有限。近期，RT-Grasp作为首个面向语言驱动抓取生成的端到端基础模型被提出，但其过度强调对话和物体语义，导致性能不佳，且仅限于单物体抓取，在复杂杂乱环境中效果差。</p>
<p>本文针对现有抓取基础模型视觉理解不足、在杂乱场景中泛化能力弱的问题，提出了一种新视角：受人类“看图思考”的启发，引入视觉思维链推理来增强模型对视觉信息的理解。本文的核心思路是：设计一个端到端的基础模型，通过多轮上下文处理范式，先定位目标物体，再对裁剪放大的区域进行细粒度推理，从而生成高质量的抓取位姿。</p>
<h2 id="方法详解">方法详解</h2>
<p>VCoT-Grasp是一个端到端的模型，通过一个中间的视觉推理步骤生成抓取位姿。其整体框架基于预训练的视觉语言模型PaliGemma-3B构建，包含四个主要组件：图像编码器（使用冻结的SigLip编码器）、视觉投影器（线性投影模块）、LLM主干（Gemma2）以及动作头（解码器）。</p>
<p><img src="https://arxiv.org/html/2510.05827v1/x2.png" alt="整体框架"></p>
<blockquote>
<p><strong>图2</strong>：VCoT-Grasp的整体框架。模型基于PaliGemma-3B VLM构建，输入投影后的视觉嵌入和分词化的任务指令，采用多轮学习来预测目标物体的位置token和抓取位姿token。评估了多种动作头设计，这些解码器利用细粒度视觉信息以自回归或回归方式生成抓取位姿。</p>
</blockquote>
<p>核心创新在于引入了视觉思维链推理。具体流程分为两轮：</p>
<ol>
<li><strong>目标定位</strong>：给定场景图像O和语言指令l_d，模型首先预测目标物体的边界框b。这一步使模型能够从无关物体中区分出目标，并提供粗粒度的定位。</li>
<li><strong>细粒度抓取生成</strong>：利用预测的边界框b对原图进行裁剪和缩放，得到聚焦于感兴趣区域的边界框图像O_b。随后，模型整合原始图像和定位图像的视觉token，结合抓取指令l_g，生成精细化的抓取位姿g。</li>
</ol>
<p>这种多轮处理范式模仿了人类先定位再细察的推理过程，增强了视觉理解。动作头的设计是另一个关键细节。论文系统研究了两种参数化形式下的四种变体：(1) MLP头（回归连续值）；(2) 基于DiT的扩散头；(3-4) 直接复用语言模型的输出头（LM头），分别使用预训练的位置token（<code>&lt;loc0000&gt;</code> 到 <code>&lt;loc1023&gt;</code>）或新引入的token（<code>&lt;pos0000&gt;</code> 到 <code>&lt;pos1023&gt;</code>）将每个抓取参数离散化为1024个token进行预测。总损失函数为抓取预测损失和边界框预测损失的加权和。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在提出的VCoT-GraspSet数据集和真实机器人平台上进行。VCoT-GraspSet包含超过16.7万张合成图像（136万+抓取标注）和400多张真实图像（1200+抓取标注），物体分为367个已见类别和21个未见类别。真实机器人实验使用Kinova Gen3机械臂。</p>
<p><strong>对比方法</strong>：对比了GG-CNN、GR-ConvNet、CLIP-Fusion、LGD以及RT-Grasp等基线。对于纯视觉方法，通过集成CLIP文本编码器使其支持语言条件。</p>
<p><strong>关键实验结果</strong>：<br>在VCoT-GraspSet测试集上，评估标准是IoU&gt;0.25且方向偏差&lt;30°的抓取预测准确率。</p>
<p><img src="https://arxiv.org/html/2510.05827v1/x3.png" alt="数据集统计"></p>
<blockquote>
<p><strong>图3</strong>：VCoT-GraspSet数据集统计。(a) 已见物体数量分布，其中“Others”是其余347个类别的聚合计数。(b) 21个未见类别的物体数量。</p>
</blockquote>
<p>如表III所示，VCoT-Grasp（LM头，使用预训练token）在已见物体上达到83.60%的准确率，在未见物体上达到58.98%，平均69.16%，全面超越基线。传统特征融合方法在未见物体上性能严重下降，而RT-Grasp因过度依赖语言、视觉理解不足，表现次优。</p>
<p>在真实机器人抓取成功率实验中（表I和表II），VCoT-Grasp在15个已见物体上平均成功率为76%，在15个未见物体上为71%，均显著高于GR-ConvNet+CLIP和RT-Grasp。</p>
<p><strong>消融实验</strong>：</p>
<ul>
<li><strong>视觉思维链推理的作用</strong>：如表IV所示，移除该推理步骤（单轮范式）导致性能大幅下降（平均准确率从61.03%降至57.06%），证明了中间视觉推理步骤的重要性。</li>
<li><strong>动作头选择</strong>：LM头（离散token）性能优于MLP头和扩散头。且复用VLM预训练的位置token比引入新token效果略好，表明预训练模型中的位置先验有益于下游空间推理任务。</li>
<li><strong>数据规模与训练周期</strong>：如图5所示，性能随训练数据量增加而持续提升；训练超过5个周期后，对未见物体的性能开始下降，出现过拟合。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.05827v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：(a) 抓取预测性能随训练样本数量的变化。(b) 训练周期数的影响，第5个周期后出现过拟合。</p>
</blockquote>
<p><strong>泛化性研究</strong>：</p>
<ul>
<li><strong>零样本迁移</strong>：如图6所示，将在合成数据集上训练的模型直接部署到真实机器人进行零样本测试，VCoT-Grasp在未见物体上的成功率比RT-Grasp高12%。</li>
<li><strong>鲁棒性</strong>：如表V和图7所示，在背景变化和存在干扰物的复杂环境下，VCoT-Grasp仍能保持较高的抓取成功率，展示了强大的鲁棒性。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.05827v1/x6.png" alt="零样本性能"></p>
<blockquote>
<p><strong>图6</strong>：在真实世界未见物体上的零样本性能。深色柱表示零样本性能，浅色柱表示在训练集上微调后的性能提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.05827v1/x7.png" alt="鲁棒性案例"></p>
<blockquote>
<p><strong>图7</strong>：真实世界案例可视化：常规设置、添加干扰物、背景变化下的抓取情况。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了VCoT-Grasp，首个将视觉思维链推理融入语言驱动抓取生成的端到端基础模型，显著提升了视觉理解和在复杂环境中的泛化能力；2) 构建并开源了大规模高质量抓取数据集VCoT-GraspSet，包含中间边界框标注；3) 验证了多轮处理范式在机器人基础模型中的有效性。</p>
<p>论文自身提到的局限性包括：训练中可能出现过拟合（见图5b），以及真实世界数据规模相对有限。这项工作对后续研究的启示在于：为机器人基础模型引入结构化的视觉推理步骤是一个有效的方向；利用预训练VLM中已有的空间先验（如位置token）可以提升下游任务性能；大规模、高质量的数据集对于抓取模型的泛化能力至关重要。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VCoT-Grasp模型，旨在解决语言驱动抓取任务中现有方法推理能力不足、泛化性差、依赖复杂模块化流程的问题。其核心技术是引入视觉思维链推理机制，通过端到端的多轮处理范式动态聚焦视觉输入，并生成可解释的推理轨迹。模型基于新构建的大规模数据集VCoT-GraspSet进行训练。实验表明，该方法显著提升了抓取成功率，并能有效泛化到未见过的物体、背景及干扰场景。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.05827" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>