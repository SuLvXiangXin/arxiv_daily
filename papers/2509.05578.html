<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.05578" target="_blank" rel="noreferrer">2509.05578</a></span>
        <span>作者: Liu, Ruixun, Kong, Lingyu, Li, Derun, Zhao, Hang</span>
        <span>日期: 2025/09/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，自动驾驶领域中的多模态大语言模型（MLLMs）虽然展现出强大的视觉-语言推理能力，但普遍缺乏对3D空间的稳健理解。现有方法主要面临两大挑战：一是依赖昂贵的人工3D标注（如坐标或边界框），这些标注稀疏且难以大规模获取；二是由于缺乏大规模的3D视觉-语言预训练数据，导致模型在融合3D信息时会丢失细粒度的空间细节。一些方法尝试将显式的3D输入（如占据栅格、激光雷达点云）直接输入给模型，但同样受限于3D视觉-语言预训练数据的匮乏。</p>
<p>本文针对上述痛点，提出了一个新颖的视角：将密集的3D占据（Occupancy）表示既作为模型的预测输出，又作为监督信号，从而让模型能够仅从2D视觉输入中学习到精细的3D空间结构。其核心思路是，通过一个统一的视觉-语言-占据主干网络，隐式地进行3D占据预测以增强模型的3D空间理解，并且在推理时可以跳过该预测过程，不引入额外的计算开销。</p>
<h2 id="方法详解">方法详解</h2>
<p>OccVLA是一个统一的框架，旨在同时完成3D占据预测和未来的自车运动规划。其输入是多视角的2D图像，输出包括3D占据预测、语言描述（用于元动作预测）以及未来的轨迹坐标。</p>
<p><img src="https://arxiv.org/html/2509.05578v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：OccVLA架构总览。框架在共享的视觉-语言-占据（V-L-O）主干网络中统一了密集3D占据预测和语言建模。模型使用用于占据预测的非自回归损失 $L_{NAR}^{occ}$ 和用于文本输出的自回归损失 $L_{AR}^{text}$ 进行联合训练。(a) 在V-L-O主干中，占据标记（右侧）通过交叉注意力查询视觉标记（左侧）的特征，而视觉标记通过自注意力建模。(b) 在通过VLM预测出元动作后，一个规划头（MLP）生成未来轨迹。</p>
</blockquote>
<p><strong>核心模块1：占据预测（Occupancy Prediction）</strong><br>为了增强模型的3D感知能力，OccVLA在原始VLM框架上扩展了一个并行的占据预测分支。该分支输入一组可学习的占据查询（Occupancy Queries），它们与VLM共享前馈层、QKV投影和归一化层。关键设计在于<strong>交叉模态注意力</strong>：占据标记可以访问VLM中间层产生的视觉特征（作为Key和Value），从而获取2D图像中的空间信息（公式1）。而文本标记的推理则主要依赖视觉特征（公式2），实验表明让文本访问占据特征对最终质量影响不大，这使得在语言推理时可以跳过占据计算，提升效率。此外，通过轻量级的适配器（Adapters）对VLM进行微调，以保留其原始的视觉-语言建模能力。</p>
<p>由于自动驾驶场景中约90%的3D空间是空的，原始高分辨率占据栅格（如200x200x16）非常稀疏且内存密集。因此，OccVLA采用了<strong>潜在空间占据预测</strong>。模型首先在紧凑的潜在空间（通过下采样率r和特征维度F定义）中预测占据特征，然后通过一个预训练的VQ-VAE解码器和一个分类头，将这些特征映射回高分辨率的原始3D占据空间。</p>
<p><strong>核心模块2：运动规划（Motion Planning）</strong><br>直接让VLM预测精确的未来轨迹坐标未能充分利用其语义推理优势。因此，OccVLA将规划任务分解为两步：首先预测高层级的<strong>元动作</strong>，然后由一个轻量级规划头将元动作转化为具体坐标。</p>
<p><img src="https://arxiv.org/html/2509.05578v1/x3.png" alt="元动作与思维链数据生成流程"></p>
<blockquote>
<p><strong>图3</strong>：元动作和思维链（CoT）数据生成流程概述。(a) 元动作生成流程：车辆轨迹坐标被处理以计算加速度（用于速度动作分类），并投影到高精地图车道以通过GPT-4o进行轨迹动作分类，再经过人工精修。两部分结合产生最终的元动作。(b) 训练数据生成流程：多视角图像和相关元动作提供给GPT-4o，用于生成场景描述、推断历史运动模式并进行未来推理，形成CoT训练数据。</p>
</blockquote>
<p><strong>元动作</strong>是一个紧凑、可解释的短时驾驶意图表示，包含两个正交部分：<strong>速度动作</strong>（保持、加速、减速）和<strong>方向动作</strong>（直行、左转、右转、向左变道、向右变道、停止）。为了激发VLM的推理能力，模型在预测元动作时采用<strong>思维链（CoT）</strong> 监督：输入多视角图像和自车过去的元动作，模型首先生成场景的自然语言描述，然后基于历史元动作推断驾驶意图，最后输出预测的未来元动作。相关的CoT标注数据通过基于GPT-4o的自动化流程生成，并辅以人工检查确保质量。</p>
<p><strong>规划头</strong>是一个简单的MLP，它以预测的元动作嵌入、上一时刻的速度以及VLM输出的视觉标记作为输入，预测未来3秒的车辆位置。整个过程不依赖高级导航指令，所有决策均源于模型对场景的理解。</p>
<p><strong>训练流程</strong> OccVLA采用三阶段训练策略，如图4所示。</p>
<p><img src="https://arxiv.org/html/2509.05578v1/x4.png" alt="训练流程"></p>
<blockquote>
<p><strong>图4</strong>：训练流程概述。阶段1：在自动驾驶场景数据上对VLM进行预训练。阶段2：进行占据-语言联合训练以增强3D场景理解。阶段3：规划头训练，利用VLM生成的元动作预测未来坐标。</p>
</blockquote>
<ol>
<li><strong>自动驾驶场景预训练</strong>：在自动驾驶数据上对VLM进行微调，使其适应特定对象类型和运动预测等任务。</li>
<li><strong>占据-语言联合训练</strong>：这是核心阶段，使用完整的图像-占据-文本数据对模型进行监督。总损失函数为语言建模的自回归损失 $L_{AR}^{text}$ 和占据预测的非自回归交叉熵损失 $L_{NAR}^{occ}$ 的加权和（公式3）。该阶段通过占据监督从视觉特征中提取3D信息表示，同时通过文本描述确保3D场景的一致性。</li>
<li><strong>规划头训练</strong>：固定VLM参数，使用其预测的元动作以及当前速度、视觉标记等输入来训练规划头，损失函数为预测坐标与真实坐标之间的均方误差（MSE）。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在nuScenes数据集上进行评估，包括其衍生的Occ3D占据数据集和NuScenes-QA视觉问答数据集。模型使用Paligemma2-3B-224px作为VLM主干，VQ-VAE解码器权重来自OccWorld。</p>
<p><strong>运动规划结果</strong>：如表1所示，OccVLA仅使用相机输入和可大规模标注的占据作为监督，在nuScenes的端到端运动规划任务上取得了0.28米的平均L2距离，达到了最先进的性能，优于依赖显式3D/BEV坐标监督的EMMA和需要相机与激光雷达双重输入的OmniDrive。</p>
<p>表1：nuScenes上端到端运动规划实验（不同输入和监督）。L代表激光雷达输入，C代表相机输入。</p>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">Input</th>
<th align="left">Supervision</th>
<th align="left">L2(m) ↓ 1s</th>
<th align="left">2s</th>
<th align="left">3s</th>
<th align="left">Avg.</th>
</tr>
</thead>
<tbody><tr>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
</tr>
<tr>
<td align="left">EMMA</td>
<td align="left">C</td>
<td align="left">None</td>
<td align="left">0.14</td>
<td align="left">0.29</td>
<td align="left">0.54</td>
<td align="left">0.32</td>
</tr>
<tr>
<td align="left">OmniDrive</td>
<td align="left">C &amp; L</td>
<td align="left">None</td>
<td align="left">0.14</td>
<td align="left">0.29</td>
<td align="left">0.55</td>
<td align="left">0.33</td>
</tr>
<tr>
<td align="left"><strong>Ours</strong></td>
<td align="left"><strong>C</strong></td>
<td align="left"><strong>Occ</strong></td>
<td align="left"><strong>0.18</strong></td>
<td align="left"><strong>0.26</strong></td>
<td align="left"><strong>0.40</strong></td>
<td align="left"><strong>0.28</strong></td>
</tr>
</tbody></table>
<p>表2进一步对比了将占据作为LLM输入的方法（如Occ-LLM）。OccVLA直接将占据预测过程集成到LLM中，仅用3B参数的模型就取得了与使用真实占据作为输入的Occ-LLM相竞争甚至更优的性能（0.28米 vs 0.43米），凸显了将占据作为输出和监督信号的优势。</p>
<p><strong>视觉问答结果</strong>：如表3所示，在挑战性的NuScenes-QA基准测试中，仅使用3B参数和纯图像输入的OccVLA，在整体准确率上达到了59.5%，超越了依赖激光雷达输入或更大参数量的模型（如7B的LiDAR-LLM和8B的OccLLaMA3.1）。这表明通过占据监督，模型从纯视觉输入中获得了更深层的3D理解能力。</p>
<p>表3：NuScenes-QA上的定量结果对比。</p>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Size</th>
<th align="left">Input</th>
<th align="left">exist(%) ↑</th>
<th align="left">count(%) ↑</th>
<th align="left">object(%) ↑</th>
<th align="left">status(%) ↑</th>
<th align="left">comparison(%) ↑</th>
<th align="left">acc(%) ↑</th>
</tr>
</thead>
<tbody><tr>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
</tr>
<tr>
<td align="left">OccLLaMA3.1</td>
<td align="left">8B</td>
<td align="left">Occ</td>
<td align="left">80.9</td>
<td align="left">19.2</td>
<td align="left">46.3</td>
<td align="left">47.8</td>
<td align="left">66.6</td>
<td align="left">54.5</td>
</tr>
<tr>
<td align="left">OpenDriveVLA</td>
<td align="left">7B</td>
<td align="left">C</td>
<td align="left">84.2</td>
<td align="left">22.7</td>
<td align="left">49.6</td>
<td align="left">54.5</td>
<td align="left">68.8</td>
<td align="left">58.2</td>
</tr>
<tr>
<td align="left"><strong>Ours</strong></td>
<td align="left"><strong>3B</strong></td>
<td align="left"><strong>C</strong></td>
<td align="left"><strong>84.3</strong></td>
<td align="left"><strong>21.9</strong></td>
<td align="left"><strong>54.5</strong></td>
<td align="left"><strong>59.5</strong></td>
<td align="left"><strong>67.2</strong></td>
<td align="left"><strong>59.5</strong></td>
</tr>
</tbody></table>
<p><strong>占据预测结果</strong>：尽管基于LLM的架构并非专为占据预测设计，且仅使用单帧图像（无时序信息），OccVLA仍能对车道、车辆、行人等关键自动驾驶元素进行有效预测，mIoU约10%。图5展示了其定性结果，模型能准确估计关键物体，但在处理被遮挡区域（如树后的建筑）时存在局限。</p>
<p><img src="https://arxiv.org/html/2509.05578v1/x5.png" alt="3D占据预测结果"></p>
<blockquote>
<p><strong>图5</strong>：OccVLA的3D占据预测结果，展示了对关键物体（如车辆、道路等）的准确估计。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>占据监督的作用</strong>：如表4所示，移除占据监督后，模型在元动作预测的平均准确率从66.37%下降到65.50%，整体VQA准确率从43.08%下降到41.48%。这表明占据监督通过增强VLM视觉特征中的3D先验，有效提升了模型的3D理解和任务性能。</li>
<li><strong>自车轨迹信息的作用</strong>：如表5所示，在运动规划任务中，如果不提供自车历史轨迹信息，平均L2距离会从0.28米显著恶化到0.48米。这证明了历史运动状态对于准确预测未来轨迹至关重要。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了OccVLA框架，通过将3D占据预测作为隐式监督信号，有效增强了纯视觉VLM的细粒度3D空间理解能力，且不依赖额外的3D传感器输入；2) 设计了可跳过的占据预测分支，在推理时无需执行该分支，避免了由3D处理带来的额外计算延迟；3) 在nuScenes基准测试的运动规划和3D视觉问答任务上均达到了最先进的性能，证明了该框架的有效性。</p>
<p>论文提到的局限性主要在于占据预测任务本身：由于模型仅使用当前时刻的单帧图像，缺乏时序信息，因此在处理严重遮挡区域时能力受限。</p>
<p>这项工作为基于纯视觉的自动驾驶提供了一个可扩展且高效的解决方案。其核心启示在于，将密集的3D几何表示作为一种自监督或弱监督信号，是弥补2D视觉模型与3D物理世界理解之间鸿沟的有效途径。这种“预测即理解”的思路，以及推理时灵活绕过复杂中间过程的设计，对未来构建更高效、更通用的具身智能模型具有借鉴意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出OccVLA模型，旨在解决自动驾驶中多模态大语言模型缺乏稳健3D空间理解的核心问题。该方法创新性地将密集3D占用表征同时作为预测输出和隐式监督信号，使模型能够直接从2D视觉输入中学习细粒度空间结构，且推理时可跳过占用预测而不增加计算开销。在nuScenes基准测试中，该模型在轨迹规划任务上取得了最先进的结果，并在3D视觉问答任务上表现出优越性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.05578" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>