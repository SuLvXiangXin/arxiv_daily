<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>UrbanVLA: A Vision-Language-Action Model for Urban Micromobility - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>UrbanVLA: A Vision-Language-Action Model for Urban Micromobility</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.23576" target="_blank" rel="noreferrer">2510.23576</a></span>
        <span>作者: Li, Anqi, Wang, Zhiyong, Zhang, Jiazhao, Li, Minghan, Qi, Yunpeng, Chen, Zhibo, Zhang, Zhizheng, Wang, He</span>
        <span>日期: 2025/10/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>城市微移动应用（如配送机器人）需要在动态、非结构化的城市环境中进行可靠的长距离导航。当前主流方法存在关键局限性：传统SLAM方法依赖详细地图，在大型动态环境中可扩展性差；基于学习的方法通常将导航简化为点目标导航任务，并利用消费级导航工具（如谷歌地图）提供高层路径点作为引导。然而，这些工具提供的路径点仅保留了粗略的拓扑连续性，缺乏几何精度，导致路径点与现实世界频繁错位，使得现有方法难以应用于大规模真实城市环境。近期，面向导航的视觉-语言-动作模型展现了强大的性能和泛化能力，但它们仍不足以应对长距离城市导航的挑战，需要解释导航应用提供的噪声路径，将其与视觉线索对齐，并同时遵守复杂的交通规则和社会规范。</p>
<p>本文针对上述痛点，提出了一个路径条件化的视觉-语言-动作框架。核心思路是：设计一个以结构化路径描述为输入的VLA模型，通过两阶段训练（监督微调与强化微调），使模型学会将噪声的“路径手册”与视觉观察对齐，并直接预测用于路径跟随的局部轨迹航点，从而实现可靠、长视野的大规模城市导航。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架基于预训练的导航基础模型NavFoM，采用两阶段微调策略。输入为高层目标路径（一系列2D坐标）和多视角RGB图像观测序列。输出是机器人当前以自我为中心坐标系下的导航轨迹（包含位置和方向的航点序列）。模型首先通过提示模板将高层路径指令编码为语言形式，并与视觉令牌一起输入大型语言模型骨干网络。在监督微调阶段，模型学习执行视频问答和路径条件化导航两项任务；在强化微调阶段，使用隐式Q学习算法在混合仿真与真实数据上进一步优化策略，以增强安全性和适应性。</p>
<p><img src="https://arxiv.org/html/2510.23576v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：UrbanVLA整体流程。模型通过两阶段流程训练：SFT阶段学习基本城市导航能力（如到达目标、避障、社会合规）；RFT阶段使用IQL在仿真-真实聚合数据集上精炼模型，增强在真实场景中的鲁棒性。架构展示了多视角视觉编码、语言指令嵌入、LLM处理及双分支（语言头和动作头）解码过程。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><strong>高层路径编码</strong>：将高层导航路径转换为结构化语言表示。包括两部分：a) 从路径中采样未来40米（每2米一个，共20个）的航点，提供路径几何和方向；b) 通过拐角检测算法分割路径为路段，并生成下一个转弯的距离和方向指令（如“30米后右转”）。这些信息被格式化为导航指令。</li>
<li><strong>VLA模型前向传播</strong>：使用视觉滑动窗口保留最近的k帧观测。采用DINOv2和SigLIP两个预训练视觉编码器提取特征，拼接后经网格池化和跨模态投影器映射为视觉令牌。语言指令被嵌入为语言令牌。所有令牌馈入Qwen2 LLM骨干网络。对于导航任务，解码当前时间步生成的动作令牌，通过一个基于MLP的动作模型得到导航轨迹；对于视频问答任务，则自回归生成语言令牌并通过语言模型头解码。</li>
<li><strong>监督微调</strong>：使用仿真环境（MetaUrban）中PPO专家生成的轨迹和网络视频（Sekai）中解析的真实世界轨迹进行SFT。关键创新是引入了<strong>启发式轨迹提升算法</strong>：从原始轨迹（仿真中的ORCA路径或去噪后的网络轨迹）中提取高层路径信息。该算法通过检测显著拐点形成粗略航点，分割轨迹，对每个路段添加高斯位置噪声以模拟真实导航的模糊性，最后平滑合并并重采样生成抽象路径。这使得模型能从视觉线索学习，而非过度依赖理想的路径输入。训练使用均方误差损失。</li>
<li><strong>强化微调</strong>：在SFT基础上，采用基于隐式Q学习的离线强化学习进一步优化。将导航任务建模为部分可观测马尔可夫决策过程。状态s由LLM骨干网络中间层（第17层）的最后一个令牌的隐藏状态表示，该表示整合了视觉和语言上下文。动作a对应模型预测的导航轨迹向量。奖励函数设计兼顾效率与安全，包含路径完成度增量、碰撞指示和偏离路径走廊指示。在仿真（约40小时）和真实世界人类遥操作（约8小时）的混合数据集上进行训练。策略通过优势加权回归目标进行更新，使模型学会处理真实世界中的边缘情况。</li>
</ol>
<p>与现有方法相比，创新点体现在：1) 首次提出用于城市微移动的路径条件化VLA，将高层导航工具引导与VLA策略学习集成；2) 通过HTL算法构建仿真-真实聚合数据集，实现仿真到真实的迁移；3) 引入基于IQL的强化微调，显式提升安全关键行为（避障、行人交互、交通合规）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台与数据集：主要使用MetaUrban模拟器中的PointNav和SocialNav基准进行评测。模型在MetaUrban-train子集上训练，在MetaUrban-test（1000场景）和MetaUrban-unseen（100场景）上测试。同时进行了大规模真实世界实验，覆盖天桥、人行横道、密集静动态障碍等多种场景，使用Unitree Go2机器人和Amap导航API。</p>
<p>对比基线：包括基于RL的方法（PPO）、安全RL方法（PPO-Lag, PPO-ET）、离线RL方法（IQL, TD3+BC）和模仿学习方法（BC, GAIL）。所有基线使用LiDAR观测，而UrbanVLA仅使用RGB观测。</p>
<p>评估指标：成功率（SR）、路径长度加权成功率（SPL）、累积成本（CC，评估避障能力）和社会导航得分（SNS，评估社会规范合规性）。</p>
<p><strong>关键实验结果</strong>：<br>在MetaUrban基准测试中，UrbanVLA在所有任务和指标上均显著优于基线。具体数值如下表所示：</p>
<ul>
<li><strong>PointNav任务</strong>：在测试集上，SR达到94%（比最佳基线PPO的66%提升28个百分点），SPL为0.91；在未见集上，SR达到97%（比最佳基线PPO-Lag的60%提升37个百分点），SPL为0.95。这表明模型在未知城市环境中具有强大的泛化能力和高效的目标到达行为。</li>
<li><strong>SocialNav任务</strong>：在测试集上，SR为91%，SNS为0.87；在未见集上，SR为88%，SNS为0.85。其SNS得分远超所有LiDAR基线（最佳基线IQL为0.67），证明了模型在有效避障的同时，能很好地遵守社会导航规范。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.23576v1/x2.png" alt="真实世界部署"></p>
<blockquote>
<p><strong>图2</strong>：UrbanVLA真实世界部署系统。包含搭载GPS、Wi-Fi、摄像头和计算单元的四足机器人，以及一个用于实时监控、发送导航目标、可视化地图和模型预测、标注遥操作数据的移动控制台。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.23576v1/x3.png" alt="定性结果"></p>
<blockquote>
<p><strong>图3</strong>：真实世界场景的定性实验结果可视化。展示了UrbanVLA在四个关键场景（过街天桥、街道转弯、人行横道、避障）中的表现。蓝色轨迹为模型在第一人称视角下生成的可执行且合理的轨迹。</p>
</blockquote>
<p><strong>消融实验总结</strong>：论文虽未以独立章节呈现系统的消融实验，但其两阶段训练设计本身验证了各组件贡献。SFT阶段使模型掌握了基本的路径跟随和目标到达能力，并利用HTL算法提升了对噪声路径的泛化能力；随后的RFT阶段，特别是基于IQL的强化微调，进一步显著增强了模型在安全关键场景（如避障和社会合规）中的决策能力，这在SocialNav任务性能的大幅提升中得以体现。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个用于城市微移动的路径条件化视觉-语言-动作模型，将消费级导航工具的高层引导与端到端的VLA策略学习相结合，解决了长距离城市导航中路径-视觉错位的挑战。</li>
<li>设计了一个仿真到真实的训练流程，创新性地提出了启发式轨迹提升算法，从演示数据中提取高层路径信息，构建了可用于监督微调的仿真-真实聚合数据集。</li>
<li>引入了基于隐式Q学习的离线强化微调，利用混合仿真与真实数据有效提升了模型在安全关键行为上的性能，如障碍物避免、行人交互和交通规则遵守。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，模型性能仍部分依赖于高层导航工具提供的路径质量，在路径信息严重错误或缺失的情况下可能面临挑战。此外，尽管系统运行频率（2Hz）足以实现平滑导航，但基于大型VLA模型的计算需求对嵌入式部署提出了要求。</p>
<p><strong>对后续研究的启示</strong>：UrbanVLA展示了将基础模型与特定领域强化学习相结合，以解决复杂具身智能任务的可行性。其路径条件化框架和两阶段训练策略为其他需要结合高层规划与低层控制的机器人应用提供了参考。未来的工作可以探索如何进一步减少对精确先验路径的依赖，增强模型在完全未知环境中的探索和推理能力，并优化模型效率以实现更低功耗的在线部署。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出UrbanVLA，一种面向城市微移动（如配送机器人）的视觉-语言-动作模型，旨在解决动态、非结构化大规模城市环境中遵循长路径指令的可靠导航问题。其核心方法是通过显式对齐噪声路径点与视觉观测来规划轨迹，并采用两阶段训练流程：先利用模拟环境和网络视频轨迹进行监督微调，再结合仿真与真实数据进行强化微调，以提升安全性与适应性。实验表明，该模型在MetaUrban的SocialNav任务上超越强基线55%以上，并在真实世界中展现出大规模环境下的可扩展性和对不确定性的鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.23576" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>