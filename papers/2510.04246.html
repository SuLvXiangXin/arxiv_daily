<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.04246" target="_blank" rel="noreferrer">2510.04246</a></span>
        <span>作者: Jang, Huiwon, Yu, Sihyun, Kwon, Heeseung, Jeon, Hojin, Seo, Younggyo, Shin, Jinwoo</span>
        <span>日期: 2025/10/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人行为克隆（BC）领域，许多任务本质上是非马尔可夫的，即仅凭当前时刻的单一观测无法做出最优决策，而需要依赖过去一系列观测提供的时序上下文。尽管利用多帧观测的重要性已被广泛认识，但现有方法在此问题上表现不一。近期研究表明，训练使用多帧观测的策略模型可能导致性能不稳定甚至下降，而另一些工作则报告了性能提升。这种不一致性使得当前主流的BC策略（包括最先进的视觉-语言-动作模型VLA）通常仅使用单帧观测进行训练，以避免潜在的性能退化风险。</p>
<p>本文针对“如何让策略模型稳定且高效地利用多帧观测”这一具体痛点，提出了新的视角。作者通过分析发现，基于视觉语言模型（VLM）构建的VLA策略模型能够有效缓解多帧训练导致的性能下降问题（如图2所示）。这表明VLM内在的时序理解能力是提取视频中有意义上下文的关键。然而，直接将高维视频序列输入庞大的VLM会带来巨大的计算开销，导致训练和推理效率低下。</p>
<p>本文的核心思路是：提出ContextVLA框架，将过去的多帧观测在VLM的中间层压缩（摊销）为单个上下文token，使策略模型能够高效地利用时序上下文生成动作，同时显著降低计算成本。</p>
<h2 id="方法详解">方法详解</h2>
<p>ContextVLA是一个用于高效训练视觉-语言-动作模型（VLA）以利用多帧观测的框架。其目标是训练一个策略模型 π_θ，该模型基于k+1帧观测 o_{t-k:t} 和语言指令 c_t，来预测未来 l+1 个机器人动作 a_{t:t+l}。</p>
<p><img src="https://arxiv.org/html/2510.04246v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：ContextVLA概览。该方法使用视觉语言模型（VLM）编码观测序列 o_{t-k:t}，其中将过去观测 o_{t-k:t-1} 在VLM的第n个块处压缩为一个上下文token m。然后利用VLM特征，通过自回归或扩散模型生成动作。</p>
</blockquote>
<p>整体流程如下：</p>
<ol>
<li><strong>输入编码</strong>：将多帧视觉观测 o_{t-k:t} 通过视觉编码器 f 转换为视觉特征 e_{t-k:t}。将其与语言指令 token c_t 拼接，形成输入序列 x = [e_{t-k:t}, c_t]。</li>
<li><strong>上下文摊销（Amortization）</strong>：这是核心创新模块。将VLM主干 g 在中间第 n 个块处分割为两部分。<ul>
<li>在前 n 个块中，处理所有输入token x，得到中间隐藏状态 h = [h_{t-k:t}, h_c]。<strong>关键细节</strong>：在处理视觉token时，无论原始VLM使用何种注意力掩码，均采用因果注意力掩码。这允许在下一时间步之前预先处理和缓存过去观测，实现高效推理。</li>
<li>然后，对过去观测（t-k 到 t-1）的隐藏状态 h_{t-k:t-1} 应用平均池化（AvgPool），压缩为单个上下文token m。</li>
<li>在剩余的 N-n 个块中，将过去观测的隐藏状态替换为上下文token m，处理序列 [m, h_t, h_c]。</li>
</ul>
</li>
<li><strong>动作解码</strong>：利用上述步骤输出的VLM特征作为条件，通过动作解码器生成动作块 a_{t:t+l}。此框架与动作解码器的具体类型无关，可兼容自回归模型（如 π_0-FAST）或基于扩散的模型（如 π_0, GR00T）。</li>
<li><strong>训练目标</strong>：最小化预测动作与专家动作之间的损失。对于自回归动作建模，使用下一个token预测损失；对于扩散动作建模，使用流匹配损失。</li>
</ol>
<p><strong>高效推理（KV-Caching）</strong>：ContextVLA通过两种机制实现高效推理。首先，由于在大部分VLM块中使用摊销后的单个token代替多帧观测，本身就减少了计算量。其次，利用因果注意力掩码，可以在时间步 t-1 预先处理 o_{t-k:t-1} 通过前 n 个VLM块，获得KV缓存和上下文token m。在时间步 t，只需使用当前观测 o_t、预计算的 m 和KV缓存来生成动作，而无需重新处理过去的观测帧。</p>
<p>与现有方法相比，创新点具体体现在：1）<strong>识别并利用VLM的时序能力</strong>：发现VLA架构能缓解多帧训练的性能退化，关键在于VLM骨干。2）<strong>高效的上下文压缩</strong>：提出在VLM中间层将过去多帧观测摊销为单个上下文token，而非处理完整视频序列，在保留时序信息的同时大幅降低计算负载。3）<strong>即插即用的通用框架</strong>：该摊销方案独立于动作解码器类型，可应用于各种现有VLA。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在模拟环境中使用了Libero（含Spatial, Object, Goal, Long四个子集）、Simpler-WidowX（Real-to-Sim挑战）和Robocasa（多样化厨房场景）三个机器人操作基准。在真实世界中设计了需要时序理解的任务，如“握紧/松开手”、“两次抓放”（PnP Twice）、“覆盖与堆叠”（CoverNStack）。</li>
<li><strong>实验平台</strong>：使用4张NVIDIA A100 80GB GPU进行训练效率比较，单张A100进行推理时间测试。</li>
<li><strong>Baseline方法</strong>：对比了近年来的开源VLA，包括Octo、OpenVLA、TraceVLA、SpatialVLA、NORA、π_0、GR00T N1.5、π_0-FAST等。重点对比了π_0、GR00T N1.5和π_0-FAST的单帧版本与使用ContextVLA的多帧版本。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.04246v1/x2.png" alt="多帧观测对不同模型的影响"></p>
<blockquote>
<p><strong>图2</strong>：(a) 传统策略模型（扩散策略）使用多帧观测训练时性能显著下降，而VLA模型（π_0和GR00T N1.5）则不会。(b) 克服此问题的关键因素是利用预训练的VLM来提取时序信息以生成动作。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟任务性能提升</strong>：ContextVLA在三个模拟基准上均能一致提升基线VLA的性能。<ul>
<li><strong>Libero（表1）</strong>：ContextVLA将π_0的平均成功率从94.6%提升至96.5%，GR00T N1.5从95.9%提升至97.0%。特别是在需要长时序理解的“Long”子集上，π_0从89.2%提升至93.8%。</li>
<li><strong>Simpler-WidowX（表2）</strong>：在存在Real-to-Sim视觉差距的挑战性设置中，ContextVLA对π_0的提升尤为显著，平均成功率从41.8%提升至56.2%（相对提升34.4%）。π_0-FAST从59.0%提升至70.7%。</li>
<li><strong>Robocasa（表3）</strong>：在包含多样化物体和位置的抓放（PnP）任务上，ContextVLA为π_0-FAST带来了3个百分点的提升（45.5%→48.5%），证明了其泛化能力。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.04246v1/x1.png" alt="真实世界任务结果"></p>
<blockquote>
<p><strong>图1</strong>：(b) 通过利用多帧观测，ContextVLA在真实世界机器人任务上取得了比所有基线策略更高的平均成功率(%)。</p>
</blockquote>
<ol start="2">
<li><p><strong>真实世界任务有效性（表4）</strong>：在特别需要时序理解的真实任务上，ContextVLA展现出巨大优势。例如，在“PnP Twice”任务中，使用ContextVLA微调的π_0取得了65%的成功率，而单帧基线仅为25%。在“CoverNStack”任务中，π_0从45%提升至60%。</p>
</li>
<li><p><strong>效率优势</strong>：</p>
<ul>
<li><strong>训练效率（图5）</strong>：使用ContextVLA进行多帧（8帧）训练，其每迭代步的墙钟时间甚至低于基线π_0进行单帧训练的时间，并且远低于处理完整8帧视频的朴素多帧训练。<br><img src="https://arxiv.org/html/2510.04246v1/x5.png" alt="训练效率对比"><blockquote>
<p><strong>图5</strong>：在Libero上微调π_0的墙钟时间。ContextVLA（8帧）的训练效率高于单帧基线，且显著高于朴素多帧（8帧）方法。</p>
</blockquote>
</li>
<li><strong>推理效率（表5）</strong>：对于8帧双视角观测，朴素多帧推理需227.2毫秒。仅使用上下文压缩可降至129.9毫秒。结合压缩与KV-Caching，推理时间进一步减少至96.3毫秒，效率提升显著。</li>
</ul>
</li>
<li><p><strong>消融实验（表6）</strong>：组件分析表明，<strong>上下文压缩</strong>是性能提升的主要贡献者（如在Simpler-WidowX上将π_0从41.8%提升至53.6%）。<strong>因果注意力</strong>的引入进一步带来了小幅提升（至56.2%），并且是实现高效KV-Caching的前提。实验也对比了不同的压缩位置（VLM块索引n），发现早期压缩（n=2）在性能和效率间取得了良好平衡。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>关键发现</strong>：系统分析了多帧观测在BC训练中效果不一致的问题，并首次指出基于VLM的VLA架构能够有效缓解由此带来的性能下降，其关键在于VLM骨干网络具备的时序理解能力。</li>
<li><strong>高效框架</strong>：提出了ContextVLA，一个新颖且高效的框架。其核心创新在于将过去的多帧观测在VLM中间层摊销（压缩）为单个上下文token，使模型能够以较低的计算开销利用时序信息。</li>
<li><strong>实证验证</strong>：在多个模拟和真实机器人操作基准上进行了广泛实验，证明ContextVLA能够一致提升现有先进VLA的性能，特别是在需要时序理解的长视野任务上，同时显著降低了训练和推理成本。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，将过去观测压缩为单个token是一种有损压缩，可能会丢失一些细节信息。此外，该方法的效果依赖于预训练VLM的质量及其时序理解能力。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>利用基础模型先验</strong>：这项工作展示了充分利用大型预训练基础模型（如VLM）内在能力（此处为时序理解）来增强机器人策略的有效性。</li>
<li><strong>效率与性能的权衡</strong>：提出的“摊销”思想为处理序列决策问题中的高维历史信息提供了一种高效的范式，平衡了模型性能与计算开销，可启发其他需要长上下文建模的研究。</li>
<li><strong>通用框架设计</strong>：ContextVLA作为与动作解码器无关的即插即用框架，展示了其良好的通用性和可扩展性，为未来集成更强大的VLM或探索其他上下文压缩机制奠定了基础。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ContextVLA模型，旨在解决机器人任务中利用多帧观测时性能提升不稳定且计算开销大的问题。其关键技术是将历史观测序列压缩为单个上下文令牌，使视觉-语言-动作模型能高效利用时序信息进行动作生成。实验表明，该方法相比单帧VLA模型持续提升任务性能，同时达到了全多帧训练的效果，并显著降低了训练与推理时间。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.04246" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>