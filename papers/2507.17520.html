<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.17520" target="_blank" rel="noreferrer">2507.17520</a></span>
        <span>作者: Yang, Shuai, Li, Hao, Chen, Yilun, Wang, Bin, Tian, Yang, Wang, Tai, Wang, Hanqing, Zhao, Feng, Liao, Yiyi, Pang, Jiangmiao</span>
        <span>日期: 2025/07/23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从大规模预训练视觉语言模型（VLM）初始化并利用大规模具身数据训练的视觉-语言-动作（VLA）模型，在机器人操作任务中展现出强大的性能。然而，这些模型在适应领域特定的机器人数据时，普遍面临灾难性遗忘的问题，即逐渐丧失了从网络规模预训练中继承的丰富多模态推理能力。这主要源于两个挑战：现有大规模机器人数据集缺乏多样化的任务场景指令，训练多局限于简单的模板化命令；同时，仅针对机器人数据的训练会加速通用多模态理解能力的侵蚀。现有缓解方法存在局限：联合训练视觉语言和操作数据的方法（如ChatVLA、Magma）往往忽略了复杂的具身推理；而将链式思维（CoT）推理嵌入操作数据集的方法（如ECoT、Emma-X）则基于预训练的动作架构和结构化推理模式，限制了通用多模态能力。</p>
<p>本文针对VLA模型中多模态理解与动作生成难以兼得的核心痛点，提出了<strong>视觉-语言-动作指令调优（VLA-IT）</strong>的新训练范式。其核心思路是：将语言引导的动作生成视为指令遵循的一个组成部分，通过混合专家（MoE）适应框架联合优化文本推理和动作生成，从而在保持VLM灵活推理能力的同时，实现领先的操作性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>InstructVLA采用一个统一的模型进行联合语言和动作生成，其整体流程分为两阶段训练。</p>
<p><img src="https://arxiv.org/html/2507.17520v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：InstructVLA整体架构。生成包含三个步骤：(1) VLM进行异步自回归推理，(2) 生成潜在动作，(3) 动作解码。MoE适应使VLM能够在推理和潜在动作预测之间切换。流匹配动作专家以潜在动作为条件解码最终动作。</p>
</blockquote>
<p><strong>整体架构与核心模块</strong>：</p>
<ol>
<li><strong>用于文本和潜在动作生成的具身VLM</strong>：模型基于Eagle2-2B主干，在产生文本输出以保持强大语言理解能力的同时，引入N个可学习的动作查询（Action Queries）。这些查询关注VLM的隐藏状态，提取出任务相关的潜在动作表示C ∈ ℝ^(N×D)，用于下游操作规划。语言输出使用交叉熵损失 ℒ_LM 监督。</li>
<li><strong>用于语言引导潜在动作的混合适应专家（MoE Adaptation）</strong>：为实现推理与操作间的无缝切换，在LLM主干中采用以LoRA模块为专家的MoE设计。一个尺度头（scale head）通过分类隐藏状态来预测每个专家的门控系数λ_i，从而根据输入上下文和推理模式自适应地混合专家输出。公式为：h = W_0 x + Σ_{i=0}^K B_i A_i x · α_i · λ_i。这允许模型在需要时激活动作生成相关的适配器。</li>
<li><strong>作为高效动作专家的流模型</strong>：为将低级控制与高级理解解耦，动作专家负责以VLM衍生的潜在动作为条件，从图像观察中生成动作。它接收来自DINOv2的图像特征、来自VLM的潜在动作、带噪声的动作嵌入以及本体感觉等可选信息，并通过具有块级因果注意力的简单Transformer架构进行融合。使用流匹配目标 ℒ_FM 进行监督。</li>
</ol>
<p><strong>两阶段训练流程</strong>：</p>
<ul>
<li><strong>阶段1：动作预训练（Action Pretraining）</strong>：在异构操作数据上预训练模型，使其同时预测动作和基于规则标注的语言运动描述。总损失为 ℒ = ℒ_LM + ℒ_FM。此阶段仅微调动作查询的输入输出嵌入以及LLM主干上的动作LoRA适配器（共6.5亿参数），得到“专家”模型。</li>
<li><strong>阶段2：视觉-语言-动作指令调优（VLA-IT）</strong>：冻结动作专家，新增语言LoRA适配器和MoE适应的尺度头进行训练（共2.2亿参数）。关键是与精心构建的VLA-IT数据集以及多模态数据集进行联合训练，以引导多模态理解。此阶段得到“通用专家”模型。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.17520v1/x3.png" alt="数据管道"></p>
<blockquote>
<p><strong>图3</strong>：视觉-语言-动作指令调优数据示例。注释侧重于：(1) 提升场景理解（场景描述、问答对）和 (2) 学习指令遵循与规划（指令改写、上下文创建）。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法相比，InstructVLA的主要创新在于：1) 提出VLA-IT训练范式，将操作任务视为指令遵循的一部分进行统一训练；2) 设计了MoE适应框架，首次在单一模型内统一了自回归VLM语言生成与基于流的动作生成，实现了推理与执行的动态、高效切换；3) 构建了大规模、多样化的VLA-IT指令调优数据集。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准</strong>：1) <strong>多模态</strong>：使用VLMEvalKit评估，包括MMMU、MMStar、MME、OCRBench等13个基准。2) <strong>操作（域内）</strong>：SimplerEnv平台，评估在Google Robot和WidowX Robot两种本体上的泛化能力。3) <strong>操作（指令泛化）</strong>：新提出的SimplerEnv-Instruct基准，包含80个零样本任务，分为指令聚合（50个）和情境推理（30个）两个层次。</li>
<li><strong>基线</strong>：对比了多模态VLM（LLaVA-OV, Bunny等）、VLA模型（RT-1-X, OpenVLA, SpatialVLA等）和通用VLA模型（Magma, 微调的OpenVLA, ECoT）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2507.17520v1/x4.png" alt="测试案例"></p>
<blockquote>
<p><strong>图4</strong>：SimplerEnv-Instruct基准中六个代表性测试案例的可视化，以及InstructVLA在评估过程中的响应。顶部列出了其他VLA的四种主要失败模式。</p>
</blockquote>
<ol>
<li><strong>多模态理解性能（表1）</strong>：InstructVLA（通用专家，2B参数）在绝大多数多模态基准上超越了同规模的基础VLM（Eagle2）以及联合训练的基线Magma（8B）。例如，在MMMU上达到44.8%，超过Eagle2的43.1%和Magma的38.8%；在MMB上达到76.6%，超过Eagle2的74.9%。这表明其有效保留了强大的多模态能力。</li>
<li><strong>机器人操作性能（表2）</strong>：<ul>
<li><strong>SimplerEnv（域内）</strong>：InstructVLA（专家）在平均成功率上达到52.9%，比表现最佳的专家基线SpatialVLA（45.9%）**提升了30.5%**。通用专家模型也保持了强劲性能（49.4%）。</li>
<li><strong>SimplerEnv-Instruct（指令泛化）</strong>：这是核心评估。InstructVLA（通用专家）平均成功率达到**46.0%<strong>，显著超过微调的OpenVLA基线（23.9%）</strong>92%<strong>，也超过了由GPT-4o辅助提供指令解释的动作专家模型（35.6%）</strong>29%**。这证明了其卓越的指令理解和任务分解能力。</li>
</ul>
</li>
<li><strong>消融实验</strong>：论文的消融研究表明：(a) MoE适应对平衡多模态和操作性能至关重要，移除后多模态分数下降；(b) 两阶段训练是必要的，直接进行VLA-IT会导致操作性能显著下降；(c) 潜在动作缓存策略能在几乎不影响性能的前提下减少VLM前向传播次数。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了InstructVLA模型及其VLA-IT训练范式，通过MoE适应框架，在高效保留VLM预训练知识的同时，将操作作为指令遵循的组成部分进行集成。</li>
<li>设计了一套实用的VLA指令遵循数据和评估流程，包括65万条VLA-IT标注数据和手动策划的SimperEnv-Instruct基准，用于评估VLA的指令泛化能力。</li>
<li>在机器人操作任务（域内及指令泛化）、多模态基准和现实部署中均取得了领先性能，实现了直观可控的操作。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在涉及长视野任务规划和需要精确时序协调的复杂操作中，模型的性能仍不稳定。</p>
<p><strong>启示</strong>：InstructVLA证明了在单一模型内联合训练语言推理和动作生成的可行性，其MoE动态切换机制为构建兼具强理解与高执行力的一般性具身智能体提供了新思路。所提出的VLA-IT数据和评估基准为未来研究提供了重要的测试平台。这项工作为弥合直观人机交互与高效策略学习之间的鸿沟铺平了道路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作（VLA）模型在集成多模态推理与精确动作生成时，易出现灾难性遗忘和泛化能力受限的核心问题，提出了InstructVLA模型。其关键技术是Vision-Language-Action Instruction Tuning（VLA-IT）训练范式，通过多模态训练与专家混合适应，在标准VLM语料和650K样本VLA-IT数据集上联合优化文本推理与动作生成。实验显示，在领域内SimplerEnv任务上性能比SpatialVLA提升30.5%；在80任务泛化基准SimplerEnv-Instruct上，比微调OpenVLA提高92%，比GPT-4o辅助动作专家提高29%，并展示了多模态任务上的优越性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.17520" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>