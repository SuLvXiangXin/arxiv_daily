<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.17561" target="_blank" rel="noreferrer">2506.17561</a></span>
        <span>作者: Gao, Chongkai, Liu, Zixuan, Chi, Zhenghao, Huang, Junshan, Fei, Xin, Hou, Yiwen, Zhang, Yuxuan, Lin, Yudi, Fang, Zhirui, Jiang, Zeyu, Shao, Lin</span>
        <span>日期: 2025/06/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人操作领域的视觉-语言-动作模型研究正从端到端的动作生成范式，转向包含任务规划与动作生成的流水线范式。这一转变旨在应对机器人操作任务固有的层次复杂性，并已证明在复杂长视野任务中能带来更强的任务推理能力、更高的成功率与样本效率。然而，现有方法在网络架构、规划范式、表示形式以及训练数据源上差异巨大，导致研究者难以精确识别性能提升的具体来源以及需要进一步改进的组件。</p>
<p>本文针对VLA领域缺乏对任务规划范式和表示进行公平、系统比较的痛点，提出了一个统一的研究视角。核心思路是构建一个可组合的VLA模型家族（VLA-OS），通过严格控制网络架构与训练数据的一致性，系统性地剖析不同任务规划表示和范式对模型性能的独立影响，并回答关于表示、范式、瓶颈、可扩展性及性能的五个核心问题。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了VLA-OS模型系列，旨在提供一个统一且可组合的架构，以支持不同的VLA任务规划范式。其整体框架基于一个可互换的视觉语言模型主干，并配备了多种即插即用的规划头和动作头。</p>
<p><img src="https://arxiv.org/html/2506.17561v1/extracted/6558238/imgs/new_res.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：VLA-OS模型家族。左侧展示了可组合的VLM主干和各种规划头、动作头。右侧展示了本实验中使用的四种VLA-OS架构，分别对应ActionOnly-VLA (OS-A)、Integrated-VLA的隐式(OS-I-I)与显式(OS-I-E)版本，以及Hierarchical-VLA (OS-H)。所有头的参数量被限制在VLM主干的大约5%，以最小化不同模型间参数量差异的影响。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><p>**VLA-OS-A (ActionOnly-VLA)**：作为基础模型，它直接根据视觉和语言输入生成动作，不包含显式规划阶段。其设计灵感来源于块级因果注意力机制，采用集成视觉编码器（DINOV2 + SigLIP）和可扩展的Qwen2.5 LLM系列作为主干。动作头是一个与LLM层数相同的Transformer解码器，其查询可以关注来自LLM、本体感知令牌及动作令牌自身的键值对。该模型还支持3D任务，通过将多视角RGBD图像融合为带有CLIP特征的点云，并将其作为额外令牌输入动作头。</p>
</li>
<li><p>**VLA-OS-I (Integrated-VLA)**：在VLA-OS-A基础上，增加了用于任务规划的“头”。论文为三种规划表示设计了对应的规划头：</p>
<ul>
<li><strong>语言规划头</strong>：生成包含任务、计划、子任务、子任务原因等8个键的语言推理链。</li>
<li><strong>视觉规划头</strong>：生成基于位置令牌（<code>&lt;loc i&gt;</code>）的视觉推理，表示物体边界框、末端执行器轨迹和目标物体可供性。</li>
<li><strong>图像前瞻规划头</strong>：以自回归、由粗到细的方式生成未来第K步的第三人称视角目标图像。<br>这些规划头均使用块级因果注意力机制，以LLM各层的输出为条件。VLA-OS-I有两种使用模式：</li>
<li>**隐式规划 (VLA-OS-I-I)**：规划头仅作为辅助训练目标，推理时不执行，旨在通过辅助损失提升模型性能。</li>
<li>**显式规划 (VLA-OS-I-E)**：动作头的生成过程会以规划头各层的输出为条件，推理时必须先执行规划步骤再生成动作，模仿思维链推理。</li>
</ul>
</li>
<li><p>**VLA-OS-H (Hierarchical-VLA)**：使用两个独立的网络分别进行任务规划和策略学习。任务规划部分使用VLM加规划头；策略学习部分使用一个编码器-解码器结构的动作头，其输入包括原始图像、本体感知观测以及规划表示。为确保公平比较，该动作头的编码器和解码器层数仅为其他范式的一半。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17561v1/extracted/6558238/imgs/networknew.png" alt="数据注释"></p>
<blockquote>
<p><strong>图3</strong>：本工作中使用的三种任务推理数据集的格式和内容：语言推理、视觉推理和图像前瞻推理。展示了使用各种视觉语言模型进行数据注释的过程（以语言推理为例）。</p>
</blockquote>
<p>与现有方法相比，VLA-OS的核心创新在于提供了一个<strong>统一、可组合的架构平台</strong>。它通过模块化设计（统一VLM主干、可插拔规划头、标准化动作头）隔离了网络架构差异，使得在同一基准上公平、系统地比较不同规划范式（ActionOnly, Integrated, Hierarchical）和规划表示（语言、视觉、图像）成为可能，从而能够精确评估各组件对最终性能的贡献。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了多个基准测试，涵盖不同物体类别、视觉模态、环境和末端执行器，包括：LIBERO（刚性物体2D操作）、The COLOSSEUM（3D与泛化评估）、FurnitureBench（复杂长视野任务）、真实世界可变形物体操作任务（折叠手帕、展开牛仔裤、拉直绳子）、DexArt（灵巧手操作）和PerAct2（双臂操作）。对比的基线方法包括Diffusion Policy、OpenVLA、CoT-VLA、DiT Policy、π₀ 及其变体π₀-FAST。</p>
<p>关键实验结果如下：</p>
<ol>
<li><strong>模型健全性检查</strong>：在LIBERO基准上，从零开始训练的VLA-OS-A-S（使用0.5B LLM）平均成功率达到85.6%，优于或媲美多个经过微调的基线模型（如OpenVLA、CoT-VLA、DiT Policy和π₀-FAST），证明了基础模型设计的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17561v1/x1.png" alt="健全性检查结果"></p>
<blockquote>
<p><strong>图1</strong>：左图展示了四种不同的VLA范式。右图对比了不同范式的性能结果。Hierarchical-VLA在性能上通常优于ActionOnly-VLA和Integrated-VLA，但代价是更高的训练和推理成本。</p>
</blockquote>
<ol start="2">
<li><strong>规划表示实验</strong>：在LIBERO-Long任务上测试不同规划表示对Integrated-VLA和Hierarchical-VLA的影响。结果表明：<ul>
<li>对于<strong>VLA-OS-I（Integrated-VLA）</strong>，<strong>隐式规划</strong>能带来正面性能收益（例如，结合语言和图像前瞻规划的成功率从66.0%提升至73.3%），而<strong>显式规划</strong>则导致性能显著下降（例如，使用视觉规划时成功率降至52.5%）。</li>
<li>对于<strong>VLA-OS-H（Hierarchical-VLA）</strong>，结合所有三种表示（L+V+IF）效果最好，成功率达74.2%。</li>
<li><strong>视觉规划表示（V, IF）</strong> 在性能上普遍优于纯语言规划表示（L）。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17561v1/extracted/6558238/imgs/comp.png" alt="规划表示比较"></p>
<blockquote>
<p><strong>图5</strong>：VLA-OS-I-E与VLA-OS-H在出现相同规划错误时的定性比较。当规划头输出存在小错误（高亮部分）时，VLA-OS-H能够纠正行为并成功完成任务，而VLA-OS-I-E则失败。</p>
</blockquote>
<ol start="3">
<li><strong>范式比较实验</strong>：在多个基准上比较三种主流范式（以最优表示配置）。关键发现是：<strong>Hierarchical-VLA在绝大多数任务上取得了最佳或相当的性能</strong>，尤其在LIBERO-Long（74.2%）、真实世界可变形物体操作（33.6%）、PerAct2双臂任务（29.0%）和泛化能力（7.4%）上表现突出。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17561v1/x2.png" alt="基准测试环境"></p>
<blockquote>
<p><strong>图4</strong>：本评估中使用的基准测试概览。</p>
</blockquote>
<ol start="4">
<li><p><strong>瓶颈分析</strong>：通过设计新的评估指标，论文发现在LIBERO任务上，无论使用哪种规划表示，<strong>策略学习部分都比任务规划部分更具挑战性</strong>。</p>
</li>
<li><p><strong>可扩展性与预训练</strong>：</p>
<ul>
<li><strong>数据可扩展性</strong>：随着训练数据量增加，所有范式的性能都提升并趋于饱和。Hierarchical-VLA在数据量较小时表现最好。</li>
<li><strong>模型可扩展性</strong>：增大VLM主干参数量（从0.5B到7B）能带来性能提升，但同样存在收益递减现象。Hierarchical-VLA在不同规模下通常表现更优。</li>
<li><strong>预训练收益</strong>：对规划头进行预训练能为Integrated-VLA和Hierarchical-VLA带来显著性能提升（分别提升5.8%和5.6%），其中Hierarchical-VLA受益最大。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17561v1/extracted/6558238/imgs/representation_time_new.png" alt="时间成本"></p>
<blockquote>
<p><strong>图7</strong>：不同规划表示和范式下的训练（左）和推理（右）时间成本。图像前瞻规划耗时最长，Hierarchical-VLA的推理速度慢于其他范式。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.17561v1/extracted/6558238/imgs/data_scalability.png" alt="数据可扩展性"></p>
<blockquote>
<p><strong>图8</strong>：不同VLA范式在LIBERO-Long上的数据可扩展性曲线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.17561v1/extracted/6558238/imgs/model_scalability_plot.png" alt="模型可扩展性"></p>
<blockquote>
<p><strong>图9</strong>：不同规模VLM主干下，各VLA范式在LIBERO-Long上的性能表现。</p>
</blockquote>
<ol start="6">
<li><strong>真实世界定性结果</strong>：展示了VLA-OS-H在真实世界可变形物体操作任务上的成功执行示例。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17561v1/extracted/6558238/imgs/jean.png" alt="真实世界任务结果"></p>
<blockquote>
<p><strong>图10</strong>：展开牛仔裤任务的成功执行轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.17561v1/extracted/6558238/imgs/handkerchief.png" alt="真实世界任务结果"></p>
<blockquote>
<p><strong>图11</strong>：折叠手帕任务的成功执行轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.17561v1/extracted/6558238/imgs/rope.png" alt="真实世界任务结果"></p>
<blockquote>
<p><strong>图12</strong>：拉直绳子任务的成功执行轨迹。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个统一、可组合的VLA-OS模型家族，为系统研究VLA中的规划问题提供了标准化平台；2) 通过大量受控实验，实证性地回答了关于规划表示、范式、瓶颈、可扩展性和性能的五个核心问题，获得了多项关键发现；3) 开源了代码、标注数据集和模型检查点，为社区提供了宝贵的资源。</p>
<p>论文明确指出了所提方法的局限性：<strong>Hierarchical-VLA范式虽然性能优越，但其训练和推理速度较慢，成本更高</strong>。这为未来研究指明了改进方向。</p>
<p>本工作的启示在于：首先，未来的VLA研究应更注重<strong>模块化设计和公平比较</strong>，以厘清各组件贡献。其次，<strong>视觉基础规划表示</strong>的有效性值得进一步探索和应用。再者，尽管Hierarchical-VLA展现出综合优势，但开发<strong>更高效的训练和推理算法</strong>以降低其计算开销是推动其广泛应用的关键。最后，研究结果表明，在当前任务上，<strong>提升低层策略学习能力</strong>比改进高层任务规划更为紧迫。这些发现为机器人VLA模型的未来设计提供了坚实的经验依据和明确指导。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型中任务规划范式与表示方法混杂、难以系统评估的问题，提出了统一的VLA-OS架构系列，支持多种规划范式。通过设计跨物体类别、视觉模态、环境与执行器的受控实验，核心结论表明：1）视觉基础的规划表示普遍优于语言表示；2）分层VLA范式在任务性能、泛化、扩展与持续学习方面表现更优或相当，但训练与推理速度较慢。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.17561" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>