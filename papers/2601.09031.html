<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.09031" target="_blank" rel="noreferrer">2601.09031</a></span>
        <span>作者: Miao Li Team</span>
        <span>日期: 2026-01-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人操作领域，当前主流方法面临两大核心挑战。在高层语义推理方面，尽管视觉语言模型（VLM）擅长解析多模态输入并生成任务规划，但其生成的逻辑计划往往因缺乏对物理几何约束的理解而无法执行，存在“语义-几何鸿沟”。例如，面对一个被压扁的易拉罐，VLM能识别其语义，却无法感知结构形变，仍会错误地选择侧边抓取这一已不可行的技能。在底层动作生成方面，扩散策略（Diffusion Policy）虽然能生成高保真轨迹，但其迭代去噪过程带来高昂的计算延迟，不适合实时高频控制；而基于Transformer的视觉-语言-动作（VLA）模型虽能前馈生成动作，但为实现鲁棒泛化通常需要海量数据和巨大参数量，数据效率低下，且缺乏捕捉机器人末端执行器与目标物体间细粒度时空关系的显式归纳偏置。</p>
<p>本文针对上述两大痛点：I) 机器人如何有效利用几何推理可靠地选择可行的操作技能？II) 机器人如何从有限的演示中高效学习鲁棒的数据高效策略？提出了一个新视角：通过轻量级几何先验注入来弥合语义与几何的鸿沟，并通过递归脉冲特征学习来嵌入显式的时空归纳偏置。本文的核心思路是：提出一个统一的RGMP-S框架，其长视野几何先验技能选择器（LGSS）将几何常识与语义指令对齐以实现技能选择，而递归自适应脉冲网络（RASNet）则通过脉冲动态递归地提取时空一致的特征，从稀疏演示中高效学习操作策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>RGMP-S框架的整体流程如图2所示。它接收语音指令和RGB视觉观察作为输入，输出6自由度机器人动作。流程分为两个阶段：首先，LGSS模块解析指令，定位目标物体，结合其形状信息与几何先验知识，从预定义的技能库中选择一个合适的技能（如侧抓、捏取等）；随后，与所选技能对应的预训练RASNet模型被激活，通过自适应递归特征提取和高斯混合模型（GMM）细化，生成精确的关节运动序列。</p>
<p><img src="https://arxiv.org/html/2601.09031v1/x2.png" alt="方法流程"></p>
<blockquote>
<p><strong>图2</strong>：RGMP-S框架的完整流程。接收到语音指令后，机器人使用LGSS识别并定位目标物体，结合物体坐标、形状线索（通过Yolov8n-seg模型提取）和几何先验知识，从技能库中选择合适的技能。每个技能关联一个预训练的RASNet模型，该模型通过自适应递归特征提取和GMM细化来精确执行任务。</p>
</blockquote>
<p><strong>核心模块1：长视野几何先验技能选择器（LGSS）</strong><br>LGSS旨在将高层语义指令与物理几何约束对齐，以选择可行的操作技能。它包含三个组件：</p>
<ol>
<li><strong>视觉语言解释模块</strong>：使用Qwen-vl API，通过结构化提示工程（如“指令：识别图像中的目标物体并输出边界框[x1, y1, x2, y2]”）处理输入，实现目标检测与定位，准确率达93.1%。</li>
<li><strong>语义分割模块</strong>：使用经过微调的YOLOv8-seg模型，对视觉语言模块提供的边界框区域进行分割，提取物体的精确形状信息，mIoU达到97.6%。</li>
<li><strong>几何接地的CoT技能推理器</strong>：这是核心创新。它采用思维链（CoT）机制，将边界框坐标和形状特征与一个包含几何常识的提示模板相结合。提示模板明确列出了可用技能（如侧抓、抬起、顶部捏取等），并要求机器人“根据观察、边界框坐标和物体形状信息，选择无碰撞的技能并生成动作计划”。通过这种方式，模型将隐式的几何可操作性与具体的操作策略进行显式映射，确保选择的技能在几何上是可行的。</li>
</ol>
<p><strong>核心模块2：递归自适应脉冲网络（RASNet）</strong><br>RASNet负责从RGB观察中生成机器人动作，其设计重点是从稀疏演示中高效学习时空特征。它主要由空间混合块（Spatial Mixing Block）和通道混合块（Channel Mixing Block）堆叠构成，其结构如图3所示。</p>
<p><img src="https://arxiv.org/html/2601.09031v1/x3.png" alt="RASNet模块结构"></p>
<blockquote>
<p><strong>图3</strong>：空间混合块与通道混合块的结构。空间混合块使用自适应衰减机制（ADM）递归生成动态衰减因子W，并利用旋转位置编码（RoPE）引入相对于空间位置的方向感知。通道混合块通过整合通道间的相关性来重新分配通道特征响应。</p>
</blockquote>
<p><strong>技术细节</strong>：</p>
<ul>
<li><strong>递归计算与全局空间记忆</strong>：网络通过递归计算（公式2: F_k = W ⊙ F_{k-1} + (1 - W) ⊙ X_k）构建全局空间记忆，其中W是自适应衰减因子，用于平衡历史特征F_{k-1}和当前输入特征X_k。</li>
<li><strong>旋转位置编码（RoPE）</strong>：在空间混合块中引入RoPE，为特征图注入方向感知，增强对空间关系的理解。</li>
<li><strong>自适应脉冲神经元（ASN）与脉冲密集特征提取（SDFE）</strong>：这是关键创新。ASN是一个动态门控机制，用于调制特征保留，放大任务关键特征并抑制冗余噪声。SDFE（公式11: Q_A = ASN(BN(Conv(Q)))）则将脉冲动态嵌入到密集特征提取中，实现对时空信息的高效处理。</li>
<li><strong>高斯混合模型（GMM）动作精炼</strong>：最终，网络输出的特征通过GMM建模动作分布（公式15）。在推理时，通过计算马氏距离（公式20）选择最可能的高斯分量，取其均值作为最终的精炼动作输出，这有助于生成平滑且物理上合理的轨迹。</li>
</ul>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>轻量级几何调优</strong>：LGSS无需对VLM进行大规模微调，仅通过精心设计的提示和少量示例注入几何先验，实现了高效的几何-语义对齐。</li>
<li><strong>递归自适应脉冲架构</strong>：RASNet将递归计算（构建空间记忆）、脉冲神经元（动态特征选择）和RoPE（方向感知）相结合，显式地建模了操作任务固有的时空机制，显著提升了从有限数据中学习鲁棒特征的能力。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：方法在ManiSkill仿真基准和三个异构的真实机器人系统上进行评估：一个定制开发的人形机器人、一个桌面机械臂和一个商业Aloha机器人。任务涵盖了拾放、折叠毛巾、倒水、箱内取物等多种长视野操作。</p>
<p><strong>对比基线</strong>：包括扩散策略（DP）、决策变换器（DT）、RT-1以及我们的会议版本（RGMP，即无脉冲特征的版本）等。</p>
<p><img src="https://arxiv.org/html/2601.09031v1/x4.png" alt="ManiSkill仿真结果"></p>
<blockquote>
<p><strong>图4</strong>：在ManiSkill基准测试中的泛化成功率对比。RGMP-S达到了89%的成功率，比扩散策略（DP）基线高出19个百分点，显著优于其他方法。</p>
</blockquote>
<p><strong>关键数值结果</strong>：</p>
<ol>
<li><strong>泛化性能</strong>：在ManiSkill的“Unseen Object Category”泛化设置中，RGMP-S取得89%的成功率，比DP（70%）高出19%，比我们的会议版本RGMP（81%）也有提升。</li>
<li><strong>数据效率</strong>：如图5所示，在仅使用10条演示数据时，RGMP-S的成功率超过80%，而DP和RT-1均低于40%。论文指出RGMP-S的数据效率是DP的5倍。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.09031v1/x5.png" alt="数据效率对比"></p>
<blockquote>
<p><strong>图5</strong>：不同演示数据量下的成功率。RGMP-S在数据极少（10条）时仍能保持高性能，显示出卓越的数据效率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.09031v1/x6.png" alt="真实世界任务成功率"></p>
<blockquote>
<p><strong>图6</strong>：在三个真实机器人平台上执行四个长视野操作任务的平均成功率。RGMP-S在所有任务和平台上均表现最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.09031v1/x7.png" alt="组件消融研究"></p>
<blockquote>
<p><strong>图7</strong>：RGMP-S各模块的消融研究。移除几何先验（w/o GP）或脉冲特征（w/o SF）都会导致性能显著下降，证明了这两个核心设计的必要性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>图7的消融实验表明：(1) 移除几何先验（w/o GP）导致性能大幅下降，尤其在需要精细几何辨别的任务（如“Crushed Can”）上。(2) 移除脉冲特征（w/o SF，即使用会议版RGMP）会降低在复杂背景或动态干扰下的鲁棒性。(3) 同时移除两者性能最差。这验证了LGSS和RASNet中脉冲设计各自的重要贡献。</p>
<p><img src="https://arxiv.org/html/2601.09031v1/x8.png" alt="复杂背景鲁棒性"></p>
<blockquote>
<p><strong>图8</strong>：在复杂/动态背景下的定性对比。RGMP-S能成功完成任务，而基线方法（如DP、RT-1）则因注意力分散到背景干扰物而失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.09031v1/x9.png" alt="脉冲特征可视化"></p>
<blockquote>
<p><strong>图9</strong>：RASNet中自适应脉冲神经元（ASN）激活的可视化。ASN能有效聚焦于任务相关区域（如物体抓取点），抑制背景噪声。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.09031v1/x10.png" alt="GMM精炼效果"></p>
<blockquote>
<p><strong>图10</strong>：GMM动作精炼前后的轨迹对比。精炼后的轨迹（绿色）更平滑、更集中，接近真实演示（蓝色），而初始预测（红色）则较为散乱。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.09031v1/x11.png" alt="长视野任务推理过程"></p>
<blockquote>
<p><strong>图11</strong>：长视野任务（折叠毛巾）的推理过程分解。LGSS通过CoT机制将任务分解为多个子技能序列（如“抓起”、“移动”、“放下”），并依次调用相应的RASNet模块执行。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.09031v1/x12.png" alt="动态干扰测试"></p>
<blockquote>
<p><strong>图12</strong>：存在动态干扰（如突然出现的手）时的操作鲁棒性测试。RGMP-S能够适应干扰并最终完成任务，展示了其策略的稳定性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个统一的RGMP-S框架，通过结合几何先验推理与递归脉冲特征学习，同时解决了人形机器人操作中高层技能选择的几何一致性问题与底层动作生成的数据效率问题。</li>
<li>设计了长视野几何先验技能选择器（LGSS），利用轻量级提示调优和思维链机制，将VLM的语义能力与几何常识对齐，实现了在未见环境中鲁棒的技能选择。</li>
<li>提出了递归自适应脉冲网络（RASNet），通过自适应脉冲神经元和递归计算显式地建模操作任务的时空机制，能够从稀疏演示中高效提炼鲁棒特征，并实现实时前馈推理。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当面对极端形状变化或高度非结构化的环境时，仅从2D图像中推断的隐式几何先验可能仍存在不足。此外，虽然框架对动态干扰具有鲁棒性，但处理极其快速或不可预测的干扰仍需进一步研究。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>更强大的几何表示</strong>：探索如何将更丰富的3D几何信息（如点云、体素）以轻量化的方式与VLM结合，以处理更复杂的形状和物理交互。</li>
<li><strong>在线适应与学习</strong>：当前技能库是预定义的。未来可以研究如何使LGSS和RASNet具备在线学习新技能或适应新物体几何的能力，进一步提升开放世界的泛化性。</li>
<li><strong>脉冲计算的优势</strong>：本工作展示了脉冲神经网络在机器人视觉运动控制中用于高效时空特征提取的潜力，这为开发更节能、更类脑的机器人控制架构提供了新思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人操作中精确场景理解与样本高效学习两大挑战，提出RGMP-S框架。其核心技术包括：利用轻量级2D几何先验构建长时域几何先验技能选择器，实现语义指令与空间约束的精准对齐；设计递归自适应脉冲网络，通过递归脉冲参数化机器人-物体交互，以提取长时域动态特征并缓解稀疏演示下的过拟合问题。实验在Maniskill仿真与三种异构真实机器人平台上验证了方法的优越性，性能较基线提升19%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.09031" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>