<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Reinforcement Learning via Implicit Imitation Guidance - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Reinforcement Learning via Implicit Imitation Guidance</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.07505" target="_blank" rel="noreferrer">2506.07505</a></span>
        <span>作者: Chelsea Finn Team</span>
        <span>日期: 2025-06-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>深度强化学习在样本效率和稀疏奖励方面面临挑战。利用离线数据（如专家示范）来引导在线强化学习是主流方向，但现有方法存在关键局限性。一类方法（如RLPD）将示范数据初始化到回放缓冲区并过采样，这仅间接利用示范信息，引导效率有限。另一类方法（如DQfD、RFT）将模仿学习作为正则项加入RL目标，直接约束策略接近专家分布。虽然这能加速早期学习，但由于模仿目标与奖励最大化不完全一致，这种约束往往会限制策略发现更优解，从而损害长期性能。近期方法（如PEX、IBRL）训练独立的模仿学习参考策略来引导RL探索，但这需要训练强模仿策略并设计可靠的策略切换机制。</p>
<p>本文针对“如何更有效地利用先验数据来加速稀疏奖励下的RL学习，同时避免模仿约束对长期性能的潜在损害”这一痛点，提出了新视角：先验数据（如专家示范）最有价值之处在于指出哪些探索性行动可能是有效的，而不是规定最终的最优行为。因此，不应直接模仿或约束策略，而应利用示范数据隐式地引导探索过程。本文核心思路是：计算专家动作与当前RL策略动作在示范状态下的差异，将此差异视为动作空间中曾导致成功结果的方向，并学习一个状态依赖的噪声分布来结构化地扰动策略输出，从而引导探索朝向示范数据所代表的区域。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架名为数据引导噪声（Data-Guided Noise, DGN）。其核心思想是学习一个采样策略 <code>π_sampling(a|s)</code>，该策略的均值 <code>μ_θ(s)</code> 通过标准的离线策略RL算法学习以最大化奖励，而其协方差 <code>Σ_ϕ(s)</code> 则通过模仿学习从先验数据集 <code>𝒟_data</code> 中学习，目的是引导探索。整体流程是交替进行的：在环境中使用采样策略收集经验并更新RL策略；定期使用最新的RL策略均值和示范数据更新噪声分布参数。</p>
<p><img src="https://arxiv.org/html/2506.07505v1/extracted/6524743/figures/method4.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DGN方法整体框架。左侧：学习过程。通过最小化示范动作在当前采样策略下的负对数似然，来更新状态依赖的协方差矩阵 <code>Σ_ϕ(s)</code>。右侧：交互过程。在环境中执行动作时，从以RL策略均值 <code>μ_θ(s)</code> 为中心、以学习到的 <code>Σ_ϕ(s)</code> 为协方差的分布中采样，从而进行数据引导的探索。</p>
</blockquote>
<p>核心模块是<strong>学习数据引导的噪声</strong>。论文给出了两种具体实现形式。第一种是<strong>零均值高斯形式</strong>：采样策略定义为 <code>π_sampling(a|s) := 𝒩(μ_θ(s), Σ_ϕ(s))</code>。其中 <code>Σ_ϕ(s)</code> 由一个MLP参数化，输出协方差矩阵的Cholesky分解 <code>A_ϕ(s)</code>。参数 <code>ϕ</code> 通过最小化采样策略在示范数据 <code>(s, a)</code> 上的负对数似然进行优化。第二种是<strong>带残差均值的替代形式</strong>：采样策略定义为 <code>π_sampling(a|s) := 𝒩(μ_θ(s) + μ_ϕ(s), Σ_ϕ(s))</code>，即同时学习一个残差均值 <code>μ_ϕ(s)</code> 和协方差 <code>Σ_ϕ(s)</code>。论文主要采用第一种形式。</p>
<p>另一个关键设计是<strong>用于探索的数据引导扰动及退火策略</strong>。在环境交互的每一步，动作从采样策略 <code>π_sampling</code> 中采样。这仅影响探索行为，而不改变策略优化目标。为了防止学习到的噪声在策略性能超越示范后仍将其拉回次优区域，论文引入了退火机制。一种是指数退火：<code>ã_t = a_t · exp(-t/τ)</code>，其中 <code>t</code> 是环境步数，<code>τ</code> 是退火时间尺度。另一种是阈值关闭：当最近 <code>n</code> 个训练回合的成功率达到 <code>m%</code> 时，直接将噪声设为零。</p>
<p><img src="https://arxiv.org/html/2506.07505v1/extracted/6524743/figures/teaser1.png" alt="不同方法对比"></p>
<blockquote>
<p><strong>图2</strong>：利用专家数据的在线RL方法行为对比。a) 仅用示范初始化回放缓冲区未能充分利用信息；b) IL正则化RL约束策略模仿专家，可能限制找到更优解；c) IL+RL参考策略方法需要强IL策略和切换机制；d) DGN使用专家-策略动作差异来学习噪声分布，隐式引导探索，避免了显式约束。</p>
</blockquote>
<p>与现有方法相比，DGN的创新点在于：1) <strong>将先验数据的用途从“约束策略优化”转变为“引导探索采样”</strong>，避免了模仿目标与奖励最大化目标不一致的问题。2) <strong>提供了一种隐式的模仿引导</strong>，无需训练独立的强模仿策略或设计复杂的策略切换机制。3) <strong>模块化设计</strong>，可以方便地集成到标准的在线RL流程或其他利用离线数据的方法之上。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了来自<strong>Adroit</strong>（pen-binary-v0, door-binary-v0, relocate-binary-v0）和<strong>Robomimic</strong>（Lift, Can, Square, Tool Hang）的共7个具有稀疏二进制奖励的连续控制任务。实验平台基于这些标准模拟环境。</p>
<p>对比的基线方法包括：1) <strong>RLPD</strong>：用先验数据初始化回放缓冲区并过采样。2) <strong>RFT</strong>：用模仿学习预训练策略，并在RL训练中加入模仿正则项。3) <strong>IQL</strong>微调：一种离线RL算法，用于在线微调。4) <strong>IBRL</strong>：先训练一个IL策略，然后根据Q函数在IL和RL策略提出的动作间选择。</p>
<p><img src="https://arxiv.org/html/2506.07505v1/extracted/6524743/figures/robomimic_and_adroit_main.png" alt="主要结果"></p>
<blockquote>
<p><strong>图4</strong>：DGN与标准RL及模仿正则化方法在Robomimic（上）和Adroit（下）任务上的平均归一化回报对比。DGN在所有任务上均达到或超越了最佳基线的性能，且在多个任务上（如Tool Hang, Relocate）优势明显，性能提升可达2-3倍。</p>
</blockquote>
<p>关键实验结果总结如下：DGN在全部7个任务上一致地达到或超越了最佳基线的性能。在Robomimic的Tool Hang和Adroit的Relocate等公认困难的任务上，DGN相比最佳基线的性能提升最为显著，达到了2-3倍。这表明DGN能有效利用示范数据加速在稀疏奖励环境下的学习。</p>
<p><img src="https://arxiv.org/html/2506.07505v1/extracted/6524743/figures/ibrl.png" alt="与IBRL对比"></p>
<blockquote>
<p><strong>图5</strong>：DGN与IBRL在四个任务上的成功率对比。DGN的性能与IBRL相当或更优，且避免了训练独立IL策略和设计动作选择机制的需要。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.07505v1/extracted/6524743/figures/mean_ablation.png" alt="消融实验：均值形式"></p>
<blockquote>
<p><strong>图7</strong>：不同噪声均值形式的消融研究。在Can和Square任务上，零均值高斯形式（DGN (Ours)）优于带学习残差均值的形式（DGN w/ Mean）和简单的行为克隆噪声（BC Noise）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.07505v1/extracted/6524743/figures/state_cond_ablation.png" alt="消融实验：状态依赖"></p>
<blockquote>
<p><strong>图8</strong>：状态依赖协方差与各向同性协方差的消融对比。学习状态依赖的协方差（DGN (Ours)）明显优于使用固定标量方差的各向同性高斯噪声（DGN (Isotropic)）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.07505v1/extracted/6524743/figures/kl_divergence2.png" alt="策略差异分析"></p>
<blockquote>
<p><strong>图9</strong>：RL策略与专家策略之间的KL散度随时间变化。DGN策略最终与专家策略的差异大于RFT，表明DGN未被约束在专家分布内，找到了不同的、性能更优的策略。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：1) <strong>状态依赖的协方差</strong>是关键，优于固定的各向同性噪声。2) <strong>零均值高斯形式</strong>优于带学习残差均值的形式。3) <strong>退火策略</strong>对于防止后期噪声干扰是必要的。此外，分析表明DGN学到的策略最终与专家策略的差异大于正则化方法，证实了其能突破专家约束。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>数据引导噪声（DGN）框架</strong>，创新性地将先验数据的用途从约束策略优化转变为引导探索采样，通过隐式模仿信号加速RL。2) 该方法<strong>避免了显式模仿约束的常见缺陷</strong>，如目标冲突、需要强IL策略和复杂切换机制，提供了一种更简洁高效的集成方式。3) 在多个具有挑战性的稀疏奖励连续控制任务上进行了全面实验，证明DGN能取得<strong>显著的性能提升（高达2-3倍）</strong>，且优于或匹配现有先进方法。</p>
<p>论文自身提到的局限性包括：学习到的噪声分布可能在策略性能超越示范后，仍将其拉回次优区域，因此需要引入退火策略来缓解。这提示我们，如何自适应地调整或停止引导是一个值得进一步研究的问题。</p>
<p>本文对后续研究的启示在于：1) <strong>重新思考先验数据在RL中的作用</strong>：引导探索可能比直接约束策略更为有效和鲁棒。2) <strong>模块化设计思想</strong>：DGN作为一种探索引导模块，可以相对独立地与其他RL算法或离线到在线方法结合，这为算法设计提供了灵活性。3) <strong>关注探索过程本身的结构化</strong>：利用领域知识（如示范）来结构化探索噪声，是提高样本效率的一个有前景的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究稀疏奖励下强化学习的样本效率问题。针对现有方法利用先验数据（如专家示范）时存在利用不足或约束过强的问题，提出**数据引导噪声（DGN）**框架。其核心思想是：示范数据最有价值之处在于指示**哪些动作值得探索**，而非直接模仿具体动作。DGN通过学习一个**状态依赖的噪声分布**，利用专家动作与当前策略动作的差异来隐式指导探索方向，避免了显式的行为克隆约束。实验表明，该方法在七个模拟连续控制任务上，性能达到之前基于离线数据方法的**2-3倍**。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.07505" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>