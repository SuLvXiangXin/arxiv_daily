<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01501" target="_blank" rel="noreferrer">2511.01501</a></span>
        <span>作者: Georgia Chalvatzaki Team</span>
        <span>日期: 2025-11-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>物体6D位姿估计是机器人和计算机视觉的基础问题，但由于部分可观测性、遮挡和物体对称性，导致观测结果存在位姿模糊性，即同一观测可能对应多个合理的位姿假设。当前主流方法，如FoundationPose等确定性深度网络，虽然在约束良好的条件下表现优异，但它们往往过于自信，无法捕捉底层位姿分布的多模态特性。现有概率方法，如基于冯·米塞斯-费希尔分布或宾厄姆分布的方法，通常是单峰的，需要混合模型来捕捉多模态，导致计算效率低下和数值不稳定；而基于扩散或归一化流的生成模型，虽然能自然捕捉多模态，但往往依赖于中间表示或局限于合成基准测试。</p>
<p>本文针对现有确定性方法无法表示多假设位姿分布、以及现有概率方法在效率与泛化性上的局限，提出了一个新的概率视角：直接在SE(3)流形上应用流匹配来估计6D物体位姿分布。本文核心思路是利用SE(3)流匹配框架，通过采样方式估计完整的位姿分布，从而自然处理对称物体、严重遮挡等模糊情况下的不确定性。</p>
<h2 id="方法详解">方法详解</h2>
<p>SE(3)-PoseFlow的目标是给定RGB-D输入，提供基于采样的6D物体位姿分布估计，而非单一确定性解。整体流程为：首先使用现成的检测器（如Mask R-CNN或CNOS）定位物体，并提取物体中心的RGB裁剪图和部分点云；随后，通过视觉和几何编码器提取特征，并利用带掩码交叉注意力的DiT*模块融合特征，以驱动SE(3)流形上的条件流匹配；最后，通过位姿选择策略从采样的多假设中选出代表性位姿。</p>
<p><img src="https://arxiv.org/html/2511.01501v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：SE(3)-PoseFlow概述。给定RGB-D输入，使用现成检测器提取物体中心的RGB裁剪图和部分点云。视觉和几何特征、时间步以及采样的位姿被编码，并通过带掩码交叉注意力的DiT*模块融合，以预测用于SE(3)流匹配的条件速度场。该框架支持对多模态位姿假设进行概率采样，并提供两种互补的位姿选择策略：无模型聚类方法和基于模型的几何评分。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>双流编码器与特征融合</strong>：视觉流使用预训练的DINOv2 ViT提取语义块嵌入，训练时主干网络冻结。点云流使用从头训练的PointNet++编码器以保留细粒度空间结构。为防止训练崩溃，对部分点云进行零均值归一化。两种模态的特征被投影到共享的256维特征空间。连续流时间步t通过傅里叶特征编码，并通过自适应层归一化注入以调节注意力块。</li>
<li><strong>带掩码交叉注意力的DiT*模块</strong>：本文用掩码交叉注意力块取代了原始DiT的自注意力。分别为平移（ℝ³）和旋转（SO(3)）分配独立的位姿token，从而解耦两个分布。交叉注意力机制让位姿token学习与图像和点云特征的关系。通过分割导出的二值掩码指定参与注意力的有效token集合，过滤背景噪声。这种设计将计算复杂度从二次方O(n²)降低到线性O(n)。精炼后的位姿token由平移头和旋转头解码，分别产生速度场𝒗_θ^pos和𝒗_θ^rot。</li>
<li><strong>SE(3)流匹配</strong>：本文采用整流线性流来定义从随机初始位姿(R₀, p₀)到目标位姿(R₁, p₁)的概率路径。平移在ℝ³中线性插值，旋转沿SO(3)上的测地线进行。通过微分该路径得到真实条件速度场。训练时，网络通过均方误差损失预测的速度场与真实速度场。总流匹配目标函数为ℒ_FM(θ) = 𝔼[λ·ℓ_pos + ℓ_rot]，其中λ=10用于平衡平移和旋转损失的敏感度尺度。推理时，通过对学习到的向量场从(R₀, p₀)积分到t=1来生成位姿样本。</li>
<li><strong>位姿选择策略</strong>：<ul>
<li><strong>无模型聚类</strong>：对假设集合应用DBSCAN聚类，距离度量结合了旋转的测地角和平移的欧氏距离。选择最大聚类，并在𝔰𝔢(3)上计算其Karcher均值作为代表。</li>
<li><strong>基于模型的几何评分</strong>：使用倒角损失或符号距离函数损失评估位姿候选与物体几何的一致性。倒角损失计算简便无需预训练，但对点云密度和噪声敏感。SDF损失提供连续全局的表面反馈，但需要高质量网格或密集点云进行训练。两种方法均将残差转换为对数似然得分，并保留前20%的位姿。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，本文的创新主要体现在：1）首次将流匹配应用于SE(3)流形进行真实世界的6D物体位姿估计，实现了高效的概率建模；2）引入了带掩码交叉注意力的DiT*模块，提升了在遮挡和杂乱现实场景下的鲁棒性；3）提供了完整的概率框架，支持从采样、选择到下游任务（如主动感知和抓取规划）的端到端不确定性推理。</p>
<p><img src="https://arxiv.org/html/2511.01501v1/x3.png" alt="下游任务应用"></p>
<blockquote>
<p><strong>图3</strong>：说明位姿不确定性下的平均抓取位姿速度。对每个位姿假设的EquiGraspFlow速度进行平均，形成平均场，积分后可采样出对位姿模糊性鲁棒的抓取（例如，对于手柄被遮挡的杯子，倾向于选择顶部抓取）。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：实验在三个广泛使用的6D物体位姿估计基准上进行：REAL275（类别级）、YCB-Video（实例级）和LINEMOD-Occlusion（实例级，遮挡严重）。<br><strong>对比方法</strong>：对比了确定性基线（NOCS, DualPoseNet, SPD）和概率性基线（GenPose, DiffusionNOCS）。<br><strong>关键实验结果</strong>：<br>在REAL275上，本文方法在10°2cm和10°5cm阈值下取得了最高准确率，分别为76.3%和89.1%。在更严格的5°2cm和5°5cm阈值下，准确率（48.8%, 56.3%）略低于GenPose（52.1%, 60.9%），论文指出这主要由于REAL275点云稀疏嘈杂，导致本文使用的倒角距离评分效果受限。<br>在BOP数据集（YCB-V和LM-O）上，当使用SDF进行几何重评分时，本文方法取得了最佳或具有竞争力的结果。在LM-O上达到35.2% (5°5cm) 和53.7% (10°5cm)，在YCB-V上达到45.4% (5°5cm) 和68.2% (10°5cm)。</p>
<p><img src="https://arxiv.org/html/2511.01501v1/figs/results.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图4</strong>：在YCB-V、LM-O和Real275数据集上的位姿估计定性对比。展示了本文方法在多物体、遮挡及对称情况下的估计效果。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>输入模态与注意力掩码</strong>（表III）：在纹理丰富的YCB-V数据集上，结合RGB和点云输入比仅使用点云输入准确率提升超过10%，证明了视觉线索的重要性。在所有数据集上，使用KV掩码的模型均优于未使用的模型，表明掩码有助于抑制背景杂波。</li>
<li><strong>位姿选择策略</strong>（表IV）：对比了无选择、无模型聚类、基于模型（倒角距离）和基于模型（SDF）四种策略。在YCB-V上，SDF评分取得了最佳准确率（45.4%, 68.2%），证实了其在处理微小对齐误差上的优势。在REAL275上，由于缺乏网格，SDF无法应用。</li>
<li><strong>推理步数的影响</strong>（表V）：对于位姿估计，增加ODE积分步数至5步时准确率提升并饱和。对于位姿跟踪，由于初始化接近正确值，即使仅使用1步推理也能获得强劲性能（51.2%, 83.6%），体现了整流流匹配的效率。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个基于SE(3)流形流匹配的概率框架，用于6D物体位姿估计，能够通过采样自然捕捉模糊情况下的多模态不确定性。</li>
<li>引入了带掩码交叉注意力的DiT*模块，增强了模型在真实杂乱和遮挡场景下的鲁棒性。</li>
<li>展示了如何将学习到的SE(3)分布应用于下游机器人任务，如主动感知以消除视角模糊性，以及在不确定性下进行抓取规划。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在REAL275这类缺乏高质量网格或密集点云的数据集上，基于SDF的几何评分无法应用，而倒角距离评分对稀疏噪声点云敏感，这限制了位姿选择的最佳性能。</p>
<p><strong>启示</strong>：本文工作将流形感知的生成建模与机器人部署实际连接起来。其基于流匹配的概率框架为处理机器人感知中的不确定性提供了高效、可扩展的新工具。未来研究可探索将类似方法应用于其他需要不确定性推理的机器人感知任务，或进一步优化在有限几何监督下的位姿选择策略。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对6D物体姿态估计中因遮挡、对称性等导致的姿态模糊性问题，提出了一种概率框架SE(3)-PoseFlow。其核心方法是利用SE(3)流形上的流匹配技术，建模完整的6D姿态分布，提供基于样本的估计以表征不确定性，而非输出单一确定性结果。该方法在Real275、YCB-V和LM-O数据集上取得了最先进性能，并能将姿态分布估计应用于主动感知、不确定性感知抓取合成等下游机器人操作任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01501" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>