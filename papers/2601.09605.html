<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.09605" target="_blank" rel="noreferrer">2601.09605</a></span>
        <span>作者: Zsolt Kira Team</span>
        <span>日期: 2026-01-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于视觉的机器人模仿学习策略取得了显著进展，但面对分布偏移（如相机视角变化）时仍然脆弱。机器人演示数据稀缺，且通常缺乏足够的相机视角多样性。许多桌面操作数据集采用固定的第三人称相机进行观测，导致训练出的策略在部署时若相机视角发生变化，成功率会急剧下降至零。模拟仿真为大规模收集具有全面视角覆盖的机器人演示提供了一条途径，但带来了视觉上的模拟到真实（sim2real）迁移挑战。</p>
<p>现有方法存在局限性：提升模拟器真实感工程量大；域随机化（颜色、纹理、光照）在鲁棒性和性能间需要权衡；现有的非配对图像翻译方法（如CycleGAN、CUT）在机器人数据集上效果不佳，因为这些数据集内部多样性极低，且现有方法容易丢失输入图像的视角信息。本文针对从固定摄像头数据集训练出的策略对视角变化极其脆弱这一具体痛点，提出利用模拟数据生成多样视角，并通过一种新的非配对图像翻译方法保持视角一致性，从而增强下游策略的鲁棒性。本文核心思路是：仅需少量固定视角的真实数据，结合一个简单的数字孪生模拟器生成的多视角模拟数据，训练一个名为MANGO的图像翻译模型，该模型能将未见过的模拟视角逼真地转换到真实域，进而用生成的多样化数据增强策略训练。</p>
<h2 id="方法详解">方法详解</h2>
<p>MANGO（Multiview Augmentation with Novel Generated Observations）是一种新颖的非配对图像到图像翻译方法，旨在将模拟（域A）观测转换到真实（域B）域，同时保持输入模拟图像的视角。</p>
<p><img src="https://arxiv.org/html/2601.09605v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MANGO训练框架。模型在未配对的真实图像（来自固定摄像头）和模拟图像（来自多样视角）上训练。采用了一种新颖的基于分割的InfoNCE损失、一个修改版的PatchNCE损失，以及对判别器D的随机块采样和旋转正则化过程，以确保翻译过程中保留模拟视角。</p>
</blockquote>
<p>整体流程如下：输入是来自两个不相交的数据集——一个包含多样相机视角的模拟图像数据集 $\mathcal{D}_A$，以及一个仅包含固定相机视角的真实图像数据集 $\mathcal{D}_B$。输出是将模拟图像 $d_A \in \mathcal{D}_A$ 翻译成具有真实域风格但保留 $d_A$ 内容（特别是视角）的图像 $G(d_A)$。</p>
<p>核心模块包括生成器 $G$、判别器 $D$ 以及用于内容保持的对比损失函数。创新点具体体现在以下三个关键技术设计上：</p>
<ol>
<li><p><strong>高度正则化的判别器（Style Loss）</strong>：采用标准GAN损失（公式1）使生成图像具备真实域风格。针对机器人数据集中背景、桌面等内容高度重复的问题，作者对判别器进行了强正则化：不仅使用PatchGAN（仅判别局部图像块），还进一步对输入判别器的图像块进行<strong>随机位置采样和逐块随机旋转</strong>。这防止了判别器记忆重复的全局细节，使其能够对未见过的视角图像有效施加风格约束。</p>
</li>
<li><p><strong>修改的PatchNCE损失（Content Loss - Modified PatchNCE）</strong>：此损失用于保持输入与输出图像间的对应特征相似性，基于CUT方法但进行了关键修改。原始InfoNCE损失（公式2）假设来自相同空间位置的特征是正样本，其他为负样本。但在机器人数据集中，许多不同位置的图像块（如背景、桌面）由于重复纹理而高度相似，在模拟数据中甚至像素值完全相同，这会导致许多“假阴性”样本被不合理地排斥。为此，作者提出了修改的评分函数 $\tilde{\rho}<em>l(\cdot)$（公式5）：当负样本对的余弦相似度超过阈值 $\theta=0.9$ 时，将其得分乘以因子 $\alpha=0.5$，从而减轻假阴性排斥问题。使用此修改评分函数的损失记为 $\tilde{\mathcal{L}}</em>{\text{PatchNCE}}$。</p>
</li>
<li><p><strong>分割NCE损失（Content Loss - Segmentation NCE Loss）</strong>：这是本文提出的新颖损失。利用模拟器可获取地面真实分割图的优势，该损失旨在通过分割类别对生成器特征进行聚类，以确保翻译过程中保留物体边界。对于输入图像 $d_A$ 的每个特征 $\mathbf{z}_i$，其所属分割类别 $y_i$ 已知。SegNCE损失（公式6, 7）将所有属于同一分割类别的特征视为正样本，计算查询特征与同类所有其他特征之间的InfoNCE损失的平均值。这迫使生成器在特征层面区分不同物体类别，从而更好地保持物体形状和边界。</p>
</li>
</ol>
<p>最终，生成器的总损失函数为（公式8）：<br>$\mathcal{L}<em>{G} = \tilde{\mathcal{L}}</em>{\text{PatchNCE}}(G, H, \mathcal{D}<em>A) + \tilde{\mathcal{L}}</em>{\text{PatchNCE}}(G, H, \mathcal{D}<em>B) + \mathcal{L}</em>{\text{SegNCE}}(G, H, \mathcal{D}<em>A) + \mathcal{L}</em>{\text{GAN}}(G, D, \mathcal{D}_A, \mathcal{D}_B)$<br>其中包含对 $\mathcal{D}_A$ 和 $\mathcal{D}_B$ 的修改PatchNCE损失（后者为恒等损失）、分割NCE损失以及GAN对抗损失。判别器损失为负的GAN损失（公式9）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>图像翻译评估</strong>：使用“pick up coke”任务的数据集。$\mathcal{D}_A$ 包含8,098张视角随机（在100x100x84 cm空间内）的模拟图像；$\mathcal{D}_B$ 包含3,094张固定视角的真实遥操作数据图像。评估使用三个测试集：固定视角、随机视角和腕部相机视角。</li>
<li><strong>模拟实验（Sim2Sim）</strong>：在Robomimic和Mimicgen的模拟任务上进行，创建两个视觉差异大的模拟环境（域A和域B），在六个任务（未见物体、共享物体、跨 embodiment）上评估。</li>
<li><strong>真实机器人实验</strong>：在四个真实操作任务（“pick up coke”、“stack cups”、“close laptop”、“stack blocks”）上评估。使用Franka Panda机械臂和外部固定RGB相机（图3）。策略采用ACT（Action Chunking Transformer）进行训练。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.09605v1/x3.png" alt="机器人实验平台"></p>
<blockquote>
<p><strong>图3</strong>：真实机器人实验平台，包括Franka Emika Panda机械臂、腕部相机和一个仅在评估时重新定位的外部相机。</p>
</blockquote>
<p><strong>对比基线</strong>：包括无数据增强、仅使用模拟数据、模拟数据+域随机化（DR）、基于扩散模型的视角增强方法VISTA，以及图像翻译方法CUT和CycleGAN。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>图像翻译质量</strong>：如表I所示，在随机视角测试集上，MANGO取得了最低的FID分数（160.9），比次优方法降低了23分。消融实验表明，高度正则化的判别器（“Basic D”）对性能提升贡献最大，而分割NCE损失和修改的PatchNCE损失也带来了显著改进。作者指出，机器人数据集 $\mathcal{D}_B$ 的多样性（通过平均成对LPIPS度量）远低于自然图像数据集（表III），这解释了现成方法（如CUT、CycleGAN）在此领域表现不佳的原因。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.09605v1/figures/sim_data_wout.png" alt="模拟实验数据与翻译结果"></p>
<blockquote>
<p><strong>图4</strong>：Sim2Sim实验中，三个任务的训练数据示例及MANGO的翻译结果，展示了从域A到域B的视觉转换。</p>
</blockquote>
<ol start="2">
<li><p><strong>模拟实验（Sim2Sim）策略性能</strong>：如表II所示，使用MANGO增强数据训练的行为克隆（BC）策略在6个任务中的5个上取得了最高成功率。例如，在“Hammer”任务上达到86%成功率，远超VISTA的56%和仅用固定相机数据的18%。</p>
</li>
<li><p><strong>真实机器人策略鲁棒性</strong>：如表IV所示，在相机视角发生偏移的测试中，使用MANGO增强数据训练的ACT策略显著优于仅用固定摄像头真实数据训练的基线策略（后者在多个任务的偏移视角上成功率接近0）。虽然在某些任务上VISTA表现略优，但MANGO仅使用3500万参数，而VISTA依赖于45亿参数的预训练模型，且计算成本极高（MANGO比ZeroNVS快约2700倍）。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.09605v1/x4.png" alt="真实实验任务与视角"></p>
<blockquote>
<p><strong>图5</strong>：左：表IV中各任务及数据增强方法的示例图像观测。右：评估中使用的三种偏移相机视角，它们被汇总为表IV中的“Shifted Cams”列。</p>
</blockquote>
<p><strong>消融实验总结</strong>：表I的消融研究表明，每个提出的组件都对提升未见视角的翻译质量有贡献。其中，<strong>高度正则化的判别器设计影响最为关键</strong>，其次是修改的PatchNCE损失和分割NCE损失。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了MANGO，一种包含新颖分割感知对比InfoNCE损失、高度正则化判别器和修改PatchNCE损失的sim2real图像翻译方法，能够在使用固定摄像头真实数据训练时，保持未见模拟视角的一致性。</li>
<li>通过模拟和真实机器人实验证明，使用MANGO生成的演示数据能有效提升下游策略对相机位置变化的鲁棒性，其性能与基于大扩散模型的方法相当甚至更优，但计算效率极高。</li>
<li>分析了MANGO在机器人数据集上成功的原因，指出该领域数据集极低的内部多样性是现有通用图像翻译方法失效的关键，并提供了相应的度量证据。</li>
</ol>
<p><strong>局限性</strong>：方法依赖于一个（即使是简单的）数字孪生模拟器来获取多视角数据和分割标签。生成的图像质量仍有提升空间（FID分数绝对值较高）。</p>
<p><strong>启示</strong>：对于机器人数据增强这类风格和内容相对确定的任务，轻量级的GAN方法相比扩散模型具有巨大的计算效率优势，仍是实用选择。结合模拟数据生成多样视角，并通过专门的图像翻译解决视觉差异，是克服机器人演示数据稀缺和视角单一问题的有效途径。未来工作可探索将分割NCE等思想融入更高效的架构中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人视觉策略对相机视角变化敏感、真实数据稀缺且视角单一的问题，提出MANGO方法：一种基于非配对图像翻译的技术，通过引入分割条件InfoNCE损失、高度正则化判别器及改进的PatchNCE损失，保持仿真到真实转换时的视角一致性。仅需少量固定视角真实数据，该方法能生成多样化的未知视角仿真图像。实验表明，经MANGO数据增强训练的模仿学习策略，在原始策略完全失败的视角上成功率可达60%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.09605" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>