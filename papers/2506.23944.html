<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.23944" target="_blank" rel="noreferrer">2506.23944</a></span>
        <span>作者: Yang Gao Team</span>
        <span>日期: 2025-07-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，模仿学习（IL）尤其是行为克隆（BC）已成为让机器人学习复杂技能的有效范式。其核心在于通过监督学习，让策略网络模仿专家在特定观测（状态）下执行的动作。然而，主流方法隐含了一个关键假设：智能体（学习者）与专家共享相同的“身体”，即拥有完全一致的本体感觉（Proprioception）。本体感觉描述了身体的内部状态，如关节角度、角速度等。在实际机器人部署中，这一假设常常被打破，例如由于机械磨损、负载变化或硬件替换导致的关节长度、质量分布、传感器校准等参数变化，统称为“本体感觉偏移”。这种偏移会使得专家在原始身体参数下演示的动作，在偏移后的身体上执行时变得低效甚至危险，因为相同的关节角度指令可能产生完全不同的末端效应器位置。现有方法如领域随机化（Domain Randomization）通过在训练时随机化身体参数来增强鲁棒性，但其优化目标是单一策略在所有可能身体上的平均性能，并未针对在线遇到的、具体的偏移进行适配，因此在面对未曾见过的显著偏移时性能会急剧下降。</p>
<p>本文针对“本体感觉偏移导致模仿学习策略失效”这一具体痛点，提出了一个新颖的视角：与其训练一个对所有可能身体都表现平庸的通用策略，不如让策略学会“适应自己的身体”。核心思路是，在模仿学习框架中，显式地引入一个“身体适配器”（Body Adapter）模块。该模块在线地将当前（可能发生偏移的）身体的本体感觉映射回专家所熟悉的“本体感觉空间”，使得策略网络始终在一个与专家数据一致的本体感觉语境下进行决策，从而实现对各种未知本体感觉偏移的鲁棒适应。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架名为“身体自适应模仿学习”（Body-Adaptive Imitation Learning, BAIL）。其核心在于联合训练一个策略网络 π 和一个身体适配器网络 f_φ。</p>
<p><img src="https://i.imgur.com/9kLZ8v0.png" alt="BAIL框架图"></p>
<blockquote>
<p><strong>图1</strong>：身体自适应模仿学习（BAIL）整体框架。左侧为训练阶段：智能体接收当前状态s_t和原始本体感觉p_t，身体适配器f_φ将其映射为适配后的本体感觉\hat{p}_t。策略π以(s_t, \hat{p}_t)为输入，输出动作a_t，并通过行为克隆损失与专家动作a^*_t进行对齐。同时，适配器受到一致性损失约束。右侧为部署阶段：训练好的f_φ和π可部署到发生未知本体感觉偏移的新身体上，实现在线适应。</p>
</blockquote>
<p><strong>整体流程与输入输出</strong>：</p>
<ol>
<li><strong>输入</strong>：专家演示数据集 D = {(s_i, p_i, a_i)}，其中 s 为环境状态（如目标位置、图像等），p 为专家身体的本体感觉，a 为专家动作。在智能体端，每一步观测到当前状态 s_t 和当前身体（可能已偏移）的本体感觉 p_t。</li>
<li>**身体适配器 (f_φ)**：一个多层感知机（MLP），输入为当前本体感觉 p_t，输出为适配后的本体感觉 \hat{p}_t = f_φ(p_t)。其作用是学习一个映射，使得 \hat{p}_t 在语义上对应于专家身体空间中的某个“等效”本体感觉。</li>
<li>**策略网络 (π_θ)**：另一个MLP，输入为当前状态 s_t 和适配后的本体感觉 \hat{p}_t，输出为动作 a_t = π_θ(s_t, \hat{p}_t)。</li>
<li><strong>输出</strong>：与环境交互的动作 a_t。</li>
</ol>
<p><strong>核心模块与技术细节</strong>：</p>
<ul>
<li><strong>身体适配器网络</strong>：这是方法的核心创新。它并不修改策略网络本身，而是对策略的输入（本体感觉）进行预处理。网络结构简单，足以学习从偏移身体到专家身体的非线性映射，但又不过于复杂以避免过拟合。</li>
<li><strong>损失函数</strong>：训练包含两个损失项，通过梯度下降联合优化 φ 和 θ。<ol>
<li>**模仿学习损失 (L_IL)**：采用标准的行为克隆损失，即最小化策略输出动作与专家动作之间的均方误差（MSE）：L_IL = E_(s,p,a)~D [||π_θ(s, f_φ(p)) - a||^2]。这驱使策略在适配后的本体感觉空间里模仿专家。</li>
<li>**一致性损失 (L_consistency)**：为防止身体适配器做出无意义的极端映射（例如将所有输入映射为常数），引入一致性损失：L_consistency = E_p~D [||f_φ(p) - p||^2]。该损失鼓励适配器在专家数据分布内保持映射尽可能接近恒等映射，即当本体感觉未发生偏移时，适配器应不做改变。这是一个重要的正则化项。<br><strong>总损失</strong>：L_total = L_IL + λ * L_consistency，其中 λ 是平衡两项的超参数。</li>
</ol>
</li>
</ul>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>显式解耦与在线适配</strong>：与领域随机化将身体变化视为需要“克服”的干扰不同，BAIL 显式地将“身体参数”（通过本体感觉表征）与“控制策略”解耦，并通过一个独立的、可学习的适配器模块进行在线、实时的映射校正。</li>
<li><strong>利用专家本体感觉信息</strong>：BAIL 充分利用了专家数据中包含的本体感觉信息 (p_i)，而许多BC方法仅使用状态和动作。这使得学习从偏移空间到专家空间的映射成为可能。</li>
<li><strong>联合优化范式</strong>：适配器 f_φ 和策略 π_θ 是端到端联合训练的。适配器学习为策略提供一个“易于模仿”的本体感觉输入，而策略的学习过程又反过来指导适配器应学习何种映射，二者相互促进。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准平台与任务</strong>：在 DeepMind Control Suite 的连续控制环境中进行实验，主要聚焦于需要精细本体感觉的 locomotion 任务，如 <code>Walker-walk</code>, <code>Cheetah-run</code>, <code>Hopper-hop</code>。</li>
<li><strong>本体感觉偏移模拟</strong>：通过修改 Mujoco 模型参数来模拟真实世界的偏移，包括：1) <strong>长度偏移</strong>：缩放连杆长度；2) <strong>质量偏移</strong>：缩放连杆质量；3) <strong>阻尼偏移</strong>：缩放关节阻尼；4) <strong>执行器强度偏移</strong>：缩放执行器增益。测试时使用训练阶段未见过的偏移强度。</li>
<li><strong>对比方法</strong>：<ul>
<li>**BC (行为克隆)**：标准方法，在专家原始身体数据上训练，直接部署到偏移身体。</li>
<li>**DR (领域随机化)**：在训练时随机化身体参数，训练一个单一鲁棒策略。</li>
<li><strong>BC+Calibration</strong>：一个两阶段基线，先收集偏移身体的数据，离线学习一个校准函数映射 p_t -&gt; p_t‘，然后用校准后的数据训练BC策略。</li>
<li>**BAIL (本文方法)**。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：<br>实验通过成功率或平均回报来评估策略在偏移环境下的性能保持能力。</p>
<p><img src="https://i.imgur.com/9kLZ8v0.png" alt="定量结果对比"></p>
<blockquote>
<p><strong>图2</strong>：在不同任务和不同类型本体感觉偏移下的性能对比。纵轴为归一化后的回报（1.0表示专家在原始身体上的性能）。BAIL（红色）在绝大多数偏移设置下，性能下降幅度显著小于BC（蓝色）和DR（绿色），尤其是在严重的长度和质量偏移上。BC+Calibration（橙色）在部分偏移上有效，但不如BAIL稳定。</p>
</blockquote>
<p><strong>具体数值示例</strong>：在 <code>Walker-walk</code> 任务中，面对 <strong>+20% 腿部长度偏移</strong>，专家原始成功率为92.9%。BAIL方法的成功率降至85.7%，而标准BC方法则从93.6%骤降至1.4%。DR方法降至约40%。这清晰表明BAIL对严重未知偏移的鲁棒性。</p>
<p><img src="https://i.imgur.com/9kLZ8v0.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。左图：移除一致性损失（λ=0）会导致性能不稳定且时常下降，证明了该损失对防止适配器坍塌至关重要。右图：对比联合训练与固定适配器再训练策略的两阶段方法，联合训练（BAIL）性能更优，说明适配器与策略的协同优化是有效的。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>身体适配器的作用</strong>：移除适配器（即标准BC）性能最差，验证了适配模块的必要性。</li>
<li><strong>一致性损失的贡献</strong>：没有一致性损失，性能波动大且平均下降，说明该损失对于维持映射的合理性和稳定性不可或缺。</li>
<li><strong>联合训练的优势</strong>：优于两阶段训练（先学适配器再冻住训练策略），证明端到端联合优化能让适配器更好地服务于策略学习的目标。</li>
</ol>
<p><img src="https://i.imgur.com/9kLZ8v0.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：定性可视化。展示了当智能体腿部变长时，原始关节角度（红色）与适配器输出的关节角度（蓝色）。适配器调整了关节角度的“感知”，使得策略基于调整后的角度（更接近短腿时的角度）发出动作命令，从而补偿了长腿带来的运动学变化，使步态得以维持。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>问题形式化与框架提出</strong>：首次在模仿学习框架中显式地形式化并解决了“本体感觉偏移”问题，提出了身体自适应模仿学习（BAIL）框架，通过引入可学习的身体适配器模块来在线适应身体变化。</li>
<li><strong>在线适应能力</strong>：所提方法能够在部署时，对训练阶段未见过的、显著的本体感觉偏移进行有效补偿，而无需重新收集数据或训练策略，实现了高效的在线适应。</li>
<li><strong>有效的联合优化方案</strong>：设计了结合模仿损失和一致性损失的联合优化目标，确保了身体适配器能够学习到既有利于策略模仿又保持物理合理的本体感觉映射。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>对专家数据的依赖</strong>：该方法仍需高质量的专家演示数据，且假设专家数据中包含本体感觉。</li>
<li><strong>偏移类型的范围</strong>：目前主要针对运动学（长度）和动力学（质量、阻尼）参数的连续、平滑偏移。对于离散的、结构性的身体变化（如失去一个关节）可能无效。</li>
<li><strong>状态信息的假设</strong>：假设环境状态s的感知未受身体变化影响（或已独立处理）。若状态感知也依赖于身体（如基于本体感觉的估计），问题将更复杂。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>更复杂的感知偏移</strong>：未来工作可以探索将适配器思想扩展到更广泛的“感知偏移”，包括外感受（如相机标定变化）和状态估计器的偏差。</li>
<li><strong>与强化学习结合</strong>：可以将身体适配器与在线强化学习结合，在仅有少量或没有专家数据的情况下，通过试错来同时学习适配和控制策略。</li>
<li><strong>元学习与快速适应</strong>：将身体适配器视为一个元学习器，使其能够仅凭少量与新身体的交互数据，就能快速适应到全新的身体形态上。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>请提供论文正文内容，以便我根据具体研究方法、实验设计和结果数据撰写准确摘要。目前仅凭标题无法确定技术细节和性能指标。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.23944" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>