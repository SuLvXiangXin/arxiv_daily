<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.20969" target="_blank" rel="noreferrer">2504.20969</a></span>
        <span>作者: Zhang, Yiting, Li, Shichen, Shrestha, Elena</span>
        <span>日期: 2025/04/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机械搜索任务要求机器人在杂乱环境中通过一系列物理交互定位并抓取被遮挡的目标物体，面临长时程规划、遮挡和部分可观测性等挑战。当前主流方法主要基于强化学习，例如视觉推抓范式及其改进方法，它们从视觉输入直接学习策略，但通常使用独立的神经网络处理每个动作基元，忽略了动作间的相互依赖性，导致在长时程任务中容易产生级联错误和决策失误。为改进动作交互，分层强化学习被引入，通过高层网络进行动作选择，然而在数据有限的机械搜索任务中，HRL难以从稀疏数据中有效学习抽象的高层表示，常导致次优的动作选择、低效探索和错误累积。</p>
<p>本文针对上述数据低效和决策质量不高的问题，提出将领域知识嵌入到结构化优先级中的新视角，以引导和简化高层策略学习。核心思路是提出一个任务驱动的动作优先级机制，并结合可学习的上下文感知切换策略，使智能体能够根据原始感官输入进行高效、可解释的决策。</p>
<h2 id="方法详解">方法详解</h2>
<p>XPG-RL框架包含两个主要组件：感知管道和基于强化学习的决策模块。整体流程为：感知模块接收RGB-D图像，提取语义和几何特征，构建结构化的场景表示；决策模块以此表示为输入，学习一个策略来输出自适应阈值，这些阈值根据预定义的优先级规则，在离散的动作候选中进行选择，最终执行选定的动作基元。</p>
<p><img src="https://arxiv.org/html/2504.20969v2/x1.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：XPG-RL框架概述。包含两个主要部分：(1) 感知管道，处理融合的RGB-D图像以提取语义和几何上下文，构建紧凑的场景表示；(2) 基于RL的决策模块，以此表示为输入，学习策略来预测自适应阈值。这些阈值指导在优先级结构的动作候选（如目标抓取、遮挡移除和视角调整）中进行选择，实现上下文感知的高效动作执行。</p>
</blockquote>
<p><strong>感知模块</strong>：输入为RGB-D图像。RGB图像提供外观线索，使用Segment Anything Model生成所有物体的外观掩码，并结合PoseCNN模型添加语义上下文，生成专注于指定目标物体的掩码。深度图像提供空间几何信息。为了进行物体级推理，提出了对象深度映射模块：将原始深度图与RGB图像中的物体掩码融合，计算每个物体的平均深度，按从远到近排序，并用渐变的色调渲染，最终生成物体级的深度图。目标特定掩码和物体级深度图共同作为RL智能体的输入。</p>
<p><strong>决策模块</strong>：将决策过程建模为部分可观测马尔可夫决策过程。在每一步t，感知模块输出观测o_t。策略π(τ_t | o_t)将观测映射为一组决策阈值τ_t = (τ₁,ₜ, τ₂,ₜ)，其中τ₁,ₜ, τ₂,ₜ ∈ [0,1]。系统使用AnyGrasp预测目标抓取质量分数Q_target,t和遮挡物移除质量分数Q_occlude,t。这些阈值被应用于质量分数，根据算法1的优先级规则选择要执行的动作a_t ∈ 𝒜。</p>
<p>奖励函数设计为：成功提取目标奖励1000，选择不可行动作惩罚-100，其他情况惩罚-1。目标是最大化期望累积折扣奖励。采用近端策略优化算法来优化输出连续阈值τ_t的策略π。</p>
<p><img src="https://arxiv.org/html/2504.20969v2/x2.png" alt="优先级动作候选"></p>
<blockquote>
<p><strong>图3</strong>：优先级引导的动作候选。动作空间包含三个离散基元——目标抓取、遮挡移除和视角调整——按优先级从高到低排序。学习的阈值控制这些动作间的切换，使智能体能够通过按优先级顺序评估动作来做出高效、可解释的决策。</p>
</blockquote>
<p><strong>优先级引导的动作候选与选择逻辑</strong>：</p>
<ol>
<li><strong>最高优先级（目标抓取）</strong>：如果Q_target,t ≥ τ₁,ₜ，则直接抓取目标物体（任务结束）。</li>
<li><strong>中间优先级（遮挡移除）</strong>：如果Q_target,t &lt; τ₁,ₜ 且 Q_occlude,t ≥ τ₂,ₜ，则抓取并移除得分最高的遮挡物，然后重新评估条件。</li>
<li><strong>最低优先级（视角调整）</strong>：如果Q_target,t &lt; τ₁,ₜ 且 Q_occlude,t &lt; τ₂,ₜ，则启动最佳下一视角策略：构建当前场景的截断符号距离函数表示，模拟一系列候选视角，预测目标抓取质量，并选择预测质量最高的视角作为下一个相机位姿。</li>
</ol>
<p><strong>创新点</strong>：与现有方法（如传统RL的独立网络、HRL的隐式高层学习、或MPGNet的固定启发式逻辑）相比，XPG-RL的创新在于将领域知识显式结构化为优先级动作候选，并学习动态的自适应阈值来进行上下文感知的切换，而非使用固定规则或完全独立的策略网络，从而在简化学习的同时保持了灵活性和可解释性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真（Isaac Lab）和真实世界环境中进行评估。使用配备RealSense相机和ROBOTIQ平行夹爪的7自由度Kinova机械臂。物体来自YCB物体集。使用Stable-Baselines3中的PPO算法训练策略。</p>
<p><strong>评估指标</strong>：任务成功率（10个动作内成功抓取并提升目标15cm的回合百分比）、平均动作次数（每回合完成任务所需的平均动作数）、任务效率（成功率与平均动作次数的比值）。</p>
<p><strong>基线方法</strong>：</p>
<ul>
<li>Target-Oriented VPG：传统RL方法，扩展原VPG，将目标掩码作为额外输入。</li>
<li>Hierarchical Policy Learning：HRL方法，使用高层策略网络建模动作序列间的相互依赖。</li>
<li>MPGNet：在Target-Oriented VPG基础上联合学习移动、推、抓三个动作，采用分层结构和基于启发式的逻辑进行高层选择。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>表I总结了不同物体数量场景下的性能。XPG-RL在所有复杂度下均取得最高成功率和最少平均动作数。例如，在20个物体的复杂场景中，XPG-RL成功率为64%，平均动作4.94次，而MPGNet为22%和7.65次，Hierarchical Policy Learning为21%和7.45次。</p>
<p><img src="https://arxiv.org/html/2504.20969v2/extracted/6541210/figures/exp_efficiency.png" alt="效率随复杂度变化"></p>
<blockquote>
<p><strong>图4</strong>：效率随复杂度缩放。XPG-RL在所有杂乱程度下均优于基线，且相对效率增益随着物体数量增加而扩大。以Target-Oriented VPG为参考基线，XPG-RL在20物体场景中的相对效率达到6.9，显示出超过4.5倍的提升。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>表II展示了两个消融变体的结果：</p>
<ol>
<li><strong>无优先级指导</strong>：RL智能体学习一个扁平策略直接选择动作，不再使用优先级方案。这导致成功率（尤其在复杂场景）和效率显著下降。</li>
<li><strong>无NBV动作</strong>：禁用最佳下一视角策略。在简单场景中影响较小，但在复杂遮挡环境中成功率明显降低。<br>消融实验证实了优先级指导和NBV策略对于在杂乱环境中实现鲁棒高效性能都是至关重要的，且其贡献随任务复杂度增加而愈加明显。</li>
</ol>
<p><strong>真实世界实验</strong>：<br>在四个包含9个物体的真实桌面场景中进行测试（场景设置见图5）。每个场景重复5次。结果如表III所示，XPG-RL在所有场景中均取得了比基线更高的成功率和更少的平均动作次数。特别是在目标被完全遮挡的场景3中，XPG-RL能有效利用NBV调整视角以成功完成任务。</p>
<p><img src="https://arxiv.org/html/2504.20969v2/x3.png" alt="真实世界实验场景"></p>
<blockquote>
<p><strong>图5</strong>：真实世界实验场景。四个桌面设置，每个包含9个物体，展示了不同的遮挡条件，包括场景3中完全被遮挡的目标。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个任务引导的动作优先级机制，将领域知识嵌入到操作基元的结构化层次中，促进了模块化、可解释的高层策略学习，并降低了长时程任务的学习复杂度。</li>
<li>提出了XPG-RL框架，利用优先级结构学习动作基元间的上下文感知切换，最大限度地减少了冗余或错误对齐的动作，提高了整体任务效率。</li>
<li>集成了最佳下一视角策略，使机器人能够主动选择信息丰富的视角，改善了遮挡场景下的感知能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身未在正文中明确讨论方法的局限性。</p>
<p><strong>启示</strong>：XPG-RL的成功表明，将领域知识（如动作优先级）与可学习的决策策略（如自适应阈值）相结合，是提升机器人操作任务，特别是长时程、部分可观测任务中鲁棒性和效率的有效途径。这种结构化的、可解释的优先级指导为设计更高效、更透明的机器人学习系统提供了新思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对杂乱环境中机械搜索效率低下、需长视野规划和处理遮挡的问题，提出XPG-RL强化学习框架。该框架集成任务驱动的动作优先级机制和上下文感知切换策略，通过自适应阈值动态选择目标抓取、遮挡移除等动作原语；感知模块融合RGB-D与语义几何特征生成结构化场景表示。实验表明，XPG-RL在任务成功率和运动效率上优于基线方法，长视野任务效率提升高达4.5倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.20969" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>