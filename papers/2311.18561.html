<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="https://arxiv.org/abs/2311.18561" target="_blank" rel="noreferrer">2311.18561</a></span>
        <span>作者: Chen, Yurui, Gu, Chun, Jiang, Junzhe, Zhu, Xiatian, Zhang, Li</span>
        <span>日期: 2023/11/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>动态、大规模城市场景的重建因其高度复杂的几何结构和不受约束的时空动态而极具挑战。现有方法，如NSG、PNF和SUDS，通常采用高层级架构先验，将静态和动态元素分离建模。这导致模型复杂度随物体数量线性增长，且严重依赖难以获取的物体级标注（如3D边界框、分割掩码）或昂贵的预计算光流。此外，基于神经辐射场（NeRF）的方法普遍存在训练和渲染效率低下的问题，成为大规模场景应用的瓶颈。显式分离场景构成部分也引入了设计复杂性，并限制了捕捉内在关联和交互的能力。</p>
<p>本文针对上述动态城市场景重建中依赖额外标注、效率低下、以及静态动态元素分离建模的痛点，提出了一种统一表示模型的新视角。本文核心思路是：在高效的三维高斯泼溅（3DGS）静态场景表示基础上，引入基于周期性振动的时间动态，从而通过单一公式统一表示场景中的静态与动态元素，并辅以时间平滑机制和位置感知自适应控制策略，以应对稀疏数据和大尺度场景的挑战。</p>
<h2 id="方法详解">方法详解</h2>
<p>PVG方法建立在3D高斯泼溅（3DGS）之上。3DGS使用一组带属性的3D高斯点（包括位置𝝁、协方差Σ（由缩放𝒔和旋转𝒒参数化）、不透明度𝑜、颜色𝒄）来表示场景，并通过可微分的瓦片化光栅化进行实时渲染。PVG的核心创新在于将每个静态的高斯点扩展为一个随时间变化的“周期性振动高斯”点。</p>
<p><img src="https://arxiv.org/html/2311.18561v3/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：PVG通过学习自适应不透明度衰减率来区分动态和静态场景元素。动态物体由寿命短的点（快速衰减）表示，静态区域由寿命长的点表示，从而在时间上表现出一致性。学习过程由每个时间步的RGB和LiDAR深度信号监督。</p>
</blockquote>
<p><strong>整体框架</strong>：输入是多传感器序列数据，包括带时间戳、内外参的图像和LiDAR点云。目标是学习一个渲染函数 ℐ̂ = ℱ_θ(𝐄_𝑜, 𝐈_𝑜, 𝑡)，以在任意所需时间戳𝑡和相机位姿[𝐄_𝑜, 𝐈_𝑜]下合成新视图。PVG训练一组可学习的周期性振动高斯点{ℋ_𝑖}，在渲染时，根据查询时间𝑡计算每个点的瞬时状态ℋ_𝑖(𝑡)，然后送入标准的3DGS渲染管线生成图像。</p>
<p><strong>核心模块1：周期性振动高斯模型</strong>。PVG为每个点引入了三个新的可学习参数：生命峰值τ、不透明度衰减率β和瞬时速度𝒗。点的位置𝝁和不透明度𝑜被修改为以τ为中心的时间函数：</p>
<ul>
<li><strong>振动位置</strong>：𝝁̃(𝑡) = 𝝁 + (𝑙/2π) · sin(2π(𝑡−τ)/𝑙) · 𝒗。该公式使点围绕中心位置𝝁做正弦振动，𝑙是作为场景先验的周期长度，𝒗决定了振动的方向和幅度，并在𝑡=τ时代表瞬时速度。</li>
<li><strong>振动不透明度</strong>：õ(𝑡) = 𝑜 · exp(-0.5(𝑡−τ)²β⁻²)。不透明度在τ处最高，并随距离τ的增大呈高斯衰减，β控制衰减速度（即点的“寿命”）。</li>
</ul>
<p>因此，一个PVG点的完整参数集为{𝝁, 𝒒, 𝒔, 𝑜, 𝒄, τ, β, 𝒗}。<strong>静态系数</strong> ρ = β/𝑙 被用来量化点的静态程度。ρ值大的点（寿命长，β大）其位置振动范围受𝒗约束，且时间期望位置为𝝁，因此擅长表示静态场景。ρ值小的点（寿命短）在τ附近出现并做近似线性运动，随后快速消失，从而表示动态物体。通过设定ρ阈值，可以分离静态与动态成分。</p>
<p><strong>核心模块2：位置感知点自适应控制</strong>。针对无界城市场景中大部分点远离场景中心的特点，原始3DGS均匀的点控制策略（克隆、分裂、剪枝）效率低下。PVG引入了一个与点位置相关的尺度因子γ(𝝁)。当点的最大尺度 max(𝒔) ≤ 𝑔·γ(𝝁) 时触发克隆，当 max(𝒔) &gt; 𝑏·γ(𝝁) 时触发剪枝。其中γ(𝝁)在点距离原点小于2倍场景半径𝑟时为1，否则线性增长。这使得远处的点允许有更大的尺度，从而用更少的点忠实表示远处细节。</p>
<p><strong>核心模块3：基于内在运动的时间平滑</strong>。自动驾驶场景中训练数据在时间和视角上都很稀疏，容易导致过拟合。PVG利用其内在动态特性，定义了点的<strong>平均速度</strong> 𝒗̄ = 𝒗 · exp(-ρ/2)。该公式源于用不透明度衰减加权平均瞬时速度的积分，其性质是：当ρ→∞（完全静态）时𝒗̄→0，当ρ→0（高度动态）时𝒗̄→𝒗。</p>
<p><img src="https://arxiv.org/html/2311.18561v3/x2.png" alt="时间平滑机制"></p>
<blockquote>
<p><strong>图3</strong>：时间平滑机制示意图。在训练时，以概率η随机采样一个时间偏移Δt，将PVG点集在时间𝑡−Δt的状态，用其平均速度𝒗̄平移Δt后，作为时间𝑡状态的估计，并以此进行渲染监督，从而增强时间一致性。</p>
</blockquote>
<p>基于“动态物体在短时间间隔内速度恒定”的假设，PVG在训练时引入自监督。具体而言，以概率(1-η)随机采样一个时间偏移Δt ~ U(-δ, +δ)，然后将点集在时间𝑡−Δt的状态ℋ(𝑡−Δt)，通过其平均速度𝒗̄平移Δt，得到时间𝑡的估计状态ℋ̂(𝑡)（如公式12所示）。用这个估计状态进行渲染并与真实图像计算损失。这迫使模型学习连贯的运动轨迹，而无需依赖光流监督。</p>
<p><strong>其他细节</strong>：</p>
<ul>
<li><strong>天空细化</strong>：使用一个可学习的高分辨率立方体贴图（cubemap）来表示天空背景，其颜色仅依赖于观察方向，物理合理且轻量。</li>
<li><strong>损失函数</strong>：总损失ℒ包含RGB的L1和SSIM损失、LiDAR投影深度损失ℒ_d、驱动不透明度趋于0（天空）或1（物体）的损失ℒ_o，以及鼓励速度稀疏性（即场景大部分静态）的平均速度稀疏损失ℒ_𝒗̄。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在Waymo Open Dataset和KITTI两个大规模自动驾驶数据集上进行评估。对比方法包括：针对静态场景的S-NeRF、StreetSurf、3DGS；针对动态场景的NSG、SUDS、EmerNeRF；以及一些并发的基于3DGS和边界框的方法（如[44,45,47,37]）。评估任务包括图像重建和新视图合成。</p>
<p><strong>关键定量结果</strong>：在新视图合成任务上，PVG在PSNR、SSIM、LPIPS等指标上均优于所有对比方法。在KITTI数据集上，PVG的PSNR达到23.17，显著高于EmerNeRF的20.05和SUDS的21.93。在Waymo数据集上，PVG的PSNR为24.29，同样领先。更重要的是，PVG的渲染速度相比最佳的动态场景方法EmerNeRF提升了<strong>900倍</strong>。</p>
<p><img src="https://arxiv.org/html/2311.18561v3/images_kitti_novel_view_kitti_0001_pvg.jpg" alt="KITTI新视图合成定性对比"><br><img src="https://arxiv.org/html/2311.18561v3/images_kitti_novel_view_kitti_0001_gt.jpg" alt="KITTI新视图合成定性对比-GT"></p>
<blockquote>
<p><strong>图11 &amp; 图13</strong>：在KITTI数据集上的新视图合成定性结果对比（左为PVG，右为真实图像）。PVG能清晰重建移动车辆等动态物体以及静态背景的细节。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2311.18561v3/images_waymo_static_1400454_ours_front.jpg" alt="Waymo静态场景重建对比"><br><img src="https://arxiv.org/html/2311.18561v3/images_waymo_static_1400454_gt_front.jpg" alt="Waymo静态场景重建对比-GT"></p>
<blockquote>
<p><strong>图18 &amp; 图19</strong>：在Waymo数据集上静态背景重建的定性对比（左为PVG，右为真实图像）。PVG即使作为动态场景模型，其静态背景重建质量也优于或媲美专门的静态场景方法。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>时间平滑机制的有效性</strong>：移除自监督的时间平滑（即设η=1）会导致动态物体重建出现重影和模糊，PSNR下降约0.4。<br><img src="https://arxiv.org/html/2311.18561v3/images_ablation_with_selfsupervise.jpg" alt="有无时间平滑对比"><br><img src="https://arxiv.org/html/2311.18561v3/images_ablation_without_selfsupervise.jpg" alt="有无时间平滑对比-差"><blockquote>
<p><strong>图36 &amp; 图37</strong>：消融时间平滑机制。左图（使用平滑）动态物体边界清晰；右图（未使用平滑）出现明显的重影和模糊。</p>
</blockquote>
</li>
<li><strong>位置感知控制策略的有效性</strong>：不使用该策略时，模型需要更多的高斯点才能达到相近的重建质量，且远处细节表现更差。<br><img src="https://arxiv.org/html/2311.18561v3/images_more_ablation_vis_ab_pac.jpg" alt="有无位置感知控制对比"><blockquote>
<p><strong>图43</strong>：消融位置感知控制策略。不使用该策略时，远处建筑物和树木的细节显著缺失。</p>
</blockquote>
</li>
<li><strong>深度监督与速度稀疏损失的重要性</strong>：消融实验表明，二者对提升几何准确性和分离静态/动态成分至关重要。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个基于3D高斯泼溅的统一动态城市场景表示模型PVG，通过引入周期性振动的时间动态，优雅地用一个公式同时表征静态与动态元素，无需物体级标注或光流估计。</li>
<li>提出了基于内在运动的时间平滑机制，利用平均速度在相邻帧间建立自监督，有效提升了在稀疏观测下的时间一致性和鲁棒性。</li>
<li>设计了位置感知的自适应点控制策略，优化了对无界大规模场景的表示效率。实验表明，PVG在新视图合成质量上超越SOTA，并在渲染速度上获得数个数量级的提升。</li>
</ol>
<p><strong>局限性</strong>：论文提到，时间平滑机制依赖于“动态物体在短时间间隔内速度恒定”的假设，这可能不适用于具有复杂非匀速运动或突然加速度的场景。</p>
<p><strong>研究启示</strong>：PVG证明了将显式表示（3DGS）与参数化时间动态模型相结合是一条有效的动态重建路径。其“周期性振动”的设计可以看作是一种基础的运动基元，为后续研究如何用更复杂的参数化运动模型（如学习运动轨迹）来表征更广泛的动态现象提供了启发。此外，其统一建模的思想和自监督的时间一致性约束，对减少动态重建对昂贵标注的依赖具有指导意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对动态大规模城市场景重建与实时渲染的挑战，提出周期性振动高斯（PVG）模型。该方法基于3D高斯泼溅技术，通过引入周期性振动表示时间动态，并采用时间平滑机制和位置感知自适应控制策略，统一捕捉静态与动态元素的协同交互。实验在Waymo和KITTI数据集上验证，PVG在重建和新视角合成方面超越现有方法，且无需对象标注或光流估计，渲染速度比最佳替代方法快900倍。</p>
      </section>
      <div class="detail-actions">
        <a href="https://arxiv.org/abs/2311.18561" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>