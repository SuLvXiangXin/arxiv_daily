<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Humanoid World Models: Open World Foundation Models for Humanoid Robotics - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Humanoid World Models: Open World Foundation Models for Humanoid Robotics</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.01182" target="_blank" rel="noreferrer">2506.01182</a></span>
        <span>作者: Ali, Muhammad Qasim, Sridhar, Aditya, Matiana, Shahbuland, Wong, Alex, Al-Sharman, Mohammad</span>
        <span>日期: 2025/06/01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，赋能人形机器人在复杂开放世界中推理、规划和行动仍是一个挑战。世界模型（World Models）——能够预测给定动作未来结果的模型——可以作为长时域规划中的动力学模型或通过生成合成数据用于策略学习，从而支持上述能力。尽管视频生成领域取得了进展，但大多数模型为娱乐应用而构建，强调视觉吸引力而非物理上合理的第一人称视角预测。此外，许多世界模型要么闭源，要么需要大规模计算资源进行训练或推理，要么并非专为人形机器人设计。这导致了一个明显的空白：既缺乏为人形机器人物理特性设计的模型，又缺少足够轻量以便在学术或小型实验室硬件（例如1-2个GPU）上训练和部署的开源模型。</p>
<p>本文旨在填补这一空白，提出一个核心问题：能否构建一个物理上合理、专为人形机器人设计、且能在少至两个GPU上训练和部署的世界模型？为此，本文引入了人形世界模型（Humanoid World Models， HWM），这是一组基于100小时人形演示视频训练的开源、轻量级世界模型。核心思路是探索两种不同的视频生成范式（掩码Transformer和流匹配）及其在紧凑设计空间（注意力机制与参数共享策略）下的变体，以在有限计算约束下实现高性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>HWM的目标是根据过去的视频帧序列和相关的动作序列，预测物理上合理的未来视频帧。具体而言，模型预测 <code>f</code> 帧未来RGB视频 <code>v_f</code>，条件输入包括 <code>p</code> 帧过去视频 <code>v_p</code>、<code>p</code> 个过去动作 <code>a_p</code> 和 <code>f</code> 个未来动作 <code>a_f</code>。动作向量包含人形机器人的关节角度、速度和夹持器状态。与先前工作一致，生成模型在VAE的压缩潜在空间中训练。</p>
<p><img src="https://arxiv.org/html/2506.01182v2/extracted/6607028/images/overview.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：人形世界模型（HWM）概述。给定过去的视频观察和人形控制令牌（关节角度、速度等），模型预测未来的视频观察。</p>
</blockquote>
<p>本文开发了基于两种不同生成范式的模型：<strong>掩码人形世界模型（Masked-HWM）</strong> 和<strong>流人形世界模型（Flow-HWM）</strong>。</p>
<p><strong>Masked-HWM</strong> 采用掩码视频建模（MVM）范式，在VQ-VAE的离散潜在空间中操作。训练时，过去和未来的潜在令牌被拼接，未来的令牌会被随机替换（噪声）和掩码。模型被训练以在掩码位置重建原始令牌，损失函数为交叉熵损失。推理时，首先掩码所有未来令牌，然后以帧为单位并行迭代地生成和细化令牌，这比自回归方法更快。</p>
<p><strong>Flow-HWM</strong> 采用流匹配（Flow Matching， FM）框架，在连续潜在空间中操作。它将视频生成表述为从简单先验分布（高斯噪声）到目标数据分布的连续变换。模型直接学习一个时间依赖的速度场，其训练目标是最小化预测速度与真实速度之间的均方误差。推理时，通过对学习到的速度场进行积分，从纯高斯噪声开始生成视频。该模型采用了无分类器引导以改善条件生成。</p>
<p>两种模型的核心架构都是Transformer。输入（视频帧和动作序列）首先被编码或标记化到统一的 <code>h</code> 维空间，形成四个令牌流：过去视频、未来视频、过去动作、未来动作。这些令牌流被送入一系列Transformer块进行处理。</p>
<p><img src="https://arxiv.org/html/2506.01182v2/extracted/6607028/images/mvm_architecture.png" alt="掩码模型架构"></p>
<blockquote>
<p><strong>图2</strong>：Masked-HWM中单个Transformer块（基础块变体）的架构。视频和动作流被独立处理，视频流还接收空间注意力。所有流通过联合时间注意力进行交互。RoPE按注意力类型应用（空间用2D，时间用1D）。每个流在FFN阶段使用不同的MLP权重。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.01182v2/extracted/6607028/images/architecture.png" alt="流匹配模型架构"></p>
<blockquote>
<p><strong>图3</strong>：Flow-HWM中单个Transformer块（基础块变体）的架构。每个令牌流（过去/未来视频和动作）对时间步调制、QKV投影和FFN MLP使用独立的权重。联合注意力整合所有流。RoPE按模态应用：视频令牌用3D，动作令牌用1D。</p>
</blockquote>
<p>本文的关键创新在于系统性地探索了Transformer块的设计空间，重点关注两个维度：</p>
<ol>
<li><strong>注意力结构</strong>：<strong>联合注意力（Joint Attention）</strong>：所有令牌流（视频和动作）在一个统一的注意力层中交互。<strong>分离注意力（Split Attention）</strong>：先进行令牌流内的自注意力，再进行跨流的交叉注意力。</li>
<li><strong>参数共享</strong>：<strong>共享参数</strong>：不同令牌流共享Transformer块中部分或全部层的权重。<strong>独立参数</strong>：每个令牌流使用完全独立的参数。</li>
</ol>
<p>这些设计旨在权衡模型性能与计算效率。论文指出，得益于高压缩率的VAE（大幅降低时空分辨率）和近期图像生成模型中的高效参数共享技术，在视频生成中采用联合注意力变得可行。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了<strong>100小时的人形机器人演示视频</strong>进行训练。在<strong>2个NVIDIA A6000 GPU</strong>的设置下进行评估。</p>
<p><strong>对比的基线</strong>主要是本文提出的两种范式（Masked-HWM和Flow-HWM）各自内部的<strong>不同架构变体</strong>，而非与其他外部模型进行直接数值比较。实验旨在探索不同设计选择的影响。</p>
<p><strong>关键实验结果总结如下</strong>：</p>
<ol>
<li><strong>范式比较</strong>：在给定的数据集和计算约束下，<strong>掩码Transformer模型（Masked-HWM） consistently outperformed Flow-Matching models</strong>，即使后者使用了更多参数并训练了更长时间。</li>
<li><strong>架构变体性能</strong>：在大多数场景下，不同架构变体表现相当。但出现了关键趋势：在掩码Transformer框架内，<strong>联合注意力变体取得了最佳整体性能</strong>；而对于流匹配模型，<strong>分离注意力被证明最有效</strong>。</li>
<li><strong>参数共享的有效性</strong>：**参数共享策略在性能或视觉保真度影响最小的情况下，将模型参数量减少了33–53%**，显著降低了计算需求。共享参数版本与不共享版本的表现近乎相同。</li>
<li><strong>定性结果</strong>：生成的未来视频帧在视觉上连贯，并能合理反映输入动作序列的意图（例如，伸手、抓取）。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.01182v2/extracted/6607028/images/mvm_sample.png" alt="掩码模型生成示例"></p>
<blockquote>
<p><strong>图5</strong>：Masked-HWM生成的视频帧示例。左侧为条件输入（过去帧），右侧为模型预测的未来帧序列，展示了伸手动作的连贯预测。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.01182v2/extracted/6607028/images/flow-sample-1.jpeg" alt="流匹配模型生成示例1"><br><img src="https://arxiv.org/html/2506.01182v2/extracted/6607028/images/flow-sample-2.jpeg" alt="流匹配模型生成示例2"><br><img src="https://arxiv.org/html/2506.01182v2/extracted/6607028/images/flow-sample-3.jpeg" alt="流匹配模型生成示例3"></p>
<blockquote>
<p><strong>图6-8</strong>：Flow-HWM生成的视频帧示例。展示了在不同动作条件下，模型对未来场景（如桌面物体交互）的预测结果，表明模型能够生成物理上合理的未来帧。</p>
</blockquote>
<p><strong>消融实验</strong>主要围绕参数共享和注意力机制展开。结果表明，<strong>参数共享是降低模型大小和计算成本的有效策略，且性能损失可忽略不计</strong>。同时，<strong>注意力机制的选择（联合 vs. 分离）对性能的影响因生成范式而异</strong>，需要根据具体框架进行选择。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文的核心贡献</strong>包括：</p>
<ol>
<li>提出了 **Humanoid World Models (HWM)**，一个面向人形机器人的、轻量级、开源的世界模型家族，旨在使世界模型的训练和部署在有限的学术硬件上变得可行。</li>
<li>系统地比较了 <strong>掩码视频建模</strong> 和 <strong>流匹配</strong> 两种生成范式在人形机器人视频预测任务上的表现，并发现掩码Transformer在给定设置下更具优势。</li>
<li>深入探索了Transformer块的<strong>架构设计空间</strong>（注意力机制、参数共享），并证明<strong>参数共享策略能大幅减少模型参数量（33-53%）而几乎不影响性能</strong>，为构建高效世界模型提供了实用指导。</li>
</ol>
<p><strong>论文提到的局限性</strong>：模型在未见过的、高度动态或快速的动作序列上可能表现不佳；训练数据规模（100小时）相对于大型基础模型而言仍然较小。</p>
<p><strong>对后续研究的启示</strong>：这项工作为在更大规模的人形机器人数据上训练轻量级世界模型奠定了基础。未来的方向可以包括：利用更大规模、更多样化的数据集进行训练；将HWM集成到具体的规划或强化学习管道中，验证其在提升下游任务（如长时域规划、模拟到真实迁移）数据效率方面的实际效用；进一步探索其他高效的架构和训练技术。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决人形机器人在复杂开放世界中难以进行推理与规划的问题。为此，提出了一个轻量级开源模型家族——人形世界模型（HWM），其核心是训练基于人形控制令牌预测未来第一视角视频的生成模型，关键技术包括掩码变换器和流匹配两种方法，并探索了不同的注意力机制与参数共享策略。实验表明，所采用的参数共享技术能将模型大小减少33-53%，同时对性能或视觉保真度影响极小，使得模型可在1-2个GPU的有限算力下训练与部署。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.01182" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>