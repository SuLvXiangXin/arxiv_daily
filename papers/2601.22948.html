<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Alignment among Language, Vision and Action Representations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Alignment among Language, Vision and Action Representations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.22948" target="_blank" rel="noreferrer">2601.22948</a></span>
        <span>作者: Milano, Nicola, Nolfi, Stefano</span>
        <span>日期: 2026/01/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在认知科学与人工智能领域，一个核心问题是不同的学习模态——语言、视觉和动作——是否会产生截然不同或共享的内部表征。传统观点认为，在不同数据类型上训练的模型会发展出专门化、不可迁移的表征。这一观点尤其强化了一种信念：仅基于语言输入训练的系统无法获得那种通过与物理世界直接感知和运动交互而产生的具身或接地表征，语言模型被认为局限于符号间的句法或统计关联。</p>
<p>然而，近期的经验证据对这一传统边界提出了挑战。研究表明，在不同模态、内容或任务上训练的系统，其内部表征几何结构往往会收敛于相似的形式。例如，研究发现大型语言模型（LLMs）与视觉语言模型（VLMs）的表示空间存在部分同构。这些发现暗示，语言共现统计中编码了大量关于世界物理和功能结构的信息。</p>
<p>本文旨在探究跨模态表征对齐理解中的一个关键空白：通过具身动作学习到的内部表征（直接介导情境智能体中目标导向的行为）是否与通过被动观察语言和视觉学习到的表征对齐。动作表征或许是最接地的一种知识形式，它并非由观测数据中的统计模式塑造，而是由通过物理交互实现目标的实际需求所塑造。如果动作表征能与语言和视觉模型的对齐，将为“核心语义结构超越模态”提供迄今为止最强有力的证据。</p>
<p>本文的核心思路是：通过在BabyAI平台上使用行为克隆训练一个基于Transformer的智能体，使其能响应自然语言指令执行动作，从而生成完全由感觉运动控制需求塑造的动作接地语言嵌入；然后将这些表征与从先进的大型语言模型和视觉语言模型中提取的表征进行比较，以检验它们之间的对齐程度。</p>
<h2 id="方法详解">方法详解</h2>
<p>本研究旨在比较通过具身动作学习得到的语言表征与通过被动观察学习得到的语言/视觉表征。整体流程如下：首先，在BabyAI平台上训练一个语言条件化的智能体（称为动作语言模型，ALM），其内部语言嵌入完全由行为克隆任务塑造。然后，从一系列预训练的LLMs（LLaMA, Qwen, DeepSeek, BERT）和VLMs（CLIP, BLIP）中提取对相同指令集的嵌入。最后，通过计算相似度矩阵和统计度量（如P@k和Procrustes分析残差）来量化这些不同模型嵌入空间之间的结构对齐程度。</p>
<p><strong>智能体架构与训练</strong>：智能体采用基于Transformer的视觉-语言架构，其输入是在每个时间步t接收的语言指令L和RGB视觉观察O_t。视觉观察通过一个残差卷积神经网络（ResNet CNN）编码为嵌入向量E_V。语言指令被分词后，通过一个Transformer块（包含多头自注意力和MLP）处理，最终汇聚为一个128维的句子级嵌入向量E_L。</p>
<p><img src="https://i.imgur.com/6pM7wKk.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：智能体的神经架构示意图。底部左侧为视觉编码器（ResNet CNN），将观察O_t映射为视觉嵌入E_V。底部右侧为语言编码器，将分词后的指令映射为语言嵌入E_L。两者通过一个“语言关注视觉”的跨模态注意力（CMA）模块集成，输出再经过一个深度残差多层感知机（MLP），最终映射到对应于BabyAI中六个可能动作的输出神经元。</p>
</blockquote>
<p>E_L和E_V通过一个“语言关注视觉”的跨模态注意力模块集成，其中语言嵌入作为查询（Query），扁平化的多尺度视觉标记作为键（Key）和值（Value）。该模块的输出经过一个深度残差MLP，最终通过softmax函数产生六维动作概率分布a_t（对应BabyAI中的六个动作：左转、右转、前进、拾取、放下、开门）。模型使用行为克隆方法训练，通过最小化网络输出动作与演示者（BabyAI平台内置的完美智能体）生成动作之间的交叉熵来学习。</p>
<p><strong>核心创新点</strong>：本方法的关键在于，语言嵌入（从词嵌入到句子嵌入E_L）的初始化是相同的向量，并在行为克隆训练过程中动态学习、完全分化。这意味着最终形成的“动作-语言”嵌入空间是<strong>专门为支持目标导向行为生成而塑造的</strong>，与以往使用预训练视觉语言模型来生成语言嵌入的工作有根本不同。</p>
<p><strong>参考模型与嵌入提取</strong>：</p>
<ol>
<li><strong>动作语言模型</strong>：从语言编码器网络提取128维的E_L向量。</li>
<li><strong>视觉语言模型</strong>：从CLIP和BLIP的文本编码器提取句子嵌入。</li>
<li><strong>大型语言模型</strong>：<ul>
<li><strong>解码器-only模型</strong>：从LLaMA、Qwen、DeepSeek的最后一层隐藏层（输出层之前）提取每个标记的上下文嵌入，然后进行平均以得到句子级表示。</li>
<li><strong>编码器-only模型</strong>：从BERT的最后一层提取特殊标记[CLS]的表示作为句子嵌入。</li>
</ul>
</li>
</ol>
<p>所有模型对相同的108条BabyAI语言指令（由有限词汇组合而成）生成嵌入，且预训练模型均未进行微调。</p>
<p><strong>对齐度量</strong>：</p>
<ol>
<li><strong>精度@k</strong>：为每个模型计算108x108的余弦相似度矩阵。对于两个嵌入空间M和N，P@k计算每个句子在空间M中的前k个最近邻，与在空间N中的前k个最近邻的重叠比例，然后对所有句子取平均。k取值从1到15。</li>
<li><strong>Procrustes分析</strong>：通过正交Procrustes分析，寻找最优正交变换矩阵Ω，将动作表征矩阵A映射到目标表征矩阵B（如LLM或VLM的表征）。用对齐后的残差平方和D = ||ΩA - B||^2来衡量两个空间的结构差异，D值越低表示对齐度越高。在维度不匹配时，使用PCA将高维空间降至低维空间的维度。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在BabyAI平台上进行，考虑了三种任务类型：GO-TO、PICK-UP和ESCAPE-FROM。使用了108条由有限词汇组合而成的语言指令。训练了10个不同随机初始化的语言-动作网络（ALM），经过20,000训练周期后，网络在ESCAPE任务上达到约95%的成功率，在GO-TO和PICK-UP任务上达到约85%的成功率。对比的基线模型包括两个VLMs（CLIP, BLIP）和四个LLMs（BERT, LLaMA, Qwen, DeepSeek），以及一个随机生成的嵌入空间作为对照。</p>
<p><img src="https://i.imgur.com/8d6gT9H.png" alt="BabyAI平台图示"></p>
<blockquote>
<p><strong>图1</strong>：BabyAI平台示意图。面板A展示了一个示例环境（6x6网格的单房间），其中包含代理（红色三角形）和各种物体（球、箱子、门、钥匙）。底部文字是一个示例语言请求：“去绿色的球那里”。面板B显示了代理从其当前位置和方向感知到的局部环境。</p>
</blockquote>
<p><strong>关键定量结果</strong>：图3展示了各模型间P@15对齐度的热力图。主要发现如下：</p>
<ul>
<li><strong>ALM与解码器-only LLMs及BLIP对齐度高</strong>：ALM与LLaMA、Qwen、DeepSeek的对齐度（P@15）在0.70–0.73之间，与BLIP的对齐度也处于相似的高水平（约0.73）。这个对齐强度<strong>接近语言模型彼此之间的对齐水平</strong>（例如LLaMA与Qwen之间为0.79）。</li>
<li><strong>ALM与CLIP和BERT对齐度弱</strong>：ALM与CLIP的对齐度较低（约0.50），与BERT的对齐度最低（约0.45）。</li>
<li><strong>随机对照</strong>：所有模型与随机生成嵌入空间的对齐度均非常低（约0.10），证实了观测到的高对齐度并非偶然。</li>
</ul>
<p><img src="https://i.imgur.com/vI0JhWY.png" alt="对齐热力图"></p>
<blockquote>
<p><strong>图3</strong>：展示七个模型及随机向量空间（RANDOM）之间P@15对齐度的热力图。颜色越亮表示对齐度越高。可以清晰看到ALM与解码器-only LLMs（Llama, Qwen, Deepseek）以及BLIP VLM之间存在强对齐（亮色块），而与CLIP和BERT的对齐较弱。</p>
</blockquote>
<p><strong>补充分析</strong>：Procrustes分析的结果与P@k结论一致。将ALM表征与各模型表征对齐后的残差D值显示，ALM与解码器-only LLMs和BLIP的D值最低（结构差异最小），而与CLIP和BERT的D值最高。此外，研究还测试了不同k值下的P@k曲线，结果显示ALM与解码器-only LLMs/BLIP的对齐度在所有k值下均稳定地高于与CLIP/BERT的对齐度，进一步证实了上述发现。</p>
<p><strong>消融实验启示</strong>：虽然论文未进行传统意义上的组件消融实验，但通过比较不同类别的模型（解码器-only LLM vs. 编码器-only BERT；生成式VLM BLIP vs. 对比式VLM CLIP），间接揭示了影响对齐的可能因素。结果表明，<strong>训练目标（自回归预测 vs. 掩码预测/对比学习）和模型架构（解码器-only vs. 编码器-only）可能比模态差异（纯文本 vs. 多模态）对表征结构的塑造影响更大</strong>，因为纯文本的解码器-only LLMs与多模态的BLIP都和动作表征高度对齐。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>首次将动作表征纳入跨模态对齐分析</strong>：本研究首次探究了通过具身、目标导向的动作学习塑造的语言表征，与通过被动观察文本或图像-文本对学习到的表征之间的对齐关系，扩展了该研究领域。</li>
<li><strong>发现了动作表征与被动学习模型之间的强对齐</strong>：尽管在训练数据、模态和目标上存在根本差异，但专门为动作生成优化的表征与解码器-only大型语言模型及BLIP视觉语言模型的表征显示了强大的结构一致性，其对齐强度接近语言模型之间的内部对齐水平。</li>
<li><strong>挑战了“接地需要直接感觉运动经验”的传统观点</strong>：研究结果表明，语言、视觉和动作表征可能收敛于部分共享的语义结构，这支持了语义组织在某种程度上是<strong>模态无关</strong>的，并为跨领域迁移提供了证据。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：使用的任务和语言词汇相对有限（BabyAI平台，108条指令）；智能体是在简化的模拟环境中操作的；分析主要基于几何相似性度量，尚未在功能任务上直接测试表征的可迁移性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>理论意义</strong>：研究结果为“语义知识的核心结构可能超越具体学习模态”的观点提供了有力支持，激励进一步探索跨模态共享的认知或计算原理。</li>
<li><strong>方法启示</strong>：解码器-only LLMs与动作表征的高对齐性，提示自回归预测目标可能是一种强大的学习范式，能够诱导出与具身智能需求兼容的抽象表征。这为设计更好的具身AI模型提供了新思路，例如，可以利用强大的LLMs作为具身智能体的先验或基础。</li>
<li><strong>未来方向</strong>：可在更复杂的环境、任务和语言指令上验证此发现；探究这种对齐的功能性后果，例如是否有利于知识迁移或零样本学习；进一步剖析是训练数据、目标函数还是架构本身导致了这种收敛。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究语言、视觉与动作表征是否共享语义结构这一核心问题。传统观点认为不同模态会形成专门化表征，但作者通过在BabyAI平台进行行为克隆，训练基于Transformer的智能体执行语言指令，生成动作基础的语言嵌入。通过比较这些嵌入与LLaMA、CLIP等大型模型的表征，发现动作表征与仅解码器语言模型及BLIP模型对齐度高（Precision@15达0.70–0.73），而与CLIP、BERT对齐较弱。结果表明，不同模态的表征会收敛于部分共享的语义空间，支持跨模态语义组织理论。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.22948" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>