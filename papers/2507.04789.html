<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Training-free Generation of Temporally Consistent Rewards from VLMs - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Training-free Generation of Temporally Consistent Rewards from VLMs</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.04789" target="_blank" rel="noreferrer">2507.04789</a></span>
        <span>作者: Jian Tang Team</span>
        <span>日期: 2025-07-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用视觉语言模型（VLM）为机器人操作生成奖励的主流方法主要分为两类。第一类方法（如VLM-CaR）在机器人操作前显式地生成奖励函数，虽然计算成本较低，但仅限于视觉信息简单的任务。第二类方法（如RL-VLM-F）则直接基于图像和语言描述查询VLM来识别目标完成状态以生成奖励信号，适用于更复杂的操作，但需要频繁查询VLM，计算成本高昂。此外，仅依赖空间观察（单帧图像）使得生成准确奖励变得困难，因为预训练VLM缺乏特定机器人领域的知识，且微调VLM的计算成本阻碍了其实时应用。</p>
<p>本文针对上述痛点，提出了一个新的视角：通过结合时序信息来修正VLM的空间推理输出，从而在减少VLM查询次数的同时提高奖励准确性。本文的核心思路是：仅在每个交互回合开始时查询一次VLM以初始化子目标状态，然后利用贝叶斯跟踪算法，结合从时序观察中提取的物体轨迹信息，动态更新目标完成状态的估计，从而生成时序一致且准确的奖励。</p>
<h2 id="方法详解">方法详解</h2>
<p>T²-VLM的整体框架如图1和图2所示。其流程为：在每一回合（episode）开始时，向VLM输入初始RGB图像和任务描述，以分解出空间感知的子目标并提供一个初始的完成状态估计。随后，在交互过程中，不再查询VLM，而是利用SAM 2从视频流中持续提取任务相关物体的边界框轨迹作为时序观察。一个集成了VLM编码功能（VLM-coding affordance）的粒子滤波器（一种贝叶斯跟踪算法）利用这些观察，动态更新和修正由VLM初始化的子目标隐藏状态（即完成状态）。最终，通过计算决策间隔内子目标隐藏状态的变化来生成用于强化学习（RL）的奖励信号。</p>
<p><img src="https://..." alt="方法对比"></p>
<blockquote>
<p><strong>图1</strong>：现有VLM奖励生成方法与T²-VLM的对比示意图。(a) 现有方法需要在每个决策步骤（τ）查询VLM以获得目标完成状态h并计算奖励r。(b) T²-VLM仅在回合开始时查询一次VLM，后续通过贝叶斯滤波器利用时序观察更新估计状态ĥ并生成奖励，显著降低了计算消耗。</p>
</blockquote>
<p><img src="https://..." alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：T²-VLM方法总览图。左侧：VLM根据初始图像和任务描述分解子目标并提供初始完成状态估计（“是/否”回答）。右侧：利用SAM 2获取物体边界框轨迹，通过贝叶斯跟踪算法（核心为粒子滤波器）动态更新子目标隐藏状态ĥ。奖励（r）通过计算ĥ在决策间隔内的变化（σ(Δĥ)）得出。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>VLM提示生成子目标与初始估计</strong>：设计链式思维（CoT）提示，让VLM首先识别任务相关物体，然后基于空间关系分解子目标。接着，将每个子目标转化为视觉问答（VQA）形式，让VLM基于初始图像判断其是否已完成，输出“是”（对应隐藏状态值1）或“否”（对应0），从而初始化一个N维的二值向量h₀（N为子目标数）。</li>
<li><strong>物体轨迹提取（SAM 2）</strong>：为了解决粒子滤波器难以直接处理高维图像的问题，采用强大的基础视觉模型SAM 2来跟踪视频中的任务相关物体，并输出其边界框序列I_t，为后续跟踪提供观测输入。</li>
<li><strong>贝叶斯跟踪更新状态（粒子滤波器）</strong>：这是方法的核心创新模块。它将子目标完成状态h_t的估计建模为一个贝叶斯滤波问题，使用粒子滤波器来近似后验分布p(h_t | I_t)。<ul>
<li><strong>状态表示与初始化</strong>：每个粒子代表一个子目标隐藏状态的假设。状态值被约束在[0,1]区间内进行软估计，值越高表示当前时间步子目标完成的可信度越高。所有K个粒子均使用VLM提供的初始估计h₀进行初始化。</li>
<li><strong>运动模型</strong>：采用恒定速度运动模型来传播粒子状态，假设隐藏状态随机器人操作平滑变化。该模型包含一个软更新超参数α，用于控制更新速度，增强对SAM 2因遮挡等原因导致的检测错误的鲁棒性。</li>
<li><strong>观察似然计算（VLM编码功能）</strong>：这是关键创新点。为了计算给定粒子状态h_t下观察到边界框I_t的似然p(I_t | h_t)，论文没有使用解析方法，而是再次利用了VLM的推理和代码生成能力。如图3所示，对于每个子目标i，会提示VLM生成两个Python函数：f_VLM¹(i) 用于输出当该子目标<strong>满足</strong>时，目标物体所有可能的位置坐标；f_VLM⁰(i) 用于输出当该子目标<strong>不满足</strong>时，目标物体所有可能的位置坐标。然后，计算观测位置与这两个函数生成的“功能位置”集合之间的最小平均距离（d⁰_i 和 d¹_i）。粒子的权重ω_k根据其状态与观测的匹配程度计算（公式6），即对于每个子目标，如果粒子状态h_k_i为1，则使用d¹_i项；若为0，则使用d⁰_i项，最终权重为所有子目标对应项的乘积。</li>
<li><strong>重采样与状态估计</strong>：根据权重对粒子进行系统重采样。估计的子目标隐藏状态ĥ_t是所有粒子的加权平均。最终，在决策时间步t，奖励计算为 r_t = σ(ĥ_t - ĥ_{t-τ})，其中τ是决策间隔，σ(·)是一个基于子目标加权的函数。</li>
</ul>
</li>
</ol>
<p><img src="https://..." alt="VLM编码功能"></p>
<blockquote>
<p><strong>图3</strong>：用于计算观察似然的VLM编码功能示意图。通过精心设计的提示，让VLM为每个子目标生成两个Python函数，分别输出在该子目标“满足”或“不满足”条件下，目标物体所有可能的位置点（affordance points）。</p>
</blockquote>
<p>与现有方法相比，T²-VLM的核心创新点在于：1) <strong>训练免费与高效性</strong>：无需微调VLM或训练奖励模型，且每个回合仅需一次VLM查询，极大降低了计算成本。2) <strong>时序一致性</strong>：通过贝叶斯滤波器整合时序观察（物体轨迹），修正了仅基于单帧图像的VLM初始估计，生成了更准确、更平滑的奖励信号。3) <strong>VLM编码功能</strong>：创新地利用VLM生成代码函数来建模复杂的观察似然，避免了手动设计难度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：论文在三个领域的六个机器人操作任务上进行了测试（图4）。模拟任务来自两个常用基准：CLIPort（Place-same-color, Stack-tower）和CALVIN（Make-line, Cleanup-desk）。此外还包括两个真实世界任务（Prepare-food, Set-table）。实验平台涉及模拟环境和真实UR机器人。</p>
<p><strong>对比方法</strong>：</p>
<ol>
<li><strong>VLM-CaR</strong>：基于少量专家和随机轨迹微调VLM生成的代码奖励函数。</li>
<li><strong>RL-VLM-F</strong>：从VLM生成的偏好标签中学习奖励模型。</li>
<li><strong>Environmental Rewards</strong>：基于环境真实状态编码的奖励（上限对比）。</li>
<li><strong>恢复性能对比</strong>：与SayCan（基于LLM的技能规划）和REFLECT（结合LLM与视觉模型进行失败分析）进行对比。</li>
</ol>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>RL训练性能</strong>：如图5所示，在四个模拟任务上，使用T²-VLM生成的奖励进行RL训练，其达到的<strong>目标完成率</strong>（goal completion ratio）与使用环境奖励的方法相当，且显著优于VLM-CaR和RL-VLM-F。T²-VLM的训练曲线也更稳定。</li>
</ol>
<p><img src="https://..." alt="训练性能对比"></p>
<blockquote>
<p><strong>图5</strong>：在不同VLM-based奖励下RL训练过程中的目标完成率。T²-VLM（绿色曲线）的性能与Environmental Rewards（黑色）相当，并优于其他VLM方法，且训练更稳定。</p>
</blockquote>
<ol start="2">
<li><strong>恢复性能</strong>：如表1所示，在引入各种失败（如空间占用、抽屉关闭、抓取/放置失败）的评估中，使用T²-VLM奖励训练出的RL策略，在<strong>恢复成功率</strong>上显著高于SayCan和REFLECT（平均0.93 vs 0.53和0.23）。同时，其完成恢复所需的<strong>恢复元步骤数</strong>也远少于其他方法，更接近理论最优步数。</li>
</ol>
<p><img src="https://..." alt="恢复性能表"></p>
<blockquote>
<p><strong>表1</strong>：面对不同类型失败时的恢复性能对比。RL w/ T²-VLM在成功率和效率上均优于基于LLM的恢复方法。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：如图6所示，消融实验验证了各组件贡献。<ul>
<li><strong>VLM初始化的必要性</strong>：粒子滤波器（PF）使用随机初始化（PF+Random）时，奖励准确率比完整T²-VLM（PF+VLM initial）低约12.88%（Place-same-color任务），说明良好的VLM初始化至关重要。</li>
<li><strong>粒子滤波器的必要性</strong>：如果移除粒子滤波器，每一步都查询VLM（T²-VLM w/o PF），奖励准确率会大幅下降（在前三个任务中低于70%），证明了整合时序信息进行动态修正的关键作用。</li>
<li><strong>任务依赖性</strong>：在VLM易于判断的任务（如Place-same-color）上，T²-VLM的准确率接近使用真实状态初始化（PF+GT initial）的上限；而在VLM难以判断空间关系（如“next to”）的任务（如Make-line）上，性能尚有提升空间，表明其性能受VLM初始空间推理能力的限制。</li>
</ul>
</li>
</ol>
<p><img src="https://..." alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：关于VLM初始化和粒子滤波器（PF）的消融研究。展示了不同配置下的奖励准确率，证明了VLM初始化和PF模块各自的重要性。</p>
</blockquote>
<ol start="4">
<li><strong>不同VLM与参数调查</strong>：表2和表3表明，T²-VLM能兼容不同的VLM（如Claude-3.5-sonnet, Qwen2-VL-72B/7B），并一致地<strong>提升奖励准确率</strong>（提升幅度从6.49%到41.17%不等）并<strong>大幅减少VLM查询次数</strong>（减少77%-88%），从而显著降低推理时间。有趣的是，更强大的VLM在单纯用于打分（VLM-score）时，在机器人场景中未必表现更好。图7显示，粒子滤波器的隐藏长度（使用的时序帧数）存在权衡，长度25在准确率和效率间取得了较好平衡。</li>
</ol>
<p><img src="https://..." alt="VLM调查表"></p>
<blockquote>
<p><strong>表2 &amp; 表3</strong>：在CLIPort和CALVIN任务上，不同基础VLM在VLM-score和T²-VLM中的表现对比。T²-VLM在显著减少查询次数和推理时间的同时，提高了奖励准确率。</p>
</blockquote>
<p><img src="https://..." alt="PF长度调查"></p>
<blockquote>
<p><strong>图7</strong>：粒子滤波器隐藏长度对奖励准确率和推理时间的影响。长度为25时达到了较好的平衡。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li>提出了 <strong>T²-VLM</strong>，一个新颖的、<strong>无需训练</strong>、<strong>时序一致</strong>的框架，用于为机器人操作任务生成准确的奖励。其核心是仅用一次VLM查询初始化，然后通过贝叶斯滤波器结合时序观察进行动态状态跟踪。</li>
<li>设计了一种<strong>集成VLM编码功能的贝叶斯跟踪算法</strong>，利用VLM的代码生成能力来建模观察似然，从而能够处理高维视觉输入并更新目标完成状态。</li>
<li>通过大量实验证明，T²-VLM在多个基准测试中实现了<strong>最先进的奖励生成性能</strong>，且计算消耗更低。使用其奖励训练的RL智能体展现出<strong>更高的失败恢复成功率和更快的恢复速度</strong>。</li>
</ol>
<p>论文提到的局限性在于，T²-VLM的性能部分依赖于VLM初始估计的准确性。对于VLM难以理解的复杂空间关系（如“相邻”），初始估计可能不准确，从而影响最终性能，这需要未来具有更强空间推理能力的VLM来改善。</p>
<p>本文的启示在于：为利用基础模型进行机器人奖励生成提供了一条高效、免训练的新路径。它表明，通过巧妙结合VLM的零样本能力与经典的状态估计方法（如贝叶斯滤波），并引入时序信息，可以在不增加昂贵计算负担的前提下，显著提升奖励信号的准确性和鲁棒性。这对于推动具身AI在长视野、动态变化任务中的应用具有积极意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出T²-VLM框架，旨在解决无需微调视觉语言模型即可为机器人操作生成准确、时序一致奖励信号的难题。方法核心是：先通过VLM查询建立空间感知的子目标及初始完成度估计；再采用贝叶斯跟踪算法动态更新子目标隐藏状态，据此生成结构化奖励。实验表明，该框架在两个机器人操作基准测试中取得最先进性能，在提升奖励准确性的同时显著降低了计算消耗。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.04789" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>