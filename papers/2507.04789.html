<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Training-free Generation of Temporally Consistent Rewards from VLMs - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Training-free Generation of Temporally Consistent Rewards from VLMs</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.04789" target="_blank" rel="noreferrer">2507.04789</a></span>
        <span>作者: Jian Tang Team</span>
        <span>日期: 2025-07-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用大规模视觉语言模型（VLMs）为机器人任务生成密集奖励已成为一种有前景的范式。主流方法通常依赖于将任务目标编码为文本提示，并利用VLM对视频帧进行评分，从而将高层语义目标转化为可供强化学习使用的标量奖励信号。然而，这类方法存在一个关键局限性：<strong>时间不一致性</strong>。由于VLM对每一帧的评分是独立进行的，忽略了帧与帧之间的时序依赖关系，导致生成的奖励信号在时间维度上存在剧烈、不合理的波动。这种噪声会严重干扰策略学习过程，降低样本效率，甚至导致学习失败。</p>
<p>本文针对VLM生成奖励信号中的“时间不一致性”这一具体痛点，提出了一个新颖的视角：<strong>无需任何训练，通过深入挖掘和利用预训练VLM模型内部的、固有的知识来构建时间一致的奖励函数</strong>。核心思路是：将奖励生成问题重新定义为从VLM的token概率分布中提取时间上平滑的、与任务目标语义对齐的特征轨迹，并通过对比学习的方式，鼓励智能体行为与理想的特征轨迹对齐，而非直接使用波动的帧级分数。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为 **Temporal Consistency Rewards (TCR)**，其核心在于从VLM的token概率输出中构建一个时间上平滑的“任务概念”嵌入轨迹，并以此作为参考，为智能体的观察序列生成奖励。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/0c5b1c2b6d814b6f8c9f4e4d2b9e6d7e.png" alt="TCR方法整体框架图"></p>
<blockquote>
<p><strong>图1</strong>：TCR方法整体框架。给定一个任务描述（如“打开抽屉”）和一段由智能体视角采集的视频，首先使用VLM（此处以CLIP为例）提取每一帧图像关于一系列预定义任务相关token的概率分布。然后，通过时间平滑处理（如移动平均）得到平滑后的概率分布，并从中选择与任务最相关的token的概率向量作为“目标概念嵌入轨迹”。最后，计算当前帧的原始token概率向量与目标轨迹中对应时间步的嵌入向量之间的余弦相似度，作为当前帧的密集奖励。</p>
</blockquote>
<p><strong>整体流程如下</strong>：</p>
<ol>
<li><strong>输入</strong>：一段由机器人策略在环境中交互产生的视频帧序列 <code>{o_1, o_2, ..., o_T}</code>，以及一个描述任务目标的文本指令 <code>G</code>。</li>
<li><strong>Token概率提取</strong>：使用一个预训练的VLM（如CLIP）的图像编码器处理每一帧 <code>o_t</code>，同时使用文本编码器处理一组精心设计的提示词（prompts）。这些提示词由任务指令 <code>G</code> 衍生而来，通常包含正例（如“a photo of opening a drawer”）和反例（如“a photo of closing a drawer”）。VLM会输出当前帧 <code>o_t</code> 与每个提示词对应的token的匹配概率，形成一个概率向量 <code>p_t</code>。</li>
<li><strong>时间一致性建模（核心创新）</strong>：直接使用 <code>p_t</code> 作为奖励会导致波动。TCR的关键步骤是对概率序列 <code>{p_t}</code> 沿时间维度进行平滑处理（例如采用低通滤波或移动平均），得到一个平滑后的概率轨迹 <code>{\bar{p}_t}</code>。这个平滑过程消除了独立帧评分带来的高频噪声，保留了与任务进度相关的低频趋势。</li>
<li><strong>奖励生成</strong>：奖励函数定义为当前帧的原始概率向量 <code>p_t</code> 与平滑后目标轨迹中对应时间步的向量 <code>\bar{p}_t</code> 之间的一致性度量。具体而言，奖励 <code>r_t</code> 计算为 <code>p_t</code> 与 <code>\bar{p}_t</code> 的余弦相似度：<code>r_t = sim(p_t, \bar{p}_t)</code>。这种设计鼓励智能体产生的视觉观察特征与时间平滑后的“理想任务进展”特征保持一致。</li>
</ol>
<p><strong>与现有方法相比的创新点</strong>：</p>
<ul>
<li><strong>训练自由</strong>：整个方法完全基于预训练的VLM，无需对VLM进行任何微调或训练额外的奖励模型，极大地简化了流程并保持了VLM的泛化能力。</li>
<li><strong>利用内部概率分布</strong>：不同于仅使用VLM输出的单一分数，TCR深入利用了VLM输出的完整token概率分布，其中包含了更丰富的语义信息。</li>
<li><strong>显式的时间平滑</strong>：通过引入一个显式的、可解释的时间平滑模块，直接解决了奖励信号时间不一致的根本问题，而非试图通过复杂的网络结构或训练策略来隐式地克服它。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：在 <strong>MetaWorld</strong>（一个机器人操作任务模拟基准）和 <strong>CALVIN</strong>（一个语言条件化的长视野机器人操作基准）上进行评估。</li>
<li><strong>任务</strong>：包括 <code>door-open-v2</code>, <code>drawer-open-v2</code>, <code>hammer-v2</code> (MetaWorld) 以及 <code>slide block to target</code>, <code>open drawer</code>, <code>turn on light</code> (CALVIN) 等需要多步推理和精确控制的任务。</li>
<li><strong>实验平台</strong>：使用模拟器，并采用 <strong>Soft Actor-Critic (SAC)</strong> 作为强化学习算法。将TCR生成的密集奖励与环境的稀疏成功奖励结合使用。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>稀疏奖励</strong>：仅使用任务完成时的稀疏奖励。</li>
<li><strong>VLM-Prompt</strong>：基线方法，直接使用VLM对帧与任务提示词的匹配分数作为奖励（即未平滑的版本）。</li>
<li><strong>其他基于预训练模型的奖励生成方法</strong>（如R3M、VIP等）。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://img-blog.csdnimg.cn/direct/3a1c8d3b5c0f4f4c8fe0e7b3c8a8e1b2.png" alt="MetaWorld和CALVIN上的成功率对比"></p>
<blockquote>
<p><strong>图2</strong>：在MetaWorld和CALVIN多个任务上的平均成功率对比。TCR方法在绝大多数任务上显著优于仅使用稀疏奖励和原始的VLM-Prompt方法，并与更复杂的基于训练的方法（如R3M）性能相当甚至更优。这证明了TCR生成的时间一致奖励能有效加速策略学习。</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/direct/8e7c5f5b6d7b4c5a8c0e9f3b2c8d6a1f3.png" alt="奖励信号曲线定性对比"></p>
<blockquote>
<p><strong>图3</strong>：在<code>drawer-open</code>任务中，TCR与VLM-Prompt生成的奖励信号随时间变化的定性对比。可以清晰看到，VLM-Prompt奖励（蓝色）噪声大、波动剧烈，而TCR奖励（红色）平滑且与任务实际进度（抽屉被拉开的程度）高度相关，在任务完成时达到峰值。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>论文对TCR的核心组件进行了消融研究：</p>
<ol>
<li><strong>时间平滑</strong>：移除平滑模块（即直接使用 <code>sim(p_t, p_t)</code>）会导致性能大幅下降，接近甚至差于VLM-Prompt基线，验证了时间平滑对于稳定训练的必要性。</li>
<li><strong>Token选择</strong>：比较了使用所有token的概率向量与仅使用与任务最相关的token子集。结果表明，精心选择的任务相关token能提供更具判别力的奖励信号，提升学习效率。</li>
<li><strong>平滑窗口大小</strong>：实验了不同的平滑窗口大小。窗口太小无法有效抑制噪声，窗口太大会过度平滑，模糊任务进展的关键转折点。存在一个最优范围，在实验中通常为5-10个时间步。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了一种无需训练的、时间一致的VLM奖励生成框架（TCR）</strong>，通过显式地平滑VLM的token概率时序分布来解决现有方法中的奖励波动问题。</li>
<li><strong>揭示了预训练VLM内部概率分布蕴含的、可用于引导强化学习的时序结构</strong>，为利用基础模型进行机器人学习提供了一个简单而有效的新途径。</li>
<li><strong>在多个机器人操作基准上进行了全面实验</strong>，证明了TCR能生成高质量奖励，显著提升策略学习的样本效率和最终性能，且无需额外训练成本。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>依赖预定义提示词</strong>：方法的有效性部分依赖于为特定任务设计合适的正/反例提示词集合，这可能需要一些领域知识或尝试。</li>
<li><strong>计算成本</strong>：虽然无需训练，但在线交互时仍需对每一帧调用VLM进行前向传播，计算开销仍高于轻量级网络。</li>
<li><strong>泛化性边界</strong>：方法依赖于VLM对任务相关视觉概念的识别能力。对于VLM难以理解的非常规任务或极端视觉变化，其生成的奖励质量可能会下降。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>挖掘基础模型内部表征</strong>：TCR的成功表明，预训练大模型中可能已经包含了丰富的、可用于决策的结构化知识，如何更高效、更通用地提取这些知识是一个有前景的方向。</li>
<li><strong>奖励设计的可解释性</strong>：相比于端到端训练的黑色箱奖励模型，TCR基于概率平滑和相似度计算的设计更具可解释性，便于调试和分析。</li>
<li><strong>迈向完全免训练的机器人学习</strong>：本文是迈向完全利用现成基础模型、免除任何针对特定任务的训练步骤（包括奖励模型训练）的重要一步，激励研究者探索更多“开箱即用”的机器人学习方法。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文旨在解决从视觉语言模型（VLMs）中无需训练生成时间上一致奖励的核心问题。关键技术方法基于VLMs的预训练能力，通过轻量级推理机制直接提取奖励信号，确保时序一致性。实验结果表明，该方法在时间一致性指标上显著提升，并在强化学习等下游任务中实现了性能改进。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.04789" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>