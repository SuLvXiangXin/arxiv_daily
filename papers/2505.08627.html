<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies Towards Visual Robustness - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies Towards Visual Robustness</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.08627" target="_blank" rel="noreferrer">2505.08627</a></span>
        <span>作者: Mirjalili, Reihaneh, Jülg, Tobias, Walter, Florian, Burgard, Wolfram</span>
        <span>日期: 2025/05/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于人类专家示范训练的视觉运动策略在各种机器人操作任务中表现出强大性能。然而，这些策略对背景变化或机器人本体差异等引起的领域偏移高度敏感，严重限制了其泛化能力。尽管增加训练数据的多样性可以部分缓解此问题，但成本高昂且难以覆盖所有现实变化。近期视觉基础模型的发展启发了一种新视角：与其训练策略去应对所有可能的视觉场景，不如将输入空间转换为一种规范表示，以消除与任务无关的场景变化。本文提出的核心思路是：为机器人配备一种面向任务的增强现实视角，选择性地过滤无关信息，仅突出对任务执行至关重要的元素，从而在不需重新训练的情况下提升策略的鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>ARRO是一个无需校准的视觉预处理流程，旨在生成任务特定的增强视觉观测。其核心思想是：利用开放词汇分割和目标检测，仅保留机器人夹爪和目标物体，并将它们叠加到一个结构化的虚拟背景上，从而创建一个一致且简化的输入空间。</p>
<p><img src="https://arxiv.org/html/2505.08627v3/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：ARRO流程概览。我们的流程利用开放词汇视觉模型分割机器人夹爪和任务相关物体，并将它们叠加到虚拟背景上。我们在训练和推理中一致地使用此过程，以增强对视觉领域偏移的鲁棒性。</p>
</blockquote>
<p>整体流程分为两个核心模块：</p>
<ol>
<li><p><strong>开放词汇分割</strong>：此模块用于在每帧图像中隔离机器人夹爪手指和任务相关物体。它采用两阶段过程：</p>
<ul>
<li><strong>初始化分割</strong>：在任务开始时，对第一帧图像 <code>I_0</code> 进行处理。<ul>
<li>对于复杂物体：使用开放词汇目标检测模型（如 Grounding DINO），根据物体类别提示词 <code>p_i^o</code> 获取边界框 <code>B_i</code>。</li>
<li>对于夹爪手指和简单形状物体：首先使用分割模型（如 SAM 2）进行无提示分割，得到一系列区域提议 <code>K_j</code>。随后，在原图 <code>I_0</code> 上每个区域中心标注数字，生成标注图 <code>I_0*</code>，并将其与任务提示 <code>p^t</code>（例如“识别左右夹爪手指”）一同输入视觉语言模型（如 GPT-4o）。VLM 输出指定物体及夹爪手指的关键点坐标 <code>K*</code>。</li>
</ul>
</li>
<li><strong>跟踪传播</strong>：利用基于记忆的分割模型（如 SAM 2），以上述初始化的边界框 <code>B_i</code> 和关键点 <code>K*</code> 作为提示，从第一帧中提取出物体和夹爪的分割掩码 <code>S_0^obj</code> 和 <code>S_0^gripper</code>。随后，该模型利用积累的记忆，在后续帧 <code>I_t (t&gt;0)</code> 中跟踪这些区域，实现跨时间的连续分割，无需VLM重新识别或额外监督。</li>
</ul>
</li>
<li><p><strong>虚拟场景重组</strong>：在每一时间步 <code>t</code> 获得分割掩码后，计算物体与夹爪掩码的并集 <code>S_t = S_t^obj ∪ S_t^gripper</code>。随后，将该掩码叠加到一个虚拟背景 <code>I_B</code> 上。背景可以是简单的黑色，也可以是手工制作的彩色网格。最终增强图像 <code>Ĩ_t</code> 通过公式 <code>Ĩ_t = S_t ⊙ I_t + (1 - S_t) ⊙ I_B</code> 计算得出，其中 <code>⊙</code> 表示逐元素相乘。此过程在训练时应用于所有演示数据帧，在推理时实时运行。</p>
</li>
</ol>
<p>与现有方法相比，ARRO的创新点主要体现在：1) <strong>零样本与无需训练</strong>：完全利用预训练的开放词汇基础模型，无需针对特定任务或环境进行模型训练。2) <strong>无需校准</strong>：通过视觉语言模型识别夹爪，无需相机标定或机器人URDF模型。3) <strong>结构化背景</strong>：使用带有视觉参考（如网格）的虚拟背景，而非纯色背景，为策略提供了额外的空间参考线索。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真和真实世界环境中进行，评估了ARRO与多种策略的兼容性：Diffusion Policy、Octo、OpenVLA 和 π_0。任务包括拾取立方体、推动立方体、放置玩偶和开合盒子等桌面操作。基线方法包括：原始策略（Vanilla）、仅使用黑色背景掩码的策略（Masked）以及ARRO。</p>
<p><img src="https://arxiv.org/html/2505.08627v3/x1.png" alt="输入格式与性能对比"></p>
<blockquote>
<p><strong>图1</strong>：（顶部）输入格式可视化，（底部）四个操作任务的性能对比。ARRO在存在视觉领域偏移的测试场景下，性能始终优于原始策略和仅掩码策略。</p>
</blockquote>
<p><strong>关键定量结果</strong>：</p>
<ul>
<li><strong>对领域偏移的鲁棒性（Diffusion Policy）</strong>：在四个真实世界任务中，当评估场景存在背景、纹理、机器人外观变化或干扰物时，原始策略性能严重下降。ARRO在所有任务上均取得了最高成功率，显著优于仅掩码策略（例如在box-v1任务中，ARRO成功率约85%，掩码策略约65%，原始策略约10%）。</li>
<li><strong>处理干扰物与空间推理</strong>：在需要根据“左侧/右侧”或“距离黄色立方体更近/更远”等空间关系选择正确蓝色立方体的推动任务中，ARRO能通过VLM理解指令并正确选择目标，随后掩蔽其他干扰物，使策略专注于相关输入，从而保持高成功率（约80-100%），而原始策略则表现不佳。</li>
<li><strong>与通用策略的兼容性</strong>：在拾取任务中，对Octo、OpenVLA和π_0进行测试。当评估视觉条件改变时，ARRO和掩码变体均能提升或恢复策略性能。例如，对于π_0，当场景中增加一个同形状的红色立方体干扰时，原始策略因忽略指令中的颜色信息而成功率降至30%，而ARRO通过掩蔽红色立方体将成功率提升至90%。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.08627v3/x6.png" alt="通用策略性能对比"></p>
<blockquote>
<p><strong>图6</strong>：在视觉条件改变下，三种通用模型（Octo, OpenVLA, π_0）的性能对比。ARRO和掩码变体通常能提升或恢复因领域偏移而下降的性能。</p>
</blockquote>
<ul>
<li><strong>真实到仿真（Real-to-Sim）迁移</strong>：测试了在真实数据上训练的策略直接迁移到仿真环境中的能力。如表I所示，原始策略的性能急剧下降（除π_0外均降至0%）。ARRO帮助OpenVLA保留了其真实世界性能的55%，并为所有策略带来了比原始基线更高的任务奖励。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.08627v3/x9.png" alt="真实到仿真奖励对比"></p>
<blockquote>
<p><strong>图9</strong>：在真实到仿真迁移实验中，Diffusion Policy、Octo和OpenVLA使用ARRO或黑色背景掩码获得的奖励高于原始基线。</p>
</blockquote>
<p><strong>定性结果与消融实验</strong>：</p>
<ul>
<li><strong>处理遮挡</strong>：如图5所示，在执行过程中，即使目标物体被机器人手臂或其他物体短暂遮挡，ARRO的分割模块在遮挡消失后也能准确恢复，显示了其时间一致性。</li>
<li><strong>背景选择消融</strong>：对比使用纯黑背景（Masked）和带网格的虚拟背景（ARRO）的结果表明，结构化的虚拟背景能提供更好的性能，说明视觉参考线索对策略有益。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了ARRO，一个无需校准的增强现实预处理流程，通过零样本开放词汇模型选择性地保留任务相关视觉信息，显著提升了视觉运动策略对背景变化、干扰物和本体差异的鲁棒性。2) 设计了一个基于视觉语言模型和计算机视觉基础模型的系统，无需任务或环境建模、相机校准或额外训练，原则上可与任何视觉运动策略结合。3) 通过广泛的实验验证了ARRO在多种策略（Diffusion Policy, Octo, OpenVLA, π_0）和任务上的有效性，包括处理空间推理、遮挡以及促进真实到仿真的跨领域迁移。</p>
<p>论文提到的局限性包括：方法性能依赖于底层分割和检测模型的质量；对于极度复杂或严重遮挡的场景，分割可能失败；当前主要评估了桌面操作任务。</p>
<p>这项工作启示后续研究可以探索：将类似的“感知前置过滤”思想应用于更复杂的场景和多模态输入；开发更高效、更鲁棒的基础模型集成方案以降低延迟；进一步研究虚拟背景的最优设计，以最大化其对不同策略架构的辅助效果。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉运动策略对背景、机器人外观等视觉领域偏移敏感、泛化能力受限的问题，提出ARRO方法。该方法利用零样本开放词汇分割与物体检测模型，无需额外训练即可实时屏蔽场景中任务无关区域，过滤视觉干扰并叠加虚拟引导。实验表明，ARRO与Diffusion Policy等策略结合，在多种桌面操作任务中能持续提升性能，增强了对场景变化的鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.08627" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>