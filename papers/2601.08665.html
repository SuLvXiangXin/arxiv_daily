<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.08665" target="_blank" rel="noreferrer">2601.08665</a></span>
        <span>作者: Junzhi Yu Team</span>
        <span>日期: 2026-01-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前具身导航的主流方法主要分为模块化方法和端到端视觉-语言-动作（VLA）模型。模块化方法依赖于手工设计的子模块（如感知、定位、规划）接口，其脆弱性和错误累积限制了在动态复杂环境中的适应性。近年来，基于大规模预训练视觉语言模型（VLM）的端到端VLA模型通过统一多模态理解和动作生成，显著提升了导航系统的适应性和表达能力。然而，现有VLA模型多为反应式系统，存在两大关键局限：一是缺乏显式推理机制，无法在面对模糊性时增加思考深度，通常采用固定的推理预算；二是缺乏持久的语义记忆，仅依赖有限的上下文窗口，导致在长轨迹中难以跟踪进度，出现重复探索、循环行为和对环境动态变化适应不良的问题。</p>
<p>本文针对上述痛点，从语言驱动的认知视角出发，提出VLA模型应具备两大能力：1）自适应推理，使智能体能根据任务复杂度调整其内部思考的粒度；2）基于语言的长期记忆，提供稳定的跨模态语义以支持一致且上下文感知的导航行为。本文的核心思路是提出VLingNav框架，通过自适应思维链（AdaCoT）机制实现快慢思维的动态切换，并通过视觉辅助语言记忆模块（VLingMem）构建持久跨模态记忆，以支持复杂、长视野的导航任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLingNav的整体框架基于一个视频VLM骨干（LLaVA-Video-7B）进行扩展，集成了一个动作模型，以实现同步的文本令牌生成和轨迹规划。输入是机器人的自我中心视频流观测序列和指令，输出是下一个动作（连续轨迹）。其核心创新在于两个模块：自适应思维链（AdaCoT）和视觉辅助语言记忆（VLingMem）。</p>
<p><img src="https://arxiv.org/html/2601.08665v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：VLingNav的整体框架。框架以视频流和多模态指令为输入，通过定制的语言设计产生机器人导航动作。AdaCoT可以根据观察自适应地生成语言思维，而VLingMem则用关键视觉特征总结CoT线索，以实现全局知情决策。</p>
</blockquote>
<p><strong>1. 观察编码</strong>：为了平衡计算负担与决策准确性，论文提出了动态FPS采样策略。该策略受艾宾浩斯遗忘曲线启发，根据历史帧与当前帧的时间间隔动态调整采样率：近期帧（短期记忆）采用较高采样率，远期帧（长期记忆）采用较低采样率。采样后，使用预训练的视觉编码器（SigLIP-400M）提取特征，并进一步通过时间间隔自适应的网格池化策略对历史视觉特征进行下采样，以捕获高层语义并控制计算成本。此外，为消除动态采样带来的时间不一致性，为每一帧视觉特征前添加了时间感知指示符令牌，该令牌使用RoPE编码绝对时间间隔信息，帮助模型感知时序。</p>
<p><strong>2. 自适应思维链（AdaCoT）与视觉辅助语言记忆（VLingMem）</strong>：将编码后的视觉令牌、语言令牌和时间指示符令牌拼接后输入VLM。AdaCoT的核心是让模型自主决定何时进行推理。VLM首先预测一个CoT指示符令牌（<code>&lt;think_on&gt;</code>或<code>&lt;think_off&gt;</code>）。若输出<code>&lt;think_on&gt;</code>，则模型以自回归方式生成具体的CoT内容，包含两部分：一是包裹在<code>&lt;reasoning&gt;</code>和<code>&lt;/reasoning&gt;</code>标签内的推理内容（包括视觉感知、任务分解、是否访问过当前位置的判断以及下一步行动决策）；二是包裹在<code>&lt;summary&gt;</code>和<code>&lt;/summary&gt;</code>标签内的当前观察环境摘要。这个环境摘要将作为语言记忆（VLingMem）被整合到后续的模型输入中，为全局决策提供跨模态的语义上下文，从而避免重复探索并推断动态环境中的移动趋势。</p>
<p><strong>3. 动作模型</strong>：为了将VLM骨干的推理和决策知识转移到机器人特定的动作空间，VLingNav集成了一个基于MLP的动作模型。该模型以VLM预测的最后一个令牌对应的隐藏状态向量为条件，将其转换为机器人的连续运动轨迹（一系列包含位置和朝向的路径点），克服了现有方法中离散动作效率低下或基于流的动作推理速度慢的问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：VLingNav在多个标准具身导航基准上进行了评估，包括HM3D和MP3D场景下的物体目标导航（ObjectNav）、具身视觉跟踪（EVT）和图像目标导航（ImageNav）。实验平台涉及仿真环境（Habitat）和真实世界机器人（TurtleBot3、Unitree Go2）。</p>
<p><strong>对比方法</strong>：基线方法包括模块化方法（如OVRL、OVON）和先进的VLA模型（如Uni-NaVid、NaVILA、StreamVLN、OctoNav、TrackVLA等）。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2601.08665v1/x4.png" alt="仿真实验结果"></p>
<blockquote>
<p><strong>图4</strong>：在HM3D和MP3D ObjectNav任务上的性能对比。VLingNav在成功率（SR）和路径长度加权成功率（SPL）上均显著优于所有基线方法，尤其是在长视野、复杂场景中优势明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.08665v1/x5.png" alt="具身视觉跟踪结果"></p>
<blockquote>
<p><strong>图5</strong>：在EVT-Bench上的性能对比。VLingNav在跟踪成功率（TSR）和平均成功率（ASR）上达到最优，展示了其在动态环境中处理移动目标的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.08665v1/x6.png" alt="图像目标导航结果"></p>
<blockquote>
<p><strong>图6</strong>：在HM3D ImageNav任务上的性能对比。VLingNav同样取得了最佳的成功率，证明了其方法在多任务上的通用性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.08665v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验验证各组件贡献。移除AdaCoT或VLingMem均导致性能显著下降，其中VLingMem对长任务（ObjectNav）提升最大（SR提升7.3%），AdaCoT对动态任务（EVT）提升最大（TSR提升5.8%）。联合使用两者效果最佳。RL后训练阶段进一步带来了性能增益。</p>
</blockquote>
<p><strong>数据集贡献</strong>：论文构建了Nav-AdaCoT-2.9M数据集，这是迄今为止最大的带有推理标注的具身导航数据集，包含290万步和47.2万条自适应CoT标注，覆盖ObjectNav、EVT和ImageNav三种任务，为模型训练提供了关键支持。</p>
<p><img src="https://arxiv.org/html/2601.08665v1/x3.png" alt="数据集统计"></p>
<blockquote>
<p><strong>图3</strong>：VLingNav训练数据集（Nav-AdaCoT-2.9M）的数据分布和指令词云。该数据集规模大、任务类型多、标注丰富。</p>
</blockquote>
<p><strong>零样本真实世界迁移</strong>：<br><img src="https://arxiv.org/html/2601.08665v1/x8.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图8</strong>：VLingNav在真实机器人上执行零样本导航任务的定性结果。成功完成了包括未见过的任务（如“找到微波炉并停在其前方”）在内的复杂指令，展示了强大的跨领域和跨任务泛化能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了VLingNav框架，创新性地集成了自适应思维链（AdaCoT）和视觉辅助语言记忆（VLingMem），使智能体具备了根据情境动态调整推理深度和利用持久语义记忆的能力；2）构建了目前最大规模的带自适应推理标注的具身导航数据集Nav-AdaCoT-2.9M，并引入了在线专家指导的强化学习后训练范式，使模型能够超越模仿学习的限制，获得更鲁棒、自优化的导航行为；3）通过大量实验验证了VLingNav在多个基准上的最先进性能，并首次实现了VLA模型在真实机器人上的零样本复杂任务导航，展现了卓越的泛化能力。</p>
<p>论文自身提到的局限性主要隐含在相关工作中：例如，基于MLP的连续动作策略空间可能仍有优化空间；动态FPS和网格池化的超参数（如记忆稳定性<code>s</code>）可能需要针对不同机器人平台或环境进行调整。</p>
<p>本研究对后续工作的启示包括：语言驱动的认知架构（自适应推理、语言记忆）是提升VLA模型在长视野、动态任务中性能的有效途径；大规模、高质量、富含认知标注的数据集对于训练此类模型至关重要；结合模仿学习与基于结果的强化学习后训练，是解锁VLA模型超越演示数据潜力、实现自我改进的关键方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VLingNav模型，旨在解决现有视觉-语言-动作（VLA）模型在具身导航中缺乏显式推理与持久记忆、难以处理复杂长视野任务的问题。其核心技术包括：1）自适应思维链（AdaCoT）机制，动态触发显式推理，实现直觉执行与深思规划的灵活切换；2）视觉辅助语言记忆模块（VLingMem），构建跨模态语义记忆以回顾历史观测、推断环境动态。实验表明，VLingNav在多个具身导航基准上达到SOTA性能，并能零样本迁移至真实机器人，成功执行未见过的导航任务，展现出强大的跨领域泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.08665" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>