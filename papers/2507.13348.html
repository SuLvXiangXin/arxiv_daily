<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.13348" target="_blank" rel="noreferrer">2507.13348</a></span>
        <span>作者: Yang, Senqiao, Li, Junyi, Lai, Xin, Yu, Bei, Zhao, Hengshuang, Jia, Jiaya</span>
        <span>日期: 2025/07/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）的性能提升往往伴随着视觉令牌数量的指数级增长，例如一张2048×1024的照片在Qwen2.5-VL中需要多达2,678个视觉令牌。为了提升效率，现有方法（如剪枝、合并）大多采用固定的压缩比率或阈值来处理所有图像。然而，作者通过实验观察到一个关键现象：对于大多数通用视觉问答（General VQA）场景（如MME、RealWorldQA），即使将图像分辨率降低四倍（视觉令牌减少75%），模型性能下降也微乎其微；但对于需要精细视觉理解（如OCR相关）的任务（如ChartQA、OCRBench），减少视觉令牌会导致性能显著下降。这表明不同样本的视觉冗余程度差异巨大。</p>
<p>因此，本文针对“是否应对所有场景应用统一的令牌压缩比率”这一具体痛点，提出了一个<strong>样本级动态自适应</strong>的新视角。其核心思路是：让模型从下采样的低分辨率图像开始推理，并智能判断信息是否足够；若不足，则输出一个特殊令牌来请求原始高分辨率图像。这种方法旨在对简单样本实现高效推理，同时对复杂样本保持高性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>VisionThink的目标是训练一个能够自主判断给定低分辨率图像信息是否足以准确回答问题的VLM。其整体流程是一个两阶段决策过程：首先输入下采样图像，若模型判断信息足够，则直接生成答案；若判断信息不足，则输出一个特殊令牌（函数调用）请求高分辨率图像，然后基于高分辨率图像再次生成答案。</p>
<p><img src="https://arxiv.org/html/2507.13348v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：VisionThink框架示意图。(a) 左侧：模型处理分辨率降低四倍的图像，认为信息足够，直接给出答案。(b) 右侧：模型检测到信息不足（如需要阅读小字），请求高分辨率图像后再回答问题。</p>
</blockquote>
<p>为了实现这一目标，需要解决两大挑战：1）在答案开放多样的通用VQA任务上应用强化学习（RL）；2）使模型学会在效率与精度间平衡，准确判断何时需要高分辨率。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>LLM-as-Judge策略（解决RL评估难题）</strong>：通用VQA的答案多样且依赖上下文，传统的基于规则的RL评估方法失效。本文提出使用一个外部大语言模型（LLM）作为评判者，<strong>纯文本地</strong>比较模型输出与标准答案，并给出0（错误）或1（正确）的离散奖励。这种方法避免了视觉内容带来的偏见，并利用了LLM的广泛知识进行灵活、语义化的判断，使得大量监督微调（SFT）数据可直接用于RL训练。</li>
<li><strong>多轮组相对策略优化（Multi-Turn GRPO）</strong>：由于VisionThink的决策过程涉及可能的两轮交互（先看低清图，可能再请求高清图），本文将标准的GRPO算法扩展为多轮版本。其优化目标（式3）对模型在多轮中生成的所有令牌进行优化，但通过掩码操作，只对VLM自身生成的文本令牌计算策略梯度，而屏蔽掉用户提供的图像令牌和函数调用响应令牌。</li>
<li><strong>精心设计的奖励函数</strong>：总奖励 <code>ℛ_overall = ℛ_accuracy + ℛ_format - 𝒫_control</code>。<ul>
<li><code>ℛ_accuracy</code>：由LLM-as-Judge给出的准确性奖励（0/1）。</li>
<li><code>ℛ_format</code>：格式奖励（最高0.5分），确保模型遵循指令格式，包括将推理过程包裹在<code>&lt;think&gt;&lt;/think&gt;</code>标签中，答案在<code>&lt;answer&gt;&lt;/answer&gt;</code>标签中，且函数调用符合特定JSON格式。</li>
<li><code>𝒫_control</code>：<strong>动态惩罚控制机制</strong>，这是平衡模型行为的关键。由于高分辨率图像通常带来更高准确率，若无惩罚，模型会倾向于总是请求高分辨率，导致效率丧失。反之，若对所有请求高分辨率的行为施加固定惩罚，模型又会因低分辨率图像偶尔能“猜对”而倾向于总是直接回答。为此，本文设计了一个基于概率阈值的动态惩罚（式5）。首先，在训练数据上统计使用低分辨率图像能正确回答的样本比例 <code>r</code>。设定一个阈值θ（设为0.2）。对于每个训练样本，若 <code>r &lt; θ</code>（即此类问题用低清图很难答对），则对模型选择“直接回答”的行为施加0.1的惩罚，鼓励其请求高分辨率；若 <code>r ≥ θ</code>（即此类问题用低清图较易答对），则对模型选择“请求高分辨率”的行为施加0.1的惩罚，鼓励其节约计算。这一机制有效避免了模型行为塌陷。</li>
</ul>
</li>
<li><strong>训练数据准备</strong>：为了使模型学会分辨何时需要高分辨率，需要构建包含两种类型样本的训练集。使用基础模型（Qwen2.5VL-Instruct）对每个训练样本进行8次随机 rollout（温度=1）。如果某样本在使用高分辨率和低分辨率图像时，8次 rollout 全部答对，则将其标记为“可用低分辨率解决”。如果某样本使用高分辨率图像的正确次数比使用低分辨率图像多6次或以上，则标记为“需要高分辨率”。最终从数据集中各选取1万个样本用于训练。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>范式创新</strong>：不同于现有方法“先处理全分辨率图像，后压缩令牌”的范式，VisionThink采用“先输入压缩图像，后按需请求高清”的范式，实现了<strong>样本级的动态自适应压缩</strong>。</li>
<li><strong>RL应用创新</strong>：提出了LLM-as-Judge策略，成功将RL训练拓展到答案开放多样的通用VQA任务，突破了以往RL多限于视觉数学推理等结构化任务的局限。</li>
<li><strong>训练机制创新</strong>：设计了动态惩罚控制机制，巧妙地引导模型在效率与精度间做出合理权衡，稳定控制了高分辨率请求的调用比例。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准数据集</strong>：在多个通用VQA基准上评估，包括强OCR相关的ChartQA、OCRBench、MathVista，以及MMVet、RealWorldQA、MathVerse等。</li>
<li><strong>对比基线</strong>：与当前开源和闭源SOTA模型对比，包括GPT-4o、Claude-3.5 Sonnet、Gemini-1.5-Pro，以及InternVL2、LLaVA-OneVision、MiniCPM-V、InternVL2.5等开源模型。</li>
<li><strong>实施细节</strong>：基于Qwen2.5-VL-7B-Instruct模型，使用veRL框架训练，总批次大小512，学习率1e-6。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.13348v1/x1.png" alt="关键观察与性能效率对比"></p>
<blockquote>
<p><strong>图1</strong>：关键观察与VisionThink的性能和效率。<strong>左</strong>：观察到在大多数通用场景（MME, RealWorldQA）中，大幅减少视觉令牌对性能影响很小，但在OCR相关任务（ChartQA, OCRBench）上性能下降显著。<strong>右</strong>：VisionThink在性能-效率帕累托前沿上优于先前的高效VLM方法（如ZipVLM, EfficientVLM）。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>总体性能</strong>：如表1（对应论文Table 6）所示，VisionThink‡（仅用完整图像训练，具备LLM-as-Judge能力但无动态分辨率决策）相比基础模型Qwen2.5-VL-7B*在多个基准上有显著提升，例如MathVista testmini从68.2提升至71.2，MMVet从61.6提升至67.5，证明了RL训练的有效性。完整的VisionThink模型在保持竞争力性能的同时，实现了效率的大幅提升。</li>
<li><strong>效率提升</strong>：如图1右所示，VisionThink在效率（视觉令牌数）和性能的权衡上达到了新的帕累托前沿。特别是在DocVQA任务上，实现了高达100%的加速（图4），因为大量简单样本无需处理高分辨率图像。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.13348v1/x3.png" alt="惩罚控制机制的影响"></p>
<blockquote>
<p><strong>图3</strong>：(a) 惩罚控制机制的影响。移除惩罚或施加固定惩罚都会导致模型行为塌陷（调用率接近100%或0%），而本文的动态惩罚机制将调用率稳定在合理水平（约20%）。(b) VisionThink通过自主请求高分辨率图像，成功解决了需要阅读细小文字的OCR问题。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验与组件分析</strong>：<ul>
<li><strong>惩罚机制</strong>：图3(a)清晰展示了动态惩罚机制的必要性。没有它，模型无法学会平衡。</li>
<li><strong>定性结果</strong>：图5展示了VisionThink能根据问题内容做出智能决策：对于需要阅读文字的问题请求高分辨率，对于物体颜色等简单问题则直接基于低分辨率图像回答。</li>
<li><strong>数据构成</strong>：使用第3.5节方法筛选的平衡数据集对模型学会分辨需求至关重要。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.13348v1/x4.png" alt="效率对比示例"></p>
<blockquote>
<p><strong>图4</strong>：在DocVQA基准上的效率对比。VisionThink在保持高性能的同时，实现了显著的加速，因为许多样本无需处理高分辨率图像。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了VisionThink，一种<strong>样本级动态自适应视觉令牌压缩</strong>的新范式，通过“先压缩，后按需请求”的机制，在保持强OCR任务性能的同时，大幅提升了简单任务的推理效率。</li>
<li>设计了<strong>LLM-as-Judge策略</strong>，成功将强化学习训练拓展到答案开放多样的通用VQA任务，为多模态RL提供了新的评估思路。</li>
<li>提出了<strong>动态惩罚控制机制</strong>和针对性的<strong>数据收集方法</strong>，确保了模型能够稳定、合理地学习何时需要高分辨率输入，避免了行为塌陷。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前方法主要聚焦于分辨率层面的动态调整，未来可与空间级令牌压缩方法（如剪枝、合并）结合以获得更大效率提升。此外，训练数据的筛选依赖于基础模型的多次采样，可能存在一定偏差。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>样本级自适应是方向</strong>：高效VLM的设计应更多考虑输入样本的异质性，实现更细粒度的计算资源分配。</li>
<li><strong>LLM-as-Judge的潜力</strong>：该评估策略可推广至其他开放域、复杂输出的多模态任务（如图像描述、视觉推理）的RL训练中。</li>
<li><strong>迈向多模态智能体</strong>：VisionThink让VLM学会了根据任务需求自主调用视觉工具（请求高分辨率），这为构建能够调用多种工具完成复杂任务的多模态智能体奠定了基础。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VisionThink，旨在解决视觉语言模型中视觉令牌数量过多、效率低下的问题。核心方法是采用强化学习框架，让模型能够根据输入图像和问题的复杂度，动态决策是否需要请求更高分辨率图像，而非对所有场景采用固定压缩比率。实验表明，在多数通用VQA任务上，仅使用1/4分辨率（减少75%视觉令牌）对性能影响甚微，而该方法在OCR等需要细粒度理解的任务上表现优异，同时显著提升了整体效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.13348" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>