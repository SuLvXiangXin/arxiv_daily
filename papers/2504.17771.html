<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.17771" target="_blank" rel="noreferrer">2504.17771</a></span>
        <span>作者: Wang, Haochen, Shi, Zhiwei, Zhu, Chengxi, Qiao, Yafei, Zhang, Cheng, Yang, Fan, Ren, Pengjie, Lu, Lan, Xuan, Dong</span>
        <span>日期: 2025/04/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>控制敏捷机器人（如运动机器人）目前主要采用基于模型或基于学习的方法。基于模型的策略（如逆运动学）能提供稳定、安全的控制，但高度依赖精确的环境模型，在复杂动态环境中鲁棒性和灵活性不足。基于学习的方法（如模仿学习IL和强化学习RL）在训练数据分布内成功率更高、适应性更强，但面临策略网络收敛困难、训练耗时（尤其是稀疏奖励任务）以及神经网络不可解释性带来的安全风险等挑战。针对敏捷羽毛球机器人这一具体任务，现有工作未能有效融合两种方法以降低训练复杂度并确保安全稳定。本文提出新视角：将基于模型的控制与基于学习的控制相结合，发挥各自优势。核心思路是：针对机器人全身控制，采用基于模型的策略控制高速移动的底盘，采用基于学习的策略控制执行击球动作的机械臂；并设计一种物理信息化的“IL+RL”训练框架来高效、安全地训练手臂策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架（Hamlet）将机器人全身控制解耦为底盘移动和手臂控制两部分。输入是视觉感知的羽毛球轨迹，输出是底盘的目标位置和手臂各关节的速度控制命令。核心模块包括：1）基于模型的底盘控制策略；2）基于学习的手臂控制策略网络；3）连接两者的坐标变换模块；4）用于训练手臂策略的物理信息化“IL+RL”流程。</p>
<p><img src="https://arxiv.org/html/2504.17771v2/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：Hamlet 系统组件概览。绿色框代表处理步骤（球的检测与轨迹预测），蓝色框代表基于模型的底盘控制策略，深橙色框代表球轨迹的刚性坐标变换，黄色框代表基于学习的手臂控制策略。</p>
</blockquote>
<p><strong>底盘控制策略（模型驱动）</strong>：出于安全性和sim2real迁移难度考虑，底盘采用基于模型的策略。该策略根据预测的击球时间 (t_h) 和当前位置 (t_{cur}) 计算可用时间 (t_{avail})。若 (t_{avail}) 较大（&gt; (T_b)），则基于固定击球高度 (H_b) 从预测轨迹 (\hat{F}<em>o) 中选取击球点 (p_h)，并通过逆运动学计算底盘目标位置 (p</em>{tar})。为增加初始阶段稳定性，引入信任因子 (\sigma) 对目标位置进行平滑（公式2,3）。若 (t_{avail}) 不足（&lt;= (T_b)），则不再重新计算 (p_{tar}) 以保持平台运动稳定。底盘最终会提供一个可达的基准位置 (p_{base}) 给手臂策略。</p>
<p><strong>手臂控制策略（学习驱动）</strong>：手臂控制被建模为马尔可夫决策过程（MDP）。策略网络 (\pi_\theta) 是一个四层MLP，输入为形状 (8, 8) 的张量，包含最近8个时间步的5维关节角度和3维（经坐标变换后的）羽毛球位置 (\widetilde{p}<em>o)。输出为5维关节速度命令。手臂策略的输入基于一个关键假设：机器人底盘已到达 (p</em>{base})。因此，需要将世界坐标系下的球轨迹 (F_o) 通过平移（由 (p_{base}) 决定）和旋转（由底盘朝向 (\alpha) 决定）刚性变换到以 (p_{base}) 为原点的机器人坐标系（公式1），得到 (\widetilde{p}_o) 供策略网络使用。</p>
<p><strong>物理信息化“IL+RL”训练方法</strong>：这是本文的核心创新。训练分为两个阶段，并由一个拥有特权信息（在仿真中可直接访问完整、准确的羽毛球轨迹）的模型策略 (\hat{\pi}_{PM}) 全程指导。</p>
<p><img src="https://arxiv.org/html/2504.17771v2/x3.png" alt="训练流程"></p>
<blockquote>
<p><strong>图3</strong>：物理信息化“IL+RL”训练流程示意图。分为模仿学习（IL）和强化学习（RL）两个阶段，由模型策略 (\hat{\pi}_{PM}) 提供监督。</p>
</blockquote>
<ol>
<li><strong>模仿学习（IL）阶段</strong>：使用DAgger算法进行策略预热。优化目标是最大化策略网络输出动作与教师动作 (\hat{\pi}_{PM}(s_t)) 的对数似然（公式4）。<strong>关键改进</strong>：在IL阶段同时训练评论家网络 (V_\phi)，通过最小化贝尔曼误差（公式5）来估计状态价值，以缓解从IL过渡到RL时可能出现的性能下降问题。</li>
<li><strong>强化学习（RL）阶段</strong>：使用近端策略优化（PPO）进行策略提升。优势估计 (\hat{A}<em>t) 由广义优势估计（GAE）计算（公式6）。策略网络的优化目标 (J^{RL}(\theta)) 包含两部分（公式8）：标准的PPO裁剪目标 (J^{CLIP}(\theta)) 和由教师策略 (\hat{\pi}</em>{PM}) 提供的辅助监督损失 (J^{SUP}(\theta))，后者通过权重 (\lambda) 进行调节。这为策略在RL阶段的探索提供了由物理模型定义的“软边界”，使其更高效、安全，且无需复杂的奖励塑形或课程学习。</li>
</ol>
<p>此外，训练准备还包括：使用增强的真实世界球轨迹数据（通过平移、翻转进行数据增强）来缩小仿真与现实差距；以及在仿真中对底盘控制注入噪声，以促进零样本（zero-shot）策略迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在自研的羽毛球机器人平台上进行（见图1），该平台包括一个全向移动底盘和一个5自由度机械臂。使用高帧率双目相机进行视觉感知。对比的基线方法包括：纯模型方法（Model-based）、纯模仿学习方法（BC）、纯强化学习方法（RL）以及先模仿学习后强化学习但无教师监督的方法（BC+RL）。</p>
<p><img src="https://arxiv.org/html/2504.17771v2/x5.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：不同方法在仿真中的成功率对比。Hamlet（Ours）取得了最高的成功率（94.5%），显著优于其他基线方法。</p>
</blockquote>
<p><strong>关键定量结果</strong>：在对抗发球机的实验中，Hamlet取得了94.5%的成功率。在对抗人类玩家的实验中，取得了90.7%的成功率，且最长连续对打（rally）长度达到40次。这验证了系统在真实动态环境中的高效性和鲁棒性。</p>
<p><img src="https://arxiv.org/html/2504.17771v2/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融实验展示了各训练组件的贡献。移除教师监督（RL w/o Sup.）、在IL阶段不训练评论家（IL w/o Critic）、或使用非特权信息的教师（Teacher w/o Priv.）都会导致性能显著下降。</p>
</blockquote>
<p><strong>消融实验分析</strong>：图4的消融实验明确了各组件贡献：1）<strong>教师监督</strong>（RL阶段的 (J^{SUP})）：其缺失导致成功率大幅下降（从94.5%降至67.8%），证明了物理模型引导对RL探索的重要性。2）<strong>IL阶段训练评论家</strong>：其缺失导致从IL到RL过渡时出现性能骤降，证明了该设计对稳定训练的必要性。3）<strong>教师拥有特权信息</strong>：使用基于不完整预测轨迹的教师，性能低于使用完整轨迹的教师，说明了高质量监督信号的价值。</p>
<p><img src="https://arxiv.org/html/2504.17771v2/x7.png" alt="泛化能力"></p>
<blockquote>
<p><strong>图7</strong>：手臂策略的零样本泛化能力测试。训练好的策略无需重新训练，即可成功迁移到不同运动特性的底盘（A, B, C型）上。</p>
</blockquote>
<p><strong>泛化与定性结果</strong>：如图7所示，训练好的手臂策略能够零样本泛化到三种不同运动特性的底盘上，无需重新训练，体现了架构解耦的优势。图6的定性结果序列展示了机器人成功回击人类对手多种击球（高远球、吊球、杀球）的过程。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文核心贡献在于：1）提出了 <strong>Hamlet</strong>，首个融合模型与学习方法的敏捷羽毛球机器人全身控制系统，采用“模型底盘+学习手臂”的混合架构。2）设计了一种 <strong>物理信息化的“IL+RL”训练流程</strong>，利用特权信息教师策略在IL阶段预热策略和评论家，在RL阶段为策略探索提供安全、高效的监督。3）在真实系统中实现了 <strong>高性能</strong>（对发球机94.5%，对人90.7%成功率）和 <strong>零样本策略泛化</strong>（适配不同底盘）。</p>
<p>论文提到的局限性包括：手臂策略的成功在一定程度上依赖于上游模块（如轨迹预测）的准确性；系统针对训练中见过的击球类型表现良好，但对全新击球类型的泛化能力未经验证。</p>
<p>本文的启示在于：这种“模型负责安全/稳定、学习负责复杂/灵活”的混合控制范式，以及利用物理模型为学习过程提供结构化指导的训练框架，可有效应用于其他需要高速反应和全身协调的移动操作任务，如高速接球、乒乓球等，为复杂机器人系统的安全、高效学习提供了可行路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对敏捷羽毛球机器人控制中，学习策略与模型方法难以协调、训练复杂且安全性不足的核心问题，提出混合控制系统Hamlet。关键技术包括：基于模型的底盘移动策略为手臂策略提供基础；采用物理信息引导的“模仿学习+强化学习”框架训练手臂策略，并在模仿学习阶段预训练评论家模型以缓解策略切换时的性能下降。在自研机器人上的实验表明，系统对抗发球机的成功率达94.5%，对抗人类玩家达90.7%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.17771" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>