<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Pixel Motion Diffusion is What We Need for Robot Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Pixel Motion Diffusion is What We Need for Robot Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.22652" target="_blank" rel="noreferrer">2509.22652</a></span>
        <span>作者: Michael S. Ryoo Team</span>
        <span>日期: 2025-09-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操控领域主要有两类方法。一类是基于像素或点跟踪的多阶段方法，如Im2Flow2Act、ATM和LangToMo，它们通过预测中间像素运动轨迹来实现模块化、可解释的控制。另一类是端到端的视觉-语言-动作模型以及基于潜在特征的层次方法，它们在基准测试上性能更优。然而，前者在高层运动生成上未能充分利用视觉生成建模的最新进展，其低层控制器也未以最优方式利用扩散策略的进步；而后者虽然性能强大，但缺乏可解释的中间表示。</p>
<p>本文针对上述性能与可解释性难以兼得的痛点，提出了一种新视角：将像素运动作为一种结构化的中间表示，并用两级扩散模型分别负责生成该运动和执行控制。本文的核心思路是：提出一个名为DAWN的统一框架，其中高层运动生成和低层动作策略均实例化为扩散模型，并通过显式的像素运动表征进行连接，从而结合层次化运动分解和端到端视觉运动智能体的优势，同时保持可解释性和模块化。</p>
<h2 id="方法详解">方法详解</h2>
<p>DAWN的整体框架由两个核心的扩散模块组成：<strong>Motion Director</strong>和<strong>Action Expert</strong>。输入包括多视角（如静态相机和夹爪相机）的当前视觉观测、语言指令以及机器人状态。整个流程是：首先，观测被编码为条件嵌入；在此基础上，Motion Director生成一个从第三人称视角看的密集像素运动表示；随后，Action Expert利用此像素运动及其他条件，生成可执行的机器人动作序列。</p>
<p><img src="https://nero1342.github.io/DAWN/figures/overview.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：DAWN方法整体框架。左侧为观测编码器；基于编码后的条件，一个潜空间扩散模型“运动引导器”生成像素运动表示；随后扩散策略“动作专家”利用该表示生成机器人动作。</p>
</blockquote>
<p><strong>Motion Director</strong> 是一个基于预训练潜扩散模型的模块，其目标是估计从当前帧 I_t 到未来某帧 I_{t+k} 的像素运动 F‘_{t,k}。它将像素运动编码为一个三通道图像（水平位移u、垂直位移v以及均值）。其核心是一个U-Net去噪器 U_M。在推理时，它将高斯噪声张量 N_{M,n} 与当前帧的VAE潜编码 V^E_M(I_t) 拼接，形成噪声潜表示 O_{M,n}。U-Net在语言嵌入 T_M(L)、另一视角的视觉嵌入 E_M(G_t) 以及时间偏移量k的联合条件下，对 O_{M,n} 进行迭代去噪。去噪后的潜张量最终由VAE解码器 V^D_M 解码为预测的像素运动图像。训练时，仅更新U-Net，损失函数为噪声估计的均方误差损失，真值像素运动由RAFT光流模型计算得到。</p>
<p><img src="https://nero1342.github.io/DAWN/figures/motion_director.png" alt="运动引导器架构"></p>
<blockquote>
<p><strong>图2</strong>：运动引导器架构。模型编码静态相机视图，并在夹爪视图、语言指令和时间偏移量的条件下，用U-Net对其进行去噪。输出被解码为预测的像素运动，提供可解释的运动表示。</p>
</blockquote>
<p><strong>Action Expert</strong> 是一个扩散策略模块，负责将像素运动转化为机器人动作。它包含视觉编码器 V_A、文本编码器 T_A、状态编码器 S_A 和一个去噪Transformer U_A。在推理时，Motion Director预测的像素运动、当前视觉观测、语言指令和机器人状态分别被编码并投影为令牌嵌入，拼接后作为条件序列。动作生成从一个采样自高斯先验的噪声动作块开始，由 U_A 在多重条件引导下迭代去噪，最终输出一个连贯的可执行动作序列。视觉和文本编码器使用预训练模型并保持冻结，状态编码器和去噪Transformer从头开始训练，损失函数同样是均方误差噪声估计损失。</p>
<p><img src="https://nero1342.github.io/DAWN/figures/action_expert.png" alt="动作专家架构"></p>
<blockquote>
<p><strong>图3</strong>：动作专家架构。模型将预测的像素运动、视觉观测、语言指令和机器人状态编码为多模态特征。这些输入条件化去噪过程，该过程迭代地将噪声动作细化为可执行的机器人轨迹。</p>
</blockquote>
<p>与现有方法相比，DAWN的创新点具体体现在：1）提出了一个<strong>完全可训练</strong>的两级扩散框架，两个模块均通过扩散过程建模；2）高层运动生成采用了<strong>迭代去噪的潜扩散模型</strong>，相比直接在像素空间扩散的方法更高效、分辨率更高；3）低层控制器是一个<strong>强化的基于Transformer的扩散策略</strong>，能够更好地处理多模态条件和时序依赖。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了三个基准：模拟环境<strong>CALVIN</strong>和<strong>MetaWorld</strong>，以及<strong>真实世界</strong>环境。对比的基线方法包括：Diffusion Policy、Robo-Flamingo、RoboUniview、Seer、VPP、GR-1、Vidman、LTM、DreamVLA等。</p>
<p>在<strong>CALVIN ABC→D</strong>的零样本长视野评估中（要求连续完成5个任务），DAWN取得了优异的成绩。在不使用外部机器人数据的情况下（表1），DAWN以平均完成长度<strong>4.00</strong>和首个任务成功率<strong>98.1%</strong> 的成绩达到最先进水平，超越了VPP（3.93）和Seer-Large（3.83）。仅使用Action Expert的简化版性能为2.78，凸显了像素运动表示的关键作用。在使用DROID作为外部数据时（表2），DAWN取得了平均长度<strong>4.10</strong>，与使用更多、更杂数据源的VPP（4.33）和DreamVLA（4.44）相比具有竞争力，展现了其数据效率。</p>
<p><img src="https://nero1342.github.io/DAWN/figures/table1.png" alt="CALVIN结果表（无外部数据）"></p>
<blockquote>
<p><strong>表1</strong>：CALVIN评估（无外部机器人数据）。DAWN在五项连续任务的成功率和平均完成长度上均达到最先进水平。</p>
</blockquote>
<p><img src="https://nero1342.github.io/DAWN/figures/table2.png" alt="CALVIN结果表（有外部数据）"></p>
<blockquote>
<p><strong>表2</strong>：CALVIN评估（使用外部机器人数据）。DAWN在使用相对较少的外部数据（DROID）下，取得了具有竞争力的性能。</p>
</blockquote>
<p>在<strong>MetaWorld</strong>的11项任务评估中（表3），DAWN取得了<strong>65.4%</strong> 的平均成功率，显著超过了之前的先进方法LTM（57.7%）和ATM（52.0%）。尤其在语义相似但目标相反的任务对上（如开门vs关门）表现突出，显示了其强大的语言理解能力。</p>
<p><img src="https://nero1342.github.io/DAWN/figures/table3.png" alt="MetaWorld结果表"></p>
<blockquote>
<p><strong>表3</strong>：MetaWorld任务成功率。DAWN在11项任务上的整体性能达到最先进水平。</p>
</blockquote>
<p>在<strong>真实世界</strong>的“拾取-放置”任务评估中（表4），仅经过少量微调的DAWN在成功放置目标物体方面表现可靠，并且错误抓取或放置非目标物体（Wrong Obj.）的情况很少。经过完整微调的DAWN*性能进一步提升，接近或达到最先进水平，证明了其在模拟到现实迁移中的有效性。</p>
<p><img src="https://nero1342.github.io/DAWN/figures/table4.png" alt="真实世界结果表"></p>
<blockquote>
<p><strong>表4</strong>：真实世界单次拾取放置评估。DAWN在成功率和减少错误操作对象方面表现出色。</p>
</blockquote>
<p><img src="https://nero1342.github.io/DAWN/figures/real_world_setup.png" alt="真实世界环境与结果可视化"></p>
<blockquote>
<p><strong>图5</strong>：真实世界环境示例与推演可视化。a) 实验设置；b-c) 相机视图；右侧对比了不同方法预测的像素运动与基线方法预测的RGB目标，DAWN的像素运动更清晰地指示了“苹果”的运动方向。</p>
</blockquote>
<p>消融实验（表5）总结了各组件贡献：1）使用<strong>像素运动</strong>作为中间表示（4.00）比使用RGB目标图像（3.21）或不用任何中间表示（2.78）性能显著提升；2）在Motion Director中加入<strong>夹爪视角</strong>作为条件能带来性能增益（从3.74提升至4.00）；3）Motion Director的<strong>扩散步数</strong>设置为25时达到最佳平衡（4.00），步数过少或过多均会导致性能略有下降。</p>
<p><img src="https://nero1342.github.io/DAWN/figures/table5.png" alt="消融实验表"></p>
<blockquote>
<p><strong>表5</strong>：在CALVIN数据集上的消融研究。验证了像素运动表示、夹爪视角条件和扩散步数对性能的影响。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了<strong>DAWN框架</strong>，首次将预训练的潜扩散模型适配用于密集像素运动生成，并用其引导一个完全可学习的扩散策略，形成了统一的两级扩散架构。2）在<strong>CALVIN、MetaWorld和真实世界基准</strong>上取得了最先进或极具竞争力的性能，证明了基于像素运动的视觉语言动作方法在数据效率和可扩展性方面的优势。3）框架设计<strong>显式地利用了预训练视觉和语言模型</strong>，模块化结构支持独立升级，实现了高效的跨域迁移，并保持了中间表示的可解释性。</p>
<p>论文自身提到的局限性主要在于<strong>模拟与真实世界之间存在较大的域差距</strong>，且可用于微调的真实世界数据有限。然而，实验表明通过最小程度的微调即可实现可靠迁移，这在一定程度上缓解了该问题。</p>
<p>本文对后续研究的启示是：<strong>结构化的像素运动表示与强大的扩散生成模型相结合</strong>，为构建可解释、数据高效且性能强大的机器人控制框架提供了一个极具前景的方向。其模块化设计允许未来分别集成视觉生成或控制策略方面的最新进展，具有很好的灵活性和扩展性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DAWN框架，解决现有机器人控制中基于像素运动的两阶段方法性能不足的问题。核心技术是构建两级扩散模型：高层“运动指导器”为潜在扩散模块，根据视觉观察与语言指令生成结构化的像素运动表示；低层“动作专家”为扩散策略，将该表示转化为机器人动作。实验表明，DAWN在CALVIN基准上达到SOTA性能，在MetaWorld上验证有效，并能以少量微调实现仿真到现实的可靠迁移。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.22652" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>