<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.20937" target="_blank" rel="noreferrer">2511.20937</a></span>
        <span>作者: Wang, Qineng, Huang, Wenlong, Zhou, Yu, Yin, Hang, Bao, Tianwei, Lyu, Jianwen, Liu, Weiyu, Zhang, Ruohan, Wu, Jiajun, Fei-Fei, Li, Li, Manling</span>
        <span>日期: 2025/11/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前评估视觉语言模型（VLMs）具身认知能力的工作，通常关注静态场景的空间感知、语言推理与规划，或简单物体间的交互推理。这些方法虽然互补，但缺乏一个统一、客观的框架，将自我中心感知与日常活动中的具身交互紧密耦合起来。本文旨在解决这一痛点，提出通过“世界建模”（World Modeling）这一新视角来评估VLMs的具身认知水平。其核心思路是：将评估具身认知转化为一个部分可观测马尔可夫决策过程（POMDP）框架下的世界建模问题，具体表现为两种序列重排序的视觉问答任务——前向世界建模（给定动作排序观测）和逆向世界建模（给定观测排序动作），从而间接评估模型对可操作性、动作-效果推理、具身意识等核心能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>ENACT的整体框架是一个从机器人仿真轨迹到问答对生成的数据流水线，其输入是机器人操作轨迹（包含仿真状态和RGB观测），输出是用于评估的前向或逆向世界建模问答对。</p>
<p><img src="https://arxiv.org/html/2511.20937v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ENACT数据构建流程概览。首先从机器人仿真（BEHAVIOR）中获取对齐的场景图（状态）和RGB观测。然后通过识别发生抽象状态变化（即场景图差异非空）的关键帧对轨迹进行分割。从这些关键帧集合中，采样出多个关键帧轨迹，用于构建前向和逆向世界建模的VQA问题。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><strong>问题形式化</strong>：将环境定义为状态空间 $\mathcal{S}$（符号化场景图）、观测空间 $\mathcal{O}$（自我中心RGB视图）和动作空间 $\mathcal{A}$（可见的场景图差异 $a_t = \Delta_{\mathrm{Vis}}(s_{t}, s_{t-1})$）。每个评估实例是一个从长轨迹中采样出的关键帧轨迹 $\pi = (0, \cdots, L-1)$，包含状态序列 $S_\pi$ 和观测序列 $O_\pi$。</li>
<li><strong>关键帧轨迹合成</strong>：<ul>
<li><strong>分割帧提取</strong>：从原始轨迹中筛选出场景图差异 $\delta(s_t, s_{t-1})$ 非空的时间戳 $t$，并通过比较变化签名避免近重复帧，得到有序的分割帧集合 $\mathcal{K}$。</li>
<li><strong>轨迹采样与验证</strong>：从 $M$ 个分割帧中，采样长度为 $L$ 的关键帧索引序列 $\pi = (i_0, \ldots, i_{L-1})$（索引递增但不必相邻）。对每个候选轨迹进行严格验证：要求连续关键帧之间的可见状态变化 $\Delta_{\mathrm{Vis}}$ 非空，且被编辑的物体在两张图像中均可见（物体转变事件允许暂时遮挡）。通过这种“选座”组合方法，可从单条长轨迹生成大量（最多 $\binom{M}{L}$ 个）评估实例，实现数据生成的规模化。</li>
</ul>
</li>
<li><strong>世界建模QA生成</strong>：基于验证通过的关键帧轨迹，构建两种序列重排序任务：<ul>
<li><strong>前向世界建模</strong>：给定初始观测 $o_0$、正确顺序的动作序列 $(a_0, \ldots, a_{L-2})$ 以及打乱顺序的未来观测列表 $O&#39;$，模型需输出一个排列 $\sigma$，将观测排序以匹配动作序列产生的效果。</li>
<li><strong>逆向世界建模</strong>：给定初始观测 $o_0$、正确顺序的观测序列 $(o_1, \ldots, o_{L-1})$ 以及打乱顺序的动作列表 $A&#39;$，模型需输出一个排列 $\tau$，将动作排序以与观测变化过程一致。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.20937v1/x2.png" alt="数据与示例"></p>
<blockquote>
<p><strong>图3</strong>：数据来源及问答示例。ENACT基于真实机器人执行的多样长时程活动（左）构建。图中展示了前向世界建模（中）和逆向世界建模（右）的示例。</p>
</blockquote>
<p>与现有方法相比，ENACT的创新点在于：1) 提出了一个统一、可扩展的评估框架，通过序列重排序任务剥离了逼真视频合成的干扰，专注于长时程交互视觉推理；2) 利用机器人仿真器（BEHAVIOR）提供的真实物理状态和场景图，实现了问答对的大规模、自动化生成；3) 首次系统性地通过前向与逆向世界建模这两个互补任务来诊断VLMs的具身认知缺陷。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与平台</strong>：基准测试数据集ENACT从BEHAVIOR仿真器的29项长时程家庭规模活动中构建，包含8,972个QA对，步骤长度 $L$ 从3到10。评估了7个专有VLMs（如GPT-5、Gemini 2.5 Pro、Claude Sonnet 4）和23个开源VLMs（如InternVL3.5、GLM-4.5V、Qwen2.5-VL）。同时进行了人工评估（3位标注者，Krippendorff‘s α=0.83）作为对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>总体性能与人类差距</strong>：如表1所示，所有VLMs的性能均显著落后于人类。人类在各项任务和步骤长度上保持高且稳定的准确率（Pairwise Accuracy大多在90%以上），而VLMs的性能随步骤长度 $L$ 增加而急剧下降。例如，在 $L=10$ 时，即使最强的GPT-5在前向和逆向任务上的Pairwise Accuracy也仅分别为46.93%和55.33%，与人类约95%的水平相去甚远。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.20937v1/figures/huggingface.png" alt="性能对比"></p>
<blockquote>
<p><strong>图1</strong>：ENACT将具身认知评估转化为通过自我中心交互进行世界建模的任务。评估显示，VLMs的性能随交互时程增长而下降，在逆向任务上表现更好，且落后于人类。</p>
</blockquote>
<ol start="2">
<li><strong>前向与逆向任务不对称性</strong>：所有模型在逆向世界建模任务上的表现 consistently优于前向任务，且随着 $L$ 增大，差距越发明显。这表明模型更擅长基于观测结果进行回溯式的语言（动作）推理，而非基于动作进行前向的视觉（效果）模拟。</li>
<li><strong>对图像真实性和相机配置的敏感性</strong>：<ul>
<li><strong>真实性</strong>：在仿真环境中，改变渲染质量（如路径追踪、仅光线追踪、风格化到真实世界）对GPT-5 mini等模型的性能没有产生统计显著性影响（图5 A）。使用真实世界视频构建的小规模测试集上，InternVL3.5模型的性能趋势与仿真中一致，未表现出明显的模拟到现实的差距（表2）。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.20937v1/x3.png" alt="真实世界评估"></p>
<blockquote>
<p><strong>图4</strong>：真实世界评估中使用的三个场景的关键帧：厨房、餐桌和工作区。</p>
</blockquote>
<ul>
<li><strong>相机配置（人类中心偏差）</strong>：当相机配置偏离人类常规设定时，模型性能下降。增大视场角（Aperture 60, 80）或使用鱼眼镜头会导致性能显著降低（图5 B.1）。提高相机高度也会对前向任务产生显著的负面影响（图5 B.2）。这表明VLMs存在对人类自我中心视角和相机内参的偏好。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.20937v1/x4.png" alt="偏差分析"></p>
<blockquote>
<p><strong>图5</strong>：通过ENACT评估图像真实性和人类视觉/具身的人类中心偏差。热图显示了与基线相比的双尾非配对t检验结果。面板C.2报告了模型在左/右手谓词上的性能，其中“混淆”比例指将真实情况中的左手或右手预测为另一只手的比例。</p>
</blockquote>
<ol start="4">
<li><strong>具身偏差（如利手性）</strong>：模型在理解涉及右手的交互动力学时表现明显更好，显示出对右利手先验的偏差（图5 C.2）。</li>
<li><strong>模型间比较</strong>：专有模型中，GPT-5和Gemini 2.5 Pro整体最强。开源模型中，InternVL3.5-241B-A28B、GLM-4.5V和Qwen2.5-VL-72B表现突出，在某些设置下甚至超越了部分专有模型。专门在具身数据上训练的Cosmos-Reason1-7B，在步骤较长时（$L&gt;5$）比同规模模型表现更稳定且更好。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>ENACT基准</strong>，首次通过前向与逆向世界建模任务，以序列重排序VQA的形式系统评估VLMs的具身认知能力；2) 设计了一套<strong>可扩展的数据生成流水线</strong>，利用机器人仿真自动生成大规模、多样化的评估数据；3) 通过实验揭示了当前VLMs在具身世界建模上的<strong>关键局限性</strong>：与人类存在巨大且随交互时程扩大的性能差距、前向推理弱于逆向推理、以及对人类中心视角、相机参数和利手性存在显著偏差。</p>
<p>论文自身提到的局限性包括：目前的工作主要集中在评估而非训练/微调VLMs；尽管真实世界评估显示了与仿真一致的趋势，但规模较小，更广泛的模拟到现实迁移仍需进一步验证。</p>
<p>这项工作对后续研究有多重启示：首先，ENACT为开发更具具身智能的VLMs提供了一个明确的评估靶点和数据源。其次，揭示的模型偏差（如人类中心视角）提示未来VLM训练数据需要纳入更多样化的具身视角（如机器人摄像头）。最后，前向与逆向任务表现的差异表明，提升模型基于动作预测视觉变化的能力是迈向更完备具身认知的关键挑战。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>该论文旨在评估现代视觉语言模型是否展现出体现认知能力，即通过感知运动交互理解世界。为此，作者提出了ENACT基准，其核心方法是将评估转化为一个基于部分可观测马尔可夫决策过程的世界建模任务，具体包括“前向世界建模”和“逆向世界建模”两个序列重排任务。实验基于大规模家庭活动模拟数据，结果显示前沿视觉语言模型的性能与人类存在差距，且该差距随交互时间增长而扩大；模型在逆向任务上表现更好，并显示出对人类视觉参数和右手习惯的偏见。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.20937" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>