<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.22578" target="_blank" rel="noreferrer">2509.22578</a></span>
        <span>作者: Liang Wang Team</span>
        <span>日期: 2025-09-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习是机器人操作中的强大范式，但基于单一眼动视角训练的策略在面对视角变化时性能会显著下降。现有方法主要分为两类：一类利用点云渲染、3D重建或图像生成模型合成新视角观测，但保持原始动作不变，导致在眼动视角下出现视觉-动作不匹配；另一类使用世界模型或动作条件视频生成进行预测或规划，而非生成成对的观测-动作示范，且未明确建模机器人运动导致的眼动视角变化。本文针对生成新眼动视角示范这一具体痛点，提出关键见解：这不仅需要合成真实的新视角观测，还必须重定向原始动作以与变化的视角对齐。本文核心思路是提出EgoDemoGen框架，通过运动学动作重定向和提出的生成式视频修复模型EgoViewTransfer，生成成对的新眼动视角示范（观测视频与重定向动作），以提升策略的视角鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>EgoDemoGen的目标是从单一眼动视角收集的示范数据集 <code>{ (V, Q, D) }</code>（包含RGB观测视频V、机器人关节动作Q和深度图D）中，为定义机器人基座移动 <code>(Δx, Δy, Δθ)</code> 的新眼动视角v，生成成对的新示范 <code>(Ṽ, Q̃)</code>。框架包含两个核心模块：动作重定向和新眼动视角观测生成。</p>
<p><img src="https://arxiv.org/html/2509.22578v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>: EgoDemoGen整体框架概述。(1) 眼动视角变换：新视角由机器人基座运动定义。(2) 动作重定向：原始关节动作Q被重定向到新的机器人基座坐标系，产生运动学可行的关节动作Q̃。(3) 新眼动视角观测生成：从原始观测视频V开始，掩蔽机器人、将场景重投影到新视角、进行空洞填充，并应用EgoViewTransfer合成连贯的观测Ṽ。(4) 新示范与策略训练：获得对齐的示范对 <code>(Ṽ, Q̃)</code> 用于训练视角鲁棒策略。</p>
</blockquote>
<p><strong>1. 动作重定向</strong>：其目标是计算在新视角v下能复现原始任务的运动学可行关节动作Q̃。对于每个机械臂a、时间步t和原始动作 <code>q_t^a</code>，首先通过正运动学计算末端执行器在原始基座坐标系中的位姿 <code>T_e^a(t)</code>。然后将此位姿变换到由v定义的目标基座坐标系中得到 <code>T_e^{a→v}(t)</code>。最后，通过逆运动学求解目标坐标系中对应的关节角度 <code>q_t&#39;^a</code>。将双机械臂的关节角度序列拼接即得到重定向后的动作序列Q̃。</p>
<p><strong>2. 新眼动视角观测生成</strong>：生成与重定向动作Q̃对齐的新观测视频Ṽ包含三个阶段。首先，<strong>场景视频准备</strong>：利用分割掩码从原始帧中移除机器人，得到场景图像 <code>I_t^scene</code>。将场景-深度对重投影到新视角，产生初始的新视角场景图像 <code>Î_t^nov</code>，并通过空洞填充处理遮挡区域，得到 <code>I_t^{scene,nov}</code>，聚合形成场景视频 <code>V_S^nov</code>。其次，<strong>机器人视频渲染</strong>：使用重定向动作序列Q̃和新视角参数，渲染仅包含机器人的视频 <code>V_R^nov</code>。最后，<strong>条件视频生成修复</strong>：通过条件生成式视频修复模型EgoViewTransfer <code>G_φ</code>，以场景视频 <code>V_S^nov</code> 和机器人视频 <code>V_R^nov</code> 为条件，生成最终的新视角视频 <code>Ṽ = G_φ(V_S^nov, V_R^nov)</code>。</p>
<p><img src="https://arxiv.org/html/2509.22578v1/x3.png" alt="EgoViewTransfer模型"></p>
<blockquote>
<p><strong>图3</strong>: EgoViewTransfer。(a) 双重投影：模拟视角变化导致的伪影和遮挡，生成用于训练的对齐输入/标签对。(b) EgoViewTransfer架构：模型以退化的场景视频和机器人视频为条件，生成与双输入一致的眼动视角观测视频。</p>
</blockquote>
<p><strong>3. EgoViewTransfer模型</strong>：这是一个从预训练视频生成模型微调而来的生成式视频修复模型，采用自监督的双重投影策略进行训练，无需新视角的真实数据。<strong>数据准备</strong>：对每个源RGB-D帧，首先将其重投影到随机采样的新视角v，得到新视角帧 <code>(Î_t^nov, D̂_t^nov)</code>；然后将此新视角帧重投影回原始源视角，得到带有伪影的场景帧 <code>V̆_S^src</code>，以模拟新视角合成的挑战。<strong>训练目标</strong>：对带有伪影的场景视频 <code>V̆_S^src</code> 进行空洞填充，得到最终的条件场景视频 <code>V̄_S^src</code>。模型学习以 <code>V̄_S^src</code> 和源机器人视频 <code>V_R^src</code>（由原始动作Q渲染）为条件，重建原始源视频 <code>V^GT</code>。训练遵循标准扩散模型去噪目标 <code>L_diff</code>。<strong>推理</strong>：在新视角v下，生成流程与前述条件视频生成修复阶段一致。</p>
<p><strong>创新点</strong>：与现有方法相比，EgoDemoGen的创新性在于同时解决了动作重定向和视觉合成问题，通过EgoViewTransfer模型将重投影的场景视频与基于重定向动作渲染的机器人视频融合，确保了新视角下视觉与动作的精确对齐。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>环境与任务</strong>：在RoboTwin2.0仿真环境（任务：Lift Pot, Handover Mic, Shake Bottle）和真实世界Mobile ALOHA平台（任务：Pick Up Bowl, Place Bowl Basket, Place Bowl Plate）上进行评估。</li>
<li><strong>Baseline方法</strong>：<ul>
<li>Standard View (50/100)：使用50或100个标准眼动视角示范训练。</li>
<li>EgoDemoGen (w/o EgoViewTransfer)：使用50个标准示范+50个朴素组合（直接合并场景视频和机器人视频）的示范训练。</li>
</ul>
</li>
<li><strong>策略模型</strong>：仿真使用ACT，真实机器人使用π₀。</li>
<li><strong>评估指标</strong>：任务成功率（主要指标），视频质量（PSNR, SSIM, LPIPS, FVD）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br><strong>仿真结果</strong>（表1）：EgoDemoGen在所有任务和视角上取得最佳平均结果。新视角平均成功率从14.7%提升至30.0%（+15.3%绝对值），标准视角从78.0%提升至80.7%（+2.7%绝对值）。与“w/o EgoViewTransfer”相比，新视角提升14.7%绝对值，标准视角提升5.0%绝对值。<br><img src="https://arxiv.org/html/2509.22578v1/x1.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>表1</strong>: 仿真任务成功率。EgoDemoGen在新视角和标准视角均带来显著提升。</p>
</blockquote>
<p><strong>真实世界结果</strong>（表2）：EgoDemoGen同样获得最佳平均成功率。新视角（CCW/CW平均）从36.7%提升至62.5%（+25.8%绝对值），标准视角从60.0%提升至78.3%（+18.3%绝对值）。<br><img src="https://arxiv.org/html/2509.22578v1/x4.png" alt="真实世界结果表"></p>
<blockquote>
<p><strong>表2</strong>: 真实世界任务成功率。EgoDemoGen在标准视角和两个旋转新视角下均表现最优。</p>
</blockquote>
<p><strong>数据混合比例分析</strong>（图5）：随着训练数据中EgoDemoGen生成示范比例的增加（混合比从1:0到1:3），标准视角和新视角的成功率均持续提升，且对新视角的提升更大。性能在1:1比例处出现明显跃升，之后收益递减。<br><img src="https://arxiv.org/html/2509.22578v1/x5.png" alt="数据混合比例曲线"></p>
<blockquote>
<p><strong>图5</strong>: 不同数据混合比例下的成功率。增加EgoDemoGen生成示范的比例能持续提升性能，尤其是在新视角下。</p>
</blockquote>
<p><strong>动作重定向验证</strong>（表3）：在新视角下回放动作，重定向动作的平均成功率（78.3%）远高于原始动作（8.3%），证明了运动学重定向对于生成可行轨迹的必要性。<br><img src="https://arxiv.org/html/2509.22578v1/x6.png" alt="动作重定向验证表"></p>
<blockquote>
<p><strong>表3</strong>: 动作重定向有效性验证。重定向动作在新视角下的回放成功率大幅领先。</p>
</blockquote>
<p><strong>视频生成质量分析</strong>：</p>
<ul>
<li><strong>定量分析</strong>（表4，表5）：在仿真和真实世界评估中，EgoViewTransfer生成的视频在衡量分布对齐的指标（LPIPS, FVD）上显著优于朴素组合方法，表明其具有更好的视觉保真度和时间一致性。</li>
<li><strong>定性分析</strong>（图6）：可视化显示，EgoViewTransfer能消除重投影模糊和伪影，并使渲染的机器人风格与场景匹配，生成更清晰、与动作对齐的观测视频。<br><img src="https://arxiv.org/html/2509.22578v1/x7.png" alt="视频生成质量对比"><blockquote>
<p><strong>图6</strong>: 生成视频的定性对比。EgoViewTransfer（中）相比朴素组合（右）能生成更清晰、与场景更融合的观测。</p>
</blockquote>
</li>
</ul>
<p><strong>消融实验总结</strong>：主要组件贡献体现在：1) <strong>动作重定向</strong>是基础，使动作在新视角下可行（表3）；2) <strong>EgoViewTransfer</strong> 是关键，其生成的高质量对齐视频带来了主要的性能增益（表1, 2中与“w/o EgoViewTransfer”的对比）。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了EgoDemoGen框架，通过同时重定向动作和合成观测，生成成对的新眼动视角示范，以提升模仿学习策略的视角鲁棒性。</li>
<li>引入了EgoViewTransfer，一种基于双重投影自监督策略微调的生成式视频修复模型，能够融合重投影场景视频和渲染机器人视频，合成真实、连贯的新眼动视角观测。</li>
<li>在仿真和真实机器人上的实验表明，将EgoDemoGen生成的示范加入训练数据，能显著提升策略在新视角和标准视角下的成功率，且性能随生成数据比例增加而提升（收益递减）。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前方法假设双腕相机相对于机器人基座的视角是固定的（即只改变基座姿态）。此外，对大型视频生成模型进行微调需要较高的计算成本。</p>
<p><strong>后续研究启示</strong>：EgoDemoGen为增强视觉运动策略的视角鲁棒性提供了一条实用路径。其“动作重定向+条件视频生成”的范式可扩展到更复杂的视角变化（如相机高度、倾斜角度）。同时，如何进一步降低数据生成的计算成本，以及探索更高效的模型架构或蒸馏方法，是值得未来研究的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出EgoDemoGen框架，旨在解决模仿学习策略因自我中心视角变化而性能下降的问题。其核心方法是：通过动作重定向与提出的生成式视频修复模型EgoViewTransfer（基于自监督双重重投影策略微调），合成配对的新视角演示数据。实验表明，混合使用生成数据与原始数据训练后，策略成功率显著提升：在仿真中，标准视角提升17.0%，新视角提升17.7%；在真实机器人上，分别提升18.3%和25.8%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.22578" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>