<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.14358" target="_blank" rel="noreferrer">2508.14358</a></span>
        <span>作者: Ioannis Stamos Team</span>
        <span>日期: 2025-08-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>类别级物体姿态估计旨在预测给定类别内任意物体的6D姿态和3D尺寸，无需依赖精确的CAD模型，相比实例级方法具有更广的泛化能力。早期方法通过预测归一化物体坐标空间（NOCS）映射并建立逐像素对应关系来估计姿态，但这些方法对2D-3D对应关系误差或噪声深度输入高度敏感。近期方法则绕过对应关系构建，直接回归姿态，简化了流程并提高了鲁棒性。其中，基于RGB-D或纯深度（Depth-only）的方法均有发展。在光照多变或物体缺乏显著颜色纹理的工业等真实场景中，深度信息通常更鲁棒，因此纯深度方法受到关注。</p>
<p>然而，现有方法仅将姿态作为训练目标，导致学到的点云特征碎片化，未能显式捕捉6D姿态固有的连续性，这造成了预测的不一致性和对未见姿态泛化能力的下降。虽然已有工作尝试使用三元组损失从RGB-D图像学习旋转感知表示，但它们仅关注旋转而忽略了平移，且三元组损失在表示学习任务中通常表现不如对比损失。最近，Rank-N-Contrast (RNC)方法提出了基于排序的对比损失来学习回归任务的表示，但它仅限于处理单一任务类型且未纳入类别信息，无法直接应用于涉及多任务（旋转和平移）和多类别数据的类别级姿态估计。</p>
<p>本文针对“现有方法未能显式学习6D姿态连续性”这一痛点，提出了HRC-Pose，一个新颖的纯深度框架。其核心思路是：通过一种新颖的分层排序对比学习策略，解耦并分别学习能够保持旋转和平移连续性的点云表示，进而提升类别级6D物体姿态估计的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>HRC-Pose的整体框架将姿态解耦为旋转和平移两部分，分别进行编码、对比学习和姿态估计。</p>
<p><img src="https://arxiv.org/html/2508.14358v1/figures/Figure2_new.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：HRC-Pose整体框架。给定点云P和类别标签cls，旋转编码器Enc^R和平移编码器Enc^t分别提取点级特征f_pc^R和f_pc^t。通过最大池化获得全局特征f^R和f^t，并输入分层排序对比学习模块，利用损失L_CL^R和L_CL^t学习保持姿态连续性的表示。最后，f_pc^R和f_pc^t被分别送入姿态回归、对称感知点云重建和边界框投票三个子模块进行最终的姿态估计。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>特征提取器</strong>：使用两个独立的点云编码器Enc^R和Enc^t，分别处理输入点云P，输出点级嵌入特征f_pc^R和f_pc^t（维度为N_p×512）。通过池化层获得对应的全局特征f^R和f^t，用于后续的对比学习。编码器主干网络采用3D-GCN with HS layers。</li>
<li><strong>分层排序对比学习</strong>：这是方法的核心创新模块，旨在学习保持旋转和平移连续性的特征。其关键在于<strong>6D姿态感知的分层排序方案</strong>。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.14358v1/figures/Figure1_new.png" alt="对比学习策略"></p>
<blockquote>
<p><strong>图1</strong>：6D姿态感知分层排序示意图。对于一个锚点点云i和一个随机选择的正样本j，根据旋转差ΔR和位移差Δt定义负样本。红色框：联合负样本对，要求k同时满足ΔR_ij &lt; ΔR_ik且Δt_ij &lt; Δt_ik（即k在旋转和平移上都比j离i更远）。蓝色/橙色框：旋转或平移特定的负样本对，分别仅根据ΔR或Δt的排序定义（例如，对于旋转，只要ΔR_ij &lt; ΔR_ik，k即为负样本）。</p>
</blockquote>
<pre><code>*   **距离度量**：旋转距离ΔR定义为两个平面法向量余弦相似度补数的和；平移距离Δt定义为均方误差（MSE）。
*   **负样本集构建**：
    *   **联合负样本集 S_i,j^(joint,g)**：基于最严格的标准，要求负样本k在旋转和平移距离上都比正样本j离锚点i更远。这些是“强负样本”。
    *   **任务特定负样本集 S_i,j^g**：分别仅根据旋转（g=R）或平移（g=t）距离排序构建，标准更宽松，能利用批次中更多数据。
*   **对比损失函数**：分别为联合负样本集和任务特定负样本集计算对比损失l^(joint,g)和l^g（公式1-4），本质是让特征相似度顺序与姿态距离顺序一致。最终，每个类别c内的损失被平均，得到多类别场景下的总对比损失L_CL^R和L_CL^t（公式5-6），其中λ是平衡权重（设为0.8）。温度τ设为2。
</code></pre>
<ol start="3">
<li><strong>姿态估计模块</strong>：利用已学习到连续性信息的点级特征f_pc^R和f_pc^t进行姿态估计。与之前工作使用共享特征不同，本文主张旋转和平移是姿态的不同方面，应分别处理。<ul>
<li><strong>旋转回归</strong>：使用f_pc^R预测旋转R。</li>
<li><strong>平移回归</strong>：使用f_pc^t预测平移t。</li>
<li><strong>对称感知重建与边界框投票</strong>：将f_pc^R和f_pc^t并行输入这两个子模块，并对它们的输出取平均，以获得最终的重建结果和边界框投票结果。</li>
</ul>
</li>
<li><strong>总体损失</strong>：整体训练损失是对比学习损失与各姿态估计子模块损失（沿用自GPV-Pose等工作）的加权和（公式7）。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，HRC-Pose的主要创新在于提出了<strong>分层排序对比学习策略</strong>。它扩展了Rank-N-Contrast (RNC)，通过联合负样本集严格保持6D姿态双任务的连续性，同时通过任务特定负样本集提高数据利用效率，并能自然地处理多类别数据，从而解决了RNC在应用于类别级姿态估计时的局限性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集与基准</strong>：使用广泛采用的NOCS数据集，包含6个类别。在其实验子集上进行评估：合成测试集CAMERA25（25K图像）和更具挑战性的真实世界测试集REAL275（2.75K图像）。</li>
<li><strong>评估指标</strong>：采用平均精度（mAP）评估姿态估计，指标为n° m cm（旋转误差小于n度且平移误差小于m厘米）。报告了5°2cm, 5°5cm, 10°2cm, 10°5cm的mAP。同时报告3D IoU在50%和75%阈值下的平均精度以评估尺寸预测。</li>
<li><strong>对比方法</strong>：与多种state-of-the-art方法对比，包括RGB-D方法（如SPD, DualPoseNet, SGPA, IST-Net, CLIPose, AG-Pose）和深度方法（如FS-Net, GPV-Pose, SAR-Net）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在REAL275测试集上，HRC-Pose在纯深度方法中取得了最佳性能。例如，在5°5cm指标上达到46.0% mAP，优于GPV-Pose (42.9%) 和SAR-Net (42.3%)；在10°5cm指标上达到78.4% mAP，优于GPV-Pose (73.3%)。在CAMERA25合成数据集上也呈现一致优势。论文强调HRC-Pose在保持高精度的同时能够实时运行（FPS计算批次大小为1，排除预处理时间）。</p>
<p><img src="https://arxiv.org/html/2508.14358v1/figures/Figure3_new.png" alt="表示可视化"></p>
<blockquote>
<p><strong>图3</strong>：在REAL275数据集上，笔记本电脑（laptop）类别关于旋转和平移的表示UMAP可视化。对比了基线方法HS-Pose的表示和HRC-Pose对比学习模块学到的表示。颜色对应旋转或平移值。HRC-Pose学到的特征空间展现出更平滑、有序的颜色过渡，表明其成功捕捉了姿态的连续性，而HS-Pose的表示则显得更杂乱。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14358v1/figures/corr_new.png" alt="相关性分析"></p>
<blockquote>
<p><strong>图4</strong>：在REAL275数据集上，姿态差异与特征表示距离之间的相关性图示。左图（旋转）：HRC-Pose（蓝点）的特征距离与旋转差异显示出比HS-Pose（红点）更强的正相关趋势（蓝线斜率更大）。右图（平移）同样显示了HRC-Pose更好的相关性。这定量证明了HRC-Pose学到的表示更好地保持了姿态连续性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14358v1/figures/Figure5.png" alt="编码器特征可视化"></p>
<blockquote>
<p><strong>图5</strong>：HRC-Pose中两个编码器Enc^R和Enc^t在REAL275数据集上学到的点云全局表示的UMAP可视化。(a) Enc^R的表示按旋转值着色；(b) Enc^t的表示按平移值着色。两个特征空间均显示出清晰、连续的颜色梯度，表明两个编码器成功解耦并分别学习了旋转和平移的连续性。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文通过消融实验验证了分层排序策略中不同负样本集的有效性。结果表明：1）<strong>联合负样本集</strong>（强负样本）对于学习严格的6D姿态连续性至关重要；2）<strong>任务特定负样本集</strong>（旋转或平移特定）通过引入更多负样本提高了数据利用效率，进一步提升了性能；3）结合两者（即完整的分层排序策略）能取得最佳结果，平衡了连续性约束和数据效率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>分层排序对比学习（Hierarchical Ranking Contrastive Learning）</strong>，基于6D姿态感知的分层排序方案，能够从多类别点云数据中提取同时保持旋转和平移连续性的特征。</li>
<li>开发了<strong>HRC-Pose框架</strong>，一个纯深度的类别级物体姿态估计框架，将姿态解耦为旋转和平移，并通过独立的编码器、对比学习模块和姿态估计模块分别进行编码、学习和利用。</li>
<li>通过大量实验证明，HRC-Pose在REAL275和CAMERA25基准上超越了现有的纯深度state-of-the-art方法，并能实时运行，展示了其有效性和实际应用潜力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，当前方法仅使用深度数据，未利用可能提供补充信息的RGB模态。在纹理丰富的场景中，结合RGB信息可能带来进一步性能提升。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>连续性学习的重要性</strong>：显式学习任务目标（如6D姿态）的连续性，而不仅仅是将其作为回归目标，可以显著提升表示质量和模型的泛化能力。</li>
<li><strong>解耦与特定表示</strong>：将复杂任务（如6D姿态估计）解耦为子任务（旋转、平移），并为其学习特定的、保持子任务性质的表示，是一种有效的设计思路。</li>
<li><strong>对比学习的扩展性</strong>：本文的分层排序策略为将对比学习应用于多任务、多目标的回归问题提供了可扩展的范例，可启发其他需要学习多个连续输出变量间结构的任务。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对类别级6D物体姿态估计中，现有方法因未显式捕捉姿态连续性而导致预测不一致、泛化能力弱的问题，提出HRC-Pose深度框架。该方法通过对比学习学习点云表示，核心采用6D姿态感知分层排序策略，分别编码旋转和平移组件，并设计专用估计模块处理。实验显示，HRC-Pose在REAL275和CAMERA25基准上持续优于现有深度方法，且能实时运行，验证了其有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.14358" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>