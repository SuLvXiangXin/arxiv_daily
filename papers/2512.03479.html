<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Object-centric Understanding for Instructional Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Towards Object-centric Understanding for Instructional Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.03479" target="_blank" rel="noreferrer">2512.03479</a></span>
        <span>作者: Guo, Wenliang, Kong, Yu</span>
        <span>日期: 2025/12/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>理解程序性活动对于开发能够推理复杂现实世界任务的未来辅助AI至关重要。现有方法主要从动作中心（action-centric）的视角出发，专注于建模动作转移、构建动作图或隐式学习时序结构。这些方法在动作分割和下一步预测等任务上有效，但在面对真实世界复杂多变的程序流程时，其泛化能力不足。程序性任务通常包含严格的前提条件和因果依赖，同时允许步骤可选、可互换或顺序可变，这种灵活性暴露了动作中心方法的局限性：它们难以稳健地适应动态多样的实际情况。</p>
<p>本文提出从不同视角解决此问题：程序活动的推进从根本上由物体动态（即物体状态的变化）驱动，而动作主要是引发这些状态转换的机制。因此，任何动作的执行都必须以相关物体的当前状态为条件。这一视角将程序性理解从动作中心范式转向物体中心（object-centric）范式。为推进这一方向，本文面临两大挑战：一是现有数据集未能捕捉程序过程中物体状态的演变；二是物体交互在时间上具有稀疏性和不连续性，需要跨非连续片段进行多跳推理。</p>
<p>本文的核心思路是：1）构建一个专注于物体中心推理的长篇教学视频问答基准（Object-IVQA）；2）提出一个代理框架，通过编排物体中心的规划、感知、分析和生成工具，实现跨非连续片段的显式证据检索和多跳推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个用于物体中心视频问答的代理框架，旨在解决教学视频中物体交互在时间上不连续所带来的多跳推理挑战。</p>
<p><img src="https://arxiv.org/html/2512.03479v1/x5.png" alt="方法框架"></p>
<blockquote>
<p><strong>图5</strong>：(a) 提出的代理框架概览。框架由规划代理、视频处理代理、视频分析代理和文本生成代理组成，它们各自配备可调用工具。(b) 多智能体协作的示例，展示了生成的工具使用计划、中间结果和最终答案。</p>
</blockquote>
<p>整体框架（Pipeline）如图5(a)所示。输入是一个长视频和一个自然语言问题。框架包含四个代理：规划代理（Planning Agent）、视频处理代理（Video-Processing Agent）、视频分析代理（Video-Analyzing Agent）和文本生成代理（Text-Generation Agent）。输出是开放式的答案及其支持性的时间证据（时间跨度）。</p>
<p><strong>核心模块详解：</strong></p>
<ol>
<li><strong>规划代理（Planning Agent）</strong>：基于大型语言模型（LLM-as-planner）。它首先解析问题，查询所有视频代理的工具箱，然后构建一个有序的工具调用序列计划，以提取感知证据并分析物体状态。该计划编码了物体中心约束、时间逻辑以及感知与推理操作的顺序。</li>
<li><strong>视频处理代理（Video-Processing Agent）</strong>：负责将非结构化视频流转换为时序组织单元。它包含三个工具：<ul>
<li><code>Video_Load</code>：读取视频文件，记录帧率、时长和分辨率。</li>
<li><code>Frame_Sample</code>：执行帧采样，返回带时间戳的帧集合。</li>
<li><code>Frame_Trim</code>：根据时间关系（如之前或之后）过滤检索到的帧，支持涉及因果或顺序关系的查询。</li>
</ul>
</li>
<li><strong>视频分析代理（Video-Analyzing Agent）</strong>：负责提取细粒度语义线索，识别动作上下文并跟踪物体随时间的演变。它包含三个工具：<ul>
<li><code>Frame_Retrieve</code>：基于EVAL-02 CLIP，检索包含规划代理预测的相关物体的帧，用于定位答案所在的近似时间窗口。</li>
<li><code>Obj_Det</code>：应用Grounding DINO进行基于文本的开放词汇指代检测，返回物体的边界框和置信度分数，这对于跟踪物体动态和状态转换至关重要。</li>
<li><code>Action_Rec</code>：使用多模态大模型Qwen3-VL生成与物体相关动作的时间对齐描述，使代理能够确定物体何时以及以何种方式被操纵。</li>
</ul>
</li>
<li><strong>文本生成代理（Text-Generation Agent）</strong>：负责将视觉信息转化为描述性语言，并生成最终答案。它包含三个工具：<ul>
<li><code>Img_Caption</code>：使用基于BLIP的生成器，生成细粒度的、以物体为中心的图像描述。</li>
<li><code>Context_Sum</code>：使用Llama-3.1，融合动作描述、图像描述和检测结果，总结出关于物体如何随时间演变的可解释文本上下文。</li>
<li><code>Answer_Gen</code>：基于Qwen3-VL，结合上下文摘要、采样帧和问题，进行多模态推理，最终生成答案及支持性时间证据。</li>
</ul>
</li>
</ol>
<p><strong>多智能体协作流程</strong>如图5(b)示例所示：规划代理生成工具使用计划；视频处理和分析代理协作，将长视频缩减为一组时间和语义对齐的片段（例如，定位到包含“黄油”的帧，并检索“融化”事件之前的帧）；文本生成代理整合前序步骤的多模态输出，生成最终答案。这种模块化、显式的协作方式，使系统能够跨非连续视频片段跟踪物体状态，并将答案基于精确的时间证据。</p>
<p><strong>与现有方法的创新点</strong>在于：1) <strong>范式转变</strong>：从评估动作转移到评估物体状态演变，提出了新的问题类型（如反事实推理、错误识别）；2) <strong>模块化推理框架</strong>：不是依赖单一端到端模型，而是通过可解释、可组合的代理和工具链，显式地处理时间不连续性和进行多跳推理。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>基准/数据集</strong>：实验在自行构建的Object-IVQA基准上进行。该基准包含107个长教学视频（平均时长535秒，超过75%的视频超过6分钟）和514个开放式问答对。视频源自CaptainCook4D、COIN和EgoPER数据集，包含第一人称和第三人称视角。问答涵盖四种类型：准备推理（36.8%）、状态演变（14.6%）、反事实推理（16.9%）和错误识别（31.7%）。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li><strong>盲测大视觉语言模型（Blind LVLMs）</strong>：不提供视频，仅根据问题回答（InternVL3.5, Qwen3-VL），用于评估模型仅凭常识答题的能力。</li>
<li><strong>开源大视觉语言模型（Open-source LVLMs）</strong>：提供视频帧，包括InternVL3.5 (8B, 38B)、Qwen3-VL (8B, 30B)、Kimi-VL (16B)。</li>
<li><strong>闭源大视觉语言模型（Close-source LVLMs）</strong>：包括Claude Opus 4、Gemini 2.5 Pro、GPT-5。</li>
<li><strong>本文提出的代理框架</strong>：使用Qwen3-VL或GPT-5作为规划代理的核心。</li>
</ul>
<p><strong>评估指标</strong>：</p>
<ol>
<li><strong>答案质量（Score）</strong>：采用LLM-as-judge（GPT-5-mini）从四个维度（上下文整合CI、细节导向DO、上下文理解CU、时间理解TU）对生成答案与真实答案进行语义比较，每个维度0-5分，最终得分为四维平均分。</li>
<li><strong>证据定位（mIoU%）</strong>：计算模型预测的支持证据时间跨度与真实时间跨度的平均交并比（mIoU）。</li>
</ol>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2512.03479v1/x6.png" alt="主要结果表1"></p>
<blockquote>
<p><strong>表2</strong>：开源模型在Object-IVQA完整数据集上的性能对比。盲测模型（无视频）得分显著低于提供视频的模型，说明任务需要视觉依据。在提供视频的模型中，Qwen3-VL 30B和InternVL3.5 38B表现较好，但在错误识别（Mistake）类型上所有模型得分均较低（&lt;1.1），凸显了此类推理的难度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.03479v1/x7.png" alt="主要结果表2"></p>
<blockquote>
<p><strong>表3</strong>：在Object-IVQA子集上，开源、闭源模型及本文代理框架的性能对比。闭源模型（GPT-5, Gemini 2.5 Pro）整体优于开源模型。<strong>本文提出的代理框架（GPT5-as-Planner）在平均得分（2.81）和平均mIoU（13.31%）上均取得了最佳性能</strong>，超越了所有基准的LVLMs，包括最强的闭源模型GPT-5（得分2.80， mIoU 12.47%）。这证明了模块化、工具调用框架在复杂物体中心推理任务上的有效性。</p>
</blockquote>
<p><strong>消融实验与深入分析</strong>：<br>论文通过分析不同问题类型上的表现，揭示了现有模型的局限性。如表2和表3所示，所有模型在“错误识别（Mistake）”类型上的表现都远差于其他类型（得分普遍低于1.3），这表明识别程序错误并基于物体状态进行解释对当前LVLMs极具挑战性。同时，证据定位的mIoU普遍较低（最佳模型约13-17%），说明即使答案正确，模型也难以精确地定位支持证据的时间跨度。本文的框架通过显式的物体检测、跟踪和分段检索，在证据定位（mIoU）上展现了优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文的核心贡献</strong>：</p>
<ol>
<li><strong>提出并形式化了物体中心的程序性理解范式</strong>，将其定义为一个需要识别物体状态并推理其动态的视频问答任务。</li>
<li><strong>构建了Object-IVQA基准</strong>，这是一个包含长视频、开放式问答、时间证据标注和四种推理类型的综合性评测数据集，系统评估并揭示了当前大视觉语言模型在物体级理解和时空推理方面的不足。</li>
<li><strong>提出了一个创新的多代理框架</strong>，通过LLM规划动态调用专用的物体中心工具链，实现了对非连续视频片段的显式证据检索和多跳推理，在性能和证据定位准确性上均优于现有的端到端大模型。</li>
</ol>
<p><strong>论文提到的局限性</strong>：</p>
<ol>
<li><strong>数据集规模</strong>：Object-IVQA目前包含107个视频和514个问答对，规模相对较小，未来需要扩展以覆盖更广泛的活动和更复杂的推理模式。</li>
<li><strong>框架效率</strong>：代理框架涉及多个模型的顺序调用，导致计算开销和延迟较高，可能不适合实时应用。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>物体中心范式具有潜力</strong>：将研究焦点从动作序列转移到物体状态演变，可能为程序性活动理解提供更鲁棒、更灵活的建模方式，尤其适用于步骤顺序可变的任务。</li>
<li><strong>模块化与组合性</strong>：对于需要复杂、长程、多跳推理的任务，本文展示了模块化、可组合的代理框架相比单一巨型端到端模型可能更具优势，特别是在可解释性和精确证据定位方面。</li>
<li><strong>错误识别是难点</strong>：基准测试表明，识别和解释程序错误是当前模型的薄弱环节，这为未来研究指明了重要的突破方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对教学视频理解，提出从动作中心转向对象中心的新范式，将动作视为驱动对象状态转换的机制。为此，作者构建了Object-IVQA基准数据集（含107个视频和514个开放问答），并提出了一个整合对象中心规划、感知、分析与生成工具的智能体框架，支持显式证据检索和多跳推理。实验表明，现有大型视觉语言模型在对象级识别与推理上存在困难，而所提框架取得了显著性能提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.03479" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>