<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.15190" target="_blank" rel="noreferrer">2506.15190</a></span>
        <span>作者: Anqi Wu Team</span>
        <span>日期: 2025-06-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>动物行为研究的一个关键方向是从大规模行为记录中识别重复出现的模式，通常被称为刻板行为音节，如梳理背部、奔跑和嗅探。现有的行为分割方法可分为三类：(1) 监督分类，(2) 基于聚类的方法，(3) 基于切换动态的方法。这些方法旨在通过将连续的姿态轨迹划分为离散的音节来揭示行为中的结构化模式。</p>
<p>然而，现有方法存在四个主要局限。首先，它们依赖于行为由离散动作音节组成的假设，这过度简化了运动固有的连续性，并在动作转换期间引入了模糊性。其次，它们通常将动作提取为抽象音节，无法捕捉个体身体部位如何对不同运动做出贡献。第三，大多数分割模型要么是非生成式的（如聚类），要么依赖于限制性的生成假设（如线性动态和马尔可夫模型），常导致生成不现实的行为。第四，也是最重要的一点，基于分类/聚类的方法忽略了时间依赖性，无法捕捉反映真实姿态动态的基元；而基于动态的方法强加了可能与实际动物行为不匹配的线性或非线性动态假设。</p>
<p>为应对这些局限，本文引入了一个新视角：在强化学习框架下建模行为。通过基于RL的模仿学习框架推断动物的策略，来研究行为动态和运动基元。这不仅通过RL实现了更真实的行为生成，还允许我们发现可重用的运动基元集来构建由内部奖励驱动的策略。通过这一视角，我们获得了对运动基元更灵活、可生成且可解释的理解，超越了离散分割的限制。本文假设动物从一个固定的核心运动基元集中提取元素，在长行为轨迹上构建多样化的运动。基于此，本文提出了基于基元的连续动态发现（MCD）框架，以解析长轨迹并揭示反映行为动态的基元与策略。核心思路是：通过基于谱分解的表示学习来学习可解释的潜在基元集，并将行为动态建模为这些基元的连续演化混合。</p>
<h2 id="方法详解">方法详解</h2>
<p>MCD框架将行为轨迹建模在马尔可夫决策过程（MDP）中。其核心是定义<strong>基元集</strong>：给定MDP中任意转移核 (P(s&#39;|s, a))，可通过谱分解表示为 (P(s&#39;|s, a) = \phi(s, a)^\top \mu(s&#39;) q(s&#39;))，其中函数 (\phi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^d) 被定义为基元集。奖励函数则被参数化为 (r(s, a) = \phi(s, a)^\top w)。这种定义使得基元学习独立于模型假设，并忠实反映行为数据中存在的基元。</p>
<p><img src="https://arxiv.org/html/2506.15190v2/x1.png" alt="策略作为基元混合"></p>
<blockquote>
<p><strong>图1</strong>：一个运动的策略可被视为来自包含基本运动基元的“字典”中“词汇”的混合。每个基元的贡献随时间连续演化，多个基元可以同时活跃以构建持续的运动。</p>
</blockquote>
<p>将上述定义代入贝尔曼方程，可推导出动作值函数 (Q(s, a) = \phi(s, a)^\top u)，其中 (u) 是与价值和基元相关的向量。在最大熵强化学习框架下，最优策略具有形式：(\pi(a|s) = \frac{\exp(\phi(s, a)^\top u)}{\sum_{a&#39;} \exp(\phi(s, a&#39;)^\top u)})。该策略不基于任何模型假设，而是自然作为基于转移核谱分解的最大熵策略出现。基元 (\phi) 为环境转移、策略和奖励提供了线性基础，且由于 (\phi) 仅从转移动态 (P(s&#39;|s, a)) 导出，它独立于任何特定奖励或任务，编码了动物可用的内在通用运动基元，而权重向量 (u) 则捕捉了产生符合不同目标行为所需的任务特定调制。</p>
<p>MCD的学习流程因行为数据性质（离散或连续）而异：</p>
<ol>
<li><strong>离散版本</strong>：对离散状态-动作空间，应用奇异值分解（SVD）等谱方法来学习基元表示 (\phi(s, a)) 和 (\mu(s&#39;))。然后通过最大似然估计（MLE）优化目标（公式5）来学习参数 (u)，从而得到基于基元的策略。</li>
<li><strong>连续版本</strong>：为应对连续空间的挑战（分解形式限制性强、分区函数难以计算），采用了替代方法。<ul>
<li><strong>基元发现</strong>：将转移核 (P(s&#39;|s, a)) 建模为基于能量的模型（EBM）：(P(s&#39;|s, a) = q(s&#39;) \exp(\psi(s, a)^\top \nu(s&#39;) - \log Z(s, a)))，其中 (\psi) 和 (\nu) 是神经网络特征映射。通过噪声对比估计（NCE）来优化 (\psi) 和 (\nu)，从而避开分区函数 (Z(s, a)) 的棘手计算（公式7）。该EBM公式可近似关联到原始的基元定义。</li>
<li><strong>基于基元的策略学习</strong>：由于从最优 (\psi) 精确获得 (\phi) 仍很困难，本文引入一个由神经网络参数化的映射 (f: \psi \rightarrow \phi)，并通过再次应用NCE来学习 (f) 和 (u)（公式8），最终得到策略 (\pi(a|s, t) \propto \exp(f(\psi(s, a))^\top u(t)))。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，MCD是首个用于行为分割的基于RL的模仿学习框架；其学习的基元和策略不依赖于动态假设或任何模型假设；它通过允许基元权重 (u(t)) 随任务或时间连续变化，并支持多个基元同时活跃，揭示了动物行为的连续性和组合性本质，超越了传统的离散切换模型。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在三个设置上验证MCD：1）多任务网格世界（模拟），2）迷宫导航任务（离散状态-动作），3）自由移动动物行为（连续姿态数据）。使用的数据集/实验平台包括自定义的3x3网格世界模拟、小鼠迷宫导航数据以及自由行为小鼠的姿势关键点轨迹。</p>
<p>对比的基线方法包括：VAME（基于自编码器的分割）、Keypoint-MoSeq（基于切换线性动态系统）和B-SOiD（无监督聚类）。评估指标涵盖重建精度（负对数似然，NLL）、生成质量（Frechet距离，FD）和行为分割的一致性（调整互信息，AMI）等。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><p><strong>模拟网格世界（图2）</strong>：MCD学习了64个基元，并利用它们重建了9个任务特定的策略和奖励函数。恢复的奖励函数与真实奖励的皮尔逊相关系数达到0.96，表明学习到的基元足以准确重建内部动机。<br><img src="https://arxiv.org/html/2506.15190v2/x2.png" alt="网格世界实验"></p>
<blockquote>
<p><strong>图2</strong>：（A）3x3网格世界设置。（C）左：9个任务的真实奖励函数热图；右：MCD恢复的奖励函数热图，两者高度相似。</p>
</blockquote>
</li>
<li><p><strong>迷宫导航（图3）</strong>：在已知动物在5种奖励条件间切换的迷宫数据上，MCD学习了共享基元，并为每个条件拟合了不同的权重向量 (u(t))。结果显示，学习到的策略权重 (u(t)) 成功区分了不同的任务阶段，并且恢复的奖励 (w(t)) 与实验已知的奖励条件高度一致。<br><img src="https://arxiv.org/html/2506.15190v2/x3.png" alt="迷宫导航实验"></p>
<blockquote>
<p><strong>图3</strong>：（A）迷宫轨迹示例。（B）学习到的5个任务（奖励条件）下的策略权重 (u(t))，显示了任务间的差异。（C）恢复的奖励权重 (w(t)) 与已知任务条件对齐。</p>
</blockquote>
</li>
<li><p><strong>自由移动动物行为（图4-7）</strong>：</p>
<ul>
<li><p><strong>基元可视化</strong>：学习到的基元 (\phi) 对应有意义的身体运动模式，例如“左前肢伸展”、“头部向下”等（图4）。<br><img src="https://arxiv.org/html/2506.15190v2/x4.png" alt="学习到的运动基元"></p>
<blockquote>
<p><strong>图4</strong>：从自由行为小鼠数据中学习到的四个示例运动基元可视化，每个基元编码了特定的身体部位运动模式。</p>
</blockquote>
</li>
<li><p><strong>连续动态捕捉</strong>：基元权重 (u(t)) 随时间平滑演化，显示了行为组成的连续性（图5）。例如，在“梳理背部”行为中，“梳理”和“转向背部”基元的权重同时升高并随时间变化。<br><img src="https://arxiv.org/html/2506.15190v2/x5.png" alt="基元权重的连续演化"></p>
<blockquote>
<p><strong>图5</strong>：在示例行为片段（梳理背部、转向、行走）期间，四个基元的权重 (u(t)) 随时间连续演化的曲线。注意多个基元可以同时活跃。</p>
</blockquote>
</li>
<li><p><strong>定量评估</strong>：在行为重建（NLL）、生成（FD）和分割（AMI）任务上，MCD均优于或与现有最佳方法（Keypoint-MoSeq）相当，特别是在生成逼真轨迹方面表现突出（图6，FD值更低）。<br><img src="https://arxiv.org/html/2506.15190v2/x6.png" alt="定量结果对比"></p>
<blockquote>
<p><strong>图6</strong>：在自由行为数据上的定量评估。MCD在重建（NLL）和生成（FD）任务上表现最佳，在分割（AMI）任务上与Keypoint-MoSeq相当。</p>
</blockquote>
</li>
<li><p><strong>生成样例</strong>：MCD生成的行为轨迹在视觉上比基线方法（如VAME）更逼真、更连贯（图7）。<br><img src="https://arxiv.org/html/2506.15190v2/x7.png" alt="生成轨迹定性对比"></p>
<blockquote>
<p><strong>图7</strong>：MCD与VAME生成的行为轨迹定性对比。MCD生成的轨迹（右列）更平滑、更逼真，而VAME生成的轨迹（中列）则显得杂乱和不自然。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p><strong>消融实验</strong>：本文进行了消融研究，验证了连续策略建模（与离散切换对比）和谱分解表示学习的关键作用。移除这些组件会导致性能下降，证实了它们对MCD框架有效性的贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）引入了首个基于强化学习模仿学习框架的行为分割方法，为理解行为提供了一个由策略和奖励驱动的决策过程新视角；2）提出了通过谱分解进行RL表示学习来发现基于基元的策略，该方法不依赖于动态或模型假设，能忠实表征行为动态；3）揭示了动物行为的连续本质，并提供了对复杂行为如何从基本运动基元的动态组合中产生的细致理解。</p>
<p>论文提到的局限性包括：方法依赖于高质量的专家演示数据进行模仿学习；在连续域中，虽然使用了NCE等技巧，但学习过程仍具有一定计算复杂度。</p>
<p>对后续研究的启示：MCD框架将控制理论、表示学习和逆强化学习的思想引入神经行为学分析，为定量研究自然行为提供了一个强大的新工具。它启发了未来工作可以探索如何将学习到的基元与特定的神经回路活动关联起来，从而在神经动力学和行为组成之间建立更直接的联系。此外，该框架也可能适用于其他需要从演示数据中分解出可解释、可组合基本技能的领域。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有动物行为分割方法将连续行为过度简化为离散音节、忽略组合动态的问题，提出了基于基元的连续动态发现框架。该框架利用行为转换结构学习任务无关的、可解释的基元集合作为潜在基函数，并将行为动态建模为这些基元的连续演化混合。实验在多任务网格世界、迷宫导航和真实动物行为数据上验证，该框架能识别可重用基元组件，捕捉连续组合动态，并生成比传统离散模型更真实的行为轨迹。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.15190" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>