<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>EdgeVLA: Efficient Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>EdgeVLA: Efficient Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.14049" target="_blank" rel="noreferrer">2507.14049</a></span>
        <span>作者: Budzianowski, Paweł, Maa, Wesley, Freed, Matthew, Mo, Jingxiang, Hsiao, Winston, Xie, Aaron, Młoduchowski, Tomasz, Tipnis, Viraj, Bolte, Benjamin</span>
        <span>日期: 2025/07/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉-语言模型（VLMs）的机器人控制策略（如OpenVLA）为解决机器人学习中的数据稀缺问题提供了有前景的途径。这些模型能够利用海量的图像-文本数据学习丰富的世界表示，并迁移到具身控制任务中。然而，这些模型通常参数量巨大（数十亿），在计算资源受限的移动平台或边缘设备（如Jetson Nano）上部署面临严峻挑战。其高计算和内存需求阻碍了实时性能，限制了研究者和实践者的可及性。现有工作通过量化或硬件特定内核优化效率，但推理速度仍仅为5-10 Hz，难以在边缘设备上实现实时控制。</p>
<p>本文针对上述部署瓶颈，提出了一种新的高效视觉-语言-动作（VLA）模型架构——EdgeVLA（EVLA）。其核心思路是通过两大关键创新来显著提升推理速度与效率，同时保持模型的表征能力：1）摒弃对末端执行器位置预测的自回归要求，实现联合预测；2）利用性能相当但计算需求显著降低的小型语言模型（SLMs）。用一句话概括，EVLA旨在不牺牲模型能力的前提下，使VLA模型能够在边缘设备上实现实时推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>EVLA的训练分为两个阶段，其整体框架及与自回归方法的对比如下图所示。</p>
<p><img src="https://arxiv.org/html/2507.14049v1/extracted/6634556/figures/model.png" alt="方法框架与对比"></p>
<blockquote>
<p><strong>图1</strong>: EVLA与OpenVLA的生成逻辑对比。两模型的第一阶段（VLM预训练）相同。在第二阶段，EVLA的大型语言模型被重新训练，以<strong>非自回归</strong>的方式一次性生成整个末端执行器位置。</p>
</blockquote>
<p><strong>第一阶段：VLM预训练</strong>。此阶段遵循PrismaticVLM模型族的方案，使用来自多样化标注数据集的120万图像-文本对以及合成的多模态指令调优示例进行训练，旨在学习鲁棒的视觉和语言表示。模型架构上，采用Qwen2-0.5B作为语言模型，以利用SLM的效率。视觉编码器采用与OpenVLA相同的双分支设计，使用预训练的SigLIP和DINOv2模型。一个投影层被训练用于将视觉表示映射到语言模型的词元空间，该层与微调后的视觉、语言组件联合学习。</p>
<p><strong>第二阶段：末端执行器预测的关节控制</strong>。此阶段使用来自OpenX数据集的大约100万个操作示例进行训练。传统VLA模型采用自回归方式预测末端执行器位置（例如，逐维度预测x, y, z），模仿语言生成的因果特性。本文提出假设：对于机器人控制，这种限制并非必要。预测整个末端执行器位置（即所有维度同时输出）不会损害模型的编码能力，却能大幅提升推理速度。</p>
<p>具体实现上，EVLA移除了语言模型中的因果掩码（causal mask），并训练模型一次性输出完整的末端执行器位置。如图1右侧所示，这种非自回归的生成方式消除了自回归解码的序列依赖，从而实现了推理速度的显著提升。这是EVLA的核心架构创新。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了BridgeData V2和OpenX数据集作为测试平台，主要对比基线为OpenVLA。实验在配备8张A100-80GB GPU的单节点（BridgeData V2）和80张A100-40GB GPU的集群（OpenX）上进行。</p>
<p><strong>训练性能对比</strong>。在BridgeData V2数据集上的早期实验表明，参数量仅为10亿的EVLA能够达到与75亿参数的OpenVLA相当的训练性能。</p>
<p><img src="https://arxiv.org/html/2507.14049v1/extracted/6634556/figures/bridge_loss.png" alt="BridgeData V2训练损失"></p>
<blockquote>
<p><strong>图2（左）</strong>: 在BridgeData V2数据集上，EVLA与OpenVLA的训练损失曲线对比。两者表现出相似的下降趋势和最终损失值。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.14049v1/extracted/6634556/figures/bridge_accuracy.png" alt="BridgeData V2动作词元准确率"></p>
<blockquote>
<p><strong>图2（右）</strong>: 在BridgeData V2数据集上，EVLA与OpenVLA的动作词元预测准确率曲线对比。两者准确率接近。</p>
</blockquote>
<p>在更大的OpenX数据集上进行的进一步评估显示，由于模型表征能力较小，EVLA的训练效率（达到同等性能所需的迭代次数）低于OpenVLA，但每次训练迭代的速度快了约7倍，并且允许使用更大的批处理大小，这在一定程度上抵消了效率差异。</p>
<p><img src="https://arxiv.org/html/2507.14049v1/extracted/6634556/figures/oxe_loss.png" alt="OpenX训练损失"></p>
<blockquote>
<p><strong>图3（左）</strong>: 在OpenX数据集上，EVLA与OpenVLA的训练损失曲线。曲线显示出停滞迹象，行为与BridgeData V2情况相似。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.14049v1/extracted/6634556/figures/oxe_accuracy.png" alt="OpenX动作词元准确率"></p>
<blockquote>
<p><strong>图3（右）</strong>: 在OpenX数据集上，EVLA与OpenVLA的动作词元预测准确率曲线。</p>
</blockquote>
<p><strong>效率增益</strong>。EVLA的架构修改带来了推理速度和内存消耗的实质性改进。如表I所示，在A100-40GB GPU上，EVLA的推理时间从OpenVLA的20毫秒减少到5毫秒，实现了<strong>4倍加速</strong>；内存占用从16GB减少到4GB，减少了75%。论文指出，这一速度提升在增加更多自由度（如关节角度）时只会更加显著。需要说明的是，此对比中OpenVLA使用了优化的<code>flash_attention2</code>内核，而EVLA是在默认的eager模式下评估的，这意味着EVLA的实际效率潜力可能更大。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出并验证了在VLA模型中，对末端执行器位置进行<strong>非自回归联合预测</strong>的可行性，这在不损失编码能力的前提下，带来了关键的推理速度提升（4-7倍）。2）成功将<strong>小型语言模型（SLMs）</strong> 集成到VLA架构中，证明了其可以达到与庞大模型相近的训练性能，同时大幅降低了计算和内存需求。</p>
<p>论文自身提到的局限性在于，目前仅为早期结果，尚未在真实机器人平台或多样化实体上进行评估以验证其少样本泛化能力。下一步计划在至少两种不同的人形机器人平台上进行评估。</p>
<p>这项工作为在资源受限的边缘设备上部署实时VLA模型开辟了道路。其对后续研究的启示包括：鼓励探索更灵活高效的注意力机制（如FlexAttention）与EVLA架构的结合；推动VLA模型在纯CPU平台等更广泛边缘设备上的部署优化；以及启发研究者重新审视自回归生成在具身AI任务中是否总是必要的，或许在其他连续动作预测任务中也可探索非自回归方案。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对大规模视觉-语言-动作模型在资源受限的边缘设备上部署困难、难以实现实时性能的问题，提出了EdgeVLA高效模型。其关键技术包括：1）**消除末端执行器位置预测的自回归要求**，改为联合预测；2）**采用小型语言模型**以降低计算需求。核心实验表明，该方法在保持与OpenVLA模型可比训练性能的同时，实现了**7倍的推理速度提升**，并显著提高了内存效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.14049" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>