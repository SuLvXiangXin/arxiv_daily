<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.15530" target="_blank" rel="noreferrer">2510.15530</a></span>
        <span>作者: Bin He Team</span>
        <span>日期: 2025-10-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人模仿学习的主流视觉运动策略方法可分为非纯视觉方法和纯视觉方法。非纯视觉方法（如DP3、3D Diffuser Actor）依赖点云或RGB-D图像等显式3D表示作为输入，虽然得益于精确的低维3D表示而取得了较高的操作精度，但其严重依赖昂贵的深度传感器，且传感器精度限制了模型性能上限，在复杂语义密集型任务中表现受限。纯视觉方法（如DP、ACT）仅以RGB图像为输入，更贴近生物感知-动作系统且硬件成本显著更低，但现有方法因表征学习模块发展不足，其性能尚未超越基于点云的方法，且在真实部署中对环境变化（如背景、光照）非常敏感。</p>
<p>本文针对纯视觉方法在表征学习上的不足这一具体痛点，提出利用预训练的视觉基础模型来从单目RGB图像中有效融合语义与几何特征的新视角。核心思路是：利用预训练的3D重建模型VGGT，从中提取语义特征（来自DINOv2）和几何特征（来自交替注意力网络），通过交叉注意力进行自适应融合，再经空间压缩后输入扩散策略头，从而仅凭单视图RGB图像实现媲美甚至超越点云方法的操作性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>VO-DP的整体框架是一个端到端的视觉运动策略学习模型，输入为包含T帧历史RGB图像和机器人本体状态（关节角度）的观测$O_t$，输出为未来N步的动作轨迹$A_t$。其核心流程包含四个模块：1) 基于几何先验的视觉编码器提取特征；2) 语义-几何特征融合器进行自适应模态融合；3) 场景表示压缩模块提炼关键信息；4) 纯视觉条件动作生成模块（策略头）预测动作。</p>
<p><img src="https://arxiv.org/html/2510.15530v4/x2.png" alt="方法整体架构"></p>
<blockquote>
<p><strong>图2</strong>：VO-DP的整体架构。包含四个核心模块：VGGT编码器提取语义和几何特征；语义-几何融合器通过残差交叉注意力融合特征；空间压缩模块使用轻量级ResNet下采样并压缩特征，再与本体观测拼接形成场景表示；基于DDPM的策略头利用场景表示生成动作轨迹。</p>
</blockquote>
<p><strong>核心模块1：基于几何先验的视觉编码器</strong>。采用在多种3D重建任务上预训练的VGGT模型作为编码器。输入图像历史$I_t$首先通过DINOv2进行分块，得到语义特征$\mathbf{h^{sem}_t} \in \mathbb{R}^{T \times P \times C}$。随后，这些令牌被送入VGGT的交替注意力（AA）网络（包含24个AA块）。每个AA块包含帧内自注意力和全局自注意力层，其输出被拼接以整合局部与全局信息，形成几何特征$\mathbf{h^{geo}_t} \in \mathbb{R}^{T \times P \times 2C}$。该设计使得模型能从单目图像中直接提取丰富的隐含3D几何信息。</p>
<p><strong>核心模块2：语义-几何特征融合器</strong>。为了自适应地利用语义和几何信息，该模块以逐帧方式进行特征融合。对于第i帧，以几何特征$\mathbf{g}$作为查询（Query），语义特征$\mathbf{s}$作为键值对（Key-Value），通过残差交叉注意力机制进行融合。具体过程为：先对$\mathbf{g}$沿特征维度进行一维平均池化（核大小为2，步长为2）得到$\mathbf{h’}$，然后与交叉注意力输出相加得到$\mathbf{h’’}$，最后再通过一个前馈网络（FFN）层得到融合后的特征$\mathbf{h^{sg}_t[i]}$。这种设计允许模型根据任务特定需求自适应地注入语义信息到几何特征中。</p>
<p><strong>核心模块3：场景表示压缩模块</strong>。该模块旨在将融合后的高维特征令牌压缩为紧凑的场景表示。首先将$\mathbf{h^{sg}_t}$的形状重塑为$\mathbb{R}^{T \times C \times H_P \times W_P}$。然后使用三个基础残差块（核大小3，步长2）进行下采样，接着通过自适应2D平均池化得到空间特征$\mathbf{h^{sp}_t}$。该特征经MLP投影到低维空间后，与本体观测$\mathbf{S_t}$拼接，最终形成场景表示$\mathbf{h^{sc}_t} \in \mathbb{R}^{T \times (C’ + J)}$，作为策略头的条件输入。</p>
<p><strong>核心模块4：纯视觉条件动作生成模块</strong>。策略头沿用扩散策略（DP）的设计，采用基于DDPM的条件去噪扩散概率模型来近似条件分布$p(A_t | O_t)$。去噪过程遵循公式$A_t^{k-1} = \alpha(A_t^k – \gamma \varepsilon_\theta(h_t^{sc}, A_t^k, k) + \mathcal{N}(0, \sigma^2 I))$，其中$\varepsilon_\theta$是一个以场景表示$h_t^{sc}$和扩散步数$k$为条件的噪声预测网络。训练时采用均方误差（MSE）损失：$\mathcal{L}(\theta) = MSE(\varepsilon^k, \varepsilon_\theta(h_t^{sc}, A_t^k, k))$。</p>
<p><strong>创新点</strong>：与现有纯视觉方法（如DP仅使用常规图像骨干网络）相比，VO-DP的创新在于引入了预训练的、具有空间感知能力的视觉基础模型VGGT作为特征提取器，并设计了专门的融合与压缩模块，从而显式且自适应地从RGB图像中挖掘并融合了语义与几何信息，极大地增强了纯视觉策略的场景理解与泛化能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：仿真实验在RoboTwin 1.0基准上进行，该基准基于SAPIEN模拟器，包含14项双手操作任务（如图3所示）。真实世界实验使用Realman RM65-B机器人，设置了4项空间操作任务和4项鲁棒性测试。对比的基线方法包括纯视觉方法DP和基于点云的方法DP3。</p>
<p><img src="https://arxiv.org/html/2510.15530v4/x3.png" alt="仿真任务示例"></p>
<blockquote>
<p><strong>图3</strong>：RoboTwin仿真基准中的14项双手操作任务示例。左图为任务的俯视RGB图像，右图为通过VGGT重建的点云。</p>
</blockquote>
<p><strong>关键仿真结果</strong>：如表II所示，在14个仿真任务的平均成功率上，VO-DP（使用3帧历史）达到63.9%，其单帧变体VO-DP-1达到64.6%，与基于点云的DP3（64.0%）性能相当，并远高于纯视觉基线DP（34.8%）。在多个任务上，VO-DP甚至超越了DP3，例如在“Pick Apple Messy”任务中，VO-DP成功率（80.0%）显著高于DP3（18.7%）和DP（31.0%）。</p>
<p><img src="https://arxiv.org/html/2510.15530v4/x22.png" alt="仿真性能对比表"></p>
<blockquote>
<p><strong>表II</strong>：RoboTwin基准测试结果。VO-DP及其单帧变体VO-DP-1的平均成功率与DP3相当，并大幅超越DP。</p>
</blockquote>
<p><strong>数据效率分析</strong>：如图4所示，随着示范数据量从20增加到100，VO-DP在复杂任务（如Pick Apple Messy、Block Hammer Beat）上表现出比DP和DP3更显著和稳定的性能提升，证明了其预训练编码器带来的高数据效率。</p>
<p><img src="https://arxiv.org/html/2510.15530v4/x4.png" alt="数据效率对比"></p>
<blockquote>
<p><strong>图4</strong>：VO-DP与基线方法在四个任务上的数据效率和扩展能力对比。随着示范数据增加，VO-DP性能提升更为显著。</p>
</blockquote>
<p><strong>消融实验</strong>：表III展示了不同特征模态的消融结果。移除几何特征（w/o geo.）或语义特征（w/o sem.）都会导致平均性能下降，而完整的VO-DP模型取得了最佳的平均成功率（80.0%），验证了语义与几何特征融合的有效性。在需要强语义理解（如Pick Apple Messy）或复杂空间结构（如Dual Bottles Pick (Easy)）的任务中，分别移除对应特征会导致性能大幅下降。</p>
<p><strong>真实世界实验结果</strong>：如图8所示，在四个真实操作任务中，VO-DP-1取得了平均87.9%的成功率，显著优于DP3（67.5%）和DP（11.2%）。这证明了VO-DP在真实场景中的卓越泛化能力和实用性。</p>
<p><img src="https://arxiv.org/html/2510.15530v4/x8.png" alt="真实世界性能对比"></p>
<blockquote>
<p><strong>图8</strong>：真实世界任务性能对比。VO-DP-1在四项任务上的平均成功率（87.9%）显著高于DP3和DP。</p>
</blockquote>
<p><strong>鲁棒性测试</strong>：如图9所示，在物体大小、外观（颜色）、光照（频闪）和背景变化的鲁棒性测试中，VO-DP-1均保持了很高的成功率（普遍在80%以上），展现出极强的环境适应性和稳定性。</p>
<p><img src="https://arxiv.org/html/2510.15530v4/x9.png" alt="鲁棒性测试结果"></p>
<blockquote>
<p><strong>图9</strong>：基于Pick&amp;Place Small Cube任务的鲁棒性测试结果。VO-DP在物体大小、颜色、光照和背景变化下均保持了高成功率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>证明了纯视觉策略的巨大潜力</strong>：首次展示了仅使用单目RGB图像的视觉运动策略可以达到与先进点云方法（DP3）相当的仿真性能，并在真实世界任务中显著超越之。</li>
<li><strong>提出了VO-DP方法</strong>：一种新颖的、自适应融合语义与几何特征的纯视觉表征学习方法，有效利用预训练视觉基础模型（VGGT）提升了纯视觉策略的感知与泛化能力。</li>
<li><strong>开源了通用训练框架</strong>：发布了基于Accelerate的机器人操作训练库DRRM，支持多机多卡并行训练、混合精度训练，并与RoboTwin模拟器兼容。</li>
</ol>
<p><strong>局限性</strong>：论文指出，在部分需要精确几何信息的结构化任务（如Diverse Bottles Pick）中，VO-DP的性能仍略低于直接使用原始点云输入的DP3。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>探索更高效的视觉表征学习</strong>：本研究凸显了预训练视觉基础模型对纯视觉机器人操作的巨大价值，未来可探索更多专为具身智能设计的视觉表征模型。</li>
<li><strong>降低对高质量3D数据的依赖</strong>：VO-DP的成功表明，通过算法从丰富但廉价的2D视觉数据中挖掘3D信息，是降低机器人系统硬件成本和复杂度的可行路径。</li>
<li><strong>推动纯视觉方法的实际部署</strong>：其出色的真实世界鲁棒性为纯视觉方案在复杂、动态现实环境中的部署提供了有力支持。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VO-DP，一种用于纯视觉机器人操作的语义-几何自适应扩散策略。核心问题是解决现有模仿学习方法过度依赖点云输入、缺乏高效纯视觉方案的问题。方法上，VO-DP利用预训练视觉基础模型（VGGT、DINOv2）提取语义与几何特征，通过交叉注意力融合并经CNN压缩后输入策略头。实验表明，在模拟任务中VO-DP平均成功率达64.6%，与点云方法DP3（64.0%）相当，并远超纯视觉基线DP（34.8%）；在真实任务中达到87.9%，显著优于DP3（67.5%）和DP（11.2%），且在不同干扰下保持高鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.15530" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>