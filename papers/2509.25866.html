<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.25866" target="_blank" rel="noreferrer">2509.25866</a></span>
        <span>作者: Zhang, Chi, Qiu, Haibo, Zhang, Qiming, Zeng, Zhixiong, Ma, Lin, Zhang, Jing</span>
        <span>日期: 2025/09/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将逐步推理（Chain-of-Thought）集成到视觉语言模型（VLMs）中，已成为提升其在复杂任务上性能的主流方法。然而，现有VLMs普遍存在“thinking over seeing”的倾向，即其生成的冗长推理轨迹常常与实际视觉输入脱节，容易误解图像关键细节甚至产生幻觉，表明其推理更多由语言先验驱动而非真正的视觉感知。为解决此问题，“thinking with images”范式被提出，它鼓励模型通过调用视觉工具（如缩放、裁剪）或生成中间视觉表征，迭代地关注细粒度区域，从而实现更深入、更可靠的基于视觉证据的推理。</p>
<p>尽管前景广阔，该新兴范式在数据构建准确性、结构设计和更广泛的应用场景上仍有很大探索空间。现有方法主要分为两类：一是基于空间定位（Grounding-based）的操作，依赖对坐标的精确预测，易受标注噪声影响；二是基于生成（Generation-based）的操作，利用图像生成模型进行编辑，但稳定性和可控性有限。这些方法共同面临操作空间受限、定位噪声以及训练难度高等挑战。</p>
<p>本文针对上述痛点，提出了一个名为DeepSketcher的完整解决方案，其核心思路是：通过构建一个所有图像均由代码渲染的高质量交错式图文推理轨迹数据集，确保视觉操作的精确性和可复现性；并设计一个自包含的模型，将整个“推理→工具调用→图像操作→推理”链内部化，直接在视觉嵌入空间操作图像，实现免工具、更灵活的“thinking with images”。</p>
<h2 id="方法详解">方法详解</h2>
<p>DeepSketcher包含一个高质量数据集和一个自包含模型。其整体目标是让模型学会在推理过程中，当视觉证据不确定时，自主生成编辑指令（如“画一条切线”），并直接在内部更新视觉表征以辅助后续推理，最终输出答案。</p>
<p><img src="https://arxiv.org/html/2509.25866v1/x3.png" alt="方法架构"></p>
<blockquote>
<p><strong>图4</strong>：DeepSketcher模型架构。给定查询Q和初始图像I0，视觉语言模型（VLM）生成推理文本Rt和编辑指令Actt。嵌入编辑器（Embedding Editor）直接在视觉嵌入空间进行操作，其预测受到代码渲染的真实编辑图像的监督，并将更新后的嵌入插回VLM的上下文中。这个过程产生了交错的推理和视觉操作轨迹，最终输出答案。</p>
</blockquote>
<p><strong>1. DeepSketcher数据集</strong><br>数据集包含31k条图文交错的推理轨迹，覆盖几何、图表、数学等多个领域（图2）。其关键创新在于所有图像均为<strong>代码渲染</strong>。每个数据实例表示为元组 <code>(C, I, Q, A)</code>，其中C是渲染代码，I是渲染器R(C)生成的图像，Q是视觉问题，A是答案。视觉操作通过修改代码C并重新渲染来实现，这避免了基于边界框定位的噪声和基于图像生成的不稳定性，确保了操作的精确性和可复现性。数据构建采用两轮流水线：第一轮，设计一个由求解器（Solver）和代码编辑器（Code Editor）两个LLM专家协作的智能体系统，在代码渲染的VQA数据上生成6k条种子轨迹；第二轮，通过<code>img2code</code>管道将更多VQA数据集的图像转换为代码，并利用训练好的中间模型在智能体系统中生成更丰富的轨迹，将数据扩展至31k条。</p>
<p><img src="https://arxiv.org/html/2509.25866v1/x2.png" alt="数据覆盖领域"></p>
<blockquote>
<p><strong>图2</strong>：数据集的学科覆盖范围，包括几何、函数图、图表、物理图示等。</p>
</blockquote>
<p><strong>2. DeepSketcher模型</strong><br>模型旨在内部化整个推理与视觉操作链。其流程如下：给定初始图像-查询对<code>(I0, Q)</code>，编码为<code>(Ev,0, Q)</code>。模型首先生成推理步骤<code>R0</code>。当需要视觉澄清时，模型自主生成一个编辑指令<code>Act0</code>。随后，嵌入编辑器接收<code>(Ev,0, Act0)</code>，并直接在视觉嵌入空间预测出“被操作后”的图像表征<code>Ev,1</code>。更新后的上下文<code>{Ev,0, Q, R0, Act0, Ev,1}</code>被反馈给模型进行下一步推理。此过程递归进行，最终产生交错的轨迹<code>(R0, Act0, Ev,1, R1, Act1, Ev,2, ..., A)</code>。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ul>
<li><strong>嵌入编辑器（Embedding Editor）</strong>：这是实现内部化视觉操作的核心。它采用类似Q-Former的架构，但进行了关键修改：以冻结视觉编码器输出的视觉令牌<code>Ev</code>作为查询（Query），将从编辑指令<code>Act</code>中提取的动作嵌入<code>E_act</code>作为键（Key）和值（Value），通过交叉注意力（Cross-Attention）块和前馈网络（FFN）来更新视觉令牌。这样，文本指令的语义就被注入到视觉空间中，产生编辑后的视觉嵌入<code>Ev_pred</code>。</li>
<li><strong>三阶段训练策略</strong>：<ul>
<li><strong>第一阶段（预热）</strong>：使用数据集中的真实图像特征（而非编辑器预测的特征）训练模型生成交错的图文序列。损失函数<code>ℒ_LM^phase-1</code>是标准的语言建模损失，仅作用于文本令牌和标志视觉内容边界的特殊令牌（<code>&lt;vision_start&gt;</code>, <code>&lt;vision_end&gt;</code>），让模型学习交错序列的结构。</li>
<li><strong>第二阶段（编辑器训练）</strong>：初始化使用第一阶段的模型，并<strong>冻结除嵌入编辑器外的所有模块</strong>。监督信号使用真实编辑图像经过视觉编码器得到的嵌入<code>Ev_gt</code>作为目标，对编辑器预测的嵌入<code>Ev_pred</code>应用L1损失。同时，语言建模损失<code>ℒ_LM^phase-2</code>的条件变为编辑器输出的视觉令牌，使VLM开始适应编辑器产生的视觉上下文。</li>
<li><strong>第三阶段（联合微调）</strong>：保持相同的训练目标，<strong>解冻LLM主干</strong>，让模型进一步适应自身编辑器的输出，确保生成的编辑指令与由此产生的视觉上下文保持一致。视觉编码器在所有阶段均保持冻结。</li>
</ul>
</li>
</ul>
<p>与现有方法相比，DeepSketcher的主要创新点在于：1) 通过代码渲染数据从根本上保证了训练时视觉操作监督信号的精确性；2) 模型将视觉操作内部化到嵌入空间，无需调用外部工具、无需预测空间坐标、也无需对生成图像进行重复编码，推理更加灵活高效；3) 嵌入编辑器在相对高语义层次的视觉令牌上进行操作，并显式地以文本指令为条件，比在高度压缩的潜空间进行编辑保留了更丰富的语义信息。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：在五个多模态推理基准上评估：MathVerse (vision-only)、MathVision (mini)、MathVista (mini)、LogicVista (Overall)、WeMath (Overall)。此外，还构建了一个内部基准Indicator-500，用于在训练期间评估嵌入编辑器。</li>
<li><strong>对比方法</strong>：与四大类基线比较：1) <strong>专有VLM</strong>：Claude3.7-Sonnet, GPT-4.1；2) <strong>开源VLM</strong>：InternVL3-8B, Qwen2.5-VL-7B；3) <strong>工具调用VLM</strong>：VILASR-7B, DeepEyes-7B；4) <strong>内部视觉思考VLM</strong>：Bagel-Zebra-CoT-7B, Mirage-7B。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2509.25866v1/x4.png" alt="主要结果表"></p>
<blockquote>
<p><strong>图5（对应论文表2）</strong>：各模型在多模态推理基准上的性能对比。DeepSketcher-7B在“内部视觉思考VLM”类别中表现最佳，平均得分46.0，超越了同类型的Bagel-Zebra-CoT-7B (43.6) 和 Mirage-7B (35.4)。相较于其基础模型Qwen2.5-VL-7B (42.1)，平均提升了3.9个百分点。</p>
</blockquote>
<p>如表所示，DeepSketcher-7B在平均得分上达到46.0，显著优于其他内部视觉思考VLM。尽管操作范式灵活得多，其性能也超过了工具调用VLM VILASR-7B (33.6) 和 DeepEyes-7B (45.1)。与基础模型Qwen2.5-VL-7B相比，平均提升3.9分。具体来看，在涉及几何和计数的MathVision上提升最大（+5.3分），在需要逻辑或数值推理的LogicVista上也提升显著（+8.3分）。然而，在包含大量开放域图像和学科知识的MathVista上提升较小（+0.9分），表明模型在域外知识密集型任务上泛化有限。</p>
<p><strong>消融实验</strong>：</p>
<ol>
<li><strong>智能体数据策划系统有效性</strong>：实验对比了求解器LLM独立工作与和代码编辑器LLM协作两种设置下的答题正确率（pass@8）。结果表明，协作设置能显著提升求解器的性能，验证了通过代码编辑获取精确视觉信息有助于解决更复杂问题，从而能收集到更高质量的训练轨迹。</li>
<li><strong>模型设计消融</strong>：论文对模型关键组件进行了消融研究。<br><img src="https://arxiv.org/html/2509.25866v1/x5.png" alt="消融研究"><blockquote>
<p><strong>图8</strong>：模型设计消融实验。<code>w/o Editor</code>表示不使用嵌入编辑器，直接使用原始图像特征；<code>w/o Phase-3</code>表示不进行第三阶段联合微调；<code>Full</code>为完整模型。完整模型在Indicator-500基准上性能最佳。</p>
</blockquote>
如图8所示，移除嵌入编辑器（<code>w/o Editor</code>）或省略第三阶段联合微调（<code>w/o Phase-3</code>）都会导致性能下降，证明了嵌入编辑器内部化视觉操作的必要性，以及让LLM主干适应编辑器输出的重要性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>高质量数据集</strong>：提出了一个大规模、高质量的图文交错推理轨迹数据集DeepSketcher，其所有图像均由代码渲染，确保了视觉操作的精确性、可复现性，并支持开放词汇的视觉操作，避免了以往数据中的定位噪声。</li>
<li><strong>自包含模型架构</strong>：设计了一个将视觉操作内部化的模型，它直接在视觉嵌入空间执行编辑，无需依赖外部工具调用或坐标预测，实现了更灵活、高效的“thinking with images”范式。</li>
<li><strong>有效性验证</strong>：在多个多模态推理基准上的实验表明，所提方法显著提升了模型性能，特别是在需要几何和逻辑推理的任务上，验证了数据集和模型设计的有效性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1) 方法依赖于代码渲染的图像数据，这虽然保证了精确性，但可能限制模型对真实世界、非合成图像的泛化能力。2) 在MathVista等包含大量开放域知识和真实图像的基准上，性能提升有限，表明模型在处理域外、知识密集型任务时面临挑战。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据策略</strong>：探索混合数据策略，将精确的代码渲染数据与真实世界图像数据结合，以增强模型的泛化能力。</li>
<li><strong>架构泛化</strong>：研究如何将嵌入编辑器的思想应用于其他模态（如视频、3D）或更广泛的视觉操作，进一步扩大模型的“思维空间”。</li>
<li><strong>训练效率</strong>：本文的三阶段训练策略计算成本较高，未来可研究更高效的端到端训练方法，以降低此类内部化视觉推理模型的训练门槛。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前视觉语言模型（VLMs）存在的“thinking over seeing”问题，即推理过程与视觉输入脱节，提出了DeepSketcher解决方案。其核心是构建了一个包含31k条高质量图像-文本交错推理轨迹的数据集，并设计了一个能直接在视觉嵌入空间内部生成“视觉思考”的模型，无需调用外部工具。该模型实现了工具无关、更灵活的“用图像思考”。在多模态推理基准测试上的广泛实验表明，其性能强劲，验证了数据集和模型设计的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.25866" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>