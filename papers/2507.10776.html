<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.10776" target="_blank" rel="noreferrer">2507.10776</a></span>
        <span>作者: Kaiyu Hang Team</span>
        <span>日期: 2025-07-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，未见过的物体实例分割（UOIS）的主流方法是基于大规模数据集训练计算昂贵的深度神经网络，以提取潜在空间特征表示。然而，这些学习方法在分布外（OOD）场景（如高度杂乱的场景）中，仍然面临欠分割和过分割的挑战，其性能受限于对静态视觉特征的过拟合以及模拟到现实的差距和数据偏见。本文针对这一局限性，基于“视觉本质上是交互式的并随时间发生”这一原则，重新思考了UOIS任务。本文提出了一种新的实时交互感知框架rt-RISeg，它通过机器人交互和分析设计的体坐标系不变特征（BFIF）来持续分割未见过的物体。核心思路是：利用机器人交互产生的物体刚体运动信息，通过分析其体坐标系不变特征，在无需任何预训练分割模型的情况下，实时、在线地完成物体实例分割。</p>
<h2 id="方法详解">方法详解</h2>
<p>rt-RISeg是一个轻量级、无模型的交互感知框架，其整体流程如算法1所示。系统在初始时刻获取机器人状态和场景RGB-D图像后，进入一个交互循环：首先，<code>FindAction</code>（算法2）基于当前分割掩码和深度图像启发式地选择下一个机器人动作；然后，在执行该动作（<code>Interact</code>）的过程中，系统实时获取新的状态和图像，并调用<code>SegmentObjs</code>（算法3）通过BFIF分析来持续更新分割掩码<code>L_t</code>。动作执行完毕后，记录中间结果，并开始下一轮交互，直到所有物体被分割（<code>FindAction</code>返回<code>null</code>）。输入是连续的RGB-D图像流<code>I_t</code>和机器人关节角<code>θ_t</code>，输出是不断更新的实例分割掩码<code>L_t</code>，其中<code>L_t(i,j)&gt;0</code>表示像素属于某物体实例ID，<code>L_t(i,j)=0</code>表示背景。</p>
<p><img src="https://arxiv.org/html/2507.10776v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：rt-RISeg交互分割流程与对比。展示了实时BFIF分组分割过程，并与静态分割模型结果对比。</p>
</blockquote>
<p>框架的核心是<strong>体坐标系不变特征（BFIF）</strong>。其原理是：附着在同一运动刚体上的不同体坐标系，当它们的运动被表示在一个公共的固定空间坐标系中时，将具有相同的空间螺旋（spatial twist），即相同的角速度<code>ω_s</code>和线速度<code>υ_s</code>，无论它们自身的绝对运动如何。这一定理是刚体运动学的基本性质。</p>
<p><img src="https://arxiv.org/html/2507.10776v1/x2.png" alt="BFIF原理"></p>
<blockquote>
<p><strong>图2</strong>：BFIF原理示意图。同一刚体上不同体坐标系{a1}、{a2}和{b1}、{b2}的运动，转换到固定空间坐标系{s}后，各自的线速度υ{s}x相同。</p>
</blockquote>
<p>基于BFIF的<strong>实时对象分割（SegmentObjs）</strong> 流程如算法3所示。具体步骤为：1) 使用光流法计算相邻帧<code>I_{t-1}</code>到<code>I_t</code>的密集运动场。2) 在图像中随机采样N个点作为“体坐标系”，利用深度图像和相机参数将它们反投影到3D空间，并根据光流计算其3D位移，进而估算每个点在固定空间坐标系下的BFIF（<code>ω_s, υ_s</code>）。3) 对BFIF向量进行聚类（如DBSCAN），同一簇内的点被认为属于同一刚体，即同一物体实例。4) 利用聚类结果和上一帧的分割掩码<code>L_{t-1}</code>，通过区域生长和标签传播策略更新得到当前帧的分割掩码<code>L_t</code>。整个过程在机器人交互过程中持续进行，实现了“边交互边观察”的范式转变。</p>
<p><strong>动作选择（FindAction）</strong> 模块的目标是选择既能有效区分物体运动，又对场景初始任务构型破坏最小的交互。其策略是：通过RANSAC平面拟合识别桌面上的物体区域，减去已分割的区域，得到待分割物体掩码。然后对该掩码进行K-means聚类，对每个聚类计算其边界点，并选择距离机器人最近且易于推动的边界点作为交互点，生成一个平移推动动作。若没有待分割物体，则返回<code>null</code>停止交互。</p>
<p>与现有方法相比，rt-RISeg的创新点主要体现在：1) <strong>范式创新</strong>：从“观察-交互-观察”转变为“边交互边观察”，实现真正的实时在线分割。2) <strong>方法创新</strong>：完全无需任何预训练的分割模型，仅依赖物理刚体运动原理（BFIF）进行分割，从根本上避免了数据偏见和过拟合问题。3) <strong>实用性创新</strong>：强调通过最小破坏性交互保持场景初始任务构型，更符合实际机器人操作需求。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个数据集上进行评估：<strong>Occluded Object Dataset (OOD)</strong> 和 <strong>YCB In-Situ Dataset</strong>。实验平台使用Franka Emika Panda机械臂和Intel RealSense D415相机。</p>
<p>对比的基线方法包括两种先进的静态分割模型：<strong>Segment Anything Model (SAM)</strong> 和 **Mean Shift Mask Transformer (MSMFormer)**。同时，论文也展示了将rt-RISeg自动生成的分割掩码作为提示输入给SAM（即rt-RISeg → SAM）的结果。</p>
<p><img src="https://arxiv.org/html/2507.10776v1/x4.png" alt="定量结果"></p>
<blockquote>
<p><strong>图4</strong>：定量评估结果。rt-RISeg在Occluded Object Dataset和YCB In-Situ Dataset上均显著优于所有对比方法。</p>
</blockquote>
<p>关键定量结果如下：在OOD数据集上，rt-RISeg的平均分割准确率达到69.6%，比表现最好的静态方法MSMFormer（42.1%）高出27.5%。在YCB In-Situ数据集上，rt-RISeg准确率为62.4%，同样大幅领先于MSMFormer（35.1%）。此外，rt-RISeg → SAM的组合取得了接近完美的分割精度（OOD: 94.3%， YCB: 96.2%），证明了rt-RISeg生成高质量提示的能力。</p>
<p><img src="https://arxiv.org/html/2507.10776v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融实验结果。验证了BFIF分组、实时更新和动作选择策略各自的有效性。</p>
</blockquote>
<p>消融实验验证了各核心组件的贡献：1) <strong>使用BFIF进行分组</strong>是分割有效性的基础，移除后（改用欧氏距离聚类）性能大幅下降。2) <strong>实时更新</strong>机制（“边交互边观察”）相比等待动作完成后再分析（“交互后观察”），能带来约10%的性能提升。3) <strong>智能动作选择策略</strong>（避免移动已分割物体、选择边界点）相比随机动作选择，能显著提高分割效率和成功率。</p>
<p><img src="https://arxiv.org/html/2507.10776v1/x6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图6</strong>：定性结果对比。rt-RISeg能准确分割堆叠、遮挡和透明物体，而静态方法在这些场景下失效。</p>
</blockquote>
<p>定性结果（图1，图6）显示，在物体堆叠、严重遮挡、透明/反光物体等具有挑战性的OOD场景中，静态分割模型（SAM, MSMFormer）普遍失效，出现严重的欠分割或过分割。而rt-RISeg通过分析交互引起的微小刚体运动，能够准确地区分和分割各个物体实例。即使物体仅发生微小位移，rt-RISeg也能实现实时、准确的分割。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>提出了“边交互边观察”的新范式</strong>，并实现了首个完全<strong>无模型</strong>的实时机器人交互式实例分割框架rt-RISeg，它不依赖任何预训练视觉模型。2) <strong>系统性地利用了刚体运动的物理原理（BFIF）</strong>，将其转化为可计算的图像特征，用于鲁棒的物体区分，在挑战性场景中性能显著优于静态学习方法。3) 展示了rt-RISeg作为<strong>自主提示生成器</strong>的潜力，能够为SAM等视觉基础模型提供高质量提示，从而结合物理交互的鲁棒性和大模型的精细分割能力。</p>
<p>论文自身提到的局限性包括：1) <strong>刚体假设</strong>：方法假设物体是刚性的，对于非刚性物体或关节物体可能不适用。2) <strong>场景假设</strong>：当前的动作选择策略假设物体放置在平面（如桌面）上，且机器人执行推动动作。这限制了其在更复杂环境中的直接应用。</p>
<p>本研究对后续工作的启示在于：为机器人感知开辟了一条不依赖于大规模静态数据、而是通过与物理世界交互来获取理解的新路径。未来工作可以探索将BFIF原理扩展到非刚性物体、设计更通用的交互策略以应对复杂环境，以及进一步探索这种物理交互感知框架与大型视觉模型协同完成复杂任务的可能性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对未见物体实例分割在分布外场景中泛化性能差的问题，提出了一种无需模型的实时机器人交互分割框架rt-RISeg。其核心方法是利用机器人交互产生的相对运动，通过设计的体坐标系不变特征（BFIF）实时识别与分割物体，无需预先训练的分割模型。实验表明，该方法的平均分割准确率比最先进的UOIS方法提升27.5%，并能自主生成分割掩码作为视觉基础模型的提示以进一步提升性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.10776" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>