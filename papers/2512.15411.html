<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.15411" target="_blank" rel="noreferrer">2512.15411</a></span>
        <span>作者: Yin, Zhenhan, Wang, Xuanhan, Jiang, Jiahao, Deng, Kaiyuan, Chen, Pengqi, Li, Shuangle, Liu, Chong, Xu, Xing, Song, Jingkuan, Gao, Lianli, Shen, Heng Tao</span>
        <span>日期: 2025/12/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型（VLAs）通过在大规模真实机器人演示数据上进行模仿或强化学习预训练，成功建立了从机器人观测和人类指令到机器人动作的多模态映射。然而，其发展受限于真实机器人数据的稀缺性，这不仅体现在收集成本高昂，也体现在难以覆盖开放世界环境的多样性，从而严重制约了VLAs的泛化能力和通用性。为克服数据稀缺，研究者转向利用仿真机器人数据和人类视频，前者提供机器人控制的行为先验，后者则蕴含丰富的真实世界任务与场景知识。但直接利用这些数据面临关键障碍：仿真数据存在典型的Sim2Real（仿真到现实）鸿沟，而人类数据则因形态差异（人与机器人）难以直接迁移。</p>
<p>本文针对上述利用仿真和人类数据时存在的视角、外观和形态不匹配问题，提出了一种新视角：通过人-机器人互模仿预训练，将仿真机器人数据的操作多样性与人类视频的行为保真度统一到一个模型中。其核心思路是：设计一个通用的、基于左右手坐标系对齐的人-机器人动作空间双向映射机制，并在此基础上，让模型在观看一种形态（如人类或机器人）的演示时，同时学习预测该形态的行为轨迹并模仿生成另一种未见形态的动作，从而构建一个具有强大泛化能力的通用VLA模型——MiVLA。</p>
<h2 id="方法详解">方法详解</h2>
<p>MiVLA的整体目标是通过人-机器人互模仿预训练，整合仿真机器人数据和人类视频中的行为先验。其流程分为两个核心阶段：1）基于运动学规则的人-机器人动作空间双向对齐；2）基于对齐后“互补动作”的互模仿预训练。</p>
<p><img src="https://arxiv.org/html/2512.15411v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MiVLA方法总览。引入通用的人-机器人动作映射机制以弥合动作空间差距。给定仿真机器人演示，训练VLA模型预测机器人动作，并学习在人类动作空间中模仿机器人行为。对于人类演示，则训练相同的策略进行人→机器人模仿。</p>
</blockquote>
<p><strong>模型架构</strong>：MiVLA建立在多模态分词器和基于扩散的动作解码器之上。视觉和语言输入分别使用DINOv2和Siglip、T5进行分词并投影到公共嵌入空间。本体感觉状态通过MLP编码。动作解码器采用扩散Transformer，将动作生成视为以观测令牌（视觉、语言、本体感觉）为条件的迭代去噪过程。</p>
<p><strong>核心模块：人-机器人动作映射</strong>。这是实现跨形态学习的关键。该方法选取人类拇指关节姿态和机器人末端执行器（EEF）姿态作为参考点，其余关节可通过逆运动学或解剖学先验推断。</p>
<ul>
<li><strong>人→机器人映射</strong>：给定机器人初始EEF姿态，目标姿态通过转换人类手腕姿态的相对变化得到（公式2），随后通过逆运动学（IK）求解器计算机器人关节角度。</li>
<li><strong>机器人→人映射</strong>：将机器人EEF姿态映射为人类拇指关节姿态（公式3），人类其余关节位置则基于拇指位置和根据解剖学先验估计的手指间距离得到。<br>通过此机制，无论是人类视频还是机器人演示，都能生成对应的“互补动作”数据。</li>
</ul>
<p><strong>核心模块：互模仿预训练目标</strong>。与传统VLA学习从观测到特定形态动作的映射不同，MiVLA学习从单一形态的观测预测跨形态的动作。</p>
<ul>
<li>给定机器人演示，模型需同时预测真实的机器人动作轨迹 (A_r) 和通过模仿生成的对应人类动作 (\hat{A}<em>h)，损失为 (\ell</em>{r2h} = |A_r - A_r^<em>|_2 + |\hat{A}_h - \hat{A}_h^</em>|_2)。</li>
<li>给定人类演示，模型需同时预测真实的人类动作轨迹 (A_h) 和通过模仿生成的对应机器人动作 (\hat{A}<em>r)，损失为 (\ell</em>{h2r} = |A_h - A_h^<em>|_2 + |\hat{A}_r - \hat{A}_r^</em>|_2)。</li>
<li>整体预训练损失为两者的结合：(\mathcal{L} = \ell_{r2h} + \ell_{h2r})。</li>
</ul>
<p><strong>创新点</strong>：与现有直接利用仿真或人类数据的方法相比，MiVLA的创新性体现在：1）提出了一个基于运动学规则的、双向的人-机器人动作空间对齐方法，为跨形态学习提供了基础；2）设计了“互模仿”的预训练范式，强制模型在单一观测下理解并生成两种形态的行为，从而将两种数据源的优势（仿真的操作多样性、人类的行为真实性）内化到一个统一的策略模型中。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真基准</strong>：在RoboTwin-2.0基准的50个双臂协作操作任务中，选取20个代表性任务进行评估，包含“简单”（干净环境）和“困难”（域随机化，如背景变化和干扰物）两种模式。</li>
<li><strong>真实机器人平台</strong>：使用三个异构机器人平台评估：单臂AgileX PiPER、单臂ARX-5和双臂LocoMan。设计了三个任务：“将瓶子移到垫子上”（PiPER）、“整理伞架”（ARX-5）和“收集散落物体”（LocoMan）。</li>
<li><strong>对比基线</strong>：ACT（从头训练）、(\bm{\pi}<em>0)（大规模真实机器人数据预训练）、(\bm{\pi}</em>{0.5})（(\bm{\pi}_0)的改进版，在多样真实家庭任务中进一步预训练）、H-RDT（在EgoDex人类数据上预训练的扩散Transformer）。</li>
<li><strong>评估指标</strong>：成功率（SR）、任务完成度（C）、耗时（T）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2512.15411v2/x5.png" alt="仿真结果"></p>
<blockquote>
<p><strong>图5</strong>：在RoboTwin-2.0仿真基准上20个任务的定量对比结果表。MiVLA在简单和困难模式下的平均成功率（69%， 66%）显著优于所有基线方法。</p>
</blockquote>
<p>如表1（图5）所示，在仿真任务中，MiVLA在简单和困难模式下的平均成功率分别达到69%和66%，远超ACT（9%， 8%）、(\bm{\pi}<em>0)（23%， 25%）、(\bm{\pi}</em>{0.5})（35%， 53%）和H-RDT（36%， 43%）。在困难模式下的优异表现表明MiVLA对环境变化具有更强的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2512.15411v2/x6.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图6</strong>：在三个真实机器人任务上的定量对比结果表。MiVLA取得了有竞争力的平均成功率（55%）和子任务完成率（69%），尤其在双臂复合体LocoMan任务上表现最佳。</p>
</blockquote>
<p>如表2（图6）所示，在真实机器人任务中，MiVLA取得了平均55%的全任务成功率和69%的子任务完成率。虽然(\bm{\pi}_{0.5})在两个单臂任务上表现最佳，但MiVLA在这两个任务上竞争力相当，并且在双臂复合体LocoMan（对预训练模型是未见形态）任务上表现最优。值得注意的是，MiVLA仅使用了约900小时的混合数据（远少于(\bm{\pi})系列超过10，000小时的真实机器人数据），却达到了可比甚至更优的泛化性能。</p>
<p><strong>消融实验</strong>：</p>
<p><img src="https://arxiv.org/html/2512.15411v2/x8.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融研究结果表。展示了逐步加入互模仿预训练目标（(\ell_{h2r}), (\ell_{r2h})）对下游任务性能的影响。</p>
</blockquote>
<p>消融实验（表3， 图8）验证了各组件贡献：</p>
<ol>
<li><strong>从头训练</strong>：性能最差。</li>
<li><strong>仅人类预训练</strong>：仅使用人类数据预训练（无互模仿）带来有限提升。</li>
<li><strong>加入人→机器人模仿损失（(\ell_{h2r})）</strong>：性能进一步提升。</li>
<li><strong>加入完整的互模仿损失（(\ell_{h2r} + \ell_{r2h})）</strong>：性能达到最佳，在仿真和所有真实机器人任务上均获得显著提升。这证明了双向互模仿对于从混合数据中学习通用动作知识至关重要。</li>
</ol>
<p>此外，少样本适应实验（表4）表明，互模仿预训练使模型仅需约20条演示就能快速适应新任务。</p>
<p><img src="https://arxiv.org/html/2512.15411v2/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：MiVLA泛化能力的定性展示。在跨位置（物体初始位置改变）、跨物体（目标物体改变）和跨场景（背景环境改变）三种设置下，MiVLA均能成功完成任务，体现了其强大的泛化能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了MiVLA，一种通过新颖的<strong>人-机器人互模仿预训练</strong>赋能的可泛化机器人模型，成功将人类数据中的真实世界行为先验与仿真机器人数据中的操作多样性整合到统一模型中。</li>
<li>引入了一种基于左右手坐标系对齐运动学规则的<strong>双向人-机器人动作空间转换方法</strong>，为实现有效的跨形态学习奠定了基础。</li>
<li>通过大量实验证明，<strong>无需真实机器人数据</strong>，仅利用仿真和人类数据并配合恰当的预训练方法（互模仿），即可训练出在仿真和真实机器人操控任务上均达到先进泛化性能的VLA。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于人-机器人动作映射机制的准确性，且预训练效果受仿真数据和人类视频数据质量与多样性的影响。</p>
<p><strong>启示</strong>：本研究为突破机器人数据瓶颈提供了一条新路径。它表明，通过精心设计的算法桥接不同形态（仿真-真实、人-机）间的语义和行为鸿沟，能够有效利用海量、易得的非机器人数据（如仿真、人类视频）来训练高性能、高泛化的机器人策略。这对未来探索更多样化的数据源（如动画、文本描述）和更复杂的跨形态迁移学习具有启发意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MiVLA模型，旨在解决视觉-语言-动作模型因相机视角、外观和形态不匹配导致的泛化能力有限问题。方法采用人类-机器人相互模仿预训练，通过运动学规则实现动作空间双向对齐，整合人类真实行为数据和模拟机器人操纵多样性。实验在模拟和真实机器人平台上进行，结果显示MiVLA在模拟任务中性能优于现有先进模型25%，在真实机器人控制任务中优于14%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.15411" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>