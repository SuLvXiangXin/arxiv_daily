<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.21169" target="_blank" rel="noreferrer">2511.21169</a></span>
        <span>作者: Qijun Chen Team</span>
        <span>日期: 2025-11-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人因其类人形态在工业应用中潜力巨大。然而，现有的运动操作方法主要侧重于灵巧操作，无法满足高负载工业场景对灵巧性和主动力交互的综合需求。当前基于强化学习的全身控制方法主要分为两类：整体控制和模块化解耦控制。整体控制方法虽然理论上能生成自然的协调运动，但在高自由度人形机器人上同时处理移动和操作任务时，常面临训练效率低和探索不足的问题。模块化解耦控制将机器人分为上下半身，缓解了高维优化挑战，但仍存在关键局限：上半身通常依赖逆运动学进行末端执行器位姿跟踪，计算量大且难以满足实时要求，其与强化学习框架的不兼容性也限制了上半身在工业环境中的泛化能力；下半身方法主要侧重于被动适应外部扰动以保证稳定移动，而主动对环境施力的能力——这是高负载工业任务的关键需求——却很少被关注；此外，在解耦框架下实现上下半身模块之间的有效协调仍然是一个未解决的难题。</p>
<p>本文针对上述痛点，提出了一个基于强化学习的、具备力交互能力的人形机器人运动操作框架。其核心思路是采用解耦的三阶段训练管道，通过设计嵌入运动学先验的启发式奖励函数加速上半身训练，并利用力基课程学习策略赋予下半身主动施力能力，最后通过增量指令策略实现上下半身在移动过程中的鲁棒协调。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架整体上包含三个策略：上半身策略、下半身策略和增量指令策略，采用三阶段独立训练管道。</p>
<p><img src="https://arxiv.org/html/2511.21169v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提训练管道的系统架构。该图展示了上半身策略、下半身策略和增量指令策略在协调运动操作任务中的集成。当观测数据传播时，实心圆表示对应颜色数据的输出，而空心圆表示未输出的数据。</p>
</blockquote>
<p>整体流程如下：高层指令空间包括世界坐标系下的末端执行器目标位姿、移动速度指令和目标施力。增量指令策略根据世界坐标系下的末端目标位姿和机器人躯干状态，计算出一个在躯干坐标系下的末端位姿补偿偏移量，以抵消移动过程中躯干运动引起的垂直方向扰动。这个经过补偿的位姿作为上半身策略的参考目标。下半身策略则根据速度指令和力指令生成移动动作。上下半身的动作组合后，通过关节级PD控制器转换为电机扭矩，驱动机器人。</p>
<p><strong>上半身策略</strong>：针对高自由度双臂直接进行强化学习训练效率低的问题，本文设计了一种利用运动学先验引导策略收敛的启发式方法。首先，通过算法1采样末端执行器目标位姿，采样过程进行了过滤以确保数据质量，包括末端位置必须在预定义的任务空间内（图2a），以及肘部连杆方向与末端执行器方向之间的夹角必须小于90度（图2b）。</p>
<p><img src="https://arxiv.org/html/2511.21169v1/x2.png" alt="任务空间与约束"></p>
<blockquote>
<p><strong>图2</strong>：上半身的任务空间与施加的约束。(a) 上半身的任务空间。左右末端执行器的任务空间相对于躯干坐标系的x轴对称。绿色长方体说明了可行的位置集合，而黄色锥体表示末端执行器的可行朝向范围，孔径角δ定义了朝向限制。(b) 肘部连杆方向与末端执行器方向之间夹角的示意图。</p>
</blockquote>
<p>基于此，设计了隐式编码运动学信息的启发式奖励函数：$R^{\text{u}} = \alpha^{\text{joint}} \cdot r^{\text{joint}} + \alpha^{\text{pose}} \cdot r^{\text{pose}}$。该函数包含关节空间项$r^{\text{joint}}$（跟踪目标关节角度）和任务空间项$r^{\text{pose}}$（跟踪目标末端位姿）。训练初期，赋予关节空间项更高权重，利用奖励函数中嵌入的正运动学信息（通过目标关节角度计算得到的目标末端位姿）高效地将智能体引导至目标位姿附近；随着策略开始收敛，逐渐提高任务空间项的权重，鼓励智能体细化其末端轨迹，实现精确的任务空间跟踪。</p>
<p><strong>下半身策略</strong>：与现有方法不同，本文的下半身训练集成了力交互能力。在目标空间$\mathcal{G}_t^{\text{l}}$中引入了额外的力目标$f_t^*$，其方向与机器人线速度相反。在仿真中，会根据牛顿第三定律对机器人躯干施加一个大小相等、方向相反的力。为了应对持续施力对移动训练带来的挑战，采用了课程学习策略，让智能体顺序掌握移动和施力技能。力的大小$f_t^*$根据公式(10)进行采样，以平衡零力、随机力和最大力三种情况。课程进度由一组评估指标$\mathcal{C}$（包括线速度跟踪、角速度跟踪、移动距离和施力能力）控制，只有当所有指标都满足条件时，才会提升课程难度（如增加最大力$f_t^{\max}$）。</p>
<p><strong>增量指令策略</strong>：该策略是协调上下半身的关键。由于下半身移动导致躯干在垂直方向（世界坐标系z轴）发生位移，这会使世界坐标系下的末端目标位姿在躯干坐标系中发生偏移。增量指令策略学习根据躯干状态和世界坐标系下的末端目标位姿，预测一个在躯干坐标系下的位姿补偿量（$\Delta \mathbf{x}^{\text{torso}}$），从而确保上半身策略接收到的参考位姿是稳定的，实现了在解耦控制下的运动协调。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台为Unitree G1人形机器人（29个驱动关节）。在仿真和真实世界中进行大量实验验证框架有效性。Baseline对比包括：无采样策略的下半身训练、无课程学习的下半身训练、以及不使用增量指令策略的协调性能。</p>
<p><img src="https://arxiv.org/html/2511.21169v1/x3.png" alt="训练结果对比"></p>
<blockquote>
<p><strong>图3</strong>：使用与不使用采样策略的训练结果对比。两者均使用课程学习，但只有(a)包含了采样策略。绿色箭头表示目标速度，蓝色箭头表示实际速度。目标线速度设为0.7 m/s，目标力为0 N。没有采样策略时，策略倾向于向高力场景收敛，导致在低力条件下移动性能下降。相比之下，采用采样策略训练的智能体展现出对不同力水平更好的泛化能力。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ol>
<li><strong>下半身力能力验证</strong>：训练后的策略能够主动施加高达150N的力。在目标速度为0.5 m/s时，施加100N力的情况下，实际速度能保持在0.45 m/s以上，速度跟踪误差在可接受范围内。</li>
<li><strong>消融实验</strong>：证明了各核心组件的必要性。<ul>
<li>力采样策略（图3）：对于泛化到不同力指令至关重要。</li>
<li>课程学习：是获得高力施放能力的关键，直接训练难以收敛到高力水平。</li>
<li>增量指令策略：显著提高了上半身末端执行器在世界坐标系中的高度稳定性。不使用该策略时，末端高度标准差为5.7厘米；使用后，标准差降低至1.9厘米。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.21169v1/x4.png" alt="消融实验：增量指令"></p>
<blockquote>
<p><strong>图4</strong>：增量指令策略的消融研究。展示了在移动过程中，世界坐标系下末端执行器高度的标准偏差。使用增量指令策略（蓝色）显著提高了高度稳定性。</p>
</blockquote>
<ol start="3">
<li><strong>整体任务性能（实物实验）</strong>：<ul>
<li><strong>负重行走</strong>：成功实现了在平地搬运4公斤物体（哑铃）行走。</li>
<li><strong>推/拉小车</strong>：成功推拉总负载达112.8公斤的小车。在推车任务中，机器人施加的力约为68N；在拉车任务中，施加的力约为98N。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.21169v1/draw_TJ_fancy.png" alt="实物实验：推拉小车"></p>
<blockquote>
<p><strong>图9</strong>：在真实Unitree G1人形机器人上进行的推拉小车实验。机器人成功推拉总负载为112.8公斤的小车，展示了其高负载运动操作能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>上半身高效训练</strong>：设计了一种通过隐式嵌入正运动学先验的启发式奖励函数，显著提升了高自由度上半身策略的训练效率和性能。</li>
<li><strong>下半身主动施力</strong>：提出了一种力基课程学习方法，赋予了下半身策略主动对环境施加和调节作用力的能力，突破了以往仅能被动适应的局限。</li>
<li><strong>全身协调控制</strong>：通过引入增量指令策略，有效解决了在模块化解耦控制框架下，因下半身移动导致的上半身任务空间扰动问题，实现了鲁棒的全身协调。</li>
</ol>
<p>论文提到的局限性包括：框架依赖于从仿真到实物的策略迁移，在更复杂的动态环境中可能面临挑战；增量指令策略是针对特定任务空间（垂直位移）设计的，对于更普遍的协调问题可能需要更通用的机制。</p>
<p>本文对后续研究的启示在于：为高负载、强交互的人形机器人工业应用提供了一种有效的模块化强化学习解决方案；其力基课程学习和运动学先验嵌入的思想，可推广至其他需要精细操作与强力交互结合的机器人任务；如何设计更通用、自适应更强的模块间协调机制，是未来值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在高负载工业场景中需同时具备灵巧操作和主动力交互能力的挑战，提出一种基于强化学习的解耦三阶段训练框架。该框架包含上半身策略、下半身策略和增量命令策略：上半身策略通过嵌入前向运动学先验的启发式奖励函数加速收敛；下半身策略采用基于力的课程学习实现主动力调节；增量命令策略抵消下半身运动引起的末端位移以保障全身协调。在Unitree G1机器人上的实验表明，该方法能完成携带4公斤物体行走、推动总负载112.8公斤推车等高负载任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.21169" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>