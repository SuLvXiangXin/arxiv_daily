<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning human-to-robot handovers through 3D scene reconstruction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning human-to-robot handovers through 3D scene reconstruction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.08726" target="_blank" rel="noreferrer">2507.08726</a></span>
        <span>作者: Changjae Oh Team</span>
        <span>日期: 2025-07-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从原始真实图像数据学习机器人操作策略通常需要在物理环境中进行大量机器人动作试验。虽然基于仿真的训练是一种经济高效的替代方案，但仿真与机器人工作空间之间的视觉域差距仍然是一个主要限制。近期，高斯泼溅（Gaussian Splatting）视觉重建方法为机器人操作提供了新方向，能够生成逼真的环境。然而，现有基于GS的方法多用于构建支持强化学习的场景，不关注人机交接任务，且仍面临现实差距问题。模仿学习方法通常依赖大规模、高质量的真实机器人专家演示，成本高昂且易受噪声影响。手眼协调方法则常依赖仿真图像或需要大量真实机器人数据集，且难以在真实世界执行抓取任务。</p>
<p>本文针对上述痛点，提出了一种全新的视角：无需真实机器人训练或真实数据收集，仅从RGB图像学习人机交接策略。核心思路是，利用稀疏视图高斯泼溅重建人机交接场景，在重建的3D场景中生成包含图像-动作对的机器人演示数据，从而将虚拟相机姿态变化直接映射为真实机器人夹爪的姿态变化，并训练一个可直接部署到真实环境的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架如图2所示，包含四个主要阶段：1) 给定稀疏视图的RGB-D交接图像，使用高斯泼溅（GS）重建3D场景，并从GS场景中提取物体和手部点云以估计抓取姿态。2) 利用GS场景和抓取姿态，生成夹爪朝向预抓取姿态的轨迹，并在每个采样姿态渲染手眼图像。3) 每条轨迹成为一个交接演示数据集，包含手眼图像、物体和手部掩码、夹爪姿态变换以及预抓取姿态标签。4) 使用该数据集训练交接策略。推理时仅需手眼RGB图像和掩码。</p>
<p><img src="https://arxiv.org/html/2507.08726v1/extracted/6615802/a2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：方法整体框架。 (a) 使用稀疏视图RGB-D图像通过高斯泼溅重建3D场景并估计抓取姿态。(b) 利用GS场景和抓取姿态生成夹爪轨迹及每个姿态的手眼图像。(c) 每条轨迹构成一个包含图像、掩码、变换和标签的演示数据集。(d) 使用数据集训练交接策略。</p>
</blockquote>
<p>核心模块之一是<strong>抓取姿态估计</strong>。由于GraspNet使用公制尺度进行预测，需要点云具有公制尺度以实现与抓取姿态的正确对齐，并使重建场景中的相机变换能直接转换为真实环境中机器人夹爪的变换。因此，方法使用FSGS从稀疏视图RGB-D图像重建3D场景，利用深度图初始化点云以获得公制尺度和高质量重建。随后，从数据集中提取带标注的物体点云 (P_o) 和手部点云 (P_h)，输入GraspNet生成抓取候选，选择得分最高的作为抓取姿态 (G_o \in SE(3))。为确保安全，通过将手部点云变换到抓取坐标系并计算欧氏距离来检查抓取姿态是否会与手部碰撞，若手部点位于安全阈值 (d_s) 内则视为不安全并移除。最后，通过 (G_{GS} = G_o - \text{Mean}_o) 将估计的抓取姿态对齐到重建的3D场景坐标系。</p>
<p><img src="https://arxiv.org/html/2507.08726v1/extracted/6615802/b2.png" alt="采样轨迹示例"></p>
<blockquote>
<p><strong>图3</strong>：从初始姿态到预抓取姿态的采样轨迹示例。夹爪轨迹颜色从深红到黄色表示动作的时间演化。蓝色点云代表手和物体。点云上的黄色夹爪指示抓取姿态。</p>
</blockquote>
<p>核心模块之二是<strong>交接演示数据集构建</strong>。对于每个目标抓取姿态 (G_{GS})，首先在抓取姿态周围半径为 (r) 的球面上采样 (k) 个初始姿态 (q_i \in SE(3))。为确保轨迹合理性，应用一系列过滤机制：手部位置 (T_h) 与初始相机位置 (T_i) 需在物体中心 (T_o) 的两侧（公式4）；为增加多样性，初始姿态的z轴方向 (z_i) 在面向物体的基础上加入随机角偏移（公式5）；初始采样点与最终抓取点的z轴方向夹角不超过最大角 (\theta_{\text{max}})（公式6）。然后，为每个初始姿态生成一条朝向预抓取姿态 (T_{\text{pre}}) 的轨迹（(T_{\text{pre}}) 位于抓取姿态沿其z轴反方向 (s) 米处）。轨迹生成分为三个阶段：第一阶段调整姿态使夹爪的接近方向与物体对齐；第二阶段仅线性插值调整位置，直到与预抓取姿态的距离小于阈值 (d)；第三阶段同时使用线性插值（位置）和球面线性插值SLERP（旋转）来精调至预抓取姿态（公式10-11）。对于轨迹上的每个时间步，渲染手眼图像 (I)，并记录手和物体掩码 (M)、相机目标6D变换 (T \in \mathbb{R}^6)，以及一个二元标签 (Cls \in {0,1}) 指示当前是否为预抓取姿态，从而构成数据集元组 ((I, M, T, Cls))。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) 首次提出完全从RGB图像（无需真实机器人数据）学习人机交接的监督学习方法。2) 利用稀疏视图GS重建生成逼真的演示数据，完全避免了昂贵且可能不安全的真实机器人数据收集。3) 通过公制尺度重建和姿态对齐，实现了从仿真（GS场景）到真实环境的直接策略部署，无需额外的域适应或校准。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了两个主要部分进行评估：1) 在<strong>高斯泼溅重建场景</strong>中进行模拟评估，使用16个家用物体的稀疏视图RGB-D图像重建场景并生成测试轨迹。2) 在<strong>真实世界人机交接环境</strong>中部署训练好的策略进行实际机器人测试。实验平台涉及机器人操作系统（ROS）和Franka Emika Panda机器人。</p>
<p>对比的基线方法包括：<strong>OHPL</strong>（一种仅使用手眼相机图像学习到达策略的方法，无需校准，但不能执行抓取）和<strong>HAN</strong>（一种在仿真中通过人类遥操作演示学习手眼协调行为的方法，未在真实机器人上实现）。</p>
<p><img src="https://arxiv.org/html/2507.08726v1/extracted/6615802/c2.png" alt="GS场景与真实环境渲染对比"></p>
<blockquote>
<p><strong>图4</strong>：GS重建场景（上排）与真实机器人环境（下排）的手眼图像对比。虚拟相机在GS场景中的移动可以模拟真实机器人手臂的运动。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08726v1/extracted/6615802/d2.png" alt="GS场景评估结果"></p>
<blockquote>
<p><strong>图5</strong>：在GS重建场景中评估策略性能。左图：不同方法在“已见”和“未见”物体上的成功率。H2RH-SGS在已见物体上达到95.6%的成功率，在未见物体上达到93.3%，显著优于基线。右图：消融研究，展示了移除各组件（抓取姿态估计-GPE、轨迹生成-TG、安全约束-SC、预抓取分类-PC）对性能的影响。</p>
</blockquote>
<p>关键实验结果：在GS重建场景的评估中，H2RH-SGS在已见物体上达到<strong>95.6%</strong> 的成功率，在未见物体上达到<strong>93.3%</strong> 的成功率，显著优于OHPL（已见：73.3%，未见：70.0%）和HAN（已见：86.7%，未见：83.3%）。消融实验表明，抓取姿态估计（GPE）、轨迹生成（TG）、安全约束（SC）和预抓取分类（PC）每个组件都对最终性能有重要贡献，移除任一组件都会导致成功率下降。</p>
<p><img src="https://arxiv.org/html/2507.08726v1/extracted/6615802/setup.png" alt="真实机器人实验设置"></p>
<blockquote>
<p><strong>图6</strong>：真实世界人机交接实验设置。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08726v1/extracted/6615802/sample-1.png" alt="真实交接定性结果1"></p>
<blockquote>
<p><strong>图7</strong>：真实人机交接任务的定性结果示例（序列1）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08726v1/extracted/6615802/sample-2.png" alt="真实交接定性结果2"></p>
<blockquote>
<p><strong>图8</strong>：真实人机交接任务的定性结果示例（序列2）。</p>
</blockquote>
<p>在真实世界人机交接实验中，H2RH-SGS在16个测试物体上取得了<strong>87.5%</strong> 的整体成功率（14/16成功）。论文展示了完整的交接序列（图7，图8），包括机器人接近、调整姿态、到达预抓取位置、最终抓取并完成交接的过程。</p>
<p><img src="https://arxiv.org/html/2507.08726v1/extracted/6615802/render-seen.png" alt="GS场景渲染图像（已见物体）"></p>
<blockquote>
<p><strong>图9</strong>：对“已见”物体进行GS场景重建和渲染的示例图像。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.08726v1/extracted/6615802/render-unseen.png" alt="GS场景渲染图像（未见物体）"></p>
<blockquote>
<p><strong>图10</strong>：对“未见”物体进行GS场景重建和渲染的示例图像。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>第一个仅从RGB图像学习人机交接任务的方法</strong>（H2RH-SGS），无需真实机器人训练、人类演示或仿真到真实的域适应。2) 创新性地利用<strong>稀疏视图高斯泼溅重建</strong>来生成高质量、逼真的机器人演示数据，有效弥合了视觉域差距。3) 通过公制尺度重建和坐标对齐，实现了<strong>从仿真（GS场景）到真实机器人环境的策略直接部署</strong>，并在真实实验中验证了其有效性。</p>
<p>论文自身提到的局限性包括：1) 方法性能依赖于准确的抓取姿态估计，不准确的抓取估计可能导致失败。2) 当前框架假设物体在交接过程中姿态基本固定，物体姿态的显著变化可能影响策略性能。</p>
<p>这项工作为机器人学习开辟了新的途径，启示后续研究可以探索：如何结合在线适应机制以处理动态变化的环境；如何进一步减少对精确抓取先验知识的依赖；以及如何将类似框架扩展到更复杂的交互任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决了人机交接任务中，从仿真到真实环境的视觉域差距问题。提出了H2RH-SGS方法，其核心是利用**稀疏视图高斯泼溅**进行3D场景重建，从RGB图像生成逼真的交接场景模拟数据，并将虚拟相机位姿变化直接映射为真实机器人夹爪的运动。实验表明，该方法仅使用16个日常物体的重建数据训练策略，便可**直接部署到真实机器人**上完成交接任务，验证了其作为人机交接任务新表征的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.08726" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>