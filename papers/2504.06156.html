<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free Visuo-Tactile Manipulation Interface - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free Visuo-Tactile Manipulation Interface</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.06156" target="_blank" rel="noreferrer">2504.06156</a></span>
        <span>作者: Liu, Fangchen, Li, Chuanyu, Qin, Yihua, Xu, Jing, Abbeel, Pieter, Chen, Rui</span>
        <span>日期: 2025/04/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习通常依赖于从示教数据集中学习，而高质量示教数据的收集多通过遥操作系统完成，这需要实时操控物理机器人，过程耗时且缺乏灵活性。便携式数据采集设备（如UMI）提供了一种无需遥操作、可扩展且低成本的替代方案，但其主要依赖纯视觉感知，在需要精细触觉反馈的接触密集型灵巧操作任务中存在局限。本文旨在解决高效数据收集和学习接触密集型任务的双重挑战，提出了一种新型的“具身无关”操作界面ViTaMIn。其核心思路是：设计一个集成视觉与全向触觉感知的手持夹爪，实现无需遥操作的多模态数据采集，并利用一种多模态表征学习策略对触觉信息进行预训练，以提升策略的数据效率与鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViTaMIn系统包含硬件数据采集界面和后续的多模态策略学习算法。</p>
<p><img src="https://arxiv.org/html/2504.06156v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ViTaMIn硬件系统概览。手持设备集成了GoPro相机、两个触觉传感器和一个用于同步的相机。左侧为系统侧视图，右侧为移除背盒盖的俯视图，展示了内部的树莓派。</p>
</blockquote>
<p><strong>硬件系统</strong>：如图1所示，核心是一个手持夹爪，包含：1）一个155°视场的GoPro 10鱼眼腕部相机，用于视觉观察（60 FPS）；2）两个AllTact手指，这是一种具有全向触觉感知能力的柔顺Fin Ray夹爪，其内置相机可捕捉手指整体形变和接触面局部形变作为触觉图像（30 FPS）；3）一个连接到树莓派5的同步相机，用于对齐视觉和触觉信息的时间戳；4）树莓派与电池用于数据记录。系统总重约1960克。</p>
<p><strong>数据处理</strong>：通过同步相机捕获的ArUco标记序列，将GoPro视频与触觉传感器视频同步，时间对齐误差小于0.05秒。随后采用与UMI类似的流程，利用同步定位与地图构建（SLAM）从视频中估计末端执行器轨迹，成功率约80%，成功轨迹用于模仿学习。</p>
<p><strong>多模态表征学习</strong>：由于触觉图像与CLIP模型的训练分布差异巨大，直接使用CLIP编码触觉效果不佳。本文提出一种掩码对比学习策略来预训练触觉编码器，该预训练可使用所有采集到的数据（无需动作标签，不依赖SLAM成功）。</p>
<p><img src="https://arxiv.org/html/2504.06156v2/x2.png" alt="预训练框架"></p>
<blockquote>
<p><strong>图2</strong>：多模态对比表征预训练示意图。训练触觉编码器捕捉补充信息，以预测未来图像中被掩码遮挡的内容。</p>
</blockquote>
<p>具体流程如图2所示：给定第k步的掩码后视觉图像 \(\tilde{I}<em>{V}^{k}\) 和完整的触觉观测 \(I</em>{T}^{k}\)，目标是让两者的融合特征与第k+1步的完整视觉图像 \(I_{V}^{k+1}\) 在CLIP嵌入空间中对齐。冻结预训练的CLIP视觉编码器 \(\phi_{V}\)，仅训练触觉编码器 \(\phi_{T}\)。将触觉嵌入 \(T_{k} = \phi_{T}(I_{T}^{k})\) 与视觉嵌入 \(V_{k} = \phi_{V}(\tilde{I}<em>{V}^{k})\) 拼接，并通过一个全连接投影层映射回512维CLIP空间，得到融合特征 \(F</em>{k}\)。使用标准的CLIP对比损失（公式1-3）来训练 \(F_{k}\) 和 \(V_{k+1}\) 之间的对齐。这种设计迫使触觉编码器专注于学习接触信息，以基于当前被破坏的视觉图像来预测未来图像，从而学习更具表达力的触觉表征。</p>
<p><strong>策略学习</strong>：预训练完成后，在SLAM过滤后的成功轨迹数据上训练一个Diffusion Policy。采用U-Net作为噪声预测网络，并使用DDIM加速推理以预测动作序列。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实机器人（Rokae xMate ER3PRO机械臂 + PGI平行夹爪）上部署策略，控制频率为10Hz。评估了五个接触密集型操作任务（图4）：1）<strong>橙子放置</strong>：将易碎橙子放到随机位置的盘子；2）<strong>动态插销插入</strong>：将销钉插入以10mm/s匀速移动的孔中；3）<strong>试管重定向</strong>：从架子上抓取透明试管，并基于触觉反馈调整其姿态；4）<strong>剪刀悬挂</strong>：抓取剪刀并挂到钩子上，可重复尝试直至成功；5）<strong>双臂刀片拉出</strong>：双臂协作，一臂抓刀并摆平，另一臂执行受约束的平移运动将其拉出。</p>
<p><img src="https://arxiv.org/html/2504.06156v2/x4.png" alt="任务示意图"></p>
<blockquote>
<p><strong>图4</strong>：用于评估ViTaMIn的五个接触密集型操作任务示意图。</p>
</blockquote>
<p><strong>对比方法</strong>：1) <strong>Vision</strong>：仅使用视觉观测（同UMI）；2) <strong>Ours w/o Pre-training</strong>：简单拼接视觉和触觉的CLIP编码特征，然后进行行为克隆微调；3) <strong>Ours</strong>：完整方法（使用预训练的触觉编码器）。</p>
<p><strong>主要结果</strong>：如表II所示，在五个任务上各进行20次随机初始条件的试验。纯视觉策略在所有任务中表现最差，尤其在依赖触觉的试管重定向（成功率0.4）和剪刀悬挂（成功率0.1）任务上。加入触觉但不经预训练的基线（Ours w/o Pre-training）性能有显著提升。完整的ViTaMIn方法（Ours）取得了最佳性能，在橙子放置、动态插销插入和双臂刀片拉出任务上达到0.9或1.0的成功率，在剪刀悬挂任务上将成功率从0.1提升至0.7。</p>
<p><img src="https://arxiv.org/html/2504.06156v2/x3.png" alt="部署硬件"></p>
<blockquote>
<p><strong>图3</strong>：策略部署的硬件设置，展示了机械臂与夹爪。</p>
</blockquote>
<p><strong>柔顺操作验证</strong>：如图5所示，设计了一个开关翻转任务，要求机器人在旋转手柄90度时最小化轴向力。实验表明，ViTaMIn策略产生的平均力显著低于纯视觉策略，证明了其柔顺操作能力。</p>
<p><img src="https://arxiv.org/html/2504.06156v2/x5.png" alt="柔顺操作实验"></p>
<blockquote>
<p><strong>图5</strong>：柔顺铰接物体操作任务。机器人需翻转开关，并在旋转过程中最小化轴向力。</p>
</blockquote>
<p><strong>消融实验 - 数据效率</strong>：如图7所示，评估了使用不同比例（25%， 50%， 100%）示教数据训练的策略性能。在所有数据量下，经过预训练的方法（Ours）均能取得比未预训练方法（Ours w/o Pre-training）更高的成功率。即使在仅使用25%数据的情况下，预训练方法也能在试管重定向等任务上达到较高的成功率。</p>
<p><img src="https://arxiv.org/html/2504.06156v2/x7.png" alt="数据效率消融"></p>
<blockquote>
<p><strong>图7</strong>：预训练对数据效率影响的消融研究。使用预训练后，即使在有限数据（25%）下也能获得高成功率。</p>
</blockquote>
<p><strong>消融实验 - 训练效率</strong>：如图8所示，评估了不同训练轮数下的策略性能。经过预训练的模型能在极早期（10个epoch内）就学会完成任务的第一阶段，且整体成功率随训练轮数增长更快。</p>
<p><img src="https://arxiv.org/html/2504.06156v2/x8.png" alt="训练效率消融"></p>
<blockquote>
<p><strong>图8</strong>：预训练对训练效率影响的消融研究。预训练策略在训练早期（10个epoch内）即可学会完成第一阶段任务。</p>
</blockquote>
<p><strong>泛化能力</strong>：如图6所示，测试了策略对未见物体和不同光照条件的泛化能力。在橙子放置和剪刀悬挂任务中，使用新物体（6种新小物体和3种新剪刀）和改变光照（增强亮度、彩色闪光灯）。如表III所示，完整方法（Ours）在泛化设置下保持了最佳性能，例如在橙子放置任务中，面对新物体和不同光照时成功率分别达到1.0和0.85，显著优于基线。</p>
<p><img src="https://arxiv.org/html/2504.06156v2/x6.png" alt="泛化设置"></p>
<blockquote>
<p><strong>图6</strong>：泛化任务中展示的新物体和不同光照条件。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了ViTaMIn，一个便携、可扩展的“具身无关”视觉-触觉操作数据采集接口，无需遥操作即可收集高质量多模态示教；2）提出了一种有效的多模态表征学习策略，通过掩码对比预训练学习鲁棒且可泛化的触觉表征，显著提升了策略的数据效率、训练效率和泛化能力；3）在五个真实的接触密集型操作任务上验证了系统的优越性，相比纯视觉基线有大幅性能提升。</p>
<p>论文提到的局限性在于，当前工作主要聚焦于使用平行夹爪的固定基座单臂/双臂任务。尽管适用于广泛的操作任务，但未来工作可扩展至灵巧手，以实现更丰富、更接近人类灵巧度的操作技能。</p>
<p>该研究为高效收集接触密集型任务的多模态数据提供了新范式，其“掩码当前视觉以预测未来”的预训练目标，为融合异质模态（视觉、触觉）的表征学习提供了新颖且有效的思路，对推动需要精细物理交互的机器人模仿学习具有重要启示。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对接触丰富的灵巧操作任务，提出了一种无需机器人实体和遥操作的视觉-触觉数据采集与学习方法ViTaMIn。核心设计是集成视觉与定制触觉传感的顺从型Fin Ray夹爪，支持手持操作并感知多方向接触力，使数据采集更直观高效。同时，采用多模态表征学习策略预训练触觉表示，以提升数据利用效率和策略鲁棒性。在5个接触密集型操作任务上的实验表明，该系统在可扩展性、效率和效果上均优于基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.06156" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>