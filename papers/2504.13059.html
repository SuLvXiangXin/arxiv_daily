<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.13059" target="_blank" rel="noreferrer">2504.13059</a></span>
        <span>作者: Mu, Yao, Chen, Tianxing, Chen, Zanxin, Peng, Shijia, Lan, Zhiqian, Gao, Zeyu, Liang, Zhixuan, Yu, Qiaojun, Zou, Yude, Xu, Mingkun, Lin, Lunkai, Xie, Zhiqiang, Ding, Mingyu, Luo, Ping</span>
        <span>日期: 2025/04/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，具备复杂双臂协调与灵巧操作能力的机器人系统在多个领域至关重要。然而，开发此类系统面临一个主要瓶颈：缺乏多样化、高质量的训练数据以及与真实世界对齐的综合评估基准。传统方法如人工遥操作能提供高质量演示，但成本高昂、耗时且难以覆盖多样场景。基于仿真的算法轨迹生成方法则通常需要针对特定任务进行设计，泛化性和可扩展性受限。近期工作（如MimicGen, RoboCaca）能从有限人类演示中生成大规模仿真专家数据，但其场景设置固定，难以处理预定义配置之外的任务变体。此外，现有基准大多关注单臂任务或双臂分离的任务，未能捕捉集成双臂系统固有的复杂性与协调需求。</p>
<p>本文针对“双臂协作任务缺乏高效、可扩展的多样化数据生成与标准化评估平台”这一痛点，提出了利用生成式数字孪生的新视角。核心思路是：利用3D生成基础模型和大语言模型，仅从单张2D图像出发，自动生成多样化的3D物体资产和空间感知的专家演示代码，从而构建一个包含仿真与真实数据、支持标准化评估的双臂机器人基准。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboTwin的整体框架是一个生成式数字孪生系统，旨在从真实世界的单张图像生成多样化的仿真训练场景和专家演示数据。</p>
<p><img src="https://robotwin-benchmark.github.io" alt="RoboTwin Benchmark"></p>
<blockquote>
<p><strong>图1</strong>：RoboTwin基准框架。该框架利用生成式基础模型创建逼真、交互式的训练场景，并为双臂机器人操作生成多样化的专家演示。</p>
</blockquote>
<p>整个pipeline分为三个核心模块：</p>
<ol>
<li><p><strong>多样化数字资产生成</strong>：从单张2D RGB图像出发，利用GPT-4V生成物体描述，并通过语言模型修改描述以创造视觉上不同的变体。使用SDXL-Turbo根据这些描述生成多样的2D图像，再通过图像条件的3D生成模型（如Deemos Rodin平台）生成具有详细几何、法线、纹理的3D模型。通过UCLIP-I相似性度量和GPT-4V视觉验证来确保资产质量，并利用GPT-4V分类材质以赋予物理参数。</p>
</li>
<li><p><strong>3D资产的空间标注框架</strong>：为了支持基于空间关系的代码生成，为生成的3D工具类资产标注关键点和轴。</p>
<ul>
<li><strong>关键点</strong>：包括<strong>功能点</strong>（如锤头的打击面）和<strong>接触点</strong>（如抓握点）。</li>
<li><strong>轴</strong>：包括<strong>功能轴</strong>（工具执行主要功能的方向）、<strong>接近轴</strong>（工具接近目标物体的方向）和<strong>横向轴</strong>（垂直于功能轴和接近轴，构成三维坐标系）。<br>对于同一物体类别的不同3D模型，采用基于Stable Diffusion编码器的特征点匹配技术，将源模型上的标注点迁移到目标模型上，避免重复标注。</li>
</ul>
</li>
</ol>
<p><img src="https://hyperhuman.deemos.com/rodin" alt="Examples of spatial annotations"></p>
<blockquote>
<p><strong>图3</strong>：空间标注示例。为工具提取功能点、接触点以及功能轴、接近轴、横向轴，以实现空间和几何感知的操作与代码生成。</p>
</blockquote>
<ol start="3">
<li><strong>专家数据生成（空间关系感知的代码生成）</strong>：这是方法的核心创新模块。基于空间标注，利用大语言模型（LLM）自动生成可执行的机器人行为代码。流程如下：<ul>
<li><strong>场景初始化</strong>：设置物体及其初始位姿。</li>
<li><strong>任务分解</strong>：LLM根据人类任务描述（如“用锤子敲击木块”）将其分解为子任务（如抓起锤子、移动到木块、敲击）。</li>
<li><strong>约束推断</strong>：对于每个子任务，LLM分析物体关键点与轴之间的功能关系，推断出空间约束（例如，对于敲击子任务，锤头的功能点需要与木块表面对齐，锤子的接近轴需要平行于桌面）。</li>
<li><strong>机器人行为生成</strong>：LLM根据推断的约束，调用预定义API生成代码。代码的核心是求解一个轨迹优化问题，计算满足约束的关节轨迹θ(t)。其数学形式化如下：<br>  min θ(t) J(θ(t))<br>  s.t.<br>  T_ee = f_FK(θ(t)) (运动学约束)<br>  P_ee = P_o - d·a_o (位置对齐：末端执行器位置相对于物体接触点)<br>  n_ee = a_o (方向对齐：末端执行器方向与物体接近轴对齐)<br>  θ(t) ∈ C, ∀t ∈ [t0, tf] (碰撞避免)<br>  其中，J(θ(t))为成本函数，f_FK为前向运动学函数，P_o和a_o分别为物体的接触点位置和接近轴方向向量，C为无碰撞构型空间。</li>
<li><strong>成功评估与迭代 refinement</strong>：系统根据任务目标（如钉子被打入的深度）评估执行是否成功。若失败，收集运行时错误、规划失败信息、最终状态与目标状态的偏差等数据，反馈给LLM以重新生成改进的代码，循环直至成功。</li>
</ul>
</li>
</ol>
<p><img src="https://global.agilex.ai/products/cobot-magic" alt="Real-to-simulation transfer and expert data generation"></p>
<blockquote>
<p><strong>图2</strong>：真实到仿真的迁移与专家数据生成流程。首先利用3D生成基础模型从2D图像创建多样化3D资产；然后实施空间标注框架；最后利用LLM通过任务分解、约束推断和关键位姿计算来生成专家演示。</p>
</blockquote>
<p>与现有方法相比，RoboTwin的创新点在于：1) 构建了仅需单张RGB图像即可生成多样化3D模型的便捷真实到仿真流程；2) 提出了结合物体空间标注与LLM的、能够处理复杂空间约束（特别是功能轴与场景关系）的自动化代码生成框架，专门面向双臂协调与碰撞避免；3) 建立了同时包含仿真生成数据与真实遥操作数据的标准化双臂操作基准。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>基准与平台</strong>：研究基于自建的RoboTwin基准进行验证，该基准包含15个双臂操作任务，底层物理引擎为ManiSkill3。仿真与真实实验均采用开源的Cobot Magic机器人平台（图4），该平台拥有四个机械臂和四个Intel RealSense D435 RGBD相机。每个任务提供了100组仿真数据和20组真实世界数据，数据格式包括多视角RGB-D图像、点云以及机械臂关节与末端执行器的位姿。</p>
<p><img src="https://global.agilex.ai/products/cobot-magic" alt="Illustration of our robot platform"></p>
<blockquote>
<p><strong>图4</strong>：机器人平台示意图，具备遥操作与数据采集能力。</p>
</blockquote>
<p><strong>基线方法</strong>：实验对比了两种基于扩散模型的模仿学习方法：<strong>2D Diffusion Policy (DP)</strong> 和 **3D Diffusion Policy (DP3)**。DP3又评估了仅使用点云几何信息（XYZ）和结合颜色信息（XYZ+RGB）两种输入形式。使用20、50、100组专家演示分别进行训练和评估。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>生成代码的成功率</strong>：首先评估了RoboTwin框架自身生成的专家代码在15个任务上的成功率，如图5所示。结果表明，该方法能够为大多数任务生成高成功率的演示代码，验证了数据生成流程的有效性。</li>
</ol>
<p><img src="https://global.agilex.ai/products/cobot-magic" alt="Success rate of the generated code for RoboTwin benchmark"></p>
<blockquote>
<p><strong>图5</strong>：RoboTwin基准中生成代码的成功率。展示了所提方法为各任务自动生成演示代码的可靠性。</p>
</blockquote>
<ol start="2">
<li><p><strong>策略学习性能对比</strong>：主要实验结果如表1所示（表1在文本中以表格形式描述，此处根据要求用文字总结）。关键发现包括：</p>
<ul>
<li><strong>DP3的少样本学习优势</strong>：在仅使用20个演示的少样本设置下，DP3（XYZ）在多个任务上显著优于DP。例如，在“Block Hammer Beat”任务中，DP3（XYZ）成功率为55.7%，而DP为0%。</li>
<li><strong>输入模态的影响</strong>：对于不同的任务，最佳输入模态不同。在“Block Hammer Beat”和“Container Place”等任务中，仅使用几何信息（XYZ）的DP3表现更好；而在“Block Handover”等任务中，结合颜色信息（XYZ+RGB）更有帮助。</li>
<li><strong>双臂任务的有效性</strong>：在“Dual Bottles Pick (Easy)”等需要双臂协调的任务上，DP3也取得了良好性能（使用100个演示时达85.7%），证明了基准和生成数据的价值。</li>
</ul>
</li>
<li><p><strong>仿真到真实的迁移潜力</strong>：论文核心验证了利用RoboTwin生成数据进行预训练，再用少量真实数据微调的策略的有效性。实验表明，在300个RoboTwin生成的仿真样本上预训练，再用20个真实世界样本微调的策略，相比仅用20个真实世界样本训练的策略，在单臂任务（如锤击）上成功率提升超过70%，在双臂协调任务（如清扫球）上提升超过40%。</p>
</li>
</ol>
<p><img src="https://global.agilex.ai/products/cobot-magic" alt="Examples of task execution in the RoboTwin benchmark"></p>
<blockquote>
<p><strong>图6</strong>：RoboTwin基准中的任务执行示例。包括多样瓶子抓取、鞋子放置、木块清扫、苹果柜存储等任务，展示了双臂协调的不同场景。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>便捷的真实到仿真流程</strong>：建立了仅需单张2D图像即可通过3D生成基础模型自动创建多样化、物理可信的3D数字孪生资产的流程。</li>
<li><strong>空间感知的自动化代码生成框架</strong>：提出了一个结合物体空间标注（关键点与轴）与大语言模型的框架，能够理解复杂空间关系并自动生成满足运动学和碰撞约束的双臂机器人演示代码。</li>
<li><strong>标准化双臂操作基准</strong>：开发了RoboTwin基准，提供15个任务，并同时包含高质量的仿真生成数据与真实世界遥操作数据，支持仿真与真实性能对齐的标准化评估。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，其方法依赖于3D生成基础模型和大语言模型的质量与能力。生成资产的物理保真度、代码生成的成功率以及迭代优化过程都可能受到这些上游模型性能的限制。此外，整个数据生成和代码优化流程可能涉及较高的计算成本。</p>
<p><strong>后续研究启示</strong>：RoboTwin展示了生成式AI（尤其是3D生成模型和LLM）在自动化机器人数据生成与任务规划方面的巨大潜力。它为未来研究指明了方向：1) 探索更高效、更鲁棒的基于生成模型的数据合成方法；2) 开发更精细的空间关系表示与推理机制，以处理更复杂的多物体、动态场景任务；3) 推动构建更多样化、更复杂的标准化机器人基准，以系统化地评估和推动泛化能力与仿真到真实的迁移性能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双臂机器人协调操作中高质量演示数据稀缺、仿真与真实世界评估脱节的核心问题，提出RoboTwin基准框架。该框架利用3D生成基础模型从单张图像创建多样化的物体数字孪生，并设计空间关系感知的代码生成框架，结合大语言模型分解任务、生成精确运动代码。在COBOT Magic Robot平台上的实验表明，基于RoboTwin生成数据预训练并有限微调的策略，相比仅用真实数据训练的模型，单臂任务成功率提升超70%，双臂任务提升超40%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.13059" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>