<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.00782" target="_blank" rel="noreferrer">2506.00782</a></span>
        <span>作者: Guo, Weiyang, Shi, Zesheng, Li, Zhuo, Wang, Yequan, Liu, Xuebo, Wang, Wenya, Liu, Fangming, Zhang, Min, Li, Jing</span>
        <span>日期: 2025/06/01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，自动化红队测试是检测大型语言模型（LLMs）安全漏洞的重要工具。现有方法主要分为两类：一类是基于预定义攻击策略的方法（如PAIR、TAP），其生成的越狱提示多样性有限；另一类是基于强化学习的方法（如CRT、GPO），它们通常依赖毒性分类器作为奖励信号，限制了攻击提示的范围。此外，多智能体架构的方法（如AutoDAN-Turbo）虽然能持续演练，但资源消耗高、进化缓慢。这些方法普遍难以在越狱提示的有效性和多样性之间取得平衡。</p>
<p>本文针对这一痛点，提出了一种新的自动化红队训练框架JAILBREAK-R1。其核心思路是利用强化学习，通过一条包含冷启动、热身探索和增强越狱的三阶段训练路径，向红队模型注入先验知识、鼓励多样性探索并稳定奖励信号，从而生成既有效又多样的攻击提示。</p>
<h2 id="方法详解">方法详解</h2>
<p>JAILBREAK-R1的整体框架是一个分阶段训练的强化学习流程，目标是训练一个红队模型（π_adv），使其能针对攻击目标（x）生成攻击提示（y），以诱使目标模型（π_tgt）产生有害响应（z）。整个训练过程分为三个阶段。</p>
<p><img src="https://..." alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：JAILBREAK-R1框架总览。（1）<strong>冷启动阶段</strong>：通过模仿学习获得越狱数据集，并以此监督微调红队模型。（2）<strong>热身探索阶段</strong>：以多样性和一致性作为奖励信号，训练模型的越狱指令遵循和多样性探索能力。（3）<strong>增强越狱阶段</strong>：引入渐进式越狱奖励，逐步提升红队模型的越狱性能。</p>
</blockquote>
<p><strong>核心算法与模块</strong>：</p>
<ol>
<li><strong>强化学习优化器（GRPO）</strong>：为解决传统PPO在越狱场景中奖励稀疏和多样性评估不准确的问题，本文采用了分组相对策略优化（Group Relative Policy Optimization， GRPO）算法。GRPO为每个输入采样一组（G个）输出，并以组内平均奖励为基线计算优势函数（A_i,t），如公式（2）所示。这种组内相对比较的方式，无需学习价值函数，能更有效地处理稀疏奖励并评估序列级别的多样性。</li>
<li><strong>输出模板</strong>：为指导红队模型，论文设计了一个固定输出模板，要求模型输出分为两部分：首先是基于攻击目标的分析和策略制定（思考部分），然后是最终的攻击提示（<code>&lt;attack&gt;...&lt;/attack&gt;</code>标签内）。这有助于模型进行规划。</li>
<li><strong>冷启动阶段（模仿学习）</strong>：首先通过模仿学习，利用现有的越狱策略库为攻击目标生成对应的“思考过程”和“攻击提示”，构建一个包含2K条数据的高质量冷启动数据集（D_cold）。然后用此数据集对基础模型（Qwen2.5-7B-Instruct）进行监督微调。如图2所示，在模仿学习微调设置下，思考过程能显著提升模型的越狱性能，为模型注入先验的越狱知识。</li>
</ol>
<p><img src="https://..." alt="思考方法对比"></p>
<blockquote>
<p><strong>图2</strong>：在不同思考方法设置下，红队模型攻击性能随尝试次数的变化。表明在模仿学习微调时，思考过程能显著提升性能。</p>
</blockquote>
<ol start="4">
<li><strong>热身探索阶段</strong>：此阶段专注于训练红队模型的指令遵循（一致性）和多样性探索能力，使用温和的奖励信号以避免陷入局部最优。<ul>
<li><strong>一致性奖励（R_consis）</strong>：训练一个意图分类模型（M_classify）来判断攻击提示（y）是否与攻击目标（x）一致。一致则奖励为1.0，否则为0.0。</li>
<li><strong>多样性奖励（R_div）</strong>：对于组内一致性为1的攻击提示，计算其与组内其他提示的文本相似度（SelfBLEU）和语义相似度（嵌入余弦相似度）。综合相似度越低（即多样性越高）的提示，获得的多样性奖励（归一化排名）越高。</li>
<li>本阶段总奖励为：R_warm(y) = R_consis(y) + R_div(y)。</li>
</ul>
</li>
<li><strong>增强越狱阶段</strong>：此阶段引入目标模型的响应（z）和越狱评估器（M_judge）来提供最终的越狱奖励。为应对越狱奖励稀疏的问题，采用了基于课程学习的渐进式训练策略。<ul>
<li>首先，使用毒性数据（D_toxicity）使目标模型安全性能逐步退化，得到n个（实验中n=3）安全性依次增强的中间模型（π_tgt_1, π_tgt_2, ..., π_tgt_n）。</li>
<li>在训练过程中，按课程从安全性最弱的模型开始，逐步切换到更强的模型作为当前阶段的目标模型。</li>
<li>本阶段奖励定义为：若评估器判断越狱成功（M_judge输出1），则奖励为 R_div(y) + 1.0；否则为0.0。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，JAILBREAK-R1的创新主要体现在：1) 使用GRPO算法优化红队模型，更好地处理稀疏奖励和评估多样性；2) 设计了三阶段的训练路径，依次解决知识缺乏、局部最优和奖励稀疏问题；3) 采用了基于课程学习的渐进式奖励策略，稳定训练过程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：使用Harmbench文本行为数据集进行评估。攻击目标数据集（D_target）从相关工作开源数据集中提取过滤得到，分为热身数据集（D_warm_target， 1k条）和训练数据集（D_train_target， 5k条）。</li>
<li><strong>评估模型</strong>：在8个主流LLMs上验证，包括GPT-3.5、GPT-4o、Claude-3.5、Gemini-2.0-flash（闭源）以及Llama-2-7B-Chat、Llama3-8B-Chat、Qwen2.5-7B-Instruct、Vicuna-7B（开源）。</li>
<li><strong>评估指标</strong>：攻击成功率（ASR）、多样性分数（DIV）、越狱效率（JE，成功越狱所需平均尝试次数）。</li>
<li><strong>基线方法</strong>：对比了AdvPrompt、TAP、PAIR、AutoDAN（基于策略/优化的方法），以及GPO、ArrAttack、AutoDAN-Turbo（自动化红队方法）。</li>
<li><strong>本文方法变体</strong>：JAILBREAK-R1（完整三阶段）和 JAILBREAK-R1-Zero（省略冷启动阶段）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>主结果对比</strong>：如表1所示，在Harmbench上，完整的JAILBREAK-R1在大多数模型上取得了最高的攻击成功率（ASR）和多样性分数（DIV）。例如，在GPT-3.5上ASR达76.5%（优于基线最佳62.0%），DIV达0.978；在Llama3-8B上ASR达58.5%，DIV达0.956。JAILBREAK-R1-Zero在ASR上略低于完整版，但多样性下降更明显，说明缺乏先验知识时强化学习易陷入局部最优。</li>
</ol>
<p><img src="https://..." alt="帕累托前沿对比"></p>
<blockquote>
<p><strong>图3</strong>：（a）攻击成功率与多样性得分的帕累托前沿图，显示JAILBREAK-R1在两者上均达到最优。（b）攻击成功率与攻击成本（提示平均token数）的帕累托前沿图，显示JAILBREAK-R1以较低成本达到最优攻击效果。</p>
</blockquote>
<ol start="2">
<li><strong>越狱效率</strong>：如表2所示，JAILBREAK-R1的越狱效率（JE平均2.05）优于其他方法，平均提升28%，意味着能以更少的尝试次数成功越狱。</li>
<li><strong>测试时扩展能力</strong>：如图4所示，在限制最多20次查询的测试时扩展中，JAILBREAK-R1的攻击成功率能随着查询次数增加而持续提升，优于其他方法（其他方法因多样性限制，后续轮次无法生成新提示导致性能停滞）。</li>
</ol>
<p><img src="https://..." alt="测试时扩展"></p>
<blockquote>
<p><strong>图4</strong>：不同方法的测试时扩展性能对比。JAILBREAK-R1的攻击成功率随尝试次数持续增长。</p>
</blockquote>
<ol start="4">
<li><strong>针对性训练与迁移性</strong>：如图5所示，针对特定目标模型（如Llama2-7B）进行有限训练后，红队模型对该模型的攻击成功率有提升（+3.8%），但对其他模型（如Llama3-8B）的性能可能出现波动（-4.3%）。这表明不同模型的安全漏洞具有特异性，同时也证明了JAILBREAK-R1框架的可迁移性。</li>
</ol>
<p><img src="https://..." alt="针对性训练"></p>
<blockquote>
<p><strong>图5</strong>：针对不同目标模型进行针对性训练前后的攻击成功率变化。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ol>
<li><strong>训练阶段消融</strong>：如图6所示，冷启动阶段显著提升了攻击多样性和有效性；热身探索阶段主要提升了攻击效率（越狱尝试次数减少）和生成提示的一致性，从而支持了更高的多样性。省略热身阶段（w/o Warm up）会略微降低多样性。JAILBREAK-R1-Zero因缺少冷启动，其攻击成功率和多样性分别比完整版低10.1%和10.7%。</li>
</ol>
<p><img src="https://..." alt="训练阶段消融"></p>
<blockquote>
<p><strong>图6</strong>：模型在不同训练阶段的性能变化（攻击成功率 vs. 多样性）。</p>
</blockquote>
<ol start="2">
<li><strong>课程学习消融</strong>：如图7所示，直接使用未退化的安全模型作为目标进行训练（w/o Curriculum），奖励信号稀疏且波动大。而采用渐进式课程训练（分三个阶段引入不同安全性的中间模型），可以获得稳定增长的奖励信号，训练更平稳。</li>
</ol>
<p><img src="https://..." alt="课程学习消融"></p>
<blockquote>
<p><strong>图7</strong>：在执行课程训练时，奖励信号随训练步数的变化。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的、基于强化学习的自动化红队训练框架JAILBREAK-R1，能够自动探索并生成多样且有效的攻击提示。</li>
<li>为红队模型设计了一条有效的三阶段（冷启动、热身探索、增强越狱）学习路径，缓解了知识缺乏、局部最优和奖励稀疏等关键挑战。</li>
<li>在多种LLMs上的大量实验表明，JAILBREAK-R1在攻击成功率和提示多样性方面优于现有基线，并显著提升了红队探索的效率，且具备测试时扩展能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身指出了三点局限性：1) 目前只能进行单轮越狱攻击，无法基于目标模型的响应进行迭代改进；2) 训练框架依赖多个模型（目标模型、分类模型、安全评估模型）协作提供奖励信号，导致资源消耗高、训练速度慢；3) 只能生成语义层面的越狱攻击，容易被安全分类器检测和识别。</p>
<p><strong>后续研究启示</strong>：<br>本工作为自动化红队测试提供了新视角。后续研究可以探索：如何将单轮攻击扩展为多轮对话式攻击以提升攻击深度；如何降低训练中对多个辅助模型的依赖，提高训练效率；以及如何结合梯度等白盒信息或生成难以检测的隐式攻击，以突破现有安全分类器的防御。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文针对自动红队方法在检测大型语言模型（LLMs）安全漏洞时，难以平衡攻击提示有效性与多样性的核心问题，提出了JAILBREAK-R1训练框架。该框架采用强化学习，分三个阶段实现：首先通过模仿学习进行监督微调（冷启动）；然后以多样性和一致性为奖励信号进行探索训练（热身探索）；最后引入渐进式越狱奖励以增强攻击性能（增强越狱）。在多种LLMs上的实验表明，JAILBREAK-R1相比现有方法，有效平衡了越狱提示的多样性和有效性，显著提升了红队探索效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.00782" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>