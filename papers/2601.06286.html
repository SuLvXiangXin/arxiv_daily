<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.06286" target="_blank" rel="noreferrer">2601.06286</a></span>
        <span>作者: Dai, Min, Compton, William D., Li, Junheng, Yang, Lizhi, Ames, Aaron D.</span>
        <span>日期: 2026/01/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>双足人形机器人在离散的受限立足点（如踏脚石、横梁）上行走时，必须精确协调平衡、步态时序和落脚点决策，微小失误都可能导致灾难性失败。传统方法主要依赖基于模型的优化与控制，能够很好地处理这些约束，但高度依赖精确的地形几何数学模型，当感知存在噪声或不完整时非常脆弱。另一方面，深度强化学习（RL）对扰动和建模误差展现了强大的鲁棒性，但端到端的策略很难从零开始发现非连续地形所需的精确落脚点放置和步态序列。这两种方法的局限性形成鲜明对比：基于模型的控制擅长在严格约束下规划可行运动，但对不确定性脆弱；RL擅长在不确定性下鲁棒执行，但难以从头发现精确的、受约束的运动。这促使研究者转向用物理结构引导学习，而非单纯依赖奖励工程。本文针对在稀疏、离散立足点上实现精确、鲁棒步行的核心痛点，提出了一种将简化模型规划与数据驱动的RL自适应紧密结合的新视角。其核心思路是：利用一个基于简化动力学模型（线性倒立摆）的步态规划器，实时生成与环境约束动态一致的参考轨迹（包括落脚点、质心轨迹和步态时序），并通过基于控制李雅普诺夫函数（CLF）的奖励设计来引导RL训练过程，从而结合模型方法的精确性和RL方法的鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的PLANC框架是一个分层结构，高层是一个基于简化模型的实时参考轨迹生成器，低层是一个基于师生蒸馏框架的RL策略，整体架构如图2所示。</p>
<p><img src="https://arxiv.org/html/2601.06286v1/Figures/arch_diagram.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：模型引导的RL架构总览。左侧展示了在仿真中训练的四种地形（上楼梯、下楼梯、平坦踏脚石、高度变化的踏脚石）。训练采用师生蒸馏方式，奖励由简化模型（RoM）信息构建的CLF奖励提供。学生策略成功部署在仿真到仿真迁移以及真实硬件上。</p>
</blockquote>
<p><strong>1. 基于简化模型的参考轨迹生成</strong><br>核心是使用带被动踝关节的线性倒立摆（LIP）模型作为高层规划器，如图3所示。该模型将高度变化的踏脚石视为连接连续石头中心的虚拟斜面，并假设质心（CoM）沿此斜面恒定高度运动。</p>
<p><img src="https://arxiv.org/html/2601.06286v1/Figures/rom_rewards.png" alt="LIP模型"></p>
<blockquote>
<p><strong>图3</strong>：用于步态合成的LIP模型示意图，包括摆动脚和质心的期望轨迹。</p>
</blockquote>
<p>规划器动态生成以下参考：</p>
<ul>
<li><strong>步态时序</strong>：利用LIP动力学的闭式解，根据当前碰撞后状态和到下一个立足点的距离，解析计算所需的单步时长 (T_s)，实现非均匀立足点上的精确时序调节。</li>
<li><strong>轨道能量调节</strong>：通过调节碰撞前的质心垂直速度 (u_{\text{des}})（作为离散输入），来控制碰撞后的角动量，使得碰撞后轨道能量 (E^+) 稳定在期望值 (E^*)，确保向前的动态可行性。</li>
<li><strong>质心轨迹</strong>：使用闭式三次样条生成平滑的CoM参考轨迹，样条由相位变量 (s = t/T_s) 参数化，边界条件由当前状态和基于期望轨道能量 (E^*) 计算的目标状态确定。</li>
<li><strong>摆动脚轨迹</strong>：使用贝塞尔多项式在起脚状态和目标落脚点之间插值生成摆动脚轨迹。目标落脚点的水平（x， z）位置严格由即将踏上的石头几何形状决定，侧向（y）位置由HLIP模型参考生成以调节步宽。</li>
<li><strong>其他参考</strong>：骨盆和摆动脚的参考方向与局部地形朝向对齐，上肢关节跟踪标称轨迹以辅助角动量抵消。</li>
</ul>
<p><strong>2. 基于CLF的奖励设计与师生蒸馏训练</strong><br>规划器生成的完整参考轨迹（包括期望输出 (\boldsymbol{y}_d)、实际输出 (\boldsymbol{y}_a) 和完整约束信息）被输入到RL训练循环中。关键创新是将CLF跟踪条件直接嵌入为RL奖励，引导策略学习跟踪这些物理上一致的参考。</p>
<p>然而，CLF奖励依赖于步态相位等特权信息，而部署时无法获得。此外，标准的非对称演员-评论家方法在此精确任务上难以收敛。因此，本文采用三阶段师生蒸馏框架：</p>
<ol>
<li><strong>教师训练</strong>：首先训练一个拥有特权信息（如真实地形、接触相位）的教师策略，使其在标称地形上掌握基本行走技能。</li>
<li><strong>蒸馏（热启动）</strong>：通过行为克隆损失 (\mathcal{L}_{\text{distill}})，将教师策略的能力蒸馏到仅使用可部署观测（本体感知、局部高度图、命令、上一动作）的学生策略中，为其提供一个稳定的行走先验。</li>
<li><strong>课程微调</strong>：使用近端策略优化（PPO）算法，让学生在渐进式难度的踏脚石课程中进行微调。课程根据连续成功次数自动提升地形难度参数（如石头间距、高度变化），确保策略稳健泛化。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在Unitree G1人形机器人上进行。仿真训练使用了四种程序化生成的踏脚石地形：上楼梯、下楼梯、平坦踏脚石和高度变化踏脚石。评估对比了多个基线方法：1) <strong>无模型RL基线</strong>：使用相同观测和奖励结构但不带模型引导的PPO策略；2) <strong>无模型RL（无课程）</strong>；3) <strong>基于模型的控制器</strong>：使用相同LIP规划器但搭配基于二次规划的CLF跟踪控制器。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>训练性能与成功率</strong>：如图4所示，PLANC方法在所有地形上都显著优于无模型基线，成功率达到90%以上，而无模型基线在高度变化石头上几乎完全失败（成功率&lt;10%）。模型引导极大地提高了样本效率和最终性能。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.06286v1/Figures/training.png" alt="训练曲线"></p>
<blockquote>
<p><strong>图4</strong>：不同方法在四种地形上的训练曲线（成功率 vs 环境步数）。PLANC方法（实线）收敛更快且最终成功率远高于无模型基线（虚线）。</p>
</blockquote>
<ul>
<li><strong>跟踪精度</strong>：如图5所示，PLANC策略在摆动脚跟踪误差（RMSE）上比无模型基线有数量级的提升（例如，在高度变化石头上，误差从0.2m降至0.02m），证明了其精确的落脚点控制能力。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.06286v1/Figures/tracking.png" alt="跟踪性能"></p>
<blockquote>
<p><strong>图5</strong>：在高度变化踏脚石地形上，PLANC与无模型基线的摆动脚位置跟踪误差（RMSE）对比。PLANC的误差显著更低。</p>
</blockquote>
<ul>
<li><strong>消融实验</strong>：论文验证了各核心组件的贡献。移除模型引导（即纯无模型RL）导致性能崩溃。移除师生蒸馏，直接训练学生策略，则无法收敛，证明了蒸馏对于在复杂优化景观中提供良好初始化的必要性。</li>
<li><strong>仿真到硬件迁移</strong>：训练好的策略成功部署到真实的Unitree G1机器人上。图6、7、8展示了硬件实验的定性结果，机器人能够稳健地行走于具有间隙和高度变化的踏脚石阵列。</li>
</ul>
<p><img src="https://arxiv.org/html/2601.06286v1/x1.png" alt="硬件实验1"></p>
<blockquote>
<p><strong>图6</strong>：Unitree G1机器人在真实踏脚石上行走的硬件实验序列（侧视图）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.06286v1/x2.png" alt="硬件实验2"></p>
<blockquote>
<p><strong>图7</strong>：硬件实验序列（前视图），展示侧向平衡。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.06286v1/x3.png" alt="硬件实验3"></p>
<blockquote>
<p><strong>图8</strong>：硬件实验序列（俯视图），展示精确的落脚点放置。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>物理引导的强化学习框架</strong>，将地形感知的简化模型步态规划器与CLF奖励设计紧密集成到RL训练循环中，为RL提供了动态与环境一致的参考，从而在离散非周期地形上实现了敏捷、安全的行走。</li>
<li>设计了一个<strong>多阶段的师生蒸馏训练流程</strong>，克服了特权信息依赖和复杂任务优化难的问题，成功训练出仅使用可部署观测的通用策略。</li>
<li>在Unitree G1人形机器人上进行了全面的<strong>数值与实验验证</strong>，证明了该方法在随机踏脚石地形上相比无模型基线在稳定性、落脚点精度和泛化性方面的显著提升。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法仍然依赖于简化模型（LIP）来生成参考。虽然LIP对平面运动捕捉良好，但在需要大幅质心垂直运动或复杂角动量管理的极端地形中可能不足。此外，参考生成和RL训练虽然比纯优化方法高效，但仍有一定的计算成本。</p>
<p><strong>研究启示</strong>：这项工作展示了将模型化先验与数据驱动学习深度融合的有效性，为在严格约束下实现鲁棒且精确的机器人控制提供了新范式。未来的方向可能包括：1) 使用更复杂的简化模型或在线模型自适应来扩展地形适应范围；2) 进一步将感知不确定性纳入参考规划或RL训练中；3) 探索该框架在其他需要精确时序和空间约束的机器人任务中的应用。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双足人形机器人在离散立足点（如踏脚石）上行走时，需精确协调平衡与落脚点的核心挑战，提出PLANC框架。该方法结合降阶步态规划与强化学习，通过规划器提供动态一致的运动目标，并利用控制李雅普诺夫函数奖励引导训练。实验表明，该框架在硬件上实现了准确、敏捷的踏脚石行走，相比传统无模型强化学习方法，可靠性显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.06286" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>