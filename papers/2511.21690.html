<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.21690" target="_blank" rel="noreferrer">2511.21690</a></span>
        <span>作者: Furong Huang Team</span>
        <span>日期: 2025-11-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域的主流世界建模方法主要分为三类：像素空间的视频生成模型、语言空间的视觉语言模型（VLM）规划器，以及轨迹预测模型。视频生成模型虽表达能力强，但将大量计算资源用于重建与操控无关的背景和纹理，推理成本高，且可能产生几何或可供性幻觉。VLM规划器输出离散的语义令牌，缺乏表示精细物体运动所需的空间和时间分辨率。现有的轨迹预测模型则大多局限于2D，或仅关注被操纵物体，需要额外的物体检测和启发式过滤模块，这引入了误差累积，且无法捕捉机器人自身的运动，导致物理表征不完整。</p>
<p>本文针对从少量演示中学习新机器人任务的核心痛点，特别是如何利用大量异质的人类和不同机器人视频（跨具身视频）来克服数据稀缺问题，提出了一个统一、符号化的新视角——3D轨迹空间。该视角认为，尽管具身形态、相机和环境存在差异，但被操纵物体和末端执行器的运动共享一个以场景为中心的3D几何结构。本文的核心思路是：构建一个名为TraceGen的世界模型，直接在紧凑的3D轨迹空间中预测未来运动，从而抽象掉外观变化，保留操控所需的几何结构，并利用大规模跨具身视频预训练获得可迁移的运动先验，实现高效的少样本适应。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架包含两个核心部分：用于构建大规模训练数据的<strong>TraceForge</strong>管道，以及基于此数据进行训练和预测的<strong>TraceGen</strong>世界模型。</p>
<p><img src="https://arxiv.org/html/2511.21690v1/figures/trace_gen_logo.png" alt="TraceForge数据构建流程"></p>
<blockquote>
<p><strong>图4</strong>：构建TraceForge数据集的流程。从输入视频开始，经过(i)事件分块与指令生成，(ii)带相机位姿和深度估计的3D点跟踪，(iii)世界到相机坐标系的变换对齐，(iv)速度重定向，最终生成用于训练的统一3D轨迹。</p>
</blockquote>
<p><strong>TraceForge数据管道</strong>负责将异质的人类和机器人视频转换为统一的3D轨迹标注。其流程如下：</p>
<ol>
<li><strong>事件分块与指令生成</strong>：从视频中分割出与任务相关的片段，并使用VLM为每个片段生成三种互补的语言指令（简短命令、多步分解、自然请求），形成“观测-轨迹-语言”三元组。</li>
<li><strong>带相机位姿和深度估计的3D点跟踪</strong>：在每个事件块开始时，选择一帧参考图像，在其上放置一个20x20的均匀网格关键点。使用改进的TAPIP3D模型估计每帧的相机位姿和深度图，并跟踪这些关键点的3D轨迹。所有轨迹最终被表达在参考相机坐标系中，以补偿相机运动。轨迹点表示为<code>(x, y, z)</code>，其中<code>(x, y)</code>为图像平面坐标，<code>z</code>为深度，这使得3D轨迹与2D轨迹具有相同的屏幕对齐方式，便于联合训练。</li>
<li><strong>世界到相机坐标系变换</strong>：利用估计的相机外参，将所有3D轨迹从世界坐标系变换到固定的参考相机坐标系，确保视角一致性。</li>
<li><strong>速度重定向</strong>：为消除不同演示在时长和执行速度上的差异，对轨迹进行时间归一化。通过计算3D路径的累积弧长，并按归一化弧长参数重新参数化，在固定长度<code>L</code>上均匀重采样，从而保留相对运动轮廓。</li>
</ol>
<p>通过TraceForge，作者构建了包含12.3万段视频、180万个“观测-轨迹-语言”三元组的大规模数据集TraceForge-123K。</p>
<p><img src="https://arxiv.org/html/2511.21690v1/figures/trace_forge_logo.png" alt="TraceGen模型概述"></p>
<blockquote>
<p><strong>图5</strong>：TraceGen模型概述。给定语言、RGB和深度输入，通过多编码器提取特征并融合，作为条件输入给基于流的解码器。解码器预测一个速度场，通过ODE积分将高斯噪声转化为轨迹块，最终输出参考相机坐标系下的3D关键点轨迹。</p>
</blockquote>
<p><strong>TraceGen世界模型</strong>是一个基于流的模型，用于从多模态观测中预测未来的3D运动轨迹。</p>
<ol>
<li><strong>多编码器特征提取</strong>：<ul>
<li><strong>RGB编码器</strong>：使用冻结的DINOv3（提取几何特征）和SigLIP（提取语义特征）双流编码。</li>
<li><strong>深度编码器</strong>：使用带可学习stem适配器的SigLIP编码器处理深度图。</li>
<li><strong>文本编码器</strong>：使用冻结的T5-base编码器处理任务指令。</li>
<li><strong>特征融合</strong>：遵循Prismatic-VLM策略，将三种视觉特征拼接后，通过一个可学习的线性层投影到统一维度，再与文本令牌拼接，形成条件输入 <code>F_cond</code>。</li>
</ul>
</li>
<li><strong>基于流的轨迹解码器</strong>：<ul>
<li><strong>架构</strong>：适配CogVideoX的3D Transformer至轨迹空间。输入是K=20x20个空间关键点跨越L=32个未来时间步的网格，每个点有<code>(x, y, z)</code>坐标。对空间进行2x2的块化处理。</li>
<li><strong>生成方式</strong>：模型不直接预测绝对轨迹，而是预测关键点的时间增量 <code>ΔT_ref^t</code>（即速度场）。采用随机插值框架，具体实现为线性插值ODE：<code>X^τ = (1-τ)X^0 + τX^1</code>，其中<code>X^0</code>为高斯噪声，<code>X^1</code>为真实轨迹增量。训练一个神经网络<code>v_θ</code>来预测恒定的速度场 <code>X^1 - X^0</code>，损失函数为 <code>L_SI = E[||v_θ(X^τ, τ, F_cond) - (X^1 - X^0)||^2]</code>。推理时，通过100步ODE积分从噪声生成轨迹。</li>
</ul>
</li>
</ol>
<p>与现有方法相比，TraceGen的创新点在于：1) 提出了<strong>场景级</strong>的3D轨迹表示，同时包含机器人和物体的运动，无需物体检测器或启发式过滤；2) 通过<strong>TraceForge管道</strong>实现了对跨具身、跨环境、带相机运动视频的大规模、统一3D轨迹标注；3) 在<strong>轨迹空间</strong>进行世界建模和少样本微调，极大提升了推理效率和对运动几何的专注度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实世界的Franka Research 3机器人上评估了四个操作任务：折叠衣物、将网球放入盒子、用刷子清扫垃圾入畚箕、将积木放入紫色区域。使用单个RGB-D帧和语言指令作为输入，TraceGen预测的3D轨迹通过逆运动学转换为关节命令执行。</p>
<p><strong>Baseline方法</strong>：对比了视频生成类方法（AVDC, NovaFlow）和轨迹预测类方法（3DFlowAction）。对于视频生成方法，使用统一的视频到轨迹提取流程；对于3DFlowAction，提供了真实掩码以避免其分割模块失效。</p>
<p><img src="https://arxiv.org/html/2511.21690v1/x1.png" alt="成功率和推理效率对比"></p>
<blockquote>
<p><strong>图7</strong>：各方法在真实机器人任务上的成功率和推理效率（每分钟预测次数）对比。TraceGen在仅用5个目标机器人视频微调后，取得了80%的成功率，同时推理速度比基于视频的世界模型快50-600倍。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>机器人→机器人适应（少量同域微调）</strong>：仅使用5个目标机器人演示视频进行微调后，TraceGen在四个任务上平均达到<strong>80%<strong>的成功率。而参数量低于100亿的所有其他方法在零样本设置下成功率均为0%。大型视频生成模型（如NovaFlow）虽有非零的零样本成功率，但推理极慢（慢</strong>600倍</strong>以上），且难以进行少样本微调。TraceGen的推理速度比轨迹预测baseline快<strong>3.8倍</strong>，比大型视频生成模型快<strong>50倍</strong>以上。</li>
<li><strong>人类→机器人迁移（无目标机器人数据）</strong>：仅使用5个未经校准的手机拍摄的人类演示视频（不同场景）进行微调，TraceGen在真实机器人上仍能达到**67.5%**的平均成功率。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.21690v1/x3.png" alt="人机迁移实验结果"></p>
<blockquote>
<p><strong>图8</strong>：人机技能迁移结果。TraceGen使用5个手持手机拍摄的人类演示视频微调后，成功率为67.5%。而从头训练的模型（From Scratch）成功率为0%，证明了跨具身预训练的重要性。</p>
</blockquote>
<ol start="3">
<li><strong>预训练与微调的作用</strong>：消融实验表明，未经大规模跨具身数据预训练的“从头训练”模型，在仅使用少量微调数据时完全失败（0%成功率），这凸显了TraceForge提供的大规模预训练对于获得可迁移的3D运动先验至关重要。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>TraceGen模型</strong>：首个在3D轨迹空间进行世界建模的方法，通过抽象外观和相机变化，实现了从跨具身、跨环境、跨任务视频中学习。</li>
<li><strong>TraceForge管道与数据集</strong>：一个统一的流水线，能将异质视频转化为一致的3D轨迹，并构建了远超现有规模（大15倍）的“观测-轨迹-语言”三元组数据集。</li>
<li><strong>高效的少样本适应能力</strong>：在仅需5个演示视频的微调下，实现了机器人→机器人适应80%的成功率，以及从非校准人类视频到机器人67.5%的成功迁移，同时推理效率显著提升。</li>
</ol>
<p><strong>局限性</strong>：论文提到，TraceGen依赖于上游的3D点跟踪精度来生成训练数据，跟踪误差可能会影响数据质量和模型性能。此外，当前工作使用了一个基本的跟踪控制器来执行预测的轨迹，开发更复杂、鲁棒的下游策略是未来的工作方向。</p>
<p><strong>对后续研究的启示</strong>：本文证明了3D轨迹空间作为一种紧凑、几何感知的中间表示，在连接视觉观察与机器人动作、促进跨具身知识迁移方面的巨大潜力。这种表示既避免了像素生成的沉重负担，又提供了比语言令牌更精确的空间信号。未来的工作可以探索将隐式世界模型的学习目标与TraceGen的显式轨迹预测相结合，以进一步增强表征能力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出TraceGen，解决机器人难以从少量演示中学习新任务的问题。核心方法是构建3D轨迹空间（trace-space）作为统一符号表示，并开发TraceGen世界模型在该空间预测运动，以及TraceForge数据管道将异构视频转换为轨迹数据。实验表明，仅用5个目标机器人视频，模型在4项任务上达到80%成功率，推理速度比现有视频世界模型快50-600倍；仅用5个手机拍摄的人类演示视频，在真实机器人上仍能实现67.5%的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.21690" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>