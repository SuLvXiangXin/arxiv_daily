<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>What Can RL Bring to VLA Generalization? An Empirical Study - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>What Can RL Bring to VLA Generalization? An Empirical Study</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.19789" target="_blank" rel="noreferrer">2505.19789</a></span>
        <span>作者: Liu, Jijia, Gao, Feng, Wei, Bingwen, Chen, Xinlei, Liao, Qingmin, Wu, Yi, Yu, Chao, Wang, Yu</span>
        <span>日期: 2025/05/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型视觉-语言-动作模型在具身智能中展现出巨大潜力，但其训练主要依赖于对演示数据的行为克隆，即监督微调。这种方法在分布偏移下容易产生误差累积问题，从而限制了模型的泛化能力。强化学习通过试错直接优化任务目标，为克服这些限制提供了可能，尤其在大型语言模型和视觉语言模型中已被证明能带来更好的分布外泛化。然而，对于VLA模型，RL微调能带来哪些具体的泛化益处，与SFT相比各自的优势何在，尚缺乏系统性的理解。本文旨在填补这一空白，通过系统的实证研究，探究RL相较于SFT能为VLA模型的泛化带来哪些独特优势。其核心思路是：构建一个全面的VLA泛化评估基准，系统比较PPO、DPO、GRPO等RL算法在VLA微调上的效果，并深入分析RL与SFT在视觉、语义、执行三个维度的泛化性能差异。</p>
<h2 id="方法详解">方法详解</h2>
<p>本研究首先探索如何对VLA模型进行有效的RL微调，识别出最有效的算法（PPO）并设计高效的训练方案，随后在构建的基准上系统评估其泛化性能。</p>
<p>整体流程基于OpenVLA模型，在一个模拟的拾放任务环境中进行。输入是单帧RGB图像和自然语言指令，输出是机器人末端执行器的笛卡尔空间位移和二进制夹持器信号。研究比较了SFT与三种源自大模型社区的RL算法：PPO、GRPO和DPO。SFT使用运动规划器生成的演示数据进行离线训练；RL方法则通过与模拟环境交互，基于稀疏奖励（成功抓取和放置）进行在线或离线优化。</p>
<p><img src="https://arxiv.org/html/2505.19789v4/x3.png" alt="不同微调方法概述"></p>
<blockquote>
<p>**图3(a)**：VLA微调方法概述。SFT从离线演示中学习；而DPO、GRPO和PPO使用基于RL的更新——分别采用偏好对齐、组相对优势估计以及带有广义优势估计的标准actor-critic PPO。</p>
</blockquote>
<p>核心模块一：RL算法比较与选择。论文评估了PPO、GRPO和DPO在VLA微调上的有效性。结果表明，PPO（包括标准版本和ORZ变体）的性能一致优于GRPO和DPO。作者分析认为，机器人任务的POMDP特性导致环境状态随动作非平稳变化，这可能破坏了GRPO优势估计的稳定性；而DPO则受限于稀疏奖励难以区分轨迹质量，以及离线数据与交互执行间的分布偏移。</p>
<p><img src="https://arxiv.org/html/2505.19789v4/x4.png" alt="PPO性能比较"></p>
<blockquote>
<p>**图3(b)**：不同RL微调算法在拾放任务上的性能比较。PPO及其变体表现最佳。</p>
</blockquote>
<p>核心模块二：高效的PPO微调方案设计。在确定PPO为有效算法后，论文通过消融实验提炼出一套高效的VLA微调配方，主要包含三个关键设计：</p>
<ol>
<li><strong>共享actor-critic骨干网络</strong>：将预训练的VLA策略作为actor，并附加一个轻量级的三层MLP价值头作为critic，两者共享整个Transformer骨干。价值头以第一个动作令牌位置对应的隐藏向量 <code>h0</code> 为输入，回归出标量价值。实验证明，这种设计在达到相近性能的同时，比独立的actor-critic架构训练速度快35%，且节省83%的显存。</li>
<li><strong>VLA模型预热</strong>：由于原始OpenVLA检查点在基准上性能不佳，使用140条演示轨迹对其进行SFT预热。预热后的模型能以约少50%的环境步数达到收敛，提升了训练效率。</li>
<li><strong>最小PPO训练轮次</strong>：实验发现，将PPO的更新轮次设置为1（即每个批次只进行一次梯度更新）在回报和样本效率上并未损失，同时能线性减少训练时间，因此固定 <code>epoch=1</code> 作为高效配置。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.19789v4/x4.png" alt="共享actor-critic架构"></p>
<blockquote>
<p>**图4(a)**：具有共享actor-critic骨干的PPO架构，其中价值 <code>V(s)</code> 由一个三层MLP预测。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.19789v4/x5.png" alt="高效设计验证"></p>
<blockquote>
<p><strong>图5</strong>：验证高效训练设计的性能比较。(a) 模型预热能加速收敛；(b,c) PPO更新轮次为1时即可达到最佳效率。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在基于ManiSkill构建的仿真拾放任务基准上进行。基准系统性地定义了视觉、语义、执行三个维度的分布外泛化测试，例如新颖背景/纹理、未见过的物体/容器、多样化的指令表述、变化的初始位姿以及 episode 中物体重定位等。</p>
<p><strong>对比方法</strong>：主要对比了不同数据规模（最高64k条轨迹）的SFT基线，以及使用上述高效配方微调的PPO策略。同时也比较了GRPO和DPO等RL算法。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>SFT数据规模效应</strong>：SFT性能在约16k条演示轨迹时达到饱和，因此后续将SFT-16k作为强基线。</li>
<li><strong>RL vs. SFT 总体泛化</strong>：RL微调在约0.4M环境步数后，在OOD（未见过的物体和桌子）任务上的表现超越了SFT-16k。收敛时，RL在训练分布上表现与SFT-16k相当，但在OOD物体和桌子上的成功率比SFT-16k高出 **42.6%**。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.19789v4/x6.png" alt="数据规模与训练曲线"></p>
<blockquote>
<p><strong>图6</strong>：(a,b) SFT性能随数据规模增加而饱和；(c,d) RL训练曲线显示其在OOD评估中快速超越最佳SFT基线。</p>
</blockquote>
<ol start="3">
<li><strong>分维度泛化分析</strong>：如图7所示，RL在<strong>语义</strong>和<strong>执行</strong>维度的OOD任务上显著优于SFT，而在<strong>视觉</strong>维度两者表现相当。具体而言，在涉及未见物体、多物体选择等语义任务中，RL表现出更强的理解与适应能力；在物体/容器位置变化、机器人初始位姿变化、以及 episode 中物体被重新定位等执行扰动下，RL展现出远优于SFT的鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.19789v4/x7.png" alt="泛化性能对比"></p>
<blockquote>
<p><strong>图7</strong>：(a) SFT与RL在不同任务上的性能对比与相对性能下降；(b,c) 成功率和OOD/IND性能比的雷达图，清晰显示RL在语义和执行上的优势。</p>
</blockquote>
<ol start="4">
<li><strong>定性分析与洞察</strong>：训练轨迹可视化（图8）显示，RL策略探索了更广阔的工作空间和更丰富的末端姿态，而SFT轨迹则紧密聚集在演示数据中的运动规划路径周围。这解释了RL在执行泛化上的优势。具体失败案例（图9）表明，SFT在强视觉噪声下无法定位容器，或对未见物体执行无效的抓取尝试；而RL则能完成放置，或从抓取失败、物体中途移位中恢复。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.19789v4/x8.png" alt="训练轨迹对比"></p>
<blockquote>
<p><strong>图8</strong>：SFT（左）与RL（右）的训练轨迹对比，颜色编码表示夹持器Z轴旋转。RL轨迹覆盖范围更广、姿态更多样。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.19789v4/x9.png" alt="定性案例"></p>
<blockquote>
<p><strong>图9</strong>：具体案例可视化。展示了SFT在视觉干扰、语义理解、执行扰动下的典型失败，以及RL对应的成功处理。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个严谨且全面的基准，用于系统评估VLA微调方法在视觉、语义、执行三个关键维度的泛化能力。</li>
<li>通过实证确定了PPO是比GRPO和DPO更有效的VLA微调RL算法，并提出了一套高效的PPO微调配方（共享骨干、模型预热、最小更新轮次）。</li>
<li>揭示了RL微调相较于SFT，能显著提升VLA在语义理解和执行鲁棒性上的分布外泛化能力，同时在视觉鲁棒性上保持相当水平。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>SFT使用的演示数据完全由运动规划器生成，未能涵盖人类数据可能具有的多样性。</li>
<li>评估任务集中于拾放操作，尚未扩展到更复杂、多任务的场景。</li>
<li>所有实验均在仿真中进行，尚未通过仿真到真实的迁移在物理机器人上验证RL微调对VLA泛化的提升。</li>
</ol>
<p><strong>启示</strong>：<br>本研究强有力地证明了强化学习是迈向更具泛化能力的具身智能体的一条有效途径。RL通过试错学习到的纠错行为和更广泛的探索经验，使其在应对语义变化和执行扰动时更具优势。未来的工作可以沿着论文指出的局限性展开，包括使用更丰富的数据源、扩展到更复杂的多任务长视野规划，以及实现从仿真到真实世界的迁移验证。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文实证研究强化学习（RL）如何提升视觉-语言-动作（VLA）模型的泛化能力。核心问题是VLA模型主要依赖监督微调（SFT）训练，在分布偏移下易产生错误累积，限制泛化。论文引入综合基准，系统评估RL微调在视觉、语义和执行维度的效果。关键技术采用PPO等RL算法进行微调，并对比DPO和GRPO。实验表明：RL微调（尤其PPO）显著增强语义理解和执行鲁棒性的泛化，视觉鲁棒性与SFT相当；PPO比DPO和GRPO更有效，并提出了高效PPO训练方案。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.19789" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>