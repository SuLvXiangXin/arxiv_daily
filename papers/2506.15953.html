<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.15953" target="_blank" rel="noreferrer">2506.15953</a></span>
        <span>作者: Jitendra Malik Team</span>
        <span>日期: 2025-06-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧操作是机器人系统以类人方式与物理世界交互的核心能力。尽管基于视觉的方法发展迅速，但触觉感知对于精细控制至关重要，尤其是在非结构化或视觉遮挡的场景中。当前主流的模仿学习方法，如扩散策略（DP）和动作分块Transformer（ACT），在有限的训练数据下表现出色，但大多局限于简单的机械手配置，且泛化能力较差。这主要源于对触觉感知的利用不足。虽然已有一些工作开始将触觉反馈集成到灵巧操作中，但所学习的触觉表征往往是浅层且未被深入探索的。具体而言，缺乏一个学习视觉-触觉跨模态表征以支持灵巧操作的有效模型。本文针对这一痛点，提出了ViTacFormer，其核心思路是：通过交叉注意力层在策略的每个阶段融合视觉和触觉线索，并引入一个自回归的触觉预测头来预测未来的接触信号，从而迫使共享的潜在空间编码可操作的触觉动态。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViTacFormer的整体框架是一个条件变分自编码器（CVAE），其核心目标是学习一个融合视觉与触觉的跨模态表征，并利用该表征生成动作。</p>
<p><img src="https://i.imgur.com/1q3dW7v.png" alt="方法整体架构"></p>
<blockquote>
<p><strong>图2</strong>：ViTacFormer的神经网络架构。左侧：一个基于Transformer的编码器，将动作序列和机器人本体感知映射为动作风格变量z。右侧：一个基于Transformer的编码器-解码器，使用风格变量z、机器人本体感知（关节）以及视觉-触觉观测，自回归地预测未来触觉信号并生成动作。</p>
</blockquote>
<p><strong>整体流程</strong>：训练时，左侧编码器从专家演示中提取动作风格变量z。推理时，z从高斯噪声中采样。右侧的编码器-解码器是策略的核心。它接收当前的多模态观测（视觉、触觉、关节状态）和风格变量z，首先通过<strong>交叉注意力模块</strong>融合视觉与触觉信息，然后<strong>自回归地预测未来的触觉信号</strong>，最后将预测的未来触觉信号与当前观测再次融合，用于<strong>生成动作序列</strong>。</p>
<p><strong>核心模块一：基于交叉注意力的多模态融合</strong>。传统方法通常将视觉和触觉观测作为独立的令牌进行简单拼接，忽略了模态间的相关信息。ViTacFormer采用交叉注意力机制进行深度融合。</p>
<p><img src="https://i.imgur.com/4t5mY9c.png" alt="交叉注意力融合"></p>
<blockquote>
<p><strong>图3</strong>：视觉与触觉观测之间基于交叉注意力的多模态融合。视觉观测的键和值用触觉信号的查询计算，反之亦然。最终，基于交叉注意力的特征被拼接成隐藏状态以供进一步学习。</p>
</blockquote>
<p>具体而言，该模块计算视觉观测的键（K_v）和值（V_v），以及触觉信号的查询（Q_t），进行交叉注意力计算，反之亦然（用触觉的K_t、V_t和视觉的Q_v）。这使得模型能够同时关注两个输入序列的不同部分，捕获它们之间的依赖关系，从而提取视觉与触觉信号之间相关的语义信息。</p>
<p><strong>核心模块二：带触觉信号预测的自回归建模</strong>。预测未来的触觉信号能使智能体感知接触信号的变化，促使潜在表征编码潜在的未来结果。自回归地利用这些预测的未来触觉信号，能使智能体利用这种先验的接触知识来更好地生成动作。<br>因此，动作生成过程分为两步：1）利用风格变量z、当前机器人本体感知和视觉-触觉观测，预测未来的触觉令牌。2）将预测的未来触觉信号与当前输入令牌拼接，用于生成动作。注意，在预测未来触觉信号和生成动作时，都进行了两次视觉-触觉信号间的交叉注意力融合。</p>
<p><strong>训练策略与损失函数</strong>：在实践中，视觉观测与预测的噪声触觉信号之间的交叉注意力融合可能会破坏表征。为此，论文提出了一种<strong>两阶段课程学习</strong>策略：在前75%的训练周期中，使用真实（ground-truth）的未来触觉令牌来训练动作生成，以稳定表征学习；在最后25%的周期中，切换到使用模型预测的未来触觉信号进行训练，以促进鲁棒的跨模态推理。<br>模型的训练损失函数为加权组合：<br><code>L = w1·L_KL + w2·L_JA + w3·L_tactile + w4·L_arm</code><br>其中，<code>L_KL</code>是动作风格变量与高斯分布之间的KL散度，<code>L_JA</code>是基于预测动作与真实动作的L1损失，<code>L_tactile</code>是基于未来触觉信号与真实信号的L1损失。特别地，<code>L_arm</code>是对机械臂末端执行器的监督，包含末端位置L2损失（<code>L_position</code>）和末端旋转L1损失（<code>L_rotation</code>）。论文发现<code>L_arm</code>对于训练灵巧操作技能非常有用。</p>
<p><strong>创新点总结</strong>：1) 提出了基于交叉注意力的深度视觉-触觉融合机制，而非简单的令牌拼接。2) 引入了自回归的未来触觉预测头，使策略具备前瞻性。3) 设计了两阶段课程学习策略，以稳定基于预测触觉的训练过程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：硬件系统包括两个Realman机械臂，各搭载一个SharpaWave五指仿人灵巧手（17自由度），配备手腕鱼眼相机、顶部ZED Mini立体相机和高分辨率（320x240）指尖触觉传感器。通过外骨骼手套和VR头显进行遥操作数据采集。构建了包含短时程和长时程任务的真实世界视觉-触觉灵巧操作基准测试。每个任务仅使用50条专家轨迹进行训练，增加了挑战性。</p>
<p><strong>基线方法</strong>：对比了扩散策略（DP）、结合触觉的HATO、动作分块Transformer（ACT）以及结合触觉输入的ACT（ACTw/T）。其中，DP和ACT不使用触觉输入，HATO和ACTw/T以简单的令牌融合方式加入触觉信号。</p>
<p><strong>关键实验结果 - 短时程任务</strong>：在四个短时程任务（插桩、拧瓶盖、擦花瓶、翻书）上评估成功率。</p>
<p><img src="https://i.imgur.com/table1.png" alt="成功率对比表"></p>
<blockquote>
<p><strong>表1</strong>：四个短时程灵巧操作任务的成功率对比。ViTacFormer在所有任务上均达到最佳性能，成功率相比基线提升约50%，近乎解决了这些任务。同时，触觉观测的加入显著提升了性能（比较ACTw/T vs ACT, HATO vs DP）。</p>
</blockquote>
<p><strong>关键实验结果 - 长时程任务</strong>：在一个包含11个阶段的复杂长时程任务“制作汉堡”上进行评估。</p>
<p><img src="https://i.imgur.com/9pLkQvH.png" alt="长时程任务成功执行序列"></p>
<blockquote>
<p><strong>图5</strong>：在长时程任务（制作汉堡）上成功的模型执行序列，展示了11个阶段中的关键帧。据作者所知，这是首个在真实机器人上成功完成如此长时程灵巧操作任务的系统，持续运行超过2.5分钟。</p>
</blockquote>
<p><img src="https://i.imgur.com/table2.png" alt="人类评分对比表"></p>
<blockquote>
<p><strong>表2</strong>：在“制作汉堡”长时程任务上各阶段的人类标准化评分（HNS）对比。基线方法（w/o Touch）在第5和第11阶段容易失败，导致整体任务成功率几乎为0。而ViTacFormer在所有阶段均表现出色，在该任务上取得了超过80%的成功率。</p>
</blockquote>
<p><strong>消融实验</strong>：为了验证各组件有效性，在短时程任务上进行了消融研究。</p>
<p><img src="https://i.imgur.com/7x8dY9z.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果对比。每个组件都在基线（无触觉输入 w/o Touch 和简单触觉融合 w/ Touch）的基础上提升了性能。具体而言：引入视觉-触觉交叉注意力（w/ CrossAttention）极大改善了模仿性能；预测下一触觉令牌（w/ NextTouchPred）提升了操作稳定性；自回归地利用预测的未来触觉令牌（w/ AutoRegressive）简化了动作推理过程。</p>
</blockquote>
<p><strong>失败案例分析</strong>：论文还分析了基线方法的典型失败案例。</p>
<p><img src="https://i.imgur.com/2s3fJpK.png" alt="失败案例分析"></p>
<blockquote>
<p><strong>图7</strong>：基线方法的失败案例分析。插桩失败源于机械手未感知孔洞位置，凸显了预测未来触觉信号的重要性；拧瓶盖失败源于机械手无法判断瓶盖开合状态，凸显了自回归架构的重要性；翻书失败源于缺乏视觉与触觉的深度融合，凸显了交叉注意力融合的重要性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了ViTacFormer，一个通过交叉注意力融合视觉触觉、并利用自回归触觉预测进行深度跨模态表征学习的统一框架。2) 设计了两阶段课程学习策略，有效稳定了基于预测触觉信号的策略训练。3) 在真实机器人上构建了全面的视觉-触觉灵巧操作基准，并首次实现了包含11个序列阶段的超长时程灵巧操作任务，性能大幅超越现有方法。</p>
<p><strong>局限性</strong>：1) 受限于模仿学习范式，策略无法自主泛化到新任务，仍需依赖人类遥操作收集数据。2) 在触觉反馈不那么关键的场景中，由于传感器噪声和表征学习过程的限制，其稳定性可能会受到影响。</p>
<p><strong>后续启示</strong>：本工作展示了深度跨模态融合与前瞻性感知在灵巧操作中的巨大潜力。未来的研究可探索如何减少对高质量演示数据的依赖（例如结合强化学习），以及如何进一步提升模型在触觉信息稀疏或噪声更大场景下的鲁棒性。ViTacFormer的框架也为整合其他模态（如听觉）提供了参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人灵巧操作中视觉与触觉信息融合不足、难以实现精细控制的问题，提出ViTacFormer模型。其核心是采用交叉注意力编码器融合高分辨率视觉与触觉信号，并引入自回归触觉预测头来预判未来接触状态。通过“从易到难”的两阶段课程学习策略，先使用真实触觉稳定训练，再切换至预测信号以提升鲁棒性。实验表明，该方法在真实世界任务中比之前最优系统的成功率提升约50%，并首次实现了长达2.5分钟、包含11个连续阶段的拟人化手长时程灵巧操作。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.15953" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>