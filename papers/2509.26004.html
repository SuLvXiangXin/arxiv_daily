<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.26004" target="_blank" rel="noreferrer">2509.26004</a></span>
        <span>作者: Messina, Nicola, Leonardi, Rosario, Ciampi, Luca, Carrara, Fabio, Farinella, Giovanni Maria, Falchi, Fabrizio, Furnari, Antonino</span>
        <span>日期: 2025/09/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，从第一人称（自我中心）视角检测和分割用户手持物体的任务，对于理解人机交互至关重要，应用广泛。然而，该领域的发展主要受限于高质量像素级标注数据的稀缺性，因为现有方法大多依赖成本高昂的全监督学习。本文针对这一具体痛点，提出利用大规模自我中心视频数据集中广泛存在且获取成本相对较低的“叙述”（即描述用户行为的自然语言文本）作为弱监督信号。本文的核心思路是：提出名为NS-iHOS的新任务，旨在仅使用训练时的配对叙述作为监督，学习从自我中心图像中分割手持物体；并设计了WISH模型，通过两阶段架构（名词-物体对齐、手-物体匹配）从叙述中蒸馏知识，最终实现无需叙述的、纯视觉推理的手持物体分割。</p>
<h2 id="方法详解">方法详解</h2>
<p>WISH模型是一个端到端的弱监督框架，其核心目标是在训练阶段利用叙述文本的语义信息，学习将图像中的物体与用户的手进行正确关联，并在测试时仅凭图像完成分割。</p>
<p><img src="https://arxiv.org/html/2509.26004v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：WISH架构总览。模型共享一个视觉主干。(a) 对象分割器和基于CLIP的主干为所有对象和手部区域提取视觉嵌入。(b) 第一阶段，学习一个共享嵌入空间，将叙述中与手相关的名词短语与其对应的视觉对象嵌入对齐。(c) 第二阶段，利用第一阶段的对齐关系生成伪标签，训练两个专用头：接触头(C)和匹配头(M)。测试时，仅使用主干和第二阶段进行无需叙述的手持物体分割。</p>
</blockquote>
<p><strong>整体流程</strong>：输入是一张自我中心图像及其对应的自然语言叙述。流程分为两个阶段：1) <strong>第一阶段（对齐）</strong>：利用CLIP等视觉语言模型，学习一个共享的嵌入空间，使得图像中检测到的物体视觉特征能与叙述中提及的、并经过手部信息增强的名词短语文本特征对齐。2) <strong>第二阶段（蒸馏与学习）</strong>：基于第一阶段学到的对齐关系，为每张图像生成“哪些物体被哪只手持有”的伪标签。利用这些伪标签训练两个轻量级的预测头，使其学会直接从视觉特征中判断手-物交互关系。测试时，丢弃第一阶段和叙述，仅使用第二阶段的两个预测头对输入图像进行推理。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉与文本表示</strong>：视觉方面，使用基于CLIP的图像编码器，并采用掩码引导的注意力机制，仅关注分割出的物体和手部区域，提取特征后通过可学习的MLP适配器进行投影。文本方面，从叙述中提取名词短语，并使用模板（如“[名词] in contact with the left hand”）将其增强为手部特定的短语，再通过CLIP文本编码器和另一个可学习的MLP适配器得到文本嵌入。</li>
<li><strong>第一阶段：手部特定对齐</strong>：该阶段目标是优化视觉和文本适配器的参数。关键挑战在于没有物体-短语配对的真实标签。为此，模型采用噪声对比估计（NCE）框架。对于一批图像-叙述对，计算每张图像中所有物体视觉嵌入与每个叙述中所有手部特定短语文本嵌入的余弦相似度矩阵。基于“每只手最多与一个物体交互”的假设，对每个文本嵌入（短语），取其与所有物体视觉嵌入的最大相似度，然后对所有短语的最大相似度取平均，得到图像-叙述对的整体相似度分数。NCE损失鼓励配对图像-叙述的相似度高于非配对组合，从而隐式地学习正确的对齐。</li>
<li><strong>第二阶段：手-物体交互学习</strong>：此阶段利用第一阶段训练好的对齐模块为训练数据生成伪标签，并训练两个专用头：<ul>
<li><strong>伪标签生成</strong>：对于匹配的图像-叙述对，计算其相似度矩阵。对于每个物体i和每只手k，取与该手相关的所有名词短语的最大相似度得分。通过动态阈值（取批次内得分的γ百分位数）生成二元“接触伪标签”。同时，为每只手选择得分最高的物体作为“匹配伪标签”。</li>
<li><strong>接触头</strong>：一个MLP，接收物体和手的特征，输出一个标量logit，经过sigmoid后表示该手与该物体接触的概率。使用焦点损失（Focal Loss）在接触伪标签上进行训练。</li>
<li><strong>匹配头</strong>：另一个MLP，同样接收物体和手特征，输出logits。对每只手，在所有物体logits上应用softmax，得到该手与每个物体匹配的概率分布。使用交叉熵损失在匹配伪标签上训练，该头负责落实“每只手最多匹配一个物体”的约束。</li>
</ul>
</li>
<li><strong>训练与推理</strong>：总损失是第一阶段NCE损失、第二阶段匹配损失和接触损失的加权和。模型进行端到端训练。推理时，对于输入图像，使用匹配头为每只手选择最匹配的物体，同时用接触头判断该匹配是否真的发生接触（概率&gt;0.5），从而输出最终的手持物体分割掩码。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，WISH的创新性主要体现在：1) 首次系统性地利用大规模自我中心数据集中现成的叙述文本作为弱监督信号，定义了NS-iHOS新任务。2) 设计了两阶段蒸馏架构，第一阶段通过NCE损失在缺乏明确配对的情况下学习鲁棒的语言-视觉对齐；第二阶段设计了互补的接触头和匹配头，共同从伪标签中学习精确的手-物交互关系，并在测试时摆脱对文本的依赖。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与基准</strong>：实验在EPIC-Kitchens（使用VISOR标注）和Ego4D（使用EgoHOS标注）两个大型自我中心数据集上进行。训练集使用图像-叙述对，测试集使用带有像素级真实分割掩码的图像。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li>**F (全监督)**：在真实像素掩码上微调的SOTA手-物分割模型（HOS）。</li>
<li>**Z (零样本)**：结合开放词汇检测器（如SAM、Grounded-SAM）和视觉语言模型（如LLaVA），以零样本方式推断交互。</li>
<li>**O (Oracle，仅EK100)**：在推理时也能访问叙述的“先知”方法，用于生成伪标签。</li>
<li>**D (蒸馏)**：用上述Oracle方法生成的伪标签来训练全监督HOS模型。</li>
<li>**W (Ours)**：本文提出的WISH模型。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.26004v2/x4.png" alt="结果表格"></p>
<blockquote>
<p><strong>图4</strong>：在EPIC-Kitchens和Ego4D数据集上的定量结果（mIoU%）。表格展示了在不同类别（整体-E，左手-L，右手-R，双手-B）上的性能。WISH（W2）在无需任何像素标注或测试时叙述的情况下，显著优于所有弱监督和零样本基线，并恢复了全监督方法超过50%的性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>主要对比</strong>：在EPIC-Kitchens上，使用Grounded-SAM的WISH（W2）在整体（E）类别上达到27.66% mIoU，显著优于所有零样本（Z系列，最高20.24%）和蒸馏（D系列，最高15.05%）基线。在Ego4D上，WISH（W2）达到23.61% mIoU，同样优于所有基线。</li>
<li><strong>与全监督对比</strong>：WISH恢复了全监督方法（F1）超过50%的性能（EPIC-Kitchens: 27.66/50.29 ≈ 55%；Ego4D: 23.61/55.29 ≈ 43%），这在没有使用任何像素级标注的情况下是一个强有力的结果。</li>
<li><strong>消融实验</strong>：论文通过消融研究验证了各组件的重要性。移除第一阶段（NCE对齐）或第二阶段任一损失（匹配损失或接触损失）都会导致性能显著下降，证明了整体架构设计的必要性。特别是，接触头和匹配头的组合被证明是有效的。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.26004v2/x3.png" alt="定性结果"></p>
<blockquote>
<p><strong>图3</strong>：WISH的定性结果示例。展示了模型能够成功分割出被左手、右手或双手持有的不同物体，即使在复杂场景和不同物体类别下也表现出较好的效果。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出新任务</strong>：首次形式化了NS-iHOS任务，即仅利用训练时的人类叙述作为弱监督来学习手持物体分割。</li>
<li><strong>提出新模型</strong>：设计了WISH两阶段弱监督模型，通过语言-视觉对齐和知识蒸馏，有效地从叙述中学习手-物交互知识，并实现测试时的纯视觉推理。</li>
<li><strong>实验验证</strong>：在两大基准数据集上验证了方法的有效性，性能显著优于多种基线，并展示了达到全监督方法一半以上性能的潜力，为减少对昂贵标注的依赖提供了可行路径。</li>
</ol>
<p><strong>局限性</strong>：论文提到，WISH的性能在一定程度上依赖于底层开放词汇物体检测器（如SAM、Grounded-SAM）的质量。如果检测器未能检测到相关物体，则后续的交互推理无法进行。</p>
<p><strong>后续启示</strong>：这项工作为自我中心视觉中的弱监督学习开辟了新方向。后续研究可以探索：1) 使用更强大的视觉语言模型或检测器来提升初始物体感知能力；2) 将方法扩展到更复杂的交互类型或动态视频序列；3) 结合少量标注数据或合成数据以进一步提升性能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自我中心视角下手持物体分割任务标注数据稀缺的问题，提出利用人类动作叙述作为弱监督信号。核心方法是提出NS-iHOS任务并设计WISH模型，该模型通过从叙述中蒸馏知识来学习手-物体关联，在测试时仅需图像输入即可分割手持物体。实验在EPIC-Kitchens和Ego4D数据集上进行，WISH超越了基于开放词汇检测器和视觉语言模型的基线方法，在不使用像素级标注的情况下，恢复了全监督方法超过50%的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.26004" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>