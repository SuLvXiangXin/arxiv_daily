<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.22195" target="_blank" rel="noreferrer">2509.22195</a></span>
        <span>作者: Hancock, Asher J., Wu, Xindi, Zha, Lihan, Russakovsky, Olga, Majumdar, Anirudha</span>
        <span>日期: 2025/09/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通过在机器人遥操作数据上微调视觉语言模型（VLM）来创建视觉语言动作模型（VLA），是训练通用机器人策略的主流范式。然而，这种方法存在一个根本性的权衡：学习生成动作通常会削弱 VLM 的基础推理和多模态理解能力，从而阻碍其在新场景下的泛化、指令遵循和语义理解。本文认为，这种灾难性遗忘是由于 VLM 的互联网规模预训练语料库与机器人微调数据之间的分布不匹配造成的。本文针对这一痛点，提出了一种新视角：在数据层面解决这种不匹配，即将低级动作用自然语言表示，使其与 VLM 的预训练表示空间对齐。这使得仅使用低秩适应（LoRA）微调 VLM 成为可能，从而最大限度地保留其原始能力。本文的核心思路是：通过将动作表示为语言来对齐数据分布，进而仅使用 LoRA 微调 VLM，以避免灾难性遗忘。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLM2VLA 的整体框架是一个数据管道和训练方法，旨在将机器人轨迹数据转化为自然语言描述，然后用 LoRA 微调 VLM，使其具备动作生成能力，同时保留基础能力。</p>
<p><img src="https://arxiv.org/html/2509.22195v1/x4.png" alt="方法流程"></p>
<blockquote>
<p><strong>图4</strong>：VLM2VLA 将现有机器人数据集 𝒟_rob 标注为自然语言描述数据集 𝒟_lan 的流程。使用 Gemini 2.5 将每个轨迹分解为具有关联子任务、运动计划和动作块的子轨迹。</p>
</blockquote>
<p>核心模块是分层动作表示和基于此的数据标注流程。</p>
<ol>
<li><strong>分层动作表示</strong>：VLM2VLA 将动作预测构建为一个三阶段的视觉问答（VQA）分层推理过程：<ul>
<li>**高层子任务预测 (l_i)**：给定观测 o_i 和语言指令 L，模型首先描述完成主任务所需的即时子任务（如“移动到物体”）。</li>
<li>**中层运动规划 (m_i)**：在当前子任务和观测条件下，模型生成一个关于机器人末端执行器的、具有空间信息的运动计划（如“主要向下并稍向右移动”），仅描述方向性移动。</li>
<li>**低级动作生成 (ā_i)**：在当前子任务和运动计划条件下，策略生成一个可变长度的动作块 ā_i 以直接在机器人上执行。动作块是一个列表的列表，其中每个内部列表包含表示为文本的、针对机器人每个自由度（DoF）的单个命令。本文仅考虑平移自由度。<br>形式上，VLA 需要建模的分布是 p_θ(ā_i, m_i, l_i | o_i, L)，并分解为三个条件概率的乘积（如论文公式1所示）。</li>
</ul>
</li>
<li><strong>数据标注流程</strong>：为了教会 VLM 上述空间锚定的推理链，需要将现有的机器人轨迹数据集重新标注为自然语言。流程如图4所示：使用 Gemini 模型自动将每个原始轨迹 τ（状态-动作序列）分解为 N 个步骤，并为每个步骤 i 生成初始观测 o_i、子任务 l_i、运动计划 m_i 和动作块 ā_i，从而构建新的自然语言标注数据集 𝒟_lan。这实质上将机器人控制问题转化为标准的监督微调任务。</li>
<li><strong>训练</strong>：使用交叉熵损失，将 LoRA 应用于 VLM（Gemma-3-12B-IT）的所有线性模块进行微调。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：</p>
<ul>
<li><strong>动作完全语言化</strong>：不同于将动作映射到最不可能令牌（Token）或使用独立的动作解码器，VLM2VLA 将高、中、低所有层级的动作都表示为自然语言，完全在 VLM 现有词汇表内。</li>
<li><strong>纯 LoRA 微调</strong>：由于数据分布对齐，无需进行昂贵的共训练或复杂的训练方案，仅通过参数高效的 LoRA 即可有效微调，这是避免灾难性遗忘的关键。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.22195v1/x3.png" alt="动作表示对比"></p>
<blockquote>
<p><strong>图3</strong>：在机器人遥操作数据上微调前，Gemma-3-12B-IT 模型下的动作概率分布。模型为表示为语言的动作分配了显著更高的对数概率，相比通过显式令牌化修改（如最不可能令牌分配）定义的动作。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>评估指标</strong>：使用多个视觉问答（VQA）基准评估多模态理解能力，并在真实机器人（6-DoF WidowX 250S 机械臂）上进行超过800次实验评估操控性能。</li>
<li><strong>Baseline方法</strong>：对比了两种基于令牌化的先进 VLA：OpenVLA（在大型 Open-X-Embodiment 数据集上微调）和 Embodied Chain-of-Thought (ECoT，仅在 Bridgev2 上微调）。还对比了共训练的 VLA：MolmoAct 和 π_0.5。</li>
<li><strong>消融实验</strong>：创建了变体 VLM2VLA-AT，将动作数据中的数字（0-9）映射到 Gemma-3 的十个最不可能令牌的解码字符串，以对比动作表示方式的影响。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>多模态理解（VQA）</strong>：如表1所示，OpenVLA 和 ECoT 相对于原始 Prismatic VLM 出现了严重的灾难性遗忘（许多基准得分接近0）。而 VLM2VLA 在所有 VQA 基准上仅表现出轻微的性能下降，保留了基础模型超过85%的性能。VLM2VLA-AT 也取得了可比的 VQA 分数，表明 LoRA 训练方案是缓解遗忘的主要原因。在与共训练 VLAs 的比较中，VLM2VLA 表现优于 MolmoAct，而 π_0.5 在大多数基准上表现显著较低，说明共训练并非解决遗忘的保证。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.22195v1/x1.png" alt="VQA结果"></p>
<blockquote>
<p><strong>图1</strong>：VLM2VLA 方法概览。经过微调的策略保留了其预训练知识，在真实机器人操控任务中实现了强大的 VQA 性能和卓越的泛化能力。</p>
</blockquote>
<ol start="2">
<li><strong>机器人操控</strong>：<ul>
<li><strong>分布内（ID）任务</strong>：在简单的“拿起胡萝卜”和“将胡萝卜放到黄盘子上”任务中，OpenVLA 表现最佳（成功率<del>93%），这得益于其在更大数据集上的训练。VLM2VLA 取得了具有竞争力的成功率（</del>80%），表明“动作即语言”方法能够实现有效的动作预测。</li>
<li><strong>分布外（OOD）任务</strong>：在需要泛化的任务中，VLM2VLA 展现出显著优势。<ul>
<li><strong>多语言指令任务（Pick Up -T）</strong>：VLM2VLA 平均成功率高达 76.7%，而 OpenVLA 和 ECoT 完全失败（0%）。</li>
<li><strong>需要语义推理的任务（Pick Up - A）</strong>：VLM2VLA 成功率为 40%，而基线模型均为 0%。</li>
<li><strong>复合任务（Pick, Place, and Lift）</strong>：VLM2VLA 成功率为 63.3%，显著高于 OpenVLA（16.7%）和 ECoT（36.7%）。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.22195v1/x5.png" alt="机器人实验结果"></p>
<blockquote>
<p><strong>图5</strong>：VLA 在分布内（ID）和分布外（OOD）机器人操控任务上的性能对比评估。VLM2VLA 在 OOD 任务上保持了高成功率，突出了其卓越的泛化能力。</p>
</blockquote>
<ol start="3">
<li><strong>消融分析</strong>：VLM2VLA-AT 虽然在 VQA 上表现接近，但在机器人任务上泛化能力显著弱于 VLM2VLA（尤其在 OOD 任务上成功率低）。这表明，虽然 LoRA 对于保留知识是必要的，但“动作即语言”的表示方式对于下游机器人任务的泛化是至关重要的因素。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.22195v1/x6.png" alt="任务分解分析"></p>
<blockquote>
<p><strong>图6</strong>：OOD 操控任务的任务分解分析。根据模型的任务计划是否正确识别任务对象（以及任务目标，如果存在）进行评分。VLM2VLA 在任务理解上表现更优。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.22195v1/x2.png" alt="灾难性遗忘示意图"></p>
<blockquote>
<p><strong>图2</strong>：传统的 VLA 训练过程常常对机器人训练数据过拟合，牺牲了其原始推理能力以换取低级动作预测（中）。相比之下，VLM2VLA（右）保留了原始 VLM（左）的世界理解能力，使模型能够推理潜在的安全风险，而不仅仅是运动命令。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了“动作即语言”的表示方法</strong>：将低级机器人模仿数据转化为文本描述，从数据层面解决了 VLM 预训练分布与 VLA 微调数据之间的不匹配问题。</li>
<li><strong>提出了一种用于知识保留的数据重标注和训练流程</strong>：基于上述表示，构建了可扩展的流程，仅通过 LoRA 微调将 VLM 转化为 VLA，无需架构修改或共训练。</li>
<li><strong>提供了动作和推理能力的经验验证</strong>：通过大量实验证明，VLM2VLA 能有效保留 VLM 的核心能力，并在真实机器人任务中实现卓越的零样本泛化，特别是在需要开放世界语义推理和多语言指令遵循的新任务上。</li>
</ol>
<p><strong>局限性</strong>：论文提到，当前工作仅考虑了机器人的平移自由度。此外，数据标注流程依赖于 Gemini 等外部模型。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据对齐的重要性</strong>：本文证明了在微调前解决数据分布不匹配是避免灾难性遗忘的有效途径，这为其他领域的迁移学习提供了思路。</li>
<li><strong>参数高效微调的潜力</strong>：在数据对齐的前提下，简单的 LoRA 微调足以实现复杂的功能适配，挑战了依赖昂贵共训练或复杂训练方案的必要性。</li>
<li><strong>探索更复杂的动作空间</strong>：未来工作可以探索如何将更复杂的动作（如旋转、力控）也有效地表示为语言，并整合到该框架中。</li>
<li><strong>减少对外部模型的依赖</strong>：可以研究如何利用 VLM 自身或更高效的方法来自动生成高质量的动作语言标注。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决将视觉语言模型微调为视觉语言动作模型时，因数据分布不匹配导致的灾难性遗忘问题。提出的VLM2VLA范式通过将低级动作表示为自然语言，实现数据对齐，并仅使用低秩适应进行微调，最小化对主干模型的修改。实验表明，该方法在保持模型原有感知与推理能力的同时，实现了对需要开放世界语义推理及多语言指令新任务的零样本泛化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.22195" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>