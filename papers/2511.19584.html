<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Massively Multitask World Models for Continuous Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Learning Massively Multitask World Models for Continuous Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.19584" target="_blank" rel="noreferrer">2511.19584</a></span>
        <span>作者: Hansen, Nicklas, Su, Hao, Wang, Xiaolong</span>
        <span>日期: 2025/11/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，学习通用控制策略的主流方法是利用海量的人类遥操作等近专家轨迹数据集，通过监督学习训练大型策略。这种方法有两个主要局限：一是极大地限制了可用于训练的数据量，二是最终策略的性能受限于演示质量。尽管“大规模预训练+轻量强化学习（RL）”的范式在游戏和推理领域取得了成功，但连续控制领域的研究仍被单任务或严格的离线范式所主导，这强化了在线交互式强化学习在此领域难以扩展的观点。</p>
<p>本文旨在挑战这一假设，探究单个策略能否通过在线强化学习同时在数百个控制任务上进行训练。针对大规模多任务在线强化学习中探索困难、任务差异大等痛点，本文提出了Newt方法，其核心思路是：首先在少量演示上预训练一个语言条件的多任务世界模型，以获得任务感知的表示和动作先验，然后通过跨所有任务的在线交互对该模型进行联合优化。</p>
<h2 id="方法详解">方法详解</h2>
<p>Newt是一种基于模型的强化学习方法，其整体框架基于TD-MPC2算法进行扩展，以支持大规模多任务在线设置。智能体（Newt）通过与环境交互收集数据，并利用这些数据持续优化其世界模型。该世界模型以状态向量、语言指令和可选的RGB图像作为输入，并通过规划输出动作。</p>
<p><img src="https://arxiv.org/html/2511.19584v2/icons/world-model-small.png" alt="方法框架"></p>
<blockquote>
<p><strong>图6</strong>：方法整体框架。智能体（Newt）通过与多任务环境交互迭代收集数据，并在收集的数据上优化其世界模型。世界模型接收状态向量、语言指令和可选的RGB观测作为输入，并通过规划输出动作。</p>
</blockquote>
<p>核心模块包括一个自预测的（无解码器）世界模型，具体由以下组件构成：</p>
<ol>
<li><strong>语言编码器</strong>：使用冻结的CLIP文本编码器将自然语言指令编码为向量 <code>g</code>。</li>
<li><strong>图像编码器（可选）</strong>：使用冻结的DINOv2编码器将RGB图像观测编码为向量 <code>x</code>。</li>
<li><strong>状态编码器</strong>：一个MLP，将状态观测 <code>s_state</code>、图像嵌入 <code>x</code> 和语言嵌入 <code>g</code> 拼接后，编码为潜在状态表示 <code>z</code>。</li>
<li><strong>潜在动力学模型</strong>：一个MLP，根据当前潜在状态 <code>z</code>、动作 <code>a</code> 和语言指令 <code>g</code> 预测下一潜在状态 <code>z&#39;</code>。</li>
<li><strong>奖励预测器</strong>：一个MLP，预测状态-动作对的即时奖励 <code>r̂</code>。</li>
<li><strong>终端值预测器</strong>：一个MLP（Q网络），预测状态-动作对的折扣回报 <code>q̂</code>。</li>
<li><strong>策略先验</strong>：一个随机最大熵策略MLP <code>p</code>，用于预测给定潜在状态和语言指令下的最优动作。</li>
</ol>
<p>世界模型的优化目标 <code>ℒ(θ)</code> 结合了三个部分：潜在状态的自预测损失（使用L2损失）、奖励预测损失和值预测损失（后两者均使用交叉熵损失，以适应多任务下不同的奖励分布）。策略先验 <code>p</code> 的优化目标 <code>ℒ_p(θ)</code> 则结合了模型化行为克隆（Model-based BC）损失（L2损失）、最大化Q值项和策略熵正则化项。</p>
<p>与现有方法相比，Newt的创新点具体体现在对演示数据的系统性利用和算法改进上：</p>
<ol>
<li><strong>基于模型的预训练</strong>：在开始在线交互前，使用所有演示数据对世界模型的所有可学习组件进行预训练，初始化任务感知的表示和策略。</li>
<li><strong>约束规划</strong>：从预训练切换到在线RL时，初始阶段将规划器偏向预训练的策略，并线性退火该偏置，以平滑过渡。</li>
<li><strong>演示过采样</strong>：维护独立的演示和在线交互回放缓冲区，在训练更新时以1:1的比例从中采样，确保演示数据在整个训练过程中持续影响模型。</li>
<li><strong>RL策略更新中的动作监督</strong>：在策略目标中引入模型化行为克隆（BC）损失项，直接利用演示提供动作监督，并在Q值估计不准时正则化RL目标。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在本文提出的MMBench基准上进行，该基准包含来自10个任务领域的200个独特连续控制任务，包括DMControl、Meta-World、ManiSkill3、MiniArcade等。实验平台基于在线强化学习设置。</p>
<p>对比的基线方法包括：行为克隆（BC）、近端策略优化（PPO）、FastTD3、仅进行模型预训练的Newt、多任务TD-MPC2（无语言、预训练、演示和BC损失）以及200个单任务TD-MPC2代理。</p>
<p>关键实验结果如下：在总计1亿环境步数的训练后，Newt在MMBench上的整体性能优于PPO、FastTD3和TD-MPC2基线，显示出更高的数据效率和最终性能。</p>
<p><img src="https://arxiv.org/html/2511.19584v2/icons/newt-small.png" alt="主要结果"></p>
<blockquote>
<p><strong>图1</strong>：大规模多任务RL的平均得分。在跨越10个任务域的200个任务上，通过在线交互训练单个智能体（Newt）的平均分数曲线，显示了其优于基线方法的性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.19584v2/x1.png" alt="分领域结果"></p>
<blockquote>
<p><strong>图7</strong>：分领域性能。基于状态的智能体在MMBench各任务域上的平均得分，显示Newt在多个领域（如DMControl, ManiSkill）表现突出。</p>
</blockquote>
<p>消融实验验证了各个改进组件的贡献。移除语言条件、模型预训练、演示过采样或策略中的BC损失项均会导致性能显著下降，其中语言条件和模型预训练的影响最为关键。</p>
<p><img src="https://arxiv.org/html/2511.19584v2/x2.png" alt="消融实验"></p>
<blockquote>
<p><strong>图62</strong>：消融研究。逐步移除Newt的关键组件（语言、预训练、演示过采样、BC损失）对性能的影响，证实了每个组件的必要性。</p>
</blockquote>
<p>此外，实验还展示了Newt的其他能力：</p>
<ul>
<li><strong>快速适应未见任务</strong>：对训练好的Newt模型在新任务上进行少量在线RL微调，能使其性能迅速提升。</li>
<li><strong>强大的开环控制</strong>：仅根据初始状态和语言指令，Newt能规划出长达50步的有效动作序列并成功执行。</li>
<li><strong>从视觉观测中受益</strong>：当提供高分辨率图像输入时，Newt的性能得到进一步提升。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了首个用于大规模多任务在线强化学习研究的基准MMBench；2) 提出了Newt方法，通过系统性地结合演示预训练和在线强化学习，证明了在数百个任务上训练单一在线强化学习智能体的可行性；3) 展示了所训练模型在任务适应、开环控制等方面的下游能力。</p>
<p>论文自身提到的局限性在于，所有RL方法（包括Newt）在MuJoCo、Box2D和Atari等领域的部分任务上表现不佳，作者推测这可能是因为这些领域的任务相对独特，与其他任务共享的底层结构较少。</p>
<p>本文对后续研究的启示是，大规模多任务在线强化学习在连续控制领域是可行且有前景的方向。它挑战了“在线RL不扩展”的固有观念，为开发更通用、更数据高效的控制智能体提供了新的算法框架和评估基准。如何更好地处理差异性极大的任务，以及如何进一步融合视觉、语言等多模态信息，是未来值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决在线强化学习在连续控制中难以扩展到大规模多任务的问题，挑战了该领域局限于单任务或离线训练的观点。提出了Newt方法，这是一个基于TD-MPC2的语言条件化多任务世界模型，先通过演示预训练获取任务感知表示和动作先验，再通过在线交互在所有任务上联合优化。实验表明，Newt在多任务性能和数据效率上优于一组强基线，具备强大的开环控制能力，并能快速适应未见任务，同时引入了包含200个任务的MMBench基准。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.19584" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>