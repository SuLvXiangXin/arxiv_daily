<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.11840" target="_blank" rel="noreferrer">2507.11840</a></span>
        <span>作者: Jiming Chen Team</span>
        <span>日期: 2025-07-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人灵巧操作是机器人学发展的核心目标，但目前性能仍远未达到人类水平。当前，基于人工智能（尤其是深度强化学习和模仿学习）的方法在机器人操作技能学习方面取得了快速进展。然而，现有的综述文献多集中于孤立地讨论学习算法本身，或主要关注简单的二指夹爪，缺乏对实现灵巧操作至关重要的数据收集范式的系统性总结，也未能全面涵盖面向多指灵巧手的具身智能操作的发展与挑战。本文针对这一痛点，提出了一个整合的视角，不仅回顾技能学习框架，更将数据收集范式视为同等重要的基石进行系统梳理。本文的核心思路是：通过梳理机器人操作从机械编程到具身智能的三个历史发展阶段，聚焦当前阶段，从数据收集（仿真、人类示范、遥操作）和技能学习（模仿学习与强化学习）两大支柱总结最新进展，并基于此提炼出制约发展的三个关键挑战。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文作为一篇综述，并未提出一个具体的技术方法，但其对领域发展脉络和现有技术体系的梳理构成了一个清晰的认知框架。其整体论述框架基于对机器人操作发展史的划分，并以此为基础分析当前技术体系。</p>
<p><img src="https://arxiv.org/html/2507.11840v2/x2.png" alt="发展历程"></p>
<blockquote>
<p><strong>图2</strong>：机器人操作的发展过程。包含三个阶段：机械编程、闭环控制和具身智能。</p>
</blockquote>
<p>整体框架遵循历史演进逻辑：<strong>第一阶段为机械编程阶段</strong>，典型如Unimate和PUMA560工业机器人，依靠预编程控制器在结构化环境中（如生产线）执行拾放操作。此阶段机器人无外部感知能力，无法适应环境变化。<strong>第二阶段为基于视觉伺服的闭环控制阶段</strong>，通过Eye-in-Hand或Eye-to-Hand相机引入视觉反馈，实现基于特征跟踪的闭环控制，使操作具备一定的任务和环境适应性，例如ABB YuMi双臂协作机器人。但此阶段控制仍依赖于对外部环境或操作对象的精确建模，在非结构化环境中表现受限。<strong>第三阶段为具身智能操作阶段</strong>，强调端到端的“感知-决策-执行”闭环。这是一个根本性的范式转变：感知上，融合视觉、力觉和触觉等多模态传感；决策上，基于实时感知反馈自主决策，摆脱精确环境建模的约束；执行上，装备多指灵巧手作为末端执行器，以获取复杂技能并实现灵活交互。该阶段是实现类人灵巧操作最有前景的技术路径。</p>
<p><img src="https://arxiv.org/html/2507.11840v2/x1.png" alt="硬件进展"></p>
<blockquote>
<p><strong>图1</strong>：用于机器人操作的末端执行器和机械臂的硬件进展。从左至右展示了从传统工业机械臂到协作机械臂、连续体机械臂、软体机械臂的演变，以及末端执行器从最简单的平行夹爪到多指夹爪、软体欠驱动手、刚性灵巧手、高度拟人全驱动手的演变。</p>
</blockquote>
<p>本文的核心分析模块围绕当前（具身智能）阶段的两大支柱展开：<strong>数据收集范式</strong>与<strong>技能学习框架</strong>。创新点体现在将数据收集提升到与学习算法同等重要的地位进行系统分类和评述，并特别强调多指灵巧手相对于简单二指夹爪的独特优势与挑战。</p>
<p>在数据收集方面，论文系统总结了三种范式：1) <strong>基于仿真平台的数据生成</strong>（如GraspM³、DexGraspNet），优势在于高效、低成本、可重复，但存在仿真到现实（Sim2Real）的鸿沟，且对复杂物体（如可变形物体）仿真能力不足。2) <strong>基于人类示范的数据收集</strong>（如Videodex、VTDexManip），利用现有人类视频或穿戴设备采集真实交互数据，能减少Sim2Real差距，但存在因机器人手与人类手形态差异导致的“人-机鸿沟”（Human-to-Robot Gap）。3) <strong>基于遥操作示范的数据收集</strong>（如Open-TeleVision、DexCap），通过遥操作系统将人类智能融入机器人操作，能有效缓解前两种鸿沟，但当前系统大多缺乏力/触觉反馈、主要针对二指夹爪，且存在延迟和敏捷性不足的问题。</p>
<p><img src="https://arxiv.org/html/2507.11840v2/x4.png" alt="人机差异"></p>
<blockquote>
<p><strong>图4</strong>：人机差异示意图。对比(a)人手、(b)Allegro手和(c)Shadow手。显然，Allegro手的手指比人手大得多，而Shadow手有一个巨大的驱动箱，这给臂-手系统带来了负面影响。</p>
</blockquote>
<p>在技能学习框架方面，论文梳理了两大类别：<strong>模仿学习</strong>和<strong>强化学习</strong>。模仿学习通过演示数据直接学习策略，包括动态运动基元、概率模型等方法。强化学习则通过智能体与环境的试错交互来学习，但在高自由度的灵巧操作中面临样本效率低、探索难度大的挑战。论文指出，结合两种范式的优势（如从演示中初始化策略再进行强化学习微调）是当前一个重要方向。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>作为综述论文，本文未进行原创性实验，但系统性地整理、对比和分析了该领域的大量代表性工作和数据。其“实验结果”体现在对现有方法、数据集和性能的归纳总结上。</p>
<p><strong>使用的基准/数据集</strong>：论文在表I和表II中列举了众多关键的数据集和学习框架。重要的数据集包括仿真生成的GraspM³（数百万抓取轨迹）、DexGraspNet（132万次Shadow手抓取数据），来自人类示范的VTDexManip（视觉-触觉数据集）、ActionSense（厨房任务多模态数据集），以及来自遥操作的Mobile ALOHA（双手移动操作数据）、DexCap（人手运动捕捉数据）等。</p>
<p><strong>对比的基线方法</strong>：在技能学习框架部分，论文对比了模仿学习中的多种方法，如DMP、ProMP、BC等，以及强化学习中的各类算法。同时，全文贯穿了对使用简单二指夹爪的方法与使用多指灵巧手的方法在能力与挑战上的对比。</p>
<p><strong>关键结果总结</strong>：</p>
<ol>
<li><strong>发展脉络清晰化</strong>：论文成功将机器人操作历史划分为三个特征鲜明的阶段，并指出具身智能是当前发展方向。</li>
<li><strong>数据收集范式评估</strong>：论文分析了三种数据收集范式的优缺点。仿真方法规模大但存在Sim2Real鸿沟；人类示范数据真实但存在人机鸿沟；遥操作能缓解鸿沟但面临反馈缺失、延迟等新挑战。</li>
<li><strong>多指灵巧手的核心地位与挑战</strong>：论文通过分析指出，多指灵巧手因其高自由度、多接触点和丰富技能潜力，是解决复杂操作挑战（如操作线性、透明、柔软物体及工具使用等）的关键，但其技能学习难度远高于二指夹爪。</li>
<li><strong>领域挑战凝练</strong>：基于以上分析，论文在结论部分提炼出三个关键挑战（见下文）。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.11840v2/x3.png" alt="操作挑战"></p>
<blockquote>
<p><strong>图3</strong>：与物理世界交互的挑战。机器人必须面对越来越复杂的被操作对象和多样化的操作类型。</p>
</blockquote>
<p>图3直观展示了机器人操作对象复杂度的提升（电缆、玻璃、布料等）和操作类型的多样化（滑动、推拉、手内操作等），这是当前研究面临的核心难题来源。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>系统性的历史梳理</strong>：首次按时间顺序将机器人操作发展划分为机械编程、闭环控制和具身智能三个阶段，并阐述了各阶段特征。</li>
<li><strong>双支柱综述视角</strong>：聚焦当前具身灵巧操作阶段，从数据收集范式和技能学习框架两个方面进行了系统性的最新进展总结，弥补了以往综述偏重算法而忽视数据基础的不足。</li>
<li><strong>关键挑战的提炼</strong>：基于现有方法分析，明确指出了制约灵巧操作发展的三个关键挑战，为未来研究指明了方向。</li>
</ol>
<p>论文自身作为综述，其局限性在于它总结而非突破现有工作。但它明确指出了领域面临的<strong>局限性</strong>，主要包括：仿真与现实的鸿沟（Sim2Real Gap）、人类与机器人的形态差异鸿沟（Human-to-Robot Gap）、以及当前遥操作系统中力/触觉反馈缺失、延迟和敏捷性不足等问题。</p>
<p>对后续研究的<strong>启示</strong>非常明确：未来的研究需要着力解决上述三大挑战。具体而言，需要在仿真真实性、跨形态技能迁移、以及高保真、低延迟的力触觉遥操作系统设计上取得突破。同时，研究应更多地聚焦于多指灵巧手这一更具普适性但也更复杂的平台，推动机器人操作向真正的类人灵巧迈进，并最终促进强人工智能的发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文综述了实现人类水平灵巧机器人操作这一核心挑战的进展。论文梳理了从机械编程到具身智能、从简单夹具到多指灵巧手的演进历程，并聚焦当前阶段，重点总结了两个关键技术方向：**灵巧操作数据收集**（通过仿真、人类演示和遥操作）与**技能学习框架**（模仿学习和强化学习）。基于对现有范式与框架的概述，论文最后总结并讨论了制约该领域发展的三个关键挑战。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.11840" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>