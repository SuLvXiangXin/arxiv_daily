<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Imitation Learning with Precisely Labeled Human Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Imitation Learning with Precisely Labeled Human Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.13803" target="_blank" rel="noreferrer">2504.13803</a></span>
        <span>作者: Song, Yilong</span>
        <span>日期: 2025/04/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，构建通用机器人模型（如视觉-语言-动作模型）面临的主要障碍是缺乏大规模、高质量的机器人演示数据。机器人演示数据收集成本高昂且耗时。因此，利用人类演示数据作为补充成为一种必要的研究方向。现有利用人类演示的方法主要分为两类：一类是“潜在动作方法”，通过自监督学习从视频中学习离散的潜在动作表示，其优势在于易于设置和可扩展性，但难以处理高维连续动作空间和细粒度运动规划；另一类是“基于流的方法”或“以物体为中心的方法”等，它们试图同时解决动作标签缺失和“具身鸿沟”（即人类与机器人身体结构差异）问题，但往往模块复杂、或对任务和物体类型有较多限制。</p>
<p>本文针对上述方法在提供<strong>精确、细粒度动作标签</strong>方面的不足，提出了一种新的视角：如果人类演示和机器人演示使用<strong>外观相同的末端执行器</strong>，并且提供<strong>具身不变的手腕摄像头视角</strong>，那么通过精确的末端执行器姿态估计，可以直接从人类视频中提取精确的连续动作标签，从而绕过具身差异，实现策略的零样本泛化。本文的核心思路是：通过为手持夹爪赋予独特颜色以便分割，并应用RANSAC和ICP配准算法进行高精度位姿估计，从而低成本地获取精确标注的人类演示数据，用于训练视觉运动策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程遵循一个简单范式：给定未标注的人类演示视频，通过末端执行器姿态估计为其添加动作标签，然后使用这些精确标注的人类演示数据（单独或与少量机器人演示数据混合）来训练一个视觉运动策略。为了在保持精度的同时兼顾便携性和成本效益，该方法仅需易于获取的硬件：一个3D打印的手持夹爪和一个或多个RGB-D相机。</p>
<p>方法的核心分为两个阶段：点云重建与末端执行器姿态估计。</p>
<p><strong>1. 点云重建</strong><br>为了精确捕捉场景的三维几何结构，特别是末端执行器的几何形状，本文部署了已知标定矩阵的外部深度相机，并对每一帧进行点云重建。具体流程是：对于每个相机视角的每一帧RGB-D图像，利用相机的内参和外参矩阵，将每个像素及其深度值反投影到全局坐标系中的三维点。将所有视角的部分点云合并，得到每一帧对应的更完整的场景点云。</p>
<p><strong>2. 末端执行器姿态估计</strong><br>这是实现精确标注的关键。本文使用一个既可用于手持也可安装在机器人上的定制末端执行器（夹爪）。其核心洞察是<strong>为夹爪赋予一个独特的、易于分割的颜色</strong>（例如绿色）。</p>
<p><img src="https://arxiv.org/html/2504.13803v1/x1.jpeg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：末端执行器姿态估计流程示意图。从左至右：1）仿真中使用的重新着色后的熊猫夹爪三角网格模型；2）从网格模型均匀采样得到的对应点云，用于姿态估计；3）仿真演示中机器人的点云（为清晰起见从场景中裁剪）；4）应用随机采样一致性（RANSAC）和迭代最近点（ICP）算法来估计一个刚体变换，将来自网格的末端执行器点云与机器人点云对齐（即得到位姿），从而实现无需训练深度学习模型的精确、可靠的末端执行器姿态估计。</p>
</blockquote>
<p>具体步骤为：</p>
<ul>
<li><strong>分割</strong>：在重建的点云中，通过颜色过滤轻松分割出属于末端执行器的点，并取最大的点簇。</li>
<li><strong>粗配准</strong>：使用RANSAC算法，在分割出的点云与夹爪CAD模型采样得到的点云之间找到一个初始的刚性变换（粗对齐）。</li>
<li><strong>精配准</strong>：以RANSAC的结果作为初始值，应用ICP算法进行迭代优化，得到精确的末端执行器姿态（位置和朝向）。</li>
<li><strong>时序优化</strong>：由于视频帧间变化小，该“RANSAC+ICP”组合只需在第一帧完整运行。对于后续帧，可以将前一帧的估计位姿作为初始粗对齐，这既提高了效率，也便于对对称末端执行器施加旋转一致性约束。</li>
</ul>
<p>通过上述流程，可以为每一帧图像标注对应的绝对末端执行器位姿。为了获得动作标签数据集，只需将位姿序列向前移动一帧，使得在帧<code>t+1</code>处估计的位姿成为帧<code>t</code>处的目标位姿。</p>
<p><strong>创新点</strong>：与需要复杂训练或专用模型的现有方法相比，本文方法创新性地利用颜色这一简单先验，结合经典、无需学习的RANSAC/ICP配准算法，实现了高精度、低成本的动作标注。该方法模块少，无需现成组件，能更精确地捕捉3D运动，并且易于与大规模模仿学习框架集成。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真环境与任务</strong>：在 robosuite 仿真环境中进行评估，选用 MimicGen 提供的任务（Square, Coffee, Stack, Threading, Three Piece Assembly），这些任务能提供较大规模的数据集。为了模拟具身鸿沟，使用 KUKA iiwa 机械臂的演示数据并经过本文位姿估计流程重新标注，来模拟“人类演示”；使用 Franka Panda 机械臂的演示数据作为“机器人演示”，并在测试时使用 Panda 机械臂。两者使用相同颜色的 Panda 夹爪。</li>
<li><strong>策略架构</strong>：下游行为克隆架构选用 Diffusion Policy（视觉扩散策略），使用其默认配置（包含一个外部视角、一个手腕摄像头视角和末端执行器位姿作为输入）。</li>
<li><strong>数据混合</strong>：实验对比了不同混合比例的训练数据性能：纯机器人演示（TD）、纯人类演示（HD）、以及固定少量TD（50条）后添加不同数量HD的混合数据。</li>
<li><strong>评估指标</strong>：在50种不同的环境初始条件下测试策略成功率。报告最佳单次测试成功率、最佳连续10个检查点的平均成功率以及这10个检查点的起始训练周期。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.13803v1/x2.jpeg" alt="实验结果"></p>
<blockquote>
<p><strong>图2</strong>：在 Square D0 任务上，使用不同数据混合比例训练时下游策略的性能曲线。该图直观展示了随着训练进行，纯人类演示（0TD+200HD）能达到接近纯机器人演示（200TD+0HD）的性能，并且在少量TD基础上添加HD能持续提升性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>（基于表I）：</p>
<ol>
<li><strong>纯人类演示 vs. 纯机器人演示</strong>：仅使用200条精确标注的人类演示（0TD+200HD）训练的策略，其平均性能（最佳测试成功率）达到了使用200条机器人演示（200TD+0HD）策略的 **88.1%**（排除异常任务Threading）。这表明精确标注的人类演示本身就能让策略获得相当高的性能。</li>
<li><strong>人类演示对机器人演示的增强效果</strong>：在已有50条机器人演示的基础上，添加人类演示能显著提升策略性能。平均而言，最佳的数据混合（50TD+400HD）策略的<strong>最高测试成功率</strong>达到了纯机器人演示（200TD+0HD）策略的 <strong>102.6%<strong>，</strong>平均最佳连续检查点成功率</strong>达到了 **104.1%**。这意味着用少量高质量机器人演示配合大量低成本人类演示，可以超越仅使用大量机器人演示的效果。</li>
<li><strong>异常任务分析</strong>：Threading任务（穿线）的结果是例外，纯人类演示效果极差（成功率2%）。论文分析认为，这是因为执行穿线动作的关键插入点在动作开始时位于手腕摄像头的视野之外。而手腕摄像头是具身不变的视角。当提供了50条机器人演示（提供了正确的具身依赖视角信息）后，再添加人类演示，Threading任务的性能也随之提升。这说明：（a）<strong>具身不变视角的存在对于零样本跨具身泛化至关重要</strong>；（b）即使存在视角遮挡，只要提供了少量同具身演示，添加精确标注的跨具身演示仍能提升策略性能。</li>
<li><strong>收敛速度</strong>：从最佳连续检查点的起始周期来看，未观察到数据集规模增大会导致收敛明显变慢的趋势。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种<strong>简单、可靠且低成本</strong>的精确标注人类演示的方法。该方法通过为末端执行器着色，结合经典的RANSAC和ICP算法，无需训练复杂模型即可实现高精度位姿估计与动作标注。</li>
<li>通过仿真实验量化证明了：<strong>a)</strong> 仅使用精确标注的人类演示，策略性能可达到使用机器人演示的约88%；<strong>b)</strong> 将精确标注的人类演示与少量机器人演示混合，能够显著提升策略的最终性能，甚至超过仅使用大量机器人演示的效果。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>缺乏真实世界实验</strong>：方法依赖于精确的点云重建，在仿真中验证有效，但真实世界的硬件限制（如深度相机噪声、光照变化）可能带来挑战。</li>
<li><strong>缺乏更大规模数据集的消融研究</strong>：实验仅在中等规模数据集上进行，未在包含更多演示或更多数据类别（用于前沿VLA模型训练）的超大规模设置下进行验证。</li>
</ol>
<p><strong>启示</strong>：</p>
<ol>
<li>为末端执行器赋予可辨识的视觉特征（如独特颜色），是一种简单而有效的先验，能极大简化跨具身数据中的动作标注问题。</li>
<li><strong>具身不变视角</strong>（如手腕摄像头）是实现零样本泛化的关键，未来工作应关注如何改善此类视角的遮挡问题以扩大任务适用范围。</li>
<li>本文方法易于集成到大规模模仿学习流程中，为构建大规模机器人训练数据集提供了一条低成本、高精度的数据扩充路径。论文最后建议，未来可探索使用生成模型自动生成精确标注的人类演示。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究模仿学习中如何有效利用人类示范数据。核心问题是解决人类示范数据动作标签不精确、本体差异大、难以与主流机器人训练流程融合的挑战。关键技术是提出一种精确标注方法：通过为手持夹爪赋予独特颜色，结合RANSAC与ICP配准算法，实现高精度的末端执行器姿态估计。核心实验结论表明，在仿真中，仅使用这种精确标注的人类示范，策略性能平均可达机器人示范的88.1%；将其与机器人示范结合，能进一步提升策略性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.13803" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>