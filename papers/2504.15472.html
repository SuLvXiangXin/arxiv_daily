<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.15472" target="_blank" rel="noreferrer">2504.15472</a></span>
        <span>作者: Jian, Pingcheng, Wei, Xiao, Liu, Yanbaihui, Moore, Samuel A., Zavlanos, Michael M., Chen, Boyuan</span>
        <span>日期: 2025/04/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人强化学习（RL）面临的核心挑战在于有效奖励函数的设计。当前主流方法包括手工设计（费时且难以捕捉高层行为）、基于专家演示的逆强化学习（数据收集成本高），以及利用大语言模型（LLM）或视觉语言模型（VLM）自动化奖励生成（难以指定如节奏、步态等细微的高层行为品质）。另一条路径是从人类或AI对轨迹对的偏好中学习，但人类标注成本高昂，而现有的基于AI反馈（RLAIF）的方法通常假设马尔可夫奖励，且仅限于低维、简单的任务。</p>
<p>本文针对“如何高效、低成本地获得能够表达复杂高层行为偏好的奖励信号”这一痛点，提出了一种新视角：将LLM深度整合到RL循环中，利用LLM直接对原始的状态-动作轨迹生成偏好标签，并以此在线训练一个奖励预测器来指导策略优化。其核心思路是：用LLM替代人类标注者，基于高层语言指令对机器人 rollout 的轨迹进行偏好评判，并利用这些偏好数据在线训练一个基于Transformer的奖励模型，从而为策略优化提供密集且符合高层行为意图的奖励信号。</p>
<h2 id="方法详解">方法详解</h2>
<p>LAPP框架旨在通过LLM生成的偏好反馈来实现偏好驱动的强化学习。其整体流程（Pipeline）包含三个核心组件：1）行为指令（用于提示LLM生成偏好标签）；2）偏好预测器训练；3）偏好驱动的强化学习。输入是高层行为语言指令和机器人与环境交互产生的原始状态-动作轨迹对，输出是优化后的机器人策略。</p>
<p><img src="https://i.imgur.com/1.png" alt="方法流程"></p>
<blockquote>
<p><strong>图1</strong>：大型语言模型辅助偏好预测（LAPP）概览。该方法接收语言行为指令，并基于原始状态-动作机器人轨迹生成偏好反馈，以指导强化学习训练。</p>
</blockquote>
<p><img src="https://i.imgur.com/2.png" alt="整体框架"></p>
<blockquote>
<p><strong>图2</strong>：LAPP整体框架。LLM基于rollout的原始状态-动作轨迹对以及高层行为指令生成偏好反馈。一个基于Transformer的奖励预测器使用这些偏好进行训练，同时优化机器人策略以最大化环境奖励和预测的偏好奖励的加权和。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>行为指令（生成偏好标签）</strong>：此模块旨在用LLM替代人类标注者。具体做法是构建一个结构化的提示（Prompt），如图3所示，包含三部分：定义LLM角色和任务目标、提供轨迹的数值状态-动作变量及其描述、规定偏好评估规则和输出格式。LLM被要求对给定的轨迹对（σ0, σ1）输出偏好标签（0,1,2,3 分别表示σ0更好、更差、同等偏好、不可比较）。为提升效率，支持批量标注（每次提示处理5对轨迹）。关键创新在于鼓励LLM生成<strong>自适应的评估标准</strong>，使其能够根据训练的不同阶段（如从学习站立到稳定移动再到优化步态）动态调整偏好侧重点，而非使用静态标准。</li>
</ol>
<p><img src="https://i.imgur.com/3.png" alt="提示示例"></p>
<blockquote>
<p><strong>图3</strong>：行为指令提示示例。提示包含三部分：定义LLM角色和任务（蓝框），指定状态变量和部分评估标准（绿框），以及建立生成偏好标签的规则和语义（紫框）。</p>
</blockquote>
<ol start="2">
<li><p><strong>偏好预测器训练</strong>：该模块负责将从LLM获得的偏好标签转化为可用于RL的密集奖励信号。预测器采用基于GPT架构的Transformer网络，包含6层掩码自注意力层。根据任务复杂度，可配置为马尔可夫（输入序列长度为1）或非马尔可夫（输入序列长度为8）奖励模型，后者能捕捉长时序依赖。训练时，将偏好数据集 D_p 按9:1划分为训练集和验证集。采用集成学习策略：训练M=9个预测器网络，每个网络通过最小化调整后的交叉熵损失（公式6，该损失考虑了LLM可能存在ε=15%的随机错误率）进行训练。为防止过拟合，设置了早停机制（当验证损失超过训练损失的α=1.3倍且迭代次数超过N_min=30时停止）。最终选择验证损失最低的C=3个预测器，以其输出的平均值作为最终的偏好奖励 r_p。具体流程见算法1。</p>
</li>
<li><p><strong>偏好驱动的强化学习</strong>：此模块将训练好的偏好预测器整合进RL循环，进行策略优化。策略π（对于四足任务为MLP，对于灵巧手任务也为MLP）使用PPO算法进行优化。每一步的总奖励r是环境奖励r_E（来自任务预设的奖励函数）和预测的偏好奖励r_p的加权和：r = β * r_p + r_E，其中β为平衡系数（大多数任务设为1.0，后空翻任务因环境奖励尺度大设为50.0）。与以往静态偏好模型不同，LAPP采用<strong>在线学习</strong>：策略 rollout 收集的新轨迹会定期（每M个epoch）提交给LLM获取新的偏好标签，并用于更新偏好数据集 D_p 和重新训练预测器（仅使用最新数据）。这种设计使得偏好标准能随训练进程动态演化。具体流程见算法2。</p>
</li>
</ol>
<p><strong>创新点</strong>：1) <strong>轨迹级LLM反馈</strong>：首次直接使用原始状态-动作轨迹（而非图像或视频）从LLM获取偏好，成本更低且能处理更复杂的任务。2) <strong>在线、自适应的偏好对齐</strong>：偏好预测器与策略同步在线更新，且LLM的评估标准可自适应训练阶段动态调整，实现了闭环的行为 refinement。3) <strong>处理非马尔可夫依赖</strong>：通过Transformer架构建模长时序轨迹，能够应对需要历史信息的复杂行为（如后空翻）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境中评估，任务包括：1) <strong>四足机器人运动</strong>：在平坦地面、楼梯、离散障碍、斜坡、波浪地形上遵循速度指令行走（图4a）。2) <strong>灵巧手操作</strong>：使用两个26-DoF的Shadow Hand完成“倒水”、“手递手传球”、“旋转杯子”任务（图4b）。3) <strong>四足后空翻</strong>：完成360度后空翻并稳定着陆（图4c）。使用GPT-4o mini作为LAPP的LLM骨干（成本约$2.5-$3/次训练），对比基线时使用GPT-4o运行Eureka。</p>
<p><img src="https://i.imgur.com/4.png" alt="仿真任务"></p>
<blockquote>
<p><strong>图4</strong>：仿真任务图示。(a) 四足运动。(b) 灵巧操作。(c) 四足后空翻。</p>
</blockquote>
<p><strong>对比基线</strong>：1) <strong>PPO</strong>：使用专家手工设计的环境奖励函数。2) <strong>RLAIF</strong>：基于AI反馈的RL，使用MLP奖励模型。3) <strong>Eureka</strong>：使用LLM进化搜索生成奖励代码。4) <strong>RL-VLM-F</strong>：使用VLM基于状态图像生成偏好反馈。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>性能与效率</strong>：如图5所示，在四足运动任务中，LAPP在训练速度（更快达到高性能）和最终性能（最高成功率）上均显著优于所有基线。在灵巧操作任务中，LAPP也达到了最高或接近最高的成功率。</li>
</ol>
<p><img src="https://i.imgur.com/5.png" alt="性能对比"></p>
<blockquote>
<p><strong>图5</strong>：与基线方法在四足运动（上）和灵巧操作（下）任务上的性能对比。LAPP在最终性能和样本效率上均优于基线。</p>
</blockquote>
<ol start="2">
<li><strong>高层行为控制</strong>：通过简单的语言指令（如“trot with a symmetric gait”），LAPP可以精确控制机器人的步态对称性和节奏，而PPO仅使用环境奖励则无法实现这种高层控制（图6）。</li>
</ol>
<p><img src="https://i.imgur.com/6.png" alt="高层行为控制"></p>
<blockquote>
<p><strong>图6</strong>：通过语言指令进行高层行为控制。LAPP能根据指令（如对称小跑）精确调节步态特征，而PPO则不能。</p>
</blockquote>
<ol start="3">
<li><strong>解决挑战性任务</strong>：在极具探索难度的四足后空翻任务中，LAPP是唯一能够成功解决的方法（最终成功率80%，见图8），而PPO、RLAIF、Eureka等方法均告失败（0%成功率）。</li>
</ol>
<p><img src="https://i.imgur.com/8.png" alt="后空翻成功率"></p>
<blockquote>
<p><strong>图8</strong>：四足后空翻任务的成功率对比。LAPP是唯一能解决此任务的方法。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>（图7）：<ul>
<li><strong>在线更新 vs. 静态预测器</strong>：在线更新偏好预测器比使用静态预测器性能更好。</li>
<li><strong>仅使用最新数据 vs. 使用全部历史数据</strong>：仅使用最新收集的偏好数据重新训练预测器，比使用全部历史数据性能更优。</li>
<li><strong>非马尔可夫 vs. 马尔可夫预测器</strong>：对于复杂任务（如后空翻），非马尔可夫预测器至关重要；对于简单任务，两者性能相近。</li>
<li><strong>集成预测器</strong>：集成预测器比单一预测器表现更稳定、更优。</li>
</ul>
</li>
</ol>
<p><img src="https://i.imgur.com/7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融研究结果。验证了在线更新、仅使用最新数据、非马尔可夫建模以及集成预测器等设计选择的有效性。</p>
</blockquote>
<ol start="5">
<li><strong>真实世界部署</strong>：将在模拟中训练好的LAPP策略迁移到真实的Unitree Go1机器人上，能够在多种地形（平坦地面、碎石、斜坡）上鲁棒运行，并成功完成后空翻（图9）。</li>
</ol>
<p><img src="https://i.imgur.com/9.png" alt="真实世界部署"></p>
<blockquote>
<p><strong>图9</strong>：真实世界部署。将LAPP训练的策略迁移到Unitree Go1机器人上，完成了多种地形运动和动态后空翻。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了LAPP框架，首次将LLM深度整合到RL循环中，利用其对状态-动作轨迹的偏好反馈来指导策略学习，实现了高效、可定制、表达性强的行为获取。2) 设计了一种在线、自适应的偏好预测器训练机制，能够动态对齐高层行为指令，并成功解决了如四足后空翻等传统方法难以完成的任务。3) 通过系统的实验和消融分析，验证了轨迹级建模、在线更新、非马尔可夫奖励以及集成学习等关键技术设计的有效性。</p>
<p><strong>局限性</strong>：论文提到，LAPP的性能在一定程度上依赖于所使用的LLM（GPT-4o mini）的质量和其提示工程的稳定性。此外，在线查询LLM和训练Transformer预测器会产生额外的计算和API成本。</p>
<p><strong>后续研究启示</strong>：LAPP为机器人学习开辟了新的方向：1) <strong>自动化奖励设计</strong>：展示了利用基础模型（LLMs）自动化提供符合人类意图的反馈的巨大潜力，可大幅减少对精细手工奖励工程的依赖。2) <strong>高层行为控制</strong>：证明了通过自然语言指令精确调控复杂、细微的机器人行为（如步态、节奏）的可行性。3) <strong>在线与持续学习</strong>：其在线偏好对齐机制为机器人在非稳态环境或面对不断演变的任务要求时进行持续学习提供了思路。未来的工作可以探索更高效的LLM交互方式、降低对提示工程的敏感性，以及将框架扩展到更广泛的机器人平台和任务领域。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LAPP框架，旨在解决强化学习中人工设计奖励函数困难、成本高且难以捕捉复杂行为细节的问题。其核心技术是利用大语言模型（LLM）自动分析原始状态-动作轨迹并生成偏好标签，进而训练在线偏好预测器以指导策略优化，实现基于高层行为指令的偏好驱动学习。实验在四足运动与灵巧操作任务上验证了LAPP的有效性，结果表明其能实现高效学习、获得更高最终性能，并能掌握如四足后空翻等高度动态的复杂技能，超越了传统奖励设计方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.15472" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>