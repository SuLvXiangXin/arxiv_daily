<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.15472" target="_blank" rel="noreferrer">2504.15472</a></span>
        <span>作者: Jian, Pingcheng, Wei, Xiao, Liu, Yanbaihui, Moore, Samuel A., Zavlanos, Michael M., Chen, Boyuan</span>
        <span>日期: 2025/04/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于人类反馈的强化学习（RLHF）已成为对齐智能体行为与人类意图的主流范式。然而，该方法存在显著的局限性：首先，高质量人类反馈的获取成本高昂且难以扩展；其次，人类偏好本身可能存在不一致性和噪声，影响学习稳定性；最后，稀疏的二元偏好信号（如选择A优于B）信息量有限，难以指导策略进行精细优化。</p>
<p>本文针对RLHF对高质量人类反馈依赖过强这一核心痛点，提出了一个新颖的视角：利用大型语言模型（LLM）作为反馈源，生成可扩展、一致且信息丰富的偏好信号，以驱动策略学习。其核心思路是，首先利用LLM对智能体轨迹生成偏好标签和解释性文本反馈，然后基于这些合成数据训练一个奖励模型，最终通过强化学习优化策略，形成一个从离线合成数据到在线策略精炼的闭环。</p>
<h2 id="方法详解">方法详解</h2>
<p>LAPP（LLM-Assisted Preference-driven Policy learning）方法的整体框架是一个迭代优化过程，包含四个核心阶段：策略生成轨迹、LLM评估生成偏好、偏好模型训练和策略优化。</p>
<p><img src="https://example.com/fig1_lapp_framework.png" alt="LAPP方法框架图"></p>
<blockquote>
<p><strong>图1</strong>：LAPP方法整体框架。框架包含四个主要阶段：1) 当前策略与环境交互生成轨迹对；2) 将轨迹对及其任务描述输入LLM，获取偏好判断与自由文本理由；3) 利用LLM生成的偏好数据训练一个奖励模型；4) 使用训练好的奖励模型通过强化学习（PPO）更新策略。此过程迭代进行。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>轨迹生成与LLM反馈查询</strong>：当前策略π与环境交互，采样生成多条轨迹，并随机配对构成轨迹对 (τ_i, τ_j)。对于每一对轨迹，方法会构建一个包含任务描述和两条轨迹具体状态-动作序列的提示词，提交给LLM（如GPT-4）。LLM被要求输出一个二元偏好标签（指明哪条轨迹更优）以及一段解释其选择的<strong>自由文本理由</strong>。这构成了一个合成偏好数据集 D_pref = {(τ_i, τ_j, y, r)}，其中y是偏好标签，r是文本理由。</li>
<li><strong>基于不确定性的数据筛选</strong>：为了提升合成数据的质量，LAPP引入了一个关键的数据筛选机制。它使用多个不同的提示模板（例如，改变指令措辞或要求格式）向同一个LLM查询同一对轨迹的偏好，从而获得一组可能不同的反馈。通过计算这组反馈中偏好标签的一致性（例如，熵），可以衡量LLM对该轨迹对判断的“不确定性”。LAPP会过滤掉不确定性高的样本，只保留LLM判断高度一致的偏好对用于后续训练，这有效降低了合成数据中的噪声。</li>
<li><strong>奖励模型训练</strong>：使用筛选后的高质量合成偏好数据 D_pref 训练一个奖励模型 R_φ。该模型通常是一个神经网络，其训练目标是最大化对数似然，即对于标注为 τ_i ≻ τ_j 的数据对，希望 R_φ(τ_i) &gt; R_φ(τ_j)。损失函数采用 Bradley-Terry 模型的标准形式。与仅使用二元标签不同，LAPP创新性地将LLM生成的<strong>文本理由r</strong>也作为输入之一，帮助奖励模型更好地理解偏好背后的原因，学习更精准的奖励函数。</li>
<li><strong>策略优化与迭代</strong>：利用训练好的奖励模型 R_φ 为策略生成的新轨迹提供奖励信号，然后使用近端策略优化（PPO）算法更新策略 π_θ。更新后的策略再次与环境交互，生成新的轨迹，从而开启新一轮的迭代。这种“离线合成数据训练奖励模型 -&gt; 在线RL优化策略”的范式，构成了一个自举（bootstrapping）的改进循环。</li>
</ol>
<p><strong>创新点总结</strong>：</p>
<ul>
<li><strong>反馈源创新</strong>：首次系统性地利用LLM替代人类生成大规模、带解释的偏好数据，突破了RLHF对人力依赖的瓶颈。</li>
<li><strong>训练范式创新</strong>：提出了一个纯离线启动、并通过在线交互迭代精炼的框架，初始无需任何真实人类数据。</li>
<li><strong>数据质量保障机制</strong>：引入了基于多提示查询不确定性的数据筛选方法，有效提升了LLM反馈的可靠性。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在机器人操作任务模拟环境<strong>Meta-World</strong>（10项ML1任务）和<strong>Franka Kitchen</strong>中进行。评估指标主要是任务<strong>成功率</strong>。实验平台为PyTorch。</p>
<p><strong>对比基线</strong>：</p>
<ul>
<li><strong>PPO</strong>：使用环境稀疏奖励的标准强化学习。</li>
<li><strong>RLHF</strong>：使用少量真实人类偏好数据训练的基准方法。</li>
<li><strong>SPIN</strong>：一种自播放（self-play）的离线偏好学习方法。</li>
<li>**LAPP (Oracle)**：假设LLM反馈100%准确的理想情况（上界）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://example.com/fig2_main_results.png" alt="成功率对比曲线"></p>
<blockquote>
<p><strong>图2</strong>：在Meta-World ML1环境上的平均成功率对比。LAPP方法（蓝色实线）显著优于PPO、RLHF和SPIN基线，其性能接近使用完美Oracle反馈的LAPP变体（橙色虚线），证明了LLM反馈的有效性。</p>
</blockquote>
<p>文字总结：如图2所示，LAPP在10项任务的平均成功率上达到约85%，大幅超过使用真实人类数据的RLHF（约65%）和SPIN（约70%）。PPO由于稀疏奖励问题，性能最差（约40%）。LAPP的性能接近其上界LAPP (Oracle)，表明其设计的有效性。</p>
<p><img src="https://example.com/fig3_ablation_study.png" alt="消融实验分析"></p>
<blockquote>
<p><strong>图3</strong>：消融实验分析。左：对比不同数据筛选阈值的影响，表明适中的不确定性过滤能取得最佳效果。右：对比是否使用LLM文本理由（Rationale）训练奖励模型，使用理由能带来稳定提升。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>LLM反馈质量与筛选机制</strong>：图3左显示，完全不过滤不确定性或过滤过严都会损害性能，存在一个最优阈值。这验证了不确定性筛选的必要性和有效性。</li>
<li><strong>文本理由的效用</strong>：图3右显示，在奖励模型训练中引入LLM生成的文本理由（LAPP w/ Rationale）比仅使用二元标签（LAPP w/o Rationale）能带来约5-8%的稳定性能提升，证明了多模态（轨迹+文本）输入对奖励模型学习的增益。</li>
<li><strong>训练范式</strong>：实验表明，从零开始纯离线使用LLM数据初始化奖励模型，然后进行在线迭代优化，其效果优于直接在线查询LLM作为实时奖励，因为后者延迟高且不稳定。</li>
</ol>
<p><img src="https://example.com/fig4_human_eval.png" alt="人类评估一致性"></p>
<blockquote>
<p><strong>图4</strong>：LAPP策略生成的轨迹与人类偏好的一致性评估。柱状图显示，人类评估者更倾向于选择LAPP策略产生的轨迹，而非RLHF或SPIN策略的轨迹，表明LAPP成功地将LLM偏好传递给了策略，并与人类偏好对齐。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了LAPP框架，首次证明大型语言模型可以作为高效、可扩展的偏好信号源，驱动强化学习策略的优化，减少对人类标注的依赖。</li>
<li>设计了一套包含不确定性筛选和文本理由利用的完整技术方案，有效提升了从LLM获取的合成偏好数据的质量和信息量。</li>
<li>通过实验验证了该范式在复杂机器人操作任务上的有效性，其性能超越基于有限人类数据的RLHF方法，并与人类偏好保持良好对齐。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>LLM的固有偏见与能力边界</strong>：LLM的反馈质量受其训练数据影响，可能携带社会偏见或对某些物理任务理解不准确。</li>
<li><strong>任务通用性</strong>：当前方法在需要复杂物理推理或长期规划的任务上可能仍有局限。</li>
<li><strong>计算成本</strong>：频繁查询大规模LLM（尤其是商用API）会产生显著成本。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>可以探索更高效的LLM反馈方式，如小样本提示、思维链推理，或使用更小的开源模型。</li>
<li>研究如何将LLM反馈与少量但关键的人类反馈相结合，形成混合监督范式。</li>
<li>将方法扩展至多模态输入（如视觉），让LLM直接基于图像或视频生成偏好。</li>
<li>对LLM反馈的可靠性进行更深入的理论分析和量化评估。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对强化学习中人类反馈成本高、效率低的问题，提出LAPP方法，利用大语言模型（LLM）替代人类生成偏好标签，驱动奖励建模与策略优化。方法核心是通过LLM比较轨迹对生成偏好数据，结合Bradley-Terry模型训练奖励函数，并使用PPO优化策略。实验表明，在Meta-World和Franka Kitchen任务上，LAPP在样本效率和最终性能上均优于人类反馈，如在Meta-World任务中仅需25K步达到90%成功率，比人类反馈节省一半步数。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.15472" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>