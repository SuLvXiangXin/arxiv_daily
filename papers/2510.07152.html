<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.07152" target="_blank" rel="noreferrer">2510.07152</a></span>
        <span>作者: Sun, Jingkai, Han, Gang, Sun, Pihai, Zhao, Wen, Cao, Jiahang, Wang, Jiaxu, Guo, Yijie, Zhang, Qiang</span>
        <span>日期: 2025/10/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，地形感知的人形机器人运动主要受限于两种范式。第一种是基于深度图像的端到端学习方法，其设计简洁，但存在训练效率低、泛化能力差，特别是受传感器噪声和遮挡影响时模拟到现实差距大的问题。第二种是基于高程图的方法，依赖传统方法重建地形并在结构化几何空间中进行规划或控制。这类方法虽然有效，但严重依赖多个外感受传感器和精确的定位系统，导致延迟和漂移，降低了实际部署的鲁棒性。此外，高程图方法难以处理被遮挡或盲区（如沟壑），通常需要引入人工规则来推断可通行性，增加了系统复杂性。</p>
<p>本文旨在统一解决上述挑战，提出一个结合结构化地形推理与端到端强化学习的新框架。核心思路是：利用预训练的高程图感知来引导强化学习（仅需最小化视觉输入），通过跨注意力Transformer从噪声和部分深度图像中重建结构化地形表示，并采用逼真的深度合成方法（包含自遮挡感知和噪声建模）来显著缩小模拟到现实差距，从而实现高效策略训练与泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架紧密集成了三个关键组件：1) 具有盲骨干的地形感知运动策略；2) 用于地形重建的多模态跨注意力Transformer；3) 逼真的深度图像合成方法。整体采用教师-学生蒸馏范式进行训练。</p>
<p><img src="https://arxiv.org/html/2510.07152v2/x1.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：所提出的人形机器人感知运动教师-学生蒸馏框架概览。（A）学生策略与环境交互生成动作，同时教师策略通过ℒ2损失提供监督。蒸馏过程将运动技能迁移到多样地形上。（B）重建模块通过基于Transformer的管道整合本体感受历史和深度信息，以细化地形表示Ĥ_t^refine，并由真实值监督。细化后的特征被输入学生策略，该策略进一步通过蒸馏和对抗性强化学习进行优化。</p>
</blockquote>
<p><strong>1. 具有盲骨干的地形感知运动策略</strong><br>该策略采用师生架构。盲策略π_blind以本体感受状态和用户命令为输入（不含地形高度图），输出关节动作a_t^blind∈ℝ^20，提供稳定的基线运动控制器。感知策略π_perc则额外输入地形高度图h_t和盲动作a_t^blind，输出调制关节动作a_t^mod∈ℝ^20，以及对步态相位和速度命令的残差调整。最终关节动作为盲动作与调制动作的凸组合：a_t = (1-α)a_t^mod + αa_t^blind。步态相位增量Δφ_t和修正后的前进速度v_t^x,mod则由感知策略输出的残差经过裁剪后与基线值相加得到。这种结构使感知策略能够在简单环境下保持鲁棒性，同时有效适应不规则地形。</p>
<p>奖励函数在经典步态相位奖励基础上，专门为人形步态设计了新项，如“绊倒”和“摆动期绊倒”惩罚项，总奖励为r = ω_i r_i + Σ_{j∈𝒯} ω_j r_j（具体项见表I）。训练采用教师-学生蒸馏：首先为特定地形（如沟壑、楼梯）训练拥有特权信息的专家策略；学生策略训练时，使用专家动作通过ℒ2损失进行监督模仿，同时结合PPO进行联合优化，使学生能直接根据部分和带噪声的观察端到端地映射到动作。</p>
<p><strong>2. 多模态跨注意力Transformer地形重建器</strong><br>该模块以本体感受历史s_{t-50:t}^perc和时间深度观测d_t^H（H=5）为输入，预测机器人局部坐标系下的周围高度图。深度图像先通过卷积编码器压缩为空间特征，本体感受历史被嵌入为捕获机器人运动状态的潜在向量。两种模态通过跨注意力Transformer融合，其中本体感受嵌入作为查询（Q），深度嵌入提供键（K）和值（V）：z_t^fused = Attn(Q=z_t^prop, K=z_t^depth, V=z_t^depth)。这使得重建器能根据机器人当前运动状态，有选择地强调深度流中最关键的地形特征。</p>
<p>融合后的特征通过循环记忆单元以保持时间一致性，解码器输出粗略高度图Ĥ_t^rough，并使用均方误差（MSE）损失相对于真实局部高度图H_t^gt进行监督。为解决粗略重建边缘模糊、表面不平整的问题，采用一个条件U-Net，以粗略预测和编码后的深度潜在特征为输入，输出细化重建Ĥ_t^refined = U-Net(Ĥ_t^rough, z_t^depth)。细化高度图使用L1损失进行优化，以锐化边缘并改善平坦度。总损失结合两阶段：ℒ = |Ĥ_t^rough - H_t^gt|_2^2 + |Ĥ_t^refined - H_t^gt|_1。</p>
<p><strong>3. 逼真的深度图像合成方法</strong><br><strong>合成深度</strong>：通过GPU光线追踪，从标定的针孔相机向由静态地形和铰接式机器人几何组成的动态3D三角形网格投射虚拟光线，生成深度图像。地形网格和机器人几何（通过运动学树解析所有视觉网格并在刚体层面聚合）被组合用于光线追踪相交计算，得到几何深度。</p>
<p><strong>深度域随机化与噪声模型</strong>：为缩小理想光线追踪深度与现实传感器输出的差距，引入了随机损坏模型。包括：裁剪图像边缘像素并重采样；根据参数化模型σ_z(z,θ) = a + b(z-μ_z)^2 + c/√z添加轴向加性噪声；添加与距离成比例的小型横向扰动σ_L(z) ≈ α z ξ以模拟边缘增厚和空间模糊；结合轴向和横向误差形成每像素不确定性σ_tot，并映射为像素级随机丢失概率，以模拟不确定性导致的损坏；额外在通过Sobel梯度检测到的几何不连续区域施加边缘条件丢失概率p_e，最终输出为应用丢失掩码后的深度D_out。这种方法统计上复现了深度突变处和大入射角处观察到的空洞模式。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在全尺寸人形机器人TienKung Ultral（20个驱动自由度）上进行，使用Orbbec 355L深度相机作为视觉传感器。控制策略使用PPO在单块NVIDIA RTX 4090 GPU上训练，地形重建器模块在Isaac Gym中使用A100 GPU训练。</p>
<p><strong>地形重建精度</strong>：表II比较了在不同地形类型上的平均绝对误差（MAE）。本文方法（Ours）在粗糙斜坡下、粗糙斜坡上、高平面和离散沟壑等多数地形上优于CNN-based和ResNet-based基线。消融实验表明，移除循环记忆单元（w/o GRU）或条件U-Net细化模块（w/o Condition）会导致误差显著增加，尤其是在复杂地形上，验证了这两个组件的有效性。</p>
<p><img src="https://arxiv.org/html/2510.07152v2/x2.png" alt="消融实验"></p>
<blockquote>
<p><strong>图2</strong>：在四种挑战性地形（楼梯上、沟壑、楼梯下、跨栏）上对所提框架的消融研究。报告了完整模型和三个消融变体（无多教师蒸馏、无盲骨干、无动作中的步态相位和命令适应）的成功率（柱状）和通过率（线状）。结果显示，移除关键组件会显著降低性能，特别是在通过沟壑和跨栏等复杂地形时，凸显了完整设计的有效性。</p>
</blockquote>
<p><strong>运动性能消融研究</strong>：图2展示了在楼梯、沟壑、跨栏等挑战性地形上的成功率与通过率。完整模型性能最佳。移除多教师蒸馏、盲骨干或动作中的步态相位与命令适应组件，均会导致性能下降，尤其是在高难度地形（如沟壑和跨栏）上通过率显著降低，证明了框架各核心组件的必要性。</p>
<p><img src="https://arxiv.org/html/2510.07152v2/x3.png" alt="模拟与真实世界运动结果"></p>
<blockquote>
<p><strong>图3</strong>：在模拟（左）和真实世界（右）中，人形机器人在各种复杂地形上成功运动的定性结果展示。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.07152v2/x4.png" alt="深度图像与重建高度图可视化"></p>
<blockquote>
<p><strong>图4</strong>：合成深度图像（左列）、真实深度图像（中列）及对应的重建高度图（右列）可视化对比，显示了方法处理噪声和遮挡的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.07152v2/x5.png" alt="噪声模型消融"></p>
<blockquote>
<p><strong>图5</strong>：逼真深度合成中噪声模型的消融研究。对比了无噪声、仅轴向噪声、轴向+横向噪声、以及完整模型（增加边缘丢失）下的地形重建MAE。完整噪声模型取得了最佳性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.07152v2/x6.png" alt="不同地形下的高度图重建对比"></p>
<blockquote>
<p><strong>图6</strong>：在不同地形（平坦、斜坡、楼梯、沟壑）上，真实高度图、粗略重建高度图和细化重建高度图的对比。细化重建显著改善了边缘清晰度和表面平整度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.07152v2/x7.png" alt="模拟到现实转移性能"></p>
<blockquote>
<p><strong>图7</strong>：在真实世界地形（铺路石板、楼梯、斜坡）上，使用合成数据训练的策略与使用真实数据微调后的策略的性能对比。微调后成功率显著提升，证明了所提深度合成与端到端微调框架对缩小模拟到现实差距的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.07152v2/x8.png" alt="机器人硬件与实验场景"></p>
<blockquote>
<p><strong>图8</strong>：实验使用的全尺寸人形机器人TienKung Ultral硬件平台，及其在真实户外不平整地形上运动的场景。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个用于基于深度感知的人形机器人运动的多阶段训练框架，支持基于预训练策略进行端到端微调，且不依赖外部定位系统；2) 引入了一个跨模态Transformer，能够从部分深度和本体感受输入中重建地形几何；3) 开发了一种逼真的深度图像合成方法，通过模拟遮挡感知和噪声损坏的深度图像来实现高效且真实的策略训练。</p>
<p>论文提及的局限性包括对深度相机校准和噪声模型准确性的依赖。未来的工作可探索更自适应的在线噪声估计和校准方法。</p>
<p>本研究对后续工作的启示在于，通过紧密集成逼真的感知模拟、基于注意力的多模态地形理解以及利用预训练先验的蒸馏学习，能够有效解决人形机器人感知运动中数据效率、模拟到现实差距和系统鲁棒性等核心挑战，为实现复杂非结构化环境中敏捷、自适应的人形机器人运动提供了可行的技术路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文针对地形感知人形机器人运动的两大范式局限——基于深度图像的端到端学习存在训练效率低、模拟到现实差距大，而基于高程图的方法依赖多传感器导致延迟和鲁棒性差——提出DPL框架。其关键技术包括：盲主干地形感知运动策略，利用预训练高程图感知指导强化学习；多模态交叉注意力Transformer，从噪声深度图像重建结构化地形；逼真深度图像合成方法，通过自遮挡感知光线投射和噪声建模合成深度观测。实验表明，地形重建误差降低超过30%，并在全尺寸人形机器人上实现了多样挑战地形上的敏捷自适应运动。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.07152" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>