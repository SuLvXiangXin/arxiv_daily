<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Benchmarking Vision, Language, &amp; Action Models in Procedurally Generated, Open Ended Action Environments - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Benchmarking Vision, Language, &amp; Action Models in Procedurally Generated, Open Ended Action Environments</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.05540" target="_blank" rel="noreferrer">2505.05540</a></span>
        <span>作者: Guruprasad, Pranav, Wang, Yangyue, Chowdhury, Sudipta, Sikka, Harshvardhan, Liang, Paul Pu</span>
        <span>日期: 2025/05/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作（VLA）模型通过整合视觉感知、语言理解和动作执行，代表了迈向通用机器人系统的重要一步。然而，对这些模型的系统性评估，特别是它们在过程生成、分布外（OOD）环境中的零样本泛化能力，仍然有限。当前模型在零样本迁移能力上常常遇到困难，尤其是在面对与训练数据差异巨大的新颖场景或任务时。这一局限在过程生成环境（如Procgen基准测试）中尤为明显，这些环境旨在多变和不可预测的设置中测试视觉理解、决策和动作生成能力。现有评估主要集中于特定领域（如机器人操作或网页交互），缺乏一个在过程生成的数字环境中统一评估通用VLA模型的基准。本文针对这一痛点，提出了MultiNet v0.2基准，旨在从零样本OOD泛化能力、架构设计影响、动作空间表示和任务复杂性等多个维度，系统评估和分析最先进的VLM和VLA模型。本文的核心思路是：在一个统一的过程生成数字环境（Procgen）框架下，对多种前沿VLA/VLM模型进行系统性的零样本性能评测，以揭示其泛化能力的现状、瓶颈及影响因素。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的MultiNet v0.2并非一个全新的算法模型，而是一个全面的基准测试框架，用于评估现有VLA/VLM模型的零样本OOD泛化能力。其整体流程包括：数据准备、模型适配、推理执行和指标计算。</p>
<p><strong>1. 数据集与数据预处理：</strong><br>研究使用Procgen基准测试中的16个子数据集（如Coinrun, Starpilot, Heist等）。数据源为Facebook公开的专家强化学习（RL）智能体离线轨迹。为了便于处理，这些轨迹被转换为TensorFlow Datasets（TFDS）格式。从每个子数据集中随机采样10%的片段作为测试集，确保评估反映模型的真实泛化能力，不受训练数据污染。</p>
<p><strong>2. 评估模型：</strong><br>研究评估了五类最先进的模型：</p>
<ul>
<li><strong>GPT-4o 和 GPT-4.1</strong>：作为通用视觉语言模型（VLM）的代表。</li>
<li><strong>OpenVLA</strong>：一个开源的视觉-语言-动作模型。</li>
<li><strong>Pi0 Base 和 Pi0 FAST</strong>：基于扩散模型的动作预测模型。</li>
</ul>
<p>所有模型均使用完整的预训练权重，未进行量化。为了适应Procgen任务，需要对各模型进行特定的适配：</p>
<ul>
<li><strong>GPT-4o/4.1</strong>：使用Genesis提示工程框架，将原始轨迹数据（图像和元数据）转换为结构化的文本描述，以引导模型生成可分析的输出。</li>
<li><strong>OpenVLA</strong>：直接输入每时刻的视觉观察（图像）和一行任务描述文本。将自回归步骤限制为1，以获得单维动作向量预测，并使用数据集统计信息进行反归一化和离散化（取整）。</li>
<li><strong>Pi0 Base</strong>：将动作范围限制为1个时间步，使用10步流匹配去噪。选取预测动作的第一个维度作为最终输出，并进行反归一化和离散化。</li>
<li><strong>Pi0 FAST</strong>：将动作范围和维度均设为1以适应Procgen。将解码步骤限制为四个特定令牌（“Action”, “:”, 空格, Paligemma位置令牌）以生成正确的动作值。由于推理速度慢，通过缓存SigLIP嵌入的静态零图像来优化速度。</li>
</ul>
<p><strong>3. 推理基础设施：</strong><br>根据模型的计算需求分配不同的硬件资源：OpenVLA使用NVIDIA L4 GPU；Pi0 Base使用单张A100 40GB GPU；Pi0 FAST使用四张A100 40GB GPU并行处理；GPT-4x系列则通过OpenAI的Batch API进行外部调用。</p>
<p><strong>4. 评估指标：</strong><br>研究设计了一套全面的指标来多角度评估模型性能：</p>
<ul>
<li><strong>Brier平均绝对误差（Brier MAE）</strong>：用于评估概率预测的校准程度，值越接近2表示校准越差（最大惩罚值）。</li>
<li><strong>归一化Brier MAE及分位数滤波归一化Brier MAE</strong>：用于分析模型是否依赖于异常值预测。</li>
<li><strong>最大相对Brier MAE</strong>：量化最坏情况误差与典型（中位数）误差的偏差。</li>
<li><strong>微精度（Micro Precision）、召回率（Recall）、F1分数、精确匹配率（Exact Match Rate）</strong>：在本文多类单标签任务中，这四个微指标值相等，衡量模型预测正确的总体比例。</li>
<li><strong>无效预测百分比</strong>：预测动作超出有效动作空间的比例，高百分比表明模型难以产生有效输出。</li>
<li><strong>排除无效后的微精度</strong>：仅基于有效预测计算精度，用于评估模型在能够产生有效输出时的潜在能力。</li>
<li><strong>类级别精度/召回率/F1及宏精度/召回率/F1</strong>：用于分析模型在不同动作类别上的性能，避免多数类主导整体指标。宏召回率被认为是受类别不平衡影响最小的代表性指标。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.05540v2/extracted/6546904/fixed_model_ranking_macro_macro_recall_with_invalids.png" alt="模型排名"></p>
<blockquote>
<p><strong>图1</strong>：各模型在不同数据集上的宏召回率排名。显示了OpenVLA和GPT-4.1 consistently表现最佳，但整体性能水平仍然较低。</p>
</blockquote>
<p><strong>创新点</strong>：本文的创新性主要体现在评估框架本身：1) <strong>首次系统性地</strong>将最先进的VLA模型（如OpenVLA, Pi0）与通用VLM（GPT-4系列）置于同一过程生成数字环境（Procgen）中进行零样本OOD能力评估；2) 设计了<strong>一套详尽的评估指标</strong>，不仅关注准确率，还深入分析概率校准、无效输出、类别偏差、异常值影响等；3) 对不同的模型架构（自回归、扩散）进行了<strong>针对性的适配和性能剖析</strong>，揭示了模型设计选择对泛化能力的具体影响。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：基准测试平台为Procgen的16个子数据集。对比的基线方法（即被评估模型）为GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, Pi0 FAST。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>零样本OOD泛化能力普遍较差</strong>：所有模型在零样本OOD任务上都表现出严重的性能局限。</p>
<ul>
<li><strong>概率校准差</strong>：如图2所示，所有模型的Brier MAE得分都接近最大值2（GPT-4o/4.1/Pi0 FAST &gt;1.70，OpenVLA &gt;1.50），表明预测概率极不可靠。<br><img src="https://arxiv.org/html/2505.05540v2/extracted/6546904/model_comparison_brier_mae.png" alt="Brier MAE"><blockquote>
<p><strong>图2</strong>：四个模型的Brier MAE得分（Pi0 Base因无logits未包含）。所有得分接近2，表明概率校准极差。</p>
</blockquote>
</li>
<li><strong>整体准确率低</strong>：如图3所示，微精度普遍很低。OpenVLA在Coinrun上取得最高微精度约27%，而GPT-4o在Starpilot上低至1%，Pi0 FAST在Heist上约为1%。性能排序为：OpenVLA &gt; Pi0 Base &gt; GPT-4.1 &gt; GPT-4o &gt; Pi0 FAST。<br><img src="https://arxiv.org/html/2505.05540v2/extracted/6546904/model_comparison_micro_micro_precision_with_invalids.png" alt="微精度"><blockquote>
<p><strong>图3</strong>：包含无效预测的微精度。OpenVLA整体最佳，Pi0 FAST最差。</p>
</blockquote>
</li>
<li><strong>无效预测问题严重</strong>：如图7所示，GPT-4o和Pi0 FAST产生了大量无效预测（某些数据集&gt;80%），而OpenVLA通过设计实现了0无效预测。<br><img src="https://arxiv.org/html/2505.05540v2/extracted/6546904/new_invalid_predictions_percentage_comparison.png" alt="无效预测百分比"><blockquote>
<p><strong>图7</strong>：各模型的无效动作预测百分比。GPT-4o和Pi0 FAST在多数数据集上无效预测率极高。</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>约束输出可提升部分模型表现</strong>：当排除无效预测计算微精度时（图4），GPT-4o和Pi0 FAST的性能有显著提升。例如，GPT-4o在Coinrun、Bossfight等数据集上的精度超过了Pi0 Base。这表明通过适当的提示工程或输出后处理来约束预测空间，可以释放这些模型的潜在能力。<br><img src="https://arxiv.org/html/2505.05540v2/extracted/6546904/new_model_comparison_micro_micro_precision_without_invalids.png" alt="排除无效后的微精度"></p>
<blockquote>
<p><strong>图4</strong>：排除无效预测后的微精度。GPT-4o和Pi0 FAST性能显著改善，但OpenVLA仍保持领先。</p>
</blockquote>
</li>
<li><p><strong>模型表现出不同的类别偏差</strong>：</p>
<ul>
<li><strong>宏精度与宏召回率分析</strong>：如图5和图6所示，OpenVLA的宏召回率相对其宏精度更高，表明其预测策略“激进”，产生了大量假阳性，但覆盖了更多类别。GPT-4o则表现出相反模式：宏精度高于宏召回率，表明其预测“保守”，对少数类别有强烈偏好，但漏掉了许多真阳性。GPT-4.1和Pi0 Base表现相对平衡，Pi0 FAST的宏召回率持续低迷。<br><img src="https://arxiv.org/html/2505.05540v2/extracted/6546904/model_comparison_macro_macro_precision_with_invalids.png" alt="宏精度"><blockquote>
<p><strong>图5</strong>：宏精度。GPT-4.1和Pi0 FAST表现波动大，OpenVLA在多数类上可靠，GPT-4o在少数类上相对较强。<br><img src="https://arxiv.org/html/2505.05540v2/extracted/6546904/model_comparison_macro_macro_recall_with_invalids.png" alt="宏召回率"><br><strong>图6</strong>：宏召回率。OpenVLA因高假阳性而召回率高，GPT-4o因类别偏见而召回率低。</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>异常值影响分析</strong>：通过归一化Brier MAE（图8）和分位数滤波归一化Brier MAE（图9）与常规Brier MAE趋势对比，发现模型性能对异常值预测的依赖极小。因为中位数误差已经很高，异常值的影响可以忽略。<br><img src="https://arxiv.org/html/2505.05540v2/extracted/6546904/model_comparison_normalized_brier_mae.png" alt="归一化Brier MAE"></p>
<blockquote>
<p><strong>图8</strong>：归一化Brier MAE趋势与常规Brier MAE一致，表明对异常值依赖小。<br><img src="https://arxiv.org/html/2505.05540v2/extracted/6546904/model_comparison_quantile_filtered_normalized_brier_mae.png" alt="分位数滤波Brier MAE"><br><strong>图9</strong>：分位数滤波归一化Brier MAE，用于评估多数预测的质量，趋势同样一致。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验与组件贡献分析</strong>：本研究本质上是基准测试而非提出新模型，因此没有传统的模块消融实验。但其分析深度体现了“评估即分析”的思想，主要“组件”贡献可理解为对不同影响因素的分析：</p>
<ol>
<li><strong>动作空间有效性约束的贡献</strong>：通过对比“微精度”和“排除无效后的微精度”，量化了输出约束对GPT-4o和Pi0 FAST性能的提升潜力。</li>
<li><strong>模型架构与训练数据的贡献</strong>：VLA模型（OpenVLA）整体上优于通用VLM（GPT-4系列），表明专门为动作决策设计的架构具有优势。同时，在机器人数据上训练的VLA（OpenVLA, Pi0）被重新用于数字环境，揭示了跨领域泛化的挑战。</li>
<li><strong>评估指标选择的贡献</strong>：通过并排分析微指标和宏指标，揭示了模型在多数类和少数类上的不同表现模式，单一指标会给出有偏的结论。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>MultiNet v0.2基准</strong>，首次在过程生成的、开放式的数字动作环境中，对最先进的VLM和VLA模型进行了系统性的零样本OOD泛化能力评估。</li>
<li>提供了 <strong>详尽的性能剖析</strong>，表明所有被评估模型在零样本OOD任务上都存在显著局限，性能受动作表示、任务复杂性、模型架构和输出约束的强烈影响。</li>
<li>揭示了 <strong>关键见解</strong>：VLA模型因鲁棒的架构设计而总体表现更优；通用VLM在输出被适当约束时显示出巨大改进潜力；模型在类别预测上存在不同偏差（激进vs保守）。</li>
</ol>
<p><strong>局限性</strong>（论文自身提及）：</p>
<ol>
<li><strong>评估范围有限</strong>：目前仅涵盖Procgen环境，未来需扩展到更广泛的数字和物理基准。</li>
<li><strong>模型适配可能引入偏差</strong>：为适应Procgen对模型进行的修改（如动作维度限制、解码步骤约束）可能并非模型原始设计的最优使用方式。</li>
<li><strong>静态轨迹评估</strong>：使用离线专家轨迹进行评估，而非在线交互环境，可能无法完全反映模型的序列决策能力。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>亟需提升零样本泛化能力</strong>：当前SOTA模型的糟糕表现表明，开发能更好适应OOD场景的VLA模型是未来关键方向。</li>
<li><strong>基准设计的重要性</strong>：需要构建统一、涵盖数字与物理领域的基准，并采用全面的评估指标（如本文的校准、无效率、类别偏差指标），以全面衡量模型能力。</li>
<li><strong>架构与训练策略探索</strong>：研究应关注如何设计能更好处理离散动作空间、理解复杂游戏语义、并减少无效预测的模型架构和训练方法。</li>
<li><strong>深入理解模型偏差</strong>：探究模型产生类别偏好和大量无效预测的根本原因，有助于设计更公平、更鲁棒的智能体。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在程序生成的开放分布外环境中零样本泛化能力评估不足的问题，提出MultiNet v0.2基准。该基准系统测试了GPT-4o、GPT-4.1、OpenVLA、Pi0等多类先进模型在Procgen任务上的表现。核心发现包括：所有模型在OOD任务上均存在显著泛化局限，性能受动作表示与任务复杂度影响；VLA模型凭借稳健架构整体表现更优；VLM变体经过适当提示约束后可获大幅改进，凸显了提示工程对性能的关键作用。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.05540" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>