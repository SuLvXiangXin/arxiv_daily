<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.15968" target="_blank" rel="noreferrer">2509.15968</a></span>
        <span>作者: Fang, Shiyu, Cui, Yiming, Liang, Haoyang, Lv, Chen, Hang, Peng, Sun, Jian</span>
        <span>日期: 2025/09/19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>自动驾驶系统在常规场景中取得了显著进展，但在长尾、安全关键场景中的性能仍然有限，这些罕见案例导致了不成比例的事故数量。当前端到端方法主要分为两类：小规模任务特定模型和大规模预训练模型。小规模任务特定模型通过多任务学习整合感知、预测和规划，但依赖于有限的数据集和固定的任务先验，在长尾或高度交互的场景中泛化能力差。大规模预训练模型，特别是视觉语言模型，凭借其世界知识和强大的推理能力，为自动驾驶提供了有前景的方案，形成了视觉-语言-动作框架。然而，VLA在长尾场景下面临两大关键挑战：1）<strong>长尾QA数据稀缺</strong>：现有公共数据集缺乏包含长尾场景的视觉问答数据；2）<strong>稀疏数据下的低效微调</strong>：长尾场景的低频特性使得模型难以从有限数据中有效学习。</p>
<p>本文针对上述痛点，提出了一种新的持续学习框架CoReVLA，其核心思路是通过“收集-精炼”的双阶段过程，利用人机交互测试收集接管数据作为高质量的长尾场景反馈，并采用直接偏好优化方法，直接从人类偏好中学习，以高效提升模型在长尾场景下的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>CoReVLA是一个三阶段的持续学习框架，旨在通过收集人类接管数据并基于此进行行为精炼，以提升自动驾驶系统在长尾场景下的性能。</p>
<p><img src="https://arxiv.org/html/2509.15968v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：CoReVLA框架总览。首先，在混合开源驾驶QA数据集上对基础VLM进行监督微调。接着，将模型部署在CAVE人机交互仿真平台中，收集因模型失效而触发的人类接管数据。最后，利用直接偏好优化方法，基于收集到的接管数据对模型进行行为精炼。</p>
</blockquote>
<p><strong>整体流程</strong>：1) <strong>预阶段</strong>：在整合的驾驶QA数据集上对基础视觉语言模型进行监督微调，使其获得对驾驶任务的基本理解。2) <strong>阶段一</strong>：将微调后的模型部署于CAVE仿真平台进行闭环测试，当模型行为不佳时由安全驾驶员接管，并记录接管事件的相关数据。3) <strong>阶段二</strong>：利用阶段一收集的接管数据，通过DPO方法对模型进行精炼，使其行为与人类偏好对齐。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>预阶段：基于QA数据的监督微调</strong></p>
<ul>
<li><strong>数据构建</strong>：整合LingoQA、BDD和HAD三个开源数据集，构建了一个70GB的领域特定数据集。每个训练实例包含连续5帧图像（间隔1秒）以及采用思维链格式的结构化问答对，模拟从场景理解到决策的人类推理过程。</li>
<li><strong>模型与训练</strong>：以Qwen2.5-VL-7B为基础模型，采用LoRA进行参数高效的微调。LoRA模块被插入到视觉投影器和LLM主干网络的关键线性层中。损失函数为标准自回归交叉熵损失（公式1），仅更新LoRA引入的可训练参数。</li>
</ul>
</li>
<li><p><strong>阶段一：在CAVE平台中收集接管数据</strong></p>
<p><img src="https://arxiv.org/html/2509.15968v1/x2.png" alt="数据收集平台"></p>
<blockquote>
<p><strong>图2</strong>：CAVE人机交互测试与接管数据收集平台。安全驾驶员通过VR头显沉浸在重建的3D场景中，实时监控并可在必要时接管由CoReVLA控制的自我车辆。</p>
</blockquote>
<ul>
<li><strong>CAVE平台</strong>：一个沉浸式、人机交互的闭环测试平台。能够从轨迹数据重建3D场景，支持背景交通的回放或交互模式。</li>
<li><strong>数据收集过程</strong>：将CoReVLA集成到CAVE中进行实时交互测试。当模型行为导致死锁或碰撞风险时，系统切换至回放模式，由佩戴VR设备的安全驾驶员进行监控和潜在接管。每个接管实例被记录为一个结构化样本，包含历史图像输入、接管时刻驾驶员的视觉注意力、驾驶员控制动作以及接管前模型生成的行为。</li>
</ul>
</li>
<li><p><strong>阶段二：基于DPO的行为精炼</strong></p>
<ul>
<li><strong>数据与目标</strong>：利用阶段一收集的数据，每个样本包含一个动作对：模型生成的次优行为（y⁻）和驾驶员执行的纠正行为（y⁺）。目标是通过学习这些隐含的人类偏好，使模型策略与人类意图对齐。</li>
<li><strong>DPO方法</strong>：DPO通过优化一个基于偏好对的损失函数（公式4），直接调整模型策略，使其对优选动作的打分高于次选动作，从而避免了传统强化学习人类反馈中需要手动设计奖励函数的问题。为防止策略漂移，在损失函数中加入了与参考策略的KL散度正则化项（公式5）。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：1) <strong>基于HITL的视觉接管数据收集</strong>：利用沉浸式CAVE平台主动收集包含视觉上下文、驾驶员行为和实时注意力信息的长尾失效案例数据。2) <strong>基于稀疏接管数据的DPO精炼</strong>：直接利用人类接管作为偏好反馈进行高效学习，避免了奖励设计困难和奖励黑客问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验分为开环评估和闭环评估两部分，以全面验证模型能力。</p>
<p><strong>开环QA评估</strong>：在LingoQA、BDD和HAD三个数据集上，使用BLEU和ROUGE指标评估模型的语言理解和推理能力。</p>
<p><img src="https://arxiv.org/html/2509.15968v1/x3.png" alt="开环响应对比"></p>
<blockquote>
<p><strong>图3</strong>：不同模型在开环场景理解与决策任务上的响应对比。CoReVLA能够准确推断周围交通参与者的意图并生成安全、符合上下文的驾驶决策，而基线模型（如Llava、Qwen2.5）则存在感知不准确或决策危险的问题。</p>
</blockquote>
<p><strong>关键结果</strong>：如表I所示，CoReVLA在所有数据集上的BLEU和ROUGE分数均显著高于基线模型（如Impromptu、Llava系列），表明SFT阶段有效提升了模型对驾驶场景的理解和决策能力。</p>
<p><strong>闭环驾驶评估</strong>：在CAVE平台进行人机交互测试与精炼，并在Bench2Drive基准上与SOTA方法进行对比。Bench2Drive包含多样且具有挑战性的长尾场景。</p>
<p><img src="https://arxiv.org/html/2509.15968v1/x4.png" alt="精炼前后轨迹对比"></p>
<blockquote>
<p><strong>图4</strong>：行为精炼前后CoReVLA的驾驶轨迹对比。在雨天跟车场景中，前车突然变道露出前方静止故障车。精炼前，模型误判形势、制动过晚导致碰撞；精炼后，模型能更早识别风险并主动变道避让。</p>
</blockquote>
<p><strong>关键结果</strong>：如表II所示，在Bench2Drive基准上，CoReVLA取得了最高的驾驶得分（72.18）和成功率（50.00%），相比次优方法分别提升了7.96 DS和14.99% SR。与精炼前的自身版本相比，DS和SR也有显著提升，验证了“收集-精炼”流程的有效性。尽管在效率和舒适度指标上并非最优，但论文指出这是由于精炼过程优先安全，导致驾驶员行为趋于谨慎所致。</p>
<p><img src="https://arxiv.org/html/2509.15968v1/x5.png" alt="跨场景泛化"></p>
<blockquote>
<p><strong>图5</strong>：展示CoReVLA行为精炼与跨平台泛化能力的场景对比。在CAVE中学习到的对路边遮挡区域减速警惕的行为，能够成功迁移到Bench2Drive中行人突然冲出的类似场景中。</p>
</blockquote>
<p><strong>消融与泛化分析</strong>：精炼前后的性能对比（表II，图4）直接证明了基于DPO的行为精炼模块的有效性。图5的案例研究表明，模型在CAVE平台中从人类接管学习到的安全行为，能够有效地泛化到Bench2Drive基准中的类似长尾场景，展示了其持续学习和避免重复失败的能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个完整的“收集-精炼”双阶段持续学习框架CoReVLA，用于提升端到端自动驾驶在长尾场景下的性能。2) 开发了基于沉浸式CAVE平台的人机交互测试流程，系统化地收集高质量的视觉接管数据。3) 创新地将DPO应用于从稀疏人类接管数据中学习驾驶策略，实现了高效、免奖励工程的行为对齐。</p>
<p><strong>局限性</strong>：论文提到，由于精炼过程优先考虑安全，模型在效率和舒适度指标上并未全面领先，体现了安全与其他驾驶指标之间的权衡。</p>
<p><strong>启示</strong>：本工作为利用人机交互和人类偏好学习来攻克自动驾驶长尾问题提供了一个切实可行的范式。未来研究方向包括向真实世界部署拓展，以及探索融合更丰富形式的人类反馈。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自动驾驶系统在长尾、安全关键场景中性能受限的核心问题，提出CoReVLA双阶段端到端框架。其关键技术为“收集-精炼”：先在驾驶问答数据集上微调建立基础理解；再于CAVE仿真中收集人类接管数据以识别失败场景；最后通过直接偏好优化（DPO）基于人类偏好进行行为精炼。实验表明，在Bench2Drive基准的长尾场景下，该模型驾驶分数达72.18，成功率50%，分别领先现有最优方法7.96分和15%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.15968" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>