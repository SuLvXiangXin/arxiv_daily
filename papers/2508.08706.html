<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.08706" target="_blank" rel="noreferrer">2508.08706</a></span>
        <span>作者: Hengdi Zhang Team</span>
        <span>日期: 2025-08-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域的主流方法是视觉-语言-动作模型，它们基于大规模预训练的视觉-语言模型，通过自然语言指令和视觉观察来生成机器人动作，展现出良好的任务泛化潜力。然而，这些模型严重依赖视觉和语言模态，普遍忽视了触觉感知的重要性。触觉感知对于接触丰富的精细操作任务至关重要，它能提供接触动力学、纹理等直接物理反馈，并对视觉遮挡具有鲁棒性。现有尝试将触觉融入VLA框架的工作，往往将触觉数据视为低级信号，未能将其与视觉、语言模态在语义层面上对齐，且触觉编码器的设计研究不足。本文针对VLA模型在接触丰富任务中因缺乏语义对齐的触觉感知而性能受限的痛点，提出了OmniVTLA模型。其核心思路是：通过构建一个包含视觉、触觉、语言的三模态语义对齐共享空间，并设计双路径触觉编码器框架，以克服不同触觉传感器间的异质性，从而提升机器人对接触物理的理解和操作性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>OmniVTLA模型的整体框架基于π0模型构建，包含分词器、骨干网络和动作头三个核心组件。其输入为语言指令、多视角图像和来自多个触觉传感器的数据，输出为未来一段时间内的动作序列。模型的关键创新在于对触觉数据的处理。</p>
<p><img src="https://arxiv.org/html/2508.08706v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：OmniVTLA整体架构。模型集成了双ViT路径的触觉编码器，以处理视觉与触觉数据之间以及不同触觉传感器之间的异质性。一条路径利用预训练的视觉编码器继承大规模图像数据的语义表示；另一条路径（SA-ViT）通过跨模态对比学习进行显式训练，实现触觉、视觉和文本模态的语义对齐。</p>
</blockquote>
<p>核心模块是<strong>双路径触觉编码器</strong>，旨在解决触觉数据的异质性：1）触觉数据与视觉数据的本质差异；2）不同触觉传感器（如基于视觉的GelSight和基于力的Paxini Gen2）在传感原理、时空分辨率上的差异。具体设计如下：</p>
<ul>
<li><strong>路径一（VTLA-Pre）</strong>：使用在大规模图像数据上预训练的视觉Transformer作为触觉编码器的初始化，并在少量遥操作数据上微调。这利用了视觉模型已有的丰富语义知识。</li>
<li><strong>路径二（VTLA-SA）</strong>：使用<strong>语义对齐的触觉ViT</strong>。该编码器首先通过跨模态对比学习进行训练，学习统一的触觉表示，然后再在少量遥操作数据上微调。</li>
</ul>
<p>两种触觉编码器将处理后的触觉数据（被归一化并重塑为3通道图像）分别编码为256个令牌，然后与经过SigLIP编码的图像令牌、经过PaliGemma分词的语言令牌拼接，一同输入Gemma-2B骨干网络。动作头则使用流匹配损失进行训练。</p>
<p>为了训练语义对齐的触觉编码器（SA-ViT），本文引入了<strong>ObjTac数据集</strong>。这是一个包含文本、视觉和基于力的触觉数据的三模态数据集，涵盖了10个类别、56个物体，总计13.5万个样本。每个样本都包含物体名称、材料类型、粗糙度、硬度等语言标注。</p>
<p><img src="https://arxiv.org/html/2508.08706v2/x3.png" alt="数据集示例"></p>
<blockquote>
<p><strong>图3</strong>：ObjTac数据集快照，展示了十个类别中的56个物体。我们收集了视觉-触觉数据对以实现语义级对齐，并展示了部分归一化后的触觉图像。</p>
</blockquote>
<p>利用此数据集及现有触觉数据集，SA-ViT通过跨模态对比学习进行训练，其对齐损失函数为：<br>ℒ_align = α_VL * (ℒ_V→L + ℒ_T→V)/2 + α_VT * (ℒ_V→T + ℒ_T→V)/2 + α_VL * (ℒ_T→L + ℒ_L→T)/2<br>其中ℒ_V→L等表示批次内视觉到语言等模态间的对比损失，借鉴自CLIP。此外，还增加了跨传感器匹配损失。最终，OmniVTLA模型融合了VTLA-Pre和VTLA-SA两条路径的触觉编码器输出，以实现更强大的跨传感器理解和知识迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了自建的ObjTac数据集以及公开的Touch and Go数据集进行触觉编码器预训练。机器人操作实验在真实世界进行，平台包括UR5机械臂、配备两个触觉传感器的二指夹爪、配备11个触觉传感器的四指灵巧手以及腕部和基座相机。任务为针对多个物体的“抓取-放置”操作。</p>
<p>对比的基线方法包括：作为非VLM基线的Diffusion Policy，作为VLA基线的π0模型，以及本文消融实验中的多个变体：VTLA-FS（触觉编码器从头训练）、VTLA-Pre、VTLA-SA。</p>
<p>关键实验结果如下：</p>
<ol>
<li><p><strong>离线验证</strong>：如图5所示，OmniVTLA在预测轨迹与真实轨迹之间的均方误差上表现最佳，平均误差为1.40×10⁻⁴，相比VLA基线在短罐和瓶子上分别降低了7.8%和23.3%。<br><img src="https://arxiv.org/html/2508.08706v2/x5.png" alt="离线验证结果"></p>
<blockquote>
<p><strong>图5</strong>：不同模型在不同物体上的离线验证结果。OmniVTLA在预测轨迹与真实轨迹之间的均方误差最低。</p>
</blockquote>
</li>
<li><p><strong>真实世界成功率与效率</strong>：</p>
<ul>
<li><strong>使用二指夹爪</strong>：如表3所示，OmniVTLA取得了96.9%的平均成功率，比VLA基线（π0）提高了21.9%。在任务完成步数上，VTLA-SA将平均步数减少了26.3%，OmniVTLA减少了24.2%。</li>
<li><strong>使用四指灵巧手</strong>：如表4所示，OmniVTLA取得了100%的平均成功率，比VLA基线提高了6.2%，并对未在训练中出现的物体（塑料高瓶和方咖啡瓶）也达到了100%的成功率。任务完成步数减少了6%。</li>
<li><strong>与Diffusion Policy对比</strong>：如表5所示，加入触觉感知后（VTA），平均成功率提升了18.7%，完成步数减少了19.9%。</li>
</ul>
</li>
<li><p><strong>轨迹平滑度</strong>：如表6所示，语义对齐的触觉编码器（VTLA-SA）带来了最平滑的轨迹，其平滑度指标比纯VLA基线降低了89.6%。这表明触觉反馈使机器人能更智能地调整动作，在非接触阶段快速移动，在接触阶段减速，避免突兀动作。</p>
</li>
<li><p><strong>定性分析</strong>：图6展示了VLA模型因接触感知不足导致抓取失败，VTLA-Pre模型持续调整夹爪却无法成功提起，而OmniVTLA则能利用语义触觉线索稳定抓取并执行平滑轨迹。<br><img src="https://arxiv.org/html/2508.08706v2/x6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图6</strong>：VLA、VTLA-FS、VTLA-Pre、VTLA-SA模型因接触感知不足、接触松散或不规则而失败的案例可视化。OmniVTLA凭借完整的触觉感知实现了成功的抓取和稳定的接触。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验总结</strong>：表3的消融结果表明，单独的语义对齐编码器（VTLA-SA）在成功率和效率上均优于从头训练的编码器（VTLA-FS）和仅用视觉预训练初始化的编码器（VTLA-Pre）。而结合了预训练和语义对齐优势的双路径设计（OmniVTLA）取得了最佳的综合性能，证明了该设计的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1）提出了OmniVTLA，一个集成视觉、触觉、语言模态的端到端操作模型，其双路径触觉编码器设计有效处理了触觉传感器的异质性；2）构建并发布了ObjTac，一个大规模、包含语义标注的基于力的触觉三模态数据集；3）基于该数据集训练了一个语义对齐的触觉编码器，并通过真实机器人实验验证了OmniVTLA在成功率、任务效率和轨迹平滑度上的显著提升。</p>
<p>论文自身提到的局限性在于，目前评估的任务和机器人类型仍然有限。本文工作为触觉感知的机器人操作奠定了重要基础。对后续研究的启示在于：强调了将触觉信号与视觉、语言在语义层面进行对齐的重要性，而不仅仅是作为低级物理信号使用；双编码器或更复杂的融合架构是处理多模态、异质传感器数据的一种有效思路；构建高质量、对齐的多模态数据集是推动触觉感知泛化能力的关键。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作（VLA）模型忽视触觉感知、在接触丰富任务中失败的问题，提出OmniVTLA模型。其关键技术包括双路径触觉编码器框架（使用预训练ViT和语义对齐触觉ViT）以及ObjTac触觉数据集（135K三模态样本），以学习统一触觉表示。实验表明，在抓取任务中，OmniVTLA使用夹爪成功率达96.9%（比基线高21.9%），使用灵巧手达100%（比基线高6.2%），同时减少任务时间并生成更平滑轨迹。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.08706" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>