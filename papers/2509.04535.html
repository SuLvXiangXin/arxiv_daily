<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>In-Context Policy Adaptation via Cross-Domain Skill Diffusion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>In-Context Policy Adaptation via Cross-Domain Skill Diffusion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.04535" target="_blank" rel="noreferrer">2509.04535</a></span>
        <span>作者: Honguk Woo Team</span>
        <span>日期: 2025-09-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>强化学习（RL）在解决序列决策问题上取得了显著成功，但一个根本性挑战出现在策略适应过程中，即将在源域中训练的RL策略应用到不同的目标域，尤其是在复杂、长视野的任务中。这一挑战在实际限制下变得更加严峻：目标域的数据可用性有限，并且禁止与目标域直接交互。现有的技能型RL方法通常利用离线专家数据集学习一组共享技能，但其通常需要在目标域内进行在线策略学习。本文针对在目标域数据极少（例如不超过5条轨迹）且禁止模型更新的严格约束下，如何实现技能型策略的快速跨域适应这一具体痛点，提出了一种新的视角：通过引入一个“中间层”进行领域级的统一适应。本文的核心思路是：首先通过跨域技能扩散方案，从离线数据中联合学习领域无关的原型技能和领域接地的技能适配器；然后利用动态领域提示方案，在无需模型更新的前提下，指导技能适配器生成与目标域更好对齐的动作序列，从而实现上下文策略适应。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的ICPAD框架旨在实现无需模型更新的跨域上下文策略适应，包含两个阶段：1）离线学习跨域技能扩散；2）通过动态领域提示进行上下文适应。</p>
<p><img src="https://arxiv.org/html/2509.04535v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ICPAD框架的离线学习和上下文适应阶段。(i) 在离线学习阶段，利用从多域收集的离线数据集，联合学习领域无关的原型技能和一个领域接地的技能适配器，该适配器被提示生成跨域一致的动作序列。(ii) 在上下文适应阶段，通过基于检索的注意力对技能适配器进行提示，促进从源域学习的策略π适应到未见过的目标域。</p>
</blockquote>
<p><strong>整体流程</strong>：如算法1所示，框架首先进行离线学习，初始化并联合训练技能编码器Φ_E、技能适配器Φ_A、技能先验Φ_R和领域编码器Ψ_E。随后，在源域使用习得的技能模型训练技能型策略π。在适应阶段，给定少量目标域数据，利用动态提示函数Ψ_D为当前状态历史检索合适的领域提示，指导技能适配器Φ_A生成适应目标域的动作序列，而策略π和适配器Φ_A的参数均保持不变。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li>**技能编码器 (Φ_E) 与领域编码器 (Ψ_E)**：两者均以H步子轨迹τ=(s, a)为输入。技能编码器输出原型技能嵌入z的分布p(z|s, a)，旨在捕捉领域无关的行为模式。领域编码器输出领域嵌入d的分布p(d|s, a)，用于表征轨迹的领域特性。</li>
<li>**技能适配器 (Φ_A)**：这是一个基于扩散模型的模块，以当前状态s_t、领域嵌入d和技能嵌入z为条件，生成动作序列a。其训练采用了扩散模型的损失函数，旨在去噪并重建动作序列。</li>
<li><strong>跨域技能扩散训练</strong>：这是离线学习的核心，通过三个损失函数联合优化上述模块：<ul>
<li>**技能模仿损失 (ℒ_skill)**：如公式(7)所示，结合了扩散重建损失和技能嵌入空间的KL正则化项，确保技能编码器和适配器能够准确重建专家行为。</li>
<li>**跨域技能先验一致性损失 (ℒ_cross-E)**：如公式(9)所示，通过一个技能先验模型Φ_R（根据状态预测技能分布），约束技能编码器Φ_E输出的技能分布与领域无关，仅与状态相关，从而确保原型技能的领域无关性。</li>
<li>**跨域动作一致性损失 (ℒ_cross-A)**：如公式(10)所示，要求技能适配器Φ_A在给定一个领域嵌入d&#39;和技能嵌入z时，生成的动作序列应能被领域编码器识别为属于d&#39;对应的领域，同时其技能嵌入应与原始的z保持一致。这确保了生成的动作品质同时适配于特定领域并保持技能语义。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.04535v1/x3.png" alt="跨域技能扩散"></p>
<blockquote>
<p><strong>图3</strong>：跨域技能扩散训练示意图。技能编码器Φ_E和领域接地的技能适配器Φ_A通过技能模仿损失ℒ_skill、跨域技能先验一致性损失ℒ_cross-E以及跨域动作一致性损失ℒ_cross-A进行联合优化，以学习领域无关的原型技能和跨域一致的技能翻译能力。</p>
</blockquote>
<ol start="4">
<li><strong>动态领域提示</strong>：在上下文适应阶段，为了将适配器快速对准目标域，设计了动态提示函数Ψ_D。首先，领域编码器Ψ_E通过对比损失ℒ_con（公式11）进行训练，以区分不同领域的轨迹。在适应时，从少量目标域示范数据𝒯_T中，基于当前状态历史s_h与示范数据中状态的欧氏距离，检索出m个最相关的示范子轨迹，并对其对应的领域嵌入进行加权平均（公式13），作为当前步骤指导技能适配器的动态领域提示d。这使得适配器能够根据在线遇到的状态上下文，灵活地调整其行为以匹配目标域特性。</li>
</ol>
<p><strong>创新点</strong>：与现有需要在线微调策略或技能解码器的方法不同，ICPAD的创新在于：1）提出了一个<strong>中间层适应策略</strong>，通过领域无关的原型技能作为“通用语”连接不同领域；2）设计了<strong>跨域技能扩散方案</strong>，将分布匹配的跨域一致性学习融入技能扩散过程，联合学习可转移的技能和适配器；3）引入了<strong>动态领域提示机制</strong>，利用极少量目标数据实现适配器的在线上下文引导，而无需梯度更新。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在机器人操作环境Metaworld（含多阶段长任务场景）和自动驾驶模拟器CARLA中评估框架。考虑了多种跨域配置差异，包括环境动力学（动作噪声、风力）、智能体具身（车辆模型）以及任务视野。</p>
<p><strong>基线方法</strong>：对比了扩散模仿学习（DiffBC+FT）、技能型RL（SPiRL及其微调变体SPiRL*）、技能型小样本模仿（FIST）以及最先进的技能型元RL方法（DCMRL）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>环境动力学变化下的适应</strong>：如表1所示，在Metaworld中，随着动作噪声或风力造成的领域差异（低、中、高）增大，ICPAD始终优于所有基线。其平均成功率比最接近的竞争者DCMRL高14.0%。在领域差异增大时，ICPAD的性能仅下降10.5%，而FIST和DCMRL分别下降了20.1%和22.6%，表明ICPAD的原型技能和适配器具有更好的跨域泛化能力。</li>
<li><strong>具身变化下的适应</strong>：如表2所示，在CARLA中，面对车辆具身以及附加天气条件的变化，ICPAD的归一化回报 consistently 超越DCMRL，优势幅度在11.6%到21.6%之间。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.04535v1/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：在CARLA中的定性适应结果示例。左侧为源域（正常天气，小型车辆），右侧为目标域（雨天，大型车辆）。ICPAD策略通过动态领域提示，在目标域中成功完成了导航任务，而基线方法DCMRL则未能适应新的领域条件，导致了碰撞。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：论文通过消融研究验证了各核心组件的贡献。移除跨域一致性损失（ℒ_cross-E和ℒ_cross-A）会导致性能显著下降，特别是在领域差异大的情况下，这证实了学习领域无关技能和跨域一致适配的重要性。移除动态领域提示，改为使用固定的平均领域嵌入，也会导致性能降低，说明了根据状态上下文动态调整提示的有效性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1）提出了一个新颖的<strong>上下文策略适应框架ICPAD</strong>，实现了无需在线模型更新的、领域级快速的技能型策略跨域适应。2）开发了<strong>跨域技能扩散方案</strong>，通过集成跨域一致性学习与技能扩散，鲁棒地学习领域无关的原型技能并实现其向领域特定动作的有效翻译。3）采用了<strong>动态领域提示方案</strong>，利用极少目标数据增强技能型策略对目标域的适应能力。</p>
<p><strong>局限性</strong>：论文自身提及的局限性包括：框架依赖于离线数据集中包含的多领域专家示范；动态提示机制需要存储和检索少量目标域数据，在计算资源极端受限的场景下可能需要进一步优化。</p>
<p><strong>启示</strong>：本研究为数据受限下的跨域强化学习提供了一个新范式，即通过构建可转移的中间表示（原型技能）和可引导的翻译模块（技能适配器）来解耦策略与领域。这对现实世界机器人应用（如不同硬件平台间的策略迁移）和模拟到真实的迁移具有启发意义。后续研究可探索更高效的提示机制、处理领域差异极大的情况，或将此框架扩展到更广泛的序列决策问题中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对长视野多任务环境中强化学习策略跨域适应的挑战，特别是在无模型更新且目标域数据有限的严格约束下，提出ICPAD框架。该框架采用跨域技能扩散技术，学习领域无关的原型技能作为策略通用表示，并结合动态域提示的技能适配器实现快速对齐。实验在Metaworld机器人操作和CARLA自动驾驶场景中进行，结果表明ICPAD在环境动态、代理体现等跨域配置下，仅凭有限目标数据即实现了优越的策略适应性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.04535" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>