<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FastStair: Learning to Run Up Stairs with Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>FastStair: Learning to Run Up Stairs with Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.10365" target="_blank" rel="noreferrer">2601.10365</a></span>
        <span>作者: Liu, Yan, Yu, Tao, Song, Haolin, Zhu, Hongbo, Hu, Nianzong, Hao, Yuzhi, Yao, Xiuyong, Zang, Xizhe, Chen, Hua, Zhao, Jie</span>
        <span>日期: 2026/01/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人上下楼梯对人类而言轻而易举，但对机器人却极具挑战，核心在于敏捷性与稳定性之间的固有权衡。当前主流方法分为两类：模型免费的强化学习方法能生成动态运动，但在楼梯等复杂地形上，其隐式的稳定性奖励（如存活、姿态奖励）常与高速跟踪奖励冲突，导致不安全行为或为保安全而牺牲速度；基于模型的落脚点规划方法通过硬约束编码接触可行性和稳定性，能提供安全保障，但严格的约束往往导致运动保守，限制了动态敏捷性。本文针对“如何同时实现高速与稳定楼梯上行”这一具体痛点，提出了一种规划器引导的多阶段学习框架，旨在融合基于模型规划的安全性与强化学习获得的敏捷性。其核心思路是：首先利用并行的基于模型的落脚点规划器引导RL探索，预训练一个注重安全的基策略；然后通过奖励重加权将该基策略微调为高低速专家策略；最后使用低秩自适应技术整合专家，得到一个能在全速域平滑、稳定运行的单策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>FastStair框架是一个三阶段的训练流程，旨在从注重安全的基础策略出发，逐步发展出高速敏捷且在全速域稳定的最终策略。</p>
<p><img src="https://arxiv.org/html/2601.10365v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：FastStair框架总览。预训练阶段，并行DCM落脚点优化器生成动态可行的接触点，通过落脚点跟踪奖励引导策略学习。后训练阶段，预训练基模型通过扩展指令速度范围和调整奖励权重，被微调为高速和低速专家策略。LoRA微调阶段，将两个专家的参数合并到单一网络中并用LoRA进行微调，得到统一的最终策略。</p>
</blockquote>
<p><strong>核心模块1: 并行落脚点规划器</strong>。为了在RL训练中提供实时、高效的显式安全引导，论文将基于发散运动分量的落脚点优化问题重构为并行离散搜索。首先，采用变高度倒立摆模型对楼梯攀登过程进行建模，并推导出适用于楼梯地形的DCM动力学。关键的简化假设是系数a≈1，从而得到DCM演化方程 ξ(t)=ξ0 e^{∫ω(τ)dτ}。基于此，落脚点优化问题被形式化为一个包含落脚点位置u_T、DCM偏移量b的最小化问题，目标函数包含对名义步态目标、名义DCM偏移的跟踪以及地形陡度惩罚S(u_T)，约束包括DCM动力学和落脚点必须在可行集P内。</p>
<p><img src="https://arxiv.org/html/2601.10365v1/x3.png" alt="楼梯攀登的倒立摆过程"></p>
<blockquote>
<p><strong>图3</strong>：应用于楼梯协商的变高度倒立摆模型示意图，展示了质心高度在摆动相线性变化的过程。</p>
</blockquote>
<p>创新的“优化即搜索”策略在于：利用高程图（蓝色点）自然离散化可行落脚点集P，并通过等式约束为每个候选u_T解析计算对应的b。随后，整个批次的候选点目标函数可以通过GPU上高效的向量化张量操作并行评估，并通过argmin归约找到最优落脚点。搜索被限制在名义目标周围的感兴趣区域内以加速查询。此方法避免了实时优化求解器，在4096个并行环境中单步延迟仅约4ms，相比并行MPC实现加速约25倍。</p>
<p><img src="https://arxiv.org/html/2601.10365v1/pic/scandot1.png" alt="地形感知信息"></p>
<blockquote>
<p><strong>图4</strong>：地形感知信息。蓝色标记为地形扫描点，绿色为平均梯度图，浅蓝色为局部平均梯度图，红色为最优落脚点。</p>
</blockquote>
<p><strong>核心模块2: 三阶段强化学习框架</strong>。策略的观察空间包含本体感知（指令、关节状态等）、步态时钟信号、以机器人为中心的局部高程图，以及特权信息（如真实基座速度、规划器计算的最优落脚点）。奖励函数在基线基础上引入了关键的落脚点跟踪奖励r_foot = exp(-10‖p_f - p_f^d‖)，其中期望落脚点轨迹p_f^d由抬升点、摆动顶点和规划的目标落地点[u_T^⊤, h_z]^⊤插值生成。</p>
<ul>
<li><strong>预训练阶段</strong>：优先考虑遍历稳定性，为落脚点跟踪奖励赋予主导权重，迫使策略瞄准安全的接触区域（楼梯踏板中心），从而编码模型规划器的几何直觉，但会牺牲速度跟踪精度。</li>
<li><strong>后训练阶段</strong>：为恢复速度跟踪精度并提高攀爬速度，调整奖励权重（增加速度跟踪项，减少落脚点跟踪项）。由于高低速动作分布存在差异，直接将指令速度范围扩展到[-0.3, 1.6] m/s训练单一策略会导致模式坍缩。因此，将指令空间划分为高速带和低速带，从预训练策略初始化，分别训练两个速度专家策略。</li>
<li><strong>LoRA微调阶段</strong>：直接切换离散专家策略会导致控制不连续。因此，将两个专家的参数整合到一个统一网络中，并在其分支上应用低秩自适应层进行微调。部署时，根据指令速度通过规则切换器激活相应专家分支，LoRA微调确保了在全速域内的平滑过渡和鲁棒性能。各阶段具体奖励权重见论文表I。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：训练在IsaacLab中进行，并行4096个环境，课程包括平地、粗糙地形和金字塔形楼梯。测试平台为LimX Oli全尺寸人形机器人（高1.65米，重55公斤，31个自由度）。感知使用Intel RealSense D435i深度相机实时重建1.8m×1.2m的局部高程图。对比方法包括：1) <strong>基线</strong>：无模型引导的标准端到端RL策略；2) <strong>预训练模型</strong>：仅第一阶段，含DCM引导但无速度专家拆分；3) <strong>AMP</strong>：基于运动重定向的先进方法。</p>
<p><img src="https://arxiv.org/html/2601.10365v1/x4.png" alt="机器人平台Limx Oli"></p>
<blockquote>
<p><strong>图5</strong>：实验所使用的全尺寸人形机器人平台LimX Oli。</p>
</blockquote>
<p><strong>落脚点跟踪与计算效率</strong>：规划器生成的落脚点（图6红点）集中在楼梯踏板中心。预训练策略的落脚点跟踪误差较小，但最终统一策略的跟踪误差增大，这反映了设计上的权衡：预训练优先安全，后训练则探索更敏捷的行为，牺牲部分落脚点精度以换取更好的速度跟踪。</p>
<p><img src="https://arxiv.org/html/2601.10365v1/x5.png" alt="落脚点截图与跟踪误差"></p>
<blockquote>
<p><strong>图6</strong>：仿真中的落脚点截图（左）以及预训练策略与最终统一策略在不同指令速度下的平均落脚点跟踪误差（右）。最终策略的跟踪误差更大，符合设计预期。</p>
</blockquote>
<p><strong>全速域速度跟踪</strong>：在MuJoCo的15cm台阶楼梯上进行下降测试，统一策略能有效跟踪从0 m/s到1.6 m/s的全程速度指令，验证了多阶段框架的有效性。</p>
<p><img src="https://arxiv.org/html/2601.10365v1/x6.png" alt="MuJoCo仿真中的速度跟踪性能"></p>
<blockquote>
<p><strong>图7</strong>：MuJoCo楼梯下降仿真截图（左）及前向速度跟踪随时间变化曲线（右）。测量速度能紧密跟踪指令速度。</p>
</blockquote>
<p><strong>训练框架评估</strong>：在具有17cm台阶的螺旋楼梯上进行评估。关键指标为遍历成功率和速度跟踪平均绝对误差。</p>
<p><img src="https://arxiv.org/html/2601.10365v1/x7.png" alt="螺旋楼梯上的成功率与速度误差"></p>
<blockquote>
<p><strong>图8</strong>：不同方法在螺旋楼梯上的性能对比。FastStair（最终）实现了100%的成功率，且速度跟踪MAE显著低于其他方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.10365v1/x8.png" alt="消融研究：各阶段贡献"></p>
<blockquote>
<p><strong>图9</strong>：消融研究结果。展示了仅预训练、预训练+后训练（专家）、以及完整FastStair框架在不同指令速度下的速度跟踪MAE。完整框架在全速域表现最优。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.10365v1/x9.png" alt="真实世界高速楼梯上行"></p>
<blockquote>
<p><strong>图10</strong>：真实机器人实验。Oli机器人以1.65 m/s的指令速度稳定上行33级螺旋楼梯（每级高17cm），耗时12秒。</p>
</blockquote>
<p><strong>关键结果总结</strong>：</p>
<ol>
<li><strong>性能对比</strong>：最终FastStair策略在螺旋楼梯上达到100%成功率，速度跟踪MAE为0.131 m/s，显著优于基线、仅预训练模型和AMP方法。</li>
<li><strong>消融实验</strong>：图9表明，仅预训练策略在低速跟踪好但高速差；引入专家策略（预训练+后训练）改善了高速性能，但在某些速度点有波动；完整的LoRA微调阶段进一步平滑了性能，实现了全速域的最佳跟踪。</li>
<li><strong>真实世界部署</strong>：机器人成功以高达1.65 m/s的指令速度稳定上行长楼梯（33级螺旋楼梯，12秒完成），并在广州塔机器人登高比赛中夺冠。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 引入基于DCM的并行落脚点规划器作为显式稳定性奖励，引导RL在复杂地形上探索动态可行的接触点；2) 将规划优化重构为并行离散搜索，极大降低了计算开销，实现了规划与大规模并行RL的高效耦合；3) 提出多阶段训练框架，通过从安全基策略微调速度专家，并用LoRA整合，解决了单一策略在全速域性能受限及专家切换不稳定的问题，最终实现了高速且稳定的楼梯上行。</p>
<p><strong>局限性</strong>：论文提到方法依赖于准确的局部高程图感知，在感知退化场景下性能可能受影响。此外，规划器基于简化的VHIP和近似a≈1的动力学模型，在极端动态条件下可能不够精确。</p>
<p><strong>启示</strong>：本研究为结合模型规划安全性与学习策略敏捷性提供了有效范式。“优化即搜索”的思路为在RL训练中高效集成复杂模型约束开辟了新途径。多阶段专业化训练（基策略→专家→统一）的策略，对于需要覆盖宽泛操作域且内部动态差异大的复杂技能学习具有借鉴意义。利用LoRA等参数高效微调技术整合多个子技能，是实现单一策略多功能化且保持高性能的可行方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人爬楼梯时敏捷性与稳定性难以兼顾的核心问题，提出FastStair框架。该方法融合基于模型的立足点规划器与无模型强化学习，通过多阶段训练：先利用规划器预训练注重安全的基础策略，再通过LoRA微调集成不同速度的专家策略，以克服保守性并实现全速域平滑控制。在Oli机器人上部署后，实现了最高1.65 m/s的稳定上楼梯速度，12秒内爬完33级螺旋楼梯，并在广州塔机器人爬楼比赛中获胜。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.10365" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>