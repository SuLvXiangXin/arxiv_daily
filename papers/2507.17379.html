<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.17379" target="_blank" rel="noreferrer">2507.17379</a></span>
        <span>作者: Tan, Shen, Zhou, Dong, Shao, Xiangyu, Wang, Junqiao, Sun, Guanghui</span>
        <span>日期: 2025/07/23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>开放词汇移动操作（OVMM）要求机器人在不同工作空间内处理未见过的物体，这对现实世界的机器人应用构成重大挑战。目前，机器人操作主要依赖两类方法：一是基于视觉的物体中心表示方法（如位姿、类别、实例分割），这类方法需要针对特定场景的训练数据，难以泛化到未见物体；二是从专家演示中学习的端到端模型，它们虽然能提取更通用的表示并零样本适应新场景，但受限于单模态数据提供的信息不足，或需要目标图像作为指令来适应新情况。在现实场景中，为每个新任务提供额外的演示或目标图像是不切实际的。因此，模型必须具备对先前未见任务的开放词汇泛化能力。</p>
<p>本文针对上述痛点，提出利用自然语言指令作为直接接口来指定目标，并结合大型预训练模型的强大语义理解和泛化能力。具体而言，本文提出了一个名为LOVMM的语言条件开放词汇移动操作框架。其核心思路是：利用大型语言模型（LLM）解析自由形式的自然语言指令并进行推理，利用视觉语言模型（VLM）进行多模态感知以构建3D视觉语言地图进行导航，并构建一个融合CLIP语义信息和Transporter网络空间信息的双流模型，来预测6自由度操作位姿，从而在复杂家庭环境中处理各种OVMM任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>LOVMM将复杂的OVMM任务分解为一系列开放词汇机器人导航和操作子任务。整体流程如图2所示。</p>
<p><img src="https://arxiv.org/html/2507.17379v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：LOVMM概览。给定自由形式的自然语言指令，LLM首先解析指令以提取目标工作空间和目标操作描述。然后利用场景的3D视觉语言地图进行开放词汇定位，使机器人导航到特定工作空间。一旦机器人到达目标位置，端到端操作模型处理目标操作描述和RGB-D观测，以预测用于操作的6-DoF动作位姿。通过迭代完成这些子任务，LOVMM使机器人能够执行复杂的OVMM任务。</p>
</blockquote>
<p><strong>开放词汇导航</strong>：给定一个自由形式的自然语言指令，LOVMM首先使用GPT-4作为解析器，解释出目标工作空间描述 $\mathbf{l}<em>{w_t}$ 和目标操作描述 $\mathbf{l}</em>{m_t}$。接着，遵循先前工作，利用当前场景的3D重建，使用LSeg像素嵌入构建一个视觉语言特征地图矩阵 $Q$。将解析出的目标工作空间文本列表用CLIP文本编码器编码成嵌入矩阵 $E$。通过计算给定目标工作空间文本与地图像素之间的相似度，并选择最相似的像素，可以定位每个目标工作空间并获得其位置 $\mathbf{p}_t$，用于机器人导航。</p>
<p><strong>端到端语言条件操作</strong>：到达目标工作空间后，机器人执行桌面操作。操作模型采用双流架构，融合视觉观察和自然语言指令，其结构如图3所示。</p>
<p><img src="https://arxiv.org/html/2507.17379v1/x3.png" alt="操作模型架构"></p>
<blockquote>
<p><strong>图3</strong>：提出的端到端操作模型架构，采用双流架构来融合拾放操作任务的视觉观察和自然语言指令。融合后的特征嵌入被裁剪并进行互相关以产生2D可供性，并进一步利用多层感知机预测6-DoF操作位姿。</p>
</blockquote>
<p>该模型基于Transporter网络的模板匹配思想进行扩展。首先，一个全卷积网络（FCN） $f_{pick}$ 以观测和操作描述 $\gamma_t = (\mathbf{o}<em>t, \mathbf{l}</em>{m_t})$ 为输入，输出拾取动作价值预测 $\mathcal{Q}<em>{\text{pick}}$，并通过取最大值得到拾取动作 $\mathcal{T}</em>{\text{pick}}$（一个SE(2)位姿）。其次，另外两个FCN $\psi$ 和 $\phi$ 处理相同输入，输出特征嵌入。然后，以 $\mathcal{T}<em>{\text{pick}}$ 为中心从 $\psi$ 的输出中裁剪出一个特征模板，与 $\phi$ 的输出进行互相关，计算放置动作价值 $\mathcal{Q}</em>{\text{place}}$，并得到放置动作 $\mathcal{T}_{\text{place}}$（也是一个SE(2)位姿）。与原始Transporter不同，本方法裁剪特征嵌入而非直接裁剪输入观测，以获得更好的感受野。</p>
<p>为了学习6-DoF操作，模型进一步利用嵌入在特征表示中的丰富空间信息。在 $\psi$ 和 $\phi$ 的输出层后应用 $1\times1$ 卷积调整特征通道维度，然后将特征通道分成三个子集。对每个子集使用单独的互相关和一个MLP网络 $f(\cdot)$，来学习剩余自由度的精确值（例如抓取高度、放置高度、偏航旋转等）。最终，完整的6-DoF操作位姿由2D平面动作和这些预测的额外自由度值组合而成。</p>
<p><strong>创新点</strong>：1) <strong>系统整合</strong>：创新性地将LLM（用于指令解析与推理）、VLM（用于开放词汇导航与感知）以及改进的模仿学习架构（用于6-DoF操作）整合到一个统一的框架中，以处理自由形式的语言指令。2) <strong>6-DoF扩展</strong>：将原本用于3-DoF操作的CLIPort风格双流模型扩展为能够预测完整6-DoF操作位姿的模型，通过特征通道分离和专用MLP来学习额外的自由度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟家庭环境中进行，使用一个带吸盘夹爪的移动机器人。实验基于CLIPort基准进行扩展，包含了16个不同的已见和未见语言条件任务，超过3.5万步演示，如图1所示。</p>
<p><img src="https://arxiv.org/html/2507.17379v1/x1.png" alt="任务示例"></p>
<blockquote>
<p><strong>图1</strong>：自然语言条件的未见OVMM任务示例。我们在模拟室内家庭场景中基于CLIPort基准进行了大规模实验，涉及16个OVMM任务。</p>
</blockquote>
<p><strong>对比方法</strong>：主要与近年来的视觉操作方法对比，包括CLIPort、PerAct、RVT，并在桌面操作任务上对比了GNFactor、VAT-Mart、RVT等方法。</p>
<p><strong>关键数值结果</strong>：</p>
<ol>
<li><strong>OVMM任务</strong>：在已见（Seen）的8个任务上，LOVMM取得了平均**86.1%<strong>的成功率。在未见（Unseen）的8个任务上，取得了平均</strong>71.3%**的成功率，显著高于其他基线方法（CLIPort: 38.8%， PerAct: 21.3%）。</li>
<li><strong>桌面操作任务</strong>：在包括堆叠、套环、包裹等任务上，LOVMM也取得了最佳或极具竞争力的性能。例如，在“套环”任务上达到**100%<strong>成功率，“堆叠”任务上达到</strong>95%**。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.17379v1/x4.png" alt="OVMM任务成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：不同方法在已见和未见OVMM任务上的平均成功率对比。LOVMM在两种设置下均显著优于基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.17379v1/x5.png" alt="桌面操作任务成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：LOVMM与最先进的视觉操作方法在桌面操作任务上的成功率对比。LOVMM在多个任务上表现最佳。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>研究移除了语言条件（No Language）、CLIP视觉编码器（No CLIP-V）、CLIP文本编码器（No CLIP-T）以及6-DoF扩展模块（2D Action）的影响。结果（图6）表明，每个组件都对性能有重要贡献。其中，语言条件和CLIP文本编码器对模型理解任务指令至关重要；CLIP视觉编码器提供了更好的语义特征；而6-DoF扩展模块对于完成需要精确高度或旋转的操作是必需的。</p>
<p><img src="https://arxiv.org/html/2507.17379v1/x6.png" alt="消融研究"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果，展示了语言条件、CLIP模块和6-DoF扩展对LOVMM在已见和未见任务上性能的影响。</p>
</blockquote>
<p><strong>定性结果</strong>：<br>图7和图8展示了LOVMM在复杂、未见过的家庭场景中成功执行多种语言指令的OVMM任务序列，例如“将客厅沙发上的瓶子打包到次卧的盒子里”，验证了其开放词汇导航和跨工作空间操作的零样本泛化能力。</p>
<p><img src="https://arxiv.org/html/2507.17379v1/x7.png" alt="定性结果序列1"></p>
<blockquote>
<p><strong>图7</strong>：LOVMM在模拟家庭环境中执行复杂语言指令的定性结果序列示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.17379v1/x8.png" alt="定性结果序列2"></p>
<blockquote>
<p><strong>图8</strong>：另一个LOVMM成功执行涉及多个工作空间和未见物体的OVMM任务的定性示例。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>LOVMM框架</strong>，首次将LLM的推理能力、VLM的开放词汇感知能力与端到端的6-DoF操作学习相结合，使机器人能够仅凭自由形式的自然语言指令在复杂家庭环境中完成开放词汇移动操作任务。</li>
<li>设计了一种<strong>端到端的6-DoF操作模型</strong>，通过双流架构有效融合多模态输入的联合语义和空间信息，高效学习精确的3D操作。</li>
<li>在扩展的OVMM任务基准上进行了全面实验，证明了LOVMM出色的<strong>多任务学习能力和零样本泛化性能</strong>，并在桌面操作任务上超越了当前先进的视觉操作方法。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，实验主要在模拟环境中进行，在真实世界中的复杂性（如动态光照、物体纹理变化、精确的机器人控制）可能带来额外挑战。此外，框架依赖于GPT-4和VLM等大型预训练模型，可能存在计算成本较高的问题。6-DoF操作预测的精度，尤其是在高度和旋转方面，仍有提升空间。</p>
<p><strong>后续启示</strong>：本研究展示了大型预训练模型（LLM/VLM）与经典机器人模仿学习架构结合的巨大潜力，为处理复杂、开放词汇的具身智能任务提供了有效范式。未来的工作可以探索更轻量化的模型替代方案以减少计算开销，进一步研究在真实物理世界中的部署和鲁棒性提升，以及如何将更复杂的动作基元（如抛掷、推等）集成到该框架中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对开放词汇移动操作（OVMM）在家庭环境中处理未见物体和跨场景泛化的核心挑战，提出了LOVMM框架。该框架通过融合大型语言模型（LLM）与视觉语言模型（VLM），利用自然语言指令直接驱动机器人完成移动抓取等复杂任务。在模拟家庭环境的大规模实验中，该方法在16项OVMM任务上展现出强大的零样本泛化与多任务学习能力，并在多项桌面操作任务中取得了优于现有先进方法的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.17379" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>