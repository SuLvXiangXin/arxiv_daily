<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.06240" target="_blank" rel="noreferrer">2511.06240</a></span>
        <span>作者: Lin, Tzu-Jung, Yeh, Jia-Fong, Su, Hung-Ting, Lin, Chung-Yi, Chen, Yi-Ting, Hsu, Winston H.</span>
        <span>日期: 2025/11/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在开放词汇移动操作（OVMM）中，任务的成功往往取决于为机器人选择一个合适的基座放置位置。现有方法通常导航到基于接近度的区域，而不考虑可操作性，导致频繁的操作失败。例如，仅依赖几何信息的经典路径规划器（如A<em>、RRT</em>）缺乏任务级的语义理解，可能产生无法对准抽屉或把手的放置位置。反之，利用视觉语言模型（VLM）的语义方法可以推断高级任务意图，但通常忽略了几何可行性和可达性约束，并且依赖于单视角RGB图像，限制了其对遮挡或不可见区域的推理能力。</p>
<p>本文针对在有限视野下，需要联合推理语义意图与几何可行性的具体痛点，提出了一个新视角：通过构建跨模态表示和粗到细的优化过程，将VLM的语义理解与几何约束相结合。本文的核心思路是：首先利用VLM提供的粗略语义先验指导搜索朝向任务相关区域，然后通过迭代优化过程，在几何约束下细化放置位置，从而减少陷入局部最优的风险。</p>
<h2 id="方法详解">方法详解</h2>
<p>该方法是一个零样本框架，旨在选择语义上合理且几何上可行的基座放置位置。整体流程分为两个关键阶段：1）<strong>可操作性引导投影</strong>：构建跨模态表示，将语义与空间布局对齐；2）<strong>可操作性驱动的粗到细优化</strong>：通过迭代采样和VLM反馈，从语义探索逐步收敛到几何精确的放置位置。</p>
<p><img src="https://arxiv.org/html/2511.06240v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Affordance-Guided Coarse-to-Fine Exploration方法整体框架。左侧为Affordance Guidance Projection，将语义线索投影到障碍物地图上；右侧为Affordance-Driven Coarse-to-Fine Optimization，通过迭代采样和VLM反馈优化基座放置。</p>
</blockquote>
<p><strong>核心模块一：Affordance Guidance Projection</strong><br>为了克服单视角感知的局限，该方法构建了两种互补的多模态表示：</p>
<ul>
<li>**Affordance RGB (I_aff)**：在原始RGB图像上叠加两种视觉元素。一是12个不同颜色、间隔30°环绕物体的方向箭头；二是一个标有“A”的箭头，指示VLM建议的粗略可操作方向。</li>
<li>**Obstacle Map+ (M_local+)**：在机器人本地的二维占据栅格地图上增强四种信息。包括分割出的目标物体轮廓区域、机器人当前位置、一个以“A”方向为中心、跨度±60°的扇形可操作区域，以及所有12个方向箭头（颜色与RGB图像中的保持一致）。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.06240v2/affordance_rgb.png" alt="Affordance RGB示例"></p>
<blockquote>
<p><strong>图8</strong>：Affordance RGB示例。在RGB图像上叠加了12个方向箭头（不同颜色）和一个标有“A”的箭头，表示VLM建议的粗略可操作方向。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.06240v2/obstacle_map_plus.png" alt="Obstacle Map+示例"></p>
<blockquote>
<p><strong>图9</strong>：Obstacle Map+示例。在俯视障碍物地图上增强了目标物体轮廓（红色区域）、机器人位置（蓝色）、扇形可操作区域（绿色）以及方向箭头。</p>
</blockquote>
<p><strong>核心模块二：Affordance-Driven Coarse-to-Fine Optimization</strong><br>优化过程从一个可操作关键点 <strong>g</strong> 开始，该点通过结合DINOv2特征、Grounded SAM分割和GPT-4o的语义选择来确定。随后进行T次迭代优化，每次迭代包含评分、采样和细化三个阶段：</p>
<ol>
<li><strong>评分</strong>：在关键点 <strong>g</strong> 周围，从一个截断的高斯分布中采样N个无碰撞的候选基座位置 <strong>x_i</strong>。每个候选位置通过一个复合评分函数 <strong>w(x)</strong> 进行评估：<br><code>w(x) = w_geo(x)^(α_t) * w_sem(x)^(1-α_t)</code><br>其中，几何项 <code>w_geo</code> 鼓励采样在距 <strong>g</strong> 一个优选距离 <code>r*</code> 附近；语义项 <code>w_sem</code> 鼓励采样靠近一个演化的语义中心 <code>μ_t</code>。关键创新在于权重系数 <code>α_t</code> 采用Sigmoid调度，从0逐渐增加到 <code>α_max</code>，实现了从早期侧重语义对齐到后期侧重几何精度的平滑过渡。</li>
<li><strong>采样</strong>：根据归一化的评分构成概率分布，从中采样 <code>N_sample</code> 个候选位置，将其索引投影到 <code>M_local+</code> 地图上。</li>
<li><strong>细化</strong>：将索引化的 <code>M_local+</code> 地图、<code>I_aff</code> 图像和子任务指令 <code>ℓ~</code> 联合作为多模态提示，提交给VLM进行语义排名。VLM返回前k个语义相关的点，用于更新语义中心 <code>μ_t</code>（<code>σ_s</code> 随迭代递减以促进收敛）。在最终迭代（t=T）时，对VLM排名前5的候选点进行过滤和平均，得到最终的基座放置位置。</li>
</ol>
<p>与现有方法相比，创新点体现在：1) 通过跨模态表示（Affordance RGB 和 Obstacle Map+）将VLM的语义推理能力扩展到空间地图上，克服了单视角RGB输入的局限；2) 提出了一个动态调整语义与几何权重的迭代优化机制，实现了真正的“粗到细”搜索，避免了单一目标函数可能导致的次优解。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在NVIDIA Isaac Sim仿真环境中进行，使用TIAGo++移动操作平台。评估了五个多样化的开放词汇移动操作任务：1) 将罐头扔进垃圾桶；2) 将锅移到红马克杯附近；3) 将马克杯放到架子上；4) 打开橱柜；5) 打开洗碗机。每个任务随机初始化执行20次。</p>
<p><strong>对比方法</strong>：</p>
<ul>
<li><em><em>Object Center + A</em>/RRT</em>**：经典几何规划器，基于目标物体中心选择固定距离的放置点。</li>
<li><em><em>Affordance Point + A</em>/RRT</em>**：使用VLM选择可操作点 <strong>g</strong>，然后基于 <strong>g</strong> 用几何规划器选择放置点。</li>
<li>**Pivot (I)**：基于PIVOT的VLM提示方法，仅使用RGB图像。</li>
<li>**Pivot (M_local+, I_aff)**：PIVOT的变体，使用与本方法相同的多模态输入（<code>M_local+</code> 和 <code>I_aff</code>）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>如表1所示，本方法在五个任务上的总体成功率达到*<em>85%*<em>，显著优于所有基线方法。几何规划器（Object Center + A</em>/RRT*）在简单任务（如扔罐头）上表现尚可，但在需要特定方向的任务上失败率高，总体成功率仅为47%和50%。引入可操作点指导的几何规划器（Affordance Point + A</em>/RRT*）有所提升，达到61%，但仍缺乏对方向和可达性的精细推理。纯VLM提示方法Pivot (I)成功率仅26%，其多模态变体Pivot (M_local+, I_aff)也仅为23%，表明它们难以产生几何可行的放置位置。</p>
<p><img src="https://arxiv.org/html/2511.06240v2/x3.png" alt="结果对比"></p>
<blockquote>
<p><strong>图3</strong>：任务“Open the cabinet”的基座放置分布演化对比。A*/RRT*基线（上行）选择的角度不理想；Pivot基线（中行）距离目标过远；我们的方法（下行）收敛到既可行又语义合适的位置。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>实验对比了评分函数中权重系数 <code>α_t</code> 的不同设置（表2）。固定 <code>α=0</code>（仅语义）导致采样集中在任务相关区域但常不可达，成功率43%。固定 <code>α=0.5</code>（平衡）引入几何约束后提升至76%。固定 <code>α=1</code>（仅几何）采样集中在可操作点周围的环上，但可能缺乏语义合适的选项，成功率79%。而采用本文提出的<strong>动态增加 <code>α_t</code> 的粗到细策略</strong>取得了最佳效果（85%），证明了从语义探索过渡到几何细化的必要性。</p>
<p><img src="https://arxiv.org/html/2511.06240v2/x4.png" alt="消融实验结果表"></p>
<blockquote>
<p><strong>图4</strong>：表2的图表形式展示。不同α设置下的任务成功率对比，表明动态增加的粗到细策略（Ours）性能最优。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>明确了在开放词汇移动操作中，基座放置是一个需要联合推理语义与几何的关键挑战。</li>
<li>提出了一个新颖的零样本框架，通过构建跨模态表示（Affordance RGB 和 Obstacle Map+）和粗到细的迭代优化过程，有效整合了语义先验与几何约束。</li>
<li>在多样化的仿真任务上实现了85%的成功率，显著超越了经典的几何规划器和纯语义的VLM方法，展示了该框架的泛化能力和有效性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前方法假设目标物体的二维位置已知（由仿真器提供），并未解决开放世界中的物体定位问题。此外，实验主要在仿真环境中进行。</p>
<p><strong>对后续研究的启示</strong>：本研究展示了结合大模型语义能力与机器人几何推理的潜力。后续工作可以探索在更开放、未知的环境中在线进行物体定位与基座放置的联合优化，或者将类似的“粗到细”和跨模态表示思想应用于移动操作的其他子问题，如臂展规划或抓取姿态选择。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决开放词汇移动操作中机器人基座放置不当导致操作失败的问题。提出“功能可供性引导的从粗到精探索”零样本框架，通过整合视觉语言模型的语义理解与几何可行性进行迭代优化。关键技术包括构建跨模态表示（Affordance RGB与Obstacle Map+）以对齐语义与空间上下文，并利用粗略语义先验引导搜索，再以几何约束细化放置。在五项多样任务上的实验表明，该方法成功率高达85%，显著优于传统几何规划器与基于VLM的方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.06240" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>