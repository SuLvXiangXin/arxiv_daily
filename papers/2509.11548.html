<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>How Auxiliary Reasoning Unleashes GUI Grounding in VLMs - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>How Auxiliary Reasoning Unleashes GUI Grounding in VLMs</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.11548" target="_blank" rel="noreferrer">2509.11548</a></span>
        <span>作者: Li, Weiming, Shao, Yan, Yang, Jing, Lu, Yujing, Zhong, Ling, Wang, Yuhan, Duan, Manni</span>
        <span>日期: 2025/09/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>图形用户界面（GUI）定位是构建GUI智能体的基础任务，要求模型根据自然语言指令定位屏幕上的特定组件并输出精确坐标。尽管通用视觉语言模型（VLMs）在多模态理解上取得进展，但在GUI定位任务上表现不佳。这是因为GUI截图包含密集文本、细粒度图标和高度结构化的布局，这些特征在标准VLM预训练中未被强调。当前提升性能的主流方法主要依赖于大规模标注数据进行微调（如UGround、CogAgent），或通过修改推理过程来引入结构/空间线索（如迭代缩小机制），前者成本高昂，后者需要改动模型流程。</p>
<p>本文发现了一个关键矛盾：通过“指向游戏”（Pointing Game）评估发现，VLMs其实拥有显著的潜在定位能力，但让它们直接输出显式坐标时，性能却急剧下降。这表明VLMs隐含的空间理解能力未被有效激发用于显式坐标预测。针对这一痛点，本文提出了一种新视角：无需微调模型参数，仅通过提供显式的空间线索作为输入图像的一部分，即可引导VLMs表达其隐含的空间理解能力，从而以零样本方式提升GUI定位性能。本文的核心思路是设计三种轻量级的辅助推理方法，通过向输入图像叠加坐标轴、网格和带标签的交点等视觉提示，解锁VLMs的GUI定位潜力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文首先详细阐述了用于探测VLM潜在定位能力的“指向游戏”（Pointing Game）的实现，然后提出了三种零样本辅助推理方法。</p>
<p><strong>指向游戏（Pointing Game）实现</strong>：该方法通过分析模型注意力机制来评估潜在能力。具体流程是：给定输入提示（如“Where should I click if I want to {instruction}?”），提取最后一个文本令牌（对应“{instruction}”）到所有图像令牌的注意力权重。对这些权重进行平均、重塑为空间图并调整至原图尺寸，得到注意力热图。定位热图中的最大值点，若该点落在真实目标区域内，则视为定位成功。论文遍历所有注意力层，取各层成功结果的并集，并将观察到的最高准确率作为模型潜在GUI定位能力的指标。需要注意的是，此方法仅用于诊断，不直接用于推理，因为最优的注意力层是任务和样本特定的。</p>
<p><strong>三种辅助推理方法</strong>：核心思想是在推理时向输入图像添加额外的空间视觉线索，以增强模型的空间感知和推理能力。</p>
<ol>
<li><strong>坐标支架（Coordinate Scaffold）</strong>：在原始图像上叠加一系列锚点（点阵），并在每个锚点旁标注其在该图像中的实际（x, y）坐标。这直接与预测显式坐标的任务目标对齐。</li>
<li><strong>轴-网格支架（Axis-Grid Scaffold）</strong>：在图像的四条边上以100像素为间隔添加坐标刻度，并在整个图像上叠加对应的网格线。这种组合提供了一个详细且显式的空间结构，有助于模型推理对象的位置和关系。</li>
<li><strong>标记-网格支架（Mark-Grid Scaffold）</strong>：该方法将连续的坐标预测简化为离散的网格ID预测任务。具体分为两步：首先，在原图上叠加一个8x8网格，每个网格单元中心标有唯一ID。模型预测目标对象四个极端位置（最左、最上、最右、最下）对应的网格ID，形成一个初始边界框。然后，用该边界框裁剪出感兴趣区域，并将其短边等比缩放到512像素，再次应用相同的8x8网格。模型同时看到带有预测边界框的原图以及放大的、带网格标记的裁剪区域，并再次预测目标在更精细视图下的四个极端网格ID。最终的目标坐标由这四个网格ID计算出的中心坐标确定。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11548v1/paper/fig1.png" alt="方法示例"></p>
<blockquote>
<p><strong>图1</strong>：不同GUI定位方法在截图样本上的对比。(a) 直接预测：模型尝试直接输出坐标。(b) 指向游戏：通过识别最高注意力点来说明模型的潜在定位能力。(c) 坐标支架、(d) 轴-网格支架和(e) 标记-网格支架是本文提出的三种辅助推理方法。</p>
</blockquote>
<p>与现有方法相比，本文的创新点在于：1）首次系统性地揭示了VLMs在GUI定位任务上潜在能力与显式输出性能之间的巨大差距；2）提出了专门针对GUI领域特性设计的零样本辅助推理方法，特别是标记-网格支架的两步细化策略，在保持轻量化的同时显著提升了坐标预测精度；3）这些方法无需修改模型参数或训练过程，兼容开源和专有VLMs。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：使用了四个GUI定位基准：ScreenSpot、ScreenSpot-v2（修正版）、ScreenSpot-Pro（专业挑战场景）和UI-I2E-Bench。</li>
<li><strong>基线方法</strong>：包括直接预测（Direct Prediction）、网格增强视觉（Grid-Augmented Vision，叠加9x9黑色网格）和原始的支架提示（Scaffold Prompting）。</li>
<li><strong>评估模型</strong>：涵盖了七种开源及专有VLM：Gemini-2.5-Flash, GPT-4o, Claude-3.5-Sonnet, Qwen2-VL-7B, Gemma-3-12B, Gemma-3-4B 和 SigLip-400M。评估指标为点击准确率（预测点落在真实边界框内的比例）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>潜在能力与显式输出的差距</strong>：如图2所示，在所有测试的VLM和基准上，指向游戏（评估潜在能力）的得分远高于直接预测（评估显式输出）。例如，在ScreenSpot上，Gemma-3-12B的直接预测准确率为9.59%，而其指向游戏得分高达47.56%。这证实了VLMs拥有未开发的定位潜力，是本文工作的核心动机。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11548v1/paper/fig2.png" alt="潜在能力与显式输出差距"></p>
<blockquote>
<p><strong>图2</strong>：四种开源VLMs在四个GUI定位基准上，直接预测与指向游戏结果的对比。显著的性能差距是本文工作的动机。</p>
</blockquote>
<ol start="2">
<li><p><strong>辅助推理方法性能对比</strong>：如表1所示，本文提出的三种辅助推理方法在大多数VLM和基准上显著优于基线方法。其中，标记-网格支架（Mark-Grid Scaffold）提升最为显著。例如，在ScreenSpot-v2上，它将Gemini-2.5-Flash的准确率从5.50%提升至72.09%。值得注意的是，对于经过专门定位训练的Qwen2-VL-7B，辅助方法的提升有限甚至有所下降，这表明本文方法对未经任务特定优化的通用VLM更为有效。</p>
</li>
<li><p><strong>消融实验</strong>：如图3所示，针对每种方法进行了详细的组件分析。</p>
<ul>
<li><strong>坐标支架</strong>：标注显式坐标（Dots+Coords）比仅有点阵（Dots）或点阵加符号索引（Dots+Indices）效果更好。</li>
<li><strong>轴-网格支架</strong>：同时添加坐标轴和网格（Axis+Grid）比仅添加坐标轴（Axis）或原始图像（Origin）性能更优。网格大小为100像素、坐标轴置于图像四边（All Sides）时效果最佳。</li>
<li><strong>标记-网格支架</strong>：网格大小与放大次数存在权衡。5x5网格配合两次放大能达到最佳性能，但8x8网格配合一次放大在性能和计算效率间取得了更好平衡，因此被选为默认配置。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.11548v1/paper/fig3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：在ScreenSpot-v2基准上使用Gemini-2.5-Flash进行的消融研究结果。(a) 坐标支架各组件效果。(b) 轴-网格支架各组件效果。(c) 网格大小对轴-网格支架的影响。(d) 坐标轴方向对轴-网格支架的影响。(e) 不同网格大小和放大次数对标记-网格支架的影响。</p>
</blockquote>
<p>消融实验总结出有效视觉辅助线索的三个关键特质：<strong>显式性</strong>（如直接坐标参考优于符号线索）、<strong>集成性</strong>（组合多种视觉元素优于单一组件）和<strong>平衡的粒度</strong>（避免过于简单或视觉杂乱）。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）首次揭示了通用VLMs在GUI定位任务上存在显著的潜在能力与显式输出性能之间的差距；2）提出了三种轻量级、零样本的辅助推理方法（坐标支架、轴-网格支架、标记-网格支架），通过添加简单的空间视觉线索，有效激发了VLMs的隐含空间理解能力，且兼容开源和专有模型；3）在四个基准和七个VLM上进行了全面评估，阐明了每种方法的独特优势和适用场景。</p>
<p>论文自身提到的局限性在于，这些方法的效果因模型而异，对于像Qwen2-VL-7B这样经过专门定位训练的模型，提升效果有限甚至可能下降。</p>
<p>本文工作对后续研究的启示在于：为提升VLMs在细粒度空间推理任务上的性能提供了一条高效、低成本的路径。无需昂贵的数据标注和模型微调，仅通过设计巧妙的输入视觉提示即可显著提升性能。未来的研究可以探索更精细或自适应的视觉提示生成方法，或者将这种辅助推理思想扩展到其他需要精确定位的视觉语言任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型在图形用户界面定位任务中输出显式坐标能力不足的问题，提出三种零样本辅助推理方法。通过向输入图像叠加提供明确空间线索（如坐标轴、网格和标记交点），引导模型将隐式的空间理解能力转化为准确的坐标输出。在四个GUI定位基准和七个开源及专有VLM上的评估表明，该方法无需微调即可显著提升模型的定位性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.11548" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>