<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.05013" target="_blank" rel="noreferrer">2510.05013</a></span>
        <span>作者: Jun Tani Team</span>
        <span>日期: 2025-10-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，在机器人或人工智能领域，让机器通过有限经验学习语言和动作的主流方法通常依赖于监督学习或批量关联映射。这些方法忽略了人类婴儿在发展中至关重要的自主探索和内在动机驱动的学习过程。本文针对“刺激贫乏”问题，即如何像人类婴儿一样仅从稀疏的输入中高效地习得可组合泛化的语言-动作能力，提出了一个基于好奇心驱动自我探索的新视角。本文的核心思路是将主动推断框架与强化学习相结合，使机器人通过最大化信息增益（好奇心）和动作随机性（动作熵）的内在奖励，自主探索环境并学习将命令语句与对应动作关联起来，从而实现发展式学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的整体框架是一个集成主动推断的强化学习模型。在每个试次中，机器人接收一个由动词、形容词和名词组成的祈使句作为目标，并尝试通过生成一系列电机命令来达成该目标。模型在探索和学习过程中，持续接收多模态感觉输入（视觉、触觉、本体感觉、命令语音和导师反馈语音）。</p>
<p><img src="https://arxiv.org/html/2510.05013v5/images/new_architecture.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：提出的模型架构。模型由三个主要部分组成：前向模型、行动者部分和评论者部分。前向模型基于当前感觉 <code>o_t</code> 和执行的电机命令 <code>a_t</code> 预测下一时刻的感觉 <code>o_{t+1}</code>。行动者模块基于整合了当前感觉的确定性潜变量 <code>h^q_t</code> 生成下一个电机命令 <code>a_{t+1}</code>。评论者基于 <code>a_t</code> 和 <code>h^q_t</code> 生成对未来Q值的预测 <code>\widehat{Q}</code>。</p>
</blockquote>
<p>模型的核心是<strong>前向模型</strong>、<strong>行动者</strong>和<strong>评论者</strong>。前向模型使用随机潜变量 <code>z_t</code> 和确定性潜变量 <code>h^q_t</code> 进行上下文和随机预测，以处理环境的隐藏状态和概率性。确定性变量在各感觉模态间共享，而随机潜变量则为每种模态单独分配，以处理不同类型的感觉信息。</p>
<p>学习过程由两种自由能最小化驱动：</p>
<ol>
<li><p>**证据自由能 <code>F</code>**：用于训练前向模型，其目标是准确预测未来感觉，并保持内部信念的一致性。<br><code>F_{\psi,t}=D_{KL}[q(z_t|o_t, h_{t-1})||p(z_t|h_{t-1})] - \mathbb{E}_{q(z_t)}[\log p(o_{t+1}|h_t)]</code>。其中第一项是复杂性（后验与先验的KL散度），第二项是准确性（预测对数似然）。通过最小化 <code>F</code> 来更新前向模型参数 <code>\psi</code>。</p>
</li>
<li><p>**期望自由能 <code>G</code>**：用于指导行动者通过强化学习生成动作，其目标是最大化信息增益、外在奖励和动作熵。<br><code>G(a_t) = -D_{KL}[q(z_t|o_t, h_{t-1})||p(z_t|h_{t-1})] - r(s_t, a_t) - \mathcal{H}(\pi_{\phi}(a_t|h_{t-1}))</code>。其中第一项是复杂性（取负号意味着要最大化KL散度，即信息增益），第二项是外在奖励（成功达成目标），第三项是动作策略的熵（鼓励探索）。</p>
</li>
</ol>
<p>与现有方法相比，核心创新点在于将主动推断与强化学习深度整合，并利用<strong>好奇心</strong>（最大化KL散度，即信息增益）和<strong>动作熵</strong>作为内在奖励来驱动自我探索。值得注意的是，前向模型学习（最小化 <code>F</code>）和行动策略优化（最小化 <code>G</code>）在复杂性项上目标相反：前者试图最小化预测不确定性，后者试图寻找能增加不确定性的动作。这形成了一种“竞争”关系，促使智能体不断探索未知，同时模型不断学习以理解新体验。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在一个物理模拟器中进行，机器人配备轮子、机械臂、摄像头和分布式触觉传感器。对象有5种形状和6种颜色。目标由祈使句指定，词汇包括6个动词、6个形容词和5个名词（共180种可能组合）。基准实验设置是仅使用全部可能组合的33%（60句）进行训练，其余67%（120句）用于测试泛化能力。对比的基线主要是不同好奇心水平（无好奇心、感觉运动好奇心、全好奇心）下的模型自身表现，以及不同词汇组合规模下的泛化性能。</p>
<p><strong>实验1：好奇心的效果</strong><br><img src="https://arxiv.org/html/2510.05013v5/images/evaluation.png" alt="好奇心效果对比"></p>
<blockquote>
<p><strong>图3</strong>：针对未学习目标的滚动成功率，比较了具有不同好奇心水平的智能体。结果显示，<strong>全好奇心</strong>（包含所有感觉模态的好奇心）条件下的性能显著优于其他条件。</p>
</blockquote>
<p>在全好奇心条件下，智能体对未学习目标的平均最终成功率达到了<strong>85%<strong>（尽管只学习了33%的组合）。同时，不同动作类别的发展顺序符合预期：更简单、作为前提的动作（如“watch”）发展最快，依赖于前者的更复杂动作（如各种“push”）发展较晚。图6(A)（未在正文中详细展示，但提及了结果）进一步表明，对已学习目标的成功率发展远快于未学习目标，验证了早期</strong>机械记忆</strong>，后期<strong>组合泛化</strong>的假设。</p>
<p><strong>实验2：组合规模的效果</strong><br><img src="https://arxiv.org/html/2510.05013v5/images/generalization.png" alt="组合规模效果"></p>
<blockquote>
<p><strong>图6</strong>：不同组合规模下，对已学习和未学习目标的滚动成功率。(A) 使用全部6动词、6形容词、5名词（最大规模）训练的智能体。(B) 使用5动词、5形容词、4名词（中等规模）。(C) 使用4动词、4形容词、3名词（最小规模）。结果显示，<strong>组合规模越大，对未学习目标的泛化性能越好</strong>。</p>
</blockquote>
<p>实验结果表明，虽然对已学习目标的测试性能在所有规模下都同样高，但对未学习目标的最终成功率随着组合规模的减小而急剧下降：从最大规模的<strong>85%</strong> 降至最小规模的约**25%**。这证实了泛化性能严重依赖于学习示例中的组合规模。</p>
<p><strong>实验3：异常规则处理</strong><br><img src="https://arxiv.org/html/2510.05013v5/images/u-shapes.png" alt="异常规则学习曲线"></p>
<blockquote>
<p><strong>图7</strong>：比较10个个体在有/无异常处理规则情况下的性能曲线。(A) 学习两个被交换的异常目标时的成功率发展，其中7个个体显示出先上升、后下降、再恢复的<strong>U形曲线</strong>。(B) 在没有异常规则的情况下学习相同两个目标时，成功率单调上升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.05013v5/images/exceptions_compositions.png" alt="异常规则表征变化"></p>
<blockquote>
<p><strong>图8</strong>：一个经历了U形发展的个体，其语言潜变量的PCA可视化。(A) 早期训练，表征混杂。(B) 中期训练，异常命令（如“watch magenta pillar”）被错误地嵌入到其字面动词的集群中，表明<strong>过度泛化</strong>发生。(C) 后期训练，异常命令被正确地重新分配到交换后的动作集群，表明发生了<strong>表征重述</strong>。</p>
</blockquote>
<p>在设置了两条命令-动作映射交换的异常规则后，最终平均成功率为：已学习目标85%，未学习目标75%，异常处理目标49%。关键发现是，10个机器人中有7个在习得异常规则时表现出显著的U形学习曲线（早期成功，中期因过度泛化而失败，后期成功恢复），这与人类儿童语言学习中的现象类似。PCA分析（图8）直观展示了内部表征从混杂、到错误泛化、再到正确重组（表征重述）的演变过程。</p>
<p><strong>消融实验总结</strong>：实验1本质上是对好奇心内在奖励机制的消融研究。结果表明，<strong>全好奇心</strong>（结合了所有模态的信息增益和动作熵）对提升学习性能和泛化能力贡献最大，其次是仅包含感觉运动的好奇心，无好奇心的性能最差。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出并验证了一个基于好奇心驱动主动推断的机器人自我探索框架，能够从稀疏的示例中高效学习语言-动作映射；2）通过系统实验揭示了<strong>组合规模</strong>是促进强大泛化能力的关键因素，而<strong>好奇心</strong>是驱动此类发展式学习的核心机制；3）在人工系统中复现了人类语言习得中的多个关键发展模式，如机械记忆先于组合泛化、简单动作先于复杂动作发展，以及异常规则学习中的U形曲线和表征重述现象。</p>
<p>论文提到的局限性包括：实验仅在模拟环境中进行；异常规则的处理成功率（49%）仍有提升空间。这些结果为后续研究提供了重要启示：未来工作可以在真实机器人上验证该框架，探索更大规模的组合泛化极限，并进一步研究如何优化内在动机形式以更稳定地处理复杂异常。该研究路径为构建更接近人类学习效率、具备强泛化与适应能力的具身智能体提供了新的思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究机器人如何通过好奇驱动的自我探索，高效学习与语言指令关联的动作，以模拟人类婴儿从有限经验中快速泛化的能力。方法上，结合主动推断与强化学习，实现内在动机驱动的发展性学习。核心实验发现：1) 组合元素规模增大显著提升泛化能力；2) 好奇心通过自我探索改善学习；3) 学习过程遵循“死记硬背配对→组合泛化”和“简单动作→复杂动作”的序列；4) 异常处理引发类似儿童语言学习的U型发展曲线。结果表明，好奇驱动的主动推断机制能支持可扩展的组合泛化与异常处理能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.05013" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>