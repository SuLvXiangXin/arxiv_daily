<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.06482" target="_blank" rel="noreferrer">2505.06482</a></span>
        <span>作者: Pan, Minting, Zheng, Yitao, Li, Jiajian, Wang, Yunbo, Yang, Xiaokang</span>
        <span>日期: 2025/05/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>离线强化学习（RL）允许智能体从静态数据集中学习策略，避免了在线交互的成本与风险。然而，由于缺乏环境交互，它面临着离线行为次优和价值估计不准确的双重挑战。当前方法主要分为模型无关和模型基础两类，前者（如CQL）专注于缓解价值高估偏差，后者（如LOMPO）通过建模世界模型来管理动态不确定性。这些方法均受限于有限的、预定义的离线数据集，难以获取更广泛的常识性知识。</p>
<p>本文针对离线RL数据集不完整、分布偏移的痛点，提出了利用海量、易得的无标签互联网视频数据来增强智能体对物理世界和人类行为常识理解的新视角。核心思路是构建一个模型基础的框架，从无标签视频中提取高层次的“行为抽象”，并利用这些抽象来指导目标域内的策略优化，从而将视频中的常识知识迁移到离线RL任务中。</p>
<h2 id="方法详解">方法详解</h2>
<p>VeoRL的整体框架是一个三阶段流程：首先，训练一个行为抽象网络（BAN），从无标签视频中推断离散的潜在行为；其次，构建一个包含Trunk Net和Plan Net的双流世界模型；最后，在世界模型上进行基于模型的策略优化，其中Trunk Net的推演受Plan Net基于潜在行为预测的长远状态引导。</p>
<p><img src="https://arxiv.org/html/2505.06482v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：VeoRL训练框架概览。(a) 从与任务无关的无标签自然视频数据中提取潜在行为抽象，以丰富世界模型对物理世界的常识理解。智能体通过与这个世界模型交互，在从自然视频中学到的潜在策略的显式指导下进行策略优化。(b) VeoRL在多个视觉控制基准（Meta-World机器人操作、CARLA自动驾驶、Minecraft开放世界游戏）上的整体性能，相比现有离线RL方法有显著提升。</p>
</blockquote>
<p><strong>核心模块1：潜在行为抽象网络（BAN）</strong><br>为解决视频数据缺乏动作标签的挑战，VeoRL没有尝试预测原始动作，而是构建了一个离散的、高层次的潜在动作空间。BAN基于向量量化技术，其核心是一个可学习的码本，包含K个连续向量，每个向量代表一种行为抽象。给定目标数据集中的连续两帧观测，图像编码器提取特征并拼接，BAN通过最近邻搜索在码本中选择一个向量作为该状态转移对应的潜在行为。训练BAN的损失函数结合了向量量化损失和计划网络的视频预测损失，确保学到的行为能有效预测后续帧。为了将训练于目标域的BAN应用于分布外（OOD）的源域视频，论文在图像编码器输出上使用了最大均值差异损失来对齐不同域的视觉嵌入。</p>
<p><img src="https://arxiv.org/html/2505.06482v2/x2.png" alt="模型架构"></p>
<blockquote>
<p><strong>图2</strong>：模型架构。(a) 通过训练BAN构建离散的高层潜在动作空间，实现独立于真实动作的前向动力学建模。(b) 单步推演中基于模型的演员-评论家学习可视化。利用行为克隆模块重放视频信息化的潜在行为，作为演员和评论家的输入以产生目标条件策略和价值估计，同时也作为Plan Net的输入以生成长时段状态推演。</p>
</blockquote>
<p><strong>核心模块2：双流世界模型</strong><br>世界模型由两个分支构成：Trunk Net和Plan Net。Trunk Net基于智能体的真实动作预测状态转移和奖励反馈，专门在离线RL数据集上训练。Plan Net则基于BAN提供的潜在行为抽象来预测长远的状态轨迹，它包含一个行为克隆模块，用于根据当前潜在状态预测下一个潜在行为。两个网络共享图像编码器和解码器参数，但拥有独立的循环状态空间模型参数。Trunk Net的训练目标包括图像重建、奖励预测以及潜在状态先验与后验分布的KL散度。</p>
<p><strong>核心创新点：基于模型的策略优化与行为引导</strong><br>策略优化在Trunk Net产生的潜在状态序列上进行。关键创新在于引入了一个内在奖励，鼓励Trunk Net的短步状态-动作推演与Plan Net基于潜在行为预测的长远未来状态逐渐对齐。具体而言，演员网络和评论家网络都以当前Trunk Net状态和Plan Net预测的某个未来目标状态为条件。这样，从视频中学到的潜在行为序列为策略优化提供了高层次、目标导向的引导，同时通过结合环境奖励和内在奖励，有助于缓解纯粹依赖环境奖励训练价值网络导致的高估偏差。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：论文在三个视觉控制基准上评估VeoRL：1) <strong>Meta-World</strong>机器人操作任务，使用BridgeData-V2作为辅助真实世界视频源；2) <strong>CARLA</strong>自动驾驶任务，使用NuScenes数据集作为辅助源域；3) <strong>MineDojo</strong>开放世界游戏环境，使用人类玩家创建的在线Minecraft视频作为无标签演示。<br><strong>对比方法</strong>：主要对比了模型无关方法CQL（与DrQ-V2视觉RL主干集成）和模型基础方法LOMPO。<br><strong>关键结果</strong>：如图1b所示，VeoRL在所有基准测试中都显著超越了现有方法。在Meta-World的10个任务中，平均成功率较CQL提升超过100%。在CARLA的3个驾驶任务中，平均成功率较LOMPO提升约50%。在MineDojo的4项复杂任务中，也取得了显著的性能提升。</p>
<p><img src="https://arxiv.org/html/2505.06482v2/x3.png" alt="定量结果"></p>
<blockquote>
<p><strong>图3</strong>：在Meta-World基准上的详细性能对比。VeoRL在绝大多数任务上优于CQL和LOMPO，在某些任务上优势极其明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.06482v2/x4.png" alt="定性分析"></p>
<blockquote>
<p><strong>图4</strong>：潜在行为抽象的可视化。学到的离散潜在行为在语义上与目标环境中的真实动作（如机器人夹爪的打开/关闭、移动方向）对齐，验证了BAN的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.06482v2/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融研究结果。依次移除潜在行为抽象（w/o BAN）、双流世界模型中的Plan Net引导（w/o Plan）、以及内在奖励（w/o IR）都会导致性能下降，证明了每个组件的必要性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验（图5）明确了各组件贡献：1) <strong>潜在行为抽象（BAN）</strong> 是性能提升的关键，移除后性能大幅下降；2) <strong>双流世界模型中的Plan Net引导</strong> 对于利用视频知识至关重要；3) <strong>内在奖励</strong> 有效地连接了Trunk Net策略与Plan Net的长远预测，进一步提升了性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>VeoRL</strong>，一种新颖的模型基础离线RL范式，能够利用海量、易得的无标签互联网视频数据来增强智能体，缓解了离线数据集有限和分布偏移的问题。</li>
<li>设计了 <strong>潜在行为抽象网络（BAN）</strong> 和 <strong>双流世界模型</strong>，实现了从无标签视频到目标RL任务的、无需动作对齐的高层次行为知识迁移。</li>
<li>在机器人操作、自动驾驶和开放世界游戏等多个具有挑战性的视觉控制基准上取得了 <strong>显著的性能提升</strong>，部分任务性能提升超过100%。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法的效果在一定程度上依赖于辅助视频数据的质量及其与目标任务的语义相关性。此外，训练包含BAN和双流世界模型的框架可能带来额外的计算成本。</p>
<p><strong>启示</strong>：这项工作为如何利用互联网规模的多元视频数据来赋能序列决策学习开辟了新方向。后续研究可以探索更高效或更具表现力的行为抽象方法，研究如何自动筛选与目标任务最相关的视频源，以及将框架扩展到更广泛的视频源和任务领域。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对离线强化学习在静态数据集中因缺乏环境交互导致的次优行为和值估计不准确问题，提出Video-Enhanced Offline RL (VeoRL)方法。该方法基于模型构建交互式世界模型，从多样未标记在线视频中提取控制策略和物理动态的常识知识，通过行为指导优化策略。实验表明，在机器人操作、自动驾驶和开放世界游戏等视觉控制任务中，性能提升显著，部分任务超过100%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.06482" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>