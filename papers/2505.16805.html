<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.16805" target="_blank" rel="noreferrer">2505.16805</a></span>
        <span>作者: Chen, Xuesong, Huang, Linjiang, Ma, Tao, Fang, Rongyao, Shi, Shaoshuai, Li, Hongsheng</span>
        <span>日期: 2025/05/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，端到端（E2E）自动驾驶方法将感知到规划的整个流程整合在一个统一框架中学习，展现了强大的潜力，但依然面临学习复杂、常识推理不足、可解释性有限和因果混淆等挑战。例如，现有方法可能将卡车上的交通锥误判为路障。与此同时，大型视觉语言模型（VLM）凭借在海量多模态数据上训练获得的常识与逻辑推理能力，为上述问题提供了潜在的解决方案。然而，VLM自身存在效率低下和3D感知能力有限的问题，难以直接集成到需要实时决策的自动驾驶系统中。现有尝试（如DriveVLM）通常采用双系统架构，VLM与E2E模型独立运行，仅在轨迹层面进行后处理协作，这限制了二者在特征层面的知识与能力互补。</p>
<p>本文针对VLM与E2E模型高效、深度融合的痛点，提出了一种新的协同视角。核心思路是：通过一个共享的视觉编码器实现特征级的知识共享，并设计一种渐进式轨迹链式思维（T-CoT）范式来优化VLM的轨迹预测，最后通过时间解耦策略将VLM的高质量轨迹预测与E2E模型的实时处理能力相结合。</p>
<h2 id="方法详解">方法详解</h2>
<p>SOLVE的整体框架旨在实现VLM与E2E模型在特征和轨迹两个层面的协同。其输入是环视相机图像序列，输出是自车未来的规划轨迹。框架包含三个核心模块：1) 顺序查询生成器（SQ-Former），用于提取并压缩视觉特征；2) 轨迹链式思维（T-CoT），用于VLM进行从粗到精的轨迹预测；3) 时间解耦协同机制，使E2E模型能异步利用VLM的历史预测。</p>
<p><img src="https://arxiv.org/html/2505.16805v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：SOLVE的整体框架。SQ-Former从图像序列中提取并压缩视觉线索，生成统一的视觉令牌，同时供给VLM和E2E模型共享，实现特征级协同。VLM通过T-CoT机制进行轨迹预测，其预测结果被存入记忆库。E2E模型可以从记忆库中异步获取VLM的历史预测作为轨迹初始化的先验，实现轨迹级协同。</p>
</blockquote>
<p><strong>SQ-Former（顺序查询生成器）</strong>：此模块负责从多视角图像中高效提取并压缩对驾驶有用的视觉信息。它将视觉线索分为三组依次处理：1) 静态环境信息（如天气、时间）；2) 动态道路参与者（车辆、行人）；3) 动态地图信息（如车道线）。</p>
<p><img src="https://arxiv.org/html/2505.16805v1/x3.png" alt="SQ-Former细节"></p>
<blockquote>
<p><strong>图3</strong>：SQ-Former的详细结构。它首先从多视角图像中捕获静态线索，然后通过不同的解码器顺序地与物体检测、车道线检测等感知任务进行对齐，最终输出富含多任务信息的压缩视觉查询。</p>
</blockquote>
<p>具体而言，SQ-Former基于一个图像编码器（初始化为EVA-02-L）获取图像特征。首先，一组收集器查询（collector queries）通过一个Transformer解码器捕获第一组静态线索。随后，这些更新后的收集器查询会与特定任务（如检测、车道线）的查询拼接，并依次输入到各自对应的Transformer解码器中，以顺序集成不同任务的视觉线索。此过程引入了一个记忆库来融入场景的时序动态。最终，收集器查询被送入一个视觉适配器以匹配VLM的输入维度，并附加一个自车令牌用于轨迹预测。这种顺序设计使得SQ-Former仅用384个查询就能捕获丰富且压缩的视觉线索，显著降低了后续VLM的计算成本。</p>
<p><strong>Trajectory Chain-of-Thought（轨迹链式思维）</strong>：为了解决VLM以自回归方式直接生成细粒度轨迹的困难，本文提出了T-CoT范式，通过一个预定义的轨迹库进行链式推理，逐步细化轨迹预测。首先，使用k-means算法在训练集上为每个导航指令（直行、左转、右转）聚类生成一个包含36条候选轨迹的轨迹库。在线推理时，结合当前自车状态和历史轨迹，从轨迹库中检索出最相似的k条轨迹，再与一个仅基于自车信息的MLP预测的1条轨迹合并，构成k+1条候选轨迹。</p>
<p><img src="https://arxiv.org/html/2505.16805v1/x4.png" alt="轨迹令牌组合"></p>
<blockquote>
<p><strong>图4</strong>：用于大语言模型规划的轨迹令牌与图像令牌、文本令牌的组合示意图。候选轨迹的终点或参考轨迹的路径点通过轨迹适配器被编码为离散令牌，插入到文本提示中供VLM处理。</p>
</blockquote>
<p>T-CoT包含两个阶段：1) <strong>轨迹选择</strong>：VLM的任务是从候选轨迹中选择最合适的一条。为了降低计算开销，每条候选轨迹的终点被一个轨迹适配器编码为单个令牌，并插入文本提示中。2) <strong>轨迹细化</strong>：以上一步选出的轨迹为参考，VLM进一步生成精确的细化轨迹。参考轨迹的每个路径点被编码为路径点令牌并注入提示词。这种设计将推理链集中在轨迹层面，所需输出简短，计算成本远低于从感知到规划的冗长推理链。</p>
<p><strong>时间解耦协同</strong>：为了兼顾VLM的高精度和E2E模型的实时性，本文提出了一种轨迹层面的时间解耦协同策略。VLM被用于预测长时程的轨迹（长于其自身延迟和E2E模型所需的预测时域），并将其预测结果存入记忆库。E2E模型（一个简单的规划头，其规划查询由训练集轨迹聚类初始化）可以从记忆库中异步获取最新的VLM预测轨迹（相对于当前时刻有延迟）作为额外的轨迹初始化先验。这样，尽管VLM存在延迟，E2E模型仍能利用其高质量的长时预测来提升自身实时轨迹的准确性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在nuScenes数据集上进行训练和评估，并使用其扩展版本OmniDrive-nuScenes中的问答对进行VLM训练。评估指标为位移误差（L2距离）和碰撞率。VLM基模型采用LLaVA v1.5-7B，并将其视觉编码器替换为本文的SQ-Former。</p>
<p><strong>对比实验</strong>：如表1所示（数据来自论文正文描述），SOLVE在端到端模型和VLM模型两条赛道上均取得了先进结果。</p>
<ul>
<li><strong>SOLVE-E2E</strong>：在平均L2误差上，比UniAD、VAD-Base和BEV-Planner分别低0.15米、0.06米和0.04米；碰撞率也分别低0.07%、0.03%和0.07%。这表明SOLVE能更好地模仿人类驾驶行为并具备更准确的空间判断能力。</li>
<li><strong>SOLVE-VLM</strong>：使用相似规模的LLM（7B参数），其平均L2误差比OmniDrive和DriveVLM分别低0.05米和0.03米，证明了SQ-Former在压缩视觉特征以及共享训练方案的有效性。</li>
<li>**SOLVE-E2E (Async)**：通过异步利用VLM历史预测作为初始化，其性能比SOLVE-E2E进一步提升，平均L2误差降低了0.01米。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.16805v1/x5.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：SOLVE的定性结果展示。红色、蓝色和黄色线条分别代表VLM、E2E-Async和E2E模块的规划结果。右侧BEV图中显示了自车（绿色框）和真实轨迹（绿色线）。该图直观展示了不同模块的轨迹输出及其与真实情况的对比。</p>
</blockquote>
<p><strong>消融与贡献分析</strong>：实验证明了各组件的作用。1) <strong>共享SQ-Former与联合训练</strong>：使得视觉编码器能通过VLM的全面QA任务（感知、预测、规划）进行优化，专注于编码对规划重要的特征，这是其他仅通过检测任务训练的E2E模型所缺乏的。2) <strong>T-CoT机制</strong>：通过轨迹库和两阶段推理，有效提升了VLM轨迹预测的精度和效率。3) <strong>时间解耦协同</strong>：异步利用VLM预测作为先验，进一步提升了E2E模型的规划性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了SOLVE协同框架，首次在特征级（通过共享SQ-Former）和轨迹级同时实现了VLM与E2E模型的高效、深度融合。</li>
<li>设计了轨迹链式思维（T-CoT）范式，使VLM能够通过轨迹库进行从粗到精的渐进式推理，显著提升了轨迹预测的准确性和效率。</li>
<li>引入了时间解耦协同策略，使计算密集的VLM可以异步提供高质量的长时轨迹先验，赋能轻量级E2E模型，在保持实时性的同时提升性能。</li>
</ol>
<p><strong>局限性</strong>：论文未明确阐述自身局限性，但根据方法描述，潜在局限可能包括：VLM部分依然依赖大规模预训练模型，计算成本较高；T-CoT的轨迹库依赖于对训练集轨迹的聚类，其覆盖度和泛化能力可能受数据集限制；整体框架的训练策略较为复杂，涉及多阶段优化。</p>
<p><strong>研究启示</strong>：SOLVE为融合大模型与传统自动驾驶算法提供了新范式。其核心启示在于：1) <strong>特征共享是深度协同的关键</strong>，而非简单的后处理融合；2) <strong>将大模型的强大能力“蒸馏”或“异步赋能”给轻量级实时系统</strong>，是解决其效率瓶颈的有效路径；3) <strong>针对特定任务（如轨迹预测）设计定制化的推理链</strong>，可以大幅提升大模型在专业领域的实用性和效率。这为后续研究如何更精巧地设计协同接口、优化知识迁移效率指明了方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SOLVE框架，解决现有视觉语言模型（VLM）与端到端自动驾驶模型集成效率低、实时性差的问题。关键技术包括：通过共享视觉编码器实现特征级协同；提出轨迹思维链（T-CoT）范式逐步优化轨迹预测；采用时间解耦策略协调VLM的高质量输出与端到端网络的实时性能。在nuScenes数据集上的实验表明，该方法显著提升了轨迹预测的准确性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.16805" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>