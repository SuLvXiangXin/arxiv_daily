<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Robust Behavior Cloning Via Global Lipschitz Regularization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Robust Behavior Cloning Via Global Lipschitz Regularization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.19250" target="_blank" rel="noreferrer">2506.19250</a></span>
        <span>作者: Sean B. Andersson Team</span>
        <span>日期: 2025-06-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>行为克隆（BC）作为一种模仿学习技术，通过专家演示的状态-动作对数据集来训练策略，无需与环境交互，已被应用于自动驾驶等安全关键领域。然而，在部署时，策略的观测可能包含测量误差或对抗性扰动，导致其偏离真实状态并做出次优动作。现有提升策略鲁棒性的方法主要分为两类：针对模型不确定性的鲁棒马尔可夫决策过程（RMDP）框架，以及针对状态观测不确定性的状态对抗马尔可夫决策过程（SA-MDP）框架。在BC的背景下，针对观测噪声的鲁棒性研究相对较少。现有方法（如对抗正则化）旨在平滑策略，使邻近状态产生相似动作，但无法提供<strong>鲁棒性证书</strong>（即无法保证在任意有界扰动下奖励下降的上界）。虽然近期工作建立了策略鲁棒性与其Lipschitz连续性之间的联系，但策略的Lipschitz连续性与神经网络Lipschitz性之间的具体关系尚未明确讨论。本文针对BC策略在面临有界状态观测扰动时缺乏可证明鲁棒性保证的痛点，提出通过<strong>全局Lipschitz正则化</strong>来增强策略网络的鲁棒性，并证明由此获得的全局Lipschitz性质能为策略提供针对不同有界范数扰动的鲁棒性证书。核心思路是：通过理论建立策略Lipschitz常数与鲁棒性证书的定量关系，并设计一种能确保特定Lipschitz常数的神经网络（LipsNet）来实际构建鲁棒的BC策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的核心是在标准BC目标上施加一个全局Lipschitz约束，并通过构建特定的Lipschitz神经网络（LipsNet）来实现该约束，从而获得具有可证明鲁棒性证书的策略。</p>
<p><strong>整体框架与理论联系</strong>：首先，在SA-MDP框架下，本文扩展了先前工作，将其适用于确定性策略（BC中常用形式）。关键定理（Theorem 3.1）建立了策略的局部Lipschitz常数 (L_{\pi}^{\epsilon}(s)) 与其鲁棒性证书 (\Theta(\pi))（定义为无扰动与最坏扰动下价值函数最大差值）的上界关系：(\Theta(\pi) \leq \alpha L_{\pi}^{\epsilon}(s) \epsilon)，其中(\epsilon)为扰动界，(\alpha)为与环境Lipschitz常数相关的系数。由于计算所有状态下的局部Lipschitz常数难以处理，转而使用更保守但易于控制的<strong>全局Lipschitz常数</strong> (L_{\pi})。因此，BC的训练目标转化为一个带约束的优化问题：在最小化模仿误差的同时，约束策略的Lipschitz常数 (|\pi_I|<em>L \leq L</em>{\pi})。</p>
<p><strong>核心模块：Lipschitz神经网络的构建（LipsNet）</strong> 关键创新在于证明了通过控制神经网络权重的(\infty)-诱导范数，可以控制相应策略的Lipschitz常数，进而获得鲁棒性证书。</p>
<ul>
<li><p><strong>理论桥梁</strong>：</p>
<ol>
<li><strong>确定性策略</strong>（Theorem 3.2）：对于一个M层全连接网络(f)（使用ReLU等1-Lipschitz激活函数），其输出变化的2-范数上界由 (m L_{\pi}^{FC} |x-y|<em>p) 控制，其中 (L</em>{\pi}^{FC} = \prod_{i=0}^{M} |W_i|<em>{\infty})。当(f)作为确定性策略(\pi)时，其Lipschitz常数(L</em>{\pi})可由此推断。</li>
<li><strong>分类（离散）随机策略</strong>（Theorem 3.3）：对于输出接softmax层的网络，其输出概率分布的总变差（TV）距离上界由 (\frac{m^2}{2} L_{\pi}^{FC} |x-y|<em>p) 控制，同样可将(L</em>{\pi}^{FC})与策略的Lipschitz性关联。</li>
</ol>
</li>
<li><p><strong>实现技术</strong>：为了实际构造一个具有指定(L_{\pi}^{FC})的Lipschitz网络，本文采用基于权重归一化（Weight Normalization, WN）的方法。如图1所示，为每个全连接层引入可训练的Lipschitz界参数(c_i)。每层的权重按下式归一化：<br>  ( \hat{W}<em>i = W_i \frac{\text{softplus}(c_i)}{|W_i|</em>{\infty}} )<br>  其中softplus函数防止分母为零。网络的整体Lipschitz上界为 (\prod_{i=1}^{M} \text{softplus}(c_i))。与追求最小化Lipschitz常数的原有方法不同，本文希望将网络Lipschitz常数设定在目标值(L_{\pi}^{FC})附近以平衡表达能力和鲁棒性。因此，设计了辅助损失函数：<br>  ( \mathcal{L}(\theta) = \lambda \max\left( \prod_{i=1}^{M} \text{softplus}(c_i) - L_{\pi}^{FC}, 0 \right) )<br>  该损失仅在网络实际Lipschitz上界超过目标值(L_{\pi}^{FC})时产生惩罚，从而将其“拉回”至目标值或以下，而不过度限制网络容量。</p>
</li>
</ul>
<p><img src="https://arxiv.org/html/2506.19250v2/figures/network_configs.png" alt="LipsNet网络配置"></p>
<blockquote>
<p><strong>图1</strong>：LipsNet网络配置。每个MLP层附加了用于权重归一化的可训练参数 (c_i)，通过控制归一化后的权重 (\hat{W}_i) 的无穷范数来调控该层及整个网络的Lipschitz上界。</p>
</blockquote>
<p><strong>创新点</strong>：1) 将SA-MDP鲁棒性分析扩展到确定性策略，并首次建立了策略全局Lipschitz常数与可计算鲁棒性证书之间的直接理论联系。2) 提出了通过控制神经网络权重无穷范数来精确约束策略Lipschitz常数的具体方法（LipsNet），并设计了相应的正则化损失函数，使BC能训练出具有可证明鲁棒性保证的策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在Gymnasium的连续控制环境（Hopper-v4, Walker2d-v4, Humanoid-v4）和离散环境（CartPole-v1）中进行实验。通过PPO（CartPole）和TD3（MuJoCo环境）训练专家策略，并采集100条轨迹构建BC数据集。</li>
<li><strong>对比方法</strong>：与<strong>Vanilla BC</strong>（无正则化）和<strong>SR2L</strong>（一种基于对抗正则化的局部平滑方法）进行对比。本文方法记为<strong>LipsNet-X</strong>，其中X是指定的目标Lipschitz常数(L_{\pi})。</li>
<li><strong>测试扰动</strong>：考虑两种有界噪声：1) <strong>策略依赖噪声</strong>：对抗性噪声（由SR2L的对抗模型生成，旨在最大化策略输出差异）。2) <strong>策略无关噪声</strong>：随机噪声（从有界均匀分布中采样）和Robust Sarsa（RS）噪声（一种基于在线评论家的最坏动作选择方法）。在多个噪声水平(\delta)下进行评估。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>表1展示了在特定噪声水平（(\epsilon)）下，面对不同类型噪声时的平均奖励。结果表明，在大多数情况下，LipsNet方法在<strong>对抗性噪声</strong>和<strong>最坏情况</strong>（即所有噪声类型中的最低奖励）下的性能显著优于Vanilla BC和SR2L。例如，在Hopper环境中对抗噪声下，LipsNet-50获得1094±510的奖励，高于Vanilla BC的889±496和SR2L的873±462。在CartPole（随机策略）中，LipsNet-10在最坏情况下奖励为207±14，也优于对比方法。这说明LipsNet能有效提升策略在 worst-case 扰动下的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2506.19250v2/figures/main_figs/hopper_v4_noise_vs_reward.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2(a)</strong>: Hopper-v4环境中，策略依赖（对抗性）噪声下，不同噪声水平的最坏情况平均奖励。LipsNet-50（橙色）在较高噪声水平下相比Vanilla BC（蓝色）和SR2L（绿色）保持了更高的奖励，展示了更好的鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.19250v2/figures/main_figs/walker_v4_noise_vs_reward.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2(b)</strong>: Walker2d-v4环境下的类似趋势，LipsNet-30（橙色）在对抗噪声下的性能下降更平缓。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.19250v2/figures/main_figs/humanoid_v4_noise_vs_reward.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2(c)</strong>: Humanoid-v4环境中，LipsNet-5（橙色）在对抗噪声下显著优于基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.19250v2/figures/main_figs/cart_v1_noise_vs_reward.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2(d)</strong>: CartPole-v1（随机策略）中，LipsNet-10（橙色）在面对对抗噪声时，其奖励下降幅度远小于Vanilla BC和SR2L。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.19250v2/figures/rand/hopper_v4_noise_vs_reward_rand.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3(a)</strong>: Hopper-v4环境中，策略无关（随机）噪声下的性能。LipsNet在不同噪声水平下与Vanilla BC性能接近，说明其鲁棒性提升并未显著牺牲在良性扰动下的性能。</p>
</blockquote>
<p><strong>消融实验与组件贡献</strong>：论文通过选择不同的目标Lipschitz常数 (L_{\pi}) 进行了实验（如表1中LipsNet-50, -30, -5, -10）。结果表明，(L_{\pi}) 的选择需要权衡：较小的 (L_{\pi})（更严格的约束）可能带来更强的鲁棒性保证（更小的证书上界），但可能增加模仿误差（插值误差），从而影响无噪声性能；较大的 (L_{\pi}) 则相反。实验显示，通过适当选择 (L_{\pi})，可以在不显著损害原始性能的前提下，大幅提升对抗性扰动下的鲁棒性。辅助损失函数中的正则化强度 (\lambda) 主要影响收敛速度，而非最终的鲁棒性水平。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>理论贡献</strong>：扩展了SA-MDP框架以涵盖确定性策略，并首次建立了策略的（全局）Lipschitz常数与可量化鲁棒性证书之间的直接理论联系，为BC策略的鲁棒性提供了可证明的保证。</li>
<li><strong>方法贡献</strong>：提出了一种实用的方法（LipsNet），通过权重归一化和辅助损失函数来构造具有指定Lipschitz常数的神经网络，从而将理论保证应用于BC训练，实现了对策略鲁棒性的显式控制。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>使用全局Lipschitz常数作为约束是保守的，一个具有较大Lipschitz常数的策略在实践中未必不鲁棒。</li>
<li>施加Lipschitz约束可能引入插值误差，导致策略在模仿专家行为时性能下降，存在鲁棒性与最优性之间的权衡。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>为安全关键应用中的模仿学习提供了一条获得<strong>可认证鲁棒性</strong>的途径。未来的工作可以探索更紧的Lipschitz上界估计方法，以减少保守性。</li>
<li>所建立的策略Lipschitz性与鲁棒性证书的理论框架，可以激励在更广泛的RL/IL设置中设计具有可证明保证的鲁棒性算法。</li>
<li>如何自适应地或根据环境特性选择最优的Lipschitz常数 (L_{\pi})，是一个值得进一步研究的问题。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对行为克隆策略在部署时易受观测误差或对抗干扰影响的问题，提出通过全局Lipschitz正则化增强策略网络的鲁棒性。该方法构建具有全局Lipschitz性质的神经网络，确保策略对于有界范数扰动具备鲁棒性证书，即保证在扰动下动作输出的变化有界。论文在Gymnasium的多类环境中进行了实证验证，但提供的正文节选未包含具体的性能提升数据。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.19250" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>