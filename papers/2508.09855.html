<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.09855" target="_blank" rel="noreferrer">2508.09855</a></span>
        <span>作者: Changjae Oh Team</span>
        <span>日期: 2025-08-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，训练人机协作系统（尤其是近距离协作任务如人向机器人手递手）通常依赖大规模的人机交互数据集。从原始图像数据中学习机器人操作策略需要在物理环境中进行大量机器人动作试验，成本高、耗时长且存在安全风险。仿真训练是一种经济高效的替代方案，但仿真环境与真实机器人工作空间之间的视觉域差距（sim-to-real gap）仍然是一个主要限制。本文针对这一系列痛点，提出了一种无需真实机器人运动或真实数据收集、仅从RGB图像训练人向机器人手递手策略的方法。其核心思路是利用稀疏视角的高斯泼溅技术重建手递手交互的3D场景，并在该重建场景中通过改变虚拟相机位姿来生成机器人演示数据，从而训练视觉运动策略，使机器人能够稳定抓取并避免与人类手部碰撞。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的核心是通过3D场景重建在仿真中生成机器人演示数据，进而训练一个端到端的策略网络。整体流程如图1所示。</p>
<p><img src="https://arxiv.org/html/2508.09855v1/a2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。(a) 给定稀疏视角的RGB-D手递手图像，使用高斯泼溅进行3D场景重建，并从重建场景中提取物体和手部点云以估计抓取位姿。(b) 利用GS场景和抓取位姿，生成夹爪朝向预抓取位姿的轨迹，以及在每个采样位姿下的手眼图像。(c) 每条轨迹构成一个包含手眼图像、物体和手部掩码、夹爪位姿变换和预抓取位姿标签的手递手演示数据集。(d) 使用该数据集训练手递手策略。推理时仅需要手眼RGB图像和掩码。</p>
</blockquote>
<p>该方法包含三个核心模块：</p>
<ol>
<li><p><strong>抓取位姿估计</strong>：首先，利用3D高斯泼溅从稀疏视角的RGB-D输入重建人-物交互场景，获得具有真实感和公制尺度的环境。接着，从重建场景中分割出物体和人类手部的点云。使用GraspNet基于这些点云估计多个可能的抓取候选位姿。为确保安全，通过检查抓取坐标系下与手部点云的接近程度，过滤掉可能发生碰撞的不安全抓取。最后，通过调整坐标偏移将抓取位姿与重建场景对齐，从而实现从相机运动到夹爪运动的直接映射，确保感知与控制的一致性。</p>
</li>
<li><p><strong>手递手演示数据集生成</strong>：为每个安全的抓取位姿模拟生成多条机器人接近轨迹，以支持灵活的手递手策略。初始机器人位姿在抓取点周围球形采样，并经过过滤以确保轨迹符合人-物几何结构和安全区域的现实约束。机器人接近物体的任务分为三个阶段：第一阶段，移动机械臂使夹爪的接近方向对准物体，将物体定位在图像中心；第二阶段，仅通过线性插值调整位置，直到进入预定义的预抓取位姿距离范围内；第三阶段，同时精调位置和方向（位置线性插值，旋转球面线性插值），最终到达预抓取位姿。利用每个时间步的6自由度手眼位姿，将手和物体投影到相机视图中，渲染出RGB图像和对应的掩码图像，从而构建用于视觉运动策略学习的演示数据集。</p>
</li>
<li><p><strong>策略学习</strong>：策略网络从模拟生成的手眼RGB图像以及手部、物体的二值掩码中学习控制机器人的6自由度运动并执行抓取。在每个时间步，策略预测一个位姿更新量和一个抓取概率，从而实现自适应的交接时机判断。训练采用结合了平移、旋转和抓取分类的多目标损失函数，使系统能够泛化到不同的物体类型和手部姿态。</p>
</li>
</ol>
<p>与现有方法相比，本文的核心创新在于完全避免了真实机器人运动数据的收集和复杂的sim-to-real适配过程，通过高质量的3D场景重建，在仿真中直接生成了与真实感知高度一致的、可用于策略训练的机器人演示数据。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真3D场景和真实世界机器人手递手任务两个层面进行评估，旨在验证策略在仿真中的有效性及其向真实世界的迁移能力。</p>
<p><img src="https://arxiv.org/html/2508.09855v1/render-seen.png" alt="仿真轨迹（已见过物体）"></p>
<blockquote>
<p>**图2 (a)**：在仿真中对已见过物体的手递手轨迹。机器人能够持续调整位姿，安全有效地接近预抓取位姿。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.09855v1/render-unseen.png" alt="仿真轨迹（未见物体）"></p>
<blockquote>
<p>**图2 (b)**：在仿真中对未见物体的手递手轨迹。策略展示了良好的泛化能力，能对新的物体执行类似的安全接近行为。</p>
</blockquote>
<p>在仿真实验中，机器人根据RGB输入预测动作直至到达预抓取状态或超时。如图2所示，无论是对于训练中见过的还是未见的物体，机器人均能持续地调整方向、接近并精调其位姿，以实现安全有效的手递手。这反映了模型自主执行符合人类意识的接近策略的能力，这对于无缝的人机协作至关重要。机器人能够保持适当距离、与物体中心对齐并避开人类手部，展示了其在自主控制下规划目标导向动作的能力。</p>
<p><img src="https://arxiv.org/html/2508.09855v1/fig5.png" alt="真实世界手递手示例"></p>
<blockquote>
<p><strong>图3</strong>：使用所提方法（输入为RGB图像和手-物掩码）成功完成对日常家用物品的人向机器人手递手的示例。</p>
</blockquote>
<p>在真实世界实验中，研究涉及了6种不同的物体和4位参与者。如图3所示，该方法能够对常见的家用物品执行人向机器人手递手。结果表明，该模型对于已见过和未见过物体都表现出了鲁棒且有效的手递手性能。这标志着向半自主机器人队友的转变：系统能够解读高级视觉线索、执行安全的抓取计划，并能适应新物体，且无需手工设计的机器人控制。这种在不同控制模式间流畅切换并自然协作的能力，支持了在人类引导下实现可信人机团队的更广泛目标。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：第一，提出了一种基于3D高斯泼溅重建的、无需真实机器人运动即可训练人向机器人手递手策略的新框架，绕过了大规模真实数据收集和sim-to-real适配的瓶颈。第二，构建了一套从稀疏视角RGB-D数据重建3D场景，并基于此生成包含图像-动作对的机器人演示数据的方法，实现了感知到控制的直接映射。第三，通过在仿真和真实环境中的实验，验证了该策略在引导机器人安全、有效接近物体方面的能力，并展示了其向真实世界手递手任务的迁移性。</p>
<p>论文自身提到的局限性包括：未来工作希望集成更丰富的人类意图理解，并利用基础模型来扩展交互的泛化能力。这启示后续研究可以探索如何将高层的人类状态（如意图、注意力）预测与低层的机器人运动控制更紧密地结合，以及利用大规模预训练模型来提升对多样化物体和交互场景的适应性，从而推动人机团队协作向更自然、更智能的方向发展。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人机协作中物体交接任务，提出一种无需真实机器人训练的数据采集方法。核心问题是解决传统方法依赖大量真实交互数据、成本高且存在仿真与现实视觉差异的局限。关键技术采用稀疏视角高斯泼溅重建交接场景三维表示，通过改变虚拟相机位姿生成机器人视点图像与动作序列，作为监督策略学习的演示数据。实验表明，该方法在重建场景和真实交接任务中均能形成有效表征，实现稳定抓取并避免人机碰撞，为人机协作提供了更无缝、鲁棒的解决方案。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.09855" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>