<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OmniSAT: Compact Action Token, Faster Auto Regression - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>OmniSAT: Compact Action Token, Faster Auto Regression</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.09667" target="_blank" rel="noreferrer">2510.09667</a></span>
        <span>作者: Lyu, Huaihai, Chen, Chaofan, Xie, Senwei, Wang, Pengwei, Chen, Xiansheng, Zhang, Shanghang, Xu, Changsheng</span>
        <span>日期: 2025/10/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型主要分为基于扩散和基于自回归（AR）的两类方法。扩散模型通过迭代去噪捕捉连续动作分布，精度高但计算成本巨大，限制了可扩展性。自回归模型将动作离散化为令牌，通过下一个令牌预测进行训练，优化效率高且支持灵活的序列构建，更适合大规模预训练。然而，当使用动作块（多步动作段）进行训练以增强时序理解时，长时程会导致令牌序列过长，拖慢AR优化。现有方法尝试通过熵引导离散化或基于令牌共现频率的字节对编码（BPE）来压缩序列，但前者在高压缩比下重建误差严重，后者则受限于训练与目标轨迹间的领域差距，导致泛化能力弱。</p>
<p>本文针对现有动作令牌化方法在压缩效率与重建质量之间难以平衡的痛点，提出了一种新的视角：学习一个紧凑、可迁移的动作表示，以加速AR训练。核心思路是设计一个两阶段的动作令牌化器（OmniSAT），先通过一致性编码将变长轨迹对齐为固定长度表示，再对动作的物理子空间（位置、旋转、夹持器）进行分组残差量化，从而在保持高保真重建的同时实现高效压缩，并构建统一的动作模式空间以支持跨具身学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>OmniSAT的整体目标是将连续的高维动作轨迹编码为紧凑的离散令牌序列，以缩短AR训练序列。其流程分为两个核心阶段：一致性编码和量化压缩。</p>
<p><img src="https://arxiv.org/html/2510.09667v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：OmniSAT令牌化流程概览。<strong>一致性编码</strong>将变长轨迹通过B样条拟合转换为时间对齐的、固定长度的控制点表示。<strong>量化压缩</strong>将控制点特征按部位分组（位置、旋转、夹持器），并应用残差向量量化以获得分层码本索引。所选索引被展平为最终的紧凑动作模式令牌。</p>
</blockquote>
<p><strong>1. 一致性编码</strong>：此阶段旨在处理不同数据集（具身）中动作的数值范围和时程差异，获得统一的表示。首先，对每个自由度（DoF）进行鲁棒归一化，将其第1和第99百分位数映射到[-1, 1]，得到数值归一化轨迹。接着，使用B样条基函数矩阵，通过岭回归将变长轨迹拟合为固定长度（Tc）的控制点表示。这一步骤将原始的变长时间序列（如图2中步骤①的虚线曲线）映射为固定维度的特征向量（步骤②的实线曲线），为后续量化提供了时序对齐且长度一致的输入。</p>
<p><strong>2. 量化压缩</strong>：基于固定长度的控制点表示，进行离散化以提取跨异构具身的共同动作模式。采用<strong>分组残差向量量化</strong>技术。首先，根据动作语义将编码后的特征<strong>z</strong>划分为三个独立组：位置、旋转和夹持器（图2步骤③）。每组使用独立的残差量化过程。<br>对于单组内的一个DoF特征向量<strong>s</strong>，残差量化以从粗到细的方式工作：初始化残差 r0 = <strong>s</strong>；在第 l 层，从该层码本 Cl 中寻找与当前残差 rl-1 最接近的码字 Cl_ql，记录索引 ql，并更新残差 rl = rl-1 - Cl_ql（图2步骤④）。经过 L 层后，重建特征为各层所选码字之和，离散令牌序列为索引列表 [q1, q2, ..., qL]。对所有DoF重复此过程，并将各组的令牌序列拼接，形成最终的紧凑动作令牌。</p>
<p><strong>训练目标</strong>：为了稳定码本学习并保证重建质量，OmniSAT使用了一个包含三个部分的损失函数：</p>
<ol>
<li><strong>重建损失</strong>：确保量化后的令牌在特征层面和原始轨迹层面都能高保真重建，包含特征重建误差和通过B样条解码回原始动作空间的轨迹重建误差。</li>
<li><strong>承诺损失</strong>：约束编码器输出靠近被选中的码字，同时推动码字向量向编码器输出靠近，防止码本崩溃。</li>
<li><strong>量化层丢弃损失</strong>：在训练时，以一定概率随机跳过某些量化层，迫使网络学习更鲁棒、表达能力更强的分层表示，增强泛化能力。<br>总损失是这三项的加权和。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，OmniSAT的创新具体体现在：1) <strong>两阶段统一框架</strong>：将数值/时程归一化（一致性编码）与结构化离散化（分组残差量化）解耦，兼顾了表示的通用性和压缩的高保真度。2) <strong>分组残差量化</strong>：依据动作物理语义分组，并采用分层残差量化，相比简单的维度分桶或全局VQ-VAE，能更精细地捕捉不同子空间模式，实现更高效的压缩。3) <strong>构建统一动作模式空间</strong>：通过在大规模数据上预训练，学习到的码本构成了一个可迁移的“动作词汇表”，为后续的跨具身学习奠定了基础。</p>
<p><img src="https://arxiv.org/html/2510.09667v1/x3.png" alt="跨具身学习策略"></p>
<blockquote>
<p><strong>图3</strong>：基于OmniSAT的跨具身操作学习。流程分为两阶段：(i) <strong>令牌化器预训练</strong>：在异构人-机器人数据上预训练OmniSAT，学习统一且压缩（×6.8）的动作令牌空间；(ii) <strong>跨具身微调</strong>：在OmniSAT令牌空间上构建混合的视觉-动作自回归序列，通过更短的序列和更低的目标熵实现高效、可扩展的微调。</p>
</blockquote>
<p><strong>跨具身操作学习</strong>：利用OmniSAT产生的统一动作令牌空间，可以融合机器人演示和人类自我中心视频（如EgoDex）进行训练。如图3所示，分别使用视觉令牌化器和OmniSAT将观测和动作转换为令牌，然后交错拼接成视觉-动作令牌流。通过一个混合权重的AR目标函数，在多个具身数据上联合训练单一策略，从而利用异构数据增强模型的泛化能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：使用大规模机器人数据集DROID（76k轨迹）进行令牌化器预训练评估。下游任务评估包括：真实机器人基准（PlaceObj, ZipSeal, TubeRack）和模拟基准（LIBERO的四个任务套件、SimplerEnv-WidowX）。</li>
<li><strong>对比基线</strong>：包括扩散或AR策略模型（Octo, OpenVLA），以及专门的行动令牌化/压缩方法（FAST, BEAST, SpatialVLA）。</li>
<li><strong>评估指标</strong>：压缩质量（平均绝对误差MAE、压缩比R）、任务成功率（SR）、训练收敛速度。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>压缩质量对比（RQ1）</strong>：在DROID数据集上，OmniSAT在重建误差（MAE）和压缩比上均优于基线。</li>
</ol>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">MAE (↓)</th>
<th align="left"><strong>R</strong> (↑)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">FAST</td>
<td align="left">&lt; 1e-5</td>
<td align="left">3.7</td>
</tr>
<tr>
<td align="left">BEAST</td>
<td align="left">8.0e-2</td>
<td align="left">4.6</td>
</tr>
<tr>
<td align="left">OmniSAT-10</td>
<td align="left">8.5e-4</td>
<td align="left">4.9</td>
</tr>
<tr>
<td align="left"><strong>OmniSAT-8</strong></td>
<td align="left"><strong>9.4e-4</strong></td>
<td align="left"><strong>6.8</strong></td>
</tr>
<tr>
<td align="left">OmniSAT-6</td>
<td align="left">1.3e-3</td>
<td align="left">8.1</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：在DROID数据集上与现有动作令牌化器的压缩质量对比。OmniSAT-8在保持较低重建误差（9.4e-4）的同时，实现了最高的压缩比（6.8倍）。</p>
</blockquote>
<ol start="2">
<li><strong>训练效率提升</strong>：更高压缩比直接带来了更快的训练收敛。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09667v1/x4.png" alt="LIBERO训练收敛曲线"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO基准上的平均成功率训练收敛曲线。使用OmniSAT的方法（红线）在相同训练步数下成功率更高，且更早（约2.5k步）达到性能平台，快于FAST（3.5k步）和BEAST（4k步），体现了更高效的优化。</p>
</blockquote>
<ol start="3">
<li><strong>下游任务性能（RQ2）</strong>：在多个基准测试中，集成OmniSAT的AR模型取得了最佳或极具竞争力的性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.09667v1/x8.png" alt="LIBERO各子任务性能"></p>
<blockquote>
<p><strong>图8</strong>：在LIBERO四个任务套件（Spatial, Object, Goal, Long）上的详细成功率。OmniSAT在Object和Goal套件上表现最佳，在Long-horizon套件上与BEAST相当，整体平均成功率领先。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Spatial SR</th>
<th align="left">Object SR</th>
<th align="left">Goal SR</th>
<th align="left">Long SR</th>
<th align="left"><strong>Average SR</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">Octo</td>
<td align="left">78.9%</td>
<td align="left">85.7%</td>
<td align="left">84.6%</td>
<td align="left">51.1%</td>
<td align="left">75.1%</td>
</tr>
<tr>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
</tr>
<tr>
<td align="left">FAST</td>
<td align="left">96.4%</td>
<td align="left">96.8%</td>
<td align="left">88.6%</td>
<td align="left">60.2%</td>
<td align="left">85.5%</td>
</tr>
<tr>
<td align="left">BEAST</td>
<td align="left">92.9%</td>
<td align="left">97.5%</td>
<td align="left">93.1%</td>
<td align="left"><strong>86.4%</strong></td>
<td align="left">92.5%</td>
</tr>
<tr>
<td align="left"><strong>OmniSAT</strong></td>
<td align="left"><strong>94.1%</strong></td>
<td align="left"><strong>98.7%</strong></td>
<td align="left"><strong>94.6%</strong></td>
<td align="left">86.0%</td>
<td align="left"><strong>93.4%</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表2</strong>：在LIBERO基准上的实验结果汇总。OmniSAT取得了最高的平均成功率（93.4%），尤其在Object和Goal任务上领先。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Put Spoon (Success)</th>
<th align="left">Put Carrot (Success)</th>
<th align="left">Stack Block (Success)</th>
<th align="left">Put Eggplant (Success)</th>
<th align="left"><strong>Overall Success</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
</tr>
<tr>
<td align="left">BEAST</td>
<td align="left">41.7%</td>
<td align="left">25.0%</td>
<td align="left">20.8%</td>
<td align="left">75.0%</td>
<td align="left">37.5%</td>
</tr>
<tr>
<td align="left">SpatialVLA</td>
<td align="left">16.7%</td>
<td align="left">25.0%</td>
<td align="left">29.2%</td>
<td align="left"><strong>100%</strong></td>
<td align="left">42.7%</td>
</tr>
<tr>
<td align="left"><strong>OmniSAT</strong></td>
<td align="left"><strong>58.3%</strong></td>
<td align="left"><strong>37.5%</strong></td>
<td align="left"><strong>29.2%</strong></td>
<td align="left">95.8%</td>
<td align="left"><strong>55.2%</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表3</strong>：在SimplerEnv-WidowX基准上的评估结果。OmniSAT在四个任务中的三个上取得了最高的最终成功率，整体成功率（55.2%）显著优于其他基线。</p>
</blockquote>
<p><strong>消融实验（RQ3）</strong>：论文通过消融实验验证了核心组件的有效性。</p>
<ul>
<li><strong>损失函数</strong>：移除承诺损失或量化层丢弃损失均会导致重建误差上升和下游任务性能下降，证明了它们对稳定训练和增强表达力的必要性。</li>
<li><strong>分组量化</strong>：与不分组的全局量化相比，按位置、旋转、夹持器分组量化能获得更低的重建误差和更高的下游任务成功率，说明语义分组能更好地捕捉动作结构。</li>
<li><strong>码本共享 vs. 独立</strong>：在跨具身学习中，让不同具身智能体共享码本，相比使用独立码本，能带来显著的下游性能提升，验证了统一动作模式空间对于知识迁移的价值。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>OmniSAT</strong>，一个统一的两阶段动作令牌化器，通过一致性编码和分组残差量化，实现了高保真、高压缩比的动作表示，为可扩展的AR预训练提供了通用的动作令牌空间。</li>
<li>探索了<strong>跨具身操作学习</strong>策略，利用OmniSAT构建的统一令牌空间，融合机器人示教与人类视频数据，增强了动作模式的泛化能力和数据利用效率。</li>
<li>在真实机器人和多种模拟基准上进行了全面实验，验证了OmniSAT在<strong>压缩效率</strong>（更高压缩比、更低重建误差）和<strong>下游VLA性能</strong>（更快收敛、更高成功率）上的持续优势。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，OmniSAT的有效性依赖于大规模、高质量的演示数据集（如DROID）进行码本预训练。此外，虽然AR训练本身更高效，但令牌化器的预训练和跨模态模型的微调仍需可观的计算资源。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>结构化动作表示</strong>：将动作空间依据物理语义分解为子空间并进行独立处理，是提升表示效率和泛化能力的一个有效方向。</li>
<li><strong>跨模态与跨具身统一表示</strong>：学习一个能够桥接不同模态（视觉、语言、动作）和不同具身形态的中间表示，是迈向通用具身智能的关键。OmniSAT在动作侧的实践为此提供了参考。</li>
<li><strong>效率与性能的权衡</strong>：面向大规模训练，设计轻量、高效且性能不妥协的组件（如令牌化器）至关重要。OmniSAT展示了通过改进数据表示本身来提升整个训练流程效率的潜力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自回归（AR）视觉-语言-动作模型在处理高维长序列动作时训练效率低下的问题，提出OmniSAT方法。该方法通过B-Spline编码归一化动作值域与时间范围，并对位置、旋转等子空间进行多阶段残差量化，生成紧凑的离散动作令牌。在Droid数据集上预训练后，该令牌化将训练序列长度缩短6.8倍，有效降低目标熵，并支持跨体现学习。实验表明，OmniSAT在保持重建质量的同时实现了更高压缩，加速了AR训练收敛并提升了模型性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.09667" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>