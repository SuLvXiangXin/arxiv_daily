<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SCRAMBLe : Enhancing Multimodal LLM Compositionality with Synthetic Preference Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>SCRAMBLe : Enhancing Multimodal LLM Compositionality with Synthetic Preference Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.04740" target="_blank" rel="noreferrer">2504.04740</a></span>
        <span>作者: Mishra, Samarth, Saenko, Kate, Saligrama, Venkatesh</span>
        <span>日期: 2025/04/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，多模态大语言模型（MLLMs）在组合性（compositionality）——即将场景正确理解为原子视觉概念的组合——方面仍然存在困难。即使是GPT-4o等最先进的模型，也可能在区分“狗追猫”与“猫追狗”这类组合上犯错。Winoground等专门评估此类推理的基准测试表明，MLLMs虽已取得显著进展，但性能仍远不及人类。现有改进组合性的方法主要依赖于在推理时增加计算开销，例如采用不同的思维链提示方法，让模型通过生成更多推理步骤来提升性能。然而，一个优秀的MLLM本不应为了回答关于图像中物体、属性及其关系的简单问题而额外花费时间生成复杂的蕴含关系。</p>
<p>本文针对上述痛点，提出通过偏好微调（preference tuning）来直接改进MLLMs，教导模型“不喜欢”那些看似正确但在细微之处出错的复杂场景描述。核心思路是：<strong>利用大语言模型（LLM）的推理能力，从现有图像-描述数据中全自动生成高质量、具有挑战性的合成负例描述，构建偏好数据对，并基于直接偏好优化目标对MLLMs进行微调，从而系统性地提升其组合性推理能力。</strong></p>
<h2 id="方法详解">方法详解</h2>
<p>SCRAMBLe方法包含三个核心阶段：合成数据生成、过滤和训练。整体目标是从一个原始的图像-描述对集合中，为每张图像生成一个高质量且具有挑战性的负例描述，形成偏好三元组（图像，正例描述，负例描述），并用其微调MLLM。</p>
<p><img src="https://arxiv.org/html/2504.04740v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SCRAMBLe方法整体框架。即使是最先进的MLLMs（如GPT-4o）也可能在组合性测试中失败（左）。SCRAMBLe提出了一种全自动方法：利用LLM专家生成合成负例描述，经过过滤后得到高质量的偏好数据，并用于微调MLLMs，从而显著提升其组合性，同时在通用QA基准上带来较小的整体改进。</p>
</blockquote>
<p><strong>1. 合成数据生成</strong><br>此阶段旨在为给定正例描述生成一个语义和语法都有效，但在组合性上与正例描述不同的负例描述。</p>
<ul>
<li><strong>基线方法（Swap Objects/Attributes）</strong>：类似于SugarCREPE的方法，通过提示LLM交换描述中的名词或形容词短语来生成负例。例如，将“A woman cutting into a cake with a man standing behind her”中的“a woman”和“a man”交换，得到“A man cutting into a cake with a woman standing behind him”。该方法虽然有效，但生成的负例常缺乏逻辑性或含义未变。</li>
<li><strong>改进方法（Chain of Thought）</strong>：这是SCRAMBLe的核心创新。与基线简单指令不同，该方法要求LLM专家（本文使用Llama-3.1-70B）通过<strong>思维链</strong>进行推理来生成负例。提示中包含了与基线相同的三个要求（描述不同场景、流畅语法正确、逻辑合理），但允许LLM更自由地重新排列词汇。关键是为LLM提供少量（5个）上下文示例，这些示例展示了如何将一个描述（如来自Winoground的配对描述）通过逐步推理转化为另一个。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.04740v2/x4.png" alt="思维链示例"></p>
<blockquote>
<p><strong>图4</strong>：将输入描述转化为负例描述的思维链推理示例。通过识别关键元素（颜色）、理解任务目标（用相同词汇描述不同图像）、并执行结构化交换（交换颜色词的位置），最终生成高质量负例“a white bird with a pink beak”。</p>
</blockquote>
<p><strong>2. 过滤</strong><br>由于生成过程可能存在噪声，SCRAMBLe引入了一个后过滤步骤以保留高质量的负例描述。</p>
<ul>
<li><strong>工具</strong>：使用Vera合理性模型和TextAttack的语法模型来评估生成的正负描述对。</li>
<li><strong>对抗性精炼</strong>：采用与SugarCREPE类似的过程，旨在基于正负描述之间的分数差异对生成的数据集进行<strong>去偏</strong>。它确保具有特定分数差异的示例数量与具有相反符号差异的示例数量相等，即分数差异的分布关于零对称。这防止了模型在训练时利用描述本身的合理性或语法正确性作为捷径来选择答案，从而迫使模型必须关注图像内容。</li>
</ul>
<p><strong>3. 训练</strong><br>获得过滤后的偏好数据集后，使用<strong>直接偏好优化</strong>（DPO）目标函数对目标MLLM进行微调。具体是在MLLM上添加低秩适配器（LoRA）进行训练。损失函数如论文公式（1）所示，其中<code>x</code>是提示（图像 + “caption : ”），<code>y_w</code>是正例描述，<code>y_l</code>是负例描述，<code>π_ref</code>是微调前的原始模型，<code>π_θ</code>是带适配器的可训练模型。DPO方法稳定且计算高效，无需训练额外的奖励模型。</p>
<p><strong>创新点总结</strong>：与NegCLIP等先前工作相比，SCRAMBLe的创新具体体现在：（1）<strong>生成质量</strong>：强调通过LLM思维链推理生成高质量、逻辑合理的硬负例，而非简单的短语交换。（2）<strong>过滤机制</strong>：采用结合语法和合理性检查的对抗性精炼，确保数据质量并消除模型可能学习的捷径。（3）<strong>应用对象</strong>：将方法应用于更强大的MLLMs，而非早期的图像-文本编码器模型（如CLIP）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：<ul>
<li><strong>组合性基准</strong>：Winoground， EqBen， COLA， ConMe。</li>
<li><strong>控制/通用QA基准</strong>：SEED-Bench（图像子集）， MM-Vet。</li>
</ul>
</li>
<li><strong>评估指标</strong>：对于Winoground、EqBen、COLA，报告组得分准确率；对于ConMe和SEED-Bench，报告平均准确率；对于MM-Vet，报告由GPT-4法官评分的平均分（0-1）。</li>
<li><strong>对比模型</strong>：测试了三个开源MLLMs：LLaVA-1.5-13B， MoLMo-7B， Llama-3.2-11B-Vision-Instruct。对比了微调前后的性能，并与基线数据生成方法进行了对比。</li>
<li><strong>评估方法</strong>：在需要图像-描述亲和力得分的任务中，使用VQAScore方法，即询问模型“Does this image show [caption]?”并取生成“Yes”的概率作为得分。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.04740v2/images/benchmark_examples_v3.png" alt="组合性基准示例"></p>
<blockquote>
<p><strong>图3</strong>：各组合性基准示例。Winoground包含通过交换对象、关系或两者来改变描述的自然图像示例。EqBen包含通常只有微小方面变化的自然和合成图像对。COLA包含自然图像中两个对象间不同关系的示例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.04740v2/x5.png" alt="ConMe基准示例"></p>
<blockquote>
<p><strong>图6</strong>：ConMe基准示例。ConMe向MLLM提出一个二选一问题，要求其输出选项A或B。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>SCRAMBLe全面提升MLLM性能</strong>：如表1所示，在三个MLLM上，SCRAMBLe微调后，在大多数组合性基准和通用QA控制基准上均取得了性能提升。例如，MoLMo-7B在Winoground上的准确率从**49.5%提升至54.8%**，在SEED-Bench上从74.04%提升至74.61%，在MM-Vet上从59.3提升至60.9。这表明SCRAMBLe在专注于提升组合性的同时，也能带来整体性能的改进。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.04740v2/x6.png" alt="主要结果表"></p>
<blockquote>
<p><strong>表1</strong>：SCRAMBLe提升了不同开源MLLM在组合性和控制基准上的性能。所有模型在大多数指标上均有改善。</p>
</blockquote>
<ol start="2">
<li><p><strong>Winoground性能领先</strong>：如表5所示，SCRAMBLe微调后的MoLMo-7B在Winoground上取得了<strong>54.8%</strong> 的组得分，超越了所有先前报告的方法（包括使用额外推理计算的CECE方法以及未微调的MoLMo基线49.5%），达到了当前最佳性能。</p>
</li>
<li><p><strong>数据质量对比</strong>：与基线“交换对象/属性”方法相比，SCRAMBLe的“思维链”方法能生成更高质量、更多样化的负例（如表2所示），并且在过滤后保留了更多有效数据（如表3所示：思维链方法最终保留57,786个示例，远高于基线交换方法的23,450个）。如表4所示，使用SCRAMBLe数据微调的LLaVA模型在组合性基准上提升更显著（如COLA从49.5%提升至55.7%），且<strong>未损害</strong>MM-Vet上的长答案生成性能（从36.2提升至38.6），而基线数据微调则导致MM-Vet性能大幅下降至30.7。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2504.04740v2/x7.png" alt="负例生成质量对比"></p>
<blockquote>
<p><strong>表2</strong>：负例生成的定性对比示例。基线方法在某些情况下有效（前三行），但在其他情况下会产生无意义输出（第四行）或直接放弃生成（第五行）。SCRAMBLe的思维链方法能处理更复杂情况，生成更高质量的硬负例。</p>
</blockquote>
<ol start="4">
<li><p><strong>消融实验</strong>：表4本身可视为对数据生成方法的消融。结果表明，<strong>高质量的负例生成（思维链）是性能全面提升的关键</strong>。低质量的负例（基线交换法）虽能在部分组合性任务上带来有限提升，但会严重损害模型的其他能力（如长文本生成）。</p>
</li>
<li><p><strong>定性分析</strong>：如图7所示，SCRAMBLe微调后的模型能够修正原始模型在细粒度识别上的错误。例如，在EqBen的一个例子中，MoLMo错误地识别了银色盘子的位置，而SCRAMBLe-Molmo则能正确判断。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2504.04740v2/x8.png" alt="定性改进示例"></p>
<blockquote>
<p><strong>图7</strong>：与SCRAMBLe-Molmo对话的示例（来自EqBen）。SCRAMBLe修正了MoLMo在识别银色盘子位置时犯的错误。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>自动化偏好数据生成流程</strong>：提出了一套全自动的管道（SCRAMBLe），能够从现有图像-描述数据中生成高质量的合成偏好数据，用于微调MLLMs，显著提升其组合性推理能力。</li>
<li><strong>高质量的硬负例生成方法</strong>：引入了基于大语言模型思维链推理的负例生成方法，并结合语法与合理性过滤，确保了负例的挑战性和逻辑有效性，避免了模型学习捷径。</li>
<li><strong>全面的性能提升</strong>：实验表明，该方法不仅在多个组合性基准上取得了显著进步（如Winoground达到当前最佳），同时也能小幅提升模型在通用视觉问答任务上的整体性能，实现了 holistic 改进。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，在微调Llama-3.2模型时，发现使用完整的57.8k合成数据集会<strong>导致过拟合</strong>，而使用一个较小的子集（约10k示例）则能取得性能提升。这表明不同模型对数据量和训练策略的敏感性可能不同，需要针对性地调整。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据质量优先</strong>：对于提升模型组合性等复杂推理能力，合成数据的质量比数量更重要。确保负例具有语义和逻辑上的挑战性，是有效训练的关键。</li>
<li><strong>轻量高效的微调</strong>：结合DPO和LoRA的微调策略，表明可以通过相对轻量和高效的训练方式，对大型MLLMs进行有针对性的能力增强。</li>
<li><strong>与其他方法的互补性</strong>：SCRAMBLe作为一种基于数据微调的方法，与需要额外推理时间的测试时方法（如思维链提示）是互补的。未来可以探索两者的结合。</li>
<li><strong>扩展应用</strong>：该方法可以扩展到其他需要细粒度区分和关系理解的多模态任务中，例如视觉常识推理、具身推理等。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多模态大语言模型（MLLMs）在组合性推理上的不足，即难以正确识别场景作为原子视觉概念的组合（如区分“狗追猫”与“猫追狗”）。为解决此问题，提出了SCRAMBLe方法：通过全自动生成高质量合成偏好数据，对MLLMs进行二元偏好学习调优。关键技术包括利用LLM推理和基于合理性、语法的过滤来生成有效且具挑战性的负样本描述。实验结果显示，SCRAMBLe调优的Molmo-7B模型在Winoground基准上性能从49.5%提升至54.8%（当前最佳），同时在一般视觉问答任务上也有1%的提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.04740" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>