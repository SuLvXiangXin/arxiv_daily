<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.07395" target="_blank" rel="noreferrer">2505.07395</a></span>
        <span>作者: Zhang, Hongyin, Zhuang, Zifeng, Zhao, Han, Ding, Pengxiang, Lu, Hongchao, Wang, Donglin</span>
        <span>日期: 2025/05/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于模仿学习范式的视觉-语言-动作模型在通用机器人决策任务中展现出潜力。这类模型通常在预训练的视觉-语言模型基础上，使用下游机器人数据进行后训练。然而，其性能受限于训练数据质量的参差不齐，即使数据来自成功演示，质量也可能不均。现有模仿学习方法难以区分这种不均匀的数据质量并充分利用混合质量数据。另一方面，离线强化学习擅长从混合质量数据中学习鲁棒的策略模型。尽管已有初步尝试将VLA与RL结合，但为视觉语言操作任务设计广泛适用的密集奖励，并将RL的收益最大化概念融入VLA模型，仍未得到充分探索。本文针对VLA模型难以充分利用混合质量数据这一痛点，提出将RL的密集回报最大化原则整合到端到端VLA模型中，其核心思路是：通过设计捕获操作任务特性的密集奖励，并利用期望分位数回归预测最大化回报，使模型能深入理解数据质量分布，从而生成更鲁棒的决策动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>ReinboT是一个新颖的端到端VLA模型，旨在将最大化密集回报的RL原则融入机器人视觉运动控制。其整体流程是：首先为长视界操作任务设计密集奖励并计算回报，然后在训练时，模型同时学习预测未来图像状态、机器人动作以及基于当前状态和目标的“最大化”回报；在推理时，模型自回归地预测最大化的回报和对应的动作，以指导执行。</p>
<p><img src="https://arxiv.org/html/2505.07395v1/extracted/6430064/figures/method_overview_3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ReinboT模型整体框架。模型利用CLIP编码语言指令，ViT（及感知重采样器）编码图像状态，MLP编码本体感知。基于GPT风格的Transformer，引入了三个预测令牌嵌入（[RTG]、[ACTION]和[IMAGE]）来分别预测ReturnToGo、机器人动作和未来图像状态。ReturnToGo解码器的最后一层隐藏特征进一步用于预测机器人动作。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><p><strong>奖励密集化方法</strong>：为解决长视界任务中的奖励分配问题，首先采用启发式方法将任务轨迹分解为仅包含单一子目标的多个轨迹段。密集奖励设计包含四个考虑因素：</p>
<ul>
<li><strong>子目标达成</strong>：奖励结合了本体感知跟踪、像素强度、图像视觉质量（SSIM）和图像特征点匹配（ORB）的相似度。</li>
<li><strong>任务进度</strong>：奖励与当前状态所在的子目标序列序号成正比，越接近最终目标奖励越大。</li>
<li><strong>行为平滑度</strong>：惩罚机器人关节速度、加速度以及动作的变化率，以鼓励平滑、自然的运动轨迹。</li>
<li><strong>任务完成度</strong>：在轨迹成功完成指令时给予稀疏奖励。<br>总奖励为这四个奖励的加权和，权重用于平衡各分量的量级。</li>
</ul>
</li>
<li><p><strong>端到端强化VLA模型</strong>：模型以GPT风格Transformer为骨干网络。它将ReturnToGo视为一种新的模态数据，并引入[RTG]预测令牌嵌入。模型通过ReturnToGo解码器预测在给定语言指令、图像状态和本体感知下的最大化回报，其损失函数采用期望分位数回归损失，目的是使预测的回报尽可能接近当前目标和状态下可实现的<strong>最大</strong>回报（由超参数m控制趋近程度）。总损失函数包含ReturnToGo损失、机械臂动作损失（平滑L1）、夹爪动作损失（交叉熵）和未来图像损失（像素级MSE）。</p>
</li>
<li><p><strong>动作预测机制</strong>：网络采用模块化设计。首先，将语言指令、历史图像和本体感知输入骨干网络，得到对应于[RTG]和[ACTION]令牌嵌入的特征。随后，将[RTG]特征输入ReturnToGo解码器得到预测的回报值，并将该回报值与其他特征拼接，最终通过动作解码器预测动作。这种设计使得动作生成能够受到预测的最大化回报的引导。</p>
</li>
</ol>
<p>与现有方法相比，创新点主要体现在：1) 提出了一种广泛适用于长视界视觉语言操作任务的密集奖励设计方法；2) 首次在通用VLA模型中实现了RL的回报最大化原则，通过预测最大化回报来显式地引导高质量动作生成，而非仅仅模仿演示数据或依赖稀疏奖励。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验主要使用了CALVIN混合质量数据集进行模拟评估，并在真实世界任务中测试了少量样本学习和分布外泛化能力。实验平台涉及模拟环境和真实机器人（UR5机械臂）。</p>
<p>对比的基线方法包括：模仿学习VLA模型GR-1；离线RL序列建模方法Decision Transformer以及其改进版本Reinformer。</p>
<p>关键实验结果如下：<br>在CALVIN数据集上，ReinboT在D、ABC三个测试集上的平均成功率达到**81.7%**，显著优于GR-1的68.0%、Decision Transformer的53.7%和Reinformer的71.0%。</p>
<p><img src="https://arxiv.org/html/2505.07395v1/extracted/6430064/figures/figures_20250124/20250124_data_distribution_histogram.png" alt="模拟环境性能对比"></p>
<blockquote>
<p><strong>图5</strong>：在CALVIN数据集上的性能对比直方图。ReinboT在D、ABC测试集上的成功率均超过80%，显著高于所有基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.07395v1/extracted/6430064/figures/figures_20250124/20250124_rtg_calvin_combined_dist_training_data.png" alt="回报预测可视化"></p>
<blockquote>
<p><strong>图7</strong>：训练数据与ReinboT预测的ReturnToGo分布对比。模型预测的回报（红色）倾向于靠近数据分布中回报较高的区域，体现了其最大化回报的特性。</p>
</blockquote>
<p>在真实世界任务中，ReinboT仅用<strong>10条演示轨迹</strong>进行微调，在4个任务上平均成功率达到**82.5%**，而GR-1仅为57.5%。在分布外泛化测试中（改变物体颜色、纹理、位置），ReinboT也表现出更强的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2505.07395v1/extracted/6430064/figures/figures_20250124/real_eval.png" alt="真实世界评估"></p>
<blockquote>
<p><strong>图4</strong>：真实世界少量样本学习评估结果。ReinboT在四个任务上的平均成功率远高于GR-1基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.07395v1/extracted/6430064/figures/real_ours_results_1.png" alt="真实世界结果展示"></p>
<blockquote>
<p><strong>图6</strong>：真实世界任务定性结果。展示了ReinboT成功执行“为我捡起绿色杯子”和“将较小的蓝色碗放入红色碗中”两个任务的序列图像。</p>
</blockquote>
<p>消融实验总结了各组件贡献：</p>
<ol>
<li><strong>ReturnToGo预测的重要性</strong>：移除ReturnToGo预测模块（即仅进行模仿学习）会使性能显著下降，在CALVIN上的成功率从81.7%降至75.3%。</li>
<li><strong>期望分位数回归超参数m</strong>：实验表明，参数m需要谨慎选择。过大的m会导致模型过于乐观地估计回报，可能预测分布外的回报，从而损害性能；论文中采用的m=0.7取得了最佳平衡。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07395v1/extracted/6430064/figures/figures_20250124/ablation_weight_tau.png" alt="消融实验：期望分位数回归参数"></p>
<blockquote>
<p><strong>图2</strong>：期望分位数回归超参数m的消融实验。m=0.7时模型性能最佳，过大或过小都会导致性能下降。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.07395v1/extracted/6430064/figures/figures_20250124/ablation_rtg.png" alt="消融实验：RTG预测"></p>
<blockquote>
<p><strong>图3</strong>：ReturnToGo预测模块的消融实验。移除RTG预测（w/o RTG）后性能明显下降，证明了最大化回报引导的有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了ReinboT，一个将RL回报最大化原则整合到端到端VLA模型中的新框架；2) 设计了一种广泛适用于长视界视觉语言操作任务的密集奖励方法，使模型能深入理解数据质量分布；3) 通过大量实验验证了ReinboT在模拟和真实任务中的最先进性能，特别是在利用混合质量数据和少量样本学习方面的优势。</p>
<p>论文自身提到的局限性在于：期望分位数回归超参数m的选择需要谨慎，以避免对回报的过估计；此外，方法仍然依赖于一定数量的高质量演示数据来构建有效的回报信号。</p>
<p>本文对后续研究的启示在于：为视觉语言操作任务设计合理的密集奖励是提升VLA模型鲁棒性和数据利用效率的有效途径；将序列建模的表示学习能力与RL的优化目标（如回报最大化）相结合，是提升决策模型性能的一个有前景的方向；如何进一步减少对高质量演示数据的依赖，或从更粗糙的数据中自动学习奖励函数，是值得探索的问题。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ReinboT模型，旨在解决视觉-语言-动作（VLA）模型因训练数据质量不均而导致的操控精度受限问题。其核心方法是将强化学习的最大化累积回报原则融入端到端VLA框架，通过预测密集回报来量化数据质量分布，并将长时程任务自动分解为单子目标轨迹段。实验表明，ReinboT在CALVIN混合质量数据集上达到领先性能，并在真实任务中展现出优异的少样本学习与分布外泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.07395" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>