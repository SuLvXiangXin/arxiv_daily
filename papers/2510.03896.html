<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.03896" target="_blank" rel="noreferrer">2510.03896</a></span>
        <span>作者: Liu, Mingyu, Huang, Zheng, Lin, Xiaoyi, Zhu, Muzhi, Zhao, Canyu, Du, Zongze, Wang, Yating, Zhu, Haoyi, Chen, Hao, Shen, Chunhua</span>
        <span>日期: 2025/10/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前将视觉语言模型（VLM）的能力应用于物理世界的主流方法主要有两类。第一类是端到端的视觉-语言-动作（VLA）模型，它将推理与动作生成集成在一个单一架构中。这类方法在稀缺、窄领域的机器人数据上进行微调时，容易导致灾难性遗忘，损害VLM原有的世界知识，从而泛化能力差。第二类是近期的双系统框架，试图将“思考”（规划）与“行动”（执行）解耦。然而，这类方法中动作模块仍需解释来自VLM的、语义模糊的中间表示（如视觉特征或语义嵌入），这给轻量级的动作策略带来了语义解释的负担，使得大规模跨任务训练不可行，泛化能力依然受限，且两系统间的协作机制定义不清。</p>
<p>本文针对动作模块面临的“语义模糊性”这一核心痛点，提出了一个新视角：使用<strong>明确的、稀疏的3D轨迹</strong>作为中间表示，来桥接高级规划与低级控制。本文的核心思路是：让VLM仅负责生成粗略的3D路径点（思考），而由一个通用化的动作专家基于实时点云观察，将这些路径点细化为稠密、可执行的动作序列（行动）。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法框架旨在通过稀疏3D空间轨迹，实现高级VLM规划器与低级动作专家的彻底解耦。</p>
<p><img src="https://arxiv.org/html/2510.03896v1/x1.png" alt="方法整体流程"></p>
<blockquote>
<p><strong>图1</strong>：方法整体流程。首先，VLM在相机坐标系下预测稀疏的3D路径点及目标末端执行器姿态；随后，这些点被转换到机器人基座坐标系并通过B样条插值为连续的姿态轨迹；最后，动作专家采样该轨迹作为引导，并结合实时点云观测，生成具体的动作序列。</p>
</blockquote>
<p>整体流程分为两个核心阶段：</p>
<ol>
<li><strong>VLM的3D推理与轨迹生成</strong>：VLM接收图像和语言指令，其任务不是直接预测机器人基座坐标，而是在<strong>相机坐标系</strong>下预测一组稀疏的3D路径点以及最终的目标末端执行器姿态。论文认为，让VLM预测基座坐标会迫使其学习复杂的相机-机器人变换，这与VLM以视觉为中心的特性相悖，易导致其记忆坐标而非真正理解空间关系，从而泛化能力差。在相机坐标系下预测则更符合VLM基于图像预训练的先验知识。为了赋予VLM这种零样本3D姿态推理能力，论文采用少量监督微调（SFT）。数据标注流程如<strong>图2</strong>所示，关键是在夹爪状态（如开/合）发生变化的关键帧进行标注，构建SFT数据集。VLM首先预测目标物体的2D锚点坐标，结合深度信息得到3D坐标，进而推理出运动路径上的路径点。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.03896v1/x2.png" alt="数据标注流程"></p>
<blockquote>
<p><strong>图2</strong>：数据标注流程。基于夹爪状态变化选择关键帧进行标注，构建SFT数据集，使VLM能够估计3D空间姿态。</p>
</blockquote>
<ol start="2">
<li><strong>通用化动作专家的训练与执行</strong>：动作专家的核心作用是将VLM提供的粗略轨迹（引导）根据实时环境点云细化为精确动作。其架构采用条件扩散模型，输入条件 $\mathcal{C}<em>{A}$ 包括：机器人本体感知状态 $S_t$、从插值轨迹中采样的引导姿态 $\mathcal{P}</em>{g}$、以及裁剪后的局部环境点云 $O_{pcd}$。这些信息通过各自编码器处理后拼接，形成条件特征向量 $f_{t}^{A}$。训练时，模型学习去噪过程，目标函数是预测所添加噪声的L2误差。</li>
</ol>
<p>本文的关键创新在于提出了 <strong>“动作预训练，点云微调”</strong> 的训练范式。动作专家的技能可分解为：基础的轨迹跟随能力，以及利用点云进行环境感知的轨迹细化能力。论文发现，将这两个耦合的技能放在一起训练成本高且学习次优。因此，首先在<strong>纯轨迹数据</strong>（不含点云）上大规模预训练动作专家，使其掌握基础的运动跟随能力；随后，再在包含点云的数据上进行微调，使其学会基于环境进行细化。这种解耦策略极大地提升了数据利用效率和收敛速度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在仿真基准（RoboTwin、ManiSkill）和真实机器人上进行。动作专家始终以<strong>零样本</strong>方式部署，VLM仅进行少量SFT以学习格式化输出和姿态推理。对比的基线方法包括通用模型（Pi0*, RDT*）和单任务专家模型（ACT, DP, DP3）。</p>
<p><strong>主要结果（仿真）</strong>：在RoboTwin基准的11个任务上（分为短、中、长视野），结果如<strong>表1</strong>所示。本文方法在所有任务上超越了通用模型。在短、中视野任务上，性能与单任务专家DP3相当。在需要VLM进行规划的长视野任务上优势显著，取得了60%的平均成功率，而其他专家模型在此类任务上几乎完全失败（成功率约0-2%）。关键区别在于，其他通用模型（Pi0, RDT）在多任务训练后仍需针对每个任务进行特定微调，而本文方法无需此步骤（<strong>表2</strong>）。</p>
<p><img src="https://arxiv.org/html/2510.03896v1/bta/Figures/robotwin_dp.png" alt="RoboTwin性能对比表"></p>
<blockquote>
<p><strong>表1</strong>：在RoboTwin基准上的性能对比。本文方法在长视野任务上表现突出，且无需任务特定微调。</p>
</blockquote>
<p><strong>泛化能力</strong>：</p>
<ul>
<li><strong>相机视角泛化</strong>：在仅用200条不同视角轨迹微调后，模型在未见过的（Out-of-Domain）相机视角下性能下降很小（<strong>表3</strong>），证明了其视角不变性。</li>
<li><strong>零样本泛化</strong>：在涉及新颜色、新物体、新语义的任务上，本文方法显著优于使用相同预训练模型的Pi0基线（<strong>表4</strong>）。</li>
<li><strong>跨环境泛化</strong>：在从未用于预训练的ManiSkill基准上测试，动作专家表现出了良好的迁移能力（<strong>表5</strong>）。</li>
</ul>
<p><strong>真实世界实验</strong>：设置了6个任务（短、中、长视野各两个）。仅使用300条多任务轨迹微调VLM，动作专家保持冻结。结果如<strong>表6</strong>所示，本文方法（VLM+DP with PromptDepth）取得了78.3%的平均成功率，优于需要单任务微调的基线模型（如DP3为60%）。VLM+IK基线（仅用逆运动学执行VLM路径点）性能较差，凸显了动作专家进行轨迹细化的必要性。</p>
<p><img src="https://arxiv.org/html/2510.03896v1/bta/Figures/realword_dp.png" alt="真实世界性能对比表"></p>
<blockquote>
<p><strong>表6</strong>：真实世界任务性能对比。本文方法（VLM+DP）在仅微调VLM的情况下取得了最佳平均性能。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ol>
<li><p><strong>VLM微调步数</strong>：<strong>图4/表7</strong>显示，使用动作专家能使系统性能更快达到饱和，大幅减少所需SFT步数，从而更好地保留VLM的语言泛化能力。<br><img src="https://arxiv.org/html/2510.03896v1/x4.png" alt="SFT步数消融"></p>
<blockquote>
<p><strong>图4/表7</strong>：随着VLM SFT步数增加，使用动作专家比直接使用逆运动学（IK）性能饱和更快，且能保留更多语言能力。</p>
</blockquote>
</li>
<li><p><strong>训练策略</strong>：<strong>图5</strong> 证实了“动作预训练，点云微调”（APPF）范式的优越性。纯轨迹预训练阶段因无需处理点云，可将批量大小增至32768，加速基础运动技能学习，实现更快收敛和更高最终性能。<br><img src="https://arxiv.org/html/2510.03896v1/x5.png" alt="训练策略消融"></p>
<blockquote>
<p><strong>图5</strong>：“动作预训练，点云微调”（APPF）策略相比端到端联合训练，收敛更快且最终成功率更高。</p>
</blockquote>
</li>
<li><p><strong>点云数据来源</strong>：<strong>表9</strong> 显示，结合多种来源的点云数据（包括质量较低的仿真和真实数据）进行训练，虽然可能在纯仿真测试中略有性能损失，但能显著缩小仿真与真实世界的性能差距，提升部署鲁棒性。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：</p>
<ol>
<li><strong>提出了一个以通用化动作专家为核心的框架</strong>，首次使用<strong>稀疏3D轨迹</strong>作为VLM与动作模块间明确、无歧义的接口，实现了规划与执行的彻底解耦，该专家无需任务特定微调即可部署。</li>
<li><strong>提出了“动作预训练，点云微调”的训练范式</strong>，通过解耦运动基础学习与环境感知细化，高效地训练出具有强泛化能力的动作专家。</li>
<li><strong>通过大量实验证明了系统的强大零样本泛化能力</strong>，包括跨视觉域、相机视角、语言指令以及从仿真到真实世界的迁移。</li>
</ol>
<p>论文提到的局限性包括：点云质量对动作专家性能有影响；VLM仍需少量SFT来学习输出格式。本文的启示在于：在分层机器人系统中，设计<strong>明确的、几何化的接口</strong>（如3D路径点）是避免语义模糊、实现模块化泛化的关键；<strong>分阶段的训练策略</strong>（先基础后细化）能有效提升学习效率和最终性能；结合多样化的、即使是不完美的数据，有助于模型适应现实世界的复杂性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决视觉语言模型（VLM）在物理世界中执行任务时，因推理与动作模块耦合或语义模糊导致的泛化能力差、需频繁微调的问题。为此，论文提出了一种以**可泛化动作专家**为核心的新框架，其关键技术是采用**稀疏3D轨迹作为中间表示**，连接VLM的高层规划与底层动作执行，并引入了 **“动作预训练，点云微调”** 的训练范式。实验表明，该方法在多样化的视觉领域、相机视角和语言指令下均实现了**高质量的零样本泛化**，无需针对新环境进行微调。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.03896" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>