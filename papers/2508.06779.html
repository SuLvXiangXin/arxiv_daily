<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning a Vision-Based Footstep Planner for Hierarchical Walking Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning a Vision-Based Footstep Planner for Hierarchical Walking Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.06779" target="_blank" rel="noreferrer">2508.06779</a></span>
        <span>作者: Michael Posa Team</span>
        <span>日期: 2025-08-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，双足机器人在非结构化地形中的导航主要采用模块化框架，包括感知、规划和控制器。现有方法通常依赖于手工设计的视觉特征和基于模型的策略，这些方法对噪声敏感，且通常局限于分段平面等简化地形假设，在现实世界中显得脆弱。同时，基于模型预测控制（MPC）和简化模型（ROM）的方法在存在噪声视觉输入时缺乏鲁棒性。尽管端到端强化学习（RL）方法减少了对人工特征的依赖，但直接将策略从仿真迁移到硬件存在挑战。</p>
<p>本文针对在非结构化环境中实现鲁棒、实时的视觉脚步规划这一痛点，提出了一种新的层次化控制视角。核心思路是：<strong>利用强化学习训练一个高层脚步规划器，该规划器基于局部高程图和低维动力学状态（ALIP模型）实时生成三维落脚点指令，并由一个低层的基于模型的 Operational Space Controller 进行轨迹跟踪执行。</strong></p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架是一个分层的视觉-控制架构，由感知模块、高层RL脚步规划器和低层全身控制器组成。</p>
<p><img src="https://arxiv.org/html/2508.06779v1/images/overview.png" alt="系统概览"></p>
<blockquote>
<p><strong>图2</strong>：系统框图。感知模块（绿色）以30Hz从RealSense D455生成高程图。高层脚步策略（蓝色）以40Hz输出脚步动作。这些动作被发送到低层控制器（紫色）进行关节控制。</p>
</blockquote>
<p><strong>1. 感知模块：</strong> 使用向下倾斜的深度相机（Intel RealSense D455）获取点云，并滤除机器人腿部。点云被输入到一个以机器人为中心的高程映射框架中，以30Hz的频率更新地图。地图被裁剪为一个64x64的网格（分辨率0.025米/单元格），并通过最近邻插值和中值滤波器进行后处理。为了提供结构化的空间信息，高程图会与一组以期望落脚点（由周期性ALIP轨迹计算得出）为中心的潜在脚步位置网格进行拼接。</p>
<p><img src="https://arxiv.org/html/2508.06779v1/images/encode.png" alt="编码输入"></p>
<blockquote>
<p><strong>图3</strong>：高程图被裁剪为64x64网格，并与XY脚步位置网格拼接。这些网格的中心位置对应期望的落脚点。</p>
</blockquote>
<p><strong>2. 高层RL脚步规划器：</strong> 策略 $\pi_{\theta}$ 的输入包括操作员提供的期望速度 $v_{des}$、质量归一化的ALIP状态 $s_{ALIP}$ 以及编码后的高程图 $M_{elevation}$，输出是在当前支撑腿坐标系下的落脚点坐标 $[p_x, p_y, p_z]$。策略网络架构包含一个处理高程图的残差网络和一个长短期记忆（LSTM）网络。残差网络提取的潜在特征与 $[v_{des}, s_{ALIP}]$ 拼接后输入LSTM生成动作。动作在输出前会与步态时序信息拼接以确保与步态周期的时间一致性。采用演员-评论家框架，且评论家网络额外接收真实高度图、关节位置和骨盆姿态等特权信息以提升训练效率。</p>
<p><strong>3. 奖励函数设计：</strong> 总奖励 $r$ 是多项奖励与惩罚的加权和，并约束为非负值。关键奖励项包括：鼓励跟踪期望速度的 $r_{v_x}$ 和 $r_{v_y}$；鼓励脚步高度与地形对齐的 $r_z$；鼓励骨盆稳定的 $r_{\phi}$；鼓励动作平滑的 $r_{a_t}$；以及鼓励动作接近LQR最优脚步的 $r_{reg}$。关键惩罚项包括：摆动脚轨迹跟踪偏差惩罚 $p_{track}$；扭矩使用惩罚 $p_{\tau}$；边缘踩踏惩罚 $p_{edge}$（使用Sobel滤波器检测）；以及脚与地形碰撞惩罚 $p_{collision}$。消融实验表明，缺少 $p_{edge}$ 会导致在楼梯边缘打滑，缺少 $p_{collision}$ 会导致机器人发展出在硬件上会失败的“脚尖试探”策略。</p>
<p><strong>4. 低层全身控制器：</strong> 采用基于逆动力学的操作空间控制二次规划（QP）来实现高层规划器的输出。控制目标包括：跟踪以落脚点为终点的多项式样条摆动脚轨迹；将质心高度控制到定义ALIP模型的虚拟平面；控制摆动脚角度与该平面平行；控制骨盆俯仰和横滚角为零；控制摆动腿偏航角为零；以及跟踪操作员指令的骨盆偏航速率。具体的权重和增益矩阵见表I。</p>
<p><strong>5. 训练与迁移策略：</strong></p>
<ul>
<li><strong>课程学习：</strong> 初始阶段在平坦地形、矮楼梯和斜坡上训练，不引入域随机化。收敛后，引入所有地形类型和域随机化，并将平坦地形的出现概率约束在10%，以防止遗忘负向速度指令。</li>
<li><strong>域随机化：</strong> 为促进仿真到现实的迁移，对动力学参数（如PD增益、关节阻尼、连杆质量、摩擦系数）、ALIP状态、高程图（整体平移和均匀噪声）、点云数据（系统偏差）以及通信延迟进行了均匀分布的随机化（具体范围见表IV）。</li>
<li><strong>对称性损失：</strong> 在PPO目标函数中添加了镜像损失 $L_{mirror}$，以鼓励对称的步态。</li>
<li><strong>高程图漂移校正：</strong> 由于状态估计器在足部接触冲击下会产生垂直方向漂移，采用基于已知支撑脚位置的校正策略，在每次地图更新前计算并应用垂直偏移量，以保持地图与地面对齐。</li>
</ul>
<p><strong>创新点：</strong> 1) 将ALIP这一低维、可解释的简化模型状态作为RL策略的核心输入之一，在降低复杂度的同时编码了关键动力学信息。2) 设计了层次化架构，将基于学习的视觉规划与基于模型的鲁棒底层控制相结合，兼具灵活性与可靠性。3) 通过精心设计的奖励函数、课程学习和广泛的域随机化，实现了从仿真到硬件的复杂策略迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与基准：</strong> 使用欠驱动双足机器人Cassie，在Drake仿真环境和真实硬件上进行评估。对比的基线方法包括：基于视觉的ALIP模型预测控制（MPC）脚步规划器。本文训练了两种策略进行对比：<strong>ALIP策略</strong>（观测空间含高程图、期望速度、ALIP状态）和<strong>Joint策略</strong>（额外包含关节位置）。</p>
<p><strong>数据集/地形：</strong> 训练和测试涵盖了六类地形（图4）：平坦地形、带随机障碍物的平坦地形、随机方块地形、楼梯、带斜坡的楼梯、斜坡。楼梯和斜坡地形用于主要性能评估。</p>
<p><img src="https://arxiv.org/html/2508.06779v1/images/terrain.png" alt="地形类型"></p>
<blockquote>
<p><strong>图4</strong>：用于训练的地形类型。上行（左至右）：平坦、带障碍物的平坦、方块地形。下行（左至右）：楼梯、带斜坡的楼梯、斜坡地形。</p>
</blockquote>
<p><strong>关键实验结果：</strong></p>
<ol>
<li><strong>控制器对比（仿真）：</strong> 如表V所示，在多种地形和摩擦系数下，两种RL策略在速度跟踪均方误差和成功率上均一致优于MPC基线。例如，在摩擦系数0.8的下楼梯任务中，ALIP、Joint和MPC的成功率分别为94.5%、91.5%和78%。<strong>Joint策略在速度跟踪精度上普遍优于ALIP策略</strong>。</li>
<li><strong>碰撞与恢复能力：</strong> RL策略显著减少了与地形的硬碰撞（力&gt;1000N）频率（ALIP: 0.1467/episode, Joint: 0.11/episode, MPC: 0.4833/episode）。碰撞后的恢复失败率也更低（ALIP: 38.88%, Joint: 27.71%, MPC: 52.68%）。</li>
<li><strong>定性分析：</strong><ul>
<li><strong>速度跟踪：</strong> 在平坦地形上跟踪预定速度剖面时，RL策略表现出色（图5）。<br><img src="https://arxiv.org/html/2508.06779v1/images/vel.png" alt="速度跟踪"><blockquote>
<p><strong>图5</strong>：在平坦地形上沿x方向按预定速度剖面运动时的仿真速度跟踪性能。</p>
</blockquote>
</li>
<li><strong>避障：</strong> 即使没有显式的避障奖励，策略也能学会在前进时绕开障碍物（图6）。<br><img src="https://arxiv.org/html/2508.06779v1/images/obs_avoid.png" alt="避障"><blockquote>
<p><strong>图6</strong>：在给定0.4m/s前进速度指令且无横向速度输入时，策略成功在前进中避开障碍物。</p>
</blockquote>
</li>
<li><strong>特征归因：</strong> 通过显著性图和积分梯度可视化（图7）发现，策略在平坦地形上关注脚步位置附近，在斜坡上关注前方更远区域以预判高度变化，在楼梯上则强烈关注楼梯边缘。<br><img src="https://arxiv.org/html/2508.06779v1/images/attribution.png" alt="特征归因"><blockquote>
<p><strong>图7</strong>：针对Joint策略在平坦、斜坡和楼梯地形上的特征归因可视化。左：显著性图，右：积分梯度。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<p><strong>硬件实验结果：</strong><br>在真实Cassie机器人上进行了验证（图8）。控制器在室内外平坦类地形上表现成功。然而，<strong>在上楼梯任务中性能下降</strong>，ALIP策略最多只能成功迈两步，而Joint策略能达到四步。这揭示了方法在复杂地形上的局限性。</p>
<p><img src="https://arxiv.org/html/2508.06779v1/images/motion_tile.png" alt="硬件运动序列"></p>
<blockquote>
<p><strong>图8</strong>：Cassie在室外地形行走的图像序列。上行：在室外平坦地形行走。下行：下楼梯（左三张）和上楼梯（右三张）。</p>
</blockquote>
<p><strong>消融实验总结：</strong> 奖励函数中边缘惩罚 ($p_{edge}$) 和碰撞惩罚 ($p_{collision}$) 的消融实验表明，它们对于防止在楼梯边缘打滑和避免产生硬件上不可行的“脚尖试探”策略至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献：</strong></p>
<ol>
<li>提出了一个完整的、基于视觉的层次化控制框架，利用单个深度相机和ALIP简化模型，通过强化学习实现高效、可解释的三维实时脚步规划。</li>
<li>成功实现了从仿真到硬件的全流程迁移，并在结构化与非结构化地形上进行了硬件验证，同时在仿真中系统性地对比并超越了MPC基线。</li>
<li>深入分析了ALIP模型作为策略输入以及层次化控制结构的作用与局限性，特别是在复杂地形上的表现和对仿真到现实迁移的影响。</li>
</ol>
<p><strong>局限性（论文自述）：</strong></p>
<ol>
<li>在硬件上攀登复杂楼梯等极具挑战性的地形时性能不佳，表明当前方法在处理高度非结构化环境时仍有局限。</li>
<li>系统的性能依赖于状态估计的准确性，高程图的垂直漂移问题需要通过额外的校正策略来缓解。</li>
<li>策略可能被困在障碍物之间，导致原地踏步行为。</li>
</ol>
<p><strong>对后续研究的启示：</strong></p>
<ol>
<li>将低维简化模型与学习框架结合是提升双足机器人规划可解释性和效率的有效途径。</li>
<li>层次化设计（学习规划+模型控制）在平衡灵活性与鲁棒性、促进仿真到现实迁移方面显示出优势。</li>
<li>未来工作可探索更强大的感知表示、改进的状态估计方法，或将本框架与更复杂的全身控制相结合，以进一步提升在极端地形上的性能。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双足机器人在非结构化地形中实时脚步规划依赖脆弱的手工视觉管道或仅凭本体感知的问题，提出一种基于视觉的分层控制框架。核心方法整合了基于局部高程图的强化学习高层脚步规划器与低层操作空间控制器，并利用角动量线性倒立摆模型构建低维状态表示以降低复杂度。通过在欠驱动机器人Cassie上进行仿真与硬件实验，该方法在不同地形条件下得到了验证，展示了其实现视觉融合步态规划的能力与挑战。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.06779" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>