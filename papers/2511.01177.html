<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Scaling Cross-Embodiment World Models for Dexterous Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Scaling Cross-Embodiment World Models for Dexterous Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01177" target="_blank" rel="noreferrer">2511.01177</a></span>
        <span>作者: He, Zihao, Ai, Bo, Mu, Tongzhou, Liu, Yulin, Wan, Weikang, Fu, Jiawei, Du, Yilun, Christensen, Henrik I., Su, Hao</span>
        <span>日期: 2025/11/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，构建能够跨越不同形态（具身）操作的通用机器人是机器人学的重要愿景。然而，不同机器人或人与机器人之间在动作空间和运动学结构上的差异，严重阻碍了数据的共享和策略的迁移。现有方法主要分为两类：一是依赖大量模拟强化学习（RL）训练的方法，受限于模拟到真实的差距；二是依赖真实世界专家演示的行为克隆（BC）方法，数据获取成本高。这引出了一个核心问题：是否存在某种不变性，允许动作在不同具身之间迁移？本文提出，环境动力学是具身不变的，而捕捉这些动力学的世界模型（World Model）可以提供一个跨具身的统一接口。为了实现这一目标，关键在于设计能够抽象掉具身特定细节、同时保留控制相关性的状态和动作表示。本文的核心思路是：将不同具身（如人手和机器人手）表示为3D粒子集合，将动作定义为粒子位移，从而创建用于异构数据和控制的共享表示；在此基础上训练一个基于图神经网络的世界模型，并结合基于模型的规划，部署到新硬件上。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法旨在学习一个具身无关的世界模型，以支持跨具身的灵巧操作。整体流程分为训练和部署两个阶段。</p>
<p><img src="https://arxiv.org/html/2511.01177v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：整体框架。关键思想是将具身和物体表示为3D粒子，动作定义为末端执行器粒子位移场。(a) 训练阶段：利用来自模拟中多种机器人手的随机交互数据以及真实世界中的人类演示数据，训练世界模型。(b) 部署阶段：通过正向运动学将采样的关节动作映射为位移场，利用世界模型进行状态预测展开，最终在目标硬件上执行最优轨迹。图中为简化展示单步规划视野。</p>
</blockquote>
<p><strong>1. 统一的粒子状态与动作空间</strong>：为了统一不同具身的数据和控制，该方法定义了粒子状态空间和动作空间。对于具身 <em>e</em>，在时间 <em>t</em> 的末端执行器由 <em>N_e</em> 个粒子表示：*X_t^(e) = { x_i,t^(e) ∈ ℝ³ }<em>。物体由 <em>N_o</em> 个粒子表示：</em>X_t^(o) = { x_i,t ∈ ℝ³ }*。因此，世界状态表示为 *X_t = (X_t^(e), X_t^(o))<em>。动作则定义为末端执行器粒子的位移场：</em>a_t^P = ΔX_t^(e) = { δ_i,t ∈ ℝ³ }*，满足 *X_{t+1}^(e) = X_t^(e) + ΔX_t^(e)*。这种表示将不同形态和自由度的末端执行器以及刚性和可变形物体统一到了一个共同的表示空间中。</p>
<p><strong>2. 感知模块</strong>：该模块负责在数据收集和部署时进行状态估计。采用多视角相机设置。对于真实世界人类数据，使用POEM-v2从多视角图像重建手部网格，并通过最远点采样（FPS）获取粒子。对于物体感知，融合多视角点云，进行泊松表面重建以获得平滑表面，再应用FPS采样粒子。在机器人部署时，机器人自身状态可通过本体感知获得，仅需对物体进行上述感知。</p>
<p><strong>3. 世界模型架构</strong>：采用图神经网络（GNN）作为世界模型架构，利用其局部性和等变性归纳偏置。具体使用DPI-Net，它通过消息传递建模局部粒子相互作用，并通过多步分层传播捕捉全局效应。模型以粒子状态图 <em>⟨X_t, E_t⟩</em>（<em>E_t</em> 为基于半径构建的边）和动作位移场 <em>a_t^P</em> 作为输入，预测下一时刻的粒子状态 *X_{t+1}*。网络通过节点编码器、边编码器、边传播器、节点传播器和节点解码器进行多步消息传递，最终输出预测的粒子位置和属性。训练目标是最小化预测状态与真实状态之间的损失（如均方误差MSE、倒角距离CD或推土机距离EMD）。</p>
<p><strong>4. 基于模型的规划</strong>：受人手运动存在于低维流形的启发，为高效规划设计了低维动作参数化空间（如针对推动任务的平移和接触手指数，针对塑性体重塑任务的<code>FingersPinch</code>、<code>PalmPress</code>、<code>ThumbPinch</code>三种运动基元）。在模型预测控制（MPC）中，从机器人的动作空间中采样动作序列，通过正向运动学映射到共享粒子动作空间，利用学习到的世界模型进行轨迹展开，并使用成本函数（如预测最终状态与目标点云之间的CD或EMD距离）进行评估和优化，选择最优轨迹执行。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的核心创新在于提出了<strong>具身无关的粒子表示法</strong>，将状态和动作统一到一个与具体形态、自由度无关的抽象空间中。这使得模型能够从异构的（模拟机器人+真人）数据中联合学习，并直接部署到具有不同运动学结构的新硬件上，无需运动重定向或收集专家演示。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟和真实硬件平台上进行，研究了三个核心问题（Q1-Q3）。</p>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>任务</strong>：刚性物体推动（目标：将盒子重新定向至目标方向）和可变形物体重塑（目标：将塑性体塑造成目标点云形状）。</li>
<li><strong>模拟环境</strong>：使用SAPIEN（刚性任务）和Rewarped（可变形任务）平台。涉及六种灵巧手：Ability Hand (6-DoF), Allegro Hand (16-DoF), XHand (12-DoF), Leap Hand (16-DoF), Shadow Hand (24-DoF)及其一个变体。</li>
<li><strong>真实世界平台</strong>：UFACTORY XArm 7机械臂配备Ability Hand和XHand，使用四个Intel RealSense相机进行多视角感知。同时收集了真人演示数据。</li>
<li><strong>Baseline</strong>：包括仅在目标具身上训练的模型、仅在人类数据上训练的模型、仅在模拟数据上训练的模型，以及不同混合比例的协同训练模型。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.01177v2/x2.png" alt="扩展趋势"></p>
<blockquote>
<p><strong>图2</strong>：跨具身世界模型学习的扩展趋势。对于每个目标手，使用不同数量的其他手进行训练。虚线表示直接在目标具身上训练的模型性能。结果表明，随着训练具身数量的增加，对未见具身的预测误差下降，方差缩小，体现了“具身扩展定律”。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>跨具身扩展性（Q1）</strong>：如图2所示，随着训练中涉及的具身数量增加，模型对未见具身的预测误差（MSE）普遍下降，且性能方差缩小。当使用5个其他具身数据训练时（零样本），性能通常达到或超过直接在目标具身上训练的模型。这表明多样的跨具身数据可以替代特定目标数据。可变形物体重塑任务中的扩展效应更为明显，推测是因为该任务接触面更大，末端执行器几何形状影响更显著。</li>
<li><strong>协同训练方法（Q2）</strong>：如图5所示，比较了不同模拟与真实数据混合比例的模型在人类held-out数据上的预测误差。仅使用模拟数据误差最高（模拟到真实差距）。仅使用人类数据效果更好。当以适当比例（实验中发现1:1最佳）混合两者时，性能进一步提升，表明模拟数据可以作为人类数据的有效正则化器。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.01177v2/x5.png" alt="训练方法比较"></p>
<blockquote>
<p><strong>图5</strong>：评估连接模拟与真实的训练方法。比较了不同模拟与真实数据混合比例下的模型性能。y轴为对人类交互的预测误差。1:1的混合比例在各项任务中表现最佳。</p>
</blockquote>
<ol start="3">
<li><strong>模型控制有效性（Q3）</strong>：在真实的塑性体重塑任务中，评估了协同训练模型与仅人类数据模型在Ability Hand和XHand上的性能。如表1所示，协同训练模型（人类数据+6种模拟机器人手数据）在倒角距离（CD）和推土机距离（EMD）指标上均优于仅使用人类数据训练的模型。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.01177v2/x4.png" alt="硬件部署"></p>
<blockquote>
<p><strong>图4</strong>：跨具身部署的定性结果。(a) 6-DoF Ability Hand 和 (b) 12-DoF XHand 使用从人类演示中学到的同一粒子空间动力学模型，成功地将可变形黏土塑造成目标形状（“X”、“R”、“T”、“A”）。</p>
</blockquote>
<p><strong>消融与深入分析</strong>：论文还观察到，扩展效益因具身形态而异。较小、几何更紧凑的手（如Ability Hand）因其粒子图连接更密集，能更早获得有竞争力的性能；而较大、空间跨度更广的手（如Shadow Hand）需要更多样的训练具身来弥补其稀疏图结构的信息缺失。这揭示了模型性能对粒子图密度的依赖性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>具身无关的粒子表示框架</strong>，将状态和动作统一抽象为3D粒子和位移场，为跨具身数据学习与策略迁移提供了统一接口。</li>
<li>通过系统性实验实证了跨具身世界模型学习的<strong>“扩展定律”</strong>：增加训练数据的具身多样性，可以持续提升模型对未见具身的泛化能力，甚至达到或超过目标特定训练的性能。</li>
<li>探索并验证了<strong>协同训练</strong>模拟机器人数据与真实人类数据的有效配方（如1:1混合），能够结合两者优势，缩小模拟到真实以及人类到机器人的双重差距，实现对新机器人硬件的有效零样本控制。</li>
</ol>
<p><strong>局限性</strong>：论文指出，基于图神经网络的世界模型性能受到所表示具身的粒子图密度影响。对于几何结构稀疏、空间跨度大的手，需要更多的训练具身多样性来获得良好泛化。这提示当前架构对图结构密度较为敏感。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>开发对图密度不敏感的模型架构是一个有前景的未来方向。</li>
<li>本文验证的“扩展定律”鼓励收集和利用更大规模、更多样化的跨具身数据集来构建更通用的世界模型。</li>
<li>所提出的粒子表示范式为整合其他形态（如非仿人手机械臂、夹具等）的数据提供了可能，有望进一步推动通用机器人智能体的发展。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对跨体现灵巧操作中，不同形态机器人（如人手与机械手）因动作空间和运动学差异导致数据共享与策略转移困难的核心问题，提出环境动态具有体现不变性，并利用世界模型作为统一接口。关键技术是设计体现无关的3D粒子状态表示与粒子位移动作表示，构建基于图的世界模型，在模拟与真实人手数据上训练。实验表明：扩展到更多训练体现能提升对未见体现的泛化能力；模拟与真实数据共同训练优于单独训练；所学模型能有效控制不同自由度的机器人。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01177" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>