<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.14659" target="_blank" rel="noreferrer">2511.14659</a></span>
        <span>作者: Hung, Chia-Yu, Majumder, Navonil, Deng, Haoyuan, Renhang, Liu, Ang, Yankang, Zadeh, Amir, Li, Chuan, Herremans, Dorien, Wang, Ziwei, Poria, Soujanya</span>
        <span>日期: 2025/11/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过在大规模专家收集的跨具身动作轨迹上进行模仿学习，并在下游任务上进行有监督微调，已在多种具身任务上展现出潜力。然而，基于SFT的适应方法严重受限于有限的、人工策划的演示数据质量，限制了模型完全泛化或超越专家数据质量的能力。为了进行更可扩展和校准的后训练，本文探索利用直接偏好优化，通过能够对VLA策略生成的动作质量进行排序的奖励模型来生成偏好数据集。本文的核心思路是：在预训练的VLA主干上集成一个基于流匹配的动作专家以构建NORA-1.5模型，并设计一个结合世界模型目标奖励和地面真值动作偏差奖励的轻量级奖励框架，通过DPO进行后训练，从而提升模型的鲁棒性和任务成功率。</p>
<h2 id="方法详解">方法详解</h2>
<p>NORA-1.5的整体框架建立在预训练的自回归VLA模型NORA之上，通过层间自注意力耦合一个可训练的、基于流匹配的动作专家。训练分为两个主要阶段：首先通过模仿学习联合训练VLA主干和动作专家，随后利用奖励模型构建偏好数据集，通过DPO对模型进行后训练对齐。</p>
<p><img src="https://arxiv.org/html/2511.14659v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：NORA-1.5的训练流程。首先通过模仿学习预训练VLA模型，随后基于奖励模型为生成的动作创建偏好数据集，用于偏好优化。WM代表世界模型引导的目标奖励，GTA代表基于地面真值动作的奖励。</p>
</blockquote>
<p><strong>核心模块一：架构与动作专家</strong>。模型使用NORA作为视觉-语言编码器，处理当前观测和自然语言指令，输出其Transformer各层的键值对。一个独立的动作专家接收这些键值对，直接回归未来N步的动作序列。该专家采用流匹配技术：给定真实动作序列，构造带噪声的序列，专家直接回归从噪声到真实动作的“速度”向量。其网络结构为堆叠的Transformer层，在注意力机制中，将VLA编码器提供的键值对与专家自身计算的键值进行拼接，从而实现条件生成。此设计旨在结合自回归VLA的强推理规划能力和流匹配专家高效、连续的动作生成优势。</p>
<p><strong>核心模块二：奖励模型设计</strong>。为了给VLA后训练提供信号，本文设计了包含两个组件的奖励机制：</p>
<ol>
<li><strong>世界模型引导的目标奖励</strong>：使用一个1.3B参数的动作条件世界模型V-JEPA2-AC。给定当前观测和候选动作序列，世界模型预测执行动作后的未来状态嵌入。奖励通过计算预测状态与目标状态（最终目标或即时子目标）嵌入之间的L1距离负值得到，用于评估动作达成目标的能力。</li>
<li><strong>基于地面真值的动作奖励</strong>：计算候选动作序列与数据集中地面真值动作序列之间的L1距离负值，作为一个稳定的参考信号。<br>总奖励是两者的加权和，其中目标奖励权重为1，动作奖励权重为0.5。这种组合旨在利用目标奖励捕捉多样化的可行轨迹，同时用动作奖励抵消世界模型可能因数据有限而产生的预测噪声。</li>
</ol>
<p><strong>核心模块三：偏好优化</strong>。利用上述奖励函数，对同一观测和指令下VLA模型生成的多个动作序列进行评分和排序，构建“优胜-劣汰”动作对组成的偏好数据集。随后使用直接偏好优化目标对模型进行微调。论文提出了一个针对流匹配专家的DPO-FM损失函数，该损失鼓励模型对优胜动作的预测更准确，对劣汰动作的预测更不准。</p>
<p><strong>创新点</strong>：与现有主要依赖SFT或需要昂贵模拟器rollout的方法相比，本文的创新在于：1) 系统性地研究了流匹配动作专家与自回归VLA主干耦合的架构优势；2) 提出了一种轻量级、可扩展的奖励构建方法，结合了世界模型的前向预测和简单动作偏差启发式；3) 将此类奖励与DPO结合，为VLA模型提供了一个计算受限、数据高效的后训练范式，避免了大规模物理执行或精确仿真的需求。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：模型在模拟基准（SimplerEnv, LIBERO）和真实世界（Galaxea A1机械臂）上进行评估。对比的基线方法包括自回归VLA模型（如SpatialVLA, RT-1, MolmoAct, Emma-X, NORA, OpenVLA）以及扩散/流匹配模型（如π0）。评估指标包括成功率、部分成功率和抓取干扰物次数。</p>
<p><strong>关键实验结果</strong>：<br>在SimplerEnv基准上，NORA-1.5展现出强大性能。零样本NORA-1.5在“Pick Coke Can”和“Move Near”任务上优于所有零样本基线，分别领先4.6%和10.7%。经过微调后，NORA-1.5在“Pick Coke Can”和“Move Near”任务上仍领先于微调后的SpatialVLA 6.8%和0.8%。经过DPO后训练后，模型在Visual Matching协议下的平均成功率进一步提升至82.8%，相比微调版本提升4.9个百分点。</p>
<p><img src="https://arxiv.org/html/2511.14659v1/x2.png" alt="SimplerEnv结果"></p>
<blockquote>
<p><strong>表1</strong>：SimplerEnv上的性能对比。NORA-1.5在多项任务上领先，且DPO后训练（NORA-1.5 (DPO)）带来了进一步的性能提升。</p>
</blockquote>
<p>在LIBERO基准的四个子集上，经过DPO后训练的NORA-1.5模型相比其SFT版本，在Spatial、Object、Goal和Long子集上分别提升了2.2%、1.8%、4.6%和3.0%的成功率，达到了新的最优水平。</p>
<p><img src="https://arxiv.org/html/2511.14659v1/x3.png" alt="LIBERO结果"></p>
<blockquote>
<p><strong>表2</strong>：LIBERO基准上的结果。DPO后训练在所有子任务上均带来了稳定的性能改进。</p>
</blockquote>
<p>在真实机器人Galaxea A1的跨具身评估中，NORA-1.5在“Seen Tasks”上取得了100%的成功率，在“Unseen-Object Seen-Distractor”和“Unseen-Instruction Seen-Distractor”任务组上也表现优异。DPO后训练显著降低了抓取干扰物的错误。</p>
<p><img src="https://arxiv.org/html/2511.14659v1/x4.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>表3</strong>：Galaxea A1真实机器人实验的结果。NORA-1.5 + DPO在成功率上表现最佳，并且显著减少了抓取干扰物的行为。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>消融实验验证了各组件贡献。单独使用世界模型目标奖励或动作奖励进行DPO都能带来提升，但两者结合效果最好。使用最终目标与子目标奖励的对比表明，在LIBERO上子目标奖励更有效，而在SimplerEnv上最终目标奖励更有效，说明最佳奖励设计可能因任务结构而异。</p>
<p><img src="https://arxiv.org/html/2511.14659v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融研究结果。左图显示结合两种奖励的DPO效果最佳；右图显示不同任务上最终目标奖励和子目标奖励的有效性不同。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了NORA-1.5模型，通过耦合流匹配动作专家与自回归VLA主干，在多个模拟和真实基准上实现了先进性能；2) 设计了一个结合世界模型目标奖励和动作偏差启发式的多策略奖励框架，为VLA后训练提供了轻量且鲁棒的信号；3) 系统性地分析了所提架构的协同效应，并证明了基于此类简单奖励的DPO后训练能持续提升VLA模型在仿真和真实环境中的可靠性与性能。</p>
<p><strong>局限性</strong>：论文提到，流匹配专家在低数据场景下可能表现不佳，可能与VLA主干联合训练不充分有关。此外，所使用的动作条件世界模型由于适应数据有限，其预测可能存在噪声。</p>
<p><strong>研究启示</strong>：本工作表明，无需依赖昂贵的模拟器或大量机器人执行，通过轻量级学习奖励模型结合偏好优化，即可实现VLA模型的有效、可扩展后训练。这为开发更适合现实世界部署的、可靠的具身智能体指明了一条经济可行的路径。未来的工作可以探索更强大的世界模型、更精细的奖励设计以及将此范式扩展到更广泛的具身任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型在跨体现和真实世界环境中可靠性与泛化性不足的核心问题，提出了NORA-1.5模型。关键技术包括：在预训练NORA骨干上添加基于流匹配的动作专家以增强架构；开发结合动作条件世界模型和偏离地面真实启发式的奖励模型，并采用直接偏好优化（DPO）进行后训练。实验表明，奖励驱动的后训练显著提升了性能，在模拟和真实机器人基准测试中优于NORA及多个先进VLA模型，实现了模型可靠性的有效改进。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.14659" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>