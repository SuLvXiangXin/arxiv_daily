<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.18316" target="_blank" rel="noreferrer">2510.18316</a></span>
        <span>作者: Li Fei-Fei Team</span>
        <span>日期: 2025-10-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>从大规模、多样的人类示范中进行模仿学习已被证明是训练机器人的有效方法，但收集此类数据成本高昂且耗时。这一挑战在<strong>多步骤双手移动操作</strong>中尤为突出，因为人类必须同时远程操作移动基座和两个高自由度机械臂。现有的X-Gen系列工作已为静态（双手）操作任务开发了自动数据生成框架，通过在仿真中改变场景配置来扩增少量人类示范，从而合成大规模数据集。然而，先前的工作在双手移动操作任务上存在不足，主要有两个原因：1）移动基座引入了<strong>如何放置机器人基座以实现后续操作（可达性）</strong>的问题；2）主动相机引入了<strong>如何定位相机以为视觉运动策略生成数据（可见性）</strong>的问题。</p>
<p>本文针对移动操作数据生成中的<strong>可达性与可见性</strong>这两个具体痛点，提出了将数据生成视为<strong>约束优化问题</strong>的新视角。核心思路是：将数据生成形式化为一个满足硬约束（如可达性）并平衡软约束（如导航时的可见性）的优化问题，从而生成多样且高质量的双手机器人移动操作示范。</p>
<h2 id="方法详解">方法详解</h2>
<p>MoMaGen的整体框架遵循一个受约束的优化流程，旨在从单个源示范生成大量新的示范。其核心思想是，对于每个任务子步骤，在满足一系列硬约束和软约束的前提下，为机器人寻找合适的全身配置（包括基座、躯干、相机和手臂），并规划运动轨迹。</p>
<p><img src="https://arxiv.org/html/2510.18316v1/figs/methodology.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MoMaGen方法流程图。给定一个源示范及其针对每个末端执行器的以物体为中心的子任务标注，MoMaGen首先随机化场景配置，并将末端执行器姿态从源示范转换到新物体的参考系中。对于每个子任务，它尝试采样一个满足可达性和可见性约束的有效基座姿态。找到后，规划基座和躯干轨迹以到达期望的基座和头部相机姿态，同时在导航过程中尝试注视目标物体。到达后，规划手臂轨迹到预抓取姿态，并使用任务空间控制进行回放，最后回缩到一个紧凑的中立姿态。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>问题形式化</strong>：将数据生成定义为公式(1)所示的约束优化问题。<strong>硬约束</strong>必须严格满足，包括：系统动力学 <code>f(st, at)</code>、运动学可行性 <code>G_kin</code>、无碰撞 <code>G_coll</code>、操作期间目标物体可见性 <code>G_vis</code>、接触丰富子任务中末端执行器相对于目标物体的相对姿态保持不变、以及最终任务成功。<strong>软约束</strong>包含在损失函数 <code>L(·)</code> 中，用于优化期望属性，如导航时的可见性和回缩。</li>
<li><strong>针对移动操作的创新约束</strong>：<ul>
<li><strong>可达性作为硬约束</strong>：确保为每个子任务采样的机器人基座姿态，能使所有必需的末端执行器轨迹都在机械臂工作空间内。这解决了物体位置随机化后，单纯回放源示范基座轨迹导致的不可达问题。</li>
<li><strong>操作期间物体可见性作为硬约束</strong>：在开始操作前，必须确保任务相关物体在机器人头部相机视野内且无遮挡，这对训练视觉运动策略至关重要。</li>
<li><strong>导航期间物体可见性作为软约束</strong>：在导航阶段，通过添加可见性成本，使头部相机倾向于朝向目标物体，以提高数据质量。</li>
<li><strong>回缩作为软约束</strong>：操作完成后，机器人将躯干和手臂回缩到紧凑配置，减少其占用空间，使后续导航更安全。</li>
</ul>
</li>
<li><strong>数据生成流程（对应算法1）</strong>：<ul>
<li><strong>源示范标注</strong>：将示范分割为时序子任务，标注目标物体、手持物体、接触前时刻、结束时刻及回缩类型。</li>
<li><strong>迭代生成</strong>：对每个子任务，首先检查当前基座和相机配置是否满足可达性和可见性硬约束。如果满足，则直接进行手臂运动规划并执行任务空间控制回放接触阶段动作。</li>
<li><strong>约束求解</strong>：如果不满足约束，则通过启发式方法采样新的基座姿态 <code>T_base</code> 和相机姿态 <code>T_cam</code>，并利用逆运动学（IK）求解对应的手臂 <code>q_arm</code> 和躯干 <code>q_torso</code> 关节配置，直到找到满足所有硬约束的全身配置。</li>
<li><strong>运动规划</strong>：使用运动规划器（cuRobo）规划基座/躯干轨迹以到达新采样的配置（同时考虑导航可见性软约束），并规划手臂轨迹到达预抓取点。</li>
<li><strong>执行与回缩</strong>：执行任务空间控制完成接触式操作，然后尝试进行回缩。</li>
</ul>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>全身运动考量</strong>：联合优化末端执行器姿态 <code>T_eef</code>、头部相机姿态 <code>T_cam</code> 和基座姿态 <code>T_base</code>，而先前工作主要只关注 <code>T_eef</code>。</li>
<li><strong>可见性保证</strong>：首次明确将操作前物体可见性作为硬约束，并将导航时可见性作为软约束。</li>
<li><strong>工作空间扩展</strong>：通过采样基座姿态和规划跨房间的基座运动，充分利用了移动机器人的机动性。</li>
<li><strong>高效生成</strong>：通过优先使用快速IK检查进行预过滤，以及将机器人配置分解为躯干/手臂子空间进行条件采样，提高了生成效率。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/数据集</strong>：在OmniGibson仿真环境中评估四个家庭任务：<strong>Pick Cup</strong>（取杯子）、<strong>Tidy Table</strong>（整理桌子-长距离移动操作）、<strong>Put Dishes Away</strong>（收拾碗碟-双手非协调运动）、<strong>Clean Frying Pan</strong>（清洁煎锅-双手协调接触式运动）。</li>
<li><strong>实验平台</strong>：OmniGibson。</li>
<li><strong>领域随机化方案</strong>：三个难度递增的级别：<strong>D0</strong>（物体在小范围内随机）、<strong>D1</strong>（物体在支撑面上任意位置和朝向）、<strong>D2</strong>（D1基础上增加操作和导航障碍物）。</li>
<li><strong>Baseline方法</strong>：<strong>SkillMimicGen</strong>（基于运动规划的单手操作生成）和<strong>DexMimicGen</strong>（灵巧双手操作生成），两者均被扩展以回放源示范的基座轨迹。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2510.18316v1/figs/data_diversity_tidy_table.png" alt="数据多样性分析"></p>
<blockquote>
<p><strong>图4</strong>：Tidy Table任务生成数据多样性分析。在相同物体随机化（D0）下，与SkillMimicGen相比，MoMaGen采样了更多样的基座姿态(b)，从而产生了更多样的末端执行器姿态(c)和关节位置(d)。MoMaGen是唯一能在D1随机化下生成数据的方法，多样性进一步大幅增加。</p>
</blockquote>
<ul>
<li><strong>数据多样性</strong>：如图4所示，在更具挑战性的D1随机化下，MoMaGen生成的物体、基座、末端执行器姿态覆盖范围远大于仅能在D0下工作的基线方法，证明了其生成高度多样化数据的能力。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.18316v1/figs/tidy_table_d0_vis_analysis.png" alt="对象可见性分析"></p>
<blockquote>
<p><strong>图5</strong>：MoMaGen及其消融实验的对象可见性分析。x轴是导航过程中目标物体可见的帧百分比，y轴是轨迹数量。得益于硬约束和软约束，MoMaGen（蓝色）显著优于消融实验，生成了大量高可见性的轨迹。</p>
</blockquote>
<ul>
<li><strong>生成成功率</strong>：如表2所示，在D0难度下，MoMaGen平均成功率为63%。对于更复杂的任务（如Clean Frying Pan），MoMaGen因能适应基座运动而表现优于基线。基线方法完全无法处理D1/D2随机化（成功率为0）。</li>
<li><strong>对象可见性</strong>：如表3和图5所示，MoMaGen的硬约束和软可见性约束极其有效，即使在激进的随机化（D1/D2）下，导航期间物体可见率也常超过75%，显著高于基线及移除了可见性约束的消融版本。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.18316v1/figs/training_results.png" alt="训练结果"></p>
<blockquote>
<p><strong>图6</strong>：使用生成数据训练的行为克隆策略在仿真中的评估结果。MoMaGen生成的数据（蓝色）训练出的策略在所有任务和随机化级别上均优于或与基线方法持平，尤其是在更具挑战性的D1设置下优势明显。</p>
</blockquote>
<ul>
<li><strong>策略学习效果</strong>：使用生成的数据训练行为克隆策略，在仿真中评估。如图6所示，由MoMaGen数据训练的策略在所有任务和随机化级别上均优于或与基线策略持平，在D1设置下优势尤其明显。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.18316v1/figs/scaling.png" alt="规模扩展"></p>
<blockquote>
<p><strong>图7</strong>：生成数据规模对策略性能的影响。随着生成数据量的增加，策略成功率单调上升，证明了MoMaGen大规模生成高质量数据的能力及其对策略性能的积极贡献。</p>
</blockquote>
<ul>
<li><strong>数据规模扩展</strong>：如图7所示，随着MoMaGen生成数据量的增加，训练出的策略成功率单调上升，证明了其大规模生成有效数据的能力。</li>
<li><strong>仿真到现实迁移</strong>：论文指出，使用MoMaGen生成的数据训练的策略，仅需少量（40个）真实世界示范进行微调，即可成功部署在真实的机器人硬件上。</li>
<li><strong>跨机器人平台生成</strong>：MoMaGen通过任务空间规划与回放，成功实现了在不同机器人平台（从Galexea R1到TIAGo）间的示范数据生成。</li>
</ul>
<p><strong>消融实验总结</strong>：<br>消融实验（移除硬/软可见性约束）表明：1) <strong>硬可见性约束</strong>对于在复杂任务中定位机器人躯干到便于下游操作的配置至关重要，有助于提高生成成功率（表2）。2) <strong>软可见性约束</strong>对于在导航期间保持高物体可见率必不可少（图5）。两者结合才能最优地服务于视觉运动策略的训练。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出统一框架</strong>：将自动化示范数据生成形式化为一个受硬约束和软约束的优化问题，为现有X-Gen方法提供了一个原则性的统一解释，并为未来方法开发奠定了基础。</li>
<li><strong>解决移动操作关键挑战</strong>：针对双手移动操作，创新性地引入了<strong>可达性作为硬约束</strong>，以及<strong>操作可见性作为硬约束</strong>、<strong>导航可见性作为软约束</strong>，确保了生成数据的可执行性和对视觉策略训练的高质量。</li>
<li><strong>实现高效多样化数据生成</strong>：所提出的MoMaGen系统能够从<strong>单个源示范</strong>出发，在高度随机化的场景中（包括障碍物），高效生成大规模、多样化的双手移动操作数据，并显著提升了后续模仿学习策略的性能和泛化能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，方法依赖于仿真进行验证，且数据生成过程涉及运动规划和约束求解，<strong>计算成本较高</strong>。此外，方法需要<strong>对源示范进行子任务标注</strong>（尽管每个任务只需标注一个示范）。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>约束设计范式</strong>：MoMaGen展示了如何通过精心设计领域特定的硬约束和软约束来指导高质量数据生成，这一范式可推广至其他复杂机器人任务。</li>
<li><strong>减少人工标注</strong>：未来工作可以探索如何自动化或减少对源示范的标注需求。</li>
<li><strong>优化生成效率</strong>：进一步优化约束求解和运动规划的效率，以实现更快速的大规模数据生成。</li>
<li><strong>跨任务泛化</strong>：探索如何将学习到的约束或采样策略迁移到新的、未见过的任务中。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MoMaGen框架，旨在解决多步骤双手移动操作任务中，大规模演示数据收集成本高昂的问题。核心挑战在于移动基座带来的可达性约束与主动相机带来的可见性约束。方法将数据生成建模为约束优化问题，同时满足硬约束（如可达性）与软约束（如导航可见性）。实验表明，MoMaGen在四个任务上生成的数据集多样性显著优于先前方法，仅需一个源演示即可训练出成功的模仿学习策略，并经40个真实演示微调后成功部署于真实机器人硬件。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.18316" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>