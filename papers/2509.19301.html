<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Residual Off-Policy RL for Finetuning Behavior Cloning Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Residual Off-Policy RL for Finetuning Behavior Cloning Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19301" target="_blank" rel="noreferrer">2509.19301</a></span>
        <span>作者: Ankile, Lars, Jiang, Zhenyu, Duan, Rocky, Shi, Guanya, Abbeel, Pieter, Nagabandi, Anusha</span>
        <span>日期: 2025/09/23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人技能学习主要有两大范式。行为克隆（BC）通过模仿人类演示数据训练策略，已在视觉运动控制中取得显著成果，但其性能受限于演示质量、高昂的数据收集成本以及性能随数据量增加而饱和的问题。另一方面，强化学习（RL）通过自主环境交互进行学习，潜力巨大，但直接应用于现实世界机器人面临样本效率低、安全性问题以及稀疏奖励长视野任务学习困难等挑战，对于高自由度（DoF）系统尤为突出。</p>
<p>本文针对如何将BC的策略先验与RL的在线优化能力相结合这一痛点，提出了一种新的视角：不直接微调庞大的BC策略网络，而是将其视为冻结的“黑盒”基础策略，通过样本高效的离策略RL学习一个轻量级的、每步执行的残差修正项。其核心思路是：首先通过BC训练一个基础策略，然后冻结该策略，并利用离策略RL学习一个以当前观测和基础策略动作为输入的残差策略，最终动作为两者之和，以此在保持BC策略稳定性的同时，利用RL提升性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的离策略残差微调（ResFiT）方法是一个两阶段流程。第一阶段，利用离线演示数据集通过行为克隆训练一个基础策略 π_base，该策略采用动作分块（action chunking）技术，每次观测预测未来k步的动作序列。第二阶段，冻结基础策略，初始化一个残差策略 π_res 和评价者网络（critic），通过在线交互收集数据，并利用离策略RL算法同时优化残差策略和评价者。</p>
<p><img src="https://arxiv.org/html/2509.19301v2/figs/pipeline.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：离策略残差微调（ResFiT）方法整体框架。左侧为第一阶段：利用演示数据集通过行为克隆训练基础策略。右侧为第二阶段：冻结基础策略，通过在线交互学习一个残差策略来修正基础动作。RL阶段利用了演示数据和在线交互数据。</p>
</blockquote>
<p>核心模块包括基础策略和残差RL框架。基础策略 π_base 通过最大化演示数据中动作序列的对数似然进行训练。在残差RL框架中，智能体在状态 s_t 下执行的动作 a_t 由基础动作 a_t^base = π_base(s_t) 和残差动作 a_t^res = π_θ(s_t, a_t^base) 相加得到，即 a_t = a_t^base + a_t^res。评价者网络 Q_φ 评估的是完整动作 a_t 的价值，而非单独的基础或残差动作。</p>
<p>与直接优化整个策略或现有残差RL方法相比，本文的创新点具体体现在：1) <strong>策略解耦</strong>：将残差策略设计为以基础动作为输入的独立模块，使其与基础策略的具体参数化方式（如分块大小、网络架构）无关，可应用于大规模的动作分块或扩散策略。2) <strong>高效的离策略配方</strong>：采用了一系列精心设计的技术来提升样本效率和稳定性，包括：使用大于1的更新数据比（UTD）、n步回报（n=3）、在评价者网络中使用层归一化（LayerNorm）以缓解价值高估、采用随机集成双Q学习（REDQ）、延迟演员更新、目标策略平滑以及从离线演示和在线缓冲区对称采样（各50%）组成训练批次。</p>
<p>算法流程（对应论文算法1）包括缓冲区预热、环境交互、以及基于批次数据的策略和评价者更新。评价者通过最小化均方贝尔曼误差（MSBE）损失进行训练，而残差策略通过最大化评价者对完整动作的估计值进行训练。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真和现实世界两个平台进行。仿真实验使用了来自Robomimic和DexMimicGen的任务，涵盖单臂操作（Can, Square）和双手协调任务（BoxCleanup, CanSort, Coffee），机器人包括Franka和GR1人形机器人，动作空间维度从7到24不等，仅使用图像和本体感知作为观测，奖励为稀疏二元信号。对比的基线方法包括：1) <strong>Tuned RLPD</strong>：不使用基础策略和残差，但采用了与ResFiT相同的离策略RL设计决策的优化版RLPD算法；2) <strong>IBRL</strong>：使用预训练BC策略提出动作并引导价值估计；3) <strong>Filtered BC</strong>：通过迭代收集成功轨迹并持续进行BC训练来微调基础策略；4) <strong>PPO残差RL</strong>：使用近端策略优化（PPO）进行残差学习。</p>
<p><img src="https://arxiv.org/html/2509.19301v2/x1.png" alt="仿真任务"></p>
<blockquote>
<p><strong>图2</strong>：仿真实验任务图示，包括单臂操作任务（Can, Square）以及双手协调任务（BoxCleanup, CanSort, Coffee）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19301v2/x2.png" alt="PPO与离策略对比"></p>
<blockquote>
<p><strong>图3</strong>：在BoxCleanup任务上，比较使用PPO的残差RL与本文的离策略ResFiT的样本效率。ResFiT（约20万步收敛）相比PPO残差RL（约4000万步收敛）实现了约200倍的样本效率提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19301v2/x3.png" alt="主要结果"></p>
<blockquote>
<p><strong>图4</strong>：不同方法在仿真任务上的成功率。ResFiT在所有任务上均能收敛到接近完美的策略性能。对于简单任务（Can），多数方法表现良好，ResFiT收敛更快（~7.5万步）。对于复杂的长视野、高自由度任务（如BoxCleanup, Coffee），只有ResFiT能高效地达到高性能，而其他基线或消融版本要么失败，要么收敛缓慢。</p>
</blockquote>
<p>关键实验结果如下：ResFiT在所有仿真任务上均达到了最先进的性能。在简单任务Can上，ResFiT约7.5万步收敛，快于其他方法（~15万步）。在更具挑战性的Square及双手任务上，ResFiT显著优于所有基线。例如，在Coffee任务上，只有ResFiT和使用了动作分块的变体能成功学习。Filtered BC基线虽然稳定，但性能提升有限，仅在初始性能很低时有改善，随后迅速饱和。</p>
<p>消融实验验证了关键设计决策的作用：<br><img src="https://arxiv.org/html/2509.19301v2/x4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：UTD比率（左）和n步回报（右）对BoxCleanup和Coffee任务性能的影响。结果表明，UTD&gt;1和n=3步回报对任务成功至关重要。</p>
</blockquote>
<p>此外，消融实验还表明：在在线RL阶段继续使用演示数据（对称采样）对性能有积极影响；在评价者网络中使用层归一化对于防止价值高估和策略崩溃至关重要。</p>
<p>现实世界实验在Vega人形机器人（双7-DoF手臂，双6-DoF灵巧手）上进行，完成了WoollyBallPnP和PackageHandover两个视觉引导的双手操作任务。ResFiT仅使用稀疏二元奖励，成功在现实世界中完成了RL训练。通过盲测A/B测试评估，微调后的策略相比预训练BC基础策略性能有显著提升。</p>
<p><img src="https://arxiv.org/html/2509.19301v2/x5.png" alt="现实世界任务"></p>
<blockquote>
<p><strong>图6</strong>：现实世界实验平台Vega人形机器人及任务设置（WoollyBallPnP和PackageHandover）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19301v2/x6.png" alt="现实世界结果"></p>
<blockquote>
<p><strong>图7</strong>：现实世界任务中，ResFiT微调后的策略与初始BC策略在盲测A/B测试中的成功率对比。ResFiT策略在两个任务上均取得了显著更高的成功率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>ResFiT</strong>，一个通用的、与基础策略参数化无关的残差微调框架，能够将现代动作分块BC策略与样本高效的离策略RL相结合。2) <strong>首次在具有五指灵巧手的人形机器人上，完全在现实世界中成功演示了RL训练</strong>，仅需稀疏二元奖励。3) 设计并验证了一套高效的离策略RL配方（包括UTD&gt;1、n步回报、层归一化、对称采样等），使其能够处理高自由度、长视野、视觉输入的稀疏奖励任务。</p>
<p>论文提到的局限性包括方法主要在操作任务上得到验证，其通用性需在更广泛领域（如导航）进一步测试。此外，虽然样本效率大幅提升，但现实世界的训练时间（数小时）仍是一个实际考量因素。</p>
<p>本文的启示在于：<strong>残差学习</strong>是结合大型预训练模型与在线学习的有效且实用的范式，它降低了优化难度并提升了安全性。同时，<strong>精心设计的离策略RL</strong>能够极大提升样本效率，使其在现实世界复杂机器人系统中的应用成为可能。这项工作为在现实世界中安全、高效地部署RL提供了一条切实可行的路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对行为克隆（BC）策略依赖人类演示、性能易饱和，以及强化学习（RL）在真实世界机器人应用中样本效率低、安全风险高的问题，提出一种残差离策略RL框架。该方法以BC策略为黑盒基础，通过样本高效的离策略RL学习轻量级每步残差校正，避免直接优化复杂策略。实验证明，该方法仅需稀疏二进制奖励信号，即可有效提升高自由度系统的操作策略，在仿真和真实世界中均实现先进性能，并首次成功应用于具有灵巧手的人形机器人真实RL训练。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19301" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>