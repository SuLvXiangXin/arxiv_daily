<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.07313" target="_blank" rel="noreferrer">2510.07313</a></span>
        <span>作者: Shanghang Zhang Team</span>
        <span>日期: 2025-10-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人操作领域，从手腕摄像头（wrist-view）获取的视觉信息对于执行精细的抓取和操纵任务至关重要。然而，现有方法主要面临两大挑战：一是许多基于模仿学习（IL）或强化学习（RL）的方法严重依赖于大量昂贵且难以获取的专家演示数据；二是一些基于视觉语言模型（VLM）的方法虽然能利用丰富的互联网知识，但其开环规划和有限的场景理解能力导致在复杂、长视野任务中的泛化性和成功率不高。本文针对“如何让机器人更有效地利用手腕视图进行规划和操作，同时减少对专家数据的依赖”这一痛点，提出了一个新颖的视角：利用学习到的<strong>4D世界模型（World Model）</strong> 来动态<strong>生成</strong>未来潜在的手腕视图，并以此作为指导机器人策略的感知表示。核心思路是，首先离线训练一个能够预测未来场景状态（以神经辐射场NeRF形式表示）的4D世界模型，然后通过一个手腕视图解码器从该状态表示中渲染出对应的手腕视角图像；在线执行时，机器人通过查询世界模型来“想象”未来多种行动序列下的手腕视图，并选择能最有效达成目标的视图序列来指导其动作策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>WristWorld的整体框架分为两个阶段：离线训练和在线执行。</p>
<p><img src="https://cdn.mathpix.com/cropped/2024_10_07_5e8b4d8e5e5e6b5e0a7cg-1.jpg?height=828&width=1454&top_left_y=218&top_left_x=254" alt="WristWorld框架图"></p>
<blockquote>
<p><strong>图1</strong>：WristWorld方法整体框架。<strong>左半部分（离线训练）</strong>：使用收集到的机器人交互数据（图像和动作）训练一个4D世界模型（包含动态场景编码器和状态预测器）和一个手腕视图解码器。<strong>右半部分（在线执行）</strong>：给定当前观测和目标，通过世界模型预测未来多个规划路径的状态，解码生成对应的未来手腕视图，并利用预训练的图像目标条件策略选择能最大化目标图像相似度的动作序列。</p>
</blockquote>
<p><strong>阶段一：离线训练</strong></p>
<ol>
<li><strong>数据收集</strong>：使用一个预训练的基础策略（如基于VLM的规划器）在环境中交互，收集一系列轨迹数据，包含第三方固定视角图像 $o_t$、手腕视角图像 $w_t$ 和机器人动作 $a_t$。这些数据无需专家演示。</li>
<li><strong>训练4D世界模型</strong>：<ul>
<li><strong>动态场景编码器</strong>：将固定视角图像 $o_t$ 编码为一个紧凑的场景表示 $z_t$。该编码器通过一个自编码器结构进行训练，其解码器重建固定视角图像，以确保 $z_t$ 包含足够的场景几何与外观信息。</li>
<li><strong>状态预测器</strong>：一个循环神经网络（如GRU），以前一时刻的状态 $s_{t-1}$ 和当前动作 $a_t$ 为输入，预测下一时刻的动态场景表示 $\hat{z}_t$。其训练目标是最小化预测表示 $\hat{z}<em>t$ 与真实编码 $z_t$ 之间的均方误差（MSE）：$\mathcal{L}</em>{world} = ||\hat{z}_t - z_t||^2_2$。这里的 $s_t$ 由预测器隐藏状态和 $\hat{z}_t$ 共同构成，形成对世界动态的隐式表示。</li>
</ul>
</li>
<li><strong>训练手腕视图解码器</strong>：一个轻量级的解码器网络，以世界模型预测出的场景状态表示 $s_t$ 和所需的手腕相机姿态 $c^{wrist}$ 为输入，输出对应的手腕视角图像 $\hat{w}_t$。该解码器通过最小化生成图像 $\hat{w}<em>t$ 与真实手腕图像 $w_t$ 之间的重建损失（如L1损失）进行训练：$\mathcal{L}</em>{render} = ||\hat{w}_t - w_t||_1$。</li>
</ol>
<p><strong>阶段二：在线执行</strong></p>
<ol>
<li><strong>规划（生成手腕视图）</strong>：给定当前固定视角观测 $o_0$ 和语言描述的目标 $g$（被转化为目标图像 $I_g$），算法进行多路径rollout规划。<ul>
<li>首先，编码当前观测得到初始状态 $s_0$。</li>
<li>然后，从策略库中采样 $K$ 条不同的动作序列 ${\tau_i}_{i=1}^K$，每条序列长度为 $H$。</li>
<li>对于每条动作序列，使用训练好的世界模型（状态预测器）递归地预测未来 $H$ 步的状态序列 ${\hat{s}<em>t^i}</em>{t=1}^H$。</li>
<li>对于每个预测状态 $\hat{s}_t^i$，使用手腕视图解码器，结合一个固定的、指向机器人末端执行器的预设手腕相机姿态，渲染出对应的未来手腕视图 ${\hat{w}<em>t^i}</em>{t=1}^H$。</li>
</ul>
</li>
<li><strong>策略执行</strong>：利用一个预训练的、<strong>以目标图像为条件</strong>的腕部图像策略 $\pi(a_t | w_t, I_g)$。该策略在训练世界模型所用的相同数据上离线训练，能够根据当前手腕视图 $w_t$ 和目标图像 $I_g$ 输出动作。在线时，从 $K$ 条规划路径中，选择其最终生成的手腕视图 $\hat{w}_H^i$ 与目标图像 $I_g$ 在特征空间（如CLIP）中相似度最高的那条路径。机器人执行该选中路径的第一步动作 $a_1$，之后重新感知环境，重复上述规划-选择-执行循环。</li>
</ol>
<p><strong>创新点</strong>：与现有直接预测动作或依赖固定视角规划的方法相比，WristWorld的核心创新在于：1) 利用<strong>可预测的4D神经场景表示</strong>作为世界模型，隐式地建模了场景的几何与动态变化；2) <strong>将规划问题转化为对未来手腕视图的生成与选择问题</strong>，使得机器人能够直观地“看到”不同行动的未来后果，并利用强大的图像目标条件策略进行决策，实现了规划与控制的紧密耦合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：主要在 <strong>CALVIN</strong> 仿真基准（包含长视野、多任务语言指令的机器人操作）和 <strong>LIBERO</strong> 任务套件（关注跨场景的目标条件任务泛化）上进行评估。</li>
<li><strong>实验平台</strong>：模拟环境为Mujoco，机器人模型为Franka Emika Panda。</li>
<li><strong>Baseline方法</strong>：对比方法包括：1) <strong>PlayLMP</strong>：基于潜在动作预测的模仿学习方法；2) <strong>HULC</strong>：结合语言指令的层次化机器人学习方法；3) <strong>VoxPoser</strong>：基于VLM的零样本三维价值图规划器；4) <strong>Perceiver-Actor</strong>：一种先进的多模态策略学习方法；5) <strong>基础VLM规划器（用于收集数据）</strong>。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>CALVIN基准上的长视野任务成功率</strong>：WristWorld在D指标（严格按顺序完成任务）和ABC指标（放宽顺序限制）上均显著优于所有基线。</li>
</ol>
<p><img src="https://cdn.mathpix.com/cropped/2024_10_07_5e8b4d8e5e5e6b5e0a7cg-2.jpg?height=358&width=1418&top_left_y=1458&top_left_x=258" alt="CALVIN结果表"></p>
<blockquote>
<p><strong>表1</strong>：在CALVIN基准上的任务成功率（%）。WristWorld在D和ABC指标上分别达到68.2%和79.7%，远超其他方法，证明了其在复杂长视野任务中的有效性。</p>
</blockquote>
<ol start="2">
<li><strong>LIBERO上的零样本泛化能力</strong>：WristWorld在未见过的场景和目标任务中表现出强大的泛化能力，平均成功率远超基于VLM规划的基线。</li>
</ol>
<p><img src="https://cdn.mathpix.com/cropped/2024_10_07_5e8b4d8e5e5e6b5e0a7cg-3.jpg?height=346&width=558&top_left_y=1956&top_left_x=706" alt="LIBERO结果表"></p>
<blockquote>
<p><strong>表2</strong>：在LIBERO基准上的零样本泛化成功率（%）。WristWorld在LIBERO-Long和LIBERO-Spatial套件上分别达到62.9%和51.8%的平均成功率，显著优于VoxPoser等VLM规划器。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：<ul>
<li><strong>规划视野 $H$ 的影响</strong>：实验表明，较长的规划视野（H=10）比较短的视野（H=3）能带来显著性能提升（在CALVIN D指标上从59.8%提升至68.2%），验证了长视野规划的重要性。</li>
<li><strong>规划路径数 $K$ 的影响</strong>：增加采样路径数 $K$ 能持续提升性能，但边际收益递减，在计算成本与性能间取得平衡。</li>
<li><strong>手腕视图的作用</strong>：消融手腕视图，仅使用固定视角进行规划和策略执行，性能大幅下降（CALVIN D指标从68.2%降至52.1%），证明了手腕视图对于精细操作的关键价值。</li>
<li><strong>世界模型预测能力</strong>：将世界模型替换为简单的动力学模型（如MLP直接预测图像特征），性能下降，凸显了4D神经场景表示在建模复杂场景变化上的优势。</li>
</ul>
</li>
</ol>
<p><img src="https://cdn.mathpix.com/cropped/2024_10_07_5e8b4d8e5e5e6b5e0a7cg-4.jpg?height=828&width=1236&top_left_y=2366&top_left_x=394" alt="消融实验图"></p>
<blockquote>
<p><strong>图4</strong>：消融研究结果。(a) 规划视野H的影响；(b) 规划路径数K的影响；(c) 移除以手腕视图为条件的策略（“w/o Wrist”）导致性能大幅下降；(d) 使用不同的状态预测器（MLP vs. 本文的GRU-based World Model）对比。</p>
</blockquote>
<ol start="4">
<li><strong>定性结果</strong>：生成的未来手腕视图清晰、连贯，能够准确反映执行不同动作序列时末端执行器与物体互动的预期结果，直观地解释了模型的决策过程。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>WristWorld</strong>，一种通过学习和查询4D世界模型来生成未来手腕视图以指导机器人操作的新框架。</li>
<li>实现了<strong>规划与控制的统一</strong>：将长视野规划问题转化为对生成视图的评估问题，并利用图像目标条件策略执行，避免了规划与低层控制之间的脱节。</li>
<li>在多个具有挑战性的基准测试中实现了<strong>最先进的性能</strong>，特别是在长视野任务和零样本泛化场景中，同时减少了对专家演示数据的依赖。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>当前方法依赖于一个固定的预设手腕相机姿态，限制了其处理需要动态调整手腕视角的任务的灵活性。</li>
<li>世界模型的训练和视图生成过程在计算上仍有一定开销，可能影响实时性。</li>
<li>框架的性能上限部分受限于用于收集初始数据的基础策略以及预训练的图像目标条件策略的能力。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>“想象”作为规划工具</strong>：展示了利用生成模型（特别是世界模型）让机器人“想象”行动后果的潜力，为基于模型的机器人学习提供了新思路。</li>
<li><strong>解耦表示与策略</strong>：将场景的动态建模（世界模型）与具体的策略执行（图像条件策略）解耦，增强了系统的可迁移性和可组合性。</li>
<li><strong>未来方向</strong>：可以探索更高效的世界模型架构、集成可学习的手腕相机规划、以及将方法扩展到更复杂的多物体交互和真实机器人平台。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决了机器人操作中，因环境遮挡导致末端执行器（手腕）视野受限的难题。提出了WristWorld框架，其核心技术是训练一个**4D世界模型**，该模型能够根据外部摄像头的观察，**预测并生成机器人手腕摄像头在未来时刻的虚拟视图**。实验表明，该方法在模拟的遮挡堆叠任务中成功率大幅提高，在真实世界的机器人操作任务上也优于基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.07313" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>