<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.00833" target="_blank" rel="noreferrer">2507.00833</a></span>
        <span>作者: Chenjia Bai Team</span>
        <span>日期: 2025-07-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域的数据集和仿真基准主要面向单臂机器人平台。对于配备双臂和灵巧手的仿人机器人，高质量的仿真任务和演示数据尤为缺乏。双臂灵巧操作本身更为复杂，需要协调的手臂运动和手部操作，使得自主数据收集极具挑战性。现有的数据生成流程主要依赖于虚拟现实（VR）遥操作或外骨骼设备，这需要部署大量实物、熟练的操作人员，难以覆盖多样化的任务和场景。现有的仿真基准如Aloha、PerAct2等主要使用双臂或轮式机器人，任务多样性有限且通常不涉及灵巧手；而HumanoidBench等虽然使用仿人机器人，但依赖VR或强化学习策略收集数据，成本高昂。</p>
<p>本文针对仿人机器人双臂灵巧操作数据稀缺这一核心痛点，提出了一种新的自动化数据生成视角。核心思路是利用大语言模型（LLM）的推理能力，结合预先定义的灵巧手原子操作和物体空间标注，自动生成描述手臂运动的空间约束链，进而通过轨迹优化求解动作并收集演示轨迹，从而实现高效、可扩展的自动化数据生成。</p>
<h2 id="方法详解">方法详解</h2>
<p>HumanoidGen是一个面向仿人机器人双臂灵巧操作的自动化场景生成、演示收集与数据泛化框架，旨在为多样化场景提供高质量演示数据以促进数据扩展和策略学习。</p>
<p><img src="https://arxiv.org/html/2507.00833v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：HumanoidGen整体框架概览。包括空间标注、场景生成、约束生成、MCTS增强推理、数据收集、场景扩展和策略评估等环节。</p>
</blockquote>
<p>整体流程如下：首先，对资产（物体）和灵巧手进行细致的空间信息标注。接着，LLM规划器根据任务描述生成场景配置代码。然后，基于生成的场景和预定义的手部原子操作，LLM进一步生成用于数据收集的空间约束链规划代码。对于长视野任务或标注不足的情况，采用蒙特卡洛树搜索（MCTS）增强LLM的推理能力。最后，执行规划并在扩展的场景中收集演示数据，用于构建基准和策略评估。</p>
<p><strong>核心模块1：空间标注与场景生成</strong><br>空间标注是LLM进行可靠规划的基础，分为三类。</p>
<p><img src="https://arxiv.org/html/2507.00833v2/x2.png" alt="空间标注与原子操作"></p>
<blockquote>
<p><strong>图2</strong>：资产和手的空间标注（关键点与关键轴）以及手的原子操作（抓握、捏取、按压）。</p>
</blockquote>
<ol>
<li><strong>手部原子操作标注</strong>：为每个原子操作（如抓握、捏取）在手部定义关键点和关键轴。关键轴分为三类：<em>接近轴</em>（手部接近预操作姿势的方向）、<em>附着轴</em>（操作执行期间手指运动的方向，例如“捏取”时从拇指尖到食指尖）和<em>平行轴</em>（垂直于前两者，平行于手掌平面，通常指定操作物体时的旋转轴）。</li>
<li><strong>资产固有信息标注</strong>：标注资产实现其功能所依赖的点与轴（例如，杯子的倾倒和储存功能对应不同的点和轴）。</li>
<li><strong>资产操作标注</strong>：定义资产上可用于各种原子操作交互的点与轴。这种关系是多对多的：同一资产可支持不同操作（如杯子可抓握也可捏取）；同一操作在同一资产上可使用不同关键点；同一关键点可支持多种操作。</li>
</ol>
<p>基于这些标注，场景信息被抽象为物体和手局部坐标系中的一组关键点和关键轴，为LLM理解空间关系、定义操作约束奠定了基础。随后，LLM规划器根据资产库信息、场景细节和任务要求，生成任务设置代码，确定资产类别、数量、摆放位姿等，并生成任务成功标准。</p>
<p><strong>核心模块2：LLM任务规划</strong><br>该方法旨在生成可执行的约束链脚本，与仅用于高层任务规划的常规方法不同，它集成了任务分解、关系动作约束和碰撞避免。</p>
<p><img src="https://arxiv.org/html/2507.00833v2/x3.png" alt="任务规划示例"></p>
<blockquote>
<p><strong>图3</strong>：“方块存储”任务的生成规划示例。LLM将任务分解为步骤序列，每个步骤用原子操作及其标注表示，并在规划中主动进行碰撞避免（例如步骤6-7的避障路径，以及底部对紧凑工作空间中自由运动的碰撞处理）。</p>
</blockquote>
<ul>
<li><strong>任务分解</strong>：LLM将长视野任务分解为动作序列𝒮 = {S₁, S₂, …, Sₙ}，每一步Sᵢ = (Sᵢˡ, Sᵢʳ)包含左右手部分，可以是手部操作Aᵢ或手臂运动Mᵢ。手臂运动Mᵢ被定义为基于约束的优化问题Mᵢ = {f(Cᵢᵍᵒᵃˡ), f(Cᵢᵖᵃᵗʰ)}，其中Cᵢᵍᵒᵃˡ和Cᵢᵖᵃᵗʰ分别是目标位姿约束和路径约束，由LLM根据上下文推断，f(·)是约束求解器。</li>
<li><strong>关系动作约束</strong>：为系统描述灵巧操作所需的空间关系，引入了动态动作坐标系集合ℱ_act，表示每只手的情境化动作空间。每个坐标系F由关键点pᵢ和轴vⱼ组成，其组成随操作状态动态演变（例如，刚性地抓取物体后，手的动作坐标系会扩展到包含物体坐标系）。每个约束c指定了ℱ_act中的元素与全局坐标系集（包含物体、手和世界坐标系）中元素之间的几何关系（如点重合、轴平行或正交）。</li>
<li><strong>碰撞避免</strong>：提出两种解决方案。一是<strong>主动碰撞避免</strong>：LLM在生成原子操作脚本时主动纳入避障行为。二是<strong>动态碰撞管理</strong>：LLM通过动态管理物体忽略列表来实现接触式操作（例如，拉抽屉时忽略手与抽屉的接触，拉出后再恢复碰撞检测）。</li>
</ul>
<p><strong>核心模块3：MCTS增强推理</strong><br>为解决长视野任务或标注不足时LLM推理失败的问题，引入了基于树搜索的推理增强方法，并提出了一种新颖的“分割-截断-合并-恢复”（STCR）机制来从LLM输出中抽象出规划搜索树。</p>
<ul>
<li><strong>STCR机制</strong>：1) <strong>分割</strong>：将规划代码按执行粒度分割成多个步骤。2) <strong>截断</strong>：顺序执行步骤，在发生错误（代码格式或动作执行失败）处截断，保留有效步骤序列𝒮_remain。3) <strong>合并</strong>：将𝒮_remain中意图一致的操作（如包含多个移动步骤的抓取动作）合并为新的执行单元S‘，每个S’代表任务树中的一个可执行分支。4) <strong>恢复</strong>：为新创建的树节点存储已执行序列的代码、错误代码和场景状态信息，供后续扩展时使用。</li>
<li><strong>基于MCTS的交互式任务规划</strong>：在STCR构建的树基础上，采用包含选择、扩展和回传步骤的MCTS算法进行搜索，并在扩展过程中集成了模拟步骤以提供节点的执行结果。</li>
<li><strong>场景扩展</strong>：为了增强数据多样性，执行房间级场景扩展，通过计算坐标变换矩阵将桌级任务场景生成的演示脚本泛化到新场景中，利用RoboCasa的丰富资产可泛化至超过120个场景。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了新构建的基准<strong>HGen-Bench</strong>，包含20个难度各异的双臂灵巧操作任务，机器人平台为配备Inspire灵巧手的Unitree H1-2仿人机器人，仿真引擎为SAPIEN。提供了六路相机视角（手腕、第一人称、第三人称）的RGB和深度图像作为观测。</p>
<p><img src="https://arxiv.org/html/2507.00833v2/x4.png" alt="HGen-Bench任务示例"></p>
<blockquote>
<p><strong>图4</strong>：HGen-Bench包含的各种难度不同的灵巧双臂操作任务示例，部署在家庭场景中。</p>
</blockquote>
<p><strong>实验1：数据生成与执行能力评估</strong><br>将20个任务按使用手臂数量、任务视野长度和碰撞场景复杂度分为4组，与基准方法RobotTwin（为其添加灵巧手和额外标注以支持原子操作，并禁用主动动态碰撞管理以反映其默认设置）进行对比，使用DeepSeek-R1生成并执行演示脚本，比较最终成功率。</p>
<p><img src="https://arxiv.org/html/2507.00833v2/x5.png" alt="数据生成与执行结果"></p>
<blockquote>
<p><strong>图5</strong>：演示生成与执行能力实验结果。HumanoidGen在所有类型的灵巧操作任务上均表现优异，平均成功率超过50%，大部分任务类型成功率高于75%，且在长视野和复杂碰撞任务上显著优于RobotTwin。</p>
</blockquote>
<p>关键结果：HumanoidGen在所有任务上均表现优异，**平均成功率超过50%<strong>。除了一些复杂度较高的双臂长视野任务，大多数任务类型的成功率</strong>高于75%**。相比之下，RobotTwin在单臂和双臂短视野任务上表现相当，但在长视野和复杂碰撞任务（如单臂“关盒子”、双臂“方块堆叠”易与难）上不如HumanoidGen。分析表明，HumanoidGen的动态碰撞管理能力使其能灵活处理碰撞场景。</p>
<p><strong>实验2：MCTS增强推理的有效性评估</strong><br>在HGen-Bench的8个长视野任务上评估MCTS的效果，比较仅使用LLM与结合MCTS的成功率。</p>
<p><img src="https://arxiv.org/html/2507.00833v2/x6.png" alt="MCTS增强效果"></p>
<blockquote>
<p><strong>图6</strong>：MCTS增强推理在长视野任务上的有效性评估。结合MCTS后，任务成功率从37.5%提升至62.5%。</p>
</blockquote>
<p>关键结果：仅使用LLM时，长视野任务的**成功率为37.5%<strong>。引入MCTS后，</strong>成功率提升至62.5%**，证明了MCTS在增强LLM长视野推理能力方面的有效性。</p>
<p><strong>实验3：生成数据用于策略训练的有效性评估</strong><br>使用生成的数据集训练2D扩散策略和3D扩散策略，并评估其在未见过的测试场景中的表现。数据集规模从2k、5k扩展到10k条轨迹。</p>
<p><img src="https://arxiv.org/html/2507.00833v2/x7.png" alt="策略性能随数据量扩展"></p>
<blockquote>
<p><strong>图7</strong>：2D和3D扩散策略性能随生成数据量增加的变化。两种策略的性能都随着数据量的增加而持续提升。</p>
</blockquote>
<p>关键结果：无论是2D还是3D扩散策略，其性能都随着生成数据量的增加而<strong>持续提升</strong>，证明了生成数据的可扩展性和有效性。</p>
<p><strong>其他实验结果</strong><br>论文还展示了丰富的定性结果、消融实验和对比分析。</p>
<p><img src="https://arxiv.org/html/2507.00833v2/x8.png" alt="任务执行序列示例"></p>
<blockquote>
<p><strong>图8</strong>：“手部交接与存储”任务的成功执行序列可视化，展示了双臂协调和灵巧操作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.00833v2/x9.png" alt="碰撞管理示例"></p>
<blockquote>
<p><strong>图9</strong>：动态碰撞管理示例。在“拉开抽屉”步骤中，抽屉被加入忽略列表以允许接触；在后续“拿取方块”步骤中，抽屉被移出忽略列表以实现避障。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.00833v2/x10.png" alt="场景扩展示例"></p>
<blockquote>
<p><strong>图10</strong>：场景扩展示例。将桌级任务演示通过坐标变换泛化到不同的房间级场景中，极大地增加了数据多样性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.00833v2/x11.png" alt="不同LLM对比"></p>
<blockquote>
<p><strong>图11</strong>：使用不同大语言模型（GPT-4o、DeepSeek-V3、DeepSeek-R1）进行规划的成功率对比。DeepSeek-R1表现最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.00833v2/x12.png" alt="与现有生成框架对比"></p>
<blockquote>
<p><strong>图12</strong>：HumanoidGen与现有数据生成框架（Gensim, RoboGen）在任务成功率和数据收集效率上的对比。HumanoidGen在专注于双臂灵巧操作任务上取得了更高的成功率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了<strong>HumanoidGen框架</strong>，首次利用LLM推理和空间约束生成，实现了仿人机器人双臂灵巧操作任务的自动化创建与高质量演示数据收集。2）构建了<strong>HGen-Bench基准</strong>，为仿人机器人灵巧操作研究提供了丰富的任务和场景。3）通过实验证明了生成数据的<strong>有效性和可扩展性</strong>，训练的策略性能随数据量增加而提升。</p>
<p>论文提到的局限性包括：框架性能依赖于仿真环境的准确性；空间标注过程仍需一定人工成本；LLM生成的规划代码的可靠性仍有提升空间。</p>
<p>本研究对后续工作的启示在于：它展示了LLM在理解和规划复杂机器人操作任务（尤其是需要精细空间推理和协调的双臂灵巧操作）方面的巨大潜力。通过将高层任务描述自动转化为可执行的底层约束，为实现“语言到动作”的通用机器人控制提供了新思路。同时，这种自动化的数据生成范式为解决机器人学习中的数据稀缺问题，特别是为仿人机器人这种高维复杂平台，开辟了一条高效且可扩展的途径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人双手机械手操作缺乏高质量演示数据的问题，提出了HumanoidGen自动化框架。其核心技术是利用原子灵巧操作进行空间标注，并借助LLM推理生成物体功能与场景驱动的、可执行的空间约束链；对于长时程任务，采用蒙特卡洛树搜索变体增强LLM规划能力。实验构建了新基准，结果表明，所生成的数据集能有效提升2D与3D扩散策略的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.00833" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>