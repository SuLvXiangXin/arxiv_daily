<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.09090" target="_blank" rel="noreferrer">2509.09090</a></span>
        <span>作者: Fang, Hengyu, Liu, Yijiang, Du, Yuan, Du, Li, Yang, Huanrui</span>
        <span>日期: 2025/09/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉-语言-动作模型在具身智能领域展现出前所未有的能力，但其巨大的计算和内存开销阻碍了实际部署。当前主流的VLA模型压缩加速方法包括量化和令牌剪枝。量化（尤其是W4A4，即4位权重和4位激活）能显著减小模型尺寸和计算成本；令牌剪枝则通过减少处理令牌数量直接降低计算负载，且对硬件友好。理论上，这两种正交的技术结合应能无缝实现高效模型。然而，本文发现两者存在内在的不兼容性：量化会深刻改变用于令牌剪枝的特征（如注意力分数）的统计分布，导致为高精度模型设计的剪枝策略在量化网络上失效；同时，剪枝后模型信息有限，使其对量化更加敏感。这种不兼容性被先前研究忽视，成为部署紧凑高效VLA模型的主要障碍。本文针对量化与令牌剪枝简单结合导致性能严重下降这一具体痛点，提出了协同设计的新视角。核心思路是：通过设计量化感知的令牌剪枝标准，使其适应量化表示；同时改进量化器设计以增强剪枝的有效性，从而实现两者在极端压缩下的高性能保留。</p>
<h2 id="方法详解">方法详解</h2>
<p>SQAP-VLA是一个结构化、无需训练的VLA推理加速框架，旨在同时实现最先进的量化和令牌剪枝。其整体流程是对模型进行W4A4量化的同时，应用量化感知的令牌剪枝策略。输入为原始VLA模型和视觉令牌序列，输出为压缩后（量化且令牌稀疏）的模型及保留的关键令牌子集，用于高效推理。</p>
<p><img src="https://arxiv.org/html/2509.09090v1/1754089121665.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：SQAP-VLA框架总览。该框架通过量化感知的剪枝标准解决了令牌剪枝在量化VLA模型上的失效问题。提出了不敏感保留、机器人感知投影和空间感知采样，以应对量化VLA模型中分散和偏移的注意力分数，从而在量化VLA模型上实现高性能、提升速度和减小模型尺寸。</p>
</blockquote>
<p>框架包含两个核心部分：面向剪枝的量化器增强和量化感知的剪枝策略。</p>
<p><strong>1. 量化感知的剪枝策略</strong>：为解决量化导致的注意力图失真（见图2），提出了三种策略。</p>
<ul>
<li><strong>量化不敏感令牌保留</strong>：经验发现，量化噪声对具有极端（最高）注意力分数的少量令牌的索引顺序影响很小。因此，策略是稳定地保留这些对任务至关重要的“top-k”令牌。其索引集合 𝒦_attn 通过选择注意力权重向量 a_q 中最大的 k 个值得到。</li>
<li><strong>机器人感知令牌保护</strong>：为补偿为保持量化稳定性而减小的k值，并创建一个真正量化不变的锚点，利用任务先验。直接根据机器人已知的3D世界坐标，通过相机内外参矩阵投影到2D像素坐标，再根据图像块大小映射到离散的令牌坐标。以此中心令牌（如机械臂末端执行器）为基础，在Chebyshev距离（ℓ∞范数）半径 R_t 内选择一个受保护的令牌环 𝒦_ring。该方法基于机器人物理状态，与量化误差无关。</li>
<li><strong>空间感知令牌采样</strong>：在保留上述高重要性令牌后，对剩余令牌 𝒯_remain 应用最远点采样，从中选择 m 个空间分布多样化的令牌子集 𝒦_fps，以在剪除冗余信息的同时保持广泛的空间覆盖。<br>最终保留的令牌是这三个集合的并集：𝒦_final = 𝒦_attn ∪ 𝒦_ring ∪ 𝒦_fps。其中 𝒦_ring 数量相对固定，𝒦_attn 的大小根据总体目标剪枝率按比例调整，剩余配额由 𝒦_fps 填充以确保空间平衡。</li>
</ul>
<p><strong>2. 面向剪枝的量化器增强</strong>：上述剪枝策略部分缓解了问题，但量化仍会严重扭曲注意力表示所需的内在表征。分析发现，VLA模型中的激活分布高度偏斜且不对称，存在显著的通道级离群值（见图3a）。这给激活量化带来了挑战。</p>
<ul>
<li><strong>通道级量化</strong>：针对激活，采用通道级（而非令牌级）量化，以获得更精细的粒度，提升量化保真度。</li>
<li><strong>Hadamard变换</strong>：为进一步抑制离群值影响，受LLM量化方法QUIP#启发，对查询和键层的权重与激活应用Hadamard变换。该变换能将激活能量更均匀地重新分布到所有通道中，从而抑制离群值效应。如图3b所示，变换后激活分布更平滑，提高了注意力图的可靠性，进而增强了令牌剪枝性能。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.09090v1/1753916942468.png" alt="注意力热力图对比"></p>
<blockquote>
<p><strong>图2</strong>：量化前后的注意力热力图。(a)和(c)量化前，注意力集中。(b)和(d)量化后，注意力变得分散和偏移。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.09090v1/1753916918060.png" alt="激活分布可视化"></p>
<blockquote>
<p><strong>图3</strong>：激活分布可视化。(a)原始激活在特定通道被大幅离群值主导。(b)经过Hadamard旋转后，来自这些离群值的能量被均匀分布，消除了极端峰值。</p>
</blockquote>
<p>与现有方法相比，SQAP-VLA的创新点在于首次系统地识别并解决了VLA模型中量化与令牌剪枝的不兼容问题，通过<strong>协同设计</strong>而非简单组合，同时从剪枝策略（适应量化）和量化器设计（服务剪枝）两个方向进行优化，实现了在极端压缩下的性能保持与效率提升。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用最先进的CogAct VLA模型及其两个官方变体（视觉匹配和变体聚合）进行评估。模型在大型Open X-Embodiment数据集上预训练。SQAP-VLA框架完全无需训练，直接应用于公开模型检查点。在标准机器人模拟基准中评估了四个具有代表性的操作任务：Pick Coke Can, Move Near, Open/Close Drawer, Place Apple in Top Drawer。评估指标为成功率（SR）和相对于FP16 CogAct基线的加速比。Baseline包括原始FP16模型，以及在FP16模型上应用的多种令牌剪枝方法：Random Dropping, FastV, VLA-Cache, EfficientVLA。</p>
<p><strong>关键结果</strong>：<br>表1和表2分别展示了在视觉匹配和变体聚合场景下的性能。SQAP-VLA（应用W4A4量化和剪枝）在几乎所有任务上都取得了最佳或极具竞争力的成功率。</p>
<blockquote>
<p><strong>表1</strong>：视觉匹配场景性能。我们的方法在W4A4量化模型上应用剪枝，取得了最佳平均性能。<br><strong>表2</strong>：变体聚合场景性能。我们的方法在激进的低比特量化后，仍展现出优于或与FP16基线高度竞争的性能。</p>
</blockquote>
<p>在视觉匹配场景，SQAP-VLA的平均成功率为79.3%，优于最佳FP16剪枝方法EfficientVLA（76.4%）2.9个百分点，并超过原始FP16基线（74.8%）4.5个百分点。在变体聚合场景，SQAP-VLA平均成功率为64.4%，超过基线（61.3%）3.1个百分点。在效率方面，SQAP-VLA实现了1.93倍的端到端系统加速。</p>
<p><img src="https://arxiv.org/html/2509.09090v1/0e64b71c546434cf65be1bbea23ed54.png" alt="速度与内存分析"></p>
<blockquote>
<p><strong>图4</strong>：延迟和内存实验。(a) SQAP-VLA在剪枝和量化上的消融研究，展示了系统级、LLM级加速以及各组件贡献。(b) 基线、EfficientVLA、FastV、VLA-Cache和SQAP-VLA的GPU峰值内存使用量比较。</p>
</blockquote>
<p>图4a的分解显示，系统级1.93倍加速由LLM骨干网2.56倍加速驱动，这源于W4A4量化（2.09倍加速）和令牌剪枝（1.21倍加速）的协同作用。图4b表明，SQAP-VLA将GPU峰值内存使用从基线的14.3 GB大幅降低至7.6 GB。</p>
<p><strong>消融实验</strong>：<br>表3研究了不同剪枝比率（0.3至0.6）。结果显示，应用本文剪枝方法的模型在多个比率下均优于原始未剪枝模型，且在剪枝比率为0.4时达到最高成功率，因此主实验选择此比率。</p>
<blockquote>
<p><strong>表3</strong>：剪枝比率的消融研究。我们报告了所有任务的平均成功率（%）。最佳性能以粗体标出。</p>
</blockquote>
<p>表4逐步增加了各压缩策略。仅应用W4A4量化会导致性能下降（视觉匹配从74.8%降至71.78%）。随后加入通用的量化不敏感剪枝（仅Top-k）未能完全恢复性能（70.9%）。引入机器人感知保护后，性能显著回升至接近基线（73.98%）。最后，加入空间感知采样，最终成功率（79.30%）不仅弥补了量化损失，还大幅超越了全精度基线。这验证了协同设计的核心假设。</p>
<blockquote>
<p><strong>表4</strong>：压缩策略的消融研究。“Baseline”表示原始全精度模型。我们使用4位权重和激活量化。依次引入三种令牌剪枝策略以展示每个组件的贡献。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文核心贡献在于：1）首次识别了VLA模型中量化与令牌剪枝的内在<strong>不兼容性</strong>，并提出了一个<strong>协同设计</strong>的量化感知剪枝框架来解决该问题。2）具体提出了三种量化感知的剪枝策略（量化不敏感保留、机器人感知保护、空间感知采样）以及面向剪枝的量化器增强方法（Hadamard变换+通道级量化），使两者能有效结合。</p>
<p>论文自身提到的局限性包括：作为一种训练后方法，其性能可能仍不及需要大量数据重新训练的量化感知训练方法；且当前工作主要围绕CogAct模型展开。</p>
<p>对后续研究的启示：1）模型压缩技术的组合需要<strong>协同设计</strong>，而非独立应用后的简单叠加，需考虑技术间的相互影响。2）在具身智能等特定领域，利用<strong>领域知识</strong>（如机器人坐标）可以构建对模型压缩扰动不敏感的稳定锚点，提升压缩鲁棒性。3）SQAP-VLA的无训练特性为快速部署高效VLA模型提供了可行路径，其思路可扩展至其他模型架构和压缩技术组合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型计算和内存成本高、难以部署的问题，提出SQAP-VLA框架。该框架协同设计量化与令牌剪枝，通过量化感知剪枝标准和改进量化器，克服两者不兼容性，实现无需训练的高效推理。实验表明，SQAP-VLA在标准VLA模型上实现1.93倍加速，平均成功率提升高达4.5%，显著提升效率并保持性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.09090" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>