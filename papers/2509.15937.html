<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.15937" target="_blank" rel="noreferrer">2509.15937</a></span>
        <span>作者: Zhai, Shaopeng, Zhang, Qi, Zhang, Tianyi, Huang, Fuxian, Zhang, Haoran, Zhou, Ming, Zhang, Shengzhe, Liu, Litao, Lin, Sixu, Pang, Jiangmiao</span>
        <span>日期: 2025/09/19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉-语言-动作（VLA）模型的机器人主要通过模仿学习进行训练，这依赖于大规模、高质量的人类专家轨迹数据收集，成本高昂且难以覆盖真实世界动态变化的任务和环境。直接在真实世界中进行强化学习（RL）是另一种有前景的范式，但面临稀疏、手工设计的奖励信号和低效探索的关键瓶颈。现有方法要么需要为每个场景单独设计任务特定的奖励，要么虽然宣称可复用，但仍需额外的任务相关数据来训练奖励替代函数或终止分类器，且产生的奖励通常只在任务接近完成时出现，中间进度无法评分。尽管一些方法引入了更密集或“通用”的进度信号，但其跨新任务、物体或目标语言的泛化能力有限。因此，中间反馈信号不可靠且难以迁移，阻碍了样本效率。</p>
<p>本文针对真实世界RL中缺乏密集、可泛化奖励信号这一具体痛点，提出了一个统一的新视角：构建一个通用的进程奖励模型，该模型不仅能提供密集的进度增量和终止信号，还能作为策略输出动作，从而将评论家（Critic）和策略（Actor）的角色整合在单一的自回归架构中。本文的核心思路是：基于InternVL大模型，利用大规模异构数据训练一个统一的视觉-语言-动作-评论家模型（VLAC），通过提示控制使其交替生成奖励和动作令牌，为真实世界RL提供免于任务特定工程化的密集奖励，并支持对未见任务和环境的单样本上下文迁移。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLAC方法的核心是一个统一的模型，它基于成对的进度理解进行构建，能够联合实现动作生成和增量任务进度生成，具备上下文学习和跨场景、跨任务的泛化能力。</p>
<p><img src="https://arxiv.org/html/2509.15937v1/figures/teaser.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。预训练在多源数据上的VLAC模型，既能为真实世界RL提供密集的进程奖励，也能作为策略输出动作，整合进真实世界RL循环中，实现操作任务的自我改进。</p>
</blockquote>
<p>整体框架如<strong>图1</strong>所示。VLAC模型建立在InternVL之上，其输入包括一对图像观测（任意两个中间任务状态）和一个语言任务描述，输出是一个有符号的进度增量值（作为奖励）以及可能的动作令牌。在部署时，该模型被集成到一个异步的真实世界RL循环中，机器人通过与环境交互收集轨迹（成功与失败），VLAC模型为这些交互提供奖励和终止判断，进而通过策略优化（如PPO）实现自我改进。</p>
<p><img src="https://arxiv.org/html/2509.15937v1/x1.png" alt="模型训练与能力"></p>
<blockquote>
<p><strong>图2</strong>：VLAC模型训练数据构成及其能力展示。模型在综合的公共机器人操作数据、人类演示数据、自收集数据及多种图像理解数据集上训练。视频数据被处理成成对样本以学习两帧间的任务进度差异。右图展示了模型对训练数据中未见过的新机器人、场景和任务所展现出的强大泛化能力，包括预测任务进度、区分失败动作/轨迹（提供密集奖励）以及直接执行操作任务。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>VLAC评论家学习</strong>：这是模型提供奖励信号的核心。其目标是理解任意两帧图像之间的相对任务进度。形式化定义为：给定一个由图像序列O和任务描述l_task构成的视频段，模型预测<code>c_{i,i+Δt} = VLAC(o_i, o_{i+Δt}; l_task)</code>，表示从状态<code>o_i</code>到<code>o_{i+Δt}</code>的任务进度增量。训练时，标签根据时间顺序设定为<code>c_{i,i+Δt} = Δt/(T-i)</code>，代表从<code>o_i</code>到<code>o_{i+Δt}</code>的任务进度百分比。这种设计对轨迹的全局起点和数据收集策略不可知，提高了泛化性和鲁棒性。</p>
<ul>
<li><strong>辅助任务</strong>：为了增强语义理解，模型还同时学习<strong>任务描述估计</strong>（根据起始帧和结束帧生成任务描述）和<strong>任务完成判断</strong>（判断单帧图像是否表示任务已完成）。</li>
<li><strong>数据构建策略</strong>：为了提升进度理解的鲁棒性，论文设计了四种策略：(1) <strong>成对图像差异过滤</strong>：若连续帧像素差异过小（&lt;1%），则将其进度增量标签设为0，让模型关注显著变化。(2) <strong>联合采样的成对进度理解</strong>：为每个采样的图像对<code>(o_i, o_{i+Δt})</code>，在批次内构造包含正向、反向、细粒度、全局理解的四个相关数据样本，确保数据平衡和对称性。(3) <strong>任务完成判断联合采样</strong>：每次采样一个已完成状态和一个未完成状态的数据对，以平衡完成判断任务的数据。(4) <strong>任务描述与图像序列的交叉采样</strong>：以5%的概率为图像对随机分配一个不属于当前轨迹的任务描述，并将进度增量标签设为0，以提升模型对语义与进度对齐的理解能力。</li>
<li><strong>上下文学习增强</strong>：为了进一步提升跨场景和跨任务的迁移能力，模型支持上下文学习。形式化为<code>c_{i,i+Δt} = VLAC(o_i, o_{i+Δt}; l_task, O_ref, o_0)</code>，其中<code>O_ref</code>是参考过程（如机器人或人类演示），<code>o_0</code>是当前轨迹的起点。这使模型能够从单个参考示例中有效学习。</li>
</ul>
</li>
<li><p><strong>VLAC动作学习</strong>：在通用任务进程理解的基础上，模型还学习在语义空间中生成动作以实现机械臂的多任务控制。动作被表示为末端执行器（End-Effector）位姿的增量（delta pose），这是一种通用的三维空间表示，与具体实体无关。动作生成公式为：<code>a_i = VLAC(o_i^0, ..., o_i^k; s_i; l_task; history_{i-1,i-th})</code>，其中<code>o_i^k</code>是第k个视角的图像，<code>s_i</code>是机械臂状态，<code>history</code>是历史动作。这种将动作表示为数字字符串并由自回归方式生成的方法，利用了预训练模型的强语义表示和生成能力。</p>
</li>
<li><p><strong>VLAC视觉-语言感知学习与数据混合</strong>：为了增强模型的多模态理解能力，训练中融合了一系列公开的视觉问答（VQA）数据集，侧重于通用对话能力、机器人理解、空间推理和成对图像差异区分。模型总共在超过4200小时（人类数据3000+小时，机器人数据1200+小时，自收集数据15+小时）的标注操作数据以及多种VQA数据上进行了训练，采样了约4000万个数据点。</p>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>架构统一</strong>：单个自回归模型通过提示控制，交替扮演评论家（输出奖励）和演员（输出动作）的角色，简化了系统设计。</li>
<li><strong>奖励机制</strong>：提供了密集、可泛化的进程增量奖励和终止信号，无需为每个新任务进行手工奖励工程或额外数据收集。</li>
<li><strong>数据利用</strong>：通过成对进度理解框架，能够联合利用无需动作信息的大规模人类数据（增强泛化）和带有动作标注的机器人数据（ grounding 动作生成），缓解了机器人数据稀缺问题。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在四个不同的真实世界机器人操作任务上进行评估：<code>Pouring</code>（倾倒）、<code>Placing</code>（放置）、<code>Sweeping</code>（清扫）、<code>Wiping</code>（擦拭）。</li>
<li><strong>对比基线</strong>：包括基于模仿学习的方法（<code>ACT</code>、<code>VINN</code>）、使用任务特定稀疏奖励的RL方法（<code>PPO (sparse)</code>），以及使用其他通用奖励模型的方法，如基于CLIP相似度的奖励（<code>CLIP (L2)</code>）、基于大型视觉语言模型（<code>GPT-4o</code>、<code>InternVL-Chat</code>）进行单帧或帧对比较的奖励。</li>
<li><strong>实验平台</strong>：构建了基于Ray的异步真实世界RL框架，机器人通过ZeroMQ与运行在GPU服务器上的VLAC模型通信，并采用了动态推理服务器分配机制以最小化延迟。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在200个真实世界交互回合内，仅使用VLAC模型进行RL自我改进，平均成功率从初始的约30%提升至约90%。当引入分级的人机交互协议（离线演示回放、回报与探索、人类引导探索）后，样本效率进一步提升了约50%，并且最终成功率达到了100%。</p>
<p><img src="https://arxiv.org/html/2509.15937v1/figures/VOC-F1_performance_comparison_log_scaled.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：不同方法在四个真实世界任务上的学习曲线对比（成功率 vs. 回合数）。VLAC (RL) 和 VLAC (HITL RL) 显著优于所有基线方法，尤其是VLAC结合人机交互后，学习更快且最终性能达到100%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15937v1/figures/case_study.png" alt="定性案例"></p>
<blockquote>
<p><strong>图5</strong>：定性结果案例研究。展示了VLAC模型在清扫任务中提供的密集奖励信号（绿色柱状图），以及在不同任务上对失败轨迹（红色）和成功轨迹（蓝色）的进度评分区分能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.15937v1/x2.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融实验。左图显示了不同训练数据混合比例对模型进度评分区分能力（F1分数）的影响，验证了混合数据策略的有效性。右图展示了人机交互（HITL）三个不同阶段对学习效率的逐级提升作用。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>数据混合</strong>：实验表明，混合人类数据、机器人数据和VQA数据对于训练出具有强泛化能力和高精度进度区分（高F1分数）的VLAC模型至关重要。</li>
<li><strong>人机交互协议</strong>：分级的人机交互（从演示回放到人类引导探索）被证明能有效加速早期探索、稳定学习过程，并大幅提升样本效率。</li>
<li><strong>模型组件</strong>：通过提示控制统一评论家与策略的功能，使得两者能够相互促进。论文指出，增强评论家（进度理解）组件也有助于下游动作生成能力的提升。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了VLAC模型</strong>：一个统一的视觉-语言-动作-评论家模型，通过单一的、基于提示控制的自回归架构，同时实现了密集且可泛化的进程奖励估计以及机器人动作生成。</li>
<li><strong>设计了密集且可泛化的奖励机制</strong>：基于成对进度理解，利用大规模异构数据训练，消除了对任务特定奖励工程的需求，并支持对未见任务的上下文迁移。</li>
<li><strong>开发了高效的真实世界RL框架</strong>：结合异步交互系统和分级人机交互协议，显著提升了机器人直接在真实环境中进行强化学习的样本效率和最终性能。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，在异步RL框架中，由于机器人执行、观测上传和模型推理的异步性，生成的动作可能无法精确对应动作生成时刻的观测。为此，在训练时对动作时间戳进行了延迟调整以匹配机器人的运动速度，但这仍是实际部署中需要精细处理的问题。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>统一架构趋势</strong>：VLAC展示了将感知、推理、奖励评估和控制整合到单一模型中的潜力，这可能是迈向更通用机器人智能的一个有前景的方向。</li>
<li><strong>利用人类世界知识</strong>：通过创新的任务表述（如成对进度理解），能够有效利用海量、易得的无动作人类视频数据来增强模型的泛化理解和奖励信号质量，缓解机器人数据瓶颈。</li>
<li><strong>人机协同学习</strong>：渐进式、分级的人机交互协议被证明是安全、高效地提升真实世界RL性能的关键，未来可以探索更智能、更自动化的交互引导机制。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人真实世界强化学习中奖励稀疏、探索低效的核心问题，提出VLAC模型。该模型基于InternVL，通过视觉-语言和机器人轨迹数据训练，输出密集进度奖励与动作令牌，统一评论家与策略，并采用分级人类在环协议加速学习。实验表明，在四个真实操作任务中，VLAC在200次交互内将成功率从约30%提升至约90%；加入人类干预后，样本效率再提高50%，最终成功率可达100%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.15937" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>