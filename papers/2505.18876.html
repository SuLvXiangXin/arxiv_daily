<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DiffusionRL: Efficient Training of Diffusion Policies for Robotic Grasping Using RL-Adapted Large-Scale Datasets - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>DiffusionRL: Efficient Training of Diffusion Policies for Robotic Grasping Using RL-Adapted Large-Scale Datasets</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.18876" target="_blank" rel="noreferrer">2505.18876</a></span>
        <span>作者: Makarova, Maria, Liu, Qian, Tsetserukou, Dzmitry</span>
        <span>日期: 2025/05/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>扩散模型因其强大的多模态分布建模能力，在机器人决策与灵巧操控领域展现出潜力。然而，现有方法面临数据限制和场景特定适应性的挑战。例如，一些工作（如结合视觉-语言-动作模型的方法）虽然取得了超过80%的成功率，但其扩散模型训练数据集仅为每个场景30-40个样本，难以有效泛化到其他物体。同时，现有的大规模预建数据集（如DexGraspNet）虽然数据量庞大（包含133个类别的5355个物体的132万个抓取示例），但直接用于训练特定任务策略时，存在大量不准确样本（不准确率超过50%），且抓取姿态依赖于特定的机械手初始位姿，限制了其直接应用。</p>
<p>本文针对上述痛点，提出了一种新视角：利用强化学习（RL）来增强和适应大规模预建数据集，从而高效训练轻量级扩散策略。核心思路是：首先使用一个轻量级RL智能体对DexGraspNet数据集中的样本进行筛选和动作增强，生成一个高质量、适应特定任务环境（如变化的机器人位姿）的数据集，然后利用该数据集训练一个紧凑的扩散策略模型，最终在随机物体姿态下实现高成功率的抓取。</p>
<h2 id="方法详解">方法详解</h2>
<p>方法的整体流程是一个端到端的管道，包含三个主要阶段：1) 基于RL的数据集增强；2) 扩散策略训练；3) 基于姿态采样算法的验证。输入是原始的DexGraspNet数据集和任务环境定义，输出是能够在随机物体姿态下成功执行抓取动作的扩散策略。</p>
<p><img src="https://arxiv.org/html/2505.18876v1/extracted/6475272/pictures_grasp/pipeline_final_one.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：基于RL的数据集增强管道。(a) 展示了整体数据流：从DexGraspNet获取物体姿态和抓手动作数据送入环境(1)，环境将观测向量传给RL智能体(2,3)，智能体预测额外的抓手动作并返回环境(4,5)，最终环境计算奖励以验证抓取成功与否(6)。(b) 详细展示了RL智能体训练和数据记录过程中的环境时间戳。</p>
</blockquote>
<p><strong>核心模块一：基于强化学习的数据集增强</strong>。此模块旨在解决原始数据集样本质量不高且与目标仿真环境不匹配的问题。具体流程为：首先对数据集样本进行视觉筛选，排除明显不合理的抓取姿态（如图1所示的不稳定抓取）。然后，训练一个RL智能体来预测额外的抓手关节动作，以补偿因机器人基座（UR10e）姿态变化或仿真模型微小差异带来的不稳定问题。智能体采用Twin Delayed DDPG (TD3)算法，其执行器（Actor）和评论家（Critic）网络均为包含四个线性层的轻量级架构。</p>
<p><img src="https://arxiv.org/html/2505.18876v1/extracted/6475272/pictures_grasp/rl_horizontal.png" alt="网络架构"></p>
<blockquote>
<p><strong>图3</strong>：RL智能体的执行器（Actor）和评论家（Critic）神经网络架构。两者均为四层线性网络。</p>
</blockquote>
<p>训练分两步进行：先在机器人静态位姿（抓手手掌垂直）下训练，筛选出即使通过额外动作也无法成功抓取的“坏样本”；然后在机器人动态初始位姿下，对剩余的“好样本”进行训练，使智能体学会在更广泛的环境条件下生成稳定的抓取动作。最终，记录由原始动作和RL预测的额外动作组合而成的高质量轨迹，形成增强后的数据集。</p>
<p><strong>核心模块二：扩散策略模型</strong>。此模块利用增强后的数据集训练一个扩散策略。模型采用条件U-Net架构，以历史观测（前2步）为条件，预测未来8步的动作序列，并在环境中执行其中一半。训练时使用DDPM噪声调度器，损失函数为均方误差（MSE）。</p>
<p><img src="https://arxiv.org/html/2505.18876v1/extracted/6475272/pictures_grasp/recalccc.png" alt="数据样本重计算"></p>
<blockquote>
<p><strong>图5</strong>：为扩散模型训练重新计算的数据样本示意图。模型以过去2步的观测为条件，预测未来8步的动作。</p>
</blockquote>
<p><strong>创新点</strong>：与直接在小规模人工收集数据或原始大规模但有噪声数据上训练扩散模型的方法相比，本文的核心创新在于引入了RL驱动的数据增强步骤。这自动化地解决了大规模预建数据集的质量和适应性问题，无需为每个新场景手动收集大量数据，显著降低了应用扩散模型的门槛并提高了数据的可扩展性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在ManiSkill仿真环境中进行，使用UR10e机械臂和24自由度的ShadowHand灵巧手。基准数据集为DexGraspNet。实验对象选择了香蕉（Banana）、瓶子（Bottle）和相机（Camera）三种。评估时，使用新开发的姿态采样算法生成随机的、未见过的物体姿态进行测试。</p>
<p><strong>基线对比与关键结果</strong>：实验主要与原始数据集质量进行对比。RL增强阶段的结果如表1所示，经过视觉筛选和静态位姿RL训练后，原始样本保留下相当比例（32%-66%）。更重要的是，在动态位姿下使用RL智能体生成的新数据集中，成功轨迹率高达93%-96%，证明了增强的有效性。</p>
<table>
<thead>
<tr>
<th align="left">物体</th>
<th align="left">DexGraspNet样本数</th>
<th align="left">视觉筛选后比例</th>
<th align="left">静态机器人训练后比例</th>
<th align="left">新数据集成功样本率</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Banana</td>
<td align="left">205</td>
<td align="left">54%</td>
<td align="left">42%</td>
<td align="left">93%</td>
</tr>
<tr>
<td align="left">Bottle</td>
<td align="left">248</td>
<td align="left">66%</td>
<td align="left">42%</td>
<td align="left">96%</td>
</tr>
<tr>
<td align="left">Camera</td>
<td align="left">206</td>
<td align="left">49%</td>
<td align="left">32%</td>
<td align="left">96%</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：数据集增强和记录过程中收集的样本统计。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.18876v1/extracted/6475272/pictures_grasp/mean_rew_static.png" alt="RL训练曲线"><br><img src="https://arxiv.org/html/2505.18876v1/extracted/6475272/pictures_grasp/success_rate_static_fa2.png" alt="RL训练曲线"><br><img src="https://arxiv.org/html/2505.18876v1/extracted/6475272/pictures_grasp/mean_rew_dynam.png" alt="RL训练曲线"><br><img src="https://arxiv.org/html/2505.18876v1/extracted/6475272/pictures_grasp/success_rate_dynamic_2.png" alt="RL训练曲线"></p>
<blockquote>
<p><strong>图6</strong>：RL智能体在静态和动态机器人位姿下训练时的平均奖励和平均成功率。动态位姿训练是在静态位姿训练筛选出的数据子集上进行的。结果显示，在动态位姿下训练收敛后，成功率接近90-100%。</p>
</blockquote>
<p><strong>扩散模型训练结果</strong>：在增强后的数据集上训练扩散模型。如图9所示，模型训练过程收敛，并且在验证集（使用姿态采样算法生成的随机物体姿态）上取得了接近80%的平均成功率。这证明了整个管道训练出的扩散策略具有良好的泛化能力。</p>
<p><img src="https://arxiv.org/html/2505.18876v1/extracted/6475272/pictures_grasp/loss_diffusion.png" alt="扩散模型训练曲线"><br><img src="https://arxiv.org/html/2505.18876v1/extracted/6475272/pictures_grasp/success_rate_diffusion.png" alt="扩散模型训练曲线"></p>
<blockquote>
<p><strong>图9</strong>：扩散模型训练过程中的MSE损失和在随机物体姿态下的成功率。最终成功率接近80%。</p>
</blockquote>
<p><strong>消融实验分析</strong>：虽然没有严格的消融实验对比表，但整个管道本身包含了一个内在的消融研究。RL增强步骤（包括视觉筛选、静态位姿训练筛选和动态位姿动作增强）是性能提升的关键。表1中的数据清晰地展示了每一步对样本质量的提升作用，从原始数据集中筛选掉大量低质量样本，并通过RL生成了高成功率（&gt;93%）的新数据，这是后续扩散模型取得高性能的基础。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个端到端的管道，利用轻量级RL智能体增强大规模预建数据集（DexGraspNet），使其适应特定任务和环境条件，从而高效训练轻量级扩散策略。2) 开发了一种基于原始数据集统计的物体姿态采样算法，用于生成多样且真实的测试姿态以验证模型泛化能力。3) 在三种物体的灵巧抓取任务上验证了方法的有效性，训练出的紧凑扩散模型在随机场景下达到了约80%的成功率。</p>
<p><strong>局限性</strong>：论文自身提到，实验目前仅在仿真环境中对三种物体进行了测试。未来需要扩展到更多物体并在真实世界中验证。</p>
<p><strong>启示</strong>：本文为在机器人领域高效利用现有大规模数据集训练高性能扩散策略提供了一条可行路径。其“RL增强预建数据集”的核心思想可以推广到其他复杂的机器人操作任务中，降低对昂贵、耗时的手动数据收集的依赖，提高策略开发的效率和可扩展性。此外，该方法可与高级规划模型（如VLA模型）结合，为其提供经过适配的、高质量的底层技能策略库。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DiffusionRL方法，解决机器人抓取中扩散策略训练面临的数据限制和场景适应性问题。关键技术包括利用强化学习增强大规模预建数据集（如DexGraspNet），进行轻量级扩散策略端到端训练，并结合姿态采样算法验证。实验表明，该方法在三个DexGraspNet对象上实现了80%的高成功率，无需手动数据收集，提升了泛化与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.18876" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>