<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning a Unified Latent Space for Cross-Embodiment Robot Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Learning a Unified Latent Space for Cross-Embodiment Robot Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.15419" target="_blank" rel="noreferrer">2601.15419</a></span>
        <span>作者: Yan, Yashuai, Lee, Dongheui</span>
        <span>日期: 2026/01/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，实现跨不同形态人形机器人的统一控制是一个长期目标，但面临巨大挑战，主要源于机器人形态、自由度数量和运动学约束的巨大差异。现有方法主要通过两种策略弥合形态差异：一是学习任务相关、领域不变的表示，例如对齐人类视频与机器人演示的技能表示，但这类方法通常需要大量配对数据，限制了向新形态平台扩展的能力；二是构建跨形态的共享潜在空间，将语义相似的动作映射到邻近点，但先前工作多专注于从人类到特定机器人的运动重定向。本文的核心痛点是：如何学习一个能够统一多种形态（包括单臂、双臂、腿式人形机器人）的共享潜在空间，并在此基础上训练一个可直接部署到所有编码在该空间中的机器人的控制策略。本文的核心思路是：采用两阶段方法，首先通过解耦的、模块化的对比学习框架构建一个统一的潜在空间，实现对不同身体部位运动的精细对齐；然后，在此潜在空间中，仅使用人类数据训练一个目标条件控制策略，该策略可零样本迁移到多种机器人上。</p>
<h2 id="方法详解">方法详解</h2>
<p>方法整体分为两个阶段（如图1和图2所示）：第一阶段学习统一的潜在表示，第二阶段在潜在空间中进行机器人控制。</p>
<p><img src="https://arxiv.org/html/2601.15419v1/figures/modeloverview.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：学习统一潜在表示的架构。该架构将人类和多种机器人的运动表示统一到一个共享潜在空间中。为精确建模局部运动模式，潜在空间被解耦为五个子空间，分别对应左臂(LA)、右臂(RA)、躯干(TK)、左腿(LL)、右腿(RL)。模型包含人类编码器(E_h)、跨形态编码器(E_X)和跨形态解码器(D_X)。每个机器人有一个可学习的、机器人特定的嵌入层(E_r)，用于将原始姿态映射到共享特征空间，D_r则执行逆映射。</p>
</blockquote>
<p><strong>第一阶段：学习统一潜在空间</strong></p>
<ol>
<li><strong>解耦的潜在空间与定制化相似度度量</strong>：为解决不同形态间映射模糊的问题，论文将潜在空间解耦为五个对应于不同身体部位的子空间（LA, RA, TK, LL, RL）。这种设计允许为每个子空间定制相似度度量。对于手臂子空间，采用结合关节旋转和末端执行器位置的混合度量：<code>S_k = D_R + ω * D_ee</code>，其中<code>D_R</code>基于关节四元数的点积计算旋转差异，<code>D_ee</code>计算归一化后的末端位置L2距离，<code>ω</code>是平衡权重。对于躯干和腿部，则仅使用旋转度量<code>S_k = D_R</code>。</li>
<li><strong>模块化对比学习框架</strong>：模型包含共享组件（E_h, E_X, D_X）和机器人特定组件（E_r, D_r）。E_r将不同维度的机器人原始姿态投影到固定维度的共享特征空间。E_h和E_X分别将人类姿态和机器人特征编码到五个解耦的潜在子空间中。训练时，在每个子空间内应用三元组对比损失（Triplet Loss），鼓励语义相似的运动（无论来自人类还是何种机器人）在潜在空间中靠近。</li>
<li><strong>多目标损失函数</strong>：总损失<code>L_total</code>是加权和，包含：对比损失<code>L_contrastive</code>、机器人姿态重构损失<code>L_rec</code>、人类-机器人潜在一致性损失<code>L_ltc</code>（确保人类姿态编码-解码-再编码后潜在表示一致）、以及时间损失<code>L_temporal</code>（对齐人类手部速度与机器人末端速度）。权重设置为<code>λ_c=10, λ_rec=5, λ_ltc=1, λ_temp=0.1</code>。</li>
<li><strong>高效扩展新机器人</strong>：预训练好共享组件后，要加入新机器人，只需冻结E_h, E_X, D_X，然后仅训练该机器人特定的嵌入层E_r和D_r即可，实现了轻量级扩展。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.15419v1/figures/RLC_CVAE.png" alt="控制策略"></p>
<blockquote>
<p><strong>图2</strong>：目标条件的潜在空间机器人控制。基于c-VAE的框架，仅使用人类演示数据学习在共享潜在空间中建模目标导向的运动动态。在训练时，模型以当前潜在姿态<code>z_t</code>和朝向采样目标的平均末端速度<code>v_ee</code>为条件，预测潜在位移<code>d_t</code>。推理时，<code>v_ee</code>根据用户指定的目标位置和时间范围计算，通过自回归迭代更新潜在状态，生成跨机器人的平滑、目标导向运动。</p>
</blockquote>
<p><strong>第二阶段：潜在空间机器人控制</strong></p>
<ol>
<li><strong>策略设计与训练</strong>：在已学得的共享潜在空间基础上，训练一个目标条件的运动生成策略。该策略采用条件变分自编码器（c-VAE）架构，<strong>仅使用人类运动数据进行训练</strong>。策略学习预测潜在空间的位移向量<code>d_t = z_{t+1} - z_t</code>，而非绝对姿态。条件信息是当前潜在状态<code>z_t</code>和朝向目标的平均末端速度<code>v_ee</code>（从人类手部位置计算得出）。</li>
<li><strong>推理与控制</strong>：部署时，用户指定目标末端位置<code>p_ee^T</code>和时间范围<code>T</code>。根据当前机器人末端位置<code>p_ee^t</code>计算<code>v_ee</code>，同时将当前机器人姿态编码为<code>z_t</code>。c-VAE解码器以<code>z_t</code>、<code>v_ee</code>和采样噪声为条件，预测位移<code>d_t</code>，从而更新潜在状态<code>z_{t+1} = z_t + d_t</code>。此过程自回归进行，生成的潜在序列通过解码器<code>D_X</code>和机器人特定解码器<code>D_r</code>转换为关节角度，驱动机器人朝向目标运动。</li>
</ol>
<p><strong>核心创新点</strong>：1) <strong>解耦的潜在空间架构</strong>，实现了跨形态的细粒度运动对齐；2) <strong>模块化对比学习与定制相似度度量</strong>，可联合学习人类与多机器人表示；3) <strong>在共享潜在空间中仅用人类数据训练控制策略</strong>，实现了真正的跨形态零样本控制；4) <strong>轻量级扩展机制</strong>，通过仅训练嵌入层高效纳入新机器人。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与平台</strong>：使用HumanML3D数据集（包含超过400万个人类姿态）。<strong>无需收集真实机器人数据</strong>，而是通过机器人的URDF文件，使用正向动力学（FK）在GPU上并行采样数十亿个随机的机器人关节配置用于训练。评估涉及多种机器人平台，包括ATLAS（全身）、H1（腿式，躯干活动有限）、TIAGO（移动操作臂）、JVRC等。</p>
<p><strong>Baseline方法</strong>：1) **ImitationNet [17]**：为每个目标机器人单独训练一个人类到该机器人的重定向模型。2) <strong>耦合潜在空间变体</strong>：与本文方法对比，使用单一、未解耦的全身潜在空间。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>人类到机器人运动重定向</strong>：在运动重定向任务上，本文方法在ATLAS、H1和TIAGO机器人上均取得了比针对各机器人单独训练的ImitationNet更低的平均位置误差（例如ATLAS: 0.043 vs. 0.047; H1: 0.041 vs. 0.045; TIAGO: 0.021 vs. 0.025）。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.15419v1/figures/human2robot.png" alt="人类到机器人重定向"></p>
<blockquote>
<p><strong>图3</strong>：人类到机器人运动重定向的定性结果。本文方法能够将多样的人类动作（如挥手、行走、踢腿）准确地重定向到具有不同形态的机器人（ATLAS, H1, TIAGO）上。</p>
</blockquote>
<ol start="2">
<li><strong>跨机器人运动重定向</strong>：解耦的潜在空间在跨机器人重定向任务上显著优于耦合版本。例如，从ATLAS重定向到H1，解耦空间的平均位置误差为0.040，而耦合空间为0.047。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.15419v1/figures/robot2robot.jpg" alt="跨机器人重定向"></p>
<blockquote>
<p><strong>图4</strong>：跨机器人运动重定向结果。展示了从源机器人（如ATLAS）到目标机器人（如H1）的运动迁移，解耦潜在空间能产生更自然、准确的结果。</p>
</blockquote>
<ol start="3">
<li><strong>机器人末端执行器控制</strong>：训练于人类数据的潜在空间控制策略，能够零样本部署到多种机器人上，完成到达指定目标位置的任务。在仿真中，对于TIAGO机器人，在100个随机目标上成功率高达93.3%。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.15419v1/figures/robot_control2.png" alt="机器人控制"></p>
<blockquote>
<p><strong>图5</strong>：潜在空间控制策略的定量评估。柱状图展示了在不同机器人上执行末端到达任务的成功率，表明策略具有良好的跨形态泛化能力。</p>
</blockquote>
<ol start="4">
<li><strong>潜在空间可视化与编辑</strong>：PCA可视化显示，解耦的潜在空间子空间内，来自不同形态的相似运动（如挥手）聚集在一起。此外，潜在空间支持语义编辑，例如通过插值改变运动风格。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.15419v1/figures/pca.png" alt="潜在空间可视化"></p>
<blockquote>
<p><strong>图12</strong>：左臂潜在子空间的PCA可视化。来自人类、ATLAS和TIAGO的“挥手”动作在潜在空间中聚集在一起，证明了跨形态的语义对齐。</p>
</blockquote>
<ol start="5">
<li><strong>消融实验</strong>：去除对比损失(<code>L_contrastive</code>)会导致重定向性能显著下降；去除时间损失(<code>L_temporal</code>)会使生成运动不平滑；去除潜在一致性损失(<code>L_ltc</code>)会损害人类到机器人的重定向质量。所有组件都对最终性能有贡献。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个<strong>解耦的潜在空间架构</strong>，实现了对多样化机器人形态的细粒度运动对齐；2) 设计了一个<strong>模块化的对比学习框架</strong>，结合定制化的相似度度量，能够从人类和多个机器人数据中联合学习运动表示；3) 实现了<strong>在共享潜在空间中仅用人类数据训练控制策略</strong>，该策略可直接、零样本地用于控制多个机器人；4) 提供了一个<strong>可扩展的框架</strong>，通过轻量级嵌入层训练即可纳入新机器人，支持高效的多机器人系统部署。</p>
<p><strong>局限性</strong>：论文提到，当前方法主要在静态环境中评估目标到达任务。在动态环境中的鲁棒性、以及泛化到更复杂的操作任务（如抓取和放置）方面仍有待探索。</p>
<p><strong>后续研究启示</strong>：1) <strong>解耦与模块化</strong>是处理形态多样性的有效途径，可推广到其他跨域学习问题。2) <strong>无需配对数据</strong>，通过合成采样和对比学习构建对齐表示的方法，为数据稀缺场景提供了新思路。3) <strong>潜在空间作为通用接口</strong>，将控制策略与具体机器人实现解耦，为构建统一的机器人学习与控制平台奠定了基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出了一种用于跨形态人形机器人控制的统一框架。核心问题是解决因机器人形态、自由度差异导致的控制策略难以迁移的挑战。方法分两步：首先，通过对比学习和结合关节旋转与末端位置的自定义相似性度量，构建一个解耦的共享潜在空间，以实现精准的运动重定向。随后，仅使用人类数据在该空间内训练一个基于条件变分自编码器的目标条件控制策略。实验表明，训练好的策略无需调整即可直接部署于多种机器人，且新机器人仅需学习一个轻量级嵌入层即可融入该框架，实现了鲁棒、可扩展且与具体形态无关的控制。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.15419" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>