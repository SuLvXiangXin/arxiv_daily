<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.24661" target="_blank" rel="noreferrer">2509.24661</a></span>
        <span>作者: Shan Luo Team</span>
        <span>日期: 2025-09-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，实现跨形态（cross-embodiment）的灵巧抓取合成，即自适应地为不同形态的机器人手生成和优化抓取，是迈向通用机器人操作的关键。现有方法主要分为两类，但都存在关键局限性。一类是基于物理的优化方法（如DFC），其仅依赖力闭合等静态物理约束，缺乏对人类抓取运动学的理解，导致在实际动态抓取场景中成功率低。另一类是基于人类示范的学习方法（如GRAB, RealDex），虽能获得类人运动学，但数据收集成本高昂，且通常需要将人手关节标定并映射到机器人关节，主要局限于拟人结构，难以泛化至非拟人机器人手（如三指Barrett手）。</p>
<p>本文针对上述痛点，提出了一种新视角：将机器人运动学模型与生成的类人接触表示对齐。核心思路是，首先利用在人类数据上预训练的生成模型高效合成类人接触模式，然后通过一种拓扑合并策略将其适配到任意机器人手形态，最后结合物理感知约束进行优化，从而大规模生成兼具类人运动学合理性和物理稳定性的跨形态灵巧抓取。</p>
<h2 id="方法详解">方法详解</h2>
<p>CEDex方法的整体流程如图2所示。给定一个物体点云 <strong>O</strong> 和一个任意机器人手的运动学模型，CEDex首先生成类人接触表示，然后执行运动学人机接触对齐，最后进行基于符号距离场（SDF）的抓取优化，输出一个物理稳定的抓取配置。</p>
<p><img src="https://arxiv.org/html/2509.24661v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：CEDex的跨形态灵巧抓取合成流程。给定物体点云和机器人手，CEDex首先使用在人类接触数据上预训练的CVAE模型生成类人接触表示。运动学人机接触对齐组件随后根据机器人运动学配置执行拓扑合并，将多个人手部件整合为统一的机器人部件，接着进行带有物理感知约束的基于SDF的抓取优化，以生成具有类人运动学理解的鲁棒且多样的抓取。</p>
</blockquote>
<p><strong>核心模块一：类人接触生成</strong><br>该模块旨在生成物体中心的类人接触表示 **[C^h, P^h]**，包括一个接触图 <strong>C^h</strong>（N×1，表示每个点的接触概率）和一个部件图 <strong>P^h</strong>（N×B，B=16，表示接触点对应的人手部件标签）。采用一个条件变分自编码器（CVAE）来建模条件概率 p(C^h, P^h | O)。具体使用两个基于Point Cloud的解码器 <strong>D_c</strong> 和 <strong>D_p</strong> 分别预测接触图和部件图，其中部件图的预测额外以接触图为条件。模型在GRAB和YCB Affordance数据集上预训练，重建损失包括接触图的L1损失、部件图的交叉熵损失以及KL正则化损失。</p>
<p><strong>核心模块二：运动学人机接触对齐</strong><br>由于人手与机器人手存在结构差异（如手指数量、关节配置不同），无法直接将类人接触表示迁移到机器人手。CEDex的核心创新在于提出了一种<strong>拓扑合并</strong>策略，在物体层面将多个人手部件（如无名指和小指）整合为更少的机器人部件（如一个夹持器手指），以实现运动学对齐。</p>
<p><img src="https://arxiv.org/html/2509.24661v1/x3.png" alt="人机映射"></p>
<blockquote>
<p><strong>图3</strong>：用于将人手部件与机器人手部件对齐的人机映射。(a) 原始人手 (b) Shadow手 (c) Allegro (d) Robotiq-3F (e) Barrett。提供了颜色索引参考。</p>
</blockquote>
<p>首先，如图3所示，根据目标机器人的运动学配置预定义一个人机映射。然后，对于需要合并的两个人手部件i和j，算法提取其对应的部件特定接触图 <strong>C^i</strong> 和 <strong>C^j</strong>。关键步骤是<strong>重映射</strong>：为每个接触点 <strong>o_x</strong> 计算一个投影方向 <strong>v_x</strong>，该方向指向两个部件接触质心之间的连线方向与物体质心到该点方向的合成，旨在将空间上分离的两个接触区域合并为一个能被单个机器人部件有效触及的统一区域。重映射后的位置 <strong>o_x&#39;</strong> 是物体点云中沿该方向射线上的最近点。对两个部件对称地执行此重映射后，通过取两个重映射接触对的并集来获得合并后的接触 **[C^m, P^m]**。对于超过两个部件的合并，此过程迭代进行。最终得到机器人特定的接触表示 **[C^r, P^r]**。</p>
<p>随后，基于此机器人接触表示进行SDF抓取优化。接触损失 <strong>L_c</strong> 促使机器人手部件向指定的接触区域靠近。</p>
<p><strong>核心模块三：物理感知约束</strong><br>为确保机器人抓取的稳定性，在优化中引入了三项物理约束：表面拉力（SPF）损失促使手部靠近物体表面；外部穿透排斥力（ERF）损失防止手与物体碰撞；自穿透排斥力（SRF）损失防止手指间自相交。这些约束增强了抓取的物理合理性和鲁棒性。</p>
<p><img src="https://arxiv.org/html/2509.24661v1/x4.png" alt="物理约束示例"></p>
<blockquote>
<p><strong>图4</strong>：Allegro手抓取灯泡的示例，展示了由物理感知约束（包括表面拉力和防止穿透）提供的鲁棒性和稳定性。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在包含10个来自ContactDB和YCB数据集的未见日常物体的测试集上进行评估。对比的机器人手包括Barrett（3指）、Allegro（4指）和Shadow手（5指）。对比的基线方法包括优化类方法DFC和GenDexGrasp（即MultiDex的方法），以及学习类方法GeoMatch、GeoMatch++和DRO-Grasp。</p>
<p><strong>评估指标</strong>：主要使用<strong>成功率</strong>（在Isaac Gym模拟器中，对物体施加多方向力后位移小于阈值）和<strong>多样性</strong>（抓取姿态在平移和旋转空间中的方差）。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2509.24661v1/x5.png" alt="实验结果"></p>
<blockquote>
<p><strong>图5</strong>：在Barrett、Allegro和Shadow手上的成功率与多样性对比。CEDex在所有机器人手上均取得了最高的成功率，并且在Barrett和Allegro手上实现了最佳的多样性权衡。</p>
</blockquote>
<p>从图5的定量结果看，CEDex在Barrett、Allegro和Shadow手上分别取得了<strong>93.3%、96.7%和90.0%</strong> 的成功率，全面超越了所有基线方法。在多样性方面，CEDex在Barrett和Allegro手上也表现最佳，在Shadow手上与DRO-Grasp相当但成功率更高。这表明CEDex能生成既稳定又多样的抓取。</p>
<p><img src="https://arxiv.org/html/2509.24661v1/x6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图6</strong>：在未见物体上的跨形态抓取生成定性结果。CEDex能为不同形态的机器人手生成物理合理且稳定的抓取。</p>
</blockquote>
<p>图6的定性结果直观展示了CEDex为不同机器人手在复杂物体上生成的抓取姿态，证明了其跨形态的有效性。</p>
<p><strong>消融实验</strong>：论文对CEDex的三个关键组件进行了消融研究。结果表明：1) <strong>使用生成的类人接触</strong>相比随机初始化或基于物理的接触，能显著提升成功率（+20%以上）和多样性；2) <strong>拓扑合并</strong>策略对于将类人接触有效适配到非拟人机器人手至关重要，移除后性能大幅下降；3) <strong>物理感知约束</strong>（SPF、ERF、SRF）对于确保最终抓取的物理稳定性和无碰撞至关重要，缺少任何一项都会导致成功率下降。</p>
<p><strong>数据集规模</strong>：<br><img src="https://arxiv.org/html/2509.24661v1/x1.png" alt="数据集对比"></p>
<blockquote>
<p><strong>表1</strong>：灵巧抓取数据集对比。CEDex数据集在已知范围内实现了跨形态灵巧手的最大规模，同时兼顾了类人运动学和运动学物理感知。</p>
</blockquote>
<p>如表1所示，利用CEDex方法构建的数据集包含<strong>50万</strong>个物体（来自Objaverse和真实数据集）和<strong>2000万</strong>个抓取（涵盖Barrett、Robotiq-3F、Allegro、Shadow四类手），是当前最大规模的跨形态灵巧抓取数据集，且首次同时包含了类人运动学和物理感知。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了CEDex，一种新颖的大规模跨形态灵巧抓取合成方法，通过将机器人运动学模型与生成的类人接触表示对齐，首次有效融合了类人运动学理解与物理约束。</li>
<li>提出了<strong>拓扑合并</strong>这一关键创新，解决了非拟人机器人手适配类人接触的难题。</li>
<li>构建了迄今最大规模的跨形态灵巧抓取数据集（500K物体，20M抓取），为相关研究提供了宝贵资源。</li>
</ol>
<p><strong>局限性</strong>：论文提到，CEDex的性能部分依赖于物体点云的质量和覆盖度。此外，抓取优化过程需要采样初始手腕姿态，虽然通过多样性采样策略缓解，但优化效率仍有提升空间。</p>
<p><strong>对后续研究的启示</strong>：CEDex证明了利用高效的类人先验（通过生成模型获得）来引导跨形态抓取合成的有效性。这为通用抓取研究开辟了新路径，即不再局限于昂贵的真人示范数据或纯物理优化。未来工作可以探索更高效的优化器、将方法扩展到包括抓取序列的动态抓取、以及利用此大规模数据集训练更强大的通用抓取策略模型。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>CEDex旨在解决跨形态灵巧抓取生成问题，即如何为不同形态的机器人手自适应生成高质量抓取。其核心技术是：首先通过预训练于人类接触数据的条件变分自编码器生成类人接触表示；随后通过拓扑合并将人手部件对齐至机器人运动学模型，并利用带物理约束的符号距离场进行抓取优化。该方法构建了迄今最大的跨形态抓取数据集（涵盖50万物体、四种夹持器、总计2000万抓取），实验验证其性能优于现有先进方法，且数据集能有效促进跨形态抓取学习。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.24661" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>