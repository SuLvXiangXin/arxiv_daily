<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.10359" target="_blank" rel="noreferrer">2506.10359</a></span>
        <span>作者: Kapil Katyal Team</span>
        <span>日期: 2025-06-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人从无序堆叠中抓取物品是仓储自动化等实际场景中的重要挑战。现有方法需对开放物品集保持鲁棒性，同时满足低延迟要求以实现高吞吐量。主流数据驱动方法，如针对吸盘抓取的方法，通常依赖专家设计的特征来评估抓取候选点。先前工作[20]表明，在包裹抓取任务中，使用专家特征的浅层模型（XGBoost）性能优于未经调优的深度卷积网络。然而，在更具挑战性的开放集物品抓取场景中，依赖人工特征限制了方法的自动化与泛化能力。本文旨在解决这一痛点：如何利用工业规模的真实世界数据，通过深度学习自动学习有效的多模态表征，以替代专家特征并提升抓取成功率预测性能。核心思路是：通过多模态（RGB、深度、语义）自监督预训练与下游抓取成功率预测微调相结合，使深度模型能够自动学习对抓取任务至关重要的视觉表征，从而超越基于专家特征的浅层模型。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法是一个两阶段训练流程：首先在大量无标签的真实场景多模态数据上进行自监督预训练，然后利用稀疏的成功/失败标签数据对模型进行微调，以预测特定抓取候选点的成功率。</p>
<p><img src="https://arxiv.org/html/2506.10359v1/extracted/6535122/figures/2025/rss2025_top2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧为输入的多模态图像（RGB、深度、抓取位置、语义分割），可裁剪为局部图像（右侧）。多模态视觉编码器（MultiMAE）学习场景表征，随后通过交叉注意力机制与抓取特征结合，最后由预测头输出抓取成功率。带<code>*</code>的模态为默认使用，带<code>†</code>的模态用于预训练，带<code>(opt.)</code>的模态在微调/推理中可选但能提升性能。</p>
</blockquote>
<p><strong>核心模块与技术细节：</strong></p>
<ol>
<li><strong>多模态视觉编码器</strong>：采用多模态多任务掩码自编码器（MultiMAE）。它接收RGB、深度和语义分割图像作为输入。语义图像通过预训练的分割模型获得，包含9个物品语义类别，并下采样至56x56以降低计算成本。预训练阶段，模型通过随机掩码图像块并重建像素来进行自监督学习，目标是学习不同模态间的关联信息。</li>
<li><strong>抓取特征编码与融合</strong>：抓取候选点由一系列特征定义，如<code>(X, Y, Z)</code>坐标、吸盘激活状态、末端执行器朝向等。这些特征被编码为一个向量。关键创新在于使用<strong>交叉注意力机制</strong>（见图4底部）来融合视觉表征与抓取特征，而非简单的平均池化。抓取特征被投影为查询（<code>Q</code>），视觉令牌被投影为键（<code>K</code>）和值（<code>V</code>），通过注意力权重对视觉令牌进行加权求和，使模型能够根据具体的抓取动作“查询”相关的视觉信息。</li>
<li><strong>训练流程</strong>：<ul>
<li><strong>预训练</strong>：在34.3万张无标签的多模态场景图像上，使用像素重建目标训练MultiMAE，初始化权重来自ImageNet上预训练的公开版本。</li>
<li><strong>微调</strong>：在27.5万条带成功/失败标签的数据上，以抓取成功率预测为目标训练整个模型（包括更新MultiMAE编码器的权重）。使用加权损失处理正负样本不均衡（约11:1）。输入图像可根据目标物品的分割掩码进行<strong>局部裁剪</strong>，并添加随机偏移以增强鲁棒性。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.10359v1/extracted/6535122/figures/token_weighting_alone.png" alt="Token加权方式对比"></p>
<blockquote>
<p><strong>图4</strong>：两种令牌加权方式对比。上图：对所有视觉令牌进行平均池化。下图：使用交叉注意力，将编码后的抓取特征作为查询，对视觉令牌进行加权求和。</p>
</blockquote>
<p><strong>与现有方法的创新点：</strong></p>
<ol>
<li><strong>领域内预训练与微调</strong>：不同于许多工作直接使用在通用数据上预训练并冻结的视觉编码器，本文强调利用工业部署产生的大规模<strong>领域内数据</strong>进行预训练和微调，带来了显著的性能提升。</li>
<li><strong>多模态的有效利用</strong>：系统性地利用了RGB、深度和语义信息，并通过MultiMAE的预训练让模型学习模态间的关系。有趣的是，在微调和推理时，即使只使用部分模态（如RGB和深度），模型仍能受益于多模态预训练学到的表征。</li>
<li><strong>交叉注意力融合</strong>：提出使用交叉注意力机制来关联视觉场景表征与结构化的抓取动作特征，这是一种更灵活的融合方式。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：<ol>
<li><strong>标准数据集</strong>：包含34.3万张图像用于预训练，27.5万条标注数据用于微调，测试集8.6万条。物品大部分未被遮挡。</li>
<li><strong>随机数据集</strong>：规模较小（训练8461条），包含更多针对部分遮挡物品的随机抓取尝试，用于测试模型泛化性。</li>
<li><strong>包裹抓取数据集</strong>：物品为盒子、信封等包裹，用于测试跨领域性能。</li>
</ol>
</li>
<li><strong>评估指标</strong>：ROC曲线下面积（AUC），适用于类别不平衡数据。</li>
<li><strong>对比基线</strong>：1) 使用专家特征的浅层模型（XGBoost）[20]；2) 使用通用数据（ImageNet, DINO, MVP）预训练且编码器冻结的ViT模型；3) 未预训练的ViT；4) 单模态MAE基线；5) 基于PointTransformerV3的点云基线。</li>
</ul>
<p><strong>关键实验结果：</strong></p>
<ol>
<li><p><strong>标准数据集（基本无遮挡）</strong>：<br><img src="https://arxiv.org/html/2506.10359v1/extracted/6535122/figures/2025/sparrow_main_rss_2025_test_auc.png" alt="主要性能对比"></p>
<blockquote>
<p><strong>图6</strong>：在标准数据集上的性能对比。本文方法（Demonstrated）测试AUC达到<strong>90.6</strong>，显著优于单模态MAE基线（79.1）和点云基线PTv3（84.6），并且超越了依赖专家特征的浅层模型（86.1），提升约4.5个AUC点。</p>
</blockquote>
</li>
<li><p><strong>随机数据集（部分遮挡）</strong>：</p>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">预训练(PT)</th>
<th align="left">微调(FT)</th>
<th align="left">测试</th>
<th align="left">性能(AUC)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Shallow</td>
<td align="left">-</td>
<td align="left">RND</td>
<td align="left">RND</td>
<td align="left">82.92</td>
</tr>
<tr>
<td align="left"><strong>Demonstrated</strong></td>
<td align="left">STD</td>
<td align="left">RND</td>
<td align="left">RND</td>
<td align="left"><strong>86.28</strong></td>
</tr>
<tr>
<td align="left">Shallow</td>
<td align="left">-</td>
<td align="left">STD</td>
<td align="left">RND</td>
<td align="left">83.58</td>
</tr>
<tr>
<td align="left"><strong>Demonstrated</strong></td>
<td align="left">STD</td>
<td align="left">STD</td>
<td align="left">RND</td>
<td align="left"><strong>85.87</strong></td>
</tr>
<tr>
<td align="left">Shallow</td>
<td align="left">-</td>
<td align="left">BOTH</td>
<td align="left">RND</td>
<td align="left">85.16</td>
</tr>
<tr>
<td align="left"><strong>Demonstrated</strong></td>
<td align="left">STD</td>
<td align="left">BOTH</td>
<td align="left">RND</td>
<td align="left"><strong>88.06</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：在随机数据集（部分遮挡）上的性能对比。本文方法在三种不同的训练/测试设置下均一致性地优于浅层模型，表明其学习到的表征对不同物品配置（遮挡情况）更具鲁棒性。</p>
</blockquote>
</li>
<li><p><strong>包裹抓取数据集</strong>：本文方法同样适用于包裹抓取任务，性能优于浅层模型，证明了其跨任务泛化能力。</p>
</li>
</ol>
<p><strong>消融实验与核心发现</strong>：</p>
<ul>
<li><strong>多模态预训练的重要性</strong>：使用RGB、深度、语义进行预训练（R-D-S）相比仅用RGB预训练，带来了巨大的性能提升（AUC从~79提升至&gt;90）。</li>
<li><strong>领域内预训练数据量</strong>：即使仅使用1%的领域内数据进行多模态预训练，也能获得大部分性能增益。</li>
<li><strong>微调策略</strong>：微调时更新MultiMAE编码器的权重至关重要。使用<strong>交叉注意力</strong>进行令牌加权优于<strong>平均池化</strong>。</li>
<li><strong>输入处理</strong>：对图像进行<strong>局部裁剪</strong>能进一步提升性能。在微调/推理阶段，即使<strong>仅使用RGB和深度</strong>（而不用语义），模型也能取得最佳性能，这得益于预训练阶段学习的跨模态关联。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.10359v1/extracted/6535122/figures/cm/rss_cm1.png" alt="多模态重建示例"></p>
<blockquote>
<p><strong>图5</strong>：MultiMAE在多模态数据上的重建示例。左列为真实图像，右列为模型对随机掩码输入的重建结果，展示了其学习到的跨模态理解能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种结合多模态（RGB、深度、语义）自监督预训练与下游任务微调的框架，成功替代了机器人抓取成功率预测中对专家设计特征的依赖，并在开放集物品抓取任务上超越了先前最佳的浅层模型。</li>
<li>通过在大规模真实工业数据集上进行全面实验，验证了该方法在标准物品抓取、部分遮挡物品抓取以及包裹抓取等多种场景下的有效性和鲁棒性。</li>
<li>进行了广泛的消融研究，揭示了领域内预训练、多模态学习、交叉注意力融合以及局部裁剪等技术选择的关键作用，为大规模机器人抓取提供了系列实用见解（例如，少量领域内预训练数据即可带来大部分收益）。</li>
</ol>
<p><strong>局限性</strong>：论文主要聚焦于2D多模态数据。尽管与点云基线进行了对比，但未来工作可进一步探索3D模态的预训练以及2D与3D模态的结合。</p>
<p><strong>启示</strong>：</p>
<ul>
<li>对于拥有大规模领域内数据的工业机器人应用，<strong>领域特定的预训练</strong>比直接使用通用的、冻结的预训练模型更具价值。</li>
<li><strong>多模态自监督学习</strong>是利用未标注工业数据、学习任务相关表征的有效途径。</li>
<li>通过适当的架构设计（如交叉注意力），可以优雅地融合非结构化的视觉信息与结构化的机器人动作参数。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人从杂乱堆中拾取多样化物品的挑战，提出通过多模态学习预测多吸盘拾取成功率的方法。核心技术是利用RGB、深度和语义分割等多模态输入，先通过自监督预训练学习跨模态关系，再微调下游模型评估拾取质量。实验表明，该方法在大型物品拾取数据集上优于原有工程策略及其他学习方案，实现了性能提升，并验证了多模态预训练的重要性及推理时可仅使用部分模态的灵活性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.10359" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>