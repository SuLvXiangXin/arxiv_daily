<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.23055" target="_blank" rel="noreferrer">2511.23055</a></span>
        <span>作者: Zhang, Ruoxuan, Zheng, Qiyun, Zhou, Zhiyu, Liao, Ziqi, Wu, Siyu, Jiang-Lin, Jian-Yu, Wen, Bin, Xie, Hongxia, Fu, Jianlong, Cheng, Wen-Huang</span>
        <span>日期: 2025/11/28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉-语言模型（VLM）的具身智能体在自主执行任务方面取得了进展，例如将复杂目标分解为子任务并逐步执行。然而，这些系统主要依赖预定义的目标或模仿信号，缺乏从自身视角出发的心理理论（Theory of Mind， ToM）推理能力。心理理论指推断他人心理状态（如信念、欲望、意图）的能力，是实现真正人机协作的前提。现有的ToM基准测试（如MuMA-ToM、MMToM-QA）虽然赋予VLM一定的心理推理能力，但通常局限于通过选择题等形式推理视频中人类的心理状态，未能建立从智能体自身视角（Robot-Centric Perspective）出发的推理。这导致VLM无法学习做出决策和生成行动，阻碍了连贯的决策与行动生成。</p>
<p>本文针对现有VLM具身智能体缺乏基于ToM的决策、以及现有基准忽略智能体自身视角这一关键痛点，提出了一个名为MindPower的<strong>机器人中心</strong>（Robot-Centric）框架。该框架的核心思路是：构建一个集成了感知、心理推理、决策与行动的统一推理层次结构，并通过一种新颖的优化目标（Mind-Reward）来鼓励VLM产生一致的心理推理和行为。</p>
<h2 id="方法详解">方法详解</h2>
<p>MindPower的整体框架是一个机器人中心的流程，包含<strong>感知</strong>、<strong>心理推理</strong>、<strong>决策与行动</strong>三个阶段。给定多模态输入（视频），框架首先感知环境和人类状态，然后执行ToM推理以同时对自身和他人进行心理建模，最后根据推断出的心理状态生成决策和行动。</p>
<p><img src="https://arxiv.org/html/2511.23055v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MindPower推理层次结构。智能体首先接收多模态输入，然后进行心理推理以形成信念、欲望和意图，最后基于此推理做出决策并生成行动计划。</p>
</blockquote>
<p>该框架的核心是 <strong>MindPower推理层次结构</strong>（MindPower Reasoning Hierarchy），它将推理过程结构化分为三个层级、共六个层面：</p>
<ol>
<li><strong>层级一：感知</strong>。<code>&lt;Perception&gt;</code>：观察环境，回答“现在发生了什么？”。</li>
<li><strong>层级二：心理推理</strong>。包含三个层面：<ul>
<li><code>&lt;Belief&gt;</code>：基于感知推理人类和环境状态。关键创新在于建模<strong>二阶信念</strong>，即智能体不仅推断自身信念，还预测场景中人类的信念。</li>
<li><code>&lt;Desire&gt;</code>：从信念推导出偏好状态或目标。对于辅助型智能体，其欲望由帮助人类的目标所塑造。</li>
<li><code>&lt;Intention&gt;</code>：基于信念和欲望形成的具体行动承诺。</li>
</ul>
</li>
<li><strong>层级三：决策与行动</strong>。<ul>
<li><code>&lt;Decision&gt;</code>：为履行意图而做出的选择或计划。</li>
<li><code>&lt;Action&gt;</code>：动作执行序列，以<code>动作（对象）</code>的高级原子操作形式执行，例如<code>打开（冰箱）</code>。</li>
</ul>
</li>
</ol>
<p>与现有方法相比，MindPower的核心创新点在于引入了<strong>机器人中心视角</strong>，要求智能体同时推理自身和人类的心理状态，并最终导向自主行动。</p>
<p>为了训练VLM学习这种一致的推理，本文提出了 <strong>Mind-Reward优化方法</strong>。该方法采用类似Visual-RFT的两阶段训练范式：先进行监督微调（SFT），然后使用<strong>组相对策略优化</strong>（GRPO）结合提出的奖励进行优化。</p>
<p><img src="https://arxiv.org/html/2511.23055v1/x5.png" alt="奖励公式"></p>
<blockquote>
<p><strong>图5</strong>：奖励公式。总体奖励整合了Mind-Reward和Format-Reward两个部分。</p>
</blockquote>
<p>奖励由两部分构成：</p>
<ol>
<li><strong>Mind-Reward</strong>：评估推理质量，包含三个加权部分：<ul>
<li><strong>原子准确性</strong>（Atomic Accuracy）：使用ROUGE-1衡量正确匹配的原子动作比例，每个动作都带有视角属性（人或智能体）以确保机器人中心视角对齐。</li>
<li><strong>局部一致性</strong>（Local Consistency）：使用ROUGE-2评估相邻原子对之间的短程推理连贯性。</li>
<li><strong>全局一致性</strong>（Global Consistency）：使用ROUGE-L评估整个推理过程的整体对齐程度。</li>
</ul>
</li>
<li><strong>Format-Reward</strong>：通过顺序正则表达式匹配检查六个推理层面是否以正确顺序出现。</li>
</ol>
<p>GRPO阶段，模型通过最大化结合了上述奖励和KL散度约束的目标函数来更新策略，从而促进跨推理层级的BDI一致性和机器人中心最优性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在提出的 <strong>MindPower基准测试</strong>上进行了实验。该基准包含590个场景，使用VirtualHome和ThreeDWorld家庭环境模拟器构建，涵盖两种核心推理任务：<strong>错误信念纠正</strong>（False-Belief Correction）和<strong>隐式目标推断与完成</strong>（Implicit Goal Inference &amp; Completion）。</p>
<p>对比的基线方法包括闭源模型（GPT-4o, Gemini-2.5 Pro/Flash）和开源模型（Qwen2.5-VL-7B, Video-LLaVA, VideoChat-R1等），并以人类研究结果作为上限参考。</p>
<p>关键定量实验结果总结如下：</p>
<ul>
<li>人类参与者在所有层面均表现最佳，显著优于所有VLM。</li>
<li>在闭源VLM中，Gemini-2.5 Pro和GPT-4o在感知、心理推理、决策与行动上得分最高。</li>
<li><strong>本文提出的模型（SFT+Mind-Reward）在决策和行动生成上显著优于最强的闭源基线GPT-4o</strong>：决策准确率（以Sentence Transformer相似度计）从GPT-4o的34.35提升至47.12（相对提升约37.2%），动作生成正确率（AC）从2.91%提升至15.40%（相对提升约429%）。论文摘要中所述的“ outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation”应为基于特定计算方式的相对提升。</li>
<li>在衡量BDI和视角一致性的BPC分数上，本文模型也达到了最高水平（8.87）。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.23055v1/x4.png" alt="实验结果"></p>
<blockquote>
<p><strong>图4</strong>：MindPower基准测试上的实验。(a) 不同推理方法的实验。移除MindPower推理层次结构或使用标准逐步推理（<code>&lt;think&gt; ... &lt;/think&gt;</code>）都会导致性能显著下降，证明了该层次结构的有效性。(b) 模型在机器人中心（RC）得分上的比较。开源VLM与人类推理相比仍有很大差距。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.23055v1/full_compare.png" alt="定量评估表"></p>
<blockquote>
<p><strong>表2</strong>：定量评估结果表。展示了各模型在六个推理层面及BPC上的详细得分。本文模型（最后一行）在行动生成（AC）和多个推理层面取得了最佳或接近最佳的性能。</p>
</blockquote>
<p>消融实验证明了各组件的重要性：</p>
<ol>
<li><strong>MindPower推理层次结构</strong>：移除该结构（直接输出决策和行动）会使GPT-4o的决策准确率下降1.24%，动作生成准确率从2.91%大幅降至0.82%。</li>
<li><strong>标准推理 vs. 层次推理</strong>：使用标准逐步推理代替MindPower层次结构，导致GPT-4o的决策准确率下降4.89%，动作准确率从2.91%降至0.90%。</li>
<li><strong>Mind-Reward的作用</strong>：仅使用SFT的模型性能已大幅提升，但加入Mind-Reward进行GRPO优化后，在决策和行动生成等关键指标上获得了进一步显著提升（例如AC从10.48%提升至15.40%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.23055v1/x6.png" alt="定性比较"></p>
<blockquote>
<p><strong>图6</strong>：定性评估。与GPT-4o和Qwen2.5-VL-7B-Instruct的比较示例。GPT-4o虽然格式正确，但错误推断人类意图为打开冰箱；Qwen2.5-VL则格式和推理均错误。本文模型正确推断出人类在寻找物品并生成了协助行动。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了一个机器人中心的感知基准</strong>（MindPower Benchmark），首次将心理推理与具身行动通过“错误信念纠正”和“隐式目标推断与完成”两大任务明确关联，包含590个交互式家庭场景。</li>
<li><strong>设计了一个统一的MindPower推理层次结构</strong>，以标准化、可解释的方式桥接了从感知到行动的完整推理链条，包含三个层级六个层面。</li>
<li><strong>提出了用于一致ToM推理的强化优化方法（Mind-Reward）</strong>，通过对齐中间推理状态与最终行动，促进了连贯的机器人中心推理，使模型性能超越顶尖闭源VLM。</li>
</ol>
<p>论文自身提到的局限性主要在于数据集的构建受限于模拟器能力，场景多样性可能不足，且未在真实物理机器人上进行验证。</p>
<p>这项工作对后续研究的重要启示在于：为构建具备社会智能的具身智能体指明了方向，即必须引入机器人中心的心理推理视角；其提出的层次化推理框架和一致性奖励机制，为如何让VLM产生更连贯、可解释且面向行动的推理提供了可行的技术路径。未来的工作可以在此基础上探索更复杂的多轮心理状态更新、在真实世界中的泛化，以及更高效的多模态推理模型架构。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前基于视觉语言模型（VLM）的具身智能体缺乏心理理论（ToM）推理能力、且现有基准忽略智能体自身视角的问题，提出了**MindPower框架**。该框架以机器人为中心，整合**感知、心理推理、决策与行动**模块，并引入**Mind-Reward优化目标**以促使VLM产生一致的ToM推理和行为。实验表明，该模型在决策制定和行动生成任务上分别比GPT-4o提升了12.77%和12.49%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.23055" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>