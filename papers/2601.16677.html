<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.16677" target="_blank" rel="noreferrer">2601.16677</a></span>
        <span>作者: Álvaro Jesús López-López Team</span>
        <span>日期: 2026-01-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>深度强化学习（DRL）在机器人控制领域展现出潜力，但其样本效率低下的问题阻碍了工业应用，因为在现实世界中收集大量训练数据成本高昂、耗时巨大。利用虚拟环境训练DRL智能体是一种高效的替代方案，但学习到的策略迁移到真实环境时，会受到“模拟到现实”（sim-to-real）差距的阻碍。实现零次（zero-shot）迁移，即智能体无需在真实环境中进行任何额外调优即可直接部署，因其高效性和实用价值而备受期待。</p>
<p>目前解决sim-to-real差距的主流方法包括域随机化（DR）和域适应（DA）。域随机化通过在模拟环境中随机化大量视觉或物理属性，期望真实环境成为已见分布的一部分。然而，本文认为，对于所解决的工业设置和问题，域适应方法更为合适。域适应旨在融合虚拟域和真实域的特征，使智能体能够在这个混合域中进行有效训练。具体到视觉域适应，生成对抗网络（GAN）及其变体（如CycleGAN）常被用于图像到图像的翻译，以缩小视觉差距。但原始的CycleGAN等方法在图像翻译中常产生伪影（如颜色渗色、边缘扭曲），可能影响后续DRL策略的学习。</p>
<p>本文针对上述痛点，提出了一种新颖的域适应方法。核心思路是：设计一个改进的CycleGAN模型（StyleID-CycleGAN, SICGAN），将原始的虚拟观测图像翻译成具有真实视觉风格的“真实-合成”图像；然后，DRL智能体完全在虚拟环境中，利用这些翻译后的图像进行训练；训练完成后，智能体可以直接部署到真实机器人上，实现零次迁移。该方法旨在提供一个高效、可扩展且考虑工业实际约束（如有限的计算资源、单一外部摄像头）的完整解决方案。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的完整流程旨在实现从虚拟训练到真实部署的零次迁移。其核心是一个新颖的域适应模型——风格识别循环一致生成对抗网络（SICGAN）。</p>
<p><img src="https://arxiv.org/html/2601.16677v1/x1.png" alt="方法整体流程"></p>
<blockquote>
<p><strong>图1</strong>：面向工业应用的sim-to-real零次迁移通用流程。阶段1：创建虚拟环境并定义MDP。阶段2：训练SICGAN。利用训练好的SICGAN，将从原始虚拟数据生成的“真实-合成”观测用于训练DRL智能体。智能体在虚拟环境中训练和评估后，通过零次转移直接部署到真实环境。</p>
</blockquote>
<p>整个pipeline如图1所示，分为离线和在线两个阶段：</p>
<ol>
<li><strong>离线训练阶段</strong>：<ul>
<li><strong>SICGAN训练</strong>：首先，使用收集的未配对虚拟图像（源域）和真实图像（目标域）数据集，离线训练SICGAN模型。训练目标是学习从虚拟域到真实域的映射。</li>
<li><strong>DRL智能体训练</strong>：训练好的SICGAN（推理模式）被用来将虚拟环境中的原始观测实时翻译成“真实-合成”图像。DRL智能体则在虚拟环境中，以这些翻译后的图像作为观测进行训练。此时，智能体感知的是具有真实视觉风格的图像，但交互的动力学仍是虚拟的。</li>
</ul>
</li>
<li><strong>在线部署阶段</strong>：<ul>
<li><strong>零次迁移评估</strong>：经过虚拟训练和评估的DRL智能体被直接部署到真实机器人上。智能体接收来自真实摄像头的原始RGB图像作为输入，并输出动作控制机器人，整个过程无需任何在真实环境中的再训练或微调。</li>
</ul>
</li>
</ol>
<p><strong>SICGAN核心模块与创新点</strong>：<br>SICGAN是基于CycleGAN架构的改进，主要引入了两项关键增强技术以解决原始CycleGAN常见的伪影问题：</p>
<ol>
<li><strong>解调卷积（Demodulated Convolutions）</strong>：SICGAN使用了解调卷积来替代标准的批量归一化。该技术源自StyleGAN系列，首先根据输入相关的风格或特征向量对卷积权重进行调制（Modulation），使网络能适应特定图像属性；随后进行解调（Demodulation），通过通道级的归一化来稳定激活，防止信号幅度的不受控放大或抑制。这带来了更一致的特征变换，有效减少了图像翻译中的伪影。</li>
<li><strong>身份损失（Identity Loss）</strong>：除了CycleGAN原有的对抗损失和循环一致性损失，SICGAN额外引入了身份损失 ℒ_id。该损失鼓励生成器在处理来自目标域的样本时，尽可能保持原图的关键特征。公式为：ℒ_id = 𝔼_y∼p_𝒴[‖G(y) - y‖_1] + 𝔼_x∼p_𝒳[‖F(x) - x‖_1]，其中G和F是两个方向的生成器。这有助于促进语义和颜色的一致性，提高翻译图像的保真度。</li>
</ol>
<p>与现有方法（如基于视觉Transformer的UVCGANv2）相比，SICGAN的创新点在于其专注于通过相对简洁的ResNet生成器架构，结合解调卷积和身份损失，在保持训练稳定性和图像质量的同时，不过度增加模型复杂度和计算开销，更符合工业场景对效率的要求。</p>
<p><strong>DRL训练细节</strong>：<br>任务定义为机械臂接近随机放置物体的马尔可夫决策过程。智能体的观测是64×64像素的RGB图像（无本体感知信息），动作空间为机械臂末端执行器在X、Y、Z三个方向上的位移。成功标准是在训练时末端与目标物体中心距离小于5cm，评估时放宽至10cm（这是一种“过度学习”策略）。奖励函数设计为基于距离减少的稀疏奖励。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/任务</strong>：机械臂接近任务（抓取放置操作的第一阶段）。目标物体在工作空间内随机放置。</li>
<li><strong>数据集</strong>：采集未配对的虚拟图像（来自MuJoCo模拟器）和真实图像（来自Intel RealSense D435摄像头）用于训练SICGAN。</li>
<li><strong>实验平台</strong>：<ul>
<li><strong>机器人</strong>：主要使用ABB IRB120工业机械臂进行流程开发；使用UR3e协作机械臂进行跨平台验证。两者在形态、运动学和动力学上完全不同。</li>
<li><strong>仿真</strong>：MuJoCo物理引擎。</li>
<li><strong>评估辅助</strong>：使用125个ArUco标记的增强现实（AR）投影目标来高效评估智能体在工作空间各区域的性能。随后使用真实物体（红/黄/蓝乐高立方体、红白杯子）验证泛化能力。</li>
</ul>
</li>
<li><strong>Baseline方法</strong>：与原始CycleGAN、以及一个基于视觉Transformer的先进域适应方法UVCGANv2进行图像翻译质量的对比。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>图像翻译质量对比</strong>：论文通过定性对比（如图13）展示了SICGAN在将虚拟图像翻译为真实风格图像时，相比原始CycleGAN，显著减少了颜色失真和伪影，生成了更清晰、视觉上更一致的结果。与UVCGANv2相比，SICGAN的结果在视觉质量上具有竞争力，同时模型更简单。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.16677v1/x13.png" alt="图像翻译对比"></p>
<blockquote>
<p><strong>图13</strong>：不同方法的图像翻译结果示例。SICGAN生成的图像在减少伪影和保持视觉一致性方面优于原始CycleGAN，与UVCGANv2结果质量相当。</p>
</blockquote>
<ol start="2">
<li><p><strong>虚拟环境训练性能</strong>：在虚拟环境中，使用SICGAN翻译图像训练的DRL智能体取得了优异的成功率。对于IRB120，在5cm成功阈值下，训练成功率达到接近100%。对于UR3e，在10cm评估阈值下，成功率超过90%。</p>
</li>
<li><p><strong>零次迁移现实部署性能</strong>：</p>
<ul>
<li><p><strong>IRB120机器人</strong>：使用AR目标评估，智能体在工作空间大部分区域（内环、中环）的接近准确率超过95%，在外环区域也达到约90%。使用真实物体（乐高立方体、杯子）测试时，准确率均保持在95%以上，展示了良好的泛化能力。<br><img src="https://arxiv.org/html/2601.16677v1/x20.png" alt="IRB120现实结果"></p>
<blockquote>
<p><strong>图20</strong>：IRB120机器人使用不同真实物体进行零次迁移评估的成功率。对于所有测试物体，成功率均高于95%。</p>
</blockquote>
</li>
<li><p><strong>UR3e机器人（跨平台验证）</strong>：同样取得了 robust 的零次迁移性能。使用AR目标评估，在内环和中环区域准确率超过95%，外环区域约为85%。对真实物体的测试成功率也普遍高于90%。<br><img src="https://arxiv.org/html/2601.16677v1/x21.png" alt="UR3e现实结果"></p>
<blockquote>
<p><strong>图21</strong>：UR3e机器人使用不同真实物体进行零次迁移评估的成功率。对于大多数测试物体，成功率高于90%，验证了流程的可迁移性。</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>消融实验</strong>：论文通过消融实验验证了SICGAN中各个组件的贡献。实验表明，同时使用解调卷积和身份损失能获得最佳性能。移除身份损失会导致图像颜色保真度下降；而使用标准批量归一化代替解调卷积，则会引入明显的伪影，影响翻译质量。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出并成功验证了一个完整的sim-to-real零次迁移流程。该流程集成了新颖的SICGAN域适应模型和基于图像的DRL训练，并在两个完全不同的工业机械臂（ABB IRB120和UR3e）上得到了验证，证明了其可扩展性和实用性。</li>
<li>提出了SICGAN这一改进的CycleGAN模型。通过引入解调卷积和身份损失，有效减少了图像翻译中常见的伪影，生成了更高质量、更一致的“真实-合成”图像，为后续DRL策略的鲁棒性奠定了基础。</li>
<li>整个方案严格考虑了工业场景的约束（如单一外部摄像头、有限的计算资源），使得训练阶段可利用GPU加速，而部署阶段仅需推理，无需昂贵硬件，具有较高的工业应用可行性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：当前方法依赖于一个固定视角的外部摄像头。动态变化的视角或遮挡可能会对性能构成挑战。此外，实验验证的任务（接近操作）相对基础，更复杂的操作（如精确抓取、装配）需要进一步探索。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>架构简化与效率</strong>：SICGAN展示了通过精妙的改进（而非一味增加模型复杂度）也能在视觉域适应任务上取得优异性能，这对在资源受限的嵌入式或边缘机器人平台上部署具有启示意义。</li>
<li><strong>系统性验证的重要性</strong>：本文不仅在单一机器人上验证方法，还进行了跨机器人平台的验证，增强了结论的可靠性。未来的sim-to-real研究应鼓励进行类似的系统性跨平台测试。</li>
<li><strong>迈向更复杂任务</strong>：本工作为基于视觉的零次迁移建立了一个稳健的基线。未来的工作可以在此基础上，将流程扩展到包含抓取、操纵等更复杂的连续任务链中，并探索应对动态环境和视角变化的方法。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决深度强化学习（DRL）中因仿真与真实视觉差异导致的策略迁移难题，以实现无需真实环境再训练的零次部署。为此，论文提出一种基于**风格识别循环一致生成对抗网络（StyleID-CycleGAN）**的域适应方法，该网络将原始虚拟观测转换为具有真实风格的合成图像，从而在混合域中训练DRL智能体。实验在两个工业机器人上进行，验证了方法的有效性：智能体在仿真中成功率可达90%-100%，并在真实拾放任务中实现了**零次迁移，在大部分工作区域准确率超过95%**，且能泛化至不同颜色和形状的真实物体。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.16677" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>