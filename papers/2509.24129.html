<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.24129" target="_blank" rel="noreferrer">2509.24129</a></span>
        <span>作者: Kristen Grauman Team</span>
        <span>日期: 2025-09-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作的主流范式集中于改变物体的运动学状态，如抓取、放置、推或旋转。然而，大量现实世界任务涉及另一类物体状态改变（Object State Change, OSC），例如捣碎、涂抹或切片，这些任务中物体的物理和视觉状态（如形态、纹理、外观）发生渐进式改变，而其位置可能不变。此类任务对机器人提出了两大关键挑战：在表示层面，原始RGB观测将物体外观与状态变化信号纠缠在一起，阻碍了对进展的感知和跨物体的泛化；在学习层面，设计有效的奖励函数十分困难——稀疏的成功奖励难以指导探索，而基于全局场景嵌入的目标条件奖励函数（如LIV）又无法捕捉OSC任务所需的细粒度、增量式进展。本文针对OSC操作任务缺乏统一框架以及上述表示与奖励设计难题，提出了一个新颖视角：利用空间进展的视觉示能图来结构化地感知和驱动物体状态改变。其核心思路是，通过一个预训练的视觉模型将物体分割为“可操作”与“已转变”区域，并以此生成结构化的策略观察和密集的、基于空间进展的奖励，从而高效学习OSC操作策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>SPARTA框架的核心是利用空间进展物体状态变化分割图（Spatially Progressing Object Change segmentation maps, SPOC）作为统一的视觉表示，并在此基础上构建两种策略变体：基于强化学习的SPARTA-L和基于贪婪控制的SPARTA-G。</p>
<p><img src="https://arxiv.org/html/2509.24129v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：SPARTA框架总览。在每个时间步，策略以当前及过去的SPOC视觉示能图（分割图）以及机器人本体感知数据作为输入，预测机械臂末端执行器的位移动作。SPARTA支持两种策略变体：(a) SPARTA-L：使用基于SPOC的密集奖励训练的强化学习智能体；(b) SPARTA-G：基于局部可操作像素密度从8个离散方向中选择的贪婪策略。</p>
</blockquote>
<p><strong>整体流程与输入输出</strong>：在每个决策步骤，系统首先从固定相机获取RGB观测。该观测被送入SPOC视觉示能图生成模块，输出一个二值分割图，将物体区域标记为“可操作”（红色）或“已转变”（绿色）。策略（SPARTA-L或SPARTA-G）的观测空间由此SPOC图序列和机器人末端位置的本体感知信息共同构成。策略输出一个在物体表面2D流形上的连续位移动作（Δx, Δy）。到达目标位置后，执行一个与任务相关的运动基元（如涂抹的刷动、捣碎的下压或切片的下划）。</p>
<p><strong>核心模块1：SPOC视觉示能图生成</strong>。此模块将原始RGB观测转化为结构化的、任务相关的抽象表示。它基于计算机视觉中的SPOC任务，但为适应机器人实时控制进行了改进。</p>
<p><img src="https://arxiv.org/html/2509.24129v1/x3.png" alt="SPOC生成流程"></p>
<blockquote>
<p><strong>图3</strong>：SPOC示能图生成流程。(a) 使用Grounded-SAM从初始帧提取物体掩码。(b) 通过最远点采样生成物体内部区域，并使用带颜色覆盖层提示的GPT-4o将其分类为可操作或已转变。(c) 分类后，使用DeAOT跟踪器在后续帧中跟踪已转变区域，以保持时间一致性。</p>
</blockquote>
<p>具体而言，流程分为三步：1) 使用Grounded-SAM获取初始物体掩码；2) 在掩码内进行最远点采样，生成多个内部区域提示点，并利用强大的视觉语言模型（GPT-4o）根据任务描述（如“涂抹了番茄酱的面包”）将这些区域分类为“可操作”或“已转变”，这比使用CLIP更准确；3) 为提升实时性，仅在第一帧进行完整的GPT-4o查询（约5秒），后续帧使用高效的视频对象跟踪器（DeAOT）来传播已转变区域的掩码（约0.2秒/帧）。生成的SPOC图剥离了物体外观细节，仅保留状态进展信息，为策略提供了可泛化的观察空间。</p>
<p><strong>核心模块2：SPARTA-L（强化学习策略）</strong>。此模块旨在通过在线强化学习学习细粒度的自适应控制。其核心创新在于一个密集的、基于空间进展的奖励函数，该函数直接来源于SPOC图的变化：<br>$r_t = \alpha R^{spoc}_t + \beta R^{succ}<em>t + \eta R^{entropy}<em>t$<br>其中关键项 $R^{spoc}<em>t = \frac{A</em>{t+1}^{trf} - A</em>{t}^{trf}}{A</em>{t}^{act}}$，它量化了从上一时刻到当前时刻新转变的区域面积相对于剩余可操作面积的比例。这为智能体提供了每一步的增量进展反馈。$R^{succ}_t$是当物体超过95%区域被转变时提供的稀疏成功奖励，$R^{entropy}_t$鼓励探索。策略优化基于SERL框架，使用SAC算法直接在真实世界中进行无演示、无仿真的样本高效学习。</p>
<p><strong>核心模块3：SPARTA-G（贪婪策略）</strong>。这是一个无需训练、轻量级的非参数化控制器。在每一步，它基于当前SPOC图和末端位置，评估8个均匀离散方向。对于每个候选方向对应的终点邻域，计算其中“可操作”像素的密度，并选择密度最高的方向作为动作（公式3）。这实质上驱使工具朝向最可能产生进展的区域。该方法适用于工具作用面积大、控制精度要求相对较低的任务。</p>
<p><strong>创新点总结</strong>：1) <strong>统一的视觉进展表示</strong>：首次将SPOC分割图作为机器人OSC操作的核心抽象，解耦了外观与状态，促进了泛化。2) <strong>密集的、空间接地的奖励</strong>：利用SPOC图的变化自动生成密集奖励，解决了OSC任务中奖励设计难题，驱动了高效的在线RL。3) <strong>灵活的双策略框架</strong>：同一表示支持下，既可进行数据驱动的强化学习（SPARTA-L），也可实现快速贪婪控制（SPARTA-G），适应不同任务复杂度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实机器人平台（Franka Emika Panda）上评估了三个具有挑战性的OSC任务：涂抹（Spreading）、捣碎（Mashing）、切片（Slicing）。使用了10种多样的真实物体（包括面包、百吉饼、黄瓜、香蕉等，以及番茄酱、芥末酱等涂层物质）。每个任务在1.5-3小时的在线RL预算内进行训练，无需人类演示或仿真。</p>
<p><strong>对比方法</strong>：1) <strong>Random</strong>：在约束动作空间内均匀随机采样动作。2) <strong>Sparse</strong>：仅使用由GPT-4o判断的二元任务完成奖励。3) <strong>LIV</strong>：最先进的基于目标条件视觉表示的方法，使用自然语言任务描述计算奖励。</p>
<p><strong>评估指标</strong>：变换覆盖率，即一集结束时物体状态发生改变的区域面积百分比。</p>
<p><strong>关键定量结果</strong>：</p>
<p><img src="https://arxiv.org/html/2509.24129v1/x4.png" alt="实验结果表"></p>
<blockquote>
<p><strong>图4</strong>：SPARTA与基线方法在三个任务（涂抹、切片、捣碎）上的变换覆盖率对比结果表。SPARTA-L和SPARTA-G在所有任务和物体（无论是否在训练中见过）上均显著优于所有基线。</p>
</blockquote>
<p>如表I（图4）所示，SPARTA方法大幅领先。例如，在涂抹任务中，SPARTA-L在未见物体上达到63%的覆盖率，而Sparse基线仅为11%，LIV为16%。在捣碎任务中，SPARTA-G在未见物体上达到75%的覆盖率。总体而言，SPARTA-L在需要精细方向控制的任务（如涂抹）中表现最佳，而SPARTA-G在工具作用面积大的任务（如捣碎）中竞争力强甚至更优。所有基线方法（Random, Sparse, LIV）的性能均很差，覆盖率普遍低于20%，表明它们无法有效学习OSC任务。</p>
<p><strong>消融实验分析</strong>：</p>
<p><img src="https://arxiv.org/html/2509.24129v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融实验。(a) 奖励函数组件分析：完整的SPOC密集奖励（橙色）学习曲线平滑且快速上升，仅使用稀疏奖励（蓝色）则几乎无学习信号，仅使用面积变化奖励（绿色）不稳定。(b) 观察空间分析：使用SPOC图（橙色）比使用原始RGB图像（蓝色）学习更快、最终性能更高。</p>
</blockquote>
<p>图5的消融实验验证了核心组件的贡献：1) <strong>奖励设计</strong>：完整的SPOC密集奖励（包含$R^{spoc}_t$）对于稳定、高效的学习至关重要；仅使用稀疏奖励或仅使用面积变化（不含标准化）的奖励均导致性能大幅下降或不稳定。2) <strong>观察空间</strong>：使用SPOC图作为观察比使用原始RGB图像能带来更快的学习速度和更高的最终性能，证明了结构化表示的有效性。</p>
<p><strong>定性结果</strong>：</p>
<p><img src="https://arxiv.org/html/2509.24129v1/x6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图6</strong>：SPARTA-L策略在涂抹（面包+番茄酱）、切片（黄瓜）、捣碎（香蕉）任务上的定性执行序列。从左至右展示了任务执行过程中SPOC图（红/绿）的演变，直观显示了策略如何逐步将红色（可操作）区域转变为绿色（已转变）区域。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了首个用于物体状态改变操作任务的统一框架SPARTA，其核心是利用空间进展视觉示能图（SPOC）作为感知和奖励的基础。2) 设计了基于SPOC的密集奖励函数，使得无需演示或仿真即可在真实世界中进行样本高效的强化学习。3) 在同一个表示框架下实例化了两种策略变体（SPARTA-L和SPARTA-G），展示了该表示的灵活性，能够同时支持自适应学习和快速贪婪规划。</p>
<p><strong>局限性</strong>：论文提到，SPARTA的性能依赖于底层视觉模型（如GPT-4o）的准确性。此外，当前的动作空间被简化为物体表面的2D位移，并配合预定义的运动基元，对于更复杂的3D交互可能需要进行扩展。</p>
<p><strong>后续研究启示</strong>：SPARTA证明了“视觉空间进展”作为一种抽象表示对于一系列非刚性物体操作任务的强大潜力。这一范式可以扩展到更多的OSC任务（如打磨、绘画）或其他需要渐进式改变的场景。未来的工作可以探索如何进一步降低对大型VLM的依赖，或将空间进展表示与更复杂的动作规划相结合。此外，将触觉等多模态反馈与视觉进展表示融合，可能有助于处理视觉遮挡或外观变化不明显的任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中物体状态变化（如捣碎、涂抹、切片）这一核心问题，提出首个统一框架SPARTA。其关键技术是利用空间进展物体变化分割图，将物体区域划分为“可操作”与“已转变”状态，从而生成结构化策略观察和密集奖励。SPARTA提供两种策略变体：无需演示或仿真的强化学习精细控制，以及快速轻量部署的贪婪控制。实验在真实机器人上对10种不同物体执行3项挑战性任务，相比稀疏奖励和视觉目标条件基线，在训练时间和准确性上均取得显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.24129" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>