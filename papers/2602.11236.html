<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.11236" target="_blank" rel="noreferrer">2602.11236</a></span>
        <span>作者: Mu Xu Team</span>
        <span>日期: 2026-02-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，构建一个能够适配多种机器人硬件平台的通用具身智能体（“一个大脑，多种形态”）是机器人学的核心挑战。主流方法依赖于从视觉-语言模型（VLM）出发构建视觉-语言-动作（VLA）模型。然而，该领域面临三个根本性障碍：首先，数据规模不足，昂贵且依赖于特定硬件平台的数据收集限制了模型接触多样形态和任务分布；其次，数据质量参差不齐且缺乏标准化，不同数据集在动作表示、坐标系和控制频率上的不一致性阻碍了模型提取跨平台的通用模式；最后，现有预训练范式不匹配，VLM的视觉编码器侧重于语义识别而非三维结构或物理动态理解，而具身行为需要精细的空间理解和因果推理能力。</p>
<p>本文针对上述痛点，提出了一个系统性的解决方案：通过联合优化数据处理流程与模型架构，将异构的原始数据转化为统一、高效的表示。其核心思路是，首先构建一个大规模、高质量、多样化的统一数据集UniACT，并在此基础上提出“动作流形假设”，引入动作流形学习机制，使模型能够直接预测位于低维、平滑流形上的有效动作序列，从而提升策略的效率和稳定性。</p>
<h2 id="方法详解">方法详解</h2>
<p>ABot-M0的整体框架是一个两阶段训练范式，模型架构由VLM骨干网络和动作专家模块组成，并可选地集成三维感知模块以增强空间理解。</p>
<p><img src="https://arxiv.org/html/2602.11236v1/x2.png" alt="模型架构"></p>
<blockquote>
<p><strong>图3</strong>：ABot-M0模型架构。整体采用VLM与动作专家分离的两组件架构，并引入动作流形学习（AML）直接预测动作。通过双流特征交互，可选的三维模块（如VGGT、Qwen-Image-Edit）可以增强空间感知。</p>
</blockquote>
<p><strong>整体框架与组件</strong>：模型以Qwen3-VL作为VLM骨干，负责处理堆叠的多视角图像序列和自然语言指令，生成融合的多模态特征。动作专家则是一个基于扩散Transformer（DiT）的生成器，其输入包括VLM提取的特征、可选的3D模块提供的几何感知特征、当前机器人状态以及带噪声的动作块，其输出是预测的干净动作序列。这种设计实现了从多模态感知到机器人动作生成的端到端映射。</p>
<p><strong>核心创新：动作流形学习（AML）</strong>：本文的核心创新在于对动作生成方式的重新思考。传统基于扩散或流匹配的生成模型通常训练网络去预测噪声（ε-pred）或速度（v-pred），这些目标本身是高维且无结构的。</p>
<p><img src="https://arxiv.org/html/2602.11236v1/x3.png" alt="动作预测目标对比"></p>
<blockquote>
<p><strong>图4</strong>：不同动作预测目标对比。传统方法预测噪声（ε-pred）或速度（v-pred），而本文提出的AML直接预测动作（a-pred），使学习聚焦于动作流形本身。</p>
</blockquote>
<p>本文提出“动作流形假设”：有效的、连贯的机器人动作序列并非随机散布在高维动作空间中，而是位于一个由物理规律和任务约束所塑造的低维、平滑流形上。基于此，AML机制让DiT网络直接预测去噪后的动作块 A^_t，而非噪声或速度。具体而言，给定带噪声的动作块 A_t^τ = τ A_t + (1-τ)ε，网络 V_θ 直接预测原始动作块 A_t 的估计值 A^_t = V_θ(φ_t, A_t^τ, q_t)。尽管网络输出是动作，但损失函数计算在由动作推导出的“速度”上，这被证明能取得更好的性能。损失函数为加权均方误差：L(θ) = E[ w(τ) || V_θ(φ_t, A_t^τ, q_t) - A_t ||^2 ]，其中权重 w(τ) = 1/(1-τ)^2。在推理时，从纯噪声开始，通过求解常微分方程进行迭代去噪，最终生成动作序列。</p>
<p><strong>双流特征交互</strong>：为了弥补标准VLM在三维空间推理上的不足，模型设计了一个双流特征交互机制。主流通路由VLM提供语义和视觉特征。另一流通路是一个即插即用的感知模块，可以接入如VGGT（用于计算场景级结构特征）或Qwen-Image-Edit（用于融合多视角观测特征）等三维模型，向动作专家注入几何先验。这两个特征流进行交互，共同为动作预测提供信息。</p>
<p><strong>数据处理与标准化</strong>：方法的基础是统一的大规模数据集UniACT。其构建流程（图1）包括从六个开源数据集中进行数据清洗（过滤无效指令、视觉异常、动作异常等）和格式标准化。</p>
<p><img src="https://arxiv.org/html/2602.11236v1/figure/bot5.png" alt="数据处理流程"></p>
<blockquote>
<p><strong>图1</strong>：构建UniACT数据集的数据清洗和预处理流程。</p>
</blockquote>
<p>标准化的核心原则包括：1) <strong>动作表示</strong>：统一使用末端执行器坐标系下的增量动作（delta action），并将所有旋转表示转换为旋转向量（轴角表示），以增强稳定性和跨平台兼容性；2) <strong>单/双臂统一</strong>：采用“填充至双臂”策略，将单臂任务中未使用的手臂动作维度填充为零，使模型始终预测双臂动作序列，从而实现参数共享；3) <strong>数据分布</strong>：通过多粒度均匀采样来平衡不同数据集和机器人形态的数据比例，以促进跨形态泛化。</p>
<p><img src="https://arxiv.org/html/2602.11236v1/x1.png" alt="数据集概览"></p>
<blockquote>
<p><strong>图2</strong>：统一的UniACT数据集概览，包含超过600万条轨迹，9500+小时数据，涵盖20多种机器人形态。饼图展示了各数据集的规模比例，两侧展示了机器人、场景、任务和视觉风格的多样性。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在多个主流机器人操作基准上评估了ABot-M0，包括LIBERO、LIBERO-Plus、RoboCasa GR1桌面任务和Robotwin2.0。对比的基线方法包括π0.5、UniVLA和OpenVLA-OFT。</p>
<p><strong>关键定量结果</strong>：ABot-M0在四个基准测试上取得了显著优于基线方法的平均成功率：LIBERO (98.6%)、LIBERO-Plus (80.5%)、RoboCasa GR1 (58.3%)、Robotwin2.0 (81.2%)。这些结果验证了从数据整理到架构设计的完整流程的有效性。</p>
<p><img src="https://arxiv.org/html/2602.11236v1/x6.png" alt="基准测试结果"></p>
<blockquote>
<p><strong>图10</strong>：在LIBERO和LIBERO-Plus基准测试上的成功率对比。ABot-M0显著优于所有基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11236v1/x7.png" alt="基准测试结果"></p>
<blockquote>
<p><strong>图11</strong>：在RoboCasa GR1和Robotwin2.0基准测试上的成功率对比。ABot-M0同样表现出领先性能。</p>
</blockquote>
<p><strong>消融实验与分析</strong>：</p>
<ol>
<li><strong>动作预测目标</strong>：实验对比了ε-pred、v-pred和本文的a-pred（AML）。结果表明，a-pred在动作预测误差（MAE）和最终任务成功率上均优于其他两种目标。</li>
<li><strong>数据混合与采样策略</strong>：研究了不同数据集混合比例和采样策略的影响。采用均匀采样策略能有效平衡不同形态和任务的数据，获得最低的跨数据集平均误差（MAE）。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.11236v1/figure/sampling_ratio.png" alt="采样策略影响"></p>
<blockquote>
<p><strong>图7</strong>：不同数据集采样比例对模型在LIBERO基准上成功率的影响。均匀采样（Uniform）取得了最佳效果。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11236v1/figure/embodiment_mae.png" alt="跨形态误差"></p>
<blockquote>
<p><strong>图8</strong>：模型在不同机器人形态（Embodiment）上的动作预测平均绝对误差（MAE）。经过统一预训练的模型在不同形态上保持了较低且一致的误差。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11236v1/figure/dataset_mae.png" alt="跨数据集误差"></p>
<blockquote>
<p><strong>图9</strong>：模型在不同来源数据集上的动作预测平均绝对误差（MAE）。表明模型学到了跨数据集的通用技能。</p>
</blockquote>
<ol start="3">
<li><strong>三维模块贡献</strong>：实验验证了引入VGGT和Qwen-Image-Edit等三维感知模块能进一步提升模型在需要精细空间推理任务上的性能，且这些模块的收益是叠加的。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.11236v1/x5.png" alt="三维模块消融"></p>
<blockquote>
<p><strong>图6</strong>：不同三维感知模块（VGGT, Qwen-Image-Edit）在RoboCasa GR1任务上的消融实验结果。引入三维先验能显著提升性能，且效果可叠加。</p>
</blockquote>
<p><strong>定性结果</strong>：可视化结果（图5）展示了ABot-M0在多样任务上的执行能力，包括单臂抓取、双臂协调操作以及长视野任务，体现了其强大的泛化能力。</p>
<p><img src="https://arxiv.org/html/2602.11236v1/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：ABot-M0在多种机器人形态和任务上的定性展示，包括单臂和双臂操作场景。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) <strong>构建了UniACT大规模数据集</strong>：通过系统化的清洗和标准化流程，整合了多个异构开源数据集，为训练通用VLA模型提供了高质量、多样化的数据基础；2) <strong>提出了动作流形假设与AML机制</strong>：将生成目标从预测噪声转向直接预测位于低维流形上的干净动作，显著提升了动作预测的效率和学习稳定性；3) <strong>设计了双流特征交互架构</strong>：在保留VLM强大语义理解的同时，通过即插即用的三维模块注入几何先验，增强了模型的空间推理能力。</p>
<p>论文也指出了自身的局限性：首先，数据集的规模和多样性仍有提升空间，例如需要包含更多形态（如半人形、全人形）和更丰富的传感器信息；其次，模型性能在一定程度上依赖于高质量的三维先验模块；最后，动作预测的稳定性和长视野任务的规划能力仍是持续挑战。</p>
<p>这项工作为后续研究提供了重要启示：它证明了通过系统化工程整合公开异构数据，而非依赖私有数据，完全可以构建高性能、可泛化的具身智能基础模型。其提出的数据标准化范式、动作流形学习思想以及模块化增强感知的架构，为迈向真正的“一个大脑，多种形态”通用机器人指明了可行的技术路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ABot-M0框架，旨在解决构建跨多样硬件的通用体现智能体时面临的数据碎片化、表示不一致等核心挑战。关键技术包括：构建大规模统一数据集UniACT-dataset（含超600万轨迹），提出动作流形学习（AML）方法，基于低维流形假设使用DiT骨干网络直接预测连续动作序列以提升解码效率；同时通过双流感知机制整合VLM语义与几何先验，增强空间理解。实验表明，各组件独立运作且效益叠加，支持通用体现智能的发展。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.11236" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>