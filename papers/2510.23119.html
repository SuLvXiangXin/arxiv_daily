<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.23119" target="_blank" rel="noreferrer">2510.23119</a></span>
        <span>作者: Wei, Yi-Lin, Luo, Zhexi, Lin, Yuhao, Lin, Mu, Liang, Zhizhao, Chen, Shuoyu, Zheng, Wei-Shi</span>
        <span>日期: 2025/10/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧抓取是机器人学的核心挑战，长期目标是建立一个通用框架，使机器人能理解并执行各种物体、任务和环境下的指令。当前主流方法基于数据驱动的条件生成模型，利用视觉观测和用户指令生成抓取动作。然而，这些方法的泛化能力和“全能”（omni-capabilities）受到可用数据集规模和多样性的限制，难以可靠处理新物体类别、多样用户指令和不同灵巧手形态。</p>
<p>近期基础模型的发展为克服泛化挑战提供了新途径，其在大规模数据上训练，学习了与抓取和操作相关的知识。然而，直接将其应用于机器人领域仍面临挑战，主要源于基础模型的高层知识与机器人执行的低层物理约束之间的鸿沟，导致其难以直接生成可执行的机器人动作，且可能因模型幻觉产生物理上不可行的交互。</p>
<p>本文针对上述痛点，提出了一种结合基础模型与迁移控制策略的新框架。核心思路是：利用基础模型强大的泛化能力生成符合指令的人手抓取图像作为中间表示，再通过专门的迁移策略将其转化为可执行的机器人动作，并引入力感知自适应抓取策略来保证物理执行的鲁棒性和稳定性。</p>
<h2 id="方法详解">方法详解</h2>
<p>OmniDexGrasp框架的目标是：给定场景的单视角RGB-D观测（点云 $\mathcal{O}$ 和RGB图像 $\mathcal{I}^{obs}$）以及用户指令 $\mathcal{C}$，生成意图对齐且高质量的灵巧动作 $\mathcal{G}^{dex}=(T_{dex},J_{dex})$，其中 $T_{dex}$ 为手臂末端执行器的6D位姿，$J_{dex}$ 为灵巧手的关节角度。</p>
<p><img src="https://arxiv.org/html/2510.23119v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>: OmniDexGrasp框架总览。包含三个核心模块：(1) 通过基础生成模型生成人手抓取图像；(2) 通过人图到机器人动作迁移策略，将抓取图像映射为灵巧机器人动作；(3) 通过力感知自适应抓取策略实现稳定抓取。</p>
</blockquote>
<p>框架包含三个核心模块：</p>
<ol>
<li><p><strong>基于基础模型的抓取图像生成</strong>：利用基础生成模型，输入RGB观测 $\mathcal{I}^{obs}$ 和用户提示（支持语言指令、观测上的抓取点/区域掩码、演示图像等多种形式），生成人手抓取图像 $\mathcal{I}^{gen}$。采用精心设计的提示模板来提升生成质量。框架支持不同类型的生成模型（如图像生成模型GPT-Image、Qwen-Image，视频生成模型Kling、WanX），默认使用GPT-Image。</p>
</li>
<li><p><strong>人图到机器人动作迁移策略</strong>：该策略解决从生成的人手图像到可执行、物理可行的机器人动作的转换问题，包含三个子步骤：</p>
<ul>
<li><strong>手-物体重建</strong>：使用HaMeR从生成图像中重建人手的MANO参数（关节姿态 $J_{mano}$ 和手腕在相机坐标系下的6D位姿 $T^{gen}<em>{mano}$）；使用Hyper3D从原始观测图像 $\mathcal{I}^{obs}$ 重建物体网格 $M_o$ 并估计其尺度 $s_o$；使用MegaPose估计生成图像中物体的6D位姿 $T</em>{o}^{gen}$。为解决沿相机深度轴的估计误差，对 $T^{gen}<em>{\text{mano}}$ 的深度平移分量 $t_z$ 进行优化以对齐手-物体交互一致性。最后将优化后的手腕位姿转换到物体坐标系：$T^{obj}</em>{mano}={T_{o}^{gen}}^{-1}T^{gen}_{mano}$。</li>
<li><strong>灵巧重定向</strong>：通过两步优化将重建的MANO人手姿态 $\mathcal{G}^{obj}<em>{mano}$ 重定向到目标灵巧手姿态 $\mathcal{G}^{obj}</em>{dex}$。首先初始化手腕6D参数和结构相似关节的参数；然后优化目标 $\min \sum_{k}\left|p_{k}^{dex,ft}-p_{k}^{mano,ft}\right|_{2}^{2}$，使灵巧手与人的指尖位置对齐。</li>
<li><strong>可执行动作变换</strong>：将物体坐标系下的灵巧姿态 $\mathcal{G}<em>{dex}^{obj}$ 转换到真实的机器人手臂坐标系。首先使用FoundationPose估计真实观测 $\mathcal{I}^{obs}$ 中物体的6D位姿 $T</em>{o}^{obs}$；然后通过手眼标定矩阵 $\mathbf{T}<em>{c \rightarrow r}$，进行坐标变换：$\mathcal{G}</em>{\text{dex}}=\mathbf{T}<em>{c \rightarrow r} \cdot T</em>{o}^{obs} \cdot \mathcal{G}_{dex}^{obj}$。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.23119v1/x3.png" alt="模型幻觉示例"></p>
<blockquote>
<p><strong>图3</strong>: 基础生成模型的输入输出可视化，说明了生成图像中物体位姿可能发生改变。直接执行对应动作常导致失败，凸显了迁移策略的必要性。</p>
</blockquote>
</li>
<li><p><strong>力感知自适应抓取策略</strong>：为解决开环执行可能导致的抓取不稳或力过大损坏物体的问题，提出基于力约束位置控制的策略。首先通过优化获得预抓取位姿 $\mathcal{G}^{pre}<em>{dex}$（接触点沿法向外移5cm）和挤压抓取位姿 $\mathcal{G}^{squ}</em>{dex}$（接触点沿法向内移1cm）。抓取执行时，手指在PD控制器作用下从预抓取位姿向挤压抓取位姿移动，一旦手指接触力 $F(t)$ 达到由基础模型（如GPT-4o）预测的目标力 $F_{\text{target}}$，则立即锁定当前位置为目标，防止过度挤压。控制器输出为 $u(t) = \mathcal{C}(\mathcal{G}^{\text{target}}-\mathcal{G}(t))$，其中 $\mathcal{G}^{\text{target}}$ 根据当前力是否达到阈值在 $\mathcal{G}^{squ}_{dex}$ 和当前位姿 $\mathcal{G}(t)$ 间切换。</p>
</li>
</ol>
<p><strong>创新点</strong>：1) 利用基础模型生成<strong>人手</strong>抓取图像作为与机器人形态解耦的中间表示，充分利用其泛化能力。2) 设计了一套完整的从图像到可执行机器人动作的<strong>迁移流程</strong>，解决了模型幻觉和坐标对齐问题。3) 引入了<strong>力反馈闭环控制</strong>，使抓取能自适应物体特性，确保稳定和安全。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在真实世界使用超过40个物体，涵盖把手、喷雾瓶、液体容器等8个类别；在仿真中使用超过100个物体，来自33个类别，并分为“已见”、“相似”、“新颖”三组以评估泛化。</li>
<li><strong>实验平台</strong>：真实世界使用Kinova Gen3机械臂、RealSense D455相机和Inspire FTP灵巧手（6自由度）；仿真使用Shadow Hand（22自由度）。还定性测试了Leap Hand和RoboSense Papert Hand。</li>
<li><strong>评估指标</strong>：抓取成功率（Suc.）和意图一致性得分（Inten., 1-5分，由GPT-4o或人类专家评估）。</li>
<li><strong>对比基线</strong>：在语义抓取任务上，与AffordDex、DexGYS、SceneDiffuser等数据驱动方法进行对比。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>全能任务评估</strong>：在六大类抓取任务（语义抓取、区域/点抓取、杂乱环境目标抓取、人机交接、单次演示抓取、易碎物体抓取）上，OmniDexGrasp平均成功率达87.9%，平均意图一致性得分达4.14，展现了强大的适应性。</p>
<p><img src="https://arxiv.org/html/2510.23119v1/x5.png" alt="任务可视化"></p>
<blockquote>
<p><strong>图5</strong>: 六大任务（上）及完整抓取运动过程（下）的可视化。</p>
</blockquote>
</li>
<li><p><strong>消融实验</strong>：如表一所示，移除力感知策略（w/o Force）平均成功率下降约40%，且可能损坏物体；移除迁移策略（w/o Transfer）性能急剧下降，因模型幻觉导致动作不可行。证明了两个策略对鲁棒性和稳定性的关键贡献。</p>
<p><img src="https://arxiv.org/html/2510.23119v1/GreanWateringCan.png" alt="力策略消融"></p>
<blockquote>
<p><strong>图6</strong>: 力感知策略消融可视化。没有该策略，机器人抓取可能损坏物体（如图中水壶被捏扁）。</p>
</blockquote>
</li>
<li><p><strong>与现有方法对比</strong>：</p>
<ul>
<li><strong>真实实验</strong>（表二）：在8个物体类别上，OmniDexGrasp在成功率和意图一致性上全面优于在大型数据集上训练的AffordDex方法。</li>
<li><strong>仿真实验</strong>（表三）：在“已见”类别上，各方法性能相当；但在“相似”和“新颖”类别上，基线方法性能显著下降（平均分从4.59降至2.45和1.98），而OmniDexGrasp保持了稳健性能（稳定性得分分别为3.96和4.20，意图得分3.55和3.88），展现了卓越的跨类别泛化能力。</li>
</ul>
</li>
<li><p><strong>基础模型分析</strong>（表IV, 图7）：闭源模型（GPT-Image, Kling-Video）通常优于开源模型（Qwen-Image, Wan2.2）。图像模型指令跟随能力更强，视频模型在保持场景和物体位姿一致性方面更好。对开源模型进行任务特定微调可显著提升性能。</p>
<p><img src="https://arxiv.org/html/2510.23119v1/PlasticMug.png" alt="生成模型对比"></p>
<blockquote>
<p><strong>图7</strong>: 不同基础生成模型生成图像的可视化对比。</p>
</blockquote>
</li>
<li><p><strong>扩展性</strong>：定性实验表明，该框架可扩展至操作任务（如触发喷雾瓶），并能适配不同形态的灵巧手（Inspire Hand, Leap Hand, Papert Hand）。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了OmniDexGrasp框架，通过结合基础模型、动作迁移和力感知控制，首次实现了在用户指令、抓取任务和灵巧手形态上的“全能”泛化灵巧抓取。</li>
<li>设计了一套从基础模型生成的人手图像到可执行机器人动作的迁移策略，有效解决了模型幻觉与物理可行性之间的鸿沟。</li>
<li>引入了力感知自适应抓取策略，通过力反馈闭环控制实现了稳定、安全的抓取执行。</li>
</ol>
<p><strong>局限性</strong>：论文提到，框架性能部分依赖于基础生成模型的质量；迁移和重建步骤可能引入累积误差；力阈值预测依赖另一个基础模型。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>基础模型+物理约束</strong>是提升机器人泛化能力的有前景方向，如何更紧密地结合二者值得深入探索。</li>
<li><strong>人手作为中间表示</strong>有效解耦了任务泛化与机器人具体形态，这一思路可推广至其他需要跨形态迁移的技能学习。</li>
<li><strong>力反馈</strong>对于实现精细、安全的灵巧操作至关重要，将其集成到高层任务规划中是实现通用 embodied intelligence 的关键。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出OmniDexGrasp框架，旨在解决灵巧抓取任务泛化能力不足以及基础模型知识与物理执行之间存在差距的核心问题。关键技术包括：1）利用基础模型根据用户指令生成人类抓取图像以增强泛化；2）通过人图到机器人动作的迁移策略实现可执行动作生成；3）结合力感知自适应抓取策略确保稳定执行。实验在仿真和真实机器人上验证了该框架对多样化用户指令、抓取任务及灵巧手均有效，并展示了其向灵巧操作任务的可扩展性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.23119" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>