<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.07077" target="_blank" rel="noreferrer">2510.07077</a></span>
        <span>作者: Kawaharazuka, Kento, Oh, Jihoon, Yamada, Jun, Posner, Ingmar, Zhu, Yuke</span>
        <span>日期: 2025/10/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人领域正努力利用大型语言模型（LLMs）和视觉语言模型（VLMs）的进展。早期工作将LLMs和VLMs与负责动作生成的底层机器人策略解耦，虽然对有限预定义任务有效，但此类系统通常依赖选择固定的运动基元或通过模仿学习学得的策略，限制了其向更广泛任务泛化的能力。学习能够从当前观察和指令泛化到未见任务的策略仍然是一个重大挑战。</p>
<p>为了克服这些限制，越来越多的研究集中于视觉-语言-动作（VLA）模型。通过在端到端框架中联合学习视觉、语言和动作模态，VLA模型旨在使机器人执行更广泛的任务。由此产生的通用策略旨在实现跨多样化任务、物体、本体和环境的泛化，并促进跨不同机器人本体的有效迁移。这种方法减少了对大量任务特定数据收集和训练的需求，显著降低了真实世界部署的成本。因此，VLA为更具可扩展性和可访问性的机器人系统提供了一条有希望的路径。</p>
<p>本文提供了一个系统性的、全栈式的综述，整合了VLA系统的软件和硬件组件，旨在为机器人学界将VLA应用于真实世界机器人系统提供实用指导。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文对VLA模型的设计策略、架构和构建模块进行了系统性回顾。VLA模型涵盖了广泛的架构设计，反映了整合感知、指令和控制的不同策略。一个广泛采用的方法是传感器模型，它联合学习视觉、语言和动作表示。这些模型以图像和语言作为输入，直接输出动作，可以采用平面或分层结构以及不同的骨干架构。</p>
<p><img src="https://arxiv.org/html/2510.07077v1/x4.png" alt="传感器模型架构"></p>
<blockquote>
<p><strong>图4</strong>：VLA传感器模型的架构。该图分类了近期VLA研究中使用的七种代表性架构，展示了从基础Transformer到结合预训练VLM及不同动作生成头（离散、扩散、流匹配）的演进。</p>
</blockquote>
<p>传感器模型目前有七种架构变体，如图4所示：</p>
<ol>
<li><strong>Transformer + 离散动作令牌</strong>：将图像和语言表示为令牌，输入Transformer以预测下一个动作，通常以离散令牌形式输出。代表模型包括VIMA、Gato和RT-1。RT-1采用非自回归方式预测所有动作令牌。</li>
<li><strong>Transformer + 扩散动作头</strong>：在（1）的基础上，在Transformer后加入扩散策略作为动作头，以生成连续平滑的动作。代表模型包括Octo和NoMAD。</li>
<li><strong>扩散Transformer</strong>：将Transformer和扩散动作头集成，直接在Transformer内部执行扩散过程。代表模型是RDT-1B。</li>
<li><strong>VLM + 离散动作令牌</strong>：用在大规模互联网数据上预训练的视觉语言模型（VLM）取代（1）中的Transformer，以提高泛化能力。代表模型是RT-2。</li>
<li><strong>VLM + 扩散动作头</strong>：在（2）的基础上，用VLM取代Transformer，结合VLM的泛化能力和扩散模型生成平滑连续动作的能力。代表模型如Diffusion-VLA等。</li>
<li><strong>VLM + 流匹配动作头</strong>：将（5）中的扩散模型替换为流匹配动作头，以提高实时响应性，同时保持平滑连续控制。代表模型是π₀。</li>
<li><strong>VLM + 扩散Transformer</strong>：将VLM与（3）中的扩散Transformer结合。VLM通常作为高层策略，扩散Transformer作为低层策略。代表模型是GR00T N1。</li>
</ol>
<p>除了主流的传感器模型，论文还回顾了其他架构范式，如<strong>世界模型</strong>（预测以语言为条件的未来感官模态演化）和<strong>可供性模型</strong>（根据语言预测与动作相关的视觉可供性，进而生成动作）。</p>
<p><img src="https://arxiv.org/html/2510.07077v1/x2.png" alt="VLA策略与架构演进"></p>
<blockquote>
<p><strong>图2</strong>：主要Vision-Language-Action模型的时间线。总结了VLA模型的历史进程：从早期基于CNN的模型，到利用预训练VLM骨干的可扩展真实世界策略，再到整合扩散和流匹配技术的模型，以及近期专注于潜在动作推断和分层控制的方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.07077v1/x3.png" alt="架构与训练策略概览"></p>
<blockquote>
<p><strong>图3</strong>：第四节和第五节的结构。总结了VLA模型的关键组成部分。中心说明了核心架构类型，包括传感器模型、世界模型和可供性模型。左侧描绘了输入和输出模态。右侧展示了训练策略及实际实施考虑。</p>
</blockquote>
<p>与现有方法相比，本文综述的系统性创新体现在其<strong>全栈视角</strong>和<strong>详尽的分类法</strong>。它不仅关注模型架构（如从CNN到Transformer，再到引入预训练VLM、扩散模型、流匹配和分层策略的演进），还深入涵盖了数据收集、机器人平台、评估基准等实际部署所必需的硬件和软件生态。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>作为一篇综述论文，本文并未报告具体的对比实验数值结果，而是系统性地梳理了支持VLA模型开发与评估的各类资源、平台和基准。</p>
<p><strong>数据集</strong>：论文指出，训练VLA模型需要大规模、多样化且对齐良好的视觉-语言-动作数据，但此类数据集在规模和多样性上均有限。文中综述了多个关键数据集，例如用于跨本体训练的Open-X Embodiment（OXE）数据集，RT-1使用的包含130,000条轨迹的大规模数据集，以及众多其他仿真和真实世界数据集。</p>
<p><strong>机器人平台</strong>：论文回顾了常用于VLA研究的机器人平台，包括各种机械臂（如Franka Emika Panda、Universal Robots）、移动机器人（如Boston Dynamics Spot、Unitree Go1）以及灵巧手（如Shadow Hand、Allegro Hand）。</p>
<p><strong>评估基准</strong>：为了公平比较VLA模型性能，论文总结了常用的评估协议和基准测试，例如Language-Table、Bridge、CALVIN、Libri等，这些基准定义了标准化的任务集、评估指标（如成功率）和实验条件。</p>
<p><img src="https://arxiv.org/html/2510.07077v1/x6.png" alt="数据集统计"></p>
<blockquote>
<p><strong>图6</strong>：公开可用的机器人数据集的模态和规模概述。展示了不同数据集包含的模态（如RGB、深度、语言、动作）以及数据量（轨迹数量），突显了当前可用于VLA训练的数据在规模和多样性上的分布情况。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.07077v1/x7.png" alt="评估基准"></p>
<blockquote>
<p><strong>图7</strong>：VLA模型的评估基准和指标。列举了在模拟环境和真实世界中用于评估VLA性能的主要基准测试，以及常用的评估指标，如任务成功率、轨迹相似度等。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.07077v1/x8.png" alt="真实世界应用"></p>
<blockquote>
<p><strong>图8</strong>：VLA模型在真实世界机器人系统中的应用示例。展示了VLA模型在各类机器人平台（机械臂、移动机器人、人形机器人）上执行复杂操作和导航任务的实际场景。</p>
</blockquote>
<p><strong>消融实验</strong>：由于是综述，本文未包含具体的消融实验。但它通过架构分类和分析，间接指出了不同组件（如预训练VLM骨干、不同的动作表示方法、分层设计）对模型能力（如泛化性、控制平滑性、长视野任务处理）的贡献。例如，采用预训练VLM（架构4-7）旨在提升常识和泛化能力；采用扩散或流匹配动作头（架构2,5,6,7）旨在改善动作连续性和实时性；分层架构（如RT-H， GR00T N1）旨在更好地处理长视野任务。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>全面的全栈综述</strong>：不同于以往仅聚焦于动作表示或高层架构的综述，本文提供了整合软件（模型架构、训练策略、数据）和硬件（机器人平台、部署）组件的系统性全栈视角。</li>
<li><strong>VLA架构的系统分类</strong>：文章清晰梳理了VLA模型的历史演进脉络，并提出了一个细致的架构分类法（特别是传感器模型的七种变体），帮助读者理解该领域的技术发展路径和当前设计选择。</li>
<li><strong>面向实际部署的实用指南</strong>：论文详细讨论了数据收集策略、可用数据集、数据增强方法、评估基准以及机器人平台选型，为研究者将VLA应用于真实世界提供了切实的参考。</li>
</ol>
<p>论文自身指出的局限性主要体现在VLA发展面临的<strong>根本性挑战</strong>上，这些也是未来研究的方向：</p>
<ul>
<li><strong>数据需求与稀缺性</strong>：对齐视觉、语言和动作的大规模高质量数据集仍然有限且获取成本高昂。</li>
<li><strong>本体迁移</strong>：跨不同机器人形态（机械臂、移动底盘、人形）的策略迁移因动作空间、传感器和运动学结构差异而极具挑战。</li>
<li><strong>计算与训练成本</strong>：训练和运行这些多模态大模型需要巨大的计算资源，在资源受限的机器人平台上进行实时推理面临延迟和内存使用的挑战。</li>
</ul>
<p>对后续研究的启示在于，要推动VLA走向广泛的实际应用，必须着力解决上述挑战。未来工作可能集中在：开发更高效的数据收集和合成方法；设计更好的跨本体表示与迁移学习算法；探索轻量化模型架构与蒸馏技术；以及构建更统一、全面的评估基准以衡量模型在开放世界中的真实泛化能力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文综述了机器人领域中的视觉-语言-动作模型，旨在解决传统方法将感知/推理与动作生成分离、导致机器人泛化能力受限的核心问题。论文系统回顾了VLA模型，其关键技术是通过端到端框架统一学习视觉、语言和动作模态，以实现跨任务、跨环境和跨机器人的泛化。作为一篇综述，本文未报告具体实验数据，但全面梳理了VLA的策略架构、学习范式、数据处理及机器人平台，为实际应用提供了从软件到硬件的系统性指导。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.07077" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>