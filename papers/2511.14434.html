<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.14434" target="_blank" rel="noreferrer">2511.14434</a></span>
        <span>作者: Matthias Scheutz Team</span>
        <span>日期: 2025-11-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人复杂行为学习的主流方法是强化学习（RL），但训练得到的行为策略难以轻易修改或适应新的环境变化或安全约束。例如，当人类进入机器人工作空间或地面出现动态禁区时，预先训练的策略无法应对。虽然存在“安全RL”等方法提供安全保证，但它们通常无法在线适应新约束。</p>
<p>本文针对“如何为已有的、可能不安全的机器人策略在线注入形式化安全保证，以适应动态环境变化”这一具体痛点，提出了一种新视角：利用源自信号时序逻辑（STL）规范的形式化方法（谐波控制李雅普诺夫-屏障函数，HCLBF）作为安全滤波器，在线修正任何给定的机器人策略（如RL策略）的输出，而无需重新训练策略本身。</p>
<p>本文的核心思路是：使用受限的STL片段描述新的时空安全约束，将其转化为HCLBF势场；该势场的负梯度定义了安全运动方向；通过将原始策略输出的速度命令投影到HCLBF定义的安全半空间，实现对策略的最小修正，从而在保持任务性能的同时获得形式化安全保证。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架是一个在线控制循环，其输入是当前机器人末端执行器的位置，输出是发送给底层操作空间控制器的安全位置目标。核心流程如下：1）查询预训练的RL策略，获得作用于物体的任务空间力；2）通过导纳模型将该力映射为任务空间期望速度；3）在当前末端位置评估HCLBF标量场及其梯度；4）检查该速度是否满足HCLBF导出的屏障不等式；若满足则直接采用，若不满足则将其投影到安全半空间得到修正速度；5）积分得到下一时刻目标位置。</p>
<p>核心模块包括：1）<strong>受限STL规范</strong>：用于定义安全约束。允许的公式包括原子公式（如<code>x&gt;0</code>）、原子公式的否定、使用∧连接的布尔组合，以及单一有界时间算子<code>G[a,b]</code>或<code>F[a,b]</code>的修饰。这用于生成目标区域和障碍区域。2）<strong>HCLBF生成器</strong>：将STL定义的区域（目标设为0，障碍设为1）作为边界条件，在离散网格上求解拉普拉斯方程<code>∇²V(x,y)=0</code>，得到一个平滑的谐波势场<code>V</code>。3）<strong>安全滤波器</strong>：核心创新点。它不修改策略内部，而是在其输出端施加约束。对于策略输出的速度命令<code>u</code>，检查屏障条件<code>∇V(x)⊤u ≤ -k_α V(x)</code>（<code>k_α&gt;0</code>为增益）。若违反，则求解凸优化问题，将<code>u</code>投影到满足该不等式的半空间内，得到最接近<code>u</code>的安全命令<code>u*</code>。</p>
<p><img src="https://via.placeholder.com/600x300.png?text=Fig.+1+HCLBF+field+and+gradient+%28left%29+and+sample+trajectories+%28right%29" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：HCLBF势场与梯度（左）及安全到达目标的示例轨迹（右）。左图展示了由拉普拉斯方程求解得到的平滑标量势场<code>V</code>，颜色从目标（低，蓝色）向障碍（高，红色）过渡，箭头代表负梯度方向，即安全运动方向。右图显示了在该场中从不同起点出发均能安全抵达目标的轨迹。</p>
</blockquote>
<p>与现有集成CBF与STL的工作相比，本文的创新点具体体现在：1）扩展了方法以包含李雅普诺夫函数功能（驱动系统至目标），形成了HCLBF；2）重点将HCLBF用于与现有RL策略的<strong>合成</strong>，通过安全滤波机制在线修正不安全动作，提供了更大的任务灵活性，且无需重新训练策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在一个概念验证设置中进行，使用了<strong>RoboSuite</strong>机器人仿真环境。任务是一个简单的2D平面机器人臂桌面导航，需要从起点移动到绿色目标点，同时避开红色障碍物。</p>
<p><img src="https://via.placeholder.com/400x300.png?text=Fig.+2+RoboSuite+simulation+with+obstacles+%28red%29+and+goal+%28green%29" alt="实验设置"></p>
<blockquote>
<p><strong>图2</strong>：RoboSuite仿真环境中的实验设置，展示了带有障碍物（红色）和机器人臂目标点（绿色）的任务场景。</p>
</blockquote>
<p>对比的基线是<strong>未经安全滤波的原始RL策略</strong>。关键实验结果表明：原始RL策略在遇到训练时未见的障碍物时会<strong>发生碰撞</strong>。而在集成了基于STL生成的HCLBF安全滤波器后，机器人臂能够<strong>成功避免与所有障碍物碰撞</strong>，并最终到达目标。论文通过轨迹图（如图1右）定性展示了安全滤波后的成功导航。虽然没有给出大量定量统计数据，但明确证明了该方法的有效性——将原本不安全的策略转变为能够安全完成任务的策略。</p>
<p>消融实验体现在对方法组件的必要性讨论上。论文指出，HCLBF的生成必须至少有一个目标单元，否则速度场会退化为围绕障碍物的平凡流动；同时，安全投影步骤是保证形式化安全的关键，它确保了即使策略输出危险命令，最终执行的命令也满足HCLBF的屏障条件。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了一种将形式化安全约束（STL-HCLBF）与任意学习策略（如RL）集成的框架，能够在线为不安全策略提供形式化安全保证。2）设计了一种基于HCLBF的安全滤波/投影机制，可对策略输出进行最小修正，在保证安全的同时最大程度保留原有任务性能。3）为动态变化环境中的机器人行为在线适应提供了一个有前景的方向，无需耗时且昂贵的策略重新训练。</p>
<p>论文自身提到的局限性包括：1）使用的STL子集非常有限，不支持析取、嵌套时序算子等复杂逻辑结构。2）HCLBF是在离散网格上近似求解的，而非连续空间，这限制了精度并可能带来计算开销。3）当前方法假设目标与障碍区域不重叠（当重叠时优先安全），且未处理智能体初始即位于障碍中的情况。4）需要未来进行更严格的定量安全评估。</p>
<p>对后续研究的启示：1）扩展可处理的STL规范范围，以描述更复杂、时变的约束。2）探索更高效的连续PDE求解器或近似方法（如神经CBF）以替代网格近似，适应高维空间和实时更新。3）可将此安全滤波机制嵌入到RL训练过程中，作为安全屏障来约束探索，实现更高效的安全RL。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出一种在线安全控制方法，核心问题是使已训练的不安全机器人策略能动态适应环境变化并保证安全，无需重新训练。关键技术为：利用信号时序逻辑描述新约束，推导调和控制李雅普诺夫-屏障函数，并通过共享速度表示将安全证书与原有策略在线集成，从而调制控制指令。实验以静态机械臂的物体导向强化学习策略为例，验证了该方法能使其在桌面运动任务中安全避开障碍物，且行为具有可证明的安全性保证。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.14434" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>