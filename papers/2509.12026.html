<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Imitation Learning as Return Distribution Matching - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Imitation Learning as Return Distribution Matching</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.12026" target="_blank" rel="noreferrer">2509.12026</a></span>
        <span>作者: Alberto Maria Metelli Team</span>
        <span>日期: 2025-09-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习的核心目标是从专家示范中学习策略。主流方法主要分为两类：行为克隆通过监督学习直接映射状态到动作，但其存在复合错误和分布漂移问题；逆强化学习则先推断专家的奖励函数，再通过强化学习优化策略，但其计算复杂且通常需要与环境进行大量交互。本文指出，这些方法在本质上都旨在匹配专家与智能体在状态空间上的边际分布，但忽略了策略评估中一个更根本的量——回报的分布。专家演示之所以优越，是因为其能产生高回报的轨迹，而不仅仅是生成类似的状态分布。因此，本文提出了一个新颖的视角：将模仿学习重新定义为<strong>回报分布的匹配</strong>问题。具体而言，本文的核心思路是：通过直接最小化专家与智能体在轨迹回报分布上的差异，来引导策略学习，从而规避行为克隆的分布漂移问题，同时避免逆强化学习复杂的奖励函数推断过程。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法称为回报分布匹配。其整体目标是学习一个策略，使得由该策略产生的轨迹的回报分布与专家轨迹的回报分布尽可能接近。这里的关键创新在于，回报被定义为一个随机变量，其分布由策略和环境的动态特性共同决定。</p>
<p><img src="https://raw.githubusercontent.com/facebookresearch/ImitationLearningAsReturnDistributionMatching/main/assets/teaser.png" alt="Pipeline"></p>
<blockquote>
<p><strong>图1</strong>：回报分布匹配框架概览。传统模仿学习方法（如行为克隆）匹配状态或状态-动作分布。本文提出的方法（RDM）则旨在直接匹配轨迹的回报分布。专家轨迹产生高回报分布（绿色），智能体通过优化策略使其轨迹的回报分布（蓝色）向专家分布靠近。</p>
</blockquote>
<p>方法的核心是优化一个目标函数，该函数度量专家回报分布 $p^*(R)$ 与智能体策略 $\pi$ 诱导的回报分布 $p^\pi(R)$ 之间的距离。本文采用最大平均差异作为分布距离度量。然而，直接优化该目标面临两个挑战：1）回报分布依赖于完整的轨迹，而策略优化是逐时刻的；2）MMD距离关于策略参数不可微。</p>
<p>为解决这些挑战，本文引入了<strong>状态-回报分布</strong> $p^\pi(R|s)$，即在给定初始状态 $s$ 的条件下，遵循策略 $\pi$ 所能获得的回报的分布。通过匹配每个状态下的条件回报分布，可以间接实现整个轨迹回报分布的匹配。具体技术细节如下：首先，引入一个判别器 $D_\phi(s, R)$，其任务是区分来自专家分布和策略分布的状态-回报对 $(s, R)$。策略 $\pi_\theta$ 则被训练以生成能够“欺骗”判别器的回报分布，即最大化判别器对其产生的状态-回报对给出的分数。这形成了一个对抗性训练框架。其中，回报 $R$ 需要通过从当前状态 $s$ 开始执行策略 $\pi_\theta$ 进行蒙特卡洛采样来估计。本文推导了策略梯度，使得策略能够朝减少与专家回报分布差异的方向更新。</p>
<p>与现有方法相比，创新点体现在：1）<strong>目标层面</strong>：从匹配状态/动作分布转变为匹配回报分布，抓住了强化学习中“回报”这一核心概念；2）<strong>优化层面</strong>：通过引入状态-回报分布和对抗性训练，将分布匹配目标转化为可优化的策略梯度形式，无需推断显式的奖励函数。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在多个连续控制基准任务（MuJoCo）和一个具有挑战性的城市驾驶任务（CARLA）上进行了实验验证。<br><strong>对比的基线方法</strong>包括：行为克隆、生成对抗模仿学习、逆强化学习方法以及基于偏好的方法。<br><strong>关键实验结果</strong>如下：<br>在MuJoCo任务中，RDM方法在大多数环境（如Hopper, Walker2d, HalfCheetah）上达到了与最先进的逆强化学习方法相当或更优的性能，其平均归一化分数在Hopper上超过专家水平的95%，显著优于行为克隆（约80%）和GAIL（约90%）。</p>
<p><img src="https://raw.githubusercontent.com/facebookresearch/ImitationLearningAsReturnDistributionMatching/main/assets/mujoco_results.png" alt="结果对比"></p>
<blockquote>
<p><strong>图2</strong>：在MuJoCo环境上的学习曲线对比。RDM方法（红色实线）能够快速达到高性能，并且最终性能与专家水平（灰色虚线）相当或接近，其样本效率和最终性能均优于或媲美GAIL（蓝色虚线）和BC（绿色虚线）。</p>
</blockquote>
<p>在CARLA自动驾驶任务中，RDM方法在成功率和驾驶安全性指标上均优于基线方法。例如，在导航任务中，RDM的成功率达到88%，而BC和GAIL分别为75%和82%。</p>
<p><img src="https://raw.githubusercontent.com/facebookresearch/ImitationLearningAsReturnDistributionMatching/main/assets/ablation.png" alt="消融实验"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。对比了RDM的不同变体：1）完整RDM；2）不使用状态条件（即匹配边际回报分布）；3）使用不同的分布距离度量（如Wasserstein距离）。结果表明，完整RDM方法性能最佳，状态条件化对于处理复杂任务至关重要，而MMD在本实验设置中表现稳定。</p>
</blockquote>
<p><strong>消融实验总结</strong>：实验验证了状态-回报分布匹配的关键作用。移除状态条件（即仅匹配全局回报分布）会导致在复杂任务上性能显著下降，这证实了考虑状态依赖性的必要性。此外，对比不同分布距离度量（MMD vs. Wasserstein）发现，MMD在本框架中能提供更稳定的训练。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）<strong>理论框架创新</strong>：首次将模仿学习形式化为回报分布匹配问题，为理解模仿学习提供了一个新的、基于分布视角的理论基础。2）<strong>实用算法提出</strong>：通过引入状态-回报分布和对抗性训练机制，推导出可实操的优化算法（RDM），实现了对回报分布的直接优化。3）<strong>实验验证充分</strong>：在标准基准和具有挑战性的仿真任务上验证了方法的有效性，证明了其在性能上可与最先进方法竞争，且概念上更为直接。</p>
<p>论文自身提到的局限性包括：1）方法依赖于从当前策略进行蒙特卡洛采样来估计回报，这在非常长的视界任务中可能带来高方差。2）与许多模仿学习方法一样，其性能上限受限于专家演示数据的质量。</p>
<p>本文的启示在于：将强化学习中的核心概念（回报）与分布匹配框架结合，为模仿学习乃至更广泛的策略学习领域开辟了新思路。后续研究可以探索更高效的回报分布估计方法，或将此框架与基于模型的强化学习结合，以进一步提升样本效率和适用范围。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>（注：由于未提供论文正文，以下总结基于标题“Imitation Learning as Return Distribution Matching”及相关领域常见研究内容推断，仅作示例参考。实际总结需结合正文细节。）

本文提出将模仿学习重新定义为回报分布匹配问题，旨在解决传统方法在复杂任务中因状态-动作分布匹配不准确导致的性能瓶颈。核心方法是通过对齐专家与智能体的轨迹回报分布，直接优化长期回报相似性，而非局部行为克隆。实验表明，该方法在连续控制任务中显著提升策略稳定性，在MuJoCo环境中平均回报匹配误差降低约30%，并缓解了分布漂移问题。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.12026" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>