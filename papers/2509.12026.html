<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Imitation Learning as Return Distribution Matching - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Imitation Learning as Return Distribution Matching</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.12026" target="_blank" rel="noreferrer">2509.12026</a></span>
        <span>作者: Alberto Maria Metelli Team</span>
        <span>日期: 2025-09-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习（IL）的主流方法，如行为克隆（BC）、生成对抗模仿学习（GAIL）等，其核心目标是找到一个策略，使其占用度量（occupancy measure）与专家的占用度量相匹配。由于占用度量完全决定了策略在任意奖励函数下的期望回报，因此这种匹配保证了模仿策略与专家的期望性能相近。然而，这种关注期望值的做法本质上是风险中性的，完全忽略了回报分布的其他特征，例如方差或尾部风险。在许多实际领域（如金融、自动驾驶）中，人类专家的行为往往是风险敏感的，其关键特征恰恰体现在回报分布的形状上。</p>
<p>现有风险敏感模仿学习方法通过额外匹配专家回报分布在某一水平α下的条件风险价值（CVaR）来扩展标准框架。但这存在两个关键局限：(i) 仅匹配期望和单一水平的CVaR只能捕捉专家完整回报分布的一个狭窄切片，对风险态度的模仿较弱；(ii) 马尔可夫策略的表达能力不足以捕捉所有相关的风险敏感行为，导致模型设定错误。</p>
<p>本文针对上述局限性，提出了一个全新的视角：将模仿学习重新定义为<strong>回报分布匹配</strong>问题。其核心思路是，直接最小化模仿策略与专家策略在整个回报分布上的差异（使用Wasserstein距离度量），并通过在一个精心设计的、高效且表达能力足够的非马尔可夫策略子类中进行搜索来实现这一目标。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的核心目标是求解公式(4)：$\hat{\pi} \in \arg\min_{\pi \in \Pi_{NM}} W(\eta^{\pi}<em>{r_E}, \eta^{\pi_E}</em>{r_E})$，即在已知专家奖励$r_E$的情况下，寻找一个策略使其回报分布与专家的回报分布之间的Wasserstein距离最小。</p>
<p><strong>整体框架与核心模块</strong>：直接在全空间$\Pi_{NM}$中优化是维数灾难。本文的关键创新是引入了一个参数化的、介于$\Pi_{NM}$和$\Pi_M$之间的非马尔可夫策略子类$\Pi(r^\theta_E)$，它既足够表达以近似最优解，又足够小以实现高效优化。</p>
<ol>
<li><strong>策略子类设计</strong>：对于任意奖励函数$r$，定义策略类$\Pi(r)$，其中的策略选择动作仅依赖于当前状态$s$、阶段$h$以及到当前为止的累积奖励$G(\omega; r)$。这相当于在一个将累积奖励作为状态一部分的扩展MDP中使用马尔可夫策略。论文证明（引理4.1），对于专家奖励$r_E$，总存在一个策略$\pi_{r_E} \in \Pi(r_E)$，其回报分布与专家完全一致。</li>
<li><strong>离散化与效率权衡</strong>：然而，$\Pi(r_E)$可能仍然很大（例如，当每条轨迹回报都不同时）。为此，引入离散化参数$\theta \in (0,1]$，将奖励$r$离散化为$r_\theta$（公式7）。由于$r_\theta$取值于有限的离散集$Y^\theta$（大小为$O(H/\theta)$），策略类$\Pi(r_E^\theta)$也变得小而高效（存储复杂度$O(SAH|Y^\theta|)$）。引理4.2证明，使用$\Pi(r_E^\theta)$中的策略$\pi_{r_E^\theta}$所带来的近似误差上界为$H\theta$。通过调整$\theta$，可以权衡表达精度与计算/存储效率。</li>
</ol>
<p>基于此策略子类，论文针对两种不同设置提出了两种算法：</p>
<p>**RS-BC (风险敏感行为克隆)**：适用于转移模型$p$未知的纯离线设置。算法直接利用专家数据集$D_E$来估计最优策略$\pi_{r_E^\theta}$。具体步骤是：统计数据集中，在特定阶段$h$、状态$s$且离散化累积奖励为$g$时，专家采取各个动作$a$的次数（算法1第1行），然后据此计算经验条件概率作为策略输出（第2行）。这本质上是将状态空间扩展为$(s, g)$后执行行为克隆。</p>
<p>**RS-KT (已知转移模型的风险敏感模仿)**：适用于转移模型$p$已知的设置。该算法分两步：首先，利用数据集$D_E$估计专家的离散化回报分布$\hat{\eta}$（算法2第1行）。然后，利用已知的$p$和$r_E$，在策略类$\Pi(r_E^\theta)$中寻找一个策略，使其诱导的回报分布与$\hat{\eta}$的Wasserstein距离最小（第2行）。这可以通过求解一个线性规划（LP）问题来实现。该LP的决策变量是扩展MDP（状态为$(s,g)$）中的占用度量$d$和回报分布$\eta$，约束条件确保$\eta$是由$d$诱导产生的，目标是最小化$W(\eta, \hat{\eta})$（公式10）。从最优的$d$可以恢复出对应的策略$\hat{\pi}$。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的创新具体体现在：(1) <strong>问题定义</strong>：首次将模仿学习形式化为完整的回报分布匹配问题，而非仅匹配期望或CVaR。(2) <strong>策略表达</strong>：论证了马尔可夫策略的不足，并设计了一个参数化、可证明高效且表达能力足够的非马尔可夫策略子类$\Pi(r_E^\theta)$来解决问题。(3) <strong>算法设计</strong>：基于该子类，为不同设置（模型已知/未知）提供了具有可证明样本效率的算法（RS-KT, RS-BC）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在随机生成的表格化MDP上进行，评估算法在不同专家轨迹数量$N$下的性能，核心指标是模仿策略与专家策略回报分布之间的Wasserstein距离。</p>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准方法</strong>：在无交互设置（未知$p$）中对比<strong>BC</strong>；在已知转移模型设置中对比<strong>MIMIC-MD</strong>。两者都是可证明高效的标准IL算法。</li>
<li><strong>评估</strong>：对50个随机MDP和专家策略，为每个$N$值运行3次，计算平均Wasserstein距离，再对所有MDP取平均。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>非马尔可夫专家下的性能优势（Q1）</strong>：当专家是非马尔可夫策略时，结果如表1（顶部）所示。BC和MIMIC-MD由于受限于马尔可夫策略，即使$N$很大（10000），误差也停滞在约0.068，存在固有偏差。而RS-BC和RS-KT的误差随$N$增加持续下降，最终分别达到0.005和0.011，显著优于基线。这表明本文方法在匹配复杂风险态度上的必要性。</p>
<blockquote>
<p><strong>表1（顶部）</strong>：$S=2, A=2, H=5$，非马尔可夫专家，$\theta=0.05$。RS-BC和RS-KT能持续学习并超越基线方法性能下限。</p>
</blockquote>
</li>
<li><p><strong>离散化参数$\theta$的影响（Q2）</strong>：增大$\theta$至0.5（表1中上部），RS-BC和RS-KT的性能均下降，因为更大的$\theta$带来了更大的近似误差（引理4.2）。但即便如此，RS-BC在大部分$N$下仍优于BC，显示了其鲁棒性。</p>
<blockquote>
<p><strong>表1（中上部）</strong>：$\theta=0.5$时，算法性能因近似误差增大而下降，但RS-BC仍常优于BC。</p>
</blockquote>
</li>
<li><p><strong>专家为马尔可夫策略时的表现（Q3）</strong>：当专家本身是马尔可夫策略时（表1中下部），BC（理论上最优）和RS-BC表现相当且优秀。RS-KT表现略差于RS-BC，论文分析这可能是因为其分两步（先估计分布，再优化策略）的过程引入了额外误差。但所有方法误差都随$N$增加而下降，说明本文方法在简单场景下不会失效。</p>
<blockquote>
<p><strong>表1（中下部）</strong>：专家为马尔可夫策略时，BC与RS-BC表现最佳且接近，RS-KT稍逊。</p>
</blockquote>
</li>
<li><p><strong>RS-KT的样本效率优势（Q4）</strong>：在一个大规模MDP（$S=300, A=5, H=5$）中（表1底部），RS-BC和BC的误差几乎不随$N$增加而变化，因为状态空间太大，有限的样本无法有效估计。而RS-KT利用已知的转移模型，其误差随$N$增加显著下降，验证了定理4.4中其样本复杂度与$S, A$无关的理论优势。</p>
<blockquote>
<p><strong>表1（底部）</strong>：大规模MDP中，RS-KT凭借已知模型实现高效学习，而RS-BC和BC因样本不足性能停滞。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验总结</strong>：实验本身对比了不同算法，实质上验证了各个组件的贡献：(1) 使用非马尔可夫策略子类$\Pi(r_E^\theta)$对于匹配非马尔可夫专家的风险态度至关重要（Q1结果）。(2) 在已知模型时，采用两阶段优化（RS-KT）比直接模仿（RS-BC）在大状态空间下具有显著的样本效率优势（Q4结果）。(3) 离散化参数$\theta$控制着表达精度与误差的权衡（Q2结果）。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>理论框架</strong>：提出了“回报分布匹配”这一新的模仿学习形式化框架，旨在全面模仿专家的风险态度，并使用Wasserstein距离作为分布差异的度量。</li>
<li><strong>算法与理论</strong>：设计了高效且表达力丰富的非马尔可夫策略子类$\Pi(r^\theta_E)$，并基于此提出了两种具有可证明样本效率的算法（RS-BC和RS-KT），分别适用于转移模型未知和已知的设置。</li>
<li><strong>扩展分析</strong>：初步证明了即使在专家奖励未知的鲁棒设置下，该问题在样本复杂度上也是可解的（定理5.1, 5.2），为后续研究奠定了基础。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>计算复杂度</strong>：RS-KT需要求解一个规模与$S, A, H, 1/\theta$相关的线性规划，在大规模问题中可能计算昂贵。</li>
<li><strong>未知奖励的算法</strong>：论文仅证明了未知奖励设置下的样本复杂度上界，并指出需要借助“神谕”求解一个更复杂的优化问题（公式12），但未给出一个像RS-KT那样具体、高效的算法。</li>
<li><strong>表格化假设</strong>：方法目前局限于表格化MDP和确定性奖励。</li>
</ol>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li>将回报分布匹配框架扩展到连续状态/动作空间和函数近似设置。</li>
<li>为未知奖励的鲁棒设置设计实用、高效的算法。</li>
<li>探索将已知模型的高样本效率（RS-KT）与在线环境交互相结合，以学习未知的转移模型。</li>
<li>研究其他分布距离度量（如最大平均差异MMD）在实践中的效果与理论性质。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究风险敏感模仿学习问题，旨在训练智能体不仅匹配专家的期望回报，还要模仿其风险态度（如回报分布的方差）。论文提出将问题形式化为用Wasserstein距离匹配专家回报分布，针对表格设定引入了一类高效且表达能力强的非马尔可夫策略。基于此，开发了两种算法：RS-BC（转移模型未知）和RS-KT（转移模型已知）。理论分析与实验表明，RS-KT通过利用动态信息显著降低了样本复杂度，且非马尔可夫策略在样本效率上优于标准的模仿学习算法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.12026" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>