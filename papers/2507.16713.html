<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.16713" target="_blank" rel="noreferrer">2507.16713</a></span>
        <span>作者: Lan, Guowei, Qu, Kaixian, Zurbrügg, René, Chen, Changan, Mower, Christopher E., Bou-Ammar, Haitham, Hutter, Marco</span>
        <span>日期: 2025/07/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大语言模型（LLMs）和视觉语言模型（VLMs）已被广泛应用于机器人任务规划，它们能够解析自然语言指令并利用常识推理生成可行的动作序列。然而，一个关键挑战在于，这些模型最初是在互联网数据上训练的，缺乏对部署的具体机器人实体（embodiment）及其能力的认知。例如，当VLM看到一个被部分遮挡的网球时，它可能基于人类直觉自信地指令机器人抓取，但机器人却可能因感知不完善而失败。这引出了核心问题：如何让VLM“了解”它所辅助的机器人的具体能力与局限？如何有效地将VLM“接地”（grounding）到机器人上？</p>
<p>本文针对VLM对机器人本体认知不足这一痛点，提出了一个新颖的视角：让VLM通过自主执行任务生成的经验来“自我接地”。核心思路是，即使初始接地效果不佳，VLM也能通过闭环执行、自我验证和反思，将成功的经验与失败的教训总结为记忆，并利用检索增强生成（RAG）技术在未来遇到相似场景时调用这些记忆，从而不断优化其任务规划能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>ExpTeach框架的核心是利用同一个VLM（本文使用GPT-4o）同时作为任务规划器（𝒯）和成功检测器（𝒟），构建一个包含短时记忆（STM）和长时记忆（LTM）的自我生成记忆系统，并辅以一个按需图像标注模块以增强空间推理。</p>
<p><img src="https://arxiv.org/html/2507.16713v1/extracted/6644291/figures/new_pipeline_figure.jpeg" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：ExpTeach流程总览。系统接收用户指令𝐈和初始观测𝐨₀，VLM将其总结为场景描述。通过RAG从长时记忆𝐌中检索相关经验，与指令、观测及短时记忆𝐦一同输入任务规划器𝒯以生成动作𝐚。执行后，由成功检测器𝒟评估结果。若任务未完成，动作与反馈存入𝐦并继续循环。任务完成后，𝐦被总结并存入𝐌。</p>
</blockquote>
<p><strong>整体流程与核心模块</strong>：</p>
<ol>
<li><strong>VLM任务规划与成功检测</strong>：规划器𝒯根据指令、当前视觉观测、STM内容和从LTM检索到的经验，生成结构化的动作（如<code>push(object=&quot;apple&quot;)</code>）。执行后，检测器𝒟根据新的视觉观测分析动作结果（成功/失败）、进行失败分析并提供恢复建议，其反馈𝐫将传递给规划器。</li>
<li><strong>短时记忆（STM）</strong>：STM记录当前任务执行过程中的动作日志，每个条目包含已执行的动作𝐚τ及其对应的反馈𝐫τ+1（即𝐦 = {(𝐚τ, 𝐫τ+1)}）。当动作失败时，STM使得VLM能够反思并调整策略，例如决定与干扰物交互（如推开容器）或创造性地使用工具（如用海绵推动小糖果），从而在有限尝试次数内显著提高任务成功率。</li>
<li><strong>长时记忆（LTM）与检索增强生成（RAG）</strong>：任务成功完成后，STM中的完整经验被一个VLM经验总结器ℰ概括，并以键值对(𝐊, 𝐄)的形式存入LTM。其中，键𝐊是场景描述（指令+初始图像描述），值𝐄是总结后的经验。当面对新任务时，系统将当前指令和场景描述嵌入为向量，通过余弦相似度从LTM中检索top-k个最相关的过去经验𝐄，并将其注入任务规划提示中。这使得机器人能够从一开始就采取正确的行动，甚至泛化到未见但相似的场景。</li>
<li><strong>按需图像标注模块</strong>：为提升抓取、放置、推动等技能在3D空间中的语义精度，框架引入了共享的图像标注工具。当VLM认为需要时（例如抓取复杂物体如肉串），会触发该工具。工具首先利用Grounded SAM进行开放词汇分割获得物体掩码，然后根据技能类型生成候选位置掩码（如使用最远点采样为放置和抓取生成候选点，为推动生成目标掩码）并叠加在图像上，供VLM选择最合适的位置。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.16713v1/x1.png" alt="图像标注工具"></p>
<blockquote>
<p><strong>图3</strong>：图像标注工具示意图。以抓取和动作为例，VLM请求标注后，系统在物体掩码上生成编号的候选位置，VLM选择其一（如抓取肉串的棍子末端而非肉块），并结合抓取置信度与位置得分最终确定动作位姿。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法（如ComeRobot, ReplanVLM）相比，ExpTeach的创新在于系统性地整合了<strong>自我生成的长时记忆机制</strong>，并通过RAG实现经验的跨任务复用与泛化。同时，其图像标注模块支持超越平面桌面操作的多种3D空间技能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与基准</strong>：实验使用搭载6自由度机械臂和夹爪的ANYmal四足机器人，配备ZED X Mini立体相机。VLM采用GPT-4o。在12个真实世界场景（包含8个未见过的泛化场景）上进行了评估。</p>
<p><strong>基线方法</strong>：</p>
<ul>
<li><strong>短时记忆评估</strong>：对比了增强视觉反馈的CaP（CaP-V）基线，该基线无STM，仅根据当前图像和指令规划下一步。</li>
<li><strong>长时记忆评估</strong>：对比了ComeRobot，该方法具备STM和反思能力，但无LTM。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>短时记忆与反思的效果</strong>：在四个挑战性任务上，ExpTeach利用STM进行反思和重规划，将平均成功率从CaP-V的36%提升至84%。具体任务成功率对比如下：<br><img src="https://arxiv.org/html/2507.16713v1/x2.png" alt="短时记忆效果表"></p>
<blockquote>
<p><strong>表2</strong>：STM对成功率的影响。ExpTeach在各项任务上均显著优于无STM的CaP-V基线，例如“移动鸡蛋”任务达到100%成功率。</p>
</blockquote>
</li>
<li><p><strong>长时记忆接地的效果</strong>：在构建了包含100条经验的LTM后，ExpTeach在12个任务（包括原始任务及其变体）上的<strong>单次尝试成功率</strong>从ComeRobot的22%大幅提升至80%。具体数据如下：<br><img src="https://arxiv.org/html/2507.16713v1/x2.png" alt="长时记忆效果表"></p>
<blockquote>
<p><strong>表3</strong>：LTM对单次尝试成功率的影响。在几乎所有任务上，ExpTeach（具备LTM）都远超ComeRobot（无LTM），例如“将苹果放到盘子上（容器阻挡）”任务达到100%成功率。</p>
</blockquote>
</li>
<li><p><strong>定性结果与涌现行为</strong>：实验观察到机器人涌现出智能的物体交互行为，包括创造性的工具使用。<br><img src="https://arxiv.org/html/2507.16713v1/extracted/6644291/figures/stm_to_ltm_example.jpg" alt="短时记忆与长时记忆示例"></p>
<blockquote>
<p><strong>图4</strong>：ExpTeach在多种场景下的示例。上排：STM示例，机器人反思后使用海绵作为工具推动糖果。中排：STM示例，在收集碗时掉落苹果后，反思并学习先移开苹果。下排：LTM泛化示例，机器人将学到的经验（如用毛巾推螺丝、先移开障碍物再抓取）成功应用于新指令。</p>
</blockquote>
</li>
<li><p><strong>记忆检索策略消融实验</strong>：对比了三种LTM检索方式对任务规划正确率的影响。<br><img src="https://arxiv.org/html/2507.16713v1/x2.png" alt="检索消融实验表"></p>
<blockquote>
<p><strong>表4</strong>：记忆检索模块消融研究。使用RAG检索top-k相关记忆的策略取得了89%的最高规划成功率，显著优于随机检索（27%）和提供全部记忆（67%），说明精准检索相关经验至关重要，过多无关信息会干扰模型。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>自我生成的记忆框架</strong>，通过结合短时记忆（用于在线反思与重规划）和长时记忆（用于存储和复用经验），使VLM能够自主地将其规划能力接地到具体机器人的物理能力上。</li>
<li>设计了一种基于<strong>RAG的长时记忆检索策略</strong>，使机器人能够从过去经验中获取相关知识，从而在面对相似场景时能从一开始就做出正确行动，并展现出良好的任务泛化能力。</li>
<li>引入了一个<strong>按需图像标注模块</strong>，增强了VLM在抓取、放置、推动等多种技能中对3D空间的语义理解，实现了更精确和鲁棒的动作执行。</li>
</ol>
<p><strong>局限性</strong>：论文提到，实验依赖于特定的VLM（GPT-4o），其泛化能力可能受模型本身限制。此外，在动作失败并改变环境后，需要人工干预重置场景，才能让机器人继续执行。</p>
<p><strong>启示</strong>：这项工作表明，让机器人在真实物理交互中“积累经验”并将其系统化地转化为可检索的记忆，是实现VLM高效、自适应机器人任务规划的有效途径。这为未来研究指明了方向：如何设计更高效的经验总结与检索机制，如何减少对人工重置的依赖以实现完全自主的长期学习，以及如何将此类框架扩展到更复杂的多任务和动态环境之中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决将基于互联网数据训练的视觉语言模型（VLM）直接应用于多样化真实机器人时存在的“落地难”问题。提出了ExpTeach框架，其核心是让VLM通过自主规划、验证、反思和调整的闭环过程，积累自我生成的经验，并构建为长期记忆。关键技术包括基于检索增强生成（RAG）的记忆检索利用，以及一个增强空间理解的按需图像标注模块。实验表明，反思机制将四个挑战性任务的执行成功率从36%提升至84%；利用长期记忆进行落地，在12个真实世界场景（含8个未见场景）中，将单次尝试成功率从22%大幅提升至80%，证明了方法的有效性和泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.16713" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>