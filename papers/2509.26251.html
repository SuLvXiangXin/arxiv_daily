<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.26251" target="_blank" rel="noreferrer">2509.26251</a></span>
        <span>作者: Cai, Zhejia, Yang, Yandan, Chang, Xinyuan, Liang, Shiyi, Chen, Ronghan, Xiong, Feng, Xu, Mu, Huang, Ruqi</span>
        <span>日期: 2025/09/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型的主流方法分为两类。一类是端到端策略（如RT-1、Octo），直接映射感知输入到动作，但其面临动作信息不足、策略与具体机器人平台强耦合、难以利用海量无标签视频的挑战。另一类是潜在动作模型，它通过无监督方式从视频观察对中学习语义化的动作表示，再解码为具体动作，实现了策略与具身的解耦，提升了泛化能力（如UniVLA、LAPA）。然而，现有LAMs存在两个关键瓶颈：1）<strong>空间理解不足</strong>：常用的端到端图像编码器偏向于学习表面纹理，忽略了场景的几何结构（如物体关系、空间布局）；2）<strong>时序感知有限</strong>：多数方法依赖稀疏的两帧输入，无法捕捉长时动态和细粒度的运动过渡。</p>
<p>本文针对LAMs在空间几何感知和长时动态建模上的不足，提出了新的视角。核心思路是：1）引入蕴含几何先验的视觉特征（DINOv2）和多帧序列输入，以增强潜在动作对空间结构和运动模式的编码能力；2）在此基础上，构建一个包含显式视觉推理（Visual Chain-of-Thought）的三阶段VLA框架，通过“先想象后行动”的范式提升决策的一致性和可解释性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的SSM-VLA是一个端到端的VLA框架，其核心是Farsighted-LAM模块和三层级联的推理流程。整体流程首先通过视觉思维链预测下一帧，然后推断一系列未来潜在动作，最后生成当前可执行的具体动作。</p>
<p><img src="https://arxiv.org/html/2509.26251v1/x2.png" alt="Farsighted-LAM架构"></p>
<blockquote>
<p><strong>图2</strong>：Farsighted-LAM架构。编码器（左）接收当前帧$s_t$的DINOv2特征和多个未来关键帧，预测对应的连续潜在动作序列，随后通过码本量化得到离散动作令牌${z_{t+k}}$。解码器（右）利用当前帧$s_t$和一个量化后的潜在动作$z_{t+k}$，重建对应的未来帧$\hat{s}_{t+k}$（包括RGB和深度）。</p>
</blockquote>
<p><strong>核心模块一：Farsighted-LAM</strong><br>该模块旨在学习一个增强了对空间结构和动态信息感知的潜在动作空间。</p>
<ul>
<li><strong>编码器</strong>：与传统LAM使用观察对不同，本编码器同时处理当前帧$s_t$和未来N个关键帧序列${s_{t+i}}<em>{i=1}^{N}$。输入不仅包括RGB图像$s_t^{rgb}$，还包含用于额外监督的深度图$s_t^{depth}$。关键创新在于使用冻结的DINOv2模型$\Phi_V$提取RGB帧的视觉特征$v_t$，该特征蕴含丰富的几何和语义先验。一个时空变换器$\Psi</em>{\text{ST}}$处理视觉特征序列${v_{t+i}}<em>{i=0}^{N}$和一组可学习的潜在动作查询$\mathcal{Q}$，为每个未来时刻$t+k$输出一个连续潜在向量$z&#39;</em>{t+k}$，随后通过最近邻查找在学习的码本$\mathcal{C}$中量化，得到离散的潜在动作令牌$z_{t+k}$。</li>
<li><strong>解码器</strong>：由一个空间变换器$\Psi_{\text{S}}$实现，其作用是根据当前帧$s_t$和一个离散潜在动作$z_{t+k}$，重建对应的未来观测$\hat{s}<em>{t+k}$（包括RGB和深度）。解码器被限制只能看到$s_t$和$z</em>{t+k}$，而看不到真实目标帧$s_{t+k}$或任何中间帧，这迫使编码器必须将足够的空间和运动信息压缩到潜在动作中。</li>
<li><strong>重建损失</strong>：使用多模态重建损失$\mathcal{L}<em>{\text{rec}}$监督解码，确保学习到的表示在表观和几何上都准确。它包括：1）<strong>光度损失</strong>$\mathcal{L}</em>{\text{rgb}}$，结合L2损失和LPIPS感知损失，保证重建的RGB图像逼真且语义正确；2）<strong>深度损失</strong>$\mathcal{L}_{\text{depth}}$，一种梯度感知的对数损失，其权重与RGB图像梯度成反比，以强化几何一致性。总损失是N个未来帧上这两个损失的加权和。</li>
</ul>
<p><strong>核心模块二：三层级联VLA策略</strong><br><img src="https://arxiv.org/html/2509.26251v1/x3.png" alt="三层级联VLA策略"></p>
<blockquote>
<p><strong>图3</strong>：三层级联VLA策略框架。阶段1（VisualCoT）基于历史观测和语言指令预测下一帧$\hat{s}<em>{t+1}$。阶段2（Latent Action Inference）进一步推断一个长时域的潜在动作计划${\hat{z}</em>{t+k}}_{k=1}^{N}$。阶段3（Action Generation）融合所有信息，通过条件流匹配模型生成最终的可执行动作$a_t$。</p>
</blockquote>
<p>整个SSM-VLA策略以级联方式运行：</p>
<ol>
<li><strong>阶段1: VisualCoT预测</strong>：一个视觉预测模块$\mathcal{M}<em>{\text{vision}}$根据历史观测$s</em>{t-H:t}$和语言指令$l$，预测下一时刻的视觉状态$\hat{s}<em>{t+1}$（RGB和深度）。使用与Farsighted-LAM解码相同的重建损失$\mathcal{L}</em>{\text{vision}}$进行监督。对于无传感器深度数据，采用基于SfM的稀疏深度图对齐单目深度估计，生成伪目标进行监督。</li>
<li><strong>阶段2: 潜在动作推断</strong>：一个潜在预测模块$\mathcal{M}<em>{\text{latent}}$基于历史上下文、语言指令以及阶段1预测的下一帧特征，自回归地推断未来N步的潜在动作分布$\hat{z}</em>{t+k}$。其监督信号来自预训练好的Farsighted-LAM编码器对真实视频帧编码得到的真实潜在动作$z_{t+k}$，使用交叉熵损失$\mathcal{L}_{\text{latent}}$。</li>
<li><strong>阶段3: 动作生成</strong>：动作模块$\mathcal{M}<em>{\text{action}}$汇总历史上下文、语言指令、预测的下一帧以及完整的潜在动作计划，生成一个综合条件特征$c_t$。该特征作为条件输入一个条件流匹配模型$V</em>{\theta}$（基于DiT网络），用于预测最终的机器人动作$a_t$，对应损失为$\mathcal{L}_{\text{action}}$。</li>
</ol>
<p>整个VLA策略通过加权总损失$\mathcal{L}<em>{\text{VLA}} = \mathcal{L}</em>{\text{action}} + \lambda_{\text{latent}}\mathcal{L}<em>{\text{latent}} + \lambda</em>{\text{vision}}\mathcal{L}_{\text{vision}}$进行端到端微调。</p>
<p><strong>创新点</strong>：与现有方法相比，其创新具体体现在：1) 在LAM中引入DINOv2几何先验特征和多帧序列建模，增强了空间理解和动态感知；2) 提出了包含显式视觉状态预测（VisualCoT）的三阶段推理框架，将长时规划分解为可解释的中间步骤；3) 通过<strong>多模态协同注意力机制</strong>在单一Transformer内实现上述级联流程，该机制设计了特定的注意力掩码，控制视觉、语言、预测帧、潜在动作和最终动作查询之间的信息流，确保推理的协同性和时序一致性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要评估平台为CALVIN仿真基准，包含34个桌面操作任务。采用最具挑战性的ABC-D设置进行零样本评估：在环境A、B、C的演示数据上训练，在未见过的环境D上测试，评估指标为连续完成1到5个任务的成功率及平均成功链长度。此外，还在真实世界的AgileX Piper机器人上进行了“将粉色玩具放入盒子”的任务验证，模型先在Open-X-Embodiment数据集上预训练，再用50条人工演示微调。</p>
<p><strong>对比方法</strong>：对比了广泛的基线模型，包括直接预测模型（Roboflamingo, OpenVLA）、潜在动作模型（Moto-GPT, UniVLA）以及集成视觉预测的模型（Seer, VPP）等。</p>
<p><strong>关键实验结果</strong>：<br>在CALVIN ABC-D基准上，SSM-VLA取得了最优性能。如表1所示，其连续完成1、2、3、4、5个任务的成功率分别为97.6%、94.1%、88.3%、81.8%、75.9%，<strong>平均成功链长度达到4.38</strong>，超越了所有对比基线。</p>
<p><img src="https://arxiv.org/html/2509.26251v1/x4.png" alt="模拟评估任务可视化"></p>
<blockquote>
<p><strong>图4</strong>：模拟评估任务可视化。展示了三个不同任务（每5个动作截取一帧）的仿真结果，证明了模型在多任务学习中的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.26251v1/x5.png" alt="真实世界实验可视化"></p>
<blockquote>
<p><strong>图5</strong>：真实世界实验可视化。模型被要求将粉色球放入盒子中。展示了两种不同布局和杂乱背景下的成功样本，证明了模型在真实场景中的泛化能力。</p>
</blockquote>
<p><strong>消融实验分析</strong>：</p>
<ol>
<li><strong>Farsighted-LAM结构的重要性</strong>：如表2所示，使用3帧上下文的完整模型性能最佳（平均长度4.38）。仅使用1帧未来帧的变体性能下降至4.28，而完全移除LAM模块的模型性能显著降至4.17。这验证了多帧输入能提供更丰富的连续信号，且LAM模块对于长时域任务成功至关重要。</li>
<li><strong>多模态协同注意力的效果</strong>：如表3所示，将精心设计的协同注意力机制替换为简单的因果注意力掩码（每个令牌只能关注其前面的令牌），会导致性能急剧下降，平均长度从4.38跌至3.70。这证明了所提出的结构化注意力流对于整合多模态信息和实现协同推理的关键作用。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>Farsighted-LAM</strong>，一种通过几何感知空间编码（DINOv2特征）和多尺度时序建模（多帧序列）来增强潜在动作空间结构性和动态性的新模型。</li>
<li>提出了<strong>SSM-VLA</strong>，一个端到端的VLA框架，将Farsighted-LAM与视觉思维链模块集成，通过“预测-规划-执行”的三阶段显式推理，提升了决策的一致性和可解释性。</li>
<li>在CALVIN基准上实现了最先进的性能，并通过仿真和真实世界实验验证了方法的有效性和泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确阐述局限性，但根据方法描述，其对能够提供几何先验的视觉编码器（如DINOv2）和（伪）深度信息有一定依赖。此外，三阶段级联推理可能带来一定的计算复杂度。</p>
<p><strong>对后续研究的启示</strong>：本文表明，在VLA系统中显式地结合几何先验、进行多尺度时序建模以及引入可解释的中间推理步骤（如VisualCoT），是提升智能体鲁棒性和泛化能力的有效途径。这为未来构建更可靠、更通用的具身智能体提供了思路，即不仅关注端到端的性能，也重视模型内部表征的结构性和推理过程的透明度。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作系统中的潜在动作模型存在的两个瓶颈：空间理解不足（忽略几何结构）与时间感知有限（难以捕捉长时动态），提出了Farsighted-LAM框架。其关键技术包括：1）利用DINOv2特征进行几何感知空间编码，以捕获场景布局与物体关系；2）通过连续帧序列进行多尺度时间建模，以感知持续运动与瞬时交互。在此基础上构建的端到端SSM-VLA框架，在仿真与真实世界多个VLA任务上取得了最先进的性能，显著增强了智能体的鲁棒性与泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.26251" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>