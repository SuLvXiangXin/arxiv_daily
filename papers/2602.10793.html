<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Semi-Supervised Cross-Domain Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Semi-Supervised Cross-Domain Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.10793" target="_blank" rel="noreferrer">2602.10793</a></span>
        <span>作者: Ping-Chun Hsieh Team</span>
        <span>日期: 2026-02-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>跨领域模仿学习旨在通过跨领域迁移专家知识来加速策略学习，这在收集目标领域专家数据成本高昂的应用中具有重要价值。现有方法主要分为两类：监督式方法依赖代理任务和显式的跨领域对齐，这需要大量成对或未配对的专家演示，数据收集成本高；无监督式方法则通过分布对齐或学习不变表示来避免配对数据，但通常假设源域和目标域之间存在同构关系，这种假设在实际中往往难以满足，导致对齐模糊和迁移不稳定。本文针对现有方法在监督成本与迁移稳定性之间的权衡难题，提出了半监督跨领域模仿学习这一新设置。其核心思路是，仅利用少量目标领域专家演示和大量未标记的目标领域不完美轨迹，结合源域知识，通过设计新颖的跨领域映射损失和自适应加权机制，实现稳定且数据高效的策略学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了AdaptDICE算法，这是首个针对半监督CDIL的算法框架。其整体流程扩展了单域分布校正估计方法，以实现跨动态、状态空间和动作空间存在差异的领域间的知识迁移。</p>
<p><img src="https://arxiv.org/html/2602.10793v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：AdaptDICE算法框架。算法利用预训练的源域模型（Q函数和密度比）和目标域离线数据，通过迭代更新跨域映射函数（G, H）、目标域伪值函数（ν_tar）和目标策略（π），实现知识迁移。自适应权重β(t)动态平衡源域和目标域密度比的贡献。</p>
</blockquote>
<p>算法的核心模块包括：</p>
<ol>
<li><p><strong>跨域映射损失（L_MAP）</strong>：该模块学习两个映射函数 G: S_tar → S_src 和 H: S_tar × A_tar → A_src，其作用是将目标域的状态-动作对映射到源域空间。映射通过最小化在映射后源域空间中的贝尔曼误差来学习，损失函数为：L_MAP = E_{(s,a,s&#39;)<del>D_tar} | r_tar(s,a) + γ E_{a&#39;</del>π(s&#39;)}[Q_src(G(s&#39;), H(s&#39;, a&#39;))] - Q_src(G(s), H(s,a)) |。其中，r_tar是目标域伪奖励，Q_src是预训练的源域伪Q函数。这使得映射能够桥接领域差异，同时保留源域知识，从而支持源域密度比向目标域的迁移。</p>
</li>
<li><p><strong>DICE损失（L_DICE）</strong>：此模块用于学习目标域的伪值函数ν_tar，进而计算目标域自身的密度比w_tar。其损失函数与单域DemoDICE一致：L_DICE(ν_tar) = (1-γ)E_{s<del>μ_tar}[ν_tar(s)] + (1+α) log E_{(s,a)</del>D_tar}[exp(A_{ν_tar}(s,a)/(1+α))]。通过优化此损失，算法能直接从有限的目标域数据中估计专家与混合数据分布之间的密度比。</p>
</li>
<li><p><strong>跨域策略提取与自适应加权</strong>：策略通过加权行为克隆损失L_BC进行学习。关键创新在于定义了混合的跨域密度比 w_cross(s,a) = β * w_src(G(s), H(s,a)) + (1-β) * w_tar(s,a)，其中w_src和w_tar分别代表源域和目标域的密度比。β ∈ [0,1]是一个平衡两者贡献的自适应权重。策略损失为 L_BC = -E_{(s,a)~D_tar}[w_cross(s,a) * log π(a|s)]。自适应权重β(t)被设计为一个关于源域和目标域密度比估计误差的相对大小的平滑函数，从而能够动态调整对源域先验知识的依赖程度，无需手动调参。</p>
</li>
</ol>
<p>与现有方法相比，AdaptDICE的创新点体现在：1) 首次形式化并解决了半监督CDIL问题，仅需极少量的目标专家轨迹；2) 提出了通过最小化贝尔曼误差来学习跨域映射的新方法，避免了强同构假设；3) 引入了基于混合密度比和自适应加权的策略提取机制，实现了源域与目标域知识的稳定融合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个主流机器人仿真平台进行：MuJoCo（Hopper, Ant, HalfCheetah）和RoboSuite（Lift, Door, Wipe）。每个任务中，源域和目标域在形态（如腿部数量）或机器人型号（如Panda与UR5e）上存在差异。</p>
<p>对比的基线方法包括：单域模仿学习方法DemoDICE（仅用目标域数据）、无监督跨域方法GWIL和OTIL、以及监督式跨域方法（如使用时间对比网络TCN和状态翻译网络STN）。实验设置中，目标域专家数据极少（仅1条轨迹），并包含大量未标记的不完美轨迹。</p>
<p><img src="https://arxiv.org/html/2602.10793v1/Figures/main_experiment/main_bar_woexp.png" alt="主要实验结果"></p>
<blockquote>
<p><strong>图22</strong>：在MuJoCo和RoboSuite任务上的主要性能对比。AdaptDICE在绝大多数任务上取得了最佳或接近最佳的性能，显著优于仅使用目标域数据的DemoDICE以及无监督跨域方法GWIL和OTIL。</p>
</blockquote>
<p>关键定量结果显示，在MuJoCo的Ant（5条腿 vs 4条腿）任务中，AdaptDICE的成功率达到约90%，而DemoDICE约为65%，GWIL和OTIL均低于40%。在RoboSuite的Door（Panda vs UR5e）任务中，AdaptDICE成功率接近100%，显著优于其他基线。这表明AdaptDICE能够利用少量专家监督和源域知识，实现稳定高效的跨域策略学习。</p>
<p><img src="https://arxiv.org/html/2602.10793v1/Figures/ablation/ablation_bar_woexp.png" alt="消融实验"></p>
<blockquote>
<p><strong>图29</strong>：消融实验结果。w/o Mapping（无映射）和w/o Source（无源）性能显著下降，验证了跨域映射和利用源域知识的必要性。Fixed β（固定β）性能不稳定，凸显了自适应加权机制的重要性。</p>
</blockquote>
<p>消融实验分析了各组件贡献：1) 移除跨域映射（w/o Mapping），性能大幅下降，验证了映射对于知识迁移的关键作用；2) 仅使用目标域数据（w/o Source），性能与单域DemoDICE相当，说明未能利用源域优势；3) 使用固定权重β（Fixed β），性能在不同任务间波动较大且通常低于自适应版本，证明了自适应机制对于平衡不同领域差异的有效性。</p>
<p><img src="https://arxiv.org/html/2602.10793v1/Figures/fixed_beta/beta_bar.png" alt="自适应权重分析"></p>
<blockquote>
<p><strong>图79</strong>：固定权重β与自适应权重β(t)的性能对比。自适应权重在多个任务上取得了更稳定且更高的性能，表明其能根据学习过程动态调整源域与目标域信息的利用比例。</p>
</blockquote>
<p>此外，实验还分析了算法对目标专家数据量的敏感性（仅1条专家轨迹即可有效工作）以及源域预训练模型质量的影响，进一步验证了方法的鲁棒性和实用性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 首次形式化并提出了一种实用的半监督跨领域模仿学习问题设置，仅需极少量目标专家监督；2) 提出了AdaptDICE算法框架，通过跨域映射损失、混合密度比和自适应加权，实现了稳定高效的知识迁移，并提供了理论收敛保证；3) 在多个具有挑战性的仿真任务上进行了全面实验，证明了该方法相对于监督式和无监督式基线的优越性。</p>
<p>论文自身提到的局限性包括：1) 方法依赖于源域预训练模型（Q_src, w_src）的质量；2) 理论分析主要在表格化设置下进行，向连续高维空间的扩展是未来方向。</p>
<p>这项研究对后续工作的启示在于：首先，半监督设置在降低数据标注成本和提高方法实用性方面具有重要价值。其次，自适应机制为处理不同程度的领域差异提供了新思路。最后，该方法为在动态、状态和动作空间均不匹配的复杂领域间进行模仿学习迁移奠定了坚实基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出半监督跨域模仿学习（SS-CDIL），旨在解决跨域模仿学习中目标域专家数据稀缺且收集成本高的问题。方法基于离线数据，仅需少量目标专家演示与未标记轨迹，通过设计跨域损失函数学习域间状态-动作映射，并引入自适应权重平衡源域与目标域知识。在MuJoCo与Robosuite上的实验表明，该方法较基线模型性能持续提升，实现了稳定且数据高效的政策学习。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.10793" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>