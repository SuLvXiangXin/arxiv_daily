<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Efficient Model-Based Reinforcement Learning for Robot Control via Online Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Efficient Model-Based Reinforcement Learning for Robot Control via Online Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.18518" target="_blank" rel="noreferrer">2510.18518</a></span>
        <span>作者: Marco Hutter Team</span>
        <span>日期: 2025-10-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人强化学习的主流方法是仿真到现实范式。控制策略先在物理仿真器中大量训练，再迁移到真实机器人。这主要因为像PPO这样的无模型强化学习算法需要海量数据，而仿真数据收集高效。然而，该方法存在关键局限性：首先，仿真器保真度不足，难以捕捉复杂硬件（如液压、气动执行器或软体结构）的细微动力学，导致仿真与现实差距，策略迁移后性能下降；其次，许多复杂机器人系统缺乏高效、高精度的专用仿真器，限制了该范式的应用。</p>
<p>本文针对上述痛点，提出了一种直接在物理机器人上进行在线训练的模型强化学习新视角。该方法摒弃了依赖离线仿真和策略优化的传统流程，转而从实时交互数据中在线学习动力学模型，并利用该模型指导策略更新。其核心思路是：通过在线学习动力学模型并利用其梯度来近似策略梯度，显著减少训练所需真实世界样本量，实现高效、直接的在机学习，从而规避仿真与现实差距问题。</p>
<h2 id="方法详解">方法详解</h2>
<p>所提方法是一个同时在线学习动力学模型和优化控制策略的算法。整体流程如算法1所示：在每个训练回合（episode）中，根据当前策略和参考轨迹在真实系统上执行轨迹滚动（rollout），收集状态-动作-下一状态数据存入缓冲区；随后，利用缓冲区数据更新动力学模型参数；最后，基于最新模型计算的近似梯度来更新策略参数。</p>
<p><img src="https://arxiv.org/html/2510.18518v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提在线模型强化学习算法的工作流程。红色部分突出模型学习过程，蓝色部分展示策略优化流程。两者并行进行，模型学习为策略梯度计算提供近似动力学信息。</p>
</blockquote>
<p>核心模块包括在线动力学模型学习和基于模型梯度的策略优化。动力学模型 $f_\theta$ 是一个神经网络，通过最小化单步预测的均方误差损失（公式3）进行学习。策略 $\pi_\phi$ 是另一个神经网络。</p>
<p>策略优化的创新点在于梯度计算方式。不同于常见的沿时间反向传播，本文利用学习模型 $f_\theta$ 在真实滚动轨迹 $(x_\tau^{(t)}, u_\tau^{(t)})$ 处的雅可比矩阵（即局部线性化），通过公式5计算策略参数的近似梯度 $\widehat{\nabla}_{\phi}g$。该公式通过构造块对角矩阵 $A_t$, $B_t$, $K_t$ 并计算 $(\mathbf{I}-(A_t+B_tK_t))^{-1}$，本质上是沿真实轨迹展开闭环动力学灵敏度，从而得到考虑反馈效应的策略梯度估计。这种方法避免了使用不准确模型进行前向模拟可能导致的误差累积问题，计算效率高。</p>
<p>策略更新采用预条件的在线梯度下降（公式6）。预条件矩阵 $\Lambda_t$（公式7）包含三项：梯度外积项、控制动作对参数的雅可比矩阵项以及一个小的单位矩阵项。这相当于一种信任域正则化，用于限制单步更新中目标函数、控制动作和策略参数的变化，从而在梯度估计可能不准确的情况下稳定学习过程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个真实机器人平台上进行：液压挖掘臂（HEAP）和模块化软体机器人臂。对比的基线方法包括：无模型强化学习（PPO）、基于模型的DreamerV3算法，以及针对液压系统的先前工作（Online Adaptation）。评价指标主要是跟踪不同参考轨迹的累计误差或成功率。</p>
<p><img src="https://arxiv.org/html/2510.18518v1/figures/picture_heap.jpg" alt="液压挖掘臂平台"></p>
<blockquote>
<p><strong>图2</strong>：实验平台之一，液压挖掘臂（HEAP）。该平台具有复杂的液压动力学，是验证算法处理未建模现象能力的理想测试床。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.18518v1/figures/photo_soft_arm2.png" alt="软体机器人臂平台"></p>
<blockquote>
<p><strong>图14</strong>：实验平台之二，模块化软体机器人臂。其软体结构的动力学建模困难，凸显了直接在线学习的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.18518v1/x2.png" alt="液压挖掘臂跟踪性能对比"></p>
<blockquote>
<p><strong>图3</strong>：在液压挖掘臂上，不同方法学习到的策略跟踪8字形轨迹的末端执行器位置误差对比。所提方法（Ours）在约4小时（120个回合）的训练后达到与精心调优的Online Adaptation方法相当的低误差水平，且明显优于PPO和DreamerV3。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.18518v1/x13.png" alt="软体机器人臂学习曲线"></p>
<blockquote>
<p><strong>图15</strong>：在软体机器人臂上，各方法训练过程中的平均跟踪误差（RMSE）学习曲线。所提方法（Ours）收敛最快，样本效率最高，在约150个回合后达到稳定性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.18518v1/x16.png" alt="消融实验：预条件器的作用"></p>
<blockquote>
<p><strong>图18</strong>：消融实验，展示在策略更新中移除预条件矩阵 $\Lambda_t$（即使用普通梯度下降）的影响。不使用预条件器会导致训练不稳定和性能下降，验证了其稳定学习过程的重要性。</p>
</blockquote>
<p>关键实验结果总结如下：在液压挖掘臂任务中，本文方法在约4小时（120个回合）的真实交互后，达到了与需要预训练和在线调优的“Online Adaptation”方法相当的低跟踪误差（约0.04m），而PPO和DreamerV3在相同时间内未能有效学习。在软体机器人臂任务中，本文方法同样展现出最高的样本效率，快速收敛至低跟踪误差。当随机化挖掘臂的有效载荷以测试对动态变化的鲁棒性时，本文方法能持续适应并保持性能。消融实验证实了预条件更新策略对于稳定训练的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一种高效、完全在线运行的模型强化学习算法，可直接在复杂物理机器人上训练控制策略，规避了仿真与现实差距问题；2) 创新性地利用学习模型在真实轨迹处的局部梯度来近似策略梯度，避免了模型滚动误差累积，提升了计算效率和实用性；3) 为算法提供了在线学习视角的理论分析框架，并给出了在适当假设下的性能保证。</p>
<p>论文提到的局限性包括：理论分析是在对两个在线学习问题（模型学习和策略学习）进行一定简化和分离假设下进行的；算法性能依赖于参考轨迹的多样性以充分探索状态空间。</p>
<p>这项工作启示后续研究可以进一步探索更紧致的联合收敛性理论，或将此在线模型学习框架扩展到具有高维视觉观测等更复杂的任务场景中，推动强化学习在各类真实机器人系统上的直接应用。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出一种在线模型强化学习算法，用于直接在真实世界中训练复杂机器人控制系统。核心解决传统sim-to-real流程依赖大量离线仿真、存在仿真与现实差距的问题。方法通过实时交互数据在线构建动力学模型，并基于该模型指导策略更新，结合在线学习分析提供次线性遗憾界限的理论保证。在液压挖掘机臂和软机器人臂的实验中，该方法展现出显著样本效率，仅用数小时即达到与模型无关方法相当的性能，并在负载随机变化时表现出良好的动态适应能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.18518" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>