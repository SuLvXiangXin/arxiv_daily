<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.01424" target="_blank" rel="noreferrer">2507.01424</a></span>
        <span>作者: Yanwei Fu Team</span>
        <span>日期: 2025-07-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉语言模型（VLM）的视觉-语言-动作（VLA）模型通过将大规模预训练的多模态理解能力迁移到机器人控制中，实现了对开放式指令的遵循和常识推理。然而，现有主流方法（如RT-2、OpenVLA等）主要采用双系统架构，其表征本质上是静态的，仅依赖一两个瞬时观测，忽略了体现智能体交互本质的序列性和动态结构。这导致智能体无法编码或利用时间上延展的经验，从而被限制在短视距、反应式的行为中，在动态的具身环境中阻碍了鲁棒的泛化能力。</p>
<p>本文针对现有VLA模型缺乏对时序经验进行积累、回忆和预测能力的核心痛点，受认知神经科学中情景记忆理论的启发，提出了“情景世界模型”的新视角。该模型旨在使机器人不仅能够回忆过去的交互，还能预测未来的动态，从而在具身环境中实现鲁棒的泛化。本文的核心思路是设计一个统一的三重系统架构（TriVLA）来实现该情景世界模型，通过整合多模态感知和动态预测来指导策略学习，生成连贯、上下文感知的动作序列。</p>
<h2 id="方法详解">方法详解</h2>
<p>TriVLA的整体框架是一个三重系统组合架构，其输入为当前图像观测、语言指令、机器人状态和动作历史，输出为机器人动作序列。该架构旨在实例化情景世界模型。</p>
<p><img src="https://arxiv.org/html/2507.01424v3/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：TriVLA的流程框架。它是一个基于三重系统范式的统一VLA框架。系统2使用预训练的Eagle-2 VLM进行情景多模态感知；系统3使用通用视频扩散模型（VDM）对情景动态和序列变化进行建模。这两个模块共同构成了一个具有丰富、时间延展表征的联合情景世界模型。系统1作为策略模块，应用动作流匹配来整合所有输出以及机器人状态和动作历史。</p>
</blockquote>
<p><strong>核心模块1：情景多模态感知（系统2）</strong>。该模块使用预训练的NVIDIA Eagle-2 VLM，其由SmolLM2语言模型和SigLIP-2图像编码器构成。它将图像（分辨率224×224，通过像素混洗产生每帧64个图像令牌）和文本指令按照预训练的聊天格式输入，从LLM的第12层（经验表明比最终层更快且效果更好）提取视觉-语言令牌 (Q_{vl})。同时，一个特定于具身的MLP将机器人状态投影为状态令牌 (Q_s)。该模块负责解释观测和指令，总结任务目标和上下文线索。</p>
<p><strong>核心模块2：情景动态感知（系统3）</strong>。该模块使用在大型人类和机器人操作数据集上微调的1.5B参数Stable Video Diffusion模型。其核心功能是编码过去状态序列并预测未来的场景轨迹，实现情景上下文积累。为了避免去噪整个视频序列的计算负担和可能引发的开环控制问题，TriVLA采用了一个关键洞察：仅使用模型的一次前向传递（输入为当前图像 (s_0) 与纯噪声的拼接），即使未生成清晰视频，其内部特征也能提供未来状态的粗略轨迹和信息性指导。具体而言，提取该视频扩散模型多个上采样层的特征，通过线性插值将其统一到共同的空间尺寸后沿通道维度拼接，形成最终的预测性视觉表征 (F_p)。对于多视角（如静态和腕部摄像头），未来状态被独立预测为 (F_p^{static}, F_p^{wrist})。这些高维特征随后通过可学习的令牌 (Q_{[0:T,0:L]}) 进行时空注意力聚合，被压缩为固定长度的预测性令牌 (Q_p)。</p>
<p><strong>核心模块3：策略学习模块（系统1）</strong>。系统2和系统3共同构成了情景世界模型，为策略学习提供了结合描述性多模态接地和预测性时间建模的联合表征。系统1采用基于扩散Transformer的扩散策略作为动作头。它将噪声动作 (a_k) 与来自系统2的 (Q_{vl}) 和系统3的 (Q_p) 通过交叉注意力层进行条件化，学习一个去噪器 (D_{\psi}) 来重建原始动作 (a_0)。最终，一个特定于具身的动作解码器 (A_d)（一个MLP）处理最终令牌以预测动作。训练损失函数为 (\mathcal{L}<em>{\text{diff}}(\psi;A)=\mathbb{E}</em>{a_{0},\epsilon,k}|A_{d}(D_{\psi}(a_{k},Q_{vl},Q_{p}))-a_{0}|^{2})。</p>
<p><strong>创新点</strong>：与现有双系统VLA方法相比，TriVLA的主要创新在于引入了受情景记忆启发的、形式化的情景世界模型，并通过三重系统架构实现。其创新具体体现在：1) 显式地整合了视频扩散模型作为动态感知模块，提供了对时序动态和未来演化的预测能力；2) 提出了高效利用视频扩散模型单步前向特征的方法，避免了完整视频生成的耗时问题；3) 策略模块通过流匹配和交叉注意力机制，同时以多模态语义和预测的未来动态为条件生成动作，实现了上下文感知的长视距规划。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：TriVLA在三个广泛使用的模拟长视距操作基准上进行了评估：CALVIN（在ABC→D零样本泛化设置下）、LIBERO（包含Spatial, Object, Goal, Long四个任务套件）和MetaWorld（包含50个任务）。同时，在真实世界中使用KINOVA GEN2机械臂和RealSense D455相机进行了部署验证，设计了需要积累、回忆和预测序列经验的长视距、高动态任务。</p>
<p><strong>对比方法</strong>：对比了多种代表性的最先进和相关的通用机器人策略方法，包括直接动作学习（RT-1， Diffusion Policy）、视觉语言集成（Robo-Flamingo）、未来预测相关（UniPi, MDT, Susie, GR-1, Vidman, Seer, VPP）以及3D感知方法（Robo-Uniview）等。</p>
<p><strong>关键实验结果</strong>：<br>在CALVIN ABC→D基准的零样本评估中，TriVLA取得了最佳性能，平均任务完成长度（Avg. Len）达到4.37，优于之前最好的VPP（4.33）。在仅使用10%标注数据的数据效率测试中，TriVLA（3.46）也优于使用100%数据的GR-1（3.06）和使用10%数据的VPP（3.25）。</p>
<p><img src="https://arxiv.org/html/2507.01424v3/x1.png" alt="实验结果表1"></p>
<blockquote>
<p><strong>表1</strong>：CALVIN ABC→D基准上的零样本长视距评估结果（Avg. Len）。TriVLA在全部5个子任务上的成功率及平均长度均表现最佳，展示了强大的长视距任务执行和泛化能力。</p>
</blockquote>
<p>在LIBERO基准上，TriVLA的平均成功率达到了87.0%，显著高于对比方法如OpenVLA（76.5%）和Octo（75.1%），尤其在需要长视距推理的Long套件上优势明显（73.2% vs. 53.7%）。</p>
<p><img src="https://arxiv.org/html/2507.01424v3/x2.png" alt="实验结果表2"></p>
<blockquote>
<p><strong>表2</strong>：LIBERO基准实验结果。TriVLA在所有任务套件上均取得了最高或极具竞争力的成功率，特别是在Long套件上表现突出。</p>
</blockquote>
<p>在MetaWorld的50个任务上，TriVLA的平均成功率为71.4%，同样超过了最强的基线VPP（67.9%）。</p>
<p><img src="https://arxiv.org/html/2507.01424v3/x3.png" alt="实验结果表3"></p>
<blockquote>
<p><strong>表3</strong>：MetaWorld基准上的性能。TriVLA在Easy、Middle、Hard三个难度分类及平均成功率上均领先。</p>
</blockquote>
<p><strong>定性结果</strong>：如图4和图5所示，在模拟和真实世界的长、短视距任务中，TriVLA能够理解连续指令的意图，并利用预测能力完成复杂的序列任务，例如在真实世界中依次完成“拿起罐子”、“打开抽屉”、“放入罐子”和“关闭抽屉”。</p>
<p><img src="https://arxiv.org/html/2507.01424v3/x4.png" alt="定性结果短视距"></p>
<blockquote>
<p><strong>图4</strong>：短视距任务的定性案例研究。TriVLA在短视距任务上表现良好，展示了其基础操作能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.01424v3/x5.png" alt="定性结果长视距"></p>
<blockquote>
<p><strong>图5</strong>：长视距任务的定性结果。给定多个连续指令，TriVLA能够理解意图并利用预测来完成长视距任务，体现了情景世界模型的价值。</p>
</blockquote>
<p><strong>消融实验与可视化</strong>：图6可视化了情景动态感知模块（系统3）的输出。结果表明，虽然完整去噪步骤可以生成合理的未来帧，但仅使用单步前向传递获得的特征（红框）已能有效捕捉物体和机械臂运动等关键动态线索，为下游动作学习提供了有益指导。</p>
<p><img src="https://arxiv.org/html/2507.01424v3/x6.png" alt="情景动态感知可视化"></p>
<blockquote>
<p><strong>图6</strong>：情景动态感知模块的可视化。红框表示单步前向预测的特征，蓝框对应完整去噪步骤的预测帧，绿框标记真实未来帧。实验表明单步前向特征足以提供有效的动态线索。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>提出了受认知神经科学启发的“情景世界模型”概念</strong>，为具身智能体实现鲁棒、自适应的控制提供了新的理论基础；2) <strong>设计并实现了TriVLA这一统一的三重系统组合架构</strong>，将预训练VLM的多模态接地能力与视频扩散模型的时序动态预测能力有机结合，实例化了该情景世界模型；3) <strong>在多个标准基准和真实任务上实现了最先进的性能</strong>，显著提升了长视距规划、开放式意图理解和在动态场景中的适应性。</p>
<p>论文自身提到的局限性主要在于：视频扩散模型去噪整个序列计算密集，可能导致开环控制问题。对此，本文通过使用模型的单步前向传递来提取特征，以平衡效率与性能，但这可能损失了部分预测精度。</p>
<p>本工作对后续研究的启示包括：情景世界模型为机器人学习提供了一个富有潜力的新范式，未来可探索更高效、更精准的时序动态建模方法；如何将预测的未来状态更紧密地与低层控制回路结合，以实现更闭环、更鲁棒的规划-执行循环，也是一个值得深入的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在动态具身环境中泛化能力弱、难以进行长时程规划的问题，提出TriVLA模型。其核心是引入受情景记忆启发的情景世界模型，通过三系统架构实现：系统2（预训练VLM）负责多模态感知，系统3（视频扩散模型）进行动态预测与未来推演，系统1（策略模块）整合时序信息生成动作。实验表明，该模型能以36Hz高效运行，在标准测试和真实操控任务上均优于基线，展现出卓越的长时程规划和开放指令理解能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.01424" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>