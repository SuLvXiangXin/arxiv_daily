<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Demonstrating the Octopi-1.5 Visual-Tactile-Language Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Demonstrating the Octopi-1.5 Visual-Tactile-Language Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.09985" target="_blank" rel="noreferrer">2507.09985</a></span>
        <span>作者: Yu, Samson, Lin, Kelvin, Soh, Harold</span>
        <span>日期: 2025/07/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>触觉对于机器人执行灵巧操作、材料识别及视觉遮挡场景下的任务至关重要。近年来，随着视觉触觉传感器（如GelSight、Digit）的发展，以及将触觉感知与视觉-语言模型（VLMs）相结合的研究兴起，视觉-触觉-语言模型（VTLMs）应运而生，使机器人能够利用自然语言提示处理各种触觉相关任务。然而，现有VTLMs在处理包含多个部件的物体以及动态学习新物体方面存在局限。</p>
<p>本文针对现有VTLMs在处理多部件物体和动态学习能力上的不足，提出了改进模型Octopi-1.5。其核心思路是在前代模型基础上，通过升级触觉编码器、采用更强大的基础VLM（Qwen2-VL 7B），并引入一个简单的检索增强生成（RAG）模块，从而提升模型性能并使其具备在交互中学习新物体触觉表征的潜力。</p>
<h2 id="方法详解">方法详解</h2>
<p>Octopi-1.5是一个基于Qwen2-VL 7B开源视觉-语言模型（VLM）进行微调的视觉-触觉-语言模型（VTLM）。其整体架构如图2所示，核心改进在于触觉编码器和RAG模块的引入。</p>
<p><img src="https://arxiv.org/html/2507.09985v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：（A）Octopi-1.5模型架构。它是一个经过微调的Qwen2-VL 7B多模态VLM，包含一个触觉编码器和一个检索模块。（B）触觉编码器中的CLIP模块在微调前使用对比损失和回归损失在PhysiCLeAR、Hardness和ObjectFolder数据集上进行预训练。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>触觉编码器</strong>：该模块负责将来自GelSight mini传感器的触觉图像帧转换为VLM可处理的token。它是一个经过微调的CLIP模块，并增加了一个投影层。为提升计算效率，模型仅处理“显著”帧，即通过启发式方法选取的与前序帧差异最大的前10帧。</li>
<li><strong>两阶段训练</strong>：<ul>
<li><strong>第一阶段（预训练触觉编码器）</strong>：在扩展的PhysiCLeAR数据集（包含标记和无标记GelSight垫）、Hardness数据集和ObjectFolder数据集（统计数据见表I）上训练CLIP模块。训练使用回归损失（预测如表II所示的人类标注的物体硬度和粗糙度分数）和对比损失（区分来自同一物体/部件的触觉输入与来自其他物体/部件的输入）。训练持续30个epoch，在6个未见物体上根据验证损失选择最佳编码器。</li>
<li><strong>第二阶段（端到端微调VTLM）</strong>：冻结预训练好的CLIP模块，使用视觉提示调优（VPT）技术，在描述和排序任务（如图3示例）上训练投影层和解码器。与之前版本不同，Octopi-1.5被明确设计为能够处理由多个部件组成的物体（例如具有不同手柄和刷毛的梳子）。</li>
</ul>
</li>
<li><strong>检索增强生成（RAG）模块</strong>：这是一个实验性模块，旨在通过从数据库中检索相似物体的文本信息来增强触觉描述。具体流程为：对于一组新的显著触觉图像，使用触觉编码器计算其平均嵌入向量；通过余弦相似度搜索在现有触觉嵌入数据集中找到最相似的Top-5物体；聚合并排序这些相似物体，将其标签和触觉描述作为附加信息（如图4蓝色部分所示）提供给模型，以生成更丰富的描述。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.09985v1/x3.png" alt="训练任务示例"></p>
<blockquote>
<p><strong>图3</strong>：训练任务示例。Octopi-1.5被训练来描述和排序一个物体的不同部分（以及不同物体）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.09985v1/x4.png" alt="RAG描述示例"></p>
<blockquote>
<p><strong>图4</strong>：由Octopi-1.5结合RAG进行的触觉描述结果。RAG模块增加的额外信息以蓝色显示。</p>
</blockquote>
<p><strong>创新点</strong>：相较于前代及现有VTLMs，Octopi-1.5的创新具体体现在：（1）增强了处理多部件物体触觉信号的能力；（2）采用了性能更强的Qwen2-VL 7B作为基础VLM，以获得更好的交互和常识知识能力；（3）引入简单的RAG机制，不仅提升了任务性能，还为实现对新物体的动态学习提供了可能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：实验使用一个便携的触觉操作接口（TMI）进行实时演示（系统如图6所示）。TMI是一个改装自通用操作接口（UMI）的夹持器（设计如图5所示），集成了GelSight Mini光学触觉传感器和TAC-02压阻式触觉传感器。模型训练使用了PhysiCLeAR、Hardness和ObjectFolder数据集。</p>
<p><img src="https://arxiv.org/html/2507.09985v1/extracted/6601239/figures/guessimg.jpeg" alt="TMI夹持器"></p>
<blockquote>
<p><strong>图5</strong>：TMI手指及CAD图纸，带有可插入Gelsight-mini或TAC-02传感器的隔间。</p>
</blockquote>
<p><strong>Baseline方法</strong>：对比方法包括仅使用触觉嵌入余弦相似度的非VLM基线（Encoder-1.5）、前代模型Octopi-1（7B和13B参数版本）以及不带RAG的Octopi-1.5。</p>
<p><strong>关键实验结果</strong>：<br>实验主要通过“猜谜游戏”和“排序任务”进行验证。猜谜游戏要求模型仅根据触觉输入从一组已知物体中识别被抓握的物体。</p>
<p><img src="https://arxiv.org/html/2507.09985v1/x1.png" alt="猜谜游戏示例"></p>
<blockquote>
<p><strong>图1</strong>：使用触觉操作接口（TMI）夹持器进行的Octopi-1.5演示。我们计划展示Octopi-1.5如何在一系列场景中描述和利用触觉感知，例如顶图所示的无视觉场景。</p>
</blockquote>
<p>表III总结了猜谜游戏的平均准确率。关键结果如下：</p>
<ul>
<li><strong>RAG的显著提升</strong>：对于已见过的“球类”和“水果”物体，启用RAG的Octopi-1.5准确率从56.00%和57.69%分别大幅提升至<strong>96.00%</strong> 和**100.00%**。</li>
<li><strong>处理未见物体的能力</strong>：对于完全未见的物体，启用RAG的Octopi-1.5取得了<strong>73.17%</strong> 的准确率。若进一步启用“教学”功能（将新物体样本动态添加到数据库），准确率可进一步提升至**95.12%**。</li>
<li><strong>相对于前代的改进</strong>：Octopi-1.5（无论是否启用RAG）在多数情况下的准确率均高于前代Octopi-1模型。</li>
</ul>
<p><strong>消融实验总结</strong>：表III的结果本身构成了一个核心的消融实验，清晰地展示了RAG模块对性能的巨大贡献。此外，编码器基线（Encoder-1.5）在已见物体上表现良好，但无法处理未见物体，这凸显了VLM结合常识推理以及RAG动态扩展能力的价值。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>多模态融合与推理</strong>：提出了Octopi-1.5 VTLM，通过升级的触觉编码器和更强的基座VLM，实现了对视觉、触觉和语言信息的深度融合，并能利用常识知识进行触觉推理（如物体识别、属性排序）。</li>
<li><strong>RAG增强与动态学习</strong>：引入简单的RAG机制，显著提升了模型在已知和未知物体上的任务性能，并探索了通过动态添加样本实现“在线学习”新物体触觉表征的路径。</li>
<li><strong>便携演示系统</strong>：开发了集成多模态触觉传感器的TMI手持接口及配套软件，构建了一个无需完整机器人、便于交互和展示的便携式系统，降低了触觉AI研究的门槛。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>模型性能仍有提升空间，例如在根据粗糙度排序的任务上容易出错，对某些未见物体的排序准确率较低（仅43.33%）。</li>
<li>当前的RAG模块仅基于文本检索，未来需要研究如何高效地检索和融合触觉样本本身的信息。</li>
<li>Octopi-1.5仅输出文本，尚未直接与机器人操作动作闭环。将其扩展为视觉-触觉-语言-动作（VTLA）模型是未来的关键方向。</li>
</ol>
<p><strong>启示</strong>：<br>本研究展示了RAG在提升VTLM性能和处理新物体方面的潜力。未来的工作可以探索更复杂的检索策略（如多模态检索）、集成更多类型的触觉传感器信号，并最终向输出具体机器人动作的闭环决策系统演进。同时，便携的TMI系统为社区提供了一个易于使用的触觉数据收集和模型交互平台。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文介绍了Octopi-1.5视觉-触觉-语言模型，旨在解决机器人触觉感知在灵巧操作、材料识别及视觉遮挡场景中的整合问题。关键技术包括：处理多物体部分触觉信号的能力；采用检索增强生成（RAG）模块以提升任务性能并支持实时学习；基于Qwen2-VL 7B基础模型的新触觉编码器。通过手持触觉接口TMI的现场演示，模型成功执行了触觉推理任务（如猜测游戏中的物体识别和排序），并展示了RAG在教习新物体方面的潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.09985" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>