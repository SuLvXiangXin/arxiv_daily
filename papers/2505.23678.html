<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Grounded Reinforcement Learning for Visual Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Grounded Reinforcement Learning for Visual Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.23678" target="_blank" rel="noreferrer">2505.23678</a></span>
        <span>作者: Sarch, Gabriel, Saha, Snigdha, Khandelwal, Naitik, Jain, Ayush, Tarr, Michael J., Kumar, Aviral, Fragkiadaki, Katerina</span>
        <span>日期: 2025/05/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉语言模型（VLMs）通常以端到端方式运行，直接预测答案，缺乏根据任务调整计算策略或暴露中间推理过程的能力。基于提示的方法（如ViperGPT、V*）虽能分解任务，但生成的推理链是固定的，无法适应输入场景的结构。最近，基于思维链的强化学习（RL）在文本领域（如数学、编程）显著提升了模型能力，但RL只能放大或组合基础模型采样分布中已有的技能或推理行为。在视觉推理中，直接对基础VLM进行RL微调的尝试通常会产生抽象、无根基的推理，而非更丰富的、视觉上扎根的认知行为。</p>
<p>本文针对的关键痛点是：如何在应用RL之前，为VLMs嵌入有用的认知行为，以实现稳健的视觉推理？本文假设，当模型的文本推理步骤被明确地锚定在特定的图像区域时，模型既能“看得更好”，也能“思考得更好”，从而在推理过程中促进文本和视觉信息之间更有针对性和系统性的交叉引用。本文的核心思路是：提出一个多回合RL框架（ViGoRL），训练VLMs以视觉上有依据的方式进行推理，将每个推理步骤与图像坐标明确关联，并允许模型在需要时动态缩放至预测坐标以获取细粒度视觉反馈。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViGoRL的整体框架是一个两阶段流水线：首先通过基于蒙特卡洛树搜索（MCTS）的监督微调（SFT）为模型注入空间扎根的推理行为，然后应用分组相对策略优化（GRPO）进行强化学习以进一步优化这些行为，并可扩展至包含视觉反馈工具调用的多回合RL。</p>
<p><img src="https://arxiv.org/html/2505.23678v2/Figures/Figure2_V3.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：ViGoRL方法概览。（左）使用带有教师模型的MCTS生成锚定于特定图像区域的推理链。（中）将这些推理树线性化并用于监督微调（SFT）来训练基础模型。（右）应用带有基于结果奖励的GRPO来进一步优化扎根推理。</p>
</blockquote>
<p><strong>核心模块与技术细节：</strong></p>
<ol>
<li><p><strong>空间扎根的推理步骤定义</strong>：将每个推理步骤重新定义为元组 <code>n_t = ⟨ s_t, (x_t, y_t) ⟩</code>，其中 <code>s_t</code> 是文本思考，<code>(x_t, y_t)</code> 将其锚定到具体的图像坐标。完整的轨迹变为 <code>τ = [n_1, …, n_T, a]</code>。</p>
</li>
<li><p><strong>基于MCTS的预热数据生成</strong>：</p>
<ul>
<li><strong>目的</strong>：生成具有丰富视觉认知行为（如广泛区域探索、视觉验证、回溯）的扎根推理轨迹，以解决基础VLM采样分布偏向抽象推理的问题。</li>
<li><strong>过程</strong>：使用一个冻结的高容量教师模型（如Qwen2.5-VL-72B）进行MCTS搜索。每个MCTS节点对应一个扎根推理步骤 <code>n_t</code>。通过选择、扩展、模拟、回传四个步骤，系统性地探索有前景的图像区域和推理步骤。从约1500个提示中生成约3万条高质量推理轨迹。</li>
<li><strong>线性化用于SFT</strong>：将选中的从根到叶的路径线性化为两种训练样本：1) 直接链（成功轨迹）；2) 修正链（包含回溯和纠正的轨迹）。在此基础上对基础VLM进行SFT，得到初始策略 <code>π_θ0</code>。</li>
</ul>
</li>
<li><p><strong>基于GRPO的强化学习</strong>：</p>
<ul>
<li><strong>目的</strong>：在 <code>π_θ0</code> 的基础上，直接最大化任务奖励，同时保持流畅性和扎根性。</li>
<li><strong>奖励设计</strong>：总奖励 <code>R(𝒯) = λ_fmt * r_fmt + λ_task * r_task</code>。其中 <code>r_fmt</code> 鼓励有效且可解释的输出格式（包括所有坐标引用必须有效），<code>r_task</code> 捕获任务特定的正确性。</li>
</ul>
</li>
<li><p><strong>多回合RL用于视觉反馈</strong>：</p>
<ul>
<li><strong>动机</strong>：即使模型参考了多个区域，视觉编码器每次处理的仍是全局调整大小的图像，细粒度线索可能模糊。为了让模型能够获取更详细的视觉信息，引入了“交互式显微镜”机制，允许模型在预测坐标后请求高分辨率裁剪。</li>
<li><strong>多回合热身</strong>：将单步MCTS推理轨迹转换为对话式轨迹。在每个回合 <code>t</code>，模型生成文本思考 <code>s_{t+1}</code>，然后可以选择发出工具调用 <code>&lt;tool_call&gt;</code> 请求以坐标 <code>p_t</code> 为中心的裁剪，环境则返回裁剪后的图像观察 <code>&lt;observation&gt;</code>。</li>
<li><strong>多回合RL奖励设计</strong>：格式奖励 <code>r_fmt = r_grammar + r_div</code>。<code>r_grammar</code> 奖励对话遵守严格的标签自动机；<code>r_div</code> 奖励每个足够独特的坐标调用（与之前坐标距离≥10像素），最多奖励4次，以鼓励探索不同区域。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>空间扎根的推理步骤</strong>：将视觉注意力机制显式化为模型输出的一部分，迫使模型在推理过程中系统性地引用图像位置作为证据。</li>
<li><strong>MCTS引导的SFT预热</strong>：通过搜索生成富含目标认知行为的轨迹来引导模型，而非依赖成本高昂的人工标注或简单的线性采样，解决了RL无法从零开始诱导新行为的问题。</li>
<li><strong>多回合RL与视觉反馈</strong>：将单回合扎根推理扩展为动态交互过程，使模型能主动获取细粒度视觉信息，模仿人类“放大查看”的行为。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>使用的Benchmark/数据集</strong>：SAT-2、BLINK、RoboSpatial（空间推理）；ScreenSpot、ScreenSpot Pro（GUI理解与定位）；VisualWebArena（基于图像的网页交互）；V* Bench（视觉搜索）。此外，使用OS-ATLAS、ICAL、以及基于Segment Anything构建的细粒度视觉搜索数据集进行训练。</p>
<p><strong>对比的Baseline方法</strong>：</p>
<ol>
<li>方法对比：SFT-direct（仅基于最终答案的SFT）、Vanilla GRPO（标准格式和答案正确性奖励的GRPO）。</li>
<li>通用专有和开源VLM。</li>
<li>VLM工具使用流程（如V*）。</li>
<li>网页扎根模型（如大规模SFT或带有人类标注的模型）。</li>
</ol>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>空间推理</strong>：在SAT-2上，ViGoRL相比Vanilla GRPO带来12.9个百分点的准确率提升（达到64.0% vs 51.1%）；在BLINK上提升2.0个百分点（达到73.5% vs 71.5%）。ViGoRL在RoboSpatial上也表现最佳。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.23678v2/x1.png" alt="空间推理结果"></p>
<blockquote>
<p><strong>图4</strong>：在SAT-2、BLINK和RoboSpatial基准测试上的准确率。ViGoRL（我们的方法）显著优于SFT-direct和Vanilla GRPO基线。</p>
</blockquote>
<ul>
<li><p><strong>GUI理解与网页任务</strong>：在ScreenSpot Pro上，ViGoRL优于Vanilla GRPO和大规模网页微调模型。多回合RL进一步提升了在ScreenSpot Pro上的性能。在仅使用视觉输入的VisualWebArena上，ViGoRL超越了直接SFT、Vanilla GRPO以及同规模模型的先前SOTA（ICAL）。</p>
</li>
<li><p><strong>视觉搜索</strong>：在V* Bench上，通过多回合RL利用动态缩放视觉反馈，ViGoRL达到了86.4%的准确率，超越了VLM工具使用流程和专有VLM。</p>
</li>
</ul>
<p><img src="https://arxiv.org/html/2505.23678v2/x2.png" alt="视觉搜索与网页任务结果"></p>
<blockquote>
<p><strong>图6</strong>：在V* Bench（视觉搜索）和ScreenSpot Pro（GUI定位）上的性能。ViGoRL（多回合）在V* Bench上达到86.4%的SOTA性能。在ScreenSpot Pro上，ViGoRL也优于基线。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>消融实验证实了扎根性的重要性：没有空间锚定的模型性能显著下降。此外，研究分析了不同训练阶段模型表现出的视觉认知行为（区域探索、视觉子目标设定、视觉验证、回溯）。</p>
<p><img src="https://arxiv.org/html/2505.23678v2/x3.png" alt="行为分析"></p>
<blockquote>
<p><strong>图7</strong>：不同训练阶段模型在SAT-2任务上的视觉认知行为分析。ViGoRL（Ours）在区域探索、视觉子目标设定和视觉验证方面表现出最高的行为密度，而Vanilla GRPO则退化。</p>
</blockquote>
<p><strong>多回合奖励消融</strong>：实验表明，多样性奖励 <code>r_div</code> 对于鼓励模型在工具调用中探索不同的坐标至关重要，移除它会导致性能下降。</p>
<p><img src="https://arxiv.org/html/2505.23678v2/Figures/turn_bonus_plot.jpg" alt="多回合奖励消融"></p>
<blockquote>
<p><strong>图5</strong>：多回合RL中多样性奖励的消融研究。包含 <code>r_div</code> 奖励显著提高了在ScreenSpot Pro上的定位成功率。</p>
</blockquote>
<p><strong>定性结果与人类评估</strong>：<br>人类评估表明，ViGoRL产生的空间引用不仅位置准确，而且有助于理解模型的推理步骤。</p>
<p><img src="https://arxiv.org/html/2505.23678v2/Figures/Figure3_2_V4.jpg" alt="定性对比"></p>
<blockquote>
<p><strong>图2</strong>：不同方法的推理过程对比。标准CoT和Vanilla GRPO（左、中）表现出视觉上无根基的推理，依赖对场景元素的模糊引用（黄色），常导致错误答案（红色）。而视觉扎根RL（右）明确引用物体位置，展示了精确的空间锚定（蓝色），并更常产生正确的推理结果。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ViGoRL，一个通过强化学习训练视觉语言模型进行空间扎根推理的框架，将每个推理步骤与图像坐标明确关联。</li>
<li>设计了一个两阶段训练流程，首先利用MCTS生成富含视觉认知行为的轨迹进行SFT预热，然后应用GRPO进行优化，并扩展支持多回合视觉反馈。</li>
<li>在多个视觉推理基准测试上实现了显著的性能提升，并通过实验证明扎根性能够放大其他有益的视觉认知行为（如区域探索、子目标设定），且产生的推理对人类更具可解释性。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确列出具体的局限性部分，但从方法描述中可推断，MCTS数据生成和RL训练过程可能计算成本较高。此外，方法依赖于能够生成合理坐标的基础VLM作为教师。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>扎根作为关键归纳偏置</strong>：显式的空间扎根可以作为一种有效的归纳偏置，引导模型发展出更系统、更像人类的视觉推理策略，这可以推广到其他需要精细感知-推理耦合的任务中。</li>
<li><strong>行为引导的RL</strong>：在视觉等复杂领域，单纯的基于结果的RL可能不足以诱导出期望的行为。结合搜索或课程学习等方式预先注入关键技能（如本工作的MCTS-SFT），是使RL成功的重要途径。</li>
<li><strong>交互式感知</strong>：多回合RL与视觉工具调用的结合展示了让模型主动控制感知输入以服务于推理的潜力，这为构建更自主、适应性更强的具身智能体提供了思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ViGoRL模型，解决视觉推理中模型难以将抽象推理步骤显式关联到具体空间位置的问题。方法采用视觉基础强化学习，通过多轮RL框架使模型在推理过程中动态聚焦相关图像区域，生成空间锚定的推理轨迹。实验表明，该方法在SAT-2、BLINK等多个视觉推理基准上超越无基础机制的基线，在V* Bench视觉搜索任务中达到86.4%准确率，且人类评估证实其视觉参考具有空间准确性并提升推理可解释性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.23678" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>