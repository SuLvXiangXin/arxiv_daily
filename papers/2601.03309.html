<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.03309" target="_blank" rel="noreferrer">2601.03309</a></span>
        <span>作者: Zhang, Jianke, Chen, Xiaoyu, Wang, Qiuyue, Li, Mingsheng, Guo, Yanjiang, Hu, Yucheng, Zhang, Jiajun, Bai, Shuai, Lin, Junyang, Chen, Jianyu</span>
        <span>日期: 2026/01/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过将预训练的大型视觉-语言模型集成到策略骨干中，以其出色的泛化能力受到广泛关注。现有VLA方法主要致力于设计更复杂的网络架构、融入额外的训练范式或多模态信息、以及改进动作解码方案。然而，一个核心但鲜被系统研究的问题是：底层VLM的选择及其具体能力如何影响下游VLA策略的性能？本文旨在重新审视这一关键问题，探究VLM的能力如何转化为下游VLA策略的性能。本文的核心思路是设计一个最小适应框架VLM4VLA，以公平、高效地比较不同VLM，并系统研究VLM的通用能力、具身特定能力以及不同模态对下游控制性能的影响。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了VLM4VLA框架，其目标是以一种公平、可复现且极简的方式，将各种通用VLM转化为VLA策略，从而隔离并评估VLM本身的能力。</p>
<p><img src="https://arxiv.org/html/2601.03309v1/fig/tongyi.jpg" alt="方法框架总览"></p>
<blockquote>
<p><strong>图1</strong>：VLM4VLA框架概览。（左）评估不同VLM骨干的流程，这些骨干在经过可选的辅助具身任务微调后，在下游任务上进行评估。（右下）系统研究影响VLM到VLA迁移的三个因素：VLM骨干的选择、在辅助具身任务上微调的影响、以及不同训练策略（冻结 vs. 微调不同VLM模块，从头训练）的影响。（右上）可视化不同VLM骨干在下游任务上的不一致性能。</p>
</blockquote>
<p>整体流程是：将选定的VLM（原始或经过辅助任务微调）封装进VLM4VLA网络，使用一致的下游机器人数据进行训练，然后在目标环境中进行评估。输入仅为单视角图像和语言指令，排除了本体感知等其他模态，以隔离VLM的贡献。</p>
<p><img src="https://arxiv.org/html/2601.03309v1/x1.png" alt="网络设计"></p>
<blockquote>
<p><strong>图2</strong>：VLM4VLA中的VLA网络设计。展示了如何将不同的VLM（如Qwen-VL, Paligemma, Kosmos）通过一致的接口（引入可学习的ActionQuery令牌）适配为VLA策略。</p>
</blockquote>
<p>核心模块是<strong>一个轻量级的网络插件</strong>。具体而言，为了适配不同VLM的预训练输入格式，为每个VLM设计特定的令牌拼接方案。在语言指令和视觉嵌入之后，引入一个<strong>可学习的动作查询令牌</strong>。将该令牌经由VLM编码后的最后一层隐藏状态，通过一个<strong>小型MLP策略头</strong>解码为一个动作块。公式表示为：<code>action = MLP(VLM([&lt;img&gt;...&lt;img&gt; &lt;text&gt;...&lt;text&gt; &lt;ActionQuery&gt;]))</code>。整个框架新增的可学习参数少于总参数的1%，确保了比较的公平性。</p>
<p>在训练目标上，本文摒弃了广泛使用的扩散或流匹配损失，因为这些损失会在推理时引入显著的随机性，不利于公平比较。相反，采用<strong>最大似然模仿学习目标</strong>，使用修改后的MSE损失（Huber损失）优化末端执行器的相对位置，并使用二元交叉熵损失优化末端执行器的离散状态。</p>
<p>与现有方法相比，VLM4VLA的创新性体现在其<strong>极简且一致的设计理念</strong>上。它并非提出一个更强大的新网络，而是构建了一个“公平的测试平台”，通过最小化策略头设计带来的干扰，专注于评估和比较不同VLM骨干本身的特性与能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个常用基准上进行：<strong>Calvin</strong> (ABC-D任务)、<strong>SimplerEnv</strong> (Bridge-V2任务套件) 和 <strong>Libero</strong> (最具挑战性的Libero-Long套件)。对比的基线方法包括专家VLA模型 <strong>OpenVLA</strong>、<strong>pi0</strong> 和 <strong>ThinkAct</strong>，以及多个开源VLM（如Qwen2.5/3-VL系列、Paligemma系列、Kosmos-2）通过VLM4VLA框架适配后的版本。</p>
<p>关键实验结果如下：</p>
<ol>
<li><strong>VLM预训练始终有益，但通用能力并非下游性能的可靠预测指标</strong>：如表1和表2所示，所有VLM初始化的策略均优于从头训练的基线（见附录），证实了VLM预训练的价值。然而，一个VLM在标准VQA基准上的通用能力与其下游控制性能关联微弱且不一致。例如，在Calvin上表现出色的QwenVL系列（如Qwen2.5VL-7B平均完成4.057个任务），在SimplerEnv-Bridge上却被较小的Kosmos-2（平均成功率60.4%）超越。Paligemma系列在SimplerEnv上优于Qwen2.5VL，但在Libero上表现相近。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.03309v1/x2.png" alt="通用能力与VLA性能相关性分析"></p>
<blockquote>
<p><strong>图3</strong>：VLM通用能力与VLA性能的线性关系对比。结果表明，Calvin任务表现与VQA基准分数高度相关，而Simpler和Libero环境则无明显关联，揭示了VLA任务需求与现有VLM评估标准之间的差距。</p>
</blockquote>
<ol start="2">
<li><p><strong>提升VLM的具身特定能力未必能改善下游控制</strong>：论文在Qwen2.5VL骨干上进行了七种辅助具身任务（如Robopoint、Vica-332k、BridgeVQA、Robo2vlm、Robobrain2等）的微调实验。结果显示，尽管这些微调显著提升了VLM在相应辅助任务上的性能（如Robopoint任务提升~20%，Robo2vlm任务提升60%），但将这些微调后的VLM作为骨干时，其在Calvin下游控制任务上的性能<strong>并未得到一致提升，甚至可能下降</strong>。这挑战了“通过辅助任务增强VLM能直接带来更好VLA策略”的直觉。</p>
</li>
<li><p><strong>视觉编码器是性能瓶颈，注入控制相关知识有效</strong>：模态层面的消融实验（图1c）表明，<strong>视觉编码器</strong>是VLA性能的主要瓶颈。微调视觉编码器对获得强大的控制性能至关重要，而语言编码器的影响相对较小。更重要的是，即使在下游微调期间<strong>冻结视觉编码器</strong>，只要在VLM预训练阶段通过FAST tokenizer等方法向其<strong>注入与控制相关的监督信息</strong>，也能带来一致的性能提升。这直接证明了当前VLM预训练目标与具身动作规划所需的视觉表征之间存在持续的领域鸿沟。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.03309v1/fig/combined_subplot_violin_plot.png" alt="消融实验：不同模块冻结的影响"></p>
<blockquote>
<p><strong>图5</strong>：（论文中未在正文详细描述，但从标题和上下文推断为消融实验）可能展示了不同模块（视觉、语言）冻结或微调设置下模型性能的分布，进一步佐证了视觉模块的关键作用。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个极简、公平的VLM4VLA框架，用于系统评估不同VLM骨干在下游机器人控制任务中的表现；2) 通过大规模实证研究，挑战了“VLM的通用能力或特定具身技能提升能直接预测或转化为更好的VLA性能”这一常见假设；3) 识别出视觉编码器是当前VLA模型的主要瓶颈，并证明缩小VLM预训练与具身控制需求之间的视觉表征差距是有效的改进方向。</p>
<p>论文自身提到的局限性包括：评估仅在仿真环境中进行；输入未包含可能有益的本体感知信息；研究的VLM参数规模主要限于1B到10B。</p>
<p>这项工作对后续研究的重要启示是：盲目追求在通用VQA基准上更强的VLM，或简单地用机器人数据对VLM进行辅助任务微调，可能并非提升VLA性能的最有效途径。未来的研究需要更深入地理解具身控制任务所需的特定视觉-语言表征，并可能需要对VLM（尤其是其视觉编码器）的预训练目标进行重新设计，以更好地对齐具身智能的需求。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究视觉-语言-动作（VLA）模型中，预训练视觉-语言模型（VLM）的选择和能力如何影响下游策略性能。提出VLM4VLA方法，通过最小适应管道将通用VLM转换为VLA策略，仅使用少量新可学习参数进行公平高效比较。实验发现：VLM初始化相比从头训练有优势，但其通用能力不能预测下游任务性能；改进VLM在特定具身技能（如具身QA）上也不保证更好控制性能。视觉模块是主要瓶颈，向VLM视觉编码器注入控制相关监督可带来一致性能提升，即使编码器冻结，这揭示了VLM预训练目标与具身动作规划要求间的领域差距。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.03309" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>