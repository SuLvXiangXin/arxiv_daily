<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DiffOG: Differentiable Policy Trajectory Optimization with Generalizability - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>DiffOG: Differentiable Policy Trajectory Optimization with Generalizability</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.13807" target="_blank" rel="noreferrer">2504.13807</a></span>
        <span>作者: Xu, Zhengtong, Miao, Zichen, Qiu, Qiang, Zhang, Zhe, She, Yu</span>
        <span>日期: 2025/04/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习已成为通过人类演示赋予机器人复杂操作技能的流行范式。基于视觉运动（visuomotor）的策略通过神经网络将传感器观测直接映射到动作，已在广泛任务中证明有效。然而，这些学习到的策略在现实世界部署时存在显著局限，尤其是在安全性、鲁棒性和严格约束满足方面至关重要时。与明确考虑运动约束并能确保平滑可靠轨迹的经典基于模型的轨迹优化方法相比，模仿学习方法可能产生次优或不平稳的动作。这种轨迹会降低整个机器人系统的鲁棒性并引发关键的安全问题。当前提高轨迹质量的方法主要基于事后处理，例如应用轨迹优化方法来细化学习策略的输出。但由于策略训练过程本身并未考虑约束或平滑性目标，这些调整可能导致轨迹偏离演示数据集的分布，最终降低策略性能，且难以泛化到高维动作空间或长时程任务。</p>
<p>本文针对上述痛点，提出了一种可泛化的可微分策略轨迹优化框架DiffOG。其核心思路是：通过一个可微分的轨迹优化层，在监督学习框架内直接学习一个轨迹优化器，该优化器能够平衡约束满足、平滑性以及对演示数据的保真度，从而在提升轨迹质量（平滑性、约束合规性）的同时，保持优化后动作与演示分布的一致性，避免策略性能下降。</p>
<h2 id="方法详解">方法详解</h2>
<p>DiffOG的整体目标是以未优化的动作轨迹 (\mathbf{a}_t) 为输入，输出一个既能完成演示任务、又能满足硬约束并提升平滑性的优化轨迹 (\mathbf{y}_t^\star)。它支持两种训练模式：1) 直接在演示数据集上进行监督学习（DiffOG数据集训练）；2) 对预训练策略生成的动作进行细化（DiffOG细化训练）。在推理阶段，DiffOG被应用于基础策略之上以优化其动作。</p>
<p><img src="https://arxiv.org/html/2504.13807v5/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：DiffOG训练与推理的高级概览。红色块（DiffOG）接收基础策略输出的未优化动作序列，通过基于变换器的可微分轨迹优化层，输出优化后的动作序列。</p>
</blockquote>
<p><strong>动作空间与轨迹公式化</strong>：模仿学习数据集格式为 (\mathcal{D} = { (o_0^i, a_0^i, ...) })，其中 (a \in \mathbb{R}^{D_a}) 是动作。动作分为离散动作（如抓握）和连续动作（如机器人运动）。DiffOG引入选择矩阵 (S \in \mathbb{R}^{D_c \times D_a}) 来筛选出需要轨迹优化的连续自由度（DOFs），得到过滤后的动作变量 (c = Sa \in \mathbb{R}^{D_c})。现代策略通常预测动作序列 (\mathbf{a}<em>t = [a_t^T, ..., a</em>{t+T_p-1}^T]^T \in \mathbb{R}^{T_p D_a})。应用序列选择矩阵 (\mathbf{S} = \text{blkdiag}(S, ..., S))，得到过滤后的动作序列 (\mathbf{c}_t = \mathbf{S}\mathbf{a}_t \in \mathbb{R}^{T_p D_c}) 及其离散时间导数 (\frac{d\mathbf{c}<em>t}{dt} = \mathbf{A}</em>{\text{diff}} \mathbf{c}<em>t)，其中 (\mathbf{A}</em>{\text{diff}}) 是差分矩阵。</p>
<p><strong>可微分轨迹优化公式</strong>：优化轨迹定义为 (\mathbf{y}_t = [\hat{a}<em>t^T, ..., \hat{a}</em>{t+T_p-1}^T]^T)，对应的过滤序列为 (\hat{\mathbf{c}}_t = \mathbf{S}\mathbf{y}_t)。DiffOG的核心是求解以下二次规划问题：<br>[<br>\mathbf{y}_t^\star = \underset{\mathbf{y}_t}{\operatorname{argmin}} \frac{1}{2} \mathbf{y}<em>t^T \mathbf{Q} \mathbf{y}<em>t + \mathbf{a}<em>t^T \mathbf{y}<em>t + \frac{\alpha}{2} \left( \frac{d\hat{\mathbf{c}}<em>t}{dt} \right)^T \frac{d\hat{\mathbf{c}}<em>t}{dt},<br>]<br>[<br>\text{subject to } d</em>{\text{min}} \Delta t \leq \hat{c}</em>{t+k+1} - \hat{c}</em>{t+k} \leq d</em>{\text{max}} \Delta t, \quad k = 0,1,...,T_p-2.<br>]<br>其中，(\mathbf{Q}) 是权重矩阵，(\alpha) 是平滑权重（控制对轨迹平滑性的重视程度），约束条件对机器人动作的导数（如速度）进行了限制。该问题在 (\mathbf{Q}) 对称正定且 (d</em>{\text{min}} &lt; d</em>{\text{max}}) 的条件下总是可行且严格凸的，从而保证了最优解的唯一性以及优化层相对于其输入和参数的可微分性。</p>
<p><strong>学习目标与可学习参数</strong>：通过对 (\mathbf{Q}) 进行Cholesky分解 ((\mathbf{Q} = \mathbf{R}^T \mathbf{R})) 并定义 (\mathbf{g}_t = -(\mathbf{R}^T)^{-1} \mathbf{a}_t)，上述优化问题可重写为：<br>[<br>\mathbf{y}_t^\star = \underset{\mathbf{y}_t}{\operatorname{argmin}} \frac{1}{2} | \mathbf{R} \mathbf{y}_t - \mathbf{g}_t |^2 + \frac{\alpha}{2} \left( \frac{d\hat{\mathbf{c}}_t}{dt} \right)^T \frac{d\hat{\mathbf{c}}_t}{dt} \quad \text{(subject to same constraints)}.<br>]<br>这种形式揭示了三个可解释的学习目标：1) <strong>变换</strong>：通过 (\mathbf{R}) 和 (\mathbf{g}_t) 对输入动作进行线性变换（若去掉约束和平滑项，解为 (\mathbf{y}_t = -\mathbf{Q}^{-1}\mathbf{a}_t)）。2) <strong>约束满足</strong>：通过硬约束确保动作导数在合理范围内。3) <strong>平滑性</strong>：通过最小化动作导数的平方和来提升轨迹平滑性。</p>
<p>为了赋予优化层强大的表征能力和跨任务泛化能力，DiffOG将 (\mathbf{R}) 和 (\mathbf{g}_t) 参数化为一个基于变换器（Transformer）的轨迹编码器的输出。该编码器以未优化的动作序列 (\mathbf{a}_t) 为输入，通过自注意力机制捕获序列内依赖关系，并输出构建优化问题所需的参数。这使得优化层能够根据输入轨迹的特定模式动态调整其行为，从而适应不同任务（包括长时程、高维双臂操作）的需求。训练时，通过可微分优化层进行前向传播，计算优化轨迹 (\mathbf{y}_t^\star) 与目标演示动作之间的损失（如均方误差），并通过该层和变换器编码器进行反向传播，端到端地学习优化器的参数。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在11个模拟任务和2个真实世界任务上进行。模拟任务基于Isaac Gym，涵盖多种操作场景。使用的数据集/基准包括PushT（推动任务）和DP3（多样化操作任务）。对比的基线方法包括：1) <strong>原始策略</strong>；2) <strong>贪婪约束裁剪</strong>：直接裁剪违反速度约束的动作；3) <strong>基于惩罚的轨迹优化</strong>：在损失函数中加入约束违反的惩罚项进行后处理优化；4) <strong>现有约束视觉运动策略Leto</strong>。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2504.13807v5/images/max_mimic_new.png" alt="最大速度对比（模仿）"></p>
<blockquote>
<p><strong>图2</strong>：在模仿学习设置下，各方法在测试集上产生的轨迹最大速度统计。DiffOG显著降低了最大速度（意味着更好的约束满足和平滑性），且性能与任务无关。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.13807v5/images/std_mimic_new.png" alt="速度标准差对比（模仿）"></p>
<blockquote>
<p><strong>图3</strong>：在模仿学习设置下，轨迹速度的标准差统计。DiffOG产生的速度波动最小，表明轨迹最为平滑。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.13807v5/images/max_dp3_new.png" alt="最大速度对比（DP3）"></p>
<blockquote>
<p><strong>图4</strong>：在DP3数据集任务上，DiffOG refine训练模式下的最大速度对比。DiffOG在几乎所有任务上都取得了最低的最大速度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.13807v5/images/std_dp3_new.png" alt="速度标准差对比（DP3）"></p>
<blockquote>
<p><strong>图5</strong>：在DP3数据集任务上，速度标准差对比。DiffOG在大多数任务上实现了最小的速度波动。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.13807v5/images/pusht_new.png" alt="PushT任务成功率"></p>
<blockquote>
<p><strong>图6</strong>：在PushT任务上的成功率对比。DiffOG在显著提升轨迹质量（降低速度）的同时，保持了与原始策略相当的高成功率，而贪婪裁剪等方法严重损害了策略性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.13807v5/images/real_traj_stat_new.png" alt="真实世界轨迹统计"></p>
<blockquote>
<p><strong>图7</strong>：真实世界任务（螺丝刀插入和U盘插入）的轨迹速度统计。DiffOG有效降低了真实机器人执行轨迹的速度和波动。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br><img src="https://arxiv.org/html/2504.13807v5/images/vssm.png" alt="消融研究：变换器的作用"></p>
<blockquote>
<p><strong>图8</strong>：消融研究：将变换器编码器替换为简单MLP对性能的影响。使用变换器的DiffOG在平滑性（速度标准差）上显著优于MLP版本，证明了变换器增强表征能力的重要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.13807v5/images/vsbond.png" alt="消融研究：平滑权重的影响"></p>
<blockquote>
<p><strong>图9</strong>：消融研究：平滑权重 (\alpha) 对性能的影响。随着 (\alpha) 增大，轨迹平滑性（速度标准差）提升，但过大的 (\alpha) 可能会略微增加最大速度并影响任务成功率，需要权衡。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个可微分的轨迹优化框架（DiffOG），能够直接优化机器人策略生成的动作轨迹，在保证任务完成（对齐演示分布）的前提下，显著提升轨迹的平滑性和硬约束满足能力。</li>
<li>将基于变换器的轨迹编码器集成到可微分优化层中，极大地增强了优化器的表征能力和跨多样化任务（包括长时程、高维操作）的泛化能力。</li>
<li>通过严谨的公式推导（Cholesky分解）和理论分析（可行性、凸性、可微性），为DiffOG提供了可解释的理论基础，明确了其同时实现变换、约束和平滑三个目标的工作原理。</li>
</ol>
<p><strong>局限性</strong>：论文提到，DiffOG的性能在一定程度上依赖于基础策略或演示数据的质量。此外，可微分优化层的引入会增加一定的计算开销。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li>DiffOG展示了将可解释的模型化优化与数据驱动的神经网络表征成功结合的潜力，为构建更安全、鲁棒的机器人学习系统提供了新思路。</li>
<li>该方法框架具有灵活性，未来可探索将其与更先进的策略架构（如基于扩散模型的策略）相结合，或扩展到处理更复杂的动态约束和状态约束中。</li>
<li>变换器在优化参数生成中的有效性表明，为优化问题注入数据驱动的自适应能力是提升其泛化性能的关键方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DiffOG框架，旨在解决模仿学习视觉运动策略生成的动作轨迹不平滑、难以满足约束的问题。方法核心是将Transformer与可微分的轨迹优化层集成，在保持与原始示范分布对齐的同时，优化动作轨迹的平滑性与约束符合性。实验在11个仿真任务和2个真实任务上验证，DiffOG显著提升了轨迹质量，且对策略性能影响极小，优于贪婪约束裁剪等基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.13807" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>