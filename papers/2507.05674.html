<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Integrating Diffusion-based Multi-task Learning with Online Reinforcement Learning for Robust Quadruped Robot Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Integrating Diffusion-based Multi-task Learning with Online Reinforcement Learning for Robust Quadruped Robot Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.05674" target="_blank" rel="noreferrer">2507.05674</a></span>
        <span>作者: Bin Liang Team</span>
        <span>日期: 2025-07-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，机器人领域在模仿学习和强化学习方面各有进展。基于生成模型，特别是扩散模型的模仿学习方法在操作任务中展现出强大的多任务泛化、有效的语言条件化和高样本效率等优势。然而，其在腿部运动中的应用相对较少，主要面临两个关键局限性：1) 四足系统复杂的高度非线性动力学使得模仿学习中累积的误差常导致运动不稳定；2) 由于缺乏捕捉中间过渡阶段的数据，实现快速鲁棒的任务转换存在困难。与此同时，在线强化学习（RL）在过去几年中在腿部机器人控制上取得了显著成果，能够通过试错提升策略鲁棒性，但其通常依赖于精心设计的复杂奖励函数，样本效率较低，且与语言条件化任务的兼容性不足。</p>
<p>本文针对上述痛点，提出了一种融合扩散模型多任务预训练与在线强化学习微调的新视角，旨在为四足机器人实现语言条件化控制与鲁棒的任务转换。核心思路是：首先利用扩散模型在多样化的离线多任务数据集上进行预训练，获得语言引导的多技能策略；随后，在仿真环境中使用在线PPO算法和简单奖励函数对该策略进行微调，以增强其鲁棒性和稳定转换能力，最终通过优化部署实现实时控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>DMLoco的整体框架是一个三阶段流程：1) <strong>离线模仿学习</strong>：在由单任务专家策略收集的多技能数据集上，使用扩散模型进行行为克隆，学习一个以状态和任务描述（结构化指令或语言指令）为条件的策略。2) <strong>在线强化学习微调</strong>：在仿真环境中，使用PPO算法和简化的奖励函数（仅含速度跟踪误差和防摔倒检测）对预训练策略进行微调，以提升鲁棒性并学习数据集中未包含的稳定步态转换。3) <strong>优化部署</strong>：利用DDIM进行高效采样，并使用TensorRT进行加速，最终将策略部署到真实机器人上，实现50Hz的板载实时推理。</p>
<p><img src="https://arxiv.org/html/2507.05674v2/pic2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DMLoco整体框架。左侧为离线模仿学习，中间为在线PPO微调，右侧为在真实机器人上的实时部署。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>扩散策略网络</strong>：采用基于UNet的去噪扩散概率模型（DDPM）作为策略。在训练时，随机采样扩散步数k，向真实动作注入高斯噪声，网络以带噪声的动作、状态序列、目标序列和步数k为输入，预测噪声分量。损失函数为预测噪声与真实噪声的均方误差。与常见于操作任务的扩散策略不同，DMLoco<strong>不采用动作分块技术</strong>，而是预测当前单步动作，以适应腿部运动对状态快速变化的敏捷反应需求。</li>
<li><strong>语言条件化模块</strong>：任务描述g可以是结构化指令（速度命令+步态类型one-hot编码）或自然语言指令l。语言指令通过一个轻量级语言编码器E_l映射为连续向量。该编码器基于预训练的all-MiniLM-L6-v2模型，后接一个两层MLP进行降维，并通过对齐损失（L_lang）训练，使其输出与对应的结构化指令语义对齐。</li>
<li><strong>在线微调模块</strong>：将DDPM策略的优化形式化为一个两层马尔可夫决策过程（MDP），从而可以直接应用PPO等策略梯度算法最大化累积奖励。微调时采用相对较高的扩散噪声调度以鼓励探索。</li>
<li><strong>高效推理模块</strong>：部署时采用DDIM采样替代DDPM。DDIM使用确定性、非随机的方式进行反向扩散，能以极少的采样步数（如5步）生成高质量样本，大幅提升推理速度。结合TensorRT进行模型加速与优化，满足实时性要求。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.05674v2/pic3.png" alt="网络架构"></p>
<blockquote>
<p><strong>图3</strong>：DMLoco网络架构。左侧为语言编码器，右侧为扩散策略网络，以观测序列和目标序列为条件输入，经过K步去噪生成动作。</p>
</blockquote>
<p>主要创新点体现在：1) <strong>两阶段训练范式</strong>：结合了扩散模型强大的多模态、多任务表征能力和在线RL的环境交互、鲁棒性优化能力。2) <strong>简化的奖励设计</strong>：在线微调阶段仅使用速度跟踪和防摔倒两项奖励，避免了复杂奖励工程，同时成功引导出稳定的步态转换行为。3) <strong>实时部署优化</strong>：通过DDIM采样和TensorRT加速，首次将扩散策略在四足机器人上的控制频率提升至50Hz，满足了实时控制需求。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：仿真实验在Isaac Gym中进行，硬件为NVIDIA GeForce RTX 4080笔记本。实物实验在搭载NVIDIA Jetson Orin NX的Unitree Aliengo机器人上进行，并集成Whisper语音识别模型。评估任务包括：1) <strong>速度跟踪任务</strong>：在四种步态（小跑、跳跃、侧对步、腾跃）下，跟踪随机生成的速度指令，每种步态进行100次仿真试验。2) <strong>步态转换任务</strong>：在真实机器人上，于三种不同速度（0.1， 0.5， 1.0 m/s）下测试四种步态转换组合，每种转换重复5次。</p>
<p><strong>对比基线</strong>：选择了两类先进方法进行对比：模仿学习类（BeT， DiffuseLoco）和多技能RL类（WTW， Cassi）。同时评估了DMLoco及其语言条件化变体DMLoco-lang。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>速度跟踪性能（表I）</strong>：DMLoco在总成功率达到100%，且跟踪误差（0.14 m²/s²）最低，全面优于所有基线。纯离线模仿方法（BeT， DiffuseLoco）成功率较低；RL方法（WTW， Cassi）因奖励函数复杂或需额外训练技能判别器，在跟踪精度上略逊一筹。DMLoco-lang仅使用语言指令训练，取得了与DMLoco相当的成功率（100%），仅跟踪误差略有增加（0.19 m²/s²），证明了其优秀的语言泛化能力。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.05674v2/pic1.png" alt="速度跟踪性能"></p>
<blockquote>
<p><strong>图2</strong>：DMLoco执行不同步态的截图，红点表示地面接触点，展示了语言条件控制、鲁棒任务转换和实时推理三大核心特性。</p>
</blockquote>
<ul>
<li><strong>步态转换性能（图4）</strong>：DMLoco在三种速度下的转换成功率均最高。DiffuseLoco（无微调）成功率低，且随速度升高急剧下降；Cassi因缺乏显式的防摔倒机制，高速下成功率降低；WTW因需平衡复杂奖励中的多项子目标，性能略低于DMLoco。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.05674v2/gait.png" alt="步态转换成功率"></p>
<blockquote>
<p><strong>图4</strong>：不同基线方法在三种速度下的步态转换成功率。DMLoco在所有速度下均表现最佳。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>微调的作用（图5）</strong>：在线微调显著提升了策略性能。仿真学习中，成功率从约0.6提升至近1.0。真实机器人实验中，微调策略在低、中、高速下的转换成功率分别比预训练策略提高了45%、70%和65%，证明了微调对增强动态条件下鲁棒性的关键作用。</li>
<li><strong>状态与动作视野的影响（图6）</strong>：对于状态视野，过短（如1）或过长（如50）都会导致性能下降，适中长度（如30）最佳。对于动作视野，采用单步预测（视野=1）性能最好，增大动作分块会因响应延迟导致成功率下降和跟踪误差上升，验证了单步预测对动态腿部运动的必要性。</li>
<li><strong>采样方法对比（表II）</strong>：DDIM仅需5步采样即可达到99%的稳定性，且推理速度（0.028秒/次）远快于需要100步采样的DDPM（0.53秒/次）。将DDPM采样步数降至5步会导致稳定性完全丧失（0%），凸显了DDIM在兼顾速度与稳定性方面的优势。</li>
<li><strong>推理速度优化（图7）</strong>：使用TensorRT优化后，在RTX 4080和Jetson Orin NX上的推理频率分别达到142 Hz和53 Hz，相比PyTorch实现了4倍和7倍的加速，成功满足了50Hz的实时控制要求。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.05674v2/combined_finetune.png" alt="微调学习曲线与真实成功率"></p>
<blockquote>
<p><strong>图5</strong>：(a) 微调过程中奖励和成功率的学习曲线，展示了稳定的提升。(b) 预训练与微调策略在真实机器人不同速度下的步态转换成功率对比。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.05674v2/combined_horizon.png" alt="视野消融实验"></p>
<blockquote>
<p><strong>图6</strong>：不同状态视野（左）和动作视野（右）下预训练策略的稳定性与跟踪误差。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.05674v2/speed.png" alt="推理频率对比"></p>
<blockquote>
<p><strong>图7</strong>：在不同硬件平台及软件优化下的推理频率。TensorRT优化后达到了实时控制所需的50Hz基准。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个新颖的四足机器人多任务学习框架DMLoco，首次将扩散模型的多任务预训练与在线PPO微调有效结合，在获得多样化技能的同时保证了强鲁棒性。2) 在训练集未包含显式转换数据的情况下，实现了仿真与实物机器人上稳定的步态转换。3) 提供了语言条件化控制接口，并展现出对未见自然语言指令的良好泛化能力。4) 通过DDIM采样和TensorRT优化，成功在资源受限的硬件上实现了50Hz的实时板载推理与部署。</p>
<p><strong>局限性</strong>：论文提到，尽管DMLoco-lang能泛化到许多未见指令，但其性能仍可能受到语言编码器语义对齐能力的限制。此外，扩散模型相对于传统前馈网络仍有更高的计算需求。</p>
<p><strong>后续启示</strong>：本工作证明了生成模型与在线RL结合在复杂动态控制任务中的巨大潜力。这种两阶段范式可推广至其他需要高维、多模态技能学习的机器人领域。未来的研究可以探索更高效的扩散模型架构或蒸馏技术以进一步降低计算开销，或尝试将视觉观测纳入条件输入以实现更复杂的导航与交互任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对扩散模型在四足机器人运动控制中应用不足的问题，提出DMLoco框架。该方法集成扩散模型多任务预训练与在线PPO微调，利用DDIM进行高效采样，并通过TensorRT优化部署，实现语言引导的鲁棒控制。核心实验表明，优化后的策略能部署于真实机器人，并以50Hz频率实时运行。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.05674" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>