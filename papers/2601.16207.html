<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.16207" target="_blank" rel="noreferrer">2601.16207</a></span>
        <span>作者: Park, Jongwoo, Ranasinghe, Kanchana, Jang, Jinhyeok, Mata, Cristina, Jang, Yoo Sung, Ryoo, Michael S</span>
        <span>日期: 2026/01/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉-语言-动作模型通常将预训练视觉编码器输出的二维图像块网格展平为一维令牌序列，并与文本令牌拼接，输入到单一Transformer管道中进行处理。这种设计虽然利用了丰富的视觉和语言知识，但破坏了图像固有的二维邻域结构，将视觉补丁视为一维的“单词”序列。展平操作削弱了局部相关性，可能模糊物体边界，导致实例级特征在与文本令牌的反复交互中被稀释。因此，恢复物体边界、属性关系和细粒度空间关系等关键线索变得困难，阻碍了需要精确物体交互的操控行为。</p>
<p>本文针对VLA模型因视觉令牌展平而导致的二维空间线索丢失这一具体痛点，提出了一种无需训练、基于提示的轻量级方法。核心思路是：在推理时，从模型内置的冻结视觉编码器中提取表征补丁间相似性的亲和力图，并将这些亲和力提示选择性地注入到语言模型中存放实例级特征的层，通过重新加权视觉令牌来恢复几何结构，同时保持所有模型参数固定。</p>
<h2 id="方法详解">方法详解</h2>
<p>IVRA的目标是通过向LLM的选定层注入从VLA冻结视觉编码器中提取的亲和力提示，来恢复VLA模型内部的二维空间结构。整体流程包括亲和力图提取、亲和力引导的视觉令牌池化，以及将此过程集成到VLA架构中。</p>
<p><img src="https://arxiv.org/html/2601.16207v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。(a) 左侧：冻结的视觉编码器提供亲和力提示，通过加权池化令牌来引导令牌混合，从而保留实例级线索并提升操控策略质量。较亮区域表示相对于参考点（红点）的更高亲和力。(b) 右侧：应用IVRA后的亲和力图显示出更清晰的物体边界和物体分离，有助于精确的机器人操控。</p>
</blockquote>
<p><strong>1. 亲和力图提取</strong>：输入图像被分割为N个补丁，通过冻结视觉编码器的中间层（最终层之前的若干块）获得对应的d维补丁嵌入 {f_i}。亲和力矩阵 A ∈ R^{N×N} 通过计算补丁嵌入间的余弦相似度得到：A_{ij} = (f_i · f_j) / (‖f_i‖ ‖f_j‖)。A_{ij}值越高，表明补丁i和j越可能属于同一物体或具有视觉相似性。该亲和力图以紧凑形式保留了二维空间布局。</p>
<p><strong>2. 亲和力引导的视觉令牌池化</strong>：大多数VLA模型将N个补丁嵌入展平为一维视觉令牌序列{v_i}，并追加到LLM输入流的文本令牌后。IVRA将补丁级的亲和力信息注回这些视觉令牌。具体操作在LLM第l层的自注意力子层之前进行。对于每个视觉令牌v_i，根据亲和力权重对其邻居令牌进行加权平均，计算得到精炼令牌v_i&#39;：v_i&#39; = Σ_{j=1}^N α_{ij} v_j，其中α_{ij} = max(A_{ij}, 0) / Σ_{k=1}^N max(A_{ik}, 0)。直观上，强相关的补丁会相互增强特征，从而保持局部空间连贯性。此操作仅修改视觉令牌，文本令牌保持不变。</p>
<p><strong>3. 集成到VLA模型</strong>：将上述池化步骤集成到LLM的少数选定层（例如第19至23层）。具体步骤为：</p>
<ol>
<li><strong>视觉编码</strong>：输入图像分块，经冻结视觉骨干网络处理得到补丁嵌入{f_i}。</li>
<li><strong>令牌构建</strong>：补丁嵌入被展平为{v_i}，并插入到LLM令牌序列中替换<code>&lt;image&gt;</code>占位符。</li>
<li><strong>亲和力引导池化</strong>：在每个选定层l的自注意力子层之前，根据公式更新每个v_i为v_i‘。</li>
<li><strong>令牌混合</strong>：为了在注入池化带来的物体感知证据的同时保留原始令牌的语义，将池化令牌与其未池化对应物进行线性混合，形成最终的视觉令牌：v_i^{mix} = (1-λ) v_i + λ v_i&#39;，其中λ∈[0,1]为混合系数。</li>
<li><strong>继续LLM前向传播</strong>：更新后的视觉令牌v_i^{mix}照常通过层归一化、自注意力及后续变换。文本令牌不被修改。</li>
<li><strong>输出解码</strong>：LLM产生用于生成策略动作或文本响应的下一阶段表征。</li>
</ol>
<p><strong>创新点</strong>：与需要额外训练、外部模块或架构修改的现有方法相比，IVRA的创新性主要体现在三点：1) <strong>无需训练</strong>：完全在推理时进行，不更新任何模型参数。2) <strong>利用内部信号</strong>：亲和力提示直接来自模型内置的视觉编码器，无需外部计算。3) <strong>选择性注入</strong>：仅将亲和力信息注入到LLM中存放实例级特征的特定中间层，而非所有层或输入层，实现了精准干预。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：IVRA在多种VLA架构（LLaRA, OpenVLA, FLOWER）上进行了评估，覆盖2D和3D模拟操控基准（VIMA和LIBERO）以及真实机器人任务。模拟实验使用标准评估协议和多个随机种子报告平均成功率。真实实验使用在合成数据上预训练的模型进行零样本泛化测试。</p>
<p><strong>对比基线</strong>：在VIMA上主要对比LLaRA和原始VIMA；在LIBERO上对比OpenVLA、FLOWER以及Diffusion Policy、Octo等其他策略学习方法；在真实任务中对比LLaRA及其变体RT-2-Style。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>VIMA模拟环境（2D操控）</strong>：如表I所示，在仅使用12%数据且无检测器的情况下，IVRA将LLaRA在四个任务（新任务、新物体、物体组合、物体放置）上的平均成功率从53.9%提升至58.1%（+4.2%），其中最具挑战性的“新任务”提升达+5.0%。在使用检测器的情况下，IVRA仍带来全面增益，并且使用12%数据的LLaRA+IVRA甚至超过了使用100%数据训练的原始VIMA。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.16207v1/x2.png" alt="VIMA结果表"></p>
<blockquote>
<p><strong>表I</strong>：VIMA基准结果。LLaRA+IVRA在四个泛化任务上的平均成功率始终优于LLaRA基线，并且在两种数据设置下均超越原始VIMA，显示了强大的实例级泛化能力。</p>
</blockquote>
<ol start="2">
<li><strong>LIBERO模拟环境（3D具身操控）</strong>：如表II所示，IVRA为OpenVLA和FLOWER带来了普遍提升。OpenVLA+IVRA将总体平均成功率从76.5%提升至77.6%（+1.1%），并超越了Diffusion Policy和Octo。即使FLOWER的基线准确率已接近饱和（96.3%），IVRA仍能将其提升至97.1%（+0.8%），表明IVRA提供了互补的结构信息。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.16207v1/x3.png" alt="LIBERO结果表"></p>
<blockquote>
<p><strong>表II</strong>：LIBERO基准结果。IVRA为OpenVLA和FLOWER均带来了一致的改进，即使在基线准确率接近饱和时（如FLOWER）也能获得提升。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界环境（零样本泛化）</strong>：如表III所示，在四个真实任务（T1:目标物体，T2:颜色匹配，T3:杂乱定位，T4:相对高度）上，LLaRA+IVRA显著优于LLaRA基线。在较简单的T1任务上提升+10%，在更具挑战性的T2、T3、T4任务上分别提升+30%、+30%、+20%，这与模拟实验中IVRA在更难任务上增益更大的趋势一致。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.16207v1/x4.png" alt="真实世界结果表"></p>
<blockquote>
<p><strong>表III</strong>：真实世界结果。LLaRA+IVRA在四个零样本泛化任务上全面大幅超越基线LLaRA，特别是在需要语义理解（颜色匹配）和精确定位（杂乱定位）的复杂任务上。</p>
</blockquote>
<p><strong>定性结果</strong>：如图1(b)及后续可视化图所示，应用IVRA后，视觉令牌的亲和力图变得更加清晰和连贯，能够明确地突出单个物体，而非将激活分散到背景中。这种细粒度的划分与性能的定量提升相符。例如，在真实任务T2（颜色匹配）中，更清晰的边界识别有助于定位正确颜色的物体。</p>
<p><img src="https://arxiv.org/html/2601.16207v1/figures/TObj/1_ivra.png" alt="真实任务T1对比"></p>
<blockquote>
<p>**图4 (上)**：T1任务中LLaRA+IVRA成功拾取指定物体（橙子）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.16207v1/figures/TObj/1_bl.png" alt="真实任务T1对比基线"></p>
<blockquote>
<p>**图9 (下)**：T1任务中LLaRA基线也成功拾取指定物体（橙子）。此任务相对简单，两者均能完成。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.16207v1/figures/CM/1_ivra.png" alt="真实任务T2对比"></p>
<blockquote>
<p>**图13 (上)**：T2任务中LLaRA+IVRA正确识别并拾取与鸭子颜色匹配的绿色西兰花。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.16207v1/figures/CM/1_bl.png" alt="真实任务T2对比基线"></p>
<blockquote>
<p>**图17 (下)**：T2任务中LLaRA基线未能拾取正确的西兰花。</p>
</blockquote>
<p><strong>消融实验</strong>：如表V所示，作者对IVRA的关键设计进行了消融研究。</p>
<ul>
<li><strong>注入层位置</strong>：将亲和力池化应用于LLaRA的中间层（如第20层）效果最佳，过早（如第11层）或过晚（如第31层）注入效果下降。</li>
<li><strong>连续层数量</strong>：在单层（第20层）应用池化效果已很好，应用更多层收益有限。</li>
<li><strong>池化在层内的位置</strong>：在自注意力块之前（位置P0）应用池化效果最佳。</li>
<li><strong>令牌混合系数λ</strong>：λ=0.3时取得了良好的平衡效果。λ=0（即不使用IVRA）性能最低，λ=1（完全使用池化令牌）可能损害原始语义。</li>
<li><strong>性能与效率</strong>：IVRA仅引入约3%的额外延迟，且不增加任何模型参数（见表IV）。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>明确揭示了当前VLA架构中将视觉令牌展平为一维序列会侵蚀二维空间线索的问题。</li>
<li>提出并验证了模型内置的视觉编码器本身即可作为恢复局部结构的亲和力提示源。</li>
<li>设计了一种无需训练、轻量级的推理时干预方法（IVRA），通过将亲和力提示选择性地注入LLM特定层，能够持续改善多种VLA模型在多个基准（模拟与真实）上的性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确阐述方法的局限性。但可以推断，其效果可能依赖于预训练视觉编码器产生的亲和力图的质量，对于视觉特征非常相似或边界极度模糊的复杂场景，提升可能有限。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>无需训练增强的可行性</strong>：IVRA证明了通过巧妙地利用模型内部已有表征进行推理时干预，可以有效提升模型在特定能力（如空间理解）上的表现，这为优化大型预训练模型提供了新思路。</li>
<li><strong>中间层特征的重要性</strong>：研究强调了LLM中间层对于维护实例级视觉信息的关键作用，提示未来工作可以更深入地探索和利用跨模态模型不同层的特征特性。</li>
<li><strong>广泛的可迁移性</strong>：该方法在多种VLA架构、2D/3D任务以及不同难度基准上均有效，表明其核心思想具有一定的普适性，可鼓励社区探索类似轻量级、即插即用的增强策略。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型因将图像块展平为一维序列而导致空间信息丢失、影响机器人精确操作的问题，提出了一种轻量级、无需训练的改进方法IVRA。该方法利用模型视觉编码器内置的亲和力提示，在推理阶段将这些空间关联信号选择性地注入语言模型层，以重新校准视觉-语言交互并保持几何结构。实验表明，IVRA可泛化应用于多种VLA架构，在2D VIMA基准上比基线LLaRA平均成功率提升4.2%，在3D LIBERO基准上即使基线准确率接近饱和（96.3%）仍能提升至97.1%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.16207" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>