<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>H$^3$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>H$^3$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.07819" target="_blank" rel="noreferrer">2505.07819</a></span>
        <span>作者: Lu, Yiyang, Tian, Yufeng, Yuan, Zhecheng, Wang, Xianbang, Hua, Pu, Xue, Zhengrong, Xu, Huazhe</span>
        <span>日期: 2025/05/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人视觉运动策略学习的主流方法是生成模型，特别是扩散模型和自回归模型，用于建模动作分布。然而，这些主流方法主要专注于分别优化感知的表示或动作的生成，往往忽视了在感知与动作之间建立紧密的对应关系。人类的决策过程则天然地包含从感知到动作的层次化信息处理。</p>
<p>本文针对视觉感知与动作生成之间耦合不足这一具体痛点，提出了一个贯穿整个策略学习流程的层次化新视角。论文的核心思路是模仿人类视觉皮层的层次处理机制，设计一个包含输入、表示和动作生成三个层次的视觉运动策略框架（H3DP），通过将多尺度视觉特征与扩散模型固有的从低频到高频的去噪过程对齐，来强化感知与动作的语义关联。</p>
<h2 id="方法详解">方法详解</h2>
<p>H3DP的整体框架是一个包含三重层次结构的视觉运动策略学习流程。输入是RGB-D观测图像和机器人位姿，输出是预测的动作序列。其核心在于三个依次递进的层次化设计：1）在输入层面，基于深度信息对RGB-D图像进行分层；2）在表示层面，为每个分层提取多尺度的视觉特征；3）在动作生成层面，利用一个层次化条件的扩散过程，将不同尺度的视觉特征对应地用于指导从粗到细的动作生成。</p>
<p><img src="https://arxiv.org/html/2505.07819v2/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：H3DP方法总览。框架集成了感知与动作生成流程中的三重层次设计原则。在输入层，RGB-D图像根据深度值被分解为多层。随后，采用多尺度视觉表示来捕捉不同粒度的特征。在动作生成阶段，去噪过程被划分为由多尺度视觉表示引导的若干阶段。</p>
</blockquote>
<p><strong>核心模块一：深度感知分层</strong>。为了充分利用深度图蕴含的几何结构，H3DP采用了一种线性递增的离散化策略，将像素根据其深度值d分配到不同的层m。具体公式为：$m = \lfloor -0.5 + 0.5\sqrt{1+4(N+1)(N+2)\frac{d-d_{\min}}{d_{\max}-d_{\min}+\epsilon}} \rfloor$。这种策略促使机器人更关注其工作空间，通过显式编码分布在不同深度平面上的物体，在保留所有视觉细节的同时实现了有意义的前景-背景分离，从而增强了策略在杂乱场景中对空间结构的感知和推理能力。</p>
<p><strong>核心模块二：多尺度视觉表示</strong>。为避免将图像特征扁平化为单一向量而丢失空间结构和语义信息，H3DP对每个输入层$I_m$，使用视觉编码器$\mathcal{E}<em>m$将其编码为多尺度特征图${f</em>{m,k}}<em>{k=1}^K$。这些特征图通过矢量量化（VQ）从一个可学习的码本$\mathcal{Z}<em>m$中映射到离散向量：$f^{(i,j)}</em>{m,k} \leftarrow \arg\min</em>{z \in \mathcal{Z}<em>m} |z - f^{(i,j)}</em>{m,k}|<em>2$。量化后的特征经过可微插值和轻量卷积，得到最终的多尺度视觉表示${\hat{f}</em>{m,k}}<em>{k=1}^K$。训练时，通过一致性损失$\mathcal{L}</em>{\text{consistency}}$来确保不同尺度的表示与原始特征$f_m$保持一致。</p>
<p><strong>核心模块三：层次化动作生成</strong>。为了匹配扩散去噪过程固有的从低频（全局结构）到高频（精细细节）重建的归纳偏置，H3DP将总步数为$T$的去噪过程划分为$K$个阶段$\cup_{k=1}^{K}(\tau_{k-1}, \tau_{k}]$。当去噪步$t$处于第$k$个阶段时，去噪网络$\epsilon_\theta^{(t)}$以对应尺度的特征图$\hat{f}<em>k$和机器人位姿$q$为条件，预测噪声$\epsilon^t$，并按照公式$a^{t-1} = \sqrt{\alpha</em>{t-1}}\left(\frac{a^{t}-\sqrt{1-\alpha_t}\cdot\epsilon^t}{\sqrt{\alpha_t}}\right) + \sqrt{1-\alpha_{t-1}-\sigma_t^2}\cdot\epsilon^t + \sigma_t\tilde{\epsilon}^t$更新动作。这样，粗粒度的视觉特征（对应低频）引导早期去噪步骤塑造动作的全局结构，而细粒度特征（对应高频）则指导后期步骤 refine 精确的细节。</p>
<p><strong>创新点</strong>：与现有方法（如DP3、CARP）通常只对动作生成进行层次化建模不同，H3DP的创新在于将层次化结构贯穿于整个视觉运动策略流水线，并显式地将多尺度视觉特征的层次与扩散模型去噪过程的层次在语义上对齐，从而实现了感知与动作更紧密的耦合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：H3DP在5个模拟基准（MetaWorld、ManiSkill、Adroit、DexArt、RoboTwin）上总计44个任务进行了评估，任务类型涵盖关节物体操作、可变形物体操作、双手操作和灵巧操作。对比的基线方法包括：Diffusion Policy (DP)、增加了RGB-D输入的Diffusion Policy (w/ depth)、以及使用点云高效编码器的DP3。在现实世界中，部署了双手机器人系统，在杂乱环境中评估了4个具有高干扰和长视野目标的挑战性任务。</p>
<p><strong>关键定量结果</strong>：在44个模拟任务上，H3DP取得了平均**65.3%<strong>的成功率，相对于基线方法平均提升了</strong>+27.5%<strong>。在4个真实世界双手操作任务上，H3DP相比Diffusion Policy实现了</strong>+32.3%**的性能提升。</p>
<p><img src="https://arxiv.org/html/2505.07819v2/x1.png" alt="模拟实验结果汇总"></p>
<blockquote>
<p><strong>图1</strong>：H3DP在44个模拟任务和4个真实世界挑战性操作任务上的性能展示。柱状图清晰显示了H3DP在多个基准上相对于基线方法的显著性能优势。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.07819v2/x3.png" alt="消融实验分析"></p>
<blockquote>
<p><strong>图3</strong>：消融实验验证各组件贡献。左图显示，完整的三重层次结构（H3DP-full）性能最佳，移除任何一层（输入、表示或动作层次）都会导致性能下降，其中动作生成层次的贡献最大。右图表明，H3DP使用的深度感知分层策略优于均匀分层和随机分层。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.07819v2/x6.png" alt="层次对应分析"></p>
<blockquote>
<p><strong>图6</strong>：层次对应分析。该图通过可视化不同去噪阶段（对应不同动作生成层次）的注意力图，证实了早期阶段（t=800）关注全局场景结构，而后期阶段（t=200）关注与动作执行相关的局部细节，验证了多尺度视觉特征与去噪过程的对齐机制。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.07819v2/x5.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图5</strong>：真实世界双手操作任务示例。H3DP成功完成了“堆叠杯子”、“打开糖罐并舀糖”、“打开药瓶并倒药”以及“打开午餐盒并取出食物”这四个长视野、高精度的复杂操作任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.07819v2/x4.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图4</strong>：模拟环境中的定性结果对比。在“移动棋子”任务中，基线方法（DP3）因无法处理遮挡而失败，而H3DP凭借其深度感知分层能力，成功规划了避障路径并完成任务。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验系统评估了三个层次化组件的贡献。结果表明，完整的H3DP（三重层次）性能最好。移除动作生成层次导致性能下降最显著（-12.8%），其次是移除多尺度视觉表示（-10.1%）和深度感知输入分层（-5.9%）。此外，论文还验证了所提出的深度感知分层策略优于均匀分层和随机分层等其他离散化方法。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1. 提出了一个<strong>三重层次化</strong>的视觉运动策略学习框架（H3DP），首次将层次化结构系统性地贯穿于输入、表示和动作生成整个流程。2. 引入了<strong>深度感知分层策略</strong>，有效利用深度信息进行前景-背景分离，增强了空间感知。3. 设计了<strong>多尺度视觉表示与层次化条件扩散过程的耦合机制</strong>，使粗到细的动作生成在语义上与多尺度视觉特征对齐，强化了感知与动作的关联。</p>
<p><strong>局限性</strong>：论文自身提到，所提出的框架在训练和推理时可能带来额外的计算开销。此外，深度离散化策略仍有改进空间，未来可以探索更自适应的分层方法。</p>
<p><strong>对后续研究的启示</strong>：H3DP的成功表明，将认知科学中的层次化处理原理引入机器人学习是富有成效的方向。未来工作可以探索更灵活的层次结构自适应学习，或将此框架扩展到更复杂的多模态输入（如触觉、语音）和更长视野的任务规划中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出H3DP，旨在解决视觉运动策略学习中视觉感知与动作生成耦合不足的问题。方法包含三重层次结构：1）基于深度信息的RGB-D输入分层；2）编码多粒度语义特征的多尺度视觉表示；3）与视觉特征对齐的从粗到细动作生成的分层条件扩散过程。实验表明，H3DP在44个模拟任务上平均相对性能提升27.5%，并在4个真实世界双手操作任务中取得优异表现。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.07819" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>