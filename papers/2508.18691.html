<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Deep Sensorimotor Control by Imitating Predictive Models of Human Motion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Deep Sensorimotor Control by Imitating Predictive Models of Human Motion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.18691" target="_blank" rel="noreferrer">2508.18691</a></span>
        <span>作者: Antonio Loquercio Team</span>
        <span>日期: 2025-08-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>利用人类与周围环境交互的数据集来训练机器人策略是一种有前景的规模化学习方法。然而，如何有效利用这些数据集训练感觉运动策略仍是一个挑战。当前主流方法主要包括两类：一是<strong>基于运动学重定向</strong>的方法，通过梯度优化将人类运动映射到机器人形态，但该方法通常需要环境模拟副本以评估可行性（如碰撞检测），且处理大规模数据集时计算量大；二是<strong>基于对抗性损失</strong>的方法，通过匹配人类与机器人的状态分布来指导强化学习，但对抗目标训练不稳定，难以扩展到大规模数据集。这些方法的核心局限在于，它们要么严重依赖精确的场景重建与仿真，要么难以稳定地处理海量、多样的人类数据。</p>
<p>本文针对上述局限性，提出了一个新颖的视角：<strong>模仿人类运动的预测模型</strong>。其核心思路是，利用类人机器人末端执行器关键点运动与人体对应关键点运动高度相似这一观察，先在人类数据上训练一个预测未来关键点位置的模型，然后让机器人策略在强化学习过程中去跟踪该模型的预测输出，从而将人类数据知识有效地注入到策略学习中，并完全绕开了基于梯度的运动学重定向和不稳定的对抗性损失。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程分为两个阶段：1）在人类-场景交互数据集上离线训练一个人类运动预测模型 Π_h；2）在机器人仿真环境中，使用强化学习训练策略 π_θ，其奖励函数由稀疏的任务奖励 r_task 和跟踪 Π_h 预测结果的奖励 r_track 共同构成。</p>
<p><img src="https://arxiv.org/html/2508.18691v1/figures/fig2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧使用人类-场景交互数据集训练运动预测模型Π_h；右侧在机器人下游任务训练策略π_θ时，将机器人状态历史输入Π_h得到预测的人类关键点位置，并构造跟踪奖励r_track来辅助稀疏任务奖励r_task。</p>
</blockquote>
<p><strong>核心模块1：人类运动预测模型 (Π_h)</strong></p>
<ul>
<li><strong>输入</strong>：一段历史时间内（长度L=16）的场景观测 o_{t:t-L}（物体点云）和人类手部关键点的3D位置 k^{h}_{t:t-L}，以及一个标识感兴趣物体的目标标签 g。</li>
<li><strong>输出</strong>：下一时刻人类关键点的预测位置 \hat{k}^{h}_{t+1}。</li>
<li><strong>技术细节</strong>：Π_h 被实例化为一个具有因果注意力机制的普通Transformer，包含6层和8个头。关键点通过线性变换标记化，点云通过一个PointNet编码器标记化，两者均映射到512维的隐藏大小。模型使用教师强制和均方误差损失进行训练。为防止在闭环机器人轨迹上进行自回归推理时分布漂移，训练时会在输入关键点 k^h_t 上注入零均值高斯噪声（目标值不加噪声）。</li>
</ul>
<p><strong>核心模块2：策略优化与跟踪奖励</strong></p>
<ul>
<li><strong>策略输入</strong>：当前场景的点云观测、机器人的本体感知状态。</li>
<li><strong>策略架构</strong>：一个具有4层、隐藏层维度为128的多层感知机（MLP），使用PPO算法进行训练。</li>
<li><strong>跟踪奖励</strong>：这是方法的核心创新点。奖励函数定义为 r_track = -‖\hat{k}^{h}<em>{t+1} - k^{r}</em>{t+1}‖^2，其中 \hat{k}^{h}<em>{t+1} 是Π_h基于机器人状态历史（经前向运动学转换为机器人关键点历史）预测的“人类”关键点位置，k^{r}</em>{t+1} 是机器人实际的关键点位置。策略的总奖励为 r_track + λ * r_task。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.18691v1/figures/mapping_final.png" alt="关键点映射"></p>
<blockquote>
<p><strong>图2</strong>：人手与机器人手之间的关键点映射。展示了人手关键点到三种不同形态机器人手（Allegro手、Xhand、SVH手）的直观映射，该映射在任务间保持一致。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法相比，本文的创新具体体现在：1) <strong>完全解耦</strong>：将人类数据的使用（训练Π_h）与机器人策略学习解耦，策略训练时无需访问庞大的人类数据集。2) <strong>零样本迁移</strong>：利用关键点抽象，使在人类数据上训练的Π_h能直接用于机器人数据，无需针对机器人进行重定向或微调。3) <strong>自动技能选择</strong>：通过条件预测，Π_h能根据当前场景和目标自动从数据分布中选取合适的行为模式，无需手动设计技能选择机制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真平台</strong>：IsaacGym（PhysX后端）。</li>
<li><strong>任务</strong>：抓握与提起、抓握与提起（杂乱场景）、提起与投掷、打开抽屉。</li>
<li><strong>机器人平台</strong>：Allegro手（16自由度）、Xhand（12自由度）、SVH手（20自由度），均安装在7自由度机械臂上。</li>
<li><strong>人类数据集</strong>：使用DexYCB数据集（包含1000条人类-物体交互轨迹）训练Π_h。</li>
<li><strong>对比基线</strong>：<ol>
<li><strong>PPO-DenseReward</strong>：使用IsaacGymEnvs中精心设计的密集奖励函数。</li>
<li><strong>PPO-TaskReward</strong>：仅使用稀疏的任务完成奖励。</li>
<li><strong>Adversarial Distribution Matching</strong>：在稀疏奖励基础上增加对抗性奖励，鼓励策略输出与人类关键点轨迹分布匹配。</li>
</ol>
</li>
</ul>
<p><img src="https://arxiv.org/html/2508.18691v1/figures/fig5_cabinet.png" alt="任务性能对比"></p>
<blockquote>
<p><strong>图3</strong>：不同任务上的性能对比。在“抓握与提起”、“抓握与提起-杂乱”、“提起与投掷”及“打开抽屉”四个任务上，本文方法（Ours）仅使用稀疏任务奖励和跟踪奖励，其学习曲线与使用精心设计密集奖励的基准方法（PPO-Dense）性能相当，而仅使用稀疏任务奖励的PPO（PPO-TaskReward）则无法有效学习。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>替代密集奖励设计</strong>：如图3所示，在四个任务上，本文方法（Ours）仅使用稀疏奖励和跟踪奖励，最终达到的成功率与使用精心设计密集奖励的PPO-DenseReward基线相当。而仅使用稀疏奖励的PPO-TaskReward基线在所有任务上都未能有效学习。这表明跟踪人类运动预测可以替代复杂的、需要手动调整的密集奖励函数。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.18691v1/figures/sparse_reward_viz.png" alt="稀疏与密集奖励对比"></p>
<blockquote>
<p><strong>图4</strong>：“抓握与提起”任务中，专家策略 rollout 上稀疏奖励与密集奖励的逐步对比。稀疏奖励仅在物体被抓握并开始向目标移动后（约第130步）激活，而密集奖励在整个过程中提供连续的塑形信号。</p>
</blockquote>
<ol start="2">
<li><p><strong>跨机器人平台泛化</strong>：如图6（上）所示，在“抓握与提起”任务中，使用同一个Π_h，方法在三种不同形态的类人机器人手上均取得了与PPO-DenseReward相当的高成功率，证明了预测模型和方法的跨平台零样本迁移能力。</p>
</li>
<li><p><strong>优于其他人类数据引导方法</strong>：如图6（下）所示，当所有方法均只使用稀疏任务奖励时，本文方法显著优于对抗分布匹配基线，后者由于训练不稳定而性能低下。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.18691v1/figures/quali.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：定性结果。使用本文方法训练的策略成功抓取两个物体。彩色点显示了Π_h基于当前及历史机器人关键点位置对下一时刻的预测。这些预测在时间上是平滑的，并引导策略快速完成抓握和提起动作。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.18691v1/figures/fig5_new_new.png" alt="跨平台与基线对比"></p>
<blockquote>
<p><strong>图6</strong>：（上）在不同多指手机器人平台上评估本文方法。在三种机器人上，本文方法的性能与使用密集奖励的PPO基准相当。（下）当均使用稀疏任务奖励训练时，本文方法优于其他利用人类数据指导RL探索的现有方法（如对抗分布匹配）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>可扩展的框架</strong>，通过模仿人类运动预测模型来训练感觉运动策略，完全绕过了对运动学重定向和对抗性损失的依赖。</li>
<li>实证验证了在人类数据上训练的关键点运动预测模型可以<strong>零样本</strong>应用于不同的机器人形态和任务，为利用大规模人类活动数据集提供了新途径。</li>
<li>证明了此类跟踪奖励可以<strong>有效替代</strong>为复杂操作任务精心设计的密集奖励函数，简化了强化学习的奖励工程。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法依赖于机器人具有类人形态（以便定义关键点映射），并且假设人类与机器人交互的场景是匹配的（例如，都有相同的物体）。对于形态差异巨大或场景完全不同的情况，方法的直接应用可能受限。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>探索更通用、不依赖于具体形态的<strong>运动抽象表示</strong>，以将方法推广到非类人机器人。</li>
<li>研究如何将本文的预测模型框架与<strong>少量、高效的运动学重定向</strong>结合，以处理更复杂的动态场景或接触约束。</li>
<li>将人类运动预测模型作为一种通用的<strong>先验或技能库</strong>，用于加速各种下游机器人任务的学习。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出了一种通过模仿人类运动预测模型来训练机器人感觉运动策略的新方法。核心问题是解决如何有效利用大规模人类-场景交互数据集来训练机器人策略，避免传统方法中基于梯度的运动重定向和对抗性损失的局限性。关键技术是直接利用人类关键点运动数据训练预测模型，并零样本应用于机器人关键点；随后训练策略跟踪该模型的预测轨迹，同时优化稀疏的任务奖励。实验表明，该方法在多种机器人和任务上均大幅超越现有基线，并且能够替代传统方法中需要精心设计的密集奖励和课程。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.18691" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>