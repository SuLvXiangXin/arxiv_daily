<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PointVLA: Injecting the 3D World into Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>PointVLA: Injecting the 3D World into Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.07511" target="_blank" rel="noreferrer">2503.07511</a></span>
        <span>作者: Li, Chengmeng, Wen, Junjie, Peng, Yan, Peng, Yaxin, Feng, Feifei, Zhu, Yichen</span>
        <span>日期: 2025/03/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人基础模型，特别是视觉-语言-动作（VLA）模型，通过利用大规模二维视觉-语言预训练，在机器人任务上表现出色。然而，这些模型主要依赖RGB图像输入，这限制了其进行空间推理的能力，而空间推理对于真实世界交互至关重要。虽然存在三维机器人数据集，但使用三维数据从头重新训练这些VLA模型计算成本高昂，而丢弃现有的宝贵二维数据集又造成资源浪费。本文针对这一痛点，提出了一种无需重新训练即可将三维世界信息注入预训练VLA模型的新视角。</p>
<p>本文的核心思路是：冻结预训练的VLA模型（特别是其动作专家模块），通过一个轻量化的模块化网络将点云特征作为补充条件信号注入其中，从而在保留原有二维知识的同时增强模型的三维空间理解能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>PointVLA的整体框架旨在将点云输入集成到预训练的VLA模型中，同时保持原有二维视觉-语言主干网络和动作专家模块的完整性。输入包括二维图像观测、语言指令和三维点云。输出是机器人动作。</p>
<p><img src="https://arxiv.org/html/2503.07511v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：PointVLA框架概览。左侧：二维图像观测和指令由视觉-语言模型处理。原始动作专家保持冻结，而新的点云表示通过模块化网络集成到动作专家中。右侧：点云注入器的细节。</p>
</blockquote>
<p>核心模块是<strong>点云注入器</strong>。其工作流程如下：</p>
<ol>
<li><strong>点云编码</strong>：采用一个简化的分层卷积架构（类似于iDP3）处理输入点云。上层卷积提取低级特征，下层卷积块学习高级场景表示，层间使用最大池化逐步降低点云密度。最终，将所有卷积块的特征嵌入拼接成一个统一的多层次三维表示嵌入。</li>
<li><strong>特征对齐与压缩</strong>：将点云嵌入的通道维度转换以匹配原始动作专家的通道维度。由于点云产生的动作嵌入可能很大，设计了一个动作嵌入瓶颈来压缩信息，并与三维点云嵌入对齐。</li>
<li><strong>选择性注入</strong>：并非向动作专家的每一个块都注入点云特征。通过<strong>跳跃块分析</strong>（见下文），识别出在推理过程中对性能影响较小的“不那么关键”的块。对于这些选定的块，首先为每个块应用一个MLP层作为适配器，然后通过加法操作将点云嵌入注入模型。这种选择性注入最小化了对从大规模二维数据中学到的预训练动作表示的干扰，并控制了计算成本。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：</p>
<ul>
<li><strong>模块化、非破坏性的三维集成</strong>：不同于将3D数据直接转换为token与2D token混合的方法（如LLaVA-3D），PointVLA将点云视为补充条件信号，与核心2D视觉编码器解耦，避免了因小规模3D数据微调导致的2D知识灾难性遗忘和3D过拟合风险。</li>
<li><strong>基于跳跃块分析的智能注入策略</strong>：通过系统分析动作专家中哪些块可以被跳过而不影响性能，来确定注入点云特征的最佳位置，实现了效率与性能的平衡。</li>
</ul>
<p><img src="https://arxiv.org/html/2503.07511v1/x2.png" alt="跳跃块分析"></p>
<blockquote>
<p><strong>图2</strong>：VLA模型动作专家的跳跃块分析。左图：一次仅跳过一个块。右图：从第11个块开始跳过多个连续块。分析表明，前11个块至关重要，而从第11块之后，最多可跳过5个连续块而不导致任务失败，这些区域适合进行点云注入。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：RoboTwin仿真平台；真实世界双臂机器人平台（UR5e和AgileX）。</li>
<li><strong>对比基线方法</strong>：Diffusion Policy (DP)、3D Diffusion Policy (DP3)、ScaleDP-1B、Octo、OpenVLA、DexVLA。其中DexVLA可视为PointVLA去除点云注入的消融版本。</li>
<li><strong>实验平台</strong>：使用RealSense D435i（腕部相机）和L515（点云采集）相机。数据采集频率为15Hz或30Hz。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>少样本多任务学习</strong>：在AgileX双臂机器人上执行充电手机、擦拭盘子、放置面包、运输水果四个任务，每个任务仅用20条演示数据训练。</p>
<p><img src="https://arxiv.org/html/2503.07511v1/x5.png" alt="少样本多任务结果"></p>
<blockquote>
<p><strong>图5</strong>：在双臂AgileX上的少样本多任务实验结果。PointVLA在所有任务上的成功率均优于基线方法，显示了其样本高效学习的能力。</p>
</blockquote>
</li>
<li><p><strong>长视野任务</strong>：在UR5e双臂机器人上执行从移动传送带上抓取并打包洗衣液的复杂任务。</p>
<blockquote>
<p><strong>表1</strong>：双臂UR5e长视野任务实验结果。任务按顺序完成。Avg. Len.表示模型的平均成功长度。</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Packing Laundry Detergent (1/11 to 5/11 success counts)</th>
<th>Avg. Len.</th>
</tr>
</thead>
<tbody><tr>
<td>PointVLA</td>
<td>3/11, 1/11, 1/11, 2/11, 2/11</td>
<td><strong>2.36</strong></td>
</tr>
<tr>
<td>DexVLA</td>
<td>2/11, 5/11, 1/11, 1/11, 0</td>
<td>1.72</td>
</tr>
<tr>
<td>ScaleDP-1B</td>
<td>4/11, 2/11, 0, 0, 0</td>
<td>0.72</td>
</tr>
<tr>
<td>其他基线</td>
<td>更低或为0</td>
<td>≤0.36</td>
</tr>
<tr>
<td>PointVLA取得了最高的平均成功长度（2.36），优于强基线DexVLA（1.72）。</td>
<td></td>
<td></td>
</tr>
</tbody></table>
</blockquote>
</li>
<li><p><strong>真实vs照片判别</strong>：将真实物体替换为其在屏幕上的照片。仅基于2D的VLA模型（如OpenVLA、DexVLA）无法区分，会试图抓取不存在的物体并陷入循环。PointVLA则能利用3D空间理解，识别出该位置实际为空，从而避免“物体幻觉”。</p>
<p><img src="https://arxiv.org/html/2503.07511v1/x6.png" alt="真实vs照片判别"></p>
<blockquote>
<p><strong>图6</strong>：真实vs照片判别实验设置及结果。在仅使用2D输入的模型中，DexVLA尝试抓取照片并失败，而PointVLA成功识别出没有真实物体。</p>
</blockquote>
</li>
<li><p><strong>高度适应性</strong>：在“放置面包”任务中，训练时面包下方有3mm泡沫垫，测试时泡沫垫增至52mm。传统2D VLA模型（OpenVLA、DP、DexVLA等）均失败，它们试图在训练所见的高度抓取。PointVLA则能通过点云感知新高度，调整夹爪并成功抓取。</p>
<p><img src="https://arxiv.org/html/2503.07511v1/x7.png" alt="高度适应性"></p>
<blockquote>
<p><strong>图7</strong>：PointVLA的高度适应性。当测试时遇到与训练不同的桌面高度时，PointVLA能适应新高度并成功完成任务，而传统2D方法完全失败。</p>
</blockquote>
</li>
<li><p><strong>仿真基准测试</strong>：在RoboTwin平台的多个任务上，使用20和50条演示数据训练。PointVLA在几乎所有任务和设置中都取得了最高的平均成功率。例如在“Block Hammer Beat”任务（20条演示）中，PointVLA成功率为61.2±4.8%，显著高于DP3（点云+RGB）的44.7±3.8%和纯3D DP3的47.7±7.4%。</p>
</li>
</ol>
<p><strong>消融实验</strong>：<br>DexVLA（PointVLA无点云注入版本）作为核心消融对照。在所有实验中，加入点云注入的PointVLA在少样本学习、长视野任务成功率、以及真实vs照片判别和高度适应性等独特泛化能力上，均 consistently 优于DexVLA，证明了点云注入模块的有效性和必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>PointVLA</strong>，一个无需重新训练即可将点云输入注入预训练VLA模型的框架。它通过冻结原始模型并添加轻量级模块化注入块，在保留已有2D知识的同时引入了3D空间感知。</li>
<li>引入了 <strong>跳跃块分析</strong> 方法，系统性地识别动作专家中适合注入新模态的特征层，最小化对预训练表示的干扰，实现了高效且有效的多模态融合。</li>
<li>通过大量实验证明了3D注入带来的关键优势：<strong>少样本多任务学习能力</strong>、<strong>解决真实vs照片判别的物体幻觉问题</strong>、以及<strong>对未见过的高度变化的适应性</strong>，这些是纯2D VLA模型难以实现的。</li>
</ol>
<p><strong>局限性</strong>：<br>论文指出，当前使用的点云编码器是一个相对简化的卷积架构，并非核心创新。作者认为采用更先进的点云编码器可能进一步提升模型性能，这留待未来探讨。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>模块化扩展</strong>：PointVLA的模块化设计范式（冻结主干，轻量适配）为将其他新模态（如触觉、音频）集成到大型预训练基础模型中提供了可行思路。</li>
<li><strong>3D理解的必要性</strong>：研究凸显了对于需要精确空间推理和物理交互的机器人任务，3D几何信息是不可或缺的，未来的VLA模型应更深入地融合3D感知。</li>
<li><strong>高效融合策略</strong>：跳跃块分析启发了如何在大型预训练网络中寻找对新信息更“宽容”的插入点，这对其他领域的模型编辑或持续学习具有参考价值。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出PointVLA框架，旨在解决现有视觉-语言-动作模型因依赖2D图像而缺乏空间推理能力的问题。核心方法是在不重新训练的前提下，通过轻量级模块块向预训练模型中注入点云特征，并采用跳过块分析定位模型中效用较低的模块，以最小化对原有表示的干扰。实验表明，PointVLA在模拟和真实机器人任务中均优于先进的2D模仿学习方法，并展现出少量样本多任务处理、真实与图像区分、高度自适应等关键优势。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.07511" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>