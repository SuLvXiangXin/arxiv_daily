<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.21796" target="_blank" rel="noreferrer">2507.21796</a></span>
        <span>作者: Joni Pajarinen Team</span>
        <span>日期: 2025-07-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>移动操作是机器人在多样化现实环境中运行的关键能力。然而，操纵可变形物体和材料对于现有的机器人学习算法仍然是一个主要挑战。虽然已经提出了各种基准来评估刚性物体的操作策略，但仍然明显缺乏针对涉及可变形物体的移动操作任务的标准基准。现有的基准主要关注刚性物体的移动操作或静态可变形物体操作，将移动可变形操作这一交叉领域留作相对未探索的空白。此外，现实世界的实验耗时、昂贵且难以标准化，特别是对于强化学习和模仿学习等数据驱动方法。一个设计良好的基于模拟的基准可以通过提供受控、可重复和可扩展的测试环境来显著加速进展。</p>
<p>本文旨在填补这一空白，提出了首个专门为移动可变形操作设计的标准化任务套件 MoDeSuite。其核心思路是：创建一个集成了移动操作与可变形物体交互的仿真基准平台，包含八项多样化任务，支持轮式和足式两种移动操作器，并利用先进的物理引擎模拟弹性和塑性变形，以促进算法开发、评估与比较。</p>
<h2 id="方法详解">方法详解</h2>
<p>MoDeSuite 是一个为加速机器人操作算法开发而设计的移动可变形操作任务套件。其整体框架基于 NVIDIA Omniverse 平台构建，支持在配备移动操作器的仿真环境中进行强化学习和模仿学习。</p>
<p><img src="https://arxiv.org/html/2507.21796v1/Figs/overview.png" alt="MoDeSuite 概览"></p>
<blockquote>
<p><strong>图2</strong>：MoDeSuite 概览。MoDeSuite 基于 NVIDIA Omniverse 构建，支持在具有两种移动操作器类型的仿真环境中进行强化学习和模仿学习。它提供 RGB、深度和基于状态的感知输入，用于八项具有不同形状的可变形操作任务。模拟使用 FEM 和 PBD 物理模型，并支持 GPU 加速的并行训练。</p>
</blockquote>
<p>该套件由两个主要元素组成：(1) 一个仿真框架，(2) 一系列多样化的任务。仿真框架基于 Isaac Lab 和 Isaac Sim，利用 NVIDIA PhysX 引擎提供刚体和可变形体的高保真物理模拟。对于可变形物体，采用了两种不同的模拟方法：弹性体使用基于四面体网格的有限元方法（FEM）进行模拟，以高效准确地模拟线性弹性物体的各种变形特征；而塑性可变形物体（如窗帘和桌布）则使用基于位置的动力学（PBD）粒子系统进行模拟，以处理大变形且避免稳定性问题。</p>
<p>套件包含两种机器人设置以适应广泛的应用场景：轮式操作器（Franka Panda 机械臂安装在 Clearpath Ridgeback 轮式底座上）和足式操作器（Boston Dynamics Spot 机身及其机械臂）。任务要求同时控制机械臂和移动底座，因此机器人动作由底座动作和手臂动作共同构成：( a_{\text{robot}}=(a_{\text{base}},a_{\text{arm}}) )。针对不同机器人，提供了连续和离散两种动作空间设置。连续动作空间包括底座的速度/位置控制和手臂的关节空间/末端执行器位姿控制；离散动作空间则映射到键盘，包含身体移动、转向、手爪移动、抓取/释放等14个基本动作。</p>
<p>观测空间支持基于图像和基于状态两种类型。基于图像的观测从机器人搭载的 RGB-D 相机获取，并使用 DiNOv2 图像编码器进行处理。基于状态的观测则提供机器人、可变形物体和任务相关环境的详细信息，其通用形式为：( O = (s_{\text{r}}, s_{\text{o}}, s_{\text{e}}) )。其中，( s_{\text{r}} ) 表示机器人状态（位置、朝向、关节位置与速度），( s_{\text{o}} ) 表示物体状态（弹性体为 FEM 元素位置，塑性体为粒子位置），( s_{\text{e}} ) 包含任务相关信息（如目标位置、障碍物位置）。</p>
<p>与现有方法相比，MoDeSuite 的创新点在于首次系统性地将移动操作与可变形物体（包括弹性和塑性）操纵整合到一个统一的、可扩展的基准测试平台中，并提供了灵活的机器人配置、动作空间和观测空间选项。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了 MoDeSuite 套件中提出的全部八项任务进行评测：五项弹性变形任务（放置 Place、弯曲 Bend、运输 Transport、拖拽 Drag、提升 Lift）和三项塑性变形任务（揭开 Uncover、覆盖 Cover、窗帘 Curtain）。实验平台为 Isaac Sim 仿真环境，并进行了真实世界的零样本迁移验证（使用 Spot 机器人）。</p>
<p>对比的基线方法包括两种最先进的强化学习算法（SAC 和 PPO）和两种模仿学习算法（行为克隆 BC 和 Diffusion Policy）。实验旨在评估：1）智能体在仿真中从交互和演示中学习的能力；2）不同输入（状态 vs. 图像）对学习的影响；3）学习到的策略从仿真到真实世界的零样本可迁移性。</p>
<p><img src="https://arxiv.org/html/2507.21796v1/Plots/rl_plot.png" alt="强化学习算法性能"></p>
<blockquote>
<p><strong>图3</strong>：SAC 和 PPO 算法在 MoDeSuite 任务（Place, Bend, Transport, Lift, Drag）上的性能，使用基于状态的观测和两种机器人平台（Franka 和 Spot）进行评估。曲线表示超过5个种子的平均回报，阴影区域显示 Place 任务的标准差。条形图显示了超过20次试验的成功率。结果突出了机器人形态和算法选择对任务有效性的影响。</p>
</blockquote>
<p>关键实验结果如下：在五项弹性任务上，使用状态观测进行训练时，SAC 和 PPO 都能学习到有效的策略，但性能因任务和机器人形态而异。例如，在 Place 任务中，两种算法在 Franka（轮式）机器人上都取得了超过80%的成功率，而在 Spot（足式）机器人上成功率较低（约20-60%），这突显了足式平台固有的不稳定性带来的挑战。在 Drag 和 Lift 等更复杂的任务中，成功率普遍较低。模仿学习方面，在 Curtain 任务上，BC 在仿真中达到了100%的成功率，而 Diffusion Policy 为80%。</p>
<p><img src="https://arxiv.org/html/2507.21796v1/Figs/curtain_traj_real.png" alt="窗帘任务仿真到真实对比"></p>
<blockquote>
<p><strong>图4</strong>：Curtain 任务的仿真到真实视觉对比，展示了不同控制策略（行为克隆 BC、图像检索引导 Retrieval、手动遥操作和仿真）的图像序列。右上角面板展示了 DiNOv2 图像嵌入的 t-SNE 图，突出了仿真（蓝色）和真实数据（红色）之间的视觉域差距。中上部显示了仿真和真实世界的物理设置。</p>
</blockquote>
<p>仿真到真实迁移实验表明，在仿真中训练的策略可以零样本迁移到真实世界的 Spot 机器人上。例如，在 Curtain 任务中，BC 策略在真实世界取得了约70%的成功率。研究还发现，基于状态的策略迁移性能优于基于图像的策略，因为后者受视觉域差距的影响更大（如图4 t-SNE 图所示）。通过引入图像检索引导机制，可以改善基于图像策略的迁移性能。</p>
<p><img src="https://arxiv.org/html/2507.21796v1/Figs/drag_real.png" alt="真实世界拖拽任务"></p>
<blockquote>
<p><strong>图5</strong>：真实世界中 Spot 机器人执行 Drag 任务的序列图像。机器人需要将弹性带拉伸并越过障碍物放置到另一侧，展示了仿真训练策略在真实复杂操作中的有效性。</p>
</blockquote>
<p>消融实验方面，论文对比了状态观测和图像观测的学习效果。结果显示，在相同算法（SAC）下，使用状态观测在所有测试的弹性任务上都取得了显著更高的成功率和学习效率，因为状态信息提供了更精确、更低维度的任务相关信息。这揭示了在可变形物体操作中，获取有效的物体状态表征是一个关键挑战，而纯视觉方法目前学习难度更大。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）<strong>填补研究空白</strong>：首次提出了一个专门用于移动可变形物体操作的标准化基准测试套件 MoDeSuite，系统整合了移动操作与弹性和塑性变形物体的交互。2）<strong>提供灵活框架</strong>：套件设计具有高度灵活性和可扩展性，支持两种移动操作平台、多种动作与观测空间、以及八项源自现实场景的多样化任务，为算法开发和评估提供了统一平台。3）<strong>验证仿真到真实迁移潜力</strong>：通过将仿真中训练的策略成功零样本部署到真实 Spot 机器人上，证明了该基准在推动实用化机器人学习方面的价值。</p>
<p>论文自身提到的局限性包括：当前实现主要依赖于特定的机器人平台（Franka/Spot）和模拟器（Isaac Sim），未来需要扩展到更多平台和模拟引擎以增强通用性；套件中的任务虽然多样，但尚未覆盖所有类型的可变形物体操作场景。</p>
<p>这项工作为后续研究提供了重要启示：首先，它确立了一个急需的研究方向——移动可变形操作，并为该方向的算法比较和进展评估奠定了基础。其次，实验结果表明，对于涉及复杂形变的任务，如何从感知数据（特别是视觉）中有效提取和理解物体状态是一个关键难点，这激励着未来在可变形物体的表征学习、sim-to-real 的视觉域适应等方面进行更深入的探索。最后，MoDeSuite 的模块化设计鼓励社区贡献新的任务、机器人和模型，有望推动该领域形成更丰富、更强大的基准生态系统。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人移动操作领域缺乏可变形物体标准化基准测试的问题，提出了首个移动操作可变形物体任务套件MoDeSuite。该套件包含8个涉及弹性和塑性变形的任务，要求机器人基座与机械臂协同工作并利用物体形变特性。作者使用两种强化学习和两种模仿学习算法在仿真中进行了基准测试，并成功将训练策略迁移至真实的Spot机器人，验证了仿真到现实的迁移潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.21796" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>