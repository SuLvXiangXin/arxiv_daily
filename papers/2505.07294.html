<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HuB: Learning Extreme Humanoid Balance - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>HuB: Learning Extreme Humanoid Balance</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.07294" target="_blank" rel="noreferrer">2505.07294</a></span>
        <span>作者: Zhang, Tong, Zheng, Boyuan, Nai, Ruiqian, Hu, Yingdong, Wang, Yen-Jen, Chen, Geng, Lin, Fanqi, Li, Jiongye, Hong, Chuye, Sreenath, Koushil, Gao, Yang</span>
        <span>日期: 2025/05/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，基于学习的人形机器人控制主流方法是跟踪参考动作范式：首先从基于视频或标记的动作捕捉系统中获取人体姿态，将其重定向为人形机器人参考动作，然后在仿真中训练控制策略来跟踪这些参考动作，最后部署到真实硬件。然而，将此范式应用于复杂平衡任务时面临三大关键挑战：1）参考动作误差导致的不稳定性：视频动作捕捉算法误差大，标记系统不适用于网络视频，且基于优化的重定向过程会因非凸优化、模型未对齐和缺乏时间连续性约束而引入伪影（如脚部滑动）；2）形态差异导致的策略学习困难：人体与人形机器人的重心（COM）不匹配，严格跟踪参考动作不一定能使人形机器人保持稳定平衡；3）仿真到现实的差距：真实世界传感器（特别是IMU和VIO）噪声以及未建模的动力学（如地面接触摩擦）会导致策略输入不准确、输出抖动，进而引发不稳定。本文针对人形机器人执行极端准静态平衡任务（如单腿站立、高踢腿）的痛点，提出了一个统一框架HuB，通过集成参考动作精炼、平衡感知策略学习和仿真到现实鲁棒性训练三个组件来分别应对上述挑战。其核心思路是：通过改进的重定向初始化与后处理提升参考动作质量；通过松弛跟踪和平衡塑形奖励引导策略学习适应机器人自身动力学的稳定行为；通过局部参考跟踪、IMU中心观测扰动和高频推动扰动来缩小仿真到现实差距。</p>
<h2 id="方法详解">方法详解</h2>
<p>HuB的整体框架是一个集成了三个核心组件的端到端流程，旨在从有噪声的人类动作视频中学习并实现稳定的极端平衡控制。</p>
<p><img src="https://arxiv.org/html/2505.07294v2/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：HuB框架总览。针对人形机器人极端平衡任务的挑战，HuB集成了三个组件：(a) 运动精炼过程，提升参考动作的质量和可行性；(b) 平衡感知策略学习策略，实现挑战性平衡动作的稳定执行；(c) 鲁棒性训练机制，改善仿真到现实的一致性和部署稳定性。</p>
</blockquote>
<p><strong>整体流程</strong>：首先，从视频片段中提取SMPL格式的人体姿态，并重定向为人形机器人参考动作。随后，采用师生学习范式：在仿真中使用PPO算法训练一个能访问特权信息的教师策略，然后通过DAgger算法蒸馏出一个仅使用板载观测的学生策略。最终的学生策略被部署到真实机器人上。</p>
<p><strong>核心模块一：参考动作精炼</strong>。该模块旨在提升参考动作的物理合理性和可行性，包含四项技术：</p>
<ol>
<li><strong>SMPL初始化的重定向</strong>：不同于将关节角初始化为零位，该方法利用人形机器人关节自由度是SMPL子集的特点，直接使用SMPL姿态的对应欧拉角进行初始化，从而获得更接近最优解的起点，加速收敛并提高精度。</li>
<li><strong>接地脚校正</strong>：针对单腿阶段，假设支撑脚应保持静止。通过调整全局根位置而保持所有局部关节角不变，确保跨连续帧的接地脚静止，消除脚部滑动伪影。</li>
<li><strong>重心过滤</strong>：计算参考动作中基于URDF定义的质量和位置得出的重心，丢弃地面投影重心偏离支撑脚中心超过0.2米的轨迹，确保参考动作的物理可行性。</li>
<li><strong>过渡稳定化</strong>：在平衡阶段前后复制其首尾帧以延长双足支撑阶段。这增加了策略学习站立平衡的时间，并在部署时为机器人提供了更多稳定时间以过渡到极端平衡姿态。</li>
</ol>
<p><strong>核心模块二：平衡感知策略学习</strong>。该模块通过修改学习目标，引导策略发现适合机器人自身动力学的平衡策略。</p>
<ol>
<li><strong>松弛参考跟踪</strong>：在奖励函数中设置较大的跟踪容忍度（σ = 0.6米），允许策略在严格跟踪会损害平衡时偏离参考动作，从而微调重心位置，探索更稳定的行为。</li>
<li><strong>平衡塑形奖励</strong>：设计三种奖励来提供结构化引导：(i) <strong>重心奖励</strong>：鼓励重心垂直投影保持在支撑多边形内；(ii) <strong>足部接触失配惩罚</strong>：惩罚人形机器人与参考动作之间足部接触状态（触地/离地）的不一致，防止非支撑脚在单腿平衡时意外触地；(iii) <strong>双脚过近惩罚</strong>：防止双脚过于靠近，降低碰撞风险并鼓励更稳定的下身姿态。</li>
</ol>
<p><strong>核心模块三：仿真到现实鲁棒性训练</strong>。该模块通过在训练中引入特定扰动，提升策略对真实世界噪声和未建模动力学的鲁棒性。</p>
<ol>
<li><strong>局部参考跟踪训练</strong>：在训练和部署时均丢弃里程计信息，将参考根与人形机器人当前根姿态对齐，所有跟踪目标在局部坐标系中表达。这避免了VIO噪声的影响，并保证了训练与部署的一致性。</li>
<li><strong>IMU中心观测扰动</strong>：为了更真实地模拟具有时间相关性的IMU噪声，在训练学生策略时，使用奥恩斯坦-乌伦贝克（OU）噪声过程来扰动以欧拉角表示的观测到的根朝向。所有其他观测值都基于这个被扰动的朝向计算，从而模拟了由IMU误差引起的结构性依赖噪声。</li>
<li><strong>高频推动扰动</strong>：在教师策略训练期间，通过向根部注入小而高频的速度偏移（每1秒推动一次，速度最高0.5米/秒），模拟真实世界中因未建模动力学引起的细微振荡和放大效应，增强对扰动的恢复能力。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在Unitree G1人形机器人上进行评估。任务包括“燕子平衡”、“李小龙踢”、“哪吒 pose”等高难度准静态平衡姿势（见图1）。仿真实验在IsaacGym中进行，测试时引入了随机外部推动（根部速度扰动最高0.1米/秒）和OU噪声形式的IMU噪声。每个策略在扰动仿真条件下评估100回合。评估指标分为任务完成度（成功率、接触失配帧数）、稳定性（脚滑移速度、双足腾空帧数、动作变化率）和跟踪误差（全局或局部的位置、速度、加速度误差）。对比的基线方法是H2O和OmniH2O，并针对HuB的各个组件进行了消融实验。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>与基线方法对比</strong>：如表1所示，在两个最具挑战性的任务“燕子平衡”和“李小龙踢”上，HuB取得了100%的成功率，且接触失配为0。而基线方法H2O和OmniH2O的成功率分别为0%和近乎0%，主要失败原因是非支撑脚发生了不应有的地面接触。此外，HuB在脚滑移、腾空时间和动作变化率等稳定性指标上均显著优于基线，表明其运动更稳定、平滑。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07294v2/x4.png" alt="仿真结果"></p>
<blockquote>
<p><strong>图4</strong>：仿真实验结果表。展示了HuB与基线方法以及各消融变体在“燕子平衡”和“李小龙踢”任务上的定量对比。HuB在成功率等关键指标上全面领先。</p>
</blockquote>
<ol start="2">
<li><p><strong>消融实验分析</strong>：</p>
<ul>
<li><strong>松弛跟踪</strong>：实验了不同的跟踪容忍度σ。σ太小（0.15米）会降低稳定性（滑移增加）；σ太大（1.2米）则导致跟踪误差剧增和成功率下降。σ=0.6米在跟踪保真度和策略稳定性间取得了最佳平衡。</li>
<li><strong>平衡塑形奖励</strong>：移除重心奖励会导致滑移和腾空时间增加；移除接触失配惩罚会直接导致接触失配激增和成功率骤降；移除双脚过近惩罚会降低站立稳定性。三者均对最终性能有重要贡献。</li>
<li><strong>鲁棒性训练</strong>：移除局部跟踪（训练用全局，部署用局部）会导致性能显著下降；移除IMU噪声注入会使策略对部署误差敏感；移除高频推动或改用低频高幅推动（如每5秒1米/秒）都会增加接触失配、降低成功率和稳定性，说明高频小扰动对于模拟平衡任务中的细微不稳定动态至关重要。</li>
</ul>
</li>
<li><p><strong>重定向改进验证</strong>：</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07294v2/x3.png" alt="重定向对比"></p>
<blockquote>
<p><strong>图3</strong>：重定向对比。展示了使用SMPL初始化与不使用初始化，在经过500步优化后的重定向损失。SMPL初始化在所有任务上都获得了更低的损失，尤其是在“深蹲”任务上优势明显。</p>
</blockquote>
<ol start="4">
<li><strong>真实世界结果</strong>：在真实Unitree G1机器人上，HuB在“燕子平衡”（4/5成功）、“李小龙踢”（5/5成功）和“哪吒 pose”（5/5成功）任务上均取得了高成功率，而OmniH2O基线在所有任务上均失败（0/5成功）。同时，HuB的局部跟踪误差也远低于基线。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）<strong>系统性地识别并形式化了</strong>学习人形机器人极端平衡任务时面临的三大关键挑战：参考动作误差、形态差异导致的平衡策略学习困难、以及仿真到现实差距。2）<strong>提出了统一的HuB框架</strong>，创新性地集成了针对上述三个挑战的解决方案：通过SMPL初始化与后处理精炼参考动作；通过松弛跟踪与平衡塑形奖励实现平衡感知策略学习；通过局部跟踪、IMU中心扰动和高频推动提升仿真到现实鲁棒性。3）<strong>在极具挑战性的准静态平衡任务上实现了突破</strong>，在仿真和真实机器人上均验证了其高成功率和强鲁棒性，显著优于现有跟踪基线方法。</p>
<p>论文自身提到的局限性在于，当前工作主要关注准静态平衡任务，尚未扩展到需要连续动态运动（如行走、跑步）的场景。此外，师生学习范式需要额外的蒸馏步骤。</p>
<p>对后续研究的启示包括：HuB的框架组件（如针对性的动作精炼、平衡感知奖励设计、传感器特性模拟的扰动方法）可被借鉴用于其他人形机器人高精度控制任务；如何将针对准静态平衡的鲁棒性训练策略与动态运动的控制相结合，是一个有前景的方向；探索更高效、无需蒸馏的端到端学习方法来获得同样鲁棒的策略也值得研究。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出HuB框架，旨在解决人形机器人执行高难度静态平衡任务时的三大挑战：参考动作误差导致的不稳定、形态不匹配增加的学习难度，以及传感器噪声等因素引发的仿真到现实差距。其核心技术整合了参考动作优化、平衡感知策略学习与仿真到现实鲁棒性训练。在Unitree G1机器人上的实验表明，该方法能稳定完成燕子平衡、李小龙踢腿等极端单腿平衡姿态，并在强物理干扰下保持稳定，而基线方法均无法完成这些任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.07294" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>