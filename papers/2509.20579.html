<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Large Pre-Trained Models for Bimanual Manipulation in 3D - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Large Pre-Trained Models for Bimanual Manipulation in 3D</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.20579" target="_blank" rel="noreferrer">2509.20579</a></span>
        <span>作者: David Meger Team</span>
        <span>日期: 2025-09-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于体素的模仿学习策略在机器人操作领域取得了显著进展，特别是在单臂操作中。这类方法（如PerAct）将多视角RGB-D观测转换为结构化的3D体素网格，并利用Transformer架构直接预测动作，提供了良好的空间推理能力和视角不变性。然而，标准的体素表示主要编码几何和颜色信息，缺乏高层的语义线索，这限制了策略在复杂任务（尤其是需要精确协调的双手操作）中的理解和泛化能力。与此同时，大规模预训练的视觉Transformer模型（如DINOv2）已被证明能从图像中提取丰富的语义特征，并在导航、定位等任务中提升性能。</p>
<p>本文针对体素表示缺乏语义信息这一具体痛点，提出将预训练ViT的注意力图作为显著性先验注入到3D体素网格中，以增强策略的场景理解能力。本文的核心思路是：从DINOv2模型中提取像素级注意力图，将其解释为图像显著性分数，并通过多视角三角测量将其提升至3D体素空间，从而为基于体素的行为克隆策略提供额外的语义通道。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程是一个轻量级的体素特征化预处理步骤，旨在增强输入给下游策略的体素表示，而无需修改策略架构本身。其输入是多视角的RGB-D图像，输出是富含语义的体素特征网格，该网格随后与本体感觉、语言指令等一同输入到基线策略（VoxAct-B）中进行训练和推理。</p>
<p><img src="https://arxiv.org/html/2509.20579v1/figures/overview.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：体素特征化流程总览。使用DINOv2从RGB相机视图提取注意力图，并通过多视角三角测量将其投影到共享的3D体素网格中。每个体素内的RGB和注意力特征被平均。</p>
</blockquote>
<p>核心模块是视觉特征提取与体素化管道。具体技术细节如下：</p>
<ol>
<li><strong>视觉特征提取</strong>：对于每个观测时刻的RGB图像，使用DINOv2 ViT-S/14模型进行处理。从最后一层Transformer的6个自注意力头中提取注意力图。这些图是补丁级别的，数值在[0,1]范围内，值越接近1表示该区域受到越多“关注”。在主要实验中，作者仅使用第一个注意力头作为轻量级语义线索。这些低分辨率（74x74）的注意力图通过双线性插值上采样至原始输入分辨率（128x128），并应用0.6的软阈值以抑制噪声。</li>
<li><strong>体素化管道</strong>：该方法扩展了James等人提出的体素化流程。在已知相机内参和同步深度图的情况下，将每个像素反投影到3D点云，并关联其RGB颜色和上一步计算得到的注意力分数。预先定义一个2m x 2m x 2m的工作空间，并将其离散化为50x50x50的体素网格（分辨率约1.2厘米）。每个3D点根据其空间坐标被映射到对应的体素中。如果多个点落入同一体素，则对其RGB值和注意力分数进行平均。</li>
<li><strong>体素特征通道</strong>：每个被占据的体素最终由一组固定特征表示，包括：平均RGB值（3通道）、DINOv2注意力分数（1通道或多通道）、体素中心在世界坐标系中的平均3D位置（X, Y, Z，3通道）、归一化到[0,1]的网格相对位置（X̄, Ȳ, Z̄，3通道）以及二进制占用标志（1通道）。因此，当使用一个注意力头时，每个体素是一个11维的特征向量。空体素的所有特征通道（除占用标志外）被置零。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.20579v1/figures/voxelization.png" alt="学习流程"></p>
<blockquote>
<p><strong>图3</strong>：体素特征化与学习流程总览。在每个时间步t，RGB-D观测经DINOv2处理提取注意力图，后者与RGB特征、几何坐标和占用值一同投影到共享3D体素网格。体素化场景与双臂本体感觉输入结合，语言指令由CLIP编码并融合。结果输入被展平后送入PerceiverIO策略学习器，以预测t+1时刻的动作。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：提出了一种轻量级、模块化的特征增强策略，通过将预训练ViT的注意力图作为额外通道注入现有体素网格，为策略提供了高层语义先验。这种方法不改变下游策略网络结构，计算开销小，易于集成到现有的体素操作框架中。</p>
<h2 id="实验与结果">实验与结果</h2>
<ul>
<li><strong>Benchmark/数据集/平台</strong>：实验在RLBench双手机器人操作基准的仿真环境中进行。使用了该基准的四个双手桌面任务：<code>打开罐子</code>、<code>打开抽屉</code>、<code>将物品放入抽屉</code>和<code>递送物品</code>。每个任务在物体位置、大小和颜色上均有变化。使用两个Franka Emika Panda 7自由度机械臂。训练数据使用RLBench生成10或100条专家演示轨迹。</li>
<li><strong>Baseline方法</strong>：对比了五个强基线方法：Diffusion Policy、ACT with Transformers、VoxPoser、Bimanual PerActs以及本文的基线VoxAct-B。</li>
<li><strong>关键实验结果</strong>：本文方法（Ours）在几乎所有任务和演示数据量（10或100条）设置下均优于所有基线。平均而言，与基线VoxAct-B相比，本文方法在所有任务上取得了<strong>8.2%的绝对提升和21.9%的相对提升</strong>。具体亮点包括：在10条演示下，<code>打开罐子</code>任务提升15.0%；在100条演示下，<code>打开抽屉</code>任务提升14.4%；在10条演示下，<code>将物品放入抽屉</code>任务提升12.0%。值得注意的是，在<code>打开抽屉</code>任务上，仅用10条演示训练的本文方法（81.6%）甚至超过了用100条演示训练的VoxAct-B（72.8%），显示了其样本效率的提升。<code>递送物品</code>任务对所有方法都具挑战性，成功率较低。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="center">Open Jar (10/100 demos)</th>
<th align="center">Open Drawer (10/100 demos)</th>
<th align="center">Put Item in Drawer (10/100 demos)</th>
<th align="center">Hand Over Item (10/100 demos)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Diffusion Policy</td>
<td align="center">4.8 / 21.6</td>
<td align="center">4.8 / 5.6</td>
<td align="center">2.4 / 4.8</td>
<td align="center">0.0 / 0.0</td>
</tr>
<tr>
<td align="left">ACT with Transformers</td>
<td align="center">4.0 / 30.4</td>
<td align="center">12.8 / 28.0</td>
<td align="center">8.8 / 44.8</td>
<td align="center">1.6 / 7.2</td>
</tr>
<tr>
<td align="left">VoxPoser</td>
<td align="center">8.0 / 8.0</td>
<td align="center">32.0 / 32.0</td>
<td align="center">4.0 / 4.0</td>
<td align="center">0.0 / 0.0</td>
</tr>
<tr>
<td align="left">Bimanual PerActs</td>
<td align="center">8.0 / –</td>
<td align="center">36.8 / –</td>
<td align="center">5.6 / –</td>
<td align="center">0.0 / –</td>
</tr>
<tr>
<td align="left">VoxAct-B</td>
<td align="center">40.0 / 59.2</td>
<td align="center">73.6 / 72.8</td>
<td align="center">39.2 / 49.6</td>
<td align="center">19.2 / 14.4</td>
</tr>
<tr>
<td align="left"><strong>Ours</strong></td>
<td align="center"><strong>55.8 / 60.4</strong></td>
<td align="center"><strong>81.6 / 87.2</strong></td>
<td align="center"><strong>51.2 / 55.2</strong></td>
<td align="center"><strong>20.8 / 22.4</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表I</strong>：使用10条和100条演示训练的单任务策略在各任务上的成功率（%）。本文方法（Ours）表现最佳。</p>
</blockquote>
<ul>
<li><strong>消融实验</strong>：作者研究了使用不同数量DINOv2注意力头的影响。以<code>打开罐子</code>任务为例，仅使用RGB特征（0个注意力图）的基线成功率为40.8%。加入1个注意力图后，成功率提升至55.8%（+15.0%）。使用3个和5个注意力图后，成功率进一步提升至64.4%和69.8%。<strong>使用5个注意力图相比基线带来了29.0%的绝对性能增益</strong>。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">Number of Attention Maps</th>
<th align="center">Open Jar Success Rates (%)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">0</td>
<td align="center">40.8</td>
</tr>
<tr>
<td align="left">1</td>
<td align="center">55.8</td>
</tr>
<tr>
<td align="left">3</td>
<td align="center">64.4</td>
</tr>
<tr>
<td align="left">5</td>
<td align="center">69.8</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表II</strong>：在<code>打开罐子</code>任务上，使用不同数量DINOv2注意力图的消融实验结果。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.20579v1/figures/train_loss_2x2.png" alt="训练损失"></p>
<blockquote>
<p><strong>图4</strong>：所有任务在10条演示下训练、平均超过5个随机种子的总训练损失曲线。曲线经过平滑处理。损失持续下降的趋势表明添加注意力图不会破坏训练稳定性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有两点：</p>
<ol>
<li>提出了一种轻量级的预处理方法，将预训练ViT衍生的注意力特征作为语义先验注入3D体素网格，且无需修改现有策略框架。</li>
<li>在RLBench双手操作基准上验证了该方法的有效性，与先进的VoxAct-B基线相比，实现了平均8.2%的绝对性能提升，证明了语义先验对提升体素策略性能的积极作用。</li>
</ol>
<p>论文自身提到的局限性包括：</p>
<ol>
<li>DINOv2的注意力头并非显式可解释的，其对性能的贡献机制尚不明确。</li>
<li>需要灵巧协调的任务（如<code>递送物品</code>）成功率仍然很低，表明当前体素分辨率和动作建模对于紧密的物理交互可能不足。</li>
<li>实验仅在仿真中进行，未验证在真实世界的泛化能力。</li>
<li>训练和评估的计算成本仍然较高。</li>
</ol>
<p>对后续研究的启示在于：将大规模预训练模型的语义先验与结构化的3D场景表示相结合，是一种提升机器人策略感知与推理能力的有效且通用的途径。这种轻量级的特征增强策略可以模块化地集成到多种基于体素或点云的框架中。未来工作可以探索更智能的注意力选择机制、针对机器人任务微调ViT表示，以及开发更适合双手紧密协调的分层规划或更精细的场景表示方法。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何利用预训练视觉变换器（ViT）增强双手机器人操作的性能。核心问题是解决双手机器人操作中视觉感知与协调的挑战，通过将语义注意力集成到3D体素表示来提升任务表现。关键技术方法为：从自监督ViT模型DINOv2提取注意力图，将其解释为RGB图像的像素级显著性分数，并提升到3D体素网格中生成体素级语义线索，最终整合到基于体素的行为克隆策略。实验结果表明，在RLBench双手机器人基准测试中，该方法使最先进的体素策略平均绝对性能提升8.2%，相对增益达21.9%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.20579" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>