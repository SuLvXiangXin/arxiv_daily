<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.19476" target="_blank" rel="noreferrer">2508.19476</a></span>
        <span>作者: Mark Cutkosky Team</span>
        <span>日期: 2025-08-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在家庭和商业场景中，密集且受限的环境（如塞满物品的橱柜和货架）十分常见。传统机器人运动规划方法通常避免接触，但在这种高度杂乱、视觉遮挡严重的横向接触场景中，往往不存在无碰撞的路径到达目标物体。现有研究在桌面杂乱场景的物体取出或整理方面取得进展，但通常依赖俯视视角，这与横向接触场景中严重的视觉遮挡不符；另一些研究虽然处理横向接触场景，但多限于较低杂乱密度，且通常不使用非抓握式触觉传感。因此，让机器人在必须接触时“拥抱接触”，同时避免损坏环境或自身，成为一个关键挑战。本文针对在密集杂乱受限环境中（物体覆盖面积达45-55%）轻柔取出物体的难题，提出结合视觉与多模态力感知（关节扭矩估计的接触扳手和非抓握式三轴触觉阵列传感），通过模仿学习训练策略。其核心思路是：利用模仿学习框架，整合手眼视觉、本体感知、扳手信息、触觉信号和吸盘压力信息，训练出一个能够感知接触力、在必要时调整姿态以避免过度施力并成功取出目标物体的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架基于扩散策略（Diffusion Policy）。策略的输入观测包含多模态传感器数据：1）“手眼”单目鱼眼相机图像；2）机器人本体感知（工具中心点TCP的笛卡尔位置和四元数姿态）；3）通过关节扭矩估计得到的、在TCP坐标系下的接触扳手（wrench）信息；4）来自末端执行器两侧的49单元三轴触觉阵列信号，被编码为触觉图像；5）基于吸盘真空管路压力读数得到的二值化抓取成功信号。这些观测经过处理后被输入网络，网络输出一个8维动作向量，包括指令的TCP位置、四元数姿态和二值化的吸盘开关命令。</p>
<p><img src="https://arxiv.org/html/2508.19476v2/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：力感知扩散策略网络。输入的“手眼”相机图像和触觉图像分别通过独立的预训练ResNet-18编码器。编码后的特征与归一化的低维传感器数据（扳手、本体感知、压力）拼接，形成一个观测。该观测与历史观测一起送入扩散策略头（Diffusion head）进行动作预测。</p>
</blockquote>
<p>核心模块包括传感器编码与策略网络。触觉信息的具体编码方式是关键细节：每个触觉阵列补充一个零力传感单元凑成50个，然后排列成20x5像素的图像。三个方向的力分别映射到RGB通道：x方向力（范围[-1N, 1N]）映射到蓝色通道，y方向力（范围[-1N, 1N]）映射到绿色通道（左右传感器y方向范围经过翻转以使末端执行器“向上”方向对应强度增加），z方向法向力（范围[0N, -5N]）映射到红色通道。0N剪切力对应蓝绿通道强度值127。</p>
<p><img src="https://arxiv.org/html/2508.19476v2/x1.png" alt="末端执行器与传感器"></p>
<blockquote>
<p><strong>图1</strong>：a) 配备软体三轴触觉传感器、吸盘和相机的末端执行器。b) 机器人伸入密集杂乱环境时，“手眼”相机提供的观测视图。c) 场景的俯视图（机器人不可见）。机器人的初始姿态（蓝色箭头）导致物体卡在左侧墙壁，产生巨大接触力（红色箭头）。动作序列使机器人移动到新姿态（绿色箭头），脱离接触的同时仍接近目标。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.19476v2/x2.png" alt="触觉数据可视化"></p>
<blockquote>
<p><strong>图2</strong>：a) 末端执行器左右两侧图像及触觉阵列坐标系。b) 分布式三轴力信息可视化：法向力大小与圆直径成正比，剪切力大小和方向与箭头成正比。c) 对应的触觉图像，x、y、z方向力分别映射到B、G、R通道。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1）在高度密集、受限的横向接触杂乱场景中，系统性地研究了扳手（关节扭矩估计）与非抓握式触觉传感两种力感知模态对策略性能的贡献及互补性；2）采用了一种将高维触觉信号编码为紧凑RGB图像并利用视觉编码器（ResNet-18）进行特征提取的有效方法；3）定义了结合净力（来自扳手）和峰值触觉力的双重“过度力冲量”安全约束，以更全面地防止损坏。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在一个38cm x 53cm x 32cm的橱柜搁板内进行，使用5x7的网格随机生成杂乱场景。物体包括3种红色目标物体和4种颜色的障碍物，每次随机放置25-28个障碍物，面积占用率为45%至55%。任务要求机器人在120秒内从起始位置导航至目标物体，用吸盘抓取并带回起始区域。</p>
<p>对比了四种策略，构成消融实验：1) <strong>基线</strong>：屏蔽扳手和触觉信息；2) <strong>扳手感知</strong>：仅使用扳手信息，屏蔽触觉；3) <strong>触觉感知</strong>：仅使用触觉信息，屏蔽扳手；4) <strong>扳手+触觉</strong>：同时使用两种力信息。所有策略基于相同的100条人类示教数据，使用相同的扩散策略架构和超参数训练200轮。</p>
<p><img src="https://arxiv.org/html/2508.19476v2/x6.png" alt="性能结果"></p>
<blockquote>
<p><strong>图6</strong>：力消融实验在40次评估中的性能结果。a) 各策略的成功、超时和过度力失败比例。*表示与基线策略的双侧配对z检验p&lt;0.05；†表示与扳手+触觉策略的双侧配对z检验p&lt;0.05。b) 各策略的成功任务耗时（误差棒为标准误）。</p>
</blockquote>
<p>关键实验结果：任何包含力感知的策略均显著优于基线。<strong>扳手+触觉</strong>策略性能最佳，相比基线实现了80%的相对提升（成功率从约25%提升至约45%）。性能提升主要源于<strong>过度力失败</strong>的大幅减少。此外，<strong>扳手+触觉</strong>策略也显著减少了<strong>超时失败</strong>。在任务完成速度上，所有力感知策略都比基线更快，其中扳手感知策略最快。</p>
<p><img src="https://arxiv.org/html/2508.19476v2/x7.png" alt="力与运动热图"></p>
<blockquote>
<p><strong>图7</strong>：各策略在所有评估时间步中，测量到的力与随后0.8秒内末端执行器位移的热图。a,b) 基线策略在净力和峰值触觉力下的运动情况。c,d) 扳手感知策略。e,f) 触觉感知策略。g,h) 扳手+触觉策略。黑色虚线为过度力阈值。右下角区域（高力、低运动）代表未能对接触做出反应。</p>
</blockquote>
<p>通过分析力与后续运动的热图（图7）发现，基线策略在高接触力下出现大量“低运动”数据点（412次未能对过大接触力做出反应），而力感知策略此类情况显著减少（扳手感知17次，触觉感知72次，扳手+触觉22次）。这表明力感知使策略学会了在接触力增大时主动调整位姿以避免持续施力。扳手感知在应对大接触力时似乎比单独触觉感知更有效。</p>
<p>消融实验总结：1) <strong>扳手信息</strong>和<strong>触觉信息</strong>各自都能独立带来性能提升（减少过度力失败，提高成功率，加快任务速度）；2) <strong>两者结合</strong>能产生最佳性能，特别是在减少超时失败方面表现出协同优势。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）首次在高度密集、受限的横向接触杂乱场景中，通过系统的消融实验，实证了关节扭矩估计的扳手信息和非抓握式触觉信息对于实现轻柔物体取出任务均至关重要，且两者结合能实现最佳性能。2）提出并验证了一套完整的硬件系统与模仿学习框架，能够有效整合视觉、多模态力感知和本体感知，处理复杂的接触式操作。3）定义了基于物理实验的双重过度力冲量安全准则，为评估“轻柔”操作提供了量化指标。</p>
<p>论文自身提到的局限性包括：研究场景（物体形状、可变形性、光学特性）的变异性有限；所确定的过度力阈值仅适用于本任务和环境。</p>
<p>本工作对后续研究的启示：首先，它表明在视觉干扰众多、存在因果混淆风险的模仿学习任务中，引入力感知这类与专家决策高度因果关联的模态，可能是提升策略鲁棒性和性能的有效途径。其次，展示了将高维触觉信号编码为图像并用成熟视觉网络处理的可行性。未来工作可探索更复杂的物体属性（如可变形状、可变形材料），并将系统扩展到更广泛的家庭或仓库整理任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究机器人如何在密集杂乱环境中安全取回物体而不造成损坏。核心方法是结合多模态感知（手眼视觉、本体感知、触觉、关节力矩估计的接触力矩、吸盘真空监测）与模仿学习，训练机器人策略以在必要接触时控制力度。实验表明，引入力感知能显著减少过度施力失败、提高成功率和速度；同时使用触觉与力矩信息时性能最佳，相比无力感知基线提升80%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.19476" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>