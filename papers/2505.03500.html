<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Task Reconstruction and Extrapolation for $\pi_0$ using Text Latent - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Task Reconstruction and Extrapolation for $\pi_0$ using Text Latent</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.03500" target="_blank" rel="noreferrer">2505.03500</a></span>
        <span>作者: Li, Quanyi</span>
        <span>日期: 2025/05/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于大规模多模态数据集训练的视觉-语言-动作模型在多样化的操作任务上展现出强大的性能。为了在新任务上取得满意效果，通常的做法是使用目标任务的演示数据对VLA进行微调。然而，这种范式虽然保证了模型在光照条件、场景布局微小扰动等分布内泛化能力，但在面对分布外任务，尤其是那些从已掌握任务中“外推”组合而成的新任务时，模型表现显著下降。例如，VLA可能分别成功执行“将奶油奶酪放入碗中”和“将碗放在橱柜上”，却无法完成外推任务“将奶油奶酪放在橱柜上”，尽管该任务所需的“拿起奶油奶酪”和“到达橱柜顶部”这两个动作基元已在不同任务中学过。这引发了一个疑问：VLA仅仅是过拟合了演示轨迹，还是学到了可组合的内部表征以支持更广泛的泛化？</p>
<p>本文针对VLA在技能组合与外推上的关键局限性，提出从模型内部表征（特别是与文本任务描述相关的隐藏状态）入手的新视角。核心思路是：从VLA（以π0为例）的隐藏状态中提取任务特定的“文本潜在向量”，通过将其写回模型的残差流可以重建对应任务行为；进一步，通过混合不同任务的文本潜在向量，可以组合技能以完成全新的外推任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文研究的对象是基于Transformer架构的VLA模型π0。模型接收图像、文本（任务描述）和本体感知信息的嵌入，经过L层Transformer处理，最终生成动作。文本潜在向量被定义为与任务描述文本令牌相关的隐藏状态集合。</p>
<p><strong>整体流程</strong>：首先，在目标任务的多条演示轨迹上运行π0，记录所有时间步中文本令牌在各Transformer层的隐藏状态；其次，对这些隐藏状态进行跨时间步和演示的平均，得到该任务的文本潜在向量𝒯；最后，在执行新任务（或重建原任务）时，将计算好的𝒯（或经过插值混合的𝒯）注入到模型对应层的残差流中，从而影响模型的决策，激活特定行为。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>文本潜在向量的识别与重建</strong>：对于给定任务，收集其演示数据中所有时间步的文本隐藏状态张量h^T，按公式𝒯 = (1/Σ|B_k|) Σ Σ h^T(i) 计算平均值。该向量被证明编码了完成任务最核心的知识。通过将𝒯写回文本令牌的残差流，即使屏蔽或清空原始任务提示，模型也能以高成功率（例如在liber-goal上从16%提升至82%）重建任务行为。此外，对𝒯的早期层向量进行“反嵌入”可以生成不可读的替代提示，也能以约70%的成功率指导模型完成任务，这揭示了潜在的安全风险（私有指令或后门攻击）。</p>
</li>
<li><p><strong>文本潜在插值</strong>：为了组合两个基础任务（Task 1和Task 2）的技能以完成外推任务，提出了文本潜在插值方法。其核心思想是在任务执行过程中，随时间线性调整两个任务文本潜在向量𝒯¹和𝒯²对模型状态的贡献。具体操作如公式所示：在每个时间步i，修改当前的文本隐藏状态h^T(i)，为其加上一项[(1-α)𝒯¹ + α𝒯²] - [(1-α)𝒯² + α𝒯¹]，其中α = i/λ（λ控制过渡速度）。这意味着在 episode 开始时，Task 2 的上下文被抑制并从残差流中减去；随着 episode 进行，Task 2 的上下文逐渐注入，而 Task 1 的上下文逐渐淡出并被最终抑制。该方法引导模型在开始时表现出Task 1的行为，随后平滑过渡到Task 2的行为，从而将两个任务的子轨迹“缝合”起来。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03500v4/x1.png" alt="方法总览图"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架与效果概览。第一行展示了两个基础任务（如“放碗入柜”和“取奶酪”），第二行展示了通过文本潜在插值（TLI）使π0成功完成的三个外推任务示例（如“放奶酪于柜上”）。底部柱状图显示，尽管经过微调，多个SOTA VLA在提出的libero-ood基准上成功率均低于21%，而应用TLI后，π0的成功率提升至83%。</p>
</blockquote>
<p><strong>创新点</strong>：与现有仅依赖模型输入输出或进行对抗攻击、特征归因的机械可解释性研究不同，本文首次在VLA中发现了内部表征（文本潜在向量）与任务行为之间的因果关系。通过主动操纵这些内部表征，不仅能重建行为，更能实现技能的组合与外推，这揭示了模型虽然学习了独立且可组合的技能表示，却缺乏自主组合它们的能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准</strong>：使用LIBERO仿真环境，包括三个标准任务套件（libero-goal, libero-object, libero-spatial，各10个任务）以及本文新提出的libero-ood外推任务套件（包含libero-goal-ood和libero-spatial-ood，共20个任务）。</li>
<li><strong>模型</strong>：主要研究对象为π0，并对比了其他SOTA VLA：π0-fast, openvla-oft, UniVLA。</li>
<li><strong>评估</strong>：每个任务进行10次独立运行，成功率计算为成功episode占总episode（10任务×10次/任务）的比例。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.03500v4/x2.png" alt="libero-ood任务可视化"></p>
<blockquote>
<p><strong>图2</strong>：libero-ood任务套件的可视化。展示了场景布局、需要拾取的物体（红色箭头起点）及放置目标（红色箭头终点）以及任务提示。这些任务的设计确保了抓取和放置的位置都单独出现在训练数据中，但连接它们的完整轨迹是全新的。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>任务重建</strong>：如表1所示，当屏蔽或清空任务提示时，π0在所有套件上的性能骤降（如libero-goal从95%降至16%）。然而，在清空提示的同时向残差流写入对应的文本潜在向量𝒯后，性能大幅恢复（libero-goal恢复至82%），证明了𝒯编码了关键任务语义。使用𝒯反嵌入得到的替代提示（𝒯₁-Prompt等）也能有效指导任务。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03500v4/x3.png" alt="任务重建成功率"></p>
<blockquote>
<p><strong>图3</strong>：任务重建实验的定量结果（对应论文表1）。展示了原始提示、掩码/清空提示、以及清空提示加文本潜在向量干预等多种设置下的成功率，证实了文本潜在向量对重建任务行为的有效性。</p>
</blockquote>
<ol start="2">
<li><strong>任务外推</strong>：在libero-ood基准上，微调后的π0成功率仅为9%。应用文本潜在插值后，其成功率跃升至83%。相比之下，其他SOTA VLA（π0-fast, openvla-oft, UniVLA）在该基准上的最高成功率不超过21%，凸显了它们在技能外推组合方面的普遍局限性。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03500v4/x4.png" alt="外推任务成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：不同VLA模型在标准LIBERO套件与libero-ood套件上的成功率对比。所有模型在标准任务上表现优异（&gt;95%），但在libero-ood上均表现不佳（&lt;21%），唯有π0在应用TLI后取得83%的高成功率。</p>
</blockquote>
<ol start="3">
<li><strong>消融分析与层间贡献</strong>：实验探究了TLI应用于不同Transformer层的效果。结果显示，同时干预所有层（Full）效果最好。单独干预早期层（如第0层）或晚期层（如第23层）也能带来显著提升，但效果不及全层干预。这说明了技能表征分布在模型的多个层次中。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03500v4/imgs/success_rate_layer.png" alt="不同层TLI效果"></p>
<blockquote>
<p><strong>图5</strong>：文本潜在插值应用于不同Transformer层时的消融实验结果。横轴表示干预的层（如“0”表示仅干预第0层，“Full”表示干预所有层），纵轴为libero-ood上的成功率。全层干预效果最佳。</p>
</blockquote>
<ol start="4">
<li><strong>定性分析与空间过拟合</strong>：进一步分析发现，VLA普遍表现出“空间过拟合”现象。例如，当指令要求拾取一个物体时，模型倾向于移动到该物体在演示场景中出现过的位置，而非其当前位置。这表明即使经过微调，VLA也未能真正理解物体或目标的概念，而是机械地记忆了轨迹与空间位置的关联。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.03500v4/x6.png" alt="空间过拟合示例"></p>
<blockquote>
<p><strong>图6</strong>：空间过拟合的定性示例。在“拿起苹果”任务中，即使苹果被移动到新位置（右图），VLA（此处为UniVLA）仍然尝试前往苹果在原始演示中的位置（左图虚线框）进行抓取，而非实际位置。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出并验证了文本潜在向量作为VLA技能表示的有效工具</strong>：首次在VLA中展示了通过操纵模型内部文本令牌的隐藏状态，可以因果性地控制其任务行为，实现任务重建与技能组合。</li>
<li><strong>设计了libero-ood基准并揭示了VLA外推能力的普遍不足</strong>：提出了一个具有挑战性但理论上可通过组合已有技能解决的外推任务集，并证实当前SOTA VLAs在此类任务上表现惨淡，突显了现有方法的局限性。</li>
<li><strong>揭示了VLA存在“空间过拟合”问题</strong>：通过分析模型失败案例，指出VLA可能并未获得真正的物体或目标理解，而是过拟合于演示轨迹中的空间位置关联。</li>
</ol>
<p><strong>局限性</strong>：论文提到，所提出的TLI方法是一种后训练技巧，需要依赖目标任务的演示数据来计算文本潜在向量。理想情况下，模型应具备自主组合内部技能表示以完成外推任务的能力。此外，文本潜在向量的反嵌入可能生成不可读的提示，这带来了潜在的安全与隐私风险（如后门攻击）。</p>
<p><strong>对后续研究的启示</strong>：本文工作表明，VLAs内部已经学习了可组合的技能表示，但缺乏自主调用和组合的机制。这激励社区去探索新的训练范式或架构设计，以解锁模型内在的外推能力，而无需依赖类似TLI的后处理技巧。同时，libero-ood基准的发布为评估和提升VLA的组合泛化能力提供了一个明确的测试平台。对空间过拟合现象的发现也提示，需要更注重培养VLA对物体属性和目标语义的鲁棒性理解，而非仅仅记忆轨迹。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型（VLAs）在任务外推上的困难，研究其是否仅过拟合演示任务而非学习可组合技能。提出使用文本潜在向量（text latent），该向量从模型隐藏状态提取，编码任务语义，可通过写入残差流重建任务行为，并通过混合不同任务的向量组合技能以产生新行为。应用于π₀模型，在libero-ood基准测试中，成功率从9%提升至83%，而其他VLAs成功率均不超过21%，验证了技能表示的可组合性及模型自主外推的不足。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.03500" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>