<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.05107" target="_blank" rel="noreferrer">2512.05107</a></span>
        <span>作者: Benjamin Busam Team</span>
        <span>日期: 2025-12-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前微调视觉-语言-动作（VLA）模型的主流方法，如轨迹级偏好优化（TPO）和近端策略优化（PPO），通常将长时程动作序列视为一个整体进行优化。这种方法存在关键局限性：信用分配模糊，难以确定轨迹中具体哪个阶段导致了成功或失败；奖励稀疏，尤其是在长时程任务中；以及由于巨大的优化空间导致训练不稳定。然而，与语言推理中句子顺序灵活但语义统一不同，动作轨迹由因果链式、学习难度各异的语义阶段构成。本文针对长时程动作轨迹的阶段性特点和现有轨迹级优化方法信用分配粗糙的痛点，提出了将轨迹分解为语义阶段并进行渐进式阶段感知优化的新视角。本文核心思路是设计一个阶段感知强化（StARe）模块来分解轨迹并提供密集的阶段对齐奖励信号，进而构建一个包含模仿、偏好、交互的三阶段序列微调流程（IPI）。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架是一个序列化的三阶段微调流程，称为 IPI（模仿→偏好→交互）。该流程首先使用专家演示通过监督微调（SFT）初始化模型；然后利用阶段感知轨迹偏好优化（StA-TPO）基于离线阶段偏好数据进行优化；最后通过阶段感知近端策略优化（StA-PPO）在在线环境中进行阶段内交互优化。输入是预训练的VLA策略、任务指令和环境观测，输出是优化后的策略，其在长时程任务中具有更高的成功率和泛化能力。</p>
<p><img src="https://arxiv.org/html/2512.05107v2/images/robot/tasks.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：StARe框架及其集成到IPI训练流程的概览。展示了IPI流程的三个步骤：模仿学习（SFT）、离线阶段感知偏好优化（StA-TPO）和在线阶段感知交互优化（StA-PPO）。核心是StARe模块，它接收轨迹（来自数据或模型交互），通过阶段分离器划分阶段，并通过阶段计算器计算阶段成本和每步奖励。</p>
</blockquote>
<p>核心模块是阶段感知强化（StARe）模块，它包含两个组件：</p>
<ol>
<li><strong>阶段分离器</strong>：基于任务特定语义规则（例如末端执行器与目标的相对位置和方向是否超过阈值）检测阶段转换事件，将完整轨迹τ分解为K个语义阶段片段{τ^(1), ..., τ^(K)}，并为每个时间步t分配阶段标签k。</li>
<li><strong>阶段计算器</strong>：用于评估每个阶段的执行质量。它计算<strong>阶段成本</strong>（如公式(3)，以“到达”阶段为例，计算末端执行器与目标物体之间的平均欧氏距离）和<strong>阶段内密集奖励</strong>。密集奖励通过基于势函数的奖励塑形实现（公式(4)-(5)），例如，为“到达”阶段定义一个关于距离的势函数，通过相邻时间步势函数之差来提供平滑的渐进奖励信号。</li>
</ol>
<p>基于StARe模块，论文提出了两个核心算法：</p>
<ul>
<li><strong>阶段感知轨迹偏好优化（StA-TPO）</strong>：这是对离线偏好优化方法TPO的改进。如算法1所示，StARe将比较的成功与失败轨迹对(τ+, τ-)分解为阶段。然后，在计算每个阶段的偏好得分q(τ^(k))时，引入了阶段成本ℓ_k(τ^(k))作为惩罚项（公式(6)），形成修正得分q̂(τ^(k))。损失函数ℒ_StA-TPO促使模型在阶段级别上偏好得分更高（即成本更低）的轨迹。这使得优化不仅能区分成功与失败，还能区分不同程度的成功，实现了更细粒度的信用分配。</li>
<li><strong>阶段感知近端策略优化（StA-PPO）</strong>：这是对在线强化学习方法PPO的改进。如算法2所示，在策略与环境交互的每一步，StARe在线确定当前阶段k并计算势函数Φ_kt(s_t)。随后，将原始的稀疏奖励r_t替换为通过势函数塑形后的密集奖励r&#39;_t（公式(5)）。PPO的目标函数ℒ_PPO中的奖励相应被替换，形成ℒ_StA-PPO。这为策略学习提供了更密集、与阶段进展对齐的反馈，加速了在稀疏奖励长时程任务中的学习。</li>
</ul>
<p>与现有方法相比，创新点具体体现在：1) <strong>阶段分解</strong>：不同于将轨迹视为整体，显式建模了动作轨迹的语义阶段结构。2) <strong>阶段成本惩罚</strong>：在StA-TPO中引入阶段成本，实现了对阶段执行质量的细粒度偏好优化。3) <strong>潜在奖励塑形</strong>：在StA-PPO中利用阶段势函数提供密集的渐进式奖励，缓解了稀疏奖励问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了两个机器人操作基准测试框架：SimplerEnv和ManiSkill3。对比的基线方法包括：监督微调（SFT）、轨迹级偏好优化（TPO）、近端策略优化（PPO），以及结合了SFT与TPO或PPO的混合方法。实验平台未明确说明，但涉及模拟环境。</p>
<p>关键实验结果：在SimplerEnv任务上，仅使用SFT的成功率为85.4%，SFT+TPO为91.7%，SFT+PPO为95.5%。而本文提出的完整IPI流程（SFT + StA-TPO + StA-PPO）取得了98.0%的成功率，达到了最先进水平。在ManiSkill3任务上，IPI流程取得了96.4%的平均成功率，显著优于SFT（87.6%）、SFT+TPO（92.0%）和SFT+PPO（94.8%）等基线方法。</p>
<p><img src="https://arxiv.org/html/2512.05107v2/x2.png" alt="主实验结果"></p>
<blockquote>
<p><strong>图3</strong>：在SimplerEnv和ManiSkill3上的主要性能对比。柱状图清晰显示，IPI流程（即SFT+StA-TPO+StA-PPO）在两个基准测试上都取得了最高的平均成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.05107v2/x3.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融研究结果。左图显示，在IPI流程中，依次添加StA-TPO和StA-PPO都能带来性能提升，验证了每个步骤的必要性。右图对比了不同奖励设计，表明StARe提供的阶段感知密集奖励（Ours）优于轨迹级密集奖励和纯稀疏奖励。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.05107v2/x4.png" alt="泛化性能"></p>
<blockquote>
<p><strong>图5</strong>：分布外泛化性能。IPI流程在物体位置、姿态和任务指令分布变化的泛化设置下，均表现出比基线方法（SFT+PPO）更优的性能，说明阶段感知优化有助于学习更鲁棒的策略。</p>
</blockquote>
<p><img src="https://arxiv.org/pdf/2512.05107v2" alt="定性结果"></p>
<blockquote>
<p><strong>图6</strong>：定性结果示例。展示了IPI与基线方法（SFT+PPO）在失败案例上的对比。IPI能成功完成长时程任务（如堆叠），而基线方法在复杂阶段（如精确放置）容易失败。</p>
</blockquote>
<p>消融实验总结：1) <strong>IPI流程的有效性</strong>：在SFT基础上，依次加入StA-TPO和StA-PPO，性能逐步提升至最优。2) <strong>奖励设计的重要性</strong>：使用StARe的阶段感知密集奖励显著优于使用轨迹级密集奖励或仅使用原始稀疏奖励。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了<strong>StARe模块</strong>，一个基于规则、能将长时程动作轨迹分解为语义阶段并提供密集评估信号的插件。2) 基于StARe，设计了<strong>StA-TPO和StA-PPO算法</strong>，分别用于离线阶段感知偏好优化和在线阶段感知交互优化，实现了更精确的信用分配。3) 整合SFT、StA-TPO和StA-PPO，提出了<strong>IPI序列微调流程</strong>，为VLA模型微调提供了一个统一且高效的框架，并在基准测试中达到了最先进的性能。</p>
<p>论文自身提到的局限性主要在于StARe模块对<strong>任务特定规则</strong>的依赖，其阶段划分和成本计算需要针对不同任务进行设计，可能限制了方法的通用性。此外，IPI的三阶段序列训练可能带来较高的计算成本。</p>
<p>对后续研究的启示包括：1) <strong>阶段感知优化</strong>是提升长时程VLA任务性能的有效方向，未来可探索更通用或可学习的阶段划分方法。2) <strong>混合训练流程</strong>（结合模仿、偏好、交互）显示出优势，值得在其他序列决策问题中进一步探索。3) <strong>密集、可解释的奖励设计</strong>对于复杂机器人任务至关重要，基于语义阶段的奖励塑形是一个有前景的思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对Vision-Language-Action (VLA)模型微调中，现有方法将长时程动作轨迹视为语言序列进行轨迹级优化，导致信用分配粗糙、训练不稳定的问题，提出了Stage-Aware Reinforcement (StARe)模块。该模块将动作轨迹分解为语义阶段，提供密集、可解释的强化信号。基于此，发展了Stage-Aware TPO (StA-TPO)和Stage-Aware PPO (StA-PPO)，并结合Imitation → Preference → Interaction (IPI)序列微调管道。实验在SimplerEnv和ManiSkill3上实现最先进性能，成功率分别达98.0%和96.4%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.05107" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>