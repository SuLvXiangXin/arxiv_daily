<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.01301" target="_blank" rel="noreferrer">2504.01301</a></span>
        <span>作者: Kobayashi, Takumi, Kobayashi, Masato, Buamanee, Thanpimon, Uranishi, Yuki</span>
        <span>日期: 2025/04/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人模仿学习领域的主流方法大多基于单边控制的遥操作设置，即人类示教者仅提供位置指令。虽然这类系统易于实现，但难以捕捉操作易碎或可变形物体所必需的丰富力交互信息。与此同时，大语言模型和视觉语言模型在机器人规划和语义感知方面取得了显著进展，但在控制精细力和扭矩方面仍显不足，而这对于安全有效的物体操作至关重要。双边控制通过在人机之间交换位置和力信息，使示教者能实时感知接触力，从而能采集到更精确反映物理交互的训练数据，已被证明能比单边设置更好地泛化到不同硬度和重量的物体。然而，现有的双边控制模仿学习方法（如Bi-ACT）虽然结合了视觉信息，但尚未能融入指定施加多大力量的自然语言指令。</p>
<p>本文针对上述痛点，提出了一种新视角：将自然语言处理与双边控制模仿学习相结合，使机器人能够根据人类可读的高级语言指令（如“轻柔地抓握杯子”）动态调整执行任务时的施加力量，而非依赖数值定义的扭矩阈值。本文的核心思路是：通过一个融合机器人状态（关节角度、速度、扭矩）、视觉观察和语言线索的多模态Transformer模型，将日常语言指令直接映射到任务执行过程中的显式力调节。</p>
<h2 id="方法详解">方法详解</h2>
<p>Bi-LAT的整体框架包含数据收集、学习模型和推理三个阶段。在数据收集阶段，操作者通过主端机器人遥操作从端机器人，同时记录自然语言指令、机器人关节数据和相机图像。在学习阶段，多模态输入被编码并输入到一个基于Transformer的条件变分自编码器模型中，预测未来主端机器人的动作序列。在推理阶段，模型根据当前的从端状态、视觉观察和语言指令，预测并执行相应的动作块。</p>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/tsr5.jpg" alt="方法整体框架"></p>
<blockquote>
<p><strong>图1</strong>：Bi-LAT方法整体框架。左侧展示了融合机器人信息、视觉信息和语言指令的多模态输入，右侧展示了基于Transformer CVAE的模型预测动作序列的过程。</p>
</blockquote>
<p>核心模块一：基于四通道双边控制的数据收集。双边控制的核心是主从系统间持续共享信息以实现协调行为，其数学表征为位置一致（θ_l - θ_f = 0）和力反作用（τ_l + τ_f = 0）两个等式。关节角度由编码器获取，扭矩响应则通过扰动观测器和反作用力观测器估算，无需力/扭矩传感器。数据收集时，操作者在遥操作的同时提供描述动作的自然语言指令（如图2所示），并采用了针对CLIP模型的提示工程技巧（如在指令前添加“a photo of a”）来提升语言模型性能。</p>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/bi-lat_collect.jpg" alt="数据收集示意图"></p>
<blockquote>
<p><strong>图2</strong>：Bi-LAT数据收集过程。人类操作者通过主端机器人控制从端机器人，并同步说出自然语言指令，系统同时记录机器人关节数据、相机图像和语音转文本的指令。</p>
</blockquote>
<p>核心模块二：语言编码器。为了将自然语言指令转换为分布式表示，论文评估了来自CLIP系列（CLIP和SigLIP）和BERT系列（DistilBERT和ModernBERT）的四种语言编码器，如图4所示。CLIP系列擅长整合视觉和文本线索，BERT系列则精于自然语言理解。</p>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/bi-lat_le.jpg" alt="语言编码器选项"></p>
<blockquote>
<p><strong>图4</strong>：Bi-LAT采用的语言编码器选项，包括DistilBERT、ModernBERT、CLIP和SigLIP。</p>
</blockquote>
<p>核心模块三：基于Transformer CVAE的学习模型。如图3所示，模型接收从端机器人的关节角度、速度、扭矩数据、视觉输入和自然语言指令。语言指令由语言编码器处理为固定长度向量。视觉特征由ResNet-18提取。这些特征与机器人状态特征融合，形成综合的潜在表示，随后输入到CVAE-based Transformer解码器中，预测未来主端机器人的动作序列（关节角度、速度、扭矩）。训练目标是最小化预测的主端关节数据与双边控制遥操作收集的真实数据之间的误差。</p>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/bi-lat.jpg" alt="学习模型架构"></p>
<blockquote>
<p><strong>图3</strong>：Bi-LAT学习模型架构。展示了多模态输入（语言、图像、机器人状态）的编码、融合，以及通过CVAE Transformer解码器预测动作序列的过程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/bi-lat_inference.jpg" alt="推理流程图"></p>
<blockquote>
<p><strong>图5</strong>：Bi-LAT推理流程。模型根据当前从端关节数据、相机图像和语言指令，预测下一动作块的主端关节信息，随后由双边控制系统转换为各关节所需的电流。</p>
</blockquote>
<p>与现有方法（如Bi-ACT）相比，Bi-LAT的核心创新在于首次将自然语言指令作为条件信号引入双边控制模仿学习框架。这使得模型能够根据语义指令区分并生成具有不同力/扭矩特性的动作，实现了基于语言的精确力调制，而不仅仅是模仿一个平均或混合的动作模式。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了两个基准任务和实验平台。1）单臂叠杯任务：使用ROBOTIS OpenManipulator-X机械臂和三台ELP USB相机。2）双臂拧海绵任务：使用ALPHA-α双手机器人平台和四台RGB相机。<br><strong>基线方法</strong>：主要对比了未使用语言指令的Bi-ACT方法，以及使用不同语言编码器（DistilBERT, ModernBERT, CLIP, SigLIP）的Bi-LAT变体。<br><strong>训练细节</strong>：采用DABI数据增强方法，将1000Hz的机器人控制数据降采样至100Hz，使原始6条示教数据扩增至60条。模型配置为4层编码器和7层解码器。</p>
<p><strong>关键实验结果</strong>：<br>在单臂叠杯任务中，所有方法在任务成功率上均达到100%（5/5）。然而，在区分“轻柔抓握”和“用力抓握”指令的力调制精度上，不同语言编码器表现差异显著。</p>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/ex1_training_data.jpg" alt="叠杯任务训练数据分布"></p>
<blockquote>
<p><strong>图8</strong>：叠杯任务中，与“轻柔抓握”和“用力抓握”指令对应的夹爪关节角度和扭矩训练数据分布。两者呈现出清晰不同的分布区间。</p>
</blockquote>
<p>表I总结了成功率与力控精度，其中力控精度用符号表示：○（优秀）、△（尚可）、×（不佳）。</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">指令</th>
<th align="left">成功率</th>
<th align="left">力控精度</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Bi-ACT (None)</td>
<td align="left">无指令</td>
<td align="left">5/5 (100%)</td>
<td align="left">×</td>
</tr>
<tr>
<td align="left">Bi-LAT (DistilBERT)</td>
<td align="left">轻柔/用力</td>
<td align="left">5/5 (100%)</td>
<td align="left">△</td>
</tr>
<tr>
<td align="left">Bi-LAT (ModernBERT)</td>
<td align="left">轻柔/用力</td>
<td align="left">5/5 (100%)</td>
<td align="left">×</td>
</tr>
<tr>
<td align="left">Bi-LAT (CLIP)</td>
<td align="left">轻柔/用力</td>
<td align="left">5/5 (100%)</td>
<td align="left">×</td>
</tr>
<tr>
<td align="left">Bi-LAT (SigLIP)</td>
<td align="left">轻柔/用力</td>
<td align="left">5/5 (100%)</td>
<td align="left">○</td>
</tr>
</tbody></table>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/ex1_result.jpg" alt="不同方法推断结果对比"></p>
<blockquote>
<p><strong>图9</strong>：Bi-ACT（无语言指令）与Bi-LAT（使用SigLIP）在叠杯任务中的自主执行结果对比图。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/ex1_no_language.jpg" alt="不同语言编码器推断的关节数据分布"></p>
<blockquote>
<p>**图10 (a)**：Bi-ACT（无语言指令）推断出的夹爪角度和扭矩分布偏向“用力”特征，无法区分两种力模式。<br><img src="https://arxiv.org/html/2504.01301v2/fig/ex1_distilbert.jpg" alt="DistilBERT结果"><br>**图10 (b)**：使用DistilBERT的Bi-LAT能产生一定区分度，但“轻柔”指令对应的角度和扭矩高于预期。<br><img src="https://arxiv.org/html/2504.01301v2/fig/ex1_modernbert.jpg" alt="ModernBERT结果"><br>**图10 (c)**：使用ModernBERT的Bi-LAT在最大值上有区分，但整体分布区分不明显。<br><img src="https://arxiv.org/html/2504.01301v2/fig/ex1_clip.jpg" alt="CLIP结果"><br>**图10 (d)**：使用CLIP的Bi-LAT结果与ModernBERT类似，区分度有限。<br><img src="https://arxiv.org/html/2504.01301v2/fig/ex1_siglip.jpg" alt="SigLIP结果"><br>**图10 (e)**：使用SigLIP的Bi-LAT提供了最清晰的分离，推断数据分布与训练数据高度吻合。</p>
</blockquote>
<p>在双臂拧海绵任务中，专注于评估性能最佳的SigLIP编码器。如表II所示，在“轻柔拧海绵”指令下，抓取、抬起、拧转三个阶段成功率均为100%（5/5）。在“用力拧海绵”指令下，抓取和抬起成功率100%，拧转成功率为80%（4/5），一次失败是由于海绵内部阻力过大导致抓握失稳。</p>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/ex2_auto.jpg" alt="双臂拧海绵任务执行结果"></p>
<blockquote>
<p><strong>图17</strong>：使用Bi-LAT（SigLIP）自主执行“轻柔拧海绵”和“用力拧海绵”任务的结果图。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.01301v2/fig/spg_ex_all.jpg" alt="双臂任务推断的关节数据"></p>
<blockquote>
<p><strong>图19</strong>：双臂拧海绵任务中，Bi-LAT模型对“轻柔”（蓝）和“用力”（红）指令推断出的关节角度和扭矩轨迹及分布。两者空间轨迹相似，但扭矩分布差异显著，且分别与各自的训练数据分布匹配。</p>
</blockquote>
<p><strong>消融实验总结</strong>：实验实质上对比了不同的语言编码器组件。结果表明，SigLIP编码器在将语言指令映射到精确的力/扭矩分布方面表现最佳，显著优于其他编码器及无语言条件的基线方法（Bi-ACT），这凸显了引入高质量语言表征对于实现基于语言的力调制的关键贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了Bi-LAT，首个将机器人位置/力数据、视觉观察和自然语言指令相融合的双边控制模仿学习框架，实现了基于自然语言的精确力调制。2）系统评估了多种语言编码器在力控任务中的表现，发现SigLIP编码器在此场景下尤为出色。3）在单臂和需要协调力控的双臂复杂任务上验证了框架的有效性。<br>论文自身提到的局限性及未来方向包括：需要处理更多样化的物体属性（刚性和可变形材料）、纳入不同任务难度、优化实时语言交互以及探索大规模数据集以增强泛化能力。<br>本文的启示在于，通过将自然语言指令与提供丰富力觉信息的双边控制相结合，为机器人模仿学习开辟了一条通往更直观、自适应人机交互的新路径。它表明，融合高层语义指令与低层物理交互信息，是实现机器人灵巧、语境感知操作的关键。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Bi-LAT框架，解决传统模仿学习在精细力控调节上的不足。方法结合双边控制与自然语言指令，通过多模态Transformer编码关节位置、速度、扭矩及视觉语言信息，以区分如“轻拿杯子”等任务的力控要求。在叠杯和拧海绵实验中，Bi-LAT能有效复现指令力控水平，其中SigLIP语言编码器表现突出。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.01301" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>