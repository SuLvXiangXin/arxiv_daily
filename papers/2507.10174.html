<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning? - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.10174" target="_blank" rel="noreferrer">2507.10174</a></span>
        <span>作者: Keith Ross Team</span>
        <span>日期: 2025-07-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在离线强化学习领域，Transformer架构，特别是决策变换器（DT），因其将回报条件策略学习构建为序列建模任务的能力而受到广泛关注。近期，Bhargava等人（2024）对DT与传统的基于多层感知机（MLP）的离线强化学习算法（如行为克隆BC和保守Q学习CQL）进行了系统比较，并声称DT在稀疏奖励和低质量数据设置中表现出优越性能。</p>
<p>本文针对这一结论提出了质疑，认为DT在稀疏奖励环境中的优势可能被高估。本文提出了一个简单而有效的新视角：与其使用复杂的Transformer架构处理所有数据，不如先对数据集进行简单的性能过滤，然后使用轻量级的MLP进行行为克隆。本文的核心思路是：在稀疏奖励环境下，一个名为“过滤行为克隆”（FBC）的极其简单的算法，能够达到甚至超越DT的性能，同时具有更高的计算和数据效率。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文对比和提出了几种方法，核心是比较基于Transformer的DT与基于MLP的简单方法在稀疏奖励设置下的表现。</p>
<p><strong>整体流程</strong>：所有方法均在静态离线数据集上训练。对于本文提出的FBC和过滤决策变换器（FDT），其流程包含一个共同的预处理步骤：根据轨迹的最终回报对数据集进行过滤，仅保留高性能轨迹。随后，FBC在过滤后的数据集上执行标准的行为克隆（使用MLP），而FDT则在相同过滤后的数据集上运行标准的DT（使用Transformer）。</p>
<p><strong>核心模块与细节</strong>：</p>
<ol>
<li><strong>决策变换器（DT）</strong>：遵循原始设计，在推理时，DT基于过去K个时间步的状态、动作和剩余回报（Return-to-go， RTG）三元组序列来预测下一个动作。模型使用具有正弦位置编码的Transformer编码器来建模序列依赖关系。在稀疏奖励设置中，目标RTG通常设为1。</li>
<li><strong>过滤行为克隆（FBC）</strong>：算法分为两步。首先，<strong>过滤</strong>：对于原生稀疏奖励设置（如Robomimic），仅保留成功的轨迹（最终奖励为1）；对于由稠密奖励稀疏化而来的设置（如D4RL），仅保留最终回报处于数据集中<strong>前10%</strong> 的轨迹。其次，<strong>克隆</strong>：在过滤后的数据集上，使用一个简单的MLP（2层，每层512个隐藏单元，ReLU激活）执行普通的行为克隆，即通过监督学习最小化动作预测的均方误差。</li>
<li><strong>过滤决策变换器（FDT）</strong>：作为对照，该方法使用与FBC完全相同的过滤规则预处理数据，然后在过滤后的数据集上训练标准的DT模型。</li>
</ol>
<p><strong>创新点</strong>：本文的主要创新并非提出一个技术上复杂的新模型，而是通过引入极其简单的FBC，对DT在稀疏奖励环境中的必要性提出了根本性质疑。其创新性体现在<strong>思路</strong>上：指出在稀疏奖励条件下，DT通过RTG conditioning所提供的“轨迹质量信号”，完全可以通过先验的数据过滤来更直接、更高效地实现，而Transformer建模长序列依赖的能力在此类任务中可能并未带来显著优势。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在两个广泛使用的离线强化学习基准上进行：1) <strong>D4RL</strong> 运动任务（HalfCheetah, Hopper, Walker2d）的稀疏化版本（将每步奖励汇总至轨迹末端）；2) <strong>Robomimic</strong> 机器人操作任务（Lift, PickPlaceCan）的原生稀疏奖励版本（仅任务成功时在末端给予奖励）。实验严格遵循Bhargava等人（2024）的设置进行比较，对比方法包括BC、CQL、DT以及本文提出的FBC和FDT。</p>
<p><strong>关键数值结果</strong>：</p>
<ul>
<li><strong>在稀疏化D4RL的9个数据集中</strong>，FBC在7个数据集上击败了DT。FBC的总平均标准化得分为<strong>78.22</strong>，高于DT的<strong>75.39</strong>，提升约4%。有趣的是，FDT（<strong>72.90</strong>）的表现略差于DT，表明DT可能从未过滤的数据（包含低质量轨迹）中学习到了一些信息。</li>
<li><strong>在Robomimic稀疏任务中</strong>，FBC在Lift和Can任务上的平均成功率为<strong>0.89</strong>，略高于DT的<strong>0.86</strong>。DT、FBC、FDT三者性能接近，但FBC仍具优势。</li>
<li><strong>效率对比</strong>：DT的训练时间约为FBC的<strong>3倍</strong>；DT的Transformer模型约有<strong>100万</strong>参数，而FBC的MLP只有约<strong>50万</strong>参数。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.10174v1/extracted/6621287/figures/appendix_can.jpeg" alt="PickPlaceCan任务示意图"></p>
<blockquote>
<p><strong>图1</strong>：Robosuite中的PickPlaceCan任务示意图。目标是将物体放入对应的容器中。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10174v1/extracted/6621287/figures/appendix_lift.png" alt="Lift任务示意图"></p>
<blockquote>
<p><strong>图2</strong>：Robosuite中的Lift任务示意图。目标是将立方体垂直抬升到预定高度以上。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10174v1/extracted/6621287/figures/Oct5_can.png" alt="Robomimic - Can任务学习曲线"></p>
<blockquote>
<p><strong>图3</strong>：PickPlaceCan任务上FBC、FDT与DT的学习曲线对比。FBC与DT性能相当，最终收敛成功率接近，且FBC学习曲线更平滑。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10174v1/extracted/6621287/figures/Oct5_lift.png" alt="Robomimic - Lift任务学习曲线"></p>
<blockquote>
<p><strong>图4</strong>：Lift任务上FBC、FDT与DT的学习曲线对比。FBC的最终性能略优于DT，且在学习初期表现更稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10174v1/extracted/6621287/figures/walker2d_medium_comparison.png" alt="D4RL Walker2d Medium数据集学习曲线"></p>
<blockquote>
<p><strong>图5</strong>：Walker2d-Medium数据集上各方法学习曲线。FBC显著且稳定地优于DT和FDT。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10174v1/extracted/6621287/figures/walker2d_medium-expert_comparison.png" alt="D4RL Walker2d Medium-Expert数据集学习曲线"></p>
<blockquote>
<p><strong>图6</strong>：Walker2d-Medium-Expert数据集上学习曲线。FBC与DT性能均很高且接近，但FBC训练更高效。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.10174v1/extracted/6621287/figures/walker2d_medium-replay_comparison.png" alt="D4RL Walker2d Medium-Replay数据集学习曲线"></p>
<blockquote>
<p><strong>图7</strong>：Walker2d-Medium-Replay数据集上学习曲线。FBC性能明显优于DT。</p>
</blockquote>
<p>（编者注：此处省略Hopper和HalfCheetah的6张类似学习曲线图，其展示要点与Walker2d类似，均显示FBC在大多数环境中具有竞争力或更优的性能，且学习过程通常更稳定。）</p>
<p><strong>消融实验</strong>：本文通过引入FDT进行了类似消融的实验。结果表明，在D4RL基准上，FDT性能不及DT，更不及FBC。这间接说明，对于DT而言，在稀疏奖励设置下，使用全部数据（含低质量轨迹）可能比仅使用过滤后的高质量数据更有益，但这依然无法使其超越简单过滤后使用MLP的FBC。这强化了本文的核心论点：DT的架构优势在稀疏奖励任务中并不明显。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>挑战主流观点</strong>：通过系统的实验，证明了在稀疏奖励的离线强化学习环境中，一个极其简单的算法（过滤行为克隆，FBC）能够达到或超越决策变换器（DT）的性能，对DT在此类设置中的“优越性”提出了有力质疑。</li>
<li><strong>强调简单性的价值</strong>：指出在离线强化学习中，算法和架构的复杂性并不自动转化为性能提升。FBC在计算效率（更快的训练速度、更少的参数）和数据效率（使用更少的数据）方面均显著优于DT。</li>
<li><strong>引发对DT适用性的重新思考</strong>：结合先前指出DT在稠密奖励环境中也通常不占优的文献，本文促使研究者重新评估DT在原始状态机器人任务中的实际应用价值。</li>
</ol>
<p><strong>局限性</strong>：论文自身明确指出，本研究主要聚焦于<strong>稀疏奖励环境</strong>。对于稠密奖励环境，本文并未进行新的实验，而是引用了前人的工作来论证DT通常也非最佳选择。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>谨慎选择模型</strong>：在解决离线强化学习问题时，不应盲目追求基于Transformer的复杂序列模型，尤其是对于稀疏奖励任务，应优先尝试简单的基线方法（如过滤后的行为克隆）。</li>
<li><strong>改进方向</strong>：本文并非全盘否定Transformer，而是批评了“原始DT”（vanilla DT）。作者指出，那些针对决策任务结构进行专门设计的Transformer变体（如Behavior Transformer, Graph Decision Transformer, Q-Transformer）可能带来真正的收益。未来的研究应致力于此类有针对性的架构改进，而非简单套用通用序列模型。</li>
<li><strong>评估标准</strong>：除了最终性能，计算效率、数据需求、训练稳定性等也应成为评估离线强化学习算法的重要维度。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文质疑决策 Transformer（DT）在离线强化学习中的优越性，特别是针对稀疏奖励环境。通过提出一种简单的过滤行为克隆（FBC）方法——即先过滤掉低性能轨迹，再对剩余数据执行普通行为克隆——并在机器人操作（Robomimic）与运动（D4RL）任务上进行实验。结果表明，FBC 在多数稀疏奖励设置中优于 DT：在 D4RL 的 9 个数据集中，FBC 在 7 个上表现更好，整体性能提升约 4%；在 Robomimic 的两个数据集中均超越 DT。因此，论文认为 DT 并非稀疏奖励环境的优选方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.10174" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>