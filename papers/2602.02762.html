<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.02762" target="_blank" rel="noreferrer">2602.02762</a></span>
        <span>作者: Sébastien Lachapelle Team</span>
        <span>日期: 2026-02-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>行为克隆（BC）是一种通过在有动作标注的专家轨迹上进行监督学习来学习控制策略的有效技术。然而，扩展BC所需的数据集需要收集动作标注的专家演示，这在需要人类演示的领域（如机器人学）中成本高昂。因此，研究者对利用大量“野生”数据（通常以视频形式存在）产生了浓厚兴趣，这些视频通常描绘了可用于策略学习的准专家或专家行为，但不包含动作标签。利用丰富的无动作数据和一个相对较小的有动作标注数据集进行学习被称为半监督模仿学习（SSIL）。</p>
<p>目前，一些高性能的SSIL方法利用逆动力学模型（IDM）从当前和下一个观测中预测动作。IDM可以在小规模有标注数据集上训练，并用于为无动作数据生成伪标签以进行下游的BC（IDM标注），或者与在无标签数据上训练的视频模型（VM）结合形成一个策略（VM-IDM）。为了有效利用无动作数据，基于IDM的SSIL方法假设IDM比在相同数量标注数据上训练的BC具有更好的泛化能力。尽管IDM的样本效率已被实证测量，但先前的工作仅对其现象提供了部分解释，例如IDM是非因果且更简单的，或类比于基于模型的强化学习的样本效率。</p>
<p>本文针对上述痛点，旨在统一基于IDM的方法，并为其在SSIL设置中的成功提供一个更完整的解释。本文的核心思路是：论证IDM学习相对于BC的样本高效性源于真实IDM相比真实策略具有更低的复杂度和随机性，这些因素是环境特定的，为理解基于IDM的学习何时以及在何种程度上能超越BC提供了一个有价值的框架。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文首先将两种主流的基于IDM的SSIL方法——VM-IDM和IDM标注——在理论上统一起来。论文表明，在无标签数据集无限且模型容量足够的极限情况下，VM-IDM和IDM标注在最优情况下会恢复相同的策略，称之为<strong>IDM-based policy</strong>。</p>
<p>具体而言，VM-IDM策略定义为：给定状态s，先从视频模型v^(s&#39;|s)采样下一个状态s^&#39;，再从逆动力学模型h^(a|s, s^&#39;)采样动作a。这等价于从联合分布h^(a|s, s&#39;) v^(s&#39;|s)进行祖先采样，对应的策略可写为π^_{v^,h^}(a|s) = ∫ h^(a|s, s&#39;) v^(s&#39;|s) ds&#39;。当无标签数据无限时，视频模型v^会收敛到专家诱导的真实视频模型v*(s&#39;|s) = p_{π*}(s&#39;|s)。此时，VM-IDM策略变为π^_{v*,h^}。</p>
<p>另一方面，IDM标注方法使用训练好的IDM h^为无标签数据(s, s&#39;)生成伪动作标签，然后在这些新标注的数据上进行BC。论文证明，在无限无标签数据下，IDM标注方法学习到的策略π^，其最优解也恰好是π^_{v*,h^}。因此，两种方法在极限下等价于同一个IDM-based policy。</p>
<p><img src="https://arxiv.org/html/2602.02762v1/figures/maze_imgs.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：迷宫环境实验设置示意图。展示了不同复杂度的迷宫（10x10, 20x20, 50x50）以及两种状态表示格式：位置坐标（S_pos）和图像（S_img）。</p>
</blockquote>
<p>本文的核心创新点在于对<strong>IDM学习为何比BC更样本高效</strong>提供了深入且系统的解释，并归结为两个主要原因：</p>
<ol>
<li><p><strong>真实IDM的复杂度低于专家策略</strong>：论文论证，在许多环境中，表达真实IDM h*(a|s, s&#39;)所需的模型复杂度低于表达专家策略π*(a|s)所需的复杂度。这意味着学习h<em>时，可以使用一个更低复杂度的假设类H，从而在偏差-方差权衡中取得更优结果（低偏差、低方差）。相比之下，学习更复杂的π</em>需要一个更大容量的假设类Π，虽然能保证无偏，但会引入更大的方差，导致泛化能力变差。论文通过迷宫环境的理论分析和实验验证了这一点：在迷宫中，真实IDM可以根据状态差s&#39;-s线性推断出动作，因此一个简单的线性分类器（或单层CNN）即可精确表达h<em>；而专家策略π</em>需要“记忆”通往终点的整个路径，无法用简单线性模型表达。</p>
</li>
<li><p><strong>真实IDM的随机性低于专家策略</strong>：论文指出，即使策略是随机的，只要环境动态是确定性的，真实IDM h*(a|s, s&#39;)就是确定性的（给定(s, s&#39;)，只有一个可能的a）。而专家策略π*(a|s)本身可以是随机的。学习一个确定性函数通常比学习一个随机函数更容易，因为后者需要建模输出分布的不确定性。因此，IDM学习的目标函数“更简单”，收敛更快。</p>
</li>
</ol>
<p>对于第一个原因，论文进一步探讨了影响策略复杂度的两个因素：<strong>环境复杂度</strong>（如迷宫大小）和<strong>目标复杂度</strong>（如目标位置的数量）。实验表明，环境越复杂，BC与IDM-based policy的性能差距越大；目标越多，策略需要处理的情境越复杂，BC性能下降更明显，而IDM-based policy受影响较小。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在多个基准上进行了广泛的实验验证：16个ProcGen环境、机器人操作任务Push-T以及多任务长视野基准Libero。</p>
<p>对比的基线方法包括：标准行为克隆（BC）、IDM标注（IDM Labeling）、VM-IDM（使用真实VM v*或学习到的VM v^）、以及作为上限的专家策略（Expert）。此外，论文还提出了一个改进的LAPO算法（Improved LAPO），该算法受IDM学习样本高效性的启发，在潜在动作策略学习中结合了IDM。</p>
<p><img src="https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_test_acc.png" alt="环境复杂度实验"></p>
<blockquote>
<p><strong>图2</strong>：在位置状态（S_pos）格式下，不同迷宫复杂度对BC和VM-IDM（使用真实VM v<em>）测试准确率的影响。低容量（LC）IDM-based policy在数据足够时达到完美准确率，而低容量BC则不能，表明h</em>比π*更简单。在高容量模型下，IDM-based policy在低数据区域也显著优于BC。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_img_test_acc.png" alt="图像状态下的环境复杂度实验"></p>
<blockquote>
<p><strong>图3</strong>：在图像状态（S_img）格式下的类似实验。趋势与位置状态一致，IDM-based policy（尤其是低容量1L CNN）在样本效率上优于BC。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.02762v1/figures/goal_plot.png" alt="目标复杂度实验"></p>
<blockquote>
<p><strong>图4</strong>：目标数量对性能的影响。随着目标数量增加，BC（带目标条件）的性能下降，而IDM-based policy（VM_G*-IDM，仅VM目标条件化）的性能保持稳定且优异，表明IDM方法对目标变化更鲁棒。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.02762v1/figures/sochastic_plot_img.png" alt="策略随机性实验"></p>
<blockquote>
<p><strong>图5</strong>：专家策略随机性对性能的影响（图像状态）。随着专家策略随机性（用熵衡量）增加，BC的性能显著下降，而IDM-based policy的性能下降幅度小得多，验证了IDM对策略随机性的鲁棒性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.02762v1/x1.png" alt="ProcGen基准结果"></p>
<blockquote>
<p><strong>图6</strong>：在16个ProcGen环境上的平均成功率对比。IDM-based方法（IDM Labeling, VM-IDM）在大多数环境中显著优于BC，平均成功率高出约10个百分点（例如，在100个有标签轨迹下，BC约65%，IDM Labeling约75%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.02762v1/x2.png" alt="Push-T和Libero结果"></p>
<blockquote>
<p><strong>图7</strong>：在Push-T和Libero任务上的成功率。IDM-based方法（特别是IDM Labeling） consistently outperforms BC，在Libero上优势尤其明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.02762v1/x3.png" alt="消融实验：VM质量"></p>
<blockquote>
<p><strong>图8</strong>：VM质量对VM-IDM的影响。使用学习到的VM（v^）比使用真实VM（v<em>）性能有所下降，但通过使用更先进的统一视频-动作预测架构（UVA）作为VM，可以接近甚至超越v</em>-IDM的性能，这为利用大规模无标签视频数据指明了方向。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.02762v1/x4.png" alt="改进LAPO的结果"></p>
<blockquote>
<p><strong>图9</strong>：改进版LAPO算法与原始LAPO及BC在ProcGen上的对比。改进版LAPO显著优于原始LAPO，并与IDM Labeling性能相当，验证了将IDM学习的高效性整合到其他SSIL框架中的有效性。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ul>
<li>在ProcGen上，IDM Labeling和VM-IDM在平均成功率上大幅超越BC（例如，在100条有标签轨迹下，约75% vs 65%）。</li>
<li>在Push-T和Libero上，IDM-based方法同样显著优于BC。</li>
<li>消融实验证实了IDM复杂度更低、随机性更低的理论主张：环境越复杂、目标越多、专家策略越随机，IDM-based方法相对于BC的优势越大。</li>
<li>改进的LAPO算法通过整合IDM学习的优势，性能得到显著提升。</li>
<li>使用UVA架构作为VM-IDM中的视频模型，可以进一步提升性能，接近使用真实VM的理想情况。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为以下三点：</p>
<ol>
<li><strong>理论统一</strong>：证明了两种主流的基于IDM的SSIL方法（VM-IDM和IDM标注）在极限情况下等价于同一个“IDM-based policy”，为理解这类方法提供了统一视角。</li>
<li><strong>机理阐释</strong>：首次系统性地从<strong>假设类复杂度</strong>和<strong>目标函数随机性</strong>两个维度，解释了IDM学习相比BC更具样本高效性的根本原因，并通过理论分析和大量实验验证了这些主张。</li>
<li><strong>方法改进与验证</strong>：基于上述洞察，提出了改进的LAPO算法，并展示了利用先进视频模型（UVA）提升VM-IDM性能的潜力，在多个具有挑战性的基准上验证了IDM-based方法的优越性。</li>
</ol>
<p><strong>论文提到的局限性</strong>：本文的分析主要基于环境动态确定或接近确定的假设。在高度随机的环境中，真实IDM本身也可能变得随机，此时IDM学习的优势可能会减弱。此外，研究主要集中在离线学习设置。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>环境评估</strong>：本文提供的框架（分析IDM与策略的复杂度和随机性）可用于预先评估在特定任务上使用IDM-based方法是否可能带来增益。</li>
<li><strong>算法设计</strong>：鼓励在SSIL及其他相关领域（如潜在动作学习）的算法设计中，有意识地利用IDM学习的样本高效性。</li>
<li><strong>视频模型利用</strong>：结果表明，结合大规模预训练或更强大的生成式视频模型（如UVA）是提升VM-IDM策略性能的有效途径，为利用海量无标签互联网视频数据进行策略学习指明了方向。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究逆动力学模型（IDM）在半监督模仿学习（SSIL）中的样本效率问题。核心在于解释为何基于IDM的方法（如VM-IDM和IDM标注）比直接的行为克隆（BC）更高效。作者提出两个关键原因：一是真实IDM的假设空间复杂度通常低于专家策略；二是真实IDM的随机性往往小于专家策略。通过理论分析和在ProcGen等基准上的实验，论文验证了这一观点，并基于此改进了现有的LAPO算法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.02762" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>