<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.25268" target="_blank" rel="noreferrer">2510.25268</a></span>
        <span>作者: Dan Guo Team</span>
        <span>日期: 2025-10-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前手部抓取合成的研究主要集中于刚性物体，主流方法包括概率生成（回归或扩散模型）和基于物理模拟器的生成。然而，这些方法在迁移到铰接物体交互（HAOI）时面临关键局限性：基于机械手的方法缺乏人手真实感；基于骨架的方法忽略了手物接触物理；接触预测模型难以整合语言描述与铰接物体动力学；扩散模型因先验不足而在生成长序列时表现不佳；且多数方法泛化性有限，仅限于文本到静态HOI的生成。</p>
<p>本文针对铰接物体操作需要同时建模物体功能性和伴随物体变形的长时序操作序列这一具体痛点，提出了一个将离散化表示与语言模型相结合的新视角。核心思路是：利用VQ-VAE将每帧HAOI离散化为token，通过一个HAOI操作语言模型在共享表示空间中对齐抓取过程与语言描述，并引入关节感知损失确保手部抓取跟随铰接物体关节的动态变化，从而实现HAOI序列的生成、预测和插值。</p>
<h2 id="方法详解">方法详解</h2>
<p>SynHLMA的整体框架包含两个主要阶段：离散铰接操作表示学习 和 HAOI操作语言模型训练。给定一个铰接物体的完整点云，模型最终输出符合语言指令的连续手部操作序列。</p>
<p><img src="https://arxiv.org/html/2510.25268v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SynHLMA整体流程。上方蓝色虚线区域展示了离散铰接操作表示模型的训练过程。下方红色区域描绘了HAOI操作语言模型，其中离散铰接操作表示的参数在训练期间被冻结。</p>
</blockquote>
<p><strong>核心模块1: 离散铰接操作表示</strong><br>该模块使用VQ-VAE将每帧HAOI离散化为分层token序列。对于物体关节，采用单阶段VQ-VAE，预测关节参数（如旋转角度或平移量），并生成关节token ⟨j⟩。对于手部，采用多阶段VQ-VAE，分三个阶段学习：全局位姿token ⟨g⟩（负责手部全局旋转R和平移T）、局部关节token ⟨l⟩（负责手部姿态P）和细化token ⟨r⟩（生成残差偏移ΔR, ΔP, ΔT）。最终手部抓取参数由公式 𝜷̂ = (ΔR̂⋅R̂, ΔP̂⋅P̂, ΔT̂+T̂) 计算得到。</p>
<p>该模块的关键创新是引入了<strong>关节感知损失</strong> ℒ_artic，它由三部分组成：</p>
<ol>
<li><strong>HAOI穿透损失</strong> ℒ_P：惩罚预测手部网格与物体表面之间的穿透。</li>
<li><strong>姿态一致性损失</strong> ℒ_C：确保手部与物体在旋转或平移上的一致性（根据关节类型选择不同计算方式）。</li>
<li><strong>关节感知损失</strong> ℒ_J：监督VQ-VAE解码器准确预测每帧的物体关节状态。<br>总损失为 ℒ_artic = λ_P⋅ℒ_P + λ_C⋅ℒ_C + λ_J⋅ℒ_J，与VQ-VAE的重建损失ℒ_r和commit损失ℒ_c共同优化。</li>
</ol>
<p><strong>核心模块2: HAOI操作语言模型</strong><br>该模块基于预训练的Vicuna-7B大语言模型，使用LoRA技术进行微调。其输入是物体点云特征、语言指令的文本token以及（根据任务可选的）部分HAOI token序列。模型将完整的操作序列表示为token序列：⟨HO⟩⟨g1⟩⟨l1⟩⟨r1⟩⟨j1⟩⋯⟨gt⟩⟨lt⟩⟨rt⟩⟨jt⟩⟨HO⟩。训练目标是最小化下一个token预测的负对数似然损失 ℒ_NLL。训练分为两个阶段：1) <strong>多模态对齐</strong>：嵌入新的特殊token并使原始嵌入与离散操作表示对齐；2) <strong>指令微调</strong>：冻结第一阶段嵌入和分词器，优化抓取序列和语言输出的联合生成。</p>
<p><strong>与现有方法的创新点</strong></p>
<ol>
<li><strong>离散化与分层表示</strong>：受语言离散性和抓取分类学启发，设计了分离的、分层的token来编码手部位姿、调整量和物体构型，提升了生成质量和可控性。</li>
<li><strong>关节感知的显式约束</strong>：通过新提出的关节感知损失，显式地建模手与铰接物体间的物理和运动学关系，解决了分布差异问题。</li>
<li><strong>面向铰接体操作的语言模型</strong>：首次提出了针对铰接物体操作的语言模型，通过抓取token化桥接自然语言与高级动作，支持生成、预测、插值三种任务。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.25268v1/x3.png" alt="数据生成过程"></p>
<blockquote>
<p><strong>图3</strong>：合成HAOI数据生成流程。(a)抓取区域感知。(b)使用强化学习生成操作序列。(c)使用GPT-4生成语言指令。(d)大量样本展示。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在自建的<strong>HAOI-lang数据集</strong>上进行评估。该数据集基于PartNet-Mobility和ArtImage构建，使用RaiSim物理模拟器和强化学习（遵循GraspXL方法）生成超过500,000个静态抓取和50,000个操作序列，并通过GPT-4为每个交互生成多样化的自然语言描述。评估了<strong>HAOI生成</strong>（给定物体和文本描述生成完整序列）、<strong>HAOI预测</strong>（给定前20%序列预测后80%）和<strong>HAOI插值</strong>（补全中间缺失的40-50%序列）三个任务。对比的<strong>Baseline方法</strong>包括HOIGPT、Text2HOI以及代表性的人体运动生成模型T2MGPT、MotionGPT、TM2T。评估指标包括FID、多样性、MMDist、IV、ADE、FDE等。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2510.25268v1/x5.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图5</strong>：SynHLMA与HOIGPT在HAOI生成、预测和插值三个任务上的定性结果对比。SynHLMA生成的手部姿态更自然，且与物体关节状态的配合更协调。</p>
</blockquote>
<p>表1数据显示，在HAOI生成任务上，SynHLMA的FID指标（越低越好）为14.121，相比最佳基线HOIGPT（19.040）提升了4.919%；多样性指标（越接近真实越好）为40.484，相比HOIGPT（26.498）提升了13.986%。其他指标（MMDist, IV, ADE, FDE）也全面优于基线。</p>
<p>表2数据显示，在HAOI预测任务上，SynHLMA的FID（21.739）相比HOIGPT（36.379）提升了14.64%，多样性（48.691）提升了19.572%。在HAOI插值任务上，FID（25.225）相比HOIGPT（34.956）提升了9.731%，多样性（44.012）提升了19.969%。</p>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>Token语义消融</strong>（表3）：移除物体关节token ⟨j⟩会导致性能显著下降并使细化token失效；增加更多细化token带来收益微小但增加复杂度；移除所有语义或共享单一码本均会损害性能。证明了分层语义化token设计的有效性。</li>
<li><strong>VQ-VAE设计消融</strong>（表4）：增大码本容量（从512到2048）和嵌入维度（从256到1024）能保留更丰富的表示并持续改善指标。实验发现对于HAOI生成，仅使用码本重置策略比结合EMA效果更好。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>构建了首个包含详细语言描述的铰接物体抓取操作数据集HAOI-Lang。</li>
<li>提出了使用分层抓取token对操作轨迹进行离散化的方法，提升了生成质量和可控性。</li>
<li>设计了关节感知损失，通过一组互补约束改善了模型对铰接物体变化的响应能力。</li>
<li>提出了首个针对铰接物体操作的语言模型，通过抓取token化桥接语言与动作，支持生成、预测、插值多种任务。</li>
</ol>
<p><strong>局限性</strong>：论文自身提及的局限性包括：1) 数据集规模虽大但仍有限；2) 模拟生成的数据与真实世界数据间存在差距；3) 方法依赖于准确的物体点云输入。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>离散表示范式</strong>：将连续、复杂的物理交互过程离散化为token序列，并与大语言模型结合，为具身智能中的长时序、组合性任务规划提供了新思路。</li>
<li><strong>物理约束的显式建模</strong>：关节感知损失表明，在生成模型中显式地加入物理和运动学先验，对于生成物理合理的结果至关重要，这可以推广到其他需要与动态环境交互的任务中。</li>
<li><strong>模拟到真实的迁移</strong>：该方法展示了利用大规模物理模拟数据训练模型的潜力，如何弥合模拟与真实的鸿沟是后续应用的关键。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SynHLMA框架，旨在解决关节物体上基于语言指令的手部操控序列生成问题，克服现有方法在整合语言描述与物体动态、长序列生成等方面的不足。关键技术采用离散HAOI表示，利用VQ-VAE对每帧交互进行离散编码，并通过HAOI Manipulation Language Model在共享表示空间中对齐抓取过程与语言描述，结合关节感知损失确保手部动作跟随物体关节变化。在自建HAOI-lang数据集上的实验表明，该方法在手部抓取序列生成性能上优于现有先进方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.25268" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>