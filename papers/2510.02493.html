<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Beyond Imitation: Recovering Dense Rewards from Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Beyond Imitation: Recovering Dense Rewards from Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.02493" target="_blank" rel="noreferrer">2510.02493</a></span>
        <span>作者: Gholamreza Haffari Team</span>
        <span>日期: 2025-10-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，大型语言模型（LLMs）的后训练通常依赖于专家演示数据，其实践几乎完全通过监督微调（SFT）实现，即通过教师强制最大似然来匹配专家令牌序列。因此，SFT通常被框定为一种模仿学习，其目标仅仅是模仿专家行为。本文挑战了这一观点，认为这种“纯模仿”的视角是不完整的。本文指出，在标准的令牌级生成假设下，SFT可以通过逆强化学习（IRL）的视角获得精确的解释。具体而言，本文证明了在无折扣的令牌马尔可夫决策过程（MDP）上，令牌级的SFT目标等价于逆软Q学习（IQ-Learn）简化目标的优化。这意味着SFT不仅仅是学习一个策略，它还在隐式地学习一个密集的、令牌级的奖励模型，该模型能够解释专家演示。本文的核心思路是：首先建立SFT与IQ-Learn的等价性，将SFT重新定义为隐式的密集奖励学习；然后，通过制定一个基于基线的相对奖励函数，直接从SFT模型中恢复出这种密集奖励信号，并利用该奖励通过强化学习进一步改进策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程分为三个核心步骤（S1-S3），并最终实例化为一个具体的强化学习算法。</p>
<p><strong>步骤1（S1）：证明SFT等价于IQ-Learn的一个特例。</strong> 在令牌MDP（状态为提示加已生成令牌，动作为下一个令牌，转移为确定性拼接）且折扣率γ=1的设定下，本文证明了最大化IQ-Learn的简化目标J*(Q)等价于最大化专家轨迹上的教师强制对数似然。关键推导利用了确定性序列中软优势项Q(s,a)-V(f(s,a))的伸缩求和性质，以及对数策略恒等式log π_Q(a|s) = Q(s,a) - V(s)。这表明SFT的logits可以直接被视为Q函数，SFT过程本质上是在进行逆Q学习。</p>
<p><strong>步骤2（S2）：证明奖励估计误差受策略误差控制（双重收缩性质）。</strong> 在凸解析IRL框架下，本文证明了若奖励正则项ψ是μ强凸的，则对于任何策略π，其诱导的最佳响应奖励估计误差‖r̂(π) - r<em>‖以（1/μ）倍的策略占用度量误差‖ρ_π - ρ_E‖</em>为上界。这意味着在IRL鞍点附近，学习奖励比学习策略本身更稳定，这为从SFT策略中恢复稳定奖励提供了理论依据。</p>
<p><strong>步骤3（S3）：从SFT模型提取密集奖励并用于策略改进。</strong></p>
<ol>
<li><strong>奖励构建</strong>：根据软最优性恒等式，SFT策略的对数概率log π_SFT(a_t|s_t)可以分解为任务奖励r(s_t, a_t)加上一个势函数差值V_SFT(s_{t+1}) - V_SFT(s_t)。根据基于势的塑形理论，两者诱导的最优策略相同。因此，可以直接使用log π_SFT作为密集的令牌奖励，而无需显式估计值函数V。</li>
<li><strong>基线相对奖励</strong>：直接最大化Σ_t log π_SFT(a_t|s_t)会偏向于生成较短的序列（因为令牌对数概率非正）。为了解决这个问题并稳定信用分配，本文提出了基线相对奖励：r̂(s,a) = log π_SFT(a|s) - log π_ref(a|s)。其中π_ref是SFT训练过程中的一个检查点模型（例如训练到一半时的模型）。该设计抵消了长度偏差，衡量了SFT训练期间获得的增量能力，并经验上降低了方差。</li>
<li><strong>策略优化</strong>：本文采用无折扣（γ=1）的令牌级REINFORCE算法进行策略优化。选择REINFORCE而非PPO等带评论家算法，是为了避免拟合由势函数V_SFT引入的、依赖于状态的异方差回报目标所带来的困难。理论分析表明，使用奖励log π_SFT的策略梯度与使用真实奖励r的策略梯度仅相差一个基线b_t(s_t)=V_SFT(s_t)。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.02493v1/figures/vis1.png" alt="信用分配示例"></p>
<blockquote>
<p><strong>图1</strong>：Dense-Path REINFORCE中的信用分配可视化（建议彩色查看）。左右两侧分别展示了对同一数学问题的正确和错误回答。每个令牌的颜色根据公式(5)计算的基线相对密集奖励着色（红色越深表示奖励越高）。可以看到，模型正确地识别了错误答案中导致计算错误的数字“2”（奖励较低），而正确答案的最终结果获得了较高奖励，表明该方法具备令牌级的信用分配能力。</p>
</blockquote>
<p><strong>算法流程</strong>：算法“Dense-Path REINFORCE”首先在专家数据集上进行SFT，得到最终策略π_SFT和中间检查点策略π_ref。然后，初始化待优化策略π_φ为π_SFT，并冻结π_SFT和π_ref。在每次迭代中，使用π_φ采样生成轨迹，为每个令牌计算基线相对奖励r̂_t，并计算从该令牌到序列结束的累积回报G_t。最终优化目标是最大化期望令牌对数概率与回报G_t的加权和，即执行REINFORCE更新。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：使用Open-Orca的10万条（提示，演示）对进行SFT和RL训练，RL阶段使用相同的提示池。</li>
<li><strong>骨干模型</strong>：使用四个仅经过预训练（未经过指令微调）的基础模型：LLaMA‑3.1‑8B、Qwen‑2.5‑7B、Mistral‑7B‑v0.1和Gemma‑3‑4B。</li>
<li><strong>基线方法</strong>：SFT、SPIN、GSIL、SR（句子级REINFORCE，仅在序列结束时分配稀疏奖励）。</li>
<li><strong>评估基准</strong>：AlpacaEval、Arena‑Hard、LIMA prompts（报告相对于SFT模型的GPT‑4o胜率）以及MT‑Bench（报告1-10分得分）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>表1展示了在四个骨干模型和四个评估基准上的主要结果。本文方法（DPR）在几乎所有情况下都一致地超越了原始的SFT模型。例如，在LLaMA-3.1-8B上，DPR在AlpacaEval、Arena-Hard和LIMA上的胜率分别达到60.6%、62.5%和62.7%，MT-Bench得分为6.01，均优于SFT及SPIN、GSIL等LfD基线。与稀疏奖励版本（SR）相比，DPR也表现出了系统性优势，特别是在Mistral-7B-v0.1上差距显著，这验证了密集令牌级信用分配的有效性。</p>
<p><strong>消融实验</strong>：<br>表2展示了关于奖励塑形和基线的消融研究结果。</p>
<ul>
<li><strong>“w/V”变体</strong>：不消除势函数V项（即使用原始奖励r(s_t,a_t) = log π_SFT(a_t|s_t) + (V_SFT(s_t) - V_SFT(s_{t+1}))）。该变体在所有模型和指标上均持续弱于完整的DPR方法，性能下降约2-7个胜率点，证实了V项会引入噪声和位置依赖的回报偏移，不利于稳定训练。</li>
<li><strong>“wo/Baseline”变体</strong>：移除SFT检查点基线（仅使用log π_SFT作为奖励）。该变体性能大幅下降，通常下降10-15个胜率点，这与理论分析的EOS病理和长度偏差相符，凸显了基线校正的必要性。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.02493v1/figures/discount_rate.png" alt="折扣率敏感性分析"></p>
<blockquote>
<p><strong>图2</strong>：奖励折扣率（γ从0.93到1.00）对四个骨干模型性能（相对于SFT的胜率）的影响。性能在未折扣设置（γ=1.0）时达到峰值。这与本文的理论分析一致：SFT与IQ-Learn的等价性在γ=1时推导；若使用折扣，早期令牌会相对于后期令牌被过度奖励，从而削弱令牌级信用分配。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>理论重构</strong>：首次在令牌MDP上建立了令牌级SFT目标与逆软Q学习（IQ-Learn）简化目标的等价性，将SFT重新定义为一种隐式的密集奖励学习机制，而非纯粹的模仿学习。</li>
<li><strong>稳定性证明</strong>：证明了在IRL鞍点附近，奖励估计误差以策略占用度量误差为上界，为从SFT策略中恢复稳定奖励提供了理论保证。</li>
<li><strong>方法实现</strong>：提出了一种通过基于势的塑形和基线选择，从SFT模型中构建有意义的令牌级奖励的实用方法，并实例化为一个简单的、基于REINFORCE的强化学习算法（Dense-Path REINFORCE），该算法在多个指令跟随基准上持续提升SFT模型性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，该方法严格限定在“从演示中学习”（LfD）的设置中，即仅使用专家演示数据，未涉及基于人类或AI偏好的数据（如RLHF/DPO）。</p>
<p><strong>启示</strong>：这项工作为理解和利用SFT开辟了新视角。它表明，即使在简单的监督微调中，模型也内在地学习了对生成内容进行细粒度评估的能力。这启发后续研究可以进一步探索如何更有效地提取和利用LLMs内部隐含的奖励信号，例如用于样本筛选、课程学习、或作为其他优化算法的辅助信号，甚至在缺乏显式奖励模型的场景下实现更精细的策略优化。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文挑战了将监督微调（SFT）视为简单模仿学习的传统观点。核心问题是证明SFT本质上等同于逆强化学习，不仅能学习策略，还能隐式学习一个解释专家演示的密集令牌级奖励模型。关键技术方法是基于逆Q学习框架，通过基线相对奖励函数从SFT模型中恢复密集奖励信号，并利用该奖励通过强化学习进一步优化策略（Dense-Path REINFORCE）。实验表明，该方法在指令遵循基准测试中持续优于原始SFT模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.02493" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>