<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.18463" target="_blank" rel="noreferrer">2509.18463</a></span>
        <span>作者: Luka Peternel Team</span>
        <span>日期: 2025-09-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，强化学习（RL）已被广泛应用于机器人技能学习，但通常需要针对特定任务精心设计奖励函数，这导致学得的策略虽然在该任务上表现良好，却缺乏泛化能力。学习新任务通常耗时且样本效率低下。一种提升效率和泛化性的方法是利用先验知识，例如来自模型或人类演示的粗略策略，但这些针对特定任务非最优的策略往往被丢弃。先前研究观察到在与人类物理交互的不可预测环境中，机器人策略会发生被动变异，产生可能对其他任务有用的技能变体，但尚缺乏主动、系统性的变异机制。</p>
<p>本文针对“如何主动、系统地生成多样化的机器人技能，而非仅仅优化单一任务”这一痛点，提出了通过主动变异奖励函数来诱导技能多样化的新视角。其核心思路是：受进化论启发，将奖励函数视为可调机制，对其关键权重施加受控的高斯噪声，从而在强化学习过程中探索出既包含原任务变体、也可能适用于意外新任务的多样化策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法框架旨在通过主动变异奖励函数的权重，在强化学习过程中实现技能多样化。整体流程基于近端策略优化（PPO）算法，并集成了一个自定义的奖励变异机制。</p>
<p><img src="https://arxiv.org/html/2509.18463v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提出的基于奖励函数变异的技能多样化概念，以液体倾倒任务为例。左侧展示了通过高斯噪声分布对奖励项（准确性、时间、努力）施加变异，这些变异的奖励函数用于训练PPO策略。右侧展示了产生的多样化技能变体，包括快/慢倾倒、边缘清洁、混合和浇水等行为。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.18463v1/x2.png" alt="PPO-MDP交互框架"></p>
<blockquote>
<p><strong>图2</strong>：PPO-MDP交互框架。PPO智能体（蓝色块）与由马尔可夫决策过程（MDP，绿色）建模的环境交互，包括状态转移、动作和观察。额外的自定义奖励机制（橙色）集成了本研究的关键项（准确性、努力和时间），其权重被系统性地改变以诱导策略变异。</p>
</blockquote>
<p><strong>奖励函数设计</strong>：奖励函数的设计灵感来源于人类运动控制的成本效益权衡模型。具体公式如下：<br>$R(s,a)=\underbrace{e^{-\frac{t}{w_{t}}}}<em>{\text{Time}}\cdot\underbrace{w</em>{a}A}<em>{\text{Accuracy}}-\underbrace{w</em>{e}E}_{\text{Effort}}$<br>其中，$A$ 代表准确性，在倾倒任务中由容器内液体质量衡量，权重为 $w_a$；$t$ 是任务已花费的时间，权重 $w_t$ 以指数形式折减奖励，鼓励快速执行；$E$ 是努力成本，通过测量的关节扭矩来惩罚能量消耗，权重为 $w_e$，该项也有助于生成平滑轨迹。</p>
<p><strong>主动变异机制</strong>：为了主动实现技能多样化，该方法对奖励权重施加受控变异。从一个能产生良好倾倒策略的基线权重配置出发，对时间权重 $w_t$ 和努力权重 $w_e$ 施加均值为0、标准差为 $\sigma$ 的高斯噪声 $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$，从而得到变异后的权重 $w_i&#39; = w_i + \epsilon_i$。准确性权重 $w_a$ 保持不变，以确保智能体始终有一个基本目标。</p>
<p><img src="https://arxiv.org/html/2509.18463v1/x3.png" alt="奖励权重采样"></p>
<blockquote>
<p><strong>图3</strong>：用于系统性分析的奖励权重在高斯分布上的采样。该图描绘了从以基线权重为中心的高斯分布中采样的奖励权重配置（x轴）的概率密度（y轴）。实验测试了每个变异项（如时间或努力）的五个权重配置，以探究奖励函数的敏感性。</p>
</blockquote>
<p><strong>系统性探索策略</strong>：为了系统性地分析变异的影响，研究者并未完全随机采样，而是为每个待变异的权重（$w_t$, $w_e$）在高斯分布曲线上选择了五个代表值：一个基线值，以及基线上下各两个值（见图3）。这构成了一个5x5的网格，共25种独特的权重配置组合，用于全面探索奖励变异空间。</p>
<p><strong>任务与训练设置</strong>：任务在NVIDIA Isaac Sim的Isaac Lab环境中构建，使用Franka Emika Panda机械臂执行液体倾倒。机器人动作定义为关节扭矩指令。对于25种权重配置中的每一种，都训练了三个策略以考虑学习随机性，每个训练好的策略再进行10次模拟评估，总计75个策略和750次模拟。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在NVIDIA Isaac Sim仿真环境中进行，使用Franka Emika Panda机械臂执行液体倾倒任务。基准方法是标准的PPO算法，而本研究的对比基线实质上是不同奖励权重配置下产生的不同策略。研究系统地测试了25种不同的奖励权重配置（时间权重和努力权重的5x5组合）。</p>
<p><img src="https://arxiv.org/html/2509.18463v1/x4.png" alt="技能分类结果"></p>
<blockquote>
<p><strong>图4</strong>：不同奖励权重对组合产生的技能分类结果。努力权重与时间权重的组合（x/y轴）被映射到其产生的技能标签：倾倒（慢/基准/快）、无策略、混合和浇水。点按行为类别着色：蓝色（达成原始目标），绿色（产生新技能但错过原始目标），红色（无可行策略）。</p>
</blockquote>
<p>关键实验结果总结如下：在25种测试的配置中，<strong>16种策略</strong>成功完成了原始倾倒任务，并展现出执行速度上的变异（慢、基准、快）。<strong>4种策略</strong>产生了明确可用于新任务的技能，包括边缘清洁、液体混合和浇水。<strong>5种策略</strong>未能与任何可立即识别的任务关联（标记为“无策略”）。图4直观地展示了不同权重配置与产出技能类型的映射关系。</p>
<p><img src="https://arxiv.org/html/2509.18463v1/x5.png" alt="快速倾倒技能"></p>
<blockquote>
<p><strong>图5</strong>：快速倾倒技能变体。故事板序列展示了快速倾倒的初始、中间和最终阶段。图表显示了末端执行器的位置、方向以及容器秤测得的力量（与液体转移量成正比）。该策略在约5.5秒内完成任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.18463v1/x6.png" alt="边缘清洁技能"></p>
<blockquote>
<p><strong>图6</strong>：边缘清洁技能变体。故事板显示机器人将液体倾倒在容器边缘。位置和力量图表表明该行为偏离了原始任务目标，但展示了针对容器边缘的交互，可用于清洁。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.18463v1/x7.png" alt="液体混合技能"></p>
<blockquote>
<p><strong>图7</strong>：液体混合技能变体。故事板和图表的振荡表明机器人在倾倒过程中加入了摇晃动作。力量图表呈现阶梯式增长，表明液体是脉冲式流入的，此行为可用于混合液体。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.18463v1/x8.png" alt="浇水技能"></p>
<blockquote>
<p><strong>图8</strong>：浇水技能变体。故事板显示机器人执行了扫洒动作。方向图表显示了协调的扭转运动，导致液体被广泛泼洒。力量读数较低，表明大部分液体未进入容器，但该行为类似于浇水或冲洗表面。</p>
</blockquote>
<p><strong>消融实验分析</strong>：本研究通过系统性地改变奖励函数中不同项的权重（可视为一种结构化的消融实验），揭示了各组件对最终策略行为的贡献。结果表明：1）<strong>准确性权重 ($w_a$)</strong> 是维持任务基本目标的锚点，当其主导时策略倾向于完成倾倒。2）<strong>时间权重 ($w_t$)</strong> 的变异主要影响任务执行速度，产生快或慢的倾倒变体。3）<strong>努力权重 ($w_e$)</strong> 的变异与运动的经济性和平滑性相关，其极端变化（过高或过低）更容易导致策略偏离原始任务，并可能催生如混合（低努力惩罚导致振荡）或浇水（异常运动模式）等新技能。权重组合的协同效应是产生多样化技能的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一个<strong>主动奖励函数变异框架</strong>，通过向奖励权重注入高斯噪声，系统性地诱导强化学习策略多样化，超越了传统的动作空间探索。2）在液体倾倒任务中实证表明，该方法不仅能产生原始任务的变体（如快/慢执行），还能<strong>自主衍生出适用于意外新任务的技能</strong>，如边缘清洁、液体混合和浇水。3）将技能产出分为“原始任务”、“潜在新任务”和“无用”三类，并讨论了在技能发现与学习时间之间的权衡。</p>
<p>论文提到的局限性包括：对新兴技能的<strong>标注依赖人工</strong>，未来需要开发自动标注方法（如结合视觉语言模型）；实验目前<strong>仅限于仿真环境</strong>；技能变体的产出比例可能因任务而异。</p>
<p>这项工作对后续研究的重要启示是：将<strong>奖励函数本身作为探索与多样化的对象</strong>是一个富有前景的方向。未来可探索结合多目标强化学习来平衡竞争目标，或引入人在环反馈来引导技能朝向更有用的方向发展。将框架扩展到其他灵巧操作任务（如搅拌、舀取），并最终迁移到物理机器人，是验证和推广该方法的关键步骤。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究如何通过主动变异强化学习的奖励函数，使机器人在执行液体倾倒任务时获得多样化的技能变体。核心方法是提出一个奖励函数主动变异框架，对基于准确性、时间和努力的奖励项权重施加高斯噪声，并使用PPO算法进行训练。实验在仿真环境中进行，结果表明，该方法能产生丰富的策略行为，不仅包括快/慢倾倒等原始任务变体，还衍生出容器边缘清洁、液体混合和浇水等可用于意外任务的新技能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.18463" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>