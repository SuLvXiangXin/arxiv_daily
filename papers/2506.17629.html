<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.17629" target="_blank" rel="noreferrer">2506.17629</a></span>
        <span>作者: Li, Kailing, Xu, Qi&#39;ao, Qian, Tianwen, Fu, Yuqian, Jiao, Yang, Wang, Xiaoling</span>
        <span>日期: 2025/06/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>具身视觉推理（EVR）旨在基于第一人称视角视频，遵循复杂的自由形式指令进行语义理解和时空推理。当前主流方法分为两类：一是基于苏格拉底式的策略，先通过视频描述模型将视频内容转化为静态文本描述，再由大语言模型（LLM）进行推理，这种方法虽能利用LLM强大的推理能力，但其生成的描述是固定的、与指令无关的，往往会遗漏细粒度、上下文敏感的视觉细节。二是依赖端到端视觉语言模型（VLM）的方法，其在特征层面融合视觉和语言模态，具备强大的开放词汇感知能力，但当前VLM在高级逻辑规划和语义推理方面存在不足，难以以连贯、任务驱动的方式组织必要的推理步骤（如事件定位、物体识别和关系提取），从而未能充分发挥其感知优势。</p>
<p>本文针对上述方法在“感知”与“推理”能力上的割裂与不平衡这一具体痛点，提出了结合LLM与VLM优势的新视角。其核心思路是：提出一个无需训练的框架CLiViS，让LLM担任高级规划器，根据指令和累积的场景认知为VLM分配一系列感知子任务，并通过VLM驱动的开放世界视觉感知迭代更新场景上下文；在此协同基础上，构建一个贯穿推理过程演化的动态认知地图，作为连接低级感知与高级推理的结构化场景表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>CLiViS的整体框架是一个由LLM与VLM协同驱动的迭代推理流程，其推理过程分为三个阶段。</p>
<p><img src="https://arxiv.org/html/2506.17629v1/x1.png" alt="整体框架"></p>
<blockquote>
<p><strong>图2</strong>：CLiViS整体框架。推理包含三个步骤：(1) 从分段的场景描述初始化认知地图和证据记忆；(2) 通过LLM-VLM交互迭代更新任务相关的视觉线索；(3) 整合上下文生成最终答案。</p>
</blockquote>
<ol>
<li><strong>认知与记忆初始化</strong>：给定第一人称视频V和语言指令I，首先将视频分割为固定长度的片段（如30秒）。对于每个片段，调用VLM生成粗略的视觉描述。然后由LLM分析这些描述，提取关键实体（如物体、区域、人）、动作和实体间关系。结合指令I，LLM进一步识别并突出与问题最相关的实体。基于这些信息，构建初始认知地图和证据记忆模块。</li>
<li><strong>语言-视觉协同与认知更新</strong>：在一个迭代推理循环中，LLM整合当前认知地图和证据记忆，评估是否已收集足够信息来回答问题。若不足，则LLM生成一个针对特定时间片段的子指令（如“检查冰箱里山楂汁左边是什么”）来引导VLM执行聚焦感知。随后，解析VLM的响应以提取关键实体、关系和支持性依据，并用它们更新认知地图和证据记忆。</li>
<li><strong>整合推理与答案生成</strong>：一旦LLM判定已收集足够信息，或推理达到预设的最大迭代次数，LLM将生成最终答案。</li>
</ol>
<p>该框架的核心是<strong>动态认知地图</strong>和<strong>证据记忆</strong>。</p>
<p><img src="https://arxiv.org/html/2506.17629v1/x2.png" alt="认知地图"></p>
<blockquote>
<p><strong>图3</strong>：认知地图由导航图和关系图组成。前者捕捉时间区域及相关实体，后者记录实体间的细粒度关系。</p>
</blockquote>
<ul>
<li><strong>认知地图</strong>：它是一个基于图的表示，由两个子图构成（公式5）：<ul>
<li><strong>导航图</strong>：捕获视频的时间结构。每个节点代表一个独立的时间片段，边表示片段间的时间相邻关系。节点记录该片段涉及的区域、实体、动作及其描述。</li>
<li><strong>关系图</strong>：建模细粒度的实体级关系。节点代表视觉实体或动作，边表示节点间的语义关系，如空间关系、智能体-物体交互、功能依赖等。</li>
</ul>
</li>
<li><strong>证据记忆</strong>：作为认知地图的补充，它是一个轻量级缓冲区，用于积累与指令相关的、从LLM-VLM交互中推导出的高级语义线索（推理依据）。一个证据原子定义为 ℰ = (r, τ, O)，其中r是关于查询的语言依据，τ是相关时间跨度，O是涉及的对象、区域或动作集合（公式6）。</li>
</ul>
<p><strong>更新机制</strong>：认知地图ℳ和证据记忆ℰ并非静态。初始状态ℳ^(0)和ℰ^(0)在初始化阶段构建。在后续的每次推理迭代i中，LLM基于指令I和先前状态，将任务分解为一组子任务Ti，引导VLM感知对应的视频片段V_Ti以提取任务相关的视觉证据（公式7、8）。<code>Update(·)</code>操作符将新信息整合到现有图中，可能包括：添加新观察到的实体；基于新推断的交互插入或细化关系边；基于累积的证据细化节点属性。这使得认知地图能够逐步演化，为最终答案推理提供结构化且最新的基础。</p>
<p><strong>LLM-VLM协同提示</strong>：两者间的协作通过精心设计的提示词来协调。如图4所示，这些提示词贯穿了推理流程的关键阶段：VLM生成分段场景描述、LLM初始化认知与记忆、LLM生成子指令、以及LLM整合VLM输出以更新地图和记忆。</p>
<p><img src="https://arxiv.org/html/2506.17629v1/x3.png" alt="提示设计"></p>
<blockquote>
<p><strong>图4</strong>：用于LLM-VLM协同的提示设计示例，展示了推理流程中关键阶段的提示。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法相比，CLiViS的创新具体体现在：1) <strong>协同设计</strong>：创造性地将LLM作为规划器、VLM作为感知器进行迭代式协同，而非简单的串行（如Socratic方法）或单一模型（如端到端VLM）。2) <strong>动态结构化记忆</strong>：引入了由双图构成的认知地图和证据记忆，为长时序、多实体的具身场景提供了可迭代更新的结构化表示，有效桥接了感知与推理。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准数据集</strong>：在三个真实世界第一人称视频问答基准上测试：OpenEQA (1079个QA对)、EgoSchema (500个多项选择题)、EgoTempo (500个QA对，专注于时序推理)。</li>
<li><strong>评估指标</strong>：OpenEQA和EgoTempo采用开放式评估，使用Qwen2.5-Max以5点李克特量表对模型回答评分，≥4分视为正确；EgoSchema报告多项选择准确率。视频按时长（&lt;30秒和≥30秒）分组分析。</li>
<li><strong>对比方法</strong>：与三类方法对比：1) <strong>Socratic-based模型</strong>：VLM（Qwen2.5-VL, InternVL3, VideoLLaMA3） + LLM（Qwen2.5-Max, DeepSeek-V3）的两阶段流水线。2) <strong>端到端VLM模型</strong>：Qwen2-VL, Qwen2.5-VL, InternVL2.5, InternVL3, VideoLLaMA3。3) <strong>视频推理模型</strong>：VideoAgent, VideoTree, LVNet, Video-R1。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.17629v1/x4.png" alt="主要结果"></p>
<blockquote>
<p><strong>图5</strong>：在不同基准上与多种方法的性能对比。CLiViS在多个设置下取得了最佳或极具竞争力的结果，尤其在处理长视频（≥30s）任务时优势明显。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>根据表1（图5）的数据，CLiViS展现了强大的有效性和泛化性：</p>
<ul>
<li><strong>整体性能</strong>：在OpenEQA上，CLiViS (VideoLLaMA3)达到57.3%准确率，优于所有对比方法；在EgoTempo上，CLiViS (InternVL3)以23.0%准确率领先；在EgoSchema上，CLiViS (InternVL3)以69.4%准确率取得最佳。</li>
<li><strong>处理长视频依赖</strong>：CLiViS在处理长视频（≥30秒）任务时优势尤为突出。例如在OpenEQA长视频组，CLiViS (InternVL3)达到55.9%，显著优于最佳端到端VLM VideoLLaMA3的57.0%和最佳Socratic方法（19.9%）。在EgoTempo长视频组，CLiViS (InternVL3)的19.4%也高于其他方法。</li>
<li><strong>泛化性</strong>：CLiViS框架能与不同的VLM（Qwen2.5-VL, InternVL3, VideoLLaMA3）有效结合，均带来性能提升，证明了其作为通用框架的潜力。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.17629v1/extracted/6559589/Figures/duration_rounds_accuracy.png" alt="推理轮次与准确率"></p>
<blockquote>
<p><strong>图6</strong>：在EgoTempo上，模型准确率随视频时长和CLiViS推理轮次的变化。随着视频变长，CLiViS通过更多轮次的迭代推理维持了较高的准确率，而基线方法性能显著下降。</p>
</blockquote>
<p><strong>消融实验与组件贡献</strong>：<br>论文通过消融实验验证了核心组件的贡献。移除认知地图或证据记忆均会导致性能下降，证明了二者在结构化信息存储和保留推理依据方面的必要性。迭代更新机制相比单轮推理带来了显著提升，尤其是在复杂、长时序任务上，表明动态细化场景认知对准确推理至关重要。</p>
<p><strong>定性分析</strong>：<br><img src="https://arxiv.org/html/2506.17629v1/extracted/6559589/Figures/case/case_1.png" alt="定性案例1"></p>
<blockquote>
<p><strong>图7</strong>：案例1展示了CLiViS通过多轮交互（定位冰箱→识别内部物体→确定空间关系）成功解答复杂空间推理问题，而基线方法失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.17629v1/extracted/6559589/Figures/case/case_2.png" alt="定性案例2"></p>
<blockquote>
<p><strong>图8</strong>：案例2展示了CLiViS通过构建和查询认知地图中的时序关系（“after”），成功回答了涉及动作先后顺序的问题。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.17629v1/extracted/6559589/Figures/case/case_3.png" alt="定性案例3"></p>
<blockquote>
<p><strong>图9</strong>：案例3展示了CLiViS如何通过迭代感知（识别物体→判断状态变化）解答涉及状态变化的复杂问题。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个无需训练的<strong>新颖框架CLiViS</strong>，通过LLM与VLM的协同，有效应对了具身视觉推理中感知与推理的双重挑战。</li>
<li>设计了一个<strong>动态认知地图</strong>，为具身场景提供结构化的语义表示，该地图在推理过程中不断演化，充当了LLM与VLM协作的桥梁。</li>
<li>在多个第一人称基准和不同基础模型上进行了广泛实验，证明了CLiViS的<strong>有效性和泛化性</strong>，特别是在处理长时序视觉依赖任务上的优势。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1) <strong>计算开销</strong>：迭代调用VLM和LLM会带来更高的计算成本。2) <strong>依赖基础模型性能</strong>：框架的性能受限于所选LLM和VLM的能力上限。3) <strong>提示工程敏感性</strong>：方法的有效性在一定程度上依赖于精心设计的提示词。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>模块化协同范式</strong>：CLiViS展示了将大型基础模型作为具有特定功能（规划、感知）的模块进行组合的潜力，这为构建更复杂、鲁棒的智能体系统提供了新思路。</li>
<li><strong>结构化场景表示</strong>：动态认知地图作为一种可解释、可操作的中介表示，不仅有助于推理，未来可能直接用于指导具身动作规划或长期记忆管理。</li>
<li><strong>高效交互策略</strong>：如何减少不必要的模型调用轮次、设计更高效的子任务规划策略以及压缩记忆表示，是降低该范式计算成本、推动其实际应用的关键方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身视觉推理中复杂指令与长视频时空动态的挑战，提出CLiViS框架。其核心是协同LLM的任务规划能力与VLM的开放世界感知能力，通过迭代更新一个动态的**认知地图**来结构化表示场景，从而桥接感知与推理。该无需训练的框架在多个基准测试中验证了有效性，特别擅长处理长期视觉依赖。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.17629" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>