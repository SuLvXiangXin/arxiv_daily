<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.07961" target="_blank" rel="noreferrer">2506.07961</a></span>
        <span>作者: Tieniu Tan Team</span>
        <span>日期: 2025-06-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用预训练的视觉语言模型构建视觉-语言-动作模型已成为学习机器人操作策略的一种有效方法。然而，大多数VLA模型仅使用2D图像作为输入，需要大量的数据收集工作。另一方面，3D机器人策略在模型设计中利用了3D结构先验，在学习复杂的3D操作任务时表现出卓越的样本效率。本文旨在结合VLA模型的有效性与3D策略的效率，构建一个统一的3D VLA模型。</p>
<p>现有的一些工作尝试将3D信息整合到VLMs中，但它们通常将动作转换为没有空间结构的token序列，并使用下一个token预测来预测动作。这种策略未能利用3D结构先验，导致样本效率低下。此外，开发3D VLA模型的另一个重大挑战在于动作微调中使用的3D输入与原始VLM预训练中使用的2D图像输入之间存在错配，导致了与原始VLM预训练的巨大分布偏移。</p>
<p>本文针对上述挑战，提出了BridgeVLA，其核心思路是通过将3D点云投影为多视角2D图像作为输入，并预测2D热图作为输出，在统一的2D空间内对齐输入和输出，从而同时利用VLM骨干中的广泛知识和3D输入中嵌入的空间结构先验。</p>
<h2 id="方法详解">方法详解</h2>
<p>BridgeVLA采用双阶段训练流程。整体目标是学习一个多任务的3D机器人操作策略π，将观测o和语言指令l映射到动作a。策略执行是一个迭代过程：预测动作、通过运动规划器移动到关键帧位姿、更新观测并重复。</p>
<p><img src="https://arxiv.org/html/2506.07961v2/x1.png" alt="方法概述"></p>
<blockquote>
<p><strong>图1</strong>：BridgeVLA概述。这是一个新颖的3D VLA模型，在统一的2D图像空间内对齐输入和输出。它在2D热图上进行目标定位预训练，并为3D操作进行动作预测微调。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.07961v2/x2.png" alt="模型架构"></p>
<blockquote>
<p><strong>图2</strong>：模型架构。(a) 2D热图预训练：在2D目标检测数据集上训练模型。输入图像和描述目标物体的语言，输出一个突出目标物体感兴趣区域的2D热图。(b) 3D动作微调：输入3D点云的三个正交投影图像和一个语言指令。输出三个2D热图，分别突出显示末端执行器在下一个关键帧在所有三个视图中的位置。对于其余动作分量，使用MLP处理图像特征token来预测下一个关键帧的旋转动作、夹爪动作和碰撞标志。</p>
</blockquote>
<p><strong>2D热图预训练</strong>：由于原始的VLM骨干被预训练为预测无空间结构的token序列，与下游策略学习所需的热图输出不兼容。因此，本文引入了一个预训练阶段，训练模型根据文本输入通过热图进行目标定位。具体使用RoboPoint的12万张目标检测数据。对于每张图像，根据所有感兴趣物体的边界框构建真实热图H^gt：为每个物体构建一个带空间截断的概率图，然后对所有物体的概率图进行平均和归一化得到最终热图。模型以图像和描述感兴趣物体的文本提示作为输入。本文采用PaliGemma作为VLM骨干。为了预测热图，首先根据其图像块位置重新排列输出的图像token以重建空间特征网格，然后通过一个凸上采样块将网格转换为与输入图像分辨率相同的热图。整个流程使用交叉熵损失进行训练，以预测定位图像中所有感兴趣物体位置的热图。此预训练策略输出具有空间感知的2D热图，与先前工作中的传统下一个token预测不同，并且具有高度可扩展性。</p>
<p><strong>3D动作微调</strong>：在微调阶段，首先从校准相机捕获的RGB-D图像重建场景点云。为了与VLM骨干的2D图像输入对齐，从三个视角（顶视图、前视图、右视图）渲染点云的三个正交投影图像，作为VLM骨干的输入图像。这些图像与任务指令一起输入预训练的VLM骨干，为三个视图各生成一个热图。在VLM前向传播过程中不引入任何额外信息（如机器人状态），以最小化预训练和微调之间的分布偏移。</p>
<p>对于平移动作，将三个视图的热图反向投影，以估计均匀分布在机器人工作空间中的所有3D点网格的分数。具有最高分数的3D点的位置决定了下一个关键帧中末端执行器的平移。对于旋转动作，使用欧拉角表示，每个轴离散化为72个区间。为了预测旋转、二值夹爪动作和碰撞避免标志，整合了全局和局部上下文特征。全局特征通过对每个输入的正交投影图像的输出token进行最大池化获得（每个视图一个token）。局部特征从每个视图的热图峰值处提取一个token。所有这些token被拼接并通过MLP来预测旋转动作、夹爪动作和碰撞避免标志。</p>
<p>BridgeVLA采用从粗到细的细化策略进行精确动作预测：在原始点云上进行初始预测后，以预测的平移为中心，用长方体裁剪并放大点云，在裁剪放大的点云上进行第二次前向传播，并使用第二次传播预测的动作执行。</p>
<p>微调阶段的训练损失包含四个部分：L = L_trans + L_rot + L_gripper + L_collision。L_trans是监督平移动作热图预测的交叉熵损失。L_rot是监督旋转预测的交叉熵损失。L_gripper和L_collision是监督夹爪动作和碰撞避免的二元交叉熵损失。为增强几何鲁棒性，在训练期间对点云和真实动作联合应用随机刚体变换。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个仿真基准（RLBench、COLOSSEUM、GemBench）和真实机器人环境中进行，旨在评估方法的有效性、样本效率、鲁棒性和泛化能力。</p>
<p><strong>RLBench实验</strong>：在CoppeliaSim中使用Franka Panda机器人，评估18个任务，每个任务提供100条专家演示。模型通过每次任务25次试验的二元成功率进行评估。</p>
<p><img src="https://arxiv.org/html/2506.07961v2/x3.png" alt="RLBench结果"></p>
<blockquote>
<p><strong>表1</strong>：RLBench上的结果。“Avg. Rank”列报告了每个方法在所有18个任务中的平均排名，数值越低表示整体性能越好。“BridgeVLA w/o heat”是指不使用中间热图直接预测动作的消融版本。“BridgeVLA w pos”是指将位置特征整合到图像特征中的消融版本。BridgeVLA在18个任务中的10个上取得了最佳性能。</p>
</blockquote>
<p>对比基线包括：Image-BC (CNN/ViT)、C2F-ARM-BC、PerAct、HiveFormer、PolarNet、Act3D、3D Diffuser Actor、RVT和RVT-2。BridgeVLA平均成功率为88.2%，平均排名为2.03，优于所有基线方法，在RLBench上确立了新的技术水平。特别是在需要高精度对齐的任务（如Insert Peg和Sort Shape）上表现出色。在Place Cups任务中表现最差，作者推测是因为目标关键点在所有正交投影视图中经常被遮挡。</p>
<p><strong>COLOSSEUM实验</strong>：该基准在RLBench基础上扩展，评估模型在12个扰动轴（如物体纹理、颜色、大小、背景、光照、干扰物和相机位姿变化）上的泛化能力。模型在原始RLBench数据上训练，在包含20,371个独特任务扰动实例的环境中评估。</p>
<p><img src="https://arxiv.org/html/2506.07961v2/x4.png" alt="COLOSSEUM结果"></p>
<blockquote>
<p><strong>表2</strong>：COLOSSEUM基准上的结果。该表显示了14种泛化设置下的成功率。“Avg. Rank”列报告了每个方法在所有扰动中的平均排名，数值越低表示整体性能越好。与最先进的基线相比，BridgeVLA将平均成功率提高了7.3%。</p>
</blockquote>
<p>对比基线包括：R3M-MLP、MVP-MLP、PerAct、RVT和RVT-2。BridgeVLA平均成功率为64.0%，平均排名为1.07，显著优于最佳基线（RVT-2的56.7%），在14个评估扰动中的13个上排名最佳，展示了强大的抗视觉扰动鲁棒性。</p>
<p><strong>GemBench实验</strong>：该基准专注于评估在物体-技能新组合上的组合泛化能力。模型在包含6个物体和6个技能组合的36个任务上训练，并在未见过的物体-技能组合上评估。</p>
<p><img src="https://arxiv.org/html/2506.07961v2/x5.png" alt="GemBench结果"></p>
<blockquote>
<p><strong>图4</strong>：GemBench上的组合泛化结果。BridgeVLA在物体-技能新组合上取得了最高的平均成功率，显著优于基线方法。</p>
</blockquote>
<p>对比基线包括：R3M-MLP、MVP-MLP、PerAct、RVT和RVT-2。BridgeVLA在未见过的组合上平均成功率为59.2%，优于所有基线，特别是在涉及新物体的任务上表现出更强的泛化能力。</p>
<p><strong>真实机器人实验</strong>：在Franka Panda机器人上进行评估，包含7种设置：原始设置、三种视觉干扰（背景纹理变化、干扰物、光照变化）、三种泛化设置（未见过的物体类别、未见过的技能描述、未见过的物体-技能组合）。每个任务仅用3条轨迹进行训练。</p>
<p><img src="https://arxiv.org/html/2506.07961v2/x6.png" alt="真实实验样本效率"></p>
<blockquote>
<p><strong>图5</strong>：真实机器人实验中的样本效率。BridgeVLA仅用每个任务3条轨迹训练，在10多个任务上平均成功率超过95%，而基线方法π0完全失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.07961v2/x7.png" alt="真实实验泛化结果"></p>
<blockquote>
<p><strong>图6</strong>：真实机器人实验中的泛化结果。BridgeVLA在视觉干扰和未见过的指令等分布外设置中表现稳健，平均超越最先进的基线方法32%。</p>
</blockquote>
<p>对比基线是π0。BridgeVLA在原始设置上平均成功率为96.8%，在7种设置上平均超越基线32%。在仅使用每个任务3条轨迹训练的情况下，在10多个任务上取得了95.4%的成功率，展示了卓越的样本效率，并且在视觉干扰和未见过的指令等分布外设置中泛化稳健。</p>
<p><strong>消融实验</strong>：</p>
<ul>
<li><strong>热图预测的作用</strong>：在RLBench上，“BridgeVLA w/o heat”（直接预测动作）的平均成功率仅为31.4%，远低于完整版（88.2%），证明了热图预测对于利用空间先验至关重要。</li>
<li><strong>位置特征的作用</strong>：“BridgeVLA w pos”（在图像特征中加入位置编码）的平均成功率为56.2%，表明额外的位置信息可能干扰了从预训练VLM中迁移的知识。</li>
<li><strong>预训练数据量</strong>：使用更多样化的预训练数据（如COCO检测+RoboPoint）能带来进一步的性能提升。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li>提出了BridgeVLA，一种新颖的3D VLA模型，通过2D热图实现输入输出对齐，从而高效且有效地利用视觉语言模型学习3D机器人操作。</li>
<li>提出了一种可扩展的预训练方法，通过目标定位使模型具备根据文本输入预测热图的能力。</li>
<li>在仿真和现实世界环境中进行了广泛的实验，结果表明BridgeVLA在两种设置下均优于最先进的方法，并在真实机器人实验中实现了卓越的样本效率。</li>
</ol>
<p>论文提到的局限性在于，当目标关键点（如在Place Cups任务中）在所有正交投影视图中被遮挡时，性能会下降。未来计划探索动态选择投影视图进行渲染以避免此问题。</p>
<p>对后续研究的启示：BridgeVLA所倡导的“在统一2D空间内对齐输入输出”的范式，为构建高效且知识丰富的3D VLA模型提供了新思路。其可扩展的预训练策略表明，将VLM的能力从序列生成迁移到空间结构化预测是可行的。该方法在极少量数据下展现出的强大学习能力，为数据稀缺场景下的机器人技能学习带来了希望。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出BridgeVLA模型，旨在解决现有3D视觉-语言-动作模型未能充分利用空间结构、导致数据效率低下的问题。方法核心是通过输入输出对齐：先将点云投影为多视图图像作为输入，预训练视觉语言模型生成2D热图，再微调整个模型以预测热图并输出动作。实验显示，该模型在RLBench上将平均成功率从81.4%提升至88.2%，在COLOSSEUM上从56.7%提升至64.0%，真实机器人实验平均性能超越基线32%，且仅需每个任务3条轨迹就能在10多个任务上达到95.4%的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.07961" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>