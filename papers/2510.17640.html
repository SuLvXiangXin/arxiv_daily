<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.17640" target="_blank" rel="noreferrer">2510.17640</a></span>
        <span>作者: Ziwei Wang Team</span>
        <span>日期: 2025-10-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言动作模型已成为通过模仿学习掌握复杂机器人操作技能的主流范式。然而，现有的模仿学习数据集通常仅包含成功的演示轨迹，缺乏失败或从失败中恢复的数据。当机器人因微小扰动或误差偏离主要策略，进入训练数据分布之外的状态时，VLA模型往往难以应对，导致行为不稳定甚至任务完全失败。现有解决分布外问题的方法，如大规模演示收集、合成或仿真数据、数据增强或强化学习，分别存在成本高昂、存在仿真到现实鸿沟、增强方式简单低效或探索不安全不稳定等局限性。</p>
<p>本文针对模仿学习策略因缺乏分布外状态数据而泛化能力弱的痛点，提出了一个通过探索性采样进行自动化分布外数据增强的新视角。其核心思路是：利用离线强化学习训练一个能准确识别当前策略下潜在次优动作的动作价值网络（评论家），并通过一种探索性采样机制，策略性地将这些代表分布外风险的“动作代理”纳入训练数据集，从而迫使策略显式学习从分布外状态恢复的能力，增强其鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>RESample框架旨在通过策略与评论家之间的分歧，系统地暴露学习策略的潜在缺陷，从而用信息丰富的经验来丰富训练数据分布。</p>
<p><img src="https://arxiv.org/html/2510.17640v2/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：RESample框架概览。从专家演示（源数据集）开始，RESample引入了一种探索性采样机制，利用策略-评论家分歧来暴露分布外状态。这些具有挑战性的状态被整合到一个增强的回放缓冲区中，并组装成恢复轨迹，形成分布外恢复数据集。该过程建立了一个反馈循环：策略被优化以处理故障，同时评论家通过接触成功和失败经验而变得更加准确。</p>
</blockquote>
<p>整体流程始于专家演示数据集。框架的核心是一个经过校准的动作价值网络（评论家）$Q_{\phi}(s, a)$和一个生成策略$\pi_{\theta}(a|s)$。探索性采样机制利用两者之间的分歧来识别“自信的错误”——即策略认为很可能但评论家评估为低价值的动作。这些动作被刻意执行，使智能体体验其潜在的失败模式，由此产生的轨迹（包含故障和恢复尝试）被纳入增强数据集。随后，策略在此 enriched 数据集上重新训练以获取恢复技能，同时评论家也基于成功和失败经验进行更新以提高评估准确性。迭代此循环形成一个课程式的渐进过程。</p>
<p><strong>价值函数估计</strong>：为获得能精细指导探索的评论家，本文在Cal-QL基础上进行了改进，提出了一个演员锚定且似然感知的评论家。其优化目标为最小化时序差分损失加上一个校准的保守正则项：$\min_{\phi}\mathcal{L}_{\text{TD}}(\phi)+\lambda R(\phi)$。其中正则项$R(\phi)$由三个互补组件构成：</p>
<ol>
<li><strong>均匀惩罚</strong> $R_{\text{uni}}$：抑制在动作空间均匀采样动作上的Q值，对明显分布外的行为实施悲观估计。</li>
<li><strong>演员锚定校准</strong> $R_{\text{act}}$：通过一个轻量级代理演员$\pi_{\psi}$来校准策略高密度区域附近的动作Q值，防止系统性低估。</li>
<li><strong>数据内保持</strong> $R_{\text{data}}$：保持对演示数据中动作对的经验Q值的保真度，稳定训练。<br>这三个组件确保了评论家能保守评估分布外动作、合理校准策略相关动作，且计算高效。</li>
</ol>
<p><strong>探索性采样机制</strong>：在状态$s_t$下，策略生成一组候选动作$\mathcal{A}<em>C \sim \pi</em>{\theta}(\cdot|s_t)$。评论家根据价值阈值$\tau_Q$过滤出探索子集：$\mathcal{A}<em>{\text{exp}} \triangleq {a \in \mathcal{A}<em>C \mid Q</em>{\phi}(s_t, a) &lt; \tau_Q}$。干预规则是：如果$\mathcal{A}</em>{\text{exp}}$非空，则执行其中策略似然最高的动作；否则，执行策略的默认最优动作。这样，每当评论家检测到“自信的错误”，该机制就强制策略将其实例化，将分歧转化为结构化的探索信号，使探索与模型的不确定性对齐，而非随机扰动。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要评估在LIBERO基准上进行，该基准包含空间、物体、目标和长时程四类任务，每类10个。真实世界实验在Galaxea A1机械臂上进行。基线策略选择了两种先进的VLA模型：DiT Policy和$\pi_0$，并将RESample框架与之集成以验证通用性。</p>
<p><img src="https://arxiv.org/html/2510.17640v2/img/mr.png" alt="模拟实验结果表"></p>
<blockquote>
<p><strong>表I</strong>：LIBERO上的实验结果。RESample显著提升了两种基线策略的平均成功率，特别是在长时程任务上提升幅度最大（DiT Policy提升19%）。</p>
</blockquote>
<p><strong>模拟实验结果</strong>：如表I所示，RESample带来了显著的性能提升。使用DiT Policy时，平均成功率从77.0%提升至89.0%（绝对提升12%）。使用$\pi_0$时，平均成功率从90.9%提升至92.9%，创造了新的SOTA。提升在长时程任务中最为显著，例如DiT Policy在长时程任务上从73.5%提升至92.5%（+19%），这表明RESample有效缓解了累积误差导致的分布偏移。</p>
<p><img src="https://arxiv.org/html/2510.17640v2/img/sample_compare.png" alt="样本对比"></p>
<blockquote>
<p><strong>图4</strong>：样本对比。可视化应用RESample框架前后策略采样的动作。增强后策略生成的动作表现出更强的鲁棒性，同时更新后的动作评论家显著扩展了安全动作边界，整体Q值更高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.17640v2/x3.png" alt="跨任务增强"></p>
<blockquote>
<p><strong>图5</strong>：跨任务增强。从一个任务（如任务2、4、5、8）生成的增强数据可以有效地迁移到同一类别中的其他任务，带来平均5-10%的额外性能提升。</p>
</blockquote>
<p>此外，研究还发现了<strong>跨任务增强</strong>现象：从一个任务生成的增强数据可以迁移到同类别其他任务，带来额外5-10%的平均提升（图5），表明RESample产生的是捕获了可泛化故障模式（如模糊空间关系）的数据。</p>
<p><strong>真实世界实验结果</strong>：<br><img src="https://arxiv.org/html/2510.17640v2/img/realworld_setup.png" alt="真实世界实验设置"></p>
<blockquote>
<p><strong>图3</strong>：真实世界实验设置。使用配备平行夹爪、腕部RealSense D435i相机和外部RealSense L515相机的Galaxea A1机械臂。</p>
</blockquote>
<p>在四个真实操作任务上，DiT Policy结合RESample将平均成功率从42.5%提升至56.3%，在更复杂的“排列方块”和“堆叠2个杯子”任务上分别有15%和25%的显著提升（表II），与模拟实验结论一致。</p>
<p><img src="https://arxiv.org/html/2510.17640v2/x4.png" alt="真实世界轨迹流"></p>
<blockquote>
<p><strong>图6</strong>：真实世界轨迹流。在真实世界堆叠方块任务中，基线策略与我们的RESample框架对比。采用我们框架的策略整体Q值更高，并在任务执行过程中表现出更鲁棒的恢复行为。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>在LIBERO空间任务套件上对DiT Policy的消融研究（表III）表明：仅使用原始策略成功率为68.5%；加入无探索性采样的在线滚动轨迹增强后提升至74.8%；使用随机采样（无评论家指导）增强后为73.0%；而完整的RESample框架达到76.5%。这证明了评论家引导的探索性采样机制是关键创新，其效果优于简单的数据扩充或随机探索。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个用于缓解机器人模仿学习中分布外问题的鲁棒数据增强框架RESample。</li>
<li>设计了一种探索性采样机制，能够自适应地将分布外样本整合到训练数据集中。</li>
<li>利用一个经过特殊设计的动作评论家来识别细粒度的分布外动作样本以进行数据增强。</li>
<li>在仿真和真实世界机器人操作任务上验证了框架的有效性、通用性和可迁移性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在基线性能已经很高的简单任务（如物体任务）上，强制探索可能引入轻微扰动，导致性能有边际下降（如$\pi_0$在物体任务上降低了1.2%）。这表明框架在接近最优的策略上应用时需要权衡探索的收益与扰动风险。</p>
<p><strong>启示</strong>：RESample展示了一种将离线学习与定向在线探索相结合的新范式，通过策略与评论家的“分歧”来驱动数据生成，为模仿学习提供了一条样本高效且安全的增强路径。其“跨任务增强”的发现启示，学习可泛化的故障恢复模式可能比针对特定任务的过拟合增强更具 scalability。未来工作可探索如何自动化调整探索阈值，或将该框架与更广泛的基础策略模型结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在模仿学习中因训练数据缺乏分布外状态而鲁棒性差的问题，提出RESample数据增强框架。该框架通过离线强化学习获取动作价值网络以识别次优动作，并利用探索性采样机制自动生成并融入分布外状态数据。实验表明，该方法在LIBERO基准和真实机器人任务上有效提升了模型的稳定性与泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.17640" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>