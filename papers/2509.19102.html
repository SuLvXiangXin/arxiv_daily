<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19102" target="_blank" rel="noreferrer">2509.19102</a></span>
        <span>作者: Jianwei Zhang Team</span>
        <span>日期: 2025-09-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，通过端到端演示学习的通用机器人技能往往产生任务特定的策略，难以泛化到训练分布之外。基于RGB图像或点云的模仿学习方法常因视角敏感性和噪声观测而泛化有限。物体中心表示（如6D位姿）虽能提升泛化能力，但现有方法（如SPOT）通常依赖于实例特定、目标条件化的轨迹，将其视为整体程序，限制了跨类别和跨任务的泛化能力。这源于将操作视为整体任务，而非模块化、可重用的行为。</p>
<p>本文针对上述痛点，提出了一种新视角：将长时程操作任务分解为一系列可重用的动作基元，每个基元由执行者（Actor）、动词（Verb）和对象（Object）定义。通过利用大视觉语言模型（VLM）提供的可供性线索，对功能相关的对象进行规范化（Canonicalization），将其映射到共享的功能坐标系中。这种功能对齐实现了自动的操纵轨迹迁移，并使得训练出的策略能够感知物体位姿并泛化到新类别。核心思路是：通过功能对象规范化实现动作基元的跨实例和跨类别泛化，从而学习通用化的机器人操作策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>FUNCanon框架包含五个核心组件，旨在将长时程任务分解、功能对齐、数据增强和策略学习统一起来。</p>
<p><img src="https://arxiv.org/html/2509.19102v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：FUNCanon整体架构。包含五个部分：(A) 动作基元建模与任务分解，(B) 功能对齐，(C) 自动轨迹迁移，(D) 动作中心扩散策略学习，(E) 仿真到现实推理。</p>
</blockquote>
<p><strong>A. 动作基元建模与任务分解</strong>：首先，将长时程操纵任务建模为可重用动作基元的序列。一个动作基元定义为三元组 <code>a = ⟨𝒜, 𝒱, 𝒪⟩</code>，分别代表执行者（如机器人夹爪、水壶）、动作动词（如抓握、倾倒）和目标对象（如杯子）。利用大语言模型（如GPT-4o）结合视觉模型提供的语义和功能线索，自动将任务演示 <code>𝒯_e</code> 分解为基元序列 <code>{a1, a2, ..., aN}</code>。这使策略学习聚焦于单个动作语义和物体交互，便于后续的基元重用和组合。</p>
<p><strong>B. 功能对齐</strong>：这是实现跨类别泛化的关键。对于给定物体 <code>o</code> 及其3D模型 <code>M_o</code>，首先从多视角进行RGB-D渲染，利用自监督视觉编码器提取特征，并结合深度信息将2D特征提升至3D空间，通过K-Means聚类得到 <code>M</code> 个候选功能区域 <code>R_m</code>。随后，利用VLM作为二元分类器 <code>Φ</code>，判断每个候选区域 <code>R_m</code> 在给定动作 <code>a</code>（如抓握）和角色假设 <code>r</code>（主动或被动）下是否具有功能相关性。最终得到物体 <code>o</code> 在动作 <code>a</code> 下的功能区域集合 <code>F_a(o)</code>。基于此，计算每个物体的功能方向向量 <code>v_o</code>（功能区域点的平均3D坐标），作为物体的“功能中心”。对于源物体 <code>o_s</code> 和目标物体 <code>o_t</code>，通过优化一个绕Z轴的旋转 <code>R*</code>，最小化 <code>‖R v_s - v_t‖</code>，从而实现两个物体在功能坐标系下的对齐。</p>
<p><strong>C. 自动轨迹迁移</strong>：利用功能对齐进行数据增强。RLBench中的轨迹通常由多个子轨迹组成，每个子轨迹对应一个AVO动作基元。给定源物体 <code>o_s</code> 的子轨迹 <code>τ_s</code>，首先将其转换到由 <code>v_s</code> 定义的功能坐标系中。为了将轨迹迁移到新物体 <code>o_t</code>，只需应用功能向量 <code>v_t</code> 将轨迹映射回目标物体的坐标系：<code>τ_t = τ_s + (v_t - v_s)</code>。此过程可在类别内跨物体重复，生成同一动作基元的多个变体，显著增加训练数据的多样性和功能一致性。</p>
<p><strong>D. 动作中心策略训练</strong>：训练一个名为FuncDiffuser的扩散策略，用于生成与动作基元对应的未来物体轨迹（而非直接预测机器人关节运动）。状态表示 <code>s</code> 编码了执行者与对象之间的相对位姿特征 <code>h_Δ</code>、两者带有功能区域信息的点云视觉特征 <code>f_A, f_O</code>（来自PointNet++），以及动作动词的CLIP嵌入 <code>v</code>。策略 <code>π_θ(a|s)</code> 使用基于扩散的去噪损失（MSE）进行训练，以对齐预测的动作轨迹与演示轨迹。通过利用功能对齐和相对位姿特征进行轨迹增强，使扩散策略能够自然地尊重物体可供性，并泛化到不同物体实例和类别。</p>
<p><strong>E. 仿真到现实推理</strong>：部署时，首先使用大语言模型将高层人类指令分解为带有执行者-对象对的子任务。对于每个目标对象，使用AIGC模型生成3D网格以弥补高质量CAD模型的缺失。随后应用功能对齐对生成的网格进行分割和对齐，识别与目标动作相关的区域。利用这些对齐的功能区域，通过现成的位姿估计器（如FoundationPose）生成物体中心的场景表示（包括功能区域的位姿和空间关系）。该表示与任务分解得到的执行者-对象对一起输入动作中心扩散策略，预测每个子任务的轨迹。</p>
<p>与现有方法相比，创新点在于：1) 将长时程任务明确分解为包含抓握阶段的、可重用的AVO动作基元；2) 利用VLM进行功能驱动的规范化，实现功能对齐和自动轨迹迁移；3) 在功能对齐数据上训练物体中心和动作中心的扩散策略，实现从实例级到类别级的泛化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：仿真实验在标准机器人操作基准RLBench中进行，系统评估了位姿级、实例级和类别级变化下的泛化能力。真实世界实验使用Franka Emika机器人和多个Intel RealSense相机，评估仿真到现实的策略迁移。</p>
<p><img src="https://arxiv.org/html/2509.19102v1/x3.png" alt="真实实验设置"></p>
<blockquote>
<p><strong>图3</strong>：真实世界实验设置，包括Franka Emika机器人和Intel RealSense相机。</p>
</blockquote>
<p><strong>对比方法</strong>：与当前先进方法SPOT和3DA进行了对比。</p>
<p><strong>关键实验结果</strong>：在仿真实验中，评估了三个核心操作任务（Put A in B, Pour A in B, Water B with A）在三种变化级别下的成功率（25次试验的平均值±标准差）。</p>
<p><img src="https://arxiv.org/html/2509.19102v1/x4.png" alt="仿真任务变体"></p>
<blockquote>
<p><strong>图4</strong>：基于功能对齐方法生成的仿真任务变体轨迹示例，展示了不同物体实例和类别下的轨迹迁移。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.19102v1/x5.png" alt="功能对齐定性结果"></p>
<blockquote>
<p><strong>图5</strong>：功能对齐的定性结果，展示了不同物体（如茶壶、水罐）上功能区域（如把手、壶嘴）的识别。</p>
</blockquote>
<p>如表I所示，FUNCanon在所有任务和变化级别上均取得了最高的成功率。在平均表现上，相比3DA，在位姿级、实例级和类别级分别提升了38.4%、35.8%和36.0%。即使在更具挑战性的从抓握开始的完整流程评估中，FUNCanon也显著优于SPOT。这表明功能对齐模块有效增强了策略对未见物体实例和类别的泛化能力。</p>
<p><strong>消融实验</strong>：为了分析关键组件的贡献，进行了消融研究，比较了以下变体：1) <strong>Monolithic Trajectory</strong>（合并所有阶段为单一轨迹）；2) <strong>No-Func-Frame</strong>（仅使用世界或相机坐标，省略功能关键点）；3) <strong>Geometry-only</strong>（仅依赖几何信息，忽略可供性和功能线索）。结果如表II所示，在Pick, Pour (L2)任务的类别级替换中，完整模型（FunCanon）的成功率为40.0%。移除功能坐标系（No-Func-Frame）导致性能大幅下降至24.0%（-16.0%），而使用整体轨迹（Monolithic）则进一步降至16.0%（-24.0%）。这表明阶段建模、功能坐标系和功能线索对于实现高成功率至关重要。</p>
<p><strong>真实世界实验</strong>：策略完全在仿真中训练，并直接部署到真实机器人上，无需微调。进行了50次真实世界试验，结果如表III所示。FUNCanon在三个任务（Pick, Place; Pick, Pour L1; Pick, Pour L2）上的成功率分别为88%、90%和88%，一致性地超越了SPOT和3DA基线，尤其在倾倒任务中优势明显，验证了功能对齐对于处理不同容器几何形状的强归纳偏置和可靠的仿真到现实迁移能力。</p>
<p><img src="https://arxiv.org/html/2509.19102v1/x6.png" alt="真实世界定性结果"></p>
<blockquote>
<p><strong>图6</strong>：真实世界实验的定性结果，展示了策略在真实场景中操作未见物体的成功案例。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>FUNCanon框架</strong>，将复杂操作任务分解为基于功能对齐物体对的可重用动作基元，并明确包含了抓握阶段；2) 利用<strong>大视觉语言模型进行功能驱动的规范化</strong>，实现了功能对齐和自动轨迹迁移，从而支持感知位姿和跨类别的策略学习；3) 开发了在功能对齐数据上训练的<strong>物体中心和动作中心扩散策略</strong>，实现了从实例级到类别级的泛化以及鲁棒的仿真到现实迁移。</p>
<p>论文自身提到的局限性包括：功能对齐和轨迹迁移依赖于离线演示轨迹的质量和覆盖范围；功能区域的识别准确性受限于VLM的能力；在极其复杂的几何形状或非刚性物体上，功能对齐可能面临挑战。</p>
<p>这项工作对后续研究的启示在于：为机器人操作提供了一种新的“组合式”学习范式，通过功能语义而非仅表观或几何来对齐和迁移技能。它展示了基础模型（VLMs/LLMs）在提供高层功能先验以引导机器人数据增强和策略学习方面的巨大潜力。未来的工作可以探索在更动态的环境中在线适应功能对齐，或将此框架扩展到更广泛的工具使用和双手操作任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出FUNCanon框架，旨在解决机器人模仿学习策略在面临新物体、新姿态和新任务时泛化能力不足的核心问题。其关键技术是**功能对象规范化**：利用大型视觉语言模型提供的功能线索，将不同物体映射到共享的功能坐标系，实现跨类别的轨迹自动迁移。基于此对齐数据训练的**对象与动作中心扩散策略FuncDiffuser**，能够自然地遵从物体功能与姿态。实验表明，该方法在模拟和真实场景中实现了**类别级泛化**、**跨任务行为重用**以及**稳健的仿真到现实部署**，验证了功能规范化能为复杂操作提供有效的归纳偏置。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19102" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>