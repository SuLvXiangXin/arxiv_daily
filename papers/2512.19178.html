<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Vision-Language-Policy Model for Dynamic Robot Task Planning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Vision-Language-Policy Model for Dynamic Robot Task Planning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.19178" target="_blank" rel="noreferrer">2512.19178</a></span>
        <span>作者: Wang, Jin, Ly, Kim Tien, Cloete, Jacques, Tsagarakis, Nikos, Havoutis, Ioannis</span>
        <span>日期: 2025/12/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在非结构化环境中，将自然语言指令与自主执行相连接仍然是机器人学的一个开放挑战。传统基于模型的机器人任务规划方法（如任务与运动规划TAMP）在结构化环境中表现良好，但缺乏语义场景理解和空间推理能力，难以在非结构化环境中生成实时、指令驱动的行为计划。近年来，大型语言模型（LLMs）和视觉语言模型（VLMs）的进展为机器人学习带来了新突破。然而，现有的视觉-语言-动作（VLA）方法通常采用端到端范式，需要大规模高质量数据集，且其高级语义输出与机器人低级运动控制之间存在鸿沟，导致行为规划可解释性差，也难以高效地在机器人上本地部署。</p>
<p>本文针对现有VLA方法在数据需求、可解释性和部署效率方面的关键痛点，提出了一种新视角：不直接生成低级动作，而是利用微调的VLM生成结构化的、分层的“策略”（Policy），该策略调用预定义的行为基元（如抓取、放置）来控制机器人。这种设计旨在弥合高级规划与低级执行之间的差距，并支持任务执行过程中的动态策略更新。本文的核心思路是：通过基于真实世界交互数据微调预训练的VLM，构建一个视觉-语言-策略（VLP）模型，该模型能根据语义指令和当前场景，生成并动态更新可解释的分层策略，从而实现对机器人的实时、自适应控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的VLP框架是一个两阶段流程：模型训练与部署。如算法1所示，第一阶段使用真实世界机器人交互数据集 $\mathcal{D}$，通过低秩适应（LoRA）微调一个预训练的视觉语言模型 $Q_v$，得到策略模型 $Q_p$。第二阶段是部署，机器人持续运行，接收语义指令 $I_t$，并初始化一个任务记忆模块 $\mathcal{M}$。在每个时间步，系统获取当前视觉观测 $o_t$ 和机器人状态 $s_t$，并连同行为基元库 $\mathcal{A}$ 和记忆 $\mathcal{M}$ 一起查询 $Q_p$，以生成分层策略 $P_t$。$P_t$ 中的每个行为被顺序执行，执行后更新感知和状态反馈。如果任务完成则循环终止；如果接收到新指令或任务状态改变（即策略触发），则更新指令 $I_t$ 和记忆 $\mathcal{M}$，并相应地重新生成 $P_t$，从而实现实时适应。</p>
<p><img src="https://arxiv.org/html/2512.19178v1/images/figure2.png" alt="系统总览"></p>
<blockquote>
<p><strong>图1</strong>：系统整体框架。第一阶段（左）使用真实世界交互数据（图像、指令、策略）对VLM进行后训练。第二阶段（右）将VLP模型本地部署，并根据语义输入生成结构化策略，实现实时机器人控制和自我更新。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.19178v1/images/figure3.png" alt="VLP模型内部结构"></p>
<blockquote>
<p><strong>图2</strong>：VLP模型内部结构及真实世界任务场景。模型基于Qwen2.5-VL微调，其视觉编码器为ViT，语言主干为Qwen2.5 LLM。微调后，语言模型解码器输出分层的JSON格式策略，用于实时规划机器人行为。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>VLP模型训练</strong>：模型基于Qwen2.5-VL进行微调。为了在保留模型原有通用理解能力的同时，使其能生成任务对齐的结构化JSON策略，论文比较了两种后训练策略（如图3所示）：<ul>
<li><strong>Vision+Decoder LoRA</strong>：将LoRA适配器应用于视觉编码器的最后6层和语言模型解码器。视觉侧使用较小的秩（r=8）以防止过拟合，解码器使用r=16。此策略旨在细化视觉感知，以更好地对齐机器人视角。</li>
<li><strong>Decoder-only LoRA</strong>：完全冻结视觉编码器，仅将LoRA适配器应用于语言解码器。此配置允许模型在不改变预训练视觉表示的情况下，调整其推理和策略生成能力。LoRA秩设为r=16，学习率为1e-4。<br>两种模型均使用因果语言建模目标进行训练，损失仅针对JSON策略生成部分计算，训练2个epoch。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.19178v1/images/figure4.png" alt="不同微调策略对比"></p>
<blockquote>
<p><strong>图3</strong>：不同模型微调策略对比。(a) Vision+Decoder LoRA：LoRA应用于视觉编码器和LM解码器。(b) Decoder-only LoRA：视觉编码器被冻结，LoRA仅应用于LM解码器。</p>
</blockquote>
<ol start="2">
<li><p><strong>策略生成与结构</strong>：VLP模型生成的策略是一个JSON格式的结构，包含一系列动作和感知行为序列。其设计灵感源于机器人规划中常用的行为树，但简化为支持实时执行状态和参数反馈的自上而下顺序执行方案。这种轻量级结构化设计确保了动作规划的可解释性，并支持动态策略更新。</p>
</li>
<li><p><strong>运动基元与视觉感知</strong>：为了连接策略规划与低级机器人执行，系统预定义了一组动作和感知行为基元，它们编码了机器人的内在能力，可由生成的策略直接调用。</p>
<ul>
<li><strong>动作基元</strong>：控制机器人全身运动，如<code>抓取</code>、<code>抬起</code>、<code>放置</code>、<code>递送</code>等。每个动作被封装为独立API，接受目标物体的笛卡尔位姿作为输入。还定义了<code>回零</code>、<code>唤醒</code>、<code>起始姿态</code>等预备和中间动作位姿。</li>
<li><strong>感知基元</strong>：基于机器人机载传感模块，用于提取物体和自身状态信息。利用模型的视觉物体定位能力进行空间推理和功能提取，结合深度图像实现通用物体定位和抓取点生成，并封装为可调用的API。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ul>
<li><strong>结构化策略生成</strong>：不同于VLA模型直接输出动作，VLP输出可解释的、分层的JSON策略，通过调用预定义基元执行，桥接了语义与执行。</li>
<li><strong>轻量高效微调</strong>：采用LoRA对中等规模VLM（Qwen2.5-VL-3B）进行微调，而非训练大规模端到端模型，降低了数据需求和计算成本。</li>
<li><strong>动态更新机制</strong>：通过任务记忆模块和策略触发条件（新指令或状态变化），实现了执行过程中的实时策略重规划。</li>
<li><strong>跨平台泛化</strong>：依赖抽象的行为基元库，使模型能力不绑定于特定机器人硬件。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/平台</strong>：在实验室环境中使用日常家居物品。在两个机器人平台上验证：1) <strong>ANYmal D + Unitree Z1 Arm</strong>（四足移动机械臂，12+6自由度）；2) <strong>丰田HSR</strong>（移动单臂机器人，11自由度）。</li>
<li><strong>Baseline方法</strong>：与多种方法进行定性（能力维度）和定量（成功率）比较，包括基于行为树的规划器（BT-Planner）、LLM驱动的行为树（LLM-BT）、VLA模型（OpenVLA，$\pi_{0.5}$）以及基于VLM的重规划方法（ReplanVLM）等。</li>
<li><strong>评估指标</strong>：规划可行性（Plan）、任务执行成功率（Succ）。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.19178v1/images/plot1.png" alt="平均任务规划成功率对比"></p>
<blockquote>
<p><strong>图5</strong>：与现有方法对比的日常操作任务和重规划场景的平均成功率。VLP模型在各种任务设置中均展现出更优的规划能力，尤其在需要因新指令而重规划时，视觉模态的引入使其成功率显著高于仅基于语言模型的规划方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.19178v1/images/figure6.png" alt="多任务场景评估结果"></p>
<blockquote>
<p><strong>图6</strong>：使用ANYmal和HSR机器人在各种自然语言任务中的真实世界实验，展示了模型在视觉物体定位方面的能力。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>多任务场景评估（表II）</strong>：在<code>拾放</code>、<code>物体递送</code>、<code>场景交互</code>三类任务共50次试验中，微调后的VLP模型（无论是Decoder-only还是Vision+Decoder）规划可行性率均超过88%，显著高于原始Qwen2.5-VL模型（62%），执行成功率均超过68%。其中，Vision+Decoder微调在需要复杂场景推理的任务（如场景交互）上表现略优。</p>
</li>
<li><p><strong>动态任务评估（图7）</strong>：在<code>动态物体递送</code>、<code>执行中目标改变</code>、<code>条件性关闭抽屉</code>等动态场景中，经过20次试验，VLP模型在动态策略生成上的性能大幅优于基准模型，成功率高出20%以上。且由于需要多轮场景推理和策略重评估，Vision+Decoder微调的模型比Decoder-only微调的模型成功率高出5–10%。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.19178v1/images/plot2.png" alt="动态任务性能"></p>
<blockquote>
<p><strong>图7</strong>：上图：动态物体递送任务的多视角快照。下图：模型在动态任务场景中的性能表现。VLP模型在动态策略生成上表现显著优于基准模型。</p>
</blockquote>
<ol start="3">
<li><strong>跨平台测试（图8）</strong>：在两个不同的机器人平台上各进行3项任务共25次试验，VLP模型始终能保持较高的任务成功率（&gt;68%）。大部分失败源于感知或执行错误（与硬件特性强相关），而由模型引起的规划错误占比不到10%，证明了方法的跨平台适应性。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.19178v1/images/plot3.png" alt="跨平台评估"></p>
<blockquote>
<p><strong>图8</strong>：模型在不同机器人平台上的评估结果。柱状图显示任务成功率，折线图显示各类错误（规划、感知、执行）的比例。结果表明模型具有强大的跨平台适应能力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>论文通过比较两种微调策略（Decoder-only LoRA vs. Vision+Decoder LoRA）进行了消融研究。结果表明，两种策略均能大幅提升规划可行性和成功率。在动态任务中，同时微调视觉编码器和解码器的模型表现更佳，因为其增强了场景推理和策略细化能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的<strong>视觉-语言-策略（VLP）动态规划框架</strong>，通过将多模态语言模型与行为基元、功能感知模块相结合，实现了对演进任务需求的实时响应。</li>
<li>基于真实世界物理交互数据训练VLP模型，该模型能整合语义推理与空间几何观测，生成并更新结合历史记忆和状态反馈的策略，<strong>有效桥接了高级规划与低级执行</strong>。</li>
<li>在<strong>不同机器人平台</strong>和多样任务上验证了框架的有效性和泛化能力，支持轻量级模型的快速部署，并提供可解释的分层规划。</li>
</ol>
<p><strong>局限性</strong>：<br>论文指出，当前系统受限于<strong>预定义的动作和感知基元集合</strong>，这在开放集环境中可能会限制规划的灵活性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>证明了<strong>对中等规模VLM进行轻量微调</strong>，并采用<strong>结构化输出（JSON策略）</strong> 结合<strong>预定义技能库</strong>的路径，是实现高效、可解释、可部署机器人任务规划的有效方案。</li>
<li>未来工作可以专注于增强感知能力、集成自主导航和长时程规划，以及<strong>扩展技能库</strong>以支持更复杂的任务，从而突破对预定义基元的依赖。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Vision-Language-Policy（VLP）模型，以解决非结构化环境中自然语言指令与机器人自主执行之间的语义鸿沟问题。该模型基于真实数据微调的视觉-语言模型，能够理解语义指令、融合场景感知与推理，并直接生成控制机器人的行为策略，且支持根据任务变化动态调整策略。实验表明，训练后的VLP模型能有效适应新场景、动态更新策略，展现出较强的规划自主性与跨机器人平台的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.19178" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>