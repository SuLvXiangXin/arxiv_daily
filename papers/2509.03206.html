<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.03206" target="_blank" rel="noreferrer">2509.03206</a></span>
        <span>作者: Daniel A. Braun Team</span>
        <span>日期: 2025-09-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在稀疏奖励任务中，强化学习面临巨大挑战。模仿学习虽收敛更快，但严重依赖人工演示。目标条件监督学习（GCSL）通过事后重标目标，使智能体能够从自身经验中进行自我模仿学习，成为了一种潜在的解决方案。然而，该框架存在两个显著局限：(1) 仅从自身生成的经验中学习会加剧智能体固有的偏见；(2) 重标策略使智能体只关注成功结果，无法从失败中学习。现有方法如加权GCSL和归一化OCBC虽试图改进，但均忽略了未能达成原始目标时所携带的负面反馈信息。本文针对GCSL无法从失败中学习的痛点，提出将对比学习原则融入GCSL框架，以同时从成功和失败中学习。核心思路是：在保留GCSL从重标目标（成功）学习的基础上，引入一个学习的距离函数来评估轨迹相对于原始目标的质量，从而提供负面反馈，引导策略更新或鼓励探索。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法称为“带负面反馈的目标条件监督学习”（GCSL-NF）。其核心在于学习一个参数化的函数 $Q_{\theta}(a_t, s_t, g)$，它近似于从状态 $s_t$ 执行动作 $a_t$ 后达成目标 $g$ 的概率。该方法通过结合两种反馈来更新策略：一种是基于重标目标 $g&#39;$（成功经验）的模仿学习；另一种是基于原始目标 $g$ 和最终达成状态 $s_T$ 之间距离的反馈。</p>
<p><img src="https://arxiv.org/html/2509.03206v1/x2.png" alt="GCSL-NF框架图"></p>
<blockquote>
<p><strong>图2</strong>：GCSL-NF方法整体框架。智能体通过采样轨迹、用重标目标和原始目标重新审视轨迹，然后更新策略和相似度函数。</p>
</blockquote>
<p>算法流程如下：</p>
<ol>
<li><strong>采样与执行</strong>：从目标空间采样目标 $g$，使用当前策略（通过 $Q_{\theta}$ 的argmax选择动作）生成轨迹 $\tau_g$ 并存入回放缓冲区 $\mathcal{R}$。</li>
<li><strong>构建正例数据集</strong>：对缓冲区中的轨迹进行重标，构建正例专家元组集 $\mathcal{T}<em>{+} = {(s_t, a_t, g&#39;=s</em>{t+i})}$，其中 $i&gt;0$，这意味着不仅将最终状态，也将轨迹中任何未来状态作为可行的重标目标。</li>
<li><strong>正例损失（模仿学习）</strong>：通过最小化损失 $L_{+}(\theta)$ 来模仿正例。该损失包含两部分：一是让 $Q_{\theta}$ 对专家动作 $(a_t, s_t, g&#39;)$ 的输出接近1的二元交叉熵（模仿学习）；二是让 $Q_{\theta}$ 对其他动作的输出接近0的二元交叉熵和（正则化）。</li>
<li><strong>构建原始目标数据集</strong>：从缓冲区采样轨迹，构建包含原始目标的元组集 $\mathcal{T}_{o} = {(s_t, a_t, s_T, g)}$。</li>
<li><strong>负反馈损失</strong>：利用学习的距离函数 $p_{\varphi}(s_T, g)$ 评估最终状态 $s_T$ 与原始目标 $g$ 的相似度（接近1表示相似，接近0表示不相似）。计算负反馈损失 $L_{o}(\theta) = \mathbb{E}<em>{\mathcal{T}</em>{o}}[\gamma^{T-t} H(Q_{\theta}(a_t, s_t, g), p_{\varphi}(s_T, g))]$，即鼓励 $Q_{\theta}$ 对原始目标 $g$ 下动作 $a_t$ 的评估值向距离函数给出的相似度 $p_{\varphi}(s_T, g)$ 靠拢。$\gamma^{T-t}$ 是时间折扣。</li>
<li><strong>策略更新</strong>：通过最小化组合损失 $\beta_1 L_{+}(\theta) + \beta_2 L_{o}(\theta)$ 来更新策略参数 $\theta$。</li>
<li><strong>距离函数更新</strong>：根据第3.3节更新距离函数 $p_{\varphi}$ 的参数 $\varphi$。</li>
</ol>
<p><strong>核心创新模块：距离函数 $p_{\varphi}$ 的学习</strong><br>$p_{\varphi}(s, s&#39;)$ 被训练来近似两个任意状态 $s, s&#39;$ 之间的距离小于给定阈值 $n$ 的概率。由于环境动态未知，无法直接基于真实距离采样，因此采用基于轨迹的对比学习方法。</p>
<p><img src="https://arxiv.org/html/2509.03206v1/x3.png" alt="距离函数学习示意图"></p>
<blockquote>
<p><strong>图3</strong>：从轨迹中学习 $p_{\varphi}$ 的示意图。展示了如何在轨迹上定义邻域关系。垂直轴表示不同轨迹，水平轴表示同一轨迹内的时间步。蓝色区域对应来自以参考状态为中心的单峰分布的正样本对，红色区域对应负样本对。</p>
</blockquote>
<p>具体采样策略如下：</p>
<ul>
<li>**正样本对 $\mathcal{D}_{+}$**：来自同一轨迹，两个状态的时间步差 $j&#39;$ 服从以参考状态时间步 $i$ 为众数、$n$ 为边界的对称三角分布 $\Delta(i, n)$，旨在捕获局部邻近状态。</li>
<li>**负样本对 $\mathcal{D}_{-}^{1}$**：来自同一轨迹，但时间步差 $|i-j| &gt; n$，即相距较远的状态。</li>
<li><strong>负样本对 $\mathcal{D}_{-}^{2}$<strong>：来自</strong>不同</strong>轨迹的状态对。</li>
</ul>
<p>距离函数通过最小化噪声对比估计（NCE）损失 $L(\varphi)$ 进行训练，该损失鼓励 $p_{\varphi}$ 对正样本对输出高值（接近1），对负样本对输出低值（接近0）。</p>
<p>与现有方法相比，GCSL-NF的创新点在于：1) 在GCSL框架中首次系统性地引入了从原始目标失败中学习的机制；2) 采用与观察结构无关的、基于对比学习的距离函数来评估状态相似性，增强了通用性；3) 通过结合成功（重标目标）和失败（原始目标+距离评估）的双重反馈，缓解了策略偏见并促进了探索。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在六个具有离散动作空间的2D导航或操控任务环境中评估方法（图4）：</p>
<ol>
<li><strong>点质量导航</strong>：基础任务。</li>
<li><strong>带初始偏见的点质量导航</strong>：智能体初始策略有向右移动的偏见。</li>
<li><strong>带障碍物的点质量导航</strong>：环境中央有圆形障碍物。</li>
<li><strong>2D四房间导航</strong>：智能体需穿越房间之间的通道。</li>
<li><strong>带障碍物的2D LiDAR导航</strong>：观察空间是64维LiDAR测距数据，与物理空间距离无直接关联。</li>
<li><strong>物体推动</strong>：智能体需先移动到物体处，再将其推至目标位置，最后自身到达目标。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.03206v1/x4.png" alt="实验环境示意图"></p>
<blockquote>
<p><strong>图4</strong>：六个实验环境示意图。红点表示智能体位置，绿色星星表示目标位置。</p>
</blockquote>
<p><strong>对比方法</strong>：与基于GCSL的方法（GCSL, WGCSL）、基于HER的方法（DQN+HER, DQN+HER+GC）以及对比学习GCRL方法进行对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>克服初始偏见</strong>：在“带初始偏见的点质量导航”环境中，GCSL因自我模仿而陷入初始偏见无法逃脱，成功率接近0%。而GCSL-NF能成功克服偏见，最终达到约80%的成功率，与DQN+HER性能相当。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.03206v1/x5.png" alt="初始偏见环境结果"></p>
<blockquote>
<p><strong>图5</strong>：在带初始偏见的点质量导航环境中，GCSL-NF与基线方法的成功率对比。GCSL因偏见被困，而GCSL-NF能有效逃脱并学习。</p>
</blockquote>
<ol start="2">
<li><strong>复杂导航任务</strong>：在“带障碍物的点质量导航”和“2D四房间导航”环境中，GCSL-NF均取得了最佳或接近最佳的性能。特别是在四房间任务中，GCSL-NF显著优于所有基线方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.03206v1/x6.png" alt="障碍物与四房间导航结果"></p>
<blockquote>
<p><strong>图6</strong>：在带障碍物的点质量导航（左）和2D四房间导航（右）环境中，GCSL-NF与基线方法的成功率对比。GCSL-NF在复杂路径规划任务中表现优异。</p>
</blockquote>
<ol start="3">
<li><strong>观察空间与物理空间解耦的任务</strong>：在“带障碍物的2D LiDAR导航”环境中，由于观察（LiDAR数据）不直接反映物理距离，GCSL-NF的性能提升更为显著，大幅领先于所有基线方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.03206v1/x7.png" alt="LiDAR导航结果"></p>
<blockquote>
<p><strong>图7</strong>：在带障碍物的2D LiDAR导航环境中，GCSL-NF与基线方法的成功率对比。GCSL-NF在观察空间信息具有欺骗性的任务中优势明显。</p>
</blockquote>
<ol start="4">
<li><strong>物体推动任务</strong>：在这个需要多阶段推理的任务中，GCSL-NF再次展现出最佳性能，成功率达到约65%，而其他方法均低于40%。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.03206v1/x8.png" alt="物体推动任务结果"></p>
<blockquote>
<p><strong>图8</strong>：在物体推动环境中，GCSL-NF与基线方法的成功率对比。GCSL-NF在需要复杂顺序决策的任务中表现最好。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文通过消融实验验证了各组件的重要性。移除负反馈损失 $L_o$ 或距离函数 $p_{\varphi}$（用L2距离代替）都会导致性能显著下降，尤其是在LiDAR导航和物体推动等复杂任务上。这证明了从失败中学习（负反馈）以及使用数据驱动的、通用的距离函数是GCSL-NF成功的关键。</p>
<p><img src="https://arxiv.org/html/2509.03206v1/x9.png" alt="消融实验"></p>
<blockquote>
<p><strong>图9</strong>：GCSL-NF及其变体（消融负反馈、使用L2距离）在点质量导航（左）和LiDAR导航（右）上的成功率对比。完整模型性能最优。</p>
</blockquote>
<p><strong>定性分析</strong>：<br>图10展示了在点质量导航任务中，GCSL与GCSL-NF智能体探索行为的差异。GCSL智能体的探索被限制在初始偏见区域内，而GCSL-NF智能体能够进行更广泛、更有效的探索。</p>
<p><img src="https://arxiv.org/html/2509.03206v1/x10.png" alt="探索行为对比"></p>
<blockquote>
<p><strong>图10</strong>：GCSL（左）与GCSL-NF（右）智能体在点质量导航环境中的探索轨迹可视化。GCSL-NF展现出更广泛、更多样化的探索。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了GCSL-NF，一种新颖的、将失败经验（负面反馈）系统性地整合进GCSL框架的方法，突破了原有方法仅从成功中学习的局限。</li>
<li>引入了一种基于对比学习、与观察结构无关的距离函数学习机制，用于自主评估状态相似性，为提供负反馈奠定了基础。</li>
<li>通过大量实验证明，GCSL-NF能有效克服智能体初始偏见，促进探索，并在多种具有挑战性的稀疏奖励、目标条件任务中（包括观察空间具有欺骗性的任务）显著优于现有GCSL和HER基线方法。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，在具有非常复杂动态或高维状态空间的环境中，学习的距离函数 $p_{\varphi}$ 的准确性可能会下降，从而影响负反馈的质量。此外，对比学习采样和双损失优化可能带来额外的计算开销。</p>
<p><strong>研究启示</strong>：<br>GCSL-NF展示了将监督学习的稳定性与从失败中自主学习的探索性相结合的有效性。这为改进基于自我模仿学习的方法提供了新思路。未来的工作可以探索将负反馈机制扩展到连续动作空间、更复杂的视觉输入任务，或研究如何使距离函数学习对非平稳环境动态更加鲁棒。此外，负反馈的形式（如基于距离的软标签）如何更精确地指导策略优化，也是一个值得深入探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对目标条件监督学习（GCSL）在稀疏奖励任务中的局限：仅从自我成功经验学习会加剧代理偏见，且无法从失败中学习。提出一种新模型，将对比学习原理集成到GCSL框架，使代理能从成功和失败中同时学习。实验表明，该算法克服了初始偏见限制，促进了更多探索行为，从而识别并采用有效策略，在多种挑战性环境中实现了性能提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.03206" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>