<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.20217" target="_blank" rel="noreferrer">2507.20217</a></span>
        <span>作者: Cui, Wei, Wang, Haoyu, Qin, Wenkang, Guo, Yijie, Han, Gang, Zhao, Wen, Cao, Jiahang, Zhang, Zhang, Zhong, Jiaru, Sun, Jingkai, Sun, Pihai, Shi, Shuai, Jiang, Botuo, Ma, Jiahao, Wang, Jiaxu, Cheng, Hao, Liu, Zhichao, Wang, Yang, Zhu, Zheng, Huang, Guan, Tang, Jian, Zhang, Qiang</span>
        <span>日期: 2025/07/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人因其类人结构与运动能力，被认为是融入人类环境最具前景的通用机器人。其视觉感知系统正朝着多模态融合、优化的空间表示和集成智能决策的方向发展。当前主流范式包括基于视觉的感知系统、多模态融合感知系统、端到端感知与决策系统以及基于空间表示的感知系统。其中，基于占用（Occupancy）的表示因其能同时编码丰富的语义和三维几何信息，成为增强环境理解的关键方向，尤其适用于需要全面环境感知的下游任务，如任务规划和导航。</p>
<p>然而，将占用感知应用于人形机器人面临独特挑战。首先，人形机器人的结构特性（如运动学干扰、遮挡）使得传感器布局设计复杂，现有自动驾驶领域的占用感知方法主要针对长距离、米级分辨率场景，忽略了人形机器人对近场（10米内）分米级高分辨率感知的需求。其次，缺乏专门针对人形机器人的全景占用数据集，这限制了相关模型的训练与评估。此外，现有方法在融合多模态数据（RGB、深度、LiDAR）和处理动态、非结构化人类交互场景方面仍有不足。</p>
<p>本文针对上述痛点，提出了“Humanoid Occupancy”系统，这是一个集成了硬件和软件组件、数据采集设备和专用标注流程的广义多模态占用感知系统。核心思路是：通过创新的传感器布局策略解决运动学干扰问题；构建首个专门为人形机器人的全景占用数据集；并设计一个融合多模态特征与时序信息的网络，以生成包含占用状态和语义标签的网格化占用输出，从而为人形机器人提供统一、高效且信息丰富的环境感知能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>Humanoid Occupancy系统是一个完整的感知解决方案，包含硬件设计、数据集构建和多模态融合网络三个核心阶段。整体目标是为人形机器人提供能够同时支持操作、移动和导航任务的占用感知。</p>
<p><img src="https://arxiv.org/html/2507.20217v2/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：Humanoid Occupancy网络整体架构。网络接收6个针孔相机图像和LiDAR点云作为输入，分别通过相机编码器和LiDAR编码器提取特征。通过LiDAR到相机的交叉注意力机制进行多模态特征融合，随后进行时序特征融合以聚合历史信息。最终，通过BEV编码器和解码头预测3D占用网格。</p>
</blockquote>
<p><strong>1. 传感器布局与数据采集</strong><br>系统在“天工”人形机器人平台上实现。传感器套件包括6个RGB相机和一个40线360度全向LiDAR。相机水平视场角118°，垂直视场角92°，采用前、后各一个，两侧各两个的布局方式；LiDAR垂直视场角为59°。此布局旨在最大化环境覆盖，同时最小化因机器人自身结构（如手臂）造成的遮挡和干扰。</p>
<p><img src="https://arxiv.org/html/2507.20217v2/x4.png" alt="硬件结构"></p>
<blockquote>
<p><strong>图4</strong>：人形机器人头部传感器硬件布局示意图。(a)侧视图，(b)等距视图，展示了6个相机和一个LiDAR的安装位置。</p>
</blockquote>
<p>由于直接使用机器人采集数据成本高、难度大，本文创新性地采用可穿戴设备进行数据采集。该设备复刻了机器人上的传感器配置，由身高约160cm的数据采集员佩戴于头部，并配备颈部稳定器以减少头部晃动带来的传感器姿态不稳定。这种方法极大地降低了数据采集的成本和难度，并力求使采集数据的分布接近真实机器人数据。</p>
<p><strong>2. 标注流程</strong><br>构建了首个针对人形机器人的全景占用数据集（HumanoidOcc）。数据场景分为家庭、工业和户外三类，每类定义了不同的点级语义类别（如表1所示）。标注流程（图5）包括两个主要部分：对动态目标（行人、自行车、车辆）进行3D边界框标注；对静态场景进行点级语义标注。对于姿态特殊的行人，还需在边界框内进行逐点标注以区分行人与其他物体。</p>
<p><img src="https://arxiv.org/html/2507.20217v2/x5.png" alt="标注流程"></p>
<blockquote>
<p><strong>图5</strong>：占用真值生成流程。首先进行3D边界框标注和语义分割标注。多帧LiDAR点云通过边界框对齐拼接动态物体，并通过运动对齐累积静态场景。合并后的点云被体素化，生成带有占用状态和语义标签的3D占用网格真值。</p>
</blockquote>
<p>生成占用真值时，将多帧静态背景点云对齐到逐帧的自我坐标系中，然后根据动态目标的逐帧姿态将动态前景点云拼接进去。最后，直接对合并后的点云进行体素化，得到最终的占用真值，无需进行泊松重建。</p>
<p><strong>3. 多模态融合网络</strong><br>网络基于在自动驾驶领域被广泛验证的鸟瞰图范式，整体架构如图3所示。</p>
<ul>
<li><strong>相机特征编码器</strong>：使用共享的卷积神经网络主干，分别提取6个未去畸变的针孔相机图像的特征。</li>
<li><strong>LiDAR特征编码器</strong>：采用PointPillar方法。将LiDAR点云体素化为非空柱体，通过简化版PointNet风格的多层感知机提取柱体特征，并散射回原始位置形成稀疏的伪BEV特征图，再通过2D卷积骨干网络处理为密集BEV表示。</li>
<li><strong>多模态融合</strong>：参考DeepFusion，采用LiDAR到相机的交叉注意力机制进行融合。以PointPillar提取的LiDAR BEV特征作为查询，6个相机的卷积特征作为键和值。为了更精确地描述从LiDAR到相机的投影关系，使用了可变形注意力进行特征采样，并在投影时考虑相机畸变模型，以消除未去畸变图像特征带来的影响。</li>
<li><strong>历史特征融合</strong>：借鉴BEVDet4D，维护一个动态特征队列存储历史BEV特征。通过自我运动变换将历史特征对齐到当前帧坐标系，然后与当前帧BEV特征沿通道维度拼接，最后通过一个BEV编码器（由ResNet残差块和特征金字塔网络构成）进行融合，以聚合时序信息。</li>
<li><strong>预测头</strong>：遵循FlashOcc的高效设计，通过通道到高度的重分布将2D BEV特征转换为3D体素空间。使用FPN特征进行上采样和合并，然后将整合后的BEV特征图按预定义的高度区间重塑为3D体素，最后通过一个3D卷积块进行处理，以最小计算成本输出最终的3D占用预测。</li>
</ul>
<p><strong>损失函数</strong>：总损失由四部分组成：用于优化占用预测的焦点损失（ℒ_focal）和Lovasz-Softmax损失（ℒ_ls），以及用于优化场景级和类别级指标（几何IoU和语义mIoU）的亲和力损失 ℒ_scal^geo 和 ℒ_scal^sem。</p>
<p><strong>创新点</strong>：1) 专门为人形机器人设计的传感器布局策略，解决了运动学干扰和遮挡问题；2) 首个全景占用数据集的构建，填补了该领域空白；3) 网络针对人形机器人需求进行了优化，包括处理未去畸变图像、高效的时序融合以及专门设计的损失函数。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验设置</strong>：实验在自建的HumanoidOcc数据集上进行，该数据集包含家庭、工业和户外三种场景，提供了密集的体素级占用和语义标注。评估指标包括衡量占用预测准确性的几何IoU，以及衡量语义预测准确性的语义mIoU。</p>
<p><strong>对比方法</strong>：与多种先进的占用感知基线方法进行了对比，包括纯视觉方法（FB-Occ, TPVFormer）、纯LiDAR方法（Cylinder3D）以及多模态融合方法（BEVFusion, OccFusion）。同时，也将本文提出的网络与这些方法的架构在HumanoidOcc数据集上进行了公平比较。</p>
<p><img src="https://arxiv.org/html/2507.20217v2/x6.png" alt="定量结果对比"></p>
<blockquote>
<p><strong>图6</strong>：在HumanoidOcc数据集上的定量结果对比。表格展示了不同方法在几何IoU和语义mIoU上的性能。本文提出的方法（最后一行）在两项指标上均取得了最佳性能，显著优于其他基线。</p>
</blockquote>
<p>关键实验结果：如表6所示，本文提出的Humanoid Occupancy网络在几何IoU和语义mIoU上均达到最优，分别为50.27%和38.19%，显著优于其他对比方法。这证明了其多模态融合与系统设计的有效性。</p>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2507.20217v2/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验研究。(a) 不同模态输入的贡献：仅LiDAR或仅相机性能均下降，融合两者带来最大增益。(b) 时序融合的作用：引入历史帧信息能稳定提升模型性能。(c) 不同损失函数的贡献：组合使用四种损失函数效果最佳。</p>
</blockquote>
<p>消融实验总结了各模块的贡献：1) <strong>多模态融合</strong>：仅使用LiDAR或仅使用相机，性能分别下降约6%和12% mIoU，融合两者带来最大增益。2) <strong>时序融合</strong>：引入历史帧信息能持续提升性能，使用过去3帧时效果最佳。3) <strong>损失函数</strong>：组合使用焦点损失、Lovasz-Softmax损失和两种亲和力损失，性能优于任何单一或部分组合。</p>
<p><strong>定性结果</strong>：<br><img src="https://arxiv.org/html/2507.20217v2/x8.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图8</strong>：在HumanoidOcc数据集上的定性结果对比。与基线方法（BEVFusion, OccFusion）相比，本文方法（Ours）预测的占用网格更完整、细节更清晰，特别是在远处物体和复杂结构（如桌椅）的几何形状与语义区分上表现更好。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.20217v2/x9.png" alt="真实机器人部署"></p>
<blockquote>
<p><strong>图9</strong>：在真实“天工”人形机器人上的部署与感知结果可视化。左图展示了机器人实体与传感器布局，右图展示了实时感知输出的3D占用网格，证明了系统的实用性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>系统化方案</strong>：首次提出并实现了一个专门为人形机器人设计的、广义的多模态占用感知系统（Humanoid Occupancy），涵盖了从传感器硬件布局、数据采集标注到融合感知算法的完整技术链条。</li>
<li><strong>基准数据集</strong>：构建了首个面向人形机器人的全景占用数据集（HumanoidOcc），为后续研究提供了宝贵的基准和资源。</li>
<li><strong>实用化设计</strong>：提出了创新的可穿戴设备数据采集方案和高效的标注流程，解决了人形机器人数据获取成本高的难题；设计了能够处理未去畸变图像、融合时序信息的鲁棒网络。</li>
</ol>
<p><strong>局限性</strong>：论文提及，当前的传感器布局可能并非绝对最优，未来可进一步探索；此外，使用可穿戴设备采集的数据与真实机器人运动时采集的数据之间仍可能存在分布差异。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>传感器布局优化</strong>：可以结合机器人具体任务（如操作 vs. 导航）进行更精细化的传感器布局自动化搜索或可重构设计。</li>
<li><strong>数据仿真与生成</strong>：为了进一步降低数据收集成本，未来可以探索利用仿真引擎或生成式模型来合成大规模、多样化的训练数据。</li>
<li><strong>模型轻量化与实时性</strong>：尽管当前网络已考虑效率，但对于需要高频控制的人形机器人，进一步压缩模型、提升推理速度仍是关键方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Humanoid Occupancy系统，旨在解决人形机器人在环境感知和导航中面临的多模态传感器布局优化、数据收集标注困难以及视觉系统标准化等核心问题。系统采用多模态融合技术与时间信息集成，生成网格化占用输出，编码占用状态和语义标签；通过克服运动干扰和遮挡，建立有效传感器布局策略，并开发首个全景占用数据集。该系统为人形机器人提供了全面的环境理解基础，支持任务规划与导航等下游应用，为通用视觉模块标准化奠定技术基础。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.20217" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>