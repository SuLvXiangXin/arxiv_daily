<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TRACE: Textual Reasoning for Affordance Coordinate Extraction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>TRACE: Textual Reasoning for Affordance Coordinate Extraction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01999" target="_blank" rel="noreferrer">2511.01999</a></span>
        <span>作者: Matthew S. Brown Team</span>
        <span>日期: 2025-11-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型在机器人高层任务规划和指令理解方面展现出巨大潜力，但在将高层指令转化为精确、低层级的空间可承受性以驱动物理操作方面仍存在关键差距。主流方法之一是让VLM预测中间动作表示，如可承受性关键点，但这些方法通常将VLM视为黑盒预测器，缺乏明确、可解释的推理过程。另一种新兴方法是视觉思维链，通过生成中间视觉状态来辅助推理，但这类方法通常计算密集，且依赖于外部工具或生成模型，导致机器人控制流程复杂。</p>
<p>本文针对VLM在空间可承受性预测中缺乏可解释、轻量级推理过程这一痛点，提出了文本推理链的新视角。核心思路是通过构建包含显式文本推理步骤的大规模数据集来微调VLM，使模型在生成最终坐标前先外化其空间推理过程，从而提升预测的精确性、鲁棒性和可解释性。</p>
<h2 id="方法详解">方法详解</h2>
<p>TRACE方法的整体流程是：给定一张图像和一个自然语言指令，模型首先进行多步文本推理，然后生成归一化的目标坐标。推理步骤包括确定目标子类型、建立参考表面、定义目标区域等。</p>
<p><img src="https://arxiv.org/html/2511.01999v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：模型推理流程总览。给定图像和指令，系统启动多步推理过程。模型首先确定目标的子类型，并基于图像建立相关参考表面，然后通过解读指令定义目标区域。该过程利用了图像编码器、分词器和大型语言模型。最后，一个投影MLP生成输出，即识别出的空闲空间内的归一化坐标点。</p>
</blockquote>
<p>核心模型架构由三个部分组成：1) 基础语言模型（主要使用Vicuna-v1.5-13B）；2) 视觉编码器（使用预训练的CLIP-ViT-Large-Patch14-336，从倒数第二层提取视觉特征）；3) 多模态投影器（一个2层MLP，使用GELU激活函数，将视觉令牌映射到语言模型的嵌入空间）。训练时使用了Flash Attention 2以提高效率。</p>
<p><img src="https://arxiv.org/html/2511.01999v1/x1.png" alt="数据集样例"></p>
<blockquote>
<p><strong>图1</strong>：TRACE推理数据集中的一个示例，展示了其整体结构。每个条目包含一张图像和一个需要空间推理的自然语言问题。数据集还提供了解决问题的显式、多步推理过程，包括识别参考对象、确定目标子类型为“放置可承受性”、在器具表面定义搜索空间，并以元组列表形式生成最终坐标。</p>
</blockquote>
<p>方法的创新性体现在其构建的TRACE数据集及相应的训练范式上。该数据集包含20万个训练样本，其中一半（10万个）是核心的推理增强样本。这些样本通过增强RoboPoint的数据生成流程，并利用Gemini API自动为每个图像-指令对生成逐步的文本推理依据。另一半（10万个）是来自LVIS和VQA的标准视觉指令调优样本，以确保与基线模型的数据规模相当。学习任务被构建为自回归预测问题：模型需先生成文本推理（对于TRACE数据）或直接答案，然后输出归一化的2D坐标 <code>{(x_i, y_i) | x_i, y_i ∈ [0,1]}</code>。这种设计旨在教会模型将其推理与最终的空间预测直接联系起来。</p>
<p>与现有方法相比，TRACE的创新点在于用<strong>文本推理链</strong>替代了计算密集的<strong>视觉思维链</strong>。它直接利用VLM的语言优势，通过生成可解释的文本依据来指导空间动作，是一种更轻量且可解释的策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了两个真实世界的基准测试：1) <strong>RoboRefIt</strong>：包含250张杂乱场景的图像，物体仅能通过关系性语言指令区分；2) <strong>Where2Place</strong>：一个具有挑战性的100张图像数据集，用于基于关系性语言识别空闲空间。其中包含一个更难子集 **W2P(h)**，包含30个训练中未见过的关系类型示例。对比的基线方法包括：原始RoboPoint模型、GPT-4o（零样本提示）、SpaceLLaVA（空间推理专用VLM）以及Gemini（零样本提示）。</p>
<p><img src="https://arxiv.org/html/2511.01999v1/x3.png" alt="定性对比"></p>
<blockquote>
<p><strong>图3</strong>：TRACE与其他领先模型在推理数据集样本上的定性对比。指令为“在玻璃容器右侧的空闲区域 pinpoint 几个点”。TRACE模型正确识别了目标区域，而其他模型则存在困难。</p>
</blockquote>
<p>关键定量结果如表1所示。在主要基准Where2Place上，TRACE微调模型（RoboPoint+TRACE）达到了48.1%的准确率，相较于原始RoboPoint的43.9%有显著提升，相对改进为9.6%。在更具挑战性的W2P(h)子集上，提升更为明显（55.0% vs. 46.9%）。TRACE模型在所有基准测试上均优于所有基线。</p>
<p><img src="https://arxiv.org/html/2511.01999v1/Figures/fig4.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：关于推理数据影响的消融研究。图表显示了在用于训练的TRACE推理数据集比例从0%（基线）增加到100%时，在三个基准测试上的性能变化。所有任务的性能都随着推理数据的增加而持续提升，证实了方法的有效性。</p>
</blockquote>
<p>消融实验（使用7B LoRA变体进行）证明了性能提升直接归因于文本推理链数据。如图4所示，随着训练数据中推理数据的比例从0%增加到100%，模型在RoboRefIt上的性能提升了7.5个百分点，在W2P上提升了7.6个百分点，在W2P(h)上更是大幅提升了10.5个百分点。性能与推理数据量呈正相关，尤其是在包含未见关系的W2P(h)上提升最显著，表明显式推理过程对于泛化到新颖复杂指令至关重要。</p>
<p><img src="https://arxiv.org/html/2511.01999v1/x4.png" alt="注意力可视化"></p>
<blockquote>
<p><strong>图5</strong>：针对指令“Find the free space in front of the window on the left.”的模型推理注意力图可视化。注意力热图叠加在输入图像上，展示了四步文本推理过程中注意力的动态变化。在最终生成坐标步骤，图像上的注意力几乎消失，表明模型依赖已完成的文本推理链而非持续的视觉关注来生成最终动作。</p>
</blockquote>
<p>对模型注意力机制的分析（图5）为性能提升提供了可解释性证据。可视化显示，在初始步骤（识别参考对象、定义目标区域），注意力是微弱且分散的。在“确定目标子类型”步骤，注意力开始集中，并在目标区域附近出现明显的高注意力区域。至关重要的是，在最后“生成输出”步骤，预测坐标点的区域几乎没有视觉注意力。这表明模型利用已完成的文本推理链作为动作生成的主要指导，而不是依赖于持续、密集的视觉 grounding。</p>
<p>统计验证表明，在主要W2P基准上的性能提升具有统计显著性（p=0.022 &lt; 0.05）。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了TRACE方法论及相应的大规模数据集，将文本推理链集成到空间可承受性预测中；2) 实验证明，使用该推理数据微调VLM能显著提升其空间理解能力和动作点预测精度，在主要基准上实现了9.6%的相对提升，且性能与推理数据量直接相关；3) 通过注意力可视化，提供了模型进行可解释、多步推理过程的定性证据，表明文本推理链充当了连接模糊语言与精确坐标的中间表示。</p>
<p>论文提到的局限性在于：数据集中推理链是合成生成的，可能无法完全捕捉人类思维的复杂性；模型缺乏显式控制推理过程或报告预测置信度的机制。</p>
<p>这项工作表明，训练VLM进行文本推理是提升基于VLM的机器人控制精度、可靠性和可解释性的一种轻量而有效的策略。对后续研究的启示包括：探索更复杂、结构更自由的推理生成方法；将CoR框架扩展到多步操作、导航等更广泛的机器人任务中；以及利用注意力图的洞察来进一步提升模型的可靠性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型(VLMs)将高级指令转化为精确机器人操作坐标时空间推理能力不足的问题，提出了TRACE方法。该方法的核心是引入文本推理链，通过构建包含指令与显式推理文本的TRACE数据集来微调VLM，使其在预测坐标前先进行外部化的文本推理。实验表明，该方法在Where2Place基准测试上达到48.1%的准确率，相对提升9.6%，且在更具挑战性的子集上达到55.0%，性能提升与推理数据量直接相关。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01999" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>