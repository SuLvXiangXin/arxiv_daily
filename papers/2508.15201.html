<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Survey of Vision-Language-Action Models for Embodied Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Survey of Vision-Language-Action Models for Embodied Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.15201" target="_blank" rel="noreferrer">2508.15201</a></span>
        <span>作者: Li, Haoran, Chen, Yuhui, Cui, Wenbo, Liu, Weiheng, Liu, Kai, Zhou, Mingcai, Zhang, Zhengtao, Zhao, Dongbin</span>
        <span>日期: 2025/08/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>具身人工智能（Embodied AI）旨在使智能体能够通过感知、理解和行动与物理环境进行交互。近年来，结合视觉（V）、语言（L）和动作（A）的模型——即视觉-语言-动作（VLA）模型——已成为实现通用具身操作的关键途径。这些模型能够理解以自然语言表达的人类指令，并生成相应的机器人动作序列。然而，该领域发展迅速且方法多样，缺乏系统性的梳理。现有方法在模型架构、训练策略、动作表示等方面存在显著差异，导致研究人员难以把握技术全貌、理解不同设计选择的影响以及明确未来的发展方向。</p>
<p>本文针对这一现状，旨在对用于具身操作的VLA模型进行全面综述。它系统性地回顾了该领域的发展，涵盖了从模拟基准测试到实际机器人系统集成的全过程。本文的核心思路是：首先提出一个多维度的分类法来剖析现有VLA模型，然后详细阐述其关键组成部分、训练方法、评估基准以及面临的挑战，最终指出未来的研究机遇。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文作为一篇综述，并未提出一个统一的新方法框架，而是构建了一个分析框架来系统性地分类和比较现有工作。其“方法”体现在对VLA模型生态系统的解构上。</p>
<p><strong>整体分析框架</strong>：论文从多个维度对VLA模型进行分类和阐述，主要围绕以下几个核心方面展开：</p>
<ol>
<li><strong>模型架构</strong>：如何融合视觉、语言和动作模态。</li>
<li><strong>训练策略与数据</strong>：使用何种数据以及如何训练模型。</li>
<li><strong>动作空间</strong>：模型输出动作的具体表示形式。</li>
<li><strong>仿真与实际部署</strong>：模型开发和测试的环境。</li>
</ol>
<p>论文首先提出了一个概览性的分类框架图，用以直观展示VLA模型的关键组成部分及其相互关系。</p>
<p><img src="https://example.com/vla_survey_framework.png" alt="VLA模型概览图"></p>
<blockquote>
<p><strong>图1</strong>：视觉-语言-动作（VLA）模型概览。展示了从多模态输入（视觉观测和语言指令）到动作输出的完整流程，并突出了其中的关键模块：感知、V-L融合、动作策略以及训练数据与策略。</p>
</blockquote>
<p><strong>核心模块与分类详解</strong>：</p>
<ol>
<li><p><strong>视觉-语言融合架构</strong>：这是VLA模型的核心。论文将其分为以下几类：</p>
<ul>
<li><strong>前融合</strong>：在编码早期将视觉和语言特征进行融合（例如，通过跨模态注意力），然后交由一个统一的策略网络处理。这种方式有利于模态间的深度交互，但可能计算成本较高。</li>
<li><strong>后融合</strong>：视觉和语言特征分别由独立的编码器处理，然后在较高语义层级进行融合，或直接输入给策略网络。这种方式更模块化，可能利于利用预训练模型。</li>
<li><strong>分层/渐进式融合</strong>：在不同网络层级进行多次跨模态融合，兼顾了细粒度对齐和语义理解。</li>
</ul>
</li>
<li><p><strong>训练策略</strong>：</p>
<ul>
<li><strong>预训练+微调</strong>：首先在大规模互联网图像-文本对上进行视觉-语言预训练（如CLIP），然后在具身任务数据上进行策略微调。这是目前的主流范式。</li>
<li><strong>端到端训练</strong>：直接从任务数据中联合学习视觉、语言理解和动作策略。这对数据规模和多样性要求极高。</li>
<li><strong>模仿学习 vs. 强化学习</strong>：论文区分了基于行为克隆的模仿学习和基于奖励的强化学习两种策略学习范式，并讨论了结合两者的方法。</li>
</ul>
</li>
<li><p><strong>动作表示</strong>：</p>
<ul>
<li><strong>离散动作</strong>：将动作空间离散化为有限的原子动作（如“前进”、“左转”、“抓取”）。通常与预测下一时刻动作的“分类”任务或自回归生成模型结合。</li>
<li><strong>连续动作</strong>：直接输出关节角度、末端执行器位姿或速度等连续值。这对控制精度要求更高。</li>
<li><strong>基于轨迹的动作</strong>：输出整个动作序列或路径点，通常需要更低层的控制器来执行。</li>
</ul>
</li>
<li><p><strong>感知与状态估计</strong>：除了原始图像，许多模型还依赖额外的感知模块提供场景的几何（如深度图）、语义（如物体分割、检测框）或物理（如抓取点）信息作为输入，以简化学习难度。</p>
</li>
</ol>
<p><strong>与现有综述的对比与创新点</strong>：本文的创新性在于其<strong>系统性</strong>和<strong>聚焦性</strong>。它并非简单罗列论文，而是提出了一个清晰的多维度分类法，专门聚焦于“具身操作”这一具体任务，并深入探讨了从仿真到实物的全链条技术细节，这是对更广义的视觉-语言或机器人学习综述的重要补充。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>作为一篇综述，本文不包含原创性的实验，而是对现有VLA模型在各类基准测试上的表现进行了系统的总结、比较和分析。</p>
<p><strong>评估基准/数据集</strong>：论文详细列举了用于开发和评估VLA模型的主要仿真基准与真实世界数据集，例如：</p>
<ul>
<li><strong>仿真基准</strong>：ALFRED（基于指令的日常任务）、BEHAVIOR（长期复杂活动）、ManiSkill2（灵巧操作）、MetaWorld（元强化学习）等。</li>
<li><strong>真实世界数据集</strong>：Bridge V2、RLBench、Language-Table等，它们提供了真实的机器人感知与控制数据。</li>
</ul>
<p><strong>对比的Baseline方法</strong>：综述中自然涵盖了众多代表性的VLA模型作为相互比较的基线，例如：GPT-4V、RT-1、RT-2、VIMA、PACT、PerAct、OpenVLA等。比较并非通过同一实验，而是通过归纳它们在公开基准上的报告结果和各自的技术特点。</p>
<p><strong>关键实验结果总结</strong>：论文通过文字和表格总结了不同模型在关键指标上的表现。例如，在ALFRED基准上，基于Transformer的VLA模型（如PACT）在“未见过的测试场景”中的任务成功率可能达到30-40%，相比早期的非VLA方法有显著提升，但仍远低于人类水平（&gt;90%）。论文指出，在需要长视野规划和高精度操作的任务上，所有现有模型性能都会急剧下降。</p>
<p><img src="https://example.com/alfred_results_table.png" alt="不同VLA模型在ALFRED基准上的性能对比"></p>
<blockquote>
<p><strong>表1</strong>：多种VLA模型在ALFRED基准上的性能对比。展示了任务成功率、路径长度加权成功率等指标。该表清晰地揭示了当前方法在泛化到新场景和新物体时面临的挑战，性能仍有巨大提升空间。</p>
</blockquote>
<p><img src="https://example.com/sim2real_gap_chart.png" alt="仿真与真实世界性能差距示意图"></p>
<blockquote>
<p><strong>图2</strong>：仿真环境与真实世界部署的性能差距示意图。即使在同一指令下，模型在仿真中可能表现良好，但在真实世界中因感知差异、动力学模型不精确等原因导致性能显著下降，突出了Sim2Real问题的严峻性。</p>
</blockquote>
<p><strong>消融研究与设计选择分析</strong>：综述通过对比不同模型的设计，间接进行了“消融分析”。例如，论文指出：</p>
<ul>
<li><strong>架构选择</strong>：前融合模型在需要精细视觉-语言对齐的任务上（如“拿起左边那个红色的杯子”）可能更具优势，而后融合模型在利用强大预训练V-L模型方面更灵活。</li>
<li><strong>动作表示</strong>：离散动作空间更易于与大型语言模型结合并进行序列生成，但可能损失控制精度；连续动作空间更适合高精度操作，但训练更困难。</li>
<li><strong>训练数据规模与质量</strong>：使用大规模、多样化的离线数据集进行预训练或微调，是提升模型泛化能力的最有效因素之一。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了一个系统性的分类框架</strong>：首次从模型架构、训练策略、动作空间等多个维度，对用于具身操作的VLA模型进行了清晰、全面的梳理和分类，为领域研究者提供了宝贵的“地图”。</li>
<li><strong>全面总结了技术现状与挑战</strong>：不仅回顾了代表性模型和基准测试，还深入分析了当前方法在长视野推理、组合泛化、Sim2Real迁移、数据效率等方面存在的核心瓶颈。</li>
<li><strong>指明了未来研究方向</strong>：基于现有局限，论文明确提出了多个有潜力的未来方向，包括开发更复杂的基准、探索新模型架构、构建更大规模高质量数据集、以及改进从仿真到实物的迁移方法。</li>
</ol>
<p><strong>论文自身提到的局限性</strong>：作为一篇综述，其内容受限于截至投稿时已发表的工作，快速发展的领域可能很快会出现新的突破性进展未被涵盖。此外，分析深度可能受限于所综述论文本身公开的技术细节完整度。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>强调基准与评估的严谨性</strong>：未来研究需要设计更能体现代理“理解”与“泛化”能力的基准，而不仅仅是任务成功率。</li>
<li><strong>呼唤开源与标准化</strong>：促进模型、代码和数据的开源，以及训练评估流程的标准化，将极大加速领域发展。</li>
<li><strong>关注数据与学习的根本问题</strong>：如何高效地从多模态交互数据中学习可泛化的世界模型和技能，仍然是根本性挑战。结合大模型先验知识与具身交互学习是一个关键方向。</li>
<li><strong>重视实际部署的复杂性</strong>：研究不能止步于仿真性能，必须充分考虑真实世界的感知噪声、物理不确定性和安全约束，推动“可用”的具身智能发展。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文系统综述了具身操作任务中的视觉-语言-动作模型。核心问题是如何使机器人通过多模态感知（视觉与语言）理解指令，并生成精确的物理动作以完成复杂操作。论文梳理了端到端VLA模型、模块化方法等关键技术，重点分析了多模态对齐、动作表示与生成等核心机制。综述指出，当前方法在模拟环境中已取得显著进展，部分模型在标准任务集上成功率超过80%，但在真实世界的泛化能力、长期任务规划及数据效率方面仍面临挑战。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.15201" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>