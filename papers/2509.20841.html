<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.20841" target="_blank" rel="noreferrer">2509.20841</a></span>
        <span>作者: Kui Jia Team</span>
        <span>日期: 2025-09-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域的主流方法可分为三类：一是将深度神经网络作为功能子模块嵌入传统的模块化管道，这提升了感知能力，但模块间的信息瓶颈和优化目标不一致会导致信息丢失和特征错位；二是端到端的像素到力矩学习策略，旨在直接映射原始感官输入到低级控制动作，以规避上述问题；三是受大语言模型和视觉语言模型启发而兴起的视觉-语言-动作模型，期望通过大规模预训练获得强大的泛化能力。然而，现有的端到端方法，包括VLA模型，在实际大规模部署中，其可靠性、精度和跨场景/物体的泛化能力仍显不足，有时甚至不及精心设计的传统模块化管道。</p>
<p>本文针对端到端策略在保持泛化潜力的同时，难以满足实际应用对精度和可靠性要求的核心痛点，提出了一个基于“可负担性”的新视角。具体而言，本文将可负担性定义为任务相关的、有语义意义的局部物体部件，并将其表示为语义化的、任务特定的定向关键点。核心思路是提出一种新颖的“移动定向关键点链”动作表示，并以此为基础构建可端到端训练的神经策略，旨在实现通用、精确且可靠的机器人操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架核心是一种新颖的基于可负担性的动作表示。整体流程是：给定场景观测（点云或图像）和任务描述，策略网络预测需要操作的对象、定义在该对象上的可负担性关键点，以及为了使任务完成，该关键点需要移动至的目标位姿序列。</p>
<p><img src="https://arxiv.org/html/2509.20841v1/x4.png" alt="方法框架"></p>
<blockquote>
<p><strong>图4</strong>：动作预测网络架构。网络以场景点云和任务描述（编码后的隐藏特征）为输入。采用基于分数匹配的网络来刻画机器人动作候选的多模态分布，通过迭代去噪过程预测每个任务阶段的可负担性关键点<code>T_affordance</code>和动作序列<code>T_action_seq</code>。</p>
</blockquote>
<p><strong>核心动作表示：链式移动定向关键点</strong><br>该方法的基本动作单元是<code>(o_manipulated, T_affordance, T_action)</code>。<code>o_manipulated</code>是机器人拥有或将拥有控制权的环境部分（如夹爪、被抓物体）。<code>T_affordance</code>是一个定义在<code>o_manipulated</code>上的、与任务相关的6自由度定向关键点。<code>T_action</code>则表示一个目标位姿：当<code>T_affordance</code>被操纵至与<code>T_action</code>对齐时，任务即告完成。此表示具有通用性，当<code>o_manipulated</code>固定为夹爪且<code>T_affordance</code>为工具中心点时，即退化为标准的末端执行器位姿动作表示。</p>
<p><strong>网络架构与实现细节</strong><br>框架包含两个主要网络：</p>
<ol>
<li><strong>任务规划网络</strong>：以RGB图像和全局任务描述<code>f_task_global</code>为输入，输出一个子任务列表<code>List[t_stage]</code>，其中每个<code>t_stage</code>包含子任务描述<code>f_task_stage</code>、待操作对象<code>o_manipulated</code>以及动作应关注的环境区域<code>o_env</code>。该网络通过微调Groma VLM实现。</li>
<li><strong>动作预测网络</strong>：以场景点云和任务规划网络输出的<code>(h_task_stage, o_manipulated, o_env)</code>为输入，联合预测所有子任务的<code>T_affordance</code>和动作序列<code>T_action_seq</code>。为处理动作的多模态性（如多个可行的抓取位姿），该网络采用了一个基于Transformer架构的分数匹配网络（一种扩散模型变体）。它通过交叉注意力层融合点云特征和任务特征，并利用注意力掩码处理不同阶段和序列长度。网络输出经过解码后得到具体的SE(3)位姿。</li>
</ol>
<p><strong>方法扩展与创新点</strong></p>
<ul>
<li><strong>多阶段任务</strong>：可将基本公式扩展为输出一个动作阶段列表（公式2），每个阶段有自己的<code>(f_task_stage, o_manipulated, T_affordance, T_action)</code>，从而处理长视野任务。</li>
<li><strong>多模态行为</strong>：通过分数匹配网络刻画动作候选的分布，以处理同一任务下多个可行的<code>o_manipulated</code>实例或多个<code>(T_affordance, T_action)</code>对。</li>
<li><strong>轨迹动作</strong>：将单个<code>T_action</code>扩展为序列<code>T_action_seq</code>（公式3），以描述需要连续路径的任务（如切割、绘图）。</li>
<li><strong>运动生成</strong>：根据预测的<code>T_affordance</code>和<code>T_action_seq</code>，可将其转换为末端执行器轨迹，再通过传统运动规划算法或学习型轨迹生成策略得到关节空间轨迹。</li>
</ul>
<p>与现有方法相比，本文的创新点具体体现在：1) 提出了一种通用、统一且可解释的动作表示，将抓取检测等多种技能纳入同一框架；2) 定向关键点的使用使其能自然地泛化到不同形状和尺寸的物体；3) 该框架能轻松处理多阶段任务、多模态行为和非刚性物体。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在模拟和真实世界中进行，使用了6自由度Rokae SR5机械臂和平行夹爪。感知采用安装在末端执行器上的RGBD传感器。使用的数据集包括来自现有工作的抓取点云数据集。</p>
<p><strong>对比基准与任务</strong>：论文主要进行了方法能力的演示和验证，展示了其处理多样化任务和泛化到不同物体的能力。具体任务包括：平行夹爪抓取、将杯子通过杯把悬挂到架子上、将电缆插入夹具、将物体稳定放置在桌面上。</p>
<p><img src="https://arxiv.org/html/2509.20841v1/x5.png" alt="实验概览"></p>
<blockquote>
<p><strong>图5</strong>：实验任务概览。(a) 将不同形状的杯子悬挂到架子上；(b) 将不同尺寸的电缆插入夹具；(c) 在模拟中将物体稳定放置到桌上。这些实验展示了方法在统一框架下处理不同任务，并应对物体大幅形状变化的能力。</p>
</blockquote>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>平行夹爪抓取</strong>：在测试集上达到了<strong>98.5%</strong> 的抓取成功率，证明了其高精度。</li>
<li><strong>杯子悬挂任务</strong>：在10次试验中，对5个不同形状的杯子取得了<strong>8次成功</strong>。</li>
<li><strong>电缆插入任务</strong>：在10次试验中，对5种不同尺寸的电缆取得了<strong>7次成功</strong>。</li>
<li><strong>物体放置任务</strong>：在模拟中，对来自YCB数据集的10个不同物体进行了测试。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.20841v1/x6.png" alt="预测分布可视化"></p>
<blockquote>
<p><strong>图6</strong>：预测动作分布的可视化。(a) 抓取任务，仅可视化<code>T_action</code>（抓取位姿）；(b-d) 悬挂、插入、放置任务，同时可视化<code>T_affordance</code>（物体上的关键点）和<code>T_action_seq</code>的一个代表性帧。该图直观展示了网络预测的多模态动作候选。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.20841v1/x7.png" alt="失败案例分析"></p>
<blockquote>
<p><strong>图7</strong>：任务执行中的几种失败模式。(a) 任务规划网络检测失败（未检测到目标物体）；(b) 运动生成中难以考虑的可达性限制（受相机线缆妨碍）。这些案例指出了当前方法的局限性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.20841v1/x1.png" alt="机器人控制权图示"></p>
<blockquote>
<p><strong>图1</strong>：机器人控制权图示。(a) 机器人可控制末端执行器的6自由度运动；(b) 机器人可控制被抓刚性物体的6自由度运动；(c) 机器人可控制被抓可变形物体局部抓取片（绿色掩码所示）的6自由度运动。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.20841v1/x2.png" alt="倒水任务示例"></p>
<blockquote>
<p><strong>图2</strong>：以倒水任务为例阐述CoMOK公式。任务分为抓取杯子、倒水、放置杯子三个阶段，每个阶段对<code>o_manipulated</code>、<code>T_affordance</code>和<code>T_action</code>有不同的定义，并展示了其对不同杯子的泛化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.20841v1/x3.png" alt="轨迹动作示例"></p>
<blockquote>
<p><strong>图3</strong>：轨迹动作示例。(a,b) 切割水果需要稀疏的轨迹（两个动作帧）；(c,d) 绘图需要密集的轨迹（数百个动作帧）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种新颖、通用且可解释的机器人操作动作表示——“链式移动定向关键点”，它将多种操作任务统一在一个框架内。</li>
<li>基于该表示，设计了一个端到端的神经策略架构，利用分数匹配网络处理动作的多模态性，并展示了其在模拟和真实任务中实现亚厘米级精度和良好泛化能力。</li>
<li>该方法能够自然地处理多阶段任务、多模态机器人行为以及刚性/非刚性物体，扩展了基于关键点的操作管道的适用范围。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：1) 任务规划网络可能产生检测失败（如图7a）；2) 运动生成受限于物理约束（如可达性、碰撞），有时难以完美实现网络预测的动作（如图7b）；3) 在真实世界实验中，学习型运动生成器的性能不尽如人意，不得不回退到传统运动规划算法。</p>
<p><strong>后续启示</strong>：这项工作为构建可靠且通用的端到端操作策略提供了一个有前景的新范式。后续研究可沿着以下几个方向深入：1) 提升任务规划与动作预测的鲁棒性和协同性；2) 开发更高效、鲁棒的学习型运动生成器，以更好地衔接高层动作表示与底层关节控制；3) 探索在更复杂、动态的环境中的应用，并进一步验证其大规模部署的潜力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有端到端机器人操作策略泛化性、精确性和可靠性不足的问题，提出了一种新颖的**Chain of Moving Oriented Keypoints (CoMOK)** 动作表示方法。该方法扩展了标准的末端执行器姿态表示，能以统一框架支持多样化的操作任务。其核心“定向关键点”设计使策略能自然泛化至不同形状尺寸的物体，并实现**亚厘米级精度**，同时易于处理多阶段任务与可变形物体。实验验证了该方法的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.20841" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>