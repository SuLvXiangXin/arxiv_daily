<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.11393" target="_blank" rel="noreferrer">2602.11393</a></span>
        <span>作者: Christopher G. Atkeson Team</span>
        <span>日期: 2026-02-11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>从人类视频中学习技能是减少机器人演示需求的有效途径。主流方法通过从视频中学习反馈信号（如奖励函数），然后通过规划或强化学习在机器人上优化该函数。其中，最常见的方法是估计视觉状态的长时期价值V(s)，将其定义为该状态到目标状态（视频最后一帧）的时间距离。然而，这类方法存在关键局限性：它们假设演示中的每一帧都产生固定成本，因此对演示数据中的噪声（如人类的多任务处理、犹豫、中断等非时间最优行为）敏感，这些噪声会负面偏置所有先前状态的价值估计。此外，学习到的价值函数必须跨越人类与机器人之间巨大的具身和环境分布差异进行迁移。</p>
<p>本文针对上述痛点，提出了一个新的视角：将奖励学习视为对演示者每一步短期偏好的建模问题，而非估计长期价值。具体而言，本文通过预测任务相关物体上跟踪点在帧间的运动来建模人类偏好，并基于机器人行为中预测运动与观测运动的一致性来定义每一步的奖励。核心思路是：从大量人类自我中心视频中学习一个运动预测模型来捕捉人类偏好，利用该模型为机器人行为生成密集的每一步奖励，并采用一种改进的残差强化学习框架，结合少量机器人演示，高效地优化策略以最大化该奖励。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架分为三个阶段：1) 从人类视频数据中学习运动预测模型；2) 使用该模型为机器人行为片段计算奖励；3) 利用奖励信号，在少量机器人演示的基础上，通过残差强化学习优化策略。</p>
<p><img src="https://arxiv.org/html/2602.11393v1/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧：从人类自我中心视频中提取点轨迹和物体掩码，用于训练运动预测Transformer (Fθ)。右侧：使用少量机器人演示训练一个基础行为克隆策略（π_base），然后利用运动预测模型为在线交互数据计算奖励，通过残差强化学习框架学习一个残差策略（π_RL）来修正基础策略。</p>
</blockquote>
<p><strong>核心模块一：运动预测奖励模型</strong><br>首先，从给定任务𝒯的人类演示视频集𝒱中学习一个运动预测模型：P_{t+1} = F_θ(o_t, P_t, t/T)。其中，o_t是t时刻的视觉观察，P_t是一组跟踪点的位置，t/T是归一化的任务进度指示器。该模型使用基于DiT架构的修改版Transformer实现，关键创新在于通过交叉注意力层融入了冻结的DinoV2预训练视觉Transformer的视觉token，以提供鲁棒的视觉特征和空间信息。训练损失为简单的回归损失：l(θ) = ||F_θ(o_t, P_t, t/T) - P_{t+1}||²。</p>
<p>在为机器人行为计算奖励时，首先在机器人视频中跟踪一组点⟨P_0^track, …, P_T^track⟩。对于每一帧t，利用训练好的F_θ模型，基于当前观察o_t和跟踪点P_t^track，预测下一帧的点位置P_{t+1}^pred。奖励r_t通过比较预测点位移Δp_t^pred = p_{t+1}^pred - p_t^pred 与观测点位移Δp_t^track = p_{t+1}^track - p_t^track 的对齐程度来计算，具体为所有点对之间余弦相似度的正值之和（公式1）。该奖励函数鼓励机器人的行为产生与人类偏好预测一致的对象运动结果。</p>
<p><strong>核心模块二：数据预处理与奖励计算</strong><br>为了从嘈杂、非结构化的人类视频中学习，需要进行预处理：1) 根据语言标签从Ego4D和Epic Kitchens数据集中检索相关视频；2) 使用OWLViT2、SAM2和Cutie检测、分割并跟踪任务相关物体；3) 修剪视频中任务相关物体不可见的部分；4) 跟踪一组网格点，并根据物体掩码将其分为物体点和背景点；5) 为补偿自我中心视频中剧烈的相机运动，从所有点位移中减去背景点位移的均值。最终，模型使用加权混合的物体点和背景点进行训练。</p>
<p><strong>核心模块三：残差强化学习框架</strong><br>为了基于学习到的奖励函数优化策略π，本文采用了一种样本高效且稳定的残差强化学习框架。首先，使用少量机器人演示数据𝒟训练一个基础行为克隆策略π_base（如扩散策略），并将其冻结。然后，学习一个残差策略π_RL，其输出为修正动作Δa。机器人执行的动作由π(s) = π_base(s) + απ_RL(s)给出，其中α为限制残差动作幅度的标量常数。<br>该框架借鉴了RLPD和SERL等工作的设计：1) 使用多个批评家网络并取最小值，并在其中使用LayerNorm以防止在数据稀疏区域高估价值；2) 在训练期间从离线数据集𝒟和在线缓冲区ℬ中均匀采样；3) 使用奖励模型标注的离线数据预训练批评家网络以热启动在线微调。对于𝒟中的状态-动作对(s_t, a_t)，将a_t替换为Δa_t = a_t - π_base(s_t)，使学习目标一致。这些正则化设计确保了在真实世界微调场景中的稳定性和效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境和真实机器人上进行了评估。模拟实验使用Franka Kitchen环境中的“开微波炉”和“开橱柜”任务。真实机器人实验使用Franka Emika Panda机械臂，在三个任务上进行：“开微波炉”、“开橱柜”和“关抽屉”。人类视频数据来自Ego4D和Epic Kitchens数据集。每个真实世界任务仅使用10次机器人演示进行初始化。</p>
<p><strong>对比方法</strong>：1) <strong>稀疏奖励</strong>：任务成功时奖励为1，否则为0。2) <strong>手工密集奖励</strong>：使用模拟中的特权信息衡量到目标的进度。3) <strong>VIP</strong>：一种基于时间距离的视觉奖励学习方法，代表当前主流方法。4) <strong>MPR</strong>：本文提出的运动预测奖励方法。</p>
<p><img src="https://arxiv.org/html/2602.11393v1/x2.png" alt="模拟任务成功率"></p>
<blockquote>
<p><strong>图2</strong>：在模拟的Franka Kitchen任务中，不同奖励模型随训练步长的成功率对比。MPR在“开微波炉”任务上优于VIP，在“开橱柜”任务上与VIP表现相当，且两者均能匹配手工稀疏奖励的性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11393v1/x4.png" alt="真实任务成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：在真实机器人任务上，经过1小时在线微调后，使用MPR奖励的策略相比基础策略和VIP奖励策略的成功率提升。MPR在所有任务上均显著优于VIP，平均提升超过30%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11393v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融实验展示了MPR各组件的重要性。使用背景点补偿相机运动（w/ BC）和使用物体点（w/ OP）均能提升性能，结合两者（Full MPR）效果最佳。仅使用背景点（w/o OP）或仅使用物体点但不补偿相机运动（w/o BC）性能下降。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11393v1/x6.png" alt="奖励信号分析"></p>
<blockquote>
<p><strong>图6</strong>：在50次成功的“开微波炉”演示上，MPR、VIP和手工密集奖励的平均信号对比。MPR奖励在任务进展中提供更密集、信息更丰富的反馈，与手工奖励趋势一致，而VIP奖励在早期提供的信息较少。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11393v1/x7.png" alt="点类型贡献分析"></p>
<blockquote>
<p><strong>图7</strong>：分析预测中使用不同点类型（物体点、背景点）对奖励信号的影响。物体点对奖励的贡献在任务结束时达到峰值，与任务完成高度相关；背景点的贡献则更早出现，可能对应接近物体的行为。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11393v1/x8.png" alt="真实机器人学习曲线"></p>
<blockquote>
<p><strong>图8</strong>：在真实机器人“开微波炉”任务上的在线学习曲线。使用MPR奖励的残差RL能在一小时内持续提升成功率，而使用VIP奖励的学习不稳定且提升有限。</p>
</blockquote>
<p><strong>关键实验结果总结</strong>：</p>
<ol>
<li><strong>模拟实验</strong>：在“开微波炉”任务上，MPR的最终成功率（约85%）显著高于VIP（约65%），并接近手工密集奖励（约90%）。在“开橱柜”任务上，MPR与VIP均能达到接近100%的成功率。</li>
<li><strong>真实机器人实验</strong>：经过1小时在线微调，MPR将“开微波炉”任务的成功率从基础策略的40%提升至80%，“开橱柜”从20%提升至75%，“关抽屉”从45%提升至85%。相比之下，VIP的提升幅度小得多（微波炉：40%→55%；橱柜：20%→40%；抽屉：45%→60%）。</li>
<li><strong>消融实验</strong>：验证了方法组件的必要性。使用背景点补偿相机运动和使用物体点分别带来了显著性能提升。完整的MPR（结合两者）效果最好。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了一种新的基于视觉运动预测的奖励学习范式</strong>：通过预测人类演示中任务相关对象的点运动来建模短期偏好，并以此生成密集的每一步奖励。该方法对演示中的噪声和跨具身分布偏移更具鲁棒性，且直接鼓励匹配任务结果而非动作。</li>
<li><strong>设计了一个结合残差强化学习的样本高效在线微调框架</strong>：通过冻结基础行为克隆策略、学习残差修正、并采用价值函数正则化和离线数据预热等技巧，实现了在真实机器人上仅用1小时交互时间即可稳定提升策略性能。</li>
<li><strong>在模拟和真实世界任务上进行了全面验证</strong>：证明了MPR奖励优于基于时间距离的VIP方法，并能利用大量人类视频和极少量机器人演示，使机器人快速学习复杂操作技能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于现成的点跟踪和分割模型的性能，在复杂场景或快速运动下可能失效。此外，奖励函数基于点运动的一致性，可能无法捕捉所有任务相关的约束（如力控、安全约束）。</p>
<p><strong>对后续研究的启示</strong>：本工作表明，关注跨具身智能的、基于结果的短期偏好建模是有效的奖励学习方向。未来的工作可以探索更鲁棒的视觉表征来替代点跟踪，或将MPR与其他奖励信号（如高层任务成功指示）相结合，以构建更全面的奖励函数。此外，所展示的样本高效、稳定的真实世界RL框架为后续的在线机器人学习研究提供了有价值的参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出一种从人类第一人称视频中学习机器人技能的方法，旨在减少机器人演示需求并克服 embodiment 和环境差距。核心技术包括：通过预测后续图像中跟踪点的运动来建模人类偏好，定义奖励函数为机器人行为中预测与观察物体运动的一致性，并使用改进的 Soft Actor Critic 算法进行策略优化。实验表明，在模拟和真实机器人上，该方法学习的策略在多个任务中匹配或优于先前方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.11393" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>