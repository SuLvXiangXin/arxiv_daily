<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ManiAgent: An Agentic Framework for General Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ManiAgent: An Agentic Framework for General Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.11660" target="_blank" rel="noreferrer">2510.11660</a></span>
        <span>作者: Xudong Liu Team</span>
        <span>日期: 2025-10-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为机器人操作领域一种有前景的方法，其通过对机器人演示数据进行模仿学习来微调大语言模型，实现从自然语言指令和视觉观察到机器人动作的端到端控制。然而，现有VLA模型存在两个关键局限：一是严重依赖大规模、高质量的机器人演示数据，这些数据收集成本高昂且难以覆盖真实环境的多样性，导致在数据稀缺或分布外条件下性能显著下降；二是面对复杂场景时任务智能不足，例如，对机器人数据的微调会侵蚀LLM原有的高级理解能力，使其难以解读间接指令或进行复杂推理，同时在长视野任务中表现出高级规划能力的不足。</p>
<p>本文针对上述痛点，提出了一种无需训练、端到端的机器人操作框架，以智能体的方式利用强大的LLM。核心思路是设计一个基于智能体的通用机器人操作框架，通过将复杂任务分解并分配给专门化的智能体来解决，形成一个完整的感知-推理-控制流水线，从而高效处理复杂的操作场景。</p>
<h2 id="方法详解">方法详解</h2>
<p>ManiAgent框架通过多个专门化智能体的无缝协作，实现从任务描述和环境输入到机器人操作动作的端到端输出。其整体流程始于感知智能体，它接收场景图像和用户指令，调用视觉语言模型生成与任务相关的场景描述；推理智能体融合场景描述和任务指令，查询大语言模型进行状态评估和子任务分解；在执行子任务时，感知智能体使用物体检测方法识别目标物体并获取详细信息；控制器智能体基于子任务查询缓存，若找到匹配的缓存动作序列则直接调用，否则结合子任务描述和物体细节查询LLM以生成完整的动作序列执行。</p>
<p><img src="https://arxiv.org/html/2510.11660v2/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：ManiAgent框架概览。1) 感知智能体处理场景图像和用户指令，调用VLM生成场景描述。2) 推理智能体接收场景描述和任务指令，查询LLM进行状态评估。3) 子任务执行时，感知智能体使用物体检测方法识别目标物体。4) 控制器智能体基于子任务查询缓存或生成动作序列。</p>
</blockquote>
<p>框架包含三个核心模块：</p>
<ol>
<li><strong>场景感知</strong>：该模块利用VLM生成场景的文本描述。输入为场景图像I、任务描述T和提示词Θ，输出为文本场景描述S（S = VLM(I, T; Θ)）。提示词Θ的调优被视作一个优化问题，旨在平衡召回率（确保描述捕获所有任务相关的真实场景信息）和相关性（最大化输出描述与任务描述的相关性，以减少冗余信息干扰）。若文本描述不足以支撑规划，则会调用物体检测方法，结合图像输入和相机标定参数计算物体的3D空间坐标，为推理提供更详细的环境信息。</li>
<li><strong>推理与规划</strong>：此模块作为整个任务执行过程的核心，接收场景描述，结合LLM内嵌的物理知识和任务当前进度，将总体目标分解为可执行的子任务。其推理是增量式的，逐步适应不断演变的场景，以确保生成的子任务在当前环境下可行。同时，推理智能体会存储历史子任务作为记忆以防止局部循环，并为每个子任务编译关键物体列表，辅助感知模块收集所需的物体细节。</li>
<li><strong>物体感知</strong>：此模块接收来自上层模块的目标物体列表，融合场景图像、深度图和相机参数，以获取物体的坐标和抓取姿态。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.11660v2/x3.png" alt="物体感知模块"></p>
<blockquote>
<p><strong>图3</strong>：ManiAgent的感知模块处理流程。接收目标物体列表，结合场景图像、深度图和相机参数，获取物体坐标和抓取姿态（需要时使用VLM筛选相同物体），最后将文本格式的物体信息发送至下一模块。</p>
</blockquote>
<p>具体而言，首先利用具备开放词汇检测能力的VLM（如Florence-v2）进行物体检测，通过添加“every”前缀以减少漏检，并提供物体边界框中心点作为图像坐标，再通过相机标定参数转换为机器人基坐标系下的3D坐标。对于场景中的多个相同物体实例，通过在图像上标注序号并输入VLM进行筛选，以准确识别目标（如“中间的辣椒”）。随后，使用AnyGrasp感知整个场景的抓取姿态，并与之前获得的物体位置匹配，选择邻域内得分最高的抓取位置和姿态作为该物体的抓取信息。输出同时包含物体的抓取位置和中心位置。<br>4.  <strong>控制器</strong>：控制器智能体负责将来自推理智能体的子任务，连同相应的场景图像和深度信息，转化为机械臂的可执行动作。它直接输出笛卡尔空间中的关键点以及每个动作步骤的文本描述。动作生成可形式化为一个在已知选项下的排序问题（公式3），LLM基于子任务T，组合和排序从物体感知阶段获得的特定物体的中心位置pi和抓取姿态gj，并确保任务相关性。提示词仅限少数基本技能（抓放、拖动物体、旋转物体），每种技能提供一个轨迹示例，以最大化LLM的泛化性能。为缓解LLM生成动作可能带来的延迟，框架采用了缓存机制。对于已执行并缓存的子任务，控制器首先查询缓存，若存在匹配记录，则检索缓存的参数化动作序列，并与当前场景中关键物体的细节（如坐标）集成以产生具体动作。完成新任务后，控制器输出的参数化动作序列会被缓存以供未来复用。</p>
<p>与现有方法相比，ManiAgent的创新点在于：1) 提出了一个完全无需训练、端到端的智能体框架，直接生成可执行动作序列。2) 通过协调三个专门化智能体，构建了感知-推理-控制流水线，实现了空间感知、任务推理和动作规划的有效集成，能够处理复杂任务执行。3) 设计了缓存机制以加速任务完成。4) 与依赖手动定义、任务特定API的现有交互式框架不同，ManiAgent消除了对此类API的需求，提供了更大的灵活性、更易的部署性和更强的泛化能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真环境（SimplerEnv平台）和真实物理平台（WidowX-250s机械臂，Realsense D435相机）上进行。使用了BridgeTable-v1和BridgeTable-v2仿真环境中的四个任务进行评估。真实实验设计了8个任务，以评估非抓放操作能力、高泛化能力、相对位置感知能力、意图推理能力、知识检索与利用能力以及多步任务规划能力。</p>
<p>对比的基线方法包括代表性的VLA模型CogACT和Pi-0，以及在物理实验中对比的基于关键点的约束方法ReKep。</p>
<p><strong>仿真实验结果</strong>：如表I所示，ManiAgent在四个任务上均取得高成功率，整体上大幅超越CogACT（51.3%）和Pi-0（55.7%）。使用GPT-5时，平均成功率达到86.8%。性能随VLM能力提升而提升，表明框架能有效将更强的感知-语言-推理能力转化为更高的动作成功率。仅在任务4（将茄子从水槽移到篮子）中，因初始放置导致部分遮挡，检测模块可能错误定位，以及模拟器中深度-RGB错位等问题，性能受到限制。</p>
<p><strong>物理实验结果</strong>：如表III所示，ManiAgent框架成功完成了所有任务。使用Claude-4-sonnet和Grok-4时取得了最高的平均成功率95.8%。开源模型因输出格式遵从性问题（令牌丢失导致无效动作格式）表现显著较差。实验观察到，框架性能与底层VLM的能力密切相关。</p>
<p><img src="https://arxiv.org/html/2510.11660v2/x4.png" alt="仿真环境任务执行"></p>
<blockquote>
<p><strong>图4</strong>：在SimplerEnv仿真环境中的任务执行过程。展示了选取的四个任务场景。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.11660v2/x5.png" alt="真实世界任务定义与示例"></p>
<blockquote>
<p><strong>图5</strong>：真实世界操作任务的定义和场景示例。列出了用于评估的8个具体任务及其指令。</p>
</blockquote>
<p><strong>与ReKep的对比</strong>：如表IV所示，在任务1、3、5、7上，ManiAgent在使用相同VLM的情况下，成功率全面显著优于ReKep，提升幅度从37.5%到75%不等。这凸显了ManiAgent在利用VLM进行任务执行方面的有效性，特别是在需要精确姿态估计和长视野规划的复杂任务上（如任务3、5、7），ReKep因其在任务规划和推理方面的局限性而表现不佳。</p>
<p><strong>消融实验（体现为组件功能分析）</strong>：论文虽未设置独立的消融实验章节，但通过各部分描述和实验现象间接说明了各组件贡献：1) <strong>感知-推理-控制流水线</strong>：是框架处理复杂、多步骤任务的基础，其增量式推理和子任务分解能力是成功的关键。2) <strong>缓存机制</strong>：旨在提高执行效率，但物理实验主结果部分禁用了缓存以测试模型在未见任务上的性能，表明核心能力不依赖于缓存。3) <strong>专用物体感知模块</strong>：提供了精确的3D坐标和抓取姿态，对于需要精确操作（如堆叠）的任务至关重要，避免了ReKep等方法因无法生成有效目标位置而失败的问题。</p>
<p><strong>自动数据收集</strong>：得益于高可靠性，ManiAgent可作为一个自动数据收集工具。</p>
<p><img src="https://arxiv.org/html/2510.11660v2/x6.png" alt="自动数据收集"></p>
<blockquote>
<p><strong>图6</strong>：ManiAgent自动化数据收集。a) 为“将胡萝卜放在盘子上”任务定义的任务链；b) 收集数据第一帧的RGB堆叠图；c) 使用本方法收集的数据训练的CogACT在真实世界部署中的性能。实验表明，用ManiAgent收集的数据训练的VLA模型，其性能可与使用人工收集数据训练的模型相媲美。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个名为ManiAgent的端到端智能体框架，无需训练即可为通用机器人操作任务直接生成可执行动作序列。2) 通过协调三个专门化智能体，设计了一个感知-推理-控制流水线，集成了空间感知、任务推理和动作规划，实现了复杂任务执行。3) 实验验证了其卓越性能，其高成功率使其能够作为全自动数据收集工具，为其他基于学习的机器人操作方法提供强有力的数据支持。</p>
<p>论文提到的局限性主要包括：1) 框架性能与底层VLM和检测模块的能力紧密相关，例如在物体部分遮挡时检测可能出错。2) 仿真实验中，间歇性的深度-RGB错位问题限制了观察到的性能，因此报告的结果可能低估了框架的真实能力。3) 开源VLM在输出格式遵从性上的不足会导致任务失败。</p>
<p>对后续研究的启示：1) ManiAgent展示了以智能体方式协同利用现有强大模型（VLM/LLM）与机器人领域专用模块（如检测、抓取生成）的潜力，为构建无需大规模训练数据的通用机器人系统提供了新范式。2) 其作为高可靠性自动数据收集工具的潜力，有助于突破学习型机器人系统高质量数据稀缺的瓶颈。3) 未来工作可致力于提升感知模块在挑战性条件（如严重遮挡）下的鲁棒性，并探索如何更好地将框架能力迁移或适配到能力稍弱但更可控的开源模型上。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在复杂推理与长时程任务规划中受限于数据稀缺和模型能力的问题，提出ManiAgent——一个用于通用机器人操作的智能体框架。该框架采用多智能体协同架构，通过感知、推理与执行三个专用智能体间的通信，实现对环境感知、子任务分解和动作生成的端到端处理。实验表明，ManiAgent在SimperEnv基准测试中达到86.8%的成功率，在真实世界取放任务中达到95.8%的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.11660" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>