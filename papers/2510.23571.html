<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RobotArena $\infty$ : Scalable Robot Benchmarking via Real-to-Sim Translation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RobotArena $\infty$ : Scalable Robot Benchmarking via Real-to-Sim Translation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.23571" target="_blank" rel="noreferrer">2510.23571</a></span>
        <span>作者: Katerina Fragkiadaki Team</span>
        <span>日期: 2025-10-27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，追求能够跨多样任务和环境的通用机器人策略（robot generalists）需要严谨且可扩展的评估方法。然而，机器人策略的真实世界测试本质上受到限制：劳动密集、速度慢、大规模测试不安全且难以复现。现有的仿真基准同样受限，因为它们通常在相同的合成域内训练和测试策略，无法评估从真实世界演示或其他仿真环境训练的模型。随着策略范围和复杂度的扩大，这些障碍只会加剧，因为机器人学中的“成功”往往依赖于对人类执行质量的细致判断。本文针对这一痛点，提出了一种新的基准测试框架，通过将视觉语言动作模型（VLA）的评估转移到结合在线人类反馈的大规模仿真环境中来克服这些挑战。本文的核心思路是：利用视觉语言模型（VLM）、2D到3D生成建模和可微分渲染的进展，自动将广泛使用的机器人数据集中的视频演示转换为仿真环境，并在这些数字孪生中，使用自动的VLM引导评分和从众包工作者收集的可扩展的人类偏好判断来评估VLA策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>RobotArena ∞ 的整体框架（Pipeline）包含三个主要阶段：1）从真实视频到仿真环境的自动转换；2）在生成的仿真环境中部署机器人策略；3）使用VLM和人类偏好对策略执行轨迹进行评估。</p>
<p><img src="https://arxiv.org/html/2510.23571v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RobotArena ∞ 提供了一个可扩展且可扩展的机器人基准测试框架，通过自动化环境构建和评估。它自动从真实视频生成仿真环境，部署机器人策略，并使用VLM和众包工作者（对执行视频进行成对偏好选择）来评估它们。</p>
</blockquote>
<p><strong>核心模块一：从演示视频到仿真环境的映射</strong><br>该模块的目标是从机器人演示视频（通常带有语言任务描述和每帧关节角度轨迹）中自动提取关键元素，构建对应的仿真环境。具体提取五个要素：1）相机相对于机器人本体的6自由度位姿；2）任务相关物体的3D网格重建、朝向、尺寸和材质属性；3）场景深度图；4）干净的背景图像；5）比例-微分（PD）控制增益。</p>
<p><strong>核心模块二：通过可微分机器人渲染进行自动机器人-相机标定</strong><br>由于演示视频通常未标定，本文采用基于合成分析的方法估计相机到机器人的变换。首先，基于机器人的URDF文件，通过可微分渲染在仿真中构建一个关节角条件化的3D高斯机器人模型。然后，给定带有每帧关节角的演示视频，渲染高斯机器人模型，并优化相机的3D平移和朝向，以最小化一个复合对齐损失。该损失包含三项：RGB损失（像素级外观差异）、光流损失（渲染运动场与视频光流的一致性）和特征损失（渲染帧与观察帧之间DINOv2嵌入的对齐）。</p>
<p><img src="https://arxiv.org/html/2510.23571v1/x3.png" alt="自动标定"></p>
<blockquote>
<p><strong>图3</strong>：通过位姿条件化3D机器人高斯模型的可微分渲染进行自动机器人-相机标定。</p>
</blockquote>
<p><strong>核心模块三：物体与场景3D重建、补全与物理属性估计</strong><br>首先，使用VLM（Gemini）分割机器人和所有任务相关物体。每个分割出的图像块经过超分辨率处理后，使用2D到3D生成模型（Hunyuan-3D）转换为带纹理的3D网格。为了恢复物体的正确3D位姿，本文渲染重建3D网格的2D图像视图，并使用特征匹配（MINIMA）将其与真实物体图像块进行比较，通过单目深度估计和SVD求解最终位姿。物体的物理和材质属性通过提示VLM推断。场景背景通过使用修复模型（LaMa）对第一帧视频中的机器人和物体区域进行修复来生成静态背景。最后，通过系统辨识调整PD控制器增益，使仿真的末端执行器轨迹与演示视频中的观测轨迹对齐。</p>
<p><strong>核心模块四：可控域扰动</strong><br>为了压力测试策略在变化下的泛化能力，系统地对生成的环境引入可控扰动：背景更换（从多样背景数据集中采样纹理替换原背景）、颜色偏移（改变场景RGB通道配置，强度从0%到100%）和物体位姿变化（随机置换场景中物体的位置）。</p>
<p><strong>核心模块五：使用VLM和人类评估机器人轨迹</strong><br>评估采用两种互补策略：</p>
<ol>
<li><strong>VLM自动绝对评估</strong>：如图4所示，向VLM提供一个打乱顺序的视频帧序列（并增加初始帧作为零进度参考）和语言指令，要求其为每帧分配任务进度分数。打乱顺序迫使模型仅基于视觉线索进行评估。最终采用轨迹最后30%帧的平均分数作为指标。</li>
<li><strong>人类偏好相对评估</strong>：通过众包进行成对、双盲比较。评估者观看同一环境、相同初始条件和任务指令下两个策略的执行视频，选择哪个更好或平局，并提供自由形式的理由说明。使用Bradley-Terry模型从成对偏好中推断出全局策略排名和能力分数。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.23571v1/figs/shuffleseq.png" alt="VLM评估"></p>
<blockquote>
<p><strong>图4</strong>：通过向Gemini提供打乱顺序的帧序列和语言指令，自动获取执行视频的任务进度分数。</p>
</blockquote>
<p><strong>创新点</strong><br>与现有工作（如需要大量人工创建资产的BEHAVIOR，或手动重建少量真实场景的SIMPLER）相比，RobotArena ∞ 的创新性主要体现在：1）<strong>全自动化的真实到模拟转换流水线</strong>，结合VLM、生成模型和可微分渲染，无需手动标定或精心标注；2）<strong>评估协议的创新</strong>，将大规模、众包的人类偏好反馈引入机器人策略评估，并将人类参与从繁琐的场景设置、重置和安全监督转变为轻量级的偏好比较；3）<strong>系统性扰动测试</strong>，用于量化策略的鲁棒性和泛化能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：构建了三个仿真环境集：<code>BridgeSim</code>（源自BridgeV2数据集）、<code>DROIDSim</code>（源自DROID数据集）、<code>Rh20TSim</code>（源自RH20T数据集）。共包含超过100个标称环境和数百个扰动环境。</li>
<li><strong>实验平台</strong>：在自动生成的仿真环境中进行评估。</li>
<li><strong>对比的Baseline策略</strong>：评估了四个开源通用机器人策略：<code>Octo</code> (Octo-Base)、<code>RoboVLM</code>、<code>SpatialVLA</code>、<code>CogAct</code>。</li>
<li><strong>评估指标</strong>：VLM自动评分（任务进度分数）和基于人类偏好计算的Bradley-Terry (BT) 排名分数。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>跨数据集泛化能力弱</strong>：如图6a所示，策略在非训练数据集衍生的环境上表现显著下降。例如，所有策略在 <code>DROIDSim</code> 和 <code>Rh20TSim</code> 上的VLM评分普遍低于在 <code>BridgeSim</code> 上。</li>
<li><strong>模型选择至关重要</strong>：<code>RoboVLM</code> 和 <code>CogAct</code> 在大多数环境下 consistently 优于 <code>Octo-Base</code> 和 <code>SpatialVLA</code>。</li>
<li><strong>3D空间建模有助于鲁棒性</strong>：如图6b所示，在物体位姿扰动下，<code>SpatialVLA</code> 表现出更好的鲁棒性，表明显式的3D空间关系建模增强了对固定资产布局依赖的泛化能力。</li>
<li><strong>主干网络强度驱动鲁棒性</strong>：具有更强VLM主干（<code>CogAct</code>, <code>RoboVLM</code>）的策略对颜色扰动更具弹性。</li>
<li><strong>策略对背景变化敏感</strong>：所有策略在背景被扰动时性能急剧下降。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.23571v1/assets/datasets_comparison_plot.png" alt="VLM评分结果"></p>
<blockquote>
<p><strong>图6</strong>：(a) 在所有RobotArena ∞ 环境中通过VLM获得的策略评估结果。(b) 在BridgeSim环境的扰动中通过VLM获得的策略评估结果。结果显示跨数据集泛化弱、模型性能差异以及对不同扰动类型的敏感性。</p>
</blockquote>
<ol start="6">
<li><strong>人类偏好与自动评分一致</strong>：如图7所示，从超过7000对偏好中计算出的BT排名显示，<code>RoboVLM</code> 和 <code>CogAct</code> 的排名高于 <code>Octo</code> 和 <code>SpatialVLA</code>，这与VLM自动评分趋势一致。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.23571v1/assets/bt_scores_exponentiated.png" alt="人类偏好BT分数"></p>
<blockquote>
<p><strong>图7</strong>：模型的人类偏好BT分数（取指数后）。误差线表示使用稳健三明治方差估计器计算的置信区间。人类偏好与VLM任务进度分数一致。</p>
</blockquote>
<p><strong>消融/验证实验</strong>：</p>
<ul>
<li><strong>仿真 vs. 真实世界验证</strong>：针对“将胡萝卜放入盘子”任务，在仿真和真实世界中重建相同场景并部署三个策略。<code>RoboVLM</code> 和 <code>SpatialVLA</code> 在两者中都成功，而 <code>Octo</code> 在两者中都失败，行为一致（试图抓取胡萝卜但失败），初步验证了仿真评估与真实世界的一致性（图8）。</li>
<li><strong>与SIMPLER基准对比</strong>：如图9所示，所有VLA在SIMPLER（4个手工重建环境）上的得分远高于在 <code>BridgeSim</code>（70个自动生成环境）上的得分，但模型相对排名在两个基准间保持一致。这表明RobotArena ∞ 的基准更具普适性和难度，适合评估未来更先进的策略。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.23571v1/assets/GVL_L30_BridgeSim_vs_RobotArena.png" alt="与SIMPLER对比"></p>
<blockquote>
<p><strong>图9</strong>：在RobotArena ∞ 与Li等人的SIMPLER基准上的任务完成策略评估结果。所有VLA在SIMPLER上得分更高，但相对模型排名一致。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>可扩展且可扩展的机器人基准测试协议</strong>，将物理引擎、真实到模拟转换和人类偏好反馈相结合，克服了真实世界评估的瓶颈。</li>
<li>引入了一个<strong>全自动的真实到模拟转换流水线</strong>，基于VLM、2D到3D生成模型和可微分渲染，无需人工监督即可从视频生成仿真环境。</li>
<li>进行了<strong>迄今为止最广泛的机器人策略评估</strong>，比较了来自全球实验室的多个VLA在数百个环境和数千条人类偏好中的表现，并揭示了当前策略在分布偏移下的泛化特性与不足。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>当前评估的策略尚未纳入腕部摄像头输入，限制了某些操作的保真度。</li>
<li>当前的模拟器在建模细粒度接触动力学（如将充电器插入插座）方面仍有困难，这些任务难以忠实复现。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>RobotArena ∞ 的框架为机器人社区提供了一个持续演进的、可复现的评估平台，其自动化特性使其能够受益于仿真、物理引擎和环境生成技术的进步。</li>
<li>基准测试结果明确指出了当前通用策略的弱点（如对训练数据分布敏感、对背景和外观变化鲁棒性差），为未来研究指明了改进方向，例如增强3D空间推理、提高对上下文变化的鲁棒性等。</li>
<li>将众包人类偏好大规模引入评估，为衡量策略的“实用性”和“可接受性”提供了新视角，其评估者是日常终端用户，这正是机器人最终要服务的对象。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人政策评估在现实世界中劳动密集、不安全且难以复制的核心问题，提出RobotArena ∞基准框架。关键技术是Real-to-Sim Translation，利用视觉语言模型、2D到3D生成建模和可微分渲染，自动将真实视频演示转换为模拟环境（数字孪生）。评估结合自动VLM评分和众包人类偏好判断，并系统扰动纹理与物体放置以测试泛化。该框架实现了可扩展、可复现的基准，解决了机器人评估的标准化瓶颈。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.23571" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>