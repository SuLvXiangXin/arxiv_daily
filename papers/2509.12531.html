<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.12531" target="_blank" rel="noreferrer">2509.12531</a></span>
        <span>作者: Sebastian W. Pattinson Team</span>
        <span>日期: 2025-09-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在视觉运动策略学习中，主流方法通常遵循“白板”范式，即从零开始联合训练策略和视觉编码器。这种方法不仅样本效率低下，而且当面对部署环境中未见的分布外（OOD）视觉输入时，学习到的策略泛化能力极差。利用预训练视觉模型（PVM）来为策略网络提供特征，已在无模型强化学习（MFRL）和模仿学习（IL）中被证明能提升鲁棒性。基于模型的强化学习（MBRL）在理论上通常比MFRL更样本高效且对分布偏移更鲁棒，但与此直觉相反，现有唯一一项研究[11]发现，在MBRL中使用冻结权重的PVM既未提升样本效率，也未改善泛化能力，甚至可能更差。</p>
<p>本文针对这一矛盾点，旨在重新评估PVM在MBRL，特别是面对视觉域偏移时的有效性。论文指出，先前研究的分布偏移较为温和（如颜色扰动），且智能体在训练中已见过同类偏移。本文引入了“硬分布偏移”的概念，即在评估时改变训练中保持恒定的任务要素（如全新的桌面纹理、未见的恶劣天气），这更能反映现实世界部署的挑战。核心思路是：在更严峻的硬偏移下系统评估PVM在MBRL中的表现，并研究不同程度的PVM微调（冻结、部分微调、全微调）对模型在分布内（ID）性能和OOD泛化之间权衡的影响。</p>
<h2 id="方法详解">方法详解</h2>
<p>本研究以SOTA MBRL算法DreamerV3为基础框架进行修改。DreamerV3包含两个主要学习循环：世界模型学习和演员-评论家策略学习。世界模型是一个循环状态空间模型（RSSM），包含编码器、解码器、GRU和动态模型等组件，用于从视觉观测中提取离散随机表征 $z_t$，并预测环境动态和奖励。演员和评论家网络则基于世界模型状态 $s_t$ 学习最大化回报的策略。</p>
<p><img src="https://arxiv.org/html/2509.12531v1/figs/rl-envs-rename.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：实验环境设置。上图：桌台操作任务。分布内（ID）物体生成在绿色方块内，简单OOD物体生成在红色圆环内。硬偏移则额外更换了桌面纹理。下图：自动驾驶任务。弱OOD和强OOD的天气偏移逐渐变得更加多雾、多雨和黑暗。</p>
</blockquote>
<p>本方法的核心修改在于视觉编码器。原DreamerV3的CNN编码器被替换为预训练的视觉模型（PVM），包括DINOv2和CLIP。为了匹配DreamerV3的架构，PVM的输出会通过一个前馈层投影到与原编码器相同维度的 $z_t$ 分布。其余架构保持不变。</p>
<p>研究的核心模块是PVM及其不同的微调配置：</p>
<ol>
<li><strong>基线</strong>：从头训练的DreamerV3 CNN编码器。</li>
<li><strong>冻结</strong>：PVM权重完全冻结，不参与训练。</li>
<li><strong>部分微调</strong>：仅微调PVM的最后四分之一层。微调学习率设置为 $4\times 10^{-6}$，是系统其余部分学习率的十分之一。</li>
<li><strong>全微调</strong>：PVM所有权重进行端到端微调。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1）在更具挑战性和真实性的“硬偏移”场景下评估PVM，而非先前工作中的温和偏移；2）系统探索了不同程度的PVM微调策略，特别是部分微调，以平衡ID性能与OOD泛化，这直接回应了[11]中关于固定表征局限性的论点。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了两个仿真环境：1）基于ManiSkill的桌台操作环境（拾取单一YCB物体），包含ID、简单OOD（未见物体和位置）和硬OOD（额外更换桌面纹理）设置；2）基于RL-ViGen（CARLA）的自动驾驶环境，包含ID、弱OOD和强OOD（逐渐恶劣的天气和光照）设置。评估指标主要为标准化回报。对比的基线是使用原始CNN编码器的DreamerV3。每个配置使用三个随机种子进行训练和评估。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>样本效率</strong>：无论是从学习曲线（图2）可视化，还是从达到90%最大回报的步骤数阈值分析（表III）及置换检验来看，使用PVM（无论是否微调）与基线CNN相比，均未表现出统计上显著的样本效率提升。世界模型损失分析（图3）表明，所有模型的图像重建和奖励预测损失都迅速收敛，而动态损失是主要瓶颈，但动态损失的差异并未转化为策略学习速度的差异。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.12531v1/figs/curves-rename.png" alt="学习曲线"></p>
<blockquote>
<p><strong>图2</strong>：两个环境中的策略学习曲线。曲线显示不同PVM配置的渐进性能存在差异，但初始学习速率没有明显区别。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.12531v1/figs/log-losses.png" alt="世界模型损失"></p>
<blockquote>
<p><strong>图3</strong>：自动驾驶任务中DreamerV3世界模型的各项损失。所有模型的图像重建和奖励预测损失都快速收敛，动态损失收敛较慢且不同模型间存在差异。</p>
</blockquote>
<ol start="2">
<li><strong>OOD泛化</strong>：这是本文的核心发现。如图4所示，在简单的OOD偏移下（桌台任务中仅改变物体和位置），PVM智能体的表现确实不如或仅与基线持平，复现了[11]的结果。然而，在<strong>硬偏移</strong>下，结果截然不同：<ul>
<li><strong>桌台任务（硬偏移：新桌面纹理）</strong>：基线和全微调的PVM模型完全失败，回报甚至低于随机策略（基线相对下降106%）。而<strong>部分微调</strong>和<strong>冻结</strong>的PVM模型保持了较高的平均回报，其中部分微调的DINOv2性能下降最小（28%）。</li>
<li><strong>自动驾驶任务（硬偏移：恶劣天气）</strong>：基线模型在弱和强偏移下性能完全崩溃（相对下降79%和87%）。全微调的PVM在弱偏移下表现尚可，但在强偏移下崩溃。而<strong>部分微调</strong>和<strong>冻结</strong>的PVM模型则表现出强大的鲁棒性，其中冻结的DINOv2性能下降最小（弱偏移6%，强偏移33%）。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.12531v1/figs/eval-rename.png" alt="评估结果"></p>
<blockquote>
<p><strong>图4</strong>：各环境评估设置下不同视觉模型的标准化回报。关键发现在于，在“硬偏移”条件下，部分微调或冻结的PVM模型（尤其是DINOv2）的泛化能力远超基线模型。</p>
</blockquote>
<p><strong>消融实验与分析</strong>：<br>论文通过分析PVM的特性来理解上述结果：</p>
<ul>
<li><strong>视觉不变性</strong>：使用UMAP对世界模型表征 $z_t$ 进行可视化（图6系列）。结果显示，基线CNN的ID和硬OOD表征分离明显。随着微调程度加深，这种分离加剧（不变性减弱）。部分微调仍保持相当程度的表征重叠。这表明，保留预训练带来的视觉不变性有助于OOD泛化。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.12531v1/figs/umaps/umap-mani-base.png" alt="UMAP可视化1"><br><img src="https://arxiv.org/html/2509.12531v1/figs/umaps/umap-mani-dinoft.png" alt="UMAP可视化2"><br><img src="https://arxiv.org/html/2509.12531v1/figs/umaps/umap-mani-dinopar.png" alt="UMAP可视化3"></p>
<blockquote>
<p><strong>图6系列</strong>：桌台任务中不同编码器的UMAP可视化。蓝色点为ID表征，粉色点为硬OOD表征。基线模型分离明显，而部分微调的DINOv2仍保持较大重叠，体现了更强的视觉不变性。</p>
</blockquote>
<ul>
<li><strong>灾难性遗忘</strong>：通过评估微调后PVM在ImageNet上的K-NN分类精度（图5）来衡量。结果显示，全微调导致分类精度大幅下降（严重遗忘），部分微调次之，冻结模型精度最高。遗忘程度与OOD泛化性能下降趋势相关。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.12531v1/figs/forget-rename.png" alt="遗忘分析"></p>
<blockquote>
<p><strong>图5</strong>：微调后PVM的ImageNet K-NN分类精度。全微调导致严重的灾难性遗忘，部分微调遗忘程度较轻。</p>
</blockquote>
<p><strong>总结每个组件的贡献</strong>：1) <strong>预训练本身</strong>：提供了强大的、对扰动不变的视觉先验，这是在硬偏移下良好泛化的基础。2) <strong>冻结权重</strong>：最大程度保留了这种不变性，在极端OOD下泛化最好，但可能限制了ID性能的优化上限。3) <strong>全微调</strong>：能更好地适应特定任务，获得最佳ID性能，但牺牲了不变性，导致在硬偏移下泛化崩溃。4) <strong>部分微调</strong>：在微调少量层（最后1/4）的情况下，取得了最佳的折衷，既获得了接近全微调的ID性能，又保留了相当程度的不变性，从而在硬偏移下保持了最强的平均鲁棒性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>证明了PVM在MBRL硬偏移下的有效性</strong>：在先前研究得出负面结论的领域，本文通过引入更具挑战性的“硬分布偏移”评估，证明了PVM（尤其是DINOv2）能显著提升MBRL策略的OOD泛化能力，性能远超从头训练的基线模型。</li>
<li><strong>提出了部分微调策略并验证其优越性</strong>：系统研究了PVM的微调程度，发现部分微调（仅微调最后部分层）能在保持高水平ID性能的同时，最大程度地维持对硬偏移的鲁棒性，实现了最佳权衡。</li>
<li><strong>分析了鲁棒视觉表征的关键属性</strong>：通过表征可视化和遗忘测试，将OOD泛化能力与视觉不变性、以及微调过程中的灾难性遗忘程度联系起来，为理解为何某些配置更鲁棒提供了依据。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，由于使用的是现成的PVM，无法控制模型容量变量（DINOv2 21M参数，CLIP 88M参数，基线CNN 5.1M参数）。此外，ViT架构本身可能比CNN需要更多样本来达到相同的样本效率。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>PVM在MBRL中具有重要价值</strong>：不应因早期负面结果而否定PVM在MBRL中的作用，关键在于评估场景的设置（硬偏移 vs 温和偏移）。</li>
<li><strong>微调策略是关键</strong>：简单地冻结或全微调可能都不是最优解。部分微调、分层适配或引入正则化以防止遗忘，是值得深入探索的方向，以平衡任务适应性与表征不变性。</li>
<li><strong>表征不变性是泛化的关键</strong>：未来的工作可以致力于设计或选择那些能天然学习到对任务无关变化具有不变性的预训练模型或预训练任务，以直接提升策略的泛化能力。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究了预训练视觉模型在基于模型的强化学习中对视觉域转移的泛化能力这一核心问题。针对现有研究认为PVMs在MBRL中无效的结论，本文通过实验验证了其在严重视觉分布偏移下的有效性。关键技术在于探索了对PVMs进行不同程度微调的影响。核心实验结论表明：在遭遇严重视觉偏移时，使用PVMs的策略性能显著优于从头训练的基线模型；而部分微调（partial fine-tuning）的方式能在最极端的分布偏移下保持最高的平均任务性能。这证明了PVMs能有效提升视觉策略学习的鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.12531" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>