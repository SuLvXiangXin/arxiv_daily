<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Mask2IV: Interaction-Centric Video Generation via Mask Trajectories - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Mask2IV: Interaction-Centric Video Generation via Mask Trajectories</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.03135" target="_blank" rel="noreferrer">2510.03135</a></span>
        <span>作者: Laura Sevilla-Lara Team</span>
        <span>日期: 2025-10-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>生成以交互为中心的视频，例如描绘人或机器人操作物体的场景，对于具身智能至关重要，能为机器人学习、操作策略训练和功能推理提供丰富的视觉先验。然而，现有方法难以建模此类复杂动态的交互。近期研究表明，掩码（mask）可作为有效的控制信号并提升生成质量，但为现实应用获取密集且精确的掩码标注仍是一大挑战。本文针对交互视频生成中依赖密集用户标注掩码（如手部掩码序列）这一不切实际的痛点，提出了一个新的视角：将生成过程解耦，先预测合理的交互轨迹，再基于轨迹生成视频。本文的核心思路是提出一个名为Mask2IV的两阶段框架，它能够根据指定的物体和动作描述或目标位置，自动生成包含操作者和物体的掩码轨迹，并以此为指导合成高质量、可控的交互视频。</p>
<h2 id="方法详解">方法详解</h2>
<p>Mask2IV采用解耦的两阶段流程：1）<strong>交互轨迹生成</strong>：根据初始图像、物体掩码以及文本提示或目标位置掩码，预测未来一段时间内操作者（手或机械臂）与物体的掩码运动轨迹。2）<strong>轨迹条件视频生成</strong>：以初始图像和预测的掩码轨迹为条件，生成最终的视频。</p>
<p><img src="https://arxiv.org/html/2510.03135v2/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：Mask2IV框架总览。包含两个阶段：交互轨迹生成和轨迹条件视频生成。第一阶段根据输入图像、物体掩码和条件（文本或位置）生成掩码轨迹；第二阶段基于该轨迹合成视频。</p>
</blockquote>
<p><strong>第一阶段：交互轨迹生成</strong><br>模型 ( f_{\theta} ) 的输入是初始帧 ( I )、物体掩码 ( M ) 以及条件信号（文本 ( T ) 或目标位置掩码 ( P )），输出为 ( N ) 帧的掩码序列 ( S )，表征交互轨迹。首先，使用VAE编码器 ( \mathcal{E} ) 将 ( I ) 和经过颜色编码的 ( M ) 转换为潜在特征。如果初始帧包含操作者，会使用GroundedSAM进行分割并赋予与物体不同的颜色以便区分。这些特征在通道维度拼接，并在时间维度复制 ( N ) 次，与噪声潜在 ( z ) 拼接后，输入到基于预训练图像到视频扩散模型（冻结时间注意力层）的潜在视频扩散模型中，最终通过VAE解码器 ( \mathcal{D} ) 生成轨迹。此阶段探索了两种条件生成变体：</p>
<ol>
<li>**文本条件轨迹生成 (TT-Gen)**：利用CLIP编码的文本提示 ( T ) 通过交叉注意力注入模型，以区分细微的交互意图（如“拿起”与“放下”）。</li>
<li>**位置条件轨迹生成 (PT-Gen)**：将目标位置掩码 ( P ) 编码后插入到最后一帧的潜在特征槽中，初始物体掩码特征置于第一帧，中间帧置零，促使模型插值出连贯的、将物体移动到指定终点的轨迹。</li>
</ol>
<p><strong>第二阶段：轨迹条件视频生成</strong><br>模型 ( f_{\psi} ) 以初始图像 ( I ) 和第一阶段生成的轨迹 ( S ) 为条件，生成视频 ( V )。首先将轨迹 ( S ) 编码为潜在特征 ( f_s )，然后将其与噪声潜在 ( z ) 以及扩展至 ( N ) 帧的初始帧潜在特征 ( f_i ) 拼接，输入到扩散模型中。为应对交互生成的独特挑战，本文提出了两项关键技术：</p>
<ol>
<li><strong>随机掩码扰动</strong>：训练时以概率 ( p=0.2 ) 对轨迹 ( S ) 中的掩码进行随机膨胀或腐蚀（核尺寸从 {3,5,7} 中随机选择），增强模型对掩码形状变化的鲁棒性。</li>
<li><strong>接触加权损失</strong>：为了更准确地合成操作者与物体的接触区域，定义了接触图 ( m_c = (\delta(m_h) \cap m_o) \cup (m_h \cap \delta(m_o)) )，其中 ( \delta(\cdot) ) 表示膨胀操作。该接触图用于对扩散模型的损失进行重加权，公式为 ( \mathcal{L} = \mathbb{E}[| w \odot (\epsilon - \epsilon_{\theta}) |_2^2] )，其中权重 ( w = (1 - m_c) + \lambda \cdot m_c )，( \lambda ) 是强调接触区域重要性的加权因子（实验中设为5）。</li>
</ol>
<p>与现有方法（如InterDyn）相比，Mask2IV的创新点在于：1) 无需用户提供密集的帧级操作者掩码标注，而是自动生成包含操作者和物体的完整交互轨迹作为控制信号；2) 通过两阶段解耦，降低了直接建模复杂交互动态的难度，并提供了通过指定物体和高级动作或目标位置进行灵活控制的可能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：构建了两个基准数据集：基于HOI4D的<strong>人-物交互(HOI)<strong>基准和基于BridgeData V2的</strong>机器人操作</strong>基准。视频长度为16帧，分辨率320×512。模型基于DynamiCrafter构建。</p>
<p><strong>对比基线</strong>：</p>
<ol>
<li>无显式控制的方法：预训练的DynamiCrafter及其在对应数据集上微调的版本 (DynamiCrafter-ft)。</li>
<li>控制方法：为公平比较，将CosHand和InterDyn适配到本文设置，并使用Mask2IV第一阶段生成的“伪”轨迹作为它们的控制信号。</li>
</ol>
<p><strong>关键实验结果</strong>：<br>定量结果如表1所示。在HOI4D和BridgeData V2两个数据集上，Mask2IV在衡量生成质量的FVD、LPIPS、PSNR、SSIM指标，以及衡量对齐度的V2V-Sim和T2V-Sim指标上，均一致优于所有基线方法。例如，在HOI4D上，Mask2IV的FVD为149.68，显著低于微调版DynamiCrafter-ft的168.73和InterDyn的172.42。</p>
<p><img src="https://arxiv.org/html/2510.03135v2/x4.png" alt="定性对比"></p>
<blockquote>
<p><strong>图4</strong>：与CosHand和InterDyn的定性对比。Mask2IV生成的视频运动更连贯，接触区域更清晰。基线方法常出现颜色不一致（如垃圾桶盖变绿）、物体部分被操作（如锅与把手分离）等伪影（黄色圆圈标出）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.03135v2/img/f7.png" alt="物体泛化"></p>
<blockquote>
<p><strong>图5</strong>：交互轨迹生成的物体泛化能力分析。给定同一初始图像（红色高亮指定不同物体），Mask2IV能生成针对不同物体的抓取轨迹，展示了良好的物体泛化性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.03135v2/x5.png" alt="控制性分析"></p>
<blockquote>
<p><strong>图6</strong>：不同文本提示和目标位置掩码的控制效果。同一初始图像下，通过改变文本（“推/拉玩具车”、“开/关笔记本”）或指定目标位置掩码，可以精细控制交互类型和物体的最终位姿。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>如表2所示，在HOI4D数据集上使用真实轨迹进行的消融实验验证了各组件贡献。1) <strong>控制方案</strong>：直接将掩码潜在特征与噪声拼接（MaskLatent）比训练辅助ControlNet性能更优。2) <strong>物体掩码</strong>：加入物体掩码（+ object mask）后各项指标持续提升，证明了联合建模操作者和物体运动的重要性。3) <strong>随机扰动</strong>：应用随机膨胀/腐蚀（+ random d/e）进一步提升了性能，增强了鲁棒性。4) <strong>接触损失</strong>：引入接触加权损失（+ contact loss）在视频质量上带来了额外增益，特别是在合成交互区域方面。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>提出了“交互中心视频生成”这一新任务</strong>，专注于生成可控且真实的人-物及机器人-物交互视频；2) <strong>提出了一个创新的解耦两阶段框架Mask2IV</strong>，通过先预测掩码轨迹再生成视频，在无需用户提供密集掩码标注的前提下，实现了对交互目标和过程的灵活控制；3) <strong>构建了两个涵盖不同交互场景的专用基准</strong>，支持该领域的系统化训练与评估。</p>
<p>论文自身未明确阐述局限性，但从方法设计来看，其性能依赖于第一阶段轨迹预测的准确性，且目前主要处理较短的视频片段（16帧）。本文的框架为可控交互视频生成提供了一个实用且有效的范式，其两阶段解耦、自动生成控制信号（轨迹）以及针对接触区域的特殊优化思路，对后续研究如何在更少人工干预下生成更复杂、长序列的交互动态具有重要的启示意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Mask2IV，旨在解决交互中心视频生成中依赖密集掩码标注、难以建模复杂动态交互的问题。方法采用解耦的两阶段流程：先预测执行者与物体的掩码运动轨迹，再基于轨迹生成视频，从而无需用户提供密集掩码输入。该方法支持通过文本提示或目标位置掩码灵活控制交互对象与运动。实验表明，Mask2IV在构建的交互基准上，视觉真实性与可控性均优于现有基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.03135" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>