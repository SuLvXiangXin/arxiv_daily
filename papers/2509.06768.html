<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.06768" target="_blank" rel="noreferrer">2509.06768</a></span>
        <span>作者: Sotomi, Oluwadamilola, Kodi, Devika, Shekar, Kiruthiga Chandra, Arab, Aliasghar</span>
        <span>日期: 2025/09/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>自主移动机器人在动态环境中的部署日益增多，确保其安全性和操作连续性的关键在于主动检测和响应异常。目前主流方法存在关键局限性：传统的自主移动机器人通常仅具备有限的情境感知能力，往往在问题发生后才做出反应，这可能导致安全事故和操作中断。早期模型依赖于基于规则或阈值检测的方法，无法捕获上下文信息或提供有意义的响应。近期的一些机器学习方法结合了视觉感知和语义推理以改进异常检测，但许多方法仍局限于预定义场景，缺乏对新异常或变化环境的适应性。</p>
<p>本文针对机器人缺乏前瞻性异常缓解能力的痛点，提出了整合视觉语言模型和大语言模型的新视角，以实现实时危险识别、报告和响应。本文的核心思路是构建一个多模态异常检测与缓解系统，通过将危险和冲突两种异常状态明确整合到机器人的决策框架中，并触发特定的缓解策略，从而提升机器人的安全性和操作可靠性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的系统旨在计算一条安全、可行且感知异常的路径，同时通过新颖的异常检测和缓解模块最大化异常检测有效性因子 ε，以提升动态环境中机器人导航的安全性和操作可靠性。方法包含四个部分：1）使用VLM和LLM开发多模态异常检测系统；2）在AMR中进行实时验证部署；3）与自主导航栈集成；4）集成针对危险状态的主动缓解策略。</p>
<p>系统的整体框架是一个由四个ROS2节点组成的模块化异常检测模型：1）相机节点，2）BLIP节点，3）热力图节点，4）LLM节点。这些节点通过ROS主题进行通信，能够与现有导航系统无缝集成。核心流程如算法1所示：机器人导航时，相机捕获图像；VLM节点检测潜在异常；热力图节点生成视觉显著性图；如果检测到异常，LLM节点会将其分类为“危险”或“冲突”，并触发相应的缓解动作；同时生成自然语言描述并叠加显示分类结果的热力图；最后更新导航路径以避开检测到的异常。</p>
<p><img src="https://arxiv.org/html/2509.06768v1/figs/flowchart_abnormally.png" alt="方法流程图"></p>
<blockquote>
<p><strong>图2</strong>：异常检测、解释和紧急呼叫的工作流程。展示了从图像捕获、通过BLIP和热力图节点处理、LLM分类到最终触发缓解动作（如重新规划路径或警报）的完整流程。</p>
</blockquote>
<p>核心模块的技术细节如下：</p>
<ul>
<li><strong>异常检测模型公式化</strong>：定义异常检测函数 A: (X, H, L) → C，其中 X 是输入的原始图像，H = g(X) 是热力图，L = f(X, H) 是语言模型的描述输出，C 是最终的异常分类。</li>
<li><strong>热力图生成</strong>：使用 Grad-CAM 生成热力图 H，公式为 H_i,j = ReLU(∑_k α_k A_c^(i,j))，其中 α_k 是特征图 k 的权重，A_c^(i,j) 是空间位置 (i, j) 的激活值，ReLU 确保正激活贡献。</li>
<li><strong>异常分类</strong>：最终分类 C 由大语言模型根据结合了热力图和图像上下文的特征表示 ψ(H, X) 生成，即 C = LLM(ψ(H, X))。用于分类的LLM提示词模板明确要求模型根据图像描述和热力图分析，将异常分类为“危险”（触发‘报告’）、“冲突”（触发‘避免’）或“正常”（触发‘继续’）。</li>
<li><strong>延迟优化</strong>：总延迟 T_total = T_camera + T_BLIP + T_heatmap + T_LLM。为了满足实时性要求（T_total ≤ T_max），论文提出了一个优化问题，并通过将计算密集的BLIP描述生成和LLM推理卸载到外部微服务来减少延迟。经验分析表明，延迟与计算能力 C 成反比：T_total ∝ 1/C。</li>
</ul>
<p>与现有方法相比，本文的创新点具体体现在：1）<strong>多模态融合</strong>：系统地整合了视觉（图像）、视觉语言（BLIP描述）、视觉显著性（热力图）和语义推理（LLM）进行上下文感知的异常检测。2）<strong>异常状态细分与策略映射</strong>：明确将异常建模为“危险”和“冲突”两种状态，并分别关联到“报告”和“避免”等不同的缓解策略，使响应更具针对性。3）<strong>实时边缘AI架构</strong>：在ROS2框架内实现了完整的管道，并针对边缘设备（如树莓派）进行了延迟优化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了搭载树莓派4B和内置摄像头的移动机器人（MYAGV），运行ROS 1 Noetic，并与ROS 2异常检测模块通信。测试环境为一个学术室内环境（创客空间实验室和走廊，2.5m × 14m），通过放置大型印刷图像（如袋子、危险标志、障碍物）来模拟异常。</p>
<p>对比的基线是<strong>无异常检测模块</strong>的机器人操作。实验分为手动导航和自主导航两种场景，每种场景下又分别测试启用和未启用异常检测模块的情况。</p>
<p>关键实验结果如下：</p>
<ul>
<li><p><strong>导航性能提升</strong>：如表I所示，在启用异常检测后，无论是手动还是自主导航，完成任务的总时间缩短，且“突然停止”的次数显著减少（手动导航从19次降至15次，自主导航从21次降至18次），表明系统能更早、更平滑地应对异常。</p>
<table>
<thead>
<tr>
<th align="left">Metric</th>
<th align="left">WoAD</th>
<th align="left">WAD</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>Test 1: Manual Navigation</strong></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Total Trajectory (m)</td>
<td align="left">5.76</td>
<td align="left">5.76</td>
</tr>
<tr>
<td align="left">Total Time (s)</td>
<td align="left">23.5</td>
<td align="left">22.1</td>
</tr>
<tr>
<td align="left">Anomalies Detected</td>
<td align="left">–</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left">Sudden Stops</td>
<td align="left">19</td>
<td align="left">15</td>
</tr>
<tr>
<td align="left"><strong>Test 2: Autonomous Navigation</strong></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Total Trajectory (m)</td>
<td align="left">5.83</td>
<td align="left">5.78</td>
</tr>
<tr>
<td align="left">Total Time (s)</td>
<td align="left">25.3</td>
<td align="left">22.6</td>
</tr>
<tr>
<td align="left">Anomalies Detected</td>
<td align="left">–</td>
<td align="left">3</td>
</tr>
<tr>
<td align="left">Sudden Stops</td>
<td align="left">21</td>
<td align="left">18</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表I</strong>：导航性能对比。WoAD = 无异常检测，WAD = 有异常检测。结果显示启用检测后，任务时间和紧急停止次数均下降。</p>
</blockquote>
</li>
<li><p><strong>用户感知调查</strong>：对30名参与者的调查显示（表III），启用异常检测后，用户的安全感、对信息的清晰度认可、对机器人安全的信心以及总体安全感均有大幅提升（分别提升23.3%、26.7%、16.7%和20%）。总体偏好得分达到83.3%。<br><img src="https://arxiv.org/html/2509.06768v1/figs/survey_anomaly_detection.png" alt="用户调查结果"></p>
<blockquote>
<p><strong>图3</strong>：关于异常检测用户感知的调查结果。表明大多数参与者在异常检测系统激活时，报告了更高的安全性、信息清晰度和信心。</p>
</blockquote>
</li>
<li><p><strong>检测准确性与延迟</strong>：</p>
<ul>
<li><strong>准确性</strong>：在125次试验中，系统实现了114次正确检测，预测准确率达到**91.2%**。基于196张图像的混淆矩阵计算出的准确率为82.14%。</li>
<li><strong>延迟</strong>：经过优化（将BLIP和LLM卸载到外部服务），延迟性能显著改善。在125次测试中，<strong>84%</strong> 的响应在14秒内完成，平均延迟为<strong>6.017秒</strong>，最小延迟为2.457秒，最大延迟为25.196秒。相比早期评估中约20秒的平均延迟，这是一个重大改进。<br><img src="https://arxiv.org/html/2509.06768v1/figs/Updated_latency.png" alt="延迟分布图"><blockquote>
<p><strong>图4</strong>：125次测试中异常检测延迟的分布。大部分响应集中在低延迟区间，表明系统具备良好的实时能力。</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>缓解动作触发</strong>：系统能根据LLM的分类成功触发预定义的缓解动作，例如针对“枪支”高风险异常触发警报和通知，针对“障碍物”中风险异常触发路径重新规划（见表II）。</p>
</li>
</ul>
<p>消融实验体现在对比“有/无”异常检测模块的导航性能和用户感知上，结果清晰表明该模块对提升操作平滑性、安全性和用户信任度有直接且显著的贡献。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了一个集成VLM和LLM的<strong>多模态主动异常检测与缓解框架</strong>，使机器人不仅能识别还能响应环境威胁。2）创新性地将异常状态建模为<strong>“危险”与“冲突”</strong> 两类，并分别关联到“报告”和“避免”等具体缓解策略，增强了响应的针对性和可解释性。3）在<strong>边缘计算设备</strong>上实现了基于ROS2的实时系统，并通过优化（计算卸载）显著降低了处理延迟。</p>
<p>论文自身提到的局限性主要包括：<strong>延迟</strong>仍然是关键考虑因素，尽管已优化，但在最坏情况下仍可能达到25秒，这可能影响对快速动态危险的响应；实验在<strong>受控的室内环境</strong>中进行，模拟的异常类型相对有限，在更复杂、非结构化的真实世界场景中的泛化能力有待进一步验证。</p>
<p>本文对后续研究的启示在于：1）<strong>延迟优化</strong>是边缘AI机器人安全系统的持续挑战，未来工作可探索更轻量化的模型或更高效的硬件加速。2）框架展示了<strong>大模型与机器人具体任务（安全）结合</strong>的有效范式，可扩展至更广泛的语义理解和决策任务。3）<strong>用户信任</strong>的量化（通过调查）和提升是部署自主系统的关键，将解释性（如热力图、分类描述）融入系统设计是提高透明度和接受度的有效途径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自主移动机器人在动态环境中需主动检测与缓解异常的核心问题，提出了一种多模态异常检测与缓解系统。关键技术是整合视觉语言模型与大型语言模型，使机器人能感知、解释并响应城市与环境异常，尤其将危险与冲突状态纳入决策框架以触发特定缓解策略。核心实验表明，在用户研究中，该系统实现了91.2%的异常检测准确率，并基于边缘AI架构获得了较低的延迟响应时间。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.06768" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>