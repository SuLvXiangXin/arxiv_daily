<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.12678" target="_blank" rel="noreferrer">2506.12678</a></span>
        <span>作者: Andrea Bajcsy Team</span>
        <span>日期: 2025-06-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于行为克隆的端到端视觉运动策略已能生成复杂、多模态的低层机器人行为。然而，在部署时，面对由物体、背景或环境变化引起的分布外视觉输入，这些策略的可靠性仍显不足。现有工作在交互式模仿学习中，通常会在OOD条件下征求专家的纠正性演示，但这种方式成本高昂且效率低下。本文观察到，在OOD条件下任务的成功并不总是需要全新的机器人行为。在分布内习得的行为，可以直接迁移到与ID条件具有功能相似性的OOD条件中。核心挑战在于，如何为当前任务辨别出哪些ID观测在功能上对应于OOD观测。本文提出，专家可以提供这种OOD到ID的功能对应关系。因此，本文的核心思路是：在测试时，通过征求专家以语言描述的功能对应关系，将OOD观测与功能相似的ID观测对齐，从而复用ID策略行为，实现低反馈成本下的OOD泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的“类比适应”方法旨在通过建立部署条件与训练条件之间的功能对应关系，来提升策略在OOD条件下的性能。其整体流程包含四个关键阶段：OOD检测、建立功能对应、细化对应关系直至确信、以及对观测进行干预以生成行为。</p>
<p><img src="https://arxiv.org/html/2506.12678v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Adapting by Analogy 方法包含四个关键阶段。（左）首先，通过检查当前观测与训练观测之间的余弦相似度，运行一个快速的OOD检测器。（中，左上）给定一个对应关系描述 <em>l</em>，我们建立OOD到ID的功能对应关系以检索对应的ID观测（中，下）。（中，右上）只要预测的行为模式存在歧义，我们就与专家一起细化对应关系。一旦确定，我们对观测进行干预并执行规划的动作（右）。</p>
</blockquote>
<p><strong>1. 检测分布外观测</strong>：在每个时间步，方法首先通过一个快速的OOD检测器判断当前观测 <em>ô</em> 是否异常。计算编码后的观测 <em>ẑ = ℰ(ô)</em> 与所有ID观测嵌入 <em>z ∈ ℰ(o), o ∈ 𝒪_ID</em> 之间的最小余弦相似度（IDScore）。若IDScore高于阈值 <em>λ</em>，则视为正常观测，直接执行策略动作 *â ~ π(·|ô)*；否则，判定为OOD，并向专家征求初始的语言指令 <em>l</em>，描述相关的功能对应关系，用于在动作生成前干预观测。</p>
<p><strong>2. 建立OOD到ID的功能对应关系</strong>：给定被检测为OOD的观测 <em>ô = (q̂, î)</em> 和专家语言描述 <em>l</em>，目标是通过复用功能相似的ID观测中学习到的行为来干预策略。这需要计算 <em>ô</em> 与每个ID观测之间的功能对齐度（公式2）。为提高效率，首先根据本体感觉状态 <em>q̂</em> 过滤演示数据集，得到一个与当前机器人状态相近的ID观测子集 <em>𝒪_q</em>（公式3）。接着，利用两个内部模型实现功能对应映射 <em>Φ</em>：一个将语言反馈 <em>l</em> 解码为一组功能特征 <em>φ_l</em>；另一个（使用Grounded Segment Anything）对ID图像 <em>i ∈ 𝒪_q</em> 和OOD图像 <em>î</em> 进行语义分割，生成图像掩码和语义标签集 <em>Ω</em> 和 <em>Ω̂</em>。功能特征 <em>φ_l</em> 应用于语义分割结果，返回 <em>K</em> 对功能对应的图像片段 *(ω^j, ω̂^j)*。通过计算这些对应片段的总交并比（IoU）来衡量功能对齐度，并据此对ID观测进行排序，得到有序的功能对应ID观测集 <em>𝒪_f ⊆ 𝒪_q</em>。</p>
<p><strong>3. 细化功能对应关系直至确信</strong>：初始的专家描述可能不足以覆盖整个任务，对应关系可能随任务执行而演变。方法通过衡量检索到的行为模式的确定性，来交互式地细化功能对应描述。具体而言，在功能对齐前，获取所有具有相同本体感觉状态的观测通过策略前向传播得到的一组动作计划 <em>𝒜_q</em>，并通过K-means聚类拟合 <em>n_c</em> 个行为模式标签。功能对齐后的观测是 <em>𝒬_q</em> 的子集 <em>𝒬_f</em>，其对应的动作计划 <em>𝒜_f ⊆ 𝒜_q</em> 可以获得标签。通过计算动作计划标签的熵来衡量行为模式的多样性。只要检索到的动作熵值较高，机器人就会继续请求专家细化其功能对应描述 <em>l</em>，然后重新进行OOD到ID的匹配。</p>
<p><strong>4. 干预观测以生成功能对应的行为</strong>：一旦对应描述 <em>l</em> 完善且检索到的动作模式不确定性足够低，机器人便干预其观测以生成功能“正确”的行为。具体做法是：根据公式2衡量的功能对齐度对最终细化的 <em>𝒪_f</em> 中的观测进行排序。为平滑动作预测，对排名前 <em>M</em> 的对应观测的嵌入进行插值：*ẑ := (1/M) Σ_{o∈𝒪_f} ℰ(o)*，然后将平均嵌入传递给策略网络以生成最终执行的动作计划。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的创新在于：1）将OOD泛化问题转化为寻找测试时OOD观测与训练时ID观测之间的功能对应关系；2）利用专家语言反馈（而非新的动作演示）来指定这种对应关系；3）提出了一个包含检测、匹配、细化和干预的完整测试时自适应流程，以高效利用少量反馈实现行为迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在真实机器人（Franka Research 3机械臂）上进行了实验。使用了两个不同的真实世界操作任务：1) <strong>清扫垃圾</strong>：机器人需根据垃圾是有机物还是可回收物，将其扫向不同的目标。2) <strong>物体入杯</strong>：机器人拾起如马克笔或钢笔等物体并将其放入杯子。训练策略为基于扩散的视觉运动策略。</p>
<p><strong>对比的基线方法</strong>：1) <strong>预训练策略</strong>：直接在OOD测试集上运行预训练的基础策略。2) <strong>嵌入最近邻</strong>：用基础策略编码器的嵌入空间进行最近邻检索并干预。3) <strong>DINOv2最近邻</strong>：用DINOv2视觉特征的嵌入空间进行最近邻检索并干预。4) <strong>重新演示</strong>：在OOD条件下收集新的专家演示并重新训练策略（作为性能上界）。</p>
<p><img src="https://arxiv.org/html/2506.12678v1/x4.png" alt="任务成功率对比"></p>
<blockquote>
<p><strong>图3</strong>：在“清扫垃圾”和“物体入杯”任务中，不同OOD泛化方法的闭环任务成功率。ABA（本文方法）在仅使用少量专家反馈（功能对应描述）的情况下，性能显著优于其他测试时干预方法，并接近需要大量新演示数据的“重新演示”上界。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li>在“清扫垃圾”任务中，面对新颜色的垃圾和新背景，基础策略成功率为46.7%。ABA方法在仅征求平均1.5次反馈的情况下，将成功率提升至80.0%，显著优于嵌入最近邻（60.0%）和DINOv2最近邻（53.3%）方法。</li>
<li>在“物体入杯”任务中，面对新物体（如铅笔、胶棒），基础策略成功率为35.0%。ABA方法在平均征求2.25次反馈后，成功率提升至75.0%，同样优于其他测试时干预方法（嵌入最近邻：60.0%， DINOv2最近邻：45.0%）。</li>
<li>ABA的性能接近需要收集大量新演示并重新训练的“重新演示”基线（清扫垃圾：86.7%， 物体入杯：80.0%），但所需的专家投入成本（提供语言描述 vs. 提供动作演示）要低得多。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.12678v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：在“物体入杯”任务上的消融研究。比较了：（1）无任何干预的基础策略，（2）仅使用状态过滤（SF），（3）状态过滤+功能对应（FC），即完整的ABA方法。结果表明，功能对应模块对性能提升贡献关键。</p>
</blockquote>
<p><strong>消融实验</strong>：图4展示了“物体入杯”任务上的消融结果。仅使用本体感觉状态过滤（SF）能将成功率从35%提升至55%，而加上功能对应（FC）模块（即完整的ABA）后，成功率进一步提升至75%。这证明了功能对应关系在准确检索合适行为上的关键作用。</p>
<p><img src="https://arxiv.org/html/2506.12678v1/x6.png" alt="定性结果"><br><img src="https://arxiv.org/html/2506.12678v1/x7.png" alt="反馈效率"></p>
<blockquote>
<p><strong>图5</strong>：定性示例。左侧显示OOD场景（铅笔）和通过ABA检索到的功能对应的ID场景（钢笔），以及最终成功执行的动作序列。右侧展示了不同方法为达到特定成功率所需征求的专家反馈次数，ABA的反馈效率最高。</p>
</blockquote>
<blockquote>
<p><strong>图6</strong>：ABA方法征求反馈的时机示例。在“清扫垃圾”任务中，机器人在需要区分垃圾类型以选择正确目标时（即行为模式不确定时）才请求细化功能对应关系，而不是在每一步都请求。</p>
</blockquote>
<p>图5和图6进一步通过定性结果和反馈效率分析，展示了ABA方法能够准确建立功能对应（如铅笔对应钢笔），并在行为模式不确定时高效地征求必要反馈，从而以较低的交互成本实现成功的OOD泛化。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了“类比适应”框架，将机器人OOD泛化问题重新定义为在测试时建立OOD与ID条件之间的功能对应关系。</li>
<li>引入了一种利用专家语言反馈（而非动作演示）来指定功能对应关系，并通过干预观测复用ID行为的交互式方法。</li>
<li>在真实机器人操作任务上验证了该方法能够以远低于重新演示的成本，显著提升基于扩散的视觉运动策略对于新物体和新背景的泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前工作假设ID和OOD环境仅在场景中的物体和背景颜色上有所不同（环境几何保持不变），且训练和测试使用相同的机器人本体。这些假设简化了问题，但未来的工作需要处理更广泛的环境变化（如布局、照明、相机视角剧变）以及跨本体的泛化。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>测试时自适应范式</strong>：提供了一种介于“完全零样本”和“全面重新训练”之间的实用范式，通过最小化的人类反馈（语言）实现高效泛化。</li>
<li><strong>功能与语义对应</strong>：强调了对于机器人操作，基于任务功能（affordance）的对应比纯粹的视觉语义相似性更为重要。</li>
<li><strong>人机交互接口</strong>：探索如何利用大语言模型等更自然、更强大的接口来获取和细化功能对应描述，是未来一个富有前景的方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉运动策略在分布外（OOD）视觉条件下泛化失败的问题，提出一种基于功能对应的类比适应方法。核心思想是通过专家反馈建立OOD观察与训练分布内（ID）观察之间的功能对应关系，而非重新收集演示数据。关键技术包括：OOD检测与行为差异识别、专家提供的功能对应反馈、以及部署时利用对应ID观察进行干预。实验在Franka Panda机器人多种操作任务中验证了该方法能够以少量反馈有效提升基于视觉的扩散策略对OOD物体和环境条件的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.12678" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>