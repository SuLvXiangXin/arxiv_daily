<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.12796" target="_blank" rel="noreferrer">2510.12796</a></span>
        <span>作者: Li, Yingyan, Shang, Shuyao, Liu, Weisong, Zhan, Bing, Wang, Haochen, Wang, Yuqi, Chen, Yuntao, Wang, Xiaoman, An, Yasong, Tang, Chufeng, Hou, Lu, Fan, Lue, Zhang, Zhaoxiang</span>
        <span>日期: 2025/10/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前自动驾驶领域存在两种主流范式。一种是基于鸟瞰图（BEV）表示的专门化模型，它们依赖精心设计的几何先验，虽对驾驶任务有效，但难以利用非驾驶数据集，且相对紧凑的架构可能限制其大规模数据扩展的潜力。另一种是新兴的视觉-语言-动作（VLA）模型，它们利用在互联网规模数据上预训练的大规模视觉语言模型（VLM），拥有更大的模型规模和内在扩展潜力。然而，VLA模型面临一个关键瓶颈：“监督赤字”。其巨大的模型容量仅由稀疏、低维度的动作（如路径点）进行监督，导致其表征能力未被充分利用。仅增加纯动作训练数据的体量无法克服这一根本限制，甚至可能导致大型VLA模型表现不如更小的BEV模型。</p>
<p>本文针对VLA模型的“监督赤字”这一具体痛点，提出了利用世界建模（World Modeling）提供密集自监督信号的新视角。核心思路是：通过让模型预测未来图像，生成一个密集、丰富的监督信号，迫使模型学习环境的底层动力学并构建丰富的预测性世界表示，从而更好地利用大规模数据并增强数据缩放定律。</p>
<h2 id="方法详解">方法详解</h2>
<p>DriveVLA-W0的训练范式包含三个关键步骤：首先建立仅用动作监督的VLA基线；然后通过世界建模提供密集自监督；最后引入轻量级动作专家以解决推理延迟问题。</p>
<p><img src="https://arxiv.org/html/2510.12796v2/x2.png" alt="方法架构"></p>
<blockquote>
<p><strong>图2</strong>：DriveVLA-W0的架构，通过两种方式实现世界建模：(a) 用于预测离散视觉令牌的自回归（AR）世界模型；(b) 用于在连续特征上操作的扩散世界模型，它以多模态输入为条件对潜在表示进行去噪。</p>
</blockquote>
<p><strong>整体框架与输入</strong>：模型处理语言指令（L_t）、前视图像（V_t）和过去动作（A_{t-1}）的序列。输入经过分词化后，形成一个深度交错的多模态序列 S_t = [L_{t-H}, V_{t-H}, A_{t-H-1}, …, L_t, V_t, A_{t-1}]，由VLM主干（如Emu3或Qwen2.5-VL）以自回归方式处理，输出分割为语言、视觉和动作特征。</p>
<p><strong>核心模块1：世界建模</strong>。这是解决监督赤字的核心创新。针对两种主流VLA架构，论文提出了两种世界模型实现：</p>
<ol>
<li>**AR世界模型 (DriveVLA-W0 (VQ))**：适用于使用离散视觉令牌（如Emu3）的VLA。模型被训练以自回归方式预测当前图像的视觉令牌序列 V_t，损失函数为交叉熵损失 ℒ_WM-AR。该方法将世界建模自然地扩展为下一个令牌预测任务。</li>
<li>**扩散世界模型 (DriveVLA-W0 (ViT))**：适用于使用连续视觉特征（如Qwen2.5-VL）的VLA。由于缺乏视觉词汇表，论文引入了一个潜在扩散模型，以当前帧输出的视觉和动作特征（F_t^V, F_t^A）为条件，预测未来图像 I_{t+1}。训练采用标准的噪声预测MSE损失 ℒ_WM-Diff。预测未来帧（而非当前帧）对于学习预测性动力学至关重要。</li>
</ol>
<p>模型通过联合优化总损失 ℒ_Total = ℒ_Action + αℒ_WM-AR（或 βℒ_WM-Diff）进行端到端训练。</p>
<p><strong>核心模块2：轻量级动作专家（MoE架构）</strong>。为降低推理延迟，论文引入了一个轻量级（500M参数）的动作专家，与大型VLA专家在混合专家（MoE）架构中协同工作。</p>
<p><img src="https://arxiv.org/html/2510.12796v2/x3.png" alt="动作专家架构"></p>
<blockquote>
<p><strong>图3</strong>：(a) 混合专家架构，将大型VLA专家与轻量级动作专家配对以实现高效推理。(b-d) 该框架作为比较三种动作解码方案的测试平台：基于查询的、自回归的和流匹配的。</p>
</blockquote>
<p>两个专家共享相似的Transformer块结构但隐藏维度不同，通过<strong>联合注意力</strong>机制进行深度融合：将两者的Query、Key、Value矩阵沿令牌序列维度拼接，进行联合注意力计算后再拆分回各自专家。该架构同时作为系统研究三种动作解码策略的测试平台：</p>
<ul>
<li><strong>基于查询的动作专家</strong>：使用可学习的动作查询与VLA上下文交互，通过MLP头直接回归连续轨迹（L1损失）。</li>
<li><strong>自回归动作专家</strong>：以自回归方式预测离散动作令牌序列（交叉熵损失）。</li>
<li><strong>流匹配动作专家</strong>：学习一个条件向量场，将噪声样本通过常微分方程求解器确定性地映射到连续动作（流匹配损失）。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要在两个平台上进行评估。1) <strong>学术基准NAVSIM</strong>：包括v1和v2版本，评估指标涵盖安全性（无过错碰撞NC、可行驶区域合规DAC等）、舒适度和进度等，并综合为预测驾驶员模型分数（PDMS/EPDMS）。2) <strong>大规模内部数据集</strong>：包含7000万帧训练数据，用于研究数据缩放定律，评估指标为3秒轨迹的平均位移误差（ADE）和碰撞率。</p>
<p><strong>对比方法</strong>：与最先进的BEV方法（如UniAD、LAW、WoTE）和VLA方法（如AutoVLA、ReCogDrive）进行了全面对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>在NAVSIM基准上的SOTA性能</strong>：如表1和表2所示，DriveVLA-W0在NAVSIM v1和v2上均取得了最先进的性能。值得注意的是，该方法仅使用单目前视摄像头，就超越了依赖多摄像头和激光雷达的竞争对手。例如在NAVSIM v1上，其PDMS达到93.0（使用AR动作专家和best-of-N策略），优于AutoVLA的92.1。</p>
</li>
<li><p><strong>世界模型增强泛化与数据缩放定律</strong>：<br><img src="https://arxiv.org/html/2510.12796v2/x4.png" alt="泛化能力"></p>
<blockquote>
<p><strong>图4</strong>：世界建模通过数据缩放解锁泛化能力。在从大规模NuPlan预训练到NAVSIM微调（动作分布不同、视觉域相似）的设置中，仅用动作监督的基线模型性能下降（红箭头），而VLA-W0模型受益于预训练（绿箭头），证明了其学习可迁移视觉表征的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12796v2/x5.png" alt="缩放定律"></p>
<blockquote>
<p><strong>图5</strong>：世界建模放大了数据缩放定律。随着训练数据从70k帧扩展到70M帧，基线VLA模型性能快速饱和，而DriveVLA-W0模型性能持续提升，证明了密集视觉监督的定性优势。</p>
</blockquote>
<p>表3的消融实验进一步量化了这一点：在70M帧规模下，为VQ基线添加世界建模使ADE降低了28.8%，为ViT基线添加世界建模使碰撞率降低了15.9%。</p>
</li>
<li><p><strong>动作专家效率与解码器趋势反转</strong>：<br><img src="https://arxiv.org/html/2510.12796v2/x6.png" alt="专家效率"></p>
<blockquote>
<p><strong>图6</strong>：轻量级动作专家将推理延迟降低至基线VLA的63.1%，同时保持了高性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.12796v2/x7.png" alt="解码器趋势"></p>
<blockquote>
<p><strong>图7</strong>：在大规模数据（70M帧）下，动作解码器的性能趋势发生反转：简单的自回归解码器超越了更复杂的流匹配解码器，这与在小数据集上观察到的趋势相反。</p>
</blockquote>
<p>表4的消融实验显示，在70M帧数据上，自回归动作专家的ADE（1.0801）和碰撞率（0.0380）均优于流匹配专家（1.1366, 0.0437）。</p>
</li>
</ol>
<p><strong>消融实验总结</strong>：世界建模模块是提升泛化能力和实现数据缩放收益的关键；轻量级动作专家模块显著降低了推理延迟；在大规模数据下，自回归动作解码器组件表现出最优性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>揭示了“监督赤字”是限制VLA模型扩展的关键瓶颈，并提出了DriveVLA-W0范式，利用世界建模（预测未来图像）为VLA提供密集的自监督学习信号。</li>
<li>通过实验揭示了世界建模的两个关键扩展优势：一是通过学习可迁移的视觉表征，增强了跨不同动作分布领域的泛化能力；二是在大规模数据（70M帧）上放大了数据缩放定律，这是单纯增加动作监督数据无法实现的。</li>
<li>引入了轻量级MoE动作专家，将推理延迟降至基线63.1%，并以此作为测试平台，发现了动作解码器在大规模数据下的性能趋势反转：简单的自回归模型超越了复杂的流匹配模型。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，在NAVSIM v2的某些舒适度相关指标（如扩展舒适度EC）上，DriveVLA-W0的表现相对较低（58.9），表明其在平滑性控制方面仍有改进空间。</p>
<p><strong>后续研究启示</strong>：这项工作表明，为大规模VLAs设计密集的、与环境动力学相关的自监督任务是释放其数据扩展潜力的有效途径。同时，模型架构（如解码器）的优劣可能高度依赖于数据规模，在大数据时代需要重新评估。如何进一步优化世界建模任务的效率与精度，以及如何更好地平衡动作性能与驾驶舒适度，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自动驾驶中视觉-语言-动作模型面临的“监督赤字”问题，即大模型容量仅由稀疏的动作信号监督，提出DriveVLA-W0训练范式。其核心方法是引入世界模型来预测未来图像，为模型提供密集的自监督信号，以学习驾驶环境的动态。具体实现了适用于离散视觉标记的自回归世界模型和适用于连续特征的扩散世界模型，并采用轻量级动作专家降低推理延迟。实验表明，该方法在NAVSIM基准和大型内部数据集上显著超越BEV和VLA基线，并放大了数据缩放定律，即随着训练数据规模增大，性能提升加速。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.12796" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>