<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.06571" target="_blank" rel="noreferrer">2508.06571</a></span>
        <span>作者: Jiang, Anqing, Gao, Yu, Wang, Yiru, Sun, Zhigang, Wang, Shuo, Heng, Yuwen, Sun, Hao, Tang, Shichen, Zhu, Lijuan, Chai, Jinhao, Wang, Jijun, Gu, Zichong, Jiang, Hao, Sun, Li</span>
        <span>日期: 2025/08/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前端到端自动驾驶领域，基于视觉-语言-动作（VLA）的模型展现出巨大潜力。主流方法（如RecogDrive、ORION）通常采用开环模仿学习，直接从人类驾驶数据中学习策略。然而，这种方法存在关键局限性：它倾向于复制数据集中记录的行为，无法充分探索驾驶任务固有的多目标（安全、效率、舒适）和多模态（多种最优解）特性，导致性能受限。同时，虽然闭环强化学习（RL）能通过与环境交互实现更好的探索，但现有方法严重依赖高保真传感器仿真，面临着巨大的Sim2Real域适应差距和沉重的计算开销。</p>
<p>本文针对这两个痛点，提出了一个新视角：能否构建一个轻量级的“奖励世界模型”来替代复杂的物理仿真器，从而实现对VLA模型高效、可扩展的闭环强化学习训练？本文的核心思路是：提出一个三阶段框架（IRL-VLA），首先通过模仿学习预训练一个VLA策略，然后通过逆强化学习构建一个奖励世界模型，最后利用该模型提供的奖励信号，通过近端策略优化（PPO）对VLA策略进行闭环强化学习微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>IRL-VLA框架是一个三阶段的训练范式，其整体流程如下图所示。</p>
<p><img src="https://arxiv.org/html/2508.06571v3/Figs/01_overall_framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：IRL-VLA框架概览。展示了三阶段流程：a) 模仿策略学习：通过传感器输入和规划轨迹以监督方式初始化VLA模型。b) 逆环境学习：利用预训练的VLA规划轨迹构建奖励世界模型（RWM）。c) 闭环强化学习：使用PPO和RWM优化策略。子图(d)、(e)、(f)分别详细说明了语义推理、3D推理和统一扩散规划器模块。</p>
</blockquote>
<p><strong>第一阶段：模仿策略学习（初始化VLA策略）</strong><br>此阶段旨在建立一个具备基础驾驶行为理解的VLA模型。模型输入为多视角相机图像和自车状态，输出未来轨迹。模型包含三个核心模块：</p>
<ol>
<li><strong>语义推理模块</strong>：基于Senna-VLM框架，采用多图像编码和多视角提示机制，实现深入的场景语义理解（对应图2.d）。</li>
<li><strong>3D推理模块</strong>：使用BEV视觉编码器和适配器将多视角图像编码到BEV空间特征图，再利用检测令牌和地图令牌从BEV特征中学习矢量化地图元素和智能体运动信息（对应图2.e）。</li>
<li><strong>统一扩散规划器</strong>：采用基于扩散的方法生成多样化的未来轨迹分布。它将带噪声的锚点提议轨迹作为输入，通过条件去噪过程，分层融合丰富的场景语义（BEV令牌、地图令牌、检测令牌），最终通过一个轻量级MLP回归头重构出多模态轨迹（对应图2.f）。训练损失结合了轨迹L1重建损失和锚点分类的二元交叉熵损失。</li>
</ol>
<p><strong>第二阶段：逆环境学习（构建奖励世界模型-RWM）</strong><br>此阶段目标是构建一个轻量级、数据驱动的RWM，以替代传统仿真器进行高效的闭环奖励计算。RWM的架构与智能体模型类似，以多视角相机信息和智能体预测的未来轨迹为输入，输出对未来奖励的预测。</p>
<ul>
<li><strong>奖励数据收集</strong>：为训练RWM，需要多样化的（轨迹，奖励）对。奖励来源于NAVSIM仿真器计算的Ego-Pseudo驾驶度量系统（EPDMS）的9个子分数。为了增强数据多样性，论文采用了三种策略：记录扩散过程中每个时间步的轨迹及其EPDMS分数；使用K-means从人类演示数据中采样多种轨迹模式；在每个场景中对多个自车姿态进行仿真。</li>
<li><strong>模型与优化</strong>：RWM为每个EPDMS子指标（NC, DAC, DDC, TLC, EP, TTC, LK, HC）配备一个独立的MLP头，用于预测该指标分数。最终奖励是各子指标的加权和。RWM通过最小化预测分数与仿真器提供的真实分数之间的误差进行训练。</li>
</ul>
<p><strong>第三阶段：基于RWM的强化学习（微调VLA策略）</strong><br>此阶段利用训练好的RWM为环境模型，使用PPO算法对预训练的VLA策略进行闭环微调，以克服模仿学习的偏差。</p>
<ul>
<li><strong>策略优化</strong>：在每次迭代中，从当前策略采样轨迹，由RWM评估并产生奖励。将扩散去噪过程视为一个内在的马尔可夫决策过程，计算整个轨迹的回报和优势值。为了提升批次稳定性，采用了分组标准化优势值。</li>
<li><strong>损失函数</strong>：强化学习损失鼓励策略生成高奖励的轨迹，同时通过KL散度项约束当前策略与参考策略（通常为模仿学习阶段的策略）不要偏离太远。最终的优化损失是强化学习损失与行为克隆损失的加权和，以确保训练稳定性并防止灾难性遗忘。</li>
</ul>
<p>与现有方法相比，IRL-VLA的核心创新在于：1) 提出了首个不依赖仿真器进行训练的、基于强化学习的VLA自动驾驶框架；2) 创新性地使用逆强化学习构建轻量级奖励世界模型，以可扩展且高效的方式解决了闭环训练中奖励计算和Sim2Real差距的难题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要评估基准是<strong>NAVSIM v2</strong>端到端驾驶数据集（包含navtrain和navhard子集）。使用其闭环规划指标<strong>EPDMS</strong>作为核心评估标准，该指标综合了安全、合规、舒适和效率等多个子项。</p>
<p><strong>对比方法</strong>：在Navhard基准上，与多种先进的端到端方法进行了对比，包括PDM-Closed、CV、LTF、DiffusionDrive、WOTE、Hydra-MDP、GTRS-Dense、GTRS-Aug等。</p>
<p><strong>关键实验结果</strong>：<br>论文在Navhard基准上进行了测试，结果如下表所示。</p>
<p><img src="https://arxiv.org/html/2508.06571v3/assets/x1.png" alt="性能对比表"></p>
<blockquote>
<p><strong>表1</strong>：在Navhard基准上的性能对比。IRL-VLA-PT（仅模仿学习阶段）取得了74.4的EPDMS分数，表现优异。经过强化学习微调后的IRL-VLA-RL将分数进一步提升至<strong>74.9</strong>，达到了最先进的性能（SOTA），并在CVPR2025自动驾驶大挑战中获得了第一名。值得注意的是，IRL-VLA-RL在多个子指标上（如EP-自车进度达到96.2）相比模仿学习版本有显著提升。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文进行了消融研究以验证各组件贡献。</p>
<p><img src="https://arxiv.org/html/2508.06571v3/assets/x2.png" alt="消融实验表"></p>
<blockquote>
<p><strong>表2</strong>：对所提VLA智能体组件的消融研究。实验表明，完整的模型（包含3D推理、语义推理和扩散规划器）性能最佳。移除语义推理模块导致性能大幅下降（EPDMS从74.4降至64.9），这凸显了语言模型提供的深层场景理解对于规划至关重要。移除3D推理或使用确定性规划器（MLP）而非扩散模型，也会导致性能下降，证明了三维几何感知和生成多模态轨迹能力的重要性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>IRL-VLA</strong>，一个开创性的、不依赖仿真器的VLA模型强化学习框架。其核心是引入了一个基于逆强化学习构建的、高效的<strong>奖励世界模型（RWM）</strong>，以可扩展的方式实现了闭环奖励估计。</li>
<li>设计了一个新颖的<strong>VLA模型</strong>，集成了语义推理、3D推理和统一扩散规划器，在模仿学习和强化学习两种范式下均能取得优异性能。</li>
<li>在NAVSIM v2基准上取得了最先进的性能，验证了该框架的有效性和泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，在构建RWM的训练数据时，排除了需要两次仿真的Extended Comfort（EC）指标，这可能会对模型全面评估驾驶舒适性造成一定影响。</p>
<p><strong>启示</strong>：IRL-VLA的工作为大规模VLA模型在自动驾驶领域的闭环训练开辟了一条新路径。它证明了通过数据驱动的方式学习一个轻量级的世界/奖励模型，可以有效规避高保真仿真的成本和域差距问题。这一思路可启发后续研究探索更高效、更准确的世界模型构建方法，并将其应用于更复杂的决策和交互场景中，进一步释放大模型在具身智能领域的潜力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文IRL-VLA旨在解决视觉-语言-动作模型在自动驾驶中的两大挑战：模仿学习在开环设置中性能受限，以及闭环训练依赖高保真模拟导致的域差距和计算效率低下。方法采用三阶段框架：首先预训练VLA策略；其次通过逆强化学习构建轻量级奖励世界模型以实现高效奖励计算；最后设计奖励世界模型引导的PPO强化学习以平衡安全、舒适和效率。实验显示，该方法在NAVSIM v2端到端驾驶基准上达到最先进性能，并在CVPR2025 Autonomous Grand Challenge中获得第二名。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.06571" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>