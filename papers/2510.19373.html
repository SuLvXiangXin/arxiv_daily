<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.19373" target="_blank" rel="noreferrer">2510.19373</a></span>
        <span>作者: Bernadette Bucher Team</span>
        <span>日期: 2025-10-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人学习领域通过扩大数据集和模型规模来提升性能已成为主流趋势。然而，这种对数据量的追求往往忽略了数据内容的精细筛选，导致收集到的大规模数据集在多个维度上存在不平衡，例如语言描述、视角、动作基元和视觉场景。其中，动作基元不平衡问题尤为关键，因为它直接影响学习策略的基本行为分布，导致模型偏向于过拟合高资源任务，而在低资源任务上泛化能力差。</p>
<p>针对存在偏差的数据集训练无偏模型，现有方法主要是数据增强和数据重加权。数据增强在机器人数据集规模巨大的情况下面临实际限制；而现有的重加权方法多基于任务复杂度或人工启发式，可能仍无法解决基础运动技能代表性不足的问题。本文聚焦于由动作基元不平衡引起的策略偏差，提出了一种简单、计算高效的采样策略——温度采样，以在训练过程中重新平衡任务分布，旨在提升策略在低资源任务上的性能，同时不损害高资源任务的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的核心方法是温度采样，用于在多任务模仿学习（行为克隆）中缓解因任务数据量不平衡导致的梯度更新偏差。</p>
<p>整体流程是在标准的随机梯度下降（SGD）训练框架内，改变每个训练批次（minibatch）中数据点的采样方式。给定一个包含m个任务的数据集D，各任务数据量大小为|D_i|，且通常|D_1| &gt;&gt; |D_2| &gt;&gt; |D_3| ...。在每次需要构建训练批次时，不再均匀随机采样，而是根据一个与数据量相关的概率分布来采样任务，再从选中的任务中采样具体数据点。</p>
<p><img src="https://arxiv.org/html/2510.19373v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：温度采样方法示意图。通过调整温度参数τ，可以控制从原始不平衡数据分布（左侧）中的采样概率，实现对低资源任务的重新平衡（右侧），从而获得更公平的训练和更好的泛化。</p>
</blockquote>
<p>核心模块即温度采样概率公式。对于任务i，其被采样到的概率p_i^(τ)定义为：<br>p_i^(τ) = (|D_i|^(1/τ)) / (Σ_{j=1}^m |D_j|^(1/τ))<br>其中，τ &gt; 0 是温度参数。此公式类似于玻尔兹曼分布，log|D_i|被视为每个任务的“能量”，τ则是温度。温度参数的作用如下：</p>
<ul>
<li>τ = 1：采样概率与数据量成正比，即退化为最常见的随机采样。</li>
<li>τ &gt; 1：增大小数据量（低资源）任务的相对采样概率，实现上采样。</li>
<li>τ &lt; 1：进一步偏向大数据量（高资源）任务。</li>
</ul>
<p>本文的一个关键创新是采用了动态的温度调度策略，而非固定温度。具体采用余弦升温调度：在训练过程中，温度τ从初始值T_start=1开始，按照余弦函数逐渐升高到最终值T_end=5。公式为：T_warmup(t) = T_start + (T_end - T_start) * (1 - cos(πt))/2，其中t是训练进度的归一化值（从0到1）。作者假设这种“升温”策略允许模型首先从高资源任务学习稳健的表示，然后在训练后期专注于低资源任务的细节，从而实现更好的整体泛化。</p>
<p>与现有方法相比，本文的创新点在于：1）将温度采样这一在自然语言处理等领域被探索的技术，针对机器人学习中的动作基元不平衡问题进行了重新阐述和应用；2）提出了余弦升温调度，并通过实验验证其在机器人策略学习上的优越性；3）方法实现简单，仅需几行代码即可集成到现有训练流程中，且计算开销低。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在三个层面验证方法：玩具实验、机器人仿真实验和真实世界机器人实验。</p>
<p><strong>使用的Benchmark/数据集</strong>:</p>
<ul>
<li><strong>玩具实验</strong>：稀疏奇偶校验任务（Sparse Parity），用于在受控环境下验证采样策略的有效性。</li>
<li><strong>仿真实验</strong>：从RoboCasa和Libero两个大规模机器人仿真数据集中人工构造不平衡子集。例如，在RoboCasa中，为7个拾放任务分配3000条示教，而为其他原子任务仅保留50条示教。</li>
<li><strong>真实世界实验</strong>：在Franka Panda机械臂上收集包含8个任务、共588条示教的不平衡数据集，并训练策略。</li>
</ul>
<p><strong>对比的Baseline方法</strong>:</p>
<ol>
<li><strong>随机采样</strong>：最常用的基线，即τ=1的情况。</li>
<li><strong>固定上采样</strong>：在整个训练过程中，始终以固定倍数（如5倍）上采样低资源任务。</li>
<li><strong>ReMix</strong>：一种基于组分布鲁棒优化的自适应重加权方法，根据任务难度（通过参考模型和代理模型的损失估计）动态调整权重。</li>
</ol>
<p><strong>关键实验结果</strong>:<br>在稀疏奇偶校验玩具实验中，温度采样能加速收敛。</p>
<p><img src="https://arxiv.org/html/2510.19373v1/x2.png" alt="稀疏奇偶校验任务损失曲线"></p>
<blockquote>
<p><strong>图2</strong>：在稀疏奇偶校验任务上的训练损失曲线。使用温度采样上采样低资源任务能带来性能提升，同时揭示了ReMix等方法因对超参数敏感而难以收敛的局限性。</p>
</blockquote>
<p>在RoboCasa仿真实验中，训练BC-Transformer策略。</p>
<p><img src="https://arxiv.org/html/2510.19373v1/x3.png" alt="仿真实验平均子任务成功率"></p>
<blockquote>
<p><strong>图4</strong>：在仿真（RoboCasa）中，不同采样策略下高、低资源任务的平均子任务成功率。温度采样（余弦升温）在低资源任务上取得了最高性能，同时保持了高资源任务的性能。</p>
</blockquote>
<p>在Libero数据集上微调UniVLA基础模型，结果如表1所示。温度采样取得了0.85的整体成功率，优于随机采样（0.76）和固定上采样（0.77），尤其在低资源任务（如Libero-10）上提升明显。</p>
<p>在真实世界Franka Panda机械臂实验中，训练扩散策略。</p>
<p><img src="https://arxiv.org/html/2510.19373v1/figures/new_figures/final_real_.png" alt="真实世界实验结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界实验中，在四个任务（一个高资源，三个低资源）上的成功率对比。余弦升温调度 consistently 提高了成功率，特别是对于低资源任务。</p>
</blockquote>
<p><strong>消融实验总结</strong>:</p>
<ol>
<li><p><strong>模型规模鲁棒性</strong>：如图6所示，在不同规模的Transformer模型（小、基础、大）上，温度采样均能带来收益。对于小模型，它能更有效地利用有限容量。<br><img src="https://arxiv.org/html/2510.19373v1/figures/new_figures/arch.png" alt="不同模型规模下的性能"></p>
<blockquote>
<p><strong>图6</strong>：不同模型规模下的性能对比。余弦升温在不同容量的Transformer架构中都保持了优势。</p>
</blockquote>
</li>
<li><p><strong>不平衡比例的影响</strong>：如图7（对应图8、图9）所示，随着任务间数据不平衡比例的增加（从300:50到3000:50），温度采样相较于随机采样的优势变得更加明显。<br><img src="https://arxiv.org/html/2510.19373v1/figures/new_figures/sim_vals_300.png" alt="300:50不平衡比例下的性能"></p>
<blockquote>
<p><strong>图8</strong>：在300:50数据不平衡分布下，不同采样策略的性能。温度采样在低资源任务上表现最佳。<br><img src="https://arxiv.org/html/2510.19373v1/figures/new_figures/sim_vals_1000.png" alt="1000:50不平衡比例下的性能"><br><strong>图9</strong>：在1000:50数据不平衡分布下，不同采样策略的性能。随着不平衡加剧，温度采样的优势更显著。</p>
</blockquote>
</li>
<li><p><strong>调度策略的影响</strong>：如表2所示，调度方式（升温 vs. 衰减）和调度形状（余弦、线性、指数）都影响最终性能。总体而言，在训练后期更多地采样低资源任务（升温）对低资源任务更有利，而余弦升温调度取得了最佳的综合表现。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一种简单有效的温度采样方法，用于训练数据不平衡下的机器人策略。该方法实现成本低，易于集成。</li>
<li>通过系统性的实验（玩具任务、仿真、真实机器人）验证了该方法在提升低资源任务性能方面的有效性，且不损害高资源任务性能。</li>
<li>进行了深入的消融研究，揭示了方法在不同模型规模、不平衡比例和调度策略下的鲁棒性与行为特性。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>本文采用了固定的训练调度（余弦升温），这需要预先确定训练步数，对训练过程变化的适应性不强。</li>
<li>方法假设任务已被清晰分割且其频率已知，这限制了其在技能异构、无清晰分割的大规模数据集上的直接应用。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>探索对训练步数不敏感或支持持续学习的动态调度策略，例如基于检查点合并的技术。</li>
<li>将温度采样与自动化的技能分解/分割方法结合，以使其能够应用于更广泛、未经标注的机器人数据集，这是将方法推广的关键方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人数据集在动作原语上严重失衡，导致训练的策略偏向高资源任务而低资源任务性能下降的问题，提出温度采样方法。该方法通过调整采样温度平衡数据分布，仅需少量代码即可集成到现有框架。实验表明，相比现有方法，该方法在低资源任务上取得显著性能提升，且不损害高资源任务表现，提升了多任务策略的泛化能力和模型容量利用效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.19373" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>