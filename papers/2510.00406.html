<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.00406" target="_blank" rel="noreferrer">2510.00406</a></span>
        <span>作者: Li, Hengtao, Ding, Pengxiang, Suo, Runze, Wang, Yihao, Ge, Zirui, Zang, Dongyuan, Yu, Kexian, Sun, Mingyang, Zhang, Hongyin, Wang, Donglin, Su, Weihua</span>
        <span>日期: 2025/10/01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型主要通过在大型数据集上进行模仿学习（监督微调）来构建。然而，这种方法在面临分布偏移时容易产生误差累积，微小的偏差可能导致策略进入陌生状态，从而削弱其鲁棒性。强化学习（RL）为克服这些限制提供了有希望的途径，它能够超越示范行为进行优化并鼓励探索。但标准的VLA RL训练面临严峻挑战：基于仿真的RL通常需要数百万次交互且存在显著的仿真到真实差距；真实世界训练成本高昂且存在安全隐患；离线RL则因无法与环境交互而难以处理分布偏移，且无法从自身动作的后果中学习。</p>
<p>本文针对上述痛点，提出了一个新颖的视角：利用一个从真实交互数据中训练得到的数据驱动世界模型（World Model, WM）作为可控模拟器，为VLA模型的强化微调（Reinforcement Fine-Tuning, RFT）提供支持。核心思路是：策略提出的动作序列在世界模型中展开，生成预测的视觉轨迹；通过将此合成轨迹与目标达成的参考视觉轨迹进行比较，设计出密集的、任务相关的“已验证奖励”（Verified Reward）；利用此奖励通过广义强化策略优化（GRPO）高效地微调VLA策略，从而以极低的样本复杂度提升模型的泛化能力和鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLA-RFT框架包含两个阶段：第一阶段预训练世界模型和VLA基础策略；第二阶段利用世界模型作为模拟器进行交互式强化微调。</p>
<p><img src="https://arxiv.org/html/2510.00406v1/x2.png" alt="训练范式"></p>
<blockquote>
<p><strong>图2</strong>：VLA-RFT的训练范式。<strong>预训练阶段</strong>：初始化世界模型（WM）和VLA策略，其中世界模型的7维动作输入格式与VLA的动作输出一致。<strong>强化微调阶段</strong>：VLA根据初始帧和语言指令生成动作块（Action Chunk），在世界模型中进行多轮展开以预测未来状态。基于预测状态计算已验证奖励，并通过GRPO优化更新VLA。</p>
</blockquote>
<p><strong>阶段 I: WM与策略预训练</strong><br>此阶段旨在为后续优化提供稳定初始化。</p>
<ol>
<li><strong>世界模型训练</strong>：世界模型是一个自回归Transformer，其作用是根据当前/历史观测图像和动作序列，预测下一帧的视觉观测。它通过最大似然估计进行训练（公式3），学习捕获环境动态。其输入输出格式与VLA策略对齐，为后续交互奠定基础。</li>
<li><strong>VLA策略预训练</strong>：VLA策略由视觉语言大模型编码器和流匹配动作头构成。在离线专家数据集上，使用流匹配的均方误差损失（公式4）进行预训练，确保策略能够生成稳定的连续动作块。</li>
</ol>
<p><strong>阶段 II: 通过WM交互进行VLA优化</strong><br>此阶段是强化微调的核心，将预训练好的确定性流匹配策略扩展为随机策略以支持探索，并在世界模型中进行策略评估。</p>
<ol>
<li><strong>SDE策略参数化</strong>：为引入探索，在流匹配头旁增加一个结构相似的Sigma Net。在推理时，通过K=10步的离散化积分（前向欧拉法），从噪声初始化生成动作。每一步，流匹配头预测均值μ_k，Sigma Net预测方差σ_k，共同定义一个高斯分布用于采样下一个动作块（公式5, 6, 7）。这种设计将确定性ODE流程推广为随机微分方程过程。</li>
<li><strong>交互式WM模拟与已验证奖励</strong>：给定VLA策略生成的一个完整动作块，世界模型从初始真实帧开始，自回归地生成对应的未来视觉轨迹（公式10）。奖励的计算基于生成的轨迹与离线数据集中真实轨迹（即目标达成参考轨迹）的对比。奖励函数是每帧L1重建损失和感知相似性损失（LPIPS）的加权负和（公式11），提供了密集且与视觉语义对齐的学习信号。</li>
<li><strong>GRPO优化</strong>：在同一初始状态下采样N个策略展开，计算每个展开的奖励。使用组内平均奖励作为基线计算优势（Advantage）（公式12）。策略优化采用GRPO目标函数（公式13），其中包含基于新旧策略平均对数概率比值的裁剪策略梯度项、一个权重较小的辅助流匹配MSE损失项（用于稳定训练）以及一项鼓励探索的策略熵项。</li>
</ol>
<p>与现有方法相比，VLA-RFT的核心创新点在于：1）使用完全数据驱动、可预测视觉观测的世界模型替代传统手工建模或物理仿真器，避免了仿真到真实差距，并支持基于视觉的密集奖励设计；2）设计了基于预测视觉轨迹与真实目标轨迹对比的“已验证奖励”机制，该奖励与任务完成度高度相关且无需人工标注；3）将流匹配策略与随机探索（Sigma Net）结合，并通过GRPO实现了高效稳定的策略优化，样本效率极高。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在LIBERO机器人操作基准上进行评估。使用轻量级VLA-Adapter变体作为基础策略。世界模型基于LLaMA架构，参数量为138M，在LIBERO数据集上预训练。对比基线为不同迭代步数的监督微调基础策略。评估指标为任务成功率。</p>
<p><strong>世界模型能力</strong>：首先评估世界模型生成未来帧的质量。如表1所示，世界模型在像素误差（MSE）、信噪比（PSNR）、结构相似性（SSIM）和感知距离（LPIPS）上均表现优异，表明其能够高保真地预测动作条件化的视觉动态，为后续强化微调提供了可靠的模拟环境。</p>
<p><img src="https://arxiv.org/html/2510.00406v1/figure/world_model_generation.png" alt="世界模型生成"></p>
<blockquote>
<p><strong>表1</strong>：世界模型生成性能。左表：在四个任务套件上的帧级指标及其平均值。右图：定性结果，左侧为仿真器序列，右侧为世界模型根据相同初始帧和动作生成的序列，显示出一致的表观和动作诱导的动态。</p>
</blockquote>
<p><strong>VLA性能提升</strong></p>
<ol>
<li><strong>标准任务套件</strong>：如表2所示，仅进行400步的VLA-RFT微调，平均成功率即从基础策略（15w步SFT）的86.6%提升至91.1%（+4.5%），在所有四个任务套件上均有提升。这证明了该方法在极低样本需求下的高效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.00406v1/x7.png" alt="标准任务性能"></p>
<blockquote>
<p><strong>表2</strong>：LIBERO标准套件下的性能。表格报告了四个套件的成功率，雷达图直观比较了不同模型阶段在各任务上的表现。VLA-RFT (400) 在所有任务上均超越基础策略 (15w)。</p>
</blockquote>
<ol start="2">
<li><strong>扰动任务套件</strong>：为评估分布外鲁棒性，构建了物体位置、目标位置、机器人状态以及组合扰动四种场景。如图4所示。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.00406v1/x4.png" alt="扰动任务示意图"></p>
<blockquote>
<p><strong>图4</strong>：LIBERO中扰动任务设置的图示。包括物体位置偏移、目标位置偏移、机器人状态修改以及所有扰动的组合。</p>
</blockquote>
<p>如表3所示，在各种扰动设置下，VLA-RFT均能比基础策略维持更高的成功率，尤其是在目标位置大扰动和组合扰动等更具挑战性的场景中提升更为明显（最高+6.7%）。这表明强化微调有效增强了策略对未见过环境变化的适应能力。</p>
<p><img src="https://arxiv.org/html/2510.00406v1/x8.png" alt="扰动任务性能"></p>
<blockquote>
<p><strong>表3</strong>：扰动设置下的性能。VLA-RFT在所有类型的扰动（物体位置、目标位置、机器人状态及组合）下，无论是小扰动还是大扰动，成功率均高于基础策略。</p>
</blockquote>
<p><strong>动作分布分析</strong>：图3可视化了策略的动作分布。与监督微调策略（SFT）的分布集中相比，经过RFT训练的策略在动作空间中的覆盖更广，这解释了其为何在面对扰动时具有更强的探索和适应能力。</p>
<p><img src="https://arxiv.org/html/2510.00406v1/x3.png" alt="动作分布"></p>
<blockquote>
<p><strong>图3</strong>：VLA-RFT与VLA-SFT的动作分布可视化。左图为RFT训练后的策略分布，右图为仅SFT的基础策略分布。RFT策略在X和Z动作维度上覆盖更广。</p>
</blockquote>
<p><strong>消融实验（关键因素分析）</strong><br>论文对比了三种不同的奖励设计：R1（动作级L1损失）、R2（基于世界模型隐状态的奖励）和R3（本文使用的基于视觉轨迹对比的奖励）。如表4所示，仅使用本文提出的视觉奖励（R3）带来了最显著的性能提升（+4.5%），显著优于其他两种奖励设计，验证了基于视觉轨迹的已验证奖励的有效性。</p>
<p><img src="https://arxiv.org/html/2510.00406v1/figure/reward_funcv3.png" alt="奖励设计消融"></p>
<blockquote>
<p><strong>表4</strong>：奖励设计对比。左表：基础策略及使用三种不同奖励进行RFT后的平均成功率。右图：对应的奖励函数结构示意图。本文的奖励设计（R3）效果最佳。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）<strong>提出了一种新颖的VLA后训练范式</strong>：利用数据驱动的世界模型作为高保真模拟器，为强化微调提供安全、高效的交互环境，规避了真实世界训练的成本与风险以及仿真到真实的鸿沟。2）<strong>设计了“已验证奖励”机制</strong>：通过对比世界模型预测的视觉轨迹与目标达成参考轨迹，自动生成密集、任务相关的奖励信号，无需人工标注，且与动作长期结果对齐。3）<strong>实现了高效的策略提升</strong>：实验表明，仅需少于400步的微调即可显著超越强监督基线，并在多种分布偏移扰动下展现出卓越的鲁棒性，证明了该框架在提升VLA模型实用性和加速部署方面的潜力。</p>
<p>论文自身未明确讨论局限性，但基于方法描述可推断其可能受限于：1）世界模型的质量严重依赖离线数据集的规模和多样性；2）视觉奖励的计算涉及图像生成与比较，可能带来较高的计算开销；3）方法目前仅在仿真基准上验证，其“sim-to-real”的最终有效性有待在真实机器人平台上进一步检验。</p>
<p>本文的成果为后续研究提供了重要启示：将生成式世界模型与强化学习相结合，有望成为一种通用、高效的VLA能力增强范式。未来工作可以探索更高效的世界模型架构、结合语言反馈的多模态奖励设计，以及将该框架迁移至更复杂的真实世界任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型因依赖模仿学习而导致的错误累积和分布偏移下鲁棒性差的问题，提出VLA-RFT强化微调框架。该方法以数据驱动的世界模型作为可控模拟器，通过预测动作对应的未来观察，并结合GRPO优化框架中的轨迹级验证奖励进行策略更新，提供高效且动作对齐的学习信号。实验表明，仅需0.4K微调步骤（少于400步），VLA-RFT性能即超越需150K迭代的强监督基线，并在扰动环境中保持稳定执行，显著提升了效率与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.00406" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>