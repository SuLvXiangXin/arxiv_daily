<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$ : Robust Imitation via Learning to Search - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$ : Robust Imitation via Learning to Search</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.05294" target="_blank" rel="noreferrer">2506.05294</a></span>
        <span>作者: Gokul Swamy Team</span>
        <span>日期: 2025-06-05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前模仿学习的主流方法是行为克隆（BC），即直接从专家演示中学习策略。然而，BC存在一个根本性局限：它只在专家访问过的状态上教导智能体专家做了什么。这意味着当BC智能体犯错并偏离演示数据的支持集时，它通常不知道如何恢复。这导致了复合误差问题。本文针对智能体在测试时因自身错误而面临未见状态时缺乏恢复能力这一具体痛点，提出了“从专家演示中学习搜索”（Learning to Search, L2S）的新视角。其核心思路是，不直接学习策略，而是学习世界模型和奖励模型，使智能体能够在测试时通过规划来匹配专家的结果，即使是在犯错之后。</p>
<h2 id="方法详解">方法详解</h2>
<p>SAILOR的整体框架是一个残差规划器，它在测试时执行局部搜索以纠正基础策略的错误。其输入是当前观测，输出是修正后的动作序列。流程如下：首先，基础策略根据当前观测生成一个“名义计划”；然后，在学习的潜在世界模型中，对多个叠加了随机残差的修正计划进行随机展开模拟；接着，使用学习的奖励模型和评论家网络对模拟轨迹进行评分；最后，通过MPPI优化器更新残差分布的均值和标准差，选择最优修正计划执行第一步，随后重新规划。</p>
<p><img src="https://arxiv.org/html/2506.05294v2/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：SAILOR推理过程。在推断时，SAILOR在世界模型WM的潜在空间中，针对学习的奖励模型RM和评论家V，搜索残差计划来纠正基础策略名义计划的错误。然后以MPC风格执行最佳修正计划的第一步并重新规划。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>基础策略</strong>：生成k步名义计划。本文实验使用通过BC训练的、基于ResNet-18编码器的扩散策略。</li>
<li><strong>世界模型</strong>：采用循环状态空间模型架构，包含编码器、潜在动力学模型和解码器。关键创新在于在<strong>混合数据</strong>（专家演示数据𝒟和基础策略/SAILOR策略的交互数据ℬ）上训练，以确保模型在智能体可能访问的分布上局部准确。</li>
<li><strong>奖励模型</strong>：一个判别器，用于评估潜在状态的“专家相似度”。其训练采用矩匹配损失，旨在最大化专家轨迹与学习者轨迹的奖励期望之差，并添加梯度惩罚以稳定训练。该模型会在线更新，以检测当前策略所犯的错误。</li>
<li><strong>评论家网络</strong>：作为终端成本估计，使有限步长的展开模拟得以进行。它通过预测λ-回报进行训练，并在使用时采用集成网络以提供不确定性感知的估计。</li>
</ol>
<p>与现有方法相比，SAILOR的创新点在于：1）<strong>无需额外人工反馈</strong>：仅利用标准BC流程中的专家演示，通过混合数据训练和在线更新，推断出恢复错误所需的潜在搜索过程。2）<strong>系统性训练流程</strong>：设计了包含预热启动、在线微调和专家迭代的三阶段训练方案，确保了学习的稳定性和样本效率。</p>
<p><img src="https://arxiv.org/html/2506.05294v2/x1.png" alt="训练流程"></p>
<blockquote>
<p><strong>图1</strong>：SAILOR方法概览。通过在世界模型和奖励模型的混合数据（专家数据和基础策略数据）上学习，赋予智能体在测试时推理如何从基础策略错误中恢复的能力。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在来自三个基准（Robomimic、RoboSuite、ManiSkill）的12个视觉操作任务上进行评估，任务类型多样。观测空间包括RGB图像和本体感知状态。使用50次测试的平均成功率作为指标。</p>
<p><strong>对比方法</strong>：1) <strong>扩散策略</strong>：通过行为克隆训练。2) <strong>DPPO-IRL</strong>：一种模型免费的逆强化学习方法，使用DPPO算法根据学习到的奖励模型直接更新策略参数。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>SAILOR显著优于基础BC方法</strong>：在所有环境和不同演示数据集规模下，SAILOR的成功率均 consistently 超过扩散策略。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.05294v2/x4.png" alt="主要结果"></p>
<blockquote>
<p><strong>图4</strong>：在12个任务上，SAILOR consistently 优于使用相同演示数据训练的扩散策略。这表明L2S范式能从相同专家数据中提取更多信息。</p>
</blockquote>
<ol start="2">
<li><strong>增加BC数据难以弥补差距</strong>：即使将用于BC的演示数据量扩大5-10倍（达到约200条），扩散策略的性能仍会 plateau，且无法达到SAILOR仅用较少数据（如20条）所达到的性能水平。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.05294v2/x5.png" alt="数据缩放"></p>
<blockquote>
<p><strong>图5</strong>：单纯增加BC训练数据量，性能提升会 plateau，且无法匹配SAILOR的性能。即使数据量多一个数量级，差距依然存在。</p>
</blockquote>
<ol start="3">
<li><strong>SAILOR具有更高的交互效率</strong>：与需要直接在真实世界中交互来更新策略的模型免费逆RL方法（DPPO-IRL）相比，SAILOR利用世界模型进行模拟规划，仅需约1/5的交互预算即可达到相当或更优的性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.05294v2/x6.png" alt="交互效率"></p>
<blockquote>
<p><strong>图6</strong>：SAILOR比直接更新策略参数的模型免费逆RL基线（DPPO-IRL）交互效率高得多。后者即使用5倍交互预算也难以追上SAILOR。</p>
</blockquote>
<ol start="4">
<li><strong>奖励模型能检测细微错误</strong>：定性分析表明，学习的奖励模型能够识别复杂任务不同阶段的细微失败（例如成功抓取后工具悬挂的微小偏差），从而使SAILOR能够进行反事实推理并避免这些错误。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.05294v2/x7.png" alt="错误检测"></p>
<blockquote>
<p><strong>图7</strong>：学习的奖励模型能够识别多步任务执行中不同阶段的多种细微失败，SAILOR能够反事实地避免它们。图中对比了基础策略的失败轨迹（紫色）和SAILOR从相同前缀恢复的成功轨迹（橙色）。</p>
</blockquote>
<ol start="5">
<li><strong>消融实验总结</strong>：论文指出，“预热启动”混合世界模型训练、在线混合世界模型微调以及定期将学习的搜索算法蒸馏到基础策略（专家迭代）对于获得良好性能至关重要。专家迭代更新带来了非平凡的性能提升。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出L2S框架SAILOR</strong>：仅从专家演示出发，学习世界模型和奖励模型，使智能体获得测试时通过规划从错误中恢复的能力，超越了传统行为克隆的局限。</li>
<li><strong>系统性的算法设计</strong>：详细拆解并验证了实现稳定、样本高效学习所需的训练阶段和关键组件（预热、在线微调、专家迭代）。</li>
<li><strong>实证验证与深入分析</strong>：在12个视觉操作任务上证明了SAILOR相对于SOTA BC方法及其数据缩放版本、以及模型免费逆RL方法的优越性，并展示了其奖励模型对细微错误的检测能力和对奖励破解的鲁棒性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，学习的世界模型仅在专家和学习者访问的分布上<strong>局部准确</strong>。虽然理论上这足以保证策略在现实中的表现，但限制了恢复的范围。</p>
<p><strong>后续启示</strong>：</p>
<ol>
<li><strong>结合生成与验证</strong>：SAILOR的成功（学习生成器/策略 + 验证器/奖励模型）呼应了LLM领域的趋势（如RLHF、Best-of-N），为具身智能指出了一条结合大规模生成模型与验证模型的道路。</li>
<li><strong>基础世界模型的潜力</strong>：一个有趣的未来方向是在互联网规模的机器人数据集上训练强大的基础世界模型，然后将其嵌入SAILOR框架，可能支持更广泛的错误恢复。</li>
<li><strong>专家迭代的扩展</strong>：将测试时计算蒸馏回策略的范式，可以扩展到更复杂的基础策略（如VLA模型），实现持续的性能提升。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对行为克隆（BC）在模仿学习中存在的根本局限——智能体一旦犯错偏离专家演示状态，便无法自主恢复——提出了一种名为SAILOR的“学习搜索”方法。其核心是通过学习世界模型和奖励模型，使智能体在测试时能够规划并恢复至专家期望的结果。在三个基准测试的十几个视觉操作任务上，SAILOR consistently 超越基于同批数据训练的SOTA扩散策略；即使将BC的演示数据量扩大5-10倍，仍存在性能差距。该方法还能识别细微错误且对奖励黑客攻击具有鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.05294" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>