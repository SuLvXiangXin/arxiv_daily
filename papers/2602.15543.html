<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Selective Perception for Robot: Task-Aware Attention in Multimodal VLA - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Selective Perception for Robot: Task-Aware Attention in Multimodal VLA</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.15543" target="_blank" rel="noreferrer">2602.15543</a></span>
        <span>作者: Soo-Chul Lim Team</span>
        <span>日期: 2026-02-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人领域的视觉-语言-动作（VLA）模型倾向于集成来自多视图输入的多模态信号。然而，大多数现有工作采用静态融合策略，即对所有视觉输入进行统一处理，这导致了不必要的计算开销，并允许与任务无关的背景信息充当噪声。静态融合存在两个根本局限：一是信息冗余和干扰问题，无关数据会阻碍有效的表示学习；二是可能降低模型鲁棒性，无关模态在某些阶段会稀释有意义的信息。</p>
<p>受人类主动感知（根据任务上下文和目标主动调节感官信息权重）的启发，本文针对VLA模型中静态信息融合效率低下且易受噪声干扰的痛点，提出了一种动态信息融合的新视角。核心思路是引入一个轻量级的自适应路由架构，实时分析文本提示和手腕相机观测，以预测多个相机视图的任务相关性，从而有条件地衰减低效用视图的计算，并仅向策略网络提供必要的视觉特征。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个集成动态路由器的VLA模型框架，旨在多视图环境中高效运行。该框架在视觉Transformer（ViT）与预训练的VLA模型之间插入一个相机路由器模块，用于计算每个输入视角信息的重要性，确保提供给策略网络的视觉信息与其任务执行的相关性成比例。</p>
<p><img src="https://arxiv.org/html/2602.15543v1/figs/Fig2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：所提出框架的概述。(a) 实验设置。(b) 所提出框架和动态路由器的架构。路由器以手腕相机和语言提示为输入，预测包括外部、热感和触觉相机在内的多个视觉源的重要性权重，从而优先处理任务相关的传感器信息。</p>
</blockquote>
<p>整体训练流程（如图2所示）采用基于LoRA的微调策略，冻结预训练基础策略的数十亿参数，仅在VLM和动作专家网络的所有权重组中插入轻量级、可训练的LoRA模块进行更新。该框架以端到端方式联合训练，以同时实现最优的机器人动作生成和高效的相机路由。总损失函数定义为动作预测损失和路由器优化损失的加权和：$L_{\mathrm{total}}=L_{\mathrm{action}}+\lambda L_{\mathrm{routing}}$，其中$\lambda$设为0.1，$L_{\mathrm{action}}$使用与基础策略一致的流匹配损失。</p>
<p><strong>核心模块：相机路由器</strong><br>路由器以手腕相机图像和任务提示作为输入来感知当前上下文。手腕相机被指定为基锚视图并始终保持激活，以确保稳定的近端视觉信息可用性。特征提取过程如下：对任务提示应用注意力池化以获得捕捉核心关键词的文本特征$h_{\mathrm{prompt}}$；同时，手腕相机令牌通过平均池化转换为视觉特征$h_{\mathrm{wrist}}$。路由器输入$x$定义为提示和手腕相机特征的拼接，然后通过一个MLP和一个Softmax层来预测剩余$K-1$个视图的重要性权重$W$（例如$W=[W_{\text{external}},W_{\text{thermal}},W_{\text{tactile}}]$）。</p>
<p><strong>动态门控</strong><br>路由器预测的权重$w$与一个可学习参数$\gamma \in \mathbb{R}^{D}$共同作用，决定每种模态对当前任务的贡献：$F_{\mathrm{gated}}=\gamma \odot (w \odot F_{\mathrm{original}})$。其中$\gamma$是一个缩放参数，用于减轻门控过程中的突变并增强训练稳定性。通过此过程，重要性较低的相机视图的特征被自然衰减，而任务相关的视觉信息得到相对强调。随后，视觉特征沿通道维度拼接，并通过线性投影融合为统一的视觉表示$F_{\mathrm{fused}}$，作为预训练VLM的输入。</p>
<p><strong>不平衡路由学习</strong><br>由于某些感官模态（如触觉、热感）在整个任务中并非持续需要，仅在涉及接触或决策的特定时刻稀疏出现，存在数据不平衡问题。为解决稀疏出现的真实标签的不平衡性，路由器使用Focal Loss进行训练：$L_{\mathrm{routing}}=-\alpha_{\text{focal}}(1-p_{t})^{\beta}\log(p_{t})$，其中$\alpha_{\text{focal}}=0.25$，$\beta=2.0$。该损失减少了简单样本的影响，从而将学习重点重新加权到困难且稀疏出现的实例上。</p>
<p><strong>VLM自动标注</strong><br>为高效获取路由器训练所需的大规模标注数据，本文建立了一个利用Qwen3-VL的自动标注流程。该流程采用历史感知提示策略，将输入视频分割为$T=30$帧的块，并从每个块中均匀采样$K=10$张图像输入VLM。提示词包含角色、任务描述、工作流定义、夹爪动作和动作历史等关键要素。VLM根据预定义规则联合考虑视觉输入和上下文信息，确定每个相机的激活状态$D_{\mathrm{vlm}}=[c_{\mathrm{ext}},c_{\mathrm{thm}},c_{\mathrm{tac}}]^{\top}\in{0,1}^{3}$。生成的$D_{\mathrm{vlm}}$通过Focal Loss为路由器提供监督信号。</p>
<p>与现有方法相比，本文的创新点在于：1) 提出了一个动态的、基于任务上下文的路由器，而非静态融合；2) 利用手腕相机作为锚点进行实时重要性预测；3) 引入了基于VLM的自动化、可扩展的标注管道来生成路由器监督信号。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实机器人操作场景中进行，使用了三个具有长时程序列的任务：电池分拣（视觉+热感）、线缆选择（视觉+触觉）和阀门操作（视觉+热感+触觉）。机器人平台配备了手腕相机（Intel RealSense D435）和外部相机（Intel RealSense L515），并根据任务额外集成了热感相机（ThermoEye TMC256B）和/或触觉传感器（Meta Digit）。使用50条专家演示数据对每个任务进行VLA执行器训练。</p>
<p>基线方法包括：仅使用RGB输入的无多模态基线（<code>w/o Multimodal</code>），以及使用多模态输入但无路由器的基线（<code>w/ Multimodal</code>）。本文方法（<code>w/ Router</code>）则集成了动态路由器。</p>
<p><img src="https://arxiv.org/html/2602.15543v1/figs/Experiment_setup.png" alt="实验设置"></p>
<blockquote>
<p><strong>图3</strong>：每个任务的实验设置。(a) 电池分拣，(b) 线缆选择，(c) 阀门操作。所有任务共享用于视觉感知的双RGB相机系统，每个设置还集成了任务特定的模态（如热感相机或触觉传感器）以提供互补的感官信息。</p>
</blockquote>
<p><strong>关键定量结果</strong><br>如表I所示，仅使用RGB输入的基线在所有任务上的成功率为0%。引入多模态输入但无路由器时，平均成功率仅为10.00%。而集成了动态路由器的本文方法，在最佳输入配置（手腕相机+提示）下，平均成功率达到了83.33%。</p>
<p><strong>路由器输入配置对比</strong><br>实验比较了三种路由器输入配置：</p>
<ul>
<li><strong>手腕相机+提示</strong>：平均成功率83.33%（任务1: 90%， 任务2: 90%， 任务3: 70%）。</li>
<li><strong>外部相机+提示</strong>：平均成功率73.33%（任务1: 90%， 任务2: 70%， 任务3: 60%）。</li>
<li><strong>手腕相机+状态</strong>：平均成功率30.00%（任务1: 20%， 任务2: 60%， 任务3: 10%）。<br>结果表明，以手腕相机（提供以目标为中心的近距离视图）和任务提示作为路由器输入效果最佳。而原始机器人状态信息未经特定处理，可能作为噪声干扰训练收敛。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.15543v1/figs/Fig_sequence.png" alt="长时程任务性能"></p>
<blockquote>
<p><strong>图4</strong>：长时程顺序任务的性能。(a) 三阶段任务进程示意图。(b) 每个子任务的成功率。本文方法在整个序列中表现出持续的性能，突出了其在长时程操作场景中的鲁棒性。</p>
</blockquote>
<p><strong>长时程任务鲁棒性</strong><br>如图4所示，在长时程任务中，基线模型（<code>w/ Multimodal</code>）的性能随着任务步骤增加而逐渐下降。相比之下，本文方法（<code>w/ Router</code>）在所有评估任务中始终保持着高达83.33%的平均成功率。这表明路由器提供的权重作为VLA模型的隐式步骤感知信号，有助于在长序列中保持任务上下文，并通过选择性过滤无关模态来减少噪声对推理性能的负面影响。</p>
<p><img src="https://arxiv.org/html/2602.15543v1/figs/Fig_label.png" alt="VLM标注验证"></p>
<blockquote>
<p><strong>图5</strong>：在任务1-3上，三种设置（无标注基线、人工标注、VLM标注数据集）经过10次试验的性能比较。</p>
</blockquote>
<p><strong>VLM自动标注有效性验证</strong><br>如图5所示，比较了使用VLM生成标签和人工标注标签训练的路由策略的性能。在所有任务中，基于VLM的标注达到了83.33%的平均成功率，与人工标注获得的86.67%成功率相当。这验证了所提出的VLM自动标注管道是人工标注的有效且可扩展的替代方案。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li><strong>提出了上下文感知的动态路由与门控机制</strong>：设计了一个轻量级路由器，能够根据手腕相机视图和任务提示实时预测多视图的重要性权重，实现按任务相关性进行动态特征选择和计算资源分配。</li>
<li><strong>提升了长时程任务的鲁棒性</strong>：通过基于路由器的特征融合结构整合关键模态，减轻了模态间干扰，在长时程操作任务上取得了比现有静态融合方法更高的成功率。</li>
<li><strong>引入了可扩展的VLM监督方法</strong>：构建了一个基于VLM的自动标注管道，无需额外数据收集成本即可生成相机标签，为路由器训练提供了高效且可扩展的监督信号来源。</li>
</ol>
<p>论文自身提到的局限性在于，路由器输入配置（手腕相机+提示）的性能优势可能依赖于实验设置（如精细操作任务），在其他配置（如固定外部视角）下可能表现不同，这暗示了路由器设计需要与任务场景和传感器配置相匹配。</p>
<p>本文工作对后续研究的启示在于：动态感知选择是提升机器人VLA模型效率和性能的有效途径；利用大模型（VLMs）进行自动标注和状态理解，是解决机器人学习数据标注瓶颈的一个有前景的方向；未来的多模态融合研究应更多地考虑任务上下文和信息的动态效用，而非简单的静态集成。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人视觉-语言-动作（VLA）模型中静态融合导致计算开销大、任务无关信息作为噪声的核心问题，提出动态信息融合框架。关键技术为轻量级自适应路由架构，通过实时分析文本提示和手腕摄像头观测，预测多视图任务相关性，并条件性衰减低效用视图计算以选择性提供关键特征。实验在真实机器人操作场景中验证，该方法相比现有VLA模型在推理效率和控制性能上均有显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.15543" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>