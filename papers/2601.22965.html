<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.22965" target="_blank" rel="noreferrer">2601.22965</a></span>
        <span>作者: Wuyue Zhao Team</span>
        <span>日期: 2026-01-30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉导航中的路径规划主流方法之一是学习基于扩散的策略。这些方法通过迭代去噪建模完整轨迹，具有良好的时空一致性，并能捕获多模态轨迹分布以适应复杂环境。然而，当前大多数扩散策略依赖模仿学习进行训练，这带来了两个关键瓶颈：首先，专家数据集的有限覆盖会损害策略的鲁棒性，使其难以应对分布偏移和新场景；其次，标准模仿学习试图模仿所有给定的轨迹（包括大量次优轨迹），导致生成轨迹的质量参差不齐。因此，在推理时通常需要依赖“先生成-后过滤”的流程，即生成大量候选轨迹，再通过辅助选择器进行筛选，这显著增加了计算延迟，尤其在资源受限的设备上。</p>
<p>本文针对传统模仿学习扩散策略存在的次优性、冗余性以及由此导致的低效推理流程这一具体痛点，提出了自模仿的新视角。核心思路是：提出自模仿扩散策略，让策略通过选择性模仿从自身采样得到的高质量轨迹来学习改进规划，从而引导轨迹分布集中，实现无需后过滤的高效、鲁棒推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>SIDP框架的核心是奖励引导的自模仿机制。其整体流程（对应论文图2）分为三个阶段：1）在线采样：策略根据当前状态生成N个候选轨迹；2）过滤与加权：根据预定义的奖励函数评估这些轨迹，选取奖励最高的k个，并计算归一化的指数重要性权重；3）参数更新：使用这些加权后的高质量轨迹计算加权去噪损失，更新策略参数。这是一个迭代的、在线策略的优化过程。</p>
<p><img src="https://arxiv.org/html/2601.22965v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：自模仿扩散策略概览。框架使用当前策略生成候选轨迹，并通过基于奖励的排序门进行过滤。高奖励样本随后被用于计算加权去噪损失，更新策略参数以迭代地对齐最优分布。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>奖励引导的自模仿机制</strong>：这是SIDP的核心创新。其理论基础是将最大化期望奖励的问题转化为策略分布与最优分布之间的KL散度最小化问题。通过重要性采样，利用当前策略作为提议分布生成候选轨迹。最优分布被形式化为与当前策略成比例、并由奖励指数加权的形式。因此，分配给候选轨迹的重要性权重仅取决于其奖励值，具体公式为归一化的Softmax（奖励/温度τ）。最终，优化目标转化为最小化<strong>奖励加权的去噪损失</strong>，即SIDP损失函数，它鼓励策略更专注于模仿自身生成的高质量轨迹。</p>
</li>
<li><p><strong>奖励函数设计</strong>：奖励是引导自模仿的关键，论文中设计了包含安全与效率的多组件奖励（如表I所示），包括：碰撞惩罚、步长成本（鼓励路径更短）、进度奖励（基于到目标地理距离的减少）以及精细停靠奖励（在接近目标时激活）。</p>
</li>
<li><p><strong>互补学习策略</strong>：</p>
<ul>
<li><strong>目标无关探索</strong>：在训练中随机采样辅助目标点，并将输入状态中的目标点替换为一个特殊的“目标无关”嵌入，同时将重要性权重设为均匀。此策略旨在激活策略的探索能力，增加轨迹池的多样性，并对点目标导航任务起到正则化作用。</li>
<li><strong>奖励驱动的课程学习</strong>：动态筛选具有高学习潜力的训练场景。判断标准基于当前策略在该场景下生成轨迹的<strong>最大奖励</strong>和<strong>奖励范围</strong>。只有最大奖励足够高（保证可行性）且奖励范围足够大（保证梯度信息量）的场景才会被用于当前训练，以此提高数据效用，促进稳定收敛。</li>
</ul>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：<br>与依赖固定专家数据集的传统模仿学习扩散策略（如NavDP）相比，SIDP的核心创新在于用<strong>在线、奖励加权的自模仿</strong>替代了离线的专家模仿。这避免了专家数据的局限性，并通过迭代地从自身高质量经验中学习，使策略的轨迹分布逐渐集中，从而在推理时<strong>无需</strong>生成大量样本并进行后过滤，实现了端到端的规划。相较于需要通过时间反向传播来优化扩散模型的RL方法，SIDP将优化问题转化为更稳定的模仿学习目标，避免了BPTT的计算负担和数值不稳定性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：主要在<strong>InternVLA-N1 S1</strong> 基准上进行评估，这是一个使用高保真Isaac Sim模拟器的闭环顺序导航基准，包含60个多样场景（家庭、商业、简单/困难杂乱环境）。</li>
<li><strong>对比方法</strong>：包括基于学习的规划基线<strong>iPlanner</strong>和<strong>ViPlanner</strong>，以及作为主要基线的纯模仿学习扩散策略<strong>NavDP</strong>。</li>
<li><strong>评估指标</strong>：成功率（SR）、路径长度加权成功率（SPL）。消融实验中还使用了碰撞率（CR）、到目标距离（DTG）和探索面积（EA）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>导航性能对比</strong>：如表II所示，SIDP在所有场景类型和难度级别上均取得了最高的平均成功率（mSR 79.11%）和平均SPL（mSPL 72.72%）。在最具挑战性的InternScenes场景（商业和家庭）中，SIDP相比NavDP在SR上分别有约9.94%和5.79%的显著提升，证明了其在复杂、未见过的非结构化环境中更强的泛化能力和鲁棒性。</p>
</li>
<li><p><strong>计算效率</strong>：如表III所示，在Jetson Orin Nano边缘设备上，SIDP（使用5步DDIM采样）的推理延迟仅为110毫秒，相比NavDP（273毫秒）实现了<strong>2.5倍的加速</strong>，且成功率更高（0.674 vs 0.549）。这得益于分布集中后，无需进行多候选采样和过滤，并允许使用更高效的确定性采样器（DDIM）和更少的去噪步数。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.22965v1/ablation_curve_multi.png" alt="学习曲线"></p>
<blockquote>
<p><strong>图3</strong>：不同温度系数τ下SIDP的训练学习曲线（平滑后）。τ=1.0的指数加权策略在收敛速度和最终性能上均优于线性加权基线（Lin. Weig）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.22965v1/temperature_ablation.png" alt="温度消融"></p>
<blockquote>
<p><strong>图4</strong>：温度系数τ的消融研究结果（在自定义基准上）。τ=1.0在成功率（SR）和碰撞率（CR）上取得了最佳平衡。τ过大（选择压力小）或过小（过于贪婪）都会导致性能下降。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验分析</strong>：<ul>
<li><strong>奖励引导自模仿的有效性</strong>：图3和图4表明，基于指数的重要性加权（τ=1.0）显著优于线性加权基线，证明了其提供适当选择压力以区分轨迹质量的重要性。温度τ需要适中，以平衡探索与利用。</li>
<li><strong>目标无关探索的作用</strong>：图5显示，启用该策略能显著增加<strong>探索面积（EA）</strong>，并降低<strong>碰撞率（CR）</strong>，说明其有效提升了策略的探索能力和安全性，起到了正则化作用。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2601.22965v1/nogoal_ablation.png" alt="目标无关探索消融"></p>
<blockquote>
<p><strong>图5</strong>：目标无关探索策略的消融结果。启用该策略（w/ NoGoal）显著提高了探索面积（EA）并降低了碰撞率（CR），验证了其提升探索多样性和安全性的作用。</p>
</blockquote>
<pre><code>*   **奖励驱动课程学习的作用**：图6显示，使用课程学习策略能加速训练初期的收敛，并带来最终性能的稳定提升。
</code></pre>
<p><img src="https://arxiv.org/html/2601.22965v1/curriculum_ablation.png" alt="课程学习消融"></p>
<blockquote>
<p><strong>图6</strong>：奖励驱动课程学习的消融结果。使用课程学习（w/ Curriculum）在训练早期收敛更快，且最终性能更优。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>自模仿扩散策略</strong>框架，通过奖励引导策略从自身高质量经验中学习，增强了鲁棒性，并促使轨迹分布集中，从而摒弃了传统扩散规划器所需的密集采样和后过滤流程，简化了推理。</li>
<li>引入了<strong>目标无关探索</strong>和<strong>奖励驱动课程学习</strong>两种互补训练策略，分别用于增加轨迹多样性、正则化导航任务以及提高困难场景下的数据效用和训练稳定性。</li>
<li>在高质量仿真基准上实现了最先进的导航性能，并在边缘计算设备上实现了显著的推理加速（2.5倍），通过了多机器人平台的实物验证，证明了其在实际机器人应用中的有效性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提及的局限性主要在于训练过程仍需要在线采样和奖励评估，这可能带来一定的计算成本。此外，方法依赖于精心设计的奖励函数。</p>
<p><strong>对后续研究的启示</strong>：<br>SIDP展示了自模仿学习在提升扩散策略性能和效率方面的潜力。其框架可以扩展到其他需要序列决策的机器人任务中。未来的工作可以探索更高效的自模仿数据收集机制、自适应温度调整策略，或者将自模仿与少量专家演示或其他离线数据结合，以进一步加速训练和提高性能上限。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出自模仿扩散策略（SIDP），以解决视觉导航中传统扩散策略因模仿学习而继承专家演示的次优性与冗余，导致推理时依赖计算密集的“生成-过滤”流程的问题。SIDP采用奖励引导的自模仿机制，使策略选择性地模仿自身采样的高质量轨迹，结合奖励驱动的课程学习与目标无关的轨迹增强，提升规划效率与鲁棒性。实验表明，SIDP在仿真与实物平台上均显著优于基线方法，在Jetson Orin Nano上推理速度达110ms，比基线NavDP（273ms）快2.5倍，实现了高效实时部署。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.22965" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>