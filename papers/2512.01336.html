<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Discovering Self-Protective Falling Policy for Humanoid Robot via Deep Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Discovering Self-Protective Falling Policy for Humanoid Robot via Deep Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.01336" target="_blank" rel="noreferrer">2512.01336</a></span>
        <span>作者: Donglin Wang Team</span>
        <span>日期: 2025-12-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人近年来虽取得显著进展，但由于其形态、动力学特性及控制策略的限制，相比四足或轮式机器人更容易摔倒。其重量大、质心高、自由度多的特点使得不受控制的摔倒可能导致严重的硬件损坏。该领域现有研究大多基于控制方法，这些方法难以适应多样化的摔倒场景（例如，不同姿态、不同推力方向），并且可能引入不适用于机器人刚性身体的人类先验（例如，优先使用膝盖或骨盆缓冲）。本文针对这些痛点，提出了一种新视角：避免强加人类先验，转而利用大规模深度强化学习（DRL）和课程学习（CL），激励人形机器人智能体自主探索并发现最适合其自身物理特性的摔倒保护策略。本文的核心思路是，通过精心设计的奖励函数和域多样化课程，在仿真中训练机器人学习形成“三角形”支撑结构，从而显著降低其刚性身体在摔倒过程中受到的损伤。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架基于并行仿真训练。训练在Nvidia Isaacgym中进行，使用了数千个环境实例。策略学习采用近端策略优化（PPO）和利普希茨约束策略（LCP），并集成了正则化在线适应（ROA）框架来处理部分可观测问题：在训练时向策略提供特权信息，在部署时则根据过去10步的观测值推断这些信息。高层DRL策略以50Hz频率输出29维关节目标位置，底层PD控制器（200Hz）将其转换为驱动机器人的扭矩。</p>
<p><img src="https://via.placeholder.com/400x200" alt="学习框架图示"></p>
<blockquote>
<p><strong>图2</strong>：本文学习框架的示意图。核心是利用大规模并行仿真、深度强化学习和课程学习，激励机器人自主探索摔倒保护行为。</p>
</blockquote>
<p>方法的核心模块是精心设计的奖励函数和训练课程。奖励函数（详见表I）旨在平衡多个目标：1）惩罚机器人所有身体部位、关节和执行器受到的损伤（主要目标）；2）在最小化人类先验的前提下，保护头部和躯干等关键部位。奖励项包括惩罚根关节线速度、惩罚关键刚体（头、躯干、骨盆）的接触力、惩罚所有刚体的接触力（脚踝接触力权重降低）、惩罚执行器脉冲（关节运动到硬件限位时的碰撞）、惩罚关节位置超限、惩罚驱动扭矩超限、惩罚大扭矩、惩罚动作变化率以及惩罚关节加速度。这种设计旨在避免机器人陷入“奖励破解”行为（例如，直接将手臂伸直撑地导致关节损坏，如图3所示）。</p>
<p><img src="https://via.placeholder.com/300x150" alt="不当行为示例"></p>
<blockquote>
<p><strong>图3</strong>：难以转移到现实世界部署的行为示例。直接激励机器人用手臂支撑可能导致其将手臂伸直抵住地面，这在实际部署中很可能损坏关节。</p>
</blockquote>
<p>训练课程的多样化（详见表II）对于学习适应各种摔倒场景的策略至关重要。课程从多个角度进行域随机化，并随训练进度<code>p</code>线性增加难度，包括：随机化初始关节位置、额外随机化腰部及手臂关节、随机化初始根关节线速度、随机化外部推力的大小、方向和作用刚体（头、躯干、骨盆），以及随机模拟执行器故障（特定关节输出扭矩恒为零）。这种设计确保了策略能够泛化到广泛的意外摔倒情况。</p>
<p>与现有方法相比，本文的创新点具体体现在：1）不预设人类摔倒动作，而是通过奖励函数引导机器人自主发现符合其刚性身体动力学的最优行为；2）通过全面的课程学习模拟多样化的摔倒诱因，提升了策略的鲁棒性和泛化能力；3）发现并利用了“三角形”支撑结构这一关键行为来分散冲击力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：本文建立了三个综合测试基准环境来量化评估摔倒损伤：1）<strong>站立推</strong>：机器人初始站立，受到外部推力（力大小、方向、作用刚体均变化），共72种配置；2）<strong>行走推</strong>：机器人初始正在行走，受到外部推力（行走速度、推力方向、作用刚体变化），共72种配置；3）<strong>行走故障</strong>：机器人行走时某个执行器发生故障（输出扭矩为零），共18种配置。实验平台为Unitree G1机器人模型。</p>
<p><strong>对比方法</strong>：1) <strong>Baseline</strong>：使用LCP训练的原站立和行走策略，未针对摔倒专门训练；2) **零扭矩控制 (ZTC)**：完全无控制，用于测试无控制下的摔倒行为；3) **默认位置控制 (DPC)**：策略始终输出初始姿态，被动依赖底层PD控制器抵抗扰动；4) <strong>Ours</strong>：本文提出的方法。</p>
<p><strong>评估指标</strong>：采用接触力、运动能量和执行器脉冲三项指标，并采用上均值（最高的5%数据点的平均值）作为鲁棒统计度量。</p>
<p><img src="https://via.placeholder.com/500x300" alt="聚合摔倒损伤结果"></p>
<blockquote>
<p><strong>图5</strong>：在各种摔倒场景下聚合的摔倒损伤结果。本文方法（Ours）在接触力和运动能量指标上表现最佳，其执行器脉冲略高表明主动使用了某些关节来抵抗摔倒。</p>
</blockquote>
<p><strong>关键结果</strong>：如图5所示，ZTC在所有指标上表现最差，表明完全失控的摔倒损伤最严重。DPC因被动抵抗，执行器脉冲最小，但在接触力和运动能量上仅略优于ZTC和Baseline。Baseline的接触力和执行器脉冲甚至比DPC更差，表明未专门训练的主动策略可能造成更大伤害。<strong>本文方法（Ours）在运动能量和接触力上取得了最佳性能</strong>，其执行器脉冲略有增加，意味着策略主动使用了某些关节来形成保护。</p>
<p><img src="https://via.placeholder.com/600x400" alt="分项摔倒损伤结果"></p>
<blockquote>
<p><strong>图9</strong>：分项实验结果。上图展示了不同被推刚体下的损伤，中图展示了不同行走速度下的损伤，下图展示了不同故障执行器下的损伤。本文方法在多数配置下均能有效降低损伤。</p>
</blockquote>
<p><img src="https://via.placeholder.com/600x400" alt="损伤热力图"></p>
<blockquote>
<p><strong>图10</strong>：不同刚体/关节的损伤随时间步变化的热力图。可以观察到骨盆在所有摔倒模式下都持续受到碰撞，而关键的手臂关节（如腕部俯仰、肩部偏航）在碰撞后承受了中等程度的接触力，表明它们被主动用作支撑。</p>
</blockquote>
<p><img src="https://via.placeholder.com/600x400" alt="损伤分布差异"></p>
<blockquote>
<p><strong>图11</strong>：Ours与DPC在损伤分布上的差异图。正值为Ours损伤增加，负值为减少。结果显示，Ours显著减少了头、躯干、骨盆的运动能量和接触力，同时增加了肩、肘、腕等部位的相关损伤，验证了其将损伤从关键部位转移至用于支撑的上臂的机制。</p>
</blockquote>
<p><strong>行为分析</strong>：可视化表明（图6，图7），学习到的策略会主动利用双臂形成“三角形”结构来抵抗摔倒冲击，并支撑关键身体部位避免严重损伤。同时，双腿会有意展开以进一步降低质心，抵抗地面上的线性移动或滚动。</p>
<p><strong>Sim2Real转移</strong>：策略被成功部署到真实的Unitree G1机器人上。在机器人原地行走时施加推力，策略能使其在摔倒过程中形成类似的“三角形”支撑结构进行自我保护（图8）。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）为量化人形机器人摔倒损伤建立了全面的基准和代表性指标；2）提出并实现了一种基于DRL和CL的框架，能够激励机器人智能体在尊重自身物理属性的前提下，自主发现有效的摔倒保护策略，关键行为是形成“三角形”支撑结构；3）成功将仿真训练的策略转移至真实机器人平台，验证了方法的实用性。</p>
<p>论文自身提到的局限性在于，未来工作希望探索方法在更多样化摔倒场景下的有效性，并增强对周围环境的感知，以使机器人在摔倒时能与附近物体进行适当交互。</p>
<p>本文对后续研究的启示包括：1）证明了深度强化学习在发现符合机器人本体特性的最优底层行为（而非模仿人类）方面的巨大潜力；2）精心设计的奖励函数和渐进式的课程学习对于解决复杂、动态的机器人控制问题至关重要；3）“三角形”结构这一发现为刚性人形机器人的被动安全设计提供了新的生物学启发思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在多样化摔倒场景中易受严重硬件损伤的问题，提出一种基于深度强化学习的自我保护摔倒策略。核心方法是通过精心设计的奖励函数和领域多样化课程学习，激励机器人自主探索适应其刚性本体特性的保护行为。研究发现，机器人通过形成“三角形”支撑结构，能显著减少摔倒冲击。实验表明，该方法能适应多种摔倒场景，性能优于传统基于模型或引入人类先验的方法，并成功迁移至真实机器人平台。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.01336" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>