<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.10874" target="_blank" rel="noreferrer">2511.10874</a></span>
        <span>作者: Jiaoyang Li Team</span>
        <span>日期: 2025-11-14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>多机器人协同操作是机器人学的核心目标之一，但现有方法通常陷入两个极端。一方面，基于学习的方法（如分层强化学习）旨在从数据中捕获整个过程，但通常牺牲了泛化性和可扩展性，仅在少数机器人、单个物体的简化环境中得到演示。另一方面，基于规划的方法（如任务与运动规划）利用结构计算协调轨迹，但严重依赖完美的模型，在现实世界中部署时可能很脆弱。这两种方法都难以处理长视野任务中多样化的物体。</p>
<p>本文针对上述痛点，提出了一种中间路线：在结构允许的地方使用规划，在模型不确定或难以手工设计的地方应用学习。具体而言，本文提出了一个统一的框架，用于协作式多机器人、多物体的非抓取式操作，该框架将流匹配协同生成与匿名多机器人运动规划相结合。其核心思路是：利用生成模型从视觉观察中共同生成接触构型和操作轨迹，同时利用新颖的运动规划器进行大规模机器人协调，从而将机器人层面和物体层面的推理统一在单一算法框架内。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的生成式协作框架（GCo）是一个闭环框架，其整体流程如算法1所示，核心在于交替进行数据驱动的操作运动生成和实现这些运动的协作轨迹规划。</p>
<p><img src="https://arxiv.org/html/2511.10874v1/figures/model_ill3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图27</strong>：GCo 框架概览。左侧为感知与物体级规划：根据观察到的图像和世界状态，为每个物体规划短视野轨迹并提取目标变换。右侧为协同生成与机器人级规划：根据图像、变换和分配的机器人预算，通过流匹配模型共同生成离散的接触点和连续的操作轨迹；然后规划机器人到达这些接触点的协作轨迹，最终拼接并执行。</p>
</blockquote>
<p>框架每次迭代始于感知，获取物体图像和世界状态。接着，在物体层面调用新的运动规划器 Gspi，为每个物体生成短视野轨迹，并从中提取目标变换 $T^j$。随后，将机器人预算 $B^j$ 分配给需要操作的物体。核心的<strong>操作模块</strong>是一个基于流匹配的协同生成模型 $\pi_\theta$，它接收物体图像 $\mathcal{I}^j$、目标变换 $T^j$ 和机器人预算 $B^j$，共同输出物体中心的接触点 $\mathcal{K}^j$ 和以原点为根的操作轨迹 $\mathcal{T}^j_{\text{manip}}$。最后，在机器人层面再次调用 Gspi，规划将机器人送至各接触点的协作轨迹 $\mathcal{T}_{\text{coop}}$，并与相应的操作轨迹拼接后执行。此循环重复直至所有物体到达目标位姿。</p>
<p><strong>核心模块1：流匹配协同生成的操作模型</strong>。本文探索了三种具体实现：</p>
<ol>
<li><strong>GCo$_{C\mathcal{T}}$（直接连续轨迹生成）</strong>：将任务视为单一合成问题，直接生成连续机器人轨迹，其起始点被视为接触点。这种方法需要在高维空间中进行推理，学习难度较大。</li>
<li><strong>GCo$_{CC}$（连续-连续协同生成）</strong>：将生成分解为两个结构化的连续部分：在物体局部坐标系中的连续接触点 $\mathcal{K}^j$ 和以原点为根的连续操作轨迹 $\mathcal{T}^j_{\text{manip}}$。两个连续的流匹配速度场 $u_t^\theta$ 和 $u_t^\phi$ 被耦合训练和演化，显式建模了“何处接触”和“如何移动”的结构。</li>
<li><strong>GCo$_{DC}$（离散-连续协同生成）</strong>：这是本文的关键贡献。它将接触点生成<strong>离散化</strong>，直接从输入图像有限的像素集合中选择接触点（表示为离散令牌序列），而操作轨迹仍为连续生成。一个离散流匹配速度场 $u_{\text{d},t}^\theta$ 和一个连续流匹配速度场 $u_{\text{c},t}^\phi$ 被<strong>协同</strong>训练，在生成时同时通过分类马尔可夫链演化离散状态和通过ODE积分演化连续状态。这种方法将接触点空间锚定在感知证据上，同时保持了运动生成的灵活性，被证明在挑战性任务中最稳定。</li>
</ol>
<p>所有模型均支持<strong>可变机器人预算</strong>，通过特殊的掩码令牌 <code>[M]</code> 来标记未使用的机器人，并训练模型在预算不足时屏蔽相应条目。</p>
<p><strong>核心模块2：Gspi 匿名多机器人运动规划器</strong>。Gspi 是专为使用运动基元的连续域设计的新规划器。它结合了基于规则的多智能体路径查找方法（如TSWAP）的效率，以及源自TSWAP和DEC-UNAV的死锁解决策略，并进行了适配以避免在有体积的机器人中引入死锁或活锁。Gspi 在GCo框架中扮演双重角色：既用于物体层面的目标变换规划，也用于机器人层面的协作运动规划，实现了机器人实体和物体实体在统一规划框架下的协调输送。</p>
<p><strong>创新点</strong>：与现有方法相比，本文的创新具体体现在：1) 首次在机器人学中应用离散-连续流匹配协同生成来联合推理接触点（离散）和操作轨迹（连续）；2) 提出了Gspi规划器，将高效的匿名路径查找思想扩展到连续运动基元领域；3) 通过GCo框架，将数据驱动的接触/操作生成与几何完备的运动规划无缝集成，实现了可扩展的协作操作。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在具有挑战性的模拟环境中进行，包含多种场景：空场地、带有障碍的“墙”环境、以及需要迂回前进的“slalom”环境。任务分为<strong>单物体操作</strong>（多个机器人协作移动一个物体）和<strong>多物体操作</strong>（多个机器人协作移动多个物体至一组目标位姿）。对比的基线方法包括基于学习的MAPUSH方法，以及基于规划的KG-HS方法。评估指标包括任务成功率、每个机器人的平均轨迹成本（衡量效率）和规划迭代时间。</p>
<p><strong>关键结果 - 操作任务</strong>：<br><img src="https://arxiv.org/html/2511.10874v1/figures/manip/single/success_rate_vs_robots.png" alt="单物体操作成功率"></p>
<blockquote>
<p><strong>图3</strong>：单物体操作任务中，不同方法随机器人数量增加的成功率。GCo的三个变体（尤其是GCo$_{DC}$）在所有环境和机器人数量下均显著优于基线MAPUSH和KG-HS。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.10874v1/figures/manip/multi/success_rate_vs_robots.png" alt="多物体操作成功率"></p>
<blockquote>
<p><strong>图6</strong>：多物体操作任务中，GCo方法（特别是GCo$_{DC}$）在成功率和可扩展性上明显优于基线MAPUSH。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.10874v1/figures/manip/single/cost_per_robot_by_environment_line.png" alt="单物体操作成本"></p>
<blockquote>
<p><strong>图4</strong>：在单物体操作中，GCo方法产生的每个机器人轨迹成本通常低于或与KG-HS相当，且远低于MAPUSH，表明其效率更高。</p>
</blockquote>
<p><strong>关键结果 - 运动规划能力</strong>：<br><img src="https://arxiv.org/html/2511.10874v1/figures/amrmp/final_frame_high_res_80_robots.png" alt="Gspi规划大规模场景"></p>
<blockquote>
<p><strong>图9</strong>：Gspi规划器成功协调80个机器人通过复杂狭窄通道的定性演示。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.10874v1/figures/amrmp/obs/success_rate_vs_robots.png" alt="规划器成功率对比"></p>
<blockquote>
<p><strong>图16</strong>：在包含障碍物的场景中，Gspi规划器的成功率显著高于基线TSWAP和CBS，尤其在机器人数量较多时优势明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.10874v1/figures/amrmp/obs/avg_iteration_time_vs_robots.png" alt="规划器耗时对比"></p>
<blockquote>
<p><strong>图18</strong>：Gspi的平均迭代时间远低于基于搜索的CBS，与高效的TSWAP相当，展现了其可扩展性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：论文通过比较GCo$<em>{C\mathcal{T}}$、GCo$</em>{CC}$和GCo$<em>{DC}$三种变体进行了消融研究。结果表明，将接触点生成与轨迹生成分离（协同生成）能提高稳定性和性能。而进一步将接触点生成离散化并锚定于图像像素（GCo$</em>{DC}$）带来了最佳的可靠性，特别是在复杂的多物体操作任务中，这验证了离散-连续协同生成设计的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了<strong>生成式协作框架（GCo）</strong>，为可扩展的多机器人、多物体协作操作提供了一个集成学习与规划的统一框架；2) 引入了<strong>流匹配协同生成交互模型</strong>，特别是<strong>离散-连续协同生成（GCo$_{DC}$）</strong>，能够仅从图像观察中生成多样且可行的接触策略与运动轨迹；3) 提出了新的<strong>匿名多机器人运动规划器（Gspi）</strong>，能够在连续运动基元域中高效、无死锁地协调大量实体。</p>
<p><strong>局限性</strong>：论文自身提到的局限性包括：对物体动力学的假设相对简单（如刚性物体、摩擦点接触）；框架依赖于视觉输入的质量和分割；实验主要在模拟中进行，未包含真实的物理不确定性。</p>
<p><strong>后续研究启示</strong>：本文展示了<strong>混合生成建模（离散+连续）与经典运动规划结合</strong>的潜力，为处理机器人学中其他具有混合决策空间的问题（如工具选择与使用、装配序列规划）提供了新思路。未来的工作可以探索更复杂的物理动力学模型、将框架迁移到真实机器人系统，以及扩展用于更广泛的协作任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多机器人协作非抓取操作中，机器人-物体交互分配、接触形态与协调运动联合规划的复杂难题，提出统一框架。核心方法为流匹配协同生成模型，它从视觉观察中联合生成接触形态与操作轨迹，并结合匿名多机器人运动规划器实现大规模协调。实验表明，该方法在模拟挑战性环境中，其运动规划与操作性能均优于基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.10874" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>