<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CLAD: Constrained Latent Action Diffusion for Vision-Language Procedure Planning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>CLAD: Constrained Latent Action Diffusion for Vision-Language Procedure Planning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.06637" target="_blank" rel="noreferrer">2503.06637</a></span>
        <span>作者: Shi, Lei, Bulling, Andreas</span>
        <span>日期: 2025/03/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>程序规划（Procedure Planning）任务旨在给定任务开始和结束的视觉观察（视频片段）时，预测出达成目标所需的中间动作序列。该任务对于人机交互至关重要，例如辅助完成组装、家务等任务。现有主流方法主要基于扩散模型（如PDPP、ActionDiffusion），它们通过去噪过程生成动作序列。然而，这些方法存在一个关键局限性：它们通常仅利用视觉观察，且未能在模型的潜在空间中显式地利用开始和结束状态来约束所生成的动作序列“路径”。扩散模型的潜在空间虽然蕴含语义，但其本身并不具备如何利用约束条件进行去噪的信息。</p>
<p>本文针对程序规划任务提出了一个更具实用性的新变体——<strong>视觉语言程序规划</strong>，即模型的输入除了开始和结束的视觉观察外，还增加了对应的自然语言描述（短语）。这要求模型能够融合多模态信息进行规划。为了解决这一新任务，并更有效地利用开始与结束状态的约束信息，本文提出了CLAD方法。其核心思路是：使用变分自编码器（VAE）在潜在空间中学习融合了视觉与语言信息的开始/结束状态表示，并将这些学习到的“潜在约束”注入到扩散模型去噪网络的最深层，从而引导模型生成更合理的动作序列。</p>
<h2 id="方法详解">方法详解</h2>
<p>CLAD方法的整体流程分为三个阶段：输入处理、潜在约束学习与任务预测、以及基于约束的扩散规划。</p>
<p><img src="https://arxiv.org/html/2503.06637v1/extracted/6264671/figures/method3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：CLAD方法总览。<strong>处理阶段</strong>：使用预训练的文本编码器和图像编码器分别从语言描述和视频帧中提取特征。<strong>潜在约束学习</strong>：将文本和图像特征拼接作为VAE的输入，学习潜在表示。<strong>任务预测</strong>：利用开始和目标的视觉特征预测任务类别。<strong>程序规划</strong>：扩散模型以开始/目标视觉特征和预测的任务类别为条件生成动作序列，并将从VAE学得的潜在约束整合到U-Net的最深层。</p>
</blockquote>
<p><strong>整体流程与输入输出</strong>：</p>
<ol>
<li><strong>输入处理</strong>：给定开始视觉观察 (o_s)、开始语言描述 (N_s)、目标视觉观察 (o_g)、目标语言描述 (N_g)，分别使用S3D预训练模型中的图像编码器和文本编码器提取特征。</li>
<li><strong>潜在约束学习与任务预测</strong>：<ul>
<li>将开始（或目标）的视觉特征与对应的文本特征拼接，作为VAE的输入 (x_s)（或 (x_g)）。VAE的目标是学习一个能够重构该多模态输入的压缩潜在表示。其编码器 (q_\phi) 输出潜在分布的均值 (\mu) 和方差 (\sigma^2)，通过重参数化技巧得到潜在代码 (\mathbf{z_s}) 和 (\mathbf{z_g})（公式6）。这些代码隐式编码了开始与结束状态的空间约束。</li>
<li>同时，使用一个单独的分类网络，根据 (o_s) 和 (o_g) 预测任务类别 (\hat{c})。</li>
</ul>
</li>
<li><strong>约束扩散规划</strong>：<ul>
<li><strong>扩散模型构建</strong>：采用DDPM框架。模型输入 (x_0) 是一个矩阵（公式4），其第一行是任务类别 (c)（训练时为真实类别，推理时为预测的 (\hat{c})），第二行是待生成的动作序列 (a_{1:T})，第三行是开始观察 (o_s) 和目标观察 (o_g)（中间位置用0填充）。去噪网络采用U-Net。</li>
<li><strong>约束整合</strong>：这是方法的核心创新点。将VAE学得的潜在代码 (\mathbf{z_s}) 和 (\mathbf{z_g}) 与一个随机噪声 (\epsilon) 拼接后，通过一个神经网络 (f(\cdot)) 进行融合，得到约束代码 (\mathbf{z_c})（公式7）。随后，将 (\mathbf{z_c}) <strong>加到U-Net最深层的特征上</strong>。这一操作基于一个关键洞察：扩散模型U-Net最深层的潜在空间已包含语义信息，在此处注入约束可以最有效地引导整个去噪过程，使生成的动作序列符合开始与结束状态的限制。</li>
<li><strong>训练</strong>：采用两阶段训练。首先独立训练VAE，其损失为标准的VAE损失（重构损失+KL散度，公式8）。然后冻结VAE权重，训练扩散模型，其损失为预测的均值 (\mu_\theta(x_n, n)) 与干净输入 (x_0) 之间的均方误差（公式9）。</li>
</ul>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：CLAD是首个专门针对视觉语言程序规划任务设计的方法。其核心创新在于提出了“潜在约束”的概念，并设计了一种新颖的约束整合机制——利用VAE在多模态潜在空间中学习开始/结束状态的联合表示，并将其作为软约束注入到扩散模型去噪过程的核心（U-Net最深层），从而更精准地引导序列生成。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验设置</strong>：实验在三个主流的教学视频数据集上进行：CrossTask（18个任务，105个动作类），Coin（180个任务，778个动作类）和NIV（5个任务，18个动作类）。采用了两种不同的数据裁剪设置来提取视觉观察特征：PDPP设置和KEPP设置。评估指标包括严格衡量序列完全正确的成功率（SR）、不考虑顺序的平均准确率（mAcc）和平均单交集并集比（mSIoU）。</p>
<p><strong>对比方法</strong>：与四个先进的程序规划基线方法对比：PDPP、ActionDiffusion、SCHEMA和KEPP。由于这些基线方法原本是为单模态（仅视觉）规划设计的，为了公平比较，在评估时将它们预测的开始和结束动作替换为真实动作。</p>
<p><strong>关键定量结果</strong>：<br>在CrossTask数据集上，CLAD在PDPP和KEPP两种设置下，对于规划长度T=3和T=4，在SR、mAcc和mSIoU三个指标上均显著优于所有基线方法。例如，在T=4的PDPP设置下，CLAD的SR达到30.4%，而最佳基线PDPP仅为19.6%；在KEPP设置下，CLAD的SR为28.1%，最佳基线KEPP为20.6%。在更大的Coin数据集和较小的NIV数据集上，CLAD同样展示了全面的性能优势。</p>
<p><strong>消融实验</strong>：<br>消融实验验证了VAE学得的潜在约束整合机制的有效性。移除VAE约束（即不向U-Net注入 (\mathbf{z_c})）会导致所有指标大幅下降。例如在某个设置下，SR从30.4%降至24.7%，mAcc从53.3%降至48.6%。这证明了将动作和观察的联合表示作为潜在约束整合到扩散过程中，是性能提升的关键。</p>
<p><img src="https://arxiv.org/html/2503.06637v1/extracted/6264671/figures/qualitative_crosstask_T4-1.png" alt="定性结果1"></p>
<blockquote>
<p><strong>图3</strong>：在CrossTask数据集（T=4）上的定性结果示例1。CLAD能够生成完全正确的动作序列（“加入黄油”-&gt;“搅拌”-&gt;“加入面粉”-&gt;“倒入模具”），而基线方法PDPP和ActionDiffusion在中间步骤出现了错误（如误将“搅拌”预测为“打蛋”）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.06637v1/extracted/6264671/figures/qualitative_crosstask_T4-2.png" alt="定性结果2"></p>
<blockquote>
<p><strong>图4</strong>：定性结果示例2。CLAD再次生成完全正确的序列，而基线方法在动作顺序和具体动作上均出现错误，例如PDPP错误地将结束动作“装饰蛋糕”提前到了序列中间。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.06637v1/extracted/6264671/figures/qualitative_crosstask_T4-3.png" alt="定性结果3"></p>
<blockquote>
<p><strong>图5</strong>：定性结果示例3。此案例展示了CLAD如何利用语言描述（“空碗”和“有面糊的碗”）来辅助规划。CLAD成功生成了从“空碗”到“有面糊的碗”所需的正确动作序列，而基线方法则产生了不合理的动作（如“加入牛奶”后未经过“搅拌”就直接“倒入模具”）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）<strong>任务定义</strong>：首次形式化并提出了更具实用性的“视觉语言程序规划”任务，要求模型融合视觉与语言多模态输入。2）<strong>方法创新</strong>：提出了CLAD方法，创新性地使用VAE学习开始/结束状态的潜在空间约束，并将其整合到扩散模型U-Net的最深层，以引导动作序列生成。3）<strong>实验验证</strong>：在三个基准数据集上进行了广泛实验，结果表明CLAD大幅超越了现有的先进方法，并通过消融研究证实了所提约束整合机制的有效性。</p>
<p><strong>局限性</strong>：论文自身提及的局限性主要在于任务设置和数据依赖性。方法性能依赖于视频数据中视觉观察与语言描述的对应关系以及预训练特征提取器的质量。此外，目前的方法是在离散的动作类别空间中进行规划，对于更细粒度的或连续的动作空间可能需要进行调整。</p>
<p><strong>对后续研究的启示</strong>：CLAD展示了在生成模型的潜在空间中显式地融入任务约束（尤其是来自多模态输入的约束）的有效性。这一思路可以扩展到其他需要满足起始/终止条件或遵循特定规则的序列生成任务中。未来的工作可以探索更复杂的约束形式（如物理规则、常识知识），或将方法应用于更长期的规划以及真实的机器人操作场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出CLAD模型，用于解决多模态输入下的程序规划问题：给定起始和结束状态的视觉与语言描述，预测合理的中间动作序列。方法核心是结合变分自编码器（VAE）与扩散模型：先通过VAE学习起始/目标状态的潜在表示作为约束，再将其注入扩散模型的去噪网络深层，以引导潜在空间中的动作序列生成。实验在CrossTask、Coin和NIV数据集上进行，结果表明CLAD显著优于现有最优方法，消融实验验证了VAE潜在约束整合对性能提升的关键作用。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.06637" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>