<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Spatial-Temporal Aware Visuomotor Diffusion Policy Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Spatial-Temporal Aware Visuomotor Diffusion Policy Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.06710" target="_blank" rel="noreferrer">2507.06710</a></span>
        <span>作者: Yanwei Fu Team</span>
        <span>日期: 2025-07-13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>视觉模仿学习是机器人学习复杂任务的有效方法。当前主流方法通常依赖从专家演示中获取的观察-动作对进行行为克隆，并使用2D图像或3D点云等作为状态表示。然而，这些方法大多依赖于对静态、受监督的历史轨迹进行克隆，导致其难以充分捕获环境的3D空间结构以及任务交互中产生的4D时空依赖关系。这种局限性使得模型在需要精确空间推理和动态环境感知的真实世界部署中表现不佳。</p>
<p>本文针对现有方法在3D空间感知和4D时空动态建模上的不足，提出了一个名为4D扩散策略（DP4）的新颖视觉模仿学习方法。其核心思路是引入一个动态高斯世界模型，通过从当前场景重建中学习3D结构，并从未来场景重建中学习4D时空动态，从而为基于扩散模型的策略提供结构化的时空感知监督。</p>
<h2 id="方法详解">方法详解</h2>
<p>DP4的整体框架旨在从单视角RGB-D观测出发，生成机器人的动作轨迹，其核心创新在于利用高斯世界模型为策略学习提供丰富的3D和4D监督信号。</p>
<p><img src="https://arxiv.org/html/2507.06710v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：4D扩散策略（DP4）框架总览。从单视角RGB-D观测构建3D点云，并提取全局和局部特征。这些多层次表征与机器人状态一同作为条件，输入扩散策略模型以生成轨迹。引入的高斯世界模型通过可泛化高斯回归器从当前观测构建3D高斯溅射（3DGS），并利用可变形MLP预测未来3DGS，分别提供3D空间和4D时空监督。</p>
</blockquote>
<p><strong>整体流程</strong>：给定时间步t的单视角RGB-D观测O(t) = (C(t), D(t))，首先利用相机参数将其转换为3D点云。随后，模型并行执行两个主要分支：1）<strong>多层次3D表征提取与空间感知增强</strong>；2）<strong>4D时空动态建模</strong>。提取的时空表征与机器人状态q共同作为条件，输入一个条件去噪扩散模型，以生成最优动作序列a(t)。训练过程同时优化表征学习和策略生成。</p>
<p><strong>核心模块一：多层次3D空间感知</strong><br>该模块旨在从点云中捕获不同粒度的3D结构信息。</p>
<ol>
<li><strong>3D局部表征</strong>：对点云进行裁剪，仅保留关键区域（如机械臂）的点，然后通过一个轻量级MLP（3D局部编码器）进行编码。</li>
<li><strong>3D全局表征</strong>：将完整点云体素化为一个100^3的网格，并通过3D全局编码器编码为深层体素特征v(t) ∈ R^(100^3×64)。</li>
<li><strong>可泛化高斯回归器与空间感知增强</strong>：为了增强全局表征v(t)对场景几何和纹理的理解，引入一个可泛化高斯回归器。它以v(t)为条件，为场景中的每个3D位置x预测一组高斯图元参数θ(t) = (μ, c, r, s, σ)，分别表示位置、颜色、旋转、尺度和不透明度。通过这些高斯图元，使用可微分的基于瓦片的栅格化和alpha混合渲染（公式1），可以重新渲染出RGB图像C(t)和深度图像D(t)。通过最小化渲染结果与真实观测之间的L2损失（公式2，ℒ_3D），迫使3D全局表征v(t)编码更准确的空间结构信息。</li>
</ol>
<p><strong>核心模块二：4D时空感知</strong><br>该模块在静态高斯世界模型的基础上引入动态性，以建模动作引发的场景变化。</p>
<ol>
<li><strong>动态高斯预测</strong>：给定当前时刻的高斯参数θ(t)和智能体动作a(t)，一个可变形MLP p_φ预测高斯参数的变化量Δθ(t)（公式3）。未来时刻的高斯参数通过θ(t+1) = θ(t) + Δθ(t)得到。</li>
<li><strong>未来场景渲染与监督</strong>：将预测的未来高斯参数θ(t+1)通过相同的渲染器ℛ，渲染出预测的未来RGB-D图像C(t+1)和D(t+1)。通过强制要求这些预测图像与真实的未来时刻观测图像一致（公式4，ℒ_4D），模型学会了编码场景级的时空动态。这种“未来场景重建”任务作为一种时空增强惩罚项，引导模型理解物理交互的后果。</li>
</ol>
<p><strong>核心模块三：基于扩散模型的决策</strong><br>最终的策略是一个以时空表征和机器人状态为条件的去噪扩散概率模型。它将随机采样的高斯噪声a^K，通过一个去噪网络ε_θ，经过K次迭代逐步去噪，最终得到干净的动作a^0（公式5）。条件信息包括：优化后的多层次3D表征（融合了局部和全局信息）以及机器人本体状态q。</p>
<p><strong>创新点</strong>：与现有扩散策略（如Diffusion Policy, 3D Diffusion Policy）主要依赖从轨迹中进行行为克隆不同，DP4的核心创新在于主动地利用一个<strong>动态高斯世界模型</strong>作为学习信号。这个模型不仅通过当前场景的3D重建（ℒ_3D）提供了<strong>显式的3D几何监督</strong>，还通过未来场景的预测与重建（ℒ_4D）提供了<strong>显式的4D时空动态监督</strong>，从而将物理世界的结构化知识嵌入到策略表征中。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：论文在广泛的模拟和真实任务上评估DP4。</p>
<ul>
<li><strong>模拟任务</strong>：涵盖4个基准测试的17个任务（共173个变体），包括Adroit（灵巧手操作）、DexArt（灵巧操作）、RLBench（机器人任务套件）以及一个额外的模拟基准。</li>
<li><strong>真实世界任务</strong>：3个真实机器人任务，包括具有挑战性的<strong>可变形物体灵巧操作</strong>。</li>
<li><strong>对比基线</strong>：与最先进的多任务视觉模仿方法对比，包括Diffusion Policy (DP)、3D Diffusion Policy (3DP)、以及这些方法结合Point Cloud和Voxel的变体等。</li>
<li><strong>评估指标</strong>：任务成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>DP4在所有测试领域均显著优于基线方法。</p>
<ul>
<li>在<strong>模拟任务</strong>中：在Adroit上平均成功率提升**16.4%<strong>，在DexArt上提升</strong>14%<strong>，在RLBench上提升</strong>6.45%**。</li>
<li>在<strong>真实世界任务</strong>中：平均成功率提升**8.6%**。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.06710v2/x3.png" alt="对比实验结果"></p>
<blockquote>
<p><strong>图3</strong>：在Adroit、DexArt和RLBench模拟任务上的定量对比结果。DP4（红色）在绝大多数任务上取得了最高的成功率，显著优于其他基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.06710v2/x4.png" alt="真实世界实验结果"></p>
<blockquote>
<p><strong>图4</strong>：真实世界任务性能对比。DP4在三个真实机器人任务（包括可变形物体操作）上均取得了最佳成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.06710v2/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融研究结果。依次移除局部特征（w/o Local）、全局特征（w/o Global）、3D监督（w/o ℒ_3D）和4D监督（w/o ℒ_4D）都会导致性能下降，验证了每个组件的必要性。其中，4D时空监督（ℒ_4D）的贡献尤为关键。</p>
</blockquote>
<p><strong>消融实验总结</strong>：图5的消融实验清晰展示了各模块的贡献。</p>
<ol>
<li><strong>局部与全局特征</strong>：两者都是有效的，移除任一方都会降低性能。</li>
<li><strong>3D空间监督（ℒ_3D）</strong>：移除后性能下降，说明显式的3D重建监督对提升空间感知至关重要。</li>
<li><strong>4D时空监督（ℒ_4D）</strong>：移除后性能下降最为显著，这强有力地证明了<strong>未来场景预测与重建</strong>对于学习交互动态、提升策略在时序上的推理能力具有不可替代的作用。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.06710v2/x6.png" alt="定性对比-模拟"></p>
<blockquote>
<p><strong>图6</strong>：模拟环境中的定性对比。DP4能够成功完成“打开微波炉门”等复杂操作任务，而基线方法（如3DP）则可能失败或动作不精确。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.06710v2/x7.png" alt="定性对比-真实"></p>
<blockquote>
<p><strong>图7</strong>：真实世界任务定性对比。在“用灵巧手拉伸弹簧”任务中，DP4能产生稳定、连续的动作完成目标，而基线方法（DP）的动作则可能导致任务失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.06710v2/x8.png" alt="高斯世界模型可视化"></p>
<blockquote>
<p><strong>图8</strong>：高斯世界模型的可视化。展示了当前观测重建的3DGS以及预测的未来3DGS，直观体现了模型对场景几何和动态变化的捕捉能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了时空感知中心的视觉模仿学习新范式</strong>：DP4突破了传统行为克隆框架，首次将显式的3D空间结构和4D时空动态建模作为策略学习的核心监督信号。</li>
<li><strong>设计了集成动态高斯世界模型的4D扩散策略框架</strong>：创新性地将高斯溅射（3DGS）与可变形场结合，构建了一个既能高效渲染又能预测动态的世界模型，并成功将其与扩散策略耦合。</li>
<li><strong>实现了卓越的性能提升</strong>：在大量多样化的模拟和真实任务上验证了方法的有效性，特别是在需要精细交互和时序推理的灵巧操作及可变形物体操作任务上表现突出。</li>
</ol>
<p><strong>局限性</strong>：论文提到，DP4的性能依赖于RGB-D输入的质量，其世界模型和策略的学习需要相对准确的深度信息。在深度数据极度嘈杂或缺失的情况下，方法可能面临挑战。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>世界模型与策略学习的深度融合</strong>：DP4展示了将具有强物理意义的显式世界模型（如3DGS）用于提升策略泛化能力的巨大潜力，这为未来机器人学习研究提供了一个富有前景的方向。</li>
<li><strong>超越行为克隆的监督信号</strong>：未来可以探索更多类型的物理或几何监督信号（如力觉、物理一致性约束）来增强策略的鲁棒性和可解释性。</li>
<li><strong>应用于更复杂的动态场景</strong>：该方法框架有望扩展到包含更多动态物体、更长时序预测以及部分可观环境下的机器人学习问题中。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出4D Diffusion Policy (DP4)，旨在解决现有视觉模仿学习方法因依赖轨迹克隆而缺乏3D空间与4D时空感知能力的问题。该方法通过动态高斯世界模型引导学习，从单视角RGB-D观测构建当前3D场景并预测未来3D场景，显式建模时空依赖以优化轨迹生成。实验在17个仿真任务（173个变体）和3个真实机器人任务上验证，DP4平均成功率在仿真任务上提升16.4%（Adroit）、14%（DexArt）和6.45%（RLBench），在真实任务上平均提升8.6%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.06710" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>