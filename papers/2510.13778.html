<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.13778" target="_blank" rel="noreferrer">2510.13778</a></span>
        <span>作者: Chen, Xinyi, Chen, Yilun, Fu, Yanwei, Gao, Ning, Jia, Jiaya, Jin, Weiyang, Li, Hao, Mu, Yao, Pang, Jiangmiao, Qiao, Yu, Tian, Yang, Wang, Bin, Wang, Bolun, Wang, Fangjing, Wang, Hanqing, Wang, Tai, Wang, Ziqin, Wei, Xueyuan, Wu, Chao, Yang, Shuai, Ye, Jinhui, Yu, Junqiu, Zeng, Jia, Zhang, Jingjing, Zhang, Jinyu, Zhang, Shi, Zheng, Feng, Zhou, Bowen, Zhu, Yangkun</span>
        <span>日期: 2025/10/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将大型多模态基础模型的能力扩展到物理机器人领域面临一个根本性挑战：机器人不仅需要理解指令的含义，还必须确定在三维世界中“在哪里行动”以及“如何行动”。现有方法主要分为两类。一类是基于规则的分层系统，它们利用基础模型显式编码空间先验，但依赖手工设计的规划启发式方法，使得符号任务结构与低级运动控制之间分离，难以自动扩展到更复杂多样的任务。另一类是数据驱动的视觉-语言-动作模型，它们利用预训练的视觉语言模型和大规模遥操作数据集直接学习机器人控制，但往往过度拟合细粒度的运动行为，而对涉及绝对或相对位置的高级语言指令泛化不足，未能将空间先验知识充分融入执行过程。</p>
<p>本文针对上述痛点，提出将“与具体机器人形态无关的空间先验”作为连接文本指令和具体形态运动命令的桥梁。核心思路是：先通过大规模空间基础预训练学习“在哪里行动”的通用空间推理能力，再通过空间引导的动作后训练，将这些空间先验知识转化为具体机器人的“如何行动”的控制信号。</p>
<h2 id="方法详解">方法详解</h2>
<p>InternVLA-M1是一个双系统、端到端的视觉-语言-动作框架，采用空间引导的两阶段训练流程。</p>
<p><img src="https://arxiv.org/html/2510.13778v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：InternVLA-M1概述。采用空间引导的两阶段训练流程。阶段1（空间基础预训练）：VLM在大规模多源多模态空间基础数据上训练，学习与具体机器人形态无关的空间先验。阶段2（空间引导动作后训练）：作为慢速但可靠的System 2推理器的VLM规划器，通过空间提示生成潜在规划令牌，作为条件输入给作为快速System 1控制器的动作专家（实例化为DiT Actor）来执行。</p>
</blockquote>
<p><strong>整体框架与核心模块</strong>：<br>模型架构上，InternVLA-M1采用双系统设计。System 2（慢推理）使用Qwen2.5-VL-3B-instruct作为多模态编码器，负责捕获空间先验。System 1（快执行）采用扩散策略作为动作专家，基于DINOv2视觉编码器和轻量级状态编码器构建，是一个紧凑的视觉-动作模型。模型总参数量约41亿。</p>
<p><strong>连接机制</strong>：为了连接VLM规划器和动作专家，模型采用了一个轻量级查询转换器，它以VLM规划器产生的潜在规划嵌入为条件。该转换器将可变长度的输入令牌映射为一组固定的可学习查询令牌，以稳定专家的学习和推理。为了显式激活在空间基础预训练中学到的空间感知能力，采用了<strong>空间提示</strong>技术。例如，在通用物体操作任务中，会在任务指令后附加如“弄清楚如何执行它，然后定位所需的关键物体”这样的提示。提取的特征嵌入为规划器提供了显式的空间线索。</p>
<p><strong>训练策略</strong>：</p>
<ol>
<li><strong>空间基础预训练（阶段1）</strong>：此阶段仅优化VLM，目标是获得更强的空间推理和规划能力。训练数据结合了互联网规模的多模态语料库和机器人专用数据集（如RefCOCO、RoboRefIt等），所有数据被重新格式化为统一的问答式结构，涵盖边界框检测、轨迹预测、功能认知和思维链推理。</li>
<li><strong>空间引导动作后训练（阶段2）</strong>：此阶段在演示数据上联合优化VLM和动作专家。采用两种策略：<ul>
<li><strong>空间提示</strong>：在预测动作前，向任务指令添加空间线索，以引发关于物体关系和任务约束的结构化推理。</li>
<li><strong>与空间基础数据协同训练</strong>：训练在机器人轨迹数据和基础数据之间交替进行。对于轨迹数据，使用预测噪声与真实噪声之间的L2损失来优化VLM主干和动作专家；对于空间基础数据，仅通过下一个令牌预测来更新VLM主干。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，InternVLA-M1的核心创新在于其<strong>空间引导的训练配方</strong>。它明确地将空间基础作为连接语言理解和动作执行的关键中间表示，并通过两阶段训练和双监督机制（同时处理多模态监督和动作监督批次）来实现，确保了感知与控制共同适应而非孤立学习。此外，在查询转换器中引入了梯度衰减因子，以减轻从动作专家传回VLM的梯度，从而在实现有效联合优化的同时，保护规划器的语义推理能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在多个模拟基准和真实世界设置中进行全面评估。</p>
<p><strong>使用的Benchmark/数据集</strong>：</p>
<ul>
<li><strong>模拟基准</strong>：<ul>
<li><strong>SimplerEnv</strong>：用于探究对视觉外观变化的鲁棒性，包含WidowX和Google Robot平台，任务涉及视觉匹配和视觉聚合。</li>
<li><strong>LIBERO</strong>：基于Franka机械臂的语言条件操作套件，包含空间、物体、目标和长视野任务集。</li>
</ul>
</li>
<li><strong>内部构建的仿真环境</strong>：用于可泛化的拾放任务（200个桌面场景）。</li>
<li><strong>真实机器人测试</strong>：长视野操作任务。</li>
</ul>
<p><strong>对比的Baseline方法</strong>：包括RT-1、RT-1-X、RT-2-X、OpenVLA、CogACT、SpatialVLA、π0、GR00T、Magma等先进的开放VLA系统，以及一个基于Qwen2.5-VL-3B-instruct和DiT动作头的Vanilla VLA变体。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>SimplerEnv基准</strong>：<ul>
<li>在Google Robot视觉匹配任务上，InternVLA-M1取得了80.7%的平均成功率，比Vanilla VLA变体提高了14.6%，比之前的最佳结果（CogACT的74.8%）提高了5.9%。</li>
<li>在WidowX视觉匹配任务上，取得了71.7%的平均成功率，比Vanilla VLA变体提高了17.0%，比之前的最佳结果（GR00T的61.9%）提高了9.8%。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.13778v1/x6.png" alt="SimplerEnv (Google Robot) 结果表"></p>
<blockquote>
<p><strong>图6</strong>：SimplerEnv (Google Robot) 基准上的结果对比。InternVLA-M1在多个任务上取得最佳性能，平均成功率提升显著。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.13778v1/x7.png" alt="SimplerEnv (WidowX) 结果表"></p>
<blockquote>
<p><strong>图7</strong>：SimplerEnv (WidowX) 基准上的结果对比。InternVLA-M1在跨机器人泛化任务上表现出强大竞争力。</p>
</blockquote>
<ol start="2">
<li><strong>消融实验与分析</strong>：<br><img src="https://arxiv.org/html/2510.13778v1/x5.png" alt="消融研究图"></li>
</ol>
<blockquote>
<p><strong>图5</strong>：辅助空间提示对协同训练机器人操作与空间基础的效果消融研究。(a) 空间基础性能（RefCOCO-g上的<a href="mailto:&#73;&#x6f;&#x55;&#64;&#48;&#46;&#x35;">&#73;&#x6f;&#x55;&#64;&#48;&#46;&#x35;</a>）；(b) 操作性能（SimplerEnv-WidowX成功率）；(c) 空间基础目标和操作目标的梯度相似性（PSS）。结果表明，空间引导训练能加速收敛，大幅提高操作成功率并增强空间基础准确性，且优化一致性更好（PSS从0.25提升至0.42）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.13778v1/x8.png" alt="综合能力评估表"></p>
<blockquote>
<p><strong>图8</strong>：VLA训练策略在多种任务上的消融研究。InternVLA-M1在取得优越机器人操作性能的同时，保持了比Vanilla VLA和普通协同训练更强的多模态理解和空间基础能力。</p>
</blockquote>
<ol start="3">
<li><strong>可泛化拾放与真实世界测试</strong>：<ul>
<li>在200个桌面场景的可泛化拾放任务中，经过少量演示微调，模型对未见过的物体和指令表现出强大的泛化能力，平均比先前工作提高了6.2%。</li>
<li>在真实世界密集拾放任务中，对未见过的物体和新配置取得了+20.6%的成功率提升。</li>
<li>在长视野推理密集型场景中，在存在干扰（如物理干涉、任务重新规划）的情况下，性能优于GR00T和π0等基线超过10%。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.13778v1/x11.png" alt="真实世界长视野任务结果"></p>
<blockquote>
<p><strong>图11</strong>：真实世界长视野操作任务的定性结果。展示了InternVLA-M1在复杂、多步骤任务中的执行能力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：空间引导的训练策略（包括空间基础预训练、空间提示和与基础数据的协同训练）是性能提升的关键。它有效提升了空间推理能力，并将该能力迁移到运动控制中，同时保持了模型的多模态理解能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>InternVLA-M1框架</strong>，其核心是<strong>空间引导的视觉-语言-动作训练</strong>，明确将空间基础作为连接指令理解和机器人动作执行的关键桥梁。</li>
<li>设计了一种<strong>两阶段训练流程</strong>（空间基础预训练 + 空间引导动作后训练）和<strong>双监督机制</strong>，有效地将通用空间先验知识与具体形态控制策略相结合。</li>
<li>构建了大规模、多样化的<strong>多模态训练数据集</strong>（超过300万样本，其中超过230万专注于空间推理）以及一个<strong>可扩展的合成数据引擎</strong>，用于生成高质量的指令跟随数据。</li>
</ol>
<p><strong>局限性</strong>：论文提到，由于采用大型VLM作为规划器，InternVLA-M1的计算成本高于一些更紧凑的VLA模型。此外，虽然框架设计是通用的，但当前实验主要集中于桌面机械臂操作任务，在其他机器人形态（如移动机器人、人形机器人）上的泛化能力有待进一步验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>空间作为中间表示的潜力</strong>：本工作证明了将“空间”作为连接高层语义规划和低层控制的有效中间抽象，这一思路可推广到更广泛的具身智能任务中。</li>
<li><strong>数据构建与协同训练的重要性</strong>：大规模、高质量、涵盖空间基础与动作执行的数据集，以及有效的协同训练策略，对于开发通用且鲁棒的机器人策略至关重要。</li>
<li><strong>双系统架构的扩展</strong>：慢推理（System 2）与快执行（System 1）的双系统设计，为平衡复杂任务规划和实时控制需求提供了一个有前景的范式，未来可探索更高效的架构实现。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出InternVLA-M1框架，旨在解决通用机器人策略中指令理解与具体动作执行之间的空间推理鸿沟。其核心技术为“空间引导的视觉-语言-动作训练”，采用两阶段流程：先通过230万空间推理数据进行空间基础预训练（确定“在哪里行动”），再通过即插即用的空间提示进行动作后训练（决定“如何行动”）。实验表明，该方法在多个机器人平台（SimperEnv、WidowX、LIBERO）上相比无空间引导的变体性能提升4.3%至17%，在真实世界聚类拾放任务中提升7.3%，对未见物体与新配置的泛化能力提升达20.6%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.13778" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>