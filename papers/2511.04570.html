<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.04570" target="_blank" rel="noreferrer">2511.04570</a></span>
        <span>作者: Tong, Jingqi, Mou, Yurong, Li, Hangcheng, Li, Mingzhe, Yang, Yongzhuo, Zhang, Ming, Chen, Qiguang, Liang, Tianyi, Hu, Xiaomeng, Zheng, Yining, Chen, Xinchi, Zhao, Jun, Huang, Xuanjing, Qiu, Xipeng</span>
        <span>日期: 2025/11/06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型语言模型（LLMs）和视觉语言模型（VLMs）的推理能力主要通过“用文本思考”和“用图像思考”范式来提升。然而，这两种范式存在固有的局限性：1) 图像只能捕捉单一瞬间，无法表示动态过程或连续变化；2) 文本和视觉被视为分离的模态，阻碍了统一的多模态理解与生成。本文旨在克服这些限制，提出了“用视频思考”的新范式，其核心思路是利用Sora-2等视频生成模型，在一个统一的时间框架内，通过在推理链中生成视频，来桥接视觉与文本推理，以实现动态推理和多模态融合。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的核心是构建并利用Video Thinking Benchmark（VideoThinkBench）来系统评估视频生成模型作为推理器的能力。该基准包含两大类任务，其整体框架和任务示例如下图所示。</p>
<p><img src="https://arxiv.org/html/2511.04570v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：VideoThinkBench中的任务类型及Sora-2的解决方案。左侧为视觉中心任务，通过绘图和想象解决；右侧为文本中心任务，通过基于文本的推理解决。</p>
</blockquote>
<p><strong>整体框架与任务构成</strong>：<br>VideoThinkBench旨在全面评估模型的推理能力，分为两类：</p>
<ol>
<li><strong>视觉中心任务</strong>：主要通过绘图和想象对视觉元素进行推理来解决。包括四类：眼力测试游戏、视觉谜题、ARC-AGI-2和迷宫。这些任务评估模型的空间推理和归纳推理能力，其样本大多可通过程序批量生成，且除视觉谜题外，视频生成结果可验证。</li>
<li><strong>文本中心任务</strong>：主要通过基于文本的推理过程来解决。主要从现有基准（如MATH、MMLU、MMMU、MathVista等）中采样子集并适配而成，涵盖纯文本推理和多模态推理，以评估数学和常识推理能力。</li>
</ol>
<p><strong>核心评估方法与技术细节</strong>：<br>评估的关键在于如何从Sora-2生成的视频和音频中提取答案，并与最先进的VLMs（Gemini-2.5-Pro, GPT5-High, Claude-Sonnet-4.5）进行对比。</p>
<ul>
<li><strong>对视觉中心任务的评估</strong>：<ul>
<li><strong>眼力测试游戏</strong>：设计了21类几何构造任务（如找中点、画角平分线等），每类50个样本。对Sora-2采用三种评估方法：音频评估（转录语音）、最后一帧评估（识别绘制的红点）、主帧评估（多帧多数投票）。VLMs则直接从文本响应中提取选项。</li>
<li><strong>视觉谜题</strong>：从PuzzleVQA适配了10类颜色填充和形状绘制任务。为Sora-2定义了偏差值（Diff）来量化生成帧与答案图像的差异，并选择Diff最小的“最佳帧”进行人工评估。VLMs则在部分任务中提供多项选择进行基于规则的评估。</li>
<li><strong>ARC-AGI-2</strong>：要求模型从给定的输入-输出网格对中学习抽象变换规则，并应用于新输入。自动评估提取最后一帧并匹配颜色网格；此外还进行了手动分类分析（完全正确、基本正确、部分正确、错误）。</li>
</ul>
</li>
<li><strong>对文本中心任务的评估</strong>：<ul>
<li><strong>输入形式</strong>：结合文本提示和参考图像。提示包含完整问题文本，参考图像将问题（对于多模态问题则嵌入原图）以印刷体形式显示在白底上。</li>
<li><strong>输出与评估</strong>：要求Sora-2在视频中显示解题过程和答案，并在音频中只念出最终答案。评估时，<strong>视频评估</strong>使用最后一帧图像，<strong>音频评估</strong>使用转录文本。采用GPT-4o作为评判模型（LLM-as-a-Judge），并报告视频准确率、音频准确率、两者皆对（V ∩ A）和至少一对（V ∪ A）的准确率。</li>
</ul>
</li>
</ul>
<p><strong>创新点</strong>：<br>本文的创新性主要体现在视角和方法上：首次系统性地将视频生成模型定位并评估为一种多模态推理器，而不仅仅是内容生成工具；提出了“用视频思考”的范式，并为此构建了一个包含可验证视觉任务和适配文本任务的综合性基准VideoThinkBench；在评估方法上，针对视频模态的特点设计了多帧投票、最佳帧选择、音视频分离评估等具体技术。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用VideoThinkBench作为评估平台。对比的Baseline是三个SOTA VLM：Gemini-2.5-Pro, GPT5-High, Claude-Sonnet-4.5。Sora-2使用其官方API。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>总体表现</strong>：如表1所示，在视觉中心任务上，Sora-2平均准确率为41.7%，与VLMs（42.5%-46.8%）总体相当；在文本中心任务上，Sora-2平均准确率为55.6%，显著低于VLMs（81.1%-83.2%），但其音频准确率表现出潜力。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.04570v1/x2.png" alt="总体结果表"></p>
<blockquote>
<p><strong>表1</strong>：所有二级任务上的准确率（%）总结。Sora-2在视觉中心任务上与VLMs相当，在文本中心任务上存在差距。</p>
</blockquote>
<ol start="2">
<li><strong>视觉中心任务细节</strong>：<ul>
<li><strong>眼力测试游戏</strong>：如表2所示，Sora-2的<strong>主帧评估</strong>方法取得了最高的40.2%平均准确率，超过了所有对比的VLM。在特定任务如“射线相交”上达到88%的准确率，展示了强大的空间几何推理能力。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.04570v1/x3.png" alt="眼力测试结果"></p>
<blockquote>
<p><strong>表2</strong>：眼力测试任务上Sora-2（三种评估方法）与VLMs的准确率。主帧评估效果最佳，且Sora-2整体优于VLMs。</p>
</blockquote>
<pre><code>- **视觉谜题**：如表3所示，Sora-2在形状绘制任务上平均准确率64.9%，与Claude Sonnet 4.5（68.6%）接近，展示了其识别和应用颜色、形状、大小等模式的一定归纳推理能力。
</code></pre>
<p><img src="https://arxiv.org/html/2511.04570v1/x4.png" alt="视觉谜题示例"></p>
<blockquote>
<p><strong>图3</strong>：视觉谜题任务概览。Sora-2能够解决涉及对称、渐变、组合等模式的谜题。</p>
</blockquote>
<pre><code>- **ARC-AGI-2**：Sora-2准确率仅为1.3%（表4），与VLMs水平相当。手动分析（图4）发现，虽然仅有3%完全正确，但有14%基本正确，表明模型有时能识别核心变换规则但执行有误，同时存在大量“未做任何修改”的情况，显示出对指令理解的困难。
</code></pre>
<p><img src="https://arxiv.org/html/2511.04570v1/x5.png" alt="ARC-AGI-2示例"></p>
<blockquote>
<p><strong>图4</strong>：Sora-2解决ARC-AGI-2的示例。展示了从错误到自我修正的过程，也揭示了其作为小样本学习者的潜力。</p>
</blockquote>
<ol start="3">
<li><strong>文本中心任务细节</strong>：<ul>
<li>如表5所示，Sora-2在文本中心任务上表现出一个显著特点：<strong>音频准确率普遍远高于视频（最后一帧）准确率</strong>。例如在GSM8K上，音频准确率达98.9%，而视频仅为75.7%；在MATH-500上，音频92.0%，视频67.0%。这表明Sora-2在生成准确书面内容上存在困难。</li>
<li>在音频模态上，Sora-2在部分数据集（如GSM8K、MATH-500、MathVista、MMBench）上达到了与SOTA VLMs相当的性能，但在更难的AIME、GPQA、MMMU上仍有明显差距。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.04570v1/img/text-centric_main.png" alt="文本中心任务评估流程"></p>
<blockquote>
<p><strong>图5</strong>：文本中心任务的输入形式和评估方法。模型接收文本提示和参考图像，在视频中生成文本解答并语音输出答案，评估时对视频帧和音频分别进行。</p>
</blockquote>
<p><strong>消融分析与能力溯源</strong>：</p>
<ul>
<li><strong>评估方法的影响（消融）</strong>：在眼力测试中，主帧评估（40.2%）优于最后一帧评估（33.4%）和音频评估（28.0%），证明利用视频的时序一致性（多帧信息）能提升评估鲁棒性和性能。</li>
<li><strong>上下文学习（ICL）能力</strong>：在ARC-AGI-2任务中，给予所有示例比仅给一个示例性能更好，表明Sora-2具备小样本学习能力。</li>
<li><strong>自洽性提升</strong>：在可验证的视频生成推理任务中，通过多次采样并选取一致答案（自洽性）可以提升Sora-2的性能。</li>
<li><strong>能力来源分析</strong>：论文通过实验推测，Sora-2在文本中心任务上的推理能力可能部分源于其内部的提示词重写模型，而非纯粹的视觉-文本统一理解。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了“用视频思考”作为一种有前景的统一多模态推理新范式，从理论上指出了视频生成模型在超越静态图像、融合时空与多模态信息方面的潜力。</li>
<li>构建了VideoThinkBench基准，系统性地评估了视频生成模型（以Sora-2为代表）在视觉和文本推理任务上的能力，为未来研究提供了工具和基线。</li>
<li>通过大量实验揭示了Sora-2作为推理器的独特性质：在视觉空间推理任务上可比甚至超越SOTA VLMs；在文本推理上音频输出准确率显著高于视频文本生成；具备初步的小样本学习和自洽性优化能力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文明确指出Sora-2存在以下局限：1) 在文本中心任务中生成完全正确的书面解题过程困难（视频文本准确率低）；2) 对复杂指令的理解可能存在偏差（如在ARC-AGI-2中常不修改目标区域）；3) 在抽象归纳推理（ARC-AGI-2）和部分视觉模式识别任务上表现仍然不佳。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>范式验证与拓展</strong>：“用视频思考”范式值得进一步探索，未来研究可设计更复杂、更需要动态模拟和跨模态对齐的推理任务来验证和推动该范式。</li>
<li><strong>模型改进方向</strong>：当前视频生成模型在文本生成准确性、复杂指令遵循和深层推理方面仍需加强。如何将更强的逻辑推理模块与视频生成能力结合是一个关键方向。</li>
<li><strong>评估方法学</strong>：本文发展的针对视频推理的评估方法（如多帧聚合、音视频分离评估、基于偏差的“最佳帧”选择）为未来评估类似模型提供了参考。</li>
<li><strong>统一模型前景</strong>：研究表明，视频生成模型有潜力发展为统一的理解与生成基础模型。后续工作可探索如何更好地利用其内部的多模态表示，或将其与其他专精模块结合，以构建更强大的通用人工智能系统。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有“思维文本”与“思维图像”范式难以表征动态过程、且文本与视觉模态分离的问题，提出“思维视频”新范式，利用Sora-2等视频生成模型在统一时序框架中进行多模态推理。关键技术包括构建VideoThinkBench基准，涵盖视觉中心与文本中心两类任务。实验表明，Sora-2在视觉任务上与先进VLM相当，在Eyeballing Games等任务中表现更优；在文本任务中，MATH准确率达92%，MMMU达75.53%。研究证实视频生成模型具备统一多模态理解与生成的潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.04570" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>