<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Balanced Behavior Cloning from Imbalanced Datasets - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Towards Balanced Behavior Cloning from Imbalanced Datasets</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.06319" target="_blank" rel="noreferrer">2508.06319</a></span>
        <span>作者: Dylan P. Losey Team</span>
        <span>日期: 2025-08-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习（Imitation Learning）旨在通过人类演示数据学习复杂技能。然而，实践中人类提供的演示数据集往往是<strong>不平衡的</strong>：某些子任务或行为被演示得比其他行为频繁得多。当前主流的模仿学习方法（如标准行为克隆）默认平等对待数据集中的每一个状态-动作对。这导致一个关键局限性：当数据集不平衡时，学习算法会过度强调高频出现的行为，而忽视低频但可能同等重要（甚至更重要，如安全关键）的行为。例如，在教机器人整理桌面的任务中，如果“打开抽屉”的演示远多于“移动滑块”，标准方法学得的策略将擅长开抽屉但可能不会移动滑块。</p>
<p>本文针对模仿学习中数据内在不平衡这一具体痛点，提出了从<strong>数据重要性重新加权</strong>的新视角来获得更平衡的策略。其核心思路是：首先理论证明标准方法在数据不平衡下会导致策略参数偏向高频行为；进而探索无需人类监督、仅基于训练过程自动重新平衡离线数据集的算法，并分析其优劣；最终提出一种新颖的元梯度平衡算法以克服现有方法的局限性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程是：首先将人类演示数据集形式化为多个子策略（行为）的混合，并理论分析标准行为克隆（BC）目标如何导致策略参数偏向高频行为。基于此分析，探索不同的数据集重新加权策略，并最终提出一个元梯度算法来迭代优化每个行为的权重，旨在使策略平等地学好每个行为。</p>
<p>核心模块包括<strong>理论分析模块</strong>和<strong>元梯度平衡模块</strong>。在理论分析中，作者将人类策略 π_h(a|s) 形式化为 k 个在不相交状态子空间 𝒮<em>i 上定义的子策略 π_i 的集合。标准BC损失（公式2）最小化 π_h 与机器人策略 π_θ 之间的KL散度。通过数学推导（公式3-4），该损失可重写为各子策略KL散度期望的加权和，权重 ρ_i 即该子策略数据在数据集中的比例 ρ_i。作者进一步在<strong>命题1</strong>中证明，对于高斯策略模型，标准BC学得的最优策略参数 θ 是各子策略参数 θ_i 的加权和：θ = Σ</em>{i=1}^k ρ_i · θ_i。这清晰地表明，学到的策略会偏向数据比例高的子策略。对于更一般的神经网络策略，推导出每个子策略的期望损失上界为 ℒ_BC / ρ_i（公式7），意味着数据比例越低，其可能达到的损失上界越高，学习越困难。</p>
<p>基于理论分析，作者探讨了无需外部目标信息、仅通过监控训练损失来重新平衡数据集的策略。主要对比了三种思路：1) <strong>过采样（Oversampling）</strong>：等价于在损失中增加低频数据的权重；2) <strong>基于训练损失的加权</strong>：根据当前策略在各行为数据子集上的损失来分配权重，损失高的行为获得更高权重；3) <strong>本文提出的元梯度平衡（Meta-Gradient Balancing）</strong>。</p>
<p><img src="https://arxiv.org/html/2508.06319v2/x2.png" alt="元梯度平衡方法框架"></p>
<blockquote>
<p><strong>图2</strong>：元梯度平衡方法概览。算法维护两组参数：策略参数 θ 和数据集权重 w。在每次迭代中，它首先用当前权重 w 更新策略 θ（内层更新），然后基于更新后的策略在验证集上的表现，通过元梯度更新权重 w（外层更新），旨在最大化所有行为上的整体性能。</p>
</blockquote>
<p>元梯度算法的创新点在于将数据权重 w 也作为可优化的元参数。其优化目标是最小化所有行为上的<strong>归一化损失</strong>之和：min_w Σ_{i=1}^k (ℒ_i(θ’) / ℒ_i^*)。其中，ℒ_i(θ’) 是使用权重 w 更新后的策略 θ’ 在行为 i 的验证数据上的损失，ℒ_i^* 是该行为可能达到的最佳损失估计（通过单独训练一个仅在该行为数据上过拟合的“专家”策略得到）。算法迭代执行两步：1) <strong>内层更新</strong>：用当前权重 w 计算梯度，更新策略参数 θ 一步得到 θ’；2) <strong>外层更新</strong>：计算 θ’ 在各行为验证集上的损失 ℒ_i(θ’)，然后通过梯度下降更新 w 以最小化上述归一化损失之和。这样，权重 w 的调整方向是使策略在所有行为上都尽可能接近其各自的最佳性能，而不是简单地让所有行为损失相等，从而避免了因行为学习难度不同而导致的过拟合或欠拟合问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在一个模拟的<strong>7自由度机械臂操作任务</strong>中进行，任务要求机器人将物体放入位于左侧或右侧的目标箱。这天然定义了两种行为（“左转”和“右转”）。通过控制数据集中左右转向演示的比例（如98%-2%, 90%-10%, 70%-30%）来创建不平衡数据集。</p>
<p>对比的基线方法包括：1) <strong>标准行为克隆（BC）</strong>；2) <strong>过采样（Oversampling）</strong>；3) <strong>基于训练损失的加权（Loss-based Weighting）</strong>，即根据当前策略在各行为训练集上的损失比例分配权重；4) <strong>本文的元梯度平衡（Meta-Gradient）</strong>。</p>
<p><img src="https://arxiv.org/html/2508.06319v2/x3.png" alt="标准BC在数据不平衡下的失败案例"></p>
<blockquote>
<p><strong>图3</strong>：标准BC（98%-2%数据比例）学得策略的执行轨迹。机器人学会了高频的“左转”行为（绿色轨迹），但完全未能学会低频的“右转”行为（红色轨迹），验证了理论分析。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.06319v2/x4.png" alt="不同平衡策略在严重不平衡数据(98%-2%)下的性能"></p>
<blockquote>
<p><strong>图4</strong>：在98%-2%的极端不平衡设置下，各方法在低频（右转）行为上的成功率。元梯度方法取得了接近80%的成功率，显著优于过采样（约20%）和基于损失的加权（约40%），标准BC接近0%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.06319v2/x5.png" alt="不同数据不平衡程度下各方法的性能"></p>
<blockquote>
<p><strong>图5</strong>：在不同数据比例下，各方法在低频行为上的成功率。元梯度方法在所有不平衡程度上都表现最佳，尤其是在数据极度不平衡时（98%-2%），优势最明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.06319v2/x6.png" alt="元梯度方法学习到的权重演变"></p>
<blockquote>
<p><strong>图6</strong>：元梯度方法在训练过程中为两个行为学习的权重 w 的变化。初期权重波动较大，随后收敛到一个稳定分布，该分布并非简单的1:1平衡，而是反映了算法对最优平衡点的自动寻找。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.06319v2/x7.png" alt="不同方法在三个行为任务上的性能对比"></p>
<blockquote>
<p><strong>图7</strong>：在包含三个行为（左转、右转、直行）的任务中，元梯度方法在三个行为上的成功率最为均衡且总体最高，展示了其处理多行为不平衡的能力。</p>
</blockquote>
<p>消融实验验证了元梯度算法中关键组件的贡献：</p>
<ul>
<li><strong>使用验证集而非训练集计算元梯度</strong>：图8显示，使用训练损失会导致权重迅速偏向单一行为并过拟合，性能大幅下降；而使用验证损失能稳定优化，获得更好性能。</li>
<li><strong>损失归一化（除以ℒ_i^*）</strong>：图9表明，不使用归一化（即直接最小化 ℒ_i(θ’) 之和）会导致算法倾向于优化容易降低损失的行为，而对难以学习的行为关注不足，导致性能不平衡；引入归一化后，所有行为都能被更平等地优化。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.06319v2/x8.png" alt="使用训练集vs验证集计算元梯度的消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融实验：在元梯度更新中使用训练损失会导致性能严重下降（蓝色线），验证了使用独立验证集计算元梯度的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.06319v2/x9.png" alt="损失归一化消融实验"></p>
<blockquote>
<p><strong>图9</strong>：消融实验：在元目标中不使用损失归一化（即不除以ℒ_i^*）会导致算法无法平等优化所有行为，低频行为性能差；使用归一化后性能更均衡。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) <strong>理论贡献</strong>：首次形式化分析了模仿学习中数据不平衡问题，并证明标准BC会导致策略参数偏向高频行为，为理解该问题提供了理论基础。2) <strong>系统性见解</strong>：对比分析了不同自动数据集平衡策略的优劣，指出其权衡取决于行为的学习难度和期望的准确度。3) <strong>新方法</strong>：提出了一种新颖的元梯度平衡算法，该算法无需外部奖励或额外标注，通过迭代优化数据权重使策略在所有行为上逼近其最佳可能性能，在实验中显著提升了不平衡数据下的策略均衡性与性能。</p>
<p>论文自身提到的局限性在于：方法假设不同行为对应的状态子空间是<strong>可区分的</strong>（disjoint），并且需要为每个行为保留一个小的验证集来计算元梯度。此外，元梯度算法涉及二阶导数计算，会带来额外的计算开销。</p>
<p>本文工作对后续研究的启示在于：为从大规模、自然收集的（必然不平衡的）机器人数据集中学习鲁棒、全面的策略提供了新思路。未来的工作可以探索将此类平衡方法扩展到在线学习或终身学习场景，或者与表征学习结合以自动发现数据中的潜在行为结构。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究模仿学习从不平衡数据集中学习时，因数据分布不均导致策略偏向高频子任务、忽视重要但低频行为的问题。为解决此问题，论文探索了对离线数据集进行重新平衡（即重新加权不同状态-动作对重要性）的方法，并引入了一种新的元梯度重新平衡算法以改进现有方法。实验表明，对数据集进行重新平衡能够提升下游模仿学习的整体策略性能，且无需额外收集数据。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.06319" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>