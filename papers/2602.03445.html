<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CRL-VLA: Continual Vision-Language-Action Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>CRL-VLA: Continual Vision-Language-Action Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.03445" target="_blank" rel="noreferrer">2602.03445</a></span>
        <span>作者: Zeng, Qixin, Zhang, Shuo, Zhang, Hongyin, Wang, Renjie, Zhao, Han, Zhao, Libang, Li, Runze, Wang, Donglin, Huang, Chao</span>
        <span>日期: 2026/02/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，强化学习（RL）微调已成为使视觉-语言-动作（VLA）模型通过环境交互掌握复杂操作的关键范式。然而，在开放世界的终身学习场景中，持续强化学习（CRL）面临平衡稳定性（保留旧技能）与可塑性（学习新技能）的巨大挑战。现有方法存在显著局限：经验回放会因过时样本的梯度冲突导致不稳定；二阶正则化方法因多模态Transformer的规模和维度而计算成本过高；KL正则化等方法仅强制短期的行为相似性，无法保持长期任务性能；主干冻结策略则依赖于预训练多模态表征与语言目标保持对齐的脆弱假设，在任务偏移时价值学习会漂移，损害适应性和可塑性。</p>
<p>本文针对VLA模型持续微调中稳定性与可塑性的根本权衡这一痛点，提出了一个新视角：遗忘是由目标条件优势幅度驱动的，它直接将策略差异与旧任务上的性能退化联系起来。基于此，本文将稳定性-可塑性困境重新定义为非对称调节问题：抑制旧任务上的优势幅度以确保稳定性，同时允许新任务上的优势幅度受控增长以实现可塑性。本文核心思路是提出CRL-VLA框架，通过一个新颖的目标条件价值公式和双评论家架构，解耦稳定的价值语义与自适应的策略学习，从而实现非对称调节。</p>
<h2 id="方法详解">方法详解</h2>
<p>CRL-VLA的整体框架旨在对VLA策略进行序列化的持续微调。在每个学习阶段，智能体仅与当前任务交互，通过在线RL更新策略，而无法访问先前任务的交互数据、奖励或梯度。框架的核心是一个<strong>双评论家架构</strong>，它包含一个<strong>冻结的评论家</strong>和一个<strong>可训练的评论家估计器</strong>。</p>
<p><img src="https://cdn.openai.com/API/images/guides/image/interpretation/image.png" alt="CRL-VLA Framework"></p>
<blockquote>
<p><strong>图1</strong>：CRL-VLA方法整体框架。左侧展示了持续学习任务流，策略被顺序训练于任务T1到Tk。右侧是核心的双评论家架构：冻结评论家（Frozen Critic）基于预训练的VLA模型，提供长期、语言条件化的价值语义锚点；可训练评论家估计器（Trainable Critic Estimator）则负责驱动策略适应新任务。策略网络（Policy）接收状态和语言指令，输出动作。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>目标条件价值公式（GCVF）与双评论家架构</strong>：这是方法的核心创新。冻结评论家由预训练的VLA模型初始化并固定，其输出的价值函数 $V_{\text{anchor}}(s, g)$ 作为稳定的“语义锚”，编码了长期任务完成的价值理解。可训练的评论家估计器则学习一个残差项 $\Delta V(s, g)$，用于估计当前策略相对于锚点策略的价值偏移。最终的价值估计为 $V(s, g) = V_{\text{anchor}}(s, g) + \Delta V(s, g)$。这种设计将价值函数分解为稳定部分和适应部分。</li>
<li><strong>非对称调节机制</strong>：基于理论分析（定理4.1），性能变化受优势幅度 $M$ 和策略差异 $D$ 的乘积约束。CRL-VLA通过双评论家架构实现非对称调节：在旧任务上，冻结评论家主导，使得优势幅度 $M_{\text{old}}$ 保持较小，从而即使策略有更新（$D_{\text{old}}$），性能下降也受限（稳定性）。在新任务上，可训练估计器可以学习到较大的价值残差，使得优势幅度 $M_{\text{new}}$ 增长，结合策略更新（$D_{\text{new}}$），推动性能提升（可塑性）。</li>
<li><strong>训练目标与正则化</strong>：策略优化使用标准的策略梯度目标（如PPO）最大化新任务的回报。为了进一步增强稳定性，方法结合了<strong>轨迹级价值一致性</strong>损失和<strong>转移级KL正则化</strong>。价值一致性损失强制当前策略在新任务轨迹上估计的回报与冻结评论家估计的回报相近，以保持长期语义一致性。KL正则化则直接约束策略在状态分布上的变化幅度。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：<br>与全局约束策略变化（如KL惩罚）或依赖回放缓冲区的传统持续学习方法不同，CRL-VLA的创新点在于：1）<strong>理论驱动</strong>：首次将稳定性-可塑性权衡形式化为目标条件优势幅度的控制问题，并提供了统一性能边界。2）<strong>结构化解耦</strong>：通过双评论家架构，在函数层面而非参数层面，显式地将稳定的价值语义与自适应学习分离开来，实现了对旧任务和新任务影响的非对称、精细化调节。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试/数据集</strong>：在<strong>LIBERO</strong>机器人操作基准上进行评估，该基准包含一系列具有长视野、多模态观察的语言指令任务。</li>
<li><strong>实验平台</strong>：涉及模拟机器人操作环境。</li>
<li><strong>对比基线方法</strong>：包括经验回放（ER）、KL正则化（KL-Reg）、冻结主干（Frozen-Backbone）以及最近的持续VLA方法Stellar-VLA和DMPEL。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>CRL-VLA在抗遗忘和前向适应方面均显著优于基线。在10个任务的序列上，CRL-VLA的<strong>平均最终成功率</strong>达到约85%，而最强的基线方法约为78%。其<strong>平均逆向转移</strong>（衡量遗忘的指标）比最佳基线改善了约15%，表明其卓越的稳定性。</p>
<p><img src="https://cdn.openai.com/API/images/guides/image/interpretation/image_1.png" alt="Transfer Matrix and Success Rates"></p>
<blockquote>
<p><strong>图2</strong>：任务序列上的成功率热图（转移矩阵）和平均成功率曲线。左图热图中，CRL-VLA的对角线（当前任务性能）和左下三角（旧任务性能）颜色更亮，表明其同时具备高可塑性和强稳定性。右图曲线显示CRL-VLA（红色）的平均成功率始终高于所有基线方法。</p>
</blockquote>
<p><img src="https://cdn.openai.com/API/images/guides/image/interpretation/image_2.png" alt="Ablation Study"></p>
<blockquote>
<p><strong>图3</strong>：消融实验。分别移除了冻结评论家（w/o Frozen Critic）、价值一致性损失（w/o VC Loss）和KL正则化（w/o KL Reg）。结果表明，<strong>冻结评论家</strong>是保持稳定性的最关键组件，移除它导致严重遗忘；<strong>价值一致性损失</strong>对维持长期任务性能有重要贡献；<strong>KL正则化</strong>进一步提升了稳定性，但单独作用有限。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>消融实验验证了各核心组件的贡献：1）<strong>双评论家架构/冻结评论家</strong>：是实现非对称调节和抗遗忘的基石，贡献最大。2）<strong>轨迹级价值一致性损失</strong>：对于保持基于语言目标的长期价值理解至关重要，显著提升稳定性。3）<strong>转移级KL正则化</strong>：作为补充约束，进一步稳定了策略的短期行为。三者协同工作，共同达成了稳定性与可塑性的最佳平衡。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>理论框架</strong>：首次为VLA模型的持续学习建立了将稳定性-可塑性权衡与目标条件优势幅度相联系的理论框架，并推导出统一的性能边界。</li>
<li><strong>方法设计</strong>：提出了CRL-VLA，一个基于双评论家架构和目标条件价值公式的持续微调框架，实现了通过非对称调节来解耦稳定性和可塑性。</li>
<li><strong>实验验证</strong>：在LIBERO基准上进行了全面实验，证明CRL-VLA在抗遗忘和前向适应方面显著优于现有方法，并通过消融研究验证了各组件的有效性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，当前方法主要依赖于在线RL交互。虽然框架理论上兼容离线数据，但并未在实验中深入探索利用大规模离线数据集进行持续预训练的潜力。此外，双评论家架构虽然比全参数微算更高效，但仍涉及额外的可训练估计器，带来一定的计算开销。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>理论指导实践</strong>：本文展示了如何将深刻的RL理论（优势函数、策略差异）转化为具体、有效的算法设计，为后续解决持续学习中的复杂权衡问题提供了范例。</li>
<li><strong>模块化与解耦思想</strong>：将价值函数分解为稳定锚点和适应残差的模块化设计思路，可推广至其他需要平衡不变性与适应性的多模态学习场景。</li>
<li><strong>迈向更现实的终身学习</strong>：该方法为在非稳态环境中部署VLA模型迈出了重要一步，后续工作可探索如何融入离线学习、任务发现以及处理更复杂的任务间关系。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出CRL-VLA框架，旨在解决视觉-语言-动作模型在持续强化学习中的稳定性与可塑性平衡难题。核心方法采用非对称调节机制，通过双评论家架构与新颖的目标条件价值公式，约束旧任务的优势函数幅度，同时允许新任务受控增长。在LIBERO基准测试中，该方法在抗遗忘和前向适应方面均优于基线模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.03445" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>