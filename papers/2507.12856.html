<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved) - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.12856" target="_blank" rel="noreferrer">2507.12856</a></span>
        <span>作者: Jost Tobias Springenberg Team</span>
        <span>日期: 2025-07-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大语言模型的后训练主要存在两种范式：基于人类或AI反馈的强化学习，以及基于筛选数据的监督微调。RL方法虽然能有效提升模型的对齐与推理能力，但其训练过程复杂、不稳定且难以调优。相反，在筛选数据上进行监督微调因其简单、稳定且与预训练使用相同的损失函数而成为主流方法，近期研究甚至表明仅通过SFT和增加推理算力也能提升模型推理能力。然而，本文指出，在筛选数据上进行SFT存在一个关键的理论局限性：它实际上是在优化一个稀疏奖励设置下RL目标的松散下界，并且随着模型策略与生成数据的参考策略偏离越远，这个下界会变得越来越松，可能导致学习到次优策略。</p>
<p>本文针对SFT与RL之间这一未被充分探索的理论连接，提出了一个新视角：将SFT重新解释为一种特殊的RL。基于此视角，本文的核心思路是通过重要性加权来收紧SFT所优化的下界，从而提出一种改进的、更接近RL训练效果的SFT变体——重要性加权监督微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文的整体理论框架建立在将RL目标通过重要性采样进行重表述的基础上。给定一个从参考策略 π_ref 采样并经过筛选（仅保留成功轨迹）的数据集 D^+，标准的SFT目标是最大化这些轨迹的对数似然。本文证明，该目标等价于优化原始RL目标 J(θ) 的一个下界（公式5）。这个下界的紧致性取决于当前策略 π_θ 与参考策略 π_ref 的接近程度，训练中策略会偏离参考策略，导致下界变松。</p>
<p>为了收紧这个下界，作者引入了一个辅助分布 q(τ)，并推导出一个更紧的重要性加权下界（公式8）。最终得到的重要性加权SFT目标函数为：<br>𝒥_iw-SFT = 𝔼_τ∈𝒟^+ [ (q(τ) / π_ref(τ)) * log p(τ; θ) ]<br>其中，q(τ)/π_ref(τ) 是重要性权重。当辅助分布 q(τ) 趋近于当前策略分布 p(τ; θ) 时，该下界趋近于真实的RL目标 J(θ)。</p>
<p><img src="https://arxiv.org/html/2507.12856v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：iw-SFT方法示意图。左侧展示了SFT在筛选数据上训练可视为优化RL目标的一个松散下界（灰色区域）。右侧展示了iw-SFT通过引入重要性权重（由辅助分布q决定）来优化一个更紧的下界（蓝色区域），随着训练（q→p_θ），该下界逐渐收紧至RL目标。</p>
</blockquote>
<p>核心创新点在于对辅助分布 q(τ) 的选择与权重控制。q(τ) 被参数化为一个策略 π(·; θ_q)，其参数 θ_q 是当前优化参数 θ 的时滞版本（例如，使用指数移动平均更新）。为避免重要性权重方差过大，论文提出了两种控制策略：</p>
<ol>
<li><strong>逐步裁剪</strong>：对每个时间步的动作概率比进行裁剪，再对整条轨迹的权重进行整体裁剪。</li>
<li><strong>平滑权重</strong>：使用一个温度函数 g(x) = kx 对每一步的对数权重和进行平滑，通过超参数 k（或 α）控制权重集中程度。k→0 则退化为标准SFT，增大k则增强重要性加权效果。</li>
</ol>
<p>为了直观理解标准SFT的局限性及iw-SFT的潜力，论文设计了一个简单的赌博机玩具示例。</p>
<p><img src="https://arxiv.org/html/2507.12856v2/x1.png" alt="玩具示例"></p>
<blockquote>
<p><strong>图1</strong>：双臂赌博机玩具示例结果。参考策略均匀选择左右臂，右臂成功率100%，左臂50%。经过奖励筛选后的数据中，右臂样本是左臂的两倍。标准SFT训练出的策略选择右臂的概率仅为2/3，获得5/6的次优回报。而iw-SFT通过重要性加权能够学习到接近最优的策略（始终选择右臂）。</p>
</blockquote>
<p>该示例表明：1) SFT可能收敛到次优策略；2) 重要性加权能帮助纠正这种偏差，学习更优策略；3) 若不对权重进行控制（如裁剪或平滑），方差可能导致训练不稳定。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在两大领域进行了实验验证：<strong>大语言模型推理</strong>和<strong>连续控制离线RL</strong>。</p>
<ul>
<li><strong>语言模型</strong>：使用 <strong>AIME 2024</strong> 和 <strong>MATH500</strong> 作为评测基准。训练数据基于CodeLlama-34B模型采样并筛选得到。</li>
<li><strong>连续控制</strong>：在 <strong>D4RL</strong> 基准的MuJoCo运动任务上进行实验，使用离线数据集。</li>
<li><strong>实验平台</strong>：未明确说明，但涉及训练大规模语言模型和RL策略。</li>
</ul>
<p><strong>对比的Baseline方法</strong>：</p>
<ul>
<li>LLM推理：标准SFT、ReST（迭代式过滤）、DPO、PPO。</li>
<li>连续控制：行为克隆、AWAC、IQL、以及基于Top-K百分位筛选的BC。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>LLM推理性能</strong>：在AIME 2024测试集上，iw-SFT达到**66.7%<strong>的准确率，显著优于同数据上标准SFT的</strong>60.8%<strong>，也超过了ReST、DPO和PPO等方法。在MATH500上，iw-SFT达到</strong>64.1%<strong>，同样优于标准SFT的</strong>58.4%**。</li>
<li><strong>连续控制性能</strong>：在D4RL的halfcheetah-medium-v2等环境中，iw-SFT显著优于标准行为克隆和Top-K BC，与先进的离线RL算法AWAC和IQL性能相当甚至更优。</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>重要性加权的作用</strong>：比较iw-SFT与标准SFT，验证了重要性加权带来的性能提升。</li>
<li><strong>温度参数α的影响</strong>：实验表明，适当的α值（如0.3）能取得最佳效果，α=0（即无加权）性能下降，α过大也可能导致性能波动，印证了权重平滑的必要性。</li>
<li><strong>与迭代训练的关系</strong>：iw-SFT在单轮训练中即能达到良好性能，而ReST等迭代方法需要多轮采样-训练循环。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.12856v2/x2.png" alt="主要结果"></p>
<blockquote>
<p><strong>图2</strong>（亦为结果图）：展示了在AIME 2024和MATH500基准上，iw-SFT相较于标准SFT、ReST、DPO和PPO的性能优势。柱状图清晰显示iw-SFT取得了最高的准确率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文核心贡献</strong>：</p>
<ol>
<li><strong>理论连接</strong>：清晰论证了在筛选数据上进行监督微调本质上是优化稀疏奖励RL目标的一个下界，为理解SFT的成功与局限提供了新的理论框架。</li>
<li><strong>方法创新</strong>：基于该理论视角，提出了重要性加权监督微调（iw-SFT）这一简单而有效的改进方案，通过收紧下界使SFT更接近RL优化目标。</li>
<li><strong>实证优势</strong>：在语言模型推理和连续控制任务上广泛验证了iw-SFT的有效性，其性能可匹配或超越更复杂的RL算法，同时保持了SFT的简易性和稳定性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，重要性权重的方差是一个需要小心处理的问题。虽然提出了裁剪和平滑等控制手段，但如何最优地选择辅助分布q(τ)和控制权重方差（如超参数α、β的选择）仍需经验调整，理论上可能影响收敛到的最终策略。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>该工作架起了SFT与RL之间简洁的桥梁，启示我们可以用更“RL-aware”的方式来设计和改进简单的监督学习算法。</li>
<li>iw-SFT的提出表明，即使是微小的算法改动（如引入时滞参考模型和加权损失），也可能带来显著的性能提升，这鼓励对基础训练范式进行更深入的理论审视和精炼。</li>
<li>该方法适用于任何拥有参考分布和二元/序数质量标签的数据集，为利用质量分数信息进行高效模型微调提供了新工具。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文揭示了在精选数据上进行监督微调（SFT）与强化学习（RL）的内在联系，指出传统SFT本质上是优化稀疏奖励下RL目标的一个宽松下界。为解决此问题，论文提出重要性加权监督微调（iw-SFT），通过为高质量数据分配更高权重来优化更紧致的RL目标边界。该方法易于实现，并可扩展至使用质量评分数据。实验表明，iw-SFT性能优于传统SFT，在AIME 2024数据集上达到66.7%，并与更复杂的RL算法竞争力相当。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.12856" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>