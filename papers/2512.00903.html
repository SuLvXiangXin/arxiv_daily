<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.00903" target="_blank" rel="noreferrer">2512.00903</a></span>
        <span>作者: Ni, Chaojun, Chen, Cheng, Wang, Xiaofeng, Zhu, Zheng, Zheng, Wenzhao, Wang, Boyuan, Chen, Tianrun, Zhao, Guosheng, Li, Haoyun, Dong, Zhehao, Zhang, Qiang, Ye, Yun, Wang, Yang, Huang, Guan, Mei, Wenjun</span>
        <span>日期: 2025/11/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于预训练视觉语言模型（VLM）构建的视觉-语言-动作（VLA）模型展现出强大潜力，但其庞大的参数量导致高推理延迟和内存占用，难以在资源受限的机器人平台上实时部署。为缓解此问题，近期研究转向使用轻量级VLM作为主干，但这通常以牺牲模型的时空推理能力为代价。如图1所示，轻量级VLM（如SmolVLM-0.5B）在空间推理任务上表现显著弱于大型VLM（如PaliGemma-3B），导致基于其构建的轻量VLA模型（如SmolVLA）任务成功率较低。现有增强时空感知的方法存在局限：一类方法（如图2b）在大VLM内部直接融合3D与2D特征，依赖重型VLM处理跨模态融合；另一类方法（如图2c）采用解耦设计，引入独立的3D处理分支，显著增加了参数量和推理开销。这些方法均未能在保持轻量设计的同时，有效解决轻量VLM时空感知能力弱的问题。本文针对轻量级VLA模型缺乏有效且高效的时空感知能力这一痛点，提出SwiftVLA。其核心思路是：在训练时引入4D时空特征作为辅助输入，并通过掩码重建策略将4D知识蒸馏到VLA模型中，使得在推理时可以丢弃4D输入分支，从而以最小开销为紧凑模型解锁4D时空动态理解能力。</p>
<p><img src="https://arxiv.org/html/2512.00903v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：现有VLA模型架构与SwiftVLA对比。(a) 仅使用2D特征输入VLM，时空感知有限。(b) 直接融合方法在大VLM内部结合空间和2D特征。(c) 解耦设计引入独立空间分支，导致参数量开销大。(d) SwiftVLA利用预训练模型提取4D特征，并应用特征重建目标来对齐4D和2D表示，同时引入Fusion Tokens和未来预测目标以加强跨模态整合。推理时移除4D输入和辅助头以保持效率。</p>
</blockquote>
<h2 id="方法详解">方法详解</h2>
<p>SwiftVLA的整体流程如图3所示，主要由预训练的轻量级VLM和动作专家（Action Expert）组成。模型接收多视角图像观测、语言指令和本体感知状态。流程分为三步：首先，从原始图像中并行提取2D视觉特征和4D时空特征；其次，这些特征与语言、状态嵌入以及可学习的Fusion Tokens一起输入VLM进行跨模态融合，Fusion Tokens的输出受机器人末端执行器未来轨迹的监督；最后，VLM产生的中间隐藏状态作为条件，输入基于扩散模型的动作专家来预测动作。训练时，采用掩码重建策略，随机掩码2D或4D特征，并让动作专家重建被掩码的特征，以此鼓励模型学习几何和动态感知的表征。</p>
<p><img src="https://arxiv.org/html/2512.00903v1/x3.png" alt="pipeline总览"></p>
<blockquote>
<p><strong>图3</strong>：SwiftVLA的流程框架。首先从输入图像中提取2D和4D特征。轻量级VLM通过Fusion Tokens处理2D和4D特征以实现跨模态整合。Fusion Tokens的输出由机器人末端执行器的未来轨迹监督。训练时，随机掩码2D或4D特征，并要求动作专家在学会生成动作的同时重建被掩码的特征。图中展示了随机掩码4D特征情况下的注意力掩码模式。</p>
</blockquote>
<p><strong>核心模块一：增量式4D特征提取</strong>。如图4所示，采用一个预训练且权重冻结的4D视觉几何变换器，它由编码器、解码器和时间缓存模块组成。该模块以流式方式增量处理多视角图像序列，无需额外传感器。对于每个时间步t和每个视角v，图像先被编码为特征嵌入。然后，解码器按照固定的视角顺序（左、右、前）依次处理这些特征，在时序注意力阶段，当前特征通过与时间缓存进行交叉注意力来整合时序上下文，处理完成后更新缓存。采用先进先出策略维护缓存，仅保留最近的K个4D特征表示。最终，仅将前视图的4D特征提供给VLM，左右视图的4D特征仅用于更新缓存以提供更全面的时空上下文，以此控制训练成本。</p>
<p><img src="https://arxiv.org/html/2512.00903v1/x4.png" alt="4D特征提取"></p>
<blockquote>
<p><strong>图4</strong>：4D特征提取过程。在每一步，我们顺序处理多视角观测，并从缓存中加载上下文信息进行时序注意力。生成的4D特征被更新到缓存中并传递给VLM。</p>
</blockquote>
<p><strong>核心模块二：Fusion Tokens</strong>。为了解决轻量级VLM难以有效融合多模态输入的问题，引入了一组可学习的Fusion Tokens。这些令牌与2D特征、4D特征、语言嵌入和状态嵌入在VLM内部通过交叉注意力进行交互，产生融合表示。该融合表示中对应于Fusion Tokens的部分，被一个轨迹预测头解码，并以机器人末端执行器未来轨迹的真实值进行监督（损失函数为L2损失）。这种显式的未来轨迹监督促使Fusion Tokens将多模态特征与时空语义对齐，使得VLM产生的中间隐藏状态更能服务于动作生成。</p>
<p><strong>核心模块三：掩码重建策略</strong>。这是实现“训练时用4D，推理时弃4D”的关键创新。训练时，以一定概率随机掩码2D或4D特征输入。在注意力机制中，被掩码的特征令牌对VLM不可见。模型需要基于剩余模态预测动作，同时动作专家还需通过两个重建头分别重建被掩码的2D或4D特征（损失函数为L2损失）。动作生成部分采用扩散模型，其损失为预测噪声的L2损失。总损失是动作损失、轨迹预测损失以及2D/4D重建损失的加权和。这种设计迫使模型学习更全面、几何感知的4D表征，而不是依赖单一模态进行动作预测，从而将4D知识蒸馏到网络权重中。推理时，移除4D特征提取器、重建头和轨迹预测头，仅保留2D特征分支、VLM和动作专家，形成一个紧凑架构。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境（具体任务未命名）和真实世界（清理桌子、扔瓶子、叠碗）任务上评估，使用任务成功率（SR）和平均轨迹长度作为主要指标。在LIBERO基准测试（包含LIBERO、Spatial、Object、Goal Long四个任务组）上进行了广泛对比。基线方法包括大型VLA模型（如π0， 3B）、其他轻量VLA模型（如SmolVLA， 0.45B/TinyVLA）以及时空增强型VLA模型（如4D-VLA， 4B）。SwiftVLA有两个推理配置：使用4D输入（约1.65B参数）和不使用4D输入（约0.45B参数）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟实验</strong>：如表1所示，SwiftVLA（无4D输入）取得了最高的平均成功率（53%），显著优于所有轻量基线（SmolVLA为29%， TinyVLA为7%），并与参数量大7倍的大型基线π0（47%）和GO-1（46%）性能相当甚至更优。使用4D输入的SwiftVLA性能略有提升（平均成功率55%）。同时，SwiftVLA完成任务的平均轨迹长度更短，表明其效率更高。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00903v1/x5.png" alt="模拟实验结果"></p>
<blockquote>
<p><strong>图5</strong>：表1：模拟环境中任务成功率和平均轨迹长度的对比。SwiftVLA在成功率和效率上均优于轻量基线，并与大7倍的大型模型性能相当。</p>
</blockquote>
<ol start="2">
<li><strong>真实世界实验</strong>：如表2所示，SwiftVLA（无4D输入）在三个真实任务上的平均成功率高达80%，远超轻量基线SmolVLA（34%）和经过相同预训练配置的SmolVLA†（53%），也大幅超过了大型基线π0（61%）。这验证了其在复杂真实环境中的有效性和鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00903v1/x6.png" alt="真实实验结果"></p>
<blockquote>
<p><strong>图6</strong>：表2：真实世界实验中任务成功率和平均轨迹长度的对比。SwiftVLA在真实机器人任务上取得了显著的优势。</p>
</blockquote>
<ol start="3">
<li><strong>LIBERO基准测试</strong>：如表3所示，参数量仅0.45B的SwiftVLA（无4D输入）取得了94.7%的平均成功率，超越了所有同类轻量VLA模型，并与许多参数量大一个数量级的大型VLA模型（如π0， 3B， 94.1%）和时空增强型VLA模型（如4D-VLA， 4B， 88.6%）性能相当，仅次于个别7B以上参数量的顶尖模型。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00903v1/x7.png" alt="LIBERO结果"></p>
<blockquote>
<p><strong>图7</strong>：表3：LIBERO基准测试上的方法对比。SwiftVLA作为时空增强型小模型，以极小参数量取得了与大型模型媲美的性能。</p>
</blockquote>
<ol start="4">
<li><strong>效率优势</strong>：如图8所示，在NVIDIA Jetson Orin边缘设备上，SwiftVLA（无4D输入）的推理速度比π0快18倍，内存占用减少12倍，同时保持了可比的任务性能。图9的消融实验表明，移除Fusion Tokens或掩码重建目标都会导致性能显著下降，验证了各核心组件的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00903v1/x8.png" alt="边缘设备性能"></p>
<blockquote>
<p><strong>图8</strong>：图1：大型VLM在空间推理上优于小型VLM，这使得基于它的π0成功率更高但推理慢。SwiftVLA在增强小型VLA时空动态的同时，保持了速度优势。成功率和速度在NVIDIA Jetson Orin上测试。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.00903v1/x9.png" alt="消融实验"></p>
<blockquote>
<p><strong>图9</strong>：消融研究。移除Fusion Tokens或掩码重建目标均导致性能下降，证明了各组件的重要性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：Fusion Tokens的未来轨迹监督和掩码重建策略是性能提升的关键。移除任一组件，模型性能都会显著下降。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了SwiftVLA，通过创新的掩码重建训练策略，将4D时空知识蒸馏到轻量级VLA中，实现了在推理时无需4D输入的高性能，在效率与能力间取得了卓越平衡。2) 设计了Fusion Tokens，并利用未来轨迹预测进行监督，有效促进了轻量VLM内的跨模态特征对齐与融合。3) 在模拟与真实实验中验证了其有效性，仅用0.45B参数即可媲美7倍大的模型，并在边缘设备上实现了18倍加速和12倍内存节省。</p>
<p>论文提到的局限性在于，为控制训练成本，仅将前视图的4D特征输入VLM，左右视图特征仅用于更新时间缓存。这可能会限制模型对全局空间结构的利用。</p>
<p>本研究为轻量级具身智能模型的开发提供了重要启示：通过设计巧妙的训练时知识蒸馏机制（如掩码重建），可以将来自重型、多模态感知模型（如4D理解模型）的丰富信息压缩到轻量级决策模型中，从而在不增加推理开销的前提下，极大提升其场景理解与决策能力。这一思路可扩展到其他模态（如音频、触觉）的融合与压缩中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对轻量级视觉-语言-动作（VLA）模型因参数少而牺牲时空推理能力的问题，提出SwiftVLA架构。关键技术包括：预训练4D视觉几何变换器及时态缓存提取时空特征；融合令牌通过未来预测目标增强跨模态集成；掩码-重构策略训练VLA重构掩码的4D输入，使模型学习有效表示后可在推理丢弃4D分支以保持效率。实验显示，SwiftVLA在边缘设备上性能与7倍大的模型相当，同时速度快18倍、内存占用减少12倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.00903" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>