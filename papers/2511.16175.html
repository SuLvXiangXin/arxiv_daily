<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16175" target="_blank" rel="noreferrer">2511.16175</a></span>
        <span>作者: Zhijie Deng Team</span>
        <span>日期: 2025-11-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过利用预训练的视觉-语言模型（VLM），将语言指令和视觉观察转化为机器人动作，已成为机器人学习领域的主流方法之一。然而，现有VLA方法面临一个根本性挑战：低维度的动作信号过于稀疏，难以充分监督处理高维感官输入的大型VLA模型，导致模型表征能力未被充分利用。为了弥补这一点，现有方法主要尝试整合视觉预见（Visual Foresight）来提供更密集的监督信号，但存在三个关键局限性：1）直接预测高维像素级未来帧会引入冗余信息，分散模型对动作预测的注意力，并导致高昂的训练成本和下游微调收敛缓慢；2）将视觉状态压缩为更紧凑的表示（如关键点轨迹）则会不可避免地造成信息瓶颈，丢失传达细粒度运动的细微视觉变化；3）现有方法通常忽视语言监督，导致模型在动作学习后，其原本具备的理解和推理能力被破坏或削弱。</p>
<p>本文针对上述痛点，提出了一个新的视角：将视觉预见预测与VLA主干网络解耦，从而为主干网络保留更多容量用于语言理解。其核心思路是提出一个名为“解耦视觉预见”（Disentangled Visual Foresight, DVF）的模块，该模块通过元查询和扩散Transformer头预测未来帧，并利用残差连接使元查询自动捕捉描述视觉轨迹的帧间动态（即潜在动作），从而为显式动作生成提供简洁而有效的指导。</p>
<h2 id="方法详解">方法详解</h2>
<p>Mantis的整体框架旨在解耦视觉预见与动作学习，其流程如下：给定时间步t的语言指令l和视觉观察（如图像帧）o_t，模型首先预测未来n步后的视觉状态o_{t+n}，然后生成从t到t+n的动作序列a_{t:t+n}。输入为当前观测和指令，输出为未来帧预测和多步动作。</p>
<p><img src="https://arxiv.org/html/2511.16175v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Mantis方法总览。<strong>左</strong>：渐进式训练策略，分阶段引入多模态数据以实现稳定优化。<strong>中</strong>：模型架构，由主干网络、DVF头和动作头组成。DVF头预测未来帧以促进潜在动作学习，从而改进动作预测。语言监督有助于保持主干网络的理解和推理能力。<strong>右</strong>：自适应时序集成（ATE）策略，根据运动稳定性需求动态调整集成强度。</p>
</blockquote>
<p>模型的核心组件包括：</p>
<ol>
<li>**VLM主干网络 (P)**：采用Qwen2.5-VL作为主干，负责处理语言指令l和当前视觉状态o_t，并融合可训练的潜在动作查询[LAT]。其输出为隐藏状态h_t。</li>
<li>**连接器 (C)**：由12层Transformer编码器和一个投影层构成，将主干输出h_t与当前帧o_t拼接并投影，作为DVF头的条件输入。</li>
<li>**DVF头 (D)**：采用高效的扩散Transformer（DiT）模型Sana，负责生成未来帧o_{t+n}。关键设计是通过残差连接将当前帧o_t也输入给DVF头。这使得[LAT]查询无需重建完整帧，而是专注于捕捉帧间动态（即潜在动作），为动作预测提供针对性指导。</li>
<li>**动作头 (π)**：同样基于DiT，用于生成动作。可训练的动作查询[ACT]通过因果注意力从输入和[LAT]查询中聚合信息，然后动作头将高斯噪声去噪为n步动作轨迹。</li>
<li><strong>可训练查询</strong>：包括潜在动作查询[LAT]、动作查询[ACT]和多间隙查询[GAP]。[GAP]用于在训练时指导生成不同时间间隔的未来帧。</li>
</ol>
<p>与现有方法相比，Mantis的核心创新点在于其<strong>解耦设计</strong>：将计算密集的像素级未来帧预测任务卸载到独立的DVF头，而VLA主干则专注于语义理解和基于潜在动作线索的动作规划。这种设计减少了主干网络的表征负担，使其能更好地保留通过语言监督获得的理解与推理能力。</p>
<p>此外，为了缓解多模态信号（视觉、动作、语言）在训练时的竞争问题，论文提出了<strong>渐进式训练策略</strong>：</p>
<ul>
<li><strong>阶段1（多间隙视觉训练）</strong>：在无动作标注的人类操作视频（SSV2）上训练，冻结主干，解冻DVF头和[LAT]、[GAP]查询，仅优化扩散损失ℒ_DVF，让模型学习从视觉动态中推断潜在动作。</li>
<li><strong>阶段2（视觉-动作联合训练）</strong>：引入机器人演示数据（DROID），固定时间间隙以对齐视觉和动作流。解冻动作查询，优化目标为αℒ_DVF + ℒ_action，平衡视觉和动作损失。</li>
<li><strong>阶段3（语言监督混合训练）</strong>：联合训练多模态数据集和机器人数据。解冻主干网络，引入语言输出的交叉熵损失ℒ_lang，总目标为αℒ_DVF + ℒ_action + βℒ_lang。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.16175v1/x3.png" alt="多间隙生成可视化"></p>
<blockquote>
<p><strong>图3</strong>：多间隙未来帧生成可视化。展示了模型根据当前帧（最左）预测不同未来时间间隔帧的能力。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：预训练使用SSV2（22万人类操作视频）、DROID（7.6万机器人演示）以及38个多模态数据集中的图像-文本对。下游评估主要在LIBERO仿真基准（包含Spatial, Object, Goal, Long四个任务套件，各10个任务）和真实世界的Agilex机器人平台进行。</li>
<li><strong>对比方法</strong>：包括非视觉增强基线（如Diffusion Policy, OpenVLA, π_0）和视觉增强基线（如ATM, CoT-VLA, WorldVLA, UniVLA, UnifiedVLA, DreamVLA, ℱ₁）。</li>
<li><strong>评估指标</strong>：成功率（SR, 0-100）、收敛速度、推理计数（IC）以及真实世界中的指令遵循和泛化能力。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>LIBERO基准性能</strong>：Mantis在LIBERO上取得了96.7%的平均成功率，在4个任务套件中的3个上表现最佳，超越了所有对比的视觉增强及非视觉增强基线。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16175v1/x1.png" alt="LIBERO结果表"></p>
<blockquote>
<p><strong>表1</strong>：LIBERO基准对比结果。Mantis在平均成功率上领先，验证了DVF对动作预测的有效性。</p>
</blockquote>
<ol start="2">
<li><strong>收敛速度</strong>：如图5所示，Mantis的收敛速度远快于采用耦合视觉预见方法的UnifiedVLA（后者前10个epoch成功率为0），与非视觉增强的OpenVLA和潜在动作监督方法UniVLA的收敛速度相当，证明了将预见预测与动作学习解耦的必要性。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16175v1/x5.png" alt="收敛速度对比"></p>
<blockquote>
<p><strong>图5</strong>：收敛速度对比。Mantis相比传统的耦合视觉预见方法（如UnifiedVLA）收敛显著更快。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界评估</strong>：在三个真实场景中，Mantis在领域内（ID）和领域外（OOD）指令上的平均成功次数均显著优于当前领先的开源VLA模型π_0.5，尤其在OOD指令的泛化上优势明显，证明了语言监督对保持模型理解和推理能力的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16175v1/x6.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图6</strong>：真实世界实验。(a) Agilex平台。(b) 场景设置与示例指令。(c) Mantis与π_0.5在三个场景中ID和OOD任务上的平均成功次数对比。(d) 场景1中每项任务的成功次数细节。</p>
</blockquote>
<ol start="4">
<li><strong>DVF有效性可视化</strong>：如图7所示，DVF头生成的未来帧序列的最后一帧与真实最终状态高度相似，证实了DVF能够捕捉有意义的视觉动态以辅助动作预测。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16175v1/x7.png" alt="未来帧生成可视化"></p>
<blockquote>
<p><strong>图7</strong>：生成的未来帧可视化。生成的最后一帧与真实最终状态高度吻合，证实了DVF在精炼动作预测方面的效力。</p>
</blockquote>
<ol start="5">
<li><strong>自适应时序集成（ATE）分析</strong>：提出的Mantis-ATE变体在保持与标准Mantis（使用TE）相当成功率的同时，将平均推理计数（IC）降低了近50%，大幅提升了推理效率。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16175v1/x8.png" alt="ATE效率对比"></p>
<blockquote>
<p><strong>图8</strong>：标准Mantis（TE）与Mantis-ATE的对比。Mantis-ATE在保持性能的同时，将推理计数降低了近50%。</p>
</blockquote>
<ol start="6">
<li><strong>消融实验</strong>：<ul>
<li><strong>DVF消融</strong>（表2）：对比了vanilla-DVF、有缺陷的DVF（无残差连接）、无DVF以及预训练DVF四个变体。结果显示，预训练DVF性能最佳，其次是vanilla-DVF，有缺陷的DVF次之，无DVF最差。这证实了：(a) DVF促进动作学习；(b) 残差连接帮助DVF更好地捕捉潜在动作；(c) 视频预训练能进一步提升DVF性能。</li>
<li><strong>语言监督消融</strong>：真实世界实验（图6）已表明，保持语言监督的Mantis在指令遵循和泛化能力上显著优于可能因动作训练而覆盖了语言对齐的基线模型π_0.5。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16175v1/x4.png" alt="ATE机制可视化"></p>
<blockquote>
<p><strong>图4</strong>：ATE可视化。展示了目标图像块（基于文本-视觉注意力）和动态图像块（基于帧间相似度）的重叠情况，重叠区域指示需要进行精细操作。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li>提出了<strong>解耦视觉预见（DVF）</strong> 的新范式，通过元查询和DiT头将密集的视觉预测从VLA主干中分离，为主干网络提供简洁且信息丰富的预见线索以改善动作预测，同时减轻主干负担以保留语言理解能力。</li>
<li>设计了一种<strong>渐进式训练策略</strong>，分阶段引入视觉、动作和语言模态，有效缓解了多模态信号竞争，实现了稳定高效的优化和多模态融合。</li>
<li>在仿真（LIBERO基准96.7%成功率）和真实机器人实验中均验证了Mantis的优越性能，特别是在指令遵循、对未见指令的泛化以及推理能力方面超越了强基线。此外，提出的自适应时序集成（ATE）策略能大幅提升推理效率。</li>
</ol>
<p>论文自身提到的局限性包括：在真实世界场景中，由于缺少机器人本体状态（如关节角度）作为输入，偶尔会出现轻微的运动回退（rollback）现象。</p>
<p>本工作对后续研究的启示在于：1）<strong>解耦设计</strong>是平衡模型容量分配、提升训练效率和保持多模态能力的有效途径；2）<strong>渐进式训练</strong>对于融合异构、不同监督强度的多模态数据具有重要价值；3）在推理阶段，根据任务需求（如运动稳定性）<strong>动态调整计算策略</strong>（如ATE）是提升VLA模型实用性的一个值得探索的方向。未来工作可考虑集成更丰富的输入模态（如3D点云）并进一步优化推理速度。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Mantis模型，旨在解决现有视觉-语言-动作模型中视觉状态预测导致模型容量分散、训练成本高，以及语言监督不足影响理解与推理能力的问题。其核心技术是解耦视觉预见，通过元查询与扩散Transformer头分离视觉预测任务，使主干模型专注于语言监督下的理解与推理。实验表明，Mantis在LIBERO基准上微调后达到96.7%的成功率，超越基线模型；其变体Mantis-ATE通过自适应时间集成策略，在保持性能的同时将推理次数减少50%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16175" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>