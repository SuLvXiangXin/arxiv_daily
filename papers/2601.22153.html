<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.22153" target="_blank" rel="noreferrer">2601.22153</a></span>
        <span>作者: Xie, Haozhe, Wen, Beichen, Zheng, Jiarui, Chen, Zhaoxi, Hong, Fangzhou, Diao, Haiwen, Liu, Ziwei</span>
        <span>日期: 2026/01/29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型在静态物体操控任务上展现出强大的泛化能力，但在动态物体操控这一开放挑战上表现不佳。动态操控要求机器人具备快速感知、时序预测和连续控制能力，即使微小的延迟也可能导致任务失败。现有方法主要分为两类：一类是基于反应式控制和手工设计感知流程的方法，仅适用于结构化场景；另一类是近期能够与快速移动目标实时交互的VLA模型，但这些任务对时序和空间误差容忍度高，不涉及精确的6自由度操控。尽管VLA模型在静态操控中取得成功，但其通常依赖数B参数的大型骨干网络，推理速度慢，且采用串行的“推理-执行”模式，在物体状态持续演变的动态场景中，推理延迟会导致感知与执行严重失配，现有方法缺乏对此问题的专门设计。本文针对动态物体操控中严格的低延迟和时序对齐需求，提出了一种集成时序推理和闭环自适应的新框架DynamicVLA，其核心思路是构建一个紧凑的VLA模型，并通过“连续推理”和“潜在感知动作流”两项关键技术，实现低延迟的预测与执行重叠，以及感知与动作间的时序对齐。</p>
<h2 id="方法详解">方法详解</h2>
<p>DynamicVLA框架旨在解决动态操控中的延迟与失配问题，其整体流程如下：在时间步t，模型接收一个时间窗口的视觉观测O_t、语言指令L_t和本体感知状态P_t，预测一个未来动作序列A_t。其创新性体现在一个紧凑的模型架构和两个专门的执行机制上。</p>
<p><img src="https://arxiv.org/html/2601.22153v1/figures/logo.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DynamicVLA概述。(a) 一个0.4B参数的VLA架构，将轻量级骨干网络与动作专家耦合，用于快速闭环控制。(b) 连续推理通过流水线化的推理窗口重叠推理和执行，实现跨连续动作块的非阻塞动作执行。(c) 潜在感知动作流通过使过时动作失效并优先执行最新动作块中的动作，来强制执行时序一致的动作流。</p>
</blockquote>
<p><strong>1. 紧凑的0.4B参数VLA架构：</strong><br>模型采用SmolLM2-360M作为语言骨干，并将语言骨干截断至前16层Transformer，以显著降低延迟。关键创新在于视觉编码器没有使用传统的基于Transformer的架构，而是采用了卷积网络FastViT。这种选择实现了高效的空间压缩，避免了处理多帧视觉输入时令牌数量的二次增长，从而在动态操控设置中实现了显著更快、更紧凑的推理。动作专家采用基于流匹配的扩散模型，其训练目标是让模型预测的去噪向量场匹配真实的数据分布。通过轻量级的线性投影层对齐跨模块的表示。</p>
<p><strong>2. 连续推理：</strong><br>传统VLA模型采用串行模式，即必须等待上一个预测的动作序列完全执行完毕后，才触发下一次推理。这引入了“块间等待”，在动态物体运动下会降低响应性。DynamicVLA的连续推理模块将推理周期设计为一旦上一次推理完成就立即触发，与前一动作序列是否执行完毕无关。只要动作序列长度n大于推理延迟m，新的动作序列就会在当前序列执行完之前可用，从而消除了块间等待，实现了推理与执行的流水线化重叠。</p>
<p><strong>3. 潜在感知动作流：</strong><br>该模块专门解决因推理延迟m导致的时序失配问题。失配体现为两方面：一是“感知-执行间隙”，即推理开始时预测的动作，在m步后才可用，此时环境已变化，使得前m个动作过时；二是“动作块重叠冲突”，即连续推理会产生针对同一执行时间步的多个候选动作。解决方案是：丢弃动作序列A_t中时间早于t+m的过时动作；对于A_t和A_{t+m}重叠的时间步，优先采用来自更新序列A_{t+m}的动作，覆盖旧序列中的动作。这确保了执行能及时适配最新的环境状态。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了论文提出的动态物体操控基准DOM，该基准包含通过自动化流程收集的模拟数据（200K条轨迹，涉及206个物体和2.8K个场景）和真实世界数据（2K条轨迹）。实验平台包括模拟环境Isaac Sim（Franka机械臂）以及真实世界的Franka和AgileX PiPER机械臂。评估围绕DOM基准的三大维度（交互、感知、泛化）及其九个子维度展开。</p>
<p>对比的基线方法涵盖了不同类型的VLA模型，包括Diffusion Policy、OpenVLA-OFT、π₀、π₀.₅、SmolVLA、GR00T-N1.5、VLA-Adapter-Pro和VLASH。所有基线均使用其官方实现和预训练权重，并在DOM数据集上进行了微调。</p>
<p><strong>关键实验结果：</strong><br>在模拟基准的综合评估中（表I），DynamicVLA取得了显著领先的成功率。其平均成功率为47.06%，远超最佳基线VLA-Adapter-Pro的13.61%和VLASH的12.33%。在交互维度（闭环反应性、动态适应性、长时程序列），DynamicVLA分别达到60.5%、38.5%和40.5%的成功率，相比最强基线提升幅度分别为+188.1%、+87.8%和+440.0%。在感知维度（视觉理解、空间推理、运动感知）和泛化维度（视觉泛化、运动泛化、干扰鲁棒性）上，DynamicVLA也全面领先，尤其在视觉泛化（59.5%）和运动泛化（65.0%）上优势明显。此外，DynamicVLA的任务完成时间最短（8.53秒），但路径长度更长（2.50米），表明其为实现动态追踪进行了更积极的运动。</p>
<p><img src="https://arxiv.org/html/2601.22153v1/x1.png" alt="模拟结果表"></p>
<blockquote>
<p><strong>表I</strong>：动态物体操控模拟基准结果。报告了九个子维度的平均成功率（SR, %）。DynamicVLA在整体平均成功率（47.06%）上大幅领先所有基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.22153v1/x2.png" alt="真实世界交互评估"></p>
<blockquote>
<p><strong>图3</strong>：真实世界交互评估。在Franka和PiPER上的六项任务中，DynamicVLA相比π₀.₅、SmolVLA和VLASH基线，取得了更高的平均成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.22153v1/x3.png" alt="真实世界感知评估"></p>
<blockquote>
<p><strong>图4</strong>：真实世界感知评估。在涉及视觉、空间和运动理解的六项真实任务中，DynamicVLA（51.7%成功率）的感知能力显著优于基线方法（如VLASH的11.7%）。</p>
</blockquote>
<p><strong>消融实验与分析：</strong><br>论文通过消融实验验证了各核心组件的贡献。移除非重叠的连续推理会导致成功率急剧下降（从47.1%降至9.4%），因为恢复了块间等待。移除潜在感知动作流则使成功率降至29.4%，因为过时动作会持续执行。将卷积视觉编码器替换为基于Transformer的编码器，会使模型参数量增至1.4B，且推理延迟增加，导致成功率下降至40.6%。这些结果证实了紧凑架构、连续推理和动作流对齐对于动态操控的必要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个专为动态物体操控设计的紧凑型0.4B参数VLA模型，采用卷积视觉编码器实现高效推理；2) 引入了连续推理和潜在感知动作流两个模块，共同解决了动态场景中的延迟和感知-执行失配问题，实现了实时闭环控制；3) 构建了首个大规模动态物体操控基准DOM，提供了涵盖模拟与真实世界的自动化数据收集流程，为领域建立了评估标准。</p>
<p>论文提到的局限性包括：紧凑的模型容量可能在需要复杂视觉-语言推理的感知任务上存在上限；真实世界的“模拟器”依赖于视觉状态估计，其噪声可能影响控制器性能。</p>
<p>这项工作对后续研究的启示在于：它证明了通过专门的系统设计（而非单纯扩大模型），轻量级模型也能在极具挑战性的动态实时任务中取得卓越性能。DOM基准的建立为动态操控研究提供了重要的数据和评估基础。未来工作可以探索更高效的架构、更鲁棒的状态估计方法，以及将此类框架扩展到更复杂的多物体动态交互任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对动态物体操作中VLA模型存在的感知延迟与动作不同步的核心挑战，提出DynamicVLA框架。其关键技术包括：1) 采用卷积视觉编码器的0.4B紧凑模型以实现快速推理；2) 连续推理机制以降低延迟；3) 潜在感知动作流以对齐感知与执行。为填补数据空白，作者构建了包含大量合成与真实数据的DOM基准。实验表明，该框架在响应速度、感知和泛化能力上均取得显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.22153" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>