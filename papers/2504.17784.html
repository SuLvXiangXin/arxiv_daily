<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.17784" target="_blank" rel="noreferrer">2504.17784</a></span>
        <span>作者: Yang, Yuyin, Cai, Zetao, Tian, Yang, Zeng, Jia, Pang, Jiangmiao</span>
        <span>日期: 2025/04/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人双手操作的行为克隆方法主要分为两类：基于关键帧的策略和基于连续控制的策略。基于关键帧的方法（如PerAct2、VoxAct-B）预测参考坐标系中的目标夹爪位姿，并通过逆运动学求解器和运动规划器执行。这类方法因监督稀疏，模型更关注局部特征，空间感知能力强。但其关键帧难以定义，且运动规划器倾向于输出近似直线路径，使其难以执行需要曲线轨迹或严格时空协调的任务（如擦盘子、端托盘）。基于连续控制的方法（如ACT、RDT）在每个时间步序贯估计动作，通用性更强。但由于其对动作的密集监督，模型容易“走捷径”，过度拟合已见过的轨迹（如本体感知），导致空间感知能力较弱。因此，在保持强大感知能力的同时，实现多样化的通用双手操作任务仍是一个关键挑战。</p>
<p>本文针对上述痛点，提出了一个新颖的视角：将关键帧信息作为接口，来引导连续动作的生成。核心思路是设计一个端到端的框架PPI，通过预测目标夹爪关键位姿和物体点流这两个中间接口，并将其与连续动作估计相结合，从而在任务多样性与空间感知之间取得平衡。</p>
<h2 id="方法详解">方法详解</h2>
<p>PPI是一个端到端的、基于接口的连续动作策略。其整体流程分为三个部分：感知、接口定义和动作预测。</p>
<p><img src="https://arxiv.org/html/2504.17784v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：PPI方法总览。(a) 感知：构建3D语义神经场并采样初始查询点。(b) 接口：定义两个中间接口——目标夹爪位姿和物体点流。(c) 预测：扩散变压器整合多种输入，通过设计的单向注意力机制，以接口为条件逐步去噪预测动作。</p>
</blockquote>
<p><strong>1. 感知模块</strong><br>输入为来自K个相机的RGBD图像和语言指令。首先，通过裁剪和下采样预处理原始点云。对于每个采样的3D点，将其投影到多个相机视角的2D图像上，使用DINOv2模型提取像素级语义特征，并通过加权和（权重由点到投影表面的距离决定）进行特征融合。为减少变压器主干网络中的场景令牌数量，使用PointNet++密集编码器对场景点进行下采样，得到一个紧凑的3D语义神经场表示 (S_t \in \mathbb{R}^{N_s \times (3+D)})，其中包含(N_s)个点的空间坐标和D维融合语义特征。<br>此外，为预测物体点流，需要在任务开始时从待操作物体上采样初始查询点(F_0)。具体使用Grounding DINO根据语言提示和图像获得物体边界框，再输入SAM模型生成物体掩膜，最后从掩膜中随机采样(N_q=200)个像素点获取其3D坐标。</p>
<p><strong>2. 接口模块</strong><br>这是PPI的核心创新，包含两个中间接口：</p>
<ul>
<li><strong>目标夹爪关键位姿</strong>：在关键帧时间步预测未来(h^k)个关键帧的目标夹爪位姿(a_t^k = {a_{t^k_i}}_{i=1}^{h^k})作为显式的动作目标，用于引导连续动作生成。关键帧根据夹爪开合度和手臂关节状态的显著变化点启发式确定。</li>
<li><strong>物体点流</strong>：定义了(N_q)个空间查询点在后续(h^k)个关键帧的位置(F \in \mathbb{R}^{h^k \times N_q \times 3})。其真实标签通过物体的6D位姿（仿真中从数据集获取，真实世界中使用BundleSDF和Foundation Pose估计）将初始查询点(F_0)进行坐标变换得到。推理时无需实时物体6D位姿估计。点流与目标夹爪位姿共同建模了物体与机器人之间的交互。</li>
</ul>
<p><strong>3. 预测模块</strong><br>基于扩散变压器构建。在时间步t和去噪步i，模型整合场景令牌(S_t)、语言令牌(l)、查询点令牌(F_0)、带噪声的关键帧动作令牌(a_t^{k,i})和连续动作令牌(a_t^{c,i})。<br>其关键创新在于设计了一个<strong>单向注意力机制</strong>：</p>
<ul>
<li>所有点流和动作令牌都关注场景和语言令牌，以整合空间和语义知识。</li>
<li>带噪声的关键帧动作令牌(a_t^{k,i})额外关注点流令牌，旨在提取物体级特征。</li>
<li>最终的连续动作令牌(a_t^{c,i})关注所有前述令牌，不仅提炼常规的场景级特征，而且充分利用接口中包含的局部细节特征。<br>此外，在点流、动作和场景点云令牌之间应用了基于相对3D位置的相对注意力（使用旋转位置编码），以编码3D空间关系。机器人的本体感知(c_t)和去噪时间步(i)通过FiLM层影响注意力。</li>
</ul>
<p><strong>4. 训练与推理细节</strong><br>训练损失由三部分加权和构成：连续动作去噪损失(\mathcal{L}_c)、关键帧动作去噪损失(\mathcal{L}_k)（均采用DDPM训练）和点流回归损失(\mathcal{L}_F)（直接L1回归）。权重(w_1, w_2, w_3)分别设为0.05, 0.05, 1。<br>推理时，首先在初始时刻采样物体上的查询点。然后在每个时间步t，从高斯分布中随机采样初始的连续动作和关键帧动作，并进行多步去噪（仿真用DDPM 1000步，真实世界用DDIM 20步）。实践中，模型预测50个连续动作以及4个关键帧动作和点流。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真基准</strong>：在RLBench2双手操作基准的7个代表性任务上进行评估。</li>
<li><strong>真实世界基准</strong>：使用两台Franka Research 3机器人，在两个场景（桌面和货架）中设计了四项具有高定位需求和运动约束的长视野任务（端托盘、递接与插入盘子、擦盘子、扫描瓶子）。</li>
<li><strong>对比方法</strong>：与基于关键帧的方法（PerAct2、VoxAct-B）、基于连续控制的方法（ACT、RDT、BiKC）以及扩展自单臂操作的扩散策略（DP3）进行了对比。</li>
</ul>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2504.17784v1/x5.png" alt="仿真结果对比"></p>
<blockquote>
<p><strong>图5</strong>：在RLBench2仿真基准上的成功率对比。PPI在7个任务上的平均成功率比之前的最佳方法高出16.1%，达到了新的SOTA性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.17784v1/x6.png" alt="真实世界结果对比"></p>
<blockquote>
<p><strong>图6</strong>：真实世界四项任务的主要结果（成功率、定位成功率、标准化得分）。PPI在所有任务和指标上均优于基线方法，平均成功率相比最佳基线提升27.5%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.17784v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融实验验证两个接口的有效性。移除点流接口导致性能显著下降（-24.3%），移除关键位姿接口也有明显下降（-11.4%），同时移除两者性能最差，证明了每个接口的独特贡献。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.17784v1/x8.png" alt="泛化与鲁棒性测试"></p>
<blockquote>
<p><strong>图8</strong>：在物体外观、光照条件和视觉干扰变化下的泛化与鲁棒性测试。PPI表现出强大的稳定性、高精度和显著的泛化能力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：点流接口对提升空间定位和泛化能力贡献最大（移除后性能下降24.3%），关键位姿接口对于引导动作生成、处理运动约束至关重要（移除后下降11.4%）。两者共同作用时效果最佳。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的框架，利用关键帧信息（目标夹爪位姿和物体点流）作为接口来引导连续动作生成，从而结合了基于关键帧方法的空间感知优势和基于连续控制方法的轨迹灵活性。</li>
<li>设计了有效的物体点流接口，通过建模物体运动显著增强了模型的空间定位能力和对未见物体的泛化能力。</li>
<li>在仿真和真实世界双手操作基准上实现了SOTA性能，并展示了出色的鲁棒性和泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文在讨论部分指出，当前方法主要处理刚性物体，对于非刚性物体（如毛巾）的变形操作，点流接口的定义和获取将面临挑战。此外，在高度动态的场景中，实时精确获取物体6D位姿以生成训练标签也可能存在困难。</p>
<p><strong>启示</strong>：PPI的工作表明，在端到端策略中引入具有明确物理意义的中间表示（接口）是提升性能的有效途径。这为后续研究提供了方向：如何为更复杂的任务（如非刚性体操作、动态环境交互）设计更鲁棒和通用的接口；如何进一步自动化或弱监督这些接口标签的获取过程，以降低对精确标注数据的依赖。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双手机器人操作中，现有关键帧方法缺乏帧间监督、难以执行曲线运动，而连续控制方法空间感知弱的问题，提出了一种名为PPI的端到端框架。该框架通过预测目标抓取器关键姿态和物体点流作为接口，并与连续动作估计相结合，从而在增强空间定位能力的同时，指导生成多样且无碰撞的运动轨迹。实验表明，PPI在RLBench2模拟基准上性能提升16.1%，在四项真实世界任务中平均增益达27.5%，实现了优越的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.17784" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>