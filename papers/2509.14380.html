<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.14380" target="_blank" rel="noreferrer">2509.14380</a></span>
        <span>作者: Choi, Seoyeon, Ryu, Kanghyun, Ock, Jonghoon, Mehr, Negar</span>
        <span>日期: 2025/09/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，多智能体强化学习（MARL）是学习多智能体协调行为的有力框架，但在机器人领域的应用仍面临巨大挑战，包括高维连续联合动作空间、复杂的奖励设计以及去中心化设置固有的非平稳性。另一方面，人类通过分阶段的课程学习复杂协调，将长远行为建立在更简单的技能之上。受此启发，本文提出利用大语言模型（LLM）和视觉语言模型（VLM）等基础模型的推理能力，充当多机器人协调任务的“教练”，以解决上述痛点。本文的核心思路是：利用LLM自动将长视野协调任务分解为子任务序列，并为每个子任务生成可执行的奖励函数，然后通过VLM引导的奖励优化循环迭代改进奖励，最终通过顺序训练学习复杂的协调策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>CRAFT框架模仿人类教练训练团队的过程，包含五个关键阶段，其整体流程如下图所示。</p>
<p><img src="https://arxiv.org/html/2509.14380v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：CRAFT整体方法框架。包含五个核心模块：1）课程生成模块（Curriculum Generation）利用LLM分解任务；2）奖励函数生成模块（Reward Function Generation）利用LLM为每个子任务生成可执行代码形式的奖励；3）策略评估模块（Policy Evaluation）利用VLM基于视觉和定量数据评估策略成败；4）奖励优化模块（Reward Refinement）在策略失败时，利用VLM提供建议，并由LLM修改奖励函数；5）子任务顺序训练模块（Sequential Training of Subtasks）按顺序训练子任务，并利用策略迁移和探索重置技术。</p>
</blockquote>
<p><strong>1. 课程生成模块</strong>：给定目标任务和环境的自然语言描述，一个课程LLM首先生成多个候选子任务序列（课程）。由于零样本生成可能不稳定，模块会将这些候选课程连同环境描述和目标任务再次提供给同一个LLM，让其合并、提炼出一个更连贯有效的最终课程𝒞 = [l₁, ..., l_K]，其中l_k是每个子任务ℳ_k的自然语言描述。描述会与环境的特定状态变量（如欧氏距离）进行关联，以提高后续步骤的精确性。下图展示了一个课程提炼的例子。</p>
<p><img src="https://arxiv.org/html/2509.14380v2/x2.png" alt="课程提炼示例"></p>
<blockquote>
<p><strong>图2</strong>：“提起并平衡锅”任务的课程提炼示例。展示了三个候选课程𝒞¹到𝒞³，LLM通过选择每个候选课程中更强的任务定义，将它们合并为最终课程𝒞。例如，𝒞³中的任务1（最小化距离并匹配方向）比𝒞¹中的（仅最小化距离）更优。</p>
</blockquote>
<p><strong>2. 奖励函数生成模块</strong>：基于子任务描述l_k，一个奖励生成LLM会生成该子任务特有的奖励函数R_k⁰（上标0表示初始版本），输出为可执行的Python代码。为降低生成难度，提示词中会提供环境描述、简单的示例奖励函数以及用于状态处理的辅助函数。同时，为了促进课程学习，还会将之前子任务的奖励函数作为上下文提供给LLM。生成奖励时，LLM被要求将奖励分解为不同的奖励组件（例如<code>lift_reward</code>, <code>balance_reward</code>），这些组件的学习曲线将用于后续的奖励优化。</p>
<p><strong>3. 策略评估模块</strong>：使用初始奖励R_k⁰训练出一个策略π_k⁰（使用如MAPPO的MARL算法）后，一个评估VLM会根据策略 rollout 的截图序列和相关的状态变量轨迹数据，来判断该策略是否成功完成了子任务l_k。由于VLM难以直接从原始3D状态推理协调行为，因此会提取并提供易于理解的任务相关指标（如方向差的Frobenius范数、物体间距离）。VLM的判断（成功或失败及失败原因）决定了是进入下一个子任务还是触发奖励优化。</p>
<p><strong>4. 奖励优化模块</strong>：当策略被评估为失败时，系统进入VLM引导的奖励优化循环。首先，一个建议VLM会基于当前子任务描述l_k、已尝试的奖励函数R_kʲ、失败原因以及各奖励组件学习曲线的图像，生成自由形式的改进建议。然后，一个优化LLM根据该建议和旧奖励R_kʲ，生成新的优化后奖励R_kʲ⁺¹。随后用新奖励重新训练策略，并再次评估，循环迭代（最多J=3次）。若迭代后仍失败，则LLM会从所有尝试的策略中选择最佳者进入下一子任务。下图展示了该过程的实例。</p>
<p><img src="https://arxiv.org/html/2509.14380v2/x3.png" alt="奖励优化示例"></p>
<blockquote>
<p><strong>图3</strong>：子任务“协调初步提起”的奖励优化示例。第一次优化循环产生的奖励R_{k=3}¹未能使锅达到0.05米高度。建议VLM分析学习曲线后指出<code>lift_reward</code>相对于<code>balance_reward</code>太弱，并给出具体修改建议（如移除高度平方项、增加提起权重、降低平衡权重）。优化LLM据此生成了修改后的奖励R_{k=3}²，使策略最终成功。</p>
</blockquote>
<p><strong>5. 子任务顺序训练</strong>：在进入新子任务ℳ_{k+1}时，策略网络π_{k+1}的权重从π_k初始化，以利用先前知识。但为防止因奖励函数相似而陷入局部最优、丧失塑性，采用了两种技术：1）重置策略网络的探索参数（如MAPPO的策略标准差层）；2）用随机权重重新初始化价值网络，而保持行动者网络权重不变，使行动者能利用先前策略，同时批评家能重新估计新奖励下的价值。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在三个模拟的多机器人协调环境中评估CRAFT：1) <strong>Quadruped Gate</strong>：两个四足机器人协调通过狭窄门而不碰撞；2) <strong>Quadruped Seesaw</strong>：两个四足机器人在跷跷板上协调，一个爬上顶部平台，另一个平衡跷跷板；3) <strong>Two Arm Lift</strong>：两个机械臂协调提起一个锅并保持水平。这些任务需要长视野协调和部分可观测下的去中心控制，极具挑战性。使用MAPPO算法进行训练，LLM使用gpt-4o，VLM使用o4-mini。</p>
<p><strong>基线方法</strong>：为回答三个关键问题（Q1-Q3），设置了以下基线：1) <strong>env_reward</strong>：使用环境提供的奖励，无课程；2) <strong>example_reward</strong>：使用提供给CRAFT的示例奖励，无课程；3) <strong>no_curriculum</strong>：使用CRAFT生成的最终奖励函数（最高成功率的top-3），但固定奖励、无分阶段课程训练；4) <strong>no_refinement</strong>：CRAFT的变体，移除VLM引导的奖励优化模块，仅依赖LLM的随机重采样来生成新奖励。</p>
<p><img src="https://arxiv.org/html/2509.14380v2/figures/go2gate_env.jpeg" alt="任务环境示意图"></p>
<blockquote>
<p><strong>图4a</strong>：Quadruped Gate任务环境，两个四足机器人需协调通过狭窄门。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.14380v2/figures/go2seesaw_env.jpeg" alt="任务环境示意图"></p>
<blockquote>
<p><strong>图4b</strong>：Quadruped Seesaw任务环境，需在动态平衡的跷跷板上进行协调。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.14380v2/figures/lift_example.png" alt="任务环境示意图"></p>
<blockquote>
<p><strong>图4c</strong>：Two Arm Lift任务环境，两个机械臂需协同提起并保持锅的水平。</p>
</blockquote>
<p><strong>关键结果</strong>：</p>
<ul>
<li>**Q1 (CRAFT有效性)**：如图6所示，在所有三个环境中，CRAFT均取得了最高的成功率（Quadruped Gate: ~90%， Quadruped Seesaw: ~60%， Two Arm Lift: 100%）。而<code>env_reward</code>在多数实验中成功率为0%，<code>example_reward</code>成功率也低于10%。这表明CRAFT能够学习到标准MARL方法即使有人工设计奖励也难以实现的复杂协调策略。定性分析显示，基线方法容易陷入局部最优，产生次优行为（例如，仅一个机器人过门，或只抓住锅但提不起来），如下图所示。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.14380v2/x4.png" alt="基线策略次优行为"></p>
<blockquote>
<p><strong>图5</strong>：使用<code>env_reward</code>（无课程）训练的策略表现出次优行为示例：左图仅一个智能体通过门；右图仅抓住锅但未能提起。</p>
</blockquote>
<ul>
<li>**Q2 (课程必要性)**：<code>no_curriculum</code>基线使用与CRAFT相同的最终奖励，但进行单阶段训练。结果显示，在Quadruped Gate和Two Arm Lift任务中，CRAFT的成功率显著高于<code>no_curriculum</code>，表明分阶段课程训练对于掌握这些长视野任务至关重要。在Quadruped Seesaw任务中，两者性能接近，但CRAFT在训练稳定性（有效课程比率）上更优。</li>
<li>**Q3 (奖励优化必要性)**：<code>no_refinement</code>基线（无VLM引导优化）的性能明显低于完整的CRAFT，尤其是在Quadruped Seesaw和Two Arm Lift任务中。这证明了VLM提供针对性建议、引导奖励迭代优化的模块对于生成有效奖励函数是关键且高效的。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.14380v2/x5.png" alt="成功率曲线"></p>
<blockquote>
<p><strong>图6</strong>：各环境中top-3课程的成功率曲线。CRAFT（红色）在三个环境中均达到最高成功率，显著优于各基线方法。</p>
</blockquote>
<p><strong>消融实验总结</strong>：实验通过对比<code>no_curriculum</code>和<code>no_refinement</code>基线，分别验证了<strong>分阶段课程训练</strong>和<strong>VLM引导的奖励优化循环</strong>这两个核心组件的贡献。两者对于CRAFT在复杂协调任务上取得高性能都是必要的。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了CRAFT框架，首次利用基础模型（LLM和VLM）作为“教练”，为多机器人长视野协调任务<strong>自动设计课程</strong>；2) 通过LLM生成可执行奖励代码，并结合<strong>VLM引导的奖励优化循环</strong>，实现了奖励函数的自动设计与迭代改进；3) 在模拟和实物机器人（四足导航）实验中验证了框架的有效性，能够学习到传统方法难以实现的复杂协调策略。</p>
<p><strong>局限性</strong>：论文提到，即使最先进的LLM/VLM也缺乏对<strong>3D运动</strong>的原生理解，这给从原始状态评估多机器人协调行为带来了挑战。因此，方法中需要提取并提供易于模型理解的、任务相关的量化指标作为辅助。</p>
<p><strong>研究启示</strong>：CRAFT展示了基础模型在自动化机器人学习流程（如课程设计、奖励生成、策略评估）中的巨大潜力。后续研究可探索：1) 如何增强基础模型对物理交互和3D动态场景的理解能力；2) 将该“教练”框架扩展到更多样化的多机器人任务和更复杂的动态环境中；3) 研究如何降低对强大基础模型的依赖，或开发更高效的交互协议。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出CRAFT框架，旨在解决多机器人协调任务中多智能体强化学习面临的高维动作空间、稀疏奖励和非平稳环境等挑战。该框架的核心方法是利用大型语言模型（LLMs）自动将长视野任务分解为子任务序列，并通过LLM生成奖励函数、结合视觉语言模型（VLM）进行奖励细化，以自主生成训练课程。实验在多四足机器人导航和双手操作任务中验证了CRAFT能够有效学习复杂协调行为，并在真实机器人平台上实现了策略部署。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.14380" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>