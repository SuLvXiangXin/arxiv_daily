<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2205.08316" target="_blank" rel="noreferrer">2205.08316</a></span>
        <span>作者: von Hartz, Jan Ole, Chisari, Eugenio, Welschehold, Tim, Valada, Abhinav</span>
        <span>日期: 2022/05/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人操作领域，从原始相机观察中学习策略通常计算成本高昂且需要大量训练数据，而地面真值场景特征在仿真环境外通常难以获取。现有视觉表示学习方法各有局限：基于姿态估计的方法需要物体的3D模型，无法处理可变形物体和遮挡；基于图像重建的方法（如β-VAE、MONet、Transporter）虽然适用于任意场景，但难以解释，且其预训练任务（重建）与下游策略学习任务差异巨大，限制了其有效性。Dense Object Nets (DON) 通过密集对应（Dense Correspondence）预训练任务生成的关键点表示，已被证明具有紧凑、可解释、利于高效策略学习的优点，但先前工作仅研究了无遮挡、固定摄像头下的简单单物体任务，无法应对移动摄像头带来的尺度变化以及多物体场景中的遮挡问题。</p>
<p>本文针对将关键点表示应用于实际机器人操作中的两大痛点：1) 如何使其具备尺度不变性以应对移动摄像头（如腕部摄像头）；2) 如何将其扩展到物体间存在遮挡且多个物体均与任务相关的复杂多物体场景。本文的核心思路是：通过自监督的密集对应预训练，直接在多物体场景数据上训练编码器，使其学习到的关键点具备尺度不变性和处理遮挡的能力，从而为样本高效的策略学习提供强大的视觉表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体流程分为三个阶段：1) <strong>数据收集与对应关系估计</strong>：移动RGB-D摄像头采集静态多物体场景图像序列，通过体积重建得到场景3D点云，过滤背景后，通过聚类将点云按物体分割。利用相机位姿和标定矩阵，可以计算出任意两帧图像之间物体表面上像素点的对应关系。2) <strong>密集对应预训练</strong>：使用估计的像素对应关系作为自监督信号，训练一个编码器网络，使其为图像中每个像素生成一个D维描述符。训练目标是拉近对应像素描述符的距离，同时推远非对应像素描述符的距离（至少间隔M）。3) <strong>策略学习</strong>：冻结预训练好的编码器，从专家演示轨迹中采样参考图像，并在相关物体掩码上选择参考像素位置，其描述符作为“参考描述符”。对于新的观测图像，通过编码器计算其像素描述符与每个参考描述符的欧氏距离，经softmax后得到激活图，其全局最大值的位置即为预测的关键点。这些关键点坐标（及深度值）与机器人状态（关节角度或末端执行器位姿）拼接，作为LSTM策略网络的输入，通过行为克隆学习操作策略。</p>
<p><img src="https://via.placeholder.com/500x300?text=Fig.+2+Keypoint+Generation" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：策略学习期间的关键点生成流程。给定参考图像和选定的参考点（红点），编码器为当前观测图像生成描述符场。计算参考描述符与观测图像中所有位置描述符的距离，生成距离图，取其负值并经softmax得到激活图（热力图）。关键点位置被确定为激活图的全局最大值（蓝点）。</p>
</blockquote>
<p>核心模块与技术细节包括：</p>
<ol>
<li><strong>密集对应预训练</strong>：编码器采用ImageNet预训练的ResNet50（步长8），特征图通过双线性上采样至输入分辨率。损失函数为像素级对比损失（公式1），对每个正样本对，在其所在图像中采样负样本点。使用Adam优化器，学习率指数衰减，并施加L2正则化。</li>
<li><strong>关键点生成</strong>：与先前工作使用期望值不同，本文发现激活图的全局模式在存在噪声和多模态情况下更鲁棒，因此关键点定位采用全局最大值。</li>
<li><strong>尺度不变性与多物体任务适配</strong>：<strong>创新点在于直接在多物体场景轨迹上预训练</strong>。通过聚类点云获得每个物体的独立掩码。在预训练每次迭代中，随机采样一个物体掩码作为“前景”，其他物体和背景均视为“背景”，从而教会模型区分不同物体。为处理移动摄像头（如腕部摄像头）带来的尺度变化，预训练数据必须包含策略学习时会遇到的各种视角和距离，以此强制模型学习尺度不变的表示。实验发现，更大的描述符维度有助于学习更多视角而不损失质量，但需用描述符维度的平方根对距离进行归一化。</li>
<li><strong>策略学习</strong>：策略网络为LSTM，输出动作（末端执行器位姿变化量）高斯分布的均值，方差固定。损失函数为负对数似然（公式2）。关键点坐标被归一化到[-1, 1]区间，并与深度值拼接形成3D关键点表示，作者发现这比投影到世界坐标系对LSTM策略学习更有效。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在RLBench仿真平台进行，使用Franka Emika Panda机器人模型。选择了四个任务进行评估：两个单物体任务（CloseMicrowave, TakeLidOffSaucePan）和两个多物体任务（PhoneOnBase, PutRubbishInBin）。单物体任务使用腕部摄像头，提供14条专家演示；多物体任务使用腕部和顶部摄像头组成的立体系统，提供140条演示。对比方法包括：端到端的CNN（含/不含深度输入CNND）、β-VAE、Transporter、MONet、本文的2D/3D关键点方法（DC KP 2D/3D），以及使用地面真值几何信息生成关键点的上界模型（GT KP）。所有策略训练后评估200个回合的成功率。</p>
<p><img src="https://via.placeholder.com/600x400?text=TABLE+I+Success+Rates" alt="实验结果表"></p>
<blockquote>
<p><strong>表1</strong>：各方法在不同任务上的策略成功率。DC KP 3D（本文方法）在多数任务上表现优异，尤其在多物体任务PhoneOnBase上接近理论上限（GT KP）。β-VAE和Transporter在多物体任务上几乎失效。MONet在TakeLidOffSaucePan任务上表现突出，但在PhoneOnBase上因混淆机械臂与物体而性能不佳。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>单物体任务</strong>：β-VAE由于预训练与下游任务差异大，表现最差。CNN方法在CloseMicrowave上尚可，但远不如表示学习方法。在TakeLidOffSaucePan任务中，物体透明且对称，2D关键点信息不足，因此DC KP 2D成功率仅0.280，而引入深度信息的DC KP 3D大幅提升至0.800。MONet在该任务上取得了0.875的最高分，论文分析其原因是MONet将锅盖的深色部分（手柄和旋钮）分割为一个独立模块，使策略能学习到可靠的抓取点，无意中规避了透明部分带来的困难。</li>
<li><strong>多物体任务</strong>：PhoneOnBase任务难度高，人类演示者成功率仅84%，GT KP模型达到72%已接近行为克隆的理论上限。本文的DC KP 3D方法取得了0.640的成功率，表现最佳且最接近理论上限。Transporter因依赖稳定的俯视视角和分离的局部特征而在此失效（0.025）。PutRubbishInBin任务因视觉杂波和垃圾桶形状复杂，所有方法性能均有下降，但DC KP 3D（0.590）仍显著优于其他学习表示的方法。</li>
<li><strong>消融实验</strong>：实验虽未设置独立的消融研究部分，但通过对比DC KP 2D与3D，凸显了深度信息对于需要精确对齐的任务至关重要。通过与固定视角、单物体场景下DON工作的对比，间接证明了本文引入的多物体场景预训练、物体掩码采样以及多视角数据对于解决尺度变化和遮挡问题的有效性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>扩展了Dense Object Nets的能力边界</strong>，通过在多物体场景数据上预训练并采样物体掩码，使学习到的关键点具备区分多物体、处理遮挡和尺度不变的能力。2) <strong>首次系统性地将关键点表示应用于多物体操作任务</strong>，并验证了其相较于其他表示学习方法（如β-VAE, Transporter, MONet）在样本效率和高精度要求下的优势。3) <strong>提供了详细的实验对比与分析</strong>，指出了不同表示学习方法在不同挑战（透明、对称、杂波、精确对齐）下的优缺点。</p>
<p>论文自身提到的局限性包括：1) <strong>泛化能力</strong>：与MONet等方法相比，关键点表示对新摄像头视角的泛化能力较差，需要针对新挑战进行专门训练。2) <strong>噪声问题</strong>：即使改进了预训练，移动腕部摄像头产生的关键点仍比固定顶部摄像头噪声更大。3) <strong>物体对称性</strong>：当上下文信息不足时（如腕部摄像头特写），无法唯一确定物体上的位置。</p>
<p>对后续研究的启示包括：1) 利用描述符距离作为编码器置信度的直观度量，可用于忽略或重采样不确定的关键点以更好处理遮挡。2) 在密集对应预训练中使用随机裁剪等数据增强技术可能有助于进一步降低噪声。3) 引入记忆组件或利用历史帧扩展上下文，可能缓解物体对称性问题。4) 在大量不同物体上预训练DON，有望实现跨类别的泛化。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中从原始视觉输入学习策略时数据效率低、计算成本高的问题，提出通过自监督的密集对应性预训练任务学习图像关键点表示。该方法扩展至多物体场景，重点解决了关键点表示的尺度不变性和遮挡鲁棒性问题。在多样机器人操作任务上的实验表明，该关键点表示能有效促进样本高效的策略学习，优于其他视觉表示方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2205.08316" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>