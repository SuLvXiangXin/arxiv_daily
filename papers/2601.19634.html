<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.19634" target="_blank" rel="noreferrer">2601.19634</a></span>
        <span>作者: Lei Zhu Team</span>
        <span>日期: 2026-01-27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型在机器人操作任务中展现出强大性能，但其闭环部署受到高延迟和高计算成本的阻碍，原因在于每个控制时间步都需要重复运行大型视觉-语言主干网络。现有的高效化方法，如静态压缩、动态计算和缓存复用，大多基于视觉线索或启发式规则进行决策，忽略了动作上下文在具身任务中的核心作用。然而，视觉复杂度与操作难度并不必然相关：视觉简单的场景可能需要完整的推理能力进行精确交互，而视觉复杂的移动阶段则可能允许更激进的剪枝。</p>
<p>本文针对现有方法忽略动作上下文这一具体痛点，提出了动作上下文感知的自适应计算新视角。其核心思路是，利用当前视觉观测、语言指令和先前的动作状态所构成的动作中心上下文，通过一个统一的机制，自适应地协调跨时间步的认知重用、令牌剪枝和模型组件的选择性执行，从而实现高效推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>AC^2-VLA的整体框架基于一个通用的VLA流程，该流程将动作生成分解为多模态主干网络和动作头。推理瓶颈在于每个控制步重复执行VLM主干。该方法旨在利用时间、空间和深度三个维度的结构化冗余。</p>
<p><img src="https://arxiv.org/html/2601.19634v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：AC^2-VLA整体框架。在每个时间步，模型从当前观测、指令和动作上下文构建动作先验条件向量c_t，并使用统一的路由器生成令牌剪枝、层跳过和缓存重用门控，实现高效计算和低延迟控制。</p>
</blockquote>
<p>框架的核心是一个轻量级的<strong>动作先验路由器</strong>。该路由器以动作中心的条件向量c_t为输入，预测一组用于控制三种互补效率机制的计算门控：缓存重用概率p_t^cache、令牌保留分数p_t^topk和层执行门控p_t^lay。</p>
<p><strong>动作先验条件向量c_t的构建</strong>是关键创新。它明确编码了机器人的动作上下文，其中上一个动作a_{t-1}被用作主要的路由信号。此外，它还融合了视觉特征和语言指令的池化摘要、动作头内部生成步骤的索引编码τ_t，以及当启用重用时包含的缓存状态提示s_t^c。所有输入被投影并融合为一个统一的向量。</p>
<p><strong>路由器预测的三个门控</strong>具体如下：</p>
<ol>
<li><strong>缓存重用门控</strong>：一个标量概率，用于判断是否尝试重用缓存的骨干网络表示。</li>
<li><strong>令牌剪枝门控</strong>：为每个视觉令牌预测保留分数，基于该令牌特征与动作条件向量c_t的匹配度。</li>
<li><strong>层跳过门控</strong>：为每个Transformer层预测执行概率，在推理时概率低的层将被有条件地绕过。</li>
</ol>
<p><strong>从统一门控到实际加速</strong>的实现涉及三个机制：</p>
<ul>
<li><strong>缓存重用</strong>：当路由器预测高重用概率时，系统尝试通过查询一个“认知缓存”来绕过昂贵的多模态骨干前向传播。缓存键结合了动作增量代理和轻量级视觉哈希，以确保运动连续性和视觉一致性。仅在请求重用但缓存未命中时才将新计算的特征写回缓存。</li>
<li><strong>令牌剪枝</strong>：根据保留分数对令牌进行物理移除和序列压缩，以实现超越注意力掩码的实际墙钟加速。对于使用RoPE的位置编码，通过保留原始补丁索引来保持位置一致性。</li>
<li><strong>层跳过</strong>：通过一个轻量级门控机制包装每个Transformer块，在训练时使用软门控，在推理时对门控进行二值化，并动态分组激活的样本以高效执行层计算。</li>
</ul>
<p>与现有方法相比，AC^2-VLA的核心创新在于：1）首次明确提出并利用<strong>动作上下文</strong>作为计算分配决策的主要依据；2）设计了一个<strong>统一的动作先验路由器</strong>，能够协同调度时间、空间、深度三个维度的效率操作，而非孤立应用；3）通过<strong>动作引导的自蒸馏方案</strong>进行训练，在实现结构化稀疏化的同时保持了原始密集策略的行为鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在<strong>SIMPLER</strong>仿真机器人操作基准上进行，该基准旨在缩小虚实差距。评估涉及两种机器人本体（Google Robot 和 WidowX）和三种协议（Google Robot视觉匹配、Google Robot变体聚合、WidowX视觉匹配）。实验平台使用NVIDIA RTX 5090 GPU，主干网络基于CogACT（采用Prismatic-7B视觉-语言主干和DiT-Base动作头），并在Open X-Embodiment的Bridge子集上对路由模块进行微调。</p>
<p>对比的基线方法包括两类：1）<strong>通用密集VLA策略</strong>，如RT-1、RT-2-X、Octo、OpenVLA和CogACT；2）<strong>效率导向方法</strong>，如VLA-Cache（时间重用）、EfficientVLA（静态剪枝）、MoLe-VLA（条件层跳过）和FastV（轻量级剪枝）。</p>
<p>关键实验结果如下：在Google Robot视觉匹配设置下，AC^2-VLA取得了76.8%的平均成功率，优于密集基线CogACT的74.8%，同时将FLOPs降低至基线的29.4%，实现了<strong>1.79倍的墙钟加速</strong>。在更具挑战性的变体聚合设置下，AC^2-VLA在取得可比成功率（61.6% vs 61.3%）的同时，实现了1.67倍加速和34.7%的FLOPs。在WidowX的精细操作任务上，AC^2-VLA也取得了54.5%的平均成功率，优于CogACT的51.3%。</p>
<p><img src="https://arxiv.org/html/2601.19634v1/x3.png" alt="效率对比"></p>
<blockquote>
<p><strong>图3</strong>：随时间变化的自适应层执行和缓存重用情况可视化。该图展示了路由器如何根据任务阶段动态调整计算深度和缓存决策。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.19634v1/x4.png" alt="令牌重要性可视化"></p>
<blockquote>
<p><strong>图4</strong>：动作条件路由器预测的令牌级重要性可视化。左图为输入观测，右图高亮了与当前操作阶段相关的区域，同时抑制了干扰物，表明计算资源被聚焦于交互关键区域。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.19634v1/x5.png" alt="帕累托前沿"></p>
<blockquote>
<p><strong>图5</strong>：在SIMPLER基准上，令牌剪枝和层跳过的帕累托前沿。AC^2-VLA（红星）在速度和准确率权衡上优于基线方法。</p>
</blockquote>
<p>消融实验（论文中表4，此处未提供图片链接）验证了三个核心组件的贡献。当<strong>禁用缓存重用</strong>时，成功率下降至70.5%，加速比降至1.66倍，表明时间重用有助于提升闭环稳定性。当<strong>禁用令牌剪枝</strong>时，加速比降至1.52倍，突出了空间稀疏化的重要性。当<strong>禁用层跳过</strong>时，加速收益有限（1.12倍）。实验表明，三个组件提供了互补的收益，联合启用时取得了最佳的速度-精度权衡。</p>
<p><img src="https://arxiv.org/html/2601.19634v1/x6.png" alt="消融研究"></p>
<blockquote>
<p><strong>图6</strong>：对AC^2-VLA各组件（缓存重用、令牌剪枝、层跳过）的消融研究结果。展示了分别禁用各组件时对成功率和加速比的影响，证实了各机制的必要性和互补性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）揭示了VLA模型的计算冗余与动作上下文而非视觉线索更相关，并提出了<strong>动作上下文感知的自适应计算</strong>框架；2）设计了<strong>AC^2-VLA</strong>，其核心是一个统一的动作先验路由器，能够自适应协调认知缓存、令牌剪枝和层跳过；3）提出了<strong>动作引导的自蒸馏训练方案</strong>，在实现结构化稀疏化的同时保持了原始策略的鲁棒性。</p>
<p>论文自身提到的局限性包括：路由器的训练需要额外的演示数据，且其设计依赖于特定的动作头类型（如扩散模型），可能需要进行调整以适应其他动作表示。</p>
<p>本工作对后续研究的启示在于：首先，<strong>动作上下文是机器人高效计算的关键信号</strong>，未来工作应更深入地探索如何利用具身智能的时序和状态信息。其次，<strong>统一路由框架</strong>有潜力扩展到其他效率维度或模型架构。最后，<strong>自蒸馏</strong>是保持高性能同时实现动态稀疏化的有效训练策略。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型在机器人操作中闭环部署时延迟高、计算成本大的问题，提出AC^2-VLA框架。其核心是通过动作上下文感知的自适应计算，联合决策跨时间步的认知重用、令牌剪枝和模型层选择性执行，并采用动作引导的自蒸馏训练策略。实验表明，该方法在保持任务成功率相当的同时，最高可实现1.79倍加速，并将计算量（FLOPs）降至基准模型的29.4%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.19634" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>