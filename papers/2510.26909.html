<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>NaviTrace: Evaluating Embodied Navigation of Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>NaviTrace: Evaluating Embodied Navigation of Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.26909" target="_blank" rel="noreferrer">2510.26909</a></span>
        <span>作者: Windecker, Tim, Patel, Manthan, Reuss, Moritz, Schwarzkopf, Richard, Cadena, Cesar, Lioutikov, Rudolf, Hutter, Marco, Frey, Jonas</span>
        <span>日期: 2025/10/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前评估视觉语言模型导航能力的主流方法主要有三类：在真实世界进行闭环测试、在仿真环境中进行测试、以及使用视觉问答基准。这些方法存在显著局限性：真实世界测试成本高昂且难以复现；仿真环境在动态、语义和场景多样性上过于简化；而现有的导航相关VQA基准通常仅输出文本答案而非路径级规划，且大多只评估人类或单一机器人形态的导航。本文针对现有基准在评估VLM跨形态导航能力方面的不足，提出了一个新的视角：通过一个基于单张真实世界图像、要求输出2D导航轨迹的VQA基准，来系统性地评估VLMs在不同具身形态下的导航策略。本文核心思路是构建一个高质量、多样化的基准（NaviTrace），并设计一个与人类偏好对齐的语义感知评分函数，用以量化评估VLMs预测的2D导航轨迹。</p>
<h2 id="方法详解">方法详解</h2>
<p>NaviTrace的整体框架是一个评估流程：输入为一个结构化提示，包含一张真实世界图像、一个语言指令以及一个指定的智能体形态；VLM需要输出一个以图像坐标序列表示的2D导航轨迹；该预测轨迹将与人工标注的真实轨迹进行比较，并通过一个新颖的语义感知评分函数计算出最终得分。</p>
<p><img src="https://arxiv.org/html/2510.26909v2/x1.png" alt="NaviTrace示例场景"></p>
<blockquote>
<p><strong>图1</strong>：NaviTrace基准示例。每个场景包含一张真实世界图像、一个语言指令，并为四种智能体形态（人类、腿式机器人、轮式机器人、自行车）提供专家标注的2D导航轨迹（地面真值）。</p>
</blockquote>
<p>该基准的核心构成如下：</p>
<ol>
<li><p><strong>数据构成</strong>：包含1000个独特场景，每个场景包括：(a) <strong>图像</strong>：来自消费设备或公开数据集（GrandTour）的第一人称真实图像，并进行了隐私匿名化处理。(b) <strong>任务指令</strong>：手动编写的、可仅从视觉信息解决的导航指令，强调不同形态的行为差异。(c) <strong>任务分类</strong>：每个场景被标记为一个或多个挑战类别，如几何/语义地形评估、可达性、可见性、社会规范、动态/静态障碍物规避等。(d) <strong>地面真值轨迹</strong>：定义为图像空间中的2D点序列，每个合适的形态至少有一条专家标注轨迹，若存在同等有效的替代路径则标注多条。(e) <strong>智能体形态</strong>：定义了四种形态以捕捉不同的导航行为：人类、腿式机器人（如ANYmal）、轮式机器人（小型送货机器人）和自行车。</p>
</li>
<li><p><strong>语义感知评分函数</strong>：这是评估方法的核心创新。评分函数旨在衡量预测轨迹与人类偏好的对齐程度，结合了三个因素：路径相似性、目标到达和安全性/相关性。公式定义为：<code>Score(T, G) = min_{T&#39;∈G} [DTW(T, T&#39;) + FDE(T, T&#39;)] + Penalty(T)</code>，其中T为预测轨迹，G为对应形态的所有地面真值轨迹集合。</p>
<ul>
<li>**轨迹相似性 (DTW)**：使用动态时间规整距离，以欧氏距离作为误差度量，来度量预测轨迹与最接近的地面真值轨迹之间的形状相似性。</li>
<li>**目标到达 (FDE)**：添加最终位移误差，即预测轨迹终点与地面真值轨迹终点之间的欧氏距离，以奖励正确到达目标。</li>
<li>**语义惩罚 (Penalty)**：引入形态特定的语义成本，惩罚轨迹穿过不希望进入的区域。使用在Mapillary Vistas上训练的Mask2Former模型推断语义分割掩码，并为每个语义类别根据形态e手动调优惩罚值<code>m_e(S_i)</code>。惩罚值沿预测轨迹进行像素级平均计算。为允许微小偏差，在地面真值周围设置了一个容忍带。</li>
<li><strong>分数缩放</strong>：为使分数易于解释，将其线性缩放至0到100之间，其中0分对应最差性能（实际为负值可能），100分对应一个简单的“直线前进”基线性能（3234.75）。缩放公式为：<code>Score_hat(T, G) = (3234.75 - Score(T, G)) / 3234.75 * 100</code>。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.26909v2/x3.png" alt="评分函数验证"></p>
<blockquote>
<p><strong>图3</strong>：左图：基于Mask2Former的自动语义惩罚掩码与人工分割掩码的对比。右图：通过计算评分排名与人类两两比较排名的斯皮尔曼相关系数，验证评分函数与人类偏好的一致性。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：1) 将导航评估从文本答案或仿真中的动作序列，扩展为在真实图像上预测可跨模型架构使用的2D轨迹；2) 首次系统性地考虑了多种具身形态的导航差异；3) 设计了一个结合几何对齐、目标到达和语义安全性的综合评分指标，并通过实验验证其与人类判断的高度相关性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了NaviTrace基准的1000个场景（均匀分为验证集和测试集），在测试集上评估模型性能。对比的基线方法包括：<strong>Human</strong>（人类专家性能上限）、<strong>Straight Forward</strong>（在图像中心画垂直线）、<strong>Oracle-Goal Straight Line</strong>（给定起点和终点画直线）、<strong>Only predict goal point</strong>（用Gemini 2.5 Pro仅预测目标点后连直线）、<strong>Only predict path</strong>（给定起点和终点，用Gemini 2.5 Pro仅预测路径形状）。评估了八种SOTA VLM：Gemini 2.5 Pro, GPT-5, o3, Claude Sonnet 4, Qwen 2.5 VL 72B, Qwen 3 VL 235B A22B Thinking, Mistral Medium 3.1, 和 Gemma 3 27B。</p>
<p><img src="https://arxiv.org/html/2510.26909v2/x4.png" alt="模型性能排名与分类表现"></p>
<blockquote>
<p><strong>图4</strong>：左图：各VLM、无信息基线（直线前进）和人类专家在不同智能体形态下的性能排名（分数越高越好）。右图：相同模型在不同任务类别下的性能表现。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ol>
<li><strong>性能差距显著</strong>：人类专家得分（75.40）远高于所有VLM，表明模型能力与任务难度之间存在巨大差距。在模型中，Gemini 2.5 Pro表现最佳，得分34.38，其次是GPT-5、Qwen 3 VL和o3。值得注意的是，简单的“直线前进”基线性能意外地接近o3。</li>
<li><strong>目标定位是主要瓶颈</strong>：分解实验表明，仅用Gemini 2.5 Pro预测目标点并连直线（得分29.65）的结果，与预测完整轨迹（34.38）相差不大，但两者都远低于已知目标点的Oracle基线（51.89）。这表明定位目标区域已是主要挑战。即使给定目标点，仅预测路径形状的Gemini 2.5 Pro（56.55）仍不及人类专家（75.40），说明路径规划本身也存在困难。</li>
<li><strong>形态与任务类别间差异小</strong>：所有模型在不同智能体形态间的性能差异很小，在不同任务类别上的表现也较为均匀。论文指出，这并非模型能力平衡，而是其整体性能较弱，掩盖了潜在的特定差异。</li>
<li><strong>评分函数有效性</strong>：消融实验（对应论文表II）验证了评分函数各组件贡献：DTW优于RMSE和弗雷歇距离；加入FDE（终点误差）带来一致提升；加入基于Mask2Former的语义惩罚后性能进一步增益；使用人工分割的惩罚仅带来有限的额外提升，证明了自动语义惩罚策略的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.26909v2/x5.png" alt="定性预测示例"></p>
<blockquote>
<p><strong>图5</strong>：模型Gemini 2.5 Pro、GPT-5、Qwen 3 VL和o3的预测示例。展示了模型在复杂场景下的轨迹输出。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.26909v2/x2.png" alt="推理与预测不匹配示例"></p>
<blockquote>
<p><strong>图6</strong>：o3模型推理过程示例。右侧文字推理正确识别了路径选项并选择了正确方案，但左侧预测的轨迹（粉色）未能与推理对齐，揭示了语言推理与空间 grounding 之间的脱节。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了<strong>NaviTrace</strong>，一个用于评估VLMs跨形态导航能力的新型高质量VQA基准，包含1000个多样化的真实世界场景和四种智能体形态；2) 设计了一种<strong>语义感知的轨迹评分函数</strong>，该函数结合了DTW、终点误差和基于自动语义分割的形态条件惩罚，并被验证与人类偏好高度相关；3) 对当前八种SOTA VLM进行了<strong>全面评估</strong>，揭示了其在导航任务上距离人类性能的巨大差距，并指出目标定位是主要失败模式。</p>
<p>论文自身提到的局限性包括：数据集地理分布上集中于瑞士，可能影响对其他地区的泛化能力；基准仅限于单图像场景，无法评估动态环境所需的时间推理和多步规划；当前智能体形态选择有限，未包含空中无人机等；评分函数依赖于预定义的语义类别和手动调优的惩罚值。</p>
<p>本研究对后续工作的启示在于：为了提升VLMs的具身导航能力，未来的研究需要着重改善模型的空间 grounding 能力和目标定位精度；同时，需要开发能够更好地将高级语言推理与低级空间预测对齐的模型架构或训练方法。NaviTrace为这一方向的进展提供了一个可扩展、可复现的评估平台。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型在具身导航中评估成本高、模拟简化、基准有限的问题，提出了**NaviTrace**基准。该基准包含1000个场景与3000余条专家轨迹，要求模型根据指令和具身类型（人/腿式/轮式机器人/自行车）在图像空间中输出2D导航轨迹。评估采用**语义感知轨迹评分**，综合动态时间规整距离、目标端点误差及基于像素语义的具身惩罚，并与人类偏好相关联。实验对8个先进VLM进行系统评估，发现其在**空间定位与目标定位**方面与人类性能存在显著差距。NaviTrace为真实世界机器人导航提供了一个可扩展、可复现的评估基准。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.26909" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>