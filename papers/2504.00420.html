<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.00420" target="_blank" rel="noreferrer">2504.00420</a></span>
        <span>作者: Yao, Yuanqi, Liu, Siao, Song, Haoming, Qu, Delin, Chen, Qizhi, Ding, Yan, Zhao, Bin, Wang, Zhigang, Li, Xuelong, Wang, Dong</span>
        <span>日期: 2025/04/01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在通用机器人领域，构建能够有效利用先验知识以持续学习新技能的终身学习机器人是一个长期目标。现有方法主要围绕缓解灾难性遗忘问题展开，例如经验回放方法显式利用先前数据，但面临内存限制和隐私问题；正则化或动态架构方法旨在学习新任务时更好地保留现有知识，但在扩展到复杂的基于视觉的操作任务时性能不佳。近期，TAIL等方法探索了为每个任务使用LoRA等技术来防止任务间干扰。然而，这些现有方法的共同局限在于未能有效利用技能之间的共享知识，限制了实现真正终身学习的能力。如图1所示，语义上不同的技能（如“抓香蕉”和“放锅”）可能共享共同的底层运动原始知识。本文针对这一具体痛点，提出了从“原始知识”这一新视角来促进终身学习。核心思路是：通过一个两阶段学习框架，首先在多技能预训练阶段学习一组可重用的“原始提示”来表示共享的原始知识；然后在终身学习阶段，通过冻结预训练的原始提示并连接和优化新的“终身提示”，实现从旧技能到新技能的知识迁移。</p>
<p><img src="https://arxiv.org/html/2504.00420v2/x1.png" alt="光学流图"></p>
<blockquote>
<p><strong>图1</strong>：光流捕获原始级运动模式，揭示了语义相似技能(a, b)和不同技能(c, d)之间潜在的共享知识。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.00420v2/x2.png" alt="原始概念图"></p>
<blockquote>
<p><strong>图2</strong>：原始概念示意图。以MimicGen中的“清理杯子”任务为例，该任务由“打开抽屉”、“抓取杯子”和“放置杯子”三个子任务组成。底行提供了原始轨迹的示意图，概念化了状态子序列。</p>
</blockquote>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的原始提示学习（PPL）是一个两阶段框架，旨在通过可重用和可扩展的原始提示实现终身机器人操作。</p>
<p><img src="https://arxiv.org/html/2504.00420v2/x3.png" alt="方法总览图"></p>
<blockquote>
<p><strong>图3</strong>：原始提示学习（PPL）总览图。在预训练阶段，输入包括本体感知、图像观察和语言指令。通过运动感知查询模块查询一组原始提示，获得加权求和后的提示P，并将其预置到基于扩散变换器的策略的每一层多头自注意力层中。对于带有专家演示的新技能获取，新的终身提示被连接并与冻结的预训练提示一起优化，遵循与预训练相同的输入/输出流程。<code>fire → ice</code> 符号表示提示在预训练阶段被优化，然后在终身学习阶段被冻结。</p>
</blockquote>
<p><strong>整体流程</strong>：给定输入演示流<code>{D_i}</code>和指令<code>T</code>，目标是学习一组可重用和可扩展的原始提示。流程分为两个阶段：1) <strong>多技能预训练阶段</strong>：学习原始提示以表示不同技能间的共享原始知识。2) <strong>终身学习阶段</strong>：冻结预训练好的原始提示，为每个新任务连接并优化新的“终身提示”，实现知识迁移。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>多技能预训练与提示集成</strong>：在基于扩散变换器的策略中，将提示<code>p</code>（维度为<code>L_p × D</code>）应用于每个多头自注意力（MSA）层。具体地，将<code>p</code>分割为<code>{p^K, p^V}</code>，并使用前缀提示方法将其预置到键<code>h^K</code>和值<code>h^V</code>中，即<code>MSA(h_Q, [p^K; h_K], [p^V; h_V])</code>。这使策略能够通过提示调节其行为。</p>
</li>
<li><p><strong>运动感知提示（Motion-Aware Prompting, MAP）</strong>：这是PPL的核心创新点，旨在同时捕获跨技能的语义和运动共享原始知识。传统提示学习方法严重依赖高级语义表示，难以促进非语义相似任务间的相互改进。MAP通过结合光流和任务条件语义信息来解决此问题。</p>
<ul>
<li><strong>光流提取</strong>：使用RAFT模型从连续图像帧中计算光流<code>F</code>，其提供了场景内运动动态的丰富表示，捕获原始动作的基本运动学特性，并对外观变化具有一定不变性。</li>
<li><strong>语义嵌入</strong>：使用预训练的CLIP模型对任务描述<code>T</code>进行编码，获得语义嵌入<code>E_CLIP(T)</code>。</li>
<li><strong>提示查询</strong>：MAP函数<code>f_prompt</code>将光流特征<code>Φ(F)</code>和CLIP语义嵌入<code>E_CLIP(T)</code>结合，形成一个综合的查询：<code>MAP(T, F) = f_prompt(E_CLIP(T), Φ(F))</code>。这个查询用于后续从原始提示库中检索和组合相关的提示。</li>
</ul>
</li>
<li><p><strong>终身技能获取机制</strong>：</p>
<ul>
<li><strong>提示组件库</strong>：维护一组可学习的提示组件<code>P ∈ ℝ^{M×D}</code>，每个组件关联着注意力向量<code>A</code>和键<code>K</code>。</li>
<li><strong>动态提示组合</strong>：对于给定任务，首先通过<code>Atten_Query = MAP(T, F) ⊙ A</code>计算注意力查询。然后，计算此查询与每个提示组件键<code>K_m</code>的余弦相似度作为权重<code>α_m</code>。最终的提示<code>p</code>是所有权重<code>α_m</code>与对应提示组件<code>P_m</code>的加权和：<code>p = Σ_m α_m P_m</code>。</li>
<li><strong>终身扩展</strong>：在终身学习阶段，<strong>冻结</strong>预训练阶段学得的原始提示组件（对应旧技能）。当学习第<code>t</code>个新任务时，<strong>扩展</strong>提示组件库<code>P</code>、键<code>K</code>和注意力向量<code>A</code>的维度（例如从<code>M</code>扩展到<code>Z</code>），并<strong>仅优化</strong>为这个新任务新增的“终身提示”组件对应的参数。在计算权重<code>α</code>时，会考虑所有（冻结的旧提示和可优化的新提示）组件，从而自然地实现了从旧知识到新任务的知识迁移，而无需访问旧数据。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法（如Dual-Prompt, Coda-Prompt, TAIL）相比，PPL的主要创新在于：1) 引入了<strong>运动感知提示</strong>，通过光流显式建模低层运动模式，能够发现并利用语义不同但运动相似的技能间的共享原始知识；2) 提出了一个<strong>两阶段的提示学习与扩展框架</strong>，通过冻结的原始提示和可扩展的终身提示，实现了高效、可扩展的终身学习，并促进了正向知识迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：构建了一个大规模技能数据集用于评估，并在模拟和真实世界任务上进行了实验。</li>
<li><strong>基准方法</strong>：对比了多种终身学习方法，包括基于经验回放的方法（ER、DER、EWC）、基于动态架构/参数的方法（TAIL、Dual-Prompt、Coda-Prompt）以及一些变体（PPL w/o flow, PPL w/o lifelong prompt等）。</li>
<li><strong>评估指标</strong>：主要使用任务成功率。</li>
</ul>
<p><img src="https://arxiv.org/html/2504.00420v2/x4.png" alt="模拟实验成功率"></p>
<blockquote>
<p><strong>图4</strong>：在模拟环境中的终身学习性能比较。PPL在大多数任务序列上取得了最高的平均成功率，显著优于所有基线方法。例如，在10个任务的序列中，PPL的平均成功率达到约68%，而表现次佳的TAIL约为58%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.00420v2/x5.png" alt="真实世界实验成功率"></p>
<blockquote>
<p><strong>图5</strong>：在真实世界机器人操作任务上的终身学习性能。PPL在7个任务的序列中取得了最佳的整体性能，成功率超过80%，展示了其从模拟到真实世界的有效迁移能力。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟实验</strong>：如图4所示，PPL在多个不同任务序列长度的终身学习设置中，均取得了最高的平均成功率。在10个任务的序列上，PPL的平均成功率（约68%）显著优于最佳基线TAIL（约58%）。</li>
<li><strong>真实世界实验</strong>：如图5所示，在7个真实机器人操作任务的序列中，PPL取得了超过80%的成功率，性能最佳，证明了其从模拟到真实世界的有效迁移和实际应用能力。</li>
<li><strong>消融实验</strong>：<ul>
<li><strong>组件贡献</strong>：如图6所示，移除了运动感知提示（w/o flow）或终身提示机制（w/o lifelong prompt）都会导致性能显著下降，证明了这两个核心组件的必要性。完整的PPL配置性能最优。</li>
<li><strong>知识迁移分析</strong>：如图7所示，与基线相比，PPL在大多数新任务上表现出正向知识迁移（性能提升），而基线方法常出现负迁移。这得益于PPL通过原始提示有效地重用和组合了先前学到的知识。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2504.00420v2/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：PPL的消融研究结果。移除光流（w/o flow）或终身提示（w/o lifelong prompt）都会导致性能下降，验证了这两个组件的有效性。使用随机初始化的提示（w/ random prompt）性能最差。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.00420v2/x7.png" alt="知识迁移分析"></p>
<blockquote>
<p><strong>图7</strong>：知识迁移分析。热图显示了学习新任务（行）时，相对于从头开始学习，从旧任务（列）转移知识带来的性能变化。PPL（右）在大多数情况下显示出正向迁移（绿色），而基线TAIL（左）则表现出更多的负迁移（红色）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.00420v2/x8.png" alt="定性结果"></p>
<blockquote>
<p><strong>图8</strong>：PPL在真实世界机器人任务上的定性结果。展示了机器人成功执行一系列学习到的技能，包括“打开微波炉”、“拿起水壶”、“放置香蕉”等。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>原始提示学习（PPL）</strong>，一种新颖的、专为终身机器人操作设计的两阶段框架，通过可重用和可扩展的原始提示来实现知识迁移。</li>
<li>设计了<strong>运动感知提示</strong>，通过结合光流和文本查询，能够有效表示技能间的语义和运动共享原始知识，从而促进更广泛、更有效的知识迁移。</li>
<li>构建了大规模技能数据集并进行了广泛的模拟与真实世界实验，证明了PPL在终身机器人操作任务上显著优于现有先进方法。</li>
</ol>
<p><strong>局限性</strong>：论文自身提及的局限性包括：1) 当前方法依赖于专家演示进行模仿学习；2) 在终身学习阶段，虽然避免了存储旧数据，但提示组件库的大小会随着任务数量线性增长，尽管其增长远小于模型参数总量。</p>
<p><strong>启示</strong>：PPL为机器人终身学习提供了一个新思路，即显式地建模和重用低层的运动“原始知识”，而非仅仅依赖高层语义或任务标识。这对后续研究的启示包括：探索更高效的提示压缩或选择机制以控制参数增长；将原始提示学习与强化学习或在线学习结合，减少对专家演示的依赖；进一步研究跨模态（如触觉）的原始知识表示与迁移。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Primitive Prompt Learning (PPL)方法，旨在解决机器人终身学习中灾难性遗忘与知识迁移效率低下的核心问题。其关键技术是通过两阶段学习：首先预训练一组“原始提示”来捕获跨不同技能的语义与运动共享基元；然后在学习新技能时，通过冻结旧提示并优化新提示实现高效知识迁移。实验在模拟与真实任务中验证了PPL的优越性能，显著优于现有方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.00420" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>