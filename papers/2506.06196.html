<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.06196" target="_blank" rel="noreferrer">2506.06196</a></span>
        <span>作者: Tingnan Zhang Team</span>
        <span>日期: 2025-06-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大规模预训练机器人模型通过利用大规模预训练数据集，在提升机器人泛化能力方面取得了显著进展。然而，这些模型在适应轻微的场景变化（如不同的空间位置、未见过的物体、不同的光照条件）时仍面临挑战。一种日益流行的方法是显式地建立机器人策略与物理世界抽象模式和关系之间更深层次的联系，例如视觉-语言-动作模型（VLAs）尝试通过机器人数据微调视觉-语言模型来获益，或者使用语言指令、关键点、轨迹等显式中层表示来为策略提供额外的基础。尽管这些方法取得了成功，但这些额外的基础所带来的泛化属性本质上仍然是高层次的，它们对简单任务的益处不一定能迁移到需要更高灵巧性或物体交互的更复杂任务中。</p>
<p>本文假设中层表示的选择高度依赖于任务特定的需求。例如，对于叠衬衫的任务，边界框可能有助于定位衬衫的大致位置，但无法提供如何操作它的可执行信息。类似地，深度感知的表示对于交接物体等接触丰富的任务至关重要，但对于头顶取放任务可能不那么重要。为了赋予模型在各种灵巧任务上更强的泛化能力，必须探索能够平衡高层抽象和低层可执行细节的表示。这些表示不仅需要编码空间和几何推理，还需要在不同动态环境中提供适应性。本文的核心思路是：系统地研究一组空间基础的中层表示，并提出一种新颖的混合专家（MoE）策略架构来整合这些表示，从而提升策略的性能和泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为Mid-Level MoE，其整体框架基于扩散策略。策略输入为来自不同视角的4张图像（2张第三人称视角和2张手腕视角），直接输出12个绝对关节位置（每只手臂6个）以及每个夹持器的连续开合值。网络首先通过独立的ResNet50编码器处理每张图像，然后通过Transformer编码器获得图像嵌入。同时，状态信息被输入到多个专门的中层专家模块中，这些专家生成的表示通过注意力机制进行整合，并与图像嵌入进行交叉注意力交互，最终由一个Transformer解码器进行去噪并预测动作。</p>
<p><img src="https://arxiv.org/html/2506.06196v1/extracted/6519669/images/Architecturev4.png" alt="方法架构"></p>
<blockquote>
<p><strong>图3</strong>：策略架构。四张图像输入Transformer编码器。同时，每张图像被输入到各个独立的中层专家模块。专家生成的表示嵌入通过交叉注意力传递给Transformer解码器。</p>
</blockquote>
<p>核心模块是四个经过专门训练、参数冻结的中层专家，分别对应四个空间基础维度：</p>
<ol>
<li><strong>物体中心专家</strong>：基于离线的OWL-ViT模型，关注场景中每个物体的位置和几何信息（如边界框）。</li>
<li><strong>运动中心专家</strong>：利用本体感知数据（手臂位置和运动传感器数据），关注机器人或物体随时间推移的运动动态（如未来轨迹点）。</li>
<li><strong>姿态感知专家</strong>：在边界框基础上结合本体感知，处理手臂与物体接触时的状态，编码物体和机器人的相对与绝对空间朝向。</li>
<li><strong>深度感知专家</strong>：利用深度信息推断环境中的空间关系和物理约束（如深度感知轨迹）。</li>
</ol>
<p>与现有方法相比，本文的创新点具体体现在架构设计和训练策略上：</p>
<ul>
<li><strong>混合专家与注意力门控</strong>：为了平衡对中层表示的<strong>敏感性</strong>（策略遵循表示的程度）与对表示中噪声的<strong>鲁棒性</strong>，作者设计了注意力门控机制。多个专家的表示首先通过多头注意力进行动态加权整合，形成统一的表示嵌入 <code>z</code>。然后，<code>z</code> 与图像嵌入进行交叉注意力，使策略能够根据当前视觉上下文动态决定强调哪些表示，从而在利用详细信息的同时抵抗表示错误。</li>
<li><strong>自一致性的加权模仿学习</strong>：为了进一步提升策略执行中层表示的精度，作者引入了<strong>自一致性</strong>概念。其核心思想是，在演示数据中，如果机器人的实际动作轨迹与其中层表示预测的“计划”高度一致，则该数据样本更可靠。因此，在行为克隆损失中，为每个训练样本 <code>(x, a)</code> 分配一个权重 <code>w(x)</code>，该权重反映了状态 <code>x</code> 下中层表示与演示动作 <code>a</code> 之间的一致性程度。一致性高的样本获得更高权重，从而引导策略更精确地遵循可靠的专家指导。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.06196v1/extracted/6519669/images/Figure3v4.png" alt="自一致性"></p>
<blockquote>
<p><strong>图4</strong>：自一致性概念。左图，机器人实际轨迹与其中层表示不匹配，导致权重较低。右图，机器人遵循其表示，导致权重较高。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在模拟环境和真实世界机器人平台上进行了评估。模拟实验使用了6个双手灵巧操作任务（如图5所示），包括打开微波炉、从架子取物、双手协调操作等。真实世界实验则包含了4个任务。对比的基线方法包括：1) <strong>标准扩散策略</strong>（仅以图像和状态为输入）；2) <strong>语言基础基线</strong>（使用大型语言模型生成的密集语言描述作为额外条件）。</p>
<p><img src="https://arxiv.org/html/2506.06196v1/extracted/6519669/images/sim_tasks.png" alt="模拟任务"></p>
<blockquote>
<p><strong>图5</strong>：模拟环境中评估的6个双手灵巧操作任务。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ol>
<li><strong>主要性能对比</strong>：在模拟任务中，Mid-Level MoE方法比标准扩散策略基线平均成功率高出**24%<strong>，比语言基础基线高出</strong>11%<strong>。在真实世界任务中，Mid-Level MoE比标准扩散策略基线平均成功率高出</strong>20%<strong>，比语言基础基线高出</strong>14%**。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.06196v1/extracted/6519669/images/main_plot_sim_v2.png" alt="模拟结果主图"></p>
<blockquote>
<p><strong>图10</strong>：模拟环境中各方法在6个任务上的成功率对比。Mid-Level MoE（橙色）显著优于标准扩散策略（蓝色）和语言基础基线（绿色）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.06196v1/extracted/6519669/images/main_plot_real_v2.png" alt="真实世界结果主图"></p>
<blockquote>
<p><strong>图11</strong>：真实世界环境中各方法的成功率对比。Mid-Level MoE（橙色）同样表现最佳。</p>
</blockquote>
<ol start="2">
<li><strong>加权模仿学习（自一致性）的效果</strong>：引入基于自一致性的加权模仿学习训练后，策略性能得到进一步提升。在模拟任务中，这带来了额外的<strong>10%</strong> 平均成功率提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.06196v1/extracted/6519669/images/plot_self_consistency_v2.png" alt="加权模仿学习效果"></p>
<blockquote>
<p><strong>图8</strong>：使用自一致性加权训练（红色）相比标准训练（蓝色）在6个模拟任务上带来的额外成功率提升。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验与组件贡献</strong>：<ul>
<li><strong>专家组合消融</strong>：实验表明，结合所有四种专家（物体、运动、姿态、深度）的性能最佳。移除任何一类专家都会导致性能下降，其中深度感知和姿态感知专家对接触丰富的任务（如“打开微波炉”）贡献最大。</li>
<li><strong>架构消融</strong>：移除注意力门控和交叉注意力机制会导致性能显著下降，验证了这些设计对于平衡敏感性与鲁棒性的必要性。</li>
<li><strong>零样本泛化</strong>：在模拟环境中对未见过的物体颜色和纹理进行测试，Mid-Level MoE展现了比基线方法更好的零样本泛化能力。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.06196v1/extracted/6519669/images/zero-shot.png" alt="零样本泛化"></p>
<blockquote>
<p><strong>图12</strong>：在物体颜色和纹理零样本泛化设置下的成功率。Mid-Level MoE（橙色）的泛化性能优于基线。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>系统化研究了空间基础的中层表示</strong>：明确了物体中心性、运动中心性、姿态感知和深度感知四个关键维度，并实证分析了它们对不同灵巧任务的差异化价值。</li>
<li><strong>提出了新颖的Mid-Level MoE架构</strong>：通过混合专家模型整合多种表示，并利用注意力门控和交叉注意力机制，巧妙地平衡了策略对中层表示的敏感性与对噪声的鲁棒性。</li>
<li><strong>引入了自一致性的加权模仿学习</strong>：将演示数据与其中层表示的一致性作为权重信号，引导策略更精确地执行专家表示的计划，从而进一步提升性能。</li>
</ol>
<p>论文自身提到的局限性包括：专家模块需要监督训练，可能成本较高；并且方法依赖于相对准确的感知模块（如OWL-ViT）来生成初始表示。</p>
<p>本文的研究对后续工作的启示在于：为机器人学习引入结构化、可解释的中间表示是提升泛化能力的有效途径。未来的研究可以探索如何以更自监督或弱监督的方式学习这些中层表示，或者研究更动态的专家选择与组合机制，以适应更广泛和复杂的任务范围。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人灵巧操作任务中策略泛化能力不足的问题，研究如何利用空间接地的中层表示（包括对象中心性、姿态感知和深度感知）连接感知与动作。通过监督学习训练专家编码器，将其输入扩散策略，并设计混合专家策略架构整合多个专家模型以提升泛化。实验表明，该方法在评估任务中相比语言基线平均成功率提高11%，较标准扩散策略提高24%；使用中层表示作为加权模仿学习的监督信号可进一步带来10%的性能提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.06196" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>