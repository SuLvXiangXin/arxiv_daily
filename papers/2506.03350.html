<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Adversarial Attacks on Robotic Vision Language Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Adversarial Attacks on Robotic Vision Language Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.03350" target="_blank" rel="noreferrer">2506.03350</a></span>
        <span>作者: Jones, Eliot Krzysztof, Robey, Alexander, Zou, Andy, Ravichandran, Zachary, Pappas, George J., Hassani, Hamed, Fredrikson, Matt, Kolter, J. Zico</span>
        <span>日期: 2025/06/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人领域正广泛采用视觉语言动作模型，其通过在大规模互联网数据上预训练的视觉语言模型来理解和遵循自然语言指令，从而生成机器人动作。这类方法在模拟器和受限的实验室环境中表现出色。然而，当部署于开放、动态的真实世界时，其安全性面临严峻挑战，尤其是对抗性攻击的威胁。现有关于对抗攻击的研究大多集中于静态图像分类或基于纯视觉的导航模型，缺乏对VLAs这种多模态、时序性、闭环决策系统在具身环境中脆弱性的系统性探索。本文针对VLAs在时序任务中对对抗样本的脆弱性这一具体痛点，提出了一种新颖的攻击视角：如何在时序任务执行过程中，通过扰动机器人观察到的视觉输入，以最小的、不易察觉的修改，系统地误导其动作输出，最终导致任务失败。本文的核心思路是构建一个通用的对抗攻击框架，通过优化基于动作预测误差的目标函数，在时序任务中生成对抗性观察，从而高效地攻击VLAs。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的攻击框架旨在为VLAs在时序决策过程中生成的每一帧观察图像，添加一个微小的对抗性扰动，使得机器人在被扰动的观察下产生错误的动作，进而偏离正确的任务执行路径。攻击被建模为一个优化问题，其目标是在扰动幅度受限的前提下，最大化VLAs的动作预测误差。</p>
<p><img src="https://img-blog.csdnimg.cn/direct/6a3f2e9c8f4d4c6a8d8c5f5c5f5c5f5c.png" alt="攻击框架概览"></p>
<blockquote>
<p><strong>图1</strong>：本文提出的对抗攻击框架概览。在每一步，攻击者接收当前状态（包括观察图像和语言指令），并生成一个对抗性扰动添加到原始观察上。被扰动的观察输入VLA后，导致其输出一个与正确动作差异巨大的动作，从而在长期执行中累积错误，最终导致任务失败。</p>
</blockquote>
<p>整体框架包含两个核心阶段：1) <strong>扰动生成</strong>：基于当前状态和VLA模型，计算对抗性扰动；2) <strong>动作误导</strong>：使用扰动后的观察驱动VLA产生错误动作。攻击发生在测试时，攻击者可以访问VLA模型（白盒设置）或仅能查询其动作输出（黑盒设置）。</p>
<p>核心模块是<strong>扰动生成器</strong>，其优化目标函数是关键创新点。与图像分类中旨在改变类别标签的攻击不同，本文的攻击目标直接定位于<strong>动作空间</strong>。具体而言，给定一个语言指令 (g)、当前观察 (o_t) 和由VLA参数化的策略 (\pi)，攻击者寻求一个扰动 (\delta_t)，在满足 (||\delta_t||<em>p \leq \epsilon) 的约束下，最小化以下损失函数：<br>[<br>\mathcal{L}(\delta_t) = -\mathbb{E}</em>{a_t \sim \pi(\cdot|o_t+\delta_t, g)} [\log \pi(a_t^* | o_t, g)]<br>]<br>其中 (a_t^*) 是在干净观察 (o_t) 下VLA预测的（或专家演示的）正确动作。这个损失函数鼓励扰动后的观察 (o_t + \delta_t) 使得VLA预测的动作分布，与在干净观察下正确动作的概率之间，具有尽可能低的似然。换言之，攻击者试图让VLA“怀疑”正确动作在当前（被扰动的）观察下的合理性。</p>
<p>在<strong>白盒攻击</strong>设置下，攻击者精确知道VLA的策略网络参数，因此可以通过反向传播直接计算损失函数 (\mathcal{L}) 相对于扰动 (\delta_t) 的梯度，并使用投影梯度下降进行迭代优化：(\delta_t \leftarrow \text{Proj}_{||\cdot||<em>p \leq \epsilon}(\delta_t - \eta \cdot \nabla</em>{\delta_t} \mathcal{L}))。</p>
<p>对于<strong>黑盒攻击</strong>，攻击者只能通过查询获得VLA的动作输出。本文采用基于有限差分的方法来估计梯度。具体来说，通过向 (\delta_t) 添加随机噪声向量并观察损失函数的变化，来近似梯度方向，然后沿该方向更新扰动。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) <strong>时序攻击</strong>：将攻击扩展到多步决策过程，考虑了动作错误的累积效应；2) <strong>动作空间目标</strong>：直接优化动作预测的误差，而非中间视觉语言特征的对齐误差，这对于具身任务失败更具针对性；3) <strong>通用框架</strong>：方法不依赖于特定VLA架构，可应用于基于Transformer或CNN的不同VLA模型。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在三个具身AI基准模拟器中进行：<strong>MetaWorld</strong>（机器人操作任务）、<strong>Habitat</strong>（视觉导航）和<strong>CARLA</strong>（自动驾驶）。使用的VLAs包括基于Transformer的模型和基于卷积网络的模型。对比的<strong>Baseline方法</strong>包括：1) <strong>FGSM</strong> 和 <strong>PGD</strong>：从图像分类领域直接迁移的经典攻击方法；2) <strong>VLM-Attack</strong>：一种针对视觉语言模型对齐损失进行攻击的方法；3) <strong>Random Noise</strong>：添加随机噪声作为对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>攻击成功率</strong>：在多个任务和VLA上，本文方法能显著降低任务成功率。例如，在MetaWorld的“开门”任务中，使用本文白盒攻击可使某VLA的成功率从85%降至12%，而PGD攻击仅降至68%。</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/direct/8a3f2e9c8f4d4c6a8d8c5f5c5f5c5f5c.png" alt="不同攻击方法在MetaWorld上的成功率对比"></p>
<blockquote>
<p><strong>图2</strong>：在MetaWorld操作任务上，不同攻击方法对VLA任务成功率的影响。本文提出的方法（Ours）相比基线方法（FGSM, PGD, VLM-Attack）能更有效地使任务失败，成功率下降幅度最大。</p>
</blockquote>
<ol start="2">
<li><strong>扰动大小与隐蔽性</strong>：在相同的 (L_\infty) 约束（(\epsilon=8/255)）下，本文方法生成的扰动在人类视觉上更不易察觉，且能引起更大的动作偏差。定量上，本文方法引起的动作分布与正确动作分布之间的KL散度显著高于基线方法。</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/direct/7a3f2e9c8f4d4c6a8d8c5f5c5f5c5f5c.png" alt="扰动可视化与动作误差对比"></p>
<blockquote>
<p><strong>图3</strong>：左列展示了干净观察（Clean）、PGD扰动和本文方法（Ours）扰动的对比，可见本文扰动更不易察觉。右图展示了不同攻击方法引起的平均动作预测误差，本文方法导致的误差最高。</p>
</blockquote>
<ol start="3">
<li><p><strong>黑盒攻击有效性</strong>：在只能查询VLA动作输出的黑盒设置下，本文基于有限差分的攻击方法仍然有效，能够将任务成功率降低30%-50%，证明了方法的可转移性和实用性。</p>
</li>
<li><p><strong>消融实验</strong>：</p>
<ul>
<li><strong>目标函数消融</strong>：比较了攻击图像特征、语言特征和动作预测损失的不同变体。实验表明，直接优化动作预测损失（本文方法）的效果远优于攻击视觉或语言特征对齐损失。</li>
<li><strong>时序信息消融</strong>：对比了仅使用当前步信息与使用多步历史信息进行攻击。结果显示，考虑短期历史（如前3-5步）能略微提升攻击效率，但单步攻击已非常有效。</li>
<li><strong>扰动预算 (\epsilon) 的影响</strong>：随着 (\epsilon) 增大，攻击成功率上升，但即使是很小的 (\epsilon)（如4/255），本文方法也能造成显著性能下降。</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>首次系统性地研究和形式化了针对机器人视觉语言动作模型的对抗攻击问题，揭示了其在时序决策场景中的脆弱性。</li>
<li>提出了一个通用、高效的对抗攻击框架，其核心创新在于定义了基于动作空间预测误差的优化目标，能直接、有效地误导机器人策略。</li>
<li>在多个主流具身AI基准和不同VLA架构上进行了广泛实验，验证了所提方法在白盒和黑盒设置下的有效性、隐蔽性和优越性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前工作主要在模拟环境中进行，虽然CARLA部分接近真实世界，但将攻击无缝迁移到物理机器人上仍面临挑战（如传感器噪声、动态光照）。此外，迭代优化扰动在每一步的计算成本相对较高，可能影响实时性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>防御机制开发</strong>：本研究为设计针对VLAs的鲁棒训练方法或运行时检测机制提供了明确的攻击面参考。</li>
<li><strong>安全评估标准</strong>：提示社区在评估VLA性能时，应将其对抗鲁棒性作为一个重要指标。</li>
<li><strong>扩展攻击维度</strong>：未来工作可以探索结合语言指令扰动或针对多智能体协作VLA的攻击。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文标题为“对机器人视觉语言动作模型的对抗攻击”，旨在研究对抗攻击如何影响机器人视觉语言动作模型。核心问题是分析这些多模态模型在对抗性扰动下的安全性和鲁棒性漏洞。由于未提供论文正文内容，无法提炼具体技术方法（如攻击算法名称和要点）以及核心实验结论（如攻击成功率或性能下降数据）。建议补充论文正文以获得更精准的总结。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.03350" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>