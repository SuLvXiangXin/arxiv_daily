<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.03574" target="_blank" rel="noreferrer">2506.03574</a></span>
        <span>作者: Jian Tang Team</span>
        <span>日期: 2025-06-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型在将自然语言指令和视觉观察映射为机器人动作方面取得了显著进展。然而，主流方法通常假设任务意图在执行过程中是静态不变的，因此当新指令在执行中途下达时，它们难以做出响应。这一局限性阻碍了在动态环境（如零售或家庭环境）中实现自然、鲁棒的交互。一些近期工作利用大型语言或视觉语言模型进行高层任务规划和交互式意图理解，但通常缺乏低层执行的灵活性，且推理延迟高，难以实现高频的实时适应。另一些工作试图通过收集额外的演示数据来增强失败恢复和任务切换能力，但这种方法扩展性差。因此，核心挑战在于：如何仅利用现有的轨迹数据，无需高层规划或额外演示，赋予VLA模型在执行过程中平滑、反应式地切换任务的能力。本文提出SwitchVLA，将任务切换建模为一个状态条件化的行为调制问题，通过引入执行状态感知和条件化的行为策略，使模型能够根据指令变化和物理接触反馈，自适应地决定是继续执行、回滚还是推进到新任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>SwitchVLA的整体框架将任务切换视为一个条件行为预测问题，而非硬性的重新规划问题。其核心是通过两个新增的监督信号——接触状态和行为模式，来调制策略的行为。</p>
<p><img src="https://arxiv.org/html/2506.03574v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SwitchVLA整体框架概述。包含视觉-语言-接触（VLC）嵌入模块和条件执行专家，共同融合多模态输入以生成具有执行感知和条件控制能力的动作。</p>
</blockquote>
<p><strong>整体流程</strong>：在推理时，策略接收当前视觉观察 (o_t)、一对指令（先前指令 (l^{\text{pre}}) 和当前指令 (l^{\text{cur}})）以及先前传播的接触状态 (c^{\text{pre}})。在每个时间步，模型联合预测新的接触状态 (c_t)、行为模式 (b_t) 以及对应的动作块 (A_t)。执行完一个动作块后，更新 (l^{\text{pre}} \leftarrow l^{\text{cur}})，并将预测的 (c_t) 传播到下一步。新指令可在任意时刻下达，触发模型重新评估 (b_t) 并动态调整行为。</p>
<p><strong>核心模块1：视觉-语言-接触（VLC）嵌入模块</strong>。该模块负责将多视角RGB观察、接触感知的执行线索和成对的任务指令融合为统一的令牌序列。视觉编码器采用基于DaViT的主干网络。指令和接触聚合器将历史指令 (l^{\text{pre}})、历史接触状态 (c^{\text{pre}}) 以及当前指令 (l^{\text{cur}}) 进行嵌入（语言使用BART，接触使用MLP）并拼接。所有令牌通过Transformer编码器-解码器进行融合，生成富含时间和语义信息的嵌入。</p>
<p><strong>核心模块2：条件执行专家</strong>。这是一个结构化的动作解码器，集成了实时接触信号与高层行为意图。它在每个时间步同步预测三个输出：1) 接触状态 (c_t \in {0,1})；2) 行为模式 (b_t \in {0:\text{forward}, 1:\text{rollback}, 2:\text{advance}})；3) 动作块 (A_t = {a_{t+k}}_{k=1}^{K})，即未来K个时间步的低层动作序列。这种结构化设计便于进行接触感知的行为适应和平滑的时间过渡。</p>
<p><img src="https://arxiv.org/html/2506.03574v1/x3.png" alt="轨迹解析"></p>
<blockquote>
<p><strong>图3</strong>：使用预训练VLM（如GPT-4o）从轨迹数据中识别和标注指定事件的时间区间。例如，根据提示“机器人（夹爪）与物体接触”，模型检索并标注轨迹内的接触时间区间。</p>
</blockquote>
<p><strong>监督信号与训练</strong>：接触状态 (c_t) 是一个二元变量，表示机器人与物体是否发生物理接触，可通过触觉、夹爪开合信号或视觉语言解析获得。行为模式 (b_t) 定义了三种策略：继续执行、回滚或推进。训练时，使用标注了接触状态和行为模式的专家轨迹。如图4所示，针对三种行为设计了特定的监督：</p>
<ul>
<li><strong>forward</strong> ((b_t=0)): 在指令匹配 ((l^{\text{cur}}=l^i)) 时，预测未来的动作块 ({a^i_{t+1}, ..., a^i_{t+K}})。训练时随机采样不同的 (l^{\text{pre}}, c^{\text{pre}}) 以增强对先前上下文的鲁棒性。</li>
<li><strong>rollback</strong> ((b_t=1)): 当指令不匹配 ((l^{\text{cur}} \neq l^i)) 且处于接触状态 ((c^i_t=1)) 时，生成反向动作 ({a^i_{t-1}, ..., a^i_{t-K}})，鼓励模型通过物理反馈从语义不匹配中优雅恢复。</li>
<li><strong>advance</strong> ((b_t=2)): 当指令更新且无物理接触时，在当前动作 (a^i_t) 和新指令 (l^{\text{cur}}) 对应的规范起始姿态 (a_0^{\text{normal}}) 之间进行关节空间线性插值 (f(a^i_t, l^{\text{cur}}))，实现快速子任务切换。</li>
</ul>
<p>动作序列使用流匹配损失进行优化，接触状态和行为模式使用标准分类目标进行监督。</p>
<p><img src="https://arxiv.org/html/2506.03574v1/x4.png" alt="训练流程"></p>
<blockquote>
<p><strong>图4</strong>：forward、rollback和advance三种行为的训练流程示意图。通过基于预测行为模式的策略调制，实现动态任务转换。</p>
</blockquote>
<p><strong>创新点</strong>：与现有VLA模型（如OpenVLA、π₀）仅进行静态的指令到动作映射相比，SwitchVLA的关键创新在于引入了执行状态（接触）感知，并明确建模了用于任务切换的多种行为模式（forward, rollback, advance），使单一策略能够根据指令变化和物理交互反馈，自主决策并生成适应性的动作序列，而无需外部规划器或额外的切换专用数据。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真和真实机器人操控任务上进行了评估。仿真实验基于LIBERO平台，使用LIBERO-Goal套件，选取了8个任务进行多任务训练。评估分为配对任务切换（早期、中期、晚期切换）和长序列任务切换。基线方法包括π₀和OpenVLA-OFT。真实实验也进行了长序列任务评估，对比了MT-ACT、Diffusion Policy、π₀等方法。任务切换成功定义为：任务A进入指定执行阶段无失败，接收到新指令后行为符合预期（如回滚或推进），随后成功完成任务B。</p>
<p><strong>关键实验结果</strong>：<br>表1显示了仿真中单个任务和无切换成功率，以及配对切换实验的成功率。在无切换情况下，SwitchVLA（93.0%）与基线方法性能相当。在任务切换中，基线方法在早期切换成功率约40%，但在中期和晚期切换骤降至约10%。SwitchVLA则在所有阶段都表现稳健，中期切换成功率从基线的8.3%提升至50.9%，晚期切换从10.2%提升至68.7%，优势显著。</p>
<p><img src="https://arxiv.org/html/2506.03574v1/x5.png" alt="配对切换成功率"></p>
<blockquote>
<p><strong>图5</strong>：仿真中配对任务切换（早期、中期、晚期）的平均成功率对比图。SwitchVLA在所有切换阶段均大幅领先基线方法。</p>
</blockquote>
<p>表2展示了长序列任务切换的累积平均成功率。在仿真中，基线方法在序列长度超过2时成功率即为0%，而SwitchVLA在A→B时达到100%，即使到A→...→F（长度6）时仍有50%的成功率。在真实实验中，SwitchVLA同样显著优于所有基线方法，例如在A→B时成功率为95.6%，而所有基线均为0%。</p>
<p><img src="https://arxiv.org/html/2506.03574v1/x6.png" alt="长序列成功率"></p>
<blockquote>
<p><strong>图6</strong>：长序列任务切换的累积平均成功率对比图（仿真与真实实验）。SwitchVLA在长短序列中均能保持较高成功率，而基线方法在涉及切换时迅速失败。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.03574v1/x7.png" alt="定性结果"></p>
<blockquote>
<p><strong>图7</strong>：真实机器人任务中的定性结果示例。左：基线方法在指令中途变更时（“拿起柠檬放在盘子上”）产生振荡并掉落物品。右：SwitchVLA能够先回滚放下原物品（饼干盒），然后平滑地执行新指令拿起柠檬并放置到盘子上。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了SwitchVLA，一个统一的、执行感知的框架，通过新颖的训练范式和架构，在不依赖额外切换专用数据的情况下支持动态任务切换。</li>
<li>设计了一个多行为条件策略，能够在单一策略主干内平滑地继续、回滚或推进动作。</li>
<li>在仿真和真实机器人操控任务上进行了实证验证，证明SwitchVLA在任务转换平滑性、恢复有效性和指令遵循方面显著优于现有VLA基线。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法中轨迹解析和接触标注依赖于预训练的视觉语言模型（VLM）进行弱监督，这可能对泛化能力产生影响。</p>
<p><strong>启示</strong>：本工作表明，将执行状态（特别是物理接触）显式地建模为条件信号，对于实现动态、交互式的机器人任务适应至关重要。它为端到端VLA模型处理实时意图变化提供了一条新路径，即通过内部的行为调制而非外部重新规划，为实现更自然、鲁棒的人机交互奠定了基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在任务执行过程中无法响应动态意图变化的局限，提出SwitchVLA框架。核心方法是将任务切换建模为基于执行状态和指令上下文的行为调制问题，通过分割专家演示为接触阶段来推断任务进度，并训练一个多行为条件策略以生成灵活的动作块。实验表明，该框架在仿真和真实机器人操作中，于任务成功率和交互自然性上均优于现有基线，实现了鲁棒的指令遵循与流畅的任务切换。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.03574" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>