<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.05855" target="_blank" rel="noreferrer">2511.05855</a></span>
        <span>作者: Renjing Xu Team</span>
        <span>日期: 2025-11-08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人操作领域，执行长时程、接触丰富的复杂任务传统上需要大量真实世界数据和专家工程，面临高昂成本和可扩展性挑战。当前，基于模仿学习的方法严重依赖昂贵且劳动密集型的人类专家演示数据获取。同时，尽管视觉语言模型框架在自动化任务规划和策略学习方面展现出潜力，但现有方法大多忽略了触觉和接触力信息的整合。具体痛点在于：现有公共数据基准缺乏对操作力的考量，而实际任务中需避免损坏物体；传感器噪声和部分可观测性带来的感知不确定性使决策复杂化；当前数据获取方法成本高昂，且缺乏有效的数据合成与扩展手段。</p>
<p>本文提出了一种新颖的框架，通过结合分层语义分解、强化学习、视觉语言模型和知识蒸馏来克服这些局限。其核心思路是：首先在仿真中使用带力约束的强化学习训练一系列鲁棒的原子技能；然后利用VLM进行高层任务分解和技能规划，自动生成高质量的专家演示数据；最后通过视觉-触觉扩散策略将这些演示知识蒸馏成一个统一的、能够端到端执行长时程温和操作的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架整体上包含三个阶段：原子技能训练、VLM规划与数据生成、知识蒸馏。输入是自然语言任务描述和场景RGB图像，输出是机器人末端执行器的动作序列。</p>
<p><img src="https://arxiv.org/html/2511.05855v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体流程。首先在仿真中通过带力约束的强化学习训练机器人触觉原子技能。随后，视觉语言模型通过解读视觉场景和语言指令来规划任务序列，生成专家演示。最后，通过视觉-触觉扩散策略将这些演示知识蒸馏成具有接触感知的操作策略，从而能够从多模态点云输入端到端地执行长时程任务。</p>
</blockquote>
<p><strong>核心模块1：力感知原子技能</strong><br>首先对接触力进行表征，将其分解为法向力、剪切力和扭矩。基于此，设计了一系列与交互模式（如“抓握”、“旋转”、“移动”）对齐的原子技能，每个技能关注特定的力分量。为实现更复杂操作（如开抽屉、开盒子），还开发了主要涉及夹持力和水平摩擦力的“水平拉”技能，以及结合垂直抓握力和单向推力的“横向移动”技能。</p>
<p><img src="https://arxiv.org/html/2511.05855v1/x2.png" alt="原子技能接触力示意图"></p>
<blockquote>
<p><strong>图2</strong>：触觉原子技能的接触力示意图。展示了不同原子技能（如抓握、旋转、移动等）所涉及的主要接触力分量。</p>
</blockquote>
<p>这些原子技能使用Soft Actor-Critic算法在Isaac Gym仿真环境中训练。观察空间包括机器人状态、物体状态数据和触觉传感器接触力。动作空间为末端执行器的相对笛卡尔命令及夹爪宽度。每个原子技能都有专门设计的奖励函数，其中<strong>关键创新</strong>是引入了力惩罚项以鼓励温和操作。例如，抓握技能的奖励函数包含了惩罚过大接触力的项 (F_p)。</p>
<p><strong>核心模块2：VLM引导的数据生成（VASK）</strong><br>为了低成本生成高质量的演示数据，提出了视觉语言原子技能框架。该框架使用预训练的VLM作为顶层规划器，并为其构建了一个包含操作上下文介绍、原子技能库和系统提示的知识库。</p>
<p><img src="https://arxiv.org/html/2511.05855v1/x3.png" alt="VASK框架图"></p>
<blockquote>
<p><strong>图3</strong>：VASK框架图。VLM接收系统提示、自然语言任务描述和RGB图像，并将这些信息与原子技能库结合，指导智能体完成操作任务并收集原始轨迹数据。</p>
</blockquote>
<p>数据生成过程分为四步：环境初始化；VLM结合指令和图像进行分层技能序列规划；机器人顺序执行原子技能并同步收集环境信息、接触力反馈和原始点云数据；筛选满足任务完成标准且接触力较低的轨迹作为高质量数据。<strong>创新点</strong>在于将接触力信息显式地整合到长时程任务数据生成中，并大幅降低了构建专家演示数据集的人工成本。</p>
<p><strong>核心模块3：视觉-触觉扩散策略</strong><br>为了将VASK生成的不稳定规划转化为稳健的策略，提出了视觉-触觉扩散策略进行知识蒸馏。该方法融合了视觉点云和触觉点云。视觉点云来自固定深度相机，触觉点云则利用TacSL的软接触模型仿真得到，能够表征接触几何形状的变形。</p>
<p><img src="https://arxiv.org/html/2511.05855v1/x4.png" alt="长时程操作任务中的点云序列"></p>
<blockquote>
<p><strong>图4</strong>：长时程操作任务中的点云序列。视觉点云左上角框内区域显示了触觉点云。图例指示了传感器与物体之间的接触深度。</p>
</blockquote>
<p>在DP3方法基础上，VT-DP对全局点云进行裁剪和降采样（视觉点云256点，触觉点云128点），并对两种点云进行分离编码以更好地捕获模态特定特征。该扩散策略基于Transformer噪声预测网络，通过约50条高质量演示轨迹即可蒸馏出对应长时程顺序任务的温和操作策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在Isaac Gym仿真环境中设计了四个长时程操作任务进行评估：物体堆叠、打开并放置、抽屉并放置、倒球。机器人平台为UR5机械臂，配备两个Gelsight Mini触觉传感器、平行夹爪和Intel RealSense D415 RGB-D相机。</p>
<p><strong>基线对比</strong>：主要对比了三种方法：1）本文完整方法；2）仅使用VLM直接规划末端路径点；3）使用人类演示数据+VT-DP。</p>
<p><strong>关键实验结果</strong>：<br>如表3所示，本文方法在四个任务上的平均成功率最高（OS:0.90, OP:0.73, CP:0.63, PB:0.90），显著优于VLM直接规划和基于人类数据的方法。同时，本文方法完成任务的平均成功路径长度最短，且平均接触力最低，验证了其高效性和温和性。</p>
<p><img src="https://arxiv.org/html/2511.05855v1/img/force.jpg" alt="四种操作任务中接触力分布对比"></p>
<blockquote>
<p><strong>图5</strong>：四种操作任务中接触力分布的箱线图对比（十次试验）。蓝色箱线为使用无力量化奖励训练的策略，红色箱线为使用力感知奖励训练的策略。结果表明，引入力感知奖励的策略能持续保持更低、更稳定的接触力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>VLM规划器消融</strong>：测试了不同VLM与原子技能组合的规划成功率（表2）。Doubao-vision-pro表现最佳，被选为最终规划器。较小的模型（如Qwen2.5-7b）因规划能力不足导致成功率极低。</li>
<li><strong>技能消融</strong>：尝试让VLM直接规划路径点而非调用原子技能（表3），该方法成功率低，尤其在复杂任务（CP）中失败，且无法实现温和操作，证明了原子技能的必要性。</li>
<li><strong>力感知组件消融</strong>：如图5所示，对比了原子技能训练中是否包含力惩罚项的策略。结果表明，引入力感知奖励的策略在执行长时程任务时接触力显著更低、更稳定。</li>
<li><strong>输入模态对比</strong>：如表4所示，比较了不同感知模态作为扩散策略输入的效果。融合视觉与触觉点云的VT-DP方法在所有任务上成功率最高（平均0.79），显著优于仅使用视觉点云、RGB图像或RGB+触觉的方法，证明了多模态点云融合的有效性。</li>
<li><strong>演示数据对比</strong>：如表3所示，使用VASK自动生成数据训练的策略，其成功率和效率均远高于使用人工采集数据训练的策略，且人工数据无法有效整合接触力信息。</li>
</ol>
<p><strong>真实世界部署</strong>：<br>通过数字孪生系统将策略部署到物理机器人上。对真实点云进行坐标对齐、背景剔除等处理，并与仿真的机械臂点云融合。触觉点云通过对齐GelSight传感器数据与仿真模型获得。</p>
<p><img src="https://arxiv.org/html/2511.05855v1/x5.png" alt="真实世界长时程操作任务执行过程"></p>
<blockquote>
<p><strong>图6</strong>：真实世界中长时程操作任务的执行过程快照，展示了策略在物理环境中的可行性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>力感知且可扩展的数据生成框架</strong>，通过仿真训练的原子技能与VLM引导的分层规划，自动合成包含接触力信息的长时程操作演示数据，大幅降低了高质量专家数据获取的成本和难度。</li>
<li>构建了<strong>视觉-触觉扩散策略基准</strong>，能够有效融合视觉与触觉点云模态，仅需少量演示即可蒸馏出稳健、温和的长时程操作策略。</li>
<li>提出了一种<strong>混合方法</strong>，结合了强化学习（训练底层原子技能）、VLM（高层规划）和模仿学习（策略蒸馏）的优势，减少了单纯依赖任何一种方法的数据需求和训练负担。</li>
</ol>
<p><strong>局限性</strong>：论文提到，尽管框架减少了人工生成数据的努力，但VLM的集成带来了额外的计算开销。此外，仿真到真实的差距仍是有效部署的挑战。</p>
<p><strong>后续研究启示</strong>：本文工作表明，利用基础模型进行任务规划和结合多模态感知进行策略学习是解决复杂长时程操作的有效途径。未来研究可着眼于：进一步缩小仿真到真实的差距，特别是在触觉感知方面；探索更高效或专用的VLM规划器以减少计算开销；将原子技能库扩展至更广泛的接触交互类型，以覆盖更多样化的任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对长时程、接触丰富的精细操作任务依赖昂贵人工演示数据的问题，提出一种结合视觉语言模型（VLM）规划与仿真强化学习（RL）的新框架。核心方法包括：1）利用VLM对复杂任务进行高层语义分解与原子技能规划；2）在仿真中为每个原子技能训练带显式力约束的RL策略，确保操作轻柔；3）通过视觉-触觉扩散策略将VLM生成的多样演示提炼为统一的可执行策略。实验表明，该方法无需人工演示即能学习长时程操作策略，并通过原子技能框架实现任务的可扩展泛化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.05855" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>