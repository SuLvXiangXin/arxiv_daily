<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PAI-Bench: A Comprehensive Benchmark For Physical AI - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>PAI-Bench: A Comprehensive Benchmark For Physical AI</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.01989" target="_blank" rel="noreferrer">2512.01989</a></span>
        <span>作者: Zhou, Fengzhe, Huang, Jiannan, Li, Jialuo, Ramanan, Deva, Shi, Humphrey</span>
        <span>日期: 2025/12/01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>物理AI旨在开发能够感知和预测现实世界动态的模型。当前，多模态大语言模型（MLLMs）和视频生成模型（VGMs）是支撑这两种能力的主要技术路线。然而，现有基准测试存在碎片化和局限性：在预测方面，针对VGMs的评估多集中于视觉质量和时间一致性（如VBench系列），或仅关注物理合理性（如PhyGenBench），缺乏对物理AI应用场景的综合考量；在感知方面，针对MLLMs的评估多聚焦于抽象推理或日常感知，对其在物理AI专业领域的真实效能评估不足。此外，随着VGMs越来越多地利用多模态控制信号（如深度图）进行引导生成，尚无基准系统评估这种可控性。</p>
<p>本文针对上述痛点，提出了一个统一且全面的基准测试PAI-Bench，旨在系统评估模型在物理AI任务上的感知与预测能力。其核心思路是：通过构建三个针对性轨道（视频生成、条件视频生成、视频理解），并基于真实世界数据与任务对齐的评估指标，全面衡量模型在物理AI领域的表现，从而揭示当前技术的不足并指导未来研究方向。</p>
<h2 id="方法详解">方法详解</h2>
<p>PAI-Bench是一个综合性基准，包含三个独立轨道，整体框架围绕物理AI的感知与预测能力构建。</p>
<p><img src="https://arxiv.org/html/2512.01989v1/assets/Logos/physical-ai-bench-logo-20250923.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：PAI-Bench框架总览。这是一个为物理AI中多样化主题设计的综合基准，包括针对文本/条件到物理世界生成的评估，以及物理世界理解。所有三个轨道遵循统一的设计原则：将评估建立在具有物理意义的任务和真实世界数据之上。</p>
</blockquote>
<p>**1. PAI-Bench-G (视频生成)**：评估VGMs的预测能力，关注生成视频的视觉质量和物理合理性。</p>
<ul>
<li><strong>数据构建</strong>：从开源数据集和公共网络来源收集专注于物理AI领域（如自动驾驶、机器人）的真实视频。使用高级MLLM（Qwen2.5-VL-72B-Instruct）进行初始视频描述生成，再经人工精细校正，最终得到1,044个视频-提示对。为评估物理合理性，基于特定本体论，通过MLLM生成候选问答对并经人工提炼，构建了5,636个QA对，平均每个视频对应5-6个QA对。</li>
<li><strong>评估指标</strong>：<ul>
<li><strong>质量分数 (Quality Score)<strong>：沿用VBench系列协议，从</strong>通用生成质量</strong>（主题/背景一致性、运动平滑度、美学/成像质量、整体一致性）和<strong>参考保真度</strong>（图像到视频的主题/背景）两方面，共8个指标评估视觉保真度、时间一致性和语义对齐。</li>
<li>**领域分数 (Domain Score)**：为评估物理合理性，采用“MLLM-as-Judge”范式。使用构建的QA对，让评判MLLM（Qwen3-VL-235B-A22B-Instruct）基于生成的视频回答问题。领域分数定义为MLLM在该QA集上的回答准确率，以此量化视频对指定物理和语义约束的遵循程度。</li>
</ul>
</li>
</ul>
<p>**2. PAI-Bench-C (条件视频生成)**：进一步探究预测能力，评估条件VGMs对输入控制信号（如深度图）的保真度。</p>
<ul>
<li><strong>数据构建</strong>：从三个数据集中各采样200个视频片段，共600个视频，分别代表机器人（AgiBot）、自动驾驶（OpenDV）和第一人称任务（Ego-Exo-4D）。使用特定模型提取真实控制信号（模糊、边缘、深度、分割图）。为评估多样性，通过人机协作流程为每个视频生成一个原始描述和五个变体描述。</li>
<li><strong>评估指标</strong>：围绕<strong>对控制信号的忠实度</strong>（Blur SSIM, Edge F1, Depth si-RMSE, Mask mIoU）、<strong>生成视频的视觉质量</strong>（DOVER）和<strong>结果的多样性</strong>（LPIPS）进行综合评估。</li>
</ul>
<p>**3. PAI-Bench-U (物理视频理解)**：评估MLLMs的感知能力，专注于物理AI领域的视频理解任务。</p>
<ul>
<li><strong>能力定义</strong>：提出物理AI模型需具备两大关键能力：<strong>物理常识推理</strong>（对空间、时间、物理世界原则的被动理解）和<strong>具身推理</strong>（预测动作效果、遵循物理约束的主动规划与交互能力）。</li>
<li><strong>数据构建</strong>：<ul>
<li><strong>物理常识推理</strong>：从网络收集1,000多个视频，基于定义的本体论手动标注5,737个问题，经严格审查后得到涵盖426个视频的604个高质量QA对。</li>
<li><strong>具身推理</strong>：从六个现有数据集中收集601个视频，并手动标注610个QA对。</li>
</ul>
</li>
<li><strong>评估方式</strong>：使用LMMs-Eval框架进行评估，模型需基于输入的视频帧回答多项选择题。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在PAI-Bench的三个轨道上进行了大规模评估，使用了多个基准数据集和实验平台。</p>
<p><strong>1. PAI-Bench-G实验结果</strong>：</p>
<ul>
<li><strong>评估模型</strong>：15个领先的VGMs，包括开源模型（如Wan、Cosmos-Predict、MAGI、CogVideoX系列等）和专有模型Veo3。</li>
<li><strong>关键结果</strong>：<ul>
<li><strong>指标有效性验证</strong>：通过基于竞技场的人类研究，将模型生成的视频进行两两比较，并将人类偏好转化为ELO分数。结果显示，PAI-Bench-G的指标分数与人类偏好ELO分数之间存在强相关性（Pearson相关系数 r=0.918）。<br><img src="https://arxiv.org/html/2512.01989v1/x5.png" alt="指标相关性分析"><blockquote>
<p><strong>图6</strong>：PAI-Bench-G指标分数与人类偏好ELO分数的Pearson相关性分析。红色阴影区域表示0.95置信区间，显示出强相关性（r=0.918）。</p>
</blockquote>
</li>
<li><strong>核心发现</strong>：如表3所示，大多数领先的VGMs在<strong>质量分数</strong>上与真实视频的分数（78.0）高度竞争甚至持平，表明当前模型在生成高视觉保真度的视频方面已相当成功。然而，在<strong>领域分数</strong>上，所有模型均远低于真实视频设定的高基准（93.1）。这揭示了当前VGMs的一个关键局限：<strong>尽管在视觉合成上表现出色，但在生成始终遵循基本物理定律的内容方面仍存在困难</strong>。</li>
</ul>
</li>
</ul>
<p><strong>2. PAI-Bench-C实验结果</strong>：</p>
<ul>
<li><strong>评估模型</strong>：2个系列的4个可控视频生成模型（Cosmos-Transfer和Wan-Fun）。</li>
<li><strong>关键结果</strong>（总结自表4）：<ul>
<li><strong>多信号条件增强质量</strong>：对于Cosmos-Transfer模型，使用所有控制信号组合（<code>All</code>）的条件获得了最高的质量分数，显著优于任何单一控制信号。这表明在实践中，从源视频中提取互补的控制信号集，可以引导模型重建出质量更高的视频。</li>
<li><strong>分割信号保真度低</strong>：以分割图作为控制信号时，模型产生的分割保真度（Mask mIoU）最低。作者推测这是因为分割图是所有控制信号中噪声最大的，即使是最先进的分割模型（如SAM2）产生的掩码也可能存在时间不一致问题，导致训练监督不可靠。</li>
</ul>
</li>
</ul>
<p><strong>3. PAI-Bench-U实验结果</strong>：</p>
<ul>
<li><strong>评估模型</strong>：21个MLLMs，包括专有模型（Claude-3.5-Sonnet, GPT-4o, GPT-5）和开源模型（Qwen2.5-VL, Qwen3-VL, Cosmos-Reason1等系列）。</li>
<li><strong>关键结果</strong>（总结自表5）：<ul>
<li><strong>与人类性能存在巨大差距</strong>：在整体准确率上，表现最好的专有模型GPT-5为81.6%，最好的开源模型Qwen3-VL-235B-A22B-Instruct为80.9%，而人类基线高达93.2%。这显示出现有MLLMs的物理推理能力<strong>远未达到人类水平</strong>。</li>
<li><strong>具身推理是更大挑战</strong>：模型在“物理常识推理”任务上的平均表现（约80%）普遍优于在“具身推理”任务上的表现（约70%）。特别是在RoboFail和Autonomous Vehicle等具体应用数据集上，模型性能下降明显，表明<strong>让模型进行与具体行动和交互相关的推理是更大的挑战</strong>。</li>
</ul>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>首个统一的综合基准</strong>：提出了PAI-Bench，这是首个对视频生成、条件视频生成和视频理解提供统一、全面评估的基准。</li>
<li><strong>专注于物理AI的高质量基准</strong>：基于真实世界数据，涵盖多样化物理AI场景（工业、自动驾驶、具身AI等），并配备了一套任务对齐的评估指标。</li>
<li><strong>大规模评估与现状揭示</strong>：对35个模型进行了大规模评估，系统性地揭示了当前SOTA系统在物理AI核心能力上的局限性：VGMs长于视觉合成但短于物理一致性；MLLMs的物理推理，尤其是具身推理，远落后于人类。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，PAI-Bench-U中使用的视频帧数有限（最多16帧），可能无法完全捕捉长时序的物理动态；此外，评估主要依赖于自动指标和MLLM作为评判者，虽然与人类偏好相关性高，但仍可能存在偏差。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>指明研究方向</strong>：实验结果明确指出了物理AI领域亟待突破的关键瓶颈——提升模型的物理一致性和具身推理能力，这应成为未来VGMs和MLLMs研究的重点。</li>
<li><strong>提供评估工具</strong>：PAI-Bench为社区提供了一个坚实的评估基础，可用于跟踪和推动物理AI领域的进展。</li>
<li><strong>促进方法融合</strong>：基准揭示了感知与预测模型各自的短板，启发未来研究可能需要探索将强大的感知（理解）能力与生成（预测）模型更深度结合的新范式，以构建更全面的物理AI系统。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前多模态大语言模型和视频生成模型在感知与预测真实世界动态方面能力不足的问题，提出了综合基准PAI-Bench。该基准包含视频生成、条件视频生成和视频理解三大任务，采用2808个真实案例与任务对齐的指标，系统评估模型的物理合理性与领域推理能力。实验表明，视频生成模型虽视觉保真度高，但难以保持物理连贯的动态；多模态大语言模型在预测与因果解释上表现有限，揭示现有系统仍处于物理AI发展的早期阶段。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.01989" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>