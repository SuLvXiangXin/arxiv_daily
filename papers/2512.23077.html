<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.23077" target="_blank" rel="noreferrer">2512.23077</a></span>
        <span>作者: Soedarmadji, Saraswati, Wei, Yunyue, Zhang, Chen, Yue, Yisong, Sui, Yanan</span>
        <span>日期: 2025/12/28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，针对高维肌肉骨骼系统的运动控制，主流方法通常依赖手工设计的启发式目标，例如速度跟踪或能量最小化。这些方法往往难以捕捉运动复杂性的细微结构，忽略了基本的解剖学原理，导致产生生物力学上不自然或低效的行为。尽管近期大型语言模型和视觉语言模型在通过高层运动质量评估来探索奖励结构方面展现出潜力，但现有方法通常依赖片段统计量或粗略的成功信号作为反馈，且主要评估于具有显式动力学的低维、扭矩驱动系统。然而，运动控制受时间上持续且结构隐式的感觉运动动力学支配，仅靠上述反馈难以捕捉。因此，这些模型能否系统地将隐式的交互动力学转化为适用于高维肌肉骨骼控制的结构化奖励，仍不明确。</p>
<p>本文针对“如何从高层目标（如自然语言描述）自动学习有效的奖励函数”这一具体痛点，提出了一个结合描述性反馈和动力学反馈的新视角。其核心思路是：提出MoVLR框架，通过控制优化与VLM反馈之间的迭代交互来探索奖励空间，将语言和视觉评估转化为结构化指导，从而为高维肌肉骨骼控制发现并精炼奖励函数。</p>
<h2 id="方法详解">方法详解</h2>
<p>MoVLR是一个控制闭环框架，它将大型语言模型和视觉语言模型集成到奖励学习过程中，旨在桥接以自然语言表达的显式行为规范与控制所需的隐式动力学表示。其核心思想是在迭代学习循环中纳入策略执行轨迹的视频观察，使模型能够联合推理语言意图和物理运动。这种多模态反馈提供了关于轨迹可行性、生物力学一致性和任务完成度的结构化见解，最终产生与底层系统动力学更一致的奖励函数。</p>
<p><img src="https://arxiv.org/html/2512.23077v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：MoVLR的工作流程。策略优化用于提供奖励候选者的高维肌肉骨骼动力学。VLM评估相应的运动视频𝜻^(𝑖)，以更新当前最佳奖励设计r*，并为LLM提供生物力学改进反馈ℱ，用于精炼下一轮奖励r^(𝑖+1)的生成。</p>
</blockquote>
<p>整体流程如算法1所示，包含三个核心模块：</p>
<ol>
<li><strong>肌肉骨骼运动动力学</strong>：对于当前迭代的奖励提案r^(𝑖)，通过策略优化（求解公式4）得到最优策略π*_r^(𝑖)，并通过滚动执行该策略获得控制轨迹𝜻^(𝑖)。此模块将奖励代码转化为具体的物理运动作为反馈。论文采用MPC2作为控制策略，这是一种基于模型、无需训练的分层规划器，能将策略优化时间从数小时缩短至数分钟，从而支持MoVLR进行更多轮次的奖励学习迭代。</li>
<li><strong>运动-语言表示</strong>：使用VLM作为语义观察者，对渲染出的控制动态视频𝜻^(𝑖)进行评估。VLM并非给出标量分数，而是生成可解释的、基于语言的评估。它将新轨迹与当前最佳奖励定义下产生的动态（𝜻<em>）进行比较。如果新提议的奖励产生了更符合运动描述R的控制序列，则同时更新当前最佳奖励r</em>和对应动态𝜻<em>。此外，VLM还会生成关于r</em>的结构化文本反馈ℱ，定性评估运动相对于R的表现。此反馈将作为LLM的反思性输入，指导后续的奖励合成。</li>
<li><strong>语言引导的奖励设计</strong>：LLM负责生成可执行的奖励函数。它基于以下上下文进行推理：<strong>运动描述R</strong>（高层目标）、<strong>环境代码ℰ</strong>（定义了状态、控制和转移动力学）以及<strong>VLM生成的反馈ℱ</strong>。给定这些信息，LLM对当前最佳奖励函数r*进行局部搜索，合成新的奖励项。每个奖励项编码一个生物力学子目标（如方向跟踪、平滑度、关节稳定性）。奖励提案被持续提出和精炼，直到识别出一个可执行的函数。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.23077v1/x2.png" alt="VLM与LLM输入输出示例"></p>
<blockquote>
<p><strong>图2</strong>：(a) VLM和(b) LLM的输入输出示例。VLM基于给定的运动描述分析视频运动序列并提供诊断反馈。LLM利用此反馈探索对奖励函数的相应代码修改。</p>
</blockquote>
<p>与现有方法相比，MoVLR的核心创新在于将动力学控制反馈（以视频形式）纳入迭代循环，使VLM能够反思运动学和姿态精度，这对于实现稳定的肌肉骨骼控制至关重要，而传统仅基于语言的方法缺乏此能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在MuJoCo模拟器中进行，使用了包括MS-Human-700模型在内的多种肌肉骨骼系统形态。任务涵盖六类（如<strong>图3</strong>所示）：平坦/粗糙/斜坡地形上的 locomotion、带损伤身体的 locomotion、转向、鸵鸟模型 locomotion，以及瓶子倾倒和立方体操纵等操作任务。VLM和LLM基于Gemini和Qwen模型构建。</p>
<p><img src="https://arxiv.org/html/2512.23077v1/x3.png" alt="任务概览"></p>
<blockquote>
<p><strong>图3</strong>：评估的六项任务概览。顶行展示了每个任务的环境设置，底行可视化学习了奖励项的相对权重。</p>
</blockquote>
<p><strong>对比方法</strong>：与三种基线方法对比：人工设计的奖励函数、Eureka（纯LLM奖励生成）和HARMON（VLM用于运动精炼）。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2512.23077v1/figures/distance.png" alt="性能对比（运动任务）"></p>
<blockquote>
<p><strong>图4a</strong>：在运动任务上的性能对比，以10秒内行走总距离衡量（越高越好）。MoVLR在所有地形环境中均取得了最高的任务性能，在平坦、斜坡和粗糙地形上获得了最长的行走距离。尽管略低于人工设计基线，但在鸵鸟环境中仍保持了强劲性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.23077v1/figures/orientation.png" alt="性能对比（操作任务）"></p>
<blockquote>
<p><strong>图4b</strong>：在操作任务上的性能对比，以物体位置和方向误差衡量（越低越好）。MoVLR相比所有基线取得了最低的平均误差，表明其交互更精确、稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.23077v1/x5.png" alt="运动阶段改进"></p>
<blockquote>
<p><strong>图5</strong>：基于运动视频，肌肉骨骼模型步态随训练阶段的渐进式改进。展示了从不稳定到协调行走的转变。</p>
</blockquote>
<p><strong>奖励权重演变分析</strong>：论文通过热图展示了奖励项权重随精炼阶段的演变过程，揭示了反馈驱动过程如何为达成生物力学一致的行为而重组内部优化格局。</p>
<p><img src="https://arxiv.org/html/2512.23077v1/figures/rough_heatmap.png" alt="粗糙地形任务奖励权重热图"></p>
<blockquote>
<p><strong>图6a</strong>：粗糙地形运动任务的各阶段加权奖励项。早期阶段权重集中在高度、速度、平衡等粗略的全局稳定性项；随着精炼进行，这些全局项的影响下降，而足部放置、髋部对齐、膝部控制等局部生物力学描述符变得突出；后期阶段权重分布高度结构化，表明向协调、有节奏的运动收敛。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.23077v1/figures/bottle_heatmap.png" alt="瓶子倾倒任务奖励权重热图"></p>
<blockquote>
<p><strong>图6b</strong>：瓶子倾倒任务的各阶段加权奖励项。呈现出类似的分层精炼模式：初始阶段强调瓶子位置、方向等粗略空间对齐；随着精炼继续，权重转向抓握质量、肘关节精度、手指协调等精细运动控制组件。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>奖励泛化性</strong>：将在平坦地形上学到的奖励函数直接迁移到新环境（粗糙/斜坡地形、损伤身体、转向任务）和不同控制算法（从MPC2迁移到RL方法DynSyn）中，无需额外精炼。结果显示性能虽有适度下降（如粗糙地形：2.41米 vs. 原2.76米），但智能体仍能保持稳定和持续运动，甚至在损伤身体设置中性能略有提升（5.12米 vs. 原4.8米），证明了所学奖励结构的鲁棒性和可迁移性。</li>
<li><strong>VLM-only奖励学习</strong>：尝试使用单一的多模态模型同时执行反馈解释和代码合成。该变体性能大幅下降，常产生无效或不完整的奖励代码，且无法在迭代中改进控制行为。这表明当前的VLM尚不具备同时处理两项任务所需的组合或程序性推理能力，验证了保持反馈与合成阶段分离的重要性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了MoVLR，一个全自动的奖励学习框架，能有效捕获隐式动力学并为控制高维肌肉骨骼系统设计显式奖励。</li>
<li>证明了MoVLR能够泛化到不同的运动任务、环境和系统形态，产生代表隐式肌肉骨骼动力学的可解释奖励项。</li>
<li>通过多模态反馈的迭代精炼，MoVLR奖励的演变反映了人类运动学习的层次结构，从粗略的稳定性约束逐步进展到细粒度的关节协调。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性体现在消融实验中，即当前单一的视觉语言模型尚不具备同时完成反馈评估和代码生成的能力，强调了模块化设计的重要性。</p>
<p><strong>对后续研究的启示</strong>：这项工作揭示了<strong>显式语言意图</strong>与<strong>隐式奖励涌现</strong>之间的基本联系。VLM作为感知桥梁，将语言目标锚定在物理动力学中，产生了可解释且动力学一致的奖励表示。这为复杂行为和形态的生物合理、可解释、可泛化的控制提供了一条可扩展且原则性的路径。未来的工作可以探索更强大的多模态模型以简化框架，或将此范式应用于更广泛的具身智能控制问题。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对高维肌肉骨骼系统控制中奖励函数设计困难的核心问题，提出MoVLR框架。该方法利用视觉语言模型（VLMs），通过控制策略优化与VLM反馈的迭代交互，将语言和视觉评估转化为具身学习的结构化指导，从而自动探索和优化奖励函数，替代手工设计。论文表明，VLMs能有效将抽象运动描述映射到生理运动控制的隐式原理中，为高维肌肉骨骼系统的运动控制提供了新途径。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.23077" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>