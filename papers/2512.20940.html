<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.20940" target="_blank" rel="noreferrer">2512.20940</a></span>
        <span>作者: Ye, Shuhao, Mao, Sitong, Cui, Yuxiang, Yu, Xuan, Zhai, Shichao, Chen, Wen, Zhou, Shunbo, Xiong, Rong, Wang, Yue</span>
        <span>日期: 2025/12/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，在连续环境下的视觉语言导航（VLN-CE）任务中，主流方法主要分为两大类。一类是基于大视觉语言模型（LVLM）的方法，它们利用大规模预训练的优势，能够深入理解语言指令。然而，这类方法在处理VLN-CE任务时存在关键局限性：其基于图像帧序列的输入格式冗余度高，且通常难以有效处理任务提供的全景视觉上下文，被迫在低层次的电机动作空间中进行长视野导航。另一类是基于拓扑图的方法，它们通过在线构建稀疏的拓扑地图，将动作空间简化为高层路径点选择，具有表示紧凑、保留全景结构的优点。然而，现有图方法未能充分利用其优势，未能像LVLM方法那样拥抱大规模训练范式。它们受限于导航专用数据，且忽略了两个有价值的数据源：来自强大LVLM的丰富语言知识（可用于增强合成指令）以及来自多个导航数据集（如R2R和RxR）的集体任务数据。此外，这些方法通常省略了强化学习微调（RFT）阶段，而基于路径点的动作空间本非常适合进行高效的闭环RFT。本文针对图方法未充分利用大规模数据和先进训练范式的痛点，提出了ETP-R1框架，其核心思路是通过Gemini API生成高质量、多样化的预训练数据，联合训练R2R和RxR任务，并首次将闭环在线RFT应用于图模型，从而弥合图方法与LVLM方法之间的性能差距。</p>
<h2 id="方法详解">方法详解</h2>
<p>ETP-R1的整体框架遵循一个三阶段的训练范式：离线联合预训练、在线监督微调（SFT）和在线强化学习微调（RFT）。输入为自然语言指令和在线构建的拓扑图（节点由全景RGBD观测表示），输出是选择下一个要前往的路径点（节点）或停止动作。</p>
<p><img src="https://arxiv.org/html/2512.20940v1/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：ETP-R1方法总览。我们的工作聚焦于三阶段训练范式内的预训练和在线RFT阶段。</p>
</blockquote>
<p><strong>核心模块一：训练免费的预训练数据标注</strong>。为了构建高质量、大规模预训练数据，我们利用Gemini API重新标注了Prevalent数据集。对于轨迹中的每个路径点，我们将其左、前、右三个单目视图（各90°视场角）水平拼接成一张复合图像，并叠加一个指向下一个路径点的红色箭头以明确行进方向。在文本提示中，我们采用了“轨迹分割标注”策略：随机将完整轨迹分割为1/2/3个连续子任务，并要求Gemini为每个子任务生成描述，最终拼接得到6条不同粒度的指令。这种方法在单次API调用中高效地产生了语言丰富、幻觉少的多样化指令，平均指令长度从原始的31词提升至48词。</p>
<p><img src="https://arxiv.org/html/2512.20940v1/x3.png" alt="Gemini标注可视化"></p>
<blockquote>
<p><strong>图3</strong>：Gemini指令标注可视化。图中也包含了原始的Prevalent说话者模型标注结果以供对比。Gemini生成的指令幻觉更少，语言更丰富。</p>
</blockquote>
<p><strong>核心模块二：跨模态规划网络</strong>。模型工作流程为：首先通过文本编码器和节点编码器分别处理指令和拓扑图节点，然后将编码后的特征输入核心的双阶段融合Transformer（DPFT）模块进行对齐，最后由单动作预测（SAP）头进行评分选择。</p>
<ul>
<li><strong>文本编码器</strong>：基于RoBERTa架构，在词嵌入中加入了指示数据源（R2R、RxR或Gemini数据）的任务嵌入，以支持联合预训练。</li>
<li><strong>节点编码器</strong>：对每个节点的12张全景RGBD图像分别使用视觉主干网络提取特征，结合视角嵌入后，通过一个全景编码器（Transformer）融合上下文，生成节点令牌。节点令牌还融合了步数嵌入、位置嵌入和任务嵌入。</li>
<li><strong>DPFT模块</strong>：该模块是核心创新，分为两个阶段。<ol>
<li><strong>对称跨模态融合网络</strong>：由 L_s 层Transformer构成，每层包含两个使用非共享权重的双向交叉注意力模块（文本关注图，图关注文本），后接自注意力和前馈网络，生成相互感知的文本特征 T_sym 和图特征 G_sym。</li>
<li><strong>文本引导的图细化网络</strong>：使用 G_sym 作为查询， T_sym 作为键和值，通过交叉注意力提取语言引导向量 G_guide。该向量经过一个FFN后与原始的 G_sym 拼接，得到最终细化的图特征 G_out。</li>
</ol>
</li>
</ul>
<p><img src="https://arxiv.org/html/2512.20940v1/x4.png" alt="DPFT框架"></p>
<blockquote>
<p><strong>图4</strong>：双阶段融合Transformer（DPFT）框架示意图。包含对称跨模态融合和文本引导的图细化两个阶段。</p>
</blockquote>
<p><strong>核心模块三：三阶段训练范式</strong>。</p>
<ol>
<li><strong>离线联合预训练</strong>：在来自R2R和RxR的五个数据集（总计约3.1M样本）上联合训练一个基础模型。训练任务为单动作预测（SAP）和掩码语言建模（MLM），以1:1的比例进行。</li>
<li><strong>在线SFT</strong>：使用DAgger算法在交互环境中进行在线监督微调。智能体以概率 p 遵循专家动作（由全局规划器提供的最优节点），以概率 1-p 采样自身策略的动作，从而学习从错误中探索和自纠正。</li>
<li><strong>在线RFT</strong>：<strong>这是本文的关键创新</strong>，首次将闭环在线RFT应用于图模型。采用分组相对策略优化（GRPO）算法，该算法无需价值函数网络，降低了计算负担。对于每个导航片段（视为一个“提示”），采样一组 G 条轨迹（视为“回答”），并根据整个片段的最终结果计算奖励。奖励函数针对R2R-CE和RxR-CE任务分别设计（公式5a, 5b），综合考虑了成功与否、路径效率（SPL/gSPL）和对指令的遵循程度（nDTW, SDTW）。计算出的片段奖励被归一化后，作为该轨迹中所有高层动作的优势值，用于GRPO策略更新。</li>
</ol>
<p><strong>创新点总结</strong>：1) 利用通用LVLM（Gemini）进行免训练的、高质量、多样化数据生成；2) 对R2R和RxR任务进行联合预训练，增强泛化能力；3) 为图模型设计了包含在线闭环RFT的三阶段训练范式，显著提升了策略优化效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在标准VLN-CE基准测试R2R-CE和RxR-CE上进行评估，使用Habitat模拟器。对比的基线方法包括VLN↻BERT、GridMM、ScaleVLN、BEVBert、ETPNav、HNR和G3D-LF等。评估指标包括导航误差（NE）、成功率（SR）、路径长度加权成功率（SPL）、归一化动态时间规整（nDTW）和nDTW加权成功率（SDTW）等。</p>
<p><strong>关键实验结果</strong>：<br>在R2R-CE的Val Unseen分割上，仅使用在线SFT的模型（Ours-DAgger）就将NE降低至4.11m，SR和SPL分别提升至63%和54%，已超越所有先前方法。进一步应用在线GRPO微调后（Ours-GRPO），性能进一步提升至NE 3.94m，SR 65%，SPL 56%，在所有指标上确立了新的最高水平。在Test Unseen分割上，Ours-GRPO模型相比之前的SOTA方法G3D-LF，将NE降低了0.59m，SR和SPL分别提高了6%和3%。</p>
<p>在RxR-CE的Val Unseen分割上，Ours-DAgger模型同样超越了之前的SOTA方法HNR。Ours-GRPO模型进一步将NE降低至5.22m，并将SR、SPL、nDTW和SDTW分别提升至59.92%、48.97%、65.31和50.41，在所有指标上达到最优。</p>
<p><img src="https://arxiv.org/html/2512.20940v1/x5.png" alt="定性结果案例"></p>
<blockquote>
<p><strong>图5</strong>：来自R2R-CE Val Unseen的一个案例展示，体现了我们模型强大的指令遵循能力。图中对比了不同方法规划的路径。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文通过消融实验验证了各核心组件的贡献。</p>
<ol>
<li><strong>数据与预训练策略</strong>：实验表明，使用Gemini重新标注的预训练数据（Prevalent_Gemini_Aug）比原始Prevalent数据带来显著提升（SR +3.7%）。进一步加入RxR数据进行联合预训练，能带来额外收益（SR +1.2%）。</li>
<li><strong>在线RFT配置</strong>：实验验证了在线RFT阶段特定配置的有效性，包括对冻结层启用Dropout、在采样阶段启用Dropout以及将GRPO更新迭代次数 μ 设为1。这些配置共同作用，带来了显著的性能提升。</li>
<li><strong>GRPO组大小</strong>：实验探索了GRPO中组大小 G 的影响。发现 G=8 时在性能与计算成本之间取得了良好平衡，过小（G=4）或过大（G=16）均会导致性能下降。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.20940v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：关于GRPO组大小 G 的消融研究结果（在R2R-CE Val Unseen上）。组大小 G=8 时在性能与计算成本间取得了最佳平衡。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一种利用通用LVLM（Gemini）API的免训练数据标注流程，生成了大规模、高质量、低幻觉的预训练指令数据；2) 提出了对多样化VLN数据集（R2R和RxR）进行联合预训练的策略，增强了模型的泛化能力；3) 首次为基于图的VLN模型配备了包含在线闭环RFT的三阶段训练范式，并采用高效的GRPO算法进行优化，在两大基准测试上实现了新的SOTA性能。</p>
<p><strong>局限性</strong>：论文提到，在利用Gemini生成指令时，对于极少数（约2%）箭头方向超出三视图复合图像范围的轨迹，采取了直接丢弃的策略。这虽然影响很小，但理论上损失了部分数据。</p>
<p><strong>研究启示</strong>：本工作表明，即使对于依赖结构化环境表示（如图）的模型，积极拥抱大规模、高质量数据构建以及先进的训练范式（如闭环在线RFT），也能极大释放其潜力，甚至超越依赖原始感知信号的LVLM方法。这为未来VLN乃至更广泛的具身智能研究提供了重要方向：即如何将大模型的数据与训练优势，与具有归纳偏置的高效结构化表示相结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对连续环境视觉语言导航中，基于图的方法在利用大规模数据和先进训练范式上落后于大视觉语言模型的问题，提出ETP-R1框架。其关键技术包括：利用Gemini API构建高质量、大规模的拓扑轨迹指令预训练数据集；统一R2R和RxR任务数据进行联合预训练；并首次将基于GRPO算法的闭环在线强化学习微调应用于图模型。实验表明，该方法在R2R-CE和RxR-CE基准测试的所有主要指标上均取得了最先进的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.20940" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>