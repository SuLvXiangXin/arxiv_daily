<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.09638" target="_blank" rel="noreferrer">2602.09638</a></span>
        <span>作者: Hui Xiong Team</span>
        <span>日期: 2026-02-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>3D可承受性（affordance）定位旨在突出3D物体上可操作的区域，对于机器人操作至关重要。先前的研究主要集中于从语言和图像等静态线索中学习可承受性知识，这些方法难以提供揭示时间和因果线索的充足动态交互上下文。为了缓解这一困境，本文收集了一个全面的基于视频的3D可承受性数据集VIDA，并提出了一种新的任务：从人类-物体交互（HOI）演示视频中定位3D物体可承受性，旨在利用大规模演示视频语料库进行以物体为中心的3D可承受性推理。本文的核心思路是激活多模态大语言模型（MLLMs）额外的可承受性分割能力，通过引入潜在动作编码器从HOI视频中提取动态交互先验，并结合空间感知损失函数，在一个统一框架内实现世界知识推理和细粒度的3D可承受性定位。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架接受一个HOI视频和一个文本指令作为输入，目标是输出物体点云上的可承受性掩码。VideoAfford主要由四个组件构成：1）一个受益于大规模3D表示学习的3D视觉编码器，为密集预测任务提供坚实基础；2）一个预训练的潜在动作编码器，提供丰富的动作先验；3）一个展现出可承受性推理能力的视频多模态大语言模型（MLLM）；4）一个基于Transformer的轻量级可承受性解码器，用于生成密集的3D可承受性掩码。</p>
<p><img src="https://arxiv.org/html/2602.09638v1/x4.png" alt="方法总览"></p>
<blockquote>
<p><strong>图4</strong>：VideoAfford方法总览。给定一个HOI视频和对应的点云，模型使用LanguageBind作为视频编码器，RenderNet作为动作编码器，获得视频嵌入和潜在动作嵌入。这些嵌入与文本指令一同输入大语言模型，预测语言令牌和可承受性令牌。另一方面，使用预训练的3D编码器提取语义丰富的点嵌入，通过几何引导的上采样和传播模块获得密集点特征。最后，可承受性令牌和点特征被输入可承受性解码器以生成可承受性掩码。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>点云编码器</strong>：采用在大型文本-图像-点配对数据上预训练的3D编码器作为骨干。给定点云，编码器将其编码为语义丰富的点嵌入，随后通过几何引导的上采样操作将语义特征传播为密集点特征。</li>
<li><strong>空间感知损失函数</strong>：为了解决先前工作只关注点类别准确性而缺乏对“空间连续性和区域重叠”约束的问题，本文引入了空间损失。其核心机制是结合点云的空间邻域信息分配自适应权重，从而支持空间连续性的训练。该损失基于改进的Dice损失，通过计算点坐标的邻近度来赋予空间相邻点更高权重，强调聚类正样本在损失计算中的贡献。</li>
<li><strong>动作编码器</strong>：为了增强动作理解能力，引入了潜在动作编码器来从紧凑的状态表示中学习可泛化的人类-物体交互动作。给定HOI视频，采样N帧并使用动作编码器提取潜在动作嵌入，并将其压缩为两个令牌。</li>
<li><strong>视频MLLM主干</strong>：选择Video-LLaVA作为主干。当输入视频和文本时，使用视频编码器和动作编码器对视频进行稀疏编码以获得视频令牌和动作令牌，将它们拼接后作为LLM的输入。模型扩展了Video-LLaVA的词表，注入了特殊令牌<code>&lt;AFF&gt;</code>来代表可承受性世界知识，该令牌的隐藏状态被投影为查询嵌入，作为可承受性条件输入解码器。</li>
<li><strong>可承受性解码器</strong>：一个基于Transformer的轻量级解码器，利用可承受性嵌入和密集点特征，通过交叉注意力模块进行融合，最终经由一个MLP网络生成可承受性掩码。</li>
<li><strong>训练目标</strong>：总损失函数是交叉熵损失（用于文本输出）、二元交叉熵损失、IoU损失（用于分割掩码预测）以及前述空间感知损失的加权和。</li>
</ol>
<p><strong>创新点</strong>：与现有主要依赖静态图像或语言的方法相比，本文的创新主要体现在：1）<strong>任务与数据创新</strong>：首次系统性地提出并构建了从HOI视频中学习物体中心3D可承受性的任务和大规模数据集VIDA。2）<strong>方法创新</strong>：设计了统一框架，将视频MLLM的世界知识推理能力、专门的动作编码器提取的动态先验，以及针对3D形态特点设计的空间感知损失相结合，实现了动态交互信息向3D空间知识的有效迁移。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与基准</strong>：实验在本文新构建的VIDA数据集上进行，该数据集包含约38K个HOI视频和22K个带标注的点云，涵盖38个物体类别和16种可承受性类型。实验分为“可见”（训练和测试集中的物体及可承受性类别一致）和“不可见”（测试集中包含训练集未出现的物体或可承受性类别）两种设置。由于是新任务，没有直接的前期工作。因此，作者选择了几种先进的基于HOI图像的3D可承受性定位方法作为模块化基线进行比较，包括XMF、PFusion、IAGNet、LASO、GREAT、Seqafford，并复现了LMAfford3D。</p>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2602.09638v1/x2.png" alt="主结果表"></p>
<blockquote>
<p><strong>表2</strong>：主要对比结果。展示了所有对比方法的mIoU、AUC、SIM和MAE指标（均为百分比）。最佳结果以粗体标出，次佳结果以下划线标出。结果表明，无论是在“可见”还是“不可见”设置下，本文提出的VideoAfford方法在所有评估指标上均显著优于所有基线方法。例如，在“可见”设置下，mIoU达到28.20%，比次优的GREAT（23.62%）高出4.58个百分点；在“不可见”设置下，mIoU为10.95%，同样领先。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.09638v1/x3.png" alt="消融实验表"></p>
<blockquote>
<p><strong>表3</strong>：消融实验结果。该实验验证了动作编码器和空间感知损失函数对模型性能的贡献。结果表明，两者均能独立带来性能提升，而结合使用时效果最佳。在“可见”设置下，同时使用两者使mIoU从基线20.16%提升至28.20%；在“不可见”设置下，从7.12%提升至10.95%，证明了这两个组件对于提升模型动态理解能力和空间预测一致性的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.09638v1/x5.png" alt="可视化结果"></p>
<blockquote>
<p><strong>图5</strong>：定性可视化结果。第一列是输入的HOI视频，最后一列是真实标注。中间列展示了VideoAfford与基线方法（IAGNet, GREAT）的预测结果对比。可视化显示，本文方法能产生更完整、空间上更连贯的可承受性区域预测，更接近真实标注，而基线方法的预测往往更加零散或不准确。</p>
</blockquote>
<p><strong>消融实验总结</strong>：动作编码器通过引入动态交互先验，提升了模型对动作意图的理解；空间感知损失函数通过强制模型学习空间连续的目标区域，有效防止了预测结果碎片化，确保了分割目标的空间紧凑性。两者协同工作，是模型取得优越性能的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>任务与数据集</strong>：首次系统性地提出了“从HOI视频中定位3D物体可承受性”的新任务，并构建并开源了大规模基准数据集VIDA，填补了该领域缺乏动态交互视频数据的空白。</li>
<li><strong>方法</strong>：提出了VideoAfford这一强基线方法，创新性地将视频MLLM的世界知识、专用动作编码器提取的动态先验，以及针对3D空间特性设计的损失函数相结合，实现了动态视频信息向3D可承受性知识的有效迁移与推理。</li>
<li><strong>性能</strong>：通过大量实验验证了所提方法的优越性，其在标准设置和开放词汇泛化（不可见类别）设置下均显著超越现有先进方法，展示了其实用潜力。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前方法依赖于视频-点云配对数据进行训练。虽然训练时视频和点云不需要严格的一对一配对，但这种数据对的收集和标注仍然具有一定成本。此外，方法整合了多个大型预训练模型，在计算效率方面可能存在优化空间。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据利用</strong>：探索如何更高效地利用大量未标注或弱标注的互联网HOI视频，减少对精确配对标注的依赖。</li>
<li><strong>模型效率</strong>：研究更轻量化的架构或训练策略，以降低VideoAfford类模型的计算和部署成本，使其更适合机器人等嵌入式平台。</li>
<li><strong>因果推理</strong>：HOI视频包含丰富的时序和因果信息（接触前、接触中、接触后）。未来工作可以深入探索如何显式地建模这些因果线索，以进一步提升可承受性推理的深度和可解释性。</li>
<li><strong>任务扩展</strong>：本框架展示了利用MLLM和动态先验处理3D空间理解任务的潜力，可启发将其应用于其他需要结合动态观察与静态几何理解的具身AI任务中。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决3D可操作区域（affordance）定位问题，指出现有方法依赖静态语言/图像线索，缺乏动态交互上下文。为此，作者构建了大规模视频数据集VIDA，并提出VideoAfford模型。关键技术包括：利用多模态大语言模型增强分割与推理能力；通过潜在动作编码器从人-物交互视频中提取动态先验；引入空间感知损失以学习3D空间知识。实验表明，该模型显著优于现有方法，并展现出强大的开放世界泛化与推理能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.09638" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>