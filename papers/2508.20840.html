<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning Primitive Embodied World Models: Towards Scalable Robotic Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning Primitive Embodied World Models: Towards Scalable Robotic Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.20840" target="_blank" rel="noreferrer">2508.20840</a></span>
        <span>作者: Qinying Gu Team</span>
        <span>日期: 2025-08-28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视频生成的具身世界模型正受到越来越多的关注，其将策略学习视为视频生成问题：给定语言描述的目标，模型生成未来的视频片段，并从中提取动作。然而，这类方法严重依赖大规模具身交互数据，而具身数据的稀缺性、收集难度和高维度从根本上限制了语言与动作之间的对齐粒度，并加剧了长时域视频生成的挑战，阻碍了生成模型在具身领域实现“GPT时刻”。</p>
<p>本文挑战了主流的长时域生成范式，提出了一个转向原始动作级别建模的新视角。核心思路是：将视频生成限制在固定的、较短的时域内，聚焦于预测即刻的、原始动作级别的状态转移，从而在降低建模复杂度的同时，实现语言与动作的细粒度对齐，并提高数据效率和推理速度。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的原始具身世界模型（Primitive Embodied World Models, PEWM）是一个分层框架，其核心是将复杂的长时域任务分解为一系列短时域、语义原子的<strong>原始动作</strong>，并训练一个视频生成模型来预测每个原始动作的执行过程。</p>
<p><strong>整体框架</strong>：系统由高层规划器和底层世界模型组成。给定一个语言指令，<strong>视觉语言模型（VLM）规划器</strong> 将其分解为一个原始动作序列，并为每个动作生成一个空间化的<strong>起点-目标热图（SGG）</strong> 作为子目标。底层<strong>原始动作世界模型</strong> 则以当前观测图像和SGG热图为条件，生成未来短时域的视频片段。从生成的视频中，可以零样本地直接提取出6-DoF末端执行器轨迹用于控制。通过将生成视频的最后一帧作为新的观测，系统可以以自回归、闭环的方式执行长时域任务。</p>
<p><img src="https://arxiv.org/html/2508.20840v3/x5.png" alt="分层长时域组合泛化流程"></p>
<blockquote>
<p><strong>图5</strong>：闭环、自回归规划流程。模型将生成的帧反馈作为输入，实现连续适应和长时域控制。高层VLM规划器将任务分解为原始动作序列并生成SGG热图；底层世界模型根据当前观测和热图生成视频；从中提取轨迹执行后，更新观测，循环进行。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>原始具身数据定义与组织</strong>：本文的核心基础是重新组织数据。一个<strong>原始动作</strong>被定义为一个有限时长的具身轨迹，与一个自然语言指令配对，需满足<strong>语义原子性</strong>（指令表达单一、连贯的操作意图）、<strong>时间局部性</strong>（时长较短，如≤2秒）和<strong>生成可行性</strong>。这种组织方式将长演示分解为多个标记好的原始动作，极大提高了数据密度和利用效率。</li>
<li><strong>双层次组合泛化</strong>：<ul>
<li><strong>原始动作内组合泛化</strong>：基于能量模型（EBM）视角，扩散模型通过加性能量分解隐式地学习了物体、动作、空间关系等语义因子的解耦表示。这使得模型能够将训练中见过的因子（如“抓取”动作和“罐子”物体）重新组合，生成未见过的组合（如“抓取罐子”）的连贯视频。</li>
<li><strong>原始动作间组合泛化</strong>：通过将训练好的原始动作世界模型作为可插拔的“技能库”，由VLM规划器在测试时灵活组合序列，即可实现对新长时域任务的零样本执行。规划器模型（𝒫_LoRA）同时用于数据自动标注，形成了“生成伪标签-人工修正-重新训练”的自我改进循环。</li>
</ul>
</li>
<li><strong>三阶段微调策略</strong>：为了将预训练的视频生成模型（基于DynamiCrafter）适配到具身任务，并平衡仿真-真实差异，采用了三阶段微调：<ul>
<li><strong>阶段1（仿真预微调）</strong>：在仿真数据上快速微调，注入机器人动力学等语义。</li>
<li><strong>阶段2（平衡混合）</strong>：以1:1比例混合仿真和真实数据训练，对齐跨域动态和外观。</li>
<li><strong>阶段3（真实中心精炼）</strong>：以80%真实和20%仿真数据的比例训练，专注于高保真的真实世界生成。</li>
</ul>
</li>
<li><strong>因果蒸馏与加速</strong>：为实现实时推理以适应闭环控制，采用知识蒸馏方法训练一个<strong>因果学生模型</strong>。该模型仅基于过去观测（而非未来上下文）以块为单位预测未来帧，每个块仅进行4步去噪。通过<strong>自强制</strong>技术，将模型自身的过去预测作为未来步骤的输入，实现了12 FPS的实时帧率。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>范式转变</strong>：从追求长时域视频生成转向短时域、原始动作级别的建模，从根本上解决了长时域预测的 ill-posed 问题和数据需求难题。</li>
<li><strong>数据与模型协同设计</strong>：强调并实践了从底层系统化设计数据策略（原始动作组织、多视角、全臂可见）与模型框架，而非简单利用现有零散数据集。</li>
<li><strong>零样本轨迹提取</strong>：通过高质量、短时域的视频生成，首次实现了无需任务特定适配或学习策略头，直接从生成视频中零样本提取6-DoF轨迹。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真环境<strong>RLBench</strong>的9个操作任务上进行评估，并在真实机器人上执行了“抓取杯子”、“移动布料”、“折叠布料”等任务。使用了<strong>LIBERO</strong>等仿真数据集以及自主收集的真实世界原始动作数据集进行训练。</p>
<p><strong>对比方法</strong>：主要与基于模仿学习或视频世界模型的基线方法对比，包括<strong>Image-BC</strong>、<strong>UniPi</strong>、<strong>4DWM</strong>以及强大的端到端VLA基线<strong>OpenVLA</strong>。</p>
<p><strong>关键定量结果</strong>：</p>
<ol>
<li><p><strong>RLBench任务成功率</strong>：如表1所示，PEWM在大多数任务上取得了最高的成功率，尤其在涉及关节和接触丰富的场景（如“打开抽屉”、“打开微波炉”）中表现一致地优于基线。</p>
<table>
<thead>
<tr>
<th align="left">Methods</th>
<th align="left">close box</th>
<th align="left">open drawer</th>
<th align="left">open jar</th>
<th align="left">open microwave</th>
<th align="left">put knife</th>
<th align="left">sweep dustpan</th>
<th align="left">to off</th>
<th align="left">weighing off</th>
<th align="left">water plants</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Image-BC</td>
<td align="left">53</td>
<td align="left">4</td>
<td align="left">0</td>
<td align="left">5</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">12</td>
<td align="left">21</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">UniPi</td>
<td align="left">81</td>
<td align="left">67</td>
<td align="left">38</td>
<td align="left">72</td>
<td align="left">66</td>
<td align="left">49</td>
<td align="left">70</td>
<td align="left">68</td>
<td align="left">35</td>
</tr>
<tr>
<td align="left">4DWM</td>
<td align="left">88</td>
<td align="left">80</td>
<td align="left">44</td>
<td align="left">70</td>
<td align="left">70</td>
<td align="left">56</td>
<td align="left">73</td>
<td align="left">62</td>
<td align="left">41</td>
</tr>
<tr>
<td align="left">Ours</td>
<td align="left"><strong>93</strong></td>
<td align="left"><strong>84</strong></td>
<td align="left">43</td>
<td align="left"><strong>78</strong></td>
<td align="left"><strong>72</strong></td>
<td align="left"><strong>63</strong></td>
<td align="left">67</td>
<td align="left">58</td>
<td align="left"><strong>56</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：在RLBench 9个任务上的整体成功率对比。PEWM方法（最后一行）在大多数任务上取得了最高成功率。</p>
</blockquote>
</li>
<li><p><strong>真实任务性能分解</strong>：如表2所示，PEWM在规划（原始动作准确率）、视频生成（帧真实性）和原始动作执行（任务成功率）三个阶段均表现良好。相比之下，OpenVLA在零样本设置下完全失败（0/20），凸显了PEWM无需任务特定微调即可泛化的优势。</p>
<table>
<thead>
<tr>
<th align="left">Task</th>
<th align="left">Stage</th>
<th align="left">Metric</th>
<th align="left">Ours</th>
<th align="left">OpenVLA</th>
<th align="left">OpenVLA (ZS)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Pick up cup</td>
<td align="left">Planning</td>
<td align="left">Primitive accuracy</td>
<td align="left">18 / 20</td>
<td align="left">N/A</td>
<td align="left">N/A</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Video Generation</td>
<td align="left">Frame realism (✓/total)</td>
<td align="left">17 / 20</td>
<td align="left">N/A</td>
<td align="left">N/A</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Primitive Execution</td>
<td align="left">Task success</td>
<td align="left">16 / 20</td>
<td align="left">12 / 20</td>
<td align="left">0 / 20</td>
</tr>
<tr>
<td align="left">Move cloth</td>
<td align="left">Planning</td>
<td align="left">Primitive accuracy</td>
<td align="left">16 / 20</td>
<td align="left">N/A</td>
<td align="left">N/A</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Video Generation</td>
<td align="left">Frame realism (✓/total)</td>
<td align="left">15 / 20</td>
<td align="left">N/A</td>
<td align="left">N/A</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Primitive Execution</td>
<td align="left">Task success</td>
<td align="left">14 / 20</td>
<td align="left">10 / 20</td>
<td align="left">0 / 20</td>
</tr>
<tr>
<td align="left">Fold cloth</td>
<td align="left">Planning</td>
<td align="left">Primitive accuracy</td>
<td align="left">15 / 20</td>
<td align="left">N/A</td>
<td align="left">N/A</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Video Generation</td>
<td align="left">Frame realism (✓/total)</td>
<td align="left">14 / 20</td>
<td align="left">N/A</td>
<td align="left">N/A</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Primitive Execution</td>
<td align="left">Task success</td>
<td align="left">13 / 20</td>
<td align="left">4 / 20</td>
<td align="left">0 / 20</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表2</strong>：三个真实世界任务在规划、视频生成和原始动作执行阶段的性能细分。</p>
</blockquote>
</li>
<li><p><strong>原始动作级组合泛化</strong>：如表3所示，模型在训练中未见过的（谓词，物体）组合上取得了高成功率（例如“抓取罐子”成功率为8/10，“打开杯子”为7/10），证明了其学习解耦语义表示并进行零样本组合的能力。</p>
<table>
<thead>
<tr>
<th align="left">Predicate \ Object</th>
<th align="left">cup</th>
<th align="left">box</th>
<th align="left">drawer</th>
<th align="left">jar</th>
</tr>
</thead>
<tbody><tr>
<td align="left">pick</td>
<td align="left">9/10</td>
<td align="left">8/10</td>
<td align="left">–</td>
<td align="left"><strong>8/10</strong></td>
</tr>
<tr>
<td align="left">open</td>
<td align="left"><strong>7/10</strong></td>
<td align="left">9/10</td>
<td align="left">8/10</td>
<td align="left">–</td>
</tr>
<tr>
<td align="left">push</td>
<td align="left"><strong>9/10</strong></td>
<td align="left">–</td>
<td align="left">–</td>
<td align="left"><strong>8/10</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表3</strong>：原始动作级组合泛化：未见过的（谓词，物体）配对上的成功率（成功次数/10次）。加粗单元格表示训练中未出现的组合。</p>
</blockquote>
</li>
</ol>
<p><strong>关键定性结果与消融</strong>：<br><img src="https://arxiv.org/html/2508.20840v3/x11.png" alt="仿真-真实混合数据生成质量对比"></p>
<blockquote>
<p><strong>图12</strong>：不同训练数据策略下的视频生成质量对比。纯真实数据训练时，模型难以重建快速移动的夹爪细节；纯仿真数据纹理简单；而采用本文的仿真-真实混合三阶段微调策略后，生成视频兼具真实数据的复杂纹理和仿真数据的清晰运动。</p>
</blockquote>
<ul>
<li><strong>仿真-真实混合策略有效性</strong>：如图12所示，单纯使用真实或仿真数据训练均有缺陷，而本文的三阶段混合策略成功结合了双方优势，生成了高质量视频。</li>
<li><strong>直接6-DoF轨迹提取</strong>：如图4所示，通过使用现成的6-DoF姿态估计器（Gen6D）处理生成视频，可以零样本地提取出平滑、精确的末端执行器运动轨迹，验证了生成视频的高时空保真度。</li>
<li><strong>因果蒸馏实现实时性</strong>：通过因果蒸馏，模型实现了12 FPS的实时帧率，使闭环控制成为可能。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出原始动作建模范式</strong>：系统性地论证并实践了将具身学习建立在短时域、语义原子的原始动作之上的新范式，为解决数据稀缺和长时域生成难题提供了根本性思路。</li>
<li><strong>数据与模型协同设计</strong>：强调了数据策略的基础性作用，提出了包含多视角、全臂可见、原始动作分割与标注的整套数据收集与组织方法，并与模型设计紧密结合。</li>
<li><strong>实现实时、可组合的闭环系统</strong>：通过因果蒸馏技术实现了实时视频预测，并结合VLM规划器构建了一个可零样本组合原始动作、支持长时域任务、并能从视频中直接提取控制信号的完整闭环系统。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法依赖于大规模预训练视频生成模型所提供的时空先验；同时，对于极其复杂或模糊的长时域任务，VLM规划器进行原始动作序列分解和SGG生成可能面临组合爆炸或长尾挑战。</p>
<p><strong>后续启示</strong>：</p>
<ol>
<li><strong>数据策略的进一步探索</strong>：如何更高效、自动化地构建大规模、高质量的原始动作数据集，以及如何更好地利用无动作标签的网络视频数据，是迈向规模化具身学习的关键。</li>
<li><strong>规划与表示的协同进化</strong>：原始动作的划分边界本身是柔性的，未来可探索如何让规划器与世界模型在交互中共同进化，动态地形成或调用不同粒度的“技能”。</li>
<li><strong>从视频到物理的桥梁</strong>：本文展示了从生成视频中直接解析物理信息（6-DoF轨迹）的可行性，这启发我们思考生成式世界模型如何更紧密地与物理引擎或物理推理模块结合，以增强其预测的物理合理性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对基于视频生成的具身世界模型严重依赖大规模交互数据、难以实现细粒度语言-动作对齐的问题，提出**原始具身世界模型（PEWM）**。该方法将视频生成限制在较短的固定时间范围内，并引入**模块化视觉语言模型（VLM）规划器**和**起止目标热图引导（SGG）机制**。其核心优势在于实现了细粒度的语言-视觉动作对齐，显著降低了学习复杂性和推理延迟，同时提高了数据收集效率，从而支持对复杂长时任务的组合泛化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.20840" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>