<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>UniDiffGrasp: A Unified Framework Integrating VLM Reasoning and VLM-Guided Part Diffusion for Open-Vocabulary Constrained Grasping with Dual Arms - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>UniDiffGrasp: A Unified Framework Integrating VLM Reasoning and VLM-Guided Part Diffusion for Open-Vocabulary Constrained Grasping with Dual Arms</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.06832" target="_blank" rel="noreferrer">2505.06832</a></span>
        <span>作者: Guo, Xueyang, Hu, Hongwei, Song, Chengye, Chen, Jiale, Zhao, Zilin, Fu, Yu, Guan, Bowen, Liu, Zhenze</span>
        <span>日期: 2025/05/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，面向任务的开集词汇抓取，特别是针对特定功能部件且需双臂协调的操作，仍是一个关键挑战。主流方法通常利用视觉语言模型（VLM）增强任务理解，但其输出的语义区域（如部件）往往传递给下游的通用抓取生成器（如GraspNet、AnyGrasp）。这种解耦导致抓取生成器可能无法高效地生成满足VLM所识别部件精确几何约束的抓取位姿，样本效率低，且在复杂几何上易失败。另一方面，专门的约束抓取生成方法（如VCGS）需要大量标注数据，难以适应开放词汇指令下的任意约束。虽然Constrained Grasp Diffusion Fields (CGDF) 提出的部件引导扩散策略能在无需重新训练的情况下实现高效的约束抓取，但其本身缺乏从开放词汇指令中定义约束区域的上游感知与推理能力。</p>
<p>本文针对上述痛点，提出了一种统一的新视角：将VLM的高级语义推理能力与CGDF的几何精确、高效的约束抓取生成能力紧密集成。核心思路是：利用VLM解析用户指令并识别语义目标（物体、部件、操作模式），通过开放词汇分割将其接地点几何约束，然后直接将这些约束用于引导CGDF的部件引导扩散策略，从而在无需针对特定约束重新训练的情况下，高效生成高质量、精确位于目标功能部件的6自由度抓取，并进一步扩展到双臂协调场景。</p>
<h2 id="方法详解">方法详解</h2>
<p>UniDiffGrasp是一个端到端的统一框架，其整体流程分为三个阶段：1）视觉语言抓取推理与接地；2）单臂抓取生成；3）双臂约束抓取（协调）。输入是用户自然语言指令（L）和RGB-D场景信息（图像I和深度），输出是针对指定功能部件的单臂或双臂6-DoF抓取位姿。</p>
<p><img src="https://arxiv.org/html/2505.06832v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：UniDiffGrasp框架总览。用户输入（语言和视觉）首先由VLM（GPT-4o）处理，解析出目标物体、功能部件和操作模式。接着通过多阶段分割（Grounded SAM用于物体定位，VLPart用于部件识别）将这些语义目标接地点几何约束（点云）。最后，这些约束指导部件引导扩散策略，在单臂模式下生成抓取，或在双臂模式下定义不同目标区域并选择稳定的协作抓取对。</p>
</blockquote>
<p><strong>核心模块一：视觉语言抓取推理与接地</strong>。该模块负责将开放词汇指令转化为几何约束。首先，VLM（如GPT-4o）分析语言指令L和场景图像I，输出结构化参数：目标物体（O）、目标功能部件（p*）和操作模式（单臂/双臂）。随后进行分层视觉接地：1) <strong>物体分割</strong>：使用Grounded SAM，以O为提示词，从I中分割出目标物体掩码Ma，并利用深度数据构建全局物体点云P。2) <strong>部件分割</strong>：将仅包含Ma的掩码视图MMa和部件描述p<em>输入给VLPart，预测物体边界内精确的部件掩码Mp</em>，进而构建目标区域点云Pt（Pt ⊆ P）。这种“物体→部件”的分层策略避免了在干扰物体上错误分割相似部件。</p>
<p><strong>核心模块二：抓取生成与估计（单臂）</strong>。当模式为“单臂”时，此模块利用Pt作为几何约束，在全局物体P上生成抓取。其核心是直接应用CGDF的<strong>部件引导扩散</strong>策略。CGDF将稳定、无碰撞的抓取位姿分布建模为SE(3)上的能量场。其创新点在于，一个仅在无约束数据上训练的模型，在推理时可以通过“部件引导扩散”生成约束于目标区域Pt的抓取。具体而言，在扩散过程的每一步k，对候选抓取Hk进行<strong>双重能量评估</strong>：分别计算其相对于全局物体P的能量ek&#39;和相对于局部部件Pt的能量ek&#39;&#39;。引导能量ek,guided = max(ek&#39;, ek&#39;&#39;)，对应的条件得分向量sθ,guided会动态调整，引导位姿同时满足全局有效性和局部约束。UniDiffGrasp以此策略迭代优化初始随机抓取，最终生成一组集中于Pt附近的候选抓取{Hgen}，并从中选择全局能量Eθ最低的作为最终抓取H*。</p>
<p><strong>核心模块三：大型物体约束抓取：双臂协调</strong>。当VLM判定为“双臂”模式时，框架扩展上述方法以实现协调操作。</p>
<p><img src="https://arxiv.org/html/2505.06832v1/x3.png" alt="双臂生成原理"></p>
<blockquote>
<p><strong>图3</strong>：UniDiffGrasp中基于协调部件引导扩散的双臂抓取生成原理。以带双把手的锅为例，展示了定义两个目标区域、独立生成抓取候选集、并通过评估选择最优抓取对的过程。</p>
</blockquote>
<p>首先，<strong>定义双目标区域</strong>（Pt1, Pt2）。方法灵活：若VLM暗示两个功能上不同的交互点（如“两个把手”），则使用语义分割（如VLPart）分别定位这两个部件；否则（如指令为“抓键盘”），则对全局物体点云P或已识别的大部件Pt进行几何分割（如左右分）。接着，<strong>独立约束抓取生成</strong>：对每个手臂，分别以其目标区域（Pt1或Pt2）为约束，独立运行上述部件引导扩散过程，为左臂和右臂各生成N个候选抓取，构成N×N个抓取对。最后，<strong>最优双臂抓取对选择</strong>：从所有抓取对中，选择一个在<strong>双目标函数</strong>下最优的对(H1*, H2*)。该目标函数综合考虑：1）每个抓取在其自身目标区域内的质量（能量低）；2）两个抓取之间的协调稳定性（如力闭合可行性、相对位姿）；3）双臂的运动学可行性（无自碰撞、可达）。通过评估所有候选对，选择综合评分最高的作为最终的双臂抓取方案。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在<strong>真实机器人平台</strong>上进行，使用了包含多种日常物体（锅、水壶、螺丝刀、键盘等）的场景。评估指标主要为<strong>抓取成功率</strong>。</p>
<p><strong>对比的基线方法</strong>包括：1) <strong>AffordGrasp</strong>：先进的VLM驱动抓取方法，使用VLM进行功能接地，然后调用通用抓取生成器（AnyGrasp）。2) **CGDF (w/ Post-Filtering)**：使用CGDF生成无约束抓取，然后后过滤出落在VLM分割出的部件区域内的抓取。3) <strong>VCGS</strong>：需要针对特定约束训练的学习型约束抓取方法。</p>
<p><img src="https://arxiv.org/html/2505.06832v1/x4.png" alt="性能对比"></p>
<blockquote>
<p><strong>图4</strong>：UniDiffGrasp与基线方法在单臂和双臂场景下的抓取成功率对比。UniDiffGrasp在单臂和双臂任务上均显著优于所有基线。</p>
</blockquote>
<p><strong>关键实验结果</strong>：UniDiffGrasp在<strong>单臂</strong>场景中取得了<strong>0.876</strong>的抓取成功率，在<strong>双臂</strong>场景中取得了<strong>0.767</strong>的成功率。与最强基线AffordGrasp相比，单臂成功率相对提升了**23.3%<strong>，双臂成功率相对提升了</strong>27.4%**。这证明了其集成框架在生成精确部件抓取和稳定双臂协调方面的优越性。CGDF (后过滤)方法成功率较低，凸显了部件引导扩散策略相对于简单后过滤的样本效率优势。VCGS由于泛化性限制，在开放词汇新物体上表现不佳。</p>
<p><img src="https://arxiv.org/html/2505.06832v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融实验研究。(a) 比较了不同分割策略对抓取成功率的影响，使用“物体→部件”分层分割（Ours）效果最佳。(b) 展示了部件引导扩散策略相比标准CGDF在生成抓取的空间分布上更集中于目标部件。(c) 验证了双臂抓取对选择中不同优化目标（仅个体质量、个体+协调、个体+协调+运动学）对最终成功率的影响，综合优化最佳。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>分层分割策略</strong>：对比仅用VLPart直接分割部件、仅用Grounded SAM分割物体后取全部点云作为约束等方法，论文采用的“Grounded SAM (物体) → VLPart (部件)”分层策略取得了最高成功率，证明了其在杂乱场景中精准定位部件的重要性。</li>
<li><strong>部件引导扩散的有效性</strong>：可视化显示，使用部件引导扩散生成的抓取紧密聚集在目标部件上，而标准CGDF或无约束生成后过滤的抓取则分布分散，证明了该策略在约束满足上的高效性。</li>
<li><strong>双臂选择策略</strong>：实验表明，仅优化单个抓取质量（能量）不足以保证双臂任务成功；结合协调稳定性（力闭合）和运动学可行性进行综合选择，才能取得最高的双臂抓取成功率。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个端到端统一框架，将先进的VLM任务理解与基于扩散的约束抓取生成无缝衔接，实现了开放词汇下对指定功能部件的精确单臂与双臂抓取。</li>
<li>创新性地将VLM推理出的语义约束直接作为几何输入，引导CGDF的部件引导扩散策略，实现了<strong>零样本</strong>、高效、高质量的约束抓取生成，无需针对新约束重新训练。</li>
<li>提出了一种原则性的双臂功能部件抓取方法，包含灵活的目标区域定义、基于部件引导扩散的独立抓取生成以及考虑协调稳定性和运动学的抓取对选择机制。</li>
</ol>
<p><strong>局限性</strong>：论文提到，系统的性能在一定程度上依赖于上游VLM推理和开放词汇分割的准确性，错误的语义理解或分割会直接影响后续抓取生成。此外，虽然扩散过程高效，但整个管道（尤其是使用大型VLM）的计算成本仍高于某些传统方法。</p>
<p><strong>对后续研究的启示</strong>：UniDiffGrasp展示了将高层语义推理与底层几何生成紧密集成的强大潜力。未来工作可以探索更轻量或专用于机器人的VLM以提升效率，将框架扩展至更复杂的多步骤操作任务，或研究如何将双臂协调策略泛化到更多机器人（如多指灵巧手）的协同操作中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对开放词汇环境下，双臂机器人需根据任务要求抓取物体特定功能部件的难题，提出统一框架UniDiffGrasp。该框架集成视觉语言模型（VLM）进行语义推理与目标识别，并利用其引导的部件扩散技术，通过约束抓取扩散场（CGDF）生成符合几何约束的6自由度抓取位姿。实验表明，该框架在真实场景中单臂抓取成功率达87.6%，双臂达76.7%，性能显著优于现有方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.06832" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>