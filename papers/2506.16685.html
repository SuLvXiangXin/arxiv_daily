<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.16685" target="_blank" rel="noreferrer">2506.16685</a></span>
        <span>作者: Shuran Song Team</span>
        <span>日期: 2025-06-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人学习领域，通过人类演示进行模仿学习（如行为克隆）已取得诸多成功。然而，要获得成功的策略，通常需要反复部署、观察失败并收集更多数据进行更新，这个过程被称为数据集聚合（DAgger）。但在真实世界的接触丰富操作任务中，有效实施DAgger面临两大关键挑战：1) <strong>如何收集信息丰富的人类校正数据？</strong> 现有方法（如离线重录演示或人类完全接管控制）可能导致数据偏离原始策略的状态-动作分布，或由于控制权突然切换引入力的不连续性，影响数据质量。2) <strong>如何利用新数据有效更新策略？</strong> 主流方法包括使用聚合数据集从头训练（计算成本高）、仅用新数据微调（对新数据质量敏感且易受分布偏移影响），或通过强化学习/模仿学习训练残差策略（通常需要大量样本）。</p>
<p>本文针对上述痛点，提出了一个名为“合规残差DAgger”（CR-DAgger）的改进系统。其核心思路是：通过一个基于合规控制的<strong>合规干预接口</strong>，允许人类在不中断机器人策略执行的情况下提供平滑、精确的增量动作校正；并设计一个<strong>合规残差策略</strong>，仅利用少量校正数据，学习结合力反馈的校正行为，从而高效提升基础策略在接触丰富任务上的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>CR-DAgger 旨在利用少量人类校正数据改进预训练的机器人策略（称为“基础策略”）。其整体框架包含两个核心组件：用于数据收集的合规干预接口，以及用于策略更新的合规残差策略。</p>
<p><img src="https://arxiv.org/html/2506.16685v5/fig/teaser_v6.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：CR-DAgger 系统概览。(a) 合规干预接口用于收集人类校正数据。(b) 使用该数据更新合规残差策略。(c) 在真实世界的两项接触丰富操作任务上部署更新后的策略。</p>
</blockquote>
<h3 id="核心模块一：合规干预接口">核心模块一：合规干预接口</h3>
<p>该接口旨在解决传统干预方法（如接管控制）带来的非平滑过渡、分布偏移、间接校正误差和信息缺失等问题。其具体设计如下：</p>
<ol>
<li><strong>增量校正而非接管校正</strong>：机器人策略持续执行，人类通过安装在末端执行器上的手柄施加力，从而在策略动作之上产生“增量动作”。人类可以通过触觉反馈感知策略的原始意图，并通过施加力的大小轻松控制干预幅度，这确保了数据的平滑性并限制了过大的校正。</li>
<li><strong>基于合规控制的交互接口</strong>：系统运行导纳控制器，在基础策略输出位置 <code>q_ref</code> 周围形成一个虚拟的弹簧-质量-阻尼系统（公式1：<code>Mq¨ = K(q_ref - q) - Dq˙ + F</code>）。其中 <code>F</code> 包含外部接触力和人类校正力。通过设置适当的刚度（约1000 N/m），使人类能够轻松干预但又不完全覆盖策略执行。</li>
<li><strong>带按钮和力传感器的校正记录</strong>：接口包含一个六维力传感器直接测量接触力，以及一个位于手柄上的单键键盘来精确记录校正开始/结束的时刻。同时记录策略的原始命令、人类的增量校正以及交互过程中的力传感器读数。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.16685v5/fig/data_collection_v2.jpg" alt="干预接口"></p>
<blockquote>
<p><strong>图2</strong>：合规干预接口的硬件设置。人类手持手柄施加力来校正机器人执行，提供在策略的增量校正。</p>
</blockquote>
<p>此外，论文识别并解决了“意图误解”问题：当人类校正运动小于机器人跟踪误差时，计算出的校正方向可能与人类真实意图相反。为此，作者在之前会议版本的基础上增加了两项改进：1) 在导纳控制律中加入速度跟踪项（公式2），以减少空间跟踪误差；2) 在计算校正运动 <code>q_delta</code> 时，为机器人反馈添加前瞻时间（公式3：<code>q_delta[t] = q[t] - q_ref[t - Δt]</code>），以减少时间（延迟）跟踪误差。这些改进将最大跟踪误差从30毫米降至5毫米以下。</p>
<p><img src="https://arxiv.org/html/2506.16685v5/x1.png" alt="意图误解"></p>
<blockquote>
<p><strong>图3</strong>：由跟踪误差引起的意图误解示意图。当跟踪误差较大时（B），记录的校正运动方向可能是错误的。</p>
</blockquote>
<h3 id="核心模块二：合规残差策略">核心模块二：合规残差策略</h3>
<p>给定校正数据，常见的策略更新方法（从头训练、微调）存在成本高、稳定性差或无法引入新输入/输出的局限。本文提出一个仅在校正数据上训练的合规残差策略。</p>
<p><img src="https://arxiv.org/html/2506.16685v5/x2.png" alt="策略更新对比"></p>
<blockquote>
<p><strong>图4</strong>：策略更新方法对比。左图：常见的从头训练和微调方法。右图：我们的方法。基础策略（1 Hz）预测末端执行器位姿。合规残差策略（50 Hz）接收额外的力输入，预测增量位姿和目标力/力矩。</p>
</blockquote>
<p><strong>策略公式与架构</strong>：如图4所示，合规残差策略接收与基础策略相同的视觉和本体感知输入（但时间窗口更短），并额外接收力模态输入。它以50 Hz频率运行，一次性输出5帧动作（对应0.1秒）。动作空间为15维：前9维表示从基础策略动作到机器人位姿命令的SE(3)增量位姿；后6维表示机器人应从外部接触中感受到的预期力/力矩（扳手）。位姿命令和预期扳手都被发送给导纳控制器执行。网络使用基础策略冻结的图像编码器提取图像嵌入，用时序卷积网络编码力向量，最后通过全连接层解码动作。整个可训练参数量仅约2MB。</p>
<p><strong>训练策略</strong>：为了仅用少量真实世界校正数据（50-100条轨迹）有效训练残差策略，作者采用以下策略：1) <strong>确保分布内数据的充分覆盖</strong>：在训练数据中纳入“无校正”的数据段（标记为零残差动作），并收集一些演示者始终握住手柄、将整条轨迹标记为校正（即使校正很小或为零）的轨迹，以帮助策略理解何时不提供校正。2) <strong>优先处理校正数据</strong>：在训练中，对校正开始后短时间内的数据提高采样频率，因为这些时刻标志着当前策略表现不佳且需要立即修复。</p>
<p><strong>优势</strong>：1) <strong>样本高效学习</strong>：轻量网络配合少量数据即可训练。2) <strong>整合新传感器模态</strong>：可以将任何基于位置的预训练策略通过收集带力模态的校正数据，转变为力感知策略。3) <strong>高频推理</strong>：高频运行结合力反馈，能实现反应性校正行为，这对接触事件中的纠错尤为重要。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在四个具有长视野和连续接触的挑战性真实世界任务上进行评估：书本翻转、皮带组装、电缆布线和齿轮插入。对于每个任务，首先使用150-400条演示训练一个扩散策略作为基础策略。然后，使用不同数据收集方法从同一基础策略收集50-100条校正轨迹。接着，使用不同的网络更新方法和训练程序更新策略。最后，在相同的测试案例下部署更新后的策略并评估其性能。</p>
<p><strong>对比基线</strong>：</p>
<ul>
<li><strong>数据收集方法</strong>：离线策略校正（Off-policy）、在策略接管校正（On-policy Takeover）、在策略增量校正（On-policy Delta，即本文方法）。</li>
<li><strong>策略更新方法</strong>：从头训练（Retrain）、微调（Finetune）、残差策略（Residual，即本文方法）。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.16685v5/fig/book-task-figure-2.jpg" alt="书本翻转任务"></p>
<blockquote>
<p><strong>图5</strong>：书本翻转任务。(a) 使用[在策略增量]数据训练的[合规残差]策略的 rollout，展示了精确的插入动作和有力的推动策略。(b) 不同的测试场景。(c) 基础策略的典型失败案例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.16685v5/x3.png" alt="皮带组装任务"></p>
<blockquote>
<p><strong>图6</strong>：皮带组装任务。(a) 使用[在策略增量]数据训练的[合规残差]策略的 rollout，展示了精确的力-位协调和适应能力。(b) 不同的测试场景。(c) 基础策略的典型失败案例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.16685v5/x4.png" alt="电缆布线任务"></p>
<blockquote>
<p><strong>图7</strong>：电缆布线任务。(a) 使用[在策略增量]数据训练的[合规残差]策略的 rollout。(b) 基础策略的典型失败案例：错过一个夹子。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.16685v5/x5.png" alt="齿轮插入任务"></p>
<blockquote>
<p><strong>图8</strong>：齿轮插入任务。(a) 使用[在策略增量]数据训练的[合规残差]策略的 rollout。(b) 基础策略的典型失败案例：齿轮与轴未对齐，未能与邻近齿轮啮合。</p>
</blockquote>
<p><strong>关键定量结果</strong>：<br>在四个任务上，CR-DAgger（On-policy Delta + Compliant Residual） consistently 取得了最佳性能。</p>
<p><img src="https://arxiv.org/html/2506.16685v5/x6.png" alt="成功率汇总"></p>
<blockquote>
<p><strong>图9</strong>：四个任务上的平均成功率。CR-DAgger 将基础策略的成功率相对提高了64%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.16685v5/x7.png" alt="各任务详细结果"></p>
<blockquote>
<p><strong>图10</strong>：各任务详细成功率。在所有任务上，本文方法（橙黄色条）均优于其他数据收集与策略更新方法的组合。</p>
</blockquote>
<p><strong>消融实验与分析</strong>：</p>
<ol>
<li><strong>数据收集方法消融</strong>：如图10所示，在策略增量校正（On-policy Delta）数据在不同策略更新方法下通常优于离线策略校正和在策略接管校正数据，验证了其质量更高。</li>
<li><strong>训练策略消融</strong>：在皮带组装任务上，对残差策略训练策略进行消融。<br><img src="https://arxiv.org/html/2506.16685v5/x8.png" alt="训练策略消融"><blockquote>
<p><strong>图11</strong>：训练策略消融研究。使用“无校正数据”和“优先校正数据”两种策略（即本文完整训练策略）能获得最佳性能。</p>
</blockquote>
</li>
<li><strong>力输入的作用</strong>：在皮带组装任务上对比了带/不带力输入的基础策略，以及使用力/仅位置输入的残差策略。<br><img src="https://arxiv.org/html/2506.16685v5/x9.png" alt="力输入消融"><blockquote>
<p><strong>图12</strong>：力输入消融研究。即使基础策略是仅位置的，通过合规残差策略引入力输入也能显著提升性能。</p>
</blockquote>
</li>
<li><strong>跟踪误差的影响</strong>：展示了减少跟踪误差前后，在齿轮插入任务上的性能提升。<br><img src="https://arxiv.org/html/2506.16685v5/x10.png" alt="跟踪误差影响"><blockquote>
<p><strong>图13</strong>：跟踪误差分析。减少跟踪误差（改进后）能显著提升使用小幅度校正运动任务（齿轮插入）的性能。</p>
</blockquote>
</li>
<li><strong>数据质量分析</strong>：对比了不同数据收集方法下，校正动作幅度与任务成功率的关系。<br><img src="https://arxiv.org/html/2506.16685v5/x11.png" alt="数据质量分析"><blockquote>
<p><strong>图14</strong>：校正数据质量分析。在策略增量校正产生的数据，其校正幅度更小、更集中，且与更高的任务成功率相关。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>合规干预接口</strong>：提出了一种新颖的在策略增量校正系统，基于导纳控制实现，允许人类在不中断运行中机器人策略的情况下，提供精确、轻柔、平滑的位姿和力校正，并记录了关键的力反馈信息。</li>
<li><strong>合规残差策略</strong>：提出了一种高效的残差策略公式，能够整合额外的力模态输入，并预测增量运动和目标力。该策略轻量、样本高效，并能将仅位姿的基础策略转变为力感知策略。</li>
<li><strong>实践指南与深入分析</strong>：通过大量真实世界实验，为高效实施DAgger提供了关于批量大小、采样策略等关键设计选择的实用指导，并深入分析了意图误解、跟踪误差、数据质量等对性能的影响。</li>
</ol>
<p><strong>局限性</strong>：论文提到，系统的性能依赖于基础策略输出的质量（作为导纳控制的参考），且力传感器需要校准。此外，虽然所需数据量已大幅减少，但仍需约50-100条真实世界校正轨迹。</p>
<p><strong>启示</strong>：CR-DAgger 为真实世界机器人学习中的高效人机交互与策略迭代提供了一个切实可行的框架。其核心思想——通过合规控制实现平滑、增量的人机协作，并利用轻量级残差网络快速吸收校正经验——可广泛应用于需要精细接触操作或持续适应性的机器人任务。对于未来研究，如何进一步减少所需的人类校正量，或将此框架与更强大的基础策略（如大规模预训练模型）结合，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对真实世界接触丰富操作任务中应用DAgger方法的两大挑战：如何收集高质量的人类纠正数据，以及如何利用这些数据高效更新策略。提出了顺从残差DAgger（CR-DAgger）方法，其核心包含两个创新组件：1）顺从干预接口，利用顺从控制实现不中断策略执行下的精准增量动作纠正；2）顺从残差策略，能结合力反馈从人类纠正中学习。实验表明，该方法仅需少量纠正数据即可大幅提升性能，在书页翻转、皮带装配等四个挑战性任务上，将基础策略成功率提升了64%，且优于从头训练与微调方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.16685" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>