<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Unified Reinforcement and Imitation Learning for Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Unified Reinforcement and Imitation Learning for Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.19307" target="_blank" rel="noreferrer">2510.19307</a></span>
        <span>作者: Yueh-Hua Wu Team</span>
        <span>日期: 2025-10-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前提升视觉语言模型性能的主流方法包括：扩大模型规模、扩充视觉指令调优数据集、进行架构修改（如集成辅助视觉或推理模块）以及采用“思考-回答”范式并结合强化学习。然而，这些方法往往导致模型参数量庞大、推理延迟增加、内存需求高，使其难以部署在智能手机、AR设备等资源受限的环境中。因此，研究界亟需一种能在保持强大视觉语言能力的同时，实现低推理延迟和轻量级模型设计的方案。</p>
<p>本文针对上述痛点，提出了一种无需修改架构、也无需冗长“思考”过程的高效训练方法。核心思路是：将强化学习与对抗模仿学习统一起来，通过一个基于LLM的判别器引导轻量级学生模型模仿大型教师模型的文本生成风格，并结合答案正确性奖励，系统地提升学生模型的生成能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>RIL 是一个统一的训练算法，它结合了强化学习（具体是 Dr.GRPO）和生成对抗模仿学习（GAIL）框架。其目标是训练一个轻量级的学生 VLM（生成器），使其生成的文本响应在风格和内容上都接近或超越大型教师 VLM。</p>
<p><img src="https://arxiv.org/html/2510.19307v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：RIL方法整体框架示意图。展示了学生VLM（生成器）接收图像和问题，生成多个响应。这些响应与来自（多个）教师VLM的响应一同输入给判别器和LLM-as-a-Judge，分别产生相似性奖励和答案奖励。结合后的总奖励用于通过Dr.GRPO算法更新学生VLM。</p>
</blockquote>
<p>整体流程始于对学生VLM的监督微调（SFT）预热。随后，RIL循环进行，每个迭代包含两个关键步骤：1）更新判别器以区分学生和教师的响应；2）使用结合了相似性奖励和答案奖励的总奖励，通过 Dr.GRPO 目标更新学生VLM。</p>
<p>核心模块包括：</p>
<ol>
<li><strong>学生VLM（生成器，π_θ）</strong>：待优化的轻量级模型（如Qwen2.5-VL-7B， InternVL3-8B）。</li>
<li><strong>教师VLM</strong>：提供模仿目标的大型高性能模型（如Qwen2.5-VL-72B， InternVL3-78B）。论文强调使用多个教师能带来更丰富的响应多样性，从而提升性能。</li>
<li><strong>基于LLM的判别器（D_φ）</strong>：这是模仿学习的核心。其架构与学生VLM的语言主干相同，但将语言头替换为一个线性判别头（ℝ^(d×1)），用于输出标量分数。判别器通过最大化公式(1)的目标进行预训练，学习为学生响应输出接近1的分数，为教师响应输出接近0的分数。在RIL中，其输出被<strong>二元化</strong>（分数&lt;0.5视为类似教师），作为相似性奖励，以提供更清晰、稳定的学习信号。</li>
<li><strong>LLM-as-a-Judge</strong>：用于评估学生生成答案的事实正确性。它不参与训练，仅根据问题、真值答案和生成响应，输出一个二元化的答案奖励。</li>
</ol>
<p>奖励设计是RIL的关键创新。总奖励 R(q, o_i) 是两种二元奖励的和：</p>
<ul>
<li><strong>相似性奖励</strong>：𝟙(D_φ(q, o_i) &lt; 0.5)，鼓励学生模仿教师的文本风格。</li>
<li><strong>答案奖励</strong>：LLM-as-a-Judge(q, a, o_i)，确保生成内容的正确性。</li>
</ul>
<p>学生VLM的更新采用 Dr.GRPO 的目标函数（公式2），该函数在标准GRPO基础上提供了无偏的优势估计。更新时，策略梯度计算同时基于学生生成的响应和教师提供的响应（共2G个），这为学生提供了更清晰的学习范例，有助于稳定训练并可能实现超越教师的表现。</p>
<p><img src="https://arxiv.org/html/2510.19307v1/x3.png" alt="判别器输入提示"></p>
<blockquote>
<p><strong>图3</strong>：判别器（D_φ）的输入提示格式。它将问题和候选响应组织成特定的文本格式，以便基于语言的判别器进行处理和评分。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.19307v1/x4.png" alt="评估器输入提示"></p>
<blockquote>
<p><strong>图4</strong>：LLM-as-a-Judge 的输入提示格式。用于评估生成响应（Prediction）与真值答案（Ground Truth）在语义上是否一致，并输出“正确”或“错误”。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在多达14个主流视觉语言评测基准上进行了验证，包括 AI2D、ChartQA、MathVista、MMB、MM-Vet、MMMU、MMMU-Pro、MMStar、BLINK、SEED、SEED2+ 和 RealWorldQA。学生模型为 Qwen2.5-VL (3B/7B) 和 InternVL3 (1B/2B/8B)，教师模型为 Qwen2.5-VL-72B 和 InternVL3-78B。LLM-as-a-Judge 采用 Qwen2.5-32B。</p>
<p><strong>基线对比</strong>：对比了仅使用答案奖励的强化学习基线（GRPO 及其高级版本 Dr.GRPO），以及众多开源和闭源的SOTA VLM。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>RIL显著提升性能</strong>：如表1所示，在几乎所有模型和数据集上，RIL（Dr.GRPO+GAIL）带来的性能提升远超单纯的RL训练。例如，Qwen2.5-VL-7B 在 ChartQA 上从 87.3% 提升至 95.4%，在 MathVista 上从 67.8% 提升至 74.5%，在 MMMU 上从 55.0% 提升至 61.8%。</li>
<li><strong>多教师策略的有效性</strong>：表2的消融实验表明，同时使用 Qwen2.5-VL-72B 和 InternVL3-78B 作为教师，相比使用单一教师，能带来更显著的性能提升。例如，Qwen2.5-VL-3B 在 MathVista 上使用双教师达到 71.7%，高于使用任一单教师（~65.2%）。</li>
<li><strong>与SOTA模型竞争</strong>：如表3和图2所示，经过RIL训练的小型模型（如7B/8B）在多个基准的平均性能上，能够与甚至超越许多更大的开源模型（如16B级别）以及领先的闭源模型（如GPT-4o, Gemini, Claude-3.5 Sonnet）。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.19307v1/x2.png" alt="性能对比雷达图"></p>
<blockquote>
<p><strong>图2</strong>：RIL应用后的VLM（基于多教师）与各类开源、闭源SOTA VLM在多个基准平均性能上的对比雷达图。显示RIL训练的小模型能达到顶尖竞争力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>模仿学习组件（相似性奖励）</strong>：是性能超越纯RL的关键。</li>
<li><strong>多教师策略</strong>：通过提供响应多样性，增强了判别器的判别能力，从而提升了学生模型的最终表现。</li>
<li><strong>答案奖励</strong>：防止模型只模仿风格而忽略正确性，是保证性能不退化的重要组件。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>统一强化与模仿学习框架</strong>，首次将对抗模仿学习（GAIL）与策略梯度强化学习（Dr.GRPO）相结合用于VLM训练，使轻量级学生模型能有效模仿并可能超越大型教师模型的文本生成能力。</li>
<li>设计了<strong>双元奖励机制</strong>（风格相似性+事实正确性），并创新性地将判别器输出二元化，解决了训练稳定性和目标模糊性问题。</li>
<li>证明了方法的<strong>高效性与通用性</strong>：训练后的模型无需“思考”步骤，保持了原始推理速度；方法独立于具体的图像编码器或分词器，且借助LLM-as-a-Judge能广泛应用于开放域视觉问答。</li>
</ol>
<p><strong>局限性</strong>：论文提到，RIL的性能提升依赖于教师模型的质量和多样性。同时，预训练判别器和运行LLM-as-a-Judge需要额外的计算资源。</p>
<p><strong>后续启示</strong>：RIL为开发高性能、轻量化的VLM提供了一条新路径。其“模仿风格+确保正确”的双奖励思想可扩展到其他生成式模型的蒸馏与强化中。如何选择最优的教师模型组合、进一步降低判别器训练成本，以及将框架适配到更复杂的多轮对话任务，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对大规模视觉语言模型在资源受限环境中不实用的问题，提出统一强化与模仿学习算法。该方法结合强化学习与对抗模仿学习，利用LLM判别器区分师生输出，并引入多教师模型提供多样指导，使学生模型既能模仿教师生成，又能通过强化信号提升能力。实验表明，该算法使轻量级模型在多个视觉语言基准上性能显著提升，缩小了与顶尖开源及闭源模型的差距，并在部分任务上实现超越。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.19307" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>