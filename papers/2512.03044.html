<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.03044" target="_blank" rel="noreferrer">2512.03044</a></span>
        <span>作者: Shanghang Zhang Team</span>
        <span>日期: 2025-12-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，机器人策略学习的主流方法主要依赖于视觉语言动作（VLA）模型，这些模型通常使用静态图像编码器（如CLIP、SigLIP）提取视觉特征，然后与语言指令融合来指导动作生成。然而，这类方法缺乏对时序动态和因果依赖的建模能力。近期，视频扩散模型（VDM）因其在建模物理世界动态方面的强大能力而受到关注，一些工作开始尝试将VDM的原始表征直接引入策略学习以增强场景理解。但现有方法普遍忽视了VDM原始特征中天然编码的、连贯且物理一致的空间结构和运动信息，这些信息本可作为更高效、更具信息量的条件来指导动作学习。</p>
<p>本文针对这一具体痛点，提出应显式地挖掘和利用VDM中蕴含的空间与运动表征。核心思路是：首先分析并验证VDM特征在机器人场景下对前景物体结构和运动一致性的捕捉能力；然后设计一种异步双系统框架，其中慢速的VDM系统（System 2）显式提取并精炼空间与运动表征，作为条件输入给高速的扩散Transformer动作头（System 1），从而实现实时、稳定且适应性强的机器人控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>Video2Act的整体框架是一个异步双系统设计，旨在高效整合VDM的时空感知能力与实时动作生成需求。</p>
<p><img src="https://arxiv.org/html/2512.03044v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：Video2Act框架。采用异步双系统框架，包含一个慢速感知VDM（System 2）和一个快速动作头（System 1）。System 2从两种图像输入中提取精炼的空间和运动表征：高分辨率图像用于通过Sobel算子进行空间滤波，长时序序列用于通过FFT进行运动提取。这些低频的时空运动特征作为条件输入给System 1，System 1同时接收高频更新的图像token。通过交叉注意力条件机制，这些异步更新的信号被有效融合，实现鲁棒的实时动作生成。</p>
</blockquote>
<p><strong>整体流程与输入输出</strong>：给定当前时刻t，输入包括一段历史图像序列 (I_{t-T:t})、机器人状态 (s_t) 和语言指令 (l)。慢系统（System 2）以较低频率运行，接收图像序列，输出精炼后的空间特征 (S_t) 和运动特征 (M_t)。快系统（System 1）以控制频率（如30Hz）运行，每步接收最新的图像（通过SigLIP编码为token）、语言指令token以及来自System 2的最新时空运动特征，输出未来H步的动作序列 (a_{t+1:t+H}) 用于机器人控制。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>VDM特征提取与精炼（System 2）</strong>：采用Hunyuan视频扩散Transformer作为VDM骨干。为了获取干净的特征并避免去噪伪影，论文采用基于反转的特征提取策略，在去噪步长 (t_{\text{diff}}=0) 时提取特征（公式1）。为了同时捕捉细粒度空间细节和长时程运动动态，System 2并行处理两个视频流：一个短窗口（(T_s=2)）的高分辨率（512x768）图像流用于空间分析；一个长窗口（(T_l=16)）的常规分辨率（256x256）图像流用于运动分析（公式2）。</p>
<ul>
<li><strong>空间结构表征（Sobel）</strong>：对高分辨率特征图 (F^H) 的每个通道，应用标准的3x3 Sobel算子计算水平和垂直梯度，最终得到梯度幅度图 (S_t)（公式3及后续推导）。该操作能增强相邻帧间结构感知的一致性，突出前景物体的边界，同时过滤背景噪声。</li>
<li><strong>运动表征（FFT）</strong>：对长序列特征图 (F^L) 的每个空间位置和通道，沿时间轴进行一维离散傅里叶变换（DFT）。然后应用一个高通滤波器（频率掩码 (\mathcal{B})），再通过逆DFT重构滤波后的序列，得到 (M_t)（公式3及后续推导）。该操作抑制了低频背景成分，突出了机器人手臂和被操纵物体随时间变化的连贯运动模式。</li>
<li><strong>特征压缩</strong>：提取到的 (S_t) 和 (M_t) 通过两个轻量级Q-former进行压缩，以减少token冗余并保持全局一致性，然后拼接为 (F_{VDM})。</li>
</ul>
</li>
<li><p><strong>扩散Transformer动作头（System 1）</strong>：采用一个10亿参数的扩散Transformer（DiT）作为动作解码器。其输入包括：由SigLIP编码的实时图像token (F_I)、由文本编码器生成的语言指令token (F_l)，以及来自System 2的时空运动条件token (F_{VDM})。这些条件通过交叉注意力层注入到DiT的各个块中。训练目标为标准去噪损失（公式4），即预测添加到动作序列上的高斯噪声。</p>
</li>
<li><p><strong>异步双系统机制</strong>：这是实现高效推理的关键创新。System 2（VDM）作为慢速感知模块（例如每秒更新1次），进行高层上下文推理并提取时空运动特征。System 1（DiT）作为快速执行模块（例如每秒生成30次动作），在每一步都利用最新的观测生成动作，但同时持续受到System 2周期性更新的特征条件所引导。这种设计允许在大幅降低VDM调用频率（从而减少计算开销）的同时，通过显式提供的运动感知条件，使System 1能够自适应地生成稳定的动作。</p>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：与直接拼接图像token的传统VLA模型（图1a）或直接使用VDM原始特征的条件方法（图1b）相比，Video2Act的创新具体体现在：1) <strong>显式表征提取</strong>：并非直接使用VDM的原始特征，而是通过可解释的、非学习的算子（Sobel和FFT）显式抽取出空间结构和运动动态信息；2) <strong>异步双系统设计</strong>：将计算密集的VDM与轻量的动作头解耦，以不同频率异步运行，在保持高性能的同时实现了实时控制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真基准</strong>：使用RoboTwin双手机器人操作基准，包含6个任务（块交接、容器放置、双瓶抓取简单版、双瓶抓取困难版、空杯放置、捡苹果）。</li>
<li><strong>真实世界实验</strong>：在ALOHA双手机器人平台上进行6个操作任务评估。</li>
<li><strong>对比方法</strong>：包括Diffusion Policy (DP)、ACT、RDT-1B、(\pi_0) 以及同样使用VDM特征的Video Prediction Policy (VPP)。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2512.03044v1/x1.png" alt="仿真结果表"></p>
<blockquote>
<p><strong>表1</strong>：在六个RoboTwin操作任务上的仿真实验结果。Video2Act取得了54.6%的平均成功率，超过了所有基线方法，相比之前的SOTA方法（RDT和π0）分别提升了9.7%和7.7%。</p>
</blockquote>
<p>在仿真实验中，Video2Act在六个任务上的平均成功率达到54.6%，优于之前最好的方法RDT（44.9%）和π0（46.9%），提升幅度分别为9.7%和7.7%。特别是在“块交接”和“空杯放置”任务上表现突出。</p>
<p><img src="https://arxiv.org/html/2512.03044v1/x4.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图4</strong>：在六个真实世界操作任务上的实验结果。Video2Act在所有任务上均优于基线RDT和VPP，平均成功率相比VPP高出21.7%。</p>
</blockquote>
<p>在真实世界实验中，Video2Act在六个任务上的平均成功率达到71.7%，显著优于VPP的50.0%和RDT的45.0%，相比VPP提升了21.7个百分点，展示了强大的鲁棒性和泛化能力。</p>
<p><strong>消融实验分析</strong>：</p>
<p><img src="https://arxiv.org/html/2512.03044v1/x5.png" alt="消融研究"></p>
<blockquote>
<p><strong>图5</strong>：消融研究。(a) 评估时空运动特征提取的有效性，结合Sobel和FFT带来最大性能增益。(b) 分析操作系统频率比对成功率和动作生成频率的影响，在1:10的比率下能在保持高性能的同时实现高频率控制。</p>
</blockquote>
<p>论文进行了详细的消融实验（图5）：</p>
<ol>
<li><strong>时空运动特征贡献</strong>：在固定1:1系统频率比下，对比了不同特征配置。仅使用原始VDM特征相比无VDM特征有提升。单独添加Sobel空间特征带来4.0%的绝对提升，单独添加FFT运动特征带来5.0%的绝对提升。而结合两者（Sobel+FFT）能达到最佳性能54.6%，总提升达8.3%。这证明了显式提取和整合这两种表征的有效性。</li>
<li><strong>异步频率比分析</strong>：实验了System 2与System 1的不同更新频率比（1:n）。结果显示，当n从1增加到10时，动作生成频率线性增长，而任务成功率保持稳定。当n=20时，性能开始下降。这表明Video2Act的异步设计是有效的，通过1:10的比率可以在几乎不损失性能的前提下，将动作生成频率提升近10倍，极大地缓解了VDM的计算瓶颈。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>系统性分析</strong>：首次在机器人操作设置下系统分析了VDM的潜在表征，揭示了其对前景物体结构和运动一致性的稳定捕捉能力，且对机器人手臂和腕部相机动态具有鲁棒性。</li>
<li><strong>显式表征整合框架</strong>：提出了Video2Act，通过空间滤波算子（Sobel）和快速傅里叶变换（FFT）显式地整合来自VDM的空间和运动感知表征到VLA模型学习中，使模型能理解“操作什么”以及“如何移动”。</li>
<li><strong>高效异步双系统策略</strong>：设计了一种异步双系统策略，其中VDM作为慢速感知系统，DiT头作为快速执行系统，使得模型能够在接收高频输入时自适应地生成稳定动作，实现了性能与效率的平衡。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，尽管异步设计缓解了计算压力，但VDM本身仍然是一个计算成本高昂的大型预训练模型。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>表征利用</strong>：鼓励后续工作更深入地探索和理解大模型（如VDM）中蕴含的、对机器人任务有益的隐式知识，并设计更精巧的机制将其显式化。</li>
<li><strong>系统设计</strong>：异步双系统范式为整合重型感知模型与轻型控制策略提供了可行思路，可应用于其他需要结合慢速深思与快速反应的智能体架构中。</li>
<li><strong>效率优化</strong>：可以进一步探索更轻量级的VDM、更高效的特征提取与缓存机制，以实现在资源受限平台上的部署。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Video2Act，解决现有视频扩散模型（VDM）用于机器人策略学习时，未能充分利用其帧间连贯且物理一致的运动表示的问题。方法上，设计异步双系统：慢系统（VDM）显式提取前景边界与帧间运动变化，过滤背景噪声；快系统（扩散变换器动作头）接收上述运动感知条件，实现高频稳定控制。实验表明，Video2Act在模拟和真实任务中的平均成功率分别超越之前最佳方法7.7%和21.7%，并展现出强泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.03044" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>