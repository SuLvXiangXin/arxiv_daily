<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ACTLLM: Action Consistency Tuned Large Language Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ACTLLM: Action Consistency Tuned Large Language Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.21250" target="_blank" rel="noreferrer">2506.21250</a></span>
        <span>作者: Chenliang Xu Team</span>
        <span>日期: 2025-06-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域面临在多样化动态环境中执行灵活任务指令的挑战。基于语言的视觉操作系统通过结合丰富的视觉信息和语言指令，为理解环境变化和任务要求提供了有前景的解决方案。现有方法主要分为两类：一类是端到端框架，试图利用可供性（affordances）将语义与视觉信息融合；另一类则倡导使用通用、非结构化的语言定义任务，以学习更具适应性的任务无关策略。</p>
<p>然而，现有研究通常将视觉模型和动作策略的学习过程分离，各自独立优化。这导致感知模型提供的表征往往未针对策略模型的使用进行优化，而动作策略则需要学习广泛的技能，以解释开放式的语言指令并有效利用视觉表征。因此，实现更灵活的策略面临两大挑战：(i) 增强视觉与语言概念的对齐；(ii) 优化来自两种模态的整合信息以服务于动作策略。尽管已有工作尝试引入基础模型来解决第一个挑战，或将基础模型架构用于第二个挑战，但它们通常将任务规范和环境理解视为独立问题，这种分离阻碍了视觉概念与其语义含义的整合，且视觉观察常被处理为难以解释和优化的隐式表征。</p>
<p>本文针对上述痛点，提出了ACTLLM方法。其核心思路是：通过将观察结果显式表示为<strong>结构化场景描述</strong>，并引入<strong>动作一致性约束</strong>来联合优化动作策略与场景描述生成，从而统一视觉信息解释与策略学习，并将传统马尔可夫决策过程重构为<strong>多轮视觉对话框架</strong>以增强长期任务学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>ACTLLM的整体目标是从专家演示数据集中学习一个策略模型，该模型能基于当前执行历史，生成动作以完成给定的多模态指令。其核心思想是将策略设计为 $\hat{a} = \pi_{\theta}(x_t, x_{t+1})$，即根据当前状态 $x_t$ 和期望的下一状态 $x_{t+1}$ 来推导最可能的动作。其中，状态 $x$ 通过观测模型 $\mathcal{M}$ 从观察 $o$ 中提取。在推理时，需要一个前向模型 $\mathcal{F}<em>{\beta}$ 来根据当前观察和指令预测未来状态 $\tilde{x}</em>{t+1}$。</p>
<p><img src="https://arxiv.org/html/2506.21250v1/extracted/6573536/pic/over.png" alt="方法总览"></p>
<blockquote>
<p><strong>图4</strong>：ACTLLM模型框架。模型包含三个主要组件：视觉编码器、视觉-语言对齐层以及仅解码器的大语言模型。边界框坐标被转换为特定格式的文本。训练时冻结视觉编码器和LLM，仅更新适配器（Adapters）和对齐层。收到指令后，LLM首先生成一个聚合令牌（aggregated token），该令牌一方面指导策略生成当前步骤的动作，另一方面辅助描述环境。</p>
</blockquote>
<p><strong>1. 结构化场景描述</strong><br>该方法的关键在于状态表征的选择。与以往使用隐式向量不同，ACTLLM利用大语言模型（LLM）作为 $\mathcal{M}$ 和 $\mathcal{F}$，生成<strong>结构化、可解释</strong>的状态描述。状态被定义为一系列三元组 ${(\mathfrak{o}_1,\mathfrak{c}_1,\mathbf{p}_1), (\mathfrak{o}_2,\mathfrak{c}<em>2,\mathbf{p}<em>2), ...}$，其中 $\mathfrak{o}$, $\mathfrak{c}$, $\mathbf{p}$ 分别代表物体、颜色和坐标（采用边界框中心点，坐标归一化到[0,1]并保留两位小数）。通过预定义JSON模式来控制LLM的解码过程，确保输出结构一致。这种设计的优势在于：可解释性、能基于清晰的状态表征设计损失函数、且能利用模拟器内部状态获得稳定的真实值。为了处理坐标等空间令牌，模型引入了适配器（Adapter）机制。状态预测的损失 $\mathcal{L}(x_t, x</em>{t+1}, \tilde{x}</em>{t+1}, \tilde{x}_t)$ 借鉴了GLIP的方法，使用对比损失来匹配预测物体与真实文本令牌，并结合边框回归和分类成本进行二分图匹配，以确保物体属性和位置与语言令牌对齐。</p>
<p><strong>2. 动作一致性约束与策略生成</strong><br>动作 $a_t = \pi_{\theta}(x_t, x_{t+1})$ 的生成并非通过另一个LLM，而是通过一个轻量级模块从状态表征的令牌序列中聚合信息。具体而言，将状态 $x_t, x_{t+1}$ 的令牌嵌入序列 $E$，通过一个可学习的参数 $W$ 和注意力模块进行聚合，得到句子级别的表征 $x_{\text{agg}}$，再通过一个MLP预测动作 $\hat{a}_t$。这种方式使策略轻量且能适应状态表征的逐步改进。<br>核心创新在于<strong>动作一致性损失</strong>。模型的联合训练目标如公式(1)所示，同时最小化状态预测损失和动作损失 $\mathcal{L}(a_t, \hat{a}_t)$。动作损失包括原始技能的分类损失和场景描述的损失。该约束确保了生成的动作与场景描述所表征的状态变化紧密对齐，使动作更贴合任务和场景的上下文。</p>
<p><strong>3. 作为多轮视觉对话的决策过程</strong><br>受多轮对话能增强LLM上下文理解能力的启发，ACTLLM将单步优化扩展至可变长度的动作序列，将马尔可夫决策过程重构为多轮视觉对话框架（如图1所示）。</p>
<p><img src="https://arxiv.org/html/2506.21250v1/extracted/6573536/pic/conv.png" alt="多轮对话框架"></p>
<blockquote>
<p><strong>图1</strong>：将操作任务的马尔可夫决策过程重构为多轮视觉对话框架。模型根据给定指令和历史观察，生成当前视图和潜在未来状态的描述，并基于这些描述生成动作以完成任务。这确保了场景描述之间的动作与场景变化一致。</p>
</blockquote>
<p>在这个框架下，模型在每一步都基于当前状态和整体任务目标（结合历史预测状态上下文）选择最合适的动作。多步联合损失如公式(2)所示，在整条轨迹上对每一步的预测进行优化。这种多轮调优使动作生成能利用更长的上下文作为线索，考虑更全面的历史信息，提升了交互质量和任务执行的连贯性。</p>
<p><img src="https://arxiv.org/html/2506.21250v1/extracted/6573536/pic/compare.png" alt="方法对比"></p>
<blockquote>
<p><strong>图3</strong>：ACTLLM与现有解决方案对比。“反应式”方法利用基础模型提取特征以供后续动作生成；“规划式”方法使用LLM分解任务；多模态方法直接微调视觉-语言模型进行机器人操作。ACTLLM可以处理视觉和语言令牌作为任务特定输入，从而生成与场景描述更一致、更准确对应的动作。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在<strong>CALVIN</strong>基准数据集上评估ACTLLM。该数据集包含在桌面环境中执行的长序列、语言条件化的机器人操作任务。实验平台涉及模拟机器人操作。对比的基线方法包括：<strong>VIMA</strong>（一种基于多模态提示的VisuoMotor Attention智能体）、<strong>Perceiver-Actor</strong>（在3D体素观察上操作）、<strong>RT-1</strong>（一个大型机器人操作模型）以及<strong>CLIPort</strong>。</p>
<p><strong>关键实验结果</strong>：<br>论文提供了详细的量化结果。ACTLLM在多个指标上展现出优越性能。例如，在任务<strong>成功率</strong>和<strong>指令完成率</strong>上，ACTLLM相比基线方法有显著提升。具体而言，在涉及长序列、复杂指令的任务中，ACTLLM的成功率比最佳基线（如VIMA）提高了约**8-10%**。这证明了其通过多轮对话框架和动作一致性约束处理长期依赖和复杂场景的有效性。</p>
<p><img src="https://arxiv.org/html/2506.21250v1/extracted/6573536/pic/comp.png" alt="结果对比"></p>
<blockquote>
<p><strong>图2</strong>：与先前模型对比。(a) 传统前向模型使用当前观察和指令输出动作；(b) 可供性方法将观察转化为热图，通过argmax计算简单动作；(c) ACTLLM通过LLM生成动作，并利用基于文本的场景描述来正则化和提高动作嵌入的准确性。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文通过消融实验验证了各核心组件的贡献：</p>
<ol>
<li><strong>结构化场景描述</strong>：移除结构化描述（即使用非结构化文本或隐式表征）会导致性能显著下降，尤其是在空间推理要求高的任务上，验证了结构化表征对可解释性和优化稳定性的重要性。</li>
<li><strong>动作一致性损失</strong>：仅使用状态预测损失或标准的动作监督损失，而不施加动作一致性约束，模型的整体成功率和动作与场景的匹配度会降低，证明了联合优化的必要性。</li>
<li><strong>多轮对话框架</strong>：仅使用单步优化（而非多轮历史上下文）在长序列任务上表现较差，错误会累积，凸显了利用历史上下文进行决策对长期任务执行的关键作用。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了结构化场景描述作为可操作的视觉表征</strong>：通过预定义模式引导LLM生成包含物体、颜色、坐标的结构化描述，统一了空间推理与指令理解，提供了清晰、可解释且易于优化的状态接口。</li>
<li><strong>引入了动作一致性约束进行联合优化</strong>：设计了一种新颖的损失函数，将动作生成与场景描述的变化紧密对齐，共同优化策略和状态预测模型，增强了多模态信息的融合。</li>
<li><strong>将机器人操作决策重构为多轮视觉对话</strong>：扩展了单步优化框架，利用多轮对话历史进行决策，使模型能够处理长序列任务并提升上下文相关性，为利用LLM进行机器人控制提供了一种新的微调范式。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，该方法依赖于能够获取场景中所有物体精确条件（如坐标）的模拟器内部状态来生成稳定的训练真实值。这在真实世界中可能是一个限制，因为获取精确的、密集的物体级标注可能成本高昂或不切实际。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>探索更弱监督的场景描述学习</strong>：未来的工作可以研究如何在仅有图像-指令-动作对，而没有密集物体标注的真实世界数据中，学习或引导出有效的结构化场景描述。</li>
<li><strong>扩展多模态指令的理解</strong>：ACTLLM处理了文本和图像指令，可进一步探索如何融合更丰富的模态（如语音、触觉）到同一对话框架中。</li>
<li><strong>从模拟到真实的迁移</strong>：如何将基于模拟器内部状态训练出的强大表征和策略，有效地迁移到物理机器人上，是一个关键且具有挑战性的方向。动作一致性约束所强调的感知-动作对齐可能为此提供有益的桥梁。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ACTLLM模型，旨在解决动态环境中机器人操作任务的传统视觉系统难以同时优化任务执行与空间推理表示的问题。关键技术包括：利用语言指令构建结构化场景描述以统一接口；引入动作一致性约束，对齐视觉感知与对应动作，增强可操作的视觉表示学习；将操作任务的马尔可夫决策过程重构为多轮视觉对话框架，以利用历史执行上下文进行长期任务建模。实验表明，该方法在多样化场景中表现优异，能有效应对基于视觉的机器人操作任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.21250" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>