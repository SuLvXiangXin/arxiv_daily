<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GelFusion: Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>GelFusion: Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.07455" target="_blank" rel="noreferrer">2505.07455</a></span>
        <span>作者: Jiang, Shulong, Zhao, Shiqi, Fan, Yuxuan, Yin, Peng</span>
        <span>日期: 2025/05/12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习策略的性能高度依赖于完整的观察。然而，主流的基于视觉的系统在接触丰富的场景中常常表现不佳，主要受到视觉线索模糊（如视野受限导致状态估计不完整）或遮挡（如被抓握物体遮挡关键操作细节）等限制。高分辨率的视触觉传感器（如GelSight）通过提供详细的表面特性和交互动态信息，为弥补视觉不足提供了可能。尽管潜力巨大，但如何有效地融合视觉和视触觉模态以进行鲁棒的政策学习仍是一个重大挑战，若设计不当，可能导致信息丢失或某一模态过度主导。</p>
<p>本文针对上述多模态融合挑战，旨在利用视触觉提供的丰富接触信息，提出了GelFusion框架。该框架的核心思路是：通过一种视觉主导的交叉注意力融合机制，将来自高分辨率GelSight传感器的视触觉反馈整合到政策学习中，并设计一种双通道的视触觉特征表示来充分提取静态几何纹理和动态交互信息，从而在视觉受限条件下增强机器人操作策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>GelFusion的整体框架基于标准的Diffusion Policy U-Net结构，作为一个下游条件去噪模型。其输入为2个时间步的观测（包括视觉图像、左右手触觉图像、本体感觉），输出为16个时间步的10自由度动作序列（末端执行器位置、6D朝向表示、1维夹爪开合值）。</p>
<p><img src="https://arxiv.org/html/2505.07455v1/x3.png" alt="网络架构"></p>
<blockquote>
<p><strong>图3</strong>：GelFusion网络架构。左侧为视觉编码器（ViT）和触觉编码器（双通道ResNet），中间是视觉主导的交叉注意力融合模块，右侧为扩散策略去噪网络。低维动态特征和本体感觉特征在融合后与高维融合特征拼接，共同指导策略。</p>
</blockquote>
<p>核心模块包括视觉编码器、触觉编码器和跨模态融合模块。</p>
<ol>
<li><strong>视觉编码器</strong>：采用CLIP预训练的ViT-B/16模型，处理224×224像素的双帧序列图像，并使用随机裁剪和颜色抖动进行数据增强。经过领域特定的微调后，使用其分类令牌作为视觉特征向量 <strong>F_v</strong>。</li>
<li><strong>触觉编码器（双通道特征表示）</strong>：独立处理左右夹爪的GelSight图像，包含两个互补的通道：<ul>
<li><strong>几何特征通道</strong>：专注于提取高维的几何和纹理特征。使用从头训练的ResNet-18编码器处理每一帧触觉图像，生成512通道、7x7的特征图，然后通过注意力池化得到紧凑的一维特征向量。观测时段内所有帧的特征向量被拼接并线性投影，以匹配视觉特征的维度，最终生成左/右触觉静态几何表示 <strong>F_T^l</strong> 和 <strong>F_T^r</strong>。</li>
<li><strong>动态特征通道</strong>：专注于提取时间动态。计算连续触觉帧之间的时空残差，并通过阈值进行二值化，以编码相对变化模式而非绝对信号强度。随后计算二值化残差图像的空间均值和方差，生成一个低维（2维）特征向量 <strong>F_dyn</strong>。均值反映变化区域的整体比例，方差指示这些变化的空间分散程度，从而概括交互的动态进程。</li>
</ul>
</li>
<li><strong>跨模态融合（视觉主导的交叉注意力）</strong>：该机制旨在有效整合视觉和触觉几何特征。以视觉特征 <strong>F_v</strong> 作为查询（<strong>Q</strong>），将视觉特征 <strong>F_v</strong>、左触觉几何特征 <strong>F_T^l</strong> 和右触觉几何特征 <strong>F_T^r</strong> 共同作为键（<strong>K</strong>）和值（<strong>V</strong>）。通过计算查询与键的缩放点积相似度并经softmax归一化，得到反映各模态重要性的注意力权重 **[W_V, W_T^l, W_T^r]**。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.07455v1/x4.png" alt="视觉主导的交叉注意力融合"></p>
<blockquote>
<p><strong>图4</strong>：视觉主导的交叉注意力融合示意图。以视觉特征为查询（Query），计算其与视觉及触觉特征（Key）的关联权重，进而对值（Value）进行加权求和，生成 attended 特征 <strong>F_att</strong>。</p>
</blockquote>
<p>随后，使用这些权重对对应的值特征进行加权求和，得到 attended 特征 <strong>F_att</strong> = W_V <strong>F_V</strong> + W_T^l <strong>F_T^l</strong> + W_T^r <strong>F_T^r</strong>。最后，将原始视觉特征 <strong>F_v</strong> 与 <strong>F_att</strong> 拼接，形成最终的高维融合特征 <strong>F_fusion</strong>。<strong>F_fusion</strong> 再与低维动态特征 <strong>F_dyn</strong> 以及本体感觉特征拼接，共同作为条件输入扩散策略去噪网络。</p>
<p>与现有方法相比，GelFusion的创新点主要体现在：1) 提出了一个专门针对无标记点GelSight传感器的双通道触觉特征表示，同时显式捕获静态纹理几何和动态交互事件；2) 设计了一种视觉主导的交叉注意力融合方案，确保在融合过程中视觉特征不被干扰，并能动态地从触觉模态中提取任务相关的上下文信息作为补充。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个接触丰富的操作任务上评估GelFusion：表面擦拭、精密插桩插入和易碎物体（薯片）抓放。硬件平台使用Fairino FR5机械臂，视觉输入来自腕部相机，触觉输入来自自制的GelSight传感器。</p>
<p>对比的基线方法包括：仅视觉基线、去除动态特征的GelFusion变体、使用简单拼接融合代替交叉注意力的变体、使用自注意力融合的变体，以及在插桩任务中对比了不同触觉特征提取骨干网络（ResNet34， ViT-B/16）。</p>
<p><img src="https://arxiv.org/html/2505.07455v1/x5.png" alt="擦拭评估"></p>
<blockquote>
<p><strong>图5</strong>：擦拭任务的评估结果。(a)测试场景；(b)典型失败案例；(c)消融实验成功率对比；(d)不同融合方法与动态特征提取方法的成功率对比。完整GelFusion取得了最高成功率（约90%），移除交叉注意力会导致“空中擦拭”（失去接触），移除动态特征会导致过度按压。</p>
</blockquote>
<p><strong>表面擦拭任务</strong>：在多样化测试场景（不同擦拭区域、线形、起始高度）下，完整GelFusion模型取得了最高的成功率（约90%）。消融实验表明，移除交叉注意力融合机制会导致频繁的“空中擦拭”（成功率降至约65%），移除动态特征则会导致因用力过猛而失败（成功率约70%）。两者仍优于仅视觉基线（成功率约45%），证明了触觉反馈的必要性以及本文所提组件的特异性贡献。自注意力融合方法表现不佳，定位精度低且泛化能力差。</p>
<p><img src="https://arxiv.org/html/2505.07455v1/x6.png" alt="插入评估"></p>
<blockquote>
<p><strong>图6</strong>：插桩插入任务的评估结果。(a)随机化测试场景；(b)典型失败案例；(c)消融实验成功率对比；(d)不同视觉骨干网络下的成功率对比。完整GelFusion在挑战性配置下仍保持高成功率（约85%），而仅视觉策略在精确交互阶段常因视觉遮挡而失败。</p>
</blockquote>
<p><strong>插桩插入任务</strong>：在初始位置、目标位置和旋转随机化的场景下，完整GelFusion框架取得了高成功率（约85%）。仅视觉策略在无需精细接触调整的阶段有效，但在精确交互时因视觉遮挡常导致对齐状态误判和过度用力而失败（成功率约40%）。消融实验再次验证了动态特征和交叉注意力机制的必要性。在触觉骨干网络对比中，从头训练的ResNet18表现最佳，使用预训练ViT（仅用[CLS]令牌）的方法性能显著下降，更深层的ResNet34并未带来显著提升。</p>
<p><img src="https://arxiv.org/html/2505.07455v1/x7.png" alt="薯片抓取评估"></p>
<blockquote>
<p><strong>图7</strong>：薯片抓取任务的评估结果。(a)任务演示；(b)仅视觉策略的典型失败案例（抓取过轻）；(c)不同配置下的性能对比。在考虑精细力控的成功标准下，引入触觉反馈的策略普遍优于仅视觉方法。</p>
</blockquote>
<p><strong>易碎物体抓取任务</strong>：评估重点从任务完成转向对演示中力控的复现。仅视觉策略倾向于非常轻柔的抓取，虽然有时能完成运输，但不符合演示的力控水平。在考虑力控的细化成功标准下，引入触觉反馈的配置普遍优于仅视觉方法。不过，不同触觉信息模块之间的性能差异不如前两个任务明显，作者分析这可能与硅胶夹爪材料本身的保护特性有关。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了GelFusion框架，通过视觉主导的交叉注意力机制，有效地将高分辨率视触觉反馈融合到扩散策略中，以应对视觉受限条件下的操作挑战；2) 设计了一种双通道的视触觉特征表示方法，能够同时提取接触区域的静态纹理几何特征和动态交互事件特征，为政策学习提供了更全面的接触信息。</p>
<p>论文自身提到的局限性包括：数据集规模（即使是100条示教）可能限制了自注意力融合等方法的有效性；在易碎物体抓取任务中，由于夹爪材料的缓冲作用，不同触觉模块的性能差异未能充分显现。</p>
<p>这项工作对后续研究的启示在于：为接触丰富的操作任务提供了一种有效的多模态（视觉-触觉）融合范式；证明了显式分离和表征触觉的静态与动态信息对政策学习具有积极意义；在视觉信息不完整或模糊的场景下，高质量的触觉感知可以成为提升机器人操作鲁棒性和可靠性的关键补充。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉受限条件下机器人操作性能受限的问题，提出GelFusion框架，通过融合视觉与高分辨率触觉信息来增强模仿学习策略。其核心是采用视觉主导的交叉注意力融合机制，并设计了双通道触觉特征表示，同时提取纹理几何特征与动态交互特征。在表面擦拭、插孔插入和易碎物体抓取放置三个接触密集任务上的实验表明，该框架有效提升了策略学习的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.07455" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>