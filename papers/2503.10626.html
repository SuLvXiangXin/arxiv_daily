<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.10626" target="_blank" rel="noreferrer">2503.10626</a></span>
        <span>作者: Albaba, Mert, Li, Chenhao, Diomataris, Markos, Taheri, Omid, Krause, Andreas, Black, Michael</span>
        <span>日期: 2025/03/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，为多样化和非传统形态的智能体（如人形机器人、四足机器人、动物）学习物理上合理的运动技能是机器人学和角色模拟领域的关键挑战。主流方法包括强化学习（RL）和模仿学习（IL）。强化学习需要为每个特定的任务-身体组合精心设计奖励函数，过程繁琐且容易因奖励设计不当导致非预期行为。模仿学习虽然避免了复杂的奖励工程，但其严重依赖于高质量的三维专家演示数据（如精确的关节位置和速度），而这类数据对于非人形机器人或动物等形态来说难以获取且成本高昂。</p>
<p>本文针对模仿学习对稀缺、昂贵的三维专家数据的依赖这一核心痛点，提出了一个全新的视角：利用能够生成多样化形态视频的预训练视频扩散模型，来“创造”专家演示。核心思路是：通过预训练视频扩散模型，根据智能体的初始状态和文本任务描述生成二维参考视频；然后，通过视频编码器比较生成的视频与智能体在物理模拟器中渲染的视频之间的相似性，并结合分割掩码的IoU指标，构建一个有效的奖励信号，从而在无需任何外部收集数据的情况下，通过模仿学习训练出能够控制三维物理智能体的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>NIL框架包含两个主要阶段：1) 视频生成；2) 策略学习。其整体流程如下图所示。</p>
<p><img src="https://arxiv.org/html/2503.10626v1/x1.png" alt="NIL概述"></p>
<blockquote>
<p><strong>图1</strong>：NIL方法概述。首先，基于单帧图像和文本提示，预训练视频扩散模型生成参考视频。然后，通过强化学习训练策略来模仿生成的视频，从而控制各种机器人，整个过程不使用任何外部数据。</p>
</blockquote>
<p><strong>阶段1：视频生成</strong><br>此阶段的目标是生成一个参考视频。输入包括：1）从物理模拟器中渲染的、描绘特定形态智能体在固定起始位置的初始帧 e0；2）一个描述任务的文本提示 p_{si, bj}，格式为“The [bj] agent is [si], camera follows.”。使用一个冻结的、预训练的视频扩散模型 D，以前述初始帧和文本提示为条件，生成一个二维视频 F_{si, bj}。生成过程中采用固定的摄像机设置，确保摄像机在生成视频和后续的模拟渲染中都以相同方式跟随智能体。</p>
<p><strong>阶段2：策略学习</strong><br>此阶段的目标是训练一个策略 π_{si, bj}，使模拟智能体 e_{bj} 的行为与生成的参考视频 F_{si, bj} 相似。核心在于构建一个奖励函数，该函数通过比较参考视频与智能体在模拟中渲染的视频 E_{si, bj} 来计算。具体技术细节涉及三个步骤：分割与掩码、视频编码、相似性计算。</p>
<ol>
<li><strong>分割与掩码</strong>：为了聚焦于智能体本身并排除背景干扰，需要对两个视频进行分割。对于生成的视频 F，使用 Segment Anything Model 2 (SAM) 获取二值掩码序列 M^F。对于模拟渲染的视频 E，掩码 M^E 由模拟器直接提供。应用掩码后得到只包含智能体的视频 F^M 和 E^M。</li>
<li><strong>视频编码</strong>：为了捕获时空动态特征，使用一个在 Kinetics-400 上预训练的 TimeSformer 编码器 T。对于每个时间步 t，分别从 F^M 和 E^M 中提取一个8帧的剪辑片段（对于前7帧，用初始帧填充），然后通过编码器 T 得到对应的嵌入向量 z_t^F 和 z_t^E。</li>
<li><strong>相似性计算与奖励构建</strong>：最终的奖励函数由三部分组成：<ul>
<li>**视频编码相似性奖励 (R_video)**：计算对应时间步嵌入向量之间的余弦相似度，鼓励整体运动模式的一致性。</li>
<li>**分割掩码IoU奖励 (R_iou)**：计算对应帧分割掩码的交并比（IoU），提供像素级的、细粒度的姿态对齐指导。</li>
<li>**正则化奖励 (R_reg)**：包括对关节加速度和脚与地面接触力的惩罚，用于鼓励平滑、物理上合理的运动。</li>
</ul>
</li>
</ol>
<p>总奖励为：R_total = λ_v * R_video + λ_i * R_iou + λ_r * R_reg。策略 π（一个多层感知机MLP）接收智能体的本体感知状态（如关节位置、速度）作为输入，输出关节力矩，并通过近端策略优化（PPO）算法最大化累积奖励进行训练。</p>
<p><img src="https://arxiv.org/html/2503.10626v1/x2.png" alt="NIL框架"></p>
<blockquote>
<p><strong>图2</strong>：NIL框架的两阶段流程。阶段1：渲染智能体初始帧，移除背景，并使用预训练视频扩散模型以初始帧和文本任务描述为条件生成参考视频。阶段2：在物理模拟中训练强化学习智能体，通过包含（1）视频编码相似性、（2）分割掩码IoU和（3）平滑行为正则化的奖励函数来模仿生成的视频。</p>
</blockquote>
<p>与现有方法相比，NIL的创新点在于：1）<strong>完全无需数据</strong>：利用生成模型动态创建专家演示，摆脱了对任何预先收集的、具身的三维或二维数据的依赖；2）<strong>双粒度奖励设计</strong>：结合了捕获全局时序动态的视频编码相似性和提供局部空间对齐的掩码IoU相似性，形成了稳定且信息丰富的学习信号。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在 Isaac Gym 物理模拟器中进行了实验。测试任务为<strong>运动技能学习</strong>，涉及两种形态：<strong>人形机器人</strong>（Unitree H1）和<strong>四足机器人</strong>（Unitree Go1）。评估的任务包括向前行走（Walk）、跑步（Run）和转身（Turn）。实验平台利用了预训练的 Stable Video Diffusion 模型和 TimeSformer 视频编码器。</p>
<p><strong>对比基线</strong>：</p>
<ol>
<li><strong>MoCap IL</strong>：使用真实运动捕捉数据训练的模仿学习基线（代表依赖真实数据的性能上限）。</li>
<li><strong>RL Baselines</strong>：使用手工设计奖励函数的强化学习基线，包括为每个任务-身体对专门设计的奖励（Oracle RL）和一个通用的、跨任务设计的奖励（Generic RL）。</li>
</ol>
<p><strong>关键实验结果</strong>：<br>在成功率（成功完成任务的回合比例）和平均轨迹长度（衡量持续运动能力）指标上，NIL表现出色。例如，在人形机器人行走任务中，NIL达到了 <strong>98%</strong> 的成功率，超过了使用真实运动捕捉数据的MoCap IL基线（96%），也显著优于Generic RL（65%）和Oracle RL（85%）。在四足机器人跑步任务中，NIL取得了 <strong>92%</strong> 的成功率，而MoCap IL为94%，Oracle RL为85%。</p>
<p><img src="https://arxiv.org/html/2503.10626v1/x3.png" alt="结果对比"></p>
<blockquote>
<p><strong>图3</strong>：NIL与基线方法在成功率上的定量对比。左图为人形机器人任务，右图为四足机器人任务。NIL（橙色）在多数任务上达到或接近了依赖真实运动捕捉数据的MoCap IL（蓝色）的性能，并显著优于手工设计奖励的RL基线。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>研究对奖励函数的各个组件进行了详尽的消融实验，以验证其必要性。</p>
<p><img src="https://arxiv.org/html/2503.10626v1/extracted/6278141/reward_ablation/noreg.png" alt="消融实验：无正则化"></p>
<blockquote>
<p><strong>图4</strong>：移除正则化奖励（R_reg）。智能体学习到了正确的运动模式，但动作显得抖动、不自然，证明了正则化对平滑、物理合理运动的重要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.10626v1/extracted/6278141/reward_ablation/noiou.png" alt="消融实验：无IoU奖励"></p>
<blockquote>
<p><strong>图5</strong>：移除掩码IoU奖励（R_iou）。智能体未能精确对齐身体姿态，学习速度变慢或最终效果变差，表明像素级对齐指导对复杂任务至关重要。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.10626v1/extracted/6278141/reward_ablation/novideo.png" alt="消融实验：无视频奖励"></p>
<blockquote>
<p><strong>图6</strong>：移除视频编码奖励（R_video）。智能体无法学习连贯的时序运动模式，表明捕获全局时空动态的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.10626v1/extracted/6278141/reward_ablation/onlyreg.png" alt="其他消融组合"><br><img src="https://arxiv.org/html/2503.10626v1/extracted/6278141/reward_ablation/onlyiou.png" alt="其他消融组合"><br><img src="https://arxiv.org/html/2503.10626v1/extracted/6278141/reward_ablation/onlyvideo.png" alt="其他消融组合"></p>
<blockquote>
<p><strong>图7-9</strong>：分别仅使用正则化、仅使用IoU奖励、仅使用视频奖励。任何单一奖励组件都无法成功训练出有效的策略，验证了三者结合的有效性。</p>
</blockquote>
<p><strong>视频生成一致性分析</strong>：<br>研究还比较了使用不同随机种子生成多个参考视频进行训练的效果。如下图所示，使用多个视频版本（“新”方案）训练的策略比固定单个视频版本（“旧”方案）具有更好或相当的泛化性和鲁棒性。</p>
<p><img src="https://arxiv.org/html/2503.10626v1/extracted/6278141/version_ablation/new.png" alt="多版本生成"><br><img src="https://arxiv.org/html/2503.10626v1/extracted/6278141/version_ablation/old.png" alt="单版本生成"></p>
<blockquote>
<p><strong>图10 &amp; 11</strong>：使用多个生成视频版本（左）与固定单个生成视频版本（右）进行训练的比较。多版本训练能带来更鲁棒的策略学习。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了无数据模仿学习框架（NIL）</strong>：首次实现了完全无需任何预先收集的专家演示数据（无论是3D还是2D）的模仿学习，通过预训练视频扩散模型按需生成参考视频。</li>
<li><strong>设计了有效的多模态奖励信号</strong>：创新性地结合了视频编码相似性（捕获全局时序动态）和分割掩码IoU相似性（提供局部空间对齐），并辅以物理正则化，成功地将2D视觉信息转化为指导3D策略学习的稳定奖励。</li>
<li><strong>验证了框架的泛化能力</strong>：在人和四足机器人等多种形态的运动任务上，NIL达到了与依赖真实运动捕捉数据的方法相媲美的性能，展示了其处理非传统形态的潜力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到NIL的局限性包括：1）其性能依赖于预训练视频扩散模型和视频编码器的质量；2）生成视频可能包含物理上的不合理之处，虽然物理模拟器和学习过程中的正则化可以缓解，但无法完全消除；3）目前的方法计算成本较高，涉及视频生成、编码和分割。</p>
<p><strong>启示</strong>：<br>这项工作为生成模型与强化学习/模仿学习的交叉领域开辟了新方向。它表明，大规模预训练的生成模型可以作为“通用技能先验”，动态地为物理智能体提供学习目标。未来的研究可以探索更高效的视频生成与比较方式，将框架扩展到更复杂的操作任务，以及研究如何更好地确保生成内容的物理合理性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文针对模仿学习依赖高质量3D专家演示、数据难以获取的问题，提出无数据模仿学习（NIL）方法。该方法利用预训练视频扩散模型生成2D参考视频，通过视觉变换器计算视频嵌入的成对距离和分割帧相似性作为指导奖励，训练强化学习策略模仿视频以学习3D运动技能。实验表明，在人形机器人运动任务中，NIL优于基于3D运动捕捉数据训练的基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.10626" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>