<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.18088" target="_blank" rel="noreferrer">2506.18088</a></span>
        <span>作者: Yao Mu Team</span>
        <span>日期: 2025-06-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>双手机器人操作对于执行装配、工具使用等复杂任务至关重要，而训练通用的视觉-语言-动作基础模型需要高质量、多样化且大规模的数据集。然而，大规模收集真实世界演示数据成本高昂且不切实际。基于仿真的数据生成提供了可扩展的替代方案，但现有方法存在三个关键局限：1) 缺乏自动化质量控制，生成的轨迹常包含执行失败或次优抓取，影响策略学习；2) 领域随机化过于表面，场景过于干净单一，忽略了杂乱、光照变化、模糊语言指令等对鲁棒性至关重要的真实世界因素；3) 忽视了不同机器人本体之间的差异，例如低自由度平台（如Piper）和高自由度平台（如Franka）具有不同的运动学能力和抓取策略，现有数据集很少编码这种本体特定的操作约束。</p>
<p>本文针对这些痛点，提出了RoboTwin 2.0，一个可扩展的仿真数据生成框架。其核心思路是：结合多模态大语言模型的自动任务代码生成与仿真闭环反馈，并引入系统性的领域随机化和本体感知的抓取适配，以生成高质量、多样化、逼真的双手操作数据，从而提升策略的鲁棒性和泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboTwin 2.0的整体框架集成了自动化的专家数据生成、全面的领域随机化以及本体感知适配，旨在生成用于策略训练和评估的多样化、逼真轨迹。</p>
<p><img src="https://arxiv.org/html/2506.18088v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：RoboTwin 2.0整体流程。框架基于RoboTwin-OD对象数据集和技能API，利用基于MLLM的代码生成与仿真反馈，生成专家任务程序，并结合领域随机化生成用于策略训练和评估的轨迹。</p>
</blockquote>
<p><strong>核心模块1：基于MLLM与仿真闭环反馈的专家代码生成</strong><br>该模块采用闭环架构，包含一个代码生成智能体和一个视觉-语言模型观察者。流程如下：</p>
<ol>
<li><strong>输入规范</strong>：任务由名称和自然语言目标描述定义。代码生成智能体以通用API列表、示例函数调用和分层约束规范为条件。</li>
<li><strong>初始代码生成</strong>：智能体基于输入合成初始Python程序，将程序合成建模为对可用API调用空间的结构化预测问题。</li>
<li><strong>仿真执行与日志记录</strong>：生成的程序在仿真环境中每次迭代执行十次以考虑随机性。系统生成结构化执行日志，记录每次试验的成功/失败及原因（如代码不可执行、左/右抓取失败）。</li>
<li><strong>多模态观察与错误定位</strong>：VLM观察者对十次试验进行逐帧检查，评估每个步骤的成功并定位失败点，诊断失败模式（逻辑错误、API使用错误等）。</li>
<li><strong>代码修复与迭代优化</strong>：代码生成智能体接收执行日志和VLM诊断两种反馈，据此修改或替换易出错的指令。更新后的程序进入下一轮迭代，直至程序在单次迭代的十次运行中达到设定的成功率，或连续优化五次后仍未达标。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.18088v2/x3.png" alt="专家代码生成流程"></p>
<blockquote>
<p><strong>图3</strong>：专家代码生成流程。展示了从任务指令输入，经过代码生成、仿真执行、VLM观察诊断到代码修复的闭环迭代过程。</p>
</blockquote>
<p><strong>核心模块2：用于鲁棒机器人操作的领域随机化</strong><br>为增强策略对真实世界变化的鲁棒性，系统在五个维度进行随机化：</p>
<ol>
<li><strong>场景杂乱度</strong>：从RoboTwin-OD中采样与任务无关的干扰物，通过碰撞感知放置确保物理合理性，并排除与任务物体视觉/语义相似的干扰物。</li>
<li><strong>多样化背景纹理</strong>：使用大型纹理库随机化桌面和背景。该库通过LLM生成描述、网络爬取，并用Stable Diffusion v2生成图像，经人工筛选后包含11,000个高质量纹理。</li>
<li><strong>光照变化</strong>：在物理合理范围内随机化光源颜色、类型、强度和位置，模拟真实世界光照条件对物体外观的影响。</li>
<li><strong>桌面高度</strong>：在合理范围内均匀随机化桌面高度，引入视角和机器人-物体空间关系的变化。</li>
<li><strong>轨迹级多样化语言指令</strong>：使用多模态LLM为每个任务和物体生成多样化的任务模板和物体描述（几何、外观、部件属性）。每条轨迹从这些池中采样组合指令。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.18088v2/x4.png" alt="领域随机化与纹理库"></p>
<blockquote>
<p><strong>图4</strong>：领域随机化与纹理库可视化。(a) 展示了沿五个维度的随机化效果，包括杂乱场景、光照变化等。(b) 展示了生成的多样化背景纹理示例。</p>
</blockquote>
<p><strong>核心模块3：本体感知的抓取适配</strong><br>针对不同自由度机器人抓取策略的差异，系统为每个物体标注了一组丰富的候选操作位姿，覆盖多个抓取轴和接近方向。为了进一步扩展可行空间，系统会施加偏向于具有更高手臂可达性方向的角扰动。具体而言，通过结合首选操作方向、随机位姿扰动和并行运动规划尝试来为每个物体生成候选抓取。</p>
<p><img src="https://arxiv.org/html/2506.18088v2/x6.png" alt="不同抓取行为"></p>
<blockquote>
<p><strong>图6</strong>：不同机器人的抓取行为差异。例如，Franka机械臂抓取罐子通常采用自上而下的方式，而低自由度的Piper机械臂则更适合侧向抓取。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用了RoboTwin 2.0框架生成的50个双手任务数据集，涉及五个机器人本体（Aloha-AgileX, Piper, Franka, UR5, ARX-X5）。基准测试包括ACT、DP、RDT和Pi0等策略。实验平台为仿真环境，并进行了真实世界验证（COBOT-Magic双平台）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>自动化专家代码生成评估</strong>：在10个任务上评估代码生成系统。使用多模态反馈后，RoboTwin 2.0的平均成功率从62.1%（单次生成）提升至71.3%。优化迭代次数从RoboTwin 1.0的2.42次减少到2.0的1.76次，初始代码的令牌数也从1236.6大幅减少到569.4，表明框架更高效。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.18088v2/x9.png" alt="成功率分布"></p>
<blockquote>
<p><strong>图9</strong>：RoboTwin成功率分布。反馈机制（尤其是多模态反馈）缩小了成功率分布范围并将中位数提高，RoboTwin 2.0结合多模态反馈的分布集中在80%以上，显示出鲁棒性和可靠性。</p>
</blockquote>
<ol start="2">
<li><p><strong>本体感知抓取适配的有效性</strong>：与缺乏多样化抓取的RoboTwin 1.0基线相比，RoboTwin 2.0在所有本体上平均成功率提升8.3%。对于低自由度平台提升显著：Aloha-AgileX (+13.7%)、Piper (+22.7%)、ARX-X5 (+5.6%)；对于高自由度平台（Franka, UR5）成功率基本不变，说明其本身已有足够的运动灵活性。</p>
</li>
<li><p><strong>策略鲁棒性评估</strong>：在仿真中评估策略对未见环境变化的鲁棒性。使用RoboTwin 2.0领域随机化数据预训练的模型，即使在下游任务仅使用干净数据微调，也展现出显著更好的泛化能力。RDT和Pi0模型分别实现了31.9%和29.3%的相对提升。</p>
</li>
<li><p><strong>仿真到真实性能评估</strong>：在四个真实世界双手任务上测试。仅使用10个真实演示的基线平均成功率较低。当用10个真实演示<strong>加上</strong>1000条RoboTwin 2.0合成数据微调时，平均成功率从29.5%提升至43.0%（相对提升45.8%）。<strong>仅使用</strong>1000条合成数据（零样本）训练的模型，平均成功率达到24.9%，相比10演示基线实现了高达367%的相对提升，证明了合成数据的有效性。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.18088v2/x10.png" alt="真实世界评估配置"></p>
<blockquote>
<p><strong>图10</strong>：真实世界评估的四种配置。包括干净/杂乱桌面，以及训练中见过/未见的背景组合，用于全面测试策略泛化能力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：代码生成实验中，对比了单次生成、仅基于执行日志反馈、结合多模态反馈三种配置，证明了多模态反馈（结合VLM诊断）对提升成功率、加速收敛和生成更简洁代码的关键作用。在策略训练中，对比了使用干净数据预训练和使用领域随机化数据预训练的效果，后者显著提升了策略在变化环境中的鲁棒性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个集成MLLM与仿真闭环反馈的自动化专家数据生成框架，能高效产出高质量任务代码和轨迹。</li>
<li>设计了一套系统性的五维领域随机化策略（杂乱度、光照、背景、桌面高度、语言指令），显著增强了数据的多样性和策略的鲁棒性。</li>
<li>引入了本体感知的抓取适配机制，并发布了大规模资源：RoboTwin-OD对象数据集、超10万条多本体领域随机化轨迹数据集、可扩展的数据生成器及标准化评估基准，为双手操作研究提供了重要基础设施。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：仿真物理与真实世界仍存在差距；当前的任务和对象库虽然规模大，但可能仍未覆盖所有现实世界的长尾情况；完全自动化的代码生成流程对于极其复杂的任务可能仍需人工干预。</p>
<p><strong>启示</strong>：本研究展示了结合大型生成模型（LLM/VLM）与仿真闭环进行自动化数据合成的强大潜力，为缓解机器人学习的数据瓶颈提供了新路径。系统性的、多维度的领域随机化是提升仿真到真实迁移性能的关键。未来工作可以探索更精细的物理仿真、更复杂的任务结构，以及将生成的数据用于训练更通用的多模态操作基础模型。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RoboTwin 2.0，旨在解决双手机器人操作中缺乏高效、可扩展的合成数据生成方法以及仿真环境过于简化的问题。关键技术包括：构建大规模物体数据集RoboTwin-OD（731个实例），利用多模态大语言模型自动合成任务程序，并采用五维结构化领域随机化增强数据多样性。实验表明，该方法使代码生成成功率提升10.9%；仅结合10个真实演示，策略性能相对基线提升367%，纯合成数据训练的零样本模型也获得228%的相对增益，显著提升了仿真到现实的迁移鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.18088" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>