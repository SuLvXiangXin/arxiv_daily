<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Trajectory Conditioned Cross-embodiment Skill Transfer - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Trajectory Conditioned Cross-embodiment Skill Transfer</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.07773" target="_blank" rel="noreferrer">2510.07773</a></span>
        <span>作者: Bin Zhao Team</span>
        <span>日期: 2025-10-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>从人类演示视频中学习机器人操作技能是一个前景广阔但极具挑战性的问题，核心障碍在于人体与机器人操纵器之间存在巨大的形态差异。现有方法主要依赖于配对的人类-机器人数据集或手工设计的奖励函数，这限制了方法的可扩展性和泛化能力。例如，Learning by Watching和Vid2Robot等方法需要精确的形态映射或大规模的配对数据，而Human2Robot和Motion Tracks等方法则对形态差异较为敏感。</p>
<p>本文针对上述“形态鸿沟”这一具体痛点，提出了一个新的视角：寻找一种紧凑、与形态无关且能保留核心任务动态的运动表示。其核心思路是利用从人类演示中提取的稀疏光流轨迹作为与形态无关的运动线索，以此条件化一个视频生成模型，联合合成时序一致的机器人操作视频并将其转换为可执行的动作，从而实现无需配对数据或强化学习的跨形态技能迁移。</p>
<h2 id="方法详解">方法详解</h2>
<p>TrajSkill框架旨在实现基于轨迹条件的跨形态技能迁移，其核心是利用稀疏光流作为与形态无关的表示。框架包含三个组成部分：(1) 形态不变光流采样，(2) 轨迹条件机器人执行，(3) 跨形态技能迁移。</p>
<p><img src="https://arxiv.org/html/2510.07773v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：TrajSkill框架的统一示意图。顶部：形态不变光流采样。从人类演示视频帧（左）出发，通过RAFT计算稠密光流（中），并根据光流幅度采样稀疏关键点轨迹并在时间上传播（右）。中部和底部：轨迹条件机器人执行概览。给定任务描述，T5模型解释指令，3D VAE提取空间特征，轨迹提取器提供稀疏光流信号。这些信息在扩散变换器内融合以预测机器人运动视频，随后由策略 $p(a|o,v)$ 解码为可执行动作。</p>
</blockquote>
<p><strong>形态不变光流采样</strong>：该模块从人类演示视频中计算稠密光流，并将其简化为紧凑的稀疏轨迹。首先，使用RAFT计算连续帧间的稠密光流 $\mathbf{F}_t$。为了构建轨迹表示，在图像上定义一个步长为 $\lambda$ 的候选位置网格 $C$。每个候选位置 $(x, y)$ 的采样概率与其初始光流幅度 $|\mathbf{F}<em>0(x,y)|<em>2$ 成正比，确保运动显著的区域更可能被选中。采样关键点的数量 $N$ 从最大预算 $N</em>{\max}$ 中均匀随机选择，然后根据概率分布 $p</em>{(x,y)}$ 无放回地选择 $N$ 个不同的候选点，形成初始关键点集 $K_0$。随后，通过积分局部光流向量将选中的关键点随时间传播：$(x^{t+1}, y^{t+1}) = (x^t, y^t) + \mathbf{F}<em>t(x^t, y^t)$，从而得到一组稀疏轨迹 $T = {(x_i^t, y_i^t)}</em>{i,t}$。最后，对稀疏光流场应用高斯模糊平滑以抑制噪声并提高空间一致性，得到最终的轨迹表示 $\tilde{S}_t$。该表示摒弃了形态特定的外观细节，仅聚焦于核心运动意图。</p>
<p><strong>轨迹条件机器人执行</strong>：该部分包含轨迹条件视频生成和视频策略到机器人执行两个阶段。<br><em>轨迹条件视频生成</em>：采用基于潜在扩散变换器的骨架，以初始帧 $I_0$、任务指令 $C_{\text{text}}$ 和稀疏轨迹信号 $C_{\text{traj}}$ 为条件，合成机器人操作序列 $V_r = G(I_0, C_{\text{text}}, C_{\text{traj}})$。创新性地引入了两阶段训练策略：第一阶段使用<strong>稠密光流监督</strong>，让模型学习精确的机器人动力学和物体交互；第二阶段过渡到<strong>稀疏轨迹对齐</strong>训练，使监督与推理条件对齐，确保形态不变的运动控制。这种设计使生成器既能获得精确的运动先验，又能适应稀疏轨迹提示。<br><em>视频策略到机器人执行</em>：将生成的视频 $V_r$ 映射为可执行的机器人动作。策略 $A_r = F(O, S, V_r)$ 同时以当前机器人观测 $O$、低级状态信息 $S$ 和生成的视频 $V_r$ 为条件。视频帧被时间聚合为紧凑的参考图像，投影到模型空间并与当前状态嵌入融合，最终解码为动作序列。</p>
<p><strong>跨形态技能迁移</strong>：结合上述两个组件，框架实现了单次跨形态模仿。形态不变光流采样模块提取的稀疏轨迹抽象掉了形态差异；两阶段训练管道实现了以人类演示轨迹为条件的机器人可执行任务视频生成，并进一步转换为动作。这使得机器人能够在无需配对数据集或强化学习的情况下复现人类演示的技能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个代表性基准上进行评估：模拟环境 <strong>MetaWorld 50 Tasks</strong>、真实世界 <strong>Franka Multi-Tasks</strong> 以及支持跨形态迁移的混合数据集 <strong>XSkill</strong>。对比的基线方法包括视频生成方法（<strong>AVDC</strong>、<strong>This&amp;That</strong>、<strong>CogVideo</strong>）和机器人执行方法（<strong>Diffusion Policy (DP)<strong>、</strong>TinyVLA</strong>、<strong>SmolVLA</strong>、<strong>OCTO</strong>）。评估指标包括衡量视频质量的弗雷歇视频距离（FVD）和核视频距离（KVD），以及衡量任务完成度的成功率（SR）。</p>
<p><img src="https://arxiv.org/html/2510.07773v1/x3.png" alt="轨迹条件视频生成"></p>
<blockquote>
<p><strong>图3</strong>：轨迹条件视频生成。为机器人手臂提供初始帧和预定义的轨迹（红色曲线）。TrajSkill生成一系列运动帧，其中机器人遵循指定的路径。该图展示了机器人在轨迹起点和终点的状态。</p>
</blockquote>
<p><strong>轨迹条件视频生成</strong>：在MetaWorld和Franka的57个测试序列上，TrajSkill在FVD和KVD指标上均优于所有基线（如表I所示）。在MetaWorld上，FVD降低了39.6%，KVD降低了36.6%；在Franka上，相比CogVideo，FVD降低了27.6%，KVD降低了31.6%。如图3所示，生成的机器人运动与输入的轨迹紧密对齐，证明了其精确的轨迹可控性。</p>
<p><img src="https://arxiv.org/html/2510.07773v1/x4.png" alt="跨形态技能迁移"></p>
<blockquote>
<p><strong>图4</strong>：轨迹条件跨形态技能迁移。顶部两行显示模拟结果，其中人类演示被抽象为球形轨迹（第一行）以指导机器人手臂运动生成（第二行）。底部四行展示了从人手演示到机器人手臂执行复杂多步骤任务的真实世界迁移。</p>
</blockquote>
<p><strong>跨形态技能迁移</strong>：在模拟迁移（球形智能体→机器人）和真实世界迁移（人手→机器人）中，如图4所示，从演示中提取的稀疏轨迹能有效指导机器人运动生成，验证了框架在模拟和真实场景中弥合形态鸿沟的能力。</p>
<p><strong>机器人执行</strong>：在MetaWorld 50基准测试中，TrajSkill实现了最高的整体成功率（44.7%），在简单和困难任务上分别达到81.8%和38.0%的成功率，显著优于其他方法（如表II所示）。在真实机器人实验中，在一个厨房“将香蕉放入篮子”的任务中，TrajSkill在抓取和放置动作上分别取得了90.9%和81.8%的最高成功率（如表III所示），验证了其从仿真到现实的有效迁移和鲁棒性能。</p>
<p><img src="https://arxiv.org/html/2510.07773v1/x5.png" alt="真实机器人实验"></p>
<blockquote>
<p><strong>图5</strong>：用于抓放任务的人控机器人视频预测。人类演示（左）控制在预测视频（右）中机器人手臂在三个不同香蕉位置（距离篮子20cm、30cm、40cm）的运动。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了TrajSkill框架，能够直接从人类演示视频进行可控视频生成和可执行机器人策略学习；2) 引入了稀疏光流轨迹作为一种与形态无关的表示，有效桥接了人类与机器人之间的形态差异；3) 通过广泛的实验验证，在视频生成质量、跨形态成功率和真实机器人执行方面均取得了显著提升。</p>
<p>论文自身提到的局限性包括需要将基于轨迹的条件控制扩展到更复杂的长期任务，并融入语言 grounding 以实现更详细的任务规范。这项工作为从非结构化人类视频演示中进行可扩展的机器人学习指明了一个有前景的方向，未来可应用于开放世界中更多样化的机器人形态。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出TrajSkill框架，解决机器人直接从人类演示视频学习操作技能时面临的“具身化差距”问题。其核心方法是将人类运动表示为稀疏光流轨迹，作为与具体形态无关的运动线索；基于此轨迹结合视觉与文本输入，联合生成时序一致的机器人操作视频并转换为可执行动作。实验表明，在MetaWorld仿真中，TrajSkill相比SOTA方法将FVD和KVD分别降低了39.6%和36.6%，并将跨具身化成功率最高提升16.7%；真实机器人厨房任务进一步验证了该方法的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.07773" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>