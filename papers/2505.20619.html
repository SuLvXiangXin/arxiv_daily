<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Gait-Conditioned Reinforcement Learning with Multi-Phase Curriculum for Humanoid Locomotion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Gait-Conditioned Reinforcement Learning with Multi-Phase Curriculum for Humanoid Locomotion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.20619" target="_blank" rel="noreferrer">2505.20619</a></span>
        <span>作者: Peng, Tianhu, Bao, Lingfan, Zhou, Chengxu</span>
        <span>日期: 2025/05/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人自然高效的运动策略开发是机器人学的核心挑战。现有方法主要分为几类：基于参考的方法（如AMP）依赖大规模运动捕捉数据，存在形态不匹配、可解释性差、难以灵活设计奖励的问题；无参考的强化学习方法虽提供更大设计自由，但收敛慢且难以处理复杂的敏捷过渡；多技能与模块化架构（如策略蒸馏、专家混合）通常需要专家预训练、切换机制，增加了系统复杂性和训练成本，且在部署时容易受到技能域间干扰。这些方法在将多种步态/技能集成到一个控制器时，局限性尤为突出。</p>
<p>本文针对多步态学习中奖励干扰严重、难以获得自然生物力学运动的核心痛点，提出了一种统一、无参考的强化学习新视角。其核心思路是：通过一个单一的循环策略，结合步态条件化的奖励路由机制和受生物启发的多阶段课程学习，使机器人能够端到端地学习站立、行走、奔跑及平滑过渡，而无需运动捕捉参考。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个统一的步态条件化强化学习框架，其目标是训练一个单一的循环策略来执行多种步态及过渡。策略采用非对称的演员-评论家架构，两者均由长短期记忆网络和多层感知机构成。演员网络接收本体感知观测（包括基座角速度、重力方向、指令速度、关节位置与速度偏移、前一时刻动作、步态相位正弦余弦编码以及独热步态ID），输出23维关节位置偏移，由固定增益的PD控制器跟踪。评论家网络在训练时额外接收特权信息（如足部接触状态、地面反作用力、外部扰动等）以提升价值估计的准确性。LSTM用于捕捉步态周期中的时间依赖性，辅助平衡恢复、处理延迟的接触效应，并在部分可观测性下实现平滑的多步态过渡。</p>
<p><img src="https://arxiv.org/html/2505.20619v3/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：本文提出的步态条件化RL框架概览。一个循环演员接收带有步态编码的本体感知状态。步态条件化的奖励掩码根据当前模式路由特定的奖励，从而实现多步态学习。</p>
</blockquote>
<p>该框架的核心创新模块是<strong>步态条件化奖励路由</strong>。在每个时间步，系统根据指令速度和机器人动态状态推断出步态模式ID，并将其作为独热向量附加到策略观测中。在训练时，一个步态掩码被应用于完整的奖励向量，以选择性地激活与当前步态相关的奖励项。</p>
<p><img src="https://arxiv.org/html/2505.20619v3/x3.png" alt="奖励路由"></p>
<blockquote>
<p><strong>图3</strong>：三个示例模式（跑、走、站）下的步态条件化奖励路由示意图。统一的奖励向量包括任务级和调节奖励（所有步态共享），以及步态特定项（如接触、推进、稳定性）。步态掩码用于在每一时间步仅激活相关项。省略号表示省略的模式（如过渡）或奖励。</p>
</blockquote>
<p>另一个核心模块是<strong>多阶段课程学习策略</strong>。为了避免同时训练所有步态带来的奖励冲突和不稳定探索，课程沿三个维度渐进展开：步态复杂性、指令速度范围和协调机制。训练分为三个阶段：第一阶段仅训练行走；第二阶段引入站立和走-站过渡，并启用步态ID切换；第三阶段引入奔跑和跑-走过渡，所有五种模式共同训练并启用完整的奖励路由。这种渐进式课程显著提高了训练稳定性和泛化能力。</p>
<p>此外，本文设计了<strong>受生物力学启发的奖励塑形</strong>，以鼓励产生自然高效的运动，而无需运动捕捉参考。关键奖励项包括：鼓励直膝支撑以减少肌肉努力，设计角动量奖励以促进臂-腿反相位协调来抵消腿部产生的角动量（具体奖励函数为 $R_{\text{momentum}} = -\left(L_{\text{total},z}^{2}\right)-0.4,\left(L_{\text{la},z}-L_{\text{ra},z}\right)^{2}$），以及针对不同步态（行走、奔跑、站立、过渡）的特定奖励，如接触模式、足部离地高度、推进动力学、平稳减速等。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Isaac Gym GPU加速物理模拟器中进行训练，并最终在真实的Unitree G1人形机器人上进行了零样本（无微调）部署验证。对比的基线主要是本文方法自身的消融实验。</p>
<p>在模拟实验中，控制器成功实现了平滑的步态切换与速度跟踪。如图4所示，在指令速度从0到3 m/s变化的过程中，实际速度能紧密跟踪指令，并自动、平滑地在站立、行走、奔跑及相应的过渡模式（W2S, R2W）间切换，没有出现突兀的接触变化。</p>
<p><img src="https://arxiv.org/html/2505.20619v3/x4.png" alt="步态切换与速度跟踪"></p>
<blockquote>
<p><strong>图4</strong>：上图：支撑接触分类与背景步态标签（包括站、走、跑、W2S、R2W）。下图：指令与实际的基座前进速度。步态过渡平滑、动态一致，反映了内部的协调性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.20619v3/x5.png" alt="角动量协调"></p>
<blockquote>
<p><strong>图5</strong>：腿部（蓝色实线）、手臂（红色虚线）和整体（黑色点划线）的Z轴角动量随时间变化。该策略在不同步态（包括过渡）中通过臂腿反相位协调保持了较低的总角动量，实现了有效抵消。</p>
</blockquote>
<p>图5进一步展示了学习策略的角动量协调能力。在行走和奔跑阶段，手臂和腿部的角动量符号相反、幅度相近，实现了有效抵消，从而保持低总角动量，这与人类步态的生物学原理一致。</p>
<p>关键的消融实验（表II）证明了课程学习和奖励路由的必要性。在相同训练预算下：1) <strong>无课程</strong>（同时训练所有五种步态）：平均episode长度仅为31.26，回报为2.45，无法收敛；2) <strong>无奖励路由</strong>（所有步态特定奖励项同时激活）：导致奖励冲突，性能完全崩溃，episode长度仅11.59，回报为0.00；3) <strong>完整方法</strong>（课程+路由）：取得了平均episode长度972.6，回报89.3的优异表现。这清晰表明，两个核心组件对于稳定、成功的多步态学习缺一不可。</p>
<p><img src="https://arxiv.org/html/2505.20619v3/figs/Real_Experiment_v2.jpeg" alt="真实世界部署"></p>
<blockquote>
<p><strong>图6</strong>：学习策略在Unitree G1人形机器人上的真实世界部署。机器人展示了行走和站立之间的平滑过渡，具有自然的肢体协调和直立姿态——直接从模拟器转移而来，未经过任何微调。</p>
</blockquote>
<p>在真实机器人实验中（图6），训练好的策略被直接部署到Unitree G1上，无需任何微调。机器人成功展示了稳定的站立、平滑的走-站过渡以及行走，整体运动协调自然，验证了该框架良好的sim-to-real迁移能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了一个统一的、无参考的RL框架，使用单一循环策略实现了人形机器人的站立、行走、奔跑及平滑过渡；2) 设计了步态条件化的奖励路由机制，有效缓解了多步态学习中的奖励干扰问题；3) 结合了受生物力学启发的奖励塑形与结构化的多阶段课程学习，促成了自然、高效的运动模式，且无需运动捕捉数据。</p>
<p>论文自身也指出了其局限性：首先，步态分类依赖于基于指令速度和弗劳德数的手动定义规则，这约束了新步态模式的自然涌现；其次，方法依赖于大量手动设计和调优的步态特定奖励项，扩展至更复杂或更多步态时可能变得繁琐；最后，随着步态数量增加，共享策略可能因目标重叠而面临灾难性遗忘或行为漂移的风险。</p>
<p>这项工作为迈向通用、自然的人形机器人控制提供了一条可扩展的路径。其对后续研究的启示包括：探索更自动化的步态发现与奖励设计机制；研究如何缓解多技能策略中的遗忘问题；以及将该框架扩展至更复杂的场景，如不平整地形穿越、视觉引导的动态环境运动及全身协同作业任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人多步态自然运动与平滑过渡的挑战，提出步态条件强化学习框架。方法核心包括：紧凑奖励路由机制基于步态ID动态激活目标以减轻干扰；生物力学奖励项促进直膝站立、臂腿协调等自然运动，无需运动捕捉；多阶段课程学习逐步引入步态复杂性。实验在仿真中实现稳健的站立、行走、奔跑及过渡，在真实Unitree G1机器人上验证了站立、行走及行走到站立过渡的稳定协调性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.20619" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>