<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.07445" target="_blank" rel="noreferrer">2509.07445</a></span>
        <span>作者: Nathan F. Lepora Team</span>
        <span>日期: 2025-09-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>灵巧的手内操作任务通常依赖于强化学习，而设计有效的奖励函数是一项巨大挑战。当前主流方法是依赖领域专家手动精心设计和调优奖励项，这一过程不仅耗时，而且容易产生次优或非预期的行为。近年来，大型语言模型（LLMs）在自动化机器人任务的政策或奖励代码生成方面展现出潜力，旨在减少人工工程负担。然而，这些进展主要集中于传统的感知模态（如视觉、本体感知）进行真实世界验证，迄今为止，无论是在模拟还是真实环境中，都尚未有工作将触觉感知整合到基于LLM的自动化奖励生成中。</p>
<p>触觉感知，特别是基于视觉的触觉传感，能够提供视觉感知难以捕捉的详细接触和力信号，对于检测精细的抓握不稳定性和接触动态至关重要，是实现人类般灵巧操作的关键。然而，将触觉这类复杂传感模态纳入学习控制系统，通常会加剧奖励设计的难度，尤其是在像全驱动多指灵巧手这样的高自由度系统中。因此，自动生成有效的基于触觉的奖励，可能是解锁高级操作稳健策略的关键。</p>
<p>本文针对“如何为依赖触觉反馈的灵巧手内操作自动设计有效奖励函数”这一具体痛点，提出了将LLM驱动的奖励生成扩展到触觉领域的新视角。核心思路是：结合迭代的LLM奖励设计和模拟到真实的师生蒸馏流程，仅使用自然语言任务描述和环境上下文，引导LLM自动生成简洁、可解释的奖励函数，最终在配备触觉传感器的真实灵巧手上实现优于人工设计基准的多轴手内物体旋转。</p>
<h2 id="方法详解">方法详解</h2>
<p>Text2Touch框架的整体流程分为两个主要阶段：基于LLM的迭代奖励生成，以及随后的模拟到真实策略蒸馏。</p>
<p><img src="https://arxiv.org/html/2509.07445v1/figures/diagrams/text2touchv3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Text2Touch训练与部署流程。左列和中列（绿色部分）构成了采用新颖提示策略的奖励生成流程；右列表示模型蒸馏与部署阶段。教师模型（黄色）是奖励生成阶段的最终输出，学生模型（淡紫色）是仅使用真实世界观测值蒸馏得到的教师模型。</p>
</blockquote>
<p><strong>1. 迭代LLM奖励设计</strong>：该方法建立在Eureka框架之上。给定环境代码/上下文 (M) 和自然语言任务描述 (l)，提示LLM生成奖励函数 (R)。在每次迭代 (k) 中，生成一批候选奖励 ({R^{(k)}})，使用PPO算法训练相应的策略 ({\pi^{(k)}})，并根据基于子目标成功次数的适应度函数 (F) 进行评估。选择最佳奖励 (R_{\text{best}}^{(k)})，并通过奖励反思（Reward Reflection）为LLM提供反馈，以生成下一批候选奖励 ({R^{(k+1)}})。此循环重复进行。</p>
<p><strong>核心模块与创新点</strong>：</p>
<ul>
<li><strong>可扩展的奖励与惩罚项</strong>：针对原始Eureka方法中固定成功奖励 (B) 可能阻碍学习的问题，本文创新地将成功奖励 (B) 和提前终止惩罚 (P) 作为可扩展变量提供给LLM（但不透露成功/失败的具体条件），让LLM在其生成的奖励函数内部自行调整这些项的尺度，从而获得更高的任务分数。</li>
<li><strong>修改后的提示结构</strong>：由于本文环境包含超过70个变量，远多于Eureka原始示例（约10个），直接使用原提示结构会导致LLM频繁产生语法错误或变量类型不匹配。为此，本文为LLM提供了一个<strong>显式的奖励函数签名</strong>，其中列出了所有可用的变量名和类型，代替了原先通用的签名。这一策略显著减少了代码失败率并提高了策略性能。</li>
<li><strong>奖励发现训练</strong>：由于完整训练（80亿步）计算成本过高，在迭代奖励生成阶段，采用较短的训练（1.5亿步）来快速评估候选奖励是否能够引导策略取得部分成功，从而为LLM提供及时的反馈以进行迭代优化。</li>
</ul>
<p><strong>2. 模拟到真实策略迁移</strong>：奖励发现完成后，使用特权信息（如物体位姿、速度）在模拟中训练一个“教师”策略。然后，通过记录教师策略的轨迹，并最小化学生策略在每一步动作上与教师动作的均方误差，来蒸馏一个仅依赖本体感知和触觉观测的“学生”策略。最终，这个训练好的学生策略可以直接部署到真实硬件上，无需进一步调优。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>任务</strong>：重力不变、多轴手内物体旋转（绕X、Y、Z轴），并在手心向上和手心向下两种手部朝向配置下进行。</li>
<li><strong>平台与传感器</strong>：模拟环境，以及真实的配备TacTip视觉触觉传感器的Allegro灵巧手（四指全驱动）。</li>
<li><strong>基准方法</strong>：与精心调优的人工设计奖励函数基线（基于AnyRotate工作）进行对比。</li>
<li><strong>评估指标</strong>：每个回合完成的完整360度旋转次数（Rots/Ep）、回合持续时间（Time To Terminate, TTT）、代码复杂度（使用变量数、代码行数、Halstead体积）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>提示策略验证（模拟，特权信息）</strong>：如表1所示，结合了可扩展奖励/惩罚项和修改后提示结构的策略（Bonus/Penalty+Mod）在五个不同的LLM上都取得了显著成功，而原始Eureka策略（Original）或仅修改模板（Mod template）均无法让策略学习到有效行为。这证明了所提提示策略的必要性和有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.07445v1/figures/diagrams/splash-tactile.png" alt="提示策略比较"></p>
<blockquote>
<p><strong>图1</strong>：Text2Touch改进了先前的奖励函数设计方法，提高了机器人手内物体旋转的速度和抓握稳定性。我们仅使用真实世界中的触觉和本体感知信息来评估LLM生成的奖励函数的性能。</p>
</blockquote>
<ol start="2">
<li><p><strong>LLM生成奖励 vs. 人工设计奖励（模拟，特权信息）</strong>：如表2所示，所有经过适当提示的LLM生成的奖励函数，在训练出的策略所能达到的<strong>平均每回合旋转次数上均超越了人工设计的基线</strong>。同时，LLM生成的奖励函数在代码质量上具有显著优势：使用的环境变量数约为基线的1/10，函数体代码行数约为1/4，Halstead体积约为1/8，表明其更简洁、更易解释。</p>
</li>
<li><p><strong>模型蒸馏与分布外测试（模拟，仅触觉/本体感知）</strong>：将上述最佳特权信息策略蒸馏为仅使用触觉和本体感知的学生模型后，在训练中未见的、更重或形状新颖的物体上进行测试（物体见图4顶部）。如表3所示，由LLM奖励蒸馏出的学生模型在平均旋转次数上<strong>继续一致性地超越人工基线</strong>。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.07445v1/figures/objects/my_objects_set.jpg" alt="实验物体"></p>
<blockquote>
<p><strong>图4</strong>：上图：用于在模拟中评估蒸馏后的学生触觉模型的物体。下图：用于在真实世界评估蒸馏模型的物体。</p>
</blockquote>
<ol start="4">
<li><strong>真实世界性能对比</strong>：选择性能最好的三个LLM生成策略（Gemini-1.5-Flash, GPT-4o, Deepseek-R1-671B）与人工基线在真实硬件上进行对比（物体见图4底部）。如表4所示，<strong>所有三个LLM生成的策略在平均每回合旋转次数和回合持续时间上都优于人工设计的基线</strong>。其中，Deepseek-R1-671B策略相比基线实现了38%的旋转次数提升和25%的回合持续时间提升。论文分析指出，LLM生成的策略倾向于学习更快的旋转动作，这种对接触变化的高度敏感性在模拟中可能不稳定，但在提供连续、丰富触觉反馈的真实世界中，反而使其能够更早地检测到不稳定性并进行重新抓握，从而获得了更好的稳健性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>首次将LLM驱动的奖励生成扩展到触觉操作领域</strong>，填补了现有LLM机器人学研究的空白。</li>
<li><strong>提出了针对高复杂度环境（&gt;70变量）的可扩展提示策略</strong>，包括将成功奖励/惩罚作为可调变量以及提供显式奖励函数签名，显著提高了LLM生成有效奖励代码的成功率和策略性能。</li>
<li><strong>通过师生蒸馏框架实现了模拟到真实的成功迁移</strong>，在真实的触觉灵巧手上验证了LLM生成奖励的有效性，其性能在旋转速度和稳定性上均超越了精心调优的人工设计基准。</li>
</ol>
<p><strong>局限性</strong>：本文评估集中于单一的手内旋转任务，虽然涵盖了不同手部朝向和旋转轴，但泛化到其他任务或传感模态的能力仍有待探索。此外，研究聚焦于奖励设计本身，保持了网络架构、课程设计和超参数的一致性，未探索LLM优化整个训练流程其他组件的潜力。</p>
<p><strong>启示</strong>：本研究证明了LLM能够自动化设计用于复杂触觉操作的奖励函数，且生成的奖励更简洁、性能更优。这大大降低了触觉机器人研究的门槛，支持更快速、可扩展的多模态机器人技能开发。未来工作可以探索将这种方法应用于需要长程规划的多阶段技能，或整合LLM来共同优化奖励函数之外的训练组件。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Text2Touch，解决在触觉灵巧操作中自动化设计强化学习奖励函数的难题。方法核心是利用大语言模型自动生成奖励函数，结合基于视觉的触觉传感信息，并通过仿真到现实的策略迁移，在真实四指灵巧手上实现多轴手内物体旋转。实验表明，该方法在旋转速度与抓取稳定性上显著优于人工精心设计的基准，且生成的奖励函数长度与复杂度降低了一个数量级。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.07445" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>