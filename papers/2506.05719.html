<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.05719" target="_blank" rel="noreferrer">2506.05719</a></span>
        <span>作者: Xiangyang Xue Team</span>
        <span>日期: 2025-06-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，在机器人抓取与操作任务中，准确估计铰接物体（如带盖子的桶、笔记本电脑）的部件级6D姿态是一个关键挑战。现有主流方法（如GAPartNet）主要遵循复杂的两阶段流程：首先对点云进行部件实例分割，然后为每个部件估计归一化部件坐标空间（NPCS）表示以恢复6D姿态。这种级联式流程存在两个关键局限性：1）计算成本高，难以满足实时机器人任务的需求；2）分割阶段的误差会累积并传播到后续的姿态估计阶段，影响整体精度。</p>
<p>本文针对上述“效率低”与“误差累积”的痛点，提出了一个全新的单阶段视角。核心思路是摒弃分割与姿态估计分离的范式，设计一个统一的端到端网络，一次性同时输出实例分割和NPCS表示，从而实现高效、准确的实时估计。</p>
<h2 id="方法详解">方法详解</h2>
<p>YOEO方法采用了一个简洁高效的单阶段pipeline。其整体流程是：给定铰接物体的点云输入，网络同时预测每个点的语义标签、质心偏移量和NPCS坐标；随后利用语义标签和质心偏移进行聚类，得到每个部件实例；最后，将每个实例预测的NPCS区域与真实点云对齐，恢复出该部件的6D姿态（旋转、平移）和尺寸。</p>
<p><img src="https://arxiv.org/html/2506.05719v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：YOEO整体架构概览。特征提取模块从部分点云中提取逐点特征。这些特征被送入三个并行模块，分别预测NPCS图、语义标签以及每个点到其所属实例质心的偏移量。随后应用聚类算法，根据相同的语义标签和相近的预测质心距离来区分不同的实例。最后，通过对齐算法将预测的NPCS图与真实点云配准，以估计6自由度姿态参数。</p>
</blockquote>
<p>网络以RandLA-Net作为骨干进行特征提取。提取的共享特征随后被送入三个并行的预测头，这是本方法的三个核心模块：</p>
<ol>
<li>**语义部件学习模块 (ℳ_s)**：该模块通过一个共享MLP，利用Focal Loss监督，预测每个点所属的部件类别（如手柄、盖子）。其作用是为后续聚类提供类别级区分，并迫使网络提取区分不同部件类的特征。</li>
<li>**质心偏移学习模块 (ℳ_c)**：该模块预测每个点到其所属部件实例几何中心的3D偏移向量（Δx_i），使用L1损失进行监督。其核心作用是区分同一语义类别下的不同实例（例如一个桶上的两个手柄）。预测后，通过计算“点坐标+预测偏移”得到投票后的质心，再基于质心距离进行聚类，即可实现实例分割。</li>
<li>**NPCS学习模块 (ℳ_n)**：该模块学习从观测点云到归一化部件坐标空间的映射Φ。技术细节上，它将NPCS坐标的每个轴离散化为100个区间，将回归问题转化为分类问题，并使用Softmax交叉熵损失进行监督。该模块为每个点预测其在标准部件坐标系下的坐标，是后续计算相似变换（旋转、平移、缩放）以恢复姿态和尺寸的基础。</li>
</ol>
<p>与现有两阶段方法相比，YOEO的创新点具体体现在：1）<strong>单阶段统一架构</strong>：将实例分割（通过语义+质心偏移隐式实现）与NPCS估计融合进一个网络，前向传播一次即可获得所有必要信息，显著提升效率。2）<strong>联合优化</strong>：三个任务（语义分割、质心偏移预测、NPCS映射）共享特征并并行训练，论文假设并验证了它们能通过特征共享相互促进，提升各自性能。3）<strong>隐式实例分割</strong>：通过“语义分类+质心投票”的方式替代显式的实例分割网络，简化了流程。</p>
<p><img src="https://arxiv.org/html/2506.05719v1/extracted/6517793/figs/det.jpg" alt="网络细节"></p>
<blockquote>
<p><strong>图4</strong>：YOEO的详细网络架构。采用RandLA-Net的编码器-解码器结构（包含随机采样RS、局部特征聚合LFA和上采样US），后端接三个由共享MLP构成的预测头。</p>
</blockquote>
<p>在获得逐点预测后，后处理流程如下：首先根据语义标签初步分组，然后在每个语义组内，根据预测的质心偏移进行聚类（如使用DBSCAN）以分离不同实例。对于每个聚类得到的实例点集，取其对应的预测NPCS坐标，与真实点云坐标利用RANSAC（去除外点）和Umeyama算法计算一个相似变换（SIM(3)），该变换的旋转R、平移t和缩放因子s即对应部件的6D姿态和尺寸。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验主要在<strong>GAPartNet数据集</strong>上进行，该数据集包含1,166个物体、8,489个部件实例，涵盖27个物体类别和9种部件类别，特点是部件具有显著的跨类别出现特性。评估指标包括：旋转误差(Re, °)、平移误差(Te, cm)、尺寸误差(Se, cm)、3D mIoU(%)、5°5cm精度(A5)和10°10cm精度(A10)，同时比较参数量(M)和推理速度(Hz)。</p>
<p>对比的<strong>Baseline方法</strong>包括：基于PointGroup和AutoGPart修改的基线、以及当前的两阶段SOTA方法GAPartNet。</p>
<p>关键实验结果总结如下：如表I所示，YOEO在大多数姿态估计精度指标上优于两阶段的GAPartNet，例如旋转误差从9.9°降低至9.0°，3D mIoU从51.2%提升至57.6%。最显著的提升在于效率：YOEO的参数量仅为1.9M（GAPartNet为7.9M），推理速度高达200Hz（GAPartNet为20Hz），实现了数量级的提升，满足了实时性要求。</p>
<p><img src="https://arxiv.org/html/2506.05719v1/x2.png" alt="定性结果-合成数据"></p>
<blockquote>
<p><strong>图5</strong>：在GAPartNet数据集上的定性结果。左两列展示了桶类别内铰接手柄的<strong>类别内</strong>估计结果；右两列展示了马桶和盒子类别间铰接盖子的<strong>跨类别</strong>估计结果，显示了方法的泛化能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.05719v1/x3.png" alt="定性结果-真实数据"></p>
<blockquote>
<p><strong>图6</strong>：使用YOEO方法进行真实世界感知的定性结果。左两列展示真实桶的手柄估计（类别内），右两列展示真实马桶和盒子的盖子估计（跨类别），证明了从合成数据训练到真实场景的有效迁移。</p>
</blockquote>
<p>消融实验验证了联合优化的有效性。如表II所示，将三个预测头单独训练再组合，其性能（如Re=19.6°, mIoU=52.3%）远逊于三个头并行（联合）训练的结果（Re=9.0°, mIoU=57.6%）。这证实了语义理解、实例定位和NPCS映射三个任务在共享特征下能够相互促进，是方法成功的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了一种新颖的单阶段框架</strong>：将类别级铰接物体6D姿态估计的实例分割与坐标回归任务统一到一个端到端网络中，实现了“一次估计”。</li>
<li><strong>设计了联合优化策略</strong>：通过并行训练语义分割、质心偏移和NPCS预测三个头，并利用它们之间的协同效应，显著提升了整体估计精度。</li>
<li><strong>实现了高效实时系统</strong>：所提方法大幅降低了计算复杂度和参数量，达到了200Hz的实时感知速度，并成功部署到真实机器人平台，完成了对未见铰接物体的实时抓取与操作。</li>
</ol>
<p>论文自身提到的局限性并不显著，但隐含的挑战可能包括对点云质量（如噪声、遮挡）的鲁棒性，以及在更复杂、更多部件的物体上的泛化能力。</p>
<p>本文对后续研究的启示在于：证明了在复杂感知任务中，通过精心设计的多任务联合学习框架，可以同时追求精度与效率，打破“多阶段更准”或“单阶段更快”的传统权衡。这为其他需要实例级理解和几何估计的机器人视觉任务（如动态物体操作、场景理解）提供了新的设计思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人抓取任务中的类别级铰接物体6D姿态估计问题，提出单阶段方法YOEO。现有方法多为多阶段流程，计算成本高、实时性差。YOEO通过统一网络一次性输出点级语义标签与质心偏移，利用聚类区分部件实例，并通过对齐归一化部件坐标空间（NPCS）区域恢复姿态与尺寸。实验表明，该方法在GAPart数据集上有效，且合成训练的模型可部署于真实场景，提供200Hz实时反馈，成功驱动机械臂与未见铰接物体交互。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.05719" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>