<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Teacher Motion Priors: Enhancing Robot Locomotion over Challenging Terrain - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Teacher Motion Priors: Enhancing Robot Locomotion over Challenging Terrain</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.10390" target="_blank" rel="noreferrer">2504.10390</a></span>
        <span>作者: Jin, Fangcheng, Wang, Yuqi, Ma, Peixin, Yang, Guodong, Zhao, Pan, Li, En, Zhang, Zhengtao</span>
        <span>日期: 2025/04/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在复杂地形上实现鲁棒的运动控制是机器人领域的核心挑战，这源于高维度的控制空间和环境的不确定性。主流的解决方案是教师-学生范式，即先利用仿真中的特权信息（如精确的地形几何）训练一个高性能的教师策略，然后将其知识迁移给仅能获取带噪声的本体感知数据（如关节位置、IMU数据）的学生策略，以解决从仿真到现实部署的难题。然而，现有方法仍面临两个关键局限性：一是分布偏移，即学生策略在自身观察分布下产生的行为与教师策略在特权信息下的行为存在差异，导致性能下降；二是网络结构依赖，学生策略的网络设计往往与教师强耦合，增加了部署复杂性。</p>
<p>本文针对上述痛点，提出了一个名为教师运动先验的新框架。其核心思路是：首先训练一个利用特权信息的高性能教师策略；然后，通过生成对抗模仿学习机制，将教师策略的行为分布直接迁移给学生策略，以缓解分布偏移并解耦网络设计；同时，为学生策略引入辅助任务学习，以增强其特征表示能力，加速训练并提升在动态地形上的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>TMP框架的训练包含两个阶段：教师策略训练和学生策略训练。整体流程如下图所示。</p>
<p><img src="https://arxiv.org/html/2504.10390v2/x1.png" alt="TMP Training Process"></p>
<blockquote>
<p><strong>图1</strong>：TMP训练流程总览。左下部分为教师阶段，教师策略接收特权信息（蓝色）和本体感知数据（绿色）进行训练。右上部分为学生阶段，学生策略仅接收本体感知数据，并通过判别器（Discriminator）与教师策略的行为进行对抗模仿学习，同时进行辅助任务（Auxiliary Task）学习。</p>
</blockquote>
<p><strong>教师策略</strong>：教师策略 $\pi^t$ 采用演员-评论家架构，其输入为 $N$ 帧历史本体感知数据 $o_{t-N+1:t}$ 和 $M$ 帧历史特权信息 $o^p_{t-M+1:t}$。评论家则接收 $M$ 帧无噪声的完整状态 $s_{t-M+1:t}$。训练使用近端策略优化算法，其损失函数为：<br>$$\mathcal{L}<em>{\text{teacher}} = \mathcal{L}</em>{\text{clip}} + \lambda_v \mathcal{L}_v - \lambda_e \mathcal{L}<em>e$$<br>其中，$\mathcal{L}</em>{\text{clip}}$ 为PPO的裁剪替代损失，$\mathcal{L}_v$ 为价值函数损失（均方误差），$\mathcal{L}_e$ 为鼓励探索的熵正则项。通过添加高斯噪声到本体感知输入来增强泛化能力。</p>
<p><strong>学生策略</strong>：学生策略 $\pi^s$ 的演员仅接收 $N$ 帧历史本体感知数据 $o_{t-N+1:t}$。其核心创新在于采用生成对抗机制进行模仿学习，并引入辅助任务。</p>
<ol>
<li><strong>生成对抗模仿</strong>：引入一个判别器 $\mathcal{D}$，其输入为最近 $S$ 帧的状态 $s_{t-S+1:t}$ 以及对应的动作 $a_t$，输出该动作来自教师策略的概率。判别器的损失 $\mathcal{L}<em>{\text{disc}}$ 包含三部分：用于区分教师/学生轨迹的二元交叉熵预测损失 $\mathcal{L}</em>{\text{pred}}$、防止过拟合的梯度正则化项 $\mathcal{L}<em>{\text{grad}}$ 以及权重正则化项 $\mathcal{L}</em>{\text{weight}}$。学生策略通过最大化判别器将其动作误判为教师动作的概率来学习模仿，即 $\mathcal{L}_{\text{disc}}$ 作为其损失的一部分。</li>
<li><strong>辅助任务学习</strong>：学生策略的演员网络前 $N-2$ 层与一个辅助网络共享。该辅助网络的任务是预测一组辅助观测值 $\hat{o}^{\text{aux}}<em>t$，其损失为与真实值的均方误差 $\mathcal{L}</em>{\text{aux}}$。这迫使网络底层学习更鲁棒的特征表示，以理解带噪声的本体感知输入。</li>
</ol>
<p>学生策略的总体损失函数为：<br>$$\mathcal{L}<em>{\text{student}} = \mathcal{L}</em>{\text{clip}} + \lambda_v \mathcal{L}<em>v - \lambda_e \mathcal{L}<em>e + \lambda</em>{\text{aux}} \mathcal{L}</em>{\text{aux}} + \lambda_{\text{disc}} \mathcal{L}_{\text{disc}}$$<br>其中前三项与教师策略定义相同，但基于学生参数优化。</p>
<p><strong>创新点</strong>：与现有方法相比，TMP的主要创新在于：1) 使用生成对抗机制（而非监督学习或强耦合的师生网络）进行行为分布迁移，有效缓解分布偏移并实现网络结构解耦；2) 为学生策略引入辅助预测任务，提升了从噪声观察中学习特征的能力，从而加速收敛并增强泛化。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Unitree H1全尺寸人形机器人平台上进行，使用Isaac Gym仿真环境并最终进行实物部署。评估场景包括平坦地面、楼梯、斜坡、随机崎岖地形以及动态移动平台。</p>
<p><strong>对比方法</strong>：TMP与多种主流基线方法进行了对比，包括：教师-学生协作训练（CTS）、正则化在线自适应（ROA）、对抗运动先验（AMP）以及仅使用PPO训练的基准。</p>
<p><strong>关键量化结果</strong>：</p>
<ol>
<li><strong>学习性能与成功率</strong>：如图2所示，在楼梯、斜坡和随机地形任务中，TMP的学生策略收敛速度最快，且最终的成功率最高（在最具挑战性的随机地形上达到约97.5%）。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.10390v2/x2.png" alt="Average Success Rate"></p>
<blockquote>
<p><strong>图2</strong>：在不同复杂地形任务上，各方法学生策略训练过程中的平均成功率曲线。TMP在所有地形上均展现出最快的学习速度和最高的最终性能。</p>
</blockquote>
<ol start="2">
<li><strong>运动质量与能耗</strong>：如图3所示，TMP在轨迹跟踪误差（关节位置、身体姿态）和运输成本（CoT，衡量能效的指标）上均优于对比方法，表明其运动更精确、更节能。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.10390v2/x3.png" alt="Tracking Error and Cost of Transport"></p>
<blockquote>
<p><strong>图3</strong>：各方法在测试地形上的性能对比箱线图。TMP在关节位置误差、身体方向误差和运输成本（CoT）上均取得最佳（最低）的中位数和分布。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：如图4所示，移除生成对抗模仿组件（w/o GA）或辅助任务组件（w/o Aux）都会导致性能下降，其中对抗模仿组件的贡献更为关键。两者结合（TMP-full）效果最佳。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.10390v2/x4.png" alt="Ablation Study"></p>
<blockquote>
<p><strong>图4</strong>：消融实验验证了生成对抗模仿（GA）和辅助任务学习（Aux）两个组件的有效性。移除任一组件性能均下降，同时使用两者效果最好。</p>
</blockquote>
<ol start="4">
<li><strong>实物部署</strong>：如图5所示，训练好的TMP学生策略被成功部署到真实的Unitree H1机器人上，能够在动态移动的平台上保持稳定行走，证明了其sim-to-real的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.10390v2/x5.png" alt="Real-World Deployment"></p>
<blockquote>
<p><strong>图5</strong>：TMP策略在真实人形机器人Unitree H1上的部署演示。机器人成功在动态移动的平台上实现了鲁棒行走。</p>
</blockquote>
<ol start="5">
<li><strong>开发成本</strong>：如图6所示，从策略训练时间、调试时间和总体开发时间三个维度对比，TMP框架所需的时间成本显著低于CTS和ROA等方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.10390v2/x6.png" alt="Development Cost Comparison"></p>
<blockquote>
<p><strong>图6</strong>：不同方法在策略训练、调试和总开发时间上的对比。TMP框架显著降低了整体开发成本。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为：1) 提出了一个整合生成对抗模仿学习和辅助任务学习的教师-学生框架（TMP），用于提升人形机器人在复杂地形上的运动能力；2) 通过生成对抗机制实现了教师行为分布向学生策略的高效、解耦迁移，有效缓解了分布偏移问题；3) 通过辅助任务学习增强了学生策略的特征表示，加速了训练并提高了泛化性；4) 在仿真和真实全尺寸人形机器人上进行了全面验证，证明了其在稳定性、能效和降低开发成本方面的优势。</p>
<p>论文自身提到的局限性主要在于对计算资源的要求较高，因为需要同时训练教师策略、学生策略和判别器。</p>
<p>这项工作对后续研究的启示在于：首先，在知识迁移中解耦师生网络设计是可行的，并能简化部署；其次，将模仿学习（对抗训练）与强化学习（任务奖励）相结合，能协同提升策略的鲁棒性和适应性；最后，在机器人学习框架中显式考虑并优化开发与部署成本，具有重要的实际意义。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人在复杂地形上运动稳健性不足的问题，提出一种基于师生范式的教师运动先验框架。该方法首先利用特权信息训练高性能教师策略以获取泛化运动技能；随后通过生成对抗机制，将教师运动分布迁移至仅依赖噪声本体感知数据的学生策略，以缓解分布偏移导致的性能下降；并引入辅助任务学习增强学生特征表示。实验表明，该框架显著提升了人形机器人在动态地形上的运动稳定性，并大幅降低了开发成本。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.10390" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>