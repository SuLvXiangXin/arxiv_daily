<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLM-TDP: VLM-guided Trajectory-conditioned Diffusion Policy for Robust Long-Horizon Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VLM-TDP: VLM-guided Trajectory-conditioned Diffusion Policy for Robust Long-Horizon Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.04524" target="_blank" rel="noreferrer">2507.04524</a></span>
        <span>作者: Lei Han Team</span>
        <span>日期: 2025-07-06</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域的主流方法之一是模仿学习，其中扩散策略通过去噪扩散概率模型近似动作分布，在处理多模态和高维动作空间方面展现出潜力。然而，现有方法主要局限于短视野任务，并且对单模态视觉输入（如RGB图像）有很强的依赖性，导致其在面对图像噪声或输入变化时性能显著下降，尤其是在复杂的长视野任务中。本文针对扩散策略在长视野任务中表现不佳以及对视觉噪声敏感这两个关键痛点，提出利用先进的视觉语言模型来提供高层任务分解和空间轨迹引导的新视角。本文的核心思路是：利用VLM将复杂的长视野任务分解为可管理的子任务，并为每个子任务生成体素化的空间轨迹，以此作为关键条件来引导扩散策略，从而提升其在长视野任务中的成功率和鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLM-TDP的整体框架分为两个核心阶段：1) 利用VLM进行任务分解与轨迹生成；2) 基于生成轨迹的条件扩散策略执行。第一阶段输入高级任务指令和初始视觉观测，输出一系列子任务描述及其对应的体素轨迹。第二阶段输入当前观测和轨迹条件，输出机器人动作序列。</p>
<p><img src="https://arxiv.org/html/2507.04524v1/extracted/6591369/images/tdp.drawio.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：VLM-TDP方法整体框架。左侧上方展示了训练阶段从示教数据中提取轨迹的过程；左侧下方展示了推理阶段利用VLM进行任务分解和轨迹生成的过程；右侧展示了轨迹条件扩散策略根据观测和轨迹条件生成鲁棒动作。</p>
</blockquote>
<p><strong>核心模块一：VLM任务分解与轨迹生成</strong>。该模块分两步。首先，<strong>任务分解</strong>：给定高级任务描述和初始观测，VLM将任务分解为一系列子任务。论文定义子任务为以打开或关闭夹爪开始、以关闭或打开夹爪结束的离散操作阶段，这避免了子任务内轨迹的重复或反转。例如，“将物品放入抽屉”被分解为“抓住底层抽屉把手”、“拉开抽屉”、“抓住物品”、“将物品放入底层抽屉”四个子任务。其次，<strong>轨迹生成</strong>：对于每个子任务，采用基于掩码的视觉提示方法。将前视相机图像投影为俯视图并补全遮挡阴影，同时创建高度图。将观测空间划分为M×N×K（论文设为6×6×6）的网格。向VLM提供带有网格标注的图像和子任务描述，VLM通过多选提示生成一系列网格坐标索引，构成该子任务的轨迹。</p>
<p><strong>核心模块二：轨迹条件扩散策略</strong>。该方法基于去噪扩散概率模型，目标是近似条件分布p(At|Ot, T)，其中T为轨迹条件。其关键修改在于噪声预测器εθ现在同时接受观测Ot、去噪步数k、带噪声的动作Atk以及轨迹条件T作为输入。训练时，从示教数据中提取轨迹：将每个时间步的末端执行器位置通过相机标定参数映射到对应的网格位置，形成体素轨迹标签。损失函数是预测噪声与实际噪声的均方误差。在实践中，为了融入轨迹条件，使用3D CNN对体素轨迹进行编码，然后将编码后的轨迹与编码的历史图像观测序列及展平的机器人状态进行拼接。值得注意的是，轨迹信息在每个子任务内仅拼接一次，假设其恒定。</p>
<p><img src="https://arxiv.org/html/2507.04524v1/extracted/6591369/images/show_subtask.drawio.png" alt="体素轨迹示例"></p>
<blockquote>
<p><strong>图2</strong>：体素空间轨迹及其对应的末端执行器运动（蓝线）示例。轨迹被表示为M×N×K矩阵，轨迹上的体素按顺序标记，非轨迹体素标记为0。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法相比，主要创新体现在：1) <strong>利用VLM进行长视野任务分解</strong>，使扩散策略只需处理更简单的子任务；2) <strong>提出基于VLM的体素空间轨迹生成方法</strong>，为策略提供了明确的空间指引；3) <strong>将生成的轨迹作为扩散策略的额外条件模态</strong>，减少了对单一RGB观测的依赖，从而提升了鲁棒性。这不同于仅使用语言、目标图像或点云进行条件化的方法。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在RLBench模拟器（使用CoppeliaSim和Franka Panda机器人）和真实世界（Franka Panda机器人）上进行。使用了单视角（前视）和多视角（前视+腕部）两种设置。对比的基线方法包括：Diffusion Policy (DP)、3D Diffusion Policy (DP3)、Language-guided object-centric Diffusion Policy (Lang-o3dp)。本文评估了两种变体：TDP（使用真实轨迹）和VLM-TDP（使用VLM生成轨迹）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>RLBench任务评估（7个任务）</strong>：如表I所示，TDP和VLM-TDP在全部7个任务上均超越了基础的Diffusion Policy。尽管是2D策略，其平均成功率与先进的3D策略（DP3, Lang-o3dp）相当甚至部分超越，平均有3%的提升。在物体拥挤（如Water Plants）或形状扁平不易区分（如Phone on Base）的任务中，本文方法优势明显。在长视野任务Put Item in Drawer上，得益于任务分解，取得了优异表现。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04524v1/extracted/6591369/images/complete_task.png" alt="RLBench任务结果表"></p>
<blockquote>
<p><strong>图10</strong>：RLBench七个任务的成功率对比表。TDP和VLM-TDP在所有任务上超过基础扩散策略，并与3D策略性能相当。</p>
</blockquote>
<ol start="2">
<li><strong>长视野任务评估（堆叠方块）</strong>：如表II所示，在堆叠1、2、4个方块的任务中，本文方法（TDP/VLM-TDP）在“抓取”子任务上的成功率相比扩散策略有50%到200%的提升。随着任务复杂度（方块数）增加，所有方法性能下降，但本文方法下降幅度更小。例如，堆叠4个方块时，完整任务成功率（Comb）从扩散策略的0.02提升到TDP的0.17。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04524v1/extracted/6591369/images/pick_task.png" alt="长视野任务结果表"></p>
<blockquote>
<p><strong>图11</strong>：长视野堆叠方块任务的成功率对比表。展示了分解后的“抓取”和“放置”子任务以及整体任务的成功率，本文方法在复杂配置下优势显著。</p>
</blockquote>
<ol start="3">
<li><p><strong>鲁棒性评估</strong>：</p>
<ul>
<li><p><strong>图像噪声</strong>：向前视RGB图像添加不同程度的高斯噪声。如图13-16所示，随着噪声增强，所有方法性能下降，但本文方法（TDP）的下降幅度比扩散策略小约20%。<br><img src="https://arxiv.org/html/2507.04524v1/extracted/6591369/images/front_rgb_noise_0.32.png" alt="噪声鲁棒性"></p>
<blockquote>
<p><strong>图15</strong>：添加噪声水平为0.32时，各方法在<code>Put Item in Drawer</code>任务上的成功率对比。TDP表现出更强的抗噪声能力。</p>
</blockquote>
</li>
<li><p><strong>环境变化</strong>：测试了背景纹理替换、物体纹理替换、物体尺寸缩放（85%，115%）。如图17-20所示，在各种变化下，TDP的性能下降幅度均小于扩散策略，验证了轨迹条件降低了对视觉外观细节的依赖。<br><img src="https://arxiv.org/html/2507.04524v1/extracted/6591369/images/eval_object_texture.png" alt="环境变化鲁棒性"></p>
<blockquote>
<p><strong>图18</strong>：物体纹理变化下的成功率对比。TDP相比扩散策略保持了更高的成功率。</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>真实世界实验</strong>：在Franka Panda机器人上执行“将物品放入抽屉”和“堆叠杯子”任务。如图25-26所示，VLM-TDP在长视野任务中表现显著优于扩散策略，性能差距比模拟中更为明显。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.04524v1/extracted/6591369/images/real-setup.png" alt="真实实验"></p>
<blockquote>
<p><strong>图25</strong>：真实世界实验设置。VLM-TDP在真实长视野操作任务中成功率高。</p>
</blockquote>
<p><strong>消融实验总结</strong>：TDP（使用真实轨迹）的性能验证了轨迹条件本身的有效性；VLM-TDP（使用VLM生成轨迹）的性能虽略低于TDP，但仍大幅超越基线，证明了整个VLM引导 pipeline 的可行性。轨迹条件模块是提升鲁棒性的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了利用VLM将长视野操作任务分解为可管理子任务的框架，显著提升了扩散策略处理复杂任务的能力；2) 创新性地提出了基于VLM的体素空间轨迹生成方法，并将其作为扩散策略的强大条件信号；3) 通过轨迹条件化，降低了策略对原始视觉观测的敏感度，在噪声和环境变化下表现出更强的鲁棒性。</p>
<p>论文自身提到的局限性在于：VLM生成的轨迹在物体靠得很近时可能产生模糊性，这是由于规划网格分辨率的限制，提高分辨率又会增加VLM生成准确轨迹的难度。</p>
<p>本文对后续研究的启示在于：展示了将VLM的高层语义规划、空间推理能力与扩散策略的灵活动作生成能力相结合的有效路径。这种“VLM规划器+条件化策略”的范式为处理更复杂、多样化的机器人操作任务提供了新思路。未来工作可探索更精细的轨迹表示、更高效的VLM提示方法，或将此框架与强化学习等其他学习范式结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对扩散策略在长视界机器人操作任务中性能受限且对图像噪声敏感的问题，提出了VLM-TDP方法。该方法利用视觉语言模型（VLM）将长任务分解为子任务，并生成体素轨迹作为条件引导轨迹条件扩散策略（TDP）。实验表明，VLM-TDP在模拟环境中平均成功率比经典扩散策略提高44%，长视界任务性能提升超100%，在噪声图像等挑战下性能下降减少20%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.04524" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>