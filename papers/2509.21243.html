<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.21243" target="_blank" rel="noreferrer">2509.21243</a></span>
        <span>作者: Koo, Jiyeon, Cho, Taewan, Kang, Hyunjoon, Pyo, Eunseom, Oh, Tae Gyun, Kim, Taeryang, Choi, Andrew Jaeyong</span>
        <span>日期: 2025/09/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型（如RT-2、OpenVLA）通过整合网络规模的知识，在机器人控制中展现出卓越的泛化能力。然而，这种成功依赖于数十亿参数的大型模型，带来了巨大的计算成本，限制了其在计算资源有限的真实机器人平台上的实际部署。为了解决效率问题，主流轻量化方法（如SmolVLA）主要通过减少模型规模（如减少VLM深度、视觉令牌缩减）来实现。但这不可避免地导致了性能与效率之间的权衡，尤其是牺牲了模型对复杂空间关系和长期上下文的理解能力，而这些对于机器人智能至关重要。</p>
<p>本文针对轻量化VLA模型牺牲空间推理能力这一具体痛点，提出了一个新颖的视角：不从“删除”信息入手，而是“主动重用”信息。具体而言，论文关注到现代大规模视觉Transformer（ViT）在训练中会产生被称为高范数离群令牌的伪影，而引入Register Token的既定解决方案是将其作为显式的“草稿本”来吸收这些伪影，随后在下游任务中被系统性地丢弃。本文质疑这一做法，提出这些被丢弃的Register Token中蕴含了场景的全局空间上下文信息（如整体布局、物体间的3D关系），对于机器人操作是至关重要的资源。</p>
<p>本文的核心思路是：将通常被丢弃的Register Token重新定义为主动的“空间上下文提供者”，并将其直接注入到轻量化VLA模型的Action Expert中，从而在保持高效的同时，显著增强模型的空间推理能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>RetoVLA的整体架构建立在标准的VLM主干和Action Expert范式之上。其关键创新在于重新设计了信息流，建立了一条双流路径：1）来自VLM的高层语义特征；2）来自Register Token的、对场景全局空间信息的压缩摘要。这两股信息流在Action Expert的交叉注意力层中进行融合，以生成最终具有空间感知能力的动作。</p>
<p><img src="https://arxiv.org/html/2509.21243v1/fig2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：RetoVLA架构。核心创新是空间上下文注入路径（虚线箭头）。从视觉编码器产生的、总结全局场景信息的Register Token，经过一个可学习的门控，然后作为Key-Value对注入到Action Expert的最终交叉注意力层中，使其能够同时基于高层语义特征和全局空间上下文进行推理。</p>
</blockquote>
<p><strong>整体框架与信息流</strong>：模型采用一个“较浅”的VLM主干（仅使用预训练VLM的前N=L/2层），以平衡语义特征提取和推理速度。视觉编码器输出的图像块嵌入被输入到<strong>空间上下文聚合器</strong>模块（由一个标准的多头注意力块实现）。该模块以一组可学习的初始Register Token作为Query，以图像块作为Key和Value，生成场景相关的Register Token。这些Token被投影并与来自VLM的标准语义Key-Value对在Action Expert的最终交叉注意力层中进行拼接，形成最终的Key和Value，供Action Expert的Query进行注意力计算。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>Register Token生成</strong>：通过注意力机制，使Register Token“查询”整个视觉场景，将最显著的全局信息汇总到一组场景相关的Register Token中。公式为：$\mathbf{R}<em>{\text{scene}}=\text{Attention}(\mathbf{Q}=\mathbf{R}</em>{\text{init}},\mathbf{K}=\mathbf{P},\mathbf{V}=\mathbf{P})$。</li>
<li><strong>注入到Action Expert</strong>：生成的$\mathbf{R}<em>{\text{scene}}$被投影并转换为Key ($\mathbf{K}</em>{\text{reg}}$)和Value ($\mathbf{V}<em>{\text{reg}}$)对。它们与VLM的语义Key-Value对 ($\mathbf{K}</em>{\text{vlm}}, \mathbf{V}<em>{\text{vlm}}$) 在最终交叉注意力层拼接：$\mathbf{K}</em>{\text{final}} = \text{Concat}(\mathbf{K}<em>{\text{vlm}},\sigma(g)\cdot\mathbf{K}</em>{\text{reg}})$, $\mathbf{V}<em>{\text{final}} = \text{Concat}(\mathbf{V}</em>{\text{vlm}},\sigma(g)\cdot\mathbf{V}_{\text{reg}})$。</li>
<li><strong>门控机制</strong>：引入一个可学习的标量参数$g$，通过sigmoid函数$\sigma(\cdot)$缩放Register Token的影响。这使得模型能够自适应地调节全局空间上下文的贡献，对于需要整体场景理解的任务增强其影响，对于需要局部精确控制的任务则抑制其影响。</li>
<li><strong>训练目标</strong>：采用条件流匹配目标进行训练。模型学习一个向量场$\mathbf{v}<em>{\theta}(\mathbf{a}</em>{t}, t, c)$，用于将噪声动作序列$\mathbf{a}<em>{t}$在条件$c$（视觉观察、语言指令、机器人状态）的引导下，向真实动作$\mathbf{a}</em>{0}$转化。损失函数为均方误差：$\mathcal{L}<em>{\text{FM}}=\mathbb{E}</em>{t,\mathbf{a}<em>{1},\mathbf{a}</em>{0},c}\left[\left|\mathbf{v}<em>{\theta}(\mathbf{a}</em>{t},t,c)-(\mathbf{a}<em>{1}-\mathbf{a}</em>{0})\right|^{2}\right]$。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：与主要通过信息缩减来实现轻量化的传统方法（如SmolVLA）不同，RetoVLA的核心创新在于<strong>信息的主动再利用</strong>。它首次将Vision Transformer中为去除伪影而引入、随后被丢弃的Register Token，重新定义为有价值的空间上下文载体，并通过一个非侵入式的注入机制，使其能够直接辅助机器人动作生成，从而补偿了轻量化模型因深度变浅而损失的空间信息。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：使用LIBERO仿真基准，包含Spatial、Object、Goal、10 (Long)四个类别共40个任务。</li>
<li><strong>真实世界实验</strong>：在自定义构建的7自由度机械臂上，执行7种不同的操作任务（详见表I）。</li>
<li><strong>自定义仿真环境</strong>：使用Unity和MuJoCo插件构建，以桥接仿真与真实世界的差距。</li>
<li><strong>对比基线</strong>：主要对比方法是SmolVLA。所有模型均使用SmolVLM2-500M的前16层作为VLM主干，在相同超参数下训练，唯一区别是RetoVLA注入了2个Register Token。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>LIBERO基准结果</strong>：如表II所示，在整体成功率上，RetoVLA与基线相比提升不大（Spatial: 75.8%→76.2%，Object: 70.8%→71.8%，Goal和Long任务持平）。但深入分析（图5）揭示，RetoVLA在需要<strong>工作记忆</strong>（+11.5%p）和<strong>全局与3D空间推理</strong>（+9.0%p）的任务上表现显著更优，而在需要<strong>极端局部精度</strong>的任务上则出现性能下降。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.21243v1/fig4.png" alt="性能分析"></p>
<blockquote>
<p><strong>图5</strong>：按核心能力分组的RetoVLA性能分析。与基线相比的平均成功率变化。RetoVLA在需要高级推理（工作记忆、全局与3D空间推理）的任务上显著提升，但在需要极端局部精度的任务上存在权衡。</p>
</blockquote>
<ol start="2">
<li><strong>真实世界实验结果</strong>：如表IV所示，RetoVLA显著优于基线，将平均成功率从50.28%提升至67.42%，实现了**+17.14%p的绝对提升**。在需要深度空间理解的任务上提升尤为显著，例如“Close Drawer”（+36%p）和“Build Domino Line”（+28%p）。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.21243v1/fig6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：重用Register Token实现了复杂的3D空间推理。基线SmolVLA因抓取视觉相似但错误的物体而失败，而RetoVLA通过利用注入的空间上下文，正确理解了“在顶部抽屉里”的指令，展示了其理解复杂多步操作指令的卓越能力。</p>
</blockquote>
<ol start="3">
<li><strong>自定义仿真实验结果</strong>：如表III所示，结果与真实世界趋势一致，平均成功率提升+12.0%p（62.8%→74.8%），在“Build Domino Line”（+24.0%p）和“Jenga”（+16.0%p）等任务上获得最大增益。</li>
</ol>
<p><strong>消融实验</strong>：<br>附录中的消融实验（表V）表明，使用<strong>2个Register Token</strong>时性能最佳（成功率76.2%），优于基线（75.8%）。增加Token数量（4、12、16）会导致性能下降，表明过多的Token可能引入干扰噪声或冗余，而非有用的全局信息。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>重新定义Register Token的角色</strong>：将其从被动的“净化器”转变为主动的“空间上下文提供者”，并设计了相应的注入架构（Spatial Context Injection）。</li>
<li><strong>分析与高效VLA的协同作用</strong>：证明了Register Token注入能够补偿因VLM深度变浅（如SmolVLA）造成的信息损失，是维持高水平空间推理的有效途径。</li>
<li><strong>实验验证</strong>：通过LIBERO基准、真实机器人及自定义仿真实验，证实RetoVLA能显著提升轻量化VLA在复杂、多步操作任务上的性能，特别是在需要工作记忆和全局3D空间推理的任务上。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>在需要极端局部精度的任务上存在性能权衡，全局上下文信息可能对精细的局部控制造成干扰。</li>
<li>研究主要聚焦于轻量化模型，尚未在OpenVLA等更大骨干网络上验证此方法的有效性。</li>
<li>尚未在动态环境或其他机器人领域（如移动导航）进行评估。</li>
</ol>
<p><strong>对后续研究的启示</strong>：<br>本文为轻量化VLA模型的设计提供了一条新思路：<strong>从“信息缩减”转向“信息再利用”</strong>。未来的工作可以探索更复杂的门控或融合机制，以更好地平衡全局上下文与局部精度；将此方法扩展到更大的模型架构和其他机器人领域；并进一步研究Register Token中所编码空间信息的具体性质。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决视觉-语言-动作模型因参数量大、计算成本高而难以实际部署的问题，同时避免轻量化方法损害空间推理能力。提出RetoVLA架构，其关键技术是重用视觉编码器中原本被丢弃的Register Tokens，通过可学习门控将其作为Key-Value对注入动作专家的交叉注意力层，从而为决策注入全局空间上下文。在7自由度机械臂的复杂操作任务上，该模型相比基线取得了17.1%的绝对成功率提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.21243" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>