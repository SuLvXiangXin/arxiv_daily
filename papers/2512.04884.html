<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Hoi! – A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Hoi! – A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.04884" target="_blank" rel="noreferrer">2512.04884</a></span>
        <span>作者: Zuria Bauer Team</span>
        <span>日期: 2025-12-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>计算机视觉正从纯粹感知转向动态交互时代，旨在理解物体如何被使用、移动或交互。这一进展主要由数据驱动方法和捕获多样化人-物交互的大规模数据集（如EpicKitchens、EgoExo4D）推动。然而，不同研究领域所研究的交互性质存在根本差异。以人为中心的视频数据集强调烹饪、家具组装等长时程活动，而机器人数据集主要针对抓放、抽屉开关等短时程操作。这种数据鸿沟使得研究有趣的迁移问题变得困难：交互力预测器能否泛化到人类视频？铰接跟踪方法在机器人外中心视角下是否依然有效？人类手演示的交互能否重定向到两指机器人夹爪？</p>
<p>铰接家具提供了一个特别丰富但研究不足的案例。尽管在日常人类活动中普遍存在，但人们与家具交互的精选视频数据却很稀少。现有的铰接数据集（如RBO）提供了有价值的标签，但主要基于静态扫描构建，缺乏将这些标注与真实交互数据配对所需的运动数据。为了弥补这一差距，本文提出了一个用于带力感知、跨视角、多具身的铰接家具操作的数据集。核心思路是构建一个耦合了视觉、力和触觉信号，并覆盖人类手、手持工具和机器人夹爪等多种具身的统一数据集，以支持跨模态感知、操作学习和具身推理的研究。</p>
<h2 id="方法详解">方法详解</h2>
<p>Hoi! 数据集的构建是一个系统性的多模态数据采集与对齐流程。</p>
<p><img src="https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiPipeline1-2.png" alt="数据采集流程总览"></p>
<blockquote>
<p><strong>图45</strong>：Hoi! 数据集采集流程总览。展示了从环境扫描、多设备设置、多具身交互演示，到数据同步与标注的完整pipeline。</p>
</blockquote>
<p><strong>整体框架与数据内容</strong>：数据集在38个真实室内环境（厨房、浴室、办公室、客厅）中采集，包含381个铰接部件（如抽屉、门、冰箱、洗碗机）上的3048个交互序列。每个交互序列的核心是<strong>四种操作具身</strong>：1) 人手；2) 人手 + 腕戴相机；3) 手持UMI夹爪；4) 自定义Hoi!夹爪（配备力扭矩和触觉传感）。对于每种具身，数据从<strong>多个视角</strong>同步采集：一个第一人称视角相机（Project Aria，提供RGB、SLAM、眼动追踪、手部姿态）、两个静态的第三人称视角iPhone（提供RGB + LiDAR深度），以及在工具具身上可能的腕戴视角。此外，每个环境在交互前后都使用Leica RTC360激光扫描仪进行高分辨率3D扫描，提供场景几何真值。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>自定义Hoi!夹爪</strong>：这是实现高质量力与触觉数据采集的关键硬件创新。<br><img src="https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/gripper_real.jpg" alt="自定义Hoi!夹爪"></p>
<blockquote>
<p><strong>图47</strong>：自定义Hoi!夹爪。采用对向设计，配备两个GelSight Digit触觉传感器进行高分辨率触觉成像，以及一个Bota SensONE六维力扭矩传感器测量腕部交互力。夹爪安装在手持“杆式”装置上，通过手柄中的负载传感器调节夹持力。<br>   该夹爪集成了GelSight Digit触觉传感器（用于高分辨率触觉成像）、Bota SensONE六维力扭矩传感器（用于测量交互力）和Dynamixel电机。它被安装在手持装置上，便于人类操作员进行演示，同时能同步记录视觉（腕戴ZED Mini立体相机和Aria设备）、触觉和力信号。</p>
</blockquote>
</li>
<li><p><strong>时空对齐流程</strong>：</p>
<ul>
<li><strong>时间对齐</strong>：所有独立运行的设备通过向每个视频流中以25Hz频率显示编码Unix时间戳的QR码进行同步。检测和解码这些QR码可为每个视频流计算相对于公共参考时间的偏移，实现所有模态在共享时间线上的对齐，精度约为10-25毫秒。</li>
<li><strong>空间对齐</strong>：所有记录设备通过基于高分辨率3D扫描的视觉定位，被配准到一个公共参考坐标系。使用<code>hloc</code>从扫描生成的全景图像建立2D-3D对应关系，进而估计每个传感器轨迹到共享世界坐标系的单一刚性变换。</li>
</ul>
</li>
<li><p><strong>标注</strong>：除了原始传感器流，数据集还提供了丰富的标注，包括：铰接类型（棱柱关节或旋转关节）和轴、物体部件的3D掩码（通过SAMv2在 panoramic 图像上预测并提升至3D点云生成）、语言描述以及序列级状态变化标签。</p>
</li>
</ol>
<p><strong>创新点</strong>：与现有数据集（总结于论文表1）相比，Hoi! 的创新性体现在首次<strong>同时</strong>提供了：1) <strong>多视角</strong>（第一人称、第三人称、腕戴）同步视频；2) <strong>多具身</strong>（人手、手持工具、机器人夹爪）执行相同任务的对齐记录；3) <strong>多模态</strong>耦合，特别是同步的<strong>力扭矩</strong>和<strong>高分辨率触觉</strong>信号；4) 以高精度<strong>3D场景扫描</strong>作为几何基础真值。这种设计旨在弥合人类活动理解与机器人物理操作之间的鸿沟。</p>
<p><img src="https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiVisuals.png" alt="多视角数据示例"></p>
<blockquote>
<p><strong>图46</strong>：数据集中多视角记录的示例。每一行对应不同的操作设置（人手、Hoi!夹爪等），展示了同步的第三人称视角（左）、第一人称视角（中）和腕戴视角（右）图像。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>本文使用Hoi!数据集本身作为benchmark，评估了三个关键任务，以展示其支持的研究价值。</p>
<p><strong>评估任务与Baseline方法</strong>：</p>
<ol>
<li><strong>野外铰接物体估计</strong>：评估方法从RGB(-D)观测中推断铰接类型和运动参数的能力。对比方法包括：ArtGS（基于高斯泼溅）、ArtiPoint（基于点跟踪）以及GPT-5视觉语言模型（仅预测类型）。</li>
<li><strong>触觉力估计</strong>：评估模型仅从触觉图像（GelSight Digit）预测作用在夹爪上的法向力和切向力的能力。使用最先进的触觉表征学习模型Sparsh（配备DINO和DINOv2解码器）进行评估。</li>
<li><strong>视觉力估计</strong>：评估模型从RGB-D观察和语言目标中预测所需三维交互力的能力。使用为文本引导操作设计的ForceSight模型进行零样本评估。</li>
</ol>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/forces_figure.png" alt="交互力曲线示例"></p>
<blockquote>
<p><strong>图43</strong>：不同铰接部件测量到的交互力曲线示例。展示了力的大小如何随铰接部件类型（如抽屉、门）变化，凸显了力信号对视觉线索的补充价值。</p>
</blockquote>
<ol>
<li><p><strong>铰接估计结果（论文表4）</strong>：现有方法在Hoi!的复杂野外场景下面临显著挑战。ArtiPoint在Hoi!上的性能（棱柱关节识别率P_prismatic=26.90%， 旋转关节识别率P_revolute=57.10%）远低于其在更受控的Art4D数据集上的表现（P_prismatic=68%， P_revolute=98%）。性能下降主要归因于单目深度估计噪声、场景杂乱以及手部遮挡。相比之下，GPT-5仅从单张图像进行类型预测表现出较强的鲁棒性（在Hoi!上P_prismatic=88.5%， P_revolute=74.3%）。</p>
</li>
<li><p><strong>触觉力估计结果（论文表5）</strong>：Sparsh模型在Hoi!数据上的力预测误差在几牛顿量级（例如，使用DINO时，组合力RMSE为3.86 N），远高于其在受控实验室环境、使用简单压头训练时达到的毫牛顿级精度。这表明，在受控实验室设定下表现优异的触觉模型，难以泛化到非结构化真实世界交互中遇到的复杂接触几何和负载状态。</p>
</li>
<li><p><strong>视觉力估计结果（论文表6）</strong>：ForceSight模型在Hoi!数据集上的力预测误差（整体RMSE较高）也显著高于其在原始数据集上的性能（0.404 N）。特别是在包含需要较大操作力的部件（如带磁铁的抽屉）的环境中，模型表现更差（例如<code>kitchen_7</code>的RMSE为3.531 N）。这揭示了现有视觉力预测模型对复杂、高力需求场景的泛化能力有限。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiStats.png" alt="数据集统计分布"></p>
<blockquote>
<p><strong>图44</strong>：Hoi!数据集中环境和铰接交互类别的分布。条形图显示了不同铰接类别上人类交互的相对频率，饼图总结了所涉及环境的比例。</p>
</blockquote>
<p><strong>消融实验总结</strong>：本文的实验本质上是利用Hoi!数据集对现有方法在新基准上的性能评估，而非对数据集自身组件的消融。实验结果清晰地<strong>验证了数据集的必要性</strong>：当前在受控环境下开发的方法（无论是铰接估计、触觉力预测还是视觉力预测）在迁移到真实、复杂、多变的野外交互场景时，性能均出现显著下降，凸显了利用Hoi!这类真实世界多模态数据进行研究和模型改进的重要性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出并开源了一个新颖、全面的多模态铰接操作数据集</strong>：Hoi! 提供了3048个序列，涵盖381个物体、38个环境、四种操作具身，并同步采集了RGB-D、力扭矩、触觉、姿态等多种信号，辅以高精度3D扫描真值和丰富标注。</li>
<li><strong>设计并开源了一个用于野外交互力采集的自定义夹爪硬件</strong>：该夹爪实现了高质量触觉与力信号的同步移动采集。</li>
<li><strong>建立了基于真实物理交互的基准测试</strong>：通过在Hoi!上评估现有SOTA方法，揭示了它们在复杂野外场景下面临的泛化挑战，为未来研究指明了方向。</li>
</ol>
<p><strong>局限性</strong>：论文提到，数据集主要关注铰接家具，可能在其他类型的交互或物体上多样性有限。此外，交互是分段录制的，而非连续的长时程活动序列。</p>
<p><strong>对后续研究的启示</strong>：Hoi!数据集为多个研究方向提供了基础：1) <strong>跨具身技能迁移</strong>：研究如何将人类演示的技能（特别是力觉策略）迁移到不同形态的机器人上；2) <strong>多模态感知融合</strong>：探索如何结合视觉、力和触觉信号来更鲁棒地理解与预测交互；3) <strong>真实世界泛化</strong>：利用该数据训练或微调模型，以提升在非结构化环境中的铰接估计、力预测和操作性能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有交互数据集在人类活动与机器人操作间存在割裂、缺乏力觉与多视角同步数据的问题，提出了Hoi!多模态数据集。该数据集核心包含3048个交互序列，覆盖381个铰接物体，并首次为每个物体提供四种操作具身（人手、腕戴相机人手、手持UMI夹爪、自定义Hoi!夹爪），同步采集RGB、深度、力觉、触觉及多视角视频。数据集通过标注铰接参数（如开合角度、位移、峰值力），支持跨视角与跨具身的迁移研究，为多模态感知、操作学习及力觉预测等任务提供了基准。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.04884" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>