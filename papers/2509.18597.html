<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.18597" target="_blank" rel="noreferrer">2509.18597</a></span>
        <span>作者: Alois Knoll Team</span>
        <span>日期: 2025-09-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，基于大语言模型（LLM）的机器人操作代码生成方法，如Code-as-Policies（CaP），通过将人类指令直接翻译成可执行代码，展现了巨大潜力。然而，现有方法存在三个关键局限性：1）语言固有的歧义性；2）LLM输出在长时间规划中噪声大、易出错；3）受限于预定义基元、手工提示词以及有限的上下文窗口。这导致智能体难以解决复杂的长时间任务。虽然已有工作探索了闭环反馈，但仅依赖LLM指导的方法在极端长时间场景中经常失败，因为LLM在机器人领域的推理能力有限，而这些错误对人类而言通常易于识别。此外，修正后的知识常以不当格式存储，限制了泛化能力并导致灾难性遗忘，突显了对可复用、可扩展技能学习的迫切需求。</p>
<p>本文针对上述痛点，提出了一个新视角：将人类置于学习循环中，通过反馈构建可复用的技能，并随时间扩展其功能，实现偏好对齐的终身学习。本文的核心思路是：提出一个名为LYRA的人机协作终身技能学习与代码生成框架，通过将人类反馈编码为结构化技能、利用外部记忆进行检索增强生成（RAG）以及用户设计的课程式扩展，使智能体能够稳健地解决需要超过20个基元规划的极端长时间任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>LYRA框架的整体流程是一个三阶段的人机协作流水线，旨在将用户反馈编码为可复用的技能，并随时间扩展其功能以应对长期任务。</p>
<p><img src="https://..." alt="框架总览"></p>
<blockquote>
<p><strong>图1</strong>：框架总览：人机协作终身技能学习与任务部署。展示了用户、智能体与外部记忆的交互流程，包括技能学习、存储、提示、检索以及任务特定计划生成与部署。</p>
</blockquote>
<p><img src="https://..." alt="方法流水线"></p>
<blockquote>
<p><strong>图2</strong>：所提出的人机协作终身技能学习流水线结构。分为三个阶段：I. 偏好对齐技能获取；II. 终身学习导向的能力扩展；III. 任务特定导向的检索与规划。</p>
</blockquote>
<p><strong>阶段一：偏好对齐技能获取</strong><br>此阶段目标是从头学习一个新技能。流程始于用户用自然语言描述所需技能（如“学习一个堆叠积木的技能”）。LLM通过“技能解析”API生成一个带有建议参数的Python函数头（如<code>def stackblocks(blocks, startpose):</code>），用户可接受或修改。定义确定后，LLM会请求一个基础环境设置（用户控制复杂度，建议1-4个基元内）。随后，智能体生成候选代码并在模拟中部署，实时显示结果。用户提供自由形式的反馈（接受、拒绝或修正），指导智能体调整实现细节，直到技能符合用户偏好。此阶段允许在同一场景下测试技能对不同条件的泛化能力（如边缘对齐、旋转）。</p>
<p><strong>阶段二：终身学习导向的能力扩展</strong><br>为应对复杂变化并防止灾难性遗忘，此阶段引入用户设计的课程，自底向上扩展技能功能。用户设计超出原始技能分布、且与长期目标一致的新任务（如从“堆叠积木”扩展到“按颜色堆叠两个塔”）。智能体在解决新任务时，会收到一个元提示，要求其“尝试保留先前功能”。扩展方式有两种：1）创建一个调用现有技能作为嵌套函数的新命名技能；2）通过模块化代码（如if-else）扩展现有技能，避免覆盖。扩展后，会重新评估先前任务以确保性能得以保持。所有学习到的技能<code>z</code>和探索成功的示例<code>(l, c)</code>（指令与代码计划）均存储于外部记忆，供后续检索。</p>
<p><strong>阶段三：任务特定导向的检索与规划</strong><br>此阶段针对新的长时间任务指令进行规划。核心是动态管理上下文窗口。框架使用两个向量数据库（分别索引示例和技能），通过计算指令与存储内容的余弦相似度，检索最相关的K个示例（K=10）和技能，并附加到提示词中。此外，框架引入了<strong>提示机制</strong>：当检索不足或返回无关信息时，用户可以通过提示（hint）直接指明可能相关的已学技能，引导智能体，从而减少不必要的修正轮次。如果所需子行为未学习且无法通过提示解决，则失败信号会提示用户暂停当前任务，先教授缺失的子行为。通过此流水线，框架能够组合已学技能，解决如“建造房屋”等极端长时间任务。</p>
<p>与现有方法相比，LYRA的创新点具体体现在：1）将人类反馈结构化为可复用、可嵌套的“技能”函数，而非扁平代码或模糊知识，实现了稳定且可泛化的知识存储；2）通过用户设计的课程和“保留功能”的元提示，系统化地扩展技能能力，实现了真正的终身学习；3）结合RAG和用户提示的动态上下文管理机制，有效解决了无关信息干扰和上下文窗口限制问题，提升了长时规划的效率和鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在多个模拟基准测试和真实世界环境中进行。<strong>数据集/平台</strong>包括：PyBullet-based Ravens（桌面操作）、Franka Kitchen（厨房长时任务）、MetaWorld（多样化桌面任务）以及真实世界的Franka FR3机器人。<strong>对比的基线方法</strong>有：Code as Policy (CaP，开环)、LoHoRavens (GPT-4o，带LLM反馈的语言生成)、DAHLIA (带LLM闭环反馈的代码生成)、以及LYRA的两个变体：LYRA w/ LLM feedback（用LLM替代人类提供反馈）和LYRA w/o memory（无检索模块，随机采样技能/示例填充上下文）。</p>
<p><img src="https://..." alt="技能学习实证分析"></p>
<blockquote>
<p><strong>图3</strong>：(a) 案例研究：在Ravens长时间任务上的基线对比，报告了20次尝试的平均成功率（SR）。(b)-(m) 快照对比：展示了为何人机协作对技能学习至关重要及其如何提升性能。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>在Ravens的六个长时间任务案例研究中（图3a），LYRA取得了0.93的平均成功率（SR）。相比之下，CaP为0.45，LoHoRavens和DAHLIA有所提升，LYRA w/ LLM feedback为0.77，LYRA w/o memory为0.66。LYRA相比变体模型最高有27%的SR提升。图3(b)-(m)的快照对比进一步说明，人类反馈能纠正LLM可能判为成功但未对齐用户偏好的结果（如积木堆叠的精确位姿和间距），实现了真正的偏好对齐。</p>
<p><img src="https://..." alt="可扩展性与反馈效率验证"></p>
<blockquote>
<p><strong>图4</strong>：(a) 基于LLM的扁平代码生成与本文人机协作技能代码生成的平均修正次数（NoC）对比。(b)-(i) Franka Kitchen和MetaWorld任务快照。</p>
</blockquote>
<p>在Franka Kitchen（4子任务）和MetaWorld（20任务）上，通过<strong>平均修正次数（NoC）</strong> 衡量反馈效率（图4a）。LYRA所需修正次数最少（Franka Kitchen: 2.75, MetaWorld: 2.55），相比基线DAHLIA（Franka Kitchen: 4.75, MetaWorld: &gt;10）和LYRA w/o memory（Franka Kitchen: 5.00, MetaWorld: 4.15），平均有42%的效率提升。这得益于技能学习和提示机制。</p>
<p><img src="https://..." alt="建造房屋案例研究"></p>
<blockquote>
<p><strong>图5</strong>：“建造房屋”案例研究。(a) 技能依赖图，展示了从核心基元（橙色）到底层技能（浅绿）和继承技能（黄色）的自底向上构建过程。(b)-(e) 任务执行过程：从混乱初始场景(b)，到组织场景(c)，建造地基(d)，最终完成屋顶(e)。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>通过LYRA w/o memory和LYRA w/ LLM feedback两个变体的实验，验证了各核心组件的贡献：1）<strong>外部记忆与RAG检索</strong>：LYRA w/o memory性能显著下降，证明了动态检索相关技能/示例的重要性，避免了无关信息干扰和上下文窗口限制。2）<strong>人类反馈</strong>：LYRA w/ LLM feedback虽然拥有完整记忆库，但SR仍低于完整版LYRA，且在某些任务上会陷入困境，凸显了人类在提供可靠评估、精确修正和复杂规划指导方面的不可替代性。3）<strong>技能结构化存储与课程式扩展</strong>：通过“建造房屋”案例（图5）证明，该机制使智能体能通过复用和扩展已学技能，解决需要超过20步规划的极端复杂任务。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了一个<strong>人机协作的终身技能学习框架（LYRA）</strong>，通过将人类修正和用户设计的课程编码为可复用、可扩展的技能函数，实现了偏好对齐的终身学习。2）设计了<strong>记忆增强的技能检索机制</strong>，结合RAG和用户提示，实现了对已学知识的动态、高效复用，支撑了长时间任务规划。3）<strong>首次成功演示了解决“建造房屋”等极端长时间任务</strong>（需超过20个基元规划），并在模拟和真实世界中进行了验证。</p>
<p>论文自身提到的局限性包括：1）框架依赖于人类的参与来提供反馈和设计课程，这可能带来负担，并且人类判断可能存在不一致性。2）学习的技能在分布外场景或全新领域的泛化能力仍有待探索。</p>
<p>本文对后续研究的启示在于：1）验证了将人类专业知识以结构化、可组合的形式融入AI智能体学习循环的有效路径。2）为突破当前LLM在机器人领域长时规划与精确执行方面的瓶颈提供了一种混合智能的解决方案。3）其技能封装、记忆检索和课程式扩展的思想，可启发更通用的终身学习与知识积累系统的设计。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对基于大语言模型（LLM）的机器人代码生成在长视野操作任务中存在的语言歧义、输出噪声、上下文限制及知识难以复用等问题，提出了一种人类在环的终身技能学习与代码生成框架。该框架通过外部记忆库、检索增强生成（RAG）和提示机制，将人类反馈编码为可重用、可扩展的技能，并支持动态检索与组合。实验表明，该框架在多个仿真和真实环境中的任务成功率高达0.93（较基线提升最高27%），反馈轮次效率提升42%，并能稳健解决需规划超20个基础动作的极端长视野任务（如“建造房屋”）。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.18597" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>