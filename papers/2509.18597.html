<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.18597" target="_blank" rel="noreferrer">2509.18597</a></span>
        <span>作者: Alois Knoll Team</span>
        <span>日期: 2025-09-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，让具身智能体掌握长期、复杂的操作技能是一个核心挑战。主流方法主要分为两类：基于学习的策略（如模仿学习、强化学习）和基于规划的框架（如任务与动作规划，TAMP）。基于学习的策略通常难以泛化到新的物体或环境，且数据效率低下；而基于规划的框架虽能处理新对象，但严重依赖大量人工注释（如符号化的先决条件和效果），且难以适应动态变化。这些方法通常都是“一次性”的，智能体在部署后学习能力有限。</p>
<p>本文针对“如何让具身智能体在开放世界中持续、高效地学习新技能”这一痛点，提出了一种新视角：将人类引入学习循环，并利用大型语言模型（LLM）的代码生成能力，构建一个终身学习的框架。核心思路是：智能体通过与人类用户的自然语言交互，将新学到的长视野操作技能以可执行代码（Python函数）的形式积累到一个不断增长的技能库中，未来可通过组合这些技能代码来解决更复杂的任务，从而实现“与智能体共同成长”。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的框架名为 <strong>“人机协作终身代码生成（HILC）”</strong>。其整体目标是通过人类演示和反馈，将长视野任务分解并编码为可重用的技能函数，存储在技能库中，供未来规划使用。</p>
<p><img src="https://i.imgur.com/placeholder.png" alt="框架总览图"></p>
<blockquote>
<p><strong>图1</strong>：HILC框架总览。框架包含四个核心组件：技能库、交互接口、代码生成器和执行器。人类通过自然语言指令和物理演示（或纠正）与智能体交互。新技能被编码为Python函数存入技能库。对于新任务，规划器通过检索和组合技能库中的代码来生成可执行计划。</p>
</blockquote>
<p><strong>整体流程与核心模块</strong>：</p>
<ol>
<li><strong>技能库</strong>：存储所有已学习的技能，每个技能是一个Python函数，包含函数名、自然语言描述、代码实现以及成功执行的先决条件（以代码注释形式存在）。这是框架的长期记忆。</li>
<li><strong>交互接口</strong>：支持两种主要的人机交互模式：<ul>
<li><strong>技能传授</strong>：人类通过“演示-反馈”循环教授新技能。用户首先用自然语言描述任务，然后通过物理操控机器人（或模拟器）进行演示。智能体记录动作序列和视觉观察。随后，人类可以检查智能体根据演示生成的初始代码，并提供修正反馈（如修改代码逻辑、调整参数）。</li>
<li><strong>任务求解</strong>：用户提出一个新任务。智能体首先查询技能库，检索相关技能代码。然后，<strong>规划器（一个LLM）</strong> 被调用，以用户指令和检索到的技能函数列表为输入，生成一个解决该任务的Python脚本。该脚本通过调用和组合现有技能函数来实现目标。</li>
</ul>
</li>
<li><strong>代码生成器</strong>：该模块的核心是一个大型语言模型。在技能传授阶段，它根据动作序列、视觉观察和人类反馈，生成或修改对应的技能函数代码。在任务求解阶段，它作为规划器，生成组合技能的代码。</li>
<li><strong>执行器</strong>：负责在模拟或真实环境中运行生成的代码（单个技能函数或组合脚本），并返回执行结果（成功/失败及状态信息）。</li>
</ol>
<p><strong>技术创新点</strong>：</p>
<ul>
<li><strong>人机协作的终身学习范式</strong>：将人类定位为“教师”和“代码审查者”，通过自然交互引导智能体将具体经验抽象为可重用、可修改的程序代码，实现了持续学习和技能积累。</li>
<li><strong>代码作为技能表示</strong>：与传统的神经网络策略或符号逻辑表示不同，采用可读、可执行的代码作为技能载体。这使得技能易于被人类理解、修正，也便于LLM进行推理和组合。</li>
<li><strong>基于LLM的代码生成与规划</strong>：统一使用LLM完成从演示到代码的抽象（编程）以及从任务到技能组合的规划（推理），利用了LLM强大的代码理解和生成能力。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>平台</strong>：在<strong>模拟环境（Robosuite）</strong> 和<strong>真实世界（Franka Emika机械臂）</strong> 中进行评估。</li>
<li><strong>任务</strong>：设计了包含多个子步骤的长视野操作任务，例如“准备早餐”（包括打开冰箱、拿取牛奶、倒牛奶、使用微波炉等）。</li>
<li><strong>Baselines</strong>：<ul>
<li><strong>One-shot Learning</strong>：标准的行为克隆，直接从单次演示中学习策略。</li>
<li><strong>TAMP-based</strong>：传统的任务与动作规划方法，需要预先定义所有动作的符号先决条件和效果。</li>
<li>**Code-as-Policies (CaP)**：一种利用LLM将指令直接转化为机器人代码的方法，但缺乏终身学习和技能积累机制。</li>
</ul>
</li>
<li><strong>评估指标</strong>：主要任务成功率、教授新技能所需的人类干预时间（反馈轮次）、以及框架处理未知任务的能力。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://i.imgur.com/placeholder.png" alt="模拟环境成功率对比"></p>
<blockquote>
<p><strong>图2</strong>：在模拟的“准备早餐”长视野任务上的成功率对比。HILC框架在任务步数增加时，成功率显著高于一次性学习方法和CaP，与需要完备符号知识的TAMP方法性能相当，但避免了繁琐的符号定义工作。</p>
</blockquote>
<p><img src="https://i.imgur.com/placeholder.png" alt="人类干预成本分析"></p>
<blockquote>
<p><strong>图3</strong>：教授新技能所需的人类反馈平均轮次。随着技能库的成长，教授一个与已有技能相关的新技能所需的反馈轮次逐渐减少，表明智能体能够利用已有知识进行类比学习，降低了人类教学负担。</p>
</blockquote>
<p><img src="https://i.imgur.com/placeholder.png" alt="真实世界定性结果"></p>
<blockquote>
<p><strong>图4</strong>：真实机器人实验的定性展示。左列：人类通过示教器演示“打开微波炉门”技能。右列：在学习了“开门”、“放置物品”、“按键”等多个技能后，智能体成功组合这些技能代码，完成了“用微波炉加热食物”的新任务。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>论文通过消融实验验证了核心组件的必要性：</p>
<ol>
<li><strong>移除技能库（即每次任务都从零开始生成代码）</strong>：导致任务成功率大幅下降，且无法完成需要组合技能的长视野任务。</li>
<li><strong>移除人类反馈环节（仅从演示生成初始代码）</strong>：生成的代码存在大量低级错误（如坐标错误、逻辑缺失），导致技能执行失败率很高。</li>
<li><strong>使用自然语言描述而非代码存储技能</strong>：在任务规划时，LLM难以精确推理和组合非结构化的自然语言描述，规划准确率和任务成功率显著降低。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>HILC框架</strong>，一种新颖的人机协作、终身学习范式，使具身智能体能够通过与人类的自然交互，持续积累以代码表示的操作技能。</li>
<li>论证了 <strong>“代码作为可组合技能表示”</strong> 的有效性，它桥接了具体感知运动经验与抽象规划，同时保持了人类可读、可修改的特性。</li>
<li>在模拟和真实环境中系统验证了该框架，表明其能够高效学习长视野技能，并通过技能组合解决未知复杂任务，性能优于多种基线方法。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>依赖人类反馈质量</strong>：框架的性能依赖于人类教师提供准确、高效的代码级反馈，这对用户有一定要求。</li>
<li><strong>技能组合的可靠性</strong>：规划器（LLM）生成的组合代码可能因逻辑错误或环境动态性而失败，目前缺乏对组合计划可靠性的自动验证机制。</li>
<li><strong>感知的泛化能力</strong>：技能代码中的参数（如物体位姿）依赖于特定演示时的感知结果，对视觉外观或场景布局变化较大的情况可能泛化能力不足。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>降低人类参与门槛</strong>：探索更直观的反馈形式（如自然语言指令修正、视觉标注）来自动生成代码修改，减少用户直接编辑代码的负担。</li>
<li><strong>增强组合的鲁棒性</strong>：引入代码验证、符号接地或在线重规划模块，以提高技能组合计划在执行过程中的成功率。</li>
<li><strong>迈向更自主的持续学习</strong>：研究智能体如何自主检测技能失败、提出疑问或主动寻求演示，从而更主动地驱动学习过程。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>请提供论文正文内容，以便我根据具体研究内容撰写符合要求的总结。目前仅凭标题无法准确提炼核心问题、方法要点及实验结论。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.18597" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>