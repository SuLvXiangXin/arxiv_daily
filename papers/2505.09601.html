<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.09601" target="_blank" rel="noreferrer">2505.09601</a></span>
        <span>作者: Yu, Justin, Fu, Letian, Huang, Huang, El-Refai, Karim, Ambrus, Rares Andrei, Cheng, Richard, Irshad, Muhammad Zubair, Goldberg, Ken</span>
        <span>日期: 2025/05/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，扩展机器人学习主要依赖于两种数据收集范式：工业机器人日志和人工遥操作。前者规模虽大但任务和本体特定，后者提供了更好的视觉和任务多样性，但其规模受限于人力投入和实时采集，成本高昂。与此同时，端到端的通用机器人策略（如视觉-语言-动作模型）的兴起，对大规模、多样化、高质量训练数据的需求急剧增加。当前最大的机器人数据集规模仍比前沿大语言模型和视觉语言模型的数据集小五个数量级。</p>
<p>为应对数据稀缺挑战，先前工作转向基于物理仿真的数据生成。然而，现代仿真器存在根本性局限：许多常用仿真器不满足基本拉格朗日力学原理；准确建模复杂物体交互通常需要大量参数调整和接触属性的手工设计；生成用于碰撞建模的高质量、合规且无交错的资产仍然劳动密集。另一种思路是“真实到仿真再真实”（Real2Sim2Real）范式，利用真实扫描构建数字孪生，但仍常依赖遥操作、手工奖励工程或精确物理模型，限制了可扩展性。</p>
<p>本文针对上述痛点，提出了一种新视角：完全避开动力学仿真和机器人硬件，仅利用智能手机捕获的物体扫描和单人演示视频，大规模生成合成机器人训练数据。核心思路是，通过三维重建获取高保真物体几何与外观，从视频中提取物体6自由度运动轨迹，并利用可扩展渲染引擎在随机化的环境背景下，合成大量视觉逼真且轨迹多样的机器人演示数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>Real2Render2Real (R2R2R) 是一个从单个人类演示视频和多视角物体扫描生成大规模合成机器人演示数据（RGB-动作对）的流水线。它包含三个主要阶段：(1) 真实到仿真的资产与轨迹提取；(2) 增强；(3) 并行渲染。</p>
<p><img src="https://arxiv.org/html/2505.09601v1/extracted/6437703/figures/assets/KindaBadR2R2RTeaser8.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：Real2Render2Real 生成机器人训练数据的流程。输入为多视角物体扫描和单目人类演示视频。随后，R2R2R 通过并行渲染合成多样化、经过域随机化的机器人执行轨迹，并输出用于策略训练的配对图像-动作数据。该流水线无需遥操作或物体动力学仿真，即可实现跨任务和跨本体的可扩展学习。</p>
</blockquote>
<p><strong>1. 真实到仿真资产提取</strong>：使用两阶段流程从智能手机扫描中提取3D物体资产。首先，利用3D高斯泼溅（3DGS）重建物体的几何和外观。然后，应用GARField将场景分割成语义上有意义的部分（通过将2D掩码提升至3D），实现物体级和部件级分解，包括铰接部件。为支持基于网格的渲染，将得到的高斯组通过扩展版本的算法转换为带纹理的三角形网格。</p>
<p><img src="https://arxiv.org/html/2505.09601v1/extracted/6437703/figures/assets/gsr2r2r_groupings.png" alt="3D重建与分割"></p>
<blockquote>
<p><strong>图3</strong>：基于特征分组进行部件级分割的3D高斯泼溅物体重建。物体被重建并利用GARField分割为刚性或铰接部件。</p>
</blockquote>
<p><strong>2. 真实到仿真轨迹提取</strong>：给定人类操作已扫描物体的智能手机视频，R2R2R 使用4D可微分部件建模（4D-DPM）提取物体及其部件的6自由度部件运动。每个3DGS物体部件都嵌入了预训练的DINO特征，通过可微分渲染实现部件姿态优化。该方法被扩展以支持从演示视频中跟踪单个或多个刚性物体以及铰接物体。</p>
<p><strong>轨迹插值以实现多样性</strong>：R2R2R 的一个关键贡献是能够从单个人类演示中合成多个有效的6自由度物体轨迹。对于多个相互作用的刚性物体，原始演示仅对特定的初始物体配置有效。为此，本文引入了一套轨迹插值和重采样技术，使原始轨迹适应新的起点和终点姿态，同时保留其语义意图。具体包括空间归一化和球面线性插值（Slerp），并应用采样启发式方法，使初始放置的分布偏向远离目标姿态。</p>
<p><img src="https://arxiv.org/html/2505.09601v1/extracted/6437703/figures/assets/traj_interp.png" alt="轨迹插值"></p>
<blockquote>
<p><strong>图4</strong>：轨迹插值——R2R2R通过空间归一化和Slerp使物体运动适应不同的起点/终点配置。</p>
</blockquote>
<p><strong>抓取姿态采样</strong>：使用手部关键点检测估计演示视频中的3D手部关键点，通过计算关键点与所有分割物体部件质心之间的欧氏距离来确定物体-手部交互。被抓取的部件被确定为在整个轨迹中与手部持续最接近的物体。为了生成物理上合理的抓取，对3DGS均值进行采样以构建更粗糙的三角形网格，应用表面平滑和简化以获得一致的法线，并使用解析的对握抓取采样器来确定候选抓取轴。对于双手任务，此过程独立应用于每只手。</p>
<p><strong>微分逆运动学</strong>：对于每个抓取和物体轨迹对，使用PyRoki求解微分逆运动学问题。求解器计算平滑的关节空间轨迹，在预抓取、抓取和后抓取阶段诱导出期望的物体运动。<strong>关键的是，该方法不需要建模物体动力学或模拟物理交互</strong>。它假设物体在接触期间刚性地跟随轨迹，避免了接触建模、柔顺性或摩擦估计等挑战。</p>
<p><strong>3. 并行渲染</strong>：使用IsaacLab包作为纯照片级、并行化的渲染引擎（将所有物体设置为运动学而非动态体），渲染多样化的环境上下文。应用广泛的域随机化，包括随机化的光照条件、相机外参（在2厘米平移和5度旋转内均匀采样）和物体初始姿态。在单个NVIDIA RTX 4090上，R2R2R使用IsaacLab框架以平均每分钟51条演示的速度渲染完整的机器人演示，而人工遥操作的速度为每分钟1.7条演示，速度提升超过27倍。此吞吐量随渲染GPU数量线性扩展。</p>
<p><strong>创新点</strong>：与现有方法相比，R2R2R的核心创新在于完全摒弃了动力学仿真，专注于从真实交互中提取物体运动轨迹，并通过可扩展渲染生成高视觉保真度的观察数据。它避免了物理参数调优、碰撞建模和奖励工程的复杂性，同时支持从单次演示生成多样化的轨迹。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在ABB YuMi IRB14000双臂机器人（一个在π₀-FAST预训练中未见过的机器人本体）上进行了1,050次物理机器人评估，涵盖五个操作任务。策略在人工遥操作数据或R2R2R生成的合成演示上训练。为评估策略性能随训练数据的扩展情况，使用每个任务50、100、150和1,000条渲染轨迹以及最多150条遥操作轨迹训练模型。五个任务包括：单物体抓取（“捡起玩具老虎”）、多物体交互（“把杯子放在咖啡机上”）、铰接物体操作（“关掉水龙头”和“打开抽屉”）以及双手协调（“用双手拿起包裹”）。</p>
<p><strong>对比方法</strong>：主要对比基于人工遥操作数据训练的模型。</p>
<p><img src="https://arxiv.org/html/2505.09601v1/extracted/6437703/figures/assets/KindaBadR2R2RTeaser8.jpg" alt="数据生成效率与平均策略性能"></p>
<blockquote>
<p><strong>图2</strong>：（左）性能可视化，显示了在真实数据（1个人类遥操作员）与合成数据（1个人类演示，1个GPU）上训练的策略的任务特定结果（浅色背景线）和跨任务平均值（带误差阴影的粗线）。按演示数量（50-1000）标记的点突出了性能的扩展以及R2R2R的显著吞吐量优势。（右）对数刻度比较，显示了R2R2R（1-100个GPU）与人工遥操作（1-100个操作员）在12小时内的数据生成吞吐量。R2R2R需要10分钟的前期时间用于人类扫描物体、演示任务、重建物体和跟踪其轨迹，随后无需人类参与。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>性能扩展与对比</strong>：R2R2R生成的数据性能随数据集规模可预测地扩展。在“把杯子放在咖啡机上”任务中，使用R2R2R数据训练的Diffusion Policy性能从150条演示时的33.3%提升到1000条演示时的53.3%，而π₀-FAST则从33.3%跃升至80.0%。在低数据区域，真实数据每演示效率更高（例如，π₀-FAST在150条真实演示时达到73.3%，而在150条R2R2R演示时为33.3%），但当规模增加到1000条演示时，R2R2R在多个任务上达到或超过了遥操作的性能。</li>
<li><strong>数据效率</strong>：基于单个人类演示生成的R2R2R数据（通过单个GPU渲染扩展）训练的策略，其性能与基于150个人类遥操作演示训练的策略相当。</li>
<li><strong>统计显著性</strong>：显著性检验和等效性检验（TOST）表明，在评估的任务上，基于R2R2R与基于人工遥操作数据训练的策略之间没有统计学上的显著差异，观察到的差异在±5%的范围内。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.09601v1/extracted/6437703/figures/assets/KindaBadR2R2RTeaser8.jpg" alt="物理实验对比"></p>
<blockquote>
<p><strong>图5</strong>：比较Real2Render2Real与人工遥操作数据效率的物理实验。任务成功率相对于数据生成时间（小时）绘制。实线代表π₀-FAST和Diffusion Policy的平均性能。Real2Render2R线（蓝色方块）包含对应由单个Nvidia RTX 4090生成的50、100、150和1000条轨迹的点。人工遥操作线（金色方块）包含对应50、100和150条轨迹的点。R2R2R数据生成时间包含10分钟的设置成本，而人工遥操作时间基于收集150条演示的真实轨迹时间。</p>
</blockquote>
<p><strong>消融实验总结</strong>：附录中的消融实验表明，轨迹插值对于从单次演示生成多样化、有效的轨迹至关重要；增加背景随机化有助于提高策略的鲁棒性；将R2R2R数据与少量真实数据共同训练可以进一步提升性能。</p>
<p><strong>定性结果</strong>：论文提供了大量任务示例的渲染图像与真实图像对比（图12至图86），展示了R2R2R生成数据的高视觉保真度，以及在不同机器人本体（如Franka Emika）上的泛化能力（图87至图98）。</p>
<p><img src="https://arxiv.org/html/2505.09601v1/extracted/6437703/figures/task/r2r2r/coffeemaker1_0.jpeg" alt="咖啡机任务示例"></p>
<blockquote>
<p><strong>图15</strong>：R2R2R为“把杯子放在咖啡机上”任务生成的合成渲染图像示例（视角0），展示了其视觉逼真度。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了Real2Render2Real (R2R2R)框架，仅使用智能手机捕获的视频（多视角物体扫描和人类演示视频），无需动力学仿真或机器人硬件，即可合成多样化、物理基础扎实的观察-动作对数据。</li>
<li>证明了该数据与现代视觉-语言-动作（VLA）和模仿学习策略（包括基于Transformer和扩散模型的架构）兼容，这些策略从RGB和本体感知输入运行。</li>
<li>通过物理实验表明，基于单个人类演示生成的R2R2R数据训练的策略，其性能可匹配基于150个人类遥操作演示训练的策略，同时数据生成所需时间显著减少。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>重建与仿真保真度</strong>：依赖的视觉重建方法（3DGS和网格转换）能产生高保真外观，但几何通常不水密或物理上不合理，难以模拟真实的物理交互（尤其是接触丰富的场景）。因此，R2R2R完全放弃了物理仿真，无法建模摩擦、柔顺性或力反馈等动力学。</li>
<li><strong>假设依赖</strong>：假设物体为刚性或铰接，在准静态条件下操作；物体表面具有低镜面反射以支持稳健重建；演示中物体不会完全相互遮挡。这些假设可能限制其在更复杂场景中的应用。</li>
</ol>
<p><strong>对后续研究的启示</strong>：<br>R2R2R展示了一条不依赖昂贵硬件或复杂物理仿真即可大规模扩展机器人数据的可行路径。其“免仿真、免硬件”的范式为收集多样化机器人数据提供了高度可访问和可扩展的方案。未来的工作可以探索将成熟的真实到仿真流程与物理仿真层相结合，以支持需要接触动力学的任务，同时继续利用R2R2R在视觉保真度和轨迹多样性方面的优势。此外，如何将这种方法推广到非准静态、动态交互或更复杂的光照和材质场景，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Real2Render2Real（R2R2R）方法，旨在解决机器人学习所需大规模数据获取成本高、依赖人工遥操作和物理硬件的问题。该方法仅需物体扫描和单段人类演示视频，利用3D高斯抛雪球（3DGS）技术重建物体几何外观并跟踪6自由度运动，进而渲染生成海量高保真、机器人无关的演示数据。实验表明，基于单一人类演示生成的R2R2R数据训练模型，其性能可媲美使用150次人工遥操作数据训练的模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.09601" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>