<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.07375" target="_blank" rel="noreferrer">2504.07375</a></span>
        <span>作者: Ma, Junyi, Bao, Wentao, Xu, Jingyi, Sun, Guanzhong, Chen, Xieyuanli, Wang, Hesheng</span>
        <span>日期: 2025/04/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于自我中心视觉的手部轨迹预测方法主要依赖于2D视频输入进行未来手部路径点的预测。这些方法存在两个关键局限性：首先，它们缺乏对3D环境结构的感知，仅使用2D输入会导致预测轨迹与真实3D手部运动之间存在模态差异；其次，现有模型要么孤立地预测手部轨迹，要么仅从过去帧中编码头部相机运动，忽略了手部运动与头部相机自我运动在未来时间段内的协同关系。这种协同关系体现在：头部运动为手部轨迹规划提供目标观察，手部运动有时快于头部运动，以及人总是试图将手保持在视野内以确保与目标物体的准确接触。</p>
<p>本文针对上述痛点，提出了一个融合多模态信息并显式解耦预测手部与相机运动的新视角。其核心思路是：设计一个名为MMTwin的双扩散模型框架，吸收包括2D RGB图像、3D点云、过去手部路径点和文本提示在内的多模态输入，通过并行的自我运动扩散和手部轨迹预测扩散模型，在预测未来手部3D轨迹的同时预测相机自我运动，并利用一个新颖的混合Mamba-Transformer模块来更好地融合多模态特征。</p>
<h2 id="方法详解">方法详解</h2>
<p>MMTwin的整体推理流程如下：模型接收多模态输入，包括过去Np帧的自我中心RGB图像序列ℐ、3D点云序列𝒟、过去3D手部路径点序列ℋp以及文本提示𝒪，目标是预测未来Nf帧的3D手部轨迹ℋf。系统首先从RGB图像计算序列单应性矩阵ℳ作为过去的相机自我运动，并通过视觉里程计将点云统一到全局坐标系（即第一帧的相机坐标系）。接着，模型通过多个编码器提取特征：使用GLIP作为视觉编码器，结合文本提示“hand”提取视觉语义特征；通过一个融合模块将过去手部路径点特征与视觉语义特征融合，生成手部轨迹预测潜变量；通过一个基于3D卷积的体素编码器将聚合、体素化后的点云转换为稀疏的3D体素块特征；使用一个MLP将单应性矩阵编码为自我运动潜变量。</p>
<p><img src="https://arxiv.org/html/2504.07375v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图II</strong>：MMTwin整体框架。(a) 从多模态数据中提取特征，(b) 通过新颖的双扩散模型解耦预测未来相机自我运动特征和3D手部轨迹。自我运动扩散使用普通Mamba进行去噪，手部轨迹预测扩散则使用(c)混合Mamba-Transformer模块作为去噪模型，该模块包含(d)自我运动感知的Mamba块和结构感知的Transformer。</p>
</blockquote>
<p>核心创新在于双扩散模型。<strong>自我运动扩散模型</strong>以过去的自我运动潜变量Fp_ego为条件，去噪采样的未来噪声Fnoise_ego，预测未来的自我运动特征F̂f_ego。该模型使用普通Mamba作为去噪模型，其多模态影响通过梯度更新间接实现。</p>
<p><strong>手部轨迹预测扩散模型</strong>是重点，它以过去的手部轨迹预测潜变量Fp_htp为条件，去噪采样的未来噪声Fnoise_htp，预测未来的手部轨迹特征F̂f_htp。此过程显式地以过去自我运动特征Fp_ego和由自我运动扩散预测的未来自我运动特征F̂f_ego作为条件。该扩散的去噪模型是一个新颖的<strong>混合Mamba-Transformer模块</strong>。</p>
<p>该模块的具体工作流程如下：首先，将过去与未来的自我运动特征拼接为F̂pf_ego，将过去手部轨迹潜变量与噪声拼接为Fpf_htp。然后，Fpf_htp经过两个<strong>自我运动感知Mamba块</strong>。EAM块采用了运动驱动选择性扫描机制，将时序的自我运动特征F̂pf_ego无缝集成到Mamba的状态转移过程中，实现对未来手部轨迹状态的合理演进。接着，处理后的特征送入<strong>结构感知Transformer</strong>。SAT首先对加入位置/时序编码的手部轨迹潜变量进行多头自注意力计算，然后利用多头交叉注意力机制，将3D体素块特征Xvox作为全局环境上下文进行融合。这种设计结合了Mamba擅长时序建模和Transformer擅长捕获全局上下文的优势。最终，去噪后的未来手部轨迹特征F̂f_htp通过一个MLP解码器生成最终的未来3D手部路径点。</p>
<p><img src="https://arxiv.org/html/2504.07375v2/x3.png" alt="手部移除"></p>
<blockquote>
<p><strong>图III</strong>：基于投影的手部点云移除。使用MobileSAM为每张输入图像生成手部掩码，并通过相机内参过滤掉投影到手部区域的3D点。</p>
</blockquote>
<p>此外，在点云处理中，论文采用了<strong>投影式手部移除</strong>策略（图III），使用MobileSAM识别图像中的手部区域，并移除投影到该区域的3D点，以避免移动手臂带来的点云干扰，提升全局3D信息表示的精度。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了三个公开数据集：EgoPAT3D-DT、H2O-PT、HOT3D-Clips，以及作者使用RealSense D435i头戴设备自记录的包含三个日常任务（放杯子、放苹果、放盒子）的数据集。对比的基线方法包括CVH、OCT、USST、S-Mamba、Diff-IP3D和MADiff3D。评估指标为3D空间（单位：米）和2D图像平面上的平均位移误差和最终位移误差。</p>
<p><img src="https://arxiv.org/html/2504.07375v2/x1.png" alt="定量结果"></p>
<blockquote>
<p><strong>表I</strong>：在EgoPAT3D-DT和H2O-PT数据集上3D/2D手部轨迹预测性能对比。最佳和次佳结果分别用黑色加粗和蓝色标示。</p>
</blockquote>
<p>关键定量结果总结如表I所示。在EgoPAT3D-DT数据集上，无论是已见场景还是未见场景，MMTwin在绝大多数指标（3D ADE/FDE, 2D ADE/FDE）上均取得了最佳性能。例如，在已见场景的3D ADE上达到0.170米，优于之前最佳的USST（0.183米）和MADiff3D（0.183米）；在2D ADE上达到0.071，也优于其他方法。在H2O-PT数据集上，MMTwin同样表现优异，3D ADE为0.030米，与USST并列最佳。这证明了其卓越的预测准确性和泛化能力。</p>
<p><img src="https://arxiv.org/html/2504.07375v2/x6.png" alt="定性可视化"></p>
<blockquote>
<p><strong>图VI</strong>：3D空间中预测手部轨迹的可视化。展示了包括观测到的过去手部路径点（绿色）、真实未来路径点（蓝色）以及由MMTwin和三个SOTA基线预测的未来路径点（红色）。</p>
</blockquote>
<p>定性结果如图VI所示，MMTwin预测的轨迹在形状上更自然、更准确，而基线方法如USST倾向于预测较短且保守的轨迹，Diff-IP3D的预测则存在较大波动。</p>
<p><img src="https://arxiv.org/html/2504.07375v2/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图VII</strong>：关于HMTM架构和3D感知的消融研究。展示了不同配置在EgoPAT3D-DT测试集上的3D ADE性能。</p>
</blockquote>
<p>消融实验（图VII）验证了各个组件的贡献：1) <strong>双扩散模型结构</strong>：同时预测自我运动（EDM）和使用自我运动作为条件（EC）对性能提升至关重要。2) <strong>混合去噪模块</strong>：结合EAM和SAT的混合设计（HMTM）优于仅使用Transformer或仅使用Mamba的变体。3) <strong>3D感知</strong>：引入3D点云输入并使用SAT融合体素特征，带来了显著的性能增益。4) <strong>手部点云移除</strong>：该策略也能有效提升预测精度。</p>
<p><img src="https://arxiv.org/html/2504.07375v2/x5.png" alt="自采数据结果"></p>
<blockquote>
<p><strong>图V</strong>：(a)用于自采数据的头戴RGBD相机，(b)自采数据示例，(c)MMTwin预测结果（红点）与真实未来手部路径点（绿点）在图像平面上的投影对比。</p>
</blockquote>
<p>在自记录数据上的实验（图V）进一步表明，MMTwin能够有效预测真实世界任务中的手部轨迹，展示了其在实际应用中的潜力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了首个用于3D手部轨迹预测的多模态双扩散模型框架MMTwin，显式解耦并协同预测未来相机自我运动与手部运动。2) 设计了新颖的混合Mamba-Transformer模块作为扩散去噪器，有效融合了时序自我运动条件和3D全局环境上下文。3) 通过全面的实验验证，在多个数据集上实现了最先进的性能，并展现了良好的泛化能力。</p>
<p>论文提到的局限性包括：对于高动态物体的交互场景，点云可能无法提供稳定的结构信息；方法对计算资源有一定要求。这些启示后续研究可以探索更高效的3D表示方式，或考虑在更复杂的动态环境中引入目标物体的显式状态估计以进一步提升预测的合理性和准确性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有3D手部轨迹预测方法仅支持2D视频输入、且忽视手部运动与头戴相机自身运动协同的问题，提出新型多模态扩散模型MMTwin。该方法以2D图像、3D点云、历史轨迹及文本提示为输入，通过孪生扩散模型（自身运动扩散与轨迹预测扩散）同步预测相机运动与未来手部轨迹，并采用混合Mamba-Transformer模块进行多模态特征融合与去噪。实验在三个公开数据集及自录数据上验证了模型能预测合理的3D手部轨迹，并具有良好的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.07375" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>