<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.16861" target="_blank" rel="noreferrer">2512.16861</a></span>
        <span>作者: Caelan Garrett Team</span>
        <span>日期: 2025-12-18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>长视野操作任务是机器人领域的长期挑战。目前，从演示中模仿学习是让智能体完成任务的有效方法，但收集演示成本高昂，且训练出的智能体容易偏离演示分布。强化学习利用环境奖励，但长视野加剧了探索困难，尤其是在奖励稀疏时。为缓解演示不足，一类工作通过物体中心的数据生成来扩充数据集，另一类使用任务与运动规划来生成演示，或将任务分层为更易解决的子目标。然而，这些数据生成方法是开环的，仅依赖离线数据，其性能受限于源演示的质量。本文针对现有自动化数据生成方法因缺乏在线探索而性能受限的痛点，提出将在线探索和环境反馈（通过强化学习）整合到分层数据生成流程中的新视角。核心思路是：首先利用少量人类演示通过合成数据生成离线数据集，训练一个混合模仿学习智能体，然后通过在线交互和强化学习对该智能体的各个组件进行微调，从而超越源演示的性能边界。</p>
<h2 id="方法详解">方法详解</h2>
<p>ReinforceGen的整体流程分为三步：1）利用少量人类源演示，通过合成数据生成创建离线数据集；2）使用该数据集训练一个混合模仿学习智能体，该智能体在运动规划（移动到预测路点）和学习到的策略（直接控制机器人）之间交替；3）通过在线环境交互和强化学习对智能体进行微调。</p>
<p><img src="https://arxiv.org/html/2512.16861v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ReinforceGen整体框架。首先通过少量人类源演示进行合成数据生成，创建离线数据集。然后使用该数据集训练一个混合模仿学习智能体。最后，通过在线环境交互和强化学习对智能体进行微调。</p>
</blockquote>
<p>该方法建立在混合技能策略框架上，将任务分解为n个阶段，每个阶段包含一个连接段和一个技能段。每个参数化技能ψ_θ_i由三部分组成：起始姿态预测器ℐ_θ_i、技能策略π_θ_i和终止分类器𝒯_θ_i。</p>
<p><img src="https://arxiv.org/html/2512.16861v1/x2.png" alt="阶段组件"></p>
<blockquote>
<p><strong>图2</strong>：ReinforceGen单个阶段的主要组件。姿态预测器ℐ_θ_i实时预测目标末端执行器姿态并更新运动规划器。到达目标后，技能策略π_θ_i接管控制以完成阶段目标，该目标由终止预测器𝒯_θ_i决定。所有三个组件首先从生成的数据集中模仿学习，然后使用在线数据进行微调。</p>
</blockquote>
<p>核心创新在于对HSP的三个组件分别设计了基于强化学习的微调流程：</p>
<ol>
<li><strong>起始姿态预测器（ℐ_θ_i）微调</strong>：该组件为运动规划器提供目标姿态，其预测误差会严重影响后续技能策略性能。<ul>
<li><strong>实时重规划</strong>：在连接段执行运动规划时，每个时间步都运行姿态预测器。当新预测姿态与当前规划目标的差异超过阈值时，使用更新后的姿态重新规划。</li>
<li><strong>从特权教师蒸馏</strong>：训练时，利用可访问的物体状态信息，使用一个准确的特权教师预测器（通过物体中心变换生成目标姿态）来蒸馏学生预测器，以缩小分布差距。</li>
</ul>
</li>
<li><strong>技能策略（π_θ_i）微调</strong>：采用残差强化学习范式。以从生成数据中训练的行为克隆策略作为基础策略π^base，训练一个输出动作残差的残差策略π^res_θ_i。最终技能动作为π_θ_i(o_t) = π^base_i(o_t) + π^res_θ_i(o_t)。训练目标是最大化正则化后的期望回报，正则项惩罚残差动作的L2范数，以约束对基础策略的偏离。</li>
<li><strong>终止分类器（𝒯_θ_i）微调</strong>：重点解决手工设计终止条件可能带来的误报（错误提前终止）问题。提出一种简化方案：训练一个预测器p_i来估计在当前状态下终止本阶段后任务最终成功的概率。然后，将原始终止条件𝒯^r_i与一个阈值ϵ_term结合，生成新的终止条件T_i(s) := 𝒯^r_i(s) · [p_i(s) &gt; ϵ_term]，从而过滤掉低成功概率的终止点。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.16861v1/x3.png" alt="组件微调示意"></p>
<blockquote>
<p><strong>图3</strong>：三个组件的微调方式示意。(A)姿态预测器向特权教师蒸馏；执行时根据新观察更新预测，偏差过大时重新规划。(B)技能策略通过残差强化学习微调。(C)从终止预测器中清除误报预测。</p>
</blockquote>
<p>部署时，ReinforceGen混合策略按顺序执行各个技能：对每个技能，预测起始姿态并规划路径，执行中可重规划；到达后执行微调后的技能策略，直到终止预测器触发。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在基于Robosuite的长视野操作任务上进行基准测试，包括Nut Assembly、Threading、Three Piece、Coffee和Coffee Preparation任务的D2变体（最大初始化范围），最多5个阶段。观测空间包括前视和腕部相机RGB图像以及本体感知。每个任务收集10个人类源演示，并自动生成1000条演示的BC数据集。<br><strong>基线方法</strong>：HSP-Priv（使用特权姿态预测器的混合技能策略）、HSP-Priv + Skill-FT（对其技能策略进行微调作为性能上界）、以及使用HSP-Priv作为教师策略通过在线轨迹蒸馏的HSP。<br><strong>关键结果</strong>：ReinforceGen在所有任务的最高重置范围设置下，使用视觉运动控制达到了80%的总成功率，几乎将先前最先进方法（SkillMimicGen）的成功率翻倍。消融研究表明，微调方法贡献了平均89%的性能提升。</p>
<p><img src="https://arxiv.org/html/2512.16861v1/x4.png" alt="姿态误差影响"></p>
<blockquote>
<p><strong>图4</strong>：在Nut Assembly任务第二阶段，随着添加到姿态目标的噪声水平增加，成功率急剧下降。这说明了起始姿态预测准确性的关键作用。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.16861v1/figures/qual-threading.png" alt="定性结果-Threading"></p>
<blockquote>
<p><strong>图5</strong>：Threading任务的定性结果展示。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.16861v1/figures/qual-threepiece.png" alt="定性结果-ThreePiece"></p>
<blockquote>
<p><strong>图6</strong>：Three Piece任务的定性结果展示。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.16861v1/x5.png" alt="消融实验-微调贡献"></p>
<blockquote>
<p><strong>图12</strong>：消融实验展示各微调组件的贡献。与基线HSP相比，引入姿态预测器蒸馏、技能策略RL微调以及终止条件过滤，均带来了显著且递增的性能提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.16861v1/x6.png" alt="消融实验-学习终止条件"></p>
<blockquote>
<p><strong>图13</strong>：使用学习到的终止条件（Learned Term）与手工设计终止条件（Handcrafted Term）的对比。学习到的终止条件性能接近手工设计条件，证明了其可行性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：图12的消融研究表明，各微调组件对最终性能均有重要贡献。从基线HSP开始，依次添加姿态蒸馏、技能RL微调和终止过滤，性能逐步提升至最高。特别是技能RL微调贡献了最大的单步性能增益。图13表明，学习到的终止分类器可以替代手工设计条件，且性能相当。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ReinforceGen框架，首次将在线强化学习微调与自动化分层数据生成相结合，突破了源演示质量对生成数据性能的上限。</li>
<li>针对混合技能策略的三个核心组件（起始姿态预测、技能执行、终止判断），分别设计了有效的微调方法（实时重规划与蒸馏、残差RL、基于成功概率的终止过滤）。</li>
<li>在多个长视野接触式操作任务上实现了显著性能提升（80%总成功率），并验证了框架能够蒸馏训练出高效的端到端视觉运动策略。</li>
</ol>
<p><strong>局限性</strong>：论文提到，方法假设每个任务的阶段序列及每个技能的参考物体由人工标注。此外，在评估中仍使用了手工设计的阶段终止条件（尽管消融实验证明了学习终止条件的可行性）。</p>
<p><strong>后续启示</strong>：ReinforceGen展示了结合开环数据生成与闭环在线优化的强大潜力。其模块化的微调思路可扩展至其他分层策略框架。如何进一步减少对人工标注（如阶段分解）的依赖，以及将学习到的终止条件完全整合到在线RL循环中，是值得探索的方向。此外，该方法为在更复杂、动态场景中利用少量演示快速提升策略鲁棒性提供了可行路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文解决长时程机器人操作任务中演示数据收集昂贵、模仿学习易偏离，以及强化学习探索困难的核心挑战。提出ReinforceGen框架，通过任务分解将任务分割为多个局部技能，并利用自动化数据生成与模仿学习构建初始策略，再结合在线适应和强化学习进行微调改进。在Robosuite数据集上的实验表明，该系统在视觉运动控制下达到80%的成功率，且消融研究证实微调方法带来了89%的平均性能提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.16861" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>