<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.17889" target="_blank" rel="noreferrer">2511.17889</a></span>
        <span>作者: Huang, Ting, Li, Dongjian, Yang, Rui, Zhang, Zeyu, Yang, Zida, Tang, Hao</span>
        <span>日期: 2025/11/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将自然语言指令转化为机器人连续控制（Vision-Language-Action， VLA）是智能机器人系统的核心挑战。尽管多模态基础模型在感知和语言理解方面取得进展，但现有方法仍存在两大局限：一是难以弥合高层语义推理与低层电机控制之间的鸿沟，直接的语言到动作映射通常导致可解释性差和接地不稳定；二是缺乏透明的推理结构，依赖隐式中间嵌入的方法虽然可能稳定，但模糊了语义逻辑，限制了组合推理和错误可追溯性。</p>
<p>本文针对上述“语义-控制”鸿沟这一具体痛点，提出了一种新的视角：通过显式的、结构化的推理过程来连接语言指令与连续控制。其核心思路是：提出一个名为MobileVLA-R1的分层VLA框架，首先生成基于多模态观察的条件化结构化思维链（Chain-of-Thought， CoT）动作计划，然后通过动作解码器将其转化为连续控制命令，并通过结合监督CoT对齐与强化学习的两阶段训练范式，来增强推理一致性、控制稳定性和长时程执行能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>MobileVLA-R1遵循一个分层的“推理-执行”范式，包含两个顺序的训练阶段。其整体目标是根据多模态感知和自然语言指令，为闭环的四足机器人控制生成连续的动作序列。在每个时间步t，智能体接收观察<code>s_t = {x_t^rgb, x_t^depth, x_t^point}</code>和指令<code>i</code>，策略<code>π_θ</code>预测动作<code>a_t = [V_x, V_y, ω_yaw, α]</code>，其中<code>V_x, V_y</code>为平移速度，<code>ω_yaw</code>为偏航角速度，<code>α</code>为从预定义动作集中采样的离散高层动作。</p>
<p><img src="https://arxiv.org/html/2511.17889v1/images/mobilevlar1_logo.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：MobileVLA-R1架构。它是一个端到端框架，集成自然语言指令与多模态感知（RGB、深度、点云），生成连续的运动动作，使移动机器人能够遵循复杂指令并实时适应多样环境。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>多模态感知前端与模型初始化</strong>：架构基于LLaVA设计，采用统一的多模态感知前端，集成RGB、深度和点云输入。模型从NaVILA初始化，并引入了额外的编码器：使用DepthAnything V2作为深度编码器，使用Point Transformer v3进行3D几何表征。多模态特征通过轻量级投影模块融合，并与LLaMA3-8B语言主干对齐。为高效调优，在主干网络的投影层和注意力层中插入了LoRA模块（r=16, α=32），视觉编码器保持冻结。</li>
<li><strong>两阶段训练范式</strong>：<ul>
<li><strong>冷启动阶段（监督微调，SFT）</strong>：此阶段旨在RL前对齐输出格式并初始化推理能力。首先在MobileVLA-CoT-Episode和MobileVLA-CoT-Nav数据集上进行微调，学习生成<code>&lt;think&gt;...&lt;/think&gt;&lt;answer&gt;...&lt;/answer&gt;</code>格式的结构化推理。随后在MobileVLA-CoT-Step的一个子集上进一步训练，其中答案包含可执行的连续速度和动作命令，这些命令被确定性地解析为控制信号，实现从语言推理到物理驱动的平滑过渡。</li>
<li><strong>强化学习阶段（GRPO）</strong>：为进一步增强推理驱动的动作生成，采用分组相对策略优化（GRPO）框架。给定输入对<code>(x, i)</code>，策略<code>π_θ</code>为每个样本生成N个响应<code>{o_1, o_2, ..., o_N}</code>，每个响应通过一组奖励函数进行评估：<ul>
<li><strong>移动奖励（R_m）</strong>：评估预测与真实运动方向的一致性，计算为两个速度向量<code>(V_x, V_y, V_yaw)</code>的余弦相似度，以促进方向对齐和动态一致的控制。</li>
<li><strong>动作奖励（R_action）</strong>：监督离散控制行为，当预测动作<code>a</code>与真实标签<code>a*</code>匹配时奖励为1，否则为0。</li>
<li><strong>格式奖励（R_format）</strong>：确保模型输出严格遵循<code>&lt;think&gt;...&lt;/think&gt;&lt;answer&gt;...&lt;/answer&gt;</code>模板，保证机器可解析的输出和一致的推理到控制映射。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17889v1/x2.png" alt="RL策略流程"></p>
<blockquote>
<p><strong>图3</strong>：RL策略流程。模型从给定输入生成N个响应，计算每个响应的奖励，经过归一化和裁剪后，与防止模型过度更新的KL散度项结合，用于更新策略。</p>
</blockquote>
<pre><code>策略更新时，计算每个响应的优势`A_i`并归一化，然后使用裁剪的GRPO目标函数`J_GRPO(θ)`进行优化，其中包含KL散度正则化项以稳定更新并保持与SFT参考模型的对齐。
</code></pre>
<p><strong>创新点</strong>：与现有方法相比，MobileVLA-R1的核心创新在于其<strong>显式的推理-执行设计</strong>和<strong>两阶段训练范式</strong>。它没有直接进行端到端映射，而是通过生成可解释的CoT中间步骤来桥接语义与控制，并通过先SFT对齐推理格式、后GRPO优化动作执行的组合方式，有效提升了推理的连贯性、控制的鲁棒性和长时程任务的稳定性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/基准</strong>：使用VLN-CE（包括R2R-CE和RxR-CE）评估高层导航推理能力，使用QUARD数据集评估低层驱动和推理对齐的控制能力。</li>
<li><strong>对比基线</strong>：在VLN-CE上对比了CMA、Sim2Sim、GridMM、ETPNav、HNR、NaVILA、StreamVLN、CorrectNav等大量SOTA方法；在QUARD上对比了CLIP、VC-1、QUART、MoRE等方法。</li>
<li><strong>评估指标</strong>：VLN-CE使用导航误差（NE）、成功率（SR）、成功率加权路径长度（SPL）等；QUARD报告六个四足控制任务上的平均成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>视觉语言导航性能</strong>：如表2所示，MobileVLA-R1在R2R-CE和RxR-CE的Val-Unseen分割上均取得了最优性能。在R2R-CE上，其SR达到68.3%，SPL达到65.2%，NE为4.05，全面优于之前的SOTA方法CorrectNav（SR 65.1%， SPL 62.3%）。在RxR-CE上，SR为71.5%，SPL为66.8%，同样领先。这证明了其卓越的推理对齐和泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17889v1/x4.png" alt="VLN-CE性能对比表"></p>
<blockquote>
<p><strong>表2</strong>：在VLN-CE Val-Unseen分割上与SOTA方法的对比。MobileVLA-R1在不依赖预训练路径点预测器的方法中表现最佳，在多个指标上领先。</p>
</blockquote>
<ol start="2">
<li><strong>四足控制与操作性能</strong>：如表3所示，在QUARD基准测试的六个任务中，MobileVLA-R1的平均成功率达到73%，显著超过了MoRE（60%）、QUART（44%）等基线，在所有难度级别（简单、中等、困难）的任务上都取得了最高成功率，展示了其在复杂具身环境中稳定的推理到控制接地和鲁棒执行能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17889v1/x5.png" alt="QUARD性能对比表"></p>
<blockquote>
<p><strong>表3</strong>：在QUARD基准上的整体性能。MobileVLA-R1在运动和操作任务上均取得最高成功率。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界部署</strong>：在Unitree Go2四足机器人上的实景测试（图4）进一步验证了其鲁棒性。如表4所示，在Workspace、Corridor、Outdoor三种环境下，无论是简单还是复杂指令，MobileVLA-R1的成功率（SR）均高于对比方法GPT-4o和NaVILA，导航误差（NE）更低，展现了其在真实复杂、部分可观测条件下的强大性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17889v1/x3.png" alt="硬件平台与部署"></p>
<blockquote>
<p><strong>图4</strong>：(a)硬件平台：Unitree Go2四足机器人。(b)部署流程：传感器数据发送给MobileVLA-R1进行推理和动作生成，结果返回给机载PC执行。(c)真实世界定性结果：模型能有效整合多模态观察，遵循长时程语言指令。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.17889v1/x6.png" alt="真实世界实验结果表"></p>
<blockquote>
<p><strong>表4</strong>：在Unitree Go2上进行的不同环境下的真实世界实验。MobileVLA-R1在简单和复杂指令下均取得最高的成功率和最低的导航误差。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：表5展示了GRPO中不同奖励组件的作用。仅使用SFT的基线SR为58.0%。单独添加移动、动作或格式奖励均能带来提升。组合使用移动和动作奖励（SR 64.5%）或动作和格式奖励（SR 65.2%）效果显著，而同时使用全部三个奖励组件（SR 68.3%）获得了最佳性能，证明了每个奖励项对提升导航成功率和路径效率都有贡献，且三者互补。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.17889v1/x7.png" alt="奖励消融实验表"></p>
<blockquote>
<p><strong>表5</strong>：在R2R-CE Val-Unseen上的奖励分解消融研究。移除任何奖励项都会降低性能，同时使用移动、动作和格式三项奖励效果最佳。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了MobileVLA-R1，一个通过显式思维链生成和连续四足执行来连接语义推理与电机控制的分层视觉-语言-动作框架，直接解决了核心的语义-控制鸿沟问题。</li>
<li>设计了一个整合监督CoT对齐与GRPO强化学习的两阶段训练框架，有效提升了推理一致性、控制鲁棒性和长时程稳定性。</li>
<li>构建了MobileVLA-CoT，一个用于具身轨迹的多粒度思维链数据集，并在具身AI基准上实现了约5%的性能提升，在Unitree Go2平台上实现了可靠部署。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确列出局限性，但根据方法描述可推断，其性能高度依赖于所构建的高质量MobileVLA-CoT数据集，而该数据集的生成依赖于大型多模态模型（Gemini-2.5-Flash），这可能引入数据偏差并增加成本。此外，模型在真实世界的长期部署中可能面临累积误差和动态环境适应等持续挑战。</p>
<p><strong>对后续研究的启示</strong>：本文的工作表明，在具身智能中引入<strong>显式、结构化的中间推理</strong>是连接高层语义与低层控制的有效途径。未来研究可以探索更自动化和可扩展的CoT数据生成方法，或将此“推理-执行”范式扩展到更广泛的机器人形态和更复杂的多任务场景中。同时，两阶段训练（SFT+RL）的策略为平衡模型的可控性、安全性与性能提供了参考框架。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MobileVLA-R1，旨在解决四足机器人将自然语言指令映射为连续控制时，高级语义推理与低级驱动难以衔接、导致接地不稳定和泛化弱的核心问题。方法上，构建了包含多粒度思维链的大规模数据集MobileVLA-CoT，并采用两阶段训练范式：先通过监督学习对齐思维链推理，再结合GRPO强化学习优化动作执行。实验表明，该框架在VLN和VLA任务上性能超越强基线约5%，并在真实四足机器人上验证了其在复杂环境中的鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.17889" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>