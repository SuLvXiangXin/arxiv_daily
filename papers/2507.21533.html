<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Model Predictive Adversarial Imitation Learning for Planning from Observation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Model Predictive Adversarial Imitation Learning for Planning from Observation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.21533" target="_blank" rel="noreferrer">2507.21533</a></span>
        <span>作者: Byron Boots Team</span>
        <span>日期: 2025-07-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从演示中学习规划的主流范式是通过逆向强化学习（IRL）学习奖励函数，然后使用模型预测控制（MPC）来部署该奖励。然而，IRL和MPC通常被视为独立的训练和部署流程。与此同时，对抗性模仿学习（AIL）在算法复杂性和样本效率上相较IRL有显著改进，但其对强化学习（RL）策略的依赖，使其在具有安全约束的应用中变得复杂。此外，由于部分可观测性，实际部署通常优先使用基于模型的规划以保证实时性能、可信度和可解释性。</p>
<p>本文针对上述痛点，提出将AIL中的标准策略网络替换为一个MPC智能体。这一新视角旨在统一IRL和MPC，实现从纯观察演示中端到端地学习规划器。本文的核心思路是：将模型预测路径积分（MPPI）控制这一规划器嵌入到AIL框架中，使其直接作为生成器，通过学习一个判别器来匹配专家行为，从而实现从观察中学习规划。</p>
<h2 id="方法详解">方法详解</h2>
<p>模型预测对抗模仿学习（MPAIL）的整体框架是将一个MPC智能体嵌入到AIL中，用于从观察中学习成本。在部署时，MPC智能体通过短期的模型推演（即规划）进行在线优化，这些推演由学习到的成本（判别器）和价值网络进行评估。</p>
<p><img src="https://arxiv.org/html/2507.21533v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：模型预测对抗模仿学习（MPAIL）框架概述。它将一个MPC智能体嵌入到对抗性模仿学习（AIL）中，用于从观察中学习成本。在训练时，智能体执行规划，其产生的状态转移与专家数据一起用于更新判别器（D）和价值网络（V）。部署时，MPC智能体利用学习到的网络进行在线规划。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>模型预测智能体（MPPI）</strong>：作为策略 $\pi_{\text{MPPI}}$。它在每个时间步采样一组动作序列（规划），使用预测模型 $f$ 进行前向推演，形成轨迹 $\tau_t^{(H)}$。这些轨迹由一个总成本 $C(\tau_t)$ 评估，该成本结合了判别器给出的即时成本和价值网络给出的终端成本。MPPI通过最小化轨迹成本并约束其与先前策略分布（通过KL散度）的偏差，来优化动作序列。优化后的动作分布被拟合为一个高斯分布，其均值作为执行的动作。</li>
<li>**判别器网络 $D_\theta(s, s&#39;)$**：用于区分智能体生成的状态转移 $(s, s&#39;)$ 和专家状态转移。其输出被转化为奖励 $r(s, s&#39;) = \log(D(s, s&#39;)) - \log(1 - D(s, s&#39;))$，即判别器的logit值，作为MPPI规划的成本依据。</li>
<li>**价值网络 $V_\phi(s)$**：用于估计状态的长期回报 $G_t$，作为MPPI有限规划视野之外的终端成本，以进行长视野推理。</li>
</ol>
<p>训练流程（算法1）与GAIL类似，但关键区别在于策略更新步骤被MPPI的在线规划所取代。具体而言，每一轮迭代中：a) 运行 $\pi_{\text{MPPI}}$ 在环境中收集转移数据；b) 使用收集到的数据和专家数据更新判别器参数（公式5）；c) 使用估计的回报更新价值网络参数（公式6）。与现有方法相比，MPAIL的创新点在于：用规划器（MPPI）完全替代了传统的参数化策略网络，实现了IRL（通过AIL）与MPC在训练流程上的统一；并专门针对仅观察（state-only）的演示设置进行了推导。</p>
<p><img src="https://arxiv.org/html/2507.21533v1/x2.png" alt="MPPI策略示意图"></p>
<blockquote>
<p><strong>图2</strong>：MPAIL中 $\pi_{\text{MPPI}}$ 的示意图。(1) 采样一组动作序列并进行模型推演。(2) 根据判别器给出的成本评估规划，使分布向专家行为偏移。训练中温度参数 $\lambda$ 下降，以在后续迭代中缩小优化分布。(3) 策略 $\pi_{\text{MPPI}}$ 是对优化后的规划及其首个动作进行高斯拟合的结果。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟控制基准和真实世界导航任务上进行。使用了自定义的10自由度车辆模拟导航任务，其中专家数据来自一个在目标点附近绕圈（而非精确停止）的PPO策略，以测试算法对次优、多模态演示的模仿能力。对比的基线方法包括经典的AIL算法GAIL和AIRL。</p>
<p>关键实验结果如下：</p>
<ul>
<li><strong>分布外（OOD）恢复与鲁棒性（Q1）</strong>：将训练好的智能体置于远离专家和训练数据分布的区域（40m×40m范围）启动。如图3和4所示，基于规划的MPAIL相比基于策略的GAIL，在OOD状态下表现出显著更好的泛化能力，能够成功导航至目标区域。量化指标显示，在大部分OOD起始点上，MPAIL获得的最终奖励远高于GAIL。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.21533v1/figures/results/ood/ood-result.png" alt="OOD定性对比"></p>
<blockquote>
<p><strong>图3</strong>：基于策略的AIL（GAIL）和基于规划的AIL（MPAIL）在分布外（OOD）状态下的行为对比。智能体在40m×40m区域内随机位姿启动。专家数据主要覆盖(0,0)到(10,10)区域。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.21533v1/x3.png" alt="OOD定量对比"></p>
<blockquote>
<p><strong>图4</strong>：OOD定量比较。横轴为初始位姿与专家数据分布的（能量距离）差异，纵轴为运行100步后的最终奖励。MPAIL（蓝色）在OOD情况下获得的奖励普遍高于GAIL（橙色）。</p>
</blockquote>
<ul>
<li><strong>从单次演示进行真实世界导航（Q2）</strong>：通过“真实-模拟-真实”流程，仅使用<strong>单个</strong>部分可观察的真人演示视频（未提供动作或深度信息），MPAIL成功学习并控制机器人完成了导航任务。图5展示了学习到的成本图与真实机器人执行轨迹。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.21533v1/x4.png" alt="真实世界导航结果"></p>
<blockquote>
<p><strong>图5</strong>：从单次部分可观察演示中进行真实世界导航。(a) 演示视频帧。(b) 学习到的成本图（热力图）覆盖在场景地图上，亮黄色区域成本低（目标区域）。(c) 机器人实际运行的轨迹（白线）成功抵达目标。</p>
</blockquote>
<ul>
<li><strong>与现有AIL算法对比（Q3）</strong>：在模拟导航基准任务上，MPAIL达到了与GAIL相当甚至更优的渐近性能，同时样本效率显著更高（图6）。AIRL在该任务上未能有效收敛。消融实验（图9）表明，价值网络对于学习长视野任务至关重要，移除后性能大幅下降；同时，判别器的光谱归一化对训练稳定性有积极影响。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.21533v1/figures/results/benchmarks/benchmarking.png" alt="基准任务性能对比"></p>
<blockquote>
<p><strong>图6</strong>：在模拟导航基准任务上，MPAIL与GAIL的对比。左图：MPAIL的样本效率（达到特定奖励所需的交互步数）显著高于GAIL。右图：两者的渐近性能（最终平均奖励）相当。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li>提出了<strong>模型预测对抗模仿学习（MPAIL）</strong>框架，首次将MPC规划器作为生成器嵌入AIL，实现了从纯观察演示中端到端学习规划器，统一了IRL训练与MPC部署。</li>
<li>实证表明，基于规划的模仿学习在<strong>样本效率、分布外泛化、鲁棒性</strong>方面优于基于策略的方法，并能从<strong>单个部分可观察演示</strong>中完成真实世界导航任务。</li>
<li>为MPPI规划器在AIL中的使用提供了理论推导，证明了其与熵正则化RL目标在状态转移层面的等价性。</li>
</ol>
<p>论文提到的局限性包括：为了集中研究规划器的作用，本文没有引入策略先验，这在更复杂的任务中可能限制性能；理论分析方面，对训练中温度衰减的稳定作用尚未完全解释。</p>
<p>本文的启示在于：对于需要安全性、可解释性和实时规划的应用，用<strong>规划器直接替代参数化策略网络</strong>是一条有效路径；<strong>模型与价值函数结合</strong>的规划方式能有效提升在稀疏奖励模仿学习任务中的泛化能力；<strong>端到端</strong>的学习框架能够更灵活地将在线约束和动态纳入训练，有利于在真实机器人系统中实现从极少量演示中学习复杂技能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MPAIL方法，旨在解决仅从模糊、不完整的观察数据中学习可靠规划策略的问题。核心方法是将模型预测控制（MPC）代理嵌入对抗模仿学习（AIL）框架，替代传统策略网络，实现端到端的从观察中学习成本函数。该方法在模拟和真实导航实验中，仅需极少甚至单次观察演示，即显著提升了样本效率、分布外泛化能力和鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.21533" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>