<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.07134" target="_blank" rel="noreferrer">2510.07134</a></span>
        <span>作者: Liu, Jiahang, Qi, Yunpeng, Zhang, Jiazhao, Li, Minghan, Wang, Shaoan, Wu, Kui, Ye, Hanjing, Zhang, Hong, Chen, Zhibo, Zhong, Fangwei, Zhang, Zhizheng, Wang, He</span>
        <span>日期: 2025/10/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>具身视觉跟踪（EVT）是机器人跟随指定移动目标的一项基础任务。当前，利用大型视觉语言模型（VLM）或语言模型（LLM）的端到端视觉-语言-动作（VLA）模型已成为主流方法，例如TrackVLA和LOVON。然而，现有方法缺乏<strong>显式的空间推理能力</strong>和<strong>有效的时序记忆机制</strong>，导致其在目标遭遇严重遮挡或存在外观相似的干扰物时，性能显著下降。</p>
<p>本文针对上述两个关键痛点，提出了一种增强推理与记忆能力的新VLA框架TrackVLA++。其核心思路是：通过一种新颖的<strong>极坐标思维链（Polar-CoT）</strong> 机制进行轻量化的空间推理，并设计一个<strong>置信度门控的目标识别记忆（TIM）</strong> 模块来维持长时程的目标身份表征，从而提升在复杂动态场景下的跟踪鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>TrackVLA++是一个端到端的VLA模型，基于导航基础模型NavFoM构建，并集成了Polar-CoT和TIM两大核心模块。其整体流程是：给定在线视频流和语言指令，模型首先通过Polar-CoT推理出目标的空间位置（极坐标令牌），然后利用此推理结果以置信度门控的方式更新TIM记忆，最后结合视觉特征、语言指令、推理令牌和记忆令牌，由LLM预测动作令牌，并解码为跟踪轨迹。</p>
<p><img src="https://arxiv.org/html/2510.07134v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：TrackVLA++的流程框架。给定视频流和语言指令，模型通过Polar-CoT模块（左）推理目标位置，生成一个极坐标令牌（或无效令牌）。同时，Target Identification Memory (TIM)模块（右）根据上一帧的推理置信度，对记忆进行门控更新。最终，视觉特征、语言令牌、推理令牌和记忆令牌被拼接，输入LLM以预测动作。</p>
</blockquote>
<p><strong>核心模块1: Polar-CoT空间推理机制</strong><br>该模块旨在为模型提供显式的、轻量化的空间推理能力。它将智能体可感知的环形视野（0.6m至5.0m）离散化为60个角度扇区和30个距离区间，每个（角度，距离）组合被编码为一个独特的词汇令牌，形成一个紧凑的、以智能体为中心的极坐标空间表示。此外，词汇表中包含一个特殊的<code>&lt;invalid&gt;</code>令牌，用于表示目标被遮挡或不在视野内。推理过程由LLM执行：将投影后的视觉嵌入、TIM记忆嵌入和语言令牌拼接后，LLM直接预测出代表目标当前位置的推理令牌。这种设计避免了传统基于边界框推理的计算冗余和歧义（尤其在多视角下），实现了高效且统一的空间表示。</p>
<p><strong>核心模块2: 目标识别记忆（TIM）</strong><br>TIM模块负责维护目标的长时程视觉身份表征，以应对遮挡和干扰。其更新采用<strong>置信度门控机制</strong>。在每一时间步T，TIM状态由前一状态和新候选特征加权平均得到。候选特征是根据上一帧Polar-CoT预测的令牌，从细粒度视觉特征中提取的目标区域嵌入。更新的权重由上一帧推理的置信度决定：置信度接近1时，新特征被大量融合；置信度接近0时，记忆状态几乎保持不变。特别地，当预测到<code>&lt;invalid&gt;</code>令牌时，强制其置信度为0，从而冻结记忆更新，保留最后一次可靠的目标表征，直到目标被重新自信地检测到。这种设计确保了记忆在目标丢失期间不会被干扰物污染。</p>
<p><strong>创新点</strong><br>与现有方法相比，TrackVLA++的主要创新体现在：1) <strong>高效的CoT设计</strong>：不同于机器人操作任务中生成冗长文本计划或视觉中间产物（如边界框）的CoT，Polar-CoT仅预测一个紧凑的令牌，在保持推理能力的同时极大提升了效率，适应EVT任务的动态性需求。2) <strong>推理引导的记忆更新</strong>：TIM的更新直接依赖于Polar-CoT的推理输出和置信度，实现了感知、推理与记忆的紧密耦合，使记忆更新更具针对性和鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：模型在EVT-Bench和Gym-UnrealCV两个基准上进行评估，对比了包括IBVS、PoliFormer、EVT、Uni-NaVid、TrackVLA和NavFoM在内的多个基线方法。评估指标包括成功率（SR）、跟踪率（TR）、碰撞率（CR）和平均回合长度（EL）。</p>
<p><strong>关键实验结果</strong>：<br>在最具挑战性的EVT-Bench DT（分心跟踪）任务上，TrackVLA++取得了显著提升。在单视角设置下，其成功率达到66.5%，优于NavFoM的61.4%和TrackVLA的57.6%。在四视角设置下，其成功率进一步提升至74.0%，远超NavFoM的62.0%。</p>
<p><img src="https://arxiv.org/html/2510.07134v1/x4.png" alt="仿真实验可视化"></p>
<blockquote>
<p><strong>图4</strong>：在EVT-Bench上的仿真实验可视化。TrackVLA++在存在遮挡和相似干扰物（如多个相同外观的人）的场景下表现稳健。左上角插图展示了Polar-CoT的预测结果（红色区域为预测的目标位置）。</p>
</blockquote>
<p>在Gym-UnrealCV基准的零样本测试中，TrackVLA++在所有子任务上均达到最优。尤其在“Distractor”任务中，其成功率（0.92）和平均回合长度（484）均超过了之前的SOTA方法TrackVLA。</p>
<p><strong>组件贡献分析</strong>：<br>论文通过消融实验验证了核心组件的有效性。在细粒度识别任务中，移除Polar-CoT模块后，模型准确率从87.5%下降至83.0%，推理速度从4.8 FPS略微提升至5.2 FPS。这证明了Polar-CoT在显著提升模型推理与识别能力的同时，仅引入了微小的计算开销。TIM模块则通过在真实世界遮挡实验中的优异表现，证实了其对于维持长时程跟踪一致性的关键作用。</p>
<p><img src="https://arxiv.org/html/2510.07134v1/x5.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图5</strong>：真实世界实验可视化与定量结果。TrackVLA++在“障碍物遮挡”、“蜿蜒路径”和“干扰物”三个挑战性场景中均表现出色。柱状图定量对比显示，TrackVLA++的成功率相比TrackVLA有显著提升。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了用于EVT任务的<strong>Polar-CoT机制</strong>，以紧凑的令牌形式实现高效的空间推理；2) 设计了<strong>目标识别记忆（TIM）模块</strong>，通过置信度门控更新策略，增强模型对遮挡和干扰的鲁棒性；3) 在多个仿真基准和真实场景中实现了<strong>最先进的性能</strong>，并展示了强大的零样本泛化能力。</p>
<p><strong>局限性</strong>：论文提到，引入Polar-CoT模块会使推理速度相较于无该模块的版本略有下降（从5.2 FPS降至4.8 FPS），这是在提升性能与保持效率之间需要权衡的一点。</p>
<p><strong>研究启示</strong>：TrackVLA++的工作表明，为动态的具身任务设计<strong>轻量化、紧耦合的推理模块</strong>至关重要。其Polar-CoT的设计思路为其他需要实时空间理解的机器人任务提供了参考。同时，<strong>基于置信度的记忆更新机制</strong>为解决长时程任务中的状态一致性问题提供了一个有效的范式，可广泛应用于需要持续跟踪或状态估计的 embodied AI 场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对具身视觉跟踪（EVT）任务中，现有模型因缺乏显式空间推理和有效时间记忆，而在严重遮挡或存在相似干扰物时易失败的问题，提出了TrackVLA++模型。其核心创新在于引入了**Polar-CoT空间推理模块**（通过思维链推断目标极坐标位置）和**门控更新的目标识别记忆模块**，以增强时空一致性。实验表明，该方法在EVT-Bench DT基准上分别以5.1%和12%的优势超越先前最佳模型，并展现出强大的零样本泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.07134" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>