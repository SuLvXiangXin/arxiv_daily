<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Accelerating Structured Chain-of-Thought in Autonomous Vehicles - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Accelerating Structured Chain-of-Thought in Autonomous Vehicles</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.02864" target="_blank" rel="noreferrer">2602.02864</a></span>
        <span>作者: Gu, Yi, Wang, Yan, Chen, Yuxiao, You, Yurong, Luo, Wenjie, Wang, Yue, Ding, Wenhao, Li, Boyi, Yang, Heng, Ivanovic, Boris, Pavone, Marco</span>
        <span>日期: 2026/02/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在自动驾驶领域，将语言作为核心模态的视觉-语言-动作（VLA）模型正日益流行，其中思维链（CoT）推理通过将复杂问题分解为一系列中间步骤，显著提升了模型的决策性能。然而，CoT的自回归解码特性（逐个生成令牌）带来了巨大的推理延迟，通常需要生成数百个额外令牌，这与自动驾驶对高频（如10Hz）实时决策的严格要求相冲突，成为实际应用的关键瓶颈。尽管已有工作探索通用推理任务（如数学、代码）的子任务分解与并行，但这些任务中可并行化的独立子任务数量有限，且分解策略复杂，难以获得理想的加速效果。</p>
<p>本文针对自动驾驶场景中CoT推理延迟过高这一具体痛点，提出了一个新的视角：利用自动驾驶推理过程固有的结构化和高并行性特点来加速CoT。作者观察到，与通用任务不同，自动驾驶代理（如评估路况、交通标志、关键物体）的多个推理组件在很大程度上是相互独立的，这为并行化提供了天然基础。此外，自动驾驶代理通常遵循基于固定观测因素的结构化推理模式，这种规律性使得无需进行临时性的任务分解，可以直接使用更复杂的算法来编排CoT轨迹的并行生成。本文的核心思路是：首先将自动驾驶的CoT格式化成一个结构化的模板，然后基于字段间的依赖关系构建一个有向无环图，最后设计一个动态调度算法，在单次模型前向传播中并行生成所有依赖已满足的字段，从而显著减少顺序计算次数。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法名为FastDriveCoT，旨在通过并行解码加速自动驾驶任务中的结构化CoT推理。其整体流程分为三步：1）为自动驾驶任务设计一个结构化的CoT模板；2）根据模板字段间的依赖关系构建依赖图，并设计动态规划算法进行最优调度；3）在语言模型推理层面，通过精心设计的注意力掩码和序列打包策略，在单次前向传播中高效地并行生成多个字段。</p>
<p><img src="https://arxiv.org/html/2602.02864v1/x2.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图2</strong>：VLA中结构化思维链（CoT）的并行解码。CoT被分解为具有特定依赖关系的字段模板，用边表示。所有依赖关系已满足的独立字段（如图中的车道、交通标志和关键对象）同时解码。LLM内的背景阴影显示了KV缓存的更新。</p>
</blockquote>
<p><strong>核心模块一：模板化CoT</strong><br>该方法首先定义了一个针对自动驾驶推理的CoT模板，将推理任务分解为一系列具体的字段。初始字段用于描述驾驶环境和关键实体，包括<code>lighting</code>（光照）、<code>road condition</code>（路况）、<code>weather</code>（天气）、<code>type of junction</code>（路口类型）、<code>type of road</code>（道路类型）、<code>lanes</code>（车道）、<code>critical objects</code>（关键对象）、<code>traffic light</code>（交通灯）、<code>traffic sign</code>（交通标志）和<code>additional traffic regulation</code>（附加交通规则）。对于<code>critical objects</code>，还进一步包含其<code>relative position</code>（相对位置）、<code>object type</code>（对象类型）和<code>justification</code>（判断依据）。随后是场景的结构化总结字段，如<code>traffic regulation</code>（交通规则总结）。模板以对自车预期行为的<code>overall summary</code>（整体总结）结束。为支持并行，对于包含可变数量实例的字段（如<code>lanes</code>和<code>critical objects</code>），采用两阶段生成：第一阶段（枚举）生成高级概述（如识别要分析的不同时间范围或枚举每个关键对象）；第二阶段（阐述）并行地为第一阶段枚举的每个实例生成详细描述。模板为这些字段预定义了固定数量的槽位（如3个车道时间范围、4个关键对象），不足时用“N/A”填充。</p>
<p><strong>核心模块二：依赖图与调度算法</strong><br>模板中的字段之间存在依赖关系。例如，交通规则总结依赖于交通灯和交通标志字段的完成；多实例字段的阐述阶段依赖于其枚举阶段。为了管理这些依赖并最大化并行度，作者引入了<strong>依赖图</strong>，这是一个有向无环图（DAG），节点代表模板字段，边（A→B）表示字段B的生成直接依赖于字段A的完成。</p>
<p><img src="https://arxiv.org/html/2602.02864v1/x3.png" alt="依赖图与注意力掩码示例"></p>
<blockquote>
<p><strong>图3</strong>：依赖图的一部分及相应的注意力掩码示例。由于独立性及由此产生的并行可能性，例如字段A、B、D、E之间不能互相关注。其他具有依赖关系的字段对，后者可以像往常一样关注前者。具体来说，固定令牌是预填充的，因此无论依赖关系如何，所有后续令牌都可以关注它们。填充令牌不能被其他令牌关注。</p>
</blockquote>
<p>基于依赖图，作者设计了一个动态规划算法（如Algorithm 1所示）来调度并行解码。算法初始化时，将所有入度为0的源节点（即没有依赖的字段）加入“就绪”集合。在每一步迭代中，模型通过单次前向传播为就绪集合中的所有字段并行生成一个新令牌。当一个字段生成完毕时，其节点会通知所有依赖它的节点。一旦某个节点的所有前置节点都已完成，该节点就被加入就绪集合。此调度算法在最小化前向传播次数方面是最优的，该最小值等于依赖图中关键路径（任何依赖链上的最大累计令牌数）的长度。</p>
<p><strong>核心模块三：语言模型推理实现</strong><br>为了实现高效的并行解码，FastDriveCoT将整个模板CoT格式化为一个连续的序列。在解码时，将多个字段的令牌沿序列长度维度打包在一起，在单次前向传播中同时计算所有并行字段的下一个令牌对数概率，同时保持原始批次大小。这种方式允许键值（KV）缓存跨所有字段完全共享和重用，最大化计算效率。为了在单个序列中强制执行正确的因果依赖，作者根据依赖图设计了一个自定义的注意力掩码。该掩码规定，字段B中的令牌只能关注字段A中的令牌，当且仅当A是B在依赖图中的祖先节点（即存在从A到B的路径）。固定模板令牌对所有后续令牌可见，而填充令牌则被设置为对所有其他令牌不可见，以避免不必要的计算。通过将每个字段填充或截断到预定义的最大长度，确保了位置编码的正确性。</p>
<p><strong>创新点</strong><br>与现有方法相比，FastDriveCoT的创新点具体体现在：1) <strong>领域特定的结构化并行</strong>：并非通用任务分解，而是利用自动驾驶推理固有的结构化模式和高并行潜力。2) <strong>最优调度</strong>：基于依赖图的动态规划算法实现了前向传播次数最优的并行调度。3) <strong>高效的序列级实现</strong>：通过将并行字段打包进单个序列并设计定制注意力掩码，实现了KV缓存的完全共享，在几乎零额外计算开销的前提下，减少了GPU内存访问瓶颈。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验使用了内部的大规模驾驶数据集（约20,000小时，717,344个训练样本，950个测试样本）。评估了三种不同规模的基模型：Qwen2-0.5B、Qwen3-1.7B和Qwen2.5-VL-3B，并采用了两种架构：纯自回归VLA（VLA-AR）和结合流匹配的Transfusion架构。对比的基线包括：1) <strong>无CoT</strong>：不生成中间CoT的端到端模型；2) <strong>自回归CoT</strong>：使用相同模板但顺序生成字段的模型。</p>
<p><strong>关键实验结果</strong>：<br>如表1所示，在任务性能方面，引入CoT推理显著提升了元动作预测的IOU和轨迹预测的ADE（平均位移误差）。例如，使用Qwen2.5-VL 3B模型时，3秒轨迹ADE从无CoT的0.617提升到自回归CoT的0.511，FastDriveCoT进一步将其提升至0.482。在效率方面，FastDriveCoT在CoT生成时间上实现了<strong>3.1倍至4.1倍</strong>的加速，端到端总体推理时间实现了<strong>1.9倍至3.1倍</strong>的加速，且在不同模型和架构上表现一致。</p>
<p><img src="https://arxiv.org/html/2602.02864v1/figures/sns-time-path.png" alt="时间分解分析"></p>
<blockquote>
<p><strong>图4</strong>：不同模型和架构下，无CoT、自回归CoT和FastDriveCoT推理过程的时间分解。堆叠条形图显示了生成CoT、元动作和轨迹所花费的时间。FastDriveCoT（绿色）显著减少了CoT生成时间。</p>
</blockquote>
<p><strong>消融与分析</strong>：<br>图4的时间分解直观展示了FastDriveCoT如何大幅削减CoT生成耗时。图5进一步分析了加速比与并行度的关系，表明依赖图中可同时解码的最大字段数（最大并行度）越高，获得的加速比就越大，这验证了利用自动驾驶推理高并行性这一设计的有效性。</p>
<p><img src="https://arxiv.org/html/2602.02864v1/figures/sns-speedup-degree.png" alt="加速比与并行度关系"></p>
<blockquote>
<p><strong>图5</strong>：FastDriveCoT相对于自回归CoT的加速比与依赖图中最大并行度（即任一解码步骤中可同时生成的字段最大数量）的关系。趋势线表明更高的并行度能带来更大的加速比。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>FastDriveCoT</strong>，一种针对自动驾驶场景加速结构化CoT推理的并行解码方法；2) 设计了基于<strong>依赖图</strong>的动态最优调度算法，能够最小化生成所需的前向传播次数；3) 实现了一种<strong>高效的序列打包与注意力掩码方案</strong>，在共享KV缓存、不增加额外计算开销的前提下，显著减少了推理延迟。</p>
<p>论文自身提到的局限性在于，其模板是固定的，对于多实例字段（如车道、关键对象）采用了预定义数量的槽位，而非根据枚举结果动态调整模板结构，这被列为未来的研究方向。</p>
<p>本文的启示在于，对于具有高度结构化、可分解推理模式的特定领域（如自动驾驶、机器人任务规划），可以充分利用其内在的并行性来设计专用的加速方案，从而在保持甚至提升性能的前提下，突破自回归推理的速度瓶颈，使其适用于对实时性要求严苛的实际系统。这为将大模型复杂推理能力部署到实时边缘设备开辟了一条可行的技术路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对自动驾驶中结构化思维链（CoT）推理因自回归解码导致的高延迟问题，提出FastDriveCoT并行解码方法。该方法将推理过程分解为识别关键对象、总结交通规则等子任务的依赖图，通过单次前向传递并行生成多个独立步骤，大幅减少顺序计算。实验表明，CoT生成速度提升3-4倍，端到端延迟显著降低，同时保持了CoT原有的下游任务性能改进。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.02864" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>