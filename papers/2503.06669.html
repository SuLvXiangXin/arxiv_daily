<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.06669" target="_blank" rel="noreferrer">2503.06669</a></span>
        <span>作者: AgiBot-World-Contributors, Bu, Qingwen, Cai, Jisong, Chen, Li, Cui, Xiuqi, Ding, Yan, Feng, Siyuan, Gao, Shenyuan, He, Xindong, Hu, Xuan, Huang, Xu, Jiang, Shu, Jiang, Yuxin, Jing, Cheng, Li, Hongyang, Li, Jialu, Liu, Chiming, Liu, Yi, Lu, Yuxiang, Luo, Jianlan, Luo, Ping, Mu, Yao, Niu, Yuehan, Pan, Yixuan, Pang, Jiangmiao, Qiao, Yu, Ren, Guanghui, Ruan, Cheng, Shan, Jiaqi, Shen, Yongjian, Shi, Chengshi, Shi, Mingkang, Shi, Modi, Sima, Chonghao, Song, Jianheng, Wang, Huijie, Wang, Wenhao, Wei, Dafeng, Xie, Chengen, Xu, Guo, Yan, Junchi, Yang, Cunbiao, Yang, Lei, Yang, Shukai, Yao, Maoqing, Zeng, Jia, Zhang, Chi, Zhang, Qinglin, Zhao, Bin, Zhao, Chengyue, Zhao, Jiaqi, Zhu, Jianchao</span>
        <span>日期: 2025/03/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域在追求通用智能时面临核心瓶颈：缺乏高质量、大规模的真实世界数据。现有主流方法依赖聚合现有数据集（如Open X-Embodiment， OXE）或通过众包收集（如DROID），但存在关键局限性：数据规模有限、任务多为实验室环境下的短视界简单任务、数据质量参差不齐、硬件异构且缺乏标准化，导致训练出的策略泛化能力弱，难以应对真实世界的复杂、长视界操作。本文针对“如何通过扩展真实世界机器人数据来有效解决现实复杂性”这一痛点，提出了一个全栈式的大规模操作平台新视角。其核心思路是构建一个超大规模、高质量、多样化的真实世界机器人数据集（AgiBot World），并基于此提出一种利用潜在动作表示的新型通用策略（GO-1），以最大化数据利用并实现可预测的性能扩展。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文工作包含两大核心：AgiBot World数据集平台与GO-1通用策略模型。数据集平台的核心是标准化的数据收集流程与高质量保证机制；GO-1策略则采用一种分层的Vision-Language-Latent-Action（ViLLA）框架进行训练。</p>
<p><strong>AgiBot World 数据平台</strong>：平台部署了100台同构的双臂仿人机器人（AgiBot G1），配备移动底盘、模块化末端执行器（夹爪或6自由度灵巧手）以及视觉触觉传感器，通过8个摄像头提供多视角观测。数据收集采用“人在回路”的验证框架以确保高质量。</p>
<p><img src="https://arxiv.org/html/2503.06669v4/x1.png" alt="数据收集流程"></p>
<blockquote>
<p><strong>图2</strong>：数据收集流程。分为任务可行性验证、标准化收集与后处理标注三个阶段，并引入失败恢复数据和人机交互反馈循环，持续优化数据质量。</p>
</blockquote>
<p>收集流程分为三阶段：1）任务可行性验证与标准制定；2）熟练操作员按标准进行数据收集与初步验证；3）后处理阶段进行标准复核与语言标注。创新性地保留了操作员失误后成功恢复的轨迹（约占总数据1%），并标注失败原因，用于策略对齐与失败反思。通过“收集-训练-部署-反馈”的迭代循环，不断优化收集协议。</p>
<p><strong>GO-1 策略模型</strong>：GO-1基于ViLLA框架，通过三个阶段训练，将网络规模视觉语言模型（VLM）的通用推理能力与机器人序列决策桥接起来。</p>
<p><img src="https://arxiv.org/html/2503.06669v4/x3.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图4</strong>：GO-1方法整体框架（ViLLA）。包含三个阶段：Stage 1在异构网络视频数据上训练潜在动作模型（LAM），学习通用动作表示；Stage 2以LAM生成的潜在动作为伪标签，训练潜在规划器进行长视界规划；Stage 3联合训练动作专家，生成具体的机器人控制指令。</p>
</blockquote>
<p><strong>阶段一：潜在动作模型（LAM）</strong>。为了利用无动作标签的网络视频（如Ego4D人类视频）和跨具身数据，LAM通过编码器-解码器结构学习连续帧之间的逆向动力学，将图像对映射到一个离散的潜在动作空间。编码器为时空Transformer，解码器为空间Transformer，使用VQ-VAE目标对潜在动作令牌进行量化。这使得潜在动作成为一种中间表示，桥接了通用图像-文本输入与具体机器人动作。</p>
<p><strong>阶段二：潜在规划器</strong>。以预训练的VLM（InternVL2.5-2B）为骨干，获得强大的场景理解和推理能力。潜在规划器是一个24层的Transformer，以多视角图像（头、左腕、右腕）和语言指令为条件，预测由LAM编码器生成的潜在动作令牌。由于潜在动作空间远小于离散化的低级动作空间，这能更高效地将通用VLM适配到机器人策略中，并支持具身无关的长视界规划。</p>
<p><strong>阶段三：动作专家</strong>。为了实现高频、灵巧的操作，引入一个基于扩散目标的动作专家来建模低级动作的连续分布。其架构与潜在规划器类似，但目标不同：它以前述模块（VLM、潜在规划器）为条件，通过迭代去噪过程回归未来一段时间窗口（H=30步）的低级动作块（包括关节位置等）。</p>
<p><strong>推理过程</strong>：GO-1整合以上所有模块：首先，VLM和潜在规划器根据观测和指令预测潜在动作令牌；然后，这些令牌与观测、本体感觉信息一同条件化动作专家，通过去噪生成最终的控制信号。</p>
<p>与现有方法相比，其创新点在于：1）提出了一个规模空前、质量可控、专注于真实世界长视界与双手机器人操作的数据集构建范式；2）在策略层面，引入了潜在动作作为中间抽象，有效融合了网络规模视频数据与机器人数据，提升了模型的泛化与规划能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：所有评估均在真实世界场景中进行。使用了AgiBot World数据集内部划分进行评估，并与多个前沿基线方法对比，包括在大规模机器人数据集上训练的策略（如基于OXE训练的策略）、以及先进的通用策略模型（如RDT）。评估任务涵盖域内任务和分布外（OOD）任务。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>数据集有效性</strong>：在AgiBot World数据集上预训练的策略，相比在先前最大规模数据集OXE上预训练的策略，平均成功率提升了30%。即使仅使用相当于OXE数据量1/10的AgiBot World数据子集，预训练策略的泛化能力仍提升了18%。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.06669v4/x4.png" alt="策略性能对比"></p>
<blockquote>
<p><strong>图5</strong>：GO-1与基线方法在真实世界任务中的成功率对比。GO-1在复杂灵巧操作和长视界任务中表现优异，平均成功率超过60%，相比RDT方法提升了32%。</p>
</blockquote>
<ol start="2">
<li><p><strong>GO-1策略性能</strong>：GO-1在复杂的真实世界灵巧和长视界任务中取得了超过60%的平均成功率，显著优于之前的RDT方法（提升32%）。图5展示了在具体任务（如“倒水”、“叠衣服”、“使用工具”）上的详细对比。</p>
</li>
<li><p><strong>消融实验</strong>：</p>
<ul>
<li><strong>潜在动作规划器的重要性</strong>：移除潜在规划器（即VLM直接条件化动作专家）会导致性能显著下降，尤其在长视界任务上。</li>
<li><strong>数据规模缩放律</strong>：实验表明，GO-1的性能随着训练数据量的增加而稳健提升，证明了其可扩展性。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2503.06669v4/x5.png" alt="消融研究与缩放律"></p>
<blockquote>
<p><strong>图6</strong>：左：消融实验显示潜在规划器对长视界任务至关重要。右：GO-1性能随训练数据量增加而提升，呈现积极的数据缩放趋势。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.06669v4/x6.png" alt="复杂任务定性结果"></p>
<blockquote>
<p><strong>图7</strong>：GO-1执行复杂长视界任务的定性示例，例如准备咖啡，展示了其处理多步骤任务的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.06669v4/x7.png" alt="失败恢复分析"></p>
<blockquote>
<p><strong>图8</strong>：GO-1在遇到干扰或错误后能够进行恢复的示例，体现了其从数据中学到的鲁棒性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>构建并开源了AgiBot World Colosseo平台，这是一个包含超100万条轨迹、专注于真实世界双手机器人长视界操作的大规模高质量数据集，其规模、多样性和数据质量保障机制均超越了现有数据集。</li>
<li>提出了GO-1通用策略，创新性地采用潜在动作表示作为中间桥梁，有效地将网络规模视觉语言模型的通用知识与机器人控制相结合，在真实世界任务中展现了卓越的泛化能力、长视界规划能力和灵巧操作性能，并验证了其性能随数据规模扩展的规律。</li>
</ol>
<p><strong>局限性</strong>：论文明确指出，目前所有评估均在真实世界中进行。仿真环境仍在开发中，这限制了快速、可重复的评估能力。</p>
<p><strong>对后续研究的启示</strong>：本研究证明了构建大规模、标准化、高质量的真实世界机器人数据集对于推进通用机器人智能的关键作用。其“人在回路”的数据质量保障流程、对失败恢复数据的收集、以及利用潜在动作抽象融合异构数据的框架，为未来机器人基础模型的研究提供了重要的数据基础和方法论参考。开源整个平台（数据、工具链、模型）有望极大促进社区在家庭助理、工业自动化等真实场景中的研究创新。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作缺乏高质量、大规模数据的关键瓶颈，提出了AgiBot World大规模操作平台。该平台通过标准化采集流程与人机验证，构建了涵盖217个任务、超100万轨迹的庞大数据集。基于此，作者提出通用策略GO-1，利用潜在动作表示最大化数据利用率。实验表明，基于该数据集的策略在域内和域外场景下平均性能比Open X-Embodiment提升30%，在复杂灵巧任务上成功率超60%，较先前RDT方法提升32%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.06669" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>