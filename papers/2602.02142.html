<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.02142" target="_blank" rel="noreferrer">2602.02142</a></span>
        <span>作者: Zhao, Ruiteng, Wang, Wenshuo, Ma, Yicheng, Li, Xiaocong, Tay, Francis E. H., Ang Jr., Marcelo H., Zhu, Haiyue</span>
        <span>日期: 2026/02/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为机器人模仿学习的主导范式，它能够将感知、推理和端到端控制统一起来。然而，纯视觉策略在现实部署中存在根本性局限：遮挡、光照变化、深度模糊以及细微的接触效应在图像中难以捕捉。在众多可能的多模态中，力/触觉感知对于VLA模型至关重要但尚未充分探索，因为它提供了关于接触动态的直接信息，而这些信息通常是视觉不可见的。</p>
<p>现有的整合力感知的VLA方法主要分为两类。一类是Tactile-VLA，它将原始力信号编码为紧凑的潜在嵌入，并通过跨模态对齐层与预训练的视觉-语言特征融合。这种方法存在干扰预训练VLM精心对齐的视觉-语言语义的风险。另一类是ForceVLA，它将力信号编码为一个专用令牌，在VLM之后注入并通过一个力感知的专家混合模块进行融合。虽然有效，但其MoE设计可能增加训练不稳定性和推理复杂性，且后期融合机制限制了细粒度的力-视觉-状态交互。</p>
<p>本文针对上述痛点，提出了一种新的视角：通过<strong>力蒸馏</strong>来整合力感知，而非直接使用传感器信号。核心思路是：设计一个力蒸馏模块，在训练阶段利用真实力信号进行监督，学习从视觉和机器人状态预测一个紧凑的力表征令牌；在推理阶段，将此预测的力令牌注入预训练VLM，从而实现无需物理力传感器的力感知推理。</p>
<h2 id="方法详解">方法详解</h2>
<p>FD-VLA的整体框架旨在将力感知整合到VLA中，同时保持预训练VLM的语义完整性，并实现无需传感器的部署。</p>
<p><img src="https://arxiv.org/html/2602.02142v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：FD-VLA框架总览。<strong>训练阶段</strong>：真实力信号通过轻量级投影编码为实际力令牌，作为监督信号。一个可学习的查询令牌在视觉和状态令牌的上下文中，通过注意力机制预测出力令牌，并与实际力令牌对齐。<strong>推理阶段</strong>：模型仅从视觉和状态输入合成预测的力令牌，无需力传感器。所有令牌（语言、图像、状态、预测力）在预训练VLM内融合，最后由动作专家生成动作序列。</p>
</blockquote>
<p><strong>整体流程</strong>：输入包括语言指令 $L$、视觉观测 $V_t$、机器人状态 $S_t$ 和力信号 $F_t$。视觉、语言和状态首先被编码为特征 $f_t^V$, $f^L$, $f_t^S$。<strong>力蒸馏模块</strong> 接收 $f_t^V$ 和 $f_t^S$，预测出力令牌 $f_t^{pF}$。所有令牌被拼接为 $f_t^{fs} = [f_t^V, f^L, f_t^S, f_t^{pF}]$，送入<strong>预训练VLM</strong>进行多模态推理。VLM的参数被冻结，通过<strong>方向性注意力掩码</strong>保护其语义流。最后，<strong>动作专家</strong> 根据VLM输出的融合特征预测未来 $H$ 步的动作序列 $[a_t, ..., a_{t+H-1}]$。</p>
<p><strong>核心模块1：力蒸馏模块</strong><br>FDM是本文的核心创新，其关键思想是力信息并非孤立测量，而是与机器人的本体状态和视觉操作上下文内在耦合。FDM通过从这些现有模态中蒸馏出力表征，消除了对额外物理传感器的需求，并学习了去噪的、任务相关的力代理。</p>
<ul>
<li><strong>实际力分支</strong>：训练时，原始力测量值通过一个轻量级MLP投影为实际力令牌 $f_t^{aF}$，作为监督信号。</li>
<li><strong>预测力分支</strong>：将力潜在表征的生成视为一个以视觉-状态上下文为条件的检索问题。定义一个上下文矩阵 $C_t = [p, f_t^V, f_t^S]$，其中 $p$ 是一个可学习的查询令牌。使用单查询多头注意力机制，$p$ 作为查询 $Q$，$C_t$ 提供键 $K$ 和值 $V$。注意力输出经过层归一化和前馈网络后，得到预测的力令牌 $f_t^{pF}$。将查询令牌 $p$ 包含在键值集中，实现了通过残差路径的自我调节。</li>
<li><strong>蒸馏损失</strong>：通过 $L_{dist} = || f_t^{pF} - f_t^{aF} ||_2^2$ 对齐预测力令牌和实际力令牌。</li>
</ul>
<p><strong>核心模块2：方向性注意力掩码机制</strong><br>为了将状态和力令牌注入冻结的预训练VLM而不破坏其语义先验，采用了方向性注意力掩码。输入令牌被分为<strong>感知流</strong>（视觉 $f^V$ 和语言 $f^L$）和<strong>控制流</strong>（状态 $f^S$ 和预测力 $f^{pF}$）。掩码 $M$ 的规则确保：1）感知流令牌只能相互关注，保持预训练对齐；2）控制流令牌可以关注感知流令牌，以融合感知上下文；3）控制流令牌内部采用因果自注意力（只能关注自身及之前的控制流令牌）。这强制了单向信息流，保护了语义，同时允许控制流整合多模态信息用于动作预测。</p>
<p><strong>核心模块3：动作专家与训练目标</strong><br>动作专家 $\pi_{\theta}$ 是一个基于Transformer的策略头，使用<strong>条件流匹配</strong>目标进行训练，预测动作块 $A_t$。整体训练目标结合了策略学习损失和力蒸馏损失：$L = L^{\tau}(\theta) + \lambda L_{dist}$，其中 $\lambda$ 是力监督的权重。</p>
<p><strong>创新点总结</strong>：</p>
<ol>
<li><strong>力蒸馏而非力传感</strong>：通过FDM从视觉和状态预测力表征，推理时无需物理传感器。</li>
<li><strong>早期跨模态融合</strong>：FDM在输入VLM之前就实现了力-视觉-状态的融合，改善了跨模态对齐。</li>
<li><strong>保护性融合机制</strong>：通过方向性注意力掩码，在向VLM注入新模态时严格保护其预训练的视觉-语言语义。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>平台与数据</strong>：使用UR5e机械臂、Azure Kinect相机和RealSense D405夹爪相机。通过遥操作收集了3个接触丰富任务的各50条专家演示，并进行了各30次的独立测试。</li>
<li><strong>任务</strong>：1) <strong>清洁白板</strong>（持续接触）；2) <strong>按下紧急按钮</strong>（精确垂直按压）；3) <strong>插入插头</strong>（精确对准与力控）。</li>
<li><strong>Baseline方法</strong>：DP3（扩散策略）、$\pi_0$（大型VLA）、SmolVLA（紧凑型VLA，作为本文基础模型）。后两者还评估了直接使用原始力输入的版本。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.02142v1/x6.png" alt="关键实验结果"></p>
<blockquote>
<p><strong>图6</strong>：三个接触丰富操作任务的成功率对比。FD-VLA在全部三个任务上取得了最高的成功率（平均61.1%），显著优于所有基线，包括参数规模大十倍的 $\pi_0$。即使基线模型使用了原始力输入，其性能仍低于FD-VLA，这凸显了力蒸馏方法的有效性。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li>FD-VLA在<strong>清洁白板</strong>、<strong>按下按钮</strong>、<strong>插入插头</strong>任务上的成功率分别为73.3%、70.0%、40.0%，**平均成功率为61.1%**。</li>
<li>对比基线：显著优于SmolVLA（无力：23.3%；有力：38.9%）、DP3（11.1%）和 $\pi_0$（无力：46.7%；有力：66.7%）。值得注意的是，FD-VLA在参数量远小于 $\pi_0$ 的情况下，性能仍优于使用了原始力输入的 $\pi_0$（66.7% vs 61.1%？此处需注意：图6显示 $\pi_0$ with force 在Press任务为80%，Plug任务为30%，Clean任务为80%，平均约为63.3%，文本描述可能存在细微出入，但FD-VLA整体表现更均衡且无需传感器）。</li>
<li><strong>核心发现</strong>：直接提供原始力信号不足以获得最优性能，而FD-VLA通过预测的、融合的力表征取得了更好或相当的结果，同时实现了<strong>传感器无关的部署</strong>。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.02142v1/x7.png" alt="消融实验表"></p>
<blockquote>
<p><strong>表I</strong>：力蒸馏模块的消融研究。结果表明：1) 使用FDM（无论令牌类型）均优于直接使用MLP编码力（w/o FDM），证明了早期深度跨模态交互的重要性；2) 使用可学习令牌的FDM（本文方法）性能最佳（61.1%），优于使用从真实力编码的固定令牌的变体（51.1%），说明可学习令牌能更好地避免原始力信号的噪声和高维问题，同时保留物理监督的益处。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>消融实验验证了FDM设计及其可学习查询令牌的有效性。移除FDM（直接用MLP编码力）性能最差（平均38.9%），表明简单的投影无法实现良好的跨模态交互。将FDM中的可学习查询令牌替换为从真实力编码的固定令牌，性能下降至51.1%，说明直接处理原始力特征存在挑战。而本文提出的<strong>可学习查询令牌结合注意力机制</strong>的方法取得了最佳性能（61.1%），证明了其能够学习去噪的、任务相关的力代理，并促进更好的跨模态对齐。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了FD-VLA框架</strong>：通过注入一个<strong>蒸馏的力令牌</strong>来增强VLA模型在接触丰富操作中的性能，实现了力感知推理而无需物理力传感器。</li>
<li><strong>设计了力蒸馏模块</strong>：利用注意力机制，以视觉和状态为条件，从一个可学习查询令牌中蒸馏出预测的力表征，并在训练中与真实力信号对齐。</li>
<li><strong>实现了实用的跨模态融合</strong>：FDM提供了力-视觉-状态的早期融合，改善了跨模态一致性；结合方向性注意力掩码，在丰富动作表征的同时保护了预训练VLM的语义完整性。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，在<strong>视觉泛化</strong>方面（如新颖背景、照明条件）的测试表明性能有所下降（图7及相关文本），这指出了当前方法在极端视觉变化下的局限性。力蒸馏的准确性可能依赖于训练数据所覆盖的接触场景范围。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>模态蒸馏范式</strong>：本文展示了通过“蒸馏”而非“直接传感”来整合关键但昂贵/脆弱模态（如力、触觉）的有效性，这一范式可扩展至其他机器人感知模态。</li>
<li><strong>保护性架构设计</strong>：方向性注意力掩码机制为如何将新模态安全地集成到大规模预训练基础模型中提供了参考，平衡了新功能引入与原始知识保留。</li>
<li><strong>迈向更鲁棒的多模态VLA</strong>：工作凸显了紧密跨模态对齐对于接触场景中感知-动作耦合的重要性。未来的研究可以探索更先进的蒸馏目标或融合架构，以进一步提升对视觉变化和未见接触模式的鲁棒性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出FD-VLA模型，旨在解决视觉-语言-动作框架在接触式操作任务中缺乏力感知能力的问题。核心方法是设计力蒸馏模块，通过视觉观测和机器人状态预测力令牌，并将其注入预训练的视觉语言模型中，实现无需物理力传感器的力感知推理。实验表明，蒸馏出的力令牌性能优于直接使用传感器测量的力信号及其他基线方法，验证了该框架在提升接触式操作感知与行动鲁棒性方面的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.02142" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>