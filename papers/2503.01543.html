<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Exo-ViHa: A Cross-Platform Exoskeleton System with Visual and Haptic Feedback for Efficient Dexterous Skill Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Exo-ViHa: A Cross-Platform Exoskeleton System with Visual and Haptic Feedback for Efficient Dexterous Skill Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.01543" target="_blank" rel="noreferrer">2503.01543</a></span>
        <span>作者: Chao, Xintao, Mu, Shilong, Liu, Yushan, Li, Shoujie, Lyu, Chuqiao, Zhang, Xiao-Ping, Ding, Wenbo</span>
        <span>日期: 2025/03/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习已成为机器人技能学习的有力范式，但其效果严重依赖于高质量、大规模的演示数据。目前主流的灵巧操作数据收集方法主要依赖于遥操作，例如虚拟现实设备、手持控制器或基于视觉的遥操作。这些方法虽然在一定程度上提高了操作的灵活性，但普遍存在关键局限性：缺乏低延迟的第一人称视觉反馈、缺少触觉反馈，以及数据收集阶段与机器人实际部署阶段之间存在不一致性。这些局限可能导致数据收集效率低下，或在真实部署时模型性能下降，从而降低模仿学习的泛化能力。</p>
<p>本文针对传统数据收集系统在采集效率、一致性和准确性之间难以平衡的痛点，提出了一种新的外骨骼设备视角。核心思路是设计一个3D打印的模块化外骨骼系统，让用户能够以第一人称视角收集数据，同时提供实时的触觉反馈，并通过硬件设计确保数据收集与部署阶段的视觉和身体姿态一致性，从而高效获取高质量、可用于模仿学习的灵巧操作数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>Exo-ViHa系统的整体框架包含硬件设计和学习框架两部分。在数据收集阶段，用户佩戴外骨骼和运动捕捉手套，系统同步采集末端执行器位姿、灵巧手运动数据和多视角视觉数据。这些数据随后用于训练模仿学习模型（ACT），模型的输出直接用于控制机械臂和灵巧手执行任务。</p>
<p><img src="https://arxiv.org/html/2503.01543v1/x4.png" alt="方法框架"></p>
<blockquote>
<p><strong>图4</strong>：Exo-ViHa数据收集与部署的工作流程。收集的数据包括末端执行器状态（6D位姿）和相机帧（外部相机和腕戴相机）。训练后，模型输出直接控制机器人。</p>
</blockquote>
<p>系统的核心模块是3D打印的外骨骼硬件。它由以下几个关键部分组成：</p>
<ol>
<li><strong>外骨骼结构</strong>：使用聚乳酸材料3D打印，总重约425克，通过魔术贴绑带固定在用户前臂。其设计确保外部相机只能看到灵巧手，而看不到用户的手。</li>
<li><strong>传感模块</strong>：<ul>
<li><strong>Intel RealSense T265 SLAM相机</strong>：安装在外骨骼的相机底座上，用于实时追踪和记录末端执行器（外骨骼）的6D位姿数据。</li>
<li><strong>腕戴相机</strong>：同样安装在相机底座上，提供第一人称视角的视觉信息。</li>
<li><strong>运动捕捉手套</strong>：用户佩戴，用于获取用户手的实时姿态信息，并映射到灵巧手上。</li>
<li><strong>外部相机</strong>：在环境中布置两个第三方视角相机，用于全面捕捉环境信息。</li>
</ul>
</li>
<li><strong>适配与补偿机制</strong>：系统通过3D打印特定的连接器，可以适配多种不同的灵巧手或二指夹爪。为补偿加装灵巧手后增加的重量（总重约1100克），系统使用了弹性带提供部分重力补偿，减轻用户手臂负担。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.01543v1/x2.png" alt="硬件细节"></p>
<blockquote>
<p><strong>图2</strong>：外骨骼系统细节。(a) 系统各组件分解图。(b) 外骨骼佩戴演示。</p>
</blockquote>
<p>与现有方法相比，Exo-ViHa的创新点具体体现在四个维度：</p>
<ul>
<li><strong>第一人称视觉反馈</strong>：用户通过自己的眼睛直接观察工作空间，操作自然，增强了任务真实感和操作准确性。</li>
<li><strong>触觉反馈</strong>：当灵巧手与物体接触时，用户通过前臂能明显感知到阻力，这种力反馈有助于在涉及多物体接触的任务中进行力调整。</li>
<li><strong>视觉一致性</strong>：相机底座同时安装在外骨骼和机器人机械臂上。只要末端执行器位姿一致，数据收集阶段和部署阶段腕戴相机获取的视觉信息几乎相同，减少了部署时的视觉差异。</li>
<li><strong>身体一致性</strong>：系统直接采集执行任务时灵巧手本身的运动数据，而非间接映射或转换的数据，确保了部署结果更可靠且与演示条件高度一致。</li>
</ul>
<p><img src="https://arxiv.org/html/2503.01543v1/x3.png" alt="系统演示"></p>
<blockquote>
<p><strong>图3</strong>：Exo-ViHa系统细节。(a) 数据收集演示及第一人称视角与触觉反馈示意图。(b) 工作空间范围示意。(c) 视觉一致性原理：在收集和部署阶段，只要末端位姿一致，腕戴相机信息就基本一致。(d) 背包供电设计支持户外数据收集。</p>
</blockquote>
<p>在学习框架中，SLAM相机记录的位姿通过校准矩阵与目标机械臂的实际末端位姿对齐。视觉输入包括两个第三方视角相机和一个腕戴相机的RGB流。作者选择使用动作分块变换器模型进行训练，该模型能确保灵巧手高效、平滑地执行预定任务。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在LeRobot平台上进行，使用NVIDIA RTX 4090 GPU进行训练，共进行160,000个离线训练步。实验从数据收集效率和真实机器人操作性能两个方面评估系统优势。</p>
<p>对比的基线方法包括<strong>人类徒手操作</strong>和<strong>传统遥操作</strong>。评估的任务包括擦白板、拾放物体、分类瓶子和锤击操作。</p>
<p>关键实验结果如下：在数据收集效率方面，对于擦白板这类多接触操作任务，Exo-ViHa的完成时间仅比人类徒手操作慢几秒，而传统遥操作由于缺乏第一人称视角来观察物体接触，经常出现问题，导致任务完成时间大幅增加。在简单的拾放任务中，Exo-ViHa的耗时仅比人类操作长8秒，而遥操作的耗时几乎翻倍。</p>
<p><img src="https://arxiv.org/html/2503.01543v1/x5.png" alt="效率对比"></p>
<blockquote>
<p><strong>图5</strong>：不同场景下的数据收集效率比较。(a-b) 擦白板任务对比分析：(a) 人类动作、Exo-ViHa和遥操作在多接触交互场景下的演示；(b) 各方法的时间效率定量比较。(c) 拾放场景中遥操作、Exo-ViHa和人类动作的执行过程对比。</p>
</blockquote>
<p>系统在多项任务上进行了30轮测试，具体性能指标如表II所示。在部署阶段，使用Piper机械臂和Inspire灵巧手执行任务，各任务的平均成功率约为80%。</p>
<p><img src="https://arxiv.org/html/2503.01543v1/x6.png" alt="部署结果"></p>
<blockquote>
<p><strong>图6</strong>：各种机器人任务的部署示例。(a) 拾放：抓取不同形状的多个物体。(b) 擦拭：握住白板擦擦除字迹。(c) 分类：根据类型对瓶子进行分类。(d) 锤击：握住锤子敲击指定位置。</p>
</blockquote>
<p><strong>表II：使用Exo-ViHa系统进行数据收集和机器人部署时，机器人操作任务的性能指标（平均时间和成功率）。</strong></p>
<table>
<thead>
<tr>
<th align="left">任务</th>
<th align="left">数据收集平均时间 (s)</th>
<th align="left">部署成功次数/试验次数</th>
<th align="left">部署成功率</th>
</tr>
</thead>
<tbody><tr>
<td align="left">拾放物体</td>
<td align="left">4.8 ± 0.9</td>
<td align="left">29/30</td>
<td align="left">86.6%</td>
</tr>
<tr>
<td align="left">分类六个瓶子</td>
<td align="left">41.8 ± 7.8</td>
<td align="left">27/30</td>
<td align="left">76.6%</td>
</tr>
<tr>
<td align="left">锤击操作</td>
<td align="left">12.4 ± 3.3</td>
<td align="left">28/30</td>
<td align="left">83.3%</td>
</tr>
<tr>
<td align="left">擦白板</td>
<td align="left">12.9 ± 2.1</td>
<td align="left">27/30</td>
<td align="left">80.0%</td>
</tr>
</tbody></table>
<p>消融实验主要体现在系统设计带来的效率与成功率提升上。实验结果表明，Exo-ViHa系统在几乎所有测试任务中都减少了执行时间并提高了任务完成率。第一人称视觉和触觉反馈直接贡献于高效、高质量的数据收集，而视觉和身体一致性则直接提升了部署阶段的成功率。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了一种新颖的、平衡的数据收集解决方案</strong>：设计并实现了Exo-ViHa，一个3D打印的跨平台外骨骼系统，在数据收集效率、一致性和准确性之间取得了良好平衡。</li>
<li><strong>提供了沉浸式的多模态反馈</strong>：系统通过硬件设计，使用户在数据收集过程中能获得真实的第一人称视觉反馈和直接的触觉感受，显著提升了演示数据的质量。</li>
<li><strong>实现了高度的部署一致性</strong>：通过相机底座复用和直接采集灵巧手数据，确保了数据收集与机器人部署阶段在视觉和身体姿态上的高度一致性，降低了模仿学习中的领域差异。</li>
</ol>
<p>论文自身提到的局限性包括：垂直方向运动范围受连接线长度限制（约120°）；尽管有弹性带补偿，佩戴灵巧手后整体重量仍对用户手臂造成一定负担；数据收集过程相对耗时。</p>
<p>这项工作对后续研究的启示在于：为高效获取机器人模仿学习数据提供了一种低成本、高兼容性的硬件系统新范式。未来的工作可以专注于进一步优化外骨骼结构以提供更大的工作空间和舒适度，改进重力补偿机制，并在更广泛的真实世界协作任务中评估系统的实用性和有效性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对灵巧操作模仿学习中，传统数据采集系统在效率、一致性和准确性上难以平衡的问题，提出了Exo-ViHa系统。该系统是一个3D打印的模块化外骨骼，集成了SLAM相机、动作捕捉手套和腕戴相机，提供第一人称视觉与实时触觉反馈，并能兼容多种机械臂与灵巧手。实验表明，该系统能显著提升灵巧操作任务数据采集的成功率与效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.01543" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>