<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.02310" target="_blank" rel="noreferrer">2503.02310</a></span>
        <span>作者: Song, Wenxuan, Chen, Jiayi, Ding, Pengxiang, Zhao, Han, Zhao, Wei, Zhong, Zhide, Ge, Zongyuan, Ma, Jun, Li, Haoang</span>
        <span>日期: 2025/03/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过整合动作分块技术，显著提升了在机器人操控任务中的性能。动作分块通过一次性预测并执行多步动作序列，增强了动作的一致性和稳定性。然而，该技术也线性地增加了单次推理需要生成的动作维度（对于7自由度机械臂，分块大小为m时，动作序列维度为7m）。当VLA模型采用自回归解码时，其生成时间与预测的令牌长度成正比，导致推理效率严重下降，限制了控制频率和任务执行效果。因此，加速集成动作分块的VLA模型成为迫切需求。</p>
<p>本文针对自回归解码在生成长动作序列时的效率瓶颈，提出了并行解码的新视角。其核心思路是将自回归动作解码过程重新表述为一个非线性方程组，并通过并行定点迭代方法（如Jacobi迭代）求解，从而在保持模型性能的同时，显著提升解码速度。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法称为PD-VLA，其整体框架基于一个基础的VLA模型（文中基于LLaVA构建），并整合了动作分块与并行解码技术。流程如下：输入包括静态场景图像、夹爪图像和语言指令；视觉编码器将图像编码为视觉令牌，与文本令牌一同输入大型语言模型；LLM以并行方式输出动作令牌；最后将动作令牌反令牌化为有效的动作值并部署到机械臂上。</p>
<p><img src="https://arxiv.org/html/2503.02310v1/extracted/6250428/model.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：PD-VLA网络架构（分块大小为m）。给定图像和语言指令，方法首先对输入进行令牌化，然后以并行解码方式将结果馈入LLM。LLM输出动作令牌，最终被反令牌化为有效动作值并部署到机械臂上。</p>
</blockquote>
<p>核心模块包括基础VLA模型、动作分块集成以及并行解码算法。</p>
<ol>
<li><strong>基础VLA模型</strong>：基于LLaVA架构，包含视觉编码器和LLM。它将两幅图像（静态场景和夹爪）与文本指令作为输入，通过自回归解码生成7维动作（3维位置、3维旋转、1维夹爪状态）。动作被离散化为256个桶，并使用LLM词汇表中256个最不常用的令牌表示。</li>
<li><strong>动作分块集成</strong>：在时间步t，给定分块大小m，模型预测一个动作序列A_t = [a_t, a_t+1, ..., a_t+m-1]。这扩展了有效动作视野，有助于捕捉时间依赖性，但增加了需生成的令牌数量（l = 7m + 2，含起始和结束令牌），加剧了自回归解码的延迟。</li>
<li><strong>并行解码</strong>：这是方法的创新核心。传统自回归解码顺序预测每个令牌，而PD-VLA采用基于Jacobi解码的并行定点迭代。<ul>
<li><strong>理论重构</strong>：将自回归解码的贪婪策略（y_i = argmax p(y | Y_i, x)）重构为求解非线性方程组 f(y_i, Y_i, x) = 0。</li>
<li><strong>并行迭代</strong>：打破顺序依赖，将因果注意力机制替换为双向注意力。首先随机初始化一个长度为解码视野n的动作令牌序列Y^(0)。在每次迭代j中，所有令牌y_i^(j+1)均基于<strong>同一</strong>上下文（即上一次迭代的完整序列Y^(j)和提示x）并行更新：y_i^(j+1) = argmax p(y | Y^(j), x)。迭代在序列收敛至固定点（Y^(k) = Y^(k-1)）时终止。</li>
<li><strong>解码视野选择</strong>：解码视野n的选择影响并行模式。文中探讨了n=7（单动作维度）、n=16（2的幂次）和n=l（总序列长度）等设置，发现n=l能使模型更好地继承原始动作建模能力，并在一次Jacobi解码中完成推理。</li>
<li><strong>加速现象</strong>：并行解码中会出现“固定令牌”现象，即某些令牌（如只有开/关两种状态的夹爪令牌）会提前预测正确并保持不变。固定令牌的存在有助于正确令牌的快速扩展，从而加速收敛。</li>
</ul>
</li>
</ol>
<p>与现有加速方法（如模型重设计、量化、令牌修剪）相比，PD-VLA的创新点在于<strong>仅优化解码机制</strong>，无需重新设计模型架构、无需额外训练、也无需修改预训练模型，实现了部署友好的训练免费加速。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在CALVIN仿真基准（ABCD-&gt;D设置）和真实机器人平台上进行。评估指标包括各子任务成功率、平均完成长度、推理速度（令牌/秒）和执行频率（Hz）。对比的基线方法包括MCIL、HULC、RT-1以及基础LLaVA-VLA模型。</p>
<p><img src="https://arxiv.org/html/2503.02310v1/extracted/6250428/teaser.jpg" alt="仿真结果对比"></p>
<blockquote>
<p><strong>图1</strong>：提出的并行解码（左）与传统自回归解码（右）对比。自回归解码顺序预测动作令牌，而并行解码同时预测所有令牌。</p>
</blockquote>
<p>关键实验结果如下表所示，PD-VLA在成功率上大幅超越基础LLaVA-VLA模型，并与先进基线模型竞争，同时实现了显著的加速。</p>
<p><img src="https://arxiv.org/html/2503.02310v1/extracted/6250428/setup.jpg" alt="结果表格"></p>
<blockquote>
<p><strong>表II</strong>：与各种操作基线在成功率和平均完成长度上的比较。PD-VLA在5/5任务上达到50.5%的成功率，平均长度3.55，表现最佳。</p>
</blockquote>
<p>消融实验验证了各组件贡献：<br><img src="https://arxiv.org/html/2503.02310v1/extracted/6250428/speed.jpg" alt="消融研究"></p>
<blockquote>
<p><strong>表III</strong>：消融研究。移除动作分块或并行解码均会导致性能下降。将PD替换为其他训练免费加速方法（FastV, SparseVLM）会损害速度或性能。PD-VLA实现了52.84 tokens/s的速度和4.56 Hz的执行频率。</p>
</blockquote>
<p>实验表明：1) PD-VLA在保持竞争力的成功率（例如，在顺序完成5个子任务上达到50.5%）的同时，执行频率达到基础VLA模型的2.52倍（4.56 Hz vs 1.81 Hz）。2) 动作分块和并行解码均是提升性能和速度的关键组件。3) 与其他训练免费的VLM加速方法（FastV, SparseVLM）相比，PD-VLA是唯一能同时提升成功率和推理速度的方法。4) 解码视野n设置为总序列长度（n=l）时效果最有效。</p>
<p><img src="https://arxiv.org/html/2503.02310v1/extracted/6250428/real.jpg" alt="真实世界实验"></p>
<blockquote>
<p><strong>图3</strong>：真实世界实验验证了PD-VLA在不同任务中的高适用性，例如倒水等灵巧任务。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) <strong>首次提出了用于集成动作分块的VLA模型的并行解码框架</strong>（PD-VLA），将自回归解码重构为并行定点迭代问题，在数学保证下保持性能并显著加速。2) <strong>设计了仅针对解码过程的加速策略</strong>，无需模型重设计、训练或修改，部署友好，并能与其他加速技术无缝协同。3) <strong>在仿真和真实世界平台上进行了全面的实证验证</strong>，包括消融研究阐明了性能权衡。</p>
<p>论文提到的局限性主要隐含在方法原理中：并行解码的收敛步数k可能随任务和初始化变化，理论上k≤n，但最坏情况可能仍需n步。此外，方法依赖于随机初始化序列，其质量可能影响收敛速度。</p>
<p>本研究对后续工作的启示在于：为提升VLA模型实时性提供了新的、低成本的解码层优化思路。这种并行迭代范式可与其他系统级优化（如模型压缩、硬件加速）结合。同时，“固定令牌”现象表明，利用动作序列的结构化先验（如某些维度变化缓慢或离散）可能进一步指导解码策略的设计，以实现更高效的收敛。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对集成动作分块的视觉-语言-动作模型因动作维度线性增加导致推理效率下降的问题，提出首个并行解码框架PD-VLA。该方法将自回归解码重构为通过并行固定点迭代求解的非线性系统，无需改变模型结构或重新训练即可实现加速。实验表明，PD-VLA在保持任务成功率竞争力的同时，在7自由度机械臂上实现了2.52倍的执行频率提升，验证了其有效性与实用性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.02310" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>