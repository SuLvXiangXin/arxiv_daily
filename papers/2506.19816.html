<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.19816" target="_blank" rel="noreferrer">2506.19816</a></span>
        <span>作者: Li, Hao, Yang, Shuai, Chen, Yilun, Chen, Xinyi, Yang, Xiaoda, Tian, Yang, Wang, Hanqing, Wang, Tai, Lin, Dahua, Zhao, Feng, Pang, Jiangmiao</span>
        <span>日期: 2025/06/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉-语言-动作模型通常基于预训练的视觉-语言模型构建，通过将连续动作离散化为令牌并进行自回归预测，来适应大规模异构操作数据集。然而，这些模型受限于视觉-语言模型的单帧图像范式，无法充分利用多帧历史观察提供的时序信息。低层策略已证明利用多帧信息能显著提升性能，提供运动线索以消除状态歧义，并在当前输入受损时依靠先前一致的观察可靠推断动作，展现出强大的观测鲁棒性。但直接将多帧历史图像输入VLA模型会带来两大挑战：VLM主干中的自注意力计算与输入令牌数量呈二次方增长，使得大规模具身预训练计算成本高昂；冗余的视觉令牌会显著降低推理速度，限制实际部署的可行性。现有如RoboVLMs等工作试图通过基于记忆的LSTM从头开始训练具身能力来扩展到多帧范式，但忽略了高效适应单帧预训练模型的潜力，也缺乏对此类多帧能力带来性能增益的定量评估。</p>
<p>本文针对高效多帧建模的痛点，提出了一个将单帧VLA模型扩展至多帧范式的统一框架。其核心思路是采用两阶段流程：首先在大规模具身数据集上进行单帧预训练，建立有效的具身视觉-语言基础；随后通过多帧后训练，将主干预测从离散令牌转向可学习特征，并通过特征分块聚合历史信息，从而在提升性能和鲁棒性的同时解决计算效率问题。</p>
<h2 id="方法详解">方法详解</h2>
<p>CronusVLA框架遵循两阶段流程：单帧预训练与多帧后训练。其整体目标是建立一个既能保留单帧感知能力，又能高效利用多帧时序信息的模型。</p>
<p><img src="https://arxiv.org/html/2506.19816v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：CronusVLA框架总览。(a) 展示了基础单帧VLA的单帧预训练过程。通过复制模型权重，进行(b)所示的多帧后训练，其中通过跨帧解码器聚合来自前面若干帧的可学习特征来实现多帧建模。(c) 展示了在特征分块上使用队列机制实现快速推理。(d) 详细说明了跨帧解码器的结构。</p>
</blockquote>
<p><strong>1. 单帧预训练</strong><br>如框架图(a)部分所示，此阶段目标是建立一个基础的单帧VLA模型。该模型适配现有的预训练VLM，在大规模操作演示数据集上进行训练。具体而言，连续动作通过扩展的动作分词器被映射到256个离散区间，转化为离散动作令牌。模型以当前观察图像和语言指令为输入，采用标准的下一令牌预测目标进行自回归训练，预测下一步的动作令牌并反解译为连续动作：<code>a_t = VLA(I_t, l)</code>。此阶段有效将视觉编码器的感知能力迁移到具身场景，为后续多帧训练奠定了有效的视觉-语言基础，同时相比直接进行多帧预训练，保留了VLM的单帧视觉感知能力且训练成本更低。</p>
<p><strong>2. 多帧后训练</strong><br>此阶段是框架的核心创新，旨在将基础的单帧VLA模型适配到多帧范式，其流程如图(b)和(d)所示。</p>
<ul>
<li><strong>从离散动作令牌到特征分块</strong>：在多帧后训练中，模型不再生成离散动作令牌，而是在主干网络的隐藏层引入可学习的连续特征<code>f_t ∈ R^d</code>，作为当前帧的视觉-语言信息摘要，即<code>f_t = VL(I_t, l)</code>。所有图像仍以单帧形式由视觉-语言主干编码，确保与标准VLM范式的兼容性。为了建立帧间关联，引入了<strong>特征分块</strong> <code>F_t^M = {f_{t-M+1}, …, f_{t-1}, f_t}</code>，它代表了在特征层面的M步多帧图像信息。训练时，在批次级别重组M步图像输入，使主干能独立处理每批<code>B×M</code>个单帧输入。推理时，如图(c)所示，采用先进先出的队列机制维护特征分块，通过重用先前的视觉-语言计算实现快速推理。</li>
<li><strong>跨帧解码器</strong>：该解码器负责解码嵌入在特征分块<code>F_t^M</code>中的多帧信息，预测未来K步的动作分块：<code>a_{t:t+K-1} = Decoder(F_t^M)</code>。解码器基于DiT构建，包含自注意力网络和MLP层，并使用扩散损失进行训练。为了平衡当前帧与过去帧可学习特征的贡献，论文设计了<strong>特征调制器</strong>。具体而言，当前特征<code>f_t</code>通过通道分割操作<code>DIV</code>进行划分，以匹配过去特征<code>f_{t-M+1:t-1}</code>的数量，然后通过调制器<code>MD</code>（一个MLP）进行处理，得到调制后的特征<code>Z_f</code>。随后，采用交叉注意力机制处理带噪声的动作和调制特征，其中<code>Z_f</code>被映射为键和值，带噪声的动作<code>a_hat</code>作为查询。在推理时，带噪声的动作在<code>Z_f</code>的条件下迭代去噪，得到最终的动作输出。</li>
<li><strong>带多帧正则化的后训练</strong>：为了将视觉-语言主干与解码器的多帧建模解耦，确保主干的训练逻辑与单帧范式保持一致，论文引入了<strong>多帧正则化</strong>。具体做法是，将特征分块<code>F_t^M</code>中过去的可学习特征<code>f_{t-M+1:t-1}</code>作为解码器的辅助输入，并通过停止梯度操作限制其梯度流不更新视觉-语言主干。这些过去特征仅作为正则化项促进训练，其计算如公式(3)和(4)所示。这种方法有两个优点：一是提取过去特征时不计算梯度，降低了计算和内存开销，实现了高效训练；二是基于单帧更新，保留了预训练的单帧感知能力并促进了更快的收敛。</li>
</ul>
<p>与现有方法相比，CronusVLA的创新点具体体现在：1) 提出了一种从单帧离散预训练模型向多帧连续预测范式迁移的高效路径，而非从头训练；2) 引入了“特征分块”的概念，在特征层面而非图像或令牌层面进行时序聚合，大幅降低了计算复杂度；3) 通过特征调制器和多帧正则化等设计，在集成多帧信息的同时，精心保留了主干网络的单帧感知能力与训练效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>论文在模拟和真实世界环境中进行了广泛实验，使用了三个主要的基准测试平台：SimplerEnv、LIBERO以及新提出的SimplerEnv-OR。对比的基线方法涵盖了早期模型（RT-1-X, RT-2-X, Octo-Base）、当前主流VLA模型（OpenVLA, CogACT, Magma, SpatialVLA, TraceVLA）以及其他采用额外状态输入的策略（π0, π0-FAST, GR00T-N1.5）。CronusVLA模型基于Llama2-7B和Qwen2.5-0.5B语言主干构建，视觉编码器使用Dinov2和SigLip。</p>
<p><strong>关键实验结果总结如下：</strong></p>
<ol>
<li><strong>在SimplerEnv上的性能</strong>：如表1所示，CronusVLA-7B在Google Robot设置中取得了最高的平均成功率（VM: 78.6%, VA: 73.8%），相对于同为多帧VLA的TraceVLA和RoboVLMs有显著提升（相对VM分数分别+71.6%和+37.9%）。特别是在长视野任务“Put in Drawer”上，成功率达到VM 64.8%和VA 65.1%，而多数先前方法在此任务上表现不佳。CronusVLA-0.5B虽参数量小，但在部分任务上超越了众多更大模型，表明其建模的有效性。</li>
<li><strong>在LIBERO上的性能</strong>：如表2所示，CronusVLA-7B仅使用额外的腕部视角图像输入，便取得了97.0%的平均成功率，在所有任务套件中均表现优异，尤其在“Long”套件上达到94.0%，相比OpenVLA有+40.3%的绝对提升，展示了强大的域内学习和长视野执行能力。</li>
<li><strong>在SimplerEnv-OR上的鲁棒性</strong>：论文新提出的SimplerEnv-OR基准包含时空维度的24类观测干扰。如表3所示，在时间维度上，随着干扰频率降低，所有模型的鲁棒性评分提升，但CronusVLA凭借有效的多帧建模，在恒定干扰下表现出强抵抗力，在稀疏干扰下近乎免疫（R-Score达96.2）。在空间维度上，CronusVLA在全球和局部干扰下均 consistently 优于其他模型，展现出最强的综合鲁棒性。值得注意的是，尽管SpatialVLA在原始SimplerEnv上性能优于RoboVLMs，但在SimplerEnv-OR上其鲁棒性得分更低，这凸显了多帧模型在鲁棒性上的优势。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.19816v2/x3.png" alt="鲁棒性基准说明"></p>
<blockquote>
<p><strong>图3</strong>：SimplerEnv-OR基准示意图。该基准通过模拟相机的不同观测干扰（包括空间和时间维度），对VLA模型的观测鲁棒性进行定量评估。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.19816v2/x4.png" alt="真实世界实验"></p>
<blockquote>
<p><strong>图4</strong>：真实世界实验评估。(a) 评估基础的抓取放置能力，(b) 展示多帧建模在处理时序依赖操作的长视野任务中的优势，(c) 在相机遮挡和各种干扰下的泛化与鲁棒性测试，突出了模型的鲁棒性。</p>
</blockquote>
<ol start="4">
<li><strong>真实世界实验</strong>：在Franka机器人平台上进行的实验表明，CronusVLA在简单任务、长视野任务以及泛化与观测鲁棒性任务上均优于对比方法DP3和OpenVLA。特别是在“按顺序按下按钮”等需要状态分辨的长视野任务中，CronusVLA表现更稳定，而OpenVLA容易出现重复按同一按钮的状态混淆。</li>
</ol>
<p><strong>消融实验分析</strong>：论文通过消融实验验证了各核心组件的贡献。结果表明，移除多帧建模（仅使用单帧特征）会导致性能显著下降，尤其是在长视野和需要鲁棒性的任务上。特征调制器和多帧正则化对于稳定训练和提升最终性能至关重要。使用队列机制的推理速度相比逐帧处理有大幅提升，验证了其高效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了CronusVLA这一通用框架，通过“单帧预训练+多帧后训练”的两阶段策略，将VLA模型高效且有效地扩展至多帧范式，在提升性能的同时保证了推理效率；2) 引入了SimplerEnv-OR这一新颖的基准测试，首次对VLA模型在多种观测干扰下的鲁棒性进行了系统性定量评估；3) 通过在模拟和真实环境中的大量实验，全面证明了CronusVLA在任务性能、长视野执行以及观测鲁棒性方面的领先优势。</p>
<p>论文自身提到的局限性主要在于其多帧后训练依赖于高质量、跨具身的数据集（如Bridge-v2和Fractal）。对于数据稀缺的领域或机器人平台，这种依赖可能成为一个挑战。</p>
<p>本工作对后续研究有多方面启示：首先，它展示了一条高效利用预训练VLM进行多帧具身建模的可行路径，为平衡模型能力与计算开销提供了新思路。其次，SimplerEnv-OR基准的建立强调了在标准性能评估之外，系统化评估模型鲁棒性的重要性，这对未来面向真实世界部署的研究至关重要。最后，CronusVLA-0.5B小模型的出色表现表明，通过精巧的架构和训练策略设计，较小的模型也可能达到甚至超越大模型的效果，这为开发更轻量、可部署的VLA模型提供了鼓励和方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型无法高效利用多帧时序信息、计算开销大的问题，提出CronusVLA框架。其核心采用两阶段训练：先进行单帧预训练建立基础，再通过多帧后训练将预测目标从离散标记转为可学习特征，并利用特征分块聚合历史信息。实验表明，该框架在SimplerEnv上取得70.9%的成功率，在LIBERO上较OpenVLA提升26.8%，并在包含多种观测干扰的SimplerEnv-OR基准测试中获得了最高的鲁棒性评分。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.19816" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>