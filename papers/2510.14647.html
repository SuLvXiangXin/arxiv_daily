<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Spatially anchored Tactile Awareness for Robust Dexterous Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Spatially anchored Tactile Awareness for Robust Dexterous Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.14647" target="_blank" rel="noreferrer">2510.14647</a></span>
        <span>作者: Huang, Jialei, Ye, Yang, Gong, Yuanqing, Zhu, Xuezhou, Gao, Yang, Zhang, Kaifeng</span>
        <span>日期: 2025/10/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前基于视觉-触觉学习的灵巧操作方法在应对亚毫米级精度的任务时面临挑战，而这些任务对于传统基于模型的方法而言是常规操作。现有方法主要分为两类：第一类将几何重建作为显式目标，如基于触觉的位姿估计或触觉SLAM，但这通常需要已知物体模型或复杂优化；第二类采用端到端学习，将触觉信号输入策略网络，但这些方法在触觉表示上存在局限——要么保留了原始图像的丰富性但缺乏空间定位，要么将触觉特征空间化为3D结构（如点云风格）而丢失了细节。核心痛点是：现有的学习框架未能有效利用触觉信号的感知丰富性及其与手部运动学之间的空间关系。理想的触觉表示应能在一个稳定的参考系中显式地锚定接触测量值，同时保留详细的感官信息，使策略不仅能检测接触发生，还能精确推断在手坐标系中的物体几何形状。</p>
<p>本文提出了SaTA（空间锚定触觉感知）框架，其核心思路是通过前向运动学将触觉特征显式地锚定到手的运动学框架中，从而在不依赖物体模型或显式位姿估计的情况下实现精确的几何推理和端到端的灵巧操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>SaTA是一个端到端的操作策略框架，其整体流程如图2所示。输入包括来自多个指尖的触觉图像、外部RGB观测（头戴和腕部相机）以及机器人关节角度，输出是目标关节角度序列。</p>
<p><img src="https://arxiv.org/html/2510.14647v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SaTA框架概览。多模态输入（头/腕RGB视图、指尖触觉图像、关节状态）通过专用编码器处理。核心创新是空间锚定触觉编码器（黄色部分），它将每个触觉测量值锚定到手的运动学框架中。详细的SAT编码过程（灰色框）展示了如何通过前向运动学计算6D位姿，应用傅里叶编码进行多尺度空间表示，并使用FiLM将空间上下文与来自ResNet的触觉特征融合。所有编码后的特征通过一个ACT风格的Transformer处理，生成用于精确操作的动作序列。</p>
</blockquote>
<p>核心模块是<strong>空间锚定触觉编码器</strong>，它通过三个关键设计实现精确的几何推理：</p>
<ol>
<li><strong>空间参考系选择</strong>：将所有触觉测量锚定到手的URDF坐标系（即手腕参考系），而非世界或相机坐标系。这利用了灵巧操作的成功取决于手内部的相对几何关系（如接触点、对齐角度）这一关键观察，该关系在手坐标系中保持不变，简化了策略学习并便于技能迁移。</li>
<li><strong>细粒度空间编码</strong>：为了感知亚毫米级的空间变化，对通过前向运动学计算得到的完整6D位姿（3D位置+3D旋转）使用<strong>傅里叶位置编码</strong>。其频率分量自然地编码了不同尺度的空间变化：低频分量捕获粗略的位姿关系，高频分量编码精细的调整需求，从而支持网络同时进行粗略定位和精细调整。</li>
<li><strong>空间感知的特征融合</strong>：为了在引入空间信息的同时不丢失触觉图像中包含的丰富局部几何信息（如纹理、边缘方向、压力分布），采用<strong>FiLM（特征线性调制）</strong> 机制。FiLM允许空间信息去调制触觉特征的处理，而不是简单的特征拼接。这使得相同的触觉模式可以根据其空间上下文（例如，在拇指检测到的边缘与在食指检测到的边缘）被差异化解读，从而生成上下文相关的动作。</li>
</ol>
<p><strong>策略架构与训练</strong>：SaTA基于ACT（Action Chunking with Transformers）框架构建，扩展其多模态输入以包含空间锚定的触觉信息。策略接收三类输入：机器人状态信息、视觉信息以及来自所有触觉传感器的空间锚定token（无论是否发生接触）。使用cVAE作为状态编码器来处理多模态输入并捕获示范数据的分布。这些输入通过Transformer架构处理，位置编码确保触觉token与其对应手指的关联。策略输出未来100步的动作序列。训练采用模仿学习，使用通过人类遥操作收集的专家示范，按照ACT范式训练策略从cVAE潜在表示中重建动作序列。空间锚定为策略网络提供了显式的几何归纳偏置。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在配备两个RealMan 7自由度机械臂和Sharpa Wave灵巧手（每个指尖集成视觉触觉传感器）的双臂机器人系统上进行实验。外部感知包括头戴立体相机和两个腕部鱼眼相机。选择了三个对位姿精度极为敏感的任务（如图3所示）：</p>
<ol>
<li><strong>USB-C对接</strong>：双手协调任务，在自由空间完成对接，要求亚毫米级位置容差和几度的角度容差。</li>
<li><strong>卡片滑动</strong>：要求亚毫米级精度和几度的角度容差，以特定角度摊开扑克牌。</li>
<li><strong>灯泡安装</strong>：需要亚毫米级精度和几度的角度容差，以实现灯泡与灯座螺纹的垂直对齐。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.14647v1/x3.png" alt="任务概览"></p>
<blockquote>
<p><strong>图3</strong>：任务概览与挑战。每行代表一个任务：（上）USB-C对接，（中）卡片滑动，（下）灯泡安装。从左到右，各列显示：初始状态、接近阶段、关键接触时刻（红框）和成功完成。红框突出了视觉信息严重退化、需要基于触觉进行精确几何推理的时刻。</p>
</blockquote>
<p><strong>基线方法</strong>：</p>
<ul>
<li><strong>Vision-Only</strong>：仅使用RGB-D图像和关节状态的纯视觉策略。</li>
<li><strong>Tactile-Flat</strong>：将每个触觉传感器编码为一个token并与其他模态拼接，无空间锚定（即SaTA去掉空间锚定组件）。</li>
<li><strong>Tactile-Global</strong>：在Tactile-Flat基础上，将指尖位姿并入本体感知数据流，但缺乏对触觉特征的显式空间锚定。</li>
</ul>
<p><strong>关键实验结果</strong>：如表I所示，SaTA在所有任务上均显著优于所有基线方法，平均成功率高达76.7%，而最强基线为46.7%。在最具挑战性的USB-C对接任务上，几乎所有基线方法都失败（0%成功率），而SaTA达到了35%的成功率。衡量首次接触对齐精度的“首次接触成功率”指标上，SaTA的优势更加明显（48.3% vs 最佳基线的25.0%）。在完成时间上，SaTA平均减少了28%。</p>
<p><strong>消融实验</strong>：在卡片滑动任务上的消融研究（表II）表明：1）用简单拼接替换FiLM（w/o FiLM）导致成功率下降25%，证明了空间调制对保持触觉信息完整性的重要性；2）移除傅里叶编码（w/o Fourier）导致性能下降25%，确认了多尺度空间编码对细粒度操作至关重要；3）将锚定参考系改为世界坐标系（World Frame）导致成功率下降35%，验证了选择手URDF坐标系以保持对臂构型不变性的正确性。</p>
<p><strong>失败模式分析</strong>：图4可视化了无空间锚定方法（Tactile-Flat基线）的几何精度差距。在灯泡安装中，策略学会了倾斜的插入角度而非所需的垂直方向，导致螺纹无法啮合。在卡片滑动中，策略施加了垂直于卡片表面的力而非沿表面方向的力，导致卡片弯曲。在USB-C对接中，策略无法学习到所需的精确拇指-食指协调摩擦运动来调整插头方向。这些失败表明，没有空间锚定，触觉感知只能检测接触发生，而无法提供精确操作所需的几何理解。</p>
<p><img src="https://arxiv.org/html/2510.14647v1/x4.png" alt="失败模式分析"></p>
<blockquote>
<p><strong>图4</strong>：Tactile-Flat基线的失败模式分析。红色箭头表示策略执行的不正确方向/朝向，绿色箭头显示所需的正确方向。（上）灯泡安装：未对齐的插入角度阻止了螺纹啮合。（中）卡片滑动：拇指的垂直运动（红）而非沿表面运动（绿）导致卡片弯曲（蓝色曲线）。（下）USB-C对接：错误的插头朝向（红）以及缺少拇指-食指协调调整运动（蓝色箭头指示了所需但未学会的运动）。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了首个将触觉特征与运动学框架显式对齐的空间锚定触觉表示方法，同时保持了感知丰富性和空间精度；2）实现了从空间锚定触觉感知到精确操作动作的直接映射，无需中间几何重建步骤的端到端策略框架SaTA；3）在包括双手USB-C对接、灯泡安装和卡片滑动在内的高精度灵巧操作任务上成功验证了基于学习的方法，并通过消融研究确认空间锚定是性能提升的关键因素。</p>
<p>论文自身提到的局限性主要在于当前范式下的数据收集：遥操作数据收集无法提供真实的触觉反馈，操作者仅通过振动感知接触事件，而非真实的力分布和纹理，导致示范数据仍以视觉为主导，触觉感知仅起辅助作用。这解释了为何SaTA在需要精细力控的任务（如灯泡安装的最后拧紧阶段）上仍有改进空间。</p>
<p>本文的启示在于，空间锚定触觉感知是实现高精度灵巧操作的关键设计原则。这一核心思想（将触觉测量锚定到运动学框架同时保留其空间语义）独立于特定的策略网络设计，为触觉驱动的灵巧操作提供了一个通用的设计思路。未来的研究可以探索能提供真实力反馈的数据收集技术（如触觉手套）或现实世界的强化学习，以实现真正以力为主导的策略。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对灵巧操作中现有视觉-触觉学习方法难以实现亚毫米级几何精度的问题，提出核心解决方案SaTA框架。该框架通过正向运动学，将触觉传感器的接触测量显式地锚定在稳定的手部运动学坐标系中，从而构建具有空间感知的触觉表征，使策略能精确推断手坐标系中的物体几何。在双手机械臂USB-C对接、灯泡安装等高精度任务上的实验表明，SaTA显著优于基线方法，成功率最高提升30%，任务完成时间减少27%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.14647" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>