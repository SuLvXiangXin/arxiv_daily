<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Learning to See and Act: Task-Aware View Planning for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Learning to See and Act: Task-Aware View Planning for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.05186" target="_blank" rel="noreferrer">2508.05186</a></span>
        <span>作者: Liang Lin Team</span>
        <span>日期: 2025-08-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，用于多任务机器人操作的视觉-语言-动作（VLA）模型通常依赖于静态视角和共享的视觉编码器。静态视角在复杂或动态场景中容易导致目标物体或末端执行器被遮挡，造成视觉信息不完整，进而影响动作预测的准确性。同时，共享的视觉编码器在处理视觉和语义差异巨大的多任务时，会产生任务间干扰，限制了模型的泛化能力和可扩展性。本文针对这两个关键痛点，提出了一种新的视角：将动态虚拟视角探索与任务感知的视觉特征学习相结合。其核心思路是，通过一个高效的探索策略主动寻找信息丰富的观察视角，并利用一个任务感知的专家混合（TaskMoE）视觉编码器来解耦不同任务的特征，从而生成更完整、更具判别力的视觉表示，以提升操作的鲁棒性和泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>TVVE框架的整体流程如图2所示。输入为来自固定视角的多个RGB-D图像、语言指令以及当前夹爪状态。首先，利用RGB-D图像重建场景的3D点云，并聚合到世界坐标系中形成全局点云。随后，流程分为两个分支：一个分支（橙色）进行粗粒度定位（Coarse Grounding），预测末端执行器的大致位置，并以此为中心对全局点云进行缩放和裁剪，保留关键区域；另一个分支（绿色）将全局点云输入到多视角探索策略（MVEP）中，以预测最优的观察视角相机参数。接着，使用这些参数，从经过红色分支处理后的点云中渲染出2D图像。最后，该渲染图像被送入细粒度定位（Fine Grounding）模块，预测最终的机器人动作，包括末端执行器的位置、旋转、夹爪状态和碰撞状态。</p>
<p><img src="https://arxiv.org/html/2508.05186v4/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：任务感知虚拟视角探索（TVVE）框架概览。框架输入为来自固定视角的多张RGB-D图像。首先将其转换为点云并聚合为全局点云。随后分为两支：一支（橙色）进行粗定位并裁剪关键点云区域；另一支（绿色）通过MVEP预测最优观察视角参数，并据此从处理后的点云渲染图像，最终输入细定位模块预测动作。</p>
</blockquote>
<p>核心模块一：任务感知专家混合（TaskMoE）。该模块旨在解决多任务学习中不同任务视觉表征和动作策略差异大的问题。其结构如图3所示，包含两大创新：1) <strong>动态专家路由</strong>：不再仅依赖任务ID，而是通过一个跨模态注意力模块融合指令和视觉信息，生成上下文感知特征，再与任务ID结合通过FiLM层来指导专家选择，使路由更具适应性和任务敏感性。2) <strong>解耦门控策略</strong>：设置 $N_G$ 个路由门供 $N_J$ 个任务共享，且 $N_G &lt; N_J$。这使得语义或视觉相似的任务（如图中的Task 1和Task 2）可以共享同一个门但路由至不同专家，而语义差异大的任务（如Task 3）则使用不同的门。这种设计鼓励发现潜在的任务簇，并促进参数共享与泛化。所有任务共享一个包含 $N_E$ 个专家的公共池，每个输入仅激活前 $k$ 个专家。</p>
<p><img src="https://arxiv.org/html/2508.05186v4/x3.png" alt="TaskMoE流程"></p>
<blockquote>
<p><strong>图3</strong>：TaskMoE流程图。输入包括任务ID、指令和视觉信息，用于指导专家选择。设计了紧凑的门控机制（$N_G$ 个门共享于 $N_J$ 个任务），允许相似任务共享同一门，不同任务使用不同门，从而实现跨多样化操作任务的有效特征专门化。</p>
</blockquote>
<p>核心模块二：多视角探索策略（MVEP）。该策略的目标是选择 $K$ 个能最大化捕获与操作目标相关信息区域的视角。它以重建的点云 $\mathcal{P}$ 及其RGB特征 $\mathbf{F}<em>{\text{img}}$ 的拼接 $\mathbf{X}$ 作为输入，通过一个多层感知机（MLP）预测 $K$ 个相机位姿的参数。每个相机位姿采用“注视”模型，用5维球坐标向量 $\mathbf{p}^{i}=(\theta^{i},\phi^{i},r^{i},\theta</em>{\text{up}}^{i},\phi_{\text{up}}^{i})$ 表示。为了便于基于梯度的优化，MVEP为每个视角输出一个5维对角高斯分布的均值 $\mu^{i}$ 和对数标准差 $\log\sigma^{i}$，并通过重参数化技巧采样得到相机位姿 $\tilde{\mathbf{p}}^{i}$，同时使用Sigmoid函数将各分量约束在有效范围内。</p>
<p>训练策略分为三个阶段：<strong>阶段一</strong>：使用前、左、顶三个默认固定视角训练TVVE的固定视角变体，损失函数包括粗/细热图交叉熵损失 $\mathcal{L}<em>{hc}$、$\mathcal{L}</em>{hf}$，旋转损失 $\mathcal{L}<em>{rot}$，夹爪状态损失 $\mathcal{L}</em>{gri}$ 和碰撞指示损失 $\mathcal{L}<em>{col}$。<strong>阶段二</strong>：使用近端策略优化（PPO）算法微调MVEP。为了降低与环境交互的成本，引入了伪环境交互机制：以阶段一训练的固定视角模型作为参考模型，其损失 $\mathcal{L}</em>{\text{ref}}$ 作为性能下界。MVEP探索动态视角产生的损失为 $\mathcal{L}<em>{\text{TVVE}}$。奖励 $r$ 由三部分加权和构成：任务损失奖励 $r_0 = \mathcal{L}</em>{\text{ref}} - \mathcal{L}_{\text{TVVE}}$（鼓励更低损失）、基于细定位热图平均负熵的置信度奖励 $r_1$（鼓励预测确定性）、以及基于相机位置间平均成对余弦距离的多样性奖励 $r_2$（鼓励视角差异）。此阶段仅MVEP可训练。<strong>阶段三</strong>：使用与阶段一相同的损失函数，对整个TVVE模型（MVEP除外）进行微调，以使MVEP更好地适应动作生成。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在RLBench仿真环境的18个多样化任务上进行评估，并构建了一个包含遮挡、背景变化、光照变化、纹理偏移和相机位姿变化等视觉扰动的OOD基准RLBench-OG。对比的基线方法包括C2F-ARM-BC、PerAct、HiveFormer、PolarNet、RVT、Act3D、3D Diffuser Actor、RVT-2和ARP。</p>
<p><img src="https://arxiv.org/html/2508.05186v4/x4.png" alt="结果对比表1"></p>
<blockquote>
<p><strong>表1</strong>：多视角设置下，18个RLBench任务的性能对比。TVVE取得了最高的平均成功率（86.6%），并且平均排名最低（2.17），表明其在所有任务上具有优越的整体性能和泛化能力。在多个具体任务上（如Close Jar, Drag Stick, Insert Peg等）取得了最佳或接近最佳的成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05186v4/x5.png" alt="结果对比表2"></p>
<blockquote>
<p><strong>表2</strong>：单视角设置下，10个RLBench任务的性能对比。TVVE的平均成功率（83.2%）和平均排名（1.6）均优于之前的先进方法，证明了其框架的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05186v4/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图4</strong>：消融实验结果。从左至右分别展示了固定视角、仅使用MVEP、仅使用TaskMoE以及完整TVVE的性能。完整TVVE组合了MVEP和TaskMoE，取得了最佳的平均成功率（86.6%），显著优于各个组件单独使用的情况，证明了二者协同工作的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.05186v4/x7.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：定性结果对比。TVVE（最右列）探索的视角（绿色虚线框）能够更全面地覆盖任务相关区域（如糖罐和末端执行器），而基线方法（RVT-2）的固定视角（前三列）则存在遮挡或信息缺失，导致TVVE的动作预测（粉色轨迹）更准确。</p>
</blockquote>
<p>消融实验（图4）表明，完整TVVE（MVEP + TaskMoE）的性能显著优于仅使用MVEP或仅使用TaskMoE的变体，证明了动态视角探索与任务感知特征学习的协同增效作用。在RLBench-OG OOD基准上的实验进一步验证了TVVE在视觉扰动和相机位姿变化下的鲁棒性优于基线方法。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了<strong>多视角探索策略（MVEP）</strong>，通过动态虚拟视角重渲染有效解决了遮挡和视角信息不足的问题，增强了3D感知能力；2) 设计了<strong>任务感知专家混合模块（TaskMoE）</strong>，利用指令和场景信息动态路由，解耦了多任务特征，提升了处理能力和鲁棒性；3) 构建了<strong>RLBench-OG基准</strong>，用于全面评估模型在OOD设置下的鲁棒性与泛化能力；4) 在仿真和真实机器人实验中均取得了<strong>优越的操作性能</strong>。</p>
<p>论文自身提到的局限性在于，用于训练MVEP的伪环境交互机制可能引入与真实物理环境的偏差。这项工作对后续研究的启示在于：为提升机器人多任务操作的鲁棒性，需要同时关注<strong>任务感知的视觉表征学习</strong>（避免特征干扰）和<strong>主动的、动态的感知策略</strong>（获取完整信息），二者结合是通向更通用机器人系统的有效路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中静态视角和共享视觉编码器导致的3D感知受限、任务干扰及泛化能力不足问题，提出了任务感知虚拟视角探索（TVVE）框架。该框架整合虚拟视角探索与任务特定表示学习，采用高效探索策略（通过伪环境加速）获取信息视角，并引入任务感知混合专家（TaskMoE）视觉编码器以解耦不同任务特征。实验在RLBench和RLBench-OG基准上进行，TVVE性能显著优于现有方法，在真实机器人操作中于视觉干扰、未见指令等分布外（OOD）设置下展现出卓越的鲁棒性和泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.05186" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>