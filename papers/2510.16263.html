<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly? - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.16263" target="_blank" rel="noreferrer">2510.16263</a></span>
        <span>作者: Peng, Jierui, Zhang, Yanyan, Duan, Yicheng, Liang, Tuo, Chaudhary, Vipin, Yin, Yu</span>
        <span>日期: 2025/10/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）智能体的评估主要依赖于粗粒度的最终任务成功率（如“拾放”任务是否完成）。这一指标虽然易于计算，但存在关键局限性：它无法揭示智能体具体在哪个子技能（如语言理解、三维感知、空间规划或控制）上失败，也无法衡量智能体在面对现实世界扰动（如光照、纹理、语言表述变化）时的鲁棒性。同时，该领域数据生态高度碎片化，如ManiSkill、LeRobot、BEHAVIOR-1k等数据集在格式、任务表示和具身体现上差异巨大，这阻碍了可复现的研究和通用模型的开发。本文针对“如何正确评估VLA智能体”这一具体痛点，提出应从简单的任务完成转向真正的能力掌握，并建立一个统一的评估生态系统。其核心思路是：构建一个名为NEBULA的生态系统，通过标准化的数据API和双轴（能力与压力）评估协议，实现对VLA智能体技能的精细诊断和鲁棒性测试。</p>
<h2 id="方法详解">方法详解</h2>
<p>NEBULA是一个为单臂操作设计的统一生态系统，旨在克服现有具身AI流程中的关键限制。其整体框架包含两大支柱：1）一个标准化的数据层与API，用于统一碎片化的数据集；2）一个双轴评估框架，用于解耦功能能力与实时鲁棒性的评估。</p>
<p><img src="https://arxiv.org/html/2510.16263v2/x1.png" alt="NEBULA生态系统总览"></p>
<blockquote>
<p><strong>图1</strong>：NEBULA生态系统。它统一了碎片化的VLA数据集和API，以支持跨数据集训练和基准测试。引入了包含能力测试和压力测试的双轴评估，通过可控变量隔离实现技能特异性诊断。通过分层任务难度、多模态标注和可视化性能摘要，NEBULA将成功率转化为诊断信号，揭示失败模式和可靠性极限。</p>
</blockquote>
<p><strong>数据与API规范</strong>：为确保一致性和可复现性，NEBULA基于SAPIEN引擎和ManiSkill3框架构建了自定义仿真平台来收集所有数据。每个操作片段（episode）记录了多模态观测（𝒪_t，包括六个固定视角的RGB、深度、分割图像以及本体感知信息）、系统状态（𝒮_t）、动作（𝒜_t）和逐时间步的成功标签（𝒮𝒰_t）。每个片段都标注有反映任务目标的自然语言指令。NEBULA提供Alpha和Beta两个数据集变体：Alpha完全使用运动规划生成的专家轨迹；Beta则结合了运动规划和人工遥操作，为选定的困难任务引入了更多样化和真实的演示。为统一异构数据源，NEBULA引入了标准化的数据模式，并提供了PyTorch API和TensorFlow适配器，以及针对多种广泛使用架构的模型特定适配器，以最小化集成开销。</p>
<p><strong>双轴评估框架</strong>：这是NEBULA的核心创新，它将评估解耦为两个维度：能力测试（Capability Tests）和压力测试（Stress Tests）。</p>
<ul>
<li><strong>能力测试</strong>：旨在评估智能体在正常条件下的“能做什么”。它基于两个关键设计原则：<strong>可控变量隔离</strong>（每个任务只改变一个能力维度，其他保持不变）和<strong>系统难度缩放</strong>（每个能力家族内的任务分为简单、中等、困难三个等级）。该测试隔离了以下六个核心能力（见图2）：<ol>
<li><strong>控制</strong>：隔离低层操作，任务从简单动作到精确的多步序列。</li>
<li><strong>感知</strong>：隔离视觉识别，任务从清晰区分到细微差异和杂乱场景。</li>
<li><strong>语言</strong>：测试指令理解，从基础接地到推理和条件句。</li>
<li><strong>动态适应</strong>：评估对动态变化的适应能力，从物体属性切换到可预测移动再到不可预测的实时事件。</li>
<li><strong>空间推理</strong>：测试空间推理，从2D放置到6自由度规划。</li>
<li><strong>鲁棒性/泛化</strong>：评估在分布偏移下的泛化能力，如干扰物、未见属性和新场景。</li>
</ol>
</li>
</ul>
<p><img src="https://arxiv.org/html/2510.16263v2/x2.png" alt="能力测试任务示例"></p>
<blockquote>
<p><strong>图2</strong>：NEBULA能力测试任务示例，涵盖六个核心能力（控制、感知、动态适应、语言、空间推理和鲁棒性），并组织为三个难度等级。任务通过控制复杂度来隔离特定技能。绿色标记物体，红色标记目标，蓝色指示上下文线索。粗体带下划线文本显示动作；斜体带下划线文本给出说明。</p>
</blockquote>
<ul>
<li><strong>压力测试</strong>：旨在量化系统在特定操作约束下的性能，即“在何种条件下可靠地做”。它包含四个独立的单指标测试，每个测试在三个校准的压力水平（v1–v3）下进行：<ol>
<li><strong>推理频率</strong>：测量动作速率以评估实时响应性。</li>
<li><strong>延迟</strong>：测量从感知到动作的延迟。</li>
<li><strong>稳定性分数</strong>：通过计算连续时间步动作之间的L2范数变化来衡量动作轨迹的平滑度（公式1），分数越高（∈[0,1]）表示越平滑。</li>
<li><strong>适应性</strong>：测试智能体对变化目标（如目标切换、指令切换）的调整能力。</li>
</ol>
</li>
</ul>
<p>与现有方法相比，NEBULA的创新点具体体现在：1) <strong>评估视角的创新</strong>：从单一的成功率指标转向能力诊断与鲁棒性衡量的双轴评估；2) <strong>方法论创新</strong>：通过“可控变量隔离”确保评估结果的明确归因；3) <strong>生态构建</strong>：不仅提供评估协议，还提供了统一的数据格式、API和聚合数据集，旨在解决领域碎片化问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：所有实验均使用NEBULA-Alpha数据集以确保一致性和可复现性。评估平台基于自定义的SAPIEN和ManiSkill3仿真环境。</p>
<p><strong>基线方法</strong>：评估了多种最先进的具身智能体，包括GR00T-1.5、SpatialVLA、RDT-1B、MT-ACT、Diffusion Policy (DP) 和 ACT，涵盖了广泛的架构设计和控制范式。所有模型均在NEBULA Alpha上使用其原始训练协议进行微调。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>能力测试结果</strong>：如图3和图4（左）所示，雷达图揭示了关键趋势。大多数模型在<strong>感知</strong>和<strong>语言</strong>任务上表现强劲，能可靠识别物体属性和执行复杂指令。<strong>控制</strong>和<strong>空间推理</strong>任务表现参差不齐，其中SpatialVLA和GR00T-1.5在控制上领先，而空间推理是大多数模型的瓶颈，即使在SpatialVLA和RDT-1B上，从中等难度开始成功率也明显下降。所有模型在<strong>动态适应</strong>和<strong>鲁棒性</strong>任务上都表现不佳，雷达分数接近于零，暴露了当前VLA在实时自适应规划和分布外泛化方面的重大缺陷。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.16263v2/x3.png" alt="能力测试雷达图（模型对比）"></p>
<blockquote>
<p><strong>图3</strong>：能力雷达图。比较了评估策略在NEBULA六个核心能力家族上的性能：感知、控制、语言、空间推理、动态适应和鲁棒性。每个轴显示了各难度等级任务变体的平均成功率。数值越靠外缘表示在该隔离技能上表现越强。可视化揭示了不同模型间独特的强弱项分布。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.16263v2/x4.png" alt="能力测试雷达图（汇总与对比）"></p>
<blockquote>
<p><strong>图4</strong>：能力测试结果汇总。左图显示了所有模型在每个任务家族、三个难度等级下成功率的均值±标准差。右图显示了各模型在简单和中等任务上的平均性能，便于跨架构比较（困难等级因模型性能接近零而被排除）。</p>
</blockquote>
<ol start="2">
<li><strong>压力测试结果</strong>：如图5所示，随着压力水平增加，所有模型均出现性能下降。<strong>推理频率</strong>普遍下降，GR00T-1.5最具韧性（v3下保持17Hz），而DP和SpatialVLA则低于2Hz。<strong>延迟</strong>随压力增加而显著上升，DP的延迟从v1到v3增加了近四倍，峰值约800ms。<strong>稳定性分数</strong>显示，SpatialVLA在v3下从0.96降至0.86，表明策略确定性存在脆弱性。<strong>适应性</strong>方面，除GR00T显示出适度成功外，其他模型在所有压力水平下成功率几乎为零，表明当前VLA系统在处理动态变化条件方面存在根本性局限。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.16263v2/x6.png" alt="压力测试评估结果"></p>
<blockquote>
<p><strong>图5</strong>：压力测试评估。该图比较了四种模型在三个压力水平（v1, v2, v3）下，在推理频率（Hz）、延迟（ms）、稳定性分数（0–1）和适应性四个指标上的表现。除延迟外，所有指标数值越高表示性能越好。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验与深入分析</strong>：<ul>
<li><strong>因子隔离有效性验证</strong>：如表2所示，在隔离设置下（仅需触碰正确物体），GR00T-1.5B在感知任务上达到100%成功率；而在纠缠设置下（需执行完整的抓放），成功率分别降至92%、68%和76%。视频回顾表明失败源于控制和3D空间推理错误，验证了NEBULA可控变量设计的必要性。</li>
<li><strong>泛化与动态性能差的原因探究</strong>：如表3所示，将SpatialVLA和GR00T的视觉语言模型（VLM）拆解出来仅用于高层规划时，即使在鲁棒性任务中也能产生有效的策略。然而，集成的VLA对应物却无法执行这些计划，成功率在中等难度下即降至零。这表明失败瓶颈在于<strong>动作头</strong>无法将抽象计划转化为精确的控制动作。</li>
<li><strong>快速推理对动态适应的关键性</strong>：如表4所示，GR00T-1.5是唯一展现出适度适应性（成功率28）的模型，其推理频率（16.98 Hz）和延迟（58.62 ms）也显著优于其他模型（如SpatialVLA的1.92 Hz和520.48 ms）。这表明实时适应不仅需要高层推理，还需要快速、低延迟的控制流水线。</li>
</ul>
</li>
</ol>
<p><strong>表2</strong>：GR00T-1.5在三个感知（简单等级）任务上的成功率，比较了因子隔离设置与包含额外干扰物的非隔离场景。结果突出了场景混淆对感知准确性的影响。</p>
<table>
<thead>
<tr>
<th align="left">感知（简单等级）</th>
<th align="center">因子隔离</th>
<th align="center">PlaceBigger Sphere</th>
<th align="center">Place Red Sphere</th>
<th align="center">Place Sphere</th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td align="center">✓</td>
<td align="center">100</td>
<td align="center">100</td>
<td align="center">100</td>
</tr>
<tr>
<td align="left"></td>
<td align="center">✗</td>
<td align="center">92</td>
<td align="center">68</td>
<td align="center">76</td>
</tr>
</tbody></table>
<p><strong>表3</strong>：不同模型在鲁棒性任务上的成功率，用于评估性能下降源于高层规划还是低层执行失败。</p>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="center">鲁棒性/泛化</th>
<th align="center">Easy StackCube</th>
<th align="center">Medium StackCube</th>
</tr>
</thead>
<tbody><tr>
<td align="left">PaliGemma</td>
<td align="center">85</td>
<td align="center">75</td>
<td align="center"></td>
</tr>
<tr>
<td align="left">SpatialVLA</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center"></td>
</tr>
<tr>
<td align="left">Qwen</td>
<td align="center">100</td>
<td align="center">90</td>
<td align="center"></td>
</tr>
<tr>
<td align="left">GR00T-1.5</td>
<td align="center">75</td>
<td align="center">0</td>
<td align="center"></td>
</tr>
</tbody></table>
<p><strong>表4</strong>：模型间推理速度、延迟和适应性分数的比较。只有GR00T-1.5表现出有意义的适应性，这很可能归因于其显著更低的延迟和更高的推理频率。</p>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="center">平均推理频率</th>
<th align="center">平均延迟</th>
<th align="center">平均适应性</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GR00T-1.5</td>
<td align="center">16.98 Hz</td>
<td align="center">58.62 ms</td>
<td align="center">28</td>
</tr>
<tr>
<td align="left">RDT-1B</td>
<td align="center">4.84 Hz</td>
<td align="center">206.77 ms</td>
<td align="center">1</td>
</tr>
<tr>
<td align="left">SpatialVLA</td>
<td align="center">1.92 Hz</td>
<td align="center">520.48 ms</td>
<td align="center">0</td>
</tr>
</tbody></table>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>提出了统一的VLA评估生态系统NEBULA</strong>，提供了标准化API和大规模聚合数据集，以促进可复现的、跨数据集的训练和基准测试。2) <strong>设计了一种新颖的双轴评估协议</strong>，将用于精确技能诊断的细粒度能力测试与用于衡量对抗现实扰动鲁棒性的系统压力测试相结合。3) <strong>通过对当前先进VLA的深入基准测试</strong>，揭示了传统成功率指标通常掩盖的关键失败模式（如空间推理和动态适应能力薄弱），并指出系统响应速度是动态适应的关键瓶颈。</p>
<p>论文自身提到的局限性包括：目前NEBULA主要专注于<strong>单臂操作</strong>任务，未来需要扩展至双手、移动操作和更复杂的多智能体场景。此外，尽管压力测试模拟了现实挑战，但<strong>模拟环境与真实物理世界之间仍存在差距</strong>。</p>
<p>本工作对后续研究的主要启示在于：1) <strong>评估需要更细致</strong>：开发VLA智能体时，不应只满足于最终任务成功率，而应通过诊断性评估明确各子技能的水平，并通过压力测试检验其鲁棒性边界。2) <strong>系统响应速度至关重要</strong>：为实现动态环境下的可靠操作，必须优化整个感知-决策-控制流水线的延迟和推理频率，特别是动作头的执行效率。3) <strong>推动数据与工具标准化</strong>：领域的发展需要像NEBULA这样致力于统一数据格式和评估标准的努力，以减少碎片化， enabling公平比较和大规模泛化研究。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>这篇论文指出当前视觉-语言-动作（VLA）智能体评估存在两大问题：依赖粗糙的最终任务成功率，无法进行细粒度技能诊断；且数据生态分散，阻碍可复现研究。为此，作者提出**NEBULA**统一评估生态系统，其核心是**双轴评估协议**：结合细粒度能力测试以精确诊断技能，以及系统压力测试以衡量对现实扰动的鲁棒性。该系统还提供标准化API与大规模聚合数据集，以促进公平比较。实验发现，传统评估掩盖了顶级VLA模型在**空间推理、动态适应**等关键能力上的严重缺陷，而NEBULA能有效揭示这些弱点，为构建鲁棒的通用具身智能体奠定基础。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.16263" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>