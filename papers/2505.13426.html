<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.13426" target="_blank" rel="noreferrer">2505.13426</a></span>
        <span>作者: Chen, Liang, Gao, Hongcheng, Liu, Tianyu, Huang, Zhiqi, Sung, Flood, Zhou, Xinyu, Wu, Yuxin, Chang, Baobao</span>
        <span>日期: 2025/05/19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）在图像描述、视觉问答等直接多模态任务上表现出色，但将其能力转化为在游戏等交互式、视觉丰富环境中的有效决策仍是一个重大挑战，即存在“知行差距”。这限制了VLMs作为自主智能体的潜力，例如领先的VLMs在简单游戏中表现不佳，有时甚至低于随机选择。主流方法如手动整理多模态思维链数据存在可扩展性限制。强化学习（RL）虽然在游戏AI训练中历史悠久，且近期在增强LLM推理能力（如RLVR）方面备受关注，但缺乏一个面向VLMs在交互游戏中可扩展的RLVR训练框架，且其对感知和推理能力的潜在益处尚不明确。</p>
<p>本文针对VLMs在视觉游戏中决策能力不足的痛点，提出了通过强化学习引导其感知与推理能力协同进化的新视角。核心思路是：首先构建一个支持可扩展并行训练的统一视觉游戏环境VLM-Gym，然后在此环境中，通过纯RL驱动的自我进化（G0模型）或结合感知增强冷启动与知识蒸馏的RL微调（G1模型），来提升VLMs的游戏性能，并发现感知与推理能力在RL过程中会相互促进。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架分为两个核心部分：1）构建训练环境VLM-Gym；2）基于此环境进行强化学习训练，迭代出G0和G1模型。</p>
<p><strong>1. VLM-Gym环境</strong>：这是一个为VLMs在交互游戏中设计的、支持可扩展强化学习的定制环境。它包含四个视觉游戏：2048、Shisen-Sho（形状匹配）、Shisen-Sho-Cifar10（使用CIFAR10图像作为瓷砖的感知增强版）和Swap（交换匹配）。其核心创新在于三个为大规模模型RL训练量身定制的特性：</p>
<ul>
<li><strong>可扩展环境</strong>：支持跨大量游戏状态以及跨多个不同游戏的并行执行，便于高效的大批量训练。</li>
<li><strong>并行动作</strong>：允许对同一观察状态并行采样多个动作并计算各自奖励，这对于GRPO等需要从同一状态采样多个输出以估计优势的先进RL算法至关重要。</li>
<li><strong>组合难度</strong>：每个游戏的难度可在多个维度（如感知复杂性、推理深度）上调整并可组合，便于精细控制任务难度和研究泛化能力。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.13426v1/x2.png" alt="VLM-Gym关键特性"></p>
<blockquote>
<p><strong>图2</strong>：VLM-Gym的关键特性，以Shisen-Sho游戏为例展示。它支持并行环境、并行动作采样以及组合难度设置，专为VLMs的大规模RL训练设计。</p>
</blockquote>
<p><strong>2. 强化学习训练</strong>：</p>
<ul>
<li><strong>训练目标与奖励</strong>：RL过程定义了三种奖励来评估模型输出：游戏奖励（GR，根据游戏规则，如是否成功合并/匹配瓷砖，值为+1或-1）、格式奖励（FR，输出是否遵循<code>&lt;perception&gt;...&lt;thinking&gt;...&lt;answer&gt;</code>的预定格式，0-1奖励）和感知奖励（PR，模型感知输出与地面真值是否完全一致，0-1奖励）。最终奖励为三者加权和：<code>Final Reward = GR + α·FR + β·PR</code>（默认α=1, β=0）。</li>
<li><strong>RL算法</strong>：采用分组相对策略优化（GRPO）作为主要RL算法。该算法从旧策略模型<code>π_θ_old</code>中采样一组（G个）输出，计算每个输出的奖励，并在组内对优势进行归一化（均值为0，标准差为1），然后通过裁剪策略比率和KL散度惩罚来优化策略模型<code>π_θ</code>。</li>
<li><strong>G0与G1模型</strong>：<ul>
<li><strong>G0</strong>：直接从基础模型（Qwen2.5-VL-7B）开始，在VLM-Gym中通过纯RL进行自我进化训练。为促进对多样化游戏状态的探索，训练初期会执行一定步数的随机策略。</li>
<li><strong>G1</strong>：为了解决G0训练中因游戏多样性带来的挑战（如感知先验差距），在RL微调前引入了一个<strong>感知增强的冷启动</strong>阶段。具体而言，利用VLM-Gym提供的<strong>地面真值感知信息</strong>，提示一个更强的教师模型（Claude-3.7-Sonnet-Thinking）生成包含思维过程和动作的响应，然后用这些“感知真值+教师推理”的数据对基础模型进行监督微调（SFT），之后再进入RL训练流程。这本质上是将教师模型的知识蒸馏到学生模型中。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2505.13426v1/x1.png" alt="方法比较"></p>
<blockquote>
<p><strong>图1</strong>：不同模型在VLM-Gym游戏上的性能比较。G1-7B模型在所有游戏上均超越了其教师模型Claude-3.7-Sonnet-Thinking。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在自建的VLM-Gym环境（包含2048、Shisen-Sho、Shisen-Sho-Cifar10、Swap四个游戏）上进行评估。对比的基线模型包括开源模型Qwen2.5-VL-72B，以及闭源模型GPT4o、OpenAI-o1和Claude-3.7-Sonnet-Thinking。评估采用多轮设置，报告多次独立运行的平均累积游戏分数。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><p><strong>总体性能</strong>：如表1所示，仅7B参数的G1模型在所有四个游戏上均取得了最佳性能，甚至超越了强大的闭源模型Claude-3.7-Sonnet-Thinking。例如，在2048中，G1-7B得分为1070，显著高于Claude的892；在Shisen-Sho中，G1-7B得分为17.5，高于Claude的15.3。仅通过纯RL训练的G0-7B模型也在多个游戏上超越了o1、GPT4o和Qwen2.5VL-72B等模型。</p>
</li>
<li><p><strong>G0模型的RL过程分析</strong>：<br><img src="https://arxiv.org/html/2505.13426v1/extracted/6455193/images/game_curves.png" alt="游戏奖励曲线"></p>
<blockquote>
<p><strong>图3</strong>：G0模型在不同游戏中RL训练过程中的平均游戏奖励曲线。显示了不同游戏学习动态的差异。</p>
</blockquote>
<p>图3展示了G0在不同游戏中的RL学习动态差异，论文据此分析了三类挑战：</p>
<ol>
<li><strong>感知先验差距</strong>：Shisen-Sho-Cifar10与基础版规则相同，但因使用CIFAR10图像导致感知难度大增，学习速度慢于基础版。</li>
<li><strong>不准确的奖励分配</strong>：在2048中，随机策略已是较强基线，模型可能产生完全错误的感知和推理但仍获得正奖励，导致学习过程出现偏差（模型崩溃，忽略图像并采取随机动作）。</li>
<li><strong>稀疏奖励</strong>：在Swap游戏中，基础模型很难获得正奖励，导致RL学习进展缓慢。</li>
</ol>
</li>
<li><p><strong>感知与推理的互促（Bootstrapping）</strong>：<br><img src="https://arxiv.org/html/2505.13426v1/x3.png" alt="G0模型感知与推理模式涌现"></p>
<blockquote>
<p><strong>图4</strong>：G0模型在RL训练过程中涌现出的感知与推理模式。训练后，模型学会了输出精确的坐标网格描述和逐步推理过程。</p>
</blockquote>
<p>图4展示了G0在Shisen-Sho游戏中通过RL涌现出的关键能力：训练前，模型感知输出模糊；训练后，模型自发学会了输出<strong>精确的坐标网格描述</strong>（如“第1行第1列是圆形”）以及<strong>结构化的逐步推理过程</strong>（如“检查(1,1)和(3,3)是否匹配…”）。这表明在RL过程中，为了获得更高的游戏奖励，模型必须同时改进感知（准确定位）和推理（有效规划），两种能力相互促进、共同进化。</p>
</li>
<li><p><strong>G1模型的优势</strong>：<br><img src="https://arxiv.org/html/2505.13426v1/extracted/6455193/images/g1_scores.png" alt="G1模型性能"></p>
<blockquote>
<p><strong>图6</strong>：G1模型在不同游戏上的得分，展示了其超越教师模型的全面优势。</p>
</blockquote>
<p>图6直观展示了G1模型在所有游戏上均超越其教师模型Claude-3.7-Sonnet-Thinking。通过感知增强的冷启动，G1有效缓解了感知先验差距等问题，为后续RL训练奠定了更好的基础，从而取得了更优的整体性能。</p>
</li>
<li><p><strong>消融实验</strong>：论文通过分析G0和G1在感知准确率和推理准确率上的表现（图7），进一步验证了感知与推理的互促关系。同时，实验也探讨了在最终奖励中加入感知奖励（PR，即β&gt;0）的影响（图8），发现这能有效引导模型在RL早期关注感知准确性，但可能以牺牲游戏得分为代价。</p>
</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了VLM-Gym</strong>：一个专为VLMs设计、支持可扩展多游戏并行训练和高级RL算法的视觉游戏RL环境。</li>
<li><strong>提出了G0和G1训练范式</strong>：展示了纯RL自我进化（G0）以及结合知识蒸馏与RL微调（G1）能够显著提升VLMs在复杂视觉游戏中的决策性能，并且小模型（7B）可以超越大模型（72B）和顶尖闭源模型。</li>
<li><strong>揭示了感知与推理的互促机制</strong>：通过系统性分析，首次发现并论证了在视觉游戏的RL训练过程中，VLMs的感知能力和推理能力会相互引导、协同进化。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，G0模型训练面临因游戏多样性带来的挑战，如感知先验差距、不准确的奖励分配和稀疏奖励问题。当前的探索策略（随机策略）较为简单，未来可采用更复杂的搜索算法或更强模型来增强探索。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>训练框架设计</strong>：为VLMs设计更通用、鲁棒的RL训练框架，需要综合考虑不同任务（游戏）的特有挑战（如奖励稀疏性、信用分配）。</li>
<li><strong>能力协同进化</strong>：感知与推理的互促现象为如何通过交互式训练全面提升VLMs的综合能力提供了新思路。</li>
<li><strong>环境与基准</strong>：VLM-Gym的发布为社区提供了一个评估和推进VLMs作为交互式智能体能力的宝贵基准和实验平台。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型在交互式视觉环境（如游戏）中决策能力不足的“知易行难”问题，提出通过强化学习提升其感知与推理能力。关键技术包括VLM-Gym训练环境（提供多样游戏、统一接口和可调难度，支持并行训练）、G0模型（纯RL驱动自我进化）和G1模型（引入感知增强冷启动后RL微调）。实验表明，G1模型在所有游戏中超越其教师模型，并优于Claude-3.7-Sonnet-Thinking等领先专有模型；分析发现感知与推理能力在RL训练中相互促进。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.13426" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>