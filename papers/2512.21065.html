<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.21065" target="_blank" rel="noreferrer">2512.21065</a></span>
        <span>作者: Hu Cao Team</span>
        <span>日期: 2025-12-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人抓取是自主操作的基础能力。传统方法包括依赖精确物体模型的模型法，以及数据驱动的判别式方法。生成式抓取检测（如GG-CNN、GR-ConvNet）将抓取视为图像域上的密集预测问题，提升了推理速度，但本质上是纯视觉的，无法理解并整合自然语言指令中蕴含的任务意图（如“抓起红色螺丝刀的手柄”）。现有语言引导抓取方法（如CLIPort、GraspMamba）通常采用浅层或单向的跨模态融合策略，仅在处理流程的早期或晚期进行一次语言-视觉融合。这种融合方式难以在抓取推理的全过程中维持语义上下文，导致语言意图与视觉抓取推理之间的对齐较弱，尤其在处理细粒度指令或复杂场景时表现不佳。本文针对现有语言引导抓取方法中跨模态融合不充分、语义对齐弱的痛点，提出了LGGD框架。其核心思路是采用从粗到细的学习范式，通过一个分层的跨模态融合流程，将语言线索逐步、多尺度地注入视觉特征重建过程，从而实现细粒度的视觉-语义对齐，生成符合任务指令的可行抓取位姿。</p>
<h2 id="方法详解">方法详解</h2>
<p>LGGD是一个端到端的语言条件化抓取检测框架，整体流程如图3所示。给定一张RGB图像和一个自然语言指令，模型输出与指令一致的4-DoF抓取位姿。框架主要包含：基于CLIP的图像与文本编码器、双交叉视觉语言融合瓶颈、分层语言引导上采样模块、粗掩码与抓取预测头，以及后续的掩码与抓取细化模块。</p>
<p><img src="https://arxiv.org/html/2512.21065v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图3</strong>：LGGD框架总览。给定RGB图像和自然语言指令，CLIP编码器分别提取视觉特征和词级/句级文本嵌入。双交叉视觉语言融合模块对齐两种模态，随后分层语言引导上采样根据文本意图逐步恢复空间细节。粗预测头输出分割掩码、抓取质量、角度和夹爪宽度。最后的掩码和抓取细化模块锐化边界并稳定抓取位姿，产生准确、符合指令的抓取。</p>
</blockquote>
<p><strong>CLIP编码器</strong>：图像编码器采用CLIP-based ResNet-50（结构见图4），输入224×224×3的RGB图像，经过四个残差阶段输出多尺度视觉特征{x0, x1, x2, x3}，其中x3经过注意力池化得到增强的全局视觉特征x4，用于后续融合。文本编码器采用CLIP的文本分支，输出词级特征y_word（77×512）和句级特征y_sentence（512）。词级特征保留细粒度语义，用于像素级对齐；句级特征提供全局指令语义，用于后续的特征调制和预测头参数生成。</p>
<p><img src="https://arxiv.org/html/2512.21065v1/x3.png" alt="图像编码器结构"></p>
<blockquote>
<p><strong>图4</strong>：CLIP-based ResNet-50图像编码器结构。通过四个残差阶段提取渐进抽象的特征图，注意力池化增强全局语义感知。</p>
</blockquote>
<p><strong>双交叉视觉语言融合</strong>：该模块（DCVLF，结构见图5）是核心创新之一，旨在实现视觉与语言特征的双向、细粒度对齐。它以视觉特征x4和词级文本特征y_word为输入。首先，视觉特征经过自注意力增强位置感知。然后，通过双向交叉注意力机制：视觉特征作为查询（Q）去关注文本键值（K, V），以聚焦于语言相关的视觉区域；同时，文本特征也作为查询去关注视觉键值，使语言表征能基于视觉证据进行细化。这种对称的交互确保了语义上下文的充分传播。最后，通过一个1×1卷积和全局多头自注意力前馈网络块进一步整合局部与全局上下文，输出融合后的鲁棒多模态表征。</p>
<p><img src="https://arxiv.org/html/2512.21065v1/x4.png" alt="双交叉视觉语言融合结构"></p>
<blockquote>
<p><strong>图5</strong>：DCVLF模块结构。视觉特征经位置编码和自注意力块增强。双向交叉注意力使图像特征能关注语言线索，文本特征能关注视觉区域，实现全语义对齐。残差连接和FFN稳定学习，后续的卷积和全局MHSA-FFN块进一步整合上下文。</p>
</blockquote>
<p><strong>分层语言引导上采样</strong>：该模块（LMAFN）负责将融合后的低分辨率特征上采样至高分辨率，同时在整个过程中保持语言条件化。它采用类似U-Net的跳跃连接结构，融合来自编码器的浅层特征{x0, x1, x2}。关键创新在于，在每个上采样阶段，都使用句级文本特征y_sentence，通过特征线性调制（FiLM）和注意力机制来调制解码器激活，从而在恢复空间细节时强调与指令相关的区域，抑制无关区域。</p>
<p><strong>语言条件化动态卷积头</strong>：在粗预测阶段，模型使用语言条件化动态卷积头（LDCH）来生成初步的抓取预测和分割掩码。LDCH的创新在于，它不是使用固定的卷积核，而是利用句级文本特征y_sentence，通过一个专家混合（MoE）公式动态生成指令自适应的卷积核权重。这使得预测头能够根据不同的指令（如“抓杯子” vs “抓杯柄”）调整其行为，实现指令感知的粗预测。</p>
<p><strong>细化模块</strong>：为了提高在复杂场景下的输出一致性和鲁棒性，模型最后包含一个轻量级的残差细化模块。它以前一阶段的粗预测（掩码和抓取图）以及对应的视觉特征为输入，通过残差学习校正局部伪影（如边界模糊、抓取角度抖动），输出更精确、稳定的最终预测结果。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：实验在OCID-VLG和Grasp-Anything++两个语言引导抓取数据集上进行。真实机器人实验部署在配备腕戴式RGB-D相机的KUKA机器人平台。</p>
<p><strong>对比方法</strong>：对比的基线方法包括：CLIPort、GraspMamba、GraspSAM、Grasp-Anything++（原方法）以及纯视觉的生成式方法GR-ConvNet。</p>
<p><strong>关键实验结果</strong>：<br>在OCID-VLG数据集上，LGGD在抓取成功率（GSR）指标上达到87.6%，显著优于GraspMamba（81.8%）和CLIPort（78.7%）。在Grasp-Anything++数据集上，LGGD在严格匹配准确率（<a href="mailto:&#65;&#99;&#x63;&#x40;&#x30;&#x2e;&#x32;&#x35;">&#65;&#99;&#x63;&#x40;&#x30;&#x2e;&#x32;&#x35;</a>）上达到68.7%，相比基线Grasp-Anything++（60.1%）有显著提升，证明了其优越的泛化能力。</p>
<p><img src="https://arxiv.org/html/2512.21065v1/x6.png" alt="OCID-VLG定量结果"></p>
<blockquote>
<p><strong>图8</strong>：在OCID-VLG数据集上的定量对比结果。LGGD在抓取成功率（GSR）和严格匹配准确率（<a href="mailto:&#x41;&#x63;&#x63;&#x40;&#x30;&#x2e;&#50;&#x35;">&#x41;&#x63;&#x63;&#x40;&#x30;&#x2e;&#50;&#x35;</a>）上均达到最佳性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21065v1/x7.png" alt="Grasp-Anything++定量结果"></p>
<blockquote>
<p><strong>图9</strong>：在Grasp-Anything++数据集上的定量对比结果。LGGD在多种评估指标上全面领先，尤其在严格匹配准确率上优势明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21065v1/x8.png" alt="消融实验"></p>
<blockquote>
<p><strong>图10</strong>：消融实验结果。依次添加DCVLF、LMAFN、LDCH和细化模块（RM），模型性能持续提升，验证了各核心组件的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21065v1/x9.png" alt="定性对比-简单指令"></p>
<blockquote>
<p><strong>图11</strong>：简单指令下的定性结果对比。LGGD能更准确地根据指令（如“白色方块”）定位并生成抓取，而基线方法可能出现错误指向或抓取位姿不佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21065v1/x10.png" alt="定性对比-复杂指令"></p>
<blockquote>
<p><strong>图12</strong>：复杂/空间指令下的定性结果对比。对于“最左边的物体”、“中间的杯子”等指令，LGGD展现了更可靠的空间推理和抓取定位能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21065v1/x11.png" alt="失败案例分析"></p>
<blockquote>
<p><strong>图13</strong>：失败案例分析。LGGD的典型错误包括对高度模糊指令的误解（左上），以及对非常规物体部分抓取点的误判（右下）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21065v1/Images/real_expe.png" alt="真实机器人实验场景"></p>
<blockquote>
<p><strong>图14</strong>：真实机器人实验设置与场景示例。展示了在真实桌面杂乱场景下，根据语言指令执行抓取任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.21065v1/x13.png" alt="真实机器人定量结果"></p>
<blockquote>
<p><strong>图15</strong>：真实机器人实验的定量成功率。在包含简单和复杂指令的测试中，LGGD取得了最高的整体抓取成功率（85%）。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验（图10）验证了各核心组件的贡献。逐步添加双交叉视觉语言融合、分层语言引导上采样、语言条件化动态卷积头和细化模块，模型的抓取成功率依次提升。其中，DCVLF带来的提升最大，证明了双向细粒度融合的关键作用；LDCH和细化模块进一步提升了预测的语义一致性和鲁棒性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1. 提出了LGGD，一个端到端的、从粗到细的语言条件化抓取检测框架，将指代分割与密集抓取预测耦合。2. 设计了双交叉视觉语言融合模块实现双向词级对齐，以及分层语言引导上采样模块在恢复空间细节时保持语言条件化。3. 开发了语言条件化动态卷积头实现指令自适应的粗预测，并结合残差细化模块与多阶段深度监督提升在杂乱场景中的鲁棒性。</p>
<p><strong>局限性</strong>：论文提到，尽管模型性能优越，但其分层融合和动态预测头的设计可能带来相对较高的计算成本。此外，模型在处理极其模糊或包含未知概念的指令时仍可能失败。</p>
<p><strong>后续研究启示</strong>：本文证明了在抓取检测全流程中进行多层次、双向跨模态融合的重要性，为后续语言引导的机器人操作研究提供了新范式。从粗到细的预测流程，结合指令自适应的动态网络结构，是处理复杂、细粒度语言指令的有效途径。未来工作可探索更高效的融合机制，并将此范式扩展至6-DoF抓取或更复杂的操作任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对语言引导的机器人抓取任务中，语义基础有限、语言意图与视觉抓取推理对齐弱的问题，提出了LGGD方法。该方法采用从粗到细的学习范式，基于CLIP嵌入进行分层跨模态融合，逐步注入语言线索以增强视觉-语义对齐；并设计语言条件动态卷积头（LDCH）实现指令自适应的粗掩码与抓取预测，最后通过细化模块提升抓取一致性。实验在OCID-VLG和Grasp-Anything++数据集上验证了LGGD优于现有方法，对未见对象和多样语言查询具有强泛化能力，真实机器人部署也证明了其实际有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.21065" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>