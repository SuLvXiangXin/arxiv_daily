<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.18629" target="_blank" rel="noreferrer">2601.18629</a></span>
        <span>作者: Hao-Shu Fang Team</span>
        <span>日期: 2026-01-26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>模仿学习的有效性严重依赖于训练数据的规模与质量。基于仿真的数据合成虽具有可扩展性，但存在因几何、外观和物理交互差异导致的“仿真到现实差距”。近期，利用神经辐射场或3D高斯泼溅（3DGS）等神经场景表示的Real-to-Sim-to-Real（R2S2R）流程，通过逼真渲染在视觉层面缩小了差距。然而，现有R2S2R方法大多局限于静态场景重建，并依赖强化学习来获取操作数据，其核心挑战在于如何在不部署昂贵机器人硬件的情况下，获取物理有效、高保真的交互数据。</p>
<p>本文针对交互数据难以在仿真中高效获取的痛点，提出了一个机器人无关的4D R2S2R框架ExoGS。其核心思路是：利用一个与机器人同构的被动外骨骼AirExo-3采集人体演示，将静态环境和动态交互完整捕获并数字化为可编辑的3DGS资产，进而在仿真中进行几何一致的重放与大规模数据增强，以支持高效的操作数据收集与策略学习。</p>
<h2 id="方法详解">方法详解</h2>
<p>ExoGS框架包含硬件采集、4D数据生成与增强、以及策略学习三个主要部分。其核心创新在于硬件-软件协同：通过低成本、高精度的外骨骼设备捕获与机器人运动学一致的演示轨迹，并利用3DGS的可编辑特性进行数据数字化与增强。</p>
<p><img src="https://arxiv.org/html/2601.18629v1/imgs/GSpipeline.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ExoGS整体流程概览。使用AirExo-3（左）采集多视角RGB-D观测和关节角度，通过3D高斯泼溅（中）重建机器人、物体和环境的可编辑数字资产，并估计物体位姿，最终在仿真中重放和增强整个操作过程（右）。</p>
</blockquote>
<p><strong>1. 硬件设计：AirExo-3</strong><br>AirExo-3是一个低成本、开源的被动外骨骼，其机械结构与目标机器人几何匹配，具有相同的运动学参数、关节限位和夹爪开合范围，以确保工作空间一致性。其核心关节模块包含一个12位微型旋转编码器，通过正向运动学可实现毫米级精度的末端定位。设备支持便捷的零位校准，并通过可调被动阻尼确保操作稳定舒适。</p>
<p><strong>2. 4D数据生成与增强</strong></p>
<ul>
<li><strong>演示采集</strong>：操作者佩戴AirExo-3进行演示，系统同步记录关节角度序列 {𝒒ₜ, gₜ} 和多视角RGB-D图像序列 ℐ。</li>
<li><strong>数字资产生成</strong>：利用多视角图像，通过COLMAP恢复相机位姿，并基于3DGS优化场景（机器人、物体、环境）的高斯参数，生成高保真、可解耦的3D资产。</li>
<li><strong>物体位姿估计与轨迹处理</strong>：使用FoundationPose对多视角RGB-D序列进行物体位姿跟踪，并通过融合得到物体在机器人基坐标系下的统一位姿序列 {𝐓ₒ,ₜ}。一个轻量级的<code>PoseProcess</code>模块可对位姿序列进行归一化和重组，例如将物体位姿固定到机器人末端，从而通过替换物体模型实现同一轨迹在不同物体上的复用。</li>
<li><strong>数据增强</strong>：利用可编辑的3DGS表示，实施四种增强策略：1）相机视角增强；2）颜色与光照增强；3）背景增强（将真实图像作为背景纹理合成）；4）物体位姿增强（扰动物体位姿与尺度，或替换为功能兼容的物体）。</li>
</ul>
<p><strong>3. 策略模块：Mask Adapter</strong><br>为了缓解视觉域偏移对策略泛化的影响，本文提出了一个轻量级的Mask Adapter模块，旨在将实例级语义信息注入到基于ViT的模仿学习策略中，引导注意力关注与交互相关的区域。</p>
<p><img src="https://arxiv.org/html/2601.18629v1/imgs/maskpipeline.png" alt="Mask Adapter框架"></p>
<blockquote>
<p><strong>图3</strong>：Mask Adapter模块概述。包含两阶段训练：第一阶段使用3DGS生成的像素级监督进行语义分割预训练；第二阶段将获得的补丁级语义标签通过增强的位置编码和掩码引导的注意力机制，注入到ViT策略中。</p>
</blockquote>
<ul>
<li><strong>掩码头</strong>：在第一阶段，对视觉编码器和一个轻量级分割头进行像素级监督的微调，以获得可迁移的语义信息，并为每个图像补丁生成标签 ℓₙ。</li>
<li><strong>掩码引导的令牌建模</strong>：在第二阶段，利用补丁标签 ℓ 来增强基础位置编码（添加可学习的标签嵌入），并构建一个加性的注意力掩码 𝐀。该掩码基于预定义的标签关系集 ℛ，允许语义一致的令牌之间进行交互，而阻止不相关令牌间的通信。</li>
<li><strong>训练目标</strong>：第二阶段联合优化原始策略的动作损失 ℒ_act 和分割损失 ℒ_seg，以稳定语义对齐。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界的Flexiv Rizon 4s机器人上进行，使用三个操作任务进行评估：<strong>拾放</strong>、<strong>拾放并关闭</strong>和<strong>拧开瓶盖</strong>。基线方法为基于遥操作收集数据训练的策略。评价指标包括数据收集效率（成功率和单次演示平均时间）和策略执行成功率（25次试验）。</p>
<p><strong>数据收集效率</strong>：招募10名无机器人背景的志愿者进行测试。如图5和表I所示，AirExo-3在所有任务上的数据收集速度均快于遥操作，且任务越复杂优势越大。同时，AirExo-3的演示成功率显著高于遥操作，尤其是在接触密集的拧瓶盖任务上（87% vs. 17%），证明了其在采集可靠接触式操作演示方面的有效性。</p>
<p><img src="https://arxiv.org/html/2601.18629v1/imgs/timeuse.png" alt="数据收集时间对比"></p>
<blockquote>
<p><strong>图5</strong>：AirExo-3与遥操作在各项任务上完成单次成功演示的平均时间对比。AirExo-3更快，且任务越复杂优势越明显。</p>
</blockquote>
<p><strong>TABLE I: 任务成功率对比: AirExo-3 vs. 遥操作</strong></p>
<table>
<thead>
<tr>
<th align="left">任务</th>
<th align="left">AirExo-3</th>
<th align="left">遥操作</th>
</tr>
</thead>
<tbody><tr>
<td align="left">拾放</td>
<td align="left">100%</td>
<td align="left">92.3%</td>
</tr>
<tr>
<td align="left">拾放并关闭</td>
<td align="left">100%</td>
<td align="left">83%</td>
</tr>
<tr>
<td align="left">拧开瓶盖</td>
<td align="left">87%</td>
<td align="left">17%</td>
</tr>
</tbody></table>
<p><strong>策略性能（无增强）</strong>：如表II所示，在大多数任务上，使用原始生成数据训练的ExoGS策略性能低于使用遥操作数据训练的基线，主要原因是渲染观测与现实观测间仍存在视觉差距。然而，在“拾放（新物体）”设定下，通过数字资产替换生成的<strong>完全合成演示</strong>使ExoGS策略取得了76%的成功率，而遥操作基线为0%，证明了低成本数据集扩展的巨大价值。</p>
<p><strong>TABLE II: ExoGS与遥操作策略成功率对比</strong></p>
<table>
<thead>
<tr>
<th align="left">任务</th>
<th align="left">ExoGS</th>
<th align="left">遥操作</th>
</tr>
</thead>
<tbody><tr>
<td align="left">拾放</td>
<td align="left">50%</td>
<td align="left">72%</td>
</tr>
<tr>
<td align="left">拾放并关闭</td>
<td align="left">48%</td>
<td align="left">64%</td>
</tr>
<tr>
<td align="left">拧开瓶盖</td>
<td align="left">24%</td>
<td align="left">8%</td>
</tr>
<tr>
<td align="left">拾放（新物体）</td>
<td align="left">76%</td>
<td align="left">0%</td>
</tr>
</tbody></table>
<p><strong>策略性能（有增强）与消融实验</strong>：使用四种增强策略将数据集扩大20倍。如图7(a)所示，数据增强显著提升了策略在物体颜色、背景、光照变化下的泛化能力，其性能甚至超过了仅在真实数据上训练的策略。</p>
<p><img src="https://arxiv.org/html/2601.18629v1/imgs/aug_all.png" alt="增强策略效果与消融"></p>
<blockquote>
<p><strong>图7</strong>：(a) 数据增强对策略泛化的影响。使用增强数据训练的策略在多种视觉扰动下表现更优。(b) 消融研究，评估不同增强策略（A:视角，B:外观，C:物体位姿）对泛化性能的贡献。颜色抖动（外观增强）带来的提升最大。</p>
</blockquote>
<p>消融实验（图7b）表明，<strong>视角变化</strong>和<strong>颜色抖动</strong>对泛化性能提升贡献最大，其中颜色抖动因能有效应对合成与真实数据间持久的颜色和光照差异，效果最为显著。而<strong>物体位姿增强</strong>带来的收益有限，因为原始数据中物体位姿已足够多样，且位姿扰动并未解决主要的视觉域差距。</p>
<p><strong>Mask Adapter的效果</strong>：如图8所示，Mask Adapter在仅使用容器和绿色方块训练的情况下，对未见过的物体和背景展现了强大的分割泛化能力。这证明了其注入的语义信息能够有效引导策略关注交互相关区域。</p>
<p><img src="https://arxiv.org/html/2601.18629v1/imgs/maskfig.jpg" alt="Mask Adapter分割结果"></p>
<blockquote>
<p><strong>图8</strong>：Mask Adapter的分割结果示例。模型仅在容器和绿色方块上训练，但在测试时对新颖物体和背景实现了鲁棒的分割。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个完整的4D、机器人无关的Real-to-Sim-to-Real框架ExoGS，将真实世界资产和操作序列重建为可编辑的3DGS资产及其动态，实现了可扩展、运动学一致的数据生成。</li>
<li>开发了开源、低成本、高精度的操作数据采集设备AirExo-3。</li>
<li>设计了轻量级Mask Adapter模块，通过向策略注入语义掩码信息来引导注意力，增强了策略在视觉域偏移下的鲁棒性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，对于像拧瓶盖这类受强运动学约束的任务，策略失败的主要原因是演示质量受操作者熟练度限制导致的轨迹次优，以及螺纹耦合中的滑动、卡死等物理问题，数据增强对此类问题的改善有限。这暗示了当前方法在物理仿真逼真度方面仍存在挑战。</p>
<p><strong>启示</strong>：ExoGS展示了一条通过“人体演示采集 + 可编辑神经场景表示”来实现高效、规模化操作数据生成的路径。其硬件-软件协同的思路，以及利用显式场景表示进行几何一致增强和语义注入的策略，为后续研究提供了重要参考。如何进一步提升物理交互仿真的真实性，以及如何更智能地利用场景的几何与语义先验来指导策略学习，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ExoGS框架，旨在解决机器人操作任务中高质量交互数据获取困难、仿真与真实世界差距大的问题。其核心方法包括：1）使用机器人同构被动外骨骼AirExo-3精准捕捉人类演示的毫米级轨迹；2）基于3D高斯泼溅技术将场景重建为可编辑的动态资产，支持几何一致的数据增强；3）引入轻量级Mask Adapter模块，为策略注入实例级语义以提升视觉域偏移下的鲁棒性。实验表明，该框架相比遥操作基线显著提升了数据效率和策略泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.18629" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>