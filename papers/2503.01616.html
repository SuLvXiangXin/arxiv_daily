<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.01616" target="_blank" rel="noreferrer">2503.01616</a></span>
        <span>作者: Liu, Haichao, Guo, Sikai, Mai, Pengfei, Cao, Jiahang, Li, Haoang, Ma, Jun</span>
        <span>日期: 2025/03/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作领域的主流方法大多使用平行夹爪，以简化高自由度的操作问题。这些方法在刚性、预定义的抓取任务上表现出色，但在处理易碎、可变形或形状复杂的物体时存在局限。同时，现有研究主要集中在抓取姿态生成上，缺乏将灵巧操作与任务级规划整合的方案，这限制了机器人执行长视野、开放词汇任务的能力。</p>
<p>本文针对上述痛点，提出了一种新的视角：将视觉语言模型驱动的任务规划与多指灵巧手操作相结合。核心思路是：利用VLM解析开放词汇的自然语言指令，生成基于原子技能的任务序列，并通过一种创新的运动学重定向方法，将成熟的平行夹爪抓取感知算法适配到灵巧手上，从而实现零样本的灵巧操作。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboDexVLM框架旨在桥接高层任务规划与底层运动控制，实现基于自然语言指令的长视野灵巧操作。</p>
<p><img src="https://arxiv.org/html/2503.01616v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：RoboDexVLM整体框架。多模态提示（人类指令、可用技能列表、RGB-D图像和相关记忆项）被发送给VLM进行任务规划。收到VLM生成的技能调用序列后，灵巧机器人按序执行技能直至任务完成。虚线表示操作失败后的恢复过程。</p>
</blockquote>
<p>整体流程如<strong>图1</strong>所示。系统接收自然语言指令、当前RGB-D图像、结构化技能库以及历史记忆，由VLM进行任务分解与规划，输出待执行的技能序列及其参数。机器人随后按序调用底层技能模块执行，并在失败时触发恢复机制。</p>
<p>核心模块包括一个基于VLM的任务规划器和一个语言引导的灵巧抓取感知算法。</p>
<ol>
<li><strong>任务规划与技能库</strong>：框架的核心是一个结构化的技能库 𝒮 = {ℱ₁(X₁), ℱ₂(X₂), ..., ℱₙ(Xₙ)}，其中封装了8个原子操作技能（如检测、抓取、移动、放置）。VLM作为规划器，其提示推理过程为 {ℛτ, 𝒪τ, ℐτ} = 𝒯( K(𝒮, ℳτ, ℒτ) )。其中，K(·)是上下文生成器，输入为包含思维链模板的系统消息𝒮、记忆消息ℳτ和时间步τ的人类指令ℒτ。𝒯代表VLM的推理过程。输出包括：思维链推理文本ℛτ（增强可解释性）、待调用技能的顺序𝒪τ，以及每个技能所需的输入参数ℐτ。</li>
<li><strong>技能执行与数据流</strong>：如<strong>图2</strong>所示，一个任务管理器根据VLM生成的𝒪来协调技能ℱi的执行。所有技能通过一个动态变量存储𝒟进行数据交换，该存储包含语言引导ℰlang、RGB-D图像像素矩阵、语义分割二值图ℬimg、抓取几何信息𝒢、最大接触力Fmax和机器人运动向量A等。技能函数可以查询𝒟以获取实时数据。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.01616v1/x2.png" alt="工作流程"></p>
<blockquote>
<p><strong>图2</strong>：RoboDexVLM的工作流程。系统包含多个互补模块，形成一个闭环操作框架。任务管理器根据VLM生成的𝒪协调技能ℱi的执行。技能的执行依赖于系统核心建立的四个基础能力。</p>
</blockquote>
<ol start="3">
<li><strong>感知-行动与灵巧抓取生成</strong>：系统遵循连贯的感知-行动范式。首先，语言引导的图像分割模块结合Grounding DINO和SAM，根据ℰlang生成像素级精确的目标物体掩码ℬimg。然后，利用对齐的RGB-D图像和掩码，通过AnyGrasp算法推断平行夹爪的最佳抓取位姿𝒢 = {𝒕, 𝑹, w}。<br>关键的创新在于将平行夹爪的抓取提案适配到灵巧手。如<strong>图3</strong>所示，系统通过运动学标定，计算从夹爪末端坐标系{E}到灵巧手坐标系{H}的变换矩阵 ᴴ𝑻ᴱ。平行夹爪抓取位姿在相机坐标系{C}中表示为 ᴴ𝑻ᶜ，通过坐标系转换（公式5,6）最终得到机械臂基坐标系{B}下的末端位姿 ᴱ𝑻ᴮ，从而引导灵巧手运动。灵巧手配备力传感模块，手指闭合直至达到预设的最大接触力阈值Fmax。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.01616v1/x3.png" alt="系统设置"></p>
<blockquote>
<p><strong>图3</strong>：RoboDexVLM的机器人操作系统设置。包括UR5机械臂、Inspire灵巧手作为末端执行器，以及安装在手上的RealSense D435i RGB-D相机。图中标明了基座{B}、手{H}、末端执行器{E}和相机{C}的坐标系。</p>
</blockquote>
<p>与现有方法相比，创新点主要体现在：1) 首次将VLM驱动的开放词汇任务规划与多指灵巧手操作紧密结合；2) 提出通过运动学重定向利用成熟平行夹爪抓取感知的方法，为灵巧手生成抓取位姿；3) 设计了包含任务级恢复机制的闭环规划-执行框架。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实物理环境中进行，使用UR5机械臂、Inspire灵巧手和RealSense D435i相机组成的平台（图3）。评估涵盖了长视野任务和灵巧抓取任务。</p>
<p>对比的基线方法包括：OmniManip（使用VLM和平行夹爪）和RoboMamba（VLA模型）。关键实验结果如下：</p>
<ul>
<li><strong>长视野任务</strong>：在“打开盒子并将较大的杨桃放入”等任务中，RoboDexVLM取得了显著更高的成功率。OmniManip由于使用平行夹爪，在抓取复杂形状物体（如杨桃）时失败率较高；RoboMamba则因难以预测精确的物体位姿变换而失败。</li>
<li><strong>灵巧抓取任务</strong>：在抓取8种不同形状和大小的物体（包括胡萝卜、马克杯、杨桃等）时，RoboDexVLM的平均成功率达到86.3%，优于基线方法。特别是在抓取形状不规则的“陌生物体”（如杨桃、大蒜）时，其优势更加明显。</li>
</ul>
<p><img src="https://arxiv.org/html/2503.01616v1/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：RoboDexVLM执行长视野操作任务的定性结果。序列展示了从解析指令“打开盒子并将较大的杨桃放入”到成功完成的整个过程，包括打开抽屉、识别抓取杨桃、放置等步骤。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.01616v1/x5.png" alt="定量对比"></p>
<blockquote>
<p><strong>图5</strong>：在灵巧抓取任务上的定量结果对比。柱状图显示，RoboDexVLM在抓取多种物体（尤其是形状不规则的陌生物体）时，成功率显著高于OmniManip和RoboMamba基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.01616v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。对比了完整框架、无任务级恢复机制、以及使用平行夹爪替代灵巧手三种配置的性能。结果表明，恢复机制和灵巧手对提升任务成功率（特别是对陌生物体）至关重要。</p>
</blockquote>
<p><strong>消融实验</strong>（图6）总结了各组件贡献：</p>
<ol>
<li><strong>任务级恢复机制</strong>：移除后，任务成功率下降，尤其是在操作失败后无法自动调整计划。</li>
<li><strong>灵巧手 vs 平行夹爪</strong>：将末端执行器替换为平行夹爪后，对陌生物体（如杨桃、大蒜）的抓取成功率急剧下降，证明了灵巧手在适应复杂形状方面的必要性。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li>提出了RoboDexVLM，一个将VLM驱动的自动化任务规划与模块化技能库相结合的新框架，有效桥接了高层规划与底层运动约束，实现了长视野灵巧操作。</li>
<li>设计了一种基于机器人运动学和形式化方法的语言引导灵巧抓取感知算法，通过运动学重定向实现零样本的灵巧操作。</li>
<li>通过大量真实世界实验验证了框架在复杂环境中的有效性、对新物体的适应性以及对陌生任务的鲁棒性。</li>
</ol>
<p>论文自身提到的局限性包括：框架性能在一定程度上依赖于所选用VLM的推理和泛化能力；当前的力控制策略（达到Fmax即停止）相对简单。</p>
<p>这项工作对后续研究的启示在于：它展示了VLM与灵巧机器人硬件结合的巨大潜力，为开放词汇的通用机器人操作指明了方向。未来的工作可以探索更鲁棒的VLM集成策略、更精细的灵巧手力控与触觉反馈融合，以及更复杂的长期任务记忆与学习机制。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RoboDexVLM框架，旨在解决机器人执行长序列、复杂灵巧操作任务时，难以抓取多样形状物体并响应开放词汇指令的核心问题。其关键技术包括：基于视觉语言模型（VLM）的鲁棒任务规划器（具备任务级恢复机制），以及结合机器人运动学与形式化方法的语言引导灵巧抓取感知算法，支持零样本操作。实验验证了该框架在处理长视野场景与执行灵巧抓取任务中具有有效性、适应性与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.01616" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>