<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.03707" target="_blank" rel="noreferrer">2512.03707</a></span>
        <span>作者: Emma Li Team</span>
        <span>日期: 2025-12-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在工业5.0时代，人机协作越来越多地涉及物理接触。在精密制造等任务中，机器人可能需要直接从人类手中取回小物体（如螺丝）。然而，这种操作可能导致与手部发生非预期的接触。本文旨在解决的核心问题是：机器人如何在不引起不适或伤害的情况下，从人手中拾取小物体？这代表了一类任务，其中安全且不可避免的接触对于任务执行至关重要。此处的安全性被定义为限制机器人在物理接触过程中施加在人手上的法向接触力 $F_N$。</p>
<p>当前，基于强化学习的运动规划方法在安全性方面主要关注静态和动态环境中的碰撞避免，大多数方法将接触视为需要避免的失败事件。然而，在许多协作任务中，接触是任务执行所必需的。在实现快速、精确到达等任务目标的同时确保低接触力，引入了安全与性能之间的复杂权衡，这是传统避撞框架所未解决的。</p>
<p>本文针对“必须接触但需保证力安全”这一具体痛点，提出了新的视角：将接触安全（通过接触力量化）明确集成到强化学习的奖励函数中，而非简单地避免接触。其核心思路是提出一个名为ContactRL的强化学习框架，通过基于力反馈的奖励直接调控接触力，并部署一个基于动能的控制屏障函数安全护盾，为学习到的策略提供形式化的安全保证。</p>
<h2 id="方法详解">方法详解</h2>
<p>ContactRL框架的核心是让RL智能体通过接收安全交互的奖励和不安全接触的惩罚（利用力反馈）来优化其策略，从而生成安全的运动轨迹。</p>
<p><img src="https://arxiv.org/html/2512.03707v1/Figure2-ContactRL.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ContactRL框架概述。机器人基于人机接触模型学习自适应运动轨迹。在机器人与物体之间应用刚体约束以实现抓取，而施加在人手上的法向接触力 $F_N$ 被反馈到奖励函数的安全组件中。</p>
</blockquote>
<p><strong>整体流程</strong>：机器人利用视觉感知提取工作空间中机器人末端执行器和物体的空间坐标，这些信息构成RL智能体的观察空间。智能体随后通过向目标物体迈进一步来执行下一个动作。控制架构由RL智能体管理的高级运动规划、将规划的末端执行器位姿转换为关节位姿的逆运动学控制器，以及使用位置控制的执行层组成。</p>
<p><strong>马尔可夫决策过程建模</strong>：任务被建模为MDP $\langle\mathcal{S},\mathcal{A}, P, R,\gamma\rangle$。</p>
<ul>
<li><strong>观察空间</strong>：为9维向量 $\mathbf{S}=[p_{\text{r}}, p_{\text{h}}, v_{\text{r}}]$，其中 $p_{\text{r}}, p_{\text{h}}\in\mathbb{R}^{3}$ 是机器人末端执行器和手部的位置，$v_{\text{r}}\in\mathbb{R}^{3}$ 是末端执行器速度。每回合手部位置随机化。</li>
<li><strong>动作空间</strong>：为3维连续空间，对应于机器人末端执行器的笛卡尔位移 $\boldsymbol{a}=[\delta_{x},\delta_{y},\delta_{z}]\in\mathbb{R}^{3}$，其幅度受 $\|\boldsymbol{a}\|\leq\delta_{\text{max}}$ 限制。</li>
<li><strong>奖励函数设计</strong>：这是一个密集奖励函数，平衡了性能和安全。<br>$$ R(\mathbf{s},\mathbf{a}) = w_{r} r_{r} + w_{s} r_{s} + w_{j} r_{j} + w_{p} r_{p} $$<br>其中：<ul>
<li>$r_r$（到达奖励）：鼓励智能体向手部移动，当末端执行器接触到物体时给予500分的接触奖励，否则惩罚与手部的距离。</li>
<li>$r_s$（安全奖励）：激励将接触力维持在阈值 $F_\tau$ 以下。当 $F_N \le F_\tau$ 时给予正奖励，当 $F_N &gt; F_\tau$ 时施加惩罚。</li>
<li>$r_j$（急动奖励）：惩罚连续动作之间的巨大差异，以最小化动作变化。</li>
<li>$r_p$（接近奖励）：鼓励机器人在开始时采取较大步长，接近人类时采取较小步长，以最小化完成时间。<br>权重 $w_r, w_s, w_j, w_p$ 通过消融研究确定最优组合。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2512.03707v1/Figure3-Sim.png" alt="模拟环境"></p>
<blockquote>
<p><strong>图3</strong>：包含UR3e机器人、物体和简化手部模型的模拟环境。右侧面板展示了交互过程中施加在手部上的接触力分量。</p>
</blockquote>
<p><strong>训练设置</strong>：使用Soft Actor-Critic算法进行策略训练。模拟环境基于PyBullet，将人手简化为平面模型，并设置了相应的质量和摩擦系数物理属性。安全接触力阈值 $F_\tau$ 设为50N。</p>
<p><strong>基于能量的屏障安全护盾</strong>：为了在策略执行期间确保安全，部署时采用了基于动能的控制屏障函数安全护盾。该护盾通过末端执行器的动能间接调控接触力。其目标是保证末端执行器的动能 $\frac{1}{2}m\|v\|^2$ 不超过根据安全力限值经验选择的预算 $E_{\max}=0.30\,J$。由于接触力大小 $F_N$ 与动量的变化率成正比，此能量约束隐式地限制了最大可达到的接触力。护盾通过求解一个二次规划问题，将指令速度投影到满足此能量约束的最接近的可容许集合上。护盾工作流程包括：对RL策略输出的原始动作进行低通滤波以抑制抖动，然后应用eCBF条件，必要时对滤波后的速度进行径向缩放以确保动能不超限。</p>
<p><strong>创新点</strong>：1) 与将安全等同于无接触的现有方法不同，ContactRL明确将接触力安全作为奖励项集成到RL策略学习中，以解决必要接触场景下的力调制问题。2) 部署时使用的eCBF护盾基于动能约束，无需精确的接触动力学模型，提供了一种轻量级、可证明安全的形式化保证机制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在PyBullet模拟环境中使用UR3e机械臂和Robotiq 2F-85夹爪进行训练和评估。真实世界验证在UR3e机器人平台上进行，配备了相同的夹爪和内置的6轴力/扭矩传感器。招募了12名参与者，使用5种不同形状的小物体，共进行了360次物理交接试验。</p>
<p><strong>对比基线</strong>：在模拟实验中，与先进的约束RL基线方法进行了对比，包括约束策略优化和SAC-Lagrangian。</p>
<p><strong>消融实验</strong>：通过改变奖励函数中各组件的权重（RF1-RF5）进行消融研究，以确定最优奖励配置。</p>
<p><img src="https://arxiv.org/html/2512.03707v1/Figure4b.png" alt="消融研究结果"></p>
<blockquote>
<p><strong>图4</strong>：对ContactRL奖励函数变体（RF1–RF5）的消融研究，评估了1000个模拟回合。左图显示了平均累积奖励和任务完成时间；右图显示了接触力分布和安全违规率。RF5（包含到达、安全、急动和接近奖励）实现了高性能和最低的安全违规率（0.2%）。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟性能对比</strong>：ContactRL在任务成功率（87.17%）上显著优于CPO（33.33%）和SACLag（55.60%）。其安全违规率（0.20%）与SACLag相当，远低于CPO（2.41%）。在运动平滑度方面，ContactRL的RMS急动度（931.32 m/s³）远低于SACLag（3924.11 m/s³）和CPO（1355.47 m/s³）。</li>
<li><strong>护盾有效性</strong>：<br><img src="https://arxiv.org/html/2512.03707v1/Figure5a.png" alt="无护盾策略"><blockquote>
<p>**图5(a)**：未使用eCBF护盾的ContactRL策略在100个具有随机动作的模拟回合中的动能和末端执行器速度。显示出持续的振荡和偶发的脉冲。<br><img src="https://arxiv.org/html/2512.03707v1/Figure5b.png" alt="有护盾策略"><br>**图5(b)**：使用eCBF护盾的ContactRL策略。动能和速度曲线更平滑，收敛单调，增强了稳定性和安全性。</p>
</blockquote>
</li>
<li><strong>真实世界验证</strong>：<br><img src="https://arxiv.org/html/2512.03707v1/Figure6.png" alt="实验设置"><blockquote>
<p><strong>图6</strong>：在UR3e机器人平台上进行小物体交接实验的真实设置。<br><img src="https://arxiv.org/html/2512.03707v1/Figure7.png" alt="接触力分析"><br><strong>图7</strong>：360次真实世界试验中测量的法向接触力 $F_z$。所有试验的最大接触力 $F_{N,\text{max}}$ 和平均接触力 $\overline{F}_{N}$ 均始终低于10N，远低于50N的安全阈值。方差分析表明，物体类型和放置姿态对最大接触力无显著影响。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ContactRL，一种新颖的基于RL的运动规划框架，通过将基于力反馈的奖励集成到策略学习中，直接解决安全人机接触问题，实现了接触力的主动调制。</li>
<li>在模拟中，该方法在1000个评估回合中仅出现1次安全违规（0.2%），在成功率、安全性和运动平滑度方面优于先进的约束RL基线。</li>
<li>在部署时，通过一个基于动能的CBF安全护盾对学习策略进行增强，提供了形式化的安全保证。在真实机器人平台上进行的360次物理交接试验验证了其有效性，测量到的法向接触力始终低于10N。</li>
</ol>
<p><strong>局限性</strong>：论文提到，模拟环境使用了简化的人手模型（平面），虽然通过设置正确的物理属性（质量、摩擦）来近似真实接触，但与复杂的人体几何和动力学仍存在差距。此外，eCBF护盾在必要时会缩放末端执行器速度，这可能会略微增加任务完成时间。研究聚焦于特定的“从手中拾取”任务，其通用性有待在更广泛的接触丰富任务中验证。</p>
<p><strong>启示</strong>：这项工作展示了将接触安全直接集成到RL奖励函数中的可行性，为处理必要物理接触的协作任务开辟了新途径。所采用的“学习策略+形式化安全护盾”的混合架构，为解决RL在安全关键应用中缺乏保证的问题提供了实用范例，即利用学习获得性能与安全的良好权衡，再通过轻量级、可证明的运行时安全机制确保部署的绝对安全边界。未来研究可探索将接触力感知和调控扩展到更复杂、动态的交互场景中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ContactRL框架，解决人机协作中必要物理接触的安全运动规划问题。核心方法是将接触力安全指标直接融入强化学习奖励函数，并采用基于动能的控制屏障函数（eCBF）作为安全护盾。实验表明，该方法在仿真中实现0.2%的安全违规率和87.7%的任务成功率；在真实机器人递送任务中，接触法向力始终低于10N，确保了安全高效的物理协作。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.03707" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>