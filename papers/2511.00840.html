<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.00840" target="_blank" rel="noreferrer">2511.00840</a></span>
        <span>作者: Suliman, William, Chaikovskaia, Ekaterina, Davydenko, Egor, Gorbachev, Roman</span>
        <span>日期: 2025/11/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，双足人形机器人的控制方法主要分为两大类。一类是基于经典控制的方法，如使用全动力学模型（混合零动力学HZD）或简化模型（线性倒立摆LIPM、角动量线性倒立摆ALIP）生成参考轨迹，再通过PD、任务空间逆动力学（TSID）等控制器进行跟踪。这类方法具有可预测性，但依赖预定义轨迹，在真实环境中的适应性有限，且复杂的数学模型可能影响能效。另一类是无模型学习方法，特别是强化学习（RL），它通过试错学习控制策略，具有高适应性和鲁棒性，但可能导致非直观行为，且许多现有RL控制器未显式考虑落脚点约束，限制了其在需要精确落脚（如不平坦或间断地面）场景中的应用。</p>
<p>本文针对在复杂环境中实现精确、鲁棒双足动态步行的需求，提出了一种混合任务框架。该框架旨在同时跟踪期望的躯干速度和期望的落脚点位置，从而在开放空间保持灵活运动，并在复杂地形中确保精确交互。核心思路是：使用一个简单的启发式步态规划器生成目标落脚点，并结合一个基于Raibert原理的调节器来修正速度跟踪误差，最终由一个强化学习策略来执行，从而避免使用复杂的动力学模型或分析模型。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架（Pipeline）如<strong>图1</strong>所示。高层控制输入为期望的躯干速度（线速度和角速度）和期望的步态周期。启发式步态规划器根据这些输入计算基础的目标步长和步宽。随后，一个Raibert式调节器根据实际速度与期望速度的误差，对落脚点位置进行偏移补偿，生成最终的目标落脚点命令。这些命令与机器人状态、用户速度命令等一同输入到通过强化学习训练得到的控制策略（策略网络）中，策略网络输出关节力矩，驱动机器人运动。</p>
<p><img src="https://arxiv.org/html/2511.00840v2/Images/LS_Controller_Ext.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提出的基于学习框架的结构与流程概览。该方法采用启发式步态规划器来确定步态位置，并结合了类似Raibert的调节器来精确跟踪期望速度。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>启发式步态规划器</strong>：根据公式计算步长（x_step）和步宽（y_step）。步长由期望速度分量v_x与期望步态周期T_d的乘积决定，并受最大步长l_max限制。步宽计算则考虑了横向速度v_y的方向和当前摆动腿（左或右），以产生符合直觉的侧向步态；否则，采用最小步宽w_min并配合速度方向符号。</li>
<li><strong>Raibert式调节器</strong>：用于提高速度跟踪精度。其公式为 Δ(x,y) = Kp*(v_k^a - v_k^d) + Kd*(v_k^a - v_{k-1}^a)，其中v_k^a和v_k^d分别是第k步的实际和期望速度，Kp和Kd为比例和微分增益。该调节器根据速度误差及其变化率，在规划器输出的基础落脚点上增加一个偏移量。</li>
<li><strong>强化学习策略</strong>：是一个具有3个隐藏层（每层256个节点）的全连接神经网络。其输入包括机器人状态、用户速度与步态周期命令、以及由高层控制器（启发式规划器+调节器）导出的步态命令。训练在Isaac Gym仿真引擎的平坦地形上进行，使用PPO算法。奖励函数设计参考了对比文献，并调整了权重（见表1），重点奖励速度跟踪、落脚点跟踪、基座高度和姿态稳定、动作平滑性、低关节力矩等。</li>
</ol>
<p>与现有方法相比，创新点主要体现在：</p>
<ul>
<li>用简单的启发式规则（线性关系）替代了复杂的模型（如LIPM）来进行步态规划。</li>
<li>在策略执行过程中，不仅将目标落脚点作为输入，还通过Raibert调节器提供了基于速度跟踪误差的反馈补偿机制，这与之前仅将落脚点用于奖励函数或完全不定义的方法不同。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.00840v2/Images/Bruce_morphology.png" alt="机器人形态"></p>
<blockquote>
<p><strong>图2</strong>：BRUCE机器人及其基本形态。这是一个约70厘米高、4.8公斤重的儿童尺寸人形机器人。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在BRUCE机器人模型上进行，使用Isaac Gym进行训练，并通过Sim-to-Sim转移到PyBullet环境中进行评估。对比的基线方法是基于线性倒立摆模型（LIPM）的步态规划方法。关键实验结果总结如下：</p>
<p><strong>速度跟踪性能</strong>：在平坦地形上跟踪预定速度剖面（如图4所示）。在参考速度为0.5 m/s时，本文提出的线性步态规划（LS）方法的平均绝对误差（MAE）为0.0136 m/s，标准差为0.0141 m/s，均显著优于LIPM方法（MAE 0.0767 m/s，标准差0.0540 m/s）。LS方法的速度跟踪误差降低了约80%。</p>
<p><img src="https://arxiv.org/html/2511.00840v2/Images/TrackVelocity_twoMethod_BW_slim.png" alt="速度跟踪对比"></p>
<blockquote>
<p><strong>图4</strong>：在平坦地形上，步态周期为0.25秒时的速度跟踪性能。基于LS的方法优于基于LIPM的方法。</p>
</blockquote>
<p><strong>落脚点跟踪性能</strong>：在平均速度0.6 m/s下行走20步，LS方法在跟踪规划落脚点位置上的平均绝对误差为0.002米，略优于LIPM方法的0.003米（表3）。</p>
<p><strong>能量效率</strong>：通过机械和电气运输成本评估。在0.6 m/s速度下行走100步，LS方法的电气成本（C_et）和机械成本（C_mt）均低于LIPM方法，表明其能效更优（表4）。</p>
<p><strong>鲁棒性（抗扰动）</strong>：对机器人基座施加外部线性推力，测试其恢复能力。LS方法在5 N·s和7 N·s冲量下的恢复成功率（分别为81%和55%）均高于LIPM方法（68%和43%），显示出更强的抗扰动能力。</p>
<p><img src="https://arxiv.org/html/2511.00840v2/x1.png" alt="抗扰动测试结果分布"></p>
<blockquote>
<p><strong>图5</strong>：在零命令速度下，机器人对推力的响应。基于LS的方法能够承受更多特定方向的推力，表现出更好的扰动稳定性。每个子图下方标明了恢复和跌倒点的总数。</p>
</blockquote>
<p><strong>不平坦地形行走</strong>：在未训练过的粗糙地形上进行“盲走”测试（图6），评估机器人维持预定速度5秒不跌倒的成功率。LS方法的成功率显著高于LIPM方法，稳定性提升了约60%（图7）。</p>
<p><img src="https://arxiv.org/html/2511.00840v2/Images/roughTerrianPyBullet.jpg" alt="粗糙地形行走"></p>
<blockquote>
<p><strong>图6</strong>：在PyBullet环境中，使用提出的基于LS的方法在粗糙地形上进行盲走。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.00840v2/x2.png" alt="粗糙地形成功率对比"></p>
<blockquote>
<p><strong>图7</strong>：在粗糙地形上行走的成功率比较。基于LS的方法在盲走粗糙地形中表现出色，试验中的稳定性比另一种方法高出约60%。</p>
</blockquote>
<p><strong>有间隙地形行走</strong>：测试了机器人在存在间隙的地形上的行走能力（图8）。当检测到下一步规划区域存在间隙时，机器人能够将落脚点调整到最近的可用平坦表面，成功跨越宽度不超过最大步长（30厘米）的间隙。</p>
<p><img src="https://arxiv.org/html/2511.00840v2/Images/GapTerrain.png" alt="间隙地形行走"></p>
<blockquote>
<p><strong>图8</strong>：评估所提方法在具有间隙的地形上行走的效果。机器人成功跨越了宽度不超过最大步长的间隙。红色—右脚命令位置；蓝色—左脚命令位置；黄点—地形高度图。</p>
</blockquote>
<p>消融实验体现在整个研究设计中：将提出的简单启发式（LS）方法与需要机器人动力学知识的模型方法（LIPM）进行对比。实验结果表明，LS方法在速度跟踪、能效、抗扰动和不平坦地形适应性等多个关键指标上均达到或优于LIPM方法，这证明了简单启发式组件的有效性，并回答了关于模型知识是否必要的研究问题。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有两点：</p>
<ol>
<li><strong>提出了一个集成启发式步态规划的学习架构</strong>：该框架结构简单，无需复杂动力学模型，通过结合Raibert式反馈调节，实现了对期望躯干速度和落脚点位置的同时精确跟踪。</li>
<li><strong>对基于模型的方法进行了系统的对比分析</strong>：通过将所提方法与基于简化模型（LIPM）的方法进行全面比较，评估了模型衍生信息对学习性能、运动稳定性、质量和策略泛化能力的影响。结果表明，即使不使用机器人动力学模型，仅凭简单的启发式规划也能达到优异性能。</li>
</ol>
<p>论文自身提到的局限性在于，当前的控制器未使用任何步高信息，无法主动适应地面高度的变化（如调整抬脚高度）。</p>
<p>本研究的启示在于：对于基于学习的双足动态步行控制，复杂的分析模型组件可能并非必需。一个设计良好的简单启发式规划器，结合强化学习策略，足以实现高精度、高鲁棒性和高能效的 locomotion，这为后续研究提供了更简洁、更高效的控制架构设计思路。未来的工作可以在此基础上，进一步集成步高调整能力，以应对更复杂的崎岖地形。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双足仿人机器人在复杂环境中实现动态稳定行走与精确交互的核心问题，提出了一种结合启发式步态规划的学习框架。该方法采用基于期望躯干速度跟踪的Raibert型控制器调节脚部放置，避免复杂模型和解析规划器。与基于模型的线性倒立摆（LIPM）控制器相比，实验表明该框架在目标速度跟踪精度上可达80%，在不平坦地形鲁棒性提升超50%，且能效更高，证明无需复杂模型即可实现鲁棒行走。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.00840" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>