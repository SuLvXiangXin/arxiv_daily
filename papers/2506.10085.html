<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VITA: Zero-Shot Value Functions via Test-Time Adaptation of Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>VITA: Zero-Shot Value Functions via Test-Time Adaptation of Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.10085" target="_blank" rel="noreferrer">2506.10085</a></span>
        <span>作者: Ziakas, Christos, Russo, Alessandra</span>
        <span>日期: 2025/06/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用视觉语言模型作为零样本目标条件值函数已成为一个研究方向。主流方法主要分为两类：一是基于对比学习的VLM（如CLIP），通过计算观测图像与任务描述在共享表示空间中的相似度来估计任务进度，但这类方法缺乏时序上下文，无法区分任务不同阶段中视觉上相似的状态（例如叠衣服与展开衣服）。二是基于自回归的VLM（如Flamingo，Gemini），它们虽然能通过提示词融入整个视觉轨迹的上下文，但继承了预训练数据按时间顺序排列所带来的预测单调递增偏差。这两类架构都依赖于冻结的预训练表示，这限制了其泛化能力和时序推理能力。本文针对现有零样本值函数方法在泛化和时序推理上的局限性，提出通过测试时适应来增强对比式VLM的能力。其核心思路是：在推理时，通过基于元学习的自监督损失对轻量级适应模块进行在线梯度更新，使每次更新都能改进值估计，并通过在轨迹上顺序更新将历史信息编码到模型参数中，从而解决时序推理问题。</p>
<h2 id="方法详解">方法详解</h2>
<p>VITA的整体框架包含一个训练阶段和一个测试时适应阶段。训练时，模型通过元学习优化一个自监督损失，使得在测试时执行一步该损失下的梯度更新后，能提升值函数预测的准确性。推理时，模型接收一个视觉轨迹和语言任务描述，并顺序地对每个时间步的观测进行适应模块的参数更新和值预测。</p>
<p><img src="https://arxiv.org/html/2506.10085v4/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图1</strong>：VITA方法概述。方法通过元学习训练一个目标条件值函数，并通过测试时适应实现对新分布轨迹的零样本泛化。左侧为训练阶段，在多样化的子轨迹上元学习自监督任务；右侧为推理阶段，对测试轨迹进行顺序的测试时更新以预测值函数。</p>
</blockquote>
<p>模型架构包含三个核心模块：</p>
<ol>
<li><strong>多模态输入表示</strong>：使用冻结的CLIP编码器分别提取视觉观测 (o_t) 和语言目标 (g) 的特征，并将它们在每个时间步拼接，形成联合多模态表示 (z_t = [\phi_v(o_t); \phi_g(g)])。</li>
<li><strong>测试时适应模块</strong>：一个轻量级的适应模块 (f_{\text{adapt}})，其参数为 (\theta)。在推理的每个时间步 (t)，该模块的参数会通过一个基于元学习自监督损失 (\ell_{\text{self}}) 的梯度步进行更新。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.10085v4/x2.png" alt="测试时适应过程"></p>
<blockquote>
<p><strong>图2</strong>：测试时适应过程。在推理时，每个时间步 (t)，适应模块 (f_{\text{adapt}}) 通过基于元学习自监督损失 (\ell_{\text{self}}) 的梯度步进行更新，从而编码时序历史。</p>
</blockquote>
<p>自监督损失定义为重构损失：(\ell_{\text{self}}(z_t; \theta_{t-1}, P_K, P_V) = | f_{\text{adapt}}(P_K z_t; \theta_{t-1}) - P_V z_t |^2)。其中 (P_K) 和 (P_V) 是可学习的线性投影，在元训练中学得，用于生成扰动输入视图和目标。参数更新公式为 (\theta_t = \theta_{t-1} - \eta \nabla_\theta \ell_{\text{self}}(z_t; \theta_{t-1}))。通过这种顺序更新，(f_{\text{adapt}}) 的参数成为一个隐式记忆，编码了过往观测的时序信息。<br>3.  <strong>零样本值函数估计器</strong>：适应后，输入 (z_t) 经由另一个可学习投影 (P_Q) 映射，再通过更新后的 (f_{\text{adapt}}) 模块和一个两层的MLP回归头 (h)，输出预测的值 (V(z_t; g) = h(f_{\text{adapt}}(P_Q z_t; \theta_t)))。回归头在训练后冻结。</p>
<p>训练过程采用基于梯度的元学习，优化目标是使经过一步自监督损失 (\ell_{\text{self}}) 更新后的模型参数，能最小化值预测的监督损失 (\ell_{\text{pred}})（均方误差）。同时，为了缓解专家轨迹中连续帧高度冗余导致的“捷径学习”问题，论文提出了<strong>基于相异性的采样策略</strong>：从轨迹中滑动窗口提取子轨迹候选集后，通过一个高效的启发式算法（公式5）选择彼此间差异最大的 (k) 个子轨迹来构建训练批次，以此增加批次内语义多样性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：主要使用机器人操作数据集BridgeData V2进行训练和评估，并在模拟基准Meta-World MT10上测试奖励塑形效果。训练集仅包含ToyKitchen环境下的拾放任务（2986条专家示教），测试则涵盖环境偏移（如不同背景的折叠、清扫任务）、本体偏移（更换机器人）以及两者结合的分布外泛化。</p>
<p><strong>基线方法</strong>：包括基于对比相似度的零方法（VLM-CL, VLM-RM）、在CLIP特征上训练的回归头（CLIP-FT）或GRU（CLIP-GRU），以及当前零样本SOTA方法GVL（使用自回归VLM，分零样本GVL-0S和单样本GVL-1S）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>分布偏移下的泛化</strong>：使用值顺序相关（VOC）指标评估，VOC越高表示预测进度与时间顺序越吻合。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.10085v4/x4.png" alt="分布偏移泛化结果表"></p>
<blockquote>
<p><strong>图4</strong>：表1：不同分布偏移下值函数估计的VOC分数。VITA在10个数据集中有6个表现最佳，展现了最强的泛化能力。CLIP-GRU表现次之，而基于自回归VLM的GVL方法在某些任务（如折叠）上表现良好，但在其他任务上泛化不佳。</p>
</blockquote>
<ol start="2">
<li><strong>区分专家与非专家轨迹</strong>：使用二进制指标BinVOC（专家轨迹VOC高于非专家轨迹则为1）评估。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.10085v4/x5.png" alt="专家与非专家区分结果"></p>
<blockquote>
<p><strong>图5</strong>：表2(a)：区分专家与非专家轨迹的BinVOC结果。VITA、GVL-0S和GVL-1S达到了完美的1.00，表明它们能一致地为专家轨迹分配更高的进度值。</p>
</blockquote>
<ol start="3">
<li><strong>离线RL的零样本奖励塑形</strong>：在Meta-World MT10上，使用VITA预测的值作为密集奖励训练离线RL策略。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.10085v4/x6.png" alt="离线RL奖励塑形结果"></p>
<blockquote>
<p><strong>图6</strong>：表2(b)：Meta-World MT10基准上离线RL策略的IQM回报。VITA（0.815）超越了所有CLIP基线和模拟器提供的模糊逻辑密集奖励Meta-WL（0.779），证明了其零样本奖励塑形的有效性。</p>
</blockquote>
<p><strong>消融实验</strong>：</p>
<ul>
<li><strong>基于相异性采样的有效性</strong>：与使用完整轨迹采样相比，基于相异性采样在区分专家/非专家任务上表现更优。</li>
<li><strong>增量更新与轨迹级更新的对比</strong>：实验表明，在轨迹上进行顺序的、每步增量参数更新，比在整条轨迹上进行一次更新（轨迹级）能获得更好的VOC性能。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了VITA，一种通过测试时适应来增强对比式VLM泛化与时序推理能力的零样本值函数学习方法，无需任务特定示教或大规模预训练；2）在真实机器人操作任务中，VITA能够从单一训练环境泛化到多样的分布外任务、环境和本体，性能优于基于自回归VLM的SOTA零样本方法；3）VITA的零样本值估计可用于离线RL的奖励塑形，在Meta-World MT10上训练出的多任务策略性能超过了使用模拟器内置密集奖励的策略。</p>
<p>论文提到的局限性包括：方法依赖于预训练的VLM（CLIP）作为基础表示；测试时的在线参数更新虽然轻量，但仍引入了额外的计算开销。</p>
<p>这项研究启示后续工作可以探索：将测试时适应范式更广泛地应用于其他需要时序推理的VLM任务；研究更高效或自适应的在线更新策略；以及探索如何将此类方法与其他形式的记忆或状态表示相结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VITA方法，解决视觉语言模型作为零样本目标条件价值函数时泛化能力有限、缺乏时序推理的问题。核心技术包括：通过测试时自适应，使用元学习自监督损失更新轻量适配模块，使每次更新提升价值估计；引入基于差异性的轨迹采样策略，缓解捷径学习。实验表明，VITA在真实机器人操作任务中，能从单一训练环境泛化到多样分布外场景，性能超越基于自回归VLM的现有零样本方法；其价值估计用于离线强化学习的奖励塑造时，在Meta-World基准上超越了使用仿真密集奖励训练的多任务策略。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.10085" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>