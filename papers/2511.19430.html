<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.19430" target="_blank" rel="noreferrer">2511.19430</a></span>
        <span>作者: Liang, Dingkang, Zhang, Cheng, Xu, Xiaopeng, Ju, Jianzhong, Luo, Zhenbo, Bai, Xiang</span>
        <span>日期: 2025/11/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，具身智能体的任务规划研究主要集中于从人类指令生成逐步的、顺序执行的动作计划（如图1(a)所示）。然而，现有方法存在两个关键局限性：第一，它们缺乏对任务属性的考量和对执行效率的优化，智能体只需生成语义上合理的动作，而无需考虑如何通过并行执行子任务来最小化总完成时间。第二，尽管这些方法假设智能体在3D环境中操作，但规划过程往往被简化为文本问答，未能明确地将每个动作步骤中的目标物体定位到3D场景中的具体位置，这严重阻碍了需要空间信息的后续导航和操作。</p>
<p>本文针对现有方法在效率优化和空间定位上的不足，提出了一个名为“基于运筹学知识的3D落地任务调度”（ORS3D）的新任务。该任务要求智能体在理解语言指令的同时，利用运筹学知识识别可并行的子任务以优化调度效率，并同步为每个步骤中的目标物体提供3D空间定位（如图1(b)和图2所示）。本文核心思路是：通过引入一个连接多模态大语言模型与外部优化求解器的调度令牌机制，使智能体能够生成高效的任务执行时间表，并同时输出每一步的动作描述和目标物体的3D位置。</p>
<p><img src="https://arxiv.org/html/2511.19430v1/x1.png" alt="不同任务完成方案对比"></p>
<blockquote>
<p><strong>图1</strong>：不同任务完成方案对比。(a) 现有方法仅生成顺序执行计划。(b) 本文提出的ORS3D任务要求智能体利用运筹学知识进行高效调度，例如在微波炉运行期间清洁水槽。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.19430v1/x2.png" alt="ORS3D任务示意图"></p>
<blockquote>
<p><strong>图2</strong>：提出的基于运筹学知识的3D落地任务调度示意图。当被分配一个复合任务时，具身智能体需要利用运筹学知识进行高效调度，并同时定位每一步的目标物体。</p>
</blockquote>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了一个名为GRANT的模型来解决ORS3D任务。其整体流程如图6所示：输入为3D场景点云和复合任务文本描述；输出为包含高效调度顺序的逐步动作描述序列，以及每一步中目标物体的3D定位掩码。</p>
<p><img src="https://arxiv.org/html/2511.19430v1/x6.png" alt="GRANT方法总览"></p>
<blockquote>
<p><strong>图6</strong>：GRANT方法总览图。场景点云经3D编码器生成场景令牌。GRANT首先推断子任务属性（阶段1），然后使用调度令牌生成最优调度序列（阶段2），最后生成逐步动作描述和目标物体掩码。</p>
</blockquote>
<p>GRANT包含四个核心组件：</p>
<ol>
<li><strong>3D场景编码器</strong>：将包含坐标和颜色信息的点云 <code>P</code> 通过稀疏卷积网络和预训练的编码器（如CLASP）处理，生成一组富含语义信息的场景查询 <code>Q_hat</code>，再投影为场景令牌 <code>T_s</code> 输入大语言模型。</li>
<li><strong>多模态大语言模型</strong>：作为核心处理器，接收场景令牌和文本令牌，统一理解多模态信息，并负责生成所有输出令牌，包括动作描述和特殊的调度、定位令牌。</li>
<li><strong>调度令牌机制</strong>：这是本文的核心创新点。为了解决LLM不擅长复杂数学优化的问题，作者引入了特殊的<code>&lt;SCH&gt;</code>令牌。具体流程为：LLM首先从任务描述中识别出每个子任务 <code>τ_i</code> 的类型 <code>c_i</code>（可并行或不可并行）和预期时间 <code>t_i</code>，构成约束信息 <code>I</code>。<code>&lt;SCH&gt;</code>令牌将此信息传递给一个外部优化求解器。该求解器将问题形式化为0-1背包问题（算法1），利用动态规划算法，旨在最大化利用可并行子任务的等待时间来执行其他不可并行子任务，从而生成最优的子任务执行顺序列表 <code>S*</code>。<code>S*</code> 随后被转换为自然语言并回注给LLM，以指导其生成符合该调度顺序的逐步动作描述。</li>
<li><strong>3D定位头</strong>：为了实现空间定位，LLM在输出动作描述时，会在对应目标物体的位置插入特殊的<code>&lt;GRU&gt;</code>令牌。这些令牌经过一个MLP头映射后，与场景编码器输出的场景查询 <code>Q_hat</code> 进行余弦相似度计算，选择最匹配的查询 <code>q*</code>。最后，通过 <code>q*</code> 与点云特征的点积和sigmoid激活，生成目标物体的点级预测掩码 <code>m</code>。</li>
</ol>
<p>与现有方法相比，GRANT的创新性主要体现在将运筹学优化求解器通过一个轻量的调度令牌与大语言模型无缝集成，实现了效率优化与多模态理解的协同，而无需依赖外部物体检测器。</p>
<p><img src="https://arxiv.org/html/2511.19430v1/x3.png" alt="可并行与不可并行子任务"></p>
<blockquote>
<p><strong>图3</strong>：可并行与不可并行子任务示例。例如“擦拭桌子”需要持续操作（不可并行），而“使用微波炉”只需启动和检查（可并行）。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在自建的ORS3D-60K数据集上进行，该数据集包含4,376个真实室内场景的60,825个复合任务。评估平台为8张RTX 4090 GPU。</p>
<p><strong>基线方法</strong>：对比了仅文本输入的商业LLM（Gemini, DeepSeek-R1, GPT-4o）、依赖外部3D物体检测器的对象级方法（3D-VisTA, PQ3D, LEO）以及场景级方法Grounded 3D LLM。</p>
<p><strong>评估指标</strong>：涵盖语言质量（METEOR, ROUGE）、调度效率（时间效率TE）和3D定位精度（<a href="mailto:&#65;&#x63;&#x63;&#x75;&#114;&#97;&#99;&#121;&#64;&#x30;&#x2e;&#x32;&#x35;">&#65;&#x63;&#x63;&#x75;&#114;&#97;&#99;&#121;&#64;&#x30;&#x2e;&#x32;&#x35;</a>）。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li>如表2所示，GRANT在调度效率（TE）上取得了72.99%的分数，相较于最强的场景级基线Grounded 3D LLM（42.46%）提升了30.53个百分点。在整体性能（各项指标平均）上，GRANT（53.49%）也显著优于所有基线。</li>
<li>在3D定位方面（表3a），GRANT（35.38%）在无需外部检测器的情况下，定位精度超过了同样不依赖检测器的Grounded 3D LLM（34.00%），展示了其端到端定位的有效性。</li>
<li>表3b显示，GRANT在识别子任务类型（准确率84.65%）和调度效率上均优于基线，证明了准确的属性识别是高效调度的基础。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.19430v1/x4.png" alt="数据集构建流程与示例"></p>
<blockquote>
<p><strong>图4</strong>：(a) ORS3D-60K数据集构建流程，基于3D场景图生成任务元信息，再合成结构化数据。(b) 数据集中的一个复合任务示例，绿色掩码表示对应步骤的真实目标物体。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.19430v1/x5.png" alt="任务复杂度与时间分布"></p>
<blockquote>
<p><strong>图5</strong>：ORS3D-60K数据集中(a)每个复合任务的子任务数量分布（4-7个）和(b)每个子任务预期时间的分布，体现了任务的多样性和真实世界的不确定性。</p>
</blockquote>
<p><strong>消融实验分析</strong>（表4）：</p>
<ul>
<li><strong>调度令牌机制（STM）的效果</strong>（表4a）：仅增加调度内容描述能使TE从21.03%提升至47.04%，而引入STM进一步将TE大幅提升至72.99%，证明了该机制对于获取最优调度的关键作用。</li>
<li><strong>不同任务难度下的表现</strong>（表4b）：随着子任务数量增加（任务变难），所有方法性能均下降，但GRANT在各个难度级别上均保持领先，尤其在复杂任务（6、7个子任务）上优势更明显。</li>
<li><strong>LLM规模的影响</strong>（表4c）：将LLM参数从1B增至7B带来了各项指标的轻微提升，说明模型能力有扩展潜力。</li>
<li><strong>求解器运行时</strong>（表4d）：优化求解器运行效率极高，即使对于50个子任务的极端情况，运行时间也小于4毫秒，验证了该模块的实用性。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>本文核心贡献</strong>：</p>
<ol>
<li><strong>提出了新的任务和评测基准</strong>：首次将运筹学知识引入3D具身任务规划，定义了ORS3D任务，并构建了大规模数据集ORS3D-60K，推动了具身智能向高效、实用化发展。</li>
<li><strong>提出了创新的解决方案GRANT</strong>：设计了一个结合调度令牌机制的多模态大语言模型，通过连接外部优化求解器，有效解决了LLM在数学优化上的短板，实现了高效调度与精准定位的协同。</li>
</ol>
<p><strong>论文提到的局限性</strong>：当前工作主要处理包含单个可并行子任务的复合任务调度。对于更复杂的、多个可并行子任务交织的场景，需要设计更通用的优化算法和模型架构。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>任务定义方向</strong>：ORS3D为具身AI研究开辟了结合经典运筹优化与深度学习的新方向，后续可探索更复杂的调度约束（如资源、路径）。</li>
<li><strong>方法设计方向</strong>：GRANT的“LLM + 专用求解器”混合架构为结合符号推理与神经计算提供了范例，可扩展至其他需要精确逻辑或数学推理的具身任务。</li>
<li><strong>模型能力方向</strong>：如何让LLM内生地学习或更好地配合优化逻辑，减少对预设模板和外部求解器的依赖，是一个值得探索的问题。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文针对具身智能体在3D环境中任务规划时，忽略运筹学优化与空间定位的局限性，提出了新任务“基于运筹学知识的3D接地任务调度”（ORS3D），要求智能体通过并行执行子任务（如边用微波炉边清洗水槽）来最小化总完成时间。为此，研究构建了大规模数据集ORS3D-60K，并提出GRANT模型，该模型采用一种简单的调度令牌机制，以生成高效的任务调度和接地的3D动作。在ORS3D-60K上的实验验证了GRANT在语言理解、3D接地和调度效率方面的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.19430" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>