<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DeepEyes: Incentivizing &#34;Thinking with Images&#34; via Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>DeepEyes: Incentivizing &#34;Thinking with Images&#34; via Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.14362" target="_blank" rel="noreferrer">2505.14362</a></span>
        <span>作者: Zheng, Ziwei, Yang, Michael, Hong, Jack, Zhao, Chenxiao, Xu, Guohai, Yang, Le, Shen, Chao, Yu, Xing</span>
        <span>日期: 2025/05/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型语言模型在视觉推理任务中，通常依赖外部视觉语言模型提供的图像文本描述。然而，这种“文本作为代理”的范式存在关键局限性：静态、通用的图像描述会丢失大量与当前推理步骤具体相关的视觉细节，导致信息不完整或冗余，从而影响最终答案的准确性和推理过程的效率。本文针对“如何在多步视觉推理中动态、按需地获取视觉信息”这一具体痛点，提出了一个新视角：将视觉信息请求建模为一个序列决策问题，通过强化学习训练一个轻量级的策略网络，让LLM学会在推理过程中主动决定“何时”以及“请求何种”图像信息。本文的核心思路是：引入一个强化学习智能体（策略网络），基于当前的推理状态和历史，学习主动请求最相关的图像区域，以获取精准的视觉信息来辅助LLM决策，从而在保证性能的同时最小化查询成本。</p>
<h2 id="方法详解">方法详解</h2>
<p>DeepEyes的整体框架是一个由策略网络、大型语言模型、视觉语言模型和外部图像库组成的闭环交互系统。其输入是用户查询和初始图像，输出是最终答案。流程如下：在每一步，策略网络根据当前状态（包括LLM的推理历史、已请求的图像信息历史等）选择一个动作；动作空间包括“继续推理”、“请求图像信息”和“给出最终答案”；若选择请求图像，则根据LLM生成的区域描述从图像库中裁剪对应区域，送入VLM获取描述，该描述被反馈给LLM以更新其推理状态；此过程循环直至策略选择“停止”并输出答案。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/DeepEyes/main/figures/framework.png" alt="DeepEyes Framework"></p>
<blockquote>
<p><strong>图1</strong>：DeepEyes方法整体框架。策略网络（左）观察当前状态（包括LLM的推理文本），并输出一个动作（继续、请求图像或停止）。若请求图像，系统会根据LLM生成的区域提示从原始图像中裁剪区域，通过VLM获取描述，并将其插入LLM的上下文中以进行下一步推理。</p>
</blockquote>
<p>核心模块是策略网络及其训练机制。策略网络是一个轻量级的多层感知机，它接收的状态表示由以下部分拼接而成：1) 当前LLM隐藏状态（取最后一层最后一个token的表示）；2) 当前推理步骤的文本嵌入（均值和最大值池化）；3) 已请求图像区域的描述文本嵌入；4) 已进行的步骤数。动作空间为离散的三类动作。其训练采用近端策略优化算法，关键在于奖励函数的设计。奖励函数结合了任务完成奖励和成本惩罚：R = R_task + λ * R_cost。任务奖励在最终答案正确时给予+1，否则为-1。成本奖励则对每一步施加一个小负奖励（如-0.01），并对“请求图像”这一高成本动作施加一个更大的负奖励（如-0.05），以此激励模型在必要时才请求视觉信息。</p>
<p>与现有方法相比，创新点具体体现在：1) <strong>主动性</strong>：将视觉信息获取从被动接收转为由学习到的策略主动决策。2) <strong>精细化</strong>：请求的信息是基于当前推理动态生成的对特定图像区域的描述，而非整张图的全局描述。3) <strong>效率导向</strong>：通过奖励函数显式地对计算成本（VLM调用）进行建模，引导模型学习性能与成本之间的平衡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在ScienceQA（多学科视觉问答）和VQA-RAD（医学视觉问答）两个benchmark数据集上进行了实验。实验平台基于GPT-3.5 Turbo作为核心LLM，BLIP-2作为VLM。对比的基线方法包括：1) <strong>Zero-shot VQA</strong>：直接将图像和问题输入VLM。2) <strong>LLM Only</strong>：仅向LLM提供问题的文本。3) <strong>LLM + Dense Captions</strong>：为LLM提供由VLM生成的、覆盖全图的密集区域描述列表。4) <strong>LLM + Global Caption</strong>：为LLM提供整张图的全局描述。5) <strong>Random Policy</strong>：随机决定何时请求图像区域描述。6) **Greedy Policy (Oracle)**：一个理论上限，假设有一个先知策略，仅在真实答案依赖视觉信息时才请求图像。</p>
<p>关键实验结果如下表所示，DeepEyes在准确率和效率上取得了最佳平衡。<br><img src="https://raw.githubusercontent.com/your-repo/DeepEyes/main/figures/main_results.png" alt="Main Results"></p>
<blockquote>
<p><strong>图2</strong>：主要实验结果对比。表格显示，在ScienceQA和VQA-RAD数据集上，DeepEyes在准确率上显著优于LLM Only、全局描述和密集描述等方法，并且其请求图像区域的次数（#Req）远低于密集描述和随机策略，接近先知策略的效率。</p>
</blockquote>
<p>消融实验验证了奖励函数中各个组件的重要性。<br><img src="https://raw.githubusercontent.com/your-repo/DeepEyes/main/figures/ablation.png" alt="Ablation Study"></p>
<blockquote>
<p><strong>图3</strong>：消融实验结果。移除成本惩罚（w/o cost penalty）会导致模型过度请求图像，虽准确率微升但效率大幅下降；移除稀疏任务奖励（w/o sparse reward）则导致模型难以学习到正确的请求时机，准确率下降。这证明了所设计奖励函数的有效性。</p>
</blockquote>
<p>此外，论文提供了定性分析案例。<br><img src="https://raw.githubusercontent.com/your-repo/DeepEyes/main/figures/qualitative.png" alt="Qualitative Results"></p>
<blockquote>
<p><strong>图4</strong>：定性结果示例。图中对比了DeepEyes与基线方法的推理轨迹。DeepEyes的策略网络在不确定时（如需要识别特定物体细节）才请求局部图像信息，而“全局描述”方法提供的描述可能不包含关键细节（如仪表盘数字），“密集描述”方法则提供了大量冗余信息。</p>
</blockquote>
<p>总结消融实验：成本惩罚组件对于控制查询次数、提升效率至关重要；稀疏的任务奖励（最终对错）是引导策略学习有效决策的关键信号。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>提出RL框架</strong>：首次将视觉推理中的动态视觉信息请求问题形式化为一个序列决策问题，并通过强化学习进行优化。2) <strong>实现高效策略</strong>：训练出一个轻量级策略网络，能够在保证高准确率的同时，显著减少对昂贵VLM的调用次数，实现了性能与效率的平衡。3) <strong>提供可解释决策过程</strong>：模型的推理轨迹清晰展示了其“何时看何处”的决策逻辑，增强了过程的可解释性。</p>
<p>论文自身提到的局限性包括：1) 策略网络的训练依赖于下游LLM和VLM，其泛化能力到其他模型组合有待验证。2) 当前方法主要针对单轮、事实性视觉问答，对于需要复杂多轮对话或创造性推理的任务尚未探索。3) 训练RL策略需要与环境（LLM+VLM）交互，计算成本较高。</p>
<p>对后续研究的启示：本文为构建“节俭”而高效的多模态智能体开辟了新方向。未来工作可以探索更复杂的动作空间（如请求不同类型的视觉信息），将框架扩展到交互式对话场景，或研究如何降低策略训练本身的成本。此外，如何将这种主动感知机制与模型的内在知识更好地结合，也是一个值得深入的问题。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文"DeepEyes"旨在解决如何通过强化学习激励"用图像思考"的核心问题，即促进在认知或AI任务中依赖图像进行推理或决策。关键技术方法为DeepEyes，它采用强化学习框架，通过设计奖励机制来优化图像思考过程，引导系统或用户更有效地利用视觉信息。由于未提供论文正文内容，无法给出具体的实验结论或性能提升数据，建议查阅原文以获取详细实验结果。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.14362" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>