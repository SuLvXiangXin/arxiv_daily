<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.04813" target="_blank" rel="noreferrer">2512.04813</a></span>
        <span>作者: Gao Huang Team</span>
        <span>日期: 2025-12-04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习方法虽然前景广阔，但其实际部署从根本上受限于数据稀缺。尽管已有工作致力于收集大规模数据集，但在实现鲁棒的空间泛化方面仍存在显著差距。当前主流的数据收集范式是静态的：每条专家轨迹，无论其长度如何，通常都是在环境单一、固定的空间配置下收集的，包括静止的物体和目标位置以及不变的相机视角。这严重限制了可用于学习的空间信息多样性，导致训练出的策略在面对新的物体姿态、目标位置或相机视角时泛化能力急剧下降。如表I所示，随着空间维度（物体位置、目标位置、相机视角）的增加，策略的成功率呈指数级下降。</p>
<p>本文针对这一数据效率的关键瓶颈，提出了一个新颖的视角：挑战静态数据收集范式本身。其核心思路是，在收集每条专家演示轨迹时，主动、持续地移动场景中的关键物体（如抓取对象、目标对象）和相机，从而将一种强大的数据增强形式直接嵌入到每个轨迹中，使单个轨迹能够编码一个连续变化的空间配置片段，而非单个点，以此显著提升每个轨迹的空间信息密度和多样性。</p>
<h2 id="方法详解">方法详解</h2>
<p>MOVE的整体框架是一个创新的数据收集流程，其核心在于“空间配置增强”策略，而非特定的策略学习算法。流程分为两步：首先，按照MOVE范式收集动态演示数据；其次，使用这些数据训练策略模型（本文采用Diffusion Policy）。</p>
<p>核心模块是空间配置增强策略，旨在为每条轨迹注入运动，以覆盖更丰富的空间配置。具体技术细节如下：</p>
<ol>
<li><strong>物体平移</strong>：为确保覆盖整个工作空间，为抓取对象和目标对象模拟线性运动轨迹。物体在时间t的位置由其初始位置、恒定速度向量和运动方向决定。速度从Beta分布中采样，参数设置为α=2, β=5，这使得速度更可能接近0而非最大值，有助于模型学习鲁棒的抓取策略。</li>
<li><strong>物体旋转</strong>：为提高对物体朝向的泛化能力，引入绕垂直轴的恒定角速度旋转。旋转角度随时间线性变化，角速度同样从Beta分布采样。此增强特别适用于非对称物体（如带把手的杯子）。</li>
<li><strong>相机运动</strong>：为模拟非静态视角，虚拟相机沿相对于场景中心的受限圆柱形路径移动。其位置更新方式与物体平移类似。</li>
<li><strong>组合增强策略</strong>：并非同时应用所有运动，而是根据任务的语义阶段采用分阶段策略。例如，在“关盒子”任务中，轨迹分为抓取阶段和放置阶段。在抓取阶段，仅对抓取对象（盒盖）应用平移和旋转，并引入相机运动；在放置阶段，仅对目标对象（盒身）应用线性平移，并继续动态相机运动。这种策略将MOVE扩展到多个空间维度。</li>
</ol>
<p>与现有方法（如ADC）相比，MOVE的创新点在于其“连续性”。ADC在一条轨迹中仅包含几个离散的物体位置重置点，而MOVE通过持续运动，在位置空间中形成一条连续曲线，从而捕获了更密集、更丰富的空间信息，并能自然地扩展到相机运动等更多维度。</p>
<p><img src="https://arxiv.org/html/2512.04813v1/pic/dynamic.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MOVE数据收集范式概览。（左）静态范式与MOVE的概念性对比。静态范式从离散、固定的空间配置中采样，每条轨迹代表空间配置空间中的一个点。而MOVE收集的每条轨迹被视为一个连续片段，物体、目标和相机都在运动中，从而在单条轨迹内产生密集且多样的空间配置集合。（右）展示了MOVE中采用的几种运动增强形式：平移、旋转和相机运动。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>Benchmark/数据集</strong>：仿真实验使用Meta-World基准测试，选取了10个具有不同难度的操作任务（如Pick-Place, Assembly, Box-Close等）。真实世界实验使用Agilex PIPER机械臂进行“抓取-放置”任务（抓取橙子放到托盘）。</li>
<li><strong>Baseline方法</strong>：1) 传统的静态数据收集范式；2) ADC方法（一种在轨迹中周期性重置物体位置的对抗性数据收集方法）。</li>
<li><strong>评估指标</strong>：主要报告任务成功率。在真实实验中还报告了归一化得分（将任务分解为多个步骤进行评分）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>初步实验（空间泛化可视化）</strong>：在Pick-Place任务中，从9个离散点稀疏采样。尽管MOVE和静态方法在相同的抓取点进行抓取，且总时间步数相同，但MOVE展现出远优于静态方法的泛化能力。<br><img src="https://arxiv.org/html/2512.04813v1/pic/static.png" alt="稀疏采样对比"></p>
<blockquote>
<p><strong>图1</strong>：在9个训练点上稀疏采样后，策略在整个空间上的成功率热图。静态方法仅在训练点附近成功（成功率29.5%），而MOVE能在整个空间的大部分区域成功（成功率80.8%），泛化能力显著更强。</p>
</blockquote>
</li>
<li><p><strong>密集采样与圆形采样实验</strong>：在密集均匀采样设置下，MOVE将成功率从66%提升至74%。当采样点限制在一个圆环上时，MOVE不仅在圆环内（44% vs 21%），在圆环外区域（67% vs 45%）也显著优于静态方法。<br><img src="https://arxiv.org/html/2512.04813v1/pic/static_90p.png" alt="密集采样对比"></p>
<blockquote>
<p><strong>图3</strong>：密集均匀采样下的泛化对比。MOVE（右）相比静态方法（左）在状态空间难以学习的区域取得了实质性改进，整体成功率从66%提升至74%。<br><img src="https://arxiv.org/html/2512.04813v1/pic/static-circle.png" alt="圆形采样对比"><br><strong>图5</strong>：采样点限制在圆形区域时的泛化对比。MOVE（右）不仅在圆形区域内表现更好，在圆形区域外也显著优于静态方法（左），展示了其强大的外推泛化能力。</p>
</blockquote>
</li>
<li><p><strong>仿真环境主实验</strong>：在10个Meta-World任务上，MOVE平均成功率达到39.1%，相比静态范式（22.2%）相对提升76.1%。在数据效率方面，MOVE仅用20k时间步的动态数据集就能达到静态方法50k甚至100k时间步数据集的性能，在某些任务上实现了2-5倍的数据效率提升。<br><img src="https://arxiv.org/html/2512.04813v1/x1.png" alt="数据缩放曲线"></p>
<blockquote>
<p><strong>图4</strong>：10个仿真任务上，不同数据规模（时间步数）下的成功率曲线。MOVE（橙色）在所有数据规模点上都一致且显著地优于静态范式（蓝色）。</p>
</blockquote>
</li>
<li><p><strong>真实环境实验</strong>：在高度随机化的真实世界抓取-放置任务中，仅使用35k时间步的MOVE数据，策略成功率达到23.3%，远超同等数据规模的静态方法（3.3%），并与使用超过两倍数据（75k）的静态方法性能（23.3%）相当。</p>
</li>
<li><p><strong>消融实验</strong>：研究了不同动态维度组合的影响。结果表明，组合多个动态维度（如同时移动抓取对象、目标对象和相机）比仅使用单个动态维度能带来更显著的性能提升，验证了组合策略的有效性。<br><img src="https://arxiv.org/html/2512.04813v1/x12.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：在Pick-Place和Assembly任务上，不同动态维度组合的消融研究结果。“All”表示组合所有相关动态维度，其性能显著优于仅使用单个维度（如仅移动抓取对象“O_pick”或仅移动相机“Camera”）。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出MOVE范式</strong>：首次系统性地提出并验证了通过在数据收集过程中注入连续运动来增强空间信息密度的思想，挑战了传统的静态数据收集范式。</li>
<li><strong>实现显著的性能与效率提升</strong>：在仿真和真实环境中，MOVE大幅提升了策略的空间泛化能力（平均相对提升76.1%）和数据效率（可达2-5倍）。</li>
<li><strong>提供可扩展的增强策略</strong>：设计了包括平移、旋转、相机运动及其分阶段组合的增强方法，可灵活应用于多维度空间变化。</li>
</ol>
<p><strong>局限性</strong>：论文提到，动态轨迹通常比静态轨迹更长，这可能会略微增加单条轨迹的收集时间。此外，在真实世界中，需要人类演示者适应并操作运动中的物体，这可能带来一定挑战。</p>
<p><strong>对后续研究的启示</strong>：本文的工作将研究焦点从纯粹的模型设计转向了数据收集过程本身。它表明，通过精心设计的数据收集策略，即使数据集规模不变，也能极大提升学习到的策略的泛化能力。这为构建更高效、更通用的机器人学习系统开辟了一条新的路径，即“动态数据收集”。未来的工作可以探索更智能的运动规划、结合语义信息的增强策略，以及将MOVE思想与其他数据生成方法（如仿真、3D重建）相结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习中因静态数据收集导致空间泛化能力不足的问题，提出MOVE数据收集范式。其核心是在单条演示轨迹中为可移动物体注入运动，从而隐式生成密集多样的空间配置，提升数据效率。实验表明，在需要强空间泛化的模拟任务中，MOVE平均成功率达39.1%，较静态方法（22.2%）相对提升76.1%，并在部分任务上实现2–5倍的数据效率增益。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.04813" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>