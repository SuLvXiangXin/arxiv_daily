<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.14689" target="_blank" rel="noreferrer">2512.14689</a></span>
        <span>作者: Chen, Sirui, Cao, Zi-ang, Luo, Zhengyi, Castañeda, Fernando, Li, Chenran, Wang, Tingwu, Yuan, Ye, Fan, Linxi &#34;Jim&#34;, Liu, C. Karen, Zhu, Yuke</span>
        <span>日期: 2025/12/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>近年来，基于强化学习（RL）的运动跟踪方法使人形机器人实现了敏捷的全身运动技能，如行走、奔跑和后空翻。然而，在需要施加可控、一致交互力的操纵任务（如移动物体、擦拭、推车）上，人形机器人仍然面临挑战。传统桌面机械臂通常使用基于模型的顺应力控制器（如阻抗或导纳控制）来解决此问题。但现代人形机器人的敏捷性主要源于RL控制器，而RL缺乏一种通用的、可泛化的顺应力控制框架。</p>
<p>现有尝试将顺应控制集成到人形机器人中的方法主要面临两大局限：一是如UniFP、FACET等方法，需要生成大量模拟弹簧-阻尼器末端执行器动力学的合成数据，这对于复杂的人体运动分布而言非常困难；二是如SoftMimic、GentleHumanoid等方法，试图通过修改参考运动或奖励函数来鼓励顺应行为，但这往往会导致优化冲突，牺牲运动跟踪的准确性或敏捷性。</p>
<p>本文针对“如何在保持人形机器人敏捷运动所需的高刚度跟踪的同时，集成任务所需的可变顺应性”这一核心痛点，提出了一种新视角：将参考运动后验地（in hindsight）解释为机器人对扰动的顺应响应。其核心思路是：在训练期间，通过“后见扰动”修改策略观察到的稀疏跟踪目标，而保持原始参考运动不变以提供高质量、密集的跟踪奖励，从而无需修改参考轨迹即可学习自适应顺应控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>CHIP是一个即插即用的模块，可集成到现有的基于关键点（3点：头部和双腕）的运动跟踪框架中。其目标是使机器人能够跟踪动态参考运动，同时根据输入的连续顺应系数（柔度，compliance coefficient，即 1/k），以期望的刚度水平对外部扰动做出顺应反应。</p>
<p><img src="https://arxiv.org/html/2512.14689v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：CHIP训练与部署概览。策略默认输入包括跟踪目标、末端执行器顺应系数、状态历史（本体感知和过去动作）。对于局部跟踪策略，还提供一个由运动规划器生成的下半身参考姿态。策略根据输入的顺应系数和从本体感知历史中隐式估计的力来调整末端执行器顺应性。训练时，策略观察的是移除了后见扰动的恢复后关键点姿态，而奖励则根据原始参考运动计算。训练后的策略可通过VR遥操作、运动规划器或视觉-语言-动作（VLA）模型等多种接口控制。</p>
</blockquote>
<p><strong>核心训练机制：后见扰动</strong><br>与现有方法修改奖励计算中的参考目标（图3b）或编辑整个运动学参考轨迹（图3c）不同，CHIP的核心创新在于修改策略的观察输入。具体流程如下：</p>
<ol>
<li><strong>施加扰动</strong>：在训练期间，随机对末端执行器施加一个随机方向和时长的扰动力 <strong>f</strong>。</li>
<li><strong>生成后见目标</strong>：策略观察到的跟踪目标被修改为 <strong>g_hind_ptb = g - (1/k) * f</strong>。这意味着从策略的视角看，原始的跟踪目标 <strong>g</strong> 被解释为是机器人在承受扰动力 <strong>f</strong> 后，以柔度 1/k 做出顺应反应所应达到的最终位置。</li>
<li><strong>计算奖励</strong>：奖励函数 <strong>r(s, g)</strong> 仍然基于<strong>原始</strong>的参考运动目标 <strong>g</strong> 进行计算。这鼓励策略在存在扰动的情况下，仍能产生与原始高质量参考运动分布一致的动作。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.14689v2/x3.png" alt="方法对比"></p>
<blockquote>
<p><strong>图3</strong>：自适应顺应跟踪策略训练的不同方案对比。(a) 本文方法（CHIP）：策略观察后见跟踪目标（原始目标减去扰动效应）。(b) 如GentleHumanoid：修改用于奖励计算的关键点目标。(c) 如SoftMimic：编辑整个运动学参考轨迹以考虑末端扰动。CHIP方案避免了修改密集的参考运动。</p>
</blockquote>
<p><strong>策略架构与细节</strong></p>
<ul>
<li><strong>输入</strong>：策略 π 的输入包括后见跟踪目标 <strong>g_hind_ptb</strong>、末端执行器顺应系数 1/k、过去10步的本体感知状态 <strong>s</strong> 和动作 <strong>a</strong>。</li>
<li><strong>输出</strong>：关节动作。</li>
<li><strong>力感知</strong>：与UniFP/FACET等显式学习力估计器的方法不同，CHIP的策略通过历史本体感知和动作输入，<strong>隐式地</strong>获取对外部力的感知能力。为进一步提升训练时对扰动的敏感性，评论家（critic）被提供了真实的外部力作为特权信息。</li>
<li><strong>阻尼模型</strong>：为实现平滑的顺应跟随，类似于FACET，CHIP在目标末端执行器姿态上实现了一个阻尼模型。当末端执行器姿态 <strong>x_eef</strong> 因受力偏离前一目标 <strong>g_{t-1}</strong> 时，新的目标按 <strong>g_t = α * x_eef + (1-α) * g_{t-1}</strong> 更新，使目标能平滑地适应力引起的偏差。</li>
</ul>
<p><strong>创新点总结</strong></p>
<ol>
<li><strong>观察空间修改而非奖励修改</strong>：将顺应性学习转化为一个观察空间的问题，避免了修改密集奖励函数带来的优化冲突和运动编辑难题。</li>
<li><strong>保持参考运动完整性</strong>：奖励始终基于原始参考运动计算，确保了学习到的策略能产生自然、高质量的运动。</li>
<li><strong>在线训练与强泛化</strong>：扰动和观察目标的修改可在训练过程中在线进行，无需离线的运动数据增强流程，并使策略能泛化到训练数据分布之外的观察。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用Unitree G1人形机器人平台，头部搭载OAK-D W相机提供视觉。所有策略在64块NVIDIA L40S GPU上训练4天。评估了局部3点跟踪策略和全局3点跟踪策略。</p>
<p><strong>对比基线</strong>：</p>
<ul>
<li><strong>FALCON</strong>：训练时施加力扰动，但无顺应性命令，旨在抵抗扰动。</li>
<li><strong>GentleHumanoid</strong>：通过奖励塑形实现顺应行为。</li>
<li><strong>No perturb force</strong>：完全不使用力扰动训练。</li>
<li>**Ours (不同1/k值)**：CHIP方法在不同顺应系数下的表现。</li>
</ul>
<p><strong>关键实验结果</strong></p>
<p><img src="https://arxiv.org/html/2512.14689v2/x1.png" alt="跟踪误差表"></p>
<blockquote>
<p><strong>表I</strong>：无外力扰动时，在MuJoCo仿真中对TWIST数据集运动轨迹的跟踪误差。CHIP在不同顺应系数（1/k）下，其位置跟踪精度与无顺应性的FALCON及无扰动基线相当，仅对方向跟踪有轻微影响。这表明策略能区分内部关节驱动力和外部扰动力。CHIP的跟踪精度显著高于通过牺牲密集关节奖励来实现顺应性的GentleHumanoid。</p>
</blockquote>
<p><strong>Q1 &amp; Q2: 跟踪性能与遥操作应用</strong><br>CHIP在保持高精度运动跟踪的同时，实现了可调的末端顺应性。如图7所示，通过VR遥操作，同一控制器可完成多种接触丰富的任务：既需要顺应性的任务（如推车、搬箱、擦拭），也需要大力度的任务（如开门、翻转重箱）。用户可在线实时调整双手不同的顺应系数，例如，一只手刚性握住白板（1/k=0.001），另一只手以顺应模式（1/k=0.05）书写。</p>
<p><img src="https://arxiv.org/html/2512.14689v2/x7.png" alt="遥操作应用"></p>
<blockquote>
<p><strong>图7</strong>：(a) 在线自适应顺应遥操作示例，展示了在复杂任务中动态调整顺应性的价值。(b) 精细调整顺应性至“刚好够硬”的水平以提升任务表现。(c) 不当的顺应性命令会导致失败，说明了连续可调顺应性空间的重要性。</p>
</blockquote>
<p><strong>Q3: 多机器人协作</strong><br>基于CHIP训练的全局3点跟踪策略，结合扩展的SpringGrasp算法，实现了双机器人协同抓取与搬运。</p>
<p><img src="https://arxiv.org/html/2512.14689v2/x5.png" alt="多机器人抓取"></p>
<blockquote>
<p><strong>图5</strong>：双机器人抓取与物体搬运实验流程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.14689v2/x6.png" alt="抓取失败案例"></p>
<blockquote>
<p><strong>图6</strong>：使用非顺应控制器（FALCON）进行协同抓取时，因接触不稳定导致的失败案例。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.14689v2/x1.png" alt="抓取成功率表"></p>
<blockquote>
<p><strong>表II</strong>：不同物体和高度下的双机器人抓取成功率。使用CHIP（1/k=0.04）平均成功率达80%，比始终刚性的控制器高75%，比无扰动训练的控制器高40%。刚性控制器因施加过大推力导致物体失稳，表现最差。</p>
</blockquote>
<p><strong>Q4: 自主VLA策略</strong><br>利用CHIP遥操作收集的配对数据（动作、顺应系数、视觉）对GR00T N1.5 VLA模型进行微调，使其能自主完成需要不同末端刚度的接触丰富任务，如图8所示的单臂擦拭、持板擦拭和双手拾放。</p>
<p><img src="https://arxiv.org/html/2512.14689v2/figures/vla_tasks.png" alt="自主VLA任务"></p>
<blockquote>
<p><strong>图8</strong>：微调后的VLA策略自主执行的三种接触丰富任务，均需不同的末端刚度设置。</p>
</blockquote>
<p><strong>Q5: 连续顺应命令空间优势</strong><br>实验表明（图7b,c），连续可调的顺应系数空间允许操作者或高层策略将刚度精细调整至“任务所需的最小刚度”，从而在施加足够力（如举起重物）和保持接触稳定性之间取得最佳平衡，这是离散或固定刚度控制器无法实现的。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>CHIP</strong>，一种通用、可扩展的即插即用模块，通过“后见扰动”机制，在不牺牲运动跟踪性能的前提下，为人形机器人运动跟踪框架引入了自适应顺应控制。</li>
<li>基于CHIP，开发了<strong>顺应且自然的局部3点跟踪控制器</strong>，解锁了有力的全身遥操作和自主VLA策略学习；以及<strong>顺应全局3点跟踪控制器</strong>，实现了稳定的多机器人协同抓取与搬运。</li>
<li>展示了<strong>连续顺应性命令空间</strong>在复杂、长视野操纵任务中的关键价值，允许实时、精细的刚度调节。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在多机器人抓取的一些失败案例中，原因是机器人的膝盖在抓取前与物体发生碰撞，这并未完全展现控制器在接触顺应性方面的能力。这表明底层运动规划或高层任务规划仍需改进以避免此类非期望接触。此外，自主VLA策略的性能依赖于遥操作收集的数据质量和多样性。</p>
<p><strong>启示</strong>：CHIP提供了一种将基于模型的阻抗控制思想优雅地嵌入数据驱动的RL训练范式的新途径。其“修改观察而非修改奖励/参考”的核心思路，为解决其他需要平衡多个竞争目标的机器人学习问题（如安全约束下的敏捷运动）提供了新的方法论启示。未来工作可探索将顺应性命令与更高层的任务理解相结合，实现完全自主的刚度调节。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在敏捷运动与柔顺操作之间的矛盾，提出CHIP模块。该方法通过**后视扰动**技术，将参考运动解释为机器人对扰动的柔顺响应，从而在保持高精度运动跟踪的同时，实现**末端执行器刚度的自适应控制**。该方法无需数据增强或奖励调整。实验表明，集成CHIP的通用运动跟踪控制器能成功完成多种需不同柔顺性的操作任务，如多机器人协作、擦拭、运送箱子和开门。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.14689" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>