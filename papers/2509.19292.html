<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19292" target="_blank" rel="noreferrer">2509.19292</a></span>
        <span>作者: Cewu Lu Team</span>
        <span>日期: 2025-09-23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人策略的自我改进旨在使机器人通过主动探索环境、收集经验来提升性能，超越初始专家示范的局限。然而，模仿学习得到的策略常因动作模式崩溃而缺乏探索能力。现有鼓励探索的方法（如在动作空间直接添加随机扰动或噪声）虽然能产生新行为，但在高维连续动作空间（尤其是分块动作表示下）通常效率低下，且会导致不稳定、不安全的抖动行为，限制了其在实际部署中的有效性。</p>
<p>本文针对上述探索不安全、低效的痛点，提出了“流形上探索”的新视角。核心思路是学习一个紧凑的、仅包含任务相关信息的潜在表示，并将探索约束在该有效动作构成的流形上，从而确保安全性、多样性和有效性。该方法可作为一个即插即用模块与任意策略模型集成，在不损害基础策略性能的前提下增强探索能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>SOE框架旨在实现安全、高效、结构化的策略自我改进。其核心在于学习一个任务流形的紧凑潜在表示，并在此流形上进行探索。</p>
<p><img src="https://arxiv.org/html/2509.19292v1/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：SOE方法总览。通过将探索约束在有效动作的流形上，该方法能产生多样且时序一致的行为，实现结构化、高效的探索。收集的交互数据用于精炼策略，实现高效的自我改进。</p>
</blockquote>
<p><strong>整体框架与双路径架构</strong>：SOE被设计为一个可即插即用的探索模块，与基础模仿学习策略（本文实例化为扩散策略）联合优化。如图2所示，其采用双路径架构：</p>
<ul>
<li><strong>基础路径</strong>：负责稳定的策略执行。观测编码器 $\mathcal{E}$ 将观测 $o_t$ 编码为嵌入 $c_t$，动作解码器 $\mathcal{D}$ 直接根据 $c_t$ 生成动作块 $a_{t:t+H}$。</li>
<li><strong>探索路径</strong>：负责生成多样化的动作提议。观测嵌入 $c_t$ 首先通过潜在编码器 $p_{\theta}$ 映射到潜在空间，得到高斯分布的参数 $(\mu_t, \sigma_t)$；随后从中采样得到扰动后的潜在变量 $z_t$；最后通过潜在解码器 $q_{\phi}$ 将 $z_t$ 解码回修改后的观测嵌入 $\tilde{c}<em>t$，再输入基础路径的动作解码器 $\mathcal{D}$ 生成探索性动作 $\tilde{a}</em>{t:t+H}$。超参数 $\alpha$ 控制探索尺度。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.19292v1/x2.png" alt="双路径架构"></p>
<blockquote>
<p><strong>图2</strong>：SOE的双路径架构。观测嵌入通过两条并行路径处理：上方的基础路径负责稳定策略执行；下方的探索路径通过在潜在空间中进行结构化扰动来生成多样化动作。</p>
</blockquote>
<p><strong>核心模块：基于变分信息瓶颈的流形学习</strong>。探索路径的核心是学习一个紧凑的潜在表示 $Z$。其编码器 $p_{\theta}(z|o)$ 被优化以最大化 $Z$ 与动作 $A$ 的互信息，同时最小化 $Z$ 与观测 $O$ 的互信息（通过系数 $\beta$ 权衡）。这驱使 $Z$ 仅保留对动作预测至关重要的信息，过滤掉观测中的无关细节。通过变分信息瓶颈推导出可优化的损失函数 $\mathcal{L}<em>{\text{IB}}$：<br>$$\mathcal{L}</em>{\text{IB}}(\theta,\phi) = \mathbb{E}<em>{(o,a)\sim\mathcal{D}}[-\mathbb{E}</em>{z\sim p_{\theta}(z|o)}\log q_{\phi}(a|z) + \beta\text{KL}[p_{\theta}(Z|o)||r(Z)]]$$<br>其中 $q_{\phi}(a|z)$ 是动作解码器，$r(Z)=\mathcal{N}(0,I)$ 是标准高斯先验。第一项（$\mathcal{L}<em>{\text{IB(IL)}}$）确保潜在变量能重建动作，第二项（$\mathcal{L}</em>{\text{IB(KL)}}$）规范潜在分布接近先验，鼓励解耦。</p>
<p><strong>联合训练</strong>：整体训练目标结合了基础路径的模仿损失和探索路径的VIB损失：$\mathcal{L} = \mathcal{L}<em>{\text{IL}}(\psi) + \mathcal{L}</em>{\text{IB}}(\theta, \phi)$。关键设计是 $\mathcal{L}<em>{\text{IB}}$ 只更新探索模块的参数 $(\theta, \phi)$，而 $\mathcal{L}</em>{\text{IL}}$ 只更新基础策略的参数 $(\psi)$，从而确保探索能力的引入不会降低基础策略的性能。</p>
<p><strong>用户引导探索</strong>：得益于 $\mathcal{L}_{\text{IB}}$ 鼓励的解耦特性，潜在空间 $Z$ 的不同维度倾向于对应不同的任务相关因子。通过计算各维度的信噪比 $\text{SNR}_i = \text{Var}(\mu_i) / \mathbb{E}[\sigma_i^2]$，可以识别出信息丰富的维度。用户可以在这些维度上进行有针对性的扰动，并通过最远点采样从生成的大量动作提议中选择具有代表性的子集进行人工筛选，从而引导探索朝向期望的行为，进一步提高样本效率和可控性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准</strong>：在三个真实世界任务（Mug Hang, Toaster Load, Lamp Cap）和四个模拟任务（来自RoboMimic的Lift, Can, Square, Transport）上评估。</li>
<li><strong>策略与基线</strong>：基础策略为扩散策略。对比基线包括：无显式探索的<strong>扩散策略</strong>，以及通过向扩散条件注入随机噪声进行探索的<strong>SIME</strong>方法。</li>
<li><strong>评估指标</strong>：任务成功率、探索有效性（Pass@5，即同一初始条件下5次尝试中至少成功一次的概率）、样本效率（收集成功经验所需的交互轮次数）、运动平滑性/安全性（末端执行器轨迹的平均加加速度Jerk）。</li>
</ul>
<p><strong>真实世界任务结果</strong>：如表I所示，SOE在所有任务上均一致优于基线。</p>
<ul>
<li><strong>有效性</strong>：SOE的Pass@5最高（例如Toaster Load任务达0.94），表明其探索更有效。经过一轮自我改进后，成功率获得显著提升（相对提升19.1%到38%）。结合用户引导后，提升进一步加大（相对提升40.4%到62%）。</li>
<li><strong>安全性与平滑性</strong>：SOE产生的动作平均Jerk值与基础DP相当，且增幅远小于SIME（例如Mug Hang任务，SOE的Jerk仅增0.23，而SIME增1.80），证明其探索更平滑、安全。</li>
<li><strong>样本效率</strong>：SOE达到高Pass@5所需的交互轮次数最少。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.19292v1/figures/mug_hang_compare/135122079702_screenshot_11.09.2025.png" alt="定性比较"><br><img src="https://arxiv.org/html/2509.19292v1/figures/mug_hang_compare/135122079702_screenshot_11.09.2025_sime_0.2.png" alt="SIME结果"><br><img src="https://arxiv.org/html/2509.19292v1/figures/mug_hang_compare/135122079702_screenshot_11.09.2025_ours.png" alt="SOE结果"></p>
<blockquote>
<p><strong>图4</strong>：Mug Hang任务中，不同方法在相同初始观测下生成的动作提议（末端执行器轨迹）定性比较。基础DP轨迹单一；SIME的轨迹多样但杂乱、超出合理范围；SOE的轨迹既多样又集中分布在任务相关的合理区域（流形上）。</p>
</blockquote>
<p><strong>模拟任务结果</strong>：在少样本示范设置下，SOE经过多轮自我改进，其性能持续提升并稳定优于基线，证明了方法的鲁棒性和迭代改进能力。</p>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2509.19292v1/x10.png" alt="消融研究"></p>
<blockquote>
<p><strong>图10</strong>：消融实验验证各组件贡献。移除KL项（$\beta=0$）或使用固定标准差（$\sigma$固定）都会导致性能下降，证明了学习流形几何结构的重要性。在观测嵌入空间（$C$空间）添加噪声的性能也较差，说明在原始高维、可能纠缠的空间中扰动效果不佳。</p>
</blockquote>
<ul>
<li><strong>潜在空间设计</strong>：移除KL正则项（$\beta=0$）或使用固定标准差（而非学习的$\sigma_t$）均会导致性能下降，验证了学习流形局部几何的必要性。</li>
<li><strong>扰动空间选择</strong>：直接在观测嵌入空间（$C$空间）添加噪声，其性能远差于在SOE学习的潜在空间（$Z$空间）扰动，证明了学习紧凑、解耦表示对有效探索的关键作用。</li>
<li><strong>信噪比分析</strong>：<br><img src="https://arxiv.org/html/2509.19292v1/x11.png" alt="信噪比分析"></li>
</ul>
<blockquote>
<p><strong>图11</strong>：潜在空间各维度的信噪比分析。信噪比呈现清晰的双峰分布，高SNR维度编码了任务相关信息（如物体位置），低SNR维度则已坍缩，这为识别有效维度以进行用户引导提供了依据。</p>
</blockquote>
<p>潜在空间各维度的SNR呈现明显分离，高SNR维度被验证与任务相关因子（如物体位置）对应，而低SNR维度已坍缩，这支持了基于SNR选择引导维度的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了“流形上探索”的原则性方法，通过变分信息瓶颈学习任务相关的紧凑潜在表示，将探索约束在有效动作流形上，实现了安全、多样、高效的策略自我改进。</li>
<li>设计了一个即插即用的双路径架构，该模块可与现有模仿学习策略联合优化，且不损害基础策略性能。</li>
<li>利用所学潜在空间的解耦特性，实现了基于信噪比筛选和人工选择的用户引导探索，进一步提高了样本效率和可控性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，潜在空间维度的选择以及先验分布 $r(Z)$ 的形式可能会影响探索行为和解耦效果，这需要根据任务进行调整。</p>
<p><strong>研究启示</strong>：</p>
<ul>
<li>流形上探索为解决机器人学习中的安全-探索权衡提供了一个有前景的方向。</li>
<li>学习解耦的、语义化的潜在表示不仅有助于探索，也为机器人策略的可解释性和人机交互控制开辟了新途径。</li>
<li>即插即用的设计思路使得该方法易于与其他先进策略架构结合，具有较好的通用性。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SOE框架，旨在解决机器人策略因探索能力不足导致的动作模式坍塌问题。其核心方法是学习任务相关因素的紧凑潜在表示，并将探索约束在有效动作的流形上，以此保证探索的安全性、多样性与高效性。实验表明，SOE在仿真与真实任务中均优于现有方法，实现了更高的任务成功率、更平滑安全的探索过程以及更优的样本效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19292" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>