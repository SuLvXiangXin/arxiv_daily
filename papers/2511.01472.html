<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01472" target="_blank" rel="noreferrer">2511.01472</a></span>
        <span>作者: Mishra, Sarthak, Yadav, Rishabh Dev, Das, Avirup, Gupta, Saksham, Pan, Wei, Roy, Spandan</span>
        <span>日期: 2025/11/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型（如SayCan、PaLM-E、RT-2）在机器人领域展现了强大的零样本语义推理能力，但将其直接部署于空中机械臂仍存在严重问题。这些模型通常需要数据密集的训练，推理频率低，且其空间感知脆弱，容易产生幻觉或误解空间关系。更重要的是，它们生成的逐步、随机输出缺乏时间连贯性，与安全飞行所需的稳定性和控制一致性相冲突。现有研究或专注于地面机器人控制，或局限于无人机纯导航任务，未能统一感知、推理与执行以解决空中操作问题。本文针对将通用视觉-语言模型（VLM）安全、可靠地适配于空中操作这一核心痛点，提出了一种无需领域微调的新视角：将“做什么”（高层推理）与“怎么做”（底层控制）解耦。其核心思路是利用结构化提示引导预训练VLM进行可解释的逐步推理，并从一个预定义的、飞行安全的技能库中选择离散动作来执行，从而在保留大模型泛化能力的同时确保稳定、可重复的控制。</p>
<h2 id="方法详解">方法详解</h2>
<p>AERMANI-VLM的整体框架是一个将感知、推理与执行闭环的流水线。系统将用户提供的自然语言指令、当前RGB图像观测以及机器人位姿作为输入。在每个时间步，系统首先将任务指令、历史、技能库和安全规则编译成一个结构化提示，连同当前观测一起输入给预训练的VLM。VLM输出一个包含描述性推理轨迹（DRT）和选定技能（STE）的结构化响应。随后，系统从预定义技能库中调用对应的、确定性的低层控制器来执行该技能，完成一个推理-动作循环，直至任务完成。</p>
<p><img src="https://arxiv.org/html/2511.01472v1/final_imgs/arch.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：AERMANI-VLM用于视觉语言引导空中操作的流水线概述。(1) 输入：用户提供自然语言指令。(2) 提示编译：指令被编译为包含前言、推理历史、技能定义和安全规则的结构化提示。(3) VLM推理：提示与当前RGB观测一同由预训练VLM处理，输出图像描述、任务摘要、显式推理轨迹和待执行的离散技能。(4) 技能执行：选定的运动基元或感知驱动例程由确定性低层控制器执行，确保在飞行动力学下的可重复性。</p>
</blockquote>
<p>方法的核心模块包括：</p>
<ol>
<li><p><strong>结构化提示设计</strong>：提示 <code>P_t</code> 由五部分组成：(i) <strong>任务指令</strong>：定义自然语言目标。(ii) <strong>前言</strong>：描述VLM的角色、机器人平台和传感设置。(iii) <strong>历史</strong>：总结先前的推理和执行过的技能，以保持序列连续性。(iv) <strong>技能库</strong>：以语言形式列出所有有效的用户定义运动和感知技能，定义了离散动作空间。(v) <strong>规则</strong>：指定输出约束，强制要求包含DRT和STE的两部分响应。这种结构消除了查询的歧义，防止生成不可执行的命令，并将高层推理直接链接到物理上有效的控制动作。</p>
</li>
<li><p><strong>结构化输出（描述性推理轨迹 - DRT）</strong>：VLM的输出 <code>γ_t</code> 被约束为结构化、机器可解释的格式：<code>{DRT_t, STE_t}</code>。DRT作为推理记录簿，包含四个字段：(i) <strong>图像描述</strong>：描述当前场景、可见物体及其空间布局，将推理锚定在真实观测上，防止目标幻觉。(ii) <strong>摘要</strong>：回顾之前的观测和执行过的技能，提供任务进展的短期记忆。(iii) <strong>动作预测</strong>：列出所有可用技能并简短估计执行后果，迫使VLM在选择前评估整个动作集。(iv) <strong>推理</strong>：综合感知、记忆和动作预测，形成连贯的决策依据，并最终得出结论选择哪个技能。<code>STE_t</code> 则是从预定义技能库中选出的一个离散技能命令 <code>a_t ∈ A</code>。</p>
</li>
<li><p><strong>技能库</strong>：技能库 <code>A</code> 定义了空中机械臂可执行的离散例程集，分为两类：</p>
<ul>
<li><strong>运动基元</strong>：用户定义的、在机体坐标系下的姿态调整，用于主动探索、重新对准和精确位姿调整（如偏航、前后、左右、上下移动）。</li>
<li><strong>感知驱动操作例程</strong>：将感知与控制耦合的复合例程，包括<code>object_localization</code>（使用CLIPSeg进行零样本分割并估计3D位姿）、<code>grasp</code>（迭代重定位、抓取器闭环驱动）、<code>placement_localization</code>（识别放置表面并计算无碰撞放置位姿）和<code>place</code>（受控下降、释放并安全悬停）。</li>
</ul>
</li>
<li><p><strong>推理-动作执行循环</strong>：如算法1所示，系统在每个时间步循环执行：获取观测 <code>o_t</code>、构建结构化提示 <code>P_t</code>、查询VLM得到 <code>γ_t</code>、执行选定技能 <code>a_t</code>、并将DRT和 <code>a_t</code> 更新到历史缓冲区 <code>H</code> 中。这种闭环结构确保每一步推理都基于真实观测，且所有动作在部分可观测性下都是可验证且飞行安全的。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.01472v1/final_imgs/frames.jpg" alt="坐标系"></p>
<blockquote>
<p><strong>图2</strong>：AERMANI-VLM中的坐标系和空间感知。定义了全局世界坐标系 <code>T_W</code>，以及相对于机械臂本体的相机坐标系 <code>T_C^AM</code> 和夹爪坐标系 <code>T_G^AM</code>，以保持感知与控制的一致性。</p>
</blockquote>
<p>与现有方法相比，本文的创新点具体体现在：<strong>首次通过结构化提示和结构化输出，将预训练VLM的高层语义推理能力与一个保证飞行安全的、离散的底层技能执行库进行了解耦</strong>。这种设计避免了昂贵的领域微调，同时通过强制VLM输出包含过去、现在、未来评估的显式推理轨迹（DRT），显著提高了决策的可解释性、时间一致性和物理可行性，从而减少了幻觉和不安全行为。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在仿真和真实硬件上进行。仿真使用基于PX4的软件在环（SITL）配置。真实硬件平台为搭载CUAV X7+飞控（运行PX4）、ZED立体RGB-D相机和两指伺服夹爪的Tarot 650六旋翼无人机，高阶自主和感知运行在NVIDIA Jetson Orin Nano上。使用的预训练VLM是Gemini 2.5-Pro，感知模块为CLIPSeg。</p>
<p><img src="https://arxiv.org/html/2511.01472v1/final_imgs/sim.jpg" alt="仿真环境"></p>
<blockquote>
<p><strong>图3</strong>：仿真环境，由随机化布局的桌子、架子和其他家具组成的自定义室内办公室场景。</p>
</blockquote>
<p><strong>任务与基线</strong>：设计了四个渐进挑战性的任务类别：定位与识别、从杂物中拾取/歧义消解、顺序指令跟随、搜索与拾取。对比的基线方法包括：专用于无人机语言导航的UAV-VLN、基于技能 affordance 的SayCan、端到端动作预测的RT-2，以及用于无人机高阶任务推理的UAV-VLA/CognitiveDrone。所有基线均使用相同的技能库和低层控制器进行公平比较。</p>
<p><strong>关键实验结果</strong>：<br>在真实世界实验中，AERMANI-VLM成功执行了完整的拾取-放置任务。<br><img src="https://arxiv.org/html/2511.01472v1/final_imgs/exps.jpg" alt="硬件实验"></p>
<blockquote>
<p><strong>图4</strong>：真实世界硬件实验的定性结果，展示了执行指令“拾取咖啡机旁边的紫色杯子并将其放在木桌上”的完整自主执行序列，包括主动搜索、视觉引导接近与抓取、寻找目标桌子和放置物体。</p>
</blockquote>
<p>仿真实验结果（表VI）显示，完整的AERMANI-VLM在所有任务类别上均取得最佳性能，平均成功率高达87.5%，远超无DRT（15.0%）和非结构化提示（5.0%）配置。其规划低效性（PI）和决策效率（DE）指标也均为最优。</p>
<p><strong>消融实验分析</strong>：<br>消融实验（表V，VI）逐步增加结构化提示和DRT的组件，验证了每个部分的重要性。从“非结构化提示”到“无DRT”，再到DRT组件逐步增加（V1-V3），最后到完整AERMANI-VLM，任务成功率（SR）从5%稳步提升至87.5%，PI和DE持续下降。这表明：结构化输入提供了可靠的语义感知，而结构化输出（完整的DRT）则强制执行了在飞行动力学下因果且物理一致的决策序列，两者缺一不可。</p>
<p><img src="https://arxiv.org/html/2511.01472v1/final_imgs/clipseg.jpg" alt="感知流水线"></p>
<blockquote>
<p><strong>图5</strong>：用于<code>object_localization</code>和<code>placement_localization</code>的开放词汇感知流水线。给定自然语言查询，CLIPSeg在RGB图像上执行零样本分割，2D掩码用于从深度数据中提取过滤后的3D点云，从而精确计算抓取或放置位姿。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.01472v1/final_imgs/failures.jpg" alt="失败案例"></p>
<blockquote>
<p><strong>图6</strong>：缺乏结构化推理的基线方法常见的失败模式，包括因幻觉运动命令导致的碰撞（1-3），以及因<code>object_localization</code>模块位姿估计不准确导致的操作失败（4-5）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.01472v1/final_imgs/output.jpg" alt="策略输出"></p>
<blockquote>
<p><strong>图7</strong>：AERMANI-VLM的策略输出示例，展示了其生成的结构化推理（DRT）和最终的技能选择（STE），体现了决策的可解释性。</p>
</blockquote>
<p>与基线方法的对比结果（表VII）显示，AERMANI-VLM在成功率（SR）上显著优于所有对比方法，并且在规划效率（PI）和决策效率（DE）上也表现最佳。SayCan和RT-2表现中等但动作选择方差较大，而UAV-VLN和UAV-VLA由于缺乏操作能力或动作分辨率较粗，任务完成率较低。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) <strong>首次形式化</strong>了视觉-语言引导的空中操作为部分可观测下的序列决策问题，统一了自然语言推理、机载感知和任务级技能选择，且无需任何任务特定训练或微调。2) 提出了<strong>结构化输入提示</strong>，将任务意图、推理历史、技能库和安全约束编码为统一查询，实现了时间一致、上下文感知且物理有效的推理。3) 设计了<strong>结构化推理输出（DRT）</strong>，约束预训练VLM产生可解释的、有时间依据的决策轨迹，强制执行“先推理后行动”，确保一致的技能选择。</p>
<p>论文自身提到的局限性包括：依赖于预定义技能库，限制了动作空间的灵活性；VLM推理的计算开销可能限制高频控制应用；技能库的通用性需要针对更复杂的操作任务进行扩展。</p>
<p>对后续研究的启示在于：<strong>解耦高层推理与底层安全执行是可靠部署大模型于安全关键机器人系统（如空中机器人）的有效范式</strong>。未来工作可以探索如何动态扩展或组合技能，如何将更丰富的世界模型整合进推理过程，以及如何优化提示工程和推理过程以进一步降低延迟和提高鲁棒性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言模型直接驱动空中机械臂存在动作不一致、易产生幻觉且动态不可行的问题，提出AERMANI-VLM框架。其核心方法是**结构化提示**，将指令、任务上下文与安全约束编码为提示，引导模型生成**自然语言推理轨迹**，并据此从预定义的**飞行安全技能库**中选择动作，实现高层推理与底层控制的解耦。该方法无需任务微调，在仿真与硬件实验中，能**可靠完成多步骤拾放任务**，并对未见过的指令、物体和环境表现出**强泛化能力**。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01472" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>