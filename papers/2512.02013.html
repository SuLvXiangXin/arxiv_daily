<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.02013" target="_blank" rel="noreferrer">2512.02013</a></span>
        <span>作者: Shanghang Zhang Team</span>
        <span>日期: 2025-12-01</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于互联网规模预训练的视觉语言模型（VLMs），视觉语言操作（VLA）模型通过在大规模机器人演示数据上训练，展现出强大的场景理解和操作泛化能力。然而，面对需要明确定义目标状态的长视野任务，例如乐高（LEGO）组装或物体重排，现有VLA模型在协调高层规划与精确操作方面仍面临挑战。其核心困难在于，模型必须严格对齐预定义的最终场景配置，并整合长视野规划与细粒度控制，同时保持对多样化现实环境的泛化能力。</p>
<p>现有方法主要分为两类：一类是端到端的VLA模型，直接将感知输入映射到动作，缺乏对中间过程的显式规划；另一类分层方法则依赖人工提供的手册或演示视频，这限制了其对未知目标状态的泛化能力，并增加了对人的依赖。本文针对“如何让VLA模型具备从期望的‘是什么’（目标状态）推断出程序性的‘怎么做’（执行步骤）这一人类能力”这一具体痛点，提出了一个新视角：将目标状态转化为可执行的多模态手册，并以此引导精确操作。本文核心思路是构建一个基于混合专家Transformer（MoT）架构的统一VLA模型（ManualVLA），通过设计的手动思维链（ManualCoT）推理过程，实现多模态手册生成与动作执行的协同合作。</p>
<h2 id="方法详解">方法详解</h2>
<p>ManualVLA的整体框架是一个基于MoT架构的统一模型，包含规划专家和操作专家两个核心模块。给定语言指令 <code>l</code>、当前状态图像 <code>I_t_current</code> 和最终目标状态图像 <code>I_goal</code>，模型首先通过规划专家生成一个中间手册，该手册包含目标物体的文本描述 <code>l_hat_t</code>、其目标2D坐标 <code>p_t</code> 以及对应的子目标图像 <code>I_t_subgoal</code>。基于此手册，通过将目标位置作为掩码叠加到当前场景图像上，构建一个提示图像 <code>I_t_prompt</code>。最后，操作专家以机器人状态 <code>s_t</code>、提示图像 <code>I_t_prompt</code> 以及生成手册时存储的隐式特征（<code>F_t_subgoal</code>, <code>F_t_p</code>, <code>F_t_l_hat</code>）为条件，生成未来 <code>h</code> 步的动作块 <code>a_t:t+h</code>。</p>
<p><img src="https://arxiv.org/html/2512.02013v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ManualVLA框架。(a) 整体流程：规划专家根据指令、当前图像和目标图像生成多模态手册（文本、位置、子目标图像）；通过显式CoT（将位置提示叠加为视觉提示）和隐式CoT（通过跨任务共享注意力机制）引导操作专家生成动作。(b) 隐式CoT细节：通过设计的注意力掩码，使操作专家能关注手册生成的特征，实现隐式引导。(c) 三阶段训练策略：分别预训练操作专家和规划专家，最后进行联合微调。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉模块</strong>：采用双视觉路径。手册生成使用基于VQGAN的视觉分词器（编码器-量化器-解码器），将图像转换为离散token。操作生成使用连续的SigLIP-Large视觉编码器，提取高维语义特征。</li>
<li><strong>混合专家Transformer LLM</strong>：基础语言模型为DeepSeek-LLM 1.5B。在其上构建MoT架构，为非嵌入组件（如前馈网络FFN、注意力投影、层归一化）引入任务特定的参数集，形成规划专家和操作专家。每个token根据其任务类别（手册或动作）选择对应的参数进行计算，而注意力权重矩阵则在所有token间全局计算，从而在统一架构内实现任务专化与跨任务信息交互。</li>
<li><strong>动作与状态组件</strong>：采用基于扩散策略的动作建模。引入噪声编码器和解码器（均为两层MLP）来处理加噪和去噪的动作。机器人状态通过另一个两层MLP（状态编码器）注入操作专家。</li>
<li><strong>手动思维链（ManualCoT）推理</strong>：<ul>
<li><strong>显式CoT</strong>：利用生成的位置坐标 <code>(U, V)</code> 在當前圖像上疊加掩碼，生成提示图像 <code>I_t_prompt</code>，作为操作专家的明确视觉引导。</li>
<li><strong>隐式CoT</strong>：在生成手册时，存储文本、位置和子目标图像对应的键值特征（<code>F_t_l_hat</code>, <code>F_t_p</code>, <code>F_t_subgoal</code>）。在操作生成阶段，通过设计的跨任务共享注意力机制，使操作专家能够关注这些特征，从而在潜在空间中获得“操作什么物体”、“放置在哪里”以及“操作后的预期视觉结果”的隐式引导。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有VLA模型直接映射感知到动作或依赖外部规划不同，ManualVLA的创新在于：1) 首次在统一VLA框架内集成多模态手册生成与动作执行；2) 提出了结合显式（视觉提示）和隐式（潜在特征引导）的ManualCoT推理机制，将生成的手册有效转化为精确操作条件；3) 利用MoT架构在单一模型中灵活协调规划与操作两种异构任务。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>任务</strong>：2D乐高组装、3D乐高组装、物体重排。</li>
<li><strong>基准/数据集</strong>：在双臂Franka机器人平台上进行真实世界评估。同时，在RLBench基准测试通用操作任务。</li>
<li><strong>对比基线</strong>：分为三类：1) 强大动作生成范式：π0, π0.5, FAST；2) 结合目标图像和子目标图像预测的CoT-VLA；3) 分层SOTA方法：CheckManual（依赖预定义手册）和Vid2Robot（依赖人类演示视频）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在长视野任务上，ManualVLA展现出显著优势。</p>
<p><img src="https://arxiv.org/html/2512.02013v1/x4.png" alt="长视野任务成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：在2D/3D LEGO组装和物体重排任务上的成功率对比。ManualVLA在所有任务上均显著优于所有基线方法。</p>
</blockquote>
<p>具体而言，ManualVLA在2D乐高组装、3D乐高组装和物体重排任务上的平均成功率达到**79.0%<strong>，比之前的分层SOTA基线（CheckManual）平均高出</strong>32%**。与端到端VLA基线（π0.5， FAST）相比，优势更为明显。ManualVLA在RLBench通用操作任务上也达到了SOTA性能。</p>
<p><strong>消融实验</strong>：</p>
<p><img src="https://arxiv.org/html/2512.02013v1/x5.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图5</strong>：消融实验。(a) 架构消融：完整的MoT架构（ManualVLA）性能最佳，移除MoT（单专家）或移除跨任务注意力（无共享）均导致性能下降。(b) CoT机制消融：同时使用显式和隐式CoT效果最好，仅使用任一或两者都不使用都会降低成功率。</p>
</blockquote>
<p>消融实验验证了核心组件的贡献：</p>
<ol>
<li><strong>MoT架构</strong>：使用完整的MoT架构（规划与操作专家分离但共享注意力）性能最佳。移除MoT（使用单一专家处理所有任务）或移除跨任务共享注意力机制，成功率分别下降17.3%和10.7%。</li>
<li><strong>CoT机制</strong>：同时使用显式CoT（视觉提示）和隐式CoT（潜在特征引导）效果最好。仅使用显式、仅使用隐式或两者都不使用，成功率分别下降15.7%、23.0%和31.3%。</li>
<li><strong>数据效率</strong>：得益于手册条件，ManualVLA在下游任务微调时仅需约100条演示轨迹即可实现泛化操作，数据效率很高。</li>
</ol>
<p><strong>泛化能力评估</strong>：</p>
<p><img src="https://arxiv.org/html/2512.02013v1/x6.png" alt="泛化能力定性结果"></p>
<blockquote>
<p><strong>图6</strong>：ManualVLA在未见过的物体形状、背景和光照条件下的泛化能力定性示例。模型能成功处理新的乐高砖块形状、复杂的桌面背景以及变化的照明。</p>
</blockquote>
<p>ManualVLA在未见过的物体形状、背景纹理和光照条件下均表现出良好的泛化能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ManualVLA，一个基于MoT架构的统一VLA模型，首次实现了从最终目标状态出发，协同进行多模态手册生成和机器人操作。</li>
<li>设计了手动思维链（ManualCoT）推理过程，通过显式视觉提示和隐式潜在特征引导，将生成的手册有效地转化为精确的操作条件。</li>
<li>开发了基于3D高斯泼溅的高保真数字孪生工具包，用于自动生成手册数据，并提出了高效的三阶段训练策略，使模型在少量真实数据微调后即能取得卓越性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，其数字孪生数据生成目前依赖于对特定物体（如乐高砖块）的3D重建和仿真，这可能需要为新的物体类别调整流程。此外，联合微调阶段仍需收集少量真实世界演示数据。</p>
<p><strong>启示</strong>：本研究为VLA模型处理长视野、目标条件任务开辟了新路径，表明显式的中间过程推理（以多模态手册形式）与隐式的特征级引导相结合，能显著提升规划的可靠性和操作的精确性。未来的工作可以探索更自动化的世界模型来生成手册数据，或将此“目标→手册→动作”的范式扩展到更广泛的具身AI任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在乐高组装等长时程任务中难以协调高层规划与精细操作的难题，提出统一框架ManualVLA。其关键技术包括：基于混合Transformer架构，设计规划专家生成多模态操作手册，并通过Manual Chain-of-Thought将手册显式与隐式信息输入动作专家以指导执行。实验表明，该模型在乐高组装与物体重排任务上平均成功率比之前最佳分层基线提升32%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.02013" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>