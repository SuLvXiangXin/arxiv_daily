<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.19859" target="_blank" rel="noreferrer">2511.19859</a></span>
        <span>作者: Sanglu Lu Team</span>
        <span>日期: 2025-11-25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于视觉语言模型（VLMs）构建的视觉-语言-动作（VLA）模型在推动通用机器人智能体方面取得了显著成功。为了增强长上下文动作推理能力，早期研究尝试将高级用户指令分解为一系列子任务，并将其视为单模态的链式思维（CoT）。然而，纯文本CoT难以充分理解复杂空间环境中的细粒度视觉上下文。因此，一个极具前景的策略是利用视觉动力学作为先验来引导机器人动作生成，例如通过预测未来帧来提供指导。尽管如此，这些策略面临两个固有挑战：(i) 高维视觉观察与低级动作之间存在模态鸿沟，导致生成的未来图像中大部分像素级细节与动作执行无关；(ii) 视觉预测代理任务与动作生成任务的优化目标相互竞争，导致训练不稳定。这种错位进一步阻碍了动作策略充分利用VLM主干学习到的丰富视觉动力学，最终限制了整体性能。此外，“先预测再行动”的推理范式会带来显著的计算延迟。</p>
<p>本文针对上述痛点，提出了通过构建视觉与动作共享的离散潜在空间来统一感知与控制的新视角。其核心思路是：学习一个跨模态共享的码本，使得VLM主干自回归生成的单一令牌序列能够同时解码为未来帧预测和机器人动作，从而将视觉动力学内部化为动作规划的归纳偏置，实现感知与动作生成的统一。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了VITA（Vision-Integrated Trajectory Alignment）框架，其训练分为三个阶段：1）<strong>预热阶段</strong>：自监督训练视觉和动作自编码器以及模态共享的量化器，无需跨模态配对数据；2）<strong>协同训练阶段</strong>：冻结码本，将视觉和动作解码器连接到VLM主干输出，使用混合数据（仅视频或同步视频-动作对）联合训练；3）<strong>微调阶段</strong>：在特定数据集上仅微调动作解码器，VLM主干保持冻结。在推理阶段，仅保留VLM主干和动作解码器以构建轻量架构。</p>
<p><img src="https://arxiv.org/html/2511.19859v2/x2.png" alt="VITA框架总览"></p>
<blockquote>
<p><strong>图2</strong>：VITA框架概览。①展示了跨模态对齐，视觉感知和运动控制模态在共享离散潜在空间中统一。②和③分别展示了视觉和动作的双自编码器架构。得益于表示对齐，④中的VLM主干通过混合注意力机制生成动态统一的令牌，这些令牌被解码为未来帧和机器人动作，形成内部CoT。</p>
</blockquote>
<p><strong>核心模块1：跨模态向量量化框架</strong><br>为弥合像素丰富的视觉输入与稀疏动作输出之间的模态不匹配，VITA构建了一个跨模态向量量化框架，将两种模态投影到一个统一的离散表示空间。给定一个共享码本 𝒞，量化操作将连续特征映射到最近的码本向量。</p>
<ul>
<li><strong>视觉自编码器与量化模块</strong>：给定连续帧(𝐈<em>t, 𝐈</em>{t+1})，使用DINOv2编码器提取特征，再通过一个记忆增强的M-Former生成紧凑的时空运动嵌入z_v。随后将其量化为ẑ<em>v，并通过视觉解码器𝒟_v重建未来帧𝐈̂</em>{t+1}。损失函数结合L1损失和SSIM损失（公式5）。</li>
<li><strong>动作自编码器与量化模块</strong>：动作序列𝐚_{t:t+H}首先通过离散余弦变换（DCT）压缩到时频域，再通过一个轻量级MLP编码为动作表示z_a。该特征使用与视觉分支相同的共享码本𝒞进行量化得到ẑ_a，并通过动作解码器𝒟_a（包含逆DCT和MLP）重建动作轨迹。损失函数为MSE损失（公式9）。<br><strong>关键设计</strong>：视觉和动作模态共享同一个码本，这为潜在空间带来了结构一致性，实现了隐式的跨模态对齐，便于下游联合优化而无需显式的跨模态标注。</li>
</ul>
<p><strong>核心模块2：VLM主干架构</strong><br>VLM主干实现了一个两阶段推理过程，将高级指令遵循与低级动作生成解耦。</p>
<ul>
<li><strong>渐进式注意力机制</strong>：如<strong>图2④</strong>所示，令牌被明确分为输入令牌、文本令牌和跨模态令牌。推理时，首先在输入令牌内部应用双向注意力以捕获全局上下文，并行生成文本令牌。在生成跨模态令牌时，分别在输入令牌和文本令牌内部应用双向注意力以实现全面的链内交互，并在这些令牌组之间施加因果注意力，建立“输入→文本→跨模态”的定向信息流。</li>
<li><strong>文本链式思维推理</strong>：首先，主干将高级指令分解为一系列符号子任务（如[GRASP], [LIFT]等）。给定语言指令𝐱、初始观察𝐈_0和机器人状态𝐬，形成多模态上下文h_ctx。VLM主干生成一个子任务序列Z_sub（公式12），随后通过文本分词器将其转换为额外的文本令牌嵌入e_sub。VITA在训练期间通过多模态联合优化间接训练文本CoT的生成，而不使用显式的子任务标签监督。</li>
<li><strong>视觉链式思维生成</strong>：在第二阶段，主干以所有上下文令牌和子任务嵌入的拼接h_mul作为输入，自回归生成一系列视觉-动作混合模态的潜在令牌{τ_i}，其中每个τ_i索引共享码本𝒞。生成的令牌序列被路由到两个并行解码器：𝐈̂_{1:T} = 𝒟<em>v({c</em>{τ_i}}) 和 𝐚̂_{1:H} = 𝒟<em>a({c</em>{τ_i}})（公式16），从而从单一潜在流中实现对未来场景和机器人动作的统一预测。</li>
</ul>
<p><strong>创新点</strong>：与现有方法将视觉和动作生成分离为两个独立流不同，VITA通过共享码本在表示层面显式桥接了模态鸿沟，并通过单一令牌流同时服务双解码任务，将视觉预测任务作为动作生成的归纳偏置，实现了前向与逆向动力学的统一建模。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与基准</strong>：训练使用了包含13个数据集的混合数据，涵盖人类演示视频（如SSv2、Ego4D）、真实世界机器人数据（如OXE、RoboMIND）和模拟机器人数据（如CALVIN-ABC、LIBERO）。评估在三个模拟基准（CALVIN、LIBERO、SimplerEnv）和真实世界（UR-5e平台）的桌面任务上进行。<br><strong>基线方法</strong>：模拟基准对比了GR-1、OpenVLA、Pi0、Octo、CogACT、UP-VLA、CoT-VLA、TraceVLA、SpatialVLA、UniVLA和DeFI。真实世界对比了Pi0、OpenVLA和GR00T N1.5。<br><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>CALVIN ABC-D基准</strong>：如<strong>表1</strong>所示，VITA在连续完成1至5个任务的所有指标上均达到最优，平均完成任务数（Avg. Len）为4.73，显著优于其他基线（如DeFI的4.51）。<br><img src="https://arxiv.org/html/2511.19859v2/x1.png" alt="CALVIN结果表"></p>
<blockquote>
<p><strong>表1</strong>：VITA在CALVIN ABC-D基准上取得了全面的最先进性能，尤其在长视野任务（连续完成3-5个任务）上表现出色。</p>
</blockquote>
</li>
<li><p><strong>LIBERO基准</strong>：VITA在90个任务上的平均成功率达到81.2%，比次优的UniVLA（71.6%）高出9.6个百分点。<br><img src="https://arxiv.org/html/2511.19859v2/x4.png" alt="LIBERO结果"></p>
<blockquote>
<p><strong>图4</strong>：VITA在LIBERO基准的10个场景中，在90个任务上的平均成功率对比，显著优于所有基线。</p>
</blockquote>
</li>
<li><p><strong>SimplerEnv基准</strong>：VITA在四个任务上的平均成功率为92.5%，比Octo（80.4%）高出12.1个百分点。<br><img src="https://arxiv.org/html/2511.19859v2/x5.png" alt="SimplerEnv结果"></p>
<blockquote>
<p><strong>图5</strong>：VITA在SimplerEnv基准上所有四个任务均达到最高成功率。</p>
</blockquote>
</li>
<li><p><strong>真实世界评估</strong>：VITA在六个桌面任务（包含ID和OOD设置）上的平均成功率为80.5%，优于Pi0（76.5%）和OpenVLA（74.0%）。<br><img src="https://arxiv.org/html/2511.19859v2/x6.png" alt="真实世界结果"></p>
<blockquote>
<p><strong>图6</strong>：VITA在六个真实世界任务上的成功率对比，展现了优秀的泛化能力。</p>
</blockquote>
</li>
<li><p><strong>消融实验</strong>：论文中的<strong>表4</strong>（未在提供的图片链接中直接显示，但根据正文引用）总结了各组件贡献。关键结论包括：共享码本对性能提升至关重要；在协同训练阶段使用视觉解码器（即使推理时不使用）能带来显著增益，验证了视觉预测作为归纳偏置的有效性；动作解码器需要足够的容量以精确重建高维动作轨迹；渐进式训练方案（预热+协同训练）比直接端到端训练更有效。<br><img src="https://arxiv.org/html/2511.19859v2/x7.png" alt="训练曲线对比"></p>
<blockquote>
<p><strong>图7</strong>：VITA的渐进式训练方案（蓝线）比直接端到端训练（橙线）收敛更稳定，性能更高。<br><img src="https://arxiv.org/html/2511.19859v2/x8.png" alt="注意力可视化"><br><strong>图8</strong>：VITA的渐进式注意力机制可视化，展示了从文本理解到跨模态动作规划的注意力流。<br><img src="https://arxiv.org/html/2511.19859v2/x9.png" alt="定性结果-模拟"><br><strong>图9</strong>：在CALVIN环境中的定性结果，VITA能成功执行复杂的多步骤任务（如打开抽屉、放入滑块、关闭抽屉）。<br><img src="https://arxiv.org/html/2511.19859v2/x10.png" alt="定性结果-真实世界"><br><strong>图10</strong>：真实世界任务定性结果，展示了VITA在分布外（OOD）物体和姿态下的泛化能力。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了VITA框架，通过统一的潜在空间对齐感知与动作，并将未来帧预测内部化为动作生成的归纳偏置，从而统一了前向与逆向动力学。</li>
<li>引入了一种渐进式训练方案，使模型能够从多样化的真实世界交互视频中学习可泛化的运动动力学，同时过滤掉与运动无关的像素级细节。</li>
<li>在模拟（CALVIN、LIBERO、SimplerEnv）和真实世界任务上实现了最先进的性能，证明了其作为通用机器人操作模型的潜力。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，VITA的训练需要大量计算资源（16张A100 GPU训练约5天）。虽然预热阶段不需要配对数据，但协同训练阶段的高性能仍部分依赖于一些同步的视觉-动作配对数据。此外，动作解码器参数容量较大（228M），以精确重建动作轨迹。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>共享表示空间的有效性</strong>：为弥合视觉与动作的模态鸿沟提供了一个强有力的解决方案，未来可探索更高效的量化或连续表示方法。</li>
<li><strong>视觉CoT作为归纳偏置</strong>：将视觉预测作为辅助任务而非显式的推理步骤，为模型提供了物理常识约束，这种思路可扩展到其他需要物理推理的领域。</li>
<li><strong>渐进式与解耦训练</strong>：先独立学习各模态表示，再联合优化的策略，有助于稳定训练并充分利用异构数据，这对于构建大规模基础模型具有借鉴意义。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人动作生成中，纯文本思维链难以理解复杂视觉细节、视觉与动作模态存在鸿沟，以及多目标训练不稳定的核心问题，提出VITA框架。该方法通过构建视觉与动作的共享离散潜在空间，并引入隐式视觉思维链，使自回归生成的标记能同时解码为未来帧预测与机器人动作，从而统一感知与运动控制。实验表明，VITA在CALVIN、LIBERO和SimplerEnv基准上分别超越基线14.5%、9.6%和12.1%，并在六项真实任务中达到80.5%的平均成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.19859" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>