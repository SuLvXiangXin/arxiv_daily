<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Vision-Language Models Unlock Task-Centric Latent Actions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>Vision-Language Models Unlock Task-Centric Latent Actions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.22714" target="_blank" rel="noreferrer">2601.22714</a></span>
        <span>作者: Nikulin, Alexander, Zisman, Ilya, Klepach, Albina, Tarasov, Denis, Derevyagin, Alexander, Polubarov, Andrei, Nikita, Lyubaykin, Kurenkov, Vladislav</span>
        <span>日期: 2026/01/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>潜在动作模型已成为领先的视觉-语言-动作模型预训练流程中的重要组成部分。然而，当观测数据中包含与动作相关的干扰物时，LAMs 会失效，往往编码噪声而非有意义的潜在动作。人类则能根据简短的任务描述，轻松区分视频中与任务相关的运动和不相关的细节。本文旨在利用视觉语言模型的常识推理能力来提供可提示的表征，以无监督的方式有效地将可控变化与噪声分离，并将这些表征用作 LAM 训练的目标。核心思路是：通过任务描述提示 VLM 提取专注于任务相关特征的视觉表征，以此作为 LAM 中前向动力学模型的预测目标，从而引导 LAM 在干扰存在下学习到纯净的、与真实动作对应的潜在动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的核心是利用 VLM 生成的可提示表征作为监督信号，改进在干扰环境下的潜在动作学习。整体流程分为两步：首先，使用任务描述提示 VLM，从包含干扰物的原始观测中提取出过滤了噪声的任务中心表征；然后，用这些表征替代原始的像素观测，作为 LAM 中前向动力学模型的预测目标进行训练。</p>
<p><img src="https://arxiv.org/html/2601.22714v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：从VLM中提取任务相关的可提示表征及其在潜在动作学习中的使用流程。给定包含干扰物的观测和任务描述，VLM生成嵌入；经过聚合策略得到单个向量表征，该表征作为LAM训练时前向动力学模型的预测目标。</p>
</blockquote>
<p>整体框架基于经典的 LAPO 架构。给定一个观测转移对 $(o_t, o_{t+1})$，逆向动力学模型 首先推断潜在动作 $z_t \sim f_{\text{IDM}}(\cdot|o_t, o_{t+1})$，然后前向动力学模型 使用该潜在动作来预测下一个状态。传统 LAPO 的预测目标是在像素空间中的下一个观测 $o_{t+1}$，损失函数为 $\mathcal{L}<em>{\mathrm{MSE}} = \mathbb{E}[| f</em>{\mathrm{FDM}}(f_{\mathrm{IDM}}(\boldsymbol{o}<em>t, \boldsymbol{o}</em>{t+1}), \boldsymbol{o}<em>t) - \boldsymbol{o}</em>{t+1} |^2]$。当 $o_{t+1}$ 包含干扰噪声时，IDM 会学习编码这些噪声以最小化预测误差，导致潜在动作质量下降。</p>
<p>本文的创新点在于替换了 FDM 的预测目标。具体而言，对于每个观测 $o_t$，使用一个与任务相关的提示（例如“专注于机械臂和[任务对象]，不要描述背景特征”）输入 VLM，并通过一个固定的聚合策略（例如对倒数第二层提示嵌入的所有令牌取均值）得到一个向量表征 $s_t \in \mathbb{R}^D$。在训练 LAM 时，FDM 的目标不再是 $o_{t+1}$，而是对应的 VLM 表征 $s_{t+1}$，即损失函数变为预测 $s_{t+1}$。由于 VLM 的表征理论上过滤了与提示无关的视觉细节（即干扰物），只保留了任务相关的可控特征，因此引导 IDM 推断出的潜在动作 $z_t$ 也仅对这些纯净的特征变化进行编码，从而与真实动作对齐。</p>
<p>与现有方法相比，本文方法的创新性体现在：1) <strong>无监督性</strong>：不同于需要少量真实动作监督的方法，本文完全利用 VLM 的零样本能力。2) <strong>每步提供纯净目标</strong>：不同于 UniVLA 仅将任务指令作为条件输入 LAM，本文方法通过 VLM 为每一步观测生成动态的、语言条件化的纯净表征作为直接监督信号。3) <strong>简单直接</strong>：无需多阶段训练或复杂的过滤机制，直接修改 LAM 的预测目标。</p>
<h2 id="实验与结果">实验与结果</h2>
<ul>
<li><strong>数据集与平台</strong>：主要使用修改版的 <strong>Distracting MetaWorld Multi-Task 10</strong> 环境。修改包括在背景中添加 DAVIS 真实世界视频作为干扰，调整摄像机位置以包含更多背景。每个任务收集5000条无动作标签的专家轨迹和少于1%（最多16条）的有标签轨迹用于最终微调。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>LAPO</strong>：原始潜在动作模型基线。</li>
<li><strong>OTTER</strong>：使用 CLIP 进行免训练、文本感知的视觉特征提取作为目标。</li>
<li><strong>UniVLA</strong>：使用 DINOv2 特征和两阶段训练流程，将任务指令嵌入作为 LAM 输入条件。</li>
</ul>
</li>
<li><strong>评估指标</strong>：1) <strong>动作探针</strong>：训练线性模型从潜在动作预测真实动作的均方误差，衡量潜在动作编码真实动作的程度。2) <strong>成功率</strong>：在少量真实动作标签上微调后，智能体在环境中的任务成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>动机实验</strong>：如图4所示，在存在干扰物时，LAPO 的潜在动作质量完全退化，成功率几乎为零。若使用“完美目标”（即同一状态的无干扰版本图像）训练 LAPO，其性能可恢复到无干扰时的水平，这证明了“正确目标”的关键性。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.22714v1/x3.png" alt="干扰影响与完美目标效果"></p>
<blockquote>
<p><strong>图4</strong>：(a) 有/无干扰物时动作探针的比值，使用完美目标（-Twin）后比值接近1；(b) 有干扰物时的具体探针值；(c) 下游成功率。证明使用理想目标可以恢复性能。</p>
</blockquote>
<ol start="2">
<li><strong>VLM 基准测试</strong>：对多种 VLM 进行了大规模评估（超过29,000次实验）。如图6所示，所有 VLM 都能在一定程度上改善 LAPO，但性能差异显著。其中 <strong>Molmo</strong> 表现最好且最稳健，而较新的 <strong>Gemma-3</strong> 反而表现较差。最佳提示是明确要求 VLM 忽略背景的指令。自监督方法 CLIP 和 DINOv2 表现最差，凸显了语言条件化的必要性。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.22714v1/x5.png" alt="VLM基准测试结果"></p>
<blockquote>
<p><strong>图6</strong>：不同VLM提供的可提示表征对潜在动作学习效果的基准测试。(a) 汇总所有超参数的结果，显示Molmo中位数最佳且方差小；(b) 汇总每个任务最佳超参数的结果，排名与(a)类似。</p>
</blockquote>
<ol start="3">
<li><strong>最终性能</strong>：在完整数据集上，使用筛选出的最佳 VLM（Molmo）及其最佳超参数进行训练。如图9所示，LAPO+Molmo 显著提升了潜在动作质量，在存在干扰物的情况下几乎追平了无干扰时的 LAPO 性能。如图10所示，在下游成功率上，LAPO+Molmo 在干扰环境下将成功率从约10%提升至超过60%，实现了 <strong>六倍的增长</strong>，大幅优于 OTTER 和 UniVLA 基线。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.22714v1/x8.png" alt="潜在动作质量对比"></p>
<blockquote>
<p><strong>图9</strong>：在完整MT10数据集上，各方法的动作探针（MSE，越低越好）。LAPO+Promptable Reps（尤其是Molmo）显著优于基线，并在有干扰时接近无干扰LAPO的水平。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.22714v1/x9.png" alt="下游成功率对比"></p>
<blockquote>
<p><strong>图10</strong>：在MT10上的下游任务成功率。LAPO+Molmo 在存在干扰物的情况下取得了最高的成功率，相比原始LAPO有巨大提升。</p>
</blockquote>
<p><strong>消融与洞察</strong>：</p>
<ul>
<li><strong>VLM选择至关重要</strong>：实验发现，最先进的 VLM（如 Gemma-3）未必提供最好的可提示表征，而一些较老的模型（如 InstructBLIP）可能表现更好，这表明当前 VLM 的评估体系存在盲区。</li>
<li><strong>架构 vs. 数据</strong>：通过对比 Molmo 的两个变体（不同LLM骨干，共享CLIP视觉编码器和数据）和 Qwen2-VL，推测 Molmo 的优异表现主要源于其预训练数据而非特定架构。</li>
<li><strong>嵌入型 VLM 未达预期</strong>：专门为生成高质量嵌入而设计的“嵌入型 VLM”在本任务中并未表现出优势，甚至不如传统 VLM。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<ul>
<li><strong>核心贡献</strong>：<ol>
<li>提出了一种利用 VLM 可提示表征作为无监督目标，以解决潜在动作模型在动作相关干扰物下失效问题的新方法。</li>
<li>通过对众多 VLM 进行大规模基准测试，揭示了不同模型在提供适用于潜在动作学习的表征方面存在显著差异，并发现模型性能与其“新潮”程度非正相关，这对机器人学和 VLA 模型研究具有重要启示。</li>
<li>实验证明，该方法能显著提升潜在动作质量与下游任务性能，在 Distracting MetaWorld 上使成功率提升高达六倍，且优于现有基线。</li>
</ol>
</li>
<li><strong>局限性</strong>：论文提到，该方法依赖于 VLM 表征的质量，而 VLM 本身在视觉聚焦和像素级理解上存在局限。基准测试表明，找到合适的 VLM 和提示需要大量实验，且最佳选择可能反直觉。</li>
<li><strong>后续启示</strong>：<ol>
<li>为无监督的具身智能学习提供了一种新的、利用丰富先验知识（VLMs）的可行路径。</li>
<li>强调了在机器人学等具体应用场景中评估 VLM 表征质量的重要性，而非仅仅依赖通用的视觉语言基准。</li>
<li>提示工程和 VLM 表征的可靠性是实际应用中的关键挑战，未来需要更鲁棒或可学习的表征提取方法。</li>
</ol>
</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对潜在动作模型（LAMs）在观测包含动作相关干扰物时易编码噪声、难以提取有意义潜在动作的问题，提出利用视觉语言模型（VLMs）的常识推理能力，通过提示生成可提示表征，以无监督方式分离可控变化与噪声，并将其作为LAM训练目标。实验表明，该方法能显著提升潜在动作质量，在Distracting MetaWorld任务上使下游成功率最高提升六倍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.22714" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>