<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Artificial Intelligence (cs.AI)</span>
      <h1>BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.24161" target="_blank" rel="noreferrer">2510.24161</a></span>
        <span>作者: Tan, Wentao, Wang, Bowen, Zhi, Heng, Liu, Chenyu, Li, Zhe, Liu, Jian, Lin, Zengrong, Dai, Yukun, Chen, Yipeng, Yang, Wenjie, Xie, Enci, Xue, Hao, Ji, Baixu, Xu, Chen, Wang, Zhibin, Wang, Tianshi, Zhu, Lei, Shen, Heng Tao</span>
        <span>日期: 2025/10/28</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，多模态大语言模型（MLLMs）在视觉语言推理方面取得了显著进展，并被越来越多地部署到具身智能体中。然而，现有方法存在显著局限：MLLMs 在数字-物理空间及不同机器人形态间的泛化能力差；视觉-语言-动作模型（VLAs）能生成低级动作，但缺乏鲁棒的高级具身推理能力；大多数具身大语言模型（ELLMs）则局限于数字空间，难以泛化到物理世界。因此，能够无缝跨越数字与物理空间、同时泛化于不同形态和任务的统一模型仍然缺失。</p>
<p>本文旨在解决一个核心痛点：如何在不损害MLLM原生推理和指令遵循能力的前提下，高效地向其注入具身知识，并使策略模块能够利用MLLM提供的、与形态无关的高级意图来控制多样化的物理实体。为此，本文提出了“无界大模型”的新视角，形式化了通用基础模型应具备的三个核心能力：跨空间转移（将数字领域知识映射到物理领域）、跨任务学习（促进任务间的语义对齐）和跨形态泛化（对齐不同形态在执行相似任务时的潜在行为模式）。本文的核心思路是通过一个两阶段训练范式，先在数字空间通过监督微调向MLLM注入具身知识并保持其指令遵循能力，然后冻结MLLM，在跨形态演示数据上训练一个统一的扩散策略模块，从而实现单一模型对数字推理和物理控制的支持。</p>
<h2 id="方法详解">方法详解</h2>
<p>BLM1的整体框架是一个两阶段训练范式，旨在构建一个统一的多模态空间基础模型。输入为多模态信息（如图像/视频帧和文本指令），经过提示工程编码和融合后，送入MLLM骨干网络。第一阶段，模型在数字空间任务上进行监督微调，以获得具身感知和推理知识。第二阶段，通过一个意图桥接接口，将MLLM与一个扩散Transformer策略头连接，使用机器人状态、含噪动作和未来预测损失进行训练。最终输出既可以是数字空间的自然语言回答，也可以是物理空间的连续动作序列。</p>
<p><img src="https://arxiv.org/html/2510.24161v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：BLM1的主要框架。多模态输入首先由提示引擎编码和融合，然后传递给MLLM骨干。BLM1遵循两阶段训练范式。第一阶段，模型在数字空间任务上进行监督微调，以获得具身知识并保留指令遵循能力。第二阶段引入一个意图桥接接口，将MLLM连接到扩散Transformer策略头。该阶段使用机器人状态、含噪动作和未来预测损失进行训练。最终得到一个能够处理数字和物理任务的单一统一模型，实现了三种无界能力：跨空间转移、跨任务学习和跨形态泛化。</p>
</blockquote>
<p>核心模块包括：</p>
<ol>
<li><strong>MLLM骨干网络</strong>：采用开源的Qwen2.5-VL-7B-Instruct模型作为数字空间任务的基础。给定采样的视频帧和文本提示，骨干网络生成响应令牌序列及对应的多模态隐藏状态。从第k个Transformer块提取的多模态隐藏状态H_k总结了用于控制的高级意图。</li>
<li><strong>意图桥接接口</strong>：为了将高级意图与低级控制有效桥接，设计基于多视角历史观察窗口的操作提示。为了从密集的意图令牌中提取与控制相关的关键信息，集成了一个Perceiver模块，将高级意图表示H_k压缩为具有固定K个令牌的H_k̃。这种令牌压缩优先考虑指导语义，同时降低了计算开销，支持高频闭环控制。</li>
<li><strong>机器人策略模块</strong>：对于连续机器人控制，使用一个普通的扩散Transformer，以从MLLM压缩得到的H_k̃为条件。当前本体感知状态q_t通过轻量级状态编码器f_s编码。定义在预测范围h内的动作块为A_t = [a_t, a_{t+1}, ..., a_{t+h-1}]。A_t被添加噪声扰动，随后由轻量级动作编码器f_a嵌入。DiT通过对状态和动作嵌入的交错自注意力，以及对H_k̃的交叉注意力进行去噪。最终，一个轻量级动作解码器重建去噪后的动作块以供执行。整个模块包含约0.76B参数。为促进跨形态泛化，DiT参数在不同形态间共享，而状态编码器和动作编码器/解码器则保持形态特定。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1）明确定义并追求跨空间、跨任务、跨形态三大统一能力；2）采用两阶段解耦训练策略，第一阶段专注于向MLLM注入具身知识（而非动作），第二阶段冻结MLLM以保全其能力，仅训练策略模块，避免了联合训练的数据分布不兼容问题；3）设计了意图桥接接口和共享的扩散策略头，使得高级语义指导能够泛化地用于不同形态的低级控制。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与基准</strong>：数字空间训练使用了RoboVQA、AgiBot、HoloAssist、BridgeData V2、EgoPlan和ShareRobot六个多模态具身语料库（总计约280万理解样本和120万推理样本）。评估则在RoboVQA、AgiBot、HoloAssist、RoboFail、EgoThink和ShareRobot六个基准上进行，包含3160个测试样本，涵盖选择题和自由形式问答。物理空间训练使用自收集的跨形态演示套件，在ManiSkill仿真器中采集，涉及Franka Emika Panda、xArm-6、xArm-7和WidowX AI四种机器人形态，执行PickCube、PullCube、StackCube、PushCube、PlaceSphere、LiftPegUpright六个渐进挑战性任务，总计约34.78万帧数据。</p>
<p><img src="https://arxiv.org/html/2510.24161v1/x13.png" alt="数字空间结果对比"></p>
<blockquote>
<p><strong>图13</strong>：BLM1与MLLMs、ELLMs、VLAs、GMLMs在数字空间基准上的性能对比。BLM1在多个数据集上达到最佳或接近最佳性能，整体相较于基线方法有显著提升。</p>
</blockquote>
<p><strong>对比方法</strong>：论文将BLM1与四大类基础模型家族进行了对比：多模态大语言模型（MLLMs，如GPT-4V、Qwen2.5-VL）、具身大语言模型（ELLMs，如RoboFlamingo、VoxPoser）、视觉-语言-动作模型（VLAs，如RT-2、OpenVLA、FAST、Diffusion Policy）和通用多模态大模型（GMLMs，如GLaMM、EmbodiedGPT、GR-1）。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>数字空间</strong>：作为一个单一模型，BLM1在数字空间推理任务上超越了所有对比的模型家族，取得了约6%的性能提升（具体数值：在RoboVQA上达到90.0%，AgiBot上76.0%，HoloAssist上88.0%，RoboFail上85.0%，EgoThink上58.7%，ShareRobot上73.6%）。</li>
<li><strong>物理空间</strong>：在物理控制任务中，BLM1同样表现最佳，平均成功率相比次优方法有约3%的提升。特别是在跨形态泛化评估中，BLM1在未见过的形态上（训练用xArm-6/xArm-7，测试用WidowX）仍能保持高性能，而基线方法（如GR-1）性能下降严重。</li>
</ul>
<p><img src="https://arxiv.org/html/2510.24161v1/x15.png" alt="物理空间结果对比"></p>
<blockquote>
<p><strong>图15</strong>：BLM1与基线模型在物理空间控制任务上的成功率对比。左侧为同形态测试结果，右侧为跨形态（训练未见过的WidowX形态）测试结果。BLM1在同形态和跨形态设置下均表现最佳，展示了强大的泛化能力。</p>
</blockquote>
<p><strong>消融实验</strong>：论文进行了详尽的消融研究，总结了每个核心组件的贡献：</p>
<ol>
<li><strong>两阶段训练</strong>：移除第一阶段（SFT）会导致数字空间指令遵循和推理能力大幅下降（平均下降17.1%），证明了向MLLM注入具身知识的重要性。</li>
<li><strong>意图桥接与Perceiver</strong>：移除Perceiver压缩模块，直接将完整MLLM隐藏状态输入DiT，会导致训练不稳定和性能下降，凸显了压缩语义信息以高效指导策略的必要性。</li>
<li><strong>未来预测损失</strong>：在第二阶段后期加入未来预测损失能带来进一步的性能提升（平均提升约1.5%），表明对齐未来状态预测有助于学习更好的策略表示。</li>
<li><strong>数据加权采样</strong>：采用平衡数据集贡献的加权采样策略，相比均匀采样能带来更稳定和更优的训练效果。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1）提出了BLM1，首个旨在统一跨空间转移、跨任务学习和跨形态泛化三大能力的多模态空间基础模型；2）设计了一种系统的两阶段训练策略，通过解耦的具身知识注入和策略学习，在单一模型内实现了数字推理与物理控制的统一，且不损害MLLM的原始能力；3）构建并开源了首个系统化改变形态、固定任务语义的跨形态机器人演示套件，用于评估跨形态泛化能力。</p>
<p>论文自身提到的局限性包括：当前模型主要在模拟环境中进行训练和评估，在真实物理世界的复杂性和噪声下的表现有待进一步验证；虽然支持跨形态，但每个新形态仍需特定的状态/动作编码器，并非完全“零样本”适应。</p>
<p>本工作对后续研究的启示在于：为构建通用的具身智能基础模型提供了一个清晰的能力定义（跨空间、跨任务、跨形态）和有效的技术路径（两阶段解耦训练、意图桥接）；其提出的跨形态评估套件和基准为未来研究提供了重要的测试平台；如何进一步减少对形态特定模块的依赖，实现更彻底的零样本跨形态泛化，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出BLM1模型，旨在解决现有多模态大模型在数字与物理空间、不同任务及机器人形态间泛化能力不足的问题。其关键技术采用两阶段训练范式：第一阶段通过数字语料注入具身知识并保持语言能力；第二阶段通过意图桥接接口，利用大模型的高层语义指导策略模块学习跨形态控制，无需微调主干模型。实验表明，单一BLM1模型在数字与物理任务中均优于四类基线模型，在数字任务上性能提升约6%，物理任务提升约3%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.24161" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>