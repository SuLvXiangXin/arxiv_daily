<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.16651" target="_blank" rel="noreferrer">2511.16651</a></span>
        <span>作者: Jiangmiao Pang Team</span>
        <span>日期: 2025-11-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，具身智能领域的视觉-语言-动作模型展现出强大的泛化能力，主要得益于大规模真实机器人数据的预训练，例如著名的π系列数据集。然而，大规模真实数据的采集成本极高，依赖于熟练的操作员、专用硬件和大量人力，使得大多数研究团队难以企及。与此同时，现有的合成数据方案存在显著局限：它们通常技能单一（主要集中在抓取放置）、物体类型有限（多为刚性物体）、需要非平凡的人工操作，并且尚未被证明能够在大规模VLA模型预训练中达到与最强真实数据相当的效果。这引出了一个核心问题：当在机器人本体、场景、技能和物理真实性上达到足够规模时，高保真的合成数据能否匹配最强真实世界数据集的预训练效果？</p>
<p>本文针对这一痛点，首次提供了证据表明，仅使用合成数据预训练的VLA模型，其性能可以匹配基于最强π数据集训练的官方模型。核心思路是构建一个超大规模、高保真、覆盖多本体、多技能、多物体域（刚体、关节体、可变形体、流体）的合成数据集InternData-A1，并通过一个高度自动化、解耦和组合式的仿真流水线生成，以极低的成本验证纯合成数据在大规模预训练中的价值。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的核心是构建InternData-A1数据集的合成流水线。该流水线是一个完全解耦、自主运行的仿真系统，将资产规范、技能策略、任务组合和渲染分离，以实现灵活、高效的大规模数据生成。</p>
<p><img src="https://arxiv.org/html/2511.16651v1/x3.png" alt="数据合成流水线"></p>
<blockquote>
<p><strong>图3</strong>：InternData-A1的数据合成流水线。包含四个阶段：(1) 环境构建，选择机器人本体、场景和物体；(2) 任务组合，通过配置命令调用模块化的原子技能；(3) 域随机化，对布局、物体位姿、光照等进行随机化；(4) 轨迹生成，使用CuRobo插值密集关节动作，通过物理仿真验证，仅将成功轨迹渲染并存储为LeRobot格式。</p>
</blockquote>
<p><strong>整体流程分为四个阶段</strong>：</p>
<ol>
<li><strong>环境构建</strong>：根据任务描述模板，从资产库中检索相关的机器人本体、场景和物体。机器人是经过接触动力学验证的USD格式模型。场景是从GRUtopia数据集中分割出的227个室内房间，附有操作区域元数据。物体库覆盖刚体、关节体、可变形体和流体四大类，均带有功能注释（如抓取位姿、关节轴、材料属性）以确保高物理保真度。</li>
<li><strong>技能组合</strong>：用户通过简单的配置文件，从技能库中选择模块化的原子技能来组合任务。每个技能是一个脚本化的策略，输入为物体和机器人状态，输出一系列作为中间目标的末端执行器6D位姿。技能库包含18种基础技能（如抓取、放置、推、打开等）。通过指定每个技能使用的机械臂（左/右）并将其按顺序或并行组织，即可系统化地构建出长时序的双臂任务。</li>
<li><strong>域随机化</strong>：为增加多样性，对相机视角（旋转±5°，平移±5cm）、环境光照（174张环境贴图，随机温度和强度）、物体实例（同类替换）、桌面和背景布局进行随机化。在功能层面，例如抓取位姿从AnyGrasp生成的Top-40候选位姿中随机选择；对于关节体和可变形体，接触区域被扩展并在邻域内随机采样接触点。</li>
<li><strong>生成与存储</strong>：使用CuRobo运动规划器将技能输出的路径点插值为密集的关节空间动作序列。仅当整个轨迹在物理仿真中验证成功后，才进行视觉渲染并记录数据。记录的数据包括物体元数据、语言指令、多视角RGB图像、相机参数、机器人本体感知状态和动作控制标签，最终转换为标准的LeRobot格式以供VLA预训练。</li>
</ol>
<p><strong>关键创新与框架优化</strong>：<br>与传统的将轨迹规划和视觉渲染耦合的单阶段流水线相比，本文的流水线进行了深度优化，实现了2-3倍的端到端性能提升：</p>
<ul>
<li><strong>阶段解耦与流水线架构</strong>：将规划（CPU密集型、串行）与渲染（GPU密集型、并行）解耦为两个独立阶段，建立流水线执行机制。</li>
<li><strong>动态资源调度</strong>：在规划器和渲染器内部采用并行批处理策略，并结合动态调度算法以最大化异构硬件利用率。</li>
<li><strong>渲染效率优化</strong>：引入堆叠渲染技术以提高吞吐量。</li>
<li><strong>集群稳定机制</strong>：设计负载均衡器和监控模块，确保大规模集群部署的稳定性和鲁棒性。<br>此优化后的流水线在8块RTX 4090 GPU上每天可生成209.7小时机器人数据，每回合成本低于0.003美元，实现了高物理和视觉保真度演示的高效、可扩展合成。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p>实验旨在验证InternData-A1作为预训练数据源的有效性，并与最强基线进行对比。</p>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：使用了49个来自RoboTwin 2.0的双臂模拟任务（分简单和困难两种模式）、5个真实世界常规任务、4个真实世界长时序灵巧任务。</li>
<li><strong>对比方法</strong>：<ul>
<li>主要基线：在闭源π数据集上预训练的官方π₀模型。</li>
<li>对照模型：在InternData-A1上从头预训练的π₀模型（相同架构），以及未经预训练的π₀ (Scratch)模型。</li>
<li>开源数据集对比：与OXE（真实）、Agibot World（真实）、RoboCasa（仿真）等开源数据集预训练的模型进行对比。</li>
</ul>
</li>
<li><strong>评估指标</strong>：任务成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>与π数据集对比（模拟）</strong>：在49个模拟任务上，InternData-A1预训练的模型在简单和困难模式下，平均成功率分别达到60.0%和26.5%，均优于官方π₀模型（55.0%和20.0%），表明其预训练效果更佳。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16651v1/x4.png" alt="模拟任务对比结果"></p>
<blockquote>
<p><strong>图4</strong>：真实世界实验设置。在涵盖三个机器人的九项真实世界任务上，评估基于InternData-A1预训练的模型与π数据集基线的性能。</p>
</blockquote>
<ol start="2">
<li><strong>与π数据集对比（真实世界）</strong>：<ul>
<li><strong>常规任务</strong>：在五项真实世界常规任务中，InternData-A1模型平均成功率为68.3%，略高于π数据集模型的62.1%。</li>
<li><strong>灵巧任务</strong>：在四项涉及新本体、新物体的长时序灵巧任务（如叠衣服、拧瓶盖）中，InternData-A1模型达到了与π数据集模型相当的性能。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16651v1/x5.png" alt="真实世界任务对比结果"></p>
<blockquote>
<p><strong>图5</strong>：与π数据集在真实世界任务上的对比。InternData-A1在9项真实世界任务（包括4项灵巧任务）上取得了与π数据集相当的性能，证明了其强大的预训练能力。</p>
</blockquote>
<ol start="3">
<li><p><strong>与开源数据集对比</strong>：在相同的预训练设置下，InternData-A1在模拟和真实任务上的性能均显著优于其他开源数据集（OXE, Agibot World, RoboCasa）。特别是在真实任务上，InternData-A1对仿真数据集RoboCasa的平均优势达到57.7%，凸显了其高保真渲染和大规模数据的价值。</p>
</li>
<li><p><strong>模拟到真实迁移与数据效率分析</strong>：</p>
<ul>
<li><strong>零样本模拟到真实迁移</strong>：选取的十个模拟任务实现了直接模拟到真实迁移，平均成功率超过50%。</li>
<li><strong>数据效率对比</strong>：在四个代表性任务上，比较了使用纯模拟数据与纯真实数据微调的性能。对于基础技能任务（如分类垃圾），200个模拟回合即可匹配200个真实回合的效果；对于更复杂的任务（如翻转包裹），约需1600个模拟回合来匹配200个真实回合的效果，模拟与真实的数据效率比在8:1以内，部分接近1:1。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2511.16651v1/x6.png" alt="模拟到真实迁移分析"></p>
<blockquote>
<p><strong>图6</strong>：模拟到真实实验设置与分析。评估了四项任务的零样本模拟到真实迁移能力，并对比了使用不同数量模拟数据与真实数据微调后的性能。结果显示，在相机视角和动作空间对齐良好的情况下，较少的模拟数据即可匹配真实数据的性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.16651v1/x7.png" alt="额外模拟到真实性能"></p>
<blockquote>
<p><strong>图7</strong>：额外的模拟到真实性能。展示了十个模拟任务直接迁移到真实世界的成功率，平均超过50%，证明了数据集的高保真度和强大的零样本迁移能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>首次证明合成数据的同等效力</strong>：首次通过严格实验证明，大规模、高质量的纯合成数据（InternData-A1）在预训练通用VLA模型上，可以达到与当前最强闭源真实机器人数据集（π数据集）相当甚至略优的性能。</li>
<li><strong>提供高效可扩展的合成流水线</strong>：提出并开源了一个高度自动化、解耦组合式的仿真数据生成流水线，该流水线经过深度优化，能以极低的成本和人力投入，大规模生成覆盖多本体、多技能、多物理域的高保真数据。</li>
<li><strong>展示了强大的零样本模拟到真实迁移能力</strong>：验证了基于InternData-A1预训练的模型在多项任务上具备直接的模拟到真实迁移能力，并且模拟数据与真实数据在微调效率上的差距远小于通常的认知（在8:1以内），为利用仿真加速机器人学习提供了有力证据。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，对于涉及复杂动态交互（如抛接）或高度依赖精确接触力控的任务，模拟与真实的差距仍然存在，可能需要更多模拟数据或专门的域适应技术。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li><strong>降低研究门槛</strong>：开源的数据集和生成流水线为更广泛的研究社区提供了接近顶级性能的、可复现的预训练数据来源，有望推动具身AI研究的民主化。</li>
<li><strong>深化数据理解</strong>：这项工作使得系统化研究数据规模、多样性（技能、本体、物体类型）、保真度与模型泛化能力之间的关系成为可能，而无需依赖难以获取的大规模真实数据。</li>
<li><strong>仿真价值的重估</strong>：研究结果表明，通过提升仿真的规模、多样性和保真度，可以极大缩小模拟到真实的鸿沟，仿真作为低成本、高效率的数据引擎价值得到凸显，鼓励对仿真技术及其与学习算法协同优化的进一步探索。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在解决大规模真实机器人数据收集成本高昂的问题，探索纯合成数据预训练通用视觉-语言-动作模型的潜力。核心方法是构建了InternData-A1大规模高保真合成数据集，其通过一个高度自主、解耦、组合式的仿真流水线生成，涵盖4种机器人形态、18项技能和超过63万条轨迹。实验表明，仅用该合成数据预训练的模型，在49项仿真任务、5项真实任务和4项长时程灵巧操作任务上，性能匹配了当前最强的基于真实数据的π0模型，并展现出零样本仿真到现实的迁移能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.16651" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>