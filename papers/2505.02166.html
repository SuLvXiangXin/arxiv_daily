<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.02166" target="_blank" rel="noreferrer">2505.02166</a></span>
        <span>作者: Li, Xiaoqi, Xu, Lingyun, Zhang, Mingxu, Liu, Jiaming, Shen, Yan, Ponomarenko, Iaroslav, Xu, Jiahui, Heng, Liang, Huang, Siyuan, Zhang, Shanghang, Dong, Hao</span>
        <span>日期: 2025/05/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人操作领域，向策略模型传达任务目标有多种方式，包括语言指令、目标图像和目标视频。然而，自然语言指令可能模糊或过于简略，也可能过于详细而难以遵循；目标图像虽能提供精确的目标状态，但常包含与任务无关的背景和物体等冗余信息；基于视频的方法（演示或生成）则存在编码负担重或质量依赖性强的问题。为应对这些挑战，视觉提示因其便捷和富有表现力而成为目标指定的一种新兴方式。现有视觉提示方法如RT-Sketch仅描绘相关物体的最终状态，忽略了执行过程中至关重要的中间关键帧；RT-Trajectory虽能描绘末端执行器的整个运动轨迹，但仅提供位置信息，缺失了对任务完成同样关键的动作方向信息，且长而重叠的轨迹可能使模型在整体任务规划上产生混淆。</p>
<p>本文针对现有目标指定方式存在冗余、模糊或信息缺失（特别是方向信息）的痛点，提出了一种以物体为中心、由提示驱动的视觉-语言-动作模型新视角。其核心思路是：利用叠加在RGB图像上的、简单而富有表现力的2D“蜡笔式”视觉提示序列（关键帧）来明确传达低层动作（如接触位姿、移动方向）和高层任务规划，并训练模型理解这些提示以预测SE(3)空间中的接触位姿和移动方向，从而顺序执行并完成长视野任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>CrayonRobo的整体流程是：对于任务序列中的每个关键帧，在物体RGB图像上手动绘制或自动生成一组特定颜色的2D视觉提示线条，并辅以包含这些提示数值信息的文本描述作为输入；模型（一个视觉-语言-动作模型）需要解读这些多模态提示，并预测出对应的6自由度接触位姿（3D位置、z轴和y轴方向）以及接触后的3D移动方向；通过顺序执行所有关键帧步骤，即可完成整个长视野任务。</p>
<p><img src="https://arxiv.org/html/2505.02166v1/x1.png" alt="方法框架与对比"></p>
<blockquote>
<p><strong>图1</strong>：(a) 不同颜色提示的含义：蓝色代表接触点；红色和绿色分别代表接触时末端执行器的z轴和y轴方向；黄色代表接触后的下一个移动方向。(b) 使用带有蜡笔视觉提示的关键帧图像序列来表达任务（例如，t1-抓取，t2-放置）。模型根据输入预测6自由度接触位姿，并在出现黄色提示时预测3D移动方向（如t1中向上抓取）。通过顺序执行关键帧步骤完成整体任务。(c) 与RT-Trajectory方法的区别：RT-Trajectory仅描绘末端执行器路径（位置），而CrayonRobo的提示传达了更丰富的方向信息。</p>
</blockquote>
<p>核心模块主要包括数据收集、模型架构与训练策略。数据收集基于模拟器，通过基于规则的启发式方法记录成功的交互数据，包括3D接触点、末端执行器z轴方向a0Z&#39;、y轴方向a0Y&#39;以及物体接触点的移动方向a0M&#39;作为真值。将这些3D信息反向投影到2D图像平面，计算出2D方向向量a0z, a0y, a0m和接触点坐标a0p，并以此生成叠加在图像上的彩色视觉提示，同时将数值信息嵌入文本提示。</p>
<p>模型架构基于一个开源视觉-语言-动作模型（VLA）的主干网络。具体使用CLIP视觉编码器提取带提示图像的特征，使用LLaMa的tokenizer编码文本提示，并通过一个多模态投影模块对齐特征表示。训练时采用高效的参数更新策略，仅微调LLaMa内部的注入适配器和多模态投影模块，而保持主干参数冻结，以保留预训练知识并实现高效训练。</p>
<p><img src="https://arxiv.org/html/2505.02166v1/x2.png" alt="训练策略"></p>
<blockquote>
<p><strong>图2</strong>：设计的训练输入对与损失函数。(a)-(d)展示了四种逐步增加信息量的视觉-文本输入对，帮助模型渐进式理解每种提示的物理意义。同时采用文本监督损失、正交损失和投影损失来指导模型进行准确的位姿预测。</p>
</blockquote>
<p>训练策略的创新性体现在精心设计的输入对和复合损失函数上。如图2所示，作者设计了四种输入对，从仅包含接触点(a)，到逐步增加z轴方向(b)、y轴方向(c)和移动方向(d)，这种渐进方式帮助模型深入理解每个组件的含义。训练目标是将位姿预测构建为语言建模任务，直接输出文本形式的3D方向。为此引入了三种损失函数：1) <strong>文本监督损失LT</strong>：将归一化3D方向向量离散化为100个区间，使用交叉熵损失监督，确保模型输出正确的文本模式。2) <strong>正交损失LO</strong>：从模型输出的文本中提取预测的a0Z和a0Y，通过Gram-Schmidt损失确保它们满足旋转矩阵所需的正交关系。3) <strong>投影损失LP</strong>：将模型预测的3D方向（a0Z, a0Y, a0M）结合深度图和相机参数，反向投影回2D平面，得到预测的2D方向向量（a0z*, a0y*, a0m<em>），然后与输入的2D提示方向（a0z, a0y, a0m）计算余弦相似度损失，从而在2D提示与3D预测之间建立显式关联。总损失函数为L = λ1</em>LT + λ2<em>LO + λ3</em>LP，其中LP根据输入提示的存在情况动态调整。</p>
<p>与现有方法相比，CrayonRobo的核心创新在于：1) <strong>信息表达的全面性</strong>：通过一套颜色编码系统，在关键帧中同时指定接触点、末端执行器朝向和后续移动方向，比仅提供最终状态（RT-Sketch）或仅提供路径（RT-Trajectory）的方法传达了更丰富、更精确的动作意图。2) <strong>规划与执行的统一</strong>：使用关键帧序列作为输入，自然地将高层任务分解与低层动作执行结合起来，通过预测的移动方向连接各步骤，形成连贯的长视野任务计划。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境和真实世界中进行。模拟评估使用RLBench基准测试中的10个长视野操作任务。真实世界评估涉及17个常见物体，执行24个不同的操作任务。实验平台包括模拟器和真实的Franka Emika Panda机械臂。</p>
<p>对比的基线方法包括：纯视觉方法（如Where2Act）、语言条件方法（RT-1、RT-2）、目标图像条件方法（VC-1）以及视觉提示条件方法（RT-Trajectory）。此外，还进行了广泛的消融实验以验证各组件贡献。</p>
<p><img src="https://arxiv.org/html/2505.02166v1/x4.png" alt="模拟实验结果"></p>
<blockquote>
<p><strong>图4</strong>：在RLBench 10个任务上的模拟实验成功率对比。CrayonRobo取得了最高的平均成功率（68.5%），相比RT-Trajectory（56.3%）提升了12.2%，并且在大多数任务上表现优于或与最佳基线方法相当。</p>
</blockquote>
<p>关键实验结果如下：在模拟环境中，CrayonRobo在10个任务上取得了<strong>68.5%<strong>的平均成功率，显著优于RT-Trajectory的</strong>56.3%<strong>，并在8个任务上表现最佳。在真实世界实验中，CrayonRobo在24个任务上的平均成功率为</strong>65.3%<strong>，相比RT-Trajectory的</strong>45.8%<strong>提升了</strong>19.5%<strong>。消融实验表明：1) 使用</strong>关键帧序列</strong>（而非单帧）作为输入对性能至关重要，移除后平均成功率下降14.2%；2) 预测<strong>移动方向</strong>对于需要后续移动的动作（如抓取、推拉）非常重要，移除黄色提示会导致相关任务性能大幅下降；3) <strong>投影损失LP</strong>和<strong>正交损失LO</strong>都对提升预测精度有积极贡献。</p>
<p><img src="https://arxiv.org/html/2505.02166v1/x5.png" alt="真实世界实验结果"></p>
<blockquote>
<p><strong>图5</strong>：真实世界24个任务上的成功率对比。CrayonRobo在整体和不同任务子类型（抓取放置、推/拉、旋转/按压）上均显著优于RT-Trajectory和RT-2等基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.02166v1/x6.png" alt="消融实验结果"></p>
<blockquote>
<p><strong>图6</strong>：消融研究结果。(a) 移除关键帧序列输入（使用单帧）导致性能显著下降。(b) 在需要移动方向的任务上，移除黄色移动提示会严重损害性能。(c) 投影损失LP和正交损失LO都对最终性能有贡献。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) 提出了使用带有“蜡笔式”视觉提示的<strong>关键帧序列</strong>来明确传达机器人操作任务的低层动作目标和高层规划流程。2) 设计了一套训练策略，使视觉-语言-动作模型能够理解这些多模态提示，并<strong>预测精确的SE(3)接触位姿和3D移动方向</strong>，从而可靠完成各关键帧子任务并串联成长视野任务。3) 在模拟和真实世界中进行大规模实验，证明了该方法在<strong>任务成功率和面对新任务的鲁棒性</strong>方面具有显著优势。</p>
<p>论文自身提到的局限性主要在于其依赖准确的视觉提示生成。对于具有复杂几何形状的物体，自动生成的提示可能不够精确，而手动绘制则需要用户具备一定的专业知识。</p>
<p>这项工作对后续研究的启示在于：首先，它展示了一种介于高度抽象（语言）和过度具体（原始图像/视频）之间的、<strong>高效的任务指定范式</strong>，可广泛应用于需要精确空间指导的人机交互场景。其次，其<strong>渐进式训练和几何约束损失</strong>的设计思路，为如何让大模型更好地理解空间物理概念并输出结构化动作提供了参考。未来方向可以探索更自动化、更智能的提示生成方法（如利用大语言模型进行规划并生成提示），以及将框架扩展到更复杂的操作任务和动态环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文针对机器人任务目标传达中语言指令模糊、图像或视频过于详细或含无关信息的问题，提出CrayonRobo模型。该方法采用对象中心的提示驱动方法，通过手动或自动生成2D视觉提示覆盖RGB图像，明确表示末端执行器姿态和移动方向等任务目标。训练策略使模型能解释多模态提示，预测SE(3)空间中的接触姿态和方向，并顺序执行关键帧以完成长期任务。在模拟和真实环境中的评估证明了其强大的操作能力与鲁棒性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.02166" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>