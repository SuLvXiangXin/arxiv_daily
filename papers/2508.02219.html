<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.02219" target="_blank" rel="noreferrer">2508.02219</a></span>
        <span>作者: Huang, Dongchi, Fang, Zhirui, Zhang, Tianle, Li, Yihang, Zhao, Lin, Xia, Chunhe</span>
        <span>日期: 2025/08/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型主要通过在大规模、异构的机器人演示数据集上进行模仿学习（监督微调，SFT）来训练。这种方法虽然取得了显著成功，但其性能严重依赖任务特定数据的质量和数量，并且难以泛化到分布外（OOD）场景。强化学习（RL）通过试错直接优化策略，有潜力超越专家数据的限制，学习纠正性行为并泛化到新场景。近期研究已开始探索使用在线RL、离线RL和测试时RL来微调VLA模型。然而，在线RL需要复杂的基础设施来确保大型模型的自主学习和训练稳定性；测试时RL则会显著增加推理时间，且性能提升有限。相比之下，离线RL直接在静态数据集上优化策略，无需在线交互基础设施，且不影响推理效率，是更可行的方案。但现有方法均未考虑VLA模型中普遍采用的“动作分块”技术（即策略一次预测一个动作序列），这导致了RL与VLA模型架构的不兼容。</p>
<p>本文针对上述痛点，提出通过结合动作分块的离线强化学习来高效微调VLA模型。核心思路是提出一个名为“分块RL”的新RL框架，将时序差分学习扩展到动作分块上，并在此基础上设计了一个两阶段算法CO-RFT：首先通过全参数模仿学习初始化模型，再利用分块离线RL进一步优化策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>CO-RFT的整体框架是一个两阶段流程。第一阶段，使用有限演示（30-60条）进行全参数行为克隆，将预训练的VLA主干网络和策略头适配到当前工作空间和机器人本体。第二阶段，利用相同的演示数据集，应用结合了动作分块的离线RL来优化已初始化的策略。</p>
<p><img src="https://arxiv.org/html/2508.02219v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：CO-RFT方法整体框架。演示数据包括语言指令、本体感知和图像。框架包含VLA模型常用的视觉语言模型和动作专家，并额外引入了一个评论家专家来生成Q值。与标准RL不同，评论家专家基于动作分块预测一个Q值序列。</p>
</blockquote>
<p>该方法的创新核心在于 <strong>“分块RL”框架</strong>。该框架重新定义了策略和评论家，使其与动作分块兼容：</p>
<ul>
<li><strong>策略</strong>：$\pi_{\psi}(a_{t:t+h} | s_t)$，在状态$s_t$下预测未来$h$个连续动作$a_t, ..., a_{t+h-1}$。</li>
<li><strong>评论家</strong>：$Q_{\theta}(s_t, a_{t:t+h})$，评估给定状态$s_t$和动作分块$a_{t:t+h}$的价值。</li>
</ul>
<p><strong>分块时序差分学习</strong>：传统的1步TD学习更新单步动作的Q值（公式4）。分块TD学习则更新跨越$h$步的动作分块的Q值（公式5）。目标回报$G_t^{(h)}$计算了$h$步内的累积折扣奖励，加上$h$步后状态$s_{t+h}$下根据目标策略采样的下一个动作分块的价值。这使得价值估计可以一次性向后传播$h$步，加速学习。</p>
<p><strong>分块评论家网络</strong>：分块RL需要为动作分块中的每个位置学习一个Q值，若独立训练多个网络则计算成本高昂。</p>
<p><img src="https://arxiv.org/html/2508.02219v1/x2.png" alt="评论家网络"></p>
<blockquote>
<p><strong>图2</strong>：分块评论家网络结构。网络以状态$s_t$和动作分块$a_{t:t+h}$为输入，通过Transformer块和因果掩码进行处理。在生成的$H+1$个嵌入输出中，最后$H$个输出对应动作分块中从第1步到第$H$步的嵌入，并分别映射为对应的Q值。该设计仅用单一网络即可学习所有Q值。</p>
</blockquote>
<p><strong>评论家训练目标</strong>：评论家$Q_{\theta}$被训练来预测动作分块中所有可能的$N$步回报（$N$从1到$H$）。其训练损失是预测Q值与对应的$N$步回报目标之间的均方误差（公式6）。</p>
<p><strong>分块离线RL算法</strong>：第二阶段采用基于CalQL的分块离线RL算法。训练目标包含标准的TD误差项和一个保守正则化项（公式1， 2）。正则化项旨在抑制对分布外动作的高估，同时补偿对数据集中动作的悲观估计，从而在离线设定下稳定训练。该框架被适配为处理动作分块输入。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在<strong>真实世界机器人操纵环境</strong>中进行，使用了Franka Emika Panda机械臂。评估了包括“放杯子”、“堆叠方块”、“喷洒消毒液”、“扫描仪”、“绕圈”、“移动杯子”在内的6项任务。<strong>数据集</strong>仅包含每个任务30至60条通过遥操作收集的演示轨迹。<strong>对比的基线方法</strong>包括：监督微调（SFT）、CoTP（一种测试时RL方法）、以及消融实验中的纯行为克隆（BC）和标准离线RL（CalQL，无分块）。</p>
<p><img src="https://arxiv.org/html/2508.02219v1/assets/enhanced_success_rate_comparison.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图10</strong>：CO-RFT与基线方法在各项任务上的平均成功率对比。CO-RFT取得了最高的平均成功率（76.0%），相比SFT（48.3%）提升了57%，相比CoTP（60.7%）也显著更高。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.02219v1/assets/professional_radar_success_rate.png" alt="成功率雷达图"></p>
<blockquote>
<p><strong>图11</strong>：各方法在不同任务上成功率的雷达图。CO-RFT在大多数任务上都表现最佳，优势明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.02219v1/assets/optimized_cycle_time_comparison.png" alt="周期时间对比"></p>
<blockquote>
<p><strong>图12</strong>：CO-RFT与SFT完成任务的周期时间对比。CO-RFT平均周期时间减少了22.3%，表明其策略不仅更成功，而且更高效。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.02219v1/assets/ood_success_rate_comparison.png" alt="OOD泛化"></p>
<blockquote>
<p><strong>图13</strong>：在位置分布外（OOD）泛化测试中的成功率。CO-RFT取得了44.3%的成功率，显著高于SFT的16.7%和CoTP的25.7%，展示了强大的泛化能力。</p>
</blockquote>
<p><strong>定性结果</strong>：论文提供了图4至图9，展示了CO-RFT在“放杯子”、“堆叠方块”等任务执行过程中的关键帧序列，直观体现了其动作的流畅性和任务完成效果。</p>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>两阶段训练的有效性</strong>：仅使用第一阶段BC，平均成功率为51.7%；增加第二阶段分块离线RL后，成功率提升至76.0%，证明了离线RL优化的必要性。</li>
<li><strong>动作分块的关键作用</strong>：使用标准CalQL（无分块）进行第二阶段训练，成功率仅为56.7%，远低于使用分块RL的CO-RFT（76.0%）。这验证了所提出的分块RL框架对于有效微调VLA模型至关重要。</li>
<li><strong>保守正则化的作用</strong>：在分块RL框架下去除保守正则化项，成功率下降至65.0%，表明该组件对于离线训练的稳定性非常重要。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>“分块RL”框架</strong>，首次将时序差分学习扩展到与VLA模型兼容的动作分块上，解决了RL与VLA架构不匹配的问题。</li>
<li>基于该框架，提出了 <strong>CO-RFT算法</strong>，一个高效的两阶段微调流程，仅需少量演示即可通过离线RL显著提升VLA模型在真实任务中的性能和泛化能力。</li>
<li>在真实机器人实验中提供了 <strong>实证结果</strong>，证明CO-RFT相比监督微调在成功率上提升57%，周期时间减少22.3%，并在分布外泛化上取得44.3%的成功率。</li>
</ol>
<p><strong>局限性</strong>：论文提到，实验主要在桌面操纵任务上进行，尚未在需要更复杂、高频控制（如动态抓取）的任务上评估。此外，动作分块本身可能限制了对需要高频反馈控制的任务的适用性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>离线RL与动作分块的结合</strong>是微调大规模VLA模型的一个高效且实用的方向，避免了在线RL的工程复杂性。</li>
<li>所提出的 <strong>分块评论家网络设计</strong>为处理序列化动作的价值评估提供了有效方案。</li>
<li><strong>两阶段微调框架</strong>（先模仿学习初始化，再离线RL优化）在数据有限的情况下具有推广价值，为将通用VLA模型快速适配到特定场景提供了可行路径。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出CO-RFT方法，旨在解决使用强化学习微调视觉-语言-动作模型时面临的样本效率低、与动作分块不兼容及训练不稳定等核心问题。关键技术包括：提出专为VLA设计的Chunked RL框架，将时序差分学习扩展至动作分块；并设计CO-RFT算法，先通过模仿学习全参数微调初始化模型，再利用带动作分块的离线强化学习优化策略。实验表明，该方法仅需30-60个演示样本，在真实任务中相比监督方法成功率提升57%，周期时间减少22.3%，并在未见过的位置实现了44.3%的成功率，展现出优异的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.02219" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>