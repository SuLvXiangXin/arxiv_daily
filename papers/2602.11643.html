<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.11643" target="_blank" rel="noreferrer">2602.11643</a></span>
        <span>作者: Huazhe Xu Team</span>
        <span>日期: 2026-02-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人灵巧操作领域，视觉信息至关重要，但触觉信息同样不可或缺，二者具有显著的相关性和互补性。现有融合视觉与触觉信息的方法大多集中于特征对齐，并倾向于使用直接拼接等简单的融合机制。这类方法未能充分利用两种模态之间的内在对应关系，更忽视了它们在遮挡等复杂场景下应发挥的互补性，导致算法在真实世界部署中的潜力受限。本文针对如何更有效地融合视觉与触觉信息以提升视觉运动算法性能这一具体痛点，提出了一种新视角：不仅要对齐视觉与触觉特征，更要利用它们的互补性来处理遮挡。核心思路是提出一个名为ViTaS的框架，通过创新的“软融合对比学习”来增强跨模态对齐，并结合一个条件变分自编码器（CVAE）来利用模态间的互补性进行特征融合与重建。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViTaS的整体框架旨在从视觉和触觉输入中学习一个融合的特征表示，用以指导策略网络。输入是视觉观测 $o_i$ 和触觉观测 $t_i$，输出是用于策略学习的融合特征表示。</p>
<p><img src="https://arxiv.org/html/2602.11643v1/figs/new-method.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ViTaS方法总览。视觉和触觉输入分别通过独立的CNN编码器处理。编码后的嵌入通过软融合对比学习方法进行融合，得到的特征表示供给策略网络。同时，一个基于CVAE的重建框架用于跨模态整合。</p>
</blockquote>
<p>框架包含两个核心模块：软融合对比学习和条件VAE（CVAE）。</p>
<p><strong>1. 软融合对比学习</strong><br>此模块旨在更好地对齐视觉与触觉特征。传统对比学习在跨模态场景下，仅因时间步不同就将两个相似样本视为负样本，可能丢失关键信息。ViTaS对此进行改进。<br>首先，定义轨迹 $\Gamma = {o_i, t_i}_{i=1}^{N}$。视觉和触觉编码器 $f_o(\cdot)$ 和 $f_t(\cdot)$ 将观测映射到潜在表示。软融合对比学习分两个阶段交替进行：</p>
<ul>
<li><strong>阶段一（更新触觉编码器）</strong>：给定图像 $o_i$，根据视觉特征余弦相似度检索其最相似的Top-K个图像 ${o_{i_1}, \cdots, o_{i_K}}$。这些图像对应的触觉观测 ${t_{i_1}, \cdots, t_{i_K}}$ 被定义为 $t_i$ 的正样本，其余触觉观测为负样本。然后应用对比损失 $\mathcal{L}_{\tt{CON},1,i}$（公式1）来训练编码器，此阶段<strong>仅更新触觉编码器</strong>，视觉编码器冻结。</li>
<li><strong>阶段二（更新视觉编码器）</strong>：为避免模态不平衡，定期交换 $o_i$ 和 $t_i$ 的角色。此时，根据触觉特征相似度检索 $t_i$ 的Top-K个最相似的触觉观测，其对应的视觉观测作为 $o_i$ 的正样本，应用对比损失 $\mathcal{L}_{\tt{CON},2,i}$（公式3），此阶段<strong>仅更新视觉编码器</strong>。</li>
</ul>
<p>两个阶段通过一个周期为 $T_{\tt{switch}}$ 的切换计划交替进行，由二元掩码序列 $u_i$ 控制。整体对比损失为 $\mathcal{L}<em>{\tt{CON}} = \sum</em>{i=1}^{N}(u_i \cdot \mathcal{L}<em>{\tt{CON},1,i} + (1-u_i) \cdot \mathcal{L}</em>{\tt{CON},2,i})$。与简单地使用时间相邻样本作为正样本（时间对比）相比，这种基于特征相似性的“软”正样本选择机制能更有效地对齐模态。</p>
<p><strong>2. 条件VAE（CVAE）视觉-触觉特征整合</strong><br>此模块旨在利用视觉与触觉的互补性，特别是在自遮挡场景下。其目标是以拼接的视觉-触觉融合特征 $c$ 为条件，重建当前图像帧 $o_{\text{cur}}$。<br>CVAE包含编码器 $p_{\theta}(\cdot)$、解码器 $q_{\psi}(\cdot)$ 和视觉-触觉嵌入投影器 $f_{\phi}(\cdot)$。重建过程为 $\hat{\mathbf{o}}<em>{\text{cur}} = q</em>{\psi}(p_{\theta}(o_{\text{cur}}, f_{\phi}(c)), f_{\phi}(c))$。其损失函数 $\mathcal{L}_{\tt{VAE}}$ 包含重建误差和KL散度正则项（公式11）。在训练中，CVAE的编码器、解码器、投影器以及视觉和触觉编码器被联合优化。在推理时，仅保留视觉和触觉编码器。</p>
<p>最终，ViTaS的总损失函数为 $\mathcal{L} = \lambda\mathcal{L}<em>{\tt{CON}} + \mu\mathcal{L}</em>{\tt{VAE}} + \mathcal{L}<em>{\tt{policy}}$，其中 $\mathcal{L}</em>{\tt{policy}}$ 根据采用的强化学习（RL）或模仿学习（IL）范式而定。</p>
<p><strong>创新点</strong>：与现有直接拼接或简单对齐的方法（如M3L、VTT）相比，ViTaS的创新具体体现在：1) 提出了<strong>软融合对比学习</strong>，通过交替更新模态编码器和基于特征相似性（而非时间邻近性）选择正样本，实现了更优的跨模态对齐；2) 引入了<strong>CVAE重建模块</strong>，显式地利用模态间的互补性来融合特征，增强了对遮挡场景的鲁棒性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估在模拟和真实世界环境中进行。模拟基准包含<strong>12个</strong>多样化操作任务，涵盖灵巧手旋转（Gymnasium）、接触丰富操作（Robosuite）、插入、移动抓取和方块旋转等<strong>5个</strong>不同环境。此外，还设计了<strong>3个</strong>辅助任务（改变物体形状、随机化目标、添加噪声）以测试泛化与鲁棒性。真实世界实验包含<strong>3个</strong>任务：双臂清洁、桌面拾放、冰箱拾放。实验平台包括RL（PPO）和IL（Diffusion Policy）两种范式。</p>
<p><strong>对比方法</strong>：与<strong>6种</strong>视觉-触觉表征学习基线对比：M3L, VTT, PoE, Concat, MViTac (MVT), ConViTaC (CVT)。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟RL与IL性能</strong>：如表I所示，在9个基础RL任务上，ViTaS的平均成功率高达**91.4%<strong>，显著优于所有基线（次优方法平均</strong>71.5%<strong>）。在需要高度协调视觉与触觉的困难任务（如Egg Rotate, Block Rotate）上，ViTaS优势尤其明显。在3个IL任务上（表II），采用ViTaS特征的Diffusion Policy平均成功率为</strong>60.4%**，远超使用CNN（38.2%）或Transformer（33.4%）编码器的原始DP。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.11643v1/basis/figs/tasks.png" alt="任务概览"></p>
<blockquote>
<p><strong>图3</strong>：实验任务概览。包括12个模拟任务（a-f）和3个真实世界任务（g）。任务类型多样，涉及不同的机器人本体和操作对象。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.11643v1/basis/figs/dp.png" alt="模仿学习框架"></p>
<blockquote>
<p><strong>图4</strong>：ViTaS与模仿学习（Diffusion Policy）结合的框架。用ViTaS的编码器替换原始DP的编码器，用于模拟和真实世界任务评估。</p>
</blockquote>
<ol start="2">
<li><strong>泛化与鲁棒性</strong>：在改变物体形状（Lift w/ Cap, Can）和添加高斯噪声（Insertion Noisy）的辅助任务中（表I），ViTaS性能下降最小，展现了强大的鲁棒性。在随机化目标角度的Pen Rotate任务中（表III），ViTaS成功率**78.4%<strong>，远超其他基线（最高</strong>51.3%**），证明了其优秀的泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.11643v1/basis/figs/recon-new.png" alt="噪声可视化"></p>
<blockquote>
<p><strong>图6</strong>：不同强度高斯噪声的可视化。在Insertion Noisy任务中，添加了噪声水平为0.3的噪声以测试鲁棒性。</p>
</blockquote>
<ol start="3">
<li><strong>真实世界性能</strong>：如表IV所示，在3个真实任务中，ViTaS平均成功率为**46.0%<strong>，比使用更多摄像头输入的DP基线（</strong>30.0%<strong>）高出</strong>16%<strong>。特别是在存在自遮挡的Fridge Pick Place任务中，ViTaS取得了</strong>76.0%**的高成功率，证明了触觉信息对视觉遮挡的有效补偿。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.11643v1/basis/figs/robotsetup2.png" alt="真实机器人设置"></p>
<blockquote>
<p><strong>图5</strong>：真实世界机器人实验设置。展示了Galaxea-R1人形机器人、摄像头配置以及触觉传感器在接触物体时产生的触觉图（亮色区域表示接触）。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>如表V所示，对ViTaS各组件进行消融研究：</p>
<ul>
<li><strong>移除触觉信息（TA）</strong>：平均性能从92.5%骤降至60.9%，证明了触觉信息的至关重要性。</li>
<li><strong>移除软融合对比学习（S）</strong>：使用统一编码器或直接拼接，性能大幅下降至54.7%，表明所提出的对比融合机制的有效性。</li>
<li><strong>移除CVAE模块（C）</strong>：性能下降至63.2%，说明利用互补性进行特征融合对提升表征质量有显著贡献。</li>
<li><strong>替换为时间对比（TC）</strong>：使用时间相邻样本作为正样本，性能（70.6%）低于软融合对比（92.5%），验证了基于特征相似性选择正样本的优越性。</li>
<li><strong>调整Top-K值</strong>：K=10时性能最佳。K=1或K值过大（20，50）均会导致性能下降，表明适度的正样本数量对学习有利。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>软融合对比学习</strong>，一种改进的传统对比学习方法，用于更有效地对齐和融合视觉与触觉模态。</li>
<li>提出了<strong>ViTaS框架</strong>，一个简单而有效的表征学习范式，通过软融合对比学习和CVAE整合视觉与触觉输入，并用于指导视觉运动学习。</li>
<li>在<strong>12个模拟和3个真实世界任务</strong>上进行了广泛评估，证明了ViTaS在性能、泛化性和鲁棒性方面均优于现有基线。</li>
</ol>
<p><strong>局限性</strong>：论文提到，软融合对比学习模块在训练初期可能引入额外的计算开销。此外，真实世界部署依赖于触觉传感器的可用性和质量。</p>
<p><strong>启示</strong>：ViTaS展示了通过同时利用多模态间的<strong>相关性</strong>（对比学习）和<strong>互补性</strong>（生成式重建）来学习融合表征的有效性。这为未来多模态机器人学习提供了新思路：不仅仅要进行特征对齐，更要设计机制以应对模态缺失或受损的场景（如遮挡）。该方法可扩展至更多模态（如音频、本体感觉）的融合。如何降低此类复杂融合方法的计算成本，以及如何应用于更广泛的下游任务，是值得探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中视觉-触觉融合方法因特征直接拼接、忽视模态互补性而导致遮挡场景性能不佳的问题，提出ViTaS框架。其关键技术为软融合对比学习（Soft Fusion Contrastive Learning）和CVAE模块，通过对齐视觉-触觉特征并利用其互补性来增强表示学习。在12个模拟和3个真实世界环境中的实验表明，ViTaS性能显著优于现有基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.11643" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>