<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.21649" target="_blank" rel="noreferrer">2505.21649</a></span>
        <span>作者: Nichols, Keanu, Tasnim, Nazia, Yan, Yuting, Ikechukwu, Nicholas, Zou, Elva, Ghadiyaram, Deepti, Plummer, Bryan A.</span>
        <span>日期: 2025/05/27</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，多模态大语言模型（MLLMs）在众多视觉-语言任务中表现出色，并被寄予厚望应用于机器人操控、增强现实等需要精确空间交互的现实场景。然而，物体方向理解作为一项基础的视觉感知能力，在现有的视觉-语言基准测试中并未被独立和系统地评估。现有基准往往将方向理解与位置关系（如上/下、物体间邻近性）和一般场景理解混为一谈，或者仅评估有限的方向判断（如简单的左右方向），存在样本规模小、问题粒度粗、未能系统评估不同参考系下的方向、依赖模拟数据或呈现方式不明确等局限性。这导致了对模型方向推理能力的不完整评估，可能产生过于乐观的性能估计，无法识别其在真实场景中的关键弱点。</p>
<p>本文针对上述痛点，提出了一个新的视角：将物体方向理解作为一个独立的核心评估目标进行解构和系统化测评。为此，本文引入了DORI（判别性方向推理智能）基准。其核心思路是：借鉴人类认知研究，将物体方向理解分解为四个渐进复杂的基本维度，并通过精心设计的、包含粗粒度和细粒度两层级评估的视觉任务，来全面、隔离地诊断MLLMs的方向感知能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>DORI基准旨在系统评估MLLMs对物体方向的理解。其整体框架是构建一个包含多层次问题的诊断性评测集。输入是来自11个数据集的图像及对应的、精心构造的多选题；输出是模型对方向相关问题的回答准确率。核心在于对“方向理解”这一能力的解构和任务设计。</p>
<p>DORI将方向理解分解为四个基本维度，共包含七项具体任务：</p>
<ol>
<li><strong>正面对齐</strong>：评估感知物体正面相对于观察者方向的基本能力。<ul>
<li><strong>视图平行度感知</strong>：量化物体正面与图像平面的偏差角度（如“物体A需要旋转多少度才能面向相机？”）。</li>
<li><strong>方向性朝向感知</strong>：判断物体正面相对于相机位置的方位（如“从相机视角看，物体A的正面朝哪个方向？”）。</li>
</ul>
</li>
<li><strong>旋转变换</strong>：评估理解通过旋转产生的方向变化的能力。<ul>
<li><strong>单轴旋转感知</strong>：评估沿单一空间维度的角度变换（如“我将左侧图像旋转了多少度（顺时针）得到右侧图像？”）。</li>
<li><strong>复合旋转感知</strong>：评估涉及多个轴的旋转（如“需要怎样的水平和垂直旋转才能使左侧物体与右侧物体对齐？”）。</li>
</ul>
</li>
<li><strong>相对方向</strong>：评估理解物体之间以及物体相对于观察者的方向关系。<ul>
<li><strong>物体间方向感知</strong>：评估两个物体的相对朝向（如“物体A需要旋转多少度才能与物体B对齐？”）。</li>
<li><strong>观察者-场景方向感知</strong>：评估从非自我中心视角（如物体自身参考系）理解方向的能力（如“物体A是否面向相机？”（此处“面向”指从物体自身角度判断））。</li>
</ul>
</li>
<li><strong>规范方向</strong>：评估识别物体是否偏离其预期（规范）方向，并确定恢复规范状态所需变换的能力（如“我需要将图像旋转多少度才能让它‘正面朝上’？”）。</li>
</ol>
<p><img src="https://cdn.mathpix.com/cropped/2025_06_04_7b6c5b0b8e5f5f7f1b8dg-1.jpg?height=1352&width=1706&top_left_y=294&top_left_x=294" alt="DORI核心维度"></p>
<blockquote>
<p><strong>图1</strong>：DORI捕捉方向推理智能的四个核心维度：(1)物体的方向对齐，(2)其相对于观察者、场景和其他物体的方向，(3)为不同目标所需的旋转变换，以及(4)其在世界中的自然/规范方向。每个维度通过在变化设置中的视觉任务来评估特定的感知能力。DORI提供了对物体方向推理的整体理解。</p>
</blockquote>
<p>基准的构建过程严谨。数据来自11个现有计算机视觉数据集（包括KITTI、COCO等真实图像和ShapeNet等模拟渲染），共包含13,652张图像（37%真实，63%模拟）和33,656道多选题，涵盖67个物体类别。问题生成采用三步法：(1) 用边界框隔离目标物体以处理杂乱场景；(2) 使用标准化的方向术语和明确的参考系；(3) 确保从简单分类判断到精确角度测量的难度递进。每个问题类型都设计为两个评估层级：<strong>粗粒度问题</strong>评估基本分类理解（如“物体A是否面向相机？”），<strong>细粒度问题</strong>探查精确的定量估计（如“物体A相对于相机的角度是多少？”）。</p>
<p><img src="https://cdn.mathpix.com/cropped/2025_06_04_7b6c5b0b8e5f5f7f1b8dg-2.jpg?height=828&width=1706&top_left_y=294&top_left_x=294" alt="提示结构"></p>
<blockquote>
<p><strong>图2</strong>：DORI中的结构化提示设计。五个关键组成部分是：任务描述、上下文信息、分步说明、多选题选项和示例。结构化格式确保了对方向感知能力的一致评估。</p>
</blockquote>
<p>提示设计采用系统化、以人为中心的方法，以隔离方向感知与其他混淆因素。如图2所示，提示包含五个关键部分：任务描述、上下文信息、分步说明、多选题选项和具体示例。这种结构化格式经过多轮人工反馈迭代优化，以确保模型性能差异真实反映其方向理解能力，而非提示解读挑战。</p>
<p>与现有方法相比，DORI的创新点在于：首次提出了一个全面、分层、诊断性的框架，专门用于解构和评估MLLMs的物体方向理解能力；其任务设计基于人类认知发展规律，覆盖了从静态对齐到动态变换、从自我中心到非自我中心参考系的完整频谱；并通过结合真实与模拟数据、粗粒与细粒度问题，实现了对模型能力更深入、更可靠的评估。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究评估了15个最先进的多模态模型，包括开源模型（如LLaVA系列、Yi-VL、Mantis系列、DeepSeek系列、Qwen2.5）和专有模型（Gemini 1.5 Pro/2.0 Flash，GPT-4 One/Omni）。评估在单个NVIDIA RTX A6000或A100-80G GPU上进行（开源模型），专有模型通过其官方API评估。使用准确率（从多选题中选择正确答案的百分比）作为评估指标。</p>
<p><strong>Baseline方法</strong>：对比了上述所有参与评测的MLLMs，并提供了一个随机猜测的基线。</p>
<p><strong>关键实验结果</strong>：总体而言，所有模型在DORI上的表现都揭示了其在方向理解上的系统性缺陷。即使在最佳情况下，模型在粗粒度任务上的平均准确率也只有54.2%，在细粒度任务上更是骤降至33.0%。表2展示了部分开源模型的详细结果。</p>
<p><img src="https://cdn.mathpix.com/cropped/2025_06_04_7b6c5b0b8e5f5f7f1b8dg-3.jpg?height=1352&width=1706&top_left_y=294&top_left_x=294" alt="结果表格"></p>
<blockquote>
<p><strong>表2</strong>：多个开源MLLMs在DORI上的性能。大多数模型表现不佳，特别是在细粒度问题上。这些实验揭示了在所研究的DORI所有四个维度上，物体方向理解存在系统性差距。C：粗粒度，G：细粒度。</p>
</blockquote>
<p>从表2可以看出几个关键点：1）几乎所有模型在细粒度问题上的表现都远差于粗粒度问题，表明它们缺乏精确的角度推理能力，只能依赖粗略的分类判断。2）一些较小模型（如DeepSeek-7B-Base）的表现甚至低于随机基线，突显了问题的挑战性。3）在模型家族内部，更大规模的模型通常在粗粒度问题上表现更好，但规模优势并非绝对。</p>
<p><strong>深入发现</strong>：论文进一步总结了关键发现：</p>
<ul>
<li><strong>动态旋转任务困难</strong>：与简单的静态方向任务（如识别当前物体姿态）相比，模型在需要跨图像心理追踪物体旋转的复杂、动态旋转任务上平均表现差30%。</li>
<li><strong>视角转换挑战</strong>：模型在需要视角转换的任务上（如适应不同于相机的视点）表现尤为挣扎，相比自我中心参考系任务，准确率下降25%。</li>
<li><strong>架构影响显著</strong>：基于令牌的集成方法（如Mantis-Idefics2-8B）在线性投影方法上表现更优，表明架构设计显著影响方向推理能力。</li>
<li><strong>规模并非万能</strong>：模型规模本身并不能保证更好的方向理解；较小的、经过对话调优的变体（如DeepSeek-1.3B-Chat）常常优于较大的基础模型（如DeepSeek-7B-Base），突出了训练目标相对于参数数量的重要性。</li>
</ul>
<p><img src="https://cdn.mathpix.com/cropped/2025_06_04_7b6c5b0b8e5f5f7f1b8dg-4.jpg?height=828&width=1706&top_left_y=294&top_left_x=294" alt="数据分布"></p>
<blockquote>
<p><strong>图3</strong>：(a) DORI基准体现了七项不同的方向任务（内圈），并在自然和模拟图像中平衡、系统地分布样本。(b) DORI捕捉了日常生活中常见的多样化物体，支持对模型方向理解能力的全面分析。</p>
</blockquote>
<p>图3展示了DORI基准中任务和数据的平衡分布，确保了评估的全面性。虽然论文未进行传统的组件消融实验，但其对不同任务、模型架构和规模的比较分析，实质上揭示了不同因素（如任务复杂度、模型架构、训练方式）对方向理解性能的贡献和影响。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>DORI</strong>，这是首个专门为推进多模态系统中方向感知而设计的诊断框架。它系统地将物体方向理解解构为四个基本维度，并通过精心设计的粗/细粒度任务进行全面评估。</li>
<li>通过对15个SOTA MLLMs的大规模评估，<strong>揭示了当前模型在方向理解上的根本性局限</strong>，特别是在细粒度角度估计、视角转换和复合旋转方面，表明其内部3D空间表征存在不足。</li>
<li>提供了关于<strong>模型架构（如令牌集成 vs. 线性投影）、规模及训练目标</strong>如何影响方向推理能力的深入见解，为未来模型设计指明了方向。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，DORI主要评估的是静态图像（或图像对）中的方向理解，而现实世界的方向交互通常是动态和连续的。此外，基准虽然涵盖了67个类别，但可能仍未覆盖所有可能的物体类型和极端场景。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>模型设计</strong>：迫切需要在未来架构中引入专用的方向表征机制。基于令牌的融合方法显示出潜力，值得进一步探索。</li>
<li><strong>训练策略</strong>：需要开发能够提升模型视角转换、心理旋转和精确角度推理能力的训练目标和数据。</li>
<li><strong>评估标准</strong>：DORI为社区提供了一个新的、更严格的评估工具，未来研究在声称具有空间理解能力时，应在此类基准上进行验证。该工作强调了将方向感知作为独立研究课题的重要性，以推动AI在需要与物理世界进行空间交互的应用中取得实质性进展。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>该论文针对当前多模态大语言模型在物体方向感知能力评估上的缺失，提出了名为DORI的基准。DORI通过精心设计的任务，从正面对齐、相对方向、旋转变换和规范方向四个核心维度，专门评估模型对物体方向的细粒度理解。实验评估了15个先进模型，结果表明模型在该能力上存在显著短板：最佳模型在粗粒度任务上准确率仅为54.2%，在细粒度方向判断上更降至33.0%，且在需要参考系转换或复合旋转的任务上性能急剧下降。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.21649" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>