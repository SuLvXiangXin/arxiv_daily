<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.01607" target="_blank" rel="noreferrer">2510.01607</a></span>
        <span>作者: Yi Xu Team</span>
        <span>日期: 2025-10-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人基础模型的发展受限于可用机器人数据的规模和与机器人具身的对齐质量。当前主流的数据收集方法，如实验室遥操作、人类视频和仿真，各有其局限性：遥操作成本高昂难以扩展；人类视频存在跨具身鸿沟（从人到机器人）；仿真则面临sim-to-real鸿沟。手持式传感器接口（如UMI）作为有前景的中间方案，能够收集动作对齐的轨迹，但大多忽视了主动的、自我中心的感知：人类通过移动头部来管理遮挡和获取上下文信息，而现有系统主要依赖腕部安装的摄像头。即使拥有广角视野，以末端执行器为中心的视角也难以服务于长时程任务和精细操作，并且与使用头戴摄像头的机器人平台不匹配。本文针对现有数据收集系统缺乏主动视觉感知这一具体痛点，提出了一个集成主动感知的便携式数据收集框架ActiveUMI。其核心思路是：通过VR头显捕获操作者头部的主动运动，使学习到的策略能够模仿人类的视觉注意力模式，动态控制机器人头部视角以应对遮挡和复杂任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>ActiveUMI是一个为大规模“野外”机器人学习设计的高移动性框架，其核心在于一个便携式VR遥操作数据收集系统，并引入了主动感知的概念。</p>
<p><img src="https://arxiv.org/html/2510.01607v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ActiveUMI整体框架。左侧展示了数据收集流程与数据集构成，使用ActiveUMI收集“野外”人类演示数据用于训练。右侧展示了模型部署与推理过程，训练好的策略控制机器人执行任务。</p>
</blockquote>
<p><strong>数据收集系统硬件构成</strong>：</p>
<ol>
<li><strong>VR夹持器控制器</strong>：基于商用Meta Quest 3控制器改装，利用头显的内向外追踪系统实现高精度6自由度位姿追踪。控制器被刚性安装到目标机器人的末端执行器上，使其位姿能直接代表机器人位姿。设计具有非侵入性，即在操作者控制器上附加一个与机器人原装夹持器相同的副本，而非替换机器人夹持器。</li>
<li><strong>夹持器驱动</strong>：在控制器上集成微型电机以驱动夹持器的开合运动，实现直观抓握控制。每个控制器还增配了一个鱼眼腕部摄像头，以提供丰富的操作环境视觉上下文。</li>
<li><strong>头戴显示器</strong>：Meta Quest3s HMD扮演双重角色。首先，其SLAM系统提供了一个稳定的世界坐标系，并同时追踪操作者头部和控制器的6自由度位姿。其次，其前置彩色摄像头作为动态的“顶部摄像头”，提供与操作者视线内在耦合的全局视角。</li>
<li><strong>可穿戴设备</strong>：一个背在操作者背上的小型计算机，使系统能够独立运行，摆脱固定工作站的束缚。</li>
<li><strong>沉浸式数据收集</strong>：在VR环境中实时渲染机器人手臂的3D模型，并与操作者手持控制器（对应机器人夹持器）精确对齐，为操作者提供直观的视觉反馈。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.01607v1/x3.png" alt="沉浸式数据收集"></p>
<blockquote>
<p><strong>图3</strong>：沉浸式数据收集。系统通过在VR环境中渲染机器人手臂模型，为操作者提供关键的实时视觉反馈。</p>
</blockquote>
<p><strong>主动感知</strong>：<br>ActiveUMI的关键创新在于捕获主动的自我中心感知。系统显式地记录操作者头戴显示器的实时6自由度位姿，作为策略的额外输入。这使得模型能够学习操作者头部运动（即视觉注意力）与其相应手部动作之间的关键关联。在部署时，策略可以预测机器人头部的6自由度位姿，从而主动模仿操作者学习到的注意力模式，通过底层控制器执行该运动，动态调整视角以克服遮挡。</p>
<p><strong>校准方法</strong>：<br>为确保高质量的数据对齐，系统引入了三种校准方法：</p>
<ol>
<li><strong>现场环境设置</strong>：操作者可按控制器‘B’键重新定位基础坐标系原点，便于在任何环境中灵活开始数据收集。</li>
<li><strong>夹持器占位符</strong>：一个物理占位夹具，为VR控制器提供固定的停靠站，当控制器放入时按下指定按钮即可基于此已知物理配置快速校准虚拟坐标系。</li>
<li><strong>零位触觉反馈</strong>：当夹持器移动到距离零点（基础坐标系原点）3厘米内时，控制器电机会产生高频振动，提供触觉提示，无需依赖数值读数即可确认对齐。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在由三个6自由度ARX R5机械臂组成的真实机器人测试平台上进行评估。其中两个手臂配备腕部鱼眼摄像头，构成双手操作系统；第三个手臂提供主动移动视角，其摄像头输入源自操作者VR头显以模拟自我中心头部摄像头。策略学习使用最先进的VLA模型π0作为基础模型进行微调。评估了六项挑战性双手任务（如图4所示）。</p>
<p><img src="https://arxiv.org/html/2510.01607v1/x4.png" alt="评估任务"></p>
<blockquote>
<p><strong>图4</strong>：评估的任务。包括需要精密操作的积木拆卸、涉及可变形物体的衬衫折叠、长时程的绳子装箱、操作铰链的工具箱清洁，以及测试位置泛化能力的瓶子放置。</p>
</blockquote>
<p><strong>基线方法与关键结果</strong>：</p>
<ol>
<li><strong>主动感知的重要性</strong>：在域内任务上，将ActiveUMI（移动头部摄像头）与两个基线对比：固定头顶摄像头、仅腕部摄像头（UMI设置）。结果如表1所示，ActiveUMI平均成功率高达70%，显著优于固定头顶摄像头（42%）和仅腕部摄像头（26%）。例如，在“从包中取饮料”任务上，ActiveUMI比固定摄像头高30%，比仅腕部摄像头高60%。</li>
</ol>
<p><img src="https://arxiv.org/html/2510.01607v1/x5.png" alt="数据收集对比与误差"></p>
<blockquote>
<p><strong>图5</strong>：(a)-(d) 数据收集效率对比。ActiveUMI在绳子装箱和衬衫折叠任务上的效率介于徒手演示和机器人遥操作之间，且始终优于遥操作。(e) 相对位姿误差对比。ActiveUMI的RPE比UMI低2.5倍，数据质量更高。</p>
</blockquote>
<ol start="2">
<li><p><strong>与遥操作数据混合训练</strong>：在衬衫折叠任务上探索数据混合策略。如表3所示，仅使用ActiveUMI数据成功率为80%；混合1%的遥操作数据可将成功率提升至95%；混合10%遥操作数据则为90%。这表明，只需混合少量高保真遥操作数据即可大幅提升基于大规模ActiveUMI数据训练的策略性能。</p>
</li>
<li><p><strong>泛化能力</strong>：在新环境、新物体上测试泛化能力（表2）。ActiveUMI策略保持了56%的平均成功率，显著高于固定头顶摄像头（16%）和仅腕部摄像头（6%），表明其学习的主动感知技能能有效迁移到新的视觉环境中。</p>
</li>
<li><p><strong>数据收集效率与精度</strong>：如图5(a)-(d)所示，在长时程任务中，ActiveUMI的数据收集速度远快于传统机器人遥操作（例如，绳子装箱任务快1.59倍），效率更接近徒手演示。精度方面，通过测量回放轨迹与标称距离的相对位姿误差，ActiveUMI的RPE比UMI低2.5倍（图5e），证明了其优越的数据质量。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个集成主动、自我中心感知的便携式“野外”机器人数据收集框架ActiveUMI，通过VR头显捕获人类操作者的头部运动，使学习到的策略能够控制视角。</li>
<li>系统性地设计了高移动性硬件和高效校准方法，确保了数据收集的便捷性、高精度以及与机器人具身的高保真对齐。</li>
<li>通过大量真实机器人实验验证了主动感知对复杂操作任务性能的显著提升（平均提升44%），以及所收集数据对训练高性能、强泛化能力策略的有效性。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确陈述具体局限性，但从方法描述可推断，系统仍依赖于人类操作员进行演示收集，尚未实现完全自主的数据收集。</p>
<p><strong>启示</strong>：这项工作为机器人基础模型的规模化数据收集提供了一条高效且可行的路径。它表明，将人类的主动感知行为编码到机器人策略中，是解决复杂、长时程、存在遮挡任务的关键。未来研究可进一步探索如何自动化或半自动化地利用此类主动感知数据，以及如何将主动感知机制与更高级的任务规划和推理相结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ActiveUMI框架，旨在解决机器人策略学习中高质量、可扩展数据收集的难题。其核心是通过一套便携式VR遥操作套件，将人类演示精确映射到机器人双臂操作。关键技术包括：将机器人夹具直接安装在VR控制器上以实现姿态对齐，以及通过记录操作者头戴显示器的头部运动来捕获主动自我中心感知，从而学习视觉注意力与操作的关联。在六个复杂双手任务上的实验表明，仅用ActiveUMI数据训练的策略，在分布内任务上平均成功率达到70%，并在新物体和新环境中展现出强泛化能力，成功率为56%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.01607" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>