<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.06951" target="_blank" rel="noreferrer">2509.06951</a></span>
        <span>作者: Lv, Qi, Kong, Weijie, Li, Hao, Zeng, Jia, Qiu, Zherui, Qu, Delin, Song, Haoming, Chen, Qizhi, Deng, Xiang, Pang, Jiangmiao</span>
        <span>日期: 2025/09/08</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，执行语言指令的具身智能体主要依赖视觉-语言-动作（VLA）模型。现有VLA模型主要采用反应式的状态到动作映射策略，即直接根据当前观察和指令输出动作。这种范式存在关键局限性：缺乏对未来状态的预测能力，导致行为短视，在动态、长视野任务中鲁棒性差。本文针对这一痛点，提出了将视觉预见生成整合到决策流程中的新视角。其核心思路是：借鉴预测逆动力学模型的思想，通过预测以目标为条件的未来视觉状态作为明确的规划目标，将动作生成重新定义为预见引导的逆动力学问题，从而将反应式策略转变为基于规划的、更具前瞻性的策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>F1-VLA 采用混合专家Transformer（Mixture-of-Transformer, MoT）架构，集成了理解、生成、动作三个专用专家模块，旨在统一地桥接感知、预测与控制。</p>
<p><img src="https://arxiv.org/html/2509.06951v2/x4.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>: F1-VLA 框架总览。模型包含理解专家、生成专家和动作专家三个核心组件。理解专家处理指令和观测，生成专家预测未来视觉状态（视觉预见），动作专家则基于当前观测和生成的预见图像，通过预测逆动力学模型输出目标动作序列。</p>
</blockquote>
<p>整体流程如下：给定语言指令 <code>l</code> 和当前观测 <code>o_t</code>，<strong>理解专家</strong>（基于预训练的视觉-语言模型初始化）对语义和视觉信息进行编码，建立共享的多模态表征。该表征与历史观测序列 <code>{o_{t-m}, ..., o_{t-1}}</code> 一同输入<strong>生成专家</strong>。生成专家利用<strong>下一尺度预测</strong>机制，自回归地生成目标条件下的未来视觉预见图像 <code>o_{t+1}</code>。该预见图像随后与指令、当前观测一同输入<strong>动作专家</strong>，后者将其视为明确的规划目标，通过<strong>流匹配</strong>目标预测一个短视界的动作序列 <code>a_{t:t+k}</code>，从而驱动机器人实现预测的视觉目标。</p>
<p>核心模块与技术细节：</p>
<ol>
<li><strong>生成专家</strong>：负责高效生成高保真视觉预见。它采用多尺度残差向量量化（VQ）编码器将历史观测帧编码为离散token序列。为平衡效率与精度，使用了<strong>下一尺度预测策略</strong>：模型按空间尺度从粗到细（如从16x16到256x256）自回归地生成未来帧的VQ token，然后解码为图像。这避免了直接生成高分辨率图像的巨大计算开销。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.06951v2/x5.png" alt="残差VQ-VAE可视化"></p>
<blockquote>
<p><strong>图3</strong>: 多尺度残差VQ-VAE示意图，展示了从16×16到256×256分辨率的编码解码过程。</p>
</blockquote>
<ol start="2">
<li><p><strong>动作专家</strong>：负责映射多模态上下文为可执行动作。它采用<strong>分块动作预测</strong>和<strong>流匹配</strong>目标来建模连续动作空间中的动作分布。其关键创新在于将生成的视觉预见 <code>o_{t+1}</code> 作为条件输入，使动作预测不仅基于当前状态，还基于一个预期的视觉目标，从而支持目标导向和时序一致的行为。</p>
</li>
<li><p><strong>注意力机制</strong>：为了协调三个异构专家，F1引入了<strong>理解-生成-动作渐进注意力</strong>机制。在专家内部，使用双向的专家内注意力（生成专家内为因果注意力以保持自回归性）。在专家之间，信息流遵循严格的因果层次：生成专家可以关注理解专家，动作专家可以关注理解专家和生成专家，但反之则不允许。这种设计确保了预见是明确的中间表示，防止动作信息“泄露”回预见生成，从而稳定训练、增强可解释性，并保证控制是真正由预测的视觉结果引导的。</p>
</li>
</ol>
<p>与现有方法相比，F1的创新点体现在：1) <strong>架构上</strong>，首次在统一骨干网中显式耦合了语义理解、视觉预测和动作执行三个模块；2) <strong>预测机制上</strong>，采用高效的下一尺度预测生成高质量的视觉预见作为规划目标；3) <strong>训练策略上</strong>，设计了渐进的三阶段训练方案。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在仿真基准和真实世界任务上进行了广泛评估。使用的数据集包括LIBERO、Open-X-Embodiment、AgiBotWorld等，总计超过33万条轨迹、136项任务。对比的基线方法包括：反应式VLA模型（π₀, gr00t-N1, OpenVLA等）、基于视觉预测的策略（VPP）以及扩散策略等。</p>
<p><strong>真实世界任务结果</strong>：在Genie机器人上的9项任务评估表明，F1显著优于所有基线。</p>
<p><img src="https://arxiv.org/html/2509.06951v2/x7.png" alt="真实世界任务结果表"></p>
<blockquote>
<p><strong>表1</strong>: 真实世界任务成功率对比。F1取得了平均82.2%的任务成功率，远超最佳基线π₀的65.2%。在需要动态协调的“Handover (R2H)”任务上优势尤其明显（73.3% vs. 40%）。</p>
</blockquote>
<p><strong>仿真基准结果</strong>：</p>
<ol>
<li><strong>LIBERO基准</strong>：评估空间推理、物体交互和长视野规划能力。F1在预训练后取得了所有测试套件中的最佳平均成功率（95.7%），尤其在挑战性的LIBERO-Long任务上表现突出（91.3%），验证了其长视野规划优势。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.06951v2/x8.png" alt="LIBERO基准结果表"></p>
<blockquote>
<p><strong>表2</strong>: LIBERO基准测试结果。F1（预训练后）在综合排名和多个子任务上位列第一，特别是在LIBERO-Long任务上成功率达到91.3%，显著优于其他方法。</p>
</blockquote>
<ol start="2">
<li><strong>SimplerEnv Bridge基准</strong>：评估复杂多步操作任务。F1在平均成功率上大幅领先，尤其在需要精确放置的任务（如“Eggplant in Basket”）上表现出色，显示了其基于预见的精细操作和泛化能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.06951v2/x9.png" alt="SimplerEnv Bridge基准结果表"></p>
<blockquote>
<p><strong>表3</strong>: SimplerEnv Bridge基准测试结果。F1在多项任务上的整体成功率领先，例如在“Eggplant in Basket”任务上达到75%，远超其他模型。</p>
</blockquote>
<p><strong>消融实验与分析</strong>：<br>论文通过消融实验验证了关键组件的贡献。移除生成专家（即退化为纯反应式模型）会导致性能显著下降，尤其是在动态和长视野任务上。渐进式三阶段训练策略被证明对于稳定优化和获得可泛化的预见能力至关重要。此外，可视化结果（论文中未提供链接的图）显示，F1生成的视觉预见图像在语义和几何上均合理，有效引导了后续动作。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个集成视觉预见生成的VLA模型新范式，将动作预测从反应式过程转变为基于明确视觉目标的规划过程；2) 设计了混合专家Transformer架构与理解-生成-动作渐进注意力机制，有效桥接了理解、生成与执行；3) 提出了一个渐进的三阶段训练方案，成功在大规模异构数据上训练了统一的模型。</p>
<p>论文提到的局限性包括：模型参数量较大（42亿），对计算资源有一定要求；视觉预见生成虽然高效，但仍增加了推理时的计算开销。</p>
<p>这项工作对后续研究的启示在于：1) <strong>显式中间表示的价值</strong>：在端到端策略中引入明确的、可解释的中间表示（如视觉预见）能有效提升规划能力和鲁棒性。2) <strong>预测与控制的耦合</strong>：将生成模型（用于预测）与决策模型紧密耦合，是迈向更通用、更稳健的具身智能体的有前景的方向。3) <strong>结构化训练策略</strong>：对于复杂多模块模型，分阶段、渐进式的训练策略是保证稳定学习和有效知识迁移的关键。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>论文解决动态视觉环境中执行语言条件任务的挑战，现有视觉-语言-动作（VLA）模型因反应式映射导致短视行为和鲁棒性差。提出ℱ1模型，采用Mixture-of-Transformer架构，集成感知、预见生成和控制模块，通过下一个尺度预测机制生成目标条件视觉预见，将动作生成转化为预见引导的逆动力学问题。使用三阶段训练方案在超过330k轨迹的136个任务数据集上训练。实验表明，ℱ1在真实任务和仿真基准中 consistently outperforms existing approaches，任务成功率和泛化能力均有显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.06951" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>