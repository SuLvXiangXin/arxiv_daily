<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Human-Computer Interaction (cs.HC)</span>
      <h1>The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.26557" target="_blank" rel="noreferrer">2509.26557</a></span>
        <span>作者: Yan, Litao, Head, Andrew, Milne, Ken, Le, Vu, Gulwani, Sumit, Parnin, Chris, Murphy-Hill, Emerson</span>
        <span>日期: 2025/09/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流AI助手（如Excel Copilot）仅在用户主动描述目标或问题后才提供帮助，这要求用户付出努力回忆并精确表述，对非专家用户而言存在困难。此外，许多基于用户行为推荐工作流的系统依赖于应用程序的内部日志、API或事件序列分析，这限制了它们在缺乏此类访问权限的跨应用或商业环境中的部署。用户在使用Excel等功能丰富的软件时，通常只使用其功能的一个有限子集，且往往无法意识到自己当前的工作流程存在低效之处。</p>
<p>本文针对“用户难以发现更优工作流”以及“现有辅助系统需要用户主动提问或依赖内部日志”这两个具体痛点，提出了“基于视觉的任务反思”这一新视角。核心思路是：直接分析用户完成任务时的屏幕录像，通过视觉-语言模型重建用户动作和上下文，再通过语言模型自动识别低效模式并生成结构化、高保真、可立即执行的改进建议，从而无需用户提示或软件内部接入。</p>
<h2 id="方法详解">方法详解</h2>
<p>InvisibleMentor采用两阶段流水线设计，旨在将屏幕录像转化为具体的工作流程改进建议。</p>
<p><img src="https://arxiv.org/html/2509.26557v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：InvisibleMentor从屏幕录像生成建议的流水线。系统分为两个阶段。 (➊) 一个视觉-语言模型以每5秒一帧的速率处理屏幕录像，提取结构化的任务表示，包括用户动作和电子表格上下文 (➋)。这些表示被分组为工作流并传递给语言模型，后者分析它们以识别低效之处并生成可操作的建议。每个建议包含一个低效工作流序列、原理说明和逐步改进建议 (➌)。</p>
</blockquote>
<p><strong>第一阶段：使用VLM捕获任务上下文</strong><br>此阶段目标是从视觉输入中构建用户任务的结构化表示，无需专门插装（G1）。系统在用户操作Excel时，每5秒采样一帧屏幕图像。这个间隔是经过试点测试（使用欧洲自行车商店销售数据集）后选择的，平衡了覆盖范围（避免错过短暂动作）和效率（避免冗余帧）。每帧图像编码了电子表格状态和界面上下文，如选中的单元格、可见公式、列标题等。</p>
<p>使用GPT-4.1（支持图文输入）作为VLM来分析这些帧。对于长录像，由于模型上下文窗口限制（约20帧），系统将录像分成最多三个时间片段进行并行处理，每个片段内再分批（每批20帧）送入VLM。VLM为每批帧输出两个关键元素：</p>
<ol>
<li><strong>动作序列</strong>：用户执行的低级动作列表（如“编辑公式”、“应用筛选器”、“切换工作表”）。</li>
<li><strong>上下文状态</strong>：每个时刻的电子表格内容和布局（如选中区域、单元格值、界面元素）。</li>
</ol>
<p><strong>第二阶段：使用LLM生成并交付建议</strong><br>此阶段基于第一阶段提取的任务表示，生成结构化、高保真的建议（G2），并主动发现未言明的改进机会（G3）。使用相同的GPT-4.1模型，但切换至文本模式，输入结构化的动作和状态信息，并遵循一个结构化的零样本提示（见附录A）。</p>
<p>LLM被提示将相关动作分组为工作流，标记那些次优的工作流，并返回最多三个优先推荐。每个推荐包含三个部分：</p>
<ol>
<li><strong>相关的低效动作</strong>：描述用户实际做了什么。</li>
<li><strong>原理</strong>：解释为什么这个工作流可能低效（例如，错失了某个内置功能）。</li>
<li><strong>具体替代方案</strong>：提供逐步的指导或具体公式，并附带一个简短的“收益”声明，对比前后差异。<br>系统在处理完第一阶段观察到的所有步骤后才生成建议，这使得LLM能够检测到不仅限于单个动作的低效，还包括更高级别的重复模式。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.26557v1/x3.png" alt="用户界面"></p>
<blockquote>
<p><strong>图3</strong>：提供结构化工作流指导的电子表格助手用户界面。助手出现在电子表格旁边的任务窗格中，遵循Excel Copilot的熟悉布局，以最小化可能影响用户研究结果的设计差异。界面包含五个关键组件：(➊) 提示想法为用户提供发起求助的高级建议。我们用一个专门的请求推荐入口点替换了原来的第五个想法。(➋) 观察到的用户动作，总结最近的电子表格活动；(➌) 工作流限制，解释潜在的低效之处；(➍) 可操作的建议，呈现逐步改进方案；(➎) “给我另一个建议”按钮，允许用户请求替代方案。</p>
</blockquote>
<p><strong>用户界面</strong><br>建议在任务完成后（G4）呈现，界面（图3）位于电子表格旁的任务窗格中，模仿Excel Copilot布局以减少界面差异带来的混淆。关键组件包括：</p>
<ul>
<li><strong>提示想法入口点</strong>：其中一个卡片被替换为“向我展示改进工作流程的建议”。</li>
<li><strong>观察到的用户动作</strong>：用自然语言总结用户近期活动，帮助用户回忆并确认系统的理解。</li>
<li><strong>工作流限制</strong>：解释所观察工作流中潜在的低效之处。</li>
<li><strong>可操作的建议</strong>：提供具体的、逐步的指令，并明确说明功能名称和访问路径（如功能区路径），最后附上收益对比。</li>
<li><strong>按需提供替代方案</strong>：初始仅显示最关键的建议，用户可通过按钮请求查看其他已识别的次优工作流建议，以避免信息过载。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ol>
<li><strong>技术评估（RQ1）</strong>：使用一个内部收集的基准数据集，包含25个屏幕录制的会话，参与者完成了14个涉及基本Excel操作（如插入行、编辑公式、切换工作表）的短任务。评估VLM从屏幕录像中识别用户动作的准确性。</li>
<li><strong>用户研究</strong>：一项涉及20名参与者的受试者内研究，比较InvisibleMentor与一个基于提示的电子表格助手（作为基线）生成的建议。评估维度包括建议的有用性、可理解性、应用所需努力程度以及对反思和学习的帮助。</li>
</ol>
<p><strong>对比基线</strong>：用户研究中，基线是一个需要用户用自然语言描述其任务或问题以获取帮助的提示型电子表格助手。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>技术评估结果</strong>：VLM在从屏幕录像中恢复14种常见电子表格动作的任务上，达到了超过90%的F1分数（基于人类评估者为90.5%，基于LLM评估者为90.4%）。评估者间一致性很高（96.4%的决策一致）。大多数识别错误涉及采样帧之间发生的快速动作或细微的UI变化。</li>
<li><strong>用户研究结果</strong>：参与者显著更偏好InvisibleMentor的建议。具体而言：<ul>
<li>参与者认为InvisibleMentor的建议<strong>更有用</strong>（平均得分4.25 vs. 3.30，p&lt;0.001）。</li>
<li>认为其建议<strong>更容易理解</strong>（4.35 vs. 3.55，p&lt;0.001）。</li>
<li>认为应用其建议<strong>所需努力更少</strong>（4.10 vs. 3.20，p&lt;0.001）。</li>
<li>InvisibleMentor还帮助参与者更有效地注意到低效之处、识别错误，并对尝试替代工作流更有信心。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2509.26557v1/x4.png" alt="建议质量对比"></p>
<blockquote>
<p><strong>图4</strong>：用户对InvisibleMentor与基于提示的助手所生成建议的质量评价。在有用性、可理解性和应用所需努力程度上，InvisibleMentor的建议均获得显著更高的评分。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.26557v1/x5.png" alt="感知影响对比"></p>
<blockquote>
<p><strong>图5</strong>：用户感知到的InvisibleMentor与基于提示的助手在帮助反思、识别错误和建立信心方面的影响对比。InvisibleMentor在所有三个方面都获得了显著更高的评分。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.26557v1/x6.png" alt="用户偏好与定性反馈"></p>
<blockquote>
<p><strong>图6</strong>：用户偏好总结及代表性定性反馈。绝大多数参与者（19/20）更偏好InvisibleMentor。定性反馈突出了其建议的针对性（基于用户实际行为）、具体性和教育价值。</p>
</blockquote>
<p><strong>消融实验分析</strong>：<br>论文虽未使用传统“消融实验”一词，但在方法详情的“试点测试和参数选择”部分，实质性地测试了不同设计选择的影响：</p>
<ol>
<li><strong>采样间隔</strong>：对比了1秒、5秒和10秒间隔。1秒间隔导致大量重复检测，增加处理时间；10秒间隔容易错过短暂动作；5秒间隔在覆盖率和效率间取得了最佳平衡。</li>
<li><strong>分段处理策略</strong>：对比了处理完整录像与分段并行处理。对于一小时录像，5秒间隔下，不分段处理需6分钟，而分成三个并行片段处理仅需约2分钟，显著降低了延迟。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了“<strong>基于视觉的任务反思</strong>”这一新的交互模式，系统通过观察用户行为主动提供改进建议，无需用户提示、软件插装或意图说明。</li>
<li>实现了<strong>InvisibleMentor系统</strong>，一个两阶段流水线，能够从屏幕录像中恢复工作流，并生成结构化、高保真的建议以改进未来工作。</li>
<li>通过<strong>基准测试和用户研究</strong>表明，InvisibleMentor能够准确检测低效工作流，并在支持用户反思、学习和改进方面优于基于提示的助手。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li><strong>采样限制</strong>：每5秒采样一帧可能错过快速发生的动作（如快速单元格选择）。</li>
<li><strong>模型依赖与成本</strong>：系统性能依赖于底层VLM/LLM（本文用GPT-4.1）的视觉理解和推理能力，存在相关成本和延迟。</li>
<li><strong>领域特定性</strong>：当前实现和评估集中于Excel环境，其通用性有待在其他复杂软件中验证。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>更细粒度的动作检测</strong>：未来工作可以探索更高帧率采样与动作去重相结合，或开发专门用于实时、细粒度界面动作理解的模型，以弥补采样可能遗漏的信息。</li>
<li><strong>跨应用通用性</strong>：验证并将此方法推广到其他生产力软件（如PPT、Word）或创意工具（如Photoshop），将极大扩展其应用范围。</li>
<li><strong>交互时机与形式</strong>：本文选择在任务后提供建议以避免中断。未来可以研究更自适应的介入时机（如检测到明显重复时）以及更丰富的建议呈现形式（如嵌入式演示、一键应用脚本）。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对用户在Excel等复杂软件中难以发现高效工作流的问题，提出了InvisibleMentor系统。该系统无需用户主动描述问题，而是通过**视觉任务反思**技术，直接分析屏幕录制视频来识别低效操作（如重复查找替换）。其核心技术为**两阶段流程**：先利用视觉语言模型重建用户操作与上下文，再由语言模型基于此生成结构化改进建议。实验表明，该系统能准确识别低效工作流，用户评价其建议比传统提示型助手更具可操作性、更量身定制，且更有助于学习和未来改进。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.26557" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>