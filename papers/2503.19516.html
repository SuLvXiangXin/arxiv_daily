<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Boosting Robotic Manipulation Generalization with Minimal Costly Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Boosting Robotic Manipulation Generalization with Minimal Costly Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2503.19516" target="_blank" rel="noreferrer">2503.19516</a></span>
        <span>作者: Zheng, Liming, Yan, Feng, Liu, Fanfan, Feng, Chengjian, Zhong, Yufeng, Ma, Lin</span>
        <span>日期: 2025/03/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型在具身智能中的应用日益广泛，这加大了对多样化操作演示数据的需求。然而，数据收集的高成本通常导致数据无法覆盖所有场景，从而限制了模型的性能。研究发现，在大工作空间中的空间推理阶段是导致任务失败的主要环节。幸运的是，这一阶段的数据可以低成本收集，这凸显了利用廉价数据提升模型性能的潜力。本文针对昂贵数据稀缺、模型泛化能力受限的痛点，提出了将操作任务轨迹分解为不同阶段，并利用大量易收集的空间推理阶段数据来增强VLA模型泛化的新视角。其核心思路是：通过任务分解，将操作轨迹区分为空间推理阶段和物理交互阶段，并以适当的比例混合这两种成本差异巨大的数据来训练模型，从而以最小化昂贵数据需求的方式提升模型在零样本场景下的泛化性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出了两个核心部分：RoboTron-Craft数据生成与评估基准，以及RoboTron-Platter数据混合训练方法。</p>
<p><strong>RoboTron-Craft基准</strong>：这是一个分阶段、低成本的数据生成管道，用于生成物理逼真的操作轨迹和模型评估环境。它基于NVIDIA Isaac Sim平台，能够生成多样化的轨迹，涵盖各种物体和环境。</p>
<p><img src="https://arxiv.org/html/2503.19516v2/x2.png" alt="数据生成管道概览"></p>
<blockquote>
<p><strong>图2</strong>：数据生成管道概览。(a) 环境首先用随机位姿的多个物体初始化。(b) 在物体中心规划阶段，对目标物体的可行操作位姿进行采样。(c) 在智能体中心规划阶段，管道将任务分割为若干段，规划执行轨迹并在执行过程中监控任务进度。</p>
</blockquote>
<p>其生成流程分解为两级层次：物体中心规划和智能体中心规划。首先随机初始化环境和若干可操作物体。在物体中心规划阶段，采样一个目标物体及其一个可行的操作位姿<code>g</code>。随后在智能体中心规划阶段，任务管理器根据已知的<code>g</code>和任务模板监督执行阶段，并使用运动规划器结合目标几何形状和场景空间占用的先验知识，规划每个阶段的无碰撞机器人轨迹。</p>
<p><strong>SRP数据收集</strong>：为了加速数据收集，论文提出专门收集空间推理阶段轨迹。具体方法是从真实操作位姿样本<code>g</code>出发，采样一个随机的偏移变换<code>ΔT</code>（包含远离物体的随机朝向和位置偏移），并将其应用于<code>g</code>，得到接近位姿<code>g_app</code>，作为SRP轨迹的最终状态。这种方法无需为运动规划器规划精确抵达目标的狭窄路径，也无需物理模拟器进行交互时的复杂碰撞计算，从而极大节省了数据收集时间。只要位姿误差在一定范围内，操作模型能够学习正确的交互位姿并在推理时自动修正误差。此方法同样适用于真实世界机器人数据收集。</p>
<p><strong>RoboTron-Platter训练方法</strong>：该方法将机器人操作轨迹根据智能体与环境中物体的交互程度，分割为空间推理阶段和物理交互阶段。通过以适当比例混合两阶段数据，旨在达到与使用完整数据训练模型相当的泛化性能，从而减少对昂贵PIP数据的依赖。</p>
<p><img src="https://arxiv.org/html/2503.19516v2/x3.png" alt="方法概览"></p>
<blockquote>
<p><strong>图3</strong>：方法概览。我们将每个训练轨迹分为两个阶段：不需要紧密交互的空间推理阶段，以及智能体直接操作目标的物理交互阶段。采样N1条SRP和N2条PIP轨迹形成新数据集（左）。这个重采样的数据集用于以模仿学习方式训练VLA模型（中）。在推理过程中，VLA模型以智能体的观察和语言指令为输入，预测下一步动作以指导智能体完成任务（右）。</p>
</blockquote>
<p>首先需要收集完整阶段演示轨迹的数据集<code>D^F</code>用于监督训练。为了利用更多易收集的SRP数据，可以引入独立收集的SRP数据集<code>D_ind^SRP</code>。利用过程如下：1) <strong>轨迹分割</strong>：基于末端执行器<code>G</code>与目标物体<code>T</code>之间的距离，将完整轨迹<code>τ_i^F</code>分割为SRP段<code>τ_i^SRP</code>和PIP段<code>τ_i^PIP</code>。PIP段从<code>G</code>位于接近位姿<code>g_app</code>的状态开始，到交互密集型阶段完成结束；其余阶段（如接近阶段）均归类为SRP段。一个轨迹可能被分为多个交替的段。2) <strong>构建混合数据集</strong>：在训练前，分别从<code>D^F</code>和<code>D_ind^SRP</code>中采样<code>N1</code>和<code>N2</code>个片段，构建新的混合数据集<code>D^Mix</code>。通常使用全部完整轨迹数据（即<code>N1 = |D^F|</code>），并选择合适的<code>N2</code>来提升在新场景上的泛化性能。通过这种方法，构建了一个使用子任务数据集<code>D^PIP</code>和<code>D^SRP ∪ D_ind^SRP</code>的隐式子目标特定训练流程，从而灵活控制各子任务的性能。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用RoboMM作为基线VLA模型。训练时输入语言指令、腕部和桌面摄像头的RGB图像及相机参数。通过改变独立SRP片段的比例<code>p_SRP = N2/(N1+N2)</code>来形成混合数据集<code>D^Mix</code>，以验证RoboTron-Platter方法的有效性。任务为对不同目标物体的抓取放置，在RoboTron-Craft基准中使用IsaacSim平台进行评估。成功标准是智能体在VLA模型生成的动作下成功抓取指令指定的目标物体。</p>
<p><img src="https://arxiv.org/html/2503.19516v2/x4.png" alt="零样本场景中各任务平均成功率受单任务训练轨迹数量影响的曲线图"></p>
<blockquote>
<p><strong>图4</strong>：零样本场景中，单任务训练轨迹数量对平均成功率的影响。参考模型使用<code>N1=100, N2=0</code>配置训练作为基线性能。横轴代表单个任务的训练轨迹数量，即<code>N1+N2</code>。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>SRP数据对泛化的提升</strong>：实验固定<code>N1=100</code>条完整轨迹，并改变<code>N2</code>条独立SRP轨迹的数量。结果显示，引入额外的自动收集SRP数据能显著提升模型成功率，相比仅使用完整轨迹数据的参考模型，最大提升幅度达到41%，并且能达到与使用高成本完整阶段数据训练相当的性能。然而，当<code>N2 &gt; 2N1</code>时会出现性能瓶颈，此时在保持<code>N1</code>不变的情况下继续增加<code>N2</code>不会带来显著改善。这表明不完整的轨迹不能无限添加，不足的操作数据可能阻碍模型学习有效的操作能力。增加<code>N1</code>可以延迟瓶颈的到来，例如将<code>N1</code>从100增加到300带来了额外的7%提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.19516v2/x5.png" alt="数据混合比例对成功率影响的曲线图"></p>
<blockquote>
<p><strong>图5</strong>：数据混合比例对成功率的影响。当SRP数据比例在40%到60%之间时，成功率最高。</p>
</blockquote>
<ol start="2">
<li><strong>数据混合比例的影响</strong>：实验探索了SRP数据比例<code>p_SRP</code>对性能的影响。结果显示，存在一个最优的混合比例范围（约40%-60%），在此范围内模型性能最佳。比例过低（SRP数据不足）或过高（PIP数据相对不足）都会导致性能下降。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.19516v2/x6.png" alt="SRP数据通用性验证的柱状图"></p>
<blockquote>
<p><strong>图6</strong>：SRP数据通用性验证。即使SRP数据和PIP数据来自不同的物体类别，也能提升模型在未见物体上的泛化能力。</p>
</blockquote>
<ol start="3">
<li><strong>SRP数据的通用性</strong>：实验验证了即使SRP数据和PIP数据来自不同的物体类别（例如，SRP数据使用“杯子”类，而PIP数据使用“瓶子”类），混合训练也能显著提升模型在未见过的“罐子”类物体上的泛化成功率，证明了SRP数据在提升空间推理能力方面的通用性。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.19516v2/x7.png" alt="消融实验：各阶段成功率贡献的柱状图"></p>
<blockquote>
<p><strong>图7</strong>：消融实验：各阶段对成功率的贡献。增加SRP数据主要提升了SRP阶段的成功率，而对PIP阶段影响较小。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：分析表明，增加SRP数据主要提升了SRP阶段的成功率（从49%提升至72%），而对PIP阶段的成功率影响较小（从82%微升至85%）。这证实了SRP数据确实作为“催化剂”，放大了昂贵PIP数据的价值，通过改善空间导航为后续物理交互创造更好条件。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.19516v2/x8.png" alt="模型性能与目标物体数量、训练轨迹数量、环境多样性之间关系的缩放定律曲线图"></p>
<blockquote>
<p><strong>图8</strong>：模型性能与目标物体数量、训练轨迹数量、环境多样性之间的缩放定律。</p>
</blockquote>
<ol start="5">
<li><strong>缩放定律</strong>：论文基于RoboTron-Craft基准揭示了VLA模型性能与目标物体多样性、训练轨迹数量以及环境多样性之间的对数缩放定律，为数据收集策略提供了指导。</li>
</ol>
<p><img src="https://arxiv.org/html/2503.19516v2/x9.png" alt="不同方法在零样本场景成功率的对比柱状图"></p>
<blockquote>
<p><strong>图9</strong>：不同方法在零样本场景成功率的对比。RoboTron-Platter方法显著优于基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.19516v2/x10.png" alt="不同方法在未见目标物体上成功率对比的柱状图"></p>
<blockquote>
<p><strong>图10</strong>：不同方法在未见目标物体上成功率的对比。展示了向新目标物体的技能迁移能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.19516v2/x11.png" alt="训练过程损失曲线对比图"></p>
<blockquote>
<p><strong>图11</strong>：训练过程损失曲线对比。混合数据训练收敛更快且更稳定。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2503.19516v2/x12.png" alt="定性结果：模型在复杂场景中成功执行任务的序列图像"></p>
<blockquote>
<p><strong>图12</strong>：定性结果：模型在复杂场景中成功执行任务的序列图像。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了RoboTron-Craft，一个用于生成逼真操作轨迹的低成本、分阶段数据生成管道和评估基准，并基于此揭示了模型性能与数据多样性之间的缩放定律。2) 提出了RoboTron-Platter数据混合训练方法，该方法通过解耦训练轨迹、利用大量低成本SRP数据，显著提升了VLA模型在零样本场景下的泛化性能，最高提升达41%。3) 通过实验证明了SRP数据可以作为催化剂，最大化昂贵操作数据集在VLA模型训练中的贡献，并能将操作技能有效迁移到新目标物体。</p>
<p>论文自身提到的局限性在于，当SRP数据量超过PIP数据量一定比例（约2倍）时，会遭遇性能瓶颈，无限增加SRP数据无法持续提升性能。此外，需要仔细调整SRP与PIP数据的混合比例以达到最佳效果。</p>
<p>本工作对后续研究的启示在于：为降低机器人学习的数据成本提供了一种有效范式，即通过任务分解和差异化数据策略来优化数据利用。未来的研究可以探索更高效的数据混合策略、自动化的混合比例调整方法，以及将SRP数据收集进一步扩展到更复杂的真实世界环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中视觉-语言-动作（VLA）模型训练数据收集成本高、覆盖不足导致泛化能力受限的核心问题，提出一种低成本数据增强方法。关键技术为 RoboTron-Platter 框架，其将操作轨迹解耦为空间推理阶段（SRP）与物理交互阶段（PIP），并利用大量易收集的低成本 SRP 数据，以特定比例与有限的昂贵 PIP 数据协同训练，最大化昂贵数据的利用率。实验表明，在零样本场景中，该方法能实现最高 41% 的成功率提升，并可将操作技能迁移到新目标。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2503.19516" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>