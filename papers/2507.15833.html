<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.15833" target="_blank" rel="noreferrer">2507.15833</a></span>
        <span>作者: Iman Soltani Team</span>
        <span>日期: 2025-07-21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，机器人模仿学习（IL）已成为实现灵巧操作的主流方法。这些方法通常以端到端的方式，统一、被动地处理原始相机图像和本体感知信息以直接输出机器人动作。然而，这与人类高效、主动的视觉处理方式存在显著差异。人类视觉系统通过注视驱动凹窝成像，将高分辨率视觉集中在任务相关区域，极大减少了视觉处理需求。当前机器人视觉方法存在两个关键局限性：一是计算成本高，尤其是广泛使用的视觉Transformer（ViT）因其自注意力机制，计算复杂度与标记数量成平方关系；二是可能受到环境中无关背景或干扰物的影响，缺乏人类视觉的鲁棒性。</p>
<p>本文针对机器人视觉处理效率低下且易受干扰的痛点，提出了一个新颖的视角：将人类注视行为与生物启发的凹窝视觉处理机制引入机器人学习。核心思路是模仿人类“先看、再聚焦、后行动”的模式，利用预测的注视点引导ViT进行非均匀的凹窝标记化，从而大幅减少计算标记数量，并增强对任务关键区域的关注。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的整体框架GIAVA（Gaze Integrated Active-Vision ALOHA）系统，旨在从人类演示中同时学习注视、主动视角控制和操作动作。流程分为数据收集与策略学习两大部分。</p>
<p><img src="https://arxiv.org/html/2507.15833v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：GIAVA系统概述。a) 使用带眼动追踪的VR头显记录双手操作演示，以 b) 控制AV-ALOHA机器人系统。数据收集后，训练一个机器人策略来模仿人类的注视、头/颈运动和操作动作。c) 策略首先预测注视点，在注视点周围进行凹窝成像处理，并通过流匹配策略生成动作。</p>
</blockquote>
<p><strong>1. 数据收集系统</strong>：基于AV-ALOHA平台扩展，使用配备眼动追踪的Meta Quest Pro VR头显。操作者通过头显控制搭载立体相机的主动视觉（AV）机械臂（模拟头颈运动），并通过手柄控制两个操作机械臂。系统实时将相机图像流传输至头显供操作者观看，并同步记录相机图像、机器人状态、动作以及操作者在对应图像上的注视坐标。通过为每帧图像添加唯一ID并与注视数据关联，解决了传输延迟带来的同步问题。</p>
<p><img src="https://arxiv.org/html/2507.15833v2/x2.png" alt="数据收集流程"></p>
<blockquote>
<p><strong>图2</strong>：GIAVA数据收集流程。立体相机图像及图像ID传输至VR头显。VR头显将头部和手柄控制器位姿发回以控制机械臂，同时发送带有对应图像ID的注视坐标，实现注视数据与图像的同步。</p>
</blockquote>
<p><strong>2. 策略架构</strong>：策略基于条件流匹配（CFM）学习，其架构如图3所示。</p>
<ul>
<li><strong>观测编码器</strong>：输入为左相机图像和机器人本体状态。图像通过ViT编码为标记序列，再经由Q-Former模块压缩为16个紧凑的视觉条件标记 <code>c_img</code>。本体状态通过MLP编码为 <code>c_proprio</code>。</li>
<li><strong>动作解码器</strong>：核心是一个Transformer，其输入为噪声动作潜变量 <code>z_t</code> 和 <code>c_proprio</code>。该解码器由多个AdaLN-Zero块构成，通过自适应层归一化注入时间步 <code>t</code> 的信息，并通过交叉注意力层让动作-本体感知序列关注视觉标记 <code>c_img</code>。解码器输出速度场 <code>v_θ</code>，用于在训练时拟合目标，在推理时通过求解ODE（欧拉积分，8步）从噪声生成动作块。</li>
</ul>
<p><strong>3. 注视估计与集成</strong>：训练时有人类注视监督，但测试时需策略自行估计。本文探索了两种方法：</p>
<ul>
<li>**两阶段方法 (Fov-UNet)**：使用一个独立的UNet（ResNet18主干）处理下采样图像，输出热图并通过空间Softmax预测注视点。该模型先单独训练，随后冻结。预测的注视点既用于引导凹窝处理，也作为额外条件与本体感知一起输入策略。</li>
<li>**端到端方法 (Fov-Act)**：将注视视为全身控制的一部分，扩展策略的动作空间，使其联合预测未来的注视轨迹和机器人动作。注视预测同样用于下一步的凹窝处理和策略条件。该方法在流匹配框架内统一了注视与动作的生成。</li>
</ul>
<p><img src="https://arxiv.org/html/2507.15833v2/x3.png" alt="策略架构与注视估计"></p>
<blockquote>
<p><strong>图3</strong>：策略架构与注视估计方法示意图。上半部分展示了两种注视估计途径（Fov-UNet和Fov-Act），下半部分展示了策略的整体架构：图像观测被标记化，由ViT和Q-Former处理为 <code>c_img</code>；本体感知由MLP编码为 <code>c_proprio</code>；流Transformer以 <code>z_t</code>、<code>c_proprio</code> 为输入，并交叉关注 <code>c_img</code> 和条件时间步 <code>t</code>，最终预测速度 <code>v_θ</code>。</p>
</blockquote>
<p><strong>4. 凹窝标记化</strong>：这是核心创新点。根据预测的注视点，对输入图像应用凹窝标记化模式。该模式模仿人类视网膜，在注视点中心放置密集的小尺寸图像块，在周边区域放置稀疏的大尺寸图像块。然后将所有图像块下采样至中心块的大小，输入标准ViT。本文使用的自定义凹窝模式仅包含20个标记。</p>
<p><img src="https://arxiv.org/html/2507.15833v2/x4.png" alt="标记化模式对比"></p>
<blockquote>
<p><strong>图4</strong>：不同图像块标记化方法的可视化对比。从左至右：凹窝模式（20个标记）、精细均匀网格（324个标记）、粗糙均匀网格（20个标记）。</p>
</blockquote>
<p>为了公平比较，论文还为三种标记化模式（凹窝、精细均匀、粗糙均匀）从头训练了ViT-B模型（使用MAE目标）。创新点在于将注视信息有机地融入了机器人学习流程：一是提出了两种将注视集成到策略中的范式（独立预测 vs. 联合预测）；二是首次将适用于ViT的凹窝标记化方案引入机器人模仿学习，用以实现高效、类人的视觉处理。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：扩展的AV-ALOHA仿真基准（6个任务）和两个真实机器人任务（Ball, Toothbrush）。收集了带眼动追踪的人类演示数据（仿真每个任务100条）。</li>
<li><strong>对比方法</strong>：评估了四种策略：<code>Fine</code>（精细均匀标记，324个标记）、<code>Coarse</code>（粗糙均匀标记，20个标记）、<code>Fov-Act</code>（凹窝标记+端到端注视估计）、<code>Fov-UNet</code>（凹窝标记+两阶段注视估计）。所有方法均评估了在ViT随机初始化和使用MAE预训练权重下的表现。</li>
<li><strong>评估环境</strong>：每个任务在“标准”环境和添加了随机颜色/形状干扰物的“干扰”环境下进行评估。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>效率显著提升</strong>：如表I和表II所示，凹窝方法（Fov-Act, Fov-UNet）与精细均匀方法（Fine）相比，在训练时速度快近8倍，GPU内存使用减少约5倍；推理速度快约3倍。这是因为凹窝标记化将ViT的标记数量从324个减少至20个，计算量（GFLOPs）从1905.4大幅降至115.6。</p>
</li>
<li><p><strong>性能保持甚至提升</strong>：仿真任务结果（表III）显示，在没有ViT预训练的标准设置下，<code>Fov-UNet</code> 在多数任务上匹配或超越了基线方法。例如在CubeTransfer任务达到100%成功率，在ThreadNeedle和PourTestTube任务也表现出色。这表明凹窝视觉不仅节约计算，还可能通过聚焦关键区域来辅助精细操作。</p>
</li>
<li><p><strong>抗干扰鲁棒性增强</strong>：在干扰物设置下，凹窝方法，特别是<code>Fov-Act</code>和<code>Fov-UNet</code>，通常比均匀标记方法表现更稳定或更好。例如在ThreadNeedle（有干扰）任务中，使用MAE预训练的<code>Fov-Act</code>取得了68%的成功率，显著高于<code>Fine</code>的48%和<code>Coarse</code>的40%。这证实了注视引导的凹窝处理能帮助策略忽略无关背景。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.15833v2/x6.png" alt="仿真与真实任务"></p>
<blockquote>
<p><strong>图6</strong>：仿真与真实机器人任务示意图。展示了评估所用的各类操作任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.15833v2/x7.png" alt="干扰物示例"></p>
<blockquote>
<p><strong>图7</strong>：评估任务中随机放置的干扰物对象示例，用于测试策略在杂乱环境中的鲁棒性。</p>
</blockquote>
<ol start="4">
<li><p><strong>真实机器人验证</strong>：真实实验（表IV）表明，<code>Fov-UNet</code> 在Ball任务（有干扰）上取得了56%的成功率，高于<code>Fine</code>的48%，验证了方法在现实世界的有效性和鲁棒性。</p>
</li>
<li><p><strong>消融与预训练影响</strong>：MAE预训练普遍提升了所有方法的性能。两阶段注视估计方法<code>Fov-UNet</code>在大多数情况下表现优于端到端的<code>Fov-Act</code>，作者分析这可能是因为UNet提供的空间归纳偏差更有利于注视点预测的稳定性。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2507.15833v2/x5.png" alt="MAE重建可视化"></p>
<blockquote>
<p><strong>图5</strong>：不同标记化模式经过MAE预训练后的重建结果可视化。从左至右：输入图像、编码器可见的标记子集、解码器重建的全图。展示了各种模式下的重建能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>生物启发的凹窝视觉系统</strong>：成功将凹窝ViT引入机器人学习，通过注视点引导的非均匀标记化，在保持甚至提升性能的同时，将视觉标记数量和相应计算量减少了94%。</li>
<li><strong>注视增强的策略学习框架</strong>：提出并系统比较了两种集成注视的范式——模块化的两阶段方法和新颖的端到端联合预测方法，为后续研究提供了设计思路。</li>
<li><strong>公开基准与数据集</strong>：开源了GIAVA硬件/仿真平台及包含同步注视数据的数据集，为社区探索注视与主动视觉在机器人学习中的应用提供了基础。</li>
</ol>
<p><strong>局限性</strong>：论文提到，由于计算资源限制，使用的MAE预训练数据规模（6万张图像）远小于标准的ImageNet-1K，这可能限制了ViT基线的性能上限。此外，端到端注视估计方法因直接预测关键点而缺乏空间归纳偏差，在动态场景中可能更容易出错。</p>
<p><strong>启示</strong>：这项工作表明，人类注视和凹窝视觉作为一种有效的归纳偏差，在机器人视觉系统中具有巨大潜力。它启示后续研究可以进一步探索：更高效的注视预测模型、将凹窝处理与更多样的机器人学习范式（如强化学习）结合，以及利用更丰富的多模态注视数据（如注视持续时间、扫视路径）来进一步提升机器人感知的智能和效率。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人视觉处理被动均匀、效率低且鲁棒性差的问题，探索通过模仿人类注视和中央凹视觉来提升性能。提出GIAVA系统，扩展AV-ALOHA平台以收集眼动追踪和操作数据，并集成中央凹Vision Transformers，采用中央凹补丁标记化方案减少计算令牌。研究了独立预测注视的两阶段模型和端到端联合估计方法。实验表明，该方法显著降低计算开销，增强对背景干扰的鲁棒性，并在高精度任务中提高成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.15833" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>