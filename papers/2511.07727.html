<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.07727" target="_blank" rel="noreferrer">2511.07727</a></span>
        <span>作者: Zhang, Xiaohan, Ding, Yan, Hayamizu, Yohei, Altaweel, Zainab, Zhu, Yifeng, Zhu, Yuke, Stone, Peter, Paxton, Chris, Zhang, Shiqi</span>
        <span>日期: 2025/11/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前任务与运动规划（TAMP）方法大多针对纯操作任务，但现实中的服务机器人常需在移动中操作（MoMa），例如布置餐桌。现有MoMa系统通常需要明确的目标位置指令，而真实的用户请求往往是欠明确的（如“布置餐桌”），这需要大量常识知识来确定物品的语义化摆放（如叉子在盘子左侧）。虽然大型语言模型（LLM）蕴含丰富的常识，但现有结合LLM的机器人规划方法多在离散任务层面，缺乏对运动层面可行性和效率的精细考量。本文针对欠明确目标下的多物体移动操作重排问题，提出利用LLM的常识知识生成语义化目标配置，并引入视觉感知来评估和优化运动层面的可行性与任务完成效率。其核心思路是：首先利用LLM生成物体间的符号化及几何空间关系，然后通过一个视觉驱动的任务与运动规划器，在考虑导航与操作不确定性的前提下，生成兼顾高成功率和低执行成本的可执行计划。</p>
<h2 id="方法详解">方法详解</h2>
<p>LLM-GROP的整体框架分为两大核心组件：用于目标生成的LLM模块和用于生成可执行计划的任务与运动规划器（TAMP）。系统输入是用户的服务请求（如“用叉子、刀和盘子布置餐桌”），输出是机器人可执行的任务-运动计划。</p>
<p><img src="https://arxiv.org/html/2511.07727v1/GROP+LLM-GROP.jpg" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：LLM-GROP方法总览。上半部分（LLM）根据用户请求生成物体的符号化及几何空间关系。下半部分（任务与运动规划器）基于LLM提供的目标配置、环境地图和实时视觉输入（顶部俯视图），利用GROP模块评估动作可行性并选择机器人站位，最终生成包含导航和操作交替进行的任务-运动计划。</p>
</blockquote>
<p><strong>1. LLM引导的目标生成</strong>：首先，通过精心设计的提示模板（可为零样本或少样本）查询LLM，生成物体间的符号化空间关系（如“叉子在盘子左边”）。为确保逻辑一致性，使用答案集编程（ASP）对LLM的输出进行逻辑冲突检测（如一个物体不能同时“在下方”和“在右侧”），若冲突则重新提示LLM。随后，将符号关系转化为具体的几何关系（即物体在桌面上的2D坐标）。这一步骤使机器人能够从欠明确的指令中推断出符合常识的、具体的摆放目标。</p>
<p><strong>2. 视觉接地的任务与运动规划</strong>：这是方法的另一创新核心，其关键子模块是GROP（Grounded Robot Task and Motion Planning）。GROP旨在解决“在哪里站立以执行操作”的问题，它通过视觉输入评估不同站位下“导航-操作”联合行为的可行性。</p>
<p><img src="https://arxiv.org/html/2511.07727v1/ijrr_grop.jpg" alt="GROP训练与数据收集"></p>
<blockquote>
<p><strong>图3</strong>：GROP的数据收集与训练流程。在仿真中，对于给定的任务（目标物体及障碍物配置），遍历每个像素点作为导航目标，多次尝试执行“导航到该点 -&gt; 操作物体”的流程，统计成功率，生成表征各位置可行性的热力图。这些“图像-热力图”对构成训练数据集，用于训练一个全卷积网络（FCN）。训练好的FCN能够根据新的环境俯视图，直接预测出类似的热力图，用于评估动作可行性并为运动规划选择最优的导航目标（机器人站位）。</p>
</blockquote>
<p>规划器以LLM生成的几何目标、先验地图和实时俯视图作为输入。它利用训练好的GROP网络预测的热力图来量化每个候选导航目标（对应一个符号位置）的可行性概率。规划过程需要优化一个效用函数：𝒰(𝑝) = ℛ · ℱ(𝑝) – 𝒞(𝑝)。其中，ℱ(𝑝)是计划𝑝的可行性（成功概率），𝒞(𝑝)是执行总成本（如时间、距离），ℛ是成功奖励。该公式明确地在高可行性（高成功率）和高效率（低成本）之间进行权衡。规划器搜索既能满足物体摆放约束，又能最大化该效用的任务序列（交替的导航和操作动作）及对应的运动轨迹。</p>
<p><strong>创新点</strong>：1) <strong>常识知识注入</strong>：利用LLM从欠明确指令中生成符合常识的语义化目标配置，无需针对性的训练数据。2) <strong>视觉接地的可行性-效率联合优化</strong>：通过GROP模块，将视觉感知与动作的可行性概率估计深度融合，并在规划目标中显式地权衡可行性与效率，而非只追求成功率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在模拟环境（使用AI2-THOR仿真器）和真实世界（Hello Robot Stretch）中进行了评估。任务涉及使用多个餐具（叉、刀、盘、杯垫、杯子等）布置餐桌，环境中包含随机摆放的椅子作为未知障碍。</p>
<p><strong>对比基线</strong>：包括纯优化成本的快速前进规划器（RRT）、仅最大化可行性的方法（Feasibility-only，类似GROP但不考虑成本）、以及两种结合LLM但不进行视觉接地可行性优化的变体（LLM+Cost, LLM+Feasibility）。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>模拟环境成功率与成本</strong>：LLM-GROP在成功率和累计动作成本之间取得了最佳平衡。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.07727v1/trials_results.jpg" alt="模拟试验结果"></p>
<blockquote>
<p><strong>图5</strong>：模拟环境中各方法的性能对比。LLM-GROP（红色）在保持高成功率（约85%）的同时，其动作成本显著低于仅优化可行性的方法（绿色），并与仅优化成本的方法（蓝色）成本相当甚至更低，证明了其权衡的有效性。</p>
</blockquote>
<ol start="2">
<li><strong>真实机器人性能</strong>：在54次真实机器人试验中，LLM-GROP的整体成功率达到84.4%。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.07727v1/SankeyGraph.jpg" alt="真实机器人试验桑基图"></p>
<blockquote>
<p><strong>图6</strong>：真实机器人试验结果的桑基图。展示了54次试验中，成功、失败及其具体原因（如导航失败、操作失败、逻辑错误）的分布。大多数失败源于操作误差，表明运动执行层仍有改进空间。</p>
</blockquote>
<ol start="3">
<li><p><strong>主观人工评估</strong>：邀请参与者对机器人布置的餐桌结果进行评分（1-5分）。LLM-GROP布置的餐桌平均得分为4.31，显著高于仅使用LLM生成几何目标但无后续优化（3.67分）和随机摆放（2.60分）的配置，说明其输出符合人类常识偏好。</p>
</li>
<li><p><strong>消融实验</strong>：</p>
<ul>
<li><strong>LLM组件</strong>：对比了使用不同LLM（GPT-3.5， GPT-4， CodeLlama）生成目标的效果，GPT-4在逻辑一致性上表现最佳。</li>
<li><strong>GROP组件</strong>：移除GROP（即不进行视觉可行性评估）会导致在复杂障碍环境中成功率大幅下降，验证了视觉接地对于在不确定环境中保证动作可行性的关键作用。</li>
</ul>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个统一的框架LLM-GROP，首次将LLM的常识推理能力与视觉接地的、考虑可行性-效率权衡的TAMP深度结合，用于解决欠明确目标下的移动操作重排任务。2) 开发了GROP模块，通过从仿真数据中学习“导航-操作”联合可行性，使机器人能够智能选择站位，弥合了符号任务规划与连续运动执行之间的鸿沟。3) 在模拟和真实机器人实验中验证了方法的有效性，实现了高成功率与高效率的平衡，并获得了优于基线方法的人类主观评价。</p>
<p><strong>局限性</strong>：论文提到，当前方法假设操作动作在可达范围内无噪声，且主要处理静态环境。真实试验中的失败多源于底层的操作误差，这表明虽然高层规划鲁棒，但低层控制精度仍需提升。此外，对LLM提示工程和逻辑一致性检查有一定依赖。</p>
<p><strong>后续启示</strong>：本研究展示了基础模型（LLM）与经典机器人规划（TAMP）结合的潜力。未来工作可探索：1) 引入视觉-语言模型（VLM）进行更精细的场景理解；2) 将动态障碍物或更复杂的物体交互纳入可行性评估；3) 构建闭环系统，使规划能根据执行反馈进行在线重规划。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LLM-GROP框架，解决移动操作机器人面对“布置餐桌”等模糊目标时，多物体重排的任务与运动规划问题。方法结合大型语言模型的常识知识进行任务规划，并利用视觉方法选择机器人基座位置以协调导航与操作。实验显示，该框架在真实世界物体重排任务中达成84.4%的成功率，但人类主观评估仍低于经验丰富的服务员。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.07727" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>