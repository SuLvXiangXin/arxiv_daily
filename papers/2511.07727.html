<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.07727" target="_blank" rel="noreferrer">2511.07727</a></span>
        <span>作者: Zhang, Xiaohan, Ding, Yan, Hayamizu, Yohei, Altaweel, Zainab, Zhu, Yifeng, Zhu, Yuke, Stone, Peter, Paxton, Chris, Zhang, Shiqi</span>
        <span>日期: 2025/11/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>任务与运动规划（TAMP）是机器人领域的核心问题，旨在交织高层任务序列规划和低层可行运动轨迹的生成。在移动操作（MoMa）场景中，机器人需交替执行导航与操作动作以完成多物体重排任务。当前大多数MoMa系统需要明确的指令来指定目标物体的精确位置或排列形状，然而现实中的用户请求往往是欠指定的（underspecified），例如“布置一张带有刀叉和盘子的餐桌”。此类任务要求机器人具备常识，以推断出语义上合理的物体配置（如餐叉应置于盘子左侧）。</p>
<p>现有方法主要依赖大量训练数据来学习语义信息，限制了其在开放世界多样化任务中的泛化能力。本文针对如何利用常识知识理解欠指定目标，并生成兼顾运动可行性与执行效率的规划这一痛点，提出了LLM-GROP框架。其核心思路是：利用大型语言模型（LLMs）的常识知识生成语义合理的物体空间关系与几何目标位姿，并通过一个视觉接地的任务与运动规划器，在考虑导航与操作不确定性的同时，优化规划的整体可行性与效率。</p>
<h2 id="方法详解">方法详解</h2>
<p>LLM-GROP的整体框架包含两个关键组件：LLM引导的目标生成器和基于视觉接地的任务与运动规划器（GROP）。系统输入是用户的欠指定服务请求（如布置餐桌），输出是机器人可直接执行的任务-运动联合规划。</p>
<p><img src="https://arxiv.org/html/2511.07727v1/GROP+LLM-GROP.jpg" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：LLM-GROP方法总览。上方流程：LLM根据用户请求生成物体间的符号及几何空间关系，作为目标配置。下方流程：任务与运动规划器接收目标配置、环境图像及地图，结合GROP模块输出的可行站位热图，生成最终的可执行规划。</p>
</blockquote>
<p><strong>核心模块1：LLM引导的目标生成</strong><br>该模块负责将欠指定指令转化为具体的、符合常识的物体目标位姿。首先，通过模板化提示（Prompt）要求LLM生成物体间的符号空间关系（如“餐叉在盘子左侧”）。为控制输出格式与逻辑一致性，可采用少样本（few-shot）提示，并利用答案集编程（Answer Set Programming, ASP）对LLM的输出进行逻辑一致性检查，若发现矛盾（如一个物体既在另一物体左侧又在其下方），则重新生成。<br>其次，将符号关系映射为具体的几何关系。论文中，通过定义参考点（如桌子中心）和相对方向（左、右、上、下）及距离，将符号关系转化为一组二维坐标候选。这些候选位姿将作为后续规划器的输入目标。</p>
<p><strong>核心模块2：基于GROP的视觉接地任务与运动规划</strong><br>这是规划器的核心，其关键是GROP模块，它通过学习来评估不同机器人站位下执行“导航-操作”动作序列的可行性。</p>
<p><img src="https://arxiv.org/html/2511.07727v1/ijrr_grop.jpg" alt="GROP训练流程"></p>
<blockquote>
<p><strong>图3</strong>：GROP的数据收集与训练流程。在仿真中，针对每个任务（特定物体卸载目标及障碍物配置），对场景中的每个像素位置作为导航目标进行多次“导航-卸载”试验，统计成功率，形成该任务对应的可行性热图。这些（图像，热图）数据对用于训练一个全卷积网络（FCN）。</p>
</blockquote>
<p>训练好的FCN能够接收一张环境顶视图图像和一个目标物体位置，输出一张热图，预测机器人站在场景中每个位置去操作该目标物体的成功率。在规划时，GROP利用此FCN进行两方面的推理：</p>
<ol>
<li><strong>动作可行性评估</strong>：对于任务规划中的每个候选动作（如“导航至位置A并卸载物体O”），通过查询热图对应位置的值，获得其可行性概率 (F(a))。</li>
<li><strong>最优站位选择</strong>：给定一个操作目标，GROP可以选择热图中可行性最高的像素位置作为具体的导航终点（运动规划目标）。</li>
</ol>
<p><strong>规划与优化</strong><br>规划器采用前向搜索在任务空间构建行动序列（交替的导航与操作动作）。对于每个候选动作，通过GROP评估其可行性，并结合动作执行成本（如导航距离、操作时间）来计算效用（Utility）。整体规划的效用 ( \mathcal{U}(p) = \mathcal{R} \cdot \mathcal{F}(p) - \mathcal{C}(p) )，其中 ( \mathcal{R} ) 是成功奖励，( \mathcal{F}(p) ) 是规划可行性（基于动作可行性连乘估算），( \mathcal{C}(p) ) 是累计成本。规划器的目标是找到效用最高的任务-运动规划。</p>
<p><strong>创新点</strong><br>与现有TAMP或LLM用于规划的方法相比，LLM-GROP的主要创新在于：1) <strong>利用LLMs进行零样本、常识驱动的几何目标生成</strong>，无需针对特定重排任务进行数据训练；2) <strong>通过视觉接地的学习模型（GROP）显式且概率化地评估动作可行性</strong>，尤其是在考虑导航不确定性对后续操作影响的情况下；3) <strong>在优化目标中统一考虑了可行性概率与执行效率（成本）</strong>，实现了两者的权衡，而非只追求成功率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在仿真（使用AI2-THOR模拟器）和真实世界（Hello Robot Stretch）两种平台上进行评估。任务场景为布置餐桌，涉及从不同位置取回餐具（如盘子、餐叉、餐刀、杯子）并摆放到目标餐桌上，环境中包含随机放置的椅子作为障碍物。</p>
<p><strong>基线方法</strong>：对比了多种方法：1) <strong>Random</strong>：随机选择物体位置和机器人站位；2) <strong>Feasibility-only</strong>：仅使用GROP最大化可行性，忽略成本；3) <strong>Efficiency-only</strong>：仅最小化路径成本，忽略可行性；4) <strong>LLM-only</strong>：仅使用LLM生成目标位姿，但使用简单的最近站位选择策略；5) <strong>StructFormer</strong>：一种需要训练数据的基于Transformer的物体排列预测方法。</p>
<p><img src="https://arxiv.org/html/2511.07727v1/trials_results.jpg" alt="成功率与成本对比"></p>
<blockquote>
<p><strong>图5</strong>：仿真实验中各方法在50次试验中的成功率和平均成本。LLM-GROP取得了最高的成功率（90%），同时保持了较低的成本。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.07727v1/SankeyGraph.jpg" alt="真实机器人试验桑基图"></p>
<blockquote>
<p><strong>图6</strong>：真实机器人32次试验的详细结果桑基图。LLM-GROP完成了84.4%的试验（27/32），失败主要源于感知误差（如物体姿态估计不准）和极端运动规划失败。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>仿真性能</strong>：LLM-GROP成功率最高（90%），显著高于仅考虑效率（72%）或仅考虑可行性（82%）的方法。LLM-only方法成功率仅76%，凸显了GROP视觉接地规划的重要性。</li>
<li><strong>真实世界性能</strong>：LLM-GROP在真实机器人上完成了84.4%的复杂重排任务试验（27/32次成功）。</li>
<li><strong>用户主观评价</strong>：邀请人类参与者对机器人布置的餐桌结果进行评分（1-5分）。LLM-GROP的布置获得了平均4.31分，显著高于Random（3.38分）和Efficiency-only（3.81分）方法，但与专业服务员（4.94分）仍有差距。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.07727v1/scatter_fixed.jpg" alt="消融实验 scatter plot"></p>
<blockquote>
<p><strong>图8</strong>：不同权重配置（权衡可行性F与成本C）下的性能散点图。每个点代表一种规划配置的结果。LLM-GROP选择的配置（红星）位于帕累托前沿（Pareto Frontier）附近，成功实现了高成功率与低成本的平衡。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>通过调整效用函数中可行性与成本的权重，进行了广泛的消融实验。结果（图8）表明，LLM-GROP所采用的参数配置在成功率和成本之间达到了良好的帕累托最优平衡。若过于偏向成本，成功率下降；若过于偏向可行性，成本急剧上升。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个统一的框架<strong>LLM-GROP</strong>，首次将LLMs的常识推理能力与视觉接地的概率化任务-运动规划紧密结合，用于解决欠指定目标下的移动操作重排问题。</li>
<li>设计了<strong>GROP</strong>模块，通过从仿真数据中学习“导航-操作”联合可行性热图，使机器人能够预测不同站位下的成功概率，并据此选择最优站位，有效处理了移动操作中的不确定性。</li>
<li>在规划优化中<strong>同时考虑了动作执行的可行性概率和效率成本</strong>，并通过实验展示了该方法能在两者间取得优越的权衡。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>依赖LLMs提供常识知识，若LLM对特定领域常识理解有误，可能生成不合理的目标。</li>
<li>GROP的热图预测模型需要在特定领域（如特定机器人、桌子、物体类型）的仿真数据上进行训练，泛化到全新物体或环境布局需重新收集数据或进行迁移学习。</li>
<li>当前系统假设操作动作（抓取、放置）在可达范围内是确定成功的，未建模其不确定性。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>展示了<strong>利用基础模型（LLMs）提供高层语义指导，与专精于物理接地的机器人学习/规划模型相结合</strong>的有效范式。</li>
<li>为解决长视野、多步骤移动操作任务中的<strong>长期效率与即时可行性的权衡</strong>问题提供了一个可量化的优化框架。</li>
<li>真实世界实验中暴露的失败案例（感知误差、规划失败）指明了未来改进方向，如<strong>提升感知鲁棒性</strong>、<strong>集成更强大的运动规划器</strong>以及<strong>引入在线重规划机制</strong>以应对执行偏差。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LLM-GROP框架，旨在解决移动操作机器人执行多物体重排任务时，任务与运动规划（TAMP）的集成问题，特别是处理“布置餐桌”这类目标模糊的指令。其关键技术是结合大语言模型的常识知识辅助任务与运动决策，并利用视觉方法选择机器人最佳基座位置以优化移动操作。核心实验表明，该框架在真实世界物体重排任务中取得了84.4%的成功率，但评估显示其性能仍低于经验丰富的人类操作者。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.07727" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>