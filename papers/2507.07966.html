<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Scaling RL to Long Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Scaling RL to Long Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.07966" target="_blank" rel="noreferrer">2507.07966</a></span>
        <span>作者: Chen, Yukang, Huang, Wei, Shi, Baifeng, Hu, Qinghao, Ye, Hanrong, Zhu, Ligeng, Liu, Zhijian, Molchanov, Pavlo, Kautz, Jan, Qi, Xiaojuan, Liu, Sifei, Yin, Hongxu, Lu, Yao, Han, Song</span>
        <span>日期: 2025/07/10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，多模态推理模型在视觉语言模型（VLMs）领域取得了显著进展，但现有方法主要聚焦于单张图像或短视频的理解。长视频理解不仅需要识别，更依赖于跨时间、空间、目标导向和叙事视角的复杂推理。然而，将强化学习（RL）等先进训练方法扩展到长视频场景面临两大关键挑战：首先，高质量长视频推理数据集的构建极其困难，因为需要对跨越数分钟甚至数小时的视频内容中复杂的动态、目标、空间关系和叙事元素进行标注，这一过程劳动密集且主观性强；其次，长视频的RL训练计算成本高昂且样本效率低下，处理数百至数千帧视频需要大量内存和冗长的rollout运行时间，现有RL框架难以应对。本文针对长视频推理能力训练的痛点，提出了一个全栈式解决方案。核心思路是通过构建大规模、高质量的长视频推理数据集（LongVideo-Reason），并结合一个创新的、高效的并行化训练框架（MR-SP），将两阶段训练（CoT监督微调和强化学习）成功扩展到长视频，从而提升VLMs的长视频推理能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>LongVILA-R1框架是一个完整的两阶段训练流程，旨在为视觉语言模型赋予长视频推理能力。</p>
<p><img src="https://arxiv.org/html/2507.07966v4/x5.png" alt="训练流程总览"></p>
<blockquote>
<p><strong>图5</strong>：LongVILA-R1训练流程总览。该流程基于LongVILA的基础训练流程扩展而来。第一阶段（Stage-1）利用多模态序列并行（MM-SP）系统对带有长思维链（CoT）的长视频理解任务进行监督微调（SFT）。第二阶段（Stage-2）通过多模态强化学习序列并行（MR-SP）进行强化学习的扩展训练。</p>
</blockquote>
<p><strong>阶段一：长视频CoT监督微调（Long CoT-SFT）</strong><br>此阶段作为后续RL的“预热”阶段。输入是长视频帧及其对应的高质量“问题-推理链-答案”数据对。利用从LongVideo-Reason数据集中筛选出的36K个高质量样本，在MM-SP系统上进行监督微调。目标是让模型初步掌握针对长视频场景的指令遵循和基础推理能力。MM-SP系统允许高效处理数百帧的输入。</p>
<p><strong>阶段二：长视频强化学习（RL）</strong><br>此阶段旨在通过强化学习进一步扩展和泛化模型的推理能力。输入同样是长视频帧和问题，但训练机制不同。模型采用分组相对策略优化（GRPO）算法。对于每个问题，策略模型会生成一组（G=8）候选回答，并根据基于规则的奖励函数（评估格式正确性和答案准确性）计算每个回答的奖励。模型优化的目标函数（公式1）在标准PPO-Clip目标的基础上，增加了与参考策略的KL散度作为正则项，以防止策略过度偏离。优势值（Ai）通过组内奖励的归一化计算（公式2）。</p>
<p><strong>核心创新：多模态强化学习序列并行（MR-SP）</strong><br>这是应对长视频RL训练挑战的关键技术。现有RL框架无法高效处理长视频带来的海量视觉令牌。MR-SP将序列并行技术集成到RL的rollout和prefill两个阶段，并结合视频嵌入缓存重用，显著提升效率。</p>
<p><img src="https://arxiv.org/html/2507.07966v4/x7.png" alt="MR-SP工作流程"></p>
<blockquote>
<p><strong>图7</strong>：多模态强化学习序列并行（MR-SP）的工作流程。该框架采用定制化的分片策略来处理多模态输入，并集成了高效的视频嵌入重用和基于vLLM的rollout加速。</p>
</blockquote>
<p><strong>MR-SP具体包含两个阶段：</strong></p>
<ol>
<li><strong>并行编码的Rollout阶段（Stage 1）</strong>：输入视频帧被均匀分割到多个GPU上，每个GPU上的视觉编码器独立处理自己分配到的帧子集。编码产生的视频嵌入通过All-Gather操作进行聚合。这些聚合后的嵌入在整个RL流程中被缓存并重复使用（例如用于后续多次rollout），避免了同一视频在每一步训练中被反复编码的巨大开销。</li>
<li><strong>序列并行的Prefill阶段（Stage 2）</strong>：对于每次rollout，策略模型和参考模型都需要进行计算密集的prefill。MR-SP将第一阶段聚合并填充至统一长度的输入嵌入序列，再次分片到各个GPU。每个GPU仅负责处理序列的一部分，并行地进行prefill计算，从而分摊长上下文prefill的计算负载。</li>
</ol>
<p>与现有方法相比，创新点在于：1) <strong>系统性</strong>：首次提出并实现了涵盖高质量数据集、两阶段训练算法和高效训练基础设施的完整长视频RL解决方案；2) <strong>效率</strong>：MR-SP通过视频嵌入缓存和双阶段序列并行，针对性解决了长视频RL的内存和计算瓶颈，实现了显著的训练加速。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究使用了多个主流视频理解基准进行评测，包括ActivityNet-QA、LongVideoBench、PerceptionTest、NExT-QA、VNBench和VideoMME。此外，还构建并使用了新的评测基准LongVideo-Reason-eval，该基准包含1000个手动标注的复杂推理问题，涵盖时间、目标、空间、叙事四个推理维度。实验平台涉及单个节点（8×A100 GPU）。</p>
<p><strong>对比方法</strong>：基线模型包括开源模型如LongVILA-7B、Video-R1-7B、LLaVA-Video-7B、NVILA-8B等，以及闭源模型如GPT-4o、Gemini-1.5-Pro。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>主流基准性能</strong>：如表1所示，LongVILA-R1-7B在所有测试基准上均一致优于其基础版本LongVILA-7B。在VideoMME基准上（表3），LongVILA-R1-7B在使用512帧输入时，在不带字幕和带字幕的设置下分别达到了65.1%和71.1%的准确率，性能领先于同类规模的先进模型。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.07966v4/x11.png" alt="性能对比表1"></p>
<blockquote>
<p><strong>表1</strong>：在多个视频基准测试上的性能对比。LongVILA-R1-7B全面超越LongVILA-7B，优势幅度因任务复杂度而异。</p>
</blockquote>
<ol start="2">
<li><strong>新推理基准性能</strong>：在LongVideo-Reason-eval基准上（表2），LongVILA-R1-7B取得了72.0%的平均准确率，超越了Video-R1-7B（68.1%）和Gemini-1.5-Pro（69.3%），尤其在时间推理和叙事推理类别上表现突出。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.07966v4/x12.png" alt="新基准性能表2"></p>
<blockquote>
<p><strong>表2</strong>：在LongVideo-Reason-eval基准上的性能。LongVILA-R1-7B取得了最强的综合得分。</p>
</blockquote>
<ol start="3">
<li><strong>帧数缩放能力</strong>：消融实验（表4）表明，LongVILA-R1-1.5B的推理能力随着输入视频帧数的增加而持续提升（从16帧的55.9%到512帧的64.3%）。相比之下，未经过RL训练的基础模型在帧数增加到256帧后遭遇瓶颈，并在512帧时性能下降。</li>
</ol>
<p><img src="https://arxiv.org/html/2507.07966v4/x2.png" alt="训练效率对比"></p>
<blockquote>
<p><strong>图2</strong>：MR-SP训练效率对比。在单节点8×A100 GPU上，完整的MR-SP系统（Stage 1&amp;2）相比基线RL系统，在512帧训练上实现了2.1倍的加速，并能扩展到1024帧而不出现GPU内存溢出（OOM）。</p>
</blockquote>
<ol start="4">
<li><p><strong>训练效率</strong>：如图2所示，MR-SP系统显著提升了长视频RL训练效率。在512帧训练中，完整MR-SP相比基线实现了2.1倍的加速，并且能够处理1024帧而基线系统会发生OOM。</p>
</li>
<li><p><strong>流程与数据消融</strong>：消融实验（表5）验证了两阶段训练流程和自建数据集的有效性。仅使用CoT-SFT或仅使用RL的效果均不如两阶段结合。使用自建的LongVideo-Reason数据进行CoT-SFT和RL训练，效果优于使用其他视频数据集。</p>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>数据集贡献</strong>：构建了大规模、高质量的长视频推理数据集LongVideo-Reason（104K QA对），并提出了自动化的标注流程；2) <strong>方法贡献</strong>：提出了一个完整的两阶段训练框架（CoT-SFT + RL），成功将强化学习应用于长视频场景以提升推理能力；3) <strong>工程贡献</strong>：设计了高效的多模态强化学习序列并行（MR-SP）系统，通过嵌入缓存和序列并行解决了长视频RL训练的内存与计算瓶颈，实现了显著的训练加速。</p>
<p>论文提到的局限性在于：尽管MR-SP系统能在单节点上高效处理长达3600帧（约小时级）的视频，但要扩展到更长的序列、融合更多模态（如音频）或使用更大的批次规模，仍需跨多节点的分布式训练，这对计算资源提出了更高要求。</p>
<p>本工作对后续研究的启示在于：它为长视频理解领域提供了一套可扩展的“数据-算法-系统”全栈解决方案，证明了通过精心设计的数据集和高效的并行化训练基础设施，能够将需要大量计算资源的先进训练方法（如RL）成功应用于长序列多模态场景。这为未来探索更复杂的视频理解任务、多模态融合以及面向具身智能等应用的模型训练提供了重要的技术路径和开源工具基础。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出一个全栈框架，将强化学习（RL）扩展到长视频推理，解决长视频理解中复杂的时间、空间、目标和叙事推理挑战。关键技术包括：大规模数据集LongVideo-Reason（104K QA对）、两阶段训练管道（链式思维监督微调CoT-SFT和RL）以及多模态强化序列并行（MR-SP）基础设施。实验表明，LongVILA-R1-7B在VideoMME基准上达到65.1%（无字幕）和71.1%（有字幕）准确率，支持处理最多8,192视频帧，且MR-SP实现高达2.1倍训练加速。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.07966" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>