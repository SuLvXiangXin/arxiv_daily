<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.14994" target="_blank" rel="noreferrer">2508.14994</a></span>
        <span>作者: Marcelo Becker Team</span>
        <span>日期: 2025-08-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在危险和远程环境中，机器人系统执行关键任务需要更高的安全性和效率。其中，配备机械臂的四足机器人因其移动性和多功能性，适用于复杂操作。然而，遥操作四足机器人面临挑战，主要因为缺乏集成的障碍物检测和直观的机械臂控制方法，在受限或动态变化的工作空间中增加了碰撞风险。当前主流的遥操作方法依赖于游戏手柄等传统输入设备，对于控制高自由度移动机械臂而言不够直观，需要较高的专业技能，导致操作员认知负荷高。为提升直观性，基于可穿戴传感器（如IMU或肌电图臂环）的人机交互接口被提出，但它们存在部署繁琐、校准耗时、物理接触侵入性强以及信号易受干扰等问题。基于视觉的接口提供了非侵入式替代方案，但单目RGB相机缺乏深度信息，导致3D姿态估计精度不足、存在尺度模糊和随时间漂移的问题，影响动态操作的精确性。</p>
<p>本文针对上述痛点，提出了一种基于视觉的共享控制遥操作框架，旨在为四足机器人的机械臂提供直观、实时的控制。其核心思路是：利用外部RGB-D深度相机追踪操作者手腕的3D位置，并将其映射为机器人末端执行器的目标位姿，同时通过识别特定手势（如握拳、张开手掌）来触发离散动作指令（如抓取、释放），实现一种结合连续运动控制与离散指令的共享控制方案。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架分为两个主要部分：基于相机的运动捕捉系统和控制框架。运动捕捉系统负责实时获取操作者手腕的3D位姿，控制框架则根据该位姿以及识别到的手势，生成相应的机器人控制指令，实现手动遥操作或半自主抓取。</p>
<p><img src="https://arxiv.org/html/2508.14994v2/figures/diagram.drawio.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：提出的控制流程概览。展示了从运动捕捉到机器人驱动的完整流程。左侧为手动遥操作模块，用户通过外部相机直观控制机器人；右侧为半自主模块，可通过用户握拳手势触发，使机器人自主抓取物体。该框架用于执行不同物体的拾放任务。</p>
</blockquote>
<p><strong>核心模块1：基于相机的运动捕捉系统</strong><br>该系统使用Intel RealSense D435i RGB-D相机（30 Hz，640×480分辨率）获取同步的RGB和深度图像。流程包含两个阶段：</p>
<ol>
<li><strong>参考系标定</strong>：使用一个ArUco标记建立稳定的世界参考系。系统利用相机内参和检测到的标记位姿，计算从相机坐标系到标记坐标系的齐次变换矩阵 ( T_{\text{marker}}^{\text{camera}} )。</li>
<li><strong>手腕跟踪</strong>：使用MediaPipe Pose从RGB图像流中连续提取操作者右手腕的2D像素坐标 ((u, v))。从对齐的深度图像中，在手腕坐标周围取一个5×5像素窗口的深度中值 (D_{\text{raw}})，以提高鲁棒性。随后通过反投影（公式2）得到手腕在相机坐标系下的3D位置 (p_{\text{camera}}^{\text{wrist}} = [X_c, Y_c, Z_c]^T)。应用指数移动平均滤波器（α=0.5）平滑轨迹，并过滤掉帧间跳跃大于25厘米的异常更新。最后，利用标定矩阵将手腕位置变换到ArUco标记坐标系：(p_{\text{marker}}^{\text{wrist}} = T_{\text{marker}}^{\text{camera}} \cdot p_{\text{camera}}^{\text{wrist}})。</li>
</ol>
<p>此外，系统利用MediaPipe输出的3D手部关键点计算手掌朝向，用于控制机器人末端执行器的方向。具体使用手腕（点0）、食指掌指关节（点5）和小指掌指关节（点17）三个点计算手掌法向量，进而推导出控制机器人末端滚转的四元数。</p>
<p><img src="https://arxiv.org/html/2508.14994v2/figures/hand.png" alt="手部标记点"></p>
<blockquote>
<p><strong>图3</strong>：MediaPipe 3D手部关键点编号顺序。这些关键点用于跟踪用户手部，以实现对机器人手臂的直观遥操作。</p>
</blockquote>
<p><strong>核心模块2：控制框架</strong><br>该框架基于ROS和Boston Dynamics Spot SDK构建，整合了MoveIt运动规划功能。控制流程以运动捕捉系统提供的操作者手腕位姿 (p_{\text{marker}}^{\text{wrist}}) 为主要输入。</p>
<ol>
<li><strong>用户输入处理</strong>：手腕位姿通过静态标定变换 (T_{\text{robot}}^{\text{marker}}) 转换到机器人的操作坐标系，作为目标末端位姿 (p_{\text{robot}}^{\text{target_ee}})。系统启动时，基于手部关键点识别举起的手指数量来选择操作模式：举起一根手指激活<strong>手动遥操作模式</strong>，举起两根手指激活<strong>半自主抓取模式</strong>。在操作过程中，一个独立的手势分类器实时识别特定手势（张开手掌、握拳），并映射为相应动作指令。</li>
<li><strong>手动遥操作模式</strong>：这是主要的控制模式。系统运行一个闭环控制循环（约5 Hz），持续将Spot机械臂的末端执行器与操作者手腕的推断位姿同步。与传统的ROS速度控制不同，本工作通过Spot SDK发送直接的位置命令。夹持器的开合由对应的手势触发。</li>
<li><strong>半自主抓取模式</strong>：当用户做出握拳手势时触发。该模式利用安装在Spot夹持器上的摄像头和YOLOv11目标检测网络，识别场景中可操作物体并估计其3D位姿。随后，机器人利用Spot SDK内置的抓取规划与执行功能，自主完成接近物体和抓取的动作。释放物体则由张开手掌手势触发。</li>
</ol>
<p>与现有方法相比，本工作的创新点具体体现在：1) <strong>非侵入式与低成本</strong>：仅需一个外部RGB-D相机，无需任何可穿戴设备或复杂校准。2) <strong>混合控制范式</strong>：将<strong>连续</strong>的手腕位置映射与<strong>离散</strong>的手势指令识别相结合，实现了直观的“移动-抓取”控制流程。3) <strong>共享控制架构</strong>：通过简单的手势切换手动与半自主模式，在保持用户直观控制的同时，将复杂的抓取动作规划交由机器人自主完成，降低了操作负担。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：实验使用Boston Dynamics Spot四足机器人（配备6自由度机械臂和夹持器）作为实体平台。在Gazebo中建立了仿真环境进行前期验证。真实实验涉及拾放日常物体（如圆柱形薯片罐、清洁产品）。</p>
<p><strong>对比基线</strong>：论文主要对提出的框架本身进行了性能验证，包括仿真测试、真实任务执行和精度分析，并未与特定的现有遥操作方法进行量化对比，但通过背景分析指出了相较于传统手柄、可穿戴设备等方案的潜在优势。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>任务成功率</strong>：两名用户使用该框架成功完成了多次拾放任务。在首次试验中，两人均成功抓取并放置了一个圆柱形薯片罐；在第二次试验中，对清洁产品也完成了类似任务。这证明了系统在实时操作中的功能性和鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.14994v2/figures/teleop_sequence.png" alt="遥操作序列"></p>
<blockquote>
<p><strong>图4</strong>：使用机器人手臂的遥操作框架概览。a) 操作者向相机举起食指，激活手动控制模式。b) 机器人将其手臂与估计的用户手腕位置对齐。c) 手臂接近目标物体。d) 通过握拳手势执行抓取。e) 操作者做出张开手掌手势以启动释放。f) 夹持器打开，物体被放下。</p>
</blockquote>
<ol start="2">
<li><strong>系统精度分析</strong>：为定量评估，研究使用Optitrack运动捕捉系统（精度高达±0.10 mm）作为地面真值，同时测量用户手腕（通过本系统估计）和机器人末端执行器的位置。</li>
</ol>
<p><img src="https://arxiv.org/html/2508.14994v2/figures/plot1.png" alt="位置跟踪对比"></p>
<blockquote>
<p><strong>图5</strong>：使用所提框架姿态估计解码的用户手腕位置、使用Optitrack系统获取的手腕位置，以及使用Optitrack系统捕获的经转换后的机器人末端执行器位置。这些测量值用于计算位置误差，以Optitrack的值为地面真值。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14994v2/figures/plot2.png" alt="位置误差与速度关系"></p>
<blockquote>
<p><strong>图6</strong>：使用所提框架解码的手腕位置的位置误差，以及使用Optitrack系统捕获的手腕速度。结果显示平均位置误差为0.07米，且误差随着手腕速度的增加而增加。这是因为机器人的末端执行器需要时间到达目标位置，导致了滞后的误差。</p>
</blockquote>
<ol start="3">
<li><strong>精度数据</strong>：如图5和图6所示，手腕位置估计与机器人末端执行器跟踪的平均位置误差为<strong>0.07米</strong>。分析指出，误差随着操作者手腕运动速度的增加而增大，这是由于机器人末端执行器追踪目标位姿存在响应延迟所致，符合预期。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出并实现了一种<strong>直观、非侵入式且低成本</strong>的视觉共享控制框架，用于四足机器人机械臂的遥操作，仅需一个RGB-D相机，无需可穿戴设备。</li>
<li>设计了一种<strong>混合控制范式</strong>，将连续的手腕3D位置跟踪与离散的手势识别相结合，并通过手势在手动遥操作和半自主抓取模式间无缝切换，实现了有效的共享控制。</li>
<li>在真实的Spot机器人平台上成功验证了该框架，通过拾放任务展示了其<strong>实时性、功能性和鲁棒性</strong>，并定量分析了系统的定位精度。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，由于相机在用户背对时无法进行鲁棒的姿态估计，因此操作物体必须放置在机器人前方，这限制了工作空间的灵活性。此外，半自主抓取模式依赖于Spot专有的SDK进行抓取规划，限制了自定义抓取策略的可能性。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>增强自主性</strong>：未来工作可以集成路径规划器和势场法等共享控制策略，以实现主动的避障功能，进一步提升操作安全性。</li>
<li><strong>扩展评估</strong>：计划进行更全面的用户研究，并与传统遥操作接口进行对比实验，以量化其在降低认知负荷、提升操作效率等方面的优势。</li>
<li><strong>提升系统鲁棒性</strong>：可以探索更复杂的手势库或结合其他模态（如语音），以支持更丰富的交互指令，并增强在不同光照、背景下的手势识别鲁棒性。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对四足机器人机械臂遥操作中存在的控制不直观、缺乏障碍物检测导致碰撞风险高的问题，提出了一种基于视觉的共享控制方案。其关键技术是：利用外部摄像头与机器学习模型构建视觉姿态估计管道，实时检测操作者手腕位置并映射为机械臂指令；同时结合轨迹规划器检测并防止与障碍物及机械臂自身发生碰撞。实验在真实机器人上验证了该方案，实现了实时、鲁棒的遥操作控制。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.14994" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>