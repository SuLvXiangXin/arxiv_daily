<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Visual Agentic Reinforcement Fine-Tuning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Visual Agentic Reinforcement Fine-Tuning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.14246" target="_blank" rel="noreferrer">2505.14246</a></span>
        <span>作者: Liu, Ziyu, Zang, Yuhang, Zou, Yushan, Liang, Zijian, Dong, Xiaoyi, Cao, Yuhang, Duan, Haodong, Lin, Dahua, Wang, Jiaqi</span>
        <span>日期: 2025/05/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型推理模型（LRMs）的一个关键趋势是具备原生代理能力，能够使用外部工具（如网络浏览器搜索、编写/执行代码进行图像处理）来进行多模态推理。在开源研究社区，虽然纯语言代理能力（如函数调用和工具集成）取得了显著进展，但涉及真正“用图像思考”的多模态代理能力及其相应基准的开发仍处于探索不足的阶段。主流方法依赖监督微调，需要精心策划的演示数据或人工设计的轨迹，成本高昂且难以泛化。本文针对多模态代理能力训练这一痛点，提出了<strong>视觉代理强化微调（Visual-ARFT）</strong>的新视角，旨在为大型视觉语言模型（LVLMs）赋予灵活、自适应的推理和工具使用能力。其核心思路是：<strong>利用基于规则的可验证奖励信号，通过强化微调策略，教导LVLMs自主进行任务规划、分解、多步推理，并在需要时调用搜索或代码执行工具来解决复杂的多模态问题。</strong></p>
<h2 id="方法详解">方法详解</h2>
<p>Visual-ARFT的整体框架旨在训练一个策略模型π_θ，使其能够处理多模态输入，生成包含中间推理步骤和工具调用决策的序列输出，以完成特定任务。该框架主要应用于两个场景：<strong>代理搜索</strong>和<strong>代理编码</strong>。</p>
<p><img src="https://arxiv.org/html/2505.14246v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：Visual-ARFT整体框架。成功赋予LVLMs多模态代理能力，包括（a）代理搜索和（b）代理编码，使其能够通过推理、分解和工具交互解决复杂的多模态任务。</p>
</blockquote>
<p>在<strong>代理搜索</strong>场景中，模型面对复杂的多模态多跳视觉问答（VQA）查询，需要分析并分解问题，通过调用网络搜索工具迭代获取外部知识来回答。在<strong>代理编码</strong>场景中，输入图像可能存在视觉退化（如旋转、模糊）或包含冗余内容，模型需要生成可执行的Python代码（如裁剪、旋转）对图像进行预处理，以提取有效信息并准确回答问题。</p>
<p><strong>核心模块是模块化的可验证奖励设计</strong>。与依赖学习型奖励模型或人类偏好的传统RLHF不同，Visual-ARFT采用基于规则的可验证奖励，为模型提供明确、客观的学习信号。奖励由两部分组成：</p>
<ol>
<li><strong>格式奖励（R_format）</strong>：鼓励模型严格遵循预定义的输出格式。推理过程需包裹在‘<think></think>’标签内；调用搜索工具时输出‘<search>查询内容</search>’；调用编码工具时输出‘<code>Python代码</code>’。此奖励确保工具调用的可靠性和输出可解析性。</li>
<li><strong>准确性奖励（R_acc）</strong>：根据输出内容类型给予不同奖励：<ul>
<li><strong>最终答案</strong>：使用答案与标准答案之间的<strong>F1分数</strong>作为奖励（R_F1），比精确匹配（EM）更宽容，提供更平滑的学习信号。</li>
<li><strong>搜索查询</strong>：计算模型生成的搜索查询与真实查询之间的<strong>语义相似度</strong>（R_sem），使用Sentence Transformer编码，对表面形式变化更鲁棒。</li>
<li><strong>代码块</strong>：只要输出在有效的代码标签内，即给予奖励值<strong>1</strong>。论文指出，不对代码内容本身施加严格正确性监督，是为了避免模型收敛到少数确定性解决方案，鼓励更广泛的探索并保持决策的代理性。</li>
</ul>
</li>
</ol>
<p>总奖励为两者之和：R_total(q, o) = R_format(o) + R_acc(q, o)。训练采用<strong>组相对策略优化（GRPO）</strong>算法，在最大化期望奖励的同时，通过KL散度项约束更新后的策略不过分偏离参考模型，以保持训练稳定性和泛化能力。</p>
<p>与现有方法相比，创新点具体体现在：<strong>首次将基于可验证奖励的强化微调系统性地应用于多模态代理任务</strong>，设计了针对多模态搜索和编码场景的模块化奖励函数，避免了大规模人工标注数据或学习奖励模型的需求。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>Benchmark/数据集</strong>：本文提出了新的<strong>多模态代理工具基准（MAT）</strong>，包含MAT-Search（150个人工标注的多模态多跳VQA样本）和MAT-Coding（200个通过自动流程生成的、涉及图像扭曲的VQA样本）。此外，还在现有的纯文本多跳QA基准上进行了泛化能力测试，包括2WikiMultihopQA、HotpotQA、MuSiQue和Bamboogle。<br><strong>实验平台</strong>：在8张GPU上使用GRPO算法对Qwen2.5-VL-3B和7B模型进行训练。<br><strong>Baseline方法</strong>：对比了开源模型（LLaVA系列、Xcomposer2.5、InternVL2.5、Qwen2.5-VL）和闭源模型（GPT-4o、OpenAI-o3）。在文本多跳QA上，还对比了Search-o1、Search-R1、ZeroSearch等方法。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>在MAT-Coding上</strong>：经过Visual-ARFT微调的Qwen2.5-VL-7B模型相比基线，在平均指标上取得了**+18.56% F1<strong>和</strong>+13.00% EM**的显著提升。在难度更高的“Hard”子集上提升尤为明显（F1提升25.93%）。值得注意的是，仅3B的模型在MAT-Coding上的表现也超过了GPT-4o（35.90 vs 34.41 F1）。</li>
<li><strong>在MAT-Search上</strong>：Visual-ARFT使Qwen2.5-VL-7B模型相比基线获得了**+10.28% F1<strong>和</strong>+8.66% EM**的平均提升。同样，其表现也超越了GPT-4o（63.77 vs 61.08 F1）。</li>
<li><strong>在现有文本多跳QA基准上的泛化</strong>：Visual-ARFT展现出强大的泛化能力。例如，在2Wiki和HotpotQA上，相比强大的Search-R1基线，分别取得了**+29.3% F1<strong>和</strong>+25.9% EM**的显著增益。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.14246v1/x4.png" alt="MAT基准结果表"></p>
<blockquote>
<p><strong>图4</strong>：在MAT-Coding和MAT-Search上的主要结果对比表。Visual-ARFT显著提升了Qwen2.5-VL模型性能，并在多个指标上超越GPT-4o。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.14246v1/x5.png" alt="定性结果示例"></p>
<blockquote>
<p><strong>图5</strong>：MAT-Coding任务上的定性示例。展示了模型如何通过生成并执行代码（如裁剪、旋转）来处理扭曲图像，从而正确回答问题。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.14246v1/x6.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融实验。对比了不同奖励组件（格式、F1、语义相似度）和不同训练方法（SFT vs. Visual-ARFT）的影响，验证了完整奖励设计和强化微调的有效性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验表明，<strong>格式奖励</strong>对于确保正确的工具调用格式至关重要；使用<strong>F1奖励</strong>（而非EM）和<strong>语义相似度奖励</strong>能带来更优的性能；与监督微调（SFT）相比，<strong>Visual-ARFT（强化微调）</strong> 能更有效地提升模型的代理能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出Visual-ARFT框架</strong>：首次系统地将基于可验证奖励的强化微调应用于多模态代理任务，为开源LVLMs赋予了任务规划、推理及使用搜索和编码工具的能力。</li>
<li><strong>设计模块化可验证奖励</strong>：针对搜索和编码任务，设计了格式奖励与准确性奖励相结合的奖励函数，无需偏好数据或学习型奖励模型，引导模型学习结构化、可解释的行为。</li>
<li><strong>构建多模态代理基准（MAT）</strong>：提出了包含MAT-Search和MAT-Coding的高质量基准，填补了多模态代理能力评估的空白。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：MAT基准的规模相对较小（共350个高质量样本）；代理编码任务中对生成代码的内容未施加直接正确性监督，这可能在需要精确代码的场景中存在风险。</p>
<p><strong>对后续研究的启示</strong>：Visual-ARFT证明了强化微调是赋予多模态模型代理能力的一条有效且数据高效的路径。其基于规则的可验证奖励设计为构建更复杂、更安全的代理奖励函数提供了思路。未来的工作可以探索将更多类型的工具（如图像生成、数据库查询）集成到该框架中，并研究如何设计更精细的奖励以平衡代码生成的灵活性与正确性。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出视觉智能体强化微调方法，解决开源大型视觉语言模型缺乏使用外部工具进行图像思考与实时交互的智能体能力问题。通过强化微调技术，使模型能够浏览网页获取实时信息，并编写代码对图像进行裁剪、旋转等处理。在自建的多模态智能体工具评测基准上，该方法在编码任务上相比基线提升18.6% F1/13.0% EM，在搜索任务上提升10.3% F1/8.7% EM，性能超越GPT-4o，并在多跳问答基准上展现出强泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.14246" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>