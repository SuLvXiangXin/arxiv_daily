<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Coordinated Humanoid Manipulation with Choice Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Coordinated Humanoid Manipulation with Choice Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.25072" target="_blank" rel="noreferrer">2512.25072</a></span>
        <span>作者: Jitendra Malik Team</span>
        <span>日期: 2025-12-31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人在以人为中心、非结构化的环境中执行复杂任务潜力巨大，但这需要协调头、手、身体进行主动搜索、定位、抓取和操作。实现这种水平的灵巧性和灵活性，特别是全身协调以及移动与操作的紧密结合，仍然是一个重大挑战。从演示中学习是获取机器人技能的常用方法，但面临两大关键局限：第一，整合所有组件（手眼协调、上下半身协调、腰部运动）非常困难；第二，现有方法在效率与表达能力之间存在权衡：扩散策略等生成模型能捕捉演示的多模态性，但迭代采样推理速度慢，难以满足实时自适应移动操作的要求；而行为克隆等方法虽然推理快，但表达能力不足，容易将多模态数据平均化，导致次优或不稳定的动作。</p>
<p>本文针对上述痛点，提出了一种结合模块化遥操作界面与可扩展学习框架的系统。核心思路是：首先，通过将人形机器人控制分解为手眼协调、抓取原语、手臂末端执行器跟踪和移动等直观子模块，高效收集高质量的全身协调演示数据；其次，提出一种名为“选择策略”的模仿学习方法，通过单次前向传播生成多个候选动作并学习为其评分，从而兼顾快速推理与对多模态行为的有效建模。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体系统包含两大核心部分：模块化遥操作界面用于数据收集，以及Choice Policy用于从收集的数据中学习自主策略。</p>
<p><img src="https://arxiv.org/html/2512.25072v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：模块化遥操作界面概览。控制被分解为四个模块：手臂控制（末端执行器跟踪）、手部控制（扳机键控制力量/精确抓握，摇杆控制拇指）、头部控制（跟踪左手或右手的手眼协调）、移动控制（全向行走）。单个摇杆在拇指控制和移动模式间共享；按下摇杆可在两种模式间切换。图中展示了两种人形平台：一种固定在架子上用于无需移动的操作任务，另一种具备完整行走能力。</p>
</blockquote>
<p><strong>模块化遥操作界面</strong>旨在简化高维全身控制，其具体模块与技术细节如下：</p>
<ol>
<li><strong>手臂控制</strong>：采用按需激活模式。仅当按下VR控制器扳机键时，控制器姿态的相对变化被映射到机器人坐标系，并计算绝对的末端执行器姿态，再通过逆运动学求解目标关节位置。这允许操作者依次操作单臂，防止空闲手臂漂移，减少疲劳，并可通过迭代重置控制器来扩展工作空间。</li>
<li><strong>手部控制</strong>：将四根非拇指手指分组，通过握柄按钮统一驱动；拇指则由摇杆独立控制。两者均提供连续值信号以映射到手指驱动，实现了对力量抓握、精确抓握等基本抓取分类的细粒度控制，同时避免了高自由度手指跟踪带来的抖动和不稳定。</li>
<li><strong>手眼协调</strong>：通过按钮触发跟踪模式，使头部跟随左手或右手。根据手部在机器人基座坐标系中的位置<code>p_h</code>和头部位置<code>p_head</code>，计算相对位移向量<code>r</code>，进而解算出使头部朝向手部的期望偏航角和俯仰角（横滚角固定为零），并在发送前进行关节限位裁剪。这确保了头部摄像头持续指向被选中的手，使操作区域保持在视野内。</li>
<li><strong>移动策略</strong>：对于需要下半身移动的任务，使用在仿真中训练并通过速度命令（站立、行走、转向等）调节的强化学习策略。遥操作时，通过Quest控制器的摇杆在移动模式（控制行走方向）和操作模式（控制拇指）之间切换。</li>
</ol>
<p><strong>Choice Policy</strong> 的学习框架旨在高效处理演示中的多模态行为。</p>
<p><img src="https://arxiv.org/html/2512.25072v1/x2.png" alt="策略架构对比"></p>
<blockquote>
<p><strong>图2</strong>：模仿学习的策略架构对比。(a) 扩散策略能建模多模态，但需要K次迭代采样，推理慢。(b) 标准行为克隆单次前向传播推理快，但难以捕捉多模态行为。(c) 选择策略（本文方法）结合两者优点：单次前向传播生成K个候选动作，并使用学习到的评分选择最佳动作，实现了快速推理与有效处理多模态。</p>
</blockquote>
<p>该方法包含三个核心网络：特征编码器、动作提议网络和评分预测网络。给定观测<code>o_t</code>，策略输出K个候选动作序列<code>{a_t^(k)}</code>及其对应评分<code>{σ_t^(k)}</code>。最终执行评分最高的动作。具体细节如下：</p>
<ul>
<li><strong>观测编码器</strong>：融合视觉（RGB使用冻结的DINOv3编码器，深度使用随机初始化的ResNet-18）和本体感知（3层MLP）输入。</li>
<li><strong>动作提议网络</strong>：一个两层MLP，输出维度为<code>|K|×|T|×|A|</code>，重塑后得到K个长度为T的动作序列。</li>
<li><strong>评分预测网络</strong>：一个两层MLP，输出K个通道的评分，每个评分预测对应候选动作与真实动作之间的负均方误差。</li>
</ul>
<p>训练时采用赢家通吃策略与评分回归相结合的目标函数。首先计算每个提议动作与真实动作的MSE损失<code>ℓ^(k)</code>。评分网络的监督目标是使预测评分<code>σ_t^(k)</code>逼近<code>ℓ^(k)</code>，即<code>ℒ_score = Σ(σ_t^(k) - ℓ^(k))^2</code>。动作提议网络仅对误差最小的那个“赢家”动作<code>a_t^(k*)</code>（其中<code>k* = argmin ℓ^(k)</code>）进行梯度更新，损失为<code>ℒ_action = ℓ^(k*)</code>。总损失为<code>ℒ = ℒ_action + ℒ_score</code>。这鼓励提议网络产生多样化的候选动作，同时评分网络学习评估它们的质量。</p>
<p><img src="https://arxiv.org/html/2512.25072v1/x3.png" alt="训练伪代码"></p>
<blockquote>
<p><strong>图3</strong>：Choice Policy训练和推理过程的PyTorch伪代码。该片段突出了方法实现的简洁性，仅需少量代码即可结合赢家通吃的动作学习与评分回归。</p>
</blockquote>
<p>推理时，模型单次前向传播生成K个候选轨迹及其评分，选择预测评分最低（即预测质量最高）的轨迹执行。这既保持了行为克隆的快速推理速度，又具备了处理多模态的能力。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个真实世界任务上评估方法：<strong>洗碗机装载</strong>和<strong>全身移动操作擦拭（白板擦拭）</strong>。使用的机器人平台包括Fourier GR-1（44个驱动自由度）和Robotera Star-1（55个驱动自由度）。对比的基线方法包括<strong>扩散策略</strong>和<strong>采用动作分块的标准行为克隆</strong>。</p>
<p><strong>洗碗机装载任务</strong>要求机器人将薄盘从桌边滑近、抓取、在手间交接，最终插入洗碗机内的碗架。由于洗碗机初始不可见且手腕摄像头会被盘子遮挡，主动的手眼协调至关重要。</p>
<p><strong>TABLE I: Main results on the dishwasher loading task. Our Choice Policy achieves superior performance compared to behavior cloning and diffusion policy. Additionally, without hand-eye coordination, none of the methods can reliably complete the full task.</strong></p>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="center">Hand-eye Coor.</th>
<th align="center">Pickup</th>
<th align="center">Handover</th>
<th align="center">Insertion</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Diffusion Policy</td>
<td align="center"></td>
<td align="center">10 / 10</td>
<td align="center">8 / 10</td>
<td align="center">1 / 10</td>
</tr>
<tr>
<td align="left">Behavior Cloning</td>
<td align="center"></td>
<td align="center">9 / 10</td>
<td align="center">6 / 10</td>
<td align="center">1 / 10</td>
</tr>
<tr>
<td align="left">Choice Policy</td>
<td align="center"></td>
<td align="center">10 / 10</td>
<td align="center">7 / 10</td>
<td align="center">2 / 10</td>
</tr>
<tr>
<td align="left">Diffusion Policy</td>
<td align="center">✓</td>
<td align="center">10 / 10</td>
<td align="center">7 / 10</td>
<td align="center">5 / 10</td>
</tr>
<tr>
<td align="left">Behavior Cloning</td>
<td align="center">✓</td>
<td align="center">9 / 10</td>
<td align="center">7 / 10</td>
<td align="center">5 / 10</td>
</tr>
<tr>
<td align="left">Choice Policy</td>
<td align="center">✓</td>
<td align="center">10 / 10</td>
<td align="center"><strong>9 / 10</strong></td>
<td align="center"><strong>7 / 10</strong></td>
</tr>
</tbody></table>
<p>表1展示了洗碗机装载任务的主要结果。关键结论如下：</p>
<ol>
<li><strong>Choice Policy性能最优</strong>：在启用手眼协调的情况下，Choice Policy在抓取（10/10）、交接（9/10）和插入（7/10）三个子任务上均取得了最佳成功率，尤其在最具挑战性的插入阶段显著优于扩散策略（7 vs 5）和行为克隆（7 vs 5）。</li>
<li><strong>手眼协调至关重要</strong>：关闭手眼协调时，所有方法在最终插入阶段均接近失败（成功率1-2/10），凸显了头部主动视觉跟踪对于长视野任务成功的必要性。</li>
<li><strong>方法有效性</strong>：对比启用手眼协调的三种方法，Choice Policy整体表现最好，证明了其处理多模态演示和进行可靠决策的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.25072v1/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：洗碗机装载任务的定性结果。从左至右展示了任务流程：观察盘子、滑动并抓取、交接、观察洗碗机内部、最终插入。图像显示了机器人第一人称视角（右上角）和第三方视角。</p>
</blockquote>
<p><strong>全身移动操作擦拭任务</strong>要求机器人在保持稳定步态行走的同时，调整身体以擦拭白板，并适应不精确的初始和最终位置带来的误差。该任务展示了遥操作系统的灵活性，并验证了学习方法可扩展到更复杂、长视野的任务中。实验结果（论文正文未提供具体数值表格）表明，Choice Policy能够处理移动中操作的多模态性，例如在接近白板时对步态和手臂轨迹的多种调整策略。</p>
<p><strong>消融实验</strong>表明，基于学习的评分选择机制显著优于随机选择或选择第一个提议等基线策略，验证了评分网络学习的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>模块化遥操作界面</strong>：提出了一种直观、可扩展的遥操作设计，通过分解控制为手眼协调、抓取原语、手臂跟踪和移动等子模块，显著降低了高质量全身协调演示数据的收集难度。</li>
<li><strong>Choice Policy算法</strong>：提出了一种新颖的模仿学习框架，通过单次前向传播生成多个动作提议并学习评分选择，在保持快速推理的同时，有效捕捉了演示数据中的多模态行为，性能优于扩散策略和行为克隆。</li>
<li><strong>系统验证</strong>：在真实人形机器人上完成了洗碗机装载和移动擦拭两个复杂任务，实证了手眼协调对长视野任务的关键作用，以及所提系统在实现全身协调操作方面的可行性。</li>
</ol>
<p>论文提到的局限性包括：当前方法主要针对相对静态的环境，在高度动态场景中的性能尚未验证；抓取原语虽然覆盖范围广，但对于需要非常精细的独立手指控制的任务（如灵巧操作）可能仍需扩展。</p>
<p>本研究为在非结构化环境中实现可扩展的人形机器人协调操作指明了一条实用路径。其启示在于：通过精心设计的模块化接口降低数据收集门槛，并结合能够高效处理多模态性的学习算法，是推动复杂人形机器人技能学习的关键。未来的工作可以探索将更多技能（如手指步态）集成到模块化界面中，并将方法应用于更动态、需实时反应的任务场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在非结构化环境中实现头、手、腿全身协调操作的挑战，提出了一种结合模块化遥操作界面和Choice Policy学习框架的系统。遥操作界面将控制分解为手眼协调、抓取原语等子模块，以高效收集高质量演示；Choice Policy通过生成并评分候选动作，实现快速推理和多模态行为建模。实验在洗碗机装载和白板擦拭任务中表明，该方法性能显著优于扩散策略和标准行为克隆，并验证了手眼协调对长期任务成功的关键作用。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.25072" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>