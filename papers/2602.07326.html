<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.07326" target="_blank" rel="noreferrer">2602.07326</a></span>
        <span>作者: Seokhwan Jeong Team</span>
        <span>日期: 2026-02-07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前多指灵巧操作的主流方法依赖于多种传感器模态的融合，如视觉、触觉、力和本体感知。视觉感知成本较低，但易受遮挡和光照变化影响，导致物体状态估计不可靠。触觉传感器能提供丰富的局部接触信息，但通常结构复杂、成本高昂、难以微型化，且对光照和反射敏感。多轴力传感器价格昂贵，而低成本FSR传感器则存在非线性、迟滞和重复性差等问题，限制了其精确力估计的能力。这些局限性阻碍了机器人系统在真实工业场景中的大规模部署。</p>
<p>本文针对上述问题，提出了一个极简感知范式：完全摒弃视觉和复杂的多轴/触觉传感，仅依靠单轴指尖力反馈和关节本体感知来实现可靠的多指抓取。其核心思路是：通过一个高效的师生训练框架，让在仿真中利用特权观测进行强化学习的教师策略生成演示，进而蒸馏出一个基于Transformer的学生策略，该策略仅使用真实部署时可用的部分观测（单轴力和关节状态）来完成“盲抓取”任务。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架是一个两阶段的师生训练流程。首先，在仿真环境中，一个教师策略利用特权观测通过强化学习学习抓取技能。然后，从教师策略的成功抓取轨迹中筛选出包含丰富单轴力交互的演示数据，用于通过模仿学习蒸馏出一个学生策略。该学生策略最终仅使用真实世界可用的传感模态——即关节位置和单轴指尖力——进行决策。</p>
<p><img src="https://arxiv.org/html/2602.07326v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提出的用于盲抓取的师生训练流程概览。左侧为在仿真中利用特权观测进行强化学习的教师策略，右侧为仅使用部分观测（关节位置和单轴力）进行决策的学生策略。</p>
</blockquote>
<p>教师策略在IsaacLab仿真环境中训练，其输入为特权观测向量 $o_{t}^{\text{priv}} \in \mathbb{R}^{95}$，包含关节位置/速度、指尖位姿、物体位姿、物体线/角速度、三轴指尖接触力、物体与夹爪中心的平面距离、单轴指尖力、6维指尖力/力矩以及上一时刻动作。输出为9维的相对关节位置变化 $a_t = \Delta\theta_t \in \mathbb{R}^9$，控制频率为20 Hz。策略网络是一个多层感知机（MLP）。</p>
<p><img src="https://arxiv.org/html/2602.07326v1/x3.png" alt="教师策略"></p>
<blockquote>
<p><strong>图3</strong>：在仿真中利用特权观测训练的教师策略流程。MLP接收特权观测并输出相对关节位置变化。每个手指包含三个驱动关节和一个单轴指尖力传感器。</p>
</blockquote>
<p>奖励函数精心设计，以鼓励从初始非接触状态建立接触、形成稳定抓握并平滑提升物体。它包含任务奖励 $r_t$、激励奖励 $r_i$（鼓励基于单轴力反馈的三指尖同时接触）以及惩罚项（关节限位 $r_l$、大动作 $r_a$、动作速率变化快 $r_{ar}$）。具体形式为 $r = w_1 r_t + w_2 r_i + w_{31} r_l + w_{32} r_a + w_{33} r_{ar}$，其中权重 $w_1=1.0, w_2=0.2, w_{31}=-500, w_{32}=-0.04, w_{33}=-0.01$。</p>
<p>学生策略是一个基于Transformer的编码器-解码器架构，其输入是部分观测的历史序列 $o_t^{\text{partial}}$，仅包含关节位置和单轴指尖力。编码器处理观测序列，解码器输出动作序列。训练采用模仿学习，数据来源于教师策略在仿真中生成的成功抓取轨迹。关键创新在于，在数据筛选阶段，特意选择了那些单轴力信号变化丰富的轨迹，以强制学生策略学习依赖力反馈进行决策。</p>
<p>与现有方法相比，本文的创新点体现在：1) 提出了一个极简的传感配置（仅单轴力+本体感知），显著降低了硬件成本和系统复杂性；2) 设计了一个针对盲抓取任务优化的师生框架，通过精心设计的奖励函数和数据筛选机制，确保学生策略能有效利用有限的力传感信息；3) 移除了对光真实感渲染和复杂触觉仿真的需求，简化了仿真流程。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实硬件平台上进行，该平台由一个基于开源D‘Claw改造的三指夹爪（共9自由度）和定制的单轴指尖力传感器组成。夹爪固定在一个静止框架上。</p>
<p>使用了18个物体进行评估，包括6个训练分布内物体（仿真中出现过的几何形状）和12个训练分布外物体（具有新形状、材料或尺寸的日常物品）。具体物体集合见图2和论文后续部分。</p>
<p><img src="https://arxiv.org/html/2602.07326v1/x2.png" alt="训练物体"></p>
<blockquote>
<p><strong>图2</strong>：仿真中用于训练教师策略的18个几何物体集合。包含六种几何形状（两种长方体A/B、一种胶囊、两种圆柱体A/B、一种球体），每种形状有三种尺寸。</p>
</blockquote>
<p>对比的基线方法包括：1) <strong>无力的盲抓取</strong>：学生策略的变体，输入仅有关节位置，没有力反馈；2) <strong>有视觉的抓取</strong>：一个利用RGB-D相机观测的端到端视觉运动策略，作为性能上限参考。</p>
<p>关键实验结果：提出的仅使用单轴力和本体感知的学生策略，在全部18个物体上取得了 <strong>98.3%</strong> 的整体抓取成功率。其中，在6个分布内物体上成功率为100%，在12个分布外物体上成功率为97.2%。相比之下，“无力的盲抓取”基线成功率仅为27.8%，而“有视觉的抓取”基线成功率为100%。</p>
<p><img src="https://arxiv.org/html/2602.07326v1/x4.png" alt="结果对比"></p>
<blockquote>
<p><strong>图4</strong>：真实世界抓取成功率对比。提出的方法（单轴力+本体）在分布内（ID）和分布外（OOD）物体上均接近100%成功率，显著优于无力的盲抓取基线。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07326v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融实验结果。(a) 不同数据筛选阈值（基于力交互丰富度）对学生策略性能的影响，表明筛选高质量力交互数据至关重要。(b) 不同观测历史长度对性能的影响，显示一定长度的历史信息（约0.5秒）能提升性能。</p>
</blockquote>
<p>消融实验总结了各组件贡献：1) <strong>力传感</strong>：是成功的关键，移除后成功率暴跌至27.8%。2) <strong>数据筛选</strong>：基于力交互丰富度筛选演示数据能显著提升学生策略的最终性能。3) <strong>历史信息</strong>：使用一定时间窗口的观测历史（Transformer架构）有助于捕捉动态，提升抓取鲁棒性。</p>
<p><img src="https://arxiv.org/html/2602.07326v1/x6.png" alt="定性结果"></p>
<blockquote>
<p><strong>图6</strong>：真实世界抓取序列示例。展示了策略从初始状态、建立接触到成功提升物体的过程，包括对应的单轴力读数变化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.07326v1/x7.png" alt="泛化测试"></p>
<blockquote>
<p><strong>图7</strong>：对分布外物体的抓取成功示例。包括软包、玩偶、马克杯等形状、材质各异的物体，证明了方法的强泛化能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 实证证明了仅使用极简的传感模态（单轴指尖力+关节位置）即可实现高成功率、高泛化性的多指盲抓取，大幅降低了传感和集成需求。2) 提出了一种针对盲抓取任务定制的师生训练框架，通过特权教师、精心设计的奖励函数和基于力交互的数据筛选，高效地蒸馏出仅依赖部分观测的学生策略。3) 在真实硬件上进行了广泛验证，取得了98.3%的整体抓取成功率，展示了强大的sim-to-real迁移和分布外泛化能力。</p>
<p>论文提到的局限性包括：当前工作集中于静态抓取和提升，未涉及复杂的操作任务；夹爪是固定基座的，未与移动底盘或手臂集成；策略是在物体初始位置已知且大致位于夹爪中心的假设下训练的。</p>
<p>这项研究对后续工作的启示在于：它展示了极简感知在特定操作任务上的巨大潜力，鼓励社区重新思考传感的必要性。未来方向可能包括：将此类方法扩展到更动态的操作任务（如重定向、插入）、与移动底座结合、探索更高效的数据收集与蒸馏方法，以及研究如何将类似的极简感知范式应用于其他接触丰富的机器人任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人抓取中视觉和高分辨率触觉传感器成本高、易损坏的问题，提出一种仅使用单轴指尖力反馈和关节本体感知的“无视觉盲抓取”方法。核心技术是采用师生训练框架：教师策略利用仿真特权信息生成演示，学生策略基于Transformer架构，仅使用真实部署时的有限传感模态进行决策蒸馏。在包含18个物体的真实硬件实验中，该方法实现了98.3%的整体抓取成功率，展现出较强的鲁棒性和泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.07326" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>