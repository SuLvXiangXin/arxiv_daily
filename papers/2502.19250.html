<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2502.19250" target="_blank" rel="noreferrer">2502.19250</a></span>
        <span>作者: Zhu, Minjie, Zhu, Yichen, Li, Jinming, Zhou, Zhongyi, Wen, Junjie, Liu, Xiaoyu, Shen, Chaomin, Peng, Yaxin, Feng, Feifei</span>
        <span>日期: 2025/02/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，模仿学习是教授机器人灵巧操作技能的有效方法，但其严重依赖大量人类演示数据，限制了在动态现实环境中的可扩展性和实用性。一个核心挑战是物体泛化问题：例如，机器人学会了“递苹果”，却难以将技能迁移到语义相似但视觉不同的“递桃子”上。在端到端视觉运动策略学习中，如何泛化到训练数据类别之外的新物体（即分布外物体）尚未得到充分解决。</p>
<p>本文针对上述痛点，提出了一种通过视觉-语言-动作模型实现物体泛化的新视角。其核心思路是：通过联合训练机器人交互数据和带有定位元数据的图像-文本数据，在视觉语言语义与机器人动作之间建立隐式链接，使模型无需针对每个新物体的演示，即可实现零样本的物体泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>ObjectVLA的整体框架基于扩散型VLA模型（具体采用DiVLA架构），其核心是使用一个混合数据集进行协同训练：一部分是机器人交互数据（专家演示轨迹），另一部分是专门构建的、带有物体定位（边界框）标注的图像-文本数据。目标是学习一个策略 $\pi$，将视觉观测 $o_r$ 和语言指令 $i_r$ 映射为动作 $a$。</p>
<p><img src="https://arxiv.org/html/2502.19250v2/x1.png" alt="机器人设置"></p>
<blockquote>
<p><strong>图1</strong>：真实机器人实验设置及任务示例。使用配备两个外部ZED相机和一个Realsense 435i腕部相机的Franka机械臂，在四种技能上评估ObjectVLA。</p>
</blockquote>
<p>方法包含两个核心的数据构建模块：</p>
<ol>
<li><strong>图像-文本数据构建</strong>：收集100个机器人交互数据中未出现的新物体，使用机器人上的三个相机为每个物体拍摄20张不同姿态的图片。文本部分采用固定模板“检测物体的边界框。”作为问题，对应的边界框坐标作为答案，共构建2000个图像-文本对。</li>
<li><strong>机器人数据的推理构建</strong>：为了在机器人数据中建立图像-文本与动作的隐式链接，为每个任务中的目标物体标注边界框（使用DinoX检测器并人工校正）。然后，按照Qwen2-VL的格式，将物体名称和其边界框信息构建成一段定位推理文本（例如“<code>&lt;||object_ref_start||&gt;{object}&lt;||object_ref_end||&gt;&lt;||box_start||&gt;(x1,y1),(x2,y2)&lt;||box_end||&gt;.</code>”）。这段推理在每次动作前生成，并通过一个可学习模块注入到策略模型中。</li>
</ol>
<p>训练策略的关键是<strong>协同训练</strong>。将上述两种数据混合，并保持机器人数据与图像-文本数据的比例约为10:1。这种平衡被证明对于在保持域内任务性能的同时，实现鲁棒的物体泛化至关重要。作者指出，对于参数量较小的模型（如2B的DiVLA），过度增加机器人数据的比例会导致模型过拟合，反而降低泛化能力。</p>
<p>与现有方法相比，ObjectVLA的创新点具体体现在：</p>
<ol>
<li><strong>利用图像-文本数据作为物体知识注入的轻量级、可扩展途径</strong>，而非仅依赖昂贵的机器人演示数据。</li>
<li><strong>通过定位感知推理（边界框）作为桥梁</strong>，显式地将视觉物体、其语言描述与需要执行的动作联系起来，这为模型理解新物体提供了结构化线索。</li>
<li><strong>设计了与图像-文本数据格式一致的机器人数据推理过程</strong>，使得预训练视觉语言模型中的丰富知识能够被有效利用。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实的Franka机器人平台上进行，使用了两个外部ZED相机和一个腕部Realsense相机。评估了多个基准任务，并与基线方法进行了对比。</p>
<p><strong>1. 物体泛化能力验证</strong></p>
<ul>
<li><strong>任务</strong>：“移动到物体”。物体放置在桌子左右两侧，模型需根据指令移向目标物体。</li>
<li><strong>评估标准</strong>：每个物体测试4次（目标侧切换），仅当4次全部成功才计为识别成功。</li>
<li><strong>对比方法</strong>：完整的ObjectVLA、移除边界框推理的消融版本、仅用机器人数据训练的DiVLA。</li>
<li><strong>关键结果</strong>：<ul>
<li><strong>In-Distribution (ID)<strong>：ObjectVLA对训练集内的物体达到</strong>100%</strong> 成功率。</li>
<li><strong>Out-of-Distribution (OOD)<strong>：对100个机器人数据中未见过的新物体，ObjectVLA取得了</strong>64%</strong> 的成功率。</li>
<li><strong>消融实验</strong>：移除边界框后，OOD成功率骤降至**19%<strong>。仅用机器人数据训练的DiVLA，OOD成功率仅为</strong>8%**（接近随机猜测），表明没有图像-文本协同训练会导致灾难性遗忘。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2502.19250v2/x4.png" alt="泛化验证实验"></p>
<blockquote>
<p><strong>图4</strong>：物体泛化验证实验结果。ObjectVLA在分布内测试和视觉变化（分布外）下均取得最佳性能。展示了各方法成功识别的物体数量（满足4次全成功的标准）。</p>
</blockquote>
<p><strong>2. 结合更多技能</strong></p>
<ul>
<li><strong>任务</strong>：“逆时针旋转物体”和“向前推物体”。</li>
<li><strong>结果</strong>：如表1所示，对于5个ID物体，旋转和推动技能分别取得13/15和12/15的成功次数。对于20个OOD物体，分别取得39/60和52/60的成功次数。失败案例多因技能执行不完全（如抓握不稳），而非错误识别物体。</li>
</ul>
<p><strong>3. 指令驱动的分拣</strong></p>
<ul>
<li><strong>任务</strong>：从装有多个物体的箱子中，根据指令（如“从箱子里捡起六角螺栓”）拣选特定目标物体。这是比普通分拣更具挑战性的跨模态理解和细粒度识别任务。</li>
<li><strong>对比基线</strong>：OpenVLA（一个先进的VLA模型）。</li>
<li><strong>结果</strong>：如表2所示，对于ID物体，ObjectVLA成功率为63.6%（21/33），显著高于OpenVLA的42.4%（14/33）。对于OOD物体，ObjectVLA成功率为58%（87/150），比OpenVLA的11.3%（17/150）提升了46.7个百分点。</li>
</ul>
<p><strong>4. 通过智能手机图片与持续学习的廉价泛化</strong></p>
<ul>
<li><strong>设置</strong>：使用智能手机拍摄两个全新物体（皮卡丘和棕色玩具猫）的各21张图片，构建图像-文本数据，然后在预训练模型上进行仅1个epoch的持续学习。</li>
<li><strong>任务</strong>：分拣环境下的抓取。</li>
<li><strong>结果</strong>：如图5所示，模型能成功识别并抓取物体，皮卡丘和玩具猫的成功率分别为<strong>80%</strong> 和**90%**。训练过程仅需约十分钟，展示了快速、低成本的适配能力。</li>
</ul>
<p><img src="https://arxiv.org/html/2502.19250v2/x5.png" alt="智能手机图片持续学习结果"></p>
<blockquote>
<p><strong>图5</strong>：使用智能手机拍摄物体并进行持续学习的实验结果。测试了两个新物体，各进行10次试验，报告了每个物体的成功率。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li><strong>提出了一种统一的、实现端到端物体泛化的管道</strong>：通过协同训练机器人交互数据和带有定位标注的图像-文本数据，在视觉语言语义与机器人动作之间建立了有效桥梁。</li>
<li><strong>实证验证了方法的广泛有效性和实用性</strong>：在移动、旋转、推动及复杂指令分拣等多种任务上实现了对大量分布外物体的零样本泛化，并展示了通过极少量智能手机图片和快速微调即可适配新物体的低成本部署路径。</li>
<li><strong>揭示了实现泛化的关键机制</strong>：通过消融实验明确了<strong>定位感知推理（边界框）</strong> 和<strong>与图像-文本数据格式一致的机器人数据构造</strong>是解锁VLA模型物体泛化能力的关键因素。</li>
</ol>
<p>论文提到的局限性包括：模型参数量（2B）相对较小，限制了其吸收更多领域特定数据而不发生过拟合的能力；在旋转等需要精密抓握的技能上，对陌生物体的执行成功率仍有下降。</p>
<p>这项工作对后续研究的启示在于：为减少机器人学习对海量演示数据的依赖提供了新思路，即充分利用互联网规模的视觉语言先验知识；定位信息作为一种“接地气”的中间表示，在连接感知与行动中显示出重要作用；持续学习与便捷数据采集（如手机拍照）的结合，为开放世界机器人系统的快速适应和实际部署指明了可行方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人模仿学习依赖大量人类演示、难以泛化到新物体的问题，提出ObjectVLA方法。该方法基于视觉-语言-动作模型，利用视觉-语言配对数据建立物体与动作的隐式链接，实现零样本物体泛化，无需针对新物体进行演示。在真实机器人实验中，该方法成功泛化到100个训练未见的新物体，选择正确物体的成功率达到64%。此外，研究提出可通过手机拍摄少量图片对预训练模型进行微调，有效减少对人类演示数据的依赖。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2502.19250" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>