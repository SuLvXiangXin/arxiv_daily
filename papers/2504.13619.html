<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.13619" target="_blank" rel="noreferrer">2504.13619</a></span>
        <span>作者: Singh, Rohan P., Morisawa, Mitsuharu, Benallegue, Mehdi, Xie, Zhaoming, Kanehiro, Fumio</span>
        <span>日期: 2025/04/18</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人在真实世界部署的关键挑战在于开发能应对复杂地形的鲁棒运动控制器，这些地形可能具有不可预知的柔顺性和不规则性。目前，基于模型的方法（如线性倒立摆模型及其变体）对脚部轨迹和环境接触有严格的时空假设，在面对不规则或柔顺（即可变形）表面时，这些假设可能因脚部过早或延迟触地而被违反，导致控制失败。同时，尽管深度强化学习（RL）在四足机器人上取得了成功，但在HRP-5P这类全尺寸、大质量的人形机器人上，针对柔顺和不平地形的学习型方法应用仍然有限。本文针对这一痛点，探索了仿真到现实（sim-to-real）深度强化学习在设计双足运动控制器上的应用。其核心思路是：通过一个简单的训练课程，在仿真中让RL智能体接触随机化的地形，仅使用本体感知反馈即可实现策略在真实人形机器人上的鲁棒行走，并进一步提出通过调节观测时钟信号来实现非周期性步态，以提升在挑战性地形上的适应性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架是一个两阶段的训练流程，旨在获得一个能零样本（zero-shot）迁移到真实机器人HRP-5P的端到端运动策略。</p>
<p><img src="https://arxiv.org/html/2504.13619v1/extracted/6372124/images/framework.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：训练框架总览。（左）提出分两阶段训练前馈RL智能体：第一阶段暴露于随机化的动力学参数；第二阶段额外暴露于随机化的不平和柔顺地形。该策略在真实HRP-5P上实现了零样本仿真到现实迁移。（右）提出一种增强策略，可进行时钟信号调制以调节步频，从而在挑战性地形上实现更高的鲁棒性。</p>
</blockquote>
<p><strong>策略输入与输出</strong>：策略的观察空间包括机器人状态和外部状态。机器人状态包含编码器、机载IMU和电机电流传感器的本体感知测量值（关节位置、速度、估算的关节级扭矩、根部朝向和角速度）。外部状态包括一个3D独热编码表示行走模式（站立、原地踏步、前进行走）、一个1D标量表示模式参考值（转向或前进速度），以及一个由循环相位变量ϕ导出的2D时钟信号（正弦和余弦值），周期L设为2秒（80个控制步）。动作空间是12维，对应机器人腿部驱动关节（每条腿6个）的期望位置增量。网络预测值会与对应标称姿态的固定电机偏移相加，然后通过一个低增益PD控制器（运行频率1000 Hz）进行跟踪以产生扭矩。上层关节通过高刚度PD控制冻结在标称配置。</p>
<p><strong>模拟挑战性地形</strong>：</p>
<ol>
<li><strong>模拟柔顺性</strong>：利用MuJoCo的软接触模型，通过调整脚部与地面接触约束的“时间常数”参数（<code>solref</code>）来模拟脚部下陷效果，该参数在(0.02, 0.4)范围内随机化，以模拟从完全刚性到类似弹簧的行为。</li>
<li><strong>模拟不平整度</strong>：使用MuJoCo的高度场功能。预先生成一个10m×10m、网格尺寸4cm的高度场。在训练期间，随机化该高度场相对于平坦地面的z位置（范围在-4cm到0cm），从而在每个随机化时产生具有不同高度障碍物的新地形。为避免高度场在随机化时在支撑脚下产生不现实的向上推力，在双足支撑阶段禁用地形不平整度随机化。</li>
</ol>
<p><strong>训练课程与动力学随机化</strong>：</p>
<ul>
<li><strong>第一阶段（基础策略）</strong>：在平坦刚性地面上训练一个能执行所有行走模式（站立、原地踏步、前进行走）的基础策略。行走模式每5秒随机切换一次，模式参考值也相应采样。</li>
<li><strong>第二阶段（微调）</strong>：从预训练的网络权重开始，在新的训练过程中对基础策略进行微调。在此阶段，除了继续随机化行走模式，还以平均0.5秒的间隔随机化每只脚的柔顺性参数，并以平均5秒的间隔随机化不平地形的3D位置（双足支撑期禁用）。</li>
<li><strong>动力学随机化</strong>：为了弥补仿真与现实的差距，在训练期间以平均0.5秒的间隔随机化一组机器人动力学参数，包括关节阻尼系数、关节静摩擦、连杆质量以及连杆质心位置。这有助于防止策略对特定动力学模型过拟合。</li>
</ul>
<p><strong>非周期性步态策略</strong>：论文认为，基于固定周期时钟信号的奖励函数将机器人限制在僵硬的步态模式中，阻碍了应对需要大幅改变摆动和支撑相持续时间的地形挑战的能力。因此，提出一种增强策略，其动作空间扩展至13维，新增一个标量输出用于预测相位变量的偏移量。相位变量的更新公式为：ϕ_{t+1} = ϕ_t + clip(a_δϕ, -5, 5) + 1。这样，策略可以通过预测的相位偏移主动调制时钟信号，从而根据地形和指令速度产生自适应的、非周期性的步态模式，放松对接触定时的约束。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在MuJoCo仿真环境中训练策略，使用PPO算法，策略网络和价值网络均为2层256维隐藏层的MLP。最终策略部署在真实的HRP-5P人形机器人上进行测试。</p>
<p><strong>测试地形与基线</strong>：室内测试地形包括刚性不规则积木、软体操垫和泡沫垫块。户外测试在铺砌街道和不规则草坪上进行。通过消融实验对比了四个策略：(a) <strong>Baseline</strong>：仅在平坦地面训练；(b) <strong>Uneven terrain</strong>：在刚性不平地形上微调；(c) <strong>Fixed compliance</strong>：在具有固定柔顺性的不平地形上微调；(d) <strong>Terrain-randomized</strong>：在具有随机化柔顺性的不平地形上微调（本文最终方法）。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>真实机器人演示</strong>：使用<strong>Terrain-randomized</strong>策略，HRP-5P成功在多种地形上行走，包括室内刚性积木、软垫、倾斜块以及户外的铺砌街道和草坪。</li>
<li><strong>成功率与行走距离</strong>：在室内9次测试中，策略成功完成了6次，成功率为67%。失败主要发生在双脚站在超过训练范围的斜坡上或跨越高于4cm的障碍物时。在户外，机器人成功在铺砌街道上行走了约25米，在草坪上行走了约30米。</li>
<li><strong>消融实验</strong>：Baseline策略在仿真中面对2cm不平度时平均仅能维持1.35秒即失败。Uneven terrain策略能在仿真和真实机器人上成功走过3.5cm的小障碍，但在柔顺地形上会失败。Fixed compliance策略在特定柔顺性地形上表现良好，但泛化能力差。只有<strong>Terrain-randomized</strong>策略能同时应对不平和柔顺地形。</li>
<li><strong>非周期性步态评估</strong>：在仿真中，对比了固定时钟策略与时钟调制策略在具有4cm高障碍的不平地形上的性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.13619v1/extracted/6372124/images/clock_control_reward.png" alt="时钟控制奖励"></p>
<blockquote>
<p><strong>图3</strong>：在具有4厘米高障碍的不平地形上，比较固定时钟策略与时钟调制策略的 episode 累积奖励。时钟调制策略（蓝色）获得更高且更稳定的奖励。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.13619v1/extracted/6372124/images/clock_control_grf.png" alt="时钟控制地面反力"></p>
<blockquote>
<p><strong>图4</strong>：跨越单个4厘米高障碍时，双脚地面反力（GRF）的对比。时钟调制策略（下图）的GRF曲线更平滑，冲击更小，显示了更好的适应性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.13619v1/extracted/6372124/images/inplace_vs_fwd.png" alt="原地与前进对比"></p>
<blockquote>
<p><strong>图5</strong>：在4厘米障碍地形上，对比原地踏步（左）与前进行走（右）模式下，时钟调制策略（蓝色）与固定时钟策略（橙色）的成功率。时钟调制策略在两种模式下均显著提升了成功率，尤其在前进行走模式下。</p>
</blockquote>
<p>仿真实验表明，时钟调制策略在 episode 累积奖励、地面反力平滑度以及跨越4cm障碍的成功率上均优于固定时钟策略。特别是在前进行走模式下，时钟调制将成功率从40%提高到了100%。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出并验证了一个简单的两阶段训练课程（动力学随机化 + 地形属性随机化），能够训练出仅依赖本体感知即可在真实全尺寸人形机器人上实现多种柔顺/不平地形零样本鲁棒行走的强化学习策略。</li>
<li>提出了一种能够调制时钟信号的增强策略，使步态频率可自适应变化，并通过仿真实验证明了非周期性步态在提升跨越挑战性障碍成功率方面的有效性。</li>
<li>在HRP-5P机器人上进行了广泛的室内外真实地形演示，验证了方法的实用性。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>策略在双脚长时间站立于训练范围外的斜坡上时容易失败，因为状态变化缓慢，策略难以察觉和补偿。</li>
<li>难以跨越高于训练时设定的最大高度（4厘米）的障碍。</li>
<li>训练中为节省计算时间而采用的高度场随机化方法，可能会在随机化时产生不现实的地面推力。</li>
</ul>
<p><strong>启示</strong>：</p>
<ul>
<li>对于复杂地形上的盲行走（无外部感知），在仿真中系统地随机化地形物理属性（柔顺性、不平度）是一种有效且简单的鲁棒性训练方法。</li>
<li>放松对步态周期性的硬性约束，允许控制器自适应调整步频和接触时序，可能是进一步提升人形机器人动态环境适应能力的关键方向。</li>
<li>这项工作为将数据驱动方法应用于大惯性、低关节速度的全尺寸人形机器人提供了有价值的实践经验和参考。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在柔顺、不平坦地形上的稳健行走控制问题，提出了一种基于深度强化学习（DRL）的解决方案。核心方法是采用sim-to-real训练范式，在仿真中使用随机化地形的简单课程训练策略，并引入可调节步频的非周期性运动控制策略，使机器人仅依靠本体感知反馈即可适应地形变化。实验表明，该方法在HRP-5P人形机器人上实现了单一策略在多种真实复杂地形（如软垫、倾斜块、铺路石、草地）上的稳健行走，无需针对不同地形调整参数。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.13619" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>