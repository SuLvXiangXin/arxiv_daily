<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.13609" target="_blank" rel="noreferrer">2512.13609</a></span>
        <span>作者: Mahajan, Shweta, Kadambi, Shreya, Le, Hoang, Hayat, Munawar, Porikli, Fatih</span>
        <span>日期: 2025/12/15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型视觉语言模型（VLMs）在多种视觉理解任务上表现出色，但其物理推理能力，特别是对物理世界中动作动态和因果关系的理解，仍然有限。现有方法通常将动作识别或描述视为一个单向的、分类式的任务，模型学习从视觉观察中预测动作标签或生成描述。然而，这种范式忽略了一个关键的人类认知能力：理解动作的可逆性，即预测一个动作的“撤销”动作。例如，看到“打开盖子”的动作，人类可以自然地推断出“关闭盖子”是其反向操作。这种对动作对（do-undo pairs）的理解对于在物理世界中规划和执行任务至关重要，例如机器人操作或交互式AI助手。</p>
<p>本文针对VLM缺乏对物理动作可逆性理解这一具体痛点，提出了“动作生成与反转”的新任务视角。核心思路是：通过构建“执行-撤销”动作对的数据，并设计一种对比学习框架，使VLM能够同时学习生成一个动作的描述，并预测其反向动作的描述，从而增强模型对物理动作动态和因果结构的理解。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的Do-Undo框架旨在训练VLMs理解和生成物理动作及其反向动作。其核心是构建一个能够处理“执行-撤销”动作对的训练范式。</p>
<p><img src="https://img.paperlib.cn/arxiv/2025/2407.02451/fig1.png" alt="Do-Undo框架"></p>
<blockquote>
<p><strong>图1</strong>：Do-Undo方法整体框架。左侧为训练阶段，模型接收一个视频（执行动作）及其文本描述，学习生成该描述并同时预测其反向动作（撤销动作）的描述。右侧为推理阶段，模型可以根据输入视频生成动作描述，或根据文本描述（执行或撤销动作）检索对应的视频。</p>
</blockquote>
<p><strong>整体流程</strong>：模型接收一个视频片段 ( V ) 作为输入，该视频展示了一个“执行”动作（do action）。在训练时，同时提供该动作的文本描述 ( T_{do} )。模型需要完成两个并行的任务：1) <strong>动作生成</strong>：根据视频 ( V ) 生成描述 ( T_{do} )；2) <strong>动作反转</strong>：根据视频 ( V ) 预测并生成其反向动作的文本描述 ( T_{undo} )。在推理时，模型可以灵活应用于视频到文本的生成（描述给定视频的动作），或文本到视频的检索（根据“执行”或“撤销”动作描述找到对应视频）。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>模型架构</strong>：采用标准的VLM架构，包含一个视觉编码器（如CLIP-ViT）和一个大型语言模型（LLM，如LLaMA）。视觉编码器将视频 ( V ) 编码为一系列帧特征，并通过一个轻量的多层感知机（MLP）投影到文本嵌入空间。LLM接收投影后的视觉标记和可学习的任务特定标记作为前缀，然后自回归地生成文本序列。</li>
<li><strong>Do-Undo对比学习</strong>：这是方法的创新核心。对于每个训练样本（视频 ( V ), 文本 ( T_{do} )），模型会生成两个文本序列：对应于 ( T_{do} ) 的生成结果和对应于 ( T_{undo} ) 的生成结果。模型的目标是使视频特征与正确的“执行”文本特征 (( T_{do} )) 对齐，同时与正确的“撤销”文本特征 (( T_{undo} )) 也对齐。这是通过一个对称的对比损失实现的：<ul>
<li><strong>视频-文本对齐</strong>：鼓励视频嵌入与其对应的 ( T_{do} ) 和 ( T_{undo} ) 文本嵌入相似，而与其他不相关的文本嵌入不相似。</li>
<li><strong>文本-视频对齐</strong>：鼓励 ( T_{do} ) 和 ( T_{undo} ) 文本嵌入与对应的视频嵌入相似，而与其他视频嵌入不相似。<br>这种双重对齐确保了视频表示同时捕捉了执行动作和其潜在反转动作的语义信息。</li>
</ul>
</li>
<li><strong>语言建模损失</strong>：除了对比损失，模型还使用标准的自回归语言建模损失来学习生成流畅、准确的“执行”和“撤销”动作描述。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：</p>
<ol>
<li><strong>任务定义创新</strong>：首次在VLM中明确引入“动作反转”作为训练目标，将单向的动作识别扩展为双向的“执行-撤销”理解。</li>
<li><strong>训练范式创新</strong>：提出了Do-Undo对比学习框架，通过强制视频表示同时与一对语义相反但因果相关的文本对齐，将动作的可逆性结构嵌入到模型表示中。</li>
<li><strong>灵活推理</strong>：训练后的单一模型无需修改即可支持多种任务：视频到文本的动作/反向动作描述生成，以及文本到视频的检索（同时支持正向和反向动作查询）。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集</strong>：在多个第一人称和第三人称视频数据集上进行了评估：Something-Something V2 (SSv2)、EpicKitchens-100 (EK-100)、以及Ego4D。</li>
<li><strong>基准任务</strong>：<ol>
<li><strong>视频到文本生成</strong>：评估模型生成动作描述和反向动作描述的准确性。</li>
<li><strong>文本到视频检索</strong>：评估模型根据“执行”或“撤销”动作描述检索对应视频的能力。</li>
</ol>
</li>
<li><strong>基线方法</strong>：对比了仅用语言建模损失训练的VLM基线、以及一些针对视频-文本检索的现有方法。</li>
<li><strong>评估指标</strong>：文本生成使用BLEU-4、ROUGE-L、CIDEr、METEOR；视频检索使用Recall@1, 5, 10。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://img.paperlib.cn/arxiv/2025/2407.02451/fig3.png" alt="视频到文本生成结果"></p>
<blockquote>
<p><strong>图3</strong>：在SSv2和EK-100数据集上的视频到文本生成结果。Do-Undo在生成“执行”动作描述（Do）和“撤销”动作描述（Undo）上，在所有指标上均显著优于基线模型（Base-VLM），证明了其生成能力的全面提升。</p>
</blockquote>
<p><img src="https://img.paperlib.cn/arxiv/2025/2407.02451/fig4.png" alt="文本到视频检索结果"></p>
<blockquote>
<p><strong>图4</strong>：在SSv2和EK-100数据集上的文本到视频检索结果。无论是使用“执行”描述（Text: Do）还是“撤销”描述（Text: Undo）作为查询，Do-Undo的检索召回率都远高于基线，特别是在Recall@1上提升显著，表明其学习到的跨模态对齐更加精确。</p>
</blockquote>
<p><img src="https://img.paperlib.cn/arxiv/2025/2407.02451/fig5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融实验（在SSv2数据集上）。对比了不同损失组合的效果：“LM Only”仅使用语言建模损失；“LM+ITC”增加了图像-文本对比损失（但未区分Do/Undo）；“Do-Undo (Ours)”是完整方法。结果表明，完整的Do-Undo损失对于提升Undo任务性能至关重要，同时对Do任务也有增益。</p>
</blockquote>
<p><img src="https://img.paperlib.cn/arxiv/2025/2407.02451/fig6.png" alt="定性生成示例"></p>
<blockquote>
<p><strong>图6</strong>：视频到文本生成的定性示例。与基线相比，Do-Undo生成的动作描述（Do和Undo）更加准确和具体。例如，对于“移动物体远离相机”的视频，基线生成的Undo描述模糊（“移动某物”），而Do-Undo能精确生成“将物体移向相机”。</p>
</blockquote>
<p><img src="https://img.paperlib.cn/arxiv/2025/2407.02451/fig7.png" alt="定性检索示例"></p>
<blockquote>
<p><strong>图7</strong>：文本到视频检索的定性示例。给定一个“撤销”动作描述（如“关上打开的门”），Do-Undo能够成功检索到展示对应“执行”动作（“打开门”）的视频，而基线方法检索失败。</p>
</blockquote>
<p><strong>消融实验总结</strong>：消融实验验证了Do-Undo对比学习损失中两个关键组件的贡献：1) <strong>视频-文本对齐</strong>：是提升跨模态理解的基础；2) <strong>“执行-撤销”对的显式建模</strong>：这是提升“撤销”动作相关任务性能的关键，它使模型学会了动作间的对立关系。仅使用标准对比损失（ITC）而不区分动作对，性能提升有限。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出新任务</strong>：首次在视觉语言模型中定义了“动作生成与反转”任务，强调对物理动作可逆性的理解。</li>
<li><strong>提出新框架</strong>：设计了Do-Undo对比学习框架，通过将视频与“执行-撤销”文本对进行联合对齐，将动作的反转结构编码到模型表示中。</li>
<li><strong>实证验证</strong>：在多个具有挑战性的视频数据集上证明，该方法能显著提升模型在动作描述生成和视频检索任务上的性能，尤其是在处理反向动作时。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>论文提到，当前方法主要处理短视频片段中的原子动作。对于长视频中复杂的、多步骤的动作序列，其“撤销”操作可能不是简单的单个反向动作，该方法可能无法直接泛化。</li>
<li>模型对“撤销”动作的生成依赖于训练数据中存在的、预定义好的动作对关系，对于未见过的或定义模糊的动作，其反转预测可能不准确。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>因果与动态推理</strong>：这项工作为将更复杂的物理常识和因果推理（如工具使用、力与效果）融入VLMs提供了一个有前景的方向。</li>
<li><strong>分层动作理解</strong>：可以探索将Do-Undo思想扩展到分层动作结构，学习子动作与整体目标之间的“执行-撤销”关系。</li>
<li><strong>机器人应用</strong>：此类模型可直接用于机器人任务规划，使机器人不仅能理解如何执行一个任务，还能推理如何撤销或纠正一个已执行的动作，这对于在非结构化环境中的操作至关重要。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本论文针对视觉语言模型中物理动作的生成与反转问题，提出Do-Undo方法。该方法通过集成动作生成和撤销机制，处理序列化交互以模拟真实世界操作。实验验证了Do-Undo在相关任务上的有效性，具体性能提升数据如准确率或效率增益详见论文正文。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.13609" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>