<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computation and Language (cs.CL)</span>
      <h1>Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.22208" target="_blank" rel="noreferrer">2512.22208</a></span>
        <span>作者: Zhao, Pu, Akbari, Arash, Shen, Xuan, Kong, Zhenglun, Shen, Yixin, Chang, Sung-En, Rupprecht, Timothy, Lu, Lei, Nan, Enfu, Yang, Changdi, He, Yumei, Shi, Weiyan, Xu, Xingchen, Huang, Yu, Jiang, Wei, Wang, Wei, Chen, Yue, He, Yong, Wang, Yanzhi</span>
        <span>日期: 2025/12/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，大型语言模型（LLM）领域由GPT-4、Claude等闭源模型以及LLaMA、Mistral等开源权重模型主导。然而，许多所谓的“开源”模型仅分享模型权重，而隐藏了训练数据、代码和实现细节，造成了“开放清洗”现象，这严重阻碍了研究的可复现性、协作创新和商业应用。本文针对这一核心痛点，基于模型开放框架（MOF），旨在推动超越单纯权重共享的完全透明开源范式。具体而言，本文在完全开源的Moxin-7B基础LLM之上，扩展构建了针对不同任务的多模态模型变体，包括视觉语言模型Moxin-VLM、视觉语言动作模型Moxin-VLA以及中文增强模型Moxin-Chinese。本文的核心思路是：遵循MOF原则构建一个完全透明的开源基础模型，并以此为骨干，通过使用开源框架和数据集进行训练，衍生出在各自领域具有竞争力的多模态专用模型。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体上，本文以完全开源的Moxin-7B为基础模型，通过不同的训练策略和数据集，衍生出三个变体。Moxin-VLM负责处理图像和文本输入；Moxin-VLA在VLM基础上增加了机器人动作控制能力；Moxin-Chinese则专注于提升模型的中文理解和生成能力。</p>
<p><strong>Moxin-VLM的核心模块与技术细节</strong>：</p>
<ol>
<li><strong>架构</strong>：采用当前VLM的主流架构，包含视觉表示骨干、视觉语言投影器和语言模型三部分。输入为图像和任意长度的文本提示令牌。</li>
<li><strong>视觉骨干</strong>：创新性地融合了DINOv2和SigLIP两种视觉编码器。DINOv2捕获图像的低级空间属性，SigLIP则提供更高级的语义特征，且其训练数据包含更多样的图像来源（如草图、图表），两者形成互补。</li>
<li><strong>LLM骨干</strong>：使用Moxin-7B-Base作为语言模型核心。</li>
<li><strong>训练数据与策略</strong>：使用完全开源的LLaVA v1.5数据混合集，包含558K图像-文本对和665K多模态指令调优示例。训练采用单阶段方法，冻结视觉表示模块，同时训练投影器和语言模型，共进行两个周期。</li>
</ol>
<p><strong>Moxin-VLA的核心模块与技术细节</strong>：</p>
<ol>
<li><strong>基础</strong>：以Moxin-VLM作为骨干模型进行微调。</li>
<li><strong>训练策略</strong>：探索了两种路径。一是进行大规模通用预训练：在包含超过100万条真实机器人轨迹、涵盖22种不同机器人本体的Open X-Embodiment数据集上进行预训练，使模型获得通用的物理常识和运动原语。二是直接微调：从Moxin-VLM检查点直接开始针对特定任务的微调，以测试其语义先验是否足以支持快速策略学习。</li>
<li><strong>高效微调方法</strong>：采用OpenVLA-OFT配方进行微调。其关键创新在于使用并行解码和动作分块技术：模型同时预测未来多个时间步的动作“块”，而非逐时间步自回归预测。这显著降低了推理延迟，并提高了动作序列的时间一致性。</li>
<li><strong>训练配置</strong>：在8xH100 GPU上训练约两周（通用预训练路径）或9万步（直接微调路径）。使用来自Open X-Embodiment的以Franka机器人平台为中心、包含探索和复杂操作的多样化数据混合。</li>
</ol>
<p><strong>Moxin-Chinese的核心模块与技术细节</strong>：</p>
<ol>
<li><strong>词汇扩展</strong>：原始Moxin词汇表对中文支持有限。为此，从WuDaoCorpus2等来源采样数据，使用SentencePiece训练中文BPE词汇表，并手动合并其他高质量中文词汇，最终词汇表扩展至约57k个词元。</li>
<li><strong>持续预训练与微调</strong>：首先使用WanJuan等多个高质量中文数据集对扩展词汇表后的模型进行持续预训练。随后，使用中英翻译数据集进行指令微调，以进一步提升翻译能力。</li>
</ol>
<p>与现有方法相比，创新点体现在：1）严格遵循MOF的完全开源实践；2）在VLM中融合DINOv2与SigLIP视觉骨干以获取更全面的视觉特征；3）在VLA中探索并比较了通用预训练与直接微调两种策略，并应用了高效的OpenVLA-OFT并行动作解码方法。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>使用的Benchmark与实验平台</strong>：</p>
<ul>
<li><strong>VLM评估</strong>：使用Prismatic VLMs评估套件，包括VizWiz（通用视觉问答）、GQA（空间推理）、RefCOCO/+/g和OCID-Ref（定位）、VSR（视觉空间关系）、TallyQA（计数）和POPE（抗幻觉）。</li>
<li><strong>VLA评估</strong>：在LIBERO仿真环境中评估，任务分为空间、物体、目标和长视野四类。</li>
<li><strong>中文模型评估</strong>：使用CMMLU和CEVAL中文理解基准。</li>
<li><strong>实验平台</strong>：VLA训练与评估使用了8x NVIDIA H100 GPU集群。</li>
</ul>
<p><strong>对比的Baseline方法</strong>：</p>
<ul>
<li><strong>VLM</strong>：LLaVA v1.5 7B，以及使用相同VLM框架但更换LLM骨干为Llama2 7B、Mistral 7B的模型。</li>
<li><strong>VLA</strong>：OpenVLA、SpatialVLA、CoT-VLA、NORA-Long（均包含机器人预训练）；以及不包含机器人预训练的OpenVLA-OFT。</li>
<li><strong>中文模型</strong>：多个开源中文LLaMA和Mistral的中文变体。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>Moxin-VLM</strong>：在7个基准测试的平均准确率达到64.68%，优于所有对比基线（LLaVA v1.5: 57.21%， Llama-2 Chat 7B: 61.43%， Mistral v0.1 7B: 62.83%）。尤其在RefCOCO+上达到71.3%，显著领先。</li>
<li><strong>Moxin-VLA</strong>：<ul>
<li>在<strong>包含机器人预训练</strong>的模型中，Moxin-VLA（9B）在LIBERO上的平均成功率达到**91.95%**，优于其他基线（如NORA-Long: 87.9%， CoT-VLA: 81.1%）。</li>
<li>在<strong>不包含机器人预训练</strong>、直接从VLM微调的设置中，Moxin-VLA平均成功率为**92.5%**，略高于报告中的OpenVLA-OFT（91.9%）。</li>
</ul>
</li>
<li><strong>Moxin-Chinese</strong>：在CMMLU和CEVAL上的得分分别为45和45.76，优于同规模（7B/13B）的其他中文开源模型。</li>
</ul>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li>对于VLA，论文进行了两项关键消融：1）发现训练至50k步与150k步性能差异可忽略，因此采用更高效的50k步设置；2）在单臂机器人设置中，加入FiLM模块对任务成功率无益，故在最终架构中将其移除以提高效率。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出并发布了严格遵循模型开放框架（MOF）的完全开源大型语言模型Moxin-7B，推动了真正的透明、可复现AI研究。</li>
<li>基于Moxin-7B骨干，成功开发并开源了三个高性能多模态变体：Moxin-VLM、Moxin-VLA和Moxin-Chinese，涵盖了视觉语言理解、机器人控制和中英双语能力。</li>
<li>通过系统的实验验证，表明所提出的模型在各自的任务评估中达到了具有竞争力的性能，部分指标显著优于现有开源基线。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到，Moxin-VLA的大规模通用预训练需要大量的计算资源（单节点8xH100 GPU训练约两周），这可能导致较高的训练成本。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>开源生态建设</strong>：Moxin项目展示了完全开源模式在促进协作和创新方面的潜力，为后续研究者建立透明、可复现的模型提供了范本。</li>
<li><strong>多模态学习路径</strong>：在VLA任务上对比通用预训练与直接微调的策略，为如何在计算成本与模型通用能力之间取得平衡提供了实证参考。</li>
<li><strong>架构融合与高效化</strong>：VLM中视觉骨干的融合、VLA中并行动作解码技术的应用，为设计更强大、更高效的多模态模型提供了具体的技术思路。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文旨在推动开源大型语言模型（LLM）向完全透明和可复现性发展。核心是提出了遵循模型开放框架（MOF）的Moxin 7B基础模型，并基于此开发了三个变体：Moxin-VLM（视觉语言）、Moxin-VLA（视觉语言-动作）和Moxin-Chinese。关键技术在于超越仅分享模型权重，实现了训练、数据集及实现细节的全面开源。实验表明，这些模型在多项评估中均取得了优异的性能表现。所有模型、代码与数据均已公开。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.22208" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>