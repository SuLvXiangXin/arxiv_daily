<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.09928" target="_blank" rel="noreferrer">2512.09928</a></span>
        <span>作者: Donglin Wang Team</span>
        <span>日期: 2025-12-10</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为机器人操作的一种有前景的框架。然而，大多数VLA模型隐含地假设马尔可夫性质，仅依赖当前观测来预测动作，导致“时间近视”问题，损害了长视野任务中的时序连贯性。现有缓解方法主要包括堆叠多帧历史图像或预测像素级未来子目标，但这些方法存在显著局限性：堆叠原始帧计算开销大、推理延迟高，并引入了大量像素级冗余和静态噪声；而像素级子目标预测则容易产生局部失真和语义漂移，且缺乏连续的时序建模。</p>
<p>本文针对VLA模型在长视野操作中时序推理效率低下和连贯性不足的核心痛点，提出了一个新视角：将<strong>运动</strong>视为一种更紧凑、信息更丰富的时序上下文和世界动态表示。运动能够捕捉状态间的变化，同时过滤静态的像素级噪声。基于此，本文提出了HiF-VLA，一个利用运动进行双向时序推理的统一框架。其核心思路是：通过运动向量紧凑编码历史动态作为后见之明，基于当前洞察推理未来运动作为先见之明，并利用一个后见调制联合专家整合两者，实现“边行动边思考”的长视野操作范式。</p>
<h2 id="方法详解">方法详解</h2>
<p>HiF-VLA的整体流程基于一个基础的VLA架构进行扩展，旨在通过补偿稀疏的视觉观测来扩展动作执行前的时序感受野。其推理过程可形式化表示为：模型不仅接收当前观测和语言指令，还接收紧凑的历史运动信息，并联合预测未来的动作和运动。</p>
<p><img src="https://arxiv.org/html/2512.09928v1/x2.png" alt="HiF-VLA整体框架图"></p>
<blockquote>
<p><strong>图2</strong>：HiF-VLA流程。（a）<strong>后见之明先验获取</strong>：将密集的历史帧序列编码为紧凑的运动向量流，形成结构化的后见之明基元。（b）<strong>带洞察的先见之明推理</strong>：VLM解释任务指令和当前观测，推理出合理的先见之明运动和对应的潜在动作令牌。（c）<strong>后见调制联合专家</strong>：将后见之明、先见之明和动作表示在统一的潜在空间中进行融合，产生时序一致且因果连贯的动作预测。</p>
</blockquote>
<p>框架包含三个核心模块：</p>
<ol>
<li><strong>后见之明先验获取</strong>：此模块旨在为决策层提供结构化的时空先验。它不直接堆叠历史RGB帧，而是采用视频编解码标准中的<strong>运动向量</strong>来紧凑编码历史动态。运动向量定义为相邻帧间宏块的位置位移，遵循MPEG-4的16x16宏块布局，形成一个低维张量。相较于原始帧，该表示显著减少了冗余，同时保留了任务相关的动态。随后，一个轻量级的基于ViT的后见编码器结合浅层3D卷积，将历史运动向量编码为紧凑的后见令牌 <code>M_h</code>。</li>
<li><strong>带洞察的先见之明推理</strong>：此模块使模型能够预见动作的视觉后果。与预测原始未来像素不同，HiF-VLA以更通用、结构化的运动向量作为时空预测目标。具体而言，在VLM的嵌入空间中引入一组可学习的先见查询令牌和一组空的动作令牌。这些令牌与任务指令、当前观测嵌入拼接后输入VLM，通过<strong>非因果注意力掩码</strong>实现对未来运动潜在令牌 <code>M_f</code> 和动作潜在令牌 <code>A_f</code> 的并行推理。这种设计丰富了VLM内部的“思维”过程，解锁了并行推理的潜力。</li>
<li><strong>后见调制联合专家</strong>：这是实现“边行动边思考”的关键模块。它将动作和运动作为共享时序潜在空间中的两个互补流进行联合建模。<strong>关键创新</strong>在于将历史运动先验作为自适应的时序条件，通过<strong>自适应层归一化</strong>来调制联合推理过程，而不是直接嵌入到VLM输入中（论文指出直接注入会破坏视觉-语言模态的对齐）。<code>M_f</code> 和 <code>A_f</code> 形成两个并行流，通过跨流联合注意力进行交互，同时保留各自的前馈网络以确保互补且解耦的表示。后见令牌 <code>M_h</code> 被投影为条件向量 <code>h_c</code>，通过AdaLN注入每个联合专家模块，调制先见和动作表示，最终由各自的头部分别生成未来运动 <code>m~</code> 和动作 <code>a~</code>。</li>
</ol>
<p>训练时，模型采用结合了动作L1损失和运动L1损失的总体目标函数，通过平衡因子λ进行调节。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在两个广泛使用的长视野操作基准上进行评估：<strong>LIBERO-Long</strong>（10个多子任务场景）和<strong>CALVIN ABC-D</strong>（在ABC场景训练，在未见过的D场景评估连续任务泛化能力）。对比的基线方法包括OpenVLA、OpenVLA-OFT、UniVLA、Seer、π₀等。实验设置分为仅使用主摄像头的第三视角和使用主、腕部双摄像头的多视角。</p>
<p><strong>关键定量结果</strong>：<br>在LIBERO-Long上，HiF-VLA在第三视角设置下平均成功率（Avg. SR）达到94.4%，比基线OpenVLA-OFT（91.0%）提升3.4个百分点；在多视角设置下达到96.4%，优于其他SOTA模型。</p>
<p><img src="https://arxiv.org/html/2512.09928v1/x5.png" alt="LIBERO-Long结果表"></p>
<blockquote>
<p><strong>表1</strong>：LIBERO-Long基准上的性能对比。HiF-VLA在第三视角和多视角设置下均取得了最佳或接近最佳的平均成功率。</p>
</blockquote>
<p>在CALVIN ABC-D上，HiF-VLA在第三视角设置下的平均任务完成长度达到4.08，优于基线（3.80）；在多视角设置下达到4.35，与当前最佳方法性能相当。</p>
<p><img src="https://arxiv.org/html/2512.09928v1/x6.png" alt="CALVIN ABC-D结果表"></p>
<blockquote>
<p><strong>表2</strong>：CALVIN ABC-D基准上的性能对比。HiF-VLA在两种视角设置下均表现出色，尤其在第三视角下提升显著。</p>
</blockquote>
<p><strong>效率与冗余分析</strong>：<br>实验表明，引入基于RGB的子目标或历史帧会显著增加推理延迟（分别达基线的1.59倍和3.15倍）和GPU内存占用，且性能提升有限甚至下降。而HiF-VLA使用运动表示，在引入先见和后见模块后，仅带来微不足道的额外开销（延迟为基线的1.67倍，但绝对值仍远低于帧堆叠方法），同时取得了最佳性能（93.2% SR）。</p>
<p><img src="https://arxiv.org/html/2512.09928v1/x8.png" alt="效率对比表"></p>
<blockquote>
<p><strong>表3</strong>：效率与冗余分析对比。HiF-VLA的变体（运动表示）在性能提升的同时，带来的内存和延迟开销远小于基于RGB帧堆叠或子目标预测的变体。</p>
</blockquote>
<p><strong>消融与可视化分析</strong>：<br><img src="https://arxiv.org/html/2512.09928v1/x3.png" alt="后见长度影响图"></p>
<blockquote>
<p><strong>图3</strong>：（b）HiF-VLA的推理延迟随历史长度增加保持低位且稳定增长；（c）性能随历史长度增加而提升，在长度8左右趋于饱和，验证了运动表示的有效性和时序可扩展性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.09928v1/x4.png" alt="后见嵌入位置消融图"></p>
<blockquote>
<p><strong>图4</strong>：后见信息嵌入位置的消融实验。将后见令牌作为条件调制专家解码器（b）的性能（93.2% SR）明显优于直接注入VLM输入（a，89.6% SR），证实了论文提出的调制策略能有效避免破坏VLM的模态对齐。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了将<strong>运动向量</strong>作为紧凑、结构化的时序基元来扩展VLA时序感受野的新视角；2) 设计了<strong>HiF-VLA统一框架</strong>，通过后见先验获取、先见推理和后见调制联合专家，实现了高效的双向时序推理；3) 实验证明了该方法在多个长视野基准上实现了性能提升，同时保持了<strong>高效率</strong>和<strong>时序可扩展性</strong>。</p>
<p>论文提到的局限性包括：运动向量的提取依赖传统的视频编解码器，可能无法完美捕捉所有精细的动态；框架在非常长的视野（远超训练时的块长度）任务中的表现仍需进一步探索。</p>
<p>这项工作对后续研究的启示在于：为VLA的时序建模提供了一个比原始像素更高效的表示空间；其“边行动边思考”的范式以及将历史信息作为条件调制（而非直接输入）的设计，为构建更复杂、更连贯的具身智能体提供了有价值的思路。未来工作可以探索更先进的运动表示学习方式，或将此框架与更长期的规划方法相结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在长时程操作中因依赖当前观测（时间近视）而导致连贯性下降的问题，提出HiF-VLA框架。该框架以运动表示为紧凑时序上下文，通过**后见编码过去动态、预见推理未来运动**，并经由**后见调制联合专家**实现双向时序推理与“边思考边行动”。实验表明，HiF-VLA在LIBERO-Long与CALVIN ABC-D基准上超越强基线，且推理延迟几乎无增加，在真实长时程操作任务中取得显著性能提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.09928" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>