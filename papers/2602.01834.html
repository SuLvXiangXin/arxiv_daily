<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.01834" target="_blank" rel="noreferrer">2602.01834</a></span>
        <span>作者: Wen, Siqi, Yang, Shu, Fu, Shaopeng, Zhang, Jingfeng, Hu, Lijie, Wang, Di</span>
        <span>日期: 2026/02/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过将多模态指令转化为可执行行为，实现了感知-动作的闭环。然而，这种能力也放大了安全风险：在大型语言模型（LLMs）中仅产生有害文本的越狱攻击，在具身系统中可能触发不安全的物理动作。现有防御方法，如对齐、过滤或提示强化，要么介入过晚，要么在错误的模态上进行干预，使得融合后的表征仍可被利用。本文针对VLA模型中不安全意图在融合表征中传播这一具体痛点，提出了一种在表征层面进行干预的新视角。核心思路是：利用具身VLA动作空间受物理约束、不安全概念集合很小的结构性不对称特点，通过构建稀疏、可解释的概念字典来识别有害概念方向，并在推理时应用基于阈值的干预来抑制不安全激活，从而将模型状态限制在安全区域内。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的SAFE-Dict方法作为一个即插即用的安全防火墙，作用于VLA模型解码器产生的融合隐藏状态 <code>h</code> 上，在动作执行之前拦截不安全意图，无需重新训练或修改骨干模型。</p>
<p><img src="https://arxiv.org/html/2602.01834v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SAFE-Dict作为具身智能体的表征级安全防火墙。该守卫作用于由端到端VLA模型和VLM驱动的具身智能体共享的融合潜在表征上，在动作执行前拦截不安全意图，无需重新训练或修改骨干模型。</p>
</blockquote>
<p>方法流程分为三个阶段：概念挖掘与刺激句构建、概念字典学习、推理时安全控制。</p>
<p><strong>1. 概念挖掘与刺激句构建</strong><br>目标是从数据集中提取语义上有意义的安全和不安全概念对应的潜在方向。首先，使用一个预训练的视觉-语言模型（如Qwen2.5-VL）分析VLA训练数据集中的图像-指令对，识别出显著的物体和实体，形成一个全局概念词汇表 <code>C = {c1, c2, ..., cM}</code>（例如 <code>gasoline</code>, <code>knife</code>, <code>child</code>）。接着，对于每个概念 <code>ci</code>，使用一个大语言模型（如Qwen-3）生成一系列“刺激句”。这些句子明确包含目标概念 <code>ci</code>，且在语言风格上与原始VLA数据集保持一致，用于激发模型针对该概念的纯净激活。同时，LLM会为每个概念分配一个危害性分数 <code>wi ∈ [0,1]</code>，用于后续风险评估。</p>
<p><img src="https://arxiv.org/html/2602.01834v1/x3.png" alt="概念示例"></p>
<blockquote>
<p><strong>图3</strong>：提取出的概念示例（如碗、汽油、有毒）以及示例刺激句，展示了原子概念如何嵌入到自然主义的任务指令中。（刺激句仅用于字典构建，不用于推理时。）</p>
</blockquote>
<p><strong>2. 概念字典学习</strong><br>对于每个概念 <code>ci</code>，将其所有刺激句输入VLA模型，并收集解码器最后一层的隐藏状态激活集合 <code>Hi</code>。然后，对每个 <code>Hi</code> 使用基于SVD的PCA方法，取其第一主成分作为该概念的潜在方向 <code>ui ∈ ℝ^d</code>。将所有概念的方向聚合，即构成概念字典 <code>D = [u1, u2, ..., uM] ∈ ℝ^(d×M)</code>。该字典将高维、纠缠的隐藏状态空间重新基底到可解释的概念方向上。</p>
<p><strong>3. 推理时安全控制</strong><br>在推理时，对于给定的输入产生的隐藏状态 <code>h</code>，首先通过ElasticNet回归将其稀疏地投影到概念字典上：<br><code>z = arg min_z ||h - Dz||_2^2 + α||z||_1 + β||z||_2^2</code><br>其中 <code>z = (z1, z2, ..., zM)</code> 是各概念的激活系数。然后，计算整体有害分数 <code>s(h) = Σ_{i=1}^M wi * zi</code>。若 <code>s(h)</code> 超过预设阈值 <code>τ</code>，则对有害概念索引集 <code>I_harm</code> 中的系数进行衰减：<code>z_i&#39; = (1-γ) * z_i</code>，其中 <code>γ</code> 是衰减强度。最后，用调整后的系数重构隐藏状态：<code>h&#39; = D z&#39;</code>。这种衰减策略旨在平滑地抑制有害概念，而非直接终止任务，从而在保证安全的同时尽可能维持任务效用。</p>
<p>与现有方法相比，创新点在于：1) <strong>首次将概念字典学习应用于具身VLA系统的推理时安全</strong>；2) <strong>在融合表征层面进行干预</strong>，直接在不安全意图形成行动计划之前进行阻断；3) <strong>统一的防御框架</strong>，能同时处理显式有害指令和对抗性越狱攻击。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在多个具身VLA安全基准上进行：显式有害指令基准 <strong>Libero-Harm</strong>，对抗性越狱攻击基准 <strong>BadRobot</strong> 和 <strong>RoboPAIR</strong>，以及交互式多步安全评估基准 <strong>IS-Bench</strong>。对比的基线方法包括无防御的默认模型、提示安全方法（Prompt-based Safety）以及其他前沿防御方法如CCE、SmoothLLM、PARDEN等。</p>
<p><strong>关键实验结果：</strong><br>在Libero-Harm上（表1），SAFE-Dict将攻击成功率（ASR）从默认模型的84.7%大幅降低至7.8%，显著优于提示安全方法的41.2%。</p>
<p><img src="https://arxiv.org/html/2602.01834v1/x4.png" alt="有害场景示例"></p>
<blockquote>
<p><strong>图4</strong>：Libero-Harm数据集中的示例场景。展示了注入LIBERO环境的三个代表性有害指令：(a)引入火灾隐患；(b)模拟中毒风险；(c)制造电气危险。</p>
</blockquote>
<p>在对抗性越狱攻击上（表2），SAFE-Dict同样表现出色。在BadRobot基准上，对于Llama-3.2-Vision模型，ASR从73.83%降至6.30%；对于Qwen2-VL模型，从29.52%降至5.43%。在RoboPAIR基准上，SAFE-Dict在ASR-auto和Syntax-auto之间取得了良好的权衡，且推理时间与默认模型相当。</p>
<p>在交互式多步安全场景IS-Bench上（表3），SAFE-Dict在维持较高任务成功率（SR: 59.2%，接近默认模型的66.5%）的同时，大幅提升了安全成功率（SSR: 72.5%）和安全召回率（SRec(All): 57.8%），避免了提示安全方法因过于保守而严重损害任务性能（SR: 29.8%）的问题。</p>
<p><strong>消融实验分析：</strong><br>论文对关键超参数进行了详尽的消融研究。图5展示了阈值 <code>τ</code> 和衰减强度 <code>γ</code> 的影响。适中的值（<code>τ ≈ 0.85</code>, <code>γ ≈ 0.6</code>）能在安全（低ASR）和效用（高SR/SSR）之间取得最佳平衡。</p>
<p><img src="https://arxiv.org/html/2602.01834v1/x5.png" alt="超参数消融"></p>
<blockquote>
<p><strong>图5</strong>：干预超参数的消融研究。(a,b) 阈值 <code>τ</code> 的影响：中等值（<code>τ ≈ 0.85</code>）在安全性和效用性之间取得了最佳权衡。(c,d) 衰减强度 <code>γ</code> 的影响：中等程度的抑制（<code>γ ≈ 0.6</code>）实现了最佳平衡。</p>
</blockquote>
<p>表4和表5分析了ElasticNet正则化参数 <code>α</code>（控制稀疏性）和 <code>β</code>（控制稳定性）的影响。增加 <code>α</code> 可以锐化概念分离、提升安全性，但过大值会损害任务成功率。添加一个较小的 <code>ℓ2</code> 惩罚（<code>β</code>）相比纯Lasso（<code>β=0</code>）能提高鲁棒性，性能在中等 <code>β</code> 值（<code>5×10^{-4}</code>）时达到峰值。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) <strong>提出了第一个用于具身系统的推理时、基于概念的安全方法</strong>，为VLA安全部署提供了新思路；2) <strong>构建了一个基于概念字典学习的可解释、表征级防御框架</strong>，该框架即插即用、无需重新训练、可泛化到不同具身模型；3) <strong>实现了统一的防御</strong>，在多个基准上显著降低了显式有害指令和对抗性越狱攻击的成功率（降低超过70%），同时保持了良性任务性能。</p>
<p>论文自身提到的局限性包括：概念提取依赖外部VLM和LLM，其性能可能影响字典质量；方法基于具身VLA不安全概念集很小的假设，在更开放域的场景中可能面临挑战。</p>
<p>本文的启示在于：对于动作空间受限的具身AI系统，在融合表征层面进行稀疏、概念化的干预是一条高效且可解释的安全途径。这鼓励后续研究进一步探索更自动化的概念发现方法，并将该框架扩展到更复杂的多模态推理和长时程规划任务中，以实现更全面的安全保证。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉语言动作模型在推理时执行不安全物理动作的安全风险，提出了一种基于概念的字典学习框架。该方法通过从模型隐藏激活中构建稀疏、可解释的字典，识别有害概念方向，并采用基于阈值的干预来抑制不安全激活。实验在多个基准上进行，结果表明该框架将攻击成功率降低了超过70%，同时保持了任务成功率，且无需重新训练即可即插即用地集成到不同模型中。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.01834" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>