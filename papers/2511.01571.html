<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.01571" target="_blank" rel="noreferrer">2511.01571</a></span>
        <span>作者: Liang, Wenqi, Sun, Gan, He, Yao, Dong, Jiahua, Dai, Suyan, Laptev, Ivan, Khan, Salman, Cong, Yang</span>
        <span>日期: 2025/11/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型通过结合大规模机器人数据与预训练的视觉-语言模型，在泛化性和指令遵循能力上展现出优势，成为学习通用视觉运动控制策略的有力工具。然而，现有主流VLA方法存在两个关键局限性：首先，它们大多继承自VLM，仅处理图像级别的视觉信息，缺乏对场景的细粒度像素级理解，这限制了空间推理能力并削弱了分布外泛化性能；其次，它们严重依赖文本指令作为提示，忽略了微妙的视觉线索，限制了多模态人机交互的灵活性。</p>
<p>本文针对上述两个具体痛点，提出了新的视角：致力于构建一个同时支持像素级推理和文本/视觉多模态提示的VLA模型。其核心思路是设计一个新颖的视觉运动指令调优框架，通过集成多尺度像素感知编码器和视觉提示编码器，将像素级理解注入VLA，并使其能够有效处理多样化的视觉提示。</p>
<h2 id="方法详解">方法详解</h2>
<p>PixelVLA的整体框架基于预训练的VLM主干（如Prismatic-7B），并引入了三个核心新模块：一个用于处理多样化视觉提示的视觉提示编码器，一个用于注入像素级理解的多尺度像素感知编码器，以及一个用于预测连续机器人动作的连续动作解码器。输入包括图像观测、文本指令、像素感知掩码和视觉提示，输出为7维的机器人动作序列。</p>
<p><img src="https://arxiv.org/html/2511.01571v1//fig//Overview.pdf" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：PixelVLA架构总览。模型集成了三个新组件：(1) 视觉提示编码器，用于处理输入的各种视觉提示；(2) 多尺度像素感知编码器，将像素级信息注入令牌嵌入；(3) 连续动作解码器，用于预测7D机器人动作。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>多尺度像素感知编码器</strong>：该模块旨在从多尺度图像特征中提取像素级信息。给定初始图像观测，首先通过视觉编码器提取多级视觉特征。然后，利用一个像素感知掩码输入，通过计算掩码区域内多级特征的平均池化，并结合线性投影和MLP，生成像素感知嵌入。同时，为了保留视觉提示的空间位置信息，使用视觉提示编码器提取特征并通过MLP生成提示感知嵌入。这些嵌入将与视觉嵌入、语言嵌入一起输入到LLM主干中。</li>
<li><strong>视觉提示编码器</strong>：为了容纳点、线、区域、掩码等多种形式的视觉提示，PixelVLA集成了一个轻量级的提示编码器（来自SAM模型），用于处理这些输入并提取相关特征。</li>
<li><strong>连续动作解码器</strong>：不同于许多现有VLA采用自回归生成离散动作令牌的方法，PixelVLA遵循π0的设计，开发了一个连续动作解码器。它直接处理LLM主干最后一层的隐藏状态，通过线性投影器、多个ResNet块和一个MLP投影器，直接预测连续的动作表示，旨在利用像素级理解来捕捉细粒度的动作细节。</li>
</ol>
<p><strong>与现有方法的创新点</strong>：主要创新体现在架构上引入了专门的模块以实现像素级理解和多模态提示处理，并采用了新颖的两阶段视觉运动指令调优流程进行训练。此外，为了生成训练所需的数据，论文提出了一种自动标注流程来构建像素级标注数据集Pixel-160K。</p>
<p><img src="https://arxiv.org/html/2511.01571v1//fig//datasets.pdf" alt="数据生成流程"></p>
<blockquote>
<p><strong>图3</strong>：Pixel-160K数据集构建流程概述。采用两阶段自动标注流程：夹爪感知区域提议阶段使用视频分割模型定位夹爪并生成目标对象的初步区域提议；多模态对象分割阶段利用LLM和开放词汇分割模型从区域提议中预测像素级标注并生成多模态提示。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在三个仿真基准上进行评估：SimplerEnv-Google Robot、SimplerEnv-WidowX和LIBERO。对比的基线方法包括RT-1-X、Octo-Base、HPT、RoboVLMs、Dita、SpatialVLA、OpenVLA、TraceVLA和π0。为了验证有效性，将PixelVLA的架构和训练流程应用于两个广泛使用的VLA：OpenVLA和π0，得到PixelVLA和PixelVLA-π0。</p>
<p><strong>关键实验结果</strong>：<br>在SimplerEnv-Google Robot基准的零样本物体操纵任务中，PixelVLA相比其基础模型OpenVLA在平均成功率上取得了显著提升。具体而言，在视觉匹配和变体聚合两种评估设置下，PixelVLA将平均成功率从OpenVLA的32.7%/40.0%提升至61.4%/50.1%。PixelVLA-π0也将π0的54.5%/54.8%提升至63.3%/56.5%。</p>
<p><img src="https://arxiv.org/html/2511.01571v1//fig//experiments.pdf" alt="实验结果表"></p>
<blockquote>
<p><strong>图4</strong>：在LIBERO基准上的性能对比。图表显示，PixelVLA在多个任务套件（空间、物体、目标、长程）上均优于基线方法OpenVLA和TraceVLA，尤其是在需要复杂空间理解的LIBERO-Spatial任务上提升明显。</p>
</blockquote>
<p>在LIBERO基准上评估对新机器人设置的适应能力。如图4所示，PixelVLA在LIBERO的四个任务套件（Spatial, Object, Goal, Long）上均持续优于其基础模型OpenVLA以及另一改进方法TraceVLA，特别是在需要空间推理的LIBERO-Spatial任务上优势显著。</p>
<p><strong>消融实验总结</strong>：论文通过消融实验量化了各模块的贡献。关键结论包括：1) <strong>多尺度像素感知编码器</strong>是性能提升的核心，移除后成功率下降最显著；2) <strong>视觉提示编码器</strong>对于处理视觉提示至关重要，尤其是在涉及视觉提示的任务中；3) <strong>两阶段训练策略</strong>（特别是像素级理解增强阶段）对于实现像素级理解是有效的；4) <strong>Pixel-160K数据集</strong>的质量对最终性能有直接影响，使用自动标注管道生成的数据训练相比使用现成VLM/分割模型直接生成标注训练，能带来明显性能增益。此外，PixelVLA仅需OpenVLA预训练计算成本的1.5%，体现了其高效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：1) <strong>提出了PixelVLA模型</strong>，首次在VLA中实现了像素级理解并支持文本和视觉多模态提示，其关键组件包括视觉提示编码器、多尺度像素感知编码器和连续动作解码器。2) <strong>设计了自动标注管道并构建了Pixel-160K数据集</strong>，这是一个包含约16万条带有像素级标注和视觉提示的机器人操作序列的大规模视觉运动指令调优数据集。3) <strong>引入了新颖的两阶段视觉运动指令调优框架</strong>，通过连续动作训练和像素级理解增强两个阶段，有效地将像素级理解能力注入现有VLA。</p>
<p>论文提到的局限性主要在于其数据生成管道依赖于自动标注工具（如SAM 2、Grounding DINO），这些工具在极其杂乱或低质量的机器人观测中可能失效，影响了Pixel-160K数据集的规模（过滤了约19.2%的失败样本）。此外，模型训练仍需要相当的计算资源。</p>
<p>这项工作对后续研究的启示在于：证明了将像素级视觉理解与多模态提示相结合是提升VLA空间推理和操作精度的有效途径。其模块化设计使得该方法可以相对低成本地集成到现有VLA中。未来研究可以探索更鲁棒和高效的像素信息编码方式，以及开发质量更高、规模更大的像素级机器人数据集。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型缺乏像素级场景理解、过度依赖文本提示的问题，提出PixelVLA模型。其关键技术包括：用于像素级推理与多模态提示的视觉运动指令调优框架，集成多尺度像素感知编码器与视觉提示编码器；以及两阶段自动标注流程构建的大规模像素标注数据集Pixel-160K。实验表明，PixelVLA在三个标准基准上比OpenVLA提升操作成功率10.1%∼28.7%，且预训练成本仅为其1.5%，实现了更精准高效的机器人控制。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.01571" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>