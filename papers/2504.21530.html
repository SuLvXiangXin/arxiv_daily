<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboGround: Robotic Manipulation with Grounded Vision-Language Priors - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RoboGround: Robotic Manipulation with Grounded Vision-Language Priors</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.21530" target="_blank" rel="noreferrer">2504.21530</a></span>
        <span>作者: Huang, Haifeng, Chen, Xinyi, Chen, Yilun, Li, Hao, Han, Xiaoshen, Wang, Zehan, Wang, Tai, Pang, Jiangmiao, Zhao, Zhou</span>
        <span>日期: 2025/04/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域的主流方法经历了从依赖模仿学习的早期低层策略，到近期利用大规模数据和预训练视觉语言模型（VLM）的视觉-语言-动作（VLA）模型的演变。然而，VLA方法仍需大量数据和额外微调才能泛化到新场景，成本高昂。中间表示作为一种替代方案，可分为两类：一是易于生成但空间精度不足的粗粒度表示（如语言指令）；二是能提供详细空间指导但数据与计算资源需求大的细粒度表示（如目标图像、点流）。本文针对中间表示在空间精度与泛化潜力之间难以平衡的痛点，提出了<strong>接地掩码</strong>作为一种新的中间表示视角。其核心思路是利用在大规模接地数据集上预训练的大视觉语言模型生成目标物体和放置区域的像素级掩码，以此作为空间引导来增强底层策略网络的泛化能力，并通过构建高复杂度的仿真数据来充分探索该表示的有效性。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboGround系统的整体框架包含两个核心模块：用于生成接地掩码的视觉语言模型，以及整合掩码引导的策略网络。</p>
<p><img src="https://arxiv.org/html/2504.21530v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：RoboGround整体架构。 (a) 接地的视觉语言模型处理图像观察和语言指令，生成目标掩码。(b) 接地的策略网络通过通道拼接掩码与图像，并在接地感知器中引导注意力，来整合掩码引导。</p>
</blockquote>
<p><strong>1. 接地的视觉语言模型</strong>：该模块基于GLaMM模型构建，输入为图像观察 (x_v) 和文本指令 (x_t)，输出为指令指定的目标物体和/或目标放置区域的二值掩码 (M_o) 和 (M_p)。模型首先使用CLIP视觉编码器提取图像特征，并通过MLP投影器映射到大语言模型（LLM）的嵌入空间。LLM整合投影后的视觉特征和文本指令，生成包含特殊<SEG>令牌的文本输出。接地头利用一个以SAM编码器初始化的细粒度图像编码器 (\mathcal{E}) 和一个类似SAM的解码器 (\mathcal{D})，将<SEG>令牌对应的特征解码为像素级分割掩码。对于机器人操作任务，模型被提示分别分割目标物体和放置区域，每个区域对应一个独立的<SEG>令牌，解码后得到两个掩码。</p>
<p><strong>2. 接地的策略网络</strong>：策略网络采用类似GR-1的语言条件化Transformer架构。其创新点在于如何将VLM生成的掩码整合进来，具体通过两种方式：</p>
<ul>
<li><strong>通道级整合</strong>：将原始图像 (x_v) 与目标物体掩码 (M_o)、放置区域掩码 (M_p) 进行通道拼接，然后通过一个线性层投影回3通道，再送入预训练的ViTMAE编码器，得到视觉特征 (\mathbf{Z_v})。</li>
<li><strong>块级整合（接地感知器）</strong>：标准感知器使用可学习的查询令牌对视觉块特征进行重采样，可能导致信息丢失。本文提出的接地感知器 (\mathcal{P}) 引入了额外的、针对目标物体和放置区域的查询令牌（(\mathbf{Q_o}, \mathbf{Q_p})）。在注意力层中，这些查询令牌与视觉块特征交互时，会受到相应掩码（(M_o, M_p)）的引导（通过掩码填充操作），确保注意力集中在关键区域。最终，感知器输出的视觉特征由全局、目标物体、放置区域三组查询令牌的结果拼接而成。</li>
</ul>
<p>策略网络的输入序列由历史视觉特征（CLS令牌 + 接地感知器处理后的块特征）、CLIP编码的文本指令特征、MLP编码的机器人状态特征以及可学习的ACT令牌构成。Transformer解码器处理该序列后，通过输出ACT令牌预测下一步的机械臂（连续动作，用Smooth-L1损失）和夹爪（二值动作，用BCE损失）动作。</p>
<p><strong>3. 训练与推理</strong>：首先，利用生成的仿真数据构建指令遵循数据集，对GLaMM VLM进行微调，损失函数包括文本生成的交叉熵损失以及分割的BCE与DICE损失。策略网络则使用动作预测的联合损失进行训练。推理时，在任务开始时由VLM生成一次掩码，并在整个单轮操作任务中持续提供给策略网络作为引导，以提高效率。对于多视角观测，每个视角独立并行处理以获取掩码。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在仿真环境中进行，使用了基于RoboCasa扩展生成的大规模数据集（24K演示，112K条指令，涵盖176个类别的3526个物体）。评估任务为语言引导的拾放操作。Baseline方法包括：纯语言条件的GR-1、使用VLM生成粗糙边界框作为中间表示的MOO，以及本文方法的多个变体（仅通道整合、仅感知器引导）。</p>
<p><strong>关键实验结果</strong>：<br><img src="https://arxiv.org/html/2504.21530v1/x4.png" alt="指令泛化结果"></p>
<blockquote>
<p><strong>图4</strong>：在不同类型指令（外观、空间、常识）下的成功率。RoboGround在所有指令类型上均显著优于GR-1和MOO，尤其在需要复杂推理的常识指令上优势明显。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.21530v1/x5.png" alt="物体与类别泛化结果"></p>
<blockquote>
<p><strong>图5</strong>：在未见物体和未见类别上的泛化性能。RoboGround在两种泛化设置下均取得了最佳性能，证明了掩码引导对于处理新对象的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.21530v1/x6.png" alt="技能评估结果"></p>
<blockquote>
<p><strong>图6</strong>：在推、扫、叠放、插入四种核心机器人技能上的评估。RoboGround在大多数技能上表现优于或与基线相当，展示了其处理多样化操作任务的能力。</p>
</blockquote>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2504.21530v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：不同模块的消融研究。同时使用通道整合和接地感知器引导（Full）效果最佳；仅使用二者之一仍有提升但不及完整模型；而使用真实标注掩码（GT Mask）的性能上限表明当前VLM生成的掩码质量仍有改进空间。</p>
</blockquote>
<p>文字总结：在整体测试集上，RoboGround达到了85.6%的成功率，显著高于GR-1的68.3%和MOO的79.5%。消融实验表明，通道整合和接地感知器引导两个组件均能带来性能提升（分别贡献约6%和10%），且二者结合时效果最佳（累计提升约17%）。使用真实标注掩码能将性能进一步提升至91.1%，指出了VLM掩码生成质量的改进方向。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出接地掩码作为中间表示</strong>：论证了其平衡空间指导精度与泛化潜力的优势，并系统性地将其整合进机器人策略网络。</li>
<li><strong>构建高复杂度仿真数据集</strong>：通过自动化流程生成了包含多样物体、复杂场景和丰富指令类型的大规模数据，为训练和评估泛化能力提供了坚实基础。</li>
<li><strong>设计接地感知器</strong>：在策略网络中创新性地引入掩码引导的注意力机制，有效保留了关键区域的空间信息，提升了操作精度。</li>
</ol>
<p><strong>局限性</strong>：论文提到，系统的性能部分依赖于接地VLM生成掩码的准确性，不完美的掩码会影响策略。此外，当前推理流程假设任务为单轮操作且目标位置稳定，可能不适用于需要动态重新接地或长视野规划的任务。</p>
<p><strong>启示</strong>：本工作展示了利用大模型提供结构化、可泛化的空间先验来增强传统策略网络的潜力。未来研究可探索更高效的掩码生成与利用方式，或将此框架扩展到需要动态感知、多步推理的更复杂操作任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RoboGround系统，以解决机器人操作策略在新物体、新场景中泛化能力有限的核心问题。其关键技术是引入“接地掩码”作为中间表示，结合大规模视觉语言模型的先验知识，为策略网络提供精确的空间目标与放置区域指导，并设计了自动生成多样化模拟数据的流程。实验表明，该方法能显著提升策略的泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.21530" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>