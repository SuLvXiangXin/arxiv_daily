<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.17148" target="_blank" rel="noreferrer">2510.17148</a></span>
        <span>作者: Gao, Yu, Jiang, Anqing, Wang, Yiru, Jijun, Wang, Jiang, Hao, Sun, Zhigang, Yuwen, Heng, Shuo, Wang, Zhao, Hao, Hao, Sun</span>
        <span>日期: 2025/10/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前端到端自动驾驶模型通过将多模态传感器输入直接映射到控制信号或驾驶轨迹，在生成物理可行的驾驶行为方面取得了显著进展。然而，这些模型依赖于强大的时空表征，在高级场景理解和语义推理方面能力有限，因此在处理长尾或未见过的场景时往往表现不佳。另一方面，视觉-语言-动作模型利用编码于大语言模型中的广泛世界知识来处理具有挑战性的案例，但其有限的3D推理能力可能导致产生物理上不可行的动作。本文针对的核心痛点是：如何将语义丰富但偶尔不可行的VLA轨迹与物理可行但语义有限的E2E轨迹进行有效结合。本文提出了一种通过度量引导对齐机制，明确桥接认知推理与端到端规划的框架DiffVLA++，其核心思路是分别构建能生成语义轨迹的VLA模块和确保物理可行性的E2E模块，并引入一个共享的、基于规则的度量评分器来对齐和统一两者的输出。</p>
<h2 id="方法详解">方法详解</h2>
<p>DiffVLA++框架包含三个主要组件：一个视觉-语言-动作模块、一个传统的端到端驾驶模块，以及一个作为度量引导对齐器的轨迹评分器。</p>
<p><img src="https://arxiv.org/html/2510.17148v4/x1.png" alt="DiffVLA++架构总览"></p>
<blockquote>
<p><strong>图1</strong>：DiffVLA++架构总览。包含三个主要组件：(i) VLA模块，(ii) 传统的E2E模块，两者都能直接生成规划轨迹，以及(iii) 一个作为度量引导对齐器的轨迹评分器，用于统一它们的输出。</p>
</blockquote>
<p><strong>VLA模块</strong>：这是一个完全可微的VLA模型，旨在生成具有显式3D推理的语义轨迹。其视觉处理流采用基于CLIP的Vision Transformer（ViT-L/14），将多视角图像编码为一组紧凑的视觉令牌。这些令牌通过一个驾驶视觉适配器进行压缩和投影，以确保与下游语言模型的兼容性。语言流则对导航命令和高级驾驶指令进行编码。视觉与语言令牌在一个大语言模型中进行多模态融合，该模型使用因果注意力在统一空间中进行联合推理。最后，LLM的最后一层将融合后的隐藏状态直接投影为自车在未来四秒内的连续轨迹（航路点），每个航路点包含横向位置、纵向位置和航向角。这种设计避免了离散化误差，并允许驾驶策略的端到端优化。</p>
<p><strong>E2E模块</strong>：该模块基于由BevFormer生成的密集鸟瞰图表示进行操作。BEV特征图被离散化为128x128的网格。模块采用多任务架构，包含三个预测头：动态智能体检测头、BEV空间语义分割头和轨迹规划头。轨迹规划头在一个预定义的密集轨迹词汇表上操作，该词汇表包含M=8192个候选轨迹，每个候选由在四秒内以2Hz采样的八个航路点组成。为了编码每个候选轨迹，使用双线性网格采样在其航路点处对BEV特征进行采样，然后通过一个以自车状态为条件的注意力机制聚合成紧凑的嵌入。这些轨迹嵌入随后通过可变形交叉注意力与来自检测头的智能体中心特征进行交互，从而生成上下文感知的轨迹假设。每个精炼后的嵌入通过一个MLP解码为残差偏移，最终预测的轨迹由选中的候选轨迹加上其残差偏移构成。</p>
<p><strong>度量引导对齐</strong>：这是DiffVLA++的关键创新，旨在桥接VLA的认知能力和E2E的物理可行性。其核心是一个轻量级的轨迹评分器，该评分器实现为一组并行的MLP头，每个头从Navsim模拟器中回归一个驾驶度量。评分器接收由规划头产生的上下文感知轨迹嵌入，并同时为每个候选轨迹预测八个度量分数。这些度量包括：衡量沿路线中心线进度的连续分数EP；指示可行驶区域合规、交通灯合规、碰撞避免、车道保持和历史舒适度的二元分数（DAC, TLC, TTC, LK, HC）；以及衡量碰撞和行驶方向的三元分数（NC, DDC）。评分器与E2E驾驶模型联合训练，将BEV特征空间与基于规则的驾驶评估相关联，并为更安全的轨迹选择提供辅助监督。训练损失是加权复合目标，针对连续、二元和三元度量分别使用MSE、二元交叉熵和交叉熵损失。通过将VLA和E2E生成的轨迹都投影到这个共享的度量空间，评分器使得通过一个共同的、可解释的性能基准进行显式对齐成为可能。</p>
<p><strong>后处理与集成</strong>：在推理时，首先使用全景驾驶感知模型从前视摄像头预测可行驶区域，并将候选轨迹投影到该视图中，丢弃落在预测可行驶区域之外的轨迹以进行安全检查。对于剩余的候选轨迹，计算其预测度量分数的加权和作为最终得分，并保留得分最高的轨迹作为E2E模块的最终输出。类似地，VLA模块生成的轨迹也使用相同的轨迹评分器进行评估得到最终得分。系统通过比较E2E和VLA的最终得分来选择整体输出轨迹。在本次竞赛中，由于时间限制，两个系统通过离线集成进行结合。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在NavsimV2基准和ICCV 2025 Autonomous Grand Challenge排行榜上进行。主要对比了框架内VLA分支和E2E分支的性能，并通过集成展示了最终效果。</p>
<p><strong>关键实验结果</strong>：<br>论文以表格形式展示了VLA分支、E2E分支以及最终集成模型在相关测试集上的性能。VLA分支在Navhard两阶段测试中取得了48.0的扩展预测驾驶员模型分数，而E2E分支取得了43.7的分数。这验证了VLA模块在复杂场景下凭借其认知推理能力所表现出的优势。</p>
<p>最终集成模型在ICCV 2025 Autonomous Grand Challenge公开排行榜上取得了EPDMS为49.1238的综合成绩，并在各项具体度量上表现优异。例如，在第一阶段，无过错碰撞合规率达到98.2143%，可行驶区域合规率达到98.5714%，行驶方向合规率达到100%，交通灯合规率达到99.2857%。这些结果证明了通过度量引导对齐集成VLA和E2E模块的有效性，其综合性能超越了任一独立模块。</p>
<blockquote>
<p><strong>表1</strong>：DiffVLA++各分支在Navhard两阶段测试中的结果。VLA分支的EPDMS为48.0，E2E分支为43.7，表明VLA在复杂场景下更具优势。<br><strong>表2</strong>：DiffVLA++在公开排行榜上的详细结果。最终集成模型取得了49.1238的EPDMS，并在安全性（如NC、DAC）、合规性（如TLC、DDC）和进度（EP）等多个度量上取得了高分，体现了框架的综合性能。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：第一，提出了一个新颖的度量引导对齐机制，为桥接认知丰富的VLA模型和物理可行的E2E模型提供了一个原则性的、可解释的共享评估空间。第二，构建了一个完全可微分的VLA模块，能够直接生成语义丰富的、连续的驾驶轨迹。第三，通过将两个模块的优势集成，在ICCV 2025挑战赛基准上实现了超越各自独立性能的综合驾驶表现。</p>
<p>论文自身提到的局限性在于，由于竞赛时间限制，VLA和E2E模块是通过离线集成而非在线联合优化进行结合的。这暗示了未来研究的一个潜在方向：探索更紧密的在线联合训练或推理机制，以进一步发挥两个范式融合的潜力。本文的工作为自动驾驶领域提供了一个重要的启示：单纯依赖数据驱动的端到端模型或知识驱动的认知模型都可能存在瓶颈，而通过设计巧妙的对齐机制将它们互补的优势结合起来，是迈向更通用、更可靠自动驾驶系统的一条有前景的路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出DiffVLA++框架，旨在解决端到端驾驶模型缺乏认知推理能力、而视觉语言动作模型物理可行性不足的核心问题。方法上，设计VLA模块生成语义驱动的轨迹，E2E模块保证物理可行性，并通过一个度量引导的轨迹评分器对齐两者输出，从而融合其互补优势。实验在ICCV 2025自动驾驶挑战中取得EPDMS 49.12的性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.17148" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>