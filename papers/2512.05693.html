<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.05693" target="_blank" rel="noreferrer">2512.05693</a></span>
        <span>作者: Du, Zhiying, Liu, Bei, Liang, Yaobo, Shen, Yichao, Cao, Haidong, Zheng, Xiangyu, Feng, Zhiyuan, Wu, Zuxuan, Yang, Jiaolong, Jiang, Yu-Gang</span>
        <span>日期: 2025/12/05</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于大规模机器人演示数据训练通用视觉-语言-动作（VLA）模型是具身智能发展的关键。主流方法通常在Open X-Embodiment等异构数据集上进行预训练，再针对特定领域微调。然而，与相对统一的视觉或语言数据不同，机器人数据存在本质的异构性，体现在机器人本体、动作空间（如关节角度控制与末端执行器控制）、传感器配置、控制频率乃至操作员风格等多个方面。现有方法缺乏处理这种异构性的原则性设计，导致难以有效整合多样数据，限制了模型的泛化能力和知识迁移效率。</p>
<p>本文针对机器人数据固有的、多来源的异构性这一核心痛点，提出了一种新的视角：通过分层的混合专家（Mixture-of-Experts, MoE）架构来显式地解耦并处理不同层次的异构性。本文的核心思路是设计一个分层MoE动作模块，在浅层专门处理动作空间差异，在相邻层处理更广泛的异构性，并通过中间的共享Transformer块将它们逐步抽象、整合为可迁移的共享知识表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>HiMoE-VLA的整体框架整合了一个预训练的视觉-语言模型（VLM）主干和一个新颖的、基于分层混合专家（HiMoE）的动作模块。其输入为语言指令l、机器人本体感知状态q_t和视觉观测o_t（多视角RGB图像），输出为未来H步的动作序列A_t。训练采用流匹配（flow-matching）目标来建模动作分布。</p>
<p><img src="https://arxiv.org/html/2512.05693v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：HiMoE-VLA整体框架。左侧蓝色部分为基于PaliGemma初始化的VLM主干，用于处理图像和文本；右侧橙色部分为提出的动作模块，其核心是分层混合专家（HiMoE），负责处理机器人状态和含噪动作，并生成最终的动作输出。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉-语言模块</strong>：采用与π0相同的PaliGemma模型，结合SigLIP视觉编码器和Gemma语言模型。关键设计是从语言模型层提取中间键值（KV）表示，供动作模块进行跨注意力操作，这比仅使用最后一层提供了更强的条件信号。推理时使用KV缓存加速。</li>
<li><strong>动作模块与分层MoE</strong>：动作模块的核心是HiMoE架构。首先，将异构的动作（如末端执行器增量或关节角度命令）和状态映射到一个统一的24维向量表示中，并通过轻量级MLP处理。HiMoE由两种专家模块与标准Transformer块交错组成。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.05693v1/x2.png" alt="HiMoE详细结构"></p>
<blockquote>
<p><strong>图2</strong>：分层混合专家（HiMoE）的详细结构。架构遵循分层设计：边界层的动作空间MoE（AS-MoE）专门处理动作空间变化，相邻层的异构平衡MoE（HB-MoE）处理更广泛的异构性，中央的Transformer块则作为共享层进行跨领域知识整合。</p>
</blockquote>
<pre><code>*   **动作空间MoE**：位于浅层，专门处理不同动作空间（如关节空间vs.末端执行器空间）的差异。
*   **异构平衡MoE**：位于AS-MoE的相邻层，负责自适应地处理更广泛的异构源，如机器人本体特定运动学、传感器配置等，并将其平衡、抽象为共享知识。
*   **共享Transformer块**：位于中间层，将经过专家处理后的异构信号整合为共享表示，促进跨域泛化。
在每一层，专家输出都与从VLM主干提取的中间KV表示进行融合，实现了低层视觉线索与高层语义信息在整个层次中的整合。
</code></pre>
<ol start="3">
<li><strong>训练目标</strong>：总损失函数为流匹配损失与两个正则化项之和：ℒ = ℒ_flow + λ_AS ℒ_AS + λ_HB ℒ_HB。<ul>
<li>**流匹配损失 (ℒ_flow)**：采用流匹配目标来学习动作序列的条件分布，定义了一个从噪声分布到目标动作分布的连续时间轨迹，模型学习一个预测去噪方向的向量场。</li>
<li>**动作空间正则化 (ℒ_AS)**：一种对比学习目标，作用于AS-MoE。它鼓励被路由到同一动作空间令牌的专家之间特征相似，而与其他专家特征相异，从而强化AS-MoE专家对动作空间差异的专门化。</li>
<li>**异构平衡正则化 (ℒ_HB)**：作用于HB-MoE，旨在平衡专家利用率。它使每个专家的期望路由概率与实际路由频率对齐，确保异构输入更均匀地分布 across 专家，防止专家利用不足，促进深层更平衡的抽象。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法（如RDT-1B使用统一动作空间但缺乏处理其内部异构性的机制，HPT使用数据集特定的头但限制了跨数据集迁移）相比，HiMoE-VLA的创新在于引入了<strong>分层MoE设计</strong>，显式地将动作空间差异（通过AS-MoE）与更广泛的异构性（通过HB-MoE）解耦处理，并通过共享层和专门的正则化目标（AS-Reg, HB-Reg）将它们逐步抽象、整合为可迁移的共享知识表示。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：模型在混合了Open X-Embodiment子集（22.5M帧）和公开ALOHA数据集（1.6M帧）的大规模数据上预训练。评估涵盖：</p>
<ul>
<li><strong>仿真基准</strong>：CALVIN（<code>D→D</code>设置）和LIBERO（四个任务套件：Spatial, Object, Goal, Long）。</li>
<li><strong>真实机器人平台</strong>：xArm7单臂机器人和ALOHA双臂机器人，各执行三项分解为子阶段的操纵任务，并测试了对干扰物和新物体的泛化能力。</li>
<li><strong>对比基线</strong>：包括Octo、OpenVLA、RDT-1B、DeeR、MDT、π0、Diffusion Policy、SpatialVLA、OpenVLA-OFT、UniVLA、CogACT等。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>CALVIN仿真</strong>：HiMoE-VLA在连续完成1到5个任务的平均数量上达到3.967，优于所有基线，尤其在长序列任务上表现出更强的鲁棒性。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.05693v1/x4.png" alt="CALVIN结果"></p>
<blockquote>
<p><strong>表1</strong>：CALVIN <code>D→D</code>设置下的任务性能。数值为连续完成1-5个任务的平均计数（越高越好）。HiMoE-VLA取得了最佳的综合性能（Sum. = 3.967）。</p>
</blockquote>
<ol start="2">
<li><strong>LIBERO仿真</strong>：HiMoE-VLA在四个任务套件上的平均成功率达到97.8%，超越了之前的SOTA方法OpenVLA-OFT（97.1%），在所有套件上均取得一致提升。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.05693v1/x5.png" alt="LIBERO结果"></p>
<blockquote>
<p><strong>表2</strong>：LIBERO四个任务套件的性能。数值为每任务50次演示的平均成功率（%）。HiMoE-VLA以97.8%的总平均成功率成为新的SOTA。</p>
</blockquote>
<ol start="3">
<li><strong>xArm7真实机器人</strong>：在三个单臂任务上，HiMoE-VLA整体平均成功率达75.0%，显著优于π0（62.5%）和CogACT（61.5%）。在包含干扰物和新物体的泛化测试中，也以67.6%的平均成功率领先。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.05693v1/x6.png" alt="xArm7真实结果"></p>
<blockquote>
<p><strong>表3</strong>：xArm7单臂机器人三项任务的真实世界评估。HiMoE-VLA在所有任务子阶段和整体平均上均取得最佳性能。</p>
</blockquote>
<ol start="4">
<li><strong>ALOHA真实机器人</strong>：在三个双臂任务上，HiMoE-VLA整体平均成功率达63.7%，优于π0（54.2%）和RDT-1B（47.5%）。在泛化测试中同样表现最佳。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.05693v1/x7.png" alt="ALOHA真实结果"></p>
<blockquote>
<p><strong>表4</strong>：ALOHA双臂机器人三项任务的真实世界评估。HiMoE-VLA在协调性要求高的任务（如Fold-Shorts）上提升显著。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.05693v1/x8.png" alt="泛化结果"></p>
<blockquote>
<p><strong>表5</strong>：单臂和双臂机器人在干扰物和新物体场景下的泛化评估。HiMoE-VLA在两种场景下均展示了最强的泛化能力。</p>
</blockquote>
<ol start="5">
<li><strong>消融实验</strong>：论文中的表6（b）展示了关键组件的贡献。移除AS-MoE或HB-MoE均会导致性能显著下降，验证了分层设计的必要性。同时使用AS-Reg和HB-Reg能取得最佳性能，表明两个正则化目标对于分别促进专家专门化和平衡抽象至关重要。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.05693v1/x3.png" alt="定性结果"></p>
<blockquote>
<p><strong>图3</strong>：在（左）单臂xArm7和（右）双臂ALOHA机器人上的真实世界执行定性示例。快照涵盖了如<code>Fruit-to-Plate</code>、<code>Block-on-Block</code>等任务的代表性阶段，直观展示了模型的实际操控能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个专门为处理异构机器人数据设计的VLA新框架HiMoE-VLA，其核心是<strong>分层混合专家架构</strong>，能够显式解耦动作空间差异与更广泛的异构性，并将它们逐步抽象为共享知识表示。</li>
<li>引入了<strong>两种针对性的正则化目标</strong>：动作空间正则化（AS-Reg）强化浅层专家对动作空间的专门化；异构平衡正则化（HB-Reg）促进深层专家对广泛异构性的平衡抽象，两者共同提升了模型的专门化与泛化能力。</li>
<li>在仿真与真实世界的多种机器人平台和任务上实现了<strong>最先进的性能</strong>，并展示了优异的跨本体、跨任务以及面对新物体、新环境的泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，MoE架构会引入额外的计算开销，且动态路由机制在实时控制中的延迟需要仔细优化。此外，方法主要针对低层次连续动作生成，如何与更高层次的规划或推理结合是未来方向。</p>
<p><strong>启示</strong>：这项工作表明，针对机器人数据固有的、多层次的异构性进行显式的架构设计是构建通用机器人基础模型的有效途径。分层处理和专门的正则化为处理复杂、多样化的真实世界机器人数据提供了新思路。未来的研究可以探索如何将这种分层抽象的思想扩展到更长的任务层次、结合不同的模态，或应用于更广泛的具身智能场景。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出HiMoE-VLA框架，旨在解决机器人示范数据在体现形式、动作空间、传感器配置等方面的高度异质性导致的模型泛化难题。其核心创新是设计了分层混合专家架构，通过自适应处理多源异质性，并逐步将其抽象为共享知识表示。实验表明，该模型在仿真与真实机器人平台上均优于现有视觉-语言-动作基线，实现了更高的准确性与跨机器人及动作空间的鲁棒泛化。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.05693" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>