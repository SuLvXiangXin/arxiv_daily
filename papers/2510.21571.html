<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.21571" target="_blank" rel="noreferrer">2510.21571</a></span>
        <span>作者: Baining Guo Team</span>
        <span>日期: 2025-10-24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，机器人操作领域的视觉-语言-动作模型预训练主要依赖于在实验室环境中通过人类遥操作收集的数据。尽管这类数据非常宝贵，但其高昂的采集成本严重限制了数据规模以及技能、物体类别和场景的多样性。因此，现有的VLA数据集在数量、多样性以及表征真实世界任务复杂性方面，都远落后于互联网规模的语言和视觉语言数据。与此同时，网络上存在大量真实的人类活动视频，包含了丰富的日常动作和与多样化环境的物理交互示例。然而，这些视频是非结构化的：它们未经编排和分割，长度和任务粒度不一，包含噪声和无关动作，且缺乏语言指令和3D动作标签。</p>
<p>本文针对的核心痛点是：能否将这些非结构化的视频，转化为与现有机器人VLA训练数据完全对齐的数据格式？本文首次对此给出了肯定的答案。其核心思路是将人类手视为灵巧的机器人末端执行器，通过开发一个全自动的整体人类活动分析框架，将无脚本的真实人类视频转化为结构化的VLA数据，并利用这些数据预训练一个灵巧手VLA模型，该模型展现出强大的零样本能力，并能通过少量真实机器人数据进行高效微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的方法包含两个主要部分：1）将人类手视频转化为VLA数据的全自动框架；2）用于灵巧操作的VLA模型架构与训练策略。</p>
<p><img src="https://arxiv.org/html/2510.21571v1/x2.png" alt="方法框架总览"></p>
<blockquote>
<p><strong>图2</strong>：整体人类活动分析框架概述。该框架通过三个阶段将无脚本的真实人类视频转化为与典型机器人数据对齐的VLA轨迹：(a) 3D运动标注，重建度量尺度的3D手部和相机轨迹；(b) 原子动作分割，将视频划分为原子级别的片段；(c) 指令标注，使用GPT为每个片段标注动作指令。</p>
</blockquote>
<p><strong>1. 数据转换框架</strong>：<br>该框架分为三个阶段，输入为任意长度的原始人类手部活动视频，输出为多个对齐的VLA轨迹。</p>
<ul>
<li><strong>3D运动标注</strong>：首先判断相机是静态还是移动的。然后估计相机内参，并对视频进行去畸变。接着，使用HaWoR进行每帧的相机空间3D手部重建（包含手腕6D位姿和基于MANO模型的关节角）。对于移动相机，使用改进版的MegaSAM进行相机位姿跟踪。结合手部重建和度量尺度的相机位姿，得到世界空间的3D手部序列，并经过平滑和去噪处理。</li>
<li><strong>原子动作分割</strong>：为了实现与机器人数据粒度的对齐，本文提出了一种基于3D手部运动速度的简单而有效的分割算法。核心思想是：在动作转换时，人手在3D空间中的速度通常会出现变化，最小值常表示动作切换。算法检测世界空间中手腕速度的局部最小值，并将其作为切割点。左右手独立分割，每个片段至少捕获一只手的一个原子动作。</li>
<li><strong>指令标注</strong>：对于每个分割出的视频片段，均匀采样8帧，并在每帧上叠加从当前帧到片段结束的手掌世界空间轨迹投影（作为视觉提示）。将这些带轨迹的帧输入GPT-4，提示其以命令形式描述指定手的动作，并将缺乏语义意义的片段标记为“N/A”。</li>
</ul>
<p><strong>2. VLA模型设计</strong>：</p>
<p><img src="https://arxiv.org/html/2510.21571v1/x3.png" alt="VLA模型架构"></p>
<blockquote>
<p><strong>图3</strong>：VLA模型架构。它由一个VLM主干和一个扩散动作专家组成。VLM接收视觉和语言指令以及相机视场角，输出一个认知特征来指导动作专家预测未来的动作块。动作专家额外接收末端执行器的当前状态和有效动作掩码，通过因果注意力进行迭代动作去噪。</p>
</blockquote>
<ul>
<li><strong>整体架构</strong>：模型由VLM主干（PaliGemma-2）和扩散动作专家（基于DiT-Base的Diffusion Transformer）组成。VLM接收图像、语言指令和相机视场角（FoV）信息，并输出一个可学习的“认知”令牌特征，作为动作专家的条件。</li>
<li><strong>动作空间</strong>：模型在相机坐标系中预测手部动作。动作定义为连续帧间手腕的相对平移、旋转（欧拉角）以及15个手部关节的局部欧拉角，左右手拼接成一个102维的向量。</li>
<li><strong>统一单/双手动作预测</strong>：通过设计动作掩码（0或1）来统一处理预训练数据中单/双手动作并存的情况。掩码与噪声动作拼接后输入动作专家，当掩码为0时，对应动作被置零并从损失计算中排除。</li>
<li><strong>因果动作去噪</strong>：为了解决许多原子动作片段短（约1秒）导致预测块超出片段末尾的问题，本文在动作去噪中采用了因果注意力机制。这确保了每个动作步骤的令牌只关注其之前的动作，防止零填充位置影响前面的预测，这些填充位置也通过掩码被排除在损失之外。</li>
<li><strong>训练与微调</strong>：首先在构建的人类手VLA数据集（约100万个片段，2600万帧）上进行预训练，应用了轨迹感知的数据增强。随后，可将模型在机器人数据上微调。通过将机器人末端执行器6D位姿映射到人类手腕运动，并将机器人关节映射到最接近的人类手部关节，实现了动作空间的对齐。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与平台</strong>：预训练数据来自Ego4D、Epic-Kitchen、EgoExo4D和SSV2的原始视频，经本文框架处理后得到Hand-VLA数据集。机器人实验使用Dora-1灵巧手进行微调和评估。</p>
<p><strong>对比方法</strong>：在零样本人类视频动作预测任务中，对比了使用数据集原始动作标注训练、使用固定长度分割+GPT标注、以及不使用轨迹可视化提示的变体。在机器人实验上，对比了从零开始训练、使用其他人类视频预训练方法（如学习潜在动作）以及本文的预训练+微调方法。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>数据多样性分析</strong>：</li>
</ol>
<p><img src="https://arxiv.org/html/2510.21571v1/x4.png" alt="视觉多样性分析"></p>
<blockquote>
<p><strong>图4</strong>：VLA数据集的视觉多样性分析。(a) 随着片段数量增加，各数据集图像特征与OpenImages数据集的平均最大余弦相似度。本文的Hand-VLA数据集（橙色）在相同数据量下覆盖了更广泛的真实世界场景。<br>    (b) 与Open-X Embodiment (OXE) 机器人数据集和另一个使用受控环境人类视频的VLA数据集（H-VLA）的对比。本文数据集的多样性显著更高。</p>
</blockquote>
<ol start="2">
<li><strong>零样本动作预测</strong>：</li>
</ol>
<p><img src="https://arxiv.org/html/2510.21571v1/x5.png" alt="零样本预测结果"></p>
<blockquote>
<p><strong>图5</strong>：在完全未见过的EgoExo4D视频上的零样本动作预测定性结果。本文模型（Ours）能够根据语言指令（如“拿起锤子”）生成合理的手部运动序列，而基线方法（Baseline）失败。</p>
</blockquote>
<ol start="3">
<li><strong>机器人操作实验</strong>：</li>
</ol>
<p><img src="https://arxiv.org/html/2510.21571v1/x6.png" alt="机器人微调实验"></p>
<blockquote>
<p><strong>图6</strong>：真实机器人灵巧操作实验结果。在“拿起XX”和“放置XX到YY”两类任务上，本文方法（Ours (PT+FT)）经过预训练和微调后，对训练过的物体和未见过的物体都取得了最高的成功率（例如，在Novel Objects上达到73.3%和80%），显著优于从零训练（Training from Scratch）和使用潜在动作预训练（Latent-Action PT）的方法。</p>
</blockquote>
<ol start="4">
<li><strong>缩放行为</strong>：</li>
</ol>
<p><img src="https://arxiv.org/html/2510.21571v1/x7.png" alt="数据规模缩放曲线"></p>
<blockquote>
<p><strong>图7</strong>：预训练数据规模对模型零样本动作预测性能（使用DTW距离衡量，越低越好）的影响。性能随着数据量增加而持续提升，未出现饱和，表明了该方法的可扩展性。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>数据构建策略</strong>：使用本文的原子动作分割和轨迹可视化提示，相比使用原始数据标注或固定长度分割，能带来显著的性能提升。</li>
<li><strong>模型设计</strong>：因果注意力机制对于处理短动作片段至关重要；提供动作掩码对于统一处理单/双手动作是有效的。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了首个将无标注、非结构化的真实人类活动视频全自动转换为与机器人VLA数据对齐格式的框架，解决了数据规模与多样性的瓶颈。</li>
<li>构建了一个大规模、多样化的灵巧手VLA预训练数据集（Hand-VLA），包含100万个片段，覆盖了广泛的物体、技能和真实环境。</li>
<li>设计了一个统一的灵巧手VLA模型，并证明其具备强大的零样本泛化能力，且通过少量机器人数据微调即可大幅提升真实世界的任务成功率和对新物体的泛化能力。实验还展示了模型性能随预训练数据规模扩展的积极趋势。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在将预训练模型微调到机器人时，人类手与机器人手的动作空间映射可能不完美，尽管微调有助于缓解差异，但更优的映射策略是未来的研究方向。</p>
<p><strong>启示</strong>：这项工作为利用海量、低成本的人类视频进行机器人VLA模型预训练铺平了道路。其框架表明，无需对活动或环境施加限制，仅需一个普通摄像头，每个人都可以成为机器人的“老师”。这为迈向真正通用的具身智能提供了一条可扩展的路径。未来可探索更高效的动作空间迁移方法，或结合强化学习进行更复杂的技能习得。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决机器人操作中视觉-语言-动作模型预训练数据稀缺且多样性不足的问题。提出一种全自动的人类活动分析方法，将无标注的真实人类手部活动视频转化为结构化VLA数据，生成原子级动作片段、语言描述及3D手部运动信息。基于此构建了包含100万片段、2600万帧的大规模手部VLA数据集。预训练后的模型在完全未见过的真实观测中表现出强零样本能力，经少量真实机器人数据微调后，显著提升了任务成功率和对新物体的泛化性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.21571" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>