<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CaRL: Learning Scalable Planning Policies with Simple Rewards - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Machine Learning (cs.LG)</span>
      <h1>CaRL: Learning Scalable Planning Policies with Simple Rewards</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.17838" target="_blank" rel="noreferrer">2504.17838</a></span>
        <span>作者: Jaeger, Bernhard, Dauner, Daniel, Beißwenger, Jens, Gerstenecker, Simon, Chitta, Kashyap, Geiger, Andreas</span>
        <span>日期: 2025/04/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在自动驾驶的“特权规划”任务中，当前主流方法包括基于规则的方法和模仿学习方法。基于规则的方法在常规驾驶中表现良好，但难以应对长尾复杂场景，需要为特定场景编写特殊规则，可扩展性差。模仿学习方法虽能随数据扩展，但由于开环训练与闭环推理间的分布偏移问题，其性能通常不及基于规则或混合方法。强化学习作为闭环训练的一种，是前景广阔的替代方案，但其关键挑战在于设计合适的奖励函数。早期研究使用简单的奖励（如最大化前进速度并在违规时终止），但经验表明这在存在其他动态参与者的复杂环境中难以提供足够的监督。因此，近期方法倾向于使用复杂的、由多个奖励项（如速度、位置、方向）加和而成的“塑形”奖励，以提供更密集的反馈。然而，这种设计存在显著局限：需要仔细权衡各项奖励、可能引入局部最优、智能体可能利用奖励漏洞，并且许多奖励依赖于次优的规则规划器来定义“理想”状态，从而限制了性能上限。</p>
<p>本文针对复杂奖励函数阻碍强化学习可扩展性的具体痛点，提出了一个全新的奖励设计视角：摒弃多目标权衡的复杂奖励，转而优化一个单一且直观的目标——路线完成度。本文的核心思路是：设计一个以路线完成度为核心、通过终止或乘性惩罚处理违规的简单奖励，该设计能与批量大小良好协同扩展，从而支持通过大规模并行数据收集进行高效训练，最终学习到高性能的规划策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的核心是一个新颖的奖励函数设计，并围绕其构建了一套可扩展的PPO训练框架。</p>
<p><strong>整体框架与核心模块</strong>：方法基于PPO算法，输入为鸟瞰图语义分割，输出车辆动作。其创新主要在于奖励函数、超参数配置和规模化训练策略。</p>
<p><strong>核心模块1：简单奖励函数设计</strong>。奖励函数定义为：<br><code>r_t = RC_t * (∏ p_t) - T</code><br>其中，<code>RC_t</code>是当前时间步完成的路线百分比；<code>p_t ∈ [0,1)</code>是软惩罚因子，当违反软约束（如超速）时按类型降低奖励；<code>T</code>是终止惩罚，仅在情节结束时应用（如碰撞或闯红灯时<code>T=1</code>）。任何主要违规（硬约束）都会直接终止情节。该设计的原理是：1）可获得的总奖励是有限的（最多100点路线完成度），防止无限奖励漏洞；2）奖励仅指定“做什么”（完成路线），而非“如何做”，不依赖任何规则规划器；3）奖励的全局最优解与评估指标（驾驶分数）的全局最优解一致。</p>
<p><img src="https://arxiv.org/html/2504.17838v3/x1.png" alt="Simple rewards scale with mini-batch size."></p>
<blockquote>
<p><strong>图1</strong>：简单奖励随批量大小扩展。典型的驾驶奖励由多个组件构成，当批量增大时，PPO会陷入局部最优，限制了可扩展性。本文提出的基于最大化路线完成的简单奖励则能很好地随批量大小扩展。</p>
</blockquote>
<p><strong>核心模块2：启用更高学习率的超参数</strong>。本文发现，Roach方法在CARLA中使用的PPO超参数（极小学习率、大量离策略更新步数）虽能收敛但效率低下。通过采用PPO在Atari游戏中的默认超参数（更高学习率、更少的离策略步数），在保持其他参数不变的情况下，训练时间减少了10小时，性能提升了11 DS，且方差降低。这为后续规模化训练奠定了基础。</p>
<p><strong>核心模块3：规模化数据训练</strong>。简单奖励的关键优势在于它允许使用更大的批量大小而不会导致性能退化。如表3所示，当批量大小从256增加到1024时，复杂奖励（Roach）的性能从34 DS崩溃至2 DS，而简单奖励（CaRL）的性能则从21 DS提升至38 DS。更大的批量大小使得通过增加并行模拟器数量来高效扩展样本吞吐量成为可能。本文采用类似分布式数据并行（DD-PPO）的方法，将训练扩展到单个8-GPU节点，在CARLA上收集了3亿样本（批量大小16384），在nuPlan上收集了5亿样本。</p>
<p><img src="https://arxiv.org/html/2504.17838v3/x3.png" alt="Method Overview"></p>
<blockquote>
<p><strong>图11</strong>：方法总览。本文提出一种基于路线完成的简单奖励。违规通过终止情节（硬惩罚）或乘性减少奖励（软惩罚）来处理。这种设计使得PPO能够利用大批量进行训练，从而实现通过大规模并行数据收集进行高效扩展。</p>
</blockquote>
<p><strong>创新点体现</strong>：与现有方法相比，创新点具体体现在：1) <strong>奖励设计理念</strong>：从多目标、依赖规则先验的复杂塑形奖励，转变为单一目标、无模型先验的简单稀疏奖励。2) <strong>可扩展性</strong>：揭示了复杂奖励在大批量下导致优化陷入局部最优的问题，并证明了简单奖励能与大批量良好协同，从而解锁了通过数据并行进行高效训练的能力。3) <strong>超参数配置</strong>：将适用于其他领域的稳健PPO超参数成功引入自动驾驶RL训练，提升了训练效率和稳定性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：主要在CARLA模拟器（0.9.15版本）的longest6 v2基准（36条1-2公里路线，包含7类安全关键场景）和nuPlan模拟器的Val14基准（1118个仿真，包含非反应性和反应性交通）上进行评估。</p>
<p><strong>对比的基线方法</strong>：</p>
<ul>
<li><strong>CARLA</strong>：规则方法PDM-Lite；模仿学习方法PlanT；强化学习方法Roach和Think2Drive。</li>
<li><strong>nuPlan</strong>：规则方法PDM-Closed；模仿学习方法PLUTO、PlanTF、Diffusion Planner；人类日志回放（LQR控制）。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>在CARLA longest6 v2上，CaRL取得了64 DS，大幅优于其他RL方法（Roach: 22 DS, Think2Drive: 7 DS），也比模仿学习方法PlanT（62 DS）高出2 DS。CaRL显著减少了与行人（0.01）和车辆（0.36）的碰撞。其推理速度（8 ms/步）也优于规则方法PDM-Lite（18 ms/步）和PlanT（18 ms/步）。</p>
<p><img src="https://arxiv.org/html/2504.17838v3/x4.png" alt="Performance on longest6 v2 (CARLA)"></p>
<blockquote>
<p><strong>图12</strong>：CARLA longest6 v2基准上的性能对比。CaRL作为RL方法取得了最佳性能（64 DS），显著领先于其他RL基线，并与最好的模仿学习方法相当，同时具有更快的推理速度。</p>
</blockquote>
<p>在nuPlan Val14基准上，CaRL在非反应性和反应性交通中分别取得了91.3和90.6的闭环分数（CLS），是所有学习方法中的最佳成绩，且明显优于Diffusion Planner（89.6/82.7）。CaRL的推理速度（14 ms/步）比基线快7-17倍。</p>
<p><strong>消融实验与组件贡献</strong>：</p>
<ol>
<li><strong>奖励设计与批量大小（表3）</strong>：这是核心贡献。使用简单奖励在批量256时性能（21 DS）低于复杂奖励（34 DS），但当批量增至1024时，简单奖励性能跃升至38 DS，而复杂奖励崩溃至2 DS。这证明了简单奖励是实现可扩展训练的关键。</li>
<li><strong>超参数集（表2）</strong>：采用Atari超参数替代原CARLA超参数，使训练时间减少10小时，性能提升11 DS，方差降低。</li>
<li><strong>数据规模（表4）</strong>：将样本量从1000万增至3亿（批量从1024增至16384），带来了33 DS的巨大性能提升（从31 DS到64 DS），验证了大规模训练的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.17838v3/x5.png" alt="Effect of scale."></p>
<blockquote>
<p><strong>图13</strong>：规模化训练的效果。将训练样本从1000万增加到3亿，同时扩大批量大小，带来了巨大的性能提升（DS从31提高到64），证明了简单奖励支持大规模数据并行训练的有效性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了可扩展的简单奖励设计</strong>：以路线完成度为核心，通过硬/软惩罚处理违规，避免了复杂奖励的权衡问题和局部最优，使得PPO算法能够利用大批量进行高效训练。</li>
<li><strong>实现了大规模数据并行训练</strong>：利用简单奖励对大批量的友好特性，将PPO训练成功扩展到数亿样本，在CARLA和nuPlan两个主流仿真平台上显著提升了RL规划器的性能，达到了学习类方法中的领先水平。</li>
<li><strong>提供了可复现的研究基础</strong>：公开了代码，并优化了CARLA仿真速度，为后续RL在自动驾驶规划中的研究提供了实用的工具和基线。</li>
</ol>
<p><strong>论文自身提到的局限性</strong>：</p>
<ol>
<li>研究范围限于城市中低速驾驶（最高80 km/h），未考虑高速公路高速驾驶的特有问题。</li>
<li>工作完全在仿真中进行，未验证从仿真到现实（Sim2Real）的迁移能力，这是RL应用于真实车辆的关键挑战。</li>
<li>训练和评估的场景类型有限（CARLA longest6 v2的7种类型），未覆盖CARLA Leaderboard 2.0提供的更广泛场景。</li>
<li>舒适度惩罚的边界设置基于宽泛假设，生成的驾驶行为在真实车辆中可能不够舒适，需要基于人类数据进一步调优。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>奖励设计哲学</strong>：在复杂任务中，追求稀疏、目标导向的简单奖励，配合大规模训练，可能比精心设计、多目标的稠密奖励更具可扩展性和最终性能潜力。</li>
<li><strong>训练规模化</strong>：RL在复杂领域的成功可能越来越依赖于大规模数据并行训练的能力，奖励函数和优化算法的设计需要充分考虑与大批量训练的兼容性。</li>
<li><strong>跨平台泛化</strong>：CaRL的方法在CARLA和nuPlan两个差异显著的平台上仅需最小适配就取得了成功，表明其核心设计具有一定的通用性，为构建更通用的驾驶策略提供了思路。未来的工作可以沿着Sim2Real和覆盖更复杂、多样化的驾驶场景方向推进。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究自动驾驶特权规划中强化学习奖励设计的可扩展性问题。针对现有复杂奖励函数导致PPO在小批量增大时陷入局部最优、限制训练效率的瓶颈，提出CaRL方法，采用以路线完成度为核心的简单奖励设计，违规时通过终止或乘法惩罚处理。实验表明，该方法使PPO能高效扩展，在CARLA中达到64 DS（longest6 v2基准），显著优于复杂奖励的RL方法；在nuPlan中扩展到5亿样本，Val14基准得分91.3（非反应性）和90.6（反应性），速度比先前工作快一个数量级。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.17838" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>