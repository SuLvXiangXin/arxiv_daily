<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.14383" target="_blank" rel="noreferrer">2508.14383</a></span>
        <span>作者: Na Li Team</span>
        <span>日期: 2025-08-20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>离线模仿学习（IL）旨在仅使用固定的专家演示数据集来模仿专家策略，无需与环境交互进行试错探索，因此被广泛应用于机器人学习任务。目前主流方法包括行为克隆（BC）和基于分布匹配的方法。行为克隆通过监督学习直接匹配策略，但忽略了马尔可夫性质，会遭受复合误差问题：单步的小误差会使智能体进入专家数据集中罕见或未见过的轨迹，从而在几步后放大学习误差。基于分布匹配的方法（如利用DICE工具的离线IL）通过匹配行为策略与专家的状态-动作平稳分布来缓解复合误差。然而，离线IL面临两大关键挑战：一是专家数据有限，导致严重的过拟合和泛化能力差；二是求解min-max优化问题计算复杂，使用神经网络参数化时极不稳定。</p>
<p>本文针对这两个具体痛点，提出了一个新的视角：通过引入一个预训练阶段来学习<strong>动态表征</strong>，该表征源自系统转移动态的分解。核心思路是：首先，理论证明离线IL的最优决策变量位于该表征空间中，从而显著减少下游IL需要学习的参数并缓解优化困难；其次，动态表征可以从具有相同动态的任意数据中学习，允许重用大量非专家数据，从而缓解数据有限的问题。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出一个两阶段算法：预训练阶段从共享动态的所有数据中学习动态表征；主训练阶段仅在表征空间中使用专家数据进行下游模仿学习。</p>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/demo.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：所提算法的示意图。左侧为预训练阶段，使用任意数据（包括专家和非专家数据）学习动态表征φ和μ。右侧为主训练阶段，在学得的表征空间中进行模仿学习优化，仅需更新线性权重ω和θ以及策略π。</p>
</blockquote>
<p><strong>核心模块1：动态表征的定义与理论依据</strong><br>动态表征定义为存在映射 φ: S×A → R^k 和 μ: S → R^k，使得转移概率可分解为：P(s&#39;|s, a) = ⟨φ(s, a), μ(s&#39;) p_n(s&#39;)⟩，其中 p_n 是一个在全状态空间上有支撑的噪声分布。该分解类似于线性MDP，但增加了受对比学习启发的噪声项 p_n，这有助于与离线设置中的密度比学习对齐，并使得学习表征的损失函数易于处理。论文从理论上证明了最优的对偶变量 Q*(s, a)（在分布匹配优化中起关键作用）可以完全由这些动态表征表示，其具体形式为：Q*_{ω,θ}(s, a) = -log ζ(s, a) + φ(s,a)^T ω - log( μ(s)^T θ + (1-γ)ρ(s)/d^exp(s) )，其中ζ是策略比，ω和θ是与表征φ和μ对应的线性权重。这意味着IL的优化可以被约束在低维的表征空间中进行，只需优化ω和θ，而非整个高维函数Q。</p>
<p><strong>核心模块2：预训练阶段——动态表征学习</strong><br>目标是学习满足上述分解的φ和μ。利用从相同动态P收集的任意数据集D（可包含大量非专家数据），构建一个受噪声对比估计启发的、易于处理的损失函数：<br>min_{φ, μ} J_repr(φ, μ) = -2 E_{(s,a,s&#39;)∼D}[φ(s,a)^T μ(s&#39;)] + E_{(s,a)∼D, s_n∼D^exp}[(φ(s,a)^T μ(s_n))^2]。<br>通过最小化该目标，学到的φ和μ能满足动态分解。为提高数值稳定性，额外添加了一个对数概率正则化项。</p>
<p><strong>核心模块3：主训练阶段——表征空间上的模仿学习</strong><br>将表征φ和μ固定，并将Q函数参数化为上述Q*_{ω,θ}的形式。然后，将原始的min-max分布匹配优化问题（公式7）转化为在表征空间中对参数ω, θ和策略π的优化问题（公式16）。优化目标包含两项：一项基于专家数据期望，涉及exp(γP^π Q - Q)；另一项是基于初始状态分布的期望。通过将优化限制在表征空间，极大地简化了min-max问题的求解难度和稳定性。</p>
<p><strong>创新点总结</strong></p>
<ol>
<li><strong>理论创新</strong>：首次证明了从动态分解中导出的表征能够完全表示离线IL的最优解，为利用表征学习提供了理论依据。</li>
<li><strong>算法创新</strong>：提出两阶段框架，将困难的离线IL问题解耦为（可重用数据的）表征学习和（数据高效的）表征空间优化。</li>
<li><strong>数据利用创新</strong>：动态表征学习允许利用任意质量的共享动态数据，打破了现有方法对辅助数据次优性的要求，极大缓解了专家数据稀缺问题。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真环境</strong>：在MuJoCo的四个连续控制任务（HalfCheetah, Hopper, Walker2d, Ant）上进行评估。</li>
<li><strong>真实系统</strong>：在Unitree Go1四足机器人上进行真实世界行走任务学习。</li>
<li><strong>基线方法</strong>：包括行为克隆（BC）、以及多种基于DICE的离线IL方法（ValueDICE, SMODICE, DEMODICE）。</li>
<li><strong>数据设置</strong>：使用有限的专家轨迹（少至1条）进行模仿学习。预训练阶段可使用额外的非专家数据集（如“medium”级别的离线RL数据集）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>少量专家数据下的性能</strong>：在MuJoCo环境中，仅使用1条专家轨迹，本文方法在大多数任务上显著优于所有基线。例如，在HalfCheetah上，本文方法获得约6000的归一化分数，而最强的基线DEMODICE仅约2500。<br><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/halfCheetah.png" alt="性能对比 HalfCheetah"><blockquote>
<p><strong>图2</strong>：HalfCheetah任务上，不同专家轨迹数量下的性能对比。本文方法（红色）在数据极少时（1条轨迹）优势明显。</p>
</blockquote>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/hopper.png" alt="性能对比 Hopper"></p>
<blockquote>
<p><strong>图3</strong>：Hopper任务上的性能对比。本文方法在1条和3条轨迹设置下均表现最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/Ant.png" alt="性能对比 Ant"></p>
<blockquote>
<p><strong>图4</strong>：Ant任务上的性能对比。随着轨迹数增加，所有方法性能提升，本文方法在数据少时仍保持领先。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/walker.png" alt="性能对比 Walker2d"></p>
<blockquote>
<p><strong>图5</strong>：Walker2d任务上的性能对比。趋势与其他环境一致。</p>
</blockquote>
<ol start="2">
<li><strong>定性结果</strong>：可视化学习到的策略轨迹与专家轨迹对比，显示本文方法能很好地模仿专家行为。<br><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/pdf2png/HalfCheetah-traj-1/HalfCheetah-traj-1-1.png" alt="定性结果 HalfCheetah"><blockquote>
<p><strong>图6</strong>：HalfCheetah任务上，学习策略（红色）与专家（蓝色）的状态轨迹对比，吻合度很高。</p>
</blockquote>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/pdf2png/Hopper-traj-1/Hopper-traj-1-1.png" alt="定性结果 Hopper"></p>
<blockquote>
<p><strong>图7</strong>：Hopper任务的轨迹对比。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/pdf2png/Walker2d-traj-1/Walker2d-traj-1-1.png" alt="定性结果 Walker2d"></p>
<blockquote>
<p><strong>图8</strong>：Walker2d任务的轨迹对比。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/pdf2png/Ant-traj-3/Ant-traj-3-1.png" alt="定性结果 Ant"></p>
<blockquote>
<p><strong>图9</strong>：Ant任务的轨迹对比。</p>
</blockquote>
<ol start="3">
<li><strong>利用非专家数据预训练的效果</strong>：实验表明，使用“medium”级别非专家数据进行预训练，能进一步提升在少量专家数据上的IL性能。<br><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/pdf2png/medium-HalfCheetah-traj-1/medium-HalfCheetah-traj-1-1.png" alt="非专家数据预训练效果 HalfCheetah"><blockquote>
<p><strong>图10</strong>：使用非专家数据预训练后，HalfCheetah任务上学得策略与专家轨迹对比。</p>
</blockquote>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/pdf2png/medium-Hopper-traj-1/medium-Hopper-traj-1-1.png" alt="非专家数据预训练效果 Hopper"></p>
<blockquote>
<p><strong>图11</strong>：Hopper任务上使用非专家数据预训练后的对比。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/pdf2png/medium-Walker2d-traj-1/medium-Walker2d-traj-1-1.png" alt="非专家数据预训练效果 Walker2d"></p>
<blockquote>
<p><strong>图12</strong>：Walker2d任务上使用非专家数据预训练后的对比。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/pdf2png/medium-Ant-traj-3/medium-Ant-traj-3-1.png" alt="非专家数据预训练效果 Ant"></p>
<blockquote>
<p><strong>图13</strong>：Ant任务上使用非专家数据预训练后的对比。</p>
</blockquote>
<ol start="4">
<li><strong>Sim-to-Real 迁移</strong>：在真实四足机器人上，仅使用约1000秒的真实硬件演示数据，结合在仿真器中预训练的动态表征，成功学会了行走。<br><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/go2.jpg" alt="真实四足机器人"><blockquote>
<p><strong>图14</strong>：Unitree Go1四足机器人实验平台。</p>
</blockquote>
</li>
</ol>
<p><img src="https://arxiv.org/html/2508.14383v1/imitation_learning/fig/simulator.png" alt="Sim-to-Real 框架"></p>
<blockquote>
<p><strong>图15</strong>：Sim-to-Real实验设置示意图。在仿真器中用任意数据预训练动态表征，然后仅用少量真实演示在表征空间学习策略。</p>
</blockquote>
<p><strong>消融实验</strong>：<br>论文进行了消融研究，验证了预训练阶段和将优化约束在表征空间这两个核心组件的贡献。移除预训练（即随机初始化表征）或不在表征空间优化（即直接优化神经网络Q函数）都会导致性能显著下降，尤其是在专家数据极少的情况下。这证实了所提两阶段设计对于解决数据有限和优化不稳定问题的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>动态表征</strong>的理论框架，证明了其对于离线模仿学习最优解的表征能力，为数据高效的IL提供了新方向。</li>
<li>设计了一个<strong>两阶段算法</strong>，通过从任意数据预训练动态表征，随后在低维表征空间进行模仿学习，有效解决了专家数据稀缺和min-max优化不稳定的双重挑战。</li>
<li>在仿真和真实机器人实验中验证了方法的有效性，展示了其仅用<strong>单条专家轨迹</strong>完成模仿学习的能力，以及<strong>Sim-to-Real迁移</strong>的潜力。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性主要在于假设：动态表征的学习依赖于“所有数据共享相同动态P”的假设。在实际复杂环境中，动态可能发生变化或难以完全一致。此外，预训练阶段仍然需要收集大量的非专家数据。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>探索更高效的动态表征学习方法</strong>：如何从更少或更杂乱的数据中学习更鲁棒、更紧凑的动态表征。</li>
<li><strong>处理动态变化或部分可观测的环境</strong>：将动态表征学习扩展到动态非平稳或部分可观测的设定中。</li>
<li><strong>与模型基RL结合</strong>：学得的动态表征本质上是转移动态的分解，可自然用于构建可解释的或可迁移的世界模型，进而与模型基强化学习范式结合。</li>
<li><strong>扩展到更广泛的示教数据</strong>：进一步探索如何利用质量极低、甚至包含对抗性噪声的任意示教数据。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对离线模仿学习在有限专家数据下的性能瓶颈问题，提出通过预训练动力学表示来增强学习效果。该方法基于转移动力学分解学习表示，可从任意同动态的非专家数据中预训练，减少下游学习参数，并采用噪声对比估计启发的损失函数。实验表明，在MuJoCo环境中仅需单个专家轨迹即可成功模仿策略；在真实四足机器人上，能利用模拟器预训练的动力学表示，从少量真实世界演示中学习行走任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.14383" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>