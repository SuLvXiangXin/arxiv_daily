<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Embodied Perception for Test-time Grasping Detection Adaptation with Knowledge Infusion - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Embodied Perception for Test-time Grasping Detection Adaptation with Knowledge Infusion</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.04795" target="_blank" rel="noreferrer">2504.04795</a></span>
        <span>作者: Liu, Jin, Xie, Jialong, Xiao, Leibing, Wang, Chaoqun, Zhou, Fengyu</span>
        <span>日期: 2025/04/07</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人抓取检测的主流方法是基于深度学习网络，通过海量人工标注数据进行训练。这些方法在面对未预见的新场景时，性能会严重下降，且数据收集过程耗时费力，阻碍了机器人的广泛部署。测试时适应技术旨在利用在线未标注的测试样本来适应新环境，为解决上述问题提供了思路。然而，现有的测试时适应技术主要针对图像分类任务，其预测是物体级别的，而抓取检测需要像素级的预测。此外，这些方法通常局限于单一视角，未能充分利用机器人的具身能力来获取更合适的样本。</p>
<p>本文针对抓取检测任务中测试时适应的具体痛点，提出了一种新的具身感知视角。核心思路是：赋予机器人主动探索环境的能力，利用其物理参数（具身参数）来评估并保留高质量的抓取样本（生成伪标签），同时构建一个知识库来引导高效的探索起点，从而在测试时无人工干预地持续适应抓取检测网络。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的具身测试时适应框架主要由三个模块构成：抓取知识检索模块、具身感知模块和网络优化模块。整体流程是：机器人首先通过知识检索模块，根据当前观测图像从知识池中检索历史知识，确定一个最优的初始观测位置；然后，具身感知模块引导机器人沿着预定义的视点轨迹进行主动探索，利用具身评估标准筛选出高质量的抓取样本；最后，网络优化模块利用收集到的高质量样本（及其伪标签）同时优化抓取检测网络和知识检索网络，使它们适应当前场景。</p>
<p><img src="https://arxiv.org/html/2504.04795v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：提出的具身测试时适应框架概览。机器人首先检索与最优候选视点相关的历史抓取知识；然后基于具身评估指标主动探索不同视点并保留最优样本；最后，基于收集的样本分别优化知识检索网络和抓取检测网络。</p>
</blockquote>
<p><strong>1. 抓取知识检索模块</strong>：该模块旨在为机器人提供高效的探索起点。首先，使用预训练的ResNet视觉编码器从捕获的RGB图像中提取语义特征。然后，计算该特征与知识池中所有压缩知识向量嵌入的余弦相似度。如果所有相似度值都低于0.95，则认为物体是全新的；否则，选择最相关的知识嵌入。接着，一个由两层全连接层构成的观察预测网络（OPNet）以该知识嵌入为输入，预测在预定义的粗粒度观测位置组中的初始观测位置（即具有最高概率值的那个）。</p>
<p><strong>2. 具身感知模块</strong>：这是框架的核心，使机器人能够主动探索并评估抓取姿态。该模块的流程见算法1，其输入包括机器人的具身参数、预定义的视点轨迹和观测位置组、抓取检测网络GNet等。</p>
<ul>
<li><strong>预定义视点</strong>：定义了一条包含V个细粒度离散视点的轨迹，并将其组织成K个粗粒度观测位置组。每个观测位置包含多个视点。探索策略遵循“由粗到细”的原则。</li>
<li><strong>抓取检测</strong>：在每一个探索视点v，使用冻结的、预训练的抓取检测网络GNet，输入该视点的RGB和深度图像，预测一组候选抓取矩形。每个抓取矩形g由中心点坐标(x, y)、夹爪开口宽度w、绕Z轴旋转角度φ和质量评分q定义。</li>
<li><strong>图像分割</strong>：为了辅助后续精细的质量评估，使用Segment Anything (SAM)模型为每个视点下的图像生成物体掩码，并利用OpenCV工具为每个物体构建凸包。</li>
<li><strong>具身参数</strong>：定义了三个关键的机器人物理约束用于筛选抓取姿态：1) 预测的夹爪宽度应在实际夹爪最大宽度的[1/10, 1/4]范围内；2) 检测到的物体到夹爪末端的距离应在相机深度检测的有效操作范围内；3) 抓取点的三维坐标应在机器人可达的工作空间内。</li>
<li><strong>质量评估</strong>：在满足上述具身参数约束的基础上，提出进一步的质量评估标准以生成伪标签。核心标准是：在当前视点下，预测的最佳抓取矩形的中心点必须落在目标物体的掩码（凸包）内。满足此条件的抓取姿态及其对应的图像将被保留为高质量样本。</li>
</ul>
<p><strong>3. 网络优化模块</strong>：利用具身感知模块收集到的高质量样本及其伪标签（即被评估为成功的抓取矩形），通过平滑L1损失同时微调抓取检测网络GNet和知识检索网络OPNet的参数，使它们适应测试场景。</p>
<p>与现有方法相比，本文的创新点具体体现在：1) 将测试时适应与机器人的具身感知能力相结合，通过物理约束和主动探索来生成可靠的伪标签，解决了像素级预测任务的适应难题；2) 引入了知识检索机制，利用历史经验指导探索，提升了适应效率。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在一个配备有RealSense D435i相机和Robotiq 2F-85自适应夹爪的真实机器人平台上进行。使用了包含YCB物体和自制家庭物品的数据集进行评估。对比的基线方法包括传统的抓取检测方法（如GPD、GGCNN）以及未经过适应或采用其他测试时适应策略（如TENT、SHOT）的GraspNet网络。</p>
<p><img src="https://arxiv.org/html/2504.04795v1/x5.png" alt="定性结果"></p>
<blockquote>
<p><strong>图5</strong>：在真实机器人上抓取新物体的定性结果。绿色框表示成功抓取，红色框表示失败。本文方法（Ours）在多种新物体上均能成功抓取。</p>
</blockquote>
<p>关键实验结果如下：在YCB物体上的抓取成功率，经过本文方法适应后，从基础GraspNet的76.8%提升到了92.3%。在家庭物品上的抓取成功率也从73.3%提升至90.0%。这表明所提框架能显著提升预训练模型在新场景中的泛化性能。</p>
<p><img src="https://arxiv.org/html/2504.04795v1/x6.png" alt="定量对比"></p>
<blockquote>
<p><strong>图6</strong>：在YCB物体和家庭物品数据集上的抓取成功率定量对比。本文方法（Ours）显著优于所有基线方法。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.04795v1/x7.png" alt="适应过程"></p>
<blockquote>
<p><strong>图7</strong>：测试时适应过程中的性能变化曲线。随着探索的进行和网络的优化，抓取成功率逐步提升并最终稳定在一个高水平。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.04795v1/x8.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融实验结果。分别移除了知识检索（w/o KR）和具身评估（w/o EA）模块，性能均出现下降，证明了这两个组件的必要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.04795v1/x9.png" alt="知识检索分析"></p>
<blockquote>
<p><strong>图9</strong>：知识检索模块的分析。展示了检索到的相似物体示例，以及在不同相似度阈值下探索效率（所需视点数量）的变化。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.04795v1/x10.png" alt="视点轨迹"></p>
<blockquote>
<p><strong>图10</strong>：机器人探索过程中的视点轨迹可视化。机器人从知识检索建议的初始观测位置开始，进行有序的探索。</p>
</blockquote>
<p>消融实验总结了每个组件的贡献：移除知识检索模块后，成功率下降，且探索所需的平均视点数增加；移除具身评估标准（仅依赖网络原始质量分）后，成功率大幅下降，证明了基于物理约束的评估对于生成可靠伪标签的关键作用。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了一个用于机器人抓取检测的具身测试时适应框架，使机器人能在无人工干预下，通过主动探索持续适应未知环境；2) 提出了一种结合机器人物理参数的具身评估标准，能有效筛选高质量的抓取样本用于网络优化；3) 设计了一个知识检索模块，利用历史经验提升探索的效率和起点质量。</p>
<p>论文自身提到的局限性包括：框架的性能在一定程度上依赖于预定义的视点轨迹和观测位置组；知识池的初始化需要一定量的先验数据。</p>
<p>本工作对后续研究的启示包括：将具身感知与测试时适应结合的思路可以扩展到更广泛的机器人感知与决策任务中，如物体分割、位姿估计等；如何动态生成或优化探索轨迹，以及如何构建和更新更高效、更通用的知识库，是值得深入探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人抓取检测在未知场景中泛化性能下降、依赖大量人工标注数据的问题，提出了一种具身测试时适应框架。核心方法包括：利用机器人探索能力，基于操作能力设计具身评估标准以筛选高质量样本；构建知识库提供初始最优视点上下文，提升探索效率；在测试时利用未标注数据持续自适应抓取检测网络。真实机器人实验表明，该框架能有效提升抓取技能在未见环境中的适应能力与泛化性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.04795" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>