<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.09109" target="_blank" rel="noreferrer">2505.09109</a></span>
        <span>作者: Chen, Yuxing, Xiao, Bowen, Wang, He</span>
        <span>日期: 2025/05/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人衣物操控领域，由于衣物的可变形特性，相关任务极具挑战性。近年来，数据驱动的学习方法，特别是模仿学习，已成为获取机器人技能的主要范式。然而，学习到的策略要能泛化到未见过的环境和物体，仍然受限于大规模、多样化、高质量演示数据的稀缺。</p>
<p>从合成数据中学习已成为一种高效的机器人学习途径，现有的衣物数据集和模拟环境可以生成操控数据。但当前方法存在两个主要局限：1) 有限的服装资产和缺乏详细标注：现有数据集通常只包含少量服装网格且缺乏丰富的标注，这限制了泛化性能的上限，同时也增加了在仿真中生成高质量演示数据的额外负担。2) 对错误恢复的处理有限：衣物操控是涉及复杂可变形物体动力学的长程任务。相比于之前主导的开环方法，闭环控制提供了在失败后重试的潜力。然而，如果训练数据只包含完美的演示，每一步的小错误会累积并可能导致衣物进入先前未见的状态，最终导致任务失败，这对学习鲁棒策略构成了重大挑战。</p>
<p>本文针对高质量合成数据稀缺的痛点，提出了一个名为FoldNet的合成衣物数据集。其核心思路是：首先通过一个基于关键点的框架，自动化生成大量多样化的高质量服装资产（几何与纹理）并附带语义关键点标注；然后利用这些资产，在仿真中通过基于关键点的策略生成高质量的闭环折叠演示，并引入一种名为KG-DAgger的关键点门控数据聚合策略，专门生成错误恢复演示，从而大幅提升策略的鲁棒性和泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>FoldNet的整体框架包含两个核心部分：高质量的服装资产合成，以及基于这些资产的闭环演示生成与策略学习。</p>
<p><img src="https://arxiv.org/html/2505.09109v4/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：FoldNet数据集概览。它提供：(1) 大量带关键点标注的合成服装资产，(2) 高质量的折叠演示，(3) 基于关键点的错误恢复演示。利用这些资产，FoldNet支持多种下游任务，包括 (a) 关键点检测，(b) 仿真中的折叠，(c) 真实世界中的折叠。</p>
</blockquote>
<p><strong>1. 服装网格合成</strong><br>资产合成的目标是生成适用于物理仿真和渲染的高质量服装网格。其流程如图2所示，主要包含三个阶段。</p>
<p><img src="https://arxiv.org/html/2505.09109v4/x2.png" alt="服装网格合成流程"></p>
<blockquote>
<p><strong>图2</strong>：服装网格合成流程。通过执行几何生成、纹理生成、结合与过滤，我们可以合成可扩展的高质量服装网格。</p>
</blockquote>
<ul>
<li><strong>几何生成</strong>：采用基于模板的方法，为T恤（长袖/短袖）、背心、连帽衫和裤子四类服装生成几何形状。为每类服装手动指定一组语义关键点（2D坐标）来定义其结构特征。这些关键点具有双重作用：标识衣物上有语义意义的操控点，并隐式定义其形状。确定关键点位置后，使用贝塞尔曲线沿边界连接它们并进行三角剖分，然后启发式地定义网格顶点的z坐标和UV坐标。在此过程中，通过保存关键点索引，自动为生成的网格标注关键点。通过随机化关键点位置，可以高效生成大量不同形状的服装。</li>
<li><strong>纹理生成</strong>：为了自动生成服装纹理，使用预训练的生成模型。首先，使用大语言模型为每类服装生成纹理描述。然后，将此描述作为提示词输入文生图模型。重复此过程可快速生成大量纹理图像。</li>
<li><strong>结合与过滤</strong>：为了增强纹理图像与服装网格之间的一致性，引入了一个额外的过滤步骤。对于每个仅有几何的服装网格，将其与不同的纹理图像结合并渲染结果。然后使用视觉语言模型自动选择最合适的纹理作为该网格的最终纹理。最终生成的服装示例如图3所示。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.09109v4/x3.png" alt="合成服装网格示例"></p>
<blockquote>
<p><strong>图3</strong>：合成服装网格示例。这些静态服装网格可用于后续的物理仿真和策略学习。</p>
</blockquote>
<p><strong>2. 演示生成与策略学习</strong><br>利用生成的服装资产，设计基于关键点的策略在仿真中自动收集演示，然后通过模仿学习训练一个视觉-动作模型。为了提升数据效率和策略鲁棒性，引入了KG-DAgger。</p>
<ul>
<li><strong>基于关键点的演示生成</strong>：为每类服装设计简单有效的基于关键点的策略，以预定义的方式折叠衣物。例如，折叠T恤的策略可能包括：先旋转衣物，然后拖动，将两个袖子向内折叠，最后折叠衬衫底部。每个阶段的初始抓取点和目标放置位置都来源于真实关键点位置，中间位置通过插值获得。得益于资产中的关键点标注，该策略在同一类别的不同形状衣物上是统一的。图4展示了演示生成过程。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.09109v4/x4.png" alt="基于关键点的演示生成"></p>
<blockquote>
<p><strong>图4</strong>：基于关键点的演示生成。整个折叠过程通常可以分为几个阶段，然后在演示生成过程中按顺序执行。</p>
</blockquote>
<ul>
<li><strong>KG-DAgger：错误恢复演示生成</strong>：KG-DAgger是一种DAgger的变体。在第i次迭代中，使用当前模型生成新的轨迹。在此过程中，采用基于关键点的错误恢复策略。这些新生成的轨迹教导模型如何从错误中恢复——特别是模型最容易犯的错误类型——从而持续改进模型性能。在最终部署时，模型在整个轨迹集上进行训练。最终策略是端到端的系统：它不需要显式的关键点检测或错误检测，因为这些能力已被模型隐式学习。图5详细说明了基于关键点的错误恢复策略是如何实现的，并将整个恢复过程分为五个阶段，以区分哪些动作应该被鼓励（赋予权重1）或抑制（赋予权重0）。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.09109v4/x5.png" alt="基于关键点的恢复策略与KG-DAgger伪代码"></p>
<blockquote>
<p><strong>图5</strong>：基于关键点的恢复策略。左图说明了恢复策略的一个示例。恢复数据被纳入数据集并联合用于训练端到端的视觉-动作模型。有了这些数据，模型可以学习当抓取尝试失败时如何重试。右图显示了KG-DAgger的伪代码。</p>
</blockquote>
<ul>
<li><strong>模型训练</strong>：选择Diffusion Policy作为视觉-动作模型，因其模型紧凑、在建模多模态行为和生成连贯动作序列方面表现良好。训练损失是原始扩散损失的修改版。对于一个动作块中的第i个动作，将损失乘以一个系数m_i。如果由于抓取失败，出现了零权重动作，则该动作及之后所有动作的m_i设置为0；否则，m_i为1。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，FoldNet的主要创新体现在：1) <strong>可扩展的服装资产生成</strong>：通过关键点控制几何，并结合生成模型与VLM过滤，自动化生成大量高质量、带标注的多样化服装资产，解决了资产稀缺问题。2) <strong>基于关键点的错误恢复数据合成（KG-DAgger）</strong>：在模仿学习迭代过程中，利用关键点自动检测失败并生成恢复演示，显著提升了闭环策略的鲁棒性，这是对传统仅使用完美演示或简单噪声注入方法的重大改进。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验设计了两项任务来验证方法的有效性：关键点检测和服装折叠。关键点检测任务更容易评估，用于说明生成的服装网格与真实衣物的相似度。服装折叠任务更全面，用于评估生成的演示数据质量。</p>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>仿真环境</strong>：使用PyFlex作为物理模拟器，Blender进行渲染。</li>
<li><strong>资产</strong>：关键点检测任务中，为每类服装生成1500个合成实例用于训练，并手动收集标注了480件T恤、82条裤子、96件背心和96件连帽衫作为真实世界测试集。折叠任务中，为每类服装生成1000个训练实例和100个未见过的测试实例。真实世界测试使用10件未见过的衣物（图8）。</li>
<li><strong>基准方法</strong>：关键点检测中，对比了本文方法（有无VLM过滤）、aRTF和Paint-it。折叠策略学习中，对比了仅使用完美演示（Perfect）、对真实动作加噪并恢复（Noised）以及完整的KG-DAgger方法。</li>
<li><strong>评估指标</strong>：关键点检测使用平均精度（mAP）和平均关键点距离（AKD）。服装折叠在仿真中通过最终折叠网格与目标网格的平均顶点距离判断成功（阈值0.4mm）；在真实世界中由人类专家判断。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.09109v4/x8.png" alt="真实世界折叠实验资产"></p>
<blockquote>
<p><strong>图8</strong>：用于服装折叠真实世界实验的资产。在真实世界实验中，每件衣物折叠两次，并计算平均成功率。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>关键点检测</strong>：如表II所示，在真实图像上，本文方法（带过滤）在平均mAP（47.2）和AKD（15.6）上均优于aRTF和Paint-it。消融实验表明，VLM过滤步骤有效提升了外观质量和检测性能。图6展示了在真实图像上的定性检测结果，图7对比了不同方法生成的网格外观。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.09109v4/x6.png" alt="关键点检测真实图像定性结果"></p>
<blockquote>
<p><strong>图6</strong>：关键点检测在真实图像上的定性结果。该图显示了我们的关键点检测模型在真实图像上的预测输出。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09109v4/x7.png" alt="生成网格示例对比"></p>
<blockquote>
<p><strong>图7</strong>：生成的网格示例对比。与其他纹理生成方法相比，我们的方法生成的纹理通常更合理。</p>
</blockquote>
<ol start="2">
<li><strong>服装折叠</strong>：<ul>
<li><strong>KG-DAgger的有效性</strong>：图9(a)显示，在仿真中，使用包含错误恢复数据的Noised和KG-DAgger方法，其策略成功率（约80%）显著高于仅使用完美演示的Perfect方法（约55%）。KG-DAgger进一步缩小了训练和测试成功率之间的差距。图10直观展示了训练数据包含恢复策略的重要性。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2505.09109v4/x9.png" alt="服装折叠定量结果"></p>
<blockquote>
<p><strong>图9</strong>：服装折叠的定量结果。我们比较了不同模型在仿真和真实世界中服装折叠任务的平均成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09109v4/x10.png" alt="不同演示数据训练模型在仿真中的对比"></p>
<blockquote>
<p><strong>图10</strong>：在仿真中，使用不同演示数据生成方法训练的模型对比。在(a)中，训练数据不包含错误的恢复策略，因此抓取失败会导致分布外情况。在(b)中，训练数据包含恢复策略，允许模型在失败后重试抓取。</p>
</blockquote>
<pre><code>*   **泛化到新折叠策略**：图11表明，通过设计新的基于关键点的折叠规则并生成相应演示，训练出的模型能够成功学习并执行新的折叠策略（如不同的袖子折叠顺序），成功率与原始策略相当。
</code></pre>
<p><img src="https://arxiv.org/html/2505.09109v4/x11.png" alt="学习新折叠策略"></p>
<blockquote>
<p><strong>图11</strong>：学习新折叠策略。通过设计新的折叠规则（例如，先折叠左袖再折叠右袖，与原始的先右后左相反），模型可以成功学习并执行新的策略。</p>
</blockquote>
<pre><code>*   **数据量扩展趋势**：图12显示，随着训练轨迹数量从1K增加到15K，仿真和真实世界的成功率均持续提升，表明合成数据的可扩展性。
</code></pre>
<p><img src="https://arxiv.org/html/2505.09109v4/x12.png" alt="数据量扩展趋势"></p>
<blockquote>
<p><strong>图12</strong>：数据量扩展趋势。随着训练轨迹数量的增加，仿真和真实世界的成功率均持续提升。</p>
</blockquote>
<pre><code>*   **仿真到真实的迁移**：图9(b)显示，在真实世界实验中，使用KG-DAgger方法训练的模型取得了75%的成功率，显著高于Perfect（50%）和Noised（65%）方法。这表明KG-DAgger生成的恢复数据有效提升了策略在真实环境中的鲁棒性。

*   **微调大型VLA模型**：图13显示，将预训练的大型视觉-语言-动作模型π0在FoldNet数据集上微调后，在未见过的机器人上测试，仿真成功率从10%提升至73%，真实世界成功率从30%提升至65%，证明了FoldNet数据对于提升现有大型模型在特定任务上性能的价值。
</code></pre>
<p><img src="https://arxiv.org/html/2505.09109v4/x13.png" alt="微调大型VLA模型结果"></p>
<blockquote>
<p><strong>图13</strong>：微调大型VLA模型结果。加载预训练的π0模型并在FoldNet数据集上微调，在π0未见过的机器人上进行测试，仿真和真实世界的成功率均大幅提升。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ul>
<li><strong>VLM过滤步骤</strong>：在关键点检测任务中，移除过滤步骤导致平均mAP从47.2降至45.8，AKD从15.6升至16.7，验证了该步骤对提升纹理-几何一致性和下游任务性能的必要性。</li>
<li><strong>KG-DAgger策略</strong>：在折叠任务中，与仅使用完美演示相比，引入错误恢复数据（Noised和KG-DAgger）将真实世界成功率提升了15-25个百分点。KG-DAgger通过使用网络预测的动作来生成恢复数据，比简单的动作加噪（Noised）取得了更好的性能（75% vs 65%）。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了一个可扩展的服装资产生成框架</strong>：通过关键点控制几何，结合生成式AI与VLM过滤，自动化生成大量高质量、多样化且附带语义关键点标注的服装网格，显著丰富了可用于衣物操控研究的合成资产库。</li>
<li><strong>提出了KG-DAgger方法</strong>：一种基于关键点的数据聚合策略，能够在模仿学习过程中自动生成错误恢复演示。该方法大幅提升了合成演示数据的质量，使训练的闭环折叠策略在真实世界的成功率从50%（仅完美演示）提升至75%。</li>
</ol>
<p><strong>局限性</strong>：<br>论文自身提到的局限性包括：1) 纹理生成依赖于外部生成模型，其输出质量和多样性受这些模型限制。2) 尽管在真实世界取得了良好性能，但仿真与真实世界之间仍然存在差距。3) 当前方法专注于折叠任务，对于更复杂的衣物操控任务（如展开、穿衣）的适用性有待进一步验证。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>高质量合成数据生成</strong>：本文展示了结合几何控制、生成式AI和VLM过滤在创建逼真、多样化仿真资产方面的潜力，此范式可推广至其他可变形物体或复杂场景的合成数据生成。</li>
<li><strong>提升策略鲁棒性的数据策略</strong>：KG-DAgger证明了在模仿学习中主动合成并学习“如何从失败中恢复”的数据的重要性，这为训练长程、接触丰富的机器人操作任务提供了新的数据构建思路。</li>
<li><strong>赋能与大模型结合</strong>：FoldNet数据集能够有效微调并提升大型通用机器人模型在特定任务（如衣物折叠）上的性能，启示了“大规模通用数据+高质量领域特定数据”相结合的训练路径价值。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人衣物折叠任务中高质量演示数据稀缺的核心问题，提出了FoldNet数据集及配套方法。关键技术包括：基于关键点构建几何模板并生成纹理以合成多样化衣物资产，利用仿真生成闭环折叠演示，并引入关键点驱动的KG-DAgger策略生成错误恢复数据以提升策略鲁棒性。实验表明，KG-DAgger将现实世界任务成功率提升了25%，最终模型在现实测试中达到75%的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.09109" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>