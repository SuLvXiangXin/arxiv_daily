<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.13774" target="_blank" rel="noreferrer">2509.13774</a></span>
        <span>作者: Yangwei You Team</span>
        <span>日期: 2025-09-17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型在机器人操作任务上展现出强大的泛化能力，但其在复杂、真实世界的长视野任务中仍面临挑战。主流的微调方法包括基于人类演示的监督微调（SFT）和强化学习（RL）。SFT受限于数据质量和数量，而RL虽能通过交互进行策略适应，但在真实机器人应用中存在样本效率低和安全问题。现有的人机交互RL方法（如Reinforced Fine-Tuning， RFT）通常在单任务场景中有效，但其在多任务和长视野操作中的适用性尚未探索，且主要依赖物理干预（如SpaceMouse），缺乏将干预转化为对策略微调有语义意义指导的系统方法。</p>
<p>本文针对上述痛点，提出了一种新的人机交互双执行器RL微调框架。核心思路是：通过一个主执行器维持鲁棒的多任务策略，一个精炼执行器在主策略的潜在噪声空间中进行细粒度、语言引导的调整，并设计了一个“说与调”（talk-and-tweak）方案，将物理干预转化为语义基础的语言指令，从而为策略学习生成新的数据集。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架是一个包含离线预热和在线交互两阶段训练的人机交互双执行器强化学习系统（如图1所示）。输入包括状态（RGB图像和本体感知）和任务指令，输出是机器人动作。核心模块包括：1) 基于扩散一致性模型的主执行器，负责生成鲁棒的多任务动作；2) 精炼执行器，学习在潜在噪声空间中根据精炼语言指令进行条件化调整；3) “说与调”人机干预机制，将物理修正映射为语言指令；4) 多任务学习架构，共享一个执行器但为每个任务配备独立的评论家（Critic）。</p>
<p><img src="https://arxiv.org/html/2509.13774v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：人机交互双执行器VLA微调框架概览。主执行器通过扩散生成鲁棒的多任务动作，而精炼执行器在潜在噪声空间中运作以提供细粒度调整。通过“说与调”方案将物理修正转化为有语义基础的精炼指令，从而整合人类干预。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>双执行器动作生成</strong>：策略头采用一致性模型。主执行器 $\pi_{\theta}^{w}$ 通过去噪从标准高斯分布（公式1）中采样的噪声 $w$ 来生成动作。精炼执行器 $\pi_{\phi}$ 则预测一个条件化的噪声分布均值 $\mu = \pi_{\phi}(s, l_{rf})$（公式2），其中 $l_{rf}$ 是精炼语言指令。当提供指令时，动作通过去噪从 $\mathcal{N}(\mu, K^2I)$ 中采样的 $w$ 生成；否则，系统默认使用主执行器的原始行为。</li>
<li><strong>两阶段训练流程</strong>（图3）：<ul>
<li><strong>离线预热阶段</strong>：使用演示数据初始化主策略 $\pi_{\theta}^{w}$ 和Q函数 $Q_{\psi}$。主策略通过混合损失（公式6）优化，包括行为克隆（BC）损失（公式5）和Q值最大化损失（公式4）。Q函数使用Calibrated Q-Learning (Cal-QL)进行训练以稳定离线策略改进。</li>
<li><strong>在线交互阶段</strong>：机器人与环境交互。主策略继续用混合损失优化，但数据来自演示缓冲区和在线交互缓冲区的混合，并调整了BC和Q损失的权重。Q函数改用标准贝尔曼损失优化。精炼执行器在此阶段引入并训练。</li>
</ul>
</li>
<li><strong>精炼执行器训练</strong>：精炼执行器 $\pi_{\phi}$ 使用编码后的状态和精炼指令预测噪声均值 $\mu$。其训练目标（公式11）包含三部分：基于干预数据的行为克隆损失 $\mathcal{L}^{\text{BC}}<em>{\pi</em>{\phi}}$（公式8）、Q值最大化损失 $\mathcal{L}^{Q}<em>{\pi</em>{\phi}}$（公式9），以及一个正则化损失 $\mathcal{L}^{\text{Reg}}<em>{\pi</em>{\phi}}$（公式10），该损失强制在没有精炼指令（使用“[null]”占位符）时，精炼执行器产生的动作与主执行器原始动作保持一致。</li>
<li><strong>“说与调”干预机制</strong>：当人类通过SpaceMouse进行物理干预时，系统记录干预动作 $a^{\text{intv}}$ 和状态 $s_t$。关键创新在于，通过一个规则映射函数 $f$，将一段时间窗口（$J=5$步）内的累积平移位移（公式12）转化为自然语言精炼指令（公式13）。例如，若$x$轴累积位移超过阈值$\sigma=0.001$米，则生成指令“positive x direction”。最终生成如“move right and forward”的指令，形成三元组 $(s_t, a_t^{\text{intv}}, l_{{rf}<em>{t}})$ 存入干预数据集 $\mathcal{D}</em>{\text{intv}}$，用于训练精炼执行器。</li>
<li><strong>多任务学习架构</strong>（图4）：采用一个共享的多任务主执行器，但为每个任务 $i$ 维护一个独立的任务特定评论家 $Q_{\psi_i}$。每个任务有三个独立的数据缓冲区：演示 $\mathcal{D}^{i}<em>{\text{demos}}$、交互 $\mathcal{D}^{i}</em>{\text{rollouts}}$ 和干预 $\mathcal{D}^{i}_{\text{intv}}$。训练时，主执行器从所有任务的演示和交互缓冲区均匀采样更新；每个评论家仅使用对应任务的数据更新；精炼执行器则使用所有任务干预数据的聚合。此外，引入自适应任务权重 $\epsilon_i$（公式16），根据各任务当前Q值动态调整其在策略损失中的比重（公式15），以平衡多任务学习进度。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.13774v1/x3.png" alt="两阶段训练"></p>
<blockquote>
<p><strong>图3</strong>：两阶段训练框架概览。在预热阶段，主执行器和评论家从离线演示中初始化。在随后的交互阶段，它们使用离线数据、人类干预和在线交互的混合数据进行精炼，同时精炼执行器从“说与调”数据集中训练。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.13774v1/x4.png" alt="多任务学习架构"></p>
<blockquote>
<p><strong>图4</strong>：多任务学习架构概览。该框架将单个共享执行器与每个任务特定的评论家耦合，实现一致的策略学习，同时保持任务特定的价值函数。</p>
</blockquote>
<p><strong>创新点</strong>：1) <strong>双执行器设计</strong>：分离了基础策略维持和语言引导的细调，兼顾鲁棒性与可控性。2) <strong>语言化干预</strong>：将低级物理修正自动转化为高级语义指令，构建了可用于训练的数据集，增强了策略调整的可解释性和样本效率。3) <strong>多任务RL微调</strong>：将高效的RL微调从单任务扩展至多任务场景，并设计了共享执行器与独立评论家、自适应权重等机制保证稳定学习。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：使用真实机器人进行实验，任务为操作桌面上的螺栓，包含三个子任务（图2）：a) 将螺栓竖立放置，b) 可靠抓取螺栓，c) 将螺栓组装到螺柱上。基准模型使用Octo作为VLA骨干网络提取表征。评估指标为成功率和任务完成步数（episode length）。</p>
<p><img src="https://arxiv.org/html/2509.13774v1/x2.png" alt="任务示意图"></p>
<blockquote>
<p><strong>图2</strong>：三个操作任务示意图。(a) 将螺栓竖立放置，(b) 抓取螺栓，(c) 将螺栓组装到螺柱上。</p>
</blockquote>
<p><strong>对比方法</strong>：与HG-DAgger、HIL-ConRFT（一种先验的VLA模型人机交互RL微调方法）、DSRL（在扩散潜在噪声空间中进行RL调控的方法）以及本文方法的消融版本（无双执行器）进行对比。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>多任务成功率</strong>（表1）：完整方法（带双执行器）在三个子任务上均达到**100%**的成功率，平均成功率显著高于所有基线方法（HG-DAgger: 25.3%， HIL-ConRFT: 0%， DSRL: 24.0%， 无双执行器版本: 86.7%）。同时，任务完成步数也最短（平均30.7步），表明其高效性。</li>
<li><strong>训练效率</strong>（图5左）：在在线微调时间方面，本文方法仅用<strong>101分钟</strong>即达到接近100%的成功率，而HIL-ConRFT和DSRL在相同时间内成功率几乎为零，HG-DAgger增长缓慢。这证明了“说与调”机制带来的显著样本效率提升。</li>
<li><strong>消融实验</strong>（图5右）：移除非语言干预（No intervention）或“说与调”机制（No talk）都会导致性能大幅下降，验证了人机交互及语言化转换的重要性。移除精炼执行器（No dual-actor）的性能也差于完整方法，证明了双执行器架构的有效性。</li>
<li><strong>长视野任务泛化</strong>（图6）：在需要连续执行超过12步操作的长视野任务中，完整方法能维持约**50%**的序列完成率，而消融版本（无精炼执行器）在步数增加后成功率迅速衰减至零，表明双执行器框架对于长视野任务的鲁棒性至关重要。</li>
<li><strong>多机器人训练扩展</strong>（图7）：将框架扩展到双机器人并行训练，结果显示训练效率（达到特定成功率所需的时间）提升最高达<strong>2倍</strong>，且训练曲线更稳定，方差更小。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.13774v1/x5.png" alt="训练效率与消融实验"></p>
<blockquote>
<p><strong>图5</strong>：左图：不同方法的在线微调时间与平均成功率对比。本文方法仅用101分钟达到近100%成功率。右图：消融研究结果，展示了移除各组件对性能的影响。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.13774v1/long_horizon.png" alt="长视野任务性能"></p>
<blockquote>
<p><strong>图6</strong>：长视野任务性能。展示了在连续操作步数增加时，完整方法与无双执行器消融版本的序列完成率对比。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.13774v1/x6.png" alt="多机器人训练"></p>
<blockquote>
<p><strong>图7</strong>：多机器人训练效率对比。使用双机器人并行训练可以显著减少达到目标性能所需的墙上时钟时间。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了一个新颖的<strong>双执行器VLA微调框架</strong>，通过主执行器维持多任务鲁棒性，并通过在潜在空间操作的精炼执行器实现语言引导的细粒度调整。2) 设计了轻量级的<strong>“说与调”人机交互方案</strong>，将物理修正自动映射为语义基础的语言指令，构建了可用于高效策略调整的干预数据集。3) 在真实机器人多任务和长视野操作中验证了方法的<strong>高效性</strong>（101分钟在线微调达到100%成功率，长视野任务50%完成率）和<strong>可扩展性</strong>（多机器人训练效率提升2倍）。</p>
<p><strong>局限性</strong>：论文提到，“说与调”中的语言映射目前是基于预定义规则的，未来可以探索更灵活的学习型映射。此外，实验主要围绕螺栓操作任务，在更广泛任务上的泛化能力有待进一步验证。</p>
<p><strong>启示</strong>：这项工作为VLA模型的高效、安全在线适应提供了新思路。将人类干预“语义化”并用于训练，提升了人机交互的效率和可解释性。双执行器架构分离“基础能力”与“指令跟随”的思路，对构建更模块化、可控的机器人策略系统具有启发意义。框架对多任务和多机器人场景的良好支持，也展示了其在复杂现实应用中部署的潜力。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-行动（VLA）模型在复杂现实任务中性能不足的问题，提出了一种基于强化学习的人机协同双行动者微调框架。其核心是结合一个负责稳健多任务行动的主要行动者，和一个在潜在噪声空间进行细粒度调整的精炼行动者。关键创新在于“对话调整”方案，将人类物理纠正转化为语义明确的语言指令以生成训练数据。实验表明，该方法在101分钟的在线微调内实现了三个任务的100%成功率，在长时域任务中能维持50%成功率超过12个连续操作，并在多机器人训练中实现最高2倍的效率提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.13774" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>