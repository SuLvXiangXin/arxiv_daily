<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLM-SFD: VLM-Assisted Siamese Flow Diffusion Framework for Dual-Arm Cooperative Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>VLM-SFD: VLM-Assisted Siamese Flow Diffusion Framework for Dual-Arm Cooperative Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.13428" target="_blank" rel="noreferrer">2506.13428</a></span>
        <span>作者: Wei Pan Team</span>
        <span>日期: 2025-06-16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前双臂协作操作的主流方法包括基于学习的（如模仿学习、强化学习）和基于传统控制的（如优化控制、运动规划）方法。这些方法普遍存在关键局限性：它们通常需要大量的双臂演示数据进行训练，且难以泛化到多样化的任务和动态非结构化环境中，特别是在涉及两个物体交互（如组装、工具使用）的场景中。基于流的策略和扩散模型虽在单臂操作中展现出前景，但扩展到双臂协作时，面临如何建模双臂交互、确保同步协调与避碰、以及基于时空约束进行任务分配等尚未解决的挑战。</p>
<p>本文针对上述痛点，提出了一种新的视角：将双臂协作问题分解为以物体为中心的运动流生成和时空任务分配两个阶段。核心思路是：利用一个孪生流扩散网络（SFDNet）从少量演示中学习生成两个物体中心化的2D运动流，再借助预训练的视觉语言模型（VLM）进行知识驱动的时空推理，动态地将运动轨迹分配给每个机械臂，从而实现高效、协调且无需额外真实世界调优的部署。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLM-SFD框架是一个两阶段流程，整体框架如图1所示。训练阶段，输入初始帧和任务指令，通过预训练模型定位物体并提取其运动流作为监督信号，训练SFDNet。测试/推理阶段，训练好的SFDNet根据指令生成预测的2D运动流，经后处理转换为3D轨迹，最后由VLM辅助的时空任务分配模块生成可执行的双臂运动计划。</p>
<p><img src="https://arxiv.org/html/2506.13428v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：VLM-SFD框架总览。训练阶段（上半部分）：利用Grounding-DINO和TAPIR从演示视频中定位目标物体并提取其运动流，用于训练SFDNet。测试阶段（下半部分）：给定初始帧和指令，SFDNet预测2D运动流，经后处理转为3D轨迹，再由VLM辅助的时空任务分配模块生成最终的双臂协调运动计划。</p>
</blockquote>
<p>核心模块是**Siamese Flow Diffusion Network (SFDNet)**，它包含三个部分：</p>
<ol>
<li><strong>Siam-VAE编码器</strong>：一个权重共享的双分支孪生结构，分别将两个目标物体的运动流 $F_1, F_2$ 编码到统一的潜在空间，得到初始潜在代码序列 $\mathbf{z}^{1:T}_i(0)$。这确保了模型学习到跨物体的一致时空表示，同时保留物体特异性。</li>
<li><strong>Siam-UNet</strong>：一个权重共享的双分支UNet，执行条件去噪扩散过程。前向过程对潜在代码加噪（公式4）。在反向去噪的每一步，每个分支在任务指令（通过文本编码器 $\mathbf{T}$ 编码）的条件引导下，预测需要去除的噪声 $\hat{\epsilon}^{1:T}_i$（公式6）。训练目标是最小化预测噪声与真实噪声的均方误差（公式7）。该过程通过多头交叉注意力（MHCA）机制融合语言条件，使生成的运动流符合任务语义。</li>
<li><strong>Siam-VAE解码器</strong>：另一个权重共享的双分支结构，将去噪后的潜在代码 $\hat{\mathbf{z}}^{1:T}_i(0)$ 解码为细化后的2D运动流 $\widehat{F}_i$。</li>
</ol>
<p>与现有方法相比，创新点体现在：1) <strong>首次将扩散模型应用于双臂运动合成</strong>，通过孪生结构和条件扩散生成同步且兼容的双流运动。2) 引入了<strong>VLM辅助的时空任务分配策略</strong>。在推理阶段，将预测的2D运动流可视化在初始帧上，输入给VLM（GPT-4o）。VLM根据任务语义、物体关系和场景上下文，动态决定将流分配给哪只手臂、将每条流分割成多少段 $M_{\text{VLM}}$，并对所有片段进行排序以优化执行顺序，同时指定夹爪的开合控制信号。这种方法无需显式的碰撞约束处理，即可实现高效、无冲突的协调。</p>
<p><img src="https://arxiv.org/html/2506.13428v2/imgs/real-process.png" alt="任务示例"></p>
<blockquote>
<p><strong>图2</strong>：四个挑战性双臂协作任务的示例及执行结果快照。每行展示一个任务：将X放入锅中、打包、倾倒、拉开抽屉并放置。从左到右分别为初始场景、运动流可视化、机器人执行的连续快照。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实世界平台上进行，使用两个Franka Research 3机械臂和一个Intel RealSense D435i RGB-D相机。评估了四个任务：1) Put X into Pot, 2) Packing, 3) Pouring, 4) Pulling Drawer &amp; Placing。每个任务仅使用<strong>10个</strong>涉及3个不同物体的人类演示视频进行训练。</p>
<p><img src="https://arxiv.org/html/2506.13428v2/x2.png" alt="实验平台"></p>
<blockquote>
<p><strong>图3</strong>：真实世界实验平台快照：两个Franka Research 3机械臂及前方的Intel RealSense D435i RGB-D相机。</p>
</blockquote>
<p>对比的基线方法包括：Action Chunking Transformers (ACT) 和 Diffusion Policy (DP)。评价指标为任务成功率。</p>
<p><img src="https://arxiv.org/html/2506.13428v2/imgs/SR_updated.png" alt="成功率对比"></p>
<blockquote>
<p><strong>图4</strong>：VLM-SFD与基线方法ACT、DP在四个双臂操作任务上的成功率（%）对比。VLM-SFD（橙色）在所有任务上均取得最高成功率，且优势显著。成功率基于每个任务20次重复试验的平均值。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ul>
<li><strong>Put X into Pot</strong>：VLM-SFD成功率 **85%**，显著高于ACT（40%）和DP（55%）。</li>
<li><strong>Packing</strong>：VLM-SFD成功率 **90%**，远高于ACT（50%）和DP（65%）。</li>
<li><strong>Pouring</strong>：VLM-SFD成功率 **80%**，优于ACT（45%）和DP（60%）。</li>
<li><strong>Pulling Drawer &amp; Placing</strong>：VLM-SFD成功率 **75%**，同样高于ACT（35%）和DP（50%）。</li>
</ul>
<p>实验结果表明，VLM-SFD在仅使用极少量演示数据（每个任务10个）且<strong>无需任何真实世界微调</strong>的情况下，在所有任务上均取得了最高的成功率，证明了其卓越的数据效率、泛化能力和部署便利性。消融实验虽未在提供内容中详细展开，但论文通过整体框架的对比实验，已证明了SFDNet（生成高质量运动流）与VLM任务分配策略（实现协调与避碰）这两个核心组件的联合贡献至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：</p>
<ol>
<li>提出了<strong>VLM-SFD整体框架</strong>，将双流运动流生成与时空任务分配相结合，实现了仅需少量演示即可快速适应并部署到新任务的双臂协作。</li>
<li>设计了<strong>SFDNet</strong>，这是首个在双臂设置中利用条件扩散进行运动合成的框架，能从有限数据中生成灵活协调的行为。</li>
<li>引入了<strong>VLM辅助的时空任务分配策略</strong>，利用预训练VLM的知识推理能力动态分配运动，优化执行顺序，避免冲突。</li>
</ol>
<p>论文提到的局限性包括：目前处理的物体主要为刚性或微变形物体，框架在涉及高度非刚性物体（如绳索）的任务上的有效性尚未验证。</p>
<p>本工作对后续研究的启示在于：展示了<strong>大规模预训练视觉语言模型（VLM）与特定领域扩散模型相结合</strong>的潜力，前者提供高层语义和常识推理，后者负责精细的运动生成。这种“大模型+小模型”的范式，为在数据稀缺的复杂机器人任务（如需要长程规划和多物体交互的任务）中实现高效学习和零样本泛化提供了新思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对双臂机器人协作操作在动态、非结构化环境中泛化能力不足的问题，提出VLM-SFD框架。该方法核心包含**Siamese Flow Diffusion Network (SFDNet)**，采用孪生编码器-解码器架构将双目标嵌入共享潜在空间，并利用扩散模型生成以物体为中心的双流运动；以及**动态任务分配策略**，结合预训练视觉语言模型（VLM）为双臂自适应分配最优动作。实验表明，该方法仅需少量人类演示，即可显著提升对多样化现实任务的快速适应与泛化能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.13428" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>