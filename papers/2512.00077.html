<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.00077" target="_blank" rel="noreferrer">2512.00077</a></span>
        <span>作者: Zhi, Bowen</span>
        <span>日期: 2025/11/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人在类人环境中的稳定双足行走是一个核心挑战，其固有的欠驱动和不稳定动力学特性使得控制尤为复杂。近年来，为增强机器人功能而附加的超限肢体（SLs）进一步加剧了这一挑战。这些重型、多关节的SLs（如可承载30kg负载的机械臂）在运动时会对主体产生持续、大幅度的动态扰动，严重破坏底层行走的稳定性。现有方法存在明显局限：传统的基于零力矩点（ZMP）的模型控制依赖于精确模型和预定义接触，难以应对SLs引入的连续内部扰动；而新兴的深度强化学习（DRL）方法虽能生成复杂步态，但若用一个单一的策略同时控制行走和SLs平衡，则会面临状态-动作空间爆炸和目标冲突的问题，导致训练难以进行。先前关于SLs用于平衡的研究多集中于静态支撑或专用非拟人附件（如尾巴、额外腿），缺乏一种利用通用拟人机械臂在动态行走中进行主动平衡的通用策略。</p>
<p>本文针对“通用超限肢体在动态行走中引入的持续扰动严重破坏稳定性”这一具体痛点，提出了一个<strong>解耦的分层控制新视角</strong>。核心思路是：将复杂的整体控制问题分解为两个相对独立的层次——底层使用基于模仿学习的DRL策略专门负责生成稳健的行走步态，高层采用一个模型驱动的控制器专门负责利用SLs进行动态平衡调节，从而有效隔离并抵消SLs带来的内部扰动。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的分层控制框架旨在实现搭载重型SLs的人形机器人稳定行走。整体架构清晰解耦，如下图所示，包含一个由课程学习调度的外层训练循环和一个标准的模仿学习内层训练循环。</p>
<p><img src="https://arxiv.org/html/2512.00077v1/picture/robot_model.png" alt="整体训练框架"></p>
<blockquote>
<p><strong>图2</strong>：整体训练框架。外层为课程调度器，根据全局训练进度调整任务难度（如SLs负载质量<code>m_i</code>和手臂姿态<code>p_i^arm</code>）。内层为标准的模仿学习过程，DRL智能体与环境交互，学习当前难度下的策略。</p>
</blockquote>
<p><strong>1. 系统与仿真平台</strong>：研究在MuJoCo高保真物理仿真中进行。机器人模型由Unitree H1人形机器人（12自由度）作为移动基座，以及两个安装在背包上的Kinova Gen3机械臂（共14自由度）作为SLs构成。为专注于算法验证，执行器扭矩未设限。</p>
<p><strong>2. 低层控制：基于DRL的步态生成</strong>。该层目标是通过模仿专家行走数据，训练一个控制人形机器人腿部和下躯干的稳健行走策略。</p>
<ul>
<li><strong>算法与网络</strong>：采用近端策略优化（PPO）算法进行训练。策略网络为三层MLP（1024, 512, 256个神经元，Tanh激活函数）。</li>
<li><strong>模仿奖励函数</strong>：奖励函数<code>R_t</code>遵循“生存优先，模仿其次”原则，是生存奖励、稳定性奖励和模仿奖励的加权和（权重见论文表2.1）。模仿奖励进一步分解为关节位置、速度以及关键身体部位朝向等与专家数据的相似度。</li>
<li><strong>核心创新：课程学习</strong>：为使策略适应SLs的扰动，实施了并行双课程学习。在总计5亿训练步中：1) <strong>负载随机化</strong>：SLs的有效负载质量从可忽略逐渐增加到目标范围（19-30kg）；2) <strong>手臂姿态课程</strong>：SLs的静态姿态从靠近身体的中立位，分四个阶段逐步移动到具有挑战性的前伸位。这迫使策略学习适应不断变化的质心位置。</li>
</ul>
<p><img src="https://arxiv.org/html/2512.00077v1/picture/rl_training_results_train.png" alt="模仿学习内循环"></p>
<blockquote>
<p><strong>图3</strong>：模仿学习内循环示意图。策略网络根据状态<code>s_t</code>生成动作<code>a_t</code>；仿真环境执行并返回新状态；奖励计算模块通过对比智能体状态<code>s_t</code>与专家状态<code>s_t*</code>计算奖励<code>R_t</code>；PPO优化器利用奖励更新策略参数<code>θ</code>。</p>
</blockquote>
<p><strong>3. 高层控制：基于模型的动态平衡</strong>。该层独立于DRL策略，其核心是利用SLs进行主动平衡调节。控制器基于一个简化的刚体动力学模型，其详细逻辑如下图所示。</p>
<p><img src="https://arxiv.org/html/2512.00077v1/picture/1_com_summary_final.png" alt="动态平衡控制逻辑"></p>
<blockquote>
<p><strong>图4</strong>：动态平衡场景的详细控制逻辑。DRL步态策略生成人形机器人腿部动作<code>a_t</code>；同时，主动平衡控制器利用机器人状态<code>s_t</code>估计质心（CoM）和支撑中心（CoS），计算平衡误差<code>e_xy</code>，并通过PD控制器生成SL手臂的补偿扭矩<code>τ_t</code>；两个控制信号合并后发送至仿真环境。</p>
</blockquote>
<ul>
<li><strong>状态估计</strong>：控制器实时计算两个关键量：1) <strong>全身质心（CoM）</strong>；2) <strong>支撑中心（CoS）</strong>。CoS的计算根据步态相位动态调整：单支撑期，CoS为支撑脚几何中心；双支撑期，CoS为基于地面反作用力（GRF）加权平均的左右脚压力中心（CoP）。这提供了整个步态周期内物理上准确的稳定性参考。</li>
<li><strong>平衡控制律</strong>：控制器将CoM与CoS在水平面的向量差<code>d_xy</code>作为动态稳定性指标（而非要消除的静态误差）。通过一个启发式的比例控制律来调整SLs的目标关节角度：<code>q_target^arm = q_base^arm - K_p^arm * d_xy</code>。其中<code>q_base^arm</code>是一个固定的、略微前伸的中立“归位”姿态。随后，一个PD控制器（<code>τ_arm = K_p*(q_target^arm - q_current^arm) - K_d * q_dot_current^arm</code>）计算驱动SLs所需的扭矩。所有增益均通过仿真中的迭代手动调参获得，旨在实现临界阻尼响应。</li>
<li><strong>控制融合</strong>：在每一仿真步中，SL手臂执行器的扭矩被平衡控制器的输出<code>τ_arm</code>覆盖，而人形机器人腿部和躯干执行器的扭矩则直接来自DRL策略的动作。这种清晰的分离确保了各司其职。</li>
</ul>
<p><strong>创新点总结</strong>：1) <strong>解耦的分层架构</strong>：将步态生成与动态平衡分离，降低了整体控制问题的复杂度；2) <strong>针对扰动的课程学习</strong>：通过渐进式增加负载和改变手臂姿态，使DRL策略能学会适应SLs的静态和动态效应；3) <strong>基于物理的模型平衡控制器</strong>：利用通用拟人机械臂，通过简单高效的比例反馈对CoM-CoS误差进行实时阻尼调节。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在MuJoCo仿真环境中，对Unitree H1搭载Kinova Gen3 SLs的模型进行定量评估。对比了三个渐进式场景：1) <strong>基线行走</strong>：无SL背包，仅由DRL策略控制；2) <strong>静态负载</strong>：SLs锁定在一个固定的、具有挑战性的前伸姿态，仅由经过完整课程训练的DRL策略控制（高层控制器禁用）；3) <strong>动态平衡</strong>：完整分层框架激活，DRL策略控制行走，高层控制器动态驱动SLs平衡。</p>
<p><strong>关键结果</strong>：</p>
<ol>
<li><strong>DRL训练性能</strong>：经过5亿步训练，策略在平均回合回报和平均回合长度上均呈现持续上升趋势。在课程难度提升的节点（如每1亿步）出现短暂性能下降后能快速恢复，验证了课程学习的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00077v1/picture/2_com_cos_dist_annotated_final.png" alt="训练性能曲线"></p>
<blockquote>
<p><strong>图5</strong>：PPO智能体超过5亿环境步的训练性能。(a)平均回合回报。(b)平均回合长度。与课程变化同步的周期性短暂下降及后续恢复，证明了策略的成功适应。</p>
</blockquote>
<ol start="2">
<li><strong>质心轨迹分析</strong>：通过动态时间规整（DTW）距离量化“静态负载”和“动态平衡”场景的CoM轨迹与“基线行走”的相似度。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00077v1/picture/2_recovery_analysis_final.png" alt="CoM轨迹对比"></p>
<blockquote>
<p><strong>图6</strong>：三种场景的CoM轨迹。DTW距离是相对于“基线行走”轨迹计算的。“动态平衡”的DTW分数更低，表明其动态模式和节奏得到了更好的保持。</p>
</blockquote>
<p>结果显示，“动态平衡”策略的DTW距离为65.54，相较于“静态负载”的123.71**降低了约47%**。这表明主动平衡控制器有效缓解了SLs引入的高频扰动，更好地保持了原始无负载步态的基本动态特征和节奏。</p>
<ol start="3">
<li><strong>动态平衡调制分析</strong>：通过分析CoM-CoS距离在步态周期中的振荡来评估平衡管理能力。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00077v1/picture/3_gait_coordination_final.png" alt="CoM-CoS距离分析"></p>
<blockquote>
<p><strong>图7</strong>：三种场景下CoM-CoS距离随时间的变化。阴影区域表示双支撑期。“动态平衡”场景（绿色）在摆动期结束、进入双支撑期后，表现出更迅速、更大幅度的距离减小，表明恢复更有效。</p>
</blockquote>
<p>关键发现是，在“动态平衡”场景下，当摆动期结束、进入双支撑期后，CoM-CoS距离的下降（即重新稳定过程）<strong>更快、幅度更大</strong>。这表明主动平衡控制器能更有效地在步态周期内辅助恢复稳定。</p>
<ol start="4">
<li><strong>步态协调性探索分析</strong>：通过分析左右脚地面反作用力（GRF）的相位关系来评估双足协调性。理想的双足行走中，左右GRF应呈180度反相（即此消彼长）。通过主成分分析（PCA）拟合GRF数据椭圆，并计算其主轴与理想135度反相轴（因数据缩放和偏移）的“定向误差”。</li>
</ol>
<ul>
<li>“基线行走”的定向误差为 <strong>6.89度</strong></li>
<li>“静态负载”的定向误差增大至 <strong>17.63度</strong></li>
<li>“动态平衡”的定向误差改善至 <strong>10.69度</strong><br>   这表明动态平衡控制器有助于在SLs扰动下，恢复更协调的反相位GRF模式。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个<strong>新颖的解耦分层控制框架</strong>，将学习型步态生成与模型型动态平衡相结合，有效解决了人形机器人因搭载通用超限肢体而引发的内部稳定性难题。</li>
<li>设计并验证了一种<strong>针对性的课程学习策略</strong>，使DRL策略能够逐步适应SLs带来的质量和姿态变化，从而学习出在扰动下依然稳健的底层行走步态。</li>
<li>实现了一个<strong>基于物理的、实时的模型平衡控制器</strong>，该控制器利用CoM和CoS的反馈，通过简单高效的比例控制驱动SLs产生补偿动量，显著提升了系统的动态稳定性和步态协调性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，仿真中未对执行器扭矩进行饱和限制，这是一个简化假设，也是未来向实物迁移（Sim-to-Real）时需要解决的关键挑战。此外，高层平衡控制器的增益依赖于手动调参。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>解耦策略的有效性</strong>：对于涉及高维、多目标、存在内部耦合扰动的复杂机器人控制问题，采用分而治之的层次化设计是一条行之有效的路径。</li>
<li><strong>混合智能控制</strong>：结合学习方法的灵活性与模型方法的可靠性，是处理机器人系统中“复杂但可学习”子任务（如步态）与“明确但关键”子任务（如平衡）的实用范式。</li>
<li><strong>通用肢体的双重用途</strong>：本研究展示了通用拟人机械臂不仅可以执行操作任务，还能作为动态平衡的辅助工具，这为增强人形机器人的整体功能性和适应性提供了新思路。未来的工作可探索更优化的平衡控制律，并解决仿真到实物的迁移问题。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人集成超数肢体时引发的动态扰动与稳定性挑战，提出了一种分层控制框架。其核心是解耦策略，结合了基于模仿学习和课程学习的底层步态生成，以及利用超数肢体进行动态平衡的高层模型控制。在物理仿真中，与静态负载相比，该动态平衡控制器使质心轨迹更接近基准步态，其动态时间规整距离降低了47%，并实现了更协调的地面反作用力模式，有效提升了行走稳定性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.00077" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>