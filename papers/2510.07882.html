<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.07882" target="_blank" rel="noreferrer">2510.07882</a></span>
        <span>作者: Li, Boyu, He, Siyuan, Xu, Hang, Yuan, Haoqi, Xu, Xinrun, Zang, Yu, Hu, Liwei, Yue, Junpeng, Jiang, Zhenxiong, Hu, Pengbo, Karlsson, Börje F., Tang, Yehui, Lu, Zongqing</span>
        <span>日期: 2025/10/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，利用多模态大语言模型（MLLMs）作为高层规划器，使机器人执行复杂的人类指令，已成为具身AI的一个主流研究方向。然而，在涉及双足人形机器人的长程任务中，现有方法的有效性仍然有限。这源于两大关键瓶颈：一是数据层面，缺乏专门为双足人形机器人设计、能够支持MLLM训练数据收集与评估的仿真平台；二是模型层面，现有MLLMs缺乏对机器人本体（embodiment）的充分感知，导致其在规划时难以推理双臂选择逻辑、身体姿态调整以及与物体的合理交互点。本文针对这两个痛点，提出了新的仿真平台和增强的MLLM模型。核心思路是：首先构建一个支持连续状态转移和随机意外事件的双臂人形机器人仿真器DualTHOR，然后在此基础上，通过引入基于运动的本体感知信息、运动基位置嵌入和跨空间编码器，提出Proprio-MLLM模型，以实现更具本体感知能力的具身规划。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的解决方案包含两大核心部分：仿真平台DualTHOR和规划模型Proprio-MLLM。</p>
<p><strong>整体框架</strong>：研究首先构建了DualTHOR仿真平台，用于生成训练数据和评估任务。基于此平台收集的数据，作者训练了Proprio-MLLM模型。该模型以第一人称视觉观察、机器人本体感知（关节位置等运动信息）和任务指令文本作为输入，经过多模态对齐和编码后，由大语言模型进行推理，输出高层动作规划序列。</p>
<p><img src="https://arxiv.org/html/2510.07882v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体概览。上半部分展示了现有仿真平台（如AI2-THOR、ManipulaTHOR等）主要针对轮式或单臂机器人，而本文提出的DualTHOR专注于双臂人形机器人。下半部分指出当前MLLMs在双臂具身任务规划中效果有限，因此提出了增强本体感知的Proprio-MLLM。</p>
</blockquote>
<p><strong>核心模块一：DualTHOR仿真平台</strong></p>
<ol>
<li><strong>物理引擎与机器人</strong>：基于Unity引擎构建，支持物理上真实的双臂操作。集成了两款人形机器人：力量型/稳定的Unitree H1和灵巧/精确的Agibot X1，两者具有不同的末端执行器（灵巧手 vs. 平行夹爪）。</li>
<li><strong>任务分类</strong>：设计了三种任务类别以严格评估双臂能力：<strong>双臂必需任务</strong>（单臂物理上无法完成，如举起重物）、<strong>双臂可选任务</strong>（单臂可完成但双臂更优，如限时内搬运两个物体）和<strong>单臂任务</strong>（基础交互任务）。</li>
<li><strong>底层控制</strong>：采用客户端-服务器模型。高层命令通过Python API发送至Unity，由模块化的逆运动学（IK）服务（使用OmniManip）计算机器人关节构型。针对H1和X1机器人分别采用了全身协调IK模型和双臂解耦IK模型。</li>
<li><strong>意外事件机制</strong>：引入了随机意外事件机制来模拟现实世界的不确定性。例如，尝试拿起一个“可倾倒”的咖啡杯时，有80%成功率，10%概率杯子破碎，10%概率内容物溢出。这迫使MLLM发展出鲁棒的重新规划能力。</li>
</ol>
<p><strong>核心模块二：Proprio-MLLM模型</strong><br>该模型基于Qwen 2.5-VL-7B-Ins扩展，通过融入运动模态来感知机器人的本体状态。</p>
<p><img src="https://arxiv.org/html/2510.07882v2/x4.png" alt="方法框架图"></p>
<blockquote>
<p><strong>图4</strong>：Proprio-MLLM的整体架构。模型接收视觉观察、机器人运动序列（本体感知）和文本指令作为输入。通过运动分词器将运动序列离散化为token，与视觉、文本token一同输入到大语言模型中进行多模态对齐和推理，最终输出规划好的动作序列。</p>
</blockquote>
<ol>
<li><p><strong>对齐数据集准备与训练</strong>：基于HumanML3D人体运动数据集，通过运动重定向得到H1和X1机器人的运动数据，并在DualTHOR中重放以获取对应的第一人称视觉观察，构建了一个机器人运动-图像-文本对齐数据集。训练分为三个阶段：</p>
<ul>
<li><strong>阶段一：运动分词器</strong>。训练一个运动VQ-VAE模型，将连续的运动序列编码并离散化为token，以便与LLM的token-in-token-out格式对齐。使用包含重建损失、码本损失和承诺损失的综合损失函数进行训练。</li>
<li><strong>阶段二：对齐训练</strong>。将运动、图像和文本的隐特征在统一的token空间中进行对齐。采用LoRA微调MLLM，初期冻结图像和文本权重以专注于运动对齐，稳定后进行所有模态的联合训练。</li>
<li><strong>阶段三：指令微调</strong>。使用自学习方法SELU收集双臂规划数据，将模型在理解所有三种模态的基础上，微调到具体的双臂规划任务上。</li>
</ul>
</li>
<li><p><strong>运动基位置嵌入（MPE）</strong>：为了增强模型基于机器人身体状态的长程推理能力，创新性地提出了MPE。与传统的基于图像像素位置的嵌入不同，MPE将空间索引编码为相对于机器人质心的方向。具体地，对于视觉token在图像中的位置<code>(x, y)</code>和机器人质心<code>(x_r, y_r)</code>，其位置嵌入为<code>[t, sign(y-y_r), sign(x-x_r)]</code>，其中<code>t</code>为时间索引。这使得模型能够区分视觉token相对于机器人的方向。</p>
</li>
<li><p><strong>跨空间编码器（CSE）</strong>：为了增强模型对机器人交互范围的估计能力，在保留Qwen2.5-VL图像编码器提取高层视觉特征的同时，引入了CUT3R模型从单张RGB图像预测3D点云特征。将2D视觉特征与对齐后的3D点云特征通过一个轻量级MLP进行融合，生成最终融合了深度几何信息的视觉token，从而提供对机器人交互范围的增强感知。</p>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，本文的创新具体体现在：1) 提出了首个专注于双臂人形机器人长程规划任务的仿真平台DualTHOR，具备连续物理和随机意外事件；2) 提出了Proprio-MLLM，首次将机器人的本体感知（运动信息）系统地整合到MLLM的规划过程中；3) 设计了运动基位置嵌入和跨空间编码器两个关键技术，分别用于增强模型对身体姿态的感知和对交互范围的判断。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验在DualTHOR仿真平台中进行。涵盖了10个不同房间环境、68个独特物体的359个任务。评估指标为任务成功率（50次试验的平均值）。对比了三大类基线方法：<strong>专有MLLMs</strong>（GPT-4o, Gemini-1.5-Pro）、<strong>开源MLLMs</strong>（Qwen2.5-VL-7B-Ins, InternVL2.5-8B）和<strong>提示增强型MLLMs</strong>（LLM-Planner, RAP, DAG-Plan）。其中DAG-Plan是当前利用图结构表示双臂技能的最先进方法。</p>
<p><img src="https://arxiv.org/html/2510.07882v2/x5.png" alt="任务分布"></p>
<blockquote>
<p><strong>图5</strong>：DualTHOR中对象和任务的分布情况。(a) 可交互对象类型的分布，(b) 单臂交互任务的分布，(c) 三种任务类别（双臂必需、双臂可选、单臂）的分布。</p>
</blockquote>
<p><strong>关键定量结果</strong>：<br><img src="https://arxiv.org/html/2510.07882v2/x6.png" alt="主要结果表"></p>
<blockquote>
<p><strong>表II</strong>：不同基线方法在DualTHOR中各类任务上的性能（成功率）。Proprio-MLLM在所有任务类别和两种机器人上均取得最佳性能，相比最好的基线DAG-Plan平均提升了19.75%。</p>
</blockquote>
<p>如表II所示，现有方法在长程双臂规划任务上表现挣扎。Proprio-MLLM通过整合本体感知信息以及提出的MPE和CSE，取得了最佳整体性能。相比最先进的DAG-Plan，Proprio-MLLM平均性能提升19.75%，在X1机器人的双臂必需任务上最高提升23.30%。结果还显示，当前MLLMs在双臂和单臂规划之间存在显著差距，凸显了现有仿真器中双臂人形规划数据的稀缺。</p>
<p><strong>失败案例分析</strong>：<br><img src="https://arxiv.org/html/2510.07882v2/x7.png" alt="失败分析"></p>
<blockquote>
<p><strong>图6</strong>：本体感知信息的影响分析。通过将失败轨迹分类为导航失败、身体调整失败和逻辑失败三类，对比基础模型、DAG-Plan和Proprio-MLLM。Proprio-MLLM在所有失败类型上均显著减少，表明其更有效地利用了本体感知信息。</p>
</blockquote>
<p>如图6所示，分析失败轨迹发现，DAG-Plan和Proprio-MLLM都减少了逻辑失败（臂选择），但DAG-Plan的基于图的提示方法无法充分利用本体感知来解决导航和身体调整失败。而Proprio-MLLM通过MPE和CSE，能更有效地整合视觉感知和本体状态，产生更连贯和自适应的双臂计划。</p>
<p><strong>意外事件处理能力</strong>：<br><img src="https://arxiv.org/html/2510.07882v2/x8.png" alt="重新规划结果"></p>
<blockquote>
<p><strong>表III</strong>：在DualTHOR中处理意外事件的重新规划性能（以H1的双臂必需任务为例）。任务按底层技能成功率分为易、中、难三个难度。Proprio-MLLM在“反思”提示的辅助下，重新规划能力显著优于DAG-Plan，尤其是在中高难度任务上。</p>
</blockquote>
<p>如表III所示，在包含意外事件的场景中，Proprio-MLLM能更有效地利用本体感知信息，且“反思”提示对提升重新规划能力贡献更大。然而，两者在高难度任务上的有限性能揭示了当前MLLMs训练数据中意外事件案例的稀缺，进一步强调了DualTHOR提供意外事件机制的重要性。</p>
<p><strong>消融实验</strong>：<br><img src="https://arxiv.org/html/2510.07882v2/x9.png" alt="消融研究"></p>
<blockquote>
<p><strong>表IV</strong>：Proprio-MLLM各组件消融研究结果。展示了移除跨空间编码器、运动基位置嵌入或两者后，成功率的变化以及三类失败数量的增减。</p>
</blockquote>
<p>消融实验（表IV）表明，整合本体感知信息能显著提升MLLM规划性能。具体而言：<strong>跨空间编码器（CSE）</strong> 主要改善机器人交互范围估计，有效减少导航失败（#1）；<strong>运动基位置嵌入（MPE）</strong> 主要帮助调整机器人身体姿态和臂选择逻辑，减少身体调整失败（#2）和逻辑失败（#3）。两者结合效果最佳。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个专为双臂人形机器人设计的仿真平台DualTHOR，它支持连续物理交互、随机意外事件，并提供了标准化的家庭双臂任务基准，填补了该领域高质量数据集的空白。</li>
<li>提出了Proprio-MLLM模型，通过引入本体感知信息、创新的运动基位置嵌入和跨空间编码器，显著增强了MLLM在双臂任务中的本体感知和空间推理能力，实验证明其规划性能平均提升19.75%。</li>
<li>系统性地评估了现有MLLMs在双臂人形规划任务上的局限性，并通过详实的实验（包括失败分析和消融研究）验证了所提各模块的有效性。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，尽管Proprio-MLLM表现优异，但在高难度的意外事件处理任务上性能仍然有限，这反映了当前MLLMs训练数据中此类案例的稀缺。此外，研究主要基于仿真环境，向真实世界的迁移（sim-to-real）仍是未来的挑战。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>DualTHOR平台为社区提供了一个急需的、专注于双臂人形机器人高层规划的研究和评估工具，可促进更多相关数据的生成和算法的开发。</li>
<li>将机器人的本体感知（如关节状态、力觉等）深度整合到高层决策模型中，是提升具身智能体在复杂、长程任务中表现的关键方向。</li>
<li>在训练数据中纳入更多意外事件和失败案例，对于培养模型鲁棒的重新规划和错误恢复能力至关重要，是迈向安全、可靠现实部署的必要步骤。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对多模态大语言模型在双足人形机器人长视界任务规划中效果有限的问题，指出缺乏专用模拟平台和模型具身感知不足两大挑战。为此，作者提出DualTHOR模拟器，具备连续过渡和应急机制；并设计Proprio-MLLM模型，通过融入本体感知信息、运动位置嵌入和跨空间编码器来增强具身感知。实验表明，Proprio-MLLM相比现有MLLMs，规划性能平均提升19.75%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.07882" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>