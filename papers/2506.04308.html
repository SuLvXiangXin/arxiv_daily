<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.04308" target="_blank" rel="noreferrer">2506.04308</a></span>
        <span>作者: Zhou, Enshen, An, Jingkun, Chi, Cheng, Han, Yi, Rong, Shanyu, Zhang, Chi, Wang, Pengwei, Wang, Zhongyuan, Huang, Tiejun, Sheng, Lu, Zhang, Shanghang</span>
        <span>日期: 2025/06/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于预训练视觉语言模型（VLM）的方法是机器人空间智能研究的主流。这些方法主要集中于增强单步空间理解能力，即感知物体的空间属性（如位置、朝向）及其关系（如距离、方向）。然而，它们存在两大关键局限性：第一，在引入3D信息时，要么依赖成本高昂的多视图3D重建，导致模态鸿沟；要么将深度图视为类RGB输入并使用共享的图像编码器，导致模态干扰并损害预训练编码器的性能。第二，对于更复杂的多步空间推理指称任务（即需要组合式、分步推理来解决包含多个空间约束的指令），由于缺乏合适的数据集和评估基准，现有方法探索不足，且严重依赖监督微调（SFT），容易导致模型记忆答案而非学会泛化的显式推理。</p>
<p>本文针对上述痛点，提出了一个名为RoboRefer的3D感知推理VLM。其核心思路是：通过引入一个解耦的专用深度编码器来精确增强单步空间理解，并设计一个结合了度量敏感过程奖励函数的强化微调（RFT）阶段，来赋予模型泛化的、显式的多步空间推理能力，从而实现从感知到推理的全面空间指称。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboRefer的整体框架是一个两阶段（SFT-RFT）训练的VLM，其输入是RGB或RGB-D图像以及空间约束的文本指令，输出是一个满足指令的2D图像空间坐标点（可映射为3D空间锚点），并伴随显式的多步推理过程。</p>
<p><img src="https://arxiv.org/html/2506.04308v4/x2.png" alt="方法总览"></p>
<blockquote>
<p><strong>图2</strong>：RoboRefer方法总览。左侧展示了模型架构及两阶段训练策略：SFT阶段通过引入深度模态和专用编码器来增强空间理解；RFT阶段利用SFT阶段获得的空间理解能力，通过强化学习进行多步显式推理。右侧展示了从单步理解到多步推理（如4步）的示例。</p>
</blockquote>
<p><strong>核心模块与技术细节：</strong></p>
<ol>
<li><strong>VLM架构</strong>：采用分离的RGB编码器和深度编码器，分别提取特征后通过各自的投影器与LLM对齐。深度编码器及其投影器从RGB对应部分初始化。在联合训练时，图像编码器不受深度输入影响，深度编码器独立更新。这种设计避免了模态干扰，无需大量RGB-only数据协同训练，同时通过深度线索（如距离、远近关系）增强了空间感知。</li>
<li><strong>监督微调（SFT）</strong>：以NVILA为基础VLM，进行两步SFT。<ul>
<li><strong>深度对齐</strong>：仅使用RefSpatial数据集的RGB-D标注，训练深度投影器，将新的深度空间与文本空间对齐。</li>
<li><strong>空间理解增强</strong>：使用RefSpatial（包含单步细粒度标注和带显式推理过程的多步数据）以及额外的指令遵循数据集，联合微调所有参数。模型同时在RGB和RGB-D输入上进行优化，图像和深度编码器分开更新。这为后续RFT提供了具备空间理解和初步推理能力的“冷启动”模型。</li>
</ul>
</li>
<li><strong>强化微调（RFT）</strong>：在SFT模型基础上，使用Group Relative Policy Optimization（GRPO）和RefSpatial中的多步推理数据进行RFT。其创新点在于设计了专门的奖励函数来引导精确的点预测和推理过程：<ul>
<li><strong>结果奖励</strong>：<code>R_OF</code>（输出格式奖励）和<code>R_P</code>（点L1奖励，预测点接近真值则得1分）。</li>
<li><strong>过程奖励</strong>：<code>R_PF</code>（过程格式奖励）和关键的<code>R_Acc</code>（准确度奖励）。<code>R_Acc</code>利用RefSpatial中关键步骤的感知标注（如坐标、距离），根据感知类型（如位置用L1距离）度量每一步预测的误差，从而精细化中间推理的准确性。此设计是顺序无关的，不约束固定的推理轨迹。<br>总奖励<code>r_i = R_OF + R_P + α*(R_PF + R_Acc)</code>（α=0.25），用于计算相对优势并更新策略，同时使用KL散度正则化稳定训练。</li>
</ul>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：1) 提出解耦的专用深度编码器，在避免模态干扰的同时增强3D感知；2) 首创将RFT与度量敏感的过程奖励结合，用于VLM的多步空间推理训练，提升了泛化性和推理精确性；3) 提出了系统的两阶段SFT-RFT训练策略。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>数据集/基准</strong>：使用多个单步空间理解基准（CV-Bench, BLINK_val, RoboSpatial, SAT, EmbSpatial）和指称基准（RoboRefIt, Where2Place, RoboSpatial）。此外，本文提出了新的多步空间指称基准 <strong>RefSpatial-Bench</strong>，包含100张定位和100张放置的真实世界杂乱场景图像，超过70%的样本需要多步推理（最多5步）。</li>
<li><strong>对比方法</strong>：包括专有模型（GPT-4o, Gemini-2.5-Pro, Claude-3.7-Sonnet）、开源通用VLM（NVILA, Qwen2.5-VL）以及空间专家模型（SpatialBot, SpatialRGPT, SpaceLLaVA, RoboPoint）。</li>
<li><strong>实验平台</strong>：基于NVILA（2B/8B）基座模型进行训练和评估。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<p><img src="https://arxiv.org/html/2506.04308v4/x4.png" alt="单步空间理解结果"></p>
<blockquote>
<p><strong>表1</strong>：在单步空间理解基准上的性能对比。RoboRefer-SFT（特别是8B版本）在几乎所有基准上达到了最先进的性能，平均成功率89.6%。使用RGB-D输入在3D相关任务（如3D-Depth, 3D-Distance）上相比仅用RGB输入有进一步提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.04308v4/x5.png" alt="多步空间指称结果"></p>
<blockquote>
<p><strong>表2</strong>：在指称及多步空间指称基准上的性能。在现有基准（RoboRefIt, Where2Place, RoboSpatial）上，RoboRefer大幅领先。在新提出的RefSpatial-Bench上，RoboRefer-RFT（2B）显著超越所有基线，在定位（L.）、放置（P.）及未见组合关系（U.）子集上，平均准确率分别达到52.0%、54.0%和41.56%，甚至超过Gemini-2.5-Pro平均17.4%。</p>
</blockquote>
<p><strong>消融实验与组件贡献</strong>：<br>论文在附录中进行了奖励函数的消融实验（对应图32，用户未提供此图片链接，但正文提及）。结果表明，移除过程奖励<code>R_Acc</code>会导致在多步推理任务上的性能显著下降，这证明了度量敏感的过程奖励对于提升中间推理精度的关键作用。同时，完整的奖励函数组合（包含<code>R_OF</code>, <code>R_P</code>, <code>R_PF</code>, <code>R_Acc</code>）能带来最佳的整体性能。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了 <strong>RoboRefer</strong>，一个通过SFT-RFT两阶段策略训练的3D感知推理VLM。其创新性地使用解耦深度编码器增强空间感知，并利用带有度量敏感过程奖励的RFT实现泛化的多步显式空间推理。</li>
<li>构建了大规模高质量数据集 <strong>RefSpatial</strong>（250万样本，2000万QA对，覆盖31种空间关系，支持最多5步推理）和评估基准 <strong>RefSpatial-Bench</strong>，填补了多步空间指称任务的数据与评估空白。</li>
<li>实验表明，RoboRefer在单步理解和多步指称任务上均达到SOTA，并能在真实杂乱场景中与多种控制策略结合，完成不同机器人（如UR5、G1人形机器人）的长时程、动态任务，展现了强大的泛化能力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，由于计算限制，RFT阶段仅在2B模型上进行，未在8B模型上应用。这可能限制了更大模型通过RFT获得进一步性能提升的潜力。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>架构设计</strong>：解耦的多模态编码器设计为在VLM中有效引入新模态（如深度、触觉）提供了参考，可避免干扰预训练主干。</li>
<li><strong>训练范式</strong>：SFT-RFT的序列化训练策略，特别是为视觉空间推理任务定制过程奖励的RFT，为提升VLM在复杂、组合式任务上的推理泛化能力提供了新思路。</li>
<li><strong>数据与评估</strong>：RefSpatial的数据构建流程（融合2D、3D、仿真数据）和RefSpatial-Bench基准的设立，为后续更复杂的具身空间推理研究提供了重要的数据和评估基础。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人难以在复杂3D场景中准确理解空间指令并进行动态推理的问题，提出了RoboRefer模型。其关键技术包括：通过监督微调整合专用深度编码器以实现精确空间理解；采用强化微调与度量敏感奖励函数推进多步空间推理。实验表明，经监督微调的模型平均成功率可达89.6%；强化微调后性能显著提升，在RefSpatial-Bench基准上平均准确率超越Gemini-2.5-Pro达17.4%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.04308" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>