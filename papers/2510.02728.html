<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4 - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.02728" target="_blank" rel="noreferrer">2510.02728</a></span>
        <span>作者: Zhang, Lingfeng, Xiao, Erjia, Zhang, Yuchen, Fu, Haoxiang, Hu, Ruibin, Ma, Yanbiao, Ding, Wenbo, Chen, Long, Ye, Hangjun, Hao, Xiaoshuai</span>
        <span>日期: 2025/10/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>跨模态无人机导航是机器人领域的一项挑战性任务，要求能够根据自然语言描述从大规模数据库中高效检索相关图像。当前的主流方法是基于端到端的学习框架，直接将文本查询映射到视觉特征。例如，GeoText-1652基线方法采用了结合图像编码器（Swin Transformer）、文本编码器（BERT）以及跨模态注意力机制的多模态框架。尽管取得了进展，这类方法仍存在语义精度和检索准确性的固有局限。直接映射通常难以捕捉细微的语义关系，尤其是在空中场景中存在多个视觉相似物体时。</p>
<p>本文针对现有方法在复杂空中场景中细粒度语义匹配不足这一痛点，提出了一个新颖的视角：利用视觉语言模型（VLM）生成的详细描述作为语义桥梁，将跨模态匹配问题转化为更易处理的文本到文本相似度计算。本文的核心思路是提出一个两阶段的检索细化方法：首先利用基线模型进行粗粒度检索，然后使用VLM为候选图像生成详细描述，并基于查询与描述之间的语义相似度进行细粒度重排序。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的Caption-Guided Retrieval System (CGRS) 采用了一个两阶段的、由粗到精的流程。</p>
<p><img src="https://arxiv.org/html/2510.02728v2/fig1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：我们的Caption-Guided Retrieval System (CGRS) 概览。我们的框架采用两阶段流程：粗粒度模型首先使用GeoText-1652基线从图库中检索出前20个候选图像。细粒度模型随后使用视觉语言模型（VLM）为这些候选图像生成详细描述，并基于查询与生成描述之间的文本到文本相似度进行语义重排序，产生最终的前20个结果。</p>
</blockquote>
<p><strong>整体流程</strong>：给定一个文本查询，第一阶段（粗粒度）使用基线模型从整个图像库中检索出最相关的前20张候选图像。第二阶段（细粒度）则专注于这20张候选图像，使用VLM为每张图像生成详细的文本描述，然后计算原始查询与每个生成描述之间的语义相似度，并融合粗粒度相似度得分进行重排序，输出最终的排序列表。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>粗粒度模型</strong>：基于GeoText-1652基线构建。图像编码器使用Swin Transformer提取视觉特征，文本编码器使用BERT提取文本特征。模型通过多种互补的损失函数进行优化：</p>
<ul>
<li>**图像-文本对比损失 (ℒ_itc)**：鼓励匹配的图像-文本对在特征空间中靠近。</li>
<li>**图像-文本匹配损失 (ℒ_itm)**：一个二元分类损失，预测图像-文本对是否匹配，并采用难负样本挖掘。</li>
<li>**区域定位损失 (ℒ_grounding)**：对于区域级文本描述，模型预测对应的边界框，损失结合了IoU损失和L1回归损失。</li>
<li>**空间关系损失 (ℒ_spatial)**：捕捉区域特征对之间的相对空间关系（如左上、居中、右下等九类）。<br>总损失为这些损失的加权和，其中空间匹配任务的权重λ设为0.1。推理时，计算查询与图库中所有图像的全局特征余弦相似度，选取前20名作为候选集。</li>
</ul>
</li>
<li><p><strong>细粒度模型</strong>：这是方法的核心创新模块。</p>
<ul>
<li><strong>描述生成</strong>：对于粗粒度阶段得到的每张候选图像，使用一个预训练的VLM（如GPT-4o）生成详细的描述。为了引导VLM生成空间感知的、语义丰富的描述，论文设计了一个提示模板，明确要求描述图像中心的主要建筑、周围建筑及其相对位置、显著地标以及整体空间布局。</li>
<li><strong>语义相似度计算</strong>：使用一个预训练的句子编码模型（如BERT）将原始查询和所有生成的描述编码到共享的语义空间，并计算它们之间的余弦相似度。</li>
<li><strong>得分融合与重排序</strong>：最终的相似度得分是粗粒度视觉相似度与细粒度语义相似度的加权组合：<code>s_final = α * s_coarse + (1-α) * s_sem</code>。论文通过实验将融合权重α设为0.3，以强调基于描述的语义细化，同时保留视觉对齐信号。最后，根据<code>s_final</code>对候选图像进行降序重排。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有直接进行视觉-文本对齐的方法相比，CGRS的创新性主要体现在引入了<strong>描述引导的重排序机制</strong>。它利用VLM的强大描述能力，将图像的视觉内容“翻译”成丰富的自然语言描述，从而将跨模态匹配问题转化为文本到文本的相似度计算。这有助于捕捉那些直接视觉特征匹配可能遗漏的细微语义差异和空间关系。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>数据集与实验平台</strong>：实验在IROS 2025 RoboSense挑战赛Track 4的官方数据集上进行，该任务基于GeoText-1652基准数据集。该数据集包含大规模的多模态数据，专门用于自然语言引导的无人机地理定位，其标注包含全局描述和细粒度的区域级描述，并明确捕捉“左”、“右上”、“中心”等空间关系。实验在8张NVIDIA A800 GPU上完成。</p>
<p><strong>对比方法</strong>：论文在挑战赛排行榜上与其余7支参赛团队进行了对比，同时也与官方基线“RoboSense2025”进行了比较。</p>
<p><strong>关键实验结果</strong>：<br>在挑战赛官方排行榜上，本文方法（Xiaomi EV-AD VLA）取得了第二名的成绩。具体指标为：Recall@1达到31.33%，Recall@5达到49.09%，Recall@10达到57.15%。</p>
<p><img src="https://arxiv.org/html/2510.02728v2/x1.png" alt="定性结果"></p>
<blockquote>
<p><strong>图2</strong>：定性结果。展示了VLM为三张代表性无人机视角图像生成的描述样例。描述能够准确捕捉细粒度的空间细节和语义关系，例如识别足球场的标记、区分带有太阳能板的停车场和附近的体育设施等，验证了生成描述与人类书面查询自然对齐的能力。</p>
</blockquote>
<p>与官方基线“RoboSense2025”相比，本文方法在R@1、R@5、R@10上分别取得了约5.89%、8.48%和8.05%的绝对提升。尽管冠军团队在R@1上表现更优（38.31%），但本文方法相比其他队伍（如第三名）展现了更均衡的性能，在排名靠前的位置保持了较高准确率，这对于实际无人机导航（通常依赖最靠前的预测结果）至关重要。</p>
<p><strong>消融实验与组件贡献</strong>：论文虽未展示完整的消融实验表格，但通过设置不同的融合权重α进行了验证实验。最终选择α=0.3，这表明细粒度语义相似度（基于描述）对最终性能的贡献权重（0.7）高于粗粒度视觉相似度（0.3），凸显了描述引导重排序机制的核心作用。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个新颖的两阶段、由粗到精的检索框架CGRS，利用视觉语言模型生成描述，以弥合自然语言查询与空中图像之间的语义鸿沟。</li>
<li>引入了一种描述引导的重排序机制，将跨模态匹配转化为文本到文本的相似度计算，实现了复杂空中场景下更细粒度的语义对齐。</li>
<li>在IROS 2025 RoboSense挑战赛中取得了Top-2的成绩，相比基线在关键指标上实现了约5-8%的稳定提升，证明了该语义细化策略在实际机器人导航场景中的实用价值。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，整个描述生成过程需要约12小时的离线处理时间。这表明该方法在实时性要求极高的在线导航场景中可能存在延迟。此外，重排序的效果依赖于VLM生成描述的质量和句子编码模型的性能。</p>
<p><strong>后续研究启示</strong>：本文工作表明，利用大规模视觉语言模型提供的显式语言表示，可以作为空间复杂空中环境下跨模态理解的有效媒介。未来研究可以探索更轻量、更快速的描述生成模型，或研究如何将描述生成过程更高效地集成到端到端的训练框架中，以平衡性能与推理速度。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对跨模态无人机导航中文本查询与视觉内容的细粒度语义匹配难题，提出一种两阶段检索优化方法：Caption-Guided Retrieval System。该方法首先利用基线模型获得查询相关度前20的初始粗排图像，随后通过视觉语言模型为候选图像生成详细描述，最后在多模态相似度计算框架中，利用生成描述对原始查询进行细粒度重排序，构建视觉与语言间的语义桥梁。实验表明，该方法在Recall@1/5/10等关键指标上均比基线持续提升5%，并在竞赛中取得前三名，验证了语义细化策略在实际机器人导航场景中的有效性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.02728" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>