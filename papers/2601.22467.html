<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.22467" target="_blank" rel="noreferrer">2601.22467</a></span>
        <span>作者: Shi, Jiaqi, Zhang, Xulong, Qu, Xiaoyang, Wang, Jianzong</span>
        <span>日期: 2026/01/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过整合视觉感知、语言理解和运动控制，在机器人任务执行中展现出潜力。然而，这些模型的发展严重依赖于预训练阶段大规模的动作监督（如关节角度、末端执行器轨迹），其获取成本高昂且难以扩展至多样化的机器人形态或跨领域数据集。为了追求可扩展性，一个研究方向是利用丰富的、无动作标注的视频数据。以Genie、LAPA、Moto和COMO为代表的方法，通过训练潜在动作模型（LAM）从视频帧间差异中学习离散或连续的潜在动作表示。然而，现有方法存在三个关键挑战：1）基于VQ-VAE的LAM的量化误差和码本崩溃等问题会直接传播给VLA模型；2）由于训练目标集中于下一帧重建，推断出的潜在动作仅压缩了连续帧间的差异，即使这些差异并非由控制动作引起，导致缺乏显式的动作编码；3）单一的重建目标和LAM结构容易使模型退化为未来帧预测器，而非潜在动作建模器，即陷入“捷径学习”。</p>
<p>本文针对上述痛点，提出了一种新的预训练策略CARE。其核心思路是：将LAM训练无缝集成到视觉语言模型（VLM）的预训练流程中，并引入包含关键点轨迹预测的多任务学习目标，仅使用视频-文本对进行无监督预训练，以学习更具语义解释性、能避免捷径学习的连续潜在动作表示。</p>
<h2 id="方法详解">方法详解</h2>
<p>CARE的训练流程包含两个阶段：1）使用视频-文本对进行无监督预训练，学习具有潜在动作预测能力的VLM；2）使用少量带动作标签的机器人数据对有监督的动作头进行微调。</p>
<p><img src="https://arxiv.org/html/2601.22467v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：CARE方法整体框架。左侧(a)展示了传统VLA训练需要大量动作标签；右侧(b)展示了本文提出的CARE方法，仅使用指令-图像对作为输入，通过多任务学习使VLM能够预测潜在动作。</p>
</blockquote>
<p><strong>整体架构与预训练输入</strong>：CARE的潜在视觉语言模型（VLM）基于Prismatic-7B VLM构建，包含视觉编码器、投影器和7B参数的LLM主干。预训练时，输入为当前观测图像 (x_{img}) 和自然语言任务指令提示 (p)。模型在 (p) 后附加潜在动作占位符，并预测动作维度。视觉编码器结合了SigLIP和DinoV2两个预训练模型，提取的特征拼接后经一个两层MLP投影，与文本嵌入拼接，共同输入LLM主干。LLM最终隐藏状态中对应于动作维度的部分，被用作连续潜在动作表示 (z)，用于后续的多任务学习。</p>
<p><strong>核心模块：多任务学习</strong>：这是CARE的核心创新。为了增强潜在动作的显式表示并防止捷径学习，模型同时优化两个解码任务：</p>
<ol>
<li><strong>下一帧特征预测</strong>：输入当前帧视觉特征 (f_v^t) 和潜在动作 (z)，通过一个交叉注意力模块融合后，由帧解码器预测下一帧的视觉特征 (f_v^{t+1})。损失为 (\mathcal{L}_f^t = MSE(f_v^{t+1}, \hat{f}_v^{t+1}))。</li>
<li><strong>关键点轨迹预测</strong>：输入当前帧中均匀分布的256个点的二维坐标 (k_t) 和潜在动作 (z)，通过另一个交叉注意力模块融合后，由点解码器预测这些点在下一帧的坐标 (k_{t+1})。其中，(k_t) 和 (k_{t+1}) 由预训练模型Co-Tracker获取。损失为 (\mathcal{L}_p^t = MSE(k^{t+1}, \hat{k}^{t+1}))。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.22467v1/pics/dataset.png" alt="预训练与微调数据集"></p>
<blockquote>
<p><strong>图2</strong>：CARE预训练与微调阶段使用的数据集示意图。预训练阶段使用机器人视频（如Open X-Embodiment）和人类活动视频（如Something-Something v2）的混合数据；微调阶段仅使用RT-1数据集中一小部分（3%）带动作标签的数据。</p>
</blockquote>
<p><strong>多任务损失函数</strong>：采用基于不确定性的加权损失（UWL）框架自适应地结合两个任务的损失：(\mathcal{L}^t = \frac{1}{\sigma_1^2}\mathcal{L}_f^t + \frac{1}{\sigma_2^2}\mathcal{L}_p^t + \log\sigma_1 + \log\sigma_2)，其中 (\sigma_1) 和 (\sigma_2) 作为可学习参数，动态调整两个损失的相对权重。</p>
<p><strong>微调阶段</strong>：预训练得到的潜在VLM不能直接控制机器人。因此，需要在一个包含真实动作标签的小规模机器人演示数据集（如RT-1的3%数据）上，通过LoRA方式微调模型，并附加一个轻量级残差MLP作为动作头。该动作头以解码器输出的隐藏表示为输入，回归真实值动作，训练目标为预测动作与真实动作之间的L1回归损失。</p>
<p><strong>创新点</strong>：1) <strong>流程集成</strong>：将LAM训练集成到VLM预训练中，将传统VLA训练的四阶段（视觉编码器预训练、VLM预训练、LAM训练、VLA微调）简化为三阶段（VLM多任务预训练、动作头微调）。2) <strong>多任务目标</strong>：引入关键点轨迹预测作为辅助任务，迫使模型关注动作驱动的点位变化，而不仅是像素级外观，从而提升动作表示的显式性和语义性。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在LIBERO基准的四个任务套件（Spatial, Object, Goal, Long）上进行评估。预训练数据集混合了约14万条Open X-Embodiment机器人轨迹和约10万段Something-Something v2人类活动视频片段。微调使用RT-1数据集的3%数据。对比的基线方法包括需要动作标签预训练的OpenVLA、Octo、Diffusion Policy、MDT，以及无需动作标签的LAPA和CoMo。</p>
<p><strong>关键实验结果（性能）</strong>：如表1所示，CARE在不使用动作标签预训练的方法中取得了最高的平均成功率（77.7%），甚至超过了部分使用动作标签预训练的方法（如OpenVLA的75.0%， Octo的73.7%）。特别是在Goal和Long任务上，分别比OpenVLA高出1.4%和12.5%。实验还表明，随着预训练数据规模的增加（从仅用人或桥接数据到结合RT-1数据），CARE的性能持续提升，符合缩放定律。</p>
<p><strong>关键实验结果（可解释性与捷径学习）</strong>：</p>
<ul>
<li><strong>线性探测（LP-MSE）</strong>：如表2所示，CARE的潜在动作在Goal、Spatial和Long任务上的LP-MSE均低于CoMo和LAPA，表明其潜在动作与真实动作的线性映射关系更强，可解释性更好。例如在Goal任务上，LP-MSE比CoMo降低22.8%，比LAPA降低47.8%。</li>
<li><strong>语义标签预测</strong>：如表3所示，仅使用初始帧和后续9个由CARE潜在动作表示的帧，语义标签预测准确率达到84.2%，优于使用LAPA（64.1%）和CoMo（71.2%）潜在动作的结果，且接近使用真实连续帧（80.4%）的性能，证明了CARE潜在动作蕴含丰富的语义信息。</li>
<li><strong>防止捷径学习（S-PCFC）</strong>：如表2所示，CARE在所有任务上的S-PCFC分数均显著低于LAPA和CoMo（分数越低越好）。例如在Goal任务上，CARE为0.833，而LAPA接近1（0.980），CoMo为0.899。这表明基于VQ-VAE的LAPA方法极易陷入捷径学习，CoMo部分缓解，而CARE通过其预训练设计从根本上更有效地避免了该问题。</li>
</ul>
<p><strong>消融实验（多任务学习）</strong>：如表4所示，同时进行下一帧预测和关键点轨迹预测的多任务预训练模型，取得了77.7%的成功率，显著高于仅进行下一帧预测（67.8%）或仅进行关键点轨迹预测（54.3%）的单任务模型。这证实了多任务学习的协同效益（1+1 &gt; 2），关键点轨迹预测任务有效辅助了模型学习更好的动作表示。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了CARE框架，将潜在动作模型训练无缝集成到VLM预训练流程中，简化了训练阶段并减少了对动作标注的依赖；2) 设计了包含关键点轨迹预测的多任务学习目标，使学习到的连续潜在动作表示具有更强的显式动作编码能力和语义可解释性；3) 通过实验验证了该方法在任务成功率、潜在动作可解释性以及避免捷径学习方面均优于已有的无监督潜在动作学习方法。</p>
<p>论文自身提到的局限性在于，与使用动作标签进行预训练的方法相比，仍存在性能差距。作者指出未来的工作方向包括引入动作分块（action chunking）和多维度感知视角，以进一步缩小这一差距。这项工作为利用海量无标注视频数据训练高性能、可解释的机器人VLA模型提供了新的有效途径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作（VLA）模型在机器人控制中依赖动作监督、可扩展性差的核心问题，提出CARE框架。该方法仅利用视频-文本对进行多任务预训练，通过新设计的目标学习连续潜在动作表示，无需动作标注；微调时使用少量标注数据训练动作头。实验表明，CARE在多个模拟任务中具有更高的成功率、语义可解释性，并能避免捷径学习，验证了其在弱监督下的有效性和可扩展性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.22467" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>