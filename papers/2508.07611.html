<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.07611" target="_blank" rel="noreferrer">2508.07611</a></span>
        <span>作者: Wang, Zifan, Yang, Xun, Zhao, Jianzhuang, Zhou, Jiaming, Ma, Teli, Gao, Ziyao, Ajoudani, Arash, Liang, Junwei</span>
        <span>日期: 2025/08/11</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前人形机器人运动控制的主流方法主要分为两类：一是仅依赖本体感知的“盲”控制器，虽在平坦地形上表现出色，但无法应对环境障碍；二是基于深度相机生成2D高度图的方法，但其对光照敏感、视野有限，且将3D信息压缩为2D地图会导致机器人无法感知悬空障碍物或行人上半身，带来碰撞风险。此外，现有方法通常通过奖励函数中的碰撞惩罚来确保安全，这种方法脆弱且难以调节，容易导致行为过于保守或不安全；同时，极少有运动策略考虑人机交互中的心理舒适度，即运动的可预测性、流畅性和非威胁性。</p>
<p>本文针对在杂乱、动态的人居环境中实现鲁棒、安全且舒适导航的痛点，提出了一个集成的端到端框架。核心思路是：将控制问题建模为约束马尔可夫决策过程，以形式化分离安全与任务目标；创新地将基于模型的<strong>控制屏障函数</strong>原理转化为模型无关RL算法中的成本函数，以实施安全约束；并引入一套基于人机交互研究的舒适导向奖励，以生成社会意识行为。</p>
<h2 id="方法详解">方法详解</h2>
<p>整体框架旨在学习一个在形式化定义的安全与舒适状态空间内操作的舒适策略。该方法将感知、安全约束和策略学习集成到一个端到端模型中，通过将基于CBF的安全条件转化为CMDP的成本函数，并使用P3O算法求解，以在满足所有约束的同时最大化任务目标。</p>
<p><img src="https://arxiv.org/html/2508.07611v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：提出的训练框架总览。原始传感器数据（LiDAR点云、本体状态、指令）由<strong>行动者</strong>（策略网络）处理。<strong>评论者</strong>网络在训练时接收额外特权信息。策略通过P3O算法进行优化，同时考虑任务奖励和安全/舒适成本。</p>
</blockquote>
<p><strong>网络架构与输入</strong>：策略网络（Actor）的输入包括：1）<strong>本体感知与指令历史</strong>：包含过去10个时间步的关节位置/速度/加速度、上一动作、基座线/角速度、基座坐标系下的重力向量、基座高度和用户指令（x/y线速度、偏航角速度）；2）<strong>外部感知特征</strong>：从原始LiDAR点云提取的64维嵌入向量，用于捕获周围几何信息。这些输入首先由GRU处理LiDAR特征历史以提取时序模式，其输出与扁平化的本体历史拼接后，送入一个MLP，构成行动者和评论者网络的主体。评论者（Critic）在训练时额外接收特权信息，如最近障碍物在8个方向上的真实距离与速度、全身连杆接触力、关节限位和安全条件等。</p>
<p><strong>通过LDCBF成本强制执行安全状态空间</strong>：为确保学得策略是安全策略，需防止其进入不安全状态空间。安全状态空间的边界通过强制执行与障碍物的最小距离来定义，并使用<strong>线性离散时间控制屏障函数</strong>进行形式化。LDCBF函数 <code>h_D(s_k)</code> 定义为机器人位置到障碍物安全偏移面的有符号距离。对于线性系统模型，一步前瞻安全条件 <code>h_D(s_k+1) ≥ (1-γ_CBF) h_D(s_k)</code> 可以写为对控制输入 <code>u_k</code> 的线性约束 <code>G_D(s_k, u_k) ≥ 0</code>。由于本框架是模型无关的，不直接使用此约束过滤动作，而是将其违反情况转化为CMDP的瞬时成本函数：<code>C_D(s_k, u_k) = max{0, -G_D(s_k, u_k)}</code>。该成本仅在所选动作预计会违反安全屏障时才为正。</p>
<p><strong>通过奖励和成本学习舒适策略</strong>：策略在CMDP框架内训练，其中奖励函数引导智能体以舒适、社会可接受的方式完成任务，而成本函数定义了硬性的安全和操作约束。<strong>奖励函数</strong>是任务导向奖励和舒适导向奖励的加权和。<strong>舒适导向奖励</strong>是人机交互研究的核心体现：1）<strong>近体舒适奖励</strong>：鼓励机器人维持与行人约1.2米的社会理想距离；2）<strong>安全接近速度/加速度惩罚</strong>：惩罚朝向最近障碍物表面法向的速度和加速度分量，阻止机器人直接快速冲向障碍物；3）<strong>切向规避奖励</strong>：鼓励机器人速度方向与最近障碍物方向垂直，促进平滑的弧线规避而非急停。此外，运动平滑性项惩罚高关节加速度以确保整体运动流畅。<strong>成本函数</strong>定义了不安全状态空间的硬边界，用于CMDP约束，包括安全距离违反（距离&lt;0.8米）、关节限位违反和自碰撞，这些条件必须严格避免。</p>
<p><strong>使用P3O进行训练</strong>：为找到在满足安全与物理成本约束下最大化任务奖励的最优策略，采用了<strong>归一化惩罚近端策略优化</strong>算法。P3O在PPO目标基础上，为每个约束违反添加了惩罚项，其损失函数包含标准PPO裁剪目标（使用归一化奖励优势）和每个成本的违反项，后者将瞬时成本与策略更新联系起来。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在<strong>NVIDIA Isaac Sim</strong>高保真仿真环境和<strong>Unitree G1</strong>实体人形机器人上进行，机器人配备Livox Mid-360 LiDAR。评估旨在测试策略在复杂3D环境中的鲁棒导航、安全执行以及生成舒适、社会意识运动的能力。</p>
<p><strong>消融实验与基线对比</strong>：对比了三种策略：1) **Ours (P3O-CBF)**：完整方法；2) <strong>PPO-RewardShaping</strong>：仅通过负奖励惩罚接近障碍物来尝试安全；3) <strong>P3O</strong>：与P3O-CBF相同，但无舒适导向奖励。</p>
<p><img src="https://arxiv.org/html/2508.07611v1/Figure/AlationStudy.png" alt="轨迹对比"></p>
<blockquote>
<p><strong>图3</strong>：静态障碍课程中消融研究的轨迹定性对比。PPO-RewardShaping（橙色）轨迹激进且易失败；P3O（绿色）因有显式安全约束而改善，但路径仍接近障碍物；P3O-CBF（蓝色）轨迹更平滑、安全，主动启动规避，保持更大安全边际。</p>
</blockquote>
<p>表II量化了各策略在6米长随机静态障碍场景中10次测试的安全与舒适违规时间（秒）。结果表明，仅靠奖励塑造无法确保鲁棒的安全和舒适。标准P3O策略通过显式安全约束将处于不安全空间（<code>D_obs &lt; 0.6m</code>）的时间减少了约30%。提出的P3O-CBF方法性能最佳，不仅将处于不安全空间的时间减少了53%（相比PPO），还显著减少了处于不舒适但安全空间（<code>0.6 ≤ D_obs &lt; 1.2m</code>）的时间，表明舒适导向奖励成功鼓励策略保持更大、更社会可接受的距离。</p>
<p><strong>多样化评估场景</strong>：在四个挑战性仿真场景中评估策略性能：a) <strong>悬空障碍</strong>（测试全身感知）；b) <strong>狭窄通道</strong>（测试精确横向控制）；c) <strong>杂乱静态课程</strong>（测试复杂环境路径规划）；d) <strong>动态智能体</strong>（测试在拥挤互动空间中的反应性规避）。</p>
<p><img src="https://arxiv.org/html/2508.07611v1/Figure/experiment_scene.png" alt="评估场景"></p>
<blockquote>
<p><strong>图4</strong>：多样化挑战性评估场景示意图。红色同心圆代表LiDAR传感器视场。(a)悬空障碍；(b)狭窄通道；(c)杂乱静态课程；(d)动态智能体。</p>
</blockquote>
<p>表III展示了各策略在每个场景30次试验中的成功率。完整P3O-CBF框架在最具挑战性的场景中优势明显，尤其在狭窄通道和动态智能体场景中成功率显著高于基线。</p>
<p><strong>真实世界部署</strong>：经过训练的策略成功迁移到实体Unitree G1机器人上，在杂乱实验室环境中实现了对静态和动态3D障碍物的敏捷、安全规避，验证了仿真到现实的转移能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一个<strong>LiDAR驱动的端到端策略</strong>，直接处理原始3D点云以导航复杂环境，克服了“盲”控制器和基于2D视觉方法的局限性；2）提出了一个<strong>基于约束强化学习的原理性安全框架</strong>，创新地将模型相关的CBF原理转化为模型无关RL算法（P3O）中的成本函数，实现了鲁棒的安全执行；3）设计了一套<strong>舒适导向的奖励结构</strong>，明确纳入人机交互研究洞察，以生成社会意识运动；4）成功在实体人形机器人上实现了<strong>仿真到现实的部署</strong>。</p>
<p>论文自身提到的局限性主要在于方法中对<strong>线性系统模型</strong>的假设，这可能会限制其在高度非线性动力学系统上的直接适用性。这项工作为后续研究提供了重要启示：将基于模型的强安全保证（如CBF）与模型无关的学习框架（如RL）相结合，是实现复杂机器人系统在动态现实中安全部署的有效途径；同时，将人机交互中的心理因素（如舒适度、可预测性）量化为可优化的目标，对于开发真正能与人类和谐共处的机器人至关重要。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在非结构化环境中导航时，现有方法存在环境感知不足或无法处理3D障碍的问题，同时难以兼顾安全性和运动舒适性。核心方案是提出一个端到端运动策略，直接将原始时空LiDAR点云映射到电机指令。关键技术包括：1）将控制问题构建为约束马尔可夫决策过程（CMDP），以分离安全与任务目标；2）创新地将控制障碍函数（CBF）原理转化为CMDP中的成本，并利用惩罚近端策略优化（P3O）在训练中强制执行安全约束；3）引入基于人机交互研究的舒适导向奖励。核心实验结论是：该方法成功实现了模拟到现实的迁移，物理机器人能够灵活安全地避开静态和动态3D障碍。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.07611" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>