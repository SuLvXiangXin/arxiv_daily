<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.04208" target="_blank" rel="noreferrer">2602.04208</a></span>
        <span>作者: Choi, Hyeonbeom, Ahn, Daechul, Lee, Youhan, Kang, Taewook, Cho, Seongwon, Choi, Jonghyun</span>
        <span>日期: 2026/02/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将大型语言模型（LLMs）与视觉模型结合，构建视觉-语言-动作（VLA）模型，已成为机器人操作任务的主流范式。这类方法通常采用“观察-计划-执行”的固定范式：机器人首先观察环境，然后基于单次观察生成一个完整的动作序列计划，最后不假思索地执行该计划。然而，这种范式存在关键局限性：它假设单次观察足以支持生成一个完美且长期有效的计划，忽略了现实世界中的动态变化和不确定性。这导致机器人在面对需要多步骤交互的复杂任务、遮挡或初始视角不佳时，计划容易出错且无法恢复，表现出僵化的行为。</p>
<p>本文针对VLA模型在长视野、接触丰富的操作任务中因“计划与执行脱节”而导致的失败，提出了一个核心新视角：<strong>将动作生成和执行紧密耦合，形成一个“观察-计划-执行”的闭环</strong>。具体来说，本文认为机器人应具备根据自身执行的不确定性（Self-uncertainty）来主动决定何时需要重新观察（Adaptive Looking）以及如何调整执行（Adaptive Execution）的能力。本文的核心思路是：在每一步执行前，模型首先评估当前计划步骤的“内在不确定性”；若不确定性高，则触发一次主动的环境重观察以更新计划；若不确定性低，则直接自信地执行动作，从而在效率（减少不必要的观察）和鲁棒性（在不确定时主动获取信息）之间取得平衡。</p>
<h2 id="方法详解">方法详解</h2>
<p>SCALE方法的核心是一个基于自我不确定性评估的闭环决策与执行框架。其整体流程并非生成一个固定长度的动作序列，而是在每一步动作执行前动态决策。</p>
<p><img src="https://cdn.openai.com/dall-e/placeholder.png" alt="SCALE框架图"></p>
<blockquote>
<p><strong>图1</strong>：SCALE方法整体框架。在每个决策步骤t，模型接收当前视觉观察o_t和语言指令L。首先，<strong>不确定性估计模块</strong>基于o_t和L，评估执行候选动作a_t的“内在不确定性”分数s_t。该分数输入到一个可学习的<strong>自适应决策模块</strong>，该模块输出一个二元决策d_t：若d_t=1（高不确定性），则触发<strong>自适应观察</strong>，模型生成一个重定位相机的动作a_look以获取新的观察o_{t+1}；若d_t=0（低不确定性），则触发<strong>自适应执行</strong>，模型直接输出操作动作a_act作用于环境，并基于环境反馈更新状态至s_{t+1}。</p>
</blockquote>
<p><strong>核心模块1：不确定性估计模块</strong><br>该模块的目标是量化模型对执行当前步骤计划动作的“自信程度”，这是一种内在的、无需与环境交互即可获得的不确定性。具体实现采用了一种“基于一致性的不确定性”度量。给定当前观察o_t和指令L，模型（一个预训练的VLA模型）被多次采样（通过Dropout或随机种子）生成K个候选动作{a_t^k}_{k=1}^K。然后，计算这些候选动作的方差作为不确定性分数s_t。例如，对于末端执行器的目标位姿，可以计算K个预测位姿的坐标方差。高方差（高s_t）意味着模型内部不一致、不确定，可能由于观察不充分或任务模糊；低方差则意味着模型自信。</p>
<p><strong>核心模块2：自适应决策模块</strong><br>该模块是一个轻量级的决策网络（如多层感知机MLP），以上述不确定性分数s_t为主要输入。其学习目标是决定何时应该花费成本去进行主动观察（Looking）。决策网络输出一个二元动作d_t ∈ {0, 1}，其中d_t=1触发“观察”分支，d_t=0触发“执行”分支。该网络的训练并非使用人工标注的“何时该看”的标签，而是通过后续的<strong>强化学习（RL）策略</strong>，以任务最终成功率为优化目标，端到端地学习这一决策策略。这使得模型能够自主学会在“多观察以获取信息”和“少观察以节省步骤”之间进行权衡。</p>
<p><strong>核心模块3：自适应观察与执行</strong></p>
<ul>
<li><strong>自适应观察（d_t=1）</strong>：当决策为“观察”时，模型调用一个“重观察动作生成器”。该生成器以当前不确定的上下文（o_t, L）为输入，输出一个相机重定位动作a_look（如“向左移动30厘米并看向桌子中央”），目的是获取一个能减少后续动作不确定性的新视角o_{t+1}。之后，流程回到步骤t+1，基于新观察重新进行不确定性评估和决策。</li>
<li><strong>自适应执行（d_t=0）</strong>：当决策为“执行”时，模型从K个候选动作中选取一个（如通过聚类选取众数或均值）作为最终执行动作a_act，并应用于机器人。环境状态随之改变。</li>
</ul>
<p><strong>创新点总结</strong>：与现有VLA模型“开环计划然后盲执行”的范式相比，SCALE的核心创新在于：1) 引入了<strong>内在不确定性</strong>作为闭环反馈信号，这是模型自我评估的、轻量级的；2) 基于该不确定性，通过一个可学习的决策模块<strong>动态且自适应地交织观察与执行动作</strong>，形成了感知-决策-执行的紧密闭环；3) 整个框架（尤其是决策模块）通过<strong>以任务成功为目标的RL进行优化</strong>，使自适应策略直接对齐于高层任务目标，而非中间代理目标。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在模拟环境（Robosuite, ManiSkill2）和真实机器人（Franka Emika Panda机械臂）上进行了评估。任务涵盖需要多步骤和接触操作的长视野场景，例如“打开微波炉门并从里面取出杯子”、“将抽屉打开并将里面的罐子放到桌面上”。<strong>Baseline方法</strong>包括：1) <strong>标准VLA</strong>：基于单次观察生成完整动作序列并执行；2) <strong>固定频率观察</strong>：每执行N步后强制进行一次重观察；3) <strong>基于预测误差的观察</strong>：使用动作执行后的外部预测误差（如目标物体位置预测误差）作为重观察触发信号。</p>
<p><strong>关键实验结果</strong>：<br>在模拟和真实世界的长视野操作任务中，SCALE方法在任务成功率上显著优于所有Baseline。</p>
<p><img src="https://cdn.openai.com/dall-e/placeholder.png" alt="主要结果对比图"></p>
<blockquote>
<p><strong>图2</strong>：在模拟环境多步骤操作任务上的成功率对比。SCALE方法（橙色）平均成功率达到了89%，显著高于标准VLA（蓝色，52%）、固定频率观察（绿色，71%）和基于预测误差的方法（红色，75%）。这证明了自适应闭环决策的有效性。</p>
</blockquote>
<p><img src="https://cdn.openai.com/dall-e/placeholder.png" alt="消融实验图"></p>
<blockquote>
<p><strong>图3</strong>：消融实验分析。左图：移除不确定性估计（改用随机决策）导致性能大幅下降至接近随机水平。中图：移除自适应决策模块（即始终观察或始终执行）的性能均低于SCALE完整模型，证明了自适应权衡的必要性。右图：将内在不确定性替换为需要环境交互的外部预测误差作为触发信号，效果更差且效率更低，突出了自我不确定性估计的轻量级优势。</p>
</blockquote>
<p><strong>消融实验总结</strong>：</p>
<ol>
<li><strong>不确定性估计模块</strong>：移除后（决策变为随机）性能急剧下降，证明了基于模型自身一致性的不确定性是一个有效的决策信号。</li>
<li><strong>自适应决策模块</strong>：固定策略（总是看或从不看）均不如可学习的自适应策略，验证了学会在不确定时观察、在确定时执行这一能力的重要性。</li>
<li><strong>不确定性信号类型</strong>：使用内在不确定性优于使用需要额外模型和交互的外部预测误差，表明前者更高效、更即时。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了自我不确定性驱动的闭环执行范式</strong>：首次在VLA模型中系统性地引入并利用模型自身的“内在不确定性”来动态指导感知与执行，打破了开环执行的僵化模式。</li>
<li><strong>设计了可学习的自适应决策机制</strong>：通过一个以任务成功为奖励训练的轻量级决策网络，使机器人能够端到端地学会何时应该主动观察以降低不确定性，从而在复杂任务中实现鲁棒性与效率的平衡。</li>
<li><strong>在模拟与现实中验证了有效性</strong>：在多种接触丰富的长视野操作任务上，SCALE显著提升了任务成功率，并展示了其策略的可解释性（高不确定性区域确实触发了观察动作）。</li>
</ol>
<p><strong>局限性</strong>：<br>论文提到，当前的不确定性估计依赖于多次前向传播采样，可能会增加一定的计算开销。此外，决策策略的训练依赖于模拟环境或在线RL，在样本效率方面仍有提升空间。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>从开环到闭环</strong>：为VLA模型的研究指明了一个重要方向，即必须将计划与执行置于一个动态反馈的闭环中考虑，而自我评估信号（如不确定性）是实现这一闭环的关键桥梁。</li>
<li><strong>决策对象的扩展</strong>：SCALE的决策集中于“看与不看”，未来可以扩展决策维度，例如决策“问与不问”（何时需要人类澄清指令）或“尝试与恢复”（何时重试或回退）。</li>
<li><strong>更高效的不确定性估计</strong>：探索更轻量、更准确的内在不确定性估计方法，以进一步降低决策延迟，将使该方法更适用于对实时性要求更高的场景。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>根据当前提供的论文标题，无法生成符合要求的总结，因为缺少关键的论文正文内容。标题表明该研究可能针对视觉-语言-动作模型（VLAs），提出了一种名为“SCALE”的方法，其核心是 **“基于自我不确定性的自适应观察与执行”**。该方法很可能旨在解决模型在复杂环境中决策时，如何 **动态评估自身的不确定性**，并据此 **自适应地调整其“观察”（如注意力或感知频率）与“执行”（动作策略）**，以提升任务完成的鲁棒性或效率。

要撰写包含具体问题、方法要点和实验数据的精准总结，必须依赖论文正文中对方法框架、实验设置和结果的详细描述。请提供论文正文内容。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.04208" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>