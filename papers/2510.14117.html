<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ViTacGen: Robotic Pushing with Vision-to-Touch Generation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ViTacGen: Robotic Pushing with Vision-to-Touch Generation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.14117" target="_blank" rel="noreferrer">2510.14117</a></span>
        <span>作者: Shan Luo Team</span>
        <span>日期: 2025-10-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人推动是一项基础性操作任务，需要精确感知末端执行器与物体间的接触力与动力学。当前主流方法主要依赖视觉或触觉输入。纯视觉策略难以捕捉精细的接触动力学，而依赖真实触觉传感器的策略则面临硬件成本高、易损坏、需要精确校准以及不同传感器间存在制造差异等挑战，导致部署困难和性能下降。本文针对真实高分辨率触觉传感器难以依赖这一具体痛点，受人类从视觉推断触觉状态的能力启发，提出了一种新视角：通过视觉生成触觉。本文核心思路是提出一个名为ViTacGen的机器人操作框架，通过一个视觉到触觉的生成网络从视觉序列合成标准化的触觉表示（接触深度图），并利用强化学习策略融合视觉与生成的触觉观察，从而实现在仅配备视觉的机器人系统上进行有效的零样本部署。</p>
<h2 id="方法详解">方法详解</h2>
<p>ViTacGen框架包含两个核心组件：用于视觉到触觉生成的编码器-解码器网络VT-Gen，以及基于视觉与生成触觉观察进行特征融合与对比学习的强化学习策略网络VT-Con。整个流程分为两个顺序阶段进行训练，以实现从仿真训练到真实世界视觉系统零样本部署。</p>
<p><img src="https://arxiv.org/html/2510.14117v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：ViTacGen工作流程。包含两个组件：(a) 在仿真中，利用预训练的RL网络收集专家轨迹中的配对视觉-触觉数据，用于训练VT-Gen网络，使其能从视觉输入生成作为标准化触觉表示的接触深度图像；(b) 结合冻结的VT-Gen，基于本体感知（机器人TCP坐标）、视觉和生成的触觉观察，训练目标条件RL策略VT-Con。</p>
</blockquote>
<p><strong>VT-Gen：视觉到触觉生成网络</strong>。给定一个视觉图像序列𝒱，VT-Gen的目标是生成当前的触觉接触深度图像𝒄^𝑔𝑒𝑛。具体而言，首先将𝒱沿通道维度拼接，并通过从粗到精的编码提取分层多尺度视觉特征。过程包括：1）通过粗编码器ℰ_𝑐𝑜𝑎𝑟𝑠𝑒提取初始视觉特征𝒇^𝑣_𝑐𝑜𝑎𝑟𝑠𝑒；2）使用跨模态注意力操作𝒜_𝑐𝑚（公式3-5）处理该特征，得到𝒇^𝑣_𝑐𝑚；3）通过精编码器ℰ_𝑟𝑒𝑓𝑖𝑛𝑒进一步提炼，得到𝒇^𝑣_𝑟𝑒𝑓𝑖𝑛𝑒；4）经过一系列残差块增强特征表示；5）最终通过一个由转置卷积层和上采样操作组成的分层解码器，生成最终的接触深度预测𝒄^𝑔𝑒𝑛。生成的𝒄^𝑔𝑒𝑛被重复N次以形成与视觉序列等长的生成触觉序列𝒞，供VT-Con使用。该网络使用VGG感知损失（公式8）进行监督训练。</p>
<p><strong>VT-Con：基于视觉-触觉对比学习的强化学习策略</strong>。该模块接收视觉序列𝒱和生成的触觉序列𝒞（均取最近3帧以捕获速度信息）。首先，使用两个结构相同的CNN编码器ℰ^𝑣和ℰ^𝑐分别提取视觉特征𝒇^𝑣和触觉特征𝒇^𝑐。</p>
<p><img src="https://arxiv.org/html/2510.14117v2/x2.png" alt="对比学习模块"></p>
<blockquote>
<p><strong>图2</strong>：VT-Con中的对比学习模块。使用两个动量编码器获取动量的视觉和触觉特征，并通过InfoNCE损失将原始特征与另一模态的动量特征进行对齐，以优化视觉和触觉编码器。</p>
</blockquote>
<p>其次，为了更有效地融合特征，引入了基于动量对比（MoCo）的跨模态对比学习。如图2所示，使用动量编码器ℳ^𝑣和ℳ^𝑐（通过动量更新公式6与原始编码器同步）从相同输入计算动量特征𝒎^𝑣和𝒎^𝑐。然后使用InfoNCE损失（公式9-11）对齐𝒇^𝑣与𝒎^𝑐，以及𝒇^𝑐与𝒎^𝑣，从而在RL训练过程中联合优化编码器。</p>
<p>最后，通过一个与公式3类似的基于注意力的视觉-触觉特征融合操作（公式7）将𝒇^𝑣和𝒇^𝑐融合为𝒇^𝑓𝑢𝑠𝑒。融合后的特征与处理后的机器人TCP坐标（本体感知）拼接，形成完整的观察向量，输入到SAC等RL算法中学习策略。</p>
<p><strong>创新点</strong>：1) 提出了“视觉生成触觉”的新范式，用生成的、标准化的触觉深度图替代真实传感器，从根本上避免了真实触觉传感器的硬件与校准问题；2) 在强化学习策略中集成跨模态对比学习，增强了视觉与生成触觉特征之间的对齐与融合效果；3) 整个框架支持零样本迁移到仅配备视觉的真实机器人系统。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在Tactile Gym 2仿真平台和真实世界中进行。使用了YCB数据集中的茶叶盒、肉罐头、杯子三个物体进行训练和仿真测试，并选取了五个未见过的真实物体（橄榄罐、苹果、咖啡罐、汤罐、陶瓷杯）进行零样本测试。机器人平台为UR5e机械臂，使用外部RGB相机。评估指标包括生成质量（PSNR, SSIM, LPIPS）和推动任务性能（累积奖励、最终距离误差、成功率）。</p>
<p><img src="https://arxiv.org/html/2510.14117v2/x3.png" alt="实验物体"></p>
<blockquote>
<p><strong>图3</strong>：实验使用的真实物体：(a)-(c)为训练/仿真测试物体，(d)为用于零样本测试的未见物体。</p>
</blockquote>
<p><strong>视觉到触觉生成效果</strong>：如表I所示，VT-Gen在生成触觉接触深度图方面表现良好，PSNR最高达30.75（茶叶盒），SSIM最高达0.9482，LPIPS最低至0.0101。图4(ii)(iii)的定性结果也显示生成的触觉图与真实触觉图高度相似。</p>
<p><img src="https://arxiv.org/html/2510.14117v2/x4.png" alt="生成与对比结果"></p>
<blockquote>
<p><strong>图4</strong>：(i) 输入视觉图像序列（3帧）；(ii) 生成的触觉接触深度图；(iii) 真实的触觉接触深度图（GT）；(iv) 使用真实触觉的ViTacGen特征相似图；(v) 使用生成触觉的ViTacGen特征相似图。特征相似图表明，使用生成触觉的模型能产生与使用真实触觉模型相似的特征激活模式。</p>
</blockquote>
<p><strong>机器人推动性能</strong>：仿真实验结果（表II）显示，在视觉-触觉策略（使用真实触觉）达到约90%成功率的基础上，仅使用视觉输入和生成触觉的ViTacGen框架取得了86%的成功率，显著优于纯视觉策略（63%）和纯触觉策略（81%），证明了生成触觉的有效性。消融实验（图5）表明，对比学习模块和特征融合模块对性能提升均有贡献。</p>
<p><img src="https://arxiv.org/html/2510.14117v2/x5.png" alt="消融与真实实验"></p>
<blockquote>
<p><strong>图5</strong>：左：消融研究，展示了在ViTacGen框架中移除对比学习（w/o CL）或特征融合（w/o FF）模块后的成功率下降。右：真实世界零样本部署结果，ViTacGen在五个未见物体上的平均成功率达到69.3%，大幅超过纯视觉基线的36.0%。</p>
</blockquote>
<p><strong>真实世界零样本部署</strong>：在真实机器人仅使用视觉的情况下进行零样本测试。如图5右所示，ViTacGen在五个未见物体上的平均成功率达到69.3%，而纯视觉基线的平均成功率仅为36.0%，证明了该方法在真实场景中的有效性和泛化能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了ViTacGen框架，通过视觉生成触觉并融合于强化学习中，首次实现了在无需真实触觉传感器的情况下进行高性能机器人推动，并支持零样本部署到视觉系统。2) 设计了VT-Gen生成网络，能产生高质量的标准化触觉深度图；以及VT-Con策略网络，利用对比学习有效对齐和融合跨模态特征。3) 在仿真和真实世界中进行了全面实验验证，证明了方法的优越性能、鲁棒性和泛化能力。</p>
<p><strong>局限性</strong>：论文提到，其性能仍受限于模拟到真实的差距以及视觉感知的质量。生成触觉的准确性依赖于仿真中收集的配对数据质量。</p>
<p><strong>研究启示</strong>：1) 为克服触觉传感器硬件限制提供了新思路，即利用视觉生成触觉的“软感知”替代“硬传感”。2) 提出的标准化触觉表示（接触深度图）有助于解决不同传感器间的差异问题。3) 跨模态对比学习与特征融合策略可推广至其他需要多模态感知的机器人学习任务。未来工作可探索更复杂的生成模型、多任务学习以及在不同操作任务上的应用。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ViTacGen框架，旨在解决机器人推动任务中真实触觉传感器成本高、易损坏且部署困难，而纯视觉策略性能不足的问题。其核心方法包含一个编码器-解码器网络，能够从视觉图像序列直接生成标准化的触觉表征（接触深度图像），以及一个基于对比学习的强化学习策略，用于融合视觉与生成的触觉观测。实验表明，该方法在仿真和真实世界中均有效，实现了高达86%的成功率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.14117" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>