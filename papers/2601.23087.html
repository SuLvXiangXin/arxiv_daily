<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2601.23087" target="_blank" rel="noreferrer">2601.23087</a></span>
        <span>作者: Liu Hong Team</span>
        <span>日期: 2026-01-30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人模仿学习的主流生成策略包括扩散策略和流匹配策略。扩散策略具有强大的行为建模能力，但其依赖迭代去噪导致推理延迟高，难以实时部署。流匹配策略通过求解常微分方程（ODE）实现快速的一步推理，但直接在原始动作空间进行流匹配会放大建模噪声，导致长时程执行时轨迹抖动和不稳定。现有方法（如DP3、RDP、Flow Policy等）在推理效率与执行稳定性之间仍存在根本性权衡。</p>
<p>本文针对生成策略难以同时实现高效推理、稳定执行和强建模能力的痛点，提出了一种新视角：在连续潜在动作空间中进行轨迹级的流匹配。核心思路是：通过编码器将动作序列映射到具有时间一致性的潜在轨迹空间，在该平滑空间中进行流匹配生成全局运动结构，再通过解码器结合执行时的多模态感知（如视觉）生成最终控制指令，从而解耦全局运动与底层控制噪声。</p>
<h2 id="方法详解">方法详解</h2>
<p>CoLA-Flow Policy的整体框架是一个模块化的模仿学习流程，包含三个核心阶段：1) 学习时间一致的潜在动作表示；2) 在潜在空间中进行流匹配生成长时程潜在轨迹；3) 结合执行时感知信息解码为可执行控制命令。</p>
<p><img src="https://arxiv.org/html/2601.23087v2/x1.png" alt="方法整体架构"></p>
<blockquote>
<p><strong>图1</strong>：CoLA-Flow Policy的整体架构。系统首先将点云观测编码为几何感知的场景特征，然后在潜在动作空间进行流匹配以生成时间一致的潜在动作轨迹，最后在视觉条件调制下解码为可执行的控制指令。</p>
</blockquote>
<p><strong>核心模块一：时间一致的潜在动作表示</strong><br>该模块旨在将原始高维动作序列编码到一个平滑、连续的潜在轨迹流形上。具体而言，给定一个长度为H的动作轨迹，将其划分为K个连续的动作块。每个动作块通过一个轻量级时间卷积网络映射为特征嵌入，然后输入一个基于GRU的循环编码器。GRU编码器通过聚合历史动作块信息，生成具有时间依赖性的潜在代码序列，从而引入强时间归纳偏置，抑制高频变化，形成连续的潜在轨迹。这与非循环的按块编码器形成对比，后者会破坏连续性并导致轨迹抖动。</p>
<p><img src="https://arxiv.org/html/2601.23087v2/x2.png" alt="潜在动作表示"></p>
<blockquote>
<p><strong>图2</strong>：基于循环编码和条件解码的轨迹级潜在动作表示。基于GRU的编码器引入了促进潜在轨迹一致性的时间归纳偏置，而通过FiLM由腕部相机特征调制的MLP解码器实现了视觉自适应的动作执行。</p>
</blockquote>
<p>为了进一步正则化潜在空间，采用了变分框架，在轨迹级别引入后验分布与标准正态先验之间的KL散度项，鼓励潜在分布的紧致性和连续性，提高对噪声演示的鲁棒性。解码器设计用于融入执行时感知信息而不干扰潜在轨迹生成。本文以腕部相机观测为例，通过一个基于预训练ResNet-18的轻量视觉编码器提取特征，并利用特征线性调制（FiLM）层注入到解码器中。这样，给定潜在动作代码和视觉特征，解码器可以重建出对应的动作块。这种设计将潜在轨迹规划与执行时适应清晰分离。</p>
<p><strong>核心模块二：基于流匹配的潜在空间动作生成</strong><br>在获得平滑的潜在动作空间后，在此空间中进行流匹配以生成长时程操作行为。流匹配学习一个时间相关的向量场，该向量场定义了一个从简单基分布到目标潜在分布的连续概率路径，并由一个ODE描述。通过直接回归该向量场，可以在推理时通过一次前向传播（或少数积分步骤）从基分布样本生成目标潜在代码。</p>
<p>与直接在原始动作空间进行流匹配相比，在预先平滑化的潜在空间中进行操作，显著降低了对局部不一致性和噪声的敏感性。生成的潜在轨迹继承了潜在空间的时间一致性，从而在解码后能产生更稳定的执行动作。</p>
<p><strong>创新点总结</strong></p>
<ol>
<li><strong>轨迹级潜在流匹配框架</strong>：首次将流匹配应用于连续潜在动作空间，以轨迹而非瞬时动作为生成单元，实现了快速推理与平滑执行的统一。</li>
<li><strong>时间一致的循环编码</strong>：使用GRU编码器构建历史依赖的潜在轨迹，为流匹配提供了平滑的生成空间，这是稳定性的关键。</li>
<li><strong>执行时多模态调制解码</strong>：通过FiLM将执行时感知（如视觉）隔离在解码阶段，在不影响潜在轨迹生成连贯性的前提下，实现了对环境的自适应。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境（RLBench）和真实机器人（Franka Emika Panda机械臂）上进行。使用了包括“推方块”、“堆叠”、“开抽屉”等在内的多种长时程操作任务进行评估。</p>
<p>对比的基线方法包括：扩散策略（DP）、在原始动作空间进行的流匹配策略（Flow Policy）、以及结合一致性训练的流匹配变体（Consistency Flow Policy）。评估指标包括任务成功率、轨迹平滑度（以加速度的均方根误差衡量）和推理延迟。</p>
<p><img src="https://arxiv.org/html/2601.23087v2/x3.png" alt="模拟实验结果"></p>
<blockquote>
<p><strong>图3</strong>：在RLBench模拟环境中的定量结果。CoLA-Flow在成功率和轨迹平滑度上均优于基线方法，同时保持了接近单步的推理速度。</p>
</blockquote>
<p>关键实验结果：在模拟实验中，CoLA-Flow Policy相比原始动作空间的流匹配基线，将轨迹平滑度提升了高达93.7%，并将任务成功率提升了高达25%。同时，其推理速度显著快于扩散策略，实现了接近单步的推理。</p>
<p><img src="https://arxiv.org/html/2601.23087v2/x4.png" alt="真实机器人实验结果"></p>
<blockquote>
<p><strong>图4</strong>：真实机器人环境下的任务成功率。CoLA-Flow在复杂的长时程任务（如Stack Wine）上表现稳健。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.23087v2/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图5</strong>：消融研究结果。移除非循环编码器（w/o Recurrent）导致平滑度大幅下降；移除视觉调制（w/o Visual）降低了在真实环境中的成功率；移除变分正则化（w/o Var）略微影响性能。</p>
</blockquote>
<p>消融实验总结了各组件贡献：</p>
<ol>
<li><strong>循环编码器</strong>：是保证轨迹平滑度的最关键组件，移除后平滑度指标严重恶化。</li>
<li><strong>视觉条件解码</strong>：对真实世界部署的鲁棒性至关重要，移除后成功率下降。</li>
<li><strong>变分正则化</strong>：有助于提升对次优演示的鲁棒性，移除后性能有轻微下降。</li>
</ol>
<p><img src="https://arxiv.org/html/2601.23087v2/x6.png" alt="轨迹可视化"></p>
<blockquote>
<p><strong>图6</strong>：生成的动作轨迹可视化对比。CoLA-Flow产生的轨迹明显更平滑，而原始动作空间流匹配（Flow Policy）的轨迹存在明显抖动。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.23087v2/x7.png" alt="潜在空间可视化"></p>
<blockquote>
<p><strong>图7</strong>：潜在空间轨迹的t-SNE可视化。CoLA-Flow学习到的潜在轨迹在流形上连续且结构清晰。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2601.23087v2/x8.png" alt="泛化能力"></p>
<blockquote>
<p><strong>图8</strong>：对异构和次优演示的容忍度测试。CoLA-Flow在不同质量演示数据上训练后，仍能保持较高的成功率，显示了其鲁棒性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了一个在连续潜在动作空间进行轨迹级流匹配的模仿学习框架，有效协调了推理速度、执行稳定性和行为表达能力；2）通过循环编码构建时间一致的潜在空间，从根本上缓解了流匹配在原始动作空间的不稳定问题；3）设计了执行时多模态调制解码机制，实现了感知适应与轨迹生成的解耦，增强了部署鲁棒性。</p>
<p>论文提到的局限性包括：框架依赖于一个预训练且固定的潜在动作编码器-解码器，这可能会限制其对全新运动模式的适应性；此外，虽然展示了视觉调制的有效性，但其他模态（如触觉）的集成有待进一步探索。</p>
<p>本研究对后续工作的启示在于：将生成建模置于结构化、平滑的表示空间，是解决机器人控制中效率-稳定性权衡的一个有前景的方向。未来工作可以探索潜在空间的在线适应、更高效的多模态融合方式，以及将该框架应用于更复杂的动态环境交互任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对长时程机器人操作任务中，现有生成策略难以兼顾建模能力、实时推理与执行稳定性的问题，提出CoLA-Flow策略。该方法的核心是**连续潜在动作流匹配**：将动作序列编码为时序连贯的潜在轨迹，并在该空间学习显式流模型，从而解耦全局运动结构与底层控制噪声。实验表明，该方法实现了接近单步的推理速度，轨迹平滑度提升最高达93.7%，任务成功率提升最高达25%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2601.23087" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>