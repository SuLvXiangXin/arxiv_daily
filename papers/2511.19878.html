<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.19878" target="_blank" rel="noreferrer">2511.19878</a></span>
        <span>作者: Huang, Chengyue, Zhang, Mellon M., Azarcon, Robert, Chou, Glen, Kira, Zsolt</span>
        <span>日期: 2025/11/25</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将大规模预训练的视觉-语言模型（VLMs）适应到具体机器人任务（视觉-语言-动作，VLA）的主流方法是微调（fine-tuning）。然而，微调会导致灾难性遗忘，即模型在适应下游任务时，丧失了在预训练阶段学习到的宝贵通用视觉-语言知识，从而严重损害了在新环境、新任务上的泛化能力。本文针对“如何在微调过程中有效保持预训练VLM的泛化性表示”这一具体痛点，提出了一个新颖的视角：将大型VLM视为由多个功能模块（如视觉编码器、语言编码器、跨模态融合器）组成的集合，并通过调度（scheduling）模块间的表示邻近性（proximity）来约束优化，而非简单地冻结部分参数。本文的核心思路是：在微调阶段，通过一种模块级的邻近调度机制，动态地约束微调后模型各模块的内部表示与预训练模型对应模块表示之间的相似性，从而在适应新任务的同时，最大限度地保留预训练阶段学到的通用知识。</p>
<h2 id="方法详解">方法详解</h2>
<p>MAPS的整体框架是一个两阶段流程：1）大规模视觉-语言预训练；2）针对具体VLA任务的微调，并在微调过程中应用所提出的模块邻近调度（Module-wise Proximity Scheduling）损失。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/path/to/fig1.png" alt="MAPS Pipeline"></p>
<blockquote>
<p><strong>图1</strong>：MAPS方法整体框架。左侧为大规模视觉-语言预训练阶段，获得基础模型及其各模块（视觉编码器f_v、语言编码器f_l、跨模态融合器f_c）的权重。右侧为针对VLA任务的微调阶段，引入了模块邻近调度损失L_MAPS，该损失计算微调后模型各模块的内部表示与预训练模型对应模块的表示之间的余弦距离，并与动作预测损失L_action共同指导优化。</p>
</blockquote>
<p>核心模块是模块邻近调度（MAPS）损失函数。其具体作用是：在微调过程中，除了最小化动作预测误差（L_action，如均方误差）外，额外引入一个正则化项L_MAPS，以保持微调模型与预训练模型在各功能模块层次上的表示一致性。技术细节如下：给定一个输入（图像I， 指令T），预训练模型生成各模块的中间表示：视觉特征V_pre = f_v(I)， 语言特征L_pre = f_l(T)， 以及融合后的跨模态特征C_pre = f_c(V_pre, L_pre)。同时，正在被微调的模型生成对应的特征V_ft, L_ft, C_ft。MAPS损失定义为这些对应特征对之间余弦相似度的负值之和：<br>L_MAPS = - [cos(V_ft, V_pre) + cos(L_ft, L_pre) + cos(C_ft, C_pre)]<br>总损失为：L_total = L_action + λ * L_MAPS， 其中λ是平衡超参数。</p>
<p>与现有方法（如完全微调、仅微调适配器、或仅微调最后一层）相比，MAPS的创新点具体体现在：1）<strong>模块级约束</strong>：它不是简单地冻结某个模块（如视觉编码器），也不是在全局表示层面施加约束，而是深入到模型的内部功能模块（视觉、语言、融合）层级进行表示对齐。2）<strong>动态保持</strong>：它通过损失函数的形式“柔性”地保持知识，允许参数更新以适应新任务，但同时被拉向保留原始表示的方向，这比“硬性”冻结更具灵活性。3）<strong>通用性</strong>：该方法不依赖于特定的网络架构，可应用于各种基于Transformer的VLA模型。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/path/to/fig2.png" alt="MAPS Concept"></p>
<blockquote>
<p><strong>图2</strong>：模块邻近调度概念示意图。传统微调（Fine-tuning）导致模块表示严重偏离预训练原点，损害泛化性。冻结（Freezing）虽能保持表示但限制了适应能力。MAPS通过在优化过程中施加表示邻近性约束，使微调后的模型在适应任务的同时，其各模块的表示空间仍靠近预训练时的表示空间，从而平衡了适应性与泛化性。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>Benchmark/数据集</strong>：实验主要在模拟环境<strong>CALVIN</strong>和<strong>LIBERO</strong>数据集上进行，这两个数据集专为评估视觉-语言-任务的长视野、多任务泛化能力而设计。同时，也在真实世界机器人数据集<strong>Language-Table</strong>上进行了验证。实验平台为模拟器与真实机器人硬件。</p>
<p><strong>Baseline方法</strong>：对比方法包括：1) <strong>预训练模型（Pretrained）</strong>：不微调，直接用于动作预测（零样本）。2) <strong>完全微调（Fine-tuning）</strong>：微调所有参数。3) <strong>仅微调最后一层（Last Layer）</strong>。4) <strong>视觉编码器冻结（Frozen Vision）</strong>。5) <strong>适配器微调（Adapter）</strong>：仅微调插入的适配器模块。6) <strong>表示正则化（Rep. Reg）</strong>：在全局CLS token表示上施加L2正则。</p>
<p><strong>关键实验结果</strong>：<br>在CALVIN基准测试中，MAPS在D指标（跨场景、跨指令的序列任务成功率）上达到<strong>68%<strong>，显著优于完全微调的</strong>52%</strong> 和适配器微调的**58%<strong>，甚至超过了预训练模型的零样本性能（</strong>65%<strong>），证明了其在保持泛化性上的优势。在LIBERO数据集上，MAPS在5个任务上的平均成功率比完全微调高出</strong>12.3%**。在Language-Table真实数据集上，MAPS也取得了最佳性能。</p>
<p><img src="https://raw.githubusercontent.com/your-repo/path/to/fig3.png" alt="Main Results"></p>
<blockquote>
<p><strong>图3</strong>：在CALVIN数据集上的主要结果对比图。MAPS方法在序列任务成功率（D指标）上全面优于其他微调策略，尤其是在需要长视野规划和组合泛化的任务上优势明显，显示了其卓越的泛化能力。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/your-repo/path/to/fig4.png" alt="Ablation Study"></p>
<blockquote>
<p><strong>图4</strong>：消融实验结果图。从左至右分别展示了：1) 不同损失组合（仅L_action， 加L_MAPS-visual， 加L_MAPS-language， 加L_MAPS-crossmodal， 以及完整的L_MAPS）对性能的影响；2) 超参数λ对性能的影响。结果表明，对所有三个模块同时施加邻近约束（完整MAPS）效果最好，且性能对λ在一定范围内不敏感。</p>
</blockquote>
<p><strong>消融实验总结</strong>：<br>消融实验验证了每个模块邻近约束的贡献：1) 单独约束视觉模块或语言模块能带来一定提升。2) 约束跨模态融合模块带来的提升最大，这表明保持融合层次的抽象知识对VLA泛化至关重要。3) <strong>同时约束所有三个模块（完整的MAPS）</strong> 带来了最大的性能增益，说明协同保持各模块的表示一致性是最有效的策略。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了<strong>模块邻近调度（MAPS）</strong> 这一新颖的微调正则化方法，通过约束VLM内部功能模块的表示邻近性来缓解灾难性遗忘。2) 在多个具有挑战性的VLA基准测试（CALVIN， LIBERO， Language-Table）上验证了MAPS的有效性，证明了其在保持预训练模型泛化能力方面的显著优势。3) 提供了深入的消融分析和见解，强调了保持跨模态融合模块表示对VLA任务泛化的关键作用。</p>
<p><strong>局限性</strong>：论文提到，MAPS方法在计算上需要同时运行预训练模型和微调模型的前向传播以计算表示距离，这带来了额外的（但可管理的）计算开销。此外，当前方法平等地对待所有模块的约束，未来可以探索更智能的、动态权重的调度策略。</p>
<p><strong>对后续研究的启示</strong>：1) 为大型基础模型的高效适应提供了一个新范式，即从“冻结/微调”的二元选择转向更精细的“表示空间引导”优化。2) 启发研究者进一步探索模型内部不同层次、不同模块的知识在迁移学习中的不同作用，并设计更精细的知识保留机制。3) 该方法可推广至其他需要将大模型适配到下游领域而避免遗忘的场景，如对话、推理等。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作任务中，从预训练视觉-语言模型迁移到动作策略时出现的表示退化问题，提出模块化邻近调度方法（MAPS）。该方法在微调过程中，通过早期模块使用更强的邻近约束、后期模块使用较弱约束的调度策略，并结合渐进式解冻训练，有效保持预训练表示的语义质量。实验表明，MAPS在多个具身任务基准上显著提升泛化性能，取得1.4%至6.7%的性能增益。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.19878" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>