<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.13977" target="_blank" rel="noreferrer">2602.13977</a></span>
        <span>作者: Dongbin Zhao Team</span>
        <span>日期: 2026-02-15</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型主要通过模仿学习进行训练，但其性能受限于演示数据的质量和覆盖范围。强化学习虽能突破这一限制，但其对大规模真实环境交互的需求使其难以在物理机器人上直接部署。近期研究尝试使用学习得到的世界模型作为策略优化的模拟器，但在闭环想象的轨迹生成中，模型不可避免地会产生幻觉，并导致长视野错误累积。这些错误不仅降低了视觉保真度，更会破坏优化信号，促使策略利用模型的不准确性而非真实的任务进展。本文针对这一核心痛点，提出不应将世界模型视为完美的模拟器，而应明确调控强化学习如何与不完美的想象动态进行交互。本文核心思路是提出一个名为WoVR的可靠性驱动框架，通过在模拟器设计、交互协议和策略-模型对齐三个层面控制幻觉，从而实现完全在想象中进行稳定、有效的策略优化。</p>
<h2 id="方法详解">方法详解</h2>
<p>WoVR的整体框架围绕控制幻觉的三个层面构建：在模拟器层面，构建一个稳定、可控的视频世界模型；在交互层面，通过关键帧初始化轨迹和掩码GRPO来重塑想象交互；在对齐层面，通过策略-模型协同进化来维持一致性。</p>
<p><img src="https://arxiv.org/html/2602.13977v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：WoVR整体框架。它首先将世界模型构建为可控模拟器；在此基础上，通过关键帧初始化轨迹设计可靠的交互协议以减少有效错误深度；最后通过PACE策略实现世界模型与演化策略的协同进化，以缓解分布偏移。</p>
</blockquote>
<p><strong>1. 稳定的动作条件世界模型</strong><br>该模型基于Wan2.2-TI2V-5B视频扩散主干构建。为了实现动作条件生成，采用了双通道动作注入设计：一是将动作嵌入与扩散时间步嵌入融合，通过AdaLN-Zero风格调制来塑造特征层面的去噪动态；二是保留原始的交叉注意力算子，但将文本嵌入替换为动作嵌入，使动作能全局地条件化网络。为了抑制长视野生成中的空间漂移和背景崩溃，采用了首帧锚定推理上下文：在每个自回归步骤，模型以初始参考帧与最近记忆帧的拼接作为条件，这约束了全局外观和场景布局。模型使用修正流目标进行训练，并通过对非参考上下文潜在表示注入扩散噪声来减少训练-推理差距，提高对自生成帧的鲁棒性。此外，模型还包含一个轻量级的奖励分类器，根据预测的下一观察生成稀疏的成功信号。</p>
<p><img src="https://arxiv.org/html/2602.13977v1/x2.png" alt="世界模型架构"></p>
<blockquote>
<p><strong>图3</strong>：提出的动作条件世界模型架构。该模型基于视频扩散主干，通过双通道动作注入设计进行条件化，实现了帧级可控性和稳定的分块自回归生成。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.13977v1/x3.png" alt="注意力可视化"></p>
<blockquote>
<p><strong>图4</strong>：自注意力概率图可视化。在去噪过程中，许多注意力头聚焦于序列的首帧，证实了首帧锚定的有效性。</p>
</blockquote>
<p><strong>2. 幻觉感知的策略优化</strong><br>为了减少想象交互的有效错误深度，提出了关键帧初始化轨迹。其核心思想是，不从初始状态开始长视野轨迹，而是从靠近任务关键中间状态（尤其是当前策略遇到的失败状态）的关键帧开始初始化一部分轨迹。这样可以使世界模型在决定性接触和修正发生的局部区域进行更可靠的预测，避免了从初始状态开始的长前缀预测中错误累积的问题。策略优化采用分组相对策略优化，并对想象轨迹进行掩码处理：由于幻觉常在想象中达成“成功”后占主导，因此掩码掉成功后的步骤，并依据轨迹的有效长度进行归一化。这使得梯度由短而关键的任务片段主导，而非长而易漂移的延续部分。</p>
<p><img src="https://arxiv.org/html/2602.13977v1/x4.png" alt="关键帧初始化轨迹效果"></p>
<blockquote>
<p><strong>图5</strong>：关键帧初始化轨迹效果示意图。从初始状态开始的长视野轨迹在早期累积预测错误，导致与真实失败矛盾的幻觉成功；而关键帧初始化轨迹在关键状态附近开始，能够实现物理一致的预测，从而促进更高效稳定的策略学习。</p>
</blockquote>
<p><strong>3. PACE：策略对齐协同进化</strong><br>为了解决策略演化导致的动作分布偏移及其与世界模型训练数据分布的不匹配问题，提出了PACE策略。该策略不将世界模型视为固定模拟器，而是允许其与VLA策略在训练过程中共同进化。具体实现为低频、策略驱动的精炼：首先使用基础VLA策略收集的轨迹训练一个初始世界模型；在第一阶段策略优化后，收集有限数量的、在演化策略下的新轨迹，用于进一步精炼世界模型。这种低频精炼避免了经典基于模型的强化学习方法中高频更新动态模型带来的不稳定，也无需在策略训练期间进行持续的真人监督或环境重置，显著降低了操作开销。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在LIBERO基准和真实机器人操作任务上进行评估。对比的基线方法包括：针对世界模型质量的EVAC、Cosmos-Predict2和OpenSora；针对策略优化的Online PPO、World-Env和WMPO。</p>
<p><strong>世界模型质量评估</strong>：在长视野、闭环、动作条件下的视频生成任务中，WoVR的世界模型在LPIPS、FID、FVD和FloLPIPS等多个感知和分布指标上均达到最优性能，同时保持了高推理效率。</p>
<p><img src="https://arxiv.org/html/2602.13977v1/x5.png" alt="世界模型定量结果"></p>
<blockquote>
<p><strong>图6</strong>：世界模型定量评估结果。WoVR在LPIPS、FID、FVD和FloLPIPS指标上均优于基线方法，显示出更高的视觉质量和时序一致性。</p>
</blockquote>
<p><strong>策略性能评估</strong>：在LIBERO基准的10个长视野操纵任务上，WoVR将平均成功率从39.95%提升至69.2%（+29.3个百分点）。在真实机器人上的6个任务中，WoVR将平均成功率从61.7%提升至91.7%（+30.0个百分点），显著优于所有基线方法。</p>
<p><img src="https://arxiv.org/html/2602.13977v1/x6.png" alt="策略性能对比"></p>
<blockquote>
<p><strong>图7</strong>：在LIBERO基准上的策略成功率对比。WoVR在所有任务上均大幅超越基线方法，平均成功率提升29.3个百分点。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.13977v1/x7.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图8</strong>：真实机器人任务成功率。WoVR优化后的策略在真实环境中取得了91.7%的平均成功率，相比基础策略提升30个百分点。</p>
</blockquote>
<p><strong>消融实验</strong>：实验验证了各个组件的贡献。移除关键帧初始化轨迹会导致性能显著下降（成功率从69.2%降至56.1%），因为长视野错误累积加剧。移除PACE策略会导致性能降至58.3%，表明分布偏移会损害模拟器可靠性。同时使用掩码GRPO和轨迹长度归一化对于稳定训练至关重要。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 明确指出闭环想象交互中的幻觉是基于世界模型的VLA策略强化学习所面临的根本性可靠性挑战；2) 提出了WoVR框架，通过可控模拟器设计、可靠交互协议和策略对齐协同进化三个层面系统性地控制幻觉；3) 在仿真和真实机器人实验上取得了显著的性能提升，证明了学习的世界模型在幻觉受控条件下可以作为强化学习的实用模拟器。</p>
<p>论文提到的局限性包括：世界模型和奖励分类器的训练仍需依赖初始的模仿学习数据；PACE策略中的世界模型精炼需要收集额外的策略轨迹，尽管频率很低。</p>
<p>这项工作对后续研究的启示在于：将世界模型用于强化学习不仅是一个建模问题，更是一个系统工程和可靠性控制问题。未来研究可以探索更高效的世界模型精炼策略、设计对分布偏移更鲁棒的模型架构，以及将类似的控制幻觉思想扩展到更复杂的多模态任务中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对强化学习（RL）训练视觉-语言-动作（VLA）策略时需大量真实交互、而使用世界模型作为模拟器易产生幻觉和长时域误差累积的问题，提出WoVR框架。其关键技术包括：可控动作条件视频世界模型以提升稳定性，关键帧初始化Rollouts以减少误差深度，以及世界模型-策略协同进化以保持对齐。实验表明，WoVR在LIBERO基准上平均成功率从39.95%提升至69.2%（+29.3点），真实机器人成功率从61.7%提升至91.7%（+30.0点）。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.13977" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>