<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.04315" target="_blank" rel="noreferrer">2602.04315</a></span>
        <span>作者: Ma, Guoqing, Wang, Siheng, Zhang, Zeyu, Yu, Shan, Tang, Hao</span>
        <span>日期: 2026/02/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将大型基础模型（如视觉语言模型VLM）的能力扩展到机器人操作领域是一个重要方向。主流方法之一是单体视觉-语言-动作模型，它通过微调预训练的VLM，以端到端的方式直接从视觉观察和语言指令输出机器人动作。然而，这类方法存在关键局限性：首先，它们严重依赖大规模、昂贵的真实机器人数据（耦合了传感器观测和动作轨迹）进行微调，难以扩展；其次，它们在零样本跨域泛化能力上远未达到基础模型在其原生领域（如视觉、语言）的水平；再者，它们难以提供精细的几何坐标（如关键点），并且在推理频率上受限，难以完成灵巧的动态任务。此外，在无示教的情况下进行3D空间的长时程轨迹规划仍然充满挑战，因为存在高层语义推理与连续几何约束之间的鸿沟，以及现有系统难以跨任务积累和复用经验。</p>
<p>本文针对上述痛点，提出了一个新的视角：构建一个分层的视觉-语言-动作模型，将轨迹规划作为核心的中间表示，以更好地利用基础模型（VLM， SAM， LLM）中蕴含的世界先验知识，从而实现零样本操作并自动生成机器人数据。本文的核心思路是：设计一个包含高层感知、中层3D轨迹规划和底层控制的三级框架，通过知识引导的轨迹规划桥接语义理解与精确执行。</p>
<h2 id="方法详解">方法详解</h2>
<p>GeneralVLA是一个分层的VLA模型，其整体框架由三个相互连接的模块组成（见图2）。高层模块是可供性分割模块，它接收RGB图像和任务文本指令，输出场景中关键物体及其可供性（如抓取点、操作点）的2D点位置和语义信息。中层模块是知识引导的轨迹规划器，它将高层输出的2D点通过深度图投影为3D点，并结合语义信息，利用大型语言模型的强大文本泛化能力进行任务理解、技能知识和轨迹规划，最终生成一个指示机器人末端执行器期望轨迹的3D路径。低层模块是路径引导的低层策略，它接收规划好的3D路径，结合3D感知（如点云）和本体感觉信息，执行精确的抓取姿态估计和动作生成，由运动规划器驱动机器人完成操作。</p>
<p><img src="https://arxiv.org/html/2602.04315v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：GeneralVLA的推理流程。(a) 高层ASM被调用以生成2D点及对应的语义信息。(b) 中层知识引导的轨迹规划器执行任务理解、3D推理和规划，产生指示机器人末端期望轨迹的3D路径。(c) 中间3D路径预测作为指导，传递给由HGM增强的低层3D感知控制策略以进行精确操作。</p>
</blockquote>
<p><strong>核心模块一：可供性分割模块</strong><br>ASM旨在感知当前场景，理解物体和障碍物的可供性，并以点的形式进行标记。为了克服传统VLM在精确定位物体及其可供性位置方面的不足，ASM整合了多模态大语言模型的推理能力和SAM的分割先验，并设计了迭代细化机制。</p>
<p><img src="https://arxiv.org/html/2602.04315v1/x3.png" alt="ASM与3DAgent详细框架"></p>
<blockquote>
<p><strong>图3</strong>：(a) ASM的详细框架。给定输入图像和任务文本作为查询，多模态LLM生成文本输出。对应<code>&lt;SEG&gt;</code>令牌的最后一层嵌入通过解码器解码成分割掩码。使用LoRA进行高效微调。</p>
</blockquote>
<p>具体而言，当LLM需要生成二进制分割掩码时，其输出序列包含一个特殊的<code>&lt;SEG&gt;</code>令牌。从LLM最后一层提取该令牌对应的隐藏表示，并通过一个MLP投影得到分割嵌入。此嵌入与SAM编码器提取的图像特征一同输入SAM解码器，得到初始分割掩码。为解决单次分割可能产生的过分割或欠分割问题，ASM引入了迭代细化机制：初始分割后，MLLM评估结果并提供两种反馈点（正点：正确分割区域；负点：错误分割区域），这些点用于引导交互式分割工具在下一次迭代中优化预测。此过程最多重复n次，当没有负点生成时，分割结果被视为最终输出。</p>
<p><strong>核心模块二：知识引导的轨迹规划</strong><br>由于现有视觉基础模型在3D场景理解和规划方面能力不足，该模块利用3D点的文本化描述，借助LLM强大的文本泛化能力来解决3D路径规划问题。ASM提供的2D点信息和物体语义通过深度信息投影到3D空间。任务指令、3D点信息和物体语义被输入LLM。LLM能够理解任务、理解场景，并规划机械臂的轨迹（包括夹爪开合状态）。一个运动轨迹可由多个阶段组成，从而实现长时程规划，并避免传统规划方法中离散子任务拼接导致的问题。此外，运动轨迹还能满足避障要求。实验中，当每个物体的点数超过3个时，LLM能有效理解物体的空间姿态。</p>
<p>为了充分利用多任务执行的经验，该模块设计了一个知识库。整合过程分为三步：知识检索、知识构建和知识巩固。在知识检索阶段，代理使用当前查询上下文查询知识库，通过基于嵌入的相似性搜索识别前k个相关经验及其对应的知识项。检索到的知识项被注入代理的系统指令中。当当前查询任务完成时，进行知识构建以提取新的知识项：首先采用“LLM-as-a-judge”来标记轨迹结果为成功或失败；基于此，成功的经验贡献已验证的操作策略，失败的经验提供反事实信号和陷阱，帮助强化安全护栏。最后，知识巩固将这些知识项通过简单的加法操作纳入知识库，形成一个持续进化的知识存储。</p>
<p><strong>核心模块三：路径引导的低层策略</strong><br>规划出的3D路径是宏观的粗略轨迹。为了在3D空间的抓取点实现精确抓取，需要精确的抓取姿态估计。为此，论文设计了多模态混合抓取模块。3D点信息可以定位物体的3D空间范围，确定范围后，融合RGB颜色和深度信息，使用逆投影法获得点云数据。然后使用抓取预测模型估计抓取姿态。由于抓取网络提供多个候选抓取点，该方法会过滤掉可能导致碰撞的点，并选择抓取中心最接近物体中心点的抓取姿态，从而获得最优的任务特定抓取姿态。</p>
<p><strong>创新点</strong>：与现有方法相比，GeneralVLA的主要创新在于其分层架构，它将ASM微调、LLM规划和低层动作预测解耦，使得高层ASM在从RGB输入预测有语义意义的2D可供性点时，无需牺牲视觉理解能力来保持长时程规划能力；中层专门负责长时程规划；低层策略则可以利用丰富的3D和本体感觉信息。这种设计使得模型能够继承VLM和SAM的通用视觉感知能力、LLM的语义推理优势以及3D策略模型的3D空间感知优势。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：实验使用了RLBench仿真基准中的14个多样化任务进行评估，涵盖了多种物体类别、位置变化和任务时长。机器人平台为Franka Panda，使用CoppeliaSim和PyRep接口，通过4个RGB-D摄像头捕获观测。对比的基线方法包括：VoxPoser、Code-as-Policies和Scaling-up。ASM的点定位准确性还在RoboRefIt数据集上进行了评估。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>零样本性能</strong>：GeneralVLA能够为全部14个任务生成成功轨迹，而Scaling-up、VoxPoser和CAP分别只能覆盖10、9和7个任务。在14个任务中的10个上，GeneralVLA的表现优于基线方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04315v1/x9.png" alt="零样本评估成功率对比表"></p>
<blockquote>
<p><strong>图9/表I</strong>：仿真中零样本评估的任务平均成功率%。GeneralVLA在RLBench的14个任务中的10个上优于其他基线。<code>w/o PA</code>表示仅使用预调VLM进行定位而不使用ASM。结果显示了GeneralVLA在多种任务上的显著优势，特别是在Play_jenga、Open_jar等复杂任务上。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.04315v1/x4.png" alt="多阶段任务执行示例"></p>
<blockquote>
<p><strong>图4</strong>：GeneralVLA在仿真中的运行示例，展示了其在多物体、多阶段场景中的强大性能，这得益于ASM的分割能力、3DAgent的空间推理能力以及低层3D策略的鲁棒执行。</p>
</blockquote>
<ol start="2">
<li><strong>行为克隆性能</strong>：使用GeneralVLA生成的演示数据训练行为克隆策略（RVT-2模型），其性能与使用人工脚本演示训练的策略相当，平均性能差异仅为2.7%。并且，使用GeneralVLA数据训练的模型性能显著优于使用VoxPoser、Scaling-up和CAP生成数据训练的模型。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04315v1/x10.png" alt="行为克隆性能对比表"></p>
<blockquote>
<p><strong>图10/表II</strong>：使用不同生成数据的行为克隆性能对比。使用GeneralVLA生成数据训练的策略在12个任务中的10个上提供了最佳性能（与其他人机交互数据生成基线相比）。结果表明GeneralVLA生成的数据质量高，可用于训练鲁棒的策略。</p>
</blockquote>
<ol start="3">
<li><p><strong>ASM点定位准确性</strong>：在RoboRefIt数据集上的定量比较显示，ASM的点预测准确率达到63.4%，显著高于GPT-4o、LLaVA-NeXT等基线方法。</p>
</li>
<li><p><strong>消融实验</strong>：</p>
<ul>
<li><strong>ASM有效性</strong>：在零样本任务评估中，移除ASM（即<code>w/o PA</code>版本）会导致所有任务的性能显著下降，证明了ASM提供的精确点定位对于后续规划至关重要。</li>
<li><strong>ASM训练数据构成</strong>：消融实验表明，结合视觉问答、LVIS、像素级标注、仿真数据和机器人数据的所有成分进行训练，才能达到最高的点定位准确率（63.4%），移除任何一部分都会导致性能下降。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04315v1/x8.png" alt="ASM训练数据消融实验表"></p>
<blockquote>
<p><strong>图8/表IV</strong>：ASM训练数据构成的消融研究。使用所有数据成分（VQA， LVIS， Pixel， Sim， Robo）能获得最高的参考准确率（63.4%），移除任一部分都会导致性能下降。</p>
</blockquote>
<ol start="5">
<li><strong>真实世界实验</strong>：在真实机器人上对4个代表性任务进行的零样本测试表明，GeneralVLA能够成功生成演示并完成任务。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.04315v1/x5.png" alt="真实世界零样本演示"></p>
<blockquote>
<p><strong>图5</strong>：GeneralVLA在真实世界中对4个任务的零样本演示。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1) 提出了一个分层的、零样本的3D轨迹规划框架GeneralVLA，它通过解耦感知、规划和执行，充分利用了基础模型的世界先验知识，既能解决机器人操作任务，又能生成丰富的机器人数据。2) 设计了ASM模块，通过迭代细化机制结合VLM和SAM，实现了精确的可供性分割；并在轨迹规划中引入了知识库，以捕获和复用跨任务技能。3) 实验表明，GeneralVLA在多样化的仿真任务上实现了优异的零样本性能，且其生成的数据可用于训练出高性能的行为克隆策略，证明了其作为可扩展机器人数据生成方法的潜力。</p>
<p>论文自身提到的局限性包括：1) 当前框架（特别是ASM和3DAgent）计算成本较高。2) 每次执行都需从头开始重新规划，未能完全利用先前成功计划的经验（尽管知识库部分解决了跨任务经验问题，但同任务仍需重新规划）。3) 为了理论验证的可行性而未过度增加系统复杂性，实验中仅使用了前视视角。</p>
<p>对后续研究的启示：分层设计并利用轨迹作为中间表示，是连接基础模型语义能力与机器人几何控制的有效途径。知识库的引入为机器人持续学习和技能积累提供了可行方案。未来的工作可以探索如何降低推理计算成本，如何更高效地利用历史规划经验进行在线适应，以及如何将系统扩展到更广泛的视角和动态环境中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出GeneralVLA模型，旨在解决机器人领域零样本泛化能力不足的核心问题。该方法构建分层视觉-语言-动作模型：高层Affordance Segmentation Module感知场景关键点；中层3DAgent进行任务理解与知识引导的轨迹规划，输出3D路径；底层3D感知控制策略执行精确操作。实验表明，该方法在14项任务上显著优于VoxPoser等基线，且其自动生成的演示数据能训练出比人类演示或其他方法更鲁棒的行为克隆策略。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.04315" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>