<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Latent Action Pretraining Through World Modeling - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Latent Action Pretraining Through World Modeling</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.18428" target="_blank" rel="noreferrer">2509.18428</a></span>
        <span>作者: Tharwat, Bahey, Nasser, Yara, Abouzeid, Ali, Reid, Ian</span>
        <span>日期: 2025/09/22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，遵循语言指令的机器人操作任务学习主要依赖于视觉-语言-动作模型。主流方法如OpenVLA和π0，需要在大规模、通过遥操作手动标注动作的数据集上进行监督训练。这些动作标注成本高昂、难以扩展，且容易引入偏差，限制了模型跨任务、环境和具体机器人形态的泛化能力。近期，LAPA和villa-X等方法引入了潜在动作表示，通过对帧间抽象视觉变化建模，实现在无标签数据集上的无监督预训练。尽管这些方法取得了良好效果，但其庞大的模型规模（如数十亿参数）使得在实际机器人上部署面临挑战。</p>
<p>本文针对上述依赖标注数据和模型规模庞大的痛点，提出了一种新的视角：通过世界建模，以自监督的方式从无标签视频数据（可以是机器人记录或人类操作日常物体的视频）中学习潜在动作表示。本文的核心思路是提出一个模型无关的框架LAWM，它结合模仿学习模型与世界模型，通过预测未来视频帧来学习动作表示，从而为下游任务提供一个强大的动作先验，实现高效微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>LAWM框架包含两个阶段：潜在动作预训练和动作微调。</p>
<p><img src="https://arxiv.org/html/2509.18428v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：LAWM整体框架概述。(a) 潜在动作预训练阶段：模仿学习模型处理输入图像和自然语言指令，预测潜在动作表示 <code>z_{t:t+n}</code>。这些潜在动作与视频帧配对，并与世界模型联合优化，通过预测未来帧进行自监督训练，使潜在动作在环境动力学中扎根，无需依赖真实动作标签。(b) 动作微调阶段：预训练好的模仿学习模型使用带标签的演示数据，适应下游机器人任务。模型通过监督学习，直接将观测（图像、语言指令和机器人状态）映射到真实动作 <code>a_{t:t+n}</code>。</p>
</blockquote>
<p><strong>潜在动作预训练阶段</strong>：此阶段采用端到端的自监督训练，学习信号来自于预测视频序列中的下一帧。系统输入包括：(i) 来自人类或机器人执行操作任务的图像帧，(ii) 描述任务目标的自然语言指令。模仿学习模型处理这些输入，产生编码了预期行为语义和时间方面的 <code>n</code> 个潜在动作。这些潜在动作不与真实动作进行显式监督，而是与当前时间步 <code>t</code> 的图像帧以及后续 <code>n-1</code> 帧配对，然后输入到世界模型中，以生成从时间步 <code>t+1</code> 到 <code>t+n</code> 的下一批图像帧。本文采用基于DreamerV3的循环状态空间模型作为世界模型。在每个时间步 <code>t</code>，模型维护一个由转移函数 <code>f_ϕ</code> 更新的确定性循环状态 <code>h_t</code>，并根据当前观测 <code>x_t</code> 从后验中采样一个随机潜在变量 <code>z_t</code>。模型状态为 <code>ŝ_t = (h_t, z_t)</code>。模仿学习模型和世界模型的参数 <code>ϕ</code> 通过最小化一个目标函数进行联合训练，该目标函数包含均方误差重构损失和KL散度正则化项：<code>ℒ(ϕ) = 𝔼_{q_ϕ}[∑_{t=1}^T ‖x_t - x̂_t‖^2 + β KL(q_ϕ(z_t|h_t, x_t) ‖ p_ϕ(z_t|h_t))]</code>。这使得框架能够以端到端的方式联合优化模仿学习模型和世界模型，产生扎根于环境动力学的动作表示，有效捕获策略而无需真实动作监督。</p>
<p><strong>动作微调阶段</strong>：在自监督预训练之后，进入使用真实动作进行监督学习的微调阶段。此阶段不再使用世界模型。预训练好的模仿学习模型（如BAKU或Diffusion Policy）在特定的下游任务上进行训练以适应目标环境。预训练模型受益于从大规模无标签视频数据中获得的鲁棒先验，从而能够在带标签数据集上高效微调。微调可以通过两种策略实现：添加一个将潜在动作解码到真实动作空间的额外头部，或者重新初始化最后的动作潜在层以匹配真实动作空间的维度并据此训练。本文采用第二种策略。对于BAKU模型，损失目标是专家动作在所学策略分布下的负对数似然最小化。对于Diffusion Policy，损失目标是训练去噪网络正确预测给定噪声动作和当前观测所添加的噪声。</p>
<p>与现有方法相比，LAWM的创新点主要体现在：1) <strong>模型无关性</strong>：不依赖于模仿学习模型或世界模型的特定架构，允许集成各种模型。2) <strong>高效的自监督预训练</strong>：通过世界建模的预测目标学习动作表示，完全摆脱了对昂贵动作标签的依赖，且模型参数量显著小于之前的潜在动作方法（如LAPA的7B参数）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究在LIBERO基准（特别是LIBERO-90及其四个任务套件：Spatial, Object, Goal, Long）、真实世界桌面操作设置（使用6-DoF Realman机械臂）上进行评估。预训练数据集使用BridgeData v2和Something-Something v2。对比的基线方法包括：1) 使用真实动作监督预训练的模型（Octo-base, OpenVLA, π0, π0.5）；2) 从互联网规模数据训练的VLM初始化的模型（π0 (PaliGemma), SmolVLA）；3) 类似的无监督视频预训练方法（villa-X）。主要评估指标为成功率。</p>
<p><img src="https://arxiv.org/html/2509.18428v1/figures/results/single_bar_chart_custom_narrows_baku-min.png" alt="BAKU在LIBERO-90上的成功率对比"></p>
<blockquote>
<p><strong>图4a</strong>：BAKU模型在LIBERO-90上不同预训练策略的成功率对比。“WM”表示使用世界建模的无标签预训练。结果显示，使用BridgeData v2 (WM)或Something-Something v2 (WM)进行世界建模预训练，其性能（94.2%， 92.6%）均优于从零训练（91.4%）和使用BridgeData v2真实动作的监督预训练（93.3%）。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.18428v1/figures/results/single_bar_chart_custom_narrows_dp-min.png" alt="Diffusion Policy在LIBERO-90上的成功率对比"></p>
<blockquote>
<p><strong>图4b</strong>：Diffusion Policy模型在LIBERO-90上不同预训练策略的成功率对比。同样，世界建模预训练（BridgeData v2 WM: 93.7%， SSv2 WM: 92.6%）的表现优于或与监督预训练（92.1%）相当。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>优于监督预训练</strong>：在LIBERO-90上，LAWM框架使用BAKU和Diffusion Policy进行世界建模预训练，其成功率均达到或超过了使用真实动作进行监督预训练的性能。例如，BAKU的监督预训练成功率为93.3%，而使用BridgeData v2进行世界建模预训练后达到94.2%。</li>
<li><strong>与先进方法对比</strong>：在LIBERO各任务套件上，使用LAWM预训练后的BAKU模型取得了极具竞争力的成绩（Spatial: 99.0%， Object: 100.0%， Goal: 96.0%， Long: 94.0%， Average: 97.25%），整体上优于villa-X（Average: 90.10%）和大多数监督预训练的大模型，仅稍逊于表现最佳的π0.5。</li>
<li><strong>仅用人类视频预训练的可行性</strong>：仅使用人类视频数据集Something-Something v2进行预训练，在LIBERO-90上微调后，BAKU和Diffusion Policy分别取得了92.6%和92.6%的成功率，接近甚至超过了使用机器人数据集BridgeData v2的监督预训练结果（93.3%和92.1%）。</li>
<li><strong>真实世界验证</strong>：在真实世界5个任务上，使用LAWM预训练（仅用人类视频）将BAKU的平均成功率从84%提升至94%。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.18428v1/x2.png" alt="真实世界操作定性结果"></p>
<blockquote>
<p><strong>图5</strong>：LAWM框架在真实世界操作设置上的定性结果。上图：拾取蓝色杯子并将其放入盘子上的碗中。下图：将蓝色杯子堆叠在白色杯子上。图像展示了连续时间步的操作过程。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.18428v1/figures/world_model_ablation.png" alt="世界模型规模消融实验"></p>
<blockquote>
<p><strong>图6</strong>：预训练阶段世界模型参数量与在LIBERO-Long套件上微调后成功率的关系。结果表明，随着世界模型规模增大，性能提升，但在100M参数后趋于平缓。</p>
</blockquote>
<p><strong>消融实验与贡献分析</strong>：图6展示了世界模型规模的影响，表明性能随模型规模增大而提升，验证了世界建模能力的重要性。通过典型相关分析（CCA）量化潜在动作与真实动作的关联性，LAWM学习到的潜在动作在各项任务类别（Put, Move, Remove, Take）中与真实动作的第一典型相关系数均很高（0.9098-0.9599），优于villa-X，表明其潜在动作空间与真实动作空间对齐良好。</p>
<p><img src="https://arxiv.org/html/2509.18428v1/x3.png" alt="潜在动作可视化"></p>
<blockquote>
<p><strong>图7</strong>：具有相似潜在动作表示的图像对示例。这些图像对对应着相似的底层机器人行为，展示了潜在空间捕获了有意义的特征。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了LAWM，一个模型无关的框架，能够从无动作标签的机器人和人类视频中，通过世界建模为模仿学习模型学习潜在动作块表示。2) 实验证明，该方法学习到的动作先验优于使用真实动作标签的监督预训练，并且在LIBERO基准和真实世界设置中超越了类似方法。3) 展示了仅从人类演示视频中预训练学习鲁棒动作先验的可行性，且框架高效实用（如微调仅需20分钟）。</p>
<p><strong>局限性</strong>：论文提到，联合训练模仿学习模型和世界模型在计算上可能具有挑战性。此外，虽然展示了从人类视频学习的潜力，但 embodiment gap（形态差异）问题仍然存在。</p>
<p><strong>后续研究启示</strong>：LAWM的成功表明，通过世界建模等自监督目标学习动作表示是一条富有前景的路径，可以大幅减少对昂贵机器人动作标注数据的依赖，并有利于学习更通用、更扎根于物理的动力学的表示。其模型无关的设计鼓励社区将各种先进的模仿学习模型和世界模型融入此框架进行探索。未来工作可以进一步探索如何利用更大量、更多样化的互联网视频进行预训练，并研究如何更好地弥合人类与机器人之间的形态差距。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LAWM框架，解决当前视觉-语言-动作模型依赖昂贵人工标注动作数据、难以跨任务泛化的问题。方法核心是通过世界建模进行自监督的潜在动作预训练：模型从无标签视频中学习预测潜在动作表示，并与世界模型联合优化以预测未来帧，从而将动作表征锚定于环境动态；随后在标注数据上进行动作微调。实验表明，LAWM在LIBERO基准和真实机器人任务上优于使用真实动作训练的模型及同类预训练方法，同时显著提升了效率与实用性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.18428" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>