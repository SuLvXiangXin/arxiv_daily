<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.11609" target="_blank" rel="noreferrer">2512.11609</a></span>
        <span>作者: Jinqiao Wang Team</span>
        <span>日期: 2025-12-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从人类演示中学习已成为具身智能领域的主导范式。然而，人类手与不同形态的机器人手之间存在“具身鸿沟”，这给学习带来了重大挑战。现有方法存在关键局限性：基于重定向的方法通常仅映射运动学姿态而忽略动态信息；模仿学习方法则局限于复现人类操作，由于形态和动力学差异，性能远低于人类水平。一些研究开始探索基于人类演示的强化学习，但其奖励函数通常强制机器人与专家轨迹在每一步都严格对齐，这仍属于模仿学习的范畴，难以发现真正适应机器人自身形态的策略。另一些研究则完全放弃人类先验，仅以物体姿态误差为中心定义奖励函数，这导致训练容易陷入局部最优且收敛困难。此外，现有方法泛化能力有限，大多针对特定机器人手，缺乏适应不同机器人的统一框架。</p>
<p>本文针对上述痛点，提出了超越单纯模仿人类演示、学习适应不同机器人形态的操作策略的新视角。其核心思路是：提出一个统一的强化学习框架UniBYD，通过动态奖励退火机制，引导策略从模仿人类演示平滑过渡到自主探索，从而发现与多样化机器人形态物理特性相契合的操控策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>UniBYD是一个统一且渐进的强化学习框架，旨在从人类演示中学习，并为各种机器人手发现超越单纯模仿的、适应形态的策略。其整体流程是：在训练阶段，策略网络预测的动作与演示中的专家动作混合，生成最终执行的动作以推进环境；在推理阶段，则完全依赖策略网络的动作完成任务。</p>
<p><img src="https://arxiv.org/html/2512.11609v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：UniBYD框架总览。UniBYD首先通过统一形态表示编码不同手型，然后采用带有奖励退火机制的动态PPO，初期利用影子引擎进行高保真模仿，随后过渡到自主探索以发现适应形态的策略。</p>
</blockquote>
<p><strong>核心模块1：统一形态表示</strong><br>为了实现跨形态泛化，UniBYD提出了统一形态表示。对于机器人手h，其本体感知状态包括固定维度的手腕状态和可变维度的关节状态。为解决关节角度周期性问题和统一维度，UMR对关节角度进行三角函数编码，并对自由度少于最大自由度的手进行零填充。此外，UMR还从URDF模型中提取关键静态形态属性构成静态描述符。最终，将手腕状态、填充后的关节状态和静态描述符拼接，形成固定维度的策略观察。这使得策略能够适应不同的手部形态。</p>
<p><strong>核心模块2：动态近端策略优化</strong><br>在UMR提供一致观察空间的基础上，UniBYD采用集成了奖励退火与损失协同平衡机制的渐进强化学习算法。</p>
<ol>
<li><strong>奖励退火</strong>：总奖励是模仿奖励和稀疏目标奖励的动态加权和。模仿奖励是一个密集的多组件奖励，量化当前状态与专家状态的相似性。目标奖励仅在整条轨迹成功完成时给予大幅奖励。动态权重根据训练阶段调整：早期（模仿驱动阶段）权重固定为1，专注于模仿；当模型通过模仿获得基本能力后，进入混合阶段，模仿奖励权重根据近期成功率动态衰减；一旦成功率超过阈值，进入探索阶段，模仿奖励权重降至极小值，更新主要由目标奖励驱动，策略可自由探索适应机器人形态的策略。</li>
<li><strong>损失协同与平衡</strong>：PPO目标函数中集成了熵正则化项和边界损失。熵正则化鼓励早期探索，其系数线性衰减。边界损失则对明显超出物理边界的动作均值进行惩罚。二者形成协同与平衡：熵项促进广泛探索，边界损失确保探索保持在物理可行的平滑动作空间内。</li>
</ol>
<p><strong>核心模块3：基于混合马尔可夫的影子引擎</strong><br>在训练初期，策略网络较弱，轻微的动作偏差会导致状态迅速偏离专家轨迹，造成频繁的早期回合终止。为此，UniBYD引入了影子引擎，在早期提供细粒度的引导。</p>
<ol>
<li><strong>灵巧手控制</strong>：执行的动作是策略预测动作和专家演示动作的动态加权混合。专家动作的权重随训练周期线性衰减至0。这使得早期学习近似于离散的逐点学习，减轻了误差累积，后期则完全由策略处理完整的马尔可夫决策过程。</li>
<li><strong>物体控制</strong>：在复杂任务中，影子引擎还通过PD控制器对物体施加动态支持力，将其约束在目标轨迹附近，防止掉落或严重偏离。该PD控制器的增益也随训练逐渐衰减至零。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.11609v1/x3.png" alt="影子引擎"></p>
<blockquote>
<p><strong>图3</strong>：影子引擎中的动作生成与物体控制概述。它将模型预测动作与专家引导动作混合生成最终执行动作，并通过PD控制器施加专家物体力来引导物体。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：本文构建了首个涵盖多种手部形态的机器人操作基准测试UniManip，包含29个任务类别，评估2指、3指、5指（单/双手）形态。评估指标包括位置误差、朝向误差、成功率以及量化策略与硬件形态契合度的适应分数。<br><strong>对比方法</strong>：与三种代表性基线对比：基于优化的逆运动学的经典重定向方法、当前SOTA方法ManipTrans、以及目标中心的方法DexMachina（*表示在部分任务上因完全失败而无法获得有意义结果）。<br><strong>关键实验结果</strong>：如表1所示，UniBYD在所有手型和任务设置上均大幅超越基线。例如，在5指单手任务上，成功率相比ManipTrans提升了57.72个百分点（从29.75%到87.47%），总体任务成功率相比当前SOTA提升了67.90%。位置误差和朝向误差也显著降低。适应分数表明UniBYD学习到的策略更符合机器人形态。</p>
<p><img src="https://arxiv.org/html/2512.11609v1/sec/fig/finalcombined_compariso.png" alt="对比结果"></p>
<blockquote>
<p><strong>图5</strong>：主要对比实验结果表。UniBYD在成功率上显著超越所有基线方法，尤其在非5指形态上优势明显。</p>
</blockquote>
<p><strong>消融实验</strong>：消融研究验证了各核心组件的贡献。移除影子引擎导致早期训练极不稳定，成功率大幅下降。移除动态奖励退火（固定模仿奖励权重）会使策略过度依赖模仿，无法探索更优的形态适应策略，性能受限。移除UMR（使用原始关节状态）会损害跨形态泛化能力。熵正则化和边界损失的协同作用也被证明对稳定训练至关重要。</p>
<p><img src="https://arxiv.org/html/2512.11609v1/x5.png" alt="消融实验"></p>
<blockquote>
<p><strong>图6</strong>：消融实验（5指单手任务）。展示了移除影子引擎、移除动态奖励退火、移除UMR对成功率的影响，证明了各核心组件的必要性。</p>
</blockquote>
<p><strong>定性结果</strong>：图7-10展示了UniBYD与基线方法在具体任务上的策略对比。可以看到，UniBYD的策略更自然、高效地利用了机器人手的形态特性，而基线方法则可能出现不协调或低效的动作。</p>
<p><img src="https://arxiv.org/html/2512.11609v1/x6.png" alt="定性对比1"></p>
<blockquote>
<p><strong>图7</strong>：在“旋转魔方”任务上的策略对比。UniBYD的策略更适应机器人手结构。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.11609v1/x7.png" alt="定性对比2"></p>
<blockquote>
<p><strong>图8</strong>：在“开抽屉”任务上的策略对比。展示了不同方法策略的差异。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1) 提出了UniBYD，首个兼容多种机器人手型、能够学习适应不同形态策略的统一强化学习框架。2) 设计了集成了混合马尔可夫影子引擎和渐进奖励退火计划的动态PPO学习机制，实现了从精细模仿到自主探索的平滑过渡。3) 构建了首个基于人类演示数据的统一基准测试UniManip，用于跨形态评估机器人操作能力。<br><strong>局限性</strong>：论文自身提到的局限性包括：框架在极其复杂的非拟人形态（如多指节或连续体机械手）上的泛化能力仍需验证；动态PPO和影子引擎涉及多个超参数，调优需要一定经验；仿真到实物的迁移性能是未来需要探索的重点。<br><strong>后续启示</strong>：UniBYD为“超越模仿”的学习范式提供了可行的技术路径。其统一形态表示和动态学习机制为构建通用的机器人技能学习系统提供了思路。未来的工作可以探索更复杂的形态、结合视觉等多模态输入，并着重解决仿真到实物的迁移挑战。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出UniBYD框架，旨在解决机器人手与人类手之间的“具身差距”导致从人类示范学习操作性能受限的核心问题。关键技术包括：统一的形态学表示（UMR）以建模多样手部形态；动态PPO算法配合退火奖励调度，使强化学习从模仿人类示范过渡至探索适应机器人自身形态的策略；以及基于马尔可夫的混合影子引擎，实现细粒度的人类操作模仿。在提出的多手形态操作基准UniManip上，实验表明其成功率相比现有最优方法提升了67.90%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.11609" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>