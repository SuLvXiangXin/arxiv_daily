<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.19528" target="_blank" rel="noreferrer">2511.19528</a></span>
        <span>作者: Yang, Rushuai, Feng, Zhiyuan, Zhang, Tianxiang, Wang, Kaixin, Zhang, Chuheng, Zhao, Li, Su, Xiu, Chen, Yi, Bian, Jiang</span>
        <span>日期: 2025/11/24</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型遵循“大规模预训练+下游任务微调”的主导范式。其效能严重依赖于预训练数据的规模与多样性。目前数据主要来源于人类遥操作，这种方法劳动密集、成本高昂，且人类演示者出于任务成功的目的，倾向于依赖少数高效策略，难以主动展示多样的可行方案，导致数据行为多样性受限。强化学习（RL）通过环境交互自主探索学习技能，是生成高质量数据的潜在途径。然而，标准的基于策略的RL旨在寻找最优策略，通常会导致收敛到固定的单一执行模式，所产生的轨迹缺乏预训练所需的丰富多样性。因此，本文针对如何利用RL生成大规模、多样化、高质量操作轨迹用于VLA预训练这一具体痛点，提出了新视角：将RL训练的目标重新定义为为每个任务发现一系列高成功率的“行为模式”集合。核心思路是提出一个三阶段的“发现、学习、强化”（DLR）框架，首先从人类演示中信息论地发现潜在行为模式，然后学习一个模式条件策略来模仿这些模式，最后利用任务奖励在线强化该策略，从而生成多模式、高成功率的轨迹用于VLA预训练。</p>
<h2 id="方法详解">方法详解</h2>
<p>DLR框架是一个三阶段的离线到在线RL流程，旨在学习一个能根据潜在变量z产生多样化成功轨迹的模式条件策略π_θ(a|s, z)。</p>
<p><img src="https://arxiv.org/html/2511.19528v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DLR框架与标准离线到在线RL基线的对比。上图展示了DLR的三阶段过程：1）从人类数据中发现潜在模式；2）通过行为克隆学习模式条件策略；3）用稀疏成功奖励在线强化每个模式策略。这产生了多样化的多模态状态访问分布。下图展示了标准基线：1）在整个未标记的人类数据集上通过行为克隆初始化策略；2）用稀疏奖励在线强化。该方法导致模式崩溃，产生单模态分布。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.19528v1/x2.png" alt="学习过程"></p>
<blockquote>
<p><strong>图2</strong>：DLR的学习过程示意图。(a) 给定覆盖多种策略的次优人类演示（S_human，灰色），目标是学习多个高成功率、高多样性的策略。(b) 阶段1（发现）：将人类数据中的状态聚类为不同的模式（z1, z2, z3）。(c) 阶段2（学习）：通过行为克隆训练条件策略π(·|z)模仿每个发现模式。(d) 阶段3（强化）：用稀疏任务奖励微调每个模式策略，使其收敛到各自的最优版本。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>理论基础与问题分析</strong>：论文首先分析了将标准任务奖励与鼓励多样性的互信息（MI）目标简单结合的弊端。多样性奖励基于已访问状态训练的判别器q_ϕ(z|s)，当策略探索新状态时，多样性奖励会骤降，而稀疏的任务奖励无法立即补偿，导致探索受罚。此外，盲目最大化多样性可能使策略学习到独特但无用的行为。因此，核心洞察是将多样性目标与策略自身的探索过程解耦，<strong>只要求在已成功的轨迹中保持多样性</strong>。</p>
</li>
<li><p><strong>三阶段实现</strong>：</p>
<ul>
<li><strong>阶段1：发现</strong>：使用基于VAE的框架，在成功的人类演示数据集𝒟_human上最大化I(Z; S)。训练一个编码器q_ϕ(z|s)，将状态映射到潜在模式变量z，从而将轨迹聚类为不同的语义模式。训练完成后，编码器固定，用于后续阶段的模式推断。</li>
<li><strong>阶段2：学习</strong>：使用训练好的编码器q_ϕ为𝒟_human中的每个状态分配一个潜在标签z，创建带标签的数据集𝒟̃_human。然后通过行为克隆（BC）在此数据集上训练模式条件策略π_θ(a|s, z)，其目标是最大化对数似然。这使策略的初始状态分布与人类数据（作为成功状态流形的估计）对齐。</li>
<li><strong>阶段3：强化</strong>：最后，仅使用环境提供的稀疏任务奖励（成功时为+1，否则为0）在线微调策略。对每个从固定分布（如均匀分布）采样的模式z，优化其条件策略以最大化期望回报。此阶段不再使用MI相关的稠密奖励。</li>
</ul>
</li>
<li><p><strong>理论保证</strong>：论文提供了定理分析，解释了为何在阶段3使用PPO等算法仅优化稀疏奖励时，不同模式的政策不会相互干扰或坍塌成一个。核心假设包括“失败隔离带”（不同成功模式间被失败区域隔开）、“分离初始化”（阶段2后策略的跨模式泄漏很小）以及PPO的KL约束。在这些条件下，更新每个模式策略的梯度主要受其自身成功轨迹支配，跨模式影响有上界，从而保证了多样性得以保持。</p>
</li>
</ol>
<p><strong>创新点</strong>：与标准RL（易模式坍塌）或简单结合任务与多样性目标的方法（阻碍探索）相比，DLR的关键创新在于<strong>分阶段解耦</strong>了多样性发现、策略学习和性能强化，并利用固定的人类演示作为稳定的成功状态流形估计来引导整个过程，从而在确保高成功率的前提下，系统性地生成多样化行为模式。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在LIBERO仿真基准测试中进行评估。使用DLR框架为LIBERO-90中的任务训练轻量级RL策略并收集轨迹，用于预训练VLA模型。随后，在未见过的下游任务套件（LIBERO-spatial, object, goal, long）上对预训练的VLA模型进行微调，测试其泛化能力。对比的基线是使用标准离线到在线RL（即图1下半部分流程）收集的等量数据预训练的VLA模型。</p>
<p><img src="https://arxiv.org/html/2511.19528v1/x3.png" alt="实验设置"></p>
<blockquote>
<p><strong>图3</strong>：实验设置与数据生成流程。(a) 使用DLR训练策略并收集多模式轨迹数据。(b, c) 用RL生成的数据预训练VLA模型，然后快速适应未见过的下游任务。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>下游任务性能提升</strong>：在四个下游任务套件上，使用DLR生成的多模式RL数据预训练的VLA模型，其平均成功率显著高于使用标准RL数据预训练的模型。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.19528v1/x5.png" alt="下游任务性能"></p>
<blockquote>
<p><strong>图5</strong>：在LIBERO下游任务套件上的微调成功率。DLR预训练的VLA模型（橙色）在所有套件上均优于使用标准RL数据预训练的基线（蓝色），平均绝对提升6.8%。</p>
</blockquote>
<ol start="2">
<li><strong>数据缩放行为</strong>：当使用DLR收集数据时，VLA模型的性能随着预训练数据量的增加而持续提升。而使用标准RL数据时，性能在数据量达到一定程度后即饱和。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.19528v1/x6.png" alt="数据缩放"></p>
<blockquote>
<p><strong>图6</strong>：预训练数据量对下游任务平均成功率的影响。DLR（橙色）表现出积极的缩放趋势，而标准RL（蓝色）在约10k轨迹后增长停滞。</p>
</blockquote>
<ol start="3">
<li><strong>轨迹多样性分析</strong>：DLR生成的轨迹在状态覆盖率和动作熵等多样性指标上均优于标准RL。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.19528v1/x7.png" alt="多样性指标"></p>
<blockquote>
<p><strong>图7</strong>：在90个预训练任务上，DLR与标准RL生成轨迹的多样性对比。DLR在状态覆盖率（左）和动作熵（右）上均更高。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：验证了DLR三个阶段的必要性。移除任一阶段都会导致性能下降，其中“发现”阶段对多样性至关重要，“强化”阶段对最终成功率提升贡献最大。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.19528v1/x8.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：DLR各阶段消融研究。完整的DLR（橙色）性能最佳。移除“发现”阶段（绿色）或“学习”阶段（红色）性能下降，仅进行“强化”（紫色）效果最差。</p>
</blockquote>
<ol start="5">
<li><strong>定性结果</strong>：DLR能够为同一任务发现多种直观上不同的成功策略。</li>
</ol>
<p><img src="https://arxiv.org/html/2511.19528v1/x4.png" alt="定性展示"></p>
<blockquote>
<p><strong>图4</strong>：DLR发现的多种行为模式示例。左：将书放入收纳盒，有顺时针或逆时针旋转书本两种策略。中：打开炉门，有不同接触和施力策略。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个原则性的三阶段框架DLR，能够利用强化学习生成高质量、多样化的机器人轨迹用于VLA预训练。</li>
<li>提供了理论分析，证明了DLR在仅使用稀疏奖励进行在线强化时，能够保持发现模式的多样性，防止坍塌到单一解。</li>
<li>通过实验证明，DLR生成的数据不仅能提供多样的成功轨迹，而且基于此预训练的VLA模型在下游任务微调中表现更优，并展现出积极的数据缩放特性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，DLR框架在阶段1依赖于固定的人类演示数据集作为模式发现的起点。这可能限制了其发现人类演示中完全未包含的全新、潜在更优策略的能力。</p>
<p><strong>启示</strong>：这项工作表明，多模式RL可以作为一个实用、可扩展的数据引擎，为具身基础模型提供数据。它为实现从以人为中心到算法生成的数据管道转变提供了可能，有望降低数据收集成本，并实现有原则的规模化扩展。后续研究可探索更无监督的模式发现方法，或将该框架应用于更复杂、开放领域的任务。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型预训练数据稀缺且多样性不足的核心瓶颈，提出DLR框架。该方法通过基于VAE的模式发现、模式条件策略学习和在线强化三个阶段，利用强化学习生成多种高成功率的行为模式，以替代昂贵的人类遥操作数据。实验表明，在LIBERO基准上，DLR相比标准RL能产生显著更多样的轨迹，学习到多种任务策略，覆盖更广的状态-动作空间。基于此数据预训练的VLA模型，在下游任务中性能优于使用同等规模标准RL数据的模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.19528" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>