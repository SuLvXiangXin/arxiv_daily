<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2507.16815" target="_blank" rel="noreferrer">2507.16815</a></span>
        <span>作者: Fu-En Yang Team</span>
        <span>日期: 2025-07-22</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，将视觉-语言模型（VLMs）应用于具身任务的主流方法是构建视觉-语言-动作（VLA）模型，通常通过在大规模机器人演示数据（如Open X-Embodiment数据集）上对预训练的多模态大语言模型（MLLMs）进行后训练，以端到端的方式直接从视觉和文本输入映射到低级动作。然而，这种方式绕过了结构化的规划或中间推理，限制了模型处理复杂指令、长视野目标或分布外场景的能力。近期一些工作尝试引入显式的思维链（CoT）提示作为中间步骤指导，并通过完全监督微调（SFT）进行训练，但高质量推理轨迹的标注成本高昂，且模型容易过拟合到特定的视觉场景或推理模式。</p>
<p>本文针对现有VLA模型缺乏显式、可泛化的多步推理能力，以及难以将推理与真实世界动作执行有效衔接的痛点，提出了一个名为ThinkAct的双系统架构新视角。其核心思路是通过基于动作对齐的视觉反馈的强化学习，激励MLLM进行具身推理并生成视觉规划潜在变量，进而指导下游动作模型在目标环境中进行鲁棒的动作执行，从而实现“先思考，再行动”。</p>
<h2 id="方法详解">方法详解</h2>
<p>ThinkAct采用双系统架构，连接结构化推理与可执行动作。整体流程分为两个核心模块：1) 用于具身推理的推理MLLM（ℱθ），负责生成视觉规划潜在变量（ct）；2) 用于动作执行的动作模型（πφ），根据ct和当前状态（观测ot，指令l）预测可执行动作。</p>
<p><img src="https://arxiv.org/html/2507.16815v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ThinkAct整体框架概述。(a) 给定观测ot和指令l，ThinkAct利用从视觉轨迹τ推导出的动作对齐奖励，来激励推理MLLM ℱθ的具身推理能力。(b) 在视觉规划潜在变量ct的条件下，基于DiT的动作模型πφ学习预测可执行动作，同时保持ℱθ冻结。推理和执行可异步操作，实现VLA推理任务的慢思考与快控制。</p>
</blockquote>
<p><strong>核心模块一：基于强化学习的视觉潜在规划</strong>。该模块旨在通过强化学习激励MLLM进行与视觉场景接地的长视野规划。其创新在于设计了一种新颖的<strong>动作对齐视觉反馈</strong>作为奖励信号，该奖励由目标奖励（r_goal）和轨迹奖励（r_traj）组成。</p>
<ul>
<li><strong>目标奖励</strong>：鼓励模型预测的轨迹起点和终点与通过现成检测器获得的真实轨迹起点和终点对齐，计算公式为两者之间L2距离的补数。</li>
<li><strong>轨迹奖励</strong>：鼓励模型预测的整个轨迹在分布上与真实轨迹匹配，使用动态时间规整（DTW）距离来衡量相似性。<br>整体视觉奖励r_visual是两者的加权和（ω_goal = ω_traj = 0.5）。最终奖励r是视觉奖励与格式正确性奖励r_format的加权组合（0.9 : 0.1）。</li>
</ul>
<p>使用分组相对策略优化（GRPO）对MLLM ℱθ进行强化微调。GRPO从当前策略采样一组响应，根据上述奖励评估其相对优势，并通过优化目标函数（包含优势函数和KL散度正则项）来更新模型，以生成高质量的推理步骤并抽象出紧凑的视觉规划潜在变量ct。</p>
<p><strong>核心模块二：推理增强的动作适应</strong>。目标是将推理MLLM推断出的高级意图ct与目标环境的动作模型πφ连接起来。具体而言，在基于Transformer（如DiT）的动作模型πφ中，通过一个潜在投影器（如Q-Former）将ct融入其输入空间。在训练时，冻结ℱθ，仅通过模仿学习更新πφ的状态编码器、潜在投影器和动作模型本身，损失函数为预测动作与标注动作之间的差异。推理和执行可异步进行，即每个规划潜在变量ct对应与环境进行N步交互。</p>
<p><strong>学习策略</strong>：采用多阶段训练。首先，MLLM ℱθ使用监督数据（轨迹和QA数据）进行SFT冷启动；动作模型πφ在OXE数据集上预训练。然后，使用动作对齐奖励对ℱθ进行GRPO强化微调。最后，在目标环境数据上，冻结ℱθ，对条件化于ct的πφ进行模仿学习微调。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：模型在16张NVIDIA A100 GPU上训练。推理MLLM ℱθ基于Qwen2.5-VL 7B初始化。动作模型πφ为基于DiT的策略。使用了包含机器人轨迹、人类视频以及多个具身QA数据集（如RoboVQA, EgoPlan-IT）的混合数据进行训练。</p>
<p><strong>评测基准</strong>：</p>
<ul>
<li><strong>机器人操控</strong>：SimplerEnv（评估视觉匹配和变体聚合的鲁棒性）和LIBERO（评估跨空间布局、物体变化、目标多样性和长视野规划的泛化能力）。</li>
<li><strong>具身推理</strong>：EgoPlan-Bench2（多步规划多选题）、RoboVQA（自由形式QA，用BLEU评分）和OpenEQA（零样本具身理解，用LLM评分）。</li>
</ul>
<p><strong>对比方法</strong>：包括Octo-Base、RT1-X、OpenVLA、DiT-Policy、TraceVLA、CoT-VLA、Magma等VLA模型，以及GPT-4V、LLaVA-Video等通用MLLM。</p>
<p><img src="https://arxiv.org/html/2507.16815v2/x5.png" alt="实验结果表1"></p>
<blockquote>
<p><strong>表1</strong>：在SimplerEnv和LIBERO机器人操控任务上的定量对比。ThinkAct在SimplerEnv的三个分集上均取得最高总体成功率（71.5%， 65.1%， 43.8%），相比基线DiT-Policy有显著提升（+15.5%， +16.9%， +11.4%）。在LIBERO上，ThinkAct以84.4%的总体成功率超越所有对比方法，证明了其在多样化和长视野操控任务上的有效性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.16815v2/x6.png" alt="实验结果表2"></p>
<blockquote>
<p><strong>表2</strong>：在具身推理任务上的定量对比。ThinkAct在EgoPlan-Bench2、RoboVQA和OpenEQA上均取得最佳或极具竞争力的性能（总体得分48.2%， BLEU-1 69.1， 总体得分56.2%），展示了其强大的多步规划、长视野推理和零样本场景理解能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.16815v2/x3.png" alt="定性结果图1"></p>
<blockquote>
<p><strong>图3</strong>：SimplerEnv和LIBERO任务的定性结果，展示了中间推理步骤和可视化的规划轨迹。例如，在LIBERO长视野任务中，模型成功将指令分解为“拿起书”、“从左移到右”、“放入隔间”三个子任务，并且执行时夹爪紧密跟随规划轨迹。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.16815v2/x4.png" alt="定性结果图2"></p>
<blockquote>
<p><strong>图4</strong>：RL微调前后推理过程的定性对比。在RoboVQA和OpenEQA任务中，经过RL微调的模型能够进行正确的多步推理并给出准确答案（绿色），而仅经过SFT冷启动的模型则推理失败或误解问题（红色），证明了RL对于提升推理能力的关键作用。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2507.16815v2/x7.png" alt="消融实验图"></p>
<blockquote>
<p><strong>表5</strong>：对目标奖励和轨迹奖励的消融研究。完整版ThinkAct性能最佳。移除轨迹奖励或目标奖励均会导致性能下降，表明两者对于学习连贯规划和长视野推理都至关重要。当两者均被移除（仅剩QA奖励）时，性能提升有限，仅略优于SFT基线，证实了动作对齐视觉反馈对于有效多步规划的关键性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了ThinkAct双系统框架，通过视觉潜在规划将基于视觉接地的具身推理与动作执行相互增强。</li>
<li>设计了基于目标完成度和轨迹对齐度的动作对齐视觉奖励，利用强化学习激励MLLM进行长视野、接地气的推理。</li>
<li>通过将推理步骤压缩为视觉规划潜在变量来引导下游动作执行，实现了跨环境的推理增强轨迹指导，并展示了模型在少样本适应、长视野规划和自我纠正方面的能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身未明确列出局限性，但从方法描述可推断，其视觉奖励依赖于现成的轨迹检测器（用于获取真实轨迹τ̂），这可能在缺乏高质量检测器的场景或复杂视觉环境中成为瓶颈。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>奖励设计</strong>：探索更复杂或更稠密的动作对齐奖励信号，以处理更精细或动态的规划任务。</li>
<li><strong>规划表示</strong>：研究除2D轨迹点之外的其他紧凑潜在表示（如3D路径、技能原语序列），以编码更丰富的规划意图。</li>
<li><strong>交互式学习</strong>：将ThinkAct的离线规划与在线交互学习结合，使机器人能在执行过程中实时更新其规划和策略。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作推理任务中，现有端到端模型缺乏显式推理、难以实现长时程规划和适应复杂任务的问题，提出ThinkAct双系统框架。其核心方法通过强化视觉潜在规划连接高层推理与低层执行：训练多模态LLM生成基于目标完成和轨迹一致性的动作对齐视觉奖励的推理计划，并压缩为视觉潜在表示以条件化下游动作模型。实验在具身推理和机器人操作基准上验证，ThinkAct实现了少样本适应、长时程规划和自校正行为。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2507.16815" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>