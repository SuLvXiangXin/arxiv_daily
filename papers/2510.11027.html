<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.11027" target="_blank" rel="noreferrer">2510.11027</a></span>
        <span>作者: Yang, Ganlin, Zhang, Tianyi, Hao, Haoran, Wang, Weiyun, Liu, Yibin, Wang, Dehui, Chen, Guanzhou, Cai, Zijian, Chen, Junting, Su, Weijie, Zhou, Wengang, Qiao, Yu, Dai, Jifeng, Pang, Jiangmiao, Luo, Gen, Wang, Wenhai, Mu, Yao, Hou, Zhi</span>
        <span>日期: 2025/10/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，具身人工智能的发展主要沿着两条路径：一是利用视觉语言模型（VLMs）增强智能体的感知泛化和推理能力，开发专注于具身推理的模型；二是将先进的VLMs扩展到视觉-语言-动作模型（VLAs），用于端到端的机器人控制。然而，这两条路径之间存在一个关键的研究空白：上游基于VLM的推理能力与下游VLA策略学习之间如何有效桥接尚不明确。现有VLA模型通常直接微调预训练的VLM，但何种VLM主干网络（初始化）能最有效地加速下游VLA策略学习的收敛并提高成功率，仍是一个未被充分探索的问题。</p>
<p>本文针对上述痛点，旨在构建一个具备协同具身推理能力的VLA模型（Vlaser），并系统性地探究不同VLM初始化对监督式VLA微调的影响。核心思路是：首先构建一个高质量、多任务的具身推理数据集（Vlaser-6M），用于增强VLM的具身推理能力；然后基于此VLM，通过添加动作专家模块进行VLA微调，并分析不同数据流（特别是领域内与领域外数据）对下游VLA性能的贡献。</p>
<h2 id="方法详解">方法详解</h2>
<p>Vlaser旨在为具身智能体整合高级推理与低级控制，并识别对VLA模型最关键的数据流。其整体框架包含两个主要组件和相应的两阶段训练流程。</p>
<p><img src="https://arxiv.org/html/2510.11027v2/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：Vlaser架构示意图。包含两个组件：1) 基于数据引擎进行具身推理增强的多模态预训练；2) 在动作专家模块上进行的、基于流匹配动作生成的VLA训练。</p>
</blockquote>
<p><strong>1. 模型结构</strong></p>
<ul>
<li><strong>VLM主干网络</strong>：以InternVL3为基础，选用2B和8B两种规模以适应机器人计算约束。视觉编码器为InternViT，语言模型分别为Qwen2.5-1.5B和Qwen2.5-7B。其核心是增强模型的具身常识推理能力。</li>
<li><strong>动作专家</strong>：在VLM基础上扩展，用于低层级的机器人控制。该模块采用类似混合专家（MoE）的设计，其中原始参数处理图像和文本输入，而新增的一组独立权重（即动作专家）专门处理机器人状态和动作令牌的输入输出。采用<strong>流匹配</strong>技术进行动作预测：在训练时，对动作块添加噪声，并训练网络预测去噪向量场；在推理时，从随机噪声开始，通过积分学习到的向量场来生成动作序列。动作专家与语言模型共享自注意力机制，并使用非因果注意力。</li>
</ul>
<p><strong>2. Vlaser数据引擎（Vlaser-6M数据集）</strong><br>该数据集是模型具身推理能力的基石，包含约600万数据，涵盖多种具身推理任务：</p>
<ul>
<li><strong>具身定位数据</strong>（约180万）：包含边界框和中心点两种2D定位格式，数据来源于多个公开数据集（如RoboPoint, ShareRobot等），并利用SA-1B分割掩码生成了额外的30万标注以增强开放词汇泛化。</li>
<li><strong>通用与空间推理数据</strong>（约170万）：包括120万通用机器人视觉问答（RoboVQA）数据和50万专门增强空间智能的数据，来源广泛（如RoboVQA, SPAR, SpaceR-151k等），并辅以从3D场景数据集手动标注的10万样本。</li>
<li><strong>规划数据</strong>（约40万）：涵盖基于语言的规划和多模态任务数据，来源包括Alpaca-15k-Instruction、MuEP、WAP、LLaRP以及以自我为中心的视频数据集（EgoPlan-IT, EgoCOT）。</li>
<li><strong>领域内数据</strong>（约200万）：专门为促进VLA的端到端策略学习而生成，与具身推理上下文对齐。数据来源于仿真平台SimplerEnv（包含Google Robot和WidowX Robot embodiment）和RoboTwin（包含双臂Aloha-AgileX Robot），并涵盖上述所有专业类别（定位、空间智能、规划、机器人状态VQA）。</li>
</ul>
<p><strong>3. 训练方法</strong><br>采用两阶段训练方案：</p>
<ul>
<li><strong>视觉-语言预训练</strong>：使用自回归语言建模损失，在Vlaser-6M数据集上对InternVL3进行监督微调，以增强其具身推理能力。</li>
<li><strong>视觉-语言-动作微调</strong>：冻结预训练好的VLM参数，仅训练新增的动作专家模块。使用流匹配损失（公式2）在机器人特定数据集（如SimplerEnv中的episodes）上进行优化。推理时，通过公式3从噪声中积分出动作。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，Vlaser不仅构建了一个全面的具身推理增强VLM，更重要的是，它系统性地研究了不同VLM数据流（特别是通过其数据引擎构造的各类数据）对下游VLA性能的影响，为未来构建面向任务的具身VLM提供了实证依据。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>1. 具身推理能力评估</strong></p>
<ul>
<li><strong>评估基准与基线</strong>：在12个具身推理基准上进行了全面评估，涵盖具身QA、规划、具身定位、空间智能和闭环仿真。对比了闭源模型（GPT-4o, Claude-3.7-Sonnet, Gemini-2.5-Pro）、小规模开源MLLMs（2B-3B，如ChatVLA-2B, Qwen2.5VL-3B等）和中规模开源MLLMs（7B-8B，如InternVL3-8B, Embodied-R1-7B等）。</li>
<li><strong>关键结果</strong>：如表1所示，Vlaser相比其基础模型InternVL3取得了显著提升。Vlaser-2B平均分从15.2提升至45.3，Vlaser-8B从22.3提升至51.3。在与同期最先进的具身专用VLMs（如RoboBrain2.0, Embodied-R1）对比中，Vlaser在多数基准上领先，并取得了最高的综合平均分（51.3），领先优势约10%。结果表明，Vlaser在具身定位和仿真评估方面提升尤为显著，且模型规模与任务复杂度存在关联：Vlaser-2B在简单的点定位任务上表现更好，而Vlaser-8B在需要多步推理的规划和闭环仿真任务上优势明显。</li>
</ul>
<p><strong>2. 下游闭环机器人任务性能</strong></p>
<ul>
<li><strong>评估平台</strong>：主要在SimplerEnv仿真套件中进行评估，该环境能可靠反映真实机器人性能。任务涉及WidowX和Google Robot两种机器人本体。也在RoboTwin平台上进行了双手机器人任务的补充实验。</li>
<li><strong>WidowX任务结果</strong>：如表2所示，以Vlaser-2B为初始化进行VLA微调后，在四个任务上的平均成功率达到了65.1%，显著优于基线模型InternVL3-2B（41.8%），也超过了当前表现最好的基线方法π0（54.9%）。</li>
<li><strong>Google Robot任务结果</strong>：如表3所示，Vlaser-2B在“视觉匹配”和“变体聚合”两个任务组上的平均成功率分别为64.0%和54.8%，综合性能具有竞争力。</li>
<li><strong>数据流消融实验</strong>：论文系统研究了Vlaser-6M中不同数据流对下游VLA微调的影响。关键发现是：<strong>领域外（OOD）的具身推理数据虽然能提升上游VLM的基准测试分数，但这些增益并不能直接或显著地转化为下游VLA性能的提升</strong>。相反，<strong>领域内数据</strong>（直接基于机器人交互数据集生成）对于加速VLA微调收敛和提高任务成功率 substantially more effective。例如，仅使用领域内QA数据微调的Vlaser-QA (2B)在WidowX任务上取得了62.6%的平均成功率，而使用所有领域外数据微调的Vlaser-OOD (2B)仅为43.2%。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>开源了具备强大具身推理能力的视觉语言模型Vlaser及其构建数据集Vlaser-6M，支持从感知推理到端到端控制的完整流程。</li>
<li>首次系统性地分析了不同视觉语言预训练数据对下游VLA策略学习迁移的有效性，关键发现是领域内数据比领域外数据对VLA性能提升更为关键。</li>
<li>在广泛的具身推理基准测试和下游仿真机器人操控任务上达到了最先进的或具有高度竞争力的性能，证明了其综合能力。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，模型规模（2B vs 8B）的选择需要根据目标应用的需求（简单直接回答 vs 复杂多步推理）进行权衡。此外，研究也揭示了当前具身感知推理基准与真实世界具身任务之间存在领域差距。</p>
<p><strong>后续启示</strong>：本文的发现为未来具身视觉语言模型的构建指明了重要方向：亟需缩小当前具身感知推理基准与真实具身任务之间的领域差距，并推动建立相应的闭环评估体系。构建<strong>任务感知的数据流</strong>以桥接互联网规模的预训练与具身特定的微调，将是提升VLA模型实际效能的关键。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出Vlaser模型，旨在解决具身智能中上游视觉语言模型推理与下游视觉-语言-动作策略学习之间的关键脱节问题。关键技术包括构建高质量的Vlaser-6M数据集，并系统研究不同VLM初始化对监督式VLA微调的影响，以缓解互联网预训练数据与具身策略数据间的领域偏移。实验表明，Vlaser在空间推理、具身基础、问答与任务规划等多个基准上达到最先进性能，并在WidowX基准上取得SOTA结果，在Google Robot基准上表现具有竞争力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.11027" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>