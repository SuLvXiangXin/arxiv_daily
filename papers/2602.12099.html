<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.12099" target="_blank" rel="noreferrer">2602.12099</a></span>
        <span>作者: Zheng Zhu Team</span>
        <span>日期: 2026-02-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前主流的视觉-语言-动作模型通常基于当前观测直接预测多步动作片段，这种架构存在固有的局限性：对场景的理解受限于即时观测，未来预测能力较弱，本质上偏向于反应式控制而非前瞻性规划。与此同时，在互联网规模视频数据上预训练的视频世界模型展现出强大的时空推理和准确的未来预测能力。因此，本文针对VLA模型在长时程规划上的“短视”痛点，提出将世界模型作为增强VLA学习的基础。核心思路是提出一个名为RAMP（基于世界模型条件策略的强化学习）的训练框架，通过世界模型预测的未来状态和价值作为丰富条件，来迭代优化VLA策略，从而赋予其前瞻性规划和自我改进能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>GigaBrain-0.5M* 建立在基础VLA模型 GigaBrain-0.5 之上，并集成了RAMP训练框架。GigaBrain-0.5 采用混合Transformer架构，使用预训练的PaliGemma-2视觉语言模型编码多模态输入，并利用基于流匹配的扩散Transformer来预测动作块。它还生成包含自回归子目标语言、离散动作令牌和2D操作轨迹的具身思维链，所有组件在一个统一的损失函数下联合优化。</p>
<p>RAMP框架的核心创新在于利用世界模型的预测来条件化策略学习。该方法包含四个迭代阶段，形成一个自我改进的闭环。</p>
<p><img src="https://arxiv.org/html/2602.12099v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：RAMP框架概述。包含四个阶段：(1) 世界模型预训练，建立未来状态预测和价值估计的统一表示空间；(2) 带世界模型条件的策略训练，用世界模型预测的未来状态和价值优势初始化策略；(3) 人在回路数据收集，通过自主执行和专家修正生成多样化的高质量轨迹；(4) 使用收集的数据进行持续训练，利用标注的轨迹数据更新策略。这个紧密集成的闭环过程促进了策略的持续精化和自我改进。</p>
</blockquote>
<p><strong>阶段1：世界模型预训练</strong>。此阶段训练一个世界模型 𝒲_ϕ，能够联合预测未来视觉状态和价值估计。模型采用Wan2.2作为主干，通过流匹配进行训练。为了统一表示，未来视觉观测被编码为时空视觉潜在表示 𝐳_t，同时当前价值估计 v_t 和本体感知状态 𝐩_t 通过空间平铺投影 Ψ(·) 转换为相同空间维度的张量。最终的潜在状态 𝐬_t 由视觉潜在、价值潜在和本体感知潜在在通道维度拼接而成。世界模型的训练目标是预测未来潜在状态序列 𝐬_future。该模型在超过4千小时的真实机器人操作数据上进行预训练。</p>
<p><strong>阶段2：带世界模型条件的策略训练</strong>。此阶段从预训练的GigaBrain-0.5检查点初始化策略，并使用世界模型预测的条件进行微调。策略接收两个由世界模型生成的辅助信号：1) 未来状态令牌 𝐳_future；2) 价值估计 v_t（通过n步时序差分转换为动作优势A，并离散化为二元改进指示符 I = 𝟙(A &gt; ϵ)）。策略的训练目标是最小化公式(3)定义的加权负对数似然损失，使其能够基于条件元组 (I, 𝐳) 生成动作。为了防止对合成世界模型信号的过度依赖，训练中采用了随机注意力掩码（掩码概率 p=0.2）来随机抑制世界模型令牌，迫使策略在条件输入部分或完全缺失时仍能保持鲁棒性能。</p>
<p><strong>阶段3：人在回路数据收集</strong>。将条件化策略部署到真实环境中收集轨迹数据。该数据集是自主执行和专家干预的混合。自主执行产生的动作分布与策略本身更匹配，提供了更有效的监督信号。当自主执行失败时，人类专家进行纠正。为了确保轨迹的时间连贯性，开发了专门的软件来自动检测并移除干预边界处的过渡伪影，生成干净、连续的数据集。</p>
<p><strong>阶段4：使用收集数据进行持续训练</strong>。使用阶段3收集的HILR数据集对策略进行微调，以掌握从自主执行和专家修正中涌现的复杂长时程行为。为了防止优势值崩溃，世界模型 𝒲_ϕ 会与HILR数据和基础数据一起进行联合训练。策略训练继续保持随机注意力掩码。这个“收集-标注-训练”循环迭代进行，形成一个自我改进的闭环。</p>
<p><strong>推理</strong>。部署时，采用乐观控制策略，固定优势指示符 I = 1。得益于训练时的随机掩码，模型支持两种执行模式：1) <strong>高效模式</strong>：绕过世界模型，策略仅基于当前观测行动，最大化推理频率；2) <strong>标准模式</strong>：世界模型主动生成未来潜在状态 𝐳，为策略提供密集的前瞻指导，用于复杂的长期规划。</p>
<p><strong>与现有方法的创新对比</strong>。RAMP受RECAP启发，两者都使用额外信息作为VLA的条件。但RECAP仅使用稀疏的优势信号（0或1），信息增益有限。RAMP则利用了预训练世界模型预测的未来状态，提供了密集的几何结构和物理动力学先验，带来了显著的信息增益。论文从概率建模角度证明了RECAP是RAMP的一个特例（当忽略未来潜在状态信息时），从信息论角度证明了引入时空潜在表示 𝐳 能显著降低动作生成的条件熵。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>基准与数据集</strong>：实验评估分为两部分。首先评估基础模型 GigaBrain-0.5 的性能，使用了8个内部设计的任务（果汁准备、盒子移动、桌面清理、纸巾准备、衣物折叠、衣物收集、盒子打包、意式浓缩咖啡准备）和公开基准RoboChallenge的30个任务。其次，评估RAMP强化学习框架，在内部挑战性任务上与基线方法进行对比。</p>
<p><strong>基线方法</strong>：基础模型对比了 π_0.5 和 GigaBrain-0。强化学习方法对比了AWR和RECAP。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>基础模型性能</strong>：GigaBrain-0.5在内部评估的8个任务上表现出色。在RoboChallenge公开基准上，其某个中间版本取得了领先排名。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.12099v1/images/gigabrain05_exp.png" alt="基础模型性能"></p>
<blockquote>
<p><strong>图4</strong>：GigaBrain-0.5在内部任务评估上的性能表现，展示了其在多种复杂操作任务上的能力。</p>
</blockquote>
<ol start="2">
<li><strong>RAMP vs. 其他RL方法</strong>：在衣物折叠、盒子打包和意式浓缩咖啡准备等挑战性任务上，RAMP相比RECAP基线取得了约30%的性能提升，并显著优于AWR方法。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.12099v1/x4.png" alt="RAMP性能对比"></p>
<blockquote>
<p><strong>图6</strong>：RAMP与基线方法（AWR， RECAP）在三个挑战性长时程任务（衣物折叠、盒子打包、意式浓缩咖啡准备）上的成功率对比。RAMP显著优于所有基线。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：论文对世界模型中的价值预测模块进行了消融研究。结果显示，移除价值预测（仅使用未来状态条件）或使用随机价值都会导致性能显著下降，验证了价值信息对于策略学习的重要性。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.12099v1/x5.png" alt="价值预测消融实验"></p>
<blockquote>
<p><strong>图7</strong>：世界模型中价值预测模块的消融实验结果。对比了完整RAMP、移除价值预测（仅状态）、使用随机价值以及RECAP基线。完整RAMP性能最优，证明了价值预测的关键作用。</p>
</blockquote>
<ol start="4">
<li><strong>数据分布</strong>：GigaBrain-0.5的预训练数据超过1万小时，包括超过6千小时的世界模型生成数据和约4千小时的真实机器人数据。</li>
</ol>
<p><img src="https://arxiv.org/html/2602.12099v1/images/pretrain-data.png" alt="预训练数据分布"></p>
<blockquote>
<p><strong>图3</strong>：GigaBrain-0.5预训练阶段的数据组成，显示了世界模型生成数据与真实机器人数据的比例。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了RAMP，一个基于世界模型的强化学习框架，通过将世界模型预测的未来状态和价值作为密集条件，显著增强了VLA模型的长时程规划和适应能力。</li>
<li>实现了一个包含世界模型预训练、策略条件训练、人在回路数据收集和持续训练的闭环自我改进系统，使VLA策略能够通过与环境交互和人类反馈不断进化。</li>
<li>在多个真实机器人复杂操作任务上验证了方法的有效性，相比现有强化学习方法（如RECAP）取得了约30%的性能提升，并展示了可靠的长时间执行能力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，该方法的性能依赖于世界模型预测未来状态的准确性和物理合理性。世界模型的质量直接影响了提供给策略的条件信息的有效性。</p>
<p><strong>研究启示</strong>：RAMP展示了将世界模型与VLA策略紧密集成的巨大潜力，为克服VLA模型的“短视”问题提供了一条有效路径。其“预测-规划-执行-改进”的闭环范式，以及利用世界模型提供密集、结构化未来信息的思想，可能启发更多关于如何将强大的生成式世界模型先验知识注入到具身智能决策中的研究。此外，支持有条件/无条件推理的灵活部署模式，为平衡计算开销与任务复杂度提供了实用方案。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对现有视觉-语言-动作模型在场景理解与未来预测方面的局限性，提出GigaBrain-0.5M*模型。其核心方法是RAMP，即通过世界模型驱动的强化学习框架，以迭代方式进行世界模型预训练、策略微调、真实环境部署与持续训练。实验表明，该方法在折叠衣物、装箱和制作咖啡等复杂任务上，相比RECAP基线取得约30%的性能提升，并展现出可靠的长期任务执行能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.12099" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>