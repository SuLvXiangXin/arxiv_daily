<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.17811" target="_blank" rel="noreferrer">2506.17811</a></span>
        <span>作者: Kwok, Jacky, Agia, Christopher, Sinha, Rohan, Foutter, Matt, Li, Shulu, Stoica, Ion, Mirhoseini, Azalia, Pavone, Marco</span>
        <span>日期: 2025/06/21</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型在机器人视觉运动控制方面展现出强大能力。提升其鲁棒性和泛化性的研究主要集中于预训练阶段（如扩大数据收集、优化数据混合、改进模型架构）和后训练阶段（如思维链微调、偏好对齐）。然而，在部署阶段，VLA模型通常被设计为每观察一次生成单个动作块，对测试时计算资源的缩放关注较少。这导致模型在非结构化现实环境中面临抓取不精确、任务推进失败、与物体碰撞等多种失败模式。</p>
<p>本文针对VLA模型在部署时鲁棒性不足的痛点，提出一个新视角：将机器人控制从纯粹的生成问题，转变为“生成-验证”范式。核心思路是，在测试时通过重复采样生成多样化的候选动作，并利用一个训练好的动作验证器从中选择最优动作，从而利用部署时的额外计算来显著提升现有VLA模型的性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>RoboMonkey是一个测试时缩放框架，其整体流程分为两个阶段：1）训练一个基于VLM的动作验证器；2）在部署时进行测试时计算缩放，执行“生成-然后-验证”的流程。</p>
<p><img src="https://arxiv.org/html/2506.17811v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：RoboMonkey方法整体框架。<strong>阶段1（左）</strong>：训练动作验证器。利用模仿学习数据集，从通用机器人策略中采样候选动作，通过聚类得到代表性动作，构建合成动作偏好对用于训练验证器。<strong>阶段2（右）</strong>：缩放测试时计算。部署时，从策略中采样少量初始动作，拟合高斯分布并利用多数投票构建动作提议分布，从中高效采样候选动作，最后由训练好的验证器选择最优动作。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>合成数据生成管道</strong>：为了训练动作验证器，需要大规模的偏好数据。论文提出自动生成合成偏好数据的方法：对于辅助数据集 <code>D_buf</code> 中的每个 <code>(s_t, a_t*, I)</code> 元组，使用一个参考机器人策略生成 <code>N</code> 个候选动作。为确保多样性，应用聚类算法将其减少到 <code>K</code> 个代表性动作。随后，构建所有 <code>(K choose 2)</code> 个动作对，并通过计算每个动作与真实动作 <code>a_t*</code> 之间的RMSE来分配偏好标签（RMSE更小的动作为“获胜”动作），形成合成偏好数据集 <code>D_comp</code>。</li>
<li><strong>奖励建模（动作验证器训练）</strong>：动作验证器 <code>R_φ</code> 是一个基于VLM的奖励模型，输入为状态、指令和候选动作，输出标量奖励。其训练损失基于Bradley-Terry模型，但进行了改进以考虑偏好程度。损失函数定义为：<br><code>L(φ; D_comp) = -E_{(a_t^W, a_t^L, a_t*, s_t, I) ~ D_comp} [ log σ( R_φ(a_t^W, s_t, I) - R_φ(a_t^L, s_t, I) - α || Δ_t* - Δ_t_hat ||^2_2 ) ]</code><br>其中，<code>Δ_t*</code> 是真实偏好程度（两动作与真实动作RMSE的绝对差值），<code>Δ_t_hat</code> 是模型预测的偏好程度（两动作奖励值的绝对差值）。增加的 <code>L2</code> 惩罚项鼓励模型不仅预测正确的偏好顺序，还能准确反映偏好程度的大小。</li>
<li><strong>测试时动作生成与选择</strong>：<ul>
<li><strong>高斯扰动与多数投票</strong>：在部署时，首先从基础VLA策略中采样一小批（<code>N_hat</code> 个，如4个）初始动作。然后，对这些动作的平移和旋转分量 <code>(Δx, Δy, Δz, Δu, Δv, Δw)</code> 拟合一个高斯分布 <code>N(μ, σ)</code>，对于夹持器状态 <code>g</code> 则采用多数投票确定。这个拟合出的分布即为高效的动作提议分布，可以从中以可忽略的开销采样大量（<code>K_hat</code> 个）候选动作。</li>
<li><strong>验证器选择</strong>：最后，使用训练好的VLM动作验证器 <code>R_φ</code> 评估所有 <code>K_hat</code> 个候选动作，选择奖励值最高的动作作为最终执行的动作。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：</p>
<ol>
<li><strong>揭示了VLA的推理时缩放定律</strong>：首次系统性地证明，在假设存在完美验证器的前提下，VLA的动作误差与生成的样本数量之间存在指数幂律关系，为测试时缩放提供了理论依据。</li>
<li><strong>提出了高效的“生成-验证”框架</strong>：创新性地将高斯扰动拟合与多数投票结合，用极低的计算成本（仅需从策略采样4个动作）构建出能生成大量高质量候选动作的分布。</li>
<li><strong>设计了可扩展的合成验证器训练方案</strong>：无需人工标注，利用现有模仿学习数据集自动生成大规模、高质量的合成动作偏好数据，用于训练强大的基于VLM的动作验证器。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：主要在SIMPLER（分布内）、LIBERO-Long（分布内长视野）、Bridge V2（用于缩放定律分析）以及真实世界（分布外）任务上进行评估。</li>
<li><strong>实验平台</strong>：模拟实验在Isaac Gym中进行，真实世界实验使用Franka Emika Panda机械臂。</li>
<li><strong>Baseline方法</strong>：对比了多个先进的通用VLA模型，包括OpenVLA（贪婪解码）、Octo以及这些模型的微调版本。也对比了不同的采样策略（随机采样、策略采样）。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>推理时缩放定律验证</strong>：如图1所示，在不同采样方法（随机、策略、高斯扰动）下，随着生成样本数量的增加，动作误差持续下降。高斯扰动在达到相近性能的同时，计算效率最高。该规律在CogACT、Octo、OpenVLA、SpatialVLA等多个VLA模型上均成立。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17811v2/extracted/6600809/images/inference_scaling.png" alt="推理时缩放定律"></p>
<blockquote>
<p><strong>图1</strong>：左图展示了不同采样方法下动作误差随样本数增加的下降趋势，高斯扰动与策略采样性能接近且高效。右图展示了高斯扰动采样在不同VLA模型（CogACT, Octo, OpenVLA, SpatialVLA）上的幂律缩放现象。</p>
</blockquote>
<ol start="2">
<li><strong>主要性能提升</strong>：<ul>
<li><strong>SIMPLER（分布内）</strong>：RoboMonkey将OpenVLA的成功率从 <strong>66.0%</strong> 提升到 **75.0%**（绝对提升 **9.0%**）。</li>
<li><strong>真实世界（分布外）</strong>：在6个未见过的桌面操作任务上，RoboMonkey将OpenVLA的成功率从 <strong>40.0%</strong> 大幅提升至 **65.0%**（绝对提升 **25.0%**）。</li>
<li><strong>LIBERO-Long（长视野任务）</strong>：当适应新机器人设置时，同时微调VLA和动作验证器比仅微调VLA带来额外 <strong>7.0%</strong> 的性能提升。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17811v2/extracted/6600809/images/results.png" alt="主要结果"></p>
<blockquote>
<p><strong>图3</strong>：在SIMPLER和真实世界任务上的性能对比。RoboMonkey显著超越了基线方法OpenVLA（贪婪解码），在分布内和分布外任务上均取得大幅提升。</p>
</blockquote>
<ol start="3">
<li><strong>合成数据缩放效应</strong>：如图6所示，随着用于训练动作验证器的合成偏好数据集规模从1K增大到1M，验证器的准确率和下游任务成功率均持续提升，证明了合成数据生成管道的可扩展性。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17811v2/extracted/6600809/images/synthetic.png" alt="合成数据缩放"></p>
<blockquote>
<p><strong>图6</strong>：合成偏好数据集规模对动作验证器准确率（左）和下游任务成功率（右）的影响。扩大数据集规模能持续提升验证器性能和最终任务表现。</p>
</blockquote>
<ol start="4">
<li><strong>消融实验</strong>：论文验证了框架各组件的重要性。如图10所示，移除高斯扰动（仅使用策略采样）或移除动作验证器（仅使用高斯扰动采样）都会导致性能下降，表明两者结合是最优方案。奖励建模损失中的偏好程度惩罚项 <code>α</code> 也对性能有积极贡献。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17811v2/extracted/6600809/images/rmse_ablation.png" alt="消融实验"></p>
<blockquote>
<p><strong>图10</strong>：消融实验结果。完整RoboMonkey方法（绿色）性能最佳，移除高斯扰动（蓝色）或移除动作验证器（红色）均会导致性能下降。</p>
</blockquote>
<ol start="5">
<li><strong>延迟分析</strong>：如图5所示，尽管RoboMonkey增加了测试时计算，但其整体延迟仅比单次推理的OpenVLA基线增加约 **10-20%**，这主要归功于高效的高斯扰动采样和并行的验证器评估。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>首次系统性地探索并揭示了视觉-语言-动作模型在测试时的计算缩放定律，表明通过重复采样和验证可以显著提升模型性能。</li>
<li>提出了RoboMonkey框架，包含一个可扩展的合成动作偏好数据生成管道，用于训练基于VLM的动作验证器，以及一套高效的测试时“生成-验证”流程（高斯扰动+多数投票）。</li>
<li>通过大量模拟和真实实验证明，该框架能大幅提升现有VLA模型的鲁棒性和泛化能力，特别是在分布外任务上效果显著。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>框架增加了部署时的计算开销（尽管通过设计已大幅降低），在计算资源极其受限的边缘设备上应用可能面临挑战。</li>
<li>当前方法主要针对静态操作任务进行评估，在动态环境或需要快速连续决策的场景中的有效性有待进一步验证。</li>
<li>合成偏好数据的质量依赖于基础参考策略和真实动作，可能存在模拟到现实的差距。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>范式转变</strong>：为机器人基础模型的研究提供了一个新思路，即从单纯追求更好的生成模型，转向构建“生成器+验证器”的协同系统。</li>
<li><strong>可扩展的测试时优化</strong>：证明了测试时计算缩放是提升模型性能的有效途径，未来可以探索更高效的采样策略、更强大的验证器架构（如利用世界模型进行前瞻验证）以及动态计算资源分配策略。</li>
<li><strong>数据生成与模拟</strong>：提出的自动合成偏好数据方法可以扩展到更多样化的任务和场景，降低对昂贵人工标注数据的依赖，加速机器人学习系统的迭代。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对视觉-语言-动作模型在非结构化环境中鲁棒性不足的问题，提出测试时扩展框架RoboMonkey。核心方法是：在部署时从VLA采样多组动作，通过高斯扰动和多数投票构建动作提议分布，并利用基于视觉语言模型的验证器选择最优动作。实验表明，该框架显著提升了VLA性能，在分布外任务上带来25%的绝对性能提升，在分布内任务上提升9%；同时微调VLA与验证器比仅微调VLA性能额外提高7%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.17811" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>