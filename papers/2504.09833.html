<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion via Model-Assumption-based Regularization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion via Model-Assumption-based Regularization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.09833" target="_blank" rel="noreferrer">2504.09833</a></span>
        <span>作者: Jung, Hyunyoung, Gu, Zhaoyuan, Zhao, Ye, Park, Hae-Won, Ha, Sehoon</span>
        <span>日期: 2025/04/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人步态控制是一个具有挑战性的任务，源于其内在的复杂性、高维动力学特性以及需要适应多样且不可预测的环境。当前主流方法主要分为两类：模型方法与学习方法。模型方法（如基于简化模型线性倒立摆LIPM的模型预测控制MPC）能产生周期性、对称的步态，但依赖于精确的模型假设，在复杂环境中的适应性较差。学习方法（如深度强化学习）通过数据驱动优化可以增强鲁棒性和敏捷性，但通常需要大量的奖励工程，训练时间长，且容易在微调时发生灾难性遗忘，即过度拟合于新任务而丢失预训练阶段学到的稳定步态风格，这对于稳定性较差的人形机器人尤为致命。</p>
<p>本文针对在模仿模型控制器后进行强化学习微调时出现的灾难性遗忘问题，提出了一个新的视角：在微调阶段引入基于模型假设的正则化，以保留专家控制器在可靠状态下的行为，同时在模型假设被违反的状态下允许策略进行超越模型的改进。本文的核心思路是：通过预训练模仿一个模型控制器获得稳定的初始策略，然后在强化学习微调阶段，根据当前状态违反模型假设的程度动态调整对专家动作的模仿正则化强度，从而实现“保护性”微调。</p>
<h2 id="方法详解">方法详解</h2>
<p>PPF框架的整体流程包含三个关键阶段：1）通过行为克隆模仿模型控制器进行预训练；2）使用强化学习对预训练策略进行微调；3）在微调阶段应用基于模型假设的正则化来防止灾难性遗忘。</p>
<p><img src="https://arxiv.org/html/2504.09833v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：基于模型假设的正则化（MAR）概览。我们的框架根据模型控制器（MBC）的假设违反情况自动调整监督损失正则化的权重。</p>
</blockquote>
<p><strong>核心模块1：预训练（模仿学习）</strong>。此阶段使用数据集聚合（DAgger）算法，通过最小化策略输出动作与专家模型控制器动作之间的均方误差损失（式4），将专家知识蒸馏到一个神经网络策略中。专家控制器采用基于角动量线性倒立摆（ALIP）模型的足部放置控制器和基于被动性的全身逆动力学控制器。预训练得到一个行为与模型控制器相似的可学习策略。</p>
<p><strong>核心模块2：微调（强化学习）</strong>。使用近端策略优化（PPO）算法，在包含速度课程和地形课程（上坡、下坡、平坦、不平等）的仿真环境中对预训练策略进行优化。奖励函数设计简单，主要包含线性/角速度跟踪、扭矩惩罚和躯干运动惩罚（表I）。此阶段旨在提升策略在复杂任务（如更高速度、更崎岖地形）上的性能。</p>
<p><strong>核心模块3：基于模型假设的正则化（MAR）</strong>。这是本文的核心创新点。为解决微调时的灾难性遗忘问题，在PPO损失基础上增加了一个正则化项，但该正则项的权重不是固定的，而是取决于当前状态对底层模型假设的违反程度。具体损失函数如式6所示。权重函数 w(s) 设计为式7，其中以躯干垂直速度 z˙ 作为衡量ALIP模型假设（恒定高度、零垂直速度）违反程度的指标。当 z˙ 接近0（假设成立）时，权重接近 w0，策略被强烈正则化以模仿专家动作；当 z˙ 较大（假设被违反，如高速运动或崎岖地形）时，权重指数衰减至接近0，允许策略自由探索以超越模型限制。此外，为实现平滑控制并抑制电机振动，引入了利普希茨连续性惩罚（式9）来约束策略网络对状态变化的敏感性。</p>
<p>与现有方法（如IFM）相比，PPF的创新点在于引入了<strong>自适应</strong>的正则化机制。IFM在微调阶段完全放开约束，容易遗忘；而简单的全正则化（FullReg）则始终强制模仿，限制了策略的改进潜力。MAR通过模型假设这一桥梁，智能地判断何时应该信赖专家知识（予以保留），何时应该允许策略突破限制（予以放松），从而在保持运动风格和提升性能之间取得了更好的平衡。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台为Agility Robotics的Digit人形机器人。仿真训练在MuJoCo中进行，并在MuJoCo和公司提供的更高保真度仿真器（AR-Sim）中进行测试，最后进行零样本硬件部署。</p>
<p>对比的基线方法包括：原始模型控制器（MBC）、模仿后微调但不加正则化的IFM、使用固定权重全正则化的FullReg、以及无任何预训练或正则化的纯强化学习方法（PureRL）。</p>
<p><strong>关键实验结果1：MuJoCo训练环境鲁棒性测试</strong>。机器人在指令下穿越难度递增的斜坡和不平等地形序列（图4）。衡量指标为成功到达的地形等级。</p>
<p><img src="https://arxiv.org/html/2504.09833v2/figures/mujoco_rough_terrains_review.png" alt="测试地形"></p>
<blockquote>
<p><strong>图4</strong>：MuJoCo中用于鲁棒性测试的斜坡和不平等地形。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.09833v2/figures/mujoco_terrain_test_cleared_level_plot_review.png" alt="地形通过情况"></p>
<blockquote>
<p><strong>图5</strong>：各控制器在MuJoCo鲁棒性测试中达到的地形等级。PPF成功在时限内穿越了所有地形。</p>
</blockquote>
<p>结果显示，PPF和PureRL表现最佳，均能到达最终地形。但IFM经常在不平等地形上因意外的足部碰撞而失败，FullReg则因跟踪性能下降而超时。这初步证明了MAR在保持鲁棒性方面的优势。</p>
<p><strong>关键实验结果2：AR-Sim仿真间迁移鲁棒性测试</strong>。在AR-Sim中测试机器人爬不同坡度上坡的成功率和速度跟踪误差。</p>
<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">10° Uphill Succ. (%) / Track Err. (%)</th>
<th align="left">14° Uphill Succ. (%) / Track Err. (%)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">MBC</td>
<td align="left">0 / 34.5</td>
<td align="left">0 / 34.4</td>
</tr>
<tr>
<td align="left">PPF</td>
<td align="left"><strong>100 / 8.4</strong></td>
<td align="left"><strong>100 / 18.9</strong></td>
</tr>
<tr>
<td align="left">IFM</td>
<td align="left">100 / 15.5</td>
<td align="left">100 / 30.7</td>
</tr>
<tr>
<td align="left">PureRL</td>
<td align="left">85 / 10.9</td>
<td align="left">0 / 54.1</td>
</tr>
</tbody></table>
<p>结果显示，PPF、IFM和FullReg均实现了100%成功率，但PPF的跟踪误差显著低于IFM和FullReg。PureRL在陡坡上成功率骤降为0，MBC完全失败。这表明结合MBC的周期性步态对于仿真间迁移至关重要，而PPF的MAR机制在保证成功的同时实现了更优的跟踪精度。</p>
<p><strong>关键实验结果3：AR-Sim随机速度跟踪测试</strong>。在平坦地形上执行训练时未见过的随机速度指令，评估跟踪奖励的回报。</p>
<p><img src="https://arxiv.org/html/2504.09833v2/figures/ar_random_vel_test_review.png" alt="随机速度跟踪回报"></p>
<blockquote>
<p><strong>图6</strong>：AR-Sim随机速度跟踪测试中跟踪奖励的平均无折扣回报。使用正则化的策略（PPF和FullReg）表现出优于无MBC正则化方法的性能。</p>
</blockquote>
<p>PPF获得了最高的回报，略优于FullReg，显著高于IFM和PureRL。IFM在遇到突然的侧向或转向指令时容易绊倒自己。</p>
<p><strong>消融实验与MAR机制验证</strong>。论文通过分析上坡测试中MAR权重与躯干垂直速度的关系，验证了MAR的动态调整机制。</p>
<p><img src="https://arxiv.org/html/2504.09833v2/x2.png" alt="MAR权重散点图"></p>
<blockquote>
<p><strong>图7</strong>：上坡测试中，MAR权重与躯干垂直速度的散点图。权重随垂直速度（模型假设违反）增大而减小，表明MAR如预期般工作。</p>
</blockquote>
<p><strong>硬件实验结果</strong>。PPF策略成功零样本部署到Digit机器人，实现了1.5 m/s的前向行走速度，并能在覆盖罂粟籽或橄榄油的白板、户外斜坡、不平等地面和沙地等多种具有挑战性的室内外地形上鲁棒行走。</p>
<p><img src="https://arxiv.org/html/2504.09833v2/figures/hw_testbed_review.png" alt="硬件测试概览"></p>
<blockquote>
<p><strong>图1</strong>：我们的PPF框架实现了1.5 m/s的前向速度，并成功穿越了覆盖罂粟籽或橄榄油的白板，以及多样化的户外地形，包括山坡、不平等表面和沙地。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）提出了PPF框架，一种结合模仿学习、强化学习微调和自适应正则化的三阶段人形步态训练范式；2）设计了基于模型假设的正则化（MAR）机制，能根据状态对模型假设的违反程度动态调整对专家知识的依赖，有效缓解灾难性遗忘，在保持运动风格稳定性和提升策略性能之间取得平衡；3）在仿真和真实Digit机器人上进行了全面验证，展示了其在高速、多变的室内外地形上的卓越鲁棒性和泛化能力。</p>
<p>论文自身提到的局限性在于，MAR权重函数的设计（基于躯干垂直速度）是针对其使用的ALIP模型假设的，对于其他模型可能需要设计不同的违反度量指标。</p>
<p>本文对后续研究的启示包括：1）为混合模型与学习的方法提供了防止灾难性遗忘的新思路，即通过模型本身的可信度来指导正则化强度；2）表明在微调阶段进行“选择性”的知识保留比完全保留或完全放弃更有效；3）成功地将从简化模型控制器学到的周期性步态风格通过自适应正则化保留下来，并推广到更复杂的领域，这为将其他领域先验知识融入端到端学习提供了参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人在复杂非结构化环境中实现鲁棒运动的核心难题，提出PPF框架。该方法结合三个关键步骤：1）通过模仿模型控制器进行预训练；2）利用强化学习进行微调；3）引入模型假设正则化（MAR），在模型假设成立的状态下对齐策略以防止灾难性遗忘。在Digit机器人上的实验表明，该方法实现了1.5 m/s的前进速度，并能在湿滑、斜坡、不平整及沙地等多种复杂地形上稳定运动。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.09833" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>