<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.19480" target="_blank" rel="noreferrer">2509.19480</a></span>
        <span>作者: Hirose, Noriaki, Glossop, Catherine, Shah, Dhruv, Levine, Sergey</span>
        <span>日期: 2025/09/23</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前机器人导航领域的主流方法是训练针对单一目标模态的专用策略，例如仅使用语言指令、2D目标位姿或自我中心目标图像。这种单模态方法限制了策略的适应性，因为现实世界场景中不同形式的目标描述（如结合GPS坐标和视觉地标）是自然且互补的。本文针对现有策略无法灵活理解和组合多种目标模态这一具体痛点，提出了一个全模态目标调节的新视角。本文的核心思路是：通过一个多模态训练框架，利用大规模、多平台收集的导航数据，训练一个能够同时处理并组合语言、位姿和图像等多种目标信号的通用视觉-语言-动作模型，从而获得更强大、更灵活的导航策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>OmniVLA的整体框架是一个端到端的导航策略，其输入包括机器人当前的视觉观测（<code>I_c</code>）以及以多种形式指定的目标信息，输出是未来N步的动作序列（<code>{â_i}</code>）。目标信息支持三种主要模态及其任意组合：2D目标位姿（<code>p_g</code>）、自我中心目标图像（<code>I_g</code>）和自然语言指令（<code>l_g</code>）。训练时通过一种随机化的模态融合策略来选择用于调节的模态（<code>t_m</code>）。</p>
<p><img src="https://..." alt="方法整体框架"></p>
<blockquote>
<p><strong>图1</strong>：方法整体框架。左侧展示了模型支持多样化的目标模态输入，包括语言提示、目标位姿、目标图像及其组合。右侧展示了模型基于当前图像，输出多步动作序列。该模型利用了超过9500小时、跨10个不同平台收集的数据进行训练。</p>
</blockquote>
<p>核心模块构建于一个高容量的视觉-语言-动作模型主干之上，具体基于OpenVLA（一个70亿参数的VLA模型）。为了支持全模态目标调节，对其架构进行了修改，主要包括：</p>
<ol>
<li><strong>视觉编码器</strong>：处理机器人当前的视觉观察。</li>
<li><strong>多模态目标编码器</strong>：将三种不同的目标模态（视觉、位置、语言）分别投影到一个共享的令牌空间中。语言提示使用预训练LLM（如Llama 2 7B）的tokenizer处理；目标图像使用视觉编码器（如SigLIP和DINOv2）处理后再经投影层；2D目标位姿则直接通过一个投影层嵌入。</li>
<li><strong>LLM主干与注意力掩码</strong>：处理来自当前观察和目标模态的令牌序列。关键创新是引入了“模态随机丢弃”机制：在训练时，为每个样本独立地从可用目标模态中采样以构建调节输入<code>t_m</code>，并为未使用或不可用的模态生成注意力掩码，使其不被关注。这确保了模型能从所有数据集中学习跨模态的目标表示。</li>
<li><strong>动作头</strong>：在LLM输出后添加一个线性层，用于预测未来N步的动作序列。</li>
</ol>
<p>训练目标是模仿N步参考动作，使用均方误差损失函数：<code>J_il = 1/N * Σ (a_ref_i - â_i)^2</code>。对于包含特定任务（如物体抵达）的数据集（如LeLaN），会添加额外的目标函数以鼓励相关行为。为了处理大规模模型训练，采用了LoRA技术，将可学习参数限制在约5%，以在训练速度和稳定性之间取得平衡。</p>
<p>与现有单模态方法相比，OmniVLA的创新点具体体现在：1) <strong>多模态训练策略</strong>：首次在导航领域统一训练可处理语言、位姿和图像目标（及其组合）的端到端VLA模型。2) <strong>模态随机丢弃</strong>：通过随机化目标模态输入和相应的注意力掩码，有效解决了多模态数据中模态不平衡和稀缺的问题，并鼓励模型学习更丰富的几何、语义和视觉表征。3) <strong>大规模异构数据利用</strong>：整合了多达9500小时、跨10个不同平台的公开数据集，构成了已知最大的端到端导航策略预训练数据集。</p>
<p><img src="https://..." alt="网络架构细节"></p>
<blockquote>
<p><strong>图2</strong>：多模态视觉导航网络架构。架构基于现有的大型VLA检查点，增加了视觉主干和投影层，以支持对自我中心目标图像和2D目标位姿的调节。在训练期间，会随机掩码这些模态以及语言提示的令牌（图中以灰色和“0”表示）。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了四个主要的benchmark数据集混合物：GNM mixture、LeLaN mixture、Frodobots-2K和BDD-V，总计约9500小时的真实世界导航数据，覆盖室内、室外、人行道、道路等多种环境。评估平台包括FrodoBots ERZ地面机器人、VizBot轮式机器人和Unitree Go1四足机器人。</p>
<p>对比的baseline方法涵盖了单模态的专家模型和通用模型，包括：语言条件导航的CoW、LeLaN、CounterfactualVLA；2D位姿条件导航的MBRA-pose、NoMaD；图像条件导航的ViNT、MBRA-image、NoMaD。此外，还将全模态训练策略应用于其他VLA主干（MiniVLA、SmolVLA）进行了对比。</p>
<p>关键实验结果如下表所示，OmniVLA在几乎所有单模态任务上都超越了对应的专家基线。</p>
<p><img src="https://..." alt="单模态条件导航定量结果"></p>
<blockquote>
<p><strong>表II</strong>：单模态条件导航的定量分析。SR和Prog.分别表示成功率和向目标的部分进度。“SR_S”平均了无障碍物的简单实验。“SR_C”平均了环境中有障碍物的复杂实验。“Behavior”表示遵循OOD语言提示的成功率。OmniVLA在语言、位姿和图像条件任务上均表现最佳。</p>
</blockquote>
<p><img src="https://..." alt="多模态训练消融实验"></p>
<blockquote>
<p><strong>表III</strong>：OmniVLA-edge的多模态训练消融研究。评估了在单一模态和多个模态上训练的OmniVLA-edge在单模态任务上的性能。结果表明，联合多模态训练（最后一行）在语言和卫星图像条件任务上带来了显著优势，并因包含了高度多样的跨具身数据（BDD-V）而提升了泛化能力。</p>
</blockquote>
<p><strong>文字总结</strong>：在语言条件导航上，OmniVLA取得了73%的成功率（简单场景）和65%的成功率（复杂场景），显著优于LeLaN（43%， 15%）和CounterfactualVLA（33%， 45%）。在遵循训练数据外（OOD）的行为指令方面，OmniVLA的成功率达到100%，而LeLaN仅为64%，显示了其强大的语言泛化能力。在2D位姿条件导航上，OmniVLA以95%的成功率和98%的进度超越了最强的专家基线MBRA-pose（86%， 92%）。在自我中心图像条件导航上，OmniVLA与最强的专家基线MBRA-image均达到了100%的成功率。</p>
<p>消融实验（表III）表明，联合多模态训练是性能提升的关键。例如，仅用语言数据训练的模型在语言任务上成功率为43%，而全模态训练的模型成功率达到60%。包含跨具身数据（BDD-V）使模型在卫星图像新模态上的性能从19%提升至57%。</p>
<p><img src="https://..." alt="语言条件导航可视化"></p>
<blockquote>
<p><strong>图4</strong>：语言条件导航轨迹可视化。OmniVLA能够在各种室内外环境中遵循OOD语言指令（如图中案例B的复杂指令），而基线方法LeLaN和CoW则导航向了错误物体。</p>
</blockquote>
<p><img src="https://..." alt="多模态组合条件导航可视化"></p>
<blockquote>
<p><strong>图5</strong>：目标位姿与语言组合条件导航轨迹可视化。在同时给定OOD语言指令（“如何做”）和远距离目标位姿（“去哪里”）的情况下，OmniVLA能够执行复杂的长时程导航任务，在抵达目标的同时遵循行为指令。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了第一个用于导航的<strong>全模态端到端VLA模型</strong>（OmniVLA），能够统一处理并组合语言、位姿和图像目标指令。2) 设计了一种<strong>随机模态丢弃的训练框架</strong>，有效利用大规模、多模态、跨平台的异构数据集（约9500小时），解决了模态不平衡问题。3) 实证表明，通过这种多模态训练获得的策略，在<strong>单模态任务上超越了专家基线</strong>，并展现出强大的<strong>泛化能力</strong>（如遵循OOD语言指令）和<strong>快速适应新模态</strong>的潜力。</p>
<p>论文自身提到的局限性包括：大型VLA模型（OmniVLA）的推理对计算资源要求较高，尽管提供了更高效的“边缘”版本（OmniVLA-edge）；训练中使用的部分合成动作和语言标签可能存在噪声。</p>
<p>本工作对后续研究的启示包括：1) <strong>多模态训练框架的推广性</strong>：证明了在导航领域借鉴操作领域经验，通过模态掩码整合异构数据的有效性，这为构建更通用的机器人基础模型提供了可扩展的路径。2) <strong>作为基础模型的潜力</strong>：OmniVLA展现出的强泛化性和对新模态的快速适应能力，使其可作为预训练检查点，方便后续研究针对特定模态、环境或平台进行微调。3) <strong>规模与架构的影响</strong>：实验结果表明，模型容量、预训练知识（来自互联网规模的VLM）以及特定于任务的架构设计都对最终性能有显著影响，这为未来导航模型的设计提供了重要参考。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出OmniVLA模型，旨在解决机器人导航策略通常局限于单一目标模态（如仅语言或仅图像），而无法灵活适应现实世界中多模态互补目标指示的问题。方法基于大规模视觉-语言-动作骨干网络，通过随机模态融合策略，统一训练语言指令、目标位姿、目标图像及其组合的多模态条件。实验表明，该模型在未见环境中泛化能力强，对稀缺模态鲁棒，且能遵循新语言指令，性能超越各模态的专用基线模型。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.19480" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>