<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.08548" target="_blank" rel="noreferrer">2505.08548</a></span>
        <span>作者: Yuan, Yifu, Cui, Haiqin, Chen, Yibin, Dong, Zibin, Ni, Fei, Kou, Longxin, Liu, Jinyi, Li, Pengyi, Zheng, Yan, Hao, Jianye</span>
        <span>日期: 2025/05/13</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人操作研究的主流方法是利用预训练的视觉语言模型（VLM），通过大规模具身数据将其转化为视觉-语言-动作（VLA）模型，旨在继承VLM在互联网数据上习得的泛化能力，以应对新物体和新任务。然而，实证表明，这种直接迁移的方法难以实现强大的零样本性能。本文认为，VLA系统泛化能力受限的根本原因在于<strong>数据的稀缺性和异构性</strong>：机器人数据规模远小于语言和视觉数据，且不同机器人平台（具身）的数据存在显著差异，导致从视觉和语言到多样化动作输出的端到端监督学习难以实现泛化。</p>
<p>本文针对上述痛点，提出了一个新视角：<strong>通过空间关系推理生成与具体机器人无关的中间层结构化表示（视觉辅助）</strong>，来桥接高级推理和低级决策。核心思路是：利用VLM的视觉理解能力，通过逐步的空间关系链式思维（Spatial Relationship-Focused CoT）推理，生成统一的空间可操作框/点和视觉轨迹等视觉辅助，为机器人操作提供细粒度、与具身无关的指导，从而克服数据稀缺和异构带来的泛化挑战。</p>
<h2 id="方法详解">方法详解</h2>
<p>FSD模型旨在通过空间推理生成视觉中间表示，其整体框架包含三个核心组件：空间关系聚焦的视觉链式思维（SrCoT）、分层（弱到强）数据构建管道、以及自一致对齐机制。</p>
<p><img src="https://arxiv.org/html/2505.08548v2/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：FSD方法整体概览。FSD通过空间关系聚焦的链式思维（Spatial Relationship-Focused CoT）解锁视觉辅助推理与生成，展示了卓越的泛化能力，实现了零样本机器人操作并在多个基准测试中取得优异性能。</p>
</blockquote>
<p><strong>视觉辅助的定义</strong>：FSD使用三种复杂度递增的视觉辅助，均在归一化的图像坐标（0-1000）中定义。</p>
<ol>
<li><strong>空间可操作框</strong>：定义物体放置的目标区域。</li>
<li><strong>空间可操作点</strong>：提供更精确、灵活的放置点。</li>
<li><strong>物体中心视觉轨迹</strong>：描述操作轨迹的有序坐标序列。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.08548v2/x2.png" alt="视觉辅助类型"></p>
<blockquote>
<p><strong>图2</strong>：视觉辅助类型示意图。从左至右分别为空间可操作框、空间可操作点、物体中心视觉轨迹。</p>
</blockquote>
<p><strong>核心模块1: 空间关系聚焦的视觉链式思维（SrCoT）</strong><br>直接监督微调VLM来生成坐标容易过拟合且泛化差。SrCoT受人类认知启发，将视觉辅助的生成过程构建为一个基于空间关系图的多步推理链。该过程分为两个阶段：</p>
<ol>
<li><strong>描述</strong>：生成以物体为中心的区域描述，建立任务相关的空间关系图（节点为带坐标的物体，边为相对空间关系）。</li>
<li><strong>推理</strong>：以空间关系图为锚点，通过物体参照和自由空间推理确定起点和终点坐标，然后迭代推导中间点，各步骤间有明确的逻辑连接。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.08548v2/x3.png" alt="SrCoT推理过程"></p>
<blockquote>
<p><strong>图3</strong>：（上）人类执行“将西兰花放入锅中”任务时的推理过程。（下）FSD使用空间关系图作为锚点，推导出视觉轨迹生成的视觉链式思维推理过程示例。</p>
</blockquote>
<p>为增强稳定性并减少幻觉，SrCoT要求模型以特定格式（如使用<code>&lt;ref&gt;</code>标记物体，<code>&lt;point&gt;</code>/<code>&lt;box&gt;</code>标记坐标）生成坐标，确保每个物体与其坐标严格绑定，实现显式的视觉-空间坐标对齐。</p>
<p><strong>核心模块2: 弱到强能力数据集构建管道</strong><br>SrCoT要求VLM具备精确的参照 grounding、空间理解和复杂指令遵循等能力。为此，FSD设计了一个包含五个层次能力的渐进式数据构建管道：</p>
<ol>
<li><strong>区域 grounding</strong>：聚焦场景关键物体。</li>
<li><strong>空间关系理解</strong>：建立空间推理的锚点知识。</li>
<li><strong>空间推理</strong>：基于锚点进行物体位置和关系的多跳分析。</li>
<li><strong>空间可操作性生成</strong>。</li>
<li><strong>视觉轨迹生成</strong>。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.08548v2/x4.png" alt="数据构建管道"></p>
<blockquote>
<p><strong>图4</strong>：FSD数据集自动构建流程。从大规模具身数据集中筛选数据，生成真实空间关系图，最终为超过10种具身形态收集了30万条涵盖5级能力的数据。</p>
</blockquote>
<p>具体而言，利用BridgeDataV2、RT-X、Droid等机器人数据集，通过GPT-4o提名物体、GroundedSAM获取边界框（Level 1），结合深度估计和相机参数重建3D语义场景图以构建空间关系（Level 2），基于空间关系图生成复杂QA（Level 3）。对于视觉辅助生成，从成功的人类演示中推断结果：从最终帧提取被操作物体的位置来生成空间可操作性标签（Level 4）；通过自监督关键点提取和Cotracker跟踪轨迹，再投影到初始帧来生成视觉轨迹标签（Level 5）。</p>
<p><strong>核心模块3: 自一致对齐机制</strong><br>为使模型理解坐标的物理意义，FSD提出了自一致对齐机制，将生成任务逆向构建为理解任务。例如，正向任务是<code>(图像, 指令) → 视觉轨迹</code>，逆向任务则是<code>(图像, 视觉轨迹) → 可能的指令</code>。这种双向方法帮助模型理解空间坐标的含义，并将坐标空间与图像-文本模态对齐。</p>
<p><strong>训练与执行</strong>：模型架构基于LLaVA-1.5，使用CLIP-ViT-L-336px图像编码器和Vicuna-13B LLM。训练分为两个阶段：1) 使用Level 1-3数据与通用VQA数据混合，增强通用空间推理能力；2) 使用Level 4-5数据及自一致数据，专门训练视觉辅助生成与理解能力。执行时，根据生成的视觉辅助（框、点或轨迹），通过简单的规划方法（如采样中心点、深度反投影到3D、结合GraspNet抓取位姿、使用CuRobo运动规划器）完成动作执行。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验从“Seeing”（空间理解与推理）和“Doing”（零样本操作）两个维度评估FSD。</p>
<p><strong>评估Seeing能力</strong></p>
<ul>
<li><p><strong>基准与基线</strong>：在5个通用空间推理基准（CVBench, BLINK, CRPE, SAT, EmbSpatial-Bench，共15个子任务）上，对比了GPT-4o/GPT-4V、LLaVA-1.5、SAT-Dynamic、RoboPoint、ASMv2等模型。在2个具身空间参照基准（RoboRefIt, Where2Place）上，对比了SpatialBot、SpaceLLaVA、RoboBrain等模型。此外，在自建的更具挑战性的视觉辅助生成基准<strong>VABench</strong>（包含300个人工标注的真实和仿真图像问题）上进行了评估。</p>
</li>
<li><p><strong>关键结果</strong>：</p>
<ul>
<li><strong>通用空间推理</strong>：FSD在多个基准上表现优异，尤其在需要复杂空间理解的子任务上。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.08548v2/x6.png" alt="通用空间推理结果"></p>
<blockquote>
<p><strong>图6</strong>：在CVBench基准上的性能对比。FSD在整体及多个子类别上表现最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.08548v2/x7.png" alt="通用空间推理结果2"></p>
<blockquote>
<p><strong>图7</strong>：在BLINK基准上的性能对比。FSD在需要空间推理的任务上优势明显。</p>
</blockquote>
<ul>
<li><strong>具身空间参照</strong>：FSD在RoboRefIt和Where2Place基准上均取得了最高准确率。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.08548v2/x8.png" alt="具身参照结果"></p>
<blockquote>
<p><strong>图8</strong>：在RoboRefIt和Where2Place基准上的物体与自由区域参照准确率。FSD显著优于其他基线。</p>
</blockquote>
<ul>
<li><strong>视觉辅助生成（VABench）</strong>：FSD在生成空间可操作点和视觉轨迹方面均优于对比方法，其生成的轨迹在GPT评分中也获得最高分。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.08548v2/x9.png" alt="VABench结果"></p>
<blockquote>
<p><strong>图9</strong>：在VABench-Point（左）和VABench-VisualTrace（右）上的评估结果。FSD在点生成准确率和轨迹预测误差（MAE, RMSE）及GPT评分上均领先。</p>
</blockquote>
</li>
</ul>
<p><strong>评估Doing能力（零样本操作）</strong></p>
<ul>
<li><p><strong>基准与基线</strong>：在仿真环境SimplerEnv和真实世界xArm机器人平台上执行8项未见过的任务，对比基线包括直接生成动作的RT-2、生成视觉辅助的LLaRVA和RoboPoint。</p>
</li>
<li><p><strong>关键结果</strong>：</p>
<ul>
<li><strong>仿真环境</strong>：FSD取得了40.6%的成功率，显著优于最佳基线RoboPoint（25.6%）。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.08548v2/x10.png" alt="仿真环境结果"></p>
<blockquote>
<p><strong>图10</strong>：在SimplerEnv仿真环境中的零样本操作成功率。FSD比最强基线RoboPoint高出约15个百分点。</p>
</blockquote>
<ul>
<li><strong>真实机器人</strong>：在8个真实任务中，FSD平均成功率达到72%，相比最强基线（RoboPoint，55.5%）提升了近30%。</li>
</ul>
<p><img src="https://arxiv.org/html/2505.08548v2/x11.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图11</strong>：在真实世界8个任务上的零样本操作成功率。FSD以72%的成功率大幅领先。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.08548v2/x5.png" alt="真实机器人任务示例"></p>
<blockquote>
<p><strong>图5</strong>：FSD为零样本新任务生成的视觉辅助示例，包括可操作框、点和视觉轨迹。</p>
</blockquote>
</li>
</ul>
<p><strong>消融实验</strong><br>消融实验验证了各核心组件的贡献。</p>
<ol>
<li><strong>移除自一致对齐机制</strong>：导致视觉轨迹生成误差（MAE/RMSE）显著上升，在VABench上的GPT评分下降。</li>
<li><strong>移除弱到强数据管道（直接使用Level 4-5数据）</strong>：严重损害了模型的空间推理和参照能力，在多个基准上的性能大幅下降。</li>
<li><strong>移除SrCoT推理（直接生成坐标）</strong>：虽然保留了部分生成能力，但在需要复杂空间推理的任务（如Where2Place）上性能下降，且操作成功率降低。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.08548v2/x12.png" alt="消融实验"></p>
<blockquote>
<p><strong>图12</strong>：消融实验结果。自一致对齐、弱到强数据管道和SrCoT推理对FSD的全面性能都至关重要。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出新范式</strong>：提出了通过视觉辅助桥接VLM推理与具身决策的新范式，利用与机器人无关的中间表示解决了数据异构性和稀缺性带来的泛化难题。</li>
<li><strong>提出SrCoT方法</strong>：设计了空间关系聚焦的视觉链式思维（SrCoT），将视觉辅助生成转化为基于空间关系图的多步推理过程，有效激活并利用了模型的通用空间知识。</li>
<li><strong>构建数据集与基准</strong>：构建了弱到强的分层能力训练数据集，并提出了一个更具挑战性的视觉辅助生成基准VABench，为相关研究提供了资源。</li>
<li><strong>实现卓越性能</strong>：在8个空间理解和参照基准上取得领先，并在仿真和真实机器人的零样本操作中显著超越基线方法（SimplerEnv: 40.6%成功率；真实任务: 72%成功率，提升30%）。</li>
</ol>
<p><strong>局限性</strong>：论文提到，由于高质量3D数据的缺乏，当前视觉辅助基于2D图像坐标定义，依赖深度相机反投影到3D执行。这可能在某些复杂3D交互场景中存在限制。此外，推理过程可能增加计算开销。</p>
<p><strong>启示</strong>：本工作表明，通过<strong>结构化、可解释的中间表示</strong>和<strong>基于推理的生成范式</strong>，能够更有效地利用大规模预训练模型的常识知识，提升机器人系统在零样本场景下的泛化能力。这为未来研究指明了方向：进一步探索更丰富的中间表示形式、更高效的推理-决策耦合机制，以及如何将3D几何信息更自然地融入VLM的推理过程中。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中泛化能力不足、特别是对未见场景和新任务的零样本性能差的问题，提出了FSD模型。该方法通过**空间关系推理链**生成中间表示以提供细粒度操作指导，并采用**分层数据构建流程**与**自一致性机制**进行训练。实验表明，FSD在多个基准测试中表现优异，在零样本机器人操作任务中，在仿真环境SimplerEnv达到40.6%成功率，在8个真实世界任务中达到72%成功率，比最强基线性能提升30%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.08548" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>