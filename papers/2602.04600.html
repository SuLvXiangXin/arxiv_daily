<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.04600" target="_blank" rel="noreferrer">2602.04600</a></span>
        <span>作者: Li, Jialiang, Qiao, Yi, Guo, Yunhan, Chen, Changwen, Lian, Wenzhao</span>
        <span>日期: 2026/02/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作领域近年来的显著进展主要得益于模仿学习和基础模型的快速发展。然而，这些方法在结构化环境中的确定性线性任务上表现良好，却难以应对复杂非结构化环境。在这些环境中，机器人需要基于交互历史进行有意的注视和探索，而不仅仅是反应式运动。例如，要取回工具箱中被遮挡的扳手，机器人可能需要探索不同视角或操作周围物体以揭示被遮挡区域。这种场景需要一个能够通过耦合的“执行-感知-执行”循环来解决模糊性的认知框架，这正是主动感知的核心。</p>
<p>现有的一些方法试图解决感知被动性问题，例如通过VR遥操作或外骨骼控制收集高质量数据来训练主动感知模型。然而，这些方法仅将主动感知控制（如头部/眼睛运动）视为额外的动作维度，并优化即时任务完成。因此，这些模型通常局限于基于头部运动的视点调整以执行线性任务，未能利用身体运动或交互式操作（如打开抽屉）作为揭示隐藏信息的策略工具。</p>
<p>针对上述问题，本文将主动感知形式化为一个连续的“执行、感知、执行”过程。具体而言，本文首先推进了对主动感知的理论理解，将其形式化为一个由信息增益和决策分支驱动的非马尔可夫决策过程，并提供视觉主动感知范式的分类；然后引入了CoMe-VLA，一个认知和记忆感知的视觉-语言-动作框架，利用大规模人类自我中心数据，使机器人能够在多样化的主动感知场景中进行鲁棒操作。本文核心思路是：通过将人类和机器人的手眼协调行为对齐到一个统一的自我中心动作空间，并利用一个包含认知辅助头和双轨记忆系统的VLA框架，分阶段从大规模人类数据中蒸馏出探索性先验，从而赋予机器人类似人类的“执行-感知-执行”策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的CoMe-VLA框架旨在以数据驱动的方式近似非马尔可夫决策过程中的信息增益和决策分支机制。</p>
<p><img src="https://arxiv.org/html/2602.04600v1/x4.png" alt="方法框架"></p>
<blockquote>
<p><strong>图4</strong>：CoMe-VLA概述。CoMe-VLA集成了一个预训练的VLM（Qwen3-VL-2B）和一个基于Transformer的本体感觉记忆编码器，以构建时间视觉-语义和本体感觉上下文，这些上下文被输入到一个流匹配动作解码器中，以生成一个29维的动作块。VLM还输出一个认知潜在标记给认知辅助头，该头预测一个用于自主任务转换的二元标签。CoMe-VLA使用人类和机器人数据分三个阶段进行训练。</p>
</blockquote>
<p><strong>整体流程</strong>：模型以时间性（历史和当前）的自我中心视觉观察、任务描述、认知标记和时间本体感觉状态作为输入。经过处理后，预测一个包含未来K个动作的动作块 <strong>A_t</strong> ∈ ℝ^(K×29)。每个动作向量包括视点和双手末端执行器的笛卡尔位置和6D旋转表示，以及标量双手夹爪状态。</p>
<p><strong>核心模块</strong>：</p>
<ol>
<li><strong>认知辅助头</strong>：由于在高维视觉空间中显式计算信息增益是计算上难以处理的，因此引入一个轻量级的认知辅助头作为此目标的可学习代理。通过在输入序列后附加一个认知标记[COG]，模型聚合视觉和语义历史来预测一个二元认知标签 <strong>c_t</strong>，该标签标志着不确定性的感知解决（例如，目标从隐藏状态变为可操作状态的信息突变）。在推理时，该标签作为一个可靠的监视器，告知策略何时在探索和利用策略之间切换。这种转换通过子任务文本指令的切换来体现，从而在新的目标上重新调节策略。</li>
<li><strong>双轨记忆系统</strong>：主动感知本质上是非马尔可夫的，需要智能体保持对先前探索状态的感知以选择适当的策略。为了实现这一点，同时避免存储所有原始数据的计算负担，本文实现了一个双轨记忆系统。利用Qwen3-VL的多图像能力，编码一个视觉观察的时间窗口以保持空间感知。并行地，一个基于Transformer的轨道编码本体感觉状态，捕捉运动行为的时间动态。流匹配动作解码器在每一层对这些双重上下文进行交叉注意力，允许模型将其决策基于视觉场景，同时根据本体感觉反馈细化轨迹。具体地，记忆窗口包括当前帧和在过去5秒内以1秒间隔采样的五个历史帧。</li>
<li><strong>隐式数据驱动决策</strong>：CoMe-VLA不是将决策分支显式建模为离散的符号选择，而是基于历史和当前观察进行推理，以隐式捕获探索性和利用性决策路径。通过在大型人类自我中心数据上训练，模型学会解决不同的感知结果，将分支行为捕获为感知-动作耦合的连续变化，而非预定义的选择。</li>
</ol>
<p><strong>训练策略</strong>：训练流程包含三个渐进阶段：</p>
<ul>
<li><strong>阶段1：认知状态预训练</strong>：目标是从大规模人类数据中建立对任务进展的基础理解。仅更新视觉语言模型和认知辅助头的参数。使用Focal Loss进行监督。</li>
<li><strong>阶段2：认知-动作联合预训练</strong>：解冻其余模块以学习人类数据中的主动感知和操作先验。除了认知Focal Loss，还引入MSE损失来监督动作解码器对五个动作分量（视点旋转和位置、双手末端执行器旋转和位置、双手夹爪状态）的预测速度。总损失为加权和。</li>
<li><strong>阶段3：机器人数据微调</strong>：最后阶段使模型适应机器人的真实动力学和执行环境，同时保留在前几个阶段建立的认知基础、动作和感知能力。切换到机器人数据，并使用与阶段2相同的损失结构继续进行全模型优化。</li>
</ul>
<p><strong>创新点</strong>：与现有方法相比，本文的创新在于：1) 将主动感知形式化为非马尔可夫决策过程，提供了理论框架和分类；2) 提出了从大规模人类自我中心数据中蒸馏探索先验的方法；3) 设计了包含认知辅助头和双轨记忆系统的CoMe-VLA框架，实现了长时程环境感知和自主子任务转换。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：使用CaptainCook4D和Ego-Exo4D作为主要人类数据源。在Corenetic Monte02轮式人形机器人平台上进行实验。</li>
<li><strong>任务</strong>：根据主动感知范式（信息发现-视点发现、信息发现-操作发现、信息丰富），设计了五个长时程任务：Croissant Search, Can Disposal, Bottle Retrieval, Cylinder Hunt, Ring Peg（如图5所示）。所有任务初始条件不确定，目标位置对模型未知且可变。</li>
<li><strong>基线方法</strong>：对比了两类基线：1) 通用VLA模型：OpenVLA-OFT和π_0.5；2) 任务特定策略：ACT和扩散策略。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.04600v1/x5.png" alt="任务图示"></p>
<blockquote>
<p><strong>图5</strong>：评估的任务。所有任务都设计为具有不确定的初始条件，其中目标或任务关键对象的位置在执行前对模型未知，并且可以在多种配置中变化。</p>
</blockquote>
<p><strong>关键实验结果</strong>：<br>如表I所示，CoMe-VLA在所有任务上始终优于所有基线方法。使用800k人类数据和每任务100k机器人数据训练的最终模型（CoMe-VLA (800k + 100k)）实现了83.3%的平均成功率，显著高于OpenVLA-OFT (12.7%)、π_0.5 (16.0%)、ACT (2.7%)和扩散策略 (16.7%)，同时保持了97.9秒的良好平均搜索时间。使用更多机器人数据（400k每任务）的变体（CoMe-VLA (800k + 400k)）取得了最佳性能：平均成功率87.3%，平均搜索时间93.4秒。</p>
<p><img src="https://arxiv.org/html/2602.04600v1/x6.png" alt="结果表格"></p>
<blockquote>
<p><strong>图6</strong>：在5个主动感知任务上的评估结果表。展示了各方法和变体的成功率和搜索时间。CoMe-VLA在数据量增加时性能持续提升，并显著优于所有基线。</p>
</blockquote>
<p><strong>分析发现</strong>：</p>
<ol>
<li><strong>虚幻探索</strong>：一些基线（如π_0.5）的搜索时间看似有效，但实际上源于随机“游走”行为而非有意的探索，当目标进入视野时常常无法稳定。</li>
<li><strong>弱视觉-运动基础</strong>：一些基线在手眼协调方面存在困难，例如出现注视方向与抓取动作方向不一致的错位行为。</li>
</ol>
<p><strong>消融实验（数据组成分析）</strong>：<br>通过训练不同数据组成的CoMe-VLA变体来评估人类数据预训练的影响。结果（表I）表明：</p>
<ul>
<li>仅使用机器人数据（0 + 400k）性能中等（平均SR 42.7%）。</li>
<li>引入人类数据预训练后，性能显著提升。使用400k人类数据预训练（400k + 400k）将平均SR提高到72.0%。</li>
<li>进一步增加人类数据到800k（800k + 400k）带来了额外增益，达到87.3%。</li>
<li>即使将每任务机器人数据从400k减少到100k（800k + 100k），模型仍能保持83.3%的高成功率，证明了人类先验在减少机器人特定演示需求方面的有效性。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.04600v1/x7.png" alt="消融实验图"></p>
<blockquote>
<p><strong>图7</strong>：消融研究结果。左：不同数据组成对平均成功率的影响，显示人类数据预训练至关重要。右：不同数据组成对平均搜索时间的影响，显示更多数据（特别是人类数据）能显著加快搜索。</p>
</blockquote>
<p>此外，本文还对认知辅助头和双轨记忆系统进行了消融研究。结果表明，移除认知辅助头会导致模型难以触发子任务转换，在找到目标后仍继续探索。移除双轨记忆则导致模型在长时程任务中出现不一致的自我和环境感知，例如重复搜索已探索过的区域。这些结果验证了所提出组件的必要性。</p>
<p><img src="https://arxiv.org/html/2602.04600v1/x8.png" alt="定性结果"></p>
<blockquote>
<p><strong>图8</strong>：定性结果。展示了CoMe-VLA在五个任务中的成功执行序列，体现了其主动探索（如转动头部、移动底盘、打开柜门）和基于感知调整策略（如根据目标位置选择抓取方向）的能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.04600v1/x9.png" alt="动态扰动测试"></p>
<blockquote>
<p><strong>图9</strong>：动态扰动下的鲁棒性测试。展示了在任务执行过程中人为移动目标物体后，CoMe-VLA能够重新定位目标并成功完成任务，体现了其对环境动态变化的适应性。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>理论框架</strong>：将主动感知形式化为一个由信息增益和决策分支驱动的非马尔可夫决策过程，超越了反应式感知，并为视觉探索提供了系统分类法。</li>
<li><strong>方法创新</strong>：提出了CoMe-VLA框架，其特点是双轨记忆系统和认知辅助头，使机器人能够在长时程中保持环境感知并自主触发子任务转换。</li>
<li><strong>数据利用</strong>：引入了从大规模人类自我中心数据集中蒸馏探索先验的方法。通过将人类和机器人协调对齐到统一的自我中心动作空间，实现了类人“执行-感知-执行”策略向机器人平台的迁移。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的局限性包括：1) 实验在相对受控的室内环境中进行；2) 任务和场景的多样性仍有扩展空间；3) 对非常长时程记忆（远超5秒）的支持未在本文中探索。</p>
<p><strong>后续研究启示</strong>：</p>
<ol>
<li><strong>扩展场景与任务</strong>：未来的工作可以在更复杂、动态的真实世界环境中测试该方法，并探索更多样化的主动感知任务。</li>
<li><strong>增强记忆与推理</strong>：可以研究更高效的长时程记忆机制，以及结合更复杂的符号推理或大语言模型来进一步提升决策的可解释性和规划能力。</li>
<li><strong>多模态主动感知</strong>：除了视觉，可以整合其他感官模态（如触觉、听觉）进行主动感知，以应对视觉信息不足的情况。</li>
<li><strong>减少机器人数据依赖</strong>：本文展示了人类先验能有效减少对机器人数据的需求，未来可进一步探索如何最小化或实现机器人数据的零样本迁移。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决机器人在非结构化环境中主动感知能力不足的问题。提出CoMe-VLA框架，利用大规模人类自我中心数据学习探索与操作先验。关键技术包括：认知辅助头实现子任务自主切换，双轨记忆系统融合本体与视觉时序信息以维持环境感知一致性。实验表明，该方法在轮式人形机器人上执行多种长时程任务时展现出强大的鲁棒性与适应性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.04600" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>