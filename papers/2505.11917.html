<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.11917" target="_blank" rel="noreferrer">2505.11917</a></span>
        <span>作者: Lin, Fanqi, Nai, Ruiqian, Hu, Yingdong, You, Jiacheng, Zhao, Junming, Gao, Yang</span>
        <span>日期: 2025/05/17</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前赋予机器人通用能力的流行方法是双系统架构，其灵感来源于 Kahneman 的双系统理论。通常，一个独立的、基于互联网预训练的视觉语言模型作为系统二，负责缓慢的高层推理，生成中间推理内容；而一个视觉-语言-动作模型作为系统一，负责将这些中间内容转化为精确的低层机器人动作。然而，这种显式的解耦导致两个系统缺乏对彼此能力的相互了解，系统二可能产生系统一无法执行的指令。此外，在实际部署中，延迟等问题可能导致系统二响应迟缓，提供过时或无关的指导。</p>
<p>本文针对双系统架构中系统间能力不匹配和延迟这两个具体痛点，提出了一个统一模型的新视角。核心思路是构建一个单一的、统一的视觉-语言-动作模型，使其同时具备动作和推理能力，并能够自适应地在关键时刻进行显式推理，在其他时刻基于最新推理结果生成动作，从而实现推理与行动的协同。</p>
<h2 id="方法详解">方法详解</h2>
<p>OneTwoVLA 的核心是一个统一的策略模型 π_θ，其关键创新在于能够自主决定在每个时间步 t 是进入推理模式还是动作模式。</p>
<p><strong>整体框架与输入输出</strong>：模型输入包括来自多个摄像头的当前图像观测 I_t^1:n、来自最近一次推理时间步的参考图像 I_ref^1:n（用于引入观测历史，防止状态歧义）、语言指令 ℓ 以及最新的推理内容 R。在推理模式下，模型输出更新的推理文本内容 R̂。在动作模式下，模型额外输入机器人的本体感觉状态 s_t，并输出一个动作块 A_t。</p>
<p><img src="https://arxiv.org/html/2505.11917v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：OneTwoVLA 概述。它是一个能够同时进行推理和行动的单一统一模型。关键特性是能够在执行过程中的关键时刻自适应地进行推理，而在其他时间生成动作。</p>
</blockquote>
<p><strong>自适应推理决策机制</strong>：这是方法的核心。模型引入了两个特殊的决策令牌：推理开始和动作开始。在每个时间步，模型首先根据输入前缀预测其中一个令牌。若预测为，则进入推理模式，生成文本推理内容直至结束句子令牌。若预测为，则进入动作模式，直接生成动作块 A_t。这种设计使得模型仅在少数关键步骤进行推理，增加的推理时间最小，同时保证了动作模式下的推理效率。</p>
<p><img src="https://arxiv.org/html/2505.11917v1/x2.png" alt="推理流程"></p>
<blockquote>
<p><strong>图2</strong>：OneTwoVLA 在两种模式下的推理流程。上图展示了推理模式下的数据流，下图展示了动作模式下的数据流。</p>
</blockquote>
<p><strong>模型实例化</strong>：方法具有通用性，大多数现有 VLA 只需最小修改即可集成。本文以 π_0 作为基础 VLA 模型。其视觉语言部分在推理时自回归生成文本，训练时使用交叉熵损失监督。为建模复杂的连续动作分布，继承了 π_0 的动作专家架构，并使用流匹配损失进行训练。</p>
<p><strong>机器人数据构建</strong>：为训练模型具备协同推理与行动的能力，论文重新构建了机器人数据格式。将人类专家提供的演示轨迹分割为区间序列，包括<strong>推理区间</strong>（在完成子任务、检测到错误或需要人机交互等关键步骤）和<strong>动作区间</strong>。推理区间被进一步标注包含四部分的文本推理内容：1）详细的场景描述；2）高层任务计划；3）简洁的历史摘要；4）待执行的具体下一步指令。此外，还专门收集并标注了从失败状态恢复的数据，以及包含人机交互上下文的数据。</p>
<p><img src="https://arxiv.org/html/2505.11917v1/x3.png" alt="数据示例"></p>
<blockquote>
<p><strong>图3</strong>：左：带有推理内容的机器人数据示例。推理内容包括场景描述、高层计划、历史摘要和下一步指令。交互文本被附加在指令之后。右：合成的、以具身推理为中心的视觉语言数据示例。</p>
</blockquote>
<p><strong>合成视觉语言数据管道</strong>：为赋予模型更强的泛化能力，设计了一个可扩展的三步流水线来合成富含具身推理内容的视觉语言数据：1）使用大语言模型生成多样化的桌面布局文本描述；2）使用文生图模型根据描述合成高质量图像，并施加鱼眼畸变或合成机器人夹爪以贴近真实观测；3）再次使用大语言模型为每张合成图像生成任务指令及对应的推理内容。生成的指令分为<strong>视觉基础化任务</strong>和<strong>长视野任务</strong>两类。通过此流程自动生成了 16,000 个数据样本。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在真实机器人平台上进行，主要使用一个配备鱼眼镜头腕部相机的 7-DoF Franka 机械臂，部分实验使用了双 ARX 机械臂平台。</p>
<p><strong>长视野任务规划</strong>：设计了三个具有挑战性的长视野操作任务：番茄炒蛋、火锅和鸡尾酒调制。对比基线包括：1）在相同数据集上微调的最先进的 VLA 模型 π_0；2）受 ViLa 启发的双系统方法，使用 Gemini 2.5 Pro 作为系统二分解指令，并微调 π_0 作为系统一。</p>
<p><img src="https://arxiv.org/html/2505.11917v1/x4.png" alt="任务与推理示例"></p>
<blockquote>
<p><strong>图4</strong>：任务图示与推理示例。前三列展示了三个具有挑战性的长视野操作任务。最右列展示了两个通用规划实验中的任务示例。每个任务都附有模型推理内容的样例。</p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">番茄炒蛋</th>
<th align="left">火锅</th>
<th align="left">鸡尾酒</th>
<th align="left">平均</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>OneTwoVLA</strong></td>
<td align="left"><strong>85%</strong></td>
<td align="left"><strong>80%</strong></td>
<td align="left"><strong>95%</strong></td>
<td align="left"><strong>87%</strong></td>
</tr>
<tr>
<td align="left"><strong>π_0</strong></td>
<td align="left">70%</td>
<td align="left">50%</td>
<td align="left">50%</td>
<td align="left">57%</td>
</tr>
<tr>
<td align="left"><strong>双系统</strong></td>
<td align="left">55%</td>
<td align="left">70%</td>
<td align="left">65%</td>
<td align="left">63%</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：长视野任务评估结果。每种方法在每个任务上测试20次。OneTwoVLA 在长视野任务规划上优于基线。</p>
</blockquote>
<p>如表1所示，OneTwoVLA 在三个任务上取得了平均 <strong>87%</strong> 的成功率，显著优于 π_0（提升 **30%**）和双系统方法（提升 **24%**）。OneTwoVLA 能持续生成正确计划、准确跟踪任务进度并输出精确动作。π_0 由于缺乏显式推理和历史上下文，有时会迷失当前步骤或执行不精确的动作。双系统方法则受限于系统间能力不匹配以及系统二的显著推理延迟。</p>
<p><strong>通用规划</strong>：通过将机器人数据与第 3.3 节合成的视觉语言数据进行协同训练，OneTwoVLA 展现出强大的泛化能力。对于未在机器人数据中出现过的指令，模型能将视觉语言数据中的知识迁移到机器人控制中，例如主动搜索不可见物体、处理遮挡关系，并能理解抽象的人类意图。</p>
<p><strong>错误检测与恢复</strong>：OneTwoVLA 能够实时检测错误，快速推理恢复策略，并执行从收集的恢复数据中学到的纠正动作。相比之下，π_0 经常忽略错误，而双系统方法则因延迟问题无法及时响应。</p>
<p><strong>自然人机交互</strong>：得益于其自适应特性和显式推理过程，OneTwoVLA 能够自然地进行人机交互，无缝处理人类干预并在面对歧义时主动寻求澄清。双系统方法在交互中经常丢失上下文，而 π_0 则无法进行基于语言的交互。</p>
<p><strong>增强的视觉基础化</strong>：实验验证了 OneTwoVLA 在空间关系、物体属性和语义特征三个关键方面的视觉基础化能力。协同训练使其具备了开放世界的视觉基础化能力，能够泛化到训练中未见的物体和场景。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：1）提出了 <strong>OneTwoVLA</strong>，一个统一的视觉-语言-动作模型，首次在单一模型中整合了自适应推理与行动能力；2）设计了一种<strong>自适应推理机制</strong>，通过特殊决策令牌在关键时刻触发推理，平衡了信息量与执行效率；3）开发了一个<strong>可扩展的合成视觉语言数据管道</strong>，显著增强了模型的推理与泛化能力；4）在真实机器人实验上系统性地验证了模型在长视野规划、错误恢复、人机交互和视觉基础化四个方面的卓越性能。</p>
<p>论文自身提到的局限性在于，训练 OneTwoVLA 仍然需要精心策划的、包含推理标注的高质量机器人数据。尽管合成视觉语言数据有助于泛化，但其与真实机器人数据之间仍存在领域差距。</p>
<p>本工作对后续研究的启示在于：将推理与行动统一于单一模型的自适应框架，是解决双系统架构固有缺陷的有效途径，为构建更智能、更协同的通用机器人策略提供了新方向。同时，利用基础模型合成高质量、面向具体任务的训练数据，是突破机器人数据稀缺瓶颈、提升模型泛化能力的重要思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对通用机器人执行多样化任务时，双系统方法（推理与执行分离）存在的系统间能力理解有限、延迟等问题，提出了OneTwoVLA统一视觉-语言-动作模型。该模型能自适应切换推理与执行模式：在任务关键时刻进行显式推理，其他时间基于推理生成动作。通过设计可扩展的具身推理数据合成管道，与机器人数据共同训练，提升模型泛化能力。实验验证了OneTwoVLA在长时程任务规划、错误检测与恢复、自然人机交互和视觉基础方面的卓越性能，能完成制作火锅等复杂操作任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.11917" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>