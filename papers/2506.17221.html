<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.17221" target="_blank" rel="noreferrer">2506.17221</a></span>
        <span>作者: Qi, Zhangyang, Zhang, Zhixiong, Yu, Yizhou, Wang, Jiaqi, Zhao, Hengshuang</span>
        <span>日期: 2025/06/20</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，基于语言模型（LLM）的视觉语言导航（VLN）系统通常在离散的拓扑图上操作，将路径规划限制在预定义的节点连接上。尽管已有研究将VLN推广到连续空间（VLN-CE），但它们往往需要额外的深度图、导航图等信息，并依赖CLIP等专门模型进行视觉语言对齐，限制了在人与智能体交互中的泛化能力。近期，大型视觉语言模型（LVLM）被用作规划器，将视觉输入转换为文本进行路径规划，但其仍然受限于预定义的导航图，阻碍了集成视觉、语言和动作的真正具身智能体的实现。</p>
<p>本文针对LVLM在VLN任务中缺乏细粒度动作级控制、以及现有方法无法在连续环境中进行端到端导航的痛点，提出了一个全新的视角：直接利用LVLM处理第一人称视角的视频流，将其转化为连续的导航动作。本文的核心思路是采用一个受DeepSeek-R1启发的、基于GRPO的两阶段训练框架：首先通过监督微调（SFT）使模型的动作序列文本预测与专家演示对齐，然后通过结合时序衰减奖励（TDR）机制的强化微调（RFT）来战略性地加权多步未来动作，从而优化长视野导航性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>VLN-R1是一个端到端的框架，旨在使LVLM能够根据自然语言指令和第一人称视频流，在连续环境中直接预测导航动作序列。其整体流程是：在每一个时间步，模型接收语言指令和由长短时记忆采样策略处理后的历史及当前视觉观察，然后输出未来多步的原子动作（如“前进”、“左转30度”）。</p>
<p><img src="https://arxiv.org/html/2506.17221v2/x1.png" alt="方法总览"></p>
<blockquote>
<p><strong>图1</strong>：VLN-R1方法总览。以往的LLM/LVLM模型基于离散位置并使用第三人称视角进行路径规划（左）。相比之下，VLN-R1直接使用第一人称视角视频在连续环境中探索（右）。训练过程包含监督微调（SFT）和强化微调（RFT）两个阶段。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><strong>VLN-Ego数据集</strong>：为训练LVLM进行连续导航而构建的新数据集。使用Habitat模拟器在Matterport3D场景中生成，包含与未来动作预测配对的第一人称视频流。其文本标注主要由三部分组成：指令部分（系统消息和自然语言指令）、视觉部分（历史记忆和当前观察）、动作部分（四个基础动作选项：FORWARD, TURN-LEFT, TURN-RIGHT, STOP，以及真实的下6个动作）。最终从R2R和RxR轨迹中获得了总计约183万条训练样本。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17221v2/x2.png" alt="数据集构建"></p>
<blockquote>
<p><strong>图2</strong>：VLN-Ego数据集构建。使用Habitat模拟引擎创建，其文本标注主要包含指令、视觉和动作三部分。</p>
</blockquote>
<ol start="2">
<li><strong>长短时记忆采样（Long-Short Memory Sampling）</strong>：为了解决均匀采样忽略近期相关性、指数衰减丢失长期上下文的问题，该策略动态平衡历史帧的重要性。它将历史帧分为短时记忆和长时记忆两部分：短时记忆以较高频率（间隔δ1）采样最近的M帧，保留细节；长时记忆以较低频率（间隔δ2 &gt; δ1）采样更早的帧，保留长期上下文。</li>
<li><strong>监督微调（SFT）阶段</strong>：此阶段目标是让模型生成的文本化多步动作序列与真实文本对齐。模型以前述采样得到的历史观察序列、当前帧和指令为条件，自回归地预测未来n步动作的文本序列（包含动作选项标识符如“B”和对应的动作描述如“Turn left 30 degrees”）。损失函数为预测文本与真实文本之间的交叉熵损失。</li>
<li><strong>强化微调（RFT）阶段与时序衰减奖励（TDR）</strong>：在SFT之后，采用基于GRPO的强化学习进行进一步优化。创新点在于设计了TDR奖励函数，以解决VLN任务中的时序依赖问题。TDR对模型预测的多步动作进行验证：如果第k步预测的动作与真实动作匹配，则给予奖励rk = γ^(k-1)，其中γ是衰减因子（0&lt;γ&lt;1）。这意味着越早的预测步骤奖励越高，鼓励模型优先保证近期关键动作的准确性，同时仍考虑远期动作。GRPO算法则根据一组（G个）响应计算出的TDR奖励，进行组内归一化得到相对优势（Advantage），用于策略优化，并辅以KL散度惩罚防止策略偏离参考模型过多。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.17221v2/x3.png" alt="模型架构"></p>
<blockquote>
<p><strong>图3</strong>：VLN-R1模型架构。采用长短时记忆策略处理视觉输入。训练包含两个阶段：SFT阶段仅监督输出文本；RFT阶段使用设计的时序衰减奖励（TDR）机制进行监督。</p>
</blockquote>
<p>与现有方法相比，VLN-R1的创新点具体体现在：1) <strong>端到端的连续导航</strong>：首次利用LVLM直接处理第一人称视频流，输出低级控制指令，实现真正的连续空间导航，无需依赖预定义图或额外模态（如深度图）。2) <strong>长短时记忆采样</strong>：提出了一种平衡短期细节与长期上下文的视觉帧采样策略。3) <strong>时序衰减奖励机制</strong>：针对导航任务的时序特性，设计了软奖励函数，优化多步动作预测。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在VLN-CE（连续环境）基准上评估导航性能，使用了其中的Room-to-Room（R2R）和Room-Across-Room（RxR）数据集。实验平台基于Habitat模拟器。</p>
<p><strong>对比方法</strong>：对比了多种基线方法，包括早期VLN方法（VLN-BERT, HAMT）、VLN-CE方法（CMA, WS-MGMap, SASRA）、以及基于LLM/LVLM的方法（NaviLLM, Navid, Uni-Navi）。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>导航成功率</strong>：在R2R val-unseen数据集上，VLN-R1达到了<strong>70.8%</strong> 的成功率（Success Rate, SR），显著优于所有对比的基线方法。在路径长度加权指标（SPL）上达到<strong>59.8%<strong>。在更长、更复杂的RxR val-unseen数据集上，VLN-R1也取得了</strong>54.0%</strong> 的成功率，优于其他方法。</li>
<li><strong>消融实验</strong>：验证了各核心组件的贡献。实验表明，同时使用SFT和RFT训练比仅用SFT在成功率上提升了约5个百分点。在RFT中，使用时序衰减奖励（TDR）比使用简单的二元匹配奖励带来了约3个百分点的性能提升。长短时记忆采样策略也优于均匀采样或指数衰减采样。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.17221v2/x4.png" alt="实验结果"></p>
<blockquote>
<p><strong>图4</strong>：在R2R和RxR数据集上的导航性能对比。VLN-R1在val-unseen分割上的成功率（SR）和路径长度加权成功率（SPL）均优于现有方法，尤其在R2R上取得了显著领先。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>VLN-Ego数据集</strong>，这是一个专门用于在连续环境中训练LVLM进行视觉语言导航的第一人称视频流数据集。</li>
<li>提出了<strong>VLN-R1框架</strong>，一个端到端的具身AI框架，首次利用LVLM直接处理第一人称视频流，实现实时的视觉语言导航与动作输出。</li>
<li>开创性地将<strong>GRPO和强化微调（RFT）</strong> 集成到LVLM的导航任务训练中，并设计了<strong>时序衰减奖励（TDR）</strong> 机制，专门用于优化导航场景下的LVLM性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，VLN-R1目前依赖于模拟器（Habitat）生成的数据进行训练和评估。尽管模拟器能提供丰富的训练数据，但其与真实物理世界的差异（sim-to-real gap）仍然是实际应用需要面对的挑战。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>高效视觉编码</strong>：直接处理高帧率视频流对计算资源要求较高。未来研究可以探索更高效的视觉token压缩或编码策略。</li>
<li><strong>向真实世界迁移</strong>：本文验证了LVLM在模拟环境中进行连续导航的潜力。一个重要的未来方向是将该方法扩展到真实机器人平台，解决sim-to-real的迁移问题。</li>
<li><strong>奖励机制设计</strong>：TDR机制证明了针对任务特性设计奖励函数的有效性。这启发了在其他具身任务中（如物体操作）设计更精细、可验证的奖励函数。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出VLN-R1框架，旨在解决视觉语言导航（VLN）中现有方法依赖离散拓扑图、无法实现端到端连续动作控制的核心问题。方法上，采用基于大型视觉语言模型（LVLM）的两阶段训练：首先进行监督微调（SFT）对齐专家动作序列，然后引入时间衰减奖励（TDR）机制进行强化微调（RFT），以平衡多步未来动作。实验表明，该框架在VLN-CE基准上取得了强劲性能，验证了LVLM通过数据高效的奖励驱动训练可实现具身导航与任务推理。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.17221" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>