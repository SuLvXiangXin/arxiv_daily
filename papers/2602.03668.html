<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.03668" target="_blank" rel="noreferrer">2602.03668</a></span>
        <span>作者: Jungwoo Lee Team</span>
        <span>日期: 2026-02-03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人学习领域面临的核心瓶颈是获取带动作标签的真实世界机器人演示数据成本高昂。利用丰富的人类操作视频进行学习（Learning from Video, LfV）成为一种有前景的替代方案。然而，这些视频缺乏低层级的机器人动作标签，阻碍了标准的监督模仿学习。为此，近期方法提出学习<strong>潜在动作</strong>，即视频帧间转换的紧凑表示，并将其用作伪动作标签来预训练视觉-语言-动作模型。为了使VLA预训练有效，潜在动作必须包含关于智能体底层动作的信息，尽管没有真实标签。</p>
<p>当前主流的潜在动作模型通常从单视角视频中学习，通过编码帧间转换并优化重建损失来预测下一帧。其关键局限性在于，视觉转换会受到<strong>外生噪声</strong>的干扰，例如背景中人物的移动，其中<strong>视角变化</strong>尤为突出。视角变化引入了相机移动和透视变换，使得视觉转换与智能体动作纠缠在一起。因此，从单视角重建学习的潜在动作可能过度拟合于视角相关的线索，从而降低了对真实动作的预测能力。</p>
<p>本文针对视角变化这一具体痛点，提出了<strong>多视角潜在动作模型</strong>。其核心思路是：利用时间同步的多视角视频，通过一个跨视角重建目标进行训练，使得从一个视角推断出的潜在动作必须能够解释另一个视角的未来观测，从而减少对视角特定线索的依赖，学习到更具“动作中心性”的潜在动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>MVP-LAM的整体框架是一个基于向量量化变分自编码器的潜在动作模型，其创新在于引入了跨视角重建的训练目标。模型输入是时间同步的多视角视频帧对，输出是离散的潜在动作标记。</p>
<p><img src="https://arxiv.org/html/2602.03668v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：MVP-LAM使用时间同步多视角视频进行训练。(1) <strong>自视角重建</strong>（左）：对于每个视角v，冻结的DINOv2提取特征$(o_t^v, o_{t+1}^v)$。一个时空编码器产生连续潜在$e_t^v$，随后被向量量化为离散标记$z_t^v$，解码器从$(o_t^v, z_t^v)$重建$o_{t+1}^v$。(2) <strong>跨视角重建</strong>（右）：MVP-LAM在重建每个视角的未来特征时，交换不同视角间的潜在标记（例如$z_t^{v_1} \leftrightarrow z_t^{v_2}$），从而鼓励$z_t$捕捉固有的转换信息。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉特征提取</strong>：使用冻结的预训练视觉编码器（如DINOv2）将每个视角的图像$I_t^v$转换为特征$o_t^v$。</li>
<li><strong>潜在动作编码与量化</strong>：一个时空编码器$E_\theta$接收当前帧特征$o_t^v$和下一帧特征$o_{t+1}^v$，输出一个连续潜在向量$e_t^v$。该向量通过向量量化操作，在可学习的码本中查找最近的嵌入，得到离散的潜在动作标记$z_t^v$。</li>
<li><strong>解码与重建</strong>：一个解码器$D_\theta$接收当前帧特征$o_t^v$和潜在动作标记$z_t$，预测下一帧特征$\hat{o}_{t+1}^v$。</li>
<li><strong>训练目标</strong>：总损失函数包含三部分：<ul>
<li><strong>自视角重建损失</strong>：$\mathcal{L}<em>{\text{self}} = \sum_v | o</em>{t+1}^v - D_\theta(o_t^v, z_t^v) |_2^2$。确保潜在动作能解释本视角的转换。</li>
<li><strong>跨视角重建损失</strong>：$\mathcal{L}<em>{\text{cross}} = \sum</em>{v_i \neq v_j} | o_{t+1}^{v_i} - D_\theta(o_t^{v_i}, z_t^{v_j}) |_2^2$。这是核心创新点。强制从一个视角$v_j$推断出的潜在动作$z_t^{v_j}$，必须能用来准确预测另一个不同视角$v_i$的下一帧。这迫使潜在动作编码与视角无关的、由动作引起的状态变化信息。</li>
<li><strong>VQ-VAE标准损失</strong>：包括量化损失$\mathcal{L}_{\text{quant}} = | \text{sg}[e_t] - z_t |<em>2^2$和承诺损失$\mathcal{L}</em>{\text{commit}} = \beta | e_t - \text{sg}[z_t] |_2^2$，其中$\text{sg}[\cdot]$表示停止梯度操作。</li>
</ul>
</li>
</ol>
<p><strong>与现有方法的创新点</strong>：与仅使用单视角视频和自重建目标的传统LAM相比，MVP-LAM明确利用了时间同步的多视角数据，并通过<strong>跨视角重建目标</strong>作为一种隐式的正则化，有效剥离了视角变化带来的噪声，使学习到的离散潜在动作更专注于编码智能体执行的动作本身。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>潜在动作评估数据集</strong>：Bridge V2（包含真实机器人动作标签）。</li>
<li><strong>下游任务基准</strong>：SIMPLER（多任务模仿学习）和LIBERO-Long（长视野任务）。</li>
<li><strong>对比基线方法</strong>：包括LAPA、Moto、UniVLA等单视角潜在动作学习方法，以及LAOM（使用少量动作监督的方法）。</li>
<li><strong>评估指标</strong>：潜在动作与真实动作的互信息、动作预测准确率、下游任务成功率。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>动作中心性评估</strong>：在Bridge V2上，MVP-LAM学习的潜在动作与真实动作之间的互信息最高，达到0.56比特，显著高于最佳基线UniVLA的0.45比特。</p>
<p><img src="https://arxiv.org/html/2602.03668v1/x3.png" alt="互信息与预测准确率"></p>
<blockquote>
<p><strong>图3</strong>：在Bridge V2数据集上，MVP-LAM在潜在动作-真实动作互信息和动作预测准确率上均优于所有基线方法。</p>
</blockquote>
</li>
<li><p><strong>分布外泛化</strong>：在视角分布外评估中，MVP-LAM的动作预测准确率（73.1%）远超最佳基线（58.3%），证明了其学习到的表示对视角变化的鲁棒性。</p>
<p><img src="https://arxiv.org/html/2602.03668v1/x4.png" alt="分布外评估"></p>
<blockquote>
<p><strong>图4</strong>：在视角分布外评估中，MVP-LAM在动作预测任务上展现出显著的性能优势，表明其潜在动作对视角变化具有更好的不变性。</p>
</blockquote>
</li>
<li><p><strong>下游任务性能</strong>：使用MVP-LAM的潜在动作作为伪标签预训练VLA模型，在SIMPLER基准上的平均成功率达到65.7%，优于使用Moto（62.3%）和UniVLA（63.7%）潜在动作的VLA。在LIBERO-Long基准的90个任务上，MVP-LAM预训练的VLA也取得了最高成功率。</p>
<p><img src="https://arxiv.org/html/2602.03668v1/x5.png" alt="下游任务成功率"></p>
<blockquote>
<p><strong>图5</strong>：在SIMPLER基准测试中，使用MVP-LAM潜在动作预训练的VLA模型在平均成功率上表现最佳。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.03668v1/x6.png" alt="LIBERO-Long结果"></p>
<blockquote>
<p><strong>图6</strong>：在LIBERO-Long基准测试中，基于MVP-LAM预训练的VLA模型在90个任务上的成功率最高。</p>
</blockquote>
</li>
<li><p><strong>消融实验</strong>：</p>
<ul>
<li><strong>跨视角重建的重要性</strong>：移除跨视角重建损失会导致互信息和动作预测性能显著下降。</li>
<li><strong>多视角数据的作用</strong>：仅使用单视角数据训练，即使有跨视角目标（通过数据增强模拟），性能也不及使用真实多视角数据。</li>
<li><strong>视角数量</strong>：增加训练时的视角数量（从2个到4个）能持续提升潜在动作的质量。</li>
</ul>
<p><img src="https://arxiv.org/html/2602.03668v1/x7.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融研究表明，跨视角重建目标和真实的多视角数据对MVP-LAM的性能都至关重要。</p>
</blockquote>
</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了MVP-LAM，首个利用时间同步多视角视频和跨视角重建目标来学习动作中心性潜在动作的模型。</li>
<li>在Bridge V2上实证表明，MVP-LAM无需动作监督即可学习到与真实动作互信息更高、且对视角变化更鲁棒的潜在动作。</li>
<li>证明了MVP-LAM生成的潜在动作作为伪标签，能有效提升VLA模型在下游机器人操作任务（SIMPLER, LIBERO-Long）上的性能。</li>
</ol>
<p><strong>局限性</strong>：论文提到，MVP-LAM依赖于时间同步的多视角视频数据，这类数据可能不如单视角视频普遍。此外，方法假设不同视角观测到的是相同的底层状态转换。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>利用多模态不变性</strong>：本研究展示了利用不同视角间的不变性来学习更纯净表征的有效性。这可以扩展到利用其他模态（如语言描述、触觉）之间的对齐来学习更具语义和功能性的表示。</li>
<li><strong>应对更复杂的外生噪声</strong>：跨视角重建主要针对视角噪声，未来工作可以探索如何将类似原理应用于消除其他类型的外生噪声（如动态背景、光照变化）。</li>
<li><strong>数据效率与泛化</strong>：该方法为从丰富的多视角人类视频（如EgoExo4D）中学习可迁移的机器人技能开辟了新途径，可能有助于解决机器人学习的数据稀缺问题。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MVP-LAM模型，旨在从多视角人类视频中学习更接近真实动作的离散潜在动作，以提升视觉-语言-动作模型的预训练效果。核心方法是利用时间同步的多视角视频，通过跨视角重建目标进行训练，使从单一视角推断的潜在动作能预测另一视角的未来状态，从而减少对视角特定线索的依赖。实验表明，在Bridge V2数据集上，MVP-LAM学习的潜在动作与真实动作互信息更高，动作预测性能更优；使用其潜在动作预训练的VLA模型在SIMPLER和LIBERO-Long基准测试中取得了更好的下游操作性能。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.03668" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>