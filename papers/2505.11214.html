<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.11214" target="_blank" rel="noreferrer">2505.11214</a></span>
        <span>作者: Zhao, Wei, Li, Gongsheng, Gong, Zhefei, Ding, Pengxiang, Zhao, Han, Wang, Donglin</span>
        <span>日期: 2025/05/16</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作（VLA）模型通过利用在大规模互联网数据上预训练的视觉-语言基础模型（VLM），能够直接从视觉观察和人类指令生成机器人动作，在机器人领域受到高度关注。以RT-2、RT-X和OpenVLA为代表的主流方法，其性能已超越传统策略模型。然而，现有VLA模型通常只接受单一形式的指令——语言指令，这限制了它们在开放人机交互场景中的适用性。在实际生活中，人类指令形式多样，例如展示一张图片让机器人抓取物体、要求机器人遵循白板上的手写指令，或模仿一段视频演示的行为。</p>
<p>本文针对VLA模型指令模态单一这一具体痛点，提出了使其能够处理开放式多模态指令的新视角。核心思路是：通过一个统一的神经架构，赋予VLA模型理解并响应交错出现的语言和图像指令的能力，从而显著扩展其应用范围。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的OE-VLA模型旨在处理开放式多模态指令。其整体架构基于一个能够处理自由形式多图像输入的视觉语言基础模型（LLaVA-Next-Interleave），并通过两阶段课程学习策略进行微调，最终使其能够输出机器人动作令牌。</p>
<p><img src="https://arxiv.org/html/2505.11214v1/x2.png" alt="方法整体架构"></p>
<blockquote>
<p><strong>图2</strong>：OE-VLA模型的整体架构。主要由三部分组成：视觉编码器（SigLIP-ViT）处理所有输入图像（观测图像和指令图像），生成视觉令牌；一个两层的MLP投影器将视觉令牌映射到与语言令牌相同的隐空间；LLM主干（Qwen-1.5）接收由观测令牌、语言令牌和图像指令令牌交错拼接而成的最终令牌序列，并自回归地预测离散化的机器人动作令牌。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>视觉编码器</strong>：采用SigLIP-400M Vision Transformer，接收384x384分辨率的图像。它处理两种来源的图像：一是机器人观测（静态工作空间视图和腕部相机动态视图），二是来自指令的描述性图像（如图片、视频帧）。所有图像经编码后生成视觉令牌。</li>
<li><strong>LLM主干与令牌化</strong>：LLM主干为Qwen-1.5，支持长达32k的上下文。文本指令被其分词器转换为语言令牌。连续机器人动作被离散化为256个区间，并用Qwen词表中最低频的语言令牌表示，作为模型的预测目标。模型预测一个五步的动作块（action chunk）。</li>
<li><strong>输入序列构建</strong>：通过一个两层MLP将视觉令牌投影到语言令牌的隐空间。然后，按照指令的自然顺序（例如：“Given observation <obs>: grasp the <img1> and put it into <img2>”），将观测令牌(<code>T_obs</code>)、语言令牌(<code>T_lang</code>)和图像指令令牌(<code>T_img</code>)交错拼接(<code>Concat</code>)成最终的输入序列<code>T_final</code>，送入LLM。</li>
<li><strong>两阶段训练流程</strong>：<ul>
<li><strong>第一阶段：多图像接地（Multi-Image Grounding）</strong>：为了弥补基础模型在视觉叙事等任务与机器人操作所需的空间关系理解之间的差距，使用MGrounding数据集对基础模型进行微调。该阶段旨在增强模型对多幅图像中物体间空间关系的精确感知能力。</li>
<li><strong>第二阶段：开放式指令微调（Open-ended Instruction Tuning）</strong>：使用本文构建的、包含多模态指令的机器人操作数据集对模型进行微调。所有训练样本被统一格式化为<code>((&lt;obs&gt;, (text1, &lt;img1&gt;, text2, &lt;img2&gt;)), &lt;act&gt;)</code>，并在训练时随机打乱。</li>
</ul>
</li>
</ol>
<p><strong>创新点</strong>：与现有VLA模型相比，OE-VLA的核心创新在于其输入接口能够原生处理交错的多模态（语言+图像）指令序列，而非仅限纯文本。此外，提出的两阶段课程学习策略专门针对提升模型从多模态指令中理解任务需求的能力。</p>
<p><strong>数据构建方法</strong>：论文提出了一种将现有带语言标注的机器人数据集转化为多模态指令数据集的通用方法。</p>
<p><img src="https://arxiv.org/html/2505.11214v1/x3.png" alt="数据构建方法"></p>
<blockquote>
<p><strong>图3</strong>：将传统数据集转化为开放式指令数据集的流程。原始数据集被划分为五个子集，其中一个保留纯语言指令，其余四个分别转化为视觉物体指定（VOS）、光学指令跟随（OIF）、视频演示学习（VDL）和视觉目标达成（VGR）任务的训练数据，具体通过替换语言部分为图像或视频帧实现。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准测试</strong>：主要使用CALVIN评测套件。此外，为评估开放式指令，论文引入了两个新基准：<strong>OE-CALVIN_base</strong>（指令使用环境内裁剪的物体图片、常规字体光学指令等）和<strong>OE-CALVIN_hard</strong>（指令使用来自网络的物体图片、手写字体、不同视角的环境视频等，难度更高）。</li>
<li><strong>数据划分</strong>：采用更具挑战性的<code>ABC → D</code>划分（在A、B、C场景训练，在D场景测试）进行主要报告。</li>
<li><strong>基线方法</strong>：由于此前没有能处理此类开放式任务的VLA模型，实验选择了多个强大的纯语言条件VLA模型作为基线进行对比，包括RoboFlamingo、OpenVLA、LLaVA-VLA和KosMos Inter.，且均不使用扩散策略头以确保公平。</li>
</ul>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>纯语言指令性能</strong>：如表1所示，尽管OE-VLA旨在处理多模态指令，其在纯语言输入的CALVIN基准上仍具强大竞争力。OE-VLA 7B模型在<code>ABC → D</code>设置下取得了最佳性能，平均成功序列长度达2.99。</li>
</ol>
<p><img src="https://arxiv.org/html/2505.11214v1/x1.png" alt="语言指令性能表"></p>
<blockquote>
<p><strong>表1</strong>：不同机器人策略模型在CALVIN <code>ABC → D</code>划分下的性能对比。OE-VLA 7b取得了最高的平均成功序列长度（2.99）。</p>
</blockquote>
<ol start="2">
<li><strong>开放式多模态指令性能</strong>：<ul>
<li><strong>OE-CALVIN_base</strong>：如表2所示，OE-VLA 1b的平均成功序列长度（2.75）甚至略高于其语言基线。OE-VLA 7b性能提升显著，平均长度达3.48，超越了先前的语言基线。模型在视觉物体指定（VOS）任务上表现尤为出色，视觉目标达成（VGR）任务最具挑战性。</li>
<li><strong>OE-CALVIN_hard</strong>：如表3所示，面对来自网络、不同视角的更具挑战性指令，模型性能有所下降，但仍具备竞争力。OE-VLA 7b在很大程度上缓解了性能下降，VOS任务表现依然良好，但VGR任务因单张图像信息有限而仍然困难。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2505.11214v1/x4.png" alt="多模态指令性能表"></p>
<blockquote>
<p><strong>表2 &amp; 表3</strong>：OE-VLA模型在OE-CALVIN_base（上）和OE-CALVIN_hard（下）基准上的性能。展示了模型在四种开放式任务类型上的成功率和平均序列长度。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：<ul>
<li><strong>两阶段训练策略的有效性</strong>：图5和图6的消融实验表明，两阶段训练流程（特别是第一阶段的多图像接地训练）显著提升了模型在<code>ABC → D</code>划分下的性能。对于<code>ABCD → D</code>划分，由于测试环境D已在训练中出现，提升相对有限。对于7B大模型，第一阶段的提升相对较小，可能因为基础模型已具备较强的空间关系理解能力。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2505.11214v1/x5.png" alt="消融实验图"></p>
<blockquote>
<p><strong>图5</strong>：在OE-CALVIN_base基准上，两阶段训练（蓝色）相比仅进行第二阶段训练（橙色）带来的性能提升，特别是在<code>ABC → D</code>划分下效果显著。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>OE-VLA</strong>，一种能够通过统一神经架构处理多样化开放式多模态人类指令的新型VLA模型。</li>
<li>提出了一种利用现有数据集<strong>构建多模态指令机器人数据集的通用方法</strong>，以及一个<strong>两阶段课程学习算法</strong>，用于将视觉语言基础模型微调为适用于开放式指令的VLA模型。</li>
<li>基于CALVIN套件引入了两个包含多样化开放式指令的稳健基准<strong>OE-CALVIN_base</strong>和<strong>OE-CALVIN_hard</strong>，以促进该方向的研究。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，视觉目标达成（VGR）任务由于仅提供单张目标状态图像，信息有限，对模型而言最具挑战性，性能相对其他任务较低。</p>
<p><strong>启示</strong>：本研究证明了使用自由形式的多模态指令引导机器人完成任务的巨大潜力，极大地扩展了VLA模型与人类交互的接口。对后续研究的启示包括：进一步探索更复杂、动态的多模态指令（如音频、手势），研究如何更好地处理VGR等信息稀疏的指令，以及将这种开放式指令能力与更强大的动作生成策略（如扩散模型）相结合。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对当前视觉-语言-动作模型仅能处理语言指令，限制了其在开放人机交互中应用的问题，提出了OE-VLA模型以探索其处理开放多模态指令的潜力。该方法使模型能直接理解并响应图像、白板文字、视频演示等多模态指令。实验表明，OE-VLA在处理语言指令时性能与传统模型相当，并在四类额外的开放多模态任务上取得了令人印象深刻的结果。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.11214" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>