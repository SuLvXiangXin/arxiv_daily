<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.18068" target="_blank" rel="noreferrer">2512.18068</a></span>
        <span>作者: Axel Krieger Team</span>
        <span>日期: 2025-12-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，从内窥镜图像中精确估计关节化手术器械的6自由度位姿是机器人辅助微创手术中的基础挑战。现有方法主要依赖于标记物、立体视觉、深度传感或机器人运动学数据，但这些方法在应用于大规模在线单目手术视频时存在局限：标记物不切实际，立体视觉和深度传感需要额外硬件，而机器人运动学数据在公开的临床视频中通常不可得。尽管基于学习的方法显示出潜力，但许多方法仅关注非关节化工具或预测2D关键点，而非完整的6自由度位姿。更近期的方法探索了“渲染-比较”策略，但单纯依赖可微分渲染进行位姿估计容易因初始估计不佳而导致优化失败。</p>
<p>本文针对从大规模在线单目手术视频中提取器械运动学数据这一具体痛点，提出了一种新视角：利用可微分渲染技术，在无需真实运动学数据的情况下，从视频中估计工具的轨迹和关节角度。本文的核心思路是：提出一个名为SurgiPose的两阶段框架，首先通过一个粗估计模块提供鲁棒的初始位姿猜测，然后利用可微分渲染迭代优化位姿和关节角，以最小化渲染图像与真实观察图像之间的差异，从而为手术机器人模仿学习提供可用的运动学数据。</p>
<h2 id="方法详解">方法详解</h2>
<p>SurgiPose的整体框架是一个两阶段的位姿估计流程。输入是单目手术视频帧，输出是手术工具末端执行器相对于相机坐标系的6自由度位姿变换矩阵 $T_{CE} \in SE(3)$ 以及关节角度 $\mathbf{q} = [q_1, q_2, q_3]^\top$（分别对应工具俯仰角和两个钳口的开合角）。第一阶段（粗估计）为视频第一帧生成一个初始位姿估计；第二阶段（细化）通过可微分渲染对该估计进行优化。对于视频序列，处理完第一帧后，将细化后的位姿作为后续帧的初始化，并进行逐帧跟踪优化。</p>
<p><img src="https://arxiv.org/html/2512.18068v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SurgiPose流程概览。第一阶段（粗估计器）初始化位姿，然后通过可微分渲染进行细化。最终估计的6自由度位姿和关节角用于运动学提取和模仿学习。</p>
</blockquote>
<p><strong>核心模块一：粗位姿估计</strong><br>此模块旨在解决可微分渲染优化对初始值敏感、易陷入局部最优的问题。它专门用于处理视频的第一帧，因为后续帧依赖于前一帧的跟踪结果。其具体流程是：首先使用SAM2分割并裁剪出手术工具，计算工具掩码的中心点。以此中心点为中心，在平行于图像平面的方向上构建一个3x3的方形网格。对于网格上的每个点，生成36个候选初始猜测，方法是在0到$2\pi$范围内均匀采样，绕指向图像内部的z轴进行旋转（不考虑x、y轴旋转和深度变化，因为这些参数可在后续细化中解决）。然后，对每个候选猜测进行可微分渲染细化，并计算像素平均损失。最终选择损失最低的候选作为初始位姿 $T_{CE}^{initial}$。</p>
<p><img src="https://arxiv.org/html/2512.18068v1/x3.png" alt="粗估计流程"></p>
<blockquote>
<p><strong>图3</strong>：粗估计流程可视化。(a)原始视频帧。(b)分割并裁剪后的手术工具，显示掩码中心及用于生成潜在初始猜测的3x3网格。(c)粗估计模块提出的粗估计。(d)基于细化后估计中损失最低的原则选择最佳初始猜测。</p>
</blockquote>
<p><strong>核心模块二：可微分渲染与优化</strong><br>该模块用于细化初始位姿并估计关节角。首先，在MuJoCo仿真中使用手术工具的URDF模型生成合成数据集，包含500个关节处于中立位的“规范”工具位姿和10,000个随机采样关节角（避免自碰撞）的“位姿条件”工具配置，每个配置从12个随机摄像机视角渲染。随后，采用与现有工作类似的三阶段训练方法学习工具的可微分高斯溅射模型：1) 从多视角图像学习工具的规范3D高斯表示；2) 引入变形场以建模不同关节配置引起的形状变化；3) 联合优化规范模型和变形场。</p>
<p>在测试时（即对真实视频进行位姿估计），优化器以 $T_{CE}^{initial}$ 和初始关节角为起点，通过最小化渲染图像 $I_{ren}$ 与观测图像 $I_{obs}$ 之间的差异来迭代更新位姿和关节角。损失函数结合了结构相似性(SSIM)和均方误差(MSE)：$L_{\text{combined}} = \alpha(1-\text{SSIM}(I_{\text{ren}}, I_{\text{obs}})) + (1-\alpha)|I_{\text{ren}} - I_{\text{obs}}|_{2}^{2}$，其中 $\alpha=0.8$。</p>
<p>位姿更新采用特定的梯度更新规则：旋转矩阵 $R_{\text{updated}} = R_{\text{current}} \cdot (\mathbf{I} + \alpha<br>abla R)$，平移向量 $t_{\text{updated}} = t_{\text{current}} - \beta \cdot \text{clamp}(<br>abla t, -\delta, \delta)$，其中 $\alpha=0.3, \beta=3\times10^{-4}, \delta=0.02$，以确保数值稳定性。关节角通过梯度下降更新：$\mathbf{q}<em>{\text{updated}} = \mathbf{q}</em>{\text{current}} - \gamma \cdot<br>abla_{\mathbf{q}}L_{\text{combined}}$ ($\gamma=10^{-3}$)，并利用 $\text{clamp}$ 函数强制执行关节角度限制。</p>
<p><strong>与现有方法的创新点</strong><br>本文的核心创新在于将<strong>粗估计模块</strong>与<strong>可微分渲染细化模块</strong>相结合。与单纯依赖可微分渲染的方法相比，这种两阶段策略通过系统性地评估多个初始假设，显著提高了优化的鲁棒性和收敛到正确位姿的可能性，特别是在视频序列的起始帧。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置与基准</strong>：实验平台为da Vinci Research Kit Si (dVRK Si)，使用大型持针器作为末端执行器。主要在两个机器人手术任务上进行评估：组织提升和针拾取。收集了220次组织提升和224次针拾取的演示数据，包含同步视频和真实运动学数据。此外，还在公开的SurgRIPE数据集和自收集的离体胆囊切除术数据集上进行了泛化性评估。对比的基线是基于真实运动学数据训练的模仿学习策略。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>轨迹提取与回放</strong>：定性实验表明，从视频中提取的运动学轨迹能够成功引导机器人复现组织提升任务。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.18068v1/x4.png" alt="轨迹回放结果"></p>
<blockquote>
<p><strong>图4</strong>：轨迹回放实验结果。前三张图显示机器人执行估计轨迹的关键瞬间，绿色掩码覆盖层代表原始视频中的对应工具位姿。(d) 将我们管道估计的末端执行器轨迹与通过正向运动学获得的真实轨迹进行比较。</p>
</blockquote>
<ol start="2">
<li><p><strong>运动学估计精度</strong>：如表I所示，对于组织提升和针拾取任务，平均位移误差(ADE)分别为9.7毫米和12.0毫米，最终位移误差(FDE)分别为15.3毫米和14.4毫米。误差主要分布在深度方向（z轴），平均误差分别为6.64毫米和5.61毫米，这凸显了单目深度估计的挑战。</p>
</li>
<li><p><strong>模仿学习策略性能</strong>：如表II所示，使用估计运动学训练的模仿学习策略在组织提升任务上成功率为70%（7/10），在针拾取任务上为60%（6/10）。虽然略低于使用真实运动学训练的策略（组织提升100%，针拾取80%），但证明了其可行性。失败案例多与深度估计不准确相关，例如在组织提升中机器人倾向于推开组织而非抬起。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2512.18068v1/x5.png" alt="模仿学习定性结果"></p>
<blockquote>
<p><strong>图5</strong>：模仿学习实验的定性结果：比较使用真实运动学和估计运动学训练的策-略在组织提升实验关键时刻的快照。上图显示使用真实运动学训练的策-略，下图显示使用直接从视频估计的运动学训练的策-略，均展示了初始位姿、抓取组织和提升组织三个时刻。</p>
</blockquote>
<p><strong>消融实验与组件贡献</strong>：虽然没有独立的消融实验表格，但论文通过两阶段设计本身（对比单纯可微分渲染）以及实验结果表明，<strong>粗估计模块</strong>对于提供鲁棒初始化、防止优化失败至关重要。而<strong>可微分渲染细化模块</strong>则负责精确估计6自由度位姿和关节角度。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个从单目手术视频生成大规模运动学数据的框架，减少了对运动捕捉系统的依赖，为互联网规模的手术机器人学习奠定了基础。</li>
<li>提出了一种新颖的单目6自由度位姿估计方法，将粗估计与可微分渲染相结合，其中粗估计器提供了关键的位姿初始化，提高了鲁棒性和准确性。</li>
<li>通过实验证明，使用估计运动学训练的模仿学习策略能够取得与基于真实数据训练的策略相当的成功率，验证了该框架的可行性。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ol>
<li>深度估计（z轴）精度相对较低，这是单目视觉固有的挑战，可能导致任务执行时出现推送而非抓取等错误。</li>
<li>方法依赖于SAM2进行工具分割，分割质量会影响后续估计。</li>
<li>目前主要针对特定工具模型进行了验证，尽管在离体数据上展示了初步泛化能力，但大规模泛化仍需进一步研究。</li>
</ol>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li>本文的工作为实现从海量在线手术视频中直接学习机器人控制策略铺平了道路，是构建大规模手术视觉-语言-动作模型的关键一步。</li>
<li>未来工作可以专注于改进深度估计，例如通过引入时间一致性约束或从大规模数据集中学习先验知识。</li>
<li>探索对更多样化手术工具和复杂场景的泛化能力，将是推动该方法走向临床实际应用的重要方向。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决从单目手术视频估计手术工具运动学数据的关键问题，以支持机器人模仿学习。提出SurgiPose方法，其核心技术是基于可微分渲染，通过优化工具姿态参数来最小化渲染图像与真实视频帧的差异，从而推断工具轨迹和关节角度。在da Vinci机器人上的组织提升与针拾取实验表明，使用视频估计的运动学数据训练的策略，其成功率与使用真实运动学数据训练的策略相当，验证了该方法的可行性。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.18068" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>