<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.08126" target="_blank" rel="noreferrer">2509.08126</a></span>
        <span>作者: Yu, Houjian, Zhou, Zheming, Sun, Min, Ghasemalizadeh, Omid, Sun, Yuyin, Kuo, Cheng-Hao, Sen, Arnie, Choi, Changhyun</span>
        <span>日期: 2025/09/09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，语言驱动的机器人抓取方法通常受限于预定义的词汇和简单的属性描述（如“红色苹果”），并且通常假设场景中的物体是唯一的、没有重复实例。这导致现有方法无法处理开放形式的语言输入或执行更具挑战性的空间推理任务。此外，这些方法严重依赖成本高昂的、密集的像素级标注来同时进行物体定位（Grounding）和抓取配置预测。多模态大语言模型（MLLMs）虽然展现了强大的多模态理解能力，但其巨大的计算需求限制了其在资源受限机器人平台上的部署。因此，本文旨在解决两个关键挑战：1）能否设计一个紧凑、计算高效的多模态融合模块，作为MLLMs的替代方案，有效对齐视觉和语言特征以用于真实世界机器人抓取？2）能否使用稀疏且不完美的标签，以弱监督方式训练抓取模型？</p>
<p>本文针对上述痛点，提出了一个名为“基于属性的物体定位与机器人抓取”（OGRG）的新框架。其核心思路是：通过一个新颖的双向视觉-语言融合模块，并结合深度信息以增强几何推理，来理解开放形式的语言指令并进行空间推理，从而在包含重复物体实例的场景中定位目标物体并预测平面抓取姿态，同时支持全监督和弱监督两种学习范式。</p>
<h2 id="方法详解">方法详解</h2>
<p>OGRG框架旨在处理两个主要抓取检测任务：指称抓取合成（RGS，全监督）和指称抓取可供性（RGA，弱监督）。整体上，模型接收RGB图像 $I$、深度图 $D$ 和基于属性的语言描述 $T$ 作为输入，输出物体定位的二值掩码 $M$ 和机器人抓取姿态 $G$。</p>
<p><img src="https://arxiv.org/html/2509.08126v1/x2.png" alt="OGRG整体架构"></p>
<blockquote>
<p><strong>图2</strong>: OGRG整体架构。模型处理开放形式的语言表达式、视觉图像和深度图以生成任务预测。双向对齐器（Bi-Aligner）融合了从Swin Transformer不同阶段提取的特征和BERT语言模型提取的特征。更新后的多模态特征 $f_{back_v}^{i}$ 和 $f_{back_l}^{i}$ 通过特征门反馈到其对应的视觉和语言分支。最后，轻量级的全卷积网络（FCN）头处理不同阶段的更新视觉特征以产生任务特定的输出。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>:</p>
<ol>
<li><strong>多模态特征提取与双向融合</strong>: 视觉主干使用Swin Transformer，语言特征提取器使用BERT Transformer，深度特征使用ResNet-18提取。核心创新是<strong>双向对齐器</strong>。在每个融合阶段，视觉特征 $f_{in_v}^{i}$ 和语言特征 $f_{in_l}^{i}$ 通过两个交叉注意力机制进行交互。首先，视觉特征与深度特征 $f_d$（仅在第一阶段使用）进行逐元素相加得到 $f_{in_vd}^{i}$。然后计算：<ul>
<li><strong>视觉到语言交叉注意力</strong>: 以语言特征为Key和Value，视觉-深度特征为Query，生成 $f_{cross_v}^{i}$。</li>
<li><strong>语言到视觉交叉注意力</strong>: 以视觉-深度特征为Key和Value，语言特征为Query，生成 $f_{cross_l}^{i}$。<br>这两个特征经过1x1卷积和ReLU激活后，形成融合特征 $f_{back_v}^{i}$ 和 $f_{back_l}^{i}$。它们随后通过可学习的特征门 $g_i$ 进行加权，并分别与原始的 $f_{in_v}^{i}$ 和 $f_{in_l}^{i}$ 相加，生成增强后的视觉和语言特征 $f_{v}^{i}$ 和 $f_{l}^{i}$，供后续阶段或解码使用。</li>
</ul>
</li>
<li><strong>任务特定解码头</strong>: 使用一个轻量级FCN头进行解码。对于RGS任务，使用 $V_i = {f_{v}^{i}}$ 作为输入；对于RGA任务，使用 $V_i = {f_{back_v}^{i}}$ 作为输入。解码过程通过上采样和卷积层融合多尺度特征，最终输出预测图。</li>
<li><strong>RGA专用的掩码条件抓取网络</strong>: 在RGA弱监督设置下，OGRG仅预测物体定位掩码 $M$。抓取可供性图 $A$ 由独立的<strong>掩码条件抓取网络</strong>预测。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.08126v1/x3.png" alt="掩码条件抓取网络"></p>
<blockquote>
<p><strong>图3</strong>: 掩码条件抓取网络。以OGRG预测的物体定位掩码 $M$ 为条件，该网络使用全卷积编码器-解码器架构，预测具有不同旋转角度的像素级抓取可供性图。</p>
</blockquote>
<p>MGN接收RGB图像 $I$、深度图 $D$ 和预测的掩码 $M$ 的拼接作为输入，通过ResNet-18主干和FCN解码器，输出 $N=6$ 个离散旋转角度（$30^{\circ}$ 倍数）的抓取可供性图。训练采用弱监督方式，仅对采样的抓取位置提供单像素的二元 {0,1} 标签，并使用来自Attribute-Grasp的运动损失 $\mathcal{L}_{grasp}$。</p>
<p><strong>与现有方法的创新点</strong>:</p>
<ul>
<li><strong>双向融合 vs. 单向融合</strong>: 不同于LAVT等方法的单向融合，OGRG的双向对齐器允许视觉和语言特征相互更新，促进更充分的多模态交互。</li>
<li><strong>深度信息集成</strong>: 在融合早期显式引入深度特征，增强了模型对场景几何的理解，有助于空间推理和抓取姿态估计。</li>
<li><strong>平衡模型效率与性能</strong>: 相比ETRG采用的激进下采样策略（可能导致信息丢失），OGRG通过有效的双向融合模块，在保持模型相对紧凑（约240M参数）的同时取得了更优性能。</li>
<li><strong>统一框架支持两种监督模式</strong>: 同一核心架构可适配全监督（RGS）和弱监督（RGA）两种抓取学习范式，提高了方法的实用性和数据效率。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>:</p>
<ul>
<li><strong>RGS任务</strong>: 使用<strong>OCID-VLG</strong>数据集进行训练和评估。输入图像分辨率调整为416x416。</li>
<li><strong>RGA任务</strong>: 在<strong>CoppeliaSim仿真器</strong>中收集自定义数据集，包含超过16,000个视觉-语言-抓取三元组，物体来自YCB数据集等，并应用了域随机化。语言指令模板涵盖颜色、形状、类别名以及绝对/相对空间关系。</li>
<li><strong>对比基线</strong>: 包括CROG、ETRG、HiFi-CS、LAVT等。</li>
<li><strong>评估指标</strong>:<ul>
<li><strong>物体定位</strong>: 平均交并比（mIoU）或整体交并比（oIoU）。</li>
<li><strong>抓取预测</strong>: RGS使用Jaccard Index $J@N$（衡量前N个预测抓取矩形与真实值的匹配度）；RGA使用<strong>抓取成功率</strong>（成功抓取正确目标的次数/总尝试次数）。</li>
</ul>
</li>
</ul>
<p><strong>关键实验结果</strong>:</p>
<ol>
<li><strong>RGS全监督性能</strong>: 如表I所示，OGRG在OCID-VLG数据集上取得了最佳性能。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.08126v1/x1.png" alt="RGS实验结果表"></p>
<blockquote>
<p><strong>图1</strong>: （根据上下文，此处应为展示RGS和RGA任务定义的示意图，但论文中Table I是RGS结果表。用户提供的图片链接x1.png对应论文图1，即任务示意图。）OGRG模型设计用于解决基于属性的定位和抓取检测任务。RGS子任务旨在通过像素级全监督预测抓取矩形。RGA子任务侧重于通过弱抓取监督预测抓取可供性。</p>
</blockquote>
<pre><code>- 物体定位（mIoU）达到95.60%，相比CROG提升+14.5%，相比LAVT提升+3.08%。
- 抓取预测（$J@1$）达到90.81%，相比CROG提升+13.61%。
- 消融实验（OGRG-nodepth）表明，**深度融合**贡献了+0.73%的mIoU提升。
- 推理速度在单块RTX 2080 Ti GPU上达到**17.59 FPS**。
</code></pre>
<ol start="2">
<li><strong>RGA弱监督性能</strong>:<ul>
<li><strong>物体定位</strong>: 如表II所示，OGRG在包含绝对/相对空间关系及多种属性组合的测试场景中，平均oIoU达到94.02%，优于所有基线（如比ETRG高+3.48%）。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.08126v1/x4.png" alt="RGA物体定位结果表"></p>
<blockquote>
<p><strong>图4</strong>: （根据上下文，此处应为展示仿真与真实世界实验所用物体的示意图，但论文中Table II是RGA物体定位结果。用户提供的图片链接x4.png对应论文图4，即实验对象图。）目标物体包括用于仿真的32个实例和用于真实机器人实验的15个实例。训练时对机器人工作空间应用域随机化以获得鲁棒性能，测试时使用来自VIMA的纹理。</p>
</blockquote>
<pre><code>- **机器人抓取**: 如表III所示，在仿真抓取实验中，OGRG在已见背景条件下的平均抓取成功率达到93.42%，相比ETRG（88.56%）提升+4.86%。在未见背景条件下也保持了91.90%的成功率，展示了良好的泛化能力。
</code></pre>
<ol start="3">
<li><strong>定性结果分析</strong>:</li>
</ol>
<p><img src="https://arxiv.org/html/2509.08126v1/x5.png" alt="定性结果对比"></p>
<blockquote>
<p><strong>图5</strong>: 物体定位掩码和抓取可供性可视化对比。(a)(b)显示ETRG方法未能正确定位目标并提供可行的抓取姿态，而OGRG-RGA能够准确分割语言所指的目标并预测可行的抓取姿态。(c)进一步显示，ETRG生成的抓取可供性图在错误的目标候选上存在冗余高值，而本文方法直接聚焦于正确的目标物体。</p>
</blockquote>
<pre><code>如图5所示，在复杂空间语言指令下（如“抓取位于绿色圆柱体绿色杯子右上方的纸巾盒”），基线方法ETRG可能定位失败或抓取预测错误，而OGRG能准确分割目标并预测合理的抓取区域。
</code></pre>
<p><strong>消融实验总结</strong>:</p>
<ul>
<li><strong>双向对齐器</strong>: 对比LAVT（单向），OGRG的双向设计带来了性能增益。</li>
<li><strong>深度融合</strong>: 对比OGRG-nodepth，加入深度信息提升了物体定位和抓取预测的精度。</li>
<li><strong>特征传递路径</strong>: 对比OGRG-db（将融合特征直接传递至下一对齐器），OGRG采用的将融合特征通过门控机制反馈回各自主干的设计效果更优。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>:</p>
<ol>
<li>提出了一个端到端的<strong>OGRG模型</strong>，能够处理开放形式的语言指令，进行空间推理，在包含重复物体的场景中同时完成物体定位（输出掩码）和机器人抓取姿态（5-DoF）预测。</li>
<li>设计了<strong>双向多模态融合模块</strong>，有效对齐视觉、语言和深度特征，无需依赖预对齐的视觉语言模型（如CLIP），在模型紧凑性和任务性能间取得了良好平衡。</li>
<li>验证了框架在<strong>全监督（RGS）和弱监督（RGA）</strong> 两种设置下的有效性，在仿真和真实机器人实验中均超越了现有基线方法，特别是在处理空间关系语言和弱标注数据方面表现突出。</li>
</ol>
<p><strong>局限性</strong>: 论文自身未明确陈述显著的局限性，但根据方法描述，深度信息仅在融合的第一阶段被引入，其在整个多阶段推理中的更深入集成或许有探索空间。</p>
<p><strong>对后续研究的启示</strong>:</p>
<ul>
<li><strong>高效多模态融合架构</strong>: OGRG证明，无需庞大MLLMs，精心设计的中等规模融合模块也能在机器人具体任务上取得优异性能，为资源受限的嵌入式部署提供了思路。</li>
<li><strong>弱监督抓取学习</strong>: 成功使用极稀疏的单像素抓取标签训练抓取可供性预测网络，大幅降低了数据标注成本，为大规模机器人技能学习提供了可行的技术路径。</li>
<li><strong>复杂指令理解</strong>: 框架对包含绝对和相对空间关系的语言指令的有效处理，推动了机器人向更自然、更复杂的人机交互方向发展。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出OGRG框架，旨在解决机器人依据开放形式自然语言指令在包含重复物体的场景中精准定位目标并完成抓取的挑战。其核心方法包含双向视觉语言融合模块，并整合深度信息以增强空间推理能力。实验表明，在完全监督的RGS设置下，模型推理速度达17.59 FPS，定位与抓取精度超越基线；在弱监督的RGA设置下，其抓取成功率在仿真与真实实验中同样优于基线方法。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.08126" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>