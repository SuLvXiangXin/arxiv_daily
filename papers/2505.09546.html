<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Distilling Realizable Students from Unrealizable Teachers - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Distilling Realizable Students from Unrealizable Teachers</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2505.09546" target="_blank" rel="noreferrer">2505.09546</a></span>
        <span>作者: Kim, Yujin, Chin, Nathaniel, Vasudev, Arnav, Choudhury, Sanjiban</span>
        <span>日期: 2025/05/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>在机器人学习领域，一个核心挑战是让机器人仅基于部分观测（如局部视觉）做出有效决策，而仿真中训练的策略通常能访问特权状态信息（如全局坐标）。策略蒸馏是应对此挑战的常用方法：首先训练一个拥有特权信息的教师策略，然后引导一个仅具备现实感知能力的学生策略进行模仿学习。然而，这种方法面临信息不对称的根本性挑战：教师基于完整状态给出的指导，在学生受限的观测空间中往往是不可实现的。这导致了状态混淆，即多个不同的教师状态映射到同一个学生观测，使得教师对同一学生观测可能给出相互矛盾的动作指令。若学生盲目模仿，其轨迹会偏离预期，引发分布偏移和性能下降。</p>
<p>现有方法主要分为两类：一是修改教师策略以产生学生可实现的次优示范；二是结合模仿学习与强化学习，让学生自行探索以推断缺失信息。这两类方法存在固有权衡：限制教师会限制性能上限，而混合IL和RL则可能产生冲突信号，导致训练不稳定。本文的核心思路是，学生不应盲目模仿教师，而应策略性地与教师互动——仅在关键时刻查询教师以获取纠正，并重置到教师可恢复的状态——从而将自己保持在自身观测空间内的可恢复路径上。本文提出了两种基于此思路的方法：基于模仿学习的CritiQ和基于强化学习的ReTRy。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文提出的蒸馏框架旨在解决信息不匹配下的学习问题。其核心思想是让学生策略性地与教师互动，以维持在可恢复的轨迹上。具体提出了两种互补的方法。</p>
<p><strong>1. 关键状态查询（CritiQ）</strong><br>CritiQ是一种模仿学习方法，其核心创新在于仅当学生面临进入不可恢复轨迹的风险时，才查询教师获取纠正动作，从而避免因在混淆状态下频繁查询而积累冲突标签。</p>
<p><img src="https://arxiv.org/html/2505.09546v1/extracted/6439231/fig/Critiq.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：DAgger与CritiQ对比示意图。学生目标是在三个关闭的盒子中找到藏有硬币的盒子。教师知道硬币位置并直接导航。在第一次迭代中，学生从三个演示中选出一条轨迹，但因演示不足无法继续。DAgger在随机状态查询专家，导致数据集越来越不可实现，引发策略发散。CritiQ则仅在关键状态（如即将选择错误盒子时）查询，确保收集必要信息的同时维持可实现性并提升策略稳定性。</p>
</blockquote>
<p>算法流程如下：首先用行为克隆初始化学生策略，并初始化一个用于区分教师与学生轨迹的判别器<code>G_φ</code>。在每一迭代中，先滚动执行当前学生策略收集轨迹。对于轨迹中的每个状态<code>s_t</code>，使用判别器判断其是否为“关键状态”（即教师未曾访问过的状态）。若判别器评分低于阈值<code>κ</code>，则查询教师获取最优动作<code>a_t*</code>，并将<code>(s_t, a_t*)</code>加入数据集<code>D</code>。随后，学生策略通过最小化组合损失<code>L_S(D, G_φ)</code>来更新，该损失包含对数据集<code>D</code>的模仿损失<code>ℓ(D, π)</code>和一项鼓励学生“愚弄”判别器的项<code>-λE_τ∼π[G_φ(τ)]</code>。同时，判别器通过最小化损失<code>L_G(π, π*)</code>（标准对抗损失，区分来自教师<code>π*</code>和学生<code>π</code>的轨迹）进行更新。此过程迭代进行，逐步优化学生策略和判别器。</p>
<p>与DAgger相比，CritiQ的理论优势在于控制了可实现性误差<code>δ</code>的增长。DAgger在每个状态都查询教师，导致在混淆状态<code>s</code>下，数据集<code>D(s)</code>不断累积来自不同隐藏上下文<code>c</code>的冲突教师动作标签，使得<code>δ</code>单调递增。而CritiQ将查询限制在关键状态（即学生尚无恢复数据的区域），使得对于非关键状态，数据集大小基本保持不变，从而显著减缓了<code>δ</code>的增长速度。</p>
<p><strong>2. 重置至教师恢复状态（ReTRy）</strong><br>ReTRy是一种强化学习方法，它通过迭代地优化学生RL训练时的重置分布来提升样本效率，从而绕开了状态混淆问题。</p>
<p><img src="https://arxiv.org/html/2505.09546v1/extracted/6439231/fig/ReTRy.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：ReTRy示意图。教师与学生之间的双向互动提供了可行的重置分布，使RL智能体能更有效地获取策略。随着迭代进行，RL学生的探索空间变得更密集，形成结构化的策略学习搜索区域。深蓝色区域代表高概率重置状态，浅蓝色区域概率较低，白色区域概率为零。</p>
</blockquote>
<p>算法流程是一个双向交互过程：初始化教师访问状态数据集<code>D^π*</code>和学生访问状态数据集<code>D^π</code>。在每一迭代中，首先从<code>D^π*</code>中采样初始状态，让学生策略<code>π_i</code>从此开始滚动执行，收集经验并存入<code>D^π</code>，随后使用策略梯度等RL方法更新学生策略。接着，从<code>D^π</code>中采样状态，让教师策略<code>π*</code>从此开始滚动执行，生成新的恢复轨迹并加入<code>D^π*</code>。这样，<code>D^π*</code>（重置分布）随着学生探索的区域动态扩展，确保学生能从其可能访问的状态附近的有效恢复点开始学习，从而引导RL进行更高效、结构化的探索。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟和真实机器人任务上验证了所提方法。模拟任务包括：<strong>Dr</strong>（动态重定向，要求四足机器人在部分观测下走向目标，但腿部电机指令被随机重定向）、<strong>Push</strong>（部分可观测的物体推送，目标位置对学生隐藏）、<strong>Nav</strong>（视觉导航，机器人仅凭第一视角图像走向目标）。真实机器人任务是在一台Unitree Go1机器人上进行的视觉导航任务。</p>
<p>对比的基线方法包括：<strong>BC</strong>（行为克隆）、<strong>DAgger</strong>、<strong>DART</strong>（一种修改教师以产生可实现演示的方法）、<strong>TPRL</strong>（教师引导的预重置RL，一种静态的重置方法）以及<strong>Oracle</strong>（拥有特权信息的最优策略）。</p>
<p><img src="https://arxiv.org/html/2505.09546v1/extracted/6439231/fig/overall_results_sim_bar.png" alt="模拟环境结果"></p>
<blockquote>
<p><strong>图7</strong>：在三个模拟任务上的平均成功率。CritiQ和ReTRy在所有任务上均显著优于基线方法BC、DAgger、DART和TPRL。例如在Dr任务中，CritiQ和ReTRy分别达到约92%和94%的成功率，而DAgger仅约65%，BC仅约20%。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09546v1/extracted/6439231/fig/training_time.png" alt="训练时间曲线"></p>
<blockquote>
<p><strong>图8</strong>：训练过程中的性能随时间变化曲线。CritiQ和ReTRy相比基线方法收敛更快，且达到更高的最终性能。DAgger在后期因混淆加剧导致性能下降，而CritiQ保持了稳定提升。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2505.09546v1/extracted/6439231/fig/real_robot_bar.png" alt="真实机器人结果"></p>
<blockquote>
<p><strong>图9</strong>：真实机器人视觉导航任务的成功率。CritiQ取得了最高的成功率（80%），显著优于BC（20%）、DAgger（40%）和TPRL（53%），证明了其在实际系统中的有效性。</p>
</blockquote>
<p>关键实验结果总结：在模拟任务中，CritiQ和ReTRy在Dr、Push、Nav任务上的平均成功率分别达到约92%/94%、85%/88%、82%/86%，全面大幅超越所有基线。在真实机器人导航任务中，CritiQ取得了80%的成功率，是最佳基线TPRL（53%）的1.5倍以上。消融实验表明，CritiQ中的判别器对于识别关键状态至关重要，移除此组件会导致性能下降至接近DAgger的水平；ReTRy中动态的双向重置机制是其优于静态重置方法（如TPRL）的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）形式化了信息不匹配下的教师-学生蒸馏问题，并分析了DAgger等基础算法在此设置下的局限性（即可实现性误差δ会因状态混淆而单调增长）。2）提出了两种新颖算法CritiQ（IL）和ReTRy（RL），它们基于“策略性师生互动以保持可恢复路径”的洞察，分别通过选择性查询和动态重置优化，在理论上控制了δ的增长或绕开了混淆问题，并在实践中显著提升了训练效率和最终性能。3）在模拟与真实机器人任务上进行了广泛验证，证明了方法的有效性、高效性和通用性。</p>
<p>论文自身提到的局限性包括：CritiQ依赖于一个需要训练的判别器来识别关键状态，这增加了复杂度；ReTRy作为RL方法，其采样效率虽然提升，但仍可能高于纯IL方法。</p>
<p>本文对后续研究的启示在于：为处理特权信息蒸馏中的信息不对称问题提供了一个新的“策略性交互”范式，而非简单地修改教师或混合信号。未来的工作可以探索如何将两种方法的优势结合，或如何将判别器的训练进一步简化或理论化。此外，该方法可应用于更广泛的领域，其中存在信息层级或不同模态间的信息不匹配。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文研究特权信息下的策略蒸馏问题，核心挑战是学生策略因部分观察无法直接模仿全状态教师，导致信息不对称和性能下降。针对现有方法效率低下的不足，提出学生应战略性地与教师交互以保持可恢复路径，引入两种关键技术：一是自适应确定查询时机的模仿学习方法，二是选择初始化点以高效探索的强化学习方法。在模拟和真实机器人任务中验证，相比标准教师-学生基线，该方法在训练效率和最终性能上均取得显著提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2505.09546" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>