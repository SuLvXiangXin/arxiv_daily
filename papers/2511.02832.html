<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.02832" target="_blank" rel="noreferrer">2511.02832</a></span>
        <span>作者: Ze, Yanjie, Zhao, Siheng, Wang, Weizhuo, Kanazawa, Angjoo, Duan, Rocky, Abbeel, Pieter, Shi, Guanya, Wu, Jiajun, Liu, C. Karen</span>
        <span>日期: 2025/11/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>人形机器人领域缺乏与双臂操作等领域同等有效的大规模数据收集框架。现有的人形遥操作系统主要分为三类：a) 上下体解耦控制（如HOMIE）；b) 部分全身控制，仅协调手臂和躯干，腿部跟踪基座速度指令（如AMO、CLONE）；c) 依赖昂贵、非便携动作捕捉系统的完整全身控制（如TWIST）。这些方法要么牺牲了便携性和可扩展性，要么牺牲了完整的全身协调能力，限制了捕捉人类自然展现的动态全身技能和进行规模化数据收集。</p>
<p>本文针对“完整全身控制”与“便携性/可扩展性”难以兼得的痛点，提出了一种无需动捕、便携且支持完整全身控制的新系统。核心思路是利用PICO 4U VR设备实时获取人体全身运动，并设计一个低成本的可附加颈部结构来提供机器人第一人称视觉，从而构建一个完整的人到人形机器人的遥操作与数据收集流水线。</p>
<h2 id="方法详解">方法详解</h2>
<p>TWIST2系统是一个分层的控制与数据收集框架，旨在基于机器人的自我中心视觉和本体感知执行多样的全身灵巧任务。整体框架包含低级控制器 π_low 和高级控制器 π_high。π_low 是一个通用的运动跟踪控制器，接收包含全身关节位置等的参考命令向量 <strong>p_cmd</strong> 和机器人本体状态 <strong>s</strong>，输出目标关节位置。π_high 则根据视觉观察 <strong>o</strong> 和本体状态 <strong>s</strong> 生成任务特定的运动命令 <strong>p_cmd</strong>。在数据收集中，π_high 由人类操作员加运动重定向器扮演；收集到的数据用于训练一个自主的视觉运动策略 π_high^auto。</p>
<p><img src="https://arxiv.org/html/2511.02832v1/x2.png" alt="系统总览"></p>
<blockquote>
<p><strong>图2</strong>：TWIST2系统概述。我们使用便携设备和自我中心主动视觉构建了一个完整的人形遥操作系统，支持可扩展的模仿数据收集。基于收集的数据，我们构建了一个能直接预测全身关节位置的分层视觉运动策略学习框架。</p>
</blockquote>
<p>系统主要由四个组件构成：</p>
<ol>
<li><strong>带主动视觉的人形机器人</strong>：使用Unitree G1，并为其设计了一个低成本、可附加的2自由度颈部（TWIST2 Neck），以提供类似人类的主动视觉能力。<br><img src="https://arxiv.org/html/2511.02832v1/figures/neck_design.png" alt="颈部设计"><blockquote>
<p><strong>图3</strong>：TWIST2颈部。我们设计了一个简单有效的2自由度颈部，非专业用户可以轻松组装，并且可以附着/拆卸到Unitree G1上，无需移除原有的LiDAR。</p>
</blockquote>
</li>
<li><strong>便携、无动捕的人体数据源</strong>：使用PICO 4U VR设备（头戴显示器、手持控制器）和两个绑在小腿上的PICO运动追踪器，以约100Hz的频率实时流式传输全身人体运动，无需复杂校准，成本约1000美元。</li>
<li><strong>完整的人到机器人运动重定向</strong>：基于GMR方法进行修改，将VR获取的人体运动映射到机器人关节空间。针对PICO全局位姿估计不准确的问题，对重定向优化进行了关键修改：对于下半身，同时优化旋转和位置约束以减少脚滑；对于上半身，仅优化旋转约束，以避免全局位姿跳跃（如操作员瞬移）带来的伪影。<br><img src="https://arxiv.org/html/2511.02832v1/x4.png" alt="运动重定向映射"><blockquote>
<p><strong>图6</strong>：将VR人体映射到机器人连杆。展示了从PICO获取的人体骨骼到机器人模型的关节映射关系。</p>
</blockquote>
</li>
<li><strong>用于低级控制的通用运动跟踪器</strong>：使用强化学习（PPO）训练一个通用的低级跟踪控制器 π_low。训练数据混合了来自AMASS、OMOMO等数据集的约20k个运动片段，以及通过PICO设备收集的73个覆盖日常动作的运动片段，以确保策略能学习全向行走并弥合领域差距。奖励函数设计为跟踪奖励和正则化奖励之和。</li>
</ol>
<p>基于以上模块，构建了可扩展的数据收集系统：操作员通过PICO获取机器人立体视觉流，其动作被实时重定向为 <strong>p_cmd</strong> 并发送给 π_low 执行。整个系统通过编程PICO手持控制器实现单操作员安全控制，支持暂停、平滑状态过渡，系统整体延迟低于0.1秒。</p>
<p>进一步，利用收集到的高质量演示数据，提出了一个分层的全身视觉运动策略学习框架。高级策略 π_high^auto 采用扩散策略（Diffusion Policy），以224x224的RGB图像和历史命令序列 <strong>p_cmd</strong> 作为输入，预测未来2秒（64步）的运动命令。使用预训练的R3M ResNet-18进行视觉编码，并应用状态和视觉数据增强以提高鲁棒性。<br><img src="https://arxiv.org/html/2511.02832v1/x5.png" alt="分层策略框架"></p>
<blockquote>
<p><strong>图7</strong>：基于TWIST2收集数据构建的分层全身视觉运动策略学习框架。与之前分别关注上半身操作或下半身运动的工作不同，我们的视觉运动策略控制整个身体，实现了需要协调全身动作的复杂任务。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台为配备TWIST2颈部和ZED Mini相机的Unitree G1机器人。研究展示了三方面能力：长时程遥操作、高效数据收集以及基于自我中心视觉的自主任务执行。</p>
<p><strong>1. 长时程遥操作</strong>：系统能够执行非常长时程、需要精细全身协调的任务。<br><img src="https://arxiv.org/html/2511.02832v1/x7.png" alt="长时程遥操作"></p>
<blockquote>
<p><strong>图10</strong>：由TWIST2支持的长时程人形遥操作。所有任务均通过流式机器人自我中心视觉、完整全身控制和单操作员实现。<br>    - <strong>折叠毛巾</strong>：机器人定位毛巾、移动、抓握、抖开、捏住角落对折、压平折痕并整齐放置。整个过程需要手腕和手的精细控制、主动视觉和全身伸展。<br>    - <strong>搬运篮子穿过门</strong>：机器人调整脚部位置、弯腰拾取篮子、直立、走到门前、用一只手开门、穿过并关门、走到目标位置放下篮子。需要全身移动和操作协调。</p>
</blockquote>
<p><strong>2. 高效数据收集</strong>：演示了高效的数据收集能力，例如在20分钟内收集了约100条成功演示且没有失败。数据收集后通过后处理GUI进行轨迹分割和过滤。</p>
<p><strong>3. 自主视觉运动策略执行</strong>：基于收集的数据训练了扩散策略，使机器人能够基于自我中心视觉自主执行任务。<br><img src="https://arxiv.org/html/2511.02832v1/x8.png" alt="自主策略执行"></p>
<blockquote>
<p><strong>图11</strong>：现实世界中的闭环全身视觉运动策略执行。TWIST2实现了有效且完整的人形全身数据收集，进而实现了多功能自主全身人形移动操作和腿部操作技能。<br>    - <strong>全身灵巧取放</strong>：机器人连续从桌面上抓取不同物体（杯子、瓶子、盒子）并将其放入篮子中。<br>    - <strong>连续踢T形盒（Kick-T）</strong>：机器人连续将一个T形盒子踢向目标区域。这需要动态的、协调的全身运动，包括平衡和精确的腿部控制。<br>    <img src="https://arxiv.org/html/2511.02832v1/figures/wbdex_results.png" alt="自主任务结果"><br><strong>图14</strong>：自主全身灵巧操作和Kick-T任务的定性结果。展示了机器人基于视觉成功执行连续取放和踢击任务。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出并实现了一个便携、无需动捕、支持完整全身控制的人形遥操作与数据收集系统（TWIST2），其关键创新包括低成本可附加颈部和基于VR的便携动捕方案；2) 基于该系统收集的数据，首次提出了一个能够基于视觉自主控制人形机器人全身（而不仅是根速度或上半身）的分层策略学习框架，并演示了复杂的全身移动操作和腿部操作任务；3) 整个系统、数据和模型均已开源，确保了完全的可复现性。</p>
<p>论文提到的局限性包括系统仍然依赖于特定的VR设备（PICO）和机器人硬件（Unitree G1），并且自主策略的复杂性目前受限于收集的数据量和任务范围。</p>
<p>这项工作为人形机器人领域提供了一个亟需的、可扩展的数据收集基础设施范例。它表明，结合消费级VR设备和巧妙的硬件设计，可以低成本地实现高质量的全身演示数据收集，从而为训练强大的人形机器人视觉运动策略铺平道路。未来的研究可以基于此开源系统，收集更多样化、更大规模的数据集，并探索更复杂的多任务、长时程自主技能学习。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人缺乏高效数据收集框架的问题，提出了TWIST2系统。该系统采用PICO4U VR实现实时全身运动捕捉，并设计了一个低成本2自由度机器人颈部以提供自我中心视觉，从而实现了无需昂贵动作捕捉设备的、便携的全身遥操作。实验表明，该系统能在15分钟内以近100%成功率收集100条演示数据，并基于此训练的分层视觉运动策略能成功执行全身灵巧操作和动态踢球等任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.02832" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>