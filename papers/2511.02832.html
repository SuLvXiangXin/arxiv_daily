<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2511.02832" target="_blank" rel="noreferrer">2511.02832</a></span>
        <span>作者: Ze, Yanjie, Zhao, Siheng, Wang, Weizhuo, Kanazawa, Angjoo, Duan, Rocky, Abbeel, Pieter, Shi, Guanya, Wu, Jiajun, Liu, C. Karen</span>
        <span>日期: 2025/11/04</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>大规模数据驱动了机器人学的突破，从双臂灵巧操作的成功模型可见一斑，但人形机器人领域尚缺乏同等高效的数据收集框架。现有的人形遥操作系统主要分为三类：a) 上下半身解耦控制；b) 部分全身控制，通常只协调上半身，腿部仅跟踪基座速度指令；c) 依赖昂贵、非便携运动捕捉系统的完整全身控制。前两类方法牺牲了全身协调能力，无法捕捉人类自然的动态全身技能；而第三类方法虽然展现了全身控制的潜力，但其高昂成本和部署限制阻碍了可扩展性。本文针对“在保持完整全身控制能力的同时，提升系统便携性与可扩展性”这一具体痛点，提出了一种基于消费级VR设备的、无动捕的便携式人形遥操作与数据收集系统TWIST2。其核心思路是：利用PICO 4U VR设备实时获取操作者全身运动，配合一个低成本的可拆卸机器人颈部实现自我中心视觉，构建一个从人到机器人的完整重定向与控制流水线，从而实现高效、单操作者的全身遥操作与数据收集。</p>
<h2 id="方法详解">方法详解</h2>
<p>TWIST2系统是一个层次化控制框架，包含低层控制器和高层控制器。低层控制器负责通用的运动跟踪，高层控制器（可以是人类操作者或自主视觉运动策略）根据自我中心视觉生成任务特定的运动指令。</p>
<p><img src="https://arxiv.org/html/2511.02832v1/x2.png" alt="系统总览"></p>
<blockquote>
<p><strong>图2</strong>：TWIST2系统总览。我们构建了一个包含便携设备和自我中心主动视觉的完整人形遥操作系统，支持可扩展的模仿数据收集。利用收集的数据，我们构建了一个直接预测全身关节位置的层次化视觉运动策略学习框架。</p>
</blockquote>
<p>系统由四个主要组件构成：</p>
<ol>
<li><strong>带主动视觉的人形机器人</strong>：使用Unitree G1（29自由度）并配备Dex31手。关键创新是设计了一个低成本（约250美元）、可附加的2自由度颈部（TWIST2 Neck），为机器人提供类似人类的主动视觉能力，这对于长时程、精细遥操作至关重要。</li>
<li><strong>便携无动捕全身动作源</strong>：使用PICO 4U VR设备（头显、手持控制器）和两个绑在小腿上的PICO运动追踪器，以约100Hz的频率实时流式传输操作者全身姿态。此方案成本约1000美元，无需复杂校准，设置仅需约1分钟，相比光学动捕系统具有显著便携优势。</li>
<li><strong>完整的人到人形机器人的运动重定向</strong>：采用改进的GMR方法将VR捕捉的人体运动映射到机器人关节指令。针对PICO全局姿态估计噪声大的问题，对优化进行了调整：对于下半身链接（如脚、踝），同时优化旋转和位置约束以减少脚滑；对于上半身链接，仅优化旋转约束，这样即使操作者进行“传送”操作（全局位置跳变），上半身重定向也不会产生伪影。</li>
<li><strong>用于低层控制的通用运动跟踪器</strong>：使用强化学习（PPO）在混合运动数据集（约2万条运动片段，来源包括AMASS、OMOMO、内部动捕数据以及少量通过PICO收集的73条日常运动）上训练一个稳健的低层策略。该策略接收参考运动指令和机器人本体感知，输出目标关节位置，由PD控制器跟踪生成最终扭矩。奖励函数设计为跟踪奖励和正则化奖励之和。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1) <strong>首次结合完整全身控制与高便携性</strong>，通过消费级VR替代动捕；2) <strong>引入低成本可附加颈部</strong>，实现了对长时程任务至关重要的自我中心主动视觉；3) <strong>设计了支持单操作者、安全控制的全套交互系统</strong>，包括通过手持控制器管理任务启停、状态平滑插值等。</p>
<p><img src="https://arxiv.org/html/2511.02832v1/figures/neck_design.png" alt="颈部设计"></p>
<blockquote>
<p><strong>图3</strong>：TWIST2颈部。我们设计了一个简单有效的2自由度颈部，非专业用户可轻松组装，并能无需移除原装LiDAR的情况下附加/拆卸到Unitree G1上。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2511.02832v1/x3.png" alt="颈部重定向"></p>
<blockquote>
<p><strong>图5</strong>：用机器人颈部模仿人类颈部运动。我们发现2自由度（偏航和俯仰）的颈部足以模仿主要的人类颈部运动。</p>
</blockquote>
<h2 id="实验与结果">实验与结果</h2>
<p>实验平台为配备TWIST2颈部和ZED Mini立体相机的Unitree G1人形机器人。论文展示了三方面能力：长时程遥操作、高效数据收集以及基于视觉的自主策略执行。</p>
<p><strong>1. 长时程遥操作</strong>：展示了两个代表性复杂任务（见图10）：</p>
<ul>
<li><strong>折叠毛巾</strong>：机器人利用自我中心视觉定位毛巾，移动、抓取、抖动铺平，然后双手捏住角落对折，重复折叠至目标尺寸并压平褶皱，最后整齐放置。该任务需要精细的手腕/手部控制、主动视觉和全身协调。</li>
<li><strong>搬运篮子穿过门</strong>：机器人调整脚步位置，弯腰拾取左侧篮子，然后起身，走向并穿过一扇门，将篮子放在门后的目标位置。该任务需要全身移动和操作协调。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.02832v1/x7.png" alt="遥操作任务"></p>
<blockquote>
<p><strong>图10</strong>：由TWIST2驱动的长时程人形遥操作。所有任务均通过流式传输的机器人自我中心视觉、完整的全身控制和单个操作者实现。</p>
</blockquote>
<p><strong>2. 高效数据收集</strong>：系统可实现高效、高成功率的演示收集。例如，可以在20分钟内收集约100次成功演示，且几乎无失败。整个系统延迟低于0.1秒，显著优于TWIST的0.5秒延迟。系统支持单操作者独立完成所有操作（启停、暂停、恢复），并通过状态插值确保安全。</p>
<p><strong>3. 自主视觉运动策略</strong>：基于收集的高质量演示数据，训练了一个层次化视觉运动策略（高层为Diffusion Policy，低层为前述运动跟踪器）。该策略以224x224的RGB图像和历史指令序列作为输入，直接预测未来的全身运动指令。</p>
<p><img src="https://arxiv.org/html/2511.02832v1/x5.png" alt="策略框架"></p>
<blockquote>
<p><strong>图7</strong>：基于TWIST2收集数据构建的层次化全身视觉运动策略学习框架。与之前分别关注上半身操作或下半身运动的工作不同，我们的视觉运动策略控制整个身体，实现了需要协调全身运动的复杂任务，如Kick-T。</p>
</blockquote>
<p>自主策略成功演示了以下任务（见图11）：</p>
<ul>
<li><strong>连续全身灵巧抓放</strong>：机器人自主从桌面抓取物体并放置到指定位置。</li>
<li><strong>连续踢T形盒至目标区域</strong>：机器人动态连续踢动一个T形盒子，将其导向目标区域。</li>
</ul>
<p><img src="https://arxiv.org/html/2511.02832v1/x8.png" alt="自主策略执行"></p>
<blockquote>
<p><strong>图11</strong>：现实世界中的闭环全身视觉运动策略执行。TWIST2支持有效且完整的人形全身数据收集，这进一步实现了多样的自主全身人形移动操作与腿部操作技能。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献有三点：1) 提出了首个兼具<strong>完整全身控制能力</strong>与<strong>高便携性</strong>的无动捕人形遥操作与数据收集系统TWIST2，并开源了全部系统与数据；2) 基于此数据管道，提出了一个<strong>层次化全身视觉运动策略学习框架</strong>，首次实现了基于自我中心视觉对完整人形身体的自主控制；3) <strong>系统性地演示</strong>了长时程遥操作技能、高效数据收集以及新的自主全身灵巧操作和动态腿部操作任务。</p>
<p>论文自身提到的局限性包括：机器人底层电机的鲁棒性（如过热问题）可能成为连续执行任务的瓶颈；当前的手部重定向将三指Dex31手简化为平行夹爪，未利用其全部自由度进行更灵巧的操作。</p>
<p>这项工作对后续研究的启示在于：它为人形机器人领域提供了一个可扩展、低成本的数据收集基准方案，有望推动大规模人形技能数据集的建立。其层次化策略框架（分离运动跟踪与任务策略）为训练复杂的全身视觉运动策略提供了可行路径。开源的系统、模型与数据将极大降低相关研究的入门门槛，促进社区发展。未来的工作可以探索更复杂的手部操作、多任务泛化以及更长期、更动态的全身协调技能。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人缺乏高效数据收集框架的问题，提出了TWIST2系统。该系统采用PICO4U VR实现实时全身运动捕捉，并设计了一个低成本2自由度机器人颈部以提供第一人称视觉，实现了无需动作捕捉的、便携的全身遥操作。实验表明，该系统能在15分钟内成功收集约100次演示，成功率接近100%。基于此数据训练的分层视觉运动策略，能够自主完成全身灵巧操作和动态踢球等任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2511.02832" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>