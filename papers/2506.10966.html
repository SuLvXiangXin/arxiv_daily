<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.10966" target="_blank" rel="noreferrer">2506.10966</a></span>
        <span>作者: Jiangmiao Pang Team</span>
        <span>日期: 2025-06-12</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人操作在现实世界中的泛化能力仍然是一个核心挑战。目前主流方法主要分为两类：一类是模块化操作系统，通过整合多模态大语言模型和视觉基础模型进行感知、推理和规划；另一类是端到端模仿学习策略，利用大规模机器人数据进行训练。然而，在现实环境中对这两类策略进行公平、系统的泛化能力评测面临高昂成本，且难以复现一致的场景。现有仿真平台如RLBench、CALVIN等往往在简化或受控环境中进行评估，牺牲了真实性和场景多样性；而RoboCasa等近期工作虽提升了真实感，但缺乏针对多样化指令和场景的标准化评估基准，评测仍主要依赖随机化布局。本文针对现有仿真平台在支持指令跟随策略泛化研究方面的不足，提出了一个专为评估通用机器人在多样化场景和指令下性能而设计的、真实的大规模桌面仿真平台GenManip。其核心思路是利用LLM驱动的任务导向场景图（ToSG）来自动合成大规模、多样化的任务，并构建一个经过人工修正的基准测试集GenManip-Bench，以系统评估策略的泛化能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>GenManip的整体框架是一个基于LLM驱动的、可控的自动化任务与场景生成流水线。其输入是海量带有视觉语言（VL）标注的3D资产，输出是布局合理的、可执行的仿真任务场景及对应的演示数据。</p>
<p><img src="https://arxiv.org/html/2506.10966v1/x1.png" alt="任务场景与资产"></p>
<blockquote>
<p><strong>图1</strong>：GenManip任务场景与资产概览。平台覆盖长时域规划、空间推理、常识推理和外观推理四类主要任务。使用任务导向场景图（ToSG）描述每个场景。包含一部分刚性（200/10K）和铰接（8/100）物体。右下表格展示了训练集和基准测试集的任务分布。</p>
</blockquote>
<p>核心模块与技术细节如下：</p>
<ol>
<li><strong>仿真平台与资产构建</strong>：平台基于NVIDIA Isaac Sim构建，利用其逼真渲染和并行数据收集能力。资产方面，从Objaverse等来源筛选并标注了10K个高质量刚性物体，并为每个资产使用GPT-4V生成了包括物体描述、物理属性（尺度、质量）和语义属性（类别、颜色、形状、材质）的结构化VL标注。此外，还包含了100个手动标注了物理约束的铰接物体（如笔记本电脑、垃圾桶）和100个室内HDRI背景以增强环境多样性。平台还预定义了一系列原始技能（如抓取、放置）用于生成演示。</li>
<li><strong>任务导向场景图（ToSG）与LLM驱动的场景合成</strong>：这是方法的核心创新。ToSG是一种为机器人操作场景设计的结构化表示，易于LLM理解和操作。一个ToSG包含：<strong>任务指令</strong>（文本描述）、<strong>场景图</strong>（节点表示物体及其状态，边表示物体间的空间关系，如左、右、上、内）和<strong>目标条件</strong>（需要达成的目标状态列表）。</li>
</ol>
<p><img src="https://arxiv.org/html/2506.10966v1/x2.png" alt="任务导向场景图生成流程"></p>
<blockquote>
<p><strong>图2</strong>：用于场景合成的任务导向场景图（ToSG）生成流程。使用GPT-4V对高质量资产进行标注。给定带标签的物体，GPT-4生成包含物体状态和关系的ToSG。ToSG指导布局构建并确保关系一致性。为清晰起见，目标物体与锚物体之间的箭头被省略。目标条件用于评估。</p>
</blockquote>
<p>其生成流水线为：首先根据VL标注随机采样初始种子物体和任务类型；然后使用GPT-4，结合详细的物体标注，合成任务场景并以ToSG格式输出。这确保了任务多样性（通过不同的物体选择）和空间关系一致性（通过结构化图表示）。<br>3.  <strong>基于ToSG的布局构建与数据收集</strong>：根据生成的ToSG，算法对物体进行拓扑排序（例如，桌子先于其上的物体放置），然后依次放置同一层级的物体，在满足场景图关系约束（如“靠近”关系需在特定距离阈值内）的同时解决碰撞问题。最后，利用场景布局和原始技能的特权信息，通过运动规划库为每个目标条件自动生成至少一条可行的演示轨迹，用于后续学习策略的训练。</p>
<p>与现有方法相比，GenManip的创新点在于：1) 提出了与LLM兼容的ToSG表示，实现了<strong>可控的、任务对齐的</strong>场景生成，而非纯粹随机布局；2) 构建了大规模、高质量、带丰富VL标注的3D资产库；3) 建立了从资产标注、任务生成、布局构建到数据收集的完整自动化流水线。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>本文在两个层面进行评估：模块化操作系统和端到端学习策略。使用的基准是<strong>GenManip-Bench</strong>，这是一个包含200个经过人工修正的、覆盖多样化指令和现实场景的测试集。实验平台为Isaac Sim。评估指标采用成功率（SR）和加权路径长度的成功率（SPL）。</p>
<p><strong>1. 模块化操作系统评估</strong>：<br>对比了两种代表性的基于提示的模块化方法：CoPA和MOKA（在GenManip中复现并适配）。评估了包括GPT-4o、GPT-4.5、Claude-3.7-Sonnet、Gemini-2.0-Flash和Qwen2.5-VL-72B在内的多种MLLM。</p>
<p><img src="https://arxiv.org/html/2506.10966v1/x4.png" alt="模块化操作系统框架"></p>
<blockquote>
<p><strong>图4</strong>：基于标记的视觉提示模块化操作系统框架。包含四个模块：可负担性提议生成器、基于网格的运动规划器、轨迹-动作链接器和长时域规划器。</p>
</blockquote>
<p>表2展示了详细结果。关键发现：</p>
<ul>
<li>使用GPT-4.5的CoPA在所有任务上取得了最佳整体SR（23.0%），显著优于其他模型组合。</li>
<li><strong>长时域任务是最具挑战性的类别</strong>，所有模型在该类任务上的平均SR仅为9.07%，暴露了提示模型在复杂任务分解和长程规划方面的局限性。</li>
<li>在需要理解物体外观和常识属性的任务上，模型表现相对更好。</li>
<li>SPL指标普遍远低于SR（例如CoPA的SPL约为SR的1/6），表明这些模块化系统在路径规划效率上存在不足。</li>
</ul>
<p><strong>2. 端到端学习策略评估</strong>：<br>评估了两种代表性策略：GR-1和ACT，并研究了数据缩放和泛化能力。</p>
<p><img src="https://arxiv.org/html/2506.10966v1/x5.png" alt="数据缩放效果消融研究"></p>
<blockquote>
<p><strong>图5</strong>：数据缩放效果的消融研究。展示了GR-1和ACT在不同训练数据量（100到1000条轨迹）和不同测试分布下的成功率。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2506.10966v1/x6.png" alt="端到端方法泛化能力消融研究"></p>
<blockquote>
<p><strong>图6</strong>：端到端方法泛化能力的消融研究。评估了在有限区域随机化、全桌面随机化、跨不同场景、未见指令和未见物体等多种设置下的性能。</p>
</blockquote>
<p>关键发现：</p>
<ul>
<li><strong>数据缩放对性能提升至关重要</strong>：如图5所示，无论是GR-1还是ACT，在大多数测试设置下，增加训练数据量（从100到1000条演示轨迹）都能显著提高成功率。</li>
<li><strong>泛化能力有限</strong>：如图6所示，当测试分布与训练分布差异增大时（如遇到<strong>未见过的物体</strong>或<strong>全新的场景</strong>），即使使用了1K数据，两种方法的性能都会急剧下降（例如在“Unseen objects (scenarios)”设置下，GR-1的SR从<del>80%降至</del>20%）。这表明现有端到端方法在指令跟随和场景泛化方面存在显著局限。</li>
</ul>
<p><strong>3. 消融实验</strong>：</p>
<ul>
<li><strong>模块化系统消融</strong>：表4显示，在运动规划模块中，“Point-to-Point”规划显著优于“Mark-based Grid”规划，表明GPT在需要同时处理连续时空序列和图像 grounding 时存在困难。在抓取提议生成中，简单的SoM提示优于更精细的Coarse-to-Fine SoM，因为后者与AnyGrasp的抓取姿态提议功能重叠，增加了错误累积风险。</li>
<li><strong>干扰物和任务长度影响</strong>：表3（右）表明，引入一个干扰物会使性能明显下降，但增加更多干扰物影响减弱，存在阈值效应。表3（左）显示，随着任务目标条件数量（长度）增加，SR和SPL急剧下降，错误主要源于任务分解阶段。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1) 提出了<strong>GenManip仿真平台</strong>，其核心是<strong>LLM驱动的任务导向场景图（ToSG）</strong>，实现了大规模、多样化、可控的桌面操作任务自动生成；2) 构建了<strong>GenManip-Bench基准测试</strong>，包含200个人工修正的真实场景，支持对指令跟随泛化能力的多维度系统评估；3) 利用该平台对模块化和端到端两类主流策略进行了<strong>系统性评测</strong>，揭示了它们各自的优势与局限：模块化系统（尤其是结合强大MLLM时）在零样本泛化上更有潜力，但长时域规划和效率是瓶颈；端到端方法严重依赖数据规模，且对分布外泛化能力不足。</p>
<p>论文提到的局限性包括：当前支持的技能类型（主要是抓放）还不够多样；长时域任务规划仍然是重大挑战。这项工作为社区提供了一个强大的评测工具，其发现强调了结合基础模型理解能力与数据驱动学习（即双系统框架）可能是迈向更通用、更鲁棒机器人操作系统的有前景方向。平台和基准的发布有望推动对策略泛化，特别是在复杂指令和多样化现实场景下泛化能力的深入研究。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人操作中策略泛化的核心挑战，提出GenManip模拟平台。该平台采用LLM驱动的任务导向场景图自动生成大规模多样化任务，并构建GenManip-Bench基准进行系统评估。实验比较模块化系统与端到端策略，发现增强基础模型的模块化系统在多样化场景中泛化更有效。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.10966" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>