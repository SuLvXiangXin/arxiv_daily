<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Green-VLA: Staged Vision-Language-Action Model for Generalist Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Green-VLA: Staged Vision-Language-Action Model for Generalist Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.00919" target="_blank" rel="noreferrer">2602.00919</a></span>
        <span>作者: Apanasevich, I., Artemyev, M., Babakyan, R., Fedotova, P., Grankin, D., Kupryashin, E., Misailidi, A., Nerus, D., Nutalapati, A., Sidorov, G., Efremov, I., Gerasyov, M., Pikurov, D., Senchenko, Y., Davidenko, S., Kulikov, D., Sultankin, M., Askarbek, K., Shamanin, O., Statovoy, D., Zalyaev, E., Zorin, I., Letkin, A., Rusakov, E., Silchenko, A., Vorobyov, V., Sobolnikov, S., Postnikov, A.</span>
        <span>日期: 2026/01/31</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型已成为具身AI领域有前景的基础范式，通过将机器人控制建立在多模态上下文和自然语言指令之上，旨在统一感知、推理和动作。π₀、Gemini Robotics、GR00T N1和AgiBot GO-1等工作展示了这一趋势，它们结合大规模数据聚合与统一架构，在操作、推理和评估基准上取得了显著进展。然而，仅靠数据规模无法解决实际部署的核心挑战：机器人数据集在观测、动作空间和采样频率上具有固有的异构性；数据质量参差不齐，存在抖动、模糊帧、执行不一致和场景多样性低等问题；主流的训练范式仍是行为克隆，它最小化动作预测误差，但这种方法会迅速饱和，且难以将策略与长视野目标和任务级奖励对齐，导致模型脆弱，跨机器人和环境的泛化能力差。此外，一些探索VLA模型中显式推理的工作，往往依赖自回归推理循环，导致显著的推理延迟，无法用于实时控制。</p>
<p>本文针对上述痛点，提出了强调质量对齐、动作统一和强化学习细化的新视角。核心思路是：设计一个分阶段的VLA训练流程，从网络规模的多模态数据中获取语义和物理常识，通过统一动作接口整合异构机器人数据，并最终利用强化学习对齐策略以超越行为克隆的局限性，从而实现高质量、可部署的通用机器人策略。</p>
<h2 id="方法详解">方法详解</h2>
<p>Green-VLA的整体框架是一个分阶段的训练流程，旨在逐步构建语义和物理先验，学习共享的具身关系，并为长视野执行对齐策略。其核心是一个用于机器人舰队的统一数据与控制栈。</p>
<p><img src="https://arxiv.org/html/2602.00919v1/images/hero/green_hero_background.png" alt="Green-VLA架构"></p>
<blockquote>
<p><strong>图1</strong>：Green-VLA架构。一个多模态视觉语言模型将指令、相机视图和本体感知编码为令牌，输入给流匹配动作专家。一个高级任务规划器将用户目标分解为子任务，查询VLA循环，并使用辅助信号来确保跨不同机器人的安全、忠实于指令的执行。</p>
</blockquote>
<p><strong>分阶段训练流程</strong>：训练分为五个渐进阶段：</p>
<ol>
<li><strong>L0: 基础VLM</strong>：作为起点的、已在大型图像/视频-文本数据上预训练的视觉语言模型。</li>
<li><strong>L1: 面向物理世界理解的网络预训练</strong>：利用网络规模的多模态数据（24M样本）建立通用推理和语义基础，学习物理、对象具身关系和任务结构等先验。</li>
<li><strong>R0: 通用机器人预训练</strong>：在超过3000小时的人形机器人和机械臂演示数据上进行预训练，学习跨不同机器人的广泛具身关系先验。</li>
<li><strong>R1: 具体机器人适配</strong>：通过监督微调，将通用预训练模型高效地适配到目标机器人。</li>
<li><strong>R2: RL对齐</strong>：将行为克隆先验与强化学习对齐相结合，利用任务奖励、失败恢复等信号改进长视野和接触丰富的操作性能。</li>
</ol>
<p><strong>核心模块与技术细节</strong>：</p>
<ul>
<li><strong>统一动作空间</strong>：设计了一个归一化的动作空间𝒜_u，通过频率插值/外推和显式的控制类型提示，使单一策略能够控制人形机器人、移动操作臂和固定基座机械臂。对于Green人形机器人，通过该接口控制上半身（头、躯干、双臂和灵巧手）的32个自由度。</li>
<li><strong>数据质量保证管道</strong>：一个可扩展的数据处理流程，用于确保数据质量而不仅仅是规模。它包括DataQA模块，使用轨迹平滑度J、图像清晰度S、视觉多样性D和状态方差σ²等指标对情节进行评分和过滤。此外，还应用了轨迹平滑以减少高频噪声，并使用平衡到目标的采样器w_i(α)来调整数据混合。</li>
<li><strong>联合预测与引导模块</strong>：一个联合预测+引导模块，通过预测精确的目标点（如像素坐标）来改进在视觉密集环境（如电商货架）中的精确对象定位。</li>
<li><strong>模型架构</strong>：基于统一的Transformer架构。一个视觉语言编码器融合RGB观测、本体感知状态和自然语言指令，并辅以指定主动效应器和动作参数化的机器人/控制类型提示c。在此基础上，一个流匹配动作专家预测𝒜_u中的统一动作块。架构通过高效注意力、轻量级头和减少的去噪步骤进行了实时部署优化。</li>
<li><strong>任务规划器模块</strong>：一个基于GigaChat的高级GigaVision VLM，负责解析用户输入，将高级目标（如“布置午餐餐桌”）分解为原子子任务序列（如“用左手拾取[物品]”），并生成结构化提示来调节Green-VLA。在执行过程中，它利用VLA预测的情节结束概率和反馈模块来决定子任务完成与否，并驱动重新规划循环。该规划器是预训练且冻结的，仅在推理时使用。</li>
</ul>
<p>与现有方法相比，Green-VLA的创新点具体体现在：1) 系统性的分阶段课程设计，明确划分了从通用语义理解到具体机器人强化学习对齐的路径；2) 强调数据质量而非单纯规模，并设计了统一动作接口以实现真正的跨机器人泛化；3) 在架构中集成了用于实时引导和状态监测的辅助模块。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：主要评估平台为Green人形机器人。使用了广泛的机器人数据集进行训练，包括开源的AgiBotWorld、DROID、Galaxea、Action_net、Fractal、Robomind、RDT、Bridge、BiPlay，以及内部收集的Green Humanoid数据集（48小时，通过镜像和时间反转增强至167小时）和ALOHA any_pick数据集。网络多模态数据包括RefSpatial、A-OKVQA、OpenSpaces等。</li>
<li><strong>基线方法</strong>：对比了其他先进的VLA模型，如π₀、EO-1、WALL-OSS、Gemini Robotics等。</li>
<li><strong>评估指标</strong>：成功率、平均链长（衡量长视野任务中连续成功执行子任务的平均数量）、时间效率等。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>Green-VLA在Green人形机器人上实现了最先进的性能，并在双手机器人基准测试中表现出色。经过完整的R2 RL对齐后，模型在长视野成功率、鲁棒性和效率方面均获得显著提升。例如，在内部评估中，RL对齐（R2）相较于仅使用行为克隆（R1）带来了显著的成功率增益。</p>
<p><img src="https://arxiv.org/html/2602.00919v1/x1.png" alt="机器人特定训练阶段"></p>
<blockquote>
<p><strong>图2</strong>：Green-VLA的机器人特定训练阶段（R0-R2）示意图，展示了其如何实现机器人适应、专业化、空间推理、任务泛化、灵巧操作和失败恢复。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.00919v1/x2.png" alt="L1阶段数据集混合"></p>
<blockquote>
<p><strong>图3</strong>：L1训练阶段使用的数据集混合情况。左图显示了各子数据集的样本数量分布，右图显示了跨类别的采样权重分配。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.00919v1/x3.png" alt="R0阶段数据集采样率"></p>
<blockquote>
<p><strong>图4</strong>：R0训练阶段使用的数据集采样率（左）和每个数据集的样本数（右），展示了数据的时间覆盖范围和相对重要性。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2602.00919v1/x4.png" alt="数据管道概览"></p>
<blockquote>
<p><strong>图5</strong>：机器人学习的数据管道概览，集成了遥操作、云端数据验证、开源数据集挖掘和模型训练，支持通过RL微调和真实机器人部署反馈进行迭代更新。</p>
</blockquote>
<p><strong>消融实验与组件贡献</strong>：<br>论文通过分阶段实验验证了每个阶段的贡献。R0通用机器人预训练阶段的结果匹配或超过了先前的预训练模型；经过R1具体机器人适配后，模型与其他VLA模型相比具有竞争力；而R2 RL对齐阶段带来了最大的性能增益，特别是在长视野成功率、恢复能力和精确任务跟随方面。这证明了统一化、数据管理和引导对于高质量VLA的重要性。数据质量保证管道通过过滤低质量数据，直接提高了样本效率和最终策略性能。联合预测与引导模块（JPM）有效提升了在密集场景中的精确操作能力。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>高质量数据管道</strong>：提出了一个包含DataQA、时间对齐过滤和轨迹平滑的数据质量与时间对齐模块，用于处理多样化的机器人数据集，确保训练数据的质量。</li>
<li><strong>分阶段VLA训练方案</strong>：设计了一个清晰的、从通用多模态预训练到真实世界机器人部署的路径，通过L0→L1→R0→R1→R2的阶段性课程，有效结合了网络先验、机器人数据、机器人适配和RL对齐。</li>
<li><strong>可部署的通用机器人策略</strong>：在Green人形机器人上实现了包含上半身统一控制的部署就绪设计，并验证了同一策略能够以零样本方式泛化到新的机器人平台，展示了其在双手机器人基准上的最先进性能。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到的未来工作方向指出了当前框架的潜在扩展空间，包括：融入多语言指令跟随以提高全球部署的包容性和数据效率；添加轻量级推理模块以进行任务分解，同时保持低延迟控制；集成具身记忆和轨迹回放以进一步提升在长视野家庭或工业任务中的性能。</p>
<p><strong>启示</strong>：Green-VLA的工作表明，构建通用机器人模型不仅需要扩大数据规模，更需要系统性的训练架构设计、严格的数据质量控制和超越行为克隆的优化目标。其分阶段方案为后续研究提供了一个可借鉴的、从语义理解到物理控制的渐进式学习范式。统一动作接口的设计为实现跨机器人平台的真正泛化提供了关键思路。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人视觉-语言-动作（VLA）模型在现实部署中面临的数据异质、质量不佳及行为克隆方法在长时程任务上泛化能力有限的核心问题，提出了分阶段训练的Green-VLA框架。其关键技术包括五阶段课程（L0至R2），构建统一的数据控制栈、可扩展数据处理管道（DataQA、时间对齐）及具身感知动作接口。实验表明，该框架在双手机器人系统上实现了零样本泛化与最先进性能，通过RL策略对齐显著提升了任务成功率、鲁棒性及长时程执行效率。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.00919" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>