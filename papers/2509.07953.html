<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and Correction - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and Correction</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.07953" target="_blank" rel="noreferrer">2509.07953</a></span>
        <span>作者: Aviral Kumar Team</span>
        <span>日期: 2025-09-09</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人模仿学习的主流范式是在大量人类遥操作演示数据上训练表达能力强的策略模型。然而，即使有数千条专家演示，策略在接触密集、涉及形变物体以及长时程任务上的性能仍远未达到完美执行。这源于现有基于人类遥操作的“专家”数据收集程序效率低下：演示数据偏向于干净、成功的轨迹，缺乏处理长时程任务中因随机性导致的复合错误所需的行为。因此，仅通过模仿学习的策略难以泛化到现实世界的随机性和动态变化，且随着数据增加，性能提升出现瓶颈。</p>
<p>本文针对模仿学习中数据分布的根本性局限，提出了一种新的训练范式。核心思路是在基于干净遥操作数据的模仿学习“预训练”之后，引入一个新的人类在环训练阶段。该阶段通过在策略执行出现偏差时进行人工干预，收集并利用包含“恢复”和“纠正”行为的轨迹数据来微调策略，从而赋予策略重试和适应的能力，以提升长时程任务的效率和鲁棒性。</p>
<h2 id="方法详解">方法详解</h2>
<p>RaC 的整体流程分为两个主要阶段：1）使用初始预算收集一轮成功的全演示数据，并训练一个初始策略；2）进行多轮迭代的人类在环数据收集与策略微调。在每一轮中，人类操作员在运行当前策略出现失败迹象时进行干预，干预数据（恢复段和纠正段）与已有数据聚合，用于训练新一代策略。</p>
<p><img src="https://arxiv.org/html/2509.07953v1/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：RaC的核心概念。左侧为仅收集纠正数据的典型干预方法（HG-DAgger风格），右侧为RaC方法，收集恢复段（将机器人带回熟悉状态）后接纠正段。这“增密”了对熟悉状态的覆盖，并教会机器人从更广泛的初始状态恢复。</p>
</blockquote>
<p>核心模块是两种由人类干预产生的数据段：</p>
<ul>
<li><strong>恢复段</strong>：当策略在状态 $s_t$ 开始偏离时，人类执行一系列动作 $(a_{t+1}^h, ..., a_{t+k}^h)$，使机器人达到的状态 $s_{t+k}^h$ 位于先前人类全演示数据 $\mathcal{D}^{\text{full}}[0:t]$ 访问过的状态分布内。其作用是将机器人重置到一个熟悉的、分布内的状态，为重新尝试任务创造条件。</li>
<li><strong>纠正段</strong>：紧接着恢复段，人类执行一系列动作，使机器人达到的状态位于全演示数据中该时间点之后 $\mathcal{D}^{\text{full}}[t+1:H]$ 的状态分布内。其作用是展示如何从恢复后的状态推进并完成当前子任务。</li>
</ul>
<p>为了在数据收集中规模化地获得这两种行为，RaC 制定了两个标准化规则：</p>
<ol>
<li><strong>恢复后纠正</strong>：每次人类接管干预都必须结构化为先执行恢复行为，再执行纠正行为。这确保了策略既能学习如何重置，也能学习如何推进。</li>
<li><strong>干预后终止</strong>：一次干预结束后，立即终止整个回合。这避免了在后续子任务中收集由学习策略和人类专家混合诱导的状态分布数据，从而将有限的数据收集预算更集中地用于改进早期容易出错的子任务。</li>
</ol>
<p>与现有方法（如 HG-DAgger）相比，RaC 的核心创新在于明确鼓励并标准化收集“恢复”行为。传统方法将人类干预视为需要模仿的最优专家解决方案，而 RaC 则认为恢复行为（其本身对于完成子任务并非最优，甚至可能撤销进度）对于提升数据缩放效率和最终性能至关重要。这利用了长时程任务中普遍存在的“验证-生成差距”：学习将机器人重置到熟悉的过去状态，通常比学习精确执行一个子任务更容易、更样本高效。</p>
<p><img src="https://arxiv.org/html/2509.07953v1/x4.png" alt="共享自治接口"></p>
<blockquote>
<p><strong>图4</strong>：用于RaC共享自治的VR手柄接口。采用“离合器”设计，按下侧键时人类接管控制，释放时机器人跟随学习策略，实现平滑交接。</p>
</blockquote>
<p>为方便干预，论文设计了一个轻量级的共享自治接口，使用Oculus Quest VR控制器和“离合器”机制，允许操作员无缝接管控制。同时，通过叠加显示机器人末端执行器在初始全演示中的访问频率热图，为操作员提供视觉引导，帮助其将机器人恢复至分布内区域。</p>
<p><img src="https://arxiv.org/html/2509.07953v1/x6.png" alt="策略架构"></p>
<blockquote>
<p><strong>图6</strong>：策略架构。使用多模态扩散Transformer（mm-DiT）通过流匹配目标进行训练。</p>
</blockquote>
<p>策略架构方面，采用一个3亿参数的多模态扩散Transformer，通过流匹配目标进行训练。策略以多视角RGB图像和机器人状态为观测，预测未来1秒（60步）的动作块，以拟合来自全演示、策略自身成功回合以及人类干预段（含恢复和纠正）的混合数据。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在三个具有挑战性的真实世界双手机器人长时程任务上进行：<strong>衬衫悬挂</strong>、<strong>密封容器盖密封</strong>、<strong>外卖盒打包</strong>，以及一个<strong>模拟装配任务</strong>。对比的基线方法包括：<strong>批量全演示收集</strong>（一次性收集所有专家数据）和 <strong>HG-DAgger风格的人类在环收集</strong>（仅收集纠正段）。实验平台为配备软体夹爪的双xArm-7机械臂系统，使用顶部和腕部摄像头。</p>
<p><img src="https://arxiv.org/html/2509.07953v1/x7.png" alt="实验结果"></p>
<blockquote>
<p><strong>图7</strong>：三个真实世界长时程任务的成功率与数据收集时间对比。RaC（蓝色）在相同数据量下性能远超批量全演示（橙色）和HG-DAgger（绿色），数据效率高出约一个数量级。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ul>
<li><strong>性能与数据效率</strong>：在所有三个真实任务中，RaC 均取得了最高的成功率，并且数据效率显著优于基线。例如，在衬衫悬挂任务中，RaC 仅使用约1.5小时的数据收集时间即可达到超过80%的成功率，而批量全演示需要约15小时的数据才能达到相似性能，数据效率提升约<strong>10倍</strong>。</li>
<li><strong>消融实验</strong>：在衬衫悬挂任务上的消融研究表明，完整 RaC（恢复+纠正）性能最佳。仅使用纠正段（HG-DAgger风格）或仅使用恢复段的数据组成，其性能均低于完整 RaC，验证了两种行为组合的必要性。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.07953v1/x8.png" alt="消融实验"></p>
<blockquote>
<p><strong>图8</strong>：消融研究。在衬衫悬挂任务上比较RaC与仅含纠正段（HG-DAgger风格）和仅含恢复段的数据组成。完整RaC（恢复+纠正）性能最佳，验证了两种行为组合的必要性。</p>
</blockquote>
<ul>
<li><strong>测试时扩展</strong>：一个关键发现是，训练后的 RaC 策略表现出<strong>测试时扩展</strong>现象：在部署时，策略执行的恢复操作（重试次数）越多，整体任务成功率越高，且呈现线性增长趋势。这类似于大语言模型中长思维链带来的性能提升。</li>
</ul>
<p><img src="https://arxiv.org/html/2509.07953v1/x10.png" alt="测试时扩展"></p>
<blockquote>
<p><strong>图10</strong>：测试时扩展现象。训练后的RaC策略在测试时执行更多恢复操作（重试次数）时，任务成功率线性提升，类似于大语言模型中的思维链扩展。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献包括：</p>
<ol>
<li>提出了 <strong>RaC 框架</strong>，通过标准化人类在环干预协议，明确收集“恢复”和“纠正”行为，并将其作为可学习的技能进行扩展，显著提升了长时程模仿学习的数据效率和策略鲁棒性。</li>
<li>在三个复杂的真实世界双手机器人任务上验证了 RaC 的有效性，相比传统方法，在达到相同性能时可减少约一个数量级的数据收集成本。</li>
<li>首次在机器人策略中展示了<strong>测试时扩展</strong>现象，即策略性能随着测试时恢复重试次数的增加而线性提升，为机器人学习开辟了新的性能缩放维度。</li>
</ol>
<p>论文提到的局限性在于其依赖于人类在环的数据收集，虽然效率更高，但仍需要人工参与。对于后续研究的启示包括：探索更自动化的恢复行为合成方法；将恢复-纠正的范式应用于模仿学习之外的其他机器人学习框架（如强化学习）；进一步研究机器人策略中测试时扩展现象的理论基础和优化方法。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对模仿学习在长时程机器人任务中性能瓶颈和数据效率低的问题，提出RaC方法。其核心是在预训练后引入人在环中的恢复与纠正训练新阶段：当策略执行即将失败时，人工介入，先回退机器人至熟悉状态，再提供纠正示范。训练此类数据使策略学会重试与适应行为。在真实世界悬挂衬衫、密封容器、打包餐盒及模拟装配任务上，RaC仅用十分之一的数据量和时间即超越此前最优方法，且策略性能与恢复操作次数呈线性增长。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.07953" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>