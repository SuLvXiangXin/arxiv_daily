<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.24956" target="_blank" rel="noreferrer">2509.24956</a></span>
        <span>作者: Abhinav Valada Team</span>
        <span>日期: 2025-09-29</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前机器人模仿学习领域，生成式方法（如扩散模型和流匹配）因其强大的表达能力和对多模态行为的表征能力而被广泛应用。然而，这些方法的一个主要局限是样本效率低下，通常需要上百条演示数据才能获得良好性能。相比之下，概率式策略（如高斯混合模型GMM、MiDiGaP）通过将学习过程结构化为多个以物体为中心的“流”，能够仅用少量演示（如5条）实现泛化。现有的物体中心生成式策略（如ACPL）虽然提升了样本效率，但仍局限于单流学习，且ACPL采用的“定向物体坐标系”会丢弃关键的物体方向信息，可能影响策略性能。因此，本文针对生成式策略样本效率低、现有物体中心方法未能充分利用多流学习优势的痛点，提出了首个用于生成式策略学习的多流物体中心框架MSG。其核心思路是：独立训练多个物体中心的生成式策略，并在推理时通过组合这些策略来形成一个联合模型，从而以极少的演示数据实现高效泛化。</p>
<h2 id="方法详解">方法详解</h2>
<p>MSG的整体流程分为两个阶段：1）<strong>训练阶段</strong>：为任务中每个相关物体（或坐标系）独立训练一个生成式策略，每个策略学习在该物体局部坐标系下的末端执行器轨迹分布。2）<strong>推理阶段</strong>：将来自同一先验的样本同时通过所有局部流场，并在世界坐标系中组合这些流的预测，以采样得到最终的动作。</p>
<p><img src="https://arxiv.org/html/2509.24956v1/figures/eyecatcher_fixedborder.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：多流生成式策略（MSG）示意图。MSG从极少演示（如5条）中学习多个物体中心模型，并在推理时将它们组合起来，从而实现样本高效泛化。</p>
</blockquote>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>多流策略学习</strong>：给定包含全局末端执行器位姿 (\boldsymbol{\xi}<em>{\mathrm{ee}}) 和多个坐标系位姿 ({\boldsymbol{\xi}</em>{f}}<em>{f=1}^{F}) 的数据集，首先根据公式(4)将全局位姿变换到每个局部坐标系 (f) 中，得到局部位姿 (\boldsymbol{\xi}</em>{\mathrm{ee}}^{(f)})。然后，为每个坐标系独立训练一个生成式策略 (\boldsymbol{v}<em>{f})，以建模局部密度 (p(\boldsymbol{\xi}</em>{\mathrm{ee}}^{(f)}))。本文主要使用黎曼流匹配进行训练，但方法通用，也适用于扩散模型等。</p>
</li>
<li><p><strong>推理时模型组合</strong>：这是MSG的核心创新。由于生成式策略隐含地表征分布，无法直接计算分布的乘积。MSG探索了两种基于单粒子的组合策略：</p>
<ul>
<li><strong>集成组合</strong>：如图3(a)所示，从先验 (p_0) 中采样一个噪声样本 (\boldsymbol{z}_0)，分别通过每个局部流 (\boldsymbol{v}<em>f) 独立积分，得到一组局部预测 ({\hat{\boldsymbol{\xi}}</em>{\mathrm{ee}}^{(f)}})。将这些预测变换回世界坐标系后，进行加权平均得到最终动作。这种方法适用于近似高斯的单峰分布。</li>
<li><strong>流组合</strong>：如图3(b)所示，将同一个噪声样本 (\boldsymbol{z}<em>0) 变换到每个局部坐标系，得到 ({\boldsymbol{z}</em>{0}^{(f)}})。在流匹配的每个积分步 (t)，计算每个局部流在当前状态下的向量 (\dot{\boldsymbol{z}}_t^{(f)} = \boldsymbol{v}_f(\boldsymbol{z}_t^{(f)}))，将其变换回世界坐标系后加权求和，用这个联合流场来更新全局状态 (\boldsymbol{z}_t)。这种方法能更好地引导多流收敛到同一模式，尤其适用于多模态分布。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.24956v1/figures/gp_world.png" alt="组合策略对比"></p>
<blockquote>
<p><strong>图2</strong>：多流高斯策略在OpenMicrowave任务中的示例。初始末端执行器坐标系下的局部模型在轨迹开始阶段方差小（更确定），而微波门坐标系下的模型在轨迹后期更确定。组合后的模型在整个轨迹上都具有高精度。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.24956v1/x1.png" alt="组合策略示意图"></p>
<blockquote>
<p><strong>图3</strong>：两种组合策略示意图。(a) 集成组合：先独立集成各个流，再组合其预测。(b) 流组合：在每一步联合集成流场，组合其预测的向量。</p>
</blockquote>
<p><strong>先验调整</strong>：为了确保流组合中采样对齐的有效性并避免分布偏移，论文没有使用标准高斯先验，而是构建了以当前局部末端执行器位姿为中心的高斯先验，或基于演示数据中物体位姿分布构建的混合高斯先验。</p>
<p><strong>加权策略</strong>：组合时需要为每个流分配权重 (\boldsymbol{w}_f)。论文提出了三种策略：</p>
<ol>
<li><strong>基于进度的调度</strong>：适用于双流任务，根据策略预测的技能进度 (p_t) 按固定规则（如常数、线性、指数）分配权重。需要先验知识。</li>
<li><strong>基于方差的加权</strong>：训练时，让每个局部模型额外预测其自身对数方差 (\boldsymbol{\psi}<em>{\boldsymbol{\xi}</em>{\mathrm{ee}}^{(f)}})（通过离散时间高斯过程监督）。推理时，权重与指数化方差的倒数成正比 (\boldsymbol{w}<em>{f} \propto \exp(-\boldsymbol{\psi}</em>{\boldsymbol{\xi}_{\mathrm{ee},f}}))。这是数据驱动的，可适用于任意数量的流。</li>
<li><strong>并行采样</strong>：推理时从先验中采样 (K) 个粒子并行积分，根据粒子集估计的方差来分配权重 (\boldsymbol{w}<em>{f}^{-1} \propto \boldsymbol{\sigma}^{2}</em>{\boldsymbol{\xi}_{\mathrm{ee}}^{(f)}})。计算成本更高，但无需训练方差预测器，且天然支持多模态。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>基准/数据集</strong>：在模拟环境（RLBench的8个任务：OpenDrawer, OpenMicrowave等）和真实机器人（4个任务：PickAndPlace, PourDrink等）上进行评估。</li>
<li><strong>实验平台</strong>：模拟实验使用RLBench，真实实验使用Franka Emika Panda机器人。</li>
<li><strong>对比方法</strong>：<ul>
<li><strong>生成式基线</strong>：Riemannian Flow Matching (RFM)。</li>
<li><strong>物体中心基线</strong>：Affordance-Centric Policy Learning (ACPL)。</li>
<li><strong>概率式基线</strong>：MiDiGaP（作为性能上界参考）。</li>
<li><strong>MSG变体</strong>：测试了不同的组合策略（集成、流组合）和加权策略（进度调度、方差加权2D/6D、并行采样）。</li>
</ul>
</li>
</ul>
<p><img src="https://arxiv.org/html/2509.24956v1/x2.png" alt="模拟与真实任务"></p>
<blockquote>
<p><strong>图4</strong>：左：RLBench模拟任务。右：真实世界任务。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>样本效率与性能</strong>：在仅使用5条演示数据时，MSG（流组合+方差加权）在模拟任务上的平均成功率达到 **89.1%**，相比单流RFM（0.0%）提升了89.1个百分点，相比ACPL（47.1%）提升了42个百分点。这相当于将演示需求减少了 **95%**，同时训练时间也同比减少。即使在数据量增加到50条时，MSG（89.7%）仍显著优于RFM（63.8%）和ACPL（73.8%）。</li>
<li><strong>组合与加权策略消融</strong>：<ul>
<li>组合策略：在多数任务上，<strong>流组合</strong>优于集成组合。</li>
<li>加权策略：数据驱动的<strong>方差加权</strong>（尤其是6D版本）普遍优于需要手工设计的进度调度。并行采样性能与方差加权相当，但计算成本更高。</li>
<li><strong>图5-7</strong>展示了不同加权策略在示例任务上的权重变化曲线，显示了方差加权如何根据各流的不确定性自适应调整权重。</li>
</ul>
</li>
</ol>
<p><img src="https://arxiv.org/html/2509.24956v1/figures/progress_schedules.png" alt="进度调度"></p>
<blockquote>
<p><strong>图5</strong>：双流任务中基于技能进度的四种权重调度函数。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.24956v1/figures/variance_train_schedules.png" alt="方差加权训练曲线"></p>
<blockquote>
<p><strong>图6</strong>：方差加权策略（6D）在训练过程中学习到的权重变化示例，展示了数据驱动的自适应能力。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2509.24956v1/figures/variance_infer_schedules.png" alt="方差加权推理曲线"></p>
<blockquote>
<p><strong>图7</strong>：方差加权策略（6D）在推理时的权重变化示例。</p>
</blockquote>
<ol start="3">
<li><strong>数据缩放分析</strong>：如<strong>图8</strong>所示，对于PlaceCups任务，即使在演示数据从5条增加到100条的过程中，MSG（流组合+方差加权）的性能始终优于或匹配其他方法，证明了其优势不依赖于极小数据量。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.24956v1/figures/data_scaling_placecups.png" alt="数据缩放曲线"></p>
<blockquote>
<p><strong>图8</strong>：PlaceCups任务上，不同方法随演示数据量增加的成功率变化。MSG在几乎所有数据规模下都保持领先。</p>
</blockquote>
<ol start="4">
<li><strong>零样本实例迁移</strong>：通过结合DINO关键点等感知模块，MSG能够实现对新物体实例的零样本泛化。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>MSG</strong>，首个用于生成式策略学习的<strong>多流物体中心框架</strong>，通过在推理时组合多个独立训练的物体中心策略，大幅提升了样本效率和泛化能力。</li>
<li>设计并系统评估了一套<strong>模型组合策略</strong>（集成 vs. 流组合）和<strong>加权策略</strong>（进度、方差、并行采样），为不同计算成本与性能需求的场景提供了实用选择。</li>
<li>通过大量的模拟与真实机器人实验证实，MSG仅需<strong>5条演示</strong>即可学习高性能策略，性能相比单流方法提升<strong>89%<strong>，并实现了</strong>零样本物体实例迁移</strong>。</li>
</ol>
<p><strong>局限性</strong>：</p>
<ul>
<li>对于多模态分布，集成组合方法可能失效，因为它缺乏引导所有流选择同一模式的机制。</li>
<li>流组合方法在理论上的严格性依赖于向量场是梯度式的，尽管实验表明非梯度场在实践中也表现良好。</li>
</ul>
<p><strong>对后续研究的启示</strong>：</p>
<ul>
<li>MSG的<strong>推理时组合、模型无关</strong>设计使其能灵活兼容未来更先进的生成式策略训练方法。</li>
<li>所探索的多种组合与加权策略为生成式模型的组合这一开放问题提供了实证参考。</li>
<li>框架与关键点/位姿估计等感知模块的结合，展示了<strong>感知-动作</strong>紧密集成在实现强泛化能力方面的潜力。</li>
</ul>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对生成式机器人策略样本效率低的问题，提出MSG框架。该框架在推理时组合多个预训练的物体中心生成策略，通过从共享先验采样粒子并同步通过各局部流场传播，结合基于精度的加权策略进行信息融合。实验表明，MSG仅需5次演示即可学习高质量策略，相比基线减少95%演示需求，并将策略性能提升89%。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.24956" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>