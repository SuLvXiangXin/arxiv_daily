<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.01661" target="_blank" rel="noreferrer">2510.01661</a></span>
        <span>作者: Nadia Figueroa Team</span>
        <span>日期: 2025-10-02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>长视野、多步骤的机器人操作在动态环境中仍然是一个挑战。当前主流方法主要分为两类，各有其关键局限性：模仿学习（IL）方法具有反应性，但缺乏组合泛化能力，因为其单体策略在场景变化时无法决定复用哪个技能；经典的任务与运动规划（TAMP）方法提供了组合性，但其规划延迟（通常需要数十到数百秒）过高，阻碍了实时故障恢复。本文针对“如何实现兼具组合泛化与实时恢复能力的操作”这一具体痛点，提出了一个统一的符号与技能协同发明框架。核心思路是：从无标签、未分段的演示数据中联合学习谓词、算子和技能，在线运行时通过符号规划器组合技能以实现符号目标，并能在运动和符号两个层面进行实时恢复。</p>
<h2 id="方法详解">方法详解</h2>
<p>SymSkill框架包含离线的符号/技能协同发明和在线执行与恢复两个阶段。</p>
<p><img src="https://arxiv.org/html/2510.01661v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：SymSkill的离线（上半部分）与在线（下半部分）流程。紫色部分（V-A）描述演示分割与参考帧选择；橙色部分（V-B）描述每个片段的谓词学习；绿色部分（V-C）描述用于在线规划的算子学习；蓝色部分（V-D）描述每个算子的技能学习；黄色部分（V-E）描述SymSkill的在线运行方式。</p>
</blockquote>
<p><strong>整体流程</strong>：离线阶段，输入是未分段、未标记的机器人演示轨迹集合 𝒟。流程首先对每条轨迹进行分割并选择参考坐标系（V-A），然后基于相对位姿聚类发明谓词（V-B），接着跟踪谓词状态变化推导出符号算子（V-C），最后为每个算子学习一个基于动态系统（DS）的运动技能（V-D）。在线阶段，给定一个由学习到的谓词指定的符号目标，系统使用符号规划器（如FastDownward）生成一个算子序列（即技能执行计划），并交由DS控制器执行，同时进行实时监控与恢复。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><p><strong>演示分割与参考帧选择（V-A）</strong>：首先根据末端执行器和物体速度的变化点，将每条演示轨迹分割为“预运动”（仅末端执行器运动）和“运动”（末端执行器与一个物体同时运动）片段。对于预运动片段，将运动物体 <code>o_int</code> 的坐标系作为参考帧。对于运动片段，为了确定有语义意义的参考物体 <code>o_ref</code>，该方法查询视觉语言模型（VLM，如Gemini-2.5-Pro），向VLM提供运动片段中的多帧图像，并要求其从场景物体列表中选出参考物体。这一步骤避免了耗时的优化搜索。</p>
<p><img src="https://arxiv.org/html/2510.01661v1/x2.png" alt="VLM提示示例"></p>
<blockquote>
<p><strong>图3</strong>：用于真实世界“从玩耍中学习”实验的VLM提示流程。首先使用初始图像获取视野内所有物体的文本描述。然后，将每个运动片段中四张等间距的图像连同所需的枚举对象输出格式一并提供给Gemini。返回的文本再映射回相应的物体名称。</p>
</blockquote>
</li>
<li><p><strong>相对位姿谓词学习（V-B）</strong>：谓词被定义为物体间或末端执行器与物体间的相对位姿分类器。对于预运动片段，谓词 <code>ψ_ee^{o_int}</code> 基于末端执行器相对于 <code>o_int</code> 的位姿分布学习。对于运动片段，谓词 <code>ψ_{o_int}^{o_ref}</code> 基于运动物体 <code>o_int</code> 相对于参考物体 <code>o_ref</code> 的位姿分布学习。具体方法是，收集演示中对应片段的所有相对位姿，分别对位置和方向分量拟合高斯分布，并计算新位姿到这些分布的Mahalanobis距离。若距离低于阈值，则谓词为真。这些高斯分布不仅定义了谓词，还可用作在线恢复时的目标位姿采样器。</p>
</li>
<li><p><strong>算子学习（V-C）</strong>：使用学习到的谓词库重新评估所有演示轨迹，在每个分割边界处生成抽象符号状态序列。通过识别具有相同效果（增加和删除的谓词集合）的重复状态转换组，自动归纳出符号算子。每个算子 <code>α</code> 包含参数、前提条件、效果、维持条件和底层技能。前提条件是转换前所有状态的谓词交集，效果是转换后新增和删除谓词的交集，维持条件是在整个转换区间内持续为真的谓词交集。</p>
</li>
<li><p><strong>SE(3)技能学习（V-D）</strong>：为每个算子学习一个稳定的运动技能，其策略 <code>f</code> 采用SE(3) LPV-DS形式，包含位置和方向两个动态系统。该技能在相应参考帧（预运动在 <code>o_int</code> 帧，运动在 <code>o_ref</code> 帧）中学习，仅需少量演示数据即可快速拟合。学习过程通过求解一个保证全局渐近稳定的半定规划问题来完成。技能输出由被动的任务空间阻抗控制器跟踪，确保稳定性和抗干扰能力。</p>
<p><img src="https://arxiv.org/html/2510.01661v1/plots/ds_policy5.png" alt="DS策略可视化"></p>
<blockquote>
<p><strong>图4</strong>：Tab. III中Op3算子的演示轨迹与SE(3) LPV-DS策略 rollout 可视化。左图显示了将“物品”放入锅中多条轨迹的多模态数据，由4个不同颜色的高斯分量捕获。右图显示了从相同初始条件开始的策略重构结果，策略在锅坐标系中的位姿吸引子用坐标轴标出。</p>
</blockquote>
</li>
</ol>
<p><strong>创新点</strong>：与现有方法（如NSIL、LAMP）相比，SymSkill的主要创新体现在：1）<strong>轻量级VLM引导</strong>：利用VLM仅进行参考物体识别，避免了复杂的谓词提议与优化搜索，使学习过程更高效、语义更明确。2）<strong>实时规划与恢复</strong>：得益于高效的符号规划和快速的DS技能执行，系统能在毫秒级进行重规划，实现符号层和运动层的实时故障恢复。3）<strong>端到端无监督</strong>：从原始、未分段、未标记的演示中直接联合发明符号和技能，所需演示数据量极少（每个任务仅需1-10条）。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在RoboCasa仿真环境和真实的Franka机器人上进行验证。使用了12个短视野任务（如开门、放置物品）的演示数据。对比的基线方法包括NSIL、LAMP和NOD-TAMP。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>短视野任务成功率</strong>：SymSkill在RoboCasa中学习24个可重用技能，在12个单步任务上达到85%的成功率。</p>
</li>
<li><p><strong>长视野组合任务</strong>：无需额外数据，SymSkill能够将这些技能组合成需要最多6次技能重组的复合任务（如“打开微波炉门，将杯子放入微波炉，关闭门”），并成功执行。</p>
</li>
<li><p><strong>真实机器人验证</strong>：从5分钟的未分段“玩耍”数据中学习后，SymSkill仅通过指定目标谓词，即可让真实机器人执行多个任务。</p>
</li>
<li><p><strong>规划时间</strong>：SymSkill的符号规划时间小于100毫秒，显著快于LAMP（&gt;50秒）和NOD-TAMP（&gt;50秒）等基于运动规划的方法。</p>
<p><img src="https://arxiv.org/html/2510.01661v1/plots/dp.png" alt="仿真结果"></p>
<blockquote>
<p><strong>图7</strong>：在RoboCasa仿真中的任务成功率对比。SymSkill在短视野任务上达到85%成功率，并能成功组合成长视野任务。NSIL由于谓词学习不准确导致失败，LAMP则因规划延迟无法完成长任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.01661v1/plots/real_world_short.jpg" alt="真实世界演示"></p>
<blockquote>
<p><strong>图6</strong>：真实世界实验示例。机器人从5分钟的玩耍数据中学习后，能够执行“将物品放入锅中”等任务。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2510.01661v1/plots/demo_timelapse_short.jpg" alt="演示数据"></p>
<blockquote>
<p><strong>图5</strong>：用于学习的未分段、未标记的演示数据示例。</p>
</blockquote>
</li>
</ol>
<p><strong>消融实验与组件贡献</strong>：论文通过对比指出，VLM引导的参考帧选择对于获得有语义意义的谓词至关重要（避免了NSIL方法的失败）。此外，使用DS技能而非运动规划是实现实时恢复的关键。被动的DS控制器则确保了执行过程中的稳定性和安全性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：1）提出了一个从无标签、未分段演示中联合发现和学习符号与面向目标的动态系统技能的框架。2）实现了在线任务和运动层面的反应式规划与故障恢复。3）提供了开源的实现，支持在RoboCasa中进行开箱即用的机器人学习。</p>
<p><strong>局限性</strong>：论文提到，当前方法假设完全可观测的确定性领域，并依赖外部感知模块提供所有物体的完整状态。对于非刚性接触或更复杂的多物体交互，其分割假设可能面临挑战。</p>
<p><strong>后续研究启示</strong>：SymSkill展示了将轻量级VLM感知与高效符号规划、稳定控制相结合的有效性。未来工作可探索在部分可观测环境中的应用，或扩展其处理更复杂交互模式（如双手操作、非刚性物体）的能力。该方法为构建数据高效、可组合且反应敏捷的机器人操作系统提供了一个有前景的路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出SymSkill框架，解决动态环境中长时程机器人操作的数据效率与实时性问题。核心结合模仿学习（IL）的响应能力与任务运动规划（TAMP）的组合泛化能力，通过**谓词与技能协同发明**技术，从无标注、未分割的演示数据中自动学习符号谓词、操作符和运动技能（如SE(3) LPV-DS技能拟合）。执行时使用符号规划器实时组合技能并恢复故障。实验表明：在RoboCasa仿真中，12个单步任务成功率85%，并能组合成最多需6次技能重组的多步计划；真实Franka机器人仅用5分钟无标注数据即可通过目标指令执行多种任务。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.01661" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>