<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2602.03310" target="_blank" rel="noreferrer">2602.03310</a></span>
        <span>作者: Liu, Songming, Li, Bangguo, Ma, Kai, Wu, Lingxuan, Tan, Hengkai, Ouyang, Xiao, Su, Hang, Zhu, Jun</span>
        <span>日期: 2026/02/03</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型被视为实现通用具身智能的有前景的范式，尤其擅长处理传统控制方法难以建模的复杂操作任务。然而，现有的VLA模型面临三大挑战：数据稀缺、架构效率低下，以及无法在不同硬件平台间泛化。具体而言，主流数据收集方法（如遥操作）成本高昂且多样性不足；在模型架构方面，基于动作离散化的方法存在量化误差和自回归推理效率低下的问题，而基于扩散模型的方法则训练收敛慢，且其连续概率分布与预训练VLM中的离散知识形式存在不匹配。最关键的是，由于机器人平台间的物理特性差异，在一个平台上训练的模型难以零样本迁移到新平台上，这严重阻碍了其实用性。</p>
<p>本文针对机器人VLA模型难以零样本跨硬件平台部署的核心痛点，提出通过构建超大规模、设备无关的UMI数据集，并设计一种新颖的三阶段训练方法，来探索数据规模扩展的极限，以实现对未见过的物体、场景、指令乃至机器人平台的组合式零样本泛化。核心思路是：重新设计并大规模部署增强版UMI硬件，收集超万小时的多样化演示数据；然后基于一个7B参数的VLM，通过残差向量量化、流匹配和蒸馏的三阶段训练策略，高效学习连续控制策略，最终实现开放词汇任务下的跨平台零样本部署。</p>
<h2 id="方法详解">方法详解</h2>
<p>RDT2的整体框架基于一个预训练的7B参数视觉语言模型，并为其配备了专门的动作头。其训练流程分为三个阶段，旨在从大规模机器人数据中高效学习。</p>
<p><img src="https://arxiv.org/html/2602.03310v1/x2.png" alt="方法框架"></p>
<blockquote>
<p><strong>图2</strong>：RDT2的三阶段训练pipeline。第一阶段：使用RVQ将连续动作编码为离散标记，并用交叉熵损失训练VLM。第二阶段：训练一个动作专家，通过流匹配损失对连续动作概率进行建模。第三阶段：将动作专家蒸馏成一个单步生成器，以实现超快推理。</p>
</blockquote>
<p><strong>第一阶段：离散动作标记化与VLM对齐</strong>。此阶段的核心是解决连续动作与VLM离散知识形式的对齐问题。具体技术细节是采用<strong>残差向量量化</strong>将高维连续动作序列 $\mathbf{A}_t$ 编码为一串离散标记。RVQ通过多层量化器递归地对残差进行量化，能更精确地表示复杂动作分布。编码后的动作标记与语言指令、视觉观测一起输入VLM，训练目标是最小化预测动作标记的交叉熵损失。这种方法避免了直接在VLM上训练连续输出头可能导致的预训练离散知识损坏。</p>
<p><strong>第二阶段：连续概率建模与动作专家训练</strong>。为了获得更具表现力且采样高效的动作模型，本阶段引入一个独立的<strong>动作专家</strong>网络。该专家以VLM编码的中间特征为条件，直接对连续动作的概率分布进行建模。训练采用<strong>流匹配</strong>损失，这是一种基于常微分方程的概率路径建模方法，相比扩散模型能实现更快的训练收敛。动作专家学习从简单先验分布到复杂动作数据分布的确定性变换。</p>
<p><strong>第三阶段：知识蒸馏以实现实时推理</strong>。尽管动作专家采样效率高于扩散模型，但其多步迭代过程仍可能无法满足机器人对实时性的严苛要求。因此，本阶段提出一种简单有效的<strong>蒸馏损失</strong>，将第二训练好的动作专家（教师模型）的知识蒸馏到一个<strong>单步生成器</strong>（学生模型）中。该生成器直接接收VLM特征并输出动作，实现了超快的单步推理速度，同时保持了动作分布建模的能力。</p>
<p><strong>数据基础：增强的大规模UMI采集系统</strong>。方法成功的关键在于其数据基础。论文重新设计了UMI硬件，采用CNC加工（尼龙66与玻璃纤维）替代原有的3D打印，提高了刚度、加工精度和一致性，使其适用于长期、高频次的数据收集。同时改进了追踪方法。制作了约100个此类增强设备，部署在超过100个真实家庭环境中，收集了涵盖多样化操作任务的、超过10,000小时的演示数据，构成了目前最大的开源机器人数据集之一。</p>
<p><strong>创新点</strong>：与现有方法相比，RDT2的核心创新体现在：1) <strong>数据规模与硬件可靠性</strong>：通过硬件重新设计，实现了前所未有的大规模、高质量、设备无关的机器人数据收集。2) <strong>三阶段训练策略</strong>：创新性地结合了离散化（对齐VLM）、连续流匹配（高效建模）和蒸馏（实时推理）的优势，解决了VLA模型在表示学习和推理效率上的矛盾。3) <strong>零样本跨平台泛化目标</strong>：明确将跨机器人平台零样本部署作为核心目标，并通过上述数据和方法实现。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：研究使用了内部收集的超大规模<strong>RDT数据集</strong>（基于UMI）。评估涵盖了多个下游任务benchmark，以测试模型在物体、场景、指令和平台四个维度的泛化能力，以及处理灵巧、长时程、动态任务（如打乒乓球）的性能。实验平台涉及多种真实的机器人臂。</p>
<p><strong>对比基线</strong>：包括当前最先进的VLA模型，如 $\pi_0$-FAST (基于扩散模型的高效变体)、$\pi_{0.5}$、OpenVLA以及前代工作RDT-1B。</p>
<p><img src="https://arxiv.org/html/2602.03310v1/x3.png" alt="实验结果"></p>
<blockquote>
<p><strong>图3</strong>：RDT2在四个泛化维度（物体、场景、指令、平台）上的零样本成功率。RDT2 (7B) 在几乎所有维度的组合上都显著优于基线方法 $\pi_0$-FAST 和 $\pi_{0.5}$，展示了其强大的组合泛化能力。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>零样本组合泛化</strong>：如图3所示，RDT2在未见过的物体、场景、指令和机器人平台的所有组合测试中，均取得了最高的零样本成功率。例如，在“Unseen All”（所有四个维度均未见）的极端设置下，RDT2 (7B) 的成功率超过40%，大幅领先于基线（约20%），证明了其是首个能同时实现这四个因素零样本泛化的模型之一。</li>
<li><strong>规模化定律</strong>：论文训练了四种不同参数规模的模型（500M， 1B， 3B， 7B）。实验发现，<strong>同时扩大模型参数量和数据集规模能带来一致且可预测的性能提升</strong>，这为机器人基础模型的进一步发展提供了实证依据。</li>
<li><strong>下游任务微调</strong>：在需要高灵巧性、长时程规划和高动态响应的挑战性任务（如打乒乓球）上，基于RDT2进行微调后的模型，其性能也超越了 $\pi_0$-FAST 和 $\pi_{0.5}$ 等基线。</li>
<li><strong>消融实验</strong>：广泛的消融研究证实了三阶段训练策略中每个组件的有效性。特别是，使用RVQ进行离散化相比简单量化能带来显著增益；流匹配训练比扩散模型收敛更快；最终的蒸馏步骤在几乎不损失性能的前提下，将推理速度提升了数个数量级，达到实时要求。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>构建并开源了迄今为止规模最大、多样性最丰富的设备无关机器人操作数据集之一（超10,000小时UMI数据），并改进了UMI硬件的可靠性与可扩展性。</li>
<li>提出了一种新颖的三阶段训练方法，巧妙地融合了离散标记化、连续流匹配和蒸馏技术，有效解决了VLA模型在知识对齐、分布建模和推理效率方面的挑战。</li>
<li>首次实证了通过大规模数据和模型联合缩放，可以实现VLA模型对物体、场景、指令和机器人平台的组合式零样本泛化，推动了机器人基础模型向实用化迈进。</li>
</ol>
<p><strong>局限性</strong>：论文自身提到，当前的跨平台零样本泛化依赖于在目标机器人上安装与UMI数据收集阶段<strong>相同型号的相机和夹爪</strong>。这虽然简化了感知对齐，但仍是一种硬件约束。此外，尽管数据规模巨大，但仍可能无法覆盖现实世界中所有可能的场景和任务组合。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>数据规模的至关重要性</strong>：本研究强有力地证明了，在机器人学习领域，追求极大规模、高质量、多样化的数据与扩大模型规模同等重要，是实现泛化能力突破的关键路径。</li>
<li><strong>UMI范式的潜力</strong>：工作展示了基于UMI的设备无关数据收集策略在构建通用机器人模型方面的巨大潜力，为未来数据收集提供了可扩展的蓝图。</li>
<li><strong>架构设计的平衡艺术</strong>：RDT2的三阶段训练策略为如何桥接预训练VLM的离散世界与机器人控制的连续世界提供了有价值的参考，启示后续研究需综合考虑表示学习、训练效率和推理速度的平衡。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出RDT2模型，旨在解决视觉-语言-动作模型数据稀缺、架构低效且难以跨硬件平台零样本泛化的核心问题。方法上，通过增强的硬件无关通用操作接口收集超10,000小时大规模数据集，并采用三阶段训练策略，结合残差向量量化、流匹配与蒸馏技术，实现离散语言知识与连续动作控制的对齐。实验表明，RDT2能同时零样本泛化至未见过的物体、场景、指令及机器人平台，并在灵巧、长视野和动态任务（如打乒乓球）上超越现有最优基线。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2602.03310" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>