<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Developing Vision-Language-Action Model from Egocentric Videos - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Developing Vision-Language-Action Model from Egocentric Videos</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2509.21986" target="_blank" rel="noreferrer">2509.21986</a></span>
        <span>作者: Yoshida, Tomoya, Kurita, Shuhei, Nishimura, Taichi, Mori, Shinsuke</span>
        <span>日期: 2025/09/26</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉-语言-动作模型（VLA）旨在学习能够遵循自然语言指令执行跨环境通用机器人行为的策略。这类模型通常依赖于大规模、多具身数据集的预训练。然而，大多数预训练数据集严重依赖专家手动远程操作来收集模仿学习所需的数据，这种方法成本高昂、劳动密集，导致了数据稀缺问题。第一人称视角视频记录了人类如何操作物体和工具，为学习物体操作提供了丰富的运动线索，是一种有前景的可扩展替代方案。但先前利用此类视频训练机器人策略的研究通常依赖于密集的辅助标注，例如详细的手部姿势记录，这限制了其可扩展性。同时，也有研究尝试从视频中学习隐式动作表示，但这些表示往往难以捕捉精细运动。因此，一个核心的开放性问题在于：能否直接从原始的第一人称视频（无需辅助标注）中训练VLA？</p>
<p>本文针对无需辅助标注直接从原始第一人称视频学习显式动作轨迹这一具体痛点，提出了利用EgoScaler框架提取6自由度物体操作轨迹的新视角。核心思路是：应用EgoScaler从四大第一人称视频数据集中提取物体操作轨迹，并自动过滤噪声数据，构建一个大规模预训练数据集，进而用于训练VLA模型，证明其有效性与可扩展性。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体流程分为两大阶段：1）利用EgoScaler框架从第一人称视频中构建大规模预训练数据集；2）使用该数据集预训练VLA模型（基于π₀架构），随后在特定具身的小规模数据集上进行后训练。</p>
<p><img src="https://arxiv.org/html/2509.21986v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：传统VLA预训练数据源与本文方法的对比。本文利用无需辅助标注的第一人称视频进行VLA预训练。通过EgoScaler提取6DoF物体操作轨迹，构建大规模数据集。</p>
</blockquote>
<p><strong>预训练数据集构建</strong> 的核心是EgoScaler框架，它从视频中提取被操作物体的6DoF姿态序列。该框架包含四个阶段：首先，使用GPT-4o识别视频片段中动作的起止时间以及被操作的物体。其次，使用开放词汇分割模型和密集3D点跟踪器提取该物体在图像中的位置序列。第三，通过点云配准将该位置序列投影到动作起始帧的相机坐标系中，以消除相机穿戴者自身运动的影响。第四，使用奇异值分解计算连续帧间物体点云之间的变换，从而获得物体的旋转序列。最终得到一个6DoF姿态序列 𝝉 = {𝝉₁, 𝝉₂, …, 𝝉_T}，其中每个𝝉_t = (x, y, z, roll, pitch, yaw)，代表了物体质心的平移和旋转，近似为机器人末端执行器的状态（不含夹爪）。</p>
<p><img src="https://arxiv.org/html/2509.21986v1/mats/extracted-samples.png" alt="提取轨迹样本"></p>
<blockquote>
<p><strong>图2</strong>：通过EgoScaler提取的轨迹样本。轨迹颜色从青色（开始）到紫色（结束）表示时间进程。红、绿、蓝箭头代表每个时间步物体坐标系帧的X、Y、Z轴。</p>
</blockquote>
<p>本文扩展了EgoScaler的应用范围，将其应用于四个大规模第一人称视频数据集：Ego4D、Ego-Exo4D、HD-EPIC和Nymeria。由于提取的轨迹可能因物体检测或点云配准错误而产生噪声，论文引入了两种自动过滤方法：</p>
<ol>
<li><strong>行程距离阈值</strong>：过滤因配准错误导致物体位置发生异常大幅跳变的轨迹。</li>
<li><strong>背景轨迹相似度阈值</strong>：过滤那些被跟踪的“物体”实际上是静止背景的检测错误轨迹。通过计算物体位移与背景位移之间的平均余弦相似度来衡量。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.21986v1/mats/filter-exp.png" alt="背景轨迹相似度计算示例"></p>
<blockquote>
<p><strong>图3</strong>：计算背景轨迹相似度的样本。红色为跟踪序列。低的背景轨迹相似度表明物体因手部交互而移动。</p>
</blockquote>
<p>此外，还对轨迹的平移分量应用了平滑滤波以抑制抖动噪声。经过筛选，最终获得了包含45,157个数据片段的数据集。</p>
<p><strong>策略训练</strong> 采用最先进的VLA架构π₀。在预训练阶段，动作表示为从第一人称视频中提取的6DoF物体姿态轨迹的位移：𝐚_t = [Δx_t, Δy_t, Δz_t, Δrot6D_t]。由于无法从视频中获得夹爪状态，因此动作是9维向量。本体感觉状态使用原始轨迹𝝉。训练目标是预测未来H步的动作序列，损失函数为预测动作与真实动作之间的均方误差。当与其他机器人数据集合并预训练时，会对动作和本体感觉向量进行填充和归一化以统一维度。</p>
<p>本文方法的创新点具体体现在：1）首次系统性地证明了直接从原始第一人称视频（无需任何辅助标注）学习显式、连续的6DoF动作轨迹用于VLA预训练的可行性。2）扩展了EgoScaler框架的应用，并设计了一套自动数据过滤流程，从现有视频资源中构建了一个大规模、高质量的数据集。3) 在动作表示和训练目标上进行了适配，使得从视频中提取的物体轨迹能够有效地转化为机器人策略的监督信号。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验在模拟环境（SIMPLER BridgeData V2）和真实机器人环境（ALOHA）中进行，共设计了4个拾取-放置任务。评估模型为基于π₀架构从头开始预训练的VLA。对比的基线方法包括：1）<strong>从头训练</strong>：仅在后训练数据集上训练。2）<strong>LAPA</strong>：一种不从视频中学习显式动作，而是学习隐式动作令牌的预训练方法。3）三个真实机器人数据集：<strong>BridgeData V2</strong>、<strong>BC-Z</strong>和<strong>Fractal</strong>。</p>
<p><img src="https://arxiv.org/html/2509.21986v1/x3.png" alt="性能对比"></p>
<blockquote>
<p><strong>图5</strong>：基线方法与我们数据集在各种操作任务上的性能对比。“ObjectA – ObjectB”表示任务“拾取ObjectA并放入ObjectB”。括号内表示用于隐式动作预训练的数据集。报告了平均成功率（%）及标准误。</p>
</blockquote>
<p>关键实验结果如下：</p>
<ol>
<li><strong>与从头训练和LAPA对比</strong>：在真实机器人任务中，使用本文数据集预训练的模型显著优于从头训练的基线（后者成功率为0%），平均成功率提升超过20%。同时，本文方法也 consistently 优于LAPA方法，表明显式动作轨迹比隐式动作表示提供了更有效的监督。</li>
<li><strong>与真实机器人数据集对比</strong>：如表II所示，仅使用本文数据集预训练的模型，其性能与领先的真实机器人数据集相当，略优于BC-Z和BridgeData V2，但稍逊于规模更大的Fractal数据集。</li>
<li><strong>数据合并效果</strong>：将本文数据集与BridgeData V2合并进行预训练，取得了最佳性能（27/40成功），超越了单独使用任一数据集的效果，证明了其与现有机器人数据的互补性。</li>
</ol>
<p><img src="https://arxiv.org/html/2509.21986v1/x4.png" alt="数据集规模消融"></p>
<blockquote>
<p><strong>图6</strong>：数据集规模与性能的关系。报告了平均成功率（%）及标准误。</p>
</blockquote>
<p><strong>消融实验</strong> 表明：</p>
<ul>
<li><strong>数据集规模</strong>：在真实机器人环境中，扩大数据集规模能显著提升性能。但在模拟环境中，使用全量数据（45K片段）的性能略低于使用10%数据（5K片段）的子集，作者归因于模拟器视觉系统简化与真实第一人称视频丰富视觉线索之间的领域差距。</li>
<li><strong>过滤超参数</strong>：背景轨迹相似度阈值δ_BGTS的选择需要在数据规模和质量间权衡。实验发现δ_BGTS=0.7能在保持较大数据规模（45,157个片段）的同时，在模拟和真实实验中取得最佳性能平衡。</li>
</ul>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献在于：1）首次成功地从无需辅助标注的原始第一人称视频中预训练了VLA模型，并证明了其有效性，性能与主流真实机器人数据集相当。2）构建了一个新的大规模预训练数据集，为社区提供了新的资源。3）揭示了该数据集与现有机器人数据集的互补性，合并训练能带来进一步的性能提升。</p>
<p>论文提到的局限性包括：在视觉系统简化的模拟环境中，来自丰富真实视频的预训练优势可能因视觉领域差距而减弱；此外，从视频中提取的轨迹不包含夹爪状态，这限制了需要精细抓取操作的任务。</p>
<p>这项工作对后续研究的启示是：第一人称视频是解决机器人学习数据稀缺问题的一个极具潜力且可扩展的资源。自动化的轨迹提取和过滤方法具有通用性，可应用于更多视频源。未来，如何更好地弥合视频与机器人控制之间的模态差距、以及如何将此类数据更有效地整合到多模态、多具身的基础模型训练中，是值得深入探索的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文解决了直接从原始第一人称视角视频训练视觉-语言-动作模型（VLA）的核心问题，避免了依赖昂贵的专家遥操作或密集辅助注释（如手部姿态）。关键技术采用EgoScaler框架，自动从视频中提取6DoF物体操作轨迹并精炼噪声数据，构建了大规模预训练数据集。实验基于π0架构在模拟和真实环境中验证：预训练相比从头训练任务成功率提升超过20%，性能与真实机器人数据集相当，且结合两者能带来进一步改进。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2509.21986" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>