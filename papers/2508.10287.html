<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2508.10287" target="_blank" rel="noreferrer">2508.10287</a></span>
        <span>作者: Jahangard, Simindokht, Mohammadi, Mehrzad, Shen, Yi, Cai, Zhixi, Rezatofighi, Hamid</span>
        <span>日期: 2025/08/14</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，视觉语言模型（VLMs）和大语言模型（LLMs）的进步极大地推动了视觉推理能力的发展，这对于机器人等具身智能体至关重要。然而，现有的视觉推理基准测试普遍存在几个关键局限：首先，它们通常将各种任务笼统地归为“推理”，缺乏对推理复杂度的明确定义和区分；其次，这些基准无法根据用户指定的参数（如任务类型、关注主体、空间或时间范围）动态生成不同难度的问题，限制了评估的定制化和全面性；最后，大多数数据集仅提供输入和最终答案标签，缺少推导答案所需的、结构化的中间推理步骤（工作流）标注，这不利于评估基于思维链或程序化推理的模型。</p>
<p>本文针对上述痛点，提出了一个新的视角：将视觉推理任务的复杂度进行形式化定义，并基于此构建一个能够自适应生成可定制难度问题及详细推理步骤的基准测试。核心思路是：将场景建模为时空图，用图中涉及的实体节点数（S）、空间关系边数（R）和时间槽数（T）来量化推理复杂度，并开发一个生成式查询引擎，根据用户偏好动态生成非预定义的问题及其逐步解答，最终在JRDB数据集基础上扩展创建了面向人机共存环境的视觉推理基准JRDB-Reasoning。</p>
<h2 id="方法详解">方法详解</h2>
<p>本文方法的整体框架是一个自适应的生成式查询引擎，它允许用户通过指定参数来定制视觉推理问题，并自动生成对应的问题描述和详细的中间推理步骤（工作流）。输入是用户对任务类型（视觉定位VG或视觉问答VQA）、模态（图像或视频）、关注主体（人、物或两者）、空间配置（单节点、配对或团）和时间配置的偏好；输出是符合这些偏好的自然语言问题，以及分解的、逐步的推理步骤标注。</p>
<p><img src="https://arxiv.org/html/2508.10287v2/x3.png" alt="方法框架"></p>
<blockquote>
<p><strong>图3</strong>：生成式查询引擎示例流程。（1）用户选择偏好：模态（图像）、主体（人&amp;物）、任务（VG）、空间（团）、时间（单槽）。（2）计算所有可能的属性组合并随机选择一组。（3）根据模态生成时空图（STG）。（4）在STG中搜索，逐步生成中间标注和最终查询。</p>
</blockquote>
<p>核心模块包括推理复杂度的形式化和自适应查询引擎。</p>
<ol>
<li><strong>推理复杂度形式化</strong>：本文提出视觉推理的复杂度存在于从感知到高级推理的连续谱上。通过将场景表示为动态时空图（节点为实体，边为交互关系），复杂度被量化为三个因素的和：D = S + R + T。其中，S代表问题涉及的实体（节点）数量；R代表单帧内实体间的空间关系（边）数量；T代表跨时间交互涉及的时间槽数量。该定义通过用户研究验证了与人类对难度的感知高度一致。</li>
<li><strong>自适应查询引擎</strong>：该引擎根据用户输入的偏好参数运作。首先，基于选择的模态和空间配置，构建或调用对应的时空图（STG）。接着，计算图中所有可能的节点属性和边关系组合，并随机选择一组作为生成问题的基础。然后，引擎在STG上执行搜索，逐步地将选中的属性或关系整合到自然语言查询中，并同步生成每一步对应的中间推理描述（例如，“识别所有人”→“筛选女性”→“找出看向机器人的女性”）。</li>
</ol>
<p>与现有方法相比，创新点具体体现在：1) 首次对视觉推理任务的复杂度进行了可量化的、基于图结构的形式化定义；2) 开发了一个动态、非预定义的查询生成引擎，支持高度定制化的问题生成；3) 为每个生成的问题提供了结构化的、逐步的中间推理步骤标注，支持对模型推理过程的细粒度评估。</p>
<p><img src="https://arxiv.org/html/2508.10287v2/x2.png" alt="复杂度形式化"></p>
<blockquote>
<p><strong>图2</strong>：推理复杂度的形式化示意图。(a) 推理复杂度从基础感知到多步推理的连续谱。(b) 场景被建模为时空图，节点（S）为实体，边（R）为交互。(c) 实体和交互随时间槽（T）演化。颜色从浅绿到深绿表示复杂度增加。</p>
</blockquote>
<p><strong>数据集扩展</strong>：为了支撑引擎和基准，本文对JRDB数据集进行了重要扩展。首先，手动标注了细粒度、多标签的人-物交互（HOI），分为姿态基、观察性、物理性和操纵性四类，并为每个标注分配了基于置信度的难度等级（易、中、难）。</p>
<p><img src="https://arxiv.org/html/2508.10287v2/x4.png" alt="HOI分类"></p>
<blockquote>
<p><strong>图4</strong>：人-物交互的四种类型：姿态基、观察性、物理性和操纵性。</p>
</blockquote>
<p>其次，计算了几何关系，包括基于角度和距离分类的空间方位关系（如前、左、后右等）以及五档距离关系（非常近、近、中等、远、非常远）。这些新增的HOI和几何关系标注被集成到查询引擎中，用于生成更丰富、更具空间逻辑的问题。</p>
<h2 id="实验与结果">实验与结果</h2>
<p>实验使用了新构建的JRDB-Reasoning基准测试，该基准基于JRDB数据集扩展了HOI和几何关系标注。实验平台涉及对多个先进的多模态大模型进行零样本评估。</p>
<p>对比的基线方法包括：用于视觉问答（VQA）的InternVL 2.5、Paligemma、Qwen2.5-VL、LLaVA-NeXT；用于视频VQA的InternVL 2.5、Qwen2.5-VL、LLaVA-NeXT-Video；用于视觉定位（VG）的Florence-V2、GroundingDINO、Qwen2.5-VL、YOLO-World。</p>
<p>关键实验结果如下：</p>
<ul>
<li><strong>VQA（图像）</strong>：如表2所示，所有模型在三个难度等级（𝒟₁: S=1,R=0,T=1；𝒟₂: S=2,R≥1,T=1；𝒟₃: S=3,R≥2,T=1）上的准确率均随复杂度增加而显著下降。例如，表现最佳的InternVL 2.5从𝒟₁的49.9%下降到𝒟₃的44.6%。人类评估准确率远高于模型（𝒟₁:96.8%，𝒟₃:87.1%），但也呈现下降趋势。</li>
<li><strong>VQA（视频）</strong>：如表3所示，视频推理任务中模型性能同样随难度上升而下降。InternVL 2.5在视频上的表现从𝒟₁的49.1%降至𝒟₃的46.0%。人类评估准确率最高（𝒟₁:98.3%），且下降幅度相对较小。</li>
<li><strong>视觉定位（VG）</strong>：如表4所示，在图像VG任务中，所有模型的mIoU指标均随问题复杂度增加而恶化。Florence-V2表现相对最好，但从𝒟₁的37.6%降至𝒟₃的25.2%。YOLO-World的性能下降最为剧烈（从13.5%到3.1%）。人类评估的mIoU同样很高且随难度下降（𝒟₁:95.5%，𝒟₃:86.6%）。</li>
</ul>
<p><img src="https://arxiv.org/html/2508.10287v2/x1.png" alt="基准示例"></p>
<blockquote>
<p><strong>图1</strong>：JRDB-Reasoning在图像和视频上的VG与VQA任务示例。(a)(e)为图像上的VG任务。(b)展示了一个带推理步骤的VQA计数问题。(c)为VG多目标跟踪任务。(d)为应用于视频的VQA-Wh问题。</p>
</blockquote>
<p>这些实验结果清晰地表明：1）当前先进的VLMs和VG模型的性能严重受限于视觉推理任务的复杂度，随着涉及实体和关系的增加，性能显著下降；2）即使在相对较低的复杂度级别（𝒟₃），模型的性能与人类评估结果之间仍存在巨大差距，揭示了现有模型在复杂多步推理能力上的不足。</p>
<h2 id="总结与启发">总结与启发</h2>
<p>本文的核心贡献可概括为三点：第一，提出了一种基于时空图（节点S、边R、时间T）的视觉推理复杂度形式化定义，为量化评估任务难度提供了系统框架；第二，开发了一个自适应查询引擎，能够根据用户偏好动态生成非预定义、可定制复杂度的问题，并自动提供详细的逐步推理步骤标注；第三，通过手动标注人-物交互和计算几何关系，扩展了JRDB数据集，创建了面向人机共存环境、按难度分级的视觉推理基准JRDB-Reasoning。</p>
<p>论文自身提到的局限性在于，尽管查询引擎能够生成极高复杂度的问题，但当前的实验评估仅聚焦于复杂度不超过𝒟₃（S=3, R≥2, T=1）的问题子集，因为现有模型尚难以处理更高难度的任务。</p>
<p>本文工作对后续研究有多重启示：首先，JRDB-Reasoning基准为系统评估和比较模型在不同难度级别上的视觉推理能力提供了标准平台，有望推动模型向更高级的推理能力发展。其次，所提供的逐步推理标注可用于训练或评估需要显式推理过程的模型（如神经符号模型、程序引导模型）。最后，其复杂度形式化框架和自适应查询生成范式，可以扩展到其他视觉语言任务乃至不同的数据模态，促进更细粒度、更可控的基准测试构建。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对机器人视觉推理领域，指出现有基准缺乏推理复杂度定义、难度可控的问题生成及结构化推理标注。为此，论文形式化了推理复杂度，并提出自适应查询引擎，用于生成可定制、分难度的问题并附带详细中间标注；同时扩展JRDB数据集，加入人-物交互与几何关系标注，构建了面向人群环境的JRDB-Reasoning基准。该基准支持对视觉推理框架进行细粒度评估，并能动态评估视觉语言模型在不同推理层级上的能力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2508.10287" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>