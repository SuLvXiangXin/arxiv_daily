<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>MM-ACT: Learn from Multimodal Parallel Generation to Act - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Computer Vision and Pattern Recognition (cs.CV)</span>
      <h1>MM-ACT: Learn from Multimodal Parallel Generation to Act</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2512.00975" target="_blank" rel="noreferrer">2512.00975</a></span>
        <span>作者: Liang, Haotian, Chen, Xinyi, Wang, Bin, Chen, Mingkang, Liu, Yitian, Zhang, Yuhao, Chen, Zanxin, Yang, Tianshuo, Chen, Yilun, Pang, Jiangmiao, Liu, Dong, Yang, Xiaokang, Mu, Yao, Shao, Wenqi, Luo, Ping</span>
        <span>日期: 2025/11/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，构建通用机器人策略的Vision-Language-Action模型主要有两类范式。一类是基于大规模预训练视觉语言模型，通过添加动作头或专家模块来桥接感知与控制。这类方法虽然语义理解能力强，但通常缺乏对物理动力学的显式建模，限制了其时序动作生成能力。另一类是将视觉预测融入策略学习的框架，这类世界模型擅长建模环境动态，但主要针对预测目标训练，在任务理解和子任务规划方面能力有限。近期出现的统一VLA模型大多继承了统一理解与生成模型的发展范式，在设计动作生成时紧密跟随基础模型的建模方式，存在架构或效率上的局限。例如，有的方法对所有模态都采用自回归生成，导致动作推断速度慢；有的则采用混合范式（文本自回归，图像和动作并行解码），迫使模型在前向过程中同时学习处理单令牌预测和块级令牌预测，增加了架构和训练流程的复杂性。</p>
<p>本文针对现有统一VLA模型在解码策略和训练目标上不一致、效率不高的痛点，提出了一个全新的视角：采用完全并行的解码策略来统一生成文本、图像和动作，并设计一个共享上下文的训练范式。本文的核心思路是：通过特定模态的分词器将文本、图像和动作统一到共享的离散令牌空间中，并利用基于掩码令牌预测的并行解码策略进行生成，同时提出“上下文共享多模态学习”训练方法，通过跨模态学习来增强动作生成能力。</p>
<h2 id="方法详解">方法详解</h2>
<p>MM-ACT的整体框架是一个基于Transformer的掩码令牌预测器，配备双向注意力机制，以促进跨三种模态的生成任务。模型使用特定模态的分词器将文本、图像和机器人的本体感知状态表示为来自三个分词器合并词表的单一离散令牌序列。</p>
<p><img src="https://arxiv.org/html/2512.00975v2/x2.png" alt="方法架构"></p>
<blockquote>
<p><strong>图2</strong>：MM-ACT的架构。模型使用特定模态的分词器将文本、图像和动作标记化到共享空间中的离散令牌。给定共享的多模态输入，模型根据模态令牌决定是执行任务规划、未来图像预测还是动作生成，每项任务对应文本、图像或动作的生成。</p>
</blockquote>
<p><strong>输入与输出</strong>：对于每个模态，输入上下文为 <code>C_modal = &lt;modal&gt; + sharedinput</code>，其中模态令牌 <code>&lt;modal&gt;</code> 属于 <code>{&lt;|mm2a|&gt;, &lt;|mmu|&gt;, &lt;|t2i|&gt;}</code>，分别对应动作生成、图像生成和文本生成。<code>sharedinput</code> 是一个遵循模板的模态交错令牌序列，包含了机器人的多视角观测、任务指令、文本描述以及可选的机器人状态。在上下文 <code>C_modal</code> 之后，会附加一个固定长度的掩码令牌块用于生成。文本块大小设为256以适应任务规划序列；图像块大小也为256，用于生成单张图像；动作块大小 <code>N_act_block = d_action * N_chunk_size</code>，用于生成一个包含 <code>N_chunk_size</code> 个动作的块。</p>
<p><strong>核心模块与技术细节</strong>：</p>
<ol>
<li><strong>分词器</strong>：文本使用LLaDA模型的分词器；图像使用来自Show-o的预训练图像量化器，将256x256图像编码为256个令牌，使用8,192个令牌的码本；动作采用bin tokenizer作为量化方法，分配2,048个专用令牌，将连续标量归一化到[-1, 1]后量化为令牌。</li>
<li><strong>上下文共享多模态学习</strong>：这是统一的训练范式。模型被训练为一个掩码令牌预测器 <code>p_θ(· | C_modal, x_t)</code>。对于每种模态，将其特定的生成块建模为令牌序列 <code>x_0</code>，然后根据时间 <code>t</code> 和模态特定的掩码调度函数 <code>f_modal(t)</code> 对 <code>x_0</code> 进行掩码，得到 <code>x_t</code>。文本模态使用线性调度，图像和动作模态使用余弦调度。对于动作模态，训练时固定 <code>t=1</code>，即从完全掩码的序列生成所有令牌。</li>
</ol>
<p><img src="https://arxiv.org/html/2512.00975v2/x3.png" alt="训练流程"></p>
<blockquote>
<p><strong>图3</strong>：MM-ACT的训练流程。在共享上下文中，三种模态的真实值根据各自模态的解码策略进行掩码，并附加到上下文后。模型接收这些输入，跨三种模态生成任务执行前向过程，并专门在掩码令牌上计算损失。</p>
</blockquote>
<ol start="3">
<li><strong>统一损失函数</strong>：采用统一的交叉熵损失，仅对掩码令牌进行计算。损失函数为公式(3)，其中包含每种模态的损失权重 <code>λ_modal</code>，用于控制各模态对优化过程的影响。</li>
<li><strong>两阶段训练策略</strong>：第一阶段，设置 <code>λ_mm2a=0</code>，仅训练文本和图像生成任务。第二阶段，主要监督动作生成，将 <code>λ_mmu</code> 和 <code>λ_t2i</code> 调整至约0.05–0.1，以保持文本和图像的生成能力。</li>
<li><strong>并行解码策略</strong>：模型被表述为块级掩码令牌预测器。<strong>动作生成</strong>采用一步并行解码策略，在单次前向传递中生成所有动作令牌以保证效率。<strong>图像生成</strong>采用重掩码并行解码策略，使用与MAGVIT-v2一致的余弦噪声计划和低置信度重掩码策略。<strong>文本生成</strong>同样在单个块内进行，限制序列长度为256令牌，不采用半自回归方式，也使用重掩码解码策略（线性调度）。</li>
</ol>
<p><strong>创新点</strong>：与现有方法相比，MM-ACT的主要创新在于：1) <strong>完全并行解码的统一架构</strong>：摒弃了自回归文本生成，所有模态均采用并行解码，使用双向注意力，架构更统一，简化了训练流程。2) <strong>上下文共享的多模态学习训练范式</strong>：在共享的上下文上联合训练文本、图像和动作生成，使用统一的掩码令牌预测损失，并通过跨模态监督（特别是文本和图像）来增强动作生成能力。3) <strong>高效的动作生成</strong>：为动作设计了一步并行解码策略，实现了低延迟推断。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验平台与数据集</strong>：仿真实验在LIBERO（基于Franka机械臂，包含Spatial, Object, Goal, Long四个子基准，各10个任务）和RoboTwin2.0（用于双手操作的仿真基准，在8个未见过的任务上评估）上进行。真实世界实验使用Franka机器人，执行按下按钮、堆叠积木、分类蔬果三个任务。</p>
<p><strong>对比基线</strong>：评估了三大类基线方法：1) <strong>VLM-based VLA</strong>：如OpenVLA、OpenVLA-OFT、π0，侧重语义理解但缺乏动态建模；2) <strong>Visual Prediction VLA</strong>：如CoT-VLA、TraceVLA、DreamVLA，强调未来预测但任务推理有限；3) <strong>Unified VLA</strong>：基于统一模型架构，如UniVLA、WorldVLA，具备多模态生成能力。</p>
<p><strong>关键实验结果</strong>：</p>
<ul>
<li><strong>LIBERO基准</strong>：如表1所示，MM-ACT (Vanilla) 在四个子基准上分别取得97.8%、99.4%、94.8%、88.0%的成功率，平均95.0%。当在Long任务上加入文本（任务规划）联合训练后（MM-ACT (+Text in Long)），Long任务成功率提升至93.0%（+5.0%），平均成功率达到了**96.3%**，超越了所有基线。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Spatial SR (%)</th>
<th align="left">Object SR (%)</th>
<th align="left">Goal SR (%)</th>
<th align="left">Long SR (%)</th>
<th align="left">Average SR (%)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">OpenVLA [21]</td>
<td align="left">84.7</td>
<td align="left">88.4</td>
<td align="left">79.2</td>
<td align="left">53.7</td>
<td align="left">76.5</td>
</tr>
<tr>
<td align="left">π0 [3]</td>
<td align="left">96.8</td>
<td align="left">98.8</td>
<td align="left">95.8</td>
<td align="left">85.2</td>
<td align="left">94.2</td>
</tr>
<tr>
<td align="left">OpenVLA-OFT [22]</td>
<td align="left">96.2</td>
<td align="left">98.3</td>
<td align="left">96.2</td>
<td align="left">90.7</td>
<td align="left">95.4</td>
</tr>
<tr>
<td align="left">UniVLA [42]</td>
<td align="left">95.4</td>
<td align="left">98.8</td>
<td align="left">93.6</td>
<td align="left">94.0</td>
<td align="left">95.5</td>
</tr>
<tr>
<td align="left"><strong>MM-ACT (Vanilla)</strong></td>
<td align="left"><strong>97.8</strong></td>
<td align="left"><strong>99.4</strong></td>
<td align="left">94.8</td>
<td align="left">88.0</td>
<td align="left">95.0</td>
</tr>
<tr>
<td align="left"><strong>MM-ACT (+Text in Long)</strong></td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left">-</td>
<td align="left"><strong>93.0</strong></td>
<td align="left"><strong>96.3</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表1</strong>：LIBERO任务性能结果。加粗为最佳性能，下划线为次佳。MM-ACT在加入文本联合训练后取得了最佳平均性能。</p>
</blockquote>
<ul>
<li><strong>RoboTwin2.0基准</strong>：如表2所示，MM-ACT (Vanilla) 在8个任务上的平均成功率为43.13%。通过上下文共享多模态学习，与仅动作训练相比，**文本-动作联合训练提升3.37%，图像-动作联合训练提升5.62%，三者联合训练提升9.25%<strong>，最终MM-ACT (+Text&amp;Image) 达到</strong>52.38%**的平均成功率。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Adjust Bottle</th>
<th align="left">Beat Hammer Block</th>
<th align="left">Click Bell</th>
<th align="left">Dump Bin Bigbin</th>
<th align="left">Move Playingcard Away</th>
<th align="left">Lift Pot</th>
<th align="left">Place Burger Fries</th>
<th align="left">Place Can Basket</th>
<th align="left"><strong>Overall Avg</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">π0 [3]</td>
<td align="left">89%</td>
<td align="left">68%</td>
<td align="left">40%</td>
<td align="left">61%</td>
<td align="left">47%</td>
<td align="left">29%</td>
<td align="left">41%</td>
<td align="left">10%</td>
<td align="left">48.13%</td>
</tr>
<tr>
<td align="left">MM-ACT (Vanilla)</td>
<td align="left">51%</td>
<td align="left">61%</td>
<td align="left">86%</td>
<td align="left">13%</td>
<td align="left">30%</td>
<td align="left">40%</td>
<td align="left">46%</td>
<td align="left">18%</td>
<td align="left">43.13%</td>
</tr>
<tr>
<td align="left">MM-ACT (+Text)</td>
<td align="left">75%</td>
<td align="left">67%</td>
<td align="left">91%</td>
<td align="left">13%</td>
<td align="left">24%</td>
<td align="left">28%</td>
<td align="left">56%</td>
<td align="left">18%</td>
<td align="left">46.5%</td>
</tr>
<tr>
<td align="left">MM-ACT (+Image)</td>
<td align="left">72%</td>
<td align="left">64%</td>
<td align="left">91%</td>
<td align="left">8%</td>
<td align="left">39%</td>
<td align="left">31%</td>
<td align="left">72%</td>
<td align="left">13%</td>
<td align="left">48.75%</td>
</tr>
<tr>
<td align="left"><strong>MM-ACT (+Text&amp;Image)</strong></td>
<td align="left"><strong>71%</strong></td>
<td align="left"><strong>78%</strong></td>
<td align="left"><strong>95%</strong></td>
<td align="left"><strong>13%</strong></td>
<td align="left"><strong>39%</strong></td>
<td align="left"><strong>31%</strong></td>
<td align="left"><strong>73%</strong></td>
<td align="left"><strong>19%</strong></td>
<td align="left"><strong>52.38%</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表2</strong>：RoboTwin任务性能结果。加粗为最佳性能。展示了多模态联合训练对动作生成的持续提升。</p>
</blockquote>
<ul>
<li><strong>Franka真实世界实验</strong>：如表3所示，MM-ACT在三个任务上的平均成功率为**72.0%**，优于π0 (70.0%) 和 OpenVLA-OFT (58.6%)。</li>
</ul>
<table>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Press Button</th>
<th align="left">Stack Block</th>
<th align="left">Sort Vegetable and Fruits</th>
<th align="left"><strong>Average</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">π0 [3]</td>
<td align="left">75.0</td>
<td align="left">70.0</td>
<td align="left">65.0</td>
<td align="left">70.0</td>
</tr>
<tr>
<td align="left">OpenVLA-OFT [22]</td>
<td align="left">70.0</td>
<td align="left">50.0</td>
<td align="left">56.0</td>
<td align="left">58.6</td>
</tr>
<tr>
<td align="left"><strong>MM-ACT</strong></td>
<td align="left"><strong>80.0</strong></td>
<td align="left"><strong>70.0</strong></td>
<td align="left"><strong>66.0</strong></td>
<td align="left"><strong>72.0</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>表3</strong>：Franka真实世界实验主要结果。MM-ACT取得了最佳平均性能。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2512.00975v2/x5.png" alt="图像生成可视化"></p>
<blockquote>
<p><strong>图5</strong>：MM-ACT在RoboTwin未见环境中的图像生成可视化。顶部为模型生成图像，底部为真实图像。展示了模型在域随机化（c,d）和干净（a,b）的未见场景中预测未来图像的能力。</p>
</blockquote>
<p><strong>消融实验总结</strong>：表2本身即是一个关键的消融实验，它系统性地评估了上下文共享多模态学习中各组件对动作生成的贡献。结果表明，无论是与文本还是与图像联合训练，都能提升动作生成性能，而三者联合训练带来的提升最大（+9.25%），验证了所提训练范式的有效性。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li><strong>提出了一个完全并行解码的统一VLA架构（MM-ACT）</strong>：将文本、图像和动作集成到共享的离散令牌空间，并使用并行解码策略进行生成，简化了模型架构和训练流程，同时保证了动作生成的高效性。</li>
<li><strong>设计了“上下文共享多模态学习”训练范式</strong>：在共享的上下文上统一监督三种模态的生成，使用一致的掩码令牌预测目标，并通过文本（任务规划）和图像（未来预测）的联合训练显著提升了动作生成性能（在RoboTwin上最高提升9.25%）。</li>
<li><strong>在多个基准上验证了方法的有效性</strong>：在LIBERO仿真、RoboTwin2.0仿真和Franka真实机器人实验上均取得了领先或极具竞争力的性能，证明了该统一范式在域内和域外任务上的强大能力。</li>
</ol>
<p><strong>局限性</strong>：论文提到，为了效率，动作生成采用了一步解码策略，这可能牺牲了重掩码解码策略带来的潜在性能提升。此外，与一些利用大规模机器人数据预训练的VLA模型相比，MM-ACT的模型规模和训练数据规模相对较小。</p>
<p><strong>对后续研究的启示</strong>：</p>
<ol>
<li><strong>解码策略的探索</strong>：可以进一步研究在保持效率的前提下，如何为动作生成引入更精细的多步解码策略以平衡效果与速度。</li>
<li><strong>规模扩展</strong>：将MM-ACT的架构和训练范式扩展到更大的模型参数和更丰富的多模态机器人数据集上，有望进一步提升其通用性和性能。</li>
<li><strong>范式推广</strong>：这种基于共享上下文和并行解码的统一多模态生成范式，可以启发更广泛的多模态序列生成任务，不局限于机器人领域。</li>
</ol>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出MM-ACT，旨在解决通用机器人策略需同时具备高层次语义理解和环境交互能力的问题。其核心是构建一个统一的视觉-语言-动作模型，将文本、图像和动作集成到共享的token空间中进行跨模态生成。关键技术包括：采用重掩码并行解码策略生成文本和图像，以及采用一步并行解码策略高效生成动作；并提出上下文共享多模态学习范式，通过共享上下文监督所有模态的生成以增强动作能力。实验表明，该模型在LIBERO仿真中成功率高达96.3%，在真实机器人任务上表现优异，且跨模态学习带来了9.25%的性能提升。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2512.00975" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>