<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Hierarchical Intention-Aware Expressive Motion Generation for Humanoid Robots - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>Hierarchical Intention-Aware Expressive Motion Generation for Humanoid Robots</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2506.01563" target="_blank" rel="noreferrer">2506.01563</a></span>
        <span>作者: Bao, Lingfan, Pan, Yan, Peng, Tianhu, Kanoulas, Dimitrios, Zhou, Chengxu</span>
        <span>日期: 2025/06/02</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>当前，实现有效的仿人机器人人机交互面临两大核心挑战。一方面，生成富有表现力的非语言行为至关重要。传统方法依赖基于规则或模板的方法，虽可预测但僵化，难以适应动态社交环境。数据驱动方法，如生成对抗网络、变分自编码器和扩散模型，能够生成新颖且上下文适应的动作，但往往需要大量训练和计算资源，且现有框架（如EMOTION++和GenEM）仍显著依赖于特定演示或预定义模板，限制了其适应性和自然度。另一方面，理解人类意图是交互的基础。尽管基于视觉语言模型的方法在推断人类目标方面展现出强大潜力，但其焦点通常局限于显式的功能性目标，而非动态社交环境中隐含的情感意图，导致其输出往往停留在任务分解，未能完成从意图到兼具功能性与社交表现力的物理响应的闭环。</p>
<p>本文针对现有方法在“社交推理”与“物理响应生成”之间的脱节这一具体痛点，提出了一种新的层次化视角。核心思路是构建一个名为HIAER的层次化框架，通过整合VLM对社交意图和情感上下文的细粒度推理，将复杂社交意图参数化为效价-唤醒度估计，进而指导生成在物理仿人机器人上实时执行的、风格恰当的姿势动作。</p>
<h2 id="方法详解">方法详解</h2>
<p>HIAER框架是一个三层架构，旨在将高层社交意图推断与低层表现性动作执行相耦合。</p>
<p><img src="https://arxiv.org/html/2506.01563v4/x1.png" alt="方法整体框架"></p>
<blockquote>
<p><strong>图1</strong>：所提工作的整体框架。(a) 高层系统架构。多模态输入XI被意图感知模块πi解析，产生结构化输出O和动作基元⟨m⟩，它们作为条件指导规划器πp和全身控制器πw生成动作at。(b) 交互场景中流程的具体示例：πi输出描述、意图、置信度和V-A值；动作生成器合成姿势并重定向以在仿人机器人上执行。</p>
</blockquote>
<p><strong>1. 意图感知与推断模块 (πi)</strong><br>该模块是框架的核心，基于预训练的Qwen2.5-VL-32B VLM构建。它接收多模态输入XI（视觉Vin和语言Lin）以及对话历史HI/O。其核心是通过精心设计的上下文学习策略，引导VLM执行一个包含链式推理的、结构化的分析过程。输出是一个结构化元组O = [d, i, c, (V, A), ⟨m⟩]，其中包含场景描述d、推断的人类意图i及置信度c、情感上下文的效价-唤醒度估计(V, A)，以及对应的动作基元⟨m⟩。</p>
<ul>
<li><strong>V-A模型用于表现力</strong>：效价-唤醒度模型是一个将情感状态映射到二维连续空间（效价：愉悦-不愉悦；唤醒度：激活-失活）的心理学框架。通过提示VLM将可观察线索映射到此空间，可以调制所选动作基元的表现风格。例如，“挥动右臂”这一基元，结合高效价、中等唤醒度状态，会产生友好、欢迎的姿势，而非机械或无生气的动作。</li>
<li><strong>预提示设计与少样本示例</strong>：预提示定义了VLM的核心指令、约束和所需的推理结构，包括角色与任务定义、输出格式与CoT强制执行、V-A上下文映射以及安全规则。其中，V-A上下文映射表将V-A空间的每个象限（如图2所示）与特定的效价-唤醒度目标范围及一组特征性物理姿势关联起来，从而将抽象情感状态锚定到具体数值和可观察动作。少样本示例则提供了从多模态输入到完整结构化输出的具体演示，对于引导VLM遵循预期的推理路径和输出格式至关重要。</li>
</ul>
<p><img src="https://arxiv.org/html/2506.01563v4/x2.png" alt="V-A空间图示"></p>
<blockquote>
<p><strong>图2</strong>：框架利用V-A空间解释人类情感上下文并调制机器人响应。蓝色区域（象限I）代表高唤醒度和消极效价状态，绿色区域（象限II）对应高唤醒度和积极效价。黄色区域（象限III）表示低唤醒度和积极效价，红色区域（象限IV）表示低唤醒度和消极效价。灰色代表未激活的中性状态。</p>
</blockquote>
<p><strong>2. 表现性动作生成 (πp)</strong><br>该模块作为中层运动规划器，将高层文本动作命令⟨m⟩转化为仿人机器人运动学上可行的轨迹yt:t+n。这是一个两阶段过程：</p>
<ul>
<li><strong>人类动作生成</strong>：采用基于文本的动作扩散模型DART。它以自回归方式运行，在每一步，以前一动作剪辑y^t-n:t-1和文本动作基元⟨m⟩为条件，合成下一动作序列y^t:t+n。为确保实时性和稳定性，系统以12.5 FPS的帧率在线运行，使用滑动窗口方法，预测窗口n设为8帧，并以4帧“站立”姿势初始化。</li>
<li><strong>动作表示与重定向</strong>：生成的人类动作序列基于SMPL模型（22关节骨架）。通过一个具有两个512单元隐藏层的MLP神经网络X执行逐帧重定向。该网络以人类姿势向量qSMPL为输入，学习提取核心运动意图，同时舍弃人类特有的关节连接方式，输出机器人所有29个驱动关节的目标状态。</li>
</ul>
<p><strong>3. 全身控制器 (πw)</strong><br>该低层控制器负责跟踪来自生成策略πp的参考动作，同时保持机器人平衡，采用强化学习实现。策略πw将观测ot（包含根状态、关节位置/速度、上一动作和生成参考yt）映射到目标关节角度at∈ℝ^29，随后由PD控制器驱动电机跟踪这些目标。训练数据集通过将AMASS动作优化重定向为Unitree G1兼容轨迹来构建。为避免过拟合，对训练数据集进行平衡重采样以实现均匀的手腕工作空间覆盖，并将πp生成的所有轨迹纳入训练数据以提供丰富的运动轨迹材料。</p>
<p><img src="https://arxiv.org/html/2506.01563v4/x3.png" alt="平衡的手腕工作空间分布"></p>
<blockquote>
<p><strong>图3</strong>：用于训练的平衡手腕工作空间分布。这确保了训练过程中手腕位置的多样性，提高了泛化能力。</p>
</blockquote>
<p><strong>创新点</strong>：与现有方法相比，HIAER的主要创新在于其完整的、基于层次化推理的闭环交互设计。它不仅仅推断功能意图，还通过VLM的上下文学习和CoT提示，显式地推理情感上下文（V-A），并将此信息作为可操作参数，系统地调制最终生成动作的表现风格，从而实现了社交感知与物理执行的深度集成。</p>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：</p>
<ul>
<li><strong>硬件</strong>：使用Unitree G1仿人机器人，头部配备Intel RealSense D405 RGB-D相机。计算在配备AMD Ryzen 9 9950X CPU和NVIDIA GeForce RTX 5090 GPU的工作站上进行。</li>
<li><strong>基准与数据集</strong>：在六个具有代表性的交互场景中进行评估，这些场景针对V-A空间的不同象限设计：S1攻击（象限I）、S2庆祝（象限II）、S3平静问候（象限III）、S4失望（象限IV）、S5中性、S6模糊（用于触发回退机制）。</li>
<li><strong>对比方法</strong>：设置了一个基线系统进行对比。该基线同样使用VLM以端到端方式选择动作⟨m⟩，但有意省略了HIAER中详细的、基于ICL的结构化推理模块。</li>
</ul>
<p><strong>关键实验结果</strong>：<br>实验从意图推断准确性、动作选择逻辑性、社交适当性感知以及系统延迟和鲁棒性多角度评估。</p>
<p><img src="https://arxiv.org/html/2506.01563v4/x4.png" alt="定性结果"></p>
<blockquote>
<p><strong>图4</strong>：六个代表性交互场景的定性结果。每个子图显示了人类的输入姿势（左）、机器人的物理响应（右）以及意图感知模块πi生成的中间输出。输出框详细说明了系统的内部推理，包括场景描述、推断意图、置信度得分、V-A估计和所选动作基元。</p>
</blockquote>
<p><em>表II展示了HIAER与基线在各场景下的性能指标。HIAER在意图推断准确率Iacc上普遍高于80%，在S2、S3、S4场景中达到93.3%。在动作选择逻辑性Sselect和情感适当性Saffect评分上，HIAER在多数场景中表现优异或与基线相当。</em><br><strong>关键发现</strong>：在意图明确的场景（如S2、S3），HIAER与基线均表现良好。然而，在具有挑战性的模糊场景S6中，HIAER的Saffect评分（3.89）显著高于基线（1.76），这得益于其V-A估计（Vavg=0.11, Aavg=0.29）引导系统选择了安全、中性的姿势，而基线则因缺乏情感上下文理解而产生了不适当的响应。这证明了层次化V-A推理在应对社交模糊性时的“安全网”作用。</p>
<p><img src="https://arxiv.org/html/2506.01563v4/x5.png" alt="生成动作多样性"></p>
<blockquote>
<p><strong>图5</strong>：由HIAER生成和执行的各种社交姿势。展示了文本-动作管道生成并在物理机器人上实时执行的一系列表现性动作，包括防御姿态、问候挥手、握手、指向、兴奋欢呼、双手叉腰、交叉双臂和双臂庆祝等。</p>
</blockquote>
<p><em>表III显示了实时流水线中各模块的延迟。意图感知模块πi的延迟为2.392秒，视频流为20Hz，结合动作生成与执行，系统能够实现流体交互。</em><br><strong>消融实验总结</strong>：通过对比HIAER（具备完整层次化推理和V-A估计）与基线（仅有关联VLM的端到端动作选择），实验实质上是对“层次化情感上下文推理”这一核心组件的消融。结果表明，该组件对于在社交意图模糊时保持响应的适当性和鲁棒性贡献显著，是HIAER框架超越简单端到端方法的关键。</p>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了一个层次化框架HIAER，首次将VLM对社交意图和情感上下文的细粒度推理与仿人机器人的表现性动作生成及执行集成在一个闭环中。</li>
<li>引入了效价-唤醒度估计，将复杂的社交意图参数化为可指导动作风格调制的连续变量，实现了情感上下文到物理表现的系统性映射。</li>
<li>在真实仿人机器人上实现并验证了完整系统，能够在具有挑战性的实时人机交互场景中产生低延迟、上下文适当的姿势。</li>
</ol>
<p><strong>局限性</strong>：论文提到，系统的性能在一定程度上依赖于VLM提示工程和少样本示例的质量。此外，生成动作的范围受到底层物理硬件（如关节极限、平衡约束）的限制。</p>
<p><strong>对后续研究的启示</strong>：HIAER的工作展示了将高层次社交智能（特别是情感理解）与低层次运动控制相结合的可行性和价值。未来的研究可以探索更动态、连续的情感状态建模，减少对精心设计提示的依赖，以及将框架扩展到更复杂的多轮交互和多人场景中。此外，如何进一步降低高层推理模块的延迟，以实现更即时自然的交互，也是一个重要的方向。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文针对人形机器人难以在实时交互中推断人类意图并生成上下文适配的、富有表现力的运动这一核心问题，提出了分层意图感知表达响应框架HIAER。该框架的关键技术在于：首先，利用基于上下文学习的视觉语言模型，同时推断交互的社交意图和情感上下文；然后，基于此生成适配风格的手势动作；最后，通过基于强化学习的全身控制器在物理机器人上稳健执行。真实世界实验表明，该系统生成的行为被评价为更具社交智能和上下文适当性，从而实现了更自然有效的人机交互。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2506.01563" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>