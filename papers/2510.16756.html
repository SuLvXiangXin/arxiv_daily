<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>End-to-end Listen, Look, Speak and Act - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Manipulation</span>
      <h1>End-to-end Listen, Look, Speak and Act</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2510.16756" target="_blank" rel="noreferrer">2510.16756</a></span>
        <span>作者: Chao Zhang Team</span>
        <span>日期: 2025-10-19</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>目前，追求通用人工智能的研究重点已转向能够与环境交互的具身智能体。人类智能的一个核心特征是<strong>全双工多模态交互</strong>能力，即我们能同时处理多种输入（如视觉、听觉）并产生多种输出（如语音、动作），并能实时适应对话中的轮流发言和打断等复杂动态。然而，现有AI模型在这一整体挑战上存在局限。一方面，全双工对话语音大模型（如Moshi, Freeze-Omni）能够实现低延迟的语音对话，并可以结合视觉信息进行视频对话，但它们本质上是“脱离实体的观察者”，无法将理解转化为与物理环境交互的动作。另一方面，视觉-语言-动作模型在机器人操作任务上取得了显著成功，但它们本质上是“聋的”和“哑的”，通常只能处理文本指令，缺乏处理原始听觉信号或生成语音响应的能力，且采用半双工、轮流式的交互范式，无法处理自然的对话行为。</p>
<p>本文旨在弥合这一鸿沟，提出了<strong>ELLSA</strong>模型，这是首个能够<strong>同时</strong>听、看、说、做的端到端模型。其核心思路是提出一种新颖的<strong>自注意力专家混合</strong>架构，将处理不同模态的预训练专家模块通过统一的注意力机制集成起来，在利用各专家强大能力的同时，减少模态干扰，实现流式、全双工的多模态输入输出。</p>
<h2 id="方法详解">方法详解</h2>
<p>ELLSA的整体目标是实现流式、全双工的多输入多输出交互。其核心创新在于<strong>SA-MoE</strong>架构以及基于交错序列的交互设计。</p>
<p><img src="https://arxiv.org/html/2510.16756v1/x1.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：（a）ELLSA概览。不同模态由不同专家处理，并通过SA-MoE架构集成以实现模态交互。（b）通过交错的时间多模态序列实现流式全双工MIMO交互。</p>
</blockquote>
<p><strong>整体流程与交互设计</strong>：ELLSA以1秒为一个时间块进行操作。在每个时间块内，它将来自不同模态的输入和输出组织成一个固定的交错序列：语音输入、图像输入、文本输出、动作输出。语音输出则直接从文本输出的嵌入中推导得出。每个模态段都用特定的起始和结束标记（如<code>&lt;bo_speech&gt;</code>）包裹以明确边界。这种设计使模型能够连续处理视觉和听觉输入，同时并行生成语音和动作，从而自主决定何时开始/停止说话或行动，实现了全双工流式交互。</p>
<p><strong>核心架构：SA-MoE</strong>：处理多模态（语音、视觉、文本、动作）的一个主要挑战是联合建模常导致性能下降，尤其是文本能力。SA-MoE通过专家分工与注意力集成来解决此问题。</p>
<p><img src="https://arxiv.org/html/2510.16756v1/x2.png" alt="SA-MoE机制"></p>
<blockquote>
<p><strong>图2</strong>：SA-MoE的工作机制。每个模态被路由到其指定的专家，跨模态交互通过注意力机制实现。在推理时，所有专家共享一个统一的KV缓存。</p>
</blockquote>
<ul>
<li><strong>专家分工</strong>：ELLSA采用两个专家。<strong>语音专家</strong>专门处理语音和文本（对话相关），<strong>动作专家</strong>专门处理视觉和动作（操作相关）。这种设计类似于将“嘴”和“手”分配给不同模块，有利于利用预训练知识（如将语音与文本、视觉与动作合并），并减少模态干扰。</li>
<li><strong>注意力集成</strong>：两个专家通过一个<strong>统一的注意力机制</strong>连接。从序列处理的角度看，整个MoE模型就像一个Transformer。在每一步，当前激活的专家会通过注意力机制访问一个<strong>共享的KV缓存</strong>，该缓存中包含了之前所有时间步由两个专家共同产生的键值对。这使得每个专家在专注于自身模态的同时，也能感知并理解来自其他模态的信息，实现跨模态的连贯理解。在任意时刻，只有一个专家的权重被激活，计算高效。</li>
</ul>
<p><strong>训练策略</strong>：采用三阶段渐进式训练策略，如图3所示。</p>
<p><img src="https://arxiv.org/html/2510.16756v1/x3.png" alt="训练策略"></p>
<blockquote>
<p><strong>图3</strong>：ELLSA的训练策略。首先训练独立专家，然后通过集成这些专家构建SA-MoE主干，最后连接语音合成器。</p>
</blockquote>
<ol>
<li><strong>阶段一：训练独立专家</strong>。语音专家由一个流式语音编码器（Mamba）与一个文本LLM（LLama-3.1-8B）通过适配器连接，在自动语音识别和语音问答任务上训练。动作专家直接使用预训练的UniVLA模型。</li>
<li><strong>阶段二：训练SA-MoE</strong>。将两个专家集成到SA-MoE框架中。在此阶段进行多任务训练，任务范围从基础能力（ASR、语音问答、语音条件机器人操作）到高级交互技能（边说边做、缺陷指令拒绝、动作打断、上下文视觉问答）。两个专家均使用LoRA进行微调。</li>
<li><strong>阶段三：连接语音合成器</strong>。将一流式语音合成器（基于CosyVoice2）以端到端方式连接到ELLSA，训练一个适配器将语音专家的隐藏状态映射给合成器，使模型获得说话能力。</li>
</ol>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：评估涵盖广泛使用的基准测试。语音交互任务在<strong>Llama Questions、Web Questions、TriviaQA和AlpacaEval</strong>上评估，报告语音到文本和语音到语音的性能。机器人操作任务在<strong>LIBERO</strong>基准（包含SPATIAL, OBJECT, GOAL, LONG四个子集）上评估，以任务成功率作为指标。此外，专门设计了高级能力评估任务，如对话/动作的轮流发言预测、缺陷指令拒绝、边说边做、动作打断和上下文视觉问答。</p>
<p><strong>对比基线</strong>：语音交互方面，对比了<strong>Moshi和Freeze-Omni</strong>等全双工语音交互大模型。机器人操作方面，对比了<strong>DP、Octo、OpenVLA、SpatialVLA、CoT-VLA、π₀-FAST</strong>等多种先进的VLA模型。</p>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><p><strong>基础能力</strong>：</p>
<ul>
<li><strong>语音交互</strong>：如表1所示，ELLSA在知识问答和开放对话基准上取得了与当前开源全双工模型相当的性能，并且在语音到语音交互上表现最佳（例如，在TriviaQA上S2S准确率达41.7%）。</li>
<li><strong>语音条件机器人操作</strong>：如表2所示，在LIBERO基准上，ELLSA取得了<strong>89.4%</strong> 的平均成功率，超越了所有对比的文本条件VLA模型。这证明了SA-MoE在模态集成上的有效性，使原本不熟悉语音输入的动作专家能成功执行语音指令。</li>
</ul>
</li>
<li><p><strong>高级能力</strong>：</p>
<ul>
<li><strong>全双工能力</strong>：如表3所示，ELLSA在对话和动作的轮流发言预测上成功率均达到或接近100%，优于对比的语音模型。对于四种类型的缺陷指令，拒绝率高达96.4%-100%。在“边说边做”任务中，能可靠区分普通问题、打断命令和静默输入，并做出正确响应（如对打断命令响应“Action Cancelled”并停止动作），成功率在94.3%-100%之间。</li>
<li><strong>边说边做能力</strong>：如表4所示，ELLSA能够同时处理语音问答和机器人操作。虽然与单独执行任一项任务相比性能有所下降（例如，LIBERO LONG子集成功率从84.4%降至73.2%），但这证明了其并发多模态生成的可行性。</li>
<li><strong>上下文视觉问答</strong>：如表5所示，在此需要结合当前动作状态回答视觉相关问题的任务中，ELLSA取得了约<strong>83%</strong> 的准确率。这表明，尽管语音专家从未单独训练过视觉数据，但通过SA-MoE的集成，它能够有效利用视觉信息进行回答。</li>
</ul>
</li>
<li><p><strong>SA-MoE有效性验证</strong>：论文在附录的消融实验中（对应表7）指出，与在全部任务上训练的单一密集模型相比，SA-MoE显著优于密集基线，证明了利用预训练专家减少模态干扰的优势。</p>
</li>
</ol>
<p><img src="https://arxiv.org/html/2510.16756v1/x4.png" alt="能力示例"></p>
<blockquote>
<p><strong>图4</strong>：ELLSA高级能力示例：从语音指令开始执行动作，进行上下文视觉问答，并支持动作打断。此实例展示了其MIMO能力和处理复杂对话动态的双工能力。</p>
</blockquote>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>SA-MoE</strong>，一种新颖的、数据高效的架构，通过统一的注意力机制集成不同模态的专家，在利用预训练能力的同时减轻模态干扰。</li>
<li>引入了<strong>ELLSA</strong>，首个在流式全双工框架下统一视觉、语音、文本和动作的端到端模型，实现了联合多模态感知和并发生成。</li>
<li>通过实证验证了ELLSA能够完成此前无法实现的任务，如对话与动作的轮流发言、缺陷指令拒绝、边说边做以及对动作打断的响应，展示了全双工多模态交互的可行性和重要性。</li>
</ol>
<p><strong>局限性</strong>：论文提到，在“边说边做”这类并发任务中，模型性能会出现可观察到的下降，表明同时处理多任务存在注意力分散的挑战。此外，当前架构只使用了两个专家，未来可探索更细粒度的专家配置。</p>
<p><strong>启示</strong>：SA-MoE架构为多模态处理提供了一个灵活且可扩展的框架，未来可方便地引入更多模态（如触觉、嗅觉）的专家。ELLSA的工作标志着向更自然、更通用的多模态交互智能迈出了一步，为追求更接近人类的通用人工智能提供了新的思路和技术路径。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出ELLSA模型，旨在解决AI难以实现人类式全双工多模态交互的核心问题。其关键技术是SA-MoE架构，通过自注意力主干将各模态路由至专用专家并融合，以统一架构支持跨视觉、文本、语音和动作的同步感知与生成。实验表明，该模型在语音交互与机器人操作任务上匹配了各模态独立基线的性能，并率先实现了对话与动作轮流、边说边做、指令拒绝等高级全双工行为。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2510.16756" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据抓取来源于 Robotics arXiv Daily</span>
    <span>本页由 GitHub Actions 定时更新</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>