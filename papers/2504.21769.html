<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>LLM-based Interactive Imitation Learning for Robotic Manipulation - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">← 返回列表</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">Robotics (cs.RO)</span>
      <h1>LLM-based Interactive Imitation Learning for Robotic Manipulation</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="http://arxiv.org/abs/2504.21769" target="_blank" rel="noreferrer">2504.21769</a></span>
        <span>作者: Werner, Jonas, Chu, Kun, Weber, Cornelius, Wermter, Stefan</span>
        <span>日期: 2025/04/30</span>
      </div>
      <section class="detail-body">
        <h2>📝 详细解读</h2>
        <h2 id="研究背景与动机">研究背景与动机</h2>
<p>机器人模仿学习（IL）是一种通过人类演示数据来训练智能体执行复杂任务的主流方法，例如行为克隆（BC）。然而，IL方法在序列决策问题中违反了训练数据独立同分布的假设，导致复合错误和数据分布偏移问题，限制了其性能。交互式模仿学习（IIL）通过允许智能体在训练过程中接收人类教师的实时反馈（包括评价性和纠正性反馈）来缓解上述问题，例如CEILing方法取得了先进性能。但IIL方法严重依赖人类教师的全程参与，成本高昂且耗时。</p>
<p>本文针对IIL对人类资源依赖过高的痛点，提出利用大语言模型（LLM）作为交互式教师的新视角。核心思路是：首先通过分层提示策略引导LLM生成任务特定的代码形式策略（CodePolicy），然后在智能体训练过程中，通过设计基于动作相似性的检查机制，让LLM间接地提供类人的评价性与纠正性反馈，从而以经济高效的方式提升智能体性能。</p>
<h2 id="方法详解">方法详解</h2>
<p>LLM-iTeach框架包含两个核心部分：LLM教师模块和IIL框架模块。整体流程是：在训练开始前，LLM教师通过分层提示生成CodePolicy；在训练过程中，智能体与环境交互，LLM教师通过比较智能体动作与CodePolicy建议的动作，动态提供反馈；智能体利用这些反馈数据持续优化自身的策略。</p>
<p><img src="https://arxiv.org/html/2504.21769v1/extracted/6393270/illustrations/Overview_IIL_LLMiTeach.png" alt="方法框架"></p>
<blockquote>
<p><strong>图1</strong>：通用IIL框架与LLM-iTeach的对比。在IIL中，人类教师观察智能体动作并提供及时反馈。而在LLM-iTeach中，首先通过提示让LLM将其推理分层封装成CodePolicy，然后通过设计的相似性检查机制，以评价或纠正的方式向智能体提供反馈。</p>
</blockquote>
<p><strong>核心模块1：LLM教师与分层提示</strong><br>为规避LLM直接推理的高延迟问题，LLM-iTeach在交互学习开始前，通过分层提示让LLM生成一个可执行的Python代码策略（CodePolicy）。该过程分为两个层次（如论文图2所示）：</p>
<ol>
<li><strong>规划器层</strong>：LLM接收包含环境物体列表和任务指令的描述，被要求将任务分解为多个步骤。</li>
<li><strong>函数生成层</strong>：LLM被提示生成两个关键函数。一是<code>action</code>函数，根据当前环境状态计算所需动作；二是<code>check</code>函数，验证当前步骤的指令是否仍然有效。若无效，则递增一个记录当前episode已达成步骤的整数变量。这些函数通过API访问环境状态和步骤变量。提示采用了少样本示例和思维链推理。</li>
</ol>
<p><strong>核心模块2：基于相似性的反馈机制</strong><br>在训练过程中，LLM教师通过执行CodePolicy来获得其认为在当前状态下的最优动作 (a_{llm})。将此动作与智能体策略输出的动作 (a_{agent}) 进行比较。相似性通过计算两个动作向量之间的夹角来判断（公式4）。若夹角小于阈值 (\beta)，则认为动作相似，提供<strong>评价性反馈</strong> (f_e)（即简单的“good”信号）；若夹角大于等于 (\beta)，则认为动作不相似，提供<strong>纠正性反馈</strong> (f_c)（即直接执行 (a_{llm}) 动作）。此机制确保了反馈的生成完全基于预先生成的CodePolicy，无需在训练中实时调用LLM，从而高效地模拟了人类教师的两种反馈类型。</p>
<p><strong>核心模块3：IIL框架与智能体策略学习</strong><br>智能体采用参数化的随机策略，其动作服从以神经网络 (f_{\theta}(s)) 输出为均值、固定方差 (\sigma^2) 的高斯分布（公式6），这种随机性有助于探索。网络架构（如论文图3所示）接收来自机器人手臂相机的状态信息。</p>
<p><img src="https://arxiv.org/html/2504.21769v1/extracted/6393270/illustrations/Agent.png" alt="智能体架构"></p>
<blockquote>
<p><strong>图3</strong>：LLM-iTeach中学习智能体的模型架构。包含处理状态输入的神经网络和输出动作的高斯分布。</p>
</blockquote>
<p>智能体通过优化损失函数（公式7）进行学习，该函数是策略下观察到的动作的负对数似然。关键创新在于为每个状态-动作对引入了权重 (q)（公式8）：</p>
<ul>
<li>当收到评价性反馈时，(q=1)，使用智能体自身的动作 (a_{agent}) 进行更新。</li>
<li>当收到纠正性反馈时，(q=N/N_c)，其中 (N) 是episode总步数，(N_c) 是该episode中纠正反馈的次数，并使用LLM教师的动作 (a_{llm}) 进行更新。<br>这种加权机制确保了即使在纠正反馈占主导的episode中，被纠正的动作对策略更新的影响也不会被稀释，与获得正面评价的动作保持平衡。此外，为避免LLM策略因未考虑运动学约束而陷入无限循环，设定了最大步数 (N_t)，超时的episode数据将被丢弃。</li>
</ul>
<h2 id="实验与结果">实验与结果</h2>
<p><strong>实验设置</strong>：在CoppeliaSim仿真器中，使用RLBench基准测试的机器人操作任务进行评估。机器人是6自由度的Franka Emika Panda。对比方法包括：行为克隆（BC）、使用人类教师的先进IIL方法CEILing，以及作为参考的仅用LLM直接控制（LLM Teacher）。为了公平比较，所有方法均使用由LLM生成的10条演示进行预热启动。除了复现CEILing的100个训练episode设置外，得益于LLM的低成本，还扩展测试了200、400、800个episode的情况。LLM采用Llama3-70B。</p>
<p><img src="https://arxiv.org/html/2504.21769v1/extracted/6393270/illustrations/All_Tasks_Screenshots_1_and_2.png" alt="任务截图"></p>
<blockquote>
<p><strong>图4</strong>：实验任务。顶行是与CEILing共享的四个任务，底行是新增的四个任务。</p>
</blockquote>
<p><strong>关键实验结果</strong>：</p>
<ol>
<li><strong>总体性能对比</strong>：在四个共享任务上，LLM-iTeach的平均成功率超过了BC，并与CEILing表现相当甚至略优（仅在CloseMicrowave任务上稍逊于CEILing）。具体数据如表1所示，例如在800个episode下，LLM-iTeach在StackWine任务上成功率达96.3%，高于BC的89.0%和CEILing的94.0%。</li>
<li><strong>扩展训练效果</strong>：如图5所示，随着训练episode数量增加，LLM-iTeach的性能持续提升，并在大多数任务上显著优于仅使用LLM演示进行训练的BC方法，证明了交互反馈的有效性。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.21769v1/extracted/6393270/results/LLM_teach_plot.png" alt="成功率曲线"></p>
<blockquote>
<p><strong>图5</strong>：BC和LLM-iTeach在不同训练episode数下的平均成功率。LLM Teacher的直接控制成功率作为参考。结果显示LLM-iTeach性能随训练增长，且优于BC。</p>
</blockquote>
<ol start="3">
<li><strong>消融实验</strong>：<ul>
<li><strong>反馈类型</strong>：如图7所示，同时使用评价性和纠正性反馈的完整LLM-iTeach方法，其性能优于仅使用其中一种反馈类型的变体，验证了两种反馈互补的重要性。</li>
<li><strong>预热启动</strong>：使用LLM生成的演示进行预热启动，相比随机初始化，能显著加速学习并提高最终性能。</li>
</ul>
</li>
<li><strong>超参数研究</strong>：如图8所示，对决定反馈类型的夹角阈值 (\beta) 进行研究。结果表明，(\beta = 10^{\circ}) 时性能最佳。角度过小会导致纠正反馈过多，限制了智能体的探索；角度过大则导致评价反馈过多，可能无法有效纠正错误。</li>
</ol>
<p><img src="https://arxiv.org/html/2504.21769v1/extracted/6393270/results/warm_no_warm_O.png" alt="消融实验"></p>
<blockquote>
<p><strong>图7</strong>：消融研究结果。显示了完整LLM-iTeach与仅使用评价反馈或仅使用纠正反馈的变体之间的性能对比，以及预热启动的影响。</p>
</blockquote>
<p><img src="https://arxiv.org/html/2504.21769v1/extracted/6393270/results/AngleHyperparameter.png" alt="超参数研究"></p>
<blockquote>
<p><strong>图8</strong>：夹角阈值 (\beta) 的超参数研究。在四个任务上，(\beta = 10^{\circ}) 时取得了最佳或接近最佳的平均成功率。</p>
</blockquote>
<ol start="5">
<li><strong>任务扩展性</strong>：在四个新增的复杂任务上测试LLM-iTeach，仅通过提供新的任务描述即可适应，并取得了良好的学习效果，证明了其易于扩展的潜力。</li>
</ol>
<h2 id="总结与启发">总结与启发</h2>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>提出了<strong>LLM-iTeach框架</strong>，首次利用LLM作为交互式教师来训练机器人操作任务中的模仿学习智能体，在取得与先进人类教师方法（CEILing）相当性能的同时，大幅降低了对人类资源的依赖。</li>
<li>设计了<strong>分层提示策略</strong>来生成代码策略（CodePolicy），以及<strong>基于动作相似性的反馈机制</strong>，使得LLM能够间接、高效地提供类人的评价性与纠正性反馈。</li>
<li>展示了该框架良好的<strong>可扩展性和适应性</strong>，仅需任务描述即可快速应用到新任务上。</li>
</ol>
<p><strong>局限性</strong>：论文指出，LLM生成的CodePolicy并未考虑机器人的逆运动学或环境的真实动力学（如物体精确尺寸、空间关系），这可能导致其建议的动作在实际中不可行或效率低下，需要通过设置最大步数 (N_t) 来中止无法完成的episode。</p>
<p><strong>研究启示</strong>：本工作证明了LLM作为经济高效、可扩展的“教师”在交互式学习中的巨大潜力。未来研究可以探索如何将物理约束更有效地集成到LLM的推理或代码生成过程中，或者将LLM-iTeach框架与强化学习等其他学习范式结合，以进一步突破LLM策略在低层控制精度上的限制。</p>

      </section>
      <section class="detail-tldr">
        <h2>💡 一句话总结</h2>
        <p>本文提出LLM-iTeach框架，旨在解决交互式模仿学习对人类教师的依赖和高成本问题。该方法采用分层提示策略引导LLM生成代码形式的策略，并设计基于相似性的反馈机制提供交互式纠正与评估。实验表明，在多种机器人操作任务上，LLM-iTeach的成功率超越行为克隆，并达到或优于使用人类教师的先进方法CEILing，证明了LLM作为高效、类人教师的潜力。</p>
      </section>
      <div class="detail-actions">
        <a href="http://arxiv.org/abs/2504.21769" target="_blank" rel="noreferrer" class="btn">查看 arXiv 原文</a>
        <a href="../index.html" class="btn btn-outline">返回列表</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>数据来源：<a href="https://jiangranlv.github.io/robotics_arXiv_daily/" target="_blank">Robotics arXiv Daily</a></span>
    <span>由 GitHub Actions 自动更新 · AI 摘要仅供参考</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>