{
  "generatedAt": "2026-02-11T12:40:38.691Z",
  "source": "https://jiangranlv.github.io/robotics_arXiv_daily/",
  "items": [
    {
      "id": "http://arxiv.org/abs/2602.09023",
      "title": "TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.09023",
      "arxivId": "2602.09023",
      "date": "2026-02-09",
      "authors": "Shanghang Zhang Team",
      "category": "Manipulation",
      "summary": "论文提出了一种名为TwinRL-VLA的数字孪生驱动强化学习框架，旨在解决现实世界中机器人操作任务中由于高昂的专家演示成本和不足的实际交互导致的Vision-Language-Action (VLA)模型泛化能力受限的问题。该方法通过智能手机捕捉场景高效重建高保真度的数字孪生环境，实现真实与模拟环境之间的双向传输。在监督微调（SFT）预热阶段，利用数字孪生扩展探索空间，增强数据轨迹分布的支持。基于此初始化，进一步提出了从模拟到现实的引导探索策略，显著提升了VLA模型在实际操作中的性能。实验结果表明，TwinRL-VLA有效提高了在线强化学习的探索效率和探索空间。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型在机器人操作任务中取得了显著进展，但这些模型仍然依赖于昂贵的专家演示和有限的真实世界交互。尽管在线强化学习（RL）在改进基础模型方面显示出潜力，但在真实世界的机器人操作中应用RL仍受到低探索效率和受限探索空间的限制。本文针对这一痛点，提出了TwinRL-VLA框架，通过数字孪生技术来扩展和引导VLA模型的探索空间。本文的核心思路是利用高保真度的数字孪生环境进行高效的并行在线RL，并通过数字孪生指导真实机器人的目标性探索。\n\n## 方法详解\n### 整体框架\nTwinRL-VLA的整体框架分为两个主要阶段：SFT预热阶段和在线RL阶段。在SFT预热阶段，通过数字孪生生成多样化的合成轨迹以扩展探索空间。在在线RL阶段，首先在数字孪生环境中进行高效的并行RL滚动，然后将这些数据用于真实世界的在线RL，从而加速训练过程。\n\n![方法框架](https://arxiv.org/html/2602.09023v1/x1.png)\n> **图1**：方法整体框架。左侧为SFT预热阶段，右侧为在线RL阶段。数字孪生环境用于生成多样化的合成轨迹，并在在线RL阶段提供高效的并行滚动。\n\n### 核心模块\n1. **数字孪生重建**：使用智能手机捕捉的场景视频，通过标准的3D Gaussian Splatting技术重建高保真的数字孪生环境。\n2. **探索空间扩展策略**：在SFT预热阶段，通过数字孪生生成多样化的合成轨迹，以扩展探索覆盖范围。\n3. **Sim-to-Real引导探索策略**：在数字孪生环境中进行高效的并行RL滚动，生成在线交互数据存储在重放缓冲区中。这些数据用于真实世界的在线RL，丰富了重放缓冲区，并减少了从离线到在线学习的过渡性能下降。\n\n### 创新点\n- 通过数字孪生技术生成多样化的合成轨迹，扩展了探索空间。\n- 在数字孪生环境中进行高效的并行在线RL，加速了真实世界的在线RL过程。\n- 通过数字孪生识别易失败但信息丰富的配置，指导真实机器人的目标性探索。\n\n## 实验与结果\n### 实验设置\n- **Benchmark/数据集**：四个不同的机器人操作任务，包括抓取、放置等。\n- **Baseline方法**：仅使用真实世界演示的SFT方法、现有的在线RL方法（如[8, 39]）。\n\n### 关键实验结果\n- **SFT预热阶段**：TwinRL-VLA在SFT预热阶段的平均成功率提高了42%。\n- **在线RL阶段**：TwinRL-VLA在分布内和分布外区域均达到了100%的成功率，相比现有方法至少提升了30%的速度，平均每个任务只需要约20分钟。\n\n![SFT预热阶段成功率对比](https://arxiv.org/html/2602.09023v1/x2.png)\n> **图2**：SFT预热阶段的成功率对比。TwinRL-VLA在SFT预热阶段的平均成功率提高了42%。\n\n![在线RL阶段成功率对比](https://arxiv.org/html/2602.09023v1/x3.png)\n> **图3**：在线RL阶段的成功率对比。TwinRL-VLA在分布内和分布外区域均达到了100%的成功率。\n\n### 消融实验\n- **数字孪生的作用**：去除数字孪生后，SFT预热阶段的成功率降低了30%，在线RL阶段的成功率降低了20%。\n- **Sim-to-Real引导探索策略**：去除该策略后，在线RL阶段的成功率降低了15%。\n\n## 总结与启发\n### 核心贡献\n1. 提出了TwinRL-VLA框架，通过数字孪生技术扩展和引导VLA模型的探索空间。\n2. 引入了探索空间扩展策略，通过数字孪生生成多样化的合成轨迹，扩展了探索覆盖范围。\n3. 提出了Sim-to-Real引导探索策略，通过数字孪生中的高效并行在线RL加速真实世界的在线RL过程。\n\n### 局限性\n- 数字孪生的重建需要高质量的输入数据，这可能在某些复杂环境中难以实现。\n- Sim-to-Real引导探索策略对数字孪生的准确性有一定依赖，如果数字孪生与真实环境存在较大差异，可能会导致性能下降。\n\n### 后续研究启示\n- 进一步优化数字孪生的重建过程，提高其在复杂环境中的适用性。\n- 探索更多有效的Sim-to-Real引导探索策略，进一步提升在线RL的效率和鲁棒性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09023v1/x1.png",
        "https://arxiv.org/html/2602.09023v1/x2.png",
        "https://arxiv.org/html/2602.09023v1/x3.png",
        "https://arxiv.org/html/2602.09023v1/x4.png",
        "https://arxiv.org/html/2602.09023v1/x5.png",
        "https://arxiv.org/html/2602.09023v1/x6.png",
        "https://arxiv.org/html/2602.09023v1/x7.png",
        "https://arxiv.org/html/2602.09023v1/x8.png",
        "https://arxiv.org/html/2602.09023v1/x9.png",
        "https://arxiv.org/html/2602.09023v1/x10.png",
        "https://arxiv.org/html/2602.09023v1/x11.png",
        "https://arxiv.org/html/2602.09023v1/x12.png",
        "https://arxiv.org/html/2602.09023v1/x13.png",
        "https://arxiv.org/html/2602.09023v1/x14.png",
        "https://arxiv.org/html/2602.09023v1/x15.png",
        "https://arxiv.org/html/2602.09023v1/x16.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09021",
      "title": "$χ_{0}$ : Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
      "url": "http://arxiv.org/abs/2602.09021",
      "arxivId": "2602.09021",
      "date": "2026-02-09",
      "authors": "Yibo Yuan Team",
      "category": "Manipulation",
      "summary": "本文针对高可靠性长时机器人操作中的分布不一致性问题，提出了一种资源高效的框架χ₀。该框架通过三个关键技术解决这一问题：（i）模型算术，一种权重空间合并策略，有效吸收不同演示的多样化分布；（ii）阶段优势，一种阶段感知的优势估计器，提供稳定、密集的进度信号；（iii）训练-部署对齐，通过时空增强、启发式DAgger校正和时间块平滑来弥合分布差距。实验表明，该方法使双臂机器人能够协同完成从展平、折叠到挂起不同衣物的长时任务，并表现出高可靠性的自主操作能力。",
      "detailedSummary": "## 研究背景与动机\n目前，高可靠性长时序机器人操作主要依赖大规模数据和计算资源来理解复杂的现实世界动态。然而，本文指出，阻碍现实世界鲁棒性的主要瓶颈不仅仅是资源规模，而是人类演示分布、策略学习的归纳偏差以及测试时执行分布之间的系统性不一致。这种不一致性会导致多阶段任务中的累积误差。针对这一痛点，本文提出了χ₀框架，通过有效模块实现生产级的机器人操作鲁棒性。\n\n## 方法详解\n### 整体框架\nχ₀框架旨在解决三个阶段的分布不一致性：训练分布（P_train）、模型分布（Q_model）和测试分布（P_test）。整体框架如图1所示。\n![方法框架](https://arxiv.org/html/2602.09021v1/x1.png)\n> **图1**：χ₀框架的整体结构。左侧为P_train，通过启发式DAgger和时空增强扩展训练覆盖范围，并进行阶段标注以估计优势；中间为Q_model，通过模型算术在权重空间中合并互补策略，由阶段感知的优势引导；右侧为P_test，通过时间块平滑确保执行准确性，同时通过在线DAgger实现闭环优化。\n\n### 核心模块\n#### 模型算术 (Model Arithmetic, MA)\nMA通过在权重空间中合并不同P_train分布训练的检查点，使策略能够高效吸收多样化的P_train分布。具体来说，MA通过合并多个模型的权重，使得Q_model能够捕捉到之前被忽略的P_train模式。实验表明，验证损失可以作为有效的权重选择启发式方法。\n\n#### 阶段优势 (Stage Advantage, SA)\nSA将长时序任务分解为语义子目标（阶段），提供稳定的阶段感知奖励信号，用于优势加权行为克隆。SA通过帧级奖励建模，解决了先前非阶段方法（如π₀.₆*）的数值不稳定问题。具体来说，SA通过直接从配对观察中预测优势，并将其条件化在语义阶段上，从而生成更平滑和稳定的监督信号。\n\n#### 训练-部署对齐 (Train-Deploy Alignment, TDA)\nTDA通过启发式DAgger和时空增强扩展P_train，使其更接近P_test，从而提高对实际分布漂移的鲁棒性。此外，TDA还提出了一种时间块平滑方法，以减轻推理-执行延迟并增强实时控制稳定性。实验表明，时空增强只有在与控制优化结合时才有效，而时间块平滑则与RTC方法正交。\n\n### 创新点\n- 通过模型算术在权重空间中合并不同P_train分布训练的检查点，提高了策略的泛化能力。\n- 通过阶段优势提供了稳定的阶段感知奖励信号，克服了数值不稳定问题。\n- 通过训练-部署对齐扩展了P_train，使其更接近P_test，提高了对实际分布漂移的鲁棒性。\n\n## 实验与结果\n### 数据集与实验平台\n本文在协作长时序衣物操作任务上进行了评估，包括展平、折叠和挂起不同衣物。这些任务涉及接触丰富的可变形动力学，放大了上述分布漂移。\n\n### 对比方法\n本文与开源基线π₀.₅进行了对比。\n\n### 关键实验结果\n- 在仅使用20小时的演示数据和8个A100 GPU的情况下，χ₀的成功率比π₀.₅提高了近250%。\n- MA提供了资源高效的机制，提升了几乎所有指标的性能。\n- TDA中的DAgger数据对于最大化成功率至关重要，尽管会增加重试成本。\n- SA的两步优势信号在数值稳定性方面优于π₀.₆*风格的优势训练，这在实验中转化为更好的整体性能。\n\n### 实验结果图表\n#### 成功率对比\n![成功率对比](https://arxiv.org/html/2602.09021v1/x3.png)\n> **图3**：χ₀与π₀.₅在成功率上的对比。χ₀在所有任务上的成功率显著高于π₀.₅。\n\n#### 重试成本对比\n![重试成本对比](https://arxiv.org/html/2602.09021v1/x4.png)\n> **图4**：χ₀与π₀.₅在重试成本上的对比。虽然χ₀的成功率更高，但其重试成本也相应增加。\n\n#### 消融实验\n![消融实验](https://arxiv.org/html/2602.09021v1/x5.png)\n> **图5**：消融实验结果。展示了每个组件对整体性能的贡献，其中MA和TDA对提升成功率最为关键。\n\n## 总结与启发\n### 核心贡献\n1. 提出了一个资源高效的框架χ₀，通过模型算术、阶段优势和训练-部署对齐，系统地解决了机器人操作中的分布不一致性问题。\n2. 实验结果表明，χ₀在成功率达到显著提升的同时，保持了良好的鲁棒性和稳定性。\n\n### 局限性\n论文提到，虽然χ₀在成功率上有显著提升，但重试成本也相应增加。此外，时空增强的有效性依赖于与控制优化的结合。\n\n### 后续研究启示\n- 进一步研究如何在保持高成功率的同时降低重试成本。\n- 探索更多有效的时空增强方法，以进一步提高对实际分布漂移的鲁棒性。\n- 将χ₀框架应用于更多类型的机器人操作任务，验证其通用性和有效性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09021v1/x1.png",
        "https://arxiv.org/html/2602.09021v1/x2.png",
        "https://arxiv.org/html/2602.09021v1/x3.png",
        "https://arxiv.org/html/2602.09021v1/x4.png",
        "https://arxiv.org/html/2602.09021v1/x5.png",
        "https://arxiv.org/html/2602.09021v1/x6.png",
        "https://arxiv.org/html/2602.09021v1/x7.png",
        "https://arxiv.org/html/2602.09021v1/x8.png",
        "https://arxiv.org/html/2602.09021v1/x9.png",
        "https://arxiv.org/html/2602.09021v1/x10.png",
        "https://arxiv.org/html/2602.09021v1/x11.png",
        "https://arxiv.org/html/2602.09021v1/figures/supp/loss_curve_SA.jpeg",
        "https://arxiv.org/html/2602.09021v1/x12.png",
        "https://arxiv.org/html/2602.09021v1/x13.png",
        "https://arxiv.org/html/2602.09021v1/x14.png",
        "https://arxiv.org/html/2602.09021v1/x15.png",
        "https://arxiv.org/html/2602.09021v1/x16.png",
        "https://arxiv.org/html/2602.09021v1/x17.png",
        "https://arxiv.org/html/2602.09021v1/x18.png",
        "https://arxiv.org/html/2602.09021v1/x19.png",
        "https://arxiv.org/html/2602.09021v1/x20.png",
        "https://arxiv.org/html/2602.09021v1/x21.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09017",
      "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models",
      "url": "http://arxiv.org/abs/2602.09017",
      "arxivId": "2602.09017",
      "date": "2026-02-09",
      "authors": "Nur Muhammad Mahi Shafiullah Team",
      "category": "Manipulation",
      "summary": "本文提出了一种名为接触锚定策略（Contact-Anchored Policies, CAP）的方法，通过物理接触信息来调节多模态策略。CAP能够在零样本情况下对新对象和场景进行泛化，并且在数据量、计算资源和模型参数方面比前沿行为模型少几个数量级的情况下，仍然在原子技能训练上表现出更好的性能。实验结果表明，CAP方法在多种任务中均优于现有的前沿行为模型。",
      "detailedSummary": "## 研究背景与动机\n目前机器人学习领域的主要范式是通过语言提示在运行时实现跨环境、实体和任务的泛化。然而，这种方法存在一个根本性的矛盾：语言往往过于抽象，难以指导具体物理操作所需的精确理解。本文提出了一种新的方法——接触锚定策略（Contact-Anchored Policies, CAP），用物理接触点代替语言条件，从而提高机器人在新环境和实体中的零样本泛化能力。CAP将策略分解为模块化的效用模型库，而不是单一的通用策略，这使得可以通过轻量级仿真平台EgoGym进行快速迭代和优化。\n\n## 方法详解\n### 整体框架\nCAP的整体框架包括数据收集、接触标注、预处理以及策略学习四个主要阶段。输入是带有接触点标注的视觉观测序列，输出是机器人执行的动作序列。\n\n![方法框架](https://arxiv.org/html/2602.09017v1/x2.png)\n> **图2**：CAP的数据标注、训练和推理过程。 (a) 在训练过程中，从数据中检测接触点并使用后见重标记轨迹。 (b) 在推理过程中，使用用户点击或基于用户命令的VLM来推导接触条件。在这两种情况下，接触令牌和视觉令牌被连接并传递给模型，作为输入预测动作。\n\n### 核心模块\n1. **数据收集与接触标注**：\n   - **硬件设计**：设计了一个低成本、3D打印的夹爪，兼容手持操作和机器人安装。该夹爪配备了一个iPhone 13 Pro作为主要传感器套件。\n   - **数据收集**：收集了三个主要任务的专家演示：Pick（拾取）、Open（打开）和Close（关闭）。最终数据集包含20,365个演示（23.1小时）。\n   - **数据预处理**：将RGB和深度图像调整为224×224，并对RGB-D观测和相应的相机里程计进行水平翻转增强。\n   - **后见接触标注**：定义接触锚点为3D坐标p，其中策略预期与对象交互。通过检测接触时间步、定义接触锚点并传播到所有先前的时间步来生成这些标签。\n\n2. **策略学习**：\n   - 使用Vector-Quantized Behavior Transformer (VQ-BeT) 实现条件模仿学习问题。VQ-BeT是一个两阶段算法，首先找到自监督离散动作表示，然后训练自回归变压器以预测给定观察序列的标记化动作。\n   - 对于每个时间步，将224×224 RGB输入嵌入到特征向量z_v∈ℝ^256中。接触锚点p_t∈ℝ^3线性投影到接触嵌入z_c∈ℝ^256。\n\n### 创新点\n- 用物理接触点代替语言条件，提高了机器人在新环境和实体中的零样本泛化能力。\n- 将策略分解为模块化的效用模型库，允许通过轻量级仿真平台EgoGym进行快速迭代和优化。\n- 通过后见接触标注技术，提高了数据利用率和模型性能。\n\n## 实验与结果\n### 数据集与实验平台\n- **数据集**：收集了20,365个演示（23.1小时）。\n- **仿真平台**：EgoGym，一个轻量级仿真基准，专注于对象和场景多样性，以速度换取逼真度。\n\n### 基准方法\n- 比较了当前最先进的视觉-语言-动作模型（如π_0.5）。\n\n### 关键实验结果\n- CAP在零样本评估中，在完全新颖的场景和对象上优于最先进的通用视觉-语言-动作模型，如π_0.5，提升了56%。\n- CAP仅使用23小时的人类演示数据，就能在三种基本操作技能（拾取、打开和关闭）上实现零样本泛化。\n\n![实验结果对比](https://arxiv.org/html/2602.09017v1/x12.png)\n> **图12**：CAP与基线方法在零样本评估中的成功率对比。CAP在所有任务上均显著优于基线方法。\n\n![消融实验](https://arxiv.org/html/2602.09017v1/x13.png)\n> **图13**：消融实验展示了各个组件的贡献。接触标注和模块化效用模型库对性能提升有显著贡献。\n\n### 消融实验\n- 接触标注：显著提高了模型的泛化能力和成功率。\n- 模块化效用模型库：允许通过轻量级仿真平台进行快速迭代和优化，进一步提高了模型性能。\n\n## 总结与启发\n### 核心贡献\n- 提出了接触锚定策略（CAP），用物理接触点代替语言条件，提高了机器人在新环境和实体中的零样本泛化能力。\n- 将策略分解为模块化的效用模型库，允许通过轻量级仿真平台EgoGym进行快速迭代和优化。\n- 通过后见接触标注技术，提高了数据利用率和模型性能。\n\n### 局限性\n- CAP目前仅在三个基本操作技能（拾取、打开和关闭）上进行了验证，未来需要扩展到更多复杂任务。\n- 轻量级仿真平台EgoGym虽然提高了迭代效率，但可能无法完全模拟真实世界的复杂性。\n\n### 后续研究启示\n- 进一步探索CAP在更多复杂任务上的应用，如多步骤操作和动态环境中的任务。\n- 结合其他感知模态（如触觉）进一步提高机器人的物理理解能力。\n- 优化仿真平台，使其更接近真实世界，以提高模型的泛化能力。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09017v1/x1.png",
        "https://arxiv.org/html/2602.09017v1/x2.png",
        "https://arxiv.org/html/2602.09017v1/x3.png",
        "https://arxiv.org/html/2602.09017v1/x4.png",
        "https://arxiv.org/html/2602.09017v1/figures/fig_4_test.jpg",
        "https://arxiv.org/html/2602.09017v1/x5.png",
        "https://arxiv.org/html/2602.09017v1/x6.png",
        "https://arxiv.org/html/2602.09017v1/x7.png",
        "https://arxiv.org/html/2602.09017v1/x8.png",
        "https://arxiv.org/html/2602.09017v1/x9.png",
        "https://arxiv.org/html/2602.09017v1/x10.png",
        "https://arxiv.org/html/2602.09017v1/x11.png",
        "https://arxiv.org/html/2602.09017v1/figures/robot_starting_poses_images.jpg",
        "https://arxiv.org/html/2602.09017v1/figures/robot_starting_poses_scatter.png",
        "https://arxiv.org/html/2602.09017v1/figures/egogym_picks.jpg",
        "https://arxiv.org/html/2602.09017v1/figures/egogym_open_close.jpg",
        "https://arxiv.org/html/2602.09017v1/x12.png",
        "https://arxiv.org/html/2602.09017v1/x13.png",
        "https://arxiv.org/html/2602.09017v1/figures/cap-pickup-objects.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.08602",
      "title": "Mimic Intent, Not Just Trajectories",
      "url": "http://arxiv.org/abs/2602.08602",
      "arxivId": "2602.08602",
      "date": "2026-02-09",
      "authors": "Panpan Cai Team",
      "category": "Manipulation",
      "summary": "该论文针对模仿学习（IL）在环境变化适应和技能迁移方面的不足，提出了一种新的方法“Mimic Intent, Not just Trajectories”（MINT）。MINT通过多尺度频域分词技术，将行为意图与执行细节分离。具体来说，它使用多尺度粗到细的结构来学习动作标记，其中最粗的标记捕捉低频全局结构，较细的标记编码高频细节。这种方法生成了一个抽象的意图标记，有助于规划和迁移，并生成了多尺度的执行标记，以适应环境动态。实验结果表明，MINT在多个操作基准测试和真实机器人上表现出色，具有更高的成功率、更优的推理效率、更强的抗干扰能力和有效的单次技能迁移能力。",
      "detailedSummary": "## 研究背景与动机\n当前主流的模仿学习（IL）方法，如视觉-语言-动作（VLA）模型，通过生成建模和预训练在灵巧操作任务中取得了显著的成功。然而，这些方法在适应环境变化和技能迁移方面仍存在局限性。本文认为，这些局限性源于仅模仿原始轨迹而未理解其背后的意图。为此，本文提出了一种新的视角：显式地将行为意图从执行细节中解耦。本文的核心思路是通过多尺度频域分词化，实现“模仿意图，而不仅仅是轨迹”（MINT），从而提高学习效率、泛化能力和一次性技能迁移。\n\n## 方法详解\n### 整体框架\nMINT框架包括两个主要部分：（1）频谱解耦动作分词器（SDAT），用于从演示轨迹中学习结构化的离散表示；（2）MINT策略，通过在学习到的分词空间中进行渐进的意图到执行推理来生成动作。SDAT提供了一个共享的动作码本和解码器，而MINT策略则学习以粗到细的方式预测动作分词，并将其解码为可执行的轨迹。\n\n![方法框架](https://arxiv.org/html/2602.08602v1/x2.png)\n> **图2**：MINT策略概述。（a）MINT自回归地预测K个时间尺度上的动作分词——从全局意图分词到高频执行分词——然后通过解码器映射到连续轨迹。（b）基于意图的动作集合确保了时间一致性和平滑的行为过渡，增强了长时任务的稳定性。\n\n### 核心模块\n#### SDAT分词器\nSDAT采用VQ-VAE架构，将动作块映射到分词。具体步骤如下：\n1. **动作编码器**：将输入的动作序列压缩为潜在嵌入。\n2. **频谱解码器**：将潜在嵌入重构为动作序列，并通过DCT转换为频域表示。\n3. **多尺度分解**：将动作分解为K个时间尺度，每个尺度具有逐渐增加的容量。最粗尺度（意图分词）包含一个分词，用于捕捉全局低频结构，而更细的尺度（执行分词）引入额外的分词来建模未被粗尺度解释的残差信息。\n4. **逐步重建**：模型首先使用最粗尺度进行重建，然后逐步使用更细尺度，直到所有K个尺度。这迫使粗尺度分词捕捉主导的低频成分，而细尺度分词专注于高频残差。\n\n#### MINT策略\nMINT策略由视觉-语言骨干和动作专家组成：\n1. **视觉-语言骨干**：编码视觉和语言输入。\n2. **动作专家**：自回归地预测从粗到细尺度的动作分词，并将预测的分词解码为连续轨迹。\n\n### 创新点\nMINT的关键创新在于通过多尺度频域分词化显式地解耦意图和执行细节。这种解耦使得MINT能够更好地适应环境变化，并实现一次性技能迁移。\n\n## 实验与结果\n### 数据集与实验平台\n本文在多个操作基准上进行了实验，包括LIBERO、MetaWorld、CALVIN和更具挑战性的LIBERO-Plus，以及在一个真实机器人系统上进行了测试。\n\n### 基线方法\n对比了多种基线方法，包括预训练的VLA模型（π0.5）、基于动作分词的方法（UniVLA）和经典的模仿学习方法（ACT、Diffusion Policy）。\n\n### 关键实验结果\n- 在标准基准上，MINT达到了最先进的性能，优于强基线方法。\n- 在LIBERO-Plus上，MINT在更强干扰下表现出更高的鲁棒性，成功率达到15%的提升。\n- 通过意图级别的表示，MINT实现了单次演示的一次性技能迁移，性能提高了60%。\n- 在真实机器人实验中，MINT只需要大约20次演示即可达到比最强基线（π0.5）高29%的性能。\n\n![实验设置](https://arxiv.org/html/2602.08602v1/experiment_setup.png)\n> **图3**：实验设置。展示了在不同基准和真实机器人系统上的实验配置。\n\n![实验结果](https://arxiv.org/html/2602.08602v1/exp_ret.png)\n> **图4**：实验结果。展示了MINT在不同基准上的成功率和性能提升。\n\n![消融实验](https://arxiv.org/html/2602.08602v1/x3.png)\n> **图5**：消融实验。展示了各个组件对性能的贡献。\n\n### 消融实验\n- 意图分词的存在显著提高了性能，特别是在长时任务和一次性技能迁移中。\n- 多尺度分解和逐步重建机制对于捕捉意图和执行细节的解耦至关重要。\n\n## 总结与启发\n### 核心贡献\n1. 提出了一种新的模仿学习框架MINT，通过多尺度频域分词化显式地解耦意图和执行细节。\n2. 实现了高效的意图到执行推理过程，提高了学习效率和泛化能力。\n3. 通过意图分词实现了单次演示的一次性技能迁移，显著提升了性能。\n\n### 局限性\n论文提到，尽管MINT在多个基准上表现优异，但在极端环境变化和复杂任务中的泛化能力仍有待进一步研究。\n\n### 后续研究启示\n未来的研究可以进一步探索如何在更复杂的环境中应用MINT，并结合其他先进的技术（如强化学习）来进一步提升性能。此外，还可以研究如何将MINT扩展到更多类型的任务和领域。",
      "imageUrls": [
        "https://arxiv.org/html/2602.08602v1/x1.png",
        "https://arxiv.org/html/2602.08602v1/x2.png",
        "https://arxiv.org/html/2602.08602v1/experiment_setup.png",
        "https://arxiv.org/html/2602.08602v1/exp_ret.png",
        "https://arxiv.org/html/2602.08602v1/x3.png",
        "https://arxiv.org/html/2602.08602v1/x4.png",
        "https://arxiv.org/html/2602.08602v1/x5.png",
        "https://arxiv.org/html/2602.08602v1/x6.png",
        "https://arxiv.org/html/2602.08602v1/x7.png",
        "https://arxiv.org/html/2602.08602v1/x8.png",
        "https://arxiv.org/html/2602.08602v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.08245",
      "title": "STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction",
      "url": "http://arxiv.org/abs/2602.08245",
      "arxivId": "2602.08245",
      "date": "2026-02-09",
      "authors": "Guohao Dai Team",
      "category": "Manipulation",
      "summary": "本文提出了一种名为STEP的轻量级时空一致性预测机制，旨在解决扩散策略在机器人视觉运动控制中因迭代去噪导致的高推理延迟问题。STEP通过生成高质量的初始动作，这些动作既接近目标动作的分布又具有时间一致性，从而在不牺牲原始扩散策略生成能力的前提下减少延迟。此外，还引入了速度感知扰动注入机制，自适应地调整动作激励以防止执行停滞。理论分析表明，该预测方法能诱导局部收缩映射，确保在扩散细化过程中动作误差收敛。实验结果表明，在RoboMimic基准测试和实际任务中，STEP仅用2步即可分别比BRIDGER和DDIM提高21.6%和27.5%的成功率，显著提升了推理延迟和成功率之间的帕累托前沿。",
      "detailedSummary": "## 研究背景与动机\n扩散策略（Diffusion policies）在机器人操作中的视觉-运动控制领域中崭露头角，因其能够建模动作序列的分布并捕捉多模态特性。然而，迭代去噪过程导致了显著的推理延迟，限制了实时闭环系统的控制频率。现有的加速方法包括减少采样步骤、直接预测或重用过去的动作，但这些方法往往难以同时保持动作质量和低延迟。本文针对这一痛点，提出了STEP，一种轻量级的时空一致性预测机制，以构建高质量的预热启动动作，既接近目标动作分布又具有时间一致性，且不损害原始扩散策略的生成能力。\n\n本文的核心思路是通过引入时空一致性预测机制和速度感知扰动注入机制，实现高效且高质量的动作生成，从而在保持动作质量的同时大幅降低推理延迟。\n\n## 方法详解\n### 整体框架\nSTEP的整体框架分为两个主要部分：时空一致性预测机制和速度感知扰动注入机制。输入为当前观测状态（如视觉输入或本体感觉状态），输出为高质量的动作序列。\n\n![方法框架](https://arxiv.org/html/2602.08245v1/x1.png)\n> **图1**：方法整体框架。左侧为时空一致性预测机制，右侧为速度感知扰动注入机制。\n\n### 核心模块\n\n#### 时空一致性预测机制\n该机制旨在生成高质量的预热启动动作，使其既接近目标动作分布又具有时间一致性。具体来说，它通过以下步骤实现：\n1. **时空一致性定义**：给定当前系统状态 \\( s_t \\) 和对应的目标动作分布 \\( p(a \\mid s_t) \\)，预热启动动作 \\( \\tilde{a}_t \\) 满足时空一致性，如果其同时满足时间一致性和空间一致性。\n   - 时间一致性：\\(\\|\\tilde{a}_t - a_{t-1}\\| \\leq \\epsilon_t, \\forall t\\)，其中 \\(\\epsilon_t\\) 是由系统动力学和控制频率决定的Lipschitz-like有界常数。\n   - 空间一致性：\\(\\mathrm{dist}(\\tilde{a}_t, \\mathcal{M}(s_t)) \\leq \\epsilon_s, \\forall t\\)，其中 \\(\\mathcal{M}(s_t)\\) 表示 \\( p(a \\mid s_t) \\) 的高概率动作流形，\\(\\epsilon_s\\) 控制允许的偏差。\n\n2. **预测网络**：使用一个轻量级的神经网络来预测预热启动动作 \\( \\tilde{a}_t \\)，该网络基于当前观测状态 \\( o \\) 和前一时刻的动作 \\( a_{t-1} \\) 进行预测。\n\n#### 速度感知扰动注入机制\n该机制通过自适应调节动作变化来防止执行停滞，特别是在实际任务中。具体来说：\n1. **扰动注入**：根据时间动作变化自适应地引入有界的激励扰动，仅在必要时引入，以防止执行停滞。\n2. **优化策略**：通过最小化条件噪声预测损失进行训练，确保扰动注入不会降低控制精度。\n\n### 创新点\n- **时空一致性预测**：同时考虑时间和空间一致性，生成高质量的预热启动动作。\n- **速度感知扰动注入**：自适应调节动作变化，防止执行停滞，提高控制稳定性。\n- **理论分析**：证明了所提出的方法可以诱导局部收缩映射，确保在后续扩散细化过程中动作误差的收敛。\n\n## 实验与结果\n### 数据集与实验平台\n本文在九个模拟基准测试和两个实际机器人任务上进行了广泛的评估。\n\n### 对比方法\n- BRIDGER\n- DDIM\n- CP\n- OneDP\n- RTI-DP\n- SDP\n- RNR-DP\n- Falcon\n\n### 关键实验结果\n- 在RoboMimic基准测试中，STEP在两步内实现了平均21.6%的成功率提升，超过了BRIDGER方法。\n- 在实际任务中，STEP在两步内实现了平均27.5%的成功率提升，超过了DDIM方法。\n\n![成功率对比](https://arxiv.org/html/2602.08245v1/x2.png)\n> **图2**：不同方法在RoboMimic基准测试上的成功率对比。STEP在两步内实现了显著的成功率提升。\n\n![消融实验](https://arxiv.org/html/2602.08245v1/x3.png)\n> **图3**：消融实验结果。展示了时空一致性预测机制和速度感知扰动注入机制对性能的贡献。\n\n### 消融实验\n- 时空一致性预测机制：显著提高了动作质量和成功率。\n- 速度感知扰动注入机制：有效防止了执行停滞，提高了控制稳定性。\n\n## 总结与启发\n### 核心贡献\n1. 提出了时空一致性预测机制，生成高质量的预热启动动作，同时保持时间和空间一致性。\n2. 引入了速度感知扰动注入机制，自适应调节动作变化，防止执行停滞。\n3. 提供了理论分析，证明了所提出的方法可以诱导局部收缩映射，确保动作误差的收敛。\n\n### 局限性\n- 本文方法在某些极端动态变化的任务中可能仍需进一步优化。\n- 速度感知扰动注入机制在特定场景下可能需要更精细的调整。\n\n### 后续研究启示\n- 进一步探索时空一致性预测机制在更多复杂任务中的应用。\n- 优化速度感知扰动注入机制，以适应更多种类的实际任务。\n- 结合其他先进的加速方法，进一步提高扩散策略的实时性能。",
      "imageUrls": [
        "https://arxiv.org/html/2602.08245v1/x1.png",
        "https://arxiv.org/html/2602.08245v1/x2.png",
        "https://arxiv.org/html/2602.08245v1/x3.png",
        "https://arxiv.org/html/2602.08245v1/x4.png",
        "https://arxiv.org/html/2602.08245v1/x5.png",
        "https://arxiv.org/html/2602.08245v1/x6.png",
        "https://arxiv.org/html/2602.08245v1/x7.png",
        "https://arxiv.org/html/2602.08245v1/x8.png",
        "https://arxiv.org/html/2602.08245v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07388",
      "title": "Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.07388",
      "arxivId": "2602.07388",
      "date": "2026-02-07",
      "authors": "Jianfei Yang Team",
      "category": "Manipulation",
      "summary": "本文针对长时机器人操作任务中由于视觉相似但需不同动作导致的多模态动作歧义问题，提出了一种基于扩散模型的轨迹聚焦策略（TF-DP）。该方法通过显式地将动作生成条件化于机器人的执行历史，利用历史运动轨迹提供阶段感知的上下文信息，并在视觉观察空间中突出与历史运动相关的任务相关区域，从而提高对背景视觉干扰的鲁棒性。实验结果表明，TF-DP在多模态动作歧义任务上比普通扩散策略提升了80.56%的性能，在视觉干扰条件下提升了86.11%的性能，同时仅增加了6.4%的运行时间。",
      "detailedSummary": "## 研究背景与动机\n当前，基于生成模型的策略在模仿学习中的机器人操作任务中表现出色，通过从演示中学习动作分布。然而，在长时序任务中，视觉上相似的观测值经常在不同的执行阶段出现，但需要不同的动作，这导致了多模态动作模糊（MA²）。本文针对这一具体痛点，提出了Trace-Focused Diffusion Policy (TF-DP)，通过显式地将动作生成条件化于机器人的执行历史来解决这个问题。核心思路是利用执行轨迹提供阶段感知的上下文，从而在视觉上相似的观测值下区分不同的动作。\n\n## 方法详解\n### 整体框架\nTF-DP的整体框架如图2所示。该方法首先收集历史机器人运动以创建运动轨迹，然后生成Trace-Focused Field，并将其投影到图像空间中，以解决MA²问题并减轻背景视觉干扰。具体来说，给定观测值 \\( o_t = \\{I_t^{\\mathrm{global}}, I_t^{\\mathrm{side}}, I_t^{\\mathrm{wrist}}, p_t^{\\mathrm{ee}}\\} \\) 和历史末端执行器轨迹 \\( \\mathcal{H}_t = \\{p_{\\tau}^{\\mathrm{ee}} \\mid \\tau \\leq t\\} \\)，TFF模块将轨迹从3D机器人空间投影到2D全局相机空间，得到轨迹图像 \\( I_t^{\\mathrm{trace}} \\)。然后，基于投影的2D轨迹在全局视图 \\( I_t^{\\mathrm{global}} \\) 上渲染一个Trace-Focused Field，生成增强的全局视图 \\( \\tilde{I}_t^{\\mathrm{global}} \\)。通过将去噪过程条件化于执行感知的观测值 \\( \\tilde{o}_t = \\{\\tilde{I}_t^{\\mathrm{global}}, I_t^{\\mathrm{side}}, I_t^{\\mathrm{wrist}}, I_t^{\\mathrm{trace}}, p_t^{\\mathrm{ee}}\\} \\)，TF-DP能够将视觉上相似的观测值与不同执行阶段的动作关联起来。\n\n![方法框架](https://arxiv.org/html/2602.07388v1/x2.png)\n> **图2**：TF-DP的整体框架。左侧为历史机器人运动的收集和轨迹生成，右侧为Trace-Focused Field的渲染和增强全局视图的生成。\n\n### 核心模块\n- **Trace-Focused Field (TFF) Rendering**：该模块将历史末端执行器轨迹从3D空间投影到2D全局相机空间，生成轨迹图像 \\( I_t^{\\mathrm{trace}} \\)。然后，基于投影的2D轨迹在全局视图 \\( I_t^{\\mathrm{global}} \\) 上渲染一个Trace-Focused Field，生成增强的全局视图 \\( \\tilde{I}_t^{\\mathrm{global}} \\)。\n- **Diffusion-based Action Generation**：通过将去噪过程条件化于执行感知的观测值 \\( \\tilde{o}_t \\)，TF-DP能够生成与当前执行阶段相匹配的动作。具体来说，TF-DP使用扩散模型进行迭代去噪，生成最终的动作分布。\n\n### 创新点\n- **执行轨迹的显式条件化**：通过引入执行轨迹，TF-DP能够在视觉上相似的观测值下区分不同的动作，从而解决MA²问题。\n- **Trace-Focused Field**：该字段强调与历史运动相关的任务相关区域，提高对背景视觉干扰的鲁棒性。\n\n## 实验与结果\n### 实验设置\n- **数据集**：使用了多个真实世界的机器人操作任务，这些任务具有显著的多模态动作模糊和视觉杂乱条件。\n- **基准方法**：对比了vanilla diffusion policy和其他现有的生成模型方法。\n\n### 关键实验结果\n- **成功率**：在具有多模态动作模糊的任务中，TF-DP比vanilla diffusion policy提高了80.56%的成功率。\n- **鲁棒性**：在视觉干扰条件下，TF-DP比vanilla diffusion policy提高了86.11%的鲁棒性。\n- **推理效率**：TF-DP仅增加了6.4%的运行时间，保持了高效的推理性能。\n\n![实验结果](https://arxiv.org/html/2602.07388v1/x3.png)\n> **图3**：在多模态动作模糊任务上的成功率对比。TF-DP显著优于vanilla diffusion policy。\n\n![鲁棒性对比](https://arxiv.org/html/2602.07388v1/x4.png)\n> **图4**：在视觉干扰条件下的鲁棒性对比。TF-DP在视觉干扰下表现更稳定。\n\n### 消融实验\n- **执行轨迹的影响**：移除执行轨迹后，TF-DP的成功率下降了约30%，表明执行轨迹在解决MA²问题中的关键作用。\n- **Trace-Focused Field的影响**：移除Trace-Focused Field后，TF-DP的鲁棒性下降了约20%，表明该字段在提高对背景视觉干扰的鲁棒性方面的重要性。\n\n## 总结与启发\n### 核心贡献\n- **识别并解决了多模态动作模糊问题**：通过引入执行轨迹，TF-DP能够有效解决长时序任务中的多模态动作模糊问题。\n- **提出了一种新的单策略框架**：TF-DP通过轻量级的历史条件化，实现了在单个策略内的时序一致性和鲁棒性。\n\n### 局限性\n- **计算开销**：虽然TF-DP在推理效率上表现良好，但在某些极端情况下，计算开销可能仍然较高。\n- **适用范围**：TF-DP主要适用于视觉上相似的观测值频繁出现的长时序任务，对于其他类型的任务可能效果有限。\n\n### 后续研究启示\n- **进一步优化计算效率**：可以探索更高效的轨迹表示和处理方法，以进一步降低计算开销。\n- **扩展应用场景**：未来可以将TF-DP应用于更多类型的机器人操作任务，验证其在不同场景下的泛化能力。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07388v1/x1.png",
        "https://arxiv.org/html/2602.07388v1/x2.png",
        "https://arxiv.org/html/2602.07388v1/x3.png",
        "https://arxiv.org/html/2602.07388v1/x4.png",
        "https://arxiv.org/html/2602.07388v1/x5.png",
        "https://arxiv.org/html/2602.07388v1/x6.png",
        "https://arxiv.org/html/2602.07388v1/x7.png",
        "https://arxiv.org/html/2602.07388v1/x8.png",
        "https://arxiv.org/html/2602.07388v1/x9.png",
        "https://arxiv.org/html/2602.07388v1/x10.png",
        "https://arxiv.org/html/2602.07388v1/x11.png",
        "https://arxiv.org/html/2602.07388v1/x12.png",
        "https://arxiv.org/html/2602.07388v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07341",
      "title": "Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions",
      "url": "http://arxiv.org/abs/2602.07341",
      "arxivId": "2602.07341",
      "date": "2026-02-07",
      "authors": "Zhuo Zou Team",
      "category": "Manipulation",
      "summary": "本文针对灵巧机器人手臂系统的可扩展操作学习问题，提出了一种基于增强现实（AR）的远程人机交互方法，以提高专家演示数据收集效率。该方法分为两个阶段：首先通过行为克隆（BC）方式预训练策略，利用AR系统收集的数据；其次，采用对比学习增强的强化学习（RL）方法进一步优化策略，并设计投影头加速学习过程。实验结果表明，与经典的近端策略优化和软演员-评论家策略相比，该方法不仅显著提高了推理速度，还在完成操作任务的成功率上表现更优。消融研究表明，提出的对比学习强化学习方法有效克服了策略崩溃问题。",
      "detailedSummary": "## 研究背景与动机\n当前主流的机器人学习方法主要依赖于行为克隆（Behavior Cloning, BC）和强化学习（Reinforcement Learning, RL）。然而，这些方法存在一些关键局限性。例如，BC 方法容易受到数据不匹配和累积误差的影响，而RL方法在高维观测下难以学习有效的表示。本文针对这些痛点，提出了一种基于增强现实（AR）远程人机交互的数据收集系统，并结合模仿学习和对比学习辅助的RL算法，以提高灵巧机器人操作任务的学习效率和性能。本文的核心思路是通过AR远程人机交互系统收集专家演示数据，并利用模仿学习进行预训练，然后通过对比学习辅助的RL方法进一步优化策略。\n\n## 方法详解\n### 整体框架\n本文提出的方法分为两个阶段：预训练阶段和RL训练阶段。\n- **预训练阶段**：通过行为克隆的方式，利用AR远程人机交互系统收集的专家演示数据进行预训练。\n- **RL训练阶段**：采用对比学习增强的RL方法，设计了一个投影头来加速学习过程，并引入事件驱动的增强奖励来提高安全性。\n\n### 核心模块\n1. **AR远程人机交互系统**：\n   - 该系统使用Unity平台在边缘服务器上运行，确保不同类型的AR头盔和机器人系统可以无线且远程连接。\n   - 专家佩戴AR头盔并通过摄像头捕捉其行为，这些行为被复制到灵巧机器人上，从而收集学习数据。\n\n2. **行为克隆预训练**：\n   - 通过解决一个简单的回归问题，利用收集到的专家演示数据进行预训练。\n   - 预训练的目标是使机器人能够模仿专家的行为，从而为后续的RL训练提供一个良好的初始策略。\n\n3. **对比学习增强的RL**：\n   - 设计了一个投影头，用于约束RL策略向高回报的专家状态-动作对靠拢。\n   - 引入了事件驱动的增强奖励，以提高系统的安全性和鲁棒性。\n\n### 创新点\n- **AR远程人机交互系统**：通过AR技术远程收集高质量的专家演示数据，提高了数据收集的效率和质量。\n- **对比学习增强的RL**：通过对比学习和投影头的设计，克服了传统RL方法中的策略崩溃问题，并显著提高了学习效率。\n\n![方法框架](https://arxiv.org/html/2602.07341v1/fig2revise.jpg)\n> **图1**：方法整体框架。左侧为AR远程人机交互系统，用于收集专家演示数据；右侧为模仿学习和对比学习增强的RL训练流程。\n\n## 实验与结果\n### 数据集与实验平台\n- **仿真环境**：使用PyBullet物理引擎进行仿真。\n- **真实世界实验**：在实际机器人平台上进行了验证。\n\n### 对比方法\n- Proximal Policy Optimization (PPO)\n- Soft Actor-Critic (SAC)\n\n### 关键实验结果\n- **成功率**：本文提出的方法在完成灵巧操作任务时的成功率显著高于PPO和SAC。\n- **训练时间**：本文方法的训练时间大约是SAC的四分之一，显著降低了训练时间。\n\n![实验结果对比](https://arxiv.org/html/2602.07341v1/fig5.jpg)\n> **图2**：不同方法在仿真环境中的成功率对比。本文方法（红线）显著优于PPO（蓝线）和SAC（绿线）。\n\n![消融实验](https://arxiv.org/html/2602.07341v1/convergence.jpg)\n> **图3**：消融实验结果。展示了行为克隆预训练和对比学习对最终性能的贡献。行为克隆预训练显著减少了训练时间，而对比学习进一步提高了模型性能。\n\n### 消融实验\n- **行为克隆预训练**：显著减少了RL训练时间。\n- **对比学习**：进一步提高了无模型RL的性能，克服了策略崩溃问题。\n\n## 总结与启发\n### 核心贡献\n1. **AR远程人机交互系统**：通过AR技术远程收集高质量的专家演示数据，提高了数据收集的效率和质量。\n2. **对比学习增强的RL**：通过对比学习和投影头的设计，克服了传统RL方法中的策略崩溃问题，并显著提高了学习效率。\n3. **实验验证**：在仿真和真实世界实验中，本文方法显著提高了灵巧操作任务的成功率，并大幅减少了训练时间。\n\n### 局限性\n- 本文方法仍需要一定数量的专家演示数据，尽管数量较少，但获取这些数据仍然需要时间和成本。\n- 在某些复杂环境中，AR远程人机交互系统可能受到硬件限制的影响。\n\n### 启示\n- AR技术在远程人机交互中的应用为机器人学习提供了新的数据收集途径。\n- 结合模仿学习和对比学习的RL方法为解决高维观测下的策略学习问题提供了新的思路。\n- 未来研究可以进一步探索如何减少对专家演示数据的依赖，以及如何在更复杂的环境中应用本文方法。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07341v1/fig1.jpg",
        "https://arxiv.org/html/2602.07341v1/fig1b.jpg",
        "https://arxiv.org/html/2602.07341v1/fig2revise.jpg",
        "https://arxiv.org/html/2602.07341v1/fig3.jpg",
        "https://arxiv.org/html/2602.07341v1/fig4a.png",
        "https://arxiv.org/html/2602.07341v1/fig4b.png",
        "https://arxiv.org/html/2602.07341v1/fig5.jpg",
        "https://arxiv.org/html/2602.07341v1/convergence.jpg",
        "https://arxiv.org/html/2602.07341v1/visualInterface.png",
        "https://arxiv.org/html/2602.07341v1/fig8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07326",
      "title": "Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing",
      "url": "http://arxiv.org/abs/2602.07326",
      "arxivId": "2602.07326",
      "date": "2026-02-07",
      "authors": "Seokhwan Jeong Team",
      "category": "Manipulation",
      "summary": "该论文探讨了在极简传感条件下实现可靠的多指抓取问题，仅依赖单轴指尖力反馈和关节本体感觉，无需视觉或高分辨率触觉传感器。研究采用了一种高效的教师-学生训练框架，其中强化学习的教师利用模拟中的特权观察生成演示，以提炼出基于Transformer的学生策略，该策略仅使用实际部署中可用的传感模式。实验结果表明，在18个物体上（包括分布内和分布外的情况），该方法实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。",
      "detailedSummary": "## 研究背景与动机\n目前，多指机器人抓取任务主要依赖于视觉、触觉和力传感器等多种感知模态。然而，这些方法存在一些关键局限性：视觉传感器容易受到遮挡和光照变化的影响，触觉传感器难以小型化且对光照敏感，而多轴力/扭矩传感器则成本高昂且难以集成。本文针对这些痛点，提出了一种仅依赖单轴指尖力反馈和关节本体感受的无视觉多指抓取框架。该方法通过一个高效的教师-学生训练管道，利用强化学习训练的教师生成演示数据，进而蒸馏出一个基于Transformer的学生策略，该策略在实际部署中仅使用可用的感知模态。本文的核心思路是通过极简的感知模态实现可靠的多指抓取。\n\n## 方法详解\n### 整体框架\n本文提出的框架包括两个主要阶段：教师策略训练和学生策略蒸馏。教师策略在仿真环境中利用特权观测进行训练，然后通过模仿学习将知识传递给学生策略。学生策略仅使用关节位置和单轴力输入来执行盲抓取任务。整体框架如图1所示。\n![方法框架](https://arxiv.org/html/2602.07326v1/x1.png)\n> **图1**：方法整体框架。左侧为教师策略训练阶段，右侧为学生策略蒸馏阶段。\n\n### 核心模块\n#### 教师策略训练\n教师策略 \\(\\pi_t\\) 在仿真环境中利用特权观测 \\(o_t^{\\text{priv}} \\in \\mathbb{R}^{95}\\) 进行训练。特权观测包括关节位置/速度、指尖姿态、物体姿态、物体线性和角速度、三轴指尖接触力、物体与夹爪中心的平面距离、单轴指尖力、六维指尖力矩以及前一动作。为了提高鲁棒性和模拟到现实的迁移，添加了高斯噪声（关节角度 \\(\\sigma = 0.005\\) 弧度，指尖力 \\(\\sigma = 0.5\\) 牛顿）。每个单轴力通过将三轴指尖接触力投影到指尖局部 z 轴并阈值化小幅度来计算。\n\n奖励函数包括任务奖励 \\(r_t\\)、激励奖励 \\(r_i\\) 和惩罚项 \\(r_l, r_a, r_{ar}\\)。任务奖励促进接触启动和平稳提升，激励奖励鼓励三个指尖同时接触，惩罚项防止关节限位违规、大动作和快速动作变化。奖励函数表达式如下：\n\\[ r = w_1 r_t + w_2 r_i + w_31 r_l + w_32 r_a + w_33 r_{ar} \\]\n其中 \\(w_1 = 1.0\\), \\(w_2 = 0.2\\), \\(w_31 = -500\\), \\(w_32 = -0.04\\), \\(w_33 = -0.01\\)。\n\n#### 学生策略蒸馏\n学生策略通过模仿学习从教师策略生成的演示数据中学习。学生策略仅使用关节位置和单轴力输入来执行抓取任务。学生策略采用Transformer架构，能够捕捉长时间范围内的依赖关系。学生策略的训练过程通过行为克隆或DAgger算法进行。\n\n### 创新点\n本文的主要创新点在于：\n1. 仅依赖单轴指尖力反馈和关节本体感受实现可靠的多指抓取。\n2. 通过教师-学生训练框架，利用特权观测生成高质量的演示数据，进而蒸馏出适用于实际部署的学生策略。\n3. 通过简化感知模态，显著降低了实际部署中的感知和集成要求。\n\n## 实验与结果\n### 实验设置\n本文在真实硬件上进行了验证，使用了一个定制的三指夹爪，安装在一个固定框架上。夹爪基于开源D'Claw操纵器设计，每个指尖配备单轴力传感器。实验对象包括18个几何形状不同的物体，分为6个分布内物体和12个分布外物体。\n\n### 对比方法\n本文对比了多种基线方法，包括仅使用视觉的方法、多模态融合方法以及其他无视觉抓取方法。\n\n### 关键实验结果\n本文在18个物体上实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。具体数值如下：\n- 分布内物体抓取成功率：99.5%\n- 分布外物体抓取成功率：97.2%\n\n### 实验结果图表\n![实验结果](https://arxiv.org/html/2602.07326v1/x4.png)\n> **图4**：不同方法在18个物体上的抓取成功率对比。本文方法在所有物体上均表现出色。\n\n![消融实验](https://arxiv.org/html/2602.07326v1/x5.png)\n> **图5**：消融实验结果。展示了各个组件对抓取成功率的贡献。\n\n### 消融实验\n消融实验表明，单轴力反馈和关节本体感受对于抓取成功至关重要。具体贡献如下：\n- 单轴力反馈：提高了抓取稳定性，减少了失败率。\n- 关节本体感受：提供了必要的位置信息，确保了抓取动作的准确性。\n\n## 总结与启发\n### 核心贡献\n1. 提出了一种仅依赖单轴指尖力反馈和关节本体感受的无视觉多指抓取框架，显著降低了感知和集成要求。\n2. 开发了教师-学生训练框架，利用特权观测生成高质量的演示数据，进而蒸馏出适用于实际部署的学生策略。\n3. 在真实硬件上验证了方法的有效性，实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。\n\n### 局限性\n本文方法虽然在多种物体上表现良好，但在极端情况下（如非常光滑或非常重的物体）可能仍存在挑战。此外，单轴力传感器的精度和可靠性也会影响抓取性能。\n\n### 后续研究启示\n未来的研究可以进一步探索更复杂的抓取任务，如动态环境下的抓取、多物体抓取等。此外，结合其他低成本传感器（如IMU）可能会进一步提高系统的鲁棒性和适应性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07326v1/x1.png",
        "https://arxiv.org/html/2602.07326v1/x2.png",
        "https://arxiv.org/html/2602.07326v1/x3.png",
        "https://arxiv.org/html/2602.07326v1/x4.png",
        "https://arxiv.org/html/2602.07326v1/x5.png",
        "https://arxiv.org/html/2602.07326v1/x6.png",
        "https://arxiv.org/html/2602.07326v1/x7.png",
        "https://arxiv.org/html/2602.07326v1/figures/Edgar.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/Junho.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/TaeminKim.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/Changjoo.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/Seokhwan.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.06512",
      "title": "Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.06512",
      "arxivId": "2602.06512",
      "date": "2026-02-06",
      "authors": "Heng Tao Shen Team",
      "category": "Manipulation",
      "summary": "这篇论文针对机器人模仿学习中训练数据呈现长尾分布的核心问题，即模型在数据丰富的头部任务上表现良好，但在数据稀缺的尾部任务上泛化能力差。研究发现，传统长尾学习策略（如重采样）对提升尾部任务性能效果有限，其根本原因是数据稀缺损害了策略的空间推理能力。为此，作者提出了“接近阶段增强”（APA）方法，通过将头部任务的知识迁移至尾部任务，无需外部演示数据。实验表明，APA方法在模拟和真实机器人操作任务中均能有效提升尾部任务的性能。",
      "detailedSummary": "## 研究背景与动机\n当前，通过大规模模仿学习训练通用机器人策略已成为主流范式，其通过在大量多样化的人类演示数据上训练，使机器人能够根据自然语言指令执行广泛的操作任务。然而，一个关键且常被忽视的挑战阻碍了这些模型的实际应用：演示数据天然存在的长尾分布。在大规模数据集中，少数常见的“头部”任务（如“拿起碗/盘子”）拥有大量演示，而绝大多数“尾部”任务（如“将酒瓶放在架子上”）仅有少量示例。在这种不平衡数据上训练的策略通常在尾部任务上表现不可靠。本文旨在解决这一核心痛点。\n\n现有的计算机视觉领域的长尾学习策略（如重采样、数据增强）在机器人策略学习中效果不佳。重采样仅复制现有数据，无法引入泛化所需的多样性；而如mixup等增强技术对状态和动作进行线性插值，忽略了底层动力学，常导致运动学上不可行或物理无效的轨迹。本文的核心问题是：在长尾演示上训练的策略性能下降的根本原因是什么？以及如何在不依赖外部演示数据的情况下解决此问题？\n\n通过深入分析，本文发现策略在尾部任务上失败的主要原因是其**空间推理能力**的退化。由于训练数据稀缺，模型无法学习尾部任务所需的精确空间关系，尽管它已从头部任务中掌握了大致概念。基于此关键洞察，本文提出了**Approaching-Phase Augmentation (APA)**，一种简单而有效的方案，通过利用头部任务的演示知识来增强尾部任务，从而提升策略在尾部任务上的空间推理能力，且无需外部数据。\n\n## 方法详解\n本文提出的Approaching-Phase Augmentation (APA)方法旨在通过从数据丰富的头部任务向数据稀缺的尾部任务转移知识，生成新的高质量训练样本。其整体流程分为三个步骤，如下图所示。\n\n![方法框架](https://arxiv.org/html/2602.06512v1/x3.png)\n> **图3**：Approaching-Phase Augmentation (APA)流程概览。包含三个步骤：(1) 头部任务轨迹分割，从头部任务中分离出目标接近阶段；(2) 尾部到头部对象嫁接，使用尾部任务对象创建增强轨迹；(3) 指令格式化与协同训练，格式化对应的语言指令，然后在组合数据集上训练策略。\n\n**整体流程：**\n1.  **输入**：原始的、不平衡的长尾演示数据集（包含头部和尾部任务的轨迹、图像和指令）。\n2.  **处理**：执行APA的三个步骤，生成仅包含目标接近阶段的增强轨迹及其对应指令。\n3.  **输出**：一个组合数据集，包含原始长尾数据以及新生成的增强数据，用于协同训练策略。\n\n**核心模块与技术细节：**\n1.  **头部任务轨迹分割**：从数据丰富的头部任务中随机选取成功执行的演示轨迹。通过监测机器人的本体感知（例如夹爪的打开/闭合状态），识别出机械臂正在接近目标的轨迹片段。这一步骤收集了一系列成功的“目标接近阶段”演示。\n2.  **尾部到头部对象嫁接**：此步骤通过将头部任务的轨迹片段中的原始对象替换为尾部任务的对象，来生成新的训练演示。在模拟的LIBERO基准测试中，这涉及直接替换对象资源。新对象的初始位置继承自源轨迹的原始对象，而其旋转方向则由尾部任务的目标指定。随后重新渲染场景以生成新的视觉数据。对于真实世界应用，可以通过YOLOv8识别对象并使用图像修复技术进行替换。这种重组操作使得目标接近阶段的训练数据多样化。\n3.  **指令格式化与协同训练**：为确保语言一致性，使用一对对应的模板来分别格式化原始轨迹和增强轨迹的语言指令。对于新生成的、仅包含目标接近阶段的增强轨迹，使用模板“approach the [target_object]”生成指令。为了在整个数据集中创建一致的两阶段格式，将所有原始演示的指令修改为“approach the [target_object] then [verb_phrase_1] and [verb_phrase_2]”。随后，策略在此组合的、语言一致的数据集上进行训练。\n\n**创新点：**\n与现有方法相比，APA的创新性体现在：\n*   **针对性**：聚焦于增强被识别为关键瓶颈的“目标接近阶段”，避免了生成完整、物理上合理的操作序列的复杂性。\n*   **高效性**：利用头部任务中已有的成功接近轨迹，通过对象替换快速生成高质量尾部任务数据，无需外部演示。\n*   **解耦性**：由于只涉及接近阶段，对替换对象的物理属性（如尺寸、质量）没有严格要求，这简化了数据生成过程。\n\n## 实验与结果\n**实验设置：**\n*   **基准/数据集**：在仿真实验中，使用基于LIBERO环境构建的**LIBERO-Core-LT**长尾数据集（包含10个任务）。在真实世界实验中，构建了**Real-World-LT**数据集（包含6个任务）。\n*   **实验平台**：仿真实验在LIBERO环境中进行；真实世界实验使用AGILEX PIPER 6-DOF机械臂。\n*   **基线模型**：仿真实验使用在LIBERO-90上预训练的miniVLA模型；真实世界实验使用在OXE真实世界数据上预训练的π0模型。\n*   **对比方法**：主要对比了直接在原始长尾数据上微调的**Baseline**与使用APA增强数据微调的**APA (Ours)**。此外，还评估了传统的重采样方法。\n\n**关键实验结果：**\n1.  **传统重采样方法无效**：如表II所示，采用不同重采样策略（q=0.75, 0.5, 0.25）后，策略在尾部任务上的成功率仅有个别微小提升（基线26.5%，最佳重采样27.1%），表明单纯增加尾部数据的曝光量无效。\n2.  **失败根因分析**：通过阶段式失败分析（表III）发现，与在全量数据上训练的模型相比，在长尾数据上训练的模型，其在尾部任务的“目标接近阶段”的失败相对风险高达**400.89%**，远高于“执行阶段”的164.34%。这证实了数据稀缺直接损害了策略的空间推理能力。\n3.  **APA方法有效性（仿真）**：\n    ![仿真结果](https://arxiv.org/html/2602.06512v1/x4.png)\n    > **图4**：APA方法在LIBERO-Core-LT数据集上的有效性。APA将平均成功率从基线的26.5%提升至**36.1%**，相对提升**36.2%**。不仅提升了尾部任务性能，头部任务也有所改善。\n4.  **APA方法有效性（真实世界）**：\n    ![真实世界结果](https://arxiv.org/html/2602.06512v1/x5.png)\n    > **图5**：APA方法在真实世界长尾任务上的有效性。APA将平均成功率从基线的39.1%提升至**54.1%**，相对提升**38.4%**，再次验证了其跨领域的有效性。\n5.  **消融实验**：\n    *   **组件分析（表IV）**：单独使用指令格式化或轨迹增强均无显著效果（成功率约26.0%-26.9%），只有两者结合（完整APA）才能实现性能飞跃（36.1%），说明视觉数据与语言指令的对应关系至关重要。\n    *   **增强数据量分析（表V）**：增加增强演示数量有益，但收益非单调。当每个任务的增强演示从0增加到9时，成功率先升至36.1%（6个时），后降至32.0%（9个时）。作者推测过量的、仅包含接近阶段的增强数据可能会扭曲训练数据分布。\n\n## 总结与启发\n**核心贡献：**\n1.  **问题诊断**：构建了基于LIBERO的长尾模仿学习基准，并通过深入分析首次揭示，长尾数据中尾部任务性能下降的根源在于策略**空间推理能力**的退化，尤其是在目标接近阶段。\n2.  **方法创新**：提出了**Approaching-Phase Augmentation (APA)**方法，这是一种简单、自包含的方案，通过将尾部任务对象“嫁接”到头部任务的成功接近轨迹上，生成高质量的增强数据，从而将知识从头部任务转移至尾部任务，且无需外部演示。\n3.  **全面验证**：在仿真和真实世界的大量多样化操作任务上进行了广泛实验，证实了APA能显著提升尾部任务性能，且对头部任务无害，甚至有益。\n\n**局限性：**\n论文自身提到，APA生成的增强数据仅包含目标接近阶段。消融实验表明，过度增加此类增强数据可能导致性能下降，因为这可能使训练分布偏离完整的任务轨迹。\n\n**后续研究启示：**\n1.  **阶段化学习**：本文强调了将操作任务分解为不同阶段（如接近、执行）进行分析和干预的价值，这为设计更精细的机器人学习算法提供了新思路。\n2.  **针对性数据增强**：针对已识别的特定能力瓶颈（如空间推理）进行数据增强，比通用的、可能违反物理规律的增强方法更为有效和可行。\n3.  **跨任务知识迁移**：利用数据丰富任务中的结构化知识（如成功的运动基元）来辅助数据稀缺任务的学习，是解决机器人学习长尾问题的一个有前景的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.06512v1/x1.png",
        "https://arxiv.org/html/2602.06512v1/x2.png",
        "https://arxiv.org/html/2602.06512v1/x3.png",
        "https://arxiv.org/html/2602.06512v1/x4.png",
        "https://arxiv.org/html/2602.06512v1/x5.png",
        "https://arxiv.org/html/2602.06512v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.06620",
      "title": "Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique",
      "url": "http://arxiv.org/abs/2602.06620",
      "arxivId": "2602.06620",
      "date": "2026-02-06",
      "authors": "Toshiaki Tsuji Team",
      "category": "Manipulation",
      "summary": "本文解决接触式任务中，如何从易得的位置轨迹生成适配特定硬件的精确力命令这一核心问题。提出了力生成模仿学习方法，通过无记忆的力生成模型结合反馈控制机制，将给定位置轨迹映射为力命令。实验表明，该方法确保了反馈控制的稳定性，有效提升了模型对未见位置轨迹的泛化能力，并在真实机器人书写任务中取得了性能改进。",
      "detailedSummary": "## 研究背景与动机\n在接触丰富的机器人任务中，位置轨迹通常易于获取，但合适的力命令往往是未知的。虽然可以考虑使用预训练的基础模型（如视觉-语言-动作模型）来生成力命令，但力控制高度依赖于机器人的特定硬件，这使得此类模型的应用面临挑战。模仿学习虽然能够学习复杂任务，但在接触丰富的任务中，通常只能处理非常静态的运动。基于双边控制的模仿学习能够收集人类操作者的力感知数据，但存在两个关键问题：同时生成位置和力命令会增加模型的维度，使训练更加困难；基于学习的运动生成方法通常比传统的基于模型的方法重现性低。本文旨在解决从给定位置轨迹生成力命令的问题，并通过引入反馈控制来提高鲁棒性和泛化能力。核心思路是提出一个分层的力生成模仿学习框架，将有记忆的上层与无记忆、可反馈控制的下层明确分离，使系统能够为未见过的位置轨迹生成有效的力命令。\n\n## 方法详解\n本文提出的方法是一个分层架构，旨在将有记忆的神经网络与无记忆的神经网络分离，以集成经典反馈控制。\n\n![方法框架](https://arxiv.org/html/2602.06620v1/x2.png)\n\n> **图2**：所提出的用于校正输出误差的分层模型。虚线框内为下层神经网络，其输入为当前状态和来自上层的未来轨迹（含PID校正项），输出为预测的下一状态和动作命令。PID控制器根据上层轨迹与预测状态的误差生成校正量，并反馈到上层轨迹输入中。\n\n整体框架如**图2**所示。上层（对应图中顶部的虚线）负责处理位置轨迹，以智能地预测未来状态。具体而言，上层输出从未来1步到10步的角度𝜽和角速度𝜽˙，并以10步为周期进行更新。输出中未来第10步的[𝜽_{k+10}, 𝜽˙_{k+10}]被传递给下层，并在下一个更新周期前保持恒定；在实际控制的每一步，使用对应的[𝜽, 𝜽˙]值。\n\n下层是一个无记忆的神经网络（图2中虚线框内部分）。其输入包括：10步后的上层轨迹[𝜽_{k+10}, 𝜽˙_{k+10}]、当前状态𝒔_k（关节角度、角速度和扭矩）。其输出是预测的下一状态𝒔̂_{k+1}和下一动作𝒂̂_{k+1}。由于下层仅根据当前状态和期望的未来轨迹进行插值预测，因此不需要内部记忆。这种结构消除了下层因保持内部状态而导致控制困难的隐患。由于𝒂̂_{k+1}包含力（扭矩）命令，该模型能够从位置轨迹生成力命令。\n\n核心创新在于将PID控制集成到该神经网络框架中，以补偿预测误差。具体流程如下：下层NN预测出状态𝒔̂_{k+1}（包含角度𝜽̂_{k+1}和角速度𝜽˙̂_{k+1}）和动作命令𝒂̂_{k+1}。𝒂̂_{k+1}被直接用作机器人的命令。同时，利用上层轨迹中对应的𝜽_{k+1}和𝜽˙_{k+1}，通过公式(1)计算PID控制误差𝒖_{k+1}。该误差经伪微分滤波后得到𝒖˙_{k+1}。随后，将𝒖_{k+1}和𝒖˙_{k+1}分别加到上层的角度𝜽_{k+10}和角速度𝜽˙_{k+10}上，形成校正后的上层角度𝜽^{upper}_{k+10}和角速度𝜽˙^{upper}_{k+10}，并作为新的输入传递给下层NN。这样，模型就能基于包含误差校正项的当前状态和上层轨迹来生成命令和响应值。\n\n与现有方法相比，本文的创新点具体体现在：1）明确提出了一个上层有记忆（处理长期轨迹）、下层无记忆（保证可控性）的分层架构；2）揭示了在反馈控制环内嵌入记忆（如下层使用LSTM）会阻碍稳定性，因此下层采用简单的多层感知机（MLP）；3）通过将PID控制误差反馈到上层轨迹输入中，使模型能够生成考虑轨迹预测误差的、更合适的力命令。\n\n## 实验与结果\n**实验设置**：使用CRANE-X7七自由度机械臂（固定第二关节，实际为6自由度）进行字符书写任务验证。通过双边控制收集了5种不同直线和2种不同圆形轨迹在两种白板高度（0cm和2cm）下的数据，共70组试验，其中56组用于训练，14组用于验证。控制频率为500 Hz。\n\n**对比方法**：\n1.  **直接教学回放**：将直接教学收集的响应值直接作为命令值回放。\n2.  **传统LSTM模型**：使用具有内部状态的LSTM作为下层网络（结构为6层LSTM+1层全连接）。\n3.  **提出的MLP模型**：下层使用无记忆的MLP（7层全连接，每层400维，激活函数为tanh）。\n\n**关键实验结果**：\n实验首先验证了PID增益的影响。**图7、8、9**分别展示了改变比例增益K_p、微分增益K_d和积分增益K_i时，笔尖x坐标轨迹的变化。\n\n![比例增益影响](https://arxiv.org/html/2602.06620v1/figures/Kp_x_comparison.png)\n> **图7**：不同比例增益K_p下笔尖x坐标轨迹。K_p=0时轨迹振荡；增大K_p能抑制振荡，使运动更贴合上层轨迹，但过大的K_p会导致扭矩参考值振荡和增大（见图10a），可能影响稳定性。\n\n![微分增益影响](https://arxiv.org/html/2602.06620v1/figures/Kd_x_comparison.png)\n> **图8**：不同微分增益K_d下笔尖x坐标轨迹。增加K_d未显著改善轨迹，当K_d=0.8或1.0时甚至出现局部振荡，扭矩参考值也出现振荡（图10b），这可能是由于连续时间伪微分计算引入了误差。\n\n![积分增益影响](https://arxiv.org/html/2602.06620v1/figures/Ki_x_comparison.png)\n> **图9**：不同积分增益K_i下笔尖x坐标轨迹。增加K_i在波形顶部减少了偏差，但在底部引入了新的波形峰值。扭矩参考值波形发生变化但未过度增大（图10c）。积分控制对NN参考值生成有影响，但也可能产生负面效果。\n\n![扭矩参考值对比](https://arxiv.org/html/2602.06620v1/x6.png)\n> **图10**：不同PID增益下基关节扭矩参考值对比。(a) K_p增大导致扭矩振荡和增大；(b) K_d增大也引起扭矩振荡；(c) K_i增大改变了扭矩波形但未导致过度增大。\n\n随后，在圆形、“A”、“B”字符书写任务中对比了不同方法。使用交并比（IoU）作为定量评估指标，比较实际绘制形状与上层轨迹形状的重合度。\n\n![模型轨迹对比](https://arxiv.org/html/2602.06620v1/x7.png)\n> **图11**：圆形绘制任务中各模型笔尖x坐标轨迹对比。提出的“MLP w/ PID”模型能最好地跟踪上层轨迹（黑色实线）。“Playback Direct Teaching”虽能跟踪轨迹但书写不准确（图1），“LSTM w/o PID”无法跟踪，“MLP w/o PID”跟踪不足。\n\n**表1** 展示了MLP模型在有/无PID控制下的IoU结果。结果显示，引入PID控制后，在所有测试高度（0cm, 1cm, 2cm）和字符类型上，IoU值均有显著提升，尤其是在1cm和2cm高度时提升更为明显。例如，在1cm高度书写“Character ABCD”时，IoU从0.165提升至0.270。\n\n![字符绘制结果对比](https://arxiv.org/html/2602.06620v1/x8.png)\n> **图12**：白板高度为1cm时，MLP无PID控制与有PID控制绘制的字符对比。可以直观看到，引入PID控制后（右列），字符书写的准确性和完整性得到明显改善。\n\n**消融实验总结**：实验通过对比LSTM和MLP模型，验证了下层网络无记忆对集成反馈控制的重要性（LSTM引入PID后性能未改善）。通过对比MLP在有/无PID控制下的表现，证明了引入PID反馈对于提高轨迹跟踪精度和任务性能的关键作用。PID各增益的调优实验则表明，适当的比例控制能有效改善跟踪，但微分和积分控制需谨慎使用，且当前的连续时间计算方式可能引入误差。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  提出了一个分层的力生成模仿学习框架，明确将有记忆的上层（负责长期轨迹预测）与无记忆、可反馈控制的下层（负责生成状态和力命令）分离开来。\n2.  该框架能够处理具有非平凡或定义不清控制目标的接触式操作任务（如书写），这类任务难以用传统的阻抗或混合控制解决，同时仍能通过经典反馈控制确保稳定性。\n3.  揭示了在反馈控制环内嵌入记忆会阻碍稳定性，并通过实验证明，采用时间尺度分离（上层慢速、有记忆，下层快速、无记忆）可以使基于学习的力生成同时实现鲁棒性和稳定性。\n\n论文自身提到的局限性主要在于PID控制的实现细节。文中指出，微分和积分计算采用了连续时间方法，这可能引入计算误差并对神经网络产生负面影响，未来需要引入离散控制技术来考虑这些计算误差。\n\n本文对后续研究的启示在于：将学习与经典控制结合时，模型的可控性是需要重点考虑的设计因素。明确分离系统的“记忆”部分和“反应”部分，并确保直接与底层控制交互的部分是简单、无记忆且易于分析的，可能是一种有效的架构设计原则。此外，该方法展示了如何利用易于获取的位置轨迹（可来自直接教学、VLA模型等）来生成硬件适配的精细力命令，为将高层智能规划与底层柔顺执行相结合提供了一条可行路径。",
      "imageUrls": [
        "https://arxiv.org/html/2602.06620v1/x1.png",
        "https://arxiv.org/html/2602.06620v1/x2.png",
        "https://arxiv.org/html/2602.06620v1/figures/task_env.png",
        "https://arxiv.org/html/2602.06620v1/x3.png",
        "https://arxiv.org/html/2602.06620v1/x4.png",
        "https://arxiv.org/html/2602.06620v1/x5.png",
        "https://arxiv.org/html/2602.06620v1/figures/Kp_x_comparison.png",
        "https://arxiv.org/html/2602.06620v1/figures/Kd_x_comparison.png",
        "https://arxiv.org/html/2602.06620v1/figures/Ki_x_comparison.png",
        "https://arxiv.org/html/2602.06620v1/x6.png",
        "https://arxiv.org/html/2602.06620v1/x7.png",
        "https://arxiv.org/html/2602.06620v1/x8.png",
        "https://arxiv.org/html/2602.06620v1/figures/compare_ordinalmodel.png",
        "https://arxiv.org/html/2602.06620v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07082",
      "title": "MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation",
      "url": "http://arxiv.org/abs/2602.07082",
      "arxivId": "2602.07082",
      "date": "2026-02-06",
      "authors": "Wei Gao Team",
      "category": "Manipulation",
      "summary": "论文针对嵌入式AI中视觉语言模型空间推理能力弱的问题，尤其是在跨帧复杂空间关系任务上，提出了MosaicThinker技术。该方法通过迭代构建空间表示，将多帧碎片化信息整合为统一的全局语义地图，并利用视觉提示引导VLM进行推理。实验结果表明，该技术能极大地提高资源受限设备上跨帧空间推理的准确性，适用于各种类型和复杂度的任务。",
      "detailedSummary": "## 研究背景与动机\n当前，具身AI应用正从传统的物体检测与识别扩展到机器人操纵和驱动规划等更高级的任务。在这些任务中，视觉语言模型需要具备三维空间推理能力，以理解物体的空间关系并指导设备行动。然而，现有的视觉语言模型由于缺乏对3D空间信息的了解，在空间推理方面能力非常薄弱，尤其是在涉及跨多个视频帧的复杂空间关系推理任务时。这种局限性在小型的设备端VLM上更为突出，因为它们的表征能力有限。此外，增强VLM空间推理能力的主流方法，如注入深度图或生成密集的鸟瞰图，要么在跨帧对齐上表现不佳，要么对于小型VLM来说过于复杂难以解读。\n\n本文针对小型设备端VLM在复杂的跨帧视觉空间推理任务上能力不足这一具体痛点，提出了一种新的推理时计算技术。其核心思路是：不重新训练VLM，而是通过整合多帧碎片化的空间信息，迭代构建一个统一的、稀疏的全局语义地图作为空间表示，并设计视觉提示来引导VLM基于该语义地图进行推理，从而突破小型VLM自身表征能力的限制。\n\n## 方法详解\nMosaicThinker的整体流程旨在处理以自我中心视角拍摄的场景视频，并回答用户关于物体空间关系的自然语言问题。其核心包含两个主要组件：语义地图的迭代构建和关键帧选择。\n\n![方法总览](https://arxiv.org/html/2602.07082v1/x6.png)\n> **图6**：MosaicThinker设计总览。系统首先通过任务接地确定任务相关物体列表，然后通过迭代的关键帧选择机制采样视频帧。选出的关键帧经过空间信息提取和跨帧对齐，迭代构建出全局语义地图。最后，该语义地图通过精心设计的视觉提示输入给设备端VLM，以完成空间推理。\n\n**1. 预处理与任务接地**：给定推理任务，系统首先使用VLM，结合任务问题和随机采样的几帧视频，生成一个按相关性和出现可能性排序的任务相关物体自然语言列表。该列表包括问题中明确提到的目标物体和作为上下文线索的路标物体。同时，为提升效率，系统会通过轻量级过滤技术（如下采样、运动跟踪）检测并移除输入视频中的冗余帧。\n\n**2. 语义地图的迭代构建**：构建语义地图的目标是将相机和所有物体的位置、姿态映射到一个全局稀疏网格中。由于小型VLM无法直接从图像准确推断3D信息，MosaicThinker采用专门的AI模型（如分割模型、深度估计模型）从单帧提取物体的3D点云信息。关键挑战在于将这些来自不同视角的碎片化信息对齐到一个统一的全局坐标系中。\n\n*   **跨帧对齐**：系统通过预训练的图像匹配模型（如MatchAnything）在RGB图像间建立像素对应关系，并利用这些匹配点（特别是位于任务相关物体边界框内的点）计算帧间的相机变换矩阵（平移和旋转）。为了高效，每个待对齐帧仅与已对齐帧中最相似的帧进行匹配，相似度通过PSNR或SSIM等轻量指标计算。\n*   **多帧全局对齐**：为避免传统顺序集成方法导致的累积误差和匹配失败，MosaicThinker采用拓扑感知的对齐策略。它将视频帧组织成以某个“全局锚点”帧为根的树状结构，树边代表帧间最大视觉重叠。通过计算到根节点的唯一路径来推导全局位姿，从而仅使用高置信度匹配对，有效防止误差传播。\n*   **处理遮挡**：对齐所有帧并估算每帧相机位姿后，系统可以推断哪些本应在视野内但被分割模型遗漏的物体。这些信息可用于引导分割模型恢复被遮挡或部分可见的物体。\n\n**3. 关键帧选择**：使用所有视频帧构建语义地图既昂贵又可能引入语义噪声。MosaicThinker通过迭代的时间搜索过程选择最相关的关键帧子集。在每一轮迭代中，系统仅根据一个采样分布对少量帧进行评分（评分基于预训练物体检测器对任务相关物体的检测置信度），并选择得分高于阈值的帧作为关键帧。采样分布初始为均匀分布，随后根据物体出现的时序局部性进行迭代优化：如果一个帧得分高，则通过高斯核函数提升其前后相邻帧被采样的概率。关键帧的数量、迭代次数和评分阈值根据设备计算能力和推理延迟要求预先定义。\n\n**4. 通过视觉提示进行空间推理**：构建好的语义地图以视觉提示的形式输入给VLM。如图9所示，该提示将语义地图表示为一个俯视网格，使用简单符号和不同颜色的边界框来示意物体的相对位置，并辅以描述每个物体边界框在语义图中绝对位置的文本。\n\n![视觉提示](https://arxiv.org/html/2602.07082v1/x9.png)\n> **图9**：从对齐的3D点云构建视觉提示。语义地图被表示为带有符号和彩色边界框的俯视图，并附有文本描述。\n\n与现有方法相比，MosaicThinker的创新点具体体现在：1) 提出了一个**稀疏的、语义化的全局空间表示**（语义地图），而非密集的BEV地图，更适合小型VLM理解；2) 采用**迭代的、拓扑感知的跨帧对齐策略**构建该表示，有效整合碎片化信息并避免误差累积；3) 设计了**基于迭代采样的高效关键帧选择机制**，在保证信息充分的同时最小化计算开销。\n\n## 实验与结果\n**实验设置**：研究在多个设备端AI平台（NVIDIA Jetson Orion, Meta AR Glass, OnePlus 12R手机）上实现了MosaicThinker。评估使用了多个空间推理基准测试，涵盖不同类型的室内场景（住宅、办公室、图书馆等），包括SpatialVQA、CLEVRER和ScanNet。任务类型聚焦于物体关系、位置识别和相机运动估计等跨帧空间推理。\n\n**对比方法**：对比的基线方法包括：1) 直接使用VLM（Qwen2.5-VL， LLaVA-NeXT）；2) 使用深度图增强VLM；3) 使用BEV地图增强VLM；4) 使用场景图重建；5) 其他先进的视频理解模型（Video-LLaVA， Video-ChatGPT）。\n\n**关键实验结果**：\n![总体性能对比](https://arxiv.org/html/2602.07082v1/x13.png)\n> **图13**：在SpatialVQA基准测试上的总体性能。MosaicThinker（橙色）在三种任务类型（物体关系、位置识别、相机运动）上均显著优于所有基线方法，平均准确率达到78.2%，比最佳基线（BEV地图）提升13.4%。\n\nMosaicThinker显著提升了困难跨帧空间推理任务的准确率。在SpatialVQA基准上，其准确率达到78.2%，比最佳基线方法（BEV地图）高出13.4%，在某些任务类型上提升幅度高达40%。在CLEVRER和ScanNet基准上也观察到一致的显著提升。\n\n![消融实验](https://arxiv.org/html/2602.07082v1/x14.png)\n> **图14**：消融实验结果。依次移除关键帧选择（KS）、跨帧对齐（CA）和语义地图（SM）组件，性能逐级下降。其中，语义地图的贡献最大，其缺失导致性能下降22.5%。\n\n**消融实验**：如图14所示，移除关键帧选择、跨帧对齐和语义地图组件均会导致性能下降。其中，**语义地图的贡献最大**，其缺失使准确率下降22.5%，证明了统一空间表示的核心作用。跨帧对齐和关键帧选择分别贡献了约7%和4%的性能提升。\n\n![定性结果](https://arxiv.org/html/2602.07082v1/x15.png)\n> **图15**：定性结果对比。MosaicThinker能够正确推理出“蓝色笔记本在书架左边”，而直接使用VLM和BEV地图方法均给出了错误答案，显示了其在复杂跨帧关系推理上的有效性。\n\n![关键帧选择分析](https://arxiv.org/html/2602.07082v1/x16.png)\n> **图16**：关键帧选择过程分析。经过3轮迭代，采样分布（蓝色曲线）成功聚焦到高似然分数（红色曲线）的视频片段，高效地定位了包含任务相关物体的关键帧。\n\n其他实验结果图表进一步展示了方法的有效性：\n![效率分析](https://arxiv.org/html/2602.07082v1/x17.png)\n> **图17**：不同设备上的推理延迟。MosaicThinker在保持高精度的同时，引入了可接受的额外计算开销。\n\n![对齐方法对比](https://arxiv.org/html/2602.07082v1/x11.png)\n> **图11**：本文的图像匹配对齐方法（右）与传统的ICP点云对齐方法（左）对比。本文方法避免了无关背景干扰，实现了更准确的对齐。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了 **MosaicThinker**，一个**无需重新训练**的推理时计算框架，首次在小型设备端VLM上实现了复杂的跨帧视觉空间推理；2) 设计了**迭代构建稀疏全局语义地图**的方法，通过拓扑感知的跨帧对齐有效整合多视角碎片化空间信息，形成了适合小型VLM理解的统一空间表示；3) 引入了**高效的关键帧选择机制**，通过迭代采样聚焦于信息丰富的视频片段，在保证推理质量的同时优化了计算效率。\n\n论文自身提到的局限性包括：1) 主要处理室内场景，且假设相机运动速度适中、遮挡不频繁；2) 专注于物体间的空间关系推理，而非精确的尺寸和距离测量；3) 其性能依赖于现有的分割、深度估计等感知模型的准确性。\n\n这项工作对后续研究的启示在于：为增强小型模型在复杂任务上的能力提供了一条“**赋能而非替代**”的新路径，即通过精心设计的外部表示和推理时计算来弥补模型内在表征能力的不足。未来工作可探索将语义地图构建方法扩展到更动态或更复杂的室外场景，与其他模态（如惯性传感）结合以提升对齐鲁棒性，并进一步优化关键帧选择和对齐算法的计算效率，以适用于资源更受限的边缘设备。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07082v1/x1.png",
        "https://arxiv.org/html/2602.07082v1/x2.png",
        "https://arxiv.org/html/2602.07082v1/x3.png",
        "https://arxiv.org/html/2602.07082v1/x4.png",
        "https://arxiv.org/html/2602.07082v1/x5.png",
        "https://arxiv.org/html/2602.07082v1/x6.png",
        "https://arxiv.org/html/2602.07082v1/x7.png",
        "https://arxiv.org/html/2602.07082v1/x8.png",
        "https://arxiv.org/html/2602.07082v1/x9.png",
        "https://arxiv.org/html/2602.07082v1/x10.png",
        "https://arxiv.org/html/2602.07082v1/x11.png",
        "https://arxiv.org/html/2602.07082v1/x12.png",
        "https://arxiv.org/html/2602.07082v1/x13.png",
        "https://arxiv.org/html/2602.07082v1/x14.png",
        "https://arxiv.org/html/2602.07082v1/x15.png",
        "https://arxiv.org/html/2602.07082v1/x16.png",
        "https://arxiv.org/html/2602.07082v1/x17.png",
        "https://arxiv.org/html/2602.07082v1/x18.png",
        "https://arxiv.org/html/2602.07082v1/x19.png",
        "https://arxiv.org/html/2602.07082v1/x20.png",
        "https://arxiv.org/html/2602.07082v1/x21.png",
        "https://arxiv.org/html/2602.07082v1/x22.png",
        "https://arxiv.org/html/2602.07082v1/x23.png",
        "https://arxiv.org/html/2602.07082v1/x24.png",
        "https://arxiv.org/html/2602.07082v1/x25.png",
        "https://arxiv.org/html/2602.07082v1/x26.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.05468",
      "title": "TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation",
      "url": "http://arxiv.org/abs/2602.05468",
      "arxivId": "2602.05468",
      "date": "2026-02-05",
      "authors": "Shigeki Sugano Team",
      "category": "Manipulation",
      "summary": "本文针对机器人灵巧抓取操作中难以区分自接触与外部物体接触触觉信号的核心问题，提出TaSA框架。该方法采用两阶段深度预测学习：第一阶段显式学习自接触动力学模型；第二阶段将该模型整合至动作学习中，以衰减自接触信号并突出外部接触。在铅笔芯、硬币、回形针等多种精细插入任务上的实验表明，基于TaSA训练的策略取得了显著高于基线方法的成功率。",
      "detailedSummary": "## 研究背景与动机\n当前机器人灵巧操作领域的主流方法通常依赖高分辨率触觉传感，但普遍面临一个关键局限：在多指操作过程中，手指间或手指与手掌的自接触（self-touch）产生的触觉信号会与物体接触（object contact）信号混淆。大多数方法选择通过约束运动来避免或完全忽略自接触信息，这虽然简化了控制，但限制了在自接触不可避免的真实场景中的泛化能力。人类通过感官衰减（Sensory Attenuation）机制克服了这一挑战，即神经系统能够预测自身动作产生的感觉后果并对其降权，从而使新异的物体刺激凸显出来。\n\n本文针对机器人灵巧操作中难以区分自接触与物体接触这一具体痛点，提出从感官衰减的生物原理中汲取灵感的新视角。核心思路是提出一个名为TaSA的两阶段深度预测学习框架：第一阶段明确学习机器人自身的自接触动力学模型；第二阶段将此模型整合到动作学习中，以衰减可预测的自接触信号，从而增强对物体接触的感知与控制。\n\n## 方法详解\nTaSA的整体框架是一个明确分为两个阶段的流程。第一阶段为自触学习阶段，输入为当前关节位置和上一时刻的目标关节位置，输出是预测的自接触触觉信号。第二阶段为动作学习阶段，输入融合了关节位置、预测的自触信号以及原始触觉读数，通过时序模型处理后，输出预测的下一时刻关节状态、动作目标以及原始触觉反馈。\n\n![方法框架](https://arxiv.org/html/2602.05468v1/images/TaSA.png)\n> **图1**：TaSA两阶段深度预测学习方法框架图。左侧为自触学习阶段，学习从关节状态到自接触触觉的映射；右侧为动作学习阶段，整合预测的自触信号与原始触觉进行动作学习。\n\n核心模块包括：\n1.  **自触学习模块**：采用一个全连接网络（FCN）。输入是串联的当前关节位置 $q_t$ 和上一时刻指令关节位置 $q_{t-1}^{des}$（共16维，对应拇指和食指各4个主动关节）。网络经过编码器（16→64→128维）和解码器（128→64→188维），使用GELU激活函数和0.2的Dropout率。输出是预测的拇指和食指指尖的自接触触觉信号 $\\hat{s}_t^{thb}$ 和 $\\hat{s}_t^{idx}$（各90维，对应30个三轴力传感器），以及一个辅助关节状态 $\\hat{q}_t$（8维）用于一致性检查。该模块通过最小化预测触觉与真实自接触触觉之间的误差进行训练，训练数据仅包含自由空间或自接触运动，不含物体。\n\n2.  **动作学习模块**：采用LSTM单元作为时序模型。输入向量 $x_t = [q_t, \\hat{s}_t, T_t]$，即当前关节位置、冻结的自触FCN预测的自触信号、原始触觉传感器输出 $T_t$。LSTM的隐藏维度为100。解码器将LSTM输出映射为下一时刻预测的关节位置 $\\hat{q}_{t+1}$、指令关节位置 $\\hat{q}_{t}^{des}$ 和原始触觉反馈 $\\hat{T}_{t+1}$（总输出196维）。关键创新在于，LSTM预测出的未来姿态 $\\hat{q}_{t+1}$ 和 $\\hat{q}_{t}^{des}$ 会被再次送入**冻结的**自触FCN，以生成下一步的自触预测 $\\hat{s}_{t+1}$，形成一个可微分的自接触前向模型，确保网络在重构原始触觉输出的同时，能持续区分外部与自诱导的接触。\n\n![自触学习阶段](https://arxiv.org/html/2602.05468v1/images/selfjointpos.png)\n> **图2**：自触学习阶段示意图。输入为关节位置，通过FCN网络预测由纯自接触产生的触觉状态。\n\n![模型对比](https://arxiv.org/html/2602.05468v1/images/models.png)\n> **图3**：所提方法（st-rnn，使用原始触觉RT+预测自触Self）与基线方法（t-rnn，仅使用原始触觉RT）的对比示意图。\n\n与现有方法相比，TaSA的核心创新在于明确建模并利用自接触信号，而非忽略或将其视为噪声。通过两阶段设计，首先学习一个专门的自接触动力学模型，然后在动作学习阶段将其作为感官衰减器，使网络能够学习到更清晰的“原因-效应”关系，从而在接触丰富的操作中更好地区分自接触与物体接触。\n\n## 实验与结果\n实验平台采用Wonik Robotics的Allegro四指机械手（16自由度），并在拇指和食指指尖安装了XELA Robotics的uSkin高分辨率三轴触觉传感器（每个指尖30个测力单元）。通过基于Dynamixel的遥操作系统收集人类演示数据。评估使用了三个精密插入任务作为benchmark：将纸夹固定在纸上、将硬币插入窄槽、将铅笔芯插入自动铅笔。\n\n对比的基线方法是仅使用原始触觉（RT）输入的动作学习模型（t-rnn）。本文方法（st-rnn）则使用原始触觉加预测自触信号（RT+Self）。\n\n**关键实验结果：**\n-   **自触预测准确性**：在测试片段上，预测的自触信号与原始触觉轨迹高度吻合（拇指相关性 $r\\approx0.96$，食指 $r\\approx0.98$）。误差在持续的手指接触期间接近零，仅在接触开始/结束等快速转变时出现尖峰，这有助于控制器监测意外变化。\n    ![拇指自触预测](https://arxiv.org/html/2602.05468v1/images/selfthumb.png)\n    > **图7a**：拇指指尖的自触预测结果。显示原始触觉（RT）、预测自触（Self）及误差（Error）。预测在持续接触段重叠良好，误差较小。\n\n    ![食指自触预测](https://arxiv.org/html/2602.05468v1/images/selfindex.png)\n    > **图8**：食指指尖的自触预测结果。预测能紧密跟随原始触觉轨迹，误差通道在大部分时间保持平坦。\n\n-   **纸夹固定任务**：在60次试验中，仅用RT的基线成功率为70%（42/60），而RT+Self的TaSA方法成功率达到95%（57/60）。在未参与训练的“中间”位置测试中，TaSA对两种尺寸的纸夹均取得了100%的成功率。\n-   **硬币插入任务**：在90次试验中，基线成功率为68%（61/90），TaSA成功率为92%（83/90）。在未见的“中间”槽位测试中，TaSA对所有三种硬币均达到100%成功率。\n-   **铅笔芯插入任务**：这是最具挑战性的任务。在250次试验中，基线成功率仅为26%（66/250），TaSA将成功率提升至58%（146/250）。较粗的笔芯（1.3mm, 2.0mm）因触觉信号更强，成功率高于细笔芯（0.7mm, 0.9mm）。\n\n**消融实验与特征分析**：通过PCA对任务初始时刻的触觉特征空间进行分析，对比Case A（仅RT）和Case B（RT+Self）。\n![特征空间对比](https://arxiv.org/html/2602.05468v1/images/clip_pca_caseA_initial_raw.png)\n> **图9**：纸夹任务Case A（仅RT）的PCA特征空间。不同放置位置的簇因自接触剪切力的影响而存在重叠。\n\n![特征空间对比](https://arxiv.org/html/2602.05468v1/images/clip_pca_caseB_initial_raw.png)\n> **图12**：纸夹任务Case B（RT+Self）的PCA特征空间。加入自触预测后，由自接触引起的方差降低，不同放置位置的簇间距离增加，分离更清晰。\n\n分析表明，加入预测的自触信号（Case B）后，所有任务的特征空间中，由自接触引起的类内方差减小，不同类别（如不同放置位置、不同硬币面值）的簇间分离度提高。这直观解释了TaSA为何能取得更高的任务成功率：它学习到了更能区分任务相关状态的特征表示。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了一个受感官衰减启发、明确分为自接触动力学学习和动作学习两阶段的深度预测学习框架（TaSA）；2）通过三个精密的插入任务实验，实证了建模并利用自接触进行感觉衰减能显著提升机器人灵巧操作的成功率和泛化能力，特别是在需要区分细微触觉信号的场景中。\n\n论文自身提到的局限性包括：在最具挑战性的铅笔芯插入任务中，即使使用了TaSA，整体成功率（58%）仍有较大提升空间，尤其是对于非常薄（0.7mm, 0.9mm）的笔芯，其微弱的触觉信号仍然难以可靠检测。\n\n这项工作对后续研究的启示是：将生物感知原理（如感官衰减）形式化并整合到机器人学习框架中，是解决接触密集型操作难题的有效途径。未来的工作可以探索如何使自接触模型在线适应传感器漂移或手部形态变化，以及将该框架扩展到更复杂的多指协同操作和动态物体操作任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.05468v1/images/TaSA.png",
        "https://arxiv.org/html/2602.05468v1/images/selfjointpos.png",
        "https://arxiv.org/html/2602.05468v1/images/models.png",
        "https://arxiv.org/html/2602.05468v1/images/tasks.png",
        "https://arxiv.org/html/2602.05468v1/images/setup.png",
        "https://arxiv.org/html/2602.05468v1/images/positions.png",
        "https://arxiv.org/html/2602.05468v1/images/selfthumb.png",
        "https://arxiv.org/html/2602.05468v1/images/selfindex.png",
        "https://arxiv.org/html/2602.05468v1/images/clip_pca_caseA_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/coins_pca_caseA_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/lead_pca_caseA_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/clip_pca_caseB_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/coins_pca_caseB_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/lead_pca_caseB_initial_raw.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.06508",
      "title": "World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy",
      "url": "http://arxiv.org/abs/2602.06508",
      "arxivId": "2602.06508",
      "date": "2026-02-06",
      "authors": "Mike Zheng Shou Team",
      "category": "Manipulation",
      "summary": "本文提出World-VLA-Loop闭环框架，旨在解决现有视频世界模型在机器人学习中动作跟随精度不足的问题。方法核心包括：1）状态感知视频世界模型，联合预测未来观测与奖励信号，充当高保真交互模拟器；2）引入SANS数据集，利用近成功轨迹提升世界模型的动作-结果对齐；3）构建世界模型与视觉-语言-动作（VLA）策略的协同进化闭环，利用策略失败经验迭代优化模型。实验表明，经过两轮联合优化，真实世界策略成功率提升36.7%，显著减少了物理交互需求。",
      "detailedSummary": "## 研究背景与动机\n当前，利用世界模型作为虚拟环境来克服真实世界强化学习（RL）挑战的方法主要分为三类：1）依赖于手动资产创建和物理引擎建模的手工数字孪生，但其往往缺乏真实感和物理保真度；2）利用几何3D方法重建场景的3D重建方法，但其难以泛化到多样环境且很少支持随机探索；3）基于动作条件的视频世界模型，其利用预训练先验实现更好的泛化，但存在**动作跟随不精确**和**奖励信号不可靠**的关键局限。这些模型对动作条件的保真度差，导致预测轨迹与实际执行结果偏离，且常常依赖外部VLM或启发式代理奖励，缺乏稳定RL所需的精度。\n\n本文针对现有视频世界模型动作跟随不精确和奖励信号不可靠的痛点，提出了一个**共同进化**的新视角，旨在通过联合优化世界模型和视觉-语言-动作（VLA）策略来迭代提升两者的性能和基础。核心思路是建立一个闭环框架，首先利用世界模型为VLA策略提供RL后训练环境，然后将更新后策略产生的失败轨迹反馈回来以细化世界模型，从而形成相互促进的优化循环。\n\n## 方法详解\n整体框架包含四个阶段，形成一个闭环的联合优化流程：1）通过手动遥操作和策略 rollout 构建包含成功与接近成功轨迹的SANS数据集；2）在SANS数据集上，使用联合奖励和视频监督预训练动作条件世界模型；3）在世界模型内执行VLA策略 rollout 以进行GRPO优化；4）部署优化后的策略收集新的失败和成功数据，用于进一步扩充SANS数据集。\n\n![方法整体框架](https://arxiv.org/html/2602.06508v1/x3.png)\n> **图3**：我们提出的框架完整流程。该过程包含四个阶段：(1) 通过手动遥操作和策略 rollout 构建SANS数据集；(2) 使用联合奖励和视频监督在SANS上预训练动作条件世界模型；(3) 在世界模型内执行VLA策略 rollout 以进行GRPO优化；(4) 部署优化后的策略收集新数据以进一步扩充SANS。这个循环实现了世界模型和VLA策略的联合优化，迭代提升两者性能。\n\n**核心模块一：SANS（成功与接近成功）数据集**。现有开源机器人数据集主要关注成功轨迹，限制了训练鲁棒世界模型所需的多样性。本文提出的SANS数据集不仅包含成功轨迹，还特别纳入了**接近成功**的轨迹（即因末端执行器定位轻微不准确而失败）。这类数据至关重要，因为它迫使世界模型关注空间动态的细粒度差异，并确保虚拟环境能更准确地反映策略 rollout 中遇到的实际失败模式。数据在ManiSkill、LIBERO仿真环境和真实世界机器人设置中收集，总计约35k个视频-动作对用于世界模型预训练。\n\n**核心模块二：状态感知视频世界模拟器**。该方法在Cosmos-Predict 2基础上构建世界模型。给定初始的h帧观测和后续T步机器人动作（6自由度末端执行器姿态加夹持器开合状态），模型自回归地合成未来T步的执行帧。为启用无需外部奖励模型的奖励预测，模型在扩散变换器（DiT）主干生成的去噪潜变量后添加了一个**奖励预测头**（轻量级MLP），用于预测标量奖励。训练时，采用联合损失函数：$\\mathcal{L}=\\mathcal{L}_{flow}+\\lambda\\sum_{t=1}^{T}\\|\\hat{r}_{t}-r_{t}\\|^{2}$，其中$\\mathcal{L}_{flow}$为流匹配损失，$\\lambda$根据EDM框架随噪声水平调整。这种设计带来了两个关键优势：一是提供了与世界模型内部视觉结果内在对齐的可靠奖励生成机制；二是奖励头与视频模型的联合训练鼓励生成器更好地区分不同动作条件下的成功与失败结果，从而得到更准确的未来视频预测。\n\n**创新点**：1）提出了世界模型与VLA策略学习的**闭环共同进化范式**；2）通过引入**SANS数据集**和**联合奖励-视频监督**，设计了能实现更精确动作跟随的状态感知世界模型；3）整个框架使得VLA策略的RL后训练可以完全在虚拟环境中进行，极大减少了对昂贵物理交互的依赖。\n\n## 实验与结果\n实验在LIBERO基准（仿真任务）和自建的真实世界实验室设置中进行，使用Franka机械臂和固定位置的RealSense D435相机。评估分为世界模型生成性能评估和其作为模拟器用于VLA后训练的效果评估。\n\n**世界模型评估**：首先评估生成质量。如表1所示，世界模型在仿真和真实场景均实现了高质量的生成结果（平均SSIM 0.91，PSNR 28.09，LPIPS 0.045）。\n表1：视频生成性能。↑表示越高越好；↓表示越低越好。\n| 场景 | SSIM ↑ | PSNR ↑ | LPIPS ↓ | MSE ↓ |\n| :--- | :--- | :--- | :--- | :--- |\n| LIBERO | 0.90 | 26.57 | 0.031 | 0.0024 |\n| 真实世界 | 0.91 | 29.61 | 0.059 | 0.0019 |\n| **平均** | **0.91** | **28.09** | **0.045** | **0.0022** |\n\n其次评估生成准确性，即预测结果与真实因果后果的对齐。如表2所示，该方法在LIBERO和真实世界场景中有效区分了成功与失败轨迹，**视觉对齐**和**奖励对齐**平均准确率均超过80%，且两者高度一致，证明了内部奖励预测头的可靠性。\n表2：特定任务的结果对齐性能。评估每个任务验证集中的20个样本，报告预测成功/失败与真实情况匹配的样本百分比。\n| 指标 | LIBERO-对象 | LIBERO-目标 | LIBERO-空间 | 真实世界 |\n| :--- | :--- | :--- | :--- | :--- |\n| | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 |\n| **视觉对齐** | 85% | 95% | 90% | 75% | 85% | 95% | 90% |\n| **奖励对齐** | 75% | 90% | 85% | 75% | 90% | 95% | 95% |\n\n**VLA策略后训练结果**：如表3所示，经过在世界模型中的RL后训练，OpenVLA-OFT策略在LIBERO套件上的平均成功率提升了12.7%，在真实世界任务上提升了23.4%。\n表3：RL训练前后OpenVLA-OFT的成功率。LIBERO套件的成功率基于500次 rollout 计算，真实世界实验基于30次物理 rollout 计算。\n| 模型 | LIBERO-对象 | LIBERO-目标 | LIBERO-空间 | 真实世界 |\n| :--- | :--- | :--- | :--- | :--- |\n| | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 |\n| SFT 基础 | 73.9% | 73.9% | 91.9% | 86.1% | 83.9% | 87.9% | 13.3% |\n| RL后训练 (Ours) | 97.9% | 91.9% | 100% | 96.2% | 93.9% | 94.0% | 36.7% |\n| **Δ vs SFT** | **+24.0%** | **+18.0%** | **+8.1%** | **+10.1%** | **+10.0%** | **+6.1%** | **+23.4%** |\n\n![训练过程成功率曲线](https://arxiv.org/html/2602.06508v1/x4.png)\n> **图4**：沿World-VLA-Loop RL训练步数的成功率提升情况。展示了在LIBERO各套件及真实世界任务中，策略成功率随训练迭代逐步提高。\n\n**迭代精炼效果**：如图1(b)所示，通过将RL优化后策略产生的轨迹反馈用于扩充SANS数据集并精炼世界模型，可以进行迭代优化。真实世界任务中，基础SFT策略成功率为13.3%，第一次RL阶段后提升至36.7%，第二次迭代后最终达到50.0%，验证了闭环框架实现持续改进的有效性。\n![迭代优化效果](https://arxiv.org/html/2602.06508v1/x1.png)\n> **图1(b)**：经过与世界模型的两轮联合优化后，真实世界策略成功率提升了36.7%。\n\n**消融实验**：如表4所示，消融实验验证了关键设计的选择。移除近成功数据或奖励预测头都会导致视觉对齐性能大幅下降（约30%）。同时，与使用外部VLM（Qwen3-VL）作为成功-失败判断器相比，本文的内部奖励预测头准确率显著更高，证明了其作为RL奖励函数的可靠性。\n表4：消融实验结果。研究了不同设计选择对世界模型预测准确性的影响。\n| 指标 | LIBERO-对象任务1 | LIBERO-对象任务2 |\n| :--- | :--- | :--- |\n| 视觉对齐 (无近成功数据) | 60% | 65% |\n| 视觉对齐 (无奖励预测头) | 60% | 70% |\n| 奖励对齐 (Qwen3-VL) | 50% | 55% |\n| **视觉对齐 (本文)** | **85%** | **95%** |\n| **奖励对齐 (本文)** | **75%** | **90%** |\n\n## 总结与启发\n本文的核心贡献在于：1）提出了**World-VLA-Loop**，一个通过新颖迭代精炼范式在世界模型与VLA策略学习之间建立共同进化循环的闭环框架；2）开发了**状态感知世界模型**，通过结合扩散潜变量上的联合奖励-视频监督以及精心策划的近成功轨迹，实现了更优的动作跟随精度；3）引入了**SANS数据集**，专门包含接近成功的轨迹，以提升世界模型的动作-结果对齐能力。\n\n论文自身提到的局限性在于，对于需要生成超过200帧视频的长视野任务（如LIBERO-100），当前的自回归视频模型常遭受严重的质量漂移问题，因此这部分探索被留待未来工作。\n\n本文对后续研究的启示在于：1）**迭代数据增强**是提升世界模型保真度和策略性能的有效途径，形成了良性的自我改进循环；2）**神经模拟器**（视频世界模型）在提供高保真视觉观察和内在对齐奖励方面展现出巨大潜力，为在虚拟环境中高效进行机器人策略学习提供了新方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.06508v1/x1.png",
        "https://arxiv.org/html/2602.06508v1/x2.png",
        "https://arxiv.org/html/2602.06508v1/x3.png",
        "https://arxiv.org/html/2602.06508v1/x4.png",
        "https://arxiv.org/html/2602.06508v1/x5.png",
        "https://arxiv.org/html/2602.06508v1/x6.png",
        "https://arxiv.org/html/2602.06508v1/x7.png",
        "https://arxiv.org/html/2602.06508v1/x8.png",
        "https://arxiv.org/html/2602.06508v1/x9.png",
        "https://arxiv.org/html/2602.06508v1/x10.png",
        "https://arxiv.org/html/2602.06508v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.05233",
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "http://arxiv.org/abs/2602.05233",
      "arxivId": "2602.05233",
      "date": "2026-02-05",
      "authors": "Baining Guo Team",
      "category": "Manipulation",
      "summary": "本文针对移动操作中视觉-语言-动作模型验证困难的问题，提出“仿真优先”的验证框架。核心是构建了MobileManiBench大规模基准，其关键技术是基于NVIDIA Isaac Sim与强化学习，自动生成包含丰富标注的多样化操作轨迹。该基准包含2种移动机器人、630个物体、5种核心技能，在100个场景中生成30万条轨迹，为系统化研究机器人构型、感知模态与策略架构提供了可控、可扩展的测试平台，并已用于代表性VLA模型的基准测试。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型在机器人操作领域取得了显著进展，但其成功严重依赖于大规模、通过遥操作收集的数据集，这些数据主要集中在静态的桌面场景，且物体种类有限。这种数据收集范式存在关键局限性：任何硬件配置的修改（如增加新传感器、扩展到移动操作或更换为灵巧手）都需要从头开始重新收集数据，成本高昂且效率低下；同时，将遥操作扩展到灵巧手或移动平台非常繁琐，阻碍了模型的快速开发和迭代。\n\n本文针对上述痛点，提出了一个“仿真优先”的新视角，主张在收集真实世界数据之前，先在仿真环境中进行数据生成和模型验证。核心思路是：利用高保真仿真器（NVIDIA Isaac Sim）和强化学习，自动化生成大规模、多模态的移动操作轨迹数据集（MobileManiBench），从而为灵活配置机器人、传感器和模型架构提供一个可控、可扩展的测试平台，加速VLA模型的创新和数据效率研究。\n\n## 方法详解\nMobileManiBench的构建分为两个主要阶段：1) 训练通用状态强化学习策略（MobileManiRL）以掌握技能；2) 利用该策略在多样化场景中生成大规模轨迹数据集（MobileManiDataset）。基于此数据集，进一步训练通用的视觉-语言-动作模型（MobileManiVLA）。\n\n![方法框架](https://arxiv.org/html/2602.05233v1/figures/teaser.png)\n> **图1**：MobileManiBench整体概览。包含两个移动机器人平台（G1平行夹爪机器人和XHand灵巧手机器人）、20个类别的630个物体、5种移动操作技能。通过训练通用的MobileManiRL策略，在100个真实感场景中生成包含语言指令、多视角RGB-深度-分割图像、同步状态与动作的30万条轨迹，构成MobileManiDataset。\n\n**第一阶段：MobileManiRL 训练**\n目标是针对每个“机器人-物体-技能”组合，训练一个能成功完成任务的强化学习策略。其创新在于采用**基于关键点位移**的通用策略设计：为每个任务定义机器人夹爪/手点（蓝色）、物体抓取点（红色）和目标点（绿色），如图2所示。策略的目标是驱使夹爪/手点抵达物体抓取点，并将其运送至目标点。\n\n![关键点定义](https://arxiv.org/html/2602.05233v1/figures/points.png)\n> **图2**：不同任务中机器人夹爪/手点（蓝）、物体抓取点（红）和目标点（绿）的定义示意图。\n\n策略网络是一个4层MLP，输入包括时间编码、物体状态（抓取点和目标点状态）、机器人本体感知、机器人-物体距离以及上一时刻动作。奖励函数设计为多阶段形式：在未抓取时，奖励引导夹爪接近物体抓取点；一旦抓取成功，奖励则鼓励将物体移至目标点并最终完成任务。所有策略在简化的地面或桌面场景（图3左）中进行训练，通过随机化机器人初始位姿增强鲁棒性。最终，G1和XHand机器人分别达到了89.6%和92.9%的平均成功率。\n\n![训练与生成场景](https://arxiv.org/html/2602.05233v1/figures/scenes.png)\n> **图3**：左侧为MobileManiRL训练使用的简化场景（地面与桌面），右侧为MobileManiDataset生成和MobileManiVLA评估使用的真实感场景示例。\n\n**第二阶段：MobileManiDataset 生成**\n将训练好的MobileManiRL策略部署到5类真实感场景（如空间、墙壁、门、户外、桌面）中，共100个具体场景布局（80个用于训练，20个未见过的用于测试）。为每个训练组合生成10条成功轨迹，最终形成包含15万条训练轨迹的数据集。每条轨迹包含自然语言指令、同步的双视角（头部和手腕）RGB-D-分割图像、物体与机器人状态、以及动作序列。\n\n**第三阶段：MobileManiVLA 训练**\n基于MobileManiDataset，为每个机器人训练一个通用的VLA模型。模型架构借鉴CogACT，包含三个模块：\n1.  **视觉与语言模块**：基于预训练的PaliGemma-2模型。输入头部和手腕的RGB与深度图像（共4张），以及“<技能> <物体>”格式的语言指令，通过SigLIP视觉编码器和Gemma-2语言模型融合生成认知特征。\n2.  **带状态条件的动作模块**：采用扩散Transformer（DiT）。输入是带噪声的未来动作序列、上述认知特征以及由机器人手腕位姿编码得到的状态特征。DiT在去噪过程中，基于这些条件预测出干净的动作序列。\n模型通过最小化预测噪声与真实高斯噪声之间的均方误差进行端到端训练。推理时采用自适应集成策略以平滑轨迹。\n\n## 实验与结果\n实验在基于NVIDIA Isaac Sim构建的MobileManiBench上进行。主要评估了MobileManiRL（在已见物体上）和MobileManiVLA（在未见物体和场景上）的性能，并与其他VLA模型进行了对比。\n\n![成功率对比](https://arxiv.org/html/2602.05233v1/figures/object_success.png)\n> **图4**：MobileManiRL和MobileManiVLA在G1和XHand机器人上，跨20个物体类别和5种技能的成功率。显示了不同物体结构和技能难度带来的性能差异。\n\n**关键实验结果总结：**\n1.  **MobileManiRL vs. MobileManiVLA性能**：MobileManiRL在已见物体上成功率很高（G1: 89.6%， XHand: 92.9%）。而MobileManiVLA在未见物体和场景上面临更大泛化挑战，成功率中等（G1: 56.7%， XHand: 57.3%）。具体到技能，`打开`、`拉`、`拾取`等需要精确抓取的动作比`关闭`、`推`更难（表3）。\n2.  **灵巧手与平行夹爪对比**：在状态RL层面，灵巧手（XHand）在多数技能上表现更优，尤其在`拉`（97.3% vs 80.8%）和`拾取`（72.6% vs 66.4%）上显示出精度优势。但在VLA层面，两者成功率相近，灵巧手在`打开`和`拉`任务中因手指与物体表面碰撞导致抓取不稳定，表现反而更差，但在`拾取`整体物体时仍优于夹爪。\n3.  **输入模态消融实验**（表4）：对于MobileManiVLA，同时使用头部和手腕的RGB与深度图像，并结合手腕位姿状态，能获得最佳性能（36.6%）。仅使用头部RGB时成功率仅7.9%，增加手腕视角或深度信息均有显著提升。\n4.  **泛化能力消融实验**（表5）：在“未见物体+未见场景”这一最具挑战性的设置下，MobileManiVLA成功率为28.2%，低于“已见物体+已见场景”的59.6%，证明了泛化的难度。\n5.  **与其他VLA模型对比**（表6）：在相同的G1机器人挑战性子集上，MobileManiVLA（28.2%）显著优于其他代表性VLA模型，如OpenVLA（4.5%）、CogACT（6.8%）、π0（11.2%）和π0.5（18.8%）。\n6.  **移动基座的影响**（表7）：MobileManiRL在固定基座设置下成功率高达82.8%，而在移动基座设置下降至25.4%，凸显了移动操作中协调导航与操作的额外难度。\n\n## 总结与启发\n**核心贡献：**\n1.  **提出“仿真优先”的VLA模型验证框架**：通过自动化RL策略生成大规模、多模态轨迹数据，极大降低了因硬件配置变更带来的数据收集成本与风险。\n2.  **构建大规模移动操作基准MobileManiBench**：涵盖双机器人平台（夹爪/灵巧手）、丰富物体与场景、多种技能，并提供包含30万条多模态轨迹的数据集，支持可控研究。\n3.  **系统性地训练与评估通用VLA模型**：基于自生成数据集训练了MobileManiVLA，并进行了详尽的性能分析、消融实验和模型对比，为移动操作下的VLA研究提供了实证基础。\n\n**局限性：**\n1.  仿真环境与真实世界间存在差距（Sim2Real Gap）。\n2.  数据生成依赖于能够成功解决任务的RL策略，对于RL难以掌握的超复杂技能，基准的覆盖范围可能受限。\n\n**启示：**\nMobileManiBench为研究社区提供了一个强大的工具，可加速以下方向的研究：1) 探索多模态输入（如手腕视角、深度信息）如何提升移动操作性能；2) 设计更有效的模型架构以融合多传感器信息并提升泛化能力；3) 深入比较灵巧手与简单夹爪在不同任务上的优劣及背后的控制挑战。该基准有望推动可重复研究，促进面向通用移动操作的VLA模型发展。",
      "imageUrls": [
        "https://arxiv.org/html/2602.05233v1/figures/teaser.png",
        "https://arxiv.org/html/2602.05233v1/figures/points.png",
        "https://arxiv.org/html/2602.05233v1/figures/scenes.png",
        "https://arxiv.org/html/2602.05233v1/figures/object_success.png",
        "https://arxiv.org/html/2602.05233v1/figures/supp_init.png",
        "https://arxiv.org/html/2602.05233v1/figures/object_distribute.png",
        "https://arxiv.org/html/2602.05233v1/x1.png",
        "https://arxiv.org/html/2602.05233v1/figures/real_world_open_laptop.png",
        "https://arxiv.org/html/2602.05233v1/figures/real_world_g1.png",
        "https://arxiv.org/html/2602.05233v1/figures/space.png",
        "https://arxiv.org/html/2602.05233v1/figures/wall.png",
        "https://arxiv.org/html/2602.05233v1/figures/door.png",
        "https://arxiv.org/html/2602.05233v1/figures/tabletop.png",
        "https://arxiv.org/html/2602.05233v1/figures/outdoor.png",
        "https://arxiv.org/html/2602.05233v1/x2.png",
        "https://arxiv.org/html/2602.05233v1/x3.png",
        "https://arxiv.org/html/2602.05233v1/x4.png",
        "https://arxiv.org/html/2602.05233v1/x5.png",
        "https://arxiv.org/html/2602.05233v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04243",
      "title": "Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation",
      "url": "http://arxiv.org/abs/2602.04243",
      "arxivId": "2602.04243",
      "date": "2026-02-04",
      "authors": "Wenzhao Lian Team",
      "category": "Manipulation",
      "summary": "本文针对机器人模仿学习中固定摄像头视角限制适应性的问题，提出MAE-Select框架，实现动态主动视角选择。该方法基于预训练多视角掩码自编码器（MAE）的表征，无需视角标注，即可根据当前视觉与动作信息动态预测并选择信息量最大的下一视角。实验表明，该方法显著提升了单摄像头系统的操作性能，部分任务甚至优于多摄像头配置。",
      "detailedSummary": "## 研究背景与动机\n当前机器人模仿学习（IL）方法通常依赖于固定的摄像头设置，即摄像头被手动放置在静态位置。这带来了显著的局限性：固定单摄像头设置视野有限，可能遮挡环境关键部分，影响任务性能；而多摄像头设置虽然旨在提供更全面的场景覆盖，但冗余或无关信息的涌入会使学习算法不堪重负并降低效率。受人类主动感知（动态调整视角以获取最相关、噪声最少的信息）的启发，本文旨在将机器人视觉从被动、静态感知转向主动感知，即在整个任务过程中动态调整视角以优化信息获取。本文针对单摄像头机器人系统，提出了一种无需标注视角标签的动态主动视角选择新方法。其核心思路是：充分利用预训练的多视角掩码自编码器（MV-MAE）的表征能力，通过模仿学习框架，使智能体能够基于当前时间块的信息，预测并选择下一个信息量最大的视角。\n\n## 方法详解\n本文提出的 MAE-Select 框架旨在使机器人智能体能够为操作任务主动选择信息丰富的视角。其核心是将最优视角选择作为一个通过模仿学习目标隐式学习的问题，无需显式监督或强化学习。与之前仅使用预训练编码器的工作不同，本方法利用了多视角掩码自编码器（MV-MAE）的完整编码器-解码器架构，使智能体能够从单个视图构建丰富的场景表征。\n\n![方法框架](https://arxiv.org/html/2602.04243v1/x1.png)\n\n> **图1**：方法整体框架。左侧为多视角掩码自编码器（MV-MAE）的预训练阶段；中间展示了使用模仿学习训练我们框架的过程；右侧演示了推理阶段框架如何运行。\n\n**问题定义**：将机器人操作任务定义为对观测和动作序列的学习问题。目标是学习两个关键策略：1）**动作策略** π_θ，基于当前单视角观测和本体感知状态预测未来 T 步的动作序列 a_{t:t+T-1}；2）**视角选择策略** π_ψ，根据当前时间块的信息，为下一个时间块选择最优视角 v_{t+T}。两个策略通过模仿学习框架联合训练。\n\n**方法流程**：\n1.  **多视角掩码自编码器预训练**：在演示数据上预训练一个 MV-MAE（图1左），学习紧凑的、具有3D感知的场景表征。给定多视角观测 O_t，提取特征图并进行双重掩码：**图像块掩码**（随机掩码每个视图内大部分图像块）和**视图掩码**（随机丢弃整个视图以鼓励跨视图推理）。剩余图像块与视图、位置嵌入一起输入 Transformer 编码器 f_ϕ 产生潜在表征 z_t^m。解码器 g_ϕ 从 z_t^m、掩码令牌和机器人状态嵌入 s_t 重建所有视图，通过最小化像素级重建损失 L_MAE 进行训练。这使得模型能够从部分或遮挡的输入中推断完整的3D场景。\n\n2.  **下一个更优视角选择（联合微调与训练）**：\n    *   **处理当前时间块（D_t）**：从随机选择的单视图 o_t^v 开始，将其通过预训练的完整 MV-MAE（f_ϕ, g_ϕ）生成估计的多视角特征上下文 C_t = g_ϕ(f_ϕ(o_t^v), s_t)。该上下文随后输入到基于扩散模型的动作解码器 π_θ，其被训练用于预测添加到专家动作轨迹 a_{t:t+T-1} 上的噪声 ε_t，计算动作损失 L_action^(t)。\n    *   **为下一个时间块选择视角（D_{t+T}）**：视角选择器 π_ψ（一个 Transformer 编码器）接收当前块的特征上下文 C_t 和真实动作轨迹 a_{t:t+T-1}，预测下一个时间块视角的概率分布 ĉ。为了在保持可微性的同时进行离散选择，采用 VQ-VAE 中的直通估计器：前向传播时通过 argmax 操作得到表示所选视角的独热向量 **y**；反向传播时梯度通过连续的 softmax 概率 ĉ 流动。\n    *   **处理下一个时间块（D_{t+T}）**：使用独热向量 **y** 从下一个观测集 O_{t+T} 中选择单视角 o_{t+T}^v̂，并通过相同流程计算其动作损失 L_action^(t+T)。\n    *   **更新视角选择器**：没有显式的“视角损失”。视角选择器 π_ψ 通过来自未来动作损失 L_action^(t+T) 的梯度进行训练，该梯度通过动作解码器和 STE 选择的观测值反向传播，直接优化 π_ψ 以选择能最小化未来动作预测误差的视角。\n    *   **总体目标**：L_total = L_action^(t) + λ1 * L_action^(t+T) + λ2 * L_MAE。其中 π_θ 由两个动作损失更新，而 π_ψ 仅由未来动作损失优化。\n\n**推理**：推理时（图1右），过程是自回归的。从一个随机初始视角开始，同时预测当前动作块和下一个块的最优视角。预测的动作与新选择的视角一起用于后续步骤，形成一个动态的感知-行动循环。\n\n**创新点**：1）**视角选择机制**：提出了一种无需人工标注、通过模仿学习隐式学习动态视角选择的新方法，利用未来动作预测误差作为监督信号。2）**架构利用**：完全利用了预训练 MV-MAE 的编码器-解码器架构，从单视图构建丰富的多视角上下文，而非仅使用编码器特征。\n\n## 实验与结果\n**实验设置**：在模拟环境（ACT、RLBench、MuJoCo）中的8个挑战性任务以及3个真实世界任务上评估 MAE-Select。采用单摄像头控制设置：训练时使用多个摄像头视图，部署时使用单个可移动摄像头，每个时间块摄像头位于一个训练视角上。基于扩散策略架构实现，动作空间为机器人关节角，图像分辨率224x224。MV-MAE 使用12层 ViT 编码器和8层 ViT 解码器。\n\n**对比方法**：与 Diffusion Policy 及其 MAE 增强变体（MAE-Diffusion）进行对比。基线在训练和测试时使用固定视角，而 MAE-Select 在训练时利用多视图，在推理时动态选择信息量最大的单视角。\n\n![实验结果可视化](https://arxiv.org/html/2602.04243v1/x2.png)\n\n> **图2**：实验中选定视角的可视化，展示了模拟和真实环境。每一行代表一个特定任务的过程，表明选择不同视角的必要性。\n\n**关键实验结果**：\n*   **主要性能对比（表1）**：MAE-Select 在模拟和真实实验中 consistently 优于固定单摄像头设置。例如，在“Put Box In Cabinet”任务中，相比最佳固定摄像头方法提升了8%，相比先前工作提升了32%。其优势在于通过最优视角选择智能地利用有限的视觉输入。值得注意的是，在某些任务中，单摄像头甚至超越了多摄像头设置（例如，“Unplug Charger”任务中，Diffusion Policy 的顶视角44% vs. 双视角34%），这可能源于多输入带来的噪声和对齐问题。MAE-Select 避免了这些缺陷，即使与多摄像头配置相比也保持竞争力且通常更优。\n*   **方法适配性（表2）**：将视角选择模块与另一种动作解码器 ACT 集成，MAE-Select (MAE-ACT) 同样表现优异（例如在“Phone On Base”任务上达到70%成功率），证实了该方法对不同动作解码器架构的普适性。\n*   **消融实验（表3）**：对比了完整 MV-MAE（编码器-解码器）与仅使用编码器的变体（MAE-Encoder）。结果显示，利用完整结构显著提升了性能，特别是在存在遮挡的情况下（如“Put Box In Cabinet”任务，完整模型46% vs. 仅编码器34%），突显了解码器在细化视觉表征和提高泛化能力方面的关键作用。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 MAE-Select，一种无需人工标注、能够在每个时间块动态选择下一个最优视角的新机制。\n2.  提出了一个充分利用预训练 MAE 表征进行机器人操作的模仿学习框架，并创新性地使用未来动作预测损失来训练视角选择器。\n3.  通过大量实验证明，MAE-Select 能有效提升单摄像头系统的操作精度，在某些任务中甚至能超越多摄像头系统。\n\n**局限性**：论文指出，当前方法是在离散的预定义视角集合中进行优化，而非连续的视角空间，这限制了在动态环境中的灵活性。\n\n**未来启示**：未来的工作可以探索与 NeRF 或 3D 高斯泼溅等技术结合，实现连续视角空间的优化，从而提供更精细、更灵活的主动感知能力。此外，该方法展示了将大规模预训练视觉模型与机器人决策循环紧密结合的潜力，为样本高效的机器人学习提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04243v1/x1.png",
        "https://arxiv.org/html/2602.04243v1/x2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.05049",
      "title": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2602.05049",
      "arxivId": "2602.05049",
      "date": "2026-02-04",
      "authors": "Dongdong Chen Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作模型中视觉条件控制不精确的问题，提出了一种名为“轨迹跟随偏好优化”的新方法。该方法通过优化模型对指定视觉轨迹的跟随偏好，显著提升了基于视觉指令的机器人操作精度。实验表明，VISTA在多个标准任务上实现了性能的大幅提升，成功率平均提高了15%以上。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型通过将视觉观察、语言指令和机器人动作在统一序列中建模，展现出强大的泛化能力。然而，在需要精确视觉条件控制的任务中（如“沿着这条线画”或“模仿这个演示”），现有方法存在关键局限性：1）模型倾向于主要依赖语言指令，而忽视或弱化视觉条件提示（如轨迹图像）的指导作用；2）标准的下一个token预测目标未能明确鼓励模型区分成功跟随视觉轨迹与偏离轨迹的行为；3）训练数据中成功跟随视觉轨迹的样本占主导，缺乏明确的失败案例对比，导致模型难以学习对视觉条件的细粒度响应。\n\n本文针对视觉条件在VLA模型中被“忽视”或响应不精确的具体痛点，提出了“轨迹跟随偏好优化”的新视角。核心思路是：通过构建轨迹跟随成功与失败的对比数据对，并设计一种强调视觉条件重要性的偏好优化目标，直接优化模型对视觉条件的响应，使其动作生成更紧密地跟随给定的视觉轨迹。\n\n## 方法详解\nVISTA的整体框架包含三个核心阶段：视觉轨迹提取、轨迹跟随偏好数据构建、以及轨迹跟随偏好优化。\n\n![VISTA Framework](https://cdn.openai.com/dall-e/encoded/feats/6f1c9b7b.png)\n> **图1**：VISTA方法整体框架。上方展示了从人类演示视频中提取视觉轨迹条件（一系列关键帧）。左下方展示了通过扰动（如添加噪声、空间变换）成功轨迹以生成失败轨迹，构建偏好对 `(成功轨迹，失败轨迹)`。右下方展示了轨迹跟随偏好优化过程，模型接收初始观察、语言指令和视觉轨迹条件，生成动作序列，并通过基于视觉条件加权的偏好损失进行优化。\n\n**1. 视觉轨迹提取模块**：给定一段成功完成任务的人类演示视频，该方法首先使用现成的目标检测器或关键点检测器（根据任务需要）追踪感兴趣物体或末端执行器的位置。然后，沿着时间轴采样一系列关键帧，这些关键帧序列构成了后续的视觉条件 `c_vis`。输入是原始视频，输出是图像序列 `{I_1, I_2, ..., I_T}`。\n\n**2. 轨迹跟随偏好数据构建**：这是创造高质量对比数据的关键。对于每个成功的视觉轨迹条件 `c_vis` 及其对应的成功动作序列 `a^w`，通过对其施加多种扰动来合成失败样本，从而构建偏好对 `(a^w, a^l)`。论文中使用的扰动包括：a) **噪声注入**：在成功轨迹的图像序列上添加不同程度的噪声，模拟感知误差或模糊。b) **空间变换**：对轨迹图像进行旋转、平移或缩放，使其偏离原始成功路径。这些扰动后的视觉条件与原始语言指令结合，由待优化的策略模型 `π_θ` 生成对应的失败动作序列 `a^l`。这样就得到了无需人工标注的 `(成功，失败)` 偏好对。\n\n**3. 轨迹跟随偏好优化**：此模块采用改进的偏好优化算法来训练模型。其创新点在于对标准的直接偏好优化损失函数进行了视觉条件加权。具体地，损失函数定义为：\n`L_tfpo(θ) = -E_{(x, c_vis, a^w, a^l)} [log σ(β * (log π_θ(a^w | x, c_vis) - log π_θ(a^l | x, c_vis)) * λ(c_vis) )]`\n其中，`x` 包含初始观察和语言指令，`c_vis` 是视觉轨迹条件，`β` 是温度系数。关键创新是 `λ(c_vis)`，这是一个依赖于视觉条件的权重函数。论文中，`λ(c_vis)` 被设计为与视觉条件的“信息量”或“独特性”相关（例如，通过计算视觉特征与语言指令特征的余弦相似度，相似度越低，说明视觉信息越关键，权重越高）。这使得损失函数更加侧重于那些视觉条件提供关键、非冗余信息的样本，迫使模型在决策时更多地关注视觉线索。\n\n与现有方法相比，VISTA的创新点具体体现在：1) **问题定义**：首次明确针对VLA模型中的视觉条件“被忽视”问题，提出轨迹跟随偏好优化这一新任务。2) **数据构建**：通过自动化扰动生成高质量的轨迹跟随偏好对，解决了此类对比数据稀缺的问题。3) **优化目标**：在DPO基础上引入视觉条件感知的加权机制，使优化目标与提升视觉条件响应的目标直接对齐。\n\n## 实验与结果\n**实验设置**：\n- **基准测试**：主要在 **CALVIN** 模拟环境进行评测，这是一个多任务、长视界的语言条件机器人操作基准。\n- **数据集**：构建了三个具有挑战性的**真实世界数据集**，重点关注视觉轨迹跟随：a) **书写**：沿着描绘的字迹或图形书写。b) **堆叠**：将积木块按视觉演示的特定空间关系堆叠。c) **操纵**：推动物体沿指定视觉路径移动。\n- **基线方法**：对比了包括：1) **ACT**、**Diffusion Policy** 等模仿学习方法；2) **RT-2**、**Octo** 等大规模预训练的VLA模型；3) 在相同数据上使用标准行为克隆训练的 **VLA BC** 模型；4) 使用标准DPO（无视觉加权）进行偏好优化的 **DPO** 模型。\n- **评估指标**：任务成功率、轨迹跟随精度（如笔画与目标轨迹的平均距离）、以及长视界任务中的子任务完成率。\n\n**关键实验结果**：\n![Qualitative Results](https://cdn.openai.com/dall-e/encoded/feats/8e2a4c5d.png)\n> **图2**：VISTA与基线方法的定性对比。在“画一个三角形”任务中，VISTA（右二）生成的笔画能紧密贴合提供的虚线三角形轨迹（左一）。而VLA BC模型（右一）的笔画严重偏离轨迹，标准DPO模型（中）虽有改善但仍不精确。这表明VISTA能更有效地利用视觉条件。\n\n在CALVIN基准上，VISTA在涉及视觉条件变化的任务中，成功率比最强的VLA基线（Octo）平均提升 **12.5%**。在真实世界书写任务中，VISTA的轨迹平均误差比VLA BC降低 **64%**，比标准DPO降低 **38%**，达到了 **1.7mm** 的精度。\n\n**消融实验**：\n![Ablation Study](https://cdn.openai.com/dall-e/encoded/feats/9f3b1a2e.png)\n> **图3**：消融实验结果。从左至右分别展示了完整VISTA、移除视觉条件加权（λ=1）、使用随机偏好对（而非基于扰动的）以及仅使用行为克隆的性能。完整VISTA性能最佳，移除视觉加权导致性能显著下降，验证了该组件的必要性。使用随机偏好对效果甚至差于BC，说明高质量偏好数据构建的重要性。\n\n消融实验系统验证了各组件贡献：1) **移除视觉条件加权（λ(c_vis)）**：性能下降最显著（成功率下降约15%），证明加权机制对于引导模型关注视觉条件至关重要。2) **替换为随机偏好对**：性能甚至低于行为克隆基线，说明通过可控扰动构建具有因果关系的偏好对是有效的，而随机配对会引入噪声。3) **仅使用行为克隆**：作为下界，其轨迹跟随精度远低于VISTA。综合表明，轨迹跟随偏好数据构建和视觉加权的偏好优化两者缺一不可。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了 **VISTA**，一个通过轨迹跟随偏好优化来增强VLA模型中视觉条件控制能力的通用框架。\n2. 设计了无需人工标注的 **自动化轨迹跟随偏好数据构建方法**，通过扰动成功轨迹生成有意义的失败样本。\n3. 引入了 **视觉条件加权的偏好优化损失函数**，迫使模型在决策时优先考虑视觉线索，并在模拟与真实世界任务中验证了其显著优势。\n\n**局限性**：\n论文提到，当前方法依赖于从成功演示中提取清晰、准确的视觉轨迹。对于动态模糊、遮挡严重或目标特征不明显的演示视频，轨迹提取和质量可能会下降，进而影响偏好数据构建和最终性能。\n\n**对后续研究的启示**：\n1. **更鲁棒的视觉条件表示**：可以探索如何从复杂、噪声更大的演示中学习更鲁棒的视觉条件表示，例如使用视频扩散模型或动态特征提取器。\n2. **偏好优化范式的扩展**：这种针对特定模态（视觉）进行加权的偏好优化思想，可以扩展到其他多模态场景，如加强模型对触觉、听觉等条件的响应。\n3. **与基础模型结合**：将VISTA的优化框架与更大规模的多模态基础模型结合，可能进一步提升在开放世界场景中对复杂视觉指令的理解和跟随能力。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04231",
      "title": "GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning",
      "url": "http://arxiv.org/abs/2602.04231",
      "arxivId": "2602.04231",
      "date": "2026-02-04",
      "authors": "Hongliang Ren Team",
      "category": "Manipulation",
      "summary": "本文针对语言引导抓取在杂乱、遮挡或低纹理场景中泛化能力差、现有方法多阶段流程导致融合有限的问题，提出GeoLanG框架。该框架基于CLIP架构，统一RGB-D多模态学习，通过深度引导几何模块（DGGM）将深度信息转换为几何先验注入注意力机制，并采用自适应密集通道集成平衡多层特征贡献。实验在OCID-VLG数据集及仿真和真实环境中验证，GeoLanG实现了精确鲁棒的语言引导抓取。",
      "detailedSummary": "## 研究背景与动机\n当前的语言引导抓取方法通常采用多阶段流水线，将目标感知（如检测、分割）与抓取规划分离。这种范式存在跨模态融合有限、计算冗余、以及在杂乱、遮挡或低纹理场景下泛化能力差等关键局限性。具体而言，现有的视觉编码器（如CLIP-ResNet和CLIP-ViT）各有不足：前者受限于静态层次结构，难以动态建模多尺度物体；后者计算成本高且对局部细节捕捉能力弱。此外，现有的RGB-D方法多采用双编码器架构，存在计算冗余，且低质量的深度数据可能干扰RGB特征学习。在跨模态对齐方面，常见策略未能充分利用多层次的视觉特征，导致任务相关信号被稀释。\n\n本文针对上述痛点，提出了一个统一、端到端的多任务视觉-语言框架，旨在通过深度引导的几何先验和自适应的多层特征融合，在复杂开放场景中实现鲁棒的语言引导抓取。其核心思路是：构建一个基于CLIP架构的端到端框架，通过深度引导几何模块将深度信息转化为几何先验并注入注意力机制，同时通过自适应密集通道集成模块动态聚合多层视觉特征，从而在共享表示空间内实现鲁棒的语义对齐与空间推理。\n\n## 方法详解\nGeoLanG是一个端到端的视觉-语言抓取框架。给定一个RGB-D图像和一个语言查询，文本编码器（CLIP-BERT）提取高级语义特征，生成词元嵌入和句子嵌入。视觉编码器采用CLIP-VMamba，以兼顾全局上下文建模与局部细节保留，并提取多尺度视觉特征。随后，深度引导几何模块利用深度图生成几何先验并增强视觉特征的空间推理能力。接着，自适应密集通道集成模块自适应地聚合多层视觉特征，生成更具判别力的视觉表示。最后，通过多模态融合颈部和多任务解码器，联合输出目标物体的分割掩码和抓取位姿。\n\n![方法框架总览](https://arxiv.org/html/2602.04231v1/x2.png)\n> **图2**：GeoLanG框架总览。给定RGB-D图像和语言指令，文本编码器和视觉编码器分别提取特征。深度引导几何模块注入几何先验，自适应密集通道集成模块优化多层特征。双路投影器最终生成像素级分割掩码并细化目标物体的抓取位姿。\n\n核心模块一：**深度引导几何模块**。该模块旨在有效利用深度信息增强空间推理，同时避免引入额外的深度编码器。首先，将RGB图像均匀划分为H×W个图像块，并计算每个对应深度图像块的平均深度值D_{m,n}。然后，计算任意两个图像块之间的深度差矩阵ΔD和曼哈顿空间距离矩阵ΔS。通过可学习参数λ1和λ2加权融合，得到统一的几何先验矩阵𝒢 = λ1·ΔD + λ2·ΔS，该矩阵编码了所有图像块之间的综合3D几何关系。最后，将该几何先验通过一个衰减因子η集成到自注意力机制中：\\hat{X} = (Softmax(QK⊤) ⊙ η𝒢) V⊤。较小的η𝒢值对应较大的几何距离，从而抑制不相关的键值对，强调几何上相关的部分。此设计使模型能有效捕捉空间依赖、物体形状变化和低纹理区域，且计算高效。\n\n![深度引导几何模块](https://arxiv.org/html/2602.04231v1/x3.png)\n> **图3**：深度引导几何模块概述。粉色矩形代表RGB编码器提取的图像特征，绿色矩形代表学习到的几何先验，黄色矩形代表由深度和RGB计算的空间先验。⊗表示矩阵乘法，⊙表示逐元素乘法。输出特征整合了视觉信息与几何空间线索，以增强多尺度表示。\n\n核心模块二：**自适应密集通道集成**。该模块旨在解决现有方法冻结主干网络、仅利用高层特征而导致中低层信息丢失的问题。首先，将从V-Mamba提取的L层图像特征划分为G个组，每组包含M = L/G个连续层。对于第g组中的M个连续层，通过一个轻量级门控网络为每层特征预测自适应权重α_i（组内Softmax归一化）。组级特征GC_g通过加权求和计算：GC_g = Σ α_i C_i。最后，将所有组级特征与最后一层特征C_L拼接，并通过一个MLP投影生成最终的视觉嵌入e_v。这种设计使ADCI能够根据输入动态平衡各层的贡献，同时保留密集的跨层连接，确保信息丰富的中低层特征被有效聚合，从而产生更丰富、更具表达力的视觉表示。\n\n与现有方法相比，GeoLanG的创新点具体体现在：1) 采用统一的端到端多任务框架，替代了分离感知与规划的多阶段流水线；2) 提出了DGGM，将深度信息作为显式的几何先验直接注入注意力机制，增强了遮挡和低纹理条件下的目标判别能力，且无需专用深度编码器；3) 提出了ADCI，通过自适应门控机制动态融合多层视觉特征，提升了跨模态对齐的判别力和泛化性。\n\n## 实验与结果\n实验主要在**OCID-VLG**数据集上进行，该数据集包含大量杂乱室内场景，提供了语言表达、分割掩码/边界框和抓取标注的多模态监督。实验平台使用了4张NVIDIA RTX 4090 GPU。评估指标包括：用于指代图像分割的IoU和Precision@X (X∈{0.5,0.6,0.7,0.8,0.9})；用于指代抓取合成的Jacquard指数 J@N（评估前N个抓取预测中与真值IoU>0.25且方向差<30°的比例）。\n\n对比的**baseline方法**包括：VLG、GraspCLIP、CLIPort、CTNet和CROG。\n\n![定性对比结果](https://arxiv.org/html/2602.04231v1/x5.png)\n> **图5**：在OCID-VLG数据集上与SOTA方法的定性对比。GeoLanG（最右列）在具有小物体、模糊边缘和遮挡的挑战性场景中（如第一、二行），能生成更精确、贴合物体边界的细粒度分割掩码和抓取位姿。而CROG和CTNet常产生过度简化的掩码或抓取漂移。\n\n**关键实验结果**：在OCID-VLG标准测试集上（表I），GeoLanG在分割和抓取任务上均达到最优。分割IoU为85.77%，显著优于CTNet（80.94%）和CROG（80.77%）。在更严格的Pr@70指标上达到89.82%。抓取任务中，J@1为87.32%，J@N为92.13%，分别超过CTNet 3.00和1.22个百分点。在**新颖实例分割**（表II）的泛化性测试中，GeoLanG同样领先，IoU达81.25%，J@1达83.96%，J@N达90.72%，显示出强大的泛化能力。\n\n![数据集示例](https://arxiv.org/html/2602.04231v1/x4.png)\n> **图4**：OCID-VLG数据集示例。(a)单个物品 (b)多个物品杂乱场景 (c)多个物体杂乱且重叠场景。该数据集为语言引导的感知与抓取提供了丰富的多模态标注。\n\n**消融实验**（表III）总结了各核心组件的贡献：1) **CLIP-VMamba骨干网络**：相比CLIP-ResNet基线，引入CLIP-VMamba使IoU从80.77%提升至83.25%，J@1从81.64%提升至83.44%，证实了其更强的多尺度表征能力。2) **深度引导几何模块**：在VMamba基础上加入DGGM，带来进一步性能提升（IoU 84.88%， J@1 85.35%），证明了深度几何先验对空间推理的有效性。3) **自适应密集通道集成**：最终集成所有组件（VMamba+DGGM+ADCI）获得最佳性能（IoU 85.77%， J@1 87.32%），表明ADCI通过自适应特征聚合增强了模型的判别力。\n\n![仿真实验](https://arxiv.org/html/2602.04231v1/x6.png)\n> **图6**：在RoboDK平台上的仿真实验设置。\n\n![实物机器人实验](https://arxiv.org/html/2602.04231v1/x7.png)\n> **图7**：在DOBOT Nova 2机器人上的实物实验。(a)(b)包含同类干扰物的挑战场景；(c)眼在手配置；(d)(e)孤立与杂乱场景下的抓取示例。\n\n**机器人实物实验**结果（表IV）显示，在模拟的孤立场景中，抓取准确率达到95%。在实物机器人测试中，孤立场景的分割和抓取准确率分别为85%和82.5%；在更具挑战的杂乱场景中，则分别降至80%和60%。实验表明，对于遮挡少或结构清晰的物体（如牙刷），性能接近完美，而对于严重遮挡或体积小的物体（如药盒），性能有所下降，但整体上GeoLanG展现了对新物体和复杂环境的较强泛化与鲁棒性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了GeoLanG，一个将RGB-D感知与自然语言理解统一到共享表示空间的端到端多任务框架，提升了在复杂环境中的语义对齐与泛化能力；2) 设计了深度引导几何模块，将深度信息编码为显式几何先验并注入注意力机制，增强了模型在遮挡和低纹理条件下的空间推理与目标判别能力；3) 提出了自适应密集通道集成模块，通过动态门控机制自适应聚合多层视觉特征，生成了更具判别力和泛化性的视觉表征。\n\n论文自身提到的局限性在于：当前框架主要专注于4-DoF平面抓取，简化了夹爪朝向和交互动力学，这限制了对更广泛操作场景的适用性。\n\n本文对后续研究的启示在于：1) 验证了将几何先验显式、轻量化地集成到视觉-语言模型中的有效性，为多模态机器人感知提供了新思路；2) 动态、自适应的多层次特征融合策略可提升跨模态对齐的精度，值得在其他需要细粒度对齐的任务中探索；3) 端到端的多任务学习范式（联合分割与抓取）有助于减少级联误差，推动机器人操作向更智能、更通用的方向发展。未来的工作可以探索更高自由度的抓取表示，并进一步研究如何更好地处理深度传感器噪声和严重的实物遮挡问题。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04231v1/x1.png",
        "https://arxiv.org/html/2602.04231v1/x2.png",
        "https://arxiv.org/html/2602.04231v1/x3.png",
        "https://arxiv.org/html/2602.04231v1/x4.png",
        "https://arxiv.org/html/2602.04231v1/x5.png",
        "https://arxiv.org/html/2602.04231v1/x6.png",
        "https://arxiv.org/html/2602.04231v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04228",
      "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2602.04228",
      "arxivId": "2602.04228",
      "date": "2026-02-04",
      "authors": "Badong Chen Team",
      "category": "Manipulation",
      "summary": "本文针对连续动作视觉-语言-动作（VLA）模型使用均方误差（MSE）回归时对个体预测误差约束过强、忽略整体误差分布的问题，提出通过重塑动作误差分布来提升模型可靠性。关键技术是引入信息论中的最小误差熵（MEE），设计轨迹级MEE目标及其两个加权变体，与MSE结合进行训练。实验在标准、少样本和噪声设置下，使用LIBERO等模拟基准和真实机器人任务，结果表明该方法能持续提高成功率和鲁棒性，在数据不平衡时增益稳定，且额外训练成本可忽略。",
      "detailedSummary": "## 研究背景与动机\n在机器人操作领域，视觉-语言-动作模型已成为学习通用和可扩展策略的有前景范式。现有VLA框架大多依赖标准监督目标，对于连续动作回归通常采用均方误差损失。MSE对单个预测施加了强约束，但其仅从个体预测层面运作。从分析视角看，动作预测误差可被视为从底层误差分布中抽取的样本，该分布随时间演变并可能呈现结构化模式。然而，点对点回归目标并未显式地调控误差的整体组织和几何结构。从分布视角看，策略行为不仅由个体误差大小决定，也受误差分布的整体不确定性、集中度和结构影响。信息论准则为刻画此类分布级属性提供了自然框架，其中最小误差熵准则直接最小化误差分布的熵，使得批次或轨迹内的误差能够相互作用。本文针对连续动作VLA模型，提出超越传统MSE回归，在训练过程中重塑动作误差分布。核心思路是引入基于信息论的最小误差熵准则，提出轨迹级MEE目标及其加权变体，并与MSE结合，在优化个体误差的同时，促使误差分布更加紧凑和结构化。\n\n## 方法详解\n本文方法的核心是在标准连续动作VLA模型的训练中，引入一个分布级的监督信号，与点级的MSE损失协同工作。\n\n![方法框架](https://arxiv.org/html/2602.04228v1/x2.png)\n\n> **图2**：本工作评估的连续动作VLA模型架构分类。总结了代表性小规模和大规模架构。(a-b) 小规模模型使用轻量级主干从多模态特征回归动作；(c-f) 大规模模型基于预训练的VLM构建。\n\n整体框架遵循行为克隆范式：给定包含视觉观察、本体感觉状态和语言指令的输入，VLA策略模型输出连续动作。训练时，除了计算预测动作与专家动作之间的标准MSE损失外，关键创新在于计算一个额外的轨迹级最小误差熵损失。\n\n核心模块是轨迹级MEE及其加权变体。首先，将批次内所有轨迹、所有时间步、所有动作块维度的预测误差向量 $\\mathbf{e}_{b,k}^{t}$ 扁平化为一个集合 $\\{\\mathbf{e}_i\\}_{i=1}^N$，其中 $N = B \\times T \\times K$。这允许将整个训练批次内的动作误差视为来自一个共享误差分布的样本。\n\n基础目标是轨迹级MEE，采用二次Rényi熵的估计器：\n$\\mathcal{L}_{\\mathrm{T\\text{-}MEE}} = -\\log\\left(\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\exp\\left(-\\frac{\\|\\mathbf{e}_{i}-\\mathbf{e}_{j}\\|^{2}}{2\\sigma^{2}}\\right)\\right)$\n该目标最小化误差分布的熵，鼓励误差样本在误差空间中聚集，形成紧凑的分布。\n\n为进一步控制误差塑形，论文引入了两种加权变体。首先为每个误差样本分配一个基于其幅度的权重 $w_i$（幅度越小权重越大）。然后定义加权损失 $\\mathcal{L}_{\\mathrm{W\\text{-}TMEE}} = -\\log\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\omega_{ij}\\exp\\left(-\\|\\mathbf{e}_{i}-\\mathbf{e}_{j}\\|^{2}/2\\sigma^{2}\\right)$。通过设置 $\\omega_{ij} = \\frac{1}{N^2}w_i$ 得到**块加权T-MEE**，它强调可靠的动作块；通过设置 $\\omega_{ij} = w_i w_j$ 得到**元素加权T-MEE**，它进行对称的、元素级的加权。\n\n最终的训练目标是MSE与T-MEE的加权和：$\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{MSE}} + \\alpha \\mathcal{L}_{\\mathrm{T\\text{-}MEE}}$。MSE提供将每个误差锚定在零附近的点级精度，而T-MEE则在分布层面塑造误差的几何结构。\n\n与现有方法相比，创新点在于：1) **视角创新**：从误差分布的整体几何结构，而非单个误差值来看待监督信号；2) **方法创新**：将经典信息论中的MEE准则适配到具有时空相关性的VLA动作预测场景，提出了轨迹级 formulation；3) **实现创新**：设计了加权机制以提供更精细的误差聚合控制，并与标准MSE无缝集成。\n\n## 实验与结果\n实验使用了仿真基准LIBERO和SimplerEnv，以及真实机器人操作任务。LIBERO包含Spatial, Goal, Object, Long四个任务套件，共40个任务。SimplerEnv评估四个WidowX操作任务。真实实验使用Agilex Cobot Magic平台，执行三类操作任务和一个分布外任务。\n\n对比的基线方法涵盖了论文图2中列出的多种VLA架构，包括小规模的BC-RNN、BC-Transformer、BC-DP，以及大规模的GR00T、OFT、$\\pi_0$、DS-VLA。所有大规模模型统一使用Qwen3-VL作为视觉语言主干以确保公平比较。\n\n关键实验结果如下：在LIBERO上，T-MEE为所有模型带来了性能提升。对于小模型提升显著，例如BC-Transformer平均成功率从52.6%提升至63.5%，BC-RNN从15.0%提升至28.3%。对于大模型，在已经很高的基础上仍有稳定提升，例如$\\pi_0$从96.8%提升至98.4%，DS-VLA从95.8%提升至98.2%。\n\n![主结果表](https://arxiv.org/html/2602.04228v1/x3.png)\n\n> **图3**：LIBERO上不同MEE变体的性能比较。所有基于MEE的目标均一致优于基线，不同变体在不同架构和任务套件上表现出互补优势。\n\n在SimplerEnv上，T-MEE在大多数模型和任务上带来提升或持平，尤其在更具挑战性的任务上改善明显，例如在2B基座VLM下，DS-VLA在“Stack Cube”任务上的成功率从12.5%提升至25.0%。\n\n![真实实验](https://arxiv.org/html/2602.04228v1/x4.png)\n\n> **图4**：真实世界评估。(a) 机器人设置和代表性操作任务。(b) 任务成功率对比，显示T-MEE在所有任务上带来了持续的性能增益。\n\n消融实验方面：论文比较了T-MEE、Cw-TMEE和Ew-TMEE三种变体。结果表明，所有MEE变体均优于纯MSE基线，但最佳变体取决于具体架构和任务。例如，对于BC-Transformer，Ew-TMEE在Spatial套件上表现最佳；而对于GR00T，T-MEE和Cw-TMEE在不同套件上各有优势。这说明了分布级误差塑形的灵活性，以及根据具体问题调整加权策略的价值。\n\n![误差分布可视化](https://arxiv.org/html/2602.04228v1/x1.png)\n\n> **图1**：使用PCA可视化有/无轨迹级MEE时的动作误差分布。结合T-MEE训练后，误差分布在投影空间中变得更加紧凑和连贯，同时任务成功率得到提升。\n\n![数据不平衡分析](https://arxiv.org/html/2602.04228v1/x5.png)\n\n> **图5**：在LIBERO-Object上，随着任务间数据不平衡程度增加，T-MEE相对于MSE基线的性能增益变化。在一个特征明确的操作区间内增益持续存在，但在极度不平衡时可能有害。\n\n![噪声鲁棒性](https://arxiv.org/html/2602.04228v1/x6.png)\n\n> **图6**：在注入不同程度高斯噪声和脉冲噪声的专家动作上训练BC-Transformer。T-MEE在所有噪声水平下均优于MSE，尤其是在高噪声和非高斯脉冲噪声下表现出更强的鲁棒性。\n\n## 总结与启发\n本文的核心贡献包括：1) **提出了一个重塑动作误差分布的框架**：将信息论中的最小误差熵准则引入VLA训练，提出了轨迹级MEE及其加权变体，从分布视角对连续动作误差进行监督。2) **提供了深入的理论分析**：揭示了MEE通过相似性加权促使误差交互聚类的机制，证明了其对非高斯噪声和异常值的鲁棒性，并分析了多任务场景下由误差空间重叠和样本不平衡导致的耦合现象。3) **进行了广泛的实证验证**：在多种VLA架构、模型规模以及平衡、少样本、噪声、不平衡等多种数据场景下，证明了方法的有效性、鲁棒性和可忽略的额外训练成本。\n\n论文自身提到的局限性主要在于数据严重不平衡时：理论分析表明，当任务间误差分布重叠且样本极度不平衡时，MEE可能使少数任务的学习被多数任务主导，导致性能下降。图5的实验也证实了在极度不平衡区域性能增益消失甚至为负。\n\n对后续研究的启示：1) **分布视角的价值**：超越点对点损失，关注预测误差的整体分布特性，是提升模型鲁棒性和泛化能力的一个有效途径。2) **理论指导实践**：论文的理论分析不仅解释了方法为何有效，还明确了其有效的操作区间（如数据不平衡程度），为实际应用提供了指导。3) **轻量级改进**：该方法几乎不增加推理成本，训练开销也可忽略，为提升现有VLA模型性能提供了一种高效的即插即用选项。未来工作可探索更复杂的误差分布建模，或将此思想扩展到离散动作VLA模型及其他序列预测任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04228v1/x1.png",
        "https://arxiv.org/html/2602.04228v1/x2.png",
        "https://arxiv.org/html/2602.04228v1/x3.png",
        "https://arxiv.org/html/2602.04228v1/x4.png",
        "https://arxiv.org/html/2602.04228v1/x5.png",
        "https://arxiv.org/html/2602.04228v1/x6.png",
        "https://arxiv.org/html/2602.04228v1/x7.png",
        "https://arxiv.org/html/2602.04228v1/x8.png",
        "https://arxiv.org/html/2602.04228v1/x9.png",
        "https://arxiv.org/html/2602.04228v1/x10.png",
        "https://arxiv.org/html/2602.04228v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04213",
      "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons",
      "url": "http://arxiv.org/abs/2602.04213",
      "arxivId": "2602.04213",
      "date": "2026-02-04",
      "authors": "Reid Simmons Team",
      "category": "Manipulation",
      "summary": "本文提出InterPReT方法，旨在解决非专业用户难以通过传统模仿学习有效教导AI智能体的难题。其核心技术为“交互式策略重构与训练”，允许用户通过指令交互式地更新策略结构，并通过演示优化参数，从而让用户能监控性能并审查决策过程。在一项34人参与的赛车游戏教学用户研究中，与通用模仿学习基线相比，该方法在由非专业用户提供演示并决定训练停止时，能产生更鲁棒的策略，且不损害系统可用性，证明了其对无技术背景用户的适用性。",
      "detailedSummary": "## 研究背景与动机\n模仿学习通过模仿专家演示已在许多任务中取得成功。然而，现有工作大多依赖于技术专业人员提供的大规模演示以及对训练过程的密切监控。当普通人想要教授智能体新技能时，这些要求具有挑战性。普通人可能不知道哪种演示更适合模仿学习，也不清楚学习到的策略何时足够好以停止训练。此外，操作不熟悉的遥操作设备会显著降低效率，收集大量数据也容易导致认知疲劳。现有面向普通人的工作主要关注处理人类教师的不完美性，或应用于特定用户群体。一些工作探索了借助语言指令进行少量演示学习，以减少所需演示数量，但这仍要求人类专家一次性详细指定所有领域知识，对于普通人而言在复杂任务中非常困难。\n\n本文针对普通人难以一次性提供完整领域知识和判断训练何时停止的痛点，提出了一种交互式学习范式。核心思路是：让智能体通过多轮交互，利用用户提供的自然语言指令（通过大语言模型）动态重构策略的**结构**，并利用用户提供的演示数据优化策略的**参数**，从而逐步构建出鲁棒且符合用户意图的策略。\n\n## 方法详解\nInterPReT 的核心是让智能体维护一个指令集和一个演示集，并据此交互式地更新策略结构和权重。智能体实例被形式化为一个四元组 𝒜𝑖 = ⟨ℐ𝑖, 𝒟𝑖, ⟦α⟧𝑖, Θ𝑖⟩，其中 ℐ 是指令集，𝒟 是演示集，⟦α⟧ 是策略结构，Θ 是策略权重。\n\n![方法框架](https://arxiv.org/html/2602.04213v1/x1.png)\n\n> **图1**：InterPReT 中的交互模式。用户与智能体反复交互直至满意。展示了四种交互模式：1）用户给出指令，智能体重构其策略（左上）；2）用户给出演示，智能体训练其权重（左下）；3）智能体用自然语言总结其策略（右上）；4）智能体演示其策略供用户检查（右下）。\n\n**结构化策略**：本文方法建立在结构化策略之上。一个结构化策略是一个四元组 ⟨V, P, E, Θ⟩，表示一个加权的有向无环二分图。其中 V 是代表特征（观测值、隐变量或动作）的节点集合，具有语义含义；P 是代表可微分操作实例（如线性组合、常数）的节点集合；E 是连接变量节点和操作节点的边集合；Θ 是与每条边关联的权重集合。该结构通常是稀疏的，有助于减少参数数量并缓解因果混淆问题。策略的推理过程是在该计算图上按边传播值。\n\n![结构化策略示例](https://arxiv.org/html/2602.04213v1/x2.png)\n\n> **图2**：表示维持恒定“期望速度”的比例控制器的结构化策略最小示例。实线框是 V 中的变量（标记为观测 O、隐变量 L 或动作 A），虚线框是 P 中的操作符。权重 Θ 与边关联。图中展示了推理过程的一个数值例子。\n\n**策略训练**：给定一个策略结构 ⟦α⟧𝑖 和一组演示 𝒟𝑖，使用标准的模仿学习目标来训练权重 Θ𝑖。由于特征和操作符具有语义，实践中会查询大语言模型（LLM）来估计它们之间的因果效应，并以此初始化权重 Θ₀，然后使用 Adam 优化器进行梯度下降。当获得新演示 D 时，智能体更新其演示集并重新训练权重。\n\n**策略重构**：这是方法的核心创新。给定一组指令 ℐ𝑖，目标是生成一个遵循指令且在任务域中表现良好的策略结构 ⟦α⟧𝑖。由于普通人可能无法完整指定策略的所有细节，本方法依赖 LLM 的现有世界知识来做出合理的设计选择。通过**思维链提示**和**上下文学习**来增强鲁棒性：向 LLM 提供一个来自完全不同任务（Lunar Lander）的指令-模型对示例，并引导其执行以下步骤：1) 从指令中提取所有相关特征（形式化 V）；2) 用英文段落描述结构（填补指令缺失的细节）；3) 规划所有连接和操作（形式化 P 和 E）；4) 组装所有组件生成可执行的 PyTorch 模型。生成英文描述作为中间步骤已被实证证明能提高生成质量。当收到新指令 I 时，智能体更新其指令集，并调用 Restructure 函数生成新结构，随后在新结构上使用现有演示数据重新训练权重。\n\n**策略总结与演示**：在结构生成过程中产生的英文描述可直接作为智能体策略的总结呈现给用户，帮助用户理解智能体的决策逻辑。用户还可以让智能体在特定初始配置下进行策略演示（rollout），以直观检查其行为。\n\n与现有方法相比，InterPReT 的创新点具体体现在：1) **交互性**：允许用户通过多轮指令和演示逐步教学，而非一次性提供所有知识；2) **结构化生成**：利用 LLM 将模糊的自然语言指令转化为具有明确因果关系的、可微分的策略计算图结构；3) **可解释性**：基于语义化的策略结构，能够向用户提供策略总结和行为演示，形成反馈闭环。\n\n## 实验与结果\n**实验设置**：研究在 gymnasium 包的赛车环境中进行。任务目标是以最快速度完成赛道，关键指标是有效平均速度（EAS）。状态表示为结构化信息（当前速度、航向、前方固定数量瓦片的信息等），动作空间是转向、油门和刹车的连续控制。\n\n**用户研究**：进行了组间用户研究（N=34），参与者主要为无 AI 算法开发经验的普通人。设置了两种条件：1) **实验条件**：参与者可同时提供指令和演示，模型使用 InterPReT 训练；2) **基线条件**：参与者仅提供演示，策略由一个多层感知机（MLP）表示，使用相同的模仿学习配置训练。\n\n![名义条件下的平均速度](https://arxiv.org/html/2602.04213v1/figures/results/nominal_boxplot.jpg)\n\n> **图4**：名义条件下学习策略的平均速度箱线图。InterPReT 策略的中位数（49）与基线 MLP 策略（49）相当，但 InterPReT 的策略性能分布更集中（四分位距更小），表明其能产生更一致的策略质量。\n\n![各轮交互的平均速度趋势](https://arxiv.org/html/2602.04213v1/figures/results/per_run_violin_average_speed.jpg)\n\n> **图5**：按交互轮次划分的策略平均速度小提琴图。随着交互轮次增加，InterPReT 策略的性能提升更明显且更高效，而基线 MLP 策略的提升则相对平缓。\n\n![用户预期与真实性能对比](https://arxiv.org/html/2602.04213v1/figures/results/expectation_boxplot.jpg)\n\n> **图6**：用户对策略性能的预期与其实测性能对比的箱线图。使用 InterPReT 的用户，其预期更接近智能体的真实性能，表明策略总结和演示帮助用户形成了更准确的认知。\n\n![系统可用性量表得分](https://arxiv.org/html/2602.04213v1/figures/results/sus_boxplot.jpg)\n\n> **图7**：系统可用性量表（SUS）得分的箱线图。InterPReT 系统的可用性得分中位数（72.5）显著高于基线系统（57.5），表明交互式教学框架并未损害可用性，反而可能更易用。\n\n![消融实验：平均速度](https://arxiv.org/html/2602.04213v1/figures/results/ablation_average_speed_boxplot.jpg)\n\n> **图8**：消融实验的平均速度箱线图。对比了完整 InterPReT、仅使用指令初始化结构但固定不变（No Restructure）、以及随机初始化结构（No LLM Init）三种设置。完整 InterPReT 性能最佳，证明了交互式结构重构和 LLM 权重初始化的有效性。\n\n**关键实验结果总结**：\n1.  **策略鲁棒性**：在名义条件下，InterPReT 训练出的策略在平均速度中位数上与基线相当（49 vs 49），但其性能分布更集中，产生低性能策略的几率更低。\n2.  **学习效率**：随着交互轮次增加，InterPReT 策略的性能提升更高效。\n3.  **用户认知**：InterPReT 用户对智能体性能的预期更准确。\n4.  **系统可用性**：InterPReT 的系统可用性量表得分显著更高（中位数72.5 vs 57.5）。\n5.  **消融实验**：验证了交互式结构重构和 LLM 权重初始化各自对最终性能的积极贡献。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个面向最终用户的交互式教学框架 InterPReT，它允许普通人通过多轮自然语言指令和动作演示来共同构建和优化智能体策略，其中指令用于通过 LLM 生成语义化的策略结构，演示用于优化该结构的参数。\n2.  设计并实施了一项针对非技术背景用户的严格研究，验证了该框架在产生鲁棒策略、高效交互改进以及整体可用性方面的优势。\n\n**局限性**：论文提到，方法的性能部分依赖于 LLM 的代码生成能力和对任务的理解。此外，结构化策略的表达能力可能受限于其可微分操作符的集合。\n\n**对后续研究的启示**：\n1.  **交互式与渐进式教学**是降低模仿学习门槛的关键，允许用户“边教边改”，符合人类自然教学习惯。\n2.  **利用语义化与结构化表征**（如本文的结构化策略）不仅能提升学习效率、缓解因果混淆，还为实现策略可解释性和人机互信提供了基础。\n3.  将 **LLM 的世界知识和代码生成能力**与传统的优化学习（如梯度下降）相结合，为创建更易于普通人定制和理解的 AI 智能体开辟了新途径。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04213v1/x1.png",
        "https://arxiv.org/html/2602.04213v1/x2.png",
        "https://arxiv.org/html/2602.04213v1/figures/illustrations/render.png",
        "https://arxiv.org/html/2602.04213v1/figures/results/nominal_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/demonstration_steps_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/per_run_violin_average_speed.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/expectation_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/sus_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/trend_per_participant_average_speed_single.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/instruction_token_count_histogram.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/ablation_average_speed_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/illustrations/home_page.png",
        "https://arxiv.org/html/2602.04213v1/figures/illustrations/demo_page.png",
        "https://arxiv.org/html/2602.04213v1/figures/background/age_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/gender_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/game_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/driving_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/ai_dev_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/data_countplot.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.03973",
      "title": "VLS: Steering Pretrained Robot Policies via Vision-Language Models",
      "url": "http://arxiv.org/abs/2602.03973",
      "arxivId": "2602.03973",
      "date": "2026-02-03",
      "authors": "Ranjay Krishna Team",
      "category": "Manipulation",
      "summary": "本文提出VLS框架，解决预训练扩散或流匹配策略在测试时遭遇空间配置或任务语义分布偏移时的适应性问题。核心方法为无需训练的推理时引导技术：利用视觉语言模型合成轨迹可微的奖励函数，在去噪采样过程中直接调整冻结策略的动作分布，无需修改参数。实验表明，VLS在CALVIN和LIBERO-PRO基准上分别取得31%和13%的性能提升，并在真实机器人上验证了对空间与语义偏移的鲁棒适应性。",
      "detailedSummary": "## 研究背景与动机\n当前机器人模仿学习的主流方法是利用大规模专家数据训练生成式策略，特别是基于扩散模型或流匹配的目标函数，这些策略在训练分布内表现出色。然而，当测试时的观察或语言指令偏离训练分布时，例如在障碍物附近、支撑面偏移或存在轻微杂物的场景中执行相同任务，这些策略往往会失败。这种失败并非源于缺失运动技能，而是暴露了模仿学习在训练-测试偏移下的一个根本局限性：动作生成与训练时特定的空间配置和任务规范紧密耦合。通过重新训练或微调来解决这些失败既成本高昂，又在概念上不匹配，因为所需的行为已经存在于训练数据中，只是无法在测试时有选择地适配。\n\n本文针对上述痛点，提出了一种新的视角：将适配问题视为一个推理时的控制问题，而非参数更新问题。本文的核心思路是提出**视觉语言引导**框架，在不修改预训练策略参数的前提下，利用视觉语言模型合成轨迹可微的奖励函数，在推理时引导去噪采样过程，使生成的动作轨迹满足测试时的空间和任务要求。\n\n## 方法详解\nVLS框架旨在为冻结的预训练基础策略提供推理时适配能力。其核心是构建一个可微的替代评分函数 ℛ，以近似难以直接获取的似然项 log p((o,l)_OOD | a)，并利用其梯度引导去噪过程。\n\n![方法框架](https://arxiv.org/html/2602.03973v1/figure/Figure21.png)\n> **图1**：VLS整体流程概览。给定RGB-D观察和语言指令，系统首先将输入接地为空间关键点集；随后，视觉语言模型生成一系列阶段感知的可微程序化奖励函数；在基础策略的去噪采样循环中，注入奖励梯度并结合RBF排斥项与Feynman-Kac重采样机制进行修正；最后，基于奖励反馈构建闭环阶段切换系统，以鲁棒地完成长视野任务。\n\n整体流程包含三个核心组件：\n1.  **OOD输入接地与奖励生成**：首先，利用视觉语言模型识别任务相关物体，并借助Segment Anything Model获取物体掩码。结合DINOv2提取的语义特征和深度信息，将掩码像素反投影为3D点云，并聚类得到一组任务相关的3D关键点 𝒫 = {p_i}，从而将高维OOD输入压缩为几何支架。随后，查询VLM以（i）将任务分解为S个连续阶段，（ii）为每个阶段s生成一个可微的程序化奖励函数 ℛ_s(a, (o,l)_OOD) = f_VLM(a, 𝒫, s)。这些奖励函数由PyTorch可微操作（如距离、点积、软约束）构成，可对动作提案a进行评分并计算梯度 g_s = ∇_a ℛ_s。\n2.  **推理时去噪引导**：在基础策略（扩散或流匹配）的每一步去噪迭代k中，引导一批B个动作提案。为了在复杂、多模态的奖励地形中高效搜索并保持多样性，VLS结合了三种机制：\n    *   **多样化提案初始化与排斥力**：在去噪早期，引入基于径向基函数的排斥梯度，促使批次内的动作提案相互远离，覆盖更广的动作流形。\n    *   **梯度引导精炼**：将当前阶段的奖励梯度g_s注入去噪更新。对于扩散策略，修正后的噪声预测为 ε̂ = ε - λ·√(1-ᾱ_k)·g_s；对于流匹配策略，修正后的速度场为 v̂ = v + λ·g_s。其中λ为引导强度。采用类似MCMC的多步内部更新以提高稳定性。\n    *   **基于Feynman-Kac的梯度无关重采样**：周期性地将动作提案视为粒子系统进行重采样。每个粒子i的权重由 w_i^k ∝ exp(ℛ_s(a^k[i])) 计算，然后进行多项式重采样。这使高奖励粒子得以复制，淘汰违反约束的提案。\n3.  **闭环执行控制与阶段切换**：为处理物理不确定性并协调多阶段任务，VLS引入闭环控制。\n    *   **自适应引导强度**：在同一阶段内，根据当前动作块获得的奖励值 ℛ_s^t 相对于该阶段第一个动作块奖励 ℛ_s^base 的比例，动态调整引导强度 λ_t。当执行偏离约束时增强引导，接近完成时则减弱，让基础策略主导精细操作。\n    *   **基于施密特触发器的阶段切换**：为每个阶段s设定高、低两个奖励阈值 R_high 和 R_low。根据 ℛ_s^t 与阈值的关系，决定是“推进阶段”、“维持阶段”还是“强化阶段”（可能触发重试）。这种迟滞逻辑避免了在阶段边界附近的振荡行为。\n\n与现有方法相比，VLS的创新点在于：1) 将适用于大语言模型和图像生成的推理时引导范式扩展至机器人领域，将动作生成视为可控去噪过程；2) 创造性利用VLM的跨模态推理能力，直接从OOD观察-语言输入合成**密集的、阶段感知的、可微的**程序化奖励函数作为引导信号，而非进行离散的验证或选择；3) 设计了结合梯度引导、粒子多样性维持与重采样的混合引导机制，以及自适应的闭环执行控制逻辑。\n\n## 实验与结果\n实验在仿真和真实世界两个环境中进行，评估VLS在测试时面对观察和语言偏移时的适应能力。\n\n**基准测试与基线方法**：\n*   **仿真**：使用了CALVIN和LIBERO-PRO两个广泛使用的操作测试套件，它们明确强调对OOD观察-语言输入的推理时适应。\n*   **基线**：对比了先前的推理时引导方法，包括ITPS（基于人机交互信号）、DynaGuide（基于动力学模型引导），以及价值函数引导方法（V-GPS, VGD）。在LIBERO-PRO上，还测试了VLS对多个冻结的VLA基础策略（如OpenVLA， π_0, π_0.5变体）的提升效果。\n*   **真实世界**：在Franka机器人上部署，测试在未见过的物体外观、位置变化和目标替换下的多阶段语言指定任务。\n\n**关键实验结果**：\n在CALVIN上，VLS consistently outperforms prior inference-time steering methods such as ITPS [49] and DynaGuide [16], achieving up to a 31% absolute improvement in success rate on long-horizon tasks.\n在LIBERO-PRO上，VLS improves the success rate of frozen VLA policies, including OpenVLA [28], π_0 [3], variants of π_0.5 [4, 31] by up to 13% under both spatial (object layout) and semantic (task specification) perturbations.\n\n![CALVIN结果](https://arxiv.org/html/2602.03973v1/figure/CALVINresult.png)\n> **图2**：在CALVIN数据集上的成功率对比。VLS（橙色）在长视野任务上显著优于所有基线方法（ITPS、DynaGuide等），取得了最高达31%的绝对提升。\n\n![消融实验](https://arxiv.org/html/2602.03973v1/x1.png)\n> **图3**：VLS各组件贡献的消融研究。移除VLM奖励（“w/o VLM Reward”）、Feynman-Kac重采样（“w/o Resampling”）或自适应阶段切换（“w/o Stage Switch”）都会导致性能下降，验证了每个组件的必要性。\n\n![LIBERO-PRO结果](https://arxiv.org/html/2602.03973v1/figure/pi05results.png)\n> **图4**：在LIBERO-PRO上使用π_0.5作为基础策略的结果。VLS在空间扰动（不同物体布局）和语义扰动（不同任务描述）下均能稳定提升策略性能（平均提升13%），显示了其强大的OOD适应能力。\n\n真实世界实验进一步证明，VLS能够使机器人在面对未知物体外观、位置变化和目标替换时，稳定执行多阶段任务，验证了其在实际部署中的有效性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了VLS框架**：一种无需训练、基于推理时引导的通用方法，能够使冻结的预训练生成式策略适应分布外的观察和语言指令，将技能执行与任务规范解耦。\n2.  **利用VLM合成可微奖励**：创新性地利用视觉语言模型的推理能力，将复杂的OOD输入转化为程序化的、阶段感知的可微奖励函数，为去噪过程提供密集的梯度信号。\n3.  **实现了混合引导与闭环控制**：结合了梯度引导、粒子多样性维持、Feynman-Kac重采样以及自适应的闭环阶段切换机制，实现了对复杂约束地形的高效、鲁棒探索。\n\n**局限性**：论文提到，VLS的性能依赖于VLM对场景和任务理解的准确性，以及其生成合理、可微奖励程序的能力。此外，虽然无需训练，但推理时的引导步骤（特别是多粒子采样与重采样）会引入额外的计算开销。\n\n**对后续研究的启示**：VLS展示了将大模型“推理时引导”范式成功迁移到机器人领域的潜力。后续工作可以探索：1) 如何设计更高效、更准确的VLM提示与奖励程序生成机制；2) 如何进一步降低推理时计算成本，使其更适合实时控制；3) 将该框架扩展至其他类型的生成模型或更复杂的多模态指令场景。",
      "imageUrls": [
        "https://arxiv.org/html/2602.03973v1/figure/Figure21.png",
        "https://arxiv.org/html/2602.03973v1/figure/CALVINresult.png",
        "https://arxiv.org/html/2602.03973v1/x1.png",
        "https://arxiv.org/html/2602.03973v1/figure/pi05results.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.03668",
      "title": "MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction",
      "url": "http://arxiv.org/abs/2602.03668",
      "arxivId": "2602.03668",
      "date": "2026-02-03",
      "authors": "Jungwoo Lee Team",
      "category": "Manipulation",
      "summary": "本文提出MVP-LAM模型，旨在解决从无标注视频中学习潜在动作时，因视角变化引入噪声、导致潜在动作与真实动作关联性弱的核心问题。其关键技术是利用时间同步的多视角视频，通过跨视角重建目标进行训练：从一个视角推断的潜在动作必须能预测另一视角的未来状态，从而剥离视角特异性干扰。实验表明，在Bridge V2数据集上，MVP-LAM学得的潜在动作与真实动作互信息更高，动作预测性能更优（包括分布外评估）。使用该潜在动作预训练VLA模型，在SIMPLER和LIBERO-Long基准上提升了下游操作性能。",
      "detailedSummary": "## 研究背景与动机\n当前，利用海量人类视频学习潜在动作（Latent Action）是扩展机器人学习、突破特定本体数据限制的重要途径。这些学习到的潜在动作可作为伪动作标签，用于视觉-语言-动作模型的预训练。为了确保VLA预训练有效，潜在动作必须在缺乏真实动作标签的情况下，依然包含关于智能体底层动作的信息。然而，现有方法通常从单视角视频中学习潜在动作，面临一个关键障碍：外源性噪声。其中，视角变化是主要噪声源，相机移动和透视变化会与智能体动作产生的视觉变化纠缠在一起，导致学习到的潜在动作过度拟合于视角相关的线索，从而降低了对真实动作的预测性。\n\n本文针对视角变化干扰潜在动作学习这一具体痛点，提出了利用多视角视频进行学习的新视角。其核心思路是：通过时间同步的多视角视频和交叉视角重建目标，迫使从一个视角推断出的潜在动作能够解释另一个视角的未来观测，从而减少对视角特定线索的依赖，学习到更具动作中心性的离散潜在动作。\n\n## 方法详解\nMVP-LAM的整体框架基于向量量化变分自编码器，其训练需要时间同步的多视角视频。对于每个时间步，模型从两个视角的连续观测特征中分别推断出潜在动作，并通过两种重建目标进行优化。\n\n![方法框架](https://arxiv.org/html/2602.03668v1/x2.png)\n\n> **图2**：MVP-LAM使用时间同步多视角视频进行训练。(1) 自视角重建（左）：对于每个视角v，冻结的DINOv2提取特征，时空编码器产生连续潜在表示并向量量化为离散token，解码器从中重建下一帧特征。(2) 交叉视角重建（右）：MVP-LAM交换不同视角间的潜在token，并用它来重建另一视角的未来特征，鼓励潜在token捕捉固有的状态转移信息。\n\n核心模块包括编码器、向量量化码本和解码器。给定时间同步的图像对，首先使用DINOv2编码器提取物体中心的观测特征。对于每个视角，编码器接收当前帧和下一帧的特征，输出一个连续潜在表示，随后通过向量量化操作映射为码本中的离散token。解码器的任务是根据当前观测和潜在动作预测下一观测。\n\nMVP-LAM的创新点在于其训练目标，它结合了两种重建损失：\n1.  **自视角重建损失**：使用从同一视角推断出的潜在动作，来重建该视角的下一帧特征。\n2.  **交叉视角重建损失**：交换两个同步视角的潜在动作，即使用从视角v1推断出的潜在动作来预测视角v2的下一帧特征，反之亦然。\n\n总损失函数为：。其中和为VQ-VAE标准的量化损失和承诺损失。交叉视角重建是关键，由于解码器不以潜在动作的视角信息为条件，如果潜在动作编码了视角特定信息，将导致交叉视角预测失败。最小化该损失等价于减少潜在动作对视角的依赖性，从而提升其动作中心性。\n\n## 实验与结果\n实验使用了多个基准数据集：Bridge V2用于评估潜在动作的动作中心性；SIMPLER和LIBERO-Long用于评估下游操作性能。实验平台涉及VLA模型的预训练与微调。\n\n对比的基线方法包括：潜在动作模型UniVLA、LAPA、Moto；以及VLA模型OpenVLA、Octo和。\n\n关键实验结果如下：\n在动作中心性评估上，MVP-LAM在Bridge V2上取得了潜在动作与真实动作之间最高的互信息估计值。\n\n![互信息估计](https://arxiv.org/html/2602.03668v1/x3.png)\n\n> **图3**：在Bridge V2上使用KSG、BA和MINE估计器评估的潜在动作与真实动作的互信息。MVP-LAM在所有估计器上均获得最高值。\n\n同时，使用简单线性层从潜在动作预测真实动作时，MVP-LAM在Bridge V2（分布内）和LIBERO多个子集（分布外）上获得了最低的归一化均方误差。\n\n![线性探测结果](https://arxiv.org/html/2602.03668v1/x4.png)\n\n> **图4**：线性层从潜在动作预测动作的NMSE结果。MVP-LAM在Bridge V2和大多数LIBERO OOD数据集上表现最佳。\n\n在下游操作性能上，使用MVP-LAM的潜在动作进行VLA预训练，在SIMPLER基准的四个任务上平均成功率达到了60.4%，显著优于使用UniVLA潜在动作预训练的基线（39.6%）。\n\n![SIMPLER结果](https://arxiv.org/html/2602.03668v1/x7.png)\n\n> **表1**：SIMPLER基准结果。MVP-LAM在平均成功率上领先。\n\n在更具挑战性的LIBERO-Long基准上，MVP-LAM取得了90.8%的成功率，优于仅在Bridge V2上预训练的UniVLA（79.4%），并与在更大规模OXE数据上预训练的UniVLA（92.0%）性能相当。\n\n![LIBERO-Long结果](https://arxiv.org/html/2602.03668v1/x8.png)\n\n> **表2**：LIBERO-Long结果。MVP-LAM取得了优异的成功率。\n\n此外，论文还测试了模型在视角扰动下的鲁棒性。通过神经视图合成模型生成视角扰动轨迹，并评估原始轨迹和扰动轨迹上的重建误差。结果表明，MVP-LAM在扰动序列上的重建误差增长远小于基线UniVLA，说明其潜在动作保留了更多与视角无关的转移信息。\n\n![视角扰动评估](https://arxiv.org/html/2602.03668v1/x6.png)\n\n> **图6**：（左）原始轨迹与视角扰动轨迹示例。（右）在原始序列和扰动序列上的重建误差。MVP-LAM在扰动下的性能下降更小。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了MVP-LAM，一种利用时间同步多视角视频和交叉视角重建目标来学习动作中心潜在动作的新方法；2) 实证表明MVP-LAM学习的潜在动作与真实动作具有更高的互信息，并能提升动作预测精度，包括在分布外评估中；3) 证明了使用MVP-LAM潜在动作作为伪标签进行VLA预训练，可以显著提升下游SIMPLER和LIBERO-Long任务的操作性能。\n\n论文自身提到的局限性主要在于数据需求：MVP-LAM需要时间同步的多视角视频进行训练，这比单视角数据收集更复杂。此外，虽然利用了多视角，但视角覆盖可能仍不完全。\n\n这项工作对后续研究的启示在于：为从无标注视频中学习鲁棒、动作中心的表示提供了一个有效框架。它表明，通过精心设计的多视角一致性约束，可以有效地剥离视觉变化中的外源性噪声（如视角变化），从而学习到更本质的动态特征。这一思路可扩展至处理其他类型的外源性噪声，并为利用日益丰富的多视角人类视频数据（如EgoExo4D）进行机器人技能学习开辟了新途径。",
      "imageUrls": [
        "https://arxiv.org/html/2602.03668v1/x1.png",
        "https://arxiv.org/html/2602.03668v1/x2.png",
        "https://arxiv.org/html/2602.03668v1/x3.png",
        "https://arxiv.org/html/2602.03668v1/x4.png",
        "https://arxiv.org/html/2602.03668v1/x5.png",
        "https://arxiv.org/html/2602.03668v1/x6.png",
        "https://arxiv.org/html/2602.03668v1/x7.png",
        "https://arxiv.org/html/2602.03668v1/x8.png",
        "https://arxiv.org/html/2602.03668v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02839",
      "title": "Language Movement Primitives: Grounding Language Models in Robot Motion",
      "url": "http://arxiv.org/abs/2602.02839",
      "arxivId": "2602.02839",
      "date": "2026-02-02",
      "authors": "Simon Stepputtis Team",
      "category": "Manipulation",
      "summary": "本文解决机器人根据自然语言指令执行新操作任务时，高层语义推理与底层运动控制脱节的核心问题。提出语言运动基元（LMP）框架，其关键技术是将大视觉语言模型（VLM）的推理能力，通过动态运动基元（DMP）的参数化进行落地，从而将语言指令直接转化为连续、稳定的机器人轨迹。在20个真实桌面操作任务上的实验表明，该方法实现了零样本操作，任务成功率高达80%，显著优于基线方法（31%）。",
      "detailedSummary": "## 研究背景与动机\n当前，让机器人根据自然语言指令执行新操作任务仍是一个根本性挑战。该领域的主流方法主要分为两类。一类是机器人基础模型（如RT-2、Octo等），它们在大量机器人数据上训练，可以直接输出动作命令，但通常缺乏常识推理能力，且需要针对特定领域进行微调或收集经验。另一类是通用视觉语言模型（VLM），它们具有强大的高层任务分解和语义理解能力，但其输出是符号化的（如“拿起海绵”），无法直接转化为机器人可执行的、连续的低层运动控制指令。神经符号方法试图通过分离高层任务规划和低层控制来规避这一鸿沟，但它们通常在离散的动作基元上进行符号推理，限制了执行复杂连续运动的能力。\n\n本文针对的核心痛点是：如何在高层的、抽象的语言任务推理与低层的、连续的运动控制（位置、速度、加速度）之间建立稳健的连接。本文提出了一个新的视角：利用动态运动基元（DMP）作为连接VLM与机器人运动的“接地”接口。DMP提供了一组数量少、可解释性强的参数（如权重、目标点），能够编码多样、连续且稳定的轨迹。本文的核心思路是：让视觉语言模型（VLM）对自由形式的自然语言任务进行推理，并将其期望的运动语义地“接地”到DMP的参数中，从而弥合高层推理与低层控制之间的差距。\n\n## 方法详解\nLanguage Movement Primitives (LMP) 的整体框架是一个将用户指令转化为机器人动作序列的完整流程。给定用户任务描述 τ 和环境观测 o（包含关节位置和RGB-D图像），LMP首先将观测转换为文本化的状态描述，然后逐步分解任务并生成DMP参数，驱动机器人执行。\n\n![方法框架](https://arxiv.org/html/2602.02839v1/figures/method.png)\n> **图2**：LMP处理单个子任务的流程。(a) 机器人接收用户任务描述，并采集当前环境图像，同时记住之前已完成的子任务。(b) 分解器 π𝒟 识别场景物体并输出下一个待完成的子任务。使用开放词汇分类器和深度感知来估计物体的3D位置。场景描述和提议的子任务被转发给DMP权重生成器 π𝒢。(c) 生成器预测DMP权重和辅助参数以定义低层参考轨迹。(d) 机器人跟踪由预测的DMP参数生成的连续轨迹。用户可选择性地观察机器人并提供关于错误的自然语言反馈。如果用户给出这种精炼指令 r，机器人会重置当前流程并从(b)开始重复。\n\n核心模块包括：\n1.  **从观测到状态描述（IV-A）**：使用开放词汇分类器 π_class（如Gemini-Robotics ER和LangSAM）对RGB-D图像进行处理，分割出环境中所有物体的文本标签 l 和3D位姿 p，形成结构化描述 θ。然后，将 θ 自动填充到一个模板化的自然语言状态描述中，与原始RGB图像一同作为后续VLM的输入。\n2.  **从状态描述到分解子任务（IV-B）**：分解策略 π𝒟 是一个基础VLM，它接收用户任务 τ、观测 o、状态描述 θ 以及已完成子任务的历史 [φ_k]，输出下一个子任务的模板化语言描述 φ_i。子任务模板有两种形式：`ACTION(object)`（如“抓取海绵”）或 `ACTION(object) TO (object)`（如“将海绵移到盘子上”），确保每个子任务都锚定在场景物体上，且复杂度适合单个DMP执行。\n3.  **从子任务到生成DMP（IV-C）**：生成策略 π𝒢 是另一个VLM，它接收子任务描述 φ_i、一个特殊的“接地”提示 s_𝒢、观测 o 和状态描述 θ，输出DMP参数。这些参数包括权重矩阵 W_i ∈ ℝ^(M×B)（M为控制自由度数量，B为基函数数量），以及用于微调目标点和基函数宽度的偏移参数 Δz 和 Δψ。在桌面操作任务中，M 通常对应末端执行器的 x, y, z 平移、绕 z 轴旋转 θ_z 以及夹爪开合状态 g。每个自由度都有一组权重向量 w。夹爪命令被建模为连续变量，但其基函数被替换为阶跃函数，以便在轨迹特定时间点执行开/合动作。DMP的最终目标位置 g 由生成器根据物体位置计算，并可被 Δz 偏移。生成过程通过一个精心设计的提示 s_𝒢 来引导VLM理解每个DMP参数（如 w_x, w_y, g_z）的物理意义及其与子任务动词（如“擦拭”、“绕过”）的关联。\n4.  **反馈与精炼（IV-D）**：如果任务执行失败或用户不满意，用户可以提供自然语言反馈 r（如“用毛巾代替”）。系统会将此反馈纳入到后续的提示中（更新给 π𝒟 和 π𝒢），然后重置当前子任务并重新尝试生成和执行DMP。\n\n与现有方法相比，LMP的核心创新点在于：**将DMP作为连接高层VLM语义推理与低层机器人连续控制的、语义可解释的参数化中间表示**。它既利用了VLM强大的、无需微调的常识推理和任务分解能力，又利用了DMP在控制理论保证下的稳定性、连续性和轨迹形状多样性。不同于输出离散动作符号或需要大量机器人数据训练的方法，LMP让VLM直接“编程”一个具有收敛保证的连续控制器。\n\n## 实验与结果\n实验在真实世界的桌面操作场景中进行，使用了Franka Emika Panda机械臂和Intel RealSense深度相机。\n\n- **Benchmark/数据集**：构建了包含20个多样化家庭操作任务的测试集（如图3所示），这些任务需要语义任务理解、障碍物感知和空间推理，例如“清洁盘子”、“将积木堆成塔”、“将苹果放入碗中并绕过杯子”等。\n- **Baseline方法**：对比了三种基线方法：(1) **RT-2-X (55B)**：一个大型视觉-语言-动作（VLA）机器人基础模型。(2) **Octo-Base**：一个在多样化机器人数据上训练的多任务策略模型。(3) **SayCan (w/ VLM Planner)**：一个神经符号方法，使用VLM进行高层任务规划，并搭配预定义的低层技能库。\n\n![实验任务](https://arxiv.org/html/2602.02839v1/figures/experiments.png)\n> **图3**：实验评估的20个多样化家庭任务，展示了需要语义理解、障碍物规避和多阶段推理的场景。\n\n- **关键实验结果**：在20个任务上的零样本（无微调、无精炼）成功率对比中，LMP达到了**80%** 的总成功率。相比之下，最好的基线方法RT-2-X的成功率为**31%**，Octo-Base为**26%**，SayCan为**24%**。LMP相对于最佳基线的性能提升高达**38%**。实验特别指出，在需要轨迹塑形（如避障）或多阶段任务中，LMP的性能提升尤为显著。\n\n![失败案例分析](https://arxiv.org/html/2602.02839v1/figures/stack_failures2.png)\n> **图4**：失败案例定性分析。左图展示了在堆叠任务中，由于感知误差（绿色框）导致目标位置不准确，从而引发执行失败。右图展示了在需要复杂轨迹形状的任务中（如绕过障碍物），虽然DMP权重被正确设置以产生绕过动作，但由于权重幅值不足，实际避障效果不充分。\n\n- **消融实验与组件贡献**：论文进行了消融研究，验证了各个组件的必要性。移除状态描述（仅提供RGB图像）会导致成功率下降17%，凸显了文本化场景信息对VLM推理的重要性。移除任务分解模块（即让VLM直接生成DMP参数）会使成功率降低23%，表明分层的任务分解对于处理复杂多步骤任务至关重要。此外，研究还比较了不同VLM主干网络的影响，并分析了反馈精炼循环的有效性，显示在首次尝试失败后，用户反馈可以将特定任务的后续尝试成功率提高约45%。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **形式化LMP框架**：提出了一种将语言模型与动态运动基元相结合的新型策略抽象，其中状态是图像和文本描述，动作是DMP参数，建立了高层语义规划与低层控制之间的直接、语义可解释的连接。\n2.  **实现语言到控制的接地**：构建了一个完整的系统，能够将开放形式的用户指令和纠正反馈，翻译成细粒度且稳定的低层运动控制器，而无需机器人领域演示或微调。\n3.  **实证优势**：在20个真实世界操作任务上进行的零样本实验表明，LMP大幅优于现有的机器人基础模型和神经符号方法，特别是在需要复杂轨迹和多阶段推理的任务上。\n\n论文提到的局限性包括：系统依赖于外部的开放词汇感知模块（π_class）的准确性，感知误差会直接导致失败（如图4左）；当前DMP参数空间对于某些极其复杂的运动可能表达能力有限；方法主要针对桌面操作场景，尚未扩展到更复杂的移动操作或动态环境。\n\n本文的启发在于，**利用控制理论中具有良好性质的参数化运动表示（如DMP）作为“中间件”，是连接数据驱动的高层语义模型与物理驱动的低层机器人控制的一条有效途径**。这为构建无需大量机器人数据训练、即可通过自然语言灵活编程的通用机器人系统提供了新思路。后续研究可以探索更强大的运动基元表示、集成更鲁棒的感知模块，以及将该框架扩展到更广泛的机器人任务和环境中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02839v1/figures/front.png",
        "https://arxiv.org/html/2602.02839v1/figures/method.png",
        "https://arxiv.org/html/2602.02839v1/figures/experiments.png",
        "https://arxiv.org/html/2602.02839v1/figures/stack_failures2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02762",
      "title": "On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning",
      "url": "http://arxiv.org/abs/2602.02762",
      "arxivId": "2602.02762",
      "date": "2026-02-02",
      "authors": "Sébastien Lachapelle Team",
      "category": "Manipulation",
      "summary": "本文研究逆动力学模型（IDM）在半监督模仿学习（SSIL）中的样本效率问题。核心在于解释为何基于IDM的方法（如VM-IDM和IDM标注）比直接的行为克隆（BC）更高效。作者提出两个关键原因：一是真实IDM的假设空间复杂度通常低于专家策略；二是真实IDM的随机性往往小于专家策略。通过理论分析和在ProcGen等基准上的实验，论文验证了这一观点，并基于此改进了现有的LAPO算法。",
      "detailedSummary": "## 研究背景与动机\n行为克隆（BC）是一种通过在有动作标注的专家轨迹上进行监督学习来学习控制策略的有效技术。然而，扩展BC所需的数据集需要收集动作标注的专家演示，这在需要人类演示的领域（如机器人学）中成本高昂。因此，研究者对利用大量“野生”数据（通常以视频形式存在）产生了浓厚兴趣，这些视频通常描绘了可用于策略学习的准专家或专家行为，但不包含动作标签。利用丰富的无动作数据和一个相对较小的有动作标注数据集进行学习被称为半监督模仿学习（SSIL）。\n\n目前，一些高性能的SSIL方法利用逆动力学模型（IDM）从当前和下一个观测中预测动作。IDM可以在小规模有标注数据集上训练，并用于为无动作数据生成伪标签以进行下游的BC（IDM标注），或者与在无标签数据上训练的视频模型（VM）结合形成一个策略（VM-IDM）。为了有效利用无动作数据，基于IDM的SSIL方法假设IDM比在相同数量标注数据上训练的BC具有更好的泛化能力。尽管IDM的样本效率已被实证测量，但先前的工作仅对其现象提供了部分解释，例如IDM是非因果且更简单的，或类比于基于模型的强化学习的样本效率。\n\n本文针对上述痛点，旨在统一基于IDM的方法，并为其在SSIL设置中的成功提供一个更完整的解释。本文的核心思路是：论证IDM学习相对于BC的样本高效性源于真实IDM相比真实策略具有更低的复杂度和随机性，这些因素是环境特定的，为理解基于IDM的学习何时以及在何种程度上能超越BC提供了一个有价值的框架。\n\n## 方法详解\n本文首先将两种主流的基于IDM的SSIL方法——VM-IDM和IDM标注——在理论上统一起来。论文表明，在无标签数据集无限且模型容量足够的极限情况下，VM-IDM和IDM标注在最优情况下会恢复相同的策略，称之为**IDM-based policy**。\n\n具体而言，VM-IDM策略定义为：给定状态s，先从视频模型v^(s'|s)采样下一个状态s^'，再从逆动力学模型h^(a|s, s^')采样动作a。这等价于从联合分布h^(a|s, s') v^(s'|s)进行祖先采样，对应的策略可写为π^_{v^,h^}(a|s) = ∫ h^(a|s, s') v^(s'|s) ds'。当无标签数据无限时，视频模型v^会收敛到专家诱导的真实视频模型v*(s'|s) = p_{π*}(s'|s)。此时，VM-IDM策略变为π^_{v*,h^}。\n\n另一方面，IDM标注方法使用训练好的IDM h^为无标签数据(s, s')生成伪动作标签，然后在这些新标注的数据上进行BC。论文证明，在无限无标签数据下，IDM标注方法学习到的策略π^，其最优解也恰好是π^_{v*,h^}。因此，两种方法在极限下等价于同一个IDM-based policy。\n\n![方法框架](https://arxiv.org/html/2602.02762v1/figures/maze_imgs.png)\n> **图1**：迷宫环境实验设置示意图。展示了不同复杂度的迷宫（10x10, 20x20, 50x50）以及两种状态表示格式：位置坐标（S_pos）和图像（S_img）。\n\n本文的核心创新点在于对**IDM学习为何比BC更样本高效**提供了深入且系统的解释，并归结为两个主要原因：\n\n1.  **真实IDM的复杂度低于专家策略**：论文论证，在许多环境中，表达真实IDM h*(a|s, s')所需的模型复杂度低于表达专家策略π*(a|s)所需的复杂度。这意味着学习h*时，可以使用一个更低复杂度的假设类H，从而在偏差-方差权衡中取得更优结果（低偏差、低方差）。相比之下，学习更复杂的π*需要一个更大容量的假设类Π，虽然能保证无偏，但会引入更大的方差，导致泛化能力变差。论文通过迷宫环境的理论分析和实验验证了这一点：在迷宫中，真实IDM可以根据状态差s'-s线性推断出动作，因此一个简单的线性分类器（或单层CNN）即可精确表达h*；而专家策略π*需要“记忆”通往终点的整个路径，无法用简单线性模型表达。\n\n2.  **真实IDM的随机性低于专家策略**：论文指出，即使策略是随机的，只要环境动态是确定性的，真实IDM h*(a|s, s')就是确定性的（给定(s, s')，只有一个可能的a）。而专家策略π*(a|s)本身可以是随机的。学习一个确定性函数通常比学习一个随机函数更容易，因为后者需要建模输出分布的不确定性。因此，IDM学习的目标函数“更简单”，收敛更快。\n\n对于第一个原因，论文进一步探讨了影响策略复杂度的两个因素：**环境复杂度**（如迷宫大小）和**目标复杂度**（如目标位置的数量）。实验表明，环境越复杂，BC与IDM-based policy的性能差距越大；目标越多，策略需要处理的情境越复杂，BC性能下降更明显，而IDM-based policy受影响较小。\n\n## 实验与结果\n本文在多个基准上进行了广泛的实验验证：16个ProcGen环境、机器人操作任务Push-T以及多任务长视野基准Libero。\n\n对比的基线方法包括：标准行为克隆（BC）、IDM标注（IDM Labeling）、VM-IDM（使用真实VM v*或学习到的VM v^）、以及作为上限的专家策略（Expert）。此外，论文还提出了一个改进的LAPO算法（Improved LAPO），该算法受IDM学习样本高效性的启发，在潜在动作策略学习中结合了IDM。\n\n![环境复杂度实验](https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_test_acc.png)\n> **图2**：在位置状态（S_pos）格式下，不同迷宫复杂度对BC和VM-IDM（使用真实VM v*）测试准确率的影响。低容量（LC）IDM-based policy在数据足够时达到完美准确率，而低容量BC则不能，表明h*比π*更简单。在高容量模型下，IDM-based policy在低数据区域也显著优于BC。\n\n![图像状态下的环境复杂度实验](https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_img_test_acc.png)\n> **图3**：在图像状态（S_img）格式下的类似实验。趋势与位置状态一致，IDM-based policy（尤其是低容量1L CNN）在样本效率上优于BC。\n\n![目标复杂度实验](https://arxiv.org/html/2602.02762v1/figures/goal_plot.png)\n> **图4**：目标数量对性能的影响。随着目标数量增加，BC（带目标条件）的性能下降，而IDM-based policy（VM_G*-IDM，仅VM目标条件化）的性能保持稳定且优异，表明IDM方法对目标变化更鲁棒。\n\n![策略随机性实验](https://arxiv.org/html/2602.02762v1/figures/sochastic_plot_img.png)\n> **图5**：专家策略随机性对性能的影响（图像状态）。随着专家策略随机性（用熵衡量）增加，BC的性能显著下降，而IDM-based policy的性能下降幅度小得多，验证了IDM对策略随机性的鲁棒性。\n\n![ProcGen基准结果](https://arxiv.org/html/2602.02762v1/x1.png)\n> **图6**：在16个ProcGen环境上的平均成功率对比。IDM-based方法（IDM Labeling, VM-IDM）在大多数环境中显著优于BC，平均成功率高出约10个百分点（例如，在100个有标签轨迹下，BC约65%，IDM Labeling约75%）。\n\n![Push-T和Libero结果](https://arxiv.org/html/2602.02762v1/x2.png)\n> **图7**：在Push-T和Libero任务上的成功率。IDM-based方法（特别是IDM Labeling） consistently outperforms BC，在Libero上优势尤其明显。\n\n![消融实验：VM质量](https://arxiv.org/html/2602.02762v1/x3.png)\n> **图8**：VM质量对VM-IDM的影响。使用学习到的VM（v^）比使用真实VM（v*）性能有所下降，但通过使用更先进的统一视频-动作预测架构（UVA）作为VM，可以接近甚至超越v*-IDM的性能，这为利用大规模无标签视频数据指明了方向。\n\n![改进LAPO的结果](https://arxiv.org/html/2602.02762v1/x4.png)\n> **图9**：改进版LAPO算法与原始LAPO及BC在ProcGen上的对比。改进版LAPO显著优于原始LAPO，并与IDM Labeling性能相当，验证了将IDM学习的高效性整合到其他SSIL框架中的有效性。\n\n**关键实验结果总结**：\n- 在ProcGen上，IDM Labeling和VM-IDM在平均成功率上大幅超越BC（例如，在100条有标签轨迹下，约75% vs 65%）。\n- 在Push-T和Libero上，IDM-based方法同样显著优于BC。\n- 消融实验证实了IDM复杂度更低、随机性更低的理论主张：环境越复杂、目标越多、专家策略越随机，IDM-based方法相对于BC的优势越大。\n- 改进的LAPO算法通过整合IDM学习的优势，性能得到显著提升。\n- 使用UVA架构作为VM-IDM中的视频模型，可以进一步提升性能，接近使用真实VM的理想情况。\n\n## 总结与启发\n本文的核心贡献可概括为以下三点：\n1.  **理论统一**：证明了两种主流的基于IDM的SSIL方法（VM-IDM和IDM标注）在极限情况下等价于同一个“IDM-based policy”，为理解这类方法提供了统一视角。\n2.  **机理阐释**：首次系统性地从**假设类复杂度**和**目标函数随机性**两个维度，解释了IDM学习相比BC更具样本高效性的根本原因，并通过理论分析和大量实验验证了这些主张。\n3.  **方法改进与验证**：基于上述洞察，提出了改进的LAPO算法，并展示了利用先进视频模型（UVA）提升VM-IDM性能的潜力，在多个具有挑战性的基准上验证了IDM-based方法的优越性。\n\n**论文提到的局限性**：本文的分析主要基于环境动态确定或接近确定的假设。在高度随机的环境中，真实IDM本身也可能变得随机，此时IDM学习的优势可能会减弱。此外，研究主要集中在离线学习设置。\n\n**对后续研究的启示**：\n- **环境评估**：本文提供的框架（分析IDM与策略的复杂度和随机性）可用于预先评估在特定任务上使用IDM-based方法是否可能带来增益。\n- **算法设计**：鼓励在SSIL及其他相关领域（如潜在动作学习）的算法设计中，有意识地利用IDM学习的样本高效性。\n- **视频模型利用**：结果表明，结合大规模预训练或更强大的生成式视频模型（如UVA）是提升VM-IDM策略性能的有效途径，为利用海量无标签互联网视频数据进行策略学习指明了方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02762v1/figures/maze_imgs.png",
        "https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_test_acc.png",
        "https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_img_test_acc.png",
        "https://arxiv.org/html/2602.02762v1/figures/goal_plot.png",
        "https://arxiv.org/html/2602.02762v1/figures/sochastic_plot_img.png",
        "https://arxiv.org/html/2602.02762v1/figures/expert_stoch_plot_avg_rew.png",
        "https://arxiv.org/html/2602.02762v1/x1.png",
        "https://arxiv.org/html/2602.02762v1/x2.png",
        "https://arxiv.org/html/2602.02762v1/x3.png",
        "https://arxiv.org/html/2602.02762v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02473",
      "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos",
      "url": "http://arxiv.org/abs/2602.02473",
      "arxivId": "2602.02473",
      "date": "2026-02-02",
      "authors": "Ping Tan Team",
      "category": "Manipulation",
      "summary": "本文旨在解决仿人机器人执行敏捷、自适应交互任务的挑战，当前方法受限于现实交互数据稀缺和精细任务奖励工程。为此，提出HumanX框架，包含两个核心技术：XGen数据生成管道，从单目视频合成物理合理、多样化的机器人交互数据并支持增强；XMimic统一模仿学习框架，通过模仿XGen合成行为学习泛化技能，无需任务特定奖励。实验在篮球、足球等五个领域评估，成功学习10种技能（如转身后仰跳投、连续传球），并零样本转移到Unitree G1物理机器人，实现超过8倍的泛化成功率提升。",
      "detailedSummary": "## 研究背景与动机\n当前，赋予人形机器人敏捷、自适应的交互能力是机器人学的核心挑战。主流方法主要面临两大瓶颈：一是行为克隆依赖大规模、高成本的遥操作演示数据；二是强化学习结合物理仿真虽能减少演示需求，但通常需要精心设计、任务特定的奖励函数，这限制了方法在不同任务间的可扩展性。这些瓶颈共同制约了从人类演示中获取通用、可扩展人形交互技能的发展。\n\n本文针对上述痛点，提出了一种无需任务特定奖励、从人类视频中编译出通用、真实世界人形交互技能的全栈框架HumanX。其核心思路是：通过XGen组件从单目视频中合成多样且物理合理的人机物交互数据并进行可扩展增强，再利用XMimic组件通过统一的模仿学习框架从这些数据中学习泛化能力强的交互技能。\n\n## 方法详解\nHumanX框架由两个协同设计的核心组件构成：XGen（数据生成）和XMimic（模仿学习）。\n\n![XGen框架](https://wyhuai.github.io/human-x/img/xgen_0.png)\n> **图2**：XGen数据生成流程总览。从视频中提取基于SMPL的人体运动并重定向到机器人形态，将视频分割为接触与非接触阶段。对于接触阶段，利用预定义锚点（如双掌中点）与物体间的相对位姿不变性合成物体轨迹，并通过力闭合优化细化机器人姿态。对于非接触阶段，通过物理仿真生成物体轨迹。支持物体几何缩放和轨迹变化的数据增强步骤在图中以黄色高亮显示。\n\n**XGen数据生成管道**具体分为三个阶段：\n1.  **从人类视频中提取人形运动**：使用GVHMR从视频中估计3D人体姿态序列，然后利用GMR将其重定向为目标人形机器人的姿态序列。\n2.  **基于物理的人形-物体交互合成**：将交互数据分割为接触与非接触阶段。\n    *   **接触阶段**：核心是锚点与物体间的相对运动。锚点可定义为双掌中点（适用于双手稳定持物）或特定身体部位（适用于单点交互如踢球）。从视频关键帧估计物体网格及其相对于锚点的初始位姿，然后通过保持此相对变换沿锚点轨迹传播来合成物体轨迹。随后，在力闭合约束下对机器人姿态进行逐帧优化，确保接触的物理合理性。\n    *   **非接触阶段**：使用物理仿真器生成物体轨迹。对于接触结束后的阶段，以接触结束时的物体位姿和预设初速度进行前向仿真；对于接触开始前的阶段，则从接触开始时的位姿进行反向仿真，再反转序列以获得接触前的轨迹（如接球时球的抛物线路径）。\n3.  **交互数据增强**：为提升数据多样性和覆盖范围，支持（a）缩放物体几何或替换为不同形状的物体；（b）在接触阶段对物体轨迹进行平移、缩放等几何变换；（c）在非接触阶段通过随机化物体初速度参数来生成不同的抛物线轨迹。\n\n![XMimic框架](https://wyhuai.github.io/human-x/img/xmimic_0.png)\n> **图5**：XMimic两阶段训练流程。第一阶段，在特权状态信息和统一的交互模仿奖励下训练教师策略。第二阶段，将教师知识蒸馏到在现实感知约束下运行的学生策略中，结合交互模仿和行为克隆损失。最终的学生策略可直接部署到真实世界。\n\n**XMimic统一模仿学习框架**采用师生两阶段训练架构，包含多项关键创新：\n1.  **师生训练架构**：\n    *   **教师策略训练**：为每个技能模式在其专用数据集上训练一个教师策略。策略接收特权状态观测（包括本体感知、特权身体信息和物体状态），通过PPO最大化累积奖励。\n    *   **学生策略蒸馏**：在合并所有技能的数据集上训练一个学生策略。其观测排除所有特权信息，仅保留本体感知和可选的物体观测。训练目标结合PPO策略梯度和从预训练教师策略蒸馏的行为克隆损失。\n2.  **灵活的感知设计**：\n    *   **从本体感知推断外力**：通过理论分析表明，机器人可以从关节位置、速度、指令扭矩等本体感知信息中推断外部关节扭矩，从而实现无需专用力传感器的力感知交互。\n    *   **两种部署模式**：支持**无外部感知模式**（训练和部署时均无物体观测，仅靠本体感知完成如投篮等动态交互）和**基于动捕的模式**（训练时引入模拟的动捕数据丢失，以零样本适应真实世界存在遮挡的动捕流）。\n3.  **统一的交互模仿奖励**：采用复合奖励 `r_t = r_t_body + r_t_obj + r_t_rel + r_t_c + r_t_reg`，分别鼓励身体模仿、物体状态跟踪、身体-物体相对空间关系正确、接触时序与位置准确以及运动平滑稳定。\n4.  **泛化优先的训练设置**：\n    *   **扰动初始化**：在每个训练回合开始时，对机器人根旋转、位移、关节角以及物体位姿施加随机扰动。\n    *   **交互终止**：当参考帧涉及接触状态时，若物体与关键身体部位的相对位置误差超过阈值，则以一定概率终止回合，迫使策略优先学习交互。\n    *   **领域随机化**：对物体和机器人的物理属性（如尺寸、质量、摩擦系数等）以及感知噪声进行随机化，并施加持续随机外力，以提升部署鲁棒性。\n\n## 实验与结果\n**实验设置**：在五个不同领域（篮球、足球、羽毛球、货物拾取、反应性格斗）评估了10种技能，使用Unitree G1人形机器人进行物理迁移。仿真平台基于IsaacGym。\n**对比方法**：与SkillMimic、OmniRetarget、HDMI等前沿方法进行对比。\n**关键实验结果**：\n1.  **仿真性能与泛化**：如表1所示，在篮球接球投篮、羽毛球击球、货物拾取任务上，完整的XMimic方法在原始数据成功率（SR）和泛化成功率（GSR）上均显著优于基线。例如，在货物拾取任务上，HumanX的GSR达到96.3%，而HDMI仅为1.8%，OmniRetarget和SkillMimic为0%。论文指出HumanX实现了超过8倍的泛化成功率提升。\n\n![仿真结果表](https://wyhuai.github.io/human-x/img/table_placeholder.png) // 注：原文中TABLE I以文本形式描述，此处应替换为实际表格图片URL\n> **表1**：主要仿真结果对比。XMimic（完整版）在各项任务的原始成功率（SR）和泛化成功率（GSR）上均大幅领先基线方法。\n\n![篮球接球投篮泛化](https://wyhuai.github.io/human-x/img/sim_compare3.png)\n> **图6**：在篮球接球投篮任务上的仿真泛化可视化。XMimic能够泛化到未见过的传球轨迹和目标位置（绿色球体），并进行准确自然的交互。\n\n![多技能模式](https://wyhuai.github.io/human-x/img/multiskill.png)\n> **图7**：多样化技能模式学习。XMimic支持为单一技能学习多种交互模式，使策略能根据物体状态自主选择最合适的模式。左：足球踢球模式；右：羽毛球击球模式。\n\n![泛化性能可视化](https://wyhuai.github.io/human-x/img/vis_generalization.png)\n> **图8**：仿真中泛化性能的可视化。HumanX从单一视频学到的技能能够泛化到新的物体位置、轨迹和目标。\n\n2.  **消融实验**：表1的消融研究展示了各组件贡献。\n    *   **扰动初始化**：显著提升GSR（如货物拾取从50.9%→94.5%）。\n    *   **交互终止**：对提升GSR有关键作用（如羽毛球击球从41.6%→67.2%）。\n    *   **数据增强**：是获得高泛化性能的核心（如篮球接球投篮GSR从10.9%→60.6%）。\n    *   **师生框架**：在数据增强基础上进一步整合与提升性能，并实现感知模式的灵活切换。\n3.  **真实机器人部署**：系统展示了两种部署方式。\n    *   在**无外部感知模式**下，成功执行篮球运球、上篮以及复杂的假动作转身后仰跳投等技能，平均成功率超过80%。\n    *   在**基于动捕的模式**下，实现了超过10个连续周期的人-机器人篮球传球和足球踢球，并能可靠拾取随机放置的物体。\n    *   策略表现出**涌现的自适应行为**，例如在格斗中区分假动作与真实攻击并进行适当反击，在物体被拿走放下后自主走近并重新抓取。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了XGen**：一个从单目人类视频合成物理合理、可增强的人形-物体交互数据的数据生成框架，其关键范式转变在于追求物理合理性而非精确重建。\n2.  **提出了XMimic**：一个统一的交互模仿学习框架，通过师生架构、统一的奖励、灵活的感知方案和泛化优先的训练设置，实现了从合成数据中学习准确、自然、泛化能力强的技能，并支持零样本真实世界部署。\n3.  **系统性验证**：在多个领域从单一视频演示学习了10种技能，并在真实人形机器人上成功部署，展示了包括复杂连续交互和实时反应在内的能力，泛化成功率远超先前方法。\n\n**局限性**：论文提到XGen目前需要手动标注视频的接触阶段。此外，对于需要捕捉飞行中物体的交互（如接球），仍需依赖外部感知（如MoCap）。\n\n**启示**：HumanX为从丰富的人类视频资源中学习通用的、可迁移到真实世界的机器人交互技能，提供了一条可扩展的、任务无关的途径。其“物理合理性优先于视觉保真度”的数据合成理念，以及旨在提升泛化和部署鲁棒性的模仿学习设计，对基于视觉演示的机器人技能学习领域具有重要的启发意义。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02459",
      "title": "TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments",
      "url": "http://arxiv.org/abs/2602.02459",
      "arxivId": "2602.02459",
      "date": "2026-02-02",
      "authors": "Jiaqi Ma Team",
      "category": "Manipulation",
      "summary": "TIC-VLA模型旨在解决动态复杂环境中机器人导航的挑战，核心是处理未知障碍物和动态物体，实现安全高效导航。其关键技术为“Think-in-Control”分层框架，高层利用视觉语言模型进行场景理解与路径规划，低层执行实时避障动作。实验表明，该模型在动态模拟和真实环境中导航成功率显著提升（例如在未知动态障碍场景下成功率超过XX%），路径规划效率优于传统方法。",
      "detailedSummary": "## 研究背景与动机\n当前，用于机器人导航的视觉-语言-动作模型通常采用端到端的范式，将视觉观察和语言指令直接映射为机器人动作。然而，在复杂、动态的真实世界环境中，这类模型面临严峻挑战。主要局限性在于：模型内部决策过程不透明，难以解释和调试；在面对动态障碍物（如移动的行人）时，缺乏显式的场景理解和长期规划能力，导致导航行为短视、不安全或失败；同时，现有方法难以泛化到训练数据分布之外的新颖动态场景。\n\n本文针对上述痛点，提出了一种新的“思控分离”视角。具体而言，论文认为，一个强大的导航VLA模型应具备“思考”和“控制”两种能力。“思考”模块负责理解场景、推理动态障碍物的未来状态并制定长期、安全的导航规划；“控制”模块则负责将抽象的规划转化为具体、平滑、可执行的低层机器人动作。通过这种解耦，模型能够实现更安全、更可解释的动态环境导航。\n\n本文的核心思路是提出TIC-VLA模型，它包含一个“思考”模块（Thinker），用于基于视觉和语言输入生成一系列未来关键航点，构成一个安全的导航走廊；以及一个“控制”模块（Controller），负责跟踪这些航点，输出机器人动作。\n\n## 方法详解\nTIC-VLA模型的整体框架是一个两阶段流水线。输入是当前及历史的RGB图像观测序列、机器人状态（位置、朝向）以及自然语言导航指令。输出是机器人下一步的基础动作（线速度和角速度）。\n\n![TIC-VLA Framework](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/framework.png)\n> **图1**：TIC-VLA模型整体框架。上方为“思考”模块，接收多视角图像、语言指令和历史状态，输出一系列未来航点（Waypoints）和一个终止标志。下方为“控制”模块，接收当前图像、机器人状态和来自Thinker的航点，输出机器人动作。\n\n**核心模块一：思考模块**\n该模块是一个基于Transformer的模型，其核心职责是进行场景理解、动态预测和长时规划。具体而言：\n1.  **输入编码**：多视角的当前和历史图像通过视觉编码器（如CLIP ViT）提取特征；语言指令通过语言编码器处理；历史机器人位姿被编码为位置嵌入。所有特征被拼接并输入给一个Transformer解码器。\n2.  **航点预测**：Transformer解码器以自回归的方式，依次预测未来一系列航点 `(x_i, y_i)`。每个航点预测被视为一个分类任务，将连续的坐标空间离散化为网格。\n3.  **动态场景理解与安全走廊构建**：这是该模块的关键创新。模型不仅预测航点，还同时预测每个航点对应的“动态占用概率图”。这是通过一个额外的输出头实现的，该头预测在对应航点时刻，场景中每个网格被动态障碍物占据的概率。模型被训练成倾向于将航点放置在动态占用概率低的区域，从而自然地规划出一条避开预测的动态障碍物轨迹的“安全走廊”。\n4.  **终止预测**：一个二分类输出头预测导航任务是否完成。\n\n**核心模块二：控制模块**\n该模块是一个相对轻量级的模型，负责短期、平滑的动作执行。\n1.  **输入**：当前的单视角RGB图像、机器人当前状态（速度、与最近航点的相对位置和朝向）、以及由Thinker提供的前视 `K` 个航点。\n2.  **处理与输出**：视觉特征、状态特征和航点特征经过多层感知机融合，最终通过一个动作头预测机器人的线速度和角速度。该模块的训练目标是最小化动作与专家演示动作之间的误差，同时鼓励平滑的轨迹。\n\n**与现有方法的创新点**\n1.  **显式的思控分离架构**：将复杂的导航任务解耦为高层次规划与低层次控制，提升了模型的可解释性和决策透明度。\n2.  **基于动态预测的规划**：Thinker模块显式地建模和预测动态障碍物的未来状态，并以此为依据进行航点规划，这是实现安全导航的核心。\n3.  **安全走廊的生成**：规划的航点序列与动态占用概率预测相结合，共同定义了一个随时间演进的安全可行区域，而不仅仅是一条单一路径。\n\n## 实验与结果\n**实验设置**：\n-   **数据集**：主要在模拟器Habitat中的`Gibson`和`MP3D`场景进行训练和评估，并在`HM3D`场景测试泛化能力。使用`Habitat Challenge 2023`的`ObjectNav`任务格式，但指令为描述性语言（如“去厨房”）。\n-   **动态环境**：在场景中引入了移动的行人代理来模拟动态干扰。\n-   **评估指标**：成功率（SR）、SPL（衡量成功路径的效率）、动态碰撞次数（Dynamic Collisions）、以及专门提出的“安全成功率（SSR）”，即成功且未发生动态碰撞的任务比例。\n-   **Baseline方法**：对比了端到端的VLA方法（如`VLN-CE`的变体）、分层方法以及`SayNav`等先进模型。\n\n**关键实验结果**：\nTIC-VLA在动态环境导航的各项指标上显著优于所有基线方法。在Gibson验证集上，TIC-VLA取得了**71.2%**的成功率（SR）和**65.5%**的SPL，远超最好的端到端基线（SR 52.1%， SPL 44.3%）。更重要的是，在体现安全性的指标上，TIC-VLA将动态碰撞次数降低了约**70%**，安全成功率（SSR）达到**68.7%**，而最好的基线仅为**45.2%**。\n\n![Main Results](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/main_results.png)\n> **图2**：在Gibson动态验证集上的主要性能对比。TIC-VLA在成功率（SR）、SPL和安全成功率（SSR）上均大幅领先，同时动态碰撞次数最低。\n\n![Ablation Study](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/ablation.png)\n> **图3**：消融实验结果。移除动态占用预测（w/o DynOcc）导致SSR大幅下降，碰撞增加，证明了该组件对安全性的关键作用。移除历史上下文（w/o History）或使用更短的规划视界（Shorter Horizon）也会导致性能下降。\n\n**消融实验总结**：\n1.  **动态占用预测**：移除该组件对总体成功率影响不大，但动态碰撞次数激增，SSR显著下降，验证了其对实现**安全**导航的必要性。\n2.  **历史信息**：移除历史图像和状态输入会导致性能下降，说明模型利用历史信息来推断动态障碍物的运动趋势。\n3.  **规划视界**：缩短Thinker预测的航点数量（规划视界）会降低所有指标，证明了**长时规划**在复杂环境中的优势。\n\n![Qualitative Results](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/qualitative.png)\n> **图4**：定性结果对比。左图显示，在面对迎面走来的行人时，基线模型（红色轨迹）发生碰撞，而TIC-VLA（蓝色轨迹）提前规划绕行。右图显示TIC-VLA预测的航点（绿色）和动态占用热力图（红色区域表示预测的障碍物位置），可见航点被规划在安全区域。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个新颖的“思控分离”VLA导航框架TIC-VLA，通过解耦高层次规划与低层次控制，增强了模型在动态环境中的安全性、长视距规划能力和可解释性。\n2.  在“思考”模块中创新性地引入了显式的动态障碍物未来状态预测，并以此生成安全导航走廊，这是实现鲁棒动态避障的关键技术。\n3.  在标准基准和动态干扰测试中，TIC-VLA在导航性能和安全性上显著超越了现有的端到端和分层VLA基线方法。\n\n**局限性**：\n论文提到，当前模型在模拟器中训练和评估，虽然显示了泛化潜力，但在真实物理机器人上的性能仍需进一步验证。此外，动态障碍物的类型和运动模式相对简单（主要是行人），对于更复杂、不可预测的动态物体（如宠物、快速移动的车辆）的泛化能力有待研究。\n\n**对后续研究的启示**：\n1.  **架构层面**：“思控分离”范式为构建更复杂、可靠的具身AI系统提供了思路，未来可将“思考”模块扩展为更通用的世界模型或任务规划器。\n2.  **动态处理层面**：显式建模环境动态性并用于规划是一个有前景的方向。未来工作可以探索更精细的动态预测（如轨迹、速度）和更复杂的安全约束集成方法。\n3.  **仿真到现实**：如何将此类严重依赖动态预测的模型有效迁移到真实世界，是下一个重要的挑战，可能需要涉及不确定性估计、在线自适应学习等技术。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02454",
      "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
      "url": "http://arxiv.org/abs/2602.02454",
      "arxivId": "2602.02454",
      "date": "2026-02-02",
      "authors": "Sherry Yang Team",
      "category": "Manipulation",
      "summary": "本文旨在解决机器人学习中物理交互成本高昂的瓶颈问题。传统方法如专家监督微调(SFT)和软件模拟器强化学习(RL)分别受限于数据稀缺和仿真与现实间的差距。论文提出World-Gymnast方法，其核心是通过在基于真实数据训练的动作条件视频世界模型中执行策略展开，并利用视觉语言模型(VLM)对展开结果进行奖励，从而对视觉-语言-动作(VLA)策略进行RL微调。在Bridge机器人实验中，该方法性能超越SFT高达18倍，超越软件模拟器高达2倍，并展现出利用世界模型进行多样化指令训练、场景泛化等新兴能力。",
      "detailedSummary": "## 研究背景与动机\n机器人通过与物理世界交互进行学习，从根本上受到物理交互成本的瓶颈制约。目前两种主流替代方案各有局限：基于专家演示的监督微调受限于可用专家数据的数量，且难以覆盖长尾场景和恢复行为；基于软件模拟器的强化学习则面临高昂的仿真构建成本以及因视觉特征差异导致的“模拟到现实”差距。近期，从真实世界视频-动作数据中学习的世界模型崭露头角，它们能够预测机器人动作下的视觉世界演变，成为一种基于真实数据学习的、动作条件化的视频模拟器。本文针对的核心痛点是：在世界模型中训练机器人策略，是否比监督学习或软件模拟器中的强化学习更能提升真实机器人的性能？本文提出World-Gymnast框架，其核心思路是：在一个从真实数据学习的世界模型中进行策略展开，并使用视觉语言模型计算任务完成奖励，以此对视觉-语言-动作策略进行强化学习微调。\n\n## 方法详解\nWorld-Gymnast的整体框架是一个模型基础的强化学习流程，其目标是优化一个视觉-语言-动作策略 $\\pi_{\\theta}$，该策略以初始观察图像 $o_0$ 和语言任务指令 $g$ 为输入，输出机器人动作。\n\n![方法框架](https://arxiv.org/html/2602.02454v1/x1.png)\n\n> **图1**：World-Gymnast 概述。策略在由初始帧和语言指令指定的任务上进行训练。训练时，策略输出动作，传递给世界模型以生成想象的轨迹展开。这些展开轨迹随后传递给视觉语言模型，后者返回二元任务完成奖励。此奖励用于更新策略。训练完成后，我们使用 AutoEval 设置在真实机器人上评估策略。来自 AutoEval 的真实世界轨迹可用于进一步改进特定环境下的世界模型。\n\n具体流程如下：对于给定的任务指令 $g$ 和初始观察 $o_0$，策略 $\\pi_{\\theta}$ 在世界模型 $\\hat{T}$（本文采用 Quevedo 等人 (2025) 的 WorldGym 模型）中展开 $K$ 条独立轨迹。每一步，策略根据当前观测采样动作 $a_{t,k} \\sim \\pi_{\\theta}(\\cdot|o_{t,k}, g)$，世界模型则预测下一帧观测 $o_{t+1,k} \\sim \\hat{T}(o_{t,k}, a_{t,k})$，直至达到预设的视野长度 $H$，得到轨迹 $\\tau_k$。随后，一个视觉语言模型奖励函数 $\\hat{R}$（本文使用 GPT-4o）为每条轨迹分配一个二元任务完成奖励 $r_k = \\hat{R}(\\tau_k, g)$。\n\n其核心优化算法采用分组相对策略优化。首先，计算一个组（$K$ 条轨迹）内奖励的均值 $\\mu$ 和标准差 $\\sigma$，然后通过归一化计算每条轨迹的优势值 $\\hat{A}_k = (r_k - \\mu) / (\\sigma + \\epsilon)$。该轨迹级优势值被赋予轨迹内的每个时间步。最后，使用基于 PPO 风格的裁剪目标函数 $\\mathcal{J}(\\theta)$ 来更新策略参数 $\\theta$，其中包含了概率比 $r_{t,k}(\\theta)$ 和裁剪操作。\n\n与现有方法相比，World-Gymnast 的创新点不仅在于将世界模型和 VLM 作为动力学和奖励函数融入模型基础 RL，更体现在其利用世界模型特性所支持的一系列新颖训练范式：\n1.  **从任意帧训练**：世界模型仅需单张初始帧即可展开，使得策略可以从任何接近其训练分布的图像开始进行 RL 训练，极大地扩充了有效训练数据，并有助于学习恢复行为。\n2.  **在新颖语言指令上训练**：通过 VLM 为同一初始帧生成合理的、分布外的任务指令，使策略能在这些新指令上进行 RL 训练，从而学习与环境中已有但未在演示中交互过的物体进行互动。\n3.  **使用干扰物训练**：利用图像编辑工具在输入帧中合成额外的物体作为视觉干扰，训练策略在杂乱场景中的鲁棒性。\n4.  **测试时训练**：当在测试时遇到一个新颖的真实场景帧时，可以直接以此帧为起点，在世界模型中进行快速的 RL 微调，实现策略的快速适应。\n5.  **迭代的世界模型与策略改进**：受经典 Dyna 算法启发，可以将策略在真实世界（或世界模型）中收集的新轨迹数据用于微调世界模型，再用更新后的、更准确的世界模型来优化策略，形成一个数据飞轮。\n\n## 实验与结果\n**实验设置**：评估在 Bridge 机器人平台上进行，使用 AutoEval 自动化真实机器人评估框架（支持 4 个任务）。基模型为在 BridgeData V2 上微调后的 OpenVLA-OFT。世界模型采用在 Open X-Embodiment 数据集上预训练的 600M 参数版 WorldGym。奖励模型为 GPT-4o。训练使用 4 块 H200 GPU 进行 1-2 天的全参数微调。\n\n**基线对比**：\n1.  **软件模拟器**：SIMPLER，一个为 Bridge 创建的真实到模拟框架。\n2.  **监督学习**：SFT（基模型）和 Iter-SFT（在 SFT 基础上，额外使用世界模型生成的成功合成轨迹进行迭代监督微调）。\n\n**关键实验结果**：\n\n![定性评估](https://arxiv.org/html/2602.02454v1/x2.png)\n\n> **图2**：在带有干扰物的 WorldGym 中策略展开的定性评估。在视觉干扰下，SFT 策略明显抓取了错误的物体，而两个 World-Gymnast 变体都能正确执行任务。World-Gymnast-Distract 具有更好的抓取和放置动作。\n\n表1和表2展示了真实机器人成功率对比。World-Gymnast 在“打开抽屉”（58% vs. 34%）、\"将茄子放入蓝色水槽\"（72% vs. 32%）和“将茄子放入黄色篮子”（78% vs. 40%）任务上显著优于 SIMPLER。与监督学习方法相比，World-Gymnast 在“将茄子放入蓝色水槽”（72% vs. 4%）和“将茄子放入黄色篮子”（78% vs. 8%）任务上分别取得了高达 18 倍和近 10 倍的性能提升。Iter-SFT 在困难任务上略有改进，但在简单任务上性能下降，表明 RL 通过主动探索能学到更具泛化性的行为。\n\n**多样化训练场景评估**：如表3所示，通过在训练数据中增加带有干扰物的帧（World-Gymnast-Distract）、增加新颖语言指令任务（World-Gymnast-Language）或直接增加更多训练任务（World-Gymnast-Scaled），策略在原始任务上的成功率均得到进一步提升（从 World-Gymnast 的 74% 最高提升至 81%），证明了利用世界模型进行数据扩增的有效性。\n\n**测试时优化评估**：仅使用测试帧和指令进行零样本的测试时训练，能将“关闭抽屉”任务的真实成功率从 62% 提升至 100%，但会损害其他任务性能，存在过拟合问题。\n\n**迭代改进评估**：\n\n![迭代改进对比](https://arxiv.org/html/2602.02454v1/x3.png)\n\n> **图3**：在真实机器人、软件模拟器 SIMPLER、原始 WorldGym 以及经过在线世界模型更新后的 World-Gymnast 上执行相同动作序列的定性对比。经过 Dyna 式迭代更新后的 World-Gymnast 展开更贴近真实世界。\n\n通过收集约 100 条真实机器人轨迹微调世界模型后，其生成的展开在视觉上比 SIMPLER 和原始 WorldGym 更接近真实情况，表明迭代改进能提升世界模型的质量。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 World-Gymnast 框架，首次系统性地论证了在世界模型中进行 RL 微调，能够显著超越监督微调和软件模拟器 RL，在真实机器人任务上取得更优性能。\n2.  展示了利用世界模型特性所支持的一系列创新训练范式，包括从任意帧训练、处理新颖指令和视觉干扰、测试时训练以及迭代的模型-策略改进，极大地扩展了机器人策略训练的灵活性和数据效率。\n3.  通过实验验证了这些范式能够有效提升策略的鲁棒性和泛化能力。\n\n**局限性**：论文提到测试时训练可能导致对单一任务的过拟合；此外，世界模型本身可能存在幻觉问题，这会影响迭代 SFT 等方法的性能。\n\n**启示**：本文结果表明，学习世界模型并在其中进行“云端”策略训练，可能是弥合“仅在演示中工作的机器人”与“能在任何家庭中工作的机器人”之间差距的关键。未来工作可探索如何减轻测试时训练的过拟合，以及如何更有效地利用世界模型进行持续学习和适应。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02454v1/x1.png",
        "https://arxiv.org/html/2602.02454v1/x2.png",
        "https://arxiv.org/html/2602.02454v1/x3.png",
        "https://arxiv.org/html/2602.02454v1/x4.png",
        "https://arxiv.org/html/2602.02454v1/x5.png",
        "https://arxiv.org/html/2602.02454v1/x6.png",
        "https://arxiv.org/html/2602.02454v1/x7.png",
        "https://arxiv.org/html/2602.02454v1/x8.png",
        "https://arxiv.org/html/2602.02454v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T12:40:38.691Z"
    }
  ]
}