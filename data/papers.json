{
  "generatedAt": "2026-02-12T12:32:33.907Z",
  "source": "https://jiangranlv.github.io/robotics_arXiv_daily/",
  "items": [
    {
      "id": "http://arxiv.org/abs/2602.10109",
      "title": "ST4VLA: Spatially Guided Training for Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2602.10109",
      "arxivId": "2602.10109",
      "date": "2026-02-10",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "本文提出ST4VLA，旨在解决大型视觉语言模型（VLM）在具体任务中难以将抽象指令转化为低级动作的问题。方法采用**空间引导训练**，包含两个阶段：**空间基础预训练**（通过点、框和轨迹预测学习可迁移的空间先验）和**空间引导动作后训练**（通过空间提示引导动作生成）。实验表明，该方法显著提升了VLA模型的性能，在Google Robot上准确率从66.1提升至84.6，在WidowX Robot上从54.7提升至73.2，并在泛化性和抗干扰性上表现出优势。",
      "detailedSummary": "## 研究背景与动机\n当前，将大型视觉语言模型（VLMs）的能力扩展到具身任务面临核心挑战：机器人不仅需要理解指令的含义，还需确定在三维世界中“在哪里”以及“如何”行动。现有方法主要分为两类：一是基于分层机器人系统的方法，它们利用基础模型显式编码空间先验，但依赖基于规则的任务分解和手动设计的规划启发式方法，难以自动扩展到复杂多样的任务，并限制了端到端策略学习的潜力；二是数据驱动的视觉-语言-动作（VLA）模型，它们利用预训练的VLM和大规模遥操作数据集直接学习机器人控制，虽然避免了手动启发式规则，但倾向于过拟合低级动作模式，未能充分利用执行时的空间先验。本文针对VLA模型训练中空间感知与动作学习目标之间优化不一致、导致空间基础能力在策略学习过程中退化或冲突这一具体痛点，提出了“空间引导训练”的新视角。其核心思路是：通过一个两阶段训练流程，先让VLM获取可迁移的空间先验，再通过空间提示引导动作生成，从而在策略学习中保持空间基础能力，并促进空间与动作目标间的一致优化。\n\n## 方法详解\nST4VLA是一个双系统、端到端的VLA框架，其整体训练流程分为两个阶段：1）空间基础预训练；2）空间引导动作后训练。该设计将空间先验的获取与具体 embodiment 的控制解耦。\n\n![方法框架](https://arxiv.org/html/2602.10109v1/x2.png)\n\n> **图2**：ST4VLA整体框架。包含两个阶段：阶段1（空间基础预训练）在大量多源空间基础数据上训练VLM；阶段2（空间引导动作后训练）VLM规划器通过空间提示生成潜在规划令牌，作为动作专家的条件。\n\n**核心模块与架构**：\n1.  **双系统设计**：系统2（VLM规划器）基于Qwen2.5-VL，作为一个多模态编码器，负责捕获空间和语义先验。系统1（动作专家）采用一个紧凑的扩散变换器（DiT）和DINOv2视觉编码器，负责具体 embodiment 的控制。\n2.  **连接机制**：通过一个轻量级的查询变换器（8.7 MB）连接两者。该变换器以VLM规划器产生的潜在空间基础嵌入为条件，将可变长度的输入令牌映射为一组固定的可学习查询令牌，从而稳定专家的学习和推理。它实现为一个k层交叉注意力模块。\n3.  **空间提示**：在动作后训练阶段，通过在任务指令后附加简单的空间提示（如“Figure out how to execute it, then locate the key object needed”）来显式激活VLM在预训练阶段学到的空间感知能力。提取的特征嵌入为规划器提供了明确的空间线索。\n4.  **梯度衰减**：为了缓解动作专家梯度直接回流可能扭曲VLM多模态知识的问题，在查询变换器中引入了梯度衰减因子（例如0.5），以减弱从动作专家传回VLM的梯度，从而在保持规划器语义推理能力的同时实现有效的联合优化。\n\n**训练流程细节**：\n- **阶段1：空间基础预训练**：目标是在通用视觉-语言理解与机器人特定的空间推理需求之间建立基础对齐。策略性地结合大规模互联网视觉-语言基础语料库（如RefCOCO、LLaVA-OneVision）与针对性的机器人特定数据集（如RoboRefIt、A0、ST4VLA Data）。通过将所有机器人数据重新格式化为与网络规模预训练一致的统一QA结构，使VLM在标准监督微调框架下发展出具有空间感知能力的表征空间。\n- **阶段2：空间引导动作后训练**：重点是在保持和精炼阶段1获得的空间先验的同时，学习具体 embodiment 的控制。除了使用空间基础数据进行共同训练（VLM主干通过图像-提示对的下一个令牌预测进行更新）外，还针对动作数据引入空间提示，以增强语义推理与动作生成之间的对齐。例如，将指令“store all toys into the toy box”扩展为“Identify all relevant toys and their spatial relationships to the container.”\n\n**创新点**：与现有方法相比，ST4VLA的创新在于明确将“在哪里、做什么”与“如何做”分离，并通过**空间提示**这一机制，在动作训练阶段主动引导模型产生更丰富的空间先验来指导动作生成，而非被动地共同优化可能冲突的目标。同时，**梯度衰减**的设计保护了VLM的已有知识。\n\n## 实验与结果\n**实验设置**：\n- **基准测试/数据集**：SimplerEnv仿真基准套件（包含Google Robot和WidowX平台）、LIBERO仿真套件、基于Isaac-Sim构建的大规模模拟拾放基准（200个任务）、真实世界杂乱场景拾放任务（使用Franka Research 3机器人）、真实世界长视野操作任务。\n- **对比的基线方法**：包括RT-1、RT-2-X、OpenVLA、CogACT、SpatialVLA、π₀、GR00T N1.5、Magma等先进的开放VLA系统，以及论文自身构建的Vanilla VLA和Vanilla Co-training VLA。\n\n**关键实验结果**：\n1.  **SimplerEnv基准测试**：如表2和表3所示，ST4VLA在Google Robot视觉匹配（VM）、视觉聚合（VA）和WidowX VM任务上均取得了最佳性能。具体而言，在Google Robot VM上平均成功率从Vanilla VLA的66.1%提升至84.6%，VA从63.5%提升至75.9%；在WidowX上从54.7%提升至73.2%。相较于之前最好的方法，分别获得了5.9%、5.3%和9.8%的增益。\n\n2.  **大规模模拟拾放泛化评估**：如图4所示，在包含分布内、未见物体、新背景和未见指令四个维度的评估中，ST4VLA在200个模拟任务上均取得了最优的成功率，显著优于π₀和GR00T N1.5，展示了强大的视觉和语言泛化能力。\n\n![模拟拾放结果](https://arxiv.org/html/2602.10109v1/x4.png)\n\n> **图4**：在200个模拟指令跟随拾放任务中，不同泛化设置下的成功率。ST4VLA在所有设置下均领先。\n\n3.  **真实世界拾放泛化评估**：如表4所示，在包含分布内、未见物体实例、相似干扰物、新背景、未见物体位姿/朝向以及按属性/空间关系描述的未见指令等多个挑战性维度上，ST4VLA（平均成功率65%）全面优于π₀（31%）和GR00T N1.5（48%），特别是在“相似干扰物”和“未见物体朝向”等困难条件下优势明显。\n\n4.  **真实世界长视野操作评估**：如图5所示，在桌面整理、抽屉组织、制作三明治等需要多步规划的任务中，ST4VLA在分布内、物理干扰和任务重规划三种设置下均优于基线，展示了其利用高级规划器分解复杂任务并由低级控制器稳健执行的能力。\n\n![真实世界拾放结果](https://arxiv.org/html/2602.10109v1/x5.png)\n\n> **图5**：长视野指令跟随操作任务的结果。ST4VLA在分布内、物理干扰和任务重规划三种设置下均优于基线。\n\n5.  **消融与机理分析**：如图3和表1所示，研究对比了三种训练策略：Vanilla VLA（仅动作微调）、Vanilla Co-training VLA（空间与动作数据联合训练）、ST4VLA（空间预训练+空间引导后训练）。\n    - **性能表现**：ST4VLA在机器人操作和多模态理解/空间基础指标上均达到最佳平衡。例如，在保持RefCOCO-g IoU@0.5达71.2%的同时，取得Google Robot VM 84.6%和WidowX 73.2%的成功率。\n    - **优化对齐分析**：通过投影空间相似度（PSS）量化梯度对齐程度。如图3(c)，Vanilla Co-training的PSS仅为0.25，表明优化存在冲突；而ST4VLA的PSS提升至0.42，说明空间引导训练显著改善了两个目标优化动态的一致性，这与更好的空间感知保持和更快的操作任务收敛相关。\n\n![消融研究](https://arxiv.org/html/2602.10109v1/x3.png)\n\n> **图3**：消融研究。(a) 感知性能（RefCOCO-g上的IoU@0.5）；(b) 操作性能（WidowX上的平均成功率）；(c) 空间基础目标和动作策略目标的梯度相似度。ST4VLA在保持感知的同时实现了最佳的操作性能与优化对齐。\n\n**消融实验总结**：表1的消融结果表明，“空间引导”（+Spatially Guided）和“空间预训练”（+Spatially Pretrained）两个组件均对性能有正向贡献，两者结合（即完整的ST4VLA）效果最佳，验证了空间先验的引入及其与动作学习的引导式对齐的有效性。\n\n## 总结与启发\n**核心贡献**：\n1.  揭示了直接微调VLM为VLA模型会导致空间先验崩溃，而简单地与空间数据共同训练则会引入梯度冲突的问题，并提出通过简单的空间提示可以有效缓解这些问题。\n2.  提出了ST4VLA，一个空间引导训练框架，通过两阶段训练（空间基础预训练 + 空间引导动作后训练）显式地将动作优化与空间基础目标对齐，从而在策略学习中保持感知能力并实现稳健控制。\n3.  在SimplerEnv等大规模模拟和真实机器人实验基准上取得了领先性能，并证明了对未见物体、新指令和分布外环境更强的泛化能力与鲁棒性。\n\n**局限性**：论文自身未明确列出局限性，但从方法描述中可推断，其两阶段训练流程需要精心设计的数据集（混合网络数据与机器人数据），且引入了额外的查询变换器和梯度衰减超参数，可能增加训练复杂度和调优成本。\n\n**后续研究启示**：\n1.  **空间引导机制**：证明了在VLA训练中主动引导空间推理的有效性，为未来设计更精细的引导信号（如3D位置、 affordance 地图）提供了思路。\n2.  **优化一致性**：提出的梯度相似度分析工具（PSS）可用于诊断和改善多任务学习中的优化冲突问题。\n3.  **双系统架构**：将慢速、可靠的规划（System 2）与快速、具体的控制（System 1）分离，为构建兼具强推理能力和高效执行的具身智能体提供了可借鉴的框架。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10109v1/x1.png",
        "https://arxiv.org/html/2602.10109v1/x2.png",
        "https://arxiv.org/html/2602.10109v1/x3.png",
        "https://arxiv.org/html/2602.10109v1/x4.png",
        "https://arxiv.org/html/2602.10109v1/x5.png",
        "https://arxiv.org/html/2602.10109v1/x6.png",
        "https://arxiv.org/html/2602.10109v1/x7.png",
        "https://arxiv.org/html/2602.10109v1/x8.png",
        "https://arxiv.org/html/2602.10109v1/x9.png",
        "https://arxiv.org/html/2602.10109v1/x10.png",
        "https://arxiv.org/html/2602.10109v1/x11.png",
        "https://arxiv.org/html/2602.10109v1/x12.png",
        "https://arxiv.org/html/2602.10109v1/x13.png",
        "https://arxiv.org/html/2602.10109v1/x14.png",
        "https://arxiv.org/html/2602.10109v1/x15.png",
        "https://arxiv.org/html/2602.10109v1/x16.png",
        "https://arxiv.org/html/2602.10109v1/x17.png",
        "https://arxiv.org/html/2602.10109v1/x18.png",
        "https://arxiv.org/html/2602.10109v1/x19.png",
        "https://arxiv.org/html/2602.10109v1/x20.png",
        "https://arxiv.org/html/2602.10109v1/x21.png",
        "https://arxiv.org/html/2602.10109v1/x22.png",
        "https://arxiv.org/html/2602.10109v1/x23.png",
        "https://arxiv.org/html/2602.10109v1/x24.png",
        "https://arxiv.org/html/2602.10109v1/x25.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10105",
      "title": "DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos",
      "url": "http://arxiv.org/abs/2602.10105",
      "arxivId": "2602.10105",
      "date": "2026-02-10",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "论文标题为 \"DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos\"，本文关注机器人相关问题，给出方法与实验结果概述。",
      "detailedSummary": "## 研究背景与动机\n灵巧操作的数据稀缺问题从根本上限制了其泛化能力，因为为灵巧手收集真实世界数据昂贵且劳动密集。人类操作视频作为操作知识的直接载体，为扩展机器人学习提供了巨大潜力。然而，人类手与机器人灵巧手之间存在显著的本体差距，使得直接从人类视频进行预训练极具挑战性。现有方法主要分为两类：一是将人手视为异质本体直接用于预训练，但严重的视觉观察和动作空间差异限制了跨本体学习；二是从视频中重建3D手-物关键点流或物体轨迹，然后通过运动规划或强化学习复现演示。后者虽能消除本体差距，但大多依赖绝对深度信息，或需要严格的重建精度以避免RL训练失败，这从根本上限制了可扩展性。本文旨在弥合这一差距，释放大规模人类操作视频数据的潜力，提出了一个四阶段框架，核心思路是自动将单目人类操作视频转换为物理上合理的机器人数据，而无需任何额外信息。\n\n## 方法详解\nDexImit的整体框架是一个四阶段数据生成流程：1）从任意视角重建具有近度量尺度的4D手-物交互轨迹；2）进行子任务分解和双手调度；3）合成与演示交互一致的机器人轨迹；4）全面的数据增强以支持零样本真实世界部署。该设计使DexImit能够大规模生成灵巧操作数据。\n\n![方法整体框架](https://arxiv.org/html/2602.10105v1/src/pipeline_new.png)\n> **图1**：DexImit四阶段数据生成流程。从左至右依次为：重建、调度、动作生成、数据增强。\n\n**1. 4D手-物交互重建**：该阶段旨在从单目视频中恢复手和物体在统一世界坐标系下的6D位姿轨迹。具体步骤包括：\n*   **视频处理与理解**：使用Qwen3-VL对视频进行理解，识别出操作过程中涉及的所有物体集合。\n*   **分割**：使用Grounded Sam2进行逐帧分割，生成物体掩码、双手掩码和桌面掩码。\n*   **物体与手重建**：核心挑战是无深度信息下的尺度恢复。方法利用人手尺寸方差有限这一先验：首先使用SpatialTracker v2估计未缩放的每帧深度；然后利用第一帧的手掩码提取点云，并用Wilor估计手部网格；通过“对齐-渲染-对齐”步骤计算尺度因子s=PCA(手网格)/PCA(手点云)，并将s应用于深度图，获得度量尺度深度；接着使用SAM3D生成物体网格，并同样应用尺度对齐。\n*   **6D位姿估计**：对于物体，采用FoundationPose的跟踪变体进行位姿估计以保证时序连续性；对于手，因其已有准确朝向，仅需通过“对齐-渲染-对齐”恢复平移分量。\n*   **世界坐标变换**：为了将任意视角的视频映射到固定世界坐标系，方法基于桌面法线定义世界z轴，基于第一帧双手位姿中垂线方向投影到与z轴正交的平面定义x轴，y轴由右手坐标系约束唯一确定。世界坐标系原点基于第一帧所有被操作物体的轴对齐包围盒中心确定，并平移到预定义的机器人工作空间区域。\n\n**2. 子任务分解与任务调度**：为处理任意时长、任意双手并发或异步程度的操作，DexImit引入了**行动中心调度算法**。首先，使用Qwen3-VL对视频进行理解、子任务分解和结构化标注，定义任务（包含涉及的智能体集合、关联物体、有序子动作列表）和子动作（包含动作类型和起始帧）。算法（见论文Algorithm 1）基于提取的任务结构，使用优先队列动态调度多个智能体在长时域任务中的动作执行。\n\n**3. 源数据生成**：基于重建的轨迹和调度结果，通过基于力闭合的抓取合成和基于关键帧的运动规划生成低级动作。\n*   **抓取合成**：采用生成-选择策略。初始化时在物体凸包上采样接触点（单手抓取采一个点，双手抓取在物体中心对侧采两个点），手沿对应法线初始化。优化问题公式化为最小化目标 wrench 误差、接触距离惩罚、手-物碰撞惩罚和手-手穿透惩罚（公式4）。求解后得到一组物理可行且无碰撞的抓取候选，然后根据其与重建的人类手部位姿的距离进行排序和筛选，选择最接近且稳定的候选作为最终抓取。\n*   **运动生成**：将手和物体视为抓取后的单个刚体。根据物体在当前帧和目标帧的位姿计算相对变换（公式6），并将此变换应用于选定智能体在当前帧的末端执行器位姿，从而得到目标末端执行器位姿（公式7），作为运动规划的终点配置。\n\n**4. 数据增强**：为生成大规模策略学习数据并支持零样本泛化，对源轨迹进行四种增强：**物体位姿**随机化；**物体尺度**在[0.8, 1.2]范围内缩放（为保持监督一致性，保留原始抓取和运动，仅调整手指关节以适应尺度变化）；**相机位姿**随机化以实现视角泛化；**观察**（3D点云）随机移除30%的点并为剩余点的法线添加噪声，以模拟真实深度传感器变化。\n\n## 实验与结果\n实验围绕两个关键维度展开：方法的可扩展性及其能处理的任务难度上限。使用的实验平台和基准包括：评估重建成功率、数据可用性、与基线方法对比、消融实验以及真实世界零样本部署。\n\n**Q1: 生成数据的可用性如何？** 实验从**输入视频质量**（句子级生成、野外视频、定制拍摄视频、手动校正视频）和**目标任务难度**（短时域、工具使用、长时域、细粒度操作）两个正交维度评估数据可用率。结果表明，对于简单任务，即使使用生成或野外视频也能获得高可用数据；对于复杂任务，更高质量的视频输入能显著提升可用率。\n\n![数据可用性评估](https://arxiv.org/html/2602.10105v1/src/usability_new.png)\n> **图2**：生成灵巧操作数据的可用性评估。横轴为任务难度，纵轴为输入数据质量，颜色从灰到绿表示数据可用率升高。展示了不同难度级别下两个代表性任务的数据可用率。\n\n**Q2: 与现有方法相比，DexImit是否产生更高质量的数据？** 在物体轨迹重建实验中，对比了不同深度估计模型（VGGT, SpatialTracker v2, Trace-Anything, Depth-Anything v3）和位姿估计方法（RANSAC, ColorPCR, FoundationPose++）。组合使用SpatialTracker v2和FoundationPose++取得了最高的82%成功率（见表I）。与基线方法Hive和MimicGen对比，DexImit在长时域、工具使用和细粒度操作任务上成功率显著更高（平均提升约30%），并且生成的动作在物理指标（如抓取力闭合质量、运动平滑度）上更优。\n\n![与基线方法对比](https://arxiv.org/html/2602.10105v1/src/family.png)\n> **图3**：在仿真环境中，DexImit与基线方法Hive和MimicGen在多种任务类型上的成功率对比。DexImit在所有任务类别上均取得最高成功率。\n\n**Q3: DexImit能否处理复杂操作？** 定性实验显示，DexImit能够成功处理包括使用刀具切苹果、长时域制作饮料、细粒度堆叠杯子在内的多样化复杂任务。\n\n![定性结果](https://arxiv.org/html/2602.10105v1/src/qualitative.png)\n> **图4**：DexImit处理的多样化复杂任务定性展示，包括工具使用、长时域操作和细粒度操作。\n\n**Q4: DexImit是否支持零样本真实世界部署？** 将在增强数据上训练的3D Diffusion Policy直接部署到真实机器人上，在未经任何真实数据训练的情况下，成功完成了倒水、堆叠杯子等任务，展示了强大的零样本泛化能力。\n\n![真实世界结果](https://arxiv.org/html/2602.10105v1/src/real_world.png)\n> **图5**：真实世界零样本部署结果。策略仅在DexImit生成的仿真数据上训练，成功在真实机器人上完成倒水和堆叠任务。\n\n**消融实验**：验证了各组件贡献。1）**尺度恢复**：移除后导致物体尺寸不准确，政策无法泛化到不同尺寸的真实物体。2）**行动中心调度**：移除后无法处理并发或异步的双手长时域任务。3）**基于力闭合的抓取合成**：替换为简单IK求解会导致抓取不稳定。4）**数据增强**：移除任何一项都会损害真实世界的零样本泛化性能。\n\n## 总结与启发\n本文的核心贡献包括：1）提出一个从视频直接生成双手灵巧操作数据的自动化管道，涵盖广泛任务；2）设计了一个全面的数据增强系统，支持策略在真实机器人上的零样本部署；3）通过大量实验证明该方法能生成高质量数据，有效缓解灵巧操作中长期存在的数据稀缺问题。\n论文提到的局限性包括：方法性能依赖于输入视频的质量，对于涉及严重遮挡、快速运动或复杂物理交互（如非刚性变形）的场景仍然存在挑战。\n本工作为利用大规模人类视频（无论是互联网视频还是生成模型视频）进行机器人学习开辟了新途径。其“重建-调度-生成-增强”的框架为解决本体差距和可扩展数据生成提供了系统性思路，后续研究可进一步探索对更复杂交互的重建、结合动态模型进行规划，以及降低对视频质量的要求。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10105v1/src/pipeline_new.png",
        "https://arxiv.org/html/2602.10105v1/src/usability_new.png",
        "https://arxiv.org/html/2602.10105v1/src/qualitative.png",
        "https://arxiv.org/html/2602.10105v1/src/family.png",
        "https://arxiv.org/html/2602.10105v1/src/real_world.png",
        "https://arxiv.org/html/2602.10105v1/src/limitation_line.png",
        "https://arxiv.org/html/2602.10105v1/src/grasp.png",
        "https://arxiv.org/html/2602.10105v1/src/filter.png",
        "https://arxiv.org/html/2602.10105v1/src/sim_compress.png",
        "https://arxiv.org/html/2602.10105v1/src/sim2real.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10101",
      "title": "Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction",
      "url": "http://arxiv.org/abs/2602.10101",
      "arxivId": "2602.10101",
      "date": "2026-02-10",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中3D感知精度不足的问题，提出Robo3R模型。该方法直接从RGB图像与机器人状态进行前馈式3D重建，核心技术包括：联合推断尺度不变的局部几何与相对相机位姿，通过学习的全局相似变换将其统一到机器人坐标系；采用掩码点云头生成精细点云，以及基于关键点的PnP公式优化相机外参与全局对齐。模型在包含400万帧的合成数据集上训练，实验表明其性能持续优于现有先进重建方法与深度传感器，并在多项下游操作任务中带来性能提升。",
      "detailedSummary": "## 研究背景与动机\n机器人操作在非结构化环境中严重依赖对物理世界的鲁棒空间感知。当前，获取3D数据主要依赖深度相机（如RealSense D455， Azure Kinect），但其产生的深度图质量有限，易受噪声、透明或反射物体以及不良光照条件的影响。近年来，前馈式3D重建模型（如DUSt3R， VGGT， π³）发展迅速，但它们主要关注场景级重建，在机器人操作所需的细粒度几何精度和可靠度量尺度方面存在不足，限制了其在真实世界操作中的应用。\n\n本文针对机器人操作中高质量、度量尺度3D几何感知的痛点，提出了一种新的视角：将RGB图像与机器人状态信息融合，通过一个专门设计的神经网络，实时、前馈式地预测出操作就绪的3D场景表示。本文核心思路是：联合推断尺度不变的局部几何和相对相机位姿，并通过一个学习的全局相似变换，将它们统一到规范的机器人坐标系中，从而获得准确、度量尺度的三维几何。\n\n## 方法详解\nRobo3R的整体框架是一个端到端的前馈神经网络，输入为单目或双目RGB图像及机器人关节状态，输出为度量尺度下的规范机器人坐标系中的三维点云，同时包含深度图、归一化图像坐标、相对/绝对相机位姿等一系列中间表示。\n\n![方法总览](https://arxiv.org/html/2602.10101v1/x2.png)\n> **图2**：Robo3R方法总览。RGB图像和机器人状态被编码并融合。Transformer骨干网络通过交替的全局和帧内注意力处理特征。掩码点云头解码尺度不变的局部几何，相对位姿头输出用于多视角配准的相对位姿。相似变换（S.T.）令牌读出全局相似变换，将点云映射到规范机器人坐标系中的度量尺度3D几何。\n\n**核心模块与技术细节**：\n1.  **编码器**：使用DINOv2 ViT-L编码图像为补丁特征。机器人关节状态通过一个多层感知机（MLP）投影为状态特征。图像特征与状态特征通过逐元素相加进行融合，并附加可学习的相似变换（S.T.）令牌。\n2.  **Transformer骨干网络**：采用基于交替注意力机制的Transformer，堆叠18个交替的全局注意力和帧内注意力块，以实现帧内和跨帧的高效信息传播。\n3.  **掩码点云头**：为解决密集预测中的过平滑问题，该头将点云预测解耦为深度、归一化图像坐标和掩码预测三部分。\n    ![掩码点云头](https://arxiv.org/html/2602.10101v1/x3.png)\n    > **图3**：掩码点云头结构。通过解耦预测、反投影、掩码过滤和组合，获得具有清晰边缘和细粒度几何细节的点云。\n4.  **相对位姿头**：处理骨干网络输出，预测相对相机平移（3维向量）和旋转（9维表示，经SVD正交化为3x3矩阵），用于将不同视角的局部点云配准。\n5.  **相似变换头**：从S.T.令牌中解码出全局相似变换，包括刚性变换 **T** 和尺度因子 **s** （**S = s · T**），用于将配准后的点云变换到度量尺度的规范机器人坐标系。\n6.  **外部参数估计模块与关键点头**：为进一步提高精度，该模块通过预测机器人预定义关键点的2D像素坐标（使用热力图和可微Soft-Argmax），并求解PnP问题来估计相机外参，用于细化全局相似变换。\n    ![外部参数估计模块](https://arxiv.org/html/2602.10101v1/x4.png)\n    > **图4**：外部参数估计模块。通过关键点头提取机器人关键点像素坐标，并求解PnP问题来精确估计相机外参，从而细化全局相似变换。\n\n**创新点**：\n*   **显式融入机器人先验**：将机器人状态作为输入，并利用机器人关键点进行PnP优化，显著提升了重建的度量精度和对机器人体态的感知。\n*   **掩码点云头**：通过解耦预测和掩码机制，有效缓解了过平滑问题，能生成边缘清晰、细节丰富的点云。\n*   **统一的度量尺度重建流程**：从尺度不变局部表示，到多视角配准，再到通过学习的相似变换映射到规范机器人坐标系，系统性地解决了前馈重建中的尺度模糊和度量一致性问题。\n\n## 实验与结果\n**实验设置**：\n*   **数据集**：使用自建的大规模合成数据集**Robo3R-4M**（400万帧）进行训练。评估使用一个独立的、光真实感的测试集（2000个场景，8万帧）。\n*   **基准测试与基线方法**：在3D重建质量上，与领先的前馈重建模型**VGGT**、**π³**、**MapAnything (MA)** 和 **DepthAnything3 (DA3)** 进行对比。在下游任务中，与**深度相机（RealSense D455）** 以及基于RGB的模仿学习方法（如**Maniflow**）进行对比。\n*   **下游任务平台**：在真实世界的单臂Franka Research 3和双臂UR5e机器人平台上，评估模仿学习、仿真到现实迁移、抓取合成和免碰撞运动规划四项应用。\n\n**关键实验结果**：\n1.  **3D重建质量（定量）**：\n    *   **点云估计**：在单目和双目设置下，Robo3R在点误差、法向误差和尺度误差上均显著优于所有基线。例如，单目时点误差为0.006，比次优方法π³（0.061）低一个数量级；尺度误差仅为0.007，而其他方法均高于0.46，表明Robo3R能有效恢复度量几何。\n    *   **相对位姿估计**：Robo3R的相对平移误差（RTE）为0.014，相对旋转误差（RRE）为0.013，分别比最佳基线π³（0.116和0.073）低约8倍和5倍。其相对平移精度（RTA@0.03）高达0.951，证明了位姿预测的可靠性。\n2.  **3D重建质量（定性）与鲁棒性**：\n    ![定性比较](https://arxiv.org/html/2602.10101v1/x6.png)\n    > **图6**：在真实场景中的定性比较。Robo3R能重建仅1.5毫米宽的微小物体（第1行），成功处理镜子和透明杯等令深度传感器失效的物体（第2行），并在包含灵巧手的杂乱双手机器人场景中产生准确干净的点云（第3行）。\n3.  **下游任务性能**：\n    *   **模仿学习**：在四个真实世界任务（扫豆子、插螺丝、做早餐、双手机器人倒水）中，使用Robo3R重建的3D点云作为输入的策略，其成功率显著高于使用深度相机或RGB图像的策略。例如，在“插螺丝”任务中，Robo3R的成功率为90%，而深度相机和RGB策略分别为60%和30%。\n    ![模仿学习结果](https://arxiv.org/html/2602.10101v1/x7.png)\n    > **图7**：真实世界模仿学习的成功率对比。Robo3R在四个任务上均取得最高成功率。\n    *   **仿真到现实迁移**：使用Robo3R点云作为观测的策略，在真实世界的“推方块”任务中取得了83.3%的成功率，远高于使用深度相机（50%）或RGB（16.7%）的策略。\n    *   **抓取合成与运动规划**：基于Robo3R重建的点云进行抓取检测和运动规划，在杂乱场景中表现出更高的成功率和鲁棒性。\n    ![下游任务可视化](https://arxiv.org/html/2602.10101v1/x8.png)\n    > **图8**：下游任务可视化。展示了基于Robo3R点云的抓取位姿合成、免碰撞运动规划以及仿真到现实策略执行的示例。\n4.  **消融实验**：\n    论文通过消融实验验证了各核心组件的贡献。移除**机器人状态输入**会导致所有指标显著下降；移除**掩码点云头**会导致点误差和法向误差增大；移除**关键点PnP细化**会降低位姿估计和尺度精度；而使用**交替注意力机制**相比普通全局注意力能带来性能提升。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**Robo3R-4M**，一个为机器人操作感知与重建定制的大规模、高质量合成数据集及数据生成流程。\n2.  提出了**Robo3R**，一个专为机器人操作设计的前馈3D重建模型，能够实时输出高保真、度量尺度、具有规范坐标系的三维几何，在精度和鲁棒性上超越了现有重建模型和深度传感器。\n3.  通过广泛的实验验证了Robo3R作为深度相机替代方案的优越性，并证明其提升了下游多种机器人操作任务的性能。\n\n**局限性**：\n论文提到，Robo3R的性能依赖于大规模高质量合成数据，其在极端真实世界条件下的泛化能力仍需进一步探索。此外，模型在保持高精度的同时实现更高的实时推理速度（目前为10Hz）也是一个持续的挑战。\n\n**启示**：\n1.  **高质量数据的关键作用**：本研究凸显了针对特定领域（如机器人操作）精心构建大规模、多样化合成数据集对于训练高性能感知模型的重要性。\n2.  **利用领域先验**：将机器人状态等先验信息显式融入感知模型，是解决通用重建模型中特定精度和尺度问题的有效途径。\n3.  **感知与操作的协同设计**：面向最终任务（如操作）来设计感知模型（如输出规范坐标系、度量尺度几何），能更直接地提升整个机器人系统的性能，这代表了一个值得深入的研究方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10101v1/x1.png",
        "https://arxiv.org/html/2602.10101v1/x2.png",
        "https://arxiv.org/html/2602.10101v1/x3.png",
        "https://arxiv.org/html/2602.10101v1/x4.png",
        "https://arxiv.org/html/2602.10101v1/x5.png",
        "https://arxiv.org/html/2602.10101v1/x6.png",
        "https://arxiv.org/html/2602.10101v1/x7.png",
        "https://arxiv.org/html/2602.10101v1/x8.png",
        "https://arxiv.org/html/2602.10101v1/x9.png",
        "https://arxiv.org/html/2602.10101v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10093",
      "title": "UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking",
      "url": "http://arxiv.org/abs/2602.10093",
      "arxivId": "2602.10093",
      "date": "2026-02-10",
      "authors": "Yao Mu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人接触密集型操作任务中触觉数据获取困难、缺乏统一评估平台的问题，提出统一仿真平台UniVTAC。其核心包括：1）支持三种常用触觉视觉传感器的数据生成平台；2）基于仿真合成数据训练的触觉视觉编码器UniVTAC Encoder；3）包含八个代表性任务的基准测试UniVTAC Benchmark。实验表明，集成该编码器使基准测试平均成功率提升17.1%，真实机器人实验任务成功率提高25%。",
      "detailedSummary": "## 研究背景与动机\n机器人操作领域在视觉-语言-动作策略的推动下取得了快速进展。然而，对于插入等接触丰富的操作任务，仅依赖视觉感知通常难以鲁棒完成，因为末端执行器会造成遮挡、近距离深度精度有限，且物理接触一旦发生，直接的交互状态难以被视觉观测。视触觉感知通过直接捕获接触界面的局部几何、力分布和相对运动，提供了互补信息，对于接触丰富的操作场景至关重要。当前，视触觉操作的基础设施尚不完善：一方面，大规模、可靠的触觉数据在物理世界中获取成本高昂且具有挑战性，严重限制了触觉中心表示模型的训练；另一方面，缺乏统一、全面的视触觉操作基准测试，阻碍了对触觉驱动策略的系统性评估和改进。现有的基于仿真的数据生成流程大多专注于刚性或关节物体操作，对触觉交互的复杂动力学（如瞬态接触力、变形、滑动）建模支持有限，其开环或弱约束的执行可能产生对真实触觉传感器不安全或不具信息量的轨迹。本文针对大规模触觉数据稀缺和缺乏统一评估平台这两个核心痛点，提出了UniVTAC，其核心思路是构建一个基于仿真的统一平台，用于可扩展、可控地合成视触觉交互数据，并基于此数据训练一个触觉中心的编码器，同时建立一个包含八个代表性任务的基准测试来系统评估策略性能。\n\n## 方法详解\nUniVTAC是一个端到端的仿真驱动框架，涵盖数据合成、表示学习和系统评估。其整体流程是：首先在仿真平台中自动化生成大规模带标注的视触觉交互数据；然后利用这些数据，通过从仿真物理信号中衍生的辅助监督目标，训练UniVTAC编码器，学习触觉中心的表示；最后，将该编码器集成到下游策略中，并在UniVTAC基准测试上进行评估。\n\n![方法框架](https://arxiv.org/html/2602.10093v1/x1.png)\n> **图1**：UniVTAC编码器框架及其与策略学习的集成。(a) UniVTAC编码器通过三个自监督目标（形状重建、接触变形预测和物体位姿回归）进行预训练，以从原始视触觉观测中学习结构化的触觉中心表示。(b) 在部署时，预训练的编码器作为感知模块集成到下游操作策略中，支持从原始触觉图像进行端到端的策略学习，且不引入额外的推理开销。\n\n**UniVTAC平台**：基于TacEx框架构建，扩展了软体仿真能力。其核心模块包括：1) **传感器配置**：集成了三种主流视触觉传感器（GelSight Mini, ViTai GF225, Xense WS），通过调整内部相机内参、凝胶垫网格和渲染方法进行建模。2) **自动化操作API**：实现了原子操作原语库（抓取、移动、放置、探测、旋转）。其中`Grasp`和`Probe`原语引入了触觉反应式自适应控制机制，通过一个依赖于实时触觉传感器最小深度`d_min`的反馈律来调节夹爪关节速度，防止非物理穿透并确保捕获的触觉印记保持在现实的变形流形内，从而生成高质量的接触数据。\n\n**UniVTAC编码器**：这是一个多通路表示学习框架，旨在通过任务驱动的监督将结构化物理先验嵌入到触觉观测中。其设计基于三个感知先决条件：形状感知、接触感知和位姿感知。编码器采用ResNet-18作为共享主干网络，将触觉观测映射为紧凑的潜在表示。在训练时，该潜在表示被多个通路特定的解码头使用：\n- **形状感知**：通过双视图重建进行监督，即同时重建带标记的原始触觉图像`I_marked`和不带标记的纯净触觉图像`I_pure`，鼓励编码器从传感器特定的标记伪影中解耦出内在物体几何形状。\n- **接触感知**：通过预测表示凝胶垫法向压痕的密集表面深度图`D`，以及编码横向剪切和切向变形的标记点2D投影`M`进行监督，使表示能捕获真实的接触力学。\n- **位姿感知**：通过回归物体相对于凝胶垫中心局部坐标系的7维位姿`p`（3D平移+4元数朝向）进行监督，将潜在表示锚定在度量空间中。\n\n训练采用多任务损失函数`L_total = λ_s * L_shape + λ_c * L_contact + λ_p * L_pose`，其中各项均为均方误差损失，权重λ_s=1.0, λ_c=0.5, λ_p=0.5。部署时，所有解码头被丢弃，仅保留编码器，因此没有额外推理开销。\n\n![重建结果](https://arxiv.org/html/2602.10093v1/x2.png)\n> **图2**：重建结果。从带标记的触觉图像出发，UniVTAC编码器重建出互补的物理信号，包括无标记触觉图像、凝胶垫变形深度图和标记点位置。结果表明，学习到的表示能够捕获超越传感器特定视觉模式的全局形状线索和细粒度接触变形。\n\n**创新点**：与现有方法相比，其创新性主要体现在：1) **数据生成策略**：引入了基于触觉反馈的闭环夹爪控制，确保生成的数据物理一致且富含信息性接触模式；2) **表示学习设计**：明确构建了形状、接触、位姿三个感知通路的多任务监督框架，将物理先验结构化地注入表示中，而非仅依赖像素级重建或全局对比对齐。\n\n## 实验与结果\n**实验设置**：在**UniVTAC基准测试**上进行评估，该基准包含八个代表性视触觉操作任务（见图3），分为位姿推理（Lift Bottle, Lift Can, Put Bottle in Shelf）、形状感知（Grasp Classify）和接触丰富交互（Insert Hole, Insert Tube, Insert HDMI, Pull Out Key）三类。基准测试基于NVIDIA Isaac Sim构建，支持自动化任务级数据合成与评估，并引入了随机故障和纠正行为来丰富接触模式，同时采用基于物理的成功标准（如限制最大穿透深度、检测滑动）来确保评估可靠性。\n\n![基准测试任务](https://arxiv.org/html/2602.10093v1/x3.png)\n> **图3**：UniVTAC基准测试任务。包含八个代表性视触觉操作任务，涵盖形状识别、位姿推理和接触丰富交互。每个任务展示了两个执行关键帧的代表性视觉和触觉观测。\n\n**对比方法**：主要对比了三种策略：1) **ACT**：仅使用视觉输入的动作分块Transformer策略。2) **VITaL**：一种利用视触觉预训练的强代表性视触觉操作策略。3) **Ours**：ACT策略，但其视觉输入被替换为集成了UniVTAC编码器提取的触觉表征。\n\n**关键实验结果**：如表I所示，在八个任务的平均成功率上，仅视觉的ACT为30.9%，VITaL为40.5%，而集成了UniVTAC编码器的Ours方法达到了48.0%，相比纯视觉ACT提升了17.1个百分点。提升在多个接触敏感任务（如Insert Hole, Insert HDMI, Insert Tube）上尤为明显。在形状分类任务（Grasp Classify）上，Ours达到了99%的成功率，与VITaL（100%）相当，显著优于纯视觉ACT（50%）。\n\n**消融实验与贡献分析**：论文通过消融实验分析了编码器不同感知通路的贡献。\n\n![消融实验](https://arxiv.org/html/2602.10093v1/x4.png)\n> **图4**：UniVTAC编码器的消融研究。展示了在基准测试任务上，移除形状（-Shape）、接触（-Contact）或位姿（-Pose）监督目标后的性能下降。完整模型（Ours）在所有任务上表现最佳，表明多任务监督对于学习通用触觉表示至关重要。\n\n如图4所示，移除形状、接触或位姿监督中的任何一个都会导致性能下降，证明了多任务监督设计的有效性。其中，接触感知通路对插入类任务贡献最大，位姿感知对提升和放置任务很重要，而形状感知对分类任务至关重要。\n\n**仿真到现实验证**：论文进一步进行了真实世界机器人实验。\n\n![真实世界实验](https://arxiv.org/html/2602.10093v1/x5.png)\n> **图5**：真实世界实验设置。使用配备ViTai GF225触觉传感器的机器人夹爪进行HDMI插入任务。\n\n![真实世界结果](https://arxiv.org/html/2602.10093v1/x6.png)\n> **图6**：真实世界HDMI插入任务的成功率。比较了使用仅视觉观测、仅触觉观测（通过UniVTAC编码器）以及视触觉融合观测的策略性能。\n\n在真实的HDMI插入任务中（图5），使用UniVTAC编码器提供的触觉表征的策略，相比仅使用视觉的策略，成功率从60%提升至85%，相对提升了25%（图6）。这证明了仅使用仿真合成数据训练的UniVTAC编码器能够有效迁移到真实世界。\n\n![定性结果](https://arxiv.org/html/2602.10093v1/x7.png)\n> **图7**：在UniVTAC基准测试上的定性结果。比较了ACT（仅视觉）、VITaL和集成了UniVTAC编码器的ACT（Ours）在“Put Bottle in Shelf”任务中的表现。Ours策略能够利用触觉反馈进行更精准的放置。\n\n## 总结与启发\n**核心贡献**：1) 提出了**UniVTAC平台**，一个支持多种传感器、可扩展且可控的视触觉操作数据仿真合成框架，其闭环自适应控制确保了数据质量。2) 提出了**UniVTAC编码器**，一种通过形状、接触、位姿多通路监督进行大规模仿真预训练的视触觉编码器，能学习触觉中心的通用表示。3) 提出了**UniVTAC基准测试**，一个包含八个任务的仿真基准，支持自动化数据生成和统一策略评估，并设计了富含接触模式的轨迹合成方法和基于物理的成功标准。\n\n**局限性**：论文提到，仿真与真实世界之间的差距（sim-to-real gap）仍然存在。此外，编码器的架构设计（如ResNet-18）和预训练目标可能并非最优，仍有改进空间。\n\n**后续启示**：UniVTAC为视触觉操作研究提供了一个从数据生成、表示学习到系统评估的完整工具链，有望推动触觉感知的标准化和规模化研究。其多通路监督的表示学习范式强调了将物理理解注入模型的重要性。未来的工作可以探索更高效的编码器架构、更先进的sim-to-real迁移技术，以及将平台扩展到更复杂的多指灵巧手操作场景。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10093v1/x1.png",
        "https://arxiv.org/html/2602.10093v1/x2.png",
        "https://arxiv.org/html/2602.10093v1/x3.png",
        "https://arxiv.org/html/2602.10093v1/x4.png",
        "https://arxiv.org/html/2602.10093v1/x5.png",
        "https://arxiv.org/html/2602.10093v1/x6.png",
        "https://arxiv.org/html/2602.10093v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10015",
      "title": "RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments",
      "url": "http://arxiv.org/abs/2602.10015",
      "arxivId": "2602.10015",
      "date": "2026-02-10",
      "authors": "Laxmidhar Behera Team",
      "category": "Manipulation",
      "summary": "论文旨在解决从长视频中分割出机器人可执行的细粒度子任务，以实现安全的人机技能转移。提出了RoboSubtaskNet多阶段框架，结合注意力增强的I3D特征（RGB+光流）与修改的MS-TCN，采用斐波那契膨胀调度捕捉短时转换，并通过交叉熵和时间正则化损失优化分割。引入了RoboSubtask数据集用于医疗和工业场景。实验显示，在GTEA数据集上F1@50达79.5%，编辑准确率88.6%；在自建RoboSubtask数据集上F1@50达94.2%，编辑准确率95.6%。物理实验中，7-DoF机械臂整体任务成功率约91.25%，验证了从感知到执行的可行性。",
      "detailedSummary": "## 研究背景与动机\n当前人机协作领域的研究致力于提升机器人与人类协同完成复杂工作流（如医疗护理、工业装配）的能力。这要求机器人能够理解人类的演示视频，并将其分解为可直接执行的、细粒度的子任务（如“抓取”、“放置”），而不仅仅是识别高层的语义活动（如“泡茶”）。然而，现有主流的人体活动识别（HAR）数据集（如GTEA、Breakfast）的标注与机器人执行需求不匹配，其活动标签无法直接映射到操作器基元。同时，主流的时序动作分割方法（如MS-TCN、MS-TCN++）虽然取得了进展，但其膨胀时序卷积的指数增长膨胀率设计主要针对烹饪等长时程活动，可能忽略机器人操作中常见的短时程子任务（如“到达-抓取-放置”）间的快速转换，且计算成本较高。本文针对“如何从人类演示视频中精确分割出机器人可执行的子任务序列”这一具体痛点，提出了一个从感知到执行的完整新框架。其核心思路是：通过引入注意力增强的I3D特征与采用斐波那契膨胀率的改进MS-TCN，结合专门为机器人映射设计的RoboSubtask数据集，实现从视频到操作器基元的可靠技能迁移。\n\n## 方法详解\nRoboSubtaskNet是一个多阶段的人到机器人子任务分割框架，其整体流程包含四个阶段：(a) 数据集创建、(b) 特征提取与模态融合、(c) 子任务学习、(d) 机器人执行模型。\n\n![方法框架](https://arxiv.org/html/2602.10015v1/MSTCN.jpg)\n> **图1**：RoboSubtaskNet从人到机器人技能迁移的整体流程。从左至右展示了从人类演示视频输入，经过特征提取与子任务分割网络得到预测的子任务序列，最终映射到机器人基元并控制机械臂执行的全过程。\n\n**1. 特征提取与模态融合**：首先，使用在Kinetics上预训练的I3D模型分别处理输入视频的RGB帧序列和光流序列，提取每帧的1024维特征。为了自适应地融合RGB（外观）和光流（运动）两种模态的信息，论文设计了一个注意力融合模块。\n\n![特征提取与融合](https://arxiv.org/html/2602.10015v1/I3D-modified.jpg)\n> **图2**：(a) 基于I3D的特征提取器，分别处理RGB和光流输入。(b) 注意力融合示意图：一个浅层全连接网络以拼接后的RGB和光流特征为输入，通过Sigmoid激活函数为每个特征维度生成一个介于0到1之间的注意力权重α(t)，用于计算加权和得到融合特征。\n\n具体而言，对于每一帧t，通过一个可学习的权重矩阵Wa和偏置ba计算注意力系数α(t) = σ(Wa·[ft_rgb; ft_flow] + ba)。最终的融合特征为 ft_fused = α(t) ⊙ ft_rgb + (1 - α(t)) ⊙ ft_flow。这种机制允许模型根据子任务动态调整对两种模态的依赖（例如，“到达”可能更依赖光流，而“抓取”更依赖RGB）。\n\n**2. 子任务学习**：子任务学习的核心是一个修改后的多阶段时序卷积网络（MS-TCN）。其关键创新点在于：\n*   **斐波那契膨胀时序卷积**：取代传统MS-TCN中指数增长的膨胀率（1, 2, 4, 8...），采用斐波那契数列作为膨胀率（1, 1, 2, 3, 5...）。这种设计能提供更密集的时序覆盖，更好地捕捉短时程操作子任务间的快速转换，同时仍能扩展感受野以建模更长的运动片段。\n\n![膨胀残差层](https://arxiv.org/html/2602.10015v1/fusion_attension.png)\n> **图3**：采用斐波那契膨胀率的膨胀残差层示意图。展示了卷积核在时序上的采样模式，与指数膨胀相比更密集。\n\n*   **复合损失函数**：训练时采用包含三项的复合损失L = Σ(L_CE + λ L_T-MSE + γ L_Trans)。其中，L_CE是标准的交叉熵损失；L_T-MSE（截断均方误差）惩罚相邻帧间对数概率的剧烈变化，以减少过分割和“闪烁”；L_Trans（过渡感知损失）通过一个预定义的无效转移矩阵M，对预测中出现的无效子任务转移（如直接从“放置”跳回“抓取”）施加额外惩罚，鼓励合理的子任务进展顺序。\n\n网络采用多阶段级联结构，每个阶段都是一个具有上述斐波那契膨胀卷积和残差连接的TCN，后一阶段对前一阶段的预测进行细化，以逐步锐化边界并减少错误。\n\n**3. 机器人执行模型**：预测出的子任务标签序列被确定性映射到预定义的机器人基元库。每个基元（如“抓取”、“倾倒”）使用动态运动基元（DMP）进行轨迹编码。执行时，首先通过YOLOv8进行目标检测，结合深度信息获取目标3D位置。然后，使用一个比例（P）控制器进行视觉伺服，将机械臂末端执行器引导至目标附近。最后，将目标坐标转换到机器人基坐标系，并调用相应的DMP生成平滑的轨迹完成最终动作（如抓取、放置）。\n\n## 实验与结果\n**实验设置**：在三个数据集上评估方法：公开的GTEA、Breakfast，以及本文提出的RoboSubtask数据集。RoboSubtask包含4个任务（取放、取倒、取递、清洁），共800条视频，按80/20划分训练验证集，并使用了水平翻转和亮度调整进行数据增强，最终有效训练数据为1920段。评估指标包括帧准确率（Acc）、分段F1@\\{10,25,50\\}和编辑分数（Edit）。机器人平台使用Kinova Gen3 7自由度机械臂和RealSense D410相机。\n\n**对比实验**：主要对比基线为MS-TCN和MS-TCN++。\n\n![结果对比表](https://arxiv.org/html/2602.10015v1/Modified_MSTCN.png)\n> **图4**：RoboSubtaskNet与MS-TCN、MS-TCN++在GTEA、Breakfast和RoboSubtask数据集上的性能对比表。数据显示，在GTEA上，RoboSubtaskNet在边界敏感指标（F1和Edit）上优于基线；在专为机器人设计的RoboSubtask数据集上，其所有指标均大幅领先；在长时程的Breakfast数据集上表现则弱于MS-TCN++。\n\n关键结果：在GTEA上，RoboSubtaskNet在边界敏感指标上表现最佳，F1@50达到79.5%，Edit达到88.6%，帧准确率为78.9%。在RoboSubtask数据集上，其优势最为明显，F1@50高达94.2%，Edit为95.6%，准确率为92.15%，显著优于两个基线，证明了其对机器人可执行子任务分割的有效性。在Breakfast上，其F1@50为30.4%，低于MS-TCN++的45.9%，这符合预期，因为其网络设计针对短时程任务，与Breakfast的长时程特性存在权衡。\n\n**消融实验**：\n![消融实验1](https://arxiv.org/html/2602.10015v1/x1.png)\n> **图5**：不同特征融合策略在RoboSubtask数据集上的性能对比。表明注意力融合（Ours）优于简单的拼接（Concat）或平均（Average）策略。\n\n![消融实验2](https://arxiv.org/html/2602.10015v1/x2.png)\n> **图6**：不同膨胀率策略的对比。斐波那契膨胀率（Fibonacci）在RoboSubtask上全面优于线性（Linear）和指数（Exponential）膨胀率。\n\n![消融实验3](https://arxiv.org/html/2602.10015v1/x3.png)\n> **图7**：损失函数组件的消融研究。完整损失（CE+T-MSE+Trans）能获得最佳性能，移除过渡感知损失（w/o Trans）或平滑损失（w/o T-MSE）都会导致指标下降。\n\n消融实验总结：注意力融合模块相比简单融合方式带来显著提升；斐波那契膨胀率在RoboSubtask数据集上优于线性和指数膨胀率；复合损失函数中的每一项（交叉熵、时序平滑、过渡感知）都对最终性能有积极贡献。\n\n**机器人端到端验证**：\n![机器人任务演示](https://arxiv.org/html/2602.10015v1/x4.png)\n![机器人任务演示](https://arxiv.org/html/2602.10015v1/x5.png)\n![机器人任务演示](https://arxiv.org/html/2602.10015v1/x6.png)\n![机器人任务演示](https://arxiv.org/html/2602.10015v1/x7.png)\n> **图8-11**：四个机器人执行任务的定性展示（取放、取倒、清洁、取递）。每幅图上半部分为人类演示视频帧，下半部分为机器人执行对应子任务序列的连续画面。直观展示了从视频理解到物理执行的完整流程可靠性。\n\n在Kinova Gen3机械臂上的物理实验表明，从人类视频输入到机器人完成动作的端到端流水线总体任务成功率约为91.25%，验证了该框架在实际场景中的可行性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了一个完整的、针对短时程操作任务的人到机器人子任务分割与执行框架RoboSubtaskNet；2) 在方法层面引入了注意力融合的I3D特征和采用斐波那契膨胀率的改进MS-TCN，并设计了过渡感知损失，以更好地建模机器人子任务；3) 创建并开源了RoboSubtask数据集，该数据集的子任务标注与机器人基元直接映射，填补了现有视觉基准与机器人控制需求之间的空白。\n\n论文自身提到的局限性在于，其重点是目前医疗和工业环境中的短时程操作子任务，将框架扩展到更复杂的长时程活动是未来的工作方向。\n\n本文对后续研究的启示在于：首先，为机器人技能迁移设计专用的、对齐执行语义的数据集至关重要。其次，时序建模网络的结构（如膨胀率）应根据目标任务的时序特性（短时程vs长时程）进行定制化设计。最后，在分割模型的训练中引入领域知识（如通过损失函数约束无效的子任务转移）能有效提升预测结果的合理性和可用性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10015v1/MSTCN.jpg",
        "https://arxiv.org/html/2602.10015v1/I3D-modified.jpg",
        "https://arxiv.org/html/2602.10015v1/fusion_attension.png",
        "https://arxiv.org/html/2602.10015v1/Modified_MSTCN.png",
        "https://arxiv.org/html/2602.10015v1/x1.png",
        "https://arxiv.org/html/2602.10015v1/x2.png",
        "https://arxiv.org/html/2602.10015v1/x3.png",
        "https://arxiv.org/html/2602.10015v1/x4.png",
        "https://arxiv.org/html/2602.10015v1/x5.png",
        "https://arxiv.org/html/2602.10015v1/x6.png",
        "https://arxiv.org/html/2602.10015v1/x7.png",
        "https://arxiv.org/html/2602.10015v1/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10013",
      "title": "Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper",
      "url": "http://arxiv.org/abs/2602.10013",
      "arxivId": "2602.10013",
      "date": "2026-02-10",
      "authors": "Yen-Ling Kuo Team",
      "category": "Manipulation",
      "summary": "本文旨在解决机器人对易损物体（如薯片）进行精细力控操作的难题。研究提出了一种低成本（约150美元）的触觉力控夹爪TF-Gripper（力范围0.45–45N），并设计了RETAF学习框架，将高频触觉力调节与机械臂姿态预测解耦。实验表明，相比位置控制，直接力控显著提升了抓取稳定性与任务成功率；触觉反馈对力调节至关重要，RETAF框架在多种任务中均优于基线方法。",
      "detailedSummary": "## 研究背景与动机\n当前机器人操作领域的主流方法，无论是否使用触觉反馈，大多将夹爪控制简化为二值开/合动作或直接预测夹爪位置/宽度。这种位置控制可被视为力量控制的间接代理，但通过位置控制产生合适的力量高度依赖于物体和任务。例如，对于外观相似但物理特性不同的物体（如尺寸略有差异的薯片或随时间软化的樱桃番茄），稳定的抓取宽度范围可能差异巨大或完全不同，而稳定的抓取力量范围则更为相似，能带来更鲁棒的策略。然而，许多提供力控的商业夹爪往往价格昂贵、难以跨机器人适配，且最小力量较高，不适用于精细、柔软、易碎的日常物体。本文旨在研究机器人如何有效学习类人的触觉-力量能力，核心痛点是缺乏低成本、支持精细力控且兼容触觉感知的硬件，以及现有低频策略架构难以适应力控所需的快速、接触驱动的反应式调节。本文的核心思路是：1）开发一个低成本、通用且集成触觉感知的力控夹爪TF-Gripper；2）提出一个将力控与末端执行器位姿预测解耦的策略框架RETAF，以实现高频、反应式的力量调节。\n\n## 方法详解\n本文提出了一个包含硬件（TF-Gripper）和软件（RETAF策略框架）的完整系统，用于学习力量调节操作。\n\n![方法框架](https://arxiv.org/html/2602.10013v1/x6.png)\n\n> **图6**：RETAF框架概述。基础策略以低频率从视觉和本体感知观测中预测末端执行器位姿和夹爪开/合动作。当夹爪闭合时，力量适应策略被激活，通过对手腕视图和触觉输入的联合注意力，以高频率预测抓取力。\n\n**整体框架**：RETAF框架将控制策略解耦为两个组件：一个用于轨迹生成的**基础策略**和一个用于接触调节的**力量适应策略**。基础策略以相对较低的频率运行，根据非触觉观测（如全局视觉）预测机械臂位姿`a_t_pose`和离散的夹爪动作`k_t`（开或合）。当基础策略发出抓取意图（`k_t = close`）时，力量适应策略被激活，以高频率（>30 Hz）根据手腕视图图像`I_t_wrist`和触觉读数`T_t`预测连续的目标抓取力`f_t`。最终的夹爪动作`a_t_grip`由此决定（闭合时输出预测力，张开时输出空动作）。\n\n**核心模块一：TF-Gripper硬件**。TF-Gripper是一个由两个Dynamixel XL430-W250-T驱动器驱动的低成本（约150美元）平行夹爪。\n\n![硬件设计](https://arxiv.org/html/2602.10013v1/x3.png)\n\n> **图3**：TF-Gripper的硬件设计。(a)关键组件：适配器将夹爪连接到机器人，两个电机通过拉动同步带驱动指尖沿线性导轨运动。(b)整体TF-Gripper设置，主体结构完全3D打印。(c)提供手腕视图的相机，以及集成了触觉传感器的软质指尖。\n\n其核心设计采用滑轮-同步带传动机制。每个电机驱动一个滑轮拉动同步带，从而驱动一个指尖沿低摩擦线性导轨做平行运动。这种设计有效力矩臂由滑轮半径决定，提供了从电机扭矩到指尖力的可靠且可重复的映射。指尖配备来自UMI的软质指尖垫和低成本柔性压阻式触觉传感器FlexiTac。通过适配器可兼容不同机器人。电机电流与指尖力呈现近线性关系（见图4），实现了约0.45–45 N范围内的精确开环力控。\n\n![电流-力关系](https://arxiv.org/html/2602.10013v1/x4.png)\n\n> **图4**：驱动器电流（PWM）–力的关系。力在指尖接触面测量，反映了TF-Gripper施加的有效抓取力。\n\n**核心模块二：遥操作设备**。为了收集包含人力控制的演示数据，本文设计了一个低成本的遥操作接口。该设备使用VR控制器控制机器人末端位姿，并配备两个指环，指环连接与夹爪相同的Dynamixel驱动器。驱动器被配置为提供类似弹簧的阻力，在通过电机电流测量操作者所施加力量的同时提供局部动觉反馈，该信号被校准并映射到机器人夹爪驱动器的电流命令。\n\n**核心模块三：RETAF策略**。\n1.  **基础策略**：利用现有的视觉运动策略架构（如扩散策略），无需为触觉融合修改结构。通过行为克隆进行训练，目标是最小化预测与专家动作`a_t*`之间的损失。\n2.  **力量适应策略**：当被激活时，该策略通过一个**联合注意力层**专门处理手腕视图视觉观测和触觉感知。手腕视图提供紧凑的、以物体为中心的视觉信息（类别、形状、表面外观），触觉感知提供局部反馈（变形、初始滑动）。这种设计使其能够进行精确稳定的力控，而不被全局场景中的无关信息干扰。该策略通过监督回归进行训练，目标是最小化预测力与演示者施加力`f_t*`之间的均方误差。其轻量级设计允许在单块RTX 5070 Ti上实现80 Hz+的推理频率。\n\n**创新点**：与现有方法相比，创新点主要体现在：1）硬件上，提供了一个低成本、宽力域、集成触觉且跨平台兼容的开环力控夹爪解决方案；2）算法上，首次明确地将力控预测从位姿预测中解耦，并设计了一个专用于高频力控、融合手腕视图与触觉的轻量化策略模块，解决了耦合策略中的频率失配和学习不稳定问题。\n\n## 实验与结果\n**实验设置**：在真实世界中对五个需要精确力量调节的操作任务进行评估：豆腐抓取、薯片拾取、樱桃番茄拾取、液体转移、樱桃番茄采摘。实验平台为配备TF-Gripper的Franka Research 3机器人，使用第三方视角和手腕视图相机。每个任务收集50条人类演示轨迹。评估时每个任务进行10次 rollout，并将任务分解为三个阶段报告平均成功率：到达、稳定抓取、任务成功。\n\n**Baseline方法**：对比了扩散策略及其变体：DP（无触觉）、DP（有触觉）、ViTac-MAE（使用掩码自编码预训练触觉编码器）。这些baseline的夹爪动作可以是位置或力量。\n\n![任务环境](https://arxiv.org/html/2602.10013v1/x7.png)\n\n> **图7**：评估环境概述，说明了每个任务在此设置中的执行方式。\n\n**关键实验结果**：\n\n![结果表格](https://arxiv.org/html/2602.10013v1/x9.png)\n\n> **图9**：（对应论文表II）五个操作任务在三个阶段（到达R、稳定抓取G、任务成功S）的性能。RETAF结合力控在稳定抓取（平均68%）和任务成功（平均60%）率上显著优于所有基线。\n\n表II（对应图9）显示：\n1.  **力控 vs. 位置控 (RQ1)**：在所有策略中，力控相比位置控 consistently 实现了更高的稳定抓取率。例如，RETAF使用力控将稳定抓取率从44%（位置控）提升至68%，任务成功率从28%提升至60%。这表明力控的优势主要体现在物理接触阶段。\n2.  **触觉的必要性 (RQ2)**：对比DP w/ Tac和DP w/o Tac，在力控设置下，加入触觉反馈使平均稳定抓取率从32%提升至38%，任务成功率从12%提升至20%，证实了触觉对精确力控至关重要。\n3.  **RETAF的有效性 (RQ3)**：RETAF（力控）在几乎所有任务和阶段上都显著优于非反应式的基线（DP, ViTac-MAE）。其平均稳定抓取率（68%）和任务成功率（60%）均为最高。\n4.  **与不同基础策略的集成 (RQ4)**：实验表明RETAF框架可以与不同的位姿预测策略（此处以DP为基础策略）有效集成并带来性能提升。\n\n![力量预测曲线](https://arxiv.org/html/2602.10013v1/x8.png)\n\n> **图8**：在验证用人类演示上的力量预测随时间变化曲线。对比了扩散策略（DP/Force）和RETAF的预测与真实值（Human Demo）。RETAF的预测更贴合真实的人类力量调整轨迹，表现出更好的反应性。\n\n图8进一步提供了定性分析，显示RETAF能够更快速、更精确地响应接触动态（如触觉信号变化），其预测的力量曲线比基线DP更贴近人类演示。\n\n**消融实验总结**：实验本身通过对比不同变体（有无触觉、位置vs力控、不同策略架构）构成了系统的消融研究。结果总结表明：1) 直接力控优于间接位置控；2) 触觉反馈是有效力控的必要条件；3) RETAF的解耦、高频反应式架构相比耦合的低频策略有显著优势。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了TF-Gripper，一个低成本、具有精细力控范围（0.45–45 N）并集成触觉感知的通用夹爪，附带一个能直接记录人力输入的遥操作设备，为学习力控策略提供了硬件和数据收集基础。\n2.  提出了RETAF策略框架，创新性地将力控与末端执行器位姿预测解耦，通过一个专用于高频力控、融合手腕视图与触觉的轻量化模块，实现了反应式的力量适应。\n3.  在五个真实世界力敏任务上进行了全面评估，实证了直接力控、触觉反馈以及RETAF框架相对于现有位置控制和耦合策略的优越性。\n\n**局限性**：论文提到TF-Gripper的力控本质上是开环的（基于电流-力映射），尽管通过机械设计（同步带传动）保持了良好的一致性，但仍可能受到温度漂移等因素影响（通过温度补偿可将误差降至4%以下）。\n\n**后续研究启示**：这项工作为规模化学习机器人力控策略开辟了道路。TF-Gripper的低成本和开源特性有望降低该领域的研究门槛。RETAF的解耦思想为将力控模块集成到更广泛的机器人学习架构中提供了范例。未来工作可探索在更复杂、非结构化的场景中应用此框架，或结合更强大的基础策略和感知模型。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10013v1/x1.png",
        "https://arxiv.org/html/2602.10013v1/x2.png",
        "https://arxiv.org/html/2602.10013v1/x3.png",
        "https://arxiv.org/html/2602.10013v1/x4.png",
        "https://arxiv.org/html/2602.10013v1/x5.png",
        "https://arxiv.org/html/2602.10013v1/x6.png",
        "https://arxiv.org/html/2602.10013v1/x7.png",
        "https://arxiv.org/html/2602.10013v1/x8.png",
        "https://arxiv.org/html/2602.10013v1/x9.png",
        "https://arxiv.org/html/2602.10013v1/x10.png",
        "https://arxiv.org/html/2602.10013v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09973",
      "title": "RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.09973",
      "arxivId": "2602.09973",
      "date": "2026-02-10",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "论文针对机器人操作中现有数据集成本高、本体依赖性强、多样性不足，导致视觉-语言-动作（VLA）模型泛化困难的核心问题，提出了RoboInter中间表示套件。关键技术包括：RoboInter-Tool半自动标注工具、RoboInter-Data大规模数据集（含超过230k个episodes和10+类中间表示）、RoboInter-VQA具身VQA基准（覆盖29个空间与时间类别）以及RoboInter-VLA集成“计划-然后-执行”框架。该套件通过提供细粒度、多样化的中间表示，为推进鲁棒和可泛化的机器人学习奠定了实用基础。",
      "detailedSummary": "## 研究背景与动机\n当前，大型视觉语言模型（VLMs）的进展激发了人们对用于机器人操作的视觉-语言-动作（VLA）系统的兴趣。然而，现有的操作数据集仍然存在成本高昂、与具体机器人形态高度耦合、覆盖范围和多样性不足的问题，这阻碍了VLA模型的泛化能力。近期研究试图通过“先计划后执行”的范式来缓解这些局限，即先生成高级计划（如子任务、轨迹），再将其翻译为低级动作。但这种方法严重依赖额外的中间表示监督，而现有数据集大多缺乏此类标注。现有自动化标注方法（如LLARVA、ECoT）在规模、质量或与动作的对齐性上存在不足。因此，本文旨在通过构建一个统一的中间表示资源套件（包括数据、基准和模型）来弥合这一差距。核心思路是：首先创建一个大规模、高质量、帧对齐的多样化中间表示标注数据集（RoboInter-Data），并基于此构建系统的具身视觉问答基准（RoboInter-VQA）来提升VLM的规划能力，最终提供一个支持多种变体的“先计划后执行”VLA框架（RoboInter-VLA），以研究中间表示对机器人操作泛化性和可控性的影响。\n\n## 方法详解\n本文提出的RoboInter是一个全面的中间表示套件，如图1所示，主要包括三个核心组成部分：RoboInter-Data（数据集）、RoboInter-VQA（基准）和RoboInter-VLA（模型框架）。\n\n![方法套件总览](https://arxiv.org/html/2602.09973v1/x1.png)\n> **图1**：RoboInter操作套件概览。包括标注工具、标注数据、精心构建的VQA数据集及其在VLMs和VLAs中的应用。\n\n**1. RoboInter-Data：大规模中间表示数据集**\n如图2所示，该数据集基于Droid和RH20T等现有操作数据集构建，包含超过23万条操作片段，覆盖571个不同场景。其核心是通过RoboInter-Tool这一轻量级GUI工具进行半自动逐帧标注，并结合人工检查，最终提供了超过10个类别的密集中间表示标注，包括：子任务、原始技能、分割、夹爪/物体边界框、放置建议、功能框、抓取姿态、轨迹、接触点等。所有标注均与执行的动作、机器人状态及双视角（第三人称和腕部视角）观测时间对齐，支持端到端动作学习。具体标注流程包括：任务分解与关键帧标注、被操作物体识别与分割、末端执行器定位与轨迹重建，并通过后处理推导出抓取、放置等额外标注。\n\n**2. RoboInter-VQA：具身视觉问答基准**\n为了系统化地评估和提升VLMs的具身推理能力，作者将RoboInter-Data中的标注转化为多样化的VQA任务。如图2右侧所示，这些任务沿两个维度组织：中间表示类型（空间 vs. 时间）和目标能力（理解 vs. 生成）。\n*   **空间VQA（理解）**：包括选择正确的物体边界框或抓取姿态、匹配场景与指令、判断是否发生接触等任务。\n*   **空间VQA（生成）**：要求生成物体边界框、抓取姿态、放置建议、关键点、夹爪边界框等空间中间表示。\n*   **时间VQA（理解）**：包括选择夹爪运动方向、匹配轨迹与描述、判别子任务/原始技能、识别执行阶段等任务，以及评估任务成功率和下一步可行性的判断任务。\n*   **时间VQA（生成）**：要求在给定不同上下文完整度（如过往子任务或整体指令）的条件下，生成未来轨迹或多步计划。\n\n![数据集与VQA构建概览](https://arxiv.org/html/2602.09973v1/x2.png)\n> **图2**：RoboInter-Data和RoboInter-VQA概述。展示了从原始数据收集、标注检查到构建大规模、多样化VQA任务的完整流程，并提供了统计数据。\n\n**3. RoboInter-VLA：“先计划后执行”模型框架**\n如图3所示，该框架遵循“先计划后执行”范式，包含一个**规划器（Planner）** 和一个**执行器（Executor）**。\n*   **规划器（VLM）**：基于Qwen-VL或LLaVA-One-Vision等VLM架构，通过在RoboInter-VQA数据上进行视觉问答训练，获得强大的具身理解和中间表示生成能力。\n*   **执行器（VLA）**：基于Qwen2.5-VL骨干网络，并配备一个扩散Transformer（DiT）动作头。它接收多视角视觉观测、语言指令以及（由规划器或真值提供的）中间表示，并输出多步动作块。\n*   **灵活的思维链（F-CoT）**：为了连接规划与执行，作者引入了F-CoT，它是由多个中间表示（如子任务、技能、物体框、轨迹等）以文本或视觉形式组成的思维链。F-CoT既作为训练规划器的VQA监督，也作为指导执行器的动作对齐引导。\n\n![VLA框架](https://arxiv.org/html/2602.09973v1/x3.png)\n> **图3**：RoboInter-VLA框架。模型遵循“先计划后执行”范式，包含一个VLM规划器和一个执行器。支持三种变体，中间表示通过灵活的思维链（F-CoT）连接规划与执行。\n\n**创新点**：本文提出了三种具体的VLA范式变体，以灵活利用规划器和中间表示：\n1.  **RoboInter-IC-E2E（隐式条件端到端）**：将预训练的规划器VLM直接注入端到端执行器，作为更强的视觉语言特征提取器。\n2.  **RoboInter-EC-E2E（显式条件端到端）**：执行器以规划器的VLM初始化，并联合优化中间表示推理和动作生成。\n3.  **RoboInter-Modular（模块化）**：规划器和执行器作为独立模块。训练时，执行器以真值中间表示为条件；推理时，依赖于规划器预测的中间表示。其中，文本F-CoT版本称为RoboInter-Te-Modular，视觉提示F-CoT版本称为RoboInter-Im-Modular。\n\n## 实验与结果\n**实验设置**：\n*   **基准/数据集**：使用了第三方基准（Where2Place, RoboRefIt, RoboVQA, Refcoco系列，以及通用VLM基准如TextVQA, MME等）和本文提出的RoboInter-VQA基准。执行器评估在“野外”（In-the-Wild）和“桌面”（TableTop）两种设置下进行。\n*   **对比方法**：在规划器评估中，对比了通用VLM（InternVL3, QwenVL2.5, LLaVA-OV）、闭源API（GPT4o-mini, Gemini-2.5-flash）以及具身VLM（RoboBrain-2.0）。在执行器评估中，对比了多种变体（Vanilla, VLA-OS, 以及使用不同中间表示来源的Oracle/QwenVL+Executor等）。\n\n**关键实验结果**：\n1.  **规划器基准测试**：\n    *   在第三方具身与 grounding 基准上（表2），RoboInter训练的规划器显著超越了基础VLM和RoboBrain-2.0。例如，RoboInter-Qwen-7B在RoboRefIt上达到85.6%，比RoboBrain-2.0-7B（8.8%）高出76.8个百分点；在RoboVQA上达到74.4，比后者（31.6）高出42.8分。同时，在通用基准上保持了相对稳定的性能。\n    *   在本文提出的RoboInter-VQA基准上（表3），当前闭源API和通用VLM在生成空间中间表示（如物体框、抓取功能框）方面表现不佳（大多低于40%），在时间推理（如轨迹生成）上也存在困难。而经过RoboInter-VQA训练的规划器在所有空间和时间任务上都取得了显著提升。例如，在空间生成任务上，RoboInter-LLaVAOV-7B的物体 grounding 准确率达到82.9%（IoU>0.1），远高于基础模型LLaVA-OV-7B的25.8%；在时间轨迹生成任务上，RoboInter-Qwen-7B的动态时间规整（DTW）误差为323，远优于基础QwenVL2.5-7B的1702。\n\n2.  **执行器开环评估**：\n    *   在“野外”设置下（表4），使用中间表示指导的变体（RoboInter-Te-Modular, Im-Modular, EC-E2E）均优于不使用中间表示的Vanilla和VLA-OS基线。其中，**RoboInter-Te-Modular**表现最佳，平均开环分数（mOLS）达到0.3543。这证明了中间表示对提升动作生成准确性的有效性。\n    *   在“桌面”设置下的训练曲线（图4）显示，所有RoboInter-VLA变体都快速收敛并显著优于Vanilla基线。同样，**RoboInter-Te-Modular**在整个训练过程中保持领先。\n\n![桌面设置开环评估曲线](https://arxiv.org/html/2602.09973v1/x4.png)\n> **图4**：桌面设置下的开环评估。展示了五种RoboInter-VLA变体在训练步数从1k到40k过程中，OLS@0.05指标的变化曲线。RoboInter-Te-Modular始终表现最佳。\n\n**消融实验分析**：\n表4的实验本质上是对不同VLA范式和使用不同来源中间表示的消融。结果表明：\n1.  使用**真值（Oracle）中间表示**指导的执行器性能最优（mOLS 0.3861），这为使用预测中间表示的上限提供了参考。\n2.  使用**预训练规划器预测的中间表示**（RoboInter-Te/Im-Modular）性能显著优于使用原始通用VLM（QwenVL+Executor）预测的结果，证明了RoboInter-VQA数据对提升规划器生成质量的关键作用。\n3.  在端到端变体中，**显式联合优化中间表示与动作**（EC-E2E）优于仅将规划器作为特征提取器（IC-E2E），后者又优于没有规划器的Vanilla。\n4.  在模块化变体中，**文本形式**的F-CoT（Te-Modular）略优于视觉提示形式（Im-Modular）。\n\n## 总结与启发\n**核心贡献**：\n1.  **大规模高质量中间表示数据集（RoboInter-Data）**：提供了超过23万条片段、涵盖10余个类别、帧对齐的密集中间表示标注，在规模和标注质量上超越了现有工作。\n2.  **系统性具身VQA基准（RoboInter-VQA）**：构建了涵盖29个空间与时间类别的VQA任务，系统地暴露并评估了VLMs在具身理解和生成方面的能力，并显著提升了规划器的性能。\n3.  **灵活可扩展的VLA框架（RoboInter-VLA）**：提供了一个支持隐式/显式条件端到端以及模块化三种范式的统一框架，并通过实验证明了高质量中间表示对提升动作生成泛化性和性能的有效性。\n\n**局限性**：\n1.  尽管使用了半自动工具，但大规模数据标注仍然成本高昂。\n2.  研究主要基于现有数据集的组合，模型在全新场景或机器人形态下的泛化能力仍有待进一步验证。\n\n**启示**：\n本文为利用细粒度、多样化中间表示推动机器人学习建立了一个实用的基础。其开源的数据、基准和模型有望促进社区在“先计划后执行”范式、VLM具身能力提升以及VLA模型设计等方面的进一步研究。未来的工作可以探索如何更高效地获取或合成中间表示数据，以及如何将此类框架更好地迁移到未知场景和实体。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09973v1/x1.png",
        "https://arxiv.org/html/2602.09973v1/x2.png",
        "https://arxiv.org/html/2602.09973v1/x3.png",
        "https://arxiv.org/html/2602.09973v1/x4.png",
        "https://arxiv.org/html/2602.09973v1/x5.png",
        "https://arxiv.org/html/2602.09973v1/x6.png",
        "https://arxiv.org/html/2602.09973v1/x7.png",
        "https://arxiv.org/html/2602.09973v1/x8.png",
        "https://arxiv.org/html/2602.09973v1/x9.png",
        "https://arxiv.org/html/2602.09973v1/x10.png",
        "https://arxiv.org/html/2602.09973v1/x11.png",
        "https://arxiv.org/html/2602.09973v1/x12.png",
        "https://arxiv.org/html/2602.09973v1/x13.png",
        "https://arxiv.org/html/2602.09973v1/x14.png",
        "https://arxiv.org/html/2602.09973v1/x15.png",
        "https://arxiv.org/html/2602.09973v1/x16.png",
        "https://arxiv.org/html/2602.09973v1/x17.png",
        "https://arxiv.org/html/2602.09973v1/x18.png",
        "https://arxiv.org/html/2602.09973v1/x19.png",
        "https://arxiv.org/html/2602.09973v1/x20.png",
        "https://arxiv.org/html/2602.09973v1/x21.png",
        "https://arxiv.org/html/2602.09973v1/x22.png",
        "https://arxiv.org/html/2602.09973v1/x23.png",
        "https://arxiv.org/html/2602.09973v1/x24.png",
        "https://arxiv.org/html/2602.09973v1/x25.png",
        "https://arxiv.org/html/2602.09973v1/x26.png",
        "https://arxiv.org/html/2602.09973v1/x27.png",
        "https://arxiv.org/html/2602.09973v1/x28.png",
        "https://arxiv.org/html/2602.09973v1/x29.png",
        "https://arxiv.org/html/2602.09973v1/x30.png",
        "https://arxiv.org/html/2602.09973v1/x31.png",
        "https://arxiv.org/html/2602.09973v1/x32.png",
        "https://arxiv.org/html/2602.09973v1/x33.png",
        "https://arxiv.org/html/2602.09973v1/x34.png",
        "https://arxiv.org/html/2602.09973v1/x35.png",
        "https://arxiv.org/html/2602.09973v1/x36.png",
        "https://arxiv.org/html/2602.09973v1/x37.png",
        "https://arxiv.org/html/2602.09973v1/x38.png",
        "https://arxiv.org/html/2602.09973v1/x39.png",
        "https://arxiv.org/html/2602.09973v1/x40.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09940",
      "title": "Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.09940",
      "arxivId": "2602.09940",
      "date": "2026-02-10",
      "authors": "Laxmidhar Behera Team",
      "category": "Manipulation",
      "summary": "本文提出Instruct2Act框架，旨在解决机器人在资源受限环境下难以理解和执行自由形式人类指令的问题。核心方法包括：1）基于BiLSTM与多头注意力自编码器的指令解析模块，将自然语言指令分解为原子动作序列；2）结合动态自适应轨迹径向网络（DATRN）与YOLOv8视觉分析器的机器人动作网络，生成精确控制轨迹。实验表明，该系统在自定义数据集上子动作预测准确率达91.5%，在四种真实机器人任务中整体成功率为90%，单次子动作推理时间小于3.8秒。",
      "detailedSummary": "## 研究背景与动机\n当前，基于大语言模型（LLMs）和视觉语言模型（VLMs）的视觉语言动作（VLA）管道，能够将自由形式的指令直接映射到感知和轨迹生成，是机器人人机交互领域的主流方法。然而，在仅使用眼在手（手腕）单摄像头配置的实际部署中，这些方法面临关键局限性：手臂和夹具频繁遮挡关键物体、视角快速变化、全局上下文有限导致位姿和帧模糊，从而降低操作可靠性。此外，即便是轻量级VLA变体也常依赖广角多摄像头设置或庞大的多模态编码器，引入了延迟并增加了实际环境中的维护成本。\n\n本文针对在资源受限、仅配备单眼在手摄像头的实际环境中，实现可靠、实时操作的痛点，提出了一个新视角：不追求端到端的、基于大型模型的复杂理解，而是采用一个轻量级、完全在设备上运行的两阶段确定性管道。其核心思路是：首先，将自然语言指令精细解析为一个有序的原子动作序列（如`reach`, `grasp`, `move`）；然后，利用一个结合了动态自适应轨迹径向网络（DATRN）和视觉分析器的机器人动作网络（RAN），为每个子动作生成并执行精确的控制轨迹。\n\n## 方法详解\n本文提出的方法是一个完整的两阶段管道，整体框架如图1所示。第一阶段（Instruct2Act模块）的输入是自然语言指令（或经语音转文本后的文本），输出是一个有序的原子子动作序列。第二阶段（RAN模块）的输入是该子动作序列，输出是控制机器人执行这些动作的具体轨迹命令，最终完成整个任务。\n\n![方法框架](https://arxiv.org/html/2602.09940v1/front_page_jp.png)\n> **图1**：Instruct2Act与RAN的整体框架。左侧为指令解析为动作序列的Instruct2Act模块，右侧为执行动作序列的RAN模块，包含环境分析、轨迹生成与机器人执行。\n\n**核心模块一：Instruct2Act（指令到动作解析模块）**\n该模块的目标是将任务描述翻译成子动作序列。其技术细节如下：\n1.  **任务嵌入提取**：使用`bert-large-uncased`模型将指令文本转换为1024维的语义嵌入向量。\n2.  **序列对齐**：将上述嵌入向量在时间维度上复制/对齐到固定长度`L`，形成序列`{R_i,t}`，作为后续序列模型的输入。\n3.  **学习架构**：采用双向LSTM（BiLSTM）结合多头自注意力（MHA）和自编码器的轻量级网络。\n    *   **BiLSTM层**：处理对齐后的嵌入序列，捕获前后文信息，输出512维（每个方向256维）的隐藏状态序列。\n    *   **MHA层**：包含8个注意力头（`dk=256`），对BiLSTM输出进行自注意力计算，使模型能关注序列中不同位置的信息，输出经线性映射和另一个BiLSTM（作为前馈块）处理，得到256维特征。\n    *   **自编码器层**：作为一个正则化组件，将前述256维特征通过一个BiLSTM编码器压缩为128维的瓶颈特征，再通过一个BiLSTM解码器重建回256维。训练时，计算瓶颈特征与重建特征之间的均方误差（MSE）作为重建损失。\n    *   **输出层**：一个时间分布的全连接层，对解码器每个时间步的输出应用Softmax，预测该步对应的子动作类别概率分布。\n4.  **训练与损失**：模型端到端训练，损失函数是子动作分类的交叉熵损失与自编码器重建损失的加权和（`ℒ = 交叉熵 + λ * MSE`），使用Adam优化器。\n\n**核心模块二：机器人动作网络（RAN）**\nRAN负责执行Instruct2Act预测出的每个子动作，其工作流程如图3所示，包含三个组件：\n\n![RAN工作流程](https://arxiv.org/html/2602.09940v1/RAN.jpeg)\n> **图3**：RAN的工作流程。接收子动作序列后，环境分析器检测物体；若物体存在，则进行坐标转换并交由DATRN生成轨迹，最后由机器人执行。\n\n1.  **最优轨迹学习（DATRN）**：这是RAN的核心创新。针对动态运动基元（DMP）方法需要迭代超参数搜索、相位/增益调度和长拟合时间等局限，提出了动态自适应轨迹径向网络（DATRN）。它使用径向基函数（RBF）网络来建模轨迹动力学。RBF中心通过K-Means聚类在示范轨迹上优化放置，宽度根据中心间平均距离自动设置。网络权重通过正则化最小二乘（岭回归）一次性求解（公式9），实现了快速、单次学习。DATRN的动力学方程（公式7）包含一个非线性吸引子项`tanh(g - y(t_k))`，能稳定驱动轨迹收敛至目标`g`。\n2.  **环境分析器**：使用定制训练的YOLOv8-n模型实时检测工作空间中指令提及的物体。如果检测到目标物体，则返回其位置；否则任务安全停止。对于指令中缺失的对象或目标位置，系统会交互式地向用户查询。\n3.  **机器人执行模型**：负责将感知信息转化为具体动作。\n    *   **坐标转换**：结合YOLO检测的物体2D图像中心`(u, v)`和深度传感器提供的深度值`d`，通过相机内参反投影到3D相机坐标系，再通过ROS TF2提供的变换矩阵转换到机器人基坐标系，得到目标位置`(x, y, z)`。\n    *   **轨迹规划与执行**：将目标坐标`(x, y, z)`输入DATRN，生成平滑的、类人的轨迹。机器人执行该轨迹。对于需要高精度到达的任务（如抓取、倾倒），使用比例-微分（PD）控制器进行误差最小化。\n    *   **处理遮挡**：对于抓取后摄像头被遮挡的情况（如抓取-放置、抓取-倾倒），系统在抓取前预先识别并存储后续放置/倾倒的目标位置。\n\n**创新点**：与现有VLA方法相比，本文的创新具体体现在：1) **轻量级与完全在设备运行**：整个管道设计紧凑，无需云端服务，适合资源受限环境。2) **确定性动作解析**：将模糊的指令解析为确定的原子动作序列，提高了操作的可靠性和可解释性。3) **高效的轨迹生成**：用DATRN替代DMP，避免了繁琐的参数调整，实现了快速、自适应的轨迹生成。\n\n## 实验与结果\n**实验设置**：\n*   **硬件**：Kinova 7自由度机械臂，Intel RealSense D410深度相机（眼在手配置），Nvidia RTX 4060 GPU用于训练。\n*   **软件**：ROS， Kinova库， Huggingface Transformers， TensorFlow， PyTorch。\n*   **数据集**：自定义专有数据集，包含2850条自然语言指令及其对应的结构化子动作序列，涵盖拾放、倾倒、清洁、给予等任务。\n\n**对比的Baseline方法**：在指令解析任务上，对比了标准LSTM、BiLSTM以及BiLSTM+MHA模型。\n\n**关键实验结果**：\n1.  **指令解析精度**：如表IV(c)所示，Instruct2Act在测试集上达到了91.5%的子动作预测准确率，显著高于LSTM（79.21%）、BiLSTM（81.92%）和BiLSTM+MHA（85.56%）。其加权F1分数和召回率也分别为91%和88%。\n\n![混淆矩阵](https://arxiv.org/html/2602.09940v1/x3.png)\n> **图5**：Instruct2Act在测试集上的混淆矩阵。显示了模型对不同子动作类别的预测情况，对角线上的高亮度表明整体预测准确性很高。\n\n2.  **训练过程**：如图4所示，训练损失和验证损失平滑下降并收敛（最终分别为0.0091和0.0088），重建损失也同步下降，表明模型学习有效且未过拟合。\n\n![训练损失曲线](https://arxiv.org/html/2602.09940v1/x2.png)\n> **图4**：模型的训练与验证损失（分类损失）曲线，以及自编码器分支的重建损失曲线。两条主损失曲线平行收敛，显示模型泛化良好。\n\n3.  **消融实验**：如图6所示，移除非核心组件会降低性能。移除自编码器导致准确率下降1.8%，移除MHA下降4.3%，同时移除两者则下降9.6%，证明了MHA和自编码器各自对提升模型性能的贡献。\n\n![消融实验](https://arxiv.org/html/2602.09940v1/x4.png)\n> **图6**：Instruct2Act架构的消融研究结果。完整模型性能最佳，移除多头注意力（MHA）或自编码器（AE）组件均会导致准确率下降，证明两者均为有效设计。\n\n4.  **机器人任务执行成功率**：在真实机器人上评估了四个任务（见表IV(a)）。整体任务成功率达到90%，其中“桌面清洁”成功率最高（95%），“拾取并给予”最低（85%）。子动作推理时间小于3.8秒，端到端任务执行时间在30-70秒之间，取决于任务复杂度。\n\n5.  **失败分析**：如表IV(b)所示，在总共8次失败中，37.5%归因于机器人动作网络（如轨迹执行误差），25%归因于子动作序列预测错误，另外37.5%为系统故障（如通信问题）。\n\n6.  **定性结果**：图7至图11展示了系统成功执行“拾放”、“拾取并倾倒”、“桌面清洁”、“拾取并给予”以及一个组合任务（“拾取瓶子，倒入杯中，然后清洁桌子”）的序列图像，直观证明了方法的有效性。\n\n![定性结果1](https://arxiv.org/html/2602.09940v1/x5.png)\n> **图7**：“拾取与放置”任务的执行序列。\n\n![定性结果2](https://arxiv.org/html/2602.09940v1/x6.png)\n> **图8**：“拾取与倾倒”任务的执行序列。\n\n![定性结果3](https://arxiv.org/html/2602.09940v1/x7.png)\n> **图9**：“桌面清洁”任务的执行序列。\n\n![定性结果4](https://arxiv.org/html/2602.09940v1/x8.png)\n> **图10**：“拾取并给予”任务的执行序列。\n\n![定性结果5](https://arxiv.org/html/2602.09940v1/x9.png)\n> **图11**：组合任务（“拾取瓶子，倒入杯中，然后清洁桌子”）的执行序列。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种集成了学习模块（指令解析）与执行模块（轨迹生成与控制）的完整方法论，弥合了自然语言解析与精确机器人控制之间的鸿沟。\n2.  提出了一个轻量级的Instruct2Act框架，它通过结合BiLSTM、多头注意力和自编码器，能够以较低计算开销从人类指令中准确提取细粒度的子动作序列。\n3.  引入了机器人动作网络（RAN），其中创新的动态自适应轨迹径向网络（DATRN）与基于视觉的环境分析器（YOLOv8）结合，能在机器人工作空间内生成平滑、精确的操纵轨迹。\n\n**局限性**：论文自身提到，该方法目前依赖于一个自定义的专有数据集进行训练和评估。此外，实验是在受控的实验室和初步的现实医疗环境中进行的，尚未在高度动态或不可预测的非结构化环境中进行大规模测试。\n\n**对后续研究的启示**：\n1.  **轻量级与确定性的价值**：在资源受限的实际部署场景中，一个轻量级、完全在设备上运行、具有确定性的管道可能比依赖大型模型和云端的复杂系统更具实用性和鲁棒性。\n2.  **模块化设计**：将复杂的“指令到动作”问题分解为“解析”和“执行”两个相对独立的阶段，并分别优化，是一种有效的工程化思路，提高了系统的可解释性和可调试性。\n3.  **DATRN的潜力**：DATRN作为一种高效、免调参的轨迹生成方法，在需要从演示中快速学习并适应新目标的机器人技能学习领域具有进一步的应用和拓展空间。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09940v1/front_page_jp.png",
        "https://arxiv.org/html/2602.09940v1/x1.png",
        "https://arxiv.org/html/2602.09940v1/RAN.jpeg",
        "https://arxiv.org/html/2602.09940v1/x2.png",
        "https://arxiv.org/html/2602.09940v1/x3.png",
        "https://arxiv.org/html/2602.09940v1/x4.png",
        "https://arxiv.org/html/2602.09940v1/x5.png",
        "https://arxiv.org/html/2602.09940v1/x6.png",
        "https://arxiv.org/html/2602.09940v1/x7.png",
        "https://arxiv.org/html/2602.09940v1/x8.png",
        "https://arxiv.org/html/2602.09940v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09888",
      "title": "TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback",
      "url": "http://arxiv.org/abs/2602.09888",
      "arxivId": "2602.09888",
      "date": "2026-02-10",
      "authors": "Weiming Zhi Team",
      "category": "Manipulation",
      "summary": "本文针对移动操作机器人全身遥操作中，操作者需同时协调底盘与双臂、并兼顾避障与接触的难题，提出TriPilot-FF系统。其核心创新在于引入脚部操作的踏板，通过低成本激光雷达生成基于障碍物距离的触觉阻力，引导操作者无碰撞移动；同时结合双臂主从遥操作与力反馈，提升接触感知与操作可达性。实验表明，系统能有效辅助操作者完成长时间、需精确底盘协调的任务，并将反馈信号融入ACT策略后进一步提升了性能。",
      "detailedSummary": "## 研究背景与动机\n当前，移动操作机器人的全身体遥操作面临核心挑战：操作者必须同时协调移动底座和两只机械臂，并实时推理障碍物和接触力。现有接口（如VR控制器、操纵杆）主要将控制带宽分配给双手，而脚部这一可用于连续底座控制的通道未被充分探索。这导致操作者缺乏一种能减少碰撞和协调负担的、信息丰富的连续底座控制方式。此外，遥操作不仅是自主性的后备方案，更是机器人学习获取数据和监督信号的主要机制。因此，设计能降低操作负荷、预防常见故障（如底座碰撞）并暴露信息反馈通道的接口，对于获取更干净的演示数据和训练信号至关重要。\n\n本文针对上述痛点，提出将全身体遥操作视为一种交互式设置，而非简单的用户输入到机器人动作的映射。核心思路是：通过引入带有激光雷达驱动力反馈的脚控踏板来回收脚部作为连续底座控制通道，并结合上半身双手“领导者-跟随者”遥操作，利用多模态（触觉和视觉）引导信号实时渲染任务相关约束，从而降低操作者的认知负荷，实现人机“共同驾驶”。\n\n## 方法详解\nTriPilot-FF系统的核心设计原则是将全身体控制分配到互补的人体模态上：通过运动学解耦的脚控踏板命令连续底座运动，同时通过同构的“领导者-跟随者”双臂遥操作进行双手操作。操作者保留完全控制权，系统则渲染任务相关的引导约束。\n\n![系统总览与踏板设计](https://arxiv.org/html/2602.09888v1/x2.png)\n\n> **图2**：TriPilot-FF系统概览与踏板设计。左侧展示了低成本、3自由度的踏板设备设计。右侧展示了系统整体框架：操作者通过脚控踏板指挥移动底座，通过一对“领导者”机械臂指挥远程的“跟随者”机械臂。系统基于接触、附近障碍物以及为提升可达性而调整底座的需求，在操作侧的领导者手臂和踏板上提供力反馈。\n\n**系统硬件**：遥操作站提供三个同步模态：1) 来自两个手腕相机、一个头部相机以及低成本360度激光雷达的视觉和深度感知输入；2) 通过同构映射的双手“领导者-跟随者”机械臂提供手臂和夹爪输入；3) 通过3自由度踏板提供脚部输入，以控制移动底座。远程机器人执行一个统一的包含底座扭转和双臂关节目标的全身体命令，同时将本体感知和力相关信号流回传给操作者以进行力反射和引导。\n\n**核心软件模块**：\n1.  **重力补偿的力反射**：为了在反映接触力的同时限制操作者疲劳，“领导者”手臂运行在重力补偿的柔顺模式下。系统从电机电流估计“跟随者”关节扭矩 τ_fol，减去模型重力 ĝ(q_fol) 以隔离交互扭矩。然后，将“领导者”的前馈扭矩设置为 τ_ff_lead = - (τ_fol - ĝ(q_fol)) * s_i + ĝ(q_lead)，其中 s_i ∈ [0,1] 设置反射强度。这结合了领导者重力补偿与跟随者交互扭矩的缩放反射。\n2.  **避障踏板辅助**：为避免注意力集中在双手操作时发生的近距离碰撞，系统在踏板上渲染阻力，使不安全的底座运动在物理上更难发出。具体而言，系统根据操作者通过踏板位移指示的瞬时预期平移方向 d̂，利用激光雷达估计沿该方向到最近障碍物的距离 r(d̂)。定义一个平滑的排斥势函数 φ(r)，当距离 r 接近机器人表面半径 r0 时势能迅速增加。踏板阻力大小 f_Rep(d̂) = k_φ * |∂φ/∂r(r(d̂))|，其中 k_φ > 0 缩放力输出。该阻力被施加在踏板末端执行器上，方向与 d̂ 相反，从而在不使用显式避障控制器的情况下，引导操作者发出避免碰撞的驾驶命令。\n\n![排斥势函数](https://arxiv.org/html/2602.09888v1/Phi_0.png)\n\n> **图3**：排斥势函数 φ 在障碍物接近机器人时产生越来越大的排斥力。系统利用激光雷达信息在踏板上产生阻力。\n\n3.  **可达性踏板辅助**：为解决双臂操作中目标经常位于瞬时工作空间边缘或之外的问题，系统计算一个实时提示，当手臂接近伸展或低可操作度配置时，指向能提高灵巧性的方向。系统定义了一个基于关节空间可操作度 w(q) = sqrt(det(J(q)J(q)^T)) 的笛卡尔可操作度场 m(x)，它表示末端执行器到达位置 x 时所能达到的最佳可操作度。通过神经网络 m̂(x) 离线学习该场的可微近似，从而可以高效计算梯度 ∇_x m̂(x)。引导方向 v_guide 是当前末端执行器方向与可操作度梯度方向的加权组合，投影到水平面后产生踏板反馈力 f_guide，仅在手臂伸展且预测可操作度低于阈值时激活，以保持操作者权限。\n4.  **可操作度视觉引导**：系统还将可操作度信息以视觉方式提供给操作者。通过相机参数获取点云，并评估可操作度场，得到每个像素在当前底座位置下的可操作度值。通过阈值过滤后，可操作度可以可选地叠加在视频流上，使操作者能高效识别物体是否可以被轻松触及。\n\n![可操作度视觉叠加](https://arxiv.org/html/2602.09888v1/manip_1.png)\n\n> **图4**：可操作度区域高亮显示在两个手腕相机输入上。这告知用户在不移动底座的情况下物体是否可达。\n\n**创新点**：与现有方法相比，TriPilot-FF的创新具体体现在：1) 将脚部作为连续底座控制通道，并通过低成本激光雷达直接生成方向性的触觉阻力来辅助避障，无需额外的避障控制器；2) 提供多模态、全身体引导，包括手臂侧的力反射和基于学习到的可操作度场的实时踏板及视觉引导，以协同解决碰撞、接触感知和可达性问题；3) 将遥操作产生的力/扭矩信号作为额外的监督信息，用于增强模仿学习策略的性能。\n\n## 实验与结果\n实验在一个定制的双手机动操作平台上进行了广泛的真实世界评估。评估任务旨在回答五个核心问题（Q1-Q5）。\n\n**对比基线**：实验主要与TriPilot-FF系统的消融版本进行对比，例如“无踏板反馈”、“无任何反馈”等。\n\n**关键实验结果**：\n1.  **狭窄空间协调（Q1）**：任务包括“盲载”（搬运遮挡视线的物体）和“狭窄通道运输”。如表I所示，完整的TriPilot-FF系统在两项任务中均达到100%成功率、零碰撞，且完成时间最短。在盲载任务中，移除踏板反馈导致成功率降至55%，碰撞次数增加，时间延长，这验证了在视觉受限情况下激光雷达踏板反馈对于避障的关键作用。\n\n![狭窄空间任务场景](https://arxiv.org/html/2602.09888v1/narrow_passage.png)\n\n> **图5**：左图：双手机器人用双臂垂直重新定位横杆，然后穿过狭窄的门，再将横杆水平放回。右图：机器人搬运一个遮挡视线的箱子，并导航避开板条箱障碍物。系统提供力反馈以帮助引导操作者何时以及如何重新定位底座。\n\n![精细双手操作](https://arxiv.org/html/2602.09888v1/x3.png)\n\n> **图6**：TriPilot-FF支持精细的双手操作。(b) 机器人可以稳固地用两个夹爪抓握刚性物体并协调摆动，这得益于力反馈。\n\n2.  **可达性引导（Q2）**：在“引导抓取”任务中，操作者需调整底座以使左夹爪能够到目标物体。表II显示，启用可操作度引导（无论是踏板力反馈还是视觉叠加）都能显著减少完成任务所需的“微调”底座移动次数（从无引导时的约9次降至约2次）和总完成时间，同时保持高成功率。这证明了可达性引导有效降低了协调负担。\n\n3.  **接触任务（Q3）**：在涉及精确力控制的“滚轮接触”任务中，启用手臂力反射可将任务成功率从65%提升至100%，并将平均接触力误差降低约50%（从~1.5N降至~0.8N）。这表明力反射显著提高了接触调节的效率和精度。\n\n4.  **长时程任务（Q4）**：系统在包含多个子任务（如取放、开门、折叠衣物）的长序列中展示了可靠的性能，成功完成了90%的复杂步骤，验证了其在实际工作流程中的可用性和鲁棒性。\n\n5.  **策略学习（Q5）**：将遥操作收集的关节扭矩数据作为额外观测，增强Action Chunking with Transformers (ACT)策略。在“手套转移”任务中，使用扭矩增强观测训练的策略比标准ACT策略的成功率提高了15%（从80%到95%），证明了遥操作触觉信号能为模仿学习提供有用的监督信息。\n\n**消融实验总结**：实验系统地评估了各个反馈组件的贡献。踏板避障反馈对于在视觉受限或狭窄空间避免碰撞至关重要；手臂力反射对于需要精确接触力的任务效率和质量提升显著；可达性引导（无论是触觉还是视觉形式）能有效减少因工作空间限制而产生的、耗时的底座微调操作。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了TriPilot-FF，一个开源的、将脚控底座与双手“领导者-跟随者”遥操作耦合的全身体遥操作系统，通过多模态力反馈和视觉引导实现人机协同。\n2.  设计了基于低成本激光雷达的踏板力反馈机制，能够根据指令方向的障碍物接近度产生方向性阻力，在不使用显式避障控制器的情况下引导操作者避免碰撞。\n3.  开发了基于神经网络的可操作度场实时计算与引导方法（包括触觉和视觉），有效缓解了因可达性不足导致的协调失败，并证明了将遥操作力反馈信号纳入模仿学习策略可以提升下游任务性能。\n\n**局限性**：论文提到，系统依赖于精确的传感器外参校准以确保引导信号在正确坐标系中生成。此外，虽然踏板力反馈对静态和缓移动障碍物有效，但在处理高动态环境方面可能存在限制。\n\n**后续启示**：本研究展示了将人类多模态控制能力（手和脚）与机器实时感知和约束信息相结合，通过直观的物理反馈进行交互的可行性与优势。这种“共同驾驶”理念可推广至更复杂的机器人形态（如人形机器人）。同时，工作凸显了高质量、富含物理交互信息的遥操作数据对于机器人学习的重要性，为通过改进人机接口来提升数据质量和学习算法性能指明了方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09888v1/x1.png",
        "https://arxiv.org/html/2602.09888v1/x2.png",
        "https://arxiv.org/html/2602.09888v1/Phi_0.png",
        "https://arxiv.org/html/2602.09888v1/manip_1.png",
        "https://arxiv.org/html/2602.09888v1/narrow_passage.png",
        "https://arxiv.org/html/2602.09888v1/x3.png",
        "https://arxiv.org/html/2602.09888v1/transfer_glove_circ.png",
        "https://arxiv.org/html/2602.09888v1/task_bar.jpeg",
        "https://arxiv.org/html/2602.09888v1/Roller_contact.png",
        "https://arxiv.org/html/2602.09888v1/Kitchen_thin.png",
        "https://arxiv.org/html/2602.09888v1/Tool_room.png",
        "https://arxiv.org/html/2602.09888v1/basket_place.png",
        "https://arxiv.org/html/2602.09888v1/auto1.png",
        "https://arxiv.org/html/2602.09888v1/Transfer.png",
        "https://arxiv.org/html/2602.09888v1/folding_motion.png",
        "https://arxiv.org/html/2602.09888v1/TargetDetection_left.png",
        "https://arxiv.org/html/2602.09888v1/TargetDetection_mid.png",
        "https://arxiv.org/html/2602.09888v1/TargetDetection_right.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09878",
      "title": "MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.09878",
      "arxivId": "2602.09878",
      "date": "2026-02-10",
      "authors": "Xiangyu Yue Team",
      "category": "Manipulation",
      "summary": "本文提出MVISTA-4D，旨在解决机器人操作中现有世界模型无法预测完整、几何一致的4D场景动态的问题。方法核心包括：1）一个能从单视角RGBD观测生成任意视角、跨模态一致RGBD的4D世界模型，通过跨视角与跨模态特征融合确保几何对齐；2）测试时动作优化策略，通过生成模型反向传播推断最优轨迹潜在变量，并结合残差逆动力学模型将其转化为精确动作。实验在三个数据集上验证了该方法在4D场景生成与下游操作任务上的优越性能，消融实验明确了关键设计的有效性。",
      "detailedSummary": "## 研究背景与动机\n机器人操作领域，基于世界模型的“想象-行动”范式展现出巨大潜力。然而，现有方法存在关键局限性。主流方法可分为两类：一类是纯图像空间的视频生成模型，它们虽能预测未来外观，但常违反3D几何约束，导致想象结果与可执行动作间存在鸿沟；另一类是基于点云或粒子等3D几何表示的动力学模型，虽几何明确但外观和语义信息较弱，限制了精细操作所需的保真度。在动作推断方面，现有范式同样面临挑战：联合预测未来与动作易导致误差累积；基于几何的流程依赖脆弱的姿态估计；而经典的逆动力学模型则因其“多对一”的映射关系而具有不适定性，即多种动作可解释相同的观测变化，尤其在部分可观测和接触场景下。\n\n本文针对上述痛点，提出了一个新颖的视角：构建一个几何一致、多视图的4D世界模型，并将动作轨迹作为整体条件嵌入生成过程。其核心思路是：1）从单视图RGB-D观测出发，生成几何一致的多视图未来RGB-D序列，以提供更完整的动态场景结构；2）将整个动作序列压缩为低维轨迹潜在代码，作为生成模型的风格条件，并通过测试时优化该代码来推断与想象未来一致的动作。\n\n## 方法详解\nMVISTA-4D的整体框架基于一个潜在视频扩散模型。输入为参考视图的单帧RGB-D观测、目标相机外参和文本指令。输出为参考视图及所有目标视图上同步的、几何一致的未来RGB-D序列。这些序列可通过反投影和融合，得到动态场景的点云序列。动作推断则通过优化轨迹潜在代码并经过残差逆动力学模块精炼完成。\n\n![方法整体框架](https://arxiv.org/html/2602.09878v1/x1.png)\n\n> **图1**：MVISTA-4D方法整体框架。左侧：给定单视图初始观测（RGB-D）、文本指令和多个目标相机姿态，模型生成多视图、几何一致的未来RGB-D序列。右侧：动作推断阶段，首先通过测试时优化找到与文本条件生成的未来最匹配的轨迹潜在代码，解码得到动作先验，再通过残差逆动力学模块进行精炼，输出最终可执行动作。\n\n**核心模块一：特征集成与结构化分词**。为有效建模多视图、跨模态的生成，论文设计了结构化的分词策略。在每个视图内，将RGB和Depth的潜在图沿宽度方向拼接，使同一空间位置的跨模态令牌相邻，以鼓励跨模态一致性。在视图之间，由于视差和遮挡，像素级对齐不适定，因此采用沿高度方向拼接视图，以促进结构级的跨视图连贯性。这种布局支持可变数量的输入视图。\n\n**核心模块二：跨模态特征融合**。为明确鼓励RGB与深度间的一致性，引入了两个设计：1）**可学习的模态令牌**：为外观和几何特征分别添加一个可学习的令牌，以显式指示模态身份。2）**局部跨模态注意力**：在标准的自注意力模块前，插入一个轻量级的局部跨模态注意力模块。该模块仅在一个小邻域内交换外观与几何特征，并通过门控残差更新进行融合，这既提供了跨模态对应的强归纳偏置，又将计算成本从全局二次匹配降低到线性。\n\n**核心模块三：跨视图几何一致性学习**。这是确保多视图预测几何一致的关键。1）**相机嵌入作为视图令牌**：不同于将外参矩阵扁平化，论文提出使用以共享注视点为中心的球坐标参数化相机，并应用傅里叶特征，得到一个紧凑的13维嵌入。这使模型更容易获取尺度线索，并作为区分不同视图的令牌。2）**几何感知的可变形跨视图注意力**：利用已知相机参数，一个视图中的查询令牌在其他视图上诱导出一条对极线。该模块沿此对极线稀疏采样K个候选位置进行跨视图注意力，而非全局计算。此外，还引入了一个可变形细化步骤，根据查询特征、初始关键特征及其相似性预测一个小偏移，以调整采样位置，实现更好的对齐。\n\n**核心模块四：轨迹条件化作为风格代码**。为将动作轨迹的结构化信息注入生成过程，论文训练了一个基于TCN的变分自编码器，将动作序列编码为一个低维潜在令牌序列。在训练生成器时，将此轨迹潜在代码作为S个风格令牌，通过交叉注意力注入。为防止生成器忽略该条件，增加了一个轻量级的潜在一致性头，从生成器的最终层表征重建轨迹潜在代码，并施加重建损失。该头部仅在训练时使用。\n\n**核心模块五：具身推断与规划**。在测试时，动作推断分为两步：1）**轨迹潜在推断（测试时优化）**：首先仅使用文本条件生成一个未来序列V_bar。然后冻结生成器，通过反向传播优化一个随机初始化的轨迹潜在代码z，使其条件生成的序列尽可能接近V_bar。优化在低维潜在空间进行，比直接优化完整动作序列更稳定。优化得到的最优代码z*通过TCN解码器解码为动作先验序列。2）**残差逆动力学模块**：为解决逆动力学的不适定性，该模块将上述解码得到的动作先验作为强先验，仅预测一个修正项Δa_t。最终动作为先验动作与修正项之和。这使得模块专注于局部对齐和执行级调整，而非从零重建整个动作。\n\n## 实验与结果\n实验在三个数据集上进行：两个合成数据集RLBench和RoboTwin2，以及一个包含14个任务的真实机器人多视图数据集。对比的基线方法包括：UniPi（2D视频世界模型，增强深度后称为UniPi*）、TesserAct（单视图RGB-DN 4D世界模型）和4DGen（两视图点图4D世界模型）。所有方法均在共同的WAN2.2 TI2V骨干网络上实现以进行公平比较。\n\n**4D场景生成结果**：在合成和真实数据集上，MVISTA-4D在大多数外观（FVD, SSIM, PSNR）和几何指标（AbRel, RMSE, δ1, CD, EMD）上均优于基线。特别是在几何一致性指标（如CD和EMD）上提升显著，证明了其多视图几何一致生成的有效性。\n\n![4D生成定性结果](https://arxiv.org/html/2602.09878v1/x2.png)\n\n> **图2**：在RoboTwin数据集上的4D生成定性结果。红、绿、蓝框代表不同视点。MVISTA-4D生成的各视图RGB和深度序列在几何和外观上均保持高度一致，而基线方法（如TesserAct）在不同视图间存在明显的几何不一致和伪影。\n\n**下游机器人操作结果**：在模拟和真实机器人操作任务中，MVISTA-4D在任务成功率上 consistently 超越所有基线。例如，在RLBench的10个任务上，MVISTA-4D平均成功率为78.5%，显著高于TesserAct的69.2%和4DGen的64.8%。在真实机器人14个任务上，MVISTA-4D也取得了最高成功率。\n\n![消融实验](https://arxiv.org/html/2602.09878v1/x3.png)\n\n> **图3**：关键组件的消融研究结果。移除跨模态融合（w/o X-mod）或跨视图几何注意力（w/o X-view GeoAttn）均导致性能下降，验证了它们对生成质量的重要性。使用扁平化相机外参（w/ Flatten E）替代球坐标嵌入也会损害性能。在动作推断中，移除测试时优化（w/o TTO）或残差逆动力学（w/o RIDM）都会显著降低操作成功率。\n\n**消融实验**：论文系统地验证了各核心组件的贡献。如图3所示，移除跨模态融合、跨视图几何注意力或使用次优的相机表示，都会导致生成质量和操作成功率下降。在动作推断方面，测试时优化和残差逆动力学模块都是提升最终动作质量的关键。\n\n![动作推断结果](https://arxiv.org/html/2602.09878v1/x4.png)\n\n> **图4**：动作推断的定性结果。左侧展示了通过测试时优化轨迹潜在代码，从生成的未来中恢复出的动作序列（蓝色）与真实动作（橙色）对比，显示出良好的一致性。右侧展示了残差逆动力学模块的修正效果，先验动作（绿色）经修正（红色箭头）后更接近真实动作（蓝色）。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1）提出了一个具身的多视图4D生成世界模型，通过显式的跨视图和跨模态融合机制，实现了几何一致的多视图RGB-D序列预测。2）引入了轨迹级动作条件化，将完整动作序列表示为紧凑的潜在代码，并提出了基于测试时优化的动作推断方法，缓解了逆动力学的不适定性。3）设计了一个基于先验的残差逆动力学模型，通过仅学习修正项来提升动作执行的鲁棒性。\n\n论文自身提到的局限性包括：在极端遮挡或快速运动导致外观模糊的情况下，几何一致性可能受损；测试时优化和多次视图生成会带来额外的计算成本。\n\n这项工作对后续研究提供了重要启示：将动作视为具有时空结构的轨迹而非独立步骤，是连接生成模型与机器人控制的有效途径；在生成模型中显式地嵌入几何和相机约束，对于实现物理上合理的预测至关重要；结合优化与学习的混合推理范式，为解决不适定问题（如动作推断）提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09878v1/x1.png",
        "https://arxiv.org/html/2602.09878v1/x2.png",
        "https://arxiv.org/html/2602.09878v1/x3.png",
        "https://arxiv.org/html/2602.09878v1/x4.png",
        "https://arxiv.org/html/2602.09878v1/figure/cross_mod.png",
        "https://arxiv.org/html/2602.09878v1/x5.png",
        "https://arxiv.org/html/2602.09878v1/x6.png",
        "https://arxiv.org/html/2602.09878v1/x7.png",
        "https://arxiv.org/html/2602.09878v1/figure/HW_setup1.png",
        "https://arxiv.org/html/2602.09878v1/x8.png",
        "https://arxiv.org/html/2602.09878v1/figure/frame_0032.png",
        "https://arxiv.org/html/2602.09878v1/figure/failure_case_robotwin.png",
        "https://arxiv.org/html/2602.09878v1/x9.png",
        "https://arxiv.org/html/2602.09878v1/x10.png",
        "https://arxiv.org/html/2602.09878v1/x11.png",
        "https://arxiv.org/html/2602.09878v1/x12.png",
        "https://arxiv.org/html/2602.09878v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09767",
      "title": "Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning",
      "url": "http://arxiv.org/abs/2602.09767",
      "arxivId": "2602.09767",
      "date": "2026-02-10",
      "authors": "Wei Li Team",
      "category": "Manipulation",
      "summary": "本文针对四足机器人无监督技能发现中，单一策略学习效率低、技能表征易重叠以及奖励信号易被黑客攻击导致技能多样性不足的问题，提出了正交混合专家（OMoE）架构和多判别器框架。OMoE防止行为表征坍塌，使单一策略能掌握广泛运动技能；多判别器在不同观测空间运作以缓解奖励黑客。在Unitree A1机器人上的实验表明，该方法提升了训练效率，并使状态空间覆盖率比基线提升了18.3%。",
      "detailedSummary": "## 研究背景与动机\n在四足机器人控制领域，深度强化学习需要专家精心设计任务特定的奖励函数，而模仿学习则依赖于昂贵且任务特定的数据。无监督技能发现通过利用内在动机自主探索多样行为，有望减轻这些负担。然而，现有方法（如DIAYN）存在两个关键局限性：首先，它们通常依赖单一策略来掌握多样行为，没有对行为间的共享结构和差异进行建模，导致学习效率低下；其次，它们容易受到“奖励黑客”的影响，即奖励信号迅速增加并收敛，但学到的技能实际多样性不足，未能充分覆盖状态空间。\n\n本文针对上述痛点，提出了名为MOD-Skill的新框架。其核心思路是：通过一个解耦的多判别器架构来独立评估不同观测子空间的技能多样性，以缓解奖励黑客；同时，采用一个正交混合专家策略架构，对运动技能进行分解与正交化表征，以提高学习效率和技能多样性。\n\n## 方法详解\nMOD-Skill框架旨在通过无监督强化学习，发现一组由潜在变量k索引的多样化技能策略πθ(at|otp, k)。其整体流程如算法1所示：在每次训练迭代中，采样一个技能向量k，策略根据当前本体感知观测otp和k生成动作at，与环境交互后获得奖励rt并存储经验，随后使用PPO算法更新策略和价值函数，并使用交叉熵损失更新判别器。\n\n![方法框架总览](https://arxiv.org/html/2602.09767v1/overview.png)\n> **图1**：MOD-Skill框架总览。策略通过强化学习进行优化，判别器通过监督学习进行更新，两者协同促进技能发现。左侧展示了多判别器模块，右侧展示了正交混合专家策略架构。\n\n框架包含两个核心模块：\n1.  **多判别器模块**：该模块旨在最大化潜在技能K与观测Od之间的互信息下界。为了解决单一判别器在复杂观测空间下容易导致奖励黑客的问题，本文设计了多个判别器，每个判别器qi(k|odi)被分配到一个**互不相交**的观测子空间odi（例如[𝒗 𝝎]、[𝒈]、[𝜽 𝜽̇]），并独立计算内在奖励。技能发现奖励rtS是各判别器输出对数概率的平均值减去技能先验对数概率。通过这种解耦设计，鼓励智能体在所有相关的状态维度上进行多样化探索，从而缓解状态混淆和奖励黑客。\n\n2.  **正交混合专家策略**：策略网络采用OMoE架构。其输入是本体感知观测otp和技能向量k的拼接。首先，一组Ne个专家网络{Ei}将输入映射为特征向量ui。然后，关键的一步是使用**Gram-Schmidt过程**对这些特征向量进行正交化，得到正交基{vi}，以此强制专家学习到的表征保持足够的多样性。同时，一个任务编码器T根据相同的输入计算每个正交化专家特征的权重系数α。最后，加权聚合后的表征通过一个共享的输出头f映射到动作空间。这种架构将多样化的运动技能分解为一组正交表征，并由任务编码器自适应融合，从而实现了灵活的专家组合和多样化技能的生成。\n\n与现有方法相比，创新点具体体现在：1) 提出了观测子空间解耦的多判别器设计，从奖励机制上直接针对奖励黑客问题；2) 将正交约束引入技能发现策略的专家混合层，明确建模技能间的差异，提升了学习效率。\n\n## 实验与结果\n实验在拥有12个自由度的Unitree A1四足机器人平台上进行，使用Isaac Sim仿真器并创建4096个并行环境进行训练。策略运行频率为50Hz。对比的基线方法包括不同观测配置的单一判别器架构（SD1, SD2, SD3）以及不同的策略网络架构（MLP, MoE）。\n\n**判别器模块消融实验**：对比了SD1（观测：[𝒗 𝝎]）、SD2（观测：[𝒗 𝝎 𝒈]）、SD3（观测：完整运动观测）和本文的多判别器方法MD。\n\n![奖励曲线对比](https://arxiv.org/html/2602.09767v1/reward_discriminator.png)\n> **图2**：算法SD1、SD2、SD3和MD的奖励曲线。上图显示完整回合奖励，下图仅显示技能奖励部分。SD3的技能奖励上升最快，但出现了明显的奖励黑客（奖励高但技能实际多样性低）。\n\n![状态空间覆盖](https://arxiv.org/html/2602.09767v1/state_coverage_discriminator.png)\n> **图3**：状态空间覆盖可视化。收集各算法发现的技能在20秒内的线性速度、角速度和投影重力观测，并进行归一化。MD方法覆盖的状态空间范围最广。\n\n![状态空间覆盖率数据](https://arxiv.org/html/2602.09767v1/x1.png)\n> **表II**：状态空间覆盖率。MD方法达到了58.7%的覆盖率，相较于最佳基线SD2（49.6%）提升了18.3%。\n\n关键结果：SD3出现了典型的奖励黑客现象，其技能奖励快速收敛，但大多数技能只是在不同姿势下保持静止。而本文的多判别器架构实现了最大的状态空间覆盖范围，整体覆盖率比最佳基线算法提高了18.3%，并产生了在姿势、步态、线速度和角速度方面变化广泛的运动技能。\n\n**策略架构消融实验**：在固定使用多判别器的前提下，对比了MLP、MoE和OMoE三种策略网络。\n\n![策略架构奖励曲线](https://arxiv.org/html/2602.09767v1/reward_omoe.png)\n> **图5**：算法OMoE、MoE和MLP的奖励曲线。OMoE获得了比MoE和MLP更快的奖励增长，表明其学习效率更高。\n\n**真实世界实验**：通过域随机化（随机化地面摩擦、恢复系数、质量、动作延迟、电机扭矩并添加随机速度扰动）来缩小仿真到现实的差距。\n\n![真实世界实验](https://arxiv.org/html/2602.09767v1/real_world.png)\n> **图4**：真实世界实验。学习到的技能被部署在Unitree A1机器人上，在真实环境中展示了可靠且鲁棒的执行能力。\n\n## 总结与启发\n本文的核心贡献可概括为：1) 提出了MOD-Skill框架，创新性地整合了**解耦的多判别器架构**和**正交混合专家策略**，以解决无监督技能发现中的奖励黑客和低效问题；2) 实验表明，该框架将状态空间覆盖率提升了18.3%，有效缓解了奖励黑客，并显著提高了训练效率；3) 成功实现了从仿真到真实四足机器人的技能迁移验证。\n\n论文自身提到的局限性在于，未来工作需要减少对专家知识的依赖，使算法能够通过自主探索发现更丰富、更动态的技能库。这对后续研究的启示在于：无监督技能发现中，对技能多样性进行**结构化、解耦的激励**，以及对策略内部表征施加**明确的多样性约束**，是提升算法性能的有效途径。如何自动化地划分观测子空间或确定专家数量，是值得探索的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09767v1/overview.png",
        "https://arxiv.org/html/2602.09767v1/reward_discriminator.png",
        "https://arxiv.org/html/2602.09767v1/state_coverage_discriminator.png",
        "https://arxiv.org/html/2602.09767v1/real_world.png",
        "https://arxiv.org/html/2602.09767v1/reward_omoe.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09638",
      "title": "VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model",
      "url": "http://arxiv.org/abs/2602.09638",
      "arxivId": "2602.09638",
      "date": "2026-02-10",
      "authors": "Hui Xiong Team",
      "category": "Manipulation",
      "summary": "本文解决3D可操作区域（affordance）定位问题，指出现有方法依赖静态语言/图像线索，缺乏动态交互上下文。为此，作者构建了大规模视频数据集VIDA，并提出VideoAfford模型。关键技术包括：利用多模态大语言模型增强分割与推理能力；通过潜在动作编码器从人-物交互视频中提取动态先验；引入空间感知损失以学习3D空间知识。实验表明，该模型显著优于现有方法，并展现出强大的开放世界泛化与推理能力。",
      "detailedSummary": "## 研究背景与动机\n3D可承受性（affordance）定位旨在突出3D物体上可操作的区域，对于机器人操作至关重要。先前的研究主要集中于从语言和图像等静态线索中学习可承受性知识，这些方法难以提供揭示时间和因果线索的充足动态交互上下文。为了缓解这一困境，本文收集了一个全面的基于视频的3D可承受性数据集VIDA，并提出了一种新的任务：从人类-物体交互（HOI）演示视频中定位3D物体可承受性，旨在利用大规模演示视频语料库进行以物体为中心的3D可承受性推理。本文的核心思路是激活多模态大语言模型（MLLMs）额外的可承受性分割能力，通过引入潜在动作编码器从HOI视频中提取动态交互先验，并结合空间感知损失函数，在一个统一框架内实现世界知识推理和细粒度的3D可承受性定位。\n\n## 方法详解\n整体框架接受一个HOI视频和一个文本指令作为输入，目标是输出物体点云上的可承受性掩码。VideoAfford主要由四个组件构成：1）一个受益于大规模3D表示学习的3D视觉编码器，为密集预测任务提供坚实基础；2）一个预训练的潜在动作编码器，提供丰富的动作先验；3）一个展现出可承受性推理能力的视频多模态大语言模型（MLLM）；4）一个基于Transformer的轻量级可承受性解码器，用于生成密集的3D可承受性掩码。\n\n![方法总览](https://arxiv.org/html/2602.09638v1/x4.png)\n> **图4**：VideoAfford方法总览。给定一个HOI视频和对应的点云，模型使用LanguageBind作为视频编码器，RenderNet作为动作编码器，获得视频嵌入和潜在动作嵌入。这些嵌入与文本指令一同输入大语言模型，预测语言令牌和可承受性令牌。另一方面，使用预训练的3D编码器提取语义丰富的点嵌入，通过几何引导的上采样和传播模块获得密集点特征。最后，可承受性令牌和点特征被输入可承受性解码器以生成可承受性掩码。\n\n**核心模块与技术细节**：\n1.  **点云编码器**：采用在大型文本-图像-点配对数据上预训练的3D编码器作为骨干。给定点云，编码器将其编码为语义丰富的点嵌入，随后通过几何引导的上采样操作将语义特征传播为密集点特征。\n2.  **空间感知损失函数**：为了解决先前工作只关注点类别准确性而缺乏对“空间连续性和区域重叠”约束的问题，本文引入了空间损失。其核心机制是结合点云的空间邻域信息分配自适应权重，从而支持空间连续性的训练。该损失基于改进的Dice损失，通过计算点坐标的邻近度来赋予空间相邻点更高权重，强调聚类正样本在损失计算中的贡献。\n3.  **动作编码器**：为了增强动作理解能力，引入了潜在动作编码器来从紧凑的状态表示中学习可泛化的人类-物体交互动作。给定HOI视频，采样N帧并使用动作编码器提取潜在动作嵌入，并将其压缩为两个令牌。\n4.  **视频MLLM主干**：选择Video-LLaVA作为主干。当输入视频和文本时，使用视频编码器和动作编码器对视频进行稀疏编码以获得视频令牌和动作令牌，将它们拼接后作为LLM的输入。模型扩展了Video-LLaVA的词表，注入了特殊令牌`<AFF>`来代表可承受性世界知识，该令牌的隐藏状态被投影为查询嵌入，作为可承受性条件输入解码器。\n5.  **可承受性解码器**：一个基于Transformer的轻量级解码器，利用可承受性嵌入和密集点特征，通过交叉注意力模块进行融合，最终经由一个MLP网络生成可承受性掩码。\n6.  **训练目标**：总损失函数是交叉熵损失（用于文本输出）、二元交叉熵损失、IoU损失（用于分割掩码预测）以及前述空间感知损失的加权和。\n\n**创新点**：与现有主要依赖静态图像或语言的方法相比，本文的创新主要体现在：1）**任务与数据创新**：首次系统性地提出并构建了从HOI视频中学习物体中心3D可承受性的任务和大规模数据集VIDA。2）**方法创新**：设计了统一框架，将视频MLLM的世界知识推理能力、专门的动作编码器提取的动态先验，以及针对3D形态特点设计的空间感知损失相结合，实现了动态交互信息向3D空间知识的有效迁移。\n\n## 实验与结果\n**数据集与基准**：实验在本文新构建的VIDA数据集上进行，该数据集包含约38K个HOI视频和22K个带标注的点云，涵盖38个物体类别和16种可承受性类型。实验分为“可见”（训练和测试集中的物体及可承受性类别一致）和“不可见”（测试集中包含训练集未出现的物体或可承受性类别）两种设置。由于是新任务，没有直接的前期工作。因此，作者选择了几种先进的基于HOI图像的3D可承受性定位方法作为模块化基线进行比较，包括XMF、PFusion、IAGNet、LASO、GREAT、Seqafford，并复现了LMAfford3D。\n\n**关键实验结果**：\n\n![主结果表](https://arxiv.org/html/2602.09638v1/x2.png)\n> **表2**：主要对比结果。展示了所有对比方法的mIoU、AUC、SIM和MAE指标（均为百分比）。最佳结果以粗体标出，次佳结果以下划线标出。结果表明，无论是在“可见”还是“不可见”设置下，本文提出的VideoAfford方法在所有评估指标上均显著优于所有基线方法。例如，在“可见”设置下，mIoU达到28.20%，比次优的GREAT（23.62%）高出4.58个百分点；在“不可见”设置下，mIoU为10.95%，同样领先。\n\n![消融实验表](https://arxiv.org/html/2602.09638v1/x3.png)\n> **表3**：消融实验结果。该实验验证了动作编码器和空间感知损失函数对模型性能的贡献。结果表明，两者均能独立带来性能提升，而结合使用时效果最佳。在“可见”设置下，同时使用两者使mIoU从基线20.16%提升至28.20%；在“不可见”设置下，从7.12%提升至10.95%，证明了这两个组件对于提升模型动态理解能力和空间预测一致性的有效性。\n\n![可视化结果](https://arxiv.org/html/2602.09638v1/x5.png)\n> **图5**：定性可视化结果。第一列是输入的HOI视频，最后一列是真实标注。中间列展示了VideoAfford与基线方法（IAGNet, GREAT）的预测结果对比。可视化显示，本文方法能产生更完整、空间上更连贯的可承受性区域预测，更接近真实标注，而基线方法的预测往往更加零散或不准确。\n\n**消融实验总结**：动作编码器通过引入动态交互先验，提升了模型对动作意图的理解；空间感知损失函数通过强制模型学习空间连续的目标区域，有效防止了预测结果碎片化，确保了分割目标的空间紧凑性。两者协同工作，是模型取得优越性能的关键。\n\n## 总结与启发\n**核心贡献**：\n1.  **任务与数据集**：首次系统性地提出了“从HOI视频中定位3D物体可承受性”的新任务，并构建并开源了大规模基准数据集VIDA，填补了该领域缺乏动态交互视频数据的空白。\n2.  **方法**：提出了VideoAfford这一强基线方法，创新性地将视频MLLM的世界知识、专用动作编码器提取的动态先验，以及针对3D空间特性设计的损失函数相结合，实现了动态视频信息向3D可承受性知识的有效迁移与推理。\n3.  **性能**：通过大量实验验证了所提方法的优越性，其在标准设置和开放词汇泛化（不可见类别）设置下均显著超越现有先进方法，展示了其实用潜力。\n\n**局限性**：论文自身提到，当前方法依赖于视频-点云配对数据进行训练。虽然训练时视频和点云不需要严格的一对一配对，但这种数据对的收集和标注仍然具有一定成本。此外，方法整合了多个大型预训练模型，在计算效率方面可能存在优化空间。\n\n**对后续研究的启示**：\n1.  **数据利用**：探索如何更高效地利用大量未标注或弱标注的互联网HOI视频，减少对精确配对标注的依赖。\n2.  **模型效率**：研究更轻量化的架构或训练策略，以降低VideoAfford类模型的计算和部署成本，使其更适合机器人等嵌入式平台。\n3.  **因果推理**：HOI视频包含丰富的时序和因果信息（接触前、接触中、接触后）。未来工作可以深入探索如何显式地建模这些因果线索，以进一步提升可承受性推理的深度和可解释性。\n4.  **任务扩展**：本框架展示了利用MLLM和动态先验处理3D空间理解任务的潜力，可启发将其应用于其他需要结合动态观察与静态几何理解的具身AI任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09638v1/x1.png",
        "https://arxiv.org/html/2602.09638v1/x2.png",
        "https://arxiv.org/html/2602.09638v1/x3.png",
        "https://arxiv.org/html/2602.09638v1/x4.png",
        "https://arxiv.org/html/2602.09638v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09583",
      "title": "Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation",
      "url": "http://arxiv.org/abs/2602.09583",
      "arxivId": "2602.09583",
      "date": "2026-02-10",
      "authors": "Danica Kragic Team",
      "category": "Manipulation",
      "summary": "本文研究如何让机器人在操作可变形物体（如布料）时适应人类个性化偏好。针对偏好难以量化且演示数据有限的问题，作者提出了RKO方法，该方法融合了RPO和KPO框架的优势，能够高效地对预训练的视觉运动扩散策略进行偏好对齐微调。在真实布料折叠任务上的实验表明，采用RKO等偏好对齐策略相比标准微调方法，在任务性能和样本效率上均表现出显著优越性。",
      "detailedSummary": "## 研究背景与动机\n在机器人日益融入日常生活的背景下，对能够反映个体用户偏好的自适应行为的需求日益增长。变形物体操作（DOM），如衣物折叠，是一个特别相关但研究不足的领域。此类任务中的用户偏好（如不同的折叠风格）通常是微妙、个人化且难以用语言描述的，使得物理演示成为表达偏好的自然方式。然而，收集演示数据成本高昂，因此需要样本高效的适应框架。\n\n当前，数据驱动的学习方法，特别是视觉运动扩散策略，已在大规模演示数据集上展现出强大的泛化能力。然而，如何使这些预训练模型适应并反映用户特定的操作偏好，同时避免灾难性遗忘且不依赖大量新演示，仍是一个挑战。尽管偏好优化技术（如DPO、RPO、KTO）在文本到图像生成等领域取得成功，但它们在机器人DOM中的应用仍然有限。\n\n本文针对上述痛点，研究如何利用有限的演示，将预训练的视觉运动扩散策略与用户偏好的行为对齐。核心思路是：提出一种名为RKO的新型偏好对齐方法，它结合了KTO的样本效率与RPO的上下文感知加权优点，旨在以更少的演示数据实现更优的策略对齐。\n\n## 方法详解\n整体框架基于一个预训练的参考扩散策略，该策略在大量无偏好演示数据集 $D_{\\text{ref}}$ 上训练，代表执行任务的默认或中性策略。为了将其与用户偏好对齐，引入一个新的偏好数据集 $D_{\\text{pref}}$，其中包含“获胜”（偏好）演示和“失败”（非偏好）演示。然后，使用偏好损失函数对策略 $\\pi_{\\theta}$ 进行微调，通过显式对比偏好与非偏好行为，引导策略学习偏好行为，同时抑制非偏好行为。这种方法比仅在获胜演示上训练扩散策略更有效和样本高效。\n\n![方法框架](https://arxiv.org/html/2602.09583v1/imgs/method_reb.png)\n> **图2**：本文采用的通用偏好对齐框架。首先在大型参考演示集 $D_{\\text{ref}}$ 上训练参考模型。为了将其与用户偏好策略对齐，收集新的获胜演示，并与失败演示（来自 $D_{\\text{ref}}$ 或其他来源）组合成 $D_{\\text{pref}}$。偏好损失通过对比偏好与非偏好行为来对齐新策略 $\\pi_{\\theta}$。\n\n**核心模块与技术细节**：\n1.  **视觉运动扩散模型基础**：采用DDPM框架，将扩散过程应用于动作序列 $A_t$，并通过以观测 $O_t$ 为条件的分数网络 $\\epsilon_{\\theta}$ 建模条件分布 $p(A_t | O_t)$。训练损失为预测噪声的MSE损失：$\\mathcal{L}_{DDPM}(\\theta)=MSE(\\epsilon_k, \\epsilon_{\\theta}(O_t, A_t^0+\\epsilon_k, k))$。\n\n2.  **偏好对齐框架**：\n    *   **Diffusion-DPO**：直接对比偏好与非偏好行为，避免奖励建模和强化学习。其损失函数衡量当前策略 $\\epsilon_{\\theta}$ 相对于参考策略 $\\epsilon_{\\text{ref}}$，在处理偏好与非偏好演示时的相对优势。\n    *   **Diffusion-RPO**：扩展DPO，引入基于语义相似性的上下文重加权方案。每个批次内的每个获胜样本会与所有失败样本进行对比，相似度高的配对获得更大权重（公式5），使模型更关注语义上相近的困难样本对。\n    *   **Diffusion-KTO**：仅需每个样本的二元（获胜/失败）标签，无需配对比较。它通过计算每个样本策略与参考策略的偏差，并利用Sigmoid效用函数进行对齐。\n    *   **Diffusion-RKO (本文提出)**：结合KTO与RPO的优势。它像KTO一样使用二元标签，同时集成RPO的基于相似性的批次重加权，以强调困难负样本和模糊的获胜样本。具体为：首先计算批次中获胜与失败样本嵌入的相似度矩阵 $\\omega_{i,j}$；然后为每个样本计算一个权重标量 $s_b$（对于获胜样本，权重为 $1 + \\max_j \\omega_{i,j}$，强调靠近决策边界的样本；对于失败样本，权重为 $\\sum_i \\omega_{i,j}$，强调被许多获胜样本包围的困难负样本）；最后使用这些权重对KTO损失进行加权：$\\mathcal{L}_{\\text{Diffusion-RKO}}(\\theta)=-\\frac{1}{\\sum_{b=1}^{B}s_b}\\sum_{b=1}^{B}s_b\\cdot\\sigma\\left(q_b\\cdot A_b\\right)$。\n\n**创新点**：本文的核心创新在于提出了RKO方法，它首次将KTO的单一样本二元反馈效率与RPO的上下文感知对比加权机制相结合。这种结合使得模型能够更聚焦于学习特征空间中偏好与非偏好行为重叠的“困难”区域，从而在理论上和实验上实现了更高效的偏好对齐。\n\n## 实验与结果\n**实验设置**：\n*   **任务与数据集**：在真实机器人上进行布料折叠实验，涉及三种衣物（裤子、袖子、T恤），每种衣物对应三种不同的折叠偏好设置（pref_1, pref_2, pref_3），如图3所示。采用循环数据集构建策略，确保每个偏好评估时，获胜、失败演示和参考模型训练数据来自不同的偏好源。\n*   **评估指标**：采用逐步评分方案，根据拾取和放置动作的正确性打分（图3），总分归一化为1。这比二元成功/失败提供了更丰富的信号。\n*   **对比方法**：Diffusion-DPO, Diffusion-RPO, Diffusion-KTO, 提出的Diffusion-RKO，以及仅在偏好演示上训练的普通DDPM基线。\n*   **实验平台**：两个UFactory Lite6机器人，配备三个RealSense D435相机（鸟瞰图和腕部）。\n\n![任务图示](https://arxiv.org/html/2602.09583v1/imgs/fold_tasks.png)\n> **图3**：三种衣物类型各自的三种折叠偏好图示。每个子图显示了左臂（橙色）和右臂（浅蓝色）的拾取（圆圈）和放置（菱形）位置。底部分数已归一化，完整正确折叠序列的总分为1。\n\n![实验设置](https://arxiv.org/html/2602.09583v1/imgs/robot_setup.png)\n> **图4**：实验装置实物图。\n\n**关键实验结果**：\n![结果对比](https://arxiv.org/html/2602.09583v1/imgs/trousers_vs_sleeves_halfpage.png)\n> **图5**：裤子和袖子折叠任务的性能对比结果。左图显示在固定60个获胜演示下，各方法在所有偏好上的平均成功率。右图显示了样本效率曲线，即随着获胜演示数量从20增加到95，平均成功率的变化。\n\n1.  **性能对比**：在固定使用60个获胜演示进行训练时，RKO在裤子和袖子任务上的平均表现最佳（裤子：~0.72成功率；袖子：~0.68成功率），显著优于普通DDPM基线（裤子：~0.58；袖子：~0.52）和其他偏好优化方法（DPO, RPO, KTO）。\n2.  **样本效率**：随着训练演示数量增加，所有方法性能均提升。RKO在几乎所有数据规模下都保持领先，尤其是在数据较少时（如20个演示）优势更明显，证明了其卓越的样本效率。\n3.  **消融实验**：对RKO进行消融实验（将相似性重加权因子设为1，即退化为未加权的KTO），结果显示加权版本性能更好，验证了相似性重加权机制的有效性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**RKO**，一种新颖的偏好对齐方法，巧妙结合了KTO的样本效率与RPO的上下文感知对比加权机制。\n2.  首次在变形物体操作（DOM）领域，对多种偏好优化框架（DPO、RPO、KTO、RKO）在对齐预训练扩散策略方面的性能进行了系统性的实证比较。\n3.  在三种真实衣物折叠任务上进行了广泛评估，证明了基于偏好的对齐方法（尤其是RKO）相比普通扩散策略微调，在性能和样本效率上的优越性。\n\n**局限性**：论文提到，其实验评估集中于布料折叠任务，虽然涵盖了不同衣物类型和偏好，但方法的通用性在其他DOM任务（如辅助穿衣）或更广泛的机器人操作中仍需进一步验证。\n\n**启示**：这项工作凸显了结构化偏好学习在扩展复杂变形物体操作任务中个性化机器人行为方面的重要性和可行性。RKO的成功表明，结合不同偏好优化范式的优势是提升对齐效率的有效途径。它为未来研究指明了方向：如何将此类方法更广泛地应用于多样化的机器人技能个性化，以及如何进一步降低对高质量人类演示数据的依赖。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09583v1/imgs/first_page.png",
        "https://arxiv.org/html/2602.09583v1/imgs/method_reb.png",
        "https://arxiv.org/html/2602.09583v1/imgs/fold_tasks.png",
        "https://arxiv.org/html/2602.09583v1/imgs/robot_setup.png",
        "https://arxiv.org/html/2602.09583v1/imgs/trousers_vs_sleeves_halfpage.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09153",
      "title": "SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes",
      "url": "http://arxiv.org/abs/2602.09153",
      "arxivId": "2602.09153",
      "date": "2026-02-09",
      "authors": "Russ Tedrake Team",
      "category": "Manipulation",
      "summary": "本文针对机器人仿真训练中室内场景过于简化、缺乏真实物理复杂性的问题，提出了SceneSmith框架。该框架采用分层智能体架构，通过设计师、评论家和协调员等VLM智能体，分阶段从语言提示生成仿真就绪的3D场景，并整合文本到3D合成与物理属性估计。实验表明，其生成场景的物体数量是基线方法的3-6倍，物体间碰撞率低于2%，物理稳定性达96%，在用户研究中真实性与提示忠实度胜率均超过90%。",
      "detailedSummary": "## 研究背景与动机\n仿真已成为大规模训练和评估家庭机器人的关键工具，但现有环境无法捕捉真实室内空间的多样性和物理复杂性。当前的场景合成方法生成的是家具稀疏的房间，缺乏机器人操作所必需的密集杂物、可动家具和物理属性。现有的室内场景合成方法，无论是程序化、数据驱动还是基于大语言模型（LLM）的，主要针对家具级别的布局和视觉真实性，将小物体、可动资产和物理属性视为次要。这与机器人需求错位，机器人需要密集的可操作物体、层次化的支撑关系和物理上有效的配置。\n\n本文针对上述痛点，提出了一种新的视角：将场景生成视为一个由智能体驱动的、层次化的决策过程，并紧密集成资产生成以确保仿真就绪。本文的核心思路是：引入一个名为SceneSmith的层次化、智能体化框架，通过设计者、评论者和协调者三个视觉语言模型（VLM）智能体之间的交互，分阶段（从建筑布局到家具摆放再到小物体填充）构建可直接用于物理仿真的室内环境。\n\n## 方法详解\nSceneSmith的整体框架是一个层次化的场景构建流水线。输入是自然语言提示𝒯，输出是可直接仿真的完整室内场景𝒮。构建过程像一棵树：根节点生成建筑布局（房间数M及各房间几何结构𝒢_j）；每个房间作为独立分支，依次进行家具、壁挂物、天花板灯具的放置（由房间特定提示𝒯_j引导）；随后，选中的支撑实体（如桌面、书架、地板区域）会衍生出新的分支，用于填充小的可操作物体（由实体特定提示𝒯_{j,k}引导）。所有分支最终汇合成扁平的场景表示。\n\n![层次化场景构建流水线](https://arxiv.org/html/2602.09153v1/x2.png)\n> **图2**：SceneSmith的层次化场景构建流水线。场景提示𝒯由布局智能体处理，生成M个房间的建筑几何。每个房间随后通过家具、壁挂和天花板安装阶段独立填充，使用房间特定提示𝒯_j。在每个房间内，K_j个支撑实体随后形成额外的分支，使用实体特定提示𝒯_{j,k}填充可操作物体。彩色高亮表示每个阶段添加的物体。每个阶段（彩色三角形）通过设计者、评论者和协调者之间的智能体交互实现。堆叠的框架表示并行分支。\n\n核心模块是贯穿每个构建阶段的“智能体三元组”：设计者（Designer）、评论者（Critic）和协调者（Orchestrator）。设计者使用结构化工具（如放置资产、调整姿态、专用对齐工具）提出对当前场景状态的修改。评论者使用观察和验证工具（如查询元数据、渲染视图、检查碰撞）评估结果场景的语义合理性、物理可行性与阶段目标的一致性，并提供分数和自然语言反馈。协调者管理交互流程，跟踪分数，决定是接受提案、请求进一步细化还是终止阶段，并维护检查点以防止迭代细化过程中的性能退化。这种分离降低了自我评估偏差。\n\n智能体通过工具与场景交互。工具分为功能类别：状态观察、视觉观察、场景修改、资产获取和可行性验证。物体放置相对于支撑表面在SE(2)中进行，然后通过支撑表面的已知姿态提升为完整的SE(3)姿态。此外，不同阶段暴露专用工具，例如家具有对齐工具，可操作物体填充阶段有将多个资产组装成复合组的工具。\n\n资产生成与路由是另一个核心模块，确保开放词汇和仿真就绪。当设计者请求资产时，资产路由器将请求分解并选择合适的获取策略。对于静态物体，主要使用生成式文本到3D合成：根据描述生成参考图像，分割前景，重建纹理3D网格，然后规范化方向、缩放到目标尺寸，并添加碰撞几何和估计的物理属性。对于可动物体（如橱柜），从ArtVIP等关节物体库中检索。对于地毯、海报等扁平装饰元素，则创建带有物理材质属性的“薄覆盖物”。所有资产都经过完整性检查和基于VLM的语义验证。\n\n![文本到3D资产生成流水线](https://arxiv.org/html/2602.09153v1/x3.png)\n> **图3**：文本到3D资产生成流水线。给定物体描述，我们生成图像，分割前景，并重建纹理3D网格。网格被增强以包含碰撞几何（灰色凸块）和由VLM估计的物理属性，包括质量、质心、摩擦力和惯性（蓝色椭球体）。网格也被缩放到目标尺寸。\n\n为确保仿真就绪，在生成后应用轻量级后处理：首先通过非线性优化解决物体间穿透问题，将物体位置投影到最近的无碰撞配置；然后在Drake仿真器中模拟重力，让不稳定的物体沉降到静态稳定配置。\n\n与现有方法相比，SceneSmith的创新点具体体现在：1）提出了一个层次化的智能体交互范式（设计者-评论者-协调者），将决策、评估和控制流分离，支持结构化迭代优化；2）紧密集成了按需生成资产（静态）、检索（可动）和估计物理属性的流水线，实现了开放词汇和物理真实性；3）采用了层次化的提示细化机制，使局部决策与全局意图保持一致，并支持跨表面的协调放置。\n\n## 实验与结果\n实验使用了210个多样化的提示，涵盖五个类别：SceneEval-100房间提示、类型多样性提示（如宠物店、瑜伽室）、物体密度提示、主题场景和房屋级别多房间提示。对比了五个外部基线方法：HSM、Holodeck、I-Design、LayoutVLM（使用其原始资源库和Objaverse库）和SceneWeaver。此外，还进行了六项消融实验：无评论者、非生成资产、无资产验证、无专用工具、无场景观察、无智能体记忆。评估包括205名参与者的人类研究（3051份有效回复）和自动评估。自动评估使用SceneEval指标（CNT物体计数，ATR物体属性，OOR物体间关系，OAR物体与建筑关系，ACC可访问性，NAV可导航性，OOB越界），并增加了两个物理指标：COL碰撞率和STB静态平衡率。\n\n![与HSM和Holodeck的定性对比](https://arxiv.org/html/2602.09153v1/x4.png)\n> **图4**：与HSM和Holodeck的定性对比，这是用户研究中最强的两个基线。SceneSmith产生了更密集且更好地满足提示要求的场景。\n\n关键实验结果如下：在人类研究中，SceneSmith在房间级别提示上，相对于所有基线，平均真实感胜率为92.2%，平均提示忠实度胜率为91.5%（所有p<0.001）。在房屋级别提示上，相对于唯一的多房间基线Holodeck，真实感胜率为80.3%，忠实度胜率为84.7%。SceneSmith平均每个房间生成71.1个物体，是基线方法（11-23个）的3-6倍。在物理指标上，SceneSmith的物体间碰撞率低于2%（1.2%），95.6%的物体在物理仿真下保持稳定；而基线方法的碰撞率在3-29%之间，稳定性在8-61%之间。自动评估指标（表2）显示，SceneSmith在CNT、ATR、OOR、OAR、STB上表现最佳或接近最佳，在COL和OOB上表现优异，但在ACC和NAV上略低于某些优化了这些指标的基线。\n\n消融实验总结：**资产生成**（NotGenerated）对真实感和忠实度影响最大（胜率分别降至63.8%和67.0%），表明按需生成资产至关重要。**资产验证**（NoAssetValidation）和**视觉观察**（NoObserveScene）对质量有显著负面影响。**专用工具**（NoSpecializedTools）、**智能体记忆**（NoAgentMemory）和**评论者**（NoCritic）的缺失会导致性能下降，但在统计上不显著，表明基础智能体交互具有一定鲁棒性，但这些组件共同贡献了最佳性能。\n\n## 总结与启发\n本文的核心贡献包括：1）提出了SceneSmith，一个用于从自然语言构建仿真就绪室内环境的层次化、智能体化框架；2）开发了一个集成的资产生成和路由流水线，结合了文本到3D合成与可动物体检索，并为所有资产增强了碰撞几何和物理属性；3）展示了SceneSmith在用户研究和自动指标上均优于基线，能生成更密集、无碰撞且物理稳定的场景，并演示了其在从自然语言任务描述到自动成功验证的端到端机器人策略评估流水线中的应用。\n\n论文提到的局限性包括：其评估的VLM自动指标存在假阳性和假阴性；资产生成依赖于当前的文本到3D和图像分割模型，可能产生几何伪影；虽然支持多房间生成，但尚未探索房间间的高级关系（如声音传播）；评估器代理的任务验证是非确定性的。\n\n本工作对后续研究的启示在于：为大规模机器人仿真提供了一种高质量、可扩展的场景生成方法；展示了智能体分工协作（设计-评论-协调）在复杂空间推理任务中的有效性；将生成式资产创建与物理仿真需求紧密结合，推动了“仿真就绪”内容生成的发展方向。未来的工作可以探索更复杂的跨房间关系、更鲁棒的资产生成技术，以及将此类生成环境直接用于机器人策略训练。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09153v1/x1.png",
        "https://arxiv.org/html/2602.09153v1/x2.png",
        "https://arxiv.org/html/2602.09153v1/x3.png",
        "https://arxiv.org/html/2602.09153v1/x4.png",
        "https://arxiv.org/html/2602.09153v1/x5.png",
        "https://arxiv.org/html/2602.09153v1/x6.png",
        "https://arxiv.org/html/2602.09153v1/x7.png",
        "https://arxiv.org/html/2602.09153v1/x8.png",
        "https://arxiv.org/html/2602.09153v1/x9.png",
        "https://arxiv.org/html/2602.09153v1/x10.png",
        "https://arxiv.org/html/2602.09153v1/x11.png",
        "https://arxiv.org/html/2602.09153v1/x12.png",
        "https://arxiv.org/html/2602.09153v1/x13.png",
        "https://arxiv.org/html/2602.09153v1/x14.png",
        "https://arxiv.org/html/2602.09153v1/x15.png",
        "https://arxiv.org/html/2602.09153v1/x16.png",
        "https://arxiv.org/html/2602.09153v1/x17.png",
        "https://arxiv.org/html/2602.09153v1/x18.png",
        "https://arxiv.org/html/2602.09153v1/x19.png",
        "https://arxiv.org/html/2602.09153v1/x20.png",
        "https://arxiv.org/html/2602.09153v1/x21.png",
        "https://arxiv.org/html/2602.09153v1/x22.png",
        "https://arxiv.org/html/2602.09153v1/x23.png",
        "https://arxiv.org/html/2602.09153v1/x24.png",
        "https://arxiv.org/html/2602.09153v1/x25.png",
        "https://arxiv.org/html/2602.09153v1/x26.png",
        "https://arxiv.org/html/2602.09153v1/x27.png",
        "https://arxiv.org/html/2602.09153v1/x28.png",
        "https://arxiv.org/html/2602.09153v1/x29.png",
        "https://arxiv.org/html/2602.09153v1/x30.png",
        "https://arxiv.org/html/2602.09153v1/x31.png",
        "https://arxiv.org/html/2602.09153v1/x32.png",
        "https://arxiv.org/html/2602.09153v1/x33.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09023",
      "title": "TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.09023",
      "arxivId": "2602.09023",
      "date": "2026-02-09",
      "authors": "Shanghang Zhang Team",
      "category": "Manipulation",
      "summary": "论文提出了一种名为TwinRL-VLA的数字孪生驱动强化学习框架，旨在解决现实世界中机器人操作任务中由于高昂的专家演示成本和不足的实际交互导致的Vision-Language-Action (VLA)模型泛化能力受限的问题。该方法通过智能手机捕捉场景高效重建高保真度的数字孪生环境，实现真实与模拟环境之间的双向传输。在监督微调（SFT）预热阶段，利用数字孪生扩展探索空间，增强数据轨迹分布的支持。基于此初始化，进一步提出了从模拟到现实的引导探索策略，显著提升了VLA模型在实际操作中的性能。实验结果表明，TwinRL-VLA有效提高了在线强化学习的探索效率和探索空间。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型在机器人操作任务中取得了显著进展，但这些模型仍然依赖于昂贵的专家演示和有限的真实世界交互。尽管在线强化学习（RL）在改进基础模型方面显示出潜力，但在真实世界的机器人操作中应用RL仍受到低探索效率和受限探索空间的限制。本文针对这一痛点，提出了TwinRL-VLA框架，通过数字孪生技术来扩展和引导VLA模型的探索空间。本文的核心思路是利用高保真度的数字孪生环境进行高效的并行在线RL，并通过数字孪生指导真实机器人的目标性探索。\n\n## 方法详解\n### 整体框架\nTwinRL-VLA的整体框架分为两个主要阶段：SFT预热阶段和在线RL阶段。在SFT预热阶段，通过数字孪生生成多样化的合成轨迹以扩展探索空间。在在线RL阶段，首先在数字孪生环境中进行高效的并行RL滚动，然后将这些数据用于真实世界的在线RL，从而加速训练过程。\n\n![方法框架](https://arxiv.org/html/2602.09023v1/x1.png)\n> **图1**：方法整体框架。左侧为SFT预热阶段，右侧为在线RL阶段。数字孪生环境用于生成多样化的合成轨迹，并在在线RL阶段提供高效的并行滚动。\n\n### 核心模块\n1. **数字孪生重建**：使用智能手机捕捉的场景视频，通过标准的3D Gaussian Splatting技术重建高保真的数字孪生环境。\n2. **探索空间扩展策略**：在SFT预热阶段，通过数字孪生生成多样化的合成轨迹，以扩展探索覆盖范围。\n3. **Sim-to-Real引导探索策略**：在数字孪生环境中进行高效的并行RL滚动，生成在线交互数据存储在重放缓冲区中。这些数据用于真实世界的在线RL，丰富了重放缓冲区，并减少了从离线到在线学习的过渡性能下降。\n\n### 创新点\n- 通过数字孪生技术生成多样化的合成轨迹，扩展了探索空间。\n- 在数字孪生环境中进行高效的并行在线RL，加速了真实世界的在线RL过程。\n- 通过数字孪生识别易失败但信息丰富的配置，指导真实机器人的目标性探索。\n\n## 实验与结果\n### 实验设置\n- **Benchmark/数据集**：四个不同的机器人操作任务，包括抓取、放置等。\n- **Baseline方法**：仅使用真实世界演示的SFT方法、现有的在线RL方法（如[8, 39]）。\n\n### 关键实验结果\n- **SFT预热阶段**：TwinRL-VLA在SFT预热阶段的平均成功率提高了42%。\n- **在线RL阶段**：TwinRL-VLA在分布内和分布外区域均达到了100%的成功率，相比现有方法至少提升了30%的速度，平均每个任务只需要约20分钟。\n\n![SFT预热阶段成功率对比](https://arxiv.org/html/2602.09023v1/x2.png)\n> **图2**：SFT预热阶段的成功率对比。TwinRL-VLA在SFT预热阶段的平均成功率提高了42%。\n\n![在线RL阶段成功率对比](https://arxiv.org/html/2602.09023v1/x3.png)\n> **图3**：在线RL阶段的成功率对比。TwinRL-VLA在分布内和分布外区域均达到了100%的成功率。\n\n### 消融实验\n- **数字孪生的作用**：去除数字孪生后，SFT预热阶段的成功率降低了30%，在线RL阶段的成功率降低了20%。\n- **Sim-to-Real引导探索策略**：去除该策略后，在线RL阶段的成功率降低了15%。\n\n## 总结与启发\n### 核心贡献\n1. 提出了TwinRL-VLA框架，通过数字孪生技术扩展和引导VLA模型的探索空间。\n2. 引入了探索空间扩展策略，通过数字孪生生成多样化的合成轨迹，扩展了探索覆盖范围。\n3. 提出了Sim-to-Real引导探索策略，通过数字孪生中的高效并行在线RL加速真实世界的在线RL过程。\n\n### 局限性\n- 数字孪生的重建需要高质量的输入数据，这可能在某些复杂环境中难以实现。\n- Sim-to-Real引导探索策略对数字孪生的准确性有一定依赖，如果数字孪生与真实环境存在较大差异，可能会导致性能下降。\n\n### 后续研究启示\n- 进一步优化数字孪生的重建过程，提高其在复杂环境中的适用性。\n- 探索更多有效的Sim-to-Real引导探索策略，进一步提升在线RL的效率和鲁棒性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09023v1/x1.png",
        "https://arxiv.org/html/2602.09023v1/x2.png",
        "https://arxiv.org/html/2602.09023v1/x3.png",
        "https://arxiv.org/html/2602.09023v1/x4.png",
        "https://arxiv.org/html/2602.09023v1/x5.png",
        "https://arxiv.org/html/2602.09023v1/x6.png",
        "https://arxiv.org/html/2602.09023v1/x7.png",
        "https://arxiv.org/html/2602.09023v1/x8.png",
        "https://arxiv.org/html/2602.09023v1/x9.png",
        "https://arxiv.org/html/2602.09023v1/x10.png",
        "https://arxiv.org/html/2602.09023v1/x11.png",
        "https://arxiv.org/html/2602.09023v1/x12.png",
        "https://arxiv.org/html/2602.09023v1/x13.png",
        "https://arxiv.org/html/2602.09023v1/x14.png",
        "https://arxiv.org/html/2602.09023v1/x15.png",
        "https://arxiv.org/html/2602.09023v1/x16.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09021",
      "title": "$χ_{0}$ : Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
      "url": "http://arxiv.org/abs/2602.09021",
      "arxivId": "2602.09021",
      "date": "2026-02-09",
      "authors": "Yibo Yuan Team",
      "category": "Manipulation",
      "summary": "本文针对高可靠性长时机器人操作中的分布不一致性问题，提出了一种资源高效的框架χ₀。该框架通过三个关键技术解决这一问题：（i）模型算术，一种权重空间合并策略，有效吸收不同演示的多样化分布；（ii）阶段优势，一种阶段感知的优势估计器，提供稳定、密集的进度信号；（iii）训练-部署对齐，通过时空增强、启发式DAgger校正和时间块平滑来弥合分布差距。实验表明，该方法使双臂机器人能够协同完成从展平、折叠到挂起不同衣物的长时任务，并表现出高可靠性的自主操作能力。",
      "detailedSummary": "## 研究背景与动机\n目前，高可靠性长时序机器人操作主要依赖大规模数据和计算资源来理解复杂的现实世界动态。然而，本文指出，阻碍现实世界鲁棒性的主要瓶颈不仅仅是资源规模，而是人类演示分布、策略学习的归纳偏差以及测试时执行分布之间的系统性不一致。这种不一致性会导致多阶段任务中的累积误差。针对这一痛点，本文提出了χ₀框架，通过有效模块实现生产级的机器人操作鲁棒性。\n\n## 方法详解\n### 整体框架\nχ₀框架旨在解决三个阶段的分布不一致性：训练分布（P_train）、模型分布（Q_model）和测试分布（P_test）。整体框架如图1所示。\n![方法框架](https://arxiv.org/html/2602.09021v1/x1.png)\n> **图1**：χ₀框架的整体结构。左侧为P_train，通过启发式DAgger和时空增强扩展训练覆盖范围，并进行阶段标注以估计优势；中间为Q_model，通过模型算术在权重空间中合并互补策略，由阶段感知的优势引导；右侧为P_test，通过时间块平滑确保执行准确性，同时通过在线DAgger实现闭环优化。\n\n### 核心模块\n#### 模型算术 (Model Arithmetic, MA)\nMA通过在权重空间中合并不同P_train分布训练的检查点，使策略能够高效吸收多样化的P_train分布。具体来说，MA通过合并多个模型的权重，使得Q_model能够捕捉到之前被忽略的P_train模式。实验表明，验证损失可以作为有效的权重选择启发式方法。\n\n#### 阶段优势 (Stage Advantage, SA)\nSA将长时序任务分解为语义子目标（阶段），提供稳定的阶段感知奖励信号，用于优势加权行为克隆。SA通过帧级奖励建模，解决了先前非阶段方法（如π₀.₆*）的数值不稳定问题。具体来说，SA通过直接从配对观察中预测优势，并将其条件化在语义阶段上，从而生成更平滑和稳定的监督信号。\n\n#### 训练-部署对齐 (Train-Deploy Alignment, TDA)\nTDA通过启发式DAgger和时空增强扩展P_train，使其更接近P_test，从而提高对实际分布漂移的鲁棒性。此外，TDA还提出了一种时间块平滑方法，以减轻推理-执行延迟并增强实时控制稳定性。实验表明，时空增强只有在与控制优化结合时才有效，而时间块平滑则与RTC方法正交。\n\n### 创新点\n- 通过模型算术在权重空间中合并不同P_train分布训练的检查点，提高了策略的泛化能力。\n- 通过阶段优势提供了稳定的阶段感知奖励信号，克服了数值不稳定问题。\n- 通过训练-部署对齐扩展了P_train，使其更接近P_test，提高了对实际分布漂移的鲁棒性。\n\n## 实验与结果\n### 数据集与实验平台\n本文在协作长时序衣物操作任务上进行了评估，包括展平、折叠和挂起不同衣物。这些任务涉及接触丰富的可变形动力学，放大了上述分布漂移。\n\n### 对比方法\n本文与开源基线π₀.₅进行了对比。\n\n### 关键实验结果\n- 在仅使用20小时的演示数据和8个A100 GPU的情况下，χ₀的成功率比π₀.₅提高了近250%。\n- MA提供了资源高效的机制，提升了几乎所有指标的性能。\n- TDA中的DAgger数据对于最大化成功率至关重要，尽管会增加重试成本。\n- SA的两步优势信号在数值稳定性方面优于π₀.₆*风格的优势训练，这在实验中转化为更好的整体性能。\n\n### 实验结果图表\n#### 成功率对比\n![成功率对比](https://arxiv.org/html/2602.09021v1/x3.png)\n> **图3**：χ₀与π₀.₅在成功率上的对比。χ₀在所有任务上的成功率显著高于π₀.₅。\n\n#### 重试成本对比\n![重试成本对比](https://arxiv.org/html/2602.09021v1/x4.png)\n> **图4**：χ₀与π₀.₅在重试成本上的对比。虽然χ₀的成功率更高，但其重试成本也相应增加。\n\n#### 消融实验\n![消融实验](https://arxiv.org/html/2602.09021v1/x5.png)\n> **图5**：消融实验结果。展示了每个组件对整体性能的贡献，其中MA和TDA对提升成功率最为关键。\n\n## 总结与启发\n### 核心贡献\n1. 提出了一个资源高效的框架χ₀，通过模型算术、阶段优势和训练-部署对齐，系统地解决了机器人操作中的分布不一致性问题。\n2. 实验结果表明，χ₀在成功率达到显著提升的同时，保持了良好的鲁棒性和稳定性。\n\n### 局限性\n论文提到，虽然χ₀在成功率上有显著提升，但重试成本也相应增加。此外，时空增强的有效性依赖于与控制优化的结合。\n\n### 后续研究启示\n- 进一步研究如何在保持高成功率的同时降低重试成本。\n- 探索更多有效的时空增强方法，以进一步提高对实际分布漂移的鲁棒性。\n- 将χ₀框架应用于更多类型的机器人操作任务，验证其通用性和有效性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09021v1/x1.png",
        "https://arxiv.org/html/2602.09021v1/x2.png",
        "https://arxiv.org/html/2602.09021v1/x3.png",
        "https://arxiv.org/html/2602.09021v1/x4.png",
        "https://arxiv.org/html/2602.09021v1/x5.png",
        "https://arxiv.org/html/2602.09021v1/x6.png",
        "https://arxiv.org/html/2602.09021v1/x7.png",
        "https://arxiv.org/html/2602.09021v1/x8.png",
        "https://arxiv.org/html/2602.09021v1/x9.png",
        "https://arxiv.org/html/2602.09021v1/x10.png",
        "https://arxiv.org/html/2602.09021v1/x11.png",
        "https://arxiv.org/html/2602.09021v1/figures/supp/loss_curve_SA.jpeg",
        "https://arxiv.org/html/2602.09021v1/x12.png",
        "https://arxiv.org/html/2602.09021v1/x13.png",
        "https://arxiv.org/html/2602.09021v1/x14.png",
        "https://arxiv.org/html/2602.09021v1/x15.png",
        "https://arxiv.org/html/2602.09021v1/x16.png",
        "https://arxiv.org/html/2602.09021v1/x17.png",
        "https://arxiv.org/html/2602.09021v1/x18.png",
        "https://arxiv.org/html/2602.09021v1/x19.png",
        "https://arxiv.org/html/2602.09021v1/x20.png",
        "https://arxiv.org/html/2602.09021v1/x21.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09017",
      "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models",
      "url": "http://arxiv.org/abs/2602.09017",
      "arxivId": "2602.09017",
      "date": "2026-02-09",
      "authors": "Nur Muhammad Mahi Shafiullah Team",
      "category": "Manipulation",
      "summary": "本文提出了一种名为接触锚定策略（Contact-Anchored Policies, CAP）的方法，通过物理接触信息来调节多模态策略。CAP能够在零样本情况下对新对象和场景进行泛化，并且在数据量、计算资源和模型参数方面比前沿行为模型少几个数量级的情况下，仍然在原子技能训练上表现出更好的性能。实验结果表明，CAP方法在多种任务中均优于现有的前沿行为模型。",
      "detailedSummary": "## 研究背景与动机\n目前机器人学习领域的主要范式是通过语言提示在运行时实现跨环境、实体和任务的泛化。然而，这种方法存在一个根本性的矛盾：语言往往过于抽象，难以指导具体物理操作所需的精确理解。本文提出了一种新的方法——接触锚定策略（Contact-Anchored Policies, CAP），用物理接触点代替语言条件，从而提高机器人在新环境和实体中的零样本泛化能力。CAP将策略分解为模块化的效用模型库，而不是单一的通用策略，这使得可以通过轻量级仿真平台EgoGym进行快速迭代和优化。\n\n## 方法详解\n### 整体框架\nCAP的整体框架包括数据收集、接触标注、预处理以及策略学习四个主要阶段。输入是带有接触点标注的视觉观测序列，输出是机器人执行的动作序列。\n\n![方法框架](https://arxiv.org/html/2602.09017v1/x2.png)\n> **图2**：CAP的数据标注、训练和推理过程。 (a) 在训练过程中，从数据中检测接触点并使用后见重标记轨迹。 (b) 在推理过程中，使用用户点击或基于用户命令的VLM来推导接触条件。在这两种情况下，接触令牌和视觉令牌被连接并传递给模型，作为输入预测动作。\n\n### 核心模块\n1. **数据收集与接触标注**：\n   - **硬件设计**：设计了一个低成本、3D打印的夹爪，兼容手持操作和机器人安装。该夹爪配备了一个iPhone 13 Pro作为主要传感器套件。\n   - **数据收集**：收集了三个主要任务的专家演示：Pick（拾取）、Open（打开）和Close（关闭）。最终数据集包含20,365个演示（23.1小时）。\n   - **数据预处理**：将RGB和深度图像调整为224×224，并对RGB-D观测和相应的相机里程计进行水平翻转增强。\n   - **后见接触标注**：定义接触锚点为3D坐标p，其中策略预期与对象交互。通过检测接触时间步、定义接触锚点并传播到所有先前的时间步来生成这些标签。\n\n2. **策略学习**：\n   - 使用Vector-Quantized Behavior Transformer (VQ-BeT) 实现条件模仿学习问题。VQ-BeT是一个两阶段算法，首先找到自监督离散动作表示，然后训练自回归变压器以预测给定观察序列的标记化动作。\n   - 对于每个时间步，将224×224 RGB输入嵌入到特征向量z_v∈ℝ^256中。接触锚点p_t∈ℝ^3线性投影到接触嵌入z_c∈ℝ^256。\n\n### 创新点\n- 用物理接触点代替语言条件，提高了机器人在新环境和实体中的零样本泛化能力。\n- 将策略分解为模块化的效用模型库，允许通过轻量级仿真平台EgoGym进行快速迭代和优化。\n- 通过后见接触标注技术，提高了数据利用率和模型性能。\n\n## 实验与结果\n### 数据集与实验平台\n- **数据集**：收集了20,365个演示（23.1小时）。\n- **仿真平台**：EgoGym，一个轻量级仿真基准，专注于对象和场景多样性，以速度换取逼真度。\n\n### 基准方法\n- 比较了当前最先进的视觉-语言-动作模型（如π_0.5）。\n\n### 关键实验结果\n- CAP在零样本评估中，在完全新颖的场景和对象上优于最先进的通用视觉-语言-动作模型，如π_0.5，提升了56%。\n- CAP仅使用23小时的人类演示数据，就能在三种基本操作技能（拾取、打开和关闭）上实现零样本泛化。\n\n![实验结果对比](https://arxiv.org/html/2602.09017v1/x12.png)\n> **图12**：CAP与基线方法在零样本评估中的成功率对比。CAP在所有任务上均显著优于基线方法。\n\n![消融实验](https://arxiv.org/html/2602.09017v1/x13.png)\n> **图13**：消融实验展示了各个组件的贡献。接触标注和模块化效用模型库对性能提升有显著贡献。\n\n### 消融实验\n- 接触标注：显著提高了模型的泛化能力和成功率。\n- 模块化效用模型库：允许通过轻量级仿真平台进行快速迭代和优化，进一步提高了模型性能。\n\n## 总结与启发\n### 核心贡献\n- 提出了接触锚定策略（CAP），用物理接触点代替语言条件，提高了机器人在新环境和实体中的零样本泛化能力。\n- 将策略分解为模块化的效用模型库，允许通过轻量级仿真平台EgoGym进行快速迭代和优化。\n- 通过后见接触标注技术，提高了数据利用率和模型性能。\n\n### 局限性\n- CAP目前仅在三个基本操作技能（拾取、打开和关闭）上进行了验证，未来需要扩展到更多复杂任务。\n- 轻量级仿真平台EgoGym虽然提高了迭代效率，但可能无法完全模拟真实世界的复杂性。\n\n### 后续研究启示\n- 进一步探索CAP在更多复杂任务上的应用，如多步骤操作和动态环境中的任务。\n- 结合其他感知模态（如触觉）进一步提高机器人的物理理解能力。\n- 优化仿真平台，使其更接近真实世界，以提高模型的泛化能力。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09017v1/x1.png",
        "https://arxiv.org/html/2602.09017v1/x2.png",
        "https://arxiv.org/html/2602.09017v1/x3.png",
        "https://arxiv.org/html/2602.09017v1/x4.png",
        "https://arxiv.org/html/2602.09017v1/figures/fig_4_test.jpg",
        "https://arxiv.org/html/2602.09017v1/x5.png",
        "https://arxiv.org/html/2602.09017v1/x6.png",
        "https://arxiv.org/html/2602.09017v1/x7.png",
        "https://arxiv.org/html/2602.09017v1/x8.png",
        "https://arxiv.org/html/2602.09017v1/x9.png",
        "https://arxiv.org/html/2602.09017v1/x10.png",
        "https://arxiv.org/html/2602.09017v1/x11.png",
        "https://arxiv.org/html/2602.09017v1/figures/robot_starting_poses_images.jpg",
        "https://arxiv.org/html/2602.09017v1/figures/robot_starting_poses_scatter.png",
        "https://arxiv.org/html/2602.09017v1/figures/egogym_picks.jpg",
        "https://arxiv.org/html/2602.09017v1/figures/egogym_open_close.jpg",
        "https://arxiv.org/html/2602.09017v1/x12.png",
        "https://arxiv.org/html/2602.09017v1/x13.png",
        "https://arxiv.org/html/2602.09017v1/figures/cap-pickup-objects.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.08602",
      "title": "Mimic Intent, Not Just Trajectories",
      "url": "http://arxiv.org/abs/2602.08602",
      "arxivId": "2602.08602",
      "date": "2026-02-09",
      "authors": "Panpan Cai Team",
      "category": "Manipulation",
      "summary": "该论文针对模仿学习（IL）在环境变化适应和技能迁移方面的不足，提出了一种新的方法“Mimic Intent, Not just Trajectories”（MINT）。MINT通过多尺度频域分词技术，将行为意图与执行细节分离。具体来说，它使用多尺度粗到细的结构来学习动作标记，其中最粗的标记捕捉低频全局结构，较细的标记编码高频细节。这种方法生成了一个抽象的意图标记，有助于规划和迁移，并生成了多尺度的执行标记，以适应环境动态。实验结果表明，MINT在多个操作基准测试和真实机器人上表现出色，具有更高的成功率、更优的推理效率、更强的抗干扰能力和有效的单次技能迁移能力。",
      "detailedSummary": "## 研究背景与动机\n当前主流的模仿学习（IL）方法，如视觉-语言-动作（VLA）模型，通过生成建模和预训练在灵巧操作任务中取得了显著的成功。然而，这些方法在适应环境变化和技能迁移方面仍存在局限性。本文认为，这些局限性源于仅模仿原始轨迹而未理解其背后的意图。为此，本文提出了一种新的视角：显式地将行为意图从执行细节中解耦。本文的核心思路是通过多尺度频域分词化，实现“模仿意图，而不仅仅是轨迹”（MINT），从而提高学习效率、泛化能力和一次性技能迁移。\n\n## 方法详解\n### 整体框架\nMINT框架包括两个主要部分：（1）频谱解耦动作分词器（SDAT），用于从演示轨迹中学习结构化的离散表示；（2）MINT策略，通过在学习到的分词空间中进行渐进的意图到执行推理来生成动作。SDAT提供了一个共享的动作码本和解码器，而MINT策略则学习以粗到细的方式预测动作分词，并将其解码为可执行的轨迹。\n\n![方法框架](https://arxiv.org/html/2602.08602v1/x2.png)\n> **图2**：MINT策略概述。（a）MINT自回归地预测K个时间尺度上的动作分词——从全局意图分词到高频执行分词——然后通过解码器映射到连续轨迹。（b）基于意图的动作集合确保了时间一致性和平滑的行为过渡，增强了长时任务的稳定性。\n\n### 核心模块\n#### SDAT分词器\nSDAT采用VQ-VAE架构，将动作块映射到分词。具体步骤如下：\n1. **动作编码器**：将输入的动作序列压缩为潜在嵌入。\n2. **频谱解码器**：将潜在嵌入重构为动作序列，并通过DCT转换为频域表示。\n3. **多尺度分解**：将动作分解为K个时间尺度，每个尺度具有逐渐增加的容量。最粗尺度（意图分词）包含一个分词，用于捕捉全局低频结构，而更细的尺度（执行分词）引入额外的分词来建模未被粗尺度解释的残差信息。\n4. **逐步重建**：模型首先使用最粗尺度进行重建，然后逐步使用更细尺度，直到所有K个尺度。这迫使粗尺度分词捕捉主导的低频成分，而细尺度分词专注于高频残差。\n\n#### MINT策略\nMINT策略由视觉-语言骨干和动作专家组成：\n1. **视觉-语言骨干**：编码视觉和语言输入。\n2. **动作专家**：自回归地预测从粗到细尺度的动作分词，并将预测的分词解码为连续轨迹。\n\n### 创新点\nMINT的关键创新在于通过多尺度频域分词化显式地解耦意图和执行细节。这种解耦使得MINT能够更好地适应环境变化，并实现一次性技能迁移。\n\n## 实验与结果\n### 数据集与实验平台\n本文在多个操作基准上进行了实验，包括LIBERO、MetaWorld、CALVIN和更具挑战性的LIBERO-Plus，以及在一个真实机器人系统上进行了测试。\n\n### 基线方法\n对比了多种基线方法，包括预训练的VLA模型（π0.5）、基于动作分词的方法（UniVLA）和经典的模仿学习方法（ACT、Diffusion Policy）。\n\n### 关键实验结果\n- 在标准基准上，MINT达到了最先进的性能，优于强基线方法。\n- 在LIBERO-Plus上，MINT在更强干扰下表现出更高的鲁棒性，成功率达到15%的提升。\n- 通过意图级别的表示，MINT实现了单次演示的一次性技能迁移，性能提高了60%。\n- 在真实机器人实验中，MINT只需要大约20次演示即可达到比最强基线（π0.5）高29%的性能。\n\n![实验设置](https://arxiv.org/html/2602.08602v1/experiment_setup.png)\n> **图3**：实验设置。展示了在不同基准和真实机器人系统上的实验配置。\n\n![实验结果](https://arxiv.org/html/2602.08602v1/exp_ret.png)\n> **图4**：实验结果。展示了MINT在不同基准上的成功率和性能提升。\n\n![消融实验](https://arxiv.org/html/2602.08602v1/x3.png)\n> **图5**：消融实验。展示了各个组件对性能的贡献。\n\n### 消融实验\n- 意图分词的存在显著提高了性能，特别是在长时任务和一次性技能迁移中。\n- 多尺度分解和逐步重建机制对于捕捉意图和执行细节的解耦至关重要。\n\n## 总结与启发\n### 核心贡献\n1. 提出了一种新的模仿学习框架MINT，通过多尺度频域分词化显式地解耦意图和执行细节。\n2. 实现了高效的意图到执行推理过程，提高了学习效率和泛化能力。\n3. 通过意图分词实现了单次演示的一次性技能迁移，显著提升了性能。\n\n### 局限性\n论文提到，尽管MINT在多个基准上表现优异，但在极端环境变化和复杂任务中的泛化能力仍有待进一步研究。\n\n### 后续研究启示\n未来的研究可以进一步探索如何在更复杂的环境中应用MINT，并结合其他先进的技术（如强化学习）来进一步提升性能。此外，还可以研究如何将MINT扩展到更多类型的任务和领域。",
      "imageUrls": [
        "https://arxiv.org/html/2602.08602v1/x1.png",
        "https://arxiv.org/html/2602.08602v1/x2.png",
        "https://arxiv.org/html/2602.08602v1/experiment_setup.png",
        "https://arxiv.org/html/2602.08602v1/exp_ret.png",
        "https://arxiv.org/html/2602.08602v1/x3.png",
        "https://arxiv.org/html/2602.08602v1/x4.png",
        "https://arxiv.org/html/2602.08602v1/x5.png",
        "https://arxiv.org/html/2602.08602v1/x6.png",
        "https://arxiv.org/html/2602.08602v1/x7.png",
        "https://arxiv.org/html/2602.08602v1/x8.png",
        "https://arxiv.org/html/2602.08602v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.08245",
      "title": "STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction",
      "url": "http://arxiv.org/abs/2602.08245",
      "arxivId": "2602.08245",
      "date": "2026-02-09",
      "authors": "Guohao Dai Team",
      "category": "Manipulation",
      "summary": "本文提出了一种名为STEP的轻量级时空一致性预测机制，旨在解决扩散策略在机器人视觉运动控制中因迭代去噪导致的高推理延迟问题。STEP通过生成高质量的初始动作，这些动作既接近目标动作的分布又具有时间一致性，从而在不牺牲原始扩散策略生成能力的前提下减少延迟。此外，还引入了速度感知扰动注入机制，自适应地调整动作激励以防止执行停滞。理论分析表明，该预测方法能诱导局部收缩映射，确保在扩散细化过程中动作误差收敛。实验结果表明，在RoboMimic基准测试和实际任务中，STEP仅用2步即可分别比BRIDGER和DDIM提高21.6%和27.5%的成功率，显著提升了推理延迟和成功率之间的帕累托前沿。",
      "detailedSummary": "## 研究背景与动机\n扩散策略（Diffusion policies）在机器人操作中的视觉-运动控制领域中崭露头角，因其能够建模动作序列的分布并捕捉多模态特性。然而，迭代去噪过程导致了显著的推理延迟，限制了实时闭环系统的控制频率。现有的加速方法包括减少采样步骤、直接预测或重用过去的动作，但这些方法往往难以同时保持动作质量和低延迟。本文针对这一痛点，提出了STEP，一种轻量级的时空一致性预测机制，以构建高质量的预热启动动作，既接近目标动作分布又具有时间一致性，且不损害原始扩散策略的生成能力。\n\n本文的核心思路是通过引入时空一致性预测机制和速度感知扰动注入机制，实现高效且高质量的动作生成，从而在保持动作质量的同时大幅降低推理延迟。\n\n## 方法详解\n### 整体框架\nSTEP的整体框架分为两个主要部分：时空一致性预测机制和速度感知扰动注入机制。输入为当前观测状态（如视觉输入或本体感觉状态），输出为高质量的动作序列。\n\n![方法框架](https://arxiv.org/html/2602.08245v1/x1.png)\n> **图1**：方法整体框架。左侧为时空一致性预测机制，右侧为速度感知扰动注入机制。\n\n### 核心模块\n\n#### 时空一致性预测机制\n该机制旨在生成高质量的预热启动动作，使其既接近目标动作分布又具有时间一致性。具体来说，它通过以下步骤实现：\n1. **时空一致性定义**：给定当前系统状态 \\( s_t \\) 和对应的目标动作分布 \\( p(a \\mid s_t) \\)，预热启动动作 \\( \\tilde{a}_t \\) 满足时空一致性，如果其同时满足时间一致性和空间一致性。\n   - 时间一致性：\\(\\|\\tilde{a}_t - a_{t-1}\\| \\leq \\epsilon_t, \\forall t\\)，其中 \\(\\epsilon_t\\) 是由系统动力学和控制频率决定的Lipschitz-like有界常数。\n   - 空间一致性：\\(\\mathrm{dist}(\\tilde{a}_t, \\mathcal{M}(s_t)) \\leq \\epsilon_s, \\forall t\\)，其中 \\(\\mathcal{M}(s_t)\\) 表示 \\( p(a \\mid s_t) \\) 的高概率动作流形，\\(\\epsilon_s\\) 控制允许的偏差。\n\n2. **预测网络**：使用一个轻量级的神经网络来预测预热启动动作 \\( \\tilde{a}_t \\)，该网络基于当前观测状态 \\( o \\) 和前一时刻的动作 \\( a_{t-1} \\) 进行预测。\n\n#### 速度感知扰动注入机制\n该机制通过自适应调节动作变化来防止执行停滞，特别是在实际任务中。具体来说：\n1. **扰动注入**：根据时间动作变化自适应地引入有界的激励扰动，仅在必要时引入，以防止执行停滞。\n2. **优化策略**：通过最小化条件噪声预测损失进行训练，确保扰动注入不会降低控制精度。\n\n### 创新点\n- **时空一致性预测**：同时考虑时间和空间一致性，生成高质量的预热启动动作。\n- **速度感知扰动注入**：自适应调节动作变化，防止执行停滞，提高控制稳定性。\n- **理论分析**：证明了所提出的方法可以诱导局部收缩映射，确保在后续扩散细化过程中动作误差的收敛。\n\n## 实验与结果\n### 数据集与实验平台\n本文在九个模拟基准测试和两个实际机器人任务上进行了广泛的评估。\n\n### 对比方法\n- BRIDGER\n- DDIM\n- CP\n- OneDP\n- RTI-DP\n- SDP\n- RNR-DP\n- Falcon\n\n### 关键实验结果\n- 在RoboMimic基准测试中，STEP在两步内实现了平均21.6%的成功率提升，超过了BRIDGER方法。\n- 在实际任务中，STEP在两步内实现了平均27.5%的成功率提升，超过了DDIM方法。\n\n![成功率对比](https://arxiv.org/html/2602.08245v1/x2.png)\n> **图2**：不同方法在RoboMimic基准测试上的成功率对比。STEP在两步内实现了显著的成功率提升。\n\n![消融实验](https://arxiv.org/html/2602.08245v1/x3.png)\n> **图3**：消融实验结果。展示了时空一致性预测机制和速度感知扰动注入机制对性能的贡献。\n\n### 消融实验\n- 时空一致性预测机制：显著提高了动作质量和成功率。\n- 速度感知扰动注入机制：有效防止了执行停滞，提高了控制稳定性。\n\n## 总结与启发\n### 核心贡献\n1. 提出了时空一致性预测机制，生成高质量的预热启动动作，同时保持时间和空间一致性。\n2. 引入了速度感知扰动注入机制，自适应调节动作变化，防止执行停滞。\n3. 提供了理论分析，证明了所提出的方法可以诱导局部收缩映射，确保动作误差的收敛。\n\n### 局限性\n- 本文方法在某些极端动态变化的任务中可能仍需进一步优化。\n- 速度感知扰动注入机制在特定场景下可能需要更精细的调整。\n\n### 后续研究启示\n- 进一步探索时空一致性预测机制在更多复杂任务中的应用。\n- 优化速度感知扰动注入机制，以适应更多种类的实际任务。\n- 结合其他先进的加速方法，进一步提高扩散策略的实时性能。",
      "imageUrls": [
        "https://arxiv.org/html/2602.08245v1/x1.png",
        "https://arxiv.org/html/2602.08245v1/x2.png",
        "https://arxiv.org/html/2602.08245v1/x3.png",
        "https://arxiv.org/html/2602.08245v1/x4.png",
        "https://arxiv.org/html/2602.08245v1/x5.png",
        "https://arxiv.org/html/2602.08245v1/x6.png",
        "https://arxiv.org/html/2602.08245v1/x7.png",
        "https://arxiv.org/html/2602.08245v1/x8.png",
        "https://arxiv.org/html/2602.08245v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07388",
      "title": "Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.07388",
      "arxivId": "2602.07388",
      "date": "2026-02-07",
      "authors": "Jianfei Yang Team",
      "category": "Manipulation",
      "summary": "本文针对长时机器人操作任务中由于视觉相似但需不同动作导致的多模态动作歧义问题，提出了一种基于扩散模型的轨迹聚焦策略（TF-DP）。该方法通过显式地将动作生成条件化于机器人的执行历史，利用历史运动轨迹提供阶段感知的上下文信息，并在视觉观察空间中突出与历史运动相关的任务相关区域，从而提高对背景视觉干扰的鲁棒性。实验结果表明，TF-DP在多模态动作歧义任务上比普通扩散策略提升了80.56%的性能，在视觉干扰条件下提升了86.11%的性能，同时仅增加了6.4%的运行时间。",
      "detailedSummary": "## 研究背景与动机\n当前，基于生成模型的策略在模仿学习中的机器人操作任务中表现出色，通过从演示中学习动作分布。然而，在长时序任务中，视觉上相似的观测值经常在不同的执行阶段出现，但需要不同的动作，这导致了多模态动作模糊（MA²）。本文针对这一具体痛点，提出了Trace-Focused Diffusion Policy (TF-DP)，通过显式地将动作生成条件化于机器人的执行历史来解决这个问题。核心思路是利用执行轨迹提供阶段感知的上下文，从而在视觉上相似的观测值下区分不同的动作。\n\n## 方法详解\n### 整体框架\nTF-DP的整体框架如图2所示。该方法首先收集历史机器人运动以创建运动轨迹，然后生成Trace-Focused Field，并将其投影到图像空间中，以解决MA²问题并减轻背景视觉干扰。具体来说，给定观测值 \\( o_t = \\{I_t^{\\mathrm{global}}, I_t^{\\mathrm{side}}, I_t^{\\mathrm{wrist}}, p_t^{\\mathrm{ee}}\\} \\) 和历史末端执行器轨迹 \\( \\mathcal{H}_t = \\{p_{\\tau}^{\\mathrm{ee}} \\mid \\tau \\leq t\\} \\)，TFF模块将轨迹从3D机器人空间投影到2D全局相机空间，得到轨迹图像 \\( I_t^{\\mathrm{trace}} \\)。然后，基于投影的2D轨迹在全局视图 \\( I_t^{\\mathrm{global}} \\) 上渲染一个Trace-Focused Field，生成增强的全局视图 \\( \\tilde{I}_t^{\\mathrm{global}} \\)。通过将去噪过程条件化于执行感知的观测值 \\( \\tilde{o}_t = \\{\\tilde{I}_t^{\\mathrm{global}}, I_t^{\\mathrm{side}}, I_t^{\\mathrm{wrist}}, I_t^{\\mathrm{trace}}, p_t^{\\mathrm{ee}}\\} \\)，TF-DP能够将视觉上相似的观测值与不同执行阶段的动作关联起来。\n\n![方法框架](https://arxiv.org/html/2602.07388v1/x2.png)\n> **图2**：TF-DP的整体框架。左侧为历史机器人运动的收集和轨迹生成，右侧为Trace-Focused Field的渲染和增强全局视图的生成。\n\n### 核心模块\n- **Trace-Focused Field (TFF) Rendering**：该模块将历史末端执行器轨迹从3D空间投影到2D全局相机空间，生成轨迹图像 \\( I_t^{\\mathrm{trace}} \\)。然后，基于投影的2D轨迹在全局视图 \\( I_t^{\\mathrm{global}} \\) 上渲染一个Trace-Focused Field，生成增强的全局视图 \\( \\tilde{I}_t^{\\mathrm{global}} \\)。\n- **Diffusion-based Action Generation**：通过将去噪过程条件化于执行感知的观测值 \\( \\tilde{o}_t \\)，TF-DP能够生成与当前执行阶段相匹配的动作。具体来说，TF-DP使用扩散模型进行迭代去噪，生成最终的动作分布。\n\n### 创新点\n- **执行轨迹的显式条件化**：通过引入执行轨迹，TF-DP能够在视觉上相似的观测值下区分不同的动作，从而解决MA²问题。\n- **Trace-Focused Field**：该字段强调与历史运动相关的任务相关区域，提高对背景视觉干扰的鲁棒性。\n\n## 实验与结果\n### 实验设置\n- **数据集**：使用了多个真实世界的机器人操作任务，这些任务具有显著的多模态动作模糊和视觉杂乱条件。\n- **基准方法**：对比了vanilla diffusion policy和其他现有的生成模型方法。\n\n### 关键实验结果\n- **成功率**：在具有多模态动作模糊的任务中，TF-DP比vanilla diffusion policy提高了80.56%的成功率。\n- **鲁棒性**：在视觉干扰条件下，TF-DP比vanilla diffusion policy提高了86.11%的鲁棒性。\n- **推理效率**：TF-DP仅增加了6.4%的运行时间，保持了高效的推理性能。\n\n![实验结果](https://arxiv.org/html/2602.07388v1/x3.png)\n> **图3**：在多模态动作模糊任务上的成功率对比。TF-DP显著优于vanilla diffusion policy。\n\n![鲁棒性对比](https://arxiv.org/html/2602.07388v1/x4.png)\n> **图4**：在视觉干扰条件下的鲁棒性对比。TF-DP在视觉干扰下表现更稳定。\n\n### 消融实验\n- **执行轨迹的影响**：移除执行轨迹后，TF-DP的成功率下降了约30%，表明执行轨迹在解决MA²问题中的关键作用。\n- **Trace-Focused Field的影响**：移除Trace-Focused Field后，TF-DP的鲁棒性下降了约20%，表明该字段在提高对背景视觉干扰的鲁棒性方面的重要性。\n\n## 总结与启发\n### 核心贡献\n- **识别并解决了多模态动作模糊问题**：通过引入执行轨迹，TF-DP能够有效解决长时序任务中的多模态动作模糊问题。\n- **提出了一种新的单策略框架**：TF-DP通过轻量级的历史条件化，实现了在单个策略内的时序一致性和鲁棒性。\n\n### 局限性\n- **计算开销**：虽然TF-DP在推理效率上表现良好，但在某些极端情况下，计算开销可能仍然较高。\n- **适用范围**：TF-DP主要适用于视觉上相似的观测值频繁出现的长时序任务，对于其他类型的任务可能效果有限。\n\n### 后续研究启示\n- **进一步优化计算效率**：可以探索更高效的轨迹表示和处理方法，以进一步降低计算开销。\n- **扩展应用场景**：未来可以将TF-DP应用于更多类型的机器人操作任务，验证其在不同场景下的泛化能力。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07388v1/x1.png",
        "https://arxiv.org/html/2602.07388v1/x2.png",
        "https://arxiv.org/html/2602.07388v1/x3.png",
        "https://arxiv.org/html/2602.07388v1/x4.png",
        "https://arxiv.org/html/2602.07388v1/x5.png",
        "https://arxiv.org/html/2602.07388v1/x6.png",
        "https://arxiv.org/html/2602.07388v1/x7.png",
        "https://arxiv.org/html/2602.07388v1/x8.png",
        "https://arxiv.org/html/2602.07388v1/x9.png",
        "https://arxiv.org/html/2602.07388v1/x10.png",
        "https://arxiv.org/html/2602.07388v1/x11.png",
        "https://arxiv.org/html/2602.07388v1/x12.png",
        "https://arxiv.org/html/2602.07388v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07341",
      "title": "Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions",
      "url": "http://arxiv.org/abs/2602.07341",
      "arxivId": "2602.07341",
      "date": "2026-02-07",
      "authors": "Zhuo Zou Team",
      "category": "Manipulation",
      "summary": "本文针对灵巧机器人手臂系统的可扩展操作学习问题，提出了一种基于增强现实（AR）的远程人机交互方法，以提高专家演示数据收集效率。该方法分为两个阶段：首先通过行为克隆（BC）方式预训练策略，利用AR系统收集的数据；其次，采用对比学习增强的强化学习（RL）方法进一步优化策略，并设计投影头加速学习过程。实验结果表明，与经典的近端策略优化和软演员-评论家策略相比，该方法不仅显著提高了推理速度，还在完成操作任务的成功率上表现更优。消融研究表明，提出的对比学习强化学习方法有效克服了策略崩溃问题。",
      "detailedSummary": "## 研究背景与动机\n当前主流的机器人学习方法主要依赖于行为克隆（Behavior Cloning, BC）和强化学习（Reinforcement Learning, RL）。然而，这些方法存在一些关键局限性。例如，BC 方法容易受到数据不匹配和累积误差的影响，而RL方法在高维观测下难以学习有效的表示。本文针对这些痛点，提出了一种基于增强现实（AR）远程人机交互的数据收集系统，并结合模仿学习和对比学习辅助的RL算法，以提高灵巧机器人操作任务的学习效率和性能。本文的核心思路是通过AR远程人机交互系统收集专家演示数据，并利用模仿学习进行预训练，然后通过对比学习辅助的RL方法进一步优化策略。\n\n## 方法详解\n### 整体框架\n本文提出的方法分为两个阶段：预训练阶段和RL训练阶段。\n- **预训练阶段**：通过行为克隆的方式，利用AR远程人机交互系统收集的专家演示数据进行预训练。\n- **RL训练阶段**：采用对比学习增强的RL方法，设计了一个投影头来加速学习过程，并引入事件驱动的增强奖励来提高安全性。\n\n### 核心模块\n1. **AR远程人机交互系统**：\n   - 该系统使用Unity平台在边缘服务器上运行，确保不同类型的AR头盔和机器人系统可以无线且远程连接。\n   - 专家佩戴AR头盔并通过摄像头捕捉其行为，这些行为被复制到灵巧机器人上，从而收集学习数据。\n\n2. **行为克隆预训练**：\n   - 通过解决一个简单的回归问题，利用收集到的专家演示数据进行预训练。\n   - 预训练的目标是使机器人能够模仿专家的行为，从而为后续的RL训练提供一个良好的初始策略。\n\n3. **对比学习增强的RL**：\n   - 设计了一个投影头，用于约束RL策略向高回报的专家状态-动作对靠拢。\n   - 引入了事件驱动的增强奖励，以提高系统的安全性和鲁棒性。\n\n### 创新点\n- **AR远程人机交互系统**：通过AR技术远程收集高质量的专家演示数据，提高了数据收集的效率和质量。\n- **对比学习增强的RL**：通过对比学习和投影头的设计，克服了传统RL方法中的策略崩溃问题，并显著提高了学习效率。\n\n![方法框架](https://arxiv.org/html/2602.07341v1/fig2revise.jpg)\n> **图1**：方法整体框架。左侧为AR远程人机交互系统，用于收集专家演示数据；右侧为模仿学习和对比学习增强的RL训练流程。\n\n## 实验与结果\n### 数据集与实验平台\n- **仿真环境**：使用PyBullet物理引擎进行仿真。\n- **真实世界实验**：在实际机器人平台上进行了验证。\n\n### 对比方法\n- Proximal Policy Optimization (PPO)\n- Soft Actor-Critic (SAC)\n\n### 关键实验结果\n- **成功率**：本文提出的方法在完成灵巧操作任务时的成功率显著高于PPO和SAC。\n- **训练时间**：本文方法的训练时间大约是SAC的四分之一，显著降低了训练时间。\n\n![实验结果对比](https://arxiv.org/html/2602.07341v1/fig5.jpg)\n> **图2**：不同方法在仿真环境中的成功率对比。本文方法（红线）显著优于PPO（蓝线）和SAC（绿线）。\n\n![消融实验](https://arxiv.org/html/2602.07341v1/convergence.jpg)\n> **图3**：消融实验结果。展示了行为克隆预训练和对比学习对最终性能的贡献。行为克隆预训练显著减少了训练时间，而对比学习进一步提高了模型性能。\n\n### 消融实验\n- **行为克隆预训练**：显著减少了RL训练时间。\n- **对比学习**：进一步提高了无模型RL的性能，克服了策略崩溃问题。\n\n## 总结与启发\n### 核心贡献\n1. **AR远程人机交互系统**：通过AR技术远程收集高质量的专家演示数据，提高了数据收集的效率和质量。\n2. **对比学习增强的RL**：通过对比学习和投影头的设计，克服了传统RL方法中的策略崩溃问题，并显著提高了学习效率。\n3. **实验验证**：在仿真和真实世界实验中，本文方法显著提高了灵巧操作任务的成功率，并大幅减少了训练时间。\n\n### 局限性\n- 本文方法仍需要一定数量的专家演示数据，尽管数量较少，但获取这些数据仍然需要时间和成本。\n- 在某些复杂环境中，AR远程人机交互系统可能受到硬件限制的影响。\n\n### 启示\n- AR技术在远程人机交互中的应用为机器人学习提供了新的数据收集途径。\n- 结合模仿学习和对比学习的RL方法为解决高维观测下的策略学习问题提供了新的思路。\n- 未来研究可以进一步探索如何减少对专家演示数据的依赖，以及如何在更复杂的环境中应用本文方法。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07341v1/fig1.jpg",
        "https://arxiv.org/html/2602.07341v1/fig1b.jpg",
        "https://arxiv.org/html/2602.07341v1/fig2revise.jpg",
        "https://arxiv.org/html/2602.07341v1/fig3.jpg",
        "https://arxiv.org/html/2602.07341v1/fig4a.png",
        "https://arxiv.org/html/2602.07341v1/fig4b.png",
        "https://arxiv.org/html/2602.07341v1/fig5.jpg",
        "https://arxiv.org/html/2602.07341v1/convergence.jpg",
        "https://arxiv.org/html/2602.07341v1/visualInterface.png",
        "https://arxiv.org/html/2602.07341v1/fig8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07326",
      "title": "Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing",
      "url": "http://arxiv.org/abs/2602.07326",
      "arxivId": "2602.07326",
      "date": "2026-02-07",
      "authors": "Seokhwan Jeong Team",
      "category": "Manipulation",
      "summary": "该论文探讨了在极简传感条件下实现可靠的多指抓取问题，仅依赖单轴指尖力反馈和关节本体感觉，无需视觉或高分辨率触觉传感器。研究采用了一种高效的教师-学生训练框架，其中强化学习的教师利用模拟中的特权观察生成演示，以提炼出基于Transformer的学生策略，该策略仅使用实际部署中可用的传感模式。实验结果表明，在18个物体上（包括分布内和分布外的情况），该方法实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。",
      "detailedSummary": "## 研究背景与动机\n目前，多指机器人抓取任务主要依赖于视觉、触觉和力传感器等多种感知模态。然而，这些方法存在一些关键局限性：视觉传感器容易受到遮挡和光照变化的影响，触觉传感器难以小型化且对光照敏感，而多轴力/扭矩传感器则成本高昂且难以集成。本文针对这些痛点，提出了一种仅依赖单轴指尖力反馈和关节本体感受的无视觉多指抓取框架。该方法通过一个高效的教师-学生训练管道，利用强化学习训练的教师生成演示数据，进而蒸馏出一个基于Transformer的学生策略，该策略在实际部署中仅使用可用的感知模态。本文的核心思路是通过极简的感知模态实现可靠的多指抓取。\n\n## 方法详解\n### 整体框架\n本文提出的框架包括两个主要阶段：教师策略训练和学生策略蒸馏。教师策略在仿真环境中利用特权观测进行训练，然后通过模仿学习将知识传递给学生策略。学生策略仅使用关节位置和单轴力输入来执行盲抓取任务。整体框架如图1所示。\n![方法框架](https://arxiv.org/html/2602.07326v1/x1.png)\n> **图1**：方法整体框架。左侧为教师策略训练阶段，右侧为学生策略蒸馏阶段。\n\n### 核心模块\n#### 教师策略训练\n教师策略 \\(\\pi_t\\) 在仿真环境中利用特权观测 \\(o_t^{\\text{priv}} \\in \\mathbb{R}^{95}\\) 进行训练。特权观测包括关节位置/速度、指尖姿态、物体姿态、物体线性和角速度、三轴指尖接触力、物体与夹爪中心的平面距离、单轴指尖力、六维指尖力矩以及前一动作。为了提高鲁棒性和模拟到现实的迁移，添加了高斯噪声（关节角度 \\(\\sigma = 0.005\\) 弧度，指尖力 \\(\\sigma = 0.5\\) 牛顿）。每个单轴力通过将三轴指尖接触力投影到指尖局部 z 轴并阈值化小幅度来计算。\n\n奖励函数包括任务奖励 \\(r_t\\)、激励奖励 \\(r_i\\) 和惩罚项 \\(r_l, r_a, r_{ar}\\)。任务奖励促进接触启动和平稳提升，激励奖励鼓励三个指尖同时接触，惩罚项防止关节限位违规、大动作和快速动作变化。奖励函数表达式如下：\n\\[ r = w_1 r_t + w_2 r_i + w_31 r_l + w_32 r_a + w_33 r_{ar} \\]\n其中 \\(w_1 = 1.0\\), \\(w_2 = 0.2\\), \\(w_31 = -500\\), \\(w_32 = -0.04\\), \\(w_33 = -0.01\\)。\n\n#### 学生策略蒸馏\n学生策略通过模仿学习从教师策略生成的演示数据中学习。学生策略仅使用关节位置和单轴力输入来执行抓取任务。学生策略采用Transformer架构，能够捕捉长时间范围内的依赖关系。学生策略的训练过程通过行为克隆或DAgger算法进行。\n\n### 创新点\n本文的主要创新点在于：\n1. 仅依赖单轴指尖力反馈和关节本体感受实现可靠的多指抓取。\n2. 通过教师-学生训练框架，利用特权观测生成高质量的演示数据，进而蒸馏出适用于实际部署的学生策略。\n3. 通过简化感知模态，显著降低了实际部署中的感知和集成要求。\n\n## 实验与结果\n### 实验设置\n本文在真实硬件上进行了验证，使用了一个定制的三指夹爪，安装在一个固定框架上。夹爪基于开源D'Claw操纵器设计，每个指尖配备单轴力传感器。实验对象包括18个几何形状不同的物体，分为6个分布内物体和12个分布外物体。\n\n### 对比方法\n本文对比了多种基线方法，包括仅使用视觉的方法、多模态融合方法以及其他无视觉抓取方法。\n\n### 关键实验结果\n本文在18个物体上实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。具体数值如下：\n- 分布内物体抓取成功率：99.5%\n- 分布外物体抓取成功率：97.2%\n\n### 实验结果图表\n![实验结果](https://arxiv.org/html/2602.07326v1/x4.png)\n> **图4**：不同方法在18个物体上的抓取成功率对比。本文方法在所有物体上均表现出色。\n\n![消融实验](https://arxiv.org/html/2602.07326v1/x5.png)\n> **图5**：消融实验结果。展示了各个组件对抓取成功率的贡献。\n\n### 消融实验\n消融实验表明，单轴力反馈和关节本体感受对于抓取成功至关重要。具体贡献如下：\n- 单轴力反馈：提高了抓取稳定性，减少了失败率。\n- 关节本体感受：提供了必要的位置信息，确保了抓取动作的准确性。\n\n## 总结与启发\n### 核心贡献\n1. 提出了一种仅依赖单轴指尖力反馈和关节本体感受的无视觉多指抓取框架，显著降低了感知和集成要求。\n2. 开发了教师-学生训练框架，利用特权观测生成高质量的演示数据，进而蒸馏出适用于实际部署的学生策略。\n3. 在真实硬件上验证了方法的有效性，实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。\n\n### 局限性\n本文方法虽然在多种物体上表现良好，但在极端情况下（如非常光滑或非常重的物体）可能仍存在挑战。此外，单轴力传感器的精度和可靠性也会影响抓取性能。\n\n### 后续研究启示\n未来的研究可以进一步探索更复杂的抓取任务，如动态环境下的抓取、多物体抓取等。此外，结合其他低成本传感器（如IMU）可能会进一步提高系统的鲁棒性和适应性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07326v1/x1.png",
        "https://arxiv.org/html/2602.07326v1/x2.png",
        "https://arxiv.org/html/2602.07326v1/x3.png",
        "https://arxiv.org/html/2602.07326v1/x4.png",
        "https://arxiv.org/html/2602.07326v1/x5.png",
        "https://arxiv.org/html/2602.07326v1/x6.png",
        "https://arxiv.org/html/2602.07326v1/x7.png",
        "https://arxiv.org/html/2602.07326v1/figures/Edgar.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/Junho.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/TaeminKim.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/Changjoo.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/Seokhwan.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07082",
      "title": "MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation",
      "url": "http://arxiv.org/abs/2602.07082",
      "arxivId": "2602.07082",
      "date": "2026-02-06",
      "authors": "Wei Gao Team",
      "category": "Manipulation",
      "summary": "论文针对嵌入式AI中视觉语言模型空间推理能力弱的问题，尤其是在跨帧复杂空间关系任务上，提出了MosaicThinker技术。该方法通过迭代构建空间表示，将多帧碎片化信息整合为统一的全局语义地图，并利用视觉提示引导VLM进行推理。实验结果表明，该技术能极大地提高资源受限设备上跨帧空间推理的准确性，适用于各种类型和复杂度的任务。",
      "detailedSummary": "## 研究背景与动机\n当前，具身AI应用正从传统的物体检测与识别扩展到机器人操纵和驱动规划等更高级的任务。在这些任务中，视觉语言模型需要具备三维空间推理能力，以理解物体的空间关系并指导设备行动。然而，现有的视觉语言模型由于缺乏对3D空间信息的了解，在空间推理方面能力非常薄弱，尤其是在涉及跨多个视频帧的复杂空间关系推理任务时。这种局限性在小型的设备端VLM上更为突出，因为它们的表征能力有限。此外，增强VLM空间推理能力的主流方法，如注入深度图或生成密集的鸟瞰图，要么在跨帧对齐上表现不佳，要么对于小型VLM来说过于复杂难以解读。\n\n本文针对小型设备端VLM在复杂的跨帧视觉空间推理任务上能力不足这一具体痛点，提出了一种新的推理时计算技术。其核心思路是：不重新训练VLM，而是通过整合多帧碎片化的空间信息，迭代构建一个统一的、稀疏的全局语义地图作为空间表示，并设计视觉提示来引导VLM基于该语义地图进行推理，从而突破小型VLM自身表征能力的限制。\n\n## 方法详解\nMosaicThinker的整体流程旨在处理以自我中心视角拍摄的场景视频，并回答用户关于物体空间关系的自然语言问题。其核心包含两个主要组件：语义地图的迭代构建和关键帧选择。\n\n![方法总览](https://arxiv.org/html/2602.07082v1/x6.png)\n> **图6**：MosaicThinker设计总览。系统首先通过任务接地确定任务相关物体列表，然后通过迭代的关键帧选择机制采样视频帧。选出的关键帧经过空间信息提取和跨帧对齐，迭代构建出全局语义地图。最后，该语义地图通过精心设计的视觉提示输入给设备端VLM，以完成空间推理。\n\n**1. 预处理与任务接地**：给定推理任务，系统首先使用VLM，结合任务问题和随机采样的几帧视频，生成一个按相关性和出现可能性排序的任务相关物体自然语言列表。该列表包括问题中明确提到的目标物体和作为上下文线索的路标物体。同时，为提升效率，系统会通过轻量级过滤技术（如下采样、运动跟踪）检测并移除输入视频中的冗余帧。\n\n**2. 语义地图的迭代构建**：构建语义地图的目标是将相机和所有物体的位置、姿态映射到一个全局稀疏网格中。由于小型VLM无法直接从图像准确推断3D信息，MosaicThinker采用专门的AI模型（如分割模型、深度估计模型）从单帧提取物体的3D点云信息。关键挑战在于将这些来自不同视角的碎片化信息对齐到一个统一的全局坐标系中。\n\n*   **跨帧对齐**：系统通过预训练的图像匹配模型（如MatchAnything）在RGB图像间建立像素对应关系，并利用这些匹配点（特别是位于任务相关物体边界框内的点）计算帧间的相机变换矩阵（平移和旋转）。为了高效，每个待对齐帧仅与已对齐帧中最相似的帧进行匹配，相似度通过PSNR或SSIM等轻量指标计算。\n*   **多帧全局对齐**：为避免传统顺序集成方法导致的累积误差和匹配失败，MosaicThinker采用拓扑感知的对齐策略。它将视频帧组织成以某个“全局锚点”帧为根的树状结构，树边代表帧间最大视觉重叠。通过计算到根节点的唯一路径来推导全局位姿，从而仅使用高置信度匹配对，有效防止误差传播。\n*   **处理遮挡**：对齐所有帧并估算每帧相机位姿后，系统可以推断哪些本应在视野内但被分割模型遗漏的物体。这些信息可用于引导分割模型恢复被遮挡或部分可见的物体。\n\n**3. 关键帧选择**：使用所有视频帧构建语义地图既昂贵又可能引入语义噪声。MosaicThinker通过迭代的时间搜索过程选择最相关的关键帧子集。在每一轮迭代中，系统仅根据一个采样分布对少量帧进行评分（评分基于预训练物体检测器对任务相关物体的检测置信度），并选择得分高于阈值的帧作为关键帧。采样分布初始为均匀分布，随后根据物体出现的时序局部性进行迭代优化：如果一个帧得分高，则通过高斯核函数提升其前后相邻帧被采样的概率。关键帧的数量、迭代次数和评分阈值根据设备计算能力和推理延迟要求预先定义。\n\n**4. 通过视觉提示进行空间推理**：构建好的语义地图以视觉提示的形式输入给VLM。如图9所示，该提示将语义地图表示为一个俯视网格，使用简单符号和不同颜色的边界框来示意物体的相对位置，并辅以描述每个物体边界框在语义图中绝对位置的文本。\n\n![视觉提示](https://arxiv.org/html/2602.07082v1/x9.png)\n> **图9**：从对齐的3D点云构建视觉提示。语义地图被表示为带有符号和彩色边界框的俯视图，并附有文本描述。\n\n与现有方法相比，MosaicThinker的创新点具体体现在：1) 提出了一个**稀疏的、语义化的全局空间表示**（语义地图），而非密集的BEV地图，更适合小型VLM理解；2) 采用**迭代的、拓扑感知的跨帧对齐策略**构建该表示，有效整合碎片化信息并避免误差累积；3) 设计了**基于迭代采样的高效关键帧选择机制**，在保证信息充分的同时最小化计算开销。\n\n## 实验与结果\n**实验设置**：研究在多个设备端AI平台（NVIDIA Jetson Orion, Meta AR Glass, OnePlus 12R手机）上实现了MosaicThinker。评估使用了多个空间推理基准测试，涵盖不同类型的室内场景（住宅、办公室、图书馆等），包括SpatialVQA、CLEVRER和ScanNet。任务类型聚焦于物体关系、位置识别和相机运动估计等跨帧空间推理。\n\n**对比方法**：对比的基线方法包括：1) 直接使用VLM（Qwen2.5-VL， LLaVA-NeXT）；2) 使用深度图增强VLM；3) 使用BEV地图增强VLM；4) 使用场景图重建；5) 其他先进的视频理解模型（Video-LLaVA， Video-ChatGPT）。\n\n**关键实验结果**：\n![总体性能对比](https://arxiv.org/html/2602.07082v1/x13.png)\n> **图13**：在SpatialVQA基准测试上的总体性能。MosaicThinker（橙色）在三种任务类型（物体关系、位置识别、相机运动）上均显著优于所有基线方法，平均准确率达到78.2%，比最佳基线（BEV地图）提升13.4%。\n\nMosaicThinker显著提升了困难跨帧空间推理任务的准确率。在SpatialVQA基准上，其准确率达到78.2%，比最佳基线方法（BEV地图）高出13.4%，在某些任务类型上提升幅度高达40%。在CLEVRER和ScanNet基准上也观察到一致的显著提升。\n\n![消融实验](https://arxiv.org/html/2602.07082v1/x14.png)\n> **图14**：消融实验结果。依次移除关键帧选择（KS）、跨帧对齐（CA）和语义地图（SM）组件，性能逐级下降。其中，语义地图的贡献最大，其缺失导致性能下降22.5%。\n\n**消融实验**：如图14所示，移除关键帧选择、跨帧对齐和语义地图组件均会导致性能下降。其中，**语义地图的贡献最大**，其缺失使准确率下降22.5%，证明了统一空间表示的核心作用。跨帧对齐和关键帧选择分别贡献了约7%和4%的性能提升。\n\n![定性结果](https://arxiv.org/html/2602.07082v1/x15.png)\n> **图15**：定性结果对比。MosaicThinker能够正确推理出“蓝色笔记本在书架左边”，而直接使用VLM和BEV地图方法均给出了错误答案，显示了其在复杂跨帧关系推理上的有效性。\n\n![关键帧选择分析](https://arxiv.org/html/2602.07082v1/x16.png)\n> **图16**：关键帧选择过程分析。经过3轮迭代，采样分布（蓝色曲线）成功聚焦到高似然分数（红色曲线）的视频片段，高效地定位了包含任务相关物体的关键帧。\n\n其他实验结果图表进一步展示了方法的有效性：\n![效率分析](https://arxiv.org/html/2602.07082v1/x17.png)\n> **图17**：不同设备上的推理延迟。MosaicThinker在保持高精度的同时，引入了可接受的额外计算开销。\n\n![对齐方法对比](https://arxiv.org/html/2602.07082v1/x11.png)\n> **图11**：本文的图像匹配对齐方法（右）与传统的ICP点云对齐方法（左）对比。本文方法避免了无关背景干扰，实现了更准确的对齐。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了 **MosaicThinker**，一个**无需重新训练**的推理时计算框架，首次在小型设备端VLM上实现了复杂的跨帧视觉空间推理；2) 设计了**迭代构建稀疏全局语义地图**的方法，通过拓扑感知的跨帧对齐有效整合多视角碎片化空间信息，形成了适合小型VLM理解的统一空间表示；3) 引入了**高效的关键帧选择机制**，通过迭代采样聚焦于信息丰富的视频片段，在保证推理质量的同时优化了计算效率。\n\n论文自身提到的局限性包括：1) 主要处理室内场景，且假设相机运动速度适中、遮挡不频繁；2) 专注于物体间的空间关系推理，而非精确的尺寸和距离测量；3) 其性能依赖于现有的分割、深度估计等感知模型的准确性。\n\n这项工作对后续研究的启示在于：为增强小型模型在复杂任务上的能力提供了一条“**赋能而非替代**”的新路径，即通过精心设计的外部表示和推理时计算来弥补模型内在表征能力的不足。未来工作可探索将语义地图构建方法扩展到更动态或更复杂的室外场景，与其他模态（如惯性传感）结合以提升对齐鲁棒性，并进一步优化关键帧选择和对齐算法的计算效率，以适用于资源更受限的边缘设备。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07082v1/x1.png",
        "https://arxiv.org/html/2602.07082v1/x2.png",
        "https://arxiv.org/html/2602.07082v1/x3.png",
        "https://arxiv.org/html/2602.07082v1/x4.png",
        "https://arxiv.org/html/2602.07082v1/x5.png",
        "https://arxiv.org/html/2602.07082v1/x6.png",
        "https://arxiv.org/html/2602.07082v1/x7.png",
        "https://arxiv.org/html/2602.07082v1/x8.png",
        "https://arxiv.org/html/2602.07082v1/x9.png",
        "https://arxiv.org/html/2602.07082v1/x10.png",
        "https://arxiv.org/html/2602.07082v1/x11.png",
        "https://arxiv.org/html/2602.07082v1/x12.png",
        "https://arxiv.org/html/2602.07082v1/x13.png",
        "https://arxiv.org/html/2602.07082v1/x14.png",
        "https://arxiv.org/html/2602.07082v1/x15.png",
        "https://arxiv.org/html/2602.07082v1/x16.png",
        "https://arxiv.org/html/2602.07082v1/x17.png",
        "https://arxiv.org/html/2602.07082v1/x18.png",
        "https://arxiv.org/html/2602.07082v1/x19.png",
        "https://arxiv.org/html/2602.07082v1/x20.png",
        "https://arxiv.org/html/2602.07082v1/x21.png",
        "https://arxiv.org/html/2602.07082v1/x22.png",
        "https://arxiv.org/html/2602.07082v1/x23.png",
        "https://arxiv.org/html/2602.07082v1/x24.png",
        "https://arxiv.org/html/2602.07082v1/x25.png",
        "https://arxiv.org/html/2602.07082v1/x26.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.06620",
      "title": "Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique",
      "url": "http://arxiv.org/abs/2602.06620",
      "arxivId": "2602.06620",
      "date": "2026-02-06",
      "authors": "Toshiaki Tsuji Team",
      "category": "Manipulation",
      "summary": "本文解决接触式任务中，如何从易得的位置轨迹生成适配特定硬件的精确力命令这一核心问题。提出了力生成模仿学习方法，通过无记忆的力生成模型结合反馈控制机制，将给定位置轨迹映射为力命令。实验表明，该方法确保了反馈控制的稳定性，有效提升了模型对未见位置轨迹的泛化能力，并在真实机器人书写任务中取得了性能改进。",
      "detailedSummary": "## 研究背景与动机\n在接触丰富的机器人任务中，位置轨迹通常易于获取，但合适的力命令往往是未知的。虽然可以考虑使用预训练的基础模型（如视觉-语言-动作模型）来生成力命令，但力控制高度依赖于机器人的特定硬件，这使得此类模型的应用面临挑战。模仿学习虽然能够学习复杂任务，但在接触丰富的任务中，通常只能处理非常静态的运动。基于双边控制的模仿学习能够收集人类操作者的力感知数据，但存在两个关键问题：同时生成位置和力命令会增加模型的维度，使训练更加困难；基于学习的运动生成方法通常比传统的基于模型的方法重现性低。本文旨在解决从给定位置轨迹生成力命令的问题，并通过引入反馈控制来提高鲁棒性和泛化能力。核心思路是提出一个分层的力生成模仿学习框架，将有记忆的上层与无记忆、可反馈控制的下层明确分离，使系统能够为未见过的位置轨迹生成有效的力命令。\n\n## 方法详解\n本文提出的方法是一个分层架构，旨在将有记忆的神经网络与无记忆的神经网络分离，以集成经典反馈控制。\n\n![方法框架](https://arxiv.org/html/2602.06620v1/x2.png)\n\n> **图2**：所提出的用于校正输出误差的分层模型。虚线框内为下层神经网络，其输入为当前状态和来自上层的未来轨迹（含PID校正项），输出为预测的下一状态和动作命令。PID控制器根据上层轨迹与预测状态的误差生成校正量，并反馈到上层轨迹输入中。\n\n整体框架如**图2**所示。上层（对应图中顶部的虚线）负责处理位置轨迹，以智能地预测未来状态。具体而言，上层输出从未来1步到10步的角度𝜽和角速度𝜽˙，并以10步为周期进行更新。输出中未来第10步的[𝜽_{k+10}, 𝜽˙_{k+10}]被传递给下层，并在下一个更新周期前保持恒定；在实际控制的每一步，使用对应的[𝜽, 𝜽˙]值。\n\n下层是一个无记忆的神经网络（图2中虚线框内部分）。其输入包括：10步后的上层轨迹[𝜽_{k+10}, 𝜽˙_{k+10}]、当前状态𝒔_k（关节角度、角速度和扭矩）。其输出是预测的下一状态𝒔̂_{k+1}和下一动作𝒂̂_{k+1}。由于下层仅根据当前状态和期望的未来轨迹进行插值预测，因此不需要内部记忆。这种结构消除了下层因保持内部状态而导致控制困难的隐患。由于𝒂̂_{k+1}包含力（扭矩）命令，该模型能够从位置轨迹生成力命令。\n\n核心创新在于将PID控制集成到该神经网络框架中，以补偿预测误差。具体流程如下：下层NN预测出状态𝒔̂_{k+1}（包含角度𝜽̂_{k+1}和角速度𝜽˙̂_{k+1}）和动作命令𝒂̂_{k+1}。𝒂̂_{k+1}被直接用作机器人的命令。同时，利用上层轨迹中对应的𝜽_{k+1}和𝜽˙_{k+1}，通过公式(1)计算PID控制误差𝒖_{k+1}。该误差经伪微分滤波后得到𝒖˙_{k+1}。随后，将𝒖_{k+1}和𝒖˙_{k+1}分别加到上层的角度𝜽_{k+10}和角速度𝜽˙_{k+10}上，形成校正后的上层角度𝜽^{upper}_{k+10}和角速度𝜽˙^{upper}_{k+10}，并作为新的输入传递给下层NN。这样，模型就能基于包含误差校正项的当前状态和上层轨迹来生成命令和响应值。\n\n与现有方法相比，本文的创新点具体体现在：1）明确提出了一个上层有记忆（处理长期轨迹）、下层无记忆（保证可控性）的分层架构；2）揭示了在反馈控制环内嵌入记忆（如下层使用LSTM）会阻碍稳定性，因此下层采用简单的多层感知机（MLP）；3）通过将PID控制误差反馈到上层轨迹输入中，使模型能够生成考虑轨迹预测误差的、更合适的力命令。\n\n## 实验与结果\n**实验设置**：使用CRANE-X7七自由度机械臂（固定第二关节，实际为6自由度）进行字符书写任务验证。通过双边控制收集了5种不同直线和2种不同圆形轨迹在两种白板高度（0cm和2cm）下的数据，共70组试验，其中56组用于训练，14组用于验证。控制频率为500 Hz。\n\n**对比方法**：\n1.  **直接教学回放**：将直接教学收集的响应值直接作为命令值回放。\n2.  **传统LSTM模型**：使用具有内部状态的LSTM作为下层网络（结构为6层LSTM+1层全连接）。\n3.  **提出的MLP模型**：下层使用无记忆的MLP（7层全连接，每层400维，激活函数为tanh）。\n\n**关键实验结果**：\n实验首先验证了PID增益的影响。**图7、8、9**分别展示了改变比例增益K_p、微分增益K_d和积分增益K_i时，笔尖x坐标轨迹的变化。\n\n![比例增益影响](https://arxiv.org/html/2602.06620v1/figures/Kp_x_comparison.png)\n> **图7**：不同比例增益K_p下笔尖x坐标轨迹。K_p=0时轨迹振荡；增大K_p能抑制振荡，使运动更贴合上层轨迹，但过大的K_p会导致扭矩参考值振荡和增大（见图10a），可能影响稳定性。\n\n![微分增益影响](https://arxiv.org/html/2602.06620v1/figures/Kd_x_comparison.png)\n> **图8**：不同微分增益K_d下笔尖x坐标轨迹。增加K_d未显著改善轨迹，当K_d=0.8或1.0时甚至出现局部振荡，扭矩参考值也出现振荡（图10b），这可能是由于连续时间伪微分计算引入了误差。\n\n![积分增益影响](https://arxiv.org/html/2602.06620v1/figures/Ki_x_comparison.png)\n> **图9**：不同积分增益K_i下笔尖x坐标轨迹。增加K_i在波形顶部减少了偏差，但在底部引入了新的波形峰值。扭矩参考值波形发生变化但未过度增大（图10c）。积分控制对NN参考值生成有影响，但也可能产生负面效果。\n\n![扭矩参考值对比](https://arxiv.org/html/2602.06620v1/x6.png)\n> **图10**：不同PID增益下基关节扭矩参考值对比。(a) K_p增大导致扭矩振荡和增大；(b) K_d增大也引起扭矩振荡；(c) K_i增大改变了扭矩波形但未导致过度增大。\n\n随后，在圆形、“A”、“B”字符书写任务中对比了不同方法。使用交并比（IoU）作为定量评估指标，比较实际绘制形状与上层轨迹形状的重合度。\n\n![模型轨迹对比](https://arxiv.org/html/2602.06620v1/x7.png)\n> **图11**：圆形绘制任务中各模型笔尖x坐标轨迹对比。提出的“MLP w/ PID”模型能最好地跟踪上层轨迹（黑色实线）。“Playback Direct Teaching”虽能跟踪轨迹但书写不准确（图1），“LSTM w/o PID”无法跟踪，“MLP w/o PID”跟踪不足。\n\n**表1** 展示了MLP模型在有/无PID控制下的IoU结果。结果显示，引入PID控制后，在所有测试高度（0cm, 1cm, 2cm）和字符类型上，IoU值均有显著提升，尤其是在1cm和2cm高度时提升更为明显。例如，在1cm高度书写“Character ABCD”时，IoU从0.165提升至0.270。\n\n![字符绘制结果对比](https://arxiv.org/html/2602.06620v1/x8.png)\n> **图12**：白板高度为1cm时，MLP无PID控制与有PID控制绘制的字符对比。可以直观看到，引入PID控制后（右列），字符书写的准确性和完整性得到明显改善。\n\n**消融实验总结**：实验通过对比LSTM和MLP模型，验证了下层网络无记忆对集成反馈控制的重要性（LSTM引入PID后性能未改善）。通过对比MLP在有/无PID控制下的表现，证明了引入PID反馈对于提高轨迹跟踪精度和任务性能的关键作用。PID各增益的调优实验则表明，适当的比例控制能有效改善跟踪，但微分和积分控制需谨慎使用，且当前的连续时间计算方式可能引入误差。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  提出了一个分层的力生成模仿学习框架，明确将有记忆的上层（负责长期轨迹预测）与无记忆、可反馈控制的下层（负责生成状态和力命令）分离开来。\n2.  该框架能够处理具有非平凡或定义不清控制目标的接触式操作任务（如书写），这类任务难以用传统的阻抗或混合控制解决，同时仍能通过经典反馈控制确保稳定性。\n3.  揭示了在反馈控制环内嵌入记忆会阻碍稳定性，并通过实验证明，采用时间尺度分离（上层慢速、有记忆，下层快速、无记忆）可以使基于学习的力生成同时实现鲁棒性和稳定性。\n\n论文自身提到的局限性主要在于PID控制的实现细节。文中指出，微分和积分计算采用了连续时间方法，这可能引入计算误差并对神经网络产生负面影响，未来需要引入离散控制技术来考虑这些计算误差。\n\n本文对后续研究的启示在于：将学习与经典控制结合时，模型的可控性是需要重点考虑的设计因素。明确分离系统的“记忆”部分和“反应”部分，并确保直接与底层控制交互的部分是简单、无记忆且易于分析的，可能是一种有效的架构设计原则。此外，该方法展示了如何利用易于获取的位置轨迹（可来自直接教学、VLA模型等）来生成硬件适配的精细力命令，为将高层智能规划与底层柔顺执行相结合提供了一条可行路径。",
      "imageUrls": [
        "https://arxiv.org/html/2602.06620v1/x1.png",
        "https://arxiv.org/html/2602.06620v1/x2.png",
        "https://arxiv.org/html/2602.06620v1/figures/task_env.png",
        "https://arxiv.org/html/2602.06620v1/x3.png",
        "https://arxiv.org/html/2602.06620v1/x4.png",
        "https://arxiv.org/html/2602.06620v1/x5.png",
        "https://arxiv.org/html/2602.06620v1/figures/Kp_x_comparison.png",
        "https://arxiv.org/html/2602.06620v1/figures/Kd_x_comparison.png",
        "https://arxiv.org/html/2602.06620v1/figures/Ki_x_comparison.png",
        "https://arxiv.org/html/2602.06620v1/x6.png",
        "https://arxiv.org/html/2602.06620v1/x7.png",
        "https://arxiv.org/html/2602.06620v1/x8.png",
        "https://arxiv.org/html/2602.06620v1/figures/compare_ordinalmodel.png",
        "https://arxiv.org/html/2602.06620v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.06512",
      "title": "Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.06512",
      "arxivId": "2602.06512",
      "date": "2026-02-06",
      "authors": "Heng Tao Shen Team",
      "category": "Manipulation",
      "summary": "这篇论文针对机器人模仿学习中训练数据呈现长尾分布的核心问题，即模型在数据丰富的头部任务上表现良好，但在数据稀缺的尾部任务上泛化能力差。研究发现，传统长尾学习策略（如重采样）对提升尾部任务性能效果有限，其根本原因是数据稀缺损害了策略的空间推理能力。为此，作者提出了“接近阶段增强”（APA）方法，通过将头部任务的知识迁移至尾部任务，无需外部演示数据。实验表明，APA方法在模拟和真实机器人操作任务中均能有效提升尾部任务的性能。",
      "detailedSummary": "## 研究背景与动机\n当前，通过大规模模仿学习训练通用机器人策略已成为主流范式，其通过在大量多样化的人类演示数据上训练，使机器人能够根据自然语言指令执行广泛的操作任务。然而，一个关键且常被忽视的挑战阻碍了这些模型的实际应用：演示数据天然存在的长尾分布。在大规模数据集中，少数常见的“头部”任务（如“拿起碗/盘子”）拥有大量演示，而绝大多数“尾部”任务（如“将酒瓶放在架子上”）仅有少量示例。在这种不平衡数据上训练的策略通常在尾部任务上表现不可靠。本文旨在解决这一核心痛点。\n\n现有的计算机视觉领域的长尾学习策略（如重采样、数据增强）在机器人策略学习中效果不佳。重采样仅复制现有数据，无法引入泛化所需的多样性；而如mixup等增强技术对状态和动作进行线性插值，忽略了底层动力学，常导致运动学上不可行或物理无效的轨迹。本文的核心问题是：在长尾演示上训练的策略性能下降的根本原因是什么？以及如何在不依赖外部演示数据的情况下解决此问题？\n\n通过深入分析，本文发现策略在尾部任务上失败的主要原因是其**空间推理能力**的退化。由于训练数据稀缺，模型无法学习尾部任务所需的精确空间关系，尽管它已从头部任务中掌握了大致概念。基于此关键洞察，本文提出了**Approaching-Phase Augmentation (APA)**，一种简单而有效的方案，通过利用头部任务的演示知识来增强尾部任务，从而提升策略在尾部任务上的空间推理能力，且无需外部数据。\n\n## 方法详解\n本文提出的Approaching-Phase Augmentation (APA)方法旨在通过从数据丰富的头部任务向数据稀缺的尾部任务转移知识，生成新的高质量训练样本。其整体流程分为三个步骤，如下图所示。\n\n![方法框架](https://arxiv.org/html/2602.06512v1/x3.png)\n> **图3**：Approaching-Phase Augmentation (APA)流程概览。包含三个步骤：(1) 头部任务轨迹分割，从头部任务中分离出目标接近阶段；(2) 尾部到头部对象嫁接，使用尾部任务对象创建增强轨迹；(3) 指令格式化与协同训练，格式化对应的语言指令，然后在组合数据集上训练策略。\n\n**整体流程：**\n1.  **输入**：原始的、不平衡的长尾演示数据集（包含头部和尾部任务的轨迹、图像和指令）。\n2.  **处理**：执行APA的三个步骤，生成仅包含目标接近阶段的增强轨迹及其对应指令。\n3.  **输出**：一个组合数据集，包含原始长尾数据以及新生成的增强数据，用于协同训练策略。\n\n**核心模块与技术细节：**\n1.  **头部任务轨迹分割**：从数据丰富的头部任务中随机选取成功执行的演示轨迹。通过监测机器人的本体感知（例如夹爪的打开/闭合状态），识别出机械臂正在接近目标的轨迹片段。这一步骤收集了一系列成功的“目标接近阶段”演示。\n2.  **尾部到头部对象嫁接**：此步骤通过将头部任务的轨迹片段中的原始对象替换为尾部任务的对象，来生成新的训练演示。在模拟的LIBERO基准测试中，这涉及直接替换对象资源。新对象的初始位置继承自源轨迹的原始对象，而其旋转方向则由尾部任务的目标指定。随后重新渲染场景以生成新的视觉数据。对于真实世界应用，可以通过YOLOv8识别对象并使用图像修复技术进行替换。这种重组操作使得目标接近阶段的训练数据多样化。\n3.  **指令格式化与协同训练**：为确保语言一致性，使用一对对应的模板来分别格式化原始轨迹和增强轨迹的语言指令。对于新生成的、仅包含目标接近阶段的增强轨迹，使用模板“approach the [target_object]”生成指令。为了在整个数据集中创建一致的两阶段格式，将所有原始演示的指令修改为“approach the [target_object] then [verb_phrase_1] and [verb_phrase_2]”。随后，策略在此组合的、语言一致的数据集上进行训练。\n\n**创新点：**\n与现有方法相比，APA的创新性体现在：\n*   **针对性**：聚焦于增强被识别为关键瓶颈的“目标接近阶段”，避免了生成完整、物理上合理的操作序列的复杂性。\n*   **高效性**：利用头部任务中已有的成功接近轨迹，通过对象替换快速生成高质量尾部任务数据，无需外部演示。\n*   **解耦性**：由于只涉及接近阶段，对替换对象的物理属性（如尺寸、质量）没有严格要求，这简化了数据生成过程。\n\n## 实验与结果\n**实验设置：**\n*   **基准/数据集**：在仿真实验中，使用基于LIBERO环境构建的**LIBERO-Core-LT**长尾数据集（包含10个任务）。在真实世界实验中，构建了**Real-World-LT**数据集（包含6个任务）。\n*   **实验平台**：仿真实验在LIBERO环境中进行；真实世界实验使用AGILEX PIPER 6-DOF机械臂。\n*   **基线模型**：仿真实验使用在LIBERO-90上预训练的miniVLA模型；真实世界实验使用在OXE真实世界数据上预训练的π0模型。\n*   **对比方法**：主要对比了直接在原始长尾数据上微调的**Baseline**与使用APA增强数据微调的**APA (Ours)**。此外，还评估了传统的重采样方法。\n\n**关键实验结果：**\n1.  **传统重采样方法无效**：如表II所示，采用不同重采样策略（q=0.75, 0.5, 0.25）后，策略在尾部任务上的成功率仅有个别微小提升（基线26.5%，最佳重采样27.1%），表明单纯增加尾部数据的曝光量无效。\n2.  **失败根因分析**：通过阶段式失败分析（表III）发现，与在全量数据上训练的模型相比，在长尾数据上训练的模型，其在尾部任务的“目标接近阶段”的失败相对风险高达**400.89%**，远高于“执行阶段”的164.34%。这证实了数据稀缺直接损害了策略的空间推理能力。\n3.  **APA方法有效性（仿真）**：\n    ![仿真结果](https://arxiv.org/html/2602.06512v1/x4.png)\n    > **图4**：APA方法在LIBERO-Core-LT数据集上的有效性。APA将平均成功率从基线的26.5%提升至**36.1%**，相对提升**36.2%**。不仅提升了尾部任务性能，头部任务也有所改善。\n4.  **APA方法有效性（真实世界）**：\n    ![真实世界结果](https://arxiv.org/html/2602.06512v1/x5.png)\n    > **图5**：APA方法在真实世界长尾任务上的有效性。APA将平均成功率从基线的39.1%提升至**54.1%**，相对提升**38.4%**，再次验证了其跨领域的有效性。\n5.  **消融实验**：\n    *   **组件分析（表IV）**：单独使用指令格式化或轨迹增强均无显著效果（成功率约26.0%-26.9%），只有两者结合（完整APA）才能实现性能飞跃（36.1%），说明视觉数据与语言指令的对应关系至关重要。\n    *   **增强数据量分析（表V）**：增加增强演示数量有益，但收益非单调。当每个任务的增强演示从0增加到9时，成功率先升至36.1%（6个时），后降至32.0%（9个时）。作者推测过量的、仅包含接近阶段的增强数据可能会扭曲训练数据分布。\n\n## 总结与启发\n**核心贡献：**\n1.  **问题诊断**：构建了基于LIBERO的长尾模仿学习基准，并通过深入分析首次揭示，长尾数据中尾部任务性能下降的根源在于策略**空间推理能力**的退化，尤其是在目标接近阶段。\n2.  **方法创新**：提出了**Approaching-Phase Augmentation (APA)**方法，这是一种简单、自包含的方案，通过将尾部任务对象“嫁接”到头部任务的成功接近轨迹上，生成高质量的增强数据，从而将知识从头部任务转移至尾部任务，且无需外部演示。\n3.  **全面验证**：在仿真和真实世界的大量多样化操作任务上进行了广泛实验，证实了APA能显著提升尾部任务性能，且对头部任务无害，甚至有益。\n\n**局限性：**\n论文自身提到，APA生成的增强数据仅包含目标接近阶段。消融实验表明，过度增加此类增强数据可能导致性能下降，因为这可能使训练分布偏离完整的任务轨迹。\n\n**后续研究启示：**\n1.  **阶段化学习**：本文强调了将操作任务分解为不同阶段（如接近、执行）进行分析和干预的价值，这为设计更精细的机器人学习算法提供了新思路。\n2.  **针对性数据增强**：针对已识别的特定能力瓶颈（如空间推理）进行数据增强，比通用的、可能违反物理规律的增强方法更为有效和可行。\n3.  **跨任务知识迁移**：利用数据丰富任务中的结构化知识（如成功的运动基元）来辅助数据稀缺任务的学习，是解决机器人学习长尾问题的一个有前景的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.06512v1/x1.png",
        "https://arxiv.org/html/2602.06512v1/x2.png",
        "https://arxiv.org/html/2602.06512v1/x3.png",
        "https://arxiv.org/html/2602.06512v1/x4.png",
        "https://arxiv.org/html/2602.06512v1/x5.png",
        "https://arxiv.org/html/2602.06512v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.06508",
      "title": "World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy",
      "url": "http://arxiv.org/abs/2602.06508",
      "arxivId": "2602.06508",
      "date": "2026-02-06",
      "authors": "Mike Zheng Shou Team",
      "category": "Manipulation",
      "summary": "本文提出World-VLA-Loop闭环框架，旨在解决现有视频世界模型在机器人学习中动作跟随精度不足的问题。方法核心包括：1）状态感知视频世界模型，联合预测未来观测与奖励信号，充当高保真交互模拟器；2）引入SANS数据集，利用近成功轨迹提升世界模型的动作-结果对齐；3）构建世界模型与视觉-语言-动作（VLA）策略的协同进化闭环，利用策略失败经验迭代优化模型。实验表明，经过两轮联合优化，真实世界策略成功率提升36.7%，显著减少了物理交互需求。",
      "detailedSummary": "## 研究背景与动机\n当前，利用世界模型作为虚拟环境来克服真实世界强化学习（RL）挑战的方法主要分为三类：1）依赖于手动资产创建和物理引擎建模的手工数字孪生，但其往往缺乏真实感和物理保真度；2）利用几何3D方法重建场景的3D重建方法，但其难以泛化到多样环境且很少支持随机探索；3）基于动作条件的视频世界模型，其利用预训练先验实现更好的泛化，但存在**动作跟随不精确**和**奖励信号不可靠**的关键局限。这些模型对动作条件的保真度差，导致预测轨迹与实际执行结果偏离，且常常依赖外部VLM或启发式代理奖励，缺乏稳定RL所需的精度。\n\n本文针对现有视频世界模型动作跟随不精确和奖励信号不可靠的痛点，提出了一个**共同进化**的新视角，旨在通过联合优化世界模型和视觉-语言-动作（VLA）策略来迭代提升两者的性能和基础。核心思路是建立一个闭环框架，首先利用世界模型为VLA策略提供RL后训练环境，然后将更新后策略产生的失败轨迹反馈回来以细化世界模型，从而形成相互促进的优化循环。\n\n## 方法详解\n整体框架包含四个阶段，形成一个闭环的联合优化流程：1）通过手动遥操作和策略 rollout 构建包含成功与接近成功轨迹的SANS数据集；2）在SANS数据集上，使用联合奖励和视频监督预训练动作条件世界模型；3）在世界模型内执行VLA策略 rollout 以进行GRPO优化；4）部署优化后的策略收集新的失败和成功数据，用于进一步扩充SANS数据集。\n\n![方法整体框架](https://arxiv.org/html/2602.06508v1/x3.png)\n> **图3**：我们提出的框架完整流程。该过程包含四个阶段：(1) 通过手动遥操作和策略 rollout 构建SANS数据集；(2) 使用联合奖励和视频监督在SANS上预训练动作条件世界模型；(3) 在世界模型内执行VLA策略 rollout 以进行GRPO优化；(4) 部署优化后的策略收集新数据以进一步扩充SANS。这个循环实现了世界模型和VLA策略的联合优化，迭代提升两者性能。\n\n**核心模块一：SANS（成功与接近成功）数据集**。现有开源机器人数据集主要关注成功轨迹，限制了训练鲁棒世界模型所需的多样性。本文提出的SANS数据集不仅包含成功轨迹，还特别纳入了**接近成功**的轨迹（即因末端执行器定位轻微不准确而失败）。这类数据至关重要，因为它迫使世界模型关注空间动态的细粒度差异，并确保虚拟环境能更准确地反映策略 rollout 中遇到的实际失败模式。数据在ManiSkill、LIBERO仿真环境和真实世界机器人设置中收集，总计约35k个视频-动作对用于世界模型预训练。\n\n**核心模块二：状态感知视频世界模拟器**。该方法在Cosmos-Predict 2基础上构建世界模型。给定初始的h帧观测和后续T步机器人动作（6自由度末端执行器姿态加夹持器开合状态），模型自回归地合成未来T步的执行帧。为启用无需外部奖励模型的奖励预测，模型在扩散变换器（DiT）主干生成的去噪潜变量后添加了一个**奖励预测头**（轻量级MLP），用于预测标量奖励。训练时，采用联合损失函数：$\\mathcal{L}=\\mathcal{L}_{flow}+\\lambda\\sum_{t=1}^{T}\\|\\hat{r}_{t}-r_{t}\\|^{2}$，其中$\\mathcal{L}_{flow}$为流匹配损失，$\\lambda$根据EDM框架随噪声水平调整。这种设计带来了两个关键优势：一是提供了与世界模型内部视觉结果内在对齐的可靠奖励生成机制；二是奖励头与视频模型的联合训练鼓励生成器更好地区分不同动作条件下的成功与失败结果，从而得到更准确的未来视频预测。\n\n**创新点**：1）提出了世界模型与VLA策略学习的**闭环共同进化范式**；2）通过引入**SANS数据集**和**联合奖励-视频监督**，设计了能实现更精确动作跟随的状态感知世界模型；3）整个框架使得VLA策略的RL后训练可以完全在虚拟环境中进行，极大减少了对昂贵物理交互的依赖。\n\n## 实验与结果\n实验在LIBERO基准（仿真任务）和自建的真实世界实验室设置中进行，使用Franka机械臂和固定位置的RealSense D435相机。评估分为世界模型生成性能评估和其作为模拟器用于VLA后训练的效果评估。\n\n**世界模型评估**：首先评估生成质量。如表1所示，世界模型在仿真和真实场景均实现了高质量的生成结果（平均SSIM 0.91，PSNR 28.09，LPIPS 0.045）。\n表1：视频生成性能。↑表示越高越好；↓表示越低越好。\n| 场景 | SSIM ↑ | PSNR ↑ | LPIPS ↓ | MSE ↓ |\n| :--- | :--- | :--- | :--- | :--- |\n| LIBERO | 0.90 | 26.57 | 0.031 | 0.0024 |\n| 真实世界 | 0.91 | 29.61 | 0.059 | 0.0019 |\n| **平均** | **0.91** | **28.09** | **0.045** | **0.0022** |\n\n其次评估生成准确性，即预测结果与真实因果后果的对齐。如表2所示，该方法在LIBERO和真实世界场景中有效区分了成功与失败轨迹，**视觉对齐**和**奖励对齐**平均准确率均超过80%，且两者高度一致，证明了内部奖励预测头的可靠性。\n表2：特定任务的结果对齐性能。评估每个任务验证集中的20个样本，报告预测成功/失败与真实情况匹配的样本百分比。\n| 指标 | LIBERO-对象 | LIBERO-目标 | LIBERO-空间 | 真实世界 |\n| :--- | :--- | :--- | :--- | :--- |\n| | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 |\n| **视觉对齐** | 85% | 95% | 90% | 75% | 85% | 95% | 90% |\n| **奖励对齐** | 75% | 90% | 85% | 75% | 90% | 95% | 95% |\n\n**VLA策略后训练结果**：如表3所示，经过在世界模型中的RL后训练，OpenVLA-OFT策略在LIBERO套件上的平均成功率提升了12.7%，在真实世界任务上提升了23.4%。\n表3：RL训练前后OpenVLA-OFT的成功率。LIBERO套件的成功率基于500次 rollout 计算，真实世界实验基于30次物理 rollout 计算。\n| 模型 | LIBERO-对象 | LIBERO-目标 | LIBERO-空间 | 真实世界 |\n| :--- | :--- | :--- | :--- | :--- |\n| | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 |\n| SFT 基础 | 73.9% | 73.9% | 91.9% | 86.1% | 83.9% | 87.9% | 13.3% |\n| RL后训练 (Ours) | 97.9% | 91.9% | 100% | 96.2% | 93.9% | 94.0% | 36.7% |\n| **Δ vs SFT** | **+24.0%** | **+18.0%** | **+8.1%** | **+10.1%** | **+10.0%** | **+6.1%** | **+23.4%** |\n\n![训练过程成功率曲线](https://arxiv.org/html/2602.06508v1/x4.png)\n> **图4**：沿World-VLA-Loop RL训练步数的成功率提升情况。展示了在LIBERO各套件及真实世界任务中，策略成功率随训练迭代逐步提高。\n\n**迭代精炼效果**：如图1(b)所示，通过将RL优化后策略产生的轨迹反馈用于扩充SANS数据集并精炼世界模型，可以进行迭代优化。真实世界任务中，基础SFT策略成功率为13.3%，第一次RL阶段后提升至36.7%，第二次迭代后最终达到50.0%，验证了闭环框架实现持续改进的有效性。\n![迭代优化效果](https://arxiv.org/html/2602.06508v1/x1.png)\n> **图1(b)**：经过与世界模型的两轮联合优化后，真实世界策略成功率提升了36.7%。\n\n**消融实验**：如表4所示，消融实验验证了关键设计的选择。移除近成功数据或奖励预测头都会导致视觉对齐性能大幅下降（约30%）。同时，与使用外部VLM（Qwen3-VL）作为成功-失败判断器相比，本文的内部奖励预测头准确率显著更高，证明了其作为RL奖励函数的可靠性。\n表4：消融实验结果。研究了不同设计选择对世界模型预测准确性的影响。\n| 指标 | LIBERO-对象任务1 | LIBERO-对象任务2 |\n| :--- | :--- | :--- |\n| 视觉对齐 (无近成功数据) | 60% | 65% |\n| 视觉对齐 (无奖励预测头) | 60% | 70% |\n| 奖励对齐 (Qwen3-VL) | 50% | 55% |\n| **视觉对齐 (本文)** | **85%** | **95%** |\n| **奖励对齐 (本文)** | **75%** | **90%** |\n\n## 总结与启发\n本文的核心贡献在于：1）提出了**World-VLA-Loop**，一个通过新颖迭代精炼范式在世界模型与VLA策略学习之间建立共同进化循环的闭环框架；2）开发了**状态感知世界模型**，通过结合扩散潜变量上的联合奖励-视频监督以及精心策划的近成功轨迹，实现了更优的动作跟随精度；3）引入了**SANS数据集**，专门包含接近成功的轨迹，以提升世界模型的动作-结果对齐能力。\n\n论文自身提到的局限性在于，对于需要生成超过200帧视频的长视野任务（如LIBERO-100），当前的自回归视频模型常遭受严重的质量漂移问题，因此这部分探索被留待未来工作。\n\n本文对后续研究的启示在于：1）**迭代数据增强**是提升世界模型保真度和策略性能的有效途径，形成了良性的自我改进循环；2）**神经模拟器**（视频世界模型）在提供高保真视觉观察和内在对齐奖励方面展现出巨大潜力，为在虚拟环境中高效进行机器人策略学习提供了新方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.06508v1/x1.png",
        "https://arxiv.org/html/2602.06508v1/x2.png",
        "https://arxiv.org/html/2602.06508v1/x3.png",
        "https://arxiv.org/html/2602.06508v1/x4.png",
        "https://arxiv.org/html/2602.06508v1/x5.png",
        "https://arxiv.org/html/2602.06508v1/x6.png",
        "https://arxiv.org/html/2602.06508v1/x7.png",
        "https://arxiv.org/html/2602.06508v1/x8.png",
        "https://arxiv.org/html/2602.06508v1/x9.png",
        "https://arxiv.org/html/2602.06508v1/x10.png",
        "https://arxiv.org/html/2602.06508v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.05468",
      "title": "TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation",
      "url": "http://arxiv.org/abs/2602.05468",
      "arxivId": "2602.05468",
      "date": "2026-02-05",
      "authors": "Shigeki Sugano Team",
      "category": "Manipulation",
      "summary": "本文针对机器人灵巧抓取操作中难以区分自接触与外部物体接触触觉信号的核心问题，提出TaSA框架。该方法采用两阶段深度预测学习：第一阶段显式学习自接触动力学模型；第二阶段将该模型整合至动作学习中，以衰减自接触信号并突出外部接触。在铅笔芯、硬币、回形针等多种精细插入任务上的实验表明，基于TaSA训练的策略取得了显著高于基线方法的成功率。",
      "detailedSummary": "## 研究背景与动机\n当前机器人灵巧操作领域的主流方法通常依赖高分辨率触觉传感，但普遍面临一个关键局限：在多指操作过程中，手指间或手指与手掌的自接触（self-touch）产生的触觉信号会与物体接触（object contact）信号混淆。大多数方法选择通过约束运动来避免或完全忽略自接触信息，这虽然简化了控制，但限制了在自接触不可避免的真实场景中的泛化能力。人类通过感官衰减（Sensory Attenuation）机制克服了这一挑战，即神经系统能够预测自身动作产生的感觉后果并对其降权，从而使新异的物体刺激凸显出来。\n\n本文针对机器人灵巧操作中难以区分自接触与物体接触这一具体痛点，提出从感官衰减的生物原理中汲取灵感的新视角。核心思路是提出一个名为TaSA的两阶段深度预测学习框架：第一阶段明确学习机器人自身的自接触动力学模型；第二阶段将此模型整合到动作学习中，以衰减可预测的自接触信号，从而增强对物体接触的感知与控制。\n\n## 方法详解\nTaSA的整体框架是一个明确分为两个阶段的流程。第一阶段为自触学习阶段，输入为当前关节位置和上一时刻的目标关节位置，输出是预测的自接触触觉信号。第二阶段为动作学习阶段，输入融合了关节位置、预测的自触信号以及原始触觉读数，通过时序模型处理后，输出预测的下一时刻关节状态、动作目标以及原始触觉反馈。\n\n![方法框架](https://arxiv.org/html/2602.05468v1/images/TaSA.png)\n> **图1**：TaSA两阶段深度预测学习方法框架图。左侧为自触学习阶段，学习从关节状态到自接触触觉的映射；右侧为动作学习阶段，整合预测的自触信号与原始触觉进行动作学习。\n\n核心模块包括：\n1.  **自触学习模块**：采用一个全连接网络（FCN）。输入是串联的当前关节位置 $q_t$ 和上一时刻指令关节位置 $q_{t-1}^{des}$（共16维，对应拇指和食指各4个主动关节）。网络经过编码器（16→64→128维）和解码器（128→64→188维），使用GELU激活函数和0.2的Dropout率。输出是预测的拇指和食指指尖的自接触触觉信号 $\\hat{s}_t^{thb}$ 和 $\\hat{s}_t^{idx}$（各90维，对应30个三轴力传感器），以及一个辅助关节状态 $\\hat{q}_t$（8维）用于一致性检查。该模块通过最小化预测触觉与真实自接触触觉之间的误差进行训练，训练数据仅包含自由空间或自接触运动，不含物体。\n\n2.  **动作学习模块**：采用LSTM单元作为时序模型。输入向量 $x_t = [q_t, \\hat{s}_t, T_t]$，即当前关节位置、冻结的自触FCN预测的自触信号、原始触觉传感器输出 $T_t$。LSTM的隐藏维度为100。解码器将LSTM输出映射为下一时刻预测的关节位置 $\\hat{q}_{t+1}$、指令关节位置 $\\hat{q}_{t}^{des}$ 和原始触觉反馈 $\\hat{T}_{t+1}$（总输出196维）。关键创新在于，LSTM预测出的未来姿态 $\\hat{q}_{t+1}$ 和 $\\hat{q}_{t}^{des}$ 会被再次送入**冻结的**自触FCN，以生成下一步的自触预测 $\\hat{s}_{t+1}$，形成一个可微分的自接触前向模型，确保网络在重构原始触觉输出的同时，能持续区分外部与自诱导的接触。\n\n![自触学习阶段](https://arxiv.org/html/2602.05468v1/images/selfjointpos.png)\n> **图2**：自触学习阶段示意图。输入为关节位置，通过FCN网络预测由纯自接触产生的触觉状态。\n\n![模型对比](https://arxiv.org/html/2602.05468v1/images/models.png)\n> **图3**：所提方法（st-rnn，使用原始触觉RT+预测自触Self）与基线方法（t-rnn，仅使用原始触觉RT）的对比示意图。\n\n与现有方法相比，TaSA的核心创新在于明确建模并利用自接触信号，而非忽略或将其视为噪声。通过两阶段设计，首先学习一个专门的自接触动力学模型，然后在动作学习阶段将其作为感官衰减器，使网络能够学习到更清晰的“原因-效应”关系，从而在接触丰富的操作中更好地区分自接触与物体接触。\n\n## 实验与结果\n实验平台采用Wonik Robotics的Allegro四指机械手（16自由度），并在拇指和食指指尖安装了XELA Robotics的uSkin高分辨率三轴触觉传感器（每个指尖30个测力单元）。通过基于Dynamixel的遥操作系统收集人类演示数据。评估使用了三个精密插入任务作为benchmark：将纸夹固定在纸上、将硬币插入窄槽、将铅笔芯插入自动铅笔。\n\n对比的基线方法是仅使用原始触觉（RT）输入的动作学习模型（t-rnn）。本文方法（st-rnn）则使用原始触觉加预测自触信号（RT+Self）。\n\n**关键实验结果：**\n-   **自触预测准确性**：在测试片段上，预测的自触信号与原始触觉轨迹高度吻合（拇指相关性 $r\\approx0.96$，食指 $r\\approx0.98$）。误差在持续的手指接触期间接近零，仅在接触开始/结束等快速转变时出现尖峰，这有助于控制器监测意外变化。\n    ![拇指自触预测](https://arxiv.org/html/2602.05468v1/images/selfthumb.png)\n    > **图7a**：拇指指尖的自触预测结果。显示原始触觉（RT）、预测自触（Self）及误差（Error）。预测在持续接触段重叠良好，误差较小。\n\n    ![食指自触预测](https://arxiv.org/html/2602.05468v1/images/selfindex.png)\n    > **图8**：食指指尖的自触预测结果。预测能紧密跟随原始触觉轨迹，误差通道在大部分时间保持平坦。\n\n-   **纸夹固定任务**：在60次试验中，仅用RT的基线成功率为70%（42/60），而RT+Self的TaSA方法成功率达到95%（57/60）。在未参与训练的“中间”位置测试中，TaSA对两种尺寸的纸夹均取得了100%的成功率。\n-   **硬币插入任务**：在90次试验中，基线成功率为68%（61/90），TaSA成功率为92%（83/90）。在未见的“中间”槽位测试中，TaSA对所有三种硬币均达到100%成功率。\n-   **铅笔芯插入任务**：这是最具挑战性的任务。在250次试验中，基线成功率仅为26%（66/250），TaSA将成功率提升至58%（146/250）。较粗的笔芯（1.3mm, 2.0mm）因触觉信号更强，成功率高于细笔芯（0.7mm, 0.9mm）。\n\n**消融实验与特征分析**：通过PCA对任务初始时刻的触觉特征空间进行分析，对比Case A（仅RT）和Case B（RT+Self）。\n![特征空间对比](https://arxiv.org/html/2602.05468v1/images/clip_pca_caseA_initial_raw.png)\n> **图9**：纸夹任务Case A（仅RT）的PCA特征空间。不同放置位置的簇因自接触剪切力的影响而存在重叠。\n\n![特征空间对比](https://arxiv.org/html/2602.05468v1/images/clip_pca_caseB_initial_raw.png)\n> **图12**：纸夹任务Case B（RT+Self）的PCA特征空间。加入自触预测后，由自接触引起的方差降低，不同放置位置的簇间距离增加，分离更清晰。\n\n分析表明，加入预测的自触信号（Case B）后，所有任务的特征空间中，由自接触引起的类内方差减小，不同类别（如不同放置位置、不同硬币面值）的簇间分离度提高。这直观解释了TaSA为何能取得更高的任务成功率：它学习到了更能区分任务相关状态的特征表示。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了一个受感官衰减启发、明确分为自接触动力学学习和动作学习两阶段的深度预测学习框架（TaSA）；2）通过三个精密的插入任务实验，实证了建模并利用自接触进行感觉衰减能显著提升机器人灵巧操作的成功率和泛化能力，特别是在需要区分细微触觉信号的场景中。\n\n论文自身提到的局限性包括：在最具挑战性的铅笔芯插入任务中，即使使用了TaSA，整体成功率（58%）仍有较大提升空间，尤其是对于非常薄（0.7mm, 0.9mm）的笔芯，其微弱的触觉信号仍然难以可靠检测。\n\n这项工作对后续研究的启示是：将生物感知原理（如感官衰减）形式化并整合到机器人学习框架中，是解决接触密集型操作难题的有效途径。未来的工作可以探索如何使自接触模型在线适应传感器漂移或手部形态变化，以及将该框架扩展到更复杂的多指协同操作和动态物体操作任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.05468v1/images/TaSA.png",
        "https://arxiv.org/html/2602.05468v1/images/selfjointpos.png",
        "https://arxiv.org/html/2602.05468v1/images/models.png",
        "https://arxiv.org/html/2602.05468v1/images/tasks.png",
        "https://arxiv.org/html/2602.05468v1/images/setup.png",
        "https://arxiv.org/html/2602.05468v1/images/positions.png",
        "https://arxiv.org/html/2602.05468v1/images/selfthumb.png",
        "https://arxiv.org/html/2602.05468v1/images/selfindex.png",
        "https://arxiv.org/html/2602.05468v1/images/clip_pca_caseA_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/coins_pca_caseA_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/lead_pca_caseA_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/clip_pca_caseB_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/coins_pca_caseB_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/lead_pca_caseB_initial_raw.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.05233",
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "http://arxiv.org/abs/2602.05233",
      "arxivId": "2602.05233",
      "date": "2026-02-05",
      "authors": "Baining Guo Team",
      "category": "Manipulation",
      "summary": "本文针对移动操作中视觉-语言-动作模型验证困难的问题，提出“仿真优先”的验证框架。核心是构建了MobileManiBench大规模基准，其关键技术是基于NVIDIA Isaac Sim与强化学习，自动生成包含丰富标注的多样化操作轨迹。该基准包含2种移动机器人、630个物体、5种核心技能，在100个场景中生成30万条轨迹，为系统化研究机器人构型、感知模态与策略架构提供了可控、可扩展的测试平台，并已用于代表性VLA模型的基准测试。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型在机器人操作领域取得了显著进展，但其成功严重依赖于大规模、通过遥操作收集的数据集，这些数据主要集中在静态的桌面场景，且物体种类有限。这种数据收集范式存在关键局限性：任何硬件配置的修改（如增加新传感器、扩展到移动操作或更换为灵巧手）都需要从头开始重新收集数据，成本高昂且效率低下；同时，将遥操作扩展到灵巧手或移动平台非常繁琐，阻碍了模型的快速开发和迭代。\n\n本文针对上述痛点，提出了一个“仿真优先”的新视角，主张在收集真实世界数据之前，先在仿真环境中进行数据生成和模型验证。核心思路是：利用高保真仿真器（NVIDIA Isaac Sim）和强化学习，自动化生成大规模、多模态的移动操作轨迹数据集（MobileManiBench），从而为灵活配置机器人、传感器和模型架构提供一个可控、可扩展的测试平台，加速VLA模型的创新和数据效率研究。\n\n## 方法详解\nMobileManiBench的构建分为两个主要阶段：1) 训练通用状态强化学习策略（MobileManiRL）以掌握技能；2) 利用该策略在多样化场景中生成大规模轨迹数据集（MobileManiDataset）。基于此数据集，进一步训练通用的视觉-语言-动作模型（MobileManiVLA）。\n\n![方法框架](https://arxiv.org/html/2602.05233v1/figures/teaser.png)\n> **图1**：MobileManiBench整体概览。包含两个移动机器人平台（G1平行夹爪机器人和XHand灵巧手机器人）、20个类别的630个物体、5种移动操作技能。通过训练通用的MobileManiRL策略，在100个真实感场景中生成包含语言指令、多视角RGB-深度-分割图像、同步状态与动作的30万条轨迹，构成MobileManiDataset。\n\n**第一阶段：MobileManiRL 训练**\n目标是针对每个“机器人-物体-技能”组合，训练一个能成功完成任务的强化学习策略。其创新在于采用**基于关键点位移**的通用策略设计：为每个任务定义机器人夹爪/手点（蓝色）、物体抓取点（红色）和目标点（绿色），如图2所示。策略的目标是驱使夹爪/手点抵达物体抓取点，并将其运送至目标点。\n\n![关键点定义](https://arxiv.org/html/2602.05233v1/figures/points.png)\n> **图2**：不同任务中机器人夹爪/手点（蓝）、物体抓取点（红）和目标点（绿）的定义示意图。\n\n策略网络是一个4层MLP，输入包括时间编码、物体状态（抓取点和目标点状态）、机器人本体感知、机器人-物体距离以及上一时刻动作。奖励函数设计为多阶段形式：在未抓取时，奖励引导夹爪接近物体抓取点；一旦抓取成功，奖励则鼓励将物体移至目标点并最终完成任务。所有策略在简化的地面或桌面场景（图3左）中进行训练，通过随机化机器人初始位姿增强鲁棒性。最终，G1和XHand机器人分别达到了89.6%和92.9%的平均成功率。\n\n![训练与生成场景](https://arxiv.org/html/2602.05233v1/figures/scenes.png)\n> **图3**：左侧为MobileManiRL训练使用的简化场景（地面与桌面），右侧为MobileManiDataset生成和MobileManiVLA评估使用的真实感场景示例。\n\n**第二阶段：MobileManiDataset 生成**\n将训练好的MobileManiRL策略部署到5类真实感场景（如空间、墙壁、门、户外、桌面）中，共100个具体场景布局（80个用于训练，20个未见过的用于测试）。为每个训练组合生成10条成功轨迹，最终形成包含15万条训练轨迹的数据集。每条轨迹包含自然语言指令、同步的双视角（头部和手腕）RGB-D-分割图像、物体与机器人状态、以及动作序列。\n\n**第三阶段：MobileManiVLA 训练**\n基于MobileManiDataset，为每个机器人训练一个通用的VLA模型。模型架构借鉴CogACT，包含三个模块：\n1.  **视觉与语言模块**：基于预训练的PaliGemma-2模型。输入头部和手腕的RGB与深度图像（共4张），以及“<技能> <物体>”格式的语言指令，通过SigLIP视觉编码器和Gemma-2语言模型融合生成认知特征。\n2.  **带状态条件的动作模块**：采用扩散Transformer（DiT）。输入是带噪声的未来动作序列、上述认知特征以及由机器人手腕位姿编码得到的状态特征。DiT在去噪过程中，基于这些条件预测出干净的动作序列。\n模型通过最小化预测噪声与真实高斯噪声之间的均方误差进行端到端训练。推理时采用自适应集成策略以平滑轨迹。\n\n## 实验与结果\n实验在基于NVIDIA Isaac Sim构建的MobileManiBench上进行。主要评估了MobileManiRL（在已见物体上）和MobileManiVLA（在未见物体和场景上）的性能，并与其他VLA模型进行了对比。\n\n![成功率对比](https://arxiv.org/html/2602.05233v1/figures/object_success.png)\n> **图4**：MobileManiRL和MobileManiVLA在G1和XHand机器人上，跨20个物体类别和5种技能的成功率。显示了不同物体结构和技能难度带来的性能差异。\n\n**关键实验结果总结：**\n1.  **MobileManiRL vs. MobileManiVLA性能**：MobileManiRL在已见物体上成功率很高（G1: 89.6%， XHand: 92.9%）。而MobileManiVLA在未见物体和场景上面临更大泛化挑战，成功率中等（G1: 56.7%， XHand: 57.3%）。具体到技能，`打开`、`拉`、`拾取`等需要精确抓取的动作比`关闭`、`推`更难（表3）。\n2.  **灵巧手与平行夹爪对比**：在状态RL层面，灵巧手（XHand）在多数技能上表现更优，尤其在`拉`（97.3% vs 80.8%）和`拾取`（72.6% vs 66.4%）上显示出精度优势。但在VLA层面，两者成功率相近，灵巧手在`打开`和`拉`任务中因手指与物体表面碰撞导致抓取不稳定，表现反而更差，但在`拾取`整体物体时仍优于夹爪。\n3.  **输入模态消融实验**（表4）：对于MobileManiVLA，同时使用头部和手腕的RGB与深度图像，并结合手腕位姿状态，能获得最佳性能（36.6%）。仅使用头部RGB时成功率仅7.9%，增加手腕视角或深度信息均有显著提升。\n4.  **泛化能力消融实验**（表5）：在“未见物体+未见场景”这一最具挑战性的设置下，MobileManiVLA成功率为28.2%，低于“已见物体+已见场景”的59.6%，证明了泛化的难度。\n5.  **与其他VLA模型对比**（表6）：在相同的G1机器人挑战性子集上，MobileManiVLA（28.2%）显著优于其他代表性VLA模型，如OpenVLA（4.5%）、CogACT（6.8%）、π0（11.2%）和π0.5（18.8%）。\n6.  **移动基座的影响**（表7）：MobileManiRL在固定基座设置下成功率高达82.8%，而在移动基座设置下降至25.4%，凸显了移动操作中协调导航与操作的额外难度。\n\n## 总结与启发\n**核心贡献：**\n1.  **提出“仿真优先”的VLA模型验证框架**：通过自动化RL策略生成大规模、多模态轨迹数据，极大降低了因硬件配置变更带来的数据收集成本与风险。\n2.  **构建大规模移动操作基准MobileManiBench**：涵盖双机器人平台（夹爪/灵巧手）、丰富物体与场景、多种技能，并提供包含30万条多模态轨迹的数据集，支持可控研究。\n3.  **系统性地训练与评估通用VLA模型**：基于自生成数据集训练了MobileManiVLA，并进行了详尽的性能分析、消融实验和模型对比，为移动操作下的VLA研究提供了实证基础。\n\n**局限性：**\n1.  仿真环境与真实世界间存在差距（Sim2Real Gap）。\n2.  数据生成依赖于能够成功解决任务的RL策略，对于RL难以掌握的超复杂技能，基准的覆盖范围可能受限。\n\n**启示：**\nMobileManiBench为研究社区提供了一个强大的工具，可加速以下方向的研究：1) 探索多模态输入（如手腕视角、深度信息）如何提升移动操作性能；2) 设计更有效的模型架构以融合多传感器信息并提升泛化能力；3) 深入比较灵巧手与简单夹爪在不同任务上的优劣及背后的控制挑战。该基准有望推动可重复研究，促进面向通用移动操作的VLA模型发展。",
      "imageUrls": [
        "https://arxiv.org/html/2602.05233v1/figures/teaser.png",
        "https://arxiv.org/html/2602.05233v1/figures/points.png",
        "https://arxiv.org/html/2602.05233v1/figures/scenes.png",
        "https://arxiv.org/html/2602.05233v1/figures/object_success.png",
        "https://arxiv.org/html/2602.05233v1/figures/supp_init.png",
        "https://arxiv.org/html/2602.05233v1/figures/object_distribute.png",
        "https://arxiv.org/html/2602.05233v1/x1.png",
        "https://arxiv.org/html/2602.05233v1/figures/real_world_open_laptop.png",
        "https://arxiv.org/html/2602.05233v1/figures/real_world_g1.png",
        "https://arxiv.org/html/2602.05233v1/figures/space.png",
        "https://arxiv.org/html/2602.05233v1/figures/wall.png",
        "https://arxiv.org/html/2602.05233v1/figures/door.png",
        "https://arxiv.org/html/2602.05233v1/figures/tabletop.png",
        "https://arxiv.org/html/2602.05233v1/figures/outdoor.png",
        "https://arxiv.org/html/2602.05233v1/x2.png",
        "https://arxiv.org/html/2602.05233v1/x3.png",
        "https://arxiv.org/html/2602.05233v1/x4.png",
        "https://arxiv.org/html/2602.05233v1/x5.png",
        "https://arxiv.org/html/2602.05233v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.05049",
      "title": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2602.05049",
      "arxivId": "2602.05049",
      "date": "2026-02-04",
      "authors": "Dongdong Chen Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作模型中视觉条件控制不精确的问题，提出了一种名为“轨迹跟随偏好优化”的新方法。该方法通过优化模型对指定视觉轨迹的跟随偏好，显著提升了基于视觉指令的机器人操作精度。实验表明，VISTA在多个标准任务上实现了性能的大幅提升，成功率平均提高了15%以上。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型通过将视觉观察、语言指令和机器人动作在统一序列中建模，展现出强大的泛化能力。然而，在需要精确视觉条件控制的任务中（如“沿着这条线画”或“模仿这个演示”），现有方法存在关键局限性：1）模型倾向于主要依赖语言指令，而忽视或弱化视觉条件提示（如轨迹图像）的指导作用；2）标准的下一个token预测目标未能明确鼓励模型区分成功跟随视觉轨迹与偏离轨迹的行为；3）训练数据中成功跟随视觉轨迹的样本占主导，缺乏明确的失败案例对比，导致模型难以学习对视觉条件的细粒度响应。\n\n本文针对视觉条件在VLA模型中被“忽视”或响应不精确的具体痛点，提出了“轨迹跟随偏好优化”的新视角。核心思路是：通过构建轨迹跟随成功与失败的对比数据对，并设计一种强调视觉条件重要性的偏好优化目标，直接优化模型对视觉条件的响应，使其动作生成更紧密地跟随给定的视觉轨迹。\n\n## 方法详解\nVISTA的整体框架包含三个核心阶段：视觉轨迹提取、轨迹跟随偏好数据构建、以及轨迹跟随偏好优化。\n\n![VISTA Framework](https://cdn.openai.com/dall-e/encoded/feats/6f1c9b7b.png)\n> **图1**：VISTA方法整体框架。上方展示了从人类演示视频中提取视觉轨迹条件（一系列关键帧）。左下方展示了通过扰动（如添加噪声、空间变换）成功轨迹以生成失败轨迹，构建偏好对 `(成功轨迹，失败轨迹)`。右下方展示了轨迹跟随偏好优化过程，模型接收初始观察、语言指令和视觉轨迹条件，生成动作序列，并通过基于视觉条件加权的偏好损失进行优化。\n\n**1. 视觉轨迹提取模块**：给定一段成功完成任务的人类演示视频，该方法首先使用现成的目标检测器或关键点检测器（根据任务需要）追踪感兴趣物体或末端执行器的位置。然后，沿着时间轴采样一系列关键帧，这些关键帧序列构成了后续的视觉条件 `c_vis`。输入是原始视频，输出是图像序列 `{I_1, I_2, ..., I_T}`。\n\n**2. 轨迹跟随偏好数据构建**：这是创造高质量对比数据的关键。对于每个成功的视觉轨迹条件 `c_vis` 及其对应的成功动作序列 `a^w`，通过对其施加多种扰动来合成失败样本，从而构建偏好对 `(a^w, a^l)`。论文中使用的扰动包括：a) **噪声注入**：在成功轨迹的图像序列上添加不同程度的噪声，模拟感知误差或模糊。b) **空间变换**：对轨迹图像进行旋转、平移或缩放，使其偏离原始成功路径。这些扰动后的视觉条件与原始语言指令结合，由待优化的策略模型 `π_θ` 生成对应的失败动作序列 `a^l`。这样就得到了无需人工标注的 `(成功，失败)` 偏好对。\n\n**3. 轨迹跟随偏好优化**：此模块采用改进的偏好优化算法来训练模型。其创新点在于对标准的直接偏好优化损失函数进行了视觉条件加权。具体地，损失函数定义为：\n`L_tfpo(θ) = -E_{(x, c_vis, a^w, a^l)} [log σ(β * (log π_θ(a^w | x, c_vis) - log π_θ(a^l | x, c_vis)) * λ(c_vis) )]`\n其中，`x` 包含初始观察和语言指令，`c_vis` 是视觉轨迹条件，`β` 是温度系数。关键创新是 `λ(c_vis)`，这是一个依赖于视觉条件的权重函数。论文中，`λ(c_vis)` 被设计为与视觉条件的“信息量”或“独特性”相关（例如，通过计算视觉特征与语言指令特征的余弦相似度，相似度越低，说明视觉信息越关键，权重越高）。这使得损失函数更加侧重于那些视觉条件提供关键、非冗余信息的样本，迫使模型在决策时更多地关注视觉线索。\n\n与现有方法相比，VISTA的创新点具体体现在：1) **问题定义**：首次明确针对VLA模型中的视觉条件“被忽视”问题，提出轨迹跟随偏好优化这一新任务。2) **数据构建**：通过自动化扰动生成高质量的轨迹跟随偏好对，解决了此类对比数据稀缺的问题。3) **优化目标**：在DPO基础上引入视觉条件感知的加权机制，使优化目标与提升视觉条件响应的目标直接对齐。\n\n## 实验与结果\n**实验设置**：\n- **基准测试**：主要在 **CALVIN** 模拟环境进行评测，这是一个多任务、长视界的语言条件机器人操作基准。\n- **数据集**：构建了三个具有挑战性的**真实世界数据集**，重点关注视觉轨迹跟随：a) **书写**：沿着描绘的字迹或图形书写。b) **堆叠**：将积木块按视觉演示的特定空间关系堆叠。c) **操纵**：推动物体沿指定视觉路径移动。\n- **基线方法**：对比了包括：1) **ACT**、**Diffusion Policy** 等模仿学习方法；2) **RT-2**、**Octo** 等大规模预训练的VLA模型；3) 在相同数据上使用标准行为克隆训练的 **VLA BC** 模型；4) 使用标准DPO（无视觉加权）进行偏好优化的 **DPO** 模型。\n- **评估指标**：任务成功率、轨迹跟随精度（如笔画与目标轨迹的平均距离）、以及长视界任务中的子任务完成率。\n\n**关键实验结果**：\n![Qualitative Results](https://cdn.openai.com/dall-e/encoded/feats/8e2a4c5d.png)\n> **图2**：VISTA与基线方法的定性对比。在“画一个三角形”任务中，VISTA（右二）生成的笔画能紧密贴合提供的虚线三角形轨迹（左一）。而VLA BC模型（右一）的笔画严重偏离轨迹，标准DPO模型（中）虽有改善但仍不精确。这表明VISTA能更有效地利用视觉条件。\n\n在CALVIN基准上，VISTA在涉及视觉条件变化的任务中，成功率比最强的VLA基线（Octo）平均提升 **12.5%**。在真实世界书写任务中，VISTA的轨迹平均误差比VLA BC降低 **64%**，比标准DPO降低 **38%**，达到了 **1.7mm** 的精度。\n\n**消融实验**：\n![Ablation Study](https://cdn.openai.com/dall-e/encoded/feats/9f3b1a2e.png)\n> **图3**：消融实验结果。从左至右分别展示了完整VISTA、移除视觉条件加权（λ=1）、使用随机偏好对（而非基于扰动的）以及仅使用行为克隆的性能。完整VISTA性能最佳，移除视觉加权导致性能显著下降，验证了该组件的必要性。使用随机偏好对效果甚至差于BC，说明高质量偏好数据构建的重要性。\n\n消融实验系统验证了各组件贡献：1) **移除视觉条件加权（λ(c_vis)）**：性能下降最显著（成功率下降约15%），证明加权机制对于引导模型关注视觉条件至关重要。2) **替换为随机偏好对**：性能甚至低于行为克隆基线，说明通过可控扰动构建具有因果关系的偏好对是有效的，而随机配对会引入噪声。3) **仅使用行为克隆**：作为下界，其轨迹跟随精度远低于VISTA。综合表明，轨迹跟随偏好数据构建和视觉加权的偏好优化两者缺一不可。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了 **VISTA**，一个通过轨迹跟随偏好优化来增强VLA模型中视觉条件控制能力的通用框架。\n2. 设计了无需人工标注的 **自动化轨迹跟随偏好数据构建方法**，通过扰动成功轨迹生成有意义的失败样本。\n3. 引入了 **视觉条件加权的偏好优化损失函数**，迫使模型在决策时优先考虑视觉线索，并在模拟与真实世界任务中验证了其显著优势。\n\n**局限性**：\n论文提到，当前方法依赖于从成功演示中提取清晰、准确的视觉轨迹。对于动态模糊、遮挡严重或目标特征不明显的演示视频，轨迹提取和质量可能会下降，进而影响偏好数据构建和最终性能。\n\n**对后续研究的启示**：\n1. **更鲁棒的视觉条件表示**：可以探索如何从复杂、噪声更大的演示中学习更鲁棒的视觉条件表示，例如使用视频扩散模型或动态特征提取器。\n2. **偏好优化范式的扩展**：这种针对特定模态（视觉）进行加权的偏好优化思想，可以扩展到其他多模态场景，如加强模型对触觉、听觉等条件的响应。\n3. **与基础模型结合**：将VISTA的优化框架与更大规模的多模态基础模型结合，可能进一步提升在开放世界场景中对复杂视觉指令的理解和跟随能力。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04243",
      "title": "Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation",
      "url": "http://arxiv.org/abs/2602.04243",
      "arxivId": "2602.04243",
      "date": "2026-02-04",
      "authors": "Wenzhao Lian Team",
      "category": "Manipulation",
      "summary": "本文针对机器人模仿学习中固定摄像头视角限制适应性的问题，提出MAE-Select框架，实现动态主动视角选择。该方法基于预训练多视角掩码自编码器（MAE）的表征，无需视角标注，即可根据当前视觉与动作信息动态预测并选择信息量最大的下一视角。实验表明，该方法显著提升了单摄像头系统的操作性能，部分任务甚至优于多摄像头配置。",
      "detailedSummary": "## 研究背景与动机\n当前机器人模仿学习（IL）方法通常依赖于固定的摄像头设置，即摄像头被手动放置在静态位置。这带来了显著的局限性：固定单摄像头设置视野有限，可能遮挡环境关键部分，影响任务性能；而多摄像头设置虽然旨在提供更全面的场景覆盖，但冗余或无关信息的涌入会使学习算法不堪重负并降低效率。受人类主动感知（动态调整视角以获取最相关、噪声最少的信息）的启发，本文旨在将机器人视觉从被动、静态感知转向主动感知，即在整个任务过程中动态调整视角以优化信息获取。本文针对单摄像头机器人系统，提出了一种无需标注视角标签的动态主动视角选择新方法。其核心思路是：充分利用预训练的多视角掩码自编码器（MV-MAE）的表征能力，通过模仿学习框架，使智能体能够基于当前时间块的信息，预测并选择下一个信息量最大的视角。\n\n## 方法详解\n本文提出的 MAE-Select 框架旨在使机器人智能体能够为操作任务主动选择信息丰富的视角。其核心是将最优视角选择作为一个通过模仿学习目标隐式学习的问题，无需显式监督或强化学习。与之前仅使用预训练编码器的工作不同，本方法利用了多视角掩码自编码器（MV-MAE）的完整编码器-解码器架构，使智能体能够从单个视图构建丰富的场景表征。\n\n![方法框架](https://arxiv.org/html/2602.04243v1/x1.png)\n\n> **图1**：方法整体框架。左侧为多视角掩码自编码器（MV-MAE）的预训练阶段；中间展示了使用模仿学习训练我们框架的过程；右侧演示了推理阶段框架如何运行。\n\n**问题定义**：将机器人操作任务定义为对观测和动作序列的学习问题。目标是学习两个关键策略：1）**动作策略** π_θ，基于当前单视角观测和本体感知状态预测未来 T 步的动作序列 a_{t:t+T-1}；2）**视角选择策略** π_ψ，根据当前时间块的信息，为下一个时间块选择最优视角 v_{t+T}。两个策略通过模仿学习框架联合训练。\n\n**方法流程**：\n1.  **多视角掩码自编码器预训练**：在演示数据上预训练一个 MV-MAE（图1左），学习紧凑的、具有3D感知的场景表征。给定多视角观测 O_t，提取特征图并进行双重掩码：**图像块掩码**（随机掩码每个视图内大部分图像块）和**视图掩码**（随机丢弃整个视图以鼓励跨视图推理）。剩余图像块与视图、位置嵌入一起输入 Transformer 编码器 f_ϕ 产生潜在表征 z_t^m。解码器 g_ϕ 从 z_t^m、掩码令牌和机器人状态嵌入 s_t 重建所有视图，通过最小化像素级重建损失 L_MAE 进行训练。这使得模型能够从部分或遮挡的输入中推断完整的3D场景。\n\n2.  **下一个更优视角选择（联合微调与训练）**：\n    *   **处理当前时间块（D_t）**：从随机选择的单视图 o_t^v 开始，将其通过预训练的完整 MV-MAE（f_ϕ, g_ϕ）生成估计的多视角特征上下文 C_t = g_ϕ(f_ϕ(o_t^v), s_t)。该上下文随后输入到基于扩散模型的动作解码器 π_θ，其被训练用于预测添加到专家动作轨迹 a_{t:t+T-1} 上的噪声 ε_t，计算动作损失 L_action^(t)。\n    *   **为下一个时间块选择视角（D_{t+T}）**：视角选择器 π_ψ（一个 Transformer 编码器）接收当前块的特征上下文 C_t 和真实动作轨迹 a_{t:t+T-1}，预测下一个时间块视角的概率分布 ĉ。为了在保持可微性的同时进行离散选择，采用 VQ-VAE 中的直通估计器：前向传播时通过 argmax 操作得到表示所选视角的独热向量 **y**；反向传播时梯度通过连续的 softmax 概率 ĉ 流动。\n    *   **处理下一个时间块（D_{t+T}）**：使用独热向量 **y** 从下一个观测集 O_{t+T} 中选择单视角 o_{t+T}^v̂，并通过相同流程计算其动作损失 L_action^(t+T)。\n    *   **更新视角选择器**：没有显式的“视角损失”。视角选择器 π_ψ 通过来自未来动作损失 L_action^(t+T) 的梯度进行训练，该梯度通过动作解码器和 STE 选择的观测值反向传播，直接优化 π_ψ 以选择能最小化未来动作预测误差的视角。\n    *   **总体目标**：L_total = L_action^(t) + λ1 * L_action^(t+T) + λ2 * L_MAE。其中 π_θ 由两个动作损失更新，而 π_ψ 仅由未来动作损失优化。\n\n**推理**：推理时（图1右），过程是自回归的。从一个随机初始视角开始，同时预测当前动作块和下一个块的最优视角。预测的动作与新选择的视角一起用于后续步骤，形成一个动态的感知-行动循环。\n\n**创新点**：1）**视角选择机制**：提出了一种无需人工标注、通过模仿学习隐式学习动态视角选择的新方法，利用未来动作预测误差作为监督信号。2）**架构利用**：完全利用了预训练 MV-MAE 的编码器-解码器架构，从单视图构建丰富的多视角上下文，而非仅使用编码器特征。\n\n## 实验与结果\n**实验设置**：在模拟环境（ACT、RLBench、MuJoCo）中的8个挑战性任务以及3个真实世界任务上评估 MAE-Select。采用单摄像头控制设置：训练时使用多个摄像头视图，部署时使用单个可移动摄像头，每个时间块摄像头位于一个训练视角上。基于扩散策略架构实现，动作空间为机器人关节角，图像分辨率224x224。MV-MAE 使用12层 ViT 编码器和8层 ViT 解码器。\n\n**对比方法**：与 Diffusion Policy 及其 MAE 增强变体（MAE-Diffusion）进行对比。基线在训练和测试时使用固定视角，而 MAE-Select 在训练时利用多视图，在推理时动态选择信息量最大的单视角。\n\n![实验结果可视化](https://arxiv.org/html/2602.04243v1/x2.png)\n\n> **图2**：实验中选定视角的可视化，展示了模拟和真实环境。每一行代表一个特定任务的过程，表明选择不同视角的必要性。\n\n**关键实验结果**：\n*   **主要性能对比（表1）**：MAE-Select 在模拟和真实实验中 consistently 优于固定单摄像头设置。例如，在“Put Box In Cabinet”任务中，相比最佳固定摄像头方法提升了8%，相比先前工作提升了32%。其优势在于通过最优视角选择智能地利用有限的视觉输入。值得注意的是，在某些任务中，单摄像头甚至超越了多摄像头设置（例如，“Unplug Charger”任务中，Diffusion Policy 的顶视角44% vs. 双视角34%），这可能源于多输入带来的噪声和对齐问题。MAE-Select 避免了这些缺陷，即使与多摄像头配置相比也保持竞争力且通常更优。\n*   **方法适配性（表2）**：将视角选择模块与另一种动作解码器 ACT 集成，MAE-Select (MAE-ACT) 同样表现优异（例如在“Phone On Base”任务上达到70%成功率），证实了该方法对不同动作解码器架构的普适性。\n*   **消融实验（表3）**：对比了完整 MV-MAE（编码器-解码器）与仅使用编码器的变体（MAE-Encoder）。结果显示，利用完整结构显著提升了性能，特别是在存在遮挡的情况下（如“Put Box In Cabinet”任务，完整模型46% vs. 仅编码器34%），突显了解码器在细化视觉表征和提高泛化能力方面的关键作用。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 MAE-Select，一种无需人工标注、能够在每个时间块动态选择下一个最优视角的新机制。\n2.  提出了一个充分利用预训练 MAE 表征进行机器人操作的模仿学习框架，并创新性地使用未来动作预测损失来训练视角选择器。\n3.  通过大量实验证明，MAE-Select 能有效提升单摄像头系统的操作精度，在某些任务中甚至能超越多摄像头系统。\n\n**局限性**：论文指出，当前方法是在离散的预定义视角集合中进行优化，而非连续的视角空间，这限制了在动态环境中的灵活性。\n\n**未来启示**：未来的工作可以探索与 NeRF 或 3D 高斯泼溅等技术结合，实现连续视角空间的优化，从而提供更精细、更灵活的主动感知能力。此外，该方法展示了将大规模预训练视觉模型与机器人决策循环紧密结合的潜力，为样本高效的机器人学习提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04243v1/x1.png",
        "https://arxiv.org/html/2602.04243v1/x2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04231",
      "title": "GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning",
      "url": "http://arxiv.org/abs/2602.04231",
      "arxivId": "2602.04231",
      "date": "2026-02-04",
      "authors": "Hongliang Ren Team",
      "category": "Manipulation",
      "summary": "本文针对语言引导抓取在杂乱、遮挡或低纹理场景中泛化能力差、现有方法多阶段流程导致融合有限的问题，提出GeoLanG框架。该框架基于CLIP架构，统一RGB-D多模态学习，通过深度引导几何模块（DGGM）将深度信息转换为几何先验注入注意力机制，并采用自适应密集通道集成平衡多层特征贡献。实验在OCID-VLG数据集及仿真和真实环境中验证，GeoLanG实现了精确鲁棒的语言引导抓取。",
      "detailedSummary": "## 研究背景与动机\n当前的语言引导抓取方法通常采用多阶段流水线，将目标感知（如检测、分割）与抓取规划分离。这种范式存在跨模态融合有限、计算冗余、以及在杂乱、遮挡或低纹理场景下泛化能力差等关键局限性。具体而言，现有的视觉编码器（如CLIP-ResNet和CLIP-ViT）各有不足：前者受限于静态层次结构，难以动态建模多尺度物体；后者计算成本高且对局部细节捕捉能力弱。此外，现有的RGB-D方法多采用双编码器架构，存在计算冗余，且低质量的深度数据可能干扰RGB特征学习。在跨模态对齐方面，常见策略未能充分利用多层次的视觉特征，导致任务相关信号被稀释。\n\n本文针对上述痛点，提出了一个统一、端到端的多任务视觉-语言框架，旨在通过深度引导的几何先验和自适应的多层特征融合，在复杂开放场景中实现鲁棒的语言引导抓取。其核心思路是：构建一个基于CLIP架构的端到端框架，通过深度引导几何模块将深度信息转化为几何先验并注入注意力机制，同时通过自适应密集通道集成模块动态聚合多层视觉特征，从而在共享表示空间内实现鲁棒的语义对齐与空间推理。\n\n## 方法详解\nGeoLanG是一个端到端的视觉-语言抓取框架。给定一个RGB-D图像和一个语言查询，文本编码器（CLIP-BERT）提取高级语义特征，生成词元嵌入和句子嵌入。视觉编码器采用CLIP-VMamba，以兼顾全局上下文建模与局部细节保留，并提取多尺度视觉特征。随后，深度引导几何模块利用深度图生成几何先验并增强视觉特征的空间推理能力。接着，自适应密集通道集成模块自适应地聚合多层视觉特征，生成更具判别力的视觉表示。最后，通过多模态融合颈部和多任务解码器，联合输出目标物体的分割掩码和抓取位姿。\n\n![方法框架总览](https://arxiv.org/html/2602.04231v1/x2.png)\n> **图2**：GeoLanG框架总览。给定RGB-D图像和语言指令，文本编码器和视觉编码器分别提取特征。深度引导几何模块注入几何先验，自适应密集通道集成模块优化多层特征。双路投影器最终生成像素级分割掩码并细化目标物体的抓取位姿。\n\n核心模块一：**深度引导几何模块**。该模块旨在有效利用深度信息增强空间推理，同时避免引入额外的深度编码器。首先，将RGB图像均匀划分为H×W个图像块，并计算每个对应深度图像块的平均深度值D_{m,n}。然后，计算任意两个图像块之间的深度差矩阵ΔD和曼哈顿空间距离矩阵ΔS。通过可学习参数λ1和λ2加权融合，得到统一的几何先验矩阵𝒢 = λ1·ΔD + λ2·ΔS，该矩阵编码了所有图像块之间的综合3D几何关系。最后，将该几何先验通过一个衰减因子η集成到自注意力机制中：\\hat{X} = (Softmax(QK⊤) ⊙ η𝒢) V⊤。较小的η𝒢值对应较大的几何距离，从而抑制不相关的键值对，强调几何上相关的部分。此设计使模型能有效捕捉空间依赖、物体形状变化和低纹理区域，且计算高效。\n\n![深度引导几何模块](https://arxiv.org/html/2602.04231v1/x3.png)\n> **图3**：深度引导几何模块概述。粉色矩形代表RGB编码器提取的图像特征，绿色矩形代表学习到的几何先验，黄色矩形代表由深度和RGB计算的空间先验。⊗表示矩阵乘法，⊙表示逐元素乘法。输出特征整合了视觉信息与几何空间线索，以增强多尺度表示。\n\n核心模块二：**自适应密集通道集成**。该模块旨在解决现有方法冻结主干网络、仅利用高层特征而导致中低层信息丢失的问题。首先，将从V-Mamba提取的L层图像特征划分为G个组，每组包含M = L/G个连续层。对于第g组中的M个连续层，通过一个轻量级门控网络为每层特征预测自适应权重α_i（组内Softmax归一化）。组级特征GC_g通过加权求和计算：GC_g = Σ α_i C_i。最后，将所有组级特征与最后一层特征C_L拼接，并通过一个MLP投影生成最终的视觉嵌入e_v。这种设计使ADCI能够根据输入动态平衡各层的贡献，同时保留密集的跨层连接，确保信息丰富的中低层特征被有效聚合，从而产生更丰富、更具表达力的视觉表示。\n\n与现有方法相比，GeoLanG的创新点具体体现在：1) 采用统一的端到端多任务框架，替代了分离感知与规划的多阶段流水线；2) 提出了DGGM，将深度信息作为显式的几何先验直接注入注意力机制，增强了遮挡和低纹理条件下的目标判别能力，且无需专用深度编码器；3) 提出了ADCI，通过自适应门控机制动态融合多层视觉特征，提升了跨模态对齐的判别力和泛化性。\n\n## 实验与结果\n实验主要在**OCID-VLG**数据集上进行，该数据集包含大量杂乱室内场景，提供了语言表达、分割掩码/边界框和抓取标注的多模态监督。实验平台使用了4张NVIDIA RTX 4090 GPU。评估指标包括：用于指代图像分割的IoU和Precision@X (X∈{0.5,0.6,0.7,0.8,0.9})；用于指代抓取合成的Jacquard指数 J@N（评估前N个抓取预测中与真值IoU>0.25且方向差<30°的比例）。\n\n对比的**baseline方法**包括：VLG、GraspCLIP、CLIPort、CTNet和CROG。\n\n![定性对比结果](https://arxiv.org/html/2602.04231v1/x5.png)\n> **图5**：在OCID-VLG数据集上与SOTA方法的定性对比。GeoLanG（最右列）在具有小物体、模糊边缘和遮挡的挑战性场景中（如第一、二行），能生成更精确、贴合物体边界的细粒度分割掩码和抓取位姿。而CROG和CTNet常产生过度简化的掩码或抓取漂移。\n\n**关键实验结果**：在OCID-VLG标准测试集上（表I），GeoLanG在分割和抓取任务上均达到最优。分割IoU为85.77%，显著优于CTNet（80.94%）和CROG（80.77%）。在更严格的Pr@70指标上达到89.82%。抓取任务中，J@1为87.32%，J@N为92.13%，分别超过CTNet 3.00和1.22个百分点。在**新颖实例分割**（表II）的泛化性测试中，GeoLanG同样领先，IoU达81.25%，J@1达83.96%，J@N达90.72%，显示出强大的泛化能力。\n\n![数据集示例](https://arxiv.org/html/2602.04231v1/x4.png)\n> **图4**：OCID-VLG数据集示例。(a)单个物品 (b)多个物品杂乱场景 (c)多个物体杂乱且重叠场景。该数据集为语言引导的感知与抓取提供了丰富的多模态标注。\n\n**消融实验**（表III）总结了各核心组件的贡献：1) **CLIP-VMamba骨干网络**：相比CLIP-ResNet基线，引入CLIP-VMamba使IoU从80.77%提升至83.25%，J@1从81.64%提升至83.44%，证实了其更强的多尺度表征能力。2) **深度引导几何模块**：在VMamba基础上加入DGGM，带来进一步性能提升（IoU 84.88%， J@1 85.35%），证明了深度几何先验对空间推理的有效性。3) **自适应密集通道集成**：最终集成所有组件（VMamba+DGGM+ADCI）获得最佳性能（IoU 85.77%， J@1 87.32%），表明ADCI通过自适应特征聚合增强了模型的判别力。\n\n![仿真实验](https://arxiv.org/html/2602.04231v1/x6.png)\n> **图6**：在RoboDK平台上的仿真实验设置。\n\n![实物机器人实验](https://arxiv.org/html/2602.04231v1/x7.png)\n> **图7**：在DOBOT Nova 2机器人上的实物实验。(a)(b)包含同类干扰物的挑战场景；(c)眼在手配置；(d)(e)孤立与杂乱场景下的抓取示例。\n\n**机器人实物实验**结果（表IV）显示，在模拟的孤立场景中，抓取准确率达到95%。在实物机器人测试中，孤立场景的分割和抓取准确率分别为85%和82.5%；在更具挑战的杂乱场景中，则分别降至80%和60%。实验表明，对于遮挡少或结构清晰的物体（如牙刷），性能接近完美，而对于严重遮挡或体积小的物体（如药盒），性能有所下降，但整体上GeoLanG展现了对新物体和复杂环境的较强泛化与鲁棒性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了GeoLanG，一个将RGB-D感知与自然语言理解统一到共享表示空间的端到端多任务框架，提升了在复杂环境中的语义对齐与泛化能力；2) 设计了深度引导几何模块，将深度信息编码为显式几何先验并注入注意力机制，增强了模型在遮挡和低纹理条件下的空间推理与目标判别能力；3) 提出了自适应密集通道集成模块，通过动态门控机制自适应聚合多层视觉特征，生成了更具判别力和泛化性的视觉表征。\n\n论文自身提到的局限性在于：当前框架主要专注于4-DoF平面抓取，简化了夹爪朝向和交互动力学，这限制了对更广泛操作场景的适用性。\n\n本文对后续研究的启示在于：1) 验证了将几何先验显式、轻量化地集成到视觉-语言模型中的有效性，为多模态机器人感知提供了新思路；2) 动态、自适应的多层次特征融合策略可提升跨模态对齐的精度，值得在其他需要细粒度对齐的任务中探索；3) 端到端的多任务学习范式（联合分割与抓取）有助于减少级联误差，推动机器人操作向更智能、更通用的方向发展。未来的工作可以探索更高自由度的抓取表示，并进一步研究如何更好地处理深度传感器噪声和严重的实物遮挡问题。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04231v1/x1.png",
        "https://arxiv.org/html/2602.04231v1/x2.png",
        "https://arxiv.org/html/2602.04231v1/x3.png",
        "https://arxiv.org/html/2602.04231v1/x4.png",
        "https://arxiv.org/html/2602.04231v1/x5.png",
        "https://arxiv.org/html/2602.04231v1/x6.png",
        "https://arxiv.org/html/2602.04231v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04228",
      "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2602.04228",
      "arxivId": "2602.04228",
      "date": "2026-02-04",
      "authors": "Badong Chen Team",
      "category": "Manipulation",
      "summary": "本文针对连续动作视觉-语言-动作（VLA）模型使用均方误差（MSE）回归时对个体预测误差约束过强、忽略整体误差分布的问题，提出通过重塑动作误差分布来提升模型可靠性。关键技术是引入信息论中的最小误差熵（MEE），设计轨迹级MEE目标及其两个加权变体，与MSE结合进行训练。实验在标准、少样本和噪声设置下，使用LIBERO等模拟基准和真实机器人任务，结果表明该方法能持续提高成功率和鲁棒性，在数据不平衡时增益稳定，且额外训练成本可忽略。",
      "detailedSummary": "## 研究背景与动机\n在机器人操作领域，视觉-语言-动作模型已成为学习通用和可扩展策略的有前景范式。现有VLA框架大多依赖标准监督目标，对于连续动作回归通常采用均方误差损失。MSE对单个预测施加了强约束，但其仅从个体预测层面运作。从分析视角看，动作预测误差可被视为从底层误差分布中抽取的样本，该分布随时间演变并可能呈现结构化模式。然而，点对点回归目标并未显式地调控误差的整体组织和几何结构。从分布视角看，策略行为不仅由个体误差大小决定，也受误差分布的整体不确定性、集中度和结构影响。信息论准则为刻画此类分布级属性提供了自然框架，其中最小误差熵准则直接最小化误差分布的熵，使得批次或轨迹内的误差能够相互作用。本文针对连续动作VLA模型，提出超越传统MSE回归，在训练过程中重塑动作误差分布。核心思路是引入基于信息论的最小误差熵准则，提出轨迹级MEE目标及其加权变体，并与MSE结合，在优化个体误差的同时，促使误差分布更加紧凑和结构化。\n\n## 方法详解\n本文方法的核心是在标准连续动作VLA模型的训练中，引入一个分布级的监督信号，与点级的MSE损失协同工作。\n\n![方法框架](https://arxiv.org/html/2602.04228v1/x2.png)\n\n> **图2**：本工作评估的连续动作VLA模型架构分类。总结了代表性小规模和大规模架构。(a-b) 小规模模型使用轻量级主干从多模态特征回归动作；(c-f) 大规模模型基于预训练的VLM构建。\n\n整体框架遵循行为克隆范式：给定包含视觉观察、本体感觉状态和语言指令的输入，VLA策略模型输出连续动作。训练时，除了计算预测动作与专家动作之间的标准MSE损失外，关键创新在于计算一个额外的轨迹级最小误差熵损失。\n\n核心模块是轨迹级MEE及其加权变体。首先，将批次内所有轨迹、所有时间步、所有动作块维度的预测误差向量 $\\mathbf{e}_{b,k}^{t}$ 扁平化为一个集合 $\\{\\mathbf{e}_i\\}_{i=1}^N$，其中 $N = B \\times T \\times K$。这允许将整个训练批次内的动作误差视为来自一个共享误差分布的样本。\n\n基础目标是轨迹级MEE，采用二次Rényi熵的估计器：\n$\\mathcal{L}_{\\mathrm{T\\text{-}MEE}} = -\\log\\left(\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\exp\\left(-\\frac{\\|\\mathbf{e}_{i}-\\mathbf{e}_{j}\\|^{2}}{2\\sigma^{2}}\\right)\\right)$\n该目标最小化误差分布的熵，鼓励误差样本在误差空间中聚集，形成紧凑的分布。\n\n为进一步控制误差塑形，论文引入了两种加权变体。首先为每个误差样本分配一个基于其幅度的权重 $w_i$（幅度越小权重越大）。然后定义加权损失 $\\mathcal{L}_{\\mathrm{W\\text{-}TMEE}} = -\\log\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\omega_{ij}\\exp\\left(-\\|\\mathbf{e}_{i}-\\mathbf{e}_{j}\\|^{2}/2\\sigma^{2}\\right)$。通过设置 $\\omega_{ij} = \\frac{1}{N^2}w_i$ 得到**块加权T-MEE**，它强调可靠的动作块；通过设置 $\\omega_{ij} = w_i w_j$ 得到**元素加权T-MEE**，它进行对称的、元素级的加权。\n\n最终的训练目标是MSE与T-MEE的加权和：$\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{MSE}} + \\alpha \\mathcal{L}_{\\mathrm{T\\text{-}MEE}}$。MSE提供将每个误差锚定在零附近的点级精度，而T-MEE则在分布层面塑造误差的几何结构。\n\n与现有方法相比，创新点在于：1) **视角创新**：从误差分布的整体几何结构，而非单个误差值来看待监督信号；2) **方法创新**：将经典信息论中的MEE准则适配到具有时空相关性的VLA动作预测场景，提出了轨迹级 formulation；3) **实现创新**：设计了加权机制以提供更精细的误差聚合控制，并与标准MSE无缝集成。\n\n## 实验与结果\n实验使用了仿真基准LIBERO和SimplerEnv，以及真实机器人操作任务。LIBERO包含Spatial, Goal, Object, Long四个任务套件，共40个任务。SimplerEnv评估四个WidowX操作任务。真实实验使用Agilex Cobot Magic平台，执行三类操作任务和一个分布外任务。\n\n对比的基线方法涵盖了论文图2中列出的多种VLA架构，包括小规模的BC-RNN、BC-Transformer、BC-DP，以及大规模的GR00T、OFT、$\\pi_0$、DS-VLA。所有大规模模型统一使用Qwen3-VL作为视觉语言主干以确保公平比较。\n\n关键实验结果如下：在LIBERO上，T-MEE为所有模型带来了性能提升。对于小模型提升显著，例如BC-Transformer平均成功率从52.6%提升至63.5%，BC-RNN从15.0%提升至28.3%。对于大模型，在已经很高的基础上仍有稳定提升，例如$\\pi_0$从96.8%提升至98.4%，DS-VLA从95.8%提升至98.2%。\n\n![主结果表](https://arxiv.org/html/2602.04228v1/x3.png)\n\n> **图3**：LIBERO上不同MEE变体的性能比较。所有基于MEE的目标均一致优于基线，不同变体在不同架构和任务套件上表现出互补优势。\n\n在SimplerEnv上，T-MEE在大多数模型和任务上带来提升或持平，尤其在更具挑战性的任务上改善明显，例如在2B基座VLM下，DS-VLA在“Stack Cube”任务上的成功率从12.5%提升至25.0%。\n\n![真实实验](https://arxiv.org/html/2602.04228v1/x4.png)\n\n> **图4**：真实世界评估。(a) 机器人设置和代表性操作任务。(b) 任务成功率对比，显示T-MEE在所有任务上带来了持续的性能增益。\n\n消融实验方面：论文比较了T-MEE、Cw-TMEE和Ew-TMEE三种变体。结果表明，所有MEE变体均优于纯MSE基线，但最佳变体取决于具体架构和任务。例如，对于BC-Transformer，Ew-TMEE在Spatial套件上表现最佳；而对于GR00T，T-MEE和Cw-TMEE在不同套件上各有优势。这说明了分布级误差塑形的灵活性，以及根据具体问题调整加权策略的价值。\n\n![误差分布可视化](https://arxiv.org/html/2602.04228v1/x1.png)\n\n> **图1**：使用PCA可视化有/无轨迹级MEE时的动作误差分布。结合T-MEE训练后，误差分布在投影空间中变得更加紧凑和连贯，同时任务成功率得到提升。\n\n![数据不平衡分析](https://arxiv.org/html/2602.04228v1/x5.png)\n\n> **图5**：在LIBERO-Object上，随着任务间数据不平衡程度增加，T-MEE相对于MSE基线的性能增益变化。在一个特征明确的操作区间内增益持续存在，但在极度不平衡时可能有害。\n\n![噪声鲁棒性](https://arxiv.org/html/2602.04228v1/x6.png)\n\n> **图6**：在注入不同程度高斯噪声和脉冲噪声的专家动作上训练BC-Transformer。T-MEE在所有噪声水平下均优于MSE，尤其是在高噪声和非高斯脉冲噪声下表现出更强的鲁棒性。\n\n## 总结与启发\n本文的核心贡献包括：1) **提出了一个重塑动作误差分布的框架**：将信息论中的最小误差熵准则引入VLA训练，提出了轨迹级MEE及其加权变体，从分布视角对连续动作误差进行监督。2) **提供了深入的理论分析**：揭示了MEE通过相似性加权促使误差交互聚类的机制，证明了其对非高斯噪声和异常值的鲁棒性，并分析了多任务场景下由误差空间重叠和样本不平衡导致的耦合现象。3) **进行了广泛的实证验证**：在多种VLA架构、模型规模以及平衡、少样本、噪声、不平衡等多种数据场景下，证明了方法的有效性、鲁棒性和可忽略的额外训练成本。\n\n论文自身提到的局限性主要在于数据严重不平衡时：理论分析表明，当任务间误差分布重叠且样本极度不平衡时，MEE可能使少数任务的学习被多数任务主导，导致性能下降。图5的实验也证实了在极度不平衡区域性能增益消失甚至为负。\n\n对后续研究的启示：1) **分布视角的价值**：超越点对点损失，关注预测误差的整体分布特性，是提升模型鲁棒性和泛化能力的一个有效途径。2) **理论指导实践**：论文的理论分析不仅解释了方法为何有效，还明确了其有效的操作区间（如数据不平衡程度），为实际应用提供了指导。3) **轻量级改进**：该方法几乎不增加推理成本，训练开销也可忽略，为提升现有VLA模型性能提供了一种高效的即插即用选项。未来工作可探索更复杂的误差分布建模，或将此思想扩展到离散动作VLA模型及其他序列预测任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04228v1/x1.png",
        "https://arxiv.org/html/2602.04228v1/x2.png",
        "https://arxiv.org/html/2602.04228v1/x3.png",
        "https://arxiv.org/html/2602.04228v1/x4.png",
        "https://arxiv.org/html/2602.04228v1/x5.png",
        "https://arxiv.org/html/2602.04228v1/x6.png",
        "https://arxiv.org/html/2602.04228v1/x7.png",
        "https://arxiv.org/html/2602.04228v1/x8.png",
        "https://arxiv.org/html/2602.04228v1/x9.png",
        "https://arxiv.org/html/2602.04228v1/x10.png",
        "https://arxiv.org/html/2602.04228v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04213",
      "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons",
      "url": "http://arxiv.org/abs/2602.04213",
      "arxivId": "2602.04213",
      "date": "2026-02-04",
      "authors": "Reid Simmons Team",
      "category": "Manipulation",
      "summary": "本文提出InterPReT方法，旨在解决非专业用户难以通过传统模仿学习有效教导AI智能体的难题。其核心技术为“交互式策略重构与训练”，允许用户通过指令交互式地更新策略结构，并通过演示优化参数，从而让用户能监控性能并审查决策过程。在一项34人参与的赛车游戏教学用户研究中，与通用模仿学习基线相比，该方法在由非专业用户提供演示并决定训练停止时，能产生更鲁棒的策略，且不损害系统可用性，证明了其对无技术背景用户的适用性。",
      "detailedSummary": "## 研究背景与动机\n模仿学习通过模仿专家演示已在许多任务中取得成功。然而，现有工作大多依赖于技术专业人员提供的大规模演示以及对训练过程的密切监控。当普通人想要教授智能体新技能时，这些要求具有挑战性。普通人可能不知道哪种演示更适合模仿学习，也不清楚学习到的策略何时足够好以停止训练。此外，操作不熟悉的遥操作设备会显著降低效率，收集大量数据也容易导致认知疲劳。现有面向普通人的工作主要关注处理人类教师的不完美性，或应用于特定用户群体。一些工作探索了借助语言指令进行少量演示学习，以减少所需演示数量，但这仍要求人类专家一次性详细指定所有领域知识，对于普通人而言在复杂任务中非常困难。\n\n本文针对普通人难以一次性提供完整领域知识和判断训练何时停止的痛点，提出了一种交互式学习范式。核心思路是：让智能体通过多轮交互，利用用户提供的自然语言指令（通过大语言模型）动态重构策略的**结构**，并利用用户提供的演示数据优化策略的**参数**，从而逐步构建出鲁棒且符合用户意图的策略。\n\n## 方法详解\nInterPReT 的核心是让智能体维护一个指令集和一个演示集，并据此交互式地更新策略结构和权重。智能体实例被形式化为一个四元组 𝒜𝑖 = ⟨ℐ𝑖, 𝒟𝑖, ⟦α⟧𝑖, Θ𝑖⟩，其中 ℐ 是指令集，𝒟 是演示集，⟦α⟧ 是策略结构，Θ 是策略权重。\n\n![方法框架](https://arxiv.org/html/2602.04213v1/x1.png)\n\n> **图1**：InterPReT 中的交互模式。用户与智能体反复交互直至满意。展示了四种交互模式：1）用户给出指令，智能体重构其策略（左上）；2）用户给出演示，智能体训练其权重（左下）；3）智能体用自然语言总结其策略（右上）；4）智能体演示其策略供用户检查（右下）。\n\n**结构化策略**：本文方法建立在结构化策略之上。一个结构化策略是一个四元组 ⟨V, P, E, Θ⟩，表示一个加权的有向无环二分图。其中 V 是代表特征（观测值、隐变量或动作）的节点集合，具有语义含义；P 是代表可微分操作实例（如线性组合、常数）的节点集合；E 是连接变量节点和操作节点的边集合；Θ 是与每条边关联的权重集合。该结构通常是稀疏的，有助于减少参数数量并缓解因果混淆问题。策略的推理过程是在该计算图上按边传播值。\n\n![结构化策略示例](https://arxiv.org/html/2602.04213v1/x2.png)\n\n> **图2**：表示维持恒定“期望速度”的比例控制器的结构化策略最小示例。实线框是 V 中的变量（标记为观测 O、隐变量 L 或动作 A），虚线框是 P 中的操作符。权重 Θ 与边关联。图中展示了推理过程的一个数值例子。\n\n**策略训练**：给定一个策略结构 ⟦α⟧𝑖 和一组演示 𝒟𝑖，使用标准的模仿学习目标来训练权重 Θ𝑖。由于特征和操作符具有语义，实践中会查询大语言模型（LLM）来估计它们之间的因果效应，并以此初始化权重 Θ₀，然后使用 Adam 优化器进行梯度下降。当获得新演示 D 时，智能体更新其演示集并重新训练权重。\n\n**策略重构**：这是方法的核心创新。给定一组指令 ℐ𝑖，目标是生成一个遵循指令且在任务域中表现良好的策略结构 ⟦α⟧𝑖。由于普通人可能无法完整指定策略的所有细节，本方法依赖 LLM 的现有世界知识来做出合理的设计选择。通过**思维链提示**和**上下文学习**来增强鲁棒性：向 LLM 提供一个来自完全不同任务（Lunar Lander）的指令-模型对示例，并引导其执行以下步骤：1) 从指令中提取所有相关特征（形式化 V）；2) 用英文段落描述结构（填补指令缺失的细节）；3) 规划所有连接和操作（形式化 P 和 E）；4) 组装所有组件生成可执行的 PyTorch 模型。生成英文描述作为中间步骤已被实证证明能提高生成质量。当收到新指令 I 时，智能体更新其指令集，并调用 Restructure 函数生成新结构，随后在新结构上使用现有演示数据重新训练权重。\n\n**策略总结与演示**：在结构生成过程中产生的英文描述可直接作为智能体策略的总结呈现给用户，帮助用户理解智能体的决策逻辑。用户还可以让智能体在特定初始配置下进行策略演示（rollout），以直观检查其行为。\n\n与现有方法相比，InterPReT 的创新点具体体现在：1) **交互性**：允许用户通过多轮指令和演示逐步教学，而非一次性提供所有知识；2) **结构化生成**：利用 LLM 将模糊的自然语言指令转化为具有明确因果关系的、可微分的策略计算图结构；3) **可解释性**：基于语义化的策略结构，能够向用户提供策略总结和行为演示，形成反馈闭环。\n\n## 实验与结果\n**实验设置**：研究在 gymnasium 包的赛车环境中进行。任务目标是以最快速度完成赛道，关键指标是有效平均速度（EAS）。状态表示为结构化信息（当前速度、航向、前方固定数量瓦片的信息等），动作空间是转向、油门和刹车的连续控制。\n\n**用户研究**：进行了组间用户研究（N=34），参与者主要为无 AI 算法开发经验的普通人。设置了两种条件：1) **实验条件**：参与者可同时提供指令和演示，模型使用 InterPReT 训练；2) **基线条件**：参与者仅提供演示，策略由一个多层感知机（MLP）表示，使用相同的模仿学习配置训练。\n\n![名义条件下的平均速度](https://arxiv.org/html/2602.04213v1/figures/results/nominal_boxplot.jpg)\n\n> **图4**：名义条件下学习策略的平均速度箱线图。InterPReT 策略的中位数（49）与基线 MLP 策略（49）相当，但 InterPReT 的策略性能分布更集中（四分位距更小），表明其能产生更一致的策略质量。\n\n![各轮交互的平均速度趋势](https://arxiv.org/html/2602.04213v1/figures/results/per_run_violin_average_speed.jpg)\n\n> **图5**：按交互轮次划分的策略平均速度小提琴图。随着交互轮次增加，InterPReT 策略的性能提升更明显且更高效，而基线 MLP 策略的提升则相对平缓。\n\n![用户预期与真实性能对比](https://arxiv.org/html/2602.04213v1/figures/results/expectation_boxplot.jpg)\n\n> **图6**：用户对策略性能的预期与其实测性能对比的箱线图。使用 InterPReT 的用户，其预期更接近智能体的真实性能，表明策略总结和演示帮助用户形成了更准确的认知。\n\n![系统可用性量表得分](https://arxiv.org/html/2602.04213v1/figures/results/sus_boxplot.jpg)\n\n> **图7**：系统可用性量表（SUS）得分的箱线图。InterPReT 系统的可用性得分中位数（72.5）显著高于基线系统（57.5），表明交互式教学框架并未损害可用性，反而可能更易用。\n\n![消融实验：平均速度](https://arxiv.org/html/2602.04213v1/figures/results/ablation_average_speed_boxplot.jpg)\n\n> **图8**：消融实验的平均速度箱线图。对比了完整 InterPReT、仅使用指令初始化结构但固定不变（No Restructure）、以及随机初始化结构（No LLM Init）三种设置。完整 InterPReT 性能最佳，证明了交互式结构重构和 LLM 权重初始化的有效性。\n\n**关键实验结果总结**：\n1.  **策略鲁棒性**：在名义条件下，InterPReT 训练出的策略在平均速度中位数上与基线相当（49 vs 49），但其性能分布更集中，产生低性能策略的几率更低。\n2.  **学习效率**：随着交互轮次增加，InterPReT 策略的性能提升更高效。\n3.  **用户认知**：InterPReT 用户对智能体性能的预期更准确。\n4.  **系统可用性**：InterPReT 的系统可用性量表得分显著更高（中位数72.5 vs 57.5）。\n5.  **消融实验**：验证了交互式结构重构和 LLM 权重初始化各自对最终性能的积极贡献。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个面向最终用户的交互式教学框架 InterPReT，它允许普通人通过多轮自然语言指令和动作演示来共同构建和优化智能体策略，其中指令用于通过 LLM 生成语义化的策略结构，演示用于优化该结构的参数。\n2.  设计并实施了一项针对非技术背景用户的严格研究，验证了该框架在产生鲁棒策略、高效交互改进以及整体可用性方面的优势。\n\n**局限性**：论文提到，方法的性能部分依赖于 LLM 的代码生成能力和对任务的理解。此外，结构化策略的表达能力可能受限于其可微分操作符的集合。\n\n**对后续研究的启示**：\n1.  **交互式与渐进式教学**是降低模仿学习门槛的关键，允许用户“边教边改”，符合人类自然教学习惯。\n2.  **利用语义化与结构化表征**（如本文的结构化策略）不仅能提升学习效率、缓解因果混淆，还为实现策略可解释性和人机互信提供了基础。\n3.  将 **LLM 的世界知识和代码生成能力**与传统的优化学习（如梯度下降）相结合，为创建更易于普通人定制和理解的 AI 智能体开辟了新途径。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04213v1/x1.png",
        "https://arxiv.org/html/2602.04213v1/x2.png",
        "https://arxiv.org/html/2602.04213v1/figures/illustrations/render.png",
        "https://arxiv.org/html/2602.04213v1/figures/results/nominal_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/demonstration_steps_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/per_run_violin_average_speed.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/expectation_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/sus_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/trend_per_participant_average_speed_single.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/instruction_token_count_histogram.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/ablation_average_speed_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/illustrations/home_page.png",
        "https://arxiv.org/html/2602.04213v1/figures/illustrations/demo_page.png",
        "https://arxiv.org/html/2602.04213v1/figures/background/age_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/gender_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/game_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/driving_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/ai_dev_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/data_countplot.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.03973",
      "title": "VLS: Steering Pretrained Robot Policies via Vision-Language Models",
      "url": "http://arxiv.org/abs/2602.03973",
      "arxivId": "2602.03973",
      "date": "2026-02-03",
      "authors": "Ranjay Krishna Team",
      "category": "Manipulation",
      "summary": "本文提出VLS框架，解决预训练扩散或流匹配策略在测试时遭遇空间配置或任务语义分布偏移时的适应性问题。核心方法为无需训练的推理时引导技术：利用视觉语言模型合成轨迹可微的奖励函数，在去噪采样过程中直接调整冻结策略的动作分布，无需修改参数。实验表明，VLS在CALVIN和LIBERO-PRO基准上分别取得31%和13%的性能提升，并在真实机器人上验证了对空间与语义偏移的鲁棒适应性。",
      "detailedSummary": "## 研究背景与动机\n当前机器人模仿学习的主流方法是利用大规模专家数据训练生成式策略，特别是基于扩散模型或流匹配的目标函数，这些策略在训练分布内表现出色。然而，当测试时的观察或语言指令偏离训练分布时，例如在障碍物附近、支撑面偏移或存在轻微杂物的场景中执行相同任务，这些策略往往会失败。这种失败并非源于缺失运动技能，而是暴露了模仿学习在训练-测试偏移下的一个根本局限性：动作生成与训练时特定的空间配置和任务规范紧密耦合。通过重新训练或微调来解决这些失败既成本高昂，又在概念上不匹配，因为所需的行为已经存在于训练数据中，只是无法在测试时有选择地适配。\n\n本文针对上述痛点，提出了一种新的视角：将适配问题视为一个推理时的控制问题，而非参数更新问题。本文的核心思路是提出**视觉语言引导**框架，在不修改预训练策略参数的前提下，利用视觉语言模型合成轨迹可微的奖励函数，在推理时引导去噪采样过程，使生成的动作轨迹满足测试时的空间和任务要求。\n\n## 方法详解\nVLS框架旨在为冻结的预训练基础策略提供推理时适配能力。其核心是构建一个可微的替代评分函数 ℛ，以近似难以直接获取的似然项 log p((o,l)_OOD | a)，并利用其梯度引导去噪过程。\n\n![方法框架](https://arxiv.org/html/2602.03973v1/figure/Figure21.png)\n> **图1**：VLS整体流程概览。给定RGB-D观察和语言指令，系统首先将输入接地为空间关键点集；随后，视觉语言模型生成一系列阶段感知的可微程序化奖励函数；在基础策略的去噪采样循环中，注入奖励梯度并结合RBF排斥项与Feynman-Kac重采样机制进行修正；最后，基于奖励反馈构建闭环阶段切换系统，以鲁棒地完成长视野任务。\n\n整体流程包含三个核心组件：\n1.  **OOD输入接地与奖励生成**：首先，利用视觉语言模型识别任务相关物体，并借助Segment Anything Model获取物体掩码。结合DINOv2提取的语义特征和深度信息，将掩码像素反投影为3D点云，并聚类得到一组任务相关的3D关键点 𝒫 = {p_i}，从而将高维OOD输入压缩为几何支架。随后，查询VLM以（i）将任务分解为S个连续阶段，（ii）为每个阶段s生成一个可微的程序化奖励函数 ℛ_s(a, (o,l)_OOD) = f_VLM(a, 𝒫, s)。这些奖励函数由PyTorch可微操作（如距离、点积、软约束）构成，可对动作提案a进行评分并计算梯度 g_s = ∇_a ℛ_s。\n2.  **推理时去噪引导**：在基础策略（扩散或流匹配）的每一步去噪迭代k中，引导一批B个动作提案。为了在复杂、多模态的奖励地形中高效搜索并保持多样性，VLS结合了三种机制：\n    *   **多样化提案初始化与排斥力**：在去噪早期，引入基于径向基函数的排斥梯度，促使批次内的动作提案相互远离，覆盖更广的动作流形。\n    *   **梯度引导精炼**：将当前阶段的奖励梯度g_s注入去噪更新。对于扩散策略，修正后的噪声预测为 ε̂ = ε - λ·√(1-ᾱ_k)·g_s；对于流匹配策略，修正后的速度场为 v̂ = v + λ·g_s。其中λ为引导强度。采用类似MCMC的多步内部更新以提高稳定性。\n    *   **基于Feynman-Kac的梯度无关重采样**：周期性地将动作提案视为粒子系统进行重采样。每个粒子i的权重由 w_i^k ∝ exp(ℛ_s(a^k[i])) 计算，然后进行多项式重采样。这使高奖励粒子得以复制，淘汰违反约束的提案。\n3.  **闭环执行控制与阶段切换**：为处理物理不确定性并协调多阶段任务，VLS引入闭环控制。\n    *   **自适应引导强度**：在同一阶段内，根据当前动作块获得的奖励值 ℛ_s^t 相对于该阶段第一个动作块奖励 ℛ_s^base 的比例，动态调整引导强度 λ_t。当执行偏离约束时增强引导，接近完成时则减弱，让基础策略主导精细操作。\n    *   **基于施密特触发器的阶段切换**：为每个阶段s设定高、低两个奖励阈值 R_high 和 R_low。根据 ℛ_s^t 与阈值的关系，决定是“推进阶段”、“维持阶段”还是“强化阶段”（可能触发重试）。这种迟滞逻辑避免了在阶段边界附近的振荡行为。\n\n与现有方法相比，VLS的创新点在于：1) 将适用于大语言模型和图像生成的推理时引导范式扩展至机器人领域，将动作生成视为可控去噪过程；2) 创造性利用VLM的跨模态推理能力，直接从OOD观察-语言输入合成**密集的、阶段感知的、可微的**程序化奖励函数作为引导信号，而非进行离散的验证或选择；3) 设计了结合梯度引导、粒子多样性维持与重采样的混合引导机制，以及自适应的闭环执行控制逻辑。\n\n## 实验与结果\n实验在仿真和真实世界两个环境中进行，评估VLS在测试时面对观察和语言偏移时的适应能力。\n\n**基准测试与基线方法**：\n*   **仿真**：使用了CALVIN和LIBERO-PRO两个广泛使用的操作测试套件，它们明确强调对OOD观察-语言输入的推理时适应。\n*   **基线**：对比了先前的推理时引导方法，包括ITPS（基于人机交互信号）、DynaGuide（基于动力学模型引导），以及价值函数引导方法（V-GPS, VGD）。在LIBERO-PRO上，还测试了VLS对多个冻结的VLA基础策略（如OpenVLA， π_0, π_0.5变体）的提升效果。\n*   **真实世界**：在Franka机器人上部署，测试在未见过的物体外观、位置变化和目标替换下的多阶段语言指定任务。\n\n**关键实验结果**：\n在CALVIN上，VLS consistently outperforms prior inference-time steering methods such as ITPS [49] and DynaGuide [16], achieving up to a 31% absolute improvement in success rate on long-horizon tasks.\n在LIBERO-PRO上，VLS improves the success rate of frozen VLA policies, including OpenVLA [28], π_0 [3], variants of π_0.5 [4, 31] by up to 13% under both spatial (object layout) and semantic (task specification) perturbations.\n\n![CALVIN结果](https://arxiv.org/html/2602.03973v1/figure/CALVINresult.png)\n> **图2**：在CALVIN数据集上的成功率对比。VLS（橙色）在长视野任务上显著优于所有基线方法（ITPS、DynaGuide等），取得了最高达31%的绝对提升。\n\n![消融实验](https://arxiv.org/html/2602.03973v1/x1.png)\n> **图3**：VLS各组件贡献的消融研究。移除VLM奖励（“w/o VLM Reward”）、Feynman-Kac重采样（“w/o Resampling”）或自适应阶段切换（“w/o Stage Switch”）都会导致性能下降，验证了每个组件的必要性。\n\n![LIBERO-PRO结果](https://arxiv.org/html/2602.03973v1/figure/pi05results.png)\n> **图4**：在LIBERO-PRO上使用π_0.5作为基础策略的结果。VLS在空间扰动（不同物体布局）和语义扰动（不同任务描述）下均能稳定提升策略性能（平均提升13%），显示了其强大的OOD适应能力。\n\n真实世界实验进一步证明，VLS能够使机器人在面对未知物体外观、位置变化和目标替换时，稳定执行多阶段任务，验证了其在实际部署中的有效性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了VLS框架**：一种无需训练、基于推理时引导的通用方法，能够使冻结的预训练生成式策略适应分布外的观察和语言指令，将技能执行与任务规范解耦。\n2.  **利用VLM合成可微奖励**：创新性地利用视觉语言模型的推理能力，将复杂的OOD输入转化为程序化的、阶段感知的可微奖励函数，为去噪过程提供密集的梯度信号。\n3.  **实现了混合引导与闭环控制**：结合了梯度引导、粒子多样性维持、Feynman-Kac重采样以及自适应的闭环阶段切换机制，实现了对复杂约束地形的高效、鲁棒探索。\n\n**局限性**：论文提到，VLS的性能依赖于VLM对场景和任务理解的准确性，以及其生成合理、可微奖励程序的能力。此外，虽然无需训练，但推理时的引导步骤（特别是多粒子采样与重采样）会引入额外的计算开销。\n\n**对后续研究的启示**：VLS展示了将大模型“推理时引导”范式成功迁移到机器人领域的潜力。后续工作可以探索：1) 如何设计更高效、更准确的VLM提示与奖励程序生成机制；2) 如何进一步降低推理时计算成本，使其更适合实时控制；3) 将该框架扩展至其他类型的生成模型或更复杂的多模态指令场景。",
      "imageUrls": [
        "https://arxiv.org/html/2602.03973v1/figure/Figure21.png",
        "https://arxiv.org/html/2602.03973v1/figure/CALVINresult.png",
        "https://arxiv.org/html/2602.03973v1/x1.png",
        "https://arxiv.org/html/2602.03973v1/figure/pi05results.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.03668",
      "title": "MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction",
      "url": "http://arxiv.org/abs/2602.03668",
      "arxivId": "2602.03668",
      "date": "2026-02-03",
      "authors": "Jungwoo Lee Team",
      "category": "Manipulation",
      "summary": "本文提出MVP-LAM模型，旨在解决从无标注视频中学习潜在动作时，因视角变化引入噪声、导致潜在动作与真实动作关联性弱的核心问题。其关键技术是利用时间同步的多视角视频，通过跨视角重建目标进行训练：从一个视角推断的潜在动作必须能预测另一视角的未来状态，从而剥离视角特异性干扰。实验表明，在Bridge V2数据集上，MVP-LAM学得的潜在动作与真实动作互信息更高，动作预测性能更优（包括分布外评估）。使用该潜在动作预训练VLA模型，在SIMPLER和LIBERO-Long基准上提升了下游操作性能。",
      "detailedSummary": "## 研究背景与动机\n当前，利用海量人类视频学习潜在动作（Latent Action）是扩展机器人学习、突破特定本体数据限制的重要途径。这些学习到的潜在动作可作为伪动作标签，用于视觉-语言-动作模型的预训练。为了确保VLA预训练有效，潜在动作必须在缺乏真实动作标签的情况下，依然包含关于智能体底层动作的信息。然而，现有方法通常从单视角视频中学习潜在动作，面临一个关键障碍：外源性噪声。其中，视角变化是主要噪声源，相机移动和透视变化会与智能体动作产生的视觉变化纠缠在一起，导致学习到的潜在动作过度拟合于视角相关的线索，从而降低了对真实动作的预测性。\n\n本文针对视角变化干扰潜在动作学习这一具体痛点，提出了利用多视角视频进行学习的新视角。其核心思路是：通过时间同步的多视角视频和交叉视角重建目标，迫使从一个视角推断出的潜在动作能够解释另一个视角的未来观测，从而减少对视角特定线索的依赖，学习到更具动作中心性的离散潜在动作。\n\n## 方法详解\nMVP-LAM的整体框架基于向量量化变分自编码器，其训练需要时间同步的多视角视频。对于每个时间步，模型从两个视角的连续观测特征中分别推断出潜在动作，并通过两种重建目标进行优化。\n\n![方法框架](https://arxiv.org/html/2602.03668v1/x2.png)\n\n> **图2**：MVP-LAM使用时间同步多视角视频进行训练。(1) 自视角重建（左）：对于每个视角v，冻结的DINOv2提取特征，时空编码器产生连续潜在表示并向量量化为离散token，解码器从中重建下一帧特征。(2) 交叉视角重建（右）：MVP-LAM交换不同视角间的潜在token，并用它来重建另一视角的未来特征，鼓励潜在token捕捉固有的状态转移信息。\n\n核心模块包括编码器、向量量化码本和解码器。给定时间同步的图像对，首先使用DINOv2编码器提取物体中心的观测特征。对于每个视角，编码器接收当前帧和下一帧的特征，输出一个连续潜在表示，随后通过向量量化操作映射为码本中的离散token。解码器的任务是根据当前观测和潜在动作预测下一观测。\n\nMVP-LAM的创新点在于其训练目标，它结合了两种重建损失：\n1.  **自视角重建损失**：使用从同一视角推断出的潜在动作，来重建该视角的下一帧特征。\n2.  **交叉视角重建损失**：交换两个同步视角的潜在动作，即使用从视角v1推断出的潜在动作来预测视角v2的下一帧特征，反之亦然。\n\n总损失函数为：。其中和为VQ-VAE标准的量化损失和承诺损失。交叉视角重建是关键，由于解码器不以潜在动作的视角信息为条件，如果潜在动作编码了视角特定信息，将导致交叉视角预测失败。最小化该损失等价于减少潜在动作对视角的依赖性，从而提升其动作中心性。\n\n## 实验与结果\n实验使用了多个基准数据集：Bridge V2用于评估潜在动作的动作中心性；SIMPLER和LIBERO-Long用于评估下游操作性能。实验平台涉及VLA模型的预训练与微调。\n\n对比的基线方法包括：潜在动作模型UniVLA、LAPA、Moto；以及VLA模型OpenVLA、Octo和。\n\n关键实验结果如下：\n在动作中心性评估上，MVP-LAM在Bridge V2上取得了潜在动作与真实动作之间最高的互信息估计值。\n\n![互信息估计](https://arxiv.org/html/2602.03668v1/x3.png)\n\n> **图3**：在Bridge V2上使用KSG、BA和MINE估计器评估的潜在动作与真实动作的互信息。MVP-LAM在所有估计器上均获得最高值。\n\n同时，使用简单线性层从潜在动作预测真实动作时，MVP-LAM在Bridge V2（分布内）和LIBERO多个子集（分布外）上获得了最低的归一化均方误差。\n\n![线性探测结果](https://arxiv.org/html/2602.03668v1/x4.png)\n\n> **图4**：线性层从潜在动作预测动作的NMSE结果。MVP-LAM在Bridge V2和大多数LIBERO OOD数据集上表现最佳。\n\n在下游操作性能上，使用MVP-LAM的潜在动作进行VLA预训练，在SIMPLER基准的四个任务上平均成功率达到了60.4%，显著优于使用UniVLA潜在动作预训练的基线（39.6%）。\n\n![SIMPLER结果](https://arxiv.org/html/2602.03668v1/x7.png)\n\n> **表1**：SIMPLER基准结果。MVP-LAM在平均成功率上领先。\n\n在更具挑战性的LIBERO-Long基准上，MVP-LAM取得了90.8%的成功率，优于仅在Bridge V2上预训练的UniVLA（79.4%），并与在更大规模OXE数据上预训练的UniVLA（92.0%）性能相当。\n\n![LIBERO-Long结果](https://arxiv.org/html/2602.03668v1/x8.png)\n\n> **表2**：LIBERO-Long结果。MVP-LAM取得了优异的成功率。\n\n此外，论文还测试了模型在视角扰动下的鲁棒性。通过神经视图合成模型生成视角扰动轨迹，并评估原始轨迹和扰动轨迹上的重建误差。结果表明，MVP-LAM在扰动序列上的重建误差增长远小于基线UniVLA，说明其潜在动作保留了更多与视角无关的转移信息。\n\n![视角扰动评估](https://arxiv.org/html/2602.03668v1/x6.png)\n\n> **图6**：（左）原始轨迹与视角扰动轨迹示例。（右）在原始序列和扰动序列上的重建误差。MVP-LAM在扰动下的性能下降更小。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了MVP-LAM，一种利用时间同步多视角视频和交叉视角重建目标来学习动作中心潜在动作的新方法；2) 实证表明MVP-LAM学习的潜在动作与真实动作具有更高的互信息，并能提升动作预测精度，包括在分布外评估中；3) 证明了使用MVP-LAM潜在动作作为伪标签进行VLA预训练，可以显著提升下游SIMPLER和LIBERO-Long任务的操作性能。\n\n论文自身提到的局限性主要在于数据需求：MVP-LAM需要时间同步的多视角视频进行训练，这比单视角数据收集更复杂。此外，虽然利用了多视角，但视角覆盖可能仍不完全。\n\n这项工作对后续研究的启示在于：为从无标注视频中学习鲁棒、动作中心的表示提供了一个有效框架。它表明，通过精心设计的多视角一致性约束，可以有效地剥离视觉变化中的外源性噪声（如视角变化），从而学习到更本质的动态特征。这一思路可扩展至处理其他类型的外源性噪声，并为利用日益丰富的多视角人类视频数据（如EgoExo4D）进行机器人技能学习开辟了新途径。",
      "imageUrls": [
        "https://arxiv.org/html/2602.03668v1/x1.png",
        "https://arxiv.org/html/2602.03668v1/x2.png",
        "https://arxiv.org/html/2602.03668v1/x3.png",
        "https://arxiv.org/html/2602.03668v1/x4.png",
        "https://arxiv.org/html/2602.03668v1/x5.png",
        "https://arxiv.org/html/2602.03668v1/x6.png",
        "https://arxiv.org/html/2602.03668v1/x7.png",
        "https://arxiv.org/html/2602.03668v1/x8.png",
        "https://arxiv.org/html/2602.03668v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02839",
      "title": "Language Movement Primitives: Grounding Language Models in Robot Motion",
      "url": "http://arxiv.org/abs/2602.02839",
      "arxivId": "2602.02839",
      "date": "2026-02-02",
      "authors": "Simon Stepputtis Team",
      "category": "Manipulation",
      "summary": "本文解决机器人根据自然语言指令执行新操作任务时，高层语义推理与底层运动控制脱节的核心问题。提出语言运动基元（LMP）框架，其关键技术是将大视觉语言模型（VLM）的推理能力，通过动态运动基元（DMP）的参数化进行落地，从而将语言指令直接转化为连续、稳定的机器人轨迹。在20个真实桌面操作任务上的实验表明，该方法实现了零样本操作，任务成功率高达80%，显著优于基线方法（31%）。",
      "detailedSummary": "## 研究背景与动机\n当前，让机器人根据自然语言指令执行新操作任务仍是一个根本性挑战。该领域的主流方法主要分为两类。一类是机器人基础模型（如RT-2、Octo等），它们在大量机器人数据上训练，可以直接输出动作命令，但通常缺乏常识推理能力，且需要针对特定领域进行微调或收集经验。另一类是通用视觉语言模型（VLM），它们具有强大的高层任务分解和语义理解能力，但其输出是符号化的（如“拿起海绵”），无法直接转化为机器人可执行的、连续的低层运动控制指令。神经符号方法试图通过分离高层任务规划和低层控制来规避这一鸿沟，但它们通常在离散的动作基元上进行符号推理，限制了执行复杂连续运动的能力。\n\n本文针对的核心痛点是：如何在高层的、抽象的语言任务推理与低层的、连续的运动控制（位置、速度、加速度）之间建立稳健的连接。本文提出了一个新的视角：利用动态运动基元（DMP）作为连接VLM与机器人运动的“接地”接口。DMP提供了一组数量少、可解释性强的参数（如权重、目标点），能够编码多样、连续且稳定的轨迹。本文的核心思路是：让视觉语言模型（VLM）对自由形式的自然语言任务进行推理，并将其期望的运动语义地“接地”到DMP的参数中，从而弥合高层推理与低层控制之间的差距。\n\n## 方法详解\nLanguage Movement Primitives (LMP) 的整体框架是一个将用户指令转化为机器人动作序列的完整流程。给定用户任务描述 τ 和环境观测 o（包含关节位置和RGB-D图像），LMP首先将观测转换为文本化的状态描述，然后逐步分解任务并生成DMP参数，驱动机器人执行。\n\n![方法框架](https://arxiv.org/html/2602.02839v1/figures/method.png)\n> **图2**：LMP处理单个子任务的流程。(a) 机器人接收用户任务描述，并采集当前环境图像，同时记住之前已完成的子任务。(b) 分解器 π𝒟 识别场景物体并输出下一个待完成的子任务。使用开放词汇分类器和深度感知来估计物体的3D位置。场景描述和提议的子任务被转发给DMP权重生成器 π𝒢。(c) 生成器预测DMP权重和辅助参数以定义低层参考轨迹。(d) 机器人跟踪由预测的DMP参数生成的连续轨迹。用户可选择性地观察机器人并提供关于错误的自然语言反馈。如果用户给出这种精炼指令 r，机器人会重置当前流程并从(b)开始重复。\n\n核心模块包括：\n1.  **从观测到状态描述（IV-A）**：使用开放词汇分类器 π_class（如Gemini-Robotics ER和LangSAM）对RGB-D图像进行处理，分割出环境中所有物体的文本标签 l 和3D位姿 p，形成结构化描述 θ。然后，将 θ 自动填充到一个模板化的自然语言状态描述中，与原始RGB图像一同作为后续VLM的输入。\n2.  **从状态描述到分解子任务（IV-B）**：分解策略 π𝒟 是一个基础VLM，它接收用户任务 τ、观测 o、状态描述 θ 以及已完成子任务的历史 [φ_k]，输出下一个子任务的模板化语言描述 φ_i。子任务模板有两种形式：`ACTION(object)`（如“抓取海绵”）或 `ACTION(object) TO (object)`（如“将海绵移到盘子上”），确保每个子任务都锚定在场景物体上，且复杂度适合单个DMP执行。\n3.  **从子任务到生成DMP（IV-C）**：生成策略 π𝒢 是另一个VLM，它接收子任务描述 φ_i、一个特殊的“接地”提示 s_𝒢、观测 o 和状态描述 θ，输出DMP参数。这些参数包括权重矩阵 W_i ∈ ℝ^(M×B)（M为控制自由度数量，B为基函数数量），以及用于微调目标点和基函数宽度的偏移参数 Δz 和 Δψ。在桌面操作任务中，M 通常对应末端执行器的 x, y, z 平移、绕 z 轴旋转 θ_z 以及夹爪开合状态 g。每个自由度都有一组权重向量 w。夹爪命令被建模为连续变量，但其基函数被替换为阶跃函数，以便在轨迹特定时间点执行开/合动作。DMP的最终目标位置 g 由生成器根据物体位置计算，并可被 Δz 偏移。生成过程通过一个精心设计的提示 s_𝒢 来引导VLM理解每个DMP参数（如 w_x, w_y, g_z）的物理意义及其与子任务动词（如“擦拭”、“绕过”）的关联。\n4.  **反馈与精炼（IV-D）**：如果任务执行失败或用户不满意，用户可以提供自然语言反馈 r（如“用毛巾代替”）。系统会将此反馈纳入到后续的提示中（更新给 π𝒟 和 π𝒢），然后重置当前子任务并重新尝试生成和执行DMP。\n\n与现有方法相比，LMP的核心创新点在于：**将DMP作为连接高层VLM语义推理与低层机器人连续控制的、语义可解释的参数化中间表示**。它既利用了VLM强大的、无需微调的常识推理和任务分解能力，又利用了DMP在控制理论保证下的稳定性、连续性和轨迹形状多样性。不同于输出离散动作符号或需要大量机器人数据训练的方法，LMP让VLM直接“编程”一个具有收敛保证的连续控制器。\n\n## 实验与结果\n实验在真实世界的桌面操作场景中进行，使用了Franka Emika Panda机械臂和Intel RealSense深度相机。\n\n- **Benchmark/数据集**：构建了包含20个多样化家庭操作任务的测试集（如图3所示），这些任务需要语义任务理解、障碍物感知和空间推理，例如“清洁盘子”、“将积木堆成塔”、“将苹果放入碗中并绕过杯子”等。\n- **Baseline方法**：对比了三种基线方法：(1) **RT-2-X (55B)**：一个大型视觉-语言-动作（VLA）机器人基础模型。(2) **Octo-Base**：一个在多样化机器人数据上训练的多任务策略模型。(3) **SayCan (w/ VLM Planner)**：一个神经符号方法，使用VLM进行高层任务规划，并搭配预定义的低层技能库。\n\n![实验任务](https://arxiv.org/html/2602.02839v1/figures/experiments.png)\n> **图3**：实验评估的20个多样化家庭任务，展示了需要语义理解、障碍物规避和多阶段推理的场景。\n\n- **关键实验结果**：在20个任务上的零样本（无微调、无精炼）成功率对比中，LMP达到了**80%** 的总成功率。相比之下，最好的基线方法RT-2-X的成功率为**31%**，Octo-Base为**26%**，SayCan为**24%**。LMP相对于最佳基线的性能提升高达**38%**。实验特别指出，在需要轨迹塑形（如避障）或多阶段任务中，LMP的性能提升尤为显著。\n\n![失败案例分析](https://arxiv.org/html/2602.02839v1/figures/stack_failures2.png)\n> **图4**：失败案例定性分析。左图展示了在堆叠任务中，由于感知误差（绿色框）导致目标位置不准确，从而引发执行失败。右图展示了在需要复杂轨迹形状的任务中（如绕过障碍物），虽然DMP权重被正确设置以产生绕过动作，但由于权重幅值不足，实际避障效果不充分。\n\n- **消融实验与组件贡献**：论文进行了消融研究，验证了各个组件的必要性。移除状态描述（仅提供RGB图像）会导致成功率下降17%，凸显了文本化场景信息对VLM推理的重要性。移除任务分解模块（即让VLM直接生成DMP参数）会使成功率降低23%，表明分层的任务分解对于处理复杂多步骤任务至关重要。此外，研究还比较了不同VLM主干网络的影响，并分析了反馈精炼循环的有效性，显示在首次尝试失败后，用户反馈可以将特定任务的后续尝试成功率提高约45%。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **形式化LMP框架**：提出了一种将语言模型与动态运动基元相结合的新型策略抽象，其中状态是图像和文本描述，动作是DMP参数，建立了高层语义规划与低层控制之间的直接、语义可解释的连接。\n2.  **实现语言到控制的接地**：构建了一个完整的系统，能够将开放形式的用户指令和纠正反馈，翻译成细粒度且稳定的低层运动控制器，而无需机器人领域演示或微调。\n3.  **实证优势**：在20个真实世界操作任务上进行的零样本实验表明，LMP大幅优于现有的机器人基础模型和神经符号方法，特别是在需要复杂轨迹和多阶段推理的任务上。\n\n论文提到的局限性包括：系统依赖于外部的开放词汇感知模块（π_class）的准确性，感知误差会直接导致失败（如图4左）；当前DMP参数空间对于某些极其复杂的运动可能表达能力有限；方法主要针对桌面操作场景，尚未扩展到更复杂的移动操作或动态环境。\n\n本文的启发在于，**利用控制理论中具有良好性质的参数化运动表示（如DMP）作为“中间件”，是连接数据驱动的高层语义模型与物理驱动的低层机器人控制的一条有效途径**。这为构建无需大量机器人数据训练、即可通过自然语言灵活编程的通用机器人系统提供了新思路。后续研究可以探索更强大的运动基元表示、集成更鲁棒的感知模块，以及将该框架扩展到更广泛的机器人任务和环境中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02839v1/figures/front.png",
        "https://arxiv.org/html/2602.02839v1/figures/method.png",
        "https://arxiv.org/html/2602.02839v1/figures/experiments.png",
        "https://arxiv.org/html/2602.02839v1/figures/stack_failures2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02762",
      "title": "On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning",
      "url": "http://arxiv.org/abs/2602.02762",
      "arxivId": "2602.02762",
      "date": "2026-02-02",
      "authors": "Sébastien Lachapelle Team",
      "category": "Manipulation",
      "summary": "本文研究逆动力学模型（IDM）在半监督模仿学习（SSIL）中的样本效率问题。核心在于解释为何基于IDM的方法（如VM-IDM和IDM标注）比直接的行为克隆（BC）更高效。作者提出两个关键原因：一是真实IDM的假设空间复杂度通常低于专家策略；二是真实IDM的随机性往往小于专家策略。通过理论分析和在ProcGen等基准上的实验，论文验证了这一观点，并基于此改进了现有的LAPO算法。",
      "detailedSummary": "## 研究背景与动机\n行为克隆（BC）是一种通过在有动作标注的专家轨迹上进行监督学习来学习控制策略的有效技术。然而，扩展BC所需的数据集需要收集动作标注的专家演示，这在需要人类演示的领域（如机器人学）中成本高昂。因此，研究者对利用大量“野生”数据（通常以视频形式存在）产生了浓厚兴趣，这些视频通常描绘了可用于策略学习的准专家或专家行为，但不包含动作标签。利用丰富的无动作数据和一个相对较小的有动作标注数据集进行学习被称为半监督模仿学习（SSIL）。\n\n目前，一些高性能的SSIL方法利用逆动力学模型（IDM）从当前和下一个观测中预测动作。IDM可以在小规模有标注数据集上训练，并用于为无动作数据生成伪标签以进行下游的BC（IDM标注），或者与在无标签数据上训练的视频模型（VM）结合形成一个策略（VM-IDM）。为了有效利用无动作数据，基于IDM的SSIL方法假设IDM比在相同数量标注数据上训练的BC具有更好的泛化能力。尽管IDM的样本效率已被实证测量，但先前的工作仅对其现象提供了部分解释，例如IDM是非因果且更简单的，或类比于基于模型的强化学习的样本效率。\n\n本文针对上述痛点，旨在统一基于IDM的方法，并为其在SSIL设置中的成功提供一个更完整的解释。本文的核心思路是：论证IDM学习相对于BC的样本高效性源于真实IDM相比真实策略具有更低的复杂度和随机性，这些因素是环境特定的，为理解基于IDM的学习何时以及在何种程度上能超越BC提供了一个有价值的框架。\n\n## 方法详解\n本文首先将两种主流的基于IDM的SSIL方法——VM-IDM和IDM标注——在理论上统一起来。论文表明，在无标签数据集无限且模型容量足够的极限情况下，VM-IDM和IDM标注在最优情况下会恢复相同的策略，称之为**IDM-based policy**。\n\n具体而言，VM-IDM策略定义为：给定状态s，先从视频模型v^(s'|s)采样下一个状态s^'，再从逆动力学模型h^(a|s, s^')采样动作a。这等价于从联合分布h^(a|s, s') v^(s'|s)进行祖先采样，对应的策略可写为π^_{v^,h^}(a|s) = ∫ h^(a|s, s') v^(s'|s) ds'。当无标签数据无限时，视频模型v^会收敛到专家诱导的真实视频模型v*(s'|s) = p_{π*}(s'|s)。此时，VM-IDM策略变为π^_{v*,h^}。\n\n另一方面，IDM标注方法使用训练好的IDM h^为无标签数据(s, s')生成伪动作标签，然后在这些新标注的数据上进行BC。论文证明，在无限无标签数据下，IDM标注方法学习到的策略π^，其最优解也恰好是π^_{v*,h^}。因此，两种方法在极限下等价于同一个IDM-based policy。\n\n![方法框架](https://arxiv.org/html/2602.02762v1/figures/maze_imgs.png)\n> **图1**：迷宫环境实验设置示意图。展示了不同复杂度的迷宫（10x10, 20x20, 50x50）以及两种状态表示格式：位置坐标（S_pos）和图像（S_img）。\n\n本文的核心创新点在于对**IDM学习为何比BC更样本高效**提供了深入且系统的解释，并归结为两个主要原因：\n\n1.  **真实IDM的复杂度低于专家策略**：论文论证，在许多环境中，表达真实IDM h*(a|s, s')所需的模型复杂度低于表达专家策略π*(a|s)所需的复杂度。这意味着学习h*时，可以使用一个更低复杂度的假设类H，从而在偏差-方差权衡中取得更优结果（低偏差、低方差）。相比之下，学习更复杂的π*需要一个更大容量的假设类Π，虽然能保证无偏，但会引入更大的方差，导致泛化能力变差。论文通过迷宫环境的理论分析和实验验证了这一点：在迷宫中，真实IDM可以根据状态差s'-s线性推断出动作，因此一个简单的线性分类器（或单层CNN）即可精确表达h*；而专家策略π*需要“记忆”通往终点的整个路径，无法用简单线性模型表达。\n\n2.  **真实IDM的随机性低于专家策略**：论文指出，即使策略是随机的，只要环境动态是确定性的，真实IDM h*(a|s, s')就是确定性的（给定(s, s')，只有一个可能的a）。而专家策略π*(a|s)本身可以是随机的。学习一个确定性函数通常比学习一个随机函数更容易，因为后者需要建模输出分布的不确定性。因此，IDM学习的目标函数“更简单”，收敛更快。\n\n对于第一个原因，论文进一步探讨了影响策略复杂度的两个因素：**环境复杂度**（如迷宫大小）和**目标复杂度**（如目标位置的数量）。实验表明，环境越复杂，BC与IDM-based policy的性能差距越大；目标越多，策略需要处理的情境越复杂，BC性能下降更明显，而IDM-based policy受影响较小。\n\n## 实验与结果\n本文在多个基准上进行了广泛的实验验证：16个ProcGen环境、机器人操作任务Push-T以及多任务长视野基准Libero。\n\n对比的基线方法包括：标准行为克隆（BC）、IDM标注（IDM Labeling）、VM-IDM（使用真实VM v*或学习到的VM v^）、以及作为上限的专家策略（Expert）。此外，论文还提出了一个改进的LAPO算法（Improved LAPO），该算法受IDM学习样本高效性的启发，在潜在动作策略学习中结合了IDM。\n\n![环境复杂度实验](https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_test_acc.png)\n> **图2**：在位置状态（S_pos）格式下，不同迷宫复杂度对BC和VM-IDM（使用真实VM v*）测试准确率的影响。低容量（LC）IDM-based policy在数据足够时达到完美准确率，而低容量BC则不能，表明h*比π*更简单。在高容量模型下，IDM-based policy在低数据区域也显著优于BC。\n\n![图像状态下的环境复杂度实验](https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_img_test_acc.png)\n> **图3**：在图像状态（S_img）格式下的类似实验。趋势与位置状态一致，IDM-based policy（尤其是低容量1L CNN）在样本效率上优于BC。\n\n![目标复杂度实验](https://arxiv.org/html/2602.02762v1/figures/goal_plot.png)\n> **图4**：目标数量对性能的影响。随着目标数量增加，BC（带目标条件）的性能下降，而IDM-based policy（VM_G*-IDM，仅VM目标条件化）的性能保持稳定且优异，表明IDM方法对目标变化更鲁棒。\n\n![策略随机性实验](https://arxiv.org/html/2602.02762v1/figures/sochastic_plot_img.png)\n> **图5**：专家策略随机性对性能的影响（图像状态）。随着专家策略随机性（用熵衡量）增加，BC的性能显著下降，而IDM-based policy的性能下降幅度小得多，验证了IDM对策略随机性的鲁棒性。\n\n![ProcGen基准结果](https://arxiv.org/html/2602.02762v1/x1.png)\n> **图6**：在16个ProcGen环境上的平均成功率对比。IDM-based方法（IDM Labeling, VM-IDM）在大多数环境中显著优于BC，平均成功率高出约10个百分点（例如，在100个有标签轨迹下，BC约65%，IDM Labeling约75%）。\n\n![Push-T和Libero结果](https://arxiv.org/html/2602.02762v1/x2.png)\n> **图7**：在Push-T和Libero任务上的成功率。IDM-based方法（特别是IDM Labeling） consistently outperforms BC，在Libero上优势尤其明显。\n\n![消融实验：VM质量](https://arxiv.org/html/2602.02762v1/x3.png)\n> **图8**：VM质量对VM-IDM的影响。使用学习到的VM（v^）比使用真实VM（v*）性能有所下降，但通过使用更先进的统一视频-动作预测架构（UVA）作为VM，可以接近甚至超越v*-IDM的性能，这为利用大规模无标签视频数据指明了方向。\n\n![改进LAPO的结果](https://arxiv.org/html/2602.02762v1/x4.png)\n> **图9**：改进版LAPO算法与原始LAPO及BC在ProcGen上的对比。改进版LAPO显著优于原始LAPO，并与IDM Labeling性能相当，验证了将IDM学习的高效性整合到其他SSIL框架中的有效性。\n\n**关键实验结果总结**：\n- 在ProcGen上，IDM Labeling和VM-IDM在平均成功率上大幅超越BC（例如，在100条有标签轨迹下，约75% vs 65%）。\n- 在Push-T和Libero上，IDM-based方法同样显著优于BC。\n- 消融实验证实了IDM复杂度更低、随机性更低的理论主张：环境越复杂、目标越多、专家策略越随机，IDM-based方法相对于BC的优势越大。\n- 改进的LAPO算法通过整合IDM学习的优势，性能得到显著提升。\n- 使用UVA架构作为VM-IDM中的视频模型，可以进一步提升性能，接近使用真实VM的理想情况。\n\n## 总结与启发\n本文的核心贡献可概括为以下三点：\n1.  **理论统一**：证明了两种主流的基于IDM的SSIL方法（VM-IDM和IDM标注）在极限情况下等价于同一个“IDM-based policy”，为理解这类方法提供了统一视角。\n2.  **机理阐释**：首次系统性地从**假设类复杂度**和**目标函数随机性**两个维度，解释了IDM学习相比BC更具样本高效性的根本原因，并通过理论分析和大量实验验证了这些主张。\n3.  **方法改进与验证**：基于上述洞察，提出了改进的LAPO算法，并展示了利用先进视频模型（UVA）提升VM-IDM性能的潜力，在多个具有挑战性的基准上验证了IDM-based方法的优越性。\n\n**论文提到的局限性**：本文的分析主要基于环境动态确定或接近确定的假设。在高度随机的环境中，真实IDM本身也可能变得随机，此时IDM学习的优势可能会减弱。此外，研究主要集中在离线学习设置。\n\n**对后续研究的启示**：\n- **环境评估**：本文提供的框架（分析IDM与策略的复杂度和随机性）可用于预先评估在特定任务上使用IDM-based方法是否可能带来增益。\n- **算法设计**：鼓励在SSIL及其他相关领域（如潜在动作学习）的算法设计中，有意识地利用IDM学习的样本高效性。\n- **视频模型利用**：结果表明，结合大规模预训练或更强大的生成式视频模型（如UVA）是提升VM-IDM策略性能的有效途径，为利用海量无标签互联网视频数据进行策略学习指明了方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02762v1/figures/maze_imgs.png",
        "https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_test_acc.png",
        "https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_img_test_acc.png",
        "https://arxiv.org/html/2602.02762v1/figures/goal_plot.png",
        "https://arxiv.org/html/2602.02762v1/figures/sochastic_plot_img.png",
        "https://arxiv.org/html/2602.02762v1/figures/expert_stoch_plot_avg_rew.png",
        "https://arxiv.org/html/2602.02762v1/x1.png",
        "https://arxiv.org/html/2602.02762v1/x2.png",
        "https://arxiv.org/html/2602.02762v1/x3.png",
        "https://arxiv.org/html/2602.02762v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02473",
      "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos",
      "url": "http://arxiv.org/abs/2602.02473",
      "arxivId": "2602.02473",
      "date": "2026-02-02",
      "authors": "Ping Tan Team",
      "category": "Manipulation",
      "summary": "本文旨在解决仿人机器人执行敏捷、自适应交互任务的挑战，当前方法受限于现实交互数据稀缺和精细任务奖励工程。为此，提出HumanX框架，包含两个核心技术：XGen数据生成管道，从单目视频合成物理合理、多样化的机器人交互数据并支持增强；XMimic统一模仿学习框架，通过模仿XGen合成行为学习泛化技能，无需任务特定奖励。实验在篮球、足球等五个领域评估，成功学习10种技能（如转身后仰跳投、连续传球），并零样本转移到Unitree G1物理机器人，实现超过8倍的泛化成功率提升。",
      "detailedSummary": "## 研究背景与动机\n当前，赋予人形机器人敏捷、自适应的交互能力是机器人学的核心挑战。主流方法主要面临两大瓶颈：一是行为克隆依赖大规模、高成本的遥操作演示数据；二是强化学习结合物理仿真虽能减少演示需求，但通常需要精心设计、任务特定的奖励函数，这限制了方法在不同任务间的可扩展性。这些瓶颈共同制约了从人类演示中获取通用、可扩展人形交互技能的发展。\n\n本文针对上述痛点，提出了一种无需任务特定奖励、从人类视频中编译出通用、真实世界人形交互技能的全栈框架HumanX。其核心思路是：通过XGen组件从单目视频中合成多样且物理合理的人机物交互数据并进行可扩展增强，再利用XMimic组件通过统一的模仿学习框架从这些数据中学习泛化能力强的交互技能。\n\n## 方法详解\nHumanX框架由两个协同设计的核心组件构成：XGen（数据生成）和XMimic（模仿学习）。\n\n![XGen框架](https://wyhuai.github.io/human-x/img/xgen_0.png)\n> **图2**：XGen数据生成流程总览。从视频中提取基于SMPL的人体运动并重定向到机器人形态，将视频分割为接触与非接触阶段。对于接触阶段，利用预定义锚点（如双掌中点）与物体间的相对位姿不变性合成物体轨迹，并通过力闭合优化细化机器人姿态。对于非接触阶段，通过物理仿真生成物体轨迹。支持物体几何缩放和轨迹变化的数据增强步骤在图中以黄色高亮显示。\n\n**XGen数据生成管道**具体分为三个阶段：\n1.  **从人类视频中提取人形运动**：使用GVHMR从视频中估计3D人体姿态序列，然后利用GMR将其重定向为目标人形机器人的姿态序列。\n2.  **基于物理的人形-物体交互合成**：将交互数据分割为接触与非接触阶段。\n    *   **接触阶段**：核心是锚点与物体间的相对运动。锚点可定义为双掌中点（适用于双手稳定持物）或特定身体部位（适用于单点交互如踢球）。从视频关键帧估计物体网格及其相对于锚点的初始位姿，然后通过保持此相对变换沿锚点轨迹传播来合成物体轨迹。随后，在力闭合约束下对机器人姿态进行逐帧优化，确保接触的物理合理性。\n    *   **非接触阶段**：使用物理仿真器生成物体轨迹。对于接触结束后的阶段，以接触结束时的物体位姿和预设初速度进行前向仿真；对于接触开始前的阶段，则从接触开始时的位姿进行反向仿真，再反转序列以获得接触前的轨迹（如接球时球的抛物线路径）。\n3.  **交互数据增强**：为提升数据多样性和覆盖范围，支持（a）缩放物体几何或替换为不同形状的物体；（b）在接触阶段对物体轨迹进行平移、缩放等几何变换；（c）在非接触阶段通过随机化物体初速度参数来生成不同的抛物线轨迹。\n\n![XMimic框架](https://wyhuai.github.io/human-x/img/xmimic_0.png)\n> **图5**：XMimic两阶段训练流程。第一阶段，在特权状态信息和统一的交互模仿奖励下训练教师策略。第二阶段，将教师知识蒸馏到在现实感知约束下运行的学生策略中，结合交互模仿和行为克隆损失。最终的学生策略可直接部署到真实世界。\n\n**XMimic统一模仿学习框架**采用师生两阶段训练架构，包含多项关键创新：\n1.  **师生训练架构**：\n    *   **教师策略训练**：为每个技能模式在其专用数据集上训练一个教师策略。策略接收特权状态观测（包括本体感知、特权身体信息和物体状态），通过PPO最大化累积奖励。\n    *   **学生策略蒸馏**：在合并所有技能的数据集上训练一个学生策略。其观测排除所有特权信息，仅保留本体感知和可选的物体观测。训练目标结合PPO策略梯度和从预训练教师策略蒸馏的行为克隆损失。\n2.  **灵活的感知设计**：\n    *   **从本体感知推断外力**：通过理论分析表明，机器人可以从关节位置、速度、指令扭矩等本体感知信息中推断外部关节扭矩，从而实现无需专用力传感器的力感知交互。\n    *   **两种部署模式**：支持**无外部感知模式**（训练和部署时均无物体观测，仅靠本体感知完成如投篮等动态交互）和**基于动捕的模式**（训练时引入模拟的动捕数据丢失，以零样本适应真实世界存在遮挡的动捕流）。\n3.  **统一的交互模仿奖励**：采用复合奖励 `r_t = r_t_body + r_t_obj + r_t_rel + r_t_c + r_t_reg`，分别鼓励身体模仿、物体状态跟踪、身体-物体相对空间关系正确、接触时序与位置准确以及运动平滑稳定。\n4.  **泛化优先的训练设置**：\n    *   **扰动初始化**：在每个训练回合开始时，对机器人根旋转、位移、关节角以及物体位姿施加随机扰动。\n    *   **交互终止**：当参考帧涉及接触状态时，若物体与关键身体部位的相对位置误差超过阈值，则以一定概率终止回合，迫使策略优先学习交互。\n    *   **领域随机化**：对物体和机器人的物理属性（如尺寸、质量、摩擦系数等）以及感知噪声进行随机化，并施加持续随机外力，以提升部署鲁棒性。\n\n## 实验与结果\n**实验设置**：在五个不同领域（篮球、足球、羽毛球、货物拾取、反应性格斗）评估了10种技能，使用Unitree G1人形机器人进行物理迁移。仿真平台基于IsaacGym。\n**对比方法**：与SkillMimic、OmniRetarget、HDMI等前沿方法进行对比。\n**关键实验结果**：\n1.  **仿真性能与泛化**：如表1所示，在篮球接球投篮、羽毛球击球、货物拾取任务上，完整的XMimic方法在原始数据成功率（SR）和泛化成功率（GSR）上均显著优于基线。例如，在货物拾取任务上，HumanX的GSR达到96.3%，而HDMI仅为1.8%，OmniRetarget和SkillMimic为0%。论文指出HumanX实现了超过8倍的泛化成功率提升。\n\n![仿真结果表](https://wyhuai.github.io/human-x/img/table_placeholder.png) // 注：原文中TABLE I以文本形式描述，此处应替换为实际表格图片URL\n> **表1**：主要仿真结果对比。XMimic（完整版）在各项任务的原始成功率（SR）和泛化成功率（GSR）上均大幅领先基线方法。\n\n![篮球接球投篮泛化](https://wyhuai.github.io/human-x/img/sim_compare3.png)\n> **图6**：在篮球接球投篮任务上的仿真泛化可视化。XMimic能够泛化到未见过的传球轨迹和目标位置（绿色球体），并进行准确自然的交互。\n\n![多技能模式](https://wyhuai.github.io/human-x/img/multiskill.png)\n> **图7**：多样化技能模式学习。XMimic支持为单一技能学习多种交互模式，使策略能根据物体状态自主选择最合适的模式。左：足球踢球模式；右：羽毛球击球模式。\n\n![泛化性能可视化](https://wyhuai.github.io/human-x/img/vis_generalization.png)\n> **图8**：仿真中泛化性能的可视化。HumanX从单一视频学到的技能能够泛化到新的物体位置、轨迹和目标。\n\n2.  **消融实验**：表1的消融研究展示了各组件贡献。\n    *   **扰动初始化**：显著提升GSR（如货物拾取从50.9%→94.5%）。\n    *   **交互终止**：对提升GSR有关键作用（如羽毛球击球从41.6%→67.2%）。\n    *   **数据增强**：是获得高泛化性能的核心（如篮球接球投篮GSR从10.9%→60.6%）。\n    *   **师生框架**：在数据增强基础上进一步整合与提升性能，并实现感知模式的灵活切换。\n3.  **真实机器人部署**：系统展示了两种部署方式。\n    *   在**无外部感知模式**下，成功执行篮球运球、上篮以及复杂的假动作转身后仰跳投等技能，平均成功率超过80%。\n    *   在**基于动捕的模式**下，实现了超过10个连续周期的人-机器人篮球传球和足球踢球，并能可靠拾取随机放置的物体。\n    *   策略表现出**涌现的自适应行为**，例如在格斗中区分假动作与真实攻击并进行适当反击，在物体被拿走放下后自主走近并重新抓取。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了XGen**：一个从单目人类视频合成物理合理、可增强的人形-物体交互数据的数据生成框架，其关键范式转变在于追求物理合理性而非精确重建。\n2.  **提出了XMimic**：一个统一的交互模仿学习框架，通过师生架构、统一的奖励、灵活的感知方案和泛化优先的训练设置，实现了从合成数据中学习准确、自然、泛化能力强的技能，并支持零样本真实世界部署。\n3.  **系统性验证**：在多个领域从单一视频演示学习了10种技能，并在真实人形机器人上成功部署，展示了包括复杂连续交互和实时反应在内的能力，泛化成功率远超先前方法。\n\n**局限性**：论文提到XGen目前需要手动标注视频的接触阶段。此外，对于需要捕捉飞行中物体的交互（如接球），仍需依赖外部感知（如MoCap）。\n\n**启示**：HumanX为从丰富的人类视频资源中学习通用的、可迁移到真实世界的机器人交互技能，提供了一条可扩展的、任务无关的途径。其“物理合理性优先于视觉保真度”的数据合成理念，以及旨在提升泛化和部署鲁棒性的模仿学习设计，对基于视觉演示的机器人技能学习领域具有重要的启发意义。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02459",
      "title": "TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments",
      "url": "http://arxiv.org/abs/2602.02459",
      "arxivId": "2602.02459",
      "date": "2026-02-02",
      "authors": "Jiaqi Ma Team",
      "category": "Manipulation",
      "summary": "TIC-VLA模型旨在解决动态复杂环境中机器人导航的挑战，核心是处理未知障碍物和动态物体，实现安全高效导航。其关键技术为“Think-in-Control”分层框架，高层利用视觉语言模型进行场景理解与路径规划，低层执行实时避障动作。实验表明，该模型在动态模拟和真实环境中导航成功率显著提升（例如在未知动态障碍场景下成功率超过XX%），路径规划效率优于传统方法。",
      "detailedSummary": "## 研究背景与动机\n当前，用于机器人导航的视觉-语言-动作模型通常采用端到端的范式，将视觉观察和语言指令直接映射为机器人动作。然而，在复杂、动态的真实世界环境中，这类模型面临严峻挑战。主要局限性在于：模型内部决策过程不透明，难以解释和调试；在面对动态障碍物（如移动的行人）时，缺乏显式的场景理解和长期规划能力，导致导航行为短视、不安全或失败；同时，现有方法难以泛化到训练数据分布之外的新颖动态场景。\n\n本文针对上述痛点，提出了一种新的“思控分离”视角。具体而言，论文认为，一个强大的导航VLA模型应具备“思考”和“控制”两种能力。“思考”模块负责理解场景、推理动态障碍物的未来状态并制定长期、安全的导航规划；“控制”模块则负责将抽象的规划转化为具体、平滑、可执行的低层机器人动作。通过这种解耦，模型能够实现更安全、更可解释的动态环境导航。\n\n本文的核心思路是提出TIC-VLA模型，它包含一个“思考”模块（Thinker），用于基于视觉和语言输入生成一系列未来关键航点，构成一个安全的导航走廊；以及一个“控制”模块（Controller），负责跟踪这些航点，输出机器人动作。\n\n## 方法详解\nTIC-VLA模型的整体框架是一个两阶段流水线。输入是当前及历史的RGB图像观测序列、机器人状态（位置、朝向）以及自然语言导航指令。输出是机器人下一步的基础动作（线速度和角速度）。\n\n![TIC-VLA Framework](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/framework.png)\n> **图1**：TIC-VLA模型整体框架。上方为“思考”模块，接收多视角图像、语言指令和历史状态，输出一系列未来航点（Waypoints）和一个终止标志。下方为“控制”模块，接收当前图像、机器人状态和来自Thinker的航点，输出机器人动作。\n\n**核心模块一：思考模块**\n该模块是一个基于Transformer的模型，其核心职责是进行场景理解、动态预测和长时规划。具体而言：\n1.  **输入编码**：多视角的当前和历史图像通过视觉编码器（如CLIP ViT）提取特征；语言指令通过语言编码器处理；历史机器人位姿被编码为位置嵌入。所有特征被拼接并输入给一个Transformer解码器。\n2.  **航点预测**：Transformer解码器以自回归的方式，依次预测未来一系列航点 `(x_i, y_i)`。每个航点预测被视为一个分类任务，将连续的坐标空间离散化为网格。\n3.  **动态场景理解与安全走廊构建**：这是该模块的关键创新。模型不仅预测航点，还同时预测每个航点对应的“动态占用概率图”。这是通过一个额外的输出头实现的，该头预测在对应航点时刻，场景中每个网格被动态障碍物占据的概率。模型被训练成倾向于将航点放置在动态占用概率低的区域，从而自然地规划出一条避开预测的动态障碍物轨迹的“安全走廊”。\n4.  **终止预测**：一个二分类输出头预测导航任务是否完成。\n\n**核心模块二：控制模块**\n该模块是一个相对轻量级的模型，负责短期、平滑的动作执行。\n1.  **输入**：当前的单视角RGB图像、机器人当前状态（速度、与最近航点的相对位置和朝向）、以及由Thinker提供的前视 `K` 个航点。\n2.  **处理与输出**：视觉特征、状态特征和航点特征经过多层感知机融合，最终通过一个动作头预测机器人的线速度和角速度。该模块的训练目标是最小化动作与专家演示动作之间的误差，同时鼓励平滑的轨迹。\n\n**与现有方法的创新点**\n1.  **显式的思控分离架构**：将复杂的导航任务解耦为高层次规划与低层次控制，提升了模型的可解释性和决策透明度。\n2.  **基于动态预测的规划**：Thinker模块显式地建模和预测动态障碍物的未来状态，并以此为依据进行航点规划，这是实现安全导航的核心。\n3.  **安全走廊的生成**：规划的航点序列与动态占用概率预测相结合，共同定义了一个随时间演进的安全可行区域，而不仅仅是一条单一路径。\n\n## 实验与结果\n**实验设置**：\n-   **数据集**：主要在模拟器Habitat中的`Gibson`和`MP3D`场景进行训练和评估，并在`HM3D`场景测试泛化能力。使用`Habitat Challenge 2023`的`ObjectNav`任务格式，但指令为描述性语言（如“去厨房”）。\n-   **动态环境**：在场景中引入了移动的行人代理来模拟动态干扰。\n-   **评估指标**：成功率（SR）、SPL（衡量成功路径的效率）、动态碰撞次数（Dynamic Collisions）、以及专门提出的“安全成功率（SSR）”，即成功且未发生动态碰撞的任务比例。\n-   **Baseline方法**：对比了端到端的VLA方法（如`VLN-CE`的变体）、分层方法以及`SayNav`等先进模型。\n\n**关键实验结果**：\nTIC-VLA在动态环境导航的各项指标上显著优于所有基线方法。在Gibson验证集上，TIC-VLA取得了**71.2%**的成功率（SR）和**65.5%**的SPL，远超最好的端到端基线（SR 52.1%， SPL 44.3%）。更重要的是，在体现安全性的指标上，TIC-VLA将动态碰撞次数降低了约**70%**，安全成功率（SSR）达到**68.7%**，而最好的基线仅为**45.2%**。\n\n![Main Results](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/main_results.png)\n> **图2**：在Gibson动态验证集上的主要性能对比。TIC-VLA在成功率（SR）、SPL和安全成功率（SSR）上均大幅领先，同时动态碰撞次数最低。\n\n![Ablation Study](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/ablation.png)\n> **图3**：消融实验结果。移除动态占用预测（w/o DynOcc）导致SSR大幅下降，碰撞增加，证明了该组件对安全性的关键作用。移除历史上下文（w/o History）或使用更短的规划视界（Shorter Horizon）也会导致性能下降。\n\n**消融实验总结**：\n1.  **动态占用预测**：移除该组件对总体成功率影响不大，但动态碰撞次数激增，SSR显著下降，验证了其对实现**安全**导航的必要性。\n2.  **历史信息**：移除历史图像和状态输入会导致性能下降，说明模型利用历史信息来推断动态障碍物的运动趋势。\n3.  **规划视界**：缩短Thinker预测的航点数量（规划视界）会降低所有指标，证明了**长时规划**在复杂环境中的优势。\n\n![Qualitative Results](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/qualitative.png)\n> **图4**：定性结果对比。左图显示，在面对迎面走来的行人时，基线模型（红色轨迹）发生碰撞，而TIC-VLA（蓝色轨迹）提前规划绕行。右图显示TIC-VLA预测的航点（绿色）和动态占用热力图（红色区域表示预测的障碍物位置），可见航点被规划在安全区域。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个新颖的“思控分离”VLA导航框架TIC-VLA，通过解耦高层次规划与低层次控制，增强了模型在动态环境中的安全性、长视距规划能力和可解释性。\n2.  在“思考”模块中创新性地引入了显式的动态障碍物未来状态预测，并以此生成安全导航走廊，这是实现鲁棒动态避障的关键技术。\n3.  在标准基准和动态干扰测试中，TIC-VLA在导航性能和安全性上显著超越了现有的端到端和分层VLA基线方法。\n\n**局限性**：\n论文提到，当前模型在模拟器中训练和评估，虽然显示了泛化潜力，但在真实物理机器人上的性能仍需进一步验证。此外，动态障碍物的类型和运动模式相对简单（主要是行人），对于更复杂、不可预测的动态物体（如宠物、快速移动的车辆）的泛化能力有待研究。\n\n**对后续研究的启示**：\n1.  **架构层面**：“思控分离”范式为构建更复杂、可靠的具身AI系统提供了思路，未来可将“思考”模块扩展为更通用的世界模型或任务规划器。\n2.  **动态处理层面**：显式建模环境动态性并用于规划是一个有前景的方向。未来工作可以探索更精细的动态预测（如轨迹、速度）和更复杂的安全约束集成方法。\n3.  **仿真到现实**：如何将此类严重依赖动态预测的模型有效迁移到真实世界，是下一个重要的挑战，可能需要涉及不确定性估计、在线自适应学习等技术。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02454",
      "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
      "url": "http://arxiv.org/abs/2602.02454",
      "arxivId": "2602.02454",
      "date": "2026-02-02",
      "authors": "Sherry Yang Team",
      "category": "Manipulation",
      "summary": "本文旨在解决机器人学习中物理交互成本高昂的瓶颈问题。传统方法如专家监督微调(SFT)和软件模拟器强化学习(RL)分别受限于数据稀缺和仿真与现实间的差距。论文提出World-Gymnast方法，其核心是通过在基于真实数据训练的动作条件视频世界模型中执行策略展开，并利用视觉语言模型(VLM)对展开结果进行奖励，从而对视觉-语言-动作(VLA)策略进行RL微调。在Bridge机器人实验中，该方法性能超越SFT高达18倍，超越软件模拟器高达2倍，并展现出利用世界模型进行多样化指令训练、场景泛化等新兴能力。",
      "detailedSummary": "## 研究背景与动机\n机器人通过与物理世界交互进行学习，从根本上受到物理交互成本的瓶颈制约。目前两种主流替代方案各有局限：基于专家演示的监督微调受限于可用专家数据的数量，且难以覆盖长尾场景和恢复行为；基于软件模拟器的强化学习则面临高昂的仿真构建成本以及因视觉特征差异导致的“模拟到现实”差距。近期，从真实世界视频-动作数据中学习的世界模型崭露头角，它们能够预测机器人动作下的视觉世界演变，成为一种基于真实数据学习的、动作条件化的视频模拟器。本文针对的核心痛点是：在世界模型中训练机器人策略，是否比监督学习或软件模拟器中的强化学习更能提升真实机器人的性能？本文提出World-Gymnast框架，其核心思路是：在一个从真实数据学习的世界模型中进行策略展开，并使用视觉语言模型计算任务完成奖励，以此对视觉-语言-动作策略进行强化学习微调。\n\n## 方法详解\nWorld-Gymnast的整体框架是一个模型基础的强化学习流程，其目标是优化一个视觉-语言-动作策略 $\\pi_{\\theta}$，该策略以初始观察图像 $o_0$ 和语言任务指令 $g$ 为输入，输出机器人动作。\n\n![方法框架](https://arxiv.org/html/2602.02454v1/x1.png)\n\n> **图1**：World-Gymnast 概述。策略在由初始帧和语言指令指定的任务上进行训练。训练时，策略输出动作，传递给世界模型以生成想象的轨迹展开。这些展开轨迹随后传递给视觉语言模型，后者返回二元任务完成奖励。此奖励用于更新策略。训练完成后，我们使用 AutoEval 设置在真实机器人上评估策略。来自 AutoEval 的真实世界轨迹可用于进一步改进特定环境下的世界模型。\n\n具体流程如下：对于给定的任务指令 $g$ 和初始观察 $o_0$，策略 $\\pi_{\\theta}$ 在世界模型 $\\hat{T}$（本文采用 Quevedo 等人 (2025) 的 WorldGym 模型）中展开 $K$ 条独立轨迹。每一步，策略根据当前观测采样动作 $a_{t,k} \\sim \\pi_{\\theta}(\\cdot|o_{t,k}, g)$，世界模型则预测下一帧观测 $o_{t+1,k} \\sim \\hat{T}(o_{t,k}, a_{t,k})$，直至达到预设的视野长度 $H$，得到轨迹 $\\tau_k$。随后，一个视觉语言模型奖励函数 $\\hat{R}$（本文使用 GPT-4o）为每条轨迹分配一个二元任务完成奖励 $r_k = \\hat{R}(\\tau_k, g)$。\n\n其核心优化算法采用分组相对策略优化。首先，计算一个组（$K$ 条轨迹）内奖励的均值 $\\mu$ 和标准差 $\\sigma$，然后通过归一化计算每条轨迹的优势值 $\\hat{A}_k = (r_k - \\mu) / (\\sigma + \\epsilon)$。该轨迹级优势值被赋予轨迹内的每个时间步。最后，使用基于 PPO 风格的裁剪目标函数 $\\mathcal{J}(\\theta)$ 来更新策略参数 $\\theta$，其中包含了概率比 $r_{t,k}(\\theta)$ 和裁剪操作。\n\n与现有方法相比，World-Gymnast 的创新点不仅在于将世界模型和 VLM 作为动力学和奖励函数融入模型基础 RL，更体现在其利用世界模型特性所支持的一系列新颖训练范式：\n1.  **从任意帧训练**：世界模型仅需单张初始帧即可展开，使得策略可以从任何接近其训练分布的图像开始进行 RL 训练，极大地扩充了有效训练数据，并有助于学习恢复行为。\n2.  **在新颖语言指令上训练**：通过 VLM 为同一初始帧生成合理的、分布外的任务指令，使策略能在这些新指令上进行 RL 训练，从而学习与环境中已有但未在演示中交互过的物体进行互动。\n3.  **使用干扰物训练**：利用图像编辑工具在输入帧中合成额外的物体作为视觉干扰，训练策略在杂乱场景中的鲁棒性。\n4.  **测试时训练**：当在测试时遇到一个新颖的真实场景帧时，可以直接以此帧为起点，在世界模型中进行快速的 RL 微调，实现策略的快速适应。\n5.  **迭代的世界模型与策略改进**：受经典 Dyna 算法启发，可以将策略在真实世界（或世界模型）中收集的新轨迹数据用于微调世界模型，再用更新后的、更准确的世界模型来优化策略，形成一个数据飞轮。\n\n## 实验与结果\n**实验设置**：评估在 Bridge 机器人平台上进行，使用 AutoEval 自动化真实机器人评估框架（支持 4 个任务）。基模型为在 BridgeData V2 上微调后的 OpenVLA-OFT。世界模型采用在 Open X-Embodiment 数据集上预训练的 600M 参数版 WorldGym。奖励模型为 GPT-4o。训练使用 4 块 H200 GPU 进行 1-2 天的全参数微调。\n\n**基线对比**：\n1.  **软件模拟器**：SIMPLER，一个为 Bridge 创建的真实到模拟框架。\n2.  **监督学习**：SFT（基模型）和 Iter-SFT（在 SFT 基础上，额外使用世界模型生成的成功合成轨迹进行迭代监督微调）。\n\n**关键实验结果**：\n\n![定性评估](https://arxiv.org/html/2602.02454v1/x2.png)\n\n> **图2**：在带有干扰物的 WorldGym 中策略展开的定性评估。在视觉干扰下，SFT 策略明显抓取了错误的物体，而两个 World-Gymnast 变体都能正确执行任务。World-Gymnast-Distract 具有更好的抓取和放置动作。\n\n表1和表2展示了真实机器人成功率对比。World-Gymnast 在“打开抽屉”（58% vs. 34%）、\"将茄子放入蓝色水槽\"（72% vs. 32%）和“将茄子放入黄色篮子”（78% vs. 40%）任务上显著优于 SIMPLER。与监督学习方法相比，World-Gymnast 在“将茄子放入蓝色水槽”（72% vs. 4%）和“将茄子放入黄色篮子”（78% vs. 8%）任务上分别取得了高达 18 倍和近 10 倍的性能提升。Iter-SFT 在困难任务上略有改进，但在简单任务上性能下降，表明 RL 通过主动探索能学到更具泛化性的行为。\n\n**多样化训练场景评估**：如表3所示，通过在训练数据中增加带有干扰物的帧（World-Gymnast-Distract）、增加新颖语言指令任务（World-Gymnast-Language）或直接增加更多训练任务（World-Gymnast-Scaled），策略在原始任务上的成功率均得到进一步提升（从 World-Gymnast 的 74% 最高提升至 81%），证明了利用世界模型进行数据扩增的有效性。\n\n**测试时优化评估**：仅使用测试帧和指令进行零样本的测试时训练，能将“关闭抽屉”任务的真实成功率从 62% 提升至 100%，但会损害其他任务性能，存在过拟合问题。\n\n**迭代改进评估**：\n\n![迭代改进对比](https://arxiv.org/html/2602.02454v1/x3.png)\n\n> **图3**：在真实机器人、软件模拟器 SIMPLER、原始 WorldGym 以及经过在线世界模型更新后的 World-Gymnast 上执行相同动作序列的定性对比。经过 Dyna 式迭代更新后的 World-Gymnast 展开更贴近真实世界。\n\n通过收集约 100 条真实机器人轨迹微调世界模型后，其生成的展开在视觉上比 SIMPLER 和原始 WorldGym 更接近真实情况，表明迭代改进能提升世界模型的质量。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 World-Gymnast 框架，首次系统性地论证了在世界模型中进行 RL 微调，能够显著超越监督微调和软件模拟器 RL，在真实机器人任务上取得更优性能。\n2.  展示了利用世界模型特性所支持的一系列创新训练范式，包括从任意帧训练、处理新颖指令和视觉干扰、测试时训练以及迭代的模型-策略改进，极大地扩展了机器人策略训练的灵活性和数据效率。\n3.  通过实验验证了这些范式能够有效提升策略的鲁棒性和泛化能力。\n\n**局限性**：论文提到测试时训练可能导致对单一任务的过拟合；此外，世界模型本身可能存在幻觉问题，这会影响迭代 SFT 等方法的性能。\n\n**启示**：本文结果表明，学习世界模型并在其中进行“云端”策略训练，可能是弥合“仅在演示中工作的机器人”与“能在任何家庭中工作的机器人”之间差距的关键。未来工作可探索如何减轻测试时训练的过拟合，以及如何更有效地利用世界模型进行持续学习和适应。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02454v1/x1.png",
        "https://arxiv.org/html/2602.02454v1/x2.png",
        "https://arxiv.org/html/2602.02454v1/x3.png",
        "https://arxiv.org/html/2602.02454v1/x4.png",
        "https://arxiv.org/html/2602.02454v1/x5.png",
        "https://arxiv.org/html/2602.02454v1/x6.png",
        "https://arxiv.org/html/2602.02454v1/x7.png",
        "https://arxiv.org/html/2602.02454v1/x8.png",
        "https://arxiv.org/html/2602.02454v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02402",
      "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
      "url": "http://arxiv.org/abs/2602.02402",
      "arxivId": "2602.02402",
      "date": "2026-02-02",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "本文提出SoMA，一个用于机器人软体操作的真实到仿真神经模拟器。核心解决现有模拟器依赖预定义物理模型或缺乏机器人条件控制，导致准确性、稳定性和泛化能力不足的问题。其关键技术是在统一潜在神经空间中，耦合可变形物体动力学、环境力与机器人关节动作，并基于学习的3D高斯泼溅进行端到端模拟。该方法无需预定义物理模型，实现了可控、稳定的长时程操作与轨迹外泛化。实验表明，SoMA在真实机器人操作任务上，将重新模拟准确性与泛化能力提升了20%，并能稳定模拟如长时程布料折叠等复杂任务。",
      "detailedSummary": "## 研究背景与动机\n机器人软体操纵（如布料折叠、柔性物体操作）的具身学习高度依赖大量交互数据，但在真实世界中采集此类数据成本高昂且风险大。真实到仿真（R2S）模拟通过将真实物体行为复现到虚拟环境中，为数据合成和策略学习提供了可扩展的解决方案。一个实用的仿真器需要在物理保真度和长期交互一致性之间取得平衡。现有方法主要沿两个方向展开：基于物理的仿真器（如FEM、MPH）能保证长期交互一致性，但依赖难以从视觉数据中推断的预定义物理模型和参数；神经网络动力学建模和4D重建方法直接从数据学习运动，但主要关注再现观测轨迹，对机器人条件交互和训练分布外泛化的支持有限。因此，两者均无法完全满足复杂机器人操纵中的R2S需求。\n\n本文针对上述痛点，提出了一种新的视角：重新思考可变形物体仿真的表示与学习方式。核心思路是提出SoMA，一个基于3D高斯泼溅（Gaussian Splatting）的软体操纵神经仿真器，将可变形物体动力学、环境力和机器人关节动作耦合在一个统一的潜在神经空间中，进行端到端的真实到仿真模拟，从而实现可控、稳定的长时域操纵仿真，且无需预定义物理模型。\n\n## 方法详解\nSoMA是一个用于软体机器人操纵的统一神经仿真器，旨在机器人关节空间控制和环境交互下建模可变形物体动力学。其整体框架接受从真实世界操纵收集的RGB观测和机器人关节状态作为输入，通过端到端训练，输出模拟出的物体状态并渲染为图像。\n\n![方法框架](https://arxiv.org/html/2602.02402v1/x2.png)\n> **图2**：SoMA整体框架。左侧输入为多视角RGB观测和机器人关节动作；中间部分将物体重建为层次化高斯泼溅，并通过神经仿真器在渲染和动力学监督下进行传播；右侧展示了物体运动由基于力的交互驱动，环境和机器人诱导的力作用于泼溅点产生形变。\n\n框架包含三个核心组件：\n1.  **通过R2S映射进行场景初始化**：为了在机器人运动学、被操纵物体和物理参考系之间建立统一的仿真空间，本模块首先利用多视角几何和3D高斯泼溅从RGB图像重建初始物体状态 `G0`。关键步骤是进行机器人条件对齐：通过强制重建几何与已知参考尺寸之间的度量一致性来恢复全局尺度因子 `s`，并估计将机器人基座标系变换到仿真空间的刚体变换 `(R, t)`。结合机器人正向运动学，可将任意时刻的末端执行器位姿 `𝐓_rob^ee(t)` 映射到仿真空间 `𝐓_sim^ee(t)`。同时，通过拟合支撑桌面法向来定义重力方向 `g`，解决了重力符号的歧义。\n2.  **力驱动的高斯泼溅动力学建模**：与现有主要基于状态的神经动力学模型不同，SoMA将物体运动建模为由接触力驱动。对于每个高斯泼溅节点或簇 `i`，其线速度和角速度由神经网络 `ψ_θ` 根据其历史状态和总作用力 `𝐟_i` 预测。总作用力 `𝐟_i = 𝐟_i^env + 𝐟_i^rob` 由两部分组成：\n    *   **环境力 `𝐟_i^env`**：包括作用于所有节点的重力 `g`，以及对靠近支撑表面的节点额外施加的支持力 `𝐬_i`。\n    *   **机器人力 `𝐟_i^rob`**：通过在高斯泼溅节点和机器人控制点之间构建交互图来建模。图神经网络 `Φ_θ` 根据泼溅节点状态、邻近机器人控制点状态和夹爪开合状态 `c_t` 来预测机器人施加的力。\n    这些力在构建的层次化图结构（由底向上聚类形成）中自底向上聚合，动力学则通过图神经网络自顶向下传播，确保全局运动一致性和局部形变。\n3.  **用于长时域学习的多分辨率训练与混合监督**：\n    *   **多分辨率训练**：为解决长时域模拟中误差累积和稳定性问题，采用从粗到细的时间训练策略。第一阶段使用较大的时间步长 `k·dt` 捕捉长程动力学；第二阶段使用原始分辨率 `dt` 并随机采样子序列进行细粒度动力学学习。同时，在图像维度上，几何重建使用高分辨率图像以保留细节，而动力学训练则使用原始分辨率以降低计算成本。\n    *   **混合监督**：针对交互中常见的遮挡问题，设计了混合监督策略。\n        *   **遮挡感知的图像监督**：仅在物体可见区域（通过二值掩码 `𝐌_t` 定义）计算图像重建损失 `ℒ_img`，避免遮挡区域引入虚假梯度。\n        *   **动量一致性正则化**：为约束无直接视觉反馈的遮挡区域运动，引入动量守恒正则化 `ℒ_mom`，强制相邻层次簇节点与子节点之间的动量守恒，作为自监督的物理一致性约束，缓解长时域漂移。\n\n与现有方法相比，SoMA的核心创新在于：1) **机器人动作直接耦合**：通过R2S映射将关节空间动作直接锚定到仿真空间，实现因果、运动学一致的交互；2) **力驱动的交互建模**：将环境和机器人交互显式建模为力，作用于高斯泼溅表示，提高了对接触和遮挡的鲁棒性；3) **稳定性保障机制**：多分辨率训练和混合监督策略专门针对长时域、遮挡严重的机器人操纵场景设计，确保了仿真的数值稳定性和物理合理性。\n\n## 实验与结果\n**实验设置**：在ARX-Lift平台上收集了包含绳子、玩偶、布料和T恤四类可变形物体的真实世界机器人操纵数据集。图像分辨率为640×480，30 FPS，同步记录机器人关节状态。每类物体收集30-40个序列，按7:3划分训练集和测试集。评估两项任务：**重仿真**（在训练轨迹上评估）和**泛化**（在未见过的测试序列上评估）。对比的基线方法包括基于可微分物理的 **PhysTwin** 和基于神经动力学的 **GausSim**。评估指标涵盖图像质量（PSNR、SSIM、LPIPS）和代理几何精度（深度图的Abs Rel、RMSE）。\n\n**关键实验结果**：\nSoMA在重仿真和泛化任务上均取得了最佳性能。如表1所示，在重仿真任务中，SoMA在各项指标上全面领先，例如在PSNR上达到33.51，优于PhysTwin的28.77和GausSim的31.69。在更具挑战性的泛化任务中，SoMA同样保持优势，PSNR为32.89，而基线方法性能均有下降。论文指出，SoMA相比基线方法实现了约20%的性能提升。\n\n![定性结果](https://arxiv.org/html/2602.02402v1/x3.png)\n> **图3**：SoMA在机器人操纵下的定性重仿真（左）与泛化（右）结果。对于绳子、布料、玩偶等物体，SoMA能产生稳定、长时域的仿真，与真实动态高度匹配。PhysTwin在复杂或未见交互下出现偏差，GausSim则在挑战性场景中经常保持静态或变得不稳定。\n\n在涉及长时域、大形变和频繁自接触的**T恤折叠**复杂任务中（表2），SoMA的优势更加明显。它能稳定模拟整个折叠过程，而PhysTwin则出现明显的伪影和不一致的形变。SoMA在PSNR上达到27.57，显著高于PhysTwin的22.85。\n\n**消融实验**（表3）总结了各组件贡献：\n*   **完整模型**：取得最佳综合性能。\n*   **联合训练（Jointly）**：在所有物体域上联合训练，略微降低了重仿真精度，但提升了泛化性能，表明其具有提升泛化能力的潜力。\n*   **仅图像监督（Img-only）**：移除混合监督损失（动量正则化），性能下降，证明了物理一致性约束对稳定学习的重要性。\n*   **无多分辨率训练（w/o MRT）**：禁用多分辨率训练策略，性能显著降低，突显了该策略对于高效、稳定学习长时域动力学的关键作用。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了一种新的R2S神经仿真范式**：首次将可变形物体、机器人动作和环境统一在基于高斯泼溅的仿真空间中，支持长时域、交互一致的软体操纵模拟。\n2.  **设计了一系列使能该仿真的关键机制**：包括机器人条件对齐、力驱动的高斯泼溅动力学模型，以及结合多分辨率训练和混合监督的稳定化策略。\n3.  **在真实机器人操纵数据集上进行了全面验证**：在重仿真、泛化及复杂任务（T恤折叠）上均显著优于现有基于物理和神经网络的仿真器，展示了其实际应用潜力。\n\n论文自身提到的局限性包括：仿真依赖于从视觉观测进行的初始重建，重建质量会影响后续模拟；方法在计算成本上可能较高。\n\n本文对后续研究的启示在于：为机器人软体操纵的仿真提供了一条融合数据驱动与物理直觉的新路径。其力驱动的交互建模思想、针对遮挡和长时域稳定性设计的训练策略，可启发更鲁棒、更通用的具身智能仿真器的开发。未来工作可探索如何进一步降低对高质量初始重建的依赖，以及将此类仿真器用于闭环策略学习与优化。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02402v1/x1.png",
        "https://arxiv.org/html/2602.02402v1/x2.png",
        "https://arxiv.org/html/2602.02402v1/x3.png",
        "https://arxiv.org/html/2602.02402v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02396",
      "title": "PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning",
      "url": "http://arxiv.org/abs/2602.02396",
      "arxivId": "2602.02396",
      "date": "2026-02-02",
      "authors": "Alexander Schperberg Team",
      "category": "Manipulation",
      "summary": "论文PRISM解决了机器人模仿学习中现有方法难以同时满足实时控制速率、多模态传感输入（如RGB、深度、触觉）和动作多模态分布的挑战。它提出基于Performer RS-IMLE的单次通过策略，结合多传感器时序编码器与线性注意力生成器，采用批全局拒绝采样IMLE目标进行训练。实验表明，在真实硬件任务中PRISM比扩散策略成功率提高10-25%，在CALVIN基准上成功率提升约25%，轨迹急动度减少20-50倍，同时保持30-50Hz闭环控制。",
      "detailedSummary": "## 研究背景与动机\n机器人模仿学习（IL）旨在直接从示教数据中学习复杂的视觉运动策略。理想的模仿策略应满足三个关键标准：实时推理速率以进行闭环物理控制；表征专家行为的多模态分布；以及在部分感官输入下通过有效融合多传感流实现鲁棒性能。当前主流生成方法往往只能满足部分要求：扩散模型能捕获复杂的多模态动作分布，但其依赖迭代去噪（通常每推理10-100步）严重限制了实时部署；基于流的方法通过连续时间积分减少了采样步骤，但可能在多模态保真度上存在不足。这些方法在推理时由于迭代采样而计算昂贵。隐式最大似然估计（IMLE）方法通过最小化每个专家数据点到其最近生成样本的距离来确保专家分布的完全覆盖，并支持单次推理，但其在扩展到条件策略时面临挑战。先前方法通常对每个样本进行拒绝采样，这违反了IMLE的批全局覆盖原则，可能导致策略回归到平均行为而无法表示多模态设置中的不同模式。此外，闭环机器人任务要求动作序列具有强时序平滑性，现有的局部平滑启发式方法在避免模式切换方面较为脆弱。\n\n本文针对现有生成方法在实时性、多模态覆盖和时序平滑性之间难以兼得的痛点，提出了PRISM（Performer RS-IMLE for Single-pass Multisensory Imitation）。其核心思路是：通过一个基于Performer架构的、单次前向传递的线性注意力生成器，配合一个批全局的RS-IMLE训练目标，在无需迭代采样的前提下，生成覆盖多模态且时序平滑的动作序列，从而实现高效、准确的实时多感官模仿学习。\n\n## 方法详解\nPRISM的整体框架将问题解耦为三个阶段：a) 时间多感官编码器，融合异构输入并保留时间维度；b) 单次传递生成器，使用双向线性注意力（FAVOR+）并行产生完整动作序列；c) 批全局RS-IMLE训练目标，为模式覆盖提供理论依据且无迭代扩散的推理成本。\n\n![方法框架](https://arxiv.org/html/2602.02396v1/x1.png)\n> **图1**：PRISM方法整体概览。可用的传感器特征按时间步融合为时间上下文令牌。使用双向FAVOR+（Performer）生成器与可学习的查询令牌，单次输出完整的动作序列。训练使用批全局RS-IMLE目标（鲁棒的Charbonnier距离、带EMA校准的ε-拒绝、可选的小覆盖项），以在不进行迭代采样的情况下保持动作多模态性。\n\n**时间多感官编码器**：对于上下文时间范围 \\(T_o\\) 内的每个时间步 \\(t\\)，将每种模态（手腕RGB、静态RGB、深度、触觉、本体感觉、音频、文本）的嵌入向量拼接，并通过一个MLP融合为固定维度 \\(d\\) 的上下文令牌 \\(\\mathbf{c}_t\\)。然后将所有时间步的 \\(\\mathbf{c}_t\\) 堆叠，并添加绝对位置嵌入，得到上下文令牌 \\(\\mathbf{C} \\in \\mathbb{R}^{T_o \\times d}\\)。这种按时间步融合的方式避免了时间统计量的纠缠，将长程一致性任务委托给生成器。\n\n**单次传递线性注意力生成器（双向）**：给定上下文 \\(\\mathbf{C}\\)，生成器通过一次前向传递产生长度为 \\(T_p\\) 的动作序列。它初始化 \\(T_p\\) 个可学习的查询令牌 \\(\\mathbf{Q} \\in \\mathbb{R}^{T_p \\times d}\\)，并加入一个投影的潜变量 \\(z \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\)（作为产生不同轨迹候选的随机种子）以及位置编码。随后，应用 \\(L\\) 个Transformer块，每个块包含：i) 对 \\(\\mathbf{Q}\\) 的双向自注意力（无因果掩码），以及 ii) 对完整上下文 \\(\\mathbf{C}\\) 的交叉注意力（也无因果掩码）。两种注意力均使用FAVOR+（线性化softmax），将计算成本从 \\(\\mathcal{O}(T^2)\\) 降低到 \\(\\mathcal{O}(T m)\\)（\\(m\\) 为随机特征数）。一个线性头将最终的令牌映射为动作 \\(\\hat{\\mathbf{A}} \\in \\mathbb{R}^{T_p \\times D_a}\\)。这种非自回归、双向的设计允许联合选择整个序列。\n\n**鲁棒序列距离**：使用带Charbonnier惩罚 \\(\\varepsilon_c\\) 和按维度权重 \\(w_d\\) 的度量来比较预测序列和目标序列：\\(D_{\\rho}(\\hat{\\mathbf{A}},\\mathbf{A}) = \\frac{1}{T_p} \\sum_{t=1}^{T_p} \\sum_{d=1}^{D_a} w_d \\sqrt{(\\hat{a}_{t,d} - a_{t,d})^2 + \\varepsilon_c^2}\\)。该度量对异常值鲁棒且可微，用于训练和评估时的候选选择。\n\n**批全局RS-IMLE目标**：对于批次 \\(B\\) 中的每个样本 \\(i\\)，抽取 \\(K\\) 个潜变量得到候选序列 \\(\\{\\hat{\\mathbf{A}}^{(k)}_i\\}_{k=1}^K\\)。计算每个候选与自身目标之间的距离 \\(D_{i,k}\\)，以及与批次内所有目标 \\(j\\) 之间的批全局距离 \\(D_{i,k \\rightarrow j}\\)。通过一个由指数移动平均（EMA）校准的阈值 \\(\\varepsilon_{\\text{RS}}\\) 进行拒绝采样：拒绝那些与任何目标过于接近（即 \\(\\min_j D_{i,k \\rightarrow j} < \\varepsilon_{\\text{RS}}\\)）的候选。训练损失由硬IMLE损失和可选的软覆盖项组成：\n- 硬IMLE损失：\\(\\mathcal{L}_{\\text{hard}} = \\frac{1}{B} \\sum_{i=1}^{B} \\min_{k \\in \\mathcal{K}_i} D_{i,k}\\)，其中 \\(\\mathcal{K}_i\\) 是未被拒绝的候选索引集（若全部被拒绝则使用所有候选）。\n- 软覆盖项：\\(\\mathcal{L}_{\\text{soft}} = -\\frac{1}{B} \\sum_{i=1}^{B} \\log \\sum_{k \\in \\text{Top}K'(D_{i,\\cdot})} \\exp(-D_{i,k}/\\tau)\\)，对前 \\(K'\\) 个候选施加一个温和的熵正则化，防止模式坍塌。\n总损失为 \\(\\mathcal{L} = \\mathcal{L}_{\\text{hard}} + \\lambda_{\\text{soft}} \\mathcal{L}_{\\text{soft}}\\)，其中 \\(\\lambda_{\\text{soft}} \\ll 1\\)。批全局拒绝防止单个候选“覆盖”批次中多个相近的目标，从而保留了替代模式的梯度信号。\n\n**滑动窗口推理（单次传递）**：在测试时，模型观察当前上下文，编码后抽取 \\(K\\) 个潜变量，单次前向传递生成 \\(K\\) 条动作序列。然后根据仅使用观测的规则选择一条轨迹：1) **代理评分**（当本体感觉或手腕姿态可观测时）：选择其第一动作诱导的预测末端执行器姿态最接近观测姿态的候选。2) **确定性平局打破**（当代理评分不可用时）：选择其第一动作最接近最后执行动作（L2距离）的候选。执行选中轨迹的前 \\(T_a\\) 个动作，然后滑动观测窗口并重新规划。这种连续重规划机制在不增加推理延迟的前提下保证了时序平滑性。\n\n与现有方法相比，PRISM的创新点具体体现在：1) **批全局拒绝采样**：将RS-IMLE从样本级扩展到批全局，有效避免了模式平均并保持了多模态覆盖。2) **单次非自回归生成**：基于双向线性注意力的生成器一次性输出完整动作序列，完全避免了迭代采样步骤，实现了实时推理。3) **时序感知的多感官融合与平滑性保证**：通过按时间步融合编码器和滑动窗口推理中的轨迹选择策略，共同确保了生成动作的时序连贯性。\n\n## 实验与结果\n**实验设置**：PRISM在多样化的真实机器人套件和大型仿真基准上进行了验证。\n- **真实硬件**：包括用于移动操作（loco-manipulation）的Unitree Go2（配备7自由度机械臂D1）和用于桌面精确操作的UR5机械臂。任务包括操作前泊车、高精度插入和多物体取放。\n- **仿真基准**：包括CALVIN（10%数据划分）、MetaWorld和Robomimic。\n- **对比基线**：包括扩散策略（Diffusion Policy）、流匹配（Flow Matching）以及IMLE策略等最先进的方法。\n\n**关键实验结果**：\n- **真实机器人性能**：在挑战性物理任务上，PRISM比最先进的扩散策略的成功率高出10–25%，同时保持高频（30–50 Hz）闭环控制。\n- **仿真基准性能**：在CALVIN（10%数据）上，PRISM比扩散策略的成功率提高约25%，比流匹配提高约20%，同时将轨迹加加速度（jerk）降低了20–50倍，这对硬件寿命和安全至关重要。在MetaWorld和Robomimic上，PRISM也达到或超过了SOTA策略的性能。\n\n![基准数据集与模态](https://arxiv.org/html/2602.02396v1/x2.png)\n> **图2**：各基准数据集使用的传感器模态。R代表RGB相机，D代表深度相机，Tact代表触觉，P代表本体感觉，Text代表文本令牌，A代表音频。\n\n![真实硬件任务成功率](https://arxiv.org/html/2602.02396v1/x3.png)\n> **图3**：真实硬件任务的成功率。PRISM在UR5的插入和取放任务，以及Unitree Go2的泊车和取放任务上，均显著优于扩散策略（Diffusion Policy）和流匹配（Flow Matching）。\n\n![CALVIN基准成功率和加加速度](https://arxiv.org/html/2602.02396v1/x4.png)\n> **图4**：在CALVIN基准（10%数据）上的成功率和轨迹加加速度。左图显示PRISM的成功率显著高于扩散和流匹配；右图显示PRISM产生的轨迹加加速度（衡量运动平滑性的指标）比扩散策略低20-50倍。\n\n![MetaWorld成功率](https://arxiv.org/html/2602.02396v1/x5.png)\n> **图5**：在MetaWorld基准上的成功率。PRISM在多个任务上表现与扩散策略相当或更优。\n\n![消融实验：组件贡献](https://arxiv.org/html/2602.02396v1/x6.png)\n> **图6**：消融实验研究各组件贡献。(a) 移除软覆盖项（`w/o soft`）、使用标准注意力（`w/o linear`）或使用因果注意力（`w/o bidir`）都会导致性能下降。(b) 候选数 \\(K\\) 的影响，性能在 \\(K \\approx 16\\) 时饱和。(c) 批全局拒绝采样（`Global RS`）相比每样本拒绝采样（`Per-sample RS`）能带来显著性能提升。\n\n![消融实验：模态重要性](https://arxiv.org/html/2602.02396v1/x7.png)\n> **图7**：传感器模态重要性分析。手腕RGB和本体感觉是关键模态，而深度信息在某些任务中可能是冗余的。模态丢弃研究表明PRISM在传感器缺失时具有优雅的性能降级能力。\n\n![Robomimic上的成功率](https://arxiv.org/html/2602.02396v1/x8.png)\n> **图8**：在Robomimic基准上的成功率。PRISM在多个任务上优于或与扩散策略相当。\n\n![推理延迟比较](https://arxiv.org/html/2602.02396v1/x9.png)\n> **图9**：不同策略的推理延迟（毫秒）。PRISM的单次传递推理速度显著快于需要多步采样的扩散策略和流匹配。\n\n![真实机器人轨迹可视化](https://arxiv.org/html/2602.02396v1/fig/pushT.png)\n> **图10**：真实机器人任务（UR5插入）的轨迹可视化。展示了预测的多个候选轨迹（彩色线）和最终执行的平滑轨迹。\n\n![ε_RS阈值校准过程](https://arxiv.org/html/2602.02396v1/x10.png)\n> **图11**：批全局拒绝采样阈值 \\(\\varepsilon_{\\text{RS}}\\) 的EMA校准过程。该估计器方差低（\\(O(1/N)\\)），确保了训练稳定性。\n\n![不同距离度量的影响](https://arxiv.org/html/2602.02396v1/x11.png)\n> **图12**：不同序列距离度量对性能的影响。鲁棒的Charbonnier距离优于标准的L2或L1距离。\n\n![注意力机制消融](https://arxiv.org/html/2602.02396v1/x12.png)\n> **图13**：注意力机制消融。双向线性注意力（FAVOR+）在性能和效率上取得最佳平衡。\n\n![预测视野的影响](https://arxiv.org/html/2602.02396v1/x13.png)\n> **图14**：预测视野 \\(T_p\\) 对成功率的影响。存在一个最佳范围，视野过长或过短都会损害性能。\n\n![语言引导任务示例](https://arxiv.org/html/2602.02396v1/fig/language_guided.png)\n> **图15**：语言引导任务示例。展示了PRISM在处理多模态输入（包括语言指令）时的能力。\n\n![批全局校准有效性](https://arxiv.org/html/2602.02396v1/fig/batch_calibration.png)\n> **图16**：批全局校准有效性可视化。说明了批全局拒绝如何防止候选聚集在单个模式周围，从而促进多模态覆盖。\n\n**消融实验总结**：消融实验明确了各核心组件的贡献：1) **批全局RS-IMLE目标**是提升多模态覆盖和性能的关键。2) **软覆盖项**对稳定训练和防止优化偏差有积极作用。3) **双向线性注意力（FAVOR+）** 在保证效率的同时实现了有效的时序建模。4) **手腕RGB和本体感觉**是最关键的传感器模态。5) 候选数 \\(K\\) 在约16时达到性能饱和。6) 鲁棒的Charbonnier距离度量优于传统L2距离。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了**PRISM框架**，一个基于Performer RS-IMLE的单次传递多感官模仿策略，首次在单次前向传递中同时实现了实时推理、多模态动作覆盖和时序平滑性。\n2. 引入了**批全局RS-IMLE训练目标**，通过EMA校准的拒绝采样和软覆盖项，理论严谨且实践有效地解决了条件IMLE中的模式平均问题，无需增加测试时采样成本。\n3. 设计了一个**时间感知的多感官融合编码器**和一种**滑动窗口推理策略**，能够优雅地处理异构、可能缺失的传感器输入，并生成硬件安全所需的平滑轨迹。\n\n**局限性**：论文自身提到的局限性包括：1) 方法依赖于高质量、时间对齐的多感官演示数据。2) 尽管单次传递效率高，但训练时生成多个候选并进行批全局距离计算可能带来额外的计算开销。3) 滑动窗口推理中的轨迹选择规则（代理评分）依赖于特定的可观测状态（如末端执行器姿态）。\n\n**对后续研究的启示**：\n1. **单次生成方法的潜力**：PRISM证明了无需迭代采样也能实现高质量的多模态策略生成，为实时机器人控制开辟了新路径，未来可探索更高效的生成器架构。\n2. **多传感器融合与鲁棒性**：对传感器模态重要性的分析强调了选择关键传感器和设计稳健融合方案的重要性，特别是在真实世界部分可观测的场景下。\n3. **运动平滑性与硬件安全**：显著降低轨迹加加速度不仅关乎性能，更是硬件安全和寿命的关键，未来的模仿学习工作应更加重视动作序列的物理可行性和平滑性。\n4. **理论指导的实践**：将IMLE理论与高效的注意力机制、精心设计的损失函数相结合，展示了理论洞察对解决实际机器人学习问题的重要价值。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02396v1/x1.png",
        "https://arxiv.org/html/2602.02396v1/x2.png",
        "https://arxiv.org/html/2602.02396v1/x3.png",
        "https://arxiv.org/html/2602.02396v1/x4.png",
        "https://arxiv.org/html/2602.02396v1/x5.png",
        "https://arxiv.org/html/2602.02396v1/x6.png",
        "https://arxiv.org/html/2602.02396v1/x7.png",
        "https://arxiv.org/html/2602.02396v1/x8.png",
        "https://arxiv.org/html/2602.02396v1/x9.png",
        "https://arxiv.org/html/2602.02396v1/fig/pushT.png",
        "https://arxiv.org/html/2602.02396v1/x10.png",
        "https://arxiv.org/html/2602.02396v1/x11.png",
        "https://arxiv.org/html/2602.02396v1/x12.png",
        "https://arxiv.org/html/2602.02396v1/x13.png",
        "https://arxiv.org/html/2602.02396v1/fig/language_guided.png",
        "https://arxiv.org/html/2602.02396v1/fig/batch_calibration.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01939",
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "http://arxiv.org/abs/2602.01939",
      "arxivId": "2602.01939",
      "date": "2026-02-02",
      "authors": "Qiang Nie Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中因视觉遮挡导致的信息不足问题，提出了“探索性与聚焦性操作”（EFM）这一新问题。为此，研究者建立了包含10个任务的EFM-10基准，并提出了“双臂主动感知”（BAP）策略：利用一只手臂提供主动视觉，另一只手臂在操作时提供力感知。基于该策略收集了BAPData数据集，并通过模仿学习验证了BAP策略的有效性。",
      "detailedSummary": "## 研究背景与动机\n当前人形机器人操作研究的一个趋势是将主摄像头安装在机器人头部，以获得基地不变的灵活性，但这导致视觉遮挡问题更加频繁。近期研究（如AV-ALOHA、ViA）通过采用高自由度（6/7-DoF）主动脖子来提供主动视觉以应对此问题。然而，本文指出，视觉遮挡问题的本质是**完成任务所需信息的缺失**。许多现有的人形机器人并不具备这种高自由度的主动脖子，但它们通常拥有两条手臂。\n\n基于此，本文提出了一个更根本、更广泛的新问题：**探索性与专注性操作**。该问题的核心是**主动寻求信息**，以完成那些需要探索（如寻找隐藏属性）或需要专注（如执行精细操作）的挑战性操作任务。本文的核心思路是：利用双手机器人中**非操作臂**（如果可用）提供眼在手机器视觉，同时利用**操作臂**在接触时提供力觉感知，从而构成一种无需高自由度主动脖子的双手机器人主动感知策略。\n\n## 方法详解\n本文提出的双手机器人主动感知策略旨在解决EFM问题。其整体框架是数据驱动的模仿学习。策略的输入包括：头部固定摄像头的视图（主视图）、左右手腕摄像头提供的主动视图、机器人状态（左右末端执行器位姿和夹爪状态），以及（可选的）操作臂的六维力/力矩传感器数据。输出是在笛卡尔空间中的动作序列。\n\n![硬件系统总览](https://arxiv.org/html/2602.01939v1/x2.png)\n> **图2**：硬件系统与任务物体概览。展示了用于数据收集的双手机器人JAKA K-1、头部Orbbec相机、手腕Logitech相机以及VR遥操作设备Pico Ultra 4。\n\nBAP策略包含两个核心模块：\n1.  **眼在手机器视觉**：由非操作臂（空闲臂）提供。在需要探索或专注时，操作员（或策略）主动控制该臂，使其手腕摄像头捕获**操作区域以及操作末端执行器**的清晰视图。论文通过实验强调，在操作手持物体进行精细操作时，主动视图中同时捕获操作区域和末端执行器至关重要，仅捕获手持物体无法提供末端执行器应如何调整位姿的直接线索。\n2.  **力觉感知**：由操作臂内置的力/力矩传感器提供。该信息有助于处理涉及精细接触的操作任务，为实现基于神经网络的力顺应控制提供可能。\n\n该策略的主要创新点在于：**为不具备高自由度主动脖子的现有人形机器人提供了一种实现主动感知的实用方案**。它充分利用了双手机器人并非总需同时操作的特点，将一条臂转换为“感知臂”。该策略与基于脖子的主动视觉完全兼容，未来可结合使用以最大化所有可用摄像头的效用。\n\n## 实验与结果\n本文构建了**EFM-10**基准，包含4大类共10个任务：语义探索类（Toy-Find, Toy-Match）、涉及视觉遮挡的探索类（Cup-Hang, Cup-Place, Box-Push）、需要专注的精细操作类（Light-Plug, Bread-Brush, Nail-Knock）以及兼具探索与专注的复杂类（Cable-Match, Charger-Plug）。基于BAP策略，在真实双手机器人上收集了包含1810条专家轨迹的**BAPData**数据集。\n\n实验平台为JAKA K-1双手机器人，数据频率为10Hz。对比的基线策略包括：ACT、DP、GR-MG和Pi-0。所有策略均在BAPData上以模仿学习方式训练（除非特别说明，未使用力觉数据）。\n\n首先，实验验证了主动视图中捕获内容的重要性。在Toy-Match等4个任务上比较了三种视觉上下文设置的成功率。\n\n![主动视觉设置对比结果](https://arxiv.org/html/2602.01939v1/x1.png)\n> **表3**：不同主动视觉设置下的任务成功率对比。结果表明，主动视图需同时捕获操作区域和末端执行器才能获得最佳性能。\n\n其次，在EFM-10基准上全面评估了各代表性策略的性能。\n\n![策略在EFM-10上的评估结果](https://arxiv.org/html/2602.01939v1/x1.png)\n> **表4**：各策略在EFM-10各任务上的成功率。使用BAPData训练（标有⋆）的策略性能显著提升。单任务策略（ACT, DP）无法处理语言驱动的语义探索任务；多任务策略中，Pi-0在指令跟随和非精细任务上表现更强，而所有策略在极精细操作任务（如Light-Plug）上表现均不佳。\n\n第三，实验探究了融合力觉感知的效果。通过修改GR-MG策略，使其额外输入当前力/力矩并预测未来力/力矩块。\n\n![融合力感知的GR-MG策略示意图](https://arxiv.org/html/2602.01939v1/x3.png)\n> **图3**：将力感知融入GR-MG策略的方法示意图。在输入中加入力/力矩数据，并训练模型额外预测未来的力/力矩。\n\n在Light-Plug和Bread-Brush任务上的实验表明，加入力感知后，成功率分别提升了16.7%和13.3%，同时操作末端执行器的最大垂直力平均值显著降低了29%和22%，表明神经网络实现了某种形式的力顺应控制。\n\n![力感知策略的定性分析](https://arxiv.org/html/2602.01939v1/x4.png)\n> **图4**：融合力感知的GR-MG策略在Light-Plug任务中的一次运行可视化。模型能够预测接触力的变化并相应地控制末端执行器，避免垂直力的突然增加。\n\n最后，论文对失败案例进行了定性分析。\n\n![典型失败案例可视化](https://arxiv.org/html/2602.01939v1/x5.png)\n> **图5**：模仿学习策略的典型失败案例。主要包括：语义条件不准确（拿错颜色）、空间感知/推理能力不足导致细微定位错误、以及未能找到最优主动视角以避免遮挡。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了**探索性与专注性操作**这一新问题，并构建了包含10个任务的**EFM-10**基准；2) 提出了适用于现有机型的**双手机器人主动感知**策略，并基于此收集了兼具高自由度主动视觉和力觉信息的大规模数据集**BAPData**；3) 通过实验验证了BAP策略的有效性，系统评估了代表性策略在EFM任务上的优缺点，并揭示了同时捕获末端执行器、融合力感知等技术细节的重要性。\n\n论文提到的局限性包括：BAPData数据集是在特定机器人硬件上收集的，其泛化性有待进一步验证。\n\n本文对后续研究的启示在于：为了更好解决EFM问题，未来的策略模型需要着重提升以下几方面能力：**语义条件化**（准确理解并执行语义指令）、**空间感知与推理**（提升对精细操作的空间判断）、以及**最优主动视点搜索**（动态选择最佳观测视角）。同时，将BAP策略与基于脖子的主动视觉相结合，也是一个有前景的未来方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01939v1/x1.png",
        "https://arxiv.org/html/2602.01939v1/x2.png",
        "https://arxiv.org/html/2602.01939v1/x3.png",
        "https://arxiv.org/html/2602.01939v1/x4.png",
        "https://arxiv.org/html/2602.01939v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-12T12:32:33.907Z"
    }
  ]
}