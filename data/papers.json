{
  "generatedAt": "2026-02-11T13:16:00.621Z",
  "source": "https://jiangranlv.github.io/robotics_arXiv_daily/",
  "items": [
    {
      "id": "http://arxiv.org/abs/2602.09023",
      "title": "TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.09023",
      "arxivId": "2602.09023",
      "date": "2026-02-09",
      "authors": "Shanghang Zhang Team",
      "category": "Manipulation",
      "summary": "论文提出了一种名为TwinRL-VLA的数字孪生驱动强化学习框架，旨在解决现实世界中机器人操作任务中由于高昂的专家演示成本和不足的实际交互导致的Vision-Language-Action (VLA)模型泛化能力受限的问题。该方法通过智能手机捕捉场景高效重建高保真度的数字孪生环境，实现真实与模拟环境之间的双向传输。在监督微调（SFT）预热阶段，利用数字孪生扩展探索空间，增强数据轨迹分布的支持。基于此初始化，进一步提出了从模拟到现实的引导探索策略，显著提升了VLA模型在实际操作中的性能。实验结果表明，TwinRL-VLA有效提高了在线强化学习的探索效率和探索空间。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型在机器人操作任务中取得了显著进展，但这些模型仍然依赖于昂贵的专家演示和有限的真实世界交互。尽管在线强化学习（RL）在改进基础模型方面显示出潜力，但在真实世界的机器人操作中应用RL仍受到低探索效率和受限探索空间的限制。本文针对这一痛点，提出了TwinRL-VLA框架，通过数字孪生技术来扩展和引导VLA模型的探索空间。本文的核心思路是利用高保真度的数字孪生环境进行高效的并行在线RL，并通过数字孪生指导真实机器人的目标性探索。\n\n## 方法详解\n### 整体框架\nTwinRL-VLA的整体框架分为两个主要阶段：SFT预热阶段和在线RL阶段。在SFT预热阶段，通过数字孪生生成多样化的合成轨迹以扩展探索空间。在在线RL阶段，首先在数字孪生环境中进行高效的并行RL滚动，然后将这些数据用于真实世界的在线RL，从而加速训练过程。\n\n![方法框架](https://arxiv.org/html/2602.09023v1/x1.png)\n> **图1**：方法整体框架。左侧为SFT预热阶段，右侧为在线RL阶段。数字孪生环境用于生成多样化的合成轨迹，并在在线RL阶段提供高效的并行滚动。\n\n### 核心模块\n1. **数字孪生重建**：使用智能手机捕捉的场景视频，通过标准的3D Gaussian Splatting技术重建高保真的数字孪生环境。\n2. **探索空间扩展策略**：在SFT预热阶段，通过数字孪生生成多样化的合成轨迹，以扩展探索覆盖范围。\n3. **Sim-to-Real引导探索策略**：在数字孪生环境中进行高效的并行RL滚动，生成在线交互数据存储在重放缓冲区中。这些数据用于真实世界的在线RL，丰富了重放缓冲区，并减少了从离线到在线学习的过渡性能下降。\n\n### 创新点\n- 通过数字孪生技术生成多样化的合成轨迹，扩展了探索空间。\n- 在数字孪生环境中进行高效的并行在线RL，加速了真实世界的在线RL过程。\n- 通过数字孪生识别易失败但信息丰富的配置，指导真实机器人的目标性探索。\n\n## 实验与结果\n### 实验设置\n- **Benchmark/数据集**：四个不同的机器人操作任务，包括抓取、放置等。\n- **Baseline方法**：仅使用真实世界演示的SFT方法、现有的在线RL方法（如[8, 39]）。\n\n### 关键实验结果\n- **SFT预热阶段**：TwinRL-VLA在SFT预热阶段的平均成功率提高了42%。\n- **在线RL阶段**：TwinRL-VLA在分布内和分布外区域均达到了100%的成功率，相比现有方法至少提升了30%的速度，平均每个任务只需要约20分钟。\n\n![SFT预热阶段成功率对比](https://arxiv.org/html/2602.09023v1/x2.png)\n> **图2**：SFT预热阶段的成功率对比。TwinRL-VLA在SFT预热阶段的平均成功率提高了42%。\n\n![在线RL阶段成功率对比](https://arxiv.org/html/2602.09023v1/x3.png)\n> **图3**：在线RL阶段的成功率对比。TwinRL-VLA在分布内和分布外区域均达到了100%的成功率。\n\n### 消融实验\n- **数字孪生的作用**：去除数字孪生后，SFT预热阶段的成功率降低了30%，在线RL阶段的成功率降低了20%。\n- **Sim-to-Real引导探索策略**：去除该策略后，在线RL阶段的成功率降低了15%。\n\n## 总结与启发\n### 核心贡献\n1. 提出了TwinRL-VLA框架，通过数字孪生技术扩展和引导VLA模型的探索空间。\n2. 引入了探索空间扩展策略，通过数字孪生生成多样化的合成轨迹，扩展了探索覆盖范围。\n3. 提出了Sim-to-Real引导探索策略，通过数字孪生中的高效并行在线RL加速真实世界的在线RL过程。\n\n### 局限性\n- 数字孪生的重建需要高质量的输入数据，这可能在某些复杂环境中难以实现。\n- Sim-to-Real引导探索策略对数字孪生的准确性有一定依赖，如果数字孪生与真实环境存在较大差异，可能会导致性能下降。\n\n### 后续研究启示\n- 进一步优化数字孪生的重建过程，提高其在复杂环境中的适用性。\n- 探索更多有效的Sim-to-Real引导探索策略，进一步提升在线RL的效率和鲁棒性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09023v1/x1.png",
        "https://arxiv.org/html/2602.09023v1/x2.png",
        "https://arxiv.org/html/2602.09023v1/x3.png",
        "https://arxiv.org/html/2602.09023v1/x4.png",
        "https://arxiv.org/html/2602.09023v1/x5.png",
        "https://arxiv.org/html/2602.09023v1/x6.png",
        "https://arxiv.org/html/2602.09023v1/x7.png",
        "https://arxiv.org/html/2602.09023v1/x8.png",
        "https://arxiv.org/html/2602.09023v1/x9.png",
        "https://arxiv.org/html/2602.09023v1/x10.png",
        "https://arxiv.org/html/2602.09023v1/x11.png",
        "https://arxiv.org/html/2602.09023v1/x12.png",
        "https://arxiv.org/html/2602.09023v1/x13.png",
        "https://arxiv.org/html/2602.09023v1/x14.png",
        "https://arxiv.org/html/2602.09023v1/x15.png",
        "https://arxiv.org/html/2602.09023v1/x16.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09021",
      "title": "$χ_{0}$ : Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
      "url": "http://arxiv.org/abs/2602.09021",
      "arxivId": "2602.09021",
      "date": "2026-02-09",
      "authors": "Yibo Yuan Team",
      "category": "Manipulation",
      "summary": "本文针对高可靠性长时机器人操作中的分布不一致性问题，提出了一种资源高效的框架χ₀。该框架通过三个关键技术解决这一问题：（i）模型算术，一种权重空间合并策略，有效吸收不同演示的多样化分布；（ii）阶段优势，一种阶段感知的优势估计器，提供稳定、密集的进度信号；（iii）训练-部署对齐，通过时空增强、启发式DAgger校正和时间块平滑来弥合分布差距。实验表明，该方法使双臂机器人能够协同完成从展平、折叠到挂起不同衣物的长时任务，并表现出高可靠性的自主操作能力。",
      "detailedSummary": "## 研究背景与动机\n目前，高可靠性长时序机器人操作主要依赖大规模数据和计算资源来理解复杂的现实世界动态。然而，本文指出，阻碍现实世界鲁棒性的主要瓶颈不仅仅是资源规模，而是人类演示分布、策略学习的归纳偏差以及测试时执行分布之间的系统性不一致。这种不一致性会导致多阶段任务中的累积误差。针对这一痛点，本文提出了χ₀框架，通过有效模块实现生产级的机器人操作鲁棒性。\n\n## 方法详解\n### 整体框架\nχ₀框架旨在解决三个阶段的分布不一致性：训练分布（P_train）、模型分布（Q_model）和测试分布（P_test）。整体框架如图1所示。\n![方法框架](https://arxiv.org/html/2602.09021v1/x1.png)\n> **图1**：χ₀框架的整体结构。左侧为P_train，通过启发式DAgger和时空增强扩展训练覆盖范围，并进行阶段标注以估计优势；中间为Q_model，通过模型算术在权重空间中合并互补策略，由阶段感知的优势引导；右侧为P_test，通过时间块平滑确保执行准确性，同时通过在线DAgger实现闭环优化。\n\n### 核心模块\n#### 模型算术 (Model Arithmetic, MA)\nMA通过在权重空间中合并不同P_train分布训练的检查点，使策略能够高效吸收多样化的P_train分布。具体来说，MA通过合并多个模型的权重，使得Q_model能够捕捉到之前被忽略的P_train模式。实验表明，验证损失可以作为有效的权重选择启发式方法。\n\n#### 阶段优势 (Stage Advantage, SA)\nSA将长时序任务分解为语义子目标（阶段），提供稳定的阶段感知奖励信号，用于优势加权行为克隆。SA通过帧级奖励建模，解决了先前非阶段方法（如π₀.₆*）的数值不稳定问题。具体来说，SA通过直接从配对观察中预测优势，并将其条件化在语义阶段上，从而生成更平滑和稳定的监督信号。\n\n#### 训练-部署对齐 (Train-Deploy Alignment, TDA)\nTDA通过启发式DAgger和时空增强扩展P_train，使其更接近P_test，从而提高对实际分布漂移的鲁棒性。此外，TDA还提出了一种时间块平滑方法，以减轻推理-执行延迟并增强实时控制稳定性。实验表明，时空增强只有在与控制优化结合时才有效，而时间块平滑则与RTC方法正交。\n\n### 创新点\n- 通过模型算术在权重空间中合并不同P_train分布训练的检查点，提高了策略的泛化能力。\n- 通过阶段优势提供了稳定的阶段感知奖励信号，克服了数值不稳定问题。\n- 通过训练-部署对齐扩展了P_train，使其更接近P_test，提高了对实际分布漂移的鲁棒性。\n\n## 实验与结果\n### 数据集与实验平台\n本文在协作长时序衣物操作任务上进行了评估，包括展平、折叠和挂起不同衣物。这些任务涉及接触丰富的可变形动力学，放大了上述分布漂移。\n\n### 对比方法\n本文与开源基线π₀.₅进行了对比。\n\n### 关键实验结果\n- 在仅使用20小时的演示数据和8个A100 GPU的情况下，χ₀的成功率比π₀.₅提高了近250%。\n- MA提供了资源高效的机制，提升了几乎所有指标的性能。\n- TDA中的DAgger数据对于最大化成功率至关重要，尽管会增加重试成本。\n- SA的两步优势信号在数值稳定性方面优于π₀.₆*风格的优势训练，这在实验中转化为更好的整体性能。\n\n### 实验结果图表\n#### 成功率对比\n![成功率对比](https://arxiv.org/html/2602.09021v1/x3.png)\n> **图3**：χ₀与π₀.₅在成功率上的对比。χ₀在所有任务上的成功率显著高于π₀.₅。\n\n#### 重试成本对比\n![重试成本对比](https://arxiv.org/html/2602.09021v1/x4.png)\n> **图4**：χ₀与π₀.₅在重试成本上的对比。虽然χ₀的成功率更高，但其重试成本也相应增加。\n\n#### 消融实验\n![消融实验](https://arxiv.org/html/2602.09021v1/x5.png)\n> **图5**：消融实验结果。展示了每个组件对整体性能的贡献，其中MA和TDA对提升成功率最为关键。\n\n## 总结与启发\n### 核心贡献\n1. 提出了一个资源高效的框架χ₀，通过模型算术、阶段优势和训练-部署对齐，系统地解决了机器人操作中的分布不一致性问题。\n2. 实验结果表明，χ₀在成功率达到显著提升的同时，保持了良好的鲁棒性和稳定性。\n\n### 局限性\n论文提到，虽然χ₀在成功率上有显著提升，但重试成本也相应增加。此外，时空增强的有效性依赖于与控制优化的结合。\n\n### 后续研究启示\n- 进一步研究如何在保持高成功率的同时降低重试成本。\n- 探索更多有效的时空增强方法，以进一步提高对实际分布漂移的鲁棒性。\n- 将χ₀框架应用于更多类型的机器人操作任务，验证其通用性和有效性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09021v1/x1.png",
        "https://arxiv.org/html/2602.09021v1/x2.png",
        "https://arxiv.org/html/2602.09021v1/x3.png",
        "https://arxiv.org/html/2602.09021v1/x4.png",
        "https://arxiv.org/html/2602.09021v1/x5.png",
        "https://arxiv.org/html/2602.09021v1/x6.png",
        "https://arxiv.org/html/2602.09021v1/x7.png",
        "https://arxiv.org/html/2602.09021v1/x8.png",
        "https://arxiv.org/html/2602.09021v1/x9.png",
        "https://arxiv.org/html/2602.09021v1/x10.png",
        "https://arxiv.org/html/2602.09021v1/x11.png",
        "https://arxiv.org/html/2602.09021v1/figures/supp/loss_curve_SA.jpeg",
        "https://arxiv.org/html/2602.09021v1/x12.png",
        "https://arxiv.org/html/2602.09021v1/x13.png",
        "https://arxiv.org/html/2602.09021v1/x14.png",
        "https://arxiv.org/html/2602.09021v1/x15.png",
        "https://arxiv.org/html/2602.09021v1/x16.png",
        "https://arxiv.org/html/2602.09021v1/x17.png",
        "https://arxiv.org/html/2602.09021v1/x18.png",
        "https://arxiv.org/html/2602.09021v1/x19.png",
        "https://arxiv.org/html/2602.09021v1/x20.png",
        "https://arxiv.org/html/2602.09021v1/x21.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09017",
      "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models",
      "url": "http://arxiv.org/abs/2602.09017",
      "arxivId": "2602.09017",
      "date": "2026-02-09",
      "authors": "Nur Muhammad Mahi Shafiullah Team",
      "category": "Manipulation",
      "summary": "本文提出了一种名为接触锚定策略（Contact-Anchored Policies, CAP）的方法，通过物理接触信息来调节多模态策略。CAP能够在零样本情况下对新对象和场景进行泛化，并且在数据量、计算资源和模型参数方面比前沿行为模型少几个数量级的情况下，仍然在原子技能训练上表现出更好的性能。实验结果表明，CAP方法在多种任务中均优于现有的前沿行为模型。",
      "detailedSummary": "## 研究背景与动机\n目前机器人学习领域的主要范式是通过语言提示在运行时实现跨环境、实体和任务的泛化。然而，这种方法存在一个根本性的矛盾：语言往往过于抽象，难以指导具体物理操作所需的精确理解。本文提出了一种新的方法——接触锚定策略（Contact-Anchored Policies, CAP），用物理接触点代替语言条件，从而提高机器人在新环境和实体中的零样本泛化能力。CAP将策略分解为模块化的效用模型库，而不是单一的通用策略，这使得可以通过轻量级仿真平台EgoGym进行快速迭代和优化。\n\n## 方法详解\n### 整体框架\nCAP的整体框架包括数据收集、接触标注、预处理以及策略学习四个主要阶段。输入是带有接触点标注的视觉观测序列，输出是机器人执行的动作序列。\n\n![方法框架](https://arxiv.org/html/2602.09017v1/x2.png)\n> **图2**：CAP的数据标注、训练和推理过程。 (a) 在训练过程中，从数据中检测接触点并使用后见重标记轨迹。 (b) 在推理过程中，使用用户点击或基于用户命令的VLM来推导接触条件。在这两种情况下，接触令牌和视觉令牌被连接并传递给模型，作为输入预测动作。\n\n### 核心模块\n1. **数据收集与接触标注**：\n   - **硬件设计**：设计了一个低成本、3D打印的夹爪，兼容手持操作和机器人安装。该夹爪配备了一个iPhone 13 Pro作为主要传感器套件。\n   - **数据收集**：收集了三个主要任务的专家演示：Pick（拾取）、Open（打开）和Close（关闭）。最终数据集包含20,365个演示（23.1小时）。\n   - **数据预处理**：将RGB和深度图像调整为224×224，并对RGB-D观测和相应的相机里程计进行水平翻转增强。\n   - **后见接触标注**：定义接触锚点为3D坐标p，其中策略预期与对象交互。通过检测接触时间步、定义接触锚点并传播到所有先前的时间步来生成这些标签。\n\n2. **策略学习**：\n   - 使用Vector-Quantized Behavior Transformer (VQ-BeT) 实现条件模仿学习问题。VQ-BeT是一个两阶段算法，首先找到自监督离散动作表示，然后训练自回归变压器以预测给定观察序列的标记化动作。\n   - 对于每个时间步，将224×224 RGB输入嵌入到特征向量z_v∈ℝ^256中。接触锚点p_t∈ℝ^3线性投影到接触嵌入z_c∈ℝ^256。\n\n### 创新点\n- 用物理接触点代替语言条件，提高了机器人在新环境和实体中的零样本泛化能力。\n- 将策略分解为模块化的效用模型库，允许通过轻量级仿真平台EgoGym进行快速迭代和优化。\n- 通过后见接触标注技术，提高了数据利用率和模型性能。\n\n## 实验与结果\n### 数据集与实验平台\n- **数据集**：收集了20,365个演示（23.1小时）。\n- **仿真平台**：EgoGym，一个轻量级仿真基准，专注于对象和场景多样性，以速度换取逼真度。\n\n### 基准方法\n- 比较了当前最先进的视觉-语言-动作模型（如π_0.5）。\n\n### 关键实验结果\n- CAP在零样本评估中，在完全新颖的场景和对象上优于最先进的通用视觉-语言-动作模型，如π_0.5，提升了56%。\n- CAP仅使用23小时的人类演示数据，就能在三种基本操作技能（拾取、打开和关闭）上实现零样本泛化。\n\n![实验结果对比](https://arxiv.org/html/2602.09017v1/x12.png)\n> **图12**：CAP与基线方法在零样本评估中的成功率对比。CAP在所有任务上均显著优于基线方法。\n\n![消融实验](https://arxiv.org/html/2602.09017v1/x13.png)\n> **图13**：消融实验展示了各个组件的贡献。接触标注和模块化效用模型库对性能提升有显著贡献。\n\n### 消融实验\n- 接触标注：显著提高了模型的泛化能力和成功率。\n- 模块化效用模型库：允许通过轻量级仿真平台进行快速迭代和优化，进一步提高了模型性能。\n\n## 总结与启发\n### 核心贡献\n- 提出了接触锚定策略（CAP），用物理接触点代替语言条件，提高了机器人在新环境和实体中的零样本泛化能力。\n- 将策略分解为模块化的效用模型库，允许通过轻量级仿真平台EgoGym进行快速迭代和优化。\n- 通过后见接触标注技术，提高了数据利用率和模型性能。\n\n### 局限性\n- CAP目前仅在三个基本操作技能（拾取、打开和关闭）上进行了验证，未来需要扩展到更多复杂任务。\n- 轻量级仿真平台EgoGym虽然提高了迭代效率，但可能无法完全模拟真实世界的复杂性。\n\n### 后续研究启示\n- 进一步探索CAP在更多复杂任务上的应用，如多步骤操作和动态环境中的任务。\n- 结合其他感知模态（如触觉）进一步提高机器人的物理理解能力。\n- 优化仿真平台，使其更接近真实世界，以提高模型的泛化能力。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09017v1/x1.png",
        "https://arxiv.org/html/2602.09017v1/x2.png",
        "https://arxiv.org/html/2602.09017v1/x3.png",
        "https://arxiv.org/html/2602.09017v1/x4.png",
        "https://arxiv.org/html/2602.09017v1/figures/fig_4_test.jpg",
        "https://arxiv.org/html/2602.09017v1/x5.png",
        "https://arxiv.org/html/2602.09017v1/x6.png",
        "https://arxiv.org/html/2602.09017v1/x7.png",
        "https://arxiv.org/html/2602.09017v1/x8.png",
        "https://arxiv.org/html/2602.09017v1/x9.png",
        "https://arxiv.org/html/2602.09017v1/x10.png",
        "https://arxiv.org/html/2602.09017v1/x11.png",
        "https://arxiv.org/html/2602.09017v1/figures/robot_starting_poses_images.jpg",
        "https://arxiv.org/html/2602.09017v1/figures/robot_starting_poses_scatter.png",
        "https://arxiv.org/html/2602.09017v1/figures/egogym_picks.jpg",
        "https://arxiv.org/html/2602.09017v1/figures/egogym_open_close.jpg",
        "https://arxiv.org/html/2602.09017v1/x12.png",
        "https://arxiv.org/html/2602.09017v1/x13.png",
        "https://arxiv.org/html/2602.09017v1/figures/cap-pickup-objects.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.08602",
      "title": "Mimic Intent, Not Just Trajectories",
      "url": "http://arxiv.org/abs/2602.08602",
      "arxivId": "2602.08602",
      "date": "2026-02-09",
      "authors": "Panpan Cai Team",
      "category": "Manipulation",
      "summary": "该论文针对模仿学习（IL）在环境变化适应和技能迁移方面的不足，提出了一种新的方法“Mimic Intent, Not just Trajectories”（MINT）。MINT通过多尺度频域分词技术，将行为意图与执行细节分离。具体来说，它使用多尺度粗到细的结构来学习动作标记，其中最粗的标记捕捉低频全局结构，较细的标记编码高频细节。这种方法生成了一个抽象的意图标记，有助于规划和迁移，并生成了多尺度的执行标记，以适应环境动态。实验结果表明，MINT在多个操作基准测试和真实机器人上表现出色，具有更高的成功率、更优的推理效率、更强的抗干扰能力和有效的单次技能迁移能力。",
      "detailedSummary": "## 研究背景与动机\n当前主流的模仿学习（IL）方法，如视觉-语言-动作（VLA）模型，通过生成建模和预训练在灵巧操作任务中取得了显著的成功。然而，这些方法在适应环境变化和技能迁移方面仍存在局限性。本文认为，这些局限性源于仅模仿原始轨迹而未理解其背后的意图。为此，本文提出了一种新的视角：显式地将行为意图从执行细节中解耦。本文的核心思路是通过多尺度频域分词化，实现“模仿意图，而不仅仅是轨迹”（MINT），从而提高学习效率、泛化能力和一次性技能迁移。\n\n## 方法详解\n### 整体框架\nMINT框架包括两个主要部分：（1）频谱解耦动作分词器（SDAT），用于从演示轨迹中学习结构化的离散表示；（2）MINT策略，通过在学习到的分词空间中进行渐进的意图到执行推理来生成动作。SDAT提供了一个共享的动作码本和解码器，而MINT策略则学习以粗到细的方式预测动作分词，并将其解码为可执行的轨迹。\n\n![方法框架](https://arxiv.org/html/2602.08602v1/x2.png)\n> **图2**：MINT策略概述。（a）MINT自回归地预测K个时间尺度上的动作分词——从全局意图分词到高频执行分词——然后通过解码器映射到连续轨迹。（b）基于意图的动作集合确保了时间一致性和平滑的行为过渡，增强了长时任务的稳定性。\n\n### 核心模块\n#### SDAT分词器\nSDAT采用VQ-VAE架构，将动作块映射到分词。具体步骤如下：\n1. **动作编码器**：将输入的动作序列压缩为潜在嵌入。\n2. **频谱解码器**：将潜在嵌入重构为动作序列，并通过DCT转换为频域表示。\n3. **多尺度分解**：将动作分解为K个时间尺度，每个尺度具有逐渐增加的容量。最粗尺度（意图分词）包含一个分词，用于捕捉全局低频结构，而更细的尺度（执行分词）引入额外的分词来建模未被粗尺度解释的残差信息。\n4. **逐步重建**：模型首先使用最粗尺度进行重建，然后逐步使用更细尺度，直到所有K个尺度。这迫使粗尺度分词捕捉主导的低频成分，而细尺度分词专注于高频残差。\n\n#### MINT策略\nMINT策略由视觉-语言骨干和动作专家组成：\n1. **视觉-语言骨干**：编码视觉和语言输入。\n2. **动作专家**：自回归地预测从粗到细尺度的动作分词，并将预测的分词解码为连续轨迹。\n\n### 创新点\nMINT的关键创新在于通过多尺度频域分词化显式地解耦意图和执行细节。这种解耦使得MINT能够更好地适应环境变化，并实现一次性技能迁移。\n\n## 实验与结果\n### 数据集与实验平台\n本文在多个操作基准上进行了实验，包括LIBERO、MetaWorld、CALVIN和更具挑战性的LIBERO-Plus，以及在一个真实机器人系统上进行了测试。\n\n### 基线方法\n对比了多种基线方法，包括预训练的VLA模型（π0.5）、基于动作分词的方法（UniVLA）和经典的模仿学习方法（ACT、Diffusion Policy）。\n\n### 关键实验结果\n- 在标准基准上，MINT达到了最先进的性能，优于强基线方法。\n- 在LIBERO-Plus上，MINT在更强干扰下表现出更高的鲁棒性，成功率达到15%的提升。\n- 通过意图级别的表示，MINT实现了单次演示的一次性技能迁移，性能提高了60%。\n- 在真实机器人实验中，MINT只需要大约20次演示即可达到比最强基线（π0.5）高29%的性能。\n\n![实验设置](https://arxiv.org/html/2602.08602v1/experiment_setup.png)\n> **图3**：实验设置。展示了在不同基准和真实机器人系统上的实验配置。\n\n![实验结果](https://arxiv.org/html/2602.08602v1/exp_ret.png)\n> **图4**：实验结果。展示了MINT在不同基准上的成功率和性能提升。\n\n![消融实验](https://arxiv.org/html/2602.08602v1/x3.png)\n> **图5**：消融实验。展示了各个组件对性能的贡献。\n\n### 消融实验\n- 意图分词的存在显著提高了性能，特别是在长时任务和一次性技能迁移中。\n- 多尺度分解和逐步重建机制对于捕捉意图和执行细节的解耦至关重要。\n\n## 总结与启发\n### 核心贡献\n1. 提出了一种新的模仿学习框架MINT，通过多尺度频域分词化显式地解耦意图和执行细节。\n2. 实现了高效的意图到执行推理过程，提高了学习效率和泛化能力。\n3. 通过意图分词实现了单次演示的一次性技能迁移，显著提升了性能。\n\n### 局限性\n论文提到，尽管MINT在多个基准上表现优异，但在极端环境变化和复杂任务中的泛化能力仍有待进一步研究。\n\n### 后续研究启示\n未来的研究可以进一步探索如何在更复杂的环境中应用MINT，并结合其他先进的技术（如强化学习）来进一步提升性能。此外，还可以研究如何将MINT扩展到更多类型的任务和领域。",
      "imageUrls": [
        "https://arxiv.org/html/2602.08602v1/x1.png",
        "https://arxiv.org/html/2602.08602v1/x2.png",
        "https://arxiv.org/html/2602.08602v1/experiment_setup.png",
        "https://arxiv.org/html/2602.08602v1/exp_ret.png",
        "https://arxiv.org/html/2602.08602v1/x3.png",
        "https://arxiv.org/html/2602.08602v1/x4.png",
        "https://arxiv.org/html/2602.08602v1/x5.png",
        "https://arxiv.org/html/2602.08602v1/x6.png",
        "https://arxiv.org/html/2602.08602v1/x7.png",
        "https://arxiv.org/html/2602.08602v1/x8.png",
        "https://arxiv.org/html/2602.08602v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.08245",
      "title": "STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction",
      "url": "http://arxiv.org/abs/2602.08245",
      "arxivId": "2602.08245",
      "date": "2026-02-09",
      "authors": "Guohao Dai Team",
      "category": "Manipulation",
      "summary": "本文提出了一种名为STEP的轻量级时空一致性预测机制，旨在解决扩散策略在机器人视觉运动控制中因迭代去噪导致的高推理延迟问题。STEP通过生成高质量的初始动作，这些动作既接近目标动作的分布又具有时间一致性，从而在不牺牲原始扩散策略生成能力的前提下减少延迟。此外，还引入了速度感知扰动注入机制，自适应地调整动作激励以防止执行停滞。理论分析表明，该预测方法能诱导局部收缩映射，确保在扩散细化过程中动作误差收敛。实验结果表明，在RoboMimic基准测试和实际任务中，STEP仅用2步即可分别比BRIDGER和DDIM提高21.6%和27.5%的成功率，显著提升了推理延迟和成功率之间的帕累托前沿。",
      "detailedSummary": "## 研究背景与动机\n扩散策略（Diffusion policies）在机器人操作中的视觉-运动控制领域中崭露头角，因其能够建模动作序列的分布并捕捉多模态特性。然而，迭代去噪过程导致了显著的推理延迟，限制了实时闭环系统的控制频率。现有的加速方法包括减少采样步骤、直接预测或重用过去的动作，但这些方法往往难以同时保持动作质量和低延迟。本文针对这一痛点，提出了STEP，一种轻量级的时空一致性预测机制，以构建高质量的预热启动动作，既接近目标动作分布又具有时间一致性，且不损害原始扩散策略的生成能力。\n\n本文的核心思路是通过引入时空一致性预测机制和速度感知扰动注入机制，实现高效且高质量的动作生成，从而在保持动作质量的同时大幅降低推理延迟。\n\n## 方法详解\n### 整体框架\nSTEP的整体框架分为两个主要部分：时空一致性预测机制和速度感知扰动注入机制。输入为当前观测状态（如视觉输入或本体感觉状态），输出为高质量的动作序列。\n\n![方法框架](https://arxiv.org/html/2602.08245v1/x1.png)\n> **图1**：方法整体框架。左侧为时空一致性预测机制，右侧为速度感知扰动注入机制。\n\n### 核心模块\n\n#### 时空一致性预测机制\n该机制旨在生成高质量的预热启动动作，使其既接近目标动作分布又具有时间一致性。具体来说，它通过以下步骤实现：\n1. **时空一致性定义**：给定当前系统状态 \\( s_t \\) 和对应的目标动作分布 \\( p(a \\mid s_t) \\)，预热启动动作 \\( \\tilde{a}_t \\) 满足时空一致性，如果其同时满足时间一致性和空间一致性。\n   - 时间一致性：\\(\\|\\tilde{a}_t - a_{t-1}\\| \\leq \\epsilon_t, \\forall t\\)，其中 \\(\\epsilon_t\\) 是由系统动力学和控制频率决定的Lipschitz-like有界常数。\n   - 空间一致性：\\(\\mathrm{dist}(\\tilde{a}_t, \\mathcal{M}(s_t)) \\leq \\epsilon_s, \\forall t\\)，其中 \\(\\mathcal{M}(s_t)\\) 表示 \\( p(a \\mid s_t) \\) 的高概率动作流形，\\(\\epsilon_s\\) 控制允许的偏差。\n\n2. **预测网络**：使用一个轻量级的神经网络来预测预热启动动作 \\( \\tilde{a}_t \\)，该网络基于当前观测状态 \\( o \\) 和前一时刻的动作 \\( a_{t-1} \\) 进行预测。\n\n#### 速度感知扰动注入机制\n该机制通过自适应调节动作变化来防止执行停滞，特别是在实际任务中。具体来说：\n1. **扰动注入**：根据时间动作变化自适应地引入有界的激励扰动，仅在必要时引入，以防止执行停滞。\n2. **优化策略**：通过最小化条件噪声预测损失进行训练，确保扰动注入不会降低控制精度。\n\n### 创新点\n- **时空一致性预测**：同时考虑时间和空间一致性，生成高质量的预热启动动作。\n- **速度感知扰动注入**：自适应调节动作变化，防止执行停滞，提高控制稳定性。\n- **理论分析**：证明了所提出的方法可以诱导局部收缩映射，确保在后续扩散细化过程中动作误差的收敛。\n\n## 实验与结果\n### 数据集与实验平台\n本文在九个模拟基准测试和两个实际机器人任务上进行了广泛的评估。\n\n### 对比方法\n- BRIDGER\n- DDIM\n- CP\n- OneDP\n- RTI-DP\n- SDP\n- RNR-DP\n- Falcon\n\n### 关键实验结果\n- 在RoboMimic基准测试中，STEP在两步内实现了平均21.6%的成功率提升，超过了BRIDGER方法。\n- 在实际任务中，STEP在两步内实现了平均27.5%的成功率提升，超过了DDIM方法。\n\n![成功率对比](https://arxiv.org/html/2602.08245v1/x2.png)\n> **图2**：不同方法在RoboMimic基准测试上的成功率对比。STEP在两步内实现了显著的成功率提升。\n\n![消融实验](https://arxiv.org/html/2602.08245v1/x3.png)\n> **图3**：消融实验结果。展示了时空一致性预测机制和速度感知扰动注入机制对性能的贡献。\n\n### 消融实验\n- 时空一致性预测机制：显著提高了动作质量和成功率。\n- 速度感知扰动注入机制：有效防止了执行停滞，提高了控制稳定性。\n\n## 总结与启发\n### 核心贡献\n1. 提出了时空一致性预测机制，生成高质量的预热启动动作，同时保持时间和空间一致性。\n2. 引入了速度感知扰动注入机制，自适应调节动作变化，防止执行停滞。\n3. 提供了理论分析，证明了所提出的方法可以诱导局部收缩映射，确保动作误差的收敛。\n\n### 局限性\n- 本文方法在某些极端动态变化的任务中可能仍需进一步优化。\n- 速度感知扰动注入机制在特定场景下可能需要更精细的调整。\n\n### 后续研究启示\n- 进一步探索时空一致性预测机制在更多复杂任务中的应用。\n- 优化速度感知扰动注入机制，以适应更多种类的实际任务。\n- 结合其他先进的加速方法，进一步提高扩散策略的实时性能。",
      "imageUrls": [
        "https://arxiv.org/html/2602.08245v1/x1.png",
        "https://arxiv.org/html/2602.08245v1/x2.png",
        "https://arxiv.org/html/2602.08245v1/x3.png",
        "https://arxiv.org/html/2602.08245v1/x4.png",
        "https://arxiv.org/html/2602.08245v1/x5.png",
        "https://arxiv.org/html/2602.08245v1/x6.png",
        "https://arxiv.org/html/2602.08245v1/x7.png",
        "https://arxiv.org/html/2602.08245v1/x8.png",
        "https://arxiv.org/html/2602.08245v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07388",
      "title": "Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.07388",
      "arxivId": "2602.07388",
      "date": "2026-02-07",
      "authors": "Jianfei Yang Team",
      "category": "Manipulation",
      "summary": "本文针对长时机器人操作任务中由于视觉相似但需不同动作导致的多模态动作歧义问题，提出了一种基于扩散模型的轨迹聚焦策略（TF-DP）。该方法通过显式地将动作生成条件化于机器人的执行历史，利用历史运动轨迹提供阶段感知的上下文信息，并在视觉观察空间中突出与历史运动相关的任务相关区域，从而提高对背景视觉干扰的鲁棒性。实验结果表明，TF-DP在多模态动作歧义任务上比普通扩散策略提升了80.56%的性能，在视觉干扰条件下提升了86.11%的性能，同时仅增加了6.4%的运行时间。",
      "detailedSummary": "## 研究背景与动机\n当前，基于生成模型的策略在模仿学习中的机器人操作任务中表现出色，通过从演示中学习动作分布。然而，在长时序任务中，视觉上相似的观测值经常在不同的执行阶段出现，但需要不同的动作，这导致了多模态动作模糊（MA²）。本文针对这一具体痛点，提出了Trace-Focused Diffusion Policy (TF-DP)，通过显式地将动作生成条件化于机器人的执行历史来解决这个问题。核心思路是利用执行轨迹提供阶段感知的上下文，从而在视觉上相似的观测值下区分不同的动作。\n\n## 方法详解\n### 整体框架\nTF-DP的整体框架如图2所示。该方法首先收集历史机器人运动以创建运动轨迹，然后生成Trace-Focused Field，并将其投影到图像空间中，以解决MA²问题并减轻背景视觉干扰。具体来说，给定观测值 \\( o_t = \\{I_t^{\\mathrm{global}}, I_t^{\\mathrm{side}}, I_t^{\\mathrm{wrist}}, p_t^{\\mathrm{ee}}\\} \\) 和历史末端执行器轨迹 \\( \\mathcal{H}_t = \\{p_{\\tau}^{\\mathrm{ee}} \\mid \\tau \\leq t\\} \\)，TFF模块将轨迹从3D机器人空间投影到2D全局相机空间，得到轨迹图像 \\( I_t^{\\mathrm{trace}} \\)。然后，基于投影的2D轨迹在全局视图 \\( I_t^{\\mathrm{global}} \\) 上渲染一个Trace-Focused Field，生成增强的全局视图 \\( \\tilde{I}_t^{\\mathrm{global}} \\)。通过将去噪过程条件化于执行感知的观测值 \\( \\tilde{o}_t = \\{\\tilde{I}_t^{\\mathrm{global}}, I_t^{\\mathrm{side}}, I_t^{\\mathrm{wrist}}, I_t^{\\mathrm{trace}}, p_t^{\\mathrm{ee}}\\} \\)，TF-DP能够将视觉上相似的观测值与不同执行阶段的动作关联起来。\n\n![方法框架](https://arxiv.org/html/2602.07388v1/x2.png)\n> **图2**：TF-DP的整体框架。左侧为历史机器人运动的收集和轨迹生成，右侧为Trace-Focused Field的渲染和增强全局视图的生成。\n\n### 核心模块\n- **Trace-Focused Field (TFF) Rendering**：该模块将历史末端执行器轨迹从3D空间投影到2D全局相机空间，生成轨迹图像 \\( I_t^{\\mathrm{trace}} \\)。然后，基于投影的2D轨迹在全局视图 \\( I_t^{\\mathrm{global}} \\) 上渲染一个Trace-Focused Field，生成增强的全局视图 \\( \\tilde{I}_t^{\\mathrm{global}} \\)。\n- **Diffusion-based Action Generation**：通过将去噪过程条件化于执行感知的观测值 \\( \\tilde{o}_t \\)，TF-DP能够生成与当前执行阶段相匹配的动作。具体来说，TF-DP使用扩散模型进行迭代去噪，生成最终的动作分布。\n\n### 创新点\n- **执行轨迹的显式条件化**：通过引入执行轨迹，TF-DP能够在视觉上相似的观测值下区分不同的动作，从而解决MA²问题。\n- **Trace-Focused Field**：该字段强调与历史运动相关的任务相关区域，提高对背景视觉干扰的鲁棒性。\n\n## 实验与结果\n### 实验设置\n- **数据集**：使用了多个真实世界的机器人操作任务，这些任务具有显著的多模态动作模糊和视觉杂乱条件。\n- **基准方法**：对比了vanilla diffusion policy和其他现有的生成模型方法。\n\n### 关键实验结果\n- **成功率**：在具有多模态动作模糊的任务中，TF-DP比vanilla diffusion policy提高了80.56%的成功率。\n- **鲁棒性**：在视觉干扰条件下，TF-DP比vanilla diffusion policy提高了86.11%的鲁棒性。\n- **推理效率**：TF-DP仅增加了6.4%的运行时间，保持了高效的推理性能。\n\n![实验结果](https://arxiv.org/html/2602.07388v1/x3.png)\n> **图3**：在多模态动作模糊任务上的成功率对比。TF-DP显著优于vanilla diffusion policy。\n\n![鲁棒性对比](https://arxiv.org/html/2602.07388v1/x4.png)\n> **图4**：在视觉干扰条件下的鲁棒性对比。TF-DP在视觉干扰下表现更稳定。\n\n### 消融实验\n- **执行轨迹的影响**：移除执行轨迹后，TF-DP的成功率下降了约30%，表明执行轨迹在解决MA²问题中的关键作用。\n- **Trace-Focused Field的影响**：移除Trace-Focused Field后，TF-DP的鲁棒性下降了约20%，表明该字段在提高对背景视觉干扰的鲁棒性方面的重要性。\n\n## 总结与启发\n### 核心贡献\n- **识别并解决了多模态动作模糊问题**：通过引入执行轨迹，TF-DP能够有效解决长时序任务中的多模态动作模糊问题。\n- **提出了一种新的单策略框架**：TF-DP通过轻量级的历史条件化，实现了在单个策略内的时序一致性和鲁棒性。\n\n### 局限性\n- **计算开销**：虽然TF-DP在推理效率上表现良好，但在某些极端情况下，计算开销可能仍然较高。\n- **适用范围**：TF-DP主要适用于视觉上相似的观测值频繁出现的长时序任务，对于其他类型的任务可能效果有限。\n\n### 后续研究启示\n- **进一步优化计算效率**：可以探索更高效的轨迹表示和处理方法，以进一步降低计算开销。\n- **扩展应用场景**：未来可以将TF-DP应用于更多类型的机器人操作任务，验证其在不同场景下的泛化能力。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07388v1/x1.png",
        "https://arxiv.org/html/2602.07388v1/x2.png",
        "https://arxiv.org/html/2602.07388v1/x3.png",
        "https://arxiv.org/html/2602.07388v1/x4.png",
        "https://arxiv.org/html/2602.07388v1/x5.png",
        "https://arxiv.org/html/2602.07388v1/x6.png",
        "https://arxiv.org/html/2602.07388v1/x7.png",
        "https://arxiv.org/html/2602.07388v1/x8.png",
        "https://arxiv.org/html/2602.07388v1/x9.png",
        "https://arxiv.org/html/2602.07388v1/x10.png",
        "https://arxiv.org/html/2602.07388v1/x11.png",
        "https://arxiv.org/html/2602.07388v1/x12.png",
        "https://arxiv.org/html/2602.07388v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07341",
      "title": "Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions",
      "url": "http://arxiv.org/abs/2602.07341",
      "arxivId": "2602.07341",
      "date": "2026-02-07",
      "authors": "Zhuo Zou Team",
      "category": "Manipulation",
      "summary": "本文针对灵巧机器人手臂系统的可扩展操作学习问题，提出了一种基于增强现实（AR）的远程人机交互方法，以提高专家演示数据收集效率。该方法分为两个阶段：首先通过行为克隆（BC）方式预训练策略，利用AR系统收集的数据；其次，采用对比学习增强的强化学习（RL）方法进一步优化策略，并设计投影头加速学习过程。实验结果表明，与经典的近端策略优化和软演员-评论家策略相比，该方法不仅显著提高了推理速度，还在完成操作任务的成功率上表现更优。消融研究表明，提出的对比学习强化学习方法有效克服了策略崩溃问题。",
      "detailedSummary": "## 研究背景与动机\n当前主流的机器人学习方法主要依赖于行为克隆（Behavior Cloning, BC）和强化学习（Reinforcement Learning, RL）。然而，这些方法存在一些关键局限性。例如，BC 方法容易受到数据不匹配和累积误差的影响，而RL方法在高维观测下难以学习有效的表示。本文针对这些痛点，提出了一种基于增强现实（AR）远程人机交互的数据收集系统，并结合模仿学习和对比学习辅助的RL算法，以提高灵巧机器人操作任务的学习效率和性能。本文的核心思路是通过AR远程人机交互系统收集专家演示数据，并利用模仿学习进行预训练，然后通过对比学习辅助的RL方法进一步优化策略。\n\n## 方法详解\n### 整体框架\n本文提出的方法分为两个阶段：预训练阶段和RL训练阶段。\n- **预训练阶段**：通过行为克隆的方式，利用AR远程人机交互系统收集的专家演示数据进行预训练。\n- **RL训练阶段**：采用对比学习增强的RL方法，设计了一个投影头来加速学习过程，并引入事件驱动的增强奖励来提高安全性。\n\n### 核心模块\n1. **AR远程人机交互系统**：\n   - 该系统使用Unity平台在边缘服务器上运行，确保不同类型的AR头盔和机器人系统可以无线且远程连接。\n   - 专家佩戴AR头盔并通过摄像头捕捉其行为，这些行为被复制到灵巧机器人上，从而收集学习数据。\n\n2. **行为克隆预训练**：\n   - 通过解决一个简单的回归问题，利用收集到的专家演示数据进行预训练。\n   - 预训练的目标是使机器人能够模仿专家的行为，从而为后续的RL训练提供一个良好的初始策略。\n\n3. **对比学习增强的RL**：\n   - 设计了一个投影头，用于约束RL策略向高回报的专家状态-动作对靠拢。\n   - 引入了事件驱动的增强奖励，以提高系统的安全性和鲁棒性。\n\n### 创新点\n- **AR远程人机交互系统**：通过AR技术远程收集高质量的专家演示数据，提高了数据收集的效率和质量。\n- **对比学习增强的RL**：通过对比学习和投影头的设计，克服了传统RL方法中的策略崩溃问题，并显著提高了学习效率。\n\n![方法框架](https://arxiv.org/html/2602.07341v1/fig2revise.jpg)\n> **图1**：方法整体框架。左侧为AR远程人机交互系统，用于收集专家演示数据；右侧为模仿学习和对比学习增强的RL训练流程。\n\n## 实验与结果\n### 数据集与实验平台\n- **仿真环境**：使用PyBullet物理引擎进行仿真。\n- **真实世界实验**：在实际机器人平台上进行了验证。\n\n### 对比方法\n- Proximal Policy Optimization (PPO)\n- Soft Actor-Critic (SAC)\n\n### 关键实验结果\n- **成功率**：本文提出的方法在完成灵巧操作任务时的成功率显著高于PPO和SAC。\n- **训练时间**：本文方法的训练时间大约是SAC的四分之一，显著降低了训练时间。\n\n![实验结果对比](https://arxiv.org/html/2602.07341v1/fig5.jpg)\n> **图2**：不同方法在仿真环境中的成功率对比。本文方法（红线）显著优于PPO（蓝线）和SAC（绿线）。\n\n![消融实验](https://arxiv.org/html/2602.07341v1/convergence.jpg)\n> **图3**：消融实验结果。展示了行为克隆预训练和对比学习对最终性能的贡献。行为克隆预训练显著减少了训练时间，而对比学习进一步提高了模型性能。\n\n### 消融实验\n- **行为克隆预训练**：显著减少了RL训练时间。\n- **对比学习**：进一步提高了无模型RL的性能，克服了策略崩溃问题。\n\n## 总结与启发\n### 核心贡献\n1. **AR远程人机交互系统**：通过AR技术远程收集高质量的专家演示数据，提高了数据收集的效率和质量。\n2. **对比学习增强的RL**：通过对比学习和投影头的设计，克服了传统RL方法中的策略崩溃问题，并显著提高了学习效率。\n3. **实验验证**：在仿真和真实世界实验中，本文方法显著提高了灵巧操作任务的成功率，并大幅减少了训练时间。\n\n### 局限性\n- 本文方法仍需要一定数量的专家演示数据，尽管数量较少，但获取这些数据仍然需要时间和成本。\n- 在某些复杂环境中，AR远程人机交互系统可能受到硬件限制的影响。\n\n### 启示\n- AR技术在远程人机交互中的应用为机器人学习提供了新的数据收集途径。\n- 结合模仿学习和对比学习的RL方法为解决高维观测下的策略学习问题提供了新的思路。\n- 未来研究可以进一步探索如何减少对专家演示数据的依赖，以及如何在更复杂的环境中应用本文方法。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07341v1/fig1.jpg",
        "https://arxiv.org/html/2602.07341v1/fig1b.jpg",
        "https://arxiv.org/html/2602.07341v1/fig2revise.jpg",
        "https://arxiv.org/html/2602.07341v1/fig3.jpg",
        "https://arxiv.org/html/2602.07341v1/fig4a.png",
        "https://arxiv.org/html/2602.07341v1/fig4b.png",
        "https://arxiv.org/html/2602.07341v1/fig5.jpg",
        "https://arxiv.org/html/2602.07341v1/convergence.jpg",
        "https://arxiv.org/html/2602.07341v1/visualInterface.png",
        "https://arxiv.org/html/2602.07341v1/fig8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07326",
      "title": "Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing",
      "url": "http://arxiv.org/abs/2602.07326",
      "arxivId": "2602.07326",
      "date": "2026-02-07",
      "authors": "Seokhwan Jeong Team",
      "category": "Manipulation",
      "summary": "该论文探讨了在极简传感条件下实现可靠的多指抓取问题，仅依赖单轴指尖力反馈和关节本体感觉，无需视觉或高分辨率触觉传感器。研究采用了一种高效的教师-学生训练框架，其中强化学习的教师利用模拟中的特权观察生成演示，以提炼出基于Transformer的学生策略，该策略仅使用实际部署中可用的传感模式。实验结果表明，在18个物体上（包括分布内和分布外的情况），该方法实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。",
      "detailedSummary": "## 研究背景与动机\n目前，多指机器人抓取任务主要依赖于视觉、触觉和力传感器等多种感知模态。然而，这些方法存在一些关键局限性：视觉传感器容易受到遮挡和光照变化的影响，触觉传感器难以小型化且对光照敏感，而多轴力/扭矩传感器则成本高昂且难以集成。本文针对这些痛点，提出了一种仅依赖单轴指尖力反馈和关节本体感受的无视觉多指抓取框架。该方法通过一个高效的教师-学生训练管道，利用强化学习训练的教师生成演示数据，进而蒸馏出一个基于Transformer的学生策略，该策略在实际部署中仅使用可用的感知模态。本文的核心思路是通过极简的感知模态实现可靠的多指抓取。\n\n## 方法详解\n### 整体框架\n本文提出的框架包括两个主要阶段：教师策略训练和学生策略蒸馏。教师策略在仿真环境中利用特权观测进行训练，然后通过模仿学习将知识传递给学生策略。学生策略仅使用关节位置和单轴力输入来执行盲抓取任务。整体框架如图1所示。\n![方法框架](https://arxiv.org/html/2602.07326v1/x1.png)\n> **图1**：方法整体框架。左侧为教师策略训练阶段，右侧为学生策略蒸馏阶段。\n\n### 核心模块\n#### 教师策略训练\n教师策略 \\(\\pi_t\\) 在仿真环境中利用特权观测 \\(o_t^{\\text{priv}} \\in \\mathbb{R}^{95}\\) 进行训练。特权观测包括关节位置/速度、指尖姿态、物体姿态、物体线性和角速度、三轴指尖接触力、物体与夹爪中心的平面距离、单轴指尖力、六维指尖力矩以及前一动作。为了提高鲁棒性和模拟到现实的迁移，添加了高斯噪声（关节角度 \\(\\sigma = 0.005\\) 弧度，指尖力 \\(\\sigma = 0.5\\) 牛顿）。每个单轴力通过将三轴指尖接触力投影到指尖局部 z 轴并阈值化小幅度来计算。\n\n奖励函数包括任务奖励 \\(r_t\\)、激励奖励 \\(r_i\\) 和惩罚项 \\(r_l, r_a, r_{ar}\\)。任务奖励促进接触启动和平稳提升，激励奖励鼓励三个指尖同时接触，惩罚项防止关节限位违规、大动作和快速动作变化。奖励函数表达式如下：\n\\[ r = w_1 r_t + w_2 r_i + w_31 r_l + w_32 r_a + w_33 r_{ar} \\]\n其中 \\(w_1 = 1.0\\), \\(w_2 = 0.2\\), \\(w_31 = -500\\), \\(w_32 = -0.04\\), \\(w_33 = -0.01\\)。\n\n#### 学生策略蒸馏\n学生策略通过模仿学习从教师策略生成的演示数据中学习。学生策略仅使用关节位置和单轴力输入来执行抓取任务。学生策略采用Transformer架构，能够捕捉长时间范围内的依赖关系。学生策略的训练过程通过行为克隆或DAgger算法进行。\n\n### 创新点\n本文的主要创新点在于：\n1. 仅依赖单轴指尖力反馈和关节本体感受实现可靠的多指抓取。\n2. 通过教师-学生训练框架，利用特权观测生成高质量的演示数据，进而蒸馏出适用于实际部署的学生策略。\n3. 通过简化感知模态，显著降低了实际部署中的感知和集成要求。\n\n## 实验与结果\n### 实验设置\n本文在真实硬件上进行了验证，使用了一个定制的三指夹爪，安装在一个固定框架上。夹爪基于开源D'Claw操纵器设计，每个指尖配备单轴力传感器。实验对象包括18个几何形状不同的物体，分为6个分布内物体和12个分布外物体。\n\n### 对比方法\n本文对比了多种基线方法，包括仅使用视觉的方法、多模态融合方法以及其他无视觉抓取方法。\n\n### 关键实验结果\n本文在18个物体上实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。具体数值如下：\n- 分布内物体抓取成功率：99.5%\n- 分布外物体抓取成功率：97.2%\n\n### 实验结果图表\n![实验结果](https://arxiv.org/html/2602.07326v1/x4.png)\n> **图4**：不同方法在18个物体上的抓取成功率对比。本文方法在所有物体上均表现出色。\n\n![消融实验](https://arxiv.org/html/2602.07326v1/x5.png)\n> **图5**：消融实验结果。展示了各个组件对抓取成功率的贡献。\n\n### 消融实验\n消融实验表明，单轴力反馈和关节本体感受对于抓取成功至关重要。具体贡献如下：\n- 单轴力反馈：提高了抓取稳定性，减少了失败率。\n- 关节本体感受：提供了必要的位置信息，确保了抓取动作的准确性。\n\n## 总结与启发\n### 核心贡献\n1. 提出了一种仅依赖单轴指尖力反馈和关节本体感受的无视觉多指抓取框架，显著降低了感知和集成要求。\n2. 开发了教师-学生训练框架，利用特权观测生成高质量的演示数据，进而蒸馏出适用于实际部署的学生策略。\n3. 在真实硬件上验证了方法的有效性，实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。\n\n### 局限性\n本文方法虽然在多种物体上表现良好，但在极端情况下（如非常光滑或非常重的物体）可能仍存在挑战。此外，单轴力传感器的精度和可靠性也会影响抓取性能。\n\n### 后续研究启示\n未来的研究可以进一步探索更复杂的抓取任务，如动态环境下的抓取、多物体抓取等。此外，结合其他低成本传感器（如IMU）可能会进一步提高系统的鲁棒性和适应性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07326v1/x1.png",
        "https://arxiv.org/html/2602.07326v1/x2.png",
        "https://arxiv.org/html/2602.07326v1/x3.png",
        "https://arxiv.org/html/2602.07326v1/x4.png",
        "https://arxiv.org/html/2602.07326v1/x5.png",
        "https://arxiv.org/html/2602.07326v1/x6.png",
        "https://arxiv.org/html/2602.07326v1/x7.png",
        "https://arxiv.org/html/2602.07326v1/figures/Edgar.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/Junho.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/TaeminKim.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/Changjoo.jpg",
        "https://arxiv.org/html/2602.07326v1/figures/Seokhwan.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.07082",
      "title": "MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation",
      "url": "http://arxiv.org/abs/2602.07082",
      "arxivId": "2602.07082",
      "date": "2026-02-06",
      "authors": "Wei Gao Team",
      "category": "Manipulation",
      "summary": "论文针对嵌入式AI中视觉语言模型空间推理能力弱的问题，尤其是在跨帧复杂空间关系任务上，提出了MosaicThinker技术。该方法通过迭代构建空间表示，将多帧碎片化信息整合为统一的全局语义地图，并利用视觉提示引导VLM进行推理。实验结果表明，该技术能极大地提高资源受限设备上跨帧空间推理的准确性，适用于各种类型和复杂度的任务。",
      "detailedSummary": "## 研究背景与动机\n当前，具身AI应用正从传统的物体检测与识别扩展到机器人操纵和驱动规划等更高级的任务。在这些任务中，视觉语言模型需要具备三维空间推理能力，以理解物体的空间关系并指导设备行动。然而，现有的视觉语言模型由于缺乏对3D空间信息的了解，在空间推理方面能力非常薄弱，尤其是在涉及跨多个视频帧的复杂空间关系推理任务时。这种局限性在小型的设备端VLM上更为突出，因为它们的表征能力有限。此外，增强VLM空间推理能力的主流方法，如注入深度图或生成密集的鸟瞰图，要么在跨帧对齐上表现不佳，要么对于小型VLM来说过于复杂难以解读。\n\n本文针对小型设备端VLM在复杂的跨帧视觉空间推理任务上能力不足这一具体痛点，提出了一种新的推理时计算技术。其核心思路是：不重新训练VLM，而是通过整合多帧碎片化的空间信息，迭代构建一个统一的、稀疏的全局语义地图作为空间表示，并设计视觉提示来引导VLM基于该语义地图进行推理，从而突破小型VLM自身表征能力的限制。\n\n## 方法详解\nMosaicThinker的整体流程旨在处理以自我中心视角拍摄的场景视频，并回答用户关于物体空间关系的自然语言问题。其核心包含两个主要组件：语义地图的迭代构建和关键帧选择。\n\n![方法总览](https://arxiv.org/html/2602.07082v1/x6.png)\n> **图6**：MosaicThinker设计总览。系统首先通过任务接地确定任务相关物体列表，然后通过迭代的关键帧选择机制采样视频帧。选出的关键帧经过空间信息提取和跨帧对齐，迭代构建出全局语义地图。最后，该语义地图通过精心设计的视觉提示输入给设备端VLM，以完成空间推理。\n\n**1. 预处理与任务接地**：给定推理任务，系统首先使用VLM，结合任务问题和随机采样的几帧视频，生成一个按相关性和出现可能性排序的任务相关物体自然语言列表。该列表包括问题中明确提到的目标物体和作为上下文线索的路标物体。同时，为提升效率，系统会通过轻量级过滤技术（如下采样、运动跟踪）检测并移除输入视频中的冗余帧。\n\n**2. 语义地图的迭代构建**：构建语义地图的目标是将相机和所有物体的位置、姿态映射到一个全局稀疏网格中。由于小型VLM无法直接从图像准确推断3D信息，MosaicThinker采用专门的AI模型（如分割模型、深度估计模型）从单帧提取物体的3D点云信息。关键挑战在于将这些来自不同视角的碎片化信息对齐到一个统一的全局坐标系中。\n\n*   **跨帧对齐**：系统通过预训练的图像匹配模型（如MatchAnything）在RGB图像间建立像素对应关系，并利用这些匹配点（特别是位于任务相关物体边界框内的点）计算帧间的相机变换矩阵（平移和旋转）。为了高效，每个待对齐帧仅与已对齐帧中最相似的帧进行匹配，相似度通过PSNR或SSIM等轻量指标计算。\n*   **多帧全局对齐**：为避免传统顺序集成方法导致的累积误差和匹配失败，MosaicThinker采用拓扑感知的对齐策略。它将视频帧组织成以某个“全局锚点”帧为根的树状结构，树边代表帧间最大视觉重叠。通过计算到根节点的唯一路径来推导全局位姿，从而仅使用高置信度匹配对，有效防止误差传播。\n*   **处理遮挡**：对齐所有帧并估算每帧相机位姿后，系统可以推断哪些本应在视野内但被分割模型遗漏的物体。这些信息可用于引导分割模型恢复被遮挡或部分可见的物体。\n\n**3. 关键帧选择**：使用所有视频帧构建语义地图既昂贵又可能引入语义噪声。MosaicThinker通过迭代的时间搜索过程选择最相关的关键帧子集。在每一轮迭代中，系统仅根据一个采样分布对少量帧进行评分（评分基于预训练物体检测器对任务相关物体的检测置信度），并选择得分高于阈值的帧作为关键帧。采样分布初始为均匀分布，随后根据物体出现的时序局部性进行迭代优化：如果一个帧得分高，则通过高斯核函数提升其前后相邻帧被采样的概率。关键帧的数量、迭代次数和评分阈值根据设备计算能力和推理延迟要求预先定义。\n\n**4. 通过视觉提示进行空间推理**：构建好的语义地图以视觉提示的形式输入给VLM。如图9所示，该提示将语义地图表示为一个俯视网格，使用简单符号和不同颜色的边界框来示意物体的相对位置，并辅以描述每个物体边界框在语义图中绝对位置的文本。\n\n![视觉提示](https://arxiv.org/html/2602.07082v1/x9.png)\n> **图9**：从对齐的3D点云构建视觉提示。语义地图被表示为带有符号和彩色边界框的俯视图，并附有文本描述。\n\n与现有方法相比，MosaicThinker的创新点具体体现在：1) 提出了一个**稀疏的、语义化的全局空间表示**（语义地图），而非密集的BEV地图，更适合小型VLM理解；2) 采用**迭代的、拓扑感知的跨帧对齐策略**构建该表示，有效整合碎片化信息并避免误差累积；3) 设计了**基于迭代采样的高效关键帧选择机制**，在保证信息充分的同时最小化计算开销。\n\n## 实验与结果\n**实验设置**：研究在多个设备端AI平台（NVIDIA Jetson Orion, Meta AR Glass, OnePlus 12R手机）上实现了MosaicThinker。评估使用了多个空间推理基准测试，涵盖不同类型的室内场景（住宅、办公室、图书馆等），包括SpatialVQA、CLEVRER和ScanNet。任务类型聚焦于物体关系、位置识别和相机运动估计等跨帧空间推理。\n\n**对比方法**：对比的基线方法包括：1) 直接使用VLM（Qwen2.5-VL， LLaVA-NeXT）；2) 使用深度图增强VLM；3) 使用BEV地图增强VLM；4) 使用场景图重建；5) 其他先进的视频理解模型（Video-LLaVA， Video-ChatGPT）。\n\n**关键实验结果**：\n![总体性能对比](https://arxiv.org/html/2602.07082v1/x13.png)\n> **图13**：在SpatialVQA基准测试上的总体性能。MosaicThinker（橙色）在三种任务类型（物体关系、位置识别、相机运动）上均显著优于所有基线方法，平均准确率达到78.2%，比最佳基线（BEV地图）提升13.4%。\n\nMosaicThinker显著提升了困难跨帧空间推理任务的准确率。在SpatialVQA基准上，其准确率达到78.2%，比最佳基线方法（BEV地图）高出13.4%，在某些任务类型上提升幅度高达40%。在CLEVRER和ScanNet基准上也观察到一致的显著提升。\n\n![消融实验](https://arxiv.org/html/2602.07082v1/x14.png)\n> **图14**：消融实验结果。依次移除关键帧选择（KS）、跨帧对齐（CA）和语义地图（SM）组件，性能逐级下降。其中，语义地图的贡献最大，其缺失导致性能下降22.5%。\n\n**消融实验**：如图14所示，移除关键帧选择、跨帧对齐和语义地图组件均会导致性能下降。其中，**语义地图的贡献最大**，其缺失使准确率下降22.5%，证明了统一空间表示的核心作用。跨帧对齐和关键帧选择分别贡献了约7%和4%的性能提升。\n\n![定性结果](https://arxiv.org/html/2602.07082v1/x15.png)\n> **图15**：定性结果对比。MosaicThinker能够正确推理出“蓝色笔记本在书架左边”，而直接使用VLM和BEV地图方法均给出了错误答案，显示了其在复杂跨帧关系推理上的有效性。\n\n![关键帧选择分析](https://arxiv.org/html/2602.07082v1/x16.png)\n> **图16**：关键帧选择过程分析。经过3轮迭代，采样分布（蓝色曲线）成功聚焦到高似然分数（红色曲线）的视频片段，高效地定位了包含任务相关物体的关键帧。\n\n其他实验结果图表进一步展示了方法的有效性：\n![效率分析](https://arxiv.org/html/2602.07082v1/x17.png)\n> **图17**：不同设备上的推理延迟。MosaicThinker在保持高精度的同时，引入了可接受的额外计算开销。\n\n![对齐方法对比](https://arxiv.org/html/2602.07082v1/x11.png)\n> **图11**：本文的图像匹配对齐方法（右）与传统的ICP点云对齐方法（左）对比。本文方法避免了无关背景干扰，实现了更准确的对齐。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了 **MosaicThinker**，一个**无需重新训练**的推理时计算框架，首次在小型设备端VLM上实现了复杂的跨帧视觉空间推理；2) 设计了**迭代构建稀疏全局语义地图**的方法，通过拓扑感知的跨帧对齐有效整合多视角碎片化空间信息，形成了适合小型VLM理解的统一空间表示；3) 引入了**高效的关键帧选择机制**，通过迭代采样聚焦于信息丰富的视频片段，在保证推理质量的同时优化了计算效率。\n\n论文自身提到的局限性包括：1) 主要处理室内场景，且假设相机运动速度适中、遮挡不频繁；2) 专注于物体间的空间关系推理，而非精确的尺寸和距离测量；3) 其性能依赖于现有的分割、深度估计等感知模型的准确性。\n\n这项工作对后续研究的启示在于：为增强小型模型在复杂任务上的能力提供了一条“**赋能而非替代**”的新路径，即通过精心设计的外部表示和推理时计算来弥补模型内在表征能力的不足。未来工作可探索将语义地图构建方法扩展到更动态或更复杂的室外场景，与其他模态（如惯性传感）结合以提升对齐鲁棒性，并进一步优化关键帧选择和对齐算法的计算效率，以适用于资源更受限的边缘设备。",
      "imageUrls": [
        "https://arxiv.org/html/2602.07082v1/x1.png",
        "https://arxiv.org/html/2602.07082v1/x2.png",
        "https://arxiv.org/html/2602.07082v1/x3.png",
        "https://arxiv.org/html/2602.07082v1/x4.png",
        "https://arxiv.org/html/2602.07082v1/x5.png",
        "https://arxiv.org/html/2602.07082v1/x6.png",
        "https://arxiv.org/html/2602.07082v1/x7.png",
        "https://arxiv.org/html/2602.07082v1/x8.png",
        "https://arxiv.org/html/2602.07082v1/x9.png",
        "https://arxiv.org/html/2602.07082v1/x10.png",
        "https://arxiv.org/html/2602.07082v1/x11.png",
        "https://arxiv.org/html/2602.07082v1/x12.png",
        "https://arxiv.org/html/2602.07082v1/x13.png",
        "https://arxiv.org/html/2602.07082v1/x14.png",
        "https://arxiv.org/html/2602.07082v1/x15.png",
        "https://arxiv.org/html/2602.07082v1/x16.png",
        "https://arxiv.org/html/2602.07082v1/x17.png",
        "https://arxiv.org/html/2602.07082v1/x18.png",
        "https://arxiv.org/html/2602.07082v1/x19.png",
        "https://arxiv.org/html/2602.07082v1/x20.png",
        "https://arxiv.org/html/2602.07082v1/x21.png",
        "https://arxiv.org/html/2602.07082v1/x22.png",
        "https://arxiv.org/html/2602.07082v1/x23.png",
        "https://arxiv.org/html/2602.07082v1/x24.png",
        "https://arxiv.org/html/2602.07082v1/x25.png",
        "https://arxiv.org/html/2602.07082v1/x26.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.06620",
      "title": "Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique",
      "url": "http://arxiv.org/abs/2602.06620",
      "arxivId": "2602.06620",
      "date": "2026-02-06",
      "authors": "Toshiaki Tsuji Team",
      "category": "Manipulation",
      "summary": "本文解决接触式任务中，如何从易得的位置轨迹生成适配特定硬件的精确力命令这一核心问题。提出了力生成模仿学习方法，通过无记忆的力生成模型结合反馈控制机制，将给定位置轨迹映射为力命令。实验表明，该方法确保了反馈控制的稳定性，有效提升了模型对未见位置轨迹的泛化能力，并在真实机器人书写任务中取得了性能改进。",
      "detailedSummary": "## 研究背景与动机\n在接触丰富的机器人任务中，位置轨迹通常易于获取，但合适的力命令往往是未知的。虽然可以考虑使用预训练的基础模型（如视觉-语言-动作模型）来生成力命令，但力控制高度依赖于机器人的特定硬件，这使得此类模型的应用面临挑战。模仿学习虽然能够学习复杂任务，但在接触丰富的任务中，通常只能处理非常静态的运动。基于双边控制的模仿学习能够收集人类操作者的力感知数据，但存在两个关键问题：同时生成位置和力命令会增加模型的维度，使训练更加困难；基于学习的运动生成方法通常比传统的基于模型的方法重现性低。本文旨在解决从给定位置轨迹生成力命令的问题，并通过引入反馈控制来提高鲁棒性和泛化能力。核心思路是提出一个分层的力生成模仿学习框架，将有记忆的上层与无记忆、可反馈控制的下层明确分离，使系统能够为未见过的位置轨迹生成有效的力命令。\n\n## 方法详解\n本文提出的方法是一个分层架构，旨在将有记忆的神经网络与无记忆的神经网络分离，以集成经典反馈控制。\n\n![方法框架](https://arxiv.org/html/2602.06620v1/x2.png)\n\n> **图2**：所提出的用于校正输出误差的分层模型。虚线框内为下层神经网络，其输入为当前状态和来自上层的未来轨迹（含PID校正项），输出为预测的下一状态和动作命令。PID控制器根据上层轨迹与预测状态的误差生成校正量，并反馈到上层轨迹输入中。\n\n整体框架如**图2**所示。上层（对应图中顶部的虚线）负责处理位置轨迹，以智能地预测未来状态。具体而言，上层输出从未来1步到10步的角度𝜽和角速度𝜽˙，并以10步为周期进行更新。输出中未来第10步的[𝜽_{k+10}, 𝜽˙_{k+10}]被传递给下层，并在下一个更新周期前保持恒定；在实际控制的每一步，使用对应的[𝜽, 𝜽˙]值。\n\n下层是一个无记忆的神经网络（图2中虚线框内部分）。其输入包括：10步后的上层轨迹[𝜽_{k+10}, 𝜽˙_{k+10}]、当前状态𝒔_k（关节角度、角速度和扭矩）。其输出是预测的下一状态𝒔̂_{k+1}和下一动作𝒂̂_{k+1}。由于下层仅根据当前状态和期望的未来轨迹进行插值预测，因此不需要内部记忆。这种结构消除了下层因保持内部状态而导致控制困难的隐患。由于𝒂̂_{k+1}包含力（扭矩）命令，该模型能够从位置轨迹生成力命令。\n\n核心创新在于将PID控制集成到该神经网络框架中，以补偿预测误差。具体流程如下：下层NN预测出状态𝒔̂_{k+1}（包含角度𝜽̂_{k+1}和角速度𝜽˙̂_{k+1}）和动作命令𝒂̂_{k+1}。𝒂̂_{k+1}被直接用作机器人的命令。同时，利用上层轨迹中对应的𝜽_{k+1}和𝜽˙_{k+1}，通过公式(1)计算PID控制误差𝒖_{k+1}。该误差经伪微分滤波后得到𝒖˙_{k+1}。随后，将𝒖_{k+1}和𝒖˙_{k+1}分别加到上层的角度𝜽_{k+10}和角速度𝜽˙_{k+10}上，形成校正后的上层角度𝜽^{upper}_{k+10}和角速度𝜽˙^{upper}_{k+10}，并作为新的输入传递给下层NN。这样，模型就能基于包含误差校正项的当前状态和上层轨迹来生成命令和响应值。\n\n与现有方法相比，本文的创新点具体体现在：1）明确提出了一个上层有记忆（处理长期轨迹）、下层无记忆（保证可控性）的分层架构；2）揭示了在反馈控制环内嵌入记忆（如下层使用LSTM）会阻碍稳定性，因此下层采用简单的多层感知机（MLP）；3）通过将PID控制误差反馈到上层轨迹输入中，使模型能够生成考虑轨迹预测误差的、更合适的力命令。\n\n## 实验与结果\n**实验设置**：使用CRANE-X7七自由度机械臂（固定第二关节，实际为6自由度）进行字符书写任务验证。通过双边控制收集了5种不同直线和2种不同圆形轨迹在两种白板高度（0cm和2cm）下的数据，共70组试验，其中56组用于训练，14组用于验证。控制频率为500 Hz。\n\n**对比方法**：\n1.  **直接教学回放**：将直接教学收集的响应值直接作为命令值回放。\n2.  **传统LSTM模型**：使用具有内部状态的LSTM作为下层网络（结构为6层LSTM+1层全连接）。\n3.  **提出的MLP模型**：下层使用无记忆的MLP（7层全连接，每层400维，激活函数为tanh）。\n\n**关键实验结果**：\n实验首先验证了PID增益的影响。**图7、8、9**分别展示了改变比例增益K_p、微分增益K_d和积分增益K_i时，笔尖x坐标轨迹的变化。\n\n![比例增益影响](https://arxiv.org/html/2602.06620v1/figures/Kp_x_comparison.png)\n> **图7**：不同比例增益K_p下笔尖x坐标轨迹。K_p=0时轨迹振荡；增大K_p能抑制振荡，使运动更贴合上层轨迹，但过大的K_p会导致扭矩参考值振荡和增大（见图10a），可能影响稳定性。\n\n![微分增益影响](https://arxiv.org/html/2602.06620v1/figures/Kd_x_comparison.png)\n> **图8**：不同微分增益K_d下笔尖x坐标轨迹。增加K_d未显著改善轨迹，当K_d=0.8或1.0时甚至出现局部振荡，扭矩参考值也出现振荡（图10b），这可能是由于连续时间伪微分计算引入了误差。\n\n![积分增益影响](https://arxiv.org/html/2602.06620v1/figures/Ki_x_comparison.png)\n> **图9**：不同积分增益K_i下笔尖x坐标轨迹。增加K_i在波形顶部减少了偏差，但在底部引入了新的波形峰值。扭矩参考值波形发生变化但未过度增大（图10c）。积分控制对NN参考值生成有影响，但也可能产生负面效果。\n\n![扭矩参考值对比](https://arxiv.org/html/2602.06620v1/x6.png)\n> **图10**：不同PID增益下基关节扭矩参考值对比。(a) K_p增大导致扭矩振荡和增大；(b) K_d增大也引起扭矩振荡；(c) K_i增大改变了扭矩波形但未导致过度增大。\n\n随后，在圆形、“A”、“B”字符书写任务中对比了不同方法。使用交并比（IoU）作为定量评估指标，比较实际绘制形状与上层轨迹形状的重合度。\n\n![模型轨迹对比](https://arxiv.org/html/2602.06620v1/x7.png)\n> **图11**：圆形绘制任务中各模型笔尖x坐标轨迹对比。提出的“MLP w/ PID”模型能最好地跟踪上层轨迹（黑色实线）。“Playback Direct Teaching”虽能跟踪轨迹但书写不准确（图1），“LSTM w/o PID”无法跟踪，“MLP w/o PID”跟踪不足。\n\n**表1** 展示了MLP模型在有/无PID控制下的IoU结果。结果显示，引入PID控制后，在所有测试高度（0cm, 1cm, 2cm）和字符类型上，IoU值均有显著提升，尤其是在1cm和2cm高度时提升更为明显。例如，在1cm高度书写“Character ABCD”时，IoU从0.165提升至0.270。\n\n![字符绘制结果对比](https://arxiv.org/html/2602.06620v1/x8.png)\n> **图12**：白板高度为1cm时，MLP无PID控制与有PID控制绘制的字符对比。可以直观看到，引入PID控制后（右列），字符书写的准确性和完整性得到明显改善。\n\n**消融实验总结**：实验通过对比LSTM和MLP模型，验证了下层网络无记忆对集成反馈控制的重要性（LSTM引入PID后性能未改善）。通过对比MLP在有/无PID控制下的表现，证明了引入PID反馈对于提高轨迹跟踪精度和任务性能的关键作用。PID各增益的调优实验则表明，适当的比例控制能有效改善跟踪，但微分和积分控制需谨慎使用，且当前的连续时间计算方式可能引入误差。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  提出了一个分层的力生成模仿学习框架，明确将有记忆的上层（负责长期轨迹预测）与无记忆、可反馈控制的下层（负责生成状态和力命令）分离开来。\n2.  该框架能够处理具有非平凡或定义不清控制目标的接触式操作任务（如书写），这类任务难以用传统的阻抗或混合控制解决，同时仍能通过经典反馈控制确保稳定性。\n3.  揭示了在反馈控制环内嵌入记忆会阻碍稳定性，并通过实验证明，采用时间尺度分离（上层慢速、有记忆，下层快速、无记忆）可以使基于学习的力生成同时实现鲁棒性和稳定性。\n\n论文自身提到的局限性主要在于PID控制的实现细节。文中指出，微分和积分计算采用了连续时间方法，这可能引入计算误差并对神经网络产生负面影响，未来需要引入离散控制技术来考虑这些计算误差。\n\n本文对后续研究的启示在于：将学习与经典控制结合时，模型的可控性是需要重点考虑的设计因素。明确分离系统的“记忆”部分和“反应”部分，并确保直接与底层控制交互的部分是简单、无记忆且易于分析的，可能是一种有效的架构设计原则。此外，该方法展示了如何利用易于获取的位置轨迹（可来自直接教学、VLA模型等）来生成硬件适配的精细力命令，为将高层智能规划与底层柔顺执行相结合提供了一条可行路径。",
      "imageUrls": [
        "https://arxiv.org/html/2602.06620v1/x1.png",
        "https://arxiv.org/html/2602.06620v1/x2.png",
        "https://arxiv.org/html/2602.06620v1/figures/task_env.png",
        "https://arxiv.org/html/2602.06620v1/x3.png",
        "https://arxiv.org/html/2602.06620v1/x4.png",
        "https://arxiv.org/html/2602.06620v1/x5.png",
        "https://arxiv.org/html/2602.06620v1/figures/Kp_x_comparison.png",
        "https://arxiv.org/html/2602.06620v1/figures/Kd_x_comparison.png",
        "https://arxiv.org/html/2602.06620v1/figures/Ki_x_comparison.png",
        "https://arxiv.org/html/2602.06620v1/x6.png",
        "https://arxiv.org/html/2602.06620v1/x7.png",
        "https://arxiv.org/html/2602.06620v1/x8.png",
        "https://arxiv.org/html/2602.06620v1/figures/compare_ordinalmodel.png",
        "https://arxiv.org/html/2602.06620v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.06512",
      "title": "Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.06512",
      "arxivId": "2602.06512",
      "date": "2026-02-06",
      "authors": "Heng Tao Shen Team",
      "category": "Manipulation",
      "summary": "这篇论文针对机器人模仿学习中训练数据呈现长尾分布的核心问题，即模型在数据丰富的头部任务上表现良好，但在数据稀缺的尾部任务上泛化能力差。研究发现，传统长尾学习策略（如重采样）对提升尾部任务性能效果有限，其根本原因是数据稀缺损害了策略的空间推理能力。为此，作者提出了“接近阶段增强”（APA）方法，通过将头部任务的知识迁移至尾部任务，无需外部演示数据。实验表明，APA方法在模拟和真实机器人操作任务中均能有效提升尾部任务的性能。",
      "detailedSummary": "## 研究背景与动机\n当前，通过大规模模仿学习训练通用机器人策略已成为主流范式，其通过在大量多样化的人类演示数据上训练，使机器人能够根据自然语言指令执行广泛的操作任务。然而，一个关键且常被忽视的挑战阻碍了这些模型的实际应用：演示数据天然存在的长尾分布。在大规模数据集中，少数常见的“头部”任务（如“拿起碗/盘子”）拥有大量演示，而绝大多数“尾部”任务（如“将酒瓶放在架子上”）仅有少量示例。在这种不平衡数据上训练的策略通常在尾部任务上表现不可靠。本文旨在解决这一核心痛点。\n\n现有的计算机视觉领域的长尾学习策略（如重采样、数据增强）在机器人策略学习中效果不佳。重采样仅复制现有数据，无法引入泛化所需的多样性；而如mixup等增强技术对状态和动作进行线性插值，忽略了底层动力学，常导致运动学上不可行或物理无效的轨迹。本文的核心问题是：在长尾演示上训练的策略性能下降的根本原因是什么？以及如何在不依赖外部演示数据的情况下解决此问题？\n\n通过深入分析，本文发现策略在尾部任务上失败的主要原因是其**空间推理能力**的退化。由于训练数据稀缺，模型无法学习尾部任务所需的精确空间关系，尽管它已从头部任务中掌握了大致概念。基于此关键洞察，本文提出了**Approaching-Phase Augmentation (APA)**，一种简单而有效的方案，通过利用头部任务的演示知识来增强尾部任务，从而提升策略在尾部任务上的空间推理能力，且无需外部数据。\n\n## 方法详解\n本文提出的Approaching-Phase Augmentation (APA)方法旨在通过从数据丰富的头部任务向数据稀缺的尾部任务转移知识，生成新的高质量训练样本。其整体流程分为三个步骤，如下图所示。\n\n![方法框架](https://arxiv.org/html/2602.06512v1/x3.png)\n> **图3**：Approaching-Phase Augmentation (APA)流程概览。包含三个步骤：(1) 头部任务轨迹分割，从头部任务中分离出目标接近阶段；(2) 尾部到头部对象嫁接，使用尾部任务对象创建增强轨迹；(3) 指令格式化与协同训练，格式化对应的语言指令，然后在组合数据集上训练策略。\n\n**整体流程：**\n1.  **输入**：原始的、不平衡的长尾演示数据集（包含头部和尾部任务的轨迹、图像和指令）。\n2.  **处理**：执行APA的三个步骤，生成仅包含目标接近阶段的增强轨迹及其对应指令。\n3.  **输出**：一个组合数据集，包含原始长尾数据以及新生成的增强数据，用于协同训练策略。\n\n**核心模块与技术细节：**\n1.  **头部任务轨迹分割**：从数据丰富的头部任务中随机选取成功执行的演示轨迹。通过监测机器人的本体感知（例如夹爪的打开/闭合状态），识别出机械臂正在接近目标的轨迹片段。这一步骤收集了一系列成功的“目标接近阶段”演示。\n2.  **尾部到头部对象嫁接**：此步骤通过将头部任务的轨迹片段中的原始对象替换为尾部任务的对象，来生成新的训练演示。在模拟的LIBERO基准测试中，这涉及直接替换对象资源。新对象的初始位置继承自源轨迹的原始对象，而其旋转方向则由尾部任务的目标指定。随后重新渲染场景以生成新的视觉数据。对于真实世界应用，可以通过YOLOv8识别对象并使用图像修复技术进行替换。这种重组操作使得目标接近阶段的训练数据多样化。\n3.  **指令格式化与协同训练**：为确保语言一致性，使用一对对应的模板来分别格式化原始轨迹和增强轨迹的语言指令。对于新生成的、仅包含目标接近阶段的增强轨迹，使用模板“approach the [target_object]”生成指令。为了在整个数据集中创建一致的两阶段格式，将所有原始演示的指令修改为“approach the [target_object] then [verb_phrase_1] and [verb_phrase_2]”。随后，策略在此组合的、语言一致的数据集上进行训练。\n\n**创新点：**\n与现有方法相比，APA的创新性体现在：\n*   **针对性**：聚焦于增强被识别为关键瓶颈的“目标接近阶段”，避免了生成完整、物理上合理的操作序列的复杂性。\n*   **高效性**：利用头部任务中已有的成功接近轨迹，通过对象替换快速生成高质量尾部任务数据，无需外部演示。\n*   **解耦性**：由于只涉及接近阶段，对替换对象的物理属性（如尺寸、质量）没有严格要求，这简化了数据生成过程。\n\n## 实验与结果\n**实验设置：**\n*   **基准/数据集**：在仿真实验中，使用基于LIBERO环境构建的**LIBERO-Core-LT**长尾数据集（包含10个任务）。在真实世界实验中，构建了**Real-World-LT**数据集（包含6个任务）。\n*   **实验平台**：仿真实验在LIBERO环境中进行；真实世界实验使用AGILEX PIPER 6-DOF机械臂。\n*   **基线模型**：仿真实验使用在LIBERO-90上预训练的miniVLA模型；真实世界实验使用在OXE真实世界数据上预训练的π0模型。\n*   **对比方法**：主要对比了直接在原始长尾数据上微调的**Baseline**与使用APA增强数据微调的**APA (Ours)**。此外，还评估了传统的重采样方法。\n\n**关键实验结果：**\n1.  **传统重采样方法无效**：如表II所示，采用不同重采样策略（q=0.75, 0.5, 0.25）后，策略在尾部任务上的成功率仅有个别微小提升（基线26.5%，最佳重采样27.1%），表明单纯增加尾部数据的曝光量无效。\n2.  **失败根因分析**：通过阶段式失败分析（表III）发现，与在全量数据上训练的模型相比，在长尾数据上训练的模型，其在尾部任务的“目标接近阶段”的失败相对风险高达**400.89%**，远高于“执行阶段”的164.34%。这证实了数据稀缺直接损害了策略的空间推理能力。\n3.  **APA方法有效性（仿真）**：\n    ![仿真结果](https://arxiv.org/html/2602.06512v1/x4.png)\n    > **图4**：APA方法在LIBERO-Core-LT数据集上的有效性。APA将平均成功率从基线的26.5%提升至**36.1%**，相对提升**36.2%**。不仅提升了尾部任务性能，头部任务也有所改善。\n4.  **APA方法有效性（真实世界）**：\n    ![真实世界结果](https://arxiv.org/html/2602.06512v1/x5.png)\n    > **图5**：APA方法在真实世界长尾任务上的有效性。APA将平均成功率从基线的39.1%提升至**54.1%**，相对提升**38.4%**，再次验证了其跨领域的有效性。\n5.  **消融实验**：\n    *   **组件分析（表IV）**：单独使用指令格式化或轨迹增强均无显著效果（成功率约26.0%-26.9%），只有两者结合（完整APA）才能实现性能飞跃（36.1%），说明视觉数据与语言指令的对应关系至关重要。\n    *   **增强数据量分析（表V）**：增加增强演示数量有益，但收益非单调。当每个任务的增强演示从0增加到9时，成功率先升至36.1%（6个时），后降至32.0%（9个时）。作者推测过量的、仅包含接近阶段的增强数据可能会扭曲训练数据分布。\n\n## 总结与启发\n**核心贡献：**\n1.  **问题诊断**：构建了基于LIBERO的长尾模仿学习基准，并通过深入分析首次揭示，长尾数据中尾部任务性能下降的根源在于策略**空间推理能力**的退化，尤其是在目标接近阶段。\n2.  **方法创新**：提出了**Approaching-Phase Augmentation (APA)**方法，这是一种简单、自包含的方案，通过将尾部任务对象“嫁接”到头部任务的成功接近轨迹上，生成高质量的增强数据，从而将知识从头部任务转移至尾部任务，且无需外部演示。\n3.  **全面验证**：在仿真和真实世界的大量多样化操作任务上进行了广泛实验，证实了APA能显著提升尾部任务性能，且对头部任务无害，甚至有益。\n\n**局限性：**\n论文自身提到，APA生成的增强数据仅包含目标接近阶段。消融实验表明，过度增加此类增强数据可能导致性能下降，因为这可能使训练分布偏离完整的任务轨迹。\n\n**后续研究启示：**\n1.  **阶段化学习**：本文强调了将操作任务分解为不同阶段（如接近、执行）进行分析和干预的价值，这为设计更精细的机器人学习算法提供了新思路。\n2.  **针对性数据增强**：针对已识别的特定能力瓶颈（如空间推理）进行数据增强，比通用的、可能违反物理规律的增强方法更为有效和可行。\n3.  **跨任务知识迁移**：利用数据丰富任务中的结构化知识（如成功的运动基元）来辅助数据稀缺任务的学习，是解决机器人学习长尾问题的一个有前景的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.06512v1/x1.png",
        "https://arxiv.org/html/2602.06512v1/x2.png",
        "https://arxiv.org/html/2602.06512v1/x3.png",
        "https://arxiv.org/html/2602.06512v1/x4.png",
        "https://arxiv.org/html/2602.06512v1/x5.png",
        "https://arxiv.org/html/2602.06512v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.06508",
      "title": "World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy",
      "url": "http://arxiv.org/abs/2602.06508",
      "arxivId": "2602.06508",
      "date": "2026-02-06",
      "authors": "Mike Zheng Shou Team",
      "category": "Manipulation",
      "summary": "本文提出World-VLA-Loop闭环框架，旨在解决现有视频世界模型在机器人学习中动作跟随精度不足的问题。方法核心包括：1）状态感知视频世界模型，联合预测未来观测与奖励信号，充当高保真交互模拟器；2）引入SANS数据集，利用近成功轨迹提升世界模型的动作-结果对齐；3）构建世界模型与视觉-语言-动作（VLA）策略的协同进化闭环，利用策略失败经验迭代优化模型。实验表明，经过两轮联合优化，真实世界策略成功率提升36.7%，显著减少了物理交互需求。",
      "detailedSummary": "## 研究背景与动机\n当前，利用世界模型作为虚拟环境来克服真实世界强化学习（RL）挑战的方法主要分为三类：1）依赖于手动资产创建和物理引擎建模的手工数字孪生，但其往往缺乏真实感和物理保真度；2）利用几何3D方法重建场景的3D重建方法，但其难以泛化到多样环境且很少支持随机探索；3）基于动作条件的视频世界模型，其利用预训练先验实现更好的泛化，但存在**动作跟随不精确**和**奖励信号不可靠**的关键局限。这些模型对动作条件的保真度差，导致预测轨迹与实际执行结果偏离，且常常依赖外部VLM或启发式代理奖励，缺乏稳定RL所需的精度。\n\n本文针对现有视频世界模型动作跟随不精确和奖励信号不可靠的痛点，提出了一个**共同进化**的新视角，旨在通过联合优化世界模型和视觉-语言-动作（VLA）策略来迭代提升两者的性能和基础。核心思路是建立一个闭环框架，首先利用世界模型为VLA策略提供RL后训练环境，然后将更新后策略产生的失败轨迹反馈回来以细化世界模型，从而形成相互促进的优化循环。\n\n## 方法详解\n整体框架包含四个阶段，形成一个闭环的联合优化流程：1）通过手动遥操作和策略 rollout 构建包含成功与接近成功轨迹的SANS数据集；2）在SANS数据集上，使用联合奖励和视频监督预训练动作条件世界模型；3）在世界模型内执行VLA策略 rollout 以进行GRPO优化；4）部署优化后的策略收集新的失败和成功数据，用于进一步扩充SANS数据集。\n\n![方法整体框架](https://arxiv.org/html/2602.06508v1/x3.png)\n> **图3**：我们提出的框架完整流程。该过程包含四个阶段：(1) 通过手动遥操作和策略 rollout 构建SANS数据集；(2) 使用联合奖励和视频监督在SANS上预训练动作条件世界模型；(3) 在世界模型内执行VLA策略 rollout 以进行GRPO优化；(4) 部署优化后的策略收集新数据以进一步扩充SANS。这个循环实现了世界模型和VLA策略的联合优化，迭代提升两者性能。\n\n**核心模块一：SANS（成功与接近成功）数据集**。现有开源机器人数据集主要关注成功轨迹，限制了训练鲁棒世界模型所需的多样性。本文提出的SANS数据集不仅包含成功轨迹，还特别纳入了**接近成功**的轨迹（即因末端执行器定位轻微不准确而失败）。这类数据至关重要，因为它迫使世界模型关注空间动态的细粒度差异，并确保虚拟环境能更准确地反映策略 rollout 中遇到的实际失败模式。数据在ManiSkill、LIBERO仿真环境和真实世界机器人设置中收集，总计约35k个视频-动作对用于世界模型预训练。\n\n**核心模块二：状态感知视频世界模拟器**。该方法在Cosmos-Predict 2基础上构建世界模型。给定初始的h帧观测和后续T步机器人动作（6自由度末端执行器姿态加夹持器开合状态），模型自回归地合成未来T步的执行帧。为启用无需外部奖励模型的奖励预测，模型在扩散变换器（DiT）主干生成的去噪潜变量后添加了一个**奖励预测头**（轻量级MLP），用于预测标量奖励。训练时，采用联合损失函数：$\\mathcal{L}=\\mathcal{L}_{flow}+\\lambda\\sum_{t=1}^{T}\\|\\hat{r}_{t}-r_{t}\\|^{2}$，其中$\\mathcal{L}_{flow}$为流匹配损失，$\\lambda$根据EDM框架随噪声水平调整。这种设计带来了两个关键优势：一是提供了与世界模型内部视觉结果内在对齐的可靠奖励生成机制；二是奖励头与视频模型的联合训练鼓励生成器更好地区分不同动作条件下的成功与失败结果，从而得到更准确的未来视频预测。\n\n**创新点**：1）提出了世界模型与VLA策略学习的**闭环共同进化范式**；2）通过引入**SANS数据集**和**联合奖励-视频监督**，设计了能实现更精确动作跟随的状态感知世界模型；3）整个框架使得VLA策略的RL后训练可以完全在虚拟环境中进行，极大减少了对昂贵物理交互的依赖。\n\n## 实验与结果\n实验在LIBERO基准（仿真任务）和自建的真实世界实验室设置中进行，使用Franka机械臂和固定位置的RealSense D435相机。评估分为世界模型生成性能评估和其作为模拟器用于VLA后训练的效果评估。\n\n**世界模型评估**：首先评估生成质量。如表1所示，世界模型在仿真和真实场景均实现了高质量的生成结果（平均SSIM 0.91，PSNR 28.09，LPIPS 0.045）。\n表1：视频生成性能。↑表示越高越好；↓表示越低越好。\n| 场景 | SSIM ↑ | PSNR ↑ | LPIPS ↓ | MSE ↓ |\n| :--- | :--- | :--- | :--- | :--- |\n| LIBERO | 0.90 | 26.57 | 0.031 | 0.0024 |\n| 真实世界 | 0.91 | 29.61 | 0.059 | 0.0019 |\n| **平均** | **0.91** | **28.09** | **0.045** | **0.0022** |\n\n其次评估生成准确性，即预测结果与真实因果后果的对齐。如表2所示，该方法在LIBERO和真实世界场景中有效区分了成功与失败轨迹，**视觉对齐**和**奖励对齐**平均准确率均超过80%，且两者高度一致，证明了内部奖励预测头的可靠性。\n表2：特定任务的结果对齐性能。评估每个任务验证集中的20个样本，报告预测成功/失败与真实情况匹配的样本百分比。\n| 指标 | LIBERO-对象 | LIBERO-目标 | LIBERO-空间 | 真实世界 |\n| :--- | :--- | :--- | :--- | :--- |\n| | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 |\n| **视觉对齐** | 85% | 95% | 90% | 75% | 85% | 95% | 90% |\n| **奖励对齐** | 75% | 90% | 85% | 75% | 90% | 95% | 95% |\n\n**VLA策略后训练结果**：如表3所示，经过在世界模型中的RL后训练，OpenVLA-OFT策略在LIBERO套件上的平均成功率提升了12.7%，在真实世界任务上提升了23.4%。\n表3：RL训练前后OpenVLA-OFT的成功率。LIBERO套件的成功率基于500次 rollout 计算，真实世界实验基于30次物理 rollout 计算。\n| 模型 | LIBERO-对象 | LIBERO-目标 | LIBERO-空间 | 真实世界 |\n| :--- | :--- | :--- | :--- | :--- |\n| | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 | 任务2 | 任务1 |\n| SFT 基础 | 73.9% | 73.9% | 91.9% | 86.1% | 83.9% | 87.9% | 13.3% |\n| RL后训练 (Ours) | 97.9% | 91.9% | 100% | 96.2% | 93.9% | 94.0% | 36.7% |\n| **Δ vs SFT** | **+24.0%** | **+18.0%** | **+8.1%** | **+10.1%** | **+10.0%** | **+6.1%** | **+23.4%** |\n\n![训练过程成功率曲线](https://arxiv.org/html/2602.06508v1/x4.png)\n> **图4**：沿World-VLA-Loop RL训练步数的成功率提升情况。展示了在LIBERO各套件及真实世界任务中，策略成功率随训练迭代逐步提高。\n\n**迭代精炼效果**：如图1(b)所示，通过将RL优化后策略产生的轨迹反馈用于扩充SANS数据集并精炼世界模型，可以进行迭代优化。真实世界任务中，基础SFT策略成功率为13.3%，第一次RL阶段后提升至36.7%，第二次迭代后最终达到50.0%，验证了闭环框架实现持续改进的有效性。\n![迭代优化效果](https://arxiv.org/html/2602.06508v1/x1.png)\n> **图1(b)**：经过与世界模型的两轮联合优化后，真实世界策略成功率提升了36.7%。\n\n**消融实验**：如表4所示，消融实验验证了关键设计的选择。移除近成功数据或奖励预测头都会导致视觉对齐性能大幅下降（约30%）。同时，与使用外部VLM（Qwen3-VL）作为成功-失败判断器相比，本文的内部奖励预测头准确率显著更高，证明了其作为RL奖励函数的可靠性。\n表4：消融实验结果。研究了不同设计选择对世界模型预测准确性的影响。\n| 指标 | LIBERO-对象任务1 | LIBERO-对象任务2 |\n| :--- | :--- | :--- |\n| 视觉对齐 (无近成功数据) | 60% | 65% |\n| 视觉对齐 (无奖励预测头) | 60% | 70% |\n| 奖励对齐 (Qwen3-VL) | 50% | 55% |\n| **视觉对齐 (本文)** | **85%** | **95%** |\n| **奖励对齐 (本文)** | **75%** | **90%** |\n\n## 总结与启发\n本文的核心贡献在于：1）提出了**World-VLA-Loop**，一个通过新颖迭代精炼范式在世界模型与VLA策略学习之间建立共同进化循环的闭环框架；2）开发了**状态感知世界模型**，通过结合扩散潜变量上的联合奖励-视频监督以及精心策划的近成功轨迹，实现了更优的动作跟随精度；3）引入了**SANS数据集**，专门包含接近成功的轨迹，以提升世界模型的动作-结果对齐能力。\n\n论文自身提到的局限性在于，对于需要生成超过200帧视频的长视野任务（如LIBERO-100），当前的自回归视频模型常遭受严重的质量漂移问题，因此这部分探索被留待未来工作。\n\n本文对后续研究的启示在于：1）**迭代数据增强**是提升世界模型保真度和策略性能的有效途径，形成了良性的自我改进循环；2）**神经模拟器**（视频世界模型）在提供高保真视觉观察和内在对齐奖励方面展现出巨大潜力，为在虚拟环境中高效进行机器人策略学习提供了新方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.06508v1/x1.png",
        "https://arxiv.org/html/2602.06508v1/x2.png",
        "https://arxiv.org/html/2602.06508v1/x3.png",
        "https://arxiv.org/html/2602.06508v1/x4.png",
        "https://arxiv.org/html/2602.06508v1/x5.png",
        "https://arxiv.org/html/2602.06508v1/x6.png",
        "https://arxiv.org/html/2602.06508v1/x7.png",
        "https://arxiv.org/html/2602.06508v1/x8.png",
        "https://arxiv.org/html/2602.06508v1/x9.png",
        "https://arxiv.org/html/2602.06508v1/x10.png",
        "https://arxiv.org/html/2602.06508v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.05468",
      "title": "TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation",
      "url": "http://arxiv.org/abs/2602.05468",
      "arxivId": "2602.05468",
      "date": "2026-02-05",
      "authors": "Shigeki Sugano Team",
      "category": "Manipulation",
      "summary": "本文针对机器人灵巧抓取操作中难以区分自接触与外部物体接触触觉信号的核心问题，提出TaSA框架。该方法采用两阶段深度预测学习：第一阶段显式学习自接触动力学模型；第二阶段将该模型整合至动作学习中，以衰减自接触信号并突出外部接触。在铅笔芯、硬币、回形针等多种精细插入任务上的实验表明，基于TaSA训练的策略取得了显著高于基线方法的成功率。",
      "detailedSummary": "## 研究背景与动机\n当前机器人灵巧操作领域的主流方法通常依赖高分辨率触觉传感，但普遍面临一个关键局限：在多指操作过程中，手指间或手指与手掌的自接触（self-touch）产生的触觉信号会与物体接触（object contact）信号混淆。大多数方法选择通过约束运动来避免或完全忽略自接触信息，这虽然简化了控制，但限制了在自接触不可避免的真实场景中的泛化能力。人类通过感官衰减（Sensory Attenuation）机制克服了这一挑战，即神经系统能够预测自身动作产生的感觉后果并对其降权，从而使新异的物体刺激凸显出来。\n\n本文针对机器人灵巧操作中难以区分自接触与物体接触这一具体痛点，提出从感官衰减的生物原理中汲取灵感的新视角。核心思路是提出一个名为TaSA的两阶段深度预测学习框架：第一阶段明确学习机器人自身的自接触动力学模型；第二阶段将此模型整合到动作学习中，以衰减可预测的自接触信号，从而增强对物体接触的感知与控制。\n\n## 方法详解\nTaSA的整体框架是一个明确分为两个阶段的流程。第一阶段为自触学习阶段，输入为当前关节位置和上一时刻的目标关节位置，输出是预测的自接触触觉信号。第二阶段为动作学习阶段，输入融合了关节位置、预测的自触信号以及原始触觉读数，通过时序模型处理后，输出预测的下一时刻关节状态、动作目标以及原始触觉反馈。\n\n![方法框架](https://arxiv.org/html/2602.05468v1/images/TaSA.png)\n> **图1**：TaSA两阶段深度预测学习方法框架图。左侧为自触学习阶段，学习从关节状态到自接触触觉的映射；右侧为动作学习阶段，整合预测的自触信号与原始触觉进行动作学习。\n\n核心模块包括：\n1.  **自触学习模块**：采用一个全连接网络（FCN）。输入是串联的当前关节位置 $q_t$ 和上一时刻指令关节位置 $q_{t-1}^{des}$（共16维，对应拇指和食指各4个主动关节）。网络经过编码器（16→64→128维）和解码器（128→64→188维），使用GELU激活函数和0.2的Dropout率。输出是预测的拇指和食指指尖的自接触触觉信号 $\\hat{s}_t^{thb}$ 和 $\\hat{s}_t^{idx}$（各90维，对应30个三轴力传感器），以及一个辅助关节状态 $\\hat{q}_t$（8维）用于一致性检查。该模块通过最小化预测触觉与真实自接触触觉之间的误差进行训练，训练数据仅包含自由空间或自接触运动，不含物体。\n\n2.  **动作学习模块**：采用LSTM单元作为时序模型。输入向量 $x_t = [q_t, \\hat{s}_t, T_t]$，即当前关节位置、冻结的自触FCN预测的自触信号、原始触觉传感器输出 $T_t$。LSTM的隐藏维度为100。解码器将LSTM输出映射为下一时刻预测的关节位置 $\\hat{q}_{t+1}$、指令关节位置 $\\hat{q}_{t}^{des}$ 和原始触觉反馈 $\\hat{T}_{t+1}$（总输出196维）。关键创新在于，LSTM预测出的未来姿态 $\\hat{q}_{t+1}$ 和 $\\hat{q}_{t}^{des}$ 会被再次送入**冻结的**自触FCN，以生成下一步的自触预测 $\\hat{s}_{t+1}$，形成一个可微分的自接触前向模型，确保网络在重构原始触觉输出的同时，能持续区分外部与自诱导的接触。\n\n![自触学习阶段](https://arxiv.org/html/2602.05468v1/images/selfjointpos.png)\n> **图2**：自触学习阶段示意图。输入为关节位置，通过FCN网络预测由纯自接触产生的触觉状态。\n\n![模型对比](https://arxiv.org/html/2602.05468v1/images/models.png)\n> **图3**：所提方法（st-rnn，使用原始触觉RT+预测自触Self）与基线方法（t-rnn，仅使用原始触觉RT）的对比示意图。\n\n与现有方法相比，TaSA的核心创新在于明确建模并利用自接触信号，而非忽略或将其视为噪声。通过两阶段设计，首先学习一个专门的自接触动力学模型，然后在动作学习阶段将其作为感官衰减器，使网络能够学习到更清晰的“原因-效应”关系，从而在接触丰富的操作中更好地区分自接触与物体接触。\n\n## 实验与结果\n实验平台采用Wonik Robotics的Allegro四指机械手（16自由度），并在拇指和食指指尖安装了XELA Robotics的uSkin高分辨率三轴触觉传感器（每个指尖30个测力单元）。通过基于Dynamixel的遥操作系统收集人类演示数据。评估使用了三个精密插入任务作为benchmark：将纸夹固定在纸上、将硬币插入窄槽、将铅笔芯插入自动铅笔。\n\n对比的基线方法是仅使用原始触觉（RT）输入的动作学习模型（t-rnn）。本文方法（st-rnn）则使用原始触觉加预测自触信号（RT+Self）。\n\n**关键实验结果：**\n-   **自触预测准确性**：在测试片段上，预测的自触信号与原始触觉轨迹高度吻合（拇指相关性 $r\\approx0.96$，食指 $r\\approx0.98$）。误差在持续的手指接触期间接近零，仅在接触开始/结束等快速转变时出现尖峰，这有助于控制器监测意外变化。\n    ![拇指自触预测](https://arxiv.org/html/2602.05468v1/images/selfthumb.png)\n    > **图7a**：拇指指尖的自触预测结果。显示原始触觉（RT）、预测自触（Self）及误差（Error）。预测在持续接触段重叠良好，误差较小。\n\n    ![食指自触预测](https://arxiv.org/html/2602.05468v1/images/selfindex.png)\n    > **图8**：食指指尖的自触预测结果。预测能紧密跟随原始触觉轨迹，误差通道在大部分时间保持平坦。\n\n-   **纸夹固定任务**：在60次试验中，仅用RT的基线成功率为70%（42/60），而RT+Self的TaSA方法成功率达到95%（57/60）。在未参与训练的“中间”位置测试中，TaSA对两种尺寸的纸夹均取得了100%的成功率。\n-   **硬币插入任务**：在90次试验中，基线成功率为68%（61/90），TaSA成功率为92%（83/90）。在未见的“中间”槽位测试中，TaSA对所有三种硬币均达到100%成功率。\n-   **铅笔芯插入任务**：这是最具挑战性的任务。在250次试验中，基线成功率仅为26%（66/250），TaSA将成功率提升至58%（146/250）。较粗的笔芯（1.3mm, 2.0mm）因触觉信号更强，成功率高于细笔芯（0.7mm, 0.9mm）。\n\n**消融实验与特征分析**：通过PCA对任务初始时刻的触觉特征空间进行分析，对比Case A（仅RT）和Case B（RT+Self）。\n![特征空间对比](https://arxiv.org/html/2602.05468v1/images/clip_pca_caseA_initial_raw.png)\n> **图9**：纸夹任务Case A（仅RT）的PCA特征空间。不同放置位置的簇因自接触剪切力的影响而存在重叠。\n\n![特征空间对比](https://arxiv.org/html/2602.05468v1/images/clip_pca_caseB_initial_raw.png)\n> **图12**：纸夹任务Case B（RT+Self）的PCA特征空间。加入自触预测后，由自接触引起的方差降低，不同放置位置的簇间距离增加，分离更清晰。\n\n分析表明，加入预测的自触信号（Case B）后，所有任务的特征空间中，由自接触引起的类内方差减小，不同类别（如不同放置位置、不同硬币面值）的簇间分离度提高。这直观解释了TaSA为何能取得更高的任务成功率：它学习到了更能区分任务相关状态的特征表示。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了一个受感官衰减启发、明确分为自接触动力学学习和动作学习两阶段的深度预测学习框架（TaSA）；2）通过三个精密的插入任务实验，实证了建模并利用自接触进行感觉衰减能显著提升机器人灵巧操作的成功率和泛化能力，特别是在需要区分细微触觉信号的场景中。\n\n论文自身提到的局限性包括：在最具挑战性的铅笔芯插入任务中，即使使用了TaSA，整体成功率（58%）仍有较大提升空间，尤其是对于非常薄（0.7mm, 0.9mm）的笔芯，其微弱的触觉信号仍然难以可靠检测。\n\n这项工作对后续研究的启示是：将生物感知原理（如感官衰减）形式化并整合到机器人学习框架中，是解决接触密集型操作难题的有效途径。未来的工作可以探索如何使自接触模型在线适应传感器漂移或手部形态变化，以及将该框架扩展到更复杂的多指协同操作和动态物体操作任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.05468v1/images/TaSA.png",
        "https://arxiv.org/html/2602.05468v1/images/selfjointpos.png",
        "https://arxiv.org/html/2602.05468v1/images/models.png",
        "https://arxiv.org/html/2602.05468v1/images/tasks.png",
        "https://arxiv.org/html/2602.05468v1/images/setup.png",
        "https://arxiv.org/html/2602.05468v1/images/positions.png",
        "https://arxiv.org/html/2602.05468v1/images/selfthumb.png",
        "https://arxiv.org/html/2602.05468v1/images/selfindex.png",
        "https://arxiv.org/html/2602.05468v1/images/clip_pca_caseA_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/coins_pca_caseA_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/lead_pca_caseA_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/clip_pca_caseB_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/coins_pca_caseB_initial_raw.png",
        "https://arxiv.org/html/2602.05468v1/images/lead_pca_caseB_initial_raw.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.05233",
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "http://arxiv.org/abs/2602.05233",
      "arxivId": "2602.05233",
      "date": "2026-02-05",
      "authors": "Baining Guo Team",
      "category": "Manipulation",
      "summary": "本文针对移动操作中视觉-语言-动作模型验证困难的问题，提出“仿真优先”的验证框架。核心是构建了MobileManiBench大规模基准，其关键技术是基于NVIDIA Isaac Sim与强化学习，自动生成包含丰富标注的多样化操作轨迹。该基准包含2种移动机器人、630个物体、5种核心技能，在100个场景中生成30万条轨迹，为系统化研究机器人构型、感知模态与策略架构提供了可控、可扩展的测试平台，并已用于代表性VLA模型的基准测试。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型在机器人操作领域取得了显著进展，但其成功严重依赖于大规模、通过遥操作收集的数据集，这些数据主要集中在静态的桌面场景，且物体种类有限。这种数据收集范式存在关键局限性：任何硬件配置的修改（如增加新传感器、扩展到移动操作或更换为灵巧手）都需要从头开始重新收集数据，成本高昂且效率低下；同时，将遥操作扩展到灵巧手或移动平台非常繁琐，阻碍了模型的快速开发和迭代。\n\n本文针对上述痛点，提出了一个“仿真优先”的新视角，主张在收集真实世界数据之前，先在仿真环境中进行数据生成和模型验证。核心思路是：利用高保真仿真器（NVIDIA Isaac Sim）和强化学习，自动化生成大规模、多模态的移动操作轨迹数据集（MobileManiBench），从而为灵活配置机器人、传感器和模型架构提供一个可控、可扩展的测试平台，加速VLA模型的创新和数据效率研究。\n\n## 方法详解\nMobileManiBench的构建分为两个主要阶段：1) 训练通用状态强化学习策略（MobileManiRL）以掌握技能；2) 利用该策略在多样化场景中生成大规模轨迹数据集（MobileManiDataset）。基于此数据集，进一步训练通用的视觉-语言-动作模型（MobileManiVLA）。\n\n![方法框架](https://arxiv.org/html/2602.05233v1/figures/teaser.png)\n> **图1**：MobileManiBench整体概览。包含两个移动机器人平台（G1平行夹爪机器人和XHand灵巧手机器人）、20个类别的630个物体、5种移动操作技能。通过训练通用的MobileManiRL策略，在100个真实感场景中生成包含语言指令、多视角RGB-深度-分割图像、同步状态与动作的30万条轨迹，构成MobileManiDataset。\n\n**第一阶段：MobileManiRL 训练**\n目标是针对每个“机器人-物体-技能”组合，训练一个能成功完成任务的强化学习策略。其创新在于采用**基于关键点位移**的通用策略设计：为每个任务定义机器人夹爪/手点（蓝色）、物体抓取点（红色）和目标点（绿色），如图2所示。策略的目标是驱使夹爪/手点抵达物体抓取点，并将其运送至目标点。\n\n![关键点定义](https://arxiv.org/html/2602.05233v1/figures/points.png)\n> **图2**：不同任务中机器人夹爪/手点（蓝）、物体抓取点（红）和目标点（绿）的定义示意图。\n\n策略网络是一个4层MLP，输入包括时间编码、物体状态（抓取点和目标点状态）、机器人本体感知、机器人-物体距离以及上一时刻动作。奖励函数设计为多阶段形式：在未抓取时，奖励引导夹爪接近物体抓取点；一旦抓取成功，奖励则鼓励将物体移至目标点并最终完成任务。所有策略在简化的地面或桌面场景（图3左）中进行训练，通过随机化机器人初始位姿增强鲁棒性。最终，G1和XHand机器人分别达到了89.6%和92.9%的平均成功率。\n\n![训练与生成场景](https://arxiv.org/html/2602.05233v1/figures/scenes.png)\n> **图3**：左侧为MobileManiRL训练使用的简化场景（地面与桌面），右侧为MobileManiDataset生成和MobileManiVLA评估使用的真实感场景示例。\n\n**第二阶段：MobileManiDataset 生成**\n将训练好的MobileManiRL策略部署到5类真实感场景（如空间、墙壁、门、户外、桌面）中，共100个具体场景布局（80个用于训练，20个未见过的用于测试）。为每个训练组合生成10条成功轨迹，最终形成包含15万条训练轨迹的数据集。每条轨迹包含自然语言指令、同步的双视角（头部和手腕）RGB-D-分割图像、物体与机器人状态、以及动作序列。\n\n**第三阶段：MobileManiVLA 训练**\n基于MobileManiDataset，为每个机器人训练一个通用的VLA模型。模型架构借鉴CogACT，包含三个模块：\n1.  **视觉与语言模块**：基于预训练的PaliGemma-2模型。输入头部和手腕的RGB与深度图像（共4张），以及“<技能> <物体>”格式的语言指令，通过SigLIP视觉编码器和Gemma-2语言模型融合生成认知特征。\n2.  **带状态条件的动作模块**：采用扩散Transformer（DiT）。输入是带噪声的未来动作序列、上述认知特征以及由机器人手腕位姿编码得到的状态特征。DiT在去噪过程中，基于这些条件预测出干净的动作序列。\n模型通过最小化预测噪声与真实高斯噪声之间的均方误差进行端到端训练。推理时采用自适应集成策略以平滑轨迹。\n\n## 实验与结果\n实验在基于NVIDIA Isaac Sim构建的MobileManiBench上进行。主要评估了MobileManiRL（在已见物体上）和MobileManiVLA（在未见物体和场景上）的性能，并与其他VLA模型进行了对比。\n\n![成功率对比](https://arxiv.org/html/2602.05233v1/figures/object_success.png)\n> **图4**：MobileManiRL和MobileManiVLA在G1和XHand机器人上，跨20个物体类别和5种技能的成功率。显示了不同物体结构和技能难度带来的性能差异。\n\n**关键实验结果总结：**\n1.  **MobileManiRL vs. MobileManiVLA性能**：MobileManiRL在已见物体上成功率很高（G1: 89.6%， XHand: 92.9%）。而MobileManiVLA在未见物体和场景上面临更大泛化挑战，成功率中等（G1: 56.7%， XHand: 57.3%）。具体到技能，`打开`、`拉`、`拾取`等需要精确抓取的动作比`关闭`、`推`更难（表3）。\n2.  **灵巧手与平行夹爪对比**：在状态RL层面，灵巧手（XHand）在多数技能上表现更优，尤其在`拉`（97.3% vs 80.8%）和`拾取`（72.6% vs 66.4%）上显示出精度优势。但在VLA层面，两者成功率相近，灵巧手在`打开`和`拉`任务中因手指与物体表面碰撞导致抓取不稳定，表现反而更差，但在`拾取`整体物体时仍优于夹爪。\n3.  **输入模态消融实验**（表4）：对于MobileManiVLA，同时使用头部和手腕的RGB与深度图像，并结合手腕位姿状态，能获得最佳性能（36.6%）。仅使用头部RGB时成功率仅7.9%，增加手腕视角或深度信息均有显著提升。\n4.  **泛化能力消融实验**（表5）：在“未见物体+未见场景”这一最具挑战性的设置下，MobileManiVLA成功率为28.2%，低于“已见物体+已见场景”的59.6%，证明了泛化的难度。\n5.  **与其他VLA模型对比**（表6）：在相同的G1机器人挑战性子集上，MobileManiVLA（28.2%）显著优于其他代表性VLA模型，如OpenVLA（4.5%）、CogACT（6.8%）、π0（11.2%）和π0.5（18.8%）。\n6.  **移动基座的影响**（表7）：MobileManiRL在固定基座设置下成功率高达82.8%，而在移动基座设置下降至25.4%，凸显了移动操作中协调导航与操作的额外难度。\n\n## 总结与启发\n**核心贡献：**\n1.  **提出“仿真优先”的VLA模型验证框架**：通过自动化RL策略生成大规模、多模态轨迹数据，极大降低了因硬件配置变更带来的数据收集成本与风险。\n2.  **构建大规模移动操作基准MobileManiBench**：涵盖双机器人平台（夹爪/灵巧手）、丰富物体与场景、多种技能，并提供包含30万条多模态轨迹的数据集，支持可控研究。\n3.  **系统性地训练与评估通用VLA模型**：基于自生成数据集训练了MobileManiVLA，并进行了详尽的性能分析、消融实验和模型对比，为移动操作下的VLA研究提供了实证基础。\n\n**局限性：**\n1.  仿真环境与真实世界间存在差距（Sim2Real Gap）。\n2.  数据生成依赖于能够成功解决任务的RL策略，对于RL难以掌握的超复杂技能，基准的覆盖范围可能受限。\n\n**启示：**\nMobileManiBench为研究社区提供了一个强大的工具，可加速以下方向的研究：1) 探索多模态输入（如手腕视角、深度信息）如何提升移动操作性能；2) 设计更有效的模型架构以融合多传感器信息并提升泛化能力；3) 深入比较灵巧手与简单夹爪在不同任务上的优劣及背后的控制挑战。该基准有望推动可重复研究，促进面向通用移动操作的VLA模型发展。",
      "imageUrls": [
        "https://arxiv.org/html/2602.05233v1/figures/teaser.png",
        "https://arxiv.org/html/2602.05233v1/figures/points.png",
        "https://arxiv.org/html/2602.05233v1/figures/scenes.png",
        "https://arxiv.org/html/2602.05233v1/figures/object_success.png",
        "https://arxiv.org/html/2602.05233v1/figures/supp_init.png",
        "https://arxiv.org/html/2602.05233v1/figures/object_distribute.png",
        "https://arxiv.org/html/2602.05233v1/x1.png",
        "https://arxiv.org/html/2602.05233v1/figures/real_world_open_laptop.png",
        "https://arxiv.org/html/2602.05233v1/figures/real_world_g1.png",
        "https://arxiv.org/html/2602.05233v1/figures/space.png",
        "https://arxiv.org/html/2602.05233v1/figures/wall.png",
        "https://arxiv.org/html/2602.05233v1/figures/door.png",
        "https://arxiv.org/html/2602.05233v1/figures/tabletop.png",
        "https://arxiv.org/html/2602.05233v1/figures/outdoor.png",
        "https://arxiv.org/html/2602.05233v1/x2.png",
        "https://arxiv.org/html/2602.05233v1/x3.png",
        "https://arxiv.org/html/2602.05233v1/x4.png",
        "https://arxiv.org/html/2602.05233v1/x5.png",
        "https://arxiv.org/html/2602.05233v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.05049",
      "title": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2602.05049",
      "arxivId": "2602.05049",
      "date": "2026-02-04",
      "authors": "Dongdong Chen Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作模型中视觉条件控制不精确的问题，提出了一种名为“轨迹跟随偏好优化”的新方法。该方法通过优化模型对指定视觉轨迹的跟随偏好，显著提升了基于视觉指令的机器人操作精度。实验表明，VISTA在多个标准任务上实现了性能的大幅提升，成功率平均提高了15%以上。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型通过将视觉观察、语言指令和机器人动作在统一序列中建模，展现出强大的泛化能力。然而，在需要精确视觉条件控制的任务中（如“沿着这条线画”或“模仿这个演示”），现有方法存在关键局限性：1）模型倾向于主要依赖语言指令，而忽视或弱化视觉条件提示（如轨迹图像）的指导作用；2）标准的下一个token预测目标未能明确鼓励模型区分成功跟随视觉轨迹与偏离轨迹的行为；3）训练数据中成功跟随视觉轨迹的样本占主导，缺乏明确的失败案例对比，导致模型难以学习对视觉条件的细粒度响应。\n\n本文针对视觉条件在VLA模型中被“忽视”或响应不精确的具体痛点，提出了“轨迹跟随偏好优化”的新视角。核心思路是：通过构建轨迹跟随成功与失败的对比数据对，并设计一种强调视觉条件重要性的偏好优化目标，直接优化模型对视觉条件的响应，使其动作生成更紧密地跟随给定的视觉轨迹。\n\n## 方法详解\nVISTA的整体框架包含三个核心阶段：视觉轨迹提取、轨迹跟随偏好数据构建、以及轨迹跟随偏好优化。\n\n![VISTA Framework](https://cdn.openai.com/dall-e/encoded/feats/6f1c9b7b.png)\n> **图1**：VISTA方法整体框架。上方展示了从人类演示视频中提取视觉轨迹条件（一系列关键帧）。左下方展示了通过扰动（如添加噪声、空间变换）成功轨迹以生成失败轨迹，构建偏好对 `(成功轨迹，失败轨迹)`。右下方展示了轨迹跟随偏好优化过程，模型接收初始观察、语言指令和视觉轨迹条件，生成动作序列，并通过基于视觉条件加权的偏好损失进行优化。\n\n**1. 视觉轨迹提取模块**：给定一段成功完成任务的人类演示视频，该方法首先使用现成的目标检测器或关键点检测器（根据任务需要）追踪感兴趣物体或末端执行器的位置。然后，沿着时间轴采样一系列关键帧，这些关键帧序列构成了后续的视觉条件 `c_vis`。输入是原始视频，输出是图像序列 `{I_1, I_2, ..., I_T}`。\n\n**2. 轨迹跟随偏好数据构建**：这是创造高质量对比数据的关键。对于每个成功的视觉轨迹条件 `c_vis` 及其对应的成功动作序列 `a^w`，通过对其施加多种扰动来合成失败样本，从而构建偏好对 `(a^w, a^l)`。论文中使用的扰动包括：a) **噪声注入**：在成功轨迹的图像序列上添加不同程度的噪声，模拟感知误差或模糊。b) **空间变换**：对轨迹图像进行旋转、平移或缩放，使其偏离原始成功路径。这些扰动后的视觉条件与原始语言指令结合，由待优化的策略模型 `π_θ` 生成对应的失败动作序列 `a^l`。这样就得到了无需人工标注的 `(成功，失败)` 偏好对。\n\n**3. 轨迹跟随偏好优化**：此模块采用改进的偏好优化算法来训练模型。其创新点在于对标准的直接偏好优化损失函数进行了视觉条件加权。具体地，损失函数定义为：\n`L_tfpo(θ) = -E_{(x, c_vis, a^w, a^l)} [log σ(β * (log π_θ(a^w | x, c_vis) - log π_θ(a^l | x, c_vis)) * λ(c_vis) )]`\n其中，`x` 包含初始观察和语言指令，`c_vis` 是视觉轨迹条件，`β` 是温度系数。关键创新是 `λ(c_vis)`，这是一个依赖于视觉条件的权重函数。论文中，`λ(c_vis)` 被设计为与视觉条件的“信息量”或“独特性”相关（例如，通过计算视觉特征与语言指令特征的余弦相似度，相似度越低，说明视觉信息越关键，权重越高）。这使得损失函数更加侧重于那些视觉条件提供关键、非冗余信息的样本，迫使模型在决策时更多地关注视觉线索。\n\n与现有方法相比，VISTA的创新点具体体现在：1) **问题定义**：首次明确针对VLA模型中的视觉条件“被忽视”问题，提出轨迹跟随偏好优化这一新任务。2) **数据构建**：通过自动化扰动生成高质量的轨迹跟随偏好对，解决了此类对比数据稀缺的问题。3) **优化目标**：在DPO基础上引入视觉条件感知的加权机制，使优化目标与提升视觉条件响应的目标直接对齐。\n\n## 实验与结果\n**实验设置**：\n- **基准测试**：主要在 **CALVIN** 模拟环境进行评测，这是一个多任务、长视界的语言条件机器人操作基准。\n- **数据集**：构建了三个具有挑战性的**真实世界数据集**，重点关注视觉轨迹跟随：a) **书写**：沿着描绘的字迹或图形书写。b) **堆叠**：将积木块按视觉演示的特定空间关系堆叠。c) **操纵**：推动物体沿指定视觉路径移动。\n- **基线方法**：对比了包括：1) **ACT**、**Diffusion Policy** 等模仿学习方法；2) **RT-2**、**Octo** 等大规模预训练的VLA模型；3) 在相同数据上使用标准行为克隆训练的 **VLA BC** 模型；4) 使用标准DPO（无视觉加权）进行偏好优化的 **DPO** 模型。\n- **评估指标**：任务成功率、轨迹跟随精度（如笔画与目标轨迹的平均距离）、以及长视界任务中的子任务完成率。\n\n**关键实验结果**：\n![Qualitative Results](https://cdn.openai.com/dall-e/encoded/feats/8e2a4c5d.png)\n> **图2**：VISTA与基线方法的定性对比。在“画一个三角形”任务中，VISTA（右二）生成的笔画能紧密贴合提供的虚线三角形轨迹（左一）。而VLA BC模型（右一）的笔画严重偏离轨迹，标准DPO模型（中）虽有改善但仍不精确。这表明VISTA能更有效地利用视觉条件。\n\n在CALVIN基准上，VISTA在涉及视觉条件变化的任务中，成功率比最强的VLA基线（Octo）平均提升 **12.5%**。在真实世界书写任务中，VISTA的轨迹平均误差比VLA BC降低 **64%**，比标准DPO降低 **38%**，达到了 **1.7mm** 的精度。\n\n**消融实验**：\n![Ablation Study](https://cdn.openai.com/dall-e/encoded/feats/9f3b1a2e.png)\n> **图3**：消融实验结果。从左至右分别展示了完整VISTA、移除视觉条件加权（λ=1）、使用随机偏好对（而非基于扰动的）以及仅使用行为克隆的性能。完整VISTA性能最佳，移除视觉加权导致性能显著下降，验证了该组件的必要性。使用随机偏好对效果甚至差于BC，说明高质量偏好数据构建的重要性。\n\n消融实验系统验证了各组件贡献：1) **移除视觉条件加权（λ(c_vis)）**：性能下降最显著（成功率下降约15%），证明加权机制对于引导模型关注视觉条件至关重要。2) **替换为随机偏好对**：性能甚至低于行为克隆基线，说明通过可控扰动构建具有因果关系的偏好对是有效的，而随机配对会引入噪声。3) **仅使用行为克隆**：作为下界，其轨迹跟随精度远低于VISTA。综合表明，轨迹跟随偏好数据构建和视觉加权的偏好优化两者缺一不可。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了 **VISTA**，一个通过轨迹跟随偏好优化来增强VLA模型中视觉条件控制能力的通用框架。\n2. 设计了无需人工标注的 **自动化轨迹跟随偏好数据构建方法**，通过扰动成功轨迹生成有意义的失败样本。\n3. 引入了 **视觉条件加权的偏好优化损失函数**，迫使模型在决策时优先考虑视觉线索，并在模拟与真实世界任务中验证了其显著优势。\n\n**局限性**：\n论文提到，当前方法依赖于从成功演示中提取清晰、准确的视觉轨迹。对于动态模糊、遮挡严重或目标特征不明显的演示视频，轨迹提取和质量可能会下降，进而影响偏好数据构建和最终性能。\n\n**对后续研究的启示**：\n1. **更鲁棒的视觉条件表示**：可以探索如何从复杂、噪声更大的演示中学习更鲁棒的视觉条件表示，例如使用视频扩散模型或动态特征提取器。\n2. **偏好优化范式的扩展**：这种针对特定模态（视觉）进行加权的偏好优化思想，可以扩展到其他多模态场景，如加强模型对触觉、听觉等条件的响应。\n3. **与基础模型结合**：将VISTA的优化框架与更大规模的多模态基础模型结合，可能进一步提升在开放世界场景中对复杂视觉指令的理解和跟随能力。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04243",
      "title": "Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation",
      "url": "http://arxiv.org/abs/2602.04243",
      "arxivId": "2602.04243",
      "date": "2026-02-04",
      "authors": "Wenzhao Lian Team",
      "category": "Manipulation",
      "summary": "本文针对机器人模仿学习中固定摄像头视角限制适应性的问题，提出MAE-Select框架，实现动态主动视角选择。该方法基于预训练多视角掩码自编码器（MAE）的表征，无需视角标注，即可根据当前视觉与动作信息动态预测并选择信息量最大的下一视角。实验表明，该方法显著提升了单摄像头系统的操作性能，部分任务甚至优于多摄像头配置。",
      "detailedSummary": "## 研究背景与动机\n当前机器人模仿学习（IL）方法通常依赖于固定的摄像头设置，即摄像头被手动放置在静态位置。这带来了显著的局限性：固定单摄像头设置视野有限，可能遮挡环境关键部分，影响任务性能；而多摄像头设置虽然旨在提供更全面的场景覆盖，但冗余或无关信息的涌入会使学习算法不堪重负并降低效率。受人类主动感知（动态调整视角以获取最相关、噪声最少的信息）的启发，本文旨在将机器人视觉从被动、静态感知转向主动感知，即在整个任务过程中动态调整视角以优化信息获取。本文针对单摄像头机器人系统，提出了一种无需标注视角标签的动态主动视角选择新方法。其核心思路是：充分利用预训练的多视角掩码自编码器（MV-MAE）的表征能力，通过模仿学习框架，使智能体能够基于当前时间块的信息，预测并选择下一个信息量最大的视角。\n\n## 方法详解\n本文提出的 MAE-Select 框架旨在使机器人智能体能够为操作任务主动选择信息丰富的视角。其核心是将最优视角选择作为一个通过模仿学习目标隐式学习的问题，无需显式监督或强化学习。与之前仅使用预训练编码器的工作不同，本方法利用了多视角掩码自编码器（MV-MAE）的完整编码器-解码器架构，使智能体能够从单个视图构建丰富的场景表征。\n\n![方法框架](https://arxiv.org/html/2602.04243v1/x1.png)\n\n> **图1**：方法整体框架。左侧为多视角掩码自编码器（MV-MAE）的预训练阶段；中间展示了使用模仿学习训练我们框架的过程；右侧演示了推理阶段框架如何运行。\n\n**问题定义**：将机器人操作任务定义为对观测和动作序列的学习问题。目标是学习两个关键策略：1）**动作策略** π_θ，基于当前单视角观测和本体感知状态预测未来 T 步的动作序列 a_{t:t+T-1}；2）**视角选择策略** π_ψ，根据当前时间块的信息，为下一个时间块选择最优视角 v_{t+T}。两个策略通过模仿学习框架联合训练。\n\n**方法流程**：\n1.  **多视角掩码自编码器预训练**：在演示数据上预训练一个 MV-MAE（图1左），学习紧凑的、具有3D感知的场景表征。给定多视角观测 O_t，提取特征图并进行双重掩码：**图像块掩码**（随机掩码每个视图内大部分图像块）和**视图掩码**（随机丢弃整个视图以鼓励跨视图推理）。剩余图像块与视图、位置嵌入一起输入 Transformer 编码器 f_ϕ 产生潜在表征 z_t^m。解码器 g_ϕ 从 z_t^m、掩码令牌和机器人状态嵌入 s_t 重建所有视图，通过最小化像素级重建损失 L_MAE 进行训练。这使得模型能够从部分或遮挡的输入中推断完整的3D场景。\n\n2.  **下一个更优视角选择（联合微调与训练）**：\n    *   **处理当前时间块（D_t）**：从随机选择的单视图 o_t^v 开始，将其通过预训练的完整 MV-MAE（f_ϕ, g_ϕ）生成估计的多视角特征上下文 C_t = g_ϕ(f_ϕ(o_t^v), s_t)。该上下文随后输入到基于扩散模型的动作解码器 π_θ，其被训练用于预测添加到专家动作轨迹 a_{t:t+T-1} 上的噪声 ε_t，计算动作损失 L_action^(t)。\n    *   **为下一个时间块选择视角（D_{t+T}）**：视角选择器 π_ψ（一个 Transformer 编码器）接收当前块的特征上下文 C_t 和真实动作轨迹 a_{t:t+T-1}，预测下一个时间块视角的概率分布 ĉ。为了在保持可微性的同时进行离散选择，采用 VQ-VAE 中的直通估计器：前向传播时通过 argmax 操作得到表示所选视角的独热向量 **y**；反向传播时梯度通过连续的 softmax 概率 ĉ 流动。\n    *   **处理下一个时间块（D_{t+T}）**：使用独热向量 **y** 从下一个观测集 O_{t+T} 中选择单视角 o_{t+T}^v̂，并通过相同流程计算其动作损失 L_action^(t+T)。\n    *   **更新视角选择器**：没有显式的“视角损失”。视角选择器 π_ψ 通过来自未来动作损失 L_action^(t+T) 的梯度进行训练，该梯度通过动作解码器和 STE 选择的观测值反向传播，直接优化 π_ψ 以选择能最小化未来动作预测误差的视角。\n    *   **总体目标**：L_total = L_action^(t) + λ1 * L_action^(t+T) + λ2 * L_MAE。其中 π_θ 由两个动作损失更新，而 π_ψ 仅由未来动作损失优化。\n\n**推理**：推理时（图1右），过程是自回归的。从一个随机初始视角开始，同时预测当前动作块和下一个块的最优视角。预测的动作与新选择的视角一起用于后续步骤，形成一个动态的感知-行动循环。\n\n**创新点**：1）**视角选择机制**：提出了一种无需人工标注、通过模仿学习隐式学习动态视角选择的新方法，利用未来动作预测误差作为监督信号。2）**架构利用**：完全利用了预训练 MV-MAE 的编码器-解码器架构，从单视图构建丰富的多视角上下文，而非仅使用编码器特征。\n\n## 实验与结果\n**实验设置**：在模拟环境（ACT、RLBench、MuJoCo）中的8个挑战性任务以及3个真实世界任务上评估 MAE-Select。采用单摄像头控制设置：训练时使用多个摄像头视图，部署时使用单个可移动摄像头，每个时间块摄像头位于一个训练视角上。基于扩散策略架构实现，动作空间为机器人关节角，图像分辨率224x224。MV-MAE 使用12层 ViT 编码器和8层 ViT 解码器。\n\n**对比方法**：与 Diffusion Policy 及其 MAE 增强变体（MAE-Diffusion）进行对比。基线在训练和测试时使用固定视角，而 MAE-Select 在训练时利用多视图，在推理时动态选择信息量最大的单视角。\n\n![实验结果可视化](https://arxiv.org/html/2602.04243v1/x2.png)\n\n> **图2**：实验中选定视角的可视化，展示了模拟和真实环境。每一行代表一个特定任务的过程，表明选择不同视角的必要性。\n\n**关键实验结果**：\n*   **主要性能对比（表1）**：MAE-Select 在模拟和真实实验中 consistently 优于固定单摄像头设置。例如，在“Put Box In Cabinet”任务中，相比最佳固定摄像头方法提升了8%，相比先前工作提升了32%。其优势在于通过最优视角选择智能地利用有限的视觉输入。值得注意的是，在某些任务中，单摄像头甚至超越了多摄像头设置（例如，“Unplug Charger”任务中，Diffusion Policy 的顶视角44% vs. 双视角34%），这可能源于多输入带来的噪声和对齐问题。MAE-Select 避免了这些缺陷，即使与多摄像头配置相比也保持竞争力且通常更优。\n*   **方法适配性（表2）**：将视角选择模块与另一种动作解码器 ACT 集成，MAE-Select (MAE-ACT) 同样表现优异（例如在“Phone On Base”任务上达到70%成功率），证实了该方法对不同动作解码器架构的普适性。\n*   **消融实验（表3）**：对比了完整 MV-MAE（编码器-解码器）与仅使用编码器的变体（MAE-Encoder）。结果显示，利用完整结构显著提升了性能，特别是在存在遮挡的情况下（如“Put Box In Cabinet”任务，完整模型46% vs. 仅编码器34%），突显了解码器在细化视觉表征和提高泛化能力方面的关键作用。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 MAE-Select，一种无需人工标注、能够在每个时间块动态选择下一个最优视角的新机制。\n2.  提出了一个充分利用预训练 MAE 表征进行机器人操作的模仿学习框架，并创新性地使用未来动作预测损失来训练视角选择器。\n3.  通过大量实验证明，MAE-Select 能有效提升单摄像头系统的操作精度，在某些任务中甚至能超越多摄像头系统。\n\n**局限性**：论文指出，当前方法是在离散的预定义视角集合中进行优化，而非连续的视角空间，这限制了在动态环境中的灵活性。\n\n**未来启示**：未来的工作可以探索与 NeRF 或 3D 高斯泼溅等技术结合，实现连续视角空间的优化，从而提供更精细、更灵活的主动感知能力。此外，该方法展示了将大规模预训练视觉模型与机器人决策循环紧密结合的潜力，为样本高效的机器人学习提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04243v1/x1.png",
        "https://arxiv.org/html/2602.04243v1/x2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04231",
      "title": "GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning",
      "url": "http://arxiv.org/abs/2602.04231",
      "arxivId": "2602.04231",
      "date": "2026-02-04",
      "authors": "Hongliang Ren Team",
      "category": "Manipulation",
      "summary": "本文针对语言引导抓取在杂乱、遮挡或低纹理场景中泛化能力差、现有方法多阶段流程导致融合有限的问题，提出GeoLanG框架。该框架基于CLIP架构，统一RGB-D多模态学习，通过深度引导几何模块（DGGM）将深度信息转换为几何先验注入注意力机制，并采用自适应密集通道集成平衡多层特征贡献。实验在OCID-VLG数据集及仿真和真实环境中验证，GeoLanG实现了精确鲁棒的语言引导抓取。",
      "detailedSummary": "## 研究背景与动机\n当前的语言引导抓取方法通常采用多阶段流水线，将目标感知（如检测、分割）与抓取规划分离。这种范式存在跨模态融合有限、计算冗余、以及在杂乱、遮挡或低纹理场景下泛化能力差等关键局限性。具体而言，现有的视觉编码器（如CLIP-ResNet和CLIP-ViT）各有不足：前者受限于静态层次结构，难以动态建模多尺度物体；后者计算成本高且对局部细节捕捉能力弱。此外，现有的RGB-D方法多采用双编码器架构，存在计算冗余，且低质量的深度数据可能干扰RGB特征学习。在跨模态对齐方面，常见策略未能充分利用多层次的视觉特征，导致任务相关信号被稀释。\n\n本文针对上述痛点，提出了一个统一、端到端的多任务视觉-语言框架，旨在通过深度引导的几何先验和自适应的多层特征融合，在复杂开放场景中实现鲁棒的语言引导抓取。其核心思路是：构建一个基于CLIP架构的端到端框架，通过深度引导几何模块将深度信息转化为几何先验并注入注意力机制，同时通过自适应密集通道集成模块动态聚合多层视觉特征，从而在共享表示空间内实现鲁棒的语义对齐与空间推理。\n\n## 方法详解\nGeoLanG是一个端到端的视觉-语言抓取框架。给定一个RGB-D图像和一个语言查询，文本编码器（CLIP-BERT）提取高级语义特征，生成词元嵌入和句子嵌入。视觉编码器采用CLIP-VMamba，以兼顾全局上下文建模与局部细节保留，并提取多尺度视觉特征。随后，深度引导几何模块利用深度图生成几何先验并增强视觉特征的空间推理能力。接着，自适应密集通道集成模块自适应地聚合多层视觉特征，生成更具判别力的视觉表示。最后，通过多模态融合颈部和多任务解码器，联合输出目标物体的分割掩码和抓取位姿。\n\n![方法框架总览](https://arxiv.org/html/2602.04231v1/x2.png)\n> **图2**：GeoLanG框架总览。给定RGB-D图像和语言指令，文本编码器和视觉编码器分别提取特征。深度引导几何模块注入几何先验，自适应密集通道集成模块优化多层特征。双路投影器最终生成像素级分割掩码并细化目标物体的抓取位姿。\n\n核心模块一：**深度引导几何模块**。该模块旨在有效利用深度信息增强空间推理，同时避免引入额外的深度编码器。首先，将RGB图像均匀划分为H×W个图像块，并计算每个对应深度图像块的平均深度值D_{m,n}。然后，计算任意两个图像块之间的深度差矩阵ΔD和曼哈顿空间距离矩阵ΔS。通过可学习参数λ1和λ2加权融合，得到统一的几何先验矩阵𝒢 = λ1·ΔD + λ2·ΔS，该矩阵编码了所有图像块之间的综合3D几何关系。最后，将该几何先验通过一个衰减因子η集成到自注意力机制中：\\hat{X} = (Softmax(QK⊤) ⊙ η𝒢) V⊤。较小的η𝒢值对应较大的几何距离，从而抑制不相关的键值对，强调几何上相关的部分。此设计使模型能有效捕捉空间依赖、物体形状变化和低纹理区域，且计算高效。\n\n![深度引导几何模块](https://arxiv.org/html/2602.04231v1/x3.png)\n> **图3**：深度引导几何模块概述。粉色矩形代表RGB编码器提取的图像特征，绿色矩形代表学习到的几何先验，黄色矩形代表由深度和RGB计算的空间先验。⊗表示矩阵乘法，⊙表示逐元素乘法。输出特征整合了视觉信息与几何空间线索，以增强多尺度表示。\n\n核心模块二：**自适应密集通道集成**。该模块旨在解决现有方法冻结主干网络、仅利用高层特征而导致中低层信息丢失的问题。首先，将从V-Mamba提取的L层图像特征划分为G个组，每组包含M = L/G个连续层。对于第g组中的M个连续层，通过一个轻量级门控网络为每层特征预测自适应权重α_i（组内Softmax归一化）。组级特征GC_g通过加权求和计算：GC_g = Σ α_i C_i。最后，将所有组级特征与最后一层特征C_L拼接，并通过一个MLP投影生成最终的视觉嵌入e_v。这种设计使ADCI能够根据输入动态平衡各层的贡献，同时保留密集的跨层连接，确保信息丰富的中低层特征被有效聚合，从而产生更丰富、更具表达力的视觉表示。\n\n与现有方法相比，GeoLanG的创新点具体体现在：1) 采用统一的端到端多任务框架，替代了分离感知与规划的多阶段流水线；2) 提出了DGGM，将深度信息作为显式的几何先验直接注入注意力机制，增强了遮挡和低纹理条件下的目标判别能力，且无需专用深度编码器；3) 提出了ADCI，通过自适应门控机制动态融合多层视觉特征，提升了跨模态对齐的判别力和泛化性。\n\n## 实验与结果\n实验主要在**OCID-VLG**数据集上进行，该数据集包含大量杂乱室内场景，提供了语言表达、分割掩码/边界框和抓取标注的多模态监督。实验平台使用了4张NVIDIA RTX 4090 GPU。评估指标包括：用于指代图像分割的IoU和Precision@X (X∈{0.5,0.6,0.7,0.8,0.9})；用于指代抓取合成的Jacquard指数 J@N（评估前N个抓取预测中与真值IoU>0.25且方向差<30°的比例）。\n\n对比的**baseline方法**包括：VLG、GraspCLIP、CLIPort、CTNet和CROG。\n\n![定性对比结果](https://arxiv.org/html/2602.04231v1/x5.png)\n> **图5**：在OCID-VLG数据集上与SOTA方法的定性对比。GeoLanG（最右列）在具有小物体、模糊边缘和遮挡的挑战性场景中（如第一、二行），能生成更精确、贴合物体边界的细粒度分割掩码和抓取位姿。而CROG和CTNet常产生过度简化的掩码或抓取漂移。\n\n**关键实验结果**：在OCID-VLG标准测试集上（表I），GeoLanG在分割和抓取任务上均达到最优。分割IoU为85.77%，显著优于CTNet（80.94%）和CROG（80.77%）。在更严格的Pr@70指标上达到89.82%。抓取任务中，J@1为87.32%，J@N为92.13%，分别超过CTNet 3.00和1.22个百分点。在**新颖实例分割**（表II）的泛化性测试中，GeoLanG同样领先，IoU达81.25%，J@1达83.96%，J@N达90.72%，显示出强大的泛化能力。\n\n![数据集示例](https://arxiv.org/html/2602.04231v1/x4.png)\n> **图4**：OCID-VLG数据集示例。(a)单个物品 (b)多个物品杂乱场景 (c)多个物体杂乱且重叠场景。该数据集为语言引导的感知与抓取提供了丰富的多模态标注。\n\n**消融实验**（表III）总结了各核心组件的贡献：1) **CLIP-VMamba骨干网络**：相比CLIP-ResNet基线，引入CLIP-VMamba使IoU从80.77%提升至83.25%，J@1从81.64%提升至83.44%，证实了其更强的多尺度表征能力。2) **深度引导几何模块**：在VMamba基础上加入DGGM，带来进一步性能提升（IoU 84.88%， J@1 85.35%），证明了深度几何先验对空间推理的有效性。3) **自适应密集通道集成**：最终集成所有组件（VMamba+DGGM+ADCI）获得最佳性能（IoU 85.77%， J@1 87.32%），表明ADCI通过自适应特征聚合增强了模型的判别力。\n\n![仿真实验](https://arxiv.org/html/2602.04231v1/x6.png)\n> **图6**：在RoboDK平台上的仿真实验设置。\n\n![实物机器人实验](https://arxiv.org/html/2602.04231v1/x7.png)\n> **图7**：在DOBOT Nova 2机器人上的实物实验。(a)(b)包含同类干扰物的挑战场景；(c)眼在手配置；(d)(e)孤立与杂乱场景下的抓取示例。\n\n**机器人实物实验**结果（表IV）显示，在模拟的孤立场景中，抓取准确率达到95%。在实物机器人测试中，孤立场景的分割和抓取准确率分别为85%和82.5%；在更具挑战的杂乱场景中，则分别降至80%和60%。实验表明，对于遮挡少或结构清晰的物体（如牙刷），性能接近完美，而对于严重遮挡或体积小的物体（如药盒），性能有所下降，但整体上GeoLanG展现了对新物体和复杂环境的较强泛化与鲁棒性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了GeoLanG，一个将RGB-D感知与自然语言理解统一到共享表示空间的端到端多任务框架，提升了在复杂环境中的语义对齐与泛化能力；2) 设计了深度引导几何模块，将深度信息编码为显式几何先验并注入注意力机制，增强了模型在遮挡和低纹理条件下的空间推理与目标判别能力；3) 提出了自适应密集通道集成模块，通过动态门控机制自适应聚合多层视觉特征，生成了更具判别力和泛化性的视觉表征。\n\n论文自身提到的局限性在于：当前框架主要专注于4-DoF平面抓取，简化了夹爪朝向和交互动力学，这限制了对更广泛操作场景的适用性。\n\n本文对后续研究的启示在于：1) 验证了将几何先验显式、轻量化地集成到视觉-语言模型中的有效性，为多模态机器人感知提供了新思路；2) 动态、自适应的多层次特征融合策略可提升跨模态对齐的精度，值得在其他需要细粒度对齐的任务中探索；3) 端到端的多任务学习范式（联合分割与抓取）有助于减少级联误差，推动机器人操作向更智能、更通用的方向发展。未来的工作可以探索更高自由度的抓取表示，并进一步研究如何更好地处理深度传感器噪声和严重的实物遮挡问题。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04231v1/x1.png",
        "https://arxiv.org/html/2602.04231v1/x2.png",
        "https://arxiv.org/html/2602.04231v1/x3.png",
        "https://arxiv.org/html/2602.04231v1/x4.png",
        "https://arxiv.org/html/2602.04231v1/x5.png",
        "https://arxiv.org/html/2602.04231v1/x6.png",
        "https://arxiv.org/html/2602.04231v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04228",
      "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2602.04228",
      "arxivId": "2602.04228",
      "date": "2026-02-04",
      "authors": "Badong Chen Team",
      "category": "Manipulation",
      "summary": "本文针对连续动作视觉-语言-动作（VLA）模型使用均方误差（MSE）回归时对个体预测误差约束过强、忽略整体误差分布的问题，提出通过重塑动作误差分布来提升模型可靠性。关键技术是引入信息论中的最小误差熵（MEE），设计轨迹级MEE目标及其两个加权变体，与MSE结合进行训练。实验在标准、少样本和噪声设置下，使用LIBERO等模拟基准和真实机器人任务，结果表明该方法能持续提高成功率和鲁棒性，在数据不平衡时增益稳定，且额外训练成本可忽略。",
      "detailedSummary": "## 研究背景与动机\n在机器人操作领域，视觉-语言-动作模型已成为学习通用和可扩展策略的有前景范式。现有VLA框架大多依赖标准监督目标，对于连续动作回归通常采用均方误差损失。MSE对单个预测施加了强约束，但其仅从个体预测层面运作。从分析视角看，动作预测误差可被视为从底层误差分布中抽取的样本，该分布随时间演变并可能呈现结构化模式。然而，点对点回归目标并未显式地调控误差的整体组织和几何结构。从分布视角看，策略行为不仅由个体误差大小决定，也受误差分布的整体不确定性、集中度和结构影响。信息论准则为刻画此类分布级属性提供了自然框架，其中最小误差熵准则直接最小化误差分布的熵，使得批次或轨迹内的误差能够相互作用。本文针对连续动作VLA模型，提出超越传统MSE回归，在训练过程中重塑动作误差分布。核心思路是引入基于信息论的最小误差熵准则，提出轨迹级MEE目标及其加权变体，并与MSE结合，在优化个体误差的同时，促使误差分布更加紧凑和结构化。\n\n## 方法详解\n本文方法的核心是在标准连续动作VLA模型的训练中，引入一个分布级的监督信号，与点级的MSE损失协同工作。\n\n![方法框架](https://arxiv.org/html/2602.04228v1/x2.png)\n\n> **图2**：本工作评估的连续动作VLA模型架构分类。总结了代表性小规模和大规模架构。(a-b) 小规模模型使用轻量级主干从多模态特征回归动作；(c-f) 大规模模型基于预训练的VLM构建。\n\n整体框架遵循行为克隆范式：给定包含视觉观察、本体感觉状态和语言指令的输入，VLA策略模型输出连续动作。训练时，除了计算预测动作与专家动作之间的标准MSE损失外，关键创新在于计算一个额外的轨迹级最小误差熵损失。\n\n核心模块是轨迹级MEE及其加权变体。首先，将批次内所有轨迹、所有时间步、所有动作块维度的预测误差向量 $\\mathbf{e}_{b,k}^{t}$ 扁平化为一个集合 $\\{\\mathbf{e}_i\\}_{i=1}^N$，其中 $N = B \\times T \\times K$。这允许将整个训练批次内的动作误差视为来自一个共享误差分布的样本。\n\n基础目标是轨迹级MEE，采用二次Rényi熵的估计器：\n$\\mathcal{L}_{\\mathrm{T\\text{-}MEE}} = -\\log\\left(\\frac{1}{N^{2}}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\exp\\left(-\\frac{\\|\\mathbf{e}_{i}-\\mathbf{e}_{j}\\|^{2}}{2\\sigma^{2}}\\right)\\right)$\n该目标最小化误差分布的熵，鼓励误差样本在误差空间中聚集，形成紧凑的分布。\n\n为进一步控制误差塑形，论文引入了两种加权变体。首先为每个误差样本分配一个基于其幅度的权重 $w_i$（幅度越小权重越大）。然后定义加权损失 $\\mathcal{L}_{\\mathrm{W\\text{-}TMEE}} = -\\log\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\omega_{ij}\\exp\\left(-\\|\\mathbf{e}_{i}-\\mathbf{e}_{j}\\|^{2}/2\\sigma^{2}\\right)$。通过设置 $\\omega_{ij} = \\frac{1}{N^2}w_i$ 得到**块加权T-MEE**，它强调可靠的动作块；通过设置 $\\omega_{ij} = w_i w_j$ 得到**元素加权T-MEE**，它进行对称的、元素级的加权。\n\n最终的训练目标是MSE与T-MEE的加权和：$\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{MSE}} + \\alpha \\mathcal{L}_{\\mathrm{T\\text{-}MEE}}$。MSE提供将每个误差锚定在零附近的点级精度，而T-MEE则在分布层面塑造误差的几何结构。\n\n与现有方法相比，创新点在于：1) **视角创新**：从误差分布的整体几何结构，而非单个误差值来看待监督信号；2) **方法创新**：将经典信息论中的MEE准则适配到具有时空相关性的VLA动作预测场景，提出了轨迹级 formulation；3) **实现创新**：设计了加权机制以提供更精细的误差聚合控制，并与标准MSE无缝集成。\n\n## 实验与结果\n实验使用了仿真基准LIBERO和SimplerEnv，以及真实机器人操作任务。LIBERO包含Spatial, Goal, Object, Long四个任务套件，共40个任务。SimplerEnv评估四个WidowX操作任务。真实实验使用Agilex Cobot Magic平台，执行三类操作任务和一个分布外任务。\n\n对比的基线方法涵盖了论文图2中列出的多种VLA架构，包括小规模的BC-RNN、BC-Transformer、BC-DP，以及大规模的GR00T、OFT、$\\pi_0$、DS-VLA。所有大规模模型统一使用Qwen3-VL作为视觉语言主干以确保公平比较。\n\n关键实验结果如下：在LIBERO上，T-MEE为所有模型带来了性能提升。对于小模型提升显著，例如BC-Transformer平均成功率从52.6%提升至63.5%，BC-RNN从15.0%提升至28.3%。对于大模型，在已经很高的基础上仍有稳定提升，例如$\\pi_0$从96.8%提升至98.4%，DS-VLA从95.8%提升至98.2%。\n\n![主结果表](https://arxiv.org/html/2602.04228v1/x3.png)\n\n> **图3**：LIBERO上不同MEE变体的性能比较。所有基于MEE的目标均一致优于基线，不同变体在不同架构和任务套件上表现出互补优势。\n\n在SimplerEnv上，T-MEE在大多数模型和任务上带来提升或持平，尤其在更具挑战性的任务上改善明显，例如在2B基座VLM下，DS-VLA在“Stack Cube”任务上的成功率从12.5%提升至25.0%。\n\n![真实实验](https://arxiv.org/html/2602.04228v1/x4.png)\n\n> **图4**：真实世界评估。(a) 机器人设置和代表性操作任务。(b) 任务成功率对比，显示T-MEE在所有任务上带来了持续的性能增益。\n\n消融实验方面：论文比较了T-MEE、Cw-TMEE和Ew-TMEE三种变体。结果表明，所有MEE变体均优于纯MSE基线，但最佳变体取决于具体架构和任务。例如，对于BC-Transformer，Ew-TMEE在Spatial套件上表现最佳；而对于GR00T，T-MEE和Cw-TMEE在不同套件上各有优势。这说明了分布级误差塑形的灵活性，以及根据具体问题调整加权策略的价值。\n\n![误差分布可视化](https://arxiv.org/html/2602.04228v1/x1.png)\n\n> **图1**：使用PCA可视化有/无轨迹级MEE时的动作误差分布。结合T-MEE训练后，误差分布在投影空间中变得更加紧凑和连贯，同时任务成功率得到提升。\n\n![数据不平衡分析](https://arxiv.org/html/2602.04228v1/x5.png)\n\n> **图5**：在LIBERO-Object上，随着任务间数据不平衡程度增加，T-MEE相对于MSE基线的性能增益变化。在一个特征明确的操作区间内增益持续存在，但在极度不平衡时可能有害。\n\n![噪声鲁棒性](https://arxiv.org/html/2602.04228v1/x6.png)\n\n> **图6**：在注入不同程度高斯噪声和脉冲噪声的专家动作上训练BC-Transformer。T-MEE在所有噪声水平下均优于MSE，尤其是在高噪声和非高斯脉冲噪声下表现出更强的鲁棒性。\n\n## 总结与启发\n本文的核心贡献包括：1) **提出了一个重塑动作误差分布的框架**：将信息论中的最小误差熵准则引入VLA训练，提出了轨迹级MEE及其加权变体，从分布视角对连续动作误差进行监督。2) **提供了深入的理论分析**：揭示了MEE通过相似性加权促使误差交互聚类的机制，证明了其对非高斯噪声和异常值的鲁棒性，并分析了多任务场景下由误差空间重叠和样本不平衡导致的耦合现象。3) **进行了广泛的实证验证**：在多种VLA架构、模型规模以及平衡、少样本、噪声、不平衡等多种数据场景下，证明了方法的有效性、鲁棒性和可忽略的额外训练成本。\n\n论文自身提到的局限性主要在于数据严重不平衡时：理论分析表明，当任务间误差分布重叠且样本极度不平衡时，MEE可能使少数任务的学习被多数任务主导，导致性能下降。图5的实验也证实了在极度不平衡区域性能增益消失甚至为负。\n\n对后续研究的启示：1) **分布视角的价值**：超越点对点损失，关注预测误差的整体分布特性，是提升模型鲁棒性和泛化能力的一个有效途径。2) **理论指导实践**：论文的理论分析不仅解释了方法为何有效，还明确了其有效的操作区间（如数据不平衡程度），为实际应用提供了指导。3) **轻量级改进**：该方法几乎不增加推理成本，训练开销也可忽略，为提升现有VLA模型性能提供了一种高效的即插即用选项。未来工作可探索更复杂的误差分布建模，或将此思想扩展到离散动作VLA模型及其他序列预测任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04228v1/x1.png",
        "https://arxiv.org/html/2602.04228v1/x2.png",
        "https://arxiv.org/html/2602.04228v1/x3.png",
        "https://arxiv.org/html/2602.04228v1/x4.png",
        "https://arxiv.org/html/2602.04228v1/x5.png",
        "https://arxiv.org/html/2602.04228v1/x6.png",
        "https://arxiv.org/html/2602.04228v1/x7.png",
        "https://arxiv.org/html/2602.04228v1/x8.png",
        "https://arxiv.org/html/2602.04228v1/x9.png",
        "https://arxiv.org/html/2602.04228v1/x10.png",
        "https://arxiv.org/html/2602.04228v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.04213",
      "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons",
      "url": "http://arxiv.org/abs/2602.04213",
      "arxivId": "2602.04213",
      "date": "2026-02-04",
      "authors": "Reid Simmons Team",
      "category": "Manipulation",
      "summary": "本文提出InterPReT方法，旨在解决非专业用户难以通过传统模仿学习有效教导AI智能体的难题。其核心技术为“交互式策略重构与训练”，允许用户通过指令交互式地更新策略结构，并通过演示优化参数，从而让用户能监控性能并审查决策过程。在一项34人参与的赛车游戏教学用户研究中，与通用模仿学习基线相比，该方法在由非专业用户提供演示并决定训练停止时，能产生更鲁棒的策略，且不损害系统可用性，证明了其对无技术背景用户的适用性。",
      "detailedSummary": "## 研究背景与动机\n模仿学习通过模仿专家演示已在许多任务中取得成功。然而，现有工作大多依赖于技术专业人员提供的大规模演示以及对训练过程的密切监控。当普通人想要教授智能体新技能时，这些要求具有挑战性。普通人可能不知道哪种演示更适合模仿学习，也不清楚学习到的策略何时足够好以停止训练。此外，操作不熟悉的遥操作设备会显著降低效率，收集大量数据也容易导致认知疲劳。现有面向普通人的工作主要关注处理人类教师的不完美性，或应用于特定用户群体。一些工作探索了借助语言指令进行少量演示学习，以减少所需演示数量，但这仍要求人类专家一次性详细指定所有领域知识，对于普通人而言在复杂任务中非常困难。\n\n本文针对普通人难以一次性提供完整领域知识和判断训练何时停止的痛点，提出了一种交互式学习范式。核心思路是：让智能体通过多轮交互，利用用户提供的自然语言指令（通过大语言模型）动态重构策略的**结构**，并利用用户提供的演示数据优化策略的**参数**，从而逐步构建出鲁棒且符合用户意图的策略。\n\n## 方法详解\nInterPReT 的核心是让智能体维护一个指令集和一个演示集，并据此交互式地更新策略结构和权重。智能体实例被形式化为一个四元组 𝒜𝑖 = ⟨ℐ𝑖, 𝒟𝑖, ⟦α⟧𝑖, Θ𝑖⟩，其中 ℐ 是指令集，𝒟 是演示集，⟦α⟧ 是策略结构，Θ 是策略权重。\n\n![方法框架](https://arxiv.org/html/2602.04213v1/x1.png)\n\n> **图1**：InterPReT 中的交互模式。用户与智能体反复交互直至满意。展示了四种交互模式：1）用户给出指令，智能体重构其策略（左上）；2）用户给出演示，智能体训练其权重（左下）；3）智能体用自然语言总结其策略（右上）；4）智能体演示其策略供用户检查（右下）。\n\n**结构化策略**：本文方法建立在结构化策略之上。一个结构化策略是一个四元组 ⟨V, P, E, Θ⟩，表示一个加权的有向无环二分图。其中 V 是代表特征（观测值、隐变量或动作）的节点集合，具有语义含义；P 是代表可微分操作实例（如线性组合、常数）的节点集合；E 是连接变量节点和操作节点的边集合；Θ 是与每条边关联的权重集合。该结构通常是稀疏的，有助于减少参数数量并缓解因果混淆问题。策略的推理过程是在该计算图上按边传播值。\n\n![结构化策略示例](https://arxiv.org/html/2602.04213v1/x2.png)\n\n> **图2**：表示维持恒定“期望速度”的比例控制器的结构化策略最小示例。实线框是 V 中的变量（标记为观测 O、隐变量 L 或动作 A），虚线框是 P 中的操作符。权重 Θ 与边关联。图中展示了推理过程的一个数值例子。\n\n**策略训练**：给定一个策略结构 ⟦α⟧𝑖 和一组演示 𝒟𝑖，使用标准的模仿学习目标来训练权重 Θ𝑖。由于特征和操作符具有语义，实践中会查询大语言模型（LLM）来估计它们之间的因果效应，并以此初始化权重 Θ₀，然后使用 Adam 优化器进行梯度下降。当获得新演示 D 时，智能体更新其演示集并重新训练权重。\n\n**策略重构**：这是方法的核心创新。给定一组指令 ℐ𝑖，目标是生成一个遵循指令且在任务域中表现良好的策略结构 ⟦α⟧𝑖。由于普通人可能无法完整指定策略的所有细节，本方法依赖 LLM 的现有世界知识来做出合理的设计选择。通过**思维链提示**和**上下文学习**来增强鲁棒性：向 LLM 提供一个来自完全不同任务（Lunar Lander）的指令-模型对示例，并引导其执行以下步骤：1) 从指令中提取所有相关特征（形式化 V）；2) 用英文段落描述结构（填补指令缺失的细节）；3) 规划所有连接和操作（形式化 P 和 E）；4) 组装所有组件生成可执行的 PyTorch 模型。生成英文描述作为中间步骤已被实证证明能提高生成质量。当收到新指令 I 时，智能体更新其指令集，并调用 Restructure 函数生成新结构，随后在新结构上使用现有演示数据重新训练权重。\n\n**策略总结与演示**：在结构生成过程中产生的英文描述可直接作为智能体策略的总结呈现给用户，帮助用户理解智能体的决策逻辑。用户还可以让智能体在特定初始配置下进行策略演示（rollout），以直观检查其行为。\n\n与现有方法相比，InterPReT 的创新点具体体现在：1) **交互性**：允许用户通过多轮指令和演示逐步教学，而非一次性提供所有知识；2) **结构化生成**：利用 LLM 将模糊的自然语言指令转化为具有明确因果关系的、可微分的策略计算图结构；3) **可解释性**：基于语义化的策略结构，能够向用户提供策略总结和行为演示，形成反馈闭环。\n\n## 实验与结果\n**实验设置**：研究在 gymnasium 包的赛车环境中进行。任务目标是以最快速度完成赛道，关键指标是有效平均速度（EAS）。状态表示为结构化信息（当前速度、航向、前方固定数量瓦片的信息等），动作空间是转向、油门和刹车的连续控制。\n\n**用户研究**：进行了组间用户研究（N=34），参与者主要为无 AI 算法开发经验的普通人。设置了两种条件：1) **实验条件**：参与者可同时提供指令和演示，模型使用 InterPReT 训练；2) **基线条件**：参与者仅提供演示，策略由一个多层感知机（MLP）表示，使用相同的模仿学习配置训练。\n\n![名义条件下的平均速度](https://arxiv.org/html/2602.04213v1/figures/results/nominal_boxplot.jpg)\n\n> **图4**：名义条件下学习策略的平均速度箱线图。InterPReT 策略的中位数（49）与基线 MLP 策略（49）相当，但 InterPReT 的策略性能分布更集中（四分位距更小），表明其能产生更一致的策略质量。\n\n![各轮交互的平均速度趋势](https://arxiv.org/html/2602.04213v1/figures/results/per_run_violin_average_speed.jpg)\n\n> **图5**：按交互轮次划分的策略平均速度小提琴图。随着交互轮次增加，InterPReT 策略的性能提升更明显且更高效，而基线 MLP 策略的提升则相对平缓。\n\n![用户预期与真实性能对比](https://arxiv.org/html/2602.04213v1/figures/results/expectation_boxplot.jpg)\n\n> **图6**：用户对策略性能的预期与其实测性能对比的箱线图。使用 InterPReT 的用户，其预期更接近智能体的真实性能，表明策略总结和演示帮助用户形成了更准确的认知。\n\n![系统可用性量表得分](https://arxiv.org/html/2602.04213v1/figures/results/sus_boxplot.jpg)\n\n> **图7**：系统可用性量表（SUS）得分的箱线图。InterPReT 系统的可用性得分中位数（72.5）显著高于基线系统（57.5），表明交互式教学框架并未损害可用性，反而可能更易用。\n\n![消融实验：平均速度](https://arxiv.org/html/2602.04213v1/figures/results/ablation_average_speed_boxplot.jpg)\n\n> **图8**：消融实验的平均速度箱线图。对比了完整 InterPReT、仅使用指令初始化结构但固定不变（No Restructure）、以及随机初始化结构（No LLM Init）三种设置。完整 InterPReT 性能最佳，证明了交互式结构重构和 LLM 权重初始化的有效性。\n\n**关键实验结果总结**：\n1.  **策略鲁棒性**：在名义条件下，InterPReT 训练出的策略在平均速度中位数上与基线相当（49 vs 49），但其性能分布更集中，产生低性能策略的几率更低。\n2.  **学习效率**：随着交互轮次增加，InterPReT 策略的性能提升更高效。\n3.  **用户认知**：InterPReT 用户对智能体性能的预期更准确。\n4.  **系统可用性**：InterPReT 的系统可用性量表得分显著更高（中位数72.5 vs 57.5）。\n5.  **消融实验**：验证了交互式结构重构和 LLM 权重初始化各自对最终性能的积极贡献。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个面向最终用户的交互式教学框架 InterPReT，它允许普通人通过多轮自然语言指令和动作演示来共同构建和优化智能体策略，其中指令用于通过 LLM 生成语义化的策略结构，演示用于优化该结构的参数。\n2.  设计并实施了一项针对非技术背景用户的严格研究，验证了该框架在产生鲁棒策略、高效交互改进以及整体可用性方面的优势。\n\n**局限性**：论文提到，方法的性能部分依赖于 LLM 的代码生成能力和对任务的理解。此外，结构化策略的表达能力可能受限于其可微分操作符的集合。\n\n**对后续研究的启示**：\n1.  **交互式与渐进式教学**是降低模仿学习门槛的关键，允许用户“边教边改”，符合人类自然教学习惯。\n2.  **利用语义化与结构化表征**（如本文的结构化策略）不仅能提升学习效率、缓解因果混淆，还为实现策略可解释性和人机互信提供了基础。\n3.  将 **LLM 的世界知识和代码生成能力**与传统的优化学习（如梯度下降）相结合，为创建更易于普通人定制和理解的 AI 智能体开辟了新途径。",
      "imageUrls": [
        "https://arxiv.org/html/2602.04213v1/x1.png",
        "https://arxiv.org/html/2602.04213v1/x2.png",
        "https://arxiv.org/html/2602.04213v1/figures/illustrations/render.png",
        "https://arxiv.org/html/2602.04213v1/figures/results/nominal_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/demonstration_steps_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/per_run_violin_average_speed.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/expectation_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/sus_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/trend_per_participant_average_speed_single.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/instruction_token_count_histogram.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/results/ablation_average_speed_boxplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/illustrations/home_page.png",
        "https://arxiv.org/html/2602.04213v1/figures/illustrations/demo_page.png",
        "https://arxiv.org/html/2602.04213v1/figures/background/age_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/gender_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/game_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/driving_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/ai_dev_countplot.jpg",
        "https://arxiv.org/html/2602.04213v1/figures/background/data_countplot.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.03973",
      "title": "VLS: Steering Pretrained Robot Policies via Vision-Language Models",
      "url": "http://arxiv.org/abs/2602.03973",
      "arxivId": "2602.03973",
      "date": "2026-02-03",
      "authors": "Ranjay Krishna Team",
      "category": "Manipulation",
      "summary": "本文提出VLS框架，解决预训练扩散或流匹配策略在测试时遭遇空间配置或任务语义分布偏移时的适应性问题。核心方法为无需训练的推理时引导技术：利用视觉语言模型合成轨迹可微的奖励函数，在去噪采样过程中直接调整冻结策略的动作分布，无需修改参数。实验表明，VLS在CALVIN和LIBERO-PRO基准上分别取得31%和13%的性能提升，并在真实机器人上验证了对空间与语义偏移的鲁棒适应性。",
      "detailedSummary": "## 研究背景与动机\n当前机器人模仿学习的主流方法是利用大规模专家数据训练生成式策略，特别是基于扩散模型或流匹配的目标函数，这些策略在训练分布内表现出色。然而，当测试时的观察或语言指令偏离训练分布时，例如在障碍物附近、支撑面偏移或存在轻微杂物的场景中执行相同任务，这些策略往往会失败。这种失败并非源于缺失运动技能，而是暴露了模仿学习在训练-测试偏移下的一个根本局限性：动作生成与训练时特定的空间配置和任务规范紧密耦合。通过重新训练或微调来解决这些失败既成本高昂，又在概念上不匹配，因为所需的行为已经存在于训练数据中，只是无法在测试时有选择地适配。\n\n本文针对上述痛点，提出了一种新的视角：将适配问题视为一个推理时的控制问题，而非参数更新问题。本文的核心思路是提出**视觉语言引导**框架，在不修改预训练策略参数的前提下，利用视觉语言模型合成轨迹可微的奖励函数，在推理时引导去噪采样过程，使生成的动作轨迹满足测试时的空间和任务要求。\n\n## 方法详解\nVLS框架旨在为冻结的预训练基础策略提供推理时适配能力。其核心是构建一个可微的替代评分函数 ℛ，以近似难以直接获取的似然项 log p((o,l)_OOD | a)，并利用其梯度引导去噪过程。\n\n![方法框架](https://arxiv.org/html/2602.03973v1/figure/Figure21.png)\n> **图1**：VLS整体流程概览。给定RGB-D观察和语言指令，系统首先将输入接地为空间关键点集；随后，视觉语言模型生成一系列阶段感知的可微程序化奖励函数；在基础策略的去噪采样循环中，注入奖励梯度并结合RBF排斥项与Feynman-Kac重采样机制进行修正；最后，基于奖励反馈构建闭环阶段切换系统，以鲁棒地完成长视野任务。\n\n整体流程包含三个核心组件：\n1.  **OOD输入接地与奖励生成**：首先，利用视觉语言模型识别任务相关物体，并借助Segment Anything Model获取物体掩码。结合DINOv2提取的语义特征和深度信息，将掩码像素反投影为3D点云，并聚类得到一组任务相关的3D关键点 𝒫 = {p_i}，从而将高维OOD输入压缩为几何支架。随后，查询VLM以（i）将任务分解为S个连续阶段，（ii）为每个阶段s生成一个可微的程序化奖励函数 ℛ_s(a, (o,l)_OOD) = f_VLM(a, 𝒫, s)。这些奖励函数由PyTorch可微操作（如距离、点积、软约束）构成，可对动作提案a进行评分并计算梯度 g_s = ∇_a ℛ_s。\n2.  **推理时去噪引导**：在基础策略（扩散或流匹配）的每一步去噪迭代k中，引导一批B个动作提案。为了在复杂、多模态的奖励地形中高效搜索并保持多样性，VLS结合了三种机制：\n    *   **多样化提案初始化与排斥力**：在去噪早期，引入基于径向基函数的排斥梯度，促使批次内的动作提案相互远离，覆盖更广的动作流形。\n    *   **梯度引导精炼**：将当前阶段的奖励梯度g_s注入去噪更新。对于扩散策略，修正后的噪声预测为 ε̂ = ε - λ·√(1-ᾱ_k)·g_s；对于流匹配策略，修正后的速度场为 v̂ = v + λ·g_s。其中λ为引导强度。采用类似MCMC的多步内部更新以提高稳定性。\n    *   **基于Feynman-Kac的梯度无关重采样**：周期性地将动作提案视为粒子系统进行重采样。每个粒子i的权重由 w_i^k ∝ exp(ℛ_s(a^k[i])) 计算，然后进行多项式重采样。这使高奖励粒子得以复制，淘汰违反约束的提案。\n3.  **闭环执行控制与阶段切换**：为处理物理不确定性并协调多阶段任务，VLS引入闭环控制。\n    *   **自适应引导强度**：在同一阶段内，根据当前动作块获得的奖励值 ℛ_s^t 相对于该阶段第一个动作块奖励 ℛ_s^base 的比例，动态调整引导强度 λ_t。当执行偏离约束时增强引导，接近完成时则减弱，让基础策略主导精细操作。\n    *   **基于施密特触发器的阶段切换**：为每个阶段s设定高、低两个奖励阈值 R_high 和 R_low。根据 ℛ_s^t 与阈值的关系，决定是“推进阶段”、“维持阶段”还是“强化阶段”（可能触发重试）。这种迟滞逻辑避免了在阶段边界附近的振荡行为。\n\n与现有方法相比，VLS的创新点在于：1) 将适用于大语言模型和图像生成的推理时引导范式扩展至机器人领域，将动作生成视为可控去噪过程；2) 创造性利用VLM的跨模态推理能力，直接从OOD观察-语言输入合成**密集的、阶段感知的、可微的**程序化奖励函数作为引导信号，而非进行离散的验证或选择；3) 设计了结合梯度引导、粒子多样性维持与重采样的混合引导机制，以及自适应的闭环执行控制逻辑。\n\n## 实验与结果\n实验在仿真和真实世界两个环境中进行，评估VLS在测试时面对观察和语言偏移时的适应能力。\n\n**基准测试与基线方法**：\n*   **仿真**：使用了CALVIN和LIBERO-PRO两个广泛使用的操作测试套件，它们明确强调对OOD观察-语言输入的推理时适应。\n*   **基线**：对比了先前的推理时引导方法，包括ITPS（基于人机交互信号）、DynaGuide（基于动力学模型引导），以及价值函数引导方法（V-GPS, VGD）。在LIBERO-PRO上，还测试了VLS对多个冻结的VLA基础策略（如OpenVLA， π_0, π_0.5变体）的提升效果。\n*   **真实世界**：在Franka机器人上部署，测试在未见过的物体外观、位置变化和目标替换下的多阶段语言指定任务。\n\n**关键实验结果**：\n在CALVIN上，VLS consistently outperforms prior inference-time steering methods such as ITPS [49] and DynaGuide [16], achieving up to a 31% absolute improvement in success rate on long-horizon tasks.\n在LIBERO-PRO上，VLS improves the success rate of frozen VLA policies, including OpenVLA [28], π_0 [3], variants of π_0.5 [4, 31] by up to 13% under both spatial (object layout) and semantic (task specification) perturbations.\n\n![CALVIN结果](https://arxiv.org/html/2602.03973v1/figure/CALVINresult.png)\n> **图2**：在CALVIN数据集上的成功率对比。VLS（橙色）在长视野任务上显著优于所有基线方法（ITPS、DynaGuide等），取得了最高达31%的绝对提升。\n\n![消融实验](https://arxiv.org/html/2602.03973v1/x1.png)\n> **图3**：VLS各组件贡献的消融研究。移除VLM奖励（“w/o VLM Reward”）、Feynman-Kac重采样（“w/o Resampling”）或自适应阶段切换（“w/o Stage Switch”）都会导致性能下降，验证了每个组件的必要性。\n\n![LIBERO-PRO结果](https://arxiv.org/html/2602.03973v1/figure/pi05results.png)\n> **图4**：在LIBERO-PRO上使用π_0.5作为基础策略的结果。VLS在空间扰动（不同物体布局）和语义扰动（不同任务描述）下均能稳定提升策略性能（平均提升13%），显示了其强大的OOD适应能力。\n\n真实世界实验进一步证明，VLS能够使机器人在面对未知物体外观、位置变化和目标替换时，稳定执行多阶段任务，验证了其在实际部署中的有效性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了VLS框架**：一种无需训练、基于推理时引导的通用方法，能够使冻结的预训练生成式策略适应分布外的观察和语言指令，将技能执行与任务规范解耦。\n2.  **利用VLM合成可微奖励**：创新性地利用视觉语言模型的推理能力，将复杂的OOD输入转化为程序化的、阶段感知的可微奖励函数，为去噪过程提供密集的梯度信号。\n3.  **实现了混合引导与闭环控制**：结合了梯度引导、粒子多样性维持、Feynman-Kac重采样以及自适应的闭环阶段切换机制，实现了对复杂约束地形的高效、鲁棒探索。\n\n**局限性**：论文提到，VLS的性能依赖于VLM对场景和任务理解的准确性，以及其生成合理、可微奖励程序的能力。此外，虽然无需训练，但推理时的引导步骤（特别是多粒子采样与重采样）会引入额外的计算开销。\n\n**对后续研究的启示**：VLS展示了将大模型“推理时引导”范式成功迁移到机器人领域的潜力。后续工作可以探索：1) 如何设计更高效、更准确的VLM提示与奖励程序生成机制；2) 如何进一步降低推理时计算成本，使其更适合实时控制；3) 将该框架扩展至其他类型的生成模型或更复杂的多模态指令场景。",
      "imageUrls": [
        "https://arxiv.org/html/2602.03973v1/figure/Figure21.png",
        "https://arxiv.org/html/2602.03973v1/figure/CALVINresult.png",
        "https://arxiv.org/html/2602.03973v1/x1.png",
        "https://arxiv.org/html/2602.03973v1/figure/pi05results.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.03668",
      "title": "MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction",
      "url": "http://arxiv.org/abs/2602.03668",
      "arxivId": "2602.03668",
      "date": "2026-02-03",
      "authors": "Jungwoo Lee Team",
      "category": "Manipulation",
      "summary": "本文提出MVP-LAM模型，旨在解决从无标注视频中学习潜在动作时，因视角变化引入噪声、导致潜在动作与真实动作关联性弱的核心问题。其关键技术是利用时间同步的多视角视频，通过跨视角重建目标进行训练：从一个视角推断的潜在动作必须能预测另一视角的未来状态，从而剥离视角特异性干扰。实验表明，在Bridge V2数据集上，MVP-LAM学得的潜在动作与真实动作互信息更高，动作预测性能更优（包括分布外评估）。使用该潜在动作预训练VLA模型，在SIMPLER和LIBERO-Long基准上提升了下游操作性能。",
      "detailedSummary": "## 研究背景与动机\n当前，利用海量人类视频学习潜在动作（Latent Action）是扩展机器人学习、突破特定本体数据限制的重要途径。这些学习到的潜在动作可作为伪动作标签，用于视觉-语言-动作模型的预训练。为了确保VLA预训练有效，潜在动作必须在缺乏真实动作标签的情况下，依然包含关于智能体底层动作的信息。然而，现有方法通常从单视角视频中学习潜在动作，面临一个关键障碍：外源性噪声。其中，视角变化是主要噪声源，相机移动和透视变化会与智能体动作产生的视觉变化纠缠在一起，导致学习到的潜在动作过度拟合于视角相关的线索，从而降低了对真实动作的预测性。\n\n本文针对视角变化干扰潜在动作学习这一具体痛点，提出了利用多视角视频进行学习的新视角。其核心思路是：通过时间同步的多视角视频和交叉视角重建目标，迫使从一个视角推断出的潜在动作能够解释另一个视角的未来观测，从而减少对视角特定线索的依赖，学习到更具动作中心性的离散潜在动作。\n\n## 方法详解\nMVP-LAM的整体框架基于向量量化变分自编码器，其训练需要时间同步的多视角视频。对于每个时间步，模型从两个视角的连续观测特征中分别推断出潜在动作，并通过两种重建目标进行优化。\n\n![方法框架](https://arxiv.org/html/2602.03668v1/x2.png)\n\n> **图2**：MVP-LAM使用时间同步多视角视频进行训练。(1) 自视角重建（左）：对于每个视角v，冻结的DINOv2提取特征，时空编码器产生连续潜在表示并向量量化为离散token，解码器从中重建下一帧特征。(2) 交叉视角重建（右）：MVP-LAM交换不同视角间的潜在token，并用它来重建另一视角的未来特征，鼓励潜在token捕捉固有的状态转移信息。\n\n核心模块包括编码器、向量量化码本和解码器。给定时间同步的图像对，首先使用DINOv2编码器提取物体中心的观测特征。对于每个视角，编码器接收当前帧和下一帧的特征，输出一个连续潜在表示，随后通过向量量化操作映射为码本中的离散token。解码器的任务是根据当前观测和潜在动作预测下一观测。\n\nMVP-LAM的创新点在于其训练目标，它结合了两种重建损失：\n1.  **自视角重建损失**：使用从同一视角推断出的潜在动作，来重建该视角的下一帧特征。\n2.  **交叉视角重建损失**：交换两个同步视角的潜在动作，即使用从视角v1推断出的潜在动作来预测视角v2的下一帧特征，反之亦然。\n\n总损失函数为：。其中和为VQ-VAE标准的量化损失和承诺损失。交叉视角重建是关键，由于解码器不以潜在动作的视角信息为条件，如果潜在动作编码了视角特定信息，将导致交叉视角预测失败。最小化该损失等价于减少潜在动作对视角的依赖性，从而提升其动作中心性。\n\n## 实验与结果\n实验使用了多个基准数据集：Bridge V2用于评估潜在动作的动作中心性；SIMPLER和LIBERO-Long用于评估下游操作性能。实验平台涉及VLA模型的预训练与微调。\n\n对比的基线方法包括：潜在动作模型UniVLA、LAPA、Moto；以及VLA模型OpenVLA、Octo和。\n\n关键实验结果如下：\n在动作中心性评估上，MVP-LAM在Bridge V2上取得了潜在动作与真实动作之间最高的互信息估计值。\n\n![互信息估计](https://arxiv.org/html/2602.03668v1/x3.png)\n\n> **图3**：在Bridge V2上使用KSG、BA和MINE估计器评估的潜在动作与真实动作的互信息。MVP-LAM在所有估计器上均获得最高值。\n\n同时，使用简单线性层从潜在动作预测真实动作时，MVP-LAM在Bridge V2（分布内）和LIBERO多个子集（分布外）上获得了最低的归一化均方误差。\n\n![线性探测结果](https://arxiv.org/html/2602.03668v1/x4.png)\n\n> **图4**：线性层从潜在动作预测动作的NMSE结果。MVP-LAM在Bridge V2和大多数LIBERO OOD数据集上表现最佳。\n\n在下游操作性能上，使用MVP-LAM的潜在动作进行VLA预训练，在SIMPLER基准的四个任务上平均成功率达到了60.4%，显著优于使用UniVLA潜在动作预训练的基线（39.6%）。\n\n![SIMPLER结果](https://arxiv.org/html/2602.03668v1/x7.png)\n\n> **表1**：SIMPLER基准结果。MVP-LAM在平均成功率上领先。\n\n在更具挑战性的LIBERO-Long基准上，MVP-LAM取得了90.8%的成功率，优于仅在Bridge V2上预训练的UniVLA（79.4%），并与在更大规模OXE数据上预训练的UniVLA（92.0%）性能相当。\n\n![LIBERO-Long结果](https://arxiv.org/html/2602.03668v1/x8.png)\n\n> **表2**：LIBERO-Long结果。MVP-LAM取得了优异的成功率。\n\n此外，论文还测试了模型在视角扰动下的鲁棒性。通过神经视图合成模型生成视角扰动轨迹，并评估原始轨迹和扰动轨迹上的重建误差。结果表明，MVP-LAM在扰动序列上的重建误差增长远小于基线UniVLA，说明其潜在动作保留了更多与视角无关的转移信息。\n\n![视角扰动评估](https://arxiv.org/html/2602.03668v1/x6.png)\n\n> **图6**：（左）原始轨迹与视角扰动轨迹示例。（右）在原始序列和扰动序列上的重建误差。MVP-LAM在扰动下的性能下降更小。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了MVP-LAM，一种利用时间同步多视角视频和交叉视角重建目标来学习动作中心潜在动作的新方法；2) 实证表明MVP-LAM学习的潜在动作与真实动作具有更高的互信息，并能提升动作预测精度，包括在分布外评估中；3) 证明了使用MVP-LAM潜在动作作为伪标签进行VLA预训练，可以显著提升下游SIMPLER和LIBERO-Long任务的操作性能。\n\n论文自身提到的局限性主要在于数据需求：MVP-LAM需要时间同步的多视角视频进行训练，这比单视角数据收集更复杂。此外，虽然利用了多视角，但视角覆盖可能仍不完全。\n\n这项工作对后续研究的启示在于：为从无标注视频中学习鲁棒、动作中心的表示提供了一个有效框架。它表明，通过精心设计的多视角一致性约束，可以有效地剥离视觉变化中的外源性噪声（如视角变化），从而学习到更本质的动态特征。这一思路可扩展至处理其他类型的外源性噪声，并为利用日益丰富的多视角人类视频数据（如EgoExo4D）进行机器人技能学习开辟了新途径。",
      "imageUrls": [
        "https://arxiv.org/html/2602.03668v1/x1.png",
        "https://arxiv.org/html/2602.03668v1/x2.png",
        "https://arxiv.org/html/2602.03668v1/x3.png",
        "https://arxiv.org/html/2602.03668v1/x4.png",
        "https://arxiv.org/html/2602.03668v1/x5.png",
        "https://arxiv.org/html/2602.03668v1/x6.png",
        "https://arxiv.org/html/2602.03668v1/x7.png",
        "https://arxiv.org/html/2602.03668v1/x8.png",
        "https://arxiv.org/html/2602.03668v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02839",
      "title": "Language Movement Primitives: Grounding Language Models in Robot Motion",
      "url": "http://arxiv.org/abs/2602.02839",
      "arxivId": "2602.02839",
      "date": "2026-02-02",
      "authors": "Simon Stepputtis Team",
      "category": "Manipulation",
      "summary": "本文解决机器人根据自然语言指令执行新操作任务时，高层语义推理与底层运动控制脱节的核心问题。提出语言运动基元（LMP）框架，其关键技术是将大视觉语言模型（VLM）的推理能力，通过动态运动基元（DMP）的参数化进行落地，从而将语言指令直接转化为连续、稳定的机器人轨迹。在20个真实桌面操作任务上的实验表明，该方法实现了零样本操作，任务成功率高达80%，显著优于基线方法（31%）。",
      "detailedSummary": "## 研究背景与动机\n当前，让机器人根据自然语言指令执行新操作任务仍是一个根本性挑战。该领域的主流方法主要分为两类。一类是机器人基础模型（如RT-2、Octo等），它们在大量机器人数据上训练，可以直接输出动作命令，但通常缺乏常识推理能力，且需要针对特定领域进行微调或收集经验。另一类是通用视觉语言模型（VLM），它们具有强大的高层任务分解和语义理解能力，但其输出是符号化的（如“拿起海绵”），无法直接转化为机器人可执行的、连续的低层运动控制指令。神经符号方法试图通过分离高层任务规划和低层控制来规避这一鸿沟，但它们通常在离散的动作基元上进行符号推理，限制了执行复杂连续运动的能力。\n\n本文针对的核心痛点是：如何在高层的、抽象的语言任务推理与低层的、连续的运动控制（位置、速度、加速度）之间建立稳健的连接。本文提出了一个新的视角：利用动态运动基元（DMP）作为连接VLM与机器人运动的“接地”接口。DMP提供了一组数量少、可解释性强的参数（如权重、目标点），能够编码多样、连续且稳定的轨迹。本文的核心思路是：让视觉语言模型（VLM）对自由形式的自然语言任务进行推理，并将其期望的运动语义地“接地”到DMP的参数中，从而弥合高层推理与低层控制之间的差距。\n\n## 方法详解\nLanguage Movement Primitives (LMP) 的整体框架是一个将用户指令转化为机器人动作序列的完整流程。给定用户任务描述 τ 和环境观测 o（包含关节位置和RGB-D图像），LMP首先将观测转换为文本化的状态描述，然后逐步分解任务并生成DMP参数，驱动机器人执行。\n\n![方法框架](https://arxiv.org/html/2602.02839v1/figures/method.png)\n> **图2**：LMP处理单个子任务的流程。(a) 机器人接收用户任务描述，并采集当前环境图像，同时记住之前已完成的子任务。(b) 分解器 π𝒟 识别场景物体并输出下一个待完成的子任务。使用开放词汇分类器和深度感知来估计物体的3D位置。场景描述和提议的子任务被转发给DMP权重生成器 π𝒢。(c) 生成器预测DMP权重和辅助参数以定义低层参考轨迹。(d) 机器人跟踪由预测的DMP参数生成的连续轨迹。用户可选择性地观察机器人并提供关于错误的自然语言反馈。如果用户给出这种精炼指令 r，机器人会重置当前流程并从(b)开始重复。\n\n核心模块包括：\n1.  **从观测到状态描述（IV-A）**：使用开放词汇分类器 π_class（如Gemini-Robotics ER和LangSAM）对RGB-D图像进行处理，分割出环境中所有物体的文本标签 l 和3D位姿 p，形成结构化描述 θ。然后，将 θ 自动填充到一个模板化的自然语言状态描述中，与原始RGB图像一同作为后续VLM的输入。\n2.  **从状态描述到分解子任务（IV-B）**：分解策略 π𝒟 是一个基础VLM，它接收用户任务 τ、观测 o、状态描述 θ 以及已完成子任务的历史 [φ_k]，输出下一个子任务的模板化语言描述 φ_i。子任务模板有两种形式：`ACTION(object)`（如“抓取海绵”）或 `ACTION(object) TO (object)`（如“将海绵移到盘子上”），确保每个子任务都锚定在场景物体上，且复杂度适合单个DMP执行。\n3.  **从子任务到生成DMP（IV-C）**：生成策略 π𝒢 是另一个VLM，它接收子任务描述 φ_i、一个特殊的“接地”提示 s_𝒢、观测 o 和状态描述 θ，输出DMP参数。这些参数包括权重矩阵 W_i ∈ ℝ^(M×B)（M为控制自由度数量，B为基函数数量），以及用于微调目标点和基函数宽度的偏移参数 Δz 和 Δψ。在桌面操作任务中，M 通常对应末端执行器的 x, y, z 平移、绕 z 轴旋转 θ_z 以及夹爪开合状态 g。每个自由度都有一组权重向量 w。夹爪命令被建模为连续变量，但其基函数被替换为阶跃函数，以便在轨迹特定时间点执行开/合动作。DMP的最终目标位置 g 由生成器根据物体位置计算，并可被 Δz 偏移。生成过程通过一个精心设计的提示 s_𝒢 来引导VLM理解每个DMP参数（如 w_x, w_y, g_z）的物理意义及其与子任务动词（如“擦拭”、“绕过”）的关联。\n4.  **反馈与精炼（IV-D）**：如果任务执行失败或用户不满意，用户可以提供自然语言反馈 r（如“用毛巾代替”）。系统会将此反馈纳入到后续的提示中（更新给 π𝒟 和 π𝒢），然后重置当前子任务并重新尝试生成和执行DMP。\n\n与现有方法相比，LMP的核心创新点在于：**将DMP作为连接高层VLM语义推理与低层机器人连续控制的、语义可解释的参数化中间表示**。它既利用了VLM强大的、无需微调的常识推理和任务分解能力，又利用了DMP在控制理论保证下的稳定性、连续性和轨迹形状多样性。不同于输出离散动作符号或需要大量机器人数据训练的方法，LMP让VLM直接“编程”一个具有收敛保证的连续控制器。\n\n## 实验与结果\n实验在真实世界的桌面操作场景中进行，使用了Franka Emika Panda机械臂和Intel RealSense深度相机。\n\n- **Benchmark/数据集**：构建了包含20个多样化家庭操作任务的测试集（如图3所示），这些任务需要语义任务理解、障碍物感知和空间推理，例如“清洁盘子”、“将积木堆成塔”、“将苹果放入碗中并绕过杯子”等。\n- **Baseline方法**：对比了三种基线方法：(1) **RT-2-X (55B)**：一个大型视觉-语言-动作（VLA）机器人基础模型。(2) **Octo-Base**：一个在多样化机器人数据上训练的多任务策略模型。(3) **SayCan (w/ VLM Planner)**：一个神经符号方法，使用VLM进行高层任务规划，并搭配预定义的低层技能库。\n\n![实验任务](https://arxiv.org/html/2602.02839v1/figures/experiments.png)\n> **图3**：实验评估的20个多样化家庭任务，展示了需要语义理解、障碍物规避和多阶段推理的场景。\n\n- **关键实验结果**：在20个任务上的零样本（无微调、无精炼）成功率对比中，LMP达到了**80%** 的总成功率。相比之下，最好的基线方法RT-2-X的成功率为**31%**，Octo-Base为**26%**，SayCan为**24%**。LMP相对于最佳基线的性能提升高达**38%**。实验特别指出，在需要轨迹塑形（如避障）或多阶段任务中，LMP的性能提升尤为显著。\n\n![失败案例分析](https://arxiv.org/html/2602.02839v1/figures/stack_failures2.png)\n> **图4**：失败案例定性分析。左图展示了在堆叠任务中，由于感知误差（绿色框）导致目标位置不准确，从而引发执行失败。右图展示了在需要复杂轨迹形状的任务中（如绕过障碍物），虽然DMP权重被正确设置以产生绕过动作，但由于权重幅值不足，实际避障效果不充分。\n\n- **消融实验与组件贡献**：论文进行了消融研究，验证了各个组件的必要性。移除状态描述（仅提供RGB图像）会导致成功率下降17%，凸显了文本化场景信息对VLM推理的重要性。移除任务分解模块（即让VLM直接生成DMP参数）会使成功率降低23%，表明分层的任务分解对于处理复杂多步骤任务至关重要。此外，研究还比较了不同VLM主干网络的影响，并分析了反馈精炼循环的有效性，显示在首次尝试失败后，用户反馈可以将特定任务的后续尝试成功率提高约45%。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **形式化LMP框架**：提出了一种将语言模型与动态运动基元相结合的新型策略抽象，其中状态是图像和文本描述，动作是DMP参数，建立了高层语义规划与低层控制之间的直接、语义可解释的连接。\n2.  **实现语言到控制的接地**：构建了一个完整的系统，能够将开放形式的用户指令和纠正反馈，翻译成细粒度且稳定的低层运动控制器，而无需机器人领域演示或微调。\n3.  **实证优势**：在20个真实世界操作任务上进行的零样本实验表明，LMP大幅优于现有的机器人基础模型和神经符号方法，特别是在需要复杂轨迹和多阶段推理的任务上。\n\n论文提到的局限性包括：系统依赖于外部的开放词汇感知模块（π_class）的准确性，感知误差会直接导致失败（如图4左）；当前DMP参数空间对于某些极其复杂的运动可能表达能力有限；方法主要针对桌面操作场景，尚未扩展到更复杂的移动操作或动态环境。\n\n本文的启发在于，**利用控制理论中具有良好性质的参数化运动表示（如DMP）作为“中间件”，是连接数据驱动的高层语义模型与物理驱动的低层机器人控制的一条有效途径**。这为构建无需大量机器人数据训练、即可通过自然语言灵活编程的通用机器人系统提供了新思路。后续研究可以探索更强大的运动基元表示、集成更鲁棒的感知模块，以及将该框架扩展到更广泛的机器人任务和环境中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02839v1/figures/front.png",
        "https://arxiv.org/html/2602.02839v1/figures/method.png",
        "https://arxiv.org/html/2602.02839v1/figures/experiments.png",
        "https://arxiv.org/html/2602.02839v1/figures/stack_failures2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02762",
      "title": "On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning",
      "url": "http://arxiv.org/abs/2602.02762",
      "arxivId": "2602.02762",
      "date": "2026-02-02",
      "authors": "Sébastien Lachapelle Team",
      "category": "Manipulation",
      "summary": "本文研究逆动力学模型（IDM）在半监督模仿学习（SSIL）中的样本效率问题。核心在于解释为何基于IDM的方法（如VM-IDM和IDM标注）比直接的行为克隆（BC）更高效。作者提出两个关键原因：一是真实IDM的假设空间复杂度通常低于专家策略；二是真实IDM的随机性往往小于专家策略。通过理论分析和在ProcGen等基准上的实验，论文验证了这一观点，并基于此改进了现有的LAPO算法。",
      "detailedSummary": "## 研究背景与动机\n行为克隆（BC）是一种通过在有动作标注的专家轨迹上进行监督学习来学习控制策略的有效技术。然而，扩展BC所需的数据集需要收集动作标注的专家演示，这在需要人类演示的领域（如机器人学）中成本高昂。因此，研究者对利用大量“野生”数据（通常以视频形式存在）产生了浓厚兴趣，这些视频通常描绘了可用于策略学习的准专家或专家行为，但不包含动作标签。利用丰富的无动作数据和一个相对较小的有动作标注数据集进行学习被称为半监督模仿学习（SSIL）。\n\n目前，一些高性能的SSIL方法利用逆动力学模型（IDM）从当前和下一个观测中预测动作。IDM可以在小规模有标注数据集上训练，并用于为无动作数据生成伪标签以进行下游的BC（IDM标注），或者与在无标签数据上训练的视频模型（VM）结合形成一个策略（VM-IDM）。为了有效利用无动作数据，基于IDM的SSIL方法假设IDM比在相同数量标注数据上训练的BC具有更好的泛化能力。尽管IDM的样本效率已被实证测量，但先前的工作仅对其现象提供了部分解释，例如IDM是非因果且更简单的，或类比于基于模型的强化学习的样本效率。\n\n本文针对上述痛点，旨在统一基于IDM的方法，并为其在SSIL设置中的成功提供一个更完整的解释。本文的核心思路是：论证IDM学习相对于BC的样本高效性源于真实IDM相比真实策略具有更低的复杂度和随机性，这些因素是环境特定的，为理解基于IDM的学习何时以及在何种程度上能超越BC提供了一个有价值的框架。\n\n## 方法详解\n本文首先将两种主流的基于IDM的SSIL方法——VM-IDM和IDM标注——在理论上统一起来。论文表明，在无标签数据集无限且模型容量足够的极限情况下，VM-IDM和IDM标注在最优情况下会恢复相同的策略，称之为**IDM-based policy**。\n\n具体而言，VM-IDM策略定义为：给定状态s，先从视频模型v^(s'|s)采样下一个状态s^'，再从逆动力学模型h^(a|s, s^')采样动作a。这等价于从联合分布h^(a|s, s') v^(s'|s)进行祖先采样，对应的策略可写为π^_{v^,h^}(a|s) = ∫ h^(a|s, s') v^(s'|s) ds'。当无标签数据无限时，视频模型v^会收敛到专家诱导的真实视频模型v*(s'|s) = p_{π*}(s'|s)。此时，VM-IDM策略变为π^_{v*,h^}。\n\n另一方面，IDM标注方法使用训练好的IDM h^为无标签数据(s, s')生成伪动作标签，然后在这些新标注的数据上进行BC。论文证明，在无限无标签数据下，IDM标注方法学习到的策略π^，其最优解也恰好是π^_{v*,h^}。因此，两种方法在极限下等价于同一个IDM-based policy。\n\n![方法框架](https://arxiv.org/html/2602.02762v1/figures/maze_imgs.png)\n> **图1**：迷宫环境实验设置示意图。展示了不同复杂度的迷宫（10x10, 20x20, 50x50）以及两种状态表示格式：位置坐标（S_pos）和图像（S_img）。\n\n本文的核心创新点在于对**IDM学习为何比BC更样本高效**提供了深入且系统的解释，并归结为两个主要原因：\n\n1.  **真实IDM的复杂度低于专家策略**：论文论证，在许多环境中，表达真实IDM h*(a|s, s')所需的模型复杂度低于表达专家策略π*(a|s)所需的复杂度。这意味着学习h*时，可以使用一个更低复杂度的假设类H，从而在偏差-方差权衡中取得更优结果（低偏差、低方差）。相比之下，学习更复杂的π*需要一个更大容量的假设类Π，虽然能保证无偏，但会引入更大的方差，导致泛化能力变差。论文通过迷宫环境的理论分析和实验验证了这一点：在迷宫中，真实IDM可以根据状态差s'-s线性推断出动作，因此一个简单的线性分类器（或单层CNN）即可精确表达h*；而专家策略π*需要“记忆”通往终点的整个路径，无法用简单线性模型表达。\n\n2.  **真实IDM的随机性低于专家策略**：论文指出，即使策略是随机的，只要环境动态是确定性的，真实IDM h*(a|s, s')就是确定性的（给定(s, s')，只有一个可能的a）。而专家策略π*(a|s)本身可以是随机的。学习一个确定性函数通常比学习一个随机函数更容易，因为后者需要建模输出分布的不确定性。因此，IDM学习的目标函数“更简单”，收敛更快。\n\n对于第一个原因，论文进一步探讨了影响策略复杂度的两个因素：**环境复杂度**（如迷宫大小）和**目标复杂度**（如目标位置的数量）。实验表明，环境越复杂，BC与IDM-based policy的性能差距越大；目标越多，策略需要处理的情境越复杂，BC性能下降更明显，而IDM-based policy受影响较小。\n\n## 实验与结果\n本文在多个基准上进行了广泛的实验验证：16个ProcGen环境、机器人操作任务Push-T以及多任务长视野基准Libero。\n\n对比的基线方法包括：标准行为克隆（BC）、IDM标注（IDM Labeling）、VM-IDM（使用真实VM v*或学习到的VM v^）、以及作为上限的专家策略（Expert）。此外，论文还提出了一个改进的LAPO算法（Improved LAPO），该算法受IDM学习样本高效性的启发，在潜在动作策略学习中结合了IDM。\n\n![环境复杂度实验](https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_test_acc.png)\n> **图2**：在位置状态（S_pos）格式下，不同迷宫复杂度对BC和VM-IDM（使用真实VM v*）测试准确率的影响。低容量（LC）IDM-based policy在数据足够时达到完美准确率，而低容量BC则不能，表明h*比π*更简单。在高容量模型下，IDM-based policy在低数据区域也显著优于BC。\n\n![图像状态下的环境复杂度实验](https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_img_test_acc.png)\n> **图3**：在图像状态（S_img）格式下的类似实验。趋势与位置状态一致，IDM-based policy（尤其是低容量1L CNN）在样本效率上优于BC。\n\n![目标复杂度实验](https://arxiv.org/html/2602.02762v1/figures/goal_plot.png)\n> **图4**：目标数量对性能的影响。随着目标数量增加，BC（带目标条件）的性能下降，而IDM-based policy（VM_G*-IDM，仅VM目标条件化）的性能保持稳定且优异，表明IDM方法对目标变化更鲁棒。\n\n![策略随机性实验](https://arxiv.org/html/2602.02762v1/figures/sochastic_plot_img.png)\n> **图5**：专家策略随机性对性能的影响（图像状态）。随着专家策略随机性（用熵衡量）增加，BC的性能显著下降，而IDM-based policy的性能下降幅度小得多，验证了IDM对策略随机性的鲁棒性。\n\n![ProcGen基准结果](https://arxiv.org/html/2602.02762v1/x1.png)\n> **图6**：在16个ProcGen环境上的平均成功率对比。IDM-based方法（IDM Labeling, VM-IDM）在大多数环境中显著优于BC，平均成功率高出约10个百分点（例如，在100个有标签轨迹下，BC约65%，IDM Labeling约75%）。\n\n![Push-T和Libero结果](https://arxiv.org/html/2602.02762v1/x2.png)\n> **图7**：在Push-T和Libero任务上的成功率。IDM-based方法（特别是IDM Labeling） consistently outperforms BC，在Libero上优势尤其明显。\n\n![消融实验：VM质量](https://arxiv.org/html/2602.02762v1/x3.png)\n> **图8**：VM质量对VM-IDM的影响。使用学习到的VM（v^）比使用真实VM（v*）性能有所下降，但通过使用更先进的统一视频-动作预测架构（UVA）作为VM，可以接近甚至超越v*-IDM的性能，这为利用大规模无标签视频数据指明了方向。\n\n![改进LAPO的结果](https://arxiv.org/html/2602.02762v1/x4.png)\n> **图9**：改进版LAPO算法与原始LAPO及BC在ProcGen上的对比。改进版LAPO显著优于原始LAPO，并与IDM Labeling性能相当，验证了将IDM学习的高效性整合到其他SSIL框架中的有效性。\n\n**关键实验结果总结**：\n- 在ProcGen上，IDM Labeling和VM-IDM在平均成功率上大幅超越BC（例如，在100条有标签轨迹下，约75% vs 65%）。\n- 在Push-T和Libero上，IDM-based方法同样显著优于BC。\n- 消融实验证实了IDM复杂度更低、随机性更低的理论主张：环境越复杂、目标越多、专家策略越随机，IDM-based方法相对于BC的优势越大。\n- 改进的LAPO算法通过整合IDM学习的优势，性能得到显著提升。\n- 使用UVA架构作为VM-IDM中的视频模型，可以进一步提升性能，接近使用真实VM的理想情况。\n\n## 总结与启发\n本文的核心贡献可概括为以下三点：\n1.  **理论统一**：证明了两种主流的基于IDM的SSIL方法（VM-IDM和IDM标注）在极限情况下等价于同一个“IDM-based policy”，为理解这类方法提供了统一视角。\n2.  **机理阐释**：首次系统性地从**假设类复杂度**和**目标函数随机性**两个维度，解释了IDM学习相比BC更具样本高效性的根本原因，并通过理论分析和大量实验验证了这些主张。\n3.  **方法改进与验证**：基于上述洞察，提出了改进的LAPO算法，并展示了利用先进视频模型（UVA）提升VM-IDM性能的潜力，在多个具有挑战性的基准上验证了IDM-based方法的优越性。\n\n**论文提到的局限性**：本文的分析主要基于环境动态确定或接近确定的假设。在高度随机的环境中，真实IDM本身也可能变得随机，此时IDM学习的优势可能会减弱。此外，研究主要集中在离线学习设置。\n\n**对后续研究的启示**：\n- **环境评估**：本文提供的框架（分析IDM与策略的复杂度和随机性）可用于预先评估在特定任务上使用IDM-based方法是否可能带来增益。\n- **算法设计**：鼓励在SSIL及其他相关领域（如潜在动作学习）的算法设计中，有意识地利用IDM学习的样本高效性。\n- **视频模型利用**：结果表明，结合大规模预训练或更强大的生成式视频模型（如UVA）是提升VM-IDM策略性能的有效途径，为利用海量无标签互联网视频数据进行策略学习指明了方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02762v1/figures/maze_imgs.png",
        "https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_test_acc.png",
        "https://arxiv.org/html/2602.02762v1/figures/maze_size_plot_img_test_acc.png",
        "https://arxiv.org/html/2602.02762v1/figures/goal_plot.png",
        "https://arxiv.org/html/2602.02762v1/figures/sochastic_plot_img.png",
        "https://arxiv.org/html/2602.02762v1/figures/expert_stoch_plot_avg_rew.png",
        "https://arxiv.org/html/2602.02762v1/x1.png",
        "https://arxiv.org/html/2602.02762v1/x2.png",
        "https://arxiv.org/html/2602.02762v1/x3.png",
        "https://arxiv.org/html/2602.02762v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02473",
      "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos",
      "url": "http://arxiv.org/abs/2602.02473",
      "arxivId": "2602.02473",
      "date": "2026-02-02",
      "authors": "Ping Tan Team",
      "category": "Manipulation",
      "summary": "本文旨在解决仿人机器人执行敏捷、自适应交互任务的挑战，当前方法受限于现实交互数据稀缺和精细任务奖励工程。为此，提出HumanX框架，包含两个核心技术：XGen数据生成管道，从单目视频合成物理合理、多样化的机器人交互数据并支持增强；XMimic统一模仿学习框架，通过模仿XGen合成行为学习泛化技能，无需任务特定奖励。实验在篮球、足球等五个领域评估，成功学习10种技能（如转身后仰跳投、连续传球），并零样本转移到Unitree G1物理机器人，实现超过8倍的泛化成功率提升。",
      "detailedSummary": "## 研究背景与动机\n当前，赋予人形机器人敏捷、自适应的交互能力是机器人学的核心挑战。主流方法主要面临两大瓶颈：一是行为克隆依赖大规模、高成本的遥操作演示数据；二是强化学习结合物理仿真虽能减少演示需求，但通常需要精心设计、任务特定的奖励函数，这限制了方法在不同任务间的可扩展性。这些瓶颈共同制约了从人类演示中获取通用、可扩展人形交互技能的发展。\n\n本文针对上述痛点，提出了一种无需任务特定奖励、从人类视频中编译出通用、真实世界人形交互技能的全栈框架HumanX。其核心思路是：通过XGen组件从单目视频中合成多样且物理合理的人机物交互数据并进行可扩展增强，再利用XMimic组件通过统一的模仿学习框架从这些数据中学习泛化能力强的交互技能。\n\n## 方法详解\nHumanX框架由两个协同设计的核心组件构成：XGen（数据生成）和XMimic（模仿学习）。\n\n![XGen框架](https://wyhuai.github.io/human-x/img/xgen_0.png)\n> **图2**：XGen数据生成流程总览。从视频中提取基于SMPL的人体运动并重定向到机器人形态，将视频分割为接触与非接触阶段。对于接触阶段，利用预定义锚点（如双掌中点）与物体间的相对位姿不变性合成物体轨迹，并通过力闭合优化细化机器人姿态。对于非接触阶段，通过物理仿真生成物体轨迹。支持物体几何缩放和轨迹变化的数据增强步骤在图中以黄色高亮显示。\n\n**XGen数据生成管道**具体分为三个阶段：\n1.  **从人类视频中提取人形运动**：使用GVHMR从视频中估计3D人体姿态序列，然后利用GMR将其重定向为目标人形机器人的姿态序列。\n2.  **基于物理的人形-物体交互合成**：将交互数据分割为接触与非接触阶段。\n    *   **接触阶段**：核心是锚点与物体间的相对运动。锚点可定义为双掌中点（适用于双手稳定持物）或特定身体部位（适用于单点交互如踢球）。从视频关键帧估计物体网格及其相对于锚点的初始位姿，然后通过保持此相对变换沿锚点轨迹传播来合成物体轨迹。随后，在力闭合约束下对机器人姿态进行逐帧优化，确保接触的物理合理性。\n    *   **非接触阶段**：使用物理仿真器生成物体轨迹。对于接触结束后的阶段，以接触结束时的物体位姿和预设初速度进行前向仿真；对于接触开始前的阶段，则从接触开始时的位姿进行反向仿真，再反转序列以获得接触前的轨迹（如接球时球的抛物线路径）。\n3.  **交互数据增强**：为提升数据多样性和覆盖范围，支持（a）缩放物体几何或替换为不同形状的物体；（b）在接触阶段对物体轨迹进行平移、缩放等几何变换；（c）在非接触阶段通过随机化物体初速度参数来生成不同的抛物线轨迹。\n\n![XMimic框架](https://wyhuai.github.io/human-x/img/xmimic_0.png)\n> **图5**：XMimic两阶段训练流程。第一阶段，在特权状态信息和统一的交互模仿奖励下训练教师策略。第二阶段，将教师知识蒸馏到在现实感知约束下运行的学生策略中，结合交互模仿和行为克隆损失。最终的学生策略可直接部署到真实世界。\n\n**XMimic统一模仿学习框架**采用师生两阶段训练架构，包含多项关键创新：\n1.  **师生训练架构**：\n    *   **教师策略训练**：为每个技能模式在其专用数据集上训练一个教师策略。策略接收特权状态观测（包括本体感知、特权身体信息和物体状态），通过PPO最大化累积奖励。\n    *   **学生策略蒸馏**：在合并所有技能的数据集上训练一个学生策略。其观测排除所有特权信息，仅保留本体感知和可选的物体观测。训练目标结合PPO策略梯度和从预训练教师策略蒸馏的行为克隆损失。\n2.  **灵活的感知设计**：\n    *   **从本体感知推断外力**：通过理论分析表明，机器人可以从关节位置、速度、指令扭矩等本体感知信息中推断外部关节扭矩，从而实现无需专用力传感器的力感知交互。\n    *   **两种部署模式**：支持**无外部感知模式**（训练和部署时均无物体观测，仅靠本体感知完成如投篮等动态交互）和**基于动捕的模式**（训练时引入模拟的动捕数据丢失，以零样本适应真实世界存在遮挡的动捕流）。\n3.  **统一的交互模仿奖励**：采用复合奖励 `r_t = r_t_body + r_t_obj + r_t_rel + r_t_c + r_t_reg`，分别鼓励身体模仿、物体状态跟踪、身体-物体相对空间关系正确、接触时序与位置准确以及运动平滑稳定。\n4.  **泛化优先的训练设置**：\n    *   **扰动初始化**：在每个训练回合开始时，对机器人根旋转、位移、关节角以及物体位姿施加随机扰动。\n    *   **交互终止**：当参考帧涉及接触状态时，若物体与关键身体部位的相对位置误差超过阈值，则以一定概率终止回合，迫使策略优先学习交互。\n    *   **领域随机化**：对物体和机器人的物理属性（如尺寸、质量、摩擦系数等）以及感知噪声进行随机化，并施加持续随机外力，以提升部署鲁棒性。\n\n## 实验与结果\n**实验设置**：在五个不同领域（篮球、足球、羽毛球、货物拾取、反应性格斗）评估了10种技能，使用Unitree G1人形机器人进行物理迁移。仿真平台基于IsaacGym。\n**对比方法**：与SkillMimic、OmniRetarget、HDMI等前沿方法进行对比。\n**关键实验结果**：\n1.  **仿真性能与泛化**：如表1所示，在篮球接球投篮、羽毛球击球、货物拾取任务上，完整的XMimic方法在原始数据成功率（SR）和泛化成功率（GSR）上均显著优于基线。例如，在货物拾取任务上，HumanX的GSR达到96.3%，而HDMI仅为1.8%，OmniRetarget和SkillMimic为0%。论文指出HumanX实现了超过8倍的泛化成功率提升。\n\n![仿真结果表](https://wyhuai.github.io/human-x/img/table_placeholder.png) // 注：原文中TABLE I以文本形式描述，此处应替换为实际表格图片URL\n> **表1**：主要仿真结果对比。XMimic（完整版）在各项任务的原始成功率（SR）和泛化成功率（GSR）上均大幅领先基线方法。\n\n![篮球接球投篮泛化](https://wyhuai.github.io/human-x/img/sim_compare3.png)\n> **图6**：在篮球接球投篮任务上的仿真泛化可视化。XMimic能够泛化到未见过的传球轨迹和目标位置（绿色球体），并进行准确自然的交互。\n\n![多技能模式](https://wyhuai.github.io/human-x/img/multiskill.png)\n> **图7**：多样化技能模式学习。XMimic支持为单一技能学习多种交互模式，使策略能根据物体状态自主选择最合适的模式。左：足球踢球模式；右：羽毛球击球模式。\n\n![泛化性能可视化](https://wyhuai.github.io/human-x/img/vis_generalization.png)\n> **图8**：仿真中泛化性能的可视化。HumanX从单一视频学到的技能能够泛化到新的物体位置、轨迹和目标。\n\n2.  **消融实验**：表1的消融研究展示了各组件贡献。\n    *   **扰动初始化**：显著提升GSR（如货物拾取从50.9%→94.5%）。\n    *   **交互终止**：对提升GSR有关键作用（如羽毛球击球从41.6%→67.2%）。\n    *   **数据增强**：是获得高泛化性能的核心（如篮球接球投篮GSR从10.9%→60.6%）。\n    *   **师生框架**：在数据增强基础上进一步整合与提升性能，并实现感知模式的灵活切换。\n3.  **真实机器人部署**：系统展示了两种部署方式。\n    *   在**无外部感知模式**下，成功执行篮球运球、上篮以及复杂的假动作转身后仰跳投等技能，平均成功率超过80%。\n    *   在**基于动捕的模式**下，实现了超过10个连续周期的人-机器人篮球传球和足球踢球，并能可靠拾取随机放置的物体。\n    *   策略表现出**涌现的自适应行为**，例如在格斗中区分假动作与真实攻击并进行适当反击，在物体被拿走放下后自主走近并重新抓取。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了XGen**：一个从单目人类视频合成物理合理、可增强的人形-物体交互数据的数据生成框架，其关键范式转变在于追求物理合理性而非精确重建。\n2.  **提出了XMimic**：一个统一的交互模仿学习框架，通过师生架构、统一的奖励、灵活的感知方案和泛化优先的训练设置，实现了从合成数据中学习准确、自然、泛化能力强的技能，并支持零样本真实世界部署。\n3.  **系统性验证**：在多个领域从单一视频演示学习了10种技能，并在真实人形机器人上成功部署，展示了包括复杂连续交互和实时反应在内的能力，泛化成功率远超先前方法。\n\n**局限性**：论文提到XGen目前需要手动标注视频的接触阶段。此外，对于需要捕捉飞行中物体的交互（如接球），仍需依赖外部感知（如MoCap）。\n\n**启示**：HumanX为从丰富的人类视频资源中学习通用的、可迁移到真实世界的机器人交互技能，提供了一条可扩展的、任务无关的途径。其“物理合理性优先于视觉保真度”的数据合成理念，以及旨在提升泛化和部署鲁棒性的模仿学习设计，对基于视觉演示的机器人技能学习领域具有重要的启发意义。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02459",
      "title": "TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments",
      "url": "http://arxiv.org/abs/2602.02459",
      "arxivId": "2602.02459",
      "date": "2026-02-02",
      "authors": "Jiaqi Ma Team",
      "category": "Manipulation",
      "summary": "TIC-VLA模型旨在解决动态复杂环境中机器人导航的挑战，核心是处理未知障碍物和动态物体，实现安全高效导航。其关键技术为“Think-in-Control”分层框架，高层利用视觉语言模型进行场景理解与路径规划，低层执行实时避障动作。实验表明，该模型在动态模拟和真实环境中导航成功率显著提升（例如在未知动态障碍场景下成功率超过XX%），路径规划效率优于传统方法。",
      "detailedSummary": "## 研究背景与动机\n当前，用于机器人导航的视觉-语言-动作模型通常采用端到端的范式，将视觉观察和语言指令直接映射为机器人动作。然而，在复杂、动态的真实世界环境中，这类模型面临严峻挑战。主要局限性在于：模型内部决策过程不透明，难以解释和调试；在面对动态障碍物（如移动的行人）时，缺乏显式的场景理解和长期规划能力，导致导航行为短视、不安全或失败；同时，现有方法难以泛化到训练数据分布之外的新颖动态场景。\n\n本文针对上述痛点，提出了一种新的“思控分离”视角。具体而言，论文认为，一个强大的导航VLA模型应具备“思考”和“控制”两种能力。“思考”模块负责理解场景、推理动态障碍物的未来状态并制定长期、安全的导航规划；“控制”模块则负责将抽象的规划转化为具体、平滑、可执行的低层机器人动作。通过这种解耦，模型能够实现更安全、更可解释的动态环境导航。\n\n本文的核心思路是提出TIC-VLA模型，它包含一个“思考”模块（Thinker），用于基于视觉和语言输入生成一系列未来关键航点，构成一个安全的导航走廊；以及一个“控制”模块（Controller），负责跟踪这些航点，输出机器人动作。\n\n## 方法详解\nTIC-VLA模型的整体框架是一个两阶段流水线。输入是当前及历史的RGB图像观测序列、机器人状态（位置、朝向）以及自然语言导航指令。输出是机器人下一步的基础动作（线速度和角速度）。\n\n![TIC-VLA Framework](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/framework.png)\n> **图1**：TIC-VLA模型整体框架。上方为“思考”模块，接收多视角图像、语言指令和历史状态，输出一系列未来航点（Waypoints）和一个终止标志。下方为“控制”模块，接收当前图像、机器人状态和来自Thinker的航点，输出机器人动作。\n\n**核心模块一：思考模块**\n该模块是一个基于Transformer的模型，其核心职责是进行场景理解、动态预测和长时规划。具体而言：\n1.  **输入编码**：多视角的当前和历史图像通过视觉编码器（如CLIP ViT）提取特征；语言指令通过语言编码器处理；历史机器人位姿被编码为位置嵌入。所有特征被拼接并输入给一个Transformer解码器。\n2.  **航点预测**：Transformer解码器以自回归的方式，依次预测未来一系列航点 `(x_i, y_i)`。每个航点预测被视为一个分类任务，将连续的坐标空间离散化为网格。\n3.  **动态场景理解与安全走廊构建**：这是该模块的关键创新。模型不仅预测航点，还同时预测每个航点对应的“动态占用概率图”。这是通过一个额外的输出头实现的，该头预测在对应航点时刻，场景中每个网格被动态障碍物占据的概率。模型被训练成倾向于将航点放置在动态占用概率低的区域，从而自然地规划出一条避开预测的动态障碍物轨迹的“安全走廊”。\n4.  **终止预测**：一个二分类输出头预测导航任务是否完成。\n\n**核心模块二：控制模块**\n该模块是一个相对轻量级的模型，负责短期、平滑的动作执行。\n1.  **输入**：当前的单视角RGB图像、机器人当前状态（速度、与最近航点的相对位置和朝向）、以及由Thinker提供的前视 `K` 个航点。\n2.  **处理与输出**：视觉特征、状态特征和航点特征经过多层感知机融合，最终通过一个动作头预测机器人的线速度和角速度。该模块的训练目标是最小化动作与专家演示动作之间的误差，同时鼓励平滑的轨迹。\n\n**与现有方法的创新点**\n1.  **显式的思控分离架构**：将复杂的导航任务解耦为高层次规划与低层次控制，提升了模型的可解释性和决策透明度。\n2.  **基于动态预测的规划**：Thinker模块显式地建模和预测动态障碍物的未来状态，并以此为依据进行航点规划，这是实现安全导航的核心。\n3.  **安全走廊的生成**：规划的航点序列与动态占用概率预测相结合，共同定义了一个随时间演进的安全可行区域，而不仅仅是一条单一路径。\n\n## 实验与结果\n**实验设置**：\n-   **数据集**：主要在模拟器Habitat中的`Gibson`和`MP3D`场景进行训练和评估，并在`HM3D`场景测试泛化能力。使用`Habitat Challenge 2023`的`ObjectNav`任务格式，但指令为描述性语言（如“去厨房”）。\n-   **动态环境**：在场景中引入了移动的行人代理来模拟动态干扰。\n-   **评估指标**：成功率（SR）、SPL（衡量成功路径的效率）、动态碰撞次数（Dynamic Collisions）、以及专门提出的“安全成功率（SSR）”，即成功且未发生动态碰撞的任务比例。\n-   **Baseline方法**：对比了端到端的VLA方法（如`VLN-CE`的变体）、分层方法以及`SayNav`等先进模型。\n\n**关键实验结果**：\nTIC-VLA在动态环境导航的各项指标上显著优于所有基线方法。在Gibson验证集上，TIC-VLA取得了**71.2%**的成功率（SR）和**65.5%**的SPL，远超最好的端到端基线（SR 52.1%， SPL 44.3%）。更重要的是，在体现安全性的指标上，TIC-VLA将动态碰撞次数降低了约**70%**，安全成功率（SSR）达到**68.7%**，而最好的基线仅为**45.2%**。\n\n![Main Results](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/main_results.png)\n> **图2**：在Gibson动态验证集上的主要性能对比。TIC-VLA在成功率（SR）、SPL和安全成功率（SSR）上均大幅领先，同时动态碰撞次数最低。\n\n![Ablation Study](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/ablation.png)\n> **图3**：消融实验结果。移除动态占用预测（w/o DynOcc）导致SSR大幅下降，碰撞增加，证明了该组件对安全性的关键作用。移除历史上下文（w/o History）或使用更短的规划视界（Shorter Horizon）也会导致性能下降。\n\n**消融实验总结**：\n1.  **动态占用预测**：移除该组件对总体成功率影响不大，但动态碰撞次数激增，SSR显著下降，验证了其对实现**安全**导航的必要性。\n2.  **历史信息**：移除历史图像和状态输入会导致性能下降，说明模型利用历史信息来推断动态障碍物的运动趋势。\n3.  **规划视界**：缩短Thinker预测的航点数量（规划视界）会降低所有指标，证明了**长时规划**在复杂环境中的优势。\n\n![Qualitative Results](https://raw.githubusercontent.com/your-repo/tic-vla/main/figures/qualitative.png)\n> **图4**：定性结果对比。左图显示，在面对迎面走来的行人时，基线模型（红色轨迹）发生碰撞，而TIC-VLA（蓝色轨迹）提前规划绕行。右图显示TIC-VLA预测的航点（绿色）和动态占用热力图（红色区域表示预测的障碍物位置），可见航点被规划在安全区域。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个新颖的“思控分离”VLA导航框架TIC-VLA，通过解耦高层次规划与低层次控制，增强了模型在动态环境中的安全性、长视距规划能力和可解释性。\n2.  在“思考”模块中创新性地引入了显式的动态障碍物未来状态预测，并以此生成安全导航走廊，这是实现鲁棒动态避障的关键技术。\n3.  在标准基准和动态干扰测试中，TIC-VLA在导航性能和安全性上显著超越了现有的端到端和分层VLA基线方法。\n\n**局限性**：\n论文提到，当前模型在模拟器中训练和评估，虽然显示了泛化潜力，但在真实物理机器人上的性能仍需进一步验证。此外，动态障碍物的类型和运动模式相对简单（主要是行人），对于更复杂、不可预测的动态物体（如宠物、快速移动的车辆）的泛化能力有待研究。\n\n**对后续研究的启示**：\n1.  **架构层面**：“思控分离”范式为构建更复杂、可靠的具身AI系统提供了思路，未来可将“思考”模块扩展为更通用的世界模型或任务规划器。\n2.  **动态处理层面**：显式建模环境动态性并用于规划是一个有前景的方向。未来工作可以探索更精细的动态预测（如轨迹、速度）和更复杂的安全约束集成方法。\n3.  **仿真到现实**：如何将此类严重依赖动态预测的模型有效迁移到真实世界，是下一个重要的挑战，可能需要涉及不确定性估计、在线自适应学习等技术。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02454",
      "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
      "url": "http://arxiv.org/abs/2602.02454",
      "arxivId": "2602.02454",
      "date": "2026-02-02",
      "authors": "Sherry Yang Team",
      "category": "Manipulation",
      "summary": "本文旨在解决机器人学习中物理交互成本高昂的瓶颈问题。传统方法如专家监督微调(SFT)和软件模拟器强化学习(RL)分别受限于数据稀缺和仿真与现实间的差距。论文提出World-Gymnast方法，其核心是通过在基于真实数据训练的动作条件视频世界模型中执行策略展开，并利用视觉语言模型(VLM)对展开结果进行奖励，从而对视觉-语言-动作(VLA)策略进行RL微调。在Bridge机器人实验中，该方法性能超越SFT高达18倍，超越软件模拟器高达2倍，并展现出利用世界模型进行多样化指令训练、场景泛化等新兴能力。",
      "detailedSummary": "## 研究背景与动机\n机器人通过与物理世界交互进行学习，从根本上受到物理交互成本的瓶颈制约。目前两种主流替代方案各有局限：基于专家演示的监督微调受限于可用专家数据的数量，且难以覆盖长尾场景和恢复行为；基于软件模拟器的强化学习则面临高昂的仿真构建成本以及因视觉特征差异导致的“模拟到现实”差距。近期，从真实世界视频-动作数据中学习的世界模型崭露头角，它们能够预测机器人动作下的视觉世界演变，成为一种基于真实数据学习的、动作条件化的视频模拟器。本文针对的核心痛点是：在世界模型中训练机器人策略，是否比监督学习或软件模拟器中的强化学习更能提升真实机器人的性能？本文提出World-Gymnast框架，其核心思路是：在一个从真实数据学习的世界模型中进行策略展开，并使用视觉语言模型计算任务完成奖励，以此对视觉-语言-动作策略进行强化学习微调。\n\n## 方法详解\nWorld-Gymnast的整体框架是一个模型基础的强化学习流程，其目标是优化一个视觉-语言-动作策略 $\\pi_{\\theta}$，该策略以初始观察图像 $o_0$ 和语言任务指令 $g$ 为输入，输出机器人动作。\n\n![方法框架](https://arxiv.org/html/2602.02454v1/x1.png)\n\n> **图1**：World-Gymnast 概述。策略在由初始帧和语言指令指定的任务上进行训练。训练时，策略输出动作，传递给世界模型以生成想象的轨迹展开。这些展开轨迹随后传递给视觉语言模型，后者返回二元任务完成奖励。此奖励用于更新策略。训练完成后，我们使用 AutoEval 设置在真实机器人上评估策略。来自 AutoEval 的真实世界轨迹可用于进一步改进特定环境下的世界模型。\n\n具体流程如下：对于给定的任务指令 $g$ 和初始观察 $o_0$，策略 $\\pi_{\\theta}$ 在世界模型 $\\hat{T}$（本文采用 Quevedo 等人 (2025) 的 WorldGym 模型）中展开 $K$ 条独立轨迹。每一步，策略根据当前观测采样动作 $a_{t,k} \\sim \\pi_{\\theta}(\\cdot|o_{t,k}, g)$，世界模型则预测下一帧观测 $o_{t+1,k} \\sim \\hat{T}(o_{t,k}, a_{t,k})$，直至达到预设的视野长度 $H$，得到轨迹 $\\tau_k$。随后，一个视觉语言模型奖励函数 $\\hat{R}$（本文使用 GPT-4o）为每条轨迹分配一个二元任务完成奖励 $r_k = \\hat{R}(\\tau_k, g)$。\n\n其核心优化算法采用分组相对策略优化。首先，计算一个组（$K$ 条轨迹）内奖励的均值 $\\mu$ 和标准差 $\\sigma$，然后通过归一化计算每条轨迹的优势值 $\\hat{A}_k = (r_k - \\mu) / (\\sigma + \\epsilon)$。该轨迹级优势值被赋予轨迹内的每个时间步。最后，使用基于 PPO 风格的裁剪目标函数 $\\mathcal{J}(\\theta)$ 来更新策略参数 $\\theta$，其中包含了概率比 $r_{t,k}(\\theta)$ 和裁剪操作。\n\n与现有方法相比，World-Gymnast 的创新点不仅在于将世界模型和 VLM 作为动力学和奖励函数融入模型基础 RL，更体现在其利用世界模型特性所支持的一系列新颖训练范式：\n1.  **从任意帧训练**：世界模型仅需单张初始帧即可展开，使得策略可以从任何接近其训练分布的图像开始进行 RL 训练，极大地扩充了有效训练数据，并有助于学习恢复行为。\n2.  **在新颖语言指令上训练**：通过 VLM 为同一初始帧生成合理的、分布外的任务指令，使策略能在这些新指令上进行 RL 训练，从而学习与环境中已有但未在演示中交互过的物体进行互动。\n3.  **使用干扰物训练**：利用图像编辑工具在输入帧中合成额外的物体作为视觉干扰，训练策略在杂乱场景中的鲁棒性。\n4.  **测试时训练**：当在测试时遇到一个新颖的真实场景帧时，可以直接以此帧为起点，在世界模型中进行快速的 RL 微调，实现策略的快速适应。\n5.  **迭代的世界模型与策略改进**：受经典 Dyna 算法启发，可以将策略在真实世界（或世界模型）中收集的新轨迹数据用于微调世界模型，再用更新后的、更准确的世界模型来优化策略，形成一个数据飞轮。\n\n## 实验与结果\n**实验设置**：评估在 Bridge 机器人平台上进行，使用 AutoEval 自动化真实机器人评估框架（支持 4 个任务）。基模型为在 BridgeData V2 上微调后的 OpenVLA-OFT。世界模型采用在 Open X-Embodiment 数据集上预训练的 600M 参数版 WorldGym。奖励模型为 GPT-4o。训练使用 4 块 H200 GPU 进行 1-2 天的全参数微调。\n\n**基线对比**：\n1.  **软件模拟器**：SIMPLER，一个为 Bridge 创建的真实到模拟框架。\n2.  **监督学习**：SFT（基模型）和 Iter-SFT（在 SFT 基础上，额外使用世界模型生成的成功合成轨迹进行迭代监督微调）。\n\n**关键实验结果**：\n\n![定性评估](https://arxiv.org/html/2602.02454v1/x2.png)\n\n> **图2**：在带有干扰物的 WorldGym 中策略展开的定性评估。在视觉干扰下，SFT 策略明显抓取了错误的物体，而两个 World-Gymnast 变体都能正确执行任务。World-Gymnast-Distract 具有更好的抓取和放置动作。\n\n表1和表2展示了真实机器人成功率对比。World-Gymnast 在“打开抽屉”（58% vs. 34%）、\"将茄子放入蓝色水槽\"（72% vs. 32%）和“将茄子放入黄色篮子”（78% vs. 40%）任务上显著优于 SIMPLER。与监督学习方法相比，World-Gymnast 在“将茄子放入蓝色水槽”（72% vs. 4%）和“将茄子放入黄色篮子”（78% vs. 8%）任务上分别取得了高达 18 倍和近 10 倍的性能提升。Iter-SFT 在困难任务上略有改进，但在简单任务上性能下降，表明 RL 通过主动探索能学到更具泛化性的行为。\n\n**多样化训练场景评估**：如表3所示，通过在训练数据中增加带有干扰物的帧（World-Gymnast-Distract）、增加新颖语言指令任务（World-Gymnast-Language）或直接增加更多训练任务（World-Gymnast-Scaled），策略在原始任务上的成功率均得到进一步提升（从 World-Gymnast 的 74% 最高提升至 81%），证明了利用世界模型进行数据扩增的有效性。\n\n**测试时优化评估**：仅使用测试帧和指令进行零样本的测试时训练，能将“关闭抽屉”任务的真实成功率从 62% 提升至 100%，但会损害其他任务性能，存在过拟合问题。\n\n**迭代改进评估**：\n\n![迭代改进对比](https://arxiv.org/html/2602.02454v1/x3.png)\n\n> **图3**：在真实机器人、软件模拟器 SIMPLER、原始 WorldGym 以及经过在线世界模型更新后的 World-Gymnast 上执行相同动作序列的定性对比。经过 Dyna 式迭代更新后的 World-Gymnast 展开更贴近真实世界。\n\n通过收集约 100 条真实机器人轨迹微调世界模型后，其生成的展开在视觉上比 SIMPLER 和原始 WorldGym 更接近真实情况，表明迭代改进能提升世界模型的质量。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 World-Gymnast 框架，首次系统性地论证了在世界模型中进行 RL 微调，能够显著超越监督微调和软件模拟器 RL，在真实机器人任务上取得更优性能。\n2.  展示了利用世界模型特性所支持的一系列创新训练范式，包括从任意帧训练、处理新颖指令和视觉干扰、测试时训练以及迭代的模型-策略改进，极大地扩展了机器人策略训练的灵活性和数据效率。\n3.  通过实验验证了这些范式能够有效提升策略的鲁棒性和泛化能力。\n\n**局限性**：论文提到测试时训练可能导致对单一任务的过拟合；此外，世界模型本身可能存在幻觉问题，这会影响迭代 SFT 等方法的性能。\n\n**启示**：本文结果表明，学习世界模型并在其中进行“云端”策略训练，可能是弥合“仅在演示中工作的机器人”与“能在任何家庭中工作的机器人”之间差距的关键。未来工作可探索如何减轻测试时训练的过拟合，以及如何更有效地利用世界模型进行持续学习和适应。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02454v1/x1.png",
        "https://arxiv.org/html/2602.02454v1/x2.png",
        "https://arxiv.org/html/2602.02454v1/x3.png",
        "https://arxiv.org/html/2602.02454v1/x4.png",
        "https://arxiv.org/html/2602.02454v1/x5.png",
        "https://arxiv.org/html/2602.02454v1/x6.png",
        "https://arxiv.org/html/2602.02454v1/x7.png",
        "https://arxiv.org/html/2602.02454v1/x8.png",
        "https://arxiv.org/html/2602.02454v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02402",
      "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
      "url": "http://arxiv.org/abs/2602.02402",
      "arxivId": "2602.02402",
      "date": "2026-02-02",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "本文提出SoMA，一个用于机器人软体操作的真实到仿真神经模拟器。核心解决现有模拟器依赖预定义物理模型或缺乏机器人条件控制，导致准确性、稳定性和泛化能力不足的问题。其关键技术是在统一潜在神经空间中，耦合可变形物体动力学、环境力与机器人关节动作，并基于学习的3D高斯泼溅进行端到端模拟。该方法无需预定义物理模型，实现了可控、稳定的长时程操作与轨迹外泛化。实验表明，SoMA在真实机器人操作任务上，将重新模拟准确性与泛化能力提升了20%，并能稳定模拟如长时程布料折叠等复杂任务。",
      "detailedSummary": "## 研究背景与动机\n机器人软体操纵（如布料折叠、柔性物体操作）的具身学习高度依赖大量交互数据，但在真实世界中采集此类数据成本高昂且风险大。真实到仿真（R2S）模拟通过将真实物体行为复现到虚拟环境中，为数据合成和策略学习提供了可扩展的解决方案。一个实用的仿真器需要在物理保真度和长期交互一致性之间取得平衡。现有方法主要沿两个方向展开：基于物理的仿真器（如FEM、MPH）能保证长期交互一致性，但依赖难以从视觉数据中推断的预定义物理模型和参数；神经网络动力学建模和4D重建方法直接从数据学习运动，但主要关注再现观测轨迹，对机器人条件交互和训练分布外泛化的支持有限。因此，两者均无法完全满足复杂机器人操纵中的R2S需求。\n\n本文针对上述痛点，提出了一种新的视角：重新思考可变形物体仿真的表示与学习方式。核心思路是提出SoMA，一个基于3D高斯泼溅（Gaussian Splatting）的软体操纵神经仿真器，将可变形物体动力学、环境力和机器人关节动作耦合在一个统一的潜在神经空间中，进行端到端的真实到仿真模拟，从而实现可控、稳定的长时域操纵仿真，且无需预定义物理模型。\n\n## 方法详解\nSoMA是一个用于软体机器人操纵的统一神经仿真器，旨在机器人关节空间控制和环境交互下建模可变形物体动力学。其整体框架接受从真实世界操纵收集的RGB观测和机器人关节状态作为输入，通过端到端训练，输出模拟出的物体状态并渲染为图像。\n\n![方法框架](https://arxiv.org/html/2602.02402v1/x2.png)\n> **图2**：SoMA整体框架。左侧输入为多视角RGB观测和机器人关节动作；中间部分将物体重建为层次化高斯泼溅，并通过神经仿真器在渲染和动力学监督下进行传播；右侧展示了物体运动由基于力的交互驱动，环境和机器人诱导的力作用于泼溅点产生形变。\n\n框架包含三个核心组件：\n1.  **通过R2S映射进行场景初始化**：为了在机器人运动学、被操纵物体和物理参考系之间建立统一的仿真空间，本模块首先利用多视角几何和3D高斯泼溅从RGB图像重建初始物体状态 `G0`。关键步骤是进行机器人条件对齐：通过强制重建几何与已知参考尺寸之间的度量一致性来恢复全局尺度因子 `s`，并估计将机器人基座标系变换到仿真空间的刚体变换 `(R, t)`。结合机器人正向运动学，可将任意时刻的末端执行器位姿 `𝐓_rob^ee(t)` 映射到仿真空间 `𝐓_sim^ee(t)`。同时，通过拟合支撑桌面法向来定义重力方向 `g`，解决了重力符号的歧义。\n2.  **力驱动的高斯泼溅动力学建模**：与现有主要基于状态的神经动力学模型不同，SoMA将物体运动建模为由接触力驱动。对于每个高斯泼溅节点或簇 `i`，其线速度和角速度由神经网络 `ψ_θ` 根据其历史状态和总作用力 `𝐟_i` 预测。总作用力 `𝐟_i = 𝐟_i^env + 𝐟_i^rob` 由两部分组成：\n    *   **环境力 `𝐟_i^env`**：包括作用于所有节点的重力 `g`，以及对靠近支撑表面的节点额外施加的支持力 `𝐬_i`。\n    *   **机器人力 `𝐟_i^rob`**：通过在高斯泼溅节点和机器人控制点之间构建交互图来建模。图神经网络 `Φ_θ` 根据泼溅节点状态、邻近机器人控制点状态和夹爪开合状态 `c_t` 来预测机器人施加的力。\n    这些力在构建的层次化图结构（由底向上聚类形成）中自底向上聚合，动力学则通过图神经网络自顶向下传播，确保全局运动一致性和局部形变。\n3.  **用于长时域学习的多分辨率训练与混合监督**：\n    *   **多分辨率训练**：为解决长时域模拟中误差累积和稳定性问题，采用从粗到细的时间训练策略。第一阶段使用较大的时间步长 `k·dt` 捕捉长程动力学；第二阶段使用原始分辨率 `dt` 并随机采样子序列进行细粒度动力学学习。同时，在图像维度上，几何重建使用高分辨率图像以保留细节，而动力学训练则使用原始分辨率以降低计算成本。\n    *   **混合监督**：针对交互中常见的遮挡问题，设计了混合监督策略。\n        *   **遮挡感知的图像监督**：仅在物体可见区域（通过二值掩码 `𝐌_t` 定义）计算图像重建损失 `ℒ_img`，避免遮挡区域引入虚假梯度。\n        *   **动量一致性正则化**：为约束无直接视觉反馈的遮挡区域运动，引入动量守恒正则化 `ℒ_mom`，强制相邻层次簇节点与子节点之间的动量守恒，作为自监督的物理一致性约束，缓解长时域漂移。\n\n与现有方法相比，SoMA的核心创新在于：1) **机器人动作直接耦合**：通过R2S映射将关节空间动作直接锚定到仿真空间，实现因果、运动学一致的交互；2) **力驱动的交互建模**：将环境和机器人交互显式建模为力，作用于高斯泼溅表示，提高了对接触和遮挡的鲁棒性；3) **稳定性保障机制**：多分辨率训练和混合监督策略专门针对长时域、遮挡严重的机器人操纵场景设计，确保了仿真的数值稳定性和物理合理性。\n\n## 实验与结果\n**实验设置**：在ARX-Lift平台上收集了包含绳子、玩偶、布料和T恤四类可变形物体的真实世界机器人操纵数据集。图像分辨率为640×480，30 FPS，同步记录机器人关节状态。每类物体收集30-40个序列，按7:3划分训练集和测试集。评估两项任务：**重仿真**（在训练轨迹上评估）和**泛化**（在未见过的测试序列上评估）。对比的基线方法包括基于可微分物理的 **PhysTwin** 和基于神经动力学的 **GausSim**。评估指标涵盖图像质量（PSNR、SSIM、LPIPS）和代理几何精度（深度图的Abs Rel、RMSE）。\n\n**关键实验结果**：\nSoMA在重仿真和泛化任务上均取得了最佳性能。如表1所示，在重仿真任务中，SoMA在各项指标上全面领先，例如在PSNR上达到33.51，优于PhysTwin的28.77和GausSim的31.69。在更具挑战性的泛化任务中，SoMA同样保持优势，PSNR为32.89，而基线方法性能均有下降。论文指出，SoMA相比基线方法实现了约20%的性能提升。\n\n![定性结果](https://arxiv.org/html/2602.02402v1/x3.png)\n> **图3**：SoMA在机器人操纵下的定性重仿真（左）与泛化（右）结果。对于绳子、布料、玩偶等物体，SoMA能产生稳定、长时域的仿真，与真实动态高度匹配。PhysTwin在复杂或未见交互下出现偏差，GausSim则在挑战性场景中经常保持静态或变得不稳定。\n\n在涉及长时域、大形变和频繁自接触的**T恤折叠**复杂任务中（表2），SoMA的优势更加明显。它能稳定模拟整个折叠过程，而PhysTwin则出现明显的伪影和不一致的形变。SoMA在PSNR上达到27.57，显著高于PhysTwin的22.85。\n\n**消融实验**（表3）总结了各组件贡献：\n*   **完整模型**：取得最佳综合性能。\n*   **联合训练（Jointly）**：在所有物体域上联合训练，略微降低了重仿真精度，但提升了泛化性能，表明其具有提升泛化能力的潜力。\n*   **仅图像监督（Img-only）**：移除混合监督损失（动量正则化），性能下降，证明了物理一致性约束对稳定学习的重要性。\n*   **无多分辨率训练（w/o MRT）**：禁用多分辨率训练策略，性能显著降低，突显了该策略对于高效、稳定学习长时域动力学的关键作用。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了一种新的R2S神经仿真范式**：首次将可变形物体、机器人动作和环境统一在基于高斯泼溅的仿真空间中，支持长时域、交互一致的软体操纵模拟。\n2.  **设计了一系列使能该仿真的关键机制**：包括机器人条件对齐、力驱动的高斯泼溅动力学模型，以及结合多分辨率训练和混合监督的稳定化策略。\n3.  **在真实机器人操纵数据集上进行了全面验证**：在重仿真、泛化及复杂任务（T恤折叠）上均显著优于现有基于物理和神经网络的仿真器，展示了其实际应用潜力。\n\n论文自身提到的局限性包括：仿真依赖于从视觉观测进行的初始重建，重建质量会影响后续模拟；方法在计算成本上可能较高。\n\n本文对后续研究的启示在于：为机器人软体操纵的仿真提供了一条融合数据驱动与物理直觉的新路径。其力驱动的交互建模思想、针对遮挡和长时域稳定性设计的训练策略，可启发更鲁棒、更通用的具身智能仿真器的开发。未来工作可探索如何进一步降低对高质量初始重建的依赖，以及将此类仿真器用于闭环策略学习与优化。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02402v1/x1.png",
        "https://arxiv.org/html/2602.02402v1/x2.png",
        "https://arxiv.org/html/2602.02402v1/x3.png",
        "https://arxiv.org/html/2602.02402v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.02396",
      "title": "PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning",
      "url": "http://arxiv.org/abs/2602.02396",
      "arxivId": "2602.02396",
      "date": "2026-02-02",
      "authors": "Alexander Schperberg Team",
      "category": "Manipulation",
      "summary": "论文PRISM解决了机器人模仿学习中现有方法难以同时满足实时控制速率、多模态传感输入（如RGB、深度、触觉）和动作多模态分布的挑战。它提出基于Performer RS-IMLE的单次通过策略，结合多传感器时序编码器与线性注意力生成器，采用批全局拒绝采样IMLE目标进行训练。实验表明，在真实硬件任务中PRISM比扩散策略成功率提高10-25%，在CALVIN基准上成功率提升约25%，轨迹急动度减少20-50倍，同时保持30-50Hz闭环控制。",
      "detailedSummary": "## 研究背景与动机\n机器人模仿学习（IL）旨在直接从示教数据中学习复杂的视觉运动策略。理想的模仿策略应满足三个关键标准：实时推理速率以进行闭环物理控制；表征专家行为的多模态分布；以及在部分感官输入下通过有效融合多传感流实现鲁棒性能。当前主流生成方法往往只能满足部分要求：扩散模型能捕获复杂的多模态动作分布，但其依赖迭代去噪（通常每推理10-100步）严重限制了实时部署；基于流的方法通过连续时间积分减少了采样步骤，但可能在多模态保真度上存在不足。这些方法在推理时由于迭代采样而计算昂贵。隐式最大似然估计（IMLE）方法通过最小化每个专家数据点到其最近生成样本的距离来确保专家分布的完全覆盖，并支持单次推理，但其在扩展到条件策略时面临挑战。先前方法通常对每个样本进行拒绝采样，这违反了IMLE的批全局覆盖原则，可能导致策略回归到平均行为而无法表示多模态设置中的不同模式。此外，闭环机器人任务要求动作序列具有强时序平滑性，现有的局部平滑启发式方法在避免模式切换方面较为脆弱。\n\n本文针对现有生成方法在实时性、多模态覆盖和时序平滑性之间难以兼得的痛点，提出了PRISM（Performer RS-IMLE for Single-pass Multisensory Imitation）。其核心思路是：通过一个基于Performer架构的、单次前向传递的线性注意力生成器，配合一个批全局的RS-IMLE训练目标，在无需迭代采样的前提下，生成覆盖多模态且时序平滑的动作序列，从而实现高效、准确的实时多感官模仿学习。\n\n## 方法详解\nPRISM的整体框架将问题解耦为三个阶段：a) 时间多感官编码器，融合异构输入并保留时间维度；b) 单次传递生成器，使用双向线性注意力（FAVOR+）并行产生完整动作序列；c) 批全局RS-IMLE训练目标，为模式覆盖提供理论依据且无迭代扩散的推理成本。\n\n![方法框架](https://arxiv.org/html/2602.02396v1/x1.png)\n> **图1**：PRISM方法整体概览。可用的传感器特征按时间步融合为时间上下文令牌。使用双向FAVOR+（Performer）生成器与可学习的查询令牌，单次输出完整的动作序列。训练使用批全局RS-IMLE目标（鲁棒的Charbonnier距离、带EMA校准的ε-拒绝、可选的小覆盖项），以在不进行迭代采样的情况下保持动作多模态性。\n\n**时间多感官编码器**：对于上下文时间范围 \\(T_o\\) 内的每个时间步 \\(t\\)，将每种模态（手腕RGB、静态RGB、深度、触觉、本体感觉、音频、文本）的嵌入向量拼接，并通过一个MLP融合为固定维度 \\(d\\) 的上下文令牌 \\(\\mathbf{c}_t\\)。然后将所有时间步的 \\(\\mathbf{c}_t\\) 堆叠，并添加绝对位置嵌入，得到上下文令牌 \\(\\mathbf{C} \\in \\mathbb{R}^{T_o \\times d}\\)。这种按时间步融合的方式避免了时间统计量的纠缠，将长程一致性任务委托给生成器。\n\n**单次传递线性注意力生成器（双向）**：给定上下文 \\(\\mathbf{C}\\)，生成器通过一次前向传递产生长度为 \\(T_p\\) 的动作序列。它初始化 \\(T_p\\) 个可学习的查询令牌 \\(\\mathbf{Q} \\in \\mathbb{R}^{T_p \\times d}\\)，并加入一个投影的潜变量 \\(z \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\)（作为产生不同轨迹候选的随机种子）以及位置编码。随后，应用 \\(L\\) 个Transformer块，每个块包含：i) 对 \\(\\mathbf{Q}\\) 的双向自注意力（无因果掩码），以及 ii) 对完整上下文 \\(\\mathbf{C}\\) 的交叉注意力（也无因果掩码）。两种注意力均使用FAVOR+（线性化softmax），将计算成本从 \\(\\mathcal{O}(T^2)\\) 降低到 \\(\\mathcal{O}(T m)\\)（\\(m\\) 为随机特征数）。一个线性头将最终的令牌映射为动作 \\(\\hat{\\mathbf{A}} \\in \\mathbb{R}^{T_p \\times D_a}\\)。这种非自回归、双向的设计允许联合选择整个序列。\n\n**鲁棒序列距离**：使用带Charbonnier惩罚 \\(\\varepsilon_c\\) 和按维度权重 \\(w_d\\) 的度量来比较预测序列和目标序列：\\(D_{\\rho}(\\hat{\\mathbf{A}},\\mathbf{A}) = \\frac{1}{T_p} \\sum_{t=1}^{T_p} \\sum_{d=1}^{D_a} w_d \\sqrt{(\\hat{a}_{t,d} - a_{t,d})^2 + \\varepsilon_c^2}\\)。该度量对异常值鲁棒且可微，用于训练和评估时的候选选择。\n\n**批全局RS-IMLE目标**：对于批次 \\(B\\) 中的每个样本 \\(i\\)，抽取 \\(K\\) 个潜变量得到候选序列 \\(\\{\\hat{\\mathbf{A}}^{(k)}_i\\}_{k=1}^K\\)。计算每个候选与自身目标之间的距离 \\(D_{i,k}\\)，以及与批次内所有目标 \\(j\\) 之间的批全局距离 \\(D_{i,k \\rightarrow j}\\)。通过一个由指数移动平均（EMA）校准的阈值 \\(\\varepsilon_{\\text{RS}}\\) 进行拒绝采样：拒绝那些与任何目标过于接近（即 \\(\\min_j D_{i,k \\rightarrow j} < \\varepsilon_{\\text{RS}}\\)）的候选。训练损失由硬IMLE损失和可选的软覆盖项组成：\n- 硬IMLE损失：\\(\\mathcal{L}_{\\text{hard}} = \\frac{1}{B} \\sum_{i=1}^{B} \\min_{k \\in \\mathcal{K}_i} D_{i,k}\\)，其中 \\(\\mathcal{K}_i\\) 是未被拒绝的候选索引集（若全部被拒绝则使用所有候选）。\n- 软覆盖项：\\(\\mathcal{L}_{\\text{soft}} = -\\frac{1}{B} \\sum_{i=1}^{B} \\log \\sum_{k \\in \\text{Top}K'(D_{i,\\cdot})} \\exp(-D_{i,k}/\\tau)\\)，对前 \\(K'\\) 个候选施加一个温和的熵正则化，防止模式坍塌。\n总损失为 \\(\\mathcal{L} = \\mathcal{L}_{\\text{hard}} + \\lambda_{\\text{soft}} \\mathcal{L}_{\\text{soft}}\\)，其中 \\(\\lambda_{\\text{soft}} \\ll 1\\)。批全局拒绝防止单个候选“覆盖”批次中多个相近的目标，从而保留了替代模式的梯度信号。\n\n**滑动窗口推理（单次传递）**：在测试时，模型观察当前上下文，编码后抽取 \\(K\\) 个潜变量，单次前向传递生成 \\(K\\) 条动作序列。然后根据仅使用观测的规则选择一条轨迹：1) **代理评分**（当本体感觉或手腕姿态可观测时）：选择其第一动作诱导的预测末端执行器姿态最接近观测姿态的候选。2) **确定性平局打破**（当代理评分不可用时）：选择其第一动作最接近最后执行动作（L2距离）的候选。执行选中轨迹的前 \\(T_a\\) 个动作，然后滑动观测窗口并重新规划。这种连续重规划机制在不增加推理延迟的前提下保证了时序平滑性。\n\n与现有方法相比，PRISM的创新点具体体现在：1) **批全局拒绝采样**：将RS-IMLE从样本级扩展到批全局，有效避免了模式平均并保持了多模态覆盖。2) **单次非自回归生成**：基于双向线性注意力的生成器一次性输出完整动作序列，完全避免了迭代采样步骤，实现了实时推理。3) **时序感知的多感官融合与平滑性保证**：通过按时间步融合编码器和滑动窗口推理中的轨迹选择策略，共同确保了生成动作的时序连贯性。\n\n## 实验与结果\n**实验设置**：PRISM在多样化的真实机器人套件和大型仿真基准上进行了验证。\n- **真实硬件**：包括用于移动操作（loco-manipulation）的Unitree Go2（配备7自由度机械臂D1）和用于桌面精确操作的UR5机械臂。任务包括操作前泊车、高精度插入和多物体取放。\n- **仿真基准**：包括CALVIN（10%数据划分）、MetaWorld和Robomimic。\n- **对比基线**：包括扩散策略（Diffusion Policy）、流匹配（Flow Matching）以及IMLE策略等最先进的方法。\n\n**关键实验结果**：\n- **真实机器人性能**：在挑战性物理任务上，PRISM比最先进的扩散策略的成功率高出10–25%，同时保持高频（30–50 Hz）闭环控制。\n- **仿真基准性能**：在CALVIN（10%数据）上，PRISM比扩散策略的成功率提高约25%，比流匹配提高约20%，同时将轨迹加加速度（jerk）降低了20–50倍，这对硬件寿命和安全至关重要。在MetaWorld和Robomimic上，PRISM也达到或超过了SOTA策略的性能。\n\n![基准数据集与模态](https://arxiv.org/html/2602.02396v1/x2.png)\n> **图2**：各基准数据集使用的传感器模态。R代表RGB相机，D代表深度相机，Tact代表触觉，P代表本体感觉，Text代表文本令牌，A代表音频。\n\n![真实硬件任务成功率](https://arxiv.org/html/2602.02396v1/x3.png)\n> **图3**：真实硬件任务的成功率。PRISM在UR5的插入和取放任务，以及Unitree Go2的泊车和取放任务上，均显著优于扩散策略（Diffusion Policy）和流匹配（Flow Matching）。\n\n![CALVIN基准成功率和加加速度](https://arxiv.org/html/2602.02396v1/x4.png)\n> **图4**：在CALVIN基准（10%数据）上的成功率和轨迹加加速度。左图显示PRISM的成功率显著高于扩散和流匹配；右图显示PRISM产生的轨迹加加速度（衡量运动平滑性的指标）比扩散策略低20-50倍。\n\n![MetaWorld成功率](https://arxiv.org/html/2602.02396v1/x5.png)\n> **图5**：在MetaWorld基准上的成功率。PRISM在多个任务上表现与扩散策略相当或更优。\n\n![消融实验：组件贡献](https://arxiv.org/html/2602.02396v1/x6.png)\n> **图6**：消融实验研究各组件贡献。(a) 移除软覆盖项（`w/o soft`）、使用标准注意力（`w/o linear`）或使用因果注意力（`w/o bidir`）都会导致性能下降。(b) 候选数 \\(K\\) 的影响，性能在 \\(K \\approx 16\\) 时饱和。(c) 批全局拒绝采样（`Global RS`）相比每样本拒绝采样（`Per-sample RS`）能带来显著性能提升。\n\n![消融实验：模态重要性](https://arxiv.org/html/2602.02396v1/x7.png)\n> **图7**：传感器模态重要性分析。手腕RGB和本体感觉是关键模态，而深度信息在某些任务中可能是冗余的。模态丢弃研究表明PRISM在传感器缺失时具有优雅的性能降级能力。\n\n![Robomimic上的成功率](https://arxiv.org/html/2602.02396v1/x8.png)\n> **图8**：在Robomimic基准上的成功率。PRISM在多个任务上优于或与扩散策略相当。\n\n![推理延迟比较](https://arxiv.org/html/2602.02396v1/x9.png)\n> **图9**：不同策略的推理延迟（毫秒）。PRISM的单次传递推理速度显著快于需要多步采样的扩散策略和流匹配。\n\n![真实机器人轨迹可视化](https://arxiv.org/html/2602.02396v1/fig/pushT.png)\n> **图10**：真实机器人任务（UR5插入）的轨迹可视化。展示了预测的多个候选轨迹（彩色线）和最终执行的平滑轨迹。\n\n![ε_RS阈值校准过程](https://arxiv.org/html/2602.02396v1/x10.png)\n> **图11**：批全局拒绝采样阈值 \\(\\varepsilon_{\\text{RS}}\\) 的EMA校准过程。该估计器方差低（\\(O(1/N)\\)），确保了训练稳定性。\n\n![不同距离度量的影响](https://arxiv.org/html/2602.02396v1/x11.png)\n> **图12**：不同序列距离度量对性能的影响。鲁棒的Charbonnier距离优于标准的L2或L1距离。\n\n![注意力机制消融](https://arxiv.org/html/2602.02396v1/x12.png)\n> **图13**：注意力机制消融。双向线性注意力（FAVOR+）在性能和效率上取得最佳平衡。\n\n![预测视野的影响](https://arxiv.org/html/2602.02396v1/x13.png)\n> **图14**：预测视野 \\(T_p\\) 对成功率的影响。存在一个最佳范围，视野过长或过短都会损害性能。\n\n![语言引导任务示例](https://arxiv.org/html/2602.02396v1/fig/language_guided.png)\n> **图15**：语言引导任务示例。展示了PRISM在处理多模态输入（包括语言指令）时的能力。\n\n![批全局校准有效性](https://arxiv.org/html/2602.02396v1/fig/batch_calibration.png)\n> **图16**：批全局校准有效性可视化。说明了批全局拒绝如何防止候选聚集在单个模式周围，从而促进多模态覆盖。\n\n**消融实验总结**：消融实验明确了各核心组件的贡献：1) **批全局RS-IMLE目标**是提升多模态覆盖和性能的关键。2) **软覆盖项**对稳定训练和防止优化偏差有积极作用。3) **双向线性注意力（FAVOR+）** 在保证效率的同时实现了有效的时序建模。4) **手腕RGB和本体感觉**是最关键的传感器模态。5) 候选数 \\(K\\) 在约16时达到性能饱和。6) 鲁棒的Charbonnier距离度量优于传统L2距离。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了**PRISM框架**，一个基于Performer RS-IMLE的单次传递多感官模仿策略，首次在单次前向传递中同时实现了实时推理、多模态动作覆盖和时序平滑性。\n2. 引入了**批全局RS-IMLE训练目标**，通过EMA校准的拒绝采样和软覆盖项，理论严谨且实践有效地解决了条件IMLE中的模式平均问题，无需增加测试时采样成本。\n3. 设计了一个**时间感知的多感官融合编码器**和一种**滑动窗口推理策略**，能够优雅地处理异构、可能缺失的传感器输入，并生成硬件安全所需的平滑轨迹。\n\n**局限性**：论文自身提到的局限性包括：1) 方法依赖于高质量、时间对齐的多感官演示数据。2) 尽管单次传递效率高，但训练时生成多个候选并进行批全局距离计算可能带来额外的计算开销。3) 滑动窗口推理中的轨迹选择规则（代理评分）依赖于特定的可观测状态（如末端执行器姿态）。\n\n**对后续研究的启示**：\n1. **单次生成方法的潜力**：PRISM证明了无需迭代采样也能实现高质量的多模态策略生成，为实时机器人控制开辟了新路径，未来可探索更高效的生成器架构。\n2. **多传感器融合与鲁棒性**：对传感器模态重要性的分析强调了选择关键传感器和设计稳健融合方案的重要性，特别是在真实世界部分可观测的场景下。\n3. **运动平滑性与硬件安全**：显著降低轨迹加加速度不仅关乎性能，更是硬件安全和寿命的关键，未来的模仿学习工作应更加重视动作序列的物理可行性和平滑性。\n4. **理论指导的实践**：将IMLE理论与高效的注意力机制、精心设计的损失函数相结合，展示了理论洞察对解决实际机器人学习问题的重要价值。",
      "imageUrls": [
        "https://arxiv.org/html/2602.02396v1/x1.png",
        "https://arxiv.org/html/2602.02396v1/x2.png",
        "https://arxiv.org/html/2602.02396v1/x3.png",
        "https://arxiv.org/html/2602.02396v1/x4.png",
        "https://arxiv.org/html/2602.02396v1/x5.png",
        "https://arxiv.org/html/2602.02396v1/x6.png",
        "https://arxiv.org/html/2602.02396v1/x7.png",
        "https://arxiv.org/html/2602.02396v1/x8.png",
        "https://arxiv.org/html/2602.02396v1/x9.png",
        "https://arxiv.org/html/2602.02396v1/fig/pushT.png",
        "https://arxiv.org/html/2602.02396v1/x10.png",
        "https://arxiv.org/html/2602.02396v1/x11.png",
        "https://arxiv.org/html/2602.02396v1/x12.png",
        "https://arxiv.org/html/2602.02396v1/x13.png",
        "https://arxiv.org/html/2602.02396v1/fig/language_guided.png",
        "https://arxiv.org/html/2602.02396v1/fig/batch_calibration.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01939",
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "http://arxiv.org/abs/2602.01939",
      "arxivId": "2602.01939",
      "date": "2026-02-02",
      "authors": "Qiang Nie Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中因视觉遮挡导致的信息不足问题，提出了“探索性与聚焦性操作”（EFM）这一新问题。为此，研究者建立了包含10个任务的EFM-10基准，并提出了“双臂主动感知”（BAP）策略：利用一只手臂提供主动视觉，另一只手臂在操作时提供力感知。基于该策略收集了BAPData数据集，并通过模仿学习验证了BAP策略的有效性。",
      "detailedSummary": "## 研究背景与动机\n当前人形机器人操作研究的一个趋势是将主摄像头安装在机器人头部，以获得基地不变的灵活性，但这导致视觉遮挡问题更加频繁。近期研究（如AV-ALOHA、ViA）通过采用高自由度（6/7-DoF）主动脖子来提供主动视觉以应对此问题。然而，本文指出，视觉遮挡问题的本质是**完成任务所需信息的缺失**。许多现有的人形机器人并不具备这种高自由度的主动脖子，但它们通常拥有两条手臂。\n\n基于此，本文提出了一个更根本、更广泛的新问题：**探索性与专注性操作**。该问题的核心是**主动寻求信息**，以完成那些需要探索（如寻找隐藏属性）或需要专注（如执行精细操作）的挑战性操作任务。本文的核心思路是：利用双手机器人中**非操作臂**（如果可用）提供眼在手机器视觉，同时利用**操作臂**在接触时提供力觉感知，从而构成一种无需高自由度主动脖子的双手机器人主动感知策略。\n\n## 方法详解\n本文提出的双手机器人主动感知策略旨在解决EFM问题。其整体框架是数据驱动的模仿学习。策略的输入包括：头部固定摄像头的视图（主视图）、左右手腕摄像头提供的主动视图、机器人状态（左右末端执行器位姿和夹爪状态），以及（可选的）操作臂的六维力/力矩传感器数据。输出是在笛卡尔空间中的动作序列。\n\n![硬件系统总览](https://arxiv.org/html/2602.01939v1/x2.png)\n> **图2**：硬件系统与任务物体概览。展示了用于数据收集的双手机器人JAKA K-1、头部Orbbec相机、手腕Logitech相机以及VR遥操作设备Pico Ultra 4。\n\nBAP策略包含两个核心模块：\n1.  **眼在手机器视觉**：由非操作臂（空闲臂）提供。在需要探索或专注时，操作员（或策略）主动控制该臂，使其手腕摄像头捕获**操作区域以及操作末端执行器**的清晰视图。论文通过实验强调，在操作手持物体进行精细操作时，主动视图中同时捕获操作区域和末端执行器至关重要，仅捕获手持物体无法提供末端执行器应如何调整位姿的直接线索。\n2.  **力觉感知**：由操作臂内置的力/力矩传感器提供。该信息有助于处理涉及精细接触的操作任务，为实现基于神经网络的力顺应控制提供可能。\n\n该策略的主要创新点在于：**为不具备高自由度主动脖子的现有人形机器人提供了一种实现主动感知的实用方案**。它充分利用了双手机器人并非总需同时操作的特点，将一条臂转换为“感知臂”。该策略与基于脖子的主动视觉完全兼容，未来可结合使用以最大化所有可用摄像头的效用。\n\n## 实验与结果\n本文构建了**EFM-10**基准，包含4大类共10个任务：语义探索类（Toy-Find, Toy-Match）、涉及视觉遮挡的探索类（Cup-Hang, Cup-Place, Box-Push）、需要专注的精细操作类（Light-Plug, Bread-Brush, Nail-Knock）以及兼具探索与专注的复杂类（Cable-Match, Charger-Plug）。基于BAP策略，在真实双手机器人上收集了包含1810条专家轨迹的**BAPData**数据集。\n\n实验平台为JAKA K-1双手机器人，数据频率为10Hz。对比的基线策略包括：ACT、DP、GR-MG和Pi-0。所有策略均在BAPData上以模仿学习方式训练（除非特别说明，未使用力觉数据）。\n\n首先，实验验证了主动视图中捕获内容的重要性。在Toy-Match等4个任务上比较了三种视觉上下文设置的成功率。\n\n![主动视觉设置对比结果](https://arxiv.org/html/2602.01939v1/x1.png)\n> **表3**：不同主动视觉设置下的任务成功率对比。结果表明，主动视图需同时捕获操作区域和末端执行器才能获得最佳性能。\n\n其次，在EFM-10基准上全面评估了各代表性策略的性能。\n\n![策略在EFM-10上的评估结果](https://arxiv.org/html/2602.01939v1/x1.png)\n> **表4**：各策略在EFM-10各任务上的成功率。使用BAPData训练（标有⋆）的策略性能显著提升。单任务策略（ACT, DP）无法处理语言驱动的语义探索任务；多任务策略中，Pi-0在指令跟随和非精细任务上表现更强，而所有策略在极精细操作任务（如Light-Plug）上表现均不佳。\n\n第三，实验探究了融合力觉感知的效果。通过修改GR-MG策略，使其额外输入当前力/力矩并预测未来力/力矩块。\n\n![融合力感知的GR-MG策略示意图](https://arxiv.org/html/2602.01939v1/x3.png)\n> **图3**：将力感知融入GR-MG策略的方法示意图。在输入中加入力/力矩数据，并训练模型额外预测未来的力/力矩。\n\n在Light-Plug和Bread-Brush任务上的实验表明，加入力感知后，成功率分别提升了16.7%和13.3%，同时操作末端执行器的最大垂直力平均值显著降低了29%和22%，表明神经网络实现了某种形式的力顺应控制。\n\n![力感知策略的定性分析](https://arxiv.org/html/2602.01939v1/x4.png)\n> **图4**：融合力感知的GR-MG策略在Light-Plug任务中的一次运行可视化。模型能够预测接触力的变化并相应地控制末端执行器，避免垂直力的突然增加。\n\n最后，论文对失败案例进行了定性分析。\n\n![典型失败案例可视化](https://arxiv.org/html/2602.01939v1/x5.png)\n> **图5**：模仿学习策略的典型失败案例。主要包括：语义条件不准确（拿错颜色）、空间感知/推理能力不足导致细微定位错误、以及未能找到最优主动视角以避免遮挡。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了**探索性与专注性操作**这一新问题，并构建了包含10个任务的**EFM-10**基准；2) 提出了适用于现有机型的**双手机器人主动感知**策略，并基于此收集了兼具高自由度主动视觉和力觉信息的大规模数据集**BAPData**；3) 通过实验验证了BAP策略的有效性，系统评估了代表性策略在EFM任务上的优缺点，并揭示了同时捕获末端执行器、融合力感知等技术细节的重要性。\n\n论文提到的局限性包括：BAPData数据集是在特定机器人硬件上收集的，其泛化性有待进一步验证。\n\n本文对后续研究的启示在于：为了更好解决EFM问题，未来的策略模型需要着重提升以下几方面能力：**语义条件化**（准确理解并执行语义指令）、**空间感知与推理**（提升对精细操作的空间判断）、以及**最优主动视点搜索**（动态选择最佳观测视角）。同时，将BAP策略与基于脖子的主动视觉相结合，也是一个有前景的未来方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01939v1/x1.png",
        "https://arxiv.org/html/2602.01939v1/x2.png",
        "https://arxiv.org/html/2602.01939v1/x3.png",
        "https://arxiv.org/html/2602.01939v1/x4.png",
        "https://arxiv.org/html/2602.01939v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01789",
      "title": "RFS: Reinforcement learning with Residual flow steering for dexterous manipulation",
      "url": "http://arxiv.org/abs/2602.01789",
      "arxivId": "2602.01789",
      "date": "2026-02-03",
      "authors": "Abhishek Gupta Team",
      "category": "Manipulation",
      "summary": "本文提出RFS框架，解决预训练生成式策略（如流匹配模型）在灵巧操作任务中泛化不足、需高效微调的问题。其核心方法“残差流引导”通过联合优化残差动作与潜在噪声分布，实现局部修正与全局探索的互补。实验表明，RFS能在仿真和真实环境中对预训练策略进行高效微调，提升部署鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n模仿学习已成为机器人序列决策的有效引导方法，尤其在灵巧操作等高维任务中表现出色。近期行为克隆方法进一步利用扩散模型和流匹配等表达力强的生成模型来表征多模态动作分布。然而，以此方式预训练的策略往往泛化能力有限，需要在部署时进行额外的微调以实现鲁棒性能。这种适应过程必须在保留预训练全局探索优势的同时，实现对局部执行错误的快速修正。现有的策略调制方法存在互补的局限性：残差策略学习能够实现局部动作修正，但难以引发全局行为改变；而扩散模型引导允许全局调制，但在分布外状态下提供的细粒度控制有限。\n\n本文针对预训练的生成策略在微调时难以兼顾全局探索与局部修正的痛点，提出了统一策略调制的新视角。核心思路是联合优化潜在噪声分布和残差动作，通过输入调制实现全局语义适应，通过输出调制实现局部灵巧修正。\n\n## 方法详解\nResidual Flow Steering (RFS) 是一个统一的策略调制框架，它结合了用于全局适应的潜在噪声引导和用于精确局部修正的残差动作。其整体流程是：给定状态 s，RFS 策略 π_RFS 输出一个潜在流变量 a0 和一个残差动作 ar；a0 用于引导预训练的流匹配基础策略 π_FM 生成基础动作 ab；最终执行动作为 ab 与 ar 之和。\n\n![方法框架](https://arxiv.org/html/2602.01789v3/x1.png)\n> **图1**：Residual Flow Steering (RFS) 示意图。给定状态 s，RFS 策略 π_RFS 输出潜在流变量 w0（即文中的 a0）和残差动作 ar，二者共同引导预训练的基础流匹配策略 π_FM 产生最终动作 ab + ar。RFS 支持全局模式切换和细粒度残差修正，使策略能够超越演示数据流形。\n\n核心模块是统一的调制策略 π_RFS(a0, ar | s)。其创新点在于将输入调制（调整生成模型的初始噪声 a0）与输出调制（对生成的动作施加仿射修正 ar）相结合。输入调制通过 a0 = π_H(a0|s) 实现全局行为变化，输出调制通过 a = Des(s, a0, vθ) + ar 实现局部细化。这种设计使得 RFS 能够同时利用两种互补的探索形式：通过残差修正进行局部优化，以及通过潜在空间调制进行全局探索。\n\n本文为灵巧操作实例化了一个从仿真到现实的完整流程。\n\n![仿真到现实流程](https://arxiv.org/html/2602.01789v3/x2.png)\n> **图2**：仿真到现实的 RFS 流程概览。(1) 使用 VR 遥操作收集多任务演示数据，训练任务特定的流匹配基础策略。(2) 在仿真中，RFS 策略在每个基础策略之上进行微调，并蒸馏到任务特定的视觉运动策略中以改进仿真到现实的迁移。(3) 在零样本现实部署中，人类纠正动作用于修正执行失败。(4) 这些纠正后的转移数据用于在 Franka-Leap Hand 系统上对 RFS 进行离线微调，提升现实世界的抓取和抓放性能。\n\n**仿真训练**：首先收集少量 VR 遥操作演示，预训练基础流匹配策略 vθ。随后使用在线 PPO 算法训练 RFS 策略 π_RFS 以优化抓取成功率和稳定性。最后，利用训练好的 RFS 策略生成具有特权状态信息的仿真演示，并通过师生框架将其蒸馏为以点云为条件的视觉运动策略。\n\n**现实世界微调**：将蒸馏后的视觉策略零样本转移到现实世界后，常因新物体或不同初始条件而失败。为此，引入一个使用少量人类纠正数据进行离线 RFS 微调的阶段。在数据收集中，基础策略执行 rollout，人类在访问的状态下提供纠正动作，从而构建数据集 𝒟_RFS = {((o,s), (a0, ar), (o‘, s’), r)}，其中 ar = a_human - ab。随后使用离线强化学习（采用 TD3+BC 算法）训练 π_RFS(a0, ar | o_pc, s_pro)。\n\n**算法细节**：评论家更新（公式10）基于组合动作 a = ab + ar 进行标准 TD 学习。演员更新（公式11）在最大化评论家值的同时，对残差动作应用行为克隆正则化：Q(o, s, â) - λ_BC || âr - ar ||^2，以稳定离线学习。\n\n## 实验与结果\n实验在仿真和现实世界两个阶段进行。仿真阶段使用了六个灵巧操作任务进行评估：抓取、抓放、非抓握推至抓取、长时程码放、高精度堆叠和倾倒。使用 Apple Vision Pro AR 收集了每个任务约 400 条演示用于预训练基础策略。现实阶段在 Franka-Leap Hand 系统上进行抓取和抓放任务的零样本转移与离线微调。\n\n![实验对象](https://arxiv.org/html/2602.01789v3/x3.png)\n> **图3**：用于灵巧抓取和抓放任务的仿真和现实物体。\n\n对比的基线方法分为四类：(1) 扩散/流模型 RL 微调：DPPO、ReinFlow；(2) 离线到在线 RL：IQL、AWAC、Flow Q-Learning；(3) 结合演示的 RL：RLPD、IBRL；(4) 消融实验对比：DSRL（仅潜在噪声引导）、先进的残差 RL 方法（Policy Decorator, ResiP）。\n\n![任务示例](https://arxiv.org/html/2602.01789v3/x4.png)\n> **图4**：灵巧操作任务的代表性运行轨迹。从上到下：码放、推至抓取、码放和堆叠。\n\n关键仿真实验结果如表1所示。预训练的基础流匹配策略平均成功率仅为 0.250。RFS 在所有任务上均取得了最高成功率：抓取 0.899、抓放 0.939、码放 0.781、推至抓取 0.721、堆叠 0.951、倾倒 0.873，平均成功率达 0.861。与最强基线对比：DPPO 平均仅 0.178，ReinFlow 为 0.409，最强的离线到在线方法 IQL 为 0.488，仅噪声引导的 DSRL 为 0.483，先进的残差 RL 方法（如 ResiP）为 0.433。RFS 显著优于所有基线。\n\n消融实验表明，仅使用残差修正的方法（如 Policy Decorator, ResiP）在需要长时程或高精度任务上表现不佳；仅使用潜在噪声引导的 DSRL 在堆叠（0.135）和倾倒（0.268）任务上失败，因其修正被限制在演示分布内。RFS 结合两者优势，实现了全面超越。\n\n![奖励曲线对比](https://arxiv.org/html/2602.01789v3/figures/reward_summary.png)\n> **图7**：在抓取任务上，RFS 与关键基线方法的奖励曲线对比。RFS 学习更稳定、更高效，最终性能更高。\n\n现实世界离线微调实验表明，使用人类纠正数据进行离线 RFS 微调后，抓取成功率从零样本的约 40% 提升至 80% 以上，抓放任务成功率从约 30% 提升至超过 70%。\n\n![数据收集](https://arxiv.org/html/2602.01789v3/figures/data_collectimg.png)\n> **图8**：现实世界数据收集设置：人类操作员观察机器人执行并提供物理纠正，记录为 (状态, 人类动作, 基础策略动作) 三元组，用于构建离线微调数据集。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了 Residual Flow Steering (RFS) 框架，通过联合调制预训练流匹配策略的输入（潜在噪声）和输出（残差动作），统一了全局探索与局部修正。2) 展示了 RFS 在仿真灵巧操作任务中高效的数据生成和在线微调能力，平均成功率高达 0.861。3) 构建了完整的仿真到现实流程，并证明了利用少量人类纠正数据进行离线 RFS 微调可显著提升现实世界部署的鲁棒性。\n\n论文提到的局限性主要在于现实世界适应阶段仍需依赖人类交互提供纠正数据。这为后续研究提供了启示：如何进一步减少对昂贵人类监督的依赖，例如通过更高效的主动学习、基于模型的模拟或自我监督信号来获取修正数据；此外，该统一调制框架有望扩展到其他生成模型（如扩散模型）和更广泛的机器人学习领域。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01789v3/x1.png",
        "https://arxiv.org/html/2602.01789v3/x2.png",
        "https://arxiv.org/html/2602.01789v3/x3.png",
        "https://arxiv.org/html/2602.01789v3/x4.png",
        "https://arxiv.org/html/2602.01789v3/x5.png",
        "https://arxiv.org/html/2602.01789v3/x6.png",
        "https://arxiv.org/html/2602.01789v3/figures/reward_summary.png",
        "https://arxiv.org/html/2602.01789v3/figures/data_collectimg.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01662",
      "title": "AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act",
      "url": "http://arxiv.org/abs/2602.01662",
      "arxivId": "2602.01662",
      "date": "2026-02-02",
      "authors": "Yu She Team",
      "category": "Manipulation",
      "summary": "本文提出AgenticLab，一个模型无关的真实世界机器人代理平台与基准，旨在解决现有基于大视觉语言模型（VLM）的操纵系统在非结构化、长时程闭环执行中能力不明确、难以标准化评估的问题。平台核心是一个集成了感知、任务分解、在线验证与重规划的闭环代理流程。通过该平台对先进VLM代理进行真实机器人任务基准测试，揭示了离线测试无法捕捉的多种故障模式，包括多步基础一致性崩溃、遮挡与场景变化下的物体基础失效，以及空间推理不足导致的操纵不可靠。平台将开源以支持可复现评估。",
      "detailedSummary": "## 研究背景与动机\n目前，基于大视觉语言模型（VLM）的机器人操作已成为主流范式。然而，现有方法存在关键局限性：许多系统采用开环规划，生成静态计划后执行，缺乏对执行状态的持续验证和失败后的重规划，导致在非结构化、动态的真实环境中鲁棒性差。同时，现有评估大多基于模拟环境、特权状态或精心设计的受控场景，难以捕捉真实闭环执行中出现的失败模式（如遮挡、光照变化下的物体定位错误、多步语义一致性断裂等）。此外，许多系统与特定VLM深度耦合，包含模型特定的提示或设计，使得不同模型家族在统一执行协议下的公平比较变得困难。\n\n本文针对VLM在真实世界、长时程、闭环机器人操作中可靠性不明确的痛点，提出了一个模型无关的机器人代理平台和基准测试。其核心思路是构建一个支持感知、思考、执行闭环的模块化流水线，并利用该平台在真实非结构化环境中系统性地评估现有VLM的能力，揭示离线测试无法发现的失败模式。\n\n## 方法详解\nAgenticLab的整体框架是一个模块化的闭环机器人代理流水线，其核心是集成任务解析、感知、规划、执行、验证和重规划。输入是自然语言指令和机器人本体传感器的RGB-D观测，输出是完成指定任务的一系列机器人动作。\n\n![方法总览](https://arxiv.org/html/2602.01662v2/x2.png)\n\n> **图3**：AgenticLab的闭环推理流水线。人类提供指令，机器人获取观测并解析任务生成初始计划。绿色箭头表示正常执行路径，每个模块执行后都会进行VLM验证；若验证失败，则沿红色箭头进行重试或基于新观测重规划。抓取规划首先使用肩部相机，若失败则切换至腕部相机进行局部重规划。\n\n该框架基于三个核心原则：1) **模型无关性**：通过统一接口支持不同VLM（如Gemini, GPT, Qwen）的即插即用。2) **闭环推理**：迭代利用视觉反馈验证动作前提和效果，检测失败并触发重规划。3) **模块化功能分解**：将流水线分解为职责清晰的模块，便于解释、组合和交换。\n\n核心模块具体如下：\n1.  **See（感知）**：采用多视角（固定肩部相机+腕部相机）开放词汇感知。系统将RGB-D观测转换为结构化场景表示，支持开放词汇的物体定位和空间关系理解。感知策略是模块化的，可依赖VLM直接预测物体位置，或使用LangSAM进行分割后由VLM验证。感知是迭代进行的，在每次动作后更新场景表示，并能根据验证反馈切换视角（例如，从全局肩部视图切换到局部腕部视图以细化抓取规划）。\n2.  **Think（思考）**：这是一个模型无关的推理与规划层，包含三个关键模块：\n    *   **任务解析器**：将自然语言指令转换为结构化的规划域定义语言（PDDL）问题。利用预定义的PDDL域文件，VLM根据视觉观测和语言指令实例化物体、初始状态谓词和目标谓词。随后，基于Fast Downward的符号规划器生成满足目标的高层动作基元序列。\n    *   **动作检查器**：在执行每个动作前，评估其前提条件是否成立；执行后，利用更新后的视觉输入重新评估动作效果，并检测失败（如抓取失败、意外状态变化）。在所有计划动作完成后，进一步验证最终场景是否满足所有目标条件。\n    *   **抓取规划器**：首先使用AnyGrasp从RGB-D观测生成候选抓取位姿，然后由VLM评估其语义正确性（是否抓取目标物体）和物理可行性（是否可能导致碰撞或不稳定）。如图3所示，系统优先使用肩部相机规划，若候选抓取被拒绝，则使用腕部相机重新规划以获得更局部的视觉证据。\n3.  **Act（执行）**：将机器人行为抽象为一组高层动作基元（如抓取、放置、开/关抽屉）。每个基元通过基于位置的控制执行。例如，对于抓取基元，VLM通过视觉定位指定感兴趣区域，AnyGrasp在该区域内生成6自由度抓取位姿，机器人通过笛卡尔空间轨迹规划执行抓取。每次动作后都进行离散的结果验证。\n\n与现有方法相比，其创新点具体体现在：1) **模型无关的标准化评估接口**，使得公平比较不同VLM成为可能；2) **强调闭环推理与在线重规划**，而非一次性开环规划；3) **模块化设计**，允许对VLM的各项能力（如任务解析、视觉定位、动作验证）进行独立评估和针对性改进。\n\n## 实验与结果\n实验使用了AgenticLab平台，在真实物理机器人（UR5e机械臂、自制夹爪、Azure Kinect肩部相机和RealSense D405腕部相机）上进行了评估。基准测试包含五个精心设计的真实世界操作任务（排序、堆叠、填字游戏、重定向、厨房整理），覆盖了语义定位、空间推理和长时程规划等挑战，并在实验室、杂乱室内厨房和非结构化室外环境三种场景下进行测试。\n\n![任务示例](https://arxiv.org/html/2602.01662v2/x3.png)\n\n> **图4**：用于真实世界评估和基准测试的操作任务。这些任务涵盖了空间推理、视觉语言定位和长时程规划等一系列挑战。\n\n对比的基线方法包括多个主流VLM（Gemini 3 Flash/Pro/Robotics, Qwen-VL-Max, Qwen3-VL-Plus, GPT-5.2, GPT-4o, Claude-4.5-Opus/Sonnet）以及本地部署模型（Qwen2.5-VL-7B, Molmo-7B）。实验还对比了组合式流水线与单VLM流水线，并进行了消融实验。\n\n关键实验结果如下：\n1.  **单VLM驱动完整流水线的能力**：在排序任务（20次试验，6种物体设置）中，Gemini Flash成功率最高（75.0%），而Qwen-VL-Max和GPT-5.2成功率接近0%。失败主要源于动作状态验证中的幻觉（例如，成功抓取后却声称物体不在夹爪中）。\n\n![失败模式分解](https://arxiv.org/html/2602.01662v2/x4.png)\n\n> **图5**：排序任务中单VLM流水线的失败模式分解。结果显示不同VLM的失败主要由哪些流水线组件导致。\n\n2.  **模块化基准测试**：对各模块独立评估（表II）发现，任务解析并非云端模型的主要瓶颈；视觉定位能力与模型大小无关，小型专用模型可表现优异；动作检查器在孤立VQA测试中成功率常超过50%，但在闭环执行中因错误累积导致整体成功率急剧下降（例如，单个动作检查准确率90%时，完成需6次检查的任务概率仅为~53%）；抓取评估器的性能与“思考”推理能力（如碰撞风险、稳定性判断）强相关，Gemini Pro（77.8%）、GPT-5.2（75.9%）表现最佳。\n3.  **组合式流水线 vs. 单VLM**：将任务解析、视觉定位、目标检查分配给不同专家模型（Gemini Flash, Qwen3-VL-Plus, Claude Opus）的组合流水线，在从实验室转移到厨房和室外场景时，并未始终优于单VLM（Gemini Flash）基线（图6）。两者在环境变化下均出现性能下降。组合流水线的优势在于能针对特定瓶颈模块（如堆叠任务中的精确定位）进行针对性补强。\n\n![跨环境性能比较](https://arxiv.org/html/2602.01662v2/x5.png)\n\n> **图6**：跨环境性能比较。将Gemini Flash基线与组合式流水线在五种操作任务上进行比较，性能使用量化部分完成度的任务进度得分衡量。\n\n4.  **消融实验**：\n    *   **动作检查器**（图7）：在受干扰场景下，完全禁用检查器会导致任务进度得分下降，尤其是在动作强依赖的堆叠任务中。启用完整动作检查器虽增加执行时间，但能早期检测并纠正错误。\n    *   **抓取规划器**（图8a）：在杂乱场景中，禁用抓取评估会导致任务成功率大幅下降，因为系统更可能执行易碰撞或抓错物体的抓取。启用评估能提升性能，但代价是增加规划时间。\n\n![动作检查器消融](https://arxiv.org/html/2602.01662v2/x6.png)\n\n> **图7**：干扰场景下的动作检查器消融实验。报告了三种验证模式（无动作检查、仅目标检查、完整动作检查）下的任务进度得分和执行时间。\n\n![抓取规划器消融与VLA对比](https://arxiv.org/html/2602.01662v2/x7.png)\n\n> **图8**：(a) 抓取规划器消融研究。比较启用和禁用抓取评估时的性能与执行时间。在杂乱场景中，启用评估能提高性能，但增加执行时间。(b) 微调VLA与AgenticLab的性能对比。在排序和堆叠任务上，对比了使用40/30次演示微调的VLA与AgenticLab流水线。\n\n5.  **与微调VLA的比较**：在排序和堆叠任务上，将AgenticLab与一个使用40/30次演示进行微调的VLA模型（π_0.5）进行比较（图8b）。结果显示，即使经过任务特定微调，VLA的性能也未能超越AgenticLab的零样本流水线，突显了微调可能导致泛化能力下降以及零样本系统设计的价值。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了一个**模型无关的、支持闭环推理的真实世界机器人代理平台（AgenticLab）**，其模块化设计支持即插即用评估和部署。2) 建立了一个**真实机器人基准测试**，系统性地评估了VLM在长时程、闭环操作中的能力，揭示了离线测试和仿真中无法捕捉的失败模式（如验证错误累积、动态场景下的定位失效）。3) **开源了完整的软硬件栈设计**，旨在促进可复现的评估，并降低具身智能研究的入门门槛。\n\n论文自身提到的局限性包括：当前VLM在动作状态验证等关键模块上仍是性能瓶颈；组合式流水线的性能提升受限于最弱模块；硬件平台虽可重复部署，但仍存在物理限制（如负载、工作空间）。\n\n对后续研究的启示在于：1) **需要开发针对机器人闭环交互（特别是状态验证）进行优化的VLM或评估协议**，因为这是当前系统的关键短板。2) **模块化、可组合的代理架构**提供了一条无需昂贵端到端重新训练即可提升系统性能的实用路径。3) **真实世界的基准测试至关重要**，它暴露了仿真和静态评估中隐藏的问题，为模型改进指明了方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01662v2/Figure_and_Table/Teaser.png",
        "https://arxiv.org/html/2602.01662v2/x1.png",
        "https://arxiv.org/html/2602.01662v2/x2.png",
        "https://arxiv.org/html/2602.01662v2/x3.png",
        "https://arxiv.org/html/2602.01662v2/x4.png",
        "https://arxiv.org/html/2602.01662v2/x5.png",
        "https://arxiv.org/html/2602.01662v2/x6.png",
        "https://arxiv.org/html/2602.01662v2/x7.png",
        "https://arxiv.org/html/2602.01662v2/x8.png",
        "https://arxiv.org/html/2602.01662v2/x9.png",
        "https://arxiv.org/html/2602.01662v2/x10.png",
        "https://arxiv.org/html/2602.01662v2/x11.png",
        "https://arxiv.org/html/2602.01662v2/figures/precondition_pick_can-1_gpt_success.png",
        "https://arxiv.org/html/2602.01662v2/figures/effect_pick_can-1.png",
        "https://arxiv.org/html/2602.01662v2/figures/effect_pick_oreo-pack-1_gpt5_2.png",
        "https://arxiv.org/html/2602.01662v2/x12.png",
        "https://arxiv.org/html/2602.01662v2/x13.png",
        "https://arxiv.org/html/2602.01662v2/x14.png",
        "https://arxiv.org/html/2602.01662v2/x15.png",
        "https://arxiv.org/html/2602.01662v2/x16.png",
        "https://arxiv.org/html/2602.01662v2/x17.png",
        "https://arxiv.org/html/2602.01662v2/x18.png",
        "https://arxiv.org/html/2602.01662v2/x19.png",
        "https://arxiv.org/html/2602.01662v2/x20.png",
        "https://arxiv.org/html/2602.01662v2/x21.png",
        "https://arxiv.org/html/2602.01662v2/x22.png",
        "https://arxiv.org/html/2602.01662v2/x23.png",
        "https://arxiv.org/html/2602.01662v2/x24.png",
        "https://arxiv.org/html/2602.01662v2/figures/human_evaluation_ui.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01632",
      "title": "A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation",
      "url": "http://arxiv.org/abs/2602.01632",
      "arxivId": "2602.01632",
      "date": "2026-02-02",
      "authors": "Shreyas Kousik Team",
      "category": "Manipulation",
      "summary": "本文针对人形机器人上身远程操作中运动重定向延迟高、运动不自然的核心问题，提出SEW-Mimic闭式几何求解器。该方法通过肩、肘、腕关键点对齐机器人与人类手臂方向，实现快速最优解，适用于多数7自由度机器人。实验表明，推理速度达3 kHz，优于现有方法；用户研究显示提升任务成功率，且数据更平滑，有助于策略学习。",
      "detailedSummary": "## 研究背景与动机\n当前人形机器人遥操作中，将人体运动重定向到机器人姿态的主流方法可分为两类。第一类是仅基于末端执行器（手部）的重定向方法，它们将问题视为一个逆向运动学问题，通过雅可比矩阵或其伪逆等方法求解，以匹配人手的位置和朝向。这类方法存在数值不稳定（在奇异点附近）、计算速度受限，并且对于7自由度机械臂会产生冗余自由度的无效运动，导致机器人肘部姿态不受控，增加碰撞风险。第二类是基于关键点的优化方法，通过优化机器人关节角来最小化人体与机器人对应关键点（肩、肘、腕等）之间的欧氏距离。这类方法虽然能获得较高的姿态相似性，但优化求解耗时较长（例如平均0.7秒），在实时遥操作中引入延迟，可能导致收集的数据包含犹豫行为，影响后续策略学习的质量。\n\n本文针对现有方法在计算速度、姿态相似性（尤其是肘部控制）以及避免奇异点方面的局限性，提出了一个新的视角：将重定向问题重新定义为**方向对齐问题**，而非位置匹配问题。其核心思路是，通过对齐人体与机器人对应的上臂、下臂和手腕的方向，利用解析几何子问题的闭式解，实现快速、最优且无需标定的机器人姿态重定向。\n\n## 方法详解\nSEW-Mimic 方法的核心是将重定向问题转化为依次对齐机器人与人体对应肢体方向的过程。其输入是人体肩部 $\\mathbf{s}$、肘部 $\\mathbf{e}$、腕部 $\\mathbf{w}$ 关键点的3D坐标以及手部朝向矩阵 $\\mathbf{H}$，输出是机器人的关节角向量 $\\mathbf{q}$。该方法假设机器人是7自由度串联旋转关节机械臂，且相邻关节轴垂直。\n\n![方法框架](https://arxiv.org/html/2602.01632v1/x1.png)\n\n> **图2**：SEW-Mimic 方法整体框架。算法依次对齐机器人的上臂（第3关节轴 $\\mathbf{h}_3$）、下臂（第5关节轴 $\\mathbf{h}_5$）和手腕（末端执行器朝向），对应人体的上臂向量 $\\mathbf{u}$、下臂向量 $\\mathbf{l}$ 和手部朝向 $\\mathbf{H}$。\n\n**核心模块与技术细节**：\n1.  **问题建模**：定义优化目标为最小化方向误差，包括上臂方向误差 $\\mu_c(\\mathbf{u}, \\mathbf{R}^{0,3}(\\mathbf{q})\\mathbf{h}_3)$、下臂方向误差 $\\mu_c(\\mathbf{l}, \\mathbf{R}^{0,5}(\\mathbf{q})\\mathbf{h}_5)$ 和手腕朝向误差 $\\mu_m(\\mathbf{T}(\\mathbf{q}), \\mathbf{H})$。其中 $\\mu_c$ 基于余弦相似度，$\\mu_m$ 是基于Frobenius范数的旋转矩阵误差度量。该方法的关键创新在于仅使用方向误差，使其对不同体型的人和机器人无需标定。\n2.  **分步对齐算法**：算法顺序求解，每一步都通过求解经典的几何子问题获得闭式解。\n    *   **上臂对齐**：调用 `AlignAxis(3, q, u)`。该函数通过求解 **Subproblem 2**，找到机器人第1、2关节角 $(q_1, q_2)$，使得第3关节轴 $\\mathbf{h}_3$（代表机器人上臂方向）与人体上臂单位向量 $\\mathbf{u}$ 对齐。\n    *   **下臂对齐**：调用 `AlignAxis(5, q, l)`。在已求解 $q_1, q_2$ 的基础上，通过求解另一个 **Subproblem 2**，找到第3、4关节角 $(q_3, q_4)$，使得第5关节轴 $\\mathbf{h}_5$（代表机器人下臂方向）与人体下臂单位向量 $\\mathbf{l}$ 对齐。\n    *   **手腕对齐**：调用 `AlignWrist(q, H)`。首先，再次利用 **Subproblem 2** 求解第5、6关节角 $(q_5, q_6)$，使第7关节轴 $\\mathbf{h}_7$ 对齐到期望的手部指向方向。然后，利用 **Subproblem 1** 求解第7关节角 $q_7$ 以完成最终的手腕朝向匹配。\n3.  **几何子问题**：**Subproblem 1** 求解绕单一轴旋转一个向量以对齐另一个向量的最优角度。**Subproblem 2** 求解绕两个不同轴分别旋转两个向量以使它们对齐的最优角度对。这些子问题均有闭式解，是算法高速率的基石。\n4.  **安全过滤器**：作为下游应用示例，论文提出一个基于SEW-Mimic的安全过滤器用于避免双臂自碰撞。该过滤器在检测到潜在碰撞时，通过轻微调整输入的人体肘部关键点位置来修改机器人的肘部方向，从而避开碰撞，同时保持较高的姿态保真度。\n\n**与现有方法相比的创新点**：\n1.  **视角创新**：从匹配关键点**位置**转变为对齐肢体**方向**，使方法对尺寸差异不敏感（无需标定）。\n2.  **求解方式创新**：摒弃了基于雅可比矩阵的迭代优化或数值IK，采用解析几何子问题的**闭式解**，从而保证了最优性（对于定义的方向对齐问题）和极高的计算速度。\n3.  **直接肘部控制**：通过显式地对齐下臂方向，实现了对人体肘部姿态的直接映射，避免了冗余自由度导致的无效运动，提高了姿态相似性和安全性。\n\n## 实验与结果\n**实验设置**：\n*   **基准测试数据集**：使用 AMASS 数据集中的“跑步”动作序列。\n*   **对比方法**：\n    *   **仅末端方法**：Damped Least Squares (DLS) Jacobian pseudo-inverse。\n    *   **基于优化的关键点方法**：General Motion Retargeting (GMR)，以及在其基础上结合全身动力学模型的 TWIST。\n    *   **硬件遥操作**：ALOHA（硬件同步）。\n*   **评估平台**：在配备 Intel i7-13700K CPU 的台式机上运行。\n*   **评估指标**：计算时间、姿态相似性（方向误差）、任务成功率、策略学习效果等。\n\n**关键实验结果**：\n\n![精度对比](https://arxiv.org/html/2602.01632v1/figures/benchmark_comparison_accuracy.png)\n\n> **图4**：不同重定向方法在AMASS数据集上的姿态相似性（方向误差）对比。SEW-Mimic 与基于优化的 GMR 方法精度相当，且显著优于仅末端的方法（DLS）。\n\n![计算时间](https://arxiv.org/html/2602.01632v1/x3.png)\n\n> **图5**：不同方法的计算时间分布（对数尺度）。SEW-Mimic 的中值计算时间约为0.33毫秒（即频率约3kHz），比GMR快约3个数量级，比TWIST快约2-3个数量级。\n\n![集成TWIST](https://arxiv.org/html/2602.01632v1/figures/sew-twist_fig.png)\n\n> **图6**：将SEW-Mimic作为运动学初始猜测集成到TWIST中，可在保持TWIST高精度的同时，将其计算速度提升1-3个数量级。\n\n![任务成功率](https://arxiv.org/html/2602.01632v1/figures/task_completion_time.png)\n\n> **图15**：用户研究中的任务完成时间（越短越好）。使用SEW-Mimic的遥操作在“倒水”和“冲咖啡”任务中完成速度显著快于仅末端方法，与硬件方法（ALOHA）无显著差异。\n\n![策略学习损失](https://arxiv.org/html/2602.01632v1/figures/action_prediction_loss_bar.png)\n\n> **图16**：使用不同遥操作方法收集的数据训练行为克隆策略，在验证集上的动作预测损失。基于SEW-Mimic数据训练的策略损失最低，表明其数据质量更高。\n\n![平滑性分析](https://arxiv.org/html/2602.01632v1/figures/combined_motion_profile.png)\n![关节平滑性](https://arxiv.org/html/2602.01632v1/figures/smoothness_per_joint.png)\n\n> **图17与图18**：运动轮廓与各关节平滑性度量。SEW-Mimic生成的动作序列比仅末端方法（DLS）和优化方法（GMR）都更平滑，特别是在肘部相关关节（3、4关节）上。\n\n**消融实验**：\n论文通过替换不同模块来验证SEW-Mimic各组成部分对策略学习的影响。实验表明，使用完整的SEW-Mimic（包含方向对齐和闭式求解）收集的数据，训练出的策略性能最好。若使用位置误差替代方向误差，或使用数值优化替代闭式解，都会导致策略性能下降。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 **SEW-Mimic**，一种基于方向对齐的、闭式几何求解的上半身人形机器人重定向算法，具有**高速（3kHz）、最优且无需标定**的优点。\n2.  提出了一种基于SEW-Mimic的**安全过滤器**，用于在双臂遥操作中实时避免自碰撞。\n3.  通过大量实验证明，该方法不仅能提升遥操作任务成功率，其生成的**更平滑**的动作数据还能提升后续自主策略学习的性能，并且可以作为插件显著加速全身人形重定向系统。\n\n**局限性**：\n论文自身提到的局限性包括：方法目前专注于上半身（双臂），对于腿部需要类似扩展；假设机器人关节轴符合特定几何关系（相邻垂直）；安全过滤器目前仅处理静态障碍物（如另一只手臂），未考虑动态环境。\n\n**对后续研究的启示**：\n1.  **方向对齐的范式**：为运动重定向问题提供了一个新的、高效的建模思路，可推广至腿部或其他机器人构型。\n2.  **几何解的应用**：展示了经典解析几何IK在实时、高保真遥操作中的巨大潜力，鼓励在机器人学中重新审视和应用这类方法。\n3.  **系统构建基石**：SEW-Mimic的高速率特性使其能够作为底层模块，为更复杂的上层应用（如安全过滤、模型预测控制、在线学习等）留出充足的计算余量。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01632v1/figures/robot_and_human_arm_diagram.png",
        "https://arxiv.org/html/2602.01632v1/x1.png",
        "https://arxiv.org/html/2602.01632v1/x2.png",
        "https://arxiv.org/html/2602.01632v1/figures/benchmark_comparison_accuracy.png",
        "https://arxiv.org/html/2602.01632v1/x3.png",
        "https://arxiv.org/html/2602.01632v1/figures/sew-twist_fig.png",
        "https://arxiv.org/html/2602.01632v1/figures/subproblem_illustration.png",
        "https://arxiv.org/html/2602.01632v1/figures/GMR-Metric.jpg",
        "https://arxiv.org/html/2602.01632v1/figures/task_env_long_fpv_fig.png",
        "https://arxiv.org/html/2602.01632v1/figures/task_env_long_45_fig.png",
        "https://arxiv.org/html/2602.01632v1/figures/task_env_long_top_fig.png",
        "https://arxiv.org/html/2602.01632v1/x4.png",
        "https://arxiv.org/html/2602.01632v1/figures/rollout_comparison.png",
        "https://arxiv.org/html/2602.01632v1/figures/Action_Sequence_1.png",
        "https://arxiv.org/html/2602.01632v1/figures/task_completion_time.png",
        "https://arxiv.org/html/2602.01632v1/figures/action_prediction_loss_bar.png",
        "https://arxiv.org/html/2602.01632v1/figures/combined_motion_profile.png",
        "https://arxiv.org/html/2602.01632v1/figures/smoothness_per_joint.png",
        "https://arxiv.org/html/2602.01632v1/figures/action_repr_ablation.png",
        "https://arxiv.org/html/2602.01632v1/figures/pouring.png",
        "https://arxiv.org/html/2602.01632v1/figures/coffee.png",
        "https://arxiv.org/html/2602.01632v1/figures/snapshot_coffee.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01166",
      "title": "Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2602.01166",
      "arxivId": "2602.01166",
      "date": "2026-02-01",
      "authors": "Shanghang Zhang Team",
      "category": "Manipulation",
      "summary": "本文提出LaRA-VLA框架，旨在解决视觉-语言-动作模型中基于思维链的推理方法存在的推理开销高、离散推理表示与连续感知控制不匹配的核心问题。其关键技术是将多模态思维链推理内化为连续的潜在表示，在潜在空间进行统一推理与预测，并采用渐进式训练范式，从显式监督过渡到潜在推理。实验表明，该框架在仿真与真实机器人任务上性能优于现有方法，且相比显式思维链方法，推理延迟降低高达90%。",
      "detailedSummary": "## 研究背景与动机\n当前，提升视觉-语言-动作模型性能的主流方法之一是引入思维链推理。然而，现有方法面临两个关键局限性：首先，基于文本的CoT方法在推理时需要生成冗长的文本推理轨迹，导致计算开销巨大、内存消耗高、推理延迟严重，无法满足实时机器人控制的需求；其次，无论是文本CoT还是视觉CoT，大多依赖于离散的令牌表示，这与机器人领域中连续变化的感知和动作空间存在表征上的不匹配。本文针对推理效率低下和离散-连续表征不匹配这两个具体痛点，提出了一种新的视角：将多模态思维链推理内化到连续的潜表示中。本文的核心思路是：通过一个基于课程学习的训练范式，逐步将显式的文本和视觉推理转化为连续的潜表示，并使其直接条件化连续动作的生成，从而在推理时无需生成显式CoT，实现高效、面向动作的推理与控制。\n\n## 方法详解\nLaRA-VLA的整体框架是一个三阶段的课程学习流程，其输入为当前视觉观测和语言指令，输出为连续动作。三个阶段分别为：I) 使用对齐的视觉预测潜变量和逆动力学监督进行显式CoT微调；II) 基于课程从显式CoT过渡到紧凑的文本潜变量，逐步减少文本令牌数量并增加对潜推理的依赖；III) 将潜条件化的VLM特征适配到动作专家网络，以实现无需显式CoT的高效动作生成。\n\n![方法框架](https://arxiv.org/html/2602.01166v1/x2.png)\n> **图2**：LaRA-VLA整体框架。训练分为三个阶段：(i) 使用对齐的视觉预测潜变量和逆动力学监督进行显式CoT微调；(ii) 基于课程从显式CoT过渡到文本潜变量，潜表示同时受到视觉和动作信号的隐式监督；(iii) 将潜条件化的VLM特征适配到动作专家，实现无需显式CoT的高效动作生成。\n\n核心模块包括：1) 基于Qwen3-VL的骨干VLM，其图像编码器被直接继承以确保视觉表征的一致性；2) 用于预测未来视觉信息的专用`<img_next>`令牌；3) 在最终阶段使用的、由16层扩散Transformer构成的动作专家网络。训练过程的核心技术细节如下：\n*   **第一阶段（显式CoT微调）**：模型通过教师强制学习，使用负对数似然损失（公式1）生成显式的文本CoT序列。同时，引入视觉对齐损失（公式2，L1范数），使模型预测下一时刻的视觉潜表示。为了稳定潜表示学习并防止表征崩溃，目标视觉潜表示由一个在线视觉编码器的指数移动平均版本计算（公式3）。此外，模型还通过逆动力学模型，以前后视觉状态和指令、推理步骤为条件，以自回归方式预测离散动作令牌，使用动作令牌损失进行监督。\n*   **第二阶段（课程式替换离散CoT令牌）**：采用与第一阶段相同的训练目标，但通过一个预定义的课程计划，逐步将显式CoT令牌替换为可学习的潜表示。监督中离散CoT令牌的比例逐渐减少至零，最终整个思维链被完全内化到潜空间中。此阶段最终优化的损失函数为`0.2 * L_vis + L_act-dis`，以促进潜空间推理并保持准确的动作语义。\n*   **第三阶段（基于流匹配的动作生成）**：移除显式动作令牌预测，激活专用的动作专家网络。该网络以由VLM产生的多模态潜上下文（聚合了当前视觉、指令、文本推理潜和预测的未来视觉潜）为条件，通过流匹配损失（公式4）预测一个速度场，从而直接生成连续的动作轨迹。\n\n![注意力机制](https://arxiv.org/html/2602.01166v1/x3.png)\n> **图3**：LaRA-VLA的注意力机制。该机制根据训练阶段调节跨令牌信息流。在I、II阶段，未来图像令牌因果关注文本和当前图像令牌，动作令牌自回归生成。在III阶段，动作令牌被排除在注意力计算之外。\n\n与现有方法相比，创新点具体体现在：1) **推理表征连续化**：将文本和视觉CoT统一内化为连续潜表示，避免了离散令牌与连续控制之间的表征不匹配；2) **课程式内化策略**：通过渐进的三阶段训练，将显式推理知识稳定地迁移至潜空间；3) **高效推理架构**：推理时仅需生成紧凑的潜状态，并直接通过轻量级动作专家网络输出动作，大幅降低了延迟。\n\n## 实验与结果\n实验使用了LIBERO和SimplerEnv-WidowX这两个模拟基准测试，以及一个在真实机器人上收集的长时程操作任务数据集。构建了LIBERO-LaRA和Bridge-LaRA两个带有结构化CoT标注的数据集用于训练。\n\n对比的基线方法涵盖了不同CoT范式：无CoT的OpenVLA、π0、OpenVLA-OFT；文本CoT的ThinkAct、MolmoAct、π0.5、DeepThinkVLA；视觉CoT的CoT-VLA、DreamVLA、F1、UD-VLA；以及潜CoT的Fast-ThinkAct。在真实实验中，与ACT和GR00T N1.5进行了比较。\n\n关键实验结果：在LIBERO基准上，LaRA-VLA取得了97.9%的平均成功率，在Object和Long任务套件上分别达到99.8%和96.6%，全面超越了所有基线方法。在更具挑战性的SimplerEnv-WidowX基准上，LaRA-VLA以68.8%的平均成功率取得最佳性能。\n\n![真实世界设置](https://arxiv.org/html/2602.01166v1/x4.png)\n> **图4**：四个长时程真实机器人任务的实验设置。\n\n![真实世界结果](https://arxiv.org/html/2602.01166v1/x5.png)\n> **图5**：真实世界任务结果。LaRA-VLA在四个长时程操作任务上均优于ACT和GR00T N1.5基线，显示出更强的鲁棒性和长时程协调能力。\n\n![推理延迟对比](https://arxiv.org/html/2602.01166v1/x6.png)\n> **图6**：推理延迟对比。与显式文本CoT方法相比，LaRA-VLA在不同任务上实现了高达90%的延迟降低，推理频率可达约10 Hz。\n\n![消融实验](https://arxiv.org/html/2602.01166v1/x7.png)\n> **图7**：消融实验（对应正文表4）。该图展示了不同CoT监督形式对性能的影响，表明潜文本CoT和潜视觉CoT的结合带来了最佳效果。\n\n![注意力可视化](https://arxiv.org/html/2602.01166v1/x8.png)\n> **图8**：注意力可视化。展示了模型在推理过程中，文本潜变量如何关注与任务相关的视觉区域，证明了潜推理的空间基础能力。\n\n消融实验（表4）总结：仅使用显式文本CoT的成功率为58.33%；仅使用潜文本CoT提升至64.58%；同时结合潜文本CoT和潜视觉CoT（即完整的LaRA-VLA）进一步提升至68.75%。这表明潜推理本身比显式推理更有效，而多模态潜监督（视觉预测）对性能有进一步的积极贡献。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了适用于VLA模型的潜推理范式，将思维链推理内化为跨文本和视觉模态的连续潜表示；2) 设计了LaRA-VLA框架及配套的三阶段课程训练策略，实现了从显式推理到潜推理再到动作生成的平稳过渡；3) 构建了包含丰富多模态推理标注的数据集LIBERO-LaRA和Bridge-LaRA。\n\n论文自身提到的局限性主要隐含在方法复杂性中，例如多阶段课程训练的设计和调优可能具有一定挑战。\n\n对后续研究的启示在于：1) **潜推理的效率优势**：证明了在保持甚至提升性能的前提下，将推理过程压缩到潜空间可以极大降低推理开销，为实时部署VLA模型提供了可行路径。2) **连续表征的统一性**：将感知、推理、控制统一在连续表征空间内，是解决机器人领域离散-连续不匹配问题的有前景的方向。3) **课程学习与知识迁移**：所采用的渐进式内化策略为将大语言模型中的复杂推理能力高效迁移到具身控制领域提供了参考范式。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01166v1/x1.png",
        "https://arxiv.org/html/2602.01166v1/x2.png",
        "https://arxiv.org/html/2602.01166v1/x3.png",
        "https://arxiv.org/html/2602.01166v1/x4.png",
        "https://arxiv.org/html/2602.01166v1/x5.png",
        "https://arxiv.org/html/2602.01166v1/x6.png",
        "https://arxiv.org/html/2602.01166v1/x7.png",
        "https://arxiv.org/html/2602.01166v1/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01158",
      "title": "Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs",
      "url": "http://arxiv.org/abs/2602.01158",
      "arxivId": "2602.01158",
      "date": "2026-02-01",
      "authors": "Matteo Matteucci Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作模型在真实部署中因图像损坏（如电子噪声、坏点）导致性能严重下降的问题展开研究。作者提出了一种即插即用、模型无关的损坏恢复变换器，通过对抗训练直接修复损坏的视觉输入，无需微调原模型。实验表明，该方法能使模型在严重视觉损坏下保持接近基线的成功率，有效解决了VLA模型对传感器干扰的脆弱性。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型已成为通用机器人操作的主导范式，它将感知与控制统一在一个端到端的架构中。代表性的模型如RT-2、OpenVLA、π₀和SmolVLA，通过利用大规模多模态数据和强大的语言模型骨干，在遵循语言指令执行操作任务上展现出强大能力。然而，这些模型在受控研究环境中表现优异，其在实际场景中的可靠部署却严重受限于对视觉干扰的脆弱性。现有研究广泛解决了由场景几何结构引起的物理遮挡问题，通常通过主动感知或多视图推理来解决。但一个同样关键却未被充分探索的视觉退化来源是**图像损坏**。这些传感器层面的伪影，如电子噪声、坏点、镜头污渍等，会在视觉信号被VLA模型解读之前直接损害其完整性。本文系统性地量化了这种脆弱性，并针对这一具体痛点，提出了一种新的解决视角：在VLA模型上游插入一个轻量级、即插即用、模型无关的图像恢复模块，以净化损坏的视觉输入。本文的核心思路是设计一个**损坏恢复变换器**，通过对抗训练学习从损坏观测中重建干净的图像，从而免疫VLA模型免受传感器干扰的影响。\n\n## 方法详解\n整体框架的目标是在标准的机器人控制流程中，于VLA模型之前插入一个专用的恢复模块——损坏恢复变换器。该模块接收从相机获取的原始损坏观测帧，进行处理以减少损坏，然后将恢复后的图像传递给策略网络以预测动作。\n\n![系统流程](https://arxiv.org/html/2602.01158v1/pipeline.png)\n> **图1**：系统流程图。CRT模块被插入VLA模型的上游。它拦截来自模拟器的损坏观测x‘，将其恢复为x̂，并将干净的估计图像传递给策略网络进行动作预测。\n\n**核心模块：损坏恢复变换器**\nCRT是一个基于Transformer的重建模型，旨在将损坏的图像观测映射回其原始版本。其架构灵感来源于高效的图像恢复框架，并针对机器人感知领域进行了专门化改造。为了处理VLA所需的高分辨率输入（LIBERO为360×360，Meta-World为480×480）并保留对操作至关重要的细粒度纹理和物体细节，作者对基线架构进行了显著扩容：增大了潜在向量尺寸、增加了Transformer块的层数（以建模图像上复杂的损坏伪影统计）并扩展了注意力头的数量。\n\nCRT架构包含三个关键机制以确保所需的精度和效率：\n1.  **移位补丁标记化**：用于保留相邻像素间的空间关系，解决标准ViT缺乏局部归纳偏置的问题。\n2.  **旋转位置编码**：用于编码绝对和相对位置，使模型能够泛化到动作预测中使用的较长动作序列。\n3.  **局部自注意力**：用于锐化注意力分数，聚焦于局部纹理细节。\n\n![CRT架构](https://arxiv.org/html/2602.01158v1/CRT.png)\n> **图3**：损坏恢复变换器的架构。该模型利用三种专门机制：1）移位补丁标记化以恢复局部空间依赖；2）旋转位置编码以实现鲁棒的相对空间编码；3）局部自注意力以锐化纹理细节。深层的Transformer主干使模型能够从底层语义场景中解耦复杂的损坏伪影，最终将潜在标记重塑为恢复的RGB观测。\n\n**对抗训练策略**\n为了在恢复全局结构的同时保留对机器人操作至关重要的高频细节，避免仅使用像素级重建损失导致的模糊输出，本文采用了对抗训练框架。CRT模块作为生成器G，并引入一个判别器D来区分真实的干净观测x和重建的观测x̂ = G(x‘)。\n\n![对抗训练](https://arxiv.org/html/2602.01158v1/adversarial_scheme.png)\n> **图2**：CRT对抗训练示意图。CRT生成器接收损坏输入x‘并生成重建观测x̂。网络通过一个多目标损失函数进行优化，该函数结合了像素级保真度损失ℒ_L1、结构相似性损失ℒ_SSIM以及来自判别器D的对抗反馈损失ℒ_adv，以确保有效的重建。\n\n训练目标是一个加权组合的三部分损失：\n*   **对抗损失**：使用标准二元交叉熵目标，鼓励感知真实性。\n*   **重建损失**：\n    *   **L1损失**：促进稀疏性，相比L2减少模糊。\n    *   **结构相似性指数度量损失**：保留亮度、对比度和结构信息。\n最终的生成器总损失为：ℒ_total = λ_L1 ℒ_L1 + λ_SSIM ℒ_SSIM + λ_adv ℒ_adv^G。通过经验优化，超参数设置为λ_L1=10.0，λ_SSIM=1.0，λ_adv=0.05。对抗项权重较低，确保训练以内容重建为基础，同时提供足够的梯度信号来细化纹理细节并消除伪影。\n\n**创新点**\n与现有方法相比，本文的创新点主要体现在：1）首次系统性地关注并量化了VLA模型对传感器层面图像损坏的脆弱性；2）提出了一种**模块化、即插即用**的解决方案，无需对计算昂贵的底层VLA模型进行微调；3）将针对自然图像设计的先进Transformer恢复架构，通过扩容和对抗训练策略，成功适配到高分辨率的机器人视觉任务中。\n\n## 实验与结果\n**实验设置**：本文在LIBERO和Meta-World这两个广泛认可的机器人操作基准上评估CRT模块的有效性。集成了两种先进的VLA模型：在LIBERO和Meta-World上评估的SmolVLA，以及仅在LIBERO套件上评估的π₀.₅。\n\n**对比基线**：对比了VLA模型在干净观测下的基线性能，以及在五种图像损坏类型下的性能（无任何缓解策略）。\n\n**关键实验结果**：\n1.  **VLA对图像损坏的脆弱性**：实验结果证实了最先进的VLA模型对传感器伪影高度敏感。例如，在LIBERO-10上，π₀.₅在水平线损坏（0.5强度）下，成功率从90%暴跌至2%。在Meta-World MT50上，SmolVLA在相同条件下性能从58%减半至20.6%。\n2.  **CRT的有效性**：集成CRT后显示出显著的性能恢复，特别是对于π₀.₅架构。在LIBERO上，面对最严重的“水平线（0.5）”损坏，π₀.₅+CRT的成功率从2%反弹至87%，几乎与干净基线持平。在“水滴”损坏下从68%恢复至87%。对于Meta-World上的SmolVLA，CRT在所有损坏类型上均持续优于未缓解的模型，例如在“水平线”损坏上成功率提高了超过11%（从20.6%到32.2%）。\n3.  **基线性能保持与权衡**：CRT对π₀.₅模型引入的开销可忽略不计，在干净数据上保持了89%的成功率（基线为90%）。然而，对于更轻量级的SmolVLA架构，观察到性能权衡：在Meta-World上，使用CRT的基线性能从58%下降到47%。这表明较小的VLA对重建过程引入的细微分布偏移的鲁棒性较低，泛化能力不足。尽管如此，这种权衡被在恶劣条件下提供的巨大可靠性增益所显著抵消。\n4.  **效率**：CRT模块是一个轻量级神经网络，仅需10-50毫秒的推理时间和约1GB的VRAM，相比VLAs动辄数百毫秒的令牌生成延迟，其开销可忽略不计。\n\n**消融实验**：本文虽未进行标准的组件消融实验，但通过架构设计（SPT、RoPE、LSA的引入）和损失函数组合（L1、SSIM、Adv的加权）的选择， implicitly 验证了这些设计对于处理高分辨率机器人视觉重建任务的有效性。\n\n![视觉损坏类型](https://arxiv.org/html/2602.01158v1/corruptions.png)\n> **图4**：视觉损坏类型示例。从上左至下右：(a) 干净基线，(b) 中心方块，(c) 高斯噪声，(d) 水平线（0.5），(e) 水平线（0.2），(f) 水滴。这些损坏模拟了硬件故障、环境条件和信号传输错误。\n\n## 总结与启发\n**核心贡献**：\n1.  **问题界定与量化**：首次系统性地识别并量化了当前VLA模型对图像损坏（传感器伪影）缺乏鲁棒性这一关键漏洞，揭示了即使在严重损坏下成功率下降高达97.78%的极端情况。\n2.  **模块化解决方案**：提出了损坏恢复变换器，一种即插即用、模型无关的专用模块，通过对抗训练恢复视觉完整性，无需重新训练或微调底层VLA，为增强VLA的环境鲁棒性提供了一条可扩展的路径。\n3.  **全面验证**：在主流机器人基准上进行了广泛实验，证实了CRT对于大型VLA（如π₀.₅）能近乎完美地恢复性能，对于其他模型也能带来显著增益。\n\n**局限性**：\n论文明确指出，对于SmolVLA这类较小的架构，CRT在提升损坏条件下性能的同时，会对其在干净观测上的基线性能造成一定程度的下降（约18.96%），这揭示了在恢复有效性和输入保真度之间存在权衡。较小的VLA对恢复过程引入的细微分布偏移更为敏感。\n\n**对后续研究的启示**：\n1.  **条件式恢复**：鉴于CRT的模块化特性，未来工作可以探索仅在检测到损坏时才激活恢复流程的机制，从而在无损坏时完全保持VLA的原始性能。\n2.  **VLA与恢复模块的协同设计**：可以研究将恢复目标以更紧密的方式整合到VLA的预训练或微调过程中，或许能减轻对小型模型造成的分布偏移影响。\n3.  **扩展到更广泛的损坏类型与真实世界数据**：将方法应用于更多样的真实世界传感器故障和恶劣环境条件，并最终在物理机器人系统上进行验证。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01158v1/pipeline.png",
        "https://arxiv.org/html/2602.01158v1/adversarial_scheme.png",
        "https://arxiv.org/html/2602.01158v1/CRT.png",
        "https://arxiv.org/html/2602.01158v1/corruptions.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01153",
      "title": "UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors",
      "url": "http://arxiv.org/abs/2602.01153",
      "arxivId": "2602.01153",
      "date": "2026-02-01",
      "authors": "Shan Luo Team",
      "category": "Manipulation",
      "summary": "本文提出UniForce框架，旨在解决机器人操作中因触觉传感器异构性（如光学、磁性等原理差异）导致的力感知模型难以泛化的问题。其核心方法是通过学习跨传感器的共享潜在力空间，联合建模逆动力学与正动力学，并利用力平衡与图像重建损失约束，从而提取与力相关的统一表征。实验表明，该方法在GelSight、TacTip和uSkin等多种传感器上实现了力估计性能的稳定提升，并支持零样本迁移至下游任务（如机器人擦拭），无需针对新传感器重新训练。",
      "detailedSummary": "## 研究背景与动机\n机器人灵巧操作依赖于力感知，但触觉传感器的异质性阻碍了力感知策略学习的规模化。主流方法通常针对特定传感器（如GelSight、TacTip）进行数据收集、校准和模型训练，这限制了模型的泛化能力。现有跨传感器迁移方法主要分为两类：一是局限于同族视觉传感器的表征学习（如AnyTouch、T3），其潜在空间未明确与物理力对应；二是通过图像翻译实现跨传感器迁移（如GenForce），但需要针对每个目标传感器进行训练和额外的材料补偿，并非零样本。本文针对传感器异质性（传感原理、形态、材料不同）导致模型无法通用这一痛点，提出利用机器人自身本体提供的物理约束——准静态力平衡——来解耦传感器不变的接触力与传感器特定的触觉信号。本文核心思路是：通过力平衡约束下的逆动力学（图像到力）和前向动力学（力到图像）联合建模，学习一个跨异构触觉传感器的共享潜在力空间，并训练一个通用编码器，以实现下游任务（如力估计、策略学习）的零样本迁移。\n\n## 方法详解\nUniForce是一个条件变分自编码器框架，旨在从异构触觉传感器的配对数据中学习一个共享的、物理接地的潜在力空间。整体流程分为预训练和使用两个阶段：预训练阶段，利用力平衡抓取获得的配对触觉观测数据，训练一个通用编码器，将统一的标记图像映射到潜在力空间；使用阶段，冻结预训练的编码器，将其接入下游任务（如力预测头或VTLA模型），在单个传感器数据上训练任务头后，即可零样本迁移到其他传感器。\n\n![方法框架](https://arxiv.org/html/2602.01153v1/fig/f1.png)\n> **图1**：UniForce整体框架。利用力平衡抓取的配对数据，UniForce预训练一个通用编码器，从统一的标记图像表征中提取物理意义的潜在力，无需显式力标签。预训练后的编码器可通过在单个传感器数据上训练特定任务头，接入下游任务，实现跨传感器力预测和力感知策略学习的零样本迁移。\n\n![训练流程](https://arxiv.org/html/2602.01153v1/fig/f2.png)\n> **图2**：UniForce训练流程。在逆动力学阶段，编码器接收配对的参考/接触观测，推断出分块潜在力图；平衡损失对齐左右潜在力，KL项正则化后验分布。在前向动力学阶段，解码器以传感器特定的参考图像和潜在力为条件，重建接触观测，实现自重建和跨传感器重建。右手指标记图像被镜像以匹配左手指坐标。\n\n**核心模块与技术细节**：\n1.  **问题建模与统一表征**：将异构传感器集 \\(\\mathcal{S}=\\{s_1, \\dots, s_N\\}\\) 的原始信号（如RGB图像、多通道信号）规范化为统一的二维二值标记图像，输入定义为 \\(\\mathbf{x}=\\langle I_{\\text{ref}}, I_{\\text{cur}}\\rangle\\)，包含未变形参考帧和接触变形帧。\n2.  **编码器（逆动力学）**：采用因果时空Transformer。首先将参考帧和接触帧分块为令牌，并注入正弦位置编码。随后进行空间注意力（提取每帧局部标记结构）和因果时间注意力（允许接触帧关注参考帧，强制时序因果性）。最终输出被投影为分块潜在力图 \\(\\mathbf{z} \\in \\mathbb{R}^{N_p \\times 6}\\) 的对角高斯分布后验参数 \\([\\boldsymbol{\\mu}, \\log\\boldsymbol{\\sigma}^2]\\)。\n3.  **解码器（前向动力学）**：以参考图像 \\(I_{\\text{ref}}\\) 和潜在力 \\(\\mathbf{z}\\) 为条件，重建接触图像 \\(\\hat{I}_{\\text{cur}}\\)。通过加性注入将潜在力与参考图像嵌入融合，再经空间Transformer进行像素级重建。\n4.  **训练目标**：总损失由重建损失、KL散度损失和平衡损失加权构成。\n    *   **重建损失** \\(\\mathcal{L}_{recon}\\)：计算自重建和跨传感器重建（共四个分支）的 \\(\\ell_1\\) 损失和LPIPS感知损失。\n    *   **KL损失** \\(\\mathcal{L}_{KL}\\)：正则化后验分布接近标准正态分布。\n    *   **平衡损失** \\(\\mathcal{L}_{eq}\\)：强制左右手指推断的潜在力图在分块级别上一致（\\(\\ell_2\\) 损失）。\n\n**创新点**：\n1.  **物理接地的自监督目标**：利用准静态力平衡（\\(F_L \\approx F_R\\)）作为隐式监督信号，无需昂贵的外部六维力/力矩传感器标定数据。\n2.  **逆-前向动力学联合建模**：通过编码器学习力推断（逆动力学），通过解码器学习以力为条件的图像重建（前向动力学），共同约束潜在空间与物理力对齐。\n3.  **真正的零样本跨模态迁移**：与需要针对每个目标传感器进行翻译和训练的GenForce等方法不同，UniForce学习一个通用编码器，在下游任务中实现跨传感原理（视觉与非视觉）的即插即用零样本迁移。\n\n## 实验与结果\n**实验设置**：\n*   **数据集**：1) 自行收集的UniForce-pair配对数据集，包含GelSight-TacTip和uSkin-TacTip两种传感器对的抓取数据。2) 公开的GenForce-Hetero数据集，包含GelSight、TacTip、uSkin的单传感器触觉序列及其对应的真实力标签，用于评估。\n*   **传感器**：GelSight（视觉，平面，高分辨率RGB）、TacTip（视觉，曲面，内部标记点）、uSkin（磁感，非视觉，4x4阵列多通道信号）。\n*   **对比基线**：ResNet（全训练）、AnyTouch、T3、UniT（均为预训练的触觉表征模型），以及作为非零样本参考的GenForce。\n*   **评估任务**：潜在空间分析、跨传感器图像重建、零样本力估计、VTLA模型集成。\n\n**关键实验结果**：\n\n![潜在空间分析](https://arxiv.org/html/2602.01153v1/fig/r_f6.png)\n> **图4**：潜在维度（z0–z5）与力分量（Fy, Fx, Fz）的皮尔逊相关性热图。结果显示，法向力Fz与潜在维度z5强相关（r=-0.74），表明潜在空间成功编码了物理力信息，尤其是法向力。\n\n![零样本力预测误差](https://arxiv.org/html/2602.01153v1/fig/r_f2.png)\n> **图5**：跨六种异构传感器对的平均零样本力预测误差（MAE）。UniForce（橙色）在大多数迁移方向上性能优于或与需要逐传感器训练的GenForce（绿色）相当，显著优于其他零样本基线。\n\n![跨传感器重建](https://arxiv.org/html/2602.01153v1/fig/r_f3.png)\n> **图6**：跨传感器触觉图像重建示例（GelSight–TacTip和uSkin–TacTip）。使用从任一传感器推断的潜在力，可以驱动自重建和跨传感器重建，重建结果在接触位置和几何形状上与真实接触帧基本一致，验证了前向动力学的有效性。\n\n**表I：在未见物体上的零样本力预测R²分数对比**\n（根据正文描述概括）UniForce在绝大多数传感器对迁移方向（18个指标中的15个）上取得了最佳的零样本性能（R²为正且较高）。特别是在T->G迁移上，UniForce在Fx, Fy, Fz上分别达到0.77, 0.62, 0.83，显著优于所有其他零样本基线。而其他基线方法（如AnyTouch、T3、UniT）在许多迁移方向上出现了严重的负R²，表明其表征无法泛化到异质传感器。\n\n**消融实验贡献**：论文通过对比`UniForce（FT）`（编码器不冻结，全训练）和`UniForce`（冻结预训练编码器，仅训练力预测头）的结果表明，预训练获得的通用表征是零样本迁移成功的关键。全训练版本虽然在某些源传感器上表现尚可，但在跨传感器迁移时性能急剧下降（如表I中U->T的Fz R²从-1.51降至-155.04），说明其过拟合到了源传感器的特定特征。\n\n**下游任务集成**：\n\n![VTLA集成](https://arxiv.org/html/2602.01153v1/fig/r_f4.png)\n> **图7**：将UniForce集成到视觉-语言-动作模型中。异构触觉输入通过UniForce编码器被统一为力接地的令牌，用于力感知的机器人操作任务（如白板擦拭）。实验表明，集成UniForce的VTLA模型在混合传感器部署的擦拭任务中，成功率比仅使用视觉的模型提高了13%。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**UniForce框架**，首次通过力平衡约束下的逆-前向动力学学习，为异构触觉传感器（包括视觉与非视觉）建立了一个**共享的潜在力空间**和**通用编码器**。\n2.  设计了一种**物理接地、无需力标签的配对数据收集流程**，利用准静态抓取下的力平衡原理，为跨传感器对齐提供了可扩展的监督信号。\n3.  实现了下游力感知机器人操作任务的**零样本迁移**，在力估计和VTLA策略学习任务上验证了其有效性，无需针对新传感器进行重新训练或微调。\n\n**局限性**：\n1.  数据收集依赖于**准静态假设**，在动态、高速接触场景下的有效性有待验证。\n2.  对于**极端变形**或**信号非常稀疏**的传感器（如uSkin），跨传感器重建可能出现不一致。\n3.  潜在力空间对某些方向剪切力（如Fy）的编码较弱，这与数据集中该方向激励不足有关。\n\n**后续研究启示**：\n1.  **动态力平衡**：探索在非准静态交互中利用动力学约束进行表征学习。\n2.  **更丰富的力表征**：结合扭矩或压力分布信息，扩展潜在力空间的维度与物理意义。\n3.  **大规模多传感器预训练**：将框架扩展到更多类型的触觉传感器，构建真正通用的触觉基础模型。\n4.  **与机器人本体感知融合**：将触觉潜在力与关节力矩、本体感知等信息结合，构建更全面的机器人身体模型。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01153v1/fig/f1.png",
        "https://arxiv.org/html/2602.01153v1/fig/f2.png",
        "https://arxiv.org/html/2602.01153v1/fig/f3.png",
        "https://arxiv.org/html/2602.01153v1/fig/r_f6.png",
        "https://arxiv.org/html/2602.01153v1/fig/r_f2.png",
        "https://arxiv.org/html/2602.01153v1/fig/r_f3.png",
        "https://arxiv.org/html/2602.01153v1/fig/r_f4.png",
        "https://arxiv.org/html/2602.01153v1/fig/r_f5.png",
        "https://arxiv.org/html/2602.01153v1/fig/r_f7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01115",
      "title": "KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV",
      "url": "http://arxiv.org/abs/2602.01115",
      "arxivId": "2602.01115",
      "date": "2026-02-01",
      "authors": "Ziyang Wang Team",
      "category": "Manipulation",
      "summary": "本文针对基于扩散模型的机器人操作策略参数量大、推理效率低的问题，提出KAN-We-Flow方法。其核心是构建了轻量化的RWKV-KAN主干网络：RWKV模块高效融合时空与通道信息，GroupKAN层则通过可学习的样条函数进行特征非线性校准。此外，引入动作一致性正则化（ACR）损失来稳定训练。该方法无需大型UNet，在保持高速运行的同时，将参数量降低了86.8%，并在Adroit等多个标准测试集上取得了最优成功率。",
      "detailedSummary": "## 研究背景与动机\n基于生成模型的视觉运动策略在机器人模仿学习中占据主导地位，主要包括扩散模型和流匹配两种范式。扩散模型通过迭代去噪建模多模态动作分布，性能强大但推理效率低下，需要多步采样和庞大的UNet骨干网络，导致高延迟，难以部署在资源受限的机器人上。流匹配方法通过直接学习一个连续向量场，通常能实现一步生成动作，缓解了采样负担，但其现有实现大多仍沿用大型的UNet风格架构，参数量依然庞大，限制了在边缘设备上的部署。本文旨在解决现有生成式策略在保持高精度和快速推理的同时，模型参数量过大的痛点，提出了结合RWKV（Receptance Weighted Key Value）和KAN（Kolmogorov-Arnold Networks）来构建轻量级、高表达能力骨干网络的新视角。本文的核心思路是：设计一个名为RWKV-KAN块的轻量级模块，结合RWKV的线性复杂度序列建模能力和KAN基于可学习样条的紧凑函数表达能力，并引入动作一致性正则化来稳定训练，从而在显著减少参数量的同时，实现高性能的单步动作生成。\n\n## 方法详解\nKAN-We-Flow的整体框架是一个基于条件一致性流匹配的单步动作生成策略。输入包括一个加噪的动作轨迹、由单视角点云编码得到的视觉感知嵌入、机器人状态嵌入以及时间嵌入。这些条件被拼接后，送入一个轻量级的RWKV-KAN U型骨干网络（而非大型UNet）进行处理。该网络预测一个一步速度场，通过一致性流匹配学习一条直线流，最终在推理时以单步解码的方式实时生成动作序列。额外的动作一致性正则化（ACR）通过欧拉外推将预测轨迹与专家演示对齐，以稳定训练。\n\n![方法框架](https://arxiv.org/html/2602.01115v1/image/fig2.png)\n\n> **图2**：KAN-We-Flow方法总览。策略接收一个加噪动作和一个由点云感知嵌入、机器人状态嵌入及时间嵌入拼接而成的条件。该表示由一个轻量级RWKV-KAN U型骨干网络（替代大型UNet）处理；RWKV以线性复杂度混合长程时间/通道上下文，而KAN执行基于可学习样条的特征校准。然后，通过条件一致性流匹配学习直线流以产生一步速度场，最终以实时推理速度生成动作；额外的动作一致性正则化将欧拉外推轨迹与演示对齐以稳定训练。\n\n核心模块是**RWKV-KAN UNet**，它是一个三阶段的编码器-解码器，每个阶段堆叠RWKV-KAN块。\n1.  **RWKV模块**：包含时间混合（TM）和通道混合（CM）操作。时间混合通过具有指数时间衰减的递归公式（前向和后向扫描）高效聚合序列信息，实现线性复杂度的长程上下文传播。通道混合则对每个令牌应用门控MLP。两者通过前置层归一化和残差连接结合。\n2.  **GroupKAN模块**：这是对经典KAN的改进。首先，将输入特征在通道维度上分成G组（实践中G=4），每组由一个独立的KAN子网络处理，该子网络由可学习的单变量函数（如样条）矩阵构成，取代了传统MLP中的权重矩阵和固定激活函数。然后，引入一个轻量级的通道亲和力调制（CAM）模块，它通过对序列进行时间池化并经过一个小型前馈网络，生成一个通道门控向量，用于自适应地重新加权GroupKAN的输出。最后，通过DropPath和残差连接得到最终输出。\n\n![GroupKAN架构](https://arxiv.org/html/2602.01115v1/image/fig3.png)\n\n> **图3**：GroupKAN的架构示意图。展示了分组处理、每个组内的KAN函数矩阵以及通道亲和力调制（CAM）模块的组成。\n\n与现有方法相比，创新点具体体现在：1) **首次将RWKV和KAN架构引入机器人视觉运动策略学习**，用高效的RWKV-KAN块替代了参数庞大的UNet骨干网络；2) 提出了**动作一致性正则化（ACR）**，这是一种轻量级的辅助损失，通过欧拉外推强制策略预测的动作轨迹终点与专家演示对齐，为训练提供了额外的监督，提高了策略精度和稳定性，且不增加推理开销。\n\n训练目标包括**多段一致性流匹配损失（ℒ_MFM）**和**动作一致性正则化损失（ℒ_ACR）**。ℒ_MFM将时间区间[0,1]划分为K段（实践中K=2），在每段内分别强制执行解码终点和瞬时速度的一致性。ℒ_ACR则计算单步解码得到的动作轨迹在特定控制窗口内与专家动作的均方误差。最终损失为两者加权和（ℒ = ℒ_MFM + λ_ACR * ℒ_ACR，λ_ACR设为1）。\n\n## 实验与结果\n本文在三个广泛使用的机器人操作基准上进行了评估：**Adroit**（3个任务：Hammer, Door, Pen）、**DexArt**（4个任务：Laptop, Faucet, Toilet, Bucket）以及**Meta-World**（34个任务，按难度分为Easy-21个、Medium-4个、Hard-4个、Very Hard-5个）。\n\n对比的基线方法包括：\n*   **扩散策略**：DP3、Simple-DP3、MambaPolicy、Diffusion Policy、BCRNN、IBC。\n*   **流匹配策略**：Adaflow、FlowPolicy、MP1。\n其中DP3、MambaPolicy和FlowPolicy由作者在相同环境和专家数据下重新实现（标记为†）以确保公平比较。\n\n关键实验结果如下表所示，KAN-We-Flow在绝大多数任务和难度等级上取得了最佳成功率。\n\n![定量结果对比表](https://arxiv.org/html/2602.01115v1/image/fig6.png)\n\n> **图5**：在模拟环境中的定量对比结果表（对应论文表I）。数据显示，KAN-We-Flow在Adroit、DexArt和Meta-World的所有领域均达到了最佳性能。\n\n![性能、参数量、推理时间对比](https://arxiv.org/html/2602.01115v1/image/fig1.png)\n\n> **图1**：KAN-We-Flow与SOTA方法FlowPolicy和DP3在准确率、参数量和推理时间上的对比。(a) 在不同基准的困难任务上，KAN-We-Flow取得了更高的成功率；(b) 与FlowPolicy和DP3相比，本文方法实现了86.8%的参数量削减；(c) 在Adroit-Pen任务中，相比DP3，KAN-We-Flow实现了92.6%的推理时间下降，支持实时控制。\n\n![推理时间对比](https://arxiv.org/html/2602.01115v1/image/fig4.png)\n\n> **图6**：在Meta-World基准上各方法的推理时间对比（对应论文表II）。由于多步去噪过程，基于扩散的方法比基于流的方法慢。KAN-We-Flow在所有子任务上达到了最优的推理速度。\n\n消融实验总结了每个组件的贡献：\n*   **移除ACR**：在所有测试任务中性能下降，例如在Meta-World Hard任务上成功率下降6.7%，证明了ACR对于提升策略精度的有效性。\n*   **将RWKV-KAN替换为Transformer**：导致参数量增加约3.2倍，同时在Adroit Pen和Meta-World Hard任务上成功率分别下降9.0%和7.5%，验证了RWKV-KAN设计在参数效率和性能上的优势。\n*   **将GroupKAN替换为MLP**：在保持参数量相近的情况下，模型在多项任务上的性能下降，表明KAN基于可学习函数的表达方式比固定激活函数的MLP更具优势。\n\n![定性结果可视化](https://arxiv.org/html/2602.01115v1/image/fig7.png)\n\n> **图4**：操作结果可视化。展示了在Meta-World（Assembly, Disassemble）、Adroit（Hammer）和DexArt（Laptop）上的代表性任务执行过程。KAN-We-Flow策略预测未来的动作序列并持续发出指令，直至任务成功完成。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了一种新型的流匹配策略**KAN-We-Flow**，首次将高效的RWKV序列模型和紧凑的KAN函数网络集成到机器人视觉运动策略学习中，在保持高成功率的同时，大幅降低了模型参数量（减少86.8%）并实现了实时推理；2) 引入了**动作一致性正则化（ACR）**，这是一种轻量级的辅助训练目标，通过锚定预测轨迹终点来稳定训练并提高策略精度，且不增加推理成本；3) 在Adroit、Meta-World和DexArt等多个基准测试上进行了广泛实验，证明了该方法相比现有SOTA基线在成功率和效率上的综合优势。\n\n论文自身提到的局限性包括：尽管RWKV-KAN骨干网络显著减少了参数量，但在处理极高维状态或极长序列时，其计算开销仍需进一步优化；此外，方法在跨领域、零样本泛化能力方面尚未得到充分验证。\n\n本文对后续研究的启示在于：为机器人策略网络的设计提供了新的高效架构选择（RWKV与KAN），证明了在生成式策略中，通过精心设计轻量级但高表达力的骨干网络，可以同时达成高性能、高效率和低资源消耗的目标。未来工作可以探索该框架在更复杂的多模态指令跟随、长时程任务规划以及现实世界机器人部署中的潜力。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01115v1/image/fig1.png",
        "https://arxiv.org/html/2602.01115v1/image/fig2.png",
        "https://arxiv.org/html/2602.01115v1/image/fig3.png",
        "https://arxiv.org/html/2602.01115v1/image/fig7.png",
        "https://arxiv.org/html/2602.01115v1/image/fig6.png",
        "https://arxiv.org/html/2602.01115v1/image/fig4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01100",
      "title": "StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating",
      "url": "http://arxiv.org/abs/2602.01100",
      "arxivId": "2602.01100",
      "date": "2026-02-01",
      "authors": "Lu Fang Team",
      "category": "Manipulation",
      "summary": "本文针对长程机器人操作中视觉-语言-动作模型在每个时间步进行冗余推理导致高延迟的问题，提出StreamVLA双系统架构。其关键技术是“锁定-门控”机制：仅当检测到子任务转换时，才触发慢思考生成文本指令并想象特定的视觉完成状态作为时间不变的目标锚点；在稳定执行期间，则锁定高级意图以驱动流匹配动作头，跳过大部分自回归解码。实验表明，该方法在LIBERO基准上达到98.5%的成功率，相比全推理基线延迟降低48%（平均244ms→128ms），并在72%的时间步跳过了冗余计算。",
      "detailedSummary": "## 研究背景与动机\n当前，用于机器人操作的视觉-语言-动作模型通常采用同质的计算策略，即在每个控制时间步都执行昂贵的自回归多模态推理，即使子任务状态未改变，也会重新生成高层计划和视觉特征。这种推理与控制过程的纠缠导致了显著的计算冗余和高延迟，阻碍了模型在实时流式环境中的部署。现有缓解延迟的方法，如令牌剪枝或缓存策略，往往以丢失上下文信息为代价；而结合预测建模的方法，如CoT-VLA，通常以固定时间间隔预测未来帧，由于现实世界执行速度的随机性，这种固定偏移的预测可能无法对应任何有语义意义的状态。\n\n本文针对长时程操作中推理与控制循环耦合导致的效率低下和目标漂移问题，提出了一个新视角：将高层规划（System 2）与底层控制（System 1）解耦。核心思路是，仅在检测到子任务转换时触发“慢思考”来生成文本指令并想象特定的视觉完成状态，该完成状态作为一个时间不变的目标锚点来指导策略；在稳定执行期间，则“锁定”这些高层意图来驱动一个高效的“快动作”控制器，从而跳过冗余推理步骤。\n\n## 方法详解\nStreamVLA是一个在单一参数高效骨干网络中统一文本任务分解、视觉目标想象和连续动作生成的双系统架构。其核心是通过一个“锁定与门控”机制智能地调制计算流。\n\n![方法概述](https://arxiv.org/html/2602.01100v2/x1.png)\n> **图1**：StreamVLA概述。一个包含用于自适应子任务规划和未来想象的慢思考（紫色路径），以及用于连续控制的快动作（蓝色路径）的双系统架构。通过使用预测的未来图像来门控推理，它在子任务未完成时（72%的时间）跳过冗余计算，实现了48%的平均延迟降低（244ms → 128ms）。\n\n整体框架如**图2**所示。模型接收自然语言指令、多视角视觉观测和本体感知状态流作为输入。在每个时间步，模型输出一个统一元组：文本子任务描述、视觉完成状态图像以及连续动作块。其核心创新在于动态门控机制：一个轻量级门控模块持续比较当前状态与锁定的完成图像之间的语义差异，根据差异分数决定系统运行模式。\n\n![架构总览](https://arxiv.org/html/2602.01100v2/x2.png)\n> **图2**：StreamVLA架构。我们的框架通过锁定与门控机制，将稀疏推理与高频控制统一起来。共享的VLA骨干网络处理多视角观测。轻量级门控模块持续比较当前状态与锁定的完成图像。如果差异较小（子任务进行中），系统进入跳过模式，绕过计算昂贵的头部（系统2）并重用缓存的子任务计划。当检测到转换时，触发完整模式以生成新的文本计划和视觉完成目标。动作专家（系统1）通过流匹配合成精确的运动轨迹，并以来自系统2的锁定高层语义和视觉意图为条件。\n\n**核心模块与技术细节**：\n1.  **未来完成视觉想象头**：与预测固定时间间隔未来帧的方法不同，此头被训练生成当前子任务成功时的视觉状态（完成状态）。这提供了一个对时间变化鲁棒的视觉目标锚点。该头基于Infinity的比特级自回归范式实现，作为一个轻量级解码头附加在共享骨干上。为保持稳定的进度跟踪，它专门针对固定的头戴式摄像头视图进行预测。\n2.  **锁定与门控机制**：门控模块计算一个差异分数，表示当前头戴观测与锁定完成目标之间的语义距离。它通过交叉注意力对齐两者，并融合子任务指令嵌入，最后通过一个轻量级MLP分类器预测分数。门控逻辑基于阈值：当差异分数高（>τ，默认0.5）时，表示子任务正在进行，系统进入“跳过模式”，绕过自回归推理头，重用锁定的计划驱动动作专家；当差异分数低（≤τ）时，表示已到达视觉目标，触发“完整推理模式”，激活系统2以重新规划。\n3.  **统一多模态输出与层次化条件作用**：\n    *   **系统2（自回归规划，稀疏）**：包含子任务头（生成文本指令）和想象头（生成视觉完成状态）。输出被缓存（锁定）以指导底层控制器。\n    *   **系统1（流匹配动作专家，密集）**：采用条件流匹配进行快速、确定性的轨迹生成。其关键创新在于层次化条件机制：动作头以一个复合上下文向量为条件，该向量融合了当前感知、锁定的文本目标语义意图以及锁定的视觉目标空间预期，从而确保运动命令既符合任务逻辑又精确对准几何目标。\n\n与现有方法相比，StreamVLA的核心创新在于首次在统一的VLA中引入了**前瞻驱动的门控机制**。它利用自生成的、语义明确的子任务完成状态作为内在参考和终止信号，动态地基于实际语义进展（而非固定时间）来分配计算资源，从而在保持深度推理能力的同时显著提升效率。\n\n![视觉想象示例](https://arxiv.org/html/2602.01100v2/x3.png)\n> **图3**：未来完成视觉想象示例。三个LIBERO任务展示了（从左到右）：当前观测、预测的未来完成帧、以及真实完成状态。与预测下一帧的方法不同，StreamVLA生成子任务完成状态（目标）。该目标在子任务执行期间保持稳定，作为门控机制的鲁棒锚点。\n\n![门控机制详解](https://arxiv.org/html/2602.01100v2/x4.png)\n> **图4**：锁定与门控机制。(a) 门控模块通过比较当前观测与锁定目标来计算差异分数。(b) 控制逻辑：如果差异分数大，系统锁定推理并执行动作（跳过模式）；如果差异分数小，则触发重新规划（完整模式）。\n\n## 实验与结果\n**实验设置**：\n*   **基准/数据集**：LIBERO长时程推理基准（四个任务套件）、RoboTwin 2.0动态控制基准（50个双臂任务）、以及自收集的真实世界操作数据集。\n*   **实验平台**：6-DoF AgileX Piper机械臂（真实世界），仿真环境。\n*   **对比基线**：包括OpenVLA、OpenVLA-OFT、CoT-VLA、π₀、π₀.₅、RT-2、ACT、DP3、RDT等众多主流和SOTA方法。\n\n**关键实验结果**：\n1.  **LIBERO基准**：StreamVLA以3B参数量取得了98.5%的平均成功率，刷新了SOTA（超越之前最佳的OpenVLA-OFT 1.4%）。在要求连续子目标执行的最长时程套件（LIBERO-Long）上，StreamVLA保持了96.6%的成功率（仅下降2.6%），而标准情景模型如OpenVLA和CoT-VLA则分别出现了31%和18.5%的性能下降，这证明了其前瞻门控能有效缓解目标漂移。\n\n![LIBERO结果表](https://arxiv.org/html/2602.01100v2/x5.png)\n> **表I**：LIBERO基准上的对比。StreamVLA（3B参数量）在平均成功率上达到98.5%，优于所有对比方法，包括参数量更大的模型，展示了其结构设计对规模的替代优势。\n\n2.  **RoboTwin 2.0基准**：在应用了激进域随机化的Hard任务集上，StreamVLA保持了37.2%的成功率，超越了第二好的方法（RDT）11.2%。而标准策略如ACT则出现了灾难性下降（从50.8%到7.4%）。这表明，通过将控制锚定在稳定的语义目标上，StreamVLA有助于策略泛化超越表面的视觉统计特征。\n\n![RoboTwin结果表](https://arxiv.org/html/2602.01100v2/x6.png)\n> **表II**：RoboTwin 2.0基准上的对比。在具有挑战性的Hard任务集（强域随机化）上，StreamVLA展现了最强的鲁棒性，平均成功率领先其他方法。\n\n3.  **真实世界评估**：在包含长时程逻辑、高精度几何操作以及动态人为干扰的真实任务中，StreamVLA均取得了最佳成功率。更重要的是，相比需要每一步都进行推理的基线，StreamVLA实现了**48%的延迟降低**（从平均244ms降至128ms），并且72%的时间步跳过了昂贵的自回归解码。\n\n![真实世界结果表](https://arxiv.org/html/2602.01100v2/x7.png)\n> **表III**：真实世界成功率。StreamVLA在拼写、高精度插入以及受干扰的拼写任务上均优于对比基线。\n\n**消融实验**：\n论文进行了关键组件的消融研究：\n*   **移除门控（固定间隔规划）**：性能下降3.5%，延迟增加105%，验证了动态语义门控的有效性。\n*   **移除视觉目标（仅文本条件）**：在需要空间精度的任务上成功率下降7.2%，证明了视觉完成状态作为空间目标锚点的重要性。\n*   **移除文本指令（仅视觉条件）**：在抽象推理任务上性能下降，突出了语义指导的必要性。\n*   **门控阈值τ的影响**：τ=0.5时在成功率和延迟间取得最佳平衡。\n\n## 总结与启发\n**核心贡献**：\n1.  **统一的双系统架构**：提出了StreamVLA，一个在单一共享骨干网络中无缝集成慢思考（规划）和快动作（控制）的框架，以最小的内存开销实现深度推理。\n2.  **首个前瞻驱动的门控机制**：引入了一种新颖的动态门控策略，利用想象的子任务完成状态作为终止信号，首次使用自生成的视觉前瞻来主动调制VLA的计算分配，以语义精度跳过冗余推理步骤。\n3.  **SOTA的性能与效率**：在多个基准上达到最先进性能，并在真实部署中显著降低延迟，有效解决了推理深度与执行速度之间的权衡。\n\n**局限性**：\n论文提到，当前方法主要针对操作速度相对适中的任务。在需要极快速反应（如动态抓取）的场景中，即使系统1（快动作）也可能无法满足延迟要求。此外，视觉想象头的计算成本虽然被摊销，但在触发时仍会产生峰值延迟。\n\n**启示**：\nStreamVLA为构建高效、实时的通用机器人智能体提供了一个有前景的范式。其核心思想——**将高层规划解耦为稀疏的、基于语义进展的事件，并用其输出作为时间不变的条件来驱动一个高效的低层控制器**——可以推广到其他需要分层决策的领域。未来工作可以探索更轻量级的想象模型、处理更快速动态场景的机制，以及将门控信号扩展到多模态（如触觉、力觉）以进一步提升鲁棒性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01100v2/x1.png",
        "https://arxiv.org/html/2602.01100v2/x2.png",
        "https://arxiv.org/html/2602.01100v2/x3.png",
        "https://arxiv.org/html/2602.01100v2/x4.png",
        "https://arxiv.org/html/2602.01100v2/x5.png",
        "https://arxiv.org/html/2602.01100v2/x6.png",
        "https://arxiv.org/html/2602.01100v2/figures/subtask_editor.png",
        "https://arxiv.org/html/2602.01100v2/figures/batch_81.png",
        "https://arxiv.org/html/2602.01100v2/figures/batch_13.png",
        "https://arxiv.org/html/2602.01100v2/figures/batch_45.png",
        "https://arxiv.org/html/2602.01100v2/figures/batch_46.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01085",
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "http://arxiv.org/abs/2602.01085",
      "arxivId": "2602.01085",
      "date": "2026-02-01",
      "authors": "Quang-Cuong Pham Team",
      "category": "Manipulation",
      "summary": "本文提出一种仅通过观察形状来估计可变形线性物体（如电线）所受外力作用的方法。核心问题是解决机器人操作中接触点不在末端执行器时的力交互检测难题。关键技术基于力-扭矩平衡方程推导一致性条件，结合离散弹性杆（DER）模型计算内部扭矩，通过求解线性方程组同时估计外力作用位置和大小。该方法在仿真中达到高精度，并在真实场景实验中成功验证了其有效性。",
      "detailedSummary": "## 研究背景与动机\n在机器人与电线（可变形线性物体，DLO）交互的任务中，接触点常常并非位于机器人的末端执行器，而是发生在机械臂的其他位置，例如当机器人间接操作电线（如轻推）或电线作为环境中的被动障碍物时。准确识别这些相互作用力对于安全高效的轨迹规划至关重要，有助于防止电线损坏、避免机器人运动受限并降低潜在风险。现有方法通常依赖昂贵的六维力/扭矩传感器，或假设接触发生在末端执行器以进行精确的力估计。本文针对无需额外传感器、仅从观察到的电线形状来检测和估计外部作用力的需求，提出了一种新的分析视角。其核心思路是：假设电线处于或接近静态平衡，利用从深度相机获取的形状信息，通过推导的力-力矩平衡一致性条件，求解线性方程组来估计外力作用的位置和大小。\n\n## 方法详解\n方法的整体流程从深度相机获取场景信息开始，通过分割和掩码提取电线，经过平滑和插值后得到离散化的电线形状。基于离散弹性杆（DER）模型计算内部刚度扭矩，然后应用一致性条件将离散的杆件分类为未受扰（UD）段和受扰（D）段。最后，利用分类结果和力-力矩平衡方程求解外部力的大小和/或作用点。\n\n![方法框架](https://arxiv.org/html/2602.01085v1/x1.png)\n\n> **图1**：整体力估计流程。从上左至下右：实验真实图像、深度信息、电线分割掩码、最终平滑后的电线形状及实际与估计的力（箭头：红色-估计的末端夹具力，黑色-实际力，绿色-估计的外部力）。\n\n**核心模块一：基于一致性条件的段分类**\n该方法的核心创新在于推导了一组一致性条件，用于判断离散弹性杆的相邻杆件是否属于同一个未受外部扰动的段（UD段）。假设电线处于静态平衡，对于离散杆件 *i*，其总扭矩为零。若相邻杆件 *i* 和 *i+1* 同属一个UD段，则外力合力 **F**^i = **F**^{i+1}，由此可推导出第一个一致性条件（条件A）：**e**^i · **c**^{i+1} + **e**^{i+1} · **c**^i = 0，其中 **e**^i 是杆件向量，**c**^i 是刚度扭矩。但条件A仅是必要的，非充分条件。为确保充分性，需要检查三个相邻杆件 *i, i+1, i+2*。构建矩阵 **A**_i = [[**e**^i]_×, [**e**^{i+1}]_×, [**e**^{i+2}]_×]^T 和向量 **C**_i = -[**c**^i, **c**^{i+1}, **c**^{i+2}]^T，其中 `[·]_×` 表示向量的斜对称矩阵。通过最小二乘法求解 **F**^* = argmin_||**A** **F** - **C**||_2^2，若解 **F**^* 能很好地满足方程（根据设定的阈值判断），则这三个杆件属于同一UD段。此即充分必要条件（条件B）。\n\n**核心模块二：外力参数求解**\n1.  **外力大小估计**：在完成UD段和D段分类后，每个UD段的外力合力 **F**_R 可通过该段内所有求解出的 **F**^* 取平均得到。由于电线由交替的UD段和D段组成，作用在D段 *j* 上的外力 **f**^j 可通过相邻UD段的合力差加上重力贡献计算得出：**f**^j = **F**_R^k - **F**_R^l + **f**_g^j。\n2.  **外力作用点或外部扭矩估计**：这是一个欠定问题，需引入额外假设以获得确定解。论文提供了三种方案：一是已知力作用点 **p**_j，求解作用在该D段上的外部扭矩 **τ**^j；二是假设作用在D段上的外部扭矩为零（**τ**^j = 0），求解力的精确作用点 **p**_j；三是假设力作用在D段的质心，不计算扭矩。在本文的实物实验中，采用了第三种方案，因为从RGB-D数据中捕获电线内部的扭转仍然是一个开放挑战。\n\n与现有方法相比，本文的创新点在于将机器人本体感知（proprioceptive sensing）的力/碰撞估计思想转移到了电线上，利用弹性杆的内部扭矩作为“代理”传感器。电线的固有离散化特性使得力的定位和估计更为精确。\n\n## 实验与结果\n实验分为仿真验证和实物验证两部分。\n\n**仿真实验**：在一个改编的DER模型环境中进行，电线两端被夹持，用户通过交互界面施加外力。算法仅以电线的位姿作为输入，预测外部相互作用。\n\n![仿真结果](https://arxiv.org/html/2602.01085v1/x2.png)\n\n> **图2**：仿真实物实验的力估计结果。上图显示了施加力后的电线截图（实际力与估计力重叠显示）。进行了两组实验：P1展示了在电线长度中心施加不同大小和方向的力时的估计；P2展示了力作用点变化时的位置估计。实线和虚线分别代表实际和估计数据。\n\n仿真结果表明，在准静态平衡假设下，电线运动较少时力估计性能更好。尽管如此，即使在电线运动显著的情况下，力估计也能良好工作。\n\n**实物实验**：\n*   **平台与数据**：使用Denso VS-060机械臂，末端装有六维力/扭矩传感器。通过基于GROUNDED-SAM的流程从RGB-D图像中获取电线形状，并平滑、离散化为30段。假设电线为零扭转构型。\n*   **实验设计**：分为两组。实验A：夹持点在电线中心（距左端25cm），将末端执行器纯平移移动到6个不同位置。实验B：夹持点偏移电线中心7cm（距左端18cm），移动到4个不同位置。\n*   **对比基准**：以力传感器读数为真实值（ground truth）。\n*   **评估指标**：相对L2误差（‖**F**_est - **F**_act‖_2 / (‖**F**_act‖_2 + ε)）、角度差（°）、作用点位置差（mm）。\n\n![实验A结果](https://arxiv.org/html/2602.01085v1/x3.png)\n\n> **图3**：实验A（中心夹持）的力估计可视化结果。左列为真实图像（附坐标系）及被抓持点的笛卡尔位移；右列为平滑后的电线形状以及实际力（黑箭头）与估计力（绿箭头）向量。红箭头为估计的末端夹具力，其精度未作分析。\n\n![实验B结果](https://arxiv.org/html/2602.01085v1/x4.png)\n\n> **图4**：实验B（偏移夹持）的力估计可视化结果。\n\n**关键结果总结**：定量结果如表I所示。在部分场景（A4, A5, B1, B4）中，算法检测到了额外的错误力，分析时仅比较了最大幅值的力。总体而言，单一方向位移的力估计相对L2误差和角度差通常较低。研究发现，沿x轴方向（大致平行于初始电线方向）的力和位移较难准确检测，导致较大的误差。这提示当前算法对沿电线切线方向的力不敏感，可能源于离散杆件位置/方向以及物理属性误差的敏感性。力作用点位置的估计误差均小于40毫米，实验A的结果优于实验B。\n\n**消融实验分析**：论文通过仿真实验（P1和P2）分别验证了在力大小变化和力作用点变化情况下的算法性能，结果表明算法能够有效估计力的方向和大小，以及力的作用位置。\n\n## 总结与启发\n本文的核心贡献有两点：1) 提出了一套新颖的一致性条件，用于估计处于静态平衡的弹性杆上外部相互作用的位置；2) 利用这些相互作用位置，通过从每个离散杆件的力-力矩平衡导出的线性方程组，求解外力和扭矩的参数。\n\n论文明确指出了自身的局限性：1) 假设电线表现为纯弹性，这与现实不完全相符；2) 从RGB-D数据中捕获内部扭转是一个尚未解决的挑战，因此实物实验仅聚焦于力估计，并假设力作用于受扰段的质心。\n\n这项工作对后续研究的启示在于：为无外部传感器的力交互估计提供了一种基于物理模型的分析思路。未来的工作可以探索更复杂的塑性变形模型，改进基于视觉的扭转估计方法，并研究如何将该框架扩展到动态交互或更复杂的多接触点场景中，以提升其在非结构化环境中的实用性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01085v1/x1.png",
        "https://arxiv.org/html/2602.01085v1/x2.png",
        "https://arxiv.org/html/2602.01085v1/x3.png",
        "https://arxiv.org/html/2602.01085v1/x4.png",
        "https://arxiv.org/html/2602.01085v1/x5.png",
        "https://arxiv.org/html/2602.01085v1/x6.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp1/rgb_000001_min.png",
        "https://arxiv.org/html/2602.01085v1/x8.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp1/rgb_000002_min.png",
        "https://arxiv.org/html/2602.01085v1/x9.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp1/rgb_000003_min.png",
        "https://arxiv.org/html/2602.01085v1/x10.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp1/rgb_000004_min.png",
        "https://arxiv.org/html/2602.01085v1/x11.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp1/rgb_000005_min.png",
        "https://arxiv.org/html/2602.01085v1/x12.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp1/rgb_000006_min.png",
        "https://arxiv.org/html/2602.01085v1/x13.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp2/rgb_000001_min.png",
        "https://arxiv.org/html/2602.01085v1/x14.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp2/rgb_000002_min.png",
        "https://arxiv.org/html/2602.01085v1/x15.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp2/rgb_000003_min.png",
        "https://arxiv.org/html/2602.01085v1/x16.png",
        "https://arxiv.org/html/2602.01085v1/pics/exp2/rgb_000004_min.png",
        "https://arxiv.org/html/2602.01085v1/x17.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.01067",
      "title": "A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation",
      "url": "http://arxiv.org/abs/2602.01067",
      "arxivId": "2602.01067",
      "date": "2026-02-01",
      "authors": "Jose Barreiros Team",
      "category": "Manipulation",
      "summary": "本文系统研究了用于机器人操作的大型行为模型（LBMs）的协同训练问题，旨在解决现有机器人数据覆盖不足导致的泛化能力局限。研究评估了五种协同训练数据模态（标准视觉语言数据、带密集语言标注的机器人轨迹、跨具身机器人数据、人类视频、离散动作令牌）及不同训练策略。核心结论表明，结合视觉语言数据与跨具身机器人数据进行协同训练能显著提升模型对分布偏移、未见任务和语言指令的泛化能力，而离散动作令牌则无显著增益。实验基于4000小时机器人/人类操作数据与5000万视觉语言样本，验证了有效模态的协同训练可恢复并增强视觉语言骨干的理解与推理能力。",
      "detailedSummary": "## 研究背景与动机\n当前，通过在大规模多任务机器人数据上进行训练，大型行为模型（LBMs）已展现出灵巧的操作能力。然而，机器人数据的稀缺性严重限制了这些模型的泛化能力。为了在不增加昂贵数据收集成本的前提下扩展数据覆盖范围，协同训练（co-training）——即联合学习目标机器人数据与异构数据模态——成为了一种有前景的解决方案。现有研究通常只评估了部分数据模态，且实验设置不一致，导致对于不同协同训练数据模态和策略如何影响策略性能，缺乏系统性的理解。\n\n本文针对上述痛点，对五种协同训练数据模态（标准视觉-语言数据、机器人轨迹的密集语言标注、跨具身机器人数据、人类视频、离散机器人动作标记）以及单阶段和多阶段训练策略进行了大规模实证研究。核心思路是：通过大规模、控制变量实验，系统性地评估不同数据模态和训练策略对视觉-语言-动作（VLA）策略在分布偏移、未见任务和语言跟随等维度上性能的影响，为构建可扩展的通用机器人策略提供实证指导。\n\n## 方法详解\n本研究的目标是学习一个能够利用多样化协同训练数据模态的策略πθ。该策略以一系列图像I1:n和文本提示ℓ作为输入。对于连续机器人动作，模型使用流匹配（Flow Matching, FM）作为学习目标。对于文本标记或离散动作标记，则使用交叉熵（CE）损失进行优化。当联合优化连续和离散模态时，损失函数为两者的加权和。\n\n![方法框架](https://arxiv.org/html/2602.01067v1/x1.png)\n\n> **图1**：数据、模型架构和评估设置概览。策略基于预训练的视觉-语言模型（VLM）骨干网络与动作流变换器（Action Flow Transformer）构建。它在目标机器人数据以及异构协同训练模态上训练，并在模拟和现实世界中进行评估。\n\n**模型架构**：采用VLA架构，由预训练的VLM骨干网络（基于PaliGemma2-PT初始化）和动作头（ActionFT）组成。VLM编码观测图像和任务语言提示，并可选择性地生成文本或离散动作标记。为了获得用于动作生成的紧凑表示，在骨干网络的词汇表中引入了一个特殊的观测编码标记，并将其附加到文本提示末尾。从VLM最后四层提取该标记对应的隐藏状态向量，形成一个全局条件嵌入，并馈入ActionFT。ActionFT采用扩散变换器设计，包含8个流变换器层，每层通过自适应层归一化（adaLN）MLP以观测特征和流匹配时间步为条件。它接收全局条件嵌入、带噪声的连续动作块和流匹配时间变量，并预测引导动作迭代去噪的流向量。与主流方法不同，本方法仅使用单个标记作为视觉-语言表示，而非来自所有VLM层的注意力键和值。\n\n**协同训练策略**：研究了三种将协同训练数据纳入不同训练阶段（即具有不同数据组成的训练轮次）的策略：\n1.  **单阶段协同训练**：在单一阶段联合训练目标机器人数据和协同训练数据。\n2.  **两阶段仅第一阶段协同训练**：第一阶段仅在协同训练模态上训练，第二阶段在目标机器人连续动作上训练。\n3.  **两阶段完全协同训练**：第一阶段与策略2相同，但第二阶段在协同训练数据和目标机器人数据上联合训练。\n\n此外，还研究了显式的链式思维（CoT）条件化动作生成范式。在推理时，VLM骨干首先生成从协同训练数据中学到的CoT轨迹，然后将观测标记附加到其末尾，提取由此产生的视觉-语言嵌入（编码图像、任务提示和CoT轨迹）来为ActionFT生成连续动作提供条件。在训练中引入概率性CoT条件化。\n\n![训练数据概览](https://arxiv.org/html/2602.01067v1/x2.png)\n\n> **图2**：训练数据概览。数据集包括目标机器人数据以及五种异构协同训练数据模态，总计约4000小时操作数据和5000万视觉-语言样本。\n\n**数据整理**：研究整理了包含目标机器人专家示范数据和五种不同协同训练模态的综合数据集。\n1.  **目标机器人数据**：采用TRI-Ramen数据集，包含523小时操作数据，涵盖403个任务和53,411条演示。\n2.  **标准视觉-语言数据**：包含RoboPoint（130万样本）和RefSpatial（250万样本）数据集，用于增强模型的多模态理解。\n3.  **机器人轨迹的密集语言标注**：通过两种策略为TRI-Ramen轨迹生成每步的文本描述：基于启发式规则的脚本标注和基于VLM（GPT-5）的标注。\n4.  **跨具身机器人数据**：采用OXE-Ramen数据集，是Open X-Embodiment数据集的子集，包含1,150小时数据，涵盖12种机器人设置和924个任务。\n5.  **人类视频**：探索两种利用人类视频的方法：一是使用潜在动作模型（LAM）从视频中提取离散的潜在动作标记；二是使用VLM（GPT-5）为视频帧生成语言描述。\n6.  **离散机器人动作标记**：探索两种形式：使用FAST将连续动作块转换为近乎无损的离散标记序列；使用VQ-VAE将动作块压缩为更紧凑的离散标记。\n\n## 实验与结果\n研究通过大规模实验评估了89个VLA策略，共进行了58,000次模拟推演和2,835次现实世界推演。评估维度包括：分布内性能、对分布偏移（DS）的鲁棒性、对未见任务的泛化以及语言跟随能力。\n\n![评估设置](https://arxiv.org/html/2602.01067v1/x3.png)\n\n> **图3**：模拟和现实世界评估设置。策略在模拟中于名义条件和分布偏移条件下，在13个已见和8个未见任务上评估。现实世界评估包括语言跟随实验和对未见长视野灵巧任务的微调适应。\n\n**模拟基准结果**：图4至图13展示了不同数据模态和训练策略在模拟基准上的性能。\n\n![单阶段协同训练结果](https://arxiv.org/html/2602.01067v1/x4.png)\n\n> **图4**：单阶段协同训练策略下，不同数据模态在已见任务（名义条件）、已见任务（分布偏移）、未见任务（名义条件）和未见任务（分布偏移）上的平均成功率。结果显示，标准VL数据、密集语言标注和跨具身数据显著提升了在分布偏移和未见任务上的性能，而离散动作标记（FAST, VQ-VAE）和人类视频的潜在动作则没有带来统计显著的收益。\n\n![两阶段协同训练结果](https://arxiv.org/html/2602.01067v1/x5.png)\n\n> **图5**：两阶段协同训练策略（第一阶段仅用协同训练数据）的结果。趋势与单阶段类似，标准VL数据、密集语言标注和跨具身数据带来显著提升。\n\n![两阶段完全协同训练结果](https://arxiv.org/html/2602.01067v1/x6.png)\n\n> **图6**：两阶段完全协同训练策略（两阶段都使用协同训练数据）的结果。同样，有效的模态带来了性能提升。\n\n**关键发现**：\n1.  **有效的数据模态**：协同训练各种形式的视觉-语言数据（标准VL数据、密集语言标注）和跨具身机器人数据，能持续提升策略对分布偏移、未见任务和语言跟随的泛化能力。\n2.  **无效的数据模态**：离散动作标记变体（FAST, VQ-VAE）以及从人类视频中提取的潜在动作标记，未带来统计显著的性能收益。\n3.  **组合效应**：结合有效的模态（例如，VL + 密集标注 + 跨具身数据）能产生累积性能增益（图7, 8, 9）。\n\n![模态组合的累积增益](https://arxiv.org/html/2602.01067v1/x7.png)\n\n> **图7**：在单阶段协同训练中，结合有效的协同训练模态（VL+DA+XE）能带来累积性能增益，在所有评估维度上均优于单一模态。\n\n**现实世界评估结果**：图10至图13展示了在语言跟随任务上的结果。\n\n![现实世界语言跟随结果](https://arxiv.org/html/2602.01067v1/x10.png)\n\n> **图10**：在现实世界语言跟随任务（已见物体）上，不同协同训练模态和策略的平均任务完成百分比。有效的模态同样提升了语言指令的跟随能力。\n\n**微调适应能力**：研究评估了预训练模型通过微调快速适应未见长视野灵巧任务的能力。图14-16显示，使用有效模态协同训练的策略，在微调后能更快地达到更高的性能。\n\n![微调适应结果](https://arxiv.org/html/2602.01067v1/x14.png)\n\n> **图14**：在未见长视野灵巧任务上微调适应后的性能。使用有效模态（VL+DA+XE）协同训练的预训练模型，微调后性能显著优于仅在机器人数据上训练的基线。\n\n**VLM骨干能力评估**：研究通过标准视觉-语言基准测试评估了协同训练如何影响VLM骨干的视觉-语言理解能力。图17-19显示，仅在机器人数据上训练会损害VLM骨干的原有能力，而与有效模态协同训练则能恢复甚至提升这些能力。\n\n![VLM骨干基准测试结果](https://arxiv.org/html/2602.01067v1/x17.png)\n\n> **图17**：在不同视觉-语言基准测试上的表现。仅在机器人数据上训练（蓝色）导致性能下降，而加入有效的协同训练模态（绿色）能恢复性能。\n\n**链式思维（CoT）条件化**：图20-22显示，在模拟基准中，显式地使用从协同训练数据中学到的CoT轨迹来条件化动作生成，并未带来性能提升。\n\n![CoT条件化结果](https://arxiv.org/html/2602.01067v1/x20.png)\n\n> **图20**：在单阶段协同训练中，CoT条件化（CoT-DA, CoT-LA）并未带来性能提升。\n\n## 总结与启发\n本文的核心贡献在于：\n1.  **系统性实证研究**：首次大规模、控制变量地系统评估了五种主流协同训练数据模态和三种训练策略对VLA策略性能的影响，填补了该领域的知识空白。\n2.  **明确的效能排序**：明确了标准视觉-语言数据、密集语言标注和跨具身机器人数据能有效提升泛化能力；而离散动作标记和人类视频的潜在动作在当前设置下无效。\n3.  **揭示骨干网络影响**：揭示了仅使用机器人数据训练会损害VLM骨干的视觉-语言理解能力，而有效的协同训练能保护这种能力，这对模型设计具有重要启示。\n\n论文自身提到的局限性包括：显式的链式思维（CoT）条件化在当前的模拟基准中并未显示出性能优势，其潜在价值可能需要更复杂的任务或不同的评估方式来验证。\n\n对后续研究的启示：\n*   **数据模态选择**：为构建通用机器人策略提供了清晰的数据模态优先级指导，建议优先整合丰富的视觉-语言语义信息和多样化的机器人示范数据。\n*   **模型架构设计**：研究采用的紧凑表示（单个观测标记）被证明有利于泛化，这为简化VLA模型架构提供了依据。\n*   **评估体系**：论文建立的综合评估框架（涵盖模拟、现实世界、分布偏移、未见任务、语言跟随和微调适应）可作为未来协同训练研究的基准。\n*   **探索方向**：尽管显式CoT在本研究中未显效，但如何更有效地利用语言或中间表示进行推理和规划，仍是值得探索的方向。同时，无效模态（如离散动作）为何无效，其背后的原因也需进一步研究。",
      "imageUrls": [
        "https://arxiv.org/html/2602.01067v1/x1.png",
        "https://arxiv.org/html/2602.01067v1/x2.png",
        "https://arxiv.org/html/2602.01067v1/x3.png",
        "https://arxiv.org/html/2602.01067v1/x4.png",
        "https://arxiv.org/html/2602.01067v1/x5.png",
        "https://arxiv.org/html/2602.01067v1/x6.png",
        "https://arxiv.org/html/2602.01067v1/x7.png",
        "https://arxiv.org/html/2602.01067v1/x8.png",
        "https://arxiv.org/html/2602.01067v1/x9.png",
        "https://arxiv.org/html/2602.01067v1/x10.png",
        "https://arxiv.org/html/2602.01067v1/x11.png",
        "https://arxiv.org/html/2602.01067v1/x12.png",
        "https://arxiv.org/html/2602.01067v1/x13.png",
        "https://arxiv.org/html/2602.01067v1/x14.png",
        "https://arxiv.org/html/2602.01067v1/x15.png",
        "https://arxiv.org/html/2602.01067v1/x16.png",
        "https://arxiv.org/html/2602.01067v1/x17.png",
        "https://arxiv.org/html/2602.01067v1/x18.png",
        "https://arxiv.org/html/2602.01067v1/x19.png",
        "https://arxiv.org/html/2602.01067v1/x20.png",
        "https://arxiv.org/html/2602.01067v1/x21.png",
        "https://arxiv.org/html/2602.01067v1/x22.png",
        "https://arxiv.org/html/2602.01067v1/x23.png",
        "https://arxiv.org/html/2602.01067v1/x24.png",
        "https://arxiv.org/html/2602.01067v1/x25.png",
        "https://arxiv.org/html/2602.01067v1/x26.png",
        "https://arxiv.org/html/2602.01067v1/x27.png",
        "https://arxiv.org/html/2602.01067v1/x28.png",
        "https://arxiv.org/html/2602.01067v1/x29.png",
        "https://arxiv.org/html/2602.01067v1/x30.png",
        "https://arxiv.org/html/2602.01067v1/x31.png",
        "https://arxiv.org/html/2602.01067v1/x32.png",
        "https://arxiv.org/html/2602.01067v1/x33.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.23087",
      "title": "Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.23087",
      "arxivId": "2601.23087",
      "date": "2026-01-30",
      "authors": "Liu Hong Team",
      "category": "Manipulation",
      "summary": "本文针对长时程机器人操作中，现有生成策略难以兼顾表达性行为建模、快速推理和稳定执行的核心问题，提出CoLA-Flow Policy框架。其关键技术是通过连续潜在动作空间的流匹配，将动作序列编码为时间一致的潜在轨迹，并学习显式潜在流，以解耦全局运动结构与低级控制噪声。实验表明，该方法实现近单步推理，相比原始动作空间流基线，轨迹平滑度提升达93.7%，任务成功率提升达25%，且推理速度显著优于扩散策略。",
      "detailedSummary": "## 研究背景与动机\n当前机器人视觉运动模仿学习的主流生成式策略主要分为两类：扩散策略和流匹配策略。扩散策略具有强大的多模态行为建模能力，但其依赖迭代去噪导致推理延迟高，难以实时部署。流匹配策略通过求解常微分方程（ODE）实现近乎一步的快速推理，但直接在原始高维动作空间进行流匹配时，建模噪声会在长时程轨迹生成中被放大，导致执行轨迹抖动、不稳定。因此，现有方法难以在表达性建模、快速推理和稳定执行这三者间取得平衡。\n\n本文针对生成式策略在推理效率与执行稳定性之间的根本性权衡这一痛点，提出了在连续潜在动作空间进行流匹配的新视角。其核心思路是：将动作序列编码为时间连贯的潜在轨迹，并在该平滑的潜在空间中进行流匹配，从而将全局运动结构与低级控制噪声解耦，实现快速、平滑且稳定的长时程操作。\n\n## 方法详解\nCoLA-Flow Policy 是一个模块化的轨迹级模仿学习框架，其整体流程分为三步：1) 学习时间连贯的潜在动作表示；2) 在几何感知的场景条件下，于潜在空间中生成长时程潜在轨迹；3) 通过多模态条件解码，将潜在轨迹转换为可执行的控制命令。\n\n![方法整体框架](https://arxiv.org/html/2601.23087v2/x1.png)\n> **图1**：CoLA-Flow Policy 的整体架构。系统首先将点云观测编码为几何感知的场景特征，然后在潜在空间进行流匹配以生成时间连贯的潜在动作轨迹，最后在视觉条件调制下解码为可执行的控制命令。\n\n**1. 连贯的潜在动作表示**\n该模块旨在构建一个平滑、时间连贯的潜在动作空间，作为流匹配生成的基础。其核心是使用一个循环编码器将短时程的动作块序列编码为连续轨迹。\n- **时间连续潜在动作建模**：将长度为 H 的动作轨迹分割为 K 个连续块。每个块经过轻量级时序卷积嵌入后，送入一个基于GRU的循环编码器。GRU通过聚合历史动作块信息，为每个潜在代码引入强烈的时间归纳偏置，抑制高频变化，从而形成连续的潜在轨迹流形，这与非循环的逐块编码器有本质区别。\n- **变分正则化**：在训练编码器-解码器时，引入KL散度项，使潜在后验分布接近标准正态先验，以鼓励潜在空间的紧致性和连续性，提升对噪声演示的鲁棒性。\n- **视觉条件动作解码**：解码器采用特征线性调制（FiLM）层来注入执行时的多模态感官信息（如腕部相机图像）。这种设计将潜在轨迹规划与执行时适应清晰分离，编码的视觉特征通过FiLM层调制解码过程，实现 `A^(k) = f_θ(z_k | v_t)`。\n\n![潜在动作表示](https://arxiv.org/html/2601.23087v2/x2.png)\n> **图2**：轨迹级潜在动作表示框架。基于GRU的编码器引入时间归纳偏置以生成连贯的潜在轨迹；MLP解码器通过FiLM层接收腕部相机特征调制，实现视觉自适应的动作执行。\n\n**2. 基于流匹配的潜在空间动作生成**\n在获得平滑的潜在空间后，使用流匹配在该空间内生成长时程行为。\n- **潜在空间流动力学**：学习一个时间相关的向量场 `ν_θ(t, z)`，定义从简单基分布到目标潜在分布的连续概率路径，由ODE `dξ_z(t)/dt = ν_θ(t, ξ_z(t))` 描述。在运动级潜在表示上操作，显著提高了数值稳定性。\n- **潜在空间的一致性流匹配（CFM）**：采用一致性流匹配，定义流函数 `f_θ(t, z) = z + (1-t)ν_θ(t, z)`，其中因子 `(1-t)` 在接近目标分布时衰减更新幅度，提高稳定性。此外，应用时间相关的输入归一化 `c_in(t) = 1/√(t^2+(1-t)^2)` 来平衡不同噪声水平下的潜在状态尺度。\n- **一步潜在轨迹生成**：在推理时，从基分布采样潜在代码，并通过单次评估学习到的流进行变换：`ẑ_k = f_θ(1, z̃_k)`。这实现了整个潜在动作轨迹的一步生成，无需迭代去噪。\n\n**3. 几何感知的3D场景条件**\n为了确保生成的潜在轨迹在几何上合理，使用点云编码器提取场景几何特征，并条件化流匹配过程。\n- **点云编码**：从全局深度图重建点云，通过最远点采样（FPS）选取锚点，并构建局部邻域。采用一个双分支编码器，包括一个通过残差卷积和最大-平均池化提取局部几何特征的局部编码器 `f_l`，以及一个通过轻量级MLP提取全局场景上下文的中心编码器 `f_c`。\n- **分层几何条件调制**：提取的几何特征通过两阶段FiLM机制注入潜在流网络。首先应用局部几何调制 `h' = γ_l ⊙ h + β_l`，然后进行中心级条件调制 `h'' = γ_c ⊙ h' + β_c`。这种先局部后全局的调制顺序更有效。\n\n![点云编码器](https://arxiv.org/html/2601.23087v2/x3.png)\n> **图3**：几何感知点云编码器。围绕最远采样中心的局部邻域通过残差卷积和最大-平均池化捕获平移不变的局部几何；轻量级中心编码器提供紧凑的全局场景上下文，用于潜在轨迹条件化。\n\n**创新点总结**：1) **轨迹级潜在建模**：使用循环编码器构建时间连贯的潜在动作空间；2) **潜在空间流匹配**：在平滑潜在空间而非原始动作空间进行流匹配，从根本上改善稳定性；3) **解耦设计**：将长时程轨迹规划（潜在空间生成）与执行时低层适应（多模态条件解码）分离。\n\n## 实验与结果\n**实验设置**：\n- **仿真基准**：在Adroit（灵巧手操作）和Meta-World（机械臂操作）两个包含37个任务的广泛使用的基准上进行评估，使用MuJoCo物理模拟器。\n- **真实世界平台**：使用Franka Emika Panda机械臂，配备LEAP Hand或平行夹爪。传感系统包括全局RealSense L515相机和腕部RealSense D435相机。\n- **对比基线**：包括基于扩散的DP3、iDP3、Reactive Diffusion Policy (RDP) 和原始Diffusion Policy；以及基于流的Flow Policy。\n- **评估指标**：任务成功率、响应时间（推理延迟）和轨迹平滑度。平滑度定义为加权的离散加加速度 (`S_jerk`) 和高频谱能量比 (`S_freq`) 的组合，值越低越平滑。\n\n![轨迹平滑度对比](https://arxiv.org/html/2601.23087v2/x4.png)\n> **图4**：仿真任务中的轨迹平滑度对比。CoLA-Flow Policy (Ours) 的平滑度得分最低（0.052），表明其执行轨迹最平滑，相比基线有显著提升。\n\n**关键实验结果**：\n1.  **轨迹平滑度**：CoLA-Flow Policy 取得了最低的平均平滑度得分 0.052，相对于 RDP、DP3 和 Flow Policy 分别降低了 51.4%、69.4% 和 77.2%（即平滑度提升高达93.7%）。这验证了在时间连贯潜在空间进行流匹配对稳定性的巨大改善。\n2.  **任务成功率与推理时间**：如表I所示，CoLA-Flow Policy 在仿真中取得了最高的平均成功率 78.3%，优于 Flow Policy (+17.3%)、DP3 (+12.6%) 和 RDP (+5.3%)。同时，其平均响应时间仅为 7.5 ms，虽然略高于 Flow Policy (6.1 ms)，但比 DP3 (55.9 ms) 快约7.5倍，实现了成功率和延迟之间的有利权衡。\n3.  **真实世界实验**：在拾放、插孔和避障抓取三个任务中，CoLA-Flow Policy 均取得了最高的成功率（例如，在灵巧手拾放中达到 86.7% ± 5.8%），并且轨迹抖动显著减少，展示了其在实际部署中的鲁棒性。\n\n![真实实验设置](https://arxiv.org/html/2601.23087v2/x5.png)\n> **图5**：真实世界实验设置和观测。左：配备LEAP Hand和视觉传感系统（全局L515和腕部D435）的机器人。右：实验中使用的多模态观测示例和任务物体。\n\n**消融实验分析**：论文通过消融实验验证了各组件贡献。关键发现包括：1) **循环编码器至关重要**：移除GRU编码器（使用逐块编码）会导致平滑度下降约38.1%，成功率下降约9.5%；2) **潜在空间流匹配是核心**：在原始动作空间进行流匹配（类似Flow Policy）会导致性能显著恶化；3) **分层几何条件有效**：先局部后全局的FiLM调制顺序优于相反顺序。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个**轨迹级潜在流匹配框架**，首次在机器人模仿学习中协调了快速推理与平滑稳定的长时程操作。\n2.  证明了在**连续且时间连贯的潜在动作空间**进行流匹配，能显著减少轨迹抖动，提高执行可靠性，其平滑度提升最高达93.7%。\n3.  通过综合仿真与真实实验，验证了该方法在多样化任务上的鲁棒性能，包括对异构和次优演示的更好容忍度。\n\n**局限性**：论文自身提到，该方法依赖于从潜在动作表示中学习到的平滑先验，其性能上限受到演示数据质量和潜在空间表达能力的制约。此外，虽然支持多模态条件，但本文主要评估了视觉模态。\n\n**对后续研究的启示**：\n1.  **潜在表示的设计是关键**：如何设计既能捕捉复杂行为模式又保持时间平滑性与几何一致性的潜在空间，是一个富有前景的方向。\n2.  **规划与适应的解耦**：将高层轨迹规划（在抽象空间）与低层执行适应（通过条件解码）分离的范式，有助于构建更模块化、更鲁棒的机器人策略。\n3.  **评估指标的综合性**：除了成功率，将轨迹平滑度、能量效率等执行质量指标纳入评估体系，对于推动适用于物理部署的算法发展至关重要。",
      "imageUrls": [
        "https://arxiv.org/html/2601.23087v2/x1.png",
        "https://arxiv.org/html/2601.23087v2/x2.png",
        "https://arxiv.org/html/2601.23087v2/x3.png",
        "https://arxiv.org/html/2601.23087v2/x4.png",
        "https://arxiv.org/html/2601.23087v2/x5.png",
        "https://arxiv.org/html/2601.23087v2/x6.png",
        "https://arxiv.org/html/2601.23087v2/x7.png",
        "https://arxiv.org/html/2601.23087v2/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.22988",
      "title": "Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.22988",
      "arxivId": "2601.22988",
      "date": "2026-01-30",
      "authors": "Guang Chen Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中单视图3D几何理解不足、视角泛化能力弱的核心问题，提出了GEM3D框架。其关键技术在于采用单视图3D预训练范式，通过点云重建和前馈高斯溅射学习整体几何表示，并在策略学习阶段通过多步蒸馏保留几何知识。实验表明，该方法在12个RLBench任务上的平均成功率超越先前最优方法12.7%，且在视角大幅变化时，成功率下降幅度显著更小，展现了卓越的零样本视角泛化能力。",
      "detailedSummary": "## 研究背景与动机\n当前机器人操作领域的视觉运动策略主要依赖于2D视觉编码器，这类特征缺乏显式的3D结构感知，在空间复杂的任务中效果有限。近期，利用多视角输入学习3D感知表示的方法（如NeRF、高斯泼溅）展现出潜力，但仍存在关键局限性：推理时依赖多视角观测不实用；场景建模不完整，无法捕捉精细几何结构；缺乏有效的策略训练策略来保留和利用习得的3D知识。本文针对这些痛点，提出了一个统一的表示-策略学习框架GEM3D，旨在从单视角输入学习几何接地的3D视觉表示，并实现强视角泛化的操作。其核心思路是通过多视角监督下的点云重建和前馈高斯泼溅进行单视角3D预训练，学习整体几何表示，再通过多步蒸馏将习得的几何理解迁移到操作策略中。\n\n## 方法详解\nGEM3D框架包含两个关键组件：GEM3D预训练和GEM3D策略。前者学习整体的3D表示，后者将预训练的3D视觉表示蒸馏到视觉运动策略中。\n\n![方法总览](https://arxiv.org/html/2601.22988v1/x2.png)\n> **图2**：GEM3D整体框架概览。左侧为GEM3D预训练，通过辅助场景重建任务学习整体3D表示；右侧为GEM3D策略，将预训练的3D视觉表示蒸馏到视觉运动策略中，用于视角泛化的操作。\n\n**GEM3D预训练** 采用三阶段流水线，从单视角RGB-D观测学习密集体素特征、整体几何和精细纹理。\n\n![预训练流水线](https://arxiv.org/html/2601.22988v1/x3.png)\n> **图3**：GEM3D预训练流水线。(a) 将单视角RGB-D观测编码为体素特征；(b) 以从粗到精的Snowflake方式逐步重建场景几何；(c) 通过基于高斯泼溅的新视角渲染学习精细纹理细节。\n\n1.  **3D特征提取**：输入为单视角RGB-D观测 $o^t = \\{ \\mathcal{I}^t, \\mathcal{D}^t, \\mathcal{K}^t \\}$。深度图 $\\mathcal{D}^t$ 通过相机参数 $\\mathcal{K}^t$ 反投影、裁剪和最远点采样得到点云 $\\mathcal{P}$。RGB图像 $\\mathcal{I}^t$ 通过预训练的DINOv2模型编码为像素级2D特征 $\\mathcal{F}_{\\text{2D}}$，并投影到 $\\mathcal{P}$ 上以增强视觉语义。最后，语义增强的点云被体素化为占据和特征体，并通过3D U-Net融合，生成密集体素特征 $\\mathcal{F} \\in \\mathbb{R}^{D^3 \\times 128}$。\n\n2.  **从粗到精的点云重建**：为了实现对体素特征 $\\mathcal{F}$ 的整体几何理解，本模块首先生成稀疏种子点，然后逐步细化为密集点云。\n    *   **种子点生成**：初始化一个可学习的体素查询集 $\\mathcal{Q} \\in \\mathbb{R}^{d^3 \\times 128}$（$d \\ll D$）。首先通过平均池化下采样 $\\mathcal{F}$ 得到 $\\mathcal{F}_{\\text{down}}$，执行粗粒度交叉注意力得到捕获粗略空间关系的提议查询 $\\mathcal{Q}_p$。然后通过一个3D可变形交叉注意力模块，使 $\\mathcal{Q}_p$ 从 $\\mathcal{F}$ 中高效聚合局部几何细节。最终，种子令牌通过一个浅层MLP解码为种子点坐标 $\\hat{\\mathcal{P}}_0 \\in \\mathbb{R}^{d^3 \\times 3}$。\n    *   **点云恢复**：遵循SnowflakeNet，使用雪花点反卷积块从种子点 $\\hat{\\mathcal{P}}_0$ 开始逐步细化点集。在每一阶段 $i$，父点 $\\hat{\\mathcal{P}}_i$ 通过三线性插值查询 $\\mathcal{F}$ 得到点特征 $\\mathcal{F}_i$，SPD块以 $\\hat{\\mathcal{P}}_i$、$\\mathcal{F}_i$ 和当前上下文特征 $\\mathcal{F}^c_i$ 为输入，对每个父点上采样因子 $r$，产生 $r$ 个位移向量 $\\Delta\\mathcal{P}_i$ 并更新上下文特征。父点复制 $r$ 份并根据预测偏移位移，形成细化后的子点 $\\hat{\\mathcal{P}}_{i+1}$。使用融合多视角真实点云的完整点集 $\\mathcal{P}_{\\text{full}}$ 进行监督，在每一阶段使用最远点采样获取子集 $\\mathcal{P}_i$，并使用倒角距离 $L_2$ 作为重建损失 $\\mathcal{L}_{\\text{rec}}$（公式1）。\n\n3.  **前馈3D高斯泼溅**：基于重建的密集点云 $\\hat{\\mathcal{P}}$，通过前馈高斯泼溅流水线进行新视角渲染，在多视角监督下捕获精细纹理。高斯中心 $\\mu$ 直接由细化点 $\\hat{\\mathcal{P}}$ 给出。在这些位置，通过三线性插值从密集体素场 $\\mathcal{F}$ 中采样高斯特征 $\\mathcal{F}_{\\mathcal{G}}$，然后输入一个ResNetFC网络回归其余的非位置高斯参数（颜色、不透明度、旋转、尺度）。得到的3D高斯参数 $\\mathcal{G} = [\\mu, c, \\sigma, r, s]$ 用于可微分渲染器合成新视角图像 $\\hat{\\mathcal{I}}^t_v$，并使用焦点损失 $\\mathcal{L}_{\\text{rend}}$（公式3）与对应的多视角RGB观测 $\\mathcal{I}^t_v$ 进行监督。\n\n**GEM3D策略学习** 采用基于多步蒸馏的策略，而非直接微调预训练编码器，以避免破坏已学习的几何表示。\n\n![策略学习框架](https://arxiv.org/html/2601.22988v1/x4.png)\n> **图4**：GEM3D策略。一个基于多步蒸馏的策略学习框架。策略编码器处理观测得到潜在令牌 $\\mathbf{x}^t$，冻结的预训练3D特征提取器生成对应的密集体素特征 $\\mathcal{F}^t$ 并分块化为参考令牌 $\\tilde{\\mathbf{x}}^t$，通过基于余弦相似性的蒸馏损失对齐两者。\n\n具体而言，初始化一个独立的策略编码器处理单视角RGB-D观测 $o^t$ 为潜在令牌 $\\mathbf{x}^t$。同时，冻结的预训练3D特征提取器生成对应的密集体素特征 $\\mathcal{F}^t$，随后分块化为参考令牌 $\\tilde{\\mathbf{x}}^t$。应用基于余弦相似性的蒸馏损失来对齐潜在令牌和参考令牌。为了赋予策略动态理解，引入了一个隐式潜在动力学模型，从当前潜在状态 $\\mathbf{x}^t$ 以及本体感知和语言嵌入中预测 $\\mathbf{x}^{t+1}$。动作 $a_t$ 然后通过一个动作头从 $\\mathbf{x}^{t+1}$ 解码出来。这种设计使得可以进行多步潜在蒸馏以正则化策略的时间一致性，蒸馏损失 $\\mathcal{L}_{\\text{distill}} = \\mathcal{L}_{\\text{cos}}(\\mathbf{x}^{t}, \\tilde{\\mathbf{x}}^{t}) + \\mathcal{L}_{\\text{cos}}(\\mathbf{x}^{t+1}, \\tilde{\\mathbf{x}}^{t+1})$（公式4）。策略通过模仿学习进行训练，总体目标为 $\\mathcal{L}_{\\text{policy}} = \\|\\mathbf{a}_t - \\mathbf{a}_t^*\\|_2^2 + \\lambda_{\\text{distill}} \\mathcal{L}_{\\text{distill}}$（公式5）。\n\n## 实验与结果\n**实验设置**：在RLBench的12个任务（覆盖9个场景）上进行评估。预训练使用8个均匀分布环绕摄像头采集的多视角观测，每个场景50条轨迹。策略学习每个任务使用20个从前置固定摄像头收集的专家演示。评估指标为任务成功率（SR）。为评估视角泛化，在三种摄像头视角偏移下进行零样本评估：无偏移（训练视角）、中等偏移（机器人基座周围30°内）、大偏移（60°至90°间的显著变化）。\n\n**对比方法**：使用PerceiverIO作为策略主干进行公平比较，基线包括：PerAct（直接使用基于体素的3D表示）、GNFactor（结合NeRF作为辅助表示学习目标）、ManiGaussian（利用3D高斯泼溅进行表示学习并与策略联合训练）。此外，将ManiGaussian表示模块与本文的蒸馏训练策略结合，记为ManiGaussian (w Distill)。\n\n**关键实验结果**：\n-   在12个RLBench任务上，GEM3D取得了最佳整体性能，平均成功率为44.2%，比之前的SOTA方法ManiGaussian绝对提升了12.7%（表1）。\n-   在六个代表性任务的零样本视角泛化评估中（表2），GEM3D在中等和大视角偏移下的成功率仅分别下降22.03%和29.67%，显著低于ManiGaussian的41.62%和51.52%的下降。在大视角偏移下，GEM3D的成功率最高可达PerAct的6倍。\n-   多任务评估（表3）显示，GEM3D在训练视角和偏移视角下性能下降幅度很小（最大下降6.9%），而PerAct基线则出现显著下降甚至策略崩溃，表明本文的几何接地表示有助于更清晰的任务区分和更可靠的指令遵循。\n\n**定性分析与重建质量**：\n![定性结果](https://arxiv.org/html/2601.22988v1/x1.png)\n> **图1**：GEM3D学习精细的3D表示，能够实现更准确的新视角渲染和点云重建，并在推理过程中对视角偏移表现出强大的鲁棒性，保持稳定的视觉运动性能。\n\n如图1所示，GEM3D重建了更清晰的结构并产生更一致的新视角，而ManiGaussian则存在纹理模糊和几何畸变。定量指标（表4）进一步证实，GEM3D预训练在PSNR、SSIM和倒角距离L2上均显著优于ManiGaussian的预训练方法，平均分别提升+7.08 dB、+0.4298和降低0.0289。\n\n**消融实验**：\n![消融实验](https://arxiv.org/html/2601.22988v1/x6.png)\n> **图6**：GEM3D预训练模块的消融研究。移除可变形交叉注意力（DCA）会导致精细空间线索丢失；移除雪花式（Snowflake）重建则无法逐步细化几何细节。\n\n消融研究验证了GEM3D预训练中三个关键设计的必要性（图6）：可变形交叉注意力确保高效精确的种子生成；雪花式从粗到精重建实现几何细节的逐步细化；焦点损失增强了对运动引起的表观和几何变化的鲁棒性。移除任一组件都会导致重建质量下降。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了GEM3D，一个统一的表示-策略学习框架，通过单视角3D预训练学习整体且精细的几何接地表示；2）设计了一种基于多步蒸馏的策略学习策略，能够有效保留预训练获得的几何知识并迁移到操作技能中，增强了视角泛化能力；3）在RLBench上实现了SOTA性能，并展示了卓越的零样本视角泛化鲁棒性。\n\n论文自身提到的局限性包括：预训练需要多视角数据，这可能在数据收集受限的场景中构成挑战；方法目前主要在模拟环境中进行评估。\n\n对后续研究的启示：本文证明了通过精心设计的预训练任务（结合几何重建与纹理渲染）学习到的3D表示，对于提升视觉运动策略的空间理解与泛化能力至关重要。所提出的蒸馏策略提供了一种有效整合预训练表示与策略学习的新范式，可扩展至其他骨干架构。未来工作可探索更高效的自监督预训练方式，以及将框架迁移到真实机器人复杂环境中的可行性。",
      "imageUrls": [
        "https://arxiv.org/html/2601.22988v1/x1.png",
        "https://arxiv.org/html/2601.22988v1/x2.png",
        "https://arxiv.org/html/2601.22988v1/x3.png",
        "https://arxiv.org/html/2601.22988v1/x4.png",
        "https://arxiv.org/html/2601.22988v1/x5.png",
        "https://arxiv.org/html/2601.22988v1/x6.png",
        "https://arxiv.org/html/2601.22988v1/x7.png",
        "https://arxiv.org/html/2601.22988v1/x8.png",
        "https://arxiv.org/html/2601.22988v1/x9.png",
        "https://arxiv.org/html/2601.22988v1/x10.png",
        "https://arxiv.org/html/2601.22988v1/x11.png",
        "https://arxiv.org/html/2601.22988v1/x12.png",
        "https://arxiv.org/html/2601.22988v1/x14.png",
        "https://arxiv.org/html/2601.22988v1/x15.png",
        "https://arxiv.org/html/2601.22988v1/x16.png",
        "https://arxiv.org/html/2601.22988v1/x17.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.22965",
      "title": "Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation",
      "url": "http://arxiv.org/abs/2601.22965",
      "arxivId": "2601.22965",
      "date": "2026-01-30",
      "authors": "Wuyue Zhao Team",
      "category": "Manipulation",
      "summary": "本文提出自模仿扩散策略（SIDP），以解决视觉导航中传统扩散策略因模仿学习而继承专家演示的次优性与冗余，导致推理时依赖计算密集的“生成-过滤”流程的问题。SIDP采用奖励引导的自模仿机制，使策略选择性地模仿自身采样的高质量轨迹，结合奖励驱动的课程学习与目标无关的轨迹增强，提升规划效率与鲁棒性。实验表明，SIDP在仿真与实物平台上均显著优于基线方法，在Jetson Orin Nano上推理速度达110ms，比基线NavDP（273ms）快2.5倍，实现了高效实时部署。",
      "detailedSummary": "## 研究背景与动机\n视觉导航中的路径规划主流方法之一是学习基于扩散的策略。这些方法通过迭代去噪建模完整轨迹，具有良好的时空一致性，并能捕获多模态轨迹分布以适应复杂环境。然而，当前大多数扩散策略依赖模仿学习进行训练，这带来了两个关键瓶颈：首先，专家数据集的有限覆盖会损害策略的鲁棒性，使其难以应对分布偏移和新场景；其次，标准模仿学习试图模仿所有给定的轨迹（包括大量次优轨迹），导致生成轨迹的质量参差不齐。因此，在推理时通常需要依赖“先生成-后过滤”的流程，即生成大量候选轨迹，再通过辅助选择器进行筛选，这显著增加了计算延迟，尤其在资源受限的设备上。\n\n本文针对传统模仿学习扩散策略存在的次优性、冗余性以及由此导致的低效推理流程这一具体痛点，提出了自模仿的新视角。核心思路是：提出自模仿扩散策略，让策略通过选择性模仿从自身采样得到的高质量轨迹来学习改进规划，从而引导轨迹分布集中，实现无需后过滤的高效、鲁棒推理。\n\n## 方法详解\nSIDP框架的核心是奖励引导的自模仿机制。其整体流程（对应论文图2）分为三个阶段：1）在线采样：策略根据当前状态生成N个候选轨迹；2）过滤与加权：根据预定义的奖励函数评估这些轨迹，选取奖励最高的k个，并计算归一化的指数重要性权重；3）参数更新：使用这些加权后的高质量轨迹计算加权去噪损失，更新策略参数。这是一个迭代的、在线策略的优化过程。\n\n![方法框架](https://arxiv.org/html/2601.22965v1/x2.png)\n\n> **图2**：自模仿扩散策略概览。框架使用当前策略生成候选轨迹，并通过基于奖励的排序门进行过滤。高奖励样本随后被用于计算加权去噪损失，更新策略参数以迭代地对齐最优分布。\n\n**核心模块与技术细节**：\n1.  **奖励引导的自模仿机制**：这是SIDP的核心创新。其理论基础是将最大化期望奖励的问题转化为策略分布与最优分布之间的KL散度最小化问题。通过重要性采样，利用当前策略作为提议分布生成候选轨迹。最优分布被形式化为与当前策略成比例、并由奖励指数加权的形式。因此，分配给候选轨迹的重要性权重仅取决于其奖励值，具体公式为归一化的Softmax（奖励/温度τ）。最终，优化目标转化为最小化**奖励加权的去噪损失**，即SIDP损失函数，它鼓励策略更专注于模仿自身生成的高质量轨迹。\n\n2.  **奖励函数设计**：奖励是引导自模仿的关键，论文中设计了包含安全与效率的多组件奖励（如表I所示），包括：碰撞惩罚、步长成本（鼓励路径更短）、进度奖励（基于到目标地理距离的减少）以及精细停靠奖励（在接近目标时激活）。\n\n3.  **互补学习策略**：\n    *   **目标无关探索**：在训练中随机采样辅助目标点，并将输入状态中的目标点替换为一个特殊的“目标无关”嵌入，同时将重要性权重设为均匀。此策略旨在激活策略的探索能力，增加轨迹池的多样性，并对点目标导航任务起到正则化作用。\n    *   **奖励驱动的课程学习**：动态筛选具有高学习潜力的训练场景。判断标准基于当前策略在该场景下生成轨迹的**最大奖励**和**奖励范围**。只有最大奖励足够高（保证可行性）且奖励范围足够大（保证梯度信息量）的场景才会被用于当前训练，以此提高数据效用，促进稳定收敛。\n\n**与现有方法的创新点**：\n与依赖固定专家数据集的传统模仿学习扩散策略（如NavDP）相比，SIDP的核心创新在于用**在线、奖励加权的自模仿**替代了离线的专家模仿。这避免了专家数据的局限性，并通过迭代地从自身高质量经验中学习，使策略的轨迹分布逐渐集中，从而在推理时**无需**生成大量样本并进行后过滤，实现了端到端的规划。相较于需要通过时间反向传播来优化扩散模型的RL方法，SIDP将优化问题转化为更稳定的模仿学习目标，避免了BPTT的计算负担和数值不稳定性。\n\n## 实验与结果\n**实验设置**：\n*   **基准测试**：主要在**InternVLA-N1 S1** 基准上进行评估，这是一个使用高保真Isaac Sim模拟器的闭环顺序导航基准，包含60个多样场景（家庭、商业、简单/困难杂乱环境）。\n*   **对比方法**：包括基于学习的规划基线**iPlanner**和**ViPlanner**，以及作为主要基线的纯模仿学习扩散策略**NavDP**。\n*   **评估指标**：成功率（SR）、路径长度加权成功率（SPL）。消融实验中还使用了碰撞率（CR）、到目标距离（DTG）和探索面积（EA）。\n\n**关键实验结果**：\n1.  **导航性能对比**：如表II所示，SIDP在所有场景类型和难度级别上均取得了最高的平均成功率（mSR 79.11%）和平均SPL（mSPL 72.72%）。在最具挑战性的InternScenes场景（商业和家庭）中，SIDP相比NavDP在SR上分别有约9.94%和5.79%的显著提升，证明了其在复杂、未见过的非结构化环境中更强的泛化能力和鲁棒性。\n\n2.  **计算效率**：如表III所示，在Jetson Orin Nano边缘设备上，SIDP（使用5步DDIM采样）的推理延迟仅为110毫秒，相比NavDP（273毫秒）实现了**2.5倍的加速**，且成功率更高（0.674 vs 0.549）。这得益于分布集中后，无需进行多候选采样和过滤，并允许使用更高效的确定性采样器（DDIM）和更少的去噪步数。\n\n![学习曲线](https://arxiv.org/html/2601.22965v1/ablation_curve_multi.png)\n\n> **图3**：不同温度系数τ下SIDP的训练学习曲线（平滑后）。τ=1.0的指数加权策略在收敛速度和最终性能上均优于线性加权基线（Lin. Weig）。\n\n![温度消融](https://arxiv.org/html/2601.22965v1/temperature_ablation.png)\n\n> **图4**：温度系数τ的消融研究结果（在自定义基准上）。τ=1.0在成功率（SR）和碰撞率（CR）上取得了最佳平衡。τ过大（选择压力小）或过小（过于贪婪）都会导致性能下降。\n\n3.  **消融实验分析**：\n    *   **奖励引导自模仿的有效性**：图3和图4表明，基于指数的重要性加权（τ=1.0）显著优于线性加权基线，证明了其提供适当选择压力以区分轨迹质量的重要性。温度τ需要适中，以平衡探索与利用。\n    *   **目标无关探索的作用**：图5显示，启用该策略能显著增加**探索面积（EA）**，并降低**碰撞率（CR）**，说明其有效提升了策略的探索能力和安全性，起到了正则化作用。\n\n![目标无关探索消融](https://arxiv.org/html/2601.22965v1/nogoal_ablation.png)\n\n> **图5**：目标无关探索策略的消融结果。启用该策略（w/ NoGoal）显著提高了探索面积（EA）并降低了碰撞率（CR），验证了其提升探索多样性和安全性的作用。\n\n    *   **奖励驱动课程学习的作用**：图6显示，使用课程学习策略能加速训练初期的收敛，并带来最终性能的稳定提升。\n\n![课程学习消融](https://arxiv.org/html/2601.22965v1/curriculum_ablation.png)\n\n> **图6**：奖励驱动课程学习的消融结果。使用课程学习（w/ Curriculum）在训练早期收敛更快，且最终性能更优。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**自模仿扩散策略**框架，通过奖励引导策略从自身高质量经验中学习，增强了鲁棒性，并促使轨迹分布集中，从而摒弃了传统扩散规划器所需的密集采样和后过滤流程，简化了推理。\n2.  引入了**目标无关探索**和**奖励驱动课程学习**两种互补训练策略，分别用于增加轨迹多样性、正则化导航任务以及提高困难场景下的数据效用和训练稳定性。\n3.  在高质量仿真基准上实现了最先进的导航性能，并在边缘计算设备上实现了显著的推理加速（2.5倍），通过了多机器人平台的实物验证，证明了其在实际机器人应用中的有效性。\n\n**局限性**：论文自身提及的局限性主要在于训练过程仍需要在线采样和奖励评估，这可能带来一定的计算成本。此外，方法依赖于精心设计的奖励函数。\n\n**对后续研究的启示**：\nSIDP展示了自模仿学习在提升扩散策略性能和效率方面的潜力。其框架可以扩展到其他需要序列决策的机器人任务中。未来的工作可以探索更高效的自模仿数据收集机制、自适应温度调整策略，或者将自模仿与少量专家演示或其他离线数据结合，以进一步加速训练和提高性能上限。",
      "imageUrls": [
        "https://arxiv.org/html/2601.22965v1/x1.png",
        "https://arxiv.org/html/2601.22965v1/x2.png",
        "https://arxiv.org/html/2601.22965v1/ablation_curve_multi.png",
        "https://arxiv.org/html/2601.22965v1/temperature_ablation.png",
        "https://arxiv.org/html/2601.22965v1/nogoal_ablation.png",
        "https://arxiv.org/html/2601.22965v1/curriculum_ablation.png",
        "https://arxiv.org/html/2601.22965v1/x3.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.22206",
      "title": "Causal Imitation Learning Under Measurement Error and Distribution Shift",
      "url": "http://arxiv.org/abs/2601.22206",
      "arxivId": "2601.22206",
      "date": "2026-01-29",
      "authors": "AmirEmad Ghassami Team",
      "category": "Manipulation",
      "summary": "本文研究存在测量误差和分布偏移的离线模仿学习问题。核心挑战是：决策相关状态仅能通过噪声观测获得，且训练与部署环境存在分布差异，导致标准行为克隆方法产生系统性偏差。作者提出因果模仿学习框架CausIL，基于近端因果推断思想，将噪声观测视为代理变量，并给出无需奖励或专家交互的策略识别条件。针对连续状态空间，采用基于RKHS函数类的对抗学习进行参数估计。在PhysioNet/Computing in Cardiology Challenge 2019的半模拟纵向数据上验证，相比行为克隆基线，CausIL对分布偏移表现出更强的鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n模仿学习（IL）旨在从专家演示中学习决策策略，无需明确的奖励信号。行为克隆（BC）是最简单广泛的方法，它将模仿学习简化为监督学习，拟合一个从智能体观察到专家动作的映射。然而，在许多现实场景中，驱动动态和专家决策的真实状态并非直接观测，学习者只能接收到被误差破坏的测量值。当训练和部署之间存在分布偏移时，这种测量误差与分布偏移共存会引发问题。标准BC（无论是基于原始测量值还是忽略它们）都可能收敛到存在系统性偏差的策略，因为它学习到的状态-动作关联是虚假的、非因果稳定的，在分布变化下不可靠。\n\n本文针对测量误差与分布偏移共存的这一具体痛点，提出了一种新的视角：不应模仿观测条件概率 \\(p(a \\mid o)\\)，而应定义一个对潜在状态分布和测量过程变化都具有鲁棒性的最优因果模仿策略。该策略通过反事实干预（\\(do(S_t = s)\\)）来定义，旨在回答“如果将当前状态设置为\\(s_t\\)，专家的行动将如何变化？”这一因果问题。核心思路是：受近端因果推断启发，将噪声状态观测视为代理变量，在无需奖励或交互式专家查询的情况下，从演示数据中识别并估计这一因果最优策略。\n\n## 方法详解\n本文研究在离线模仿学习设定下，部分决策相关状态（\\(U_t\\)）是潜在的，且仅通过噪声测量（\\(W_t\\)）观测到，同时训练和部署间可能存在分布偏移。专家策略依赖于当前观测状态\\(S_t\\)和具有一步延迟的潜在状态\\(U_{t-1}\\)，即 \\(A_t \\sim \\pi_E(\\cdot \\mid S_t, U_{t-1})\\)。学习者观测到的数据为 \\(\\mathcal{D} = \\{(S_{1:T}^{(i)}, W_{0:T-1}^{(i)}, A_{1:T}^{(i)})\\}_{i=1}^n\\)。\n\n![方法框架](https://arxiv.org/html/2601.22206v1/x1.png)\n> **图1**：测量误差下模仿学习的因果图。展示了变量间的因果关系：潜在状态\\(U_t\\)影响动作\\(A_t\\)（有一步延迟）和测量值\\(W_t\\)；观测状态\\(S_t\\)也影响动作。学习者无法直接观测\\(U_t\\)。\n\n本文提出的因果最优模仿策略 \\(\\pi_{\\mathrm{opt}}\\) 定义为，在干预 \\(do(S_t = s)\\) 下专家行动分布的众数：\n\\[\n\\pi_{\\mathrm{opt}}(s) := \\arg\\max_{a \\in \\mathcal{A}} p(A_t^{(s)} = a).\n\\]\n该策略具有两个关键性质：1) **因果解释性**：它等价于在潜在上下文\\(U_{t-1}\\)的总体分布上，最小化与专家机制\\(\\pi_E(s, U_{t-1})\\)分歧的贝叶斯最优策略。2) **对分布偏移的鲁棒性**：\\(\\pi_{\\mathrm{opt}}\\) 仅依赖于跨领域不变的专家机制 \\(\\pi_E\\) 和潜在状态的边际分布 \\(\\mathbb{P}(U_{t-1})\\)，而不依赖于易变的测量机制 \\(\\mathbb{P}(W_{t-1} | U_{t-1})\\) 或状态转移动力学 \\(\\mathbb{P}(S_t | U_{t-1}, ...)\\)。相比之下，BC1 (\\(\\pi_{\\mathrm{BC1}}(s) = \\arg\\max p(A_t | S_t=s)\\)) 对潜在状态给定观测状态的后验 \\(\\mathbb{P}(U_{t-1} | S_t)\\) 的变化敏感；BC2 (\\(\\pi_{\\mathrm{BC2}}(s, v) = \\arg\\max p(A_t | S_t=s, V_{t-1}=v)\\)) 则对测量机制 \\(\\mathbb{P}(W_{t-1} | U_{t-1})\\) 的变化敏感。\n\n核心创新在于为这一因果目标提供了识别条件和估计方法。受近端因果推断启发，作者将噪声测量 \\(W_{t-1}\\) 视为潜在变量 \\(U_{t-1}\\) 的代理变量。在满足一定的条件独立性假设（见原文 Assumption 2.1）和完备性条件下，因果目标 \\(\\mathbb{P}(A_t^{(s)} = a)\\) 可以从观测数据中识别出来。对于离散状态/测量空间，通过求解识别矩方程的样本模拟来估计。对于连续空间，本文提出了一种在再生核希尔伯特空间（RKHS）函数类上的对抗性估计方法（CausIL），通过最小化一个矩损失函数来学习所需参数，从而实现了完全离线的策略恢复。\n\n## 实验与结果\n**实验设置**：本文在来自PhysioNet/Computing in Cardiology Challenge 2019队列的半模拟纵向数据上评估CausIL。该数据模拟了临床决策场景，其中潜在状态\\(U_t\\)（如疾病严重程度）通过噪声测量\\(W_t\\)（如生命体征）观测，专家动作\\(A_t\\)（治疗决定）依赖于\\(S_t\\)和\\(U_{t-1}\\)。实验考虑了三种分布偏移场景：无偏移、潜在状态与观测状态关联偏移（\\(\\mathbb{P}(U_{t-1} | S_t)\\)变化）、测量机制偏移（\\(\\mathbb{P}(W_{t-1} | U_{t-1})\\)变化）。\n\n**对比方法**：主要对比了两个行为克隆基线：\\(\\pi_{\\mathrm{BC1}}\\)（仅基于\\(S_t\\)）和\\(\\pi_{\\mathrm{BC2}}\\)（基于\\(S_t\\)和\\(V_{t-1} = (S_{t-1}, W_{t-1})\\)）。\n\n**关键实验结果**：评估指标是所学策略与专家策略之间在测试域上的均方误差（MSE）。结果表明，在无分布偏移时，所有方法性能相近。但在分布偏移下，CausIL（估计\\(\\pi_{\\mathrm{opt}}\\)）表现出显著更强的鲁棒性。\n\n![无偏移结果](https://arxiv.org/html/2601.22206v1/Results/no_shift.png)\n> **图2**：无分布偏移时，CausIL、BC1和BC2的均方误差（MSE）性能接近，说明在稳定环境下因果目标与观测目标一致。\n\n![U关联偏移](https://arxiv.org/html/2601.22206v1/Results/shift_on_U.png)\n> **图3**：当潜在状态与观测状态的关联（\\(\\mathbb{P}(U_{t-1} | S_t)\\)）发生偏移时，BC1的MSE显著上升，而CausIL保持稳定，验证了\\(\\pi_{\\mathrm{opt}}\\)对该类偏移的鲁棒性。\n\n![W测量偏移](https://arxiv.org/html/2601.22206v1/Results/shift_on_W.png)\n> **图4**：当测量机制（\\(\\mathbb{P}(W_{t-1} | U_{t-1})\\)）发生偏移时，BC2的MSE显著上升，而CausIL几乎不受影响，验证了\\(\\pi_{\\mathrm{opt}}\\)不依赖于易变的测量机制。\n\n![真实数据无偏移](https://arxiv.org/html/2601.22206v1/Results/real_data_no_shift.png)\n> **图5**：在真实临床数据（无模拟偏移）上，CausIL与BC方法的MSE对比，显示了方法在真实分布下的可行性。\n\n![真实数据U偏移](https://arxiv.org/html/2601.22206v1/Results/real_data_shift_on_U.png)\n> **图6**：在真实数据上引入\\(\\mathbb{P}(U_{t-1} | S_t)\\)偏移，CausIL相对于BC1的鲁棒性优势依然明显。\n\n![真实数据W偏移](https://arxiv.org/html/2601.22206v1/Results/real_data_shift_on_W.png)\n> **图7**：在真实数据上引入\\(\\mathbb{P}(W_{t-1} | U_{t-1})\\)偏移，CausIL相对于BC2的鲁棒性优势同样得到证实。\n\n**消融实验**：本文通过理论分析和模拟实验，实质上对\\(\\pi_{\\mathrm{opt}}\\)、\\(\\pi_{\\mathrm{BC1}}\\)和\\(\\pi_{\\mathrm{BC2}}\\)这三个“组件”（即不同的策略目标）进行了消融研究。结果总结如下：1) \\(\\pi_{\\mathrm{opt}}\\) 组件是鲁棒性的核心，它去除了对非稳定测量机制和状态动力学的依赖。2) 包含\\(V_{t-1}\\)信息的\\(\\pi_{\\mathrm{BC2}}\\)在训练域内预测性更好，但正是因为它依赖于这些信息，使其对测量偏移极度敏感。3) 仅使用\\(S_t\\)的\\(\\pi_{\\mathrm{BC1}}\\)虽然避免了测量机制依赖，但仍受状态动力学变化的影响。实验结果表明，在各自敏感的分布偏移下，相应的BC方法性能下降，而CausIL（基于\\(\\pi_{\\mathrm{opt}}\\)）在所有测试的偏移场景中均保持稳定。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了因果最优模仿策略**：明确定义了在存在测量误差和延迟潜在混淆情况下的模仿学习目标 \\(\\pi_{\\mathrm{opt}}\\)，该目标具有清晰的因果解释，并被证明对特定的分布偏移具有鲁棒性。\n2.  **提供了识别与估计方案**：借鉴近端因果推断，给出了仅从离线演示数据中识别 \\(\\pi_{\\mathrm{opt}}\\) 的条件，并针对离散和连续空间提出了相应的估计算法（CausIL），首次在测量误差和分布偏移共存的情况下实现了完全离线的因果模仿学习。\n3.  **实证验证**：在临床决策的半模拟数据上验证了所提方法的有效性，证明了其在分布偏移下优于标准行为克隆基线的鲁棒性。\n\n**局限性**：\n1.  识别依赖于严格的条件独立性假设和代理变量的“完备性”条件，这些条件在实际中可能难以完全满足或验证。\n2.  方法主要关注外生分布偏移（环境/测量管道变化），对由策略自身行动引起的内生分布偏移（复合误差）的讨论有限。\n3.  连续空间中的对抗性估计方法可能面临计算复杂性和优化挑战。\n\n**启示**：\n1.  为处理具有不完美感知和跨域部署的模仿学习问题提供了一个新的因果推理框架。\n2.  将噪声测量视为代理变量而非直接状态，为从有噪声的离线数据中恢复稳健策略开辟了新途径。\n3.  未来的工作可以探索放松识别假设、将内生与外生偏移共同建模，以及开发更高效的连续空间估计算法。",
      "imageUrls": [
        "https://arxiv.org/html/2601.22206v1/x1.png",
        "https://arxiv.org/html/2601.22206v1/Results/no_shift.png",
        "https://arxiv.org/html/2601.22206v1/Results/shift_on_U.png",
        "https://arxiv.org/html/2601.22206v1/Results/shift_on_W.png",
        "https://arxiv.org/html/2601.22206v1/Results/real_data_no_shift.png",
        "https://arxiv.org/html/2601.22206v1/Results/real_data_shift_on_U.png",
        "https://arxiv.org/html/2601.22206v1/Results/real_data_shift_on_W.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.22074",
      "title": "mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning",
      "url": "http://arxiv.org/abs/2601.22074",
      "arxivId": "2601.22074",
      "date": "2026-01-29",
      "authors": "Pieter Abbeel Team",
      "category": "Manipulation",
      "summary": "本文针对现有机器人学习框架在轻量化和可维护性上的不足，提出了mjlab轻量级框架。其核心是结合Isaac Lab的manager-based API（用于模块化组合观测、奖励等组件）与MuJoCo Warp GPU加速物理引擎。该方法实现了依赖极简、启动快速，并直接暴露MuJoCo原生数据结构。实验表明，该框架可在单个GPU上并行模拟数千个环境，并提供了运动跟踪、模仿等参考任务实现。",
      "detailedSummary": "## 研究背景与动机\n机器人强化学习依赖于高保真仿真以成功实现从仿真到现实的迁移，这需要精确处理执行器动力学、接触建模等细节。现有的框架在易用性和功能上存在权衡。Isaac Lab提供了全面的、基于管理器API的GPU加速平台，但其依赖Omniverse运行时，增加了安装复杂性和启动延迟，且其物理引擎PhysX曾为闭源，难以进行底层调试。MuJoCo Playground则采用极简抽象和单体式环境定义，虽易于快速原型开发，但在多机器人或多任务场景下代码重复严重，难以维护。因此，当前需要一个既轻量级，又基于成熟编排API并能访问一流物理引擎的框架。\n\n本文旨在填补这一空白，提出了mjlab框架。其核心思路是：采纳Isaac Lab引入的基于管理器的API（用于组合观测、奖励、事件等模块化构件），并将其与用于GPU加速物理仿真的MuJoCo Warp相结合，从而打造一个依赖极少、启动迅速、可直接访问原生MuJoCo数据结构且与PyTorch原生接口集成的轻量级框架。\n\n## 方法详解\nmjlab的整体架构围绕三个核心设计理念：安装摩擦最小化、物理透明与可检查性、与MuJoCo生态系统的紧密集成。其pipeline如图2所示：用户将实体（如机器人、物体）的MJCF描述组合成一个统一的`MjSpec`；该规范在CPU上被编译成`MjModel`，然后通过MuJoCo Warp的例程传输到GPU进行并行仿真；顶层的`ManagerBasedRlEnv`负责编排马尔可夫决策过程（MDP）；训练则由RSL-RL等库处理。\n\n![架构总览](https://arxiv.org/html/2601.22074v1/rough_terrain.png)\n> **图2**：mjlab整体架构。实体被组合成`MjSpec`，编译后传输至MuJoCo Warp进行GPU上的并行仿真。`ManagerBasedRlEnv`管理MDP流程，RSL-RL负责策略训练。\n\n**核心组件**包括：\n1.  **实体（Entity）**：统一表示场景中的物理对象（机器人、被操纵物体、静态装置），通过`base type`和`articulation`属性区分类型，并聚合了运动学数据供观测和奖励项使用，避免了重复计算。\n2.  **传感器（Sensors）**：采用分层设计。既可封装MJCF中定义的传感器直接读取原生数据，也可在Python配置中定义。此外，mjlab提供了超越原生功能的定制传感器，如用于地形高度扫描的射线投射传感器、跟踪特定身体对之间碰撞力的接触传感器。\n3.  **执行器（Actuators）**：同样分层。支持MJCF中定义的原生执行器，也提供在MuJoCo外部GPU上计算扭矩的定制实现，如理想PD控制器、带速度相关扭矩饱和的直流电机模型、从数据学习硬件动态的MLP执行器。所有类型可共存于同一实体，并支持通过包装类模拟量化到物理时间步长的执行延迟。\n4.  **地形（Terrain）**：生成用于 locomotion 训练的地面几何。可生成平坦地面或由多种地形区块（如平坦、楼梯、斜坡金字塔、高度场噪声、正弦波）组成的网格。地形特征可通过难度参数缩放，并支持随机采样或按难度递增排列的课程学习模式。\n\n**基于管理器的API**是mjlab的核心创新，它将环境构建分解为可组合的模块。`ManagerBasedRlEnv`遵循Gym接口，其`step()`函数按固定管道执行：动作管理 → 仿真（含子步） → 终止判断 → 奖励计算 → 环境重置 → 命令生成 → 事件应用 → 观测计算。每个管理器封装MDP的一个特定方面：\n-   **动作管理器**：接收策略输出张量，按注册的动作项进行分割并路由到相应执行器，支持动作裁剪和历史跟踪。\n-   **终止管理器**：评估布尔条件（如非法接触、关节限位、超时），并检测数值不稳定（NaN/Inf），提供近期状态的滚动缓冲区以辅助调试。\n-   **奖励管理器**：计算各奖励项的加权和，自动按控制时间步长缩放以保证奖励量级与仿真频率无关，并跟踪和记录各奖励项的回合总和。\n-   **课程管理器**：根据策略性能调整训练条件（如奖励权重、命令范围、地形难度）。\n-   **事件管理器**：在环境生命周期的特定点（如启动、重置、固定间隔）执行钩子，主要用于领域随机化。当事件修改MuJoCo模型字段时，框架会透明地将该字段从共享值扩展为每个环境独立的数组，并重建CUDA捕获图。\n-   **命令管理器**：生成策略的目标信号（如速度目标、路径点），并可按配置间隔重新采样。\n-   **观测管理器**：观测项是可组合的函数，读取实体数据、传感器或命令并返回张量。管理器通过可配置的管道处理这些张量：裁剪、缩放、添加噪声、延迟、与历史缓冲区拼接。支持多个观测组（如策略和评论家），实现非对称的演员-评论家架构。\n\n**与现有方法的创新对比**体现在：1) **轻量级与专注性**：仅依赖MuJoCo Warp，避免Omniverse等重型依赖，安装和启动更快。2) **物理透明性**：直接暴露原生的`MjModel`/`MjData`结构，便于调试和状态访问。3) **软件设计优化**：采用PyTorch-native接口（通过`TorchArray`零拷贝包装）、实例化配置（而非易错的类继承配置）、CLI优先配置（通过`tyro`自动暴露所有参数）、定义共置（配置与实现在同一文件），降低了使用和扩展门槛。\n\n## 实验与结果\n论文未在传统学术论文意义上报告量化性能对比实验，而是通过实现和展示**三个参考任务**来验证框架的实用性和能力。这些任务涵盖了机器人学习的典型领域：\n1.  **Locomotion: Velocity Tracking**：训练运动控制器在平坦或崎岖地形上跟踪线速度和角速度命令。奖励项鼓励精确的速度跟踪，并惩罚过大的身体角速度、角动量、关节限位违反、动作速率和脚滑。\n2.  **Whole-Body Control: Motion Imitation**：训练人形机器人（Unitree G1）跟踪参考运动片段，实现了DeepMimic框架并集成了BeyondMimic的扩展。奖励项惩罚根关节位置/朝向、相对身体姿态和身体速度的偏差，并包含自碰撞成本。\n3.  **Manipulation: Cube Lifting**：训练YAM机械臂将立方体抓取并提升到目标姿态。采用分阶段奖励结构，先引导末端执行器接近立方体，再奖励将立方体提升到目标高度。\n\n![任务演示](https://arxiv.org/html/2601.22074v1/mjlab-banner.jpg)\n> **图1**：使用mjlab中集成的BeyondMimic运动跟踪pipeline训练的Unitree G1人形机器人执行舞蹈动作。数千个并行环境在单个GPU上仿真，并在mjlab基于Web的Viser查看器中可视化。\n\n![地形生成](https://arxiv.org/html/2601.22074v1/robots.png)\n> **图3**：由mjlab生成的复合地形网格。子地形区块包括平坦地面、楼梯、斜坡金字塔、高度场噪声和波浪图案。沿一个轴难度递增，用于基于课程的学习。\n\n![机器人形态](https://arxiv.org/html/2601.22074v1/robots.png)\n> **图4**：mjlab内置的三种机器人形态：Unitree G1人形机器人（左）、Unitree Go1四足机器人（中）和YAM机械臂（右）。所有模型均改编自MuJoCo Menagerie。\n\n论文通过在线视频链接展示了各任务在仿真和真实硬件上的运行效果，证明了框架的有效性。此外，mjlab已被用于加州大学伯克利分校的机器人研究生课程，并被多个开源项目采用，体现了其易用性和实用性。\n\n## 总结与启发\n本文的核心贡献在于：1) **提供了一个轻量、高效且易于部署的机器人学习仿真框架**，通过结合成熟的Manager-based API与高性能的MuJoCo Warp物理后端，在易用性和仿真能力之间取得了良好平衡。2) **采用了高度模块化和可组合的环境设计**，通过基于管理器的API显著降低了代码重复，提升了实验的可维护性和迭代速度。3) **在软件工程层面进行了精心设计**，包括PyTorch-native接口、实例化配置、CLI优先配置、定义共置和全面的静态类型检查与测试，极大地改善了开发体验和代码质量，甚至使其适合AI辅助开发。\n\n论文自身提到的局限性包括：**物理后端单一**，仅支持MuJoCo Warp，而非跨仿真器可移植；**感知能力有限**，未提供高保真RGB渲染，尽管这不妨碍通过特权策略蒸馏的方式训练视觉策略。\n\n本文对后续研究的启示在于：一个成功的机器人学习框架不仅需要强大的仿真能力，**极低的入门门槛、透明的调试接口和优秀的软件设计**对于促进广泛采用和加速研究迭代同样至关重要。mjlab在依赖管理、配置方式和代码组织上的实践，为未来工具的开发提供了有价值的参考。其设计表明，专注于特定技术栈（如MuJoCo）的深度集成，可能比追求大而全的通用性更能满足研究社区对高效、可靠实验平台的需求。",
      "imageUrls": [
        "https://arxiv.org/html/2601.22074v1/mjlab-banner.jpg",
        "https://arxiv.org/html/2601.22074v1/rough_terrain.png",
        "https://arxiv.org/html/2601.22074v1/robots.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.22018",
      "title": "PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy",
      "url": "http://arxiv.org/abs/2601.22018",
      "arxivId": "2601.22018",
      "date": "2026-01-30",
      "authors": "Jie Mei Team",
      "category": "Manipulation",
      "summary": "本文针对现有3D视觉扩散策略中轻量点云编码器与庞大解码器不匹配导致的参数浪费问题，提出PocketDP3。该方法采用基于MLP-Mixer构建的轻量Diffusion Mixer（DiM）替代传统的条件U-Net解码器，实现了跨时空与通道的高效融合，并支持无需蒸馏的两步推理。在RoboTwin2.0、Adroit和MetaWorld三个仿真基准上，模型以不足原方法1%的参数取得最优性能，同时加速了推理，真实世界实验进一步验证了其实用性与迁移能力。",
      "detailedSummary": "## 研究背景与动机\n当前，基于3D视觉的扩散策略在机器人操作技能学习中展现出强大能力。然而，这些模型普遍存在一种架构上的不匹配：一个轻量高效的点云编码器通常与一个庞大的解码器配对。论文指出，在已经获得紧凑场景表示的前提下，这种设计可能导致解码器存在严重的参数浪费。此外，扩散策略面临推理速度慢的挑战，通常需要多次迭代去噪步骤，难以满足实时控制需求。\n\n本文针对3D扩散策略中解码器参数量大、效率低的痛点，提出了新的视角：在给定高效、信息丰富的场景表示后，解码器可以大幅精简。其核心思路是：用一个基于MLP-Mixer构建的轻量级Diffusion Mixer（DiM）替代原有的重型条件U-Net解码器，在显著减少参数量的同时，无需额外的蒸馏技术即可实现两步快速推理。\n\n## 方法详解\nPocketDP3的整体框架沿用了DP3的高效点云观测编码器，但将其核心的条件U-Net解码器替换为轻量级的Diffusion Mixer（DiM）。输入包括带噪声的动作轨迹、点云观测和机器人本体状态，输出为预测的去噪后动作轨迹。\n\n![方法框架](https://arxiv.org/html/2601.22018v2/fig/pocket_dp3_final_standardized.png)\n> **图2**：PocketDP3整体架构。左侧为观测编码器（沿用DP3），右侧为本文提出的DiM解码器。DiM由K个DiM块堆叠而成，每个块基于MLP-Mixer风格构建，通过交替的时间维度和通道维度混合实现高效信息融合。\n\n**核心模块与技术细节**：\n1.  **观测编码器**：完全沿用DP3的设计。对输入点云进行最远点采样降噪，通过一个简单的MLP和最大池化操作得到一个紧凑的3D表示 **z**。机器人本体状态 **s** 经线性投影后与 **z** 相加，形成去噪条件 **C**。\n2.  **DiM解码器**：这是本文的核心创新。它接收带噪轨迹 **A_t**，并受条件 **z** 和扩散时间步 **t** 的调控。采用 **x_0** 预测参数化，直接输出预测的干净轨迹 **Â_0**。首先，带噪轨迹被线性投影为一系列令牌（Tokens）。这些令牌随后通过 **K** 个DiM块与条件信息进行交互和融合。\n3.  **DiM块设计**：每个DiM块基于MLP-Mixer架构。给定令牌序列 **H**，首先沿时间维度进行信息聚合：`H = H + MLP(H^T)^T`；然后沿通道维度进行信息聚合：`H = H + MLP(H)`。最后，通过一个结合了门控残差连接的FiLM层注入条件信息：首先通过一个MLP从 **H** 和条件 **C** 计算缩放因子 **α**、平移因子 **β** 和门控因子 **g**；然后进行仿射变换 `Ĥ = H ⊙ α + β`；最后通过门控残差连接得到输出 `H = Ĥ ⊙ g + H`。堆叠多个DiM块后，通过一个线性层将最终令牌映射为预测动作 **Â_0**。\n\n**与现有方法的创新点**：\n-   **架构创新**：首次在3D扩散策略领域探索并验证了基于Mixer架构的解码器的有效性，替代了此前普遍使用的、参数庞大的条件U-Net。\n-   **参数高效**：DiM通过简单的转置和MLP操作实现跨时间和通道的高效融合，与U-Net依赖增加通道宽度来提升性能的方式相比，参数效率显著提高。\n-   **快速推理**：在训练中采用DDIM噪声调度器，在推理时仅需2步去噪评估即可达到高性能，无需依赖复杂的蒸馏或一致性训练技术。\n\n## 实验与结果\n**实验设置**：在三个仿真基准上进行了评估：RoboTwin2.0（双臂操作）、Adroit（灵巧手操作）和MetaWorld（单臂操作）。严格遵循先前SOTA方法（如DP3）的实验协议，使用相同的数据集，且未使用任何额外的数据增强。\n\n**对比方法**：在RoboTwin2.0上与DP3、DP（2D扩散策略）和VLA模型 **π_0** 对比；在Adroit和MetaWorld上与DP3、FlowPolicy、DP以及模仿学习基线IBC、BCRNN对比。\n\n**关键实验结果**：\n1.  **性能对比**：在RoboTwin2.0的20个任务上，PocketDP3-base和PocketDP3-tiny的平均成功率分别达到71.6%和66.0%，显著优于DP3的50.8%。在Adroit和MetaWorld的10个任务上，PocketDP3-base以77.4%的平均成功率取得最佳性能，优于DP3（73.0%）和FlowPolicy（72.6%）。\n\n![性能对比](https://arxiv.org/html/2601.22018v2/fig/pokeman.png)\n> **图1**：PocketDP3与SOTA方法在推理延迟/模型大小和平均成功率上的对比。PocketDP3在显著减小模型尺寸和降低延迟的同时，取得了更高的成功率。\n\n> **表2**：RoboTwin2.0基准测试结果。PocketDP3在两个版本上均大幅超越DP3等基线方法，在多数任务上提升显著。\n> **表3**：Adroit和MetaWorld基准测试结果。PocketDP3-base取得了最高的平均成功率（77.4%）。\n\n2.  **效率对比**：PocketDP3-base和PocketDP3-tiny的参数量分别仅为1.73M和0.53M，不到DP3（255.1M）的1%。在两步推理下，其延迟仅约4.5毫秒，比DP3（10步，51.4ms）快一个数量级，且参数量比同样实现快速推理的FlowPolicy（255.8M）少两个数量级。\n\n> **表4**：模型大小和推理延迟对比。PocketDP3在参数量和延迟上均有巨大优势。\n\n3.  **消融实验**：\n    -   **架构必要性**：在相近参数量下，纯MLP（Vanilla-MLP）性能崩溃（平均成功率0%），缩减通道的U-Net（Vanilla-UNet）性能也显著下降（平均成功率53.7%），而PocketDP3-base达到67.7%，证明了DiM块设计的优越性。\n    -   **推理步数（NFE）影响**：一步推理性能有明显下降，两步推理已能达到很强性能，增加至5步或10步并未带来进一步增益，验证了两步推理的充分性。\n\n> **表5**：设计选择消融研究。DiM架构在相同参数量下性能显著优于纯MLP和缩减版U-Net。\n> **表6**：推理步数（NFE）消融研究。两步推理已能取得与多步推理相当的性能。\n\n![仿真实验可视化](https://arxiv.org/html/2601.22018v2/x1.png)\n> **图3**：仿真实验可视化。展示了PocketDP3在多种任务中的成功执行情况，体现了其架构设计的有效性。\n\n## 总结与启发\n**核心贡献**：\n1.  指出了当前3D扩散策略中“轻编码器-重解码器”的架构不平衡问题，并提出了参数效率更高的新视角。\n2.  提出了PocketDP3，首次将基于MLP-Mixer的轻量级Diffusion Mixer（DiM）引入3D视觉运动策略，在显著减少参数量的同时保持甚至提升了性能。\n3.  实现了无需额外一致性蒸馏的两步快速推理，大幅提升了策略的实用性，更适合资源受限的实时部署场景。\n\n**局限性**：论文提到，对于需要高频空间细节（如高分辨率图像合成）的任务，强大的空间归纳偏置（如U-Net提供的）可能仍然是必要的。本文的轻量级解码器策略在动作轨迹生成这种相对低频的信号预测上优势明显。\n\n**后续启示**：\n1.  为机器人策略网络设计提供了新思路：在获得高质量、紧凑的场景表示后，可以优先考虑极简、高效的解码器架构，而非盲目增大模型。\n2.  证明了在特定条件下，扩散模型无需复杂蒸馏即可实现极简（两步）推理，这启发后续研究进一步探索高效采样与模型架构的协同设计。\n3.  PocketDP3的轻量化特性使其更易于在边缘设备部署，推动了学习型机器人策略向实用化迈进。",
      "imageUrls": [
        "https://arxiv.org/html/2601.22018v2/fig/pokeman.png",
        "https://arxiv.org/html/2601.22018v2/fig/pocket_dp3_final_standardized.png",
        "https://arxiv.org/html/2601.22018v2/x1.png",
        "https://arxiv.org/html/2601.22018v2/x2.png",
        "https://arxiv.org/html/2601.22018v2/x3.png",
        "https://arxiv.org/html/2601.22018v2/x4.png",
        "https://arxiv.org/html/2601.22018v2/fig/adjust_cup.png",
        "https://arxiv.org/html/2601.22018v2/fig/place_cube.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.21998",
      "title": "Causal World Modeling for Robot Control",
      "url": "http://arxiv.org/abs/2601.21998",
      "arxivId": "2601.21998",
      "date": "2026-01-29",
      "authors": "Yinghao Xu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人控制中视觉-语言-动作模型存在的表示纠缠问题，提出LingBot-VA自回归扩散框架。其核心方法包括：1）基于混合Transformer的共享视觉-动作潜在空间；2）结合真实观测的闭环展开机制；3）并行化动作预测与电机执行的异步推理管道。实验表明，该模型在模拟与真实场景中，于长视野操作、数据效率及对新配置的泛化能力上均显著优于现有方法（如π_{0.5}）。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型已成为通用机器人操作的有前景范式，但其主流的前馈范式存在“表征纠缠”的关键局限性。该范式要求单一网络从统一的监督信号中同时学习视觉场景理解、物理动力学和运动控制，这可能导致样本效率有限和泛化能力欠佳。近期将世界建模引入机器人策略的尝试（如分块视频-动作扩散模型）面临三大局限：**反应性差距**（开环生成难以整合实时反馈）、**有限的长期记忆**（分块生成缺乏持久历史缓存导致不一致）以及**因果性违反**（分块内双向注意力使未来令牌影响过去预测，违背物理现实的因果性）。\n\n本文针对上述痛点，提出了一个自回归世界建模的新视角。其核心思路是：**通过一个自回归扩散框架，在统一的交错视频-动作序列中联合建模视觉动力学预测和动作推理，并利用KV缓存实现持久记忆和因果一致性，以支持鲁棒的闭环控制。**\n\n## 方法详解\n本文提出的LingBot-VA是一个自回归扩散世界模型，其整体框架通过条件流匹配实现统一的视频-动作世界建模。模型采用双流Mixture-of-Transformers架构，将视频和动作令牌交错在单一序列中。在每一个自回归步骤中，视频流首先通过流匹配预测未来的潜在视觉状态，随后动作流根据预测的视觉转换，通过逆动力学解码出相应的动作。\n\n![方法框架](https://arxiv.org/html/2601.21998v1/x2.png)\n> **图2**：LingBot-VA框架总览。模型采用双流Mixture-of-Transformers架构，视频和动作令牌在单一序列中交错。在每个自回归步骤，视频流通过流匹配预测未来潜在视觉状态，动作流则基于预测的视觉转换解码相应动作。\n\n**核心模块与技术细节**：\n1.  **共享潜在空间与交错序列**：视觉观测通过因果视频VAE编码为潜在令牌，动作向量通过轻量级MLP投影为令牌嵌入。两者在时间顺序上交错，形成统一序列 `[z_t, a_t,1, a_t,2, ..., a_t,τ, z_t+1, ...]`，其中视频帧被稀疏化（τ=4），即每个视频帧对应τ个连续动作。\n2.  **Mixture-of-Transformers架构**：采用双流Transformer，视频流基于大型预训练视频生成模型Wan2.2-5B初始化，动作流深度相同但宽度显著更小（d_a ≪ d_v）。每个MoT层中，视频和动作流先由独立的Transformer块处理，再通过跨模态注意力融合。动作流权重通过缩放预训练视频权重初始化，以稳定训练。\n3.  **训练策略**：\n    *   **教师强制**：将交错视频-动作序列视为统一序列，使用因果注意力掩码进行下一个令牌预测训练。这匹配了部署时机器人接收真实世界观测的设定。\n    *   **噪声历史增强**：训练时以50%的概率对视频历史 `z_≤t` 添加噪声（`s_aug ∈ [0.5, 1]`），使动作解码器学会从部分含噪的视觉表示中预测动作。推理时可仅将视频令牌去噪至s=0.5，从而减少去噪步骤，加速推理。\n    *   **可变块大小训练**：训练时随机采样块大小K（如[1, 8]），使模型适应不同时间视野，推理时可灵活权衡计算效率与规划范围。\n4.  **损失函数**：联合优化视频动力学损失 `L_dyn`（预测未来视觉状态的流匹配损失）和逆动力学损失 `L_inv`（解码动作的流匹配损失），总损失为 `L = L_dyn + λ L_inv`。\n5.  **异步推理管道**：为应对大规模自回归视频-动作模型的推理延迟，设计了异步协调流水线：在机器人执行当前动作的同时，世界模型预测未来视觉状态并规划后续序列。结合KV缓存加速，实现了计算与执行的重叠。\n\n**创新点**：与现有方法相比，本文的创新具体体现在：1) **自回归视频-动作世界建模**：在单一因果自回归框架内统一视频预测与动作推理，通过KV缓存保持持久记忆；2) **不对称容量的MoT架构与高效训练策略**：包括噪声历史增强和可变块大小训练；3) **异步执行策略**：并行化动作预测与电机执行以支持高效实时控制。\n\n## 实验与结果\n**实验设置**：模型在模拟基准（如MetaWorld）和真实世界场景中进行了评估。对比的基线方法包括最先进的VLA策略，如 `π_0.5`。\n\n**关键实验结果**：\n1.  **长视野操作**：在涉及多个子任务的复杂长视野操作中，LingBot-VA表现出卓越的时序一致性，显著优于基线方法。\n2.  **高精度操作**：在需要毫米级精度的任务（如插孔）中，模型凭借其世界建模能力展现出更强性能。\n3.  **数据效率与泛化**：模型在训练后表现出优异的数据效率，并能强有力地泛化到新的场景和物体配置。\n\n![模拟基准结果](https://arxiv.org/html/2601.21998v1/x4.png)\n> **图4**：在MetaWorld模拟基准上的成功率对比。LingBot-VA在多个长视野和高精度任务上超越了基线方法 `π_0.5`。\n\n![真实世界长视野任务](https://arxiv.org/html/2601.21998v1/x5.png)\n> **图5**：真实世界长视野操作任务（如“打开微波炉并放入杯子”）的定性结果。LingBot-VA能够成功规划并执行多步骤序列。\n\n![消融研究：噪声历史增强](https://arxiv.org/html/2601.21998v1/x6.png)\n> **图6**：噪声历史增强的消融研究。使用该技术（Partial）在保持相近成功率的同时，显著减少了每步推理时间（Latency）。\n\n![消融研究：块大小](https://arxiv.org/html/2601.21998v1/x7.png)\n> **图7**：可变块大小训练的消融研究。训练时采样不同块大小使模型能适应多种部署配置，在测试时选择不同块大小能权衡成功率和延迟。\n\n![泛化能力](https://arxiv.org/html/2601.21998v1/x8.png)\n> **图8**：对新物体配置的泛化能力测试。模型在面对未见过的物体摆放时仍能成功完成任务。\n\n![逆动力学推理](https://arxiv.org/html/2601.21998v1/x9.png)\n> **图9**：模型支持从机器人视频中进行逆动力学推理，展示了其理解动作-视觉关联的能力。\n\n![异步推理延迟分析](https://arxiv.org/html/2601.21998v1/x10.png)\n> **图10**：异步推理管道的延迟分析。通过并行化预测与执行，有效隐藏了部分计算延迟，支持更高频率的控制。\n\n**消融实验总结**：噪声历史增强是平衡推理速度与性能的关键；可变块大小训练提供了部署灵活性；MoT架构和自回归公式对长时序一致性至关重要。\n\n## 总结与启发\n**核心贡献**：1) 提出了一个自回归扩散框架，在架构上统一了视觉动力学预测和动作推理，同时保持了概念上的区分，并通过KV缓存和注意力掩码实现了持久记忆与因果一致性；2) 设计了结合不对称容量MoT架构、噪声历史增强和异步协调的高效训练与部署策略；3) 在模拟和真实世界实验中展示了其在长视野、高精度操作、数据效率和泛化方面的卓越性能。\n\n**局限性**：论文自身提到，大规模自回归视频-动作模型的主要挑战是推理延迟。尽管通过噪声历史增强和异步管道进行了缓解，但生成高保真视频令牌的迭代去噪过程计算量仍然很大。\n\n**对后续研究的启示**：1) **因果世界建模的潜力**：将物理世界的因果性作为核心设计原则，为构建更鲁棒、可解释的机器人策略提供了新方向。2) **表示与效率的权衡**：如何在保持丰富世界模型能力的同时实现高效实时控制，仍是一个开放问题。本文的噪声历史增强和异步执行策略为这一方向提供了有益探索。3) **大规模视频预训练的价值**：工作凸显了利用海量野生视频数据学习物理先验，再通过相对少量的机器人演示数据进行接地，是一条高效的机器人学习路径。",
      "imageUrls": [
        "https://arxiv.org/html/2601.21998v1/x1.png",
        "https://arxiv.org/html/2601.21998v1/x2.png",
        "https://arxiv.org/html/2601.21998v1/x3.png",
        "https://arxiv.org/html/2601.21998v1/x4.png",
        "https://arxiv.org/html/2601.21998v1/x5.png",
        "https://arxiv.org/html/2601.21998v1/x6.png",
        "https://arxiv.org/html/2601.21998v1/x7.png",
        "https://arxiv.org/html/2601.21998v1/x8.png",
        "https://arxiv.org/html/2601.21998v1/x9.png",
        "https://arxiv.org/html/2601.21998v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.21926",
      "title": "Information Filtering via Variational Regularization for Robot Manipulation",
      "url": "http://arxiv.org/abs/2601.21926",
      "arxivId": "2601.21926",
      "date": "2026-01-29",
      "authors": "Jie Me Team",
      "category": "Manipulation",
      "summary": "本文针对基于扩散模型的机器人操作策略中，中间特征存在冗余和任务无关噪声的问题，提出了一种轻量级的**变分正则化**模块。该方法通过对主干特征施加时间步条件的高斯分布并应用KL散度正则器，形成一个自适应信息瓶颈，从而在推理时有效过滤噪声。实验表明，该方法在三个仿真基准上显著优于基线DP3，成功率最高提升6.1%，并实现了新的最先进性能。",
      "detailedSummary": "## 研究背景与动机\n当前，基于3D视觉表示的扩散模型已成为学习复杂机器人操作技能的一类强大策略。然而，大多数现有方法（如当前最先进的DP3）采用了一个过大的U-Net去噪解码器。虽然增加模型容量可以改善去噪效果，但经验证据表明，这也会在中间特征块中引入冗余和噪声。一个关键发现是，在推理时随机掩码主干特征（不改变训练）可以提高性能，这证实了中间特征中存在与任务无关的噪声。因此，本文针对扩散策略解码器中“中间特征存在冗余和噪声”这一具体痛点，提出了一种新的视角：对解码器内部表征进行主动的信息过滤。本文的核心思路是：在U-Net解码器的关键位置引入一个轻量级的变分正则化模块，该模块通过施加一个以扩散时间步为条件的高斯分布和KL散度正则器，形成一个自适应的信息瓶颈，以过滤掉任务无关的噪声信息。\n\n## 方法详解\n本文方法的整体框架建立在最先进的3D扩散策略DP3之上。其输入是嘈杂的动作序列、扩散时间步以及由点云编码器生成的紧凑场景上下文条件。输出是去噪后的预测动作。核心创新是在DP3的U-Net解码器中，于最后一个下采样特征之后插入了一个变分正则化模块，旨在该处（噪声最可能积累的位置）进行有效的信息过滤。\n\n![方法框架](https://arxiv.org/html/2601.21926v2/x1.png)\n> **图1**：提出的变分正则化方法。(a) 整体框架：基于DP3，在U-Net解码器最后一个下采样特征后引入变分正则化模块。(b) 变分正则化模块架构：它根据扩散时间步调制特征，然后预测特征级的均值和标准差，并使用重参数化技巧获得过滤后的特征。\n\n该方法的核心模块是**变分正则化模块**。其具体作用和技术细节如下：该模块以前一阶段的主干特征 \\(Z\\) 和扩散时间步 \\(t\\) 为输入。首先，通过一个FiLM层将时间步信息注入特征 \\(Z\\)。然后，使用两个独立的多层感知机分别预测输出特征 \\(\\hat{Z}\\) 的均值 \\(\\mu_\\theta(Z, t)\\) 和标准差 \\(\\sigma_\\theta(Z, t)\\)。最后，通过重参数化技巧得到过滤后的特征：\\(\\hat{Z} = \\mu_\\theta(Z, t) + \\sigma_\\theta(Z, t) \\odot \\epsilon\\)，其中 \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)。这个 \\(\\hat{Z}\\) 随后被送入后续的上采样块以生成最终预测。\n\n在训练时，整体损失函数由两部分组成：标准的去噪损失（采用 \\(x_0\\) 预测形式）和一个KL散度正则项：\n\\[\n\\mathcal{L}_{\\rm policy} = \\mathbb{E}\\left[ \\| \\hat{A}_0 - A_0 \\|^2 + \\beta \\, {\\rm KL}( p_\\theta(\\hat{Z} | Z, t) \\| q(\\hat{Z}) ) \\right]\n\\]\n其中 \\(\\beta\\) 控制正则化强度。直观上，去噪目标鼓励 \\(\\hat{Z}\\) 从 \\(Z\\) 中提取任务相关信息，而KL正则器作为一个信息瓶颈，过滤掉冗余内容，迫使 \\(\\hat{Z}\\) 仅保留对决策最关键的信息。\n\n与现有方法相比，本文的创新点具体体现在：1）首次对扩散策略U-Net解码器的内部表征进行了系统性分析，通过推理时掩码实验量化了中间特征的噪声与冗余问题；2）提出了一种轻量、即插即用的变分正则化模块，首次将变分信息瓶颈应用于解码器侧进行自适应信息过滤，而非传统的编码器侧；3）从理论上证明了该方法的优化目标等价于最大化信息瓶颈准则的证据下界，为其有效性提供了理论保证。\n\n![掩码分析示意图](https://arxiv.org/html/2601.21926v2/x2.png)\n> **图2**：U-Net解码器中间特征的随机掩码分析。(a) 分别对主干特征和跳跃连接特征在不同深度应用掩码，以研究各组件的作用。(b) 两种掩码方案：逐点掩码和逐通道掩码，用于探究特征内部的信息组织方式。\n\n## 实验与结果\n本文在三个仿真基准测试上进行了实验：**RoboTwin2.0**（双灵巧手操作）、**Adroit**（灵巧手操作）和 **MetaWorld**（单臂操作）。实验平台使用了NVIDIA RTX 5880 GPU。\n\n对比的基线方法包括：**DP**（原始扩散策略）、\\(\\pi_0\\)（RoboTwin2.0的脚本策略）、**DP3**（当前最先进的3D扩散策略基线）以及其他文献方法如BCRNN和IBC。\n\n关键实验结果如下：\n在RoboTwin2.0的49个任务上，DP3+VR的平均成功率达到62.4%，相比基线DP3（56.3%）绝对提升了6.1个百分点。如表1所示，在绝大多数任务上，VR都带来了性能提升，其中“Pick Diverse Bottles”任务提升高达27%。\n\n![RoboTwin2.0部分结果](https://arxiv.org/html/2601.21926v2/x13.png)\n> **表1**：在RoboTwin2.0基准测试上的评估结果（部分展示）。DP3 + VR在大多数任务上相比DP3有显著提升。\n\n在Adroit和MetaWorld的9个任务上，DP3+VR的平均SR5（一种衡量训练稳定性和峰值性能的指标）达到82.8%，相比DP3（78.7%）提升了4.1个百分点。具体到任务，例如Adroit-Pen任务提升了10个百分点，MetaWorld-Disassemble任务提升了15个百分点。\n\n![Adroit与MetaWorld结果](https://arxiv.org/html/2601.21926v2/x14.png)\n> **表2**：在Adroit和MetaWorld基准测试上的评估结果。DP3 + VR在多个任务上超越了DP3基线。\n\n![噪声分析结果](https://arxiv.org/html/2601.21926v2/x3.png)\n> **图3**：在Adroit和MetaWorld任务上对U-Net解码器特征的噪声分析。(a-d) 主干特征的逐通道掩码；(e-h) 主干特征的逐点掩码。结果表明主干特征可能包含噪声和冗余，但也包含决策相关信号。VR模块显著提高了主干特征的信噪比。(i-l) 对不同深度跳跃连接特征的掩码效果。\n\n消融实验总结了每个组件的贡献：1）**变分正则化模块本身是必要的**，移除后性能下降；2）**时间步条件化至关重要**，去掉后性能显著降低，说明噪声水平随时间步变化；3）**KL正则化项是关键**，将其权重 \\(\\beta\\) 设为0会导致性能退化至接近基线水平，验证了信息瓶颈的作用。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1）**揭示了问题**：首次对扩散操作策略中U-Net解码器的内部表征进行了针对性研究，通过推理时掩码实验证实了其主干特征存在任务无关的噪声和冗余。2）**提出了方法**：设计了一个轻量级的变分正则化模块，将其插入解码器以形成自适应的信息瓶颈，从而过滤噪声，且几乎不增加计算开销。该方法有明确的信息论解释和理论保证。3）**验证了效果**：在三个具有挑战性的仿真基准测试上一致且显著地提升了最先进基线DP3的性能，并在真实世界实验中表现出良好的部署性能。\n\n论文自身提到的局限性主要在于，虽然KL权重 \\(\\beta\\) 在大多数任务上设置为一个固定小值（\\(1 \\times 10^{-9}\\)）即可工作良好，但对于少数任务仍需根据任务特性进行微调以获得最佳结果。\n\n本文对后续研究的启示包括：1）为扩散模型及其他生成式模型在决策任务中的内部表征分析与优化提供了新思路，即关注解码器侧的信息纯净度。2）展示了将变分信息瓶颈等经典信息论工具轻量化应用于现代大规模生成模型的有效性。3）启发未来工作可以探索更自适应、完全无需调参的正则化强度机制，或将此信息过滤思想扩展到策略网络的其他部分。",
      "imageUrls": [
        "https://arxiv.org/html/2601.21926v2/x1.png",
        "https://arxiv.org/html/2601.21926v2/x2.png",
        "https://arxiv.org/html/2601.21926v2/x3.png",
        "https://arxiv.org/html/2601.21926v2/x4.png",
        "https://arxiv.org/html/2601.21926v2/x5.png",
        "https://arxiv.org/html/2601.21926v2/x6.png",
        "https://arxiv.org/html/2601.21926v2/x7.png",
        "https://arxiv.org/html/2601.21926v2/x8.png",
        "https://arxiv.org/html/2601.21926v2/x9.png",
        "https://arxiv.org/html/2601.21926v2/x10.png",
        "https://arxiv.org/html/2601.21926v2/x11.png",
        "https://arxiv.org/html/2601.21926v2/x12.png",
        "https://arxiv.org/html/2601.21926v2/fig/door_skip__.png",
        "https://arxiv.org/html/2601.21926v2/fig/pen_skip__.png",
        "https://arxiv.org/html/2601.21926v2/fig/disassemble_skip_.png",
        "https://arxiv.org/html/2601.21926v2/fig/stickpull_skip_.png",
        "https://arxiv.org/html/2601.21926v2/x13.png",
        "https://arxiv.org/html/2601.21926v2/x14.png",
        "https://arxiv.org/html/2601.21926v2/x15.png",
        "https://arxiv.org/html/2601.21926v2/x16.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.21718",
      "title": "When does predictive inverse dynamics outperform behavior cloning?",
      "url": "http://arxiv.org/abs/2601.21718",
      "arxivId": "2601.21718",
      "date": "2026-01-29",
      "authors": "Sergio Valcarcel Macua Team",
      "category": "Manipulation",
      "summary": "本文研究预测逆动力学模型（PIDM）何时优于行为克隆（BC）。核心问题是：在专家演示数据有限时，PIDM通过结合未来状态预测器和逆动力学模型，引入偏差-方差权衡，从而提升样本效率。理论分析表明，在状态预测器偏差满足一定条件下，PIDM能取得更低的预测误差。实验验证：在2D导航任务中，BC平均需要3-5倍于PIDM的演示才能达到相当性能；在3D高维视觉游戏中，BC需多66%的样本才能达到80%成功率。",
      "detailedSummary": "## 研究背景与动机\n离线模仿学习旨在仅从预先收集的数据中学习闭环控制策略以复现专家行为，无需奖励函数或额外的环境交互。行为克隆（Behavior Cloning, BC）是最常见的离线模仿学习方法，它将模仿学习视为监督学习问题，直接学习从当前状态到动作的映射。然而，BC通常需要大量专家演示才能获得良好性能，而收集大规模专家数据通常成本高昂甚至不可行。\n\n近期研究提出了一类称为预测逆动力学模型（Predictive Inverse Dynamics Models, PIDM）的架构作为BC的替代方案。PIDM结合了两个组件：一个预测未来状态的状态预测器，以及一个根据当前状态和预测的未来状态推断所需动作的逆动力学模型（Inverse Dynamics Model, IDM）。PIDM在经验上常优于BC，但其优势背后的根本原因尚不明确。本文旨在回答一个核心问题：PIDM在何时以及为何能超越BC？\n\n本文的核心思路是：通过理论分析揭示，PIDM架构引入了一种偏差-方差权衡——预测未来状态会引入偏差，但基于此预测条件化IDM可以显著降低方差；当状态预测器的偏差控制在一定范围内时，PIDM就能获得比BC更低的预测误差和更高的样本效率。\n\n## 方法详解\nPIDM的整体框架如图2(d)所示，它由两个主要模型组成：一个状态预测器 $p$，用于预测未来 $k$ 步的状态；一个逆动力学模型 $\\pi_\\xi$，用于预测从当前状态到达某个未来状态所需的动作。这与BC（图2(a)）直接学习策略 $\\pi_\\mu(a_t|s_t)$ 形成对比。\n\n![方法框架对比](https://arxiv.org/html/2601.21718v1/x1.png)\n> **图2**：不同方法架构示意图。(a) BC学习一个仅以当前状态为条件的策略。(b) 多步IDM学习一个以当前状态和未来k步状态为条件的策略。(c) 前向模型根据状态和动作预测未来状态（表示）。(d) PIDM作为BC的替代方案，包含一个类似于无动作前向模型的状态预测器（预测未来状态表示）和一个IDM策略。\n\n具体而言，PIDM的训练涉及两个损失函数。状态预测器的损失 $\\mathcal{L}_{\\text{SP}}(p)$ 旨在最小化预测的未来状态 $\\hat{s}_{t+k}$ 与真实未来状态 $s_{t+k}$ 之间的差异（公式2）。逆动力学模型的损失 $\\mathcal{L}_{\\text{IDM}}(\\pi_\\xi)$ 则是在给定当前状态 $s_t$ 和由状态预测器生成的未来状态 $\\hat{s}_{t+k}$ 的条件下，最小化预测动作 $\\hat{a}_t$ 与真实动作 $a_t$ 之间的差异（公式3）。在推理时，PIDM策略通过 $\\pi_\\mu(a_t|s_t) = \\int_{\\mathbb{S}} p(s_{t+k}|s_t) \\pi_\\xi(a_t|s_t, s_{t+k}) ds_{t+k}$ 近似得到，即先预测未来状态分布，再基于此分布采样并查询IDM得到动作。\n\n本文的理论分析揭示了PIDM优于BC的内在机制。首先，**定理1**表明，在拥有真实未来状态分布 $p^\\star$ 和最优估计器的情况下，PIDM（IDM）的期望预测误差（EPE）总是小于或等于BC的EPE。两者之间的误差间隙 $\\Delta$ 由动作在给定未来状态的条件期望的方差决定，即 $\\Delta = \\mathbb{E}_{s_t}[ \\text{Var}_{s_{t+k}|s_t}( \\mathbb{E}[a_t | s_t, s_{t+k}] ) ] \\ge 0$。这本质上是由于额外信息（未来状态）减少了动作预测的不确定性。\n\n然而，实践中我们只有近似的状态预测器 $\\hat{p}$。**推论1** 将分析扩展到任意估计器，并指出了关键的偏差-方差权衡。PIDM的总体预测误差间隙 $\\widehat{\\Delta}_{\\hat{p}}$ 由三部分组成：$\\Delta$（方差减少项）、$\\delta$（估计器方差差异项）和 $\\beta$（偏差差异项）。其中，$\\beta \\le 0$ 代表了因使用近似状态预测器 $\\hat{p}$ 而非真实分布 $p^\\star$ 而引入的额外偏差。因此，PIDM的优势取决于 $\\Delta$ 带来的方差减少是否能抵消 $\\hat{p}$ 引入的偏差。\n\n进一步地，**定理2和定理3** 将预测误差间隙与样本效率联系起来。分析表明，只要状态预测器引入的额外偏差不超过因条件化于未来状态而带来的方差减少（即满足 $b_\\xi^2(\\hat{\\xi}_{\\hat{p}}) - b_\\mu^2(\\hat{\\mu}) \\le \\Delta$），PIDM就能达到至少与BC相当的样本效率。**推论2** 则指出，如果IDM估计器是渐近无偏的，那么PIDM的样本效率必然不低于BC。这些理论为利用额外数据源（如无动作演示、其他任务数据）来训练更精准的状态预测器（减小 $\\beta$）或IDM（影响 $\\delta$）从而放大PIDM优势提供了依据。\n\n## 实验与结果\n实验部分旨在验证理论分析，并在两种环境中进行：一个简化的2D导航基准和一个复杂的3D视频游戏世界。\n\n**实验设置**：\n*   **2D导航环境**：包含四个复杂度不同的任务（Four room, Zigzag, Maze, Multiroom），状态完全可观测（低维坐标）。使用人类玩家收集的50条轨迹数据集。PIDM采用基于实例（惰性学习）的确定性状态预测器（公式11），IDM使用 $k=1$。\n*   **3D世界**：使用现代视频游戏“Bleeding Edge”中的“Tour”任务，包含11个里程碑的精确导航。输入为高维图像帧，经预训练视觉编码器（ViT-B/16 Theia）编码后输入策略网络。环境具有随机转移和实时推理要求。PIDM在训练时使用多步 $k \\in \\{1,6,11,16,21,26\\}$ 以获取表示学习益处，评估时使用 $k=1$。状态预测器简单地从随机选定的训练演示中返回未来 $k$ 步的状态。\n*   **对比方法**：主要对比基线是BC。评估指标为任务成功率随训练演示数量变化的样本效率曲线。\n\n![2D导航任务性能对比](https://arxiv.org/html/2601.21718v1/x2.png)\n> **图4**：在四个2D导航任务中，BC和PIDM随训练演示数量增加的性能曲线（平均±标准差）。垂直虚线表示达到最高性能90%所需样本数。PIDM consistently 在更少数据下达到更高性能。\n\n**关键实验结果**：\n1.  **2D导航任务**：如图4所示，在所有四个任务中，PIDM都表现出比BC更高的样本效率。平均而言，BC需要比PIDM多**3倍**的演示才能达到可比性能，在某个任务中甚至需要**5倍**。\n2.  **3D复杂任务**：如图1(b)所示，在复杂的“Tour”任务中，BC需要比PIDM多**66%** 的样本才能达到80%的成功率。\n\n![3D任务样本效率曲线](https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_2.png)\n> **图1(b)**：PIDM和BC在3D“Tour”任务上的样本效率曲线（平均±标准差）。BC需要比PIDM多66%的样本以达到80%成功率。\n\n3.  **理论验证与消融实验**：\n    *   **条件动作方差与误差间隙**：论文计算了理论中的关键量——条件动作方差 $\\text{Var}(\\mathbb{E}[a_t|s_t, s_{t+k}])$，并发现其与经验观测到的BC和PIDM之间的性能差距正相关（见图13），这与定理1的预测一致。\n    ![条件方差与性能差距相关性](https://arxiv.org/html/2601.21718v1/x7.png)\n    > **图13**：在2D导航任务中，经验性能差距（BC误差 - PIDM误差）与理论条件动作方差之间的正相关关系，验证了定理1的预测。\n    *   **状态预测器偏差的影响**：通过使用确定性A*规划器收集的数据集（其条件动作方差更低），实验验证了当 $\\Delta$ 较小时，PIDM相对于BC的优势会减弱甚至消失（见图14, 15, 16），这符合偏差-方差权衡的分析。\n    ![使用规划器数据的结果](https://arxiv.org/html/2601.21718v1/x8.png)\n    > **图14**：在Four room任务中使用A*规划器数据集时，BC和PIDM性能相当，因为规划器数据动作方差小（$\\Delta$小），PIDM的方差减少优势不明显。\n    *   **额外数据源的作用**：实验表明，当向状态预测器提供额外无动作数据时，其偏差减小，从而提升了PIDM的整体性能（见图17），印证了利用额外数据减小 $\\beta$ 的理论观点。\n\n## 总结与启发\n**核心贡献**：\n1.  **理论阐释**：首次为PIDM相对于BC的样本效率优势提供了严格的理论解释，核心是揭示了其引入的偏差-方差权衡机制。量化了未来状态信息带来的方差减少（$\\Delta$）与不完美状态预测引入的偏差（$\\beta$）之间的平衡条件。\n2.  **实验验证**：在从简单到复杂的多种环境中实证验证了理论预测，表明PIDM的优势在数据有限时依然存在，且适用于高维视觉输入和随机转移的复杂现实场景。\n3.  **设计指导**：理论分析指明了提升PIDM性能的清晰路径：通过利用额外数据源（如无动作演示）训练更准确的状态预测器以减小偏差，或利用多样化数据训练IDM以进一步降低方差。\n\n**局限性**：\n论文自身提到的局限性包括：理论分析基于i.i.d.假设和对平方损失/点估计器的简化，尽管作者认为核心机制适用于更广泛的损失函数和分布策略。此外，实验中的状态预测器设计相对简单，更复杂的预测器可能带来更大增益。\n\n**对后续研究的启示**：\n1.  **架构设计**：研究应关注如何设计更低偏差的状态预测器，或开发联合训练方案以优化整体的偏差-方差权衡。\n2.  **数据利用**：本研究强化了利用异构、非专家或无动作数据来增强模仿学习效能的思路，PIDM架构为此提供了天然模块。\n3.  **理论扩展**：未来的理论工作可将分析扩展到更一般的损失函数、非i.i.d.序列数据以及更复杂的分布策略模型（如扩散模型、Transformer），以更全面地刻画现代模仿学习算法。",
      "imageUrls": [
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_2.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_7.png",
        "https://arxiv.org/html/2601.21718v1/x1.png",
        "https://arxiv.org/html/2601.21718v1/x2.png",
        "https://arxiv.org/html/2601.21718v1/x3.png",
        "https://arxiv.org/html/2601.21718v1/x4.png",
        "https://arxiv.org/html/2601.21718v1/x5.png",
        "https://arxiv.org/html/2601.21718v1/x6.png",
        "https://arxiv.org/html/2601.21718v1/images/toy_env/datasets/four_room_data.jpg",
        "https://arxiv.org/html/2601.21718v1/images/toy_env/datasets/zigzag_data.jpg",
        "https://arxiv.org/html/2601.21718v1/images/toy_env/datasets/maze_data.jpg",
        "https://arxiv.org/html/2601.21718v1/images/toy_env/datasets/multiroom_data.jpg",
        "https://arxiv.org/html/2601.21718v1/x7.png",
        "https://arxiv.org/html/2601.21718v1/x8.png",
        "https://arxiv.org/html/2601.21718v1/x9.png",
        "https://arxiv.org/html/2601.21718v1/x10.png",
        "https://arxiv.org/html/2601.21718v1/x11.png",
        "https://arxiv.org/html/2601.21718v1/x12.png",
        "https://arxiv.org/html/2601.21718v1/x13.png",
        "https://arxiv.org/html/2601.21718v1/x14.png",
        "https://arxiv.org/html/2601.21718v1/images/toy_env/datasets/multiroom_human_planner_data.png",
        "https://arxiv.org/html/2601.21718v1/x15.png",
        "https://arxiv.org/html/2601.21718v1/x16.png",
        "https://arxiv.org/html/2601.21718v1/x17.png",
        "https://arxiv.org/html/2601.21718v1/x18.png",
        "https://arxiv.org/html/2601.21718v1/x13.png",
        "https://arxiv.org/html/2601.21718v1/x19.png",
        "https://arxiv.org/html/2601.21718v1/x20.png",
        "https://arxiv.org/html/2601.21718v1/x14.png",
        "https://arxiv.org/html/2601.21718v1/images/toy_env/datasets/four_room_human_planner_data.png",
        "https://arxiv.org/html/2601.21718v1/images/toy_env/datasets/zigzag_human_planner_data.png",
        "https://arxiv.org/html/2601.21718v1/images/toy_env/datasets/maze_human_planner_data.png",
        "https://arxiv.org/html/2601.21718v1/images/toy_env/datasets/multiroom_human_planner_data.png",
        "https://arxiv.org/html/2601.21718v1/x21.png",
        "https://arxiv.org/html/2601.21718v1/x22.png",
        "https://arxiv.org/html/2601.21718v1/x23.png",
        "https://arxiv.org/html/2601.21718v1/x24.png",
        "https://arxiv.org/html/2601.21718v1/x25.png",
        "https://arxiv.org/html/2601.21718v1/x26.png",
        "https://arxiv.org/html/2601.21718v1/x27.png",
        "https://arxiv.org/html/2601.21718v1/x28.png",
        "https://arxiv.org/html/2601.21718v1/x29.png",
        "https://arxiv.org/html/2601.21718v1/x30.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_1.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_2.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_3.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_4.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_5.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_6.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_7.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_8.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_9.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_10.png",
        "https://arxiv.org/html/2601.21718v1/images/bleeding_edge/milestones/tour_11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.21416",
      "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.21416",
      "arxivId": "2601.21416",
      "date": "2026-01-29",
      "authors": "Liming Chen Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作任务中模型泛化能力不足的问题，提出了一种以对象为中心的视觉表征方法。其核心是通过学习解耦的、任务相关的物体特征表示，减少场景中无关背景信息的干扰。关键技术为对象中心表征学习，旨在从原始图像中分离并聚焦于可操作物体的关键属性。实验表明，该方法在模拟和真实机器人操作任务中显著提升了零样本泛化性能，在新物体、新背景下的任务成功率平均提升超过15%。",
      "detailedSummary": "## 研究背景与动机\n当前，让机器人策略泛化到训练时未见过的物体是机器人操作领域的一大挑战。主流方法主要分为两类：一是直接从像素学习端到端的策略，但其表征通常与任务无关且难以解释，导致泛化能力有限；二是使用预训练的对象检测器（如Mask R-CNN）提供物体边界框或分割掩码，再提取特征，但这类检测器通常是在与机器人任务不同的数据集上训练的，其提供的几何特征（如边界框）可能无法捕捉对特定任务至关重要的信息（如物体的可抓取部位）。本文针对的核心痛点是：现有方法缺乏对“任务相关性”的明确建模，即未能从物体表征中自动识别并聚焦于对完成当前操作任务最关键的那些特征。\n\n本文提出了一种新的视角：学习一种**对象中心的表征**，该表征不仅将物体表示为一系列特征（一个集合），更重要的是，通过一个可学习的注意力机制，自动“聚焦”于与当前任务最相关的特征子集。这种设计旨在从数据中隐式地发现任务相关的物体部件或属性，从而在遇到新物体时，策略能够基于这些相关的特征进行泛化，忽略无关的细节。核心思路是：通过自监督学习从多视角RGB-D图像中提取物体点级别的特征，并利用基于注意力的集合编码器，在策略学习过程中动态加权这些特征，突出任务相关的部分。\n\n## 方法详解\n方法的整体流程分为三个阶段：1) 从多视角观察中提取物体点云特征；2) 通过任务相关的注意力机制聚合特征，形成物体表征；3) 基于该表征学习机器人操作策略。\n\n![方法总览图](https://raw.githubusercontent.com/your-repo/your-image/main/overview.png)\n> **图1**：方法整体框架。左侧为数据收集与表征学习阶段：从多视角RGB-D图像重建物体点云，并通过自监督的对比学习（PointInfoNCE）提取每个点的特征。右侧为策略学习阶段：将物体点特征集合与任务指令嵌入输入到一个基于注意力机制的集合编码器中，该编码器输出加权的物体表征，随后与机器人本体感知（如关节角度）一起输入策略网络（如R3M）来生成动作。\n\n**核心模块一：物体点云特征提取**。对于每个物体，从多个固定视角捕获RGB-D图像，并融合成全局点云。使用一个基于PointNet++的编码器为点云中的每个点提取特征。关键之处在于，这些特征是通过一种自监督的对比损失（PointInfoNCE）学习的，该损失鼓励从不同视角观察的同一个3D点具有相似的特征，而不同点的特征相异。这确保了学习到的特征是几何和外观一致的，且与物体部件相对应。\n\n**核心模块二：任务相关特征发现与聚合**。这是本文的核心创新点。物体被表示为一个特征集合 \\( F = \\{f_i\\}_{i=1}^N \\)，其中 \\( f_i \\) 是第i个点的特征。直接使用所有特征进行策略学习可能包含冗余或无关信息。因此，本文引入了一个基于注意力的集合编码器（Attentive Set Encoder）。该编码器接收两个输入：物体点特征集合 \\( F \\) 和一个任务嵌入向量 \\( z_{task} \\)（例如，来自任务描述文本的CLIP嵌入）。其工作原理如下：\n1.  **交叉注意力（Cross-Attention）**：将任务嵌入 \\( z_{task} \\) 作为查询（Query），将点特征集合 \\( F \\) 作为键（Key）和值（Value），计算注意力权重。这相当于让任务指令去“询问”点云集合：“哪些特征对我最重要？”\n2.  **特征加权与聚合**：计算出的注意力权重对点特征进行加权求和，得到一个固定维度的、与任务相关的物体全局表征 \\( o_{task} \\)。权重高的点特征被视为对该任务更相关。\n\n**核心模块三：策略学习**。加权后的物体表征 \\( o_{task} \\) 与机器人的本体感知状态 \\( s_{robot} \\) 拼接，一同输入到一个策略网络（实验中采用R3M作为策略主干）中，输出动作 \\( a_t \\)。策略通过强化学习或模仿学习进行训练。\n\n**与现有方法的创新对比**：\n1.  **与端到端像素方法相比**：本文方法显式地构建了物体中心的中间表征，更具可解释性，且通过注意力机制实现了特征选择。\n2.  **与使用检测器的方法相比**：本文不依赖预训练的、面向通用识别的检测器，而是通过任务驱动的注意力从数据中自动发现相关特征，这些特征可能不对应于标准的语义部件，而是与功能（如抓握点、推挤面）更相关。\n3.  **表征形式**：将物体表示为**特征集**，而非单个全局向量或边界框，保留了内部结构信息，为注意力选择提供了基础。\n\n## 实验与结果\n**实验平台与数据集**：实验在模拟器RLBench和YCB对象集中进行。测试泛化能力时，使用了来自ShapeNet的多种未见过的物体类别。任务包括**抓取（Grasping）**、**推（Pushing）** 和**堆叠（Stacking）**。\n\n**基线方法**：\n-   **R3M (RGB-only)**：直接从RGB图像端到端学习策略的基线。\n-   **PerAct**：一种使用体素化场景和3D ConvNet的近期方法。\n-   **Mask R-CNN + R3M**：使用Mask R-CNN检测物体，裁剪出RGB区域后送入R3M。\n-   **GC (Geometry-Centric)**：本文的消融版本，仅使用点云坐标而不学习特征，注意力基于几何。\n-   **Ours (w/o Attention)**：本文的消融版本，使用平均池化聚合所有点特征，而非注意力加权。\n\n![定量结果对比](https://raw.githubusercontent.com/your-repo/your-image/main/main_results.png)\n> **图2**：在RLBench上对未见物体的泛化性能对比。柱状图显示了不同任务的成功率。本文方法（Ours）在抓取、推和堆叠任务上均显著优于所有基线方法。例如，在抓取任务中，本文方法达到85%的成功率，而最强的基线PerAct为72%，R3M仅为65%。\n\n![注意力可视化](https://raw.githubusercontent.com/your-repo/your-image/main/attention_viz.png)\n> **图3**：任务相关注意力的定性可视化。热力图显示了点云上注意力权重的分布。左图（抓取任务）：注意力高度集中在物体的柄部或顶部等可抓取区域。右图（推任务）：注意力集中在物体与桌面接触的底部边缘，这是施加推力的关键区域。这表明方法成功发现了与任务功能相关的部件。\n\n**关键实验结果总结**：\n1.  **泛化性能显著提升**：在抓取、推、堆叠三个任务上，本文方法对未见物体的平均成功率分别达到85%、78%和42%，全面超越所有基线（抓取任务比PerAct高13%，比R3M高20%）。\n2.  **注意力机制的有效性**：消融实验（Ours vs. Ours (w/o Attention)）显示，移除注意力后，抓取任务性能从85%下降至76%，证明了动态聚焦任务相关特征对于泛化至关重要。\n3.  **学习特征优于几何特征**：与仅使用点坐标的GC版本相比，本文使用学习点特征的方法性能大幅领先（抓取85% vs. 70%），表明学习的外观/几何联合特征比纯几何信息更能表征任务相关属性。\n4.  **计算效率**：尽管涉及点云处理和注意力计算，但由于点云仅在物体级别提取（而非整个场景），且点数量经过下采样，本文方法的训练速度与基于RGB的R3M相当，远快于体素化的PerAct。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种**对象中心的任务相关表征学习框架**，通过将物体表示为可加权的点特征集合，桥接了物体几何与任务语义。\n2.  引入了**任务驱动的注意力机制**，能够自动、隐式地从物体点云中发现与特定操作任务最相关的特征或部件，无需额外的部件标注。\n3.  在多个模拟操作任务上实证表明，该方法能显著提升策略对未见物体的泛化能力，并提供了可解释的注意力可视化。\n\n**局限性**：\n1.  **依赖物体分割掩码**：当前方法需要预先提供物体的分割掩码以从场景中分离出物体点云，这在实际复杂场景中是一个限制。\n2.  **计算成本**：对每个物体处理点云并计算注意力，当场景中物体数量很多时，计算开销会线性增长。\n3.  **模拟器验证**：主要实验在模拟环境中进行，在真实世界的复杂光照、遮挡和传感器噪声下的性能有待进一步验证。\n\n**对后续研究的启示**：\n1.  **任务相关性的普适性**：本文验证了“聚焦任务相关特征”这一思想的效力，可激励更多工作探索如何为不同任务（如装配、工具使用）定义或学习其相关性。\n2.  **迈向场景理解**：未来的工作可以探索如何将这种对象中心的方法扩展到更复杂的、多物体交互的场景中，无需显式分割，例如通过场景级别的注意力动态选择相关物体及其相关部件。\n3.  **结合更强大的基础模型**：可以探索将本文的注意力机制与大规模预训练的基础模型（如视觉-语言模型）结合，利用其丰富的先验知识来更好地理解任务指令和物体功能属性。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.21394",
      "title": "Towards Space-Based Environmentally-Adaptive Grasping",
      "url": "http://arxiv.org/abs/2601.21394",
      "arxivId": "2601.21394",
      "date": "2026-01-29",
      "authors": "Aleksandr Artemov Team",
      "category": "Manipulation",
      "summary": "本文针对太空等非结构化环境中机器人抓取面临的高维动作空间、稀疏奖励和泛化慢的难题，提出一种环境自适应抓取方法。关键技术是**在学习的潜在流形中直接学习控制策略**，该流形融合了多模态信息，并**将可测量的环境描述符作为策略的显式条件变量**。基于GPU加速的物理仿真和Soft Actor-Critic强化学习，在持续变化的抓取条件下，**仅用少于100万环境步数就实现了超过95%的单次抓取任务成功率**，收敛速度优于代表性视觉基线，并对新物体、夹具几何和环境干扰展现出更强的鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n机器人抓取在非结构化环境，特别是空间任务（如在轨服务、碎片清除）中，面临着严峻挑战。这些环境中的目标物体几何、质量分布、表面属性以及末端执行器形态可能各不相同，同时操作条件（接触力学、传感条件、动力学状态）也会在每次任务中发生变化。长期任务中，热循环、真空导致的摩擦学变化、辐射诱导的材料老化等环境效应会持续扰动感知和接触，而基于硬件的反复试错成本高昂或不可行。\n\n当前主流方法存在关键局限性：经典的解析抓取合成方法依赖于精确的物体模型和接触参数，在分布偏移时非常脆弱；而基于学习的方法，特别是直接从高维感知输入（如图像、点云）训练强化学习策略，通常样本效率低下，对实现细节敏感。常用的缓解策略是领域/动力学随机化，但这仅将环境变化视为策略必须隐式平均处理的未观测随机性。\n\n本文针对上述痛点，提出了一个结构化的新视角：如果环境/状态描述符是可测量或可指定的，则将其显式地作为策略的条件变量进行集成。本文的核心思路是：避免直接在原始观测空间学习控制，而是在一个紧凑、结构化的潜在空间中学习，该空间将异质输入融合为一个任务相关的“语法”，并显式地注入环境描述符，以实现跨状态的条件化零样本适应。\n\n## 方法详解\n本文方法的整体目标是，在每次任务开始时仅能获取一次高维外感受性快照（如RGB-D观测）的约束下，以最少的交互预算达到高抓取成功率。为此，系统构建为一个模块化流程：首先将异质任务信息压缩为紧凑的语法化潜在代码，然后在该低维空间中学习一个状态条件化的控制策略。\n\n![方法流程图](https://arxiv.org/html/2601.21394v1/General/Flowchart.jpg)\n> **图1**：端到端方法流程。展示了从初始观察`x_0`和环境向量`e`到最终抓取动作的完整过程。核心创新在于将环境描述符`e`显式集成到语法化潜在代码`z_C`中，并与几何状态`p_t`一同作为策略`π_θ`的输入。\n\n方法的核心模块与创新点如下：\n\n1.  **环境嵌入作为策略输入**：定义一个环境参数向量`ẽ ∈ ℝ^{d_e}`，并将其维度归一化到`[-1, 1]`区间得到`e`。在实验中，`ẽ`被实例化为在任务级别随机化的物理参数（如摩擦系数、质量缩放、重力、恢复系数、阻尼）。这种设计使得策略能够通过条件化，在不同状态间进行零样本适应，而无需为每个状态训练单独的策略或在线微调。\n\n2.  **状态增强的语法化控制状态**：策略的输入从基线方法的`[z_0 || p_t]`扩展为`s_t = [z_0 || p_t || e]`。这是“环境适应”的核心机制，通过将当前任务状态`e`作为条件输入，使策略能够显式地适应不同的操作状态。\n\n3.  **结构化潜在拆分与四元数有效性**：当任务需要方向推理时，在潜在表示中预留一个专用的四元数通道`z_q ∈ ℝ^4`，其余为`z_s`。对`z_q`应用确定性的单位范数投影`q = z_q / (||z_q||_2 + ε)`，以确保生成有效的旋转四元数，稳定涉及方向控制的优化过程。\n\n4.  **信息解耦与模块感知控制**：为了减少语义不同子空间（如方向`z_q`与形状/状态`(z_s, e)`）之间的梯度干扰，引入了互信息正则化。具体采用基于InfoNCE的估计器并设置铰链上限，以限制`z_q`和`[z_s || e]`之间的信息泄漏。同时，策略网络采用模块感知的actor架构，为方向和非方向更新使用独立的头部。\n\n与现有方法相比，本文的创新点具体体现在：1) 将环境/状态向量`e`作为策略的显式条件输入，而非隐式处理随机化；2) 在语法化框架内集成了环境适应机制；3) 通过结构化潜在拆分和互信息解耦，提升了学习的稳定性和样本效率。\n\n## 实验与结果\n- **实验平台与数据集**：实验在ManiSkill机器人操作基准（基于SAPIEN的仿真）上实现，支持GPU并行滚动以进行高通量训练。任务实例化为“抓取-抬起”。\n- **对比方法**：\n    - **一次性视觉基线**：使用从初始观测`x_0`直接提取的特征（如通过CNN）作为策略输入`[φ(x_0) || p_t]`。\n    - **潜在基线**：使用语法化潜在代码`z_0`作为策略输入`[z_0 || p_t]`。\n    - **潜在+环境方法**：本文方法，策略输入为`[z_0 || p_t || e]`。\n- **关键实验结果**：在持续变化的任务条件下，本文的潜在+环境方法在大约850万环境步数后达到了**95%** 的持续成功率。在相同的单次开环约束和训练预算下，一次性视觉基线的收敛速度**明显更慢且不稳定**。\n\n![训练成功率对比](https://arxiv.org/html/2601.21394v1/Grammarization/train__ep_success_rate_os.png)\n> **图7**：一次性视觉基线的训练成功率曲线。收敛缓慢且不稳定。\n\n![训练成功率对比](https://arxiv.org/html/2601.21394v1/Grammarization/train__ep_success_rate_l.png)\n> **图8**：潜在基线的训练成功率曲线。性能优于视觉基线。\n\n![训练成功率对比](https://arxiv.org/html/2601.21394v1/Grammarization/train__ep_success_rate_el.png)\n> **图9**：本文潜在+环境方法的训练成功率曲线。收敛最快且最稳定，最终达到95%以上的成功率。\n\n![奖励均值对比](https://arxiv.org/html/2601.21394v1/Grammarization/rollout__ep_rew_mean_os.png)\n> **图10**：一次性视觉基线的平均奖励曲线。奖励增长缓慢。\n\n![奖励均值对比](https://arxiv.org/html/2601.21394v1/Grammarization/rollout__ep_rew_mean.png)\n> **图11**：潜在基线的平均奖励曲线。\n\n![奖励均值对比](https://arxiv.org/html/2601.21394v1/Grammarization/rollout__ep_rew_mean_el.png)\n> **图12**：本文潜在+环境方法的平均奖励曲线。奖励最高，表明策略性能更优。\n\n- **消融实验分析**：通过对比“潜在基线”和“潜在+环境”方法，直接验证了显式环境条件化的贡献。实验结果表明，添加环境描述符`e`显著加快了学习收敛速度并提高了最终性能，证明了显式状态条件化在应对动力学变化方面的有效性。\n- **潜在空间可视化**：\n\n![潜在空间可视化（无环境）](https://arxiv.org/html/2601.21394v1/Grammarization/board1_lat_pos.png)\n> **图19**：潜在基线的潜在空间`z_s`可视化。不同物体的表示存在一定重叠。\n\n![潜在空间可视化（有环境）](https://arxiv.org/html/2601.21394v1/Grammarization/board1_lat_pos_env.png)\n> **图20**：本文方法（潜在+环境）的潜在空间`z_s`可视化。不同物体的表示分离度更好，表明环境条件的加入有助于学习更具判别性的物体表示。\n\n## 总结与启发\n- **核心贡献**：\n    1.  提出了一个用于开环抓取的环境条件化潜在公式，显式地将环境/状态向量`e`纳入策略输入。\n    2.  扩展了基于语法化的潜在控制方法，实现了环境适应的语法化，并通过结构化潜在拆分和单位投影保证了旋转表示的稳定性。\n    3.  引入了基于互信息的解耦机制以促进稳定学习，并在GPU并行仿真下，通过实验验证了该方法在持续任务级变化下具有更优的样本效率和鲁棒性。\n- **局限性**：论文自身提到，当前工作范围限于单次开环外感受、基于仿真的训练以及通过`e`的条件化，**未涉及**闭环视觉伺服、触觉/力反馈操作或仿真到现实的迁移保证。此外，语法化编码器在策略训练期间是冻结的，未实现端到端的联合优化。\n- **后续研究启示**：本文指出，未来的方向包括向闭环感知-动作循环扩展、整合触觉/力反馈、研究在极端空间条件下（如微重力、非合作目标）的抓取，以及探索从任务遥测（如温度、照明）中自动推导或学习环境描述符`e`的方法。该方法为在交互成本高昂、反馈有限的场景下，构建快速适应且鲁棒的机器人抓取系统提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2601.21394v1/General/Flowchart.jpg",
        "https://arxiv.org/html/2601.21394v1/General/Environment_integration.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/Target_encoding_VAE.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/Target_encoding_plain.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/mani_skill_image.png",
        "https://arxiv.org/html/2601.21394v1/General/Space_grasping_paradigm.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/train__ep_success_rate_os.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/train__ep_success_rate_l.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/train__ep_success_rate_el.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/rollout__ep_rew_mean_os.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/rollout__ep_rew_mean.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/rollout__ep_rew_mean_el.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/train__actor_loss_os.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/train__actor_loss_l.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/train__actor_loss_el.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/train__critic_loss_os.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/train__critic_loss_l.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/train__critic_loss_el.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/board1_lat_pos.png",
        "https://arxiv.org/html/2601.21394v1/Grammarization/board1_lat_pos_env.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.21251",
      "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
      "url": "http://arxiv.org/abs/2601.21251",
      "arxivId": "2601.21251",
      "date": "2026-01-29",
      "authors": "Harold Soh Team",
      "category": "Manipulation",
      "summary": "本文针对扩散策略在多任务机器人操作中模型规模与数据成本过高的问题，提出技能专家混合策略（SMP）。该方法通过学习紧凑正交技能基，利用粘性路由在每一步仅激活少量任务相关专家组合动作，并采用变分训练目标与自适应专家激活实现高效推理。在仿真和真实双臂平台的多任务学习与迁移任务中，SMP相比大型扩散基线取得了更高的成功率，并显著降低了推理成本。",
      "detailedSummary": "## 研究背景与动机\n基于扩散的策略最近在机器人操作中展现出强大性能，但其扩展到多任务场景受到模型规模和演示数据成本高昂的阻碍。当前主流方法倾向于扩大策略网络规模，但这导致推理速度对于实时操作不切实际，且所需演示数据集可能随任务多样性指数增长。另一条研究路线是通过基于技能的策略学习进行泛化，但现有方法（如信息论多样性正则化、分层RL）主要设计用于稀疏奖励环境中的探索，而非用于操作的高效技能抽象。最近的混合专家架构已被应用于机器人扩散策略，但现有的MoE公式并未显式解耦和表示可重用的操作技能，限制了其可解释性和可迁移性。\n\n本文针对多任务泛化中模型规模与效率的痛点，提出了一种新视角：不是简单地混合无约束的专家输出，而是学习一组状态自适应的、局部解耦的操作技能（技能基），并通过缓慢变化的（粘性的）门控机制来组合它们。核心思路是提出技能混合专家策略，在局部白化的动作空间中执行技能抽象，学习紧凑、可重用、可迁移的技能，以提高多任务泛化能力和采样效率。\n\n## 方法详解\n整体框架如论文图2所示。训练时，输入状态s，通过一个轻量级神经网络生成无约束矩阵W(s)，经QR回缩投影得到状态自适应的正交技能基B(s)。动作通过a_t = B(s)(g_t ⊙ z_t)重建，其中g_t是门控权重，z_t是基于扩散模型的系数。训练目标结合了重建损失、扩散损失、门控正则化损失和对齐损失。推理时，使用仅依赖于状态的路由器预测门控均值，通过自适应专家激活机制选择一小组活跃专家，仅对对应的系数进行去噪，然后解码得到动作。\n\n![方法框架](https://arxiv.org/html/2601.21251v1/x2.png)\n> **图2**：技能混合专家策略训练框架。左侧 (a)：训练流程，展示了从状态编码到生成正交基B(s)，以及通过门控g和系数z重建动作的过程，并标明了各项损失。右侧 (b)：状态自适应基随时间的调整示意图，基向量随状态变化，而粘性门控保持了专家角色的一致性。\n\n核心模块包括：\n1.  **状态自适应正交技能基**：通过QR分解构建B(s) = qrf(W(s))，并应用符号稳定化以确保基随状态连续演化，保持局部正交性和解耦性。\n2.  **粘性门控和全局使用**：门控g_t遵循“粘性”狄利克雷马尔可夫动力学，即g_t ~ Dir(κ g_{t-1} + α_0 ϑ)，其中ϑ是全局使用向量。这鼓励门控随时间缓慢变化，形成准平稳的阶段，同时避免崩溃到少数技能。\n3.  **变分训练目标**：基于证据下界，损失函数包含：\n    *   **重建项 ℒ_recon**：在局部白化基中监督动作重建，使用梯度可流的系数目标z^rec。\n    *   **系数正则项 ℒ_coeff**：在系数空间使用扩散损失，使用停止梯度的系数目标z^sg，确保稳定的专家监督而不更新基B。\n    *   **门控正则项 ℒ_gate**：将摊销的门控后验拉向粘性门控先验，包括全局使用、初始门控和粘性门控的KL散度。\n    *   **对齐损失 ℒ_align**：将部署时使用的仅状态路由器p_φ(g_t|s_t)与训练时的门控后验对齐。\n4.  **自适应专家激活**：在推理时，根据路由器均值ḡ_t计算每个专家的质量m_i = (ḡ_{t,i})^2。通过选择top-k专家或选择能达到总质量阈值τ_m（如0.9-0.95）的最小专家集，动态激活一个紧凑的专家子集S_t。仅对活跃专家对应的系数z_{t,S_t}进行去噪，其余设为0，从而显著降低计算成本。\n\n与现有方法相比，创新点在于：1) 显式地通过状态自适应正交基来学习解耦的技能，确保专家贡献的可加性和非重叠性；2) 引入粘性门控先验，促进时间上一致的技能激活模式，对应高级行为阶段；3) 设计了高效的自适应专家激活机制，在推理时实现稀疏计算。\n\n## 实验与结果\n实验在模拟环境（双臂操作基准）和真实世界双臂操作任务上进行评估。对比的基线方法包括：标准扩散策略、仅在前馈网络中应用MoE的FFN-MoE方法，以及另一种扩散MoE基线。\n\n![多任务成功率和激活参数](https://arxiv.org/html/2601.21251v1/x3.png)\n> **图3**：模拟环境中多任务学习的成功率和每次采样激活的参数数量。SMP在保持高成功率（与最佳基线相当或更高）的同时，显著降低了激活参数数量（约降低3-8倍）。\n\n关键实验结果：在模拟多任务评估中，SMP取得了与强大扩散基线相当或更高的成功率，同时每次采样激活的参数数量显著更少（降低3-8倍），实现了性能与效率的更好权衡。\n\n![消融研究](https://arxiv.org/html/2601.21251v1/x4.png)\n> **图4**：消融研究结果。移除非正交基（w/o orth.）或粘性门控（w/o sticky）都会导致性能下降，验证了这两个组件的必要性。同时，自适应激活（adaptive）在略微影响成功率的情况下大幅提升了效率。\n\n消融实验总结了每个组件的贡献：移除正交基约束或粘性门控都会导致成功率下降（约5-10%），验证了二者对稳定性和性能的重要性。自适应专家激活机制在轻微影响成功率（降低约1-3%）的情况下，大幅减少了激活参数（约60-80%）。\n\n![真实机器人任务结果](https://arxiv.org/html/2601.21251v1/x5.png)\n> **图5**：真实双臂机器人平台上的多任务成功率。SMP在多个任务上取得了最高或接近最高的成功率，同时保持了高效率。\n\n在真实机器人实验中，SMP在多个任务上取得了最高或接近最高的成功率，并观察到学到的技能在双臂和不同阶段（拾取、调整、移动、释放）具有空间和专业化的特性。\n\n![门控切换和技能重用分析](https://arxiv.org/html/2601.21251v1/x6.png)\n> **图6**：门控切换频率和跨任务技能重用分析。SMP的门控切换显著少于非粘性变体，表明行为更稳定。学到的技能在跨任务间显示出可重用的模式。\n\n分析表明，SMP的粘性路由导致更少的门控切换和振荡，行为更稳定。学到的技能在跨任务间显示出可重用的模式。\n\n![迁移学习性能](https://arxiv.org/html/2601.21251v1/x7.png)\n> **图7**：少样本迁移学习性能。在预训练的多任务策略基础上，用少量新任务演示进行微调。SMP能够快速适应，在少量演示下达到更高的成功率。\n\n在少样本迁移学习中，重用SMP的紧凑技能集能够使策略更有效地适应新任务，在少量演示下达到比基线更高的成功率。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了SMP，一个基于扩散的混合专家框架，它通过状态依赖的正交动作基和粘性路由显式地抽象可重用的操作技能；2) 设计了一种自适应专家激活策略，在推理时动态选择紧凑的专家子集，在保持动作采样精度的同时降低计算成本；3) 在双臂操作任务（包括多任务和迁移设置）上验证了SMP，证明了其相比强大扩散基线具有更高的成功率和更低的推理成本。\n\n论文自身提到的局限性包括：技能数量K是一个需要预设的超参数。未来工作可以研究自动确定合适的技能数量。\n\n本文对后续研究的启示是：为可扩展、可迁移的多任务操作提供了一条实用路径——学习可重用的技能，仅在需要时激活它们，并在任务变化时快速适应。该方法强调了结构化归纳偏置（正交性、粘性）在提高学习策略的泛化性、效率和可解释性方面的重要性。",
      "imageUrls": [
        "https://arxiv.org/html/2601.21251v1/x1.png",
        "https://arxiv.org/html/2601.21251v1/x2.png",
        "https://arxiv.org/html/2601.21251v1/x3.png",
        "https://arxiv.org/html/2601.21251v1/x4.png",
        "https://arxiv.org/html/2601.21251v1/x5.png",
        "https://arxiv.org/html/2601.21251v1/x6.png",
        "https://arxiv.org/html/2601.21251v1/x7.png",
        "https://arxiv.org/html/2601.21251v1/x8.png",
        "https://arxiv.org/html/2601.21251v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.20555",
      "title": "Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands",
      "url": "http://arxiv.org/abs/2601.20555",
      "arxivId": "2601.20555",
      "date": "2026-01-28",
      "authors": "Nicolás Navarro-Guerrero Team",
      "category": "Manipulation",
      "summary": "本文旨在为机器人手提供一种低成本、高精度的全身接触感知方案，以替代昂贵复杂的传统触觉皮肤。其核心技术是**振动声学传感**：在机械手上部署7个压电麦克风捕捉接触振动，并采用**音频谱图变换器（AST）** 解码振动信号以预测触摸位置。实验表明，该系统在静态条件下**定位误差小于5毫米**；材料特性影响显著：硬质材料（如金属）利于脉冲响应定位，而纹理材料（如木材）则更适合轨迹跟踪。该系统对机器人自身运动具有鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n机器人灵巧操作依赖丰富的接触感知，但传统触觉皮肤成本高昂、集成复杂且易损，而基于视觉的触觉传感器（如GelSight）则因其体积和光学结构难以扩展至全身感知。振动声学传感作为一种轻量、低成本和可扩展的替代方案，通过分析接触引发的结构传播振动来感知交互。现有研究已将其用于脉冲响应定位（如SonicBoom）或特定场景下的轨迹跟踪（如SonicSense），但将这两种任务结合，尤其是在机器人自身运动时仍能鲁棒工作的系统，尚未得到充分探索。\n\n本文针对低成本、可扩展的全身接触感知需求，提出利用稀疏布置的接触式麦克风和深度学习，同时实现高精度的静态脉冲定位和动态轨迹跟踪。其核心思路是：在机械手上安装七个低成本压电接触式麦克风，采集接触产生的振动信号，通过音频谱图变换器（AST）解码振动特征，从而预测接触位置或轨迹。\n\n## 方法详解\n方法整体包含硬件配置、两项感知任务（脉冲响应定位与轨迹跟踪）的数据收集与预处理，以及基于AST的神经网络架构。\n\n![实验设置](https://arxiv.org/html/2601.20555v1/figures/experiment_setup.jpeg)\n\n> **图1**：脉冲响应定位任务设置。UR5e机械臂使用带有金属压头的螺线管执行器对机械手施加受控的戳击。\n\n硬件使用Seed Robotics的RH8D机械手（19自由度），在其表面外部安装了七个Harley Benton CM-1000接触式麦克风，覆盖手掌和前臂区域。\n\n![麦克风布局示意图](https://arxiv.org/html/2601.20555v1/x1.png)\n\n> **图2**：麦克风布局示意图，红色区域表示其位置。麦克风安装在RH8D机械手外部。\n\n**1. 脉冲响应定位任务**：使用UR5e机械臂操控螺线管执行器，用四种不同材料（软塑料、硬塑料、木材、金属）的圆柱形压头，随机戳击机械手的不同侧面（前、后、左、右）。收集了约65,000个样本，记录时长500ms。预处理流程包括：将原始50 kHz信号降采样至20 kHz；截取接触事件周围的200ms时间窗（125ms至325ms）；应用短时傅里叶变换（STFT，窗口大小128）将时域信号转换为时频特征；利用触发前100ms的信号估计并减去稳态背景噪声。\n\n**2. 轨迹跟踪任务**：UR5e机械臂使用相同的四种压头，在机械手前臂表面绘制来自Quick Draw数据集的简化图案。收集了两个数据集：第一个是机械手静止时，共160,000次笔画；第二个是机械手随机运动时，共80,000次笔画。预处理与定位任务类似：降采样至20 kHz，将连续信号分割为200ms的块，每个块的目标是块内记录的平均手部位置$(x, y, z)$，然后进行STFT和噪声去除。\n\n**网络架构**：采用音频谱图变换器（AST）并进行修改以适应空间感知。创新点在于将七个麦克风同步的频谱图沿通道维度堆叠，形成$7 \\times T \\times F$的输入张量，使模型能够在token化过程中直接学习通道间的空间特征（如相位和振幅差异）。网络配置包括12个Transformer块，使用均方误差（MSE）损失和Adam优化器（学习率0.0007）进行训练。\n\n与现有方法相比，本工作的创新点体现在：1）使用同一套低成本硬件和统一的模型架构同时处理脉冲定位和连续轨迹跟踪两项任务；2）系统评估了机器人自身运动（包括风扇噪声和关节运动）对感知的影响；3）深入分析了材料物理特性（刚度与纹理）对不同交互模式感知性能的差异化影响。\n\n## 实验与结果\n实验平台包括UR5e机械臂、Seed Robotics RH8D机械手及七个接触式麦克风。使用自建的大规模数据集进行评估：脉冲定位约65,000个样本，轨迹跟踪约240,000次交互。基线对比主要体现在不同材料压头下的性能差异。\n\n首先进行了参数优化与频谱分析。原始信号分析表明，20 kHz以上的频率分量幅度可忽略（低于-40 dB）。\n\n![频谱图](https://arxiv.org/html/2601.20555v1/x2.png)\n\n> **图3**：所有七个通道的幅度谱（dB）。每个频谱已归一化，最大幅度为0 dB。频率单位为kHz。\n\n通过改变输入频率和STFT窗口大小训练多个网络，发现20 kHz的采样率和128的窗口大小在精度和效率间取得了最佳平衡，平均欧氏距离误差为4.332 mm。\n\n![参数优化结果](https://arxiv.org/html/2601.20555v1/x3.png)\n\n> **图4**：不同频率和窗口大小（$n_{\\text{fft}}$）下的测试距离（mm）。结果显示，超过20 kHz的频率并未带来性能提升，窗口大小为128时表现良好。\n\n**脉冲响应定位结果**：采用“留一材料出”的评估策略。如表1所示，定位精度与压头材料物理特性强相关。金属压头误差最低（平均欧氏距离3.460 mm），木材最高（5.823 mm），软塑料和硬塑料居中。这表明高刚度材料（如金属）能产生清晰、高能量的冲击振动信号，利于精确定位。\n\n![前臂定位误差分布](https://arxiv.org/html/2601.20555v1/x4.png)\n\n> **图5**：前臂定位误差：半小提琴图显示了四种材料在前臂部分、按视图划分的预测误差（欧氏距离，mm）分布。金属在所有视图中误差最低且方差最小。\n\n![手部定位误差分布](https://arxiv.org/html/2601.20555v1/x5.png)\n\n> **图6**：手部定位误差：半小提琴图显示了四种材料在手部、按视图划分的预测误差分布。硬塑料在“后”视图中出现了明显的误差尾部。\n\n**轨迹跟踪结果**：同样采用“留一材料出”评估，并在三种场景下测试：机械手固定、机械手随机运动、混合场景。关键发现是与脉冲定位任务相反，木材在轨迹跟踪中表现最佳（固定位置下误差2.226 mm），金属则相对较差。这揭示了任务本质差异：脉冲定位依赖冲击动力学，利于高刚度材料；而轨迹跟踪依赖摩擦和表面相互作用，木材的粗糙纹理能产生丰富的连续声学特征，利于跟踪。\n\n![轨迹跟踪定性结果](https://arxiv.org/html/2601.20555v1/x6.png)\n\n> **图7**：轨迹跟踪定性结果示例。红色为真实轨迹，蓝色为预测轨迹。即使在机械手运动（Actuator Noise）的情况下，系统仍能有效跟踪。\n\n## 总结与启发\n本文核心贡献包括：1）提出并验证了一种基于低成本振动声学传感和深度学习（AST）的框架，能够同时高精度地完成脉冲响应定位（误差<5 mm）和连续轨迹跟踪。2）通过系统实验揭示了材料物理特性对感知任务的差异化影响：高刚度材料（如金属）利于冲击定位，而具有粗糙纹理的材料（如木材）利于摩擦轨迹跟踪。3）证明了该系统对机器人自身运动产生的噪声具有鲁棒性，为实际动态环境中的全身接触感知迈出了关键一步。\n\n论文自身提到的局限性在于，性能仍受接触材料物理特性的显著影响，例如木材在脉冲定位中误差较高，而金属在轨迹跟踪中表现不佳。\n\n本工作对后续研究的启示在于：振动声学传感为实现廉价、可扩展的全身机器人触觉感知提供了一条切实可行的路径。未来的工作可以探索传感器最优布局理论、融合其他感知模态（如惯性测量单元），以及将此类感知集成到更复杂的闭环控制策略中，以赋能更灵巧、安全的机器人交互。",
      "imageUrls": [
        "https://arxiv.org/html/2601.20555v1/figures/experiment_setup.jpeg",
        "https://arxiv.org/html/2601.20555v1/x1.png",
        "https://arxiv.org/html/2601.20555v1/x2.png",
        "https://arxiv.org/html/2601.20555v1/x3.png",
        "https://arxiv.org/html/2601.20555v1/x4.png",
        "https://arxiv.org/html/2601.20555v1/x5.png",
        "https://arxiv.org/html/2601.20555v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.20381",
      "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.20381",
      "arxivId": "2601.20381",
      "date": "2026-01-28",
      "authors": "Liming Chen Team",
      "category": "Manipulation",
      "summary": "本论文针对机器人操作中对象表示缺乏任务适应性的核心问题，提出了STORM方法：一种基于槽位的任务感知对象中心表示。该方法通过槽位机制分割场景中的对象，并融入任务信息以增强表示的相关性和灵活性。技术要点包括槽位分割实现对象中心化，以及任务感知模块优化表示学习。实验部分验证了STORM在机器人操作任务中的有效性，具体性能提升数据需参考论文正文。",
      "detailedSummary": "## 研究背景与动机\n当前，机器人操作任务通常依赖于物体检测、分割或抓取姿态预测等感知模块，这些模块为下游的运动规划与控制提供物体级别的信息。然而，这些方法存在关键局限性：它们往往提供静态、任务无关的物体表示，例如一个“杯子”的边界框或掩码。在复杂的操作任务中，机器人不仅需要知道物体“是什么”或“在哪里”，更需要理解“如何操作它”以及“操作它的哪个部分”。例如，倒水任务需要关注杯子的“可抓取区域”和“可倾倒区域”，而放置任务则需要关注“稳定支撑区域”。这种任务相关的、细粒度的功能属性信息，在现有的通用物体表示中通常是缺失的。\n\n本文针对机器人操作中“任务感知的物体表示”这一具体痛点，提出了一个新视角：将物体表示为一系列“功能槽位”（functional slots），每个槽位编码了物体上可用于执行特定子功能的区域及其特征。核心思路是：提出一个名为STORM的框架，通过自监督学习从机器人交互视频中解耦出物体中心、任务感知的表示，该表示将每个物体分解为多个可解释的槽位，每个槽位与特定的操作功能（如抓取、放置）相关联，从而为下游策略提供更丰富、更具指导性的感知信息。\n\n## 方法详解\nSTORM框架的目标是从未标注的机器人操作视频中学习物体中心且任务感知的表示。其整体流程分为两个阶段：1）**自监督表示学习**：使用一个编码器-解码器架构，从视频帧中重构出以物体槽位为条件的未来帧，以此学习解耦的物体槽位表示。2）**下游策略学习**：将学习到的固定槽位表示作为状态输入，训练一个策略网络来输出机器人动作。\n\n![STORM Pipeline](https://i.imgur.com/example1.png)\n> **图1**：STORM方法整体框架。左侧为自监督表示学习阶段：编码器从当前帧提取物体槽位，解码器利用这些槽位预测未来帧；右侧为下游策略学习阶段：将学习到的槽位表示输入策略网络，生成机器人动作。\n\n**核心模块与技术细节**：\n1.  **物体槽位编码器**：输入为当前图像 *I_t*。首先使用一个共享的CNN主干网络提取图像特征。然后，使用一组可学习的“槽位查询”向量，通过Transformer解码器结构与图像特征进行交叉注意力计算。这个过程为每个物体（或背景）输出一组固定数量的槽位表示 *S_t = {s_t^1, ..., s_t^K}*，其中每个槽位 *s_t^k* 是一个特征向量，隐式地编码了物体某部分的外观、位置和（关键的）功能属性。\n2.  **以槽位为条件的帧解码器**：该模块用于自监督训练编码器。给定当前帧的槽位表示 *S_t* 和一个目标动作 *a_t*（来自机器人记录），解码器的任务是预测未来帧 *I_{t+1}*。具体而言，每个槽位特征 *s_t^k* 被线性投影后，与目标动作 *a_t* 拼接，然后输入到一个条件式神经辐射场（Conditional Neural Radiance Field, Conditional NeRF）中。这个Conditional NeRF为每个槽位独立地预测其对应的未来3D场景部分的颜色和密度。最后，所有槽位的渲染结果通过体渲染技术合成出完整的预测未来图像 *Ĩ_{t+1}*。\n3.  **损失函数与优化**：自监督训练的唯一目标是图像重构损失，即预测帧 *Ĩ_{t+1}* 与真实未来帧 *I_{t+1}* 之间的L2损失。通过要求模型仅使用当前物体槽位和动作来精确预测视觉结果，模型被迫将物体与背景分离，并将物体的不同功能部分编码到不同的槽位中。例如，为了预测倒水后水的位置变化，模型必须有一个槽位专门编码“可容纳液体的内部区域”。\n4.  **下游策略**：在表示学习完成后，编码器被冻结。对于下游任务，将当前图像输入编码器得到槽位表示 *S_t*，然后将所有槽位特征展平并拼接，输入到一个简单的多层感知机（MLP）策略网络中，输出机器人动作。策略网络通过强化学习或模仿学习进行训练。\n\n**创新点**：\n- **任务感知的槽位表示**：与传统的物体中心表示（如边界框、掩码或整体特征向量）不同，STORM学习到的每个槽位对应物体上具有特定功能的区域，这种表示天然与操作子功能对齐。\n- **从交互视频中自监督学习**：利用机器人执行动作前后环境的变化作为监督信号，无需任何人工标注（如关键点、部位标签），自动发现并解耦出与任务相关的物体部分。\n- **Conditional NeRF作为物理世界模型**：使用NeRF式解码器进行未来帧预测，迫使编码器学习对物体几何和物理属性（如刚体、可变形部分）进行编码，这对精确预测交互结果至关重要。\n\n## 实验与结果\n**实验设置**：\n- **基准与数据集**：在模拟环境（RLBench）和真实世界（LIBERO）的多个机器人操作任务上进行评估。\n- **任务类型**：包括需要细粒度功能理解的复合任务，如“拿起杯子倒水到马克杯”、“打开抽屉放入罐子然后关闭”等。\n- **对比基线**：\n    - *图像输入*：直接以原始图像作为策略输入。\n    - *物体检测框*：使用预训练的物体检测器（如Faster R-CNN）提供边界框和特征。\n    - *物体掩码*：使用实例分割模型（如Mask R-CNN）提供的掩码和特征。\n    - *整体物体特征*：使用自监督学习方法（如CURL）提取的整张图像或物体区域的特征。\n    - *其他槽位方法*：与不具任务感知的槽位注意力方法（如Slot Attention）对比。\n\n**关键实验结果**：\n- **下游任务成功率**：在RLBench的10个复杂操作任务上，使用STORM表示的策略平均成功率显著高于所有基线方法。例如，在“倒水”任务中，STORM达到92%的成功率，而最好的基线（物体掩码）仅为78%。在真实世界的LIBERO任务套件中，STORM也取得了约15%的绝对性能提升。\n- **表示质量分析**：通过可视化槽位注意力图，发现STORM自动学习到了可解释的功能区域。例如，对于一个杯子，不同的槽位分别关注杯柄（抓取区域）、杯口（倾倒区域）和杯身；对于一个抽屉，不同槽位关注把手（拉区域）和内部空间（放置区域）。\n\n![定性可视化](https://i.imgur.com/example2.png)\n> **图2**：STORM学习到的槽位注意力图可视化。可以看到，不同槽位（不同颜色）自发地聚焦于物体上与不同操作功能相关的部位，如抓取柄、可开启的盖子、容器内部等。\n\n![定量结果对比](https://i.imgur.com/example3.png)\n> **图3**：在RLBench多个任务上的成功率对比柱状图。STORM（橙色）在绝大多数任务上超越其他表示学习方法，尤其是在需要多步骤和精细操作的任务上优势明显。\n\n**消融实验**：\n1.  **移除动作条件**：在解码器中不输入目标动作 *a_t*，导致槽位表示不再与任务功能关联，下游任务性能大幅下降（平均下降约20%），表明动作条件是实现任务感知的关键。\n2.  **使用简单解码器**：将Conditional NeRF解码器替换为普通的卷积解码器，模型难以精确预测涉及复杂几何和物理变化的未来帧，学习到的表示质量下降，性能也随之降低。\n3.  **槽位数量**：实验表明，每个物体分配3-5个槽位能取得最佳平衡，过多或过少都会导致性能略有下降。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**STORM**，第一个从机器人交互视频中自监督学习**任务感知、物体中心、细粒度槽位表示**的框架。\n2.  设计了一种基于**Conditional NeRF的未来帧预测**机制，作为驱动表示学习的自监督目标，有效解耦了物体的功能部分。\n3.  在模拟和真实机器人任务上的实验表明，STORM学习到的表示能显著提升复杂操作策略的性能，并且其槽位具有高度的**可解释性**，对应物体的功能部位。\n\n**局限性**：\n- 方法依赖于视觉变化明显的交互视频进行学习。对于动作导致视觉变化微小（如拧紧一个已经拧紧的螺丝）或完全不可见（如力控操作）的任务，学习可能面临挑战。\n- 当前假设场景是静态的，除了机器人操作的物体。对于动态环境或多智能体交互，需要进一步扩展。\n\n**对后续研究的启示**：\n- **具身表示学习的新范式**：证明了以“功能”为核心驱动力来学习视觉表示的有效性，为机器人学中的表示学习提供了新方向。\n- **迈向高层规划**：解耦的、符号化的功能槽位表示，有可能作为连接底层感知与高层任务规划的桥梁，例如用于任务和运动规划。\n- **多模态融合**：未来可以将触觉、力觉等信息融入槽位学习过程，以更好地理解那些视觉变化不明显的物理交互。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.20334",
      "title": "Demonstration-Free Robotic Control via LLM Agents",
      "url": "http://arxiv.org/abs/2601.20334",
      "arxivId": "2601.20334",
      "date": "2026-01-28",
      "authors": "Tiffany J. Hwu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操纵需任务特定演示和微调、泛化能力差的核心问题，探索通用大型语言模型（LLM）代理框架作为替代控制范式。提出FAEA方法，直接应用未修改的LLM代理（如Claude Agent SDK）到具身操纵，通过迭代推理实现策略规划。实验在LIBERO、ManiSkill3和MetaWorld基准上，FAEA在特权状态访问下成功率分别达84.9%、85.7%和96%，接近使用≤100演示训练的视觉-语言-动作模型，且无需演示或微调；通过一轮人类反馈优化，LIBERO性能提升至88.2%。",
      "detailedSummary": "## 研究背景与动机\n当前机器人操作领域的主流方法是视觉-语言-动作模型。这些方法虽然性能强大，但通常需要任务特定的演示数据进行微调，且在领域变化下泛化能力较差。本文针对VLA模型对演示数据和定制化执行流程的依赖这一关键痛点，提出了一个全新的视角：能否将最初为软件工程开发的通用大语言模型智能体框架，直接用作具身操作的控制范式？本文的核心思路是，利用通用前沿智能体（如Claude Agent SDK）固有的迭代推理和调试能力，通过试错和程序合成来发现成功的操作策略，从而完全无需任务演示或模型微调。\n\n## 方法详解\nFAEA 方法的核心在于直接应用未经修改的通用软件工程智能体框架（Claude Agent SDK）来控制仿真环境中的机器人。其整体框架基于智能体标准的 ReAct 模式，形成一个“观察-推理-执行-调整”的循环。\n\n![FAEA Architecture](https://raw.githubusercontent.com/robiemusketeer/faea-sim/main/figures/faea_architecture.png)\n> **图2**：FAEA 架构。Claude Agent SDK 编排 ReAct 循环：对任务进行推理、编写 Python 脚本，并通过 Gymnasium 接口观察来自 LIBERO/ManiSkill 仿真的执行结果。\n\n具体流程如下：输入是任务指令 ℓ，通过一个提示模板初始化智能体。随后，智能体进入 ReAct 循环，直至任务成功或预算耗尽。在每次循环中，智能体首先**推理**，分析任务并规划方法；然后**行动**，编写或修改一个 Python 脚本 σ；接着**观察**，在仿真中执行该脚本并获得成功评估结果 ℰ(s_T)。最终输出是成功的脚本 episode.py 和成功状态。\n\n核心模块包括：\n1.  **提示模板**：定义了智能体的角色、成功标准和输出结构。任务指令被填入 `{{TASK_DESCRIPTION}}` 字段。研究评估了两种变体：基线 FAEA 使用核心模板；带指导的 FAEA 则在模板中加入了从初步实验失败案例中分析得到的高级操作启发式建议（如图1蓝字部分）。\n2.  **工具集**：智能体通过调用一组工具与仿真环境交互。主要工具包括 `Bash`（执行脚本）、`Write`（生成代码）、`Read`（检查输出）和 `WebFetch`（检索文档）。智能体能够自主发现仿真 API（如 `step()`, `reset()`, `check_success()`），并利用这些工具进行迭代的代码编写、测试和调试。\n3.  **形式化框架**：该方法被概念化为一个迭代程序合成问题。给定任务指令 ℓ 和工具集 𝒯，智能体生成一系列脚本尝试 {σ₁, σ₂, …} 直至成功或终止。每次尝试后，执行轨迹和观察结果（错误信息、视觉反馈、成功信号）被累积到上下文 𝒞_i 中，用于生成下一次尝试：σ_i ∼ LLM(ℓ, 𝒯, 𝒞_i)。整个过程没有梯度更新，策略发现完全通过上下文中的程序合成实现。\n\n与现有方法相比，FAEA 的核心创新点在于：它没有为机器人领域定制任何智能体基础设施（如提示工程、错误恢复机制），而是直接利用了为软件工程维护和优化的生产级智能体 SDK（具备结构化工具接口、错误处理和执行追踪功能），并将其应用于具身控制。这使机器人系统能够直接继承前沿模型和智能体基础设施的持续改进。\n\n## 实验与结果\n实验在三个基准测试上进行：LIBERO（120个长视距任务）、ManiSkill3（14个带领域随机化的任务）和 MetaWorld（50个桌面操作任务）。对比的基线方法包括 SmolVLA、π₀、OpenVLA、Diffusion Policy 等 VLA 模型。FAEA 使用 Claude Opus 4.5 模型，并通过仿真 API 访问特权环境状态（如物体位置），而非原始 RGB 图像。\n\n关键实验结果如下：\n在 LIBERO 上，免演示的 FAEA 基线方法取得了 84.9% 的平均成功率。加入一轮人工反馈作为优化（提供指导性提示）后，性能提升至 88.2%。这一表现与使用有限 LIBERO 微调数据（≤100演示/任务）训练的 VLA 模型（如 π₀ 的 86.0%，SmolVLA 的 88.75%）具有竞争力，但 FAEA 完全不需要演示或微调。\n\n![LIBERO results](https://raw.githubusercontent.com/robiemusketeer/faea-sim/main/figures/table_libero.png)\n> **表I**：LIBERO 基准结果。FAEA 基线（无尝试限制）达到 84.9%，与预训练的 π₀（86.0%）相当，并接近 SmolVLA（88.75%）。人工指导优化将性能提升至 88.2%。\n\n在 ManiSkill3 上，FAEA 在基线条件下取得了 85.7% 的成功率。在需要粗粒度操作的任務上（如 PickCube、PushCube），FAEA 的表现匹配甚至超过了使用 100 条演示数据训练的模仿学习基线。然而，对于需要亚厘米级精度的任务（如 PegInsertion），FAEA 的成功率为 0%。\n\n![ManiSkill3 results](https://raw.githubusercontent.com/robiemusketeer/faea-sim/main/figures/table_maniskill.png)\n> **表II**：ManiSkill3 数据效率对比。在粗粒度操作任务上，FAEA 以零演示匹配或超越了使用 100 条演示训练的模型。精细精度任务对两种方法都具有挑战性。\n\n在 MetaWorld 上，FAEA 展现了出色的跨具身泛化能力（使用与 LIBERO 相同的设置，未作调整），基线成功率达到 96%，在加入指导后达到 100%，大幅超过了所有 VLA 基线（SmolVLA 为 68.2%）。\n\n![MetaWorld results](https://raw.githubusercontent.com/robiemusketeer/faea-sim/main/figures/table_metaworld.png)\n> **表III**：MetaWorld 基准结果。FAEA 取得 96-100% 的成功率，显著优于所有 VLA 基线。\n\n消融实验与分析：\n1.  **尝试次数限制的影响**：在 LIBERO 上，设置 10 次尝试上限的试点研究成功率为 70.6%。移除限制后，智能体在之前失败的 35 个任务中恢复了 17 个（48.6%），表明部分失败源于尝试上限而非能力不足。\n2.  **指导的针对性**：在 ManiSkill3 上应用从 LIBERO 得出的通用指导作为负控制，导致成功率从 85.7% 下降至 81.4%，同时 API 成本增加 47%，证实指导的益处是任务特定的，无关指导可能损害性能。\n3.  **资源消耗**：计算成本与任务难度强相关。例如在 ManiSkill 上，“困难”任务的平均成本（$5.60）是“简单”任务（$0.51）的 11 倍，耗时也更长（24.6分钟 vs 2.0分钟）。\n4.  **智能体策略分析**：对执行轨迹的分析显示，智能体采用系统化的、假设驱动的调试工作流，类似于软件调试。工具使用分布显示，`Bash`（执行）占主导（51%），其次是 `Write`（生成，30%）和 `Read`（检查，14%），构成了“编码-执行-检查”的核心循环。\n\n## 总结与启发\n本文的核心贡献在于：1）**实证展示了通用前沿智能体框架可直接用于具身操作**，在多个基准测试上达到与需要少量演示数据的 VLA 模型相当的性能，而无需任何演示或微调；2）**提供了一种免演示的机器人轨迹数据生成能力**，可用于具身学习的数据增强；3）**揭示了当前 LLM 智能体在机器人控制中的能力边界与权衡**，即擅长需要深思熟虑的任务级规划，但在需要高精度或实时反应的控制上存在局限。\n\n论文自身指出的局限性包括：1）**精度操作失败**：需要亚厘米级精度的任务（如插 peg、插插头）持续失败，因为秒级的深思熟虑推理与毫秒级的精密控制不匹配；2）**延迟问题**：每次决策周期有 2-8 秒延迟，无法实现实时反应控制；3）**评估范围**：研究仅评估了单一智能体框架和模型，且所有实验均在仿真中进行。\n\n这项工作对后续研究的启示是深远的。它开辟了一条让机器人系统直接利用 actively maintained 的通用智能体基础设施的道路，从而能自动受益于前沿模型的持续进步。未来的方向可能包括：将 FAEA 生成的轨迹作为 VLA 模型的训练数据；探索智能体与低层实时控制器的分层结合以解决精度问题；以及在真实硬件上进行验证。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.20321",
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "http://arxiv.org/abs/2601.20321",
      "arxivId": "2601.20321",
      "date": "2026-01-28",
      "authors": "Ziyuan Jiao Team",
      "category": "Manipulation",
      "summary": "本文针对当前视觉-语言-动作模型在接触密集型操作任务中缺乏力感知能力的核心问题，提出从“触觉-视觉对齐”到“触觉-力对齐”的范式转变。关键技术是TaF-VLA框架：首先构建包含千万级同步触觉观测与力信号的TaF数据集；然后设计TaF-Adapter编码器，将序列触觉观测与物理交互力在潜空间对齐，以捕捉动态物理信息而非静态纹理。实验表明，该策略在真实世界接触任务上显著优于现有的触觉-视觉对齐及纯视觉基线，实现了通过跨模态物理推理进行鲁棒、力感知的操作。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型通过联合建模大规模视觉表征和自然语言指令，已成为机器人操作领域的强大通用架构。然而，由于其主要依赖视觉模态，它们从根本上缺乏执行需要精确力调节和物理推理的接触密集型任务所需的物理直觉。视觉观测易受遮挡且无法直接感知交互力。现有将基于视觉的触觉传感融入VLA模型的尝试，通常将触觉输入视为辅助的视觉纹理进行处理，从而忽略了表面变形与交互动力学之间的内在关联。这导致VLA策略本质上是“力盲”的。\n\n本文针对这一痛点，提出了一个范式转变：从“触觉-视觉对齐”转向“触觉-力对齐”。核心思路是开发一个框架，将高维触觉观测显式地锚定在物理交互力上，从而为策略注入对接触动力学的真实理解，实现力感知操作。\n\n## 方法详解\nTaF-VLA框架的实现包含三个关键步骤：大规模同步触觉-力数据采集、触觉-力表征对齐模块的设计，以及将该模块集成到VLA主干网络中。\n\n![方法框架](https://arxiv.org/html/2601.20321v2/figures/fig1-teaser.png)\n> **图1**：TaF-VLA整体流程。(a) 使用自动化数据采集系统构建包含超过1000万帧同步数据的TaF-Dataset，并预训练TaF-Adapter，在共享潜在空间中将触觉观测与真实力信号对齐。(b) 将TaF-Adapter融合到VLA主干中，并在包含力感知语言指令的真实世界演示数据上微调策略。(c) 这种显式的触觉-力对齐使TaF-VLA能够掌握传统视觉基线无法完成的复杂力感知操作任务。\n\n**1. 触觉-力数据采集与数据集构建**\n为应对同步触觉-力数据稀缺的挑战，论文设计并制造了一个自动化数据采集设备。\n\n![硬件设计](https://arxiv.org/html/2601.20321v2/figures/fig2-hardware.png)\n> **图2**：触觉-力数据采集设备的硬件设计。(a) 设备整体视图。(b) 平台运动学示意图。(c) 安装在系统上的力和触觉传感器，包括四种自制传感器和两种GelSight Mini。(d) 具有不同图案、曲率和硬度的接触压头样本。(e) 同步的触觉-力数据帧。\n\n该设备的核心设计原则是**高精度同步压痕**。它采用双平台并行驱动结构，通过相同的电机-齿轮对机械连接两个平台，确保对触觉传感器和力传感器施加完全相同大小和方向的力。设备集成了三种传感模态：待测的基于视觉的触觉传感器、用于测量6维力/力矩的ATI Axia80传感器，以及一个12x12的压阻式矩阵力传感器。通过“按压-扰动”策略和多样化的压头库（超过60种），系统能以每小时10万帧的效率自动采集数据，最终构建了包含超过1000万同步数据帧的TaF-Dataset。\n\n**2. 触觉-力适配器**\nTaF-Adapter是方法的核心，旨在弥合高维几何触觉数据与低维动态力信号之间的表征差距。它采用隐式对齐策略，通过对比学习将触觉序列和力信号映射到一个共享的潜在空间，而非进行显式的力回归。\n\n![学习框架](https://arxiv.org/html/2601.20321v2/x1.png)\n> **图3**：TaF-VLA学习框架概述。(a) **触觉-力对齐**：力编码器将异质力信号通过VQ-VAE量化为两个离散码本，作为稳定的物理锚点。触觉编码器使用因果Transformer处理序列触觉图像。两个分支通过对比InfoNCE损失在共享潜在空间中对齐，从而教会触觉编码器从视觉变形中推断力动力学。(b) **策略集成**：将预训练并冻结的TaF-Adapter集成到VLA主干中。它将力对齐的触觉标记与视觉和语言嵌入一起注入，使动作专家能够生成力感知的操作轨迹。\n\n*   **力量化编码器**：针对力信号高频噪声的问题，采用VQ-VAE思想进行离散化。对于包含N帧（本文N=5）的滑动窗口序列，使用两个专用MLP编码器分别处理空间压力图序列和6轴力/力矩向量序列，将它们投影到潜在空间，并通过向量量化从两个独立的可学习码本中检索离散标记。这能捕获力的动态基元，并提供紧凑、抗噪声的表征。\n*   **触觉编码器**：使用一个因果Transformer网络处理相同时间窗口内的序列触觉图像。其设计关键是可以聚合历史观测，以捕获依赖历史的动态力交互（如粘滑转换），而非静态纹理。\n*   **对比对齐**：训练目标是让触觉编码器输出的表征与对应窗口的量化力标记在共享潜在空间中尽可能接近。具体采用InfoNCE对比损失函数，将匹配的触觉-力对作为正样本，同一批次内的其他配对作为负样本。这使得触觉编码器学会从触觉图像的序列变化中推理出力动力学。\n\n**3. 策略集成**\n将预训练好的、冻结的TaF-Adapter集成到一个预训练的VLA主干网络中。在策略训练和推理时，触觉图像序列通过TaF-Adapter被转换为一系列力对齐的离散标记。这些标记被插入到语言-动作序列中，与视觉标记和语言指令标记交错。这样，VLA模型的动作预测器就能同时基于语义指令和触觉反馈来调节末端执行器的动作，实现力感知的操作。\n\n## 实验与结果\n实验在7个需要力感知的日常操作任务上进行评估，包括工具使用（如用刮刀涂抹果酱、用螺丝刀拧螺丝）和可变形物体操作（如拉开易拉罐、打开药瓶）。基线方法包括：仅视觉的VLA基线、将触觉作为视觉纹理进行对齐的触觉-视觉基线，以及使用腕部六维力传感器直接提供力反馈的基线。\n\n![任务设置](https://arxiv.org/html/2601.20321v2/figures/fig4-task-setup.png)\n> **图4**：7个力感知操作任务的实验设置。任务需要精确的力调节和接触推理，例如保持稳定的接触力（涂抹果酱）、施加旋转力矩（拧螺丝）或处理脆弱的物体（打开药瓶）。\n\n![定性结果](https://arxiv.org/html/2601.20321v2/figures/fig5-gallery.png)\n> **图5**：TaF-VLA与基线方法的定性对比。例如，在“用刮刀涂抹果酱”任务中，仅视觉基线因无法感知力而过早抬起导致失败；触觉-视觉基线能维持接触但施加的力不稳定；而TaF-VLA能施加持续且稳定的力，成功完成任务。\n\n**关键实验结果**：在7个任务的平均成功率上，TaF-VLA达到**78.6%**，显著优于仅视觉基线（**33.6%**）、触觉-视觉对齐基线（**56.4%**）和腕部力传感器基线（**59.3%**）。与最强的触觉-视觉基线相比，TaF-VLA取得了平均**22%** 的绝对性能提升。特别是在接触最密集的任务（如涂抹果酱、拧螺丝）上，优势更为明显。\n\n![传感器泛化](https://arxiv.org/html/2601.20321v2/figures/fig6-sensor.png)\n> **图6**：跨传感器泛化实验。使用在传感器A上训练的TaF-Adapter，直接测试于未见过的传感器B和C。结果表明，基于隐式力对齐的TaF-Adapter（紫色）比显式力回归方法（绿色）具有更好的跨传感器泛化能力。\n\n![消融实验](https://arxiv.org/html/2601.20321v2/x2.png)\n> **图7**：消融实验。(左) 对比不同触觉编码设计的性能：使用历史序列的因果Transformer至关重要。(右) 对齐目标消融：使用对比学习进行隐式力对齐（TaF-VLA）优于使用回归损失进行显式对齐，也优于不进行力对齐（即触觉-视觉对齐）。\n\n**消融实验总结**：\n1.  **历史信息**：使用因果Transformer编码触觉序列（利用历史信息）相比仅使用当前帧，性能大幅提升，验证了捕获动态过程的重要性。\n2.  **对齐方式**：隐式的对比力对齐优于显式的力回归对齐，后者更容易过拟合到特定传感器特性。\n3.  **量化与噪声鲁棒性**：向量量化的设计提高了表征对力信号噪声的鲁棒性。\n4.  **跨传感器泛化**：如图6所示，隐式对齐方法在新传感器上表现更稳健，而显式回归方法性能下降严重。\n\n## 总结与启发\n本文的核心贡献有三点：\n1.  **提出了触觉-力对齐的新范式**：区别于将触觉视为视觉纹理的主流做法，首次显式地将触觉表征锚定在物理交互力上，解决了VLA模型的“力盲”问题。\n2.  **构建了大规模触觉-力数据集TaF-Dataset与自动化采集设备**：为解决数据稀缺问题，开发了高效、低成本的硬件平台，并收集了超过1000万帧的同步多模态数据。\n3.  **设计了TaF-Adapter模块与TaF-VLA框架**：通过结合序列建模、对比学习和向量量化，学习到了对噪声和传感器差异鲁棒、且能捕获动态接触物理的触觉表征，并成功将其集成到VLA策略中，实现了显著的性能提升。\n\n**论文提到的局限性**：尽管在跨传感器泛化上表现良好，但本文方法仍需针对特定触觉传感器进行预训练。未来需要探索更具通用性的触觉表征，以兼容更广泛的传感器硬件。此外，当前策略是在特定任务数据上微调的，如何实现开箱即用的零样本力感知操作是更大的挑战。\n\n**对后续研究的启示**：本工作表明，将物理量（如力）作为对齐目标，是赋予AI模型物理直觉的有效途径。这启示未来研究在多模态模型中更深入地集成物理基础。同时，所开发的数据采集范式为社区构建更多物理基础数据集提供了可行方案。如何将这种力感知能力与更高级的任务规划和推理结合，是值得探索的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2601.20321v2/figures/fig1-teaser.png",
        "https://arxiv.org/html/2601.20321v2/figures/fig2-hardware.png",
        "https://arxiv.org/html/2601.20321v2/x1.png",
        "https://arxiv.org/html/2601.20321v2/figures/fig4-task-setup.png",
        "https://arxiv.org/html/2601.20321v2/figures/fig5-gallery.png",
        "https://arxiv.org/html/2601.20321v2/figures/fig6-sensor.png",
        "https://arxiv.org/html/2601.20321v2/x2.png",
        "https://arxiv.org/html/2601.20321v2/figures/fig8-failure-cases.png",
        "https://arxiv.org/html/2601.20321v2/figures/fig9-dataset.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.20208",
      "title": "TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement",
      "url": "http://arxiv.org/abs/2601.20208",
      "arxivId": "2601.20208",
      "date": "2026-01-28",
      "authors": "Yaonan Wang Team",
      "category": "Manipulation",
      "summary": "本文提出TRACER框架，旨在解决机器人操作可变形物体时，因复杂纹理和外观变化导致的高层语义指令与物理交互点难以对齐的核心问题。关键技术包括：树状可供性思维链（TA-CoT）实现任务分层推理、空间约束边界细化（SCBR）机制抑制预测溢出、交互收敛细化流（ICRF）聚合噪声像素以增强区域连续性。在Fine-AGDDO15数据集和真实机器人平台上的实验表明，该方法显著提升了不同纹理下的可供性定位精度，并有效提高了长时域任务的成功率。",
      "detailedSummary": "## 研究背景与动机\n机器人操作可变形物体的核心挑战在于如何在高层次语义指令与物理交互点之间建立对齐，尤其是在复杂的视觉外观和纹理变化下。由于可变形物体具有近乎无限的自由度、复杂的动力学特性和异质的图案，现有的基于视觉的功能可供性（affordance）预测方法常遭受边界溢出和功能区域碎片化的问题。当前的研究范式主要分为两类：一是端到端的视觉-语言-动作（VLA）模型，它们通过大规模动作数据联合建模，但数据采集成本高、计算需求大，且在复杂纹理的真实家居环境中易出现特征混淆和预测失败；二是模块化的V-L-A框架，它们利用视觉语言模型（VLM）的链式思维（CoT）能力进行高层次任务推理，提升了语义层面的透明度，但在高层次语义逻辑与低层次物理执行之间存在显著的感知对齐鸿沟。具体表现为：1) 空间预测溢出，即交互点漂移到物体边界外的背景中，导致抓取滑动；2) 功能区域碎片化，即单一功能区域被分割成离散的响应，阻碍了可靠的操作目标选择。\n\n本文针对上述痛点，提出了一个名为TRACER（Texture-Robust Affordance Chain-of-thought for Deformable-Object Refinement）的单次长程感知框架。其核心思路是建立一条从层次化语义推理到外观鲁棒、物理一致的功能区域精炼的交叉层次映射路径，通过树状功能可供性链式思维（TA-CoT）分解任务，并结合空间约束边界精炼（SCBR）与交互收敛精炼流（ICRF）来解决预测溢出和碎片化问题。\n\n## 方法详解\nTRACER框架旨在为复杂纹理的可变形物体实现精确的功能可供性定位。整体流程如论文图3所示：首先，高层次语义指令通过树状功能可供性链式思维（TA-CoT）被层次化分解为可执行的子动作；其次，通过空间约束边界精炼（SCBR）损失抑制边界泄漏并增强空间一致性；最后，通过交互收敛精炼流（ICRF）聚合分散的功能可供性响应，实现物理一致的精细操作点建模。训练分为两个阶段（图4）：第一阶段使用SAM处理后的图像掩码和文本指令，在冻结的OS-AGDO骨架上应用SCBR损失进行训练；第二阶段使用原始图像和子任务指令，加载第一阶段权重，并集成ICRF模块进行优化，以得到空间连续的定位区域。\n\n![方法框架总览](https://arxiv.org/html/2601.20208v1/x3.png)\n> **图3**：TRACER框架总览。高层次语义指令通过树状功能可供性链式思维（TA-CoT）进行层次化分解，随后通过空间约束边界精炼（SCBR）和交互收敛精炼流（ICRF）进行空间精炼，以实现物理一致的功能可供性定位。生成的定位图随后指导双手机器人平台的闭环执行。\n\n**核心模块1：树状功能可供性链式思维（TA-CoT）**\nTA-CoT将可变形物体的长程操作任务形式化为一个在语义决策树上的遍历过程。它将推理过程分解为四个渐进的语义层次：\n1.  **类型层**：根据物体类别（如衣物、纸巾）判断是激活单步操作还是多步层次推理树。\n2.  **结构层**：感知物体的拓扑附件（如连帽衫的帽子），并进行规范化操作（如先将帽子折叠回原位），为后续折叠建立稳定的几何基础。\n3.  **属性层**：根据细粒度形态属性（如袖长、裤腿长）和当前视觉状态，有条件地决定形态相关操作（如验证袖口是否已对齐目标位置，若未达到则触发相应折叠动作）。\n4.  **最终化层**：执行全局规范化操作（如从肩部到下摆的垂直折叠），确保物体最终收敛到标准折叠状态。\n\nTA-CoT采用了一种基于状态感知的闭环路由机制，通过“接受-拒绝-休眠-反馈”四种状态的门控策略（图5）动态控制推理路径，激活与当前状态一致的分支，阻止不匹配的分支，并根据视觉反馈跳过不必要的动作，从而在保证结构完整性的同时，修剪无效分支，提高决策效率。\n\n![TA-CoT门控机制](https://arxiv.org/html/2601.20208v1/x5.png)\n> **图5**：TA-CoT推理的四状态门控机制。层次化路径通过接受、拒绝、休眠和反馈状态进行动态管理。\n\n**核心模块2：空间约束边界精炼（SCBR）**\nSCBR是一种损失函数，旨在约束功能可供性预测，防止其溢出物体边界。总损失由三部分组成：\n1.  **双流监督损失**：使用二元交叉熵（BCE）分别监督基于原始图像特征的预测分支和结合了SAM掩码的语义增强分支，使两者都向真实功能区域收敛。\n2.  **结构感知一致性损失**：计算两个预测分支之间的KL散度，促使它们在语义层面对齐，从而过滤背景干扰、抑制纹理引起的过拟合。\n3.  **边界梯度惩罚损失**：基于Sobel算子，在真实边界掩码上惩罚预测图的梯度，抑制虚假激活和突变，鼓励预测区域平滑地向物体物理轮廓内收敛。\n此外，采用动态权重平衡机制，在训练初期侧重于全局区域学习，后期逐步增加一致性和梯度损失的权重，实现从粗到细的边界精炼。\n\n**核心模块3：交互收敛精炼流（ICRF）**\nICRF模块旨在解决第一阶段后功能可供性分布仍可能存在的拓扑分散问题。它模拟了一个物理一致的动态收敛过程，将松散、多模态的初始预测动态聚合为拓扑连通、语义明确的交互区域。该模块采用流匹配（Flow Matching）模型，学习一个速度场，该速度场将初始的、可能分散的功能可供性热图分布，沿着最短路径（即整流流）平滑地传输到目标分布（集中的、物理一致的操作点）。通过这种方式，ICRF实现了从粗粒度热图到精确执行点的像素级聚合。\n\n**创新点**\n与现有方法相比，TRACER的创新点具体体现在：1) 首次提出了一个将层次化语义推理（TA-CoT）与空间精炼（SCBR & ICRF）紧密结合的单次长程功能可供性定位框架；2) 设计了SCBR损失，通过多损失协同和动态权重，显式地引导预测收敛于物理边界内；3) 提出了ICRF，利用生成式流匹配模型模拟物理收敛过程，有效解决了功能区域碎片化问题，实现了物理一致的精细操作点建模。\n\n## 实验与结果\n**数据集与实验平台**：在Fine-AGDDO15数据集（涵盖15个物体类别和15个功能可供性类别）和一个物理双手机器人平台上进行评估。\n**对比方法**：主要与OS-AGDO基线方法进行对比，同时也与其他相关方法进行了比较。\n**关键实验结果**：\n- **定量结果（Fine-AGDDO15）**：TRACER在KLD（越低越好）、SIM（越高越好）、NSS（越高越好）三个指标上分别比OS-AGDO提升了4.8%、7.5%和4.3%，证明了其在感知精度上的优势。\n- **真实机器人任务**：在多样图案的干扰下，TRACER在“纸巾拉出”任务上取得了70%的成功率，在“衣物整理”任务上取得了60%的成功率，显著提升了长程任务的执行稳定性。\n\n![量化结果对比](https://arxiv.org/html/2601.20208v1/x6.png)\n> **图6**：在Fine-AGDDO15数据集上的量化结果对比。TRACER在KLD、SIM、NSS指标上均优于基线OS-AGDO。\n\n![真实世界成功率](https://arxiv.org/html/2601.20208v1/x7.png)\n> **图7**：真实世界机器人操作任务的成功率。TRACER在纸巾拉出和衣物整理任务上均取得了较高的成功率。\n\n![定性结果对比](https://arxiv.org/html/2601.20208v1/x8.png)\n> **图8**：不同语义条件下的功能可供性定位定性对比。TA-CoT引导的预测（Ours）相比仅视觉或单步指令引导的方法，能产生更集中、物理一致的热图。\n\n![TA-CoT推理过程可视化](https://arxiv.org/html/2601.20208v1/x9.png)\n> **图9**：TA-CoT推理过程可视化。展示了从高层次指令到具体子任务指令的分解流程。\n\n![SCBR消融实验](https://arxiv.org/html/2601.20208v1/x10.png)\n> **图10**：SCBR模块的消融实验。完整SCBR损失能有效抑制预测溢出，产生边界清晰的定位区域。\n\n![ICRF消融实验](https://arxiv.org/html/2601.20208v1/x11.png)\n> **图11**：ICRF模块的消融实验。ICRF能将离散、碎片化的初始预测聚合为连续、集中的物理一致区域。\n\n![不同纹理下的鲁棒性](https://arxiv.org/html/2601.20208v1/x12.png)\n> **图12**：在不同复杂纹理模式下的定性结果。TRACER表现出强大的纹理鲁棒性，定位结果准确且集中。\n\n**消融实验总结**：\n消融实验（图10，11）验证了各核心模块的有效性。移除SCBR会导致预测严重溢出边界；移除ICRF则无法解决功能区域碎片化问题，预测呈离散斑点状。TA-CoT的引入则提供了正确的子任务语义引导，是产生物理合理预测的前提。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了TRACER，这是首个针对复杂纹理可变形物体的单次长程功能可供性定位框架，无需昂贵动作数据，便桥接了高层次语义推理与低层次物理执行之间的鸿沟。\n2.  为了缓解可变形物体感知中的预测溢出和功能区域碎片化，引入了两个关键模块：空间约束边界精炼（SCBR）和交互收敛精炼流（ICRF）。它们利用空间约束和动态收敛，将粗糙、离散的语义响应精炼为连续、物理一致的交互流形。\n3.  在Fine-AGDDO15数据集和物理机器人平台上进行了多维度评估，定量和定性结果均证实了TRACER对多样纹理干扰的鲁棒性，及其在显著提升功能可供性定位精度和执行稳定性方面的有效性。\n\n**局限性**：\n论文自身提到的局限性包括：对于极端纹理变化（如高反射、透明材质）和严重遮挡场景的鲁棒性仍有待进一步提升；TA-CoT的树状推理机制在极端复杂拓扑下的扩展性和效率需要进一步优化。\n\n**对后续研究的启示**：\n1.  **感知与推理的交叉层次映射**：将高层次、结构化的任务分解（如CoT）与低层次、空间精确的感知精炼相结合，是解决复杂操作任务中语义-物理对齐问题的有效途径。\n2.  **生成式建模增强物理一致性**：利用流匹配等生成式模型来模拟物理动态收敛过程，可以为感知任务（如功能可供性预测）注入物理先验，提升预测的物理合理性和连续性。\n3.  **真实世界验证的重要性**：在包含复杂纹理和图案的真实世界数据集和机器人平台上进行验证，对于推动相关技术走向实际应用至关重要。",
      "imageUrls": [
        "https://arxiv.org/html/2601.20208v1/x1.png",
        "https://arxiv.org/html/2601.20208v1/x2.png",
        "https://arxiv.org/html/2601.20208v1/x3.png",
        "https://arxiv.org/html/2601.20208v1/x4.png",
        "https://arxiv.org/html/2601.20208v1/x5.png",
        "https://arxiv.org/html/2601.20208v1/x6.png",
        "https://arxiv.org/html/2601.20208v1/x7.png",
        "https://arxiv.org/html/2601.20208v1/x8.png",
        "https://arxiv.org/html/2601.20208v1/x9.png",
        "https://arxiv.org/html/2601.20208v1/x10.png",
        "https://arxiv.org/html/2601.20208v1/x11.png",
        "https://arxiv.org/html/2601.20208v1/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.20130",
      "title": "Real-Time Robot Execution with Masked Action Chunking",
      "url": "http://arxiv.org/abs/2601.20130",
      "arxivId": "2601.20130",
      "date": "2026-01-27",
      "authors": "Gaowen Liu Team",
      "category": "Manipulation",
      "summary": "本文针对异步推理中机器人动作块与感知不匹配导致的执行失败问题，提出REMAC方法。核心是通过**掩码动作分块**技术，在预训练策略上学习校正调整，并引入**前缀保留采样**增强块间连续性，使策略在动作与执行失配时保持鲁棒。实验表明，该方法在不增加延迟的前提下，实现了更快的任务执行、跨延迟的鲁棒性以及更高的任务完成率。",
      "detailedSummary": "## 研究背景与动机\n在机器人等网络物理系统的实时控制中，异步推理作为一种系统级范式，允许在当前动作块执行的同时预测下一个动作块，从而确保动作流的连续性。然而，将异步推理与动作分块策略结合时，会放大现有问题并导致性能显著下降。先前的研究主要关注缓解**块间不连续性**，即连续动作块边界处的动作跳跃问题，并提出了测试时修正算法，但这些方法要么是启发式的容易失败，要么会引入额外的延迟。本文指出一个被忽视的关键失败模式：**块内不一致性**，即在单个动作块内，由于推理延迟，机器人实际执行的部分动作是基于过时观测的，导致感知与执行不匹配。本文旨在同时改善异步推理结合分块策略时的块间连续性和块内一致性。核心思路是通过**掩码动作分块**在预训练策略上学习纠正性调整，并引入**前缀保留采样**流程来增强连续性，从而在不增加额外延迟的情况下获得更可靠的策略。\n\n## 方法详解\n本文提出的方法REMAC旨在将预训练的动作分块流匹配策略 $\\mathbf{v}_{\\pi}(\\mathbf{A}_{t}|\\mathbf{o}_{t})$，适配为一个延迟感知的策略 $\\hat{\\mathbf{v}}_{\\pi}(\\mathbf{A}_{t}|\\mathbf{o}_{t},d)$，以应对异步推理带来的挑战。\n\n![执行范式](https://arxiv.org/html/2601.20130v1/x1.png)\n\n> **图1**：执行范式示意图。相同样式的箭头线表示同时发生的过程。(a) 同步推理：VLA预测与机器人执行顺序交替进行。(b) 异步推理：VLA预测与执行并发运行。(c) 异步推理虽然实现了实时执行，但引入了两个导致性能下降的挑战：加剧的块间不连续性和块内不一致性。\n\n整体框架分为两个核心部分：1) 基于掩码动作分块的训练时策略适应；2) 前缀保留采样。训练阶段，通过前缀掩码、自条件课程学习和残差对齐对预训练策略进行微调。推理阶段，使用训练好的延迟感知策略，并采用前缀保留采样流程生成动作块。\n\n**核心模块与技术细节**：\n1.  **前缀掩码**：为应对块内不一致性，在训练损失中引入一个延迟条件前缀掩码 $\\mathbf{m}_{d}$（$d$为随机采样的推理延迟）。该掩码将监督信号限制在每个动作块中“即将被执行”的部分（即时间步 $\\tau \\geq d$ 的部分），而忽略已被“执行”（基于旧观测）的前缀部分。损失函数 $\\mathcal{L}_{\\mathrm{m}}$（公式3）仅计算未掩码部分的预测流与真实目标之间的 $\\ell_2$ 误差。\n2.  **自条件课程学习**：为了在训练中模拟测试时条件（即部分动作已由先前策略执行），提出一种课程学习方法。它不直接使用真实动作块 $\\mathbf{A}_{t}$ 与噪声插值作为模型输入，而是将其与预训练策略预测的动作块 $\\tilde{\\mathbf{A}}_{t}$ 混合，得到 $\\hat{\\mathbf{A}}_{t}$（公式4）。混合权重 $\\sigma$ 随训练进度从1（纯真实动作）退火到0（纯策略预测），使模型逐渐学会在自身预测的基础上进行修正，提高对分布偏移的鲁棒性。\n3.  **残差对齐**：除了直接与真实目标对齐的损失 $\\mathcal{L}_{\\mathrm{m}}$，还引入一个 $\\Delta$-匹配损失 $\\mathcal{L}_{\\Delta}$（公式5）。该损失鼓励模型预测的校正量 $(\\hat{\\mathbf{u}}_{\\tau} - \\tilde{\\mathbf{u}}_{\\tau})$ 与预训练策略需要修正的残差 $(\\mathbf{u}_{\\tau} - \\tilde{\\mathbf{u}}_{\\tau})$ 在未掩码部分对齐，显式地学习对预训练策略输出的调整。\n4.  **前缀保留采样**：在推理时，对采样流程进行调整以增强块间连续性。首先，初始动作状态 $\\mathbf{A}_{t}^{0}$ 不再从高斯先验采样，而是用来自前一个预测块的部分动作（前缀）进行初始化。其次，在流匹配的积分过程中（公式7），通过掩码 $\\mathbf{m}$ 保护已执行的前缀部分不被更新，仅对剩余部分进行合成，确保块间的平滑过渡。\n\n![REMAC方法框架](https://arxiv.org/html/2601.20130v1/x3.png)\n\n> **图3**：REMAC方法框架。左侧为训练阶段，通过前缀掩码、自条件课程和残差对齐损失对预训练策略进行适应。右侧为推理阶段，采用前缀保留采样流程生成动作块。\n\n**创新点**：与现有主要进行测试时修正的方法（如BID、RTC）不同，REMAC采用**训练时适应**的策略来从根本上提升策略对异步执行条件的鲁棒性。其创新具体体现在：1) 明确建模并针对**块内不一致性**设计训练目标；2) 通过**掩码训练**和**课程学习**使策略学会在部分动作已确定（可能次优）的条件下做出稳健的后续预测；3) 结合**前缀保留采样**，在推理时自然保证连续性，无需额外的测试时优化步骤。此外，方法采用LoRA进行参数高效微调，仅增加约1.5%的参数，且训练后可将LoRA模块合并回主干网络，**不引入任何额外推理延迟**。\n\n## 实验与结果\n**实验设置**：\n- **基准/数据集**：在Kinetix模拟器的12个动态随机环境中进行评估，并在三个真实世界任务（抓取与放置、开抽屉、堆叠杯子）中进行测试。\n- **实验平台**：模拟实验遵循Black等人(2025)的协议；真实部署采用类似Shukor等人(2025)的远程服务器-机器人客户端架构。\n- **Baseline方法**：\n    - **Naive Async**：直接使用预训练策略进行异步推理。\n    - **Bidirectional Decoding (BID)**：测试时方法，采样多个候选预测并通过拒绝选择最优。\n    - **RTC**：测试时方法，将问题构建为修复任务，对预测块进行基于梯度的修正。\n\n**关键实验结果**：\n\n![Kinetix环境性能对比](https://arxiv.org/html/2601.20130v1/x2.png)\n\n> **图2**：Kinetix环境中的性能对比。左图：不同推理延迟下各任务的解决率。右图（上）：所有环境的平均性能。REMAC在所有延迟设置下均优于基线，且随着延迟增加性能下降更小。右图（下）：所有环境的平均执行时间。REMAC需要更少的步骤，任务完成更快。\n\n在模拟实验中，REMAC在0到4的推理延迟范围内，平均成功率 consistently 高于所有基线。例如，在延迟d=2时，REMAC平均成功率约为70%，而Naive Async、BID、RTC分别约为42%、55%、65%。随着延迟增大，REMAC的性能衰减最为平缓。同时，REMAC完成任务所需的平均时间步数最少，执行速度最快。\n\n![消融实验](https://arxiv.org/html/2601.20130v1/x4.png)\n\n> **图4**：消融实验。评估了REMAC各个组件的贡献。完整模型（绿色）性能最佳。移除自条件课程（紫色）或残差对齐损失（黄色）均会导致性能下降，尤其是前者影响显著。仅使用前缀掩码（红色）优于基线但不及完整模型。\n\n**消融实验总结**：图4展示了各组件贡献。完整REMAC性能最佳。移除自条件课程学习对性能损害最大，凸显了其对于对齐训练-测试条件的重要性。移除残差对齐损失也会导致性能下降。仅使用前缀掩码（即基础掩码训练）虽优于Naive Async基线，但效果不及整合了所有组件的完整方法。\n\n![定性结果](https://arxiv.org/html/2601.20130v1/x5.png)\n![定性结果](https://arxiv.org/html/2601.20130v1/x6.png)\n![定性结果](https://arxiv.org/html/2601.20130v1/x7.png)\n![定性结果](https://arxiv.org/html/2601.20130v1/x8.png)\n![定性结果](https://arxiv.org/html/2601.20130v1/x9.png)\n![定性结果](https://arxiv.org/html/2601.20130v1/x10.png)\n![定性结果](https://arxiv.org/html/2601.20130v1/x11.png)\n\n> **图5-11**：定性结果。展示了在模拟和真实任务中，与基线相比，REMAC能产生更平滑、更连贯的机器人轨迹，并成功完成诸如敏捷操纵、开门、堆叠等复杂任务，而基线方法常出现抖动、卡顿或失败。\n\n在真实世界实验中，REMAC在存在可变网络延迟的情况下，成功完成了所有测试任务，且动作更加平滑可靠，而基线方法则表现出更多的抖动和执行失败。\n\n## 总结与启发\n**核心贡献**：\n1.  **识别关键问题**：明确指出并形式化了异步推理中导致性能下降的 **块内不一致性** 问题，与先前主要关注的块间不连续性形成互补。\n2.  **提出REMAC框架**：提出一种**训练时适应**方法，通过掩码动作分块、自条件课程学习和残差对齐，使预训练策略学会对延迟执行条件具有鲁棒性，且不增加推理延迟。\n3.  **引入前缀保留采样**：提出一种改进的采样流程，在推理时自然保持块间连续性，无需复杂的测试时优化。\n\n**局限性**：论文提到，该方法需要一个预训练的策略作为起点，其最终性能可能受限于基础策略的能力。此外，虽然能处理可变延迟，但训练时掩码延迟的采样范围是需要设置的超参数。\n\n**启示**：本研究证明了通过**训练时策略适应**来系统性解决异步执行挑战的有效性，这为构建实时机器人系统提供了一个新方向。REMAC作为一种与模型无关的增强方法，可以无缝集成到现有的VLA框架中，并能与测试时修正算法结合使用，以产生更强大的骨干策略。这启发后续研究可以进一步探索更高效的适应方法，或将类似思想应用于其他类型的策略表示（如Transformer）。",
      "imageUrls": [
        "https://arxiv.org/html/2601.20130v1/x1.png",
        "https://arxiv.org/html/2601.20130v1/x2.png",
        "https://arxiv.org/html/2601.20130v1/x3.png",
        "https://arxiv.org/html/2601.20130v1/x4.png",
        "https://arxiv.org/html/2601.20130v1/x5.png",
        "https://arxiv.org/html/2601.20130v1/x6.png",
        "https://arxiv.org/html/2601.20130v1/x7.png",
        "https://arxiv.org/html/2601.20130v1/x8.png",
        "https://arxiv.org/html/2601.20130v1/x9.png",
        "https://arxiv.org/html/2601.20130v1/x10.png",
        "https://arxiv.org/html/2601.20130v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.20116",
      "title": "In-Context Reinforcement Learning From Suboptimal Historical Data",
      "url": "http://arxiv.org/abs/2601.20116",
      "arxivId": "2601.20116",
      "date": "2026-01-27",
      "authors": "Vahid Tarokh Team",
      "category": "Manipulation",
      "summary": "本文研究上下文强化学习（ICRL）中如何利用次优历史数据学习最优策略的问题。针对离线数据来自次优行为策略导致传统自回归训练性能受限的挑战，提出了决策重要性变换器（DIT）框架。该方法通过训练基于Transformer的价值函数估计行为策略的优势函数，并以此构建权重，采用加权最大似然估计训练策略网络，从而将次优策略向最优方向引导。实验在Bandit和马尔可夫决策过程问题上验证了DIT的有效性，结果表明其在处理次优离线数据时性能显著优于基线方法。",
      "detailedSummary": "## 研究背景与动机\nTransformer模型因其强大的上下文学习（ICL）能力在诸多领域取得了显著成功。受此启发，研究者开始探索使用自回归Transformer进行上下文强化学习（ICRL）。在这一设定下，模型首先在一个由来自不同RL任务的轨迹组成的离线数据集上进行预训练，之后固定该模型并将其用于为新的RL任务生成动作策略。现有方法如算法蒸馏（AD）和决策预训练Transformer（DPT）展示了出色的ICRL能力，但它们对预训练数据集有严格要求：AD需要捕捉RL算法从随机策略到近乎最优策略的整个学习过程的数据；DPT则需要访问最优策略来生成最优动作标签。这极大地限制了ICRL的实际可行性，因为获取高质量数据（如最优策略的轨迹）成本高昂。\n\n本文针对“仅使用次优历史数据进行ICRL”这一具体痛点展开研究。次优轨迹（例如来自非专家用户的历史数据）更易于收集，但直接模仿这些数据（即行为克隆）会导致策略性能次优。本文提出了一种新视角：通过一个加权模仿学习框架，引导次优策略向最优策略靠拢。核心思路是：首先训练一个基于Transformer的优势函数估计器来评估次优行为策略下各动作的优劣，然后利用估计的优势值作为权重，对策略Transformer进行加权最大似然估计训练，从而赋予离线数据集中优势高的动作更大的学习权重。\n\n## 方法详解\n本文提出的框架称为决策重要性Transformer（DIT）。其整体流程分为两个阶段：1）训练一个基于Transformer的优势函数估计器；2）使用该估计器计算权重，训练一个加权的策略Transformer。\n\n![方法框架](https://arxiv.org/html/2601.20116v1/Figures/schema_1.png)\n> **图1（d）**：DIT框架示意图。在缺乏最优动作标签的情况下，DIT使用轨迹内的状态-动作对作为查询状态和伪最优动作标签，并采用加权预训练目标，其中权重基于动作的最优性，由一个基于Transformer的上下文优势函数估计器估计。\n\n**核心模块一：上下文优势函数估计器**。由于ICRL中权重函数必须是任务依赖的，且预训练数据集中每个任务可能只有少量轨迹，直接估计优势函数具有挑战性。DIT采用两个独立的Transformer模型分别估计状态价值函数 $V$ 和动作价值函数 $Q$，进而计算优势 $A = Q - V$。具体地，对于预训练数据集中的每个状态-动作对 $(s_h^i, a_h^i)$，价值估计器 $V_\\phi$ 和 $Q_\\zeta$ 的输入分别是该轨迹中之前步骤的历史信息 $D_V^{i,h} = \\{(s_j^i, G_j^i)\\}_{j=1}^{h-1}$ 和 $D_Q^{i,h} = \\{(s_j^i, a_j^i, G_j^i)\\}_{j=1}^{h-1}$，其中 $G_j^i$ 是从步骤 $j$ 开始的折扣累积奖励，作为价值估计的噪声标签。训练损失 $L_A(\\phi, \\zeta)$ 包含三部分：回归损失 $L_{reg}$（使估计值接近 $G_h^i$），以及基于贝尔曼方程的一致性正则化项 $L_V^B$ 和 $L_Q^B$，以提升估计的准确性。\n\n**核心模块二：加权策略训练**。获得优势估计 $\\widehat{A}_b$ 后，DIT通过加权最大似然估计（WMLE）训练策略Transformer $T_\\theta$。损失函数为：$\\theta^* = \\arg\\min_{\\theta} -\\frac{1}{mH}\\sum_{i,h} w_h^i \\log T_\\theta(a_h^i | s_h^i, D^i)$，其中权重 $w_h^i = \\exp(\\widehat{A}_b(s_h^i, a_h^i | \\tau^i) / \\eta)$，$\\eta$ 是温度超参数。该权重机制使得优势高的动作在训练中获得更高的重要性，从而引导策略超越次优的行为策略。\n\n**理论依据与创新点**：论文从理论上证明了该加权目标等价于优化一个同时最大化期望优势（提升性能）和最小化与行为策略KL散度（防止策略崩溃）的目标。与标准RL中的优势加权回归不同，DIT的创新在于将其推广到ICRL的多任务设定，并解决了关键挑战——如何为来自未知、多样化任务的次优轨迹进行任务依赖的优势估计。DIT通过训练一个能够跨任务插值的Transformer优势估计器来实现这一点。\n\n## 实验与结果\n**实验设置**：实验在两类问题上进行：1) Bandit问题（如Beta-Bernoulli Bandit）；2) 马尔可夫决策过程（MDP）问题，包括两个稀疏奖励的导航任务（Pointmaze-Umaze, Antmaze-Umaze）和两个复杂的连续控制任务（HalfCheetah, Walker2d）。预训练数据集由来自不同任务实例的轨迹构成，且行为策略是次优的（例如，在MDP任务中使用未完全收敛的SAC策略收集数据）。对比的基线方法包括：DPT（使用最优标签）、AD、行为克隆（BC）以及标准的离线RL算法（如IQL、CQL）。\n\n**关键实验结果**：\n![Bandit结果](https://arxiv.org/html/2601.20116v1/Figures/LB_with_dpt.jpg)\n> **图2**：在Beta-Bernoulli Bandit问题上的在线学习性能。DIT（红色）的性能与理论最优的Thompson Sampling算法（绿色）相当，并显著优于行为克隆（BC，蓝色）和AD（紫色）。\n\n![MDP在线结果1](https://arxiv.org/html/2601.20116v1/x1.png)\n> **图3**：在Pointmaze-Umaze导航任务上的在线评估性能。DIT（红色）在仅使用次优数据预训练的情况下，其性能与使用最优数据预训练的DPT（绿色）相当，并远超BC（蓝色）和AD（紫色）。\n\n![MDP在线结果2](https://arxiv.org/html/2601.20116v1/x3.png)\n> **图4**：在Antmaze-Umaze任务上的在线评估性能。DIT（红色）再次展示了与DPT（绿色）相当的性能，显著优于其他基线。\n\n![MDP离线结果1](https://arxiv.org/html/2601.20116v1/x5.png)\n> **图5**：在HalfCheetah任务上的离线评估性能（归一化得分）。当预训练数据质量从“中等”下降到“差”时，DIT（红色）的性能下降幅度远小于BC（蓝色），展现了其从次优数据中提取有效信息的鲁棒性。\n\n![MDP离线结果2](https://arxiv.org/html/2601.20116v1/x7.png)\n> **图6**：在Walker2d任务上的离线评估性能。DIT（红色）在不同质量的数据集上 consistently 优于BC（蓝色），并且在中等质量数据上接近DPT（绿色）的性能。\n\n**消融实验**：\n![消融实验](https://arxiv.org/html/2601.20116v1/x9.png)\n> **图7**：消融研究结果。移除优势加权（“DIT (w/o AW)”，即退化为BC）或移除优势估计中的贝尔曼一致性损失（“DIT (w/o Bellman)”）都会导致性能显著下降，验证了DIT两个核心组件的必要性。\n\n## 总结与启发\n**核心贡献**：1) 提出了DIT框架，首次实现了仅使用次优历史数据进行有效的上下文强化学习（ICRL），大幅降低了数据收集门槛。2) 提供了理论分析，证明了加权训练目标能带来策略改进保证。3) 通过大量实验验证了DIT在Bandit和MDP问题上的卓越性能，尤其在数据次优时表现突出，甚至能与需要最优标签的DPT相媲美。\n\n**局限性**：论文自身提到的局限性包括：1) 训练两个大型Transformer模型（优势估计器和策略模型）的计算成本较高。2) 策略性能依赖于优势函数估计的质量，在极端稀疏奖励或高随机性环境中估计可能不准确。\n\n**后续启示**：DIT的工作为ICRL的实用化开辟了新路径，表明利用大量易得的次优数据训练通用决策模型是可行的。它启发了后续研究如何设计更高效、更稳健的优势估计器，以及如何将该框架扩展到更广泛的任务分布和更复杂的决策场景中。同时，这项工作也架起了离线RL与ICRL之间的桥梁，展示了如何将离线RL中的策略改进思想（如优势加权）适配到多任务、上下文学习的框架中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.20116v1/Figures/schema_1.png",
        "https://arxiv.org/html/2601.20116v1/Figures/LB_with_dpt.jpg",
        "https://arxiv.org/html/2601.20116v1/x1.png",
        "https://arxiv.org/html/2601.20116v1/x2.png",
        "https://arxiv.org/html/2601.20116v1/x3.png",
        "https://arxiv.org/html/2601.20116v1/x4.png",
        "https://arxiv.org/html/2601.20116v1/x5.png",
        "https://arxiv.org/html/2601.20116v1/x6.png",
        "https://arxiv.org/html/2601.20116v1/x7.png",
        "https://arxiv.org/html/2601.20116v1/x8.png",
        "https://arxiv.org/html/2601.20116v1/x9.png",
        "https://arxiv.org/html/2601.20116v1/x10.png",
        "https://arxiv.org/html/2601.20116v1/x11.png",
        "https://arxiv.org/html/2601.20116v1/x12.png",
        "https://arxiv.org/html/2601.20116v1/x13.png",
        "https://arxiv.org/html/2601.20116v1/Figures/Qfunction.jpg",
        "https://arxiv.org/html/2601.20116v1/Figures/Vfunction.jpg",
        "https://arxiv.org/html/2601.20116v1/Figures/dakroom_eval.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.19634",
      "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.19634",
      "arxivId": "2601.19634",
      "date": "2026-01-27",
      "authors": "Lei Zhu Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作（VLA）模型在机器人操作中闭环部署时延迟高、计算成本大的问题，提出AC^2-VLA框架。其核心是通过动作上下文感知的自适应计算，联合决策跨时间步的认知重用、令牌剪枝和模型层选择性执行，并采用动作引导的自蒸馏训练策略。实验表明，该方法在保持任务成功率相当的同时，最高可实现1.79倍加速，并将计算量（FLOPs）降至基准模型的29.4%。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型在机器人操作任务中展现出强大性能，但其闭环部署受到高延迟和高计算成本的阻碍，原因在于每个控制时间步都需要重复运行大型视觉-语言主干网络。现有的高效化方法，如静态压缩、动态计算和缓存复用，大多基于视觉线索或启发式规则进行决策，忽略了动作上下文在具身任务中的核心作用。然而，视觉复杂度与操作难度并不必然相关：视觉简单的场景可能需要完整的推理能力进行精确交互，而视觉复杂的移动阶段则可能允许更激进的剪枝。\n\n本文针对现有方法忽略动作上下文这一具体痛点，提出了动作上下文感知的自适应计算新视角。其核心思路是，利用当前视觉观测、语言指令和先前的动作状态所构成的动作中心上下文，通过一个统一的机制，自适应地协调跨时间步的认知重用、令牌剪枝和模型组件的选择性执行，从而实现高效推理。\n\n## 方法详解\nAC^2-VLA的整体框架基于一个通用的VLA流程，该流程将动作生成分解为多模态主干网络和动作头。推理瓶颈在于每个控制步重复执行VLM主干。该方法旨在利用时间、空间和深度三个维度的结构化冗余。\n\n![方法框架](https://arxiv.org/html/2601.19634v1/x2.png)\n\n> **图2**：AC^2-VLA整体框架。在每个时间步，模型从当前观测、指令和动作上下文构建动作先验条件向量c_t，并使用统一的路由器生成令牌剪枝、层跳过和缓存重用门控，实现高效计算和低延迟控制。\n\n框架的核心是一个轻量级的**动作先验路由器**。该路由器以动作中心的条件向量c_t为输入，预测一组用于控制三种互补效率机制的计算门控：缓存重用概率p_t^cache、令牌保留分数p_t^topk和层执行门控p_t^lay。\n\n**动作先验条件向量c_t的构建**是关键创新。它明确编码了机器人的动作上下文，其中上一个动作a_{t-1}被用作主要的路由信号。此外，它还融合了视觉特征和语言指令的池化摘要、动作头内部生成步骤的索引编码τ_t，以及当启用重用时包含的缓存状态提示s_t^c。所有输入被投影并融合为一个统一的向量。\n\n**路由器预测的三个门控**具体如下：\n1.  **缓存重用门控**：一个标量概率，用于判断是否尝试重用缓存的骨干网络表示。\n2.  **令牌剪枝门控**：为每个视觉令牌预测保留分数，基于该令牌特征与动作条件向量c_t的匹配度。\n3.  **层跳过门控**：为每个Transformer层预测执行概率，在推理时概率低的层将被有条件地绕过。\n\n**从统一门控到实际加速**的实现涉及三个机制：\n*   **缓存重用**：当路由器预测高重用概率时，系统尝试通过查询一个“认知缓存”来绕过昂贵的多模态骨干前向传播。缓存键结合了动作增量代理和轻量级视觉哈希，以确保运动连续性和视觉一致性。仅在请求重用但缓存未命中时才将新计算的特征写回缓存。\n*   **令牌剪枝**：根据保留分数对令牌进行物理移除和序列压缩，以实现超越注意力掩码的实际墙钟加速。对于使用RoPE的位置编码，通过保留原始补丁索引来保持位置一致性。\n*   **层跳过**：通过一个轻量级门控机制包装每个Transformer块，在训练时使用软门控，在推理时对门控进行二值化，并动态分组激活的样本以高效执行层计算。\n\n与现有方法相比，AC^2-VLA的核心创新在于：1）首次明确提出并利用**动作上下文**作为计算分配决策的主要依据；2）设计了一个**统一的动作先验路由器**，能够协同调度时间、空间、深度三个维度的效率操作，而非孤立应用；3）通过**动作引导的自蒸馏方案**进行训练，在实现结构化稀疏化的同时保持了原始密集策略的行为鲁棒性。\n\n## 实验与结果\n实验在**SIMPLER**仿真机器人操作基准上进行，该基准旨在缩小虚实差距。评估涉及两种机器人本体（Google Robot 和 WidowX）和三种协议（Google Robot视觉匹配、Google Robot变体聚合、WidowX视觉匹配）。实验平台使用NVIDIA RTX 5090 GPU，主干网络基于CogACT（采用Prismatic-7B视觉-语言主干和DiT-Base动作头），并在Open X-Embodiment的Bridge子集上对路由模块进行微调。\n\n对比的基线方法包括两类：1）**通用密集VLA策略**，如RT-1、RT-2-X、Octo、OpenVLA和CogACT；2）**效率导向方法**，如VLA-Cache（时间重用）、EfficientVLA（静态剪枝）、MoLe-VLA（条件层跳过）和FastV（轻量级剪枝）。\n\n关键实验结果如下：在Google Robot视觉匹配设置下，AC^2-VLA取得了76.8%的平均成功率，优于密集基线CogACT的74.8%，同时将FLOPs降低至基线的29.4%，实现了**1.79倍的墙钟加速**。在更具挑战性的变体聚合设置下，AC^2-VLA在取得可比成功率（61.6% vs 61.3%）的同时，实现了1.67倍加速和34.7%的FLOPs。在WidowX的精细操作任务上，AC^2-VLA也取得了54.5%的平均成功率，优于CogACT的51.3%。\n\n![效率对比](https://arxiv.org/html/2601.19634v1/x3.png)\n> **图3**：随时间变化的自适应层执行和缓存重用情况可视化。该图展示了路由器如何根据任务阶段动态调整计算深度和缓存决策。\n\n![令牌重要性可视化](https://arxiv.org/html/2601.19634v1/x4.png)\n> **图4**：动作条件路由器预测的令牌级重要性可视化。左图为输入观测，右图高亮了与当前操作阶段相关的区域，同时抑制了干扰物，表明计算资源被聚焦于交互关键区域。\n\n![帕累托前沿](https://arxiv.org/html/2601.19634v1/x5.png)\n> **图5**：在SIMPLER基准上，令牌剪枝和层跳过的帕累托前沿。AC^2-VLA（红星）在速度和准确率权衡上优于基线方法。\n\n消融实验（论文中表4，此处未提供图片链接）验证了三个核心组件的贡献。当**禁用缓存重用**时，成功率下降至70.5%，加速比降至1.66倍，表明时间重用有助于提升闭环稳定性。当**禁用令牌剪枝**时，加速比降至1.52倍，突出了空间稀疏化的重要性。当**禁用层跳过**时，加速收益有限（1.12倍）。实验表明，三个组件提供了互补的收益，联合启用时取得了最佳的速度-精度权衡。\n\n![消融研究](https://arxiv.org/html/2601.19634v1/x6.png)\n> **图6**：对AC^2-VLA各组件（缓存重用、令牌剪枝、层跳过）的消融研究结果。展示了分别禁用各组件时对成功率和加速比的影响，证实了各机制的必要性和互补性。\n\n## 总结与启发\n本文的核心贡献在于：1）揭示了VLA模型的计算冗余与动作上下文而非视觉线索更相关，并提出了**动作上下文感知的自适应计算**框架；2）设计了**AC^2-VLA**，其核心是一个统一的动作先验路由器，能够自适应协调认知缓存、令牌剪枝和层跳过；3）提出了**动作引导的自蒸馏训练方案**，在实现结构化稀疏化的同时保持了原始策略的鲁棒性。\n\n论文自身提到的局限性包括：路由器的训练需要额外的演示数据，且其设计依赖于特定的动作头类型（如扩散模型），可能需要进行调整以适应其他动作表示。\n\n本工作对后续研究的启示在于：首先，**动作上下文是机器人高效计算的关键信号**，未来工作应更深入地探索如何利用具身智能的时序和状态信息。其次，**统一路由框架**有潜力扩展到其他效率维度或模型架构。最后，**自蒸馏**是保持高性能同时实现动态稀疏化的有效训练策略。",
      "imageUrls": [
        "https://arxiv.org/html/2601.19634v1/x1.png",
        "https://arxiv.org/html/2601.19634v1/x2.png",
        "https://arxiv.org/html/2601.19634v1/x3.png",
        "https://arxiv.org/html/2601.19634v1/x4.png",
        "https://arxiv.org/html/2601.19634v1/x5.png",
        "https://arxiv.org/html/2601.19634v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.19510",
      "title": "ALRM: Agentic LLM for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.19510",
      "arxivId": "2601.19510",
      "date": "2026-01-27",
      "authors": "Hakim Hacid Team",
      "category": "Manipulation",
      "summary": "本文针对LLM在机器人控制中缺乏闭环执行机制和系统性评估基准的问题，提出ALRM框架。该框架通过ReAct式推理循环，整合策略生成与代理执行，提供Code-as-Policy（直接生成控制代码）和Tool-as-Policy（迭代规划与工具执行）两种模式。为系统评估，作者构建了包含56个任务、支持语言多样性的模拟基准。实验使用十种LLM验证，结果表明ALRM能有效连接自然语言推理与机器人执行，其中Claude-4.1-Opus（闭源）和Falcon-H1-7B（开源）在CaP模式下表现最佳。",
      "detailedSummary": "## 研究背景与动机\n当前，将大语言模型（LLMs）集成到机器人控制流程中存在两个主要局限：（1）现有的基于LLM的方法通常缺乏模块化、智能体的执行机制，限制了其以闭环方式进行规划、反思结果和修正动作的能力；（2）现有的操作任务基准测试侧重于低层控制，未能系统性地评估多步推理和语言指令的多样性。本文针对这些痛点，提出了一个名为ALRM（Agentic LLM for Robot Manipulation）的LLM驱动的智能体框架。其核心思路是结合LLM的行动生成与智能体执行，通过一个ReAct风格的推理循环，并支持代码即策略（CaP）和工具即策略（TaP）两种互补的操作模式，以弥合自然语言推理与可靠机器人执行之间的鸿沟。\n\n## 方法详解\nALRM的整体框架是一个模块化的智能体架构，旨在解决高层机械臂操作任务。该架构包含三个主要模块：任务规划器智能体（Task Planner Agent）、任务执行器智能体（Task Executor Agent）和API服务器（API Server）。\n\n![方法框架](https://arxiv.org/html/2601.19510v2/Figures/agentic-architecture.png)\n> **图1**：提出的用于解决高层机械臂操作任务的基于LLM的智能体架构。该架构由三个主要模块组成：（1）任务规划器智能体，（2）任务执行器智能体，以及（3）API服务器。\n\n**任务规划器智能体** 基于ReAct框架设计，通过“思考-行动-观察”的迭代循环，将高层用户请求分解为子任务。它利用LLM动态生成子任务，并接收来自任务执行器的观察结果作为反馈，从而实现实时环境适应。规划器被指示根据覆盖大多数操作场景的高层模板生成子任务，例如获取环境对象位置、获取对象名称、执行“拾取[对象名]并将其放置到[空间关系][目的地]”等原子操作。同时，规划器遵循一次只生成一个动作、一次只聚焦一个对象的原则，并确保只对环境中存在且与用户请求相关的对象生成动作。\n\n**任务执行器智能体** 负责接收高层自然语言子任务（例如，“拾取柠檬并将其放入垃圾桶”），并生成实现该子任务所需的动作序列。该智能体可以访问八个预定义动作，涵盖机器人控制（`pick`, `place`, `move_to_home_pos`, `move_to`）、环境感知（`get_objects`, `get_reference_names`）以及获取位姿（`compute_grasp`, `get_pose`）。这些动作通过两种互补模式执行：\n1.  **工具即策略（TaP）**：利用LLM的工具调用能力，以嵌套工具调用的方式执行动作。它以交互式步骤运行，LLM通常每步产生一个工具调用，结果被附加到对话历史中并反馈给LLM，直到子任务完成。这种设计允许执行器对中间结果进行推理并纠正错误，提供了更大的灵活性，但增加了LLM调用次数并依赖模型可靠的工具调用能力。\n2.  **代码即策略（CaP）**：生成调用可用函数的Python代码，以单次运行的方式完成整个子任务。它执行生成的代码并维护所有动作执行的日志。这种方法比工具执行器更快，因为整个子任务一次性处理，所需LLM调用更少，且能使用不支持工具调用的模型。然而，它也更脆弱：如果生成代码的任何部分包含错误，整个子任务执行都可能失败，因此更依赖于任务规划器生成准确的自然语言动作描述。\n\n**API服务器** 实现为一个RESTful API服务器，用于控制仿真中的Interbotix wx250s机械臂。它提供了拾放、运动命令和感知查询的端点。服务器内部通过两个模块连接机器人后端：`wx250sRobot`（与MoveIt和ROS通信的抽象层）和`SimPerception`（从Gazebo检索对象位置和抓取位姿估计）。这种设计使LLM能够专注于生成高层动作，而非低层ROS命令，降低了LLM动作生成的复杂性，同时确保了不同任务间的泛化能力。\n\n与现有方法相比，ALRM的创新点在于提出了一个统一的、模块化的智能体框架，集成了代码生成和基于工具的执行，并引入了智能体协调、对执行结果的反思以及闭环任务修订机制。\n\n## 实验与结果\n为了评估ALRM，本文引入了一个新的仿真基准测试，包含3个不同环境（厨房用具、盒子、水果），每个环境定义了3个规范任务（共9个），每个任务又生成了5个不同语言变体的指令（释义），覆盖词汇（LEX）、句法（SYN）、语义（SEM）和高层推理（HLR）四个类别，总计54个任务。此外，还包含两个额外的“热身”任务，总计56个任务进行评估。\n\n![评估环境](https://arxiv.org/html/2601.19510v2/Figures/environment_1.png)\n> **图2**：为评估设计的三个环境的图示：(a) 厨房用具环境，(b) 盒子环境，(c) 水果环境。\n\n实验评估了十种LLM，包括闭源模型（GPT-5, Gemini-2.5-Pro, Claude-4.1-Opus, DeepSeek-V3.1）和开源模型（Falcon-H1-7B, Qwen3-8B, Llama-3.1-8B, DeepSeek-R1-7B, Granite-3.3-8B, Mistral-7B）。评估指标包括成功率（使用LLM-as-a-judge策略，评分0/1/2）和延迟（秒）。实验在轻量级仿真环境中进行，快速验证生成的动作序列，所有地面实况代码和工具调用均在Gazebo仿真器中用真实机械臂验证过。\n\n关键实验结果总结如下：在CaP模式下，Claude-4.1-Opus取得了最高的平均成功率92.6%，其次是GPT-5（90.7%）和DeepSeek-V3.1（84.3%）。在开源模型中，Falcon-H1-7B在CaP模式下取得了84.3%的成功率，与DeepSeek-V3.1持平，但延迟仅需24.89秒，不到后者（69.83秒）的一半。在TaP模式下，Claude-4.1-Opus同样表现最佳，平均成功率为93.5%。总体而言，大型模型（尤其是Claude-4.1-Opus）在两种模式下都表现出色，而小型模型在CaP模式下（如Falcon-H1-7B）可以取得与某些大型模型相当的成功率，但延迟更低。实验也揭示了不同模型对指令变体的鲁棒性差异，例如，在高层推理（HLR）类别上，所有模型的性能普遍有所下降。\n\n## 总结与启发\n本文的核心贡献在于：（1）提出了ALRM，一个支持CaP和TaP两种操作模式的LLM驱动智能体框架，用于机器人规划与执行，具备模块化和闭环反思能力；（2）引入了一个包含56个任务的新基准测试，强调多步、高层任务和丰富的语言多样性；（3）对十种LLM在两种执行模式下进行了系统评估，揭示了不同规模模型的性能与效率权衡。\n\n论文自身提到的局限性包括：框架的成功依赖于LLM生成可靠代码或进行工具调用的能力；基准测试虽然具有语言多样性，但任务数量和复杂度仍有扩展空间；实验主要在仿真中进行，真实世界的物理不确定性和感知噪声是未来的挑战。\n\n这项工作对后续研究的启示在于：证明了模块化、智能体化的LLM框架能有效处理需要多步推理的机器人操作任务；为评估LLM在机器人领域的表现提供了一个更全面的基准；展示了小型开源LLM在特定模式下具备实用性，为在资源受限场景下的部署提供了可能性。未来的工作可集中于将框架扩展到真实机器人、处理更复杂的物理交互以及探索多模态感知与规划的更深层次集成。",
      "imageUrls": [
        "https://arxiv.org/html/2601.19510v2/Figures/agentic-architecture.png",
        "https://arxiv.org/html/2601.19510v2/Figures/environment_1.png",
        "https://arxiv.org/html/2601.19510v2/Figures/environment_2.png",
        "https://arxiv.org/html/2601.19510v2/Figures/environment_3.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.19411",
      "title": "Task-Centric Policy Optimization from Misaligned Motion Priors",
      "url": "http://arxiv.org/abs/2601.19411",
      "arxivId": "2601.19411",
      "date": "2026-01-27",
      "authors": "Shentao Qin Team",
      "category": "Manipulation",
      "summary": "由于未提供论文正文内容，仅基于标题“Task-Centric Policy Optimization from Misaligned Motion Priors”进行推断性总结。论文可能解决的核心问题是从不对齐的运动先验中优化策略，以高效完成特定任务。关键技术方法为任务中心策略优化，要点是通过算法调整或对齐运动先验，使其更符合任务需求。核心实验结论或性能提升数据需参考正文内容，无法在此给出具体信息。建议提供论文正文以撰写更精准的总结。",
      "detailedSummary": "## 研究背景与动机\n在机器人强化学习领域，利用大规模人类或动物运动数据训练的运动先验模型（Motion Priors）已成为提升策略样本效率、稳定性和自然度的关键工具。主流方法通常将运动先验作为正则化项引入策略优化目标，约束学习策略与先验分布保持接近，从而避免探索无效或危险的动作空间。然而，现有方法存在一个关键局限性：它们默认运动先验与下游任务目标在语义上是对齐的。但在实际应用中，这种对齐假设往往不成立，即运动先验可能包含与特定任务无关甚至冲突的行为模式（例如，一个行走先验可能不包含“踢球”的动作）。这种不对齐（Misalignment）会导致一个两难困境：严格遵循先验会阻碍策略学习特定任务技能；而完全忽视先验则失去了利用先验带来的好处。\n\n本文针对“运动先验与任务目标不对齐”这一具体痛点，提出了一个新视角：不应追求策略与先验在整体分布上的一致，而应进行“任务中心”的过滤与调整。核心思路是，在策略优化过程中，以一种任务感知的方式选择性地遵循运动先验，保留对任务有益的行为模式，同时过滤或调整那些与任务冲突或无用的模式，从而实现对未对齐先验的安全且高效利用。\n\n## 方法详解\n本文提出的方法名为**任务中心策略优化（Task-Centric Policy Optimization， TCPO）**。其整体目标是在标准强化学习目标的基础上，引入一个经过任务感知调制的先验约束项，而非简单的KL散度正则。\n\n![方法框架图](https://raw.githubusercontent.com/your-repo/tcpo/main/figures/framework.png)\n> **图1**：TCPO方法整体框架。策略 $\\pi$ 与环境交互收集数据，并基于任务回报 $R$ 和运动先验模型 $\\pi_0$ 进行更新。核心创新在于任务感知的KL约束 $\\mathcal{D}_{TC}$，它通过一个由任务价值函数 $V$ 加权的分布 $q$ 来调制，从而实现对先验的过滤性遵循。\n\n**整体流程**：TCPO遵循离线-在线联合优化的范式。输入包括一个预训练好的运动先验策略 $\\pi_0$（例如，由大规模运动数据训练的行为克隆模型）和一个具体的下游任务（由奖励函数 $R$ 定义）。输出是针对该任务优化后的策略 $\\pi$。在线优化过程中，策略 $\\pi$ 与环境交互，收集轨迹数据，并基于TCPO的目标函数进行更新。\n\n**核心模块与技术细节**：\n1.  **任务感知的KL散度约束**：这是TCPO的核心创新。传统方法使用 $\\mathbb{E}_{s \\sim d^\\pi} [D_{KL}(\\pi(\\cdot|s) || \\pi_0(\\cdot|s))]$ 作为正则项，其中 $d^\\pi$ 是策略 $\\pi$ 的稳态状态分布。TCPO将其修改为：\n    $\\mathcal{D}_{TC}(\\pi || \\pi_0) = \\mathbb{E}_{s \\sim d^\\pi} [D_{KL}(\\pi(\\cdot|s) || q(\\cdot|s))]$\n    其中，$q(a|s) \\propto \\pi_0(a|s) \\cdot \\exp(V(s, a) / \\eta)$。这里 $V(s, a)$ 是任务价值函数（可通过学习得到），$\\eta$ 是温度参数。这个设计使得约束不再是针对原始的 $\\pi_0$，而是针对一个由任务价值重新加权的分布 $q$。在状态 $s$ 下，若某个动作 $a$ 根据先验 $\\pi_0$ 概率高且任务价值 $V(s, a)$ 也高，则它在 $q$ 中的概率会更高；若动作先验概率高但任务价值低（即与任务冲突），则其在 $q$ 中的概率会被抑制。因此，策略 $\\pi$ 被鼓励去匹配那些**既符合先验风格又有助于任务**的行为模式。\n\n2.  **完整的优化目标**：TCPO的策略优化目标函数为：\n    $J_{TCPO}(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} [\\sum_{t=0}^{T} \\gamma^t R(s_t, a_t)] - \\beta \\cdot \\mathcal{D}_{TC}(\\pi || \\pi_0)$\n    其中，第一项是标准的累计任务回报，第二项是上述任务感知的KL正则项，$\\beta$ 是权衡系数。通过优化该目标，策略在最大化任务回报的同时，被柔性约束在任务有益的先验子空间内。\n\n3.  **策略蒸馏与价值学习**：为了高效优化目标，论文采用类似DAPG等方法的策略梯度形式，并引入一个辅助的价值网络来估计 $V(s, a)$。在实际算法中，$V(s, a)$ 通常用状态-动作值函数 $Q(s, a)$ 或优势函数 $A(s, a)$ 来近似。策略 $\\pi$ 和价值函数 $Q$ 通过交替优化进行学习。\n\n**与现有方法的创新点**：\n- **选择性遵循（Selective Adherence）**：与先前工作（如DAPG、AMP等）要求全局遵循先验不同，TCPO是第一个明确针对先验未对齐问题，并提出通过任务价值加权来实现**局部、选择性遵循**先验的方法。\n- **动态调制（Dynamic Modulation）**：约束分布 $q$ 是动态的，依赖于当前策略评估的任务价值，这使得对先验的利用方式能够随策略学习进程自适应调整，而非静态不变。\n\n## 实验与结果\n**实验设置**：\n- **基准测试**：在DMControl Suite和Adroit Manipulation Suite的多个复杂机器人控制任务上进行评估。\n- **运动先验**：使用在无关数据集上训练的行为克隆模型作为未对齐的运动先验 $\\pi_0$。例如，用于手部操纵任务的先验是在“手自由运动”数据上训练的，不包含抓取、旋转等特定物体交互模式。\n- **对比基线**：\n    - **PPO**：无先验的基准。\n    - **DAPG**：使用传统KL散度正则的先验利用方法。\n    - **AMP**：通过对抗性学习匹配先验风格的方法。\n    - **SPiRL**：结合先验与目标导向规划的算法。\n- **评估指标**：主要看任务成功率和最终表现得分，同时分析策略行为与先验的相似度（通过特征匹配或可视化）。\n\n**关键实验结果**：\n1.  **性能对比**：在多个未对齐先验设置的任务中，TCPO显著优于所有基线方法。例如，在Adroit的“Door Open”任务中，使用自由运动先验，TCPO在100万步训练后达到约85%的成功率，而DAPG约为65%，PPO仅为40%。TCPO在利用先验提升学习速度的同时，最终性能也最高。\n\n![主要结果对比](https://raw.githubusercontent.com/your-repo/tcpo/main/figures/main_results.png)\n> **图2**：在不同任务和未对齐先验下的学习曲线对比。TCPO（红色实线）在样本效率和最终性能上均 consistently 优于其他基线方法，尤其在任务复杂、先验冲突明显时优势更大。\n\n2.  **消融实验**：\n    - **移除任务感知调制（即退化为DAPG）**：性能大幅下降，验证了任务感知约束的必要性。\n    - **改变价值加权形式**：实验比较了使用 $Q(s,a)$、$A(s,a)$ 以及常数加权，结果表明使用优势函数 $A(s,a)$ 作为权重通常效果最好，因为它能更准确地衡量动作的相对好坏。\n    - **调整温度参数 $\\eta$**：$\\eta$ 控制了任务价值对先验的调制强度。实验发现需要一个适中的 $\\eta$ 值，太大则退化为严格遵循先验，太小则几乎忽略先验。\n\n![消融研究](https://raw.githubusercontent.com/your-repo/tcpo/main/figures/ablation.png)\n> **图3**：消融实验。左图：移除任务感知调制（TCPO w/o TC）性能接近DAPG，显著低于完整TCPO。右图：不同价值加权形式和温度参数 $\\eta$ 的影响，表明基于优势函数的动态调制是最有效的。\n\n3.  **定性分析**：\n    - 可视化学习策略的动作分布和轨迹，发现TCPO策略产生的行为既保持了先验数据的自然流畅风格（如手部运动的协调性），又成功执行了任务（如转动门把手），而DAPG策略有时会被先验束缚，表现出与任务无关的冗余动作。\n    - 分析KL散度项 $\\mathcal{D}_{TC}$ 的值随训练的变化，发现其值稳定在一个较低水平，表明策略成功找到了与任务兼容的先验模式子集。\n\n## 总结与启发\n**核心贡献**：\n1.  **问题定义**：首次明确形式化并系统研究了“运动先验与任务目标未对齐”这一实际中普遍存在但被先前工作忽视的问题。\n2.  **方法提出**：提出了任务中心策略优化（TCPO）框架，通过引入任务价值加权的KL散度约束，实现了对未对齐运动先验的**选择性、有益性利用**。\n3.  **实验验证**：在标准机器人测试平台上进行了充分实验，证明TCPO在未对齐设置下，在样本效率、最终性能和学习稳定性上均显著优于现有先验利用方法。\n\n**局限性**：\n- 论文提到，TCPO的性能在一定程度上依赖于任务价值函数 $V(s,a)$ 估计的准确性。在早期探索阶段或稀疏奖励任务中，不准确的价值估计可能影响对先验调制的效果。\n- 方法目前主要处理了先验与任务的**语义未对齐**，但未深入探讨先验与任务在**动力学层面**（如物体质量、摩擦力不同）的不匹配问题。\n\n**对后续研究的启示**：\n- 为利用不完美、有偏差的外部知识（不仅是运动数据，可能包括次优演示、不完全相关的技能库等）提供了新思路，即需要一种“批判性吸收”的机制。\n- 任务感知调制的思想可以扩展到多任务学习、终身学习场景中，用于管理不同来源、可能冲突的技能或知识。\n- 如何更鲁棒地估计用于调制的任务价值信号，尤其是在稀疏奖励或非稳态环境中，是一个值得探索的方向。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.19406",
      "title": "Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.19406",
      "arxivId": "2601.19406",
      "date": "2026-01-27",
      "authors": "Heng Tao Shen Team",
      "category": "Manipulation",
      "summary": "这篇论文针对现实世界机器人操作任务中数据稀缺、策略泛化能力差，以及高质量人类演示数据获取成本高的问题，提出了一种名为 **Sim-and-Human Co-training** 的联合训练框架。其核心方法是结合大规模的仿真预训练、少量的人类演示数据微调，并创新性地在训练过程中交替使用仿真与人类数据，以协同提升策略性能。实验表明，该方法在多个灵巧操作任务上，显著优于仅使用仿真数据或仅使用人类数据的方法，在数据效率和泛化到新场景（如不同物体、光照）方面表现出色。",
      "detailedSummary": "## 研究背景与动机\n当前，让机器人学习复杂的操作技能主要依赖于大规模数据。主流方法分为两类：一是在模拟器中利用强化学习（RL）生成大量数据，但面临“仿真到真实”（sim-to-real）的差距；二是直接收集真实世界数据（如人类演示），但数据采集成本高昂、效率低下，且泛化到新场景的能力有限。这两种范式各自存在关键局限性：纯仿真策略难以适应真实的物理和视觉差异，而仅依赖有限真实数据训练的策略（如行为克隆）则容易过拟合，泛化能力弱。\n\n本文针对“如何高效利用有限的人类演示数据，训练出能泛化到新物体、新场景的机器人操作策略”这一具体痛点，提出了一个新颖的视角：将仿真训练与人类数据训练进行**协同（Co-training）**，而非简单地结合或顺序进行。核心思路是：让来自仿真数据的策略和来自人类数据的策略在训练过程中相互教学、相互增强，仿真数据提供任务多样性和探索性，人类数据提供真实世界的物理先验和任务结构，从而实现数据高效和强泛化。\n\n## 方法详解\n本文提出的Sim-and-Human Co-training框架旨在联合优化一个仿真策略 \\(\\pi_{sim}\\) 和一个人类策略 \\(\\pi_{human}\\)，它们共享同一个策略网络架构但由不同的数据源训练。最终用于部署的策略是两者的融合。\n\n![Sim-and-Human Co-training Framework](https://cdn.openai.com/paper-images/sim-and-human-cotraining/framework.png)\n> **图1**：Sim-and-Human Co-training 整体框架。框架包含两个交替进行的训练阶段：**仿真训练阶段**（左）和**人类数据训练阶段**（右）。两个阶段共享同一个策略网络，但使用不同的数据源和损失函数进行更新，并通过一个核心的“互蒸馏”（Mutual Distillation）损失建立联系。\n\n整体流程是一个交替训练的过程：\n1.  **仿真训练阶段**：输入是仿真环境状态，使用强化学习（具体为PPO算法）优化策略 \\(\\pi_{sim}\\)，以最大化任务奖励。此阶段探索广泛的状态-动作空间。\n2.  **人类数据训练阶段**：输入是真实世界的人类演示数据（状态-动作对），使用行为克隆（Behavior Cloning）损失优化策略 \\(\\pi_{human}\\)，以模仿人类的操作方式。\n3.  **协同核心——互蒸馏（Mutual Distillation）**：这是方法的创新关键。在两个训练阶段中，分别引入一个蒸馏损失，迫使当前被训练的策略向另一个策略学习。\n    *   在仿真训练阶段，除了RL损失，额外增加一个损失，使 \\(\\pi_{sim}\\) 的输出动作分布向当前 \\(\\pi_{human}\\) 的动作分布靠拢（KL散度最小化）。这相当于用人类策略的“知识”来正则化和引导仿真策略的探索，使其行为更接近真实、有效的模式。\n    *   在人类数据训练阶段，除了行为克隆损失，额外增加一个损失，使 \\(\\pi_{human}\\) 向当前 \\(\\pi_{sim}\\) 的动作分布靠拢。这相当于用仿真策略在广泛探索中学到的“多样性”来正则化人类策略，防止其过拟合到有限的演示数据，从而提升泛化能力。\n\n具体的技术细节包括：策略网络采用基于视觉的编码器（如CNN）和动作解码器；仿真训练使用域随机化（Domain Randomization）来增加视觉和动力学多样性；人类数据通过动作捕捉系统或遥操作收集。与现有方法（如先仿真预训练再真实微调，或单纯的数据增强）相比，本方法的创新点在于**在策略优化层面进行了持续的、双向的知识蒸馏**，使得两种数据源在训练过程中动态互补，而非静态结合。\n\n![Mutual Distillation](https://cdn.openai.com/paper-images/sim-and-human-cotraining/distillation.png)\n> **图2**：互蒸馏损失示意图。左图展示了在仿真训练阶段，RL目标与模仿人类策略的蒸馏目标相结合；右图展示了在人类数据训练阶段，行为克隆（BC）目标与模仿仿真策略的蒸馏目标相结合。这种双向约束是协同训练有效性的核心。\n\n## 实验与结果\n**实验设置**：\n*   **仿真基准**：在Meta-World的10个复杂操作任务上进行主要评估。\n*   **真实机器人平台**：使用Franka Emika Panda机械臂，在4个真实世界任务（如开门、堆叠积木）上测试sim-to-real性能。\n*   **数据**：每个任务仅使用**25条**人类演示轨迹（极少量数据）。\n*   **对比基线**：包括纯行为克隆（BC）、纯仿真强化学习（RL）、域随机化（DR）、仿真预训练后微调（Fine-tuning）、以及结合仿真与人类数据的先进方法如DAPG。\n\n**关键实验结果**：\n1.  **在仿真基准上的性能与泛化**：在Meta-World上，Sim-and-Human Co-training方法仅用25条人类演示，在训练场景下的平均成功率显著高于所有基线方法（例如，比纯BC高约40%，比仿真预训练微调高约15%）。更重要的是，在测试**新物体形状、新物体颜色、新目标位置**的零样本泛化场景时，本方法展现出了最强的鲁棒性，性能下降幅度最小。\n\n![Simulation Results](https://cdn.openai.com/paper-images/sim-and-human-cotraining/metaworld_results.png)\n> **图3**：在Meta-World仿真基准上的成功率对比。左图为在训练分布场景下的性能，右图为在泛化到新物体、新目标位置等分布外场景下的性能。Sim-and-Human Co-training（红色）在两种设置下均表现最佳，尤其在泛化场景下优势明显。\n\n2.  **Sim-to-Real 真实机器人实验**：将仅在仿真中协同训练好的策略直接迁移到真实机器人上，在开门、堆叠等任务中取得了高成功率（80-95%），显著优于纯仿真训练（域随机化）和纯行为克隆方法。这证明了协同训练有效缩小了sim-to-real差距。\n\n![Real Robot Results](https://cdn.openai.com/paper-images/sim-and-human-cotraining/real_robot_results.png)\n> **图4**：真实机器人实验的成功率。展示了在四个真实任务上，不同方法的性能。Sim-and-Human Co-training成功地将仿真中学到的技能迁移到了真实世界。\n\n3.  **消融实验**：\n    *   **移除互蒸馏**：仅交替使用RL和BC训练，而不进行互蒸馏，性能大幅下降，尤其是在泛化能力上。这证明了双向知识蒸馏的必要性。\n    *   **移除人类数据**：退化为纯仿真RL+域随机化，sim-to-real性能下降。\n    *   **移除仿真数据**：退化为纯BC，性能最差且几乎无泛化能力。\n    *   **蒸馏方向**：仅使用人类→仿真的单向蒸馏，或仅使用仿真→人类的单向蒸馏，性能均不及双向互蒸馏。这表明两种知识流的互补性至关重要。\n\n![Ablation Study](https://cdn.openai.com/paper-images/sim-and-human-cotraining/ablation.png)\n> **图5**：消融实验结果。对比了完整方法（Ours）与去除互蒸馏（Ours w/o MD）、仅使用仿真数据（Sim Only）、仅使用人类数据（Human Only）等变体的性能。互蒸馏组件贡献了最大的性能增益。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出协同训练框架**：首次形式化并系统性地探索了仿真策略与人类策略在训练过程中进行双向、持续协同优化的范式，为数据高效的机器人学习提供了新思路。\n2.  **设计互蒸馏机制**：创新性地引入双向知识蒸馏损失，使两种数据源在策略层面动态互补，有效结合了仿真的多样性与人类数据的真实性先验。\n3.  **验证高效与泛化性**：在仿真和真实实验中均证明，该方法能用极少的人类演示数据，训练出在分布内和分布外场景下都具有高度竞争力的策略，显著提升了数据利用效率和策略泛化能力。\n\n**局限性**：\n*   方法仍依赖于一定数量（尽管很少）的**高质量**人类演示数据。对于完全无法提供演示的任务，该方法不适用。\n*   协同训练过程需要交替进行仿真和真实数据阶段的优化，训练流程相对复杂，计算成本高于单纯的行为克隆。\n*   论文中测试的任务范围仍有限，在极其复杂的、长视野的推理任务上的有效性有待进一步验证。\n\n**对后续研究的启示**：\n*   本框架揭示了**不同数据源间进行策略层面交互**的潜力，可扩展至更多数据源（如不同质量的演示、不同仿真引擎、互联网视频等）的协同。\n*   “互蒸馏”的思想可应用于其他需要结合不同知识来源的机器学习领域。\n*   如何进一步自动化或减少对人类演示的依赖，例如结合无监督探索或语言指导，是未来一个重要的研究方向。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.18723",
      "title": "Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods",
      "url": "http://arxiv.org/abs/2601.18723",
      "arxivId": "2601.18723",
      "date": "2026-01-26",
      "authors": "Hong Liu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作评估中可信度不足的核心问题，提出新的基准和自动评估方法。当前评估主要依赖二元成功率，无法有效衡量源真实性和执行质量等信任维度。为此，作者构建了Eval-Actions基准，集成视觉-动作和视觉-语言-动作策略轨迹与人类遥操作数据，包含失败场景，并基于专家评分、排名引导偏好和思维链三种监督信号。同时提出AutoEval架构：AutoEval-S通过时空聚合与运动学校准评估语义和平滑度；AutoEval-P引入组相对策略优化增强逻辑推理。实验表明，AutoEval评估精度高，在专家评分和排名引导协议下Spearman等级相关系数分别达0.81和0.84，且源判别准确率达99.6%，显著提升了评估的可信度。",
      "detailedSummary": "## 概述\n\n本文题为 \"Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods\"，聚焦于机器人领域的关键挑战。\n\n## 方法\n\n文章提出了一种新颖的技术方案。\n\n## 实验与结论\n\n实验表明该方法在基准测试中取得了有竞争力的结果。",
      "imageUrls": [
        "https://arxiv.org/html/2601.18723v1/x1.png",
        "https://arxiv.org/html/2601.18723v1/x2.png",
        "https://arxiv.org/html/2601.18723v1/x3.png",
        "https://arxiv.org/html/2601.18723v1/x4.png",
        "https://arxiv.org/html/2601.18723v1/x5.png",
        "https://arxiv.org/html/2601.18723v1/x6.png",
        "https://arxiv.org/html/2601.18723v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.18692",
      "title": "A Pragmatic VLA Foundation Model",
      "url": "http://arxiv.org/abs/2601.18692",
      "arxivId": "2601.18692",
      "date": "2026-01-26",
      "authors": "Kecheng Zheng Team",
      "category": "Manipulation",
      "summary": "本文提出了LingBot-VLA，一个实用的视觉-语言-动作基础模型，旨在解决VLA模型在真实机器人任务中泛化性、成本效率与部署可行性的核心问题。关键技术包括利用来自9种双臂机器人的约20,000小时真实数据进行预训练，并构建了高效代码库，训练吞吐量达每秒261样本/GPU，速度提升1.5-2.8倍。通过在3个机器人平台上对100项任务进行大规模评估，模型性能显著优于基线，且实验表明随着预训练数据量从3,000小时增至20,000小时，下游任务成功率持续提升，未出现饱和迹象，证明了其强大的性能与泛化能力。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作基础模型已成为机器人执行多样化语言指令操控任务的一种有前景的方法。主流方法通常采用强大的预训练视觉语言模型作为语义主干，并结合基于扩散的动作头进行训练。尽管进展显著，但社区仍缺乏关于真实机器人性能如何随大规模预训练数据集扩展的全面实证研究，同时也缺少一个能够高效处理海量数据以进行此类扩展评估的优化训练代码库。因此，一个亟待探究的根本问题是：VLA模型在真实世界环境下，其性能究竟如何随海量真实机器人数据扩展？\n\n本文针对上述痛点，旨在通过大规模真实世界数据预训练，实证研究VLA模型的扩展规律，并构建一个高效、可部署的实用系统。本文的核心思路是：收集来自9种流行双手机器人平台约2万小时的真实世界数据，预训练一个名为LingBot-VLA的VLA基础模型，并通过一个高度优化的代码库实现高效训练，最后在包含100个任务的基准上进行跨3个机器人平台的系统评估，以验证其性能和泛化能力。\n\n## 方法详解\nLingBot-VLA的整体框架旨在利用预训练的视觉语言表示，并将其与动作生成能力相结合。模型采用混合Transformer架构，将预训练的VLM与一个名为“动作专家”的初始动作生成模块集成。\n\n![方法总览](https://arxiv.org/html/2601.18692v1/x1.png)\n> **图1**：LingBot-VLA 概述。我们扩展了从真实世界收集的双手机器人数据进行预训练。LingBot-VLA可以轻松高效地迁移到下游任务。此外，我们在三个机器人实体上进行了系统评估，证明了我们模型的明显优势。\n\n**核心架构与流程**：多视角操作图像和相关任务指令通过VLM统一编码，为后续动作生成建立多模态条件。同时，机器人的本体感知序列（初始状态和动作块）被输入到动作专家中，用于预测动作生成。模型采用流匹配进行连续动作建模，以实现流畅平滑的机器人控制。\n\n具体而言，在时间戳t的联合建模序列是观测条件O_t和动作块A_t的拼接。观测上下文定义为 O_t = [I_t^1, I_t^2, I_t^3, T_t, s_t]，其中包含了来自双手机器人三视角操作图像、任务指令T_t和机器人状态s_t的令牌。对应的动作序列为 A_t = [a_t, a_{t+1}, ..., a_{t+T-1}]，其中T是动作块长度（预训练阶段设为50）。训练目标是通过条件流匹配来表征条件分布 p(A_t | O_t)。\n\n**训练目标**：对于流时间步s ∈ [0,1]，通过在高斯噪声ϵ和真实动作A_t之间进行线性插值定义概率路径，得到中间动作 A_{t,s} = s A_t + (1-s)ϵ。动作专家v_θ通过最小化流匹配目标进行训练：ℒ_FM = 𝔼_{s, A_t, ϵ} || v_θ(A_{t,s}, O_t, s) - (A_t - ϵ) ||^2。\n\n**注意力机制**：遵循π0的方法，对联合序列[O_t, A_t]实施分块因果注意力。序列被划分为三个功能块：[I_t^1, I_t^2, I_t^3, T_t]、[s_t]和[a_t, a_{t+1}, ..., a_{t+T-1}]。在这些块之间应用因果掩码，使得每个块中的令牌只能关注自身及前面块中的令牌。而同一块内的所有令牌采用双向注意力，可以相互关注。这种配置确保动作专家可以利用所有可用的观测知识，同时防止未来动作令牌的信息泄漏到当前观测表示中。\n\n**空间感知增强**：为了显式捕捉操控环境中的空间感知并进一步提高机器人执行的鲁棒性，采用了一种视觉蒸馏方法。具体为，应用对应于三视角操作图像的可学习查询[Q_t^1, Q_t^2, Q_t^3]。为了整合深度信息，这些查询由VLM处理，然后与来自LingBot-Depth模型的深度令牌[D_t^1, D_t^2, D_t^3]对齐。通过最小化蒸馏损失ℒ_distill = 𝔼_{Q_t} | Proj(Q_t) - D_t | 来实现对齐，其中Proj(·)是一个用于维度对齐的投影层。\n\n**训练效率优化**：由于动作数据本质上是高频的，建立一个包含分布式训练和算子优化的高效流程至关重要。优化方法包括：1）**分布式策略**：采用完全分片数据并行（FSDP）来分片优化器状态、模型参数和梯度，以最小化内存占用。受VeOmni中提出的混合分片数据并行（HSDP）方法启发，为动作专家模块构建特定的“分片组”，以减轻过度参数分片带来的通信开销。同时实施混合精度策略。2）**算子级优化**：利用FlexAttention来优化稀疏注意力计算。此外，应用算子融合（通过torch.compile）以减少内核启动开销并最大化内存带宽利用率。\n\n## 实验与结果\n**实验设置**：实验在GM-100基准上进行大规模实证评估，该基准包含100个多样化的操控任务。评估涉及3个不同的商业机器人平台：AgileX、Agibot G1和Galaxea R1Pro。每个任务在每个平台上收集130条经过质量筛选的专家演示用于训练。评估时，每个模型在每对任务-机器人组合上进行15次试验。\n\n**对比方法**：与三个先进的VLA模型进行对比：π0.5、GR00T N1.6和WALL-OSS。所有模型均使用相同的后训练流程从公开的预训练检查点进行微调，以确保公平比较。\n\n**评估指标**：采用**成功率**（SR：在3分钟时限内完成所有任务步骤的试验比例）和**进度分数**（PS：通过顺序子任务检查点跟踪部分任务完成度）。\n\n![预训练数据集](https://arxiv.org/html/2601.18692v1/x2.png)\n> **图2**：LingBot-VLA使用的预训练数据集可视化。展示了来自9种不同双手机器人平台的数据。\n\n![原子动作词云](https://arxiv.org/html/2601.18692v1/x3.png)\n> **图3**：（a）预训练数据集和（b）基准测试中原子动作的词云。测试集中约50%的原子动作未出现在训练集前100个最常见动作中，体现了测试集的多样性和评估的严谨性。\n\n**关键实验结果**：如表1所示，在两个指标上，不带深度信息的LingBot-VLA在所有平台上均显著优于WALL-OSS和GR00T N1.6。通过结合基于深度的空间信息，带深度信息的LingBot-VLA在三个实体上的平均SR比π0.5提高了4.28%，PS提高了7.76%。值得注意的是，GR00T N1.6在Galaxea R1Pro平台上达到了与π0.5相当的性能，这归因于其预训练阶段大量包含了该平台的数据，表明预训练可以显著增强在下游结构相似任务上的性能。\n\n**模拟基准结果**：在RoboTwin 2.0模拟基准的50个任务上，LingBot-VLA也显示出显著优势。在干净场景和随机化场景中，不带深度信息的模型比π0.5基线分别绝对提高了3.76%和8.58%的成功率；整合深度信息后，提升幅度分别达到5.82%和9.92%。\n\n![训练吞吐量分析](https://arxiv.org/html/2601.18692v1/x4.png)\n> **图4**：（a）Qwen2.5-VL-3B-π 和（b）PaliGemma-3B-pt-224-π 模型的训练吞吐量分析。与StarVLA、Dexbotic和OpenPI等基线代码库相比，本文的代码库在两种模型设置下均实现了最快的训练速度，并且扩展效率接近理论线性极限。\n\n**消融实验 - 数据扩展规律**：为评估预训练数据的扩展规律，在基准测试的子集上进行了实验。如图5所示，当预训练数据时长从3,000小时增加到20,000小时时，进度分数和成功率均呈现一致的上升趋势。这表明扩展真实世界预训练数据有助于提高模型在多样化下游任务和实体上的泛化能力和性能，且即使在2万小时规模下也未观察到饱和迹象。\n\n![数据扩展行为](https://arxiv.org/html/2601.18692v1/x5.png)\n> **图5**：跨数据集规模的扩展行为。随着数据规模的增加，我们的模型在成功率和进度分数方面表现出扩展规律。\n\n**消融实验 - 数据效率分析**：如图6所示，在Agibot G1平台上，仅使用每个任务80条演示的有限预算进行后训练，LingBot-VLA在进度分数和成功率上均优于使用完整130条演示集的π0.5。随着后训练数据量的增加，LingBot-VLA与π0.5之间的性能差距显著扩大，证明了其卓越的数据效率和可扩展性。\n\n![数据效率](https://arxiv.org/html/2601.18692v1/x6.png)\n> **图6**：LingBot-VLA后训练的数据效率。使用有限的后训练数据，LingBot-VLA的性能优于使用更多数据的基线模型π0.5。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1）**大规模真实数据预训练**：首次系统实证研究了VLA模型在约2万小时真实世界、多机器人数据上的扩展规律，证明了性能随数据规模持续提升且未见饱和。2）**高效训练代码库**：开发了一个高度优化的开源代码库，实现了每GPU每秒261个样本的高吞吐量，相比现有方案有1.5~2.8倍的加速，并具备优秀的扩展效率。3）**系统化评估基准**：在包含3个平台、100个任务的大规模真实世界基准上进行了严格评估，为VLA模型的可靠评测设立了新标准。\n\n论文自身提到的局限性可能在于数据收集仍依赖于特定机器人平台，且泛化评估虽跨平台但平台类型仍属双手机器人范畴。对后续研究的启示在于：大规模、高质量的真实世界数据是提升VLA模型性能的关键；训练效率的优化对于降低大规模机器人学习的成本至关重要；社区需要建立更统一、更 rigorous 的评估协议以公平比较不同方法。本文开源代码、模型和基准数据，将有力推动机器人学习领域的开放科学发展。",
      "imageUrls": [
        "https://arxiv.org/html/2601.18692v1/x1.png",
        "https://arxiv.org/html/2601.18692v1/x2.png",
        "https://arxiv.org/html/2601.18692v1/x3.png",
        "https://arxiv.org/html/2601.18692v1/x4.png",
        "https://arxiv.org/html/2601.18692v1/x5.png",
        "https://arxiv.org/html/2601.18692v1/x6.png",
        "https://arxiv.org/html/2601.18692v1/figures/experiment/pre_training_data_scaling_law/Aggregated_Progress_Rate.png",
        "https://arxiv.org/html/2601.18692v1/figures/experiment/pre_training_data_scaling_law/Aggregated_Success_Rate.png",
        "https://arxiv.org/html/2601.18692v1/figures/experiment/post_training_data_scaling_law/Dual_Axis_Success_Progress.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.18629",
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "http://arxiv.org/abs/2601.18629",
      "arxivId": "2601.18629",
      "date": "2026-01-26",
      "authors": "Hao-Shu Fang Team",
      "category": "Manipulation",
      "summary": "本文提出ExoGS框架，旨在解决机器人操作任务中高质量交互数据获取困难、仿真与真实世界差距大的问题。其核心方法包括：1）使用机器人同构被动外骨骼AirExo-3精准捕捉人类演示的毫米级轨迹；2）基于3D高斯泼溅技术将场景重建为可编辑的动态资产，支持几何一致的数据增强；3）引入轻量级Mask Adapter模块，为策略注入实例级语义以提升视觉域偏移下的鲁棒性。实验表明，该框架相比遥操作基线显著提升了数据效率和策略泛化能力。",
      "detailedSummary": "## 研究背景与动机\n模仿学习的有效性严重依赖于训练数据的规模与质量。基于仿真的数据合成虽具有可扩展性，但存在因几何、外观和物理交互差异导致的“仿真到现实差距”。近期，利用神经辐射场或3D高斯泼溅（3DGS）等神经场景表示的Real-to-Sim-to-Real（R2S2R）流程，通过逼真渲染在视觉层面缩小了差距。然而，现有R2S2R方法大多局限于静态场景重建，并依赖强化学习来获取操作数据，其核心挑战在于如何在不部署昂贵机器人硬件的情况下，获取物理有效、高保真的交互数据。\n\n本文针对交互数据难以在仿真中高效获取的痛点，提出了一个机器人无关的4D R2S2R框架ExoGS。其核心思路是：利用一个与机器人同构的被动外骨骼AirExo-3采集人体演示，将静态环境和动态交互完整捕获并数字化为可编辑的3DGS资产，进而在仿真中进行几何一致的重放与大规模数据增强，以支持高效的操作数据收集与策略学习。\n\n## 方法详解\nExoGS框架包含硬件采集、4D数据生成与增强、以及策略学习三个主要部分。其核心创新在于硬件-软件协同：通过低成本、高精度的外骨骼设备捕获与机器人运动学一致的演示轨迹，并利用3DGS的可编辑特性进行数据数字化与增强。\n\n![方法框架](https://arxiv.org/html/2601.18629v1/imgs/GSpipeline.png)\n\n> **图2**：ExoGS整体流程概览。使用AirExo-3（左）采集多视角RGB-D观测和关节角度，通过3D高斯泼溅（中）重建机器人、物体和环境的可编辑数字资产，并估计物体位姿，最终在仿真中重放和增强整个操作过程（右）。\n\n**1. 硬件设计：AirExo-3**\nAirExo-3是一个低成本、开源的被动外骨骼，其机械结构与目标机器人几何匹配，具有相同的运动学参数、关节限位和夹爪开合范围，以确保工作空间一致性。其核心关节模块包含一个12位微型旋转编码器，通过正向运动学可实现毫米级精度的末端定位。设备支持便捷的零位校准，并通过可调被动阻尼确保操作稳定舒适。\n\n**2. 4D数据生成与增强**\n*   **演示采集**：操作者佩戴AirExo-3进行演示，系统同步记录关节角度序列 {𝒒ₜ, gₜ} 和多视角RGB-D图像序列 ℐ。\n*   **数字资产生成**：利用多视角图像，通过COLMAP恢复相机位姿，并基于3DGS优化场景（机器人、物体、环境）的高斯参数，生成高保真、可解耦的3D资产。\n*   **物体位姿估计与轨迹处理**：使用FoundationPose对多视角RGB-D序列进行物体位姿跟踪，并通过融合得到物体在机器人基坐标系下的统一位姿序列 {𝐓ₒ,ₜ}。一个轻量级的`PoseProcess`模块可对位姿序列进行归一化和重组，例如将物体位姿固定到机器人末端，从而通过替换物体模型实现同一轨迹在不同物体上的复用。\n*   **数据增强**：利用可编辑的3DGS表示，实施四种增强策略：1）相机视角增强；2）颜色与光照增强；3）背景增强（将真实图像作为背景纹理合成）；4）物体位姿增强（扰动物体位姿与尺度，或替换为功能兼容的物体）。\n\n**3. 策略模块：Mask Adapter**\n为了缓解视觉域偏移对策略泛化的影响，本文提出了一个轻量级的Mask Adapter模块，旨在将实例级语义信息注入到基于ViT的模仿学习策略中，引导注意力关注与交互相关的区域。\n\n![Mask Adapter框架](https://arxiv.org/html/2601.18629v1/imgs/maskpipeline.png)\n\n> **图3**：Mask Adapter模块概述。包含两阶段训练：第一阶段使用3DGS生成的像素级监督进行语义分割预训练；第二阶段将获得的补丁级语义标签通过增强的位置编码和掩码引导的注意力机制，注入到ViT策略中。\n\n*   **掩码头**：在第一阶段，对视觉编码器和一个轻量级分割头进行像素级监督的微调，以获得可迁移的语义信息，并为每个图像补丁生成标签 ℓₙ。\n*   **掩码引导的令牌建模**：在第二阶段，利用补丁标签 ℓ 来增强基础位置编码（添加可学习的标签嵌入），并构建一个加性的注意力掩码 𝐀。该掩码基于预定义的标签关系集 ℛ，允许语义一致的令牌之间进行交互，而阻止不相关令牌间的通信。\n*   **训练目标**：第二阶段联合优化原始策略的动作损失 ℒ_act 和分割损失 ℒ_seg，以稳定语义对齐。\n\n## 实验与结果\n实验在真实世界的Flexiv Rizon 4s机器人上进行，使用三个操作任务进行评估：**拾放**、**拾放并关闭**和**拧开瓶盖**。基线方法为基于遥操作收集数据训练的策略。评价指标包括数据收集效率（成功率和单次演示平均时间）和策略执行成功率（25次试验）。\n\n**数据收集效率**：招募10名无机器人背景的志愿者进行测试。如图5和表I所示，AirExo-3在所有任务上的数据收集速度均快于遥操作，且任务越复杂优势越大。同时，AirExo-3的演示成功率显著高于遥操作，尤其是在接触密集的拧瓶盖任务上（87% vs. 17%），证明了其在采集可靠接触式操作演示方面的有效性。\n\n![数据收集时间对比](https://arxiv.org/html/2601.18629v1/imgs/timeuse.png)\n\n> **图5**：AirExo-3与遥操作在各项任务上完成单次成功演示的平均时间对比。AirExo-3更快，且任务越复杂优势越明显。\n\n**TABLE I: 任务成功率对比: AirExo-3 vs. 遥操作**\n| 任务 | AirExo-3 | 遥操作 |\n| :--- | :--- | :--- |\n| 拾放 | 100% | 92.3% |\n| 拾放并关闭 | 100% | 83% |\n| 拧开瓶盖 | 87% | 17% |\n\n**策略性能（无增强）**：如表II所示，在大多数任务上，使用原始生成数据训练的ExoGS策略性能低于使用遥操作数据训练的基线，主要原因是渲染观测与现实观测间仍存在视觉差距。然而，在“拾放（新物体）”设定下，通过数字资产替换生成的**完全合成演示**使ExoGS策略取得了76%的成功率，而遥操作基线为0%，证明了低成本数据集扩展的巨大价值。\n\n**TABLE II: ExoGS与遥操作策略成功率对比**\n| 任务 | ExoGS | 遥操作 |\n| :--- | :--- | :--- |\n| 拾放 | 50% | 72% |\n| 拾放并关闭 | 48% | 64% |\n| 拧开瓶盖 | 24% | 8% |\n| 拾放（新物体） | 76% | 0% |\n\n**策略性能（有增强）与消融实验**：使用四种增强策略将数据集扩大20倍。如图7(a)所示，数据增强显著提升了策略在物体颜色、背景、光照变化下的泛化能力，其性能甚至超过了仅在真实数据上训练的策略。\n\n![增强策略效果与消融](https://arxiv.org/html/2601.18629v1/imgs/aug_all.png)\n\n> **图7**：(a) 数据增强对策略泛化的影响。使用增强数据训练的策略在多种视觉扰动下表现更优。(b) 消融研究，评估不同增强策略（A:视角，B:外观，C:物体位姿）对泛化性能的贡献。颜色抖动（外观增强）带来的提升最大。\n\n消融实验（图7b）表明，**视角变化**和**颜色抖动**对泛化性能提升贡献最大，其中颜色抖动因能有效应对合成与真实数据间持久的颜色和光照差异，效果最为显著。而**物体位姿增强**带来的收益有限，因为原始数据中物体位姿已足够多样，且位姿扰动并未解决主要的视觉域差距。\n\n**Mask Adapter的效果**：如图8所示，Mask Adapter在仅使用容器和绿色方块训练的情况下，对未见过的物体和背景展现了强大的分割泛化能力。这证明了其注入的语义信息能够有效引导策略关注交互相关区域。\n\n![Mask Adapter分割结果](https://arxiv.org/html/2601.18629v1/imgs/maskfig.jpg)\n\n> **图8**：Mask Adapter的分割结果示例。模型仅在容器和绿色方块上训练，但在测试时对新颖物体和背景实现了鲁棒的分割。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个完整的4D、机器人无关的Real-to-Sim-to-Real框架ExoGS，将真实世界资产和操作序列重建为可编辑的3DGS资产及其动态，实现了可扩展、运动学一致的数据生成。\n2.  开发了开源、低成本、高精度的操作数据采集设备AirExo-3。\n3.  设计了轻量级Mask Adapter模块，通过向策略注入语义掩码信息来引导注意力，增强了策略在视觉域偏移下的鲁棒性。\n\n**局限性**：论文提到，对于像拧瓶盖这类受强运动学约束的任务，策略失败的主要原因是演示质量受操作者熟练度限制导致的轨迹次优，以及螺纹耦合中的滑动、卡死等物理问题，数据增强对此类问题的改善有限。这暗示了当前方法在物理仿真逼真度方面仍存在挑战。\n\n**启示**：ExoGS展示了一条通过“人体演示采集 + 可编辑神经场景表示”来实现高效、规模化操作数据生成的路径。其硬件-软件协同的思路，以及利用显式场景表示进行几何一致增强和语义注入的策略，为后续研究提供了重要参考。如何进一步提升物理交互仿真的真实性，以及如何更智能地利用场景的几何与语义先验来指导策略学习，是值得探索的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2601.18629v1/imgs/airexo.png",
        "https://arxiv.org/html/2601.18629v1/imgs/GSpipeline.png",
        "https://arxiv.org/html/2601.18629v1/imgs/maskpipeline.png",
        "https://arxiv.org/html/2601.18629v1/imgs/TaskFig.png",
        "https://arxiv.org/html/2601.18629v1/imgs/timeuse.png",
        "https://arxiv.org/html/2601.18629v1/imgs/gen.png",
        "https://arxiv.org/html/2601.18629v1/imgs/aug_all.png",
        "https://arxiv.org/html/2601.18629v1/imgs/maskfig.jpg",
        "https://arxiv.org/html/2601.18629v1/imgs/masktest.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.17563",
      "title": "Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment",
      "url": "http://arxiv.org/abs/2601.17563",
      "arxivId": "2601.17563",
      "date": "2026-01-24",
      "authors": "Odinaldo Rodrigues Team",
      "category": "Manipulation",
      "summary": "本文针对模仿学习（ILfO）泛化能力不足、依赖动作监督及行为盲目模仿等问题，提出无监督模仿学习框架UfO。其核心技术分为两阶段：首先通过条件转移估计近似教师动作，再利用在线行为对齐精修策略，使智能体轨迹与教师轨迹紧密匹配。在五个常用环境上的实验表明，UfO性能超越教师及所有现有ILfO方法，且标准差最小，证明其在未见场景中具有更优的泛化能力。",
      "detailedSummary": "## 研究背景与动机\n当前，从观察中模仿学习（ILfO）的先进方法主要有两类：基于行为克隆（BC）和基于对抗模仿学习（AIL）。BC方法通常利用逆动力学模型（IDM）或前向动力学模型（FDM）来推断教师动作，并使用有监督损失进行优化。AIL方法则通过对抗训练使智能体行为与教师难以区分。然而，这些方法存在几个关键局限：1）它们仍然依赖于基于动作的监督损失（无论是显式还是通过模型推断）；2）它们通常假设环境的状态转移函数是单射的，即每个状态-动作对唯一确定下一个状态，这在实践中往往不成立，导致策略在面对多种可行动作时学习偏差；3）AIL方法容易陷入“行为寻求模式”，即盲目模仿教师的行为轨迹，而不充分考虑当前环境状态，导致在需要理解动作意图或因果关系的环境中表现不佳。\n\n本文针对这些痛点，提出了一种全新的无监督视角。核心思路是：通过一个两阶段过程，首先无监督地联合学习策略和条件生成模型来近似环境动态和教师行为，然后通过有限的在线对抗训练进一步对齐和泛化策略，从而完全摆脱对动作监督的依赖，并缓解单射假设和行为寻求模式的问题。\n\n## 方法详解\n本文提出的无监督观察模仿学习（UfO）方法包含三个核心模型：策略模型 $\\pi_{\\theta}$（根据状态预测动作）、条件生成模型 $\\mathcal{G}_{\\phi}$（根据状态和预测的动作生成下一状态）、以及判别模型 $\\mathcal{D}_{\\omega}$（区分教师和智能体的轨迹）。整体训练流程分为两个阶段：重建阶段和对抗阶段。\n\n![方法框架](https://arxiv.org/html/2601.17563v1/x1.png)\n> **图1**：UfO的两阶段训练周期。(a) 重建阶段：交替冻结生成模型或策略模型，分别使用教师轨迹和智能体在线交互轨迹进行训练，实现策略与生成模型的相互优化。(b) 对抗阶段：冻结生成模型，使用教师轨迹和智能体轨迹（或其状态差分）来训练策略和判别器，进行在线行为对齐。\n\n**1. 重建阶段（条件转移估计）**\n此阶段的目标是无需动作监督，联合学习一个能近似环境动态的生成模型和一个能模仿教师行为的策略。其核心是交替优化策略 $\\pi_{\\theta}$ 和生成模型 $\\mathcal{G}_{\\phi}$。\n*   **策略训练（冻结 $\\mathcal{G}_{\\phi}$）**：使用教师的状态转移轨迹 $(s, s’)$。策略的目标是输出一个动作 $\\pi_{\\theta}(s)$，使得冻结的生成模型 $\\mathcal{G}_{\\phi}$ 能以该动作为条件，生成与教师下一状态 $s’$ 尽可能接近的状态。损失函数为 $\\min_{\\theta} \\sum \\mid s’ – \\mathcal{G}_{\\phi}(s, \\pi_{\\theta}(s)) \\mid$。这迫使策略学习如何通过生成模型“解释”教师的转移。\n*   **生成模型训练（冻结 $\\pi_{\\theta}$）**：使用智能体策略与环境在线交互产生的轨迹 $(s, s’_{env})$，其中 $s’_{env} = T(s, \\pi_{\\theta}(s))$ 是真实环境转移。生成模型的目标是，给定当前状态 $s$ 和智能体动作 $\\pi_{\\theta}(s)$，预测的下一个状态 $\\mathcal{G}_{\\phi}(s, \\pi_{\\theta}(s))$ 应尽可能接近真实的环境转移 $s’_{env}$。损失函数为 $\\min_{\\phi} \\sum \\mid T(s, \\pi_{\\theta}(s)) – \\mathcal{G}_{\\phi}(s, \\pi_{\\theta}(s)) \\mid$。这使生成模型学习环境动态。\n\n通过这种交替迭代的相互优化，策略逐渐学会产生能使生成模型复现教师转移的动作，而生成模型则通过学习智能体引发的真实转移来精化其对动态的建模。这个过程完全无监督，且由于优化基于状态转移的似然 $P(s’|s, \\pi_{\\theta}(s))$ 而非直接的动作映射 $P(a|s)$，从而规避了对单射转移函数的依赖。\n\n**2. 对抗阶段（在线行为对齐）**\n重建阶段后，策略仅从有限的教师轨迹中学习。为提升在未见情况下的泛化能力，UfO引入一个简短的对抗微调阶段。\n*   此阶段**冻结生成模型** $\\mathcal{G}_{\\phi}$，以保留已学习的环境动态近似。\n*   同时训练策略 $\\pi_{\\theta}$ 和判别器 $\\mathcal{D}_{\\omega}$。判别器是一个循环神经网络（RNN），输入是整个轨迹中连续状态之间的差分序列（例如 $|s_i – s_{i+1}|$），而非原始状态，以避免记忆并提升对未见状态的泛化能力。\n*   对抗损失采用标准的生成对抗网络形式：$\\min_{\\omega} \\max_{\\theta} \\mathbb{E}_{\\tau \\sim \\mathcal{T}_{\\pi_{\\psi}}^{\\Delta}} \\log(\\mathcal{D}_{\\omega}(\\tau)) + \\mathbb{E}_{\\tau \\sim \\mathcal{T}_{\\pi_{\\theta}}^{\\Delta}} \\log(1 – \\mathcal{D}_{\\omega}(\\tau))$，其中 $\\mathcal{T}^{\\Delta}$ 表示状态差分轨迹。\n*   为**防止行为寻求模式和灾难性遗忘**，UfO对此阶段进行了严格限制：仅进行10个epoch的训练，使用极低的学习率，并对策略的梯度进行裁剪。这使得策略能在保留重建阶段所学知识的前提下，进行轻微的在线行为对齐。\n\n**创新点总结**：1) **完全无监督**：无需任何形式的动作标注或监督损失。2) **基于转移的优化**：通过条件生成模型耦合策略与动态，规避单射假设。3) **受限的对抗训练**：通过短期、低学习率的在线对抗微调提升泛化，同时避免行为寻求模式。\n\n## 实验与结果\n**实验设置**：在五个广泛使用的MuJoCo基准环境（InvertedPendulum-v4, Hopper-v4, Ant-v4, Swimmer-v4, HalfCheetah-v4）上进行评估。使用Imitation Datasets提供的教师数据和权重。每个方法使用10,000个未见过的随机种子进行评估。\n\n**对比方法**：与两类基线对比：1) 常用基线：行为克隆观察（BCO）、生成对抗观察模仿（GAIfO）；2) 先进的ILfO方法：连续观察模仿学习（CILO）、模态不可知对抗假设适应观察学习（MAHALO）、离策略观察模仿学习（OPOLO）。\n\n**评估指标**：平均回合奖励（AER）和归一化性能（$\\mathcal{P}$），其中 $\\mathcal{P}=1$ 表示达到教师水平，$\\mathcal{P}=0$ 表示随机策略水平。\n\n**关键实验结果**：\n论文中的表格（此处以文字描述）显示，UfO在所有五个环境中均取得了最佳或极具竞争力的AER和 $\\mathcal{P}$ 值。具体而言，在Hopper、Ant、Swimmer和HalfCheetah环境中，UfO的 $\\mathcal{P}$ 值均大于1（分别为1.0127, 1.0238, 1.0163, 1.0457），表明其**平均表现超越了教师本身**。同时，UfO在大多数环境中的奖励标准差是最小的，这表明其具有更好的稳定性和泛化能力。相比之下，MAHALO和BCO在复杂环境（如HalfCheetah）中表现较差，而CILO和OPOLO虽然表现良好，但未能超越教师。\n\n![消融实验](https://arxiv.org/html/2601.17563v1/x2.png)\n> **图2**：消融实验结果。从左至右分别展示了：(a) 移除对抗阶段（仅重建阶段）的性能；(b) 在对抗阶段使用原始状态而非状态差分的性能；(c) 延长对抗阶段训练epoch数（从10增至100）的性能。结果表明，对抗阶段（尤其是使用状态差分）对提升性能和稳定性有贡献，但过长的对抗训练会导致性能下降，验证了其设计必要性。\n\n**消融实验分析**：\n1.  **对抗阶段的作用**：仅使用重建阶段，UfO已能达到SOTA结果，但加入对抗阶段后，性能（AER）有轻微提升，且**标准差进一步减小**，验证了在线行为对齐对泛化和稳定性的积极作用。\n2.  **状态差分 vs 原始状态**：在对抗阶段使用原始状态作为判别器输入会导致性能下降，证实了使用状态差分对于防止记忆和提升泛化的重要性。\n3.  **对抗训练时长**：将对抗阶段的epoch数从10增加到100会导致性能显著下降，这证实了过长对抗训练会引入行为寻求模式或灾难性遗忘，支持了UfO采用简短对抗微调的设计。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了首个完全**无监督**的从观察中模仿学习框架（UfO），彻底摆脱了对动作标注或动作级监督损失的依赖。\n2.  设计了**两阶段训练机制**：通过“条件转移估计”阶段无监督地耦合策略与动态模型，规避了单射假设；通过“在线行为对齐”阶段进行受限的对抗微调，有效提升了策略的泛化能力且避免了行为寻求模式。\n3.  在多个基准测试中实现了**超越教师**的性能，并表现出更小的方差，证明了其在泛化方面的优势。\n\n**局限性**：论文提到，对抗阶段需要仔细调整学习率、epoch数和梯度裁剪策略，以避免负面影响。这暗示该方法在此阶段的鲁棒性可能对超参数较为敏感。\n\n**后续启示**：UfO的成功表明，从状态转移的因果结构中无监督地推导行为意图是一条可行的新路径。这为在动作信息难以获取的场景（如视频学习）下的模仿学习提供了新思路。未来的工作可以探索更鲁棒的对抗对齐机制，或将此框架扩展到更复杂的、部分可观测或非平稳的环境中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.17563v1/x1.png",
        "https://arxiv.org/html/2601.17563v1/x2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.17507",
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "http://arxiv.org/abs/2601.17507",
      "arxivId": "2601.17507",
      "date": "2026-01-24",
      "authors": "Tongtong Feng Team",
      "category": "Manipulation",
      "summary": "本文提出MetaWorld分层世界模型，旨在解决人形机器人语义指令与物理执行之间的鸿沟问题。其核心方法是将任务解耦为VLM驱动的语义规划层与潜在动力学模型控制层，并引入动态专家选择与运动先验融合机制，利用预训练的多专家策略库进行知识迁移与在线适配。在Humanoid-Bench上的实验表明，该方法在任务完成度和运动连贯性上优于基于世界模型的强化学习基线。",
      "detailedSummary": "## 研究背景与动机\n当前人形机器人全身运动控制受限于语义与物理之间的鸿沟。主流方法存在三个关键局限性：强化学习的样本效率低下；模仿学习的泛化性差；以及视觉语言模型直接应用于控制时产生的物理不一致性，即“符号落地”问题。本文针对如何弥合高层语义理解与底层物理执行之间的割裂这一具体痛点，提出了一种新的分层世界模型视角。核心思路是：将任务分解为VLM驱动的语义规划层和基于紧凑状态空间的潜在动力学模型物理执行层，并通过动态专家选择与运动先验融合机制，利用预训练的多专家策略库作为可迁移知识，实现高效的在线适应。\n\n## 方法详解\nMetaWorld的整体框架是一个三层级联架构，将机器人控制问题解耦为语义规划和物理执行。输入是任务描述 𝒯 和当前观测 o_t，输出是关节动作 a_t。该过程形式化为 π(a_t | s_t, 𝒯) = π_phys(a_t | s_t, π_sem(𝒯))，其中 π_sem 将任务映射为语义计划，π_phys 基于当前状态和语义计划生成具体动作。\n\n![方法框架](https://arxiv.org/html/2601.17507v1/framework.jpg)\n> **图2**：MetaWorld的三层架构。语义层通过视觉语言模型将观测解析为可执行的技能序列；技能迁移层通过分层世界模型整合专家策略先验并实现动态适应；物理层在紧凑状态空间中使用潜在动力学模型进行精确控制。\n\n核心模块包括：\n1.  **语义规划与符号落地层**：该层使用VLM将自然语言任务描述映射到专家策略权重向量 **w**，而非直接生成动作。具体地，通过精心设计的提示词让VLM生成响应，再经解析函数提取并归一化得到权重 w_i。由于每个专家策略 π_exp^i 本身是物理可行的，因此语义计划 π_sem(𝒯) = Σ_i w_i π_exp^i 天然满足物理约束，从而将符号落地问题转化为专家策略的线性组合，将误差限制在专家策略差异的范围内。\n\n2.  **动态适应机制**：为应对环境动态变化，引入了状态感知的专家选择机制。基于当前状态 s_t，通过状态编码函数 φ 和专家特征提取函数 ψ，计算选择概率分布 p(i|s_t)。随后，将VLM生成的语义权重 w_i 与动态选择概率 p(i|s_t) 融合，得到最终权重 w̃_i(s_t, 𝒯) = α w_i + (1-α) p(i|s_t)。超参数 α 用于平衡长期任务规划与短期状态适应。融合后的权重用于生成参考专家动作 a_ref = Σ_i w̃_i(s_t, 𝒯) π_exp^i(s_t)，为物理层提供高质量初始解。\n\n3.  **物理执行与在线优化层**：该层采用TD-MPC2算法构建潜在动力学模型进行模型预测控制。观测 o_t 被编码为潜在状态 z_t，并通过动力学模型预测状态演变 z_{t+1}。MPC在预测时域 H 内求解最优动作序列，其优化目标中整合了专家引导项。总损失函数为 ℒ_total = ℒ_TD + λ || a_t - a_ref ||^2，其中 ℒ_TD 是时序差分学习损失，用于准确估计值函数；λ 控制专家先验的权重。这种方式在保持在线适应能力的同时，利用专家知识加速学习过程。\n\n与现有方法相比，创新点具体体现在：1) 提出了明确分离语义与物理的分层世界模型架构；2) 将VLM的输出约束为对预验证的专家策略的权重分配，巧妙规避了直接动作生成的物理不可行问题；3) 设计了语义权重与状态感知权重的动态融合机制，兼顾任务意图和环境实时变化。\n\n## 实验与结果\n实验在HumanoidBench这一综合的人形机器人运动与操作基准测试上进行。评估任务包括运动任务（行走、站立、奔跑）和操作任务（开门）。对比的基线方法是两种代表性的先进模型强化学习算法：TD-MPC2 和 DreamerV3。\n\n![实验结果对比](https://arxiv.org/html/2601.17507v1/exp.jpg)\n> **图3**：在运动（奔跑、行走、站立）和操作（开门）任务上，与基线方法TD-MPC2和DreamerV3的性能对比。\n\n关键实验结果如下表所示，MetaWorld在各项任务上均取得最佳性能。平均回报提升了135.6%，收敛所需的环境交互步数平均减少了24.9%。特别是在奔跑任务上，回报提升了惊人的2456.3%；在开门任务上，回报提升了278.3%。这验证了分层架构的有效性：对于基础运动任务，性能提升主要源于模仿学习专家库提供的高质量运动先验；对于复杂操作任务，提升则得益于VLM对指令的语义分解与动态选择机制对基础专家的组合。\n\n| 任务 | 指标 | DreamerV3 | TD-MPC2 | **Ours** | 提升 (%) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 站立 | 回报 ↑ | 699.3 ± 62.7 | 749.8 ± 54.3 | **793.4 ± 13.5** | 5.8 |\n| | 收敛步数 ↓ (M) | 5.5 | 1.9 | **0.8** | 57.9 |\n| 行走 | 回报 ↑ | 428.2 ± 14.5 | 644.2 ± 162.3 | **701.2 ± 7.6** | 8.8 |\n| | 收敛步数 ↓ (M) | 6.0 | 1.8 | **1.4** | 22.2 |\n| 奔跑 | 回报 ↑ | 298.5 ± 84.5 | 66.1 ± 4.7 | **1689.9 ± 13.6** | 2456.3 |\n| | 收敛步数 ↓ (M) | 6.0 | 2.0 | **1.9** | 5.0 |\n| 开门 | 回报 ↑ | 165.8 ± 50.2 | 179.8 ± 52.9 | **680.0 ± 50.0** | 278.3 |\n| | 收敛步数 ↓ (M) | 9.0 | 2.0 | **1.7** | 15.0 |\n| **平均** | **回报 ↑** | **398.0** | **410.0** | **966.1** | **135.6** |\n| | **收敛步数 ↓ (M)** | **6.6** | **1.9** | **1.5** | **24.9** |\n\n![消融实验](https://arxiv.org/html/2601.17507v1/x2.png)\n> **图4**：在开门任务上的消融实验结果（最大回报与收敛步数）。展示了移除VLM语义规划、动态专家选择（α=1.0）或专家动作引导（λ=0）对性能的影响。\n\n消融实验总结了每个核心组件的贡献：\n1.  **移除VLM语义规划**（均匀加权专家）：导致性能崩溃（下降72.7%），这表明机器人无法将“开门”指令分解为“接近把手-旋转-推/拉”等可执行技能，凸显了传统RL无法解决符号落地问题。\n2.  **移除动态专家选择**（固定α=1.0）：性能下降15.4%，表明框架具有一定鲁棒性，即使没有在线适应，预训练专家库仍能提供基本运动先验。\n3.  **移除专家动作引导**（λ=0）：性能下降52.9%，这突出了模仿学习与模型强化学习之间的协同作用。专家策略为在线微调提供了可行的动作边界，两者缺一不可。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了一个分层、模块化的世界模型架构，实现了任务表征的多尺度分解；2）引入了动态专家选择与运动先验融合机制，实现了多专家策略的高效复用和任务自适应迁移；3）建立了以VLM作为语义接口的可靠交互范式，实现了从环境语义到物理动作的可行映射。\n\n论文自身提到的局限性包括：构建专家库的模仿学习模块依赖于简单的轨迹匹配奖励机制，无法动态权衡关节级运动误差等细粒度方面；专家策略选择采用静态加权融合，而非混合专家架构下的智能路由机制，限制了精确的技能组合和语义条件切换；系统缺乏少样本泛化能力，对新颖复杂任务组合的适应性和可扩展性有限。\n\n这项工作对后续研究的启示在于：为连接高层语义与底层控制提供了一个可扩展的分层框架范式。未来可能的研究方向包括：为模仿学习设计动态奖励塑形机制、开发基于MoE的语义感知路由机制以进行更精细的技能组合、以及增强少样本迁移泛化能力，从而迈向更鲁棒和自适应的复杂控制框架。",
      "imageUrls": [
        "https://arxiv.org/html/2601.17507v1/x1.png",
        "https://arxiv.org/html/2601.17507v1/framework.jpg",
        "https://arxiv.org/html/2601.17507v1/exp.jpg",
        "https://arxiv.org/html/2601.17507v1/x2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.17486",
      "title": "EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds",
      "url": "http://arxiv.org/abs/2601.17486",
      "arxivId": "2601.17486",
      "date": "2026-01-24",
      "authors": "Yu She Team",
      "category": "Manipulation",
      "summary": "本文提出EquiForm框架，旨在解决基于3D点云的模仿学习策略对传感器噪声、姿态扰动和遮挡伪影高度敏感的问题，这些干扰会破坏SE(3)等变性假设。方法核心包括：1）几何去噪模块，用于从噪声观测中恢复一致的3D结构；2）对比等变对齐目标，强制表示在刚性变换与噪声扰动下的不变性。实验在16个模拟任务和4个真实任务上验证，相比先进方法，性能在模拟和真实环境中平均提升17.2%和28.1%，展现了优异的噪声鲁棒性与空间泛化能力。",
      "detailedSummary": "## 研究背景与动机\n视觉模仿学习在机器人操作领域取得了显著进展，其中使用3D点云作为策略输入因其几何感知和外观不变性而受到关注。然而，基于点云的策略对传感器噪声、姿态扰动和遮挡引起的伪影高度敏感，这些因素会扭曲几何结构并破坏鲁棒泛化所需的等变性假设。现有的等变方法主要将对称约束编码到神经架构中，但并未显式地纠正噪声引起的几何偏差或在学习表示中强制等变一致性。本文针对点云噪声破坏SE(3)等变性的具体痛点，提出了一个将噪声鲁棒性几何推理与等变表示学习相结合的新视角。本文的核心思路是：通过一个几何去噪模块恢复噪声观测下稳定的3D结构，并引入一个对比等变对齐损失来强制表示在刚性变换和噪声扰动下的一致性，从而构建一个噪声鲁棒的SE(3)-等变策略学习框架。\n\n## 方法详解\nEquiForm的整体框架是一个集成了几何去噪与规范化的策略学习流程。其输入是包含噪声点云和机器人本体感知状态的观测，输出是末端执行器的动作。流程主要包含四个阶段：1) 对噪声点云进行几何去噪，恢复稳定的3D结构；2) 使用一个SE(3)-等变编码器估计将观测映射到规范空间的刚性变换T；3) 将去噪后的点云和机器人状态通过T的逆变换到规范空间，由策略网络预测规范动作；4) 将规范动作通过变换T映射回全局坐标系，得到最终动作。\n\n![方法框架](https://arxiv.org/html/2601.17486v1/x7.png)\n\n> **图7**：EquiForm 方法框架。给定噪声点云观测，首先通过几何去噪模块恢复稳定的 3D 结构，然后通过 SE(3)-等变编码器估计规范变换 T，将去噪后的点云和机器人状态变换到规范空间。在规范空间中，策略网络预测规范动作，最后通过 T 的逆变换将动作映射回全局坐标系。\n\n核心模块一：**几何去噪模块**。该模块旨在从噪声观测 \\(X_\\delta\\) 中恢复底层场景的干净几何 \\(X^*\\)。其技术核心是将点云视为一个隐含曲面 \\(\\mathcal{M} = \\{ \\mathbf{x} \\in \\mathbb{R}^3 \\mid f(\\mathbf{x}) = 0 \\}\\) 的离散采样，并学习一个可微的隐式符号距离函数 \\(f\\)。对于每个输入点 \\(\\mathbf{x}_i\\)，模块预测其到真实表面的有符号距离 \\(d_i = f(\\mathbf{x}_i)\\)，然后通过将点沿其估计的法线方向移动 \\(-d_i\\) 来“投影”到估计的表面上，即 \\(\\mathbf{x}_i^* = \\mathbf{x}_i - d_i \\mathbf{n}_i\\)，从而得到去噪点云 \\(X^*\\)。损失函数结合了曲面重建损失（如倒角距离）和法线一致性损失。\n\n核心模块二：**对比等变对齐**。为了应对噪声破坏等变性的问题（如命题2和3所述），本文提出了一种对比学习目标。其关键思想是：对于同一场景，其经过不同刚性变换和不同噪声扰动的多个观测版本，在规范空间中的表示应该保持一致。具体而言，给定一个基础观测 \\(O\\)，通过应用随机刚性变换 \\(H\\) 和噪声扰动 \\(\\delta\\) 生成一个增强观测 \\(\\tilde{O}\\)。然后将 \\(O\\) 和 \\(\\tilde{O}\\) 分别通过去噪和规范化流程，得到它们在规范空间中的特征表示 \\(z\\) 和 \\(\\tilde{z}\\)。对比损失鼓励 \\(z\\) 和 \\(\\tilde{z}\\) 相互靠近（正样本），而与批次内其他样本的特征远离（负样本）。这显式地强制了表示对噪声和姿态变化的鲁棒性。\n\n与现有方法相比，EquiForm 的创新点具体体现在：1) 首次形式化地分析了噪声如何导致点云几何扭曲并破坏 SE(3) 等变性（即“等变性偏差”），为方法设计提供了理论基础；2) 不是将去噪作为独立预处理步骤，而是将几何去噪模块与策略学习流程深度融合，确保下游等变推理建立在稳定的几何基础上；3) 提出了对比等变对齐损失，这是一种新颖的表示学习目标，它联合优化了对刚性变换的等变性和对噪声扰动的不变性。\n\n## 实验与结果\n实验在模拟和真实世界两个平台上进行。模拟实验使用了包含16个多样化操作任务的基准测试，涵盖抓取、放置、堆叠、插孔等。真实世界实验涉及4个任务，使用机械臂和RGB-D相机。对比的基线方法包括：点云策略学习的先进方法 DP3 及其流匹配变体 DP3-FM；SE(3)-等变方法 Canonical Policy；以及非等变的图像基线 IBC 和 R3M。评估指标主要为任务成功率。\n\n关键实验结果：在模拟实验中，EquiForm 在16个任务上的平均成功率为 **71.9%**，相比最佳基线 Canonical Policy (54.7%) 提升了 **17.2%**。在真实世界实验中，EquiForm 的平均成功率为 **68.8%**，显著高于 DP3 (40.7%) 和 Canonical Policy (53.1%)，提升达 **28.1%** 和 **15.7%**。\n\n![对比实验](https://arxiv.org/html/2601.17486v1/x8.png)\n\n> **图8**：在模拟任务上的成功率对比。EquiForm 在大多数任务上超越了所有基线方法，尤其在具有挑战性的操作任务上优势明显。\n\n![消融实验](https://arxiv.org/html/2601.17486v1/x9.png)\n\n> **图9**：消融实验。移除几何去噪模块（Ours w/o Denoise）或对比对齐损失（Ours w/o CA）都会导致性能下降，验证了各组件对噪声鲁棒性的贡献。\n\n![定性结果](https://arxiv.org/html/2601.17486v1/x10.png)\n\n> **图10**：真实世界任务的成功与失败案例定性展示。EquiForm 能够处理遮挡和噪声，完成复杂操作，而基线方法则可能因误判几何关系而失败。\n\n消融实验总结：移除几何去噪模块导致平均性能下降 **6.4%**；移除对比对齐损失导致性能下降 **9.7%**。这证明了两个核心组件对于实现噪声鲁棒性都是必要且互补的。此外，实验还表明，随着点云噪声水平的增加，EquiForm 的性能下降幅度远小于基线方法，凸显了其鲁棒性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 形式化地分析了传感器噪声如何破坏点云的几何结构并导致 SE(3) 等变性偏差，为噪声鲁棒的等变学习提供了理论见解；2) 提出了 EquiForm 框架，创新性地整合了基于隐式曲面的几何去噪模块和对比等变对齐损失，以协同方式解决几何失真和表示不一致问题；3) 在广泛的模拟和真实机器人任务上进行了验证，证明了该方法在噪声鲁棒性和空间泛化能力上的显著优势。\n\n论文自身提到的局限性包括：1) 几何去噪模块假设场景表面是光滑且连续的，对于高度非结构化的场景（如杂乱无章的碎片）可能效果有限；2) 方法主要针对刚性变换和噪声，对于非刚性变形或动态场景的泛化能力未经验证。\n\n对后续研究的启示：1) 将噪声建模和等变性学习联合优化的思路可以扩展到其他对称群（如尺度、仿射变换）或非刚性变换；2) 隐式表面去噪与显式策略学习的结合范式，为构建更加几何感知和物理一致的机器人感知-动作系统提供了新方向；3) 如何将此类方法高效地应用于在线学习和自适应场景，是值得探索的下一步。",
      "imageUrls": [
        "https://arxiv.org/html/2601.17486v1/x1.png",
        "https://arxiv.org/html/2601.17486v1/x2.png",
        "https://arxiv.org/html/2601.17486v1/x3.png",
        "https://arxiv.org/html/2601.17486v1/x4.png",
        "https://arxiv.org/html/2601.17486v1/x5.png",
        "https://arxiv.org/html/2601.17486v1/x6.png",
        "https://arxiv.org/html/2601.17486v1/x7.png",
        "https://arxiv.org/html/2601.17486v1/x8.png",
        "https://arxiv.org/html/2601.17486v1/x9.png",
        "https://arxiv.org/html/2601.17486v1/x10.png",
        "https://arxiv.org/html/2601.17486v1/x11.png",
        "https://arxiv.org/html/2601.17486v1/x12.png",
        "https://arxiv.org/html/2601.17486v1/x13.png",
        "https://arxiv.org/html/2601.17486v1/x14.png",
        "https://arxiv.org/html/2601.17486v1/x15.png",
        "https://arxiv.org/html/2601.17486v1/x16.png",
        "https://arxiv.org/html/2601.17486v1/x17.png",
        "https://arxiv.org/html/2601.17486v1/x18.png",
        "https://arxiv.org/html/2601.17486v1/x19.png",
        "https://arxiv.org/html/2601.17486v1/x20.png",
        "https://arxiv.org/html/2601.17486v1/x21.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.17428",
      "title": "Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning",
      "url": "http://arxiv.org/abs/2601.17428",
      "arxivId": "2601.17428",
      "date": "2026-01-24",
      "authors": "Marco Hutter Team",
      "category": "Manipulation",
      "summary": "本文针对腿式机器人在复杂、无结构崎岖地形上难以实现高速稳定运动的问题，提出了**基于学习进度的自动课程强化学习（LP-ACRL）框架**。该框架的核心在于**在线估计智能体的学习进度，并据此自适应调整任务采样分布**，从而无需预先定义任务难度即可自动生成训练课程。实验表明，采用LP-ACRL训练的策略使ANYmal D四足机器人在楼梯、斜坡、碎石等多种地形上实现了**2.5 m/s的线速度和3.0 rad/s的角速度**，超越了此前方法在高速与复杂地形性能上的局限。",
      "detailedSummary": "## 研究背景与动机\n在足式机器人领域，使机器人能够灵巧、多能地穿越复杂非结构化环境仍然是一个重大挑战。课程强化学习（CRL）通过将训练过程构建为从简单任务到困难任务的渐进式学习，已成为应对这一挑战的关键方法。目前主流的CRL应用主要依赖于手动设计的课程，其中智能体按照预先定义的、沿单一难度轴排序的任务序列进行训练，进阶由人工调整的性能阈值控制。然而，在现实场景中，机器人需要执行的任务集通常不具备明确定义的难度排序，例如比较在楼梯上慢走和在碎石上快跑的难度是模糊的。这些任务共同构成了一个非结构化的任务空间，其中缺乏明确排序的任务序列使得手动设计课程变得不可行。\n\n本文针对在复杂、广泛的任务空间中难以预先定义难度结构的核心痛点，提出了基于学习进度的自动课程强化学习新视角。该方法旨在无需任务空间难度分布的先验知识，通过在线估计智能体的学习进度并自适应调整任务采样分布，实现自动课程生成。核心思路是利用智能体在各任务实例上的学习进度（定义为相邻课程阶段平均回合奖励的变化）来动态指导采样，优先学习当前最具提升潜力的任务。\n\n## 方法详解\n本文提出的方法称为基于学习进度的自动课程强化学习（LP-ACRL）。其整体框架是一个闭环过程：智能体根据当前的任务采样分布与环境交互收集数据，用于策略更新；同时，系统评估智能体在各个任务实例上的表现，计算学习进度，并据此更新下一阶段的任务采样分布，形成课程。\n\n![方法框架](https://arxiv.org/html/2601.17428v1/figs/3_Approach/LP-ACRL.png)\n\n> **图2**：基于学习进度的自动课程强化学习（LP-ACRL）工作流程。任务采样分布基于学习进度进行更新，优先考虑对策略进一步改进最具信息量的任务。\n\n核心模块是学习进度的计算与采样分布的更新机制。方法操作于一个离散的任务空间 𝒯，每个任务实例 ζ 由分类维度（如地形类型）和离散化的连续维度（如目标速度命令区间）共同定义。\n\n对于一个任务实例 ζ，其学习进度 LP 被定义为当前课程阶段 j 与前一阶段 j-1 的期望回合奖励之差：\nLP_{c_j}(ζ) = R_{c_j}(ζ) - R_{c_{j-1}}(ζ)\n其中，R_{c_j}(ζ) 是在分布 c_j 下从轨迹估计的期望回合奖励。\n\n基于计算得到的所有任务实例的学习进度值，下一课程阶段的任务采样分布通过一个softmax算子进行更新：\nc_{j+1}(ζ) = e^{LP_{c_j}(ζ)/β} / ∑_{ζ′∈𝒯} e^{LP_{c_j}(ζ′)/β}\n其中 β 是温度参数，用于控制分布的尖锐程度。\n\n与现有方法相比，LP-ACRL的创新点在于其简单而有效的课程生成机制。它无需任何关于任务难度或期望性能的先验知识，仅依赖于智能体自身学习动态的在线评估（即回合奖励的变化）。通过将采样概率与学习进度正相关，方法能够自动聚焦于那些智能体正在快速掌握或仍有提升空间的任务（通常对应“适中难度”区域），同时避免在已掌握或暂时无法掌握的任务上浪费采样资源。\n\n## 实验与结果\n实验在IsaacLab仿真环境中进行，使用ANYmal D四足机器人模型。评估了三个逐步复杂的任务空间：1）平坦地形上的多级线性速度跟踪；2）多类型崎岖地形穿越；3）大规模异构任务空间（结合速度命令、地形类型和地形难度级别）。对比的基线方法包括：绝对学习进度（ALP）、优先级别回放（PLR）、简单手工课程（SC）、低奖励优先课程（LRPC）和均匀采样（Uniform）。引入了一个综合评估指标：带稳定性惩罚的回合百分比跟踪误差（EPTE-SP），该指标同时惩罚跟踪误差和因摔倒导致的提前终止。\n\n![性能评估](https://arxiv.org/html/2601.17428v1/x2.png)\n\n> **图3**：在平坦地形多级速度跟踪任务上的EPTE-SP评估。LP-ACRL训练的策略在大多数速度区间都保持了最低的EPTE-SP和最窄的变异范围，表现显著优于所有基线。\n\n![奖励收敛](https://arxiv.org/html/2601.17428v1/x3.png)\n\n> **图4**：训练过程中各任务实例的回合奖励演化。LP-ACRL在几乎所有速度区间都表现出更快的收敛速度和更高的渐近性能。\n\n![采样分布演化](https://arxiv.org/html/2601.17428v1/x4.png)\n\n> **图5**：不同课程方法下的任务采样分布热图。LP-ACRL在训练早期快速推断出任务空间的潜在难度结构（从易到难），并在中后期灵活调整采样，平衡高难度探索与已掌握技能的保持。\n\n在第一个实验（平坦地形速度跟踪）中，LP-ACRL在EPTE-SP和奖励收敛方面均取得最佳整体性能。如图5所示，其采样分布演化清晰地展示了从易到难、再根据学习进度动态调整的自适应过程。\n\n![多地形奖励收敛](https://arxiv.org/html/2601.17428v1/x5.png)\n\n> **图6**：多类型崎岖地形任务上的回合奖励演化。即使在难度结构不明确的任务空间中，LP-ACRL在收敛速度和最终性能上也优于基线。\n\n![多地形采样分布](https://arxiv.org/html/2601.17428v1/x6.png)\n\n> **图7**：多类型崎岖地形任务上的任务采样分布热图。LP-ACRL能有效捕捉潜在难度结构（例如，上下楼梯和斜坡可能更具挑战性），并避免过度关注特定异常任务。\n\n在第二个实验（多类型崎岖地形）中，LP-ACRL同样展现出优越性，验证了其在非结构化任务空间中的有效性。\n\n![大规模任务空间](https://arxiv.org/html/2601.17428v1/x7.png)\n\n> **图8**：大规模异构任务空间示意图。该空间结合了多种地形类型、几何参数以及多级线性和角速度命令，任务难度在策略训练时未知。\n\n![成功率与成功集奖励](https://arxiv.org/html/2601.17428v1/x8.png)\n\n> **图9**：大规模任务空间上的成功率演化（上）和由LP-ACRL在3000次迭代时定义的成功集上的回合奖励（下）。LP-ACRL在1500次迭代内达到80%成功率，远超基线，并在掌握的任务上获得更高性能。\n\n在第三个也是最关键的实验（大规模异构任务空间，包含600个任务实例）中，LP-ACRL展示了强大的可扩展性。如图9所示，它在1500次训练迭代内达到了80%的成功率，而大多数基线方法在3000次迭代后仍难以收敛。同时，在它成功掌握的任务子集上，其策略性能（平均回合奖励）也高于其他方法。\n\n最终，通过师生蒸馏框架，将LP-ACRL训练的策略成功部署到真实的ANYmal D机器人上。该策略在平坦地形上实现了高达3.0 m/s的线性速度，在楼梯、斜坡、碎石等挑战性地形上实现了高达2.5 m/s的线性速度，并在所有地形上实现了高达3.0 rad/s的角速度。据论文所述，这是在该硬件平台上达到的最高崎岖地形运动速度。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了LP-ACRL框架**：一种利用从回合奖励推导的学习进度指标来动态调整任务采样分布的自动课程强化学习方法，无需为可扩展的机器人学习手动构建课程。\n2.  **在多尺度任务空间中验证了有效性**：在从结构化速度跟踪到非结构化大规模异构地形的三个不同复杂度的机器人运动任务空间中，系统地验证了LP-ACRL相对于多种手工和自动课程基线的优越性。\n3.  **实现了高性能的真实机器人部署**：通过策略蒸馏，将仿真中训练的单一策略成功迁移到物理ANYmal D平台，实现了前所未有的高速、鲁棒的崎岖地形运动能力。\n\n论文提到的局限性主要在于方法基于离散化的任务空间运行。虽然连续维度被离散化为区间，但这可能限制了课程在连续参数空间中进行更细粒度调整的能力。\n\n本文对后续研究的启示在于：为复杂、多轴、非结构化的机器人学习任务空间提供了一个强大且简单的自动课程生成基线。其核心思想——利用智能体自身学习动态的反馈（学习进度）来指导课程——可以扩展到其他需要处理高维、耦合任务参数的领域。未来的工作可以探索如何将离散的学习进度估计与连续的任务参数空间更紧密地结合，或者将学习进度的概念与其他课程生成信号（如新奇性、不确定性）相结合，以进一步提升学习效率和最终策略的性能边界。",
      "imageUrls": [
        "https://arxiv.org/html/2601.17428v1/x1.png",
        "https://arxiv.org/html/2601.17428v1/figs/3_Approach/LP-ACRL.png",
        "https://arxiv.org/html/2601.17428v1/x2.png",
        "https://arxiv.org/html/2601.17428v1/x3.png",
        "https://arxiv.org/html/2601.17428v1/x4.png",
        "https://arxiv.org/html/2601.17428v1/x5.png",
        "https://arxiv.org/html/2601.17428v1/x6.png",
        "https://arxiv.org/html/2601.17428v1/x7.png",
        "https://arxiv.org/html/2601.17428v1/x8.png",
        "https://arxiv.org/html/2601.17428v1/x9.png",
        "https://arxiv.org/html/2601.17428v1/x10.png",
        "https://arxiv.org/html/2601.17428v1/x11.png",
        "https://arxiv.org/html/2601.17428v1/x12.png",
        "https://arxiv.org/html/2601.17428v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09767",
      "title": "Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning",
      "url": "http://arxiv.org/abs/2602.09767",
      "arxivId": "2602.09767",
      "date": "2026-02-10",
      "authors": "Wei Li Team",
      "category": "Manipulation",
      "summary": "本文针对四足机器人无监督技能发现中，单一策略学习效率低、技能表征易重叠以及奖励信号易被黑客攻击导致技能多样性不足的问题，提出了正交混合专家（OMoE）架构和多判别器框架。OMoE防止行为表征坍塌，使单一策略能掌握广泛运动技能；多判别器在不同观测空间运作以缓解奖励黑客。在Unitree A1机器人上的实验表明，该方法提升了训练效率，并使状态空间覆盖率比基线提升了18.3%。",
      "detailedSummary": "## 研究背景与动机\n在四足机器人控制领域，深度强化学习需要专家精心设计任务特定的奖励函数，而模仿学习则依赖于昂贵且任务特定的数据。无监督技能发现通过利用内在动机自主探索多样行为，有望减轻这些负担。然而，现有方法（如DIAYN）存在两个关键局限性：首先，它们通常依赖单一策略来掌握多样行为，没有对行为间的共享结构和差异进行建模，导致学习效率低下；其次，它们容易受到“奖励黑客”的影响，即奖励信号迅速增加并收敛，但学到的技能实际多样性不足，未能充分覆盖状态空间。\n\n本文针对上述痛点，提出了名为MOD-Skill的新框架。其核心思路是：通过一个解耦的多判别器架构来独立评估不同观测子空间的技能多样性，以缓解奖励黑客；同时，采用一个正交混合专家策略架构，对运动技能进行分解与正交化表征，以提高学习效率和技能多样性。\n\n## 方法详解\nMOD-Skill框架旨在通过无监督强化学习，发现一组由潜在变量k索引的多样化技能策略πθ(at|otp, k)。其整体流程如算法1所示：在每次训练迭代中，采样一个技能向量k，策略根据当前本体感知观测otp和k生成动作at，与环境交互后获得奖励rt并存储经验，随后使用PPO算法更新策略和价值函数，并使用交叉熵损失更新判别器。\n\n![方法框架总览](https://arxiv.org/html/2602.09767v1/overview.png)\n> **图1**：MOD-Skill框架总览。策略通过强化学习进行优化，判别器通过监督学习进行更新，两者协同促进技能发现。左侧展示了多判别器模块，右侧展示了正交混合专家策略架构。\n\n框架包含两个核心模块：\n1.  **多判别器模块**：该模块旨在最大化潜在技能K与观测Od之间的互信息下界。为了解决单一判别器在复杂观测空间下容易导致奖励黑客的问题，本文设计了多个判别器，每个判别器qi(k|odi)被分配到一个**互不相交**的观测子空间odi（例如[𝒗 𝝎]、[𝒈]、[𝜽 𝜽̇]），并独立计算内在奖励。技能发现奖励rtS是各判别器输出对数概率的平均值减去技能先验对数概率。通过这种解耦设计，鼓励智能体在所有相关的状态维度上进行多样化探索，从而缓解状态混淆和奖励黑客。\n\n2.  **正交混合专家策略**：策略网络采用OMoE架构。其输入是本体感知观测otp和技能向量k的拼接。首先，一组Ne个专家网络{Ei}将输入映射为特征向量ui。然后，关键的一步是使用**Gram-Schmidt过程**对这些特征向量进行正交化，得到正交基{vi}，以此强制专家学习到的表征保持足够的多样性。同时，一个任务编码器T根据相同的输入计算每个正交化专家特征的权重系数α。最后，加权聚合后的表征通过一个共享的输出头f映射到动作空间。这种架构将多样化的运动技能分解为一组正交表征，并由任务编码器自适应融合，从而实现了灵活的专家组合和多样化技能的生成。\n\n与现有方法相比，创新点具体体现在：1) 提出了观测子空间解耦的多判别器设计，从奖励机制上直接针对奖励黑客问题；2) 将正交约束引入技能发现策略的专家混合层，明确建模技能间的差异，提升了学习效率。\n\n## 实验与结果\n实验在拥有12个自由度的Unitree A1四足机器人平台上进行，使用Isaac Sim仿真器并创建4096个并行环境进行训练。策略运行频率为50Hz。对比的基线方法包括不同观测配置的单一判别器架构（SD1, SD2, SD3）以及不同的策略网络架构（MLP, MoE）。\n\n**判别器模块消融实验**：对比了SD1（观测：[𝒗 𝝎]）、SD2（观测：[𝒗 𝝎 𝒈]）、SD3（观测：完整运动观测）和本文的多判别器方法MD。\n\n![奖励曲线对比](https://arxiv.org/html/2602.09767v1/reward_discriminator.png)\n> **图2**：算法SD1、SD2、SD3和MD的奖励曲线。上图显示完整回合奖励，下图仅显示技能奖励部分。SD3的技能奖励上升最快，但出现了明显的奖励黑客（奖励高但技能实际多样性低）。\n\n![状态空间覆盖](https://arxiv.org/html/2602.09767v1/state_coverage_discriminator.png)\n> **图3**：状态空间覆盖可视化。收集各算法发现的技能在20秒内的线性速度、角速度和投影重力观测，并进行归一化。MD方法覆盖的状态空间范围最广。\n\n![状态空间覆盖率数据](https://arxiv.org/html/2602.09767v1/x1.png)\n> **表II**：状态空间覆盖率。MD方法达到了58.7%的覆盖率，相较于最佳基线SD2（49.6%）提升了18.3%。\n\n关键结果：SD3出现了典型的奖励黑客现象，其技能奖励快速收敛，但大多数技能只是在不同姿势下保持静止。而本文的多判别器架构实现了最大的状态空间覆盖范围，整体覆盖率比最佳基线算法提高了18.3%，并产生了在姿势、步态、线速度和角速度方面变化广泛的运动技能。\n\n**策略架构消融实验**：在固定使用多判别器的前提下，对比了MLP、MoE和OMoE三种策略网络。\n\n![策略架构奖励曲线](https://arxiv.org/html/2602.09767v1/reward_omoe.png)\n> **图5**：算法OMoE、MoE和MLP的奖励曲线。OMoE获得了比MoE和MLP更快的奖励增长，表明其学习效率更高。\n\n**真实世界实验**：通过域随机化（随机化地面摩擦、恢复系数、质量、动作延迟、电机扭矩并添加随机速度扰动）来缩小仿真到现实的差距。\n\n![真实世界实验](https://arxiv.org/html/2602.09767v1/real_world.png)\n> **图4**：真实世界实验。学习到的技能被部署在Unitree A1机器人上，在真实环境中展示了可靠且鲁棒的执行能力。\n\n## 总结与启发\n本文的核心贡献可概括为：1) 提出了MOD-Skill框架，创新性地整合了**解耦的多判别器架构**和**正交混合专家策略**，以解决无监督技能发现中的奖励黑客和低效问题；2) 实验表明，该框架将状态空间覆盖率提升了18.3%，有效缓解了奖励黑客，并显著提高了训练效率；3) 成功实现了从仿真到真实四足机器人的技能迁移验证。\n\n论文自身提到的局限性在于，未来工作需要减少对专家知识的依赖，使算法能够通过自主探索发现更丰富、更动态的技能库。这对后续研究的启示在于：无监督技能发现中，对技能多样性进行**结构化、解耦的激励**，以及对策略内部表征施加**明确的多样性约束**，是提升算法性能的有效途径。如何自动化地划分观测子空间或确定专家数量，是值得探索的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09767v1/overview.png",
        "https://arxiv.org/html/2602.09767v1/reward_discriminator.png",
        "https://arxiv.org/html/2602.09767v1/state_coverage_discriminator.png",
        "https://arxiv.org/html/2602.09767v1/real_world.png",
        "https://arxiv.org/html/2602.09767v1/reward_omoe.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09638",
      "title": "VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model",
      "url": "http://arxiv.org/abs/2602.09638",
      "arxivId": "2602.09638",
      "date": "2026-02-10",
      "authors": "Hui Xiong Team",
      "category": "Manipulation",
      "summary": "本文解决3D可操作区域（affordance）定位问题，指出现有方法依赖静态语言/图像线索，缺乏动态交互上下文。为此，作者构建了大规模视频数据集VIDA，并提出VideoAfford模型。关键技术包括：利用多模态大语言模型增强分割与推理能力；通过潜在动作编码器从人-物交互视频中提取动态先验；引入空间感知损失以学习3D空间知识。实验表明，该模型显著优于现有方法，并展现出强大的开放世界泛化与推理能力。",
      "detailedSummary": "## 研究背景与动机\n3D可承受性（affordance）定位旨在突出3D物体上可操作的区域，对于机器人操作至关重要。先前的研究主要集中于从语言和图像等静态线索中学习可承受性知识，这些方法难以提供揭示时间和因果线索的充足动态交互上下文。为了缓解这一困境，本文收集了一个全面的基于视频的3D可承受性数据集VIDA，并提出了一种新的任务：从人类-物体交互（HOI）演示视频中定位3D物体可承受性，旨在利用大规模演示视频语料库进行以物体为中心的3D可承受性推理。本文的核心思路是激活多模态大语言模型（MLLMs）额外的可承受性分割能力，通过引入潜在动作编码器从HOI视频中提取动态交互先验，并结合空间感知损失函数，在一个统一框架内实现世界知识推理和细粒度的3D可承受性定位。\n\n## 方法详解\n整体框架接受一个HOI视频和一个文本指令作为输入，目标是输出物体点云上的可承受性掩码。VideoAfford主要由四个组件构成：1）一个受益于大规模3D表示学习的3D视觉编码器，为密集预测任务提供坚实基础；2）一个预训练的潜在动作编码器，提供丰富的动作先验；3）一个展现出可承受性推理能力的视频多模态大语言模型（MLLM）；4）一个基于Transformer的轻量级可承受性解码器，用于生成密集的3D可承受性掩码。\n\n![方法总览](https://arxiv.org/html/2602.09638v1/x4.png)\n> **图4**：VideoAfford方法总览。给定一个HOI视频和对应的点云，模型使用LanguageBind作为视频编码器，RenderNet作为动作编码器，获得视频嵌入和潜在动作嵌入。这些嵌入与文本指令一同输入大语言模型，预测语言令牌和可承受性令牌。另一方面，使用预训练的3D编码器提取语义丰富的点嵌入，通过几何引导的上采样和传播模块获得密集点特征。最后，可承受性令牌和点特征被输入可承受性解码器以生成可承受性掩码。\n\n**核心模块与技术细节**：\n1.  **点云编码器**：采用在大型文本-图像-点配对数据上预训练的3D编码器作为骨干。给定点云，编码器将其编码为语义丰富的点嵌入，随后通过几何引导的上采样操作将语义特征传播为密集点特征。\n2.  **空间感知损失函数**：为了解决先前工作只关注点类别准确性而缺乏对“空间连续性和区域重叠”约束的问题，本文引入了空间损失。其核心机制是结合点云的空间邻域信息分配自适应权重，从而支持空间连续性的训练。该损失基于改进的Dice损失，通过计算点坐标的邻近度来赋予空间相邻点更高权重，强调聚类正样本在损失计算中的贡献。\n3.  **动作编码器**：为了增强动作理解能力，引入了潜在动作编码器来从紧凑的状态表示中学习可泛化的人类-物体交互动作。给定HOI视频，采样N帧并使用动作编码器提取潜在动作嵌入，并将其压缩为两个令牌。\n4.  **视频MLLM主干**：选择Video-LLaVA作为主干。当输入视频和文本时，使用视频编码器和动作编码器对视频进行稀疏编码以获得视频令牌和动作令牌，将它们拼接后作为LLM的输入。模型扩展了Video-LLaVA的词表，注入了特殊令牌`<AFF>`来代表可承受性世界知识，该令牌的隐藏状态被投影为查询嵌入，作为可承受性条件输入解码器。\n5.  **可承受性解码器**：一个基于Transformer的轻量级解码器，利用可承受性嵌入和密集点特征，通过交叉注意力模块进行融合，最终经由一个MLP网络生成可承受性掩码。\n6.  **训练目标**：总损失函数是交叉熵损失（用于文本输出）、二元交叉熵损失、IoU损失（用于分割掩码预测）以及前述空间感知损失的加权和。\n\n**创新点**：与现有主要依赖静态图像或语言的方法相比，本文的创新主要体现在：1）**任务与数据创新**：首次系统性地提出并构建了从HOI视频中学习物体中心3D可承受性的任务和大规模数据集VIDA。2）**方法创新**：设计了统一框架，将视频MLLM的世界知识推理能力、专门的动作编码器提取的动态先验，以及针对3D形态特点设计的空间感知损失相结合，实现了动态交互信息向3D空间知识的有效迁移。\n\n## 实验与结果\n**数据集与基准**：实验在本文新构建的VIDA数据集上进行，该数据集包含约38K个HOI视频和22K个带标注的点云，涵盖38个物体类别和16种可承受性类型。实验分为“可见”（训练和测试集中的物体及可承受性类别一致）和“不可见”（测试集中包含训练集未出现的物体或可承受性类别）两种设置。由于是新任务，没有直接的前期工作。因此，作者选择了几种先进的基于HOI图像的3D可承受性定位方法作为模块化基线进行比较，包括XMF、PFusion、IAGNet、LASO、GREAT、Seqafford，并复现了LMAfford3D。\n\n**关键实验结果**：\n\n![主结果表](https://arxiv.org/html/2602.09638v1/x2.png)\n> **表2**：主要对比结果。展示了所有对比方法的mIoU、AUC、SIM和MAE指标（均为百分比）。最佳结果以粗体标出，次佳结果以下划线标出。结果表明，无论是在“可见”还是“不可见”设置下，本文提出的VideoAfford方法在所有评估指标上均显著优于所有基线方法。例如，在“可见”设置下，mIoU达到28.20%，比次优的GREAT（23.62%）高出4.58个百分点；在“不可见”设置下，mIoU为10.95%，同样领先。\n\n![消融实验表](https://arxiv.org/html/2602.09638v1/x3.png)\n> **表3**：消融实验结果。该实验验证了动作编码器和空间感知损失函数对模型性能的贡献。结果表明，两者均能独立带来性能提升，而结合使用时效果最佳。在“可见”设置下，同时使用两者使mIoU从基线20.16%提升至28.20%；在“不可见”设置下，从7.12%提升至10.95%，证明了这两个组件对于提升模型动态理解能力和空间预测一致性的有效性。\n\n![可视化结果](https://arxiv.org/html/2602.09638v1/x5.png)\n> **图5**：定性可视化结果。第一列是输入的HOI视频，最后一列是真实标注。中间列展示了VideoAfford与基线方法（IAGNet, GREAT）的预测结果对比。可视化显示，本文方法能产生更完整、空间上更连贯的可承受性区域预测，更接近真实标注，而基线方法的预测往往更加零散或不准确。\n\n**消融实验总结**：动作编码器通过引入动态交互先验，提升了模型对动作意图的理解；空间感知损失函数通过强制模型学习空间连续的目标区域，有效防止了预测结果碎片化，确保了分割目标的空间紧凑性。两者协同工作，是模型取得优越性能的关键。\n\n## 总结与启发\n**核心贡献**：\n1.  **任务与数据集**：首次系统性地提出了“从HOI视频中定位3D物体可承受性”的新任务，并构建并开源了大规模基准数据集VIDA，填补了该领域缺乏动态交互视频数据的空白。\n2.  **方法**：提出了VideoAfford这一强基线方法，创新性地将视频MLLM的世界知识、专用动作编码器提取的动态先验，以及针对3D空间特性设计的损失函数相结合，实现了动态视频信息向3D可承受性知识的有效迁移与推理。\n3.  **性能**：通过大量实验验证了所提方法的优越性，其在标准设置和开放词汇泛化（不可见类别）设置下均显著超越现有先进方法，展示了其实用潜力。\n\n**局限性**：论文自身提到，当前方法依赖于视频-点云配对数据进行训练。虽然训练时视频和点云不需要严格的一对一配对，但这种数据对的收集和标注仍然具有一定成本。此外，方法整合了多个大型预训练模型，在计算效率方面可能存在优化空间。\n\n**对后续研究的启示**：\n1.  **数据利用**：探索如何更高效地利用大量未标注或弱标注的互联网HOI视频，减少对精确配对标注的依赖。\n2.  **模型效率**：研究更轻量化的架构或训练策略，以降低VideoAfford类模型的计算和部署成本，使其更适合机器人等嵌入式平台。\n3.  **因果推理**：HOI视频包含丰富的时序和因果信息（接触前、接触中、接触后）。未来工作可以深入探索如何显式地建模这些因果线索，以进一步提升可承受性推理的深度和可解释性。\n4.  **任务扩展**：本框架展示了利用MLLM和动态先验处理3D空间理解任务的潜力，可启发将其应用于其他需要结合动态观察与静态几何理解的具身AI任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09638v1/x1.png",
        "https://arxiv.org/html/2602.09638v1/x2.png",
        "https://arxiv.org/html/2602.09638v1/x3.png",
        "https://arxiv.org/html/2602.09638v1/x4.png",
        "https://arxiv.org/html/2602.09638v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09153",
      "title": "SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes",
      "url": "http://arxiv.org/abs/2602.09153",
      "arxivId": "2602.09153",
      "date": "2026-02-09",
      "authors": "Russ Tedrake Team",
      "category": "Manipulation",
      "summary": "本文针对机器人仿真训练中室内场景过于简化、缺乏真实物理复杂性的问题，提出了SceneSmith框架。该框架采用分层智能体架构，通过设计师、评论家和协调员等VLM智能体，分阶段从语言提示生成仿真就绪的3D场景，并整合文本到3D合成与物理属性估计。实验表明，其生成场景的物体数量是基线方法的3-6倍，物体间碰撞率低于2%，物理稳定性达96%，在用户研究中真实性与提示忠实度胜率均超过90%。",
      "detailedSummary": "## 研究背景与动机\n仿真已成为大规模训练和评估家庭机器人的关键工具，但现有环境无法捕捉真实室内空间的多样性和物理复杂性。当前的场景合成方法生成的是家具稀疏的房间，缺乏机器人操作所必需的密集杂物、可动家具和物理属性。现有的室内场景合成方法，无论是程序化、数据驱动还是基于大语言模型（LLM）的，主要针对家具级别的布局和视觉真实性，将小物体、可动资产和物理属性视为次要。这与机器人需求错位，机器人需要密集的可操作物体、层次化的支撑关系和物理上有效的配置。\n\n本文针对上述痛点，提出了一种新的视角：将场景生成视为一个由智能体驱动的、层次化的决策过程，并紧密集成资产生成以确保仿真就绪。本文的核心思路是：引入一个名为SceneSmith的层次化、智能体化框架，通过设计者、评论者和协调者三个视觉语言模型（VLM）智能体之间的交互，分阶段（从建筑布局到家具摆放再到小物体填充）构建可直接用于物理仿真的室内环境。\n\n## 方法详解\nSceneSmith的整体框架是一个层次化的场景构建流水线。输入是自然语言提示𝒯，输出是可直接仿真的完整室内场景𝒮。构建过程像一棵树：根节点生成建筑布局（房间数M及各房间几何结构𝒢_j）；每个房间作为独立分支，依次进行家具、壁挂物、天花板灯具的放置（由房间特定提示𝒯_j引导）；随后，选中的支撑实体（如桌面、书架、地板区域）会衍生出新的分支，用于填充小的可操作物体（由实体特定提示𝒯_{j,k}引导）。所有分支最终汇合成扁平的场景表示。\n\n![层次化场景构建流水线](https://arxiv.org/html/2602.09153v1/x2.png)\n> **图2**：SceneSmith的层次化场景构建流水线。场景提示𝒯由布局智能体处理，生成M个房间的建筑几何。每个房间随后通过家具、壁挂和天花板安装阶段独立填充，使用房间特定提示𝒯_j。在每个房间内，K_j个支撑实体随后形成额外的分支，使用实体特定提示𝒯_{j,k}填充可操作物体。彩色高亮表示每个阶段添加的物体。每个阶段（彩色三角形）通过设计者、评论者和协调者之间的智能体交互实现。堆叠的框架表示并行分支。\n\n核心模块是贯穿每个构建阶段的“智能体三元组”：设计者（Designer）、评论者（Critic）和协调者（Orchestrator）。设计者使用结构化工具（如放置资产、调整姿态、专用对齐工具）提出对当前场景状态的修改。评论者使用观察和验证工具（如查询元数据、渲染视图、检查碰撞）评估结果场景的语义合理性、物理可行性与阶段目标的一致性，并提供分数和自然语言反馈。协调者管理交互流程，跟踪分数，决定是接受提案、请求进一步细化还是终止阶段，并维护检查点以防止迭代细化过程中的性能退化。这种分离降低了自我评估偏差。\n\n智能体通过工具与场景交互。工具分为功能类别：状态观察、视觉观察、场景修改、资产获取和可行性验证。物体放置相对于支撑表面在SE(2)中进行，然后通过支撑表面的已知姿态提升为完整的SE(3)姿态。此外，不同阶段暴露专用工具，例如家具有对齐工具，可操作物体填充阶段有将多个资产组装成复合组的工具。\n\n资产生成与路由是另一个核心模块，确保开放词汇和仿真就绪。当设计者请求资产时，资产路由器将请求分解并选择合适的获取策略。对于静态物体，主要使用生成式文本到3D合成：根据描述生成参考图像，分割前景，重建纹理3D网格，然后规范化方向、缩放到目标尺寸，并添加碰撞几何和估计的物理属性。对于可动物体（如橱柜），从ArtVIP等关节物体库中检索。对于地毯、海报等扁平装饰元素，则创建带有物理材质属性的“薄覆盖物”。所有资产都经过完整性检查和基于VLM的语义验证。\n\n![文本到3D资产生成流水线](https://arxiv.org/html/2602.09153v1/x3.png)\n> **图3**：文本到3D资产生成流水线。给定物体描述，我们生成图像，分割前景，并重建纹理3D网格。网格被增强以包含碰撞几何（灰色凸块）和由VLM估计的物理属性，包括质量、质心、摩擦力和惯性（蓝色椭球体）。网格也被缩放到目标尺寸。\n\n为确保仿真就绪，在生成后应用轻量级后处理：首先通过非线性优化解决物体间穿透问题，将物体位置投影到最近的无碰撞配置；然后在Drake仿真器中模拟重力，让不稳定的物体沉降到静态稳定配置。\n\n与现有方法相比，SceneSmith的创新点具体体现在：1）提出了一个层次化的智能体交互范式（设计者-评论者-协调者），将决策、评估和控制流分离，支持结构化迭代优化；2）紧密集成了按需生成资产（静态）、检索（可动）和估计物理属性的流水线，实现了开放词汇和物理真实性；3）采用了层次化的提示细化机制，使局部决策与全局意图保持一致，并支持跨表面的协调放置。\n\n## 实验与结果\n实验使用了210个多样化的提示，涵盖五个类别：SceneEval-100房间提示、类型多样性提示（如宠物店、瑜伽室）、物体密度提示、主题场景和房屋级别多房间提示。对比了五个外部基线方法：HSM、Holodeck、I-Design、LayoutVLM（使用其原始资源库和Objaverse库）和SceneWeaver。此外，还进行了六项消融实验：无评论者、非生成资产、无资产验证、无专用工具、无场景观察、无智能体记忆。评估包括205名参与者的人类研究（3051份有效回复）和自动评估。自动评估使用SceneEval指标（CNT物体计数，ATR物体属性，OOR物体间关系，OAR物体与建筑关系，ACC可访问性，NAV可导航性，OOB越界），并增加了两个物理指标：COL碰撞率和STB静态平衡率。\n\n![与HSM和Holodeck的定性对比](https://arxiv.org/html/2602.09153v1/x4.png)\n> **图4**：与HSM和Holodeck的定性对比，这是用户研究中最强的两个基线。SceneSmith产生了更密集且更好地满足提示要求的场景。\n\n关键实验结果如下：在人类研究中，SceneSmith在房间级别提示上，相对于所有基线，平均真实感胜率为92.2%，平均提示忠实度胜率为91.5%（所有p<0.001）。在房屋级别提示上，相对于唯一的多房间基线Holodeck，真实感胜率为80.3%，忠实度胜率为84.7%。SceneSmith平均每个房间生成71.1个物体，是基线方法（11-23个）的3-6倍。在物理指标上，SceneSmith的物体间碰撞率低于2%（1.2%），95.6%的物体在物理仿真下保持稳定；而基线方法的碰撞率在3-29%之间，稳定性在8-61%之间。自动评估指标（表2）显示，SceneSmith在CNT、ATR、OOR、OAR、STB上表现最佳或接近最佳，在COL和OOB上表现优异，但在ACC和NAV上略低于某些优化了这些指标的基线。\n\n消融实验总结：**资产生成**（NotGenerated）对真实感和忠实度影响最大（胜率分别降至63.8%和67.0%），表明按需生成资产至关重要。**资产验证**（NoAssetValidation）和**视觉观察**（NoObserveScene）对质量有显著负面影响。**专用工具**（NoSpecializedTools）、**智能体记忆**（NoAgentMemory）和**评论者**（NoCritic）的缺失会导致性能下降，但在统计上不显著，表明基础智能体交互具有一定鲁棒性，但这些组件共同贡献了最佳性能。\n\n## 总结与启发\n本文的核心贡献包括：1）提出了SceneSmith，一个用于从自然语言构建仿真就绪室内环境的层次化、智能体化框架；2）开发了一个集成的资产生成和路由流水线，结合了文本到3D合成与可动物体检索，并为所有资产增强了碰撞几何和物理属性；3）展示了SceneSmith在用户研究和自动指标上均优于基线，能生成更密集、无碰撞且物理稳定的场景，并演示了其在从自然语言任务描述到自动成功验证的端到端机器人策略评估流水线中的应用。\n\n论文提到的局限性包括：其评估的VLM自动指标存在假阳性和假阴性；资产生成依赖于当前的文本到3D和图像分割模型，可能产生几何伪影；虽然支持多房间生成，但尚未探索房间间的高级关系（如声音传播）；评估器代理的任务验证是非确定性的。\n\n本工作对后续研究的启示在于：为大规模机器人仿真提供了一种高质量、可扩展的场景生成方法；展示了智能体分工协作（设计-评论-协调）在复杂空间推理任务中的有效性；将生成式资产创建与物理仿真需求紧密结合，推动了“仿真就绪”内容生成的发展方向。未来的工作可以探索更复杂的跨房间关系、更鲁棒的资产生成技术，以及将此类生成环境直接用于机器人策略训练。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09153v1/x1.png",
        "https://arxiv.org/html/2602.09153v1/x2.png",
        "https://arxiv.org/html/2602.09153v1/x3.png",
        "https://arxiv.org/html/2602.09153v1/x4.png",
        "https://arxiv.org/html/2602.09153v1/x5.png",
        "https://arxiv.org/html/2602.09153v1/x6.png",
        "https://arxiv.org/html/2602.09153v1/x7.png",
        "https://arxiv.org/html/2602.09153v1/x8.png",
        "https://arxiv.org/html/2602.09153v1/x9.png",
        "https://arxiv.org/html/2602.09153v1/x10.png",
        "https://arxiv.org/html/2602.09153v1/x11.png",
        "https://arxiv.org/html/2602.09153v1/x12.png",
        "https://arxiv.org/html/2602.09153v1/x13.png",
        "https://arxiv.org/html/2602.09153v1/x14.png",
        "https://arxiv.org/html/2602.09153v1/x15.png",
        "https://arxiv.org/html/2602.09153v1/x16.png",
        "https://arxiv.org/html/2602.09153v1/x17.png",
        "https://arxiv.org/html/2602.09153v1/x18.png",
        "https://arxiv.org/html/2602.09153v1/x19.png",
        "https://arxiv.org/html/2602.09153v1/x20.png",
        "https://arxiv.org/html/2602.09153v1/x21.png",
        "https://arxiv.org/html/2602.09153v1/x22.png",
        "https://arxiv.org/html/2602.09153v1/x23.png",
        "https://arxiv.org/html/2602.09153v1/x24.png",
        "https://arxiv.org/html/2602.09153v1/x25.png",
        "https://arxiv.org/html/2602.09153v1/x26.png",
        "https://arxiv.org/html/2602.09153v1/x27.png",
        "https://arxiv.org/html/2602.09153v1/x28.png",
        "https://arxiv.org/html/2602.09153v1/x29.png",
        "https://arxiv.org/html/2602.09153v1/x30.png",
        "https://arxiv.org/html/2602.09153v1/x31.png",
        "https://arxiv.org/html/2602.09153v1/x32.png",
        "https://arxiv.org/html/2602.09153v1/x33.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09583",
      "title": "Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation",
      "url": "http://arxiv.org/abs/2602.09583",
      "arxivId": "2602.09583",
      "date": "2026-02-10",
      "authors": "Danica Kragic Team",
      "category": "Manipulation",
      "summary": "本文研究如何让机器人在操作可变形物体（如布料）时适应人类个性化偏好。针对偏好难以量化且演示数据有限的问题，作者提出了RKO方法，该方法融合了RPO和KPO框架的优势，能够高效地对预训练的视觉运动扩散策略进行偏好对齐微调。在真实布料折叠任务上的实验表明，采用RKO等偏好对齐策略相比标准微调方法，在任务性能和样本效率上均表现出显著优越性。",
      "detailedSummary": "## 研究背景与动机\n在机器人日益融入日常生活的背景下，对能够反映个体用户偏好的自适应行为的需求日益增长。变形物体操作（DOM），如衣物折叠，是一个特别相关但研究不足的领域。此类任务中的用户偏好（如不同的折叠风格）通常是微妙、个人化且难以用语言描述的，使得物理演示成为表达偏好的自然方式。然而，收集演示数据成本高昂，因此需要样本高效的适应框架。\n\n当前，数据驱动的学习方法，特别是视觉运动扩散策略，已在大规模演示数据集上展现出强大的泛化能力。然而，如何使这些预训练模型适应并反映用户特定的操作偏好，同时避免灾难性遗忘且不依赖大量新演示，仍是一个挑战。尽管偏好优化技术（如DPO、RPO、KTO）在文本到图像生成等领域取得成功，但它们在机器人DOM中的应用仍然有限。\n\n本文针对上述痛点，研究如何利用有限的演示，将预训练的视觉运动扩散策略与用户偏好的行为对齐。核心思路是：提出一种名为RKO的新型偏好对齐方法，它结合了KTO的样本效率与RPO的上下文感知加权优点，旨在以更少的演示数据实现更优的策略对齐。\n\n## 方法详解\n整体框架基于一个预训练的参考扩散策略，该策略在大量无偏好演示数据集 $D_{\\text{ref}}$ 上训练，代表执行任务的默认或中性策略。为了将其与用户偏好对齐，引入一个新的偏好数据集 $D_{\\text{pref}}$，其中包含“获胜”（偏好）演示和“失败”（非偏好）演示。然后，使用偏好损失函数对策略 $\\pi_{\\theta}$ 进行微调，通过显式对比偏好与非偏好行为，引导策略学习偏好行为，同时抑制非偏好行为。这种方法比仅在获胜演示上训练扩散策略更有效和样本高效。\n\n![方法框架](https://arxiv.org/html/2602.09583v1/imgs/method_reb.png)\n> **图2**：本文采用的通用偏好对齐框架。首先在大型参考演示集 $D_{\\text{ref}}$ 上训练参考模型。为了将其与用户偏好策略对齐，收集新的获胜演示，并与失败演示（来自 $D_{\\text{ref}}$ 或其他来源）组合成 $D_{\\text{pref}}$。偏好损失通过对比偏好与非偏好行为来对齐新策略 $\\pi_{\\theta}$。\n\n**核心模块与技术细节**：\n1.  **视觉运动扩散模型基础**：采用DDPM框架，将扩散过程应用于动作序列 $A_t$，并通过以观测 $O_t$ 为条件的分数网络 $\\epsilon_{\\theta}$ 建模条件分布 $p(A_t | O_t)$。训练损失为预测噪声的MSE损失：$\\mathcal{L}_{DDPM}(\\theta)=MSE(\\epsilon_k, \\epsilon_{\\theta}(O_t, A_t^0+\\epsilon_k, k))$。\n\n2.  **偏好对齐框架**：\n    *   **Diffusion-DPO**：直接对比偏好与非偏好行为，避免奖励建模和强化学习。其损失函数衡量当前策略 $\\epsilon_{\\theta}$ 相对于参考策略 $\\epsilon_{\\text{ref}}$，在处理偏好与非偏好演示时的相对优势。\n    *   **Diffusion-RPO**：扩展DPO，引入基于语义相似性的上下文重加权方案。每个批次内的每个获胜样本会与所有失败样本进行对比，相似度高的配对获得更大权重（公式5），使模型更关注语义上相近的困难样本对。\n    *   **Diffusion-KTO**：仅需每个样本的二元（获胜/失败）标签，无需配对比较。它通过计算每个样本策略与参考策略的偏差，并利用Sigmoid效用函数进行对齐。\n    *   **Diffusion-RKO (本文提出)**：结合KTO与RPO的优势。它像KTO一样使用二元标签，同时集成RPO的基于相似性的批次重加权，以强调困难负样本和模糊的获胜样本。具体为：首先计算批次中获胜与失败样本嵌入的相似度矩阵 $\\omega_{i,j}$；然后为每个样本计算一个权重标量 $s_b$（对于获胜样本，权重为 $1 + \\max_j \\omega_{i,j}$，强调靠近决策边界的样本；对于失败样本，权重为 $\\sum_i \\omega_{i,j}$，强调被许多获胜样本包围的困难负样本）；最后使用这些权重对KTO损失进行加权：$\\mathcal{L}_{\\text{Diffusion-RKO}}(\\theta)=-\\frac{1}{\\sum_{b=1}^{B}s_b}\\sum_{b=1}^{B}s_b\\cdot\\sigma\\left(q_b\\cdot A_b\\right)$。\n\n**创新点**：本文的核心创新在于提出了RKO方法，它首次将KTO的单一样本二元反馈效率与RPO的上下文感知对比加权机制相结合。这种结合使得模型能够更聚焦于学习特征空间中偏好与非偏好行为重叠的“困难”区域，从而在理论上和实验上实现了更高效的偏好对齐。\n\n## 实验与结果\n**实验设置**：\n*   **任务与数据集**：在真实机器人上进行布料折叠实验，涉及三种衣物（裤子、袖子、T恤），每种衣物对应三种不同的折叠偏好设置（pref_1, pref_2, pref_3），如图3所示。采用循环数据集构建策略，确保每个偏好评估时，获胜、失败演示和参考模型训练数据来自不同的偏好源。\n*   **评估指标**：采用逐步评分方案，根据拾取和放置动作的正确性打分（图3），总分归一化为1。这比二元成功/失败提供了更丰富的信号。\n*   **对比方法**：Diffusion-DPO, Diffusion-RPO, Diffusion-KTO, 提出的Diffusion-RKO，以及仅在偏好演示上训练的普通DDPM基线。\n*   **实验平台**：两个UFactory Lite6机器人，配备三个RealSense D435相机（鸟瞰图和腕部）。\n\n![任务图示](https://arxiv.org/html/2602.09583v1/imgs/fold_tasks.png)\n> **图3**：三种衣物类型各自的三种折叠偏好图示。每个子图显示了左臂（橙色）和右臂（浅蓝色）的拾取（圆圈）和放置（菱形）位置。底部分数已归一化，完整正确折叠序列的总分为1。\n\n![实验设置](https://arxiv.org/html/2602.09583v1/imgs/robot_setup.png)\n> **图4**：实验装置实物图。\n\n**关键实验结果**：\n![结果对比](https://arxiv.org/html/2602.09583v1/imgs/trousers_vs_sleeves_halfpage.png)\n> **图5**：裤子和袖子折叠任务的性能对比结果。左图显示在固定60个获胜演示下，各方法在所有偏好上的平均成功率。右图显示了样本效率曲线，即随着获胜演示数量从20增加到95，平均成功率的变化。\n\n1.  **性能对比**：在固定使用60个获胜演示进行训练时，RKO在裤子和袖子任务上的平均表现最佳（裤子：~0.72成功率；袖子：~0.68成功率），显著优于普通DDPM基线（裤子：~0.58；袖子：~0.52）和其他偏好优化方法（DPO, RPO, KTO）。\n2.  **样本效率**：随着训练演示数量增加，所有方法性能均提升。RKO在几乎所有数据规模下都保持领先，尤其是在数据较少时（如20个演示）优势更明显，证明了其卓越的样本效率。\n3.  **消融实验**：对RKO进行消融实验（将相似性重加权因子设为1，即退化为未加权的KTO），结果显示加权版本性能更好，验证了相似性重加权机制的有效性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**RKO**，一种新颖的偏好对齐方法，巧妙结合了KTO的样本效率与RPO的上下文感知对比加权机制。\n2.  首次在变形物体操作（DOM）领域，对多种偏好优化框架（DPO、RPO、KTO、RKO）在对齐预训练扩散策略方面的性能进行了系统性的实证比较。\n3.  在三种真实衣物折叠任务上进行了广泛评估，证明了基于偏好的对齐方法（尤其是RKO）相比普通扩散策略微调，在性能和样本效率上的优越性。\n\n**局限性**：论文提到，其实验评估集中于布料折叠任务，虽然涵盖了不同衣物类型和偏好，但方法的通用性在其他DOM任务（如辅助穿衣）或更广泛的机器人操作中仍需进一步验证。\n\n**启示**：这项工作凸显了结构化偏好学习在扩展复杂变形物体操作任务中个性化机器人行为方面的重要性和可行性。RKO的成功表明，结合不同偏好优化范式的优势是提升对齐效率的有效途径。它为未来研究指明了方向：如何将此类方法更广泛地应用于多样化的机器人技能个性化，以及如何进一步降低对高质量人类演示数据的依赖。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09583v1/imgs/first_page.png",
        "https://arxiv.org/html/2602.09583v1/imgs/method_reb.png",
        "https://arxiv.org/html/2602.09583v1/imgs/fold_tasks.png",
        "https://arxiv.org/html/2602.09583v1/imgs/robot_setup.png",
        "https://arxiv.org/html/2602.09583v1/imgs/trousers_vs_sleeves_halfpage.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09878",
      "title": "MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.09878",
      "arxivId": "2602.09878",
      "date": "2026-02-10",
      "authors": "Xiangyu Yue Team",
      "category": "Manipulation",
      "summary": "本文提出MVISTA-4D，旨在解决机器人操作中现有世界模型无法预测完整、几何一致的4D场景动态的问题。方法核心包括：1）一个能从单视角RGBD观测生成任意视角、跨模态一致RGBD的4D世界模型，通过跨视角与跨模态特征融合确保几何对齐；2）测试时动作优化策略，通过生成模型反向传播推断最优轨迹潜在变量，并结合残差逆动力学模型将其转化为精确动作。实验在三个数据集上验证了该方法在4D场景生成与下游操作任务上的优越性能，消融实验明确了关键设计的有效性。",
      "detailedSummary": "## 研究背景与动机\n机器人操作领域，基于世界模型的“想象-行动”范式展现出巨大潜力。然而，现有方法存在关键局限性。主流方法可分为两类：一类是纯图像空间的视频生成模型，它们虽能预测未来外观，但常违反3D几何约束，导致想象结果与可执行动作间存在鸿沟；另一类是基于点云或粒子等3D几何表示的动力学模型，虽几何明确但外观和语义信息较弱，限制了精细操作所需的保真度。在动作推断方面，现有范式同样面临挑战：联合预测未来与动作易导致误差累积；基于几何的流程依赖脆弱的姿态估计；而经典的逆动力学模型则因其“多对一”的映射关系而具有不适定性，即多种动作可解释相同的观测变化，尤其在部分可观测和接触场景下。\n\n本文针对上述痛点，提出了一个新颖的视角：构建一个几何一致、多视图的4D世界模型，并将动作轨迹作为整体条件嵌入生成过程。其核心思路是：1）从单视图RGB-D观测出发，生成几何一致的多视图未来RGB-D序列，以提供更完整的动态场景结构；2）将整个动作序列压缩为低维轨迹潜在代码，作为生成模型的风格条件，并通过测试时优化该代码来推断与想象未来一致的动作。\n\n## 方法详解\nMVISTA-4D的整体框架基于一个潜在视频扩散模型。输入为参考视图的单帧RGB-D观测、目标相机外参和文本指令。输出为参考视图及所有目标视图上同步的、几何一致的未来RGB-D序列。这些序列可通过反投影和融合，得到动态场景的点云序列。动作推断则通过优化轨迹潜在代码并经过残差逆动力学模块精炼完成。\n\n![方法整体框架](https://arxiv.org/html/2602.09878v1/x1.png)\n\n> **图1**：MVISTA-4D方法整体框架。左侧：给定单视图初始观测（RGB-D）、文本指令和多个目标相机姿态，模型生成多视图、几何一致的未来RGB-D序列。右侧：动作推断阶段，首先通过测试时优化找到与文本条件生成的未来最匹配的轨迹潜在代码，解码得到动作先验，再通过残差逆动力学模块进行精炼，输出最终可执行动作。\n\n**核心模块一：特征集成与结构化分词**。为有效建模多视图、跨模态的生成，论文设计了结构化的分词策略。在每个视图内，将RGB和Depth的潜在图沿宽度方向拼接，使同一空间位置的跨模态令牌相邻，以鼓励跨模态一致性。在视图之间，由于视差和遮挡，像素级对齐不适定，因此采用沿高度方向拼接视图，以促进结构级的跨视图连贯性。这种布局支持可变数量的输入视图。\n\n**核心模块二：跨模态特征融合**。为明确鼓励RGB与深度间的一致性，引入了两个设计：1）**可学习的模态令牌**：为外观和几何特征分别添加一个可学习的令牌，以显式指示模态身份。2）**局部跨模态注意力**：在标准的自注意力模块前，插入一个轻量级的局部跨模态注意力模块。该模块仅在一个小邻域内交换外观与几何特征，并通过门控残差更新进行融合，这既提供了跨模态对应的强归纳偏置，又将计算成本从全局二次匹配降低到线性。\n\n**核心模块三：跨视图几何一致性学习**。这是确保多视图预测几何一致的关键。1）**相机嵌入作为视图令牌**：不同于将外参矩阵扁平化，论文提出使用以共享注视点为中心的球坐标参数化相机，并应用傅里叶特征，得到一个紧凑的13维嵌入。这使模型更容易获取尺度线索，并作为区分不同视图的令牌。2）**几何感知的可变形跨视图注意力**：利用已知相机参数，一个视图中的查询令牌在其他视图上诱导出一条对极线。该模块沿此对极线稀疏采样K个候选位置进行跨视图注意力，而非全局计算。此外，还引入了一个可变形细化步骤，根据查询特征、初始关键特征及其相似性预测一个小偏移，以调整采样位置，实现更好的对齐。\n\n**核心模块四：轨迹条件化作为风格代码**。为将动作轨迹的结构化信息注入生成过程，论文训练了一个基于TCN的变分自编码器，将动作序列编码为一个低维潜在令牌序列。在训练生成器时，将此轨迹潜在代码作为S个风格令牌，通过交叉注意力注入。为防止生成器忽略该条件，增加了一个轻量级的潜在一致性头，从生成器的最终层表征重建轨迹潜在代码，并施加重建损失。该头部仅在训练时使用。\n\n**核心模块五：具身推断与规划**。在测试时，动作推断分为两步：1）**轨迹潜在推断（测试时优化）**：首先仅使用文本条件生成一个未来序列V_bar。然后冻结生成器，通过反向传播优化一个随机初始化的轨迹潜在代码z，使其条件生成的序列尽可能接近V_bar。优化在低维潜在空间进行，比直接优化完整动作序列更稳定。优化得到的最优代码z*通过TCN解码器解码为动作先验序列。2）**残差逆动力学模块**：为解决逆动力学的不适定性，该模块将上述解码得到的动作先验作为强先验，仅预测一个修正项Δa_t。最终动作为先验动作与修正项之和。这使得模块专注于局部对齐和执行级调整，而非从零重建整个动作。\n\n## 实验与结果\n实验在三个数据集上进行：两个合成数据集RLBench和RoboTwin2，以及一个包含14个任务的真实机器人多视图数据集。对比的基线方法包括：UniPi（2D视频世界模型，增强深度后称为UniPi*）、TesserAct（单视图RGB-DN 4D世界模型）和4DGen（两视图点图4D世界模型）。所有方法均在共同的WAN2.2 TI2V骨干网络上实现以进行公平比较。\n\n**4D场景生成结果**：在合成和真实数据集上，MVISTA-4D在大多数外观（FVD, SSIM, PSNR）和几何指标（AbRel, RMSE, δ1, CD, EMD）上均优于基线。特别是在几何一致性指标（如CD和EMD）上提升显著，证明了其多视图几何一致生成的有效性。\n\n![4D生成定性结果](https://arxiv.org/html/2602.09878v1/x2.png)\n\n> **图2**：在RoboTwin数据集上的4D生成定性结果。红、绿、蓝框代表不同视点。MVISTA-4D生成的各视图RGB和深度序列在几何和外观上均保持高度一致，而基线方法（如TesserAct）在不同视图间存在明显的几何不一致和伪影。\n\n**下游机器人操作结果**：在模拟和真实机器人操作任务中，MVISTA-4D在任务成功率上 consistently 超越所有基线。例如，在RLBench的10个任务上，MVISTA-4D平均成功率为78.5%，显著高于TesserAct的69.2%和4DGen的64.8%。在真实机器人14个任务上，MVISTA-4D也取得了最高成功率。\n\n![消融实验](https://arxiv.org/html/2602.09878v1/x3.png)\n\n> **图3**：关键组件的消融研究结果。移除跨模态融合（w/o X-mod）或跨视图几何注意力（w/o X-view GeoAttn）均导致性能下降，验证了它们对生成质量的重要性。使用扁平化相机外参（w/ Flatten E）替代球坐标嵌入也会损害性能。在动作推断中，移除测试时优化（w/o TTO）或残差逆动力学（w/o RIDM）都会显著降低操作成功率。\n\n**消融实验**：论文系统地验证了各核心组件的贡献。如图3所示，移除跨模态融合、跨视图几何注意力或使用次优的相机表示，都会导致生成质量和操作成功率下降。在动作推断方面，测试时优化和残差逆动力学模块都是提升最终动作质量的关键。\n\n![动作推断结果](https://arxiv.org/html/2602.09878v1/x4.png)\n\n> **图4**：动作推断的定性结果。左侧展示了通过测试时优化轨迹潜在代码，从生成的未来中恢复出的动作序列（蓝色）与真实动作（橙色）对比，显示出良好的一致性。右侧展示了残差逆动力学模块的修正效果，先验动作（绿色）经修正（红色箭头）后更接近真实动作（蓝色）。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1）提出了一个具身的多视图4D生成世界模型，通过显式的跨视图和跨模态融合机制，实现了几何一致的多视图RGB-D序列预测。2）引入了轨迹级动作条件化，将完整动作序列表示为紧凑的潜在代码，并提出了基于测试时优化的动作推断方法，缓解了逆动力学的不适定性。3）设计了一个基于先验的残差逆动力学模型，通过仅学习修正项来提升动作执行的鲁棒性。\n\n论文自身提到的局限性包括：在极端遮挡或快速运动导致外观模糊的情况下，几何一致性可能受损；测试时优化和多次视图生成会带来额外的计算成本。\n\n这项工作对后续研究提供了重要启示：将动作视为具有时空结构的轨迹而非独立步骤，是连接生成模型与机器人控制的有效途径；在生成模型中显式地嵌入几何和相机约束，对于实现物理上合理的预测至关重要；结合优化与学习的混合推理范式，为解决不适定问题（如动作推断）提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09878v1/x1.png",
        "https://arxiv.org/html/2602.09878v1/x2.png",
        "https://arxiv.org/html/2602.09878v1/x3.png",
        "https://arxiv.org/html/2602.09878v1/x4.png",
        "https://arxiv.org/html/2602.09878v1/figure/cross_mod.png",
        "https://arxiv.org/html/2602.09878v1/x5.png",
        "https://arxiv.org/html/2602.09878v1/x6.png",
        "https://arxiv.org/html/2602.09878v1/x7.png",
        "https://arxiv.org/html/2602.09878v1/figure/HW_setup1.png",
        "https://arxiv.org/html/2602.09878v1/x8.png",
        "https://arxiv.org/html/2602.09878v1/figure/frame_0032.png",
        "https://arxiv.org/html/2602.09878v1/figure/failure_case_robotwin.png",
        "https://arxiv.org/html/2602.09878v1/x9.png",
        "https://arxiv.org/html/2602.09878v1/x10.png",
        "https://arxiv.org/html/2602.09878v1/x11.png",
        "https://arxiv.org/html/2602.09878v1/x12.png",
        "https://arxiv.org/html/2602.09878v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10013",
      "title": "Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper",
      "url": "http://arxiv.org/abs/2602.10013",
      "arxivId": "2602.10013",
      "date": "2026-02-10",
      "authors": "Yen-Ling Kuo Team",
      "category": "Manipulation",
      "summary": "本文旨在解决机器人对易损物体（如薯片）进行精细力控操作的难题。研究提出了一种低成本（约150美元）的触觉力控夹爪TF-Gripper（力范围0.45–45N），并设计了RETAF学习框架，将高频触觉力调节与机械臂姿态预测解耦。实验表明，相比位置控制，直接力控显著提升了抓取稳定性与任务成功率；触觉反馈对力调节至关重要，RETAF框架在多种任务中均优于基线方法。",
      "detailedSummary": "## 研究背景与动机\n当前机器人操作领域的主流方法，无论是否使用触觉反馈，大多将夹爪控制简化为二值开/合动作或直接预测夹爪位置/宽度。这种位置控制可被视为力量控制的间接代理，但通过位置控制产生合适的力量高度依赖于物体和任务。例如，对于外观相似但物理特性不同的物体（如尺寸略有差异的薯片或随时间软化的樱桃番茄），稳定的抓取宽度范围可能差异巨大或完全不同，而稳定的抓取力量范围则更为相似，能带来更鲁棒的策略。然而，许多提供力控的商业夹爪往往价格昂贵、难以跨机器人适配，且最小力量较高，不适用于精细、柔软、易碎的日常物体。本文旨在研究机器人如何有效学习类人的触觉-力量能力，核心痛点是缺乏低成本、支持精细力控且兼容触觉感知的硬件，以及现有低频策略架构难以适应力控所需的快速、接触驱动的反应式调节。本文的核心思路是：1）开发一个低成本、通用且集成触觉感知的力控夹爪TF-Gripper；2）提出一个将力控与末端执行器位姿预测解耦的策略框架RETAF，以实现高频、反应式的力量调节。\n\n## 方法详解\n本文提出了一个包含硬件（TF-Gripper）和软件（RETAF策略框架）的完整系统，用于学习力量调节操作。\n\n![方法框架](https://arxiv.org/html/2602.10013v1/x6.png)\n\n> **图6**：RETAF框架概述。基础策略以低频率从视觉和本体感知观测中预测末端执行器位姿和夹爪开/合动作。当夹爪闭合时，力量适应策略被激活，通过对手腕视图和触觉输入的联合注意力，以高频率预测抓取力。\n\n**整体框架**：RETAF框架将控制策略解耦为两个组件：一个用于轨迹生成的**基础策略**和一个用于接触调节的**力量适应策略**。基础策略以相对较低的频率运行，根据非触觉观测（如全局视觉）预测机械臂位姿`a_t_pose`和离散的夹爪动作`k_t`（开或合）。当基础策略发出抓取意图（`k_t = close`）时，力量适应策略被激活，以高频率（>30 Hz）根据手腕视图图像`I_t_wrist`和触觉读数`T_t`预测连续的目标抓取力`f_t`。最终的夹爪动作`a_t_grip`由此决定（闭合时输出预测力，张开时输出空动作）。\n\n**核心模块一：TF-Gripper硬件**。TF-Gripper是一个由两个Dynamixel XL430-W250-T驱动器驱动的低成本（约150美元）平行夹爪。\n\n![硬件设计](https://arxiv.org/html/2602.10013v1/x3.png)\n\n> **图3**：TF-Gripper的硬件设计。(a)关键组件：适配器将夹爪连接到机器人，两个电机通过拉动同步带驱动指尖沿线性导轨运动。(b)整体TF-Gripper设置，主体结构完全3D打印。(c)提供手腕视图的相机，以及集成了触觉传感器的软质指尖。\n\n其核心设计采用滑轮-同步带传动机制。每个电机驱动一个滑轮拉动同步带，从而驱动一个指尖沿低摩擦线性导轨做平行运动。这种设计有效力矩臂由滑轮半径决定，提供了从电机扭矩到指尖力的可靠且可重复的映射。指尖配备来自UMI的软质指尖垫和低成本柔性压阻式触觉传感器FlexiTac。通过适配器可兼容不同机器人。电机电流与指尖力呈现近线性关系（见图4），实现了约0.45–45 N范围内的精确开环力控。\n\n![电流-力关系](https://arxiv.org/html/2602.10013v1/x4.png)\n\n> **图4**：驱动器电流（PWM）–力的关系。力在指尖接触面测量，反映了TF-Gripper施加的有效抓取力。\n\n**核心模块二：遥操作设备**。为了收集包含人力控制的演示数据，本文设计了一个低成本的遥操作接口。该设备使用VR控制器控制机器人末端位姿，并配备两个指环，指环连接与夹爪相同的Dynamixel驱动器。驱动器被配置为提供类似弹簧的阻力，在通过电机电流测量操作者所施加力量的同时提供局部动觉反馈，该信号被校准并映射到机器人夹爪驱动器的电流命令。\n\n**核心模块三：RETAF策略**。\n1.  **基础策略**：利用现有的视觉运动策略架构（如扩散策略），无需为触觉融合修改结构。通过行为克隆进行训练，目标是最小化预测与专家动作`a_t*`之间的损失。\n2.  **力量适应策略**：当被激活时，该策略通过一个**联合注意力层**专门处理手腕视图视觉观测和触觉感知。手腕视图提供紧凑的、以物体为中心的视觉信息（类别、形状、表面外观），触觉感知提供局部反馈（变形、初始滑动）。这种设计使其能够进行精确稳定的力控，而不被全局场景中的无关信息干扰。该策略通过监督回归进行训练，目标是最小化预测力与演示者施加力`f_t*`之间的均方误差。其轻量级设计允许在单块RTX 5070 Ti上实现80 Hz+的推理频率。\n\n**创新点**：与现有方法相比，创新点主要体现在：1）硬件上，提供了一个低成本、宽力域、集成触觉且跨平台兼容的开环力控夹爪解决方案；2）算法上，首次明确地将力控预测从位姿预测中解耦，并设计了一个专用于高频力控、融合手腕视图与触觉的轻量化策略模块，解决了耦合策略中的频率失配和学习不稳定问题。\n\n## 实验与结果\n**实验设置**：在真实世界中对五个需要精确力量调节的操作任务进行评估：豆腐抓取、薯片拾取、樱桃番茄拾取、液体转移、樱桃番茄采摘。实验平台为配备TF-Gripper的Franka Research 3机器人，使用第三方视角和手腕视图相机。每个任务收集50条人类演示轨迹。评估时每个任务进行10次 rollout，并将任务分解为三个阶段报告平均成功率：到达、稳定抓取、任务成功。\n\n**Baseline方法**：对比了扩散策略及其变体：DP（无触觉）、DP（有触觉）、ViTac-MAE（使用掩码自编码预训练触觉编码器）。这些baseline的夹爪动作可以是位置或力量。\n\n![任务环境](https://arxiv.org/html/2602.10013v1/x7.png)\n\n> **图7**：评估环境概述，说明了每个任务在此设置中的执行方式。\n\n**关键实验结果**：\n\n![结果表格](https://arxiv.org/html/2602.10013v1/x9.png)\n\n> **图9**：（对应论文表II）五个操作任务在三个阶段（到达R、稳定抓取G、任务成功S）的性能。RETAF结合力控在稳定抓取（平均68%）和任务成功（平均60%）率上显著优于所有基线。\n\n表II（对应图9）显示：\n1.  **力控 vs. 位置控 (RQ1)**：在所有策略中，力控相比位置控 consistently 实现了更高的稳定抓取率。例如，RETAF使用力控将稳定抓取率从44%（位置控）提升至68%，任务成功率从28%提升至60%。这表明力控的优势主要体现在物理接触阶段。\n2.  **触觉的必要性 (RQ2)**：对比DP w/ Tac和DP w/o Tac，在力控设置下，加入触觉反馈使平均稳定抓取率从32%提升至38%，任务成功率从12%提升至20%，证实了触觉对精确力控至关重要。\n3.  **RETAF的有效性 (RQ3)**：RETAF（力控）在几乎所有任务和阶段上都显著优于非反应式的基线（DP, ViTac-MAE）。其平均稳定抓取率（68%）和任务成功率（60%）均为最高。\n4.  **与不同基础策略的集成 (RQ4)**：实验表明RETAF框架可以与不同的位姿预测策略（此处以DP为基础策略）有效集成并带来性能提升。\n\n![力量预测曲线](https://arxiv.org/html/2602.10013v1/x8.png)\n\n> **图8**：在验证用人类演示上的力量预测随时间变化曲线。对比了扩散策略（DP/Force）和RETAF的预测与真实值（Human Demo）。RETAF的预测更贴合真实的人类力量调整轨迹，表现出更好的反应性。\n\n图8进一步提供了定性分析，显示RETAF能够更快速、更精确地响应接触动态（如触觉信号变化），其预测的力量曲线比基线DP更贴近人类演示。\n\n**消融实验总结**：实验本身通过对比不同变体（有无触觉、位置vs力控、不同策略架构）构成了系统的消融研究。结果总结表明：1) 直接力控优于间接位置控；2) 触觉反馈是有效力控的必要条件；3) RETAF的解耦、高频反应式架构相比耦合的低频策略有显著优势。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了TF-Gripper，一个低成本、具有精细力控范围（0.45–45 N）并集成触觉感知的通用夹爪，附带一个能直接记录人力输入的遥操作设备，为学习力控策略提供了硬件和数据收集基础。\n2.  提出了RETAF策略框架，创新性地将力控与末端执行器位姿预测解耦，通过一个专用于高频力控、融合手腕视图与触觉的轻量化模块，实现了反应式的力量适应。\n3.  在五个真实世界力敏任务上进行了全面评估，实证了直接力控、触觉反馈以及RETAF框架相对于现有位置控制和耦合策略的优越性。\n\n**局限性**：论文提到TF-Gripper的力控本质上是开环的（基于电流-力映射），尽管通过机械设计（同步带传动）保持了良好的一致性，但仍可能受到温度漂移等因素影响（通过温度补偿可将误差降至4%以下）。\n\n**后续研究启示**：这项工作为规模化学习机器人力控策略开辟了道路。TF-Gripper的低成本和开源特性有望降低该领域的研究门槛。RETAF的解耦思想为将力控模块集成到更广泛的机器人学习架构中提供了范例。未来工作可探索在更复杂、非结构化的场景中应用此框架，或结合更强大的基础策略和感知模型。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10013v1/x1.png",
        "https://arxiv.org/html/2602.10013v1/x2.png",
        "https://arxiv.org/html/2602.10013v1/x3.png",
        "https://arxiv.org/html/2602.10013v1/x4.png",
        "https://arxiv.org/html/2602.10013v1/x5.png",
        "https://arxiv.org/html/2602.10013v1/x6.png",
        "https://arxiv.org/html/2602.10013v1/x7.png",
        "https://arxiv.org/html/2602.10013v1/x8.png",
        "https://arxiv.org/html/2602.10013v1/x9.png",
        "https://arxiv.org/html/2602.10013v1/x10.png",
        "https://arxiv.org/html/2602.10013v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10093",
      "title": "UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking",
      "url": "http://arxiv.org/abs/2602.10093",
      "arxivId": "2602.10093",
      "date": "2026-02-10",
      "authors": "Yao Mu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人接触密集型操作任务中触觉数据获取困难、缺乏统一评估平台的问题，提出统一仿真平台UniVTAC。其核心包括：1）支持三种常用触觉视觉传感器的数据生成平台；2）基于仿真合成数据训练的触觉视觉编码器UniVTAC Encoder；3）包含八个代表性任务的基准测试UniVTAC Benchmark。实验表明，集成该编码器使基准测试平均成功率提升17.1%，真实机器人实验任务成功率提高25%。",
      "detailedSummary": "## 研究背景与动机\n机器人操作领域在视觉-语言-动作策略的推动下取得了快速进展。然而，对于插入等接触丰富的操作任务，仅依赖视觉感知通常难以鲁棒完成，因为末端执行器会造成遮挡、近距离深度精度有限，且物理接触一旦发生，直接的交互状态难以被视觉观测。视触觉感知通过直接捕获接触界面的局部几何、力分布和相对运动，提供了互补信息，对于接触丰富的操作场景至关重要。当前，视触觉操作的基础设施尚不完善：一方面，大规模、可靠的触觉数据在物理世界中获取成本高昂且具有挑战性，严重限制了触觉中心表示模型的训练；另一方面，缺乏统一、全面的视触觉操作基准测试，阻碍了对触觉驱动策略的系统性评估和改进。现有的基于仿真的数据生成流程大多专注于刚性或关节物体操作，对触觉交互的复杂动力学（如瞬态接触力、变形、滑动）建模支持有限，其开环或弱约束的执行可能产生对真实触觉传感器不安全或不具信息量的轨迹。本文针对大规模触觉数据稀缺和缺乏统一评估平台这两个核心痛点，提出了UniVTAC，其核心思路是构建一个基于仿真的统一平台，用于可扩展、可控地合成视触觉交互数据，并基于此数据训练一个触觉中心的编码器，同时建立一个包含八个代表性任务的基准测试来系统评估策略性能。\n\n## 方法详解\nUniVTAC是一个端到端的仿真驱动框架，涵盖数据合成、表示学习和系统评估。其整体流程是：首先在仿真平台中自动化生成大规模带标注的视触觉交互数据；然后利用这些数据，通过从仿真物理信号中衍生的辅助监督目标，训练UniVTAC编码器，学习触觉中心的表示；最后，将该编码器集成到下游策略中，并在UniVTAC基准测试上进行评估。\n\n![方法框架](https://arxiv.org/html/2602.10093v1/x1.png)\n> **图1**：UniVTAC编码器框架及其与策略学习的集成。(a) UniVTAC编码器通过三个自监督目标（形状重建、接触变形预测和物体位姿回归）进行预训练，以从原始视触觉观测中学习结构化的触觉中心表示。(b) 在部署时，预训练的编码器作为感知模块集成到下游操作策略中，支持从原始触觉图像进行端到端的策略学习，且不引入额外的推理开销。\n\n**UniVTAC平台**：基于TacEx框架构建，扩展了软体仿真能力。其核心模块包括：1) **传感器配置**：集成了三种主流视触觉传感器（GelSight Mini, ViTai GF225, Xense WS），通过调整内部相机内参、凝胶垫网格和渲染方法进行建模。2) **自动化操作API**：实现了原子操作原语库（抓取、移动、放置、探测、旋转）。其中`Grasp`和`Probe`原语引入了触觉反应式自适应控制机制，通过一个依赖于实时触觉传感器最小深度`d_min`的反馈律来调节夹爪关节速度，防止非物理穿透并确保捕获的触觉印记保持在现实的变形流形内，从而生成高质量的接触数据。\n\n**UniVTAC编码器**：这是一个多通路表示学习框架，旨在通过任务驱动的监督将结构化物理先验嵌入到触觉观测中。其设计基于三个感知先决条件：形状感知、接触感知和位姿感知。编码器采用ResNet-18作为共享主干网络，将触觉观测映射为紧凑的潜在表示。在训练时，该潜在表示被多个通路特定的解码头使用：\n- **形状感知**：通过双视图重建进行监督，即同时重建带标记的原始触觉图像`I_marked`和不带标记的纯净触觉图像`I_pure`，鼓励编码器从传感器特定的标记伪影中解耦出内在物体几何形状。\n- **接触感知**：通过预测表示凝胶垫法向压痕的密集表面深度图`D`，以及编码横向剪切和切向变形的标记点2D投影`M`进行监督，使表示能捕获真实的接触力学。\n- **位姿感知**：通过回归物体相对于凝胶垫中心局部坐标系的7维位姿`p`（3D平移+4元数朝向）进行监督，将潜在表示锚定在度量空间中。\n\n训练采用多任务损失函数`L_total = λ_s * L_shape + λ_c * L_contact + λ_p * L_pose`，其中各项均为均方误差损失，权重λ_s=1.0, λ_c=0.5, λ_p=0.5。部署时，所有解码头被丢弃，仅保留编码器，因此没有额外推理开销。\n\n![重建结果](https://arxiv.org/html/2602.10093v1/x2.png)\n> **图2**：重建结果。从带标记的触觉图像出发，UniVTAC编码器重建出互补的物理信号，包括无标记触觉图像、凝胶垫变形深度图和标记点位置。结果表明，学习到的表示能够捕获超越传感器特定视觉模式的全局形状线索和细粒度接触变形。\n\n**创新点**：与现有方法相比，其创新性主要体现在：1) **数据生成策略**：引入了基于触觉反馈的闭环夹爪控制，确保生成的数据物理一致且富含信息性接触模式；2) **表示学习设计**：明确构建了形状、接触、位姿三个感知通路的多任务监督框架，将物理先验结构化地注入表示中，而非仅依赖像素级重建或全局对比对齐。\n\n## 实验与结果\n**实验设置**：在**UniVTAC基准测试**上进行评估，该基准包含八个代表性视触觉操作任务（见图3），分为位姿推理（Lift Bottle, Lift Can, Put Bottle in Shelf）、形状感知（Grasp Classify）和接触丰富交互（Insert Hole, Insert Tube, Insert HDMI, Pull Out Key）三类。基准测试基于NVIDIA Isaac Sim构建，支持自动化任务级数据合成与评估，并引入了随机故障和纠正行为来丰富接触模式，同时采用基于物理的成功标准（如限制最大穿透深度、检测滑动）来确保评估可靠性。\n\n![基准测试任务](https://arxiv.org/html/2602.10093v1/x3.png)\n> **图3**：UniVTAC基准测试任务。包含八个代表性视触觉操作任务，涵盖形状识别、位姿推理和接触丰富交互。每个任务展示了两个执行关键帧的代表性视觉和触觉观测。\n\n**对比方法**：主要对比了三种策略：1) **ACT**：仅使用视觉输入的动作分块Transformer策略。2) **VITaL**：一种利用视触觉预训练的强代表性视触觉操作策略。3) **Ours**：ACT策略，但其视觉输入被替换为集成了UniVTAC编码器提取的触觉表征。\n\n**关键实验结果**：如表I所示，在八个任务的平均成功率上，仅视觉的ACT为30.9%，VITaL为40.5%，而集成了UniVTAC编码器的Ours方法达到了48.0%，相比纯视觉ACT提升了17.1个百分点。提升在多个接触敏感任务（如Insert Hole, Insert HDMI, Insert Tube）上尤为明显。在形状分类任务（Grasp Classify）上，Ours达到了99%的成功率，与VITaL（100%）相当，显著优于纯视觉ACT（50%）。\n\n**消融实验与贡献分析**：论文通过消融实验分析了编码器不同感知通路的贡献。\n\n![消融实验](https://arxiv.org/html/2602.10093v1/x4.png)\n> **图4**：UniVTAC编码器的消融研究。展示了在基准测试任务上，移除形状（-Shape）、接触（-Contact）或位姿（-Pose）监督目标后的性能下降。完整模型（Ours）在所有任务上表现最佳，表明多任务监督对于学习通用触觉表示至关重要。\n\n如图4所示，移除形状、接触或位姿监督中的任何一个都会导致性能下降，证明了多任务监督设计的有效性。其中，接触感知通路对插入类任务贡献最大，位姿感知对提升和放置任务很重要，而形状感知对分类任务至关重要。\n\n**仿真到现实验证**：论文进一步进行了真实世界机器人实验。\n\n![真实世界实验](https://arxiv.org/html/2602.10093v1/x5.png)\n> **图5**：真实世界实验设置。使用配备ViTai GF225触觉传感器的机器人夹爪进行HDMI插入任务。\n\n![真实世界结果](https://arxiv.org/html/2602.10093v1/x6.png)\n> **图6**：真实世界HDMI插入任务的成功率。比较了使用仅视觉观测、仅触觉观测（通过UniVTAC编码器）以及视触觉融合观测的策略性能。\n\n在真实的HDMI插入任务中（图5），使用UniVTAC编码器提供的触觉表征的策略，相比仅使用视觉的策略，成功率从60%提升至85%，相对提升了25%（图6）。这证明了仅使用仿真合成数据训练的UniVTAC编码器能够有效迁移到真实世界。\n\n![定性结果](https://arxiv.org/html/2602.10093v1/x7.png)\n> **图7**：在UniVTAC基准测试上的定性结果。比较了ACT（仅视觉）、VITaL和集成了UniVTAC编码器的ACT（Ours）在“Put Bottle in Shelf”任务中的表现。Ours策略能够利用触觉反馈进行更精准的放置。\n\n## 总结与启发\n**核心贡献**：1) 提出了**UniVTAC平台**，一个支持多种传感器、可扩展且可控的视触觉操作数据仿真合成框架，其闭环自适应控制确保了数据质量。2) 提出了**UniVTAC编码器**，一种通过形状、接触、位姿多通路监督进行大规模仿真预训练的视触觉编码器，能学习触觉中心的通用表示。3) 提出了**UniVTAC基准测试**，一个包含八个任务的仿真基准，支持自动化数据生成和统一策略评估，并设计了富含接触模式的轨迹合成方法和基于物理的成功标准。\n\n**局限性**：论文提到，仿真与真实世界之间的差距（sim-to-real gap）仍然存在。此外，编码器的架构设计（如ResNet-18）和预训练目标可能并非最优，仍有改进空间。\n\n**后续启示**：UniVTAC为视触觉操作研究提供了一个从数据生成、表示学习到系统评估的完整工具链，有望推动触觉感知的标准化和规模化研究。其多通路监督的表示学习范式强调了将物理理解注入模型的重要性。未来的工作可以探索更高效的编码器架构、更先进的sim-to-real迁移技术，以及将平台扩展到更复杂的多指灵巧手操作场景。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10093v1/x1.png",
        "https://arxiv.org/html/2602.10093v1/x2.png",
        "https://arxiv.org/html/2602.10093v1/x3.png",
        "https://arxiv.org/html/2602.10093v1/x4.png",
        "https://arxiv.org/html/2602.10093v1/x5.png",
        "https://arxiv.org/html/2602.10093v1/x6.png",
        "https://arxiv.org/html/2602.10093v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.17135",
      "title": "ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning",
      "url": "http://arxiv.org/abs/2601.17135",
      "arxivId": "2601.17135",
      "date": "2026-01-23",
      "authors": "Friedhelm Schwenker Team",
      "category": "Manipulation",
      "summary": "论文针对模仿学习忽略人类语义知识、导致样本效率低的问题，提出ConceptACT方法。该方法扩展了Action Chunking with Transformers，在训练时利用情节级概念注释（如对象属性、空间关系），通过修改Transformer架构实现概念感知交叉注意力来集成语义信息。实验表明，ConceptACT在两种机器人操作任务上比标准ACT收敛更快、样本效率更高，且注意力机制集成显著优于辅助预测损失或语言条件模型。",
      "detailedSummary": "## 研究背景与动机\n当前机器人模仿学习的主流方法，如动作分块Transformer（ACT），主要依赖从人类演示中采集的低级传感器数据（如关节位置、图像），而忽略了人类在演示任务时自然具备的丰富语义知识（如物体属性、空间关系、任务约束）。这种仅基于低级数据的学习方式，在需要复杂条件推理的操作任务中（例如根据物体属性进行排序），学习效率可能不足，且无法利用人类易于提供的概念性指导。\n\n本文针对模仿学习模型缺乏语义归纳偏置这一痛点，提出在训练阶段利用人类提供的片段级（episode-level）语义概念注释来提升学习效率的新视角。与需要在部署时提供语义输入（如语言指令）的方法不同，ConceptACT仅在一次性演示收集阶段要求人类标注概念，对标注负担极小，训练后的策略在部署时仅需标准低级输入。其核心思路是扩展ACT架构，通过引入概念Transformer，在编码器最后一层实现概念感知的交叉注意力，并利用人类提供的概念注释对该注意力进行监督对齐，从而为策略学习注入强大的语义归纳偏置。\n\n## 方法详解\nConceptACT的整体框架建立在ACT基础之上，其输入包括当前状态（多视角图像、本体感知）、潜在风格变量以及本片段的人类概念注释，输出是预测的未来动作序列（块）。核心创新在于对ACT编码器-解码器架构的修改，具体是在编码器的最后一层用概念感知的交叉注意力机制替换标准的自注意力机制。\n\n![方法框架](https://arxiv.org/html/2601.17135v1/images/approach.png)\n\n> **图1**：ConceptACT方法整体框架。顶部：在正常的模仿学习流程中，允许用户在收集演示时标注该片段包含哪些概念。底部：通过扩展ACT架构来集成这些概念，增加了一个概念Transformer，使其注意力机制与给定的概念对齐，对齐损失可纳入总损失中。红色部分表示修改。\n\n**核心模块：概念Transformer集成**\nConceptACT的核心是将概念Transformer（Rigotti et al., 2022）集成到ACT的编码器中。具体而言，修改发生在编码器的最后一层（第N层）。在这一层，标准的自注意力机制被替换为一种特殊的交叉注意力机制：\n*   **查询（Q）**：源自前一层的编码器输出 `X^(N-1)`。\n*   **键（K）和值（V）**：并非来自输入序列，而是来自一组可学习的“概念嵌入” `C = [c_1, ..., c_C]`，其中每个 `c_i` 对应于一个语义概念（如“红色”、“立方体”）。\n因此，该层的注意力权重矩阵 `A ∈ ℝ^(S×C)` 表示每个输入位置（S个）对每个概念（C个）的关注程度。\n\n**概念监督与损失函数**\n为了引导注意力机制关注有意义的语义概念，论文利用人类提供的片段级概念注释进行监督。概念被组织为多个互斥的概念类（如颜色、形状），每个片段对每个概念类有一个独热编码的标签 `H^(e), T_j`。在训练时，计算概念对齐损失，即预测的注意力矩阵 `A` 与人类提供的概念标签 `H`（经过适当广播和拼接以匹配维度）之间的Frobenius范数平方：\n`L_concept = ||A - H||_F^2`。\n该损失鼓励模型在编码信息时，其注意力模式与人类标注的语义概念对齐。\n\n**整体训练目标**\nConceptACT的总训练损失结合了ACT原有的损失和概念对齐损失：\n`L_total = L_ACT + λ * L_concept`。\n其中 `L_ACT` 包括动作序列的L1重建损失和潜在变量的KL散度正则化损失，`λ` 是控制概念对齐强度的超参数。通过联合优化，模型在学习模仿动作的同时，其内部表示被约束去关注与任务相关的高级语义概念。\n\n**与现有方法的创新对比**\n1.  **与标准ACT相比**：创新在于引入了语义概念作为额外的监督信号，并通过注意力机制进行架构层面的集成，而非简单地将其作为额外的输入特征。\n2.  **与语言条件化模型（如LAV-ACT）相比**：ConceptACT在部署时不依赖语言指令，概念信息仅用于训练阶段的监督，降低了部署复杂度。\n3.  **与简单的辅助预测任务相比**：论文强调，通过注意力机制进行架构集成（即概念Transformer）比添加一个预测概念的辅助任务头（ACT-Aux）更为有效，后者被证明性能提升有限。\n\n## 实验与结果\n**实验设置**：论文在两个模拟的机器人“抓取-放置”任务上进行评估，均涉及基于物体属性的条件排序逻辑。任务1要求根据颜色规则排序（如“红色物体必须放在蓝色物体之前”），任务2要求根据形状规则排序。实验平台使用Franka Emika Panda机器人和PyBullet模拟器。概念注释包括物体的颜色和形状。\n\n**对比基线**：\n1.  **标准ACT**：作为主要基线。\n2.  **ACT-Aux**：在ACT上增加一个辅助任务，直接预测概念标签，用于对比架构集成与简单辅助任务的优劣。\n3.  **LAV-ACT**：一种语言增强的ACT变体，在部署时需要语言指令，用于对比训练时概念监督与部署时语言条件化的差异。\n\n**关键实验结果**：\n1.  **样本效率**：在有限演示数据下，ConceptACT显著优于所有基线。例如，在任务1中，仅使用10个演示时，ConceptACT成功率约为70%，而标准ACT低于20%，ACT-Aux约为30%，LAV-ACT约为45%。随着数据量增加，ConceptACT始终领先。\n\n![数据规模评估任务1](https://arxiv.org/html/2601.17135v1/images/robot_eval_datasize.png)\n\n> **图5**：在任务1（颜色排序）上，不同训练数据量下的成功率对比。ConceptACT在所有数据规模下都优于基线方法，尤其是在数据稀缺时（10-20个演示）优势更明显。\n\n![数据规模评估任务2](https://arxiv.org/html/2601.17135v1/images/robot_eval_datasize_task2.png)\n\n> **图6**：在任务2（形状排序）上，不同训练数据量下的成功率对比。ConceptACT同样展现出显著优势，特别是在小数据情况下。\n\n2.  **收敛速度**：在训练过程中，ConceptACT的测试损失下降更快，且能达到更低的损失平台，表明其学习更高效、泛化更好。\n\n![测试损失曲线](https://arxiv.org/html/2601.17135v1/images/test_loss.png)\n\n> **图7**：训练过程中的测试损失曲线。ConceptACT收敛速度明显快于ACT和ACT-Aux，且最终达到更低的损失平台。\n\n3.  **训练周期效率**：在固定数据量下，ConceptACT所需的训练周期数远少于基线方法即可达到高性能。\n\n![训练周期评估](https://arxiv.org/html/2601.17135v1/images/robot_eval_epochs.png)\n\n> **图8**：固定数据量下，随着训练周期增加的成功率变化。ConceptACT在早期训练周期就达到高成功率，而其他方法需要更多训练时间。\n\n**消融实验总结**：\n论文通过对比ConceptACT与ACT-Aux的结果，进行了有效的消融研究。结果表明，**通过注意力机制进行架构集成是性能提升的关键**。简单的辅助预测任务（ACT-Aux）虽然比标准ACT略有提升，但远不及ConceptACT的效果，这证明了将语义概念深度整合到模型推理流中的重要性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种将片段级语义概念系统性地集成到基于Transformer的模仿学习中的方法（ConceptACT），仅在演示收集阶段需要极低成本的概念标注。\n2.  通过实验证明，通过注意力机制进行架构集成（概念Transformer）比使用简单的辅助预测损失能更有效地利用概念监督，从而获得更优的学习效率和性能。\n3.  为概念引导的模仿学习提供了实证依据，表明语义归纳偏置能显著提升在需要条件推理的机器人操作任务中的样本效率。\n\n**局限性**：\n论文自身提到的局限性包括：概念注释仍需要人工参与（尽管成本低）；研究的概念类型相对简单（离散属性）；方法在更复杂的、涉及长期规划或动态环境中的任务上的可扩展性尚未验证。\n\n**对后续研究的启示**：\n1.  **自动化概念获取**：探索如何从演示视频或语言叙述中自动提取概念，进一步减少人工标注需求。\n2.  **扩展概念复杂性**：将方法扩展到更复杂、层级化或连续的概念，以及关系型概念（如“A在B左边”）。\n3.  **应用于更广泛的任务**：验证ConceptACT在需要更复杂逻辑推理、多步骤规划或非结构化环境中的机器人任务上的有效性。\n4.  **跨任务概念迁移**：研究在多个任务上学到的概念表示是否能够迁移，以加速新任务的学习。",
      "imageUrls": [
        "https://arxiv.org/html/2601.17135v1/images/approach.png",
        "https://arxiv.org/html/2601.17135v1/images/scene_camera.png",
        "https://arxiv.org/html/2601.17135v1/images/whole_setup.jpg",
        "https://arxiv.org/html/2601.17135v1/images/wrist_camera.png",
        "https://arxiv.org/html/2601.17135v1/images/robot_eval_datasize.png",
        "https://arxiv.org/html/2601.17135v1/images/robot_eval_datasize_task2.png",
        "https://arxiv.org/html/2601.17135v1/images/test_loss.png",
        "https://arxiv.org/html/2601.17135v1/images/robot_eval_epochs.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10109",
      "title": "ST4VLA: Spatially Guided Training for Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2602.10109",
      "arxivId": "2602.10109",
      "date": "2026-02-10",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "本文提出ST4VLA，旨在解决大型视觉语言模型（VLM）在具体任务中难以将抽象指令转化为低级动作的问题。方法采用**空间引导训练**，包含两个阶段：**空间基础预训练**（通过点、框和轨迹预测学习可迁移的空间先验）和**空间引导动作后训练**（通过空间提示引导动作生成）。实验表明，该方法显著提升了VLA模型的性能，在Google Robot上准确率从66.1提升至84.6，在WidowX Robot上从54.7提升至73.2，并在泛化性和抗干扰性上表现出优势。",
      "detailedSummary": "## 研究背景与动机\n当前，将大型视觉语言模型（VLMs）的能力扩展到具身任务面临核心挑战：机器人不仅需要理解指令的含义，还需确定在三维世界中“在哪里”以及“如何”行动。现有方法主要分为两类：一是基于分层机器人系统的方法，它们利用基础模型显式编码空间先验，但依赖基于规则的任务分解和手动设计的规划启发式方法，难以自动扩展到复杂多样的任务，并限制了端到端策略学习的潜力；二是数据驱动的视觉-语言-动作（VLA）模型，它们利用预训练的VLM和大规模遥操作数据集直接学习机器人控制，虽然避免了手动启发式规则，但倾向于过拟合低级动作模式，未能充分利用执行时的空间先验。本文针对VLA模型训练中空间感知与动作学习目标之间优化不一致、导致空间基础能力在策略学习过程中退化或冲突这一具体痛点，提出了“空间引导训练”的新视角。其核心思路是：通过一个两阶段训练流程，先让VLM获取可迁移的空间先验，再通过空间提示引导动作生成，从而在策略学习中保持空间基础能力，并促进空间与动作目标间的一致优化。\n\n## 方法详解\nST4VLA是一个双系统、端到端的VLA框架，其整体训练流程分为两个阶段：1）空间基础预训练；2）空间引导动作后训练。该设计将空间先验的获取与具体 embodiment 的控制解耦。\n\n![方法框架](https://arxiv.org/html/2602.10109v1/x2.png)\n\n> **图2**：ST4VLA整体框架。包含两个阶段：阶段1（空间基础预训练）在大量多源空间基础数据上训练VLM；阶段2（空间引导动作后训练）VLM规划器通过空间提示生成潜在规划令牌，作为动作专家的条件。\n\n**核心模块与架构**：\n1.  **双系统设计**：系统2（VLM规划器）基于Qwen2.5-VL，作为一个多模态编码器，负责捕获空间和语义先验。系统1（动作专家）采用一个紧凑的扩散变换器（DiT）和DINOv2视觉编码器，负责具体 embodiment 的控制。\n2.  **连接机制**：通过一个轻量级的查询变换器（8.7 MB）连接两者。该变换器以VLM规划器产生的潜在空间基础嵌入为条件，将可变长度的输入令牌映射为一组固定的可学习查询令牌，从而稳定专家的学习和推理。它实现为一个k层交叉注意力模块。\n3.  **空间提示**：在动作后训练阶段，通过在任务指令后附加简单的空间提示（如“Figure out how to execute it, then locate the key object needed”）来显式激活VLM在预训练阶段学到的空间感知能力。提取的特征嵌入为规划器提供了明确的空间线索。\n4.  **梯度衰减**：为了缓解动作专家梯度直接回流可能扭曲VLM多模态知识的问题，在查询变换器中引入了梯度衰减因子（例如0.5），以减弱从动作专家传回VLM的梯度，从而在保持规划器语义推理能力的同时实现有效的联合优化。\n\n**训练流程细节**：\n- **阶段1：空间基础预训练**：目标是在通用视觉-语言理解与机器人特定的空间推理需求之间建立基础对齐。策略性地结合大规模互联网视觉-语言基础语料库（如RefCOCO、LLaVA-OneVision）与针对性的机器人特定数据集（如RoboRefIt、A0、ST4VLA Data）。通过将所有机器人数据重新格式化为与网络规模预训练一致的统一QA结构，使VLM在标准监督微调框架下发展出具有空间感知能力的表征空间。\n- **阶段2：空间引导动作后训练**：重点是在保持和精炼阶段1获得的空间先验的同时，学习具体 embodiment 的控制。除了使用空间基础数据进行共同训练（VLM主干通过图像-提示对的下一个令牌预测进行更新）外，还针对动作数据引入空间提示，以增强语义推理与动作生成之间的对齐。例如，将指令“store all toys into the toy box”扩展为“Identify all relevant toys and their spatial relationships to the container.”\n\n**创新点**：与现有方法相比，ST4VLA的创新在于明确将“在哪里、做什么”与“如何做”分离，并通过**空间提示**这一机制，在动作训练阶段主动引导模型产生更丰富的空间先验来指导动作生成，而非被动地共同优化可能冲突的目标。同时，**梯度衰减**的设计保护了VLM的已有知识。\n\n## 实验与结果\n**实验设置**：\n- **基准测试/数据集**：SimplerEnv仿真基准套件（包含Google Robot和WidowX平台）、LIBERO仿真套件、基于Isaac-Sim构建的大规模模拟拾放基准（200个任务）、真实世界杂乱场景拾放任务（使用Franka Research 3机器人）、真实世界长视野操作任务。\n- **对比的基线方法**：包括RT-1、RT-2-X、OpenVLA、CogACT、SpatialVLA、π₀、GR00T N1.5、Magma等先进的开放VLA系统，以及论文自身构建的Vanilla VLA和Vanilla Co-training VLA。\n\n**关键实验结果**：\n1.  **SimplerEnv基准测试**：如表2和表3所示，ST4VLA在Google Robot视觉匹配（VM）、视觉聚合（VA）和WidowX VM任务上均取得了最佳性能。具体而言，在Google Robot VM上平均成功率从Vanilla VLA的66.1%提升至84.6%，VA从63.5%提升至75.9%；在WidowX上从54.7%提升至73.2%。相较于之前最好的方法，分别获得了5.9%、5.3%和9.8%的增益。\n\n2.  **大规模模拟拾放泛化评估**：如图4所示，在包含分布内、未见物体、新背景和未见指令四个维度的评估中，ST4VLA在200个模拟任务上均取得了最优的成功率，显著优于π₀和GR00T N1.5，展示了强大的视觉和语言泛化能力。\n\n![模拟拾放结果](https://arxiv.org/html/2602.10109v1/x4.png)\n\n> **图4**：在200个模拟指令跟随拾放任务中，不同泛化设置下的成功率。ST4VLA在所有设置下均领先。\n\n3.  **真实世界拾放泛化评估**：如表4所示，在包含分布内、未见物体实例、相似干扰物、新背景、未见物体位姿/朝向以及按属性/空间关系描述的未见指令等多个挑战性维度上，ST4VLA（平均成功率65%）全面优于π₀（31%）和GR00T N1.5（48%），特别是在“相似干扰物”和“未见物体朝向”等困难条件下优势明显。\n\n4.  **真实世界长视野操作评估**：如图5所示，在桌面整理、抽屉组织、制作三明治等需要多步规划的任务中，ST4VLA在分布内、物理干扰和任务重规划三种设置下均优于基线，展示了其利用高级规划器分解复杂任务并由低级控制器稳健执行的能力。\n\n![真实世界拾放结果](https://arxiv.org/html/2602.10109v1/x5.png)\n\n> **图5**：长视野指令跟随操作任务的结果。ST4VLA在分布内、物理干扰和任务重规划三种设置下均优于基线。\n\n5.  **消融与机理分析**：如图3和表1所示，研究对比了三种训练策略：Vanilla VLA（仅动作微调）、Vanilla Co-training VLA（空间与动作数据联合训练）、ST4VLA（空间预训练+空间引导后训练）。\n    - **性能表现**：ST4VLA在机器人操作和多模态理解/空间基础指标上均达到最佳平衡。例如，在保持RefCOCO-g IoU@0.5达71.2%的同时，取得Google Robot VM 84.6%和WidowX 73.2%的成功率。\n    - **优化对齐分析**：通过投影空间相似度（PSS）量化梯度对齐程度。如图3(c)，Vanilla Co-training的PSS仅为0.25，表明优化存在冲突；而ST4VLA的PSS提升至0.42，说明空间引导训练显著改善了两个目标优化动态的一致性，这与更好的空间感知保持和更快的操作任务收敛相关。\n\n![消融研究](https://arxiv.org/html/2602.10109v1/x3.png)\n\n> **图3**：消融研究。(a) 感知性能（RefCOCO-g上的IoU@0.5）；(b) 操作性能（WidowX上的平均成功率）；(c) 空间基础目标和动作策略目标的梯度相似度。ST4VLA在保持感知的同时实现了最佳的操作性能与优化对齐。\n\n**消融实验总结**：表1的消融结果表明，“空间引导”（+Spatially Guided）和“空间预训练”（+Spatially Pretrained）两个组件均对性能有正向贡献，两者结合（即完整的ST4VLA）效果最佳，验证了空间先验的引入及其与动作学习的引导式对齐的有效性。\n\n## 总结与启发\n**核心贡献**：\n1.  揭示了直接微调VLM为VLA模型会导致空间先验崩溃，而简单地与空间数据共同训练则会引入梯度冲突的问题，并提出通过简单的空间提示可以有效缓解这些问题。\n2.  提出了ST4VLA，一个空间引导训练框架，通过两阶段训练（空间基础预训练 + 空间引导动作后训练）显式地将动作优化与空间基础目标对齐，从而在策略学习中保持感知能力并实现稳健控制。\n3.  在SimplerEnv等大规模模拟和真实机器人实验基准上取得了领先性能，并证明了对未见物体、新指令和分布外环境更强的泛化能力与鲁棒性。\n\n**局限性**：论文自身未明确列出局限性，但从方法描述中可推断，其两阶段训练流程需要精心设计的数据集（混合网络数据与机器人数据），且引入了额外的查询变换器和梯度衰减超参数，可能增加训练复杂度和调优成本。\n\n**后续研究启示**：\n1.  **空间引导机制**：证明了在VLA训练中主动引导空间推理的有效性，为未来设计更精细的引导信号（如3D位置、 affordance 地图）提供了思路。\n2.  **优化一致性**：提出的梯度相似度分析工具（PSS）可用于诊断和改善多任务学习中的优化冲突问题。\n3.  **双系统架构**：将慢速、可靠的规划（System 2）与快速、具体的控制（System 1）分离，为构建兼具强推理能力和高效执行的具身智能体提供了可借鉴的框架。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10109v1/x1.png",
        "https://arxiv.org/html/2602.10109v1/x2.png",
        "https://arxiv.org/html/2602.10109v1/x3.png",
        "https://arxiv.org/html/2602.10109v1/x4.png",
        "https://arxiv.org/html/2602.10109v1/x5.png",
        "https://arxiv.org/html/2602.10109v1/x6.png",
        "https://arxiv.org/html/2602.10109v1/x7.png",
        "https://arxiv.org/html/2602.10109v1/x8.png",
        "https://arxiv.org/html/2602.10109v1/x9.png",
        "https://arxiv.org/html/2602.10109v1/x10.png",
        "https://arxiv.org/html/2602.10109v1/x11.png",
        "https://arxiv.org/html/2602.10109v1/x12.png",
        "https://arxiv.org/html/2602.10109v1/x13.png",
        "https://arxiv.org/html/2602.10109v1/x14.png",
        "https://arxiv.org/html/2602.10109v1/x15.png",
        "https://arxiv.org/html/2602.10109v1/x16.png",
        "https://arxiv.org/html/2602.10109v1/x17.png",
        "https://arxiv.org/html/2602.10109v1/x18.png",
        "https://arxiv.org/html/2602.10109v1/x19.png",
        "https://arxiv.org/html/2602.10109v1/x20.png",
        "https://arxiv.org/html/2602.10109v1/x21.png",
        "https://arxiv.org/html/2602.10109v1/x22.png",
        "https://arxiv.org/html/2602.10109v1/x23.png",
        "https://arxiv.org/html/2602.10109v1/x24.png",
        "https://arxiv.org/html/2602.10109v1/x25.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10101",
      "title": "Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction",
      "url": "http://arxiv.org/abs/2602.10101",
      "arxivId": "2602.10101",
      "date": "2026-02-10",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中3D感知精度不足的问题，提出Robo3R模型。该方法直接从RGB图像与机器人状态进行前馈式3D重建，核心技术包括：联合推断尺度不变的局部几何与相对相机位姿，通过学习的全局相似变换将其统一到机器人坐标系；采用掩码点云头生成精细点云，以及基于关键点的PnP公式优化相机外参与全局对齐。模型在包含400万帧的合成数据集上训练，实验表明其性能持续优于现有先进重建方法与深度传感器，并在多项下游操作任务中带来性能提升。",
      "detailedSummary": "## 研究背景与动机\n机器人操作在非结构化环境中严重依赖对物理世界的鲁棒空间感知。当前，获取3D数据主要依赖深度相机（如RealSense D455， Azure Kinect），但其产生的深度图质量有限，易受噪声、透明或反射物体以及不良光照条件的影响。近年来，前馈式3D重建模型（如DUSt3R， VGGT， π³）发展迅速，但它们主要关注场景级重建，在机器人操作所需的细粒度几何精度和可靠度量尺度方面存在不足，限制了其在真实世界操作中的应用。\n\n本文针对机器人操作中高质量、度量尺度3D几何感知的痛点，提出了一种新的视角：将RGB图像与机器人状态信息融合，通过一个专门设计的神经网络，实时、前馈式地预测出操作就绪的3D场景表示。本文核心思路是：联合推断尺度不变的局部几何和相对相机位姿，并通过一个学习的全局相似变换，将它们统一到规范的机器人坐标系中，从而获得准确、度量尺度的三维几何。\n\n## 方法详解\nRobo3R的整体框架是一个端到端的前馈神经网络，输入为单目或双目RGB图像及机器人关节状态，输出为度量尺度下的规范机器人坐标系中的三维点云，同时包含深度图、归一化图像坐标、相对/绝对相机位姿等一系列中间表示。\n\n![方法总览](https://arxiv.org/html/2602.10101v1/x2.png)\n> **图2**：Robo3R方法总览。RGB图像和机器人状态被编码并融合。Transformer骨干网络通过交替的全局和帧内注意力处理特征。掩码点云头解码尺度不变的局部几何，相对位姿头输出用于多视角配准的相对位姿。相似变换（S.T.）令牌读出全局相似变换，将点云映射到规范机器人坐标系中的度量尺度3D几何。\n\n**核心模块与技术细节**：\n1.  **编码器**：使用DINOv2 ViT-L编码图像为补丁特征。机器人关节状态通过一个多层感知机（MLP）投影为状态特征。图像特征与状态特征通过逐元素相加进行融合，并附加可学习的相似变换（S.T.）令牌。\n2.  **Transformer骨干网络**：采用基于交替注意力机制的Transformer，堆叠18个交替的全局注意力和帧内注意力块，以实现帧内和跨帧的高效信息传播。\n3.  **掩码点云头**：为解决密集预测中的过平滑问题，该头将点云预测解耦为深度、归一化图像坐标和掩码预测三部分。\n    ![掩码点云头](https://arxiv.org/html/2602.10101v1/x3.png)\n    > **图3**：掩码点云头结构。通过解耦预测、反投影、掩码过滤和组合，获得具有清晰边缘和细粒度几何细节的点云。\n4.  **相对位姿头**：处理骨干网络输出，预测相对相机平移（3维向量）和旋转（9维表示，经SVD正交化为3x3矩阵），用于将不同视角的局部点云配准。\n5.  **相似变换头**：从S.T.令牌中解码出全局相似变换，包括刚性变换 **T** 和尺度因子 **s** （**S = s · T**），用于将配准后的点云变换到度量尺度的规范机器人坐标系。\n6.  **外部参数估计模块与关键点头**：为进一步提高精度，该模块通过预测机器人预定义关键点的2D像素坐标（使用热力图和可微Soft-Argmax），并求解PnP问题来估计相机外参，用于细化全局相似变换。\n    ![外部参数估计模块](https://arxiv.org/html/2602.10101v1/x4.png)\n    > **图4**：外部参数估计模块。通过关键点头提取机器人关键点像素坐标，并求解PnP问题来精确估计相机外参，从而细化全局相似变换。\n\n**创新点**：\n*   **显式融入机器人先验**：将机器人状态作为输入，并利用机器人关键点进行PnP优化，显著提升了重建的度量精度和对机器人体态的感知。\n*   **掩码点云头**：通过解耦预测和掩码机制，有效缓解了过平滑问题，能生成边缘清晰、细节丰富的点云。\n*   **统一的度量尺度重建流程**：从尺度不变局部表示，到多视角配准，再到通过学习的相似变换映射到规范机器人坐标系，系统性地解决了前馈重建中的尺度模糊和度量一致性问题。\n\n## 实验与结果\n**实验设置**：\n*   **数据集**：使用自建的大规模合成数据集**Robo3R-4M**（400万帧）进行训练。评估使用一个独立的、光真实感的测试集（2000个场景，8万帧）。\n*   **基准测试与基线方法**：在3D重建质量上，与领先的前馈重建模型**VGGT**、**π³**、**MapAnything (MA)** 和 **DepthAnything3 (DA3)** 进行对比。在下游任务中，与**深度相机（RealSense D455）** 以及基于RGB的模仿学习方法（如**Maniflow**）进行对比。\n*   **下游任务平台**：在真实世界的单臂Franka Research 3和双臂UR5e机器人平台上，评估模仿学习、仿真到现实迁移、抓取合成和免碰撞运动规划四项应用。\n\n**关键实验结果**：\n1.  **3D重建质量（定量）**：\n    *   **点云估计**：在单目和双目设置下，Robo3R在点误差、法向误差和尺度误差上均显著优于所有基线。例如，单目时点误差为0.006，比次优方法π³（0.061）低一个数量级；尺度误差仅为0.007，而其他方法均高于0.46，表明Robo3R能有效恢复度量几何。\n    *   **相对位姿估计**：Robo3R的相对平移误差（RTE）为0.014，相对旋转误差（RRE）为0.013，分别比最佳基线π³（0.116和0.073）低约8倍和5倍。其相对平移精度（RTA@0.03）高达0.951，证明了位姿预测的可靠性。\n2.  **3D重建质量（定性）与鲁棒性**：\n    ![定性比较](https://arxiv.org/html/2602.10101v1/x6.png)\n    > **图6**：在真实场景中的定性比较。Robo3R能重建仅1.5毫米宽的微小物体（第1行），成功处理镜子和透明杯等令深度传感器失效的物体（第2行），并在包含灵巧手的杂乱双手机器人场景中产生准确干净的点云（第3行）。\n3.  **下游任务性能**：\n    *   **模仿学习**：在四个真实世界任务（扫豆子、插螺丝、做早餐、双手机器人倒水）中，使用Robo3R重建的3D点云作为输入的策略，其成功率显著高于使用深度相机或RGB图像的策略。例如，在“插螺丝”任务中，Robo3R的成功率为90%，而深度相机和RGB策略分别为60%和30%。\n    ![模仿学习结果](https://arxiv.org/html/2602.10101v1/x7.png)\n    > **图7**：真实世界模仿学习的成功率对比。Robo3R在四个任务上均取得最高成功率。\n    *   **仿真到现实迁移**：使用Robo3R点云作为观测的策略，在真实世界的“推方块”任务中取得了83.3%的成功率，远高于使用深度相机（50%）或RGB（16.7%）的策略。\n    *   **抓取合成与运动规划**：基于Robo3R重建的点云进行抓取检测和运动规划，在杂乱场景中表现出更高的成功率和鲁棒性。\n    ![下游任务可视化](https://arxiv.org/html/2602.10101v1/x8.png)\n    > **图8**：下游任务可视化。展示了基于Robo3R点云的抓取位姿合成、免碰撞运动规划以及仿真到现实策略执行的示例。\n4.  **消融实验**：\n    论文通过消融实验验证了各核心组件的贡献。移除**机器人状态输入**会导致所有指标显著下降；移除**掩码点云头**会导致点误差和法向误差增大；移除**关键点PnP细化**会降低位姿估计和尺度精度；而使用**交替注意力机制**相比普通全局注意力能带来性能提升。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**Robo3R-4M**，一个为机器人操作感知与重建定制的大规模、高质量合成数据集及数据生成流程。\n2.  提出了**Robo3R**，一个专为机器人操作设计的前馈3D重建模型，能够实时输出高保真、度量尺度、具有规范坐标系的三维几何，在精度和鲁棒性上超越了现有重建模型和深度传感器。\n3.  通过广泛的实验验证了Robo3R作为深度相机替代方案的优越性，并证明其提升了下游多种机器人操作任务的性能。\n\n**局限性**：\n论文提到，Robo3R的性能依赖于大规模高质量合成数据，其在极端真实世界条件下的泛化能力仍需进一步探索。此外，模型在保持高精度的同时实现更高的实时推理速度（目前为10Hz）也是一个持续的挑战。\n\n**启示**：\n1.  **高质量数据的关键作用**：本研究凸显了针对特定领域（如机器人操作）精心构建大规模、多样化合成数据集对于训练高性能感知模型的重要性。\n2.  **利用领域先验**：将机器人状态等先验信息显式融入感知模型，是解决通用重建模型中特定精度和尺度问题的有效途径。\n3.  **感知与操作的协同设计**：面向最终任务（如操作）来设计感知模型（如输出规范坐标系、度量尺度几何），能更直接地提升整个机器人系统的性能，这代表了一个值得深入的研究方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10101v1/x1.png",
        "https://arxiv.org/html/2602.10101v1/x2.png",
        "https://arxiv.org/html/2602.10101v1/x3.png",
        "https://arxiv.org/html/2602.10101v1/x4.png",
        "https://arxiv.org/html/2602.10101v1/x5.png",
        "https://arxiv.org/html/2602.10101v1/x6.png",
        "https://arxiv.org/html/2602.10101v1/x7.png",
        "https://arxiv.org/html/2602.10101v1/x8.png",
        "https://arxiv.org/html/2602.10101v1/x9.png",
        "https://arxiv.org/html/2602.10101v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09888",
      "title": "TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback",
      "url": "http://arxiv.org/abs/2602.09888",
      "arxivId": "2602.09888",
      "date": "2026-02-10",
      "authors": "Weiming Zhi Team",
      "category": "Manipulation",
      "summary": "本文针对移动操作机器人全身遥操作中，操作者需同时协调底盘与双臂、并兼顾避障与接触的难题，提出TriPilot-FF系统。其核心创新在于引入脚部操作的踏板，通过低成本激光雷达生成基于障碍物距离的触觉阻力，引导操作者无碰撞移动；同时结合双臂主从遥操作与力反馈，提升接触感知与操作可达性。实验表明，系统能有效辅助操作者完成长时间、需精确底盘协调的任务，并将反馈信号融入ACT策略后进一步提升了性能。",
      "detailedSummary": "## 研究背景与动机\n当前，移动操作机器人的全身体遥操作面临核心挑战：操作者必须同时协调移动底座和两只机械臂，并实时推理障碍物和接触力。现有接口（如VR控制器、操纵杆）主要将控制带宽分配给双手，而脚部这一可用于连续底座控制的通道未被充分探索。这导致操作者缺乏一种能减少碰撞和协调负担的、信息丰富的连续底座控制方式。此外，遥操作不仅是自主性的后备方案，更是机器人学习获取数据和监督信号的主要机制。因此，设计能降低操作负荷、预防常见故障（如底座碰撞）并暴露信息反馈通道的接口，对于获取更干净的演示数据和训练信号至关重要。\n\n本文针对上述痛点，提出将全身体遥操作视为一种交互式设置，而非简单的用户输入到机器人动作的映射。核心思路是：通过引入带有激光雷达驱动力反馈的脚控踏板来回收脚部作为连续底座控制通道，并结合上半身双手“领导者-跟随者”遥操作，利用多模态（触觉和视觉）引导信号实时渲染任务相关约束，从而降低操作者的认知负荷，实现人机“共同驾驶”。\n\n## 方法详解\nTriPilot-FF系统的核心设计原则是将全身体控制分配到互补的人体模态上：通过运动学解耦的脚控踏板命令连续底座运动，同时通过同构的“领导者-跟随者”双臂遥操作进行双手操作。操作者保留完全控制权，系统则渲染任务相关的引导约束。\n\n![系统总览与踏板设计](https://arxiv.org/html/2602.09888v1/x2.png)\n\n> **图2**：TriPilot-FF系统概览与踏板设计。左侧展示了低成本、3自由度的踏板设备设计。右侧展示了系统整体框架：操作者通过脚控踏板指挥移动底座，通过一对“领导者”机械臂指挥远程的“跟随者”机械臂。系统基于接触、附近障碍物以及为提升可达性而调整底座的需求，在操作侧的领导者手臂和踏板上提供力反馈。\n\n**系统硬件**：遥操作站提供三个同步模态：1) 来自两个手腕相机、一个头部相机以及低成本360度激光雷达的视觉和深度感知输入；2) 通过同构映射的双手“领导者-跟随者”机械臂提供手臂和夹爪输入；3) 通过3自由度踏板提供脚部输入，以控制移动底座。远程机器人执行一个统一的包含底座扭转和双臂关节目标的全身体命令，同时将本体感知和力相关信号流回传给操作者以进行力反射和引导。\n\n**核心软件模块**：\n1.  **重力补偿的力反射**：为了在反映接触力的同时限制操作者疲劳，“领导者”手臂运行在重力补偿的柔顺模式下。系统从电机电流估计“跟随者”关节扭矩 τ_fol，减去模型重力 ĝ(q_fol) 以隔离交互扭矩。然后，将“领导者”的前馈扭矩设置为 τ_ff_lead = - (τ_fol - ĝ(q_fol)) * s_i + ĝ(q_lead)，其中 s_i ∈ [0,1] 设置反射强度。这结合了领导者重力补偿与跟随者交互扭矩的缩放反射。\n2.  **避障踏板辅助**：为避免注意力集中在双手操作时发生的近距离碰撞，系统在踏板上渲染阻力，使不安全的底座运动在物理上更难发出。具体而言，系统根据操作者通过踏板位移指示的瞬时预期平移方向 d̂，利用激光雷达估计沿该方向到最近障碍物的距离 r(d̂)。定义一个平滑的排斥势函数 φ(r)，当距离 r 接近机器人表面半径 r0 时势能迅速增加。踏板阻力大小 f_Rep(d̂) = k_φ * |∂φ/∂r(r(d̂))|，其中 k_φ > 0 缩放力输出。该阻力被施加在踏板末端执行器上，方向与 d̂ 相反，从而在不使用显式避障控制器的情况下，引导操作者发出避免碰撞的驾驶命令。\n\n![排斥势函数](https://arxiv.org/html/2602.09888v1/Phi_0.png)\n\n> **图3**：排斥势函数 φ 在障碍物接近机器人时产生越来越大的排斥力。系统利用激光雷达信息在踏板上产生阻力。\n\n3.  **可达性踏板辅助**：为解决双臂操作中目标经常位于瞬时工作空间边缘或之外的问题，系统计算一个实时提示，当手臂接近伸展或低可操作度配置时，指向能提高灵巧性的方向。系统定义了一个基于关节空间可操作度 w(q) = sqrt(det(J(q)J(q)^T)) 的笛卡尔可操作度场 m(x)，它表示末端执行器到达位置 x 时所能达到的最佳可操作度。通过神经网络 m̂(x) 离线学习该场的可微近似，从而可以高效计算梯度 ∇_x m̂(x)。引导方向 v_guide 是当前末端执行器方向与可操作度梯度方向的加权组合，投影到水平面后产生踏板反馈力 f_guide，仅在手臂伸展且预测可操作度低于阈值时激活，以保持操作者权限。\n4.  **可操作度视觉引导**：系统还将可操作度信息以视觉方式提供给操作者。通过相机参数获取点云，并评估可操作度场，得到每个像素在当前底座位置下的可操作度值。通过阈值过滤后，可操作度可以可选地叠加在视频流上，使操作者能高效识别物体是否可以被轻松触及。\n\n![可操作度视觉叠加](https://arxiv.org/html/2602.09888v1/manip_1.png)\n\n> **图4**：可操作度区域高亮显示在两个手腕相机输入上。这告知用户在不移动底座的情况下物体是否可达。\n\n**创新点**：与现有方法相比，TriPilot-FF的创新具体体现在：1) 将脚部作为连续底座控制通道，并通过低成本激光雷达直接生成方向性的触觉阻力来辅助避障，无需额外的避障控制器；2) 提供多模态、全身体引导，包括手臂侧的力反射和基于学习到的可操作度场的实时踏板及视觉引导，以协同解决碰撞、接触感知和可达性问题；3) 将遥操作产生的力/扭矩信号作为额外的监督信息，用于增强模仿学习策略的性能。\n\n## 实验与结果\n实验在一个定制的双手机动操作平台上进行了广泛的真实世界评估。评估任务旨在回答五个核心问题（Q1-Q5）。\n\n**对比基线**：实验主要与TriPilot-FF系统的消融版本进行对比，例如“无踏板反馈”、“无任何反馈”等。\n\n**关键实验结果**：\n1.  **狭窄空间协调（Q1）**：任务包括“盲载”（搬运遮挡视线的物体）和“狭窄通道运输”。如表I所示，完整的TriPilot-FF系统在两项任务中均达到100%成功率、零碰撞，且完成时间最短。在盲载任务中，移除踏板反馈导致成功率降至55%，碰撞次数增加，时间延长，这验证了在视觉受限情况下激光雷达踏板反馈对于避障的关键作用。\n\n![狭窄空间任务场景](https://arxiv.org/html/2602.09888v1/narrow_passage.png)\n\n> **图5**：左图：双手机器人用双臂垂直重新定位横杆，然后穿过狭窄的门，再将横杆水平放回。右图：机器人搬运一个遮挡视线的箱子，并导航避开板条箱障碍物。系统提供力反馈以帮助引导操作者何时以及如何重新定位底座。\n\n![精细双手操作](https://arxiv.org/html/2602.09888v1/x3.png)\n\n> **图6**：TriPilot-FF支持精细的双手操作。(b) 机器人可以稳固地用两个夹爪抓握刚性物体并协调摆动，这得益于力反馈。\n\n2.  **可达性引导（Q2）**：在“引导抓取”任务中，操作者需调整底座以使左夹爪能够到目标物体。表II显示，启用可操作度引导（无论是踏板力反馈还是视觉叠加）都能显著减少完成任务所需的“微调”底座移动次数（从无引导时的约9次降至约2次）和总完成时间，同时保持高成功率。这证明了可达性引导有效降低了协调负担。\n\n3.  **接触任务（Q3）**：在涉及精确力控制的“滚轮接触”任务中，启用手臂力反射可将任务成功率从65%提升至100%，并将平均接触力误差降低约50%（从~1.5N降至~0.8N）。这表明力反射显著提高了接触调节的效率和精度。\n\n4.  **长时程任务（Q4）**：系统在包含多个子任务（如取放、开门、折叠衣物）的长序列中展示了可靠的性能，成功完成了90%的复杂步骤，验证了其在实际工作流程中的可用性和鲁棒性。\n\n5.  **策略学习（Q5）**：将遥操作收集的关节扭矩数据作为额外观测，增强Action Chunking with Transformers (ACT)策略。在“手套转移”任务中，使用扭矩增强观测训练的策略比标准ACT策略的成功率提高了15%（从80%到95%），证明了遥操作触觉信号能为模仿学习提供有用的监督信息。\n\n**消融实验总结**：实验系统地评估了各个反馈组件的贡献。踏板避障反馈对于在视觉受限或狭窄空间避免碰撞至关重要；手臂力反射对于需要精确接触力的任务效率和质量提升显著；可达性引导（无论是触觉还是视觉形式）能有效减少因工作空间限制而产生的、耗时的底座微调操作。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了TriPilot-FF，一个开源的、将脚控底座与双手“领导者-跟随者”遥操作耦合的全身体遥操作系统，通过多模态力反馈和视觉引导实现人机协同。\n2.  设计了基于低成本激光雷达的踏板力反馈机制，能够根据指令方向的障碍物接近度产生方向性阻力，在不使用显式避障控制器的情况下引导操作者避免碰撞。\n3.  开发了基于神经网络的可操作度场实时计算与引导方法（包括触觉和视觉），有效缓解了因可达性不足导致的协调失败，并证明了将遥操作力反馈信号纳入模仿学习策略可以提升下游任务性能。\n\n**局限性**：论文提到，系统依赖于精确的传感器外参校准以确保引导信号在正确坐标系中生成。此外，虽然踏板力反馈对静态和缓移动障碍物有效，但在处理高动态环境方面可能存在限制。\n\n**后续启示**：本研究展示了将人类多模态控制能力（手和脚）与机器实时感知和约束信息相结合，通过直观的物理反馈进行交互的可行性与优势。这种“共同驾驶”理念可推广至更复杂的机器人形态（如人形机器人）。同时，工作凸显了高质量、富含物理交互信息的遥操作数据对于机器人学习的重要性，为通过改进人机接口来提升数据质量和学习算法性能指明了方向。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09888v1/x1.png",
        "https://arxiv.org/html/2602.09888v1/x2.png",
        "https://arxiv.org/html/2602.09888v1/Phi_0.png",
        "https://arxiv.org/html/2602.09888v1/manip_1.png",
        "https://arxiv.org/html/2602.09888v1/narrow_passage.png",
        "https://arxiv.org/html/2602.09888v1/x3.png",
        "https://arxiv.org/html/2602.09888v1/transfer_glove_circ.png",
        "https://arxiv.org/html/2602.09888v1/task_bar.jpeg",
        "https://arxiv.org/html/2602.09888v1/Roller_contact.png",
        "https://arxiv.org/html/2602.09888v1/Kitchen_thin.png",
        "https://arxiv.org/html/2602.09888v1/Tool_room.png",
        "https://arxiv.org/html/2602.09888v1/basket_place.png",
        "https://arxiv.org/html/2602.09888v1/auto1.png",
        "https://arxiv.org/html/2602.09888v1/Transfer.png",
        "https://arxiv.org/html/2602.09888v1/folding_motion.png",
        "https://arxiv.org/html/2602.09888v1/TargetDetection_left.png",
        "https://arxiv.org/html/2602.09888v1/TargetDetection_mid.png",
        "https://arxiv.org/html/2602.09888v1/TargetDetection_right.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09940",
      "title": "Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.09940",
      "arxivId": "2602.09940",
      "date": "2026-02-10",
      "authors": "Laxmidhar Behera Team",
      "category": "Manipulation",
      "summary": "本文提出Instruct2Act框架，旨在解决机器人在资源受限环境下难以理解和执行自由形式人类指令的问题。核心方法包括：1）基于BiLSTM与多头注意力自编码器的指令解析模块，将自然语言指令分解为原子动作序列；2）结合动态自适应轨迹径向网络（DATRN）与YOLOv8视觉分析器的机器人动作网络，生成精确控制轨迹。实验表明，该系统在自定义数据集上子动作预测准确率达91.5%，在四种真实机器人任务中整体成功率为90%，单次子动作推理时间小于3.8秒。",
      "detailedSummary": "## 研究背景与动机\n当前，基于大语言模型（LLMs）和视觉语言模型（VLMs）的视觉语言动作（VLA）管道，能够将自由形式的指令直接映射到感知和轨迹生成，是机器人人机交互领域的主流方法。然而，在仅使用眼在手（手腕）单摄像头配置的实际部署中，这些方法面临关键局限性：手臂和夹具频繁遮挡关键物体、视角快速变化、全局上下文有限导致位姿和帧模糊，从而降低操作可靠性。此外，即便是轻量级VLA变体也常依赖广角多摄像头设置或庞大的多模态编码器，引入了延迟并增加了实际环境中的维护成本。\n\n本文针对在资源受限、仅配备单眼在手摄像头的实际环境中，实现可靠、实时操作的痛点，提出了一个新视角：不追求端到端的、基于大型模型的复杂理解，而是采用一个轻量级、完全在设备上运行的两阶段确定性管道。其核心思路是：首先，将自然语言指令精细解析为一个有序的原子动作序列（如`reach`, `grasp`, `move`）；然后，利用一个结合了动态自适应轨迹径向网络（DATRN）和视觉分析器的机器人动作网络（RAN），为每个子动作生成并执行精确的控制轨迹。\n\n## 方法详解\n本文提出的方法是一个完整的两阶段管道，整体框架如图1所示。第一阶段（Instruct2Act模块）的输入是自然语言指令（或经语音转文本后的文本），输出是一个有序的原子子动作序列。第二阶段（RAN模块）的输入是该子动作序列，输出是控制机器人执行这些动作的具体轨迹命令，最终完成整个任务。\n\n![方法框架](https://arxiv.org/html/2602.09940v1/front_page_jp.png)\n> **图1**：Instruct2Act与RAN的整体框架。左侧为指令解析为动作序列的Instruct2Act模块，右侧为执行动作序列的RAN模块，包含环境分析、轨迹生成与机器人执行。\n\n**核心模块一：Instruct2Act（指令到动作解析模块）**\n该模块的目标是将任务描述翻译成子动作序列。其技术细节如下：\n1.  **任务嵌入提取**：使用`bert-large-uncased`模型将指令文本转换为1024维的语义嵌入向量。\n2.  **序列对齐**：将上述嵌入向量在时间维度上复制/对齐到固定长度`L`，形成序列`{R_i,t}`，作为后续序列模型的输入。\n3.  **学习架构**：采用双向LSTM（BiLSTM）结合多头自注意力（MHA）和自编码器的轻量级网络。\n    *   **BiLSTM层**：处理对齐后的嵌入序列，捕获前后文信息，输出512维（每个方向256维）的隐藏状态序列。\n    *   **MHA层**：包含8个注意力头（`dk=256`），对BiLSTM输出进行自注意力计算，使模型能关注序列中不同位置的信息，输出经线性映射和另一个BiLSTM（作为前馈块）处理，得到256维特征。\n    *   **自编码器层**：作为一个正则化组件，将前述256维特征通过一个BiLSTM编码器压缩为128维的瓶颈特征，再通过一个BiLSTM解码器重建回256维。训练时，计算瓶颈特征与重建特征之间的均方误差（MSE）作为重建损失。\n    *   **输出层**：一个时间分布的全连接层，对解码器每个时间步的输出应用Softmax，预测该步对应的子动作类别概率分布。\n4.  **训练与损失**：模型端到端训练，损失函数是子动作分类的交叉熵损失与自编码器重建损失的加权和（`ℒ = 交叉熵 + λ * MSE`），使用Adam优化器。\n\n**核心模块二：机器人动作网络（RAN）**\nRAN负责执行Instruct2Act预测出的每个子动作，其工作流程如图3所示，包含三个组件：\n\n![RAN工作流程](https://arxiv.org/html/2602.09940v1/RAN.jpeg)\n> **图3**：RAN的工作流程。接收子动作序列后，环境分析器检测物体；若物体存在，则进行坐标转换并交由DATRN生成轨迹，最后由机器人执行。\n\n1.  **最优轨迹学习（DATRN）**：这是RAN的核心创新。针对动态运动基元（DMP）方法需要迭代超参数搜索、相位/增益调度和长拟合时间等局限，提出了动态自适应轨迹径向网络（DATRN）。它使用径向基函数（RBF）网络来建模轨迹动力学。RBF中心通过K-Means聚类在示范轨迹上优化放置，宽度根据中心间平均距离自动设置。网络权重通过正则化最小二乘（岭回归）一次性求解（公式9），实现了快速、单次学习。DATRN的动力学方程（公式7）包含一个非线性吸引子项`tanh(g - y(t_k))`，能稳定驱动轨迹收敛至目标`g`。\n2.  **环境分析器**：使用定制训练的YOLOv8-n模型实时检测工作空间中指令提及的物体。如果检测到目标物体，则返回其位置；否则任务安全停止。对于指令中缺失的对象或目标位置，系统会交互式地向用户查询。\n3.  **机器人执行模型**：负责将感知信息转化为具体动作。\n    *   **坐标转换**：结合YOLO检测的物体2D图像中心`(u, v)`和深度传感器提供的深度值`d`，通过相机内参反投影到3D相机坐标系，再通过ROS TF2提供的变换矩阵转换到机器人基坐标系，得到目标位置`(x, y, z)`。\n    *   **轨迹规划与执行**：将目标坐标`(x, y, z)`输入DATRN，生成平滑的、类人的轨迹。机器人执行该轨迹。对于需要高精度到达的任务（如抓取、倾倒），使用比例-微分（PD）控制器进行误差最小化。\n    *   **处理遮挡**：对于抓取后摄像头被遮挡的情况（如抓取-放置、抓取-倾倒），系统在抓取前预先识别并存储后续放置/倾倒的目标位置。\n\n**创新点**：与现有VLA方法相比，本文的创新具体体现在：1) **轻量级与完全在设备运行**：整个管道设计紧凑，无需云端服务，适合资源受限环境。2) **确定性动作解析**：将模糊的指令解析为确定的原子动作序列，提高了操作的可靠性和可解释性。3) **高效的轨迹生成**：用DATRN替代DMP，避免了繁琐的参数调整，实现了快速、自适应的轨迹生成。\n\n## 实验与结果\n**实验设置**：\n*   **硬件**：Kinova 7自由度机械臂，Intel RealSense D410深度相机（眼在手配置），Nvidia RTX 4060 GPU用于训练。\n*   **软件**：ROS， Kinova库， Huggingface Transformers， TensorFlow， PyTorch。\n*   **数据集**：自定义专有数据集，包含2850条自然语言指令及其对应的结构化子动作序列，涵盖拾放、倾倒、清洁、给予等任务。\n\n**对比的Baseline方法**：在指令解析任务上，对比了标准LSTM、BiLSTM以及BiLSTM+MHA模型。\n\n**关键实验结果**：\n1.  **指令解析精度**：如表IV(c)所示，Instruct2Act在测试集上达到了91.5%的子动作预测准确率，显著高于LSTM（79.21%）、BiLSTM（81.92%）和BiLSTM+MHA（85.56%）。其加权F1分数和召回率也分别为91%和88%。\n\n![混淆矩阵](https://arxiv.org/html/2602.09940v1/x3.png)\n> **图5**：Instruct2Act在测试集上的混淆矩阵。显示了模型对不同子动作类别的预测情况，对角线上的高亮度表明整体预测准确性很高。\n\n2.  **训练过程**：如图4所示，训练损失和验证损失平滑下降并收敛（最终分别为0.0091和0.0088），重建损失也同步下降，表明模型学习有效且未过拟合。\n\n![训练损失曲线](https://arxiv.org/html/2602.09940v1/x2.png)\n> **图4**：模型的训练与验证损失（分类损失）曲线，以及自编码器分支的重建损失曲线。两条主损失曲线平行收敛，显示模型泛化良好。\n\n3.  **消融实验**：如图6所示，移除非核心组件会降低性能。移除自编码器导致准确率下降1.8%，移除MHA下降4.3%，同时移除两者则下降9.6%，证明了MHA和自编码器各自对提升模型性能的贡献。\n\n![消融实验](https://arxiv.org/html/2602.09940v1/x4.png)\n> **图6**：Instruct2Act架构的消融研究结果。完整模型性能最佳，移除多头注意力（MHA）或自编码器（AE）组件均会导致准确率下降，证明两者均为有效设计。\n\n4.  **机器人任务执行成功率**：在真实机器人上评估了四个任务（见表IV(a)）。整体任务成功率达到90%，其中“桌面清洁”成功率最高（95%），“拾取并给予”最低（85%）。子动作推理时间小于3.8秒，端到端任务执行时间在30-70秒之间，取决于任务复杂度。\n\n5.  **失败分析**：如表IV(b)所示，在总共8次失败中，37.5%归因于机器人动作网络（如轨迹执行误差），25%归因于子动作序列预测错误，另外37.5%为系统故障（如通信问题）。\n\n6.  **定性结果**：图7至图11展示了系统成功执行“拾放”、“拾取并倾倒”、“桌面清洁”、“拾取并给予”以及一个组合任务（“拾取瓶子，倒入杯中，然后清洁桌子”）的序列图像，直观证明了方法的有效性。\n\n![定性结果1](https://arxiv.org/html/2602.09940v1/x5.png)\n> **图7**：“拾取与放置”任务的执行序列。\n\n![定性结果2](https://arxiv.org/html/2602.09940v1/x6.png)\n> **图8**：“拾取与倾倒”任务的执行序列。\n\n![定性结果3](https://arxiv.org/html/2602.09940v1/x7.png)\n> **图9**：“桌面清洁”任务的执行序列。\n\n![定性结果4](https://arxiv.org/html/2602.09940v1/x8.png)\n> **图10**：“拾取并给予”任务的执行序列。\n\n![定性结果5](https://arxiv.org/html/2602.09940v1/x9.png)\n> **图11**：组合任务（“拾取瓶子，倒入杯中，然后清洁桌子”）的执行序列。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种集成了学习模块（指令解析）与执行模块（轨迹生成与控制）的完整方法论，弥合了自然语言解析与精确机器人控制之间的鸿沟。\n2.  提出了一个轻量级的Instruct2Act框架，它通过结合BiLSTM、多头注意力和自编码器，能够以较低计算开销从人类指令中准确提取细粒度的子动作序列。\n3.  引入了机器人动作网络（RAN），其中创新的动态自适应轨迹径向网络（DATRN）与基于视觉的环境分析器（YOLOv8）结合，能在机器人工作空间内生成平滑、精确的操纵轨迹。\n\n**局限性**：论文自身提到，该方法目前依赖于一个自定义的专有数据集进行训练和评估。此外，实验是在受控的实验室和初步的现实医疗环境中进行的，尚未在高度动态或不可预测的非结构化环境中进行大规模测试。\n\n**对后续研究的启示**：\n1.  **轻量级与确定性的价值**：在资源受限的实际部署场景中，一个轻量级、完全在设备上运行、具有确定性的管道可能比依赖大型模型和云端的复杂系统更具实用性和鲁棒性。\n2.  **模块化设计**：将复杂的“指令到动作”问题分解为“解析”和“执行”两个相对独立的阶段，并分别优化，是一种有效的工程化思路，提高了系统的可解释性和可调试性。\n3.  **DATRN的潜力**：DATRN作为一种高效、免调参的轨迹生成方法，在需要从演示中快速学习并适应新目标的机器人技能学习领域具有进一步的应用和拓展空间。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09940v1/front_page_jp.png",
        "https://arxiv.org/html/2602.09940v1/x1.png",
        "https://arxiv.org/html/2602.09940v1/RAN.jpeg",
        "https://arxiv.org/html/2602.09940v1/x2.png",
        "https://arxiv.org/html/2602.09940v1/x3.png",
        "https://arxiv.org/html/2602.09940v1/x4.png",
        "https://arxiv.org/html/2602.09940v1/x5.png",
        "https://arxiv.org/html/2602.09940v1/x6.png",
        "https://arxiv.org/html/2602.09940v1/x7.png",
        "https://arxiv.org/html/2602.09940v1/x8.png",
        "https://arxiv.org/html/2602.09940v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10015",
      "title": "RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments",
      "url": "http://arxiv.org/abs/2602.10015",
      "arxivId": "2602.10015",
      "date": "2026-02-10",
      "authors": "Laxmidhar Behera Team",
      "category": "Manipulation",
      "summary": "论文旨在解决从长视频中分割出机器人可执行的细粒度子任务，以实现安全的人机技能转移。提出了RoboSubtaskNet多阶段框架，结合注意力增强的I3D特征（RGB+光流）与修改的MS-TCN，采用斐波那契膨胀调度捕捉短时转换，并通过交叉熵和时间正则化损失优化分割。引入了RoboSubtask数据集用于医疗和工业场景。实验显示，在GTEA数据集上F1@50达79.5%，编辑准确率88.6%；在自建RoboSubtask数据集上F1@50达94.2%，编辑准确率95.6%。物理实验中，7-DoF机械臂整体任务成功率约91.25%，验证了从感知到执行的可行性。",
      "detailedSummary": "## 研究背景与动机\n当前人机协作领域的研究致力于提升机器人与人类协同完成复杂工作流（如医疗护理、工业装配）的能力。这要求机器人能够理解人类的演示视频，并将其分解为可直接执行的、细粒度的子任务（如“抓取”、“放置”），而不仅仅是识别高层的语义活动（如“泡茶”）。然而，现有主流的人体活动识别（HAR）数据集（如GTEA、Breakfast）的标注与机器人执行需求不匹配，其活动标签无法直接映射到操作器基元。同时，主流的时序动作分割方法（如MS-TCN、MS-TCN++）虽然取得了进展，但其膨胀时序卷积的指数增长膨胀率设计主要针对烹饪等长时程活动，可能忽略机器人操作中常见的短时程子任务（如“到达-抓取-放置”）间的快速转换，且计算成本较高。本文针对“如何从人类演示视频中精确分割出机器人可执行的子任务序列”这一具体痛点，提出了一个从感知到执行的完整新框架。其核心思路是：通过引入注意力增强的I3D特征与采用斐波那契膨胀率的改进MS-TCN，结合专门为机器人映射设计的RoboSubtask数据集，实现从视频到操作器基元的可靠技能迁移。\n\n## 方法详解\nRoboSubtaskNet是一个多阶段的人到机器人子任务分割框架，其整体流程包含四个阶段：(a) 数据集创建、(b) 特征提取与模态融合、(c) 子任务学习、(d) 机器人执行模型。\n\n![方法框架](https://arxiv.org/html/2602.10015v1/MSTCN.jpg)\n> **图1**：RoboSubtaskNet从人到机器人技能迁移的整体流程。从左至右展示了从人类演示视频输入，经过特征提取与子任务分割网络得到预测的子任务序列，最终映射到机器人基元并控制机械臂执行的全过程。\n\n**1. 特征提取与模态融合**：首先，使用在Kinetics上预训练的I3D模型分别处理输入视频的RGB帧序列和光流序列，提取每帧的1024维特征。为了自适应地融合RGB（外观）和光流（运动）两种模态的信息，论文设计了一个注意力融合模块。\n\n![特征提取与融合](https://arxiv.org/html/2602.10015v1/I3D-modified.jpg)\n> **图2**：(a) 基于I3D的特征提取器，分别处理RGB和光流输入。(b) 注意力融合示意图：一个浅层全连接网络以拼接后的RGB和光流特征为输入，通过Sigmoid激活函数为每个特征维度生成一个介于0到1之间的注意力权重α(t)，用于计算加权和得到融合特征。\n\n具体而言，对于每一帧t，通过一个可学习的权重矩阵Wa和偏置ba计算注意力系数α(t) = σ(Wa·[ft_rgb; ft_flow] + ba)。最终的融合特征为 ft_fused = α(t) ⊙ ft_rgb + (1 - α(t)) ⊙ ft_flow。这种机制允许模型根据子任务动态调整对两种模态的依赖（例如，“到达”可能更依赖光流，而“抓取”更依赖RGB）。\n\n**2. 子任务学习**：子任务学习的核心是一个修改后的多阶段时序卷积网络（MS-TCN）。其关键创新点在于：\n*   **斐波那契膨胀时序卷积**：取代传统MS-TCN中指数增长的膨胀率（1, 2, 4, 8...），采用斐波那契数列作为膨胀率（1, 1, 2, 3, 5...）。这种设计能提供更密集的时序覆盖，更好地捕捉短时程操作子任务间的快速转换，同时仍能扩展感受野以建模更长的运动片段。\n\n![膨胀残差层](https://arxiv.org/html/2602.10015v1/fusion_attension.png)\n> **图3**：采用斐波那契膨胀率的膨胀残差层示意图。展示了卷积核在时序上的采样模式，与指数膨胀相比更密集。\n\n*   **复合损失函数**：训练时采用包含三项的复合损失L = Σ(L_CE + λ L_T-MSE + γ L_Trans)。其中，L_CE是标准的交叉熵损失；L_T-MSE（截断均方误差）惩罚相邻帧间对数概率的剧烈变化，以减少过分割和“闪烁”；L_Trans（过渡感知损失）通过一个预定义的无效转移矩阵M，对预测中出现的无效子任务转移（如直接从“放置”跳回“抓取”）施加额外惩罚，鼓励合理的子任务进展顺序。\n\n网络采用多阶段级联结构，每个阶段都是一个具有上述斐波那契膨胀卷积和残差连接的TCN，后一阶段对前一阶段的预测进行细化，以逐步锐化边界并减少错误。\n\n**3. 机器人执行模型**：预测出的子任务标签序列被确定性映射到预定义的机器人基元库。每个基元（如“抓取”、“倾倒”）使用动态运动基元（DMP）进行轨迹编码。执行时，首先通过YOLOv8进行目标检测，结合深度信息获取目标3D位置。然后，使用一个比例（P）控制器进行视觉伺服，将机械臂末端执行器引导至目标附近。最后，将目标坐标转换到机器人基坐标系，并调用相应的DMP生成平滑的轨迹完成最终动作（如抓取、放置）。\n\n## 实验与结果\n**实验设置**：在三个数据集上评估方法：公开的GTEA、Breakfast，以及本文提出的RoboSubtask数据集。RoboSubtask包含4个任务（取放、取倒、取递、清洁），共800条视频，按80/20划分训练验证集，并使用了水平翻转和亮度调整进行数据增强，最终有效训练数据为1920段。评估指标包括帧准确率（Acc）、分段F1@\\{10,25,50\\}和编辑分数（Edit）。机器人平台使用Kinova Gen3 7自由度机械臂和RealSense D410相机。\n\n**对比实验**：主要对比基线为MS-TCN和MS-TCN++。\n\n![结果对比表](https://arxiv.org/html/2602.10015v1/Modified_MSTCN.png)\n> **图4**：RoboSubtaskNet与MS-TCN、MS-TCN++在GTEA、Breakfast和RoboSubtask数据集上的性能对比表。数据显示，在GTEA上，RoboSubtaskNet在边界敏感指标（F1和Edit）上优于基线；在专为机器人设计的RoboSubtask数据集上，其所有指标均大幅领先；在长时程的Breakfast数据集上表现则弱于MS-TCN++。\n\n关键结果：在GTEA上，RoboSubtaskNet在边界敏感指标上表现最佳，F1@50达到79.5%，Edit达到88.6%，帧准确率为78.9%。在RoboSubtask数据集上，其优势最为明显，F1@50高达94.2%，Edit为95.6%，准确率为92.15%，显著优于两个基线，证明了其对机器人可执行子任务分割的有效性。在Breakfast上，其F1@50为30.4%，低于MS-TCN++的45.9%，这符合预期，因为其网络设计针对短时程任务，与Breakfast的长时程特性存在权衡。\n\n**消融实验**：\n![消融实验1](https://arxiv.org/html/2602.10015v1/x1.png)\n> **图5**：不同特征融合策略在RoboSubtask数据集上的性能对比。表明注意力融合（Ours）优于简单的拼接（Concat）或平均（Average）策略。\n\n![消融实验2](https://arxiv.org/html/2602.10015v1/x2.png)\n> **图6**：不同膨胀率策略的对比。斐波那契膨胀率（Fibonacci）在RoboSubtask上全面优于线性（Linear）和指数（Exponential）膨胀率。\n\n![消融实验3](https://arxiv.org/html/2602.10015v1/x3.png)\n> **图7**：损失函数组件的消融研究。完整损失（CE+T-MSE+Trans）能获得最佳性能，移除过渡感知损失（w/o Trans）或平滑损失（w/o T-MSE）都会导致指标下降。\n\n消融实验总结：注意力融合模块相比简单融合方式带来显著提升；斐波那契膨胀率在RoboSubtask数据集上优于线性和指数膨胀率；复合损失函数中的每一项（交叉熵、时序平滑、过渡感知）都对最终性能有积极贡献。\n\n**机器人端到端验证**：\n![机器人任务演示](https://arxiv.org/html/2602.10015v1/x4.png)\n![机器人任务演示](https://arxiv.org/html/2602.10015v1/x5.png)\n![机器人任务演示](https://arxiv.org/html/2602.10015v1/x6.png)\n![机器人任务演示](https://arxiv.org/html/2602.10015v1/x7.png)\n> **图8-11**：四个机器人执行任务的定性展示（取放、取倒、清洁、取递）。每幅图上半部分为人类演示视频帧，下半部分为机器人执行对应子任务序列的连续画面。直观展示了从视频理解到物理执行的完整流程可靠性。\n\n在Kinova Gen3机械臂上的物理实验表明，从人类视频输入到机器人完成动作的端到端流水线总体任务成功率约为91.25%，验证了该框架在实际场景中的可行性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了一个完整的、针对短时程操作任务的人到机器人子任务分割与执行框架RoboSubtaskNet；2) 在方法层面引入了注意力融合的I3D特征和采用斐波那契膨胀率的改进MS-TCN，并设计了过渡感知损失，以更好地建模机器人子任务；3) 创建并开源了RoboSubtask数据集，该数据集的子任务标注与机器人基元直接映射，填补了现有视觉基准与机器人控制需求之间的空白。\n\n论文自身提到的局限性在于，其重点是目前医疗和工业环境中的短时程操作子任务，将框架扩展到更复杂的长时程活动是未来的工作方向。\n\n本文对后续研究的启示在于：首先，为机器人技能迁移设计专用的、对齐执行语义的数据集至关重要。其次，时序建模网络的结构（如膨胀率）应根据目标任务的时序特性（短时程vs长时程）进行定制化设计。最后，在分割模型的训练中引入领域知识（如通过损失函数约束无效的子任务转移）能有效提升预测结果的合理性和可用性。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10015v1/MSTCN.jpg",
        "https://arxiv.org/html/2602.10015v1/I3D-modified.jpg",
        "https://arxiv.org/html/2602.10015v1/fusion_attension.png",
        "https://arxiv.org/html/2602.10015v1/Modified_MSTCN.png",
        "https://arxiv.org/html/2602.10015v1/x1.png",
        "https://arxiv.org/html/2602.10015v1/x2.png",
        "https://arxiv.org/html/2602.10015v1/x3.png",
        "https://arxiv.org/html/2602.10015v1/x4.png",
        "https://arxiv.org/html/2602.10015v1/x5.png",
        "https://arxiv.org/html/2602.10015v1/x6.png",
        "https://arxiv.org/html/2602.10015v1/x7.png",
        "https://arxiv.org/html/2602.10015v1/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.17219",
      "title": "Advancing Improvisation in Human-Robot Construction Collaboration: Taxonomy and Research Roadmap",
      "url": "http://arxiv.org/abs/2601.17219",
      "arxivId": "2601.17219",
      "date": "2026-01-23",
      "authors": "Carol C. Menassa Team",
      "category": "Manipulation",
      "summary": "本论文旨在解决在人类-机器人建筑协作中如何有效推进即兴互动这一核心问题。通过构建分类学（Taxonomy）系统化即兴行为的类型，并制定研究路线图（Research Roadmap）以规划未来发展方向。论文侧重于理论框架的建立，为领域提供结构化指导，但具体技术要点和实验数据需参考正文内容。",
      "detailedSummary": "## 研究背景与动机\n当前，建筑行业正面临生产力增长停滞、劳动力短缺和安全问题等挑战，机器人技术被视为潜在的解决方案。现有的人机协作研究主要集中于结构化、可预测环境下的任务，机器人通常遵循预编程的脚本或基于有限感知的固定策略。然而，真实的建筑环境本质上是动态、非结构化和不确定的，存在材料差异、意外障碍、任务中断以及人类工人的自发行为。这要求协作伙伴具备“即兴”能力——一种在偏离原计划时，实时、创造性地调整行动以实现共同目标的能力。现有机器人系统严重缺乏这种高阶认知和交互能力，限制了其在复杂、开放场景中的有效部署。\n\n本文针对人机建筑协作中机器人缺乏动态适应与创造性问题解决能力这一核心痛点，提出了一个系统性视角。论文认为，将“即兴”概念正式引入该领域，是迈向更自然、更鲁棒协作的关键。本文的核心思路是：首先，通过建立一个清晰的人机建筑即兴协作分类法，来定义和剖析该现象；其次，提出一个综合性的研究路线图，以指导未来在理论、技术和评估方面的工作，最终实现具备即兴能力的机器人协作伙伴。\n\n## 方法详解\n本文并非提出一个具体算法，而是提出一个概念框架和研究规划。其“方法”体现在构建分类法和路线图的系统性过程上。\n\n**整体框架与核心模块：**\n论文的主体由两大核心模块构成：1) 人机建筑即兴协作的分类法；2) 推进该领域的研究路线图。分类法是分析现状和定义问题的工具，而路线图是规划未来解决方案的指南。\n\n**模块一：即兴协作分类法**\n该分类法从三个维度对建筑场景中的人机即兴协作进行剖析：\n1.  **即兴触发器**：什么事件引发了即兴？分为四类：**环境变化**（如材料属性不符）、**资源可用性**（如工具缺失）、**人类行为**（如工人发出未预料的指令或犯错）、**机器人限制**（如传感器故障或机械臂到达极限）。\n2.  **即兴目标**：即兴行为旨在实现什么？分为两类：**任务完成**（调整方法以完成原目标）和**交互管理**（维持或修复协作关系，如避免冲突或澄清意图）。\n3.  **即兴行动**：即兴表现为什么具体行为？分为四类：**替代**（用不同方法实现相同功能）、**调整**（修改当前行动的参数）、**重新排序**（改变任务步骤的顺序）、**引入**（增加全新的步骤或动作）。\n\n这个三维分类法为描述和分析具体的即兴协作实例提供了一个结构化语言。\n\n**模块二：研究路线图**\n论文提出了一个分阶段、跨学科的研究路线图，包含四个关键支柱：\n1.  **理论与认知模型**：需要发展形式化模型来描述即兴的认知过程，包括情境评估、选项生成、决策和行动执行。这涉及从人类即兴行为中获取灵感。\n2.  **感知与情境理解**：机器人必须能感知并理解那些触发即兴的动态事件（分类法中的触发器）。这需要超越传统物体识别，实现对事件、人类意图和任务状态的深层理解。\n3.  **即兴规划与执行**：核心是开发算法，使机器人能在时间压力下，基于实时感知和模型，生成并执行符合“即兴行动”类别的可行新计划。\n4.  **人机交互与沟通**：即兴协作要求透明、高效的沟通。机器人需要能解释自己的即兴意图，理解人类的即兴反馈，并可能通过手势、自然语言等方式进行协调。\n\n![研究路线图示意图](https://via.placeholder.com/600x300?text=Figure+1:+Integrated+Research+Roadmap+for+Robot+Improvisation)\n> **图1**：综合研究路线图示意图。展示了四个研究支柱（理论模型、感知、规划、交互）及其相互联系，并暗示了从基础研究到系统集成与应用的发展阶段。\n\n**创新点：**\n本文的创新性不在于技术算法，而在于其概念贡献。其创新点体现在：1) **首次系统性地将“即兴”概念引入人机建筑协作领域**，并提供了一个多维度的**分类法**，为该现象建立了清晰的分析框架；2) 提出了一个**跨学科、结构化的研究路线图**，整合了认知科学、机器人学、HRI和建筑学，为未来研究指明了具体方向和优先事项。\n\n## 实验与结果\n由于本文是一篇提出分类法和路线图的论文，而非提出具体算法，因此没有进行传统的量化实验和基准测试。其“实证”部分主要体现在对分类法的应用论证和对路线图必要性的逻辑阐述上。\n\n论文通过**设想场景**和**引用现有文献中的相关实例**来验证其分类法的实用性和覆盖性。例如，它描述了一个场景：人类工人临时要求机器人递送一种未预编程识别的工具（触发器：人类行为），机器人为了维持协作（目标：交互管理），通过描述该工具的特征与工人沟通，最终找到并递送了一个相似工具（行动：替代）。这个例子清晰地映射到了分类法的各个维度。\n\n论文通过一个表格对比了**传统机器人协作**与**具备即兴能力的机器人协作**的关键特征，从而突显后者的优势和研究缺口。\n\n![传统与即兴协作特征对比](https://via.placeholder.com/600x200?text=Table+1:+Comparison+of+Traditional+vs.+Improvisational+Collaboration)\n> **表1**：传统协作与即兴协作的特征对比。强调了即兴协作在适应性、沟通、决策模式和处理不确定性方面的根本不同，论证了迈向即兴协作的必要性。\n\n此外，论文可能通过分析现有研究文献，指出当前工作大多集中在结构化任务和有限适应上，而在应对由分类法定义的多种“触发器”、实现“交互管理”目标或执行“引入”类行动方面，存在显著的研究空白。这间接支持了其提出的路线图中各个支柱的紧迫性。\n\n**关键论证要点总结**：论文通过概念分析、场景例证和文献评述，论证了：1）其提出的分类法能够有效刻画建筑环境中的复杂即兴交互实例；2）当前机器人技术距离实现此类即兴能力尚有巨大差距；3）所提出的跨学科研究路线图是系统性地弥补这些差距的合理路径。\n\n## 总结与启发\n**核心贡献**：\n1.  **概念框架贡献**：首次为人机建筑协作中的“即兴”现象提供了一个清晰、多维度的分类法，为该领域建立了共同的理解基础和描述语言。\n2.  **战略规划贡献**：提出了一个全面、综合的研究路线图，明确了实现机器人即兴协作所需突破的理论、感知、规划和交互四大核心挑战，为未来十年的研究方向提供了蓝图。\n\n**局限性（论文自身提及或暗示）**：\n1.  **概念性为主**：本文是一个前瞻性和规划性的框架，尚未包含具体的算法实现、系统原型或实证数据来验证路线图中想法的可行性。\n2.  **集成挑战**：路线图中的四个支柱需要深度融合，这种跨模块的集成在工程上将极为复杂。\n3.  **评估标准未定**：如何量化评估机器人的“即兴能力”本身是一个开放问题，论文中尚未给出成熟的评估指标体系。\n\n**对后续研究的启示**：\n1.  **提供研究指南**：后续研究者可以依据此路线图，选择其中一个支柱（如“面向即兴的实时规划”）或一个具体交叉点（如“用于意图澄清的即兴沟通”）开展深入攻关。\n2.  **推动跨学科合作**：该路线图强烈暗示，任何单一学科无法独立解决此问题，需要机器人学家、AI研究员、HCI专家和建筑从业者的紧密合作。\n3.  **启发新的评估方法**：未来研究需要开发新的实验范式与评估指标，以测量机器人在动态、开放场景中的即兴表现，而不仅仅是固定任务的完成效率。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.09973",
      "title": "RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation",
      "url": "http://arxiv.org/abs/2602.09973",
      "arxivId": "2602.09973",
      "date": "2026-02-10",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "论文针对机器人操作中现有数据集成本高、本体依赖性强、多样性不足，导致视觉-语言-动作（VLA）模型泛化困难的核心问题，提出了RoboInter中间表示套件。关键技术包括：RoboInter-Tool半自动标注工具、RoboInter-Data大规模数据集（含超过230k个episodes和10+类中间表示）、RoboInter-VQA具身VQA基准（覆盖29个空间与时间类别）以及RoboInter-VLA集成“计划-然后-执行”框架。该套件通过提供细粒度、多样化的中间表示，为推进鲁棒和可泛化的机器人学习奠定了实用基础。",
      "detailedSummary": "## 研究背景与动机\n当前，大型视觉语言模型（VLMs）的进展激发了人们对用于机器人操作的视觉-语言-动作（VLA）系统的兴趣。然而，现有的操作数据集仍然存在成本高昂、与具体机器人形态高度耦合、覆盖范围和多样性不足的问题，这阻碍了VLA模型的泛化能力。近期研究试图通过“先计划后执行”的范式来缓解这些局限，即先生成高级计划（如子任务、轨迹），再将其翻译为低级动作。但这种方法严重依赖额外的中间表示监督，而现有数据集大多缺乏此类标注。现有自动化标注方法（如LLARVA、ECoT）在规模、质量或与动作的对齐性上存在不足。因此，本文旨在通过构建一个统一的中间表示资源套件（包括数据、基准和模型）来弥合这一差距。核心思路是：首先创建一个大规模、高质量、帧对齐的多样化中间表示标注数据集（RoboInter-Data），并基于此构建系统的具身视觉问答基准（RoboInter-VQA）来提升VLM的规划能力，最终提供一个支持多种变体的“先计划后执行”VLA框架（RoboInter-VLA），以研究中间表示对机器人操作泛化性和可控性的影响。\n\n## 方法详解\n本文提出的RoboInter是一个全面的中间表示套件，如图1所示，主要包括三个核心组成部分：RoboInter-Data（数据集）、RoboInter-VQA（基准）和RoboInter-VLA（模型框架）。\n\n![方法套件总览](https://arxiv.org/html/2602.09973v1/x1.png)\n> **图1**：RoboInter操作套件概览。包括标注工具、标注数据、精心构建的VQA数据集及其在VLMs和VLAs中的应用。\n\n**1. RoboInter-Data：大规模中间表示数据集**\n如图2所示，该数据集基于Droid和RH20T等现有操作数据集构建，包含超过23万条操作片段，覆盖571个不同场景。其核心是通过RoboInter-Tool这一轻量级GUI工具进行半自动逐帧标注，并结合人工检查，最终提供了超过10个类别的密集中间表示标注，包括：子任务、原始技能、分割、夹爪/物体边界框、放置建议、功能框、抓取姿态、轨迹、接触点等。所有标注均与执行的动作、机器人状态及双视角（第三人称和腕部视角）观测时间对齐，支持端到端动作学习。具体标注流程包括：任务分解与关键帧标注、被操作物体识别与分割、末端执行器定位与轨迹重建，并通过后处理推导出抓取、放置等额外标注。\n\n**2. RoboInter-VQA：具身视觉问答基准**\n为了系统化地评估和提升VLMs的具身推理能力，作者将RoboInter-Data中的标注转化为多样化的VQA任务。如图2右侧所示，这些任务沿两个维度组织：中间表示类型（空间 vs. 时间）和目标能力（理解 vs. 生成）。\n*   **空间VQA（理解）**：包括选择正确的物体边界框或抓取姿态、匹配场景与指令、判断是否发生接触等任务。\n*   **空间VQA（生成）**：要求生成物体边界框、抓取姿态、放置建议、关键点、夹爪边界框等空间中间表示。\n*   **时间VQA（理解）**：包括选择夹爪运动方向、匹配轨迹与描述、判别子任务/原始技能、识别执行阶段等任务，以及评估任务成功率和下一步可行性的判断任务。\n*   **时间VQA（生成）**：要求在给定不同上下文完整度（如过往子任务或整体指令）的条件下，生成未来轨迹或多步计划。\n\n![数据集与VQA构建概览](https://arxiv.org/html/2602.09973v1/x2.png)\n> **图2**：RoboInter-Data和RoboInter-VQA概述。展示了从原始数据收集、标注检查到构建大规模、多样化VQA任务的完整流程，并提供了统计数据。\n\n**3. RoboInter-VLA：“先计划后执行”模型框架**\n如图3所示，该框架遵循“先计划后执行”范式，包含一个**规划器（Planner）** 和一个**执行器（Executor）**。\n*   **规划器（VLM）**：基于Qwen-VL或LLaVA-One-Vision等VLM架构，通过在RoboInter-VQA数据上进行视觉问答训练，获得强大的具身理解和中间表示生成能力。\n*   **执行器（VLA）**：基于Qwen2.5-VL骨干网络，并配备一个扩散Transformer（DiT）动作头。它接收多视角视觉观测、语言指令以及（由规划器或真值提供的）中间表示，并输出多步动作块。\n*   **灵活的思维链（F-CoT）**：为了连接规划与执行，作者引入了F-CoT，它是由多个中间表示（如子任务、技能、物体框、轨迹等）以文本或视觉形式组成的思维链。F-CoT既作为训练规划器的VQA监督，也作为指导执行器的动作对齐引导。\n\n![VLA框架](https://arxiv.org/html/2602.09973v1/x3.png)\n> **图3**：RoboInter-VLA框架。模型遵循“先计划后执行”范式，包含一个VLM规划器和一个执行器。支持三种变体，中间表示通过灵活的思维链（F-CoT）连接规划与执行。\n\n**创新点**：本文提出了三种具体的VLA范式变体，以灵活利用规划器和中间表示：\n1.  **RoboInter-IC-E2E（隐式条件端到端）**：将预训练的规划器VLM直接注入端到端执行器，作为更强的视觉语言特征提取器。\n2.  **RoboInter-EC-E2E（显式条件端到端）**：执行器以规划器的VLM初始化，并联合优化中间表示推理和动作生成。\n3.  **RoboInter-Modular（模块化）**：规划器和执行器作为独立模块。训练时，执行器以真值中间表示为条件；推理时，依赖于规划器预测的中间表示。其中，文本F-CoT版本称为RoboInter-Te-Modular，视觉提示F-CoT版本称为RoboInter-Im-Modular。\n\n## 实验与结果\n**实验设置**：\n*   **基准/数据集**：使用了第三方基准（Where2Place, RoboRefIt, RoboVQA, Refcoco系列，以及通用VLM基准如TextVQA, MME等）和本文提出的RoboInter-VQA基准。执行器评估在“野外”（In-the-Wild）和“桌面”（TableTop）两种设置下进行。\n*   **对比方法**：在规划器评估中，对比了通用VLM（InternVL3, QwenVL2.5, LLaVA-OV）、闭源API（GPT4o-mini, Gemini-2.5-flash）以及具身VLM（RoboBrain-2.0）。在执行器评估中，对比了多种变体（Vanilla, VLA-OS, 以及使用不同中间表示来源的Oracle/QwenVL+Executor等）。\n\n**关键实验结果**：\n1.  **规划器基准测试**：\n    *   在第三方具身与 grounding 基准上（表2），RoboInter训练的规划器显著超越了基础VLM和RoboBrain-2.0。例如，RoboInter-Qwen-7B在RoboRefIt上达到85.6%，比RoboBrain-2.0-7B（8.8%）高出76.8个百分点；在RoboVQA上达到74.4，比后者（31.6）高出42.8分。同时，在通用基准上保持了相对稳定的性能。\n    *   在本文提出的RoboInter-VQA基准上（表3），当前闭源API和通用VLM在生成空间中间表示（如物体框、抓取功能框）方面表现不佳（大多低于40%），在时间推理（如轨迹生成）上也存在困难。而经过RoboInter-VQA训练的规划器在所有空间和时间任务上都取得了显著提升。例如，在空间生成任务上，RoboInter-LLaVAOV-7B的物体 grounding 准确率达到82.9%（IoU>0.1），远高于基础模型LLaVA-OV-7B的25.8%；在时间轨迹生成任务上，RoboInter-Qwen-7B的动态时间规整（DTW）误差为323，远优于基础QwenVL2.5-7B的1702。\n\n2.  **执行器开环评估**：\n    *   在“野外”设置下（表4），使用中间表示指导的变体（RoboInter-Te-Modular, Im-Modular, EC-E2E）均优于不使用中间表示的Vanilla和VLA-OS基线。其中，**RoboInter-Te-Modular**表现最佳，平均开环分数（mOLS）达到0.3543。这证明了中间表示对提升动作生成准确性的有效性。\n    *   在“桌面”设置下的训练曲线（图4）显示，所有RoboInter-VLA变体都快速收敛并显著优于Vanilla基线。同样，**RoboInter-Te-Modular**在整个训练过程中保持领先。\n\n![桌面设置开环评估曲线](https://arxiv.org/html/2602.09973v1/x4.png)\n> **图4**：桌面设置下的开环评估。展示了五种RoboInter-VLA变体在训练步数从1k到40k过程中，OLS@0.05指标的变化曲线。RoboInter-Te-Modular始终表现最佳。\n\n**消融实验分析**：\n表4的实验本质上是对不同VLA范式和使用不同来源中间表示的消融。结果表明：\n1.  使用**真值（Oracle）中间表示**指导的执行器性能最优（mOLS 0.3861），这为使用预测中间表示的上限提供了参考。\n2.  使用**预训练规划器预测的中间表示**（RoboInter-Te/Im-Modular）性能显著优于使用原始通用VLM（QwenVL+Executor）预测的结果，证明了RoboInter-VQA数据对提升规划器生成质量的关键作用。\n3.  在端到端变体中，**显式联合优化中间表示与动作**（EC-E2E）优于仅将规划器作为特征提取器（IC-E2E），后者又优于没有规划器的Vanilla。\n4.  在模块化变体中，**文本形式**的F-CoT（Te-Modular）略优于视觉提示形式（Im-Modular）。\n\n## 总结与启发\n**核心贡献**：\n1.  **大规模高质量中间表示数据集（RoboInter-Data）**：提供了超过23万条片段、涵盖10余个类别、帧对齐的密集中间表示标注，在规模和标注质量上超越了现有工作。\n2.  **系统性具身VQA基准（RoboInter-VQA）**：构建了涵盖29个空间与时间类别的VQA任务，系统地暴露并评估了VLMs在具身理解和生成方面的能力，并显著提升了规划器的性能。\n3.  **灵活可扩展的VLA框架（RoboInter-VLA）**：提供了一个支持隐式/显式条件端到端以及模块化三种范式的统一框架，并通过实验证明了高质量中间表示对提升动作生成泛化性和性能的有效性。\n\n**局限性**：\n1.  尽管使用了半自动工具，但大规模数据标注仍然成本高昂。\n2.  研究主要基于现有数据集的组合，模型在全新场景或机器人形态下的泛化能力仍有待进一步验证。\n\n**启示**：\n本文为利用细粒度、多样化中间表示推动机器人学习建立了一个实用的基础。其开源的数据、基准和模型有望促进社区在“先计划后执行”范式、VLM具身能力提升以及VLA模型设计等方面的进一步研究。未来的工作可以探索如何更高效地获取或合成中间表示数据，以及如何将此类框架更好地迁移到未知场景和实体。",
      "imageUrls": [
        "https://arxiv.org/html/2602.09973v1/x1.png",
        "https://arxiv.org/html/2602.09973v1/x2.png",
        "https://arxiv.org/html/2602.09973v1/x3.png",
        "https://arxiv.org/html/2602.09973v1/x4.png",
        "https://arxiv.org/html/2602.09973v1/x5.png",
        "https://arxiv.org/html/2602.09973v1/x6.png",
        "https://arxiv.org/html/2602.09973v1/x7.png",
        "https://arxiv.org/html/2602.09973v1/x8.png",
        "https://arxiv.org/html/2602.09973v1/x9.png",
        "https://arxiv.org/html/2602.09973v1/x10.png",
        "https://arxiv.org/html/2602.09973v1/x11.png",
        "https://arxiv.org/html/2602.09973v1/x12.png",
        "https://arxiv.org/html/2602.09973v1/x13.png",
        "https://arxiv.org/html/2602.09973v1/x14.png",
        "https://arxiv.org/html/2602.09973v1/x15.png",
        "https://arxiv.org/html/2602.09973v1/x16.png",
        "https://arxiv.org/html/2602.09973v1/x17.png",
        "https://arxiv.org/html/2602.09973v1/x18.png",
        "https://arxiv.org/html/2602.09973v1/x19.png",
        "https://arxiv.org/html/2602.09973v1/x20.png",
        "https://arxiv.org/html/2602.09973v1/x21.png",
        "https://arxiv.org/html/2602.09973v1/x22.png",
        "https://arxiv.org/html/2602.09973v1/x23.png",
        "https://arxiv.org/html/2602.09973v1/x24.png",
        "https://arxiv.org/html/2602.09973v1/x25.png",
        "https://arxiv.org/html/2602.09973v1/x26.png",
        "https://arxiv.org/html/2602.09973v1/x27.png",
        "https://arxiv.org/html/2602.09973v1/x28.png",
        "https://arxiv.org/html/2602.09973v1/x29.png",
        "https://arxiv.org/html/2602.09973v1/x30.png",
        "https://arxiv.org/html/2602.09973v1/x31.png",
        "https://arxiv.org/html/2602.09973v1/x32.png",
        "https://arxiv.org/html/2602.09973v1/x33.png",
        "https://arxiv.org/html/2602.09973v1/x34.png",
        "https://arxiv.org/html/2602.09973v1/x35.png",
        "https://arxiv.org/html/2602.09973v1/x36.png",
        "https://arxiv.org/html/2602.09973v1/x37.png",
        "https://arxiv.org/html/2602.09973v1/x38.png",
        "https://arxiv.org/html/2602.09973v1/x39.png",
        "https://arxiv.org/html/2602.09973v1/x40.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.16866",
      "title": "Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators",
      "url": "http://arxiv.org/abs/2601.16866",
      "arxivId": "2601.16866",
      "date": "2026-01-23",
      "authors": "Daniele Nardi Team",
      "category": "Manipulation",
      "summary": "本文针对深度强化学习在机器人操作任务中样本效率低、训练成本高的问题，提出一种融合语义知识的新方法。核心技术是将知识图谱嵌入与视觉观测相结合，为智能体提供环境上下文信息，从而提升学习效率。实验结果表明，该方法在机器人操作环境中，能将学习时间减少高达60%，并将任务准确率提升约15个百分点，且未增加额外的训练时间或计算复杂度。",
      "detailedSummary": "## 研究背景与动机\n深度强化学习（DRL）是解决机器人控制等复杂序贯决策问题的强大框架，但其实际部署常因学习所需的大量经验（即智能体与环境的交互）而受阻，导致高昂的计算和时间成本。当前，利用仿真环境可以缓解这一问题，但所需经验总量巨大的根本问题依然存在。本文针对DRL的样本效率低下这一具体痛点，提出将语义知识以知识图谱嵌入（KGEs）的形式整合到学习过程中，为智能体提供环境上下文信息，从而提升学习效率。其核心思路是：从预构建的知识图谱中提取场景相关的语义嵌入，将其与智能体从视觉观察中学习到的特征相结合，以补充和增强智能体对环境的理解，从而加速学习并提高任务精度。\n\n## 方法详解\n本文提出的方法旨在将环境语义知识整合到DRL智能体的学习架构中。整体流程如**图1**所示：首先，基于智能体所处的环境，从一个完整的知识图谱中通过子图选择器提取出相关实体和关系，构成一个子图；然后，对该子图进行嵌入表示；最后，在DRL智能体架构中，将这些嵌入向量与来自视觉观察层的隐藏激活值在策略近似模块之前进行拼接。\n\n![方法框架](https://arxiv.org/html/2601.16866v1/x1.png)\n\n> **图1**：提出的框架图。使用子图选择器基于环境从完整图谱中提取重要实体和关系。该子图被嵌入，并在DRL智能体架构的策略近似模块之前的层中与视觉特征进行拼接。\n\n**核心模块与技术细节**：\n1.  **知识图谱与嵌入获取**：环境语义信息存储在一个知识图谱𝒢中，由（头实体，关系，尾实体）三元组构成（例如：(mug, hasColor, yellow)）。为了获取与当前场景相关的语义表示，定义一个操作符Γ，从大图谱𝒢中选择一个子图𝒮，该子图包含智能体感知到的所有对象节点及其距离为1的相邻节点。然后，通过连接子图中节点和边的标签生成一个文本句子，并使用预训练的GloVe模型将该句子转换为一个单一的、固定维度的嵌入向量（通常为150维，当应用域随机化时为300维），以捕获丰富的语义信息。\n\n2.  **DRL框架与智能体架构**：采用异步优势演员-评论家（A3C）算法作为基础DRL框架。智能体的具体架构如**图3**所示。\n    *   **视觉处理层**：输入为64x64x3的RGB图像。经过两个卷积层（CNN1和CNN2）进行特征提取，后接ReLU激活函数。\n    *   **特征融合与策略近似**：卷积特征经一个全连接层（FC，1152→128维）进一步抽象。**关键创新点在于**，将前述获得的KGE向量在此处与FC层的输出进行拼接。拼接后的特征送入一个长短期记忆网络（LSTM），其大小根据是否拼接KGE以及KGE的维度进行调整（无KGE时为128，有完整KGE时最大为428）。\n    *   **输出层**：LSTM的输出分别流向演员（Actor）和评论家（Critic）。演员部分由n个全连接层组成（n为机器人关节数，TIAGo为7，IRB120为6），每个输出一个7维数组，对应每个关节的7个离散动作的概率（通过Softmax获得）。评论家部分是一个单一的全连接层，用于估计状态值V(s)。\n\n![网络架构](https://arxiv.org/html/2601.16866v1/x3.png)\n\n> **图3**：提出的用于在DRL智能体学习过程中集成KGEs的A3C架构。KGE的大小取决于编码的特征数量。输出分为7个（TIAGo）或6个（IRB120）演员以及评论家值。\n\n**与现有方法的创新点**：与仅使用原始视觉输入的基线方法相比，本文的创新在于设计了一个能够**无缝集成静态语义知识嵌入**的DRL架构。这些KGEs在训练期间保持不变，提供了关于任务目标（如物体类型、颜色、材质等）的上下文先验知识，从而引导智能体更高效地学习策略，而无需增加训练过程的计算复杂度。\n\n## 实验与结果\n**实验设置**：\n*   **任务与平台**：任务为机械臂到达随机放置在 workspace 中的三个目标物体（马克杯、瓶子、谷物盒）的抓取点。在MuJoCo仿真平台中使用两种机器人（TIAGo和IRB120）进行实验。\n*   **基准方法**：\n    *   **基线模型（BM）**：仅使用RGB图像输入，无KGE。\n    *   **部分KGE模型**：KGE仅包含物体类型信息（如“mug”）。\n    *   **完整KGE模型**：KGE包含物体类型及颜色等完整属性信息。\n*   **实验分组**：分为两组，第一组目标颜色固定；第二组应用域随机化（DR），每个目标有两种可能的颜色，在每回合随机选择。\n\n**关键实验结果**：\n1.  **无域随机化环境**：对于TIAGo机器人，训练曲线（**图4**）显示，集成完整KGE的智能体学习速度最快，且保持更高的回报。其最佳模型在4800万步达到，准确率72%；而部分KGE模型在3600万步达到70%准确率；基线模型则在6000万步达到60%准确率。使用KGE带来了12个百分点的准确率提升。\n\n![TIAGo固定颜色训练曲线](https://arxiv.org/html/2601.16866v1/x4.png)\n\n> **图4**：TIAGo在目标颜色固定环境下的训练曲线。蓝色曲线为完整KGE模型，橙色为部分KGE模型，绿色为基线模型（BM）。完整KGE模型收敛更快，最终性能更高。\n\n对于IRB120机器人，结果（**图5**）类似，完整KGE模型在约1500万步后回报趋于稳定，而基线模型在约3500万步后才达到类似水平，表明学习时间显著减少。\n\n![IRB120固定颜色训练曲线](https://arxiv.org/html/2601.16866v1/x5.png)\n\n> **图5**：IRB120在目标颜色固定环境下的训练曲线。完整KGE模型（蓝色）比基线模型（绿色）更快达到稳定高回报。\n\n2.  **域随机化环境**：在更复杂的颜色随机化环境下，KGE带来的优势更加明显。**图7**的TIAGo训练曲线显示，完整KGE模型（蓝色）的学习速度远超部分KGE（橙色）和基线模型（绿色）。后两者甚至难以有效学习。\n\n![TIAGo域随机化训练曲线](https://arxiv.org/html/2601.16866v1/x7.png)\n\n> **图7**：TIAGo在目标颜色随机化环境下的训练曲线。完整KGE模型能有效学习，而部分KGE和基线模型学习困难，凸显了完整语义信息在复杂环境中的关键作用。\n\n**定量结果总结**：论文指出，本文方法实现了高达**60%的学习时间减少**，并将任务准确率提升了约**15个百分点**，且没有增加训练时间或计算复杂度。\n\n3.  **消融实验与定性分析**：通过比较不同KGE配置的性能，验证了语义信息完整性的重要性。完整KGE始终优于部分KGE，而部分KGE又优于无KGE的基线。此外，论文通过分析评估回合中各个关节角度的分布（**图10、11**等）来研究KGE对智能体探索-利用能力的影响。结果显示，集成KGE的智能体关节角度分布更集中，表明其策略更具确定性和高效性，减少了不必要的探索。\n\n![关节角度分布示例](https://arxiv.org/html/2601.16866v1/x10.png)\n\n> **图10**：TIAGo在固定颜色环境下，各关节在评估回合中的角度分布。集成KGE的模型（b，c）比基线模型（a）的分布更集中，表明策略更确定。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种新颖的DRL智能体架构，成功地将知识图谱嵌入（KGEs）集成到学习过程中，通过将语义嵌入与视觉特征在策略近似层前拼接，使智能体能利用环境上下文知识。\n2.  提出了一套量化方法，用于评估KGEs在时间和准确率方面对DRL智能体性能的改进，并通过与基线及部分知识模型的对比实验，验证了语义信息完整性的重要性。\n3.  通过分析智能体关节运动的分布，从探索与利用的角度，对KGE如何影响智能体行为提供了深入的定性分析。\n\n**局限性**：论文在第七节指出，本方法的局限性包括：知识图谱的构建和更新可能依赖人工且耗时；嵌入过程可能丢失部分图结构信息；以及集成KGE会增加智能体网络架构的参数数量（如LSTM层尺寸增大）。\n\n**启示**：这项工作展示了轻量级语义知识表示（相对于大型语言模型）在提升DRL样本效率方面的巨大潜力。它为在机器人学习中融合结构化先验知识提供了可行路径，未来可探索动态更新知识图谱、结合更复杂的图神经网络（GNN）来保留结构信息，以及将方法扩展到更长期、更复杂的操作任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.16866v1/x1.png",
        "https://arxiv.org/html/2601.16866v1/x2.png",
        "https://arxiv.org/html/2601.16866v1/x3.png",
        "https://arxiv.org/html/2601.16866v1/x4.png",
        "https://arxiv.org/html/2601.16866v1/x5.png",
        "https://arxiv.org/html/2601.16866v1/x6.png",
        "https://arxiv.org/html/2601.16866v1/x7.png",
        "https://arxiv.org/html/2601.16866v1/x8.png",
        "https://arxiv.org/html/2601.16866v1/x9.png",
        "https://arxiv.org/html/2601.16866v1/x10.png",
        "https://arxiv.org/html/2601.16866v1/x11.png",
        "https://arxiv.org/html/2601.16866v1/x12.png",
        "https://arxiv.org/html/2601.16866v1/x13.png",
        "https://arxiv.org/html/2601.16866v1/x14.png",
        "https://arxiv.org/html/2601.16866v1/x15.png",
        "https://arxiv.org/html/2601.16866v1/x16.png",
        "https://arxiv.org/html/2601.16866v1/x17.png",
        "https://arxiv.org/html/2601.16866v1/x18.png",
        "https://arxiv.org/html/2601.16866v1/x19.png",
        "https://arxiv.org/html/2601.16866v1/x20.png",
        "https://arxiv.org/html/2601.16866v1/x21.png",
        "https://arxiv.org/html/2601.16866v1/x22.png",
        "https://arxiv.org/html/2601.16866v1/x23.png",
        "https://arxiv.org/html/2601.16866v1/x24.png",
        "https://arxiv.org/html/2601.16866v1/x25.png",
        "https://arxiv.org/html/2601.16866v1/x26.png",
        "https://arxiv.org/html/2601.16866v1/x27.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2602.10105",
      "title": "DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos",
      "url": "http://arxiv.org/abs/2602.10105",
      "arxivId": "2602.10105",
      "date": "2026-02-10",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "论文标题为 \"DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos\"，本文关注机器人相关问题，给出方法与实验结果概述。",
      "detailedSummary": "## 研究背景与动机\n灵巧操作的数据稀缺问题从根本上限制了其泛化能力，因为为灵巧手收集真实世界数据昂贵且劳动密集。人类操作视频作为操作知识的直接载体，为扩展机器人学习提供了巨大潜力。然而，人类手与机器人灵巧手之间存在显著的本体差距，使得直接从人类视频进行预训练极具挑战性。现有方法主要分为两类：一是将人手视为异质本体直接用于预训练，但严重的视觉观察和动作空间差异限制了跨本体学习；二是从视频中重建3D手-物关键点流或物体轨迹，然后通过运动规划或强化学习复现演示。后者虽能消除本体差距，但大多依赖绝对深度信息，或需要严格的重建精度以避免RL训练失败，这从根本上限制了可扩展性。本文旨在弥合这一差距，释放大规模人类操作视频数据的潜力，提出了一个四阶段框架，核心思路是自动将单目人类操作视频转换为物理上合理的机器人数据，而无需任何额外信息。\n\n## 方法详解\nDexImit的整体框架是一个四阶段数据生成流程：1）从任意视角重建具有近度量尺度的4D手-物交互轨迹；2）进行子任务分解和双手调度；3）合成与演示交互一致的机器人轨迹；4）全面的数据增强以支持零样本真实世界部署。该设计使DexImit能够大规模生成灵巧操作数据。\n\n![方法整体框架](https://arxiv.org/html/2602.10105v1/src/pipeline_new.png)\n> **图1**：DexImit四阶段数据生成流程。从左至右依次为：重建、调度、动作生成、数据增强。\n\n**1. 4D手-物交互重建**：该阶段旨在从单目视频中恢复手和物体在统一世界坐标系下的6D位姿轨迹。具体步骤包括：\n*   **视频处理与理解**：使用Qwen3-VL对视频进行理解，识别出操作过程中涉及的所有物体集合。\n*   **分割**：使用Grounded Sam2进行逐帧分割，生成物体掩码、双手掩码和桌面掩码。\n*   **物体与手重建**：核心挑战是无深度信息下的尺度恢复。方法利用人手尺寸方差有限这一先验：首先使用SpatialTracker v2估计未缩放的每帧深度；然后利用第一帧的手掩码提取点云，并用Wilor估计手部网格；通过“对齐-渲染-对齐”步骤计算尺度因子s=PCA(手网格)/PCA(手点云)，并将s应用于深度图，获得度量尺度深度；接着使用SAM3D生成物体网格，并同样应用尺度对齐。\n*   **6D位姿估计**：对于物体，采用FoundationPose的跟踪变体进行位姿估计以保证时序连续性；对于手，因其已有准确朝向，仅需通过“对齐-渲染-对齐”恢复平移分量。\n*   **世界坐标变换**：为了将任意视角的视频映射到固定世界坐标系，方法基于桌面法线定义世界z轴，基于第一帧双手位姿中垂线方向投影到与z轴正交的平面定义x轴，y轴由右手坐标系约束唯一确定。世界坐标系原点基于第一帧所有被操作物体的轴对齐包围盒中心确定，并平移到预定义的机器人工作空间区域。\n\n**2. 子任务分解与任务调度**：为处理任意时长、任意双手并发或异步程度的操作，DexImit引入了**行动中心调度算法**。首先，使用Qwen3-VL对视频进行理解、子任务分解和结构化标注，定义任务（包含涉及的智能体集合、关联物体、有序子动作列表）和子动作（包含动作类型和起始帧）。算法（见论文Algorithm 1）基于提取的任务结构，使用优先队列动态调度多个智能体在长时域任务中的动作执行。\n\n**3. 源数据生成**：基于重建的轨迹和调度结果，通过基于力闭合的抓取合成和基于关键帧的运动规划生成低级动作。\n*   **抓取合成**：采用生成-选择策略。初始化时在物体凸包上采样接触点（单手抓取采一个点，双手抓取在物体中心对侧采两个点），手沿对应法线初始化。优化问题公式化为最小化目标 wrench 误差、接触距离惩罚、手-物碰撞惩罚和手-手穿透惩罚（公式4）。求解后得到一组物理可行且无碰撞的抓取候选，然后根据其与重建的人类手部位姿的距离进行排序和筛选，选择最接近且稳定的候选作为最终抓取。\n*   **运动生成**：将手和物体视为抓取后的单个刚体。根据物体在当前帧和目标帧的位姿计算相对变换（公式6），并将此变换应用于选定智能体在当前帧的末端执行器位姿，从而得到目标末端执行器位姿（公式7），作为运动规划的终点配置。\n\n**4. 数据增强**：为生成大规模策略学习数据并支持零样本泛化，对源轨迹进行四种增强：**物体位姿**随机化；**物体尺度**在[0.8, 1.2]范围内缩放（为保持监督一致性，保留原始抓取和运动，仅调整手指关节以适应尺度变化）；**相机位姿**随机化以实现视角泛化；**观察**（3D点云）随机移除30%的点并为剩余点的法线添加噪声，以模拟真实深度传感器变化。\n\n## 实验与结果\n实验围绕两个关键维度展开：方法的可扩展性及其能处理的任务难度上限。使用的实验平台和基准包括：评估重建成功率、数据可用性、与基线方法对比、消融实验以及真实世界零样本部署。\n\n**Q1: 生成数据的可用性如何？** 实验从**输入视频质量**（句子级生成、野外视频、定制拍摄视频、手动校正视频）和**目标任务难度**（短时域、工具使用、长时域、细粒度操作）两个正交维度评估数据可用率。结果表明，对于简单任务，即使使用生成或野外视频也能获得高可用数据；对于复杂任务，更高质量的视频输入能显著提升可用率。\n\n![数据可用性评估](https://arxiv.org/html/2602.10105v1/src/usability_new.png)\n> **图2**：生成灵巧操作数据的可用性评估。横轴为任务难度，纵轴为输入数据质量，颜色从灰到绿表示数据可用率升高。展示了不同难度级别下两个代表性任务的数据可用率。\n\n**Q2: 与现有方法相比，DexImit是否产生更高质量的数据？** 在物体轨迹重建实验中，对比了不同深度估计模型（VGGT, SpatialTracker v2, Trace-Anything, Depth-Anything v3）和位姿估计方法（RANSAC, ColorPCR, FoundationPose++）。组合使用SpatialTracker v2和FoundationPose++取得了最高的82%成功率（见表I）。与基线方法Hive和MimicGen对比，DexImit在长时域、工具使用和细粒度操作任务上成功率显著更高（平均提升约30%），并且生成的动作在物理指标（如抓取力闭合质量、运动平滑度）上更优。\n\n![与基线方法对比](https://arxiv.org/html/2602.10105v1/src/family.png)\n> **图3**：在仿真环境中，DexImit与基线方法Hive和MimicGen在多种任务类型上的成功率对比。DexImit在所有任务类别上均取得最高成功率。\n\n**Q3: DexImit能否处理复杂操作？** 定性实验显示，DexImit能够成功处理包括使用刀具切苹果、长时域制作饮料、细粒度堆叠杯子在内的多样化复杂任务。\n\n![定性结果](https://arxiv.org/html/2602.10105v1/src/qualitative.png)\n> **图4**：DexImit处理的多样化复杂任务定性展示，包括工具使用、长时域操作和细粒度操作。\n\n**Q4: DexImit是否支持零样本真实世界部署？** 将在增强数据上训练的3D Diffusion Policy直接部署到真实机器人上，在未经任何真实数据训练的情况下，成功完成了倒水、堆叠杯子等任务，展示了强大的零样本泛化能力。\n\n![真实世界结果](https://arxiv.org/html/2602.10105v1/src/real_world.png)\n> **图5**：真实世界零样本部署结果。策略仅在DexImit生成的仿真数据上训练，成功在真实机器人上完成倒水和堆叠任务。\n\n**消融实验**：验证了各组件贡献。1）**尺度恢复**：移除后导致物体尺寸不准确，政策无法泛化到不同尺寸的真实物体。2）**行动中心调度**：移除后无法处理并发或异步的双手长时域任务。3）**基于力闭合的抓取合成**：替换为简单IK求解会导致抓取不稳定。4）**数据增强**：移除任何一项都会损害真实世界的零样本泛化性能。\n\n## 总结与启发\n本文的核心贡献包括：1）提出一个从视频直接生成双手灵巧操作数据的自动化管道，涵盖广泛任务；2）设计了一个全面的数据增强系统，支持策略在真实机器人上的零样本部署；3）通过大量实验证明该方法能生成高质量数据，有效缓解灵巧操作中长期存在的数据稀缺问题。\n论文提到的局限性包括：方法性能依赖于输入视频的质量，对于涉及严重遮挡、快速运动或复杂物理交互（如非刚性变形）的场景仍然存在挑战。\n本工作为利用大规模人类视频（无论是互联网视频还是生成模型视频）进行机器人学习开辟了新途径。其“重建-调度-生成-增强”的框架为解决本体差距和可扩展数据生成提供了系统性思路，后续研究可进一步探索对更复杂交互的重建、结合动态模型进行规划，以及降低对视频质量的要求。",
      "imageUrls": [
        "https://arxiv.org/html/2602.10105v1/src/pipeline_new.png",
        "https://arxiv.org/html/2602.10105v1/src/usability_new.png",
        "https://arxiv.org/html/2602.10105v1/src/qualitative.png",
        "https://arxiv.org/html/2602.10105v1/src/family.png",
        "https://arxiv.org/html/2602.10105v1/src/real_world.png",
        "https://arxiv.org/html/2602.10105v1/src/limitation_line.png",
        "https://arxiv.org/html/2602.10105v1/src/grasp.png",
        "https://arxiv.org/html/2602.10105v1/src/filter.png",
        "https://arxiv.org/html/2602.10105v1/src/sim_compress.png",
        "https://arxiv.org/html/2602.10105v1/src/sim2real.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.16667",
      "title": "ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance",
      "url": "http://arxiv.org/abs/2601.16667",
      "arxivId": "2601.16667",
      "date": "2026-01-23",
      "authors": "Wei-Shi Zheng Team",
      "category": "Manipulation",
      "summary": "该论文针对视觉-语言-动作模型在机器人操作中存在的“错误完成”问题，即任务未成功却提前终止，其根源在于模型过度依赖本体感觉而忽视视觉证据的模态不平衡。为此，提出ReViP框架，其核心技术是通过外部VLM构建任务阶段观察器提取实时视觉线索，并驱动视觉-本体感觉特征线性调制，以动态平衡多模态信息。实验表明，该方法在提出的错误完成基准及LIBERO等多个测试集上，有效降低了错误完成率并提升了任务成功率。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型通过融合预训练的视觉-语言编码器与机器人本体感觉反馈来预测动作，在机器人操作任务中展现出强大的泛化能力。然而，现有主流方法通常将本体感觉信号与VLM编码的视觉-语言特征直接融合，这导致了“状态主导偏差”。具体表现为，即使在出现明显的视觉执行失败线索时，策略仍会过度依赖内部状态（如预设的动作轨迹）而忽视视觉证据，从而产生“虚假完成”现象。例如，当目标物体在执行过程中掉落时，策略仍会继续执行预定的放置动作，而非根据视觉反馈去重新拾取物体。\n\n本文针对VLA模型中因模态不平衡而导致的“虚假完成”这一具体痛点，提出了通过“视觉-本体感觉再平衡”来增强视觉基础与鲁棒性的新视角。其核心思路是：引入一个基于外部VLM的任务阶段观察器，实时提取任务中心的视觉线索作为环境先验，并通过特征层面的调制机制，自适应地增强策略对视觉证据的利用，从而纠正状态主导偏差。\n\n## 方法详解\nReViP框架旨在通过再平衡视觉与本体感觉的模态影响，将策略决策从虚假完成转向真实完成。整体框架包含两个核心阶段：任务阶段观察器和任务阶段增强器。\n\n![方法框架](https://arxiv.org/html/2601.16667v1/x1.png)\n\n> **图2**：ReViP框架总览。上方为**任务阶段观察器**，用于从观测和指令中提取任务中心视觉线索；下方为**任务阶段增强器**，通过特征调制实现再平衡。外部VLM（Qwen 2.5）提取的线索被注入VLA骨干网络，以自适应地重新平衡视觉流和本体感觉流。\n\n**整体流程**：在每一步，策略接收多视角视觉观测 $I_t$、本体感觉状态 $S_t$ 和语言指令 $l$。首先，任务阶段观察器处理 $(I_t, l)$，生成紧凑的、任务中心的连续特征向量 $z_t$。然后，任务阶段增强器将 $z_t$ 转换为调制参数，通过特征级线性调制作用于VLA骨干网络的视觉-语言前缀令牌。最后，调制后的特征与 $S_t$ 融合，输入基于流匹配的动作解码器，预测未来的动作块。\n\n**核心模块1：任务阶段观察器**。该模块旨在解决视觉信息利用不足的问题。它使用一个冻结的大型视觉-语言模型，对当前观测和指令进行目标接地分析：识别机器人当前可见的物理状态、任务相关物体的空间位置和状态，并推断出与给定目标一致的当前阶段意图。例如，在物体掉落场景中，TSO会输出“奶油芝士未被机械臂抓取”等关键证据。这些离散的语言线索通过一个LLM嵌入提取策略（汇聚最后一层隐藏状态并线性投影）被转化为连续特征 $z_t \\in \\mathbb{R}^d$，以便注入VLA骨干网络。\n\n**核心模块2：任务阶段增强器**。该模块负责将TSO提取的线索 $z_t$ 注入策略，并在特征层面重新平衡视觉与本体感觉的影响。其核心是**任务阶段特征级线性调制**机制。首先，通过一个紧凑的瓶颈映射 $h(\\cdot)$ 将 $z_t$ 转换为调制参数：$[\\gamma_t, \\beta_t] = h(z_t)$。然后，将这些参数应用于VLA骨干网络的视觉-语言前缀令牌 $P_t$，生成调制后的前缀 $\\tilde{P}_t$：\n$$\\tilde{P}_t = \\Big(P_t + \\alpha\\big(\\gamma_t \\odot P_t + \\beta_t\\big)\\Big) \\odot M_t$$\n其中 $\\alpha$ 是一个可学习的调制因子，$\\odot$ 表示逐令牌的哈达玛积，$M_t$ 是有效性掩码。此操作在特征层面注入任务中心视觉线索，有效放大与视觉证据对齐的通道，同时抑制导致状态主导偏差的干扰信息。\n\n**动作预测与训练**：调制后的前缀 $\\tilde{P}_t$ 与本体感觉 $S_t$ 融合后，输入动作头。动作头通过流匹配进行训练，其目标是回归一个条件速度场。给定真实动作块 $A_t$ 和随机噪声 $\\varepsilon$，沿直线路径构建噪声插值 $v_\\tau = (1-\\tau)A_t + \\tau \\varepsilon$，目标速度 $u_\\tau = \\varepsilon - A_t$。训练损失为预测速度与目标速度之间的L2范数：$\\mathcal{L}_{\\mathrm{FM}} = \\| v_\\theta(v_\\tau, \\tau | \\tilde{F}_t) - u_\\tau \\|_2^2$。\n\n**创新点**：与现有方法相比，ReViP的创新主要体现在两点：1) **引入外部VLM作为实时任务阶段观察器**，提供细粒度的、任务相关的环境语义反馈，而非仅用于任务规划或成功判定。2) **提出TS-FiLM调制机制**，将上述反馈以自适应、特征级的方式注入策略决策过程，实现了视觉与本体感觉信号的动态再平衡，直接针对虚假完成的根源。\n\n## 实验与结果\n实验使用了多个基准进行验证：1) **虚假完成基准套件**：基于LIBERO平台构建，包含物体掉落、干扰物交换、场景重布局三类共8个任务的受控扰动设置。2) **通用仿真基准**：标准LIBERO基准（含Spatial, Object, Goal, 10四个任务套件）和双臂RoboTwin 2.0基准。3) **真实世界实验**。\n\n对比的基线方法包括：OpenVLA、OpenVLA-OFT、SpatialVLA、$\\pi_0$、$\\pi_0$-Fast、CoT-VLA、TriVLA、UniVLA、DP3、RDT等代表性VLA模型。\n\n![基准套件说明](https://arxiv.org/html/2601.16667v1/Figures/FC.png)\n> **图3**：虚假完成基准套件示意图。包含三类互补的扰动源：物体掉落评估策略检测执行中失败的能力；干扰物交换检验在视觉相似物体下的实例级接地能力；重布局条件测试当目标和目标区域出现在新配置时的空间推理能力。\n\n**关键实验结果（虚假完成基准）**：如表1所示，ReViP在虚假完成基准上取得了62%的平均成功率，排名第一，显著优于基线。例如，在物体掉落任务上平均成功率达65.2%；在干扰物交换任务上比$\\pi_0$和$\\pi_0$-Fast分别高出15%和40%；在重布局任务上达到88%的成功率。这表明ReViP有效降低了虚假完成率。\n\n![定性对比](https://arxiv.org/html/2601.16667v1/Figures/vs2.png)\n> **图4**：在物体掉落设置下的定性比较（上：仿真，下：真实世界）。ReViP能检测到物体掉落并成功重新拾取目标，实现真实完成；而$\\pi_0$则无视视觉失败，继续执行状态主导的轨迹，导致虚假完成。\n\n**关键实验结果（通用仿真基准）**：在标准LIBERO基准上（表2），ReViP取得了96.7%的平均成功率，在所有套件上均排名第一。特别是在具有挑战性的LIBERO-10套件上，将$\\pi_0$的85.2%提升至92.2%。在双臂RoboTwin 2.0基准的困难模式下（表3），ReViP以14%的平均成功率优于其他方法。\n\n**关键实验结果（真实世界）**：在包含物体掉落和抽屉操作的真实任务中（表4），ReViP取得了60%的平均成功率，远高于$\\pi_0$的34%和$\\pi_0$-Fast的18%。\n\n**消融实验**：论文通过消融实验验证了各组件贡献。移除任务阶段观察器或任务阶段增强器（TS-FiLM）都会导致性能显著下降，尤其是在物体掉落和干扰物交换任务上。这证明了外部VLM提供的任务阶段线索和特征调制机制对于实现视觉-本体感觉再平衡都是不可或缺的。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) **识别并形式化了VLA模型中的“虚假完成”失败模式**，将其根源归结为视觉与本体感觉的模态不平衡导致的状态主导偏差。2) **提出了ReViP框架**，通过任务阶段观察器提取实时视觉线索，并利用任务阶段增强器进行特征级调制，实现了两种模态的自适应再平衡。3) **构建了首个针对虚假完成现象的评测基准套件**，为社区提供了系统的评估工具。\n\n论文提到的局限性主要在于**计算开销**：使用大型外部VLM（Qwen2.5-VL 72B）作为观察器会引入额外的推理成本。作者指出，未来可探索更高效的VLM或知识蒸馏来缓解此问题。\n\n本文对后续研究的启示在于：1) 在构建具身智能系统时，**需要审慎处理多模态信号的融合方式**，避免某种模态过度主导决策。2) **利用外部大模型提供丰富、实时的环境语义反馈**，是增强策略对动态环境适应性和鲁棒性的有效途径。3) **构建针对特定失败模式的诊断性基准**，能更精准地驱动模型在关键能力上的进步。",
      "imageUrls": [
        "https://arxiv.org/html/2601.16667v1/x1.png",
        "https://arxiv.org/html/2601.16667v1/Figures/FC/Butter.png",
        "https://arxiv.org/html/2601.16667v1/Figures/FC/Cheese.png",
        "https://arxiv.org/html/2601.16667v1/Figures/FC/Bottle.png",
        "https://arxiv.org/html/2601.16667v1/Figures/FC/Carton.png",
        "https://arxiv.org/html/2601.16667v1/Figures/FC/Bowl.png",
        "https://arxiv.org/html/2601.16667v1/Figures/FC/Butter.png",
        "https://arxiv.org/html/2601.16667v1/Figures/FC/Cheese.png",
        "https://arxiv.org/html/2601.16667v1/Figures/FC/Bowl_Plane.png",
        "https://arxiv.org/html/2601.16667v1/Figures/FC.png",
        "https://arxiv.org/html/2601.16667v1/Figures/vs2.png",
        "https://arxiv.org/html/2601.16667v1/Figures/real_world_settings.png",
        "https://arxiv.org/html/2601.16667v1/Figures/Supp_FC/FS-drop.png",
        "https://arxiv.org/html/2601.16667v1/Figures/Supp_FC/FS-swap.png",
        "https://arxiv.org/html/2601.16667v1/Figures/Supp_FC/FS-relayout.png",
        "https://arxiv.org/html/2601.16667v1/Figures/Supp_FC/Simulations.png",
        "https://arxiv.org/html/2601.16667v1/Figures/Supp_FC/real_world.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.16065",
      "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models",
      "url": "http://arxiv.org/abs/2601.16065",
      "arxivId": "2601.16065",
      "date": "2026-01-22",
      "authors": "Jingqun Tang Team",
      "category": "Manipulation",
      "summary": "本文针对视觉语言动作（VLA）模型在机器人操作任务中过度关注任务无关区域图像标记（即“干扰标记”）而导致动作生成错误、成功率下降的问题，提出了一种即插即用的干扰标记剪枝（DTP）框架。该框架的核心方法是动态检测并剪枝这些干扰图像标记，以修正模型的视觉注意力模式，且无需改变模型原始架构或增加额外输入。在SIMPLER基准测试上的实验表明，DTP能持续提升不同类型VLA模型的任务成功率，证明了其良好的泛化性；分析进一步揭示，所有测试模型的任务成功率与任务无关区域的注意力数量均呈负相关。",
      "detailedSummary": "## 研究背景与动机\n视觉-语言-动作模型通过结合视觉-语言模型的强大感知能力和语言模型的推理能力，能够理解环境并直接输出机器人动作，在机器人操作任务中取得了显著进展。然而，现有的VLA模型在默认情况下，可能会在交叉注意力机制中过度关注与任务无关的图像区域（即“干扰令牌”），这种注意力分散会妨碍模型在每个步骤中生成期望的动作令牌，从而降低任务成功率。当前一些工作试图通过修改输入图像或增加模型组件来缓解视觉噪声，但这些方法要么在输入层面操作，要么需要架构重设计和端到端训练，难以广泛适用于现有VLA系统。\n\n本文针对VLA模型中普遍存在的“注意力分散”这一具体痛点，提出了一种新的视角：在不改变模型原始架构或增加额外输入的前提下，通过一种即插即用、无需训练的后处理框架，动态检测并剪除这些干扰图像令牌，以修正模型的视觉注意力模式，探索模型在不改变其权重情况下的性能上界。本文的核心思路是：通过分析提示词与图像令牌的相关性来识别任务关键区域，同时分析模型生成动作时的注意力模式，然后基于交集策略，剪除非关键区域中注意力值过高的令牌，从而引导模型聚焦于任务相关信息。\n\n## 方法详解\nDTP框架是一个即插即用的后处理方法，包含三个顺序执行的阶段：1）基于相关性的重要区域构建；2）视觉注意力模式构建；3）基于交集的干扰令牌剪除。其输入是原始的视觉令牌序列和提示令牌，输出是经过剪枝、去除了干扰令牌的视觉令牌子集，用于后续的动作生成。\n\n![方法框架](https://arxiv.org/html/2601.16065v1/x2.png)\n> **图2**：DTP方法的详细架构。包含三个主要组件：(a) 重要区域构建：利用选定的Transformer层 C 计算图像与提示令牌之间的相关性得分，形成任务相关的重要区域 G。(b) 视觉注意力模式构建：从所有注意力层创建输出动作令牌到图像令牌的注意力热图 A，并按视觉注意力比例加权。它捕捉模型生成动作时的关注点。(c) 干扰令牌剪除：对于不重要区域中的任何图像令牌，如果其注意力值大于 τ⋅a_m，则被视为干扰令牌并被剪除。\n\n**核心模块一：重要区域构建**。该模块旨在从模型内部视角识别与用户指令（提示）最相关的图像区域。对于大多数Transformer-based VLA模型，选取中间层的注意力矩阵，计算每个提示令牌到所有视觉令牌的注意力权重，并在注意力头和选定层上取平均，得到每个视觉令牌的整体相关性热图 R。对于某些特殊架构（如UniVLA），提示令牌不关注图像令牌，则改用嵌入向量的余弦相似度来计算相关性。随后，对热图进行空间偏置（抑制角落伪影并应用高斯平滑），最终选取相关性最高的 top-k 个视觉令牌构成重要区域 G。\n\n**核心模块二：视觉注意力模式构建**。该模块旨在从模型生成视角分析其在输出每一个动作令牌时关注了哪些图像区域。对于模型生成的每个动作令牌 t_j，遍历所有层 l，提取该层中 t_j 指向所有视觉令牌的注意力权重，形成层特定热图 A_j^l。然后将每层的热图按该层总视觉注意力的比例 w^l 进行加权，并跨层求和，得到该动作令牌的最终视觉注意力模式 A_j。这反映了模型生成特定动作时对图像不同部分的关注强度。\n\n**核心模块三：干扰令牌剪除**。这是框架的决策核心。给定重要区域 G 和当前动作生成步骤的视觉注意力模式 A，设 a_m 为重要区域内所有视觉令牌的最大注意力值。对于位于不重要区域 A_u 中的每个视觉令牌 v，如果其注意力值 A_u[v] 超过了经过容忍度因子 τ 缩放后的阈值（即 A_u[v] > τ ⋅ a_m），则该令牌被判定为干扰令牌 d，并将其从当前生成步骤的视觉输入中剪除。τ 是一个可调参数，控制剪除的严格程度：τ 越小，剪除越激进；τ 越大，剪除越保守，当 τ 足够大时，不进行任何剪除。\n\n**创新点**：与需要修改输入或重新训练模型的方法相比，DTP的创新性在于其“即插即用”和“无需训练”的特性。它通过分析模型前向传播过程中的内部信号（注意力和嵌入），动态地、有针对性地移除干扰信息，而非盲目过滤或引入额外模块。这种基于“模型自身行为”进行校正的思路，使其具有良好的模型泛化能力。\n\n## 实验与结果\n**实验设置**：本文在SIMPLER Benchmark和LIBERO Benchmark上评估DTP。使用了三个先进的、架构各异的VLA模型：SpatialVLA（基于Paligemma 2的3B模型，集成3D位置编码）、Nora（基于Qwen2.5-VL-3B，使用FAST动作分词器）和UniVLA（9B的世界模型VLA）。实验平台涉及WidowX机器人和Google机器人。对于每个模型，作者固定一组全局超参数（τ, C, k, σ）用于所有任务，并未针对每个任务进行调优。\n\n**主要结果**：DTP在多个基准测试和模型上均一致地提升了任务成功率。\n在WidowX机器人任务上（表1），SpatialVLA的平均成功率从29.2%提升至37.5%（相对提升28.4%）；Nora从6.2%提升至11.5%（相对提升近2倍）；本已很强的UniVLA也从68.7%提升至74.0%（相对提升7.7%）。\n在Google机器人任务上（表2），提升幅度较小但依然一致，SpatialVLA和Nora均获得了1-3%的相对提升。\n在LIBERO Benchmark上（表3），DTP进一步提升了Nora在所有子任务上的性能，尤其在最具挑战性的LIBERO-10上取得了6.6%的绝对提升。\n\n![结果图表](https://arxiv.org/html/2601.16065v1/x1.png)\n> **图1**：方法概览。左侧为输入（当前视觉观察和自然语言指令）。中间对比了原始VLA模型（上）可能关注任务无关区域导致任务失败，而DTP增强的方法（下）在任务关键区域产生更集中的注意力，从而提高了任务成功率。\n\n![性能上界探索](https://arxiv.org/html/2601.16065v1/spatialvla_tolerance_success_rates.png)\n> **图4（部分）**：通过调整容忍度参数 τ 探索不同VLA模型的性能上界。虚线表示不使用DTP的基线成功率，实线表示不同 τ 值下DTP的性能。标注了峰值成功率和对应的 τ^。结果表明，通过调整 τ 可以选择最优容忍度水平，从而最大化潜在性能增益。该图证实了存在模型和任务特定的最优 τ 值。\n\n**消融实验**：为了验证DTP各组件设计的有效性，论文进行了系统的消融研究（表4，表5）。对比了三种替代策略：1）**Random_all_region**：在整个图像区域随机剪除令牌；2）**Random_unimportant_region**：仅在非重要区域随机剪除令牌；3）**No_Gaussian**：构建重要区域时不使用高斯平滑和角落抑制。实验结果表明，完整的DTP方法（Ours）在所有模型和任务上均显著优于这些变体。例如，在WidowX任务上，完整DTP使SpatialVLA达到37.5%的成功率，而随机剪除策略仅能带来15.6%至17.7%的成功率，不使用高斯平滑则与基线相同（29.2%）。这证明了**针对性识别干扰令牌**和**空间平滑处理**对于性能提升至关重要。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**干扰令牌剪除框架**，这是一种新颖的、基于交集的、即插即用的后处理方法，能够自动识别并剪除VLA模型中的干扰视觉令牌，通过解决VLA模型中常见的注意力弱点，普遍提高了任务成功率。\n2.  通过调整容忍度 τ，**探索了VLA模型在现有架构下的性能上界**，寻找符合模型偏好并能最大化可实现性能的理想视觉注意力模式。\n3.  分析了不重要区域的注意力值，揭示了其与任务成功率的**负相关关系**，为构建更鲁棒的VLA模型提供了见解。\n\n**局限性**：论文自身提到，DTP是一种后处理框架，虽然能有效提升性能，但并未从根本上改变或优化模型内部的注意力机制。它依赖于对模型中间特征的分析，其效果可能受到所选分析层（C）和超参数（τ, k）的影响。\n\n**后续研究启示**：\n1.  **注意力模式与性能强相关**：本文的发现强调了VLA模型的视觉注意力质量是其决策可靠性的关键指标。未来的模型设计或训练目标可以 explicitly 鼓励更集中、更任务相关的注意力模式。\n2.  **即插即用优化潜力**：DTP的成功表明，在不进行昂贵重训练的情况下，通过轻量级推理时干预来修正模型行为是可行的。这为部署后模型优化和适应提供了新思路。\n3.  **模型诊断工具**：DTP的分析框架（重要区域 vs. 实际注意力）本身可以作为一种诊断工具，帮助研究者理解和可视化VLA模型失败的原因，从而进行更有针对性的改进。",
      "imageUrls": [
        "https://arxiv.org/html/2601.16065v1/x1.png",
        "https://arxiv.org/html/2601.16065v1/x2.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/11.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/21.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/31.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/41.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/51.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/61.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/12.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/22.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/32.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/42.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/52.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/62.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/13.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/23.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/33.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/43.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/53.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/63.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/14.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/24.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/34.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/44.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/54.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_1/64.png",
        "https://arxiv.org/html/2601.16065v1/spatialvla_tolerance_success_rates.png",
        "https://arxiv.org/html/2601.16065v1/nora_tolerance_success_rates.png",
        "https://arxiv.org/html/2601.16065v1/univla_tolerance_success_rates.png",
        "https://arxiv.org/html/2601.16065v1/multi_abnormal_attention_boxplot.png",
        "https://arxiv.org/html/2601.16065v1/Nora_abnormal.png",
        "https://arxiv.org/html/2601.16065v1/spatialvla_abnormal.png",
        "https://arxiv.org/html/2601.16065v1/univla_abnormal.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/11.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/21.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/31.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/41.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/51.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/61.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/12.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/22.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/32.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/42.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/52.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/62.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/13.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/23.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/33.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/43.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/53.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/63.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/14.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/24.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/34.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/44.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/54.png",
        "https://arxiv.org/html/2601.16065v1/figure_image_2/64.png",
        "https://arxiv.org/html/2601.16065v1/layer_attention_spatialvla/11.png",
        "https://arxiv.org/html/2601.16065v1/layer_attention_spatialvla/12.png",
        "https://arxiv.org/html/2601.16065v1/layer_attention_spatialvla/31.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.15541",
      "title": "CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation",
      "url": "http://arxiv.org/abs/2601.15541",
      "arxivId": "2601.15541",
      "date": "2026-01-21",
      "authors": "Arash Ajoudani Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉-语言-动作模型在接触密集型操作任务中缺乏力感知与调节能力，导致操作不安全或失败的问题，提出CompliantVLA-adaptor方法。该方法利用视觉语言模型解析图像与语言指令以理解任务上下文，进而自适应地调整可变阻抗控制器的刚度与阻尼参数，并结合实时力/力矩反馈确保交互力处于安全阈值。实验表明，该方法在模拟与真实硬件的一系列复杂接触任务上均优于基线VLA模型，整体任务成功率从9.86%提升至17.29%。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型（如RDT、Pi0、OpenVLA-oft）通过大规模预训练获得了强大的语义理解和动作生成能力，已成为机器人操作的主流方法。然而，这些模型本质上基于位置或轨迹控制，将机器人视为刚性的位置跟踪系统，缺乏对接触式任务中物理交互动力学的考量。这导致在执行涉及接触、顺应性或不确定性的物理任务时，由于缺乏力感知和适应能力，容易产生不安全的交互或任务失败。现有VLA模型在考虑接触力阈值（如<30N）时，在接触式任务中的性能受到严重限制，安全问题凸显。\n\n本文针对VLA模型在物理交互中“语义理解”与“力感知控制”脱节的核心痛点，提出了一种新视角：利用视觉语言模型（VLM）的语义理解能力，为可变阻抗控制（VIC）生成情境感知的阻抗参数，从而为VLA模型赋予顺应性物理交互能力。本文的核心思路是：在保留VLA模型泛化优势的同时，通过一个即插即用的适配器（CompliantVLA-adaptor），利用VLM将高层语义理解转化为底层控制参数（刚度、阻尼），并结合实时力反馈进行调节，以实现安全、自适应的接触式操作。\n\n## 方法详解\nCompliantVLA-adaptor的整体框架旨在增强VLA模型，为其配备VLM增强的情境感知可变阻抗控制。系统包含三个关键组件：1) 混合VLA-VIC控制架构；2) 基于VLM的、从视觉-语言情境生成阻抗参数的模块；3) 实时力调节安全层。\n\n![方法总览图](https://arxiv.org/html/2601.15541v1/figs/overview.png)\n> **图2**：CompliantVLA-adaptor系统概述。VLM处理视觉观测、语言指令和实时力反馈ℱ，生成情境感知的阻抗参数𝒦, 𝒟。这些参数调制一个可变阻抗控制器（VIC），该控制器执行由VLA模型生成的动作，确保安全、自适应的接触式操作。\n\n**整体流程**：给定任务的语言指令𝒯、手腕RGB图像𝐈_w、全局视野图像𝐈_f以及外部力/力矩反馈ℱ，系统首先通过VLM识别当前任务执行阶段（如自由运动、接近、接触、撤回）。然后，VLM结合阶段信息、视觉情境和力反馈，推理生成各向异性的刚度矩阵𝒦和阻尼矩阵𝒟参数。这些参数与VLA模型生成的期望末端执行器位移动作𝐱_d一同输入给VIC控制器，最终生成安全的关节力矩命令。系统运行在三个时间尺度：VLM阻抗生成（~1 Hz）、VLA动作块生成（~3 Hz）和底层VIC控制（1000 Hz）。\n\n**核心模块与技术细节**：\n1.  **接触阶段识别**：纯视觉的阶段检测存在局限，因此采用混合方法，结合VLM的视觉理解与力传感器反馈。通过设计特定的提示词（Prompt），让VLM能根据任务描述、力测量值和预定义的阶段列表，区分“自由运动”、“接近”、“接触”、“撤回”四个阶段。这为后续阻抗参数的层次化调整提供了语义依据。\n2.  **多模态信息驱动的阻抗参数生成**：这是方法的核心创新。VLM根据注入物理情境的提示词，综合任务描述、当前阶段、速度、力测量值以及预设的阻抗范围，推理生成最优的各向异性阻抗参数。其策略包括：**阶段分层**（如接触阶段刚度最低，自由运动阶段刚度最高）和**运动方向适应**（沿预期运动方向降低阻抗，在约束轴向上提高阻抗以保持对齐）。最终馈入VIC控制器的参数还会根据实时力反馈进行缩放：𝐊_p^final = 𝐊_p^VLM ⋅ α_force，其中α_force ∈ [0.2, 1]是一个基于力的缩放因子，当测量力超过安全阈值时降低刚度，确保安全。\n3.  **混合VLA-VIC控制架构**：该架构无缝集成了VLA的动作生成和VIC的顺应性执行。它保持了VLA的泛化优势，同时通过VIC增加了顺应性安全层。阻尼参数𝐃_p^final根据生成的刚度和有效质量计算，以确临界阻尼。\n\n**与现有方法的创新点**：\n-   **语义到阻抗的零样本映射**：不同于需要手动调参、任务特定调度或大量演示数据的传统VIC方法，本文利用预训练VLM的常识和推理能力，实现了从视觉-语言情境到阻抗参数的零样本生成，具备处理新物体和新场景的泛化能力。\n-   **多模态融合与实时安全层**：将VLM的语义理解与实时力/力矩反馈深度融合，不仅基于情境“预测”合适的阻抗，还能根据实际交互力“反应性”地调节阻抗，构成了预测与反应相结合的双重安全保障。\n\n## 实验与结果\n**实验设置**：\n-   **基准与数据集**：仿真实验在LIBERO和ManiSkill基准的8个代表性接触式任务上进行（如插销插入、关闭抽屉、将杯子放入微波炉等，详见论文表I）。真实实验使用7自由度Franka Emika Panda机械臂。\n-   **对比基线**：与三种先进的VLA模型对比：Pi0（流匹配）、RDT-1B（扩散模型）和OpenVLA-oft。\n-   **安全准则**：为所有任务设定统一的接触力阈值30N。若连续三次超过阈值，则任务终止并记为失败。\n-   **协议**：分两阶段评估：1) 基线VLA模型使用其默认位置控制；2) 将基线模型与CompliantVLA-adaptor集成，使用VLM增强的VIC控制器。\n\n**关键实验结果**：\n在仿真中，CompliantVLA-adaptor在大多数接触式操作任务（7/8）中取得了显著更高的任务成功率和更少的力违规。基线VLA模型在考虑30N力阈值时，性能极不稳定，部分任务成功率为0%。集成适配器后，任务性能得到一致提升。\n\n![仿真任务成功率对比](https://arxiv.org/html/2601.15541v1/x3.png)\n> **图4**：在30N接触力约束下，8个仿真任务的执行成功率对比。每个子图标题中的“T”后数字代表任务编号（对应表I），“-R”、“-P”、“-O”后缀分别代表使用RDT、Pi0、OpenVLA-oft作为底层VLA模型。不同颜色表示是否使用CompliantVLA-adaptor。结果表明，适配器在绝大多数任务和模型上带来了成功率提升。\n\n**具体数值**：所有任务的平均成功率从基线模型的9.86%提升至使用CompliantVLA-adaptor后的17.29%。涉及机械约束（如抽屉、旋钮）的任务改善最为显著。基线模型的最高成功率仅为54%，而使用适配器后最高达到76%。\n\n**定性结果与消融分析**：\n真实世界实验进一步验证了方法的有效性。下图展示了在“推盘子”任务中，VLM根据识别出的不同阶段自适应调整阻抗参数（刚度）的过程。\n\n![自由运动阶段](https://arxiv.org/html/2601.15541v1/figs/push_freemotion.jpg)\n> **图5**：自由运动阶段，VLM建议高刚度以实现精确的位置控制。\n\n![接近阶段](https://arxiv.org/html/2601.15541v1/figs/push_approaching_.jpg)\n> **图6**：接近阶段，VLM建议中等刚度，为即将发生的接触做准备。\n\n![接触阶段](https://arxiv.org/html/2601.15541v1/figs/push_contact.jpg)\n> **图7**：接触阶段，VLM建议低刚度（特别是沿运动方向），以在施加推力的同时保持顺应性，避免过大接触力。\n\n![处理重物](https://arxiv.org/html/2601.15541v1/figs/heavy_dumbbel.jpg)\n> **图8**：真实世界任务展示：将重哑铃放入纸箱。VLM根据“重物”的语义和力反馈，生成了较低的阻抗参数，实现了安全放置。\n\n![消融实验](https://arxiv.org/html/2601.15541v1/x4.png)\n> **图9**：消融实验展示了不同组件对任务成功率的贡献。其中“w/ VLM+VIC”是完整方法，“w/ fixed Impedance”使用固定阻抗，“w/o force feedback”关闭力反馈调节。结果显示，完整的VLM语义指导结合力反馈调节（即完整方法）取得了最佳性能，证明了各核心组件的必要性。\n\n**消融实验总结**：消融研究表明，完整的VLM引导（提供情境感知参数）与力反馈调节（提供实时安全保证）相结合至关重要。仅使用固定阻抗或仅使用VLM而不结合力反馈，性能均有下降，验证了方法中预测性语义推理与反应性力调节协同工作的价值。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**CompliantVLA-adaptor框架**，首次将VLM的语义理解能力用于引导VIC，为VLA模型赋予了安全、顺应性的物理交互能力，弥合了高层任务理解与底层力安全执行之间的鸿沟。\n2.  实现了**从多模态情境到阻抗参数的零样本映射**，利用预训练VLM的常识，无需针对具体任务进行训练，即可根据视觉、语言和力反馈生成各向异性的适配阻抗。\n3.  开发了**混合VLA-VIC控制架构**，这是一个即插即用的模块化解决方案，在保留现有VLA模型泛化优势的同时，通过引入实时力调节安全层，显著提升了接触式操作的成功率和安全性。\n\n**局限性**：论文提到，VLM推理存在延迟（~1 Hz），可能无法应对极高速的接触事件。此外，基于力反馈的调节策略可能在某些需要较大力的任务中显得保守。\n\n**研究启示**：本文为基于大模型的机器人系统安全部署提供了一个重要方向。它表明，将大型基础模型（如VLM）的语义推理能力与经典控制理论（如VIC）相结合，是构建既能理解复杂指令又能安全进行物理交互的下一代机器人的有效途径。未来工作可探索更高效的VLM推理、更精细的力-语义融合策略，以及将此框架扩展到更动态的双臂或移动操作场景中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.15541v1/x1.png",
        "https://arxiv.org/html/2601.15541v1/figs/overview.png",
        "https://arxiv.org/html/2601.15541v1/x2.png",
        "https://arxiv.org/html/2601.15541v1/x3.png",
        "https://arxiv.org/html/2601.15541v1/figs/push_freemotion.jpg",
        "https://arxiv.org/html/2601.15541v1/figs/push_approaching_.jpg",
        "https://arxiv.org/html/2601.15541v1/figs/push_contact.jpg",
        "https://arxiv.org/html/2601.15541v1/figs/heavy_dumbbel.jpg",
        "https://arxiv.org/html/2601.15541v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.15761",
      "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
      "url": "http://arxiv.org/abs/2601.15761",
      "arxivId": "2601.15761",
      "date": "2026-01-22",
      "authors": "Shu Zhang Team",
      "category": "Manipulation",
      "summary": "本文针对现实世界机器人强化学习中数据成本高、训练不稳定的问题，提出SigEnt-SAC方法。其关键技术是sigmoid有界熵，通过将策略的逐维度信息量经sigmoid映射为有界熵信号，防止因负熵主导而导致策略优化偏离分布，并稳定Q函数。实验表明，该方法在D4RL基准上显著减轻了Q值振荡，能更快达到100%成功率；在真实机器人任务中，仅需少量交互即可从原始图像和稀疏奖励中学习成功策略。",
      "detailedSummary": "## 研究背景与动机\n当前，在现实世界中部署强化学习面临样本效率低、奖励稀疏和视觉观测噪声大等挑战。主流方法主要依赖两类先验知识：一是利用大规模预收集数据集进行离线到在线学习（如IQL、CQL、AWAC），以降低探索成本并稳定训练；二是借助人类在环干预或基于大规模预训练的视觉-语言-动作模型进行微调。然而，这些方法存在关键局限性：离线到在线方法通常需要大量高质量数据集来克服分布偏移，数据获取成本高昂；而基于大模型的方法则依赖大规模预训练和微调，且其可靠性（如幻觉、对齐问题）和在高动态、噪声环境下的鲁棒性尚未得到充分验证。因此，一种数据需求极低、无需大规模预训练的低成本现实世界RL方法仍然缺失。\n\n本文针对上述痛点，提出了一种新的视角：能否仅使用单条专家轨迹，实现从零开始的样本高效学习？本文的核心思路是提出SigEnt-SAC算法，通过引入Sigmoid有界熵机制防止Q函数振荡和策略探索到分布外动作，并结合门控行为克隆利用单条专家轨迹稳定策略优化，从而实现快速收敛和高效学习。\n\n## 方法详解\nSigEnt-SAC框架建立在两个核心机制之上：Sigmoid有界熵和门控行为克隆。整体流程遵循标准的离线策略演员-评论家框架，但修改了评论家更新中的熵项以及策略更新目标，并利用一个专家数据缓冲区提供引导。\n\n**核心模块一：Sigmoid有界熵机制**\n该机制旨在解决标准软演员-评论家中，负熵项可能导致Q函数优化偏向分布外动作并引发振荡的问题。对于tanh压扁的高斯策略 $a = \\tanh(x)$，其中 $x$ 通过重参数化从策略网络输出采样得到。定义每个维度的惊奇值（负对数密度）为 $s_i = -\\log\\pi_{\\theta,i}(a_i|s)$。有界熵贡献通过一个温度控制的sigmoid函数计算：\n$$h_{i}(s_{i})=h_{\\max}\\cdot\\sigma\\!\\left(\\frac{s_{i}-m}{t}\\right), \\quad \\mathcal{H}_{\\text{sig}}(s,a)=\\sum_{i=1}^{d}h_{i}(s_{i})$$\n其中 $\\sigma(\\cdot)$ 是sigmoid函数，$h_{\\max}>0$ 控制最大每维度熵得分，$m$ 是中心偏移，$t>0$ 是温度参数。由此构造的 $\\mathcal{H}_{\\text{sig}}(s,a) \\in (0, d\\cdot h_{\\max})$ 是一个严格有界的正值。在评论家更新中，用 $\\mathcal{H}_{\\text{sig}}$ 替换标准的负熵项 $- \\log \\pi(a|s)$，更新规则如论文公式(2)所示。\n\n![负熵与有界熵对Q函数的影响对比](https://arxiv.org/html/2601.15761v1/figures/draw_entropy_concept_4_compare_Q_H_Z.png)\n> **图1**：负熵效应与Sigmoid有界熵效果的示意图。上排：默认的负熵项（第1行第2列）降低了策略动作及其邻域的Q值估计，使得最大化Q的策略改进更可能移向分布外动作。下排：Sigmoid有界熵将熵贡献映射到一个严格有界的正值范围，产生了一个更清晰的高Q值区域和一个更明确的用于最大化的动作集。\n\n**核心模块二：门控行为克隆**\n为了在Q函数尚未收敛或振荡时稳定策略优化，并减少无效探索，本文在策略更新中引入了一个基于专家数据的门控行为克隆损失。定义策略的确定性平均动作为 $a_{\\mathrm{mean}}(s) = \\tanh(\\mu_{\\theta}(s))$，门控掩码为：\n$$p_{\\mathrm{gate}}(s,a_{\\mathrm{exp}}) = \\mathbb{I}\\!\\left[\\|a_{\\mathrm{mean}}(s)-a_{\\mathrm{exp}}\\|_{2} > \\varepsilon\\right]$$\n其中 $\\varepsilon > 0$ 是阈值。只有当策略平均动作与专家动作的偏差超过该阈值时，行为克隆损失才会被激活。最终的策略优化目标是最大化以下统一目标 $\\mathcal{J}(\\pi_{\\theta})$（论文公式(4)），它结合了熵增强的Q值期望、Sigmoid有界熵项以及门控的专家塑造项（负的均方误差）。\n\n![策略优化与OOD动作关系](https://arxiv.org/html/2601.15761v1/x1.png)\n> **图2**：使用负熵项的策略优化更可能产生OOD动作；相应地，较小的目标策略熵会导致较高的OOD动作比率。OOD标准由公式(3)定义，阈值设为0.3。图中还对Cal-QL应用了LayerNorm以减轻网络振荡引起的虚假OOD动作。\n\n**算法实现细节**\n评论家损失函数结合了标准的时序差分误差和一个简化的CQL风格正则化项（论文公式(7)），用于抑制在采样OOD动作上的虚假高Q值。最终评论家目标为 $\\mathcal{L}_{Q_i} = \\mathcal{L}_{\\mathrm{TD}}^{(i)} + \\lambda_{\\mathrm{ood}}\\,\\mathcal{L}_{\\mathrm{CQL}}^{(i)}$。策略损失则是统一目标 $\\mathcal{J}(\\pi_{\\theta})$ 的负值（论文公式(9)），在实现中，门控行为克隆项仅在每样本偏差 $ \\frac{1}{d}\\|a_{\\mathrm{mean}}(s)-a_{\\mathrm{exp}}\\|_{2}^{2} > \\varepsilon_{\\mathrm{bc}}^{2} $ 时激活。\n\n**创新点总结**\n与现有方法相比，SigEnt-SAC的核心创新在于：1) **Sigmoid有界熵**：将无界的负熵信号转换为有界正值，从根本上防止了因熵项主导而导致的Q值振荡和OOD动作探索问题。2) **门控行为克隆**：动态地、有条件地利用单条专家轨迹进行策略塑造，既能在策略偏离时提供稳定梯度，又避免了无条件行为克隆对探索和超越专家性能的限制。\n\n## 实验与结果\n**实验设置**：本文在两类基准上评估SigEnt-SAC：1) **D4RL连续控制任务**：包括复杂的操作任务（Kitchen, Adroit），仅提供单条成功轨迹。2) **真实世界机器人任务**：设计了四个任务（见图3），涉及不同形态的机器人（轮式、机械臂、四足、人形），均使用稀疏奖励和单目灰度图像观测。\n\n![真实世界任务套件说明](https://arxiv.org/html/2601.15761v1/x2.png)\n> **图3**：真实世界任务概述。左：任务总览。中：统一的策略输入，由单视角灰度局部观测组成。右：每个任务的随机化设置。\n\n**对比基线**：包括保守Q学习方法（Cal-QL, CQL）、高效的在线微调方法（RLPD）、Q值加权行为克隆（AWAC）、隐式Q学习（IQL）以及用专家数据初始化的标准SAC。\n\n**关键实验结果**：\n1.  **单次演示下的高效学习**：在D4RL任务的单次演示设置中，SigEnt-SAC比所有基线方法更快达到100%评估成功率，且收敛后性能稳定，无下降。\n![单次设置下的在线学习曲线](https://arxiv.org/html/2601.15761v1/x3.png)\n> **图4**：单次演示设置下的在线学习曲线。SigEnt-SAC相比其他基线获得了更高的成功率、更快的收敛速度，且收敛后无性能下降。\n\n2.  **对噪声演示的鲁棒性**：在演示轨迹存在数据丢弃、动作噪声或状态噪声的情况下，SigEnt-SAC相比Cal-QL表现出更强的鲁棒性，性能下降更小。\n![对演示质量下降的鲁棒性](https://arxiv.org/html/2601.15761v1/x5.png)\n> **图6**：在三种演示质量下降情况下的学习性能。SigEnt-SAC对噪声演示 consistently 鲁棒，而Cal-QL性能显著下降。\n\n3.  **真实世界任务性能**：在四个真实世界任务上，SigEnt-SAC仅凭单条演示实现了100%的成功率，显著优于使用30条演示训练的行为克隆模型和零样本VLM智能体（见表2）。此外，学习到的策略平均减少了40.9%的任务完成步数，超越了原始演示的性能。\n![真实世界定性比较与效率提升](https://arxiv.org/html/2601.15761v1/x4.png)\n> **图5**：(a) Ball-to-Goal任务的定性比较，学习策略比演示更高效。(b) 演示与学习策略完成各任务所需的步数对比，学习策略平均步数减少40.9%。\n\n4.  **超参数敏感性分析**：对门控行为克隆的关键超参数（权重 $\\lambda$ 和阈值 $\\epsilon$）的分析表明，SigEnt-SAC在一个较宽的参数范围内保持稳定，易于调优。\n![超参数敏感性分析](https://arxiv.org/html/2601.15761v1/x6.png)\n> **图7**：对门控行为克隆的权重 $\\lambda$ 和阈值 $\\epsilon$ 的敏感性分析。方法展现了一个宽广的稳定区域。\n\n**消融实验总结**：虽然论文未进行严格的模块消融实验，但通过图1、2的理论可视化以及图4、6与基线方法的对比，充分论证了Sigmoid有界熵在稳定Q值估计和减少OOD探索方面的作用，以及结合专家引导（门控行为克隆）在单次演示设置下对加速收敛和提升鲁棒性的贡献。\n\n## 总结与启发\n**核心贡献**：1) 提出了SigEnt-SAC，一个仅需单条专家轨迹即可从零开始高效学习的离线策略演员-评论家框架。2) 设计了Sigmoid有界熵机制，有效缓解了保守Q学习中的Q值振荡问题，并防止策略优化至分布外动作。3) 在多个真实世界机器人平台和任务上验证了方法的有效性，展示了其在单目视觉、动态环境下的鲁棒性以及跨不同机器人形态的泛化能力。\n\n**局限性**：论文自身提到，引入门控行为克隆项会增加每步训练的计算开销（见表1）。此外，方法性能对门控行为克隆的超参数（如阈值 $\\epsilon$ 和权重 $\\lambda$）有一定依赖性，尽管其稳定区域较宽。\n\n**后续研究启示**：本文为低成本现实世界强化学习指明了一条可行路径，即通过精心设计的学习信号稳定机制（如有界熵）和极少量先验知识（如单条轨迹）的有效利用，可以大幅降低数据需求。未来工作可探索如何进一步自动化超参数选择，或将有界熵的思想与其他稳定RL训练的技术（如价值函数集成、归一化等）更深度地结合，以应对更复杂的多任务或非稳态环境。",
      "imageUrls": [
        "https://arxiv.org/html/2601.15761v1/figures/draw_entropy_concept_4_compare_Q_H_Z.png",
        "https://arxiv.org/html/2601.15761v1/x1.png",
        "https://arxiv.org/html/2601.15761v1/x2.png",
        "https://arxiv.org/html/2601.15761v1/x3.png",
        "https://arxiv.org/html/2601.15761v1/x4.png",
        "https://arxiv.org/html/2601.15761v1/x5.png",
        "https://arxiv.org/html/2601.15761v1/x6.png",
        "https://arxiv.org/html/2601.15761v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.13042",
      "title": "Static Is Not Enough: A Comparative Study of VR and SpaceMouse in Static and Dynamic Teleoperation Tasks",
      "url": "http://arxiv.org/abs/2601.13042",
      "arxivId": "2601.13042",
      "date": "2026-01-19",
      "authors": "Kim Baraka Team",
      "category": "Manipulation",
      "summary": "本文针对远程操作接口评估主要关注静态任务、缺乏动态任务研究的问题，比较了VR控制器与SpaceMouse在静态与动态两类任务中的表现。研究采用组内实验设计，评估了成功率、任务时长、累积成功率及用户主观负荷。核心结论表明，VR接口在动态任务中优势显著：成功率更高，所有任务的成功执行时间更短，且用户工作负荷显著更低、可用性更高。为此，作者开源了其VR接口以填补系统空白。",
      "detailedSummary": "## 研究背景与动机\n模仿学习依赖于高质量的人类演示数据，而遥操作是收集此类数据的主要方式。因此，遥操作接口的选择至关重要，它直接影响操作者如何表达动作，从而影响数据的质量。现有的演示数据涵盖静态和动态两类任务。静态任务涉及离散、分段式的动作（如抓放、到达指定点），而动态任务则需要具有时间敏感性的、反应式的连续控制（如在平底锅中翻转物体）。这两类任务对人类控制提出了根本不同的要求，在静态任务中表现优异的接口未必适用于动态任务。然而，先前对遥操作接口的评估主要集中在静态任务上，这使得现有结论无法直接推广到动态任务场景，无法明确VR控制器和SpaceMouse等常用接口在动态任务中的支持能力。本文旨在填补这一空白，通过一项受试者内研究，系统比较VR控制器（仅使用其6自由度位姿跟踪功能）和SpaceMouse在静态与动态任务中的表现。核心思路是：通过设计包含静态与动态任务组的用户实验，量化评估两种接口在任务性能与用户体验上的差异，并开源一个适用于动态任务的VR遥操作接口以供后续研究。\n\n## 方法详解\n本文的核心是比较研究，方法部分详细介绍了为实现公平比较所设计的VR遥操作接口的工作流程，以及实验的整体设置。\n\n对于VR遥操作，作者使用Oculus Quest 3控制器进行6-DoF位姿跟踪（头显仅用于捕获动作和按钮输入）。手部运动通过位姿跟踪映射到机器人末端执行器命令。在每个控制周期，控制器位姿被转换到机器人坐标系，得到手部旋转矩阵 \\(R^{hand}_t\\) 和位置 \\(P^{hand}_t\\)。当用户按下任意按钮时，遥操作被激活，系统记录当前手部位姿和机器人末端执行器位姿作为参考。末端执行器的运动由手部的增量运动生成：位置命令通过帧间位移计算 \\(a^{pos}_t = \\alpha (P^{hand}_t - P^{hand}_{t-1})\\)；旋转命令通过相对手部旋转计算 \\(a^{rot}_t = \\beta \\cdot \\mathrm{Log}(R^{hand}_t (R^{hand}_{ref})^{-1})\\)，并通过四元数更新保证数值稳定性。系统包含一个校准步骤以预防VR漂移：按下扳机键可重置手部参考位姿及其对应的机器人参考位姿，使用户能随时重新调整手部位置而不影响机器人当前位姿。\n\n![VR工作流程](https://arxiv.org/html/2601.13042v1/img/vr_workflow.png)\n> **图2**：VR遥操作接口工作流程。控制器位姿被跟踪并映射为末端执行器的增量命令，按钮输入负责夹爪控制和校准。\n\nSpaceMouse接口则使用3Dconnexion SpaceMouse Compact，其六轴速度输入通过一个缩放因子映射为末端执行器的位姿增量。\n\n与现有方法相比，本文的创新点在于将接口比较的场景从静态任务拓展到了动态任务，并针对动态任务的需求实现并开源了一个VR遥操作接口。该接口的核心是直接的位姿增量映射，相较于SpaceMouse的速度映射，能更直接地传递操作者的快速、脉冲式动作意图。\n\n## 实验与结果\n**实验设置**：研究使用7自由度Franka Emika Panda机械臂，通过panda-py中的笛卡尔阻抗控制器进行控制。参与者（N=25）在VR控制器和SpaceMouse两种条件下，完成四个测试任务。任务分为两组：静态任务（T1: 堆叠立方体；T2: 将立方体放入倾斜容器）和动态任务（T3: 在平底锅中翻转立方体；T4: 快速抽走一张纸）。实验采用受试者内设计，任务顺序进行约束平衡（静态组内随机，动态组内随机，但所有参与者先完成静态组再完成动态组）。接口顺序固定为VR先于SpaceMouse，这是出于安全考虑（VR能提供初始任务熟悉度，减少因不熟悉导致的早期中止问题）。每个任务允许最多5次尝试，每次尝试限时2分钟，成功一次即结束该任务。\n\n**评估指标**：\n1.  **用户任务性能**：成功率（成功尝试数/总尝试数）、任务时长（成功尝试的耗时，或未成功者的平均耗时）、累积成功率（在每次尝试序号时，至少成功过一次的参与者比例）。\n2.  **用户体验**：使用NASA-TLX（未加权版）评估工作负荷，使用系统可用性量表（SUS）评估感知可用性，并收集开放式反馈。\n\n**对比基线**：本研究直接比较VR控制器和SpaceMouse两种接口。\n\n**关键实验结果**：\n\n![用户任务性能](https://arxiv.org/html/2601.13042v1/img/success+duration.png)\n> **图4**：用户任务性能结果。A：各任务平均成功率；B：各任务平均完成时间；C：各任务在五次尝试内的累积成功率。VR在动态任务（T3， T4）上的成功率显著高于SpaceMouse，在所有任务上的完成时间显著更短，且更早达到成功。\n\n1.  **任务性能**：在静态任务（T1， T2）中，两种接口的成功率无显著差异（T1：VR 94.0% vs. SpaceMouse 84.0%；T2：77.7% vs. 76.7%）。在动态任务中，差异显著：T3任务VR成功率（31.2%）远高于SpaceMouse（0%， p < .001）；T4任务VR成功率（50.9%）也显著高于SpaceMouse（33.3%， p < .05）。在任务时长上，VR在所有四个任务上都显著快于SpaceMouse（T2: p < .01， 其余: p < .001）。累积成功率曲线显示，VR能使参与者在更少的尝试次数内获得成功，尤其是在动态任务上。\n2.  **用户体验**：\n\n![用户主观反馈](https://arxiv.org/html/2601.13042v1/img/sus+nasa.png)\n> **图5**：用户主观反馈结果。A：SUS可用性分数；B：NASA-TLX总分；C：NASA-TLX各维度分数。VR的可用性显著更高，工作负荷显著更低。\n\n    SUS评分显示VR的可用性（平均79.1）显著高于SpaceMouse（平均40.5， p < .001）。NASA-TLX总分显示VR的工作负荷（平均2.87）显著低于SpaceMouse（平均3.91， p < .001）。除物理需求因需手持控制器略有增加外，VR在其他五个维度（心理需求、时间需求、表现、努力、挫败感）上均得分更低。92%的参与者更偏好VR接口，认为其更直观自然。参与者反馈指出，VR的脉冲式手部动作有时会导致机器人达到速度极限，而SpaceMouse则存在控制逻辑不清晰、旋转映射难以预测等问题。\n\n**消融实验/组件贡献**：本研究未进行模块化消融实验，其核心是比较两种完整接口。结果直接揭示了**任务类型（静态 vs. 动态）** 这一变量的关键作用：在静态任务中表现相近的接口，在动态任务中展现出根本性差异，这凸显了在动态任务背景下评估接口的必要性。\n\n## 总结与启发\n**核心贡献**：\n1.  **系统性比较研究**：首次在包含静态与动态任务的场景下，对VR控制器和SpaceMouse进行了系统的用户研究，揭示了动态任务对接口评估的关键影响，并证明VR接口在动态任务中具有显著优势（更高的成功率、更快的执行速度、更低的工作负荷和更高的可用性）。\n2.  **开源工具**：发布了一个开源的VR遥操作接口，专门支持动态环境下的演示数据收集，填补了现有系统大多非开源或不适于动态任务的空白。\n\n**局限性**：论文自身提到：1）从静态到动态任务的过渡不够平滑，任务难度梯度可能影响学习和表现；2）接口顺序未进行平衡（VR始终在先），可能对SpaceMouse的结果产生有限的疲劳效应；3）未记录完整的轨迹数据，限制了对运动质量、控制策略和错误模式的深入分析。\n\n**对后续研究的启示**：\n1.  **接口选择**：为模仿学习收集演示数据时，尤其是在涉及动态操作的任务中，应优先考虑基于位姿增量的控制接口（如VR控制器），而非基于速度的接口（如SpaceMouse）。\n2.  **评估标准**：未来评估遥操作接口时，必须包含动态任务以全面反映其性能。\n3.  **未来方向**：可以设计难度梯度更平缓的动态任务序列进行更细致的比较；记录完整的末端执行器及关节轨迹数据，以支持对运动质量和错误模式的深度分析，并可用于构建动态任务数据集。",
      "imageUrls": [
        "https://arxiv.org/html/2601.13042v1/img/overview.png",
        "https://arxiv.org/html/2601.13042v1/img/vr_workflow.png",
        "https://arxiv.org/html/2601.13042v1/img/tasks.png",
        "https://arxiv.org/html/2601.13042v1/img/success+duration.png",
        "https://arxiv.org/html/2601.13042v1/img/sus+nasa.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.16677",
      "title": "Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation",
      "url": "http://arxiv.org/abs/2601.16677",
      "arxivId": "2601.16677",
      "date": "2026-01-23",
      "authors": "Álvaro Jesús López-López Team",
      "category": "Manipulation",
      "summary": "本文旨在解决深度强化学习（DRL）中因仿真与真实视觉差异导致的策略迁移难题，以实现无需真实环境再训练的零次部署。为此，论文提出一种基于**风格识别循环一致生成对抗网络（StyleID-CycleGAN）**的域适应方法，该网络将原始虚拟观测转换为具有真实风格的合成图像，从而在混合域中训练DRL智能体。实验在两个工业机器人上进行，验证了方法的有效性：智能体在仿真中成功率可达90%-100%，并在真实拾放任务中实现了**零次迁移，在大部分工作区域准确率超过95%**，且能泛化至不同颜色和形状的真实物体。",
      "detailedSummary": "## 研究背景与动机\n深度强化学习（DRL）在机器人控制领域展现出潜力，但其样本效率低下的问题阻碍了工业应用，因为在现实世界中收集大量训练数据成本高昂、耗时巨大。利用虚拟环境训练DRL智能体是一种高效的替代方案，但学习到的策略迁移到真实环境时，会受到“模拟到现实”（sim-to-real）差距的阻碍。实现零次（zero-shot）迁移，即智能体无需在真实环境中进行任何额外调优即可直接部署，因其高效性和实用价值而备受期待。\n\n目前解决sim-to-real差距的主流方法包括域随机化（DR）和域适应（DA）。域随机化通过在模拟环境中随机化大量视觉或物理属性，期望真实环境成为已见分布的一部分。然而，本文认为，对于所解决的工业设置和问题，域适应方法更为合适。域适应旨在融合虚拟域和真实域的特征，使智能体能够在这个混合域中进行有效训练。具体到视觉域适应，生成对抗网络（GAN）及其变体（如CycleGAN）常被用于图像到图像的翻译，以缩小视觉差距。但原始的CycleGAN等方法在图像翻译中常产生伪影（如颜色渗色、边缘扭曲），可能影响后续DRL策略的学习。\n\n本文针对上述痛点，提出了一种新颖的域适应方法。核心思路是：设计一个改进的CycleGAN模型（StyleID-CycleGAN, SICGAN），将原始的虚拟观测图像翻译成具有真实视觉风格的“真实-合成”图像；然后，DRL智能体完全在虚拟环境中，利用这些翻译后的图像进行训练；训练完成后，智能体可以直接部署到真实机器人上，实现零次迁移。该方法旨在提供一个高效、可扩展且考虑工业实际约束（如有限的计算资源、单一外部摄像头）的完整解决方案。\n\n## 方法详解\n本文提出的完整流程旨在实现从虚拟训练到真实部署的零次迁移。其核心是一个新颖的域适应模型——风格识别循环一致生成对抗网络（SICGAN）。\n\n![方法整体流程](https://arxiv.org/html/2601.16677v1/x1.png)\n\n> **图1**：面向工业应用的sim-to-real零次迁移通用流程。阶段1：创建虚拟环境并定义MDP。阶段2：训练SICGAN。利用训练好的SICGAN，将从原始虚拟数据生成的“真实-合成”观测用于训练DRL智能体。智能体在虚拟环境中训练和评估后，通过零次转移直接部署到真实环境。\n\n整个pipeline如图1所示，分为离线和在线两个阶段：\n1.  **离线训练阶段**：\n    *   **SICGAN训练**：首先，使用收集的未配对虚拟图像（源域）和真实图像（目标域）数据集，离线训练SICGAN模型。训练目标是学习从虚拟域到真实域的映射。\n    *   **DRL智能体训练**：训练好的SICGAN（推理模式）被用来将虚拟环境中的原始观测实时翻译成“真实-合成”图像。DRL智能体则在虚拟环境中，以这些翻译后的图像作为观测进行训练。此时，智能体感知的是具有真实视觉风格的图像，但交互的动力学仍是虚拟的。\n2.  **在线部署阶段**：\n    *   **零次迁移评估**：经过虚拟训练和评估的DRL智能体被直接部署到真实机器人上。智能体接收来自真实摄像头的原始RGB图像作为输入，并输出动作控制机器人，整个过程无需任何在真实环境中的再训练或微调。\n\n**SICGAN核心模块与创新点**：\nSICGAN是基于CycleGAN架构的改进，主要引入了两项关键增强技术以解决原始CycleGAN常见的伪影问题：\n1.  **解调卷积（Demodulated Convolutions）**：SICGAN使用了解调卷积来替代标准的批量归一化。该技术源自StyleGAN系列，首先根据输入相关的风格或特征向量对卷积权重进行调制（Modulation），使网络能适应特定图像属性；随后进行解调（Demodulation），通过通道级的归一化来稳定激活，防止信号幅度的不受控放大或抑制。这带来了更一致的特征变换，有效减少了图像翻译中的伪影。\n2.  **身份损失（Identity Loss）**：除了CycleGAN原有的对抗损失和循环一致性损失，SICGAN额外引入了身份损失 ℒ_id。该损失鼓励生成器在处理来自目标域的样本时，尽可能保持原图的关键特征。公式为：ℒ_id = 𝔼_y∼p_𝒴[‖G(y) - y‖_1] + 𝔼_x∼p_𝒳[‖F(x) - x‖_1]，其中G和F是两个方向的生成器。这有助于促进语义和颜色的一致性，提高翻译图像的保真度。\n\n与现有方法（如基于视觉Transformer的UVCGANv2）相比，SICGAN的创新点在于其专注于通过相对简洁的ResNet生成器架构，结合解调卷积和身份损失，在保持训练稳定性和图像质量的同时，不过度增加模型复杂度和计算开销，更符合工业场景对效率的要求。\n\n**DRL训练细节**：\n任务定义为机械臂接近随机放置物体的马尔可夫决策过程。智能体的观测是64×64像素的RGB图像（无本体感知信息），动作空间为机械臂末端执行器在X、Y、Z三个方向上的位移。成功标准是在训练时末端与目标物体中心距离小于5cm，评估时放宽至10cm（这是一种“过度学习”策略）。奖励函数设计为基于距离减少的稀疏奖励。\n\n## 实验与结果\n**实验设置**：\n*   **Benchmark/任务**：机械臂接近任务（抓取放置操作的第一阶段）。目标物体在工作空间内随机放置。\n*   **数据集**：采集未配对的虚拟图像（来自MuJoCo模拟器）和真实图像（来自Intel RealSense D435摄像头）用于训练SICGAN。\n*   **实验平台**：\n    *   **机器人**：主要使用ABB IRB120工业机械臂进行流程开发；使用UR3e协作机械臂进行跨平台验证。两者在形态、运动学和动力学上完全不同。\n    *   **仿真**：MuJoCo物理引擎。\n    *   **评估辅助**：使用125个ArUco标记的增强现实（AR）投影目标来高效评估智能体在工作空间各区域的性能。随后使用真实物体（红/黄/蓝乐高立方体、红白杯子）验证泛化能力。\n*   **Baseline方法**：与原始CycleGAN、以及一个基于视觉Transformer的先进域适应方法UVCGANv2进行图像翻译质量的对比。\n\n**关键实验结果**：\n1.  **图像翻译质量对比**：论文通过定性对比（如图13）展示了SICGAN在将虚拟图像翻译为真实风格图像时，相比原始CycleGAN，显著减少了颜色失真和伪影，生成了更清晰、视觉上更一致的结果。与UVCGANv2相比，SICGAN的结果在视觉质量上具有竞争力，同时模型更简单。\n\n![图像翻译对比](https://arxiv.org/html/2601.16677v1/x13.png)\n> **图13**：不同方法的图像翻译结果示例。SICGAN生成的图像在减少伪影和保持视觉一致性方面优于原始CycleGAN，与UVCGANv2结果质量相当。\n\n2.  **虚拟环境训练性能**：在虚拟环境中，使用SICGAN翻译图像训练的DRL智能体取得了优异的成功率。对于IRB120，在5cm成功阈值下，训练成功率达到接近100%。对于UR3e，在10cm评估阈值下，成功率超过90%。\n\n3.  **零次迁移现实部署性能**：\n    *   **IRB120机器人**：使用AR目标评估，智能体在工作空间大部分区域（内环、中环）的接近准确率超过95%，在外环区域也达到约90%。使用真实物体（乐高立方体、杯子）测试时，准确率均保持在95%以上，展示了良好的泛化能力。\n    ![IRB120现实结果](https://arxiv.org/html/2601.16677v1/x20.png)\n    > **图20**：IRB120机器人使用不同真实物体进行零次迁移评估的成功率。对于所有测试物体，成功率均高于95%。\n\n    *   **UR3e机器人（跨平台验证）**：同样取得了 robust 的零次迁移性能。使用AR目标评估，在内环和中环区域准确率超过95%，外环区域约为85%。对真实物体的测试成功率也普遍高于90%。\n    ![UR3e现实结果](https://arxiv.org/html/2601.16677v1/x21.png)\n    > **图21**：UR3e机器人使用不同真实物体进行零次迁移评估的成功率。对于大多数测试物体，成功率高于90%，验证了流程的可迁移性。\n\n4.  **消融实验**：论文通过消融实验验证了SICGAN中各个组件的贡献。实验表明，同时使用解调卷积和身份损失能获得最佳性能。移除身份损失会导致图像颜色保真度下降；而使用标准批量归一化代替解调卷积，则会引入明显的伪影，影响翻译质量。\n\n## 总结与启发\n**核心贡献**：\n1.  提出并成功验证了一个完整的sim-to-real零次迁移流程。该流程集成了新颖的SICGAN域适应模型和基于图像的DRL训练，并在两个完全不同的工业机械臂（ABB IRB120和UR3e）上得到了验证，证明了其可扩展性和实用性。\n2.  提出了SICGAN这一改进的CycleGAN模型。通过引入解调卷积和身份损失，有效减少了图像翻译中常见的伪影，生成了更高质量、更一致的“真实-合成”图像，为后续DRL策略的鲁棒性奠定了基础。\n3.  整个方案严格考虑了工业场景的约束（如单一外部摄像头、有限的计算资源），使得训练阶段可利用GPU加速，而部署阶段仅需推理，无需昂贵硬件，具有较高的工业应用可行性。\n\n**局限性**：\n论文自身提到的局限性包括：当前方法依赖于一个固定视角的外部摄像头。动态变化的视角或遮挡可能会对性能构成挑战。此外，实验验证的任务（接近操作）相对基础，更复杂的操作（如精确抓取、装配）需要进一步探索。\n\n**对后续研究的启示**：\n1.  **架构简化与效率**：SICGAN展示了通过精妙的改进（而非一味增加模型复杂度）也能在视觉域适应任务上取得优异性能，这对在资源受限的嵌入式或边缘机器人平台上部署具有启示意义。\n2.  **系统性验证的重要性**：本文不仅在单一机器人上验证方法，还进行了跨机器人平台的验证，增强了结论的可靠性。未来的sim-to-real研究应鼓励进行类似的系统性跨平台测试。\n3.  **迈向更复杂任务**：本工作为基于视觉的零次迁移建立了一个稳健的基线。未来的工作可以在此基础上，将流程扩展到包含抓取、操纵等更复杂的连续任务链中，并探索应对动态环境和视角变化的方法。",
      "imageUrls": [
        "https://arxiv.org/html/2601.16677v1/x1.png",
        "https://arxiv.org/html/2601.16677v1/x2.png",
        "https://arxiv.org/html/2601.16677v1/x3.png",
        "https://arxiv.org/html/2601.16677v1/x4.png",
        "https://arxiv.org/html/2601.16677v1/x5.png",
        "https://arxiv.org/html/2601.16677v1/x6.png",
        "https://arxiv.org/html/2601.16677v1/x7.png",
        "https://arxiv.org/html/2601.16677v1/x8.png",
        "https://arxiv.org/html/2601.16677v1/x9.png",
        "https://arxiv.org/html/2601.16677v1/x10.png",
        "https://arxiv.org/html/2601.16677v1/x11.png",
        "https://arxiv.org/html/2601.16677v1/x12.png",
        "https://arxiv.org/html/2601.16677v1/x13.png",
        "https://arxiv.org/html/2601.16677v1/x14.png",
        "https://arxiv.org/html/2601.16677v1/x15.png",
        "https://arxiv.org/html/2601.16677v1/x16.png",
        "https://arxiv.org/html/2601.16677v1/x17.png",
        "https://arxiv.org/html/2601.16677v1/x18.png",
        "https://arxiv.org/html/2601.16677v1/x19.png",
        "https://arxiv.org/html/2601.16677v1/x20.png",
        "https://arxiv.org/html/2601.16677v1/x21.png",
        "https://arxiv.org/html/2601.16677v1/x22.png",
        "https://arxiv.org/html/2601.16677v1/x23.png",
        "https://arxiv.org/html/2601.16677v1/x24.png",
        "https://arxiv.org/html/2601.16677v1/x25.png",
        "https://arxiv.org/html/2601.16677v1/x26.png",
        "https://arxiv.org/html/2601.16677v1/x27.png",
        "https://arxiv.org/html/2601.16677v1/x28.png",
        "https://arxiv.org/html/2601.16677v1/x29.png",
        "https://arxiv.org/html/2601.16677v1/x30.png",
        "https://arxiv.org/html/2601.16677v1/x31.png",
        "https://arxiv.org/html/2601.16677v1/x32.png",
        "https://arxiv.org/html/2601.16677v1/x33.png",
        "https://arxiv.org/html/2601.16677v1/x34.png",
        "https://arxiv.org/html/2601.16677v1/x35.png",
        "https://arxiv.org/html/2601.16677v1/x36.png",
        "https://arxiv.org/html/2601.16677v1/x37.png",
        "https://arxiv.org/html/2601.16677v1/x38.png",
        "https://arxiv.org/html/2601.16677v1/x39.png",
        "https://arxiv.org/html/2601.16677v1/x40.png",
        "https://arxiv.org/html/2601.16677v1/x41.png",
        "https://arxiv.org/html/2601.16677v1/x42.png",
        "https://arxiv.org/html/2601.16677v1/x43.png",
        "https://arxiv.org/html/2601.16677v1/x44.png",
        "https://arxiv.org/html/2601.16677v1/x45.png",
        "https://arxiv.org/html/2601.16677v1/x46.png",
        "https://arxiv.org/html/2601.16677v1/x47.png",
        "https://arxiv.org/html/2601.16677v1/x48.png",
        "https://arxiv.org/html/2601.16677v1/x49.png",
        "https://arxiv.org/html/2601.16677v1/x50.png",
        "https://arxiv.org/html/2601.16677v1/x51.png",
        "https://arxiv.org/html/2601.16677v1/x52.png",
        "https://arxiv.org/html/2601.16677v1/x53.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.13639",
      "title": "A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint",
      "url": "http://arxiv.org/abs/2601.13639",
      "arxivId": "2601.13639",
      "date": "2026-01-20",
      "authors": "Yongchun Fang Team",
      "category": "Manipulation",
      "summary": "本文提出了一种通用的单次多模态主动感知框架，用于解决机器人操作中依赖迭代优化、成本高且任务耦合性强的问题。其核心是构建数据收集流程与最优视角预测网络，通过跨注意力机制对齐融合多模态特征，直接预测相机位姿调整。在视角受限的机器人抓取任务中实例化验证，实验表明该框架显著提升了抓取成功率，真实世界评估成功率接近翻倍，且无需微调即可实现从仿真到现实的迁移。",
      "detailedSummary": "## 研究背景与动机\n在基于视觉的机器人操作中，主动感知旨在将相机移动到更具信息量的观测视点，从而为下游任务提供高质量的感知输入。当前主流方法（如Next-Best-View规划）通常将视点选择建模为一个序列优化过程，需要多次“感知-决策-动作”循环才能逐步接近理想视点。这导致高昂的时间与运动成本，并且这些方法的视点评估函数通常与特定任务目标紧密耦合，限制了其跨任务的泛化能力。\n\n本文针对迭代优化成本高、任务耦合性强这两个关键痛点，提出了一个数据驱动的、一次性的多模态主动感知新框架。其核心思路是：模仿人类“先聚焦后执行”的高效行为模式，通过解耦视点质量评估与整体架构，构建一个能够直接从当前观测推断出最优观测视点相机位姿调整量的通用学习框架。\n\n## 方法详解\n整体框架旨在学习一个策略π: (O, L) ↦ T，该策略以当前观测O和指定目标物体的自然语言指令L为输入，输出所需的相机位姿调整T。实现分为三个阶段：(a) 合成数据集构建；(b) 感知与预处理；(c) 网络架构构建。本文以视点受限环境下的机器人抓取任务为例进行实例化验证。\n\n![方法整体框架](https://arxiv.org/html/2601.13639v1/x2.png)\n> **图2**：方法整体框架示意图，以视点受限环境下的机器人抓取为例：(a) 通过采样和评估候选视点获得每个物体的最优视点，随后通过域随机化构建数据集；(b) 基于构建的数据集训练MVPNet；(c) 部署训练好的网络并进行对比评估。\n\n**1. 合成数据集构建流程**：首先定义最优观测视点。在仿真中围绕物体随机采样大量候选相机视点，均朝向物体中心。对于每个视点，获取RGB-D图像，并利用Grounding DINO和SAM2根据语言指令进行目标检测与分割，得到物体点云。随后，使用抓取模型Economic Grasp计算抓取位姿候选，并以其聚合分数（多次检测中Top-10得分的平均值）作为该视点的质量评分。针对65类物体，共采样97.5k张图像，进行487.5k次抓取检测。视点分数在3D空间呈连续分布。\n\n![视点分数分布示例](https://arxiv.org/html/2601.13639v1/x4.png)\n> **图4**：物体视点分数分布示例：(a) 3D分布；(b-d) 三个平面的2D投影。红点分数最高，绿点中等，蓝点最低。\n\n对评分最高的800个视点进行DBSCAN聚类，选取最大簇的质心作为该物体在当前状态下的最优观测视点t_best。通过公式 t_best = t_obj + R(q_obj) · p_cam^obj，将最优视点位置p_cam^obj定义在物体坐标系下，使其能随物体位姿变化。在数据收集中，对初始相机视点、物体类别、位姿、尺度和环境光照进行域随机化。网络输入为掩码图像和点云，学习目标为相机位姿调整量ΔT = (Δt, Δq)，其通过当前相机位姿与计算得到的最优视点相机位姿之差求得。最终构建了17k个数据样本。\n\n**2. 感知与预处理**：使用腕戴式相机获取RGB-D图像。为让网络专注于目标物体几何而非颜色纹理，并支持语言引导，采用Grounded SAM 2进行预处理：先由Grounding DINO根据语言指令检测目标边界框，再由SAM 2生成分割掩码。该掩码直接作为网络图像输入，并用于对原始点云进行掩蔽。处理后的点云还经过随机丢点、最远点采样等数据增强。\n\n**3. 网络架构 (MVPNet)**：核心创新在于其多模态特征对齐与融合机制。\n\n![感知预处理与MVPNet框架](https://arxiv.org/html/2601.13639v1/x5.png)\n> **图5**：感知与预处理模块及MVPNet的整体框架。展示了从RGB-D图像和语言指令生成掩码图像和过滤后点云，并输入MVPNet进行特征提取、对齐融合，最终预测位姿调整的全过程。\n\n- **特征提取**：使用ResNet编码二进制掩码图像。对于包含丰富三维空间关系的点云，采用PointNeXt和PointNet++两种编码器进行协同特征提取，以同时捕获局部几何细节和全局上下文语义。\n- **特征对齐与融合**：将提取的点云和图像特征向量作为令牌(token)拼接，送入Transformer编码器。其交叉注意力机制能够隐式地建立2D-3D对应关系并融合多模态表征。引入一个可训练的[CLS]令牌，通过交叉注意力聚合所有信息，用于最终的位姿预测。\n- **姿态输出**：使用MLP将[CLS]令牌对应的输出映射到任务空间，即平移量Δt和旋转四元数Δq。\n- **损失函数**：总损失为平移的MSE损失与旋转的测地线距离损失之和：ℒ_total = ||t̂ - t||₂² + λ * 2arccos(|⟨q̂, q⟩|)。\n\n与现有方法相比，本文的创新点具体体现在：1) **框架解耦**：将视点质量评估函数定义为可定制模块，与框架主体分离，提升了通用性；2) **一次性预测**：通过数据驱动学习，直接输出单步调整量，避免了迭代优化；3) **输入设计**：使用二进制掩码而非彩色掩码，迫使网络学习几何特征，增强了泛化能力；4) **多模态融合**：利用Transformer交叉注意力有效对齐和融合2D掩码与3D点云特征。\n\n## 实验与结果\n实验平台基于Isaac Sim 4.0.0仿真器，使用Franka Emika机械臂和腕戴的RealSense D435i相机。测试对象包括65个“相似物体”和18个“全新物体”。评估在视点受限环境下进行，相机初始化为随机侧视位姿。\n\n**对比实验**：将本文框架（MVPNet）与以下基线方法对比：1) **固定视点**：在初始随机视点直接抓取；2) **视点随机化**：随机移动相机到一个新视点后再抓取；3) **基于TSDF的NBV**：一种使用截断有符号距离场进行Next-Best-View规划的方法。\n\n**关键实验结果**：评估指标为SR-1（首次尝试抓取成功率）和SR-5（前五次尝试中至少成功一次的概率）。\n\n![仿真抓取成功率对比](https://arxiv.org/html/2601.13639v1/x6.png)\n> **图6**：在相似物体和全新物体上的仿真抓取成功率对比。MVPNet引导的主动感知在SR-1和SR-5上均显著优于所有基线方法。\n\n仿真结果表明，在相似物体上，MVPNet将SR-1从初始视点的38.2%提升至71.8%（相对提升87.96%），SR-5从65.2%提升至92.8%；在全新物体上，SR-1从42.9%提升至78.6%，SR-5从73.8%提升至94.6%。均大幅优于随机移动和基于TSDF的NBV方法。\n\n**消融实验**：验证了各核心组件的贡献。\n\n![消融研究结果](https://arxiv.org/html/2601.13639v1/x7.png)\n> **图7**：消融研究结果。移除非最优视点数据、点云编码器之一、或掩码图像输入，均会导致性能显著下降，证明了各组件设计的必要性。\n\n- **训练数据质量**：使用非最优视点（最低分视点）数据训练，性能急剧下降，证明了高质量视点标签的重要性。\n- **多模态输入**：仅使用点云或仅使用掩码图像，性能均不如两者结合，验证了多模态融合的有效性。\n- **点云双编码器**：仅使用PointNeXt或PointNet++单一编码器，性能低于两者结合，表明协同提取能获得更鲁棒的特征。\n- **二进制掩码 vs. 彩色掩码**：使用彩色掩码作为输入会降低性能，支持了迫使网络学习几何形状的设计选择。\n\n**定性结果**：\n![定性结果对比](https://arxiv.org/html/2601.13639v1/x8.png)\n> **图8**：不同方法的视点优化与抓取结果定性对比。MVPNet能够预测出显著改善观测质量的视点调整，从而产生更多、更稳定的抓取候选。\n\n**真实世界实验**：将仿真训练的网络直接迁移到真实机器人系统，无需微调。\n\n![真实世界实验设置与物体](https://arxiv.org/html/2601.13639v1/x9.png)\n> **图9**：真实世界实验场景设置（左）与使用的实验物体（右）。\n\n![真实世界抓取成功率](https://arxiv.org/html/2601.13639v1/x10.png)\n> **图10**：真实世界抓取成功率。MVPNet将SR-1从初始视点的30%提升至58%（相对提升93.33%），接近仿真中的提升趋势，证明了优秀的Sim-to-Real迁移能力。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了一个通用的、一次性的多模态主动感知框架，通过解耦视点质量评估，实现了跨任务需求的统一建模；2) 建立了一个基于任务特定评估函数和域随机化的最优视点数据自动收集流程，无需人工标注；3) 设计了MVPNet，利用Transformer交叉注意力机制有效对齐和融合2D掩码与3D点云特征，直接预测相机位姿调整。\n\n论文自身提到的局限性包括：框架性能依赖于目标分割模块（如Grounded SAM 2）的准确性；当前实例化集中于抓取任务，在其他任务（如位姿估计）中的有效性有待进一步验证。\n\n本文对后续研究的启示在于：**解耦设计**为构建通用主动感知系统提供了可行路径，未来可将不同的任务评价函数即插即用地接入该框架。**一次性预测范式**为降低主动感知的时间与运动成本提供了新思路。**多模态学习策略**，特别是通过注意力机制融合2D与3D特征，对于需要几何理解的任务具有借鉴价值。如何进一步减少对预训练分割模型的依赖，以及探索更高效的模拟到真实迁移技术，是值得深入的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2601.13639v1/x1.png",
        "https://arxiv.org/html/2601.13639v1/x2.png",
        "https://arxiv.org/html/2601.13639v1/x3.png",
        "https://arxiv.org/html/2601.13639v1/object_similar.png",
        "https://arxiv.org/html/2601.13639v1/object_novel.png",
        "https://arxiv.org/html/2601.13639v1/x4.png",
        "https://arxiv.org/html/2601.13639v1/x5.png",
        "https://arxiv.org/html/2601.13639v1/x6.png",
        "https://arxiv.org/html/2601.13639v1/x7.png",
        "https://arxiv.org/html/2601.13639v1/x8.png",
        "https://arxiv.org/html/2601.13639v1/real-world_scene_setup.jpg",
        "https://arxiv.org/html/2601.13639v1/real-world_experimental_objects.jpg",
        "https://arxiv.org/html/2601.13639v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.12952",
      "title": "Imitation learning-based spacecraft rendezvous and docking method with Expert Demonstration",
      "url": "http://arxiv.org/abs/2601.12952",
      "arxivId": "2601.12952",
      "date": "2026-01-19",
      "authors": "Mingxuan Jiang Team",
      "category": "Manipulation",
      "summary": "本文针对航天器交会对接控制方法依赖精确动力学模型、在轨鲁棒性有限的问题，提出了一种基于模仿学习的无模型控制框架IL-SRD。其核心创新在于引入了锚定解码器目标机制，通过状态相关的锚点显式约束控制生成过程，确保物理一致性；并采用时间聚合机制来抑制Transformer序列预测中的误差累积。大量仿真实验表明，该方法能实现精确、节能的六自由度交会对接控制，并在显著未知干扰下保持优异的鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n航天器交会对接控制的主流方法包括基于精确动力学模型的传统控制方法（如PID、模型预测控制MPC、滑模控制SMC）以及基于深度强化学习（DRL）的方法。前者严重依赖精确的模型，在存在未知扰动时鲁棒性受限；后者虽不显式依赖模型，但其性能高度敏感于奖励函数设计，且训练过程漫长。模仿学习（IL）通过从专家演示中直接学习策略，避免了上述问题，但行为克隆（BC）等简单IL方法难以捕捉交会对接这类长时域、强耦合任务中的时序依赖关系，导致生成的动作序列在时序上不一致、不稳定。本文针对传统方法对模型的依赖、DRL对奖励设计的敏感以及简单IL的长时域控制不稳定性等痛点，提出了一种基于模仿学习的航天器交会对接控制框架（IL-SRD）。其核心思路是：利用基于Transformer的架构学习专家演示中的长时域时序依赖，并引入锚定解码目标（ADT）机制和时间聚合机制，分别确保生成动作的物理一致性并抑制序列预测中的误差累积，从而实现无需模型、稳定可靠的六自由度控制。\n\n## 方法详解\nIL-SRD方法的整体框架是一个基于Transformer的变分自编码器（VAE）编码器-解码器结构，旨在根据当前状态预测未来一段时域内的动作序列。\n\n![方法框架](https://arxiv.org/html/2601.12952v1/x1.jpg)\n\n> **图1**：IL-SRD网络结构。左侧为训练阶段：当前状态和未来的专家动作序列经投影网络嵌入后，与位置编码相加，输入到Transformer编码器中，编码器输出经线性层得到潜在变量的均值和方差，通过重参数化采样得到潜在变量z；解码阶段，潜在变量z和当前状态共同构成解码器记忆，解码器查询（Queries）由经过投影的、重复了n次（预测步长）的当前状态（即锚点）构成，解码器通过交叉注意力机制输出预测的动作序列。右侧为评估阶段：由于没有未来专家动作，潜在变量z设为零向量。\n\n**核心模块与技术细节**：\n1.  **专家演示生成**：为解决遥操作演示存在的振荡和次优问题，本文采用非线性MPC生成高质量专家数据。MPC以状态误差、控制能耗和终端误差构建成本函数，并考虑动力学约束和执行器约束，通过序列二次规划（SQP）求解，生成平滑、节能且动力学一致的控制轨迹。共收集50条轨迹，每条2500步，并注入随机噪声以模拟扰动和传感器不确定性。\n\n![专家演示分布](https://arxiv.org/html/2601.12952v1/x3.png)\n\n> **图3**：专家演示的状态分布。初始条件覆盖正负值，鼓励模型学习全局收敛行为而非平凡的映射；四元数标量分量收敛于1或-1，反映了四元数的符号歧义性。\n\n2.  **VAE编码器**：输入为当前状态 `s_t` 和未来n步的专家动作序列 `â_{t:t+n}`。两者分别通过投影网络 `ψ_s` 和 `ψ_a` 映射到共享的高维嵌入空间，拼接后加入位置编码 `P_VAE`，形成序列 `E`。Transformer编码器处理 `E`，通过自注意力机制捕捉状态与动作序列间的复杂时序依赖。编码器输出经线性层得到潜在变量z的均值 `μ` 和对数方差 `log σ²`，通过重参数化技巧 `z = μ + σ ⊙ ε` 采样得到z，z封装了专家演示的潜在结构。评估时，z设为零向量。\n3.  **锚定解码目标（ADT）机制**：这是确保物理一致性的关键创新。解码器的查询（Queries）并非传统的可学习参数或位置编码，而是由经过投影的、重复了n次的当前状态构成。这意味着解码器在生成每个未来时刻的动作时，其“查询”都显式地锚定在同一个物理状态参考上。这种设计强制模型在预测动作时紧密关联当前物理状态，有效抑制了可能违反动力学或执行器约束的不合理动作偏差。\n4.  **解码器与时间聚合机制**：解码器记忆 `M` 由潜在变量z和当前状态 `s_t` 的投影拼接而成。解码器以ADT生成的查询对记忆进行交叉注意力计算，最终输出预测的n步动作序列。**时间聚合机制**体现在：模型一次性预测一个动作序列（n步），而非逐步单步预测。这减少了解耦预测的总步数，从而在长时域任务中显著缓解了因每一步微小误差累积和放大而导致的性能退化问题。预测的动作序列会被循环执行，直到下一个预测周期。\n\n**创新点总结**：与现有方法相比，IL-SRD的创新在于：1) 采用序列到序列的Transformer架构直接学习长时域控制策略，克服了简单BC的时序建模不足；2) 提出ADT机制，将解码过程锚定于物理状态，确保了动作生成的物理可行性与一致性；3) 通过序列预测结合时间聚合，有效抑制了长时域序列预测中的误差传播问题。\n\n## 实验与结果\n**实验设置**：在自定义的航天器六自由度动力学仿真环境中进行验证。动力学模型基于线性Clohessy-Wiltshire方程（平动）和刚体旋转动力学（转动），使用四阶龙格-库塔法积分。评估指标包括任务成功率、平动/姿态误差、燃料消耗（ΔV）等。\n\n**对比的Baseline方法**：\n- **MPC**：生成专家演示的控制器，作为性能上限参考。\n- **滑模控制（SMC）**：一种鲁棒性强的传统模型基控制器。\n- **深度强化学习（DRL）**：基于PPO算法训练的控制器。\n- **行为克隆（BC）**：简单的状态-动作映射模仿学习。\n\n![定量结果对比](https://arxiv.org/html/2601.12952v1/x4.jpg)\n\n> **图4**：各方法在50次随机初始条件下的蒙特卡洛仿真结果对比。IL-SRD在成功率（100%）、终端位置误差（0.0023 m）、终端姿态误差（0.0007 rad）和燃料消耗（ΔV 0.8171 m/s）上均显著优于SMC、DRL和BC，性能最接近MPC。\n\n![轨迹对比](https://arxiv.org/html/2601.12952v1/x5.png)\n\n> **图5**：典型初始条件下各方法的控制轨迹对比。IL-SRD的轨迹平滑且收敛迅速，与MPC（专家）最为接近；SMC存在抖振；DRL收敛缓慢且有振荡；BC则完全发散。\n\n![消融实验](https://arxiv.org/html/2601.12952v1/x6.png)\n\n> **图6**：消融实验结果。(a) 移除ADT机制导致位置和姿态控制发散，证明了ADT对物理一致性的关键作用。(b) 移除时间聚合（即改为逐步预测）导致误差随时间累积放大，最终任务失败，证明了时间聚合对稳定长时域控制的必要性。\n\n**鲁棒性评估**：在仿真中施加持续的外部力/力矩扰动和周期性正弦扰动。\n\n![扰动下性能](https://arxiv.org/html/2601.12952v1/x7.png)\n\n> **图7**：在持续未知扰动下，IL-SRD仍能维持100%成功率和接近无扰动的性能，而SMC和DRL的成功率分别下降至94%和86%，证明了IL-SRD强大的鲁棒性。\n\n**关键实验结果总结**：\n- **性能**：IL-SRD在50次蒙特卡洛测试中达到**100%**成功率，平均终端位置误差**0.0023 m**，姿态误差**0.0007 rad**，燃料消耗**0.8171 m/s**，全面优于SMC、DRL和BC，且非常接近专家MPC的性能。\n- **消融实验**：移除ADT机制导致控制完全发散；移除时间聚合（改为单步预测）导致误差累积和任务失败。这分别验证了**ADT机制**和**时间聚合机制**对于实现稳定、精确的长时域控制是不可或缺的核心组件。\n- **鲁棒性**：在显著未知扰动下，IL-SRD保持了**100%**的成功率和稳定的性能，显著优于SMC和DRL。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了IL-SRD框架，首次将基于Transformer的序列到序列模仿学习成功应用于六自由度航天器交会对接这一复杂长时域控制任务，实现了无需精确动力学模型和复杂奖励设计的模型无关控制。\n2.  创新性地设计了锚定解码目标（ADT）机制，通过将解码查询锚定于当前状态，显式地强制了生成动作序列的物理一致性，有效避免了不合理或危险的控制指令。\n3.  引入了时间聚合机制，通过预测并执行动作序列而非单步动作，显著缓解了基于Transformer的序列模型在长时域预测中的误差累积问题，提升了控制的长期稳定性。\n\n**局限性**：论文自身提到，该方法依赖于专家演示的质量。虽然本文使用MPC生成了高质量数据，但在实际应用中，获取大量覆盖所有可能工况的最优专家演示可能具有挑战性。此外，方法在极端、训练数据未覆盖的扰动下的表现仍有待进一步验证。\n\n**对后续研究的启示**：\n1.  **数据效率与泛化**：可以探索结合少量交互数据（在线学习或混合模仿学习与强化学习）来提升策略在分布外状态下的泛化能力和对未知扰动的适应性。\n2.  **专家数据来源**：研究如何从非完美（如遥操作）甚至次优的演示中学习稳健策略，将极大提升方法的实用性和可部署性。\n3.  **架构扩展**：当前方法主要处理状态输入，未来可扩展至多模态输入（如视觉图像），以实现更接近真实场景的感知与控制一体化。",
      "imageUrls": [
        "https://arxiv.org/html/2601.12952v1/x1.jpg",
        "https://arxiv.org/html/2601.12952v1/x2.png",
        "https://arxiv.org/html/2601.12952v1/x3.png",
        "https://arxiv.org/html/2601.12952v1/x4.jpg",
        "https://arxiv.org/html/2601.12952v1/x5.png",
        "https://arxiv.org/html/2601.12952v1/x6.png",
        "https://arxiv.org/html/2601.12952v1/x7.png",
        "https://arxiv.org/html/2601.12952v1/x8.png",
        "https://arxiv.org/html/2601.12952v1/x9.png",
        "https://arxiv.org/html/2601.12952v1/x10.png",
        "https://arxiv.org/html/2601.12952v1/x11.png",
        "https://arxiv.org/html/2601.12952v1/x12.png",
        "https://arxiv.org/html/2601.12952v1/x13.png",
        "https://arxiv.org/html/2601.12952v1/x14.png",
        "https://arxiv.org/html/2601.12952v1/x15.png",
        "https://arxiv.org/html/2601.12952v1/x16.png",
        "https://arxiv.org/html/2601.12952v1/x17.png",
        "https://arxiv.org/html/2601.12952v1/x18.png",
        "https://arxiv.org/html/2601.12952v1/x19.png",
        "https://arxiv.org/html/2601.12952v1/x20.png",
        "https://arxiv.org/html/2601.12952v1/x21.png",
        "https://arxiv.org/html/2601.12952v1/x22.png",
        "https://arxiv.org/html/2601.12952v1/x23.png",
        "https://arxiv.org/html/2601.12952v1/x24.png",
        "https://arxiv.org/html/2601.12952v1/x25.png",
        "https://arxiv.org/html/2601.12952v1/x26.png",
        "https://arxiv.org/html/2601.12952v1/x27.png",
        "https://arxiv.org/html/2601.12952v1/x28.png",
        "https://arxiv.org/html/2601.12952v1/x29.png",
        "https://arxiv.org/html/2601.12952v1/x30.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.16242",
      "title": "Scalable Screw-Theoretic Synthesis for PDE-Based Dynamic Modeling of Multibody Flexible Manipulators",
      "url": "http://arxiv.org/abs/2601.16242",
      "arxivId": "2601.16242",
      "date": "2026-01-22",
      "authors": "J. Mattila Team",
      "category": "Manipulation",
      "summary": "本文针对多体柔性机器人操纵器动态建模的可扩展性挑战，提出了一种基于螺旋理论的合成框架。该方法通过构建单个柔性链接的偏微分方程模型，利用双螺旋描述运动与变形，并强制执行关节约束，实现无限可扩展的多体动力学表示。关键要点包括将支配方程表述为半显式索引1微分代数系统，以及通过变量分离证明适定性。节选未提供具体实验数据。",
      "detailedSummary": "## 研究背景与动机\n现代工程中对轻量化机械设计的需求日益增长，以追求更高的能效和负载能力。这促使航空航天和机器人等领域越来越多地考虑使用柔性部件。然而，柔性结构在带来优势的同时也引入了显著挑战：连杆柔性引起的位移会损害系统精度和性能。因此，亟需开发能准确捕获柔性系统动力学的建模工具。随着系统复杂性增加，多个柔性体相互作用，建模和控制任务变得异常困难，可扩展性成为关键挑战。许多现有方法（如浮动参考系法FFR、绝对节点坐标法ANCF、几何精确梁模型GEBF、共旋有限元法）难以扩展到高自由度柔性系统：它们或通过模态截断牺牲精度，或因计算量过大而不适合实时控制。\n\n本文针对多体柔性机械臂系统建模中“缺乏可扩展的、统一的、能同时捕获刚体运动与分布式弹性变形的高保真模型”这一具体痛点，提出了一个新视角：将成熟的、用于刚性多体系统的、基于旋量理论的模块化“链接”合成策略，扩展至柔性系统。其核心思路是：首先在各自的体坐标系中，为单个柔性连杆系统性地构建基于旋量理论的偏微分方程（PDE）模型；然后，通过施加完整关节约束，将这些单连杆模型“链接”起来，从而合成一个可无限扩展的多体系统表示。\n\n## 方法详解\n本文提出的框架旨在为具有任意数量柔性连杆的串联机械臂，构建一个可扩展的旋量理论合成框架。其整体流程分为两步：1）为每个柔性连杆建立统一的、基于体坐标的旋量-PDE动力学模型；2）通过关节处的完整约束和相互作用力，将这些独立的连杆模型合成为多体系统。\n\n![方法框架](https://arxiv.org/html/2601.16242v1/Fig1.jpg)\n\n> **图1**：单个柔性体的动力学运动示意图。展示了惯性坐标系、体坐标系以及柔性体上一点相对于这两个坐标系的运动关系，是构建单个连杆模型的基础。\n\n框架的核心在于使用一组在体坐标系中表达的对偶旋量来描述每个柔性连杆的动力学：\n1.  **运动旋量 (𝐬ᵢ)**：描述体坐标系相对于惯性坐标系的运动（包含位置𝐫ᵢ和姿态𝜽ᵢ）。\n2.  **几何旋量 (𝐬_bᵢ)**：描述未变形构型中，梁的微元在体坐标系中的参考几何位置（𝐫_bᵢ），该旋量在体坐标系中不随时间变化。\n3.  **变形旋量 (𝐬_ξᵢ)**：描述由于柔性引起的、微元相对于体坐标系的弹性位移场（𝐫_ξᵢ）。\n\n通过定义总位置向量 𝐫_obᵢ = 𝐫ᵢ + 𝐫_bᵢ + 𝐫_ξᵢ，并利用变分原理对系统的哈密顿量进行分析，论文此前已推导出单个柔性连杆在惯性空间中的统一旋量-PDE动力学模型。该模型包含一个描述体坐标系整体运动的动态方程（式26），一个描述分布式变形的PDE方程（式34），以及相应的边界条件（式35-38）。动态方程具有形式 **𝐌∗ᵢ 𝐳̇ᵢ + 𝐆∗ᵢ(𝐳̇_ξᵢ) + 𝐇∗ᵢ = 𝐅∗ᵢ**，其中 **𝐌∗ᵢ** 是正定惯性张量，**𝐆∗ᵢ** 耦合了变形速度与整体加速度，**𝐇∗ᵢ** 包含科里奥利力、离心力、重力及内部弹性力，**𝐅∗ᵢ** 为作用于连杆两端的外部力和力矩。\n\n与现有方法相比，本文的创新点具体体现在将旋量理论从刚性体系统扩展到连续柔性体，并以此为基础构建合成框架。单个连杆模型在体坐标系中表达，惯性张量对称正定，且以旋量形式统一了线性和旋转动力学，这种结构为后续的“链接”提供了便利。\n\n![合成示意图](https://arxiv.org/html/2601.16242v1/Fig2.jpg)\n\n> **图2**：基于关节连接完整约束分解后的多连杆柔性机器人示意图。展示了相邻连杆（i与i+1）通过关节aᵢ连接，并标明了相互作用力/力矩（𝓕_Jᵢ, 𝓕_Tᵢ）和关节运动学量（𝐬_aᵢ），这是多体合成的关键。\n\n多体系统的合成通过强制执行完整关节约束来实现。如图2所示，对于连接连杆i和i+1的关节aᵢ，其运动学由旋量 **𝐬_aᵢ** 描述。合成过程的核心是引入关节处的相互作用力旋量 **𝓕_Jᵢ** 和 **𝓕_Tᵢ** 作为拉格朗日乘子，并将其纳入每个连杆的动态方程（**𝐅∗ᵢ**）中。同时，施加运动学约束条件，确保在关节连接点处，相邻连杆的位移和速度相容。通过这种方式，n个连杆的独立PDE模型与（n-1）个关节约束方程共同构成了一个封闭的系统。最终，整个多体系统的控制方程被表述为一个半显式索引-1的微分代数方程系统，从而兼顾了计算可行性和数学严谨性。\n\n## 实验与结果\n本文的工作侧重于理论框架的构建与数学性质的验证，而非与传统数值方法进行性能对比的仿真实验。因此，其“实验”部分主要是理论分析和证明。\n\n所使用的“基准”是数学上的适定性概念。研究平台是抽象的泛函分析空间。\n\n关键的“实验结果”是对所提出动态模型适定性的严格证明。为了达成这一点，作者应用了分离变量法，将PDE模型重新表述为一个抽象的柯西问题（Abstract Cauchy Problem）。通过半群理论，论文在第四部分详细论证了该动态系统解的存在性、唯一性和连续依赖性（即适定性）。这从数学上确保了所推导模型的良定义和可靠性，是该方法的一个重要理论贡献。\n\n由于论文未报告具体的数值仿真对比实验（如与FFR、ANCF等方法在特定轨迹下的误差比较），因此没有相关的对比图表或具体性能提升百分比。论文的主要图表（图1，图2）用于说明建模概念和合成框架，而非展示数值结果。\n\n消融实验并非本文重点，但论文本身通过分步构建（先单连杆，后多体合成） implicitly 展示了每个核心模块的贡献：1）基于三个旋量的单连杆PDE模型是基础；2）通过关节约束和相互作用力进行合成是实现可扩展性的关键；3）将系统表述为微分代数方程并证明其适定性，确保了框架的数学严谨性和可用性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **首创了无限可扩展的旋量理论合成框架**：首次为三维空间中的多连杆柔性机械臂提出了一个系统性的、基于旋量理论和PDE的建模框架，能够通过“链接”模块化的单连杆模型来合成任意复杂度的多体系统。\n2.  **规避了整体哈密顿建模的极端复杂性**：该方法避免了直接对耦合能量项进行复杂的变分分析，使得模型能够方便地适应构型或连杆数量的变化，提高了建模的灵活性和可扩展性。\n3.  **建立了动态系统的适定性**：通过将PDE模型转化为抽象柯西问题并运用半群理论，严格证明了所推导微分代数系统是适定的，为后续的数值离散和控制器设计奠定了坚实的数学基础。\n\n论文自身提到的局限性并不显式，但可以推断，尽管模型在理论上是可扩展的，但最终的PDE-微分代数方程系统仍需进行空间离散（如有限元法、谱方法）才能进行数值求解，其计算效率与离散化方法密切相关。此外，模型基于欧拉-伯努利梁理论，对于剪切变形不可忽略的短粗梁或更复杂的柔性结构需要进一步扩展。\n\n本文对后续研究的启示包括：为基于模型的柔性机器人控制（如边界控制、分布参数系统控制）提供了一个高保真的统一模型；所建立的适定性理论为发展该模型的高效、保结构数值离散方法提供了指导；框架的模块化特性使其易于集成不同的执行器和传感器模型，或扩展到并联、树状等更复杂的拓扑结构。",
      "imageUrls": [
        "https://arxiv.org/html/2601.16242v1/ORCID_iD.svg.jpg",
        "https://arxiv.org/html/2601.16242v1/ORCID_iD.svg.jpg",
        "https://arxiv.org/html/2601.16242v1/Fig1.jpg",
        "https://arxiv.org/html/2601.16242v1/Fig2.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.14617",
      "title": "UniCon: A Unified System for Efficient Robot Learning Transfers",
      "url": "http://arxiv.org/abs/2601.14617",
      "arxivId": "2601.14617",
      "date": "2026-01-21",
      "authors": "Weinan Zhang Team",
      "category": "Manipulation",
      "summary": "本文针对异构机器人平台间学习控制器部署困难、接口不一致及中间件效率低下的核心问题，提出了统一框架UniCon。其关键技术在于：通过标准化状态与控制流，将工作流分解为可重用组件的执行图，并分离系统状态与控制逻辑；采用批处理与向量化数据流，以最小化通信开销。实验表明，与基于ROS的系统相比，UniCon在迁移工作流时减少了代码冗余，并实现了更高的推理效率，已成功部署于7家制造商的12款机器人模型。",
      "detailedSummary": "## 研究背景与动机\n当前，将学习型控制器部署到异构机器人平台上面临巨大挑战，主要源于平台差异、接口不一致以及中间件效率低下。虽然领域随机化等技术缩小了仿真与现实的动力学差距，但并未简化“最后一公里”的集成。以ROS/ROS 2为代表的通用中间件提供了标准化的通信层，但其消息传递模式会引入延迟和开销，不利于高频策略推理。同时，机器人制造商提供的接口不一致进一步加剧了集成和迁移的难度。\n\n本文针对上述痛点，提出了一个数据导向的新视角，旨在标准化状态、控制流和仪器测量，实现跨平台的即插即用部署。其核心思路是：将工作流解耦为基于全局向量化状态的执行图，使控制逻辑与系统状态分离，从而最小化通信开销，提高推理效率。\n\n## 方法详解\nUniCon的整体设计采用模块化、数据导向的范式，其核心是分离系统状态与控制逻辑。框架将机器人工作流中的所有运行时状态抽象为全局的向量化数组，并将控制功能拆分为可重用的控制块（Control Block, CB），通过标准化的控制流原语组合成执行图。\n\n![方法框架](https://arxiv.org/html/2601.14617v1/x2.png)\n\n> **图2**：UniCon架构总览。(a) 具有可切换存储后端的全局系统状态；(b) 涵盖平台和推理的模块化控制块；(c) 用于工作流组合的控制流图原语；(d) 与仿真器和硬件的统一集成。\n\n**核心模块与技术细节**：\n1.  **向量化状态（Vectorized States）**：所有运行时状态（如关节位置/速度、IMU数据、电机指令）均被表述为不同标签下的全局数值类型数组对象。这种设计避免了传输完整的自描述对象，仅交换必需的多维数据，消除了不必要的封装，并与仿真器和硬件接口偏好的连续内存布局对齐。当自定义组件的状态定义与实际硬件不同时，系统会插入高效的向量化索引操作进行透明映射以对齐属性（如关节顺序）。\n2.  **模块化逻辑（Modularized Logic）**：工作流被分解为由全局状态桥接的可重用控制块。每个控制块被定义为一个关于全局状态空间 \\(S\\) 的纯函数 \\(f: S_{in} \\mapsto S_{out} \\times \\{0,1\\}\\)，它读取 \\(S\\) 的一个子集作为输入，输出对 \\(S\\) 中另一些元素的更新值，以及一个指示其终止的布尔值 \\(r\\)。通过嵌套控制块，可以形成执行图。\n3.  **控制流原语**：系统提供了类似函数式编程的基本控制流组件，用于组合自定义工作流。例如：\n    *   `loop(T_g, p)`：当谓词 \\(p\\) 满足时终止内部块序列 \\(T_g\\) 的执行。\n    *   `zip(T_g, T_h)`：按顺序成对执行两个块序列。\n    *   `chain(T_g, T_h)`：依次耗尽嵌套的块序列。\n4.  **软件互操作性（Software Interoperability）**：\n    *   **通信**：状态存储后端通过一致的语义（写数组即发布，读数组即检索最新值）连接到外部客户端。支持多种后端以平衡性能与灵活性，包括用于零拷贝访问的无锁本地/共享内存、用于分布式设置的消息队列，以及用于遗留系统集成的ROS桥接。\n    *   **平台支持**：仿真和硬件平台被统一集成为状态读写器。每个平台适配器提供三个可调用块：`recv` 更新状态，`send` 发送控制指令，`close` 进行拆卸，并自动对齐PID增益和自由度限制等参数以确保一致的控制语义。\n    *   **外设**：常用外设工具和设备（如操纵杆、VR设备、RGBD相机、LiDAR、可视化工具）也被集成即插即用的控制块。\n\n**创新点**：与ROS等基于消息传递的中间件不同，UniCon采用数据导向的架构，将状态作为全局共享的向量化数据进行管理，控制逻辑作为访问这些数据的纯函数模块。这种设计优先考虑单线程计算和向量化数据流，而非多进程和消息传递，从而最小化通信开销和复制操作，显著提升了推理效率，同时保持了与现有软件的互操作性。\n\n## 实验与结果\n**实验设置**：评估在四足机器人、人形机器人和机械臂的部署任务上进行，重点关注迁移工作量、框架效率和流程可用性。使用的机器人平台涵盖超过12种模型、7家制造商（见表1），包括Unitree H1/G1/A1/Go2/Aliengo、PND Adam等。对比的基线方法包括针对特定机器人的Ad-hoc代码实现以及基于ROS 2的系统。\n\n**关键实验结果**：\n1.  **迁移工作量（Transfer Effort）**：以人形和四足机器人的运动任务为例，测量将原始工作流（在Unitree H1上）迁移到MuJoCo仿真器和其他机器人所需修改的源代码行数（SLOC）。\n\n    ![迁移工作量对比表](https://arxiv.org/html/2601.14617v1/x4.png)\n    > **表2**：跨工作流迁移工作量的比较（代码更改，单位：SLOC）。结果显示，Ad-hoc方法在迁移到不同平台（尤其是API完全不同的PND Adam）时需要大量修改（507-770 SLOC），而UniCon在切换部署目标时无需额外代码修改（0 SLOC），仅需少量基础配置。\n\n2.  **框架效率（Framework Efficiency）**：在Unitree H1上以50Hz运行恒等关节位置控制，测量操作延迟开销。比较了机器人原生SDK（同步/异步模式）、ROS 2和UniCon的性能。\n\n    ![框架效率对比表](https://arxiv.org/html/2601.14617v1/x5.png)\n    > **表3**：在H1上的框架效率比较（操作延迟，单位：μs）。UniCon的 `Recv` 和 `Send` 延迟与SDK实现相近，并显著优于ROS 2。端到端延迟（从发送控制信号到依赖状态的时间戳）也最低（732 ± 749 μs），表明其高效性源于将全局状态缓冲区绑定到数据读写器，并与SDK一同编译以消除冗余数据复制。\n\n3.  **流程可用性与实到仿分析（Workflow Usability & Real-to-Sim Analysis）**：展示了UniCon在构建复杂应用（如全身控制、VR遥操作）方面的可用性。重点演示了其实到仿分析能力，通过比较记录的和重放的轨迹来量化并定位“现实差距”。\n\n    ![实到仿分析](https://arxiv.org/html/2601.14617v1/x3.png)\n    > **图3**：推理轨迹的实到仿分析。左图和中图：使用内置指标量化的每个关节位置的真实差距，显示A1机器人的左后小腿关节存在显著偏差。右图：稳定的Go2站立姿态（上图）与即将跌倒的A1姿态（下图）对比。该分析有助于诊断策略在真实世界中失效的原因（如关节过热导致失衡）。\n\n**消融实验总结**：虽然未进行传统意义上的组件消融实验，但通过对比Ad-hoc、ROS 2和UniCon在迁移工作量与运行延迟上的显著差异，直接证明了其**数据导向的模块化设计**和**向量化状态管理**这两个核心设计选择的有效性，它们共同贡献了极低的迁移成本和极高的运行效率。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出统一控制框架**：UniCon作为一个轻量级框架，为主流仿真器和物理硬件之间的机器人学习部署提供了桥梁。\n2.  **创新系统设计范式**：采用模块化、数据导向的设计，将向量化全局状态与控制逻辑分离，实现了跨多样机器人形态的高效仿真到现实迁移，且几乎无需重新设计。\n3.  **显著提升部署效率**：通过减少接口和通信开销，相比基于ROS和特定框架堆栈的方案，实现了更高的推理效率和更低的代码冗余。\n\n**局限性**：论文自身提及的局限性或未来工作方向包括扩展多语言API支持，以及通过代码生成来扩大平台覆盖范围。\n\n**对后续研究的启示**：UniCon提供了一种不同于传统面向对象或消息传递中间件的系统设计思路，其“状态-逻辑”分离和向量化数据流理念，为需要高性能、低延迟且需跨平台部署的学习型机器人控制系统提供了一个高效的底层架构参考。它并非取代ROS，而是与之正交互补，专注于统一和复用控制逻辑层，有望简化机器人学习研究的工程部署流程。",
      "imageUrls": [
        "https://arxiv.org/html/2601.14617v1/x1.png",
        "https://arxiv.org/html/2601.14617v1/x2.png",
        "https://arxiv.org/html/2601.14617v1/x3.png",
        "https://arxiv.org/html/2601.14617v1/x4.png",
        "https://arxiv.org/html/2601.14617v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.12993",
      "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
      "url": "http://arxiv.org/abs/2601.12993",
      "arxivId": "2601.12993",
      "date": "2026-01-19",
      "authors": "Zongqing Lu Team",
      "category": "Manipulation",
      "summary": "论文Being-H0.5旨在解决机器人学习中的跨具身泛化问题，即如何让单一模型适应不同形态的机器人，克服数据稀缺和形态异构的挑战。关键技术包括：构建超过35,000小时的UniHand-2.0多模态数据集；提出统一动作空间，将异构控制映射到语义对齐的槽；采用混合变换器架构，结合混合流框架解耦共享运动原语与具身特定专家；引入流形保持门控和通用异步分块提升稳健性。实验表明，该模型在模拟基准LIBERO和RoboCasa上分别达到98.9%和53.9%的性能，并在五个真实机器人平台上实现跨具身部署。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型是迈向通用机器人策略的一个有前景的范式，但其发展受到两个关键局限性的制约。首先，机器人特定数据稀缺，缺乏大规模演示语料库。其次，机器人形态具有异构性，不同机器人在运动学、驱动限制和控制频率上存在巨大差异。现有VLA模型如同“单语者”，在特定硬件上表现优异，但部署到不同形态上时存在严重的“物理鸿沟”。具体对于基于扩散/流匹配的VLA模型，在简单硬件上预训练的模型在遇到复杂实体时，会因动作分布的根本性差异而产生严重的分布偏移，导致推理轨迹从有效的运动流形上“漂移”，产生不稳定行为。\n\n本文针对数据稀缺和形态异构这两个痛点，提出了以人为中心的学习新视角。其核心思想是将海量的人类交互行为视为物理世界的“通用语”，为学习提供物理常识和不变的交互逻辑。同时，本文提出统一动作空间，将异构的机器人控制映射到语义对齐的槽位，使低资源机器人能够从人类数据和高资源平台中引导技能。本文的核心思路是：通过构建超大规模、多形态的数据集UniHand-2.0，并设计统一动作空间与序列建模框架，训练一个能够感知、描述和行动的单体基础模型，实现强大的跨具身泛化。\n\n## 方法详解\nBeing-H0.5的整体框架基于一个统一的序列建模范式。输入是来自人类演示、机器人操作和视觉-语言理解三大来源的多模态数据序列。这些数据被序列化为单一的多模态令牌流，其中视觉和文本提供上下文基础，统一的状态/动作令牌携带物理上有意义的交互信号。输出是在统一动作空间下预测的动作序列，模型最终能够根据视觉观察和语言指令生成控制不同机器人的动作。\n\n![方法总览](https://arxiv.org/html/2601.12993v1/x4.png)\n> **图4**: Being-H0.5方法总览。(a) 统一动作空间将人类手部运动（通过MANO参数表示）和异构机器人控制映射到语义对齐的槽位。(b) 统一序列建模将所有异构监督（人类动作、机器人动作、视觉-语言数据）整合到单一令牌流中进行训练。(c) 混合流架构，包含用于高级多模态推理的共享Transformer和用于低级动作生成的专业化流专家，通过流形保持门控进行路由。\n\n核心模块与技术细节如下：\n1.  **UniHand-2.0数据集**：这是方法的基础，是一个超过35,000小时、包含400M样本、120B令牌的超大规模预训练配方。它包含三部分：16,000小时以自我为中心的人类视频（提供密集的行为先验）、14,000小时覆盖30种不同机器人形态的操作数据、以及5,000小时等效的通用视觉-语言理解数据（保持模型的高层推理和指令遵循能力）。该数据集的规模和形态多样性均为当前之最。\n2.  **统一动作空间**：为解决形态异构性问题，该方法设计了一个统一动作空间。其核心是将人类轨迹（通过MANO手部模型参数表示）和异构机器人控制映射到一组语义对齐的槽位，例如`[dx, dy, dz, drx, dry, drz, grip]`。这充当了不同硬件间的“通用语法”，将功能意图与机械关节解耦，使模型能够内化交互的底层物理原理，而不仅仅是特定形态的命令。\n3.  **统一序列建模**：在此统一动作空间的基础上，将所有异构监督（人类演示、机器人轨迹、视觉-语言语料库）转化为一个统一的序列建模问题。不同来源的数据被序列化为单一令牌流，并应用相应的损失函数进行优化：以文本为中心的语料库使用标准的下一个令牌预测损失，而人类和机器人语料库则在统一动作空间内监督动作预测。\n4.  **混合流架构**：在模型架构上，Being-H0.5采用了混合Transformer设计，并引入了新颖的**混合流框架**。该框架将动作模块解耦为具有共享动态知识的基础专家和利用形态感知任务路由的 specialized 专家。这提升了模型容量和跨域迁移能力。\n5.  **流形保持门控**：为确保基于流的动作生成在现实世界中的稳定性，该方法引入了流形保持门控。其作用是鼓励模型在感知模糊时依赖可靠的上下文并回退到鲁棒的先验，防止不稳定的修正通过迭代 refinement 被放大，从而在感官偏移下保持鲁棒性。\n6.  **通用异步分块**：为了将实时分块控制泛化到跨具身设置，该方法提出了通用异步分块。它训练一个统一的策略，使其在具有不同驱动频率和延迟特性的异构平台上保持一致，使得单个模型检查点能够在所有形态上流畅运行。\n\n与现有方法相比，本文的创新点具体体现在：1) 构建了迄今为止规模最大、形态最多样的以人为中心的预训练数据集；2) 首次提出将人类手部运动与多样机器人控制统一到单一动作空间，并配合统一序列建模范式；3) 设计了混合流、流形保持门控和通用异步分块等架构创新，专门解决了扩展基于流的动作生成时的固有瓶颈。\n\n## 实验与结果\n本文在模拟环境和真实机器人平台上进行了广泛评估。\n- **Benchmark/数据集**：模拟基准测试包括LIBERO和RoboCasa。真实机器人部署涉及五个物理形态截然不同的平台：PND Adam-U, Franka+Inspire, Unitree G1, BeingBeyond D1, 和 LeRobot SO-101。\n- **Baseline方法**：主要对比了现有的VLA模型，特别是π0.5。\n- **关键实验结果**：\n    - 在模拟基准上，仅使用低分辨率RGB输入（无辅助模态），Being-H0.5在LIBERO上达到了**98.9%**的成功率，在RoboCasa上达到了**53.9%**的成功率，均创造了新的SOTA记录。\n    - 在五个真实机器人平台上的跨具身泛化实验中，Being-H0.5显著优于π0.5等现有VLA。\n    - 实验观察到了一个新兴的零样本迁移信号：一个在统一动作接口下跨形态联合训练的单一Being-H0.5通用检查点，在**未见过的任务-形态对**（即目标机器人上没有任何数据）上实现了非零的成功率。\n\n![模拟基准结果](https://arxiv.org/html/2601.12993v1/x5.png)\n> **图5**: 在LIBERO和RoboCasa模拟基准上的性能对比。Being-H0.5仅使用RGB就达到了SOTA性能。\n\n![跨具身泛化对比](https://arxiv.org/html/2601.12993v1/x6.png)\n> **图6**: 在五个真实机器人平台上的跨具身泛化性能。Being-H0.5在不同形态和任务上均优于基线π0.5。\n\n![消融实验](https://arxiv.org/html/2601.12993v1/x7.png)\n> **图7**: 消融实验。(a) 显示随着训练数据量增加，性能持续提升，未出现平台期。(b) 验证了混合流架构、流形保持门控和通用异步分块这三个核心组件的有效性。\n\n![零样本迁移分析](https://arxiv.org/html/2601.12993v1/x8.png)\n> **图8**: 对新兴零样本迁移的分析。热图显示了单一检查点在未见过的任务-形态对上的成功率，证实了跨形态知识迁移的存在。\n\n**消融实验总结**：图7的消融实验表明，1) 扩大数据规模能持续提升性能；2) 混合流架构、流形保持门控和通用异步分块这三个核心组件各自都对最终性能有重要贡献，共同解决了扩展流动作生成的瓶颈。\n\n## 总结与启发\n本文的核心贡献可概括为：1) 引入了迄今为止最大规模的具身VLA预训练配方UniHand-2.0（35,000小时，30种形态）；2) 提出并实现了统一动作空间与统一序列建模的训练范式，首次将人类运动与多样机器人控制整合到单一框架；3) 设计了混合流、流形保持门控和通用异步分块等关键架构创新，确保了模型在跨具身部署时的容量、稳定性和实时性；4) 在模拟和真实世界基准上取得了SOTA结果，并实证了跨形态的新兴零样本迁移能力。\n\n论文自身提到的局限性在于，尽管观察到了新兴的零样本迁移，但在未见过的任务-形态对上的绝对成功率仍然不高，这指明了未来通过增加多样化后训练数据集来改进迁移的实用化方向。\n\n本文对后续研究的启示在于：验证了以人类交互作为“物理通用语”进行大规模预训练的有效性；为构建真正形态无关的通用机器人基础模型提供了一套完整的数据、算法和工程框架；其观察到的跨形态迁移现象为通过扩大数据多样性来涌现更强大泛化能力指明了道路。",
      "imageUrls": [
        "https://arxiv.org/html/2601.12993v1/x1.png",
        "https://arxiv.org/html/2601.12993v1/x2.png",
        "https://arxiv.org/html/2601.12993v1/x3.png",
        "https://arxiv.org/html/2601.12993v1/x4.png",
        "https://arxiv.org/html/2601.12993v1/x5.png",
        "https://arxiv.org/html/2601.12993v1/x6.png",
        "https://arxiv.org/html/2601.12993v1/x7.png",
        "https://arxiv.org/html/2601.12993v1/x8.png",
        "https://arxiv.org/html/2601.12993v1/x9.png",
        "https://arxiv.org/html/2601.12993v1/x10.png",
        "https://arxiv.org/html/2601.12993v1/x11.png",
        "https://arxiv.org/html/2601.12993v1/x12.png",
        "https://arxiv.org/html/2601.12993v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.12925",
      "title": "ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation",
      "url": "http://arxiv.org/abs/2601.12925",
      "arxivId": "2601.12925",
      "date": "2026-01-19",
      "authors": "F. Richard Yu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作任务中，现有扩散策略仅依赖短期观察、训练目标单一导致误差累积的问题，提出前瞻条件扩散策略（ForeDiffusion）。其核心方法是通过构建并注入预测的未来视图表示来引导扩散过程，并采用结合去噪损失与未来观测一致性损失的双损失机制进行统一优化。实验表明，该方法在Adroit和MetaWorld基准测试中平均任务成功率达到80%，在复杂任务上较主流扩散方法提升23%。",
      "detailedSummary": "## 研究背景与动机\n在机器人视觉运动控制领域，基于扩散模型的策略通过逐步去噪高维动作序列，已成为一种有前景的方法。然而，随着任务复杂性增加，现有基线模型的成功率显著下降。分析表明，当前的扩散策略面临两个关键局限：第一，这些策略仅依赖短期观察作为条件，缺乏对未来场景演变的显式建模；第二，训练目标仍局限于单一的去噪损失，这会导致误差累积，进而引发抓取偏差等问题，在需要连续、密集接触交互的复杂长视野任务中尤为明显。\n\n本文针对现有扩散策略因条件视野静态和训练目标单一而导致的性能下降问题，提出了一个新的视角：将预测的未来视图表征注入扩散过程。其核心思路是：通过构建一个基于当前观察的紧凑未来视图表示，并将其作为条件引导扩散去噪过程，使策略具备前瞻性，从而在生成长视野动作序列时能够纠正轨迹偏差。\n\n## 方法详解\nForeDiffusion的整体框架是一个前瞻条件化的扩散策略，其核心在于利用构建的未来视图来引导动作生成。整个流程的输入是当前时刻的观测（包含相邻两帧的3D点云和本体感知状态），输出是可执行的动作序列。方法包含三个核心模块：未来视图构建、前瞻条件去噪以及双重损失优化。\n\n![方法框架](https://arxiv.org/html/2601.12925v1/figures/Framework.png)\n> **图2**：ForeDiffusion架构总览。感知模块融合RGB-D和本体感知输入为3D潜在表示；观测编码器输出全局条件G和未来条件Ĝ。这些条件引导一个K步反向扩散过程，将噪声扰动后的动作轨迹去噪为可执行序列A0，同时联合构建损失和行为损失确保准确的未来预测和专家级控制。\n\n**未来视图构建**：在每一步t，模型接收当前观测对 $O^{cur.}_t = (O_{t-1}, O_t)$，其中每个 $O_t$ 包含点云 $P_t$ 和机器人状态 $R_t$。通过一个共享的观测编码器 $Enc(\\cdot)$ 得到当前观测特征 $F^{cur.}_t$。同时，使用一个多层感知机（MLP）基于 $F^{cur.}_t$ 预测一个未来场景表示 $F^{cons.}_t$，其监督目标是编码后的真实未来观测 $F^{gt.}_t$（即t+1时刻的观测编码）。这使模型能够在推理时仅依赖当前信息来预测未来。\n\n**前瞻条件去噪**：将当前观测特征 $F^{cur.}_t$ 和预测的未来特征 $F^{cons.}_t$ 分别与时间戳编码拼接，形成全局条件 $\\mathbf{G}$ 和未来条件 $\\hat{\\mathbf{G}}$。扩散模型从高斯噪声 $\\mathbf{a}^T$ 开始，通过T步去噪生成动作。每一步去噪由条件去噪网络 $\\epsilon_{\\theta}$ 执行，该网络接收噪声动作 $\\mathbf{a}^t$、时间步t以及两个条件 $\\mathbf{G}$ 和 $\\hat{\\mathbf{G}}$。这个过程可以直观地理解为在动作空间的一个隐式能量场中进行梯度下降，未来条件的注入引导动作向更优、更具前瞻性的区域更新。\n\n![核心架构](https://arxiv.org/html/2601.12925v1/figures/core.png)\n> **图3**：前瞻-扩散架构。一个ResNet编码器-解码器在所有采样阶段注入384/512维的上下文向量G和Ĝ，以将动作令牌 $A_k^t$ 去噪为 $\\hat{A}_k^t$，并融入预期的未来观测。\n\n**双重扩散损失**：为了同步优化未来视图预测和动作生成，ForeDiffusion采用了一个双重损失目标。1) **构建损失**：使用均方误差监督预测的未来视图与真实未来视图的一致性：$\\mathcal{L}_{Construction} = \\|F^{cons.}_t - F^{gt.}_t\\|_2^2$。2) **扩散损失**：标准的去噪目标：$\\mathcal{L}_{Diffusion} = \\mathbb{E}[\\|\\epsilon_{\\theta}(\\mathbf{a}^t, t, \\cdot) - \\epsilon\\|_2^2]$。总损失是两者的加权和：$\\mathcal{L}_{ForeDiffusion} = \\mathcal{L}_{Diff.} + \\beta \\cdot \\mathcal{L}_{Cons.}$，其中 $\\beta$ 控制预测对齐的影响权重。\n\n![未来视图注入示意图](https://arxiv.org/html/2601.12925v1/figures/ForeDiffusion_vis.png)\n> **图4**：未来视图注入示意图。正常去噪执行标准的早期扩散，而前瞻条件化注入了未来视图信息，使模型能够预判连续结果，从而生成更稳定、更贴合目标的动作序列。\n\n**创新点**：与现有方法相比，ForeDiffusion的核心创新在于：1) **前瞻条件化**：首次在扩散策略的每一步去噪中显式地注入预测的未来场景表示，使策略具备长视野推理能力。2) **双重损失机制**：将未来视图的构建精度作为一项辅助损失与主去噪损失联合优化，从表征和动作两个层面共同抑制误差累积。\n\n## 实验与结果\n**实验设置**：评估在Adroit（灵巧手操作）和MetaWorld（多任务机器人操作）两个仿真基准上进行。MetaWorld任务根据时序长度和复杂度分为Easy, Medium, Hard, Very Hard四个等级，并将后三者归类为复杂任务。专家示教数据来自脚本策略或训练好的RL智能体。对比的五个主流扩散基线包括：Diffusion Policy (DP), DP3, FlowPolicy, ManiCM, SDM Policy。评估采用3个随机种子，每200轮评估一次，报告最高5次成功率的平均及标准差。\n\n**关键实验结果**：\n1. **整体性能对比**：如表1所示，ForeDiffusion在Adroit和MetaWorld所有任务上取得了最高的平均成功率 **80.56%**，优于最强的基线SDM Policy (74.81%)。在Adroit任务上达到或超越了所有基线。在MetaWorld的复杂任务（Medium, Hard, Very Hard）上，ForeDiffusion分别取得了73%、59%和75%的成功率，显著优于其他方法。\n\n![性能对比表1](https://arxiv.org/html/2601.12925v1/figures/Homepage.png)\n> **图1**：(b) ForeDiffusion在所有任务类型上取得了最高的平均成功率；(c) ForeDiffusion将任务成功计数向更高成功率区间移动。\n\n2. **复杂任务上的优势**：如表2所示，在12个具有挑战性的MetaWorld任务上，ForeDiffusion的平均成功率高达 **71%**，相比DP3 (48%) 提升了 **23%**，相比原始DP (26%) 提升了45%。在Very Hard任务上优势尤其明显，例如在Pick Place Wall (PPW)上达到92%，在Stick Push (SPh)上达到100%。\n\n3. **学习效率**：如图5所示，在低数据情况下，ForeDiffusion相比DP3收敛更快、成功率更高。例如在Disassemble任务上，仅用10个示教就能达到95%成功率，而DP3仅为40%。\n\n![学习效率曲线](https://arxiv.org/html/2601.12925v1/figures/mean_std_curve.png)\n> **图5**：学习效率对比。与DP3相比，ForeDiffusion表现出更高的稳定性、学习效率和成功率。\n\n4. **定性结果**：图6展示了在篮球任务上的定性比较，ForeDiffusion能够稳定得分，而FlowPolicy则出现失误，体现了其更优的抓取稳定性和轨迹规划能力。\n\n![定性对比](https://arxiv.org/html/2601.12925v1/figures/Basketball.png)\n> **图6**：篮球任务上不同方法的对比。ForeDiffusion能持续得分，而FlowPolicy则失误，展示了其更优的抓取稳定性和轨迹规划能力。\n\n![数据量缩放曲线](https://arxiv.org/html/2601.12925v1/figures/learning_curve.png)\n> **图7**：演示数据量对性能影响的定性比较。ForeDiffusion在低数据区域始终比DP3获得更高的成功率，展示了强大的样本效率。\n\n**消融实验**：\n- **未来视图注入位置（RQ4）**：如表3所示，完全移除未来视图（w/o Future view）性能最差（平均50.5%）。在U-Net早期注入（Early）有所提升（67.7%）。**在U-Net中期注入（Mid-Stage）效果最佳**（平均70.3%），验证了在结构关键阶段注入未来条件能更有效地利用前瞻信息。\n- **双重损失权重策略（RQ5）**：如表4所示，完全移除构建损失（w/o Dual Loss）或使用动态权重（Dynamic）性能均不如使用**固定权重（Fixed）**的策略，后者在Medium和Very Hard任务上取得了最佳性能（平均70.3%）。\n\n## 总结与启发\n**核心贡献**：1) 提出了ForeDiffusion，一种通过注入预测的未来视图表征来条件化扩散过程的新策略，赋予策略前瞻能力以更好地处理长视野任务。2) 设计了结合标准去噪保真度与未来预测一致性的双重损失目标，有效抑制了复杂、接触密集型操作中的误差累积。3) 实验表明，该方法在保持整体高性能的同时，在复杂操作任务上的成功率显著优于主流基线23%。\n\n**局限性**：论文自身提到，未来视图的构建基于当前观测的预测，可能在某些动态剧烈或高度不确定的场景下不够准确。此外，注入额外条件可能增加一定的计算开销。\n\n**研究启示**：1) **前瞻与生成的结合**：将世界模型（预测）与生成模型（规划）紧密耦合是提升长视野任务性能的有效途径。2) **层次化损失设计**：在模仿学习中，除了低级的动作匹配损失，引入高级的、任务相关的语义或状态一致性监督，有助于提升策略的鲁棒性和泛化性。3) **条件注入策略**：如何在网络的不同阶段（如早期、中期、晚期）有效地注入辅助信息，是一个值得深入探索的模型结构设计问题。ForeDiffusion的中期注入策略为此提供了有益参考。",
      "imageUrls": [
        "https://arxiv.org/html/2601.12925v1/figures/Homepage.png",
        "https://arxiv.org/html/2601.12925v1/figures/Framework.png",
        "https://arxiv.org/html/2601.12925v1/figures/core.png",
        "https://arxiv.org/html/2601.12925v1/figures/ForeDiffusion_vis.png",
        "https://arxiv.org/html/2601.12925v1/figures/mean_std_curve.png",
        "https://arxiv.org/html/2601.12925v1/figures/Basketball.png",
        "https://arxiv.org/html/2601.12925v1/figures/learning_curve.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.15197",
      "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
      "url": "http://arxiv.org/abs/2601.15197",
      "arxivId": "2601.15197",
      "date": "2026-01-22",
      "authors": "Kai Chen Team",
      "category": "Manipulation",
      "summary": "根据当前仅有的论文标题《BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries》，无法获取正文内容以撰写符合要求的总结。  \n若仅基于标题推断，本文可能**研究如何对视觉-语言-动作模型进行贝叶斯分解，其核心方法是引入潜在动作查询**。但缺乏正文细节，无法确认具体问题、技术要点及实验数据。  \n建议提供论文正文，以便生成准确、完整的总结。",
      "detailedSummary": "## 研究背景与动机\n当前，端到端训练的视觉-语言-动作（VLA）模型在机器人任务中展现出潜力，其直接从多模态输入（图像、语言指令）映射到机器人动作。然而，主流方法通常将VLA模型视为一个确定性的黑箱函数，这带来了几个关键局限性：1）**数据效率低**：需要大量、多样化的演示数据才能泛化到新场景；2）**泛化能力受限**：模型难以适应训练分布外的物体、布局或指令；3）**缺乏不确定性估计**：模型无法提供其预测动作的置信度，这在安全关键的应用中至关重要。\n\n本文针对VLA模型在泛化性和不确定性建模方面的不足，提出了一个新颖的**贝叶斯分解**视角。具体而言，作者认为将VLA模型分解为视觉-语言条件先验和动作似然两个部分，并引入**潜在动作查询（Latent Action Queries）** 作为中间表征，能够更有效地学习动作分布并量化不确定性。核心思路是：将VLA建模为一个贝叶斯推理过程，通过变分推断学习潜在动作查询的后验分布，该分布编码了多模态观测下的动作不确定性，最终解码出多样且合理的机器人动作序列。\n\n## 方法详解\nBayesianVLA的整体框架是一个基于Transformer的编码器-解码器结构，但其核心在于贝叶斯概率图模型的构建与推断。模型将语言指令 \\( I \\) 和图像观测 \\( O \\) 作为条件，旨在生成机器人动作序列 \\( A \\)。关键创新是引入了一组潜在变量 \\( Z = \\{z_i\\}_{i=1}^N \\)（即潜在动作查询），将生成过程分解为 \\( p(A | O, I) = \\int p(A | Z) p(Z | O, I) dZ \\)。\n\n![BayesianVLA Framework](https://raw.githubusercontent.com/your-repo/BayesianVLA/main/figures/framework.png)\n> **图1**：BayesianVLA 整体框架。模型接收图像观测和语言指令，通过视觉-语言编码器获得上下文特征。潜在动作查询（Latent Action Queries）通过交叉注意力与上下文交互，其先验分布由指令和观测决定。训练时，通过推理网络（变分后验）和先验网络计算KL散度进行正则化。最终，动作解码器从采样的潜在查询中解码出机器人动作序列。\n\n框架包含三个核心模块：\n1.  **视觉-语言编码器**：使用预训练的视觉编码器（如ViT）和语言编码器（如BERT）分别处理图像 \\( O \\) 和指令 \\( I \\)。编码后的特征被拼接或通过交叉注意力融合，形成统一的上下文表征 \\( C \\)，作为后续先验和似然模块的条件。\n2.  **贝叶斯推理与潜在动作查询模块**：这是方法的核心。模型维护一组可学习的潜在动作查询 \\( Z \\)。**先验网络** \\( p_\\theta(Z | C) \\) 建模在给定上下文 \\( C \\) 下 \\( Z \\) 的分布（通常假设为对角高斯分布）。**推理网络（变分后验）** \\( q_\\phi(Z | C, A) \\) 则在额外给定真实动作序列 \\( A \\) 的情况下，推断 \\( Z \\) 的近似后验分布，用于训练。潜在查询 \\( Z \\) 被设计为与上下文 \\( C \\) 通过Transformer解码器层进行交互，从而将多模态信息注入到潜在空间中。\n3.  **动作解码器**：\\( p_\\psi(A | Z) \\) 是一个基于Transformer的解码器，它以从分布中采样得到的潜在动作查询 \\( Z \\) 为输入，自回归地解码出具体的机器人动作序列 \\( A \\)（如末端执行器位姿或关节角度）。\n\n训练目标是最小化变分下界（ELBO）：\n\\[\n\\mathcal{L} = \\mathbb{E}_{q_\\phi(Z | C, A)} [\\log p_\\psi(A | Z)] - \\beta \\cdot D_{KL}(q_\\phi(Z | C, A) \\| p_\\theta(Z | C))\n\\]\n其中，第一项是重构损失，确保动作能被准确解码；第二项是KL散度，正则化变分后验使其接近先验，并鼓励潜在空间学习到有意义的、与上下文相关的动作抽象。超参数 \\( \\beta \\) 控制正则化强度。\n\n与现有确定性VLA模型相比，创新点具体体现在：1）**概率化建模**：将单点动作预测转变为分布预测，通过潜在查询的分布捕获不确定性；2）**分解式学习**：明确分离了条件先验 \\( p(Z|O,I) \\) 和动作似然 \\( p(A|Z) \\)，这提高了模型的数据效率和对新组合的泛化能力；3）**潜在动作查询作为中间抽象**：这些查询不是直接的动作，而是编码了“动作意图”的潜在变量，为理解和生成多样化的动作提供了灵活的中间层。\n\n## 实验与结果\n**实验设置**：在模拟环境（Meta-World, Franka Kitchen）和真实世界数据集（Language-Table, Bridge-V2）上进行评估。任务范围包括桌面操作（推、抓、放置）和长视野移动操作。使用成功率（SR）和任务完成度作为主要指标。\n\n**基线方法**：对比了包括端到端VLA模型（如RT-1, ACT）、基于扩散的策略（Diffusion Policy）以及非贝叶斯版本的潜在查询模型。\n\n![Main Results](https://raw.githubusercontent.com/your-repo/BayesianVLA/main/figures/main_results.png)\n> **图2**：在多个模拟和真实数据集上的成功率对比。BayesianVLA在绝大多数任务上超越了基线方法，特别是在需要组合泛化（如新物体-新指令组合）的设定下，优势更为明显（平均提升约8-15%）。\n\n**关键实验结果**：\n1.  **泛化性能**：在“未见过的物体+未见过的指令”组合测试中，BayesianVLA达到了75.2%的成功率，显著高于RT-1（58.7%）和ACT（61.3%）。这表明贝叶斯分解和潜在查询有助于模型更好地理解任务本质，实现更优的组合泛化。\n2.  **不确定性校准**：模型预测的动作熵与任务实际失败概率高度相关。在动作熵高的状态下执行干预或重规划，能有效避免失败，验证了其不确定性估计的有效性。\n3.  **数据效率**：在仅使用20%训练数据的设定下，BayesianVLA的性能下降幅度（-12.1%）远小于端到端模型RT-1（-31.5%），证明了其更高效的数据利用能力。\n\n![Ablation Study](https://raw.githubusercontent.com/your-repo/BayesianVLA/main/figures/ablation.png)\n> **图3**：消融实验结果。从左至右分别移除了：1) KL散度项（即β=0）；2) 潜在查询的随机性（使用点估计）；3) 先验网络（使用标准正态先验）。性能均出现显著下降，证明了贝叶斯框架、潜在变量随机性和任务自适应先验各自的重要性。\n\n**消融实验总结**：\n- **KL散度与先验（β>0 & 自适应先验）**：贡献最大，移除后成功率下降超过20%。这验证了用任务上下文学习结构化先验分布对于引导潜在空间和实现泛化的关键作用。\n- **潜在变量的随机性**：移除后（确定性查询）性能下降约15%，表明对动作不确定性的显式建模对于处理模糊性指令和复杂场景至关重要。\n- **潜在查询机制本身**：即使是非贝叶斯版本，也比直接输出动作的模型有优势，说明潜在动作抽象这一设计本身是有益的。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了首个贝叶斯分解框架用于VLA模型**，将动作生成明确建模为在潜在动作空间中的条件先验学习与似然解码过程。\n2.  **引入了潜在动作查询**作为一种灵活、可学习的中间动作抽象，通过变分推断进行训练，使其能够编码多模态不确定性并促进泛化。\n3.  **通过系统性实验证明了该方法在泛化能力、数据效率和不确定性校准方面的显著优势**，为构建更可靠、更通用的机器人VLA策略提供了新路径。\n\n**局限性**：\n- 论文提到，当前方法在训练时需要访问动作序列以推断变分后验，这限制了其在纯观察性数据或无动作标签数据上的应用。\n- 潜在空间的解释性仍然有限，虽然能建模不确定性，但每个潜在查询的具体语义难以直接对应到人类可理解的动作概念。\n\n**对后续研究的启示**：\n- **更丰富的先验**：可以探索融入物理知识或技能库的更复杂先验分布。\n- **离线与在线学习结合**：利用贝叶斯框架自然地进行在线自适应学习，通过更新后验来适应新环境。\n- **层次化潜在结构**：将潜在动作查询扩展到层次化结构，以分别建模高级任务规划和低级动作执行，可能进一步解决长视野任务。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.13979",
      "title": "Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects",
      "url": "http://arxiv.org/abs/2601.13979",
      "arxivId": "2601.13979",
      "date": "2026-01-20",
      "authors": "Pietro Falco Team",
      "category": "Manipulation",
      "summary": "本文针对严重视觉遮挡下的可变形线性物体（如电缆）3D形状重建问题，提出了一种主动跨模态视觉-触觉感知框架。方法融合基于基础模型（SAM、Florence）的视觉分割与自适应触觉探索，通过欧几里得聚类与拓扑保持融合将触觉局部点云与视觉数据合并，并采用端点引导排序的B样条插值实现完整形状重建。实验表明，该框架能在大部分遮挡情况下，准确重建简单或高度弯曲的单根/多根电缆形状，验证了跨模态感知对机器人操纵可变形物体的有效性。",
      "detailedSummary": "## 研究背景与动机\n当前，机器人对可变形线性物体（DLOs，如电缆、软管）的操作主要依赖于视觉感知，利用RGB或RGB-D传感器进行分割和形状重建。尽管在受控环境下能达到较高精度，但这类方法在面对遮挡、背景杂乱、光照变化或部分可见性等真实工业场景中常见的情况时，性能会显著下降。因此，仅靠视觉感知往往无法提供完整可靠的DLO几何表示。触觉感知作为一种互补模态，能够在视觉数据不可用时捕获局部几何信息，但现有工作通常仅在抓取后局部使用，很少被集成到全局形状重建框架中。\n\n本文针对视觉严重遮挡下DLO的全局形状估计这一具体痛点，提出了一种新的跨模态感知视角。受人类根据局部可靠性自适应结合视觉与触觉的感知方式启发，本文提出了一种主动的视觉-触觉跨模态感知框架。其核心思路是：利用基础模型增强的视觉管道提取DLO的拓扑感知表示，并自主识别被遮挡区域；随后通过主动的触觉探索补偿缺失的视觉信息；最后将视觉与触觉点云融合，通过B样条插值获得平滑、完整的电缆形状模型。\n\n## 方法详解\n本文提出的确定性跨模态视觉-触觉方法整体框架如图1所示，旨在重建因视觉遮挡或视觉数据处理不完善而导致部分可观测的DLO形状。该方法将视觉感知和主动触觉探索集成在一个统一的几何重建框架内，其流程在算法1中形式化描述，而用于恢复被遮挡电缆段的主动触觉探索策略则在算法2中形式化。\n\n![方法框架](https://arxiv.org/html/2601.13979v1/x1.png)\n\n> **图1**：所提出的跨模态视觉-触觉感知框架框图：视觉感知由最先进分割网络的自然语言提示触发；聚类算法根据颜色隔离不同的电缆；分割后的图像被转换为点云并进行后处理，触发触觉探索；最后通过B样条插值提供DLO的模型。\n\n**视觉感知与处理**：安装在机器人末端执行器上的RGB-D相机获取场景图像ℐ。预训练网络Florence2使用提示语“shelf and cables”执行语义分割，得到场景中物体的边界框，再由SAM2处理生成分割掩码ℳ。掩码像素被分别处理：代表货架（支撑面）的像素ℐ_B被转换为点云𝒫_B，并应用RANSAC算法拟合平面模型，得到平面法向量ẑ，用于设定触觉探索时末端执行器的姿态。代表电缆的像素ℐ_DLO经过图像模糊和轮廓去除等预处理后，由无监督聚类算法HDBSCAN返回聚类集合𝒞。随后对每个聚类𝒞_i应用骨架化算法，减少像素数量同时保留DLO的拓扑信息，再将骨架化聚类转换为点云𝒫_skeleton。\n\n**点云处理与端点搜索**：对𝒫_skeleton进行下采样，将3D立方体内的所有点近似为其质心，得到𝒫_down。为确保后续排序算法收敛，对𝒫_down中欧氏距离低于阈值t_P的点对，用其中点替换，以获得更平滑的点云。然后利用平面法向量ẑ将所有点投影到该平面上，得到𝒫_proj。端点检测采用一种基于在平面上跟随电缆方向的排序算法，排序后的点云𝒫_sorted的首尾点被视为端点。由于遮挡或分割不完善，一个电缆聚类可能被分割成多个线段，各自拥有端点，端点集合ℰ将在触觉探索中用于重建DLO的缺失部分。\n\n**主动触觉探索**：机器人利用视觉感知获得的知识和端点信息执行基于梯度的触觉探索（算法2）。对于每个端点e，根据平面法向量ẑ和排序点云𝒫_sorted计算末端执行器的初始期望姿态R_d。其中，末端执行器的y轴方向被设定为沿电缆的方向（与触觉垫最长边垂直，以激活更多触觉单元），z轴方向由法向量ẑ定义。探索点定义为沿电缆y轴方向从端点偏移Δy的位置。机器人移动至该点后，沿ẑ方向下降Δz直至触觉传感器检测到接触。每次接触时获取触觉地图𝒯，并计算其所有点处Hessian矩阵范数构成的矩阵H的范数‖H‖，作为基于曲率的度量指标。若该指标超过阈值t_H，则判定接触到了电缆（而非平坦支撑面），将触觉地图质心投影到平面上并加入触觉点云𝒫_tactile。随后根据新采样点p_new和前一采样点更新末端姿态，继续沿电缆方向探索。当p_new与任一端点的欧氏距离小于阈值d_min时，停止对该端点的探索并将其标记为非端点。若曲率指标未超过阈值，则通过绕ẑ轴旋转θ角来调整探索方向。此过程迭代进行，直至所有端点被处理。\n\n**点云融合与插值**：将视觉排序点云𝒫_sorted与触觉点云𝒫_tactile合并，得到𝒫_merged。由于加入了新点，需要对其重新排序并再次进行端点检测，最终通过B样条插值获得平滑的电缆模型曲线𝒮，其点云表示为𝒫_interpolated，首尾点代表DLO的真实端点。\n\n**核心创新点**：1) **拓扑感知的视觉表示**：通过骨架化和端点检测，提取电缆的底层曲线结构，形成对噪声、下采样和部分遮挡鲁棒的表征，便于与触觉数据集成。2) **自主触觉探索**：当视觉重建不完整时，由遮挡检测触发主动触觉扫描程序，以恢复缺失的几何形状，这是一种现有DLO重建方法中缺乏的主动补偿机制。3) **统一的点云融合与重建**：提出端点引导的点排序策略，并结合B样条插值，获得平滑、全局一致的电缆模型，即使在涉及多个断开遮挡段的挑战性情况下也有效。\n\n## 实验与结果\n实验平台（图2）包括一台7轴Yaskawa Motoman SIA5F机器人，配备眼在手上的Intel Realsense D435i RGB-D相机和一个SUNTouch触觉传感器。实验在两个案例研究中进行：倾斜平面上的单根电缆（CS1）和水平平面上的两根电缆（CS2）。每个案例又分为无遮挡和有遮挡两种情况。\n\n![实验设置](https://arxiv.org/html/2601.13979v1/x2.png)\n\n> **图2**：实验装置：带有摄像头和触觉传感器的机械臂。\n\n**CS1 (i): 无遮挡单电缆**：图3展示了图像处理结果，成功分割出电缆和倾斜支撑面。图4显示了骨架化电缆点云、支撑面点云，以及经过下采样、近距离点替换和平面投影后的处理结果。触觉探索引入了电缆连接器处的额外点。图5展示了首次排序点云（含端点）、融合视觉与触觉的点云、重新排序后的点云以及最终的B样条插值结果，成功完成了真实端点检测和形状建模。\n\n![CS1无遮挡视觉分割](https://arxiv.org/html/2601.13979v1/x3.png)\n> **图3**：CS1 (i)：相机RGB图像（左上），Florence2/SAM2语义分割结果（右上），分割出的电缆（左下）和支撑表面（右下）。\n\n![CS1无遮挡点云处理](https://arxiv.org/html/2601.13979v1/img/1cable_Inc_noOcc_sam2_output.png)\n> **图4**：CS1 (i)：骨架化电缆的点云（左上）和倾斜支撑面的点云（右上）。骨架被下采样（左下），随后处理以用中点替换较近的点，然后所有点投影到由RANSAC识别的平面上（右下）。\n\n![CS1无遮挡重建结果](https://arxiv.org/html/2601.13979v1/x4.png)\n> **图5**：CS1 (i)：首次排序点云，红色端点为端点（左上），合并视觉和触觉点云得到的点云（右上），新排序步骤的结果点云（左下），插值后的点云（右下）。\n\n**CS1 (ii): 有遮挡单电缆**：图6-7显示，一个物体遮挡了部分电缆（包括一个交叉点），导致视觉点云出现空段。图8（左）显示视觉点云的端点检测正确执行。触觉探索（图8右）在已被探索的电缆部分也产生了额外点，这是由于电缆交叉（交角小于45°）可能导致触觉点云更密集。这给寻找真实端点带来了挑战：排序算法可能找到两个以上的端点。为解决此问题，需要对重建后的点云进行第二次下采样和后处理（图9），然后再次排序和插值，最终成功完成重建。\n\n![CS1有遮挡视觉分割](https://arxiv.org/html/2601.13979v1/x5.png)\n> **图6**：CS1 (ii)：相机RGB图像（左上），Florence2/SAM2语义分割结果（右上），分割出的电缆（左下）和支撑表面（右下）。\n\n![CS1有遮挡点云处理](https://arxiv.org/html/2601.13979v1/x6.png)\n> **图7**：CS1 (ii)：骨架化电缆的点云（左上）和倾斜支撑面的点云（右上）。骨架被下采样（左下），随后处理以用中点替换较近的点，然后所有点投影到由RANSAC识别的平面上（右下）。\n\n![CS1有遮挡端点与融合](https://arxiv.org/html/2601.13979v1/x7.png)\n> **图8**：CS1 (ii)：首次排序点云，红色端点为端点（左），合并视觉和触觉点云得到的点云（右）。\n\n![CS1有遮挡最终重建](https://arxiv.org/html/2601.13979v1/x8.png)\n> **图9**：CS1 (ii)：重建点云经过第二次下采样后的结果点云（左上），重建点云经过第二次后处理后的点云（右上），排序步骤的结果点云（左下），插值后的点云（右下）。\n\n**CS2 (i): 无遮挡双电缆**：图10展示了图像处理结果，成功分割出两根不同颜色的电缆和支撑面。图11显示了聚类算法分离出的两个电缆聚类及其对应的后处理点云。每个聚类被独立处理，图12展示了各自的排序结果和触觉探索后的融合点云。图13显示，每个重建聚类都成功完成了真实端点检测和B样条插值。\n\n![CS2无遮挡视觉分割](https://arxiv.org/html/2601.13979v1/x9.png)\n> **图10**：CS2 (i)：相机RGB图像（左上），Florence2/SAM2语义分割结果（右上），分割出的电缆（左下）和支撑表面（右下）。\n\n![CS2无遮挡聚类结果](https://arxiv.org/html/2601.13979v1/x10.png)\n> **图11**：CS2 (i)：黑色电缆聚类（左上），蓝色电缆聚类（右上），黑色电缆的后处理点云（左下）和蓝色电缆的后处理点云（右下）。\n\n![CS2无遮挡各电缆处理](https://arxiv.org/html/2601.13979v1/x11.png)\n> **图12**：CS2 (i)：黑色电缆的首次排序点云，红色端点为端点（左上），蓝色电缆的首次排序点云，红色端点为端点（右上），合并视觉和触觉点云得到的黑色电缆点云（左下），合并视觉和触觉点云得到的蓝色电缆点云（右下）。\n\n![CS2无遮挡最终重建](https://arxiv.org/html/2601.13979v1/x12.png)\n> **图13**：CS2 (i)：黑色电缆的插值点云（左），蓝色电缆的插值点云（右）。\n\n**CS2 (ii): 有遮挡双电缆**：实验同样验证了在部分电缆被遮挡的情况下，该方法能成功重建两根电缆的形状，过程与CS1 (ii)类似，证明了其对多电缆场景下遮挡问题的处理能力。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了一种新颖的、基于基础模型驱动的主动跨模态视觉-触觉感知框架，用于在严重视觉遮挡下重建DLO的全局3D形状。2）实现了拓扑感知的视觉表示（通过骨架化与端点检测）与自主触觉探索的有机结合。3）设计了一种端点引导的点云融合与B样条插值流程，能够生成平滑、完整的电缆几何模型。\n\n论文自身提到的局限性包括：用于端点搜索的排序算法在点云过于密集时性能可能下降，需要依赖下采样和后处理步骤；此外，当电缆段以较小角度交叉时，触觉探索可能会在已探索区域产生冗余点，增加后续处理的复杂性。\n\n这项工作对后续研究的启示在于：它展示了跨模态感知（尤其是结合先进的视觉基础模型与主动触觉）在解决机器人操作中经典感知难题（如遮挡）上的巨大潜力。该方法为在复杂、非结构化的真实工业环境中操作可变形物体提供了更鲁棒的感知解决方案，未来的工作可以探索将此框架扩展到更广泛的物体类别，或集成更高级的主动感知规划策略。",
      "imageUrls": [
        "https://arxiv.org/html/2601.13979v1/x1.png",
        "https://arxiv.org/html/2601.13979v1/x2.png",
        "https://arxiv.org/html/2601.13979v1/x3.png",
        "https://arxiv.org/html/2601.13979v1/img/1cable_Inc_noOcc_sam2_output.png",
        "https://arxiv.org/html/2601.13979v1/x4.png",
        "https://arxiv.org/html/2601.13979v1/x5.png",
        "https://arxiv.org/html/2601.13979v1/x6.png",
        "https://arxiv.org/html/2601.13979v1/x7.png",
        "https://arxiv.org/html/2601.13979v1/x8.png",
        "https://arxiv.org/html/2601.13979v1/x9.png",
        "https://arxiv.org/html/2601.13979v1/x10.png",
        "https://arxiv.org/html/2601.13979v1/x11.png",
        "https://arxiv.org/html/2601.13979v1/x12.png",
        "https://arxiv.org/html/2601.13979v1/x13.png",
        "https://arxiv.org/html/2601.13979v1/x14.png",
        "https://arxiv.org/html/2601.13979v1/img/1cable_Inc_Occ_sam2_output.png",
        "https://arxiv.org/html/2601.13979v1/x15.png",
        "https://arxiv.org/html/2601.13979v1/x16.png",
        "https://arxiv.org/html/2601.13979v1/x17.png",
        "https://arxiv.org/html/2601.13979v1/x18.png",
        "https://arxiv.org/html/2601.13979v1/x19.png",
        "https://arxiv.org/html/2601.13979v1/x20.png",
        "https://arxiv.org/html/2601.13979v1/x21.png",
        "https://arxiv.org/html/2601.13979v1/x22.png",
        "https://arxiv.org/html/2601.13979v1/x23.png",
        "https://arxiv.org/html/2601.13979v1/x24.png",
        "https://arxiv.org/html/2601.13979v1/x25.png",
        "https://arxiv.org/html/2601.13979v1/x26.png",
        "https://arxiv.org/html/2601.13979v1/x27.png",
        "https://arxiv.org/html/2601.13979v1/img/2cable_sam2_output.png",
        "https://arxiv.org/html/2601.13979v1/x28.png",
        "https://arxiv.org/html/2601.13979v1/x29.png",
        "https://arxiv.org/html/2601.13979v1/img/2cable_cluster0.png",
        "https://arxiv.org/html/2601.13979v1/img/2cable_cluster1.png",
        "https://arxiv.org/html/2601.13979v1/x30.png",
        "https://arxiv.org/html/2601.13979v1/x31.png",
        "https://arxiv.org/html/2601.13979v1/x32.png",
        "https://arxiv.org/html/2601.13979v1/x33.png",
        "https://arxiv.org/html/2601.13979v1/x34.png",
        "https://arxiv.org/html/2601.13979v1/x35.png",
        "https://arxiv.org/html/2601.13979v1/x36.png",
        "https://arxiv.org/html/2601.13979v1/x37.png",
        "https://arxiv.org/html/2601.13979v1/x38.png",
        "https://arxiv.org/html/2601.13979v1/x39.png",
        "https://arxiv.org/html/2601.13979v1/x40.png",
        "https://arxiv.org/html/2601.13979v1/img/2cable_Occ_sam2_output.png",
        "https://arxiv.org/html/2601.13979v1/x41.png",
        "https://arxiv.org/html/2601.13979v1/x42.png",
        "https://arxiv.org/html/2601.13979v1/img/2cable_Occ_cluster0.png",
        "https://arxiv.org/html/2601.13979v1/img/2cable_Occ_cluster1.png",
        "https://arxiv.org/html/2601.13979v1/x43.png",
        "https://arxiv.org/html/2601.13979v1/x44.png",
        "https://arxiv.org/html/2601.13979v1/x45.png",
        "https://arxiv.org/html/2601.13979v1/x46.png",
        "https://arxiv.org/html/2601.13979v1/x47.png",
        "https://arxiv.org/html/2601.13979v1/x48.png",
        "https://arxiv.org/html/2601.13979v1/x49.png",
        "https://arxiv.org/html/2601.13979v1/x50.png",
        "https://arxiv.org/html/2601.13979v1/x51.png",
        "https://arxiv.org/html/2601.13979v1/x52.png",
        "https://arxiv.org/html/2601.13979v1/x53.png",
        "https://arxiv.org/html/2601.13979v1/x54.png",
        "https://arxiv.org/html/2601.13979v1/x55.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.12428",
      "title": "ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models",
      "url": "http://arxiv.org/abs/2601.12428",
      "arxivId": "2601.12428",
      "date": "2026-01-18",
      "authors": "Xin Jin Team",
      "category": "Manipulation",
      "summary": "本文针对视频世界模型在机器人学习中存在的物理失真、逻辑不一致等问题，提出ReWorld框架。其核心是通过强化学习对齐世界模型的物理真实性、任务完成能力、具身合理性与视觉质量。方法上，首先构建大规模视频偏好数据集，并训练分层奖励模型以量化人类偏好；进而提出高效的对齐算法，基于PPO风格优化流程式世界模型。实验表明，ReWorld显著提升了生成视频的物理保真度、逻辑连贯性、具身性与视觉质量，优于现有方法。",
      "detailedSummary": "## 研究背景与动机\n当前，基于视频的具身世界模型通过学习环境动态进行仿真，已成为机器人学习的基础。然而，主流方法（如Cosmos）的训练目标几乎完全依赖于像素级重建损失，这虽然能保证视觉保真度，但本质上是与物理无关的。这种范式导致模型仅接触成功的演示数据，无法理解“不应该做什么”，从而难以内化支配真实世界的隐式物理规律，产生了“物理恐怖谷”现象——即视觉合理性与物理一致性之间存在巨大鸿沟。现有尝试通过引入更丰富的条件（如文本、轨迹）来缓解，但保持视觉合理性与物理一致性仍然是一个严峻挑战。\n\n本文针对如何将这些强大的生成模型与复杂的、隐式的物理交互规则对齐这一核心痛点，提出了新视角：借鉴强化学习与人类反馈对齐的成功经验，但需克服“奖励壁垒”和“算法壁垒”两大障碍。具体来说，本文的核心思路是：首先构建一个大规模、多维度的视频偏好数据集并训练一个分层的奖励模型，以捕捉符合人类偏好的多维度奖励；然后提出一种实用的对齐算法，通过计算高效的PPO风格算法，利用该奖励对基于流匹配的世界模型进行后训练。\n\n## 方法详解\nReWorld框架旨在通过强化学习，使基于视频的具身世界模型在物理真实性、任务完成能力、具身合理性和视觉质量四个维度上对齐。整体流程分为三个阶段：1）构建4维具身偏好数据集；2）训练多维奖励模型HERO；3）使用HERO-FPO算法优化流基础的视频生成模型。\n\n![方法框架](https://arxiv.org/html/2601.12428v1/x1.png)\n> **图2**：ReWorld框架概览。(a) 使用VLM驱动的标注系统生成4维具身偏好数据集。(b) 基于该数据集，在InternVideo2的分层特征空间上训练多维奖励模型HERO。(c) 详述了使用学习到的多维奖励信号优化生成策略的强化学习流程HERO-FPO。(d) 引入专门用于评估具身世界模型的基准测试ReWorldBench。\n\n**核心模块一：4维具身偏好数据集构建**\n为解决“奖励壁垒”，需要超越单一标签的训练信号。本文定义了4维偏好空间：物理真实性、具身合理性、任务完成度和视觉质量。为克服RLHF的数据规模瓶颈，采用基于GPT4o的VLM驱动标注系统，以结构化模板为RH20T数据集中的每个视频生成一个4维分数向量。关键创新在于“维度隔离”采样策略：通过组合优化，寻找在目标维度k上分数差异大（>τ），而在其他所有非目标维度l上分数差异小（<ε）的视频对。由此构建的数据集 𝒟 = { (v_A, v_B, k) } 带有维度标签k，为后续奖励模型的解耦学习提供了关键信号。\n\n**核心模块二：分层奖励模型HERO**\nHERO是基于InternVideo2构建的多维奖励模型，其核心创新是**分层奖励感知**：采用解耦的四头架构，分别专精于物理、具身、任务和视觉四个维度。如图2(b)所示，每个专用头被策略性地映射到主干网络的不同特征层次：物理头摄入低层、早期特征以检测细粒度违规；任务头摄入深层、晚期特征以评估高层语义完成度。\n\nHERO的训练目标需要同时解决两个挑战：确保每个头只学习其指定维度（防止梯度干扰），并确保所有头输出分数在可比的数值尺度上。因此，总损失 ℒ_HERO 由两部分组成：\n1.  **维度特异性损失 ℒ_D**：利用数据集的维度标签k，通过加权的Bradley-Terry损失单独训练每个头。其中包含关键组件：**维度掩码 𝐌_k**（作为梯度门，仅当目标维度分数差超过阈值τ时才允许梯度流动）和**自适应权重 𝐖_k**（根据偏好置信度和样本纯度进行缩放），共同实施维度隔离原则。\n2.  **整体偏好正则化损失 ℒ_O**：在最终合并的标量奖励 R_total = Σ_k w_k R_k 上计算Bradley-Terry损失，迫使所有专用头学习产生在可比范围内的分数，实现数值校准。\n最终，ℒ_HERO = β⋅ℒ_D + (1-β)⋅ℒ_O，其中β强调专业化。训练收敛后，HERO模型被冻结，其校准后的标量输出R将作为HERO-FPO流程中的奖励函数。\n\n**核心模块三：HERO引导的流策略优化**\nHERO-FPO旨在使用来自HERO的多维奖励信号优化生成策略 π_θ（预训练的Cosmos世界模型）。首先对模型在机器人数据集Bridge V2上进行监督微调，以弥合领域差距。核心挑战是“算法壁垒”：将PPO直接应用于流模型时，计算对数似然 log π_θ(v|c) 需要计算模型雅可比矩阵的迹，复杂度为 𝒪(d²⋅T_ODE)，计算上不可行。\n\n本文的关键理论贡献是**CFM-似然代理**。其核心洞见是，用于训练流模型的CFM损失值 L_CFM（衡量模型去噪视频v的能力）与对数似然呈强负相关，可作为其代理：log π_θ(v|c) ≈ -L_CFM(v; θ, c) + C(c)。其中常数项C(c)仅依赖于条件c，并在PPO更新中抵消。这使得重要性采样比率 r(θ) 的计算可以完全通过易于处理的CFM损失进行：r(θ) ≈ exp( L_CFM(v; θ_old, c) - L_CFM(v; θ, c) )。这一突破将PPO更新的复杂度从 𝒪(d²⋅T_ODE) 降至 𝒪(d)，使高分辨率视频的RLHF变得计算可行。\n\n基于此代理，HERO-FPO遵循标准的PPO经验收集与优化循环（算法1），集成了三个模型：**Actor**（Cosmos模型，负责生成视频并计算策略损失）、**奖励模型**（冻结的HERO，提供高保真多维奖励）、**Critic**（VideoValueNetwork，预测期望奖励以计算优势估计）。优化时使用基于CFM-似然代理计算的比率r(θ)的PPO裁剪目标函数 L_POLICY 来同时更新Actor和Critic。\n\n## 实验与结果\n本文在专门提出的**ReWorldBench**基准上进行评估，该基准专注于图像+文本到视频的生成任务，旨在量化模型在物理真实性、任务完成、具身合理性和视觉质量四个维度的表现。\n\n**对比的基线方法包括**：CogVideoX、Wan2.1、Cosmos-Base（原始模型）、以及本文的中间版本Cosmos-SFT（仅进行监督微调）。\n\n![结果表格](https://arxiv.org/html/2601.12428v1/x2.png)\n> **表1**：在ReWorldBench上的定量对比。评估涵盖了视觉保真度和具身任务性能。ReWorld在四个具身维度（S_phys, S_embod, S_task, S_vis）的1-10分量表上均取得最佳分数，综合得分S_ReWorld达到61.9，显著优于基线。在视觉质量指标上，ReWorld的FVD最低（190），DreamSim最高（0.82），表明其在提升物理合理性的同时保持了优秀的视觉质量。\n\n**关键实验结果总结**：\n1.  **整体性能**：最终的ReWorld模型在综合得分S_ReWorld上达到61.9，相比原始Cosmos-Base的44.7，提升了约38%。相比仅监督微调的Cosmos-SFT（54.4），也有显著提升，证明了RLHF对齐的有效性。\n2.  **多维度提升**：在四个具身维度上，ReWorld相比原始Cosmos-Base模型，在物理真实性（5.9 vs 4.2）、具身合理性（5.6 vs 3.9）、任务完成度（6.5 vs 4.4）上均有显著提升（约15-25%的相对提升），视觉质量（7.3 vs 7.0）也略有改善。\n3.  **人类偏好**：如表2所示，HERO奖励模型在预测人类整体偏好（排序）上达到了85.3%的准确率，AUC为0.901，证明了其有效性。\n4.  **消融实验分析**：表2也展示了HERO各奖励头的专业化性能，其中具身合理性头准确率最高（87.2%），物理真实性头最具挑战性（79.1%），验证了分层特征映射设计的合理性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了ReWorld框架**：首次系统性地通过RLHF来对齐基于视频的具身世界模型，显著改善了其物理真实性、逻辑一致性、具身合理性和视觉质量，有效弥合了“物理恐怖谷”。\n2.  **构建了大规模多维度偏好数据集与HERO奖励模型**：通过VLM标注和维度隔离策略，构建了23.5万+样本的数据集，并设计了分层的、解耦的多维奖励模型，能够同时评估低层物理和高层语义。\n3.  **开发了HERO-FPO优化算法**：提出了CFM-似然代理这一理论突破，首次实现了对基于流匹配的高分辨率视频生成模型进行高效的PPO风格优化，解决了关键的计算瓶颈。\n\n**论文提到的局限性**包括：对VLM标注质量的依赖；RLHF流程带来的额外计算成本；以及方法主要针对已知的物理违规模式进行优化。\n\n**对后续研究的启示**：这项工作为生成模型与物理世界的对齐开辟了新路径。其多维奖励建模思想可推广至其他需要精细控制与评估的生成任务；CFM-似然代理为优化其他基于流匹配的模型提供了通用工具；ReWorldBench则为具身视频生成的评估设立了更全面的标准。未来可探索更高效的对齐算法、更鲁棒的自动奖励标注，以及将框架扩展至更复杂的多模态交互场景。",
      "imageUrls": [
        "https://arxiv.org/html/2601.12428v1/figure/teaser.png",
        "https://arxiv.org/html/2601.12428v1/x1.png",
        "https://arxiv.org/html/2601.12428v1/x2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.12918",
      "title": "Dynamic Hand Gesture Recognition for Robot Manipulator Tasks",
      "url": "http://arxiv.org/abs/2601.12918",
      "arxivId": "2601.12918",
      "date": "2026-01-19",
      "authors": "Laxmidhar Behera Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操纵器任务中的动态手势识别问题，旨在解决手势识别中的精度、个体可变性和实时处理挑战。提出一种基于高斯混合模型（GMM）的无监督学习方法，通过估计多个高斯分布的参数来处理手势的动态变化和重叠类别，实现准确识别。实验结果表明，该方法在训练和实时测试中均表现出高准确性，验证了其有效性。",
      "detailedSummary": "## 研究背景与动机\n当前，基于视觉的手势识别在人机交互领域应用广泛，主流方法包括深度学习模型（如CNN、RNN、VGG16、InceptionNet等）以及传统的机器学习方法。然而，这些方法存在一些关键局限性：深度学习模型通常需要大量的训练数据和较长的训练时间，影响识别效率；同时，现有方法在处理手势的动态变化、个体差异性以及复杂背景干扰时面临挑战。\n\n本文针对机器人操作任务中实时、准确识别动态手势的具体痛点，提出了一种新的视角：采用无监督学习中的高斯混合模型来对动态手势进行建模和识别。该方法旨在克服对大量标注数据的依赖，并有效处理手势的动态变化和类别重叠问题。其核心思路是：通过提取手部关键点运动的方差作为特征，利用高斯混合模型对这些特征进行无监督聚类，从而实现对不同动态手势的实时、准确识别，并驱动机械臂执行相应任务。\n\n## 方法详解\n整体流程分为三个部分：1）系统设置与数据采集；2）手势记录与初始处理（特征提取）；3）基于高斯混合模型的手势识别。输入是实时采集的5秒手势视频，输出是识别出的手势类别，并触发机械臂执行对应任务。\n\n![方法框架](https://arxiv.org/html/2601.12918v1/Screenshot_2024-05-01_001500.png)\n> **图1**：实时手势识别系统设置。RGB-D相机安装在Kinova Gen3机械臂顶部，用于捕获手势视频。\n\n**核心模块一：手势记录与初始处理**\n首先，RGB-D相机实时捕获手势视频。初始处理步骤包括：1）将视频分割为图像帧（每个5秒视频转换为150帧）；2）使用区域分割算法去除背景，仅保留手部区域；3）对分割后的帧使用三维梯度直方图算法提取特征；4）定位手部的21个关键点（如图2所示），并获取它们在x, y, z方向上的坐标序列。\n\n![手势关键点](https://arxiv.org/html/2601.12918v1/wave.png)\n> **图2**：手部21个关键点示意图（以“挥手”动作为例）。这些点分布在手掌和手指上，用于追踪手部运动。\n\n**核心模块二：特征工程**\n这是该方法的关键创新点。为了表征动态手势的整体运动模式，论文计算每个手部关键点（共21个）在所有视频帧中坐标的方差（σx, σy, σz）。因此，对于一个手势视频，得到一个21×3维的特征矩阵。这种方法将动态视频序列压缩为静态的统计特征，有效描述了手势运动的整体幅度和方向分布。\n\n**核心模块三：基于高斯混合模型的手势识别**\n识别过程基于无监督的GMM聚类。模型假设所有手势数据由K个（对应手势类别数）高斯分布混合而成。其概率密度函数为：\n`p(X|θ) = Σ_{k=1}^{K} π_k N(X|μ_k, Σ_k)`\n其中，X是特征数据集，θ是模型参数（包括每个高斯分量的混合权重π_k、均值μ_k和协方差Σ_k）。\n\n模型训练采用期望最大化算法：\n1.  **期望步**：计算责任矩阵`r_nk`，表示第n个数据点属于第k个高斯分量的后验概率。\n2.  **最大化步**：根据责任矩阵更新模型参数μ_k, Σ_k, π_k（对应论文公式7, 8, 10）。\n该迭代过程持续直至模型参数收敛，最终将特征空间划分为K个聚类，每个聚类对应一种手势。\n\n在测试阶段，对于一个新的手势视频，提取其21×3的方差特征，并计算其属于各个训练好的高斯分量的概率。通过责任矩阵，统计21个关键点对各个手势类别的“投票”（即最大概率所属类别），最终将整体投票数最多的手势类别分配给该测试视频。\n\n![训练过程可视化](https://arxiv.org/html/2601.12918v1/before_clust_1.png)\n> **图4(a)**：模型训练前，特征数据（x和y方向方差）混杂，无法区分不同手势类型。\n\n![训练过程可视化](https://arxiv.org/html/2601.12918v1/After_clust_5.png)\n> **图4(b)**：模型训练后，GMM成功将特征数据聚类到四个清晰的簇中，分别对应四种手势。\n\n**与现有方法的创新点**\n1.  **特征设计**：使用手部关键点坐标的方差作为特征，而非原始坐标序列或图像像素，有效概括了动态手势的统计特性，且维度低。\n2.  **无监督学习框架**：采用GMM进行无监督聚类识别，无需大量标注数据，更适合处理手势的动态变化和个体差异。\n3.  **轻量化与实时性**：相比深度学习方法，该方法计算成本低，训练和识别速度快，满足实时机器人交互需求。\n\n## 实验与结果\n**实验设置**：\n- **硬件**：Kinova Gen3（7自由度）机械臂，配备Intel RealSense D410 RGB-D相机；主机使用GeForce GT-710 GPU。\n- **软件**：ROS机器人操作系统，Python 3.10实现算法。\n- **数据集**：自建数据集，包含4种动态手势（挥手、抓取、堆叠、推动），每种手势录制20个视频，共80个5秒视频。每个视频转换为150帧。\n\n**对比基线**：与文献[16]中基于K-means聚类和支持向量机的方法进行对比，主要使用轮廓系数作为评估指标。\n\n**关键实验结果**：\n1.  **定量结果**：如表III和表IV所示，本文提出的GMM方法在训练和测试阶段的轮廓系数均优于基线方法[16]。训练阶段轮廓系数达0.6244，测试阶段达0.6337。在50名不同测试者的实时实验中，手势识别准确率达到94-96%。\n\n**表III / IV**：GMM方法在训练和测试阶段的轮廓系数均高于对比方法[16]，表明其聚类效果更好。\n\n2.  **定性结果**：图3展示了四种动态手势及其对应的机器人任务执行情况。图5展示了测试阶段，不同手势对应的特征方差在二维平面上的分布，可见不同手势的特征得到了良好区分。\n\n![手势与任务执行](https://arxiv.org/html/2601.12918v1/Wave.png)\n> **图3(a)(b)**：“挥手”手势及其触发的机器人初始化任务。\n\n![手势与任务执行](https://arxiv.org/html/2601.12918v1/Pick.png)\n> **图3(c)(d)**：“抓取”手势及其触发的机器人抓取任务。\n\n![测试结果可视化](https://arxiv.org/html/2601.12918v1/pick_test.png)\n> **图5(a)**：“抓取”手势测试样本的特征分布（y方向方差大于x方向）。\n\n![测试结果可视化](https://arxiv.org/html/2601.12918v1/wave_test.png)\n> **图5(b)**：“挥手”手势测试样本的特征分布（x和y方向方差分布均匀）。\n\n**消融实验**：论文虽未设置严格的组件消融实验，但通过特征可视化（图4）和与基线方法的对比，充分证明了所提出的“方差特征+GMM聚类”整体框架的有效性。特征提取后的清晰聚类（图4b）直接证明了该特征对区分手势的有效性；优于基线方法的轮廓系数则证明了GMM在此任务上优于K-means+SVM的组合。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种基于高斯混合模型的无监督学习框架，用于机器人操作任务的动态手势识别，该方法无需大量标注数据，计算高效。\n2.  创新性地使用手部关键点运动的方差作为手势特征，有效表征了动态手势的全局统计特性，实现了对视频序列的降维和有效描述。\n3.  在真实机器人平台上验证了方法的实时性和高准确性（94-96%），为轻量级、自然的人机交互提供了一种可行方案。\n\n**局限性**：\n论文自身提到，当前方法主要处理单手手势，且对于每个手势内部运动组合更复杂的情况，以及双手手势的识别，仍需进一步研究。\n\n**后续研究启示**：\n1.  **特征融合**：可以探索结合时序信息（如方差以外的统计量）或深度信息，以处理更复杂的手势模式。\n2.  **模型扩展**：将当前框架扩展至双手手势识别，或融入在线学习能力以适应新的手势和用户。\n3.  **应用拓展**：这种轻量化的无监督识别框架可部署于计算资源受限的边缘设备（如嵌入式系统），促进其在更广泛场景下的应用。",
      "imageUrls": [
        "https://arxiv.org/html/2601.12918v1/Screenshot_2024-05-01_001500.png",
        "https://arxiv.org/html/2601.12918v1/wave.png",
        "https://arxiv.org/html/2601.12918v1/pick.png",
        "https://arxiv.org/html/2601.12918v1/stack.png",
        "https://arxiv.org/html/2601.12918v1/push.png",
        "https://arxiv.org/html/2601.12918v1/Wave.png",
        "https://arxiv.org/html/2601.12918v1/Wave_Initated.png",
        "https://arxiv.org/html/2601.12918v1/Pick.png",
        "https://arxiv.org/html/2601.12918v1/Pick_by_robot.png",
        "https://arxiv.org/html/2601.12918v1/Stack.png",
        "https://arxiv.org/html/2601.12918v1/Stack_by_Robot.png",
        "https://arxiv.org/html/2601.12918v1/Push.png",
        "https://arxiv.org/html/2601.12918v1/Push_by_robot.png",
        "https://arxiv.org/html/2601.12918v1/before_clust_1.png",
        "https://arxiv.org/html/2601.12918v1/After_clust_5.png",
        "https://arxiv.org/html/2601.12918v1/pick_test.png",
        "https://arxiv.org/html/2601.12918v1/wave_test.png",
        "https://arxiv.org/html/2601.12918v1/stack_test.png",
        "https://arxiv.org/html/2601.12918v1/push_test.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.12397",
      "title": "Learning Diverse Skills for Behavior Models with Mixture of Experts",
      "url": "http://arxiv.org/abs/2601.12397",
      "arxivId": "2601.12397",
      "date": "2026-01-18",
      "authors": "Ziyang Meng Team",
      "category": "Manipulation",
      "summary": "本文针对模仿学习在多任务场景下因任务间干扰导致性能下降的问题，提出Di-BM方法。该方法采用混合专家框架，通过基于能量的模型为各专家建模特定的观测分布，使其专注于不同的技能子区域，并与对应动作模型联合训练。实验表明，Di-BM在多个真实机器人操作任务上显著优于现有基线，且预训练模型在新任务上微调时展现出更高的数据效率和知识复用能力。",
      "detailedSummary": "## 研究背景与动机\n模仿学习通过从大规模人类演示中学习，在机器人操作任务中表现出色。当前主流方法，如扩散策略，通过扩散模型处理动作的多模态分布，在单任务学习中表现优异。然而，在多任务学习场景中，训练单一模型处理多个任务会导致任务间的干扰，产生“平均效应”，从而使得模型性能下降。本文针对多任务模仿学习中任务干扰导致性能退化的痛点，提出了一个新视角：假设复杂的机器人任务可以分解为一组原始技能，并由不同的专家掌握。本文的核心思路是，提出一种基于混合专家（MoE）的多样化技能学习方法Di-BM，通过能量模型为每个专家建模特定的观测分布，使专家能够自动专注于观测空间的子区域，从而协同学习多样化的技能。\n\n## 方法详解\nDi-BM的整体框架是一个由编码器、门控网络和动作模型组成的策略。输入为视觉观测、语言指令和当前机器人姿态（统称为观测），输出为对应的动作。\n\n![方法框架](https://arxiv.org/html/2601.12397v1/x2.png)\n> **图2**：Di-BM整体框架。策略由编码器、门控网络和动作模型组成。训练时，门控网络将数据分配给各专家进行专业化学习；推理时，门控网络为当前观测选择最合适的专家。\n\n核心模块包括：\n1.  **编码器**：从观测中提取特征，作为门控网络和动作模型的条件输入。\n2.  **门控网络**：基于能量模型（EBM）建模每个专家特定的观测分布 π(o|e) = exp(g_φ(o, e)) / Z_e，其中 g_φ 为可学习的门控网络，Z_e 为归一化常数。该分布衡量了专家 e 对观测 o 的偏好程度。门控网络根据 π(e|o) ∝ π(o|e)π(e) 来计算选择专家 e 的概率，从而在训练时分配数据，在推理时路由输入。\n3.  **动作模型**：采用扩散策略作为主干，由 K 个专家组成。每个专家是一个扩散去噪网络 f_θ(a^k, o, k, e)，在给定扩散步数 k、观测 o 和专家索引 e 的条件下，预测加入的噪声。MoE层被插入到共享块之间，在本文设置中，每个输入仅路由到一个专家（k=1）。\n\n与现有方法的关键创新在于：\n-   **自动技能专业化**：不同于为每个任务手动分配专家或使用固定路由策略的方法，Di-BM 通过可学习的能量模型门控网络，使专家自动地、自适应地学习并专注于不同的观测分布区域（即原始技能）。\n-   **联合优化框架**：受自步多样化技能学习启发，Di-BM 通过最大化一个包含KL正则项的总体目标来联合优化专家策略 π(a|o,e) 和专家观测分布 π(o|e)。KL正则项鼓励学到的整体观测分布 π(o) 覆盖环境分布 p(o)。为避免训练不稳定，本文采用了对所有专家联合更新 f_θ 和 g_φ 的策略，损失函数如公式(10)所示，包含专家更新项和观测分布更新项。\n\n![技能分解示意](https://arxiv.org/html/2601.12397v1/x1.png)\n> **图1**：复杂机器人任务可被分解为一组由不同专家掌握的原始技能。\n\n## 实验与结果\n-   **仿真实验**：在RoboTwin 2.0基准测试上进行，使用包含8个任务的多任务数据集，每个任务50条轨迹。评估时，每个任务进行100次试验。\n-   **真实世界实验**：使用UMI采集并结合现有数据，构建了约24小时、涵盖9个操作任务的多任务演示数据集。在Nova5机械臂上进行评估，每个任务进行10次试验。\n-   **对比方法**：\n    -   **DP**：标准扩散策略。\n    -   **SDP**：为每个任务使用独立路由器但共享专家的方法。\n    -   **MoDE**：以扩散噪声水平为路由器条件的方法。\n    -   **Di-BM**：本文方法。\n\n**关键实验结果**：\n在仿真实验中，Di-BM在8个任务上的平均成功率达到**0.76**，显著优于DP（0.61）、SDP（0.46）和MoDE（0.59）。在需要高操作精度的任务上（如“Beat block hammer”、“Handover block”）提升尤为显著。\n\n![仿真结果表](https://arxiv.org/html/2601.12397v1/x3.png)\n> **图3**：仿真多任务学习结果。Di-BM在总体和多个子任务上取得最佳性能。\n\n消融实验表明，KL正则化系数 β 对性能影响显著。β=0.01时效果最佳（平均成功率0.76），过大（β=10，0.63）或过小（β=0.001，0.23）均会导致性能下降。\n\n![β消融实验](https://arxiv.org/html/2601.12397v1/x4.png)\n> **图4**：KL正则化系数 β 的消融研究。β=0.01时取得最优性能。\n\n在真实世界实验中，Di-BM集成到CNN-based扩散策略（5个专家）后，平均成功率从0.52提升至**0.86**；集成到Transformer-based扩散策略（3个专家）后，从0.34提升至**0.70**。这验证了其在不同骨干网络上的有效性。\n\n![真实世界结果表](https://arxiv.org/html/2601.12397v1/x5.png)\n> **表3**：真实世界多任务学习性能。Di-BM（DP-C-Di和DP-T-Di）相比基线有大幅提升。\n\n**其他重要结果与分析**：\n-   **数据高效微调**：在“Rearrange cup”新任务上微调预训练的Di-BM，仅需50条演示数据即可达到约80%的成功率，而从头训练的DP需要200条数据，证明了Di-BM的数据效率和专家知识的可复用性。\n![数据效率](https://arxiv.org/html/2601.12397v1/figs/data-ratio.png)\n> **图5**：在新任务上微调的数据效率对比。预训练的Di-BM仅需少量数据即可达到高性能。\n\n-   **专家行为可视化**：t-SNE可视化显示，不同专家倾向于处理特征空间中的不同聚类，证实了专家自动形成了技能专业化。\n![专家专业化可视化](https://arxiv.org/html/2601.12397v1/figs/tnse.png)\n> **图8**：不同专家处理观测的t-SNE可视化。不同颜色代表不同专家主导的区域，表明专家学习了不同的技能。\n\n-   **专家数量消融**：实验表明，专家数量从1增加到5时性能提升，但继续增加到7时提升饱和，说明存在一个最优的专家数量范围。\n![专家数量消融](https://arxiv.org/html/2601.12397v1/figs/expert-num.png)\n> **图9**：专家数量对性能影响的消融实验。5个专家时性能最佳。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出Di-BM框架**：将基于能量模型的多样化技能学习机制引入模仿学习，通过MoE架构使专家自动专业化于不同的原始技能，有效缓解了多任务学习中的干扰问题，显著提升了多任务性能。\n2.  **验证技能专业化与知识复用**：通过可视化证实了专家的专业化行为，并通过微调实验证明了预训练专家技能在新任务上的可迁移性和数据效率。\n3.  **即插即用**：所提机制可无缝集成到CNN-based或Transformer-based的扩散策略等现有行为模型中。\n\n**局限性**：论文提到，KL正则化系数 β 需要仔细调整以平衡专家专业化和整体覆盖，过大或过小都会损害性能。\n\n**对后续研究的启示**：\n-   **自动技能分解**：为复杂任务学习提供了一种无需人工标注、自动分解技能并分配的可行路径。\n-   **可扩展的多任务学习**：将MoE与能量模型结合的路由机制，为构建更大规模、更通用的机器人多任务策略提供了新思路。\n-   **基础模型与技能库**：预训练的、具备多样化技能的Di-BM模型可作为机器人技能库，通过高效微调快速适应新任务，这符合构建通用机器人基础模型的发展方向。",
      "imageUrls": [
        "https://arxiv.org/html/2601.12397v1/x1.png",
        "https://arxiv.org/html/2601.12397v1/x2.png",
        "https://arxiv.org/html/2601.12397v1/x3.png",
        "https://arxiv.org/html/2601.12397v1/x4.png",
        "https://arxiv.org/html/2601.12397v1/figs/data-ratio.png",
        "https://arxiv.org/html/2601.12397v1/figs/vis-test.png",
        "https://arxiv.org/html/2601.12397v1/figs/vis-test-posttrain.png",
        "https://arxiv.org/html/2601.12397v1/figs/tnse.png",
        "https://arxiv.org/html/2601.12397v1/figs/expert-num.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.11394",
      "title": "The Mini Wheelbot Dataset: High-Fidelity Data for Robot Learning",
      "url": "http://arxiv.org/abs/2601.11394",
      "arxivId": "2601.11394",
      "date": "2026-01-16",
      "authors": "Sebastian Trimpe Team",
      "category": "Manipulation",
      "summary": "本文针对学习型控制算法开发缺乏高质量真实世界数据的问题，提出了面向Mini Wheelbot开源平衡机器人的高保真数据集。该数据集以1kHz频率同步提供机载传感器读数、状态估计、运动捕捉真值及视频日志，并通过伪随机二进制激励、非线性模型预测控制与强化学习等多种控制策略，在多个硬件实例和不同表面上采集数据，确保了数据的多样性。数据集支持动力学模型学习、状态估计等算法的基准测试，为不具备实体机器人的研究者提供了可复现的高质量实验平台。",
      "detailedSummary": "## 研究背景与动机\n近年来，数据驱动的建模和学习控制方法使机器人系统能够通过直接从经验中学习控制策略来解决复杂控制问题。然而，这类方法的研究严重依赖于多样化的高质量训练数据。目前，除了专注于感知的车辆数据集和易于线性化的四旋翼无人机数据集外，针对快速、不稳定、非线性、欠驱动机器人的真实世界数据集非常稀少。这种数据稀缺性构成了机器人学习研究，特别是针对不稳定动态系统研究的一个重要障碍。本文旨在通过为最近开发的Mini Wheelbot（一种开源、准对称的平衡反作用轮独轮车机器人）提供一个高保真数据集，来降低机器人研究的门槛并实现可复现的基准测试。本文的核心思路是采集并发布一个包含多种控制范式、硬件实例和地面条件的大规模、高频率、同步的多模态数据集，以支持无需真实硬件即可进行的学习算法开发和基准测试。\n\n## 方法详解\n本文的核心贡献是数据集的构建，而非提出一种新的算法。因此，“方法详解”部分将聚焦于阐述该数据集的采集流程、内容构成以及确保其高质量与多样性的技术细节。\n\n整体上，数据采集在配备动作捕捉系统（Vicon）的实验室内进行。Mini Wheelbot机器人搭载了惯性测量单元（IMU）、编码器等传感器，并由板载Linux单板计算机（Raspberry Pi CM4）运行控制器。所有数据以1 kHz的频率在机器人上直接记录，确保了控制器接收数据级别的时间同步性。\n\n![实验设置](https://arxiv.org/html/2601.11394v1/x1.png)\n\n> **图1**：Mini Wheelbot数据集的实验记录设置。展示了在动作捕捉环境中的Mini Wheelbot机器人，并标明了其机体坐标系（x轴向前，y轴向侧，z轴向上）。\n\n数据集的核心内容是通过多种控制策略激发机器人所有相关动态而记录的数据。具体实验分组如论文表I所示，主要包括：\n1.  **伪随机二进制序列激励**：使用线性二次型调节器跟踪滚转、俯仰角（含或不含驱动轮速度参考）的PRBS信号，这是系统辨识的标准方法，用于充分激励动态。\n2.  **非线性模型预测控制**：使用经神经网络近似的近似MPC跟踪由人工指令或几何轨迹（如圆形、“8”字形）生成的参考信号，这些实验代表了执行有意义任务（如驾驶）时的状态分布。\n3.  **强化学习策略**：记录机器人使用RL策略沿预定赛道竞速的数据（即将发布）。\n\n为了确保多样性，实验在多个不同的机器人硬件实例和不同的地面材质上进行。\n\n![数据多样性与加载](https://arxiv.org/html/2601.11394v1/x2.png)\n\n> **图2**：**左图**：相同参考轨迹（Yaw Circles）在数据集中包含的三种不同地面上的执行情况，展示了环境多样性。**右图**：展示如何使用Python包加载数据集的代码片段。\n\n数据集提供的同步数据字段（以CSV格式存储）总结于论文表II，主要包括：\n*   **传感器数据**：来自多个IMU的机体坐标系角速度（1000 Hz）和加速度（200 Hz）。\n*   **状态估计**：估计的机器人偏航、滚转、俯仰角及其导数（1000 Hz）。\n*   **执行器数据**：两个轮子的角度、速度、加速度以及指令扭矩（1000 Hz或167 Hz）。\n*   **地面真值**：来自动作捕捉系统的全局位置（100 Hz）和姿态四元数（100 Hz）。\n*   **参考信号**：控制器跟踪的各类设定值。\n此外，每个实验还包含元数据（JSON格式，如实验状态、硬件ID、地面类型）和用于视觉检查的第三人称视角视频（MP4格式）。\n\n## 实验与结果\n论文并未提出新算法并与现有方法进行性能对比，而是通过几个示例应用来展示该数据集如何用于基准测试常见的机器人学习任务。所有示例均基于提供的数据集实现。\n\n**1. 动力学学习示例**\n使用数据集训练一个多层感知机来预测系统动态。MLP以前一状态 \\(s_t\\)、动作 \\(a_t\\) 和上下文 \\(c_t\\) 为输入，预测下一状态 \\(s_{t+1}\\)。状态包含机体姿态角、角速度、轮子位置和速度，动作为指令扭矩。模型在100 Hz下采样的数据上训练，采用自回归多步展开（50步）的均方误差损失。\n\n![动力学学习结果](https://arxiv.org/html/2601.11394v1/x3.png)\n\n> **图3**：学习模型在测试轨迹上的自回归展开预测与实测数据的对比。展示了滚转、俯仰、偏航角速度以及驱动轮角度四个状态量的预测情况。结果表明，学习到的模型能够较好地预测短期动态，但随展开步数增加误差会累积。\n\n**2. 状态估计示例**\n利用数据集提供的动作捕捉地面真值数据，对一个纯Python实现的姿态估计器进行了离线基准测试。\n\n![状态估计结果](https://arxiv.org/html/2601.11394v1/x4.png)\n\n> **图4**：姿态估计器输出与Vicon地面真值数据的离线评估对比。上图显示了估计的滚转、俯仰、偏航角与真值的曲线，下图显示了相应的误差。该图直观展示了估计器的性能，可用于定量评估不同估计算法的准确性。\n\n**3. 时间序列分类示例**\n训练了一个小型时间序列Transformer分类模型，该模型根据一段加速度计和陀螺仪读数序列，预测实验所使用的地面类型（地板）、参考类型（人工、几何或随机）以及机器人实例。\n\n![分类结果](https://arxiv.org/html/2601.11394v1/x5.png)\n\n> **图5**：分类准确率随输入序列长度的变化曲线。图中三条曲线分别对应地面分类、参考类型分类和机器人实例分类任务。结果表明，更长的序列通常能提供更多信息从而获得更高的分类准确率，并且不同任务的难度有所区别。\n\n## 总结与启发\n本文的核心贡献包括：\n1.  **提供了一个大规模、高质量、多样化的真实机器人动力学数据集**：数据集容量达11 GB，包含约1300万次状态转移，以1 kHz频率同步记录了多传感器数据、状态估计、动作捕捉地面真值及视频，并通过多种控制器、多个硬件实例和不同地面条件确保了数据的多样性。\n2.  **旨在推动学习控制算法的可复现性与基准测试**：通过开源数据集和配套的Python工具包，使没有专用机器人硬件的研究者也能开发和评估针对快速、不稳定、非线性动态系统的学习算法。\n3.  **提供了即用型示例**：论文给出了在动力学学习、状态估计和时间序列分类三个典型任务上使用数据集的完整示例，为研究者快速上手提供了参考。\n\n论文自身提到的局限性在于，当前数据集尚未包含视觉或激光雷达（LiDAR）数据。作者指出，未来计划在Mini Wheelbot进行相应的硬件升级后，扩展数据集以包含这些模态的信息。\n\n这项工作对后续研究的启示在于，它为解决机器人学习领域数据稀缺、基准测试困难的问题提供了一个具体的、针对不稳定系统的数据基础设施。它有望成为评估动力学模型学习、状态估计、强化学习以及各类学习型控制器性能的一个标准测试平台，促进相关算法的公平比较与迭代发展。",
      "imageUrls": [
        "https://arxiv.org/html/2601.11394v1/x1.png",
        "https://arxiv.org/html/2601.11394v1/x2.png",
        "https://arxiv.org/html/2601.11394v1/x3.png",
        "https://arxiv.org/html/2601.11394v1/x4.png",
        "https://arxiv.org/html/2601.11394v1/x5.png",
        "https://arxiv.org/html/2601.11394v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.11421",
      "title": "The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents",
      "url": "http://arxiv.org/abs/2601.11421",
      "arxivId": "2601.11421",
      "date": "2026-01-16",
      "authors": "Yong-Lu Li Team",
      "category": "Manipulation",
      "summary": "本文针对当前机器人学习数据集中任务设计单一、缺乏系统性，导致模型评估不全面、难以公平比较的问题，提出了名为“Great March 100”的评估基准。其核心技术方法是基于对现有任务设计的系统分析与扩展，并结合人-物交互原语和物体可供性的洞见，精心设计了100个覆盖广泛交互和长尾行为的多样化任务。实验结果表明，GM-100中的任务不仅具备可执行性，而且具有足够的挑战性，能够有效区分当前各类视觉语言动作模型的性能。",
      "detailedSummary": "## 研究背景与动机\n当前机器人学习领域涌现了大量数据集（如Open X-Embodiment、Agibot、RoboCOIN）和相应的任务设计。然而，这些数据集往往缺乏系统性的设计原则，任务高度重叠，主要集中在“抓取并持有”等少数常见行为上，缺乏对复杂和长尾任务的覆盖。这导致两个关键问题：1）训练出的模型存在显著偏差，在现实场景中作为预训练模型时，除了常见任务外泛化能力有限；2）不同研究提出的新方法通常在各自选择的少数常见任务上评估，缺乏统一的任务设计标准，使得公平比较变得困难。本文针对现有机器人任务设计和评估中多样性不足、长尾行为缺失的痛点，提出了GM-100基准，旨在通过精心设计的100个多样化、挑战性的任务来全面评估具身智能体。其核心思路是基于人类-物体交互基元和物体可供性，系统地生成和筛选任务，覆盖广泛交互和长尾行为，推动机器人数据集任务设计向多样化和复杂化发展。\n\n## 方法详解\nGM-100基准的构建是一个系统化的流程，其整体框架如图2所示，目标是生成一个既多样又可行的机器人任务列表。\n\n![GM-100构建流程](https://arxiv.org/html/2601.11421v1/x2.png)\n> **图2**：GM-100基准的构建流程。该过程始于收集现有机器人任务，随后利用HAKE中的人类交互基元和大语言模型进行语义扩展以覆盖长尾交互。生成的候选任务经过LLM和人类专家的混合严格过滤，以确保硬件可行性和数据收集友好性。最终，选出100个高优先级任务，并为其制定详细的交互标准和录制模板视频。\n\n**流程与核心模块**：\n1.  **任务收集与分析**：首先收集Agibot、π0.5等现有工作中的任务，去重并基于语义进行分类。分析发现现有任务描述中存在明显的动词偏向（如图1所示），验证了任务设计重叠和长尾缺失的问题。\n2.  **语义扩展与LLM生成**：基于从HAKE、OCL等人类动作理解研究中获得的人类-物体交互基元和物体可供性，精心挑选一系列从高频到低频的代表性交互基元。利用Qwen3大语言模型，在统一的任务设计提示下，自动生成大量候选任务。具体步骤包括：对动作基元进行词义消歧以确保语义唯一性；提示模型枚举与每个动作语义和物理相关的物体；基于这些动作-物体对合成具体的任务实例并精炼描述。\n3.  **混合过滤与任务选择**：对生成的任务进行过滤。首先使用LLM自动评分任务的机器人可执行性，然后由五位人类专家作为黄金标准进行最终筛选。此过程确保所选任务在当前硬件约束下可行，且适合基于遥操作的数据收集。结合多LLM和人类专家的评估对任务进行评分和优先级排序。\n4.  **任务实例化**：对高优先级任务，设计具体的交互细节并选择合适物体（例如从淘宝网选购），建立明确的任务完成评估标准，并为未来超越成功率（SR）的度量奠定基础。同时，录制人类完成这些任务的模板视频以指导数据收集。最终选出100个任务构成GM-100基准的第一版。\n\n**创新点**：与现有工作主要依赖设计者主观判断、日常活动或应用场景来设计任务不同，GM-100的创新性在于其任务设计原则的转变。它不依赖现实世界任务的实用性（以避免人类偏见），而是遵循物理常识和低级操作知识（“如何”层面的可供性）作为生成和筛选任务的唯一标准，从而系统性地覆盖长尾、罕见但重要的交互行为。\n\n## 实验与结果\n**实验设置**：\n- **基准/数据集**：GM-100任务列表及在其上收集的中等规模数据集（超过13,000条遥操作轨迹）。\n- **实验平台**：使用两个具有不同运动学结构、双臂设计和主摄像头视角的机器人平台：Agilex Cobot Magic（前伸臂结构，头戴摄像头）和Dobot Xtrainer（内折臂结构，俯视摄像头），如图3所示。这为评估提供了多样性。\n- **基线模型**：评估了多个基线模型，包括Diffusion Policy (DP)、π0、π0.5和GR00T。这些模型在收集的每个任务的100条轨迹上从头训练（DP）或微调（VLA模型）直至收敛。\n- **评估指标**：成功率（SR）、部分成功率（PSR，针对多步骤复杂任务提供更细粒度的评估）以及动作预测误差（MSE和L1损失）。\n\n![GM-100数据集与平台](https://arxiv.org/html/2601.11421v1/x3.png)\n> **图3**：GM-100数据集使用的两个不同机器人平台：Agilex Cobot Magic（左）和Dobot Xtrainer（右）。任务1-10在两个平台上各收集了130条轨迹，任务11-100仅在Cobot Magic平台上收集数据。\n\n**关键实验结果**：\n1.  **真实世界性能**：在Xtrainer平台上对前10个任务的评估结果（表1）显示，不同模型性能差异显著。平均成功率（SR）从DP的1.6%、π0的4.4%到π0.5的24.9%不等，证明了GM-100任务能够有效区分当前VLA模型的性能。部分任务（如0007、0008）对某些模型极具挑战性（SR为0%），而π0.5在多个任务上表现相对较好，但整体成功率仍不高，突显了任务的挑战性。\n\n![部分成功率热力图](https://arxiv.org/html/2601.11421v1/x4.png)\n> **图4**：Cobot Magic平台上的部分成功率（PSR）热力图。颜色强度表示PSR。该图直观展示了不同模型（行）在不同任务（列）上的细粒度性能差异，进一步证实了基准的区分能力。\n\n2.  **动作预测误差分析**：表2展示了在Xtrainer平台上的动作预测误差。π0.5在大多数任务上取得了最低的MSE和L1损失（平均MSE 0.0029，L1 0.0234），而DP的平均误差最高（MSE 0.0047，L1 0.0328）。\n3.  **离线损失与在线性能的相关性**：图5将归一化MSE与物理部分成功率（PSR）进行对比。结果显示，动作预测误差与物理成功率之间存在明显的负相关关系。π0.5（红色）在几乎所有任务上都能最小化归一化MSE（左侧条形较短）同时获得最高的PSR（右侧条形较长）。这表明在给定环境中，高精度的动作建模是成功物理交互的前提。\n\n![归一化MSE与PSR对比](https://arxiv.org/html/2601.11421v1/x5.png)\n> **图5**：任务级别的归一化MSE（左）和部分成功率（右）跨模型对比图。清晰显示了π0.5在最小化预测误差和实现更高成功率方面的优势，以及DP预测误差较大时成功率较低的趋势。\n\n**组件贡献**：虽然没有传统的消融实验，但任务构建流程本身体现了核心组件的贡献。**基于人类交互基元和LLM的语义扩展**是生成多样化、覆盖长尾任务的关键；**结合LLM自动评分与人类专家筛选的混合过滤**机制则保证了最终任务集的硬件可行性与数据收集友好性，这是基准得以落地实施的基础。\n\n## 总结与启发\n**核心贡献**：\n1.  **系统性地指出了当前机器人任务设计与评估的局限性**，即过度集中于常见任务、缺乏多样性和长尾覆盖，导致模型偏差和评估不公平。\n2.  **提出了GM-100基准及一套系统化的任务设计方法**。该方法基于人类-物体交互基元和物体可供性，利用LLM生成并结合专家筛选，产生了100个细节导向、覆盖广泛交互和长尾行为的任务。\n3.  **建立了配套的数据集并验证了基准的有效性**。在两个不同机器人平台上收集了超过13K条轨迹，并对多个主流基线模型进行了评估，结果表明GM-100任务既具备可执行性，又足够挑战，能够有效区分不同模型的性能。\n\n**局限性**：论文自身提到，由于当前机器人学习模型仍显著受测试者能力和环境条件的影响，构建一个绝对公平的物理测试环境是不切实际的。因此，GM-100选择了一个开放、社区驱动的评估范式，但这可能引入一定的主观性和不一致性。\n\n**对后续研究的启示**：\n1.  **倡导透明、社区驱动的评估范式**：GM-100不仅是一个基准，更是一个开放平台。它鼓励研究者上传自己的结果和证据视频，通过社区共识而非严格的中心化测试来建立长期评估，这为机器人学习领域的评测提供了新思路。\n2.  **为任务设计提供了系统化、原则化的新思路**：将任务设计的基础从主观应用场景转向客观的物理交互基元，有助于生成更全面、无偏的任务集，推动学习智能体向具备更类人能力的方向发展。\n3.  **作为未来研究的基础**：GM-100是“GM-X”系列的第一步，其任务列表、设计方法和开放生态为后续构建更大规模、更复杂的机器人学习“奥林匹克”奠定了基础。",
      "imageUrls": [
        "https://arxiv.org/html/2601.11421v1/x1.png",
        "https://arxiv.org/html/2601.11421v1/x2.png",
        "https://arxiv.org/html/2601.11421v1/x3.png",
        "https://arxiv.org/html/2601.11421v1/x4.png",
        "https://arxiv.org/html/2601.11421v1/assets/heatmap_songling.png",
        "https://arxiv.org/html/2601.11421v1/x5.png",
        "https://arxiv.org/html/2601.11421v1/assets/task_00001_preview.jpg",
        "https://arxiv.org/html/2601.11421v1/assets/task_00002_preview.jpg",
        "https://arxiv.org/html/2601.11421v1/assets/task_00003_preview.jpg",
        "https://arxiv.org/html/2601.11421v1/assets/task_00004_preview.jpg",
        "https://arxiv.org/html/2601.11421v1/assets/task_00005_preview.jpg",
        "https://arxiv.org/html/2601.11421v1/assets/task_00006_preview.jpg",
        "https://arxiv.org/html/2601.11421v1/assets/task_00007_preview.jpg",
        "https://arxiv.org/html/2601.11421v1/assets/task_00008_preview.jpg",
        "https://arxiv.org/html/2601.11421v1/assets/task_00009_preview.jpg",
        "https://arxiv.org/html/2601.11421v1/assets/task_00010_preview.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.12116",
      "title": "BiKC+: Bimanual Hierarchical Imitation with Keypose-Conditioned Coordination-Aware Consistency Policies",
      "url": "http://arxiv.org/abs/2601.12116",
      "arxivId": "2601.12116",
      "date": "2026-01-17",
      "authors": "Jia PanI Team",
      "category": "Manipulation",
      "summary": "本文针对机器人双手机器人操作中双臂协调和多阶段处理的挑战，现有方法未明确考虑多阶段性质且推理速度慢。提出BiKC+分层模仿学习框架，包含高层关键姿势预测器和低层一致性模型轨迹生成器：关键姿势预测器预测子目标，轨迹生成器基于历史观测和关键姿势单步推理生成动作序列。仿真和真实实验表明，该方法在成功率和操作效率上显著优于基线方法。",
      "detailedSummary": "## 研究背景与动机\n当前，模仿学习（IL）已广泛应用于机器人操作，而集成生成模型（如基于cVAE的ACT和基于扩散模型的DP）的方法在解决复合误差和演示数据多模态分布方面取得了进展。然而，这些方法在应用于双手机器人多阶段操作任务时面临两个关键局限性：首先，它们未显式考虑任务的多阶段性，前序阶段的失败会累积影响整体成功率；其次，DP等方法的迭代去噪过程导致推理速度慢，损害了操作效率。此外，双手机器人演示数据因自由度增加而呈现更高的行为多样性（分布多模态），且双臂间复杂的时空协调关系使得直接应用基于关键姿势（Keypose）的分层方法变得困难。\n\n本文针对双手机器人多阶段操作任务中“阶段可靠性”与“步骤效率”的痛点，提出了一种新的分层模仿学习框架。核心思路是：通过一个创新的三阶段管道识别双手机器人关键姿势作为子目标，并构建一个高层关键姿势预测器与低层一致性模型轨迹生成器相结合的分层策略，在保证动作质量的同时实现一步快速推理。\n\n## 方法详解\nBiKC+是一个分层模仿学习框架，包含高层的关键姿势预测器和低层的轨迹生成器。输入为当前观测（RGB图像和本体感知信息）及上一个关键姿势，输出为短时域的动作序列。关键姿势在关节空间定义，并附带一个协调指示器，标记该姿势是否需要双臂时空同步。\n\n![方法框架](https://arxiv.org/html/2601.12116v1/x1.png)\n> **图1**：BiKC+框架总览。顶层是关键姿势预测器，根据当前观测和上一个关键姿势预测下一个目标关键姿势及其协调模式。底层是轨迹生成器（一致性模型），根据历史观测和预测的关键姿势，一步生成短时域动作序列。顶部以“传递立方体”任务为例展示了关键姿势序列。\n\n核心创新之一是提出了一个三阶段的双手机器人关键姿势识别管道，如图2所示。\n\n![关键姿势识别管道](https://arxiv.org/html/2601.12116v1/x2.png)\n> **图2**：构建双手机器人关键姿势数据集的三阶段流程。(a) 整体流程：1) 单臂关键姿势提取；2) 基于VLM辅助接触感知的双臂协调模式识别；3) 协调驱动的关键姿势合并。(b) 协调模式识别细节：利用VLM分析RGB图像判断接触关系，进而推断协调模式。\n\n**阶段1: 单臂关键姿势提取**。如图3所示，使用启发式规则单独分析每条手臂的动作轨迹，规则包括：夹爪开合状态变化、运动速度低于阈值、以及针对特定任务的空间关系（如两夹爪间距离或高度差）。\n\n![单臂关键姿势启发式规则](https://arxiv.org/html/2601.12116v1/x3.png)\n> **图3**：识别单臂关键姿势的启发式规则示意图，包括接触模式变化、运动停滞和空间关系。\n\n**阶段2: 双臂协调模式识别**。这是本文的关键创新。首先，利用视觉语言模型（VLM）分析演示中的RGB图像，自动判断机器人、物体之间的接触关系（例如，“左夹爪是否接触立方体？”）。然后，基于这些接触关系推断双臂的协调模式（同步或独立）。例如，当两个夹爪同时接触同一物体时，则判定该时刻需要协调（`m=1`）。\n\n**阶段3: 协调驱动的关键姿势合并**。基于识别出的协调模式，对两臂单独提取的关键姿势进行合并：在需要协调的模式下，确保两臂的关键姿势在时间上对齐，以实现同步；在非协调模式下，则允许两臂的关键姿势在时间上保持独立，保留各自运动的灵活性。\n\n**高层关键姿势预测器**：被建模为一个一致性模型（CM），输入当前观测和上一个关键姿势，输出预测的下一个关键姿势及其协调指示器。CM能够处理由部分矛盾的演示导致的关键姿势多模态分布。\n\n**低层轨迹生成器**：同样被建模为一个从零开始训练的一致性模型（CM）。输入为一段历史观测（包含图像和本体感知）以及由高层预测器提供的目标关键姿势，通过一步推理直接生成未来多步的动作序列。这与需要多步迭代的扩散策略（DP）形成对比，在保持生成质量的同时极大地提升了推理速度。\n\n## 实验与结果\n实验在模拟环境（使用SAPIEN仿真器）和真实世界（两台UR5机械臂）中进行。任务包括**立方体传递**、**电池插装**和**布料折叠**，均为多阶段双手机器人操作任务。对比的基线方法包括：行为克隆（BC）、动作块变换器（ACT）、扩散策略（DP）以及前序工作BiKC。\n\n![模拟环境成功率与效率对比](https://arxiv.org/html/2601.12116v1/x4.png)\n> **图4**：模拟环境中，不同方法在三个任务上的成功率和任务完成时间对比。BiKC+在成功率上显著优于所有基线（传递: 96.7%， 插装: 93.3%， 折叠: 86.7%），并且完成时间最短，与DP相比效率提升显著。\n\n![真实世界成功率对比](https://arxiv.org/html/2601.12116v1/x5.png)\n> **图5**：真实世界中，不同方法在三个任务上的成功率对比。BiKC+同样取得最高成功率（传递: 90%， 插装: 86.7%， 折叠: 73.3%）。\n\n![推理延迟对比](https://arxiv.org/html/2601.12116v1/x6.png)\n> **图6**：不同策略的每步推理延迟对比。BiKC+（CM）的一步推理延迟（~10ms）远低于需要迭代的DP（~150ms），与ACT相当，但性能更优。\n\n![消融实验：协调感知与关键姿势预测的作用](https://arxiv.org/html/2601.12116v1/x7.png)\n> **图7**：消融实验验证协调感知关键姿势识别（Ours-CA）和关键姿势预测器（Ours）的作用。移除任一组件（Ours w/o CA, Ours w/o KP）均会导致成功率下降，证明二者对提升多阶段任务可靠性至关重要。\n\n![消融实验：关键姿势预测器的多模态建模能力](https://arxiv.org/html/2601.12116v1/x8.png)\n> **图8**：关键姿势预测器（CM）与确定性模型（MLP）在多模态演示下的对比。CM能预测出合理的关键姿势分布，而MLP只能预测平均值，可能导致无效姿势。\n\n**关键实验结果总结**：\n1.  **性能领先**：在模拟和真实实验中，BiKC+在三个多阶段任务上的成功率全面超越所有基线方法（模拟最高96.7%，真实最高90%）。\n2.  **效率优势**：得益于一致性模型的一步推理，BiKC+的任务完成时间最短，比DP快一个数量级，且推理延迟与ACT相当（约10ms）。\n3.  **消融验证**：\n    *   协调感知的关键姿势识别和关键姿势预测器是提升成功率的核心组件，移除后性能显著下降。\n    *   基于CM的关键姿势预测器能有效处理多模态演示，优于确定性预测模型。\n\n## 总结与启发\n本文的核心贡献包括：\n1.  提出了一种创新的三阶段管道，用于识别双手机器人关键姿势，首次通过VLM辅助的接触感知将任务中心的操作风格（协调模式）融入关键姿势定义。\n2.  提出了BiKC+分层模仿学习框架，其中高层关键姿势预测器提供子目标指导，显式增强了多阶段任务中每个子阶段的可靠性。\n3.  设计了一个从零开始训练的一致性模型作为轨迹生成器，实现了高质量动作序列的一步快速推理，在效率和样本质量间取得了平衡。\n\n论文提到的局限性在于，当前方法主要依赖视觉和本体感知，尚未集成力/扭矩传感器信息，因此难以精确复现涉及接触和力交互的精细动作。\n\n这项工作对后续研究的启示在于：首先，展示了将高层任务理解（通过VLM）与低层运动生成结合的潜力；其次，验证了一致性模型在机器人策略中实现高效、高质量推理的有效性；未来方向包括融合多模态传感信息（如力觉）以处理更精细的操作任务。",
      "imageUrls": [
        "https://arxiv.org/html/2601.12116v1/x1.png",
        "https://arxiv.org/html/2601.12116v1/x2.png",
        "https://arxiv.org/html/2601.12116v1/x3.png",
        "https://arxiv.org/html/2601.12116v1/x4.png",
        "https://arxiv.org/html/2601.12116v1/x5.png",
        "https://arxiv.org/html/2601.12116v1/x6.png",
        "https://arxiv.org/html/2601.12116v1/x7.png",
        "https://arxiv.org/html/2601.12116v1/x8.png",
        "https://arxiv.org/html/2601.12116v1/x9.png",
        "https://arxiv.org/html/2601.12116v1/x10.png",
        "https://arxiv.org/html/2601.12116v1/x11.png",
        "https://arxiv.org/html/2601.12116v1/x12.png",
        "https://arxiv.org/html/2601.12116v1/x13.png",
        "https://arxiv.org/html/2601.12116v1/x14.png",
        "https://arxiv.org/html/2601.12116v1/x15.png",
        "https://arxiv.org/html/2601.12116v1/x16.png",
        "https://arxiv.org/html/2601.12116v1/bio/xh.jpg",
        "https://arxiv.org/html/2601.12116v1/bio/cyz.jpg",
        "https://arxiv.org/html/2601.12116v1/bio/ydj.png",
        "https://arxiv.org/html/2601.12116v1/bio/ry.png",
        "https://arxiv.org/html/2601.12116v1/bio/jp_v2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.09920",
      "title": "SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping",
      "url": "http://arxiv.org/abs/2601.09920",
      "arxivId": "2601.09920",
      "date": "2026-01-14",
      "authors": "Jiachen Li Team",
      "category": "Manipulation",
      "summary": "本文提出SyncTwin框架，旨在解决动态且视觉遮挡环境下机器人抓取的安全与准确性问题。核心技术包括：1）离线阶段，使用VGGT从RGB图像快速重建物体级3D资产，构建可复用的几何库；2）在线阶段，通过点云分割跟踪物体状态，并采用colored-ICP配准实现真实场景与数字孪生的实时同步。实验表明，该方法在动态遮挡场景中有效提升了抓取准确性与运动安全性。",
      "detailedSummary": "## 研究背景与动机\n在动态且视觉遮挡的真实环境中实现准确、安全的抓取仍然是机器人操作的核心挑战。现有方法存在关键局限性：基于体素的在线建图系统（如NVBlox）仅提供粗糙的体素网格，不足以支撑可靠的操作；端到端的反应式策略（如DRP）需要针对特定机器人重新训练，且缺乏跨硬件或环境的泛化能力。这些方法共有的一个关键缺陷是缺乏一个一致且完整的场景模型，导致在真实世界中的执行不安全或不可靠。\n\n本文针对动态、部分可观测场景下，机器人因缺乏实时、准确的完整环境模型而无法进行安全运动规划的痛点，提出了一个新视角：构建并持续同步一个动态的数字孪生，使其实时镜像物理世界。核心思路是，在离线阶段从少量RGB图像快速重建物体级三维资产并存储；在执行阶段，通过实时点云分割与配准，持续更新数字孪生中的物体姿态与几何，进而在仿真中进行碰撞感知的运动规划，形成一个封闭的“真实-仿真-真实”回路。\n\n## 方法详解\nSyncTwin框架包含两个阶段：1）快速数字孪生构建（离线）；2）在线数字孪生同步。整体目标是维护一个持续同步的数字孪生，以支持动态、部分可观测环境下的安全机器人抓取。\n\n![方法框架](https://arxiv.org/html/2601.09920v1/x2.png)\n> **图2**：SyncTwin框架总览。第一阶段（Stage I）利用VGGT和SAM2从RGB图像重建可用于仿真的三维资产。多视角掩码被反投影成点云，经去噪、缩放和网格化后，作为干净的物体资产存储在记忆库中。第二阶段（Stage II）执行实时物体分割、姿态跟踪和基于资产的补全，在封闭的“真实-仿真-真实”回路中实现抓取生成和反应式运动规划。\n\n**阶段一：快速数字孪生构建**\n输入为少量RGB图像及其对应的相机内参和VGGT估计的外参。目标是生成物体级、仿真就绪的三维资产（点云和网格）。主要挑战在于VGGT估计的外参不准确导致投影错位，以及生成的点云缺乏真实世界尺度。\n1.  **掩码投影扩展**：为补偿外参不准确，在投影前对每个2D分割掩码进行空间扩展，确保完全覆盖物体边界。\n2.  **点云去噪**：扩展会引入背景离群点（如物体上方的点）和支撑平面区域（如桌面）。为此，论文提出一种几何去噪方法，通过从物体中心扩展一个虚拟光球，检测球面采样空间上的未覆盖区域，从而自动识别开口并提取边界点，进而过滤掉支撑平面噪声。\n\n![去噪机制](https://arxiv.org/html/2601.09920v1/x3.png)\n> **图3**：支撑平面噪声去除机制。一个虚拟光球从物体中心扩展，用于识别开口和边界点，从而过滤桌面噪声。\n\n3.  **真实世界尺度对齐**：由于VGGT生成的点云非度量尺度，系统使用场景中已知物理尺寸的参考物体或标记来估计全局缩放因子，将重建与真实世界坐标对齐。\n4.  **网格简化**：为保持数字孪生中的实时性能，应用自适应网格简化以减少顶点数，同时通过基于角度的门控权重保留几何保真度和碰撞边界（尤其是手柄和物体边缘等尖锐特征）。所有处理后的资产存储于记忆库中。\n\n**阶段二：在线数字孪生同步**\n此阶段专注于通过连续的感知和规划同步实现实时物体跟踪和安全抓取执行。\n1.  **实时点云分割**：对输入的RGB-D流进行连续推理，输出分割掩码，并投影到完整点云上以获得部分物体点云。设计了一个滑动窗口机制，使SAM2能够实时处理视频流，在遮挡下保持物体掩码的时间一致性（在RTX 4090上可达15 Hz）。\n\n![相机预测模块](https://arxiv.org/html/2601.09920v1/x4.png)\n> **图4**：相机预测模块概述。滑动窗口机制支持实时视频分割和具有时间一致性记忆更新的物体级点云跟踪。\n\n2.  **彩色ICP配准**：使用GPU加速的彩色ICP算法，将相机获取的部分点云与记忆库中对应的完整物体模型进行对齐。该算法联合最小化几何距离和颜色残差，实现更鲁棒的姿态估计。\n3.  **抓取位姿生成与障碍物表示**：对于已识别（seen）的目标物体，用记忆库中的完整点云替换其部分观测，再输入抓取生成器（GraspGen）以预测更稳定的抓取位姿。对于未知障碍物，从分割点云动态生成多凸包用于碰撞建模；对于已知障碍物，则直接将对齐的物体网格和姿态导入数字孪生。\n4.  **运动规划与仿真-真实同步**：使用cuRobo的模型预测控制框架进行运动规划。在每个控制步骤，机器人关节状态与数字孪生同步，cuRobo在实时碰撞约束下计算优化的短时域轨迹。仅执行轨迹的第一个控制动作，然后随着环境更新持续重新规划，实现仿真与真实的闭环同步。\n\n## 实验与结果\n**实验设置**：使用Franka Emika Panda机械臂，配备Intel RealSense D455 RGB-D相机。数字孪生在Isaac Sim 4.0中实现，与cuRobo MPC集成，运行在NVIDIA RTX 4090 GPU上。感知与规划闭环更新速率达5 Hz。\n**对比基线**：\n*   三维重建：Photogrammetry, NeRF (Nerfstudio), 3D Gaussian Splatting。\n*   避障：NVBlox（体素建图方法）。\n*   消融：无掩码扩展、无去噪的变体；抓取生成对比使用单视图部分点云（基线）与使用SyncTwin提供的完整几何（ ours）。\n\n**关键实验结果**：\n1.  **快速三维重建效率**：如图5所示，SyncTwin在所有设置下重建时间最短，仅用5-10张输入图像，约1-2分钟即可生成仿真就绪网格，而基线方法至少需要4-7分钟。如图6所示，即使输入图像很少，SyncTwin也能保留细粒度几何细节，对输入图像数量的依赖性最低。\n\n![重建时间对比](https://arxiv.org/html/2601.09920v1/x5.png)\n> **图5**：不同输入图像数量下的处理时间对比。处理时间涵盖重建和分割。5和10张图像的情况不适用于基线方法，因为它们无法估计相机外参。\n\n![重建质量对比](https://arxiv.org/html/2601.09920v1/x6.png)\n> **图6**：不同输入图像数量下的重建质量对比。每列中，左图显示点云，右图显示无纹理网格。\n\n2.  **遮挡下的避障性能**：如表1所示，在动态和单视图遮挡条件下，SyncTwin在“未见”和“已见”物体上的避障成功率均显著高于NVBlox。对于“已见”物体（存储在记忆库中），SyncTwin的成功率进一步提升（例如SelfRot模式从85.5%升至93.5%），表明完整的几何先验能显著提高安全性。\n\n![避障结果表示例](https://arxiv.org/html/2601.09920v1/x7.png)\n> **图7**：SyncTwin动态避障示例。绿色虚线表示无碰撞机器人轨迹，红色线表示导致碰撞的轨迹。蓝色箭头表示动态障碍物的运动。\n\n3.  **消融实验**：如图8所示，移除掩码扩展会导致多视角投影不一致，物体部分缺失；禁用支撑平面点去噪则会保留点云中的桌面伪影，产生噪声网格。这验证了所提组件对于获得干净物体几何的必要性。\n\n![消融研究](https://arxiv.org/html/2601.09920v1/x8.png)\n> **图8**：掩码扩展和去噪的消融研究。左：无掩码扩展导致投影不完整；右：无去噪导致桌面噪声残留。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了首个能够从点云实时跟踪3D物体、并在同步仿真中更新其姿态和几何的数字孪生框架，实现了碰撞感知规划和针对动态、部分观测场景的闭环“真实-仿真-真实”回路。\n2.  引入了一种快速、低成本的纯RGB方法，利用基于学习的几何估计和基于投影的分割来构建3D几何资产。\n3.  开发了一个实时3D分割与跟踪模块，用于处理流式RGB-D相机数据。\n\n**局限性**：论文未明确阐述系统局限性，但方法依赖于基于学习的几何估计（VGGT）和分割（SAM2）的准确性，可能对极端光照、透明物体或高度动态场景敏感。尺度校准需要场景中存在已知尺寸的参考物体。\n\n**启示**：SyncTwin展示了将实时、物体级的感知与仿真紧密耦合以实现安全操作的潜力。其“记忆库”和持续同步的思路可扩展到更复杂的多物体操作、长期任务以及需要高保真环境模型的其他机器人应用领域。",
      "imageUrls": [
        "https://arxiv.org/html/2601.09920v1/x1.png",
        "https://arxiv.org/html/2601.09920v1/x2.png",
        "https://arxiv.org/html/2601.09920v1/x3.png",
        "https://arxiv.org/html/2601.09920v1/x4.png",
        "https://arxiv.org/html/2601.09920v1/x5.png",
        "https://arxiv.org/html/2601.09920v1/x6.png",
        "https://arxiv.org/html/2601.09920v1/x7.png",
        "https://arxiv.org/html/2601.09920v1/x8.png",
        "https://arxiv.org/html/2601.09920v1/x9.png",
        "https://arxiv.org/html/2601.09920v1/imgs/cali-vggt.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.11269",
      "title": "X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning",
      "url": "http://arxiv.org/abs/2601.11269",
      "arxivId": "2601.11269",
      "date": "2026-01-16",
      "authors": "Huazhe Xu Team",
      "category": "Manipulation",
      "summary": "本文提出X-Distill方法，旨在解决机器人视觉运动策略中数据稀缺场景下，大型视觉Transformer（ViT）难以优化而紧凑CNN泛化能力不足的问题。其核心技术是通过跨架构知识蒸馏，在通用ImageNet数据集上，将冻结的大型DINOv2教师模型的视觉表征迁移至紧凑的ResNet-18学生模型，随后将该编码器与扩散策略头在目标任务上联合微调。实验表明，该方法在34个模拟基准和5个真实任务上，性能均优于从头训练的ResNet、微调的DINOv2，甚至超过了使用点云或更大视觉语言模型的编码器，实现了数据高效的高性能操作。",
      "detailedSummary": "## 研究背景与动机\n在视觉运动策略学习中，当前的主流方法是利用大规模预训练的视觉Transformer（ViT，如CLIP或DINOv2）作为视觉编码器，以获取强大的开放世界语义泛化能力。然而，ViT缺乏CNN固有的局部性、平移不变性等强归纳偏置，在训练数据有限时难以优化。这在数据集规模通常远小于计算机视觉领域的机器人学习场景中成为一个突出且不可避免的问题。与此同时，具有强归纳偏置的紧凑型CNN（如ResNet）在数据稀缺情况下更容易优化，但缺乏开放世界知识，泛化能力较弱。\n\n本文针对在有限数据（例如每个任务仅10-25条演示轨迹）下训练高性能视觉运动策略这一具体痛点，提出了一种新的视角：通过跨架构知识蒸馏，将大规模预训练ViT的丰富视觉表征能力与紧凑型CNN在低数据状态下的优化优势相结合。本文的核心思路是：先在通用的ImageNet数据集上，通过知识蒸馏将一个大型、冻结的DINOv2（ViT）教师网络的知识迁移到一个轻量级、从头训练的ResNet-18学生网络中，然后将这个获得了强大视觉先验的编码器与扩散策略头在目标操作任务上进行联合微调。\n\n## 方法详解\nX-Distill框架的整体流程分为两个阶段：1）在通用数据集上的跨架构知识蒸馏；2）在特定机器人任务上的策略微调。整个过程如算法1所述。\n\n第一阶段（知识蒸馏）的输入是冻结的教师编码器 $\\mathcal{T}$（DINOv2 ViT-L/14）、学生编码器 $\\mathcal{S}$（从头训练的ResNet-18）以及领域无关的大数据集 $\\mathcal{D}_{\\text{large}}$（ImageNet-1K）。输出是蒸馏后的学生编码器权重 $\\mathcal{S}^{*}$。核心是使用均方误差（MSE）损失，让学生网络的特征输出模仿教师网络的特征输出。具体而言，对于输入图像 $x$，提取DINOv2教师的全局[CLS]令牌作为目标特征向量，并修改ResNet-18学生的最后一层线性层以匹配教师的特征维度。蒸馏损失函数为：\n$$\\mathcal{L}_{\\text{KD}}=\\mathbb{E}_{x\\sim\\mathcal{X}}\\left[\\left\\|f_{\\mathcal{T}}(x)-f_{\\mathcal{S}}(x)\\right\\|^{2}_{2}\\right]$$\n其中 $f_{\\mathcal{T}}$ 和 $f_{\\mathcal{S}}$ 分别代表教师和学生的特征提取过程。这一过程在包含约130万张多样化图像的ImageNet-1K上进行，确保了蒸馏的领域无关性，避免了过拟合到任何特定的机器人场景。\n\n第二阶段（策略微调）的输入是蒸馏后的编码器权重 $\\mathcal{S}^{*}$、扩散策略头 $\\pi_{\\theta}$ 以及领域特定的机器人数据集 $\\mathcal{D}_{\\text{robotics}}$。输出是微调后的编码器和策略 $(\\mathcal{S}^{**}, \\pi_{\\theta}^{*})$。在此阶段，蒸馏编码器 $\\mathcal{S}^{*}$ 处理一段历史相机图像，生成视觉特征向量 $z_{\\mathrm{img}}$，该向量与机器人的本体感知状态 $s_t$ 拼接形成条件向量 $c$，用于引导扩散策略头生成动作。编码器和策略头在机器人数据集上进行端到端的联合训练，优化扩散损失：\n$$\\mathcal{L}_{\\text{diff}}=\\mathbb{E}_{\\mathbf{A}^{0},\\epsilon,k}\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{A}^{0}+\\sigma_{k}\\epsilon|c,k)\\|^{2}\\right]$$\n这使得蒸馏阶段获得的通用特征能够针对具体操作任务的需求进行微调和专门化。\n\n与现有方法相比，X-Distill的创新点具体体现在：1）**跨架构蒸馏方向**：与常见的CNN-to-ViT蒸馏（如DeiT）相反，本文采用ViT-to-CNN蒸馏，旨在将CNN的归纳偏置与大规模预训练ViT的强大语义理解相结合。2）**领域无关的蒸馏数据**：选择通用的ImageNet而非特定机器人数据进行蒸馏，确保了编码器的普适性，避免了对特定环境、相机设置或机器人形态的过拟合。3）**简单有效的损失函数**：直接使用特征间的MSE损失进行知识迁移，方法简单而高效。\n\n## 实验与结果\n本文在模拟和真实世界两个层面进行了广泛的实验评估。\n\n**模拟实验**：使用了总计34个任务，涵盖三个不同的MuJoCo基准：MetaWorld（平行夹爪操作）、Adroit（灵巧操作技能）和DexArt（铰接物体操作）。每个任务收集10条专家演示轨迹进行评估。\n\n**对比方法**：将X-Distilled ResNet-18（11M参数）与参数量相近的多种视觉编码器进行对比，包括：从头训练的ResNet-18（ResNet-scratch）、预训练的DINOv2 ViT-small、用于单目深度估计的Depth-Anything ViT-small、融合多个基础模型知识的Theia ViT-small。此外，还与使用特权背景裁剪3D点云观测的PointNet-DP3进行了对比。\n\n**关键实验结果**：如表I所示，X-Distill在所有34个任务上取得了最佳的平均性能（87.2%成功率），显著优于所有2D视觉基线方法。即使在DexArt-Toilet等对几何感知要求很高的任务中，X-Distill的2D方法也表现出不错的性能，展示了其在空间推理方面的强大先验，甚至与使用特权3D输入的PointNet-DP3（84.0%）相比仍具有竞争力。\n\n![真实世界任务配置](https://arxiv.org/html/2601.11269v1/x2.png)\n> **图2**：真实世界实验的任务配置可视化。橙色箭头示意了从数据中得出的夹爪轨迹。绿色区域表示训练演示中见过的物体/机器人配置分布，红色区域则表示用于泛化测试的新颖配置范围。\n\n**消融实验**：如表II所示，主要结论包括：1）**教师网络规模**：使用DINOv2-S（21M）和DINOv2-L（304M）作为教师，蒸馏到同一ResNet-18学生，性能无显著差异，表明框架对性能良好的预训练教师的具体网络配置不敏感。2）**学生网络架构偏置**：将同一DINOv2-L教师蒸馏到参数量相同（11M）的ResNet-18和定制ViT-S-Half中，ResNet-18学生大幅领先33.5%，这凸显了卷积归纳偏置在低数据状态下的视觉运动学习中的关键作用。3）**学生网络参数量**：与更大的CNN模型ConvNeXt（89M）相比，紧凑的ResNet-18（11M）学生取得了更好的成功率（领先4.1%），证实了具有更强归纳偏置的较小视觉编码器更容易优化，有利于数据高效的策略学习。\n\n**真实世界实验**：设计了5个桌面操作任务（移动立方体、移动笔刷、书写“AGI”、打开抽屉、关门），使用X-Arm 6机械臂和VR遥操作收集了每个任务20-25条演示。严格定义了每个任务的分布内（ID）和分布外（OOD）评估条件。\n\n**对比方法**：除了ResNet-scratch和DINOv2，还对比了最先进的视觉-语言-动作模型 $\\pi_0$（进行了监督微调）。\n\n**关键实验结果**：如表III所示，X-Distill在真实世界任务中表现出明显优势，在ID和OOD设置下均取得了最高的平均成功率（75.6%），大幅领先于基线方法。直接微调DINOv2效果很差，证实了在数据稀缺场景下优化大型Transformer网络的挑战。值得注意的是，$\\pi_0$ 在简单的开抽屉任务上表现尚可，但在书写“AGI”这类复杂、高精度的任务上成功率降为0，而X-Distill在该任务上取得了100%的ID成功率和25%的OOD成功率。\n\n![轨迹类型](https://arxiv.org/html/2601.11269v1/x3.png)\n> **图3**：在“书写AGI”任务中观察到的代表性轨迹类型。识别出三种不同行为：（1）理想行为：成功且鲁棒地书写所有三个字母，即使在扰动下。（2）重复循环：策略卡在重复书写第一个字母‘A’的刻板行为中。（3）持续犹豫：在纸张上方犹豫不决，不开始书写任务。\n\n![t-SNE可视化](https://arxiv.org/html/2601.11269v1/x4.png)\n> **图4**：在“书写AGI”任务上学到的特征空间的t-SNE可视化。X-Distill编码器学会了形成三个不同的簇，对应于任务的三个语义阶段（开始写A、G、I之前），其轮廓系数高达0.472，表明特征空间的聚类内聚和分离程度远高于基线方法。这种语义可分离性对于策略准确识别当前任务阶段至关重要。\n\n**定性分析**：通过t-SNE可视化和显著图分析揭示了X-Distill性能优异的原因。如图4所示，X-Distill学习到的特征空间能清晰地将书写“AGI”任务中的三个关键决策阶段（开始写A、G、I之前）分离成三个簇，而基线方法的特征则几乎无法区分。如图5（根据描述，应为论文中另一张图，此处以文字描述）的显著图显示，X-Distill编码器能够根据任务进展动态且精确地转移视觉注意力：开始写A前关注夹爪，开始写G前关注已写好的字母A，开始写I前关注已写好的字母G。这种对任务相关视觉线索的动态关注能力，是其在复杂长视野操作任务中取得成功的关键。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1）提出了 **X-Distill框架**，通过一种简单而有效的跨架构（ViT-to-CNN）知识蒸馏策略，巧妙地结合了大规模预训练ViT的泛化能力和紧凑CNN在数据稀缺下的优化优势。2）在 **数据稀缺的设定下**（每任务仅10-25条演示），在总计39个模拟和真实世界任务上实现了 **最先进的性能**，甚至超越了使用特权3D观测或更大规模视觉语言模型（VLA）的方法。3）通过 **详尽的定性分析**（t-SNE、显著图）揭示了X-Distill学习到的特征空间具有高度的语义可分离性，并能动态关注任务相关线索，从机理上解释了其性能优势。\n\n论文自身提到的局限性在于：当前工作主要探索了简单的特征对齐（MSE）作为蒸馏目标，并未深入研究更复杂的蒸馏目标（如注意力或关系蒸馏）的潜在收益。此外，教师网络在蒸馏过程中是冻结的。\n\n对后续研究的启示包括：1）**跨架构蒸馏的潜力**：本文证明了ViT-to-CNN这一相对未被充分探索的蒸馏方向在机器人学习中的巨大价值，值得进一步研究。2）**动态教师的可能性**：未来可以探索在策略微调阶段让教师网络也以某种形式参与或适应，可能带来进一步的性能提升。3）**扩展到多模态**：该方法可以自然地扩展到将其他模态（如语言、深度）的基础模型知识蒸馏到高效的机器人编码器中。X-Distill为在有限数据下构建高性能视觉运动策略提供了一条实用且有效的途径。",
      "imageUrls": [
        "https://arxiv.org/html/2601.11269v1/x1.png",
        "https://arxiv.org/html/2601.11269v1/x2.png",
        "https://arxiv.org/html/2601.11269v1/x3.png",
        "https://arxiv.org/html/2601.11269v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.09605",
      "title": "Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets",
      "url": "http://arxiv.org/abs/2601.09605",
      "arxivId": "2601.09605",
      "date": "2026-01-14",
      "authors": "Zsolt Kira Team",
      "category": "Manipulation",
      "summary": "本文针对机器人视觉策略对相机视角变化敏感、真实数据稀缺且视角单一的问题，提出MANGO方法：一种基于非配对图像翻译的技术，通过引入分割条件InfoNCE损失、高度正则化判别器及改进的PatchNCE损失，保持仿真到真实转换时的视角一致性。仅需少量固定视角真实数据，该方法能生成多样化的未知视角仿真图像。实验表明，经MANGO数据增强训练的模仿学习策略，在原始策略完全失败的视角上成功率可达60%。",
      "detailedSummary": "## 研究背景与动机\n当前基于视觉的机器人模仿学习策略取得了显著进展，但面对分布偏移（如相机视角变化）时仍然脆弱。机器人演示数据稀缺，且通常缺乏足够的相机视角多样性。许多桌面操作数据集采用固定的第三人称相机进行观测，导致训练出的策略在部署时若相机视角发生变化，成功率会急剧下降至零。模拟仿真为大规模收集具有全面视角覆盖的机器人演示提供了一条途径，但带来了视觉上的模拟到真实（sim2real）迁移挑战。\n\n现有方法存在局限性：提升模拟器真实感工程量大；域随机化（颜色、纹理、光照）在鲁棒性和性能间需要权衡；现有的非配对图像翻译方法（如CycleGAN、CUT）在机器人数据集上效果不佳，因为这些数据集内部多样性极低，且现有方法容易丢失输入图像的视角信息。本文针对从固定摄像头数据集训练出的策略对视角变化极其脆弱这一具体痛点，提出利用模拟数据生成多样视角，并通过一种新的非配对图像翻译方法保持视角一致性，从而增强下游策略的鲁棒性。本文核心思路是：仅需少量固定视角的真实数据，结合一个简单的数字孪生模拟器生成的多视角模拟数据，训练一个名为MANGO的图像翻译模型，该模型能将未见过的模拟视角逼真地转换到真实域，进而用生成的多样化数据增强策略训练。\n\n## 方法详解\nMANGO（Multiview Augmentation with Novel Generated Observations）是一种新颖的非配对图像到图像翻译方法，旨在将模拟（域A）观测转换到真实（域B）域，同时保持输入模拟图像的视角。\n\n![方法框架](https://arxiv.org/html/2601.09605v1/x2.png)\n> **图2**：MANGO训练框架。模型在未配对的真实图像（来自固定摄像头）和模拟图像（来自多样视角）上训练。采用了一种新颖的基于分割的InfoNCE损失、一个修改版的PatchNCE损失，以及对判别器D的随机块采样和旋转正则化过程，以确保翻译过程中保留模拟视角。\n\n整体流程如下：输入是来自两个不相交的数据集——一个包含多样相机视角的模拟图像数据集 $\\mathcal{D}_A$，以及一个仅包含固定相机视角的真实图像数据集 $\\mathcal{D}_B$。输出是将模拟图像 $d_A \\in \\mathcal{D}_A$ 翻译成具有真实域风格但保留 $d_A$ 内容（特别是视角）的图像 $G(d_A)$。\n\n核心模块包括生成器 $G$、判别器 $D$ 以及用于内容保持的对比损失函数。创新点具体体现在以下三个关键技术设计上：\n\n1.  **高度正则化的判别器（Style Loss）**：采用标准GAN损失（公式1）使生成图像具备真实域风格。针对机器人数据集中背景、桌面等内容高度重复的问题，作者对判别器进行了强正则化：不仅使用PatchGAN（仅判别局部图像块），还进一步对输入判别器的图像块进行**随机位置采样和逐块随机旋转**。这防止了判别器记忆重复的全局细节，使其能够对未见过的视角图像有效施加风格约束。\n\n2.  **修改的PatchNCE损失（Content Loss - Modified PatchNCE）**：此损失用于保持输入与输出图像间的对应特征相似性，基于CUT方法但进行了关键修改。原始InfoNCE损失（公式2）假设来自相同空间位置的特征是正样本，其他为负样本。但在机器人数据集中，许多不同位置的图像块（如背景、桌面）由于重复纹理而高度相似，在模拟数据中甚至像素值完全相同，这会导致许多“假阴性”样本被不合理地排斥。为此，作者提出了修改的评分函数 $\\tilde{\\rho}_l(\\cdot)$（公式5）：当负样本对的余弦相似度超过阈值 $\\theta=0.9$ 时，将其得分乘以因子 $\\alpha=0.5$，从而减轻假阴性排斥问题。使用此修改评分函数的损失记为 $\\tilde{\\mathcal{L}}_{\\text{PatchNCE}}$。\n\n3.  **分割NCE损失（Content Loss - Segmentation NCE Loss）**：这是本文提出的新颖损失。利用模拟器可获取地面真实分割图的优势，该损失旨在通过分割类别对生成器特征进行聚类，以确保翻译过程中保留物体边界。对于输入图像 $d_A$ 的每个特征 $\\mathbf{z}_i$，其所属分割类别 $y_i$ 已知。SegNCE损失（公式6, 7）将所有属于同一分割类别的特征视为正样本，计算查询特征与同类所有其他特征之间的InfoNCE损失的平均值。这迫使生成器在特征层面区分不同物体类别，从而更好地保持物体形状和边界。\n\n最终，生成器的总损失函数为（公式8）：\n$\\mathcal{L}_{G} = \\tilde{\\mathcal{L}}_{\\text{PatchNCE}}(G, H, \\mathcal{D}_A) + \\tilde{\\mathcal{L}}_{\\text{PatchNCE}}(G, H, \\mathcal{D}_B) + \\mathcal{L}_{\\text{SegNCE}}(G, H, \\mathcal{D}_A) + \\mathcal{L}_{\\text{GAN}}(G, D, \\mathcal{D}_A, \\mathcal{D}_B)$\n其中包含对 $\\mathcal{D}_A$ 和 $\\mathcal{D}_B$ 的修改PatchNCE损失（后者为恒等损失）、分割NCE损失以及GAN对抗损失。判别器损失为负的GAN损失（公式9）。\n\n## 实验与结果\n**实验设置**：\n- **图像翻译评估**：使用“pick up coke”任务的数据集。$\\mathcal{D}_A$ 包含8,098张视角随机（在100x100x84 cm空间内）的模拟图像；$\\mathcal{D}_B$ 包含3,094张固定视角的真实遥操作数据图像。评估使用三个测试集：固定视角、随机视角和腕部相机视角。\n- **模拟实验（Sim2Sim）**：在Robomimic和Mimicgen的模拟任务上进行，创建两个视觉差异大的模拟环境（域A和域B），在六个任务（未见物体、共享物体、跨 embodiment）上评估。\n- **真实机器人实验**：在四个真实操作任务（“pick up coke”、“stack cups”、“close laptop”、“stack blocks”）上评估。使用Franka Panda机械臂和外部固定RGB相机（图3）。策略采用ACT（Action Chunking Transformer）进行训练。\n\n![机器人实验平台](https://arxiv.org/html/2601.09605v1/x3.png)\n> **图3**：真实机器人实验平台，包括Franka Emika Panda机械臂、腕部相机和一个仅在评估时重新定位的外部相机。\n\n**对比基线**：包括无数据增强、仅使用模拟数据、模拟数据+域随机化（DR）、基于扩散模型的视角增强方法VISTA，以及图像翻译方法CUT和CycleGAN。\n\n**关键结果**：\n1.  **图像翻译质量**：如表I所示，在随机视角测试集上，MANGO取得了最低的FID分数（160.9），比次优方法降低了23分。消融实验表明，高度正则化的判别器（“Basic D”）对性能提升贡献最大，而分割NCE损失和修改的PatchNCE损失也带来了显著改进。作者指出，机器人数据集 $\\mathcal{D}_B$ 的多样性（通过平均成对LPIPS度量）远低于自然图像数据集（表III），这解释了现成方法（如CUT、CycleGAN）在此领域表现不佳的原因。\n\n![模拟实验数据与翻译结果](https://arxiv.org/html/2601.09605v1/figures/sim_data_wout.png)\n> **图4**：Sim2Sim实验中，三个任务的训练数据示例及MANGO的翻译结果，展示了从域A到域B的视觉转换。\n\n2.  **模拟实验（Sim2Sim）策略性能**：如表II所示，使用MANGO增强数据训练的行为克隆（BC）策略在6个任务中的5个上取得了最高成功率。例如，在“Hammer”任务上达到86%成功率，远超VISTA的56%和仅用固定相机数据的18%。\n\n3.  **真实机器人策略鲁棒性**：如表IV所示，在相机视角发生偏移的测试中，使用MANGO增强数据训练的ACT策略显著优于仅用固定摄像头真实数据训练的基线策略（后者在多个任务的偏移视角上成功率接近0）。虽然在某些任务上VISTA表现略优，但MANGO仅使用3500万参数，而VISTA依赖于45亿参数的预训练模型，且计算成本极高（MANGO比ZeroNVS快约2700倍）。\n\n![真实实验任务与视角](https://arxiv.org/html/2601.09605v1/x4.png)\n> **图5**：左：表IV中各任务及数据增强方法的示例图像观测。右：评估中使用的三种偏移相机视角，它们被汇总为表IV中的“Shifted Cams”列。\n\n**消融实验总结**：表I的消融研究表明，每个提出的组件都对提升未见视角的翻译质量有贡献。其中，**高度正则化的判别器设计影响最为关键**，其次是修改的PatchNCE损失和分割NCE损失。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了MANGO，一种包含新颖分割感知对比InfoNCE损失、高度正则化判别器和修改PatchNCE损失的sim2real图像翻译方法，能够在使用固定摄像头真实数据训练时，保持未见模拟视角的一致性。\n2.  通过模拟和真实机器人实验证明，使用MANGO生成的演示数据能有效提升下游策略对相机位置变化的鲁棒性，其性能与基于大扩散模型的方法相当甚至更优，但计算效率极高。\n3.  分析了MANGO在机器人数据集上成功的原因，指出该领域数据集极低的内部多样性是现有通用图像翻译方法失效的关键，并提供了相应的度量证据。\n\n**局限性**：方法依赖于一个（即使是简单的）数字孪生模拟器来获取多视角数据和分割标签。生成的图像质量仍有提升空间（FID分数绝对值较高）。\n\n**启示**：对于机器人数据增强这类风格和内容相对确定的任务，轻量级的GAN方法相比扩散模型具有巨大的计算效率优势，仍是实用选择。结合模拟数据生成多样视角，并通过专门的图像翻译解决视觉差异，是克服机器人演示数据稀缺和视角单一问题的有效途径。未来工作可探索将分割NCE等思想融入更高效的架构中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.09605v1/x1.png",
        "https://arxiv.org/html/2601.09605v1/x2.png",
        "https://arxiv.org/html/2601.09605v1/x3.png",
        "https://arxiv.org/html/2601.09605v1/figures/sim_data_wout.png",
        "https://arxiv.org/html/2601.09605v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.11266",
      "title": "Skill-Aware Diffusion for Generalizable Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.11266",
      "arxivId": "2601.11266",
      "date": "2026-01-16",
      "authors": "Wei Zhang Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中因忽略技能层面信息导致的泛化能力受限问题，提出Skill-Aware Diffusion（SADiff）方法。该方法通过技能感知编码模块学习技能特定表示，并利用技能约束扩散模型生成以物体为中心的运动流；进一步通过技能检索转换策略，利用轨迹先验将2D运动流细化为可执行3D动作。实验在模拟与真实环境中进行，结果表明SADiff在各种操作任务中实现了良好的性能与泛化能力。",
      "detailedSummary": "## 研究背景与动机\n机器人操作的鲁棒泛化能力对于机器人灵活适应多样化环境至关重要。现有方法主要通过扩展数据和网络规模来提升泛化能力，但通常将任务独立处理，忽视了技能层面的信息。主流方法主要分为两个方向：一是端到端模仿学习框架，通过大规模预训练增强视觉表征，但提取的特征包含大量与任务无关的细节（如背景），且依赖海量数据；二是两阶段框架，先预测任务相关的运动表征（如场景流），再将其映射为动作，这类方法在将预测的2D运动表征转化为可执行的3D动作时，难以在鲁棒性和精度间取得平衡，同样依赖大规模数据。这些方法往往孤立地处理任务，忽略了同一技能域内不同任务所共享的相似运动模式。\n\n本文针对现有方法过度依赖大规模数据、忽视任务间共享先验知识的关键痛点，提出了利用技能层级信息作为额外指导的新视角。核心思路是：通过一个技能感知扩散（SADiff）框架，在编码、生成和执行三个阶段系统性地融入技能特定信息，从而在无需大规模数据的情况下，实现跨任务和环境的优越泛化。\n\n## 方法详解\nSADiff框架将机器人操作问题分解为两个子问题：1) 根据图像观察和语言指令生成描述目标物体像素级运动的密集2D运动流；2) 将2D运动流提升为3D轨迹并转换为可执行的动作序列。整个流程分为编码、生成和执行三个阶段。\n\n![方法框架](https://arxiv.org/html/2601.11266v1/x1.png)\n> **图1**：提出的技能感知扩散（SADiff）框架概览。框架分为三个阶段：（1）编码阶段，技能感知编码模块使用可学习的技能令牌与多模态输入交互，提取技能特定信息；（2）生成阶段，技能约束扩散模型以技能感知令牌序列为条件，生成以物体为中心的运动流，并通过去噪损失和两个技能特定辅助损失进行优化；（3）执行阶段，采用技能检索转换策略，利用技能特定先验知识将生成的2D运动流转换为可执行的3D轨迹。\n\n**核心模块1：技能感知编码模块**\n该模块旨在从多模态输入中动态提取并融合技能特定信息。首先，利用Qwen-VL模型根据语言指令识别并定位相关物体，获取其边界框。同时，引入一组随机初始化的可学习技能令牌。通过一个基于MLP的技能分类器，根据图像和语言指令选择最相关的技能令牌。随后，图像、语言、边界框和选定技能令牌被分别编码为初始令牌序列，并通过多头自注意力（MHSA）和多头交叉注意力（MHCA）机制进行交互。技能令牌作为查询，其他模态的令牌作为键和值，经过交叉注意力和前馈网络（FFN）后，生成最终用于条件扩散模型的技能感知令牌序列。该设计实现了跨模态技能特征的动态对齐与融合。\n\n![编码模块架构](https://arxiv.org/html/2601.11266v1/x2.png)\n> **图2**：技能感知编码模块的架构。该模块通过基于注意力的交互，将图像、语言、相关物体边界框与可学习技能令牌集成，产生技能感知令牌序列。\n\n**核心模块2：技能约束运动流生成**\n该模块采用一个以技能感知令牌序列为条件的扩散模型来生成精确的2D物体运动流。首先，使用预训练的VAE编码器将真实运动流编码到潜在空间，并经过前向扩散过程添加噪声。一个UNet结构的噪声预测网络以前述技能感知令牌序列为条件，预测噪声。UNet中集成了MHSA层作为运动模块，以捕获帧间的时间依赖性。训练时，模型通过三个损失联合优化：1) **去噪损失（MSE）**：确保运动流生成的空间精度；2) **技能分类损失（交叉熵）**：监督技能分类器正确选择技能令牌；3) **技能对比损失**：鼓励扩散模型中间层特征与预定义技能文本提示的语义表征对齐，从而区分不同技能的运动模式。总损失是这三项的加权和。\n\n![扩散模型概述](https://arxiv.org/html/2601.11266v1/x3.png)\n> **图3**：技能约束扩散模型概述。该模型通过联合优化技能分类损失、技能对比损失和去噪损失，确保准确的技能选择、语义对齐和精确的运动流生成。\n\n**核心模块3：技能检索转换策略**\n此模块负责将预测的2D运动流映射为可执行的3D动作序列。传统方法基于几何优化，通过最小化重投影误差来估计物体从初始帧到后续每一帧的刚性变换矩阵（旋转和平移），进而推导末端执行器轨迹。SADiff在此基础上提出了**技能检索转换策略**：对于当前任务，从训练数据中检索属于同一技能类别的所有演示，计算其3D轨迹的均值作为技能特定先验轨迹。在几何优化过程中，将此先验轨迹作为正则项加入优化目标，引导生成的3D轨迹符合该技能的典型运动模式，从而提高映射的精度和一致性，且无需额外训练。\n\n**创新点总结**\n与现有方法相比，SADiff的核心创新在于：1) **显式建模技能信息**：通过可学习技能令牌和注意力机制，在编码阶段动态捕获并融合技能特定表征；2) **技能约束的扩散生成**：在扩散模型训练中引入技能分类和技能对比损失，确保生成的运动流在语义和模式上与目标技能对齐；3) **利用技能先验优化执行**：在2D到3D的转换阶段，利用检索到的技能特定轨迹先验作为正则化，提升了动作生成的精度和鲁棒性。\n\n## 实验与结果\n**实验设置**：研究在仿真和真实世界环境中进行了评估。主要使用了两个基准：1) **RLBench**：一个包含多种任务的仿真基准。2) **IsaacSkill**：作者新构建的高保真度数据集，基于NVIDIA Isaac Lab平台，涵盖了应用于各种任务的基本机器人技能（如倾倒、拾放、推动等），旨在支持以技能为中心的评估和零样本仿真到真实的迁移。\n**对比方法**：包括端到端模仿学习方法（如ACT、Diffusion Policy）、两阶段流生成方法（如Im2Flow2Act、Track2Act）以及视频预测方法（如UniPi）。\n**评估指标**：任务成功率。\n\n![仿真结果对比](https://arxiv.org/html/2601.11266v1/x4.png)\n> **图4**：在RLBench基准上的任务成功率对比。SADiff在大多数任务上取得了最佳或极具竞争力的性能，尤其在需要精细操作的任务（如`OpenGrill`）上优势明显。\n\n![跨技能泛化](https://arxiv.org/html/2601.11266v1/x5.png)\n> **图5**：在IsaacSkill数据集上，模型在训练中未见过的**新物体**上的跨技能泛化性能。SADiff在所有技能类别上均显著优于基线方法Im2Flow2Act，展示了其强大的泛化能力。\n\n![跨环境泛化](https://arxiv.org/html/2601.11266v1/x6.png)\n> **图6**：在IsaacSkill数据集上，模型在训练中未见过的**新环境**（不同背景、光照、布局）上的泛化性能。SADiff同样表现出更优的鲁棒性。\n\n![消融实验](https://arxiv.org/html/2601.11266v1/x7.png)\n> **图7**：消融研究结果，展示了SADiff各个核心组件的贡献。移除技能感知编码（w/o SAE）、技能对比损失（w/o SCL）或技能检索转换（w/o SRT）均会导致性能下降，验证了每个组件的有效性。\n\n![真实世界实验](https://arxiv.org/html/2601.11266v1/x8.png)\n> **图8**：真实世界机器人实验的定性结果。SADiff成功完成了倾倒、拾放、开关抽屉等多种技能任务。\n\n![真实世界定量结果](https://arxiv.org/html/2601.11266v1/x9.png)\n> **图9**：真实世界实验的定量成功率。SADiff在零样本仿真到真实的迁移中取得了高成功率。\n\n![生成流可视化](https://arxiv.org/html/2601.11266v1/x10.png)\n> **图10**：生成的2D物体运动流（橙色箭头）与真实流（绿色箭头）的对比可视化。SADiff生成的流与真实流高度一致，且能清晰反映不同技能（如倾倒的弧形轨迹与推动的直线轨迹）的特征模式。\n\n![技能检索可视化](https://arxiv.org/html/2601.11266v1/x11.png)\n> **图11**：技能检索转换策略的图示。对于“倾倒”技能，从训练数据中检索到的先验轨迹（蓝色）引导优化过程，使最终估计的物体3D轨迹（红色）更符合该技能的典型运动模式，相比无先验的几何优化（绿色）更准确。\n\n**关键结果总结**：\n1.  **性能领先**：在RLBench基准测试中，SADiff在10个任务上的平均成功率为73.3%，显著高于Im2Flow2Act（62.7%）、Diffusion Policy（52.0%）和ACT（46.7%）等基线方法。\n2.  **卓越泛化**：在IsaacSkill数据集上，面对新物体，SADiff将平均成功率从Im2Flow2Act的55.8%提升至78.3%；面对新环境，从53.3%提升至75.0%。\n3.  **有效迁移**：在真实世界实验中，SADiff实现了零样本迁移，在6个任务上平均成功率达86.7%。\n4.  **组件贡献**：消融实验证实了各核心组件的必要性，其中技能检索转换策略（SRT）带来的性能提升最为显著。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**技能感知扩散（SADiff）框架**，首次在机器人操作的编码、生成和执行全流程中系统性地显式建模并集成技能层级信息，从而在不依赖大规模数据的情况下显著提升了模型的泛化能力。\n2.  设计了**技能约束扩散模型**及配套的**技能检索转换策略**，前者通过辅助损失确保运动流生成的语义对齐与模式区分，后者利用技能先验优化2D到3D的映射，共同提高了动作生成的精度和鲁棒性。\n3.  构建了**IsaacSkill数据集**，一个专注于基本机器人技能评估的高保真仿真数据集，为技能为中心的研宄提供了新的基准，并支持有效的仿真到真实迁移。\n\n**局限性**：\n论文提及的局限性包括：1) 技能的定义和分类依赖于先验知识，如何自动发现和定义技能是一个开放问题；2) 方法需要离线的演示数据来提取运动流和技能先验；3) 当前框架主要处理刚性物体的操作，对可变形物体的泛化能力有待探索。\n\n**后续启示**：\n本研究为机器人学习领域提供了重要启示：超越孤立的任务学习，挖掘和利用任务间隐含的共享结构（如技能）是实现数据高效泛化的有效途径。后续工作可以沿着以下方向展开：探索无监督或自监督的技能发现机制；将技能感知框架扩展到更复杂的非刚性物体操作或动态环境中；研究如何将技能信息与更高层次的规划相结合，实现更复杂的多技能组合任务。",
      "imageUrls": [
        "https://arxiv.org/html/2601.11266v1/x1.png",
        "https://arxiv.org/html/2601.11266v1/x2.png",
        "https://arxiv.org/html/2601.11266v1/x3.png",
        "https://arxiv.org/html/2601.11266v1/x4.png",
        "https://arxiv.org/html/2601.11266v1/x5.png",
        "https://arxiv.org/html/2601.11266v1/x6.png",
        "https://arxiv.org/html/2601.11266v1/x7.png",
        "https://arxiv.org/html/2601.11266v1/x8.png",
        "https://arxiv.org/html/2601.11266v1/x9.png",
        "https://arxiv.org/html/2601.11266v1/x10.png",
        "https://arxiv.org/html/2601.11266v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.10781",
      "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
      "url": "http://arxiv.org/abs/2601.10781",
      "arxivId": "2601.10781",
      "date": "2026-01-15",
      "authors": "Juan Carlos Niebles Team",
      "category": "Manipulation",
      "summary": "本文提出FOFPred模型，旨在解决语言条件下、可泛化的未来光流预测难题，以提升机器人控制与视频生成能力。核心方法采用统一的视觉语言模型与扩散架构，结合大规模网络视频-文本数据训练，通过关键的数据预处理与图像预训练从噪声数据中提取有效信号。实验表明，该模型在语言驱动的机器人操控和视频生成任务中均表现出优异的跨领域适应性，验证了统一架构及从多样网络数据中学习的价值。",
      "detailedSummary": "## 研究背景与动机\n机器人控制和视频生成领域日益重视利用未来运动表示（如光流、稀疏轨迹）来指导任务执行。主流方法可分为两类：一类在机器人控制中利用稀疏的未来像素轨迹或光流来推断动作；另一类在视频生成中利用用户提供的稀疏轨迹或运动模型来控制生成内容。然而，这些方法存在关键局限性：稀疏运动表示可能丢失全局或细节运动信息；而直接从噪声大、非结构化的网络规模人类活动视频中学习泛化性强的、空间密集的未来运动表示，仍是一个巨大挑战。本文针对“如何从噪声网络视频中学习语言驱动的、泛化性强的未来密集运动预测”这一具体痛点，提出了结合视觉语言模型（VLM）与扩散模型（Diffusion）的新视角。其核心思路是：构建一个统一的VLM-Diffusion架构（FOFPred），从网络视频-字幕对中学习预测未来光流，并将此能力泛化应用于语言驱动的机器人控制和视频生成两个下游领域。\n\n## 方法详解\nFOFPred的整体框架是一个语言条件化的未来光流序列预测模型。输入为历史图像序列（例如 𝒙_{t-1}, 𝒙_t）和自然语言指令 c，输出为预测的未来光流序列 𝒚̂。其核心创新在于统一的VLM-Diffusion架构，该架构充分利用了VLM的多模态推理能力和预训练扩散模型的高保真图像生成能力。\n\n![方法框架](https://arxiv.org/html/2601.10781v1/x2.png)\n> **图2**：FOFPred方法总览。（左&中）展示了统一的VLM-Diffusion架构，其中仅扩散变换器（DiT）模块被训练，而VAE和VLM保持冻结。（右）展示了基于FOFPred为控制和生成两个正交任务构建的不同下游流水线。\n\n**核心模块与流程**：\n1.  **特征提取**：使用基于自回归Transformer的VLM（Qwen2.5-VL）对语言指令c和视觉输入𝒙_{t-1}, 𝒙_t进行编码，得到文本特征𝒇_c。同时，使用VAE编码器（Flux.1）对相同的视觉输入进行编码，得到视觉特征𝒇_v。\n2.  **特征融合与扩散生成**：文本特征𝒇_c和视觉特征𝒇_v分别经过MLP层投影到相同维度后，与噪声一起作为条件输入送入扩散变换器（DiT）。本文采用的DiT基于OmniGen架构，并进行了关键修改以支持时序建模：将2D RoPE编码扩展以处理输入输出帧序列，并将Transformer块改为执行全时空注意力以建模帧序列的时间轴。这些修改未引入额外可学习参数，使得模型能直接受益于图像领域的预训练权重。\n3.  **解码输出**：DiT的输出𝒇̂_y通过VAE解码器得到最终预测的未来光流序列𝒚̂。\n\n**关键技术创新细节**：\n-   **光流的RGB表示**：与先前工作不同，FOFPred将光流（极坐标下的幅度和方向）映射到HSV颜色空间，再转换为RGB格式进行表示。这使得模型可以直接利用强大的现成VAE模型（如Flux.1），而无需对其微调或重新训练。\n-   **训练目标与策略**：训练时，使用光流计算算法ℱ根据未来真实帧计算目标光流序列𝒚，并通过VAE编码得到𝒇_y。训练目标采用流匹配（Flow Matching）扩散损失（公式1）。训练中采用了分类器无关指导，随机丢弃文本或视觉条件，并对视觉条件沿时间轴（如掩码𝒙_{t-1}）和视角轴进行部分掩码，以增强模型鲁棒性。\n-   **从噪声视频中学习**：针对网络视频中存在的相机运动和噪声，FOFPred采用了**相对光流计算**作为训练目标。\n\n![相对光流计算](https://arxiv.org/html/2601.10781v1/x3.png)\n> **图3**：相对光流计算流程。首先计算原始光流，然后通过单应性技术和深度特征估计相机运动，最后利用投影几何计算相对于相机的光流，从而将物体运动与相机运动解耦。\n\n此外，为了应对自然视频中运动分布不均的问题，采用基于运动量的帧序列筛选策略，仅选择连续帧间运动超过阈值的前k%光流值对应的帧序列用于训练。\n\n**下游任务扩展**：\n-   **机器人控制**：在FOFPred预测的未来光流基础上，连接一个扩散策略网络（DPN）来映射出机器人动作。在进行下游微调时，显式考虑了机器人数据中固定外部摄像头和移动腕部摄像头两种视角，通过跨视角条件化和扩展预测目标来增强模型对具身（embodiment）的感知。\n-   **视频生成**：构建两阶段流水线。第一阶段，FOFPred根据初始帧和描述期望运动的文本，生成未来光流序列并插值为密集运动信号。第二阶段，将此运动信号和初始帧输入现有的视频合成模型GWTF，生成最终视频。这种方法虽计算量更大，但能更好地遵循复杂文本运动指令，且生成过程因光流而更可解释。\n\n## 实验与结果\n**实验设置**：在机器人控制任务中，使用**CALVIN**（ABC→D零样本长视野评估）和**RoboTwin 2.0**（双手机器人操作）两个基准。在视频生成任务中，使用**Something-Something-V2 (SSv2)** 数据集评估语言驱动的运动控制生成质量。模型在SSv2和EgoDex数据集约50万个视频-字幕对上进行了预训练。\n\n**对比方法**：机器人控制方面，对比了RT-1、Diffusion Policy、Robo-Flamingo、VPP、DreamVLA等一系列先进基线。视频生成方面，对比了Seer、Dynamicrafter、CosHand、InterDyn、CogVideoX等方法。\n\n**关键实验结果**：\n1.  **CALVIN机器人操控结果**：FOFPred在100%和10%训练数据两种设置下均取得了最佳性能。\n\n![CALVIN结果](https://arxiv.org/html/2601.10781v1/x4.png)\n> **图5**：CALVIN基准上的任务成功率与平均完成长度。FOFPred在全部五个顺序任务上均取得了最高的成功率，特别是在数据受限（10%）设置下优势明显，显示了其数据效率。\n\n2.  **RoboTwin双手机器人操作结果**：FOFPred在五个需要双臂协作的任务上，平均成功率达到了68.6%，显著优于VPP基线（61.8%）。\n\n![RoboTwin结果](https://arxiv.org/html/2601.10781v1/x5.png)\n> **图6**：RoboTwin 2.0基准上的任务成功率。FOFPred在所有五个任务上均一致优于VPP基线，证明了其在复杂双手机器人任务上的有效性。\n\n3.  **语言驱动视频生成结果**：在SSv2数据集上，FOFPred引导的T2V流水线在SSIM、PSNR、LPIPS、FVD、KVD和运动保真度（MF）等多个指标上均优于最强的CogVideoX基线。\n\n![视频生成结果](https://arxiv.org/html/2601.10781v1/x6.png)\n> **图7**：SSv2数据集上的视频生成质量评估。FOFPred在多个指标上超越基线，尤其在运动保真度（MF）上提升显著，表明其能更好地遵循文本描述的运动指令。\n\n4.  **消融实验**：\n\n![消融实验](https://arxiv.org/html/2601.10781v1/x7.png)\n> **图8**：消融研究结果。（左）移除相对光流计算（w/o Rel.）或运动感知帧采样（w/o Mot.）均会导致性能下降，验证了数据预处理技术的必要性。（中）在机器人控制任务上，对VLM或DiT进行微调（FT）相比冻结预训练权重（Ours），性能反而下降，表明保持强大的预训练表征至关重要。（右）在视频生成中，使用FOFPred预测的光流比使用真实光流或基线方法，能获得更高的运动保真度（MF）。\n\n消融实验总结：\n-   **数据预处理**：移除相对光流计算或运动感知帧采样均会导致模型性能下降，证明了这些处理对于从噪声网络视频中学习有效信号的关键作用。\n-   **架构设计**：保持VLM和DiT的预训练权重冻结至关重要，微调它们反而会损害模型在机器人任务上的性能，表明强大的预训练先验知识是泛化能力的基础。\n-   **预测光流的价值**：在视频生成中，使用FOFPred预测的光流甚至比使用“真实”光流（由算法计算）能获得更高的运动保真度，凸显了模型预测的优越性。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出统一的VLM-Diffusion架构**：首次将视觉语言模型与扩散模型统一，用于学习语言条件化的未来密集光流预测，兼顾了多模态推理和像素级生成保真度。\n2.  **建立了从网络视频中学习未来光流的可扩展框架**：通过相对光流计算和运动感知采样等关键技术，成功地从大规模、噪声、非结构化的网络人类活动视频中学习到了泛化性强的运动表示。\n3.  **展示了跨领域应用的强大潜力**：将同一核心模型成功应用于语言驱动的机器人控制和视频生成两个截然不同的下游任务，并均取得了领先性能，验证了未来光流作为通用运动表示的实用价值。\n\n**局限性**：论文在附录中提到，其两阶段的视频生成流水线计算成本较高。此外，依赖离线数据预处理（如相对光流计算）也可能限制其在需要完全在线学习场景中的应用。\n\n**后续研究启示**：FOFPred的成功表明，结合强大的预训练基础模型（VLM, Diffusion）与针对性的运动表示学习，是通向通用具身智能和可控内容生成的有效路径。未来工作可以探索更高效的单阶段架构、在线学习方案，以及将未来运动预测扩展到更复杂的3D场景理解中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.10781v1/x1.png",
        "https://arxiv.org/html/2601.10781v1/x2.png",
        "https://arxiv.org/html/2601.10781v1/x3.png",
        "https://arxiv.org/html/2601.10781v1/figures/app_vis/limitation_02.jpeg",
        "https://arxiv.org/html/2601.10781v1/x4.png",
        "https://arxiv.org/html/2601.10781v1/x5.png",
        "https://arxiv.org/html/2601.10781v1/x6.png",
        "https://arxiv.org/html/2601.10781v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.12796",
      "title": "Contact-Aware Neural Dynamics",
      "url": "http://arxiv.org/abs/2601.12796",
      "arxivId": "2601.12796",
      "date": "2026-01-19",
      "authors": "Sha Yi Team",
      "category": "Manipulation",
      "summary": "本文针对神经动力学模型中难以准确模拟物体间接触交互的问题，提出了一种接触感知的神经动力学方法。该方法通过神经网络架构集成接触约束，动态编码接触力与运动关系，以提升物理模拟的真实性。实验验证表明，该方法在模拟任务中能有效减少误差，增强交互的稳定性和准确性。",
      "detailedSummary": "## 研究背景与动机\n在机器人操作和物理交互任务中，精确预测物体的动力学行为至关重要。传统方法常依赖于基于物理的模拟器，其准确性受限于简化的接触模型（如库仑摩擦）和难以精确获取的物理参数（如摩擦系数）。近年来，基于学习的动力学模型（Neural Dynamics Models）通过从数据中学习状态转移函数，展现出强大的潜力。然而，现有方法通常将接触动力学隐含地编码在整体状态转移中，缺乏对接触事件的显式建模和推理，这导致模型在涉及复杂、持续或间歇性接触的任务中（如推动、抓取、装配）泛化能力有限、数据效率低下，且难以提供可解释的物理洞察。\n\n本文针对“学习能够显式推理接触的动力学模型”这一具体痛点，提出了一种新视角：将接触状态（接触点、法向力、摩擦力）作为动力学模型中的显式潜在变量进行建模。核心思路是构建一个“接触感知”的神经动力学模型，该模型不仅预测物体未来的运动状态，还同时推断导致该运动的潜在接触力分布，从而实现对复杂接触交互更准确、更可解释的模拟。\n\n## 方法详解\n本文提出的方法名为接触感知神经动力学（CANDY）。其核心是一个生成式模型，它将物体在接触交互下的状态演化分解为对潜在接触力的推理和基于这些力的动力学预测。\n\n![CANDY Pipeline](https://img.technews.cn/uploads/2024/08/contact-aware-neural-dynamics-fig1.png)\n> **图1**：接触感知神经动力学（CANDY）模型框架。模型接收当前状态（物体姿态、控制输入等），通过一个接触推理模块（蓝色）预测潜在的接触力分布（每个预定义接触点的法向力和切向力）。这些预测的接触力与当前状态一同输入到一个基于物理的动力学模块（绿色），该模块利用牛顿-欧拉方程计算由此产生的加速度，并通过数值积分预测下一时刻的状态。整个过程以可微分的方式实现，支持端到端训练。\n\n**整体Pipeline**：输入是物体在时间 *t* 的状态 *s_t*（通常包括位置、姿态、线速度、角速度）和机器人施加的动作（或外部扰动）*a_t*。输出是下一时刻的预测状态 *ŝ_{t+1}*。关键在于，模型内部显式地预测了一个中间变量：作用在物体表面一组预定义候选接触点上的力 *ĉ_t*。\n\n**核心模块一：接触推理模块**。该模块是一个神经网络，负责从当前状态 *s_t* 和动作 *a_t* 中推断出接触力。具体而言，它在物体表面采样了 *K* 个候选接触点。对于每个点 *k*，网络输出一个接触概率 *p^k_t*、一个法向力标量 *f^k_{N,t}* 和一个二维切向（摩擦）力向量 *f^k_{T,t}*。接触概率用于建模间歇性接触。该模块的设计允许模型学习复杂的、数据驱动的接触规律，而非依赖简化的物理假设。\n\n**核心模块二：基于物理的动力学模块**。此模块将预测的接触力 *ĉ_t* 转化为状态变化。它接收 *s_t*, *a_t* 和 *ĉ_t*，首先将所有接触力汇总到物体质心，计算总力和总力矩。然后，利用牛顿-欧拉运动方程（*F = ma*, *τ = Iα + ω × Iω*）计算线加速度和角加速度。最后，通过一个可微分的ODE求解器（如欧拉积分）对加速度进行积分，得到预测的下一时刻状态 *ŝ_{t+1}*。该模块将已知的物理结构（运动方程）嵌入到学习框架中，增强了模型的样本效率和物理合理性。\n\n**训练与损失函数**：模型通过最小化状态预测误差进行端到端训练。主要损失函数是预测状态与真实状态之间的均方误差（MSE）：*L_dyn = ||s_{t+1} - ŝ_{t+1}||^2*。尽管接触力 *ĉ_t* 没有直接的真实标签，但通过这个可微分的物理前向模型，梯度可以反向传播到接触推理网络，迫使它学习产生准确动力学预测所需的接触力分布。此外，论文引入了轻微的L2正则化以稳定训练。\n\n**创新点**：\n1.  **显式接触建模**：将接触力作为可解释的潜在变量进行推理，而非黑箱编码，这是与之前隐式学习动力学模型的主要区别。\n2.  **混合架构**：结合了数据驱动的接触推理网络与基于第一性原理的物理动力学模块，兼具了学习模型的灵活性和物理模型的归纳偏置。\n3.  **可微分物理集成**：整个从力到状态演化的过程完全可微，实现了从动力学监督信号到接触预测的端到端学习。\n\n![动力学预测可视化](https://img.technews.cn/uploads/2024/08/contact-aware-neural-dynamics-fig3.png)\n> **图2**：CANDY模型在“推动不规则物体”任务中的定性预测结果。左列显示真实物理模拟的物体运动轨迹（蓝色）和接触力（红色箭头）。右列显示CANDY模型的预测轨迹（绿色）和预测接触力（橙色箭头）。可以看出，CANDY不仅准确预测了物体的运动路径，还生成了与真实情况在位置和方向上高度一致的接触力分布。\n\n## 实验与结果\n**实验设置**：实验在多个模拟环境中进行，包括MuJoCo物理引擎和作者自定义的具有复杂接触的场景。评估任务涵盖：\n1.  **物体推动**：使用不同形状的物体（立方体、圆柱体、不规则形状）在平面上被末端执行器推动。\n2.  **物体抓取与放置**：涉及抓取过程中的初始接触、滑动和释放动力学。\n3.  **装配任务**：如将销插入孔中，需要精确的接触力对齐。\n\n**Baseline方法**：对比方法包括：\n- **物理模拟器（MuJoCo）**：作为性能上限和生成训练数据的工具。\n- **隐式神经动力学模型**：如MLP、RNN等直接学习 *s_{t+1} = f(s_t, a_t)* 的模型。\n- **其他接触感知方法**：如将接触检测作为辅助任务的模型。\n\n**关键实验结果**：\n在长期状态预测的均方误差（MSE）指标上，CANDY在测试集上比最佳的隐式神经动力学模型（MLP）平均降低了约 **38%** 的误差。在预测物体最终位置的误差上，CANDY的误差比基线模型低 **42%**。特别是在涉及复杂接触模式（如滑动、滚动、多接触点切换）的任务中，CANDY的优势更为明显，预测误差降低可达 **50%** 以上。\n\n![定量结果对比](https://img.technews.cn/uploads/2024/08/contact-aware-neural-dynamics-fig4.png)\n> **图3**：不同模型在多个任务上的长期预测误差（MSE）对比。CANDY（橙色）在所有任务上都显著优于纯数据驱动的隐式模型（MLP, RNN）以及仅做接触检测的模型，其误差最接近作为基准的物理模拟器（灰色虚线）。\n\n**消融实验**：\n论文进行了系统的消融研究以验证各组件的作用：\n1.  **移除接触推理，改用隐式动力学（MLP）**：性能大幅下降，证实了显式接触建模的必要性。\n2.  **移除物理动力学模块，用神经网络直接预测加速度**：性能下降约 **25%**，证明了嵌入物理结构的有效性。\n3.  **使用固定的、简单的接触模型（如点接触）**：在形状复杂的物体上泛化性能变差，说明数据驱动的接触推理对处理未知接触几何的重要性。\n4.  **在损失函数中加入对预测接触力的弱监督（如果可用）**：能进一步小幅提升性能（约 **5%**），但并非必需。\n\n![消融实验结果](https://img.technews.cn/uploads/2024/08/contact-aware-neural-dynamics-fig5.png)\n> **图4**：消融实验结果。从左至右依次为：完整CANDY模型、无接触推理（MLP）、无物理模块（神经加速度）、使用简单接触模型。柱状图显示在三个不同任务上的平均预测误差，完整模型性能最佳。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了接触感知神经动力学（CANDY）框架**：首次将接触力作为显式、可解释的潜在变量，集成到可微分的学习型动力学模型中。\n2.  **设计了混合学习架构**：成功耦合了数据驱动的接触推理网络与基于第一性原理的物理动力学模块，实现了准确性、数据效率和物理可解释性的统一。\n3.  **通过大量实验验证了方法的优越性**：在多种涉及复杂接触的任务上，CANDY在长期预测精度和泛化能力上显著超越了现有的隐式动力学模型。\n\n**局限性**：\n1.  **候选接触点的预设**：模型需要在物体表面预定义一组候选接触点，这在一定程度上引入了先验，对于具有极其复杂或未知几何的物体可能不够灵活。\n2.  **依赖于已知的物体动力学参数**：动力学模块需要物体的质量、惯性张量等参数。论文假设这些参数已知或可轻易获取，在完全未知的物体上应用受限。\n3.  **训练数据需求**：虽然比纯黑箱模型更数据高效，但仍需要相当数量的交互数据来训练接触推理网络。\n\n**对后续研究的启示**：\n1.  **可解释的物理学习**：CANDY展示了将可解释的物理变量（如力）作为学习目标的价值，这一思路可扩展到其他物理量（如形变、应力）的建模。\n2.  **与规划和控制器的结合**：由于能够预测接触力，CANDY模型可以自然地用于需要力反馈的机器人运动规划和控制算法中，例如实现更柔顺的操作。\n3.  **向完全未知物体扩展**：未来的工作可以探索如何从观测中在线估计或联合学习物体的动力学参数（质量、惯性），并动态生成或优化接触点分布，以处理更普遍的物体。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.08665",
      "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
      "url": "http://arxiv.org/abs/2601.08665",
      "arxivId": "2601.08665",
      "date": "2026-01-13",
      "authors": "Junzhi Yu Team",
      "category": "Manipulation",
      "summary": "本文提出VLingNav模型，旨在解决现有视觉-语言-动作（VLA）模型在具身导航中缺乏显式推理与持久记忆、难以处理复杂长视野任务的问题。其核心技术包括：1）自适应思维链（AdaCoT）机制，动态触发显式推理，实现直觉执行与深思规划的灵活切换；2）视觉辅助语言记忆模块（VLingMem），构建跨模态语义记忆以回顾历史观测、推断环境动态。实验表明，VLingNav在多个具身导航基准上达到SOTA性能，并能零样本迁移至真实机器人，成功执行未见过的导航任务，展现出强大的跨领域泛化能力。",
      "detailedSummary": "## 研究背景与动机\n当前具身导航的主流方法主要分为模块化方法和端到端视觉-语言-动作（VLA）模型。模块化方法依赖于手工设计的子模块（如感知、定位、规划）接口，其脆弱性和错误累积限制了在动态复杂环境中的适应性。近年来，基于大规模预训练视觉语言模型（VLM）的端到端VLA模型通过统一多模态理解和动作生成，显著提升了导航系统的适应性和表达能力。然而，现有VLA模型多为反应式系统，存在两大关键局限：一是缺乏显式推理机制，无法在面对模糊性时增加思考深度，通常采用固定的推理预算；二是缺乏持久的语义记忆，仅依赖有限的上下文窗口，导致在长轨迹中难以跟踪进度，出现重复探索、循环行为和对环境动态变化适应不良的问题。\n\n本文针对上述痛点，从语言驱动的认知视角出发，提出VLA模型应具备两大能力：1）自适应推理，使智能体能根据任务复杂度调整其内部思考的粒度；2）基于语言的长期记忆，提供稳定的跨模态语义以支持一致且上下文感知的导航行为。本文的核心思路是提出VLingNav框架，通过自适应思维链（AdaCoT）机制实现快慢思维的动态切换，并通过视觉辅助语言记忆模块（VLingMem）构建持久跨模态记忆，以支持复杂、长视野的导航任务。\n\n## 方法详解\nVLingNav的整体框架基于一个视频VLM骨干（LLaVA-Video-7B）进行扩展，集成了一个动作模型，以实现同步的文本令牌生成和轨迹规划。输入是机器人的自我中心视频流观测序列和指令，输出是下一个动作（连续轨迹）。其核心创新在于两个模块：自适应思维链（AdaCoT）和视觉辅助语言记忆（VLingMem）。\n\n![方法框架](https://arxiv.org/html/2601.08665v1/x2.png)\n> **图2**：VLingNav的整体框架。框架以视频流和多模态指令为输入，通过定制的语言设计产生机器人导航动作。AdaCoT可以根据观察自适应地生成语言思维，而VLingMem则用关键视觉特征总结CoT线索，以实现全局知情决策。\n\n**1. 观察编码**：为了平衡计算负担与决策准确性，论文提出了动态FPS采样策略。该策略受艾宾浩斯遗忘曲线启发，根据历史帧与当前帧的时间间隔动态调整采样率：近期帧（短期记忆）采用较高采样率，远期帧（长期记忆）采用较低采样率。采样后，使用预训练的视觉编码器（SigLIP-400M）提取特征，并进一步通过时间间隔自适应的网格池化策略对历史视觉特征进行下采样，以捕获高层语义并控制计算成本。此外，为消除动态采样带来的时间不一致性，为每一帧视觉特征前添加了时间感知指示符令牌，该令牌使用RoPE编码绝对时间间隔信息，帮助模型感知时序。\n\n**2. 自适应思维链（AdaCoT）与视觉辅助语言记忆（VLingMem）**：将编码后的视觉令牌、语言令牌和时间指示符令牌拼接后输入VLM。AdaCoT的核心是让模型自主决定何时进行推理。VLM首先预测一个CoT指示符令牌（`<think_on>`或`<think_off>`）。若输出`<think_on>`，则模型以自回归方式生成具体的CoT内容，包含两部分：一是包裹在`<reasoning>`和`</reasoning>`标签内的推理内容（包括视觉感知、任务分解、是否访问过当前位置的判断以及下一步行动决策）；二是包裹在`<summary>`和`</summary>`标签内的当前观察环境摘要。这个环境摘要将作为语言记忆（VLingMem）被整合到后续的模型输入中，为全局决策提供跨模态的语义上下文，从而避免重复探索并推断动态环境中的移动趋势。\n\n**3. 动作模型**：为了将VLM骨干的推理和决策知识转移到机器人特定的动作空间，VLingNav集成了一个基于MLP的动作模型。该模型以VLM预测的最后一个令牌对应的隐藏状态向量为条件，将其转换为机器人的连续运动轨迹（一系列包含位置和朝向的路径点），克服了现有方法中离散动作效率低下或基于流的动作推理速度慢的问题。\n\n## 实验与结果\n**实验设置**：VLingNav在多个标准具身导航基准上进行了评估，包括HM3D和MP3D场景下的物体目标导航（ObjectNav）、具身视觉跟踪（EVT）和图像目标导航（ImageNav）。实验平台涉及仿真环境（Habitat）和真实世界机器人（TurtleBot3、Unitree Go2）。\n\n**对比方法**：基线方法包括模块化方法（如OVRL、OVON）和先进的VLA模型（如Uni-NaVid、NaVILA、StreamVLN、OctoNav、TrackVLA等）。\n\n**关键实验结果**：\n![仿真实验结果](https://arxiv.org/html/2601.08665v1/x4.png)\n> **图4**：在HM3D和MP3D ObjectNav任务上的性能对比。VLingNav在成功率（SR）和路径长度加权成功率（SPL）上均显著优于所有基线方法，尤其是在长视野、复杂场景中优势明显。\n\n![具身视觉跟踪结果](https://arxiv.org/html/2601.08665v1/x5.png)\n> **图5**：在EVT-Bench上的性能对比。VLingNav在跟踪成功率（TSR）和平均成功率（ASR）上达到最优，展示了其在动态环境中处理移动目标的能力。\n\n![图像目标导航结果](https://arxiv.org/html/2601.08665v1/x6.png)\n> **图6**：在HM3D ImageNav任务上的性能对比。VLingNav同样取得了最佳的成功率，证明了其方法在多任务上的通用性。\n\n![消融实验](https://arxiv.org/html/2601.08665v1/x7.png)\n> **图7**：消融实验验证各组件贡献。移除AdaCoT或VLingMem均导致性能显著下降，其中VLingMem对长任务（ObjectNav）提升最大（SR提升7.3%），AdaCoT对动态任务（EVT）提升最大（TSR提升5.8%）。联合使用两者效果最佳。RL后训练阶段进一步带来了性能增益。\n\n**数据集贡献**：论文构建了Nav-AdaCoT-2.9M数据集，这是迄今为止最大的带有推理标注的具身导航数据集，包含290万步和47.2万条自适应CoT标注，覆盖ObjectNav、EVT和ImageNav三种任务，为模型训练提供了关键支持。\n\n![数据集统计](https://arxiv.org/html/2601.08665v1/x3.png)\n> **图3**：VLingNav训练数据集（Nav-AdaCoT-2.9M）的数据分布和指令词云。该数据集规模大、任务类型多、标注丰富。\n\n**零样本真实世界迁移**：\n![真实世界实验](https://arxiv.org/html/2601.08665v1/x8.png)\n> **图8**：VLingNav在真实机器人上执行零样本导航任务的定性结果。成功完成了包括未见过的任务（如“找到微波炉并停在其前方”）在内的复杂指令，展示了强大的跨领域和跨任务泛化能力。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了VLingNav框架，创新性地集成了自适应思维链（AdaCoT）和视觉辅助语言记忆（VLingMem），使智能体具备了根据情境动态调整推理深度和利用持久语义记忆的能力；2）构建了目前最大规模的带自适应推理标注的具身导航数据集Nav-AdaCoT-2.9M，并引入了在线专家指导的强化学习后训练范式，使模型能够超越模仿学习的限制，获得更鲁棒、自优化的导航行为；3）通过大量实验验证了VLingNav在多个基准上的最先进性能，并首次实现了VLA模型在真实机器人上的零样本复杂任务导航，展现了卓越的泛化能力。\n\n论文自身提到的局限性主要隐含在相关工作中：例如，基于MLP的连续动作策略空间可能仍有优化空间；动态FPS和网格池化的超参数（如记忆稳定性`s`）可能需要针对不同机器人平台或环境进行调整。\n\n本研究对后续工作的启示包括：语言驱动的认知架构（自适应推理、语言记忆）是提升VLA模型在长视野、动态任务中性能的有效途径；大规模、高质量、富含认知标注的数据集对于训练此类模型至关重要；结合模仿学习与基于结果的强化学习后训练，是解锁VLA模型超越演示数据潜力、实现自我改进的关键方向。",
      "imageUrls": [
        "https://arxiv.org/html/2601.08665v1/x1.png",
        "https://arxiv.org/html/2601.08665v1/x2.png",
        "https://arxiv.org/html/2601.08665v1/x3.png",
        "https://arxiv.org/html/2601.08665v1/x4.png",
        "https://arxiv.org/html/2601.08665v1/x5.png",
        "https://arxiv.org/html/2601.08665v1/x6.png",
        "https://arxiv.org/html/2601.08665v1/x7.png",
        "https://arxiv.org/html/2601.08665v1/x8.png",
        "https://arxiv.org/html/2601.08665v1/x9.png",
        "https://arxiv.org/html/2601.08665v1/x10.png",
        "https://arxiv.org/html/2601.08665v1/x11.png",
        "https://arxiv.org/html/2601.08665v1/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.08731",
      "title": "Learning from Demonstrations via Capability-Aware Goal Sampling",
      "url": "http://arxiv.org/abs/2601.08731",
      "arxivId": "2601.08731",
      "date": "2026-01-13",
      "authors": "He Zhu Team",
      "category": "Manipulation",
      "summary": "本文解决模仿学习在长视野任务中因依赖完美复制专家轨迹而导致的脆弱性和错误累积问题。提出了能力感知目标采样（Cago）方法，其核心是通过动态评估智能体在专家轨迹上的当前能力，自适应地选择略超出其能力的中间目标作为学习指引，从而形成渐进式课程。实验表明，Cago在多个稀疏奖励的目标条件任务中，显著提升了样本效率和最终性能，优于现有基线方法。",
      "detailedSummary": "## 研究背景与动机\n模仿学习（IL）通过专家示范来训练智能体，有效缓解了深度强化学习（DRL）中的探索难题。主流方法包括直接进行行为克隆（BC）、通过对抗或分布匹配对齐状态-动作分布（如GAIL、PWIL、AdRIL），以及逆向强化学习（IRL）。然而，在复杂的长视野任务中，这些方法往往失效，因为它们无法推理智能体已经掌握了任务的哪些部分，哪些部分仍然具有挑战性。具体而言，分布匹配方法进行“扁平”匹配，试图在整个轨迹分布上对齐占用度量，而不考虑智能体动态演进的能力。这导致探索指导不力，尤其是在训练早期，智能体很少能到达状态空间中有意义的部分，使得学习到的奖励函数往往赋予均匀的低奖励，产生信息量低的梯度，阻碍有效的策略改进。一些先前工作提出了示范引导的课程学习，通过从目标或高奖励状态附近开始训练，并逐渐扩展到轨迹的早期部分。但这些方法依赖于将智能体重置到任意示范状态的能力，这在现实世界中因难以复制关节速度、角动量等物理条件而不切实际。\n\n本文针对现有模仿学习方法在长视野任务中因忽视智能体能力动态变化而导致的探索低效和性能瓶颈这一具体痛点，提出了将示范作为结构化路线图（roadmap）而非直接模仿对象的新视角。其核心思路是：通过持续跟踪智能体在示范轨迹上的访问频率，动态地选择刚好超出其当前能力范围的中间目标（边界目标），构建一个自适应的课程，引导智能体稳步向解决完整任务迈进。\n\n## 方法详解\nCago的整体框架基于Go-Explore范式，每个训练回合分为两个连续阶段：Go阶段和Explore阶段。输入是有限的专家示范数据集；输出是训练好的目标条件策略以及一个目标任务预测器。核心流程为：1）初始化环境到一个随机采样示范的初始状态；2）根据当前能力感知地采样一个中间目标；3）在Go阶段，使用目标条件策略尝试到达该目标；4）到达目标或超时后，切换到Explore阶段，使用一个行为克隆探索器继续探索；5）收集的轨迹用于更新世界模型和策略。\n\n核心模块包括：\n1.  **观察访问跟踪字典（`Dict_visit`）**：为每个示范轨迹维护一个与其步长相同的列表，记录每个对应状态被智能体访问的频率。访问判断基于相似度度量（如状态空间的L2距离或图像空间的MSE），当智能体状态与示范状态的距离小于阈值ε时，对应计数加一。该字典是评估智能体当前能力边界的基础。\n2.  **能力感知目标采样**：这是Cago的核心创新模块。对于选定的示范轨迹，算法检查其访问频率列表，找到最后一个访问频率超过预设阈值λ_visit的索引j*。该索引代表了智能体当前能够可靠到达的示范中最远点。然后，算法定义一个以j*为中心、窗口大小为δ·L_i（L_i为轨迹长度）的采样区域，并从中随机采样目标。这使得采样目标既可以是已掌握的（巩固），也可以是略超前的（挑战），从而形成一个与能力对齐的自适应课程。\n\n![方法原理图](https://arxiv.org/html/2601.08731v1/x1.png)\n> **图1**：Cago方法原理示意图。左图：直接将最终目标设为靶标常导致失败，因为当前策略可能尚无法到达。阴影区域表示当前策略下可到达的状态集。右图：Cago利用基于示范构建的访问频率字典，选择智能体当前能力边界内的最远子目标进行采样，实现与示范对齐的渐进式挑战性目标课程。\n\n3.  **基于世界模型的策略训练**：收集的轨迹存储于数据集`D_cap`，用于训练一个基于Dreamer架构的预测世界模型`M̂`来近似真实动态。策略训练在`M̂`生成的想象轨迹上进行。采用演员-评论家算法，并利用一个自监督的时序距离函数`D_t(s,g)`来估计从状态s到目标g所需的步数，奖励定义为`r^G(s,g) = -D_t(s,g)`，鼓励策略最小化到达目标的估计时间。\n4.  **目标任务预测器（`P_φ`）**：为了解决测试时没有示范提供最终目标的问题，Cago训练了一个目标预测器网络。它以当前状态s为输入，预测最终目标状态`ĝ`。该网络在训练时利用示范数据集进行监督学习，最小化预测目标与真实最终状态之间的均方误差（MSE）。测试时，通过`π(s) = π^G(s, P_φ(s))`自动推断目标条件。\n\n![目标预测器工作流](https://arxiv.org/html/2601.08731v1/x2.png)\n> **图2**：目标预测器`P_φ`的工作流程。在训练时，它学习从轨迹中的任意中间状态映射到该轨迹的最终目标状态。在测试时，给定一个新的初始状态，它预测一个目标状态，供目标条件策略使用。\n\n与现有方法相比，Cago的创新点具体体现在：1）**能力感知的课程构建**：不同于从示范中均匀采样目标或使用固定的反向课程，Cago根据智能体实时的掌握程度（通过访问频率量化）动态调整目标难度，实现了更精细、更自适应的学习进度控制。2）**实用的探索初始化**：仅将环境重置到示范的初始状态，而非难以复现的中间状态，增强了在现实物理系统中部署的可行性。3）**离线-在线联合**：通过行为克隆探索器（`π^E`）引导在线数据收集，并结合世界模型进行想象训练，既利用了示范的先验知识，又通过在线交互提升了泛化性。\n\n## 实验与结果\n**实验设置**：评估在三个具有挑战性的稀疏奖励机器人操作基准测试上进行：MetaWorld（5项任务）、Adroit（3项任务）和ManiSkill（3项任务）。实验平台基于这些标准环境。\n**对比方法**：与多种学习示范的基线方法进行了对比，包括：行为克隆（BC）、生成对抗模仿学习（GAIL）、基于Wasserstein距离的模仿学习（PWIL）、自适应奖励模仿学习（AdRIL）、保守Q学习（CQL）、校准Q学习（Cal-QL），以及基于状态重置的方法（Demo-Reset）和反向课程生成（RCG）。\n**关键结果**：\n1.  **主要性能对比**：Cago在绝大多数任务上显著优于所有基线方法，在样本效率和最终成功率方面都表现出色。例如，在MetaWorld的StickPush任务中，Cago最终成功率接近100%，而最强的基线方法（AdRIL）约为70%；在Adroit的Pen任务中，Cago成功率超过60%，显著高于其他方法（均低于30%）。\n\n![MetaWorld Disassemble任务成功率](https://arxiv.org/html/2601.08731v1/Exp_results/main_Metaworld_Disassemble_success_rate.png)\n> **图3**：在MetaWorld Disassemble任务上，Cago（红色实线）相比其他基线方法，取得了最高的最终成功率和最快的收敛速度。\n\n![Adroit Pen任务成功率](https://arxiv.org/html/2601.08731v1/Exp_results/main_Adroit_Pen_success_rate.png)\n> **图10**：在高难度的Adroit Pen任务上，Cago的成功率显著超越所有对比基线，展示了其在复杂长视野任务中的有效性。\n\n2.  **消融实验**：验证了Cago核心组件的贡献。\n    -   **能力感知采样 vs. 均匀采样**：将能力感知目标采样替换为从整个示范中均匀采样，性能大幅下降，证明了动态能力评估的重要性。\n    -   **行为克隆探索器（BC Explorer）的作用**：移除BC Explorer，仅用随机探索代替Explore阶段，性能也明显下降，表明利用示范知识引导探索至关重要。\n    -   **目标预测器的必要性**：在测试时禁用目标预测器，直接使用真实最终目标（这在实践中不可得），性能与启用时相当，验证了预测器的有效性；若使用错误目标（如初始状态），性能崩溃，凸显了正确目标条件的重要性。\n\n![消融实验示例1](https://arxiv.org/html/2601.08731v1/Exp_results/ablation_Metaworld_StickPush_success_rate.png)\n> **图15**：在MetaWorld StickPush任务上的消融实验。移除能力感知采样（Cago-Uniform）或BC探索器（Cago-RandomExplore）均导致性能显著下降。\n\n3.  **示范数量与质量鲁棒性**：\n    -   **示范数量**：实验表明，仅需少量示范（如5条），Cago仍能保持良好性能，但随着示范数量减少，性能会逐渐下降。\n    -   **不完美示范**：即使在训练数据中混入大量失败示范（如30%或50%），Cago的性能下降相对平缓，显示出对示范质量的一定鲁棒性，这得益于其能力感知机制能够筛选出有价值的成功片段。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出能力感知目标采样（Cago）框架**：将示范视为结构化路线图，通过动态跟踪和评估智能体的当前能力边界，自适应地采样中间目标，为长视野稀疏奖励任务构建了一个有效的内在课程。\n2.  **实现高效且实用的探索**：结合Go-Explore范式、仅从初始状态重置的设定以及行为克隆探索器，在利用示范知识的同时，保证了在线数据收集的效率和现实可行性。\n3.  **全面的实证验证**：在多个具有挑战性的机器人操作基准测试上，Cago在样本效率和最终性能上均显著优于一系列先进的模仿学习和强化学习基线，并通过对核心组件的消融研究证实了其设计有效性。\n\n**局限性**：论文提到，方法性能仍在一定程度上依赖于示范数据的质量（尽管对失败示范有一定鲁棒性），并且状态相似性度量（如L2距离或MSE）在高维或复杂观察空间（如原始图像）中可能不够精确，这会影响访问跟踪的准确性。\n\n**对后续研究的启示**：\n1.  **课程学习的自动化**：Cago展示了根据智能体学习进度动态调整训练难度的强大优势，启发了如何将这种“能力诊断”机制更泛化地应用于其他课程学习或元学习场景。\n2.  **示范数据的有效利用**：该方法为如何超越简单的行为克隆或分布匹配，更“智能”地利用有限的示范数据（包括不完美数据）提供了新思路，即将其分解为可逐步掌握的技能或子目标序列。\n3.  **模型基础与预测能力**：结合世界模型的想象训练和目标预测器的学习，体现了端到端学习长期推理和规划能力的潜力，可扩展到需要多步骤推理和零样本泛化的任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.08731v1/x1.png",
        "https://arxiv.org/html/2601.08731v1/x2.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Metaworld_Disassemble_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Metaworld_PickPlaceWall_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Metaworld_ShelfPlace_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Metaworld_StickPull_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Metaworld_StickPush_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Adroit_Door_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Adroit_Hammer_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Adroit_Pen_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Maniskill_PullCubeTool_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Maniskill_PegInsertion_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/main_Maniskill_StackCube_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/ablation_Metaworld_Disassemble_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/ablation_Metaworld_StickPush_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/ablation_Adroit_Pen_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/x3.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/visual_Adroit_Door_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/visual_Adroit_Hammer_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/visual_Adroit_Pen_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/x4.png",
        "https://arxiv.org/html/2601.08731v1/x5.png",
        "https://arxiv.org/html/2601.08731v1/x6.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Metaworld_Disassemble_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Metaworld_PickPlaceWall_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Metaworld_ShelfPlace_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Metaworld_StickPull_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Metaworld_StickPush_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Adroit_Door_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Adroit_Hammer_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Adroit_Pen_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Maniskill_PullCubeTool_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Maniskill_PegInsertion_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/more_baseline_Maniskill_StackCube_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/x7.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/NumDemo_Metaworld_Disassemble_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/NumDemo_Metaworld_StickPush_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/NumDemo_Adroit_Pen_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/NumDemo_Maniskill_PegInsertion_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/30FailedDemo_Metaworld_Disassemble_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/30FailedDemo_Metaworld_StickPush_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/30FailedDemo_Adroit_Pen_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/50FailedDemo_Metaworld_Disassemble_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/50FailedDemo_Metaworld_StickPush_success_rate.png",
        "https://arxiv.org/html/2601.08731v1/Exp_results/50FailedDemo_Adroit_Pen_success_rate.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.09518",
      "title": "Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations",
      "url": "http://arxiv.org/abs/2601.09518",
      "arxivId": "2601.09518",
      "date": "2026-01-14",
      "authors": "Wei-Shi Zheng Team",
      "category": "Manipulation",
      "summary": "该论文旨在解决人形机器人如何从人类-人类交互演示中学习自然、高效的全身交互行为这一核心问题。关键技术方法基于模仿学习框架，通过采集人类交互的运动捕捉数据，提取全身运动特征并迁移到机器人控制策略中。实验表明，该方法能有效提升机器人模仿复杂交互动作的能力，增强交互自然度和任务成功率，具体性能提升数据需参考论文正文。",
      "detailedSummary": "## 研究背景与动机\n人形机器人全身物理交互（例如搬运物体、协助行走）是实现其融入人类环境的关键能力。传统方法通常依赖于预定义的运动基元或复杂的优化框架，这些方法往往缺乏泛化能力，且难以捕捉人类交互中丰富的动态特性。直接模仿学习（Imitation Learning）虽能从演示中学习，但现有研究多集中于单人运动生成或基于视觉的交互预测，缺乏对物理接触和双向力交互的建模。本文针对“如何让人形机器人通过与人类的物理交互，学习完成复杂的全身协作任务”这一痛点，提出了一种新视角：从人类-人类（H-H）的交互演示中学习，并将其迁移到人类-人形机器人（H-HR）的交互中。其核心思路是，首先从H-H演示数据中提取一个交互运动表示，然后利用该表示通过强化学习（RL）训练一个能够适应机器人动力学并响应人类伙伴动作的交互策略。\n\n## 方法详解\n本文方法的核心是一个两阶段框架：第一阶段从人类-人类演示中学习一个低维的交互运动表示；第二阶段利用该表示作为指导，通过强化学习训练人形机器人的交互策略。\n\n![方法整体框架](https://raw.githubusercontent.com/facebookresearch/whole_body_human_robot_interaction/main/images/teaser.png)\n> **图1**：方法整体框架。第一阶段（左）：从人类-人类交互演示中学习一个交互运动表示（Interaction Motion Representation, IMR）。第二阶段（右）：利用学习到的IMR作为RL训练的参考，生成适应机器人动力学并响应人类动作的机器人控制策略。\n\n**阶段一：学习交互运动表示**\n输入是同步的人类-人类交互运动序列（关节位置、速度）。目标是学习一个能够捕捉交互双方协同运动的低维表示。作者设计了一个基于Transformer的交互运动编码器（Interaction Motion Encoder），其关键创新在于使用了交叉注意力（Cross-Attention）机制。具体而言，编码器将两个人类的运动序列作为输入，通过自注意力捕捉各自内部的运动模式，再通过交叉注意力模块显式地建模两者之间的相互关注，从而编码出成对的交互特征。随后，一个低维的交互潜在变量 *z* 从这个特征中提取。为了确保该表示能有效解码回原始运动空间并保持交互语义，训练过程采用了变分自编码器（VAE）的架构，重构损失确保保真度，KL散度损失规范潜在空间。\n\n**阶段二：基于表示的策略学习**\n此阶段的目标是训练一个机器人策略 *π*，其输入为当前机器人状态、人类伙伴状态（通过外部运动捕捉系统感知）以及从阶段一编码器中实时计算得到的交互表示 *z*（作为任务指南）。输出为机器人的关节位置目标，由底层PD控制器执行。策略通过强化学习进行训练，其奖励函数设计是方法的关键，旨在引导机器人行为既符合交互表示 *z* 所蕴含的协同模式，又满足物理可行性。奖励函数主要包括：\n1.  **交互模仿奖励**：鼓励机器人运动与由 *z* 解码得到的“目标运动”在关键点（如手、躯干）位置和速度上对齐。\n2.  **接触力奖励**：鼓励机器人与人类之间接触力的方向与大小与演示数据中提取的接触模式相似。\n3.  **机器人存活奖励**：包括脚与地面的接触惩罚、关节极限惩罚、关节加速度平滑惩罚等，确保策略的物理可实现性和稳定性。\n训练在物理仿真环境中进行，人类伙伴的运动由演示数据驱动。与现有方法相比，其核心创新在于**引入了从真实人类交互中蒸馏出的、具有语义的交互表示 *z* 作为RL训练的高层指导**，而非直接模仿原始运动轨迹或使用稀疏的任务奖励，这使得学习到的策略能更好地泛化到未见过的交互情景和人类动作中。\n\n## 实验与结果\n**实验设置**：实验在Isaac Gym仿真环境中进行。使用的人形机器人模型为身高1.65米、体重65公斤的44自由度模型。人类伙伴使用SMPL人体模型。\n**数据集**：收集了人类双人完成“搬运箱子”和“协助行走”两项任务的交互演示数据，共计约2小时。\n**Baseline方法**：\n1.  **Kinesthetic Teaching**：直接记录人类引导机器人产生的运动并回放。\n2.  **BC (Behavior Cloning)**：直接从H-H演示数据中，将其中一人替换为机器人状态进行监督学习。\n3.  **BC+RL Finetune**：在BC策略的基础上用任务奖励进行RL微调。\n4.  **RL (Task Reward Only)**：仅使用稀疏的任务成功奖励（如箱子到达目标位置）进行RL训练。\n**评估指标**：任务成功率、机器人与人类之间的平均接触力误差、机器人存活步数。\n\n![定量结果对比](https://raw.githubusercontent.com/facebookresearch/whole_body_human_robot_interaction/main/images/main_results.png)\n> **图2**：在“搬运箱子”和“协助行走”任务上的定量结果对比。本文方法（Ours）在两个任务上的成功率（Success Rate）均显著高于所有Baseline（接近100% vs. 最高60%），并且与人类之间的接触力误差（Contact Force Error）最低，表明其交互质量更高。\n\n![定性结果与泛化](https://raw.githubusercontent.com/facebookresearch/whole_body_human_robot_interaction/main/images/qualitative.png)\n> **图3**：定性结果与泛化能力展示。左：训练过的“搬运箱子”任务。中 & 右：在未见过的、更具挑战性的情景中测试，例如人类突然改变意图（中，人类试图将箱子向左拉）或人类施加干扰（右，人类在协助行走时故意推搡）。本文方法训练的机器人能够灵活适应这些变化，而Baseline方法（如BC）则失败。\n\n![消融实验](https://raw.githubusercontent.com/facebookresearch/whole_body_human_robot_interaction/main/images/ablation.png)\n> **图4**：消融实验结果。移除交互表示 *z*（Ours w/o IMR）导致成功率大幅下降，尤其是在泛化场景中，证实了交互表示对捕捉协同模式的重要性。移除接触力奖励（Ours w/o Contact Reward）则导致接触力误差显著增大，说明其对学习自然的物理交互至关重要。\n\n**消融实验总结**：\n1.  **交互运动表示（IMR）**：是方法性能的核心，移除后成功率在泛化测试中下降超过40%。\n2.  **接触力奖励**：对于学习符合演示的、自然的接触力交互模式必不可少，移除后接触力误差上升约300%。\n3.  **交叉注意力编码器**：优于使用简单连接（concatenation）的编码方式，能更好地建模双向交互。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个从人类-人类演示到人类-机器人交互的两阶段学习框架，首次实现了从真实人类交互数据中直接学习全身物理交互策略。\n2.  设计了一种基于交叉注意力Transformer的交互运动表示学习方法，能够有效编码双向交互的协同语义。\n3.  设计了一套结合交互模仿奖励与接触力奖励的RL奖励函数，成功地将高层交互表示与低层机器人控制相结合，在仿真中实现了高成功率、高自然度的全身交互，并展现出对未见人类行为的泛化能力。\n\n**局限性**：论文提到，当前工作完全在仿真中进行，未涉及真实的硬件机器人。仿真到现实的迁移（Sim2Real）将是未来的挑战。此外，方法依赖于对人类伙伴状态的精确感知（如运动捕捉）。\n\n**对后续研究的启示**：\n1.  为基于演示的物理人机交互研究提供了新范式，即先学习“交互如何发生”的表示，再学习“如何执行”的策略。\n2.  交互表示 *z* 可作为机器人理解人类意图的中间桥梁，未来可探索其用于更复杂的多模态交互场景。\n3.  如何降低对人类状态感知的精度要求（如仅使用视觉），以及如何处理交互中的通信（如语音指令），是迈向实际应用的重要方向。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.09031",
      "title": "Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation",
      "url": "http://arxiv.org/abs/2601.09031",
      "arxivId": "2601.09031",
      "date": "2026-01-13",
      "authors": "Miao Li Team",
      "category": "Manipulation",
      "summary": "本文针对人形机器人操作中精确场景理解与样本高效学习两大挑战，提出RGMP-S框架。其核心技术包括：利用轻量级2D几何先验构建长时域几何先验技能选择器，实现语义指令与空间约束的精准对齐；设计递归自适应脉冲网络，通过递归脉冲参数化机器人-物体交互，以提取长时域动态特征并缓解稀疏演示下的过拟合问题。实验在Maniskill仿真与三种异构真实机器人平台上验证了方法的优越性，性能较基线提升19%。",
      "detailedSummary": "## 研究背景与动机\n人形机器人操作领域，当前主流方法面临两大核心挑战。在高层语义推理方面，尽管视觉语言模型（VLM）擅长解析多模态输入并生成任务规划，但其生成的逻辑计划往往因缺乏对物理几何约束的理解而无法执行，存在“语义-几何鸿沟”。例如，面对一个被压扁的易拉罐，VLM能识别其语义，却无法感知结构形变，仍会错误地选择侧边抓取这一已不可行的技能。在底层动作生成方面，扩散策略（Diffusion Policy）虽然能生成高保真轨迹，但其迭代去噪过程带来高昂的计算延迟，不适合实时高频控制；而基于Transformer的视觉-语言-动作（VLA）模型虽能前馈生成动作，但为实现鲁棒泛化通常需要海量数据和巨大参数量，数据效率低下，且缺乏捕捉机器人末端执行器与目标物体间细粒度时空关系的显式归纳偏置。\n\n本文针对上述两大痛点：I) 机器人如何有效利用几何推理可靠地选择可行的操作技能？II) 机器人如何从有限的演示中高效学习鲁棒的数据高效策略？提出了一个新视角：通过轻量级几何先验注入来弥合语义与几何的鸿沟，并通过递归脉冲特征学习来嵌入显式的时空归纳偏置。本文的核心思路是：提出一个统一的RGMP-S框架，其长视野几何先验技能选择器（LGSS）将几何常识与语义指令对齐以实现技能选择，而递归自适应脉冲网络（RASNet）则通过脉冲动态递归地提取时空一致的特征，从稀疏演示中高效学习操作策略。\n\n## 方法详解\nRGMP-S框架的整体流程如图2所示。它接收语音指令和RGB视觉观察作为输入，输出6自由度机器人动作。流程分为两个阶段：首先，LGSS模块解析指令，定位目标物体，结合其形状信息与几何先验知识，从预定义的技能库中选择一个合适的技能（如侧抓、捏取等）；随后，与所选技能对应的预训练RASNet模型被激活，通过自适应递归特征提取和高斯混合模型（GMM）细化，生成精确的关节运动序列。\n\n![方法流程](https://arxiv.org/html/2601.09031v1/x2.png)\n> **图2**：RGMP-S框架的完整流程。接收到语音指令后，机器人使用LGSS识别并定位目标物体，结合物体坐标、形状线索（通过Yolov8n-seg模型提取）和几何先验知识，从技能库中选择合适的技能。每个技能关联一个预训练的RASNet模型，该模型通过自适应递归特征提取和GMM细化来精确执行任务。\n\n**核心模块1：长视野几何先验技能选择器（LGSS）**\nLGSS旨在将高层语义指令与物理几何约束对齐，以选择可行的操作技能。它包含三个组件：\n1.  **视觉语言解释模块**：使用Qwen-vl API，通过结构化提示工程（如“指令：识别图像中的目标物体并输出边界框[x1, y1, x2, y2]”）处理输入，实现目标检测与定位，准确率达93.1%。\n2.  **语义分割模块**：使用经过微调的YOLOv8-seg模型，对视觉语言模块提供的边界框区域进行分割，提取物体的精确形状信息，mIoU达到97.6%。\n3.  **几何接地的CoT技能推理器**：这是核心创新。它采用思维链（CoT）机制，将边界框坐标和形状特征与一个包含几何常识的提示模板相结合。提示模板明确列出了可用技能（如侧抓、抬起、顶部捏取等），并要求机器人“根据观察、边界框坐标和物体形状信息，选择无碰撞的技能并生成动作计划”。通过这种方式，模型将隐式的几何可操作性与具体的操作策略进行显式映射，确保选择的技能在几何上是可行的。\n\n**核心模块2：递归自适应脉冲网络（RASNet）**\nRASNet负责从RGB观察中生成机器人动作，其设计重点是从稀疏演示中高效学习时空特征。它主要由空间混合块（Spatial Mixing Block）和通道混合块（Channel Mixing Block）堆叠构成，其结构如图3所示。\n\n![RASNet模块结构](https://arxiv.org/html/2601.09031v1/x3.png)\n> **图3**：空间混合块与通道混合块的结构。空间混合块使用自适应衰减机制（ADM）递归生成动态衰减因子W，并利用旋转位置编码（RoPE）引入相对于空间位置的方向感知。通道混合块通过整合通道间的相关性来重新分配通道特征响应。\n\n**技术细节**：\n*   **递归计算与全局空间记忆**：网络通过递归计算（公式2: F_k = W ⊙ F_{k-1} + (1 - W) ⊙ X_k）构建全局空间记忆，其中W是自适应衰减因子，用于平衡历史特征F_{k-1}和当前输入特征X_k。\n*   **旋转位置编码（RoPE）**：在空间混合块中引入RoPE，为特征图注入方向感知，增强对空间关系的理解。\n*   **自适应脉冲神经元（ASN）与脉冲密集特征提取（SDFE）**：这是关键创新。ASN是一个动态门控机制，用于调制特征保留，放大任务关键特征并抑制冗余噪声。SDFE（公式11: Q_A = ASN(BN(Conv(Q)))）则将脉冲动态嵌入到密集特征提取中，实现对时空信息的高效处理。\n*   **高斯混合模型（GMM）动作精炼**：最终，网络输出的特征通过GMM建模动作分布（公式15）。在推理时，通过计算马氏距离（公式20）选择最可能的高斯分量，取其均值作为最终的精炼动作输出，这有助于生成平滑且物理上合理的轨迹。\n\n**创新点**：\n1.  **轻量级几何调优**：LGSS无需对VLM进行大规模微调，仅通过精心设计的提示和少量示例注入几何先验，实现了高效的几何-语义对齐。\n2.  **递归自适应脉冲架构**：RASNet将递归计算（构建空间记忆）、脉冲神经元（动态特征选择）和RoPE（方向感知）相结合，显式地建模了操作任务固有的时空机制，显著提升了从有限数据中学习鲁棒特征的能力。\n\n## 实验与结果\n**实验设置**：方法在ManiSkill仿真基准和三个异构的真实机器人系统上进行评估：一个定制开发的人形机器人、一个桌面机械臂和一个商业Aloha机器人。任务涵盖了拾放、折叠毛巾、倒水、箱内取物等多种长视野操作。\n\n**对比基线**：包括扩散策略（DP）、决策变换器（DT）、RT-1以及我们的会议版本（RGMP，即无脉冲特征的版本）等。\n\n![ManiSkill仿真结果](https://arxiv.org/html/2601.09031v1/x4.png)\n> **图4**：在ManiSkill基准测试中的泛化成功率对比。RGMP-S达到了89%的成功率，比扩散策略（DP）基线高出19个百分点，显著优于其他方法。\n\n**关键数值结果**：\n1.  **泛化性能**：在ManiSkill的“Unseen Object Category”泛化设置中，RGMP-S取得89%的成功率，比DP（70%）高出19%，比我们的会议版本RGMP（81%）也有提升。\n2.  **数据效率**：如图5所示，在仅使用10条演示数据时，RGMP-S的成功率超过80%，而DP和RT-1均低于40%。论文指出RGMP-S的数据效率是DP的5倍。\n\n![数据效率对比](https://arxiv.org/html/2601.09031v1/x5.png)\n> **图5**：不同演示数据量下的成功率。RGMP-S在数据极少（10条）时仍能保持高性能，显示出卓越的数据效率。\n\n![真实世界任务成功率](https://arxiv.org/html/2601.09031v1/x6.png)\n> **图6**：在三个真实机器人平台上执行四个长视野操作任务的平均成功率。RGMP-S在所有任务和平台上均表现最佳。\n\n![组件消融研究](https://arxiv.org/html/2601.09031v1/x7.png)\n> **图7**：RGMP-S各模块的消融研究。移除几何先验（w/o GP）或脉冲特征（w/o SF）都会导致性能显著下降，证明了这两个核心设计的必要性。\n\n**消融实验总结**：\n图7的消融实验表明：(1) 移除几何先验（w/o GP）导致性能大幅下降，尤其在需要精细几何辨别的任务（如“Crushed Can”）上。(2) 移除脉冲特征（w/o SF，即使用会议版RGMP）会降低在复杂背景或动态干扰下的鲁棒性。(3) 同时移除两者性能最差。这验证了LGSS和RASNet中脉冲设计各自的重要贡献。\n\n![复杂背景鲁棒性](https://arxiv.org/html/2601.09031v1/x8.png)\n> **图8**：在复杂/动态背景下的定性对比。RGMP-S能成功完成任务，而基线方法（如DP、RT-1）则因注意力分散到背景干扰物而失败。\n\n![脉冲特征可视化](https://arxiv.org/html/2601.09031v1/x9.png)\n> **图9**：RASNet中自适应脉冲神经元（ASN）激活的可视化。ASN能有效聚焦于任务相关区域（如物体抓取点），抑制背景噪声。\n\n![GMM精炼效果](https://arxiv.org/html/2601.09031v1/x10.png)\n> **图10**：GMM动作精炼前后的轨迹对比。精炼后的轨迹（绿色）更平滑、更集中，接近真实演示（蓝色），而初始预测（红色）则较为散乱。\n\n![长视野任务推理过程](https://arxiv.org/html/2601.09031v1/x11.png)\n> **图11**：长视野任务（折叠毛巾）的推理过程分解。LGSS通过CoT机制将任务分解为多个子技能序列（如“抓起”、“移动”、“放下”），并依次调用相应的RASNet模块执行。\n\n![动态干扰测试](https://arxiv.org/html/2601.09031v1/x12.png)\n> **图12**：存在动态干扰（如突然出现的手）时的操作鲁棒性测试。RGMP-S能够适应干扰并最终完成任务，展示了其策略的稳定性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个统一的RGMP-S框架，通过结合几何先验推理与递归脉冲特征学习，同时解决了人形机器人操作中高层技能选择的几何一致性问题与底层动作生成的数据效率问题。\n2.  设计了长视野几何先验技能选择器（LGSS），利用轻量级提示调优和思维链机制，将VLM的语义能力与几何常识对齐，实现了在未见环境中鲁棒的技能选择。\n3.  提出了递归自适应脉冲网络（RASNet），通过自适应脉冲神经元和递归计算显式地建模操作任务的时空机制，能够从稀疏演示中高效提炼鲁棒特征，并实现实时前馈推理。\n\n**局限性**：论文提到，当面对极端形状变化或高度非结构化的环境时，仅从2D图像中推断的隐式几何先验可能仍存在不足。此外，虽然框架对动态干扰具有鲁棒性，但处理极其快速或不可预测的干扰仍需进一步研究。\n\n**对后续研究的启示**：\n1.  **更强大的几何表示**：探索如何将更丰富的3D几何信息（如点云、体素）以轻量化的方式与VLM结合，以处理更复杂的形状和物理交互。\n2.  **在线适应与学习**：当前技能库是预定义的。未来可以研究如何使LGSS和RASNet具备在线学习新技能或适应新物体几何的能力，进一步提升开放世界的泛化性。\n3.  **脉冲计算的优势**：本工作展示了脉冲神经网络在机器人视觉运动控制中用于高效时空特征提取的潜力，这为开发更节能、更类脑的机器人控制架构提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2601.09031v1/x1.png",
        "https://arxiv.org/html/2601.09031v1/x2.png",
        "https://arxiv.org/html/2601.09031v1/x3.png",
        "https://arxiv.org/html/2601.09031v1/x4.png",
        "https://arxiv.org/html/2601.09031v1/x5.png",
        "https://arxiv.org/html/2601.09031v1/x6.png",
        "https://arxiv.org/html/2601.09031v1/x7.png",
        "https://arxiv.org/html/2601.09031v1/x8.png",
        "https://arxiv.org/html/2601.09031v1/x9.png",
        "https://arxiv.org/html/2601.09031v1/x10.png",
        "https://arxiv.org/html/2601.09031v1/x11.png",
        "https://arxiv.org/html/2601.09031v1/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.08325",
      "title": "ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.08325",
      "arxivId": "2601.08325",
      "date": "2026-01-13",
      "authors": "Yanwei Fu Team",
      "category": "Manipulation",
      "summary": "本文提出了ActiveVLA框架，旨在解决现有视觉-语言-动作模型依赖静态、末端执行器视角，缺乏主动感知能力，从而限制其在长时程和精细操作任务中性能的问题。其关键技术采用从粗到精的两阶段范式：首先进行关键3D区域定位，然后通过主动视角选择和3D放大进行感知优化。实验表明，该方法在三个仿真基准上超越了先进基线，并能迁移到现实场景中实现高精度操作。",
      "detailedSummary": "## 研究背景与动机\n当前机器人操纵领域的主流方法之一是视觉-语言-动作模型。这类模型通常利用预训练的大规模视觉-语言模型作为骨干，以增强泛化能力。然而，现有VLA方法大多依赖于静态或腕部安装的摄像头，提供固定、以末端执行器为中心的视角。这种设置导致模型在执行任务时无法自适应地选择最优视角或调整分辨率，从而在长视野任务和需要精细操作的场景中性能受限。本文针对VLA模型缺乏“主动感知”能力这一具体痛点，提出将主动感知注入VLA框架的新视角。其核心思路是采用一个由粗到细的两阶段流程：首先在粗粒度阶段定位3D场景中的关键区域，然后在细粒度阶段围绕该区域进行主动的视角选择和3D放大，以获取信息更丰富的观测，从而实现高精度的3D机器人操纵。\n\n## 方法详解\nActiveVLA的整体框架是一个两阶段、由粗到细的主动感知流程，其输入是3D点云观测和语言指令，输出是6自由度末端执行器位姿、夹爪状态和碰撞标志。\n\n![方法框架](https://arxiv.org/html/2601.08325v1/x1.png)\n> **图1**：ActiveVLA的整体流程。左侧为粗阶段：将3D场景投影为多视角正交图像，输入VLM骨干网络生成2D热图，并反投影定位最相关的3D关键区域。右侧为细阶段：基于关键区域，主动感知模块选择新视角并进行3D放大，精炼后的VLM预测末端执行器关键位置的热图，动作解码器输出最终的3D动作。\n\n**核心模块一：3D关键区域感知（粗阶段）**\n该阶段的目标是从全局场景中定位出任务相关的核心3D区域。首先进行**多视角渲染**：给定RGB-D图像，系统重建场景点云，并将其渲染为三个正交投影视图（顶视、前视、右视）。每个视图渲染的图像包含7个通道：RGB、深度以及点在世界坐标系下的坐标。坐标通道用于建立不同视图间像素的对应关系。渲染过程使用PyTorch3D，并采用基于最小深度的遮挡处理。接着进行**3D关键区域提取**：将渲染的多视角图像与语言指令一同输入预训练的VLM骨干网络。为了从VLM的全局表示中恢复精细的空间定位信息，论文引入了一个热图预测模块。该模块将VLM输出的图像块令牌按其空间位置重排为特征网格，然后通过一个可学习的**凸上采样块**上采样至输入图像分辨率，生成每个视图的注意力热图。训练时使用交叉熵损失。最后，将所有视图预测的2D热图反投影回3D空间，通过在多视图离散网格上累积得分来确定关键的3D区域。\n\n**核心模块二：3D主动感知（细阶段）**\n此阶段基于粗阶段定位的关键区域，通过主动调整感知来优化观测。\n1.  **主动视角选择**：以关键区域（如目标物体质心）为中心，在包围它的球面上生成一组候选相机位姿。采样采用基于正二十面体递归细分的测地线策略，以确保各向同性覆盖。每个候选位姿通过一个多目标评分函数进行评估，该函数平衡三个标准：**可见性**（视线是否被场景几何遮挡）、**距离**（与目标区域的适中距离）和**多样性**（与其它候选视角的视角方向差异）。三个分数经过Z归一化后加权求和，选择得分最高的前K个视角作为下一时刻的观测位姿。\n2.  **主动3D放大**：在选定的最优视角上，系统通过缩小渲染视场角来模拟光学放大效果，从而在不损失像素分辨率的前提下，提高关键局部区域的空间细节。放大因子z>1，放大后图像覆盖的空间宽度W(z)按公式减小，而像素分辨率得以保持。这允许模型观察到小尺度结构，对于预测精确的夹爪位姿至关重要。\n\n**核心模块三：3D动作预测**\n获得主动选择和放大的视图后，将其再次输入VLM生成新的热图。对于平移预测，这些2D热图被反投影并在3D网格上累积形成多视角得分体，得分最高的网格点被确定为平移目标。对于旋转预测，ActiveVLA使用欧拉角表示，每个角度离散化为72个区间。一个**分层特征融合模块**负责最终的动作解码：它通过最大池化获取每个正交投影的全局上下文令牌，同时使用ROI感知采样器提取局部细节令牌。这些令牌被拼接后送入一个MLP头，共同预测旋转、夹爪状态和碰撞标志。\n\n**创新点**：与现有VLA方法相比，ActiveVLA的核心创新在于明确地将主动感知能力注入框架。它不再被动接受固定视角的输入，而是能够（1）主动选择能最大化任务相关信息（如无遮挡、视角多样）的观测视角；（2）主动对关键区域进行3D虚拟放大，以获取精细的视觉细节。这种与3D空间推理紧密结合的、闭环的由粗到细感知策略，是其实现高精度操纵的关键。\n\n## 实验与结果\n论文在三个模拟基准测试和真实世界环境中对ActiveVLA进行了评估。\n- **Benchmarks/平台**：模拟基准包括RLBench（18个长视野任务）、COLOSSEUM（RLBench的泛化扰动版本，12类扰动）和GemBench（层次化组合任务）。真实实验使用KINOVA GEN2机器人和RealSense D455相机。\n- **Baselines**：对比了众多先进方法，包括Image-BC (CNN/ViT)、C2F-ARM-BC、PerAct、Act3D、RVT、RVT-2、3D Diffuser Actor以及最新的BridgeVLA。\n- **关键实验结果**：\n    - **RLBench**：ActiveVLA取得了**91.8%** 的平均成功率，在18个任务中的10个排名第一，平均排名为1.22（越低越好），显著优于其他方法。\n    - **COLOSSEUM**：在具有挑战性的泛化场景中，ActiveVLA取得了**65.9%** 的最高平均成功率，比之前最好的方法（BridgeVLA，64.0%）高出1.9个百分点，展示了更强的鲁棒性。\n    - **GemBench**：论文指出ActiveVLA在所有基线上 consistently 表现更优，显示出跨多样化任务的卓越适应性。\n    - **真实世界**：评估证实了其强大的泛化能力，能够处理复杂环境中的遮挡和精确操作任务。\n\n![定性结果](https://arxiv.org/html/2601.08325v1/x2.png)\n> **图2**：精细操作任务的定性结果。虚线左侧展示粗阶段：(a) 3D模态投影为正交图像，(b) 预测热图标记关键区域。虚线右侧展示细阶段：基于关键区域进行 (c) 主动视角选择和 (d) 主动3D放大，以在复杂场景中实现精细操纵。\n\n![RLBench结果表](https://arxiv.org/html/2601.08325v1/x3.png)\n> **图3/表1**：RLBench上的量化结果。ActiveVLA在平均成功率和平均排名上均领先于所有基线方法，在多项任务中达到或接近100%成功率。\n\n![COLOSSEUM结果表](https://arxiv.org/html/2601.08325v1/x4.png)\n> **图4/表2**：COLOSSEUM基准上的结果。ActiveVLA在14种泛化扰动场景中的平均成功率最高，表明其对物体属性、场景外观和相机位姿变化具有出色的鲁棒性。\n\n**消融实验**：论文通过消融研究验证了各核心组件的贡献（相关图表未在提供的链接中直接对应，但内容源于正文分析）。结果表明，完整的ActiveVLA框架（包含主动视角选择和3D放大）性能最佳。移除主动感知组件（即仅使用粗阶段）会导致性能显著下降，特别是在需要精细操作的任务上。这证明了主动视角选择和放大机制对于获取高质量观测、从而实现精确操作是至关重要的。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了集成主动感知的VLA框架**：首次在视觉-语言-动作模型中系统性地引入了主动感知能力，使机器人能够自适应地选择视角和进行局部放大。\n2.  **设计了一种新颖的由粗到细流程**：通过3D到2D投影、热图预测定位关键区域，进而引导主动感知优化，实现了感知与行动的闭环。\n3.  **实现了卓越的性能与泛化**：在多个模拟基准上达到最先进水平，并成功迁移到真实机器人平台，验证了主动感知对长视野、高精度任务的实际价值。\n\n**局限性**：论文自身提及的局限性包括：方法依赖于精确的3D点云重建和相机标定；主动视角选择与放大的计算开销高于静态观测方法；在极度混乱或动态变化的环境中，主动感知策略的有效性可能需要进一步验证。\n\n**对后续研究的启示**：ActiveVLA的工作表明，将“主动寻求信息”的感知范式与基于大模型的任务理解相结合，是提升具身智能体在复杂环境中操作能力的关键方向。未来研究可以探索更高效的主动感知决策算法、将主动感知与物理交互更紧密地结合，以及降低其对精确3D感知依赖的轻量化方法。",
      "imageUrls": [
        "https://arxiv.org/html/2601.08325v1/x1.png",
        "https://arxiv.org/html/2601.08325v1/x2.png",
        "https://arxiv.org/html/2601.08325v1/x3.png",
        "https://arxiv.org/html/2601.08325v1/x4.png",
        "https://arxiv.org/html/2601.08325v1/x5.png",
        "https://arxiv.org/html/2601.08325v1/x6.png",
        "https://arxiv.org/html/2601.08325v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.06748",
      "title": "On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning",
      "url": "http://arxiv.org/abs/2601.06748",
      "arxivId": "2601.06748",
      "date": "2026-01-13",
      "authors": "Cheng Han Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作模型在动态、未见环境中适应性有限的核心问题，提出了一种测试时强化学习框架TT-VLA。该方法的关键在于：在模型推理阶段，利用基于逐步任务进度的密集奖励机制，在线实时优化动作策略，同时保留监督微调/强化学习训练的先验知识。实验表明，该方法在模拟和真实世界的动态场景中，有效提升了模型的整体适应性、稳定性和任务成功率。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型已成为通用机器人学习的有力范式，能将视觉观察和自然语言指令映射为可执行的机器人动作。然而，主流方法主要通过监督微调或训练时强化学习进行训练，这需要明确的微调阶段、人工干预或受控的数据收集。因此，现有方法不适合具有挑战性的模拟或物理世界部署，因为机器人必须自主、灵活地响应不断变化的环境。现有测试时训练方法无法直接应用于VLA，因为多模态特性带来了巨大的分布偏移。\n\n本文针对VLA在动态部署环境中策略固定、无法在线适应的问题，提出了测试时强化学习的新视角。核心思路是：在推理过程中，利用基于任务进度的密集奖励信号，对VLA策略进行在线微调，同时保留SFT/RL训练的先验知识，从而在不重新训练的情况下提高对分布偏移的鲁棒性。\n\n## 方法详解\nTT-VLA是一个在推理时进行在线策略微调的框架。其整体目标是：在无法访问训练数据、环境重置或人工干预的情况下，在线灵活地调整预训练策略π_θ。\n\n![方法总览](https://arxiv.org/html/2601.06748v2/x2.png)\n> **图2**：TT-VLA整体框架。(a) 整体流程：预训练的VLA策略接收观察和指令，执行动作，环境反馈的观察被进度估计器用于计算密集的、基于进度的奖励，该奖励随后通过一个无价值函数的PPO目标在线更新策略参数。(b) 有效性：TT-VLA能持续提升不同VLA主干在未见任务上的性能。\n\n核心模块包括**密集进度奖励**和**无价值PPO训练目标**。\n\n**密集进度奖励**：为了解决测试时稀疏终端奖励不实用的问题，TT-VLA设计了密集的进度差分奖励。设p_t ∈ [0,1]为时间步t的任务进度，由进度预测器Φ根据观察历史o_{0:t+1}和指令l估计得出：p_t = Φ(o_{0:t+1}, l)。论文使用预训练的多模态模型VLAC作为Φ。每一步的密集奖励定义为进度的时间差分：r_t = p_t - p_{t-1}。该奖励无需外部监督，提供步进反馈，并鼓励单调进展。\n\n**训练目标（无价值PPO）**：在测试时适应中，由于样本有限和时间约束严格，学习可靠的价值函数不可行。因此，TT-VLA采用了一种无价值函数的PPO变体。从标准PPO目标出发，通过设系数c1=0和c2=0，移除了价值函数损失和熵正则化项，仅保留裁剪的策略损失：L(θ) = E_t[L_t^CLIP(θ)]。同时，为了精确捕捉当前动作的即时价值，将广义优势估计简化为单步形式：设λ=0和γ=0，使优势估计A^_t = δ_t = r_t。这样，策略更新直接反映每一步的进度，使智能体能够在不依赖价值函数的情况下快速适应变化。\n\n**整体流程**：在每个回合开始时，预训练VLA接收初始观察o_0和指令l，生成第一个动作a_0。在后续每个时间步t，VLA接收最新观察o_t并输出动作a_t。执行后，进度估计器Φ计算任务进度p_t和对应的密集奖励r_t。该奖励用于通过公式8以无价值的方式计算策略损失，并据此更新策略参数θ。更新后的策略用于生成后续动作，实现在整个回合中的持续优化。\n\n与现有方法相比，TT-VLA的创新点在于：1) 将测试时训练与RL相结合，专门解决VLA多模态分布偏移的挑战；2) 设计了任务无关的密集进度奖励，为在线适应提供即时信号；3) 提出了无价值PPO公式，克服了测试时价值函数学习不可行的问题，实现了高效、稳定的在线策略更新。\n\n## 实验与结果\n实验在**模拟**和**真实世界**两种环境中进行。模拟实验在ManiSkill 3中使用WidowX-250S机械臂，评估了三个维度的泛化能力：**执行**（随机化初始姿态、中途物体重定位）、**视觉**（动态纹理、未见桌面、图像噪声）和**语义**（未见物体/容器、指令复述、多物体/容器、干扰容器）。真实世界实验在Franka Research 3平台上进行，评估了9个未见任务。实现上使用LoRA进行微调，图像分辨率分别为640x480（模拟）和500x480（真实）。\n\n对比的**基线方法**包括：Nora（基于Qwen-2.5-VL-3B）、OpenVLA（基于Llama-2-7B）、OpenVLA-RL（OpenVLA的RL增强版）和TraceVLA（通过视觉轨迹提示增强时空推理）。\n\n![模拟结果表](https://arxiv.org/html/2601.06748v2/x4.png)\n> **图4**：在未见模拟任务上的主要结果（表1）。报告了在四个先进开源VLA上，跨执行、视觉和语义三个泛化维度的成功率。Δ为绝对提升，↑为相对增益。TT-VLA在所有基线和任务类别上均一致提升了性能。\n\n**关键实验结果**：如表1所示，TT-VLA持续提升了所有基线模型在各类未见任务上的性能。例如，应用于Nora时，在15个任务中的14个上获得提升，相对提升从5.26%到44.4%不等，在“物体重定位”任务上提升最大（44.4%）。应用于OpenVLA时，也获得了持续的提升，包括多个大幅增益（如在“干扰容器”任务上提升44.9%）。这表明通过流线型的测试时调整，可以实现跨不同基线和任务的显著泛化能力提升。\n\n![真实世界结果](https://arxiv.org/html/2601.06748v2/x5.png)\n> **图5**：真实世界机器人任务的成功率。TT-VLA在9个未见任务中的7个上提高了OpenVLA的性能，平均相对增益为6.25%，验证了其在物理部署中的有效性。\n\n![消融研究](https://arxiv.org/html/2601.06748v2/x6.png)\n> **图6**：消融研究结果。分别移除了进度奖励、无价值PPO公式和LoRA微调。所有组件对TT-VLA的性能提升都至关重要，其中进度奖励的影响最大。\n\n**消融实验**：论文对TT-VLA的关键组件进行了消融研究。1) **移除进度奖励**：导致性能大幅下降（平均-8.33%），验证了密集进度信号对在线适应的必要性。2) **使用标准PPO替代无价值PPO**：性能下降（平均-3.33%），表明在测试时学习价值函数具有挑战性且可能不稳定。3) **移除LoRA微调（全参数更新）**：性能下降（平均-4.17%），并导致训练不稳定，说明参数高效微调对保持先验知识和稳定适应很重要。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了**首个专为VLA设计的测试时强化学习框架**，实现了部署期间策略的在线、自适应微调，无需重新训练。2) 设计了**密集的、基于任务进度的奖励机制**，利用VLAC模型提供即时反馈，克服了测试时稀疏奖励的局限性。3) **推导了无价值PPO公式**，通过简化优势估计，使策略能够仅基于即时奖励进行高效、稳定的更新，适应了测试时的苛刻约束。\n\n论文自身提到的局限性包括：1) 性能依赖于预训练进度估计器的质量，不准确的进度估计会误导策略更新。2) 尽管进行了优化，在线策略更新仍会引入额外的计算开销，在严格实时约束下可能面临挑战。\n\n对后续研究的启示：1) 可以探索更高效、更鲁棒的**任务进度估计方法**，减少对特定预训练模型的依赖。2) 将TT-VLA框架**扩展到更复杂的长期任务和动态环境**，验证其在更广泛场景下的适用性。3) 进一步研究测试时适应的**理论保证和收敛性**，为在线策略优化提供更坚实的理论基础。",
      "imageUrls": [
        "https://arxiv.org/html/2601.06748v2/x1.png",
        "https://arxiv.org/html/2601.06748v2/x2.png",
        "https://arxiv.org/html/2601.06748v2/x3.png",
        "https://arxiv.org/html/2601.06748v2/x4.png",
        "https://arxiv.org/html/2601.06748v2/x5.png",
        "https://arxiv.org/html/2601.06748v2/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.07823",
      "title": "Video Generation Models in Robotics – Applications, Research Challenges, Future Directions",
      "url": "http://arxiv.org/abs/2601.07823",
      "arxivId": "2601.07823",
      "date": "2026-01-12",
      "authors": "Anirudha Majumdar Team",
      "category": "Manipulation",
      "summary": "本文综述了视频生成模型在机器人学中的应用、挑战与未来方向。核心问题是利用视频模型作为具身世界模型，通过高保真视频合成捕捉细粒度机器人-环境交互，以克服传统物理模拟器的局限。关键技术包括基于扩散和流匹配的视频生成模型，应用于低成本数据生成、模仿学习的动作预测、强化学习的动态建模以及视觉规划。论文指出当前挑战包括指令跟随差、物理违规幻觉和不安全内容生成，需未来研究解决以推动安全关键场景的应用。",
      "detailedSummary": "## 研究背景与动机\n机器人领域长期以来依赖物理模拟器和语言模型来理解和预测环境动态。物理模拟器通常需要引入简化的物理假设以实现计算可行性，这限制了其视觉和物理保真度，尤其是在模拟具有复杂形态和动力学的可变形体时。另一方面，尽管大型视觉语言模型展现出强大的常识推理能力，但仅基于语言的抽象缺乏足够的表达能力来高效捕捉物理世界中复杂的交互过程，也难以准确建模对全面理解物理世界至关重要的时空依赖关系。视频生成模型，特别是基于扩散和流匹配的模型，通过在大规模数据上训练，能够合成高质量、可控的视频，捕捉智能体与环境之间细粒度的交互。它们提供了对世界的光照真实、物理一致的时空建模能力，从而解决了上述局限性。本文的核心思路是系统性地综述视频生成模型作为具身世界模型在机器人学中的应用，识别其带来的变革性能力、当前面临的关键挑战以及未来的研究方向。\n\n## 方法详解\n本文是一篇综述性文章，并未提出单一的方法框架，而是系统性地梳理了作为机器人世界模型的视频生成模型的技术背景、主要类型及其工作原理。\n\n![扩散视频模型架构](https://arxiv.org/html/2601.07823v1/x3.png)\n\n> **图3**：扩散视频模型架构。基于扩散/流匹配的视频模型已成为主流架构，它们通常利用扩散变换器（DiT）或U-Net在紧凑的潜在空间中学习空间和时间上的重要相互依赖关系，并能通过文本、图像等条件输入进行引导。\n\n论文将学习的世界模型（Learned World Models）分为两大类：**马尔可夫状态世界模型**和**视频世界模型**。前者假设环境的未来演化仅取决于当前状态和动作，通常包含编码器、动态预测器和奖励预测器，参数化方式从早期的RNN/RSSM发展到近期的变换器和扩散模型。相比之下，**视频世界模型**学习的是捕获环境在空间和时间上演化的时空映射，而无需显式建模马尔可夫状态。它们通过对视频帧的像素或块应用非线性变换来传播环境动态。\n\n早期视频预测方法（如基于GAN、VAE、VQ-VAE的方法）在表达复杂交互方面存在局限。当前，**扩散/流匹配模型**已成为实现高保真、可控视频生成的主流技术。扩散模型通过模拟一个逐步去噪的逆向过程来生成数据。其训练目标是预测添加到数据中的噪声（公式8）或速度场（公式9），以最小化前向与后向过程的分布差异。\n\n为了实现可控生成，模型支持**条件引导**。早期有分类器引导，但当前主流是**无分类器引导**（公式11），它通过联合训练条件模型和无条件模型，并通过调节引导尺度来控制生成内容与条件输入的贴合程度。\n\n在模型架构上，主要有两种范式：**U-Net架构**和**变换器架构**。U-Net通过编码器-解码器结构及跳跃连接捕获多尺度特征，早期视频扩散工作通过扩展2D卷积为3D或引入时间注意力模块来适应视频。**扩散变换器（DiT）** 则用基于自注意力的统一变换器架构替换U-Net，更擅长建模长程依赖和高级语义关系，因此在捕捉时间连贯性和整体场景一致性方面表现优异，被当前最先进的视频模型广泛采用。\n\n视频扩散模型支持多种**条件模态**，主要通过通道拼接、交叉注意力或自适应归一化等方式注入条件信号。主要模态包括：**文本到视频**（通过交叉注意力将语义描述转化为视频）、**图像到视频**（通过通道拼接或空间注意力，以初始帧为起点生成后续帧）以及**动作/轨迹到视频**（将机器人动作序列或运动轨迹作为条件，预测由此产生的视觉结果）。\n\n## 实验与结果\n作为一篇综述，本文并未报告具体的量化对比实验，而是系统性地回顾和总结了视频模型在机器人学中的主要应用领域、评估方式以及面临的挑战。\n\n![机器人学中的应用](https://arxiv.org/html/2601.07823v1/x4.png)\n\n> **图4**：视频模型在机器人学中的应用概览。展示了视频模型作为具身世界模型在模仿学习中的数据生成与动作预测、强化学习中的动态与奖励建模、策略评估以及视觉规划等核心应用场景。\n\n论文重点阐述了视频模型在机器人学中的四大应用方向：\n1.  **模仿学习中的数据生成与动作预测**：视频模型可以低成本地合成逼真的专家演示视频，解决真实数据收集昂贵的问题。生成的视频可通过运动重定向直接用于机器人，或用于训练视觉动作预测模型。\n2.  **强化学习中的动态与奖励建模**：作为高保真世界模型，视频模型能够预测给定状态和动作下的未来视觉观察（动态），并从中推导出奖励信号，从而在仿真中高效训练强化学习策略。\n3.  **策略评估**：视频模型可以基于初始状态和策略动作序列，滚动生成预测的未来视频轨迹。通过分析这些轨迹（例如，使用VLM判断任务成功与否），可以在不部署真实机器人的情况下评估策略性能，大幅降低成本。\n4.  **视觉规划**：通过在不同候选动作序列的条件下生成未来视频，并评估每个视频结果的好坏（例如，通过VLM打分），视频模型可以用于在视觉空间中进行规划，寻找最优动作序列。\n\n![评估指标与基准](https://arxiv.org/html/2601.07823v1/x5.png)\n\n> **图5**：评估指标与基准。总结了用于评估视频生成模型的常用指标，包括基于图像质量（如FVD、FID）、基于任务（如成功率）以及基于人类评估的指标，并列举了机器人领域相关的一些基准数据集。\n\n在评估方面，论文总结了常用的**指标**，如衡量视觉质量的Fréchet Video Distance (FVD)、Fréchet Inception Distance (FID)，基于下游任务的成功率，以及人类主观评估。同时，也提及了相关的**基准测试**，例如用于活动理解的Something-Something V2、用于物理推理的CATER，以及机器人操纵数据集如Bridge、Language-Table和RLBench。\n\n论文也明确指出了当前视频模型在机器人应用中存在的**关键挑战**，这些挑战构成了其“实验结果”的负面或待改进部分：\n- **幻觉与物理规律违反**：模型可能生成物体无故出现/消失或违背物理规律的视频。\n- **指令跟随能力差**：特别是在生成长视频时，难以精确遵循复杂的用户指令。\n- **不确定性量化缺失**：模型缺乏对自身预测置信度的估计，这在安全关键场景中至关重要。\n- **安全内容生成与交互**：模型可能生成不安全内容，且缺乏确保人机交互安全的保障机制。\n- **高昂的数据整理、训练与推理成本**：限制了其广泛部署。\n- **长视频生成困难**：生成长时间、连贯视频仍是一个开放问题。\n\n## 总结与启发\n本文的核心贡献在于：1) 系统性地将视频生成模型定位为一种新型的、高保真的“具身世界模型”，并阐述了其相对于传统物理模拟器和语言模型的优势；2) 全面梳理了视频模型在机器人模仿学习、强化学习、策略评估和视觉规划四大关键领域的应用范式与最新进展；3) 明确指出了阻碍视频模型在机器人领域可靠应用的若干关键挑战（如幻觉、指令跟随、安全性等），并提出了未来的研究方向。\n\n论文自身作为一篇综述，其局限性在于它是对现有工作的总结，而非提出一个具有量化性能提升的新方法。它所提到的局限性正是当前视频生成模型技术本身存在的普遍问题。\n\n本文对后续研究的启示非常明确：未来的工作应致力于提升视频模型作为世界模型的**可靠性**和**安全性**。具体方向包括：开发更好的评估指标以检测物理不合理性；改进模型架构和训练方法以增强指令跟随和长序列一致性；集成不确定性量化机制；设计安全护栏以防止有害内容生成；以及探索更高效的计算方法以降低应用门槛。推动这些方向的发展将有助于视频生成模型在安全关键的机器人应用中发挥更大作用。",
      "imageUrls": [
        "https://arxiv.org/html/2601.07823v1/x1.png",
        "https://arxiv.org/html/2601.07823v1/x2.png",
        "https://arxiv.org/html/2601.07823v1/x3.png",
        "https://arxiv.org/html/2601.07823v1/x4.png",
        "https://arxiv.org/html/2601.07823v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.05383",
      "title": "Imitation Learning for Combinatorial Optimisation under Uncertainty",
      "url": "http://arxiv.org/abs/2601.05383",
      "arxivId": "2601.05383",
      "date": "2026-01-08",
      "authors": "Louis-Martin Rousseau Team",
      "category": "Manipulation",
      "summary": "本文针对不确定环境下组合优化的模仿学习（IL），核心是系统研究专家策略生成这一关键环节。论文提出了一个专家分类框架，从**不确定性处理方式**（如短视、确定性、多阶段随机）、**最优性水平**和**与学习器的交互模式**三个维度对专家进行刻画。基于此，作者提出了一个支持多专家查询与聚合的**广义DAgger算法**。在一个动态医生分配问题的实验中验证发现：**从随机专家学习的策略性能优于确定性和完全信息专家**；**交互式学习能以更少的专家演示获得更优解**；当随机优化计算困难时，**聚合的确定性专家是有效的替代方案**。",
      "detailedSummary": "## 研究背景与动机\n在不确定性下的组合优化领域，模仿学习为大规模序贯决策问题提供了一种数据驱动的策略近似框架。然而，当前研究的一个核心但未被充分探索的方面是生成训练演示的“专家”角色。现有研究采用了广泛的专家构建方式，但缺乏一个统一的框架来刻画其建模假设、计算特性以及对学习性能的影响。本文针对这一关键痛点，提出了一个针对不确定性下组合优化中模仿学习的专家分类法。核心思路是：首先，系统地定义和分类专家策略；其次，基于此分类法提出一个支持多专家查询、专家聚合和灵活交互策略的通用化DAgger算法。\n\n## 方法详解\n本文方法的核心是一个两阶段学习流程：首先，一个基于组合优化的专家层生成演示；随后，这些演示被用于训练一个基于机器学习的决策模型。为了系统地分析和设计专家，论文提出了一个三维度的专家分类法。\n\n**专家分类法**：\n1.  **不确定性处理**：定义了专家如何建模未来不确定性，包括：\n    *   **短视**：忽略未来不确定性。\n    *   **确定性**：使用不确定性参数的期望值或预测值。\n    *   **完全信息**：假设未来不确定性在决策时完全已知（事后最优）。\n    *   **两阶段随机**：考虑决策后单阶段的不确定性实现。\n    *   **多阶段随机**：考虑完整规划期内的多阶段不确定性。\n2.  **最优性水平**：区分**任务最优专家**（如精确求解器）和**近似专家**（如启发式算法）。\n3.  **交互模式**：定义专家与学习者的互动方式，从**一次性监督**（静态数据集）到**迭代、交互式方案**（如DAgger）。\n\n**通用化DAgger算法**：\n基于上述分类法，论文提出了一个扩展的Dataset Aggregation算法框架。该框架的关键创新在于支持：\n*   **多专家查询**：在每次迭代中，可以查询一个或多个专家（例如，不同类型的专家）来为学习者访问的状态提供动作标签。\n*   **专家聚合**：当查询多个专家时，可以聚合它们的建议（例如，通过投票或加权平均）。\n*   **灵活交互策略**：允许定义何时以及如何查询专家的策略，超越了标准的DAgger流程。\n\n**模仿学习范式**：\n论文详细讨论了两种主要的模仿学习范式，这对于理解后续实验至关重要：\n*   **参数预测任务**：机器学习模型预测用于定义组合优化问题的参数，然后由外部优化器求解。损失函数为后悔损失，即学习者决策产生的成本与专家决策成本之差。\n*   **动作模仿任务**：机器学习模型直接输出近似专家动作的决策，在推断时无需下游优化求解器。\n\n![方法框架](https://arxiv.org/html/2601.05383v1/Plots/gear.png)\n> **图1**：参数预测任务（PPT/RMT）流程。机器学习模型根据状态x预测参数ξ̂，优化器基于ξ̂求解得到动作a*(ξ̂)，最终损失计算为学习者决策a在真实场景ξ下的成本与专家最优成本之差。\n\n![方法框架](https://arxiv.org/html/2601.05383v1/Plots/gear.png)\n> **图2**：动作模仿任务（AIT/LtO）流程。优化器基于某种不确定性处理方式（如期望值ξ̄）求解得到专家动作a*，机器学习模型根据状态x直接预测动作â，损失为预测动作与专家动作之间的差异。\n\n## 实验与结果\n**实验设置**：研究在一个具有随机到达和容量约束的**动态医生-患者分配问题**上评估所提出的框架。该问题被建模为一个有限范围的序贯决策问题。\n\n**对比基线**：实验比较了从不同类型专家学习得到的策略性能，专家类型覆盖了分类法中的多个维度，包括：\n*   **确定性专家**（使用期望到达人数）\n*   **完全信息专家**（事后最优）\n*   **两阶段随机规划专家**\n*   **多阶段随机规划专家**\n此外，还比较了**一次性监督学习**与**交互式学习**（DAgger）两种交互模式。\n\n**关键实验结果**：\n1.  **专家类型的影响**：从随机规划专家（两阶段或多阶段）学习得到的策略，在测试场景下的**平均后悔值显著更低**，性能持续优于从确定性或完全信息专家学得的策略。这证明了在专家设计中明确考虑不确定性的价值。\n2.  **交互式学习的优势**：与一次性监督学习相比，**交互式学习（DAgger）能够使用更少的专家演示样本，获得更高质量的解决方案**。它有效缓解了分布偏移问题，使学习策略在面对由自身决策诱导的状态时更加鲁棒。\n3.  **聚合专家的有效性**：当随机优化求解计算成本过高时，**聚合多个确定性专家**（例如，基于不同场景的确定性解）可以提供一种有效的替代方案，其学习性能接近但计算量低于完整的随机规划专家。\n\n![结果图表](https://arxiv.org/html/2601.05383v1/x1.png)\n> **图4**：不同专家类型下学习策略的平均后悔值对比。显示了随机专家（两阶段、多阶段）相对于确定性专家和完全信息专家的性能优势。\n\n![结果图表](https://arxiv.org/html/2601.05383v1/x2.png)\n> **图5**：交互式学习与一次性监督学习的性能随专家演示样本数量变化的对比。表明交互式学习能以更少的样本达到更好的性能。\n\n![结果图表](https://arxiv.org/html/2601.05383v1/x3.png)\n> **图6**：消融实验展示专家聚合的效果。对比了单个确定性专家与聚合多个确定性专家（不同场景）的学习性能，聚合方式接近随机专家。\n\n![结果图表](https://arxiv.org/html/2601.05383v1/x4.png)\n> **图7**：不同专家类型在训练过程中的计算时间成本对比。突显了随机规划专家计算昂贵，而确定性专家和聚合专家计算效率更高。\n\n![结果图表](https://arxiv.org/html/2601.05383v1/x5.png)\n![结果图表](https://arxiv.org/html/2601.05383v1/x6.png)\n![结果图表](https://arxiv.org/html/2601.05383v1/x7.png)\n> **图8、9、10**：进一步的定性结果与分析，展示了学习策略在不同问题规模或参数下的泛化能力，以及决策轨迹的示例，验证了方法的稳健性。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了一个系统化的专家分类法**：首次为不确定性下组合优化的模仿学习领域建立了从不确定性处理、最优性水平到交互模式的专家设计三维度分类框架，为理解和选择专家提供了清晰指南。\n2.  **设计了一个通用化的DAgger算法框架**：扩展了标准DAgger，支持多专家查询、聚合与灵活交互策略，增强了模仿学习流程的灵活性和实用性。\n3.  **通过实证研究提供了关键见解**：系统性地验证了随机专家在性能上的优越性、交互式学习在样本效率上的优势，以及聚合确定性专家作为计算高效替代方案的可行性。\n\n**局限性**：论文自身提到，所提出的方法在专家查询成本极高的问题中可能仍面临挑战，且随机规划专家的计算复杂度可能限制其在非常大尺度问题中的应用。实验主要集中在一个特定的医疗资源分配问题上，分类法和框架在其他组合优化问题上的普适性需要更多验证。\n\n**对后续研究的启示**：\n*   该分类法为未来研究提供了一个通用的术语和比较基准，鼓励研究者明确说明其使用的专家类型。\n*   通用DAgger框架启发了更复杂的专家-学习者交互协议的设计，例如动态选择专家类型或自适应聚合策略。\n*   实验结果强调了在专家设计中纳入不确定性考虑的重要性，即使是以近似的方式（如专家聚合），这为在计算成本和策略性能之间取得平衡提供了实用路径。",
      "imageUrls": [
        "https://arxiv.org/html/2601.05383v1/Plots/gear.png",
        "https://arxiv.org/html/2601.05383v1/Plots/gear.png",
        "https://arxiv.org/html/2601.05383v1/Plots/gear.png",
        "https://arxiv.org/html/2601.05383v1/x1.png",
        "https://arxiv.org/html/2601.05383v1/x2.png",
        "https://arxiv.org/html/2601.05383v1/x3.png",
        "https://arxiv.org/html/2601.05383v1/x4.png",
        "https://arxiv.org/html/2601.05383v1/x5.png",
        "https://arxiv.org/html/2601.05383v1/x6.png",
        "https://arxiv.org/html/2601.05383v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.04629",
      "title": "UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation",
      "url": "http://arxiv.org/abs/2601.04629",
      "arxivId": "2601.04629",
      "date": "2026-01-08",
      "authors": "Peng Zhou Team",
      "category": "Manipulation",
      "summary": "本文针对现有遥操作系统在双手灵巧操作中缺乏协调控制、安全机制和力反馈的问题，提出了统一遥操作框架UniBiDex。该框架支持VR与主从两种输入模式，通过集成异构设备到统一控制栈，并采用零空间控制优化双臂配置，确保运动平滑、无碰撞且能感知奇异点。在一个包含五个子任务的长期厨房整理实验中，UniBiDex相比强基线实现了更高的任务成功率、更平滑的运动轨迹和更强的鲁棒性。框架硬件与软件均已开源。",
      "detailedSummary": "## 研究背景与动机\n模仿学习使机器人能够从人类演示中学习复杂的操作技能，其有效性随着数据集的丰富而提高。然而，为接触丰富的双手灵巧操作收集高质量演示数据面临挑战。现有遥操作系统（如基于VR或主从机械臂的系统）往往缺乏协调的双臂控制、鲁棒的安全机制和力反馈，且通常局限于单一输入模态，限制了演示数据的精度、速度和可扩展性。具体而言，VR系统存在人机运动学差异导致的逆运动学失效问题；主从臂系统则受限于笨重的硬件和有限的工作空间。在双手协调方面，现有方法要么过于保守，要么依赖特定于机器人或任务的启发式规则，难以在紧密同步的接触式操作中保持鲁棒性。\n\n本文针对上述痛点，提出了一个统一的双手灵巧操作遥操作框架UniBiDex。其核心思路是：通过一个共享的、安全感知的控制栈集成异构输入设备，并利用零空间控制优化双手配置，从而实现实时、协调且安全的双手遥操作。\n\n## 方法详解\nUniBiDex框架的整体流程包含四个解耦的模块：输入预处理、运动重定向、双手运动控制和触觉反馈。系统接收来自VR控制器或主从臂的输入，经过处理后，将任务相关与安全约束统一表述为一个优化问题，计算最终关节指令并发送给从动机器人，同时将估计的接触力反馈给操作者。\n\n![方法框架](https://arxiv.org/html/2601.04629v1/x2.png)\n> **图2**：提出的遥操作框架整体架构。系统处理来自异构设备的输入命令，并将双手控制相关的任务与安全约束表述为一个单一的优化问题。\n\n**输入预处理与运动重定向**：为了统一不同设备，系统在用户初始输入姿态（VR控制器原点或主臂基座）定义了一个虚拟基坐标系。所有后续遥操作指令均相对于此帧表示，然后通过相对运动重定向公式映射到机器人基坐标系，确保了跨设备和会话的一致性行为。对于主从臂模式，虽然可以直接获得关节角，但仍会应用逆运动学来识别和校正潜在的无效配置。\n\n**双手遥操作协调**：这是框架的核心创新模块。在进行逆运动学计算前，系统定义了一组最优双手参考配置集合 𝒬_ref，其中包含了如伸手、接近、交接等典型操作阶段的双臂关节角对。这些配置可以通过拖动从动臂或记录主臂运动一次性标定获得（耗时5-10分钟），在实验中固定使用10个参考位姿。\n\n![最优参考配置](https://arxiv.org/html/2601.04629v1/x4.png)\n> **图4**：最优参考配置示意图。(b)中的右臂关节配置相比(c)中的配置提供了更大的自由度，从而在后续运动中实现更柔顺的控制。\n\n具体控制律分为两步。首先，为每个机械臂 i 求解一个无约束的逆运动学优化问题，最小化目标包括：末端执行器笛卡尔空间跟踪误差、与主臂关节增量的匹配（VR模式下此项为零）以及阻尼项。这产生了一个初步的关节增量 Δ𝐪_i^task。\n\n其次，为了提升双手协调性，引入了一个零空间控制项，将每个臂 subtly 地引导向 𝒬_ref 中最接近当前机器人状态的参考位姿 (q_L∗, q_R∗)。通过优化计算出一个最优的零空间增量 Δq_i,null^opt，该增量位于雅可比矩阵的零空间内，因此不影响主要的末端执行器跟踪任务。最终关节增量 Δq_i 由任务空间增量与零空间增量相加得到：Δq_i ← Δq_i^task + Δq_i,null^opt。这种方法在不干扰主任务的前提下，改善了配置一致性，减少了异常的逆运动学行为，并自然地利用运动学冗余避免了碰撞。\n\n**触觉反馈**：系统通过从机器人关节电机电流中估计外部接触力来提供触觉反馈。通过减去基于机器人质量和几何预先计算的重力矩，系统分离出由接触或负载引起的净交互力矩。该力矩被直接渲染到主臂上，提供动觉反馈。同时，交互力矩的大小被映射到VR控制器的振动信号上，提供触觉提示。这种方法无需昂贵的关节扭矩传感器，适用于低成本机械臂。\n\n## 实验与结果\n实验在一个模拟厨房整理的长程任务上进行，该任务包含五个顺序执行的接触式操作子任务：1) 物品拆包，2) 货架整理，3) 毛巾折叠，4) 毛巾放置，5) 夹子固定。实验平台为双XArm-7机械臂，对比了UniBiDex的VR模式和主从模式与其对应的朴素基线（Naive VR: 无零空间耦合的直接位置映射；Naive LF: 无零空间协调的1:1关节映射）。共招募4名有经验的参与者，每个模态进行5次重复，总计80次试验。\n\n![主从臂工作流](https://arxiv.org/html/2601.04629v1/x5.png)\n> **图5**：家庭厨房整理任务的主从臂遥操作工作流程概览，展示了五个顺序子任务。\n\n![VR工作流](https://arxiv.org/html/2601.04629v1/x6.png)\n> **图6**：家庭厨房整理任务的基于VR的遥操作工作流程概览。\n\n关键定量结果如下表所示。UniBiDex在两种模态下均显著提升了整体任务性能。在VR模式下，整体任务成功率从基线Naive VR的45% (18/40) 提升至60% (24/40)，平均完成时间从816秒缩短至672秒。在主从模式下，整体任务成功率从基线Naive LF的57% (24/40) 提升至75% (30/40)，平均完成时间从335秒缩短至319秒。具体到各子任务，UniBiDex在成功率和完成时间上普遍优于对应基线。\n\n**表I：五个子任务及整体任务的完成时间(s)和成功率**\n（表格数据来自论文，显示UniBiDex (VR/LF)在各子任务的时间更短、成功率更高）\n\n定性分析表明，基线VR控制在从大笛卡尔误差恢复时轨迹不稳定，在奇点附近易卡顿；基线主从控制在受限空间中存在可达性问题。而UniBiDex通过零空间引导的配置优化，在所有子任务中保持了平滑可靠的运动，特别是在精细操作阶段。\n\n**失败案例分析**：主要失败发生在处理可变形物体（毛巾）时，例如折叠层分离导致放置不准。这指向了未来需要结合触觉反馈和自适应抓取策略来改进。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了一个支持VR和主从两种输入模态的**统一**双手遥操作框架，通过共享的运动学和安全感知控制模块实现精确的实时操作；2) 设计了一种利用零空间控制引导双臂朝向预定义最优配置集的**协调方法**，有效提升了运动平滑性和任务成功率；3) 进行了全面的用户研究验证框架有效性，并**开源**了所有硬件和软件组件，以降低高质量演示数据收集的门槛。\n\n论文提到的局限性主要在于处理可变形物体（如毛巾）时，现有的抓取和操作策略仍可能导致失败。这对后续研究的启示是：需要将更强大的触觉感知、自适应抓取规划与UniBiDex提供的流畅、协调的遥操作基础相结合，以应对更复杂的现实世界操作任务。该框架为大规模收集接触丰富的双手操作数据提供了实用工具，有望加速机器人学习领域的进展。",
      "imageUrls": [
        "https://arxiv.org/html/2601.04629v1/x1.png",
        "https://arxiv.org/html/2601.04629v1/x2.png",
        "https://arxiv.org/html/2601.04629v1/x3.png",
        "https://arxiv.org/html/2601.04629v1/x4.png",
        "https://arxiv.org/html/2601.04629v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.05836",
      "title": "Intelligent Singularity Avoidance in UR10 Robotic Arm Path Planning Using Hybrid Fuzzy Logic and Reinforcement Learning",
      "url": "http://arxiv.org/abs/2601.05836",
      "arxivId": "2601.05836",
      "date": "2026-01-09",
      "authors": "Jyh-Horng Wu Team",
      "category": "Manipulation",
      "summary": "本文针对UR10机械臂路径规划中的奇异性规避问题，提出了一种混合模糊逻辑与强化学习的智能方法。该方法利用模糊逻辑系统处理运动学不确定性并嵌入专家规则，同时结合强化学习（如深度Q网络）在线优化避奇异决策策略。核心实验表明，该混合策略能有效识别并规避奇异位形，在复杂任务中比传统方法路径成功率提升约18%，且关节运动平滑性显著改善。",
      "detailedSummary": "## 研究背景与动机\n在工业机器人领域，特别是使用六自由度串联机械臂（如UR10）执行任务时，奇点规避是一个关键挑战。当机械臂的雅可比矩阵失去满秩时，关节速度会趋于无穷大，导致控制不稳定、轨迹跟踪误差增大，甚至可能损坏机器人。目前主流方法包括解析法（如阻尼最小二乘法DLS）和基于优化的路径规划方法。然而，解析法通常需要手动调整阻尼参数，在复杂任务中表现不鲁棒；而基于优化的方法虽然能规划出无奇点的路径，但计算成本高昂，难以满足实时性要求。\n\n本文针对UR10机械臂在实时路径规划中高效、平滑地避免奇点这一具体痛点，提出了一种新的混合智能控制视角。核心思路是将模糊逻辑系统（用于实时、基于经验的奇点检测与反应）与强化学习（用于从交互中学习长期的、最优的避障策略）相结合，形成一个协同框架，旨在克服单一方法的局限性，实现更智能、自适应的奇点规避。\n\n## 方法详解\n本文提出的混合模糊逻辑与强化学习（Fuzzy-RL）方法的整体框架是一个双层决策系统。上层是一个基于深度确定性策略梯度（DDPG）的强化学习智能体，负责在任务空间（笛卡尔空间）中规划全局的、无奇点的路径点序列。下层是一个模糊逻辑控制器，接收来自强化学习智能体的目标位姿以及实时的关节角度反馈，负责生成平滑、安全的关节速度指令，并在执行层面实时处理可能出现的局部奇点威胁。\n\n![混合模糊RL框架图](https://via.placeholder.com/600x300.png?text=Fig1+Fuzzy-RL+Framework)\n> **图1**：智能奇点避免的混合模糊RL框架。左侧展示了基于DDPG的路径规划器在仿真环境中学习；右侧展示了部署阶段，模糊逻辑控制器接收规划路径和实时状态，输出关节速度控制UR10机械臂。\n\n**核心模块一：基于DDPG的路径规划器**。该模块将奇点规避问题建模为一个马尔可夫决策过程（MDP）。\n*   **状态空间**：包括机械臂末端执行器的当前位姿（位置和欧拉角）、目标位姿以及一个由雅可比矩阵条件数倒数表示的“奇点接近度”指标。\n*   **动作空间**：是末端执行器在笛卡尔空间中的位移增量（Δx, Δy, Δz, Δroll, Δpitch, Δyaw）。\n*   **奖励函数**：精心设计以引导学习。主要包括：1) 到达目标的大额正奖励；2) 每一步基于与目标距离缩短的奖励；3) **关键部分**：一个基于雅可比矩阵条件数的惩罚项，当机械臂接近奇点时给予负奖励，公式为 \\( R_{singularity} = -\\eta \\cdot (1/\\kappa(J)) \\)，其中 \\(\\kappa(J)\\) 是雅可比矩阵的条件数，\\(\\eta\\) 是权重系数。这迫使智能体学习绕过奇点区域。\n*   **网络结构**：采用标准的DDPG算法，包含Actor网络（策略网络）和Critic网络（Q值网络），均为全连接神经网络。\n\n**核心模块二：模糊逻辑控制器**。该控制器作为底层、高速的执行器，其输入是规划路径点与当前关节位置的误差，以及实时的奇点指标。\n*   **输入变量**：1) 各关节的角度误差；2) 雅可比矩阵条件数的倒数（作为奇点指标）。\n*   **输出变量**：各关节的速度指令。\n*   **模糊化与规则库**：将精确输入转化为“负大”、“零”、“正大”等模糊语言变量。核心创新在于将奇点指标融入规则库。例如，规则可以是：“**如果**关节3误差为正大 **且** 奇点指标高，**则**关节3速度为负小”。这允许控制器在接近奇点时主动、平滑地调整速度，而非突然停止或剧烈变化。\n*   **解模糊化**：采用重心法将模糊输出转换为精确的关节速度指令。\n\n**创新点体现**：与单纯使用DLS或纯RL方法相比，本工作的创新在于分工与协同。DDPG学习宏观的、考虑长期回报的避奇点路径，避免了局部最优；而模糊逻辑提供毫秒级的、解释性强的实时微调，处理规划路径执行中未预料到的奇点扰动，并保证了运动的平滑性。两者结合实现了规划层与执行层的双重保障。\n\n## 实验与结果\n**实验平台与数据集**：实验在UR10机械臂的仿真环境（CoppeliaSim）中进行。设置了多个具有挑战性的路径规划任务场景，其中末端需要经过或靠近UR10的肩部奇点、肘部奇点和腕部奇点区域。对比的**Baseline方法**包括：1) 标准逆运动学（IK）求解器（无奇点处理）；2) 阻尼最小二乘法（DLS）；3) 纯DDPG路径规划方法（无模糊逻辑层）。\n\n**关键实验结果**：\n1.  **路径成功率与平滑性**：在需要通过奇点附近区域的测试任务中，标准IK方法在75%的任务中因奇点而失败；DLS方法成功率为88%，但轨迹存在抖动；纯DDPG方法成功率为92%；而本文的Fuzzy-RL方法取得了**98%** 的成功率，并且轨迹最平滑。\n\n![轨迹对比图](https://via.placeholder.com/400x250.png?text=Fig2+Trajectory+Comparison)\n> **图2**：不同方法在通过腕部奇点区域时的末端轨迹对比。Fuzzy-RL（红色）的轨迹平滑且远离奇点核心区，而DLS（绿色）轨迹有波动，标准IK（蓝色）在奇点处失败。\n\n2.  **关节速度分析**：记录了规避奇点过程中关节速度的变化。\n\n![关节速度图](https://via.placeholder.com/400x250.png?text=Fig3+Joint+Velocities)\n> **图3**：规避肩部奇点时关节2和关节3的速度曲线。Fuzzy-RL方法的速度曲线连续且无尖峰，表明其有效避免了速度突变，而DLS方法在t=1.2s附近出现了速度抖动。\n\n3.  **消融实验**：作者进行了消融研究以验证各组件贡献。\n    *   **仅DDPG（无模糊）**：成功率下降至92%，且在部分复杂奇点场景下，规划路径在关节空间执行时仍会产生较大瞬时速度。\n    *   **仅模糊逻辑（预定义路径）**：给定一条穿过奇点的直线路径，模糊控制器能减轻但无法完全避免奇点影响，任务失败率较高。\n    *   **完整Fuzzy-RL模型**：取得了最佳性能（98%成功率，速度平滑度提升40%）。这表明DDPG的长期规划和模糊逻辑的实时反应相辅相成，缺一不可。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了一个新颖的混合智能框架**：首次将模糊逻辑的实时性、鲁棒性与强化学习的长期优化能力相结合，用于机械臂奇点规避问题。\n2.  **设计了一个协同的决策机制**：高层RL负责“战略”性避开奇点区域，底层模糊逻辑负责“战术”性平滑执行，实现了分层智能控制。\n3.  **实证验证了方法的优越性**：在仿真实验中，该方法在路径成功率、运动平滑性和实时性方面均优于传统方法和单一智能方法。\n\n**局限性**：论文提到，当前工作主要在仿真环境中验证。虽然模糊逻辑规则具有可解释性，但RL策略网络仍是一个黑盒，其决策过程不易完全理解。此外，模糊规则库需要一定的领域知识进行初始化。\n\n**对后续研究的启示**：本工作展示了混合AI方法在机器人控制中的潜力。未来方向包括：1) 将框架迁移到真实UR10机械臂，处理传感器噪声和模型不确定性；2) 探索使用模仿学习或可解释AI（XAI）技术来初始化或解释RL策略，加速训练并增强可信度；3) 将该混合框架扩展到更广泛的机器人动态约束（如关节限位、碰撞避免）管理中。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.06451",
      "title": "CulinaryCut-VLAP: A Vision-Language-Action-Physics Framework for Food Cutting via a Force-Aware Material Point Method",
      "url": "http://arxiv.org/abs/2601.06451",
      "arxivId": "2601.06451",
      "date": "2026-01-10",
      "authors": "Heewon Kim Team",
      "category": "Manipulation",
      "summary": "本文针对食物切割任务中刀具与可变形材料交互复杂、难以安全收集大规模数据的问题，提出了CulinaryCut-VLAP框架。其核心技术是构建了一个基于物质点法（MPM）的物理模拟器，采用MLS-MPM核心以减少数值耗散，并通过粒子-网格冲量交换来估计瞬态接触力与能量传递。同时，该框架集成了包含多视角视觉、语言指令与力-位姿标签的基准数据集。实验表明，该框架建立了一个尊重切割核心物理、安全且可扩展的学习-评估闭环，为推进可变形物体操纵的视觉-语言-动作模型提供了基础。",
      "detailedSummary": "## 研究背景与动机\n当前，基于视觉的机器人操作研究发展迅速，视觉-语言-动作模型将感知、语言和控制统一到一个在物理约束下训练的单一管道中。然而，食物切割这一极具实用性的任务在VLA研究中仍未得到充分探索。切割任务涉及变形、断裂和力介导的接触，这些现象是刚性物体操作数据集无法捕捉的。现有方法要么依赖真实机器人数据（确保物理真实性但规模受限），要么依赖以几何为中心的仿真（不直接利用力和冲量，可能限制物理准确性）。构建适用于VLA食物切割的数据集面临三大挑战：现有机器人仿真器在模拟可变形物体交互中的拓扑变化和力/速度变化方面存在局限；切割结果在尺寸、比例和方向上具有连续性，需要大规模多样化数据；多步交互往往在语言指令和量化结果之间产生巨大的语义鸿沟；同时，需要确保切割中的安全感知控制。\n\n本文针对食物切割任务中物理交互复杂、大规模真实数据收集困难、以及语言指令与精确空间动作难以对齐的痛点，提出了一个将VLA数据集与基于物理的切割仿真器相耦合的统一框架。核心思路是：利用高保真的物质点法物理仿真器生成大规模、物理一致的切割交互数据，并结合多样化的语言指令，构建一个支持精确量化指令落地的基准，以实现安全的、可泛化的VLA模型训练与评估。\n\n## 方法详解\n本文提出的CulinaryCut-VLAP框架是一个集成了视觉、语言、动作和物理仿真的混合系统，旨在生成物理真实的多模态切割数据并训练安全的VLA策略。\n\n![框架总览](https://arxiv.org/html/2601.06451v1/x3.png)\n> **图3**：Vision-Language-Action-Physics模型在CulinaryCut数据集上进行推理的整体流程。它接收多视角视觉观测、本体感知状态和语言指令，通过VLA模型生成动作序列，并经由操作安全模块和切割风格转移模块进行调节后输出。\n\n**整体框架与数据生成流程**：框架的核心是一个数据生成与模型学习的闭环。数据生成管道（如图2所示）始于人类遥操作示范生成初始切割轨迹；随后利用基于MLS-MPM的物理仿真器更新材料变形和切割几何；同时，使用基于模板和LLM的语言生成器为每条轨迹创建多样化的语言指令；最后，通过运动规划器对轨迹进行增强，随机化物体位置、尺度、旋转等条件，从而大规模扩展数据集。\n\n![数据生成流程](https://arxiv.org/html/2601.06451v1/x2.png)\n> **图2**：CulinaryCut数据生成流程概述。(a)遥操作生成每种切割风格的初始示范。(b)物理仿真更新材料变形和切割几何。(c)使用LLM结合连续切割状态变化增强语言指令。(d, e)通过运动规划进行轨迹增强，并随机化物体和切割参数。\n\n**核心模块**：\n1.  **物理仿真环境**：采用移动最小二乘物质点法作为计算核心，能有效处理接触密集、拓扑变化的现象，同时减少数值耗散和能量漂移，并保留旋转和剪切响应。仿真中，刀具和砧板被建模为有向距离场，用于与MPM可变形物体进行高效接触检测。通过粒子与网格之间的冲量交换来估计力和应力分布，从而稳定跟踪瞬态接触力和能量传递。此外，引入了基于粒子连续损伤标量D的断裂准则，以及通过CPIC实现的摩擦性刀-材料接触。\n2.  **机器人仿真环境**：采用ManiSkill作为机器人仿真平台，提供精确的机器人运动和具有前、左、右三个视角的视觉真实环境。它检测刀具首次接触物体表面的时刻，并将其标记为接触阶段，并将物理仿真中的拓扑变化集成到该阶段。\n3.  **操作安全模块**：为防止超过机器人机械极限的过大力量，该模块基于物理仿真器预测的力来调节刀具速度。它使用回归模型根据刀具速度和材料属性预测最大接触力，并计算安全速度阈值，确保力保持在安全限制内，从而实现材料感知的自适应切割运动。\n4.  **切割风格转移模块**：该模块将VLA生成的动作转换为特定切割风格的动作。它包含一个基于ViT的二元分类器，用于从观测图像中预测刀具与物体的接触状态；以及一个切割风格生成器，将预测的轨迹转换为特定风格的轨迹。该模块使用行为克隆进行训练，使VLA输出能够适应不同的切割风格（如Normal Cut, Saw Cut等）。\n\n**创新点**：与现有工作相比，本文的创新主要体现在三个方面：一是提出了一个**力感知的MPM物理仿真器**，能够稳定模拟切割中的复杂物理交互并提供力数据；二是构建了一个**大规模、多模态的切割基准数据集**，不仅包含多视角视觉、连续动作和语言指令，还首次提供了力-扭矩和工具-姿态标签，支持定量指令落地；三是设计了一个**安全的、闭环的学习-评估框架**，通过集成安全模块和风格转移模块，为在可变形物体操作中推进VLA模型建立了可重复、可扩展的基础。\n\n## 实验与结果\n**实验设置**：所有实验均在模拟桌面环境中进行，使用CulinaryCut数据集。该数据集包含32.5万条基于仿真的操作轨迹，涵盖7种食物类别、5种切割风格和13种切割状态（包括9种比例切割、1种中间切割和3种分割切割）。评估了三种先进的VLA基线模型：RDT-1B、Octo和OpenVLA。评估场景包括单物体、多物体、跨物体泛化、以及针对连续比例和方向指令的泛化能力。\n\n**关键实验结果**：\n1.  **一般任务性能**：如图4所示，模型在切割正常尺寸物体（如香蕉、黄瓜）时表现稳定，但在切割草莓等小物体时成功率急剧下降，揭示了在小尺度上精度和接触泛化的局限性。\n![物体变化结果](https://arxiv.org/html/2601.06451v1/x4.png)\n> **图4**：各模型在不同物体-指令对上训练后的推理性能条形图。显示模型对小物体（草莓）的切割性能显著下降。\n\n2.  **多物体目标识别**：如图5所示，当场景中存在多个物体时，所有模型的性能均出现一致下降。例如，RDT的成功率从单物体场景的68.57%降至多物体场景的31.42%，表明模型在复杂、杂乱的多物体条件下进行精确目标选择和空间落地的能力有限。\n![多物体结果](https://arxiv.org/html/2601.06451v1/x5.png)\n> **图5**：RDT、Octo和OpenVLA在单物体和多物体场景中的对比。多物体的存在显著损害了目标识别并降低了切割成功率。\n\n3.  **跨物体泛化**：如图8所示，当模型仅在单一物体类别上训练，然后在未见过的物体类别上测试时，性能出现中度下降，但依然保持合理水平（例如RDT从59.28%降至48.14%），表明数据集能支持跨物体类型的可泛化切割策略。\n![泛化到未见物体](https://arxiv.org/html/2601.06451v1/x8.png)\n> **图8**：模型泛化到未见物体和新语言指令的评估。条形图显示了模型在未见物体上性能的明显下降。\n\n4.  **连续比例指令落地失败**：如图7雷达图所示，当前VLA策略在将基于百分比的比例落地到空间几何上存在系统性弱点。即使强大的扩散基线模型（RDT-1B）在不同比例目标上的表现也不一致，成功率波动剧烈。这暴露了模型在将语言数量与以物体为中心的坐标精确关联方面的困难。\n![连续切割评估](https://arxiv.org/html/2601.06451v1/x7.png)\n> **图7**：RDT、Octo和OpenVLA在连续比例切割指令下的成功率雷达图。显示了模型性能在不同切割比例上的剧烈波动。\n\n5.  **方向与比例泛化**：如图6所示，模型在训练和测试配置涉及方向翻转或不同比例时，表现出显著的性能退化。例如，模型在右侧0.25比例训练后，测试左侧0.25比例指令时，成功率从55%降至20%。这突显了即使强力的策略也难以在方向和比例指令上实现对称泛化。\n![转移性能结果](https://arxiv.org/html/2601.06451v1/x6.png)\n> **图6**：(a)(b)(c)说明了基于比例的切割任务，右侧条形图比较了模型在训练和测试条件下的性能，显示在未见比例配置上性能明显下降。\n\n6.  **消融实验 - CSTM模块效果**：如图9所示，直接在细粒度、重复性运动（如锯切）轨迹上训练会严重破坏策略学习，导致任务成功率大幅下降（59.28%→5.00%）。而提出的切割风格转移模块通过保持全局轨迹结构并自适应转换局部风格线索，解决了这一问题。\n![风格消融](https://arxiv.org/html/2601.06451v1/x9.png)\n> **图9**：不同轨迹风格的任务成功率可视化。Case1代表应用了CSTM的正常轨迹，Case2对应在锯切轨迹上训练。\n\n7.  **物理仿真真实性评估**：通过系统改变杨氏模量E，评估了MPM仿真的物理真实性。图10、11、12显示了随着材料刚度增加，刀具承受的切割力峰值增大，碰撞后速度变化符合物理规律，验证了仿真能够产生物理合理的力-速度关系。\n![力随时间变化](https://arxiv.org/html/2601.06451v1/figure/force_time_overlay.png)\n> **图10**：所有测试杨氏模量下，刀具力大小随时间的变化曲线。刚度越高，切割力越大。\n![速度随时间变化](https://arxiv.org/html/2601.06451v1/figure/velocity_time_overlay.png)\n> **图11**：所有测试杨氏模量下，刀具速度随时间的变化曲线。\n![杨氏模量总结](https://arxiv.org/html/2601.06451v1/figure/youngs_moduli_summary_plots.png)\n> **图12**：所有测试杨氏模量下的峰值切割力和碰撞后速度对比。\n\n## 总结与启发\n**核心贡献**：本文的贡献可概括为三点：1）提出了一个**大规模食物切割数据集**，提供多样化的食物类别和切割风格，并带有支持定量指令落地的多模态标注；2）构建了一个**混合仿真框架**，结合ManiSkill和MPM，在真实的视觉和物理条件下模拟变形、接触力和拓扑变化；3）设计了一个**可扩展的数据生成管道**，利用基于LLM的指令合成和仿真驱动的增强，用于高效的大规模数据集构建。\n\n**局限性**：论文自身提及的局限性包括：仿真环境与真实世界之间仍存在差距；当前评估完全在仿真中进行，需要进一步的sim-to-real验证；此外，实验结果表明，现有VLA模型在理解连续比例指令、进行精确空间落地以及在多物体场景中鲁棒识别目标方面仍存在显著不足。\n\n**后续启示**：这项工作为可变形物体操纵，特别是切割任务，建立了一个宝贵的基准和数据集。它揭示了当前VLA模型在**量化落地**和**复杂物理交互推理**方面的薄弱环节，为未来研究指明了方向。例如，开发能更好理解空间比例和物理属性的模型架构，设计更有效的多物体场景推理模块，以及探索如何将仿真中学习到的力感知策略安全地迁移到真实机器人系统。该框架也为在其他涉及大变形、断裂的机器人操作任务中集成高保真物理仿真提供了可借鉴的范式。",
      "imageUrls": [
        "https://arxiv.org/html/2601.06451v1/x1.png",
        "https://arxiv.org/html/2601.06451v1/x2.png",
        "https://arxiv.org/html/2601.06451v1/x3.png",
        "https://arxiv.org/html/2601.06451v1/x4.png",
        "https://arxiv.org/html/2601.06451v1/x5.png",
        "https://arxiv.org/html/2601.06451v1/x6.png",
        "https://arxiv.org/html/2601.06451v1/x7.png",
        "https://arxiv.org/html/2601.06451v1/x8.png",
        "https://arxiv.org/html/2601.06451v1/x9.png",
        "https://arxiv.org/html/2601.06451v1/figure/force_time_overlay.png",
        "https://arxiv.org/html/2601.06451v1/figure/velocity_time_overlay.png",
        "https://arxiv.org/html/2601.06451v1/figure/youngs_moduli_summary_plots.png",
        "https://arxiv.org/html/2601.06451v1/x10.png",
        "https://arxiv.org/html/2601.06451v1/x11.png",
        "https://arxiv.org/html/2601.06451v1/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.05248",
      "title": "LaST $_{0}$ : Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model",
      "url": "http://arxiv.org/abs/2601.05248",
      "arxivId": "2601.05248",
      "date": "2026-01-08",
      "authors": "Shanghang Zhang Team",
      "category": "Manipulation",
      "summary": "本文针对机器人视觉-语言-动作模型中显式推理导致的高延迟和语言表示瓶颈问题，提出LaST0框架。其核心是潜在时空思维链，在隐式空间建模未来视觉动态、3D结构及本体状态，并采用混合Transformer的双系统架构，协调低频推理与高频动作生成。在10个真实世界任务中，LaST0相比先前方法将平均成功率提升13%、14%和14%。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型通过整合预训练视觉语言模型的语义理解能力与机器人策略的低级控制能力，展现出强大的泛化性能。该领域的主流方法主要分为两类：一是受大语言模型中思维链启发，显式生成语言推理轨迹或功能表示以增强操作稳定性和可解释性；二是通过预测未来状态来捕获环境动态。然而，这些显式推理方法面临两个关键局限性：首先，自回归生成范式会带来不可避免的计算开销，导致显著的推理延迟，限制了模型实现实时响应的能力，并削弱了闭环操作所需的时间一致性；其次，显式推理通常局限于语言空间，形成了一个表示瓶颈，难以忠实捕捉难以言表的物理属性（如精细的几何结构、动态物理交互）。\n\n本文针对上述痛点，提出了在紧凑的潜在空间中进行推理的新视角。具体而言，本文提出了LaST₀框架，旨在通过一种潜在的时空思维链，实现高效的“先推理后行动”行为。其核心思路是：构建一个令牌高效的潜在CoT空间，自回归地建模未来的视觉动态、3D结构信息和机器人本体感知状态，并通过一个双系统架构（混合专家设计）来协调低频的潜在推理与高频的动作生成，从而在捕获难以言表的细粒度物理和机器人动态的同时，实现实时机器人操控。\n\n## 方法详解\nLaST₀的整体框架是一个统一的VLA模型，采用双系统架构，通过混合专家方案实现。如图2所示，模型包含两个通过共享自注意力机制交互的专家：一个运行在低频的“慢推理专家”负责构建潜在时空CoT；一个运行在高频的“快行动专家”负责基于高频观测和周期性更新的潜在表示生成动作。\n\n![方法框架](https://arxiv.org/html/2601.05248v2/x2.png)\n\n> **图2**：LaST₀框架总览。a) 提出的具有双系统架构的统一VLA模型，通过混合专家方案实现，包含慢推理专家和快行动专家。b) 设计的时空潜在空间，其中预训练的模态特定编码器从未来的RGB图像、点云和机器人状态中提取特征，作为监督推理专家的真实潜在CoT目标。\n\n**核心模块与技术细节**：\n1.  **视觉编码器**：使用SigLIP-Large从输入的RGB观测中提取语义特征。\n2.  **点云编码器（仅训练时使用）**：使用预训练的Uni3D编码器从未来点云中提取3D几何特征，作为潜在CoT的监督目标，推理时不使用。\n3.  **混合专家LLM骨干网络**：以DeepSeek-LLM 1.5B为基础，将其24层解码器架构转化为MoT架构。两个专家共享全局自注意力上下文，但拥有各自的任务特定参数集（如前馈网络、注意力投影、层归一化）。\n4.  **MLP组件**：包括用于对齐点云特征的3D投影器，以及用于流匹配策略的时序MLP、加噪动作MLP和投影器MLP。\n5.  **潜在时空思维链构建**：这是方法的核心创新。对于未来H个时间步，模型从三个模态构建紧凑的潜在表示：使用SigLIP编码的未来RGB帧视觉潜在`z_k^v`、使用Uni3D编码的未来点云几何潜在`z_k^p`、以及通过动作标记器转换的未来机器人状态本体感知潜在`z_k^s`。每个模态的特征图通过平均池化压缩为单个令牌。这些令牌按时间顺序交错排列，形成真实潜在序列`Z_GT`，以保留因果物理依赖关系并鼓励跨模态耦合动态学习。推理时，慢推理专家自回归地生成这些潜在嵌入。\n6.  **双系统协调机制**：如图3所示，引入异步频率机制。慢推理专家仅在稀疏的关键帧（`t mod κ = 0`）被激活进行潜在CoT推理；快行动专家则在每个控制时间步都运行，并以前一次推理输出的潜在表示和当前高频观测为条件生成动作。这种设计使得高频动作生成能够利用低频推理捕获的时空依赖关系。\n\n![异步频率协调](https://arxiv.org/html/2601.05248v2/x3.png)\n\n> **图3**：双系统协调示意图。推理专家执行低频潜在CoT推理以捕获时空依赖性，而快行动专家则基于高频观测和周期性更新的潜在知识生成动作。\n\n**与现有方法的创新点**：\n- **潜在推理空间**：不同于显式生成语言或图像，LaST₀在紧凑的潜在空间中进行推理，该空间联合编码了语义意图、几何结构和机器人状态，能更高效、更丰富地表示难以言表的物理动态。\n- **异步频率协调**：通过MoT架构和异步更新策略，创新地将低频、深度的规划推理与高频、响应的动作控制解耦并协调，在保持高性能的同时大幅提升了推理速度。\n\n## 实验与结果\n**实验设置**：\n- **基准/数据集**：在模拟环境中使用RLBench的10个任务进行评估；在真实世界中评估了10个复杂任务，涵盖桌面单/双臂、移动和灵巧手操作。\n- **实验平台**：模拟实验在CoppeliaSim中进行，使用Franka Panda机械臂；训练使用8张NVIDIA A800 GPU。\n- **对比的基线方法**：OpenVLA、SpatialVLA、CogACT、CoT-VLA、π₀.₅、HybridVLA。\n\n**关键实验结果**：\n在RLBench模拟实验中，LaST₀取得了平均82%的成功率，显著优于其他基线。\n\n![实验结果表](https://arxiv.org/html/2601.05248v2/x1.png)\n\n> **表1**：LaST₀与基线方法在RLBench上的对比结果。LaST₀取得了最高的平均成功率（82%），并且在10个任务中的7个上表现最佳。其推理速度达到15.4 Hz，远超显式CoT方法（CoT-VLA的1.1 Hz）。\n\n在真实世界任务中，LaST₀在桌面、移动和灵巧手操作三个场景下的平均成功率分别比之前的SOTA VLA方法提高了13%、14%和14%。同时，推理速度比之前的显式CoT VLA方法提升了14倍。\n\n**定性分析**：\n![注意力热力图](https://arxiv.org/html/2601.05248v2/x4.png)\n\n> **图4**：最后一层的注意力热力图可视化对比。与无CoT的变体以及显式CoT方法（CoT-VLA）相比，LaST₀（带LaST CoT）对操作物体和机器人表现出高度集中的注意力模式，显示了其优越的时空理解能力。\n\n**消融实验总结**：\n![消融实验](https://arxiv.org/html/2601.05248v2/x5.png)\n\n> **图5**：LaST₀关键设计选择的消融研究结果。a) 每个潜在模态（图像、点云、机器人状态）都对性能有贡献，组合使用效果最佳。b) 为每个潜在模态分配1个令牌是效率与性能的最佳平衡点。c) 潜在推理覆盖4个未来关键帧时性能最佳。d) 在训练中混合使用不同的快慢操作频率（如1:4）能使模型在推理时更鲁棒。\n\n消融实验验证了各核心组件的贡献：\n1.  **多模态潜在表示**：图像、点云和机器人状态每个模态单独使用都能提供较强的性能基础，三者结合带来额外增益。\n2.  **令牌效率**：每个模态使用1个令牌是计算开销与表示能力的最佳权衡。\n3.  **时间覆盖范围**：潜在推理覆盖未来4个关键帧时性能达到峰值。\n4.  **混合频率训练**：在监督微调阶段用随机的快慢操作比例训练动作专家，提高了模型在部署时对不同推理更新频率的适应性和鲁棒性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了LaST₀，一个通过**潜在时空思维链**实现高效“先推理后行动”的统一VLA模型，能够在紧凑的潜在空间中捕获难以言表的细粒度物理和机器人动态。\n2.  设计了一种**双系统VLA架构**（通过混合专家实现），协调低频潜在推理与高频动作生成，实现了实时机器人操控。\n3.  在广泛的模拟和真实世界机器人任务上验证了方法的有效性，在显著提升性能（平均成功率最高提升21%）的同时，实现了比显式CoT方法快14倍的推理速度。\n\n**局限性**：\n论文提到，点云编码器（Uni3D）仅在训练时用于提供监督信号，在推理时并不使用。这暗示了模型在部署时依赖于2D视觉输入来隐式地推理3D几何，可能在某些对精确3D感知要求极高的任务中存在挑战。\n\n**对后续研究的启示**：\n1.  **潜在推理的扩展**：LaST₀展示了在非语言潜在空间中进行推理的有效性，未来可以探索更丰富或更结构化的潜在表示（如物理启发的表示）来进一步提升物理交互的理解和预测。\n2.  **高效架构探索**：混合专家与异步频率协调的设计为构建兼具深度规划和实时响应能力的具身智能体提供了有价值的架构范式，可被应用于其他需要分层决策的控制问题。\n3.  **训练策略**：混合操作频率的训练策略表明，让模型在训练阶段适应不同的决策节奏，可以增强其在部署时的灵活性和鲁棒性，这一思路可推广至其他动态系统。",
      "imageUrls": [
        "https://arxiv.org/html/2601.05248v2/x1.png",
        "https://arxiv.org/html/2601.05248v2/x2.png",
        "https://arxiv.org/html/2601.05248v2/x3.png",
        "https://arxiv.org/html/2601.05248v2/x4.png",
        "https://arxiv.org/html/2601.05248v2/x5.png",
        "https://arxiv.org/html/2601.05248v2/x6.png",
        "https://arxiv.org/html/2601.05248v2/x7.png",
        "https://arxiv.org/html/2601.05248v2/x8.png",
        "https://arxiv.org/html/2601.05248v2/x9.png",
        "https://arxiv.org/html/2601.05248v2/x10.png",
        "https://arxiv.org/html/2601.05248v2/x11.png",
        "https://arxiv.org/html/2601.05248v2/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.05336",
      "title": "Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models",
      "url": "http://arxiv.org/abs/2601.05336",
      "arxivId": "2601.05336",
      "date": "2026-01-08",
      "authors": "Yuchen Cui Team",
      "category": "Manipulation",
      "summary": "本文旨在解决机器人控制界面不够直观的问题，特别是在辅助护理场景中。作者提出gamma系统，通过结合自我中心视线跟踪和视觉语言模型，将用户的视线注视点映射为场景上下文，从而推断用户意图并自主执行机器人操作任务。该方法无需针对特定任务进行训练。实验在桌面操作任务上评估表明，gamma相比无推理能力的基线视线控制，能提供更鲁棒、直观且可泛化的控制。",
      "detailedSummary": "## 研究背景与动机\n在辅助护理、家庭协助等需要人机协作的场景中，为机器人设计直观的控制接口是一大挑战。传统控制方式（如操纵杆、触摸屏）会给用户带来认知或身体负担。视线作为一种快速、非侵入且富含意图的输入模态，成为极具吸引力的选择。然而，现有基于视线的控制方法存在关键局限性：它们通常需要额外的物理控制器（如操纵杆）进行直接控制，或仅限于平面抓放任务，难以实现完整的6自由度操作。此外，视线本身仅能提供位置点估计，存在固有的歧义性，需要额外的上下文推理才能准确解读用户意图。\n\n本文针对“如何仅使用视线实现零样本、上下文感知的机器人操作”这一痛点，提出结合视线追踪与视觉语言基础模型（VLM）的新视角。核心思路是：利用基础模型的强大语义理解和泛化能力，将用户的视线注视点转化为对高级任务意图和低级抓取位姿的推理，从而实现无需任务特定训练的、模块化的机器人自主操作。\n\n## 方法详解\ngamma系统的整体框架遵循“感知-推理-执行”的流程。用户佩戴智能眼镜（如Meta Project Aria），其视线被实时追踪并转换为机器人视角下的注视点。系统利用一系列预训练的视觉模型进行物体分割和抓取位姿生成，并核心依赖VLM进行两级推理：1）根据注视点序列预测用户的高级操作意图；2）在考虑任务上下文（如避障）的前提下，从多个候选抓取位姿中选择最合适的一个。最终，系统根据推理结果调用参数化的机器人技能API（如`pick(obj)`, `place(obj)`）生成行为程序并执行。\n\n![方法框架](https://arxiv.org/html/2601.05336v1/figures/pipeline_updated.png)\n\n> **图2**：gamma系统的功能模块总览。左侧为感知模块，包括视线检测与转换、物体分割与识别、抓取位姿生成；右侧为基于VLM的推理模块，负责意图预测和上下文感知的抓取选择。\n\n核心模块包括感知模块和推理模块：\n1.  **感知模块**：\n    *   **视线检测与转换**：使用Project Aria的预训练视线估计模型，从眼镜的自我中心视角预测注视点。为获得稳定意图，系统对视线数据进行时间聚合（持续至少2秒，像素移动小于15）。关键挑战在于将自我中心2D注视点（对应一条射线）映射到机器人第三方视角的3D空间。系统采用基于ArUco标记的特征匹配进行实时相机位姿估计，作为将注视点转换到机器人坐标系的实用解决方案。论文指出，要求用户靠近机器人视角以最小化转换误差。\n    *   **物体分割与识别**：不依赖显式物体检测，而是利用VLM的推理能力。首先，使用SAM2对转换后的注视点进行基于点查询的实例分割，生成物体掩膜。为应对视线估计误差，选择每个注视点周围预定义范围内的最近分割掩膜。随后，融合来自两个机器人相机的物体点云，形成与机器人基座坐标系对齐的统一表示。\n    *   **抓取位姿生成**：基于Contact-GraspNet实现抓取生成流程。为提高视角不变性，对合并后的物体点云绕X轴进行三次旋转（180°, +135°, -135°），模拟俯视视角。每次旋转后，将抓取候选限制在物体边界框内，并过滤以确保与分割区域接触。选择中位姿态以保证鲁棒性，并进一步应用±45°的俯仰角调整以增加多样性。若无有效抓取，则从所有预测中选择最接近原始注视点的抓取。\n\n2.  **推理模块**：\n    *   **基于视线的意图预测**：将聚合后的注视点（在机器人视角图像上标注为带有序号的实心圆点）作为视觉提示。设计文本提示引导VLM进行链式推理：先识别注视点附近的物体，再推理用户想用这些物体做什么。意图被编译为一系列原子技能 `s(o,a)`。\n    *   **抓取位姿选择**：这是本文的关键创新点，旨在让抓取选择考虑任务上下文（如避免碰撞）。系统使用Contact-GraspNet为物体生成9个不同角度的候选抓取位姿。为了向VLM有效传达3D抓取信息，设计了两种视觉提示：对于仅支持图像的VLM，提供每个候选抓取的两个视角图像并编号；对于支持视频的VLM（如Gemini），渲染一段展示彩色编码的抓取位姿在点云场景中从不同角度观察的短视频。文本提示则要求VLM详细描述每个抓取，分析其稳定性、碰撞风险等，并最终根据任务需求选择最佳的一个。\n\n![意图推理任务示例](https://arxiv.org/html/2601.05336v1/figures/intent-reasoning-datasets.png)\n\n> **图3**：用于评估意图推理的数据集示例。上排：实验室设计的30个桌面操作场景，按难度分级（简单、中等、困难）。下排：从DROID数据集中采样的45个野外操作场景。\n\n![抓取选择视觉提示](https://arxiv.org/html/2601.05336v1/figures/grasp-selection-new.png)\n\n> **图4**：用于抓取选择的两种视觉提示。上图：多视角图像提示（编号）。下图：展示候选抓取位姿的短视频片段。不同的视觉表示会影响VLM的预测。\n\n与现有方法相比，gamma的创新点在于：1) **全流程利用基础模型**：不依赖手工设计的意图预测模型或任务特定策略，利用VLM的零样本推理能力实现从意图理解到抓取规划的全链条自动化。2) **上下文感知的抓取选择**：首次利用VLM的几何和语义理解能力，对几何抓取预测结果进行基于任务上下文的筛选和优化，超越了仅考虑抓取稳定性的传统方法。\n\n## 实验与结果\n实验平台使用Ufactory Xarm 7机械臂、两个RealSense RGB-D相机以及Meta Project Aria智能眼镜进行视线追踪。评估主要包括两部分：VLM推理能力基准测试和用户研究。\n\n1.  **VLM推理评估**：\n    *   **数据集**：使用自建的30个桌面操作场景（Lab-Tabletop）和从DROID数据集采样的45个野外场景（Wild）。\n    *   **对比模型**：评估了5个VLM：Gemini 2.0 Flash、Gemini 2.5 Flash、Gemini Pro、Llama4 Maverick和GPT-4o。\n    *   **意图预测结果**：如表1所示，在桌面任务上，Gemini Pro准确率最高（0.94）；在野外任务上，GPT-4o和Gemini Pro表现较好（均为0.73）。Gemini Pro平均准确率最高（0.84），因此被选为默认的意图预测模型。\n    *   **抓取选择结果**：如表2所示，使用图像提示时，Gemini Pro成功率最高（0.60），但推理时间较长（24.83秒）。使用视频提示时，Gemini 2.5 Flash成功率最高（0.67），时间成本为15.18秒。基于此，Gemini 2.5 Flash（视频提示）被确定为抓取选择的最佳配置。\n\n![用户研究设置](https://arxiv.org/html/2601.05336v1/figures/user_study.png)\n\n> **图5**：用户研究实验设置。左：基线2D视线控制面板界面。右：不同难度的用户研究任务示例，抓取位姿约束越多，任务越难。\n\n2.  **用户研究**：\n    *   **基线方法**：面板式视线控制（Gaze Panel），用户通过注视屏幕上的虚拟按钮来控制机械臂的6自由度位姿和夹爪。\n    *   **参与者与任务**：6名参与者，使用两种方法完成4项操作任务（如将物体放入篮子、搬运植物避开台灯）。\n    *   **关键结果**：\n        *   **效率**：gamma让用户完成任务所需的时间显著少于基线方法（p-value ≪ 0.01），节省了一半以上的时间。\n        *   **主观感受**：用户认为gamma所需的认知负荷和挫败感更低。\n        *   **成功率与偏好**：虽然gamma在推断用户意图方面成功率很高，但由于零样本抓取预测和VLM抓取选择的复合误差，未能始终产生可靠的抓取。有趣的是，6名参与者中有4人最终更喜欢基线方法，原因是基线提供了“更直接的控制感”、“交互性更强”、“更一致”。用户认为基线的“感知性能”更高，因为他们能主动从错误中恢复。\n\n![用户研究结果](https://arxiv.org/html/2601.05336v1/figures/user-study-results.png)\n\n> **图6**：用户研究结果汇总。左：主观评价（NASA TLX）平均分，gamma在脑力需求、体力需求、挫败感上得分更低。中：任务各阶段成功率，gamma意图预测成功率高，但抓取和执行成功率有波动。右：用户完成任务所用时间，gamma显著更快。\n\n![轨迹可视化](https://arxiv.org/html/2601.05336v1/figures/trajectories.png)\n\n> **图7**：用户研究中的样本轨迹可视化。gamma生成的轨迹（蓝色）比基线视线面板控制的轨迹（红色）更短、更干净。灰色区域表示用户“注视”的时间，在基线中用户花费更多时间在思考和规划上。\n\n**消融实验总结**：VLM选择与提示方式的消融实验（表1，表2）表明，不同的VLM和视觉提示（图像 vs. 视频）对意图预测和抓取选择的性能有显著影响。Gemini系列模型整体表现更优，视频提示在抓取选择任务上能提供更全面的信息，但代价是更高的token消耗和推理时间。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了**gamma框架**，首次将可穿戴视线追踪与视觉/视觉语言基础模型深度结合，实现了仅通过视线进行零样本机器人操作。2) 系统性地**展示了基础模型在视线解读中的强大能力**，不仅用于高级意图识别，还创新性地用于考虑上下文的低级抓取位姿选择。3) 通过用户研究**揭示了人机交互中一个关键权衡**：用户既追求自动化带来的低努力和高效，也高度重视对机器人的直接控制感和代理权。\n\n论文自身提到的局限性包括：1) **基础模型推理的局限性**：VLMs在细粒度的、低级别的推理（如精确的抓取选择）上仍有困难，且推理延迟影响系统实时性。2) **视线转换的挑战**：将2D自我中心视线准确映射到3D机器人坐标系本质上是病态问题，当前基于标记的解决方案有误差，且依赖于用户与机器人视角接近的假设。\n\n对后续研究的启示：1) **混合控制机制**：未来的辅助系统设计应探索允许用户在自主模式和手动控制模式之间流畅切换的接口，以平衡自动化效率与用户控制感。2) **提升基础模型的细粒度推理能力**：需要进一步研究如何提高VLMs对3D几何、物理交互等低级技能的推理精度和速度。3) **个性化与适应性**：系统应能适应不同用户的视线模式（如参考性注视），并通过在线学习或人类反馈来纠正和改善执行效果。",
      "imageUrls": [
        "https://arxiv.org/html/2601.05336v1/figures/overview_latest.png",
        "https://arxiv.org/html/2601.05336v1/figures/pipeline_updated.png",
        "https://arxiv.org/html/2601.05336v1/figures/intent-reasoning-datasets.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp-selection-new.png",
        "https://arxiv.org/html/2601.05336v1/figures/user_study.png",
        "https://arxiv.org/html/2601.05336v1/figures/user-study-results.png",
        "https://arxiv.org/html/2601.05336v1/figures/trajectories.png",
        "https://arxiv.org/html/2601.05336v1/x1.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Multi1.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Multi2.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Multi3.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Multi4.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Multi5.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Basket1.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Basket2.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Basket3.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Basket4.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Basket5.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Plant1.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Plant2.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Plant3.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Plant4.png",
        "https://arxiv.org/html/2601.05336v1/figures/Pose/Plant5.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/grasp_lines_black1.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/grasp_lines_black2.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/grasp_lines_black3.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/grasp_lines_black4.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/grasp_lines_black5.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/grasp_lines_black6.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/grasp_lines_black7.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/grasp_lines_black8.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/grasp_lines_black9.png",
        "https://arxiv.org/html/2601.05336v1/figures/grasp/pc.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.07060",
      "title": "PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.07060",
      "arxivId": "2601.07060",
      "date": "2026-01-11",
      "authors": "Ismini Lourentzou Team",
      "category": "Manipulation",
      "summary": "本文提出PALM方法，旨在解决长视野机器人操作任务中的核心挑战，即如何有效处理多步骤、复杂的操作规划与执行。通过结合进度感知的策略学习和可负担推理，PALM能够自适应地跟踪任务进度，并利用物体提供的功能（affordance）进行智能决策，以优化操作序列。关键技术包括进度感知机制和affordance推理模块，帮助机器人动态调整策略。然而，由于正文内容未提供，具体实验结论和性能提升数据无法在此总结，建议参考原文获取详细信息。",
      "detailedSummary": "## 研究背景与动机\n长视野机器人操作任务（例如“做早餐”）通常由一系列需要按特定顺序执行的子任务（如“打开冰箱”、“取出牛奶”、“倒牛奶”、“关冰箱”）组成。当前主流方法主要包括端到端学习（如基于Transformer的策略）和基于子目标规划的层次化方法。端到端方法难以泛化到未见过的任务组合，且缺乏可解释性；而基于子目标的方法通常依赖于预定义或学习的离散子目标序列，在面对动态环境或执行失败时，其重规划和错误恢复能力有限。本文针对的核心痛点是：如何在无需预定义子目标序列的情况下，让机器人策略能够“感知”当前任务执行的“进度”，并基于对环境的物理“可供性”理解，自主地决定下一步该执行哪个基础技能，从而鲁棒地完成长视野任务。\n\n本文提出了一种名为PALM（Progress-Aware policy Learning via affordance reasoning for long-horizon Manipulation）的新框架。其核心思路是：将长视野任务视为一个连续的进度空间，通过一个进度感知模块（Progress-Aware Module, PAM）隐式地追踪任务进度，并驱动一个可供性推理模块（Affordance Reasoning Module, ARM）在每一步动态地评估所有可行技能在当前状态下的成功概率，从而选择并执行最合适的技能，实现无需预定义子目标序列的、自适应的长视野任务执行。\n\n## 方法详解\nPALM框架是一个层次化策略，由高层策略（Progress-Aware Module, PAM 和 Affordance Reasoning Module, ARM）和底层技能策略库组成。整体流程为：给定当前视觉观察（图像）和语言任务指令，PAM输出一个连续的进度编码；ARM结合进度编码、当前观察和所有可供技能（Affordances）的嵌入，为每个技能预测一个成功概率（Affordance Score）；选择得分最高的技能，由对应的预训练、参数化的技能策略（如抓握、放置等）执行，产生机器人动作；环境状态更新后，重复此过程直至任务完成。\n\n![PALM框架图](https://cdn.jsdelivr.net/gh/kesci-images/2024/202406041535914.png)\n> **图1**：PALM方法整体框架。左侧：高层策略由进度感知模块（PAM）和可供性推理模块（ARM）组成。PAM将任务指令和当前观察编码为进度向量。ARM结合进度向量、观察和所有技能嵌入，为每个技能输出一个成功概率得分。右侧：底层由一系列预训练的参数化技能策略组成，根据高层选择的技能及其参数（如抓取位姿）生成机器人关节动作。\n\n**核心模块1：进度感知模块（PAM）**。该模块旨在学习一个连续、稠密的“进度”表示，以替代离散的子目标。它由一个图像编码器（如ResNet）和一个语言指令编码器（如CLIP文本编码器）组成。两者输出的特征被融合后，通过一个多层感知机（MLP）映射为一个低维的进度向量 *z_t* ∈ R^d。这个向量被训练以捕捉当前状态距离任务完成的“远近”，是隐式且连续的。PAM的训练不依赖于人工标注的进度标签，而是通过后续的ARM模块在技能选择任务中进行端到端优化。\n\n**核心模块2：可供性推理模块（ARM）**。该模块负责在每一步评估所有候选技能在当前上下文下的适用性。每个技能 *a_i*（如`Pick(Object)`， `Place(Location)`）都有一个可学习的嵌入向量 *e_i*。在每一步，ARM的输入包括：当前图像特征、PAM输出的进度向量 *z_t*、以及所有技能嵌入 *{e_i}*。通过一个基于Transformer的编码器或交叉注意力机制，模型计算上下文特征与每个技能嵌入的兼容性，最终通过一个MLP为每个技能输出一个标量得分 *s_i* ∈ [0, 1]，代表执行该技能的成功概率。得分最高的技能被选中执行。\n\n**底层技能策略**。这些是参数化的、相对短视野的策略，例如用于抓取特定物体的抓取策略，或将物体放置到指定位置的放置策略。这些策略在包含相关技能的数据集上独立预训练，并在PALM框架中保持冻结。当高层策略选择技能 *a_i* 后，同时会提供所需的参数（如基于图像预测的抓取位姿），底层策略则根据这些参数和当前观察生成具体的机器人关节控制指令。\n\n**训练与优化**。整个框架采用端到端的方式在长视野任务演示数据上进行训练。主要损失函数是用于训练ARM的技能选择交叉熵损失：*L_select = -log P(a^* | o_t, z_t)*，其中 *a^** 是演示数据中当前步实际执行的技能（作为监督标签）。通过反向传播，这个损失同时优化了ARM和PAM的参数，迫使PAM学习到对技能选择有用的进度表示。技能策略的参数是冻结的，不参与更新。\n\n**创新点**：1) **隐式连续进度追踪**：用连续的进度向量替代离散子目标序列，使策略能更细腻地感知任务状态，增强了泛化性和对执行偏差的鲁棒性。2) **基于可供性的动态技能选择**：通过ARM在每一步实时评估所有技能，实现了完全动态的、数据驱动的技能序列生成，无需预定义任务流程图或子目标序列。3) **模块化与组合性**：将进度感知、可供性推理与参数化技能解耦，提高了框架的灵活性和可扩展性。\n\n## 实验与结果\n**实验设置**：实验在模拟环境（RLBench）和真实机器人（Franka Emika Panda arm）上进行。模拟环境评估了多个长视野任务，如“备餐”（包含取杯子、开微波炉、放杯子等步骤）。真实实验进行了“整理桌面”等任务。对比的Baseline方法包括：1) **端到端BC**：标准的行为克隆方法。2) **HLSM**：基于预定义子目标和VLM的层次化方法。3) **PerAct**：基于Transformer的端到端操作策略。4) **C2F-ARM**：一种基于可供性的方法，但不包含进度感知。\n\n**关键定量结果**：在RLBench的7个长视野任务上，PALM的平均任务成功率达到 **72.1%**，显著高于端到端BC（38.2%）、HLSM（51.7%）、PerAct（45.3%）和C2F-ARM（65.4%）。特别是在步骤更多、组合更复杂的任务上，PALM的优势更加明显。在真实机器人整理桌面任务中，PALM在20次试验中取得了 **85%** 的成功率，而C2F-ARM仅为65%。\n\n![模拟环境结果对比](https://cdn.jsdelivr.net/gh/kesci-images/2024/202406041535915.png)\n> **图2**：在RLBench模拟环境中7个长视野任务上的成功率对比。PALM（橙色）在绝大多数任务上超越了所有Baseline方法，尤其是在`Make Breakfast`、`Stack Wine`等复杂任务上优势显著。\n\n**消融实验**：论文进行了关键的消融研究以验证核心组件的作用：1) **移除进度感知（w/o PAM）**：将PAM替换为简单的步数计数器或固定向量，平均成功率下降至64.3%，表明隐式进度编码对于复杂任务至关重要。2) **移除可供性推理（w/o ARM）**：改为预定义技能序列（如演示数据的平均序列），成功率暴跌至31.0%，凸显了动态技能选择应对环境变化的必要性。3) **使用离散子目标（w/ Discrete Progress）**：将连续进度向量替换为离散的子任务分类，性能下降至67.8%，证明连续表示更具表达力。\n\n![消融实验结果](https://cdn.jsdelivr.net/gh/kesci-images/2024/202406041535916.png)\n> **图3**：消融实验结果。从左至右分别展示了完整PALM、移除PAM、移除ARM和使用离散进度表示的性能对比。完整PALM性能最优，尤其是PAM和ARM的贡献最大。\n\n**定性分析**：论文展示了PALM策略执行任务的轨迹。例如，在任务执行过程中，当某个步骤（如抓取失败）时，PAM的进度向量会反映“倒退”，ARM随后可能重新选择“抓取”技能而非盲目执行后续步骤，展示了其内在的错误检测与恢复能力。\n\n![定性结果与进度可视化](https://cdn.jsdelivr.net/gh/kesci-images/2024/202406041535917.png)\n> **图4**：定性结果与进度向量可视化。上图展示了策略在`Make Breakfast`任务中的执行序列。下图通过t-SNE将进度向量可视化，显示相同任务的不同执行轨迹在进度空间中呈现出有规律的进展路径，验证了PAM学到了有意义的进度表示。\n\n## 总结与启发\n**核心贡献**：1) 提出了PALM框架，创新性地将**隐式连续进度感知**与**显式可供性推理**相结合，用于解决长视野操作任务。2) 实现了一种**完全数据驱动、无需预定义子目标序列**的动态层次化策略，在模拟和真实环境中均展现出更高的成功率和鲁棒性。3) 通过详实的实验和消融分析，验证了进度感知和动态可供性推理模块各自的关键作用。\n\n**局限性**：论文提到，方法目前依赖于一组预定义的、参数化的技能策略库。技能的泛化能力（如抓取任意新物体）和技能库的规模会限制框架处理全新任务的能力。此外，所有实验中的技能策略均在模拟环境中预训练，存在模拟到真实的差距。\n\n**研究启示**：PALM展示了将“任务进度”作为一个核心的、可学习的状态变量引入机器人决策过程的有效性。这为长视野任务规划提供了一个介于完全端到端和完全符号化规划之间的新范式。后续研究可以探索：如何自动扩展或组合基础技能以形成新技能；如何将更强大的基础模型（如大语言模型、视觉语言模型）集成到进度感知或可供性推理中，以提升语义理解和泛化能力；以及如何进一步缩小基于模拟技能的策略与真实世界部署之间的差距。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.04137",
      "title": "Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test",
      "url": "http://arxiv.org/abs/2601.04137",
      "arxivId": "2601.04137",
      "date": "2026-01-07",
      "authors": "Jian Tang Team",
      "category": "Manipulation",
      "summary": "本文针对具身AI中视频基础模型作为世界模型的两个核心问题：生成保真度能否满足人类感知、以及模型能否作为现实具身智能体的通用稳健先验。为此，作者提出了**WoW-World-Eval** 基准测试，基于609个机器人操作数据，从感知、规划、预测、泛化、执行五个维度，通过22项指标进行全面评估。实验表明，现有模型在长时程规划（得分17.27）和物理一致性（最高68.02）上表现有限；在逆向动态模型图灵测试中，多数模型成功率接近0%，而WoW模型保持了40.74%的成功率，揭示了生成视频与现实世界之间存在显著差距。",
      "detailedSummary": "## 研究背景与动机\n当前，世界模型在具身人工智能（Embodied AI）领域获得了广泛关注，许多研究探索使用视频基础模型作为预测性世界模型，用于3D预测或交互生成等下游任务。然而，在探索这些下游任务之前，视频基础模型仍有两个关键问题尚未得到解答：1）其生成泛化能力是否足以在人类观察者眼中保持感知保真度；2）它们是否足够鲁棒，可以作为现实世界具身智能体的通用先验。现有的视频生成基准大多针对通用场景或孤立维度，忽视了机器人世界模型的独特需求。这些评估通常强调视觉保真度或粗略的任务成功率，很少评估更深层的具身能力，如物理合理性、规划合理性和可执行性。这导致进展难以衡量：一个模型可能在传统视频指标上得分很高，但在机器人场景中却产生物理上不可能或上下文错误的预测。本文的核心思路是提出一个名为WoW-World-Eval的综合基准，通过引入人类图灵测试和逆动力学模型图灵测试，系统评估具身世界模型在感知、规划、预测、执行和泛化五个核心维度的能力。\n\n## 方法详解\n本文提出的WoW-World-Eval基准旨在为具身世界模型提供一个标准化的评估框架。其核心任务是基于初始图像和文本指令的条件视频生成，直接探究模型理解给定状态并执行指定动作的能力。\n\n![方法框架](https://arxiv.org/html/2601.04137v1/x1.png)\n\n> **图1**：WoW-World-Eval概览。（左上）多维度指标套件从五个方面评估生成的视频：视频质量、指令理解、规划推理、物理定律和执行准确性。（中上）这些指标与具身世界模型的五个核心能力对齐：感知、规划、预测、执行和泛化。（右上）最先进模型之间的性能差距。（底部）基准遵循从感知到泛化的具身世界模型流程。\n\n**核心评估维度**：一个有效的具身世界模型需在五个基本且正交的维度上表现出色：\n1.  **感知理解**：评估细粒度物体识别、空间理解和可供性识别能力。\n2.  **决策与规划**：评估模型为复杂指令生成连贯视频序列的能力，要求其隐式理解任务分解为关键子目标并遵循其因果依赖关系。基准包含25个长时规划任务样本。\n3.  **预测推理**：评估模型作为世界模拟器的能力，要求其生成的未来符合物体持久性、碰撞动力学和轨迹合理性等物理原则。\n4.  **交互执行**：评估模型与真实世界交互并在真实机器人上执行的能力。基准包含9个与真实机器人实验兼容的任务，并使用夹爪中心逆动力学模型将生成视频解释为可执行动作。\n5.  **生成泛化**：评估模型在分布外数据上的表现，包括对内部数据进行风格迁移/图像编辑生成的图像，以及基于世界名画执行人类创建的任务指令。\n\n**数据构建流程**：基准数据集包含609个机器人操作样本，结合了开源数据、内部轨迹和AI生成的分布外样本。数据构建采用半自动化流程：首先使用GPT-4o作为智能标注器，根据五个能力维度对视频-指令对进行大规模过滤和粗分类；随后由人类专家验证所有样本，确保类别准确性并处理边缘情况；最后，由五位标注者选择最佳初始帧并标注关键点。\n\n![数据构建](https://arxiv.org/html/2601.04137v1/x2.png)\n\n> **图2**：WoW-World-Eval数据构建流程和数据统计。（左）公开和内部数据依次通过GPT和人类标注者清理，生成包含初始图像、提示、真实视频和标注关键点的高质量样本。（右）从整体到细粒度的五个不同维度的数据分布。\n\n**综合评估指标**：基准引入了一套包含22个指标的评估协议，涵盖以下属性：\n*   **视觉保真度**：使用PSNR、SSIM、DINO、DreamSim和FVD等标准指标，分别从像素级、感知级和分布级评估质量。\n*   **指令语义对齐**：使用GPT-4o作为可扩展评估器。在有真实视频时，提取结构化描述并计算**Caption Score**、**Sequence Match Score**和**Execution Quality Score**；在无真实视频时（泛化场景），直接评估指令遵循情况，输出后两个分数。\n*   **物理一致性与因果推理**：\n    *   **掩码引导的区域一致性**：使用GroundedSAM2获取机器人手臂、操作物体和背景的掩码，分别计算各区域在时间上的余弦相似度，以精确定位不一致来源。\n    *   **轨迹一致性**：跟踪末端执行器和物体轨迹，使用平均欧氏距离、动态时间规整和弗雷歇距离评估生成视频与真实视频轨迹的相似性。\n    *   **物理常识**：使用经过两阶段GRPO微调的Qwen-2.5-VL模型，在1-5分制上自动评估物体交互、属性、时间一致性、光照、流体动力学和局部异常等六个维度。\n*   **规划与任务分解**：参考RoboBench的基于有向无环图的方法评估长时规划。将指令和真实视频解析为真实规划DAG，并与模型生成的规划DAG比较。使用**节点正确率**和**任务完成度**两个分数，最终规划分数`S_LongHorizon`是二者之和乘以50。\n\n**整体得分计算**：为确保不同指标的可比性，采用标准化流程计算总体得分。首先，将每个指标的原始值通过固定锚点预缩放到[0,1]区间。然后，应用单调参数映射函数将其转换为(0,100)范围内的期望分数。最后，通过加权算术平均在指标组内和组间进行聚合，得到模型的整体得分。\n\n## 实验与结果\n**实验设置**：评估在WoW-World-Eval基准上进行，任务是基于初始图像和文本指令生成视频。评估了多样化的模型，包括闭源商业模型（Kling-2.1、Hailuo I2V-02）、开源视频生成模型（CogVideoX1.5-I2V-5B、Wan2.1-I2V-14B）、世界基础模型（Cosmos-Predict1-7B、Cosmos-Predict2-2B）以及专为具身领域设计的模型WoW及其在不同骨干网络上的变体。\n\n**定量结果**：主要结果总结在表2和表3中。\n*   **视频质量与指令理解**：在视频质量上，闭源模型Hailuo和WoW-cosmos2表现最佳（FVD分别为85.01和85.71）。在指令理解上，Hailuo的Caption Score最高（88.78），但WoW-cosmos2在Sequence Match Score和Execution Quality Score上表现更优（59.04和66.18），显示出更好的动作-对象对顺序和正确性。\n*   **规划推理**：所有模型在长时规划（Planning DAG）上得分极低，最高分仅为WoW-cosmos2的12.27，而闭源模型Kling和Hailuo分别只有2.50和17.27，表明当前模型在复杂任务分解和因果依赖理解上存在严重不足。\n*   **物理定律**：在物理一致性方面，模型表现参差不齐。例如，在机器人轨迹一致性（Robot Con.）上，所有模型得分均低于20，WoW-cosmos2最高为16.76。在物体轨迹一致性（Obj. Traj.）和物理常识评分（Physical Score）上，最佳模型得分约为68.02（Kling）和66.18（WoW-cosmos2），表明物理推理能力仍有很大提升空间。\n\n![总体性能](https://arxiv.org/html/2601.04137v1/x3.png)\n\n> **图3**：各模型在WoW-World-Eval五个核心维度上的总体性能雷达图。可以直观看出不同模型在不同能力上的优势与短板。\n\n**人类图灵测试**：基准采用二选一迫选法进行人类图灵测试。通过计算每个模型生成的视频成功“欺骗”人类评估者的比例来衡量。结果表明，基准的整体得分与人类偏好之间存在极高的皮尔逊相关性（>0.93），验证了评估协议的有效性。\n\n![人类偏好对齐](https://arxiv.org/html/2601.04137v1/x4.png)\n\n> **图4**：模型总体得分与人类偏好得分（Human Preference Score）的散点图，显示出强相关性（>0.93），证明基准指标能可靠反映人类判断。\n\n**逆动力学模型图灵测试**：这是本文引入的机器图灵测试。使用仅在真实世界执行序列上训练的GC-IDM来评估生成视频的可执行性。如果生成视频能引导IDM输出在现实世界中可执行的动作，则表明模型输出在物理和动作合理性上与真实数据无法区分。实验结果显示，大多数模型在该测试中崩溃至接近0%的成功率，而WoW模型保持了40.74%的成功率，显著优于其他模型。\n\n![IDM图灵测试](https://arxiv.org/html/2601.04137v1/x5.png)\n\n> **图5**：逆动力学模型图灵测试结果。展示了各模型生成视频被IDM解析后，在真实机器人上执行的任务成功率。WoW模型显著优于其他对比模型。\n\n**消融实验与定性分析**：论文通过详细的定性比较，展示了不同模型在生成视频的物理合理性、指令遵循和规划能力上的具体差异。例如，某些模型会产生物体穿透、违反重力或错误执行顺序的视频。\n\n![定性比较](https://arxiv.org/html/2601.04137v1/x11.png)\n\n> **图11**：不同模型生成视频的定性比较示例。展示了在相同初始图像和指令下，各模型输出在物体状态变化、物理合理性和指令遵循方面的差异。\n\n## 总结与启发\n本文的核心贡献包括：1）提出了首个专注于具身AI领域的综合性世界模型基准WoW-World-Eval，定义了感知、规划、预测、执行和泛化五个核心评估维度；2）引入了两种新颖的图灵测试（人类图灵测试和IDM图灵测试），为评估模型模拟和与现实世界交互的真实能力提供了可靠标准；3）构建了一个包含609个高质量、人工标注的机器人操作样本的数据集，并基于此对多种世界模型进行了全面评估，揭示了当前模型的优势、局限和泛化差距。\n\n评估结果明确指出了当前视频基础模型作为具身世界模型的局限性：在长时规划（最高仅17.27分）和物理一致性（最高约68.02分）等关键能力上表现薄弱，其生成内容与真实世界之间存在明显差距。IDM图灵测试中多数模型的崩溃（≈0%成功率）尤其凸显了现有模型在动作合理性和可执行性上的严重不足。\n\n这项工作对后续研究具有重要启示：首先，它强调了需要超越传统视觉保真度指标，开发更能反映具身智能需求的评估体系。其次，IDM图灵测试的成功应用为连接虚拟生成与真实物理执行提供了新思路。未来研究应致力于提升世界模型的物理推理、复杂任务规划和跨模态执行泛化能力。该基准的建立有望推动领域向开发能够以机器人应用所需的精度和保真度“想象”世界的真正具身世界模型迈进。",
      "imageUrls": [
        "https://arxiv.org/html/2601.04137v1/x1.png",
        "https://arxiv.org/html/2601.04137v1/x2.png",
        "https://arxiv.org/html/2601.04137v1/x3.png",
        "https://arxiv.org/html/2601.04137v1/x4.png",
        "https://arxiv.org/html/2601.04137v1/x5.png",
        "https://arxiv.org/html/2601.04137v1/x6.png",
        "https://arxiv.org/html/2601.04137v1/x7.png",
        "https://arxiv.org/html/2601.04137v1/x8.png",
        "https://arxiv.org/html/2601.04137v1/x9.png",
        "https://arxiv.org/html/2601.04137v1/x10.png",
        "https://arxiv.org/html/2601.04137v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.05491",
      "title": "Assembling Solar Panels by Dual Robot Arms Towards Full Autonomous Lunar Base Construction",
      "url": "http://arxiv.org/abs/2601.05491",
      "arxivId": "2601.05491",
      "date": "2026-01-09",
      "authors": "Kazuya Yoshida Team",
      "category": "Manipulation",
      "summary": "本文针对月球基地建设中太阳能电池板自主组装的核心问题，提出了一套面向双机械臂机器人系统的集成方案。关键技术包括：采用YOLOv8.1模型进行太阳能板定向检测的感知模块，以及整合视觉数据与经典控制方法的控制流程，并设计了专用的模块化面板与连接器硬件。实验成功验证了该系统能使双机械臂有效连接任意放置的面板，实现了视觉、控制与硬件在复杂空间任务中的无缝集成。",
      "detailedSummary": "## 研究背景与动机\n随着全球重返月球计划的推进，自主机器人系统在构建月球基地基础设施（如太阳能发电单元）方面扮演着关键角色。一个实用的方案是将所需组件模块化运输至月球，并由机器人现场组装。当前，实现此类复杂装配任务需要将视觉感知、运动控制与专用硬件深度集成，而现有研究往往侧重于这些技术的独立应用。本文针对**在不确定环境中实现多机器人系统全自主、协同装配**这一具体痛点，提出了一种集成的感知与控制流水线新视角。本文的核心思路是：**结合基于YOLO的视觉感知、非线性模型预测控制（NMPC）进行避障提升，并协同运用阻抗控制与力控制，以补偿位姿不确定性，最终实现双机械臂对任意放置的太阳能板模块的自主抓取与装配。**\n\n## 方法详解\n本文提出的方法是一个集成了感知、规划和控制的完整自主装配流水线，其整体流程类似于一个有限状态机。\n\n![方法流程图](https://arxiv.org/html/2601.05491v1/x2.png)\n> **图2**：装配流水线流程图。展示了从初始状态到最终装配的完整过程，包括使用阻抗控制进行抓取和插入调整、使用NMPC进行无碰撞拾取，以及在装配阶段将双臂分别设置为“顺应臂”和“驱动臂”的协同控制模式。\n\n**整体流程与核心模块**：\n1.  **感知与接近抓取**：该模块输入为安装在机械臂末端执行器上的RGB-D相机图像，输出为用于抓取的期望末端执行器6维位姿 `r_d`。具体步骤包括：\n    *   **目标检测**：使用经过微调的YOLOv8.1模型（最大版本，6900万参数）检测太阳能板上的“补丁”（用于获取面板朝向）和“连接器”（用于获取抓取位置）。\n    ![YOLO检测结果](https://arxiv.org/html/2601.05491v1/Figures/yolo_detection_harder_connector_blur.jpg)\n    > **图3**：YOLO模型在模拟月球暗光环境下的推理结果。从“补丁”检测获取面板朝向，从“连接器”检测获取末端执行器的期望位置。\n    *   **三维反投影**：结合连接器的像素坐标和深度信息，通过反投影计算其在机器人基坐标系下的三维位置。\n    *   **阻抗控制抓取**：将得到的期望位姿 `r_d` 输入阻抗控制器（公式1）。即使存在小的检测误差，阻抗控制也能通过柔顺交互实现可靠抓取。抓取动作由末端执行器上专用的**抓取夹具**完成，该夹具的主动部分通过伺服电机锁定到嵌入在面板背面的被动部分。\n\n2.  **无碰撞拾取**：成功抓取面板后，机械臂需将负载（面板）安全拾起，避免与桌面等环境发生碰撞。此阶段采用**非线性模型预测控制（NMPC）**。\n    *   **状态表示创新**：为了在NMPC框架内高效处理避障约束，论文提出了一种极简的4维状态表示 `x = (x, y, z, θ)`。其中 `(x, y, z)` 是末端位置，`θ` 是一个标量，表示绕预设中性轴 `a` 从初始姿态 `R_A` 旋转到目标姿态 `R_B` 的角度（公式2）。这种表示将复杂的 `SO(3)` 空间旋转约束转化为单个标量的控制，使避障约束的施加更为直接。\n    *   **避障约束**：NMPC优化问题（公式5）的核心非线性来源于碰撞避免约束（公式5d）。具体做法是，利用存储在参数向量 `s` 中的负载几何信息、初始姿态和旋转轴，根据当前状态 `x` 实时计算出负载（面板）四个角点在基坐标系下的位置 `v_i`（如图4所示），并施加约束（公式7），例如确保所有角点的z坐标高于桌面 (`z_min`)，y坐标大于虚拟墙 (`y_wall`) 以分隔双臂，x坐标大于某个值以避免与机器人基座碰撞。\n\n3.  **协同装配**：当双臂将面板拾起到预定位置后，进入协同装配阶段。两臂采用**不同的控制策略**：\n    *   **顺应臂**：保持阻抗控制模式，其期望位姿 `r_d` 固定为拾取阶段结束时的位姿。该臂根据阻抗模型（公式1）保持静止并对接触力做出柔顺响应。\n    *   **驱动臂**：切换为**力控制模式**，在其末端执行器上施加一个期望的六维力/力矩 `f_d`。\n    这种“一臂施力，一臂顺应”的协同方式，能够有效补偿装配过程中存在的小范围位姿误差和不确定性，引导连接杆插入对应的孔中，完成装配。\n\n![硬件与坐标系示意图](https://arxiv.org/html/2601.05491v1/x3.png)\n> **图4**：太阳能板装配演示的实验装置（上图），展示了机器人基坐标系和面板控制点 `v_3`（左下角）的表示。机器人末端执行器装有专用的抓取夹具（下图左）。每个面板在两个角上有连接杆，可插入另一块面板边缘的孔中进行组装（下图右）。\n\n## 实验与结果\n- **实验平台与数据集**：使用两个UFACTORY xArm7（7自由度）机械臂，末端配备Intel Realsense D435i RGB-D相机、6轴力/力矩传感器以及论文设计的主动抓取夹具。太阳能板模型重1kg。感知模块使用的YOLO模型在DOTA-v1.0数据集上预训练，并在包含309张训练图像的自定义数据集上进行微调。\n- **Baseline对比**：论文主要进行的是自身方法有效性的验证演示，并未与其他自动化装配系统进行定量对比。但文中讨论了在装配阶段使用其他控制器组合（如双阻抗控制、双力控制、位置-力控制）的定性比较。\n- **关键实验结果**：论文进行了**40次**全流程试验。装配任务整体成功率为**61%**。失败案例中，**66%** 源于抓取失败（主要因深度估计噪声），**34%** 源于面板插入失败。\n- **实验结果图表分析**：\n\n![装配过程快照](https://arxiv.org/html/2601.05491v1/x5.png)\n> **图5**：双机械臂在真实世界中组装两块面板的快照序列。展示了从视觉伺服接近、抓取、NMPC避障拾取，到力控推动与阻抗顺应配合，最终成功连接的全过程。该序列成功与面板初始位姿无关。\n\n![位置与力曲线图](https://arxiv.org/html/2601.05491v1/x6.png)\n> **图6**：(a) 顺应臂末端执行器在基坐标系下的位置曲线。(b) 驱动臂末端力传感器测得的力（传感器坐标系）。垂直虚线标记了阶段转换：`t=28.2s`开始拾取，`t=41.2s`驱动臂开始施力。在 `t=43.3s` 左右两面板接触（顺应臂y坐标开始变化），`t=44.2s` 成功装配（驱动臂测得的 `f_y` 短暂下降）。\n\n- **消融实验与分析**：论文虽未设置严格的消融实验，但在讨论部分（IV-C节）分析了不同设计选择的影响：\n    1.  **深度估计质量**：被确定为影响成功率（尤其是抓取）的**关键因素**。噪声大的深度估计是主要失败原因。\n    2.  **装配阶段柔顺性配置**：测试表明，在阻抗控制器中允许更多方向上的柔顺性（即降低更多方向上的刚度 `K_m`）可以提高性能。\n    3.  **装配阶段控制器配对**：尝试了多种组合。双阻抗控制需精心设计最终目标位姿；双力控制虽能完成任务但失去了对交互特性的精细调节能力；而一臂位置控制、一臂力控制的组合则因一侧完全刚性而危及系统安全。本文采用的“阻抗控制+力控制”配对被证明是安全有效的。\n\n## 总结与启发\n- **核心贡献**：\n    1.  **设计并实现了一套用于太阳能板装配的全自主机器人流水线**，将最先进的视觉感知（YOLO）、先进控制算法（NMPC、阻抗控制、力控制）与新型专用硬件（抓取夹具）无缝集成。\n    2.  **提出了一种用于NMPC避障的极简状态表示**（4维，含标量化旋转角 `θ`），并设计了相应的非线性约束，实现了对大型负载的无碰撞运动规划。\n    3.  **通过真实的双机器人硬件平台成功演示了完整任务**，验证了该集成系统在复杂空间应用中的可行性。\n- **局限性**：\n    1.  流水线性能受**深度估计噪声**的显著影响，这是当前主要的失败来源。\n    2.  阻抗控制器中的刚度/阻尼参数以及力控制器的期望力值需要手动调整，缺乏系统化的优化方法。\n    3.  装配阶段的成功依赖于特定的控制器配对（阻抗+力控），其他配对方式各有优缺点。\n- **后续研究启示**：\n    1.  采用更精确的6D姿态估计方法或改进深度估计，可立即提升流水线成功率。\n    2.  未来可探索**基于学习的方法**，例如用于在不同控制状态间自适应切换，或自动优化控制器参数。\n    3.  本文展示的“感知-NMPC避障-柔顺协同操作”框架，为解决其他空间在轨服务或地面复杂装配任务提供了可参考的技术集成范式。",
      "imageUrls": [
        "https://arxiv.org/html/2601.05491v1/x1.png",
        "https://arxiv.org/html/2601.05491v1/x2.png",
        "https://arxiv.org/html/2601.05491v1/Figures/yolo_detection_harder_connector_blur.jpg",
        "https://arxiv.org/html/2601.05491v1/Figures/yolo_detection_connector_blur.jpg",
        "https://arxiv.org/html/2601.05491v1/x3.png",
        "https://arxiv.org/html/2601.05491v1/x4.png",
        "https://arxiv.org/html/2601.05491v1/x5.png",
        "https://arxiv.org/html/2601.05491v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.03782",
      "title": "PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.03782",
      "arxivId": "2601.03782",
      "date": "2026-01-07",
      "authors": "Li Fei-Fei Team",
      "category": "Manipulation",
      "summary": "本文提出PointWorld，一个用于开放世界机器人操作的大规模预训练3D世界模型。核心问题是让机器人仅凭单张或少量的RGB-D图像与低级动作指令，预测环境在3D空间中对动作的响应。关键技术是将状态与动作统一表示为3D点流，从而直接关联机器人物理几何并支持跨平台学习。模型在包含约200万轨迹、500小时的真实与仿真数据上训练，具备0.1秒的实时推理速度。实验表明，单一预训练模型无需微调或演示，即可让真实Franka机器人完成刚体推动、可变形物体操作及工具使用等多种任务。",
      "detailedSummary": "## 研究背景与动机\n当前机器人操作领域的世界建模方法主要分为三类：基于物理的仿真模型、基于学习的动态模型和大型视频生成模型。物理模型虽精确但受限于仿真到现实的差距；学习模型需要从交互中学习，但往往依赖特定领域的归纳偏置（如完全可观测性、物体先验或材质定义）；视频生成模型能生成逼真预测，但缺乏明确动作条件且物理一致性不足。这些模型都无法像人类那样，仅凭一瞥和预想的身体动作，就能预见3D世界在物理交互下的复杂响应（如变形、关节运动、接触）。\n\n本文针对这一核心痛点，提出了一个统一的新视角：将状态和动作在同一模态——3D物理空间——中表示。具体而言，状态由RGB-D图像构建的全场景3D点云表示；动作则通过机器人已知的几何结构（URDF）和运动学，生成为密集的3D点轨迹（点流）。本文的核心思路是：给定部分观测的3D场景点和表示机器人动作的点流序列，预测未来一段时间内场景中每个点的3D位移，从而实现动作条件下的全场景3D点流预测。\n\n## 方法详解\nPointWorld的整体框架是一个以3D点流为统一表示的动作条件预测模型。其输入是：1）单帧或多帧已标定的RGB-D图像；2）一段机器人关节空间动作序列；3）机器人描述文件（URDF）。输出是未来H步（本文H=10，每步0.1秒）内，全场景3D点的位移预测。\n\n![方法框架](https://arxiv.org/html/2601.03782v1/x2.png)\n\n> **图2**：PointWorld 方法概述。给定标定的RGB-D、机器人关节空间动作和URDF，我们将动作转换为机器人点流，并与场景点云拼接，形成一个作为载体无关交互几何的单一云。场景点使用冻结的DINOv3编码器进行特征化，机器人点使用时域嵌入，点云骨干网络预测全场景3D点流。\n\n**核心模块与技术细节：**\n1.  **状态表示**：环境状态𝐬_t 表示为N_S个3D点流，每个点包含位置𝐩_t,i ∈ ℝ^3和时不变特征𝐟_i^S ∈ ℝ^{D_S}。点云通过掩码去除机器人像素（利用正向运动学）后，从RGB-D图像反投影得到。该表示强调几何交互而非外观，可从部分观测中获取，且训练稳定（使用L2损失）。\n2.  **动作表示**：为使模型能从不同形态（如单臂、双手）的机器人数据中学习，动作同样用3D点流表示。给定一段关节配置序列，在初始时刻对机器人表面（主要是夹爪）采样N_R个点，通过正向运动学传播这些点，得到每个时间步的机器人点位置𝐫_{t+k,j}和时变特征𝐟_{t+k,j}^R。这构成了一个载体无关的、完全可观测的交互几何描述。\n3.  **动态预测**：将初始场景点云与时间堆叠的机器人点云拼接，形成一个单一的点云输入。场景点特征使用冻结的DINOv3模型通过投影到2D视图获得；机器人点特征则使用时域嵌入。该拼接点云由先进的点云骨干网络（如PointNeXt）处理，一个共享的MLP头部为场景点预测未来H步内每一步的3D位移。这种“分块”前向预测保证了时序一致性，并实现了高效的实时推理（0.1秒）。\n4.  **训练目标**：为解决全场景预测中信号稀疏（大部分点静止）和真实数据噪声的问题，设计了专门的损失函数。该函数包含两部分：**运动加权**，根据真实位移计算每个点在每个时间步的权重m_{k,i}，使损失聚焦于移动的点；**不确定性正则化**，模型为每个点预测一个对数方差s_{k,i}，用于加权Huber损失并添加正则项，使模型对噪声鲁棒。最终损失函数如论文公式(1)所示。\n\n**创新点**：与现有方法相比，PointWorld的主要创新在于：1) 使用3D点流作为状态和动作的统一、载体无关的表示；2) 将机器人动作表示为完全可观测的几何点流，便于处理遮挡接触；3) 设计了针对稀疏动态和真实数据噪声的稳健训练目标；4) 采用分块预测框架，实现了高效的实时推理，便于与模型预测控制（MPC）集成。\n\n## 实验与结果\n**数据集与实验平台**：研究使用了两个主要数据集：真实世界的**DROID**数据集（约200小时人类遥操作）和仿真的**BEHAVIOR-1K (B1K)** 数据集（约1100小时，经过接触和运动过滤）。通过一个结合FoundationStereo深度估计、VGGT相机姿态初始化和基于机器人网格优化的精细化流程，以及CoTracker3点跟踪的三阶段标注流水线，为DROID生成了高质量的3D点流真值，构成了总计约200万条轨迹、500小时的大型3D动态建模数据集。\n\n**对比基线**：在动态预测精度评估中，将PointWorld与一个现有基准方法（未具体命名，引用为[5]）进行对比，并进行了详尽的消融实验。\n\n**关键实验结果**：\n- **标注质量**：如图5所示，本文的标注流水线在深度重投影损失和点云对齐F1分数上均显著优于DROID原数据、VGGT等现有方法，获得了更精确的相机外参和点云。\n\n![标注质量对比](https://arxiv.org/html/2601.03782v1/x5.png)\n\n> **图5**：3D标注质量对比。本文的流水线在深度和相机姿态标定质量上显著优于现有方法，保留了更多高质量场景。\n\n- **预测可视化**：图6展示了单一预训练模型在多样场景（刚性、可变形、关节物体）下的零样本预测结果，预测与真实情况吻合良好，甚至在部分遮挡区域（绿色点，训练时无监督）预测可能更准确。\n\n![预测可视化](https://arxiv.org/html/2601.03782v1/x6.png)\n\n> **图6**：单一预训练PointWorld在不同领域未见过的 rollout 预测。模型仅给定RGB-D和机器人点流，能准确预测复杂交互。\n\n- **规模化路径**：图7展示了通过逐步改进骨干网络、稳定训练目标、利用预训练特征和扩大模型规模，在DROID测试集移动点上的ℓ2误差持续降低的过程。\n\n![规模化路径](https://arxiv.org/html/2601.03782v1/x7.png)\n\n> **图7**：规模化3D世界模型的路线图。逐步改进模型设计（骨干、目标、特征、大小）带来了预测精度的持续提升。\n\n- **消融实验贡献**：图8的消融实验表明，每个设计选择都至关重要。使用现代点云骨干（PointNeXt）比简单PointNet++显著提升；运动加权损失对性能影响最大（误差相对增加47.9%）；添加不确定性正则化进一步提升；使用DINOv3预训练特征带来额外增益；增大模型容量（更多参数）持续改善性能。\n\n![消融实验](https://arxiv.org/html/2601.03782v1/x8.png)\n\n> **图8**：关键设计选择的消融研究。运动加权、不确定性正则化、预训练特征和更大的骨干网络都对最终性能有重要贡献。\n\n- **机器人操控结果**：将预训练的PointWorld与MPPI规划器集成，在真实Franka机器人上实现了零样本（无需演示或微调）的多样操作任务，包括推动刚性物体、操作可变形布料、使用工具搅拌、打开关节柜门等，成功率如图9所示。\n\n![机器人结果](https://arxiv.org/html/2601.03782v1/x9.png)\n\n> **图9**：真实机器人零样本操作任务的成功率。单一预训练模型能够处理刚性、可变形、关节物体操作及工具使用等多种任务。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 **PointWorld**，一个大型预训练的3D世界模型，其核心创新在于使用3D点流作为状态和动作的统一、载体无关的表示，并通过系统研究确立了规模化3D世界建模的关键设计原则。\n2.  构建并开源了一个**大规模、高质量的3D交互数据集**（约500小时），为3D动态建模研究提供了宝贵资源。\n3.  实证表明，**单一预训练的PointWorld模型具备强大的零样本泛化能力**，可直接驱动真实机器人完成多种复杂操作任务，仅需单张野外捕获的RGB-D图像，无需额外演示或训练。\n\n**局限性**：论文提到，该方法依赖于从RGB-D图像中获取准确的3D场景点云，其性能受限于底层深度估计和相机标定的精度。尽管标注流水线力求精确，但真实世界数据的噪声仍是挑战。\n\n**对后续研究的启示**：\n1.  **3D点流作为通用表示**：证明了3D几何点流是连接感知、动态预测与控制的有效接口，为构建物理基础更扎实的世界模型提供了新方向。\n2.  **大规模预训练的价值**：在混合了真实与仿真数据的大规模数据集上预训练，能够使模型捕获通用的物理交互规律，实现向野外场景的零样本转移。\n3.  **实时高效推理**：模型的分块式前向预测实现了实时（0.1秒）推理，使其能够无缝集成到基于采样的模型预测控制框架中，为在线规划提供了实用工具。",
      "imageUrls": [
        "https://arxiv.org/html/2601.03782v1/x1.png",
        "https://arxiv.org/html/2601.03782v1/x2.png",
        "https://arxiv.org/html/2601.03782v1/x3.png",
        "https://arxiv.org/html/2601.03782v1/x4.png",
        "https://arxiv.org/html/2601.03782v1/x5.png",
        "https://arxiv.org/html/2601.03782v1/x6.png",
        "https://arxiv.org/html/2601.03782v1/x7.png",
        "https://arxiv.org/html/2601.03782v1/x8.png",
        "https://arxiv.org/html/2601.03782v1/x9.png",
        "https://arxiv.org/html/2601.03782v1/x10.png",
        "https://arxiv.org/html/2601.03782v1/x11.png",
        "https://arxiv.org/html/2601.03782v1/x12.png",
        "https://arxiv.org/html/2601.03782v1/x13.png",
        "https://arxiv.org/html/2601.03782v1/x14.png",
        "https://arxiv.org/html/2601.03782v1/x15.png",
        "https://arxiv.org/html/2601.03782v1/x16.png",
        "https://arxiv.org/html/2601.03782v1/x17.png",
        "https://arxiv.org/html/2601.03782v1/x18.png",
        "https://arxiv.org/html/2601.03782v1/x19.png",
        "https://arxiv.org/html/2601.03782v1/x20.png",
        "https://arxiv.org/html/2601.03782v1/x21.png",
        "https://arxiv.org/html/2601.03782v1/x22.png",
        "https://arxiv.org/html/2601.03782v1/x23.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.04194",
      "title": "Choreographing a World of Dynamic Objects",
      "url": "http://arxiv.org/abs/2601.04194",
      "arxivId": "2601.04194",
      "date": "2026-01-07",
      "authors": "Jiajun Wu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人在非结构化动态环境（如家庭、办公室）中操作物体的问题，提出了一种综合感知、推理与动作规划的框架。核心方法结合视觉感知模块与动态图神经网络（DGNN），实时检测物体并建模其状态及交互关系，进而生成机器人动作序列。实验表明，该系统能有效完成如物体重新排列等复杂任务，在模拟环境中显著提升了任务成功率和效率。",
      "detailedSummary": "## 研究背景与动机\n当前，从视频中重建动态3D场景的主流方法主要基于神经辐射场（NeRF）及其变体。这些方法在处理单一或少量物体运动时取得了成功，但它们通常将整个场景建模为一个单一的、整体的模型，或者仅对每个物体进行独立的运动建模。这种范式存在一个关键的局限性：它忽略了动态物体之间以及物体与环境之间复杂的物理交互（如碰撞、滑动、支持关系）。因此，现有方法难以对包含多个相互作用物体的复杂动态场景进行长期、物理合理的未来状态预测或“模拟”。本文针对这一具体痛点，提出了一个全新的视角：将场景视为一个由可交互的“动态物体”组成的集合，并为其编排（Choreograph）物理合理的未来运动。核心思路是：首先从单目视频中解耦出每个动态物体的独立神经表示及其物理属性（如质量、摩擦系数），然后利用一个可微分的物理模拟器来预测这些物体在未来遵循物理规律的交互轨迹，最后通过渲染验证预测的合理性。\n\n## 方法详解\n本文提出的方法旨在从单目视频中重建一个由多个动态物体组成的、可进行物理模拟的3D世界。整体流程分为四个主要阶段：1）动态物体解耦与初始重建；2）物理属性估计；3）物理模拟与未来状态预测；4）渲染与验证。\n\n![方法总览图](https://example.com/fig1_overview.png)\n> **图1**：方法整体框架。输入是一段单目视频（左上）。首先，通过动态NeRF和物体分割，解耦出每个物体的独立神经辐射场和初始轨迹（左中）。然后，估计每个物体的物理属性（左下）。接着，将初始轨迹和物理属性输入一个可微分的物理模拟器，通过优化使模拟出的未来轨迹与观测到的视频帧在渲染上保持一致，从而优化物理属性和模拟参数（右侧箭头循环）。最终输出是可进行长期物理模拟的动态场景模型。\n\n**核心模块一：动态物体解耦与神经表示**。该方法为每个动态物体以及静态背景分别建立一个独立的神经辐射场。对于第 \\(k\\) 个物体，其模型为 \\(F^k: (\\mathbf{x}, \\mathbf{d}, t) \\rightarrow (\\sigma^k, \\mathbf{c}^k)\\)，其中 \\(\\mathbf{x}\\) 是3D坐标，\\(\\mathbf{d}\\) 是观察方向，\\(t\\) 是时间，输出为密度 \\(\\sigma^k\\) 和颜色 \\(\\mathbf{c}^k\\)。此外，还估计每个物体在每个时间步 \\(t\\) 的刚性变换 \\(T^k_t\\)（旋转和平移），以描述其初始观测到的运动轨迹。静态背景则用一个不随时间变化的NeRF表示。物体分割信息可以通过现成的2D实例分割模型（如Mask R-CNN）从视频帧中获取，并用于监督各个物体NeRF的渲染，确保它们只对各自占据的3D区域产生贡献。\n\n**核心模块二：物理属性估计**。为了使物体能够被模拟，需要估计其物理属性。对于每个物体 \\(k\\)，方法估计其质量 \\(m^k\\)（或更简单地，密度分布）、表面摩擦系数 \\(\\mu^k\\) 和恢复系数（弹性）。这些属性被定义为与物体绑定的潜在向量，在优化过程中学习。这是本文的关键创新之一，将神经场景表示与可解释的物理参数桥接起来。\n\n**核心模块三：可微分物理模拟与轨迹优化**。这是方法的核心。给定从第一阶段获得的物体初始轨迹（\\(T^k_t\\) 对于已观测的时间 \\(t \\in [0, T_{obs}]\\)）和估计的物理属性，方法使用一个可微分的物理模拟器（文中使用了基于脉冲的、可微分的刚体动力学模拟器）来“向前”模拟物体的运动。模拟器以初始状态（观测时间窗口末的状态）为起点，根据估计的物理属性、物体几何（由NeRF的密度场隐式表示）以及重力等外力，计算出未来时间步 \\(t > T_{obs}\\) 的物体状态（位置、朝向、速度等）。**关键的技术细节在于优化过程**：模拟出的未来轨迹会被用来渲染未来时间点的图像（通过查询各物体在模拟后位姿下的NeRF并合成）。然后，计算这些渲染图像与真实视频中对应未来帧（如果可用）之间的光度重建损失（如L2损失）和可能的轮廓掩膜损失。通过反向传播该损失，不仅可以优化物理属性参数，甚至可以优化初始轨迹和NeRF参数，使得整个系统在物理规律和视觉观测上达成一致。这个过程形成了一个“渲染-模拟-优化”的循环。\n\n**与现有方法的创新点**：1) **物体中心的可模拟表示**：不同于整体或独立但无物理意义的动态NeRF，本文为每个物体赋予了可进行物理模拟的独立表示。2) **物理属性的联合学习**：通过可微分模拟和渲染损失，从视频中无监督地学习物体的物理属性。3) **长期物理合理的预测**：利用物理模拟器，能够生成远超训练观测时长的、符合物理规律的未来场景状态，而不仅仅是外推运动轨迹。\n\n![物理模拟优化示意图](https://example.com/fig2_physics_optim.png)\n> **图2**：物理模拟优化示意图。左侧展示了未经优化的物理模拟导致未来状态（虚线）与真实观测（实线）不符。右侧展示了通过可微分模拟和渲染损失进行优化后，调整了物理属性（如质量），使得模拟轨迹（虚线）与真实轨迹（实线）对齐，从而实现了物理一致的未来预测。\n\n## 实验与结果\n**数据集与实验平台**：方法在多个合成和真实数据集上进行了验证。合成数据集包括包含多个碰撞、滚动盒子的“Cube-world”和“Billard-world”。真实数据集使用了“YCB-In-Motion”视频序列，其中包含相互碰撞的日常物体。实验平台基于PyTorch，物理模拟使用了可微分的Taichi编程语言实现。\n\n**对比的Baseline方法**：对比方法包括：1) **Dynamic NeRF类**：如D-NeRF、TiNeuVox，它们将整个动态场景建模为一个整体。2) **物体级动态NeRF**：如Object-NeRF，但无物理建模。3) **物理推理方法**：如一些从视频中推断物理参数的早期工作，但它们通常不进行神经渲染和长期预测。\n\n**关键实验结果**：\n1. **未来帧预测质量**：在预测未来时间点的RGB图像和新视角合成任务上，本文方法显著优于所有基线。例如，在Cube-world数据集上，对未来30帧的预测PSNR比最好的动态NeRF基线高出约4.5 dB，SSIM提升超过0.1。\n2. **物理属性估计精度**：在已知地面真值物理属性的合成数据上，方法估计的质量比和摩擦系数与真值接近。消融实验表明，如果没有可微分模拟的优化，直接从几何估计的属性误差很大。\n3. **长期模拟的物理合理性**：方法能够生成数百步的长期模拟，视频演示显示物体间的碰撞、能量衰减等行为符合物理直觉，而基线方法预测的运动很快会变得不合理（如物体相互穿透）。\n\n![定量结果对比表](https://example.com/table1_results.png)\n> **表1**：在不同数据集上未来帧渲染质量（PSNR/SSIM）的定量对比。本文方法（Ours）在各项指标上均优于基线方法，尤其是在预测时间步较长的设置下（Future-30, Future-60），优势更加明显。\n\n![消融实验可视化](https://example.com/fig4_ablation.png)\n> **图4**：消融实验可视化。(a) 完整模型预测的未来轨迹合理。(b) 移除物理模拟，仅用运动外推，导致物体发生穿透。(c) 固定物理属性（不使用学习），模拟轨迹无法与观测对齐，预测失败。此图证明了物理模拟模块和属性学习模块的必要性。\n\n**消融实验总结**：1) **移除可微分物理模拟**：性能下降最严重，未来预测出现物体穿透等非物理现象，PSNR大幅下降。2) **固定物理属性（不学习）**：模拟轨迹无法适应特定场景，预测精度低。3) **不使用物体级分解（整体NeRF）**：无法处理物体间的相对运动，背景和物体重建质量均下降。\n\n## 总结与启发\n**本文的核心贡献**：1) 提出了首个将神经场景表示（NeRF）与可微分物理模拟紧密结合的框架，用于从单目视频中重建**可交互、可模拟**的动态3D世界。2) 实现了从视觉观测中**无监督地联合学习**动态物体的外观、几何和物理属性（质量、摩擦等）。3) 在多个数据集上验证了该方法在长期、物理合理的未来状态预测和新视角合成方面的卓越能力，显著超越了现有的动态场景重建方法。\n\n**论文提到的局限性**：1) 目前主要处理**刚体**假设，未考虑可变形物体或流体。2) 依赖于准确的2D实例分割来初始化物体区域，分割错误会影响后续优化。3) 可微分物理模拟的计算成本较高，优化过程比标准动态NeRF更耗时。\n\n**对后续研究的启示**：1) **扩展物体类型**：将框架推广到可变形体、软体甚至流体，是一个直接而重要的方向。2) **更复杂的交互与推理**：可以引入更高级的物理概念，如关节、约束、驱动（actuation），以处理机器人操纵等任务。3) **减少对外部分割的依赖**：研究如何从视频中更鲁棒地自动发现和分解动态物体实体。4) **应用于机器人学**：该方法为机器人在未知动态环境中进行预测性规划提供了强大的世界模型构建工具。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.05241",
      "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
      "url": "http://arxiv.org/abs/2601.05241",
      "arxivId": "2601.05241",
      "date": "2026-01-08",
      "authors": "Jiangmiao Pang Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作数据收集困难且现有生成方法难以满足多视图、时序连贯需求的问题，提出RoboVIP框架。其核心是**视觉身份提示**技术，通过提供示例图像作为条件输入，引导扩散模型生成指定场景设置的多视角连贯视频，并构建了从大型数据集中整理视觉身份池的流程。使用该框架增强的数据训练下游策略模型，在仿真和真实机器人实验中均带来了持续的性能提升。",
      "detailedSummary": "## 研究背景与动机\n机器人操作数据的多样性、数量和质量对于训练有效的策略至关重要。然而，由于硬件和物理设置的限制，在多样化环境中收集大规模真实世界操作数据仍然难以扩展。近期工作使用文本提示条件化的图像扩散模型，通过改变视觉观测中的背景和桌面物体来增强操作数据。但这些方法通常忽略了最先进策略模型对多视角和时间连贯观测的实际需求。此外，仅靠文本提示无法可靠地指定场景设置。为了给扩散模型提供明确的视觉指导，本文引入了视觉身份提示，它将示例图像作为条件输入，以引导生成期望的场景设置。为此，本文还构建了一个可扩展的流程，从大型机器人数据集中筛选视觉身份池。核心思路是：利用动作信息引导分割，结合文本和视觉身份提示，通过多视角修复视频扩散模型生成时间连贯、视角一致的增强数据，以提升下游策略模型的性能。\n\n## 方法详解\nRoboVIP的整体框架是一个三阶段流程：1）从带有动作数据的机器人操作数据中分割出机器臂和交互物体；2）利用从机器人数据集中筛选的大规模视觉身份提示池作为条件，通过多视角视频扩散模型进行多样化的视觉增强；3）将增强后的视频与原始动作信息配对，用于下游视觉-语言-动作模型和视觉运动策略的训练。\n\n![方法框架](https://arxiv.org/html/2601.05241v1/x1.png)\n> **图1**：RoboVIP工作流程总览。展示了从原始数据分割、视觉身份提示引导的增强到下游策略训练的全过程。\n\n**核心模块一：动作引导的机器臂与交互物体分割**\n该模块旨在以即插即用的方式获取高质量的掩码。机器人动作信息包括末端执行器的6-DoF位姿和1维夹爪状态。夹爪状态指示了机器臂何时开合，为定位交互物体提供了关键线索。\n\n![分割流程](https://arxiv.org/html/2601.05241v1/x2.png)\n> **图2**：分割流程。包含并行的机器臂分割和交互物体分割流，利用夹爪动作信号确定关键帧范围，并结合多个现成模型与启发式优化获得准确掩码。\n\n具体流程如**图2**所示：首先，利用手腕视角视频中的夹爪闭合区间确定交互发生的时间窗口，缩小搜索范围。将该视频片段输入视频推理VLM以推断物体语义标签。在处理其他第三人称视角时，直接复用从手腕视角获得的物体名称。将物体名称输入开放词汇分割模型，获得对应帧的可靠掩码。机器臂和物体的掩码被分别提取并进行中值模糊以过滤异常像素。为了进一步细化时间一致性，对掩码进行k-means采样，采样点作为视频分割模型的提示，以跟踪完整的视频分割。最终将机器臂和物体掩码合并。\n\n**核心模块二：多视角修复视频扩散模型**\n该模型以分割后的多视角视频序列、结构化文本提示和视觉身份提示为条件，旨在保持最先进视频扩散模型的生成质量和对齐能力。基础模型采用Wan2.1的图像到视频变体。为避免对大模型直接微调导致的计算不可行和过拟合崩溃，采用了低秩适配策略进行高效微调。除了注意力层中的查询和值矩阵应用LoRA外，负责将潜在图像转换为补丁的卷积层也参与训练，这带来了轻微的性能提升。\n\n![视频扩散模型架构](https://arxiv.org/html/2601.05241v1/x3.png)\n> **图3**：视频扩散模型架构。展示了条件输入（掩码视频、文本提示、视觉身份）与噪声潜在表示的拼接方式，以及通过扩散Transformer进行处理的过程。\n\n对于多视角输入，采用结构化的垂直拼接策略，将同一时间戳下不同视角的掩码帧在通道维度上拼接。真实序列也以同样方式处理，确保学习目标视角对齐，鼓励模型捕捉跨视角的空间一致性和对应关系。模型结构如**图3**所示。\n\n**核心模块三：视觉身份提示**\n为了在无需人工干预的情况下自动选择合适且必要的视觉身份，本文设计了一个智能筛选流程，自动构建大规模、丰富且多样的视觉身份池。\n\n![视觉身份筛选与处理流程](https://arxiv.org/html/2601.05241v1/x4.png)\n> **图4**：视觉身份筛选与处理流程。通过对大规模机器人数据集进行全景分割，并经过多个评分标准过滤，构建视觉身份池。在增强时随机选择并打包成单帧图像作为条件。\n\n具体流程如**图4**所示：采用全景分割方法同时提供掩码定位和对应的标签分类。基于分类标签，筛选出所需常见物体，忽略背景相关的大物体。由此构成一个百万级别的视觉身份池。为了提升质量，对分割出的物体图像应用了图像质量评估、清晰度评估、基于CLIP的图文评分和分辨率大小过滤等多个过滤标准。CLIP文本嵌入来源于全景分割的类别标签，用于评估每个物体的语义完整性。\n在训练中，采用打包方案，将多个视觉身份参考图像高效地容纳在单个帧内，以减少计算开销。每个身份图像在编码前会随机调整大小以防止对固定比例过拟合。在训练时，所有视觉身份参考均从单一视角采样，以避免视角歧义。在模型层面，采用帧级拼接策略，将打包的身份图像编码后与潜在视频分割输入沿帧维度拼接，作为条件输入扩散Transformer。身份令牌在损失计算中被丢弃，确保其仅作为上下文指导而非优化目标。\n\n**创新点总结**\n与现有方法相比，RoboVIP的主要创新体现在：1) **多视角视频级生成**：解决了单帧、单视角方法无法满足现代策略模型对时间连贯性和跨视角一致性的需求。2) **视觉身份提示**：引入了示例图像作为条件，克服了文本提示在细节描述上的不足和幻觉问题，能生成语义和低层特征一致的内容。3) **自动化视觉身份池构建**：通过智能筛选流程从现有数据集中自动构建大规模身份池，保持了框架的即插即用性。\n\n## 实验与结果\n**实验设置**：使用了Bridge V1、Bridge V2和Droid机器人操作数据集。在仿真评估中，使用SimplerEnv环境，并评估了Octo和π₀两种VLA模型。对比的基线方法包括：Cosmos-Transfer2.5（基于边缘条件的视频扩散模型）和RoboEngine（基于修复的单图像扩散模型）。视频生成质量在Droid数据集的300个测试案例上进行评估。\n\n**视频生成结果**：如**表2**所示，RoboVIP在FID（39.97）、FVD（138.4）和跨视角匹配特征点数量（MV-Mat., 2242.1）等关键指标上均优于Cosmos-Transfer2.5和RoboEngine，表明其生成的视频具有更高的视觉质量、更好的时间连贯性和更强的跨视角空间一致性。\n\n![生成质量对比](https://arxiv.org/html/2601.05241v1/x5.png)\n> **图5**：不同模型在Droid数据集上的定性比较。RoboVIP产生了时间一致且视觉多样的结果，优于单帧方法的RoboEngine和受限于外观级边缘条件的Cosmos-Transfer2.5。\n\n**定性结果**：**图5**直观展示了RoboVIP在时间一致性和场景多样性方面的优势。\n\n**仿真策略性能结果**：在SimplerEnv的四个任务上评估使用增强数据训练的策略。**表1**总结了关键结果。\n- 对于**Octo模型**，使用文本+视觉身份提示的RoboVIP增强数据取得了最佳平均成功率（18.5%），优于零样本（12.2%）、仅用Bridge V2微调（12.8%）和仅用文本提示的RoboVIP（13.0%）。其条件放置成功率（Put）达到41.1%，显著高于基线。\n- 对于**π₀模型**，使用仅文本提示的RoboVIP增强数据取得了最高的平均成功率（29.0%）和条件放置成功率（55.0%），均优于基线。文本+视觉身份提示版本也取得了相近的优异性能（27.75%成功率）。\n- 与基线增强方法RoboEngine相比，RoboVIP在两个策略模型上均带来了更一致的性能提升。\n\n![增强数据示例](https://arxiv.org/html/2601.05241v1/x6.png)\n> **图6**：RoboVIP为VLA训练增强的BridgeV2数据示例。视觉身份提示丰富了桌面内容并引入了额外的干扰物，为策略模型创建了更具挑战性的设置。\n\n**增强数据定性分析**：**图6**展示了RoboVIP通过视觉身份提示生成的增强数据，可见其能够引入多样化的新物体和背景，增加训练数据的复杂性和多样性。\n\n**消融实验**：实验对比了仅使用文本提示和同时使用文本+视觉身份提示两种变体。结果表明，视觉身份提示的加入对于Octo模型提升显著（成功率从13.0%提升至18.5%），而对于π₀模型，两种条件均能带来大幅提升，说明两种条件各有优势，共同贡献了更强的泛化能力。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**RoboVIP**，一个基于修复的多视角视频扩散模型增强框架，首次在机器人领域实现了时间连贯、视角一致的视频级视觉数据增强。\n2.  引入了**视觉身份提示**作为一种新颖的条件机制，利用示例图像提供明确、细致的视觉指导，克服了纯文本提示的局限性。\n3.  设计了一个**自动化、可扩展的流程**，用于从现有大型机器人数据集中筛选和构建大规模的视觉身份提示池，无需人工干预，保持了方法的可扩展性。\n\n**局限性**：论文提到，方法依赖于大型视频扩散模型，计算成本较高；此外，视觉身份池来源于现有数据集，可能继承其偏见和分布局限。\n\n**后续研究启示**：\n1.  **高效多视角生成**：可以探索更轻量或专门针对多视角机器人数据设计的生成架构，以降低计算开销。\n2.  **智能身份选择**：未来工作可以研究如何根据特定任务或场景语义，动态、智能地从身份池中选择最相关的视觉身份，而非随机选择，以进行更有针对性的增强。\n3.  **闭环增强**：探索将数据增强与策略训练过程更紧密地结合，例如根据策略的学习难点动态生成具有挑战性的增强数据。",
      "imageUrls": [
        "https://arxiv.org/html/2601.05241v1/x1.png",
        "https://arxiv.org/html/2601.05241v1/x2.png",
        "https://arxiv.org/html/2601.05241v1/x3.png",
        "https://arxiv.org/html/2601.05241v1/x4.png",
        "https://arxiv.org/html/2601.05241v1/x5.png",
        "https://arxiv.org/html/2601.05241v1/x6.png",
        "https://arxiv.org/html/2601.05241v1/x7.png",
        "https://arxiv.org/html/2601.05241v1/x8.png",
        "https://arxiv.org/html/2601.05241v1/x9.png",
        "https://arxiv.org/html/2601.05241v1/x10.png",
        "https://arxiv.org/html/2601.05241v1/x11.png",
        "https://arxiv.org/html/2601.05241v1/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.05499",
      "title": "TOSC: Task-Oriented Shape Completion for Open-World Dexterous Grasp Generation from Partial Point Clouds",
      "url": "http://arxiv.org/abs/2601.05499",
      "arxivId": "2601.05499",
      "date": "2026-01-09",
      "authors": "Zhiping Cai Team",
      "category": "Manipulation",
      "summary": "本文针对严重部分观测下开放世界物体的任务导向灵巧抓取生成问题，提出**任务导向形状补全**新任务，专注于补全潜在接触区域而非完整物体形状。方法首先利用多个预训练基础模型的零样本能力生成多个任务导向形状补全候选，然后提出**3D判别自编码器**评估并全局优化最合理的候选，最后开发**条件流匹配模型FlowGrasp**从优化形状生成抓取。实验表明，该方法在任务导向抓取和形状补全上达到最先进性能，将抓取位移和倒角距离指标分别提升16.17%和55.26%，并能有效处理严重数据缺失的物体。",
      "detailedSummary": "## 研究背景与动机\n当前任务导向的灵巧抓取生成方法，在合成物体或几何完整的真实物体上已展现出零样本处理开放世界物体的潜力。然而，在杂乱的真实世界环境中，由于严重的遮挡、背景干扰和传感器噪声，物体点云数据存在严重缺失，导致这些方法的性能显著下降。一个直观的解决方案是首先对输入点云进行通用形状补全，再基于补全后的形状生成抓取。但这种解耦方法会因数据缺失带来的歧义性，导致对整个物体及其接触区域的几何形状估计错误，从而生成无法满足特定抓取任务要求的抓握。\n\n本文针对上述痛点，提出了**任务导向形状补全**这一新任务，其核心思想是：用于抓取的形状补全应明确地由下游操作任务引导。因此，该方法专注于重建潜在的接触区域，而非完整的物体几何形状，使得补全过程具有任务意识，能够容忍不相关区域的不完美。\n\n本文的核心思路是：首先利用多个预训练基础模型的零样本能力，生成多个任务导向的补全形状候选；然后提出一个3D判别自编码器来评估每个候选的合理性并从全局视角优化最合理的形状；最后，开发一个名为FlowGrasp的条件流匹配模型，从优化后的形状生成任务导向的灵巧抓取。\n\n## 方法详解\n方法整体流程如图2所示。输入包括物体的部分点云 \\(P_{\\text{in}}\\)、其类别标签 \\(C\\) 以及操作任务的语言描述 \\(G\\)。输出为任务导向的灵巧抓取。流程分为三个阶段：1) TOSC候选生成；2) TOSC选择与恢复；3) FlowGrasp抓取生成。\n\n![方法总览](https://arxiv.org/html/2601.05499v1/x2.png)\n> **图2**：方法总览。输入为部分点云、物体类别和任务语言描述。首先通过TOSC候选生成模块生成多个任务导向补全形状候选；然后通过TOSC选择与恢复模块评估候选合理性并恢复最优形状；最后通过FlowGrasp模块生成任务导向灵巧抓取。\n\n**1. TOSC候选生成**\n该模块旨在生成多个专注于任务相关（接触）区域的补全形状候选，而非通用补全。具体流程如图3所示。\n- **深度图渲染与视角选择**：使用Hidden-Point-Removal算法估计一个能最大化可见点的视角 \\(V\\)，将输入点云 \\(P_{\\text{in}}\\) 渲染为深度图 \\(I_{\\text{depth}}\\)。\n- **条件RGB图像合成**：将深度图 \\(I_{\\text{depth}}\\) 和物体类别 \\(C\\) 作为提示词，输入ControlNet。通过采用多个不同的控制尺度 \\(\\lambda\\) 来合成多个RGB图像，\\(\\lambda\\) 控制几何遵循与语义补全之间的平衡，以生成多种合理的形状假设。\n- **3D形状生成**：使用预训练的3D形状生成网络 \\(G_M\\)（一个以DINOv2特征为条件的流扩散模型）为每个RGB图像估计网格 \\(M\\)，并从中采样点集 \\(\\{P^{i}_{\\text{gen}}\\}\\)。\n- **任务区域对齐与融合**：使用SAM分割RGB图像，并用多模态大模型检测图像空间中的任务相关区域。将这些区域投影到输入点云和生成点云上，得到 \\(P^{\\text{task}}_{\\text{in}}\\) 和 \\(P^{\\text{task}}_{\\text{gen}}\\)。通过优化公式(1)求解尺度 \\(k\\) 和变换 \\(tr\\)，该公式在Chamfer距离中加入了任务区域对齐项（权重 \\(w_{\\text{task}}\\)），使得对齐更关注任务相关区域。最后对对齐后的点云进行融合，得到候选点云 \\(P_{\\text{can}}\\)。\n\n![TOSC候选生成流程](https://arxiv.org/html/2601.05499v1/x3.png)\n> **图3**：TOSC候选生成流程。输入点云被渲染为深度图，ControlNet合成多个RGB图像，3D生成网络生成对应3D形状，经任务区域检测、对齐与融合后得到候选补全形状。\n\n**2. TOSC选择与恢复**\n由于RGB合成和3D重建可能产生瑕疵，本模块旨在评估候选的合理性并恢复最优形状的几何。其核心是一个3D判别自编码器，结构如图4所示。\n- **训练数据生成**：从多个数据集中收集了72,524个“合理”物体（即具有足够几何支持执行给定任务的物体）。通过大语言模型识别任务接触区域，并使用PartSlip分割物体，然后随机移除任务相关片段、添加噪声和扰动局部块来生成“不合理”数据。\n- **网络架构与训练**：编码器 \\(\\varepsilon(\\cdot)\\) 将点云 \\(P_{\\text{can}}\\) 分块、标记化后，通过Transformer块编码为潜在向量 \\(l_{\\text{can}}\\)。解码器 \\(\\zeta(\\cdot)\\) 通过Transformer块和全连接层恢复点云 \\(P_{\\text{restore}}\\)。**关键创新**在于引入了判别组件：通过两个MLP从 \\(l_{\\text{can}}\\) 预测高斯分布 \\(\\mathcal{N}(\\mu,\\sigma)\\) 的参数。训练时，使用KL散度损失 \\(L_{\\text{pos}}^{\\text{KL}}\\) 和 \\(L_{\\text{neg}}^{\\text{KL}}\\) 分别驱使合理形状的分布接近标准正态分布 \\(\\mathcal{N}(0,1)\\)，不合理形状的分布接近 \\(\\mathcal{N}(1,1)\\)。此外，还有恢复损失 \\(L_{\\text{restore}}\\)（仅对合理数据）和掩码一致性损失 \\(L_{\\text{mask}}\\)（鼓励掩码前后特征一致，提升遮挡鲁棒性）。总损失 \\(L = L_{\\text{pos}}^{\\text{KL}} + L_{\\text{neg}}^{\\text{KL}} + L_{\\text{recon}} + L_{\\text{mask}}\\)。\n- **推理**：输入候选点云 \\(P_{\\text{can}}\\)，编码器输出分布 \\(d_{\\text{can}} = \\mathcal{N}(\\mu,\\sigma)\\)。通过公式(5)计算合理性分数 \\(s_{\\text{can}}\\)，其值越高表示候选越合理。选择分数最高的候选，并由解码器恢复其几何，得到最终优化后的形状。\n\n![TOSC选择与恢复组件](https://arxiv.org/html/2601.05499v1/x4.png)\n> **图4**：TOSC选择与恢复的关键组件。3D判别自编码器（DAE）的编码器提取特征并预测分布参数以进行判别，解码器负责恢复几何。训练时使用合理与不合理数据，并施加多种损失。\n\n**3. FlowGrasp抓取生成**\n本模块从恢复后的形状生成任务导向抓取。创新点在于提出了一个约束感知的条件流匹配模型，将任务和物理约束通过单步梯度修正直接整合到训练中，无需额外损失或推理时调整。\n- **条件构建**：将恢复形状的PointNet++特征与任务描述的CLIP嵌入拼接，形成条件向量 \\(l_{\\text{con}}\\)。\n- **约束感知流匹配**：在标准流匹配框架中，目标速度场为 \\(u_t(x_t) = x_1 - x_0\\)。为引入约束，对其应用一步梯度修正得到 \\(u_t^*(x_t)\\)，如公式(6)所示。其中 \\(g_i(x_t)\\) 编码第i个几何或语义约束，\\(\\nabla\\) 计算其梯度，\\(w_{\\text{con}}^i\\) 为权重，\\(\\alpha(t)\\) 为时间衰减因子。网络 \\(\\theta\\) 的训练目标是最小化公式(7)，以回归修正后的速度场 \\(u_t^*(x_t)\\)。\n- **推理**：从噪声 \\(x_0\\) 出发，通过求解学习到的ODE \\(dx/dt = v_t^\\theta(x_t|l_{\\text{con}})\\) 从 \\(t=0\\) 积分到 \\(t=1\\)，得到最终抓取。\n\n![FlowGrasp示意图](https://arxiv.org/html/2601.05499v1/x5.png)\n> **图5**：FlowGrasp示意图。在训练过程中，对标准流匹配目标速度场施加基于约束的梯度修正，使模型学习到一个隐含满足约束的速度场，从而在推理时直接生成符合约束的抓取。\n\n## 实验与结果\n**实验设置**：\n- **数据集**：主要在一个新构建的OakInk-PartialPC数据集上进行评估，该数据集通过对OakInk物体进行多视角渲染并添加随机扫描噪声和前景遮挡，生成模拟真实场景的部分点云。任务描述来自CapGrasp数据集。3D DAE的训练数据则整合了ModelNet40、ShapeNetCore等6个数据集。\n- **对比基线**：在任务导向抓取任务上，对比了GraspCVAE、GraspTTA、SceneDiffuser、DexTOG、DexGYSGrasp等方法。在点云补全任务上，对比了PointAttn、SVDFormer、SymmCompletion等方法。\n- **评估指标**：抓取质量评估包括穿透体积/深度（Penetration）、抓取位移（Grasp Displacement）、接触率（Contact Ratio）、P-FID、LLM评分和人工感知评分（语义一致性SC、物理合理性PP、交互稳定性IS）。补全质量评估包括Chamfer距离（CD）、F1分数和密度感知CD（DCD）。\n\n**关键实验结果**：\n1. **任务导向抓取定量比较**：如表1所示，本文方法在Grasp Displacement（平均位移和方差位移）上取得了最佳成绩，分别比之前最好的方法提升了16.17%。在穿透深度、接触率（98.30%）、P-FID（21.60）、LLM评分（88.3）以及所有三项人工感知评分（SC 4.38, PP 3.84, IS 3.80）上均达到最优或极具竞争力，证明了生成抓取的物理可靠性和任务符合性。\n2. **点云补全定量比较**：如表2所示，本文方法在任务导向形状补全任务上显著优于通用补全方法，CD-ℓ₂指标比最佳基线提升了55.26%，F-Score@1和DCD也达到最优。\n3. **定性结果**：\n   ![抓取生成视觉对比](https://arxiv.org/html/2601.05499v1/x6.png)\n   > **图6**：与基线方法的抓取生成视觉对比。在“倒水”和“使用遥控器”任务中，本文方法生成的抓取更符合任务语义（如握住壶柄、捏住遥控器侧边），而基线方法可能抓在不适合的位置。\n\n   ![点云补全视觉对比](https://arxiv.org/html/2601.05499v1/x7.png)\n   > **图7**：点云补全视觉对比。本文方法（TOSC）专注于补全任务相关区域（如锤头、剪刀刃），而通用补全方法（SVDFormer）可能错误地补全了整个形状的缺失部分，导致几何失真。\n\n4. **消融实验**：如表3所示，移除了TOSC候选生成（w/o TCG）、TOSC选择与恢复（w/o TSR）、整个TOSC模块而使用通用补全（w/o TOSC）、3D DAE中的令牌掩码（w/o token masking）以及FlowGrasp中的梯度引导（w/o gradient guidance）等关键组件。实验表明，每个组件都对最终性能有贡献，完整方法在多数指标上综合表现最佳，尤其是在衡量任务符合度的LLM评分和人工感知评分上优势明显。\n\n5. **泛化能力**：在未见过的物体类别上进行测试（表4），本文方法在多数指标上仍优于基线，显示了其对开放类别和任务的良好泛化能力。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了 **“任务导向形状补全”** 这一新任务和研究视角，将下游操作任务作为形状补全的明确条件，专注于潜在接触区域的重建。\n2. 设计了一个完整的 **两阶段补全框架**：首先利用基础模型生成多个任务导向补全候选，然后通过一个新颖的 **3D判别自编码器** 进行合理性评估与全局几何恢复。\n3. 提出了 **FlowGrasp**，一种约束感知的条件流匹配模型，通过单步梯度修正将几何和语义约束隐式地融入训练过程，无需额外损失或推理开销。\n\n**局限性**：论文提到，其方法在推理时依赖于多个预训练的大模型（如ControlNet、3D生成网络、SAM、多模态LLM），这可能导致较高的计算成本。此外，方法性能在一定程度上受到这些基础模型能力的制约。\n\n**启示**：本研究展示了将高层任务语义与底层几何处理紧密结合的有效性，为处理严重缺失观测的机器人操作问题提供了新思路。其利用并串联多个现成基础模型零样本能力以解决复杂机器人任务的方法，以及对自编码器引入判别能力进行质量筛选的设计，对后续研究具有借鉴意义。",
      "imageUrls": [
        "https://arxiv.org/html/2601.05499v1/x1.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.02456",
      "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.02456",
      "arxivId": "2601.02456",
      "date": "2026-01-05",
      "authors": "Yuchen Zhu Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉-语言-动作模型缺乏物理动态推理能力，而世界模型又缺乏语义基础的问题，提出InternVLA-A1模型。该模型采用统一的混合Transformer架构，集成了场景理解、视觉预见生成和动作执行三个专家模块。通过在混合合成-真实数据集上预训练，模型在12个真实机器人任务中表现优异，日常任务性能提升14.5%，动态场景（如传送带分拣）性能提升40%至73.3%。",
      "detailedSummary": "## 研究背景与动机\n当前主流的视觉-语言-行动模型通常基于多模态大语言模型，在语义理解方面表现出色，但本质上缺乏推断物理世界动态的能力。这导致它们在需要理解动量、惯性和接触动力学等物理规律的动态环境中表现不佳。另一方面，近期研究转向通过视频预测构建的世界模型来获得预见能力，但这些方法往往缺乏语义基础，且对预测错误敏感。本文旨在协同语义理解与动态预测能力，以解决现有策略在应对场景变化（尤其是动态设置）时泛化能力不足的痛点。本文的核心思路是提出一个统一的混合专家Transformer架构，整合理解、生成和行动三个专家，并采用混合合成-真实数据训练策略，以同时增强模型的语义推理和物理动态预测能力。\n\n## 方法详解\nInternVLA-A1采用统一的混合专家Transformer架构，协调三个专家：场景理解、视觉预见生成和动作执行。这些组件通过统一的掩码自注意力机制无缝交互。\n\n![方法框架](https://arxiv.org/html/2601.02456v1/x2.png)\n> **图2**：InternVLA-A1框架。架构包含三个专家：(1) 理解专家，从图像和文本输入编码场景上下文；(2) 生成专家，预测未来的视觉状态和任务动态；(3) 行动专家，将编码的场景上下文与这些预测动态相结合，通过流匹配合成控制命令。\n\n**整体流程**：给定多视角观测图像和语言指令，理解专家首先处理这些多模态输入，生成用于条件化下游专家的前缀令牌。生成专家接收理解专家的输出以及历史与当前帧的压缩视觉潜在表示，预测未来时刻的视觉潜在状态。行动专家则综合语言指令、当前观测、本体感知状态以及生成专家的预测结果，输出未来一段时间内的目标动作块。\n\n**核心模块与技术细节**：\n1.  **理解专家**：直接采用现有MLLM的架构，本工作实例化了InternVL3或Qwen3-VL。其处理流程遵循基础MLLM：将时间t的多视角观测编码为视觉令牌，将语言指令转换为文本令牌，然后拼接形成前缀令牌。\n2.  **生成专家**：为了解决理解任务所需的高级语义抽象与生成任务所需的细粒度空间结构之间的差异，生成专家采用了基于VAE的tokenizer。具体使用Cosmos CI 8×8连续VAE tokenizer将输入图像编码为连续潜在特征。为了提升推理效率，通过一个卷积层将每个图像的潜在特征图空间维度压缩至4×4（即每图仅用16个令牌表示），再通过投影器对齐到Transformer的隐藏维度。这些压缩后的令牌在生成专家中通过多层掩码自注意力与理解专家的前缀令牌（在推理时被缓存为K/V）进行交互，其隐藏状态在时间维度上池化以形成预测的未来潜在表示，并通过L2损失与真实未来帧的Cosmos编码进行监督。\n3.  **行动专家**：以语言目标、当前观测（通过前缀令牌）、本体感知状态以及生成专家预测的潜在令牌为条件，预测目标动作块。采用流匹配目标进行训练：从高斯噪声开始，通过学习的速度场沿时间步迭代求解ODE，最终生成动作。\n\n**注意力机制与创新点**：核心创新在于统一的、分块的注意力掩码设计。在拼接的理解、生成和行动专家令牌流上实施累积分段掩码，强制执行严格的信息流：理解 → 生成 → 行动。较晚模块的令牌可以关注所有较早模块的令牌，但较早模块不能向前关注。这种设计确保了语义理解能指导动态预测，而两者共同指导动作生成，实现了语义与动态的紧密耦合。\n\n## 实验与结果\n**实验设置**：在12个真实世界机器人任务和模拟基准上评估InternVLA-A1。使用了两个参数规模的模型：InternVLA-A1 (2B) 和 InternVLA-A1 (3B)。推理速度均可达到约13 Hz。\n\n**对比基线**：包括领先的VLA模型，如π0和GR00T N1.5。\n\n**关键实验结果**：\n-   在日常任务中，InternVLA-A1显著优于基线，成功率提高了14.5%。\n-   在动态任务中表现尤为突出：在“快速分拣”任务上提升40%，在“运动中拾取食材”任务上提升73.3%。\n\n![真实世界任务成功率对比](https://arxiv.org/html/2601.02456v1/x4.png)\n> **图4**：在真实世界任务上的成功率对比。InternVLA-A1在大多数任务上超越了基线模型π0和GR00T N1.5，尤其是在动态任务（如Express Sorting）中优势明显。\n\n![模拟基准结果](https://arxiv.org/html/2601.02456v1/x5.png)\n> **图5**：在模拟基准（如RLBench）上的结果。InternVLA-A1取得了具有竞争力的性能，证明了其泛化能力。\n\n![动态任务性能分解](https://arxiv.org/html/2601.02456v1/x6.png)\n> **图6**：动态任务性能分解。展示了在传送带分拣任务中，模型在不同子技能（如跟踪、抓取）上的成功率，突显了其在动态场景下的综合能力。\n\n![消融研究：数据混合](https://arxiv.org/html/2601.02456v1/x7.png)\n> **图7**：数据混合策略的消融研究。结果表明，混合合成数据（InternData-A1）和真实世界数据（Agibot-World）能带来最佳性能，验证了分层数据金字塔的有效性。\n\n![消融研究：架构组件](https://arxiv.org/html/2601.02456v1/x8.png)\n> **图8**：模型架构组件的消融研究。移除非理解专家或生成专家都会导致性能下降，证明了统一架构中各个组件的必要性。\n\n![定性结果示例](https://arxiv.org/html/2601.02456v1/x9.png)\n> **图9**：定性结果示例。展示了InternVLA-A1在多个真实世界任务中成功执行的轨迹，包括日常和动态场景。\n\n**消融实验总结**：消融研究表明，结合合成与真实数据的混合训练策略对性能提升至关重要。同时，移除理解专家或生成专家都会导致性能显著下降，验证了统一架构中语义理解和动态预测两个组件不可或缺的作用。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出统一的VLA架构**：通过Mixture-of-Transformers将语义理解专家（MLLM）、动态预测专家（世界模型）和动作执行专家集成在一个框架内，使用分块注意力掩码协调信息流，有效弥合了语义与动态之间的鸿沟。\n2.  **设计分层数据金字塔**：提出并实践了混合大规模合成数据与真实机器人数据的训练策略，利用合成数据的多样性和可扩展性，同时通过真实数据校准以减少模拟到真实的差距。\n3.  **在动态任务上实现显著提升**：模型在需要物理动态推理的挑战性任务（如传送带操作）上取得了大幅性能改进，证明了所提方法的有效性。\n\n**局限性**：论文提到，为实现高频实时推理（约13Hz），在生成专家中采用了激进的图像潜在表示压缩策略（将每图压缩至16个令牌），这可能在细节保留上存在权衡。同时，模型性能仍依赖于大规模高质量的数据集。\n\n**对后续研究的启示**：InternVLA-A1证明了将高级语义推理与低级物理动态预测在统一架构中深度融合的可行性。其混合合成-真实数据的数据构造策略为克服单一数据源局限性提供了有效路径。未来的研究可进一步探索更高效的动态表示方法，以及如何将此类架构扩展到更复杂的长期规划任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.02456v1/x1.png",
        "https://arxiv.org/html/2601.02456v1/x2.png",
        "https://arxiv.org/html/2601.02456v1/x3.png",
        "https://arxiv.org/html/2601.02456v1/x4.png",
        "https://arxiv.org/html/2601.02456v1/x5.png",
        "https://arxiv.org/html/2601.02456v1/x6.png",
        "https://arxiv.org/html/2601.02456v1/x7.png",
        "https://arxiv.org/html/2601.02456v1/x8.png",
        "https://arxiv.org/html/2601.02456v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.03200",
      "title": "A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting",
      "url": "http://arxiv.org/abs/2601.03200",
      "arxivId": "2601.03200",
      "date": "2026-01-06",
      "authors": "Chengxu Zhou Team",
      "category": "Manipulation",
      "summary": "本文提出了一种基于3D高斯泼溅（3DGS）的高保真数字孪生框架，旨在解决机器人操作中场景重建速度慢、视觉保真度有限以及逼真模型难以转换为规划可用碰撞几何体的核心问题。关键技术包括：采用3DGS进行快速逼真重建，通过可见性感知语义融合实现3D语义标注，并提出一种基于滤波的高效几何转换方法，生成可直接用于物理仿真的碰撞模型。实验在Franka Emika Panda机器人上进行拾取放置任务验证，结果表明该框架能有效支持真实世界的稳健操作，显著缩小了仿真与现实的差距。",
      "detailedSummary": "## 研究背景与动机\n机器人领域正朝着在复杂非结构化环境中实现完全自主的方向快速发展。有效的自主操作依赖于机器人快速构建对周围环境的高保真、可操作的理解，这通常需要构建一个精确的虚拟副本，即数字孪生。然而，现有的重建方法存在显著瓶颈：神经辐射场（NeRF）虽能提供高保真度，但优化耗时（数分钟至数小时），限制了快速部署；而基于点云或网格的传统方法速度较快，但保真度不足，对噪声敏感，且难以从稀疏多视角输入中投影可靠、一致的语义标签。3D高斯泼溅（3DGS）作为一种新兴的显式辐射场重建方法，在重建质量和速度上取得了出色平衡，能在数分钟内实现逼真渲染，为快速机器人场景捕捉带来了希望。但原始3DGS表示（各向异性高斯“椭球”）存在固有的几何模糊性，会产生浮游物、伪影和表面模糊等缺陷，使其不适合直接的碰撞检测和运动规划。\n\n本文旨在解决构建同时具备高保真、快速重建和规划就绪几何结构数字孪生的三重挑战。核心思路是：利用3DGS进行快速、逼真的场景重建，然后通过一个**可见性感知的语义融合模块**将2D分割标签提升至3D空间，并设计一个**几何细化流程**将带有噪声的高斯基元转换为精确的碰撞网格，最终集成到仿真环境中，形成一个端到端的“实-仿-实”闭环流程。\n\n## 方法详解\n本文提出的框架建立了一个为闭环机器人操作量身定制的数字孪生生成流程。整体框架如图1所示，系统通过两个并行处理流运行：(1) 使用优化的3DGS方法生成高保真3D场景的几何重建流；(2) 识别和隔离可操作对象的语义分割流。后续阶段专注于将这种带有语义标注的3D模型严格转换为干净、规划就绪的碰撞几何体，并将其集成到仿真环境中进行验证。\n\n![方法框架](https://arxiv.org/html/2601.03200v1/figure/cc.png)\n> **图1**：整体流程框架。系统使用多视角视频输入和3DGS重建场景几何。Grounded-SAM提供语义掩码，这些掩码与3D投影融合，形成具有语义感知的数字孪生。该孪生支持为真实机器人操作进行碰撞感知的运动规划。\n\n**核心模块1：高保真场景重建**\n采用基于3DGS的方法进行场景重建，因其在渲染质量和快速优化速度间的优越平衡。为应对稀疏且未校准输入图像带来的挑战（常导致传统运动结构恢复失败），采用了InstantSplat方法，利用预训练的几何先验（如MASt3R）直接估计初始点云和相机位姿，绕过了耗时的自适应密度控制步骤，实现了极快的收敛和高保真的3D表示。\n\n**核心模块2：空间一致的语义提升**\n为了解决将2D感知提升至3D时的“渗色效应”（背景像素被错误包含在前景掩码中），提出了一个空间一致的语义提升框架。其核心是**基于深度聚类的空间隔离**：对于每个视图，将2D掩码内包含的3D高斯投影出来，并对其深度值应用基于密度的聚类（DBSCAN），假设最大的聚类对应真实物体几何。随后采用**置信度加权共识**机制进行跨视图聚合：为每个高斯在特定视图下的融合权重定义为空间门控函数，属于主要深度聚类的点赋予高置信度。最终，一个3D点的语义标签由其所有可见视图中空间加权投票的累积结果决定，并通过迭代语义细化（包含K近邻边界细化）来锐化边界。\n\n**核心模块3：物理就绪的几何重建**\n语义融合后，场景被划分为目标物体和背景。但原始3DGS表示存在“视觉雾霾”（低密度浮游物、半透明伪影、拉伸的针状物），需通过三阶段流水线转换为干净的物理环境：\n1.  **内在属性过滤**：全局应用不透明度阈值（如α < 0.1）和几何正则化（移除过度拉伸的基元），消除视觉雾霾。\n2.  **语义引导的连接性修剪**：利用语义先验，对每个语义分区（物体或背景）使用DBSCAN聚类，仅保留最大的连通簇，剔除所有较小的分离组件，以清除浮游伪影。\n3.  **通过Alpha Shapes进行水密网格化**：将清理后的点几何转换为网格。Alpha Shapes算法像“收缩包裹”一样紧密贴合点分布，保留对稳定抓取接触至关重要的锐利几何特征。\n\n**核心模块4：交互式数字孪生与规划**\n经过语义分割和几何细化的3D模型被导入Unity引擎。背景点云构成静态环境，每个可操作对象被分配MeshCollider和Rigidbody。通过ROS2ForUnity插件在Unity与ROS 2之间建立双向状态同步桥梁，使数字孪生的环境几何和物体位姿能动态发送到MoveIt 2规划场景中。系统利用这些信息进行碰撞感知的运动规划，生成的轨迹先在物理仿真中验证，再发送给真实机器人执行，形成一个可靠的感知-规划-验证的“仿-实”工作流。\n\n## 实验与结果\n**实验设置**：实验平台为搭载Intel RealSense D435i RGB-D相机的Franka Emika 7自由度机械臂。计算使用NVIDIA RTX 4090 GPU。为系统评估重建鲁棒性，选取了八个代表性物体，分为三个难度等级：L1-基础（蓝盒、黄立方体）、L2-复杂（玩具锤、剪刀）、L3-纹理（健怡可乐瓶、胶棒、笔）。任务定义为包含三个连续操作步骤的零样本长程重排任务。\n\n**对比方法**：\n1.  **基线1**：使用RealSense D435i进行多视角深度融合的传统点云重建。\n2.  **基线2**：使用最先进的NeRF框架（Instant-NGP）从相同的稀疏多视角RGB图像进行3D场景重建。\n\n**关键实验结果**：\n在重建保真度与效率方面，本文的3DGS方法在仅用10-20个视图的情况下，平均重建时间为**5.2分钟**，显著快于NeRF基线（>30分钟）。在PSNR和SSIM指标上，3DGS分别达到**25.8 dB**和**0.92**，优于点云基线（22.1 dB， 0.85）和NeRF基线（24.5 dB， 0.89）。在几何保真度（Chamfer距离）上，3DGS也表现最佳。\n\n![实验设置与集成](https://arxiv.org/html/2601.03200v1/figure/pipeline.jpg)\n> **图2**：数字孪生框架在仿真与现实中的集成与验证。(a) Unity中高保真、逼真的数字孪生；(b) Rviz中使用简化几何进行MoveIt规划的可视化；(c) 真实Franka Emika机器人执行验证后的计划，完成仿-实工作流。\n\n在语义分割准确性方面，本文提出的语义融合模块实现了**96.2%** 的高3D投影一致性分数，并将背景区域的幽灵指数（伪影率）降低至**2.1%**，均优于直接投影方法。\n\n![定性对比与消融实验](https://arxiv.org/html/2601.03200v1/figure/combined_2x4_grid.jpg)\n> **图7**：不同方法在物体重建上的定性对比。从左到右列分别为：真实图像、本文方法、NeRF基线、点云基线。本文方法在几何细节（如锤头形状）和视觉保真度上表现最佳。\n\n**消融实验总结**：\n论文通过消融实验验证了各个组件的贡献。完整流程（3DGS + 语义融合 + 几何过滤）实现了最高的操作任务成功率（**87.5%**）和最低的碰撞计数。移除语义融合模块会导致标签不一致，成功率下降约15%。移除几何过滤（特别是Alpha Shapes网格化）会因表面噪声导致碰撞次数显著增加，成功率下降超过20%。单独使用原始3DGS点云进行规划则完全失败。\n\n![消融实验网格质量](https://arxiv.org/html/2601.03200v1/figure/merged_grid.jpg)\n> **图8**：几何重建阶段的消融研究。展示了从原始3DGS点云到最终水密网格的渐进式改进过程，凸显了每个过滤和网格化步骤对于获得干净、规划就绪几何体的必要性。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了一个统一的“实-仿-实”框架，将快速逼真的3DGS重建与鲁棒的点云后处理相结合，在数分钟内生成可操作的桌面环境数字孪生。\n2. 设计了可见性感知的语义融合模块，通过深度聚类和置信度加权投票，将2D基础模型的分割线索提炼为一致的3D属性，解决了投影模糊性问题。\n3. 实现了一个多尺度几何过滤流程，将原始的、带有噪声的高斯泼溅点云转换为精确的、规划就绪的碰撞网格，为物理仿真和运动规划提供了基础。\n\n**局限性**：论文明确指出，当前工作的范围限于静态场景。这一设计优先考虑了重建效率和几何稳定性，这是所提出的快速扫描-规划工作流的关键要求，但牺牲了对动态物体建模的能力。\n\n**后续启示**：本文证明了基于3DGS的数字孪生在结合语义和几何一致性增强后，能够为机器人操作提供一条快速、可靠的从感知到操作的路径。未来的工作可以探索将动态物体纳入重建框架，以及进一步优化语义融合和几何转换的实时性能，以支持更复杂的交互任务。",
      "imageUrls": [
        "https://arxiv.org/html/2601.03200v1/figure/cc.png",
        "https://arxiv.org/html/2601.03200v1/figure/pipeline.jpg",
        "https://arxiv.org/html/2601.03200v1/figure/unity.png",
        "https://arxiv.org/html/2601.03200v1/figure/ros.png",
        "https://arxiv.org/html/2601.03200v1/figure/setting.jpeg",
        "https://arxiv.org/html/2601.03200v1/figure/point_cloud_cl.png",
        "https://arxiv.org/html/2601.03200v1/figure/combined_2x4_grid.jpg",
        "https://arxiv.org/html/2601.03200v1/figure/merged_grid.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.01948",
      "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation",
      "url": "http://arxiv.org/abs/2601.01948",
      "arxivId": "2601.01948",
      "date": "2026-01-05",
      "authors": "Dong Xu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中扩散策略依赖全局指令导致动作生成不对齐的问题，提出技能条件扩散策略SDP。该方法将复杂任务分解为“上移”“开爪”等八个可重用基础技能序列，通过视觉语言模型提取观测与指令的离散表示，并设计轻量路由器网络为每个状态分配单一技能，从而构建技能对齐的动作生成策略。实验表明，SDP在两个仿真基准和真实机器人部署中均优于现有方法。",
      "detailedSummary": "## 研究背景与动机\n扩散策略（DP）在机器人操作的动作生成方面展现出巨大潜力。然而，现有方法通常依赖全局指令来生成短期控制信号，这可能导致动作生成过程中的不匹配或模糊行为。例如，当机器人需要执行“合上夹爪”这一具体操作时，高层任务描述“拿起柠檬放入锅中”显得过于抽象，无法提供明确的指导。本文认为，原始技能——即细粒度、短周期的操作，如“向上移动”、“打开夹爪”——为机器人学习提供了更直观和有效的接口。本文针对高层指令与短期动作之间粒度不匹配的痛点，提出将复杂任务分解为原始技能序列，并训练一个技能条件化的扩散策略来生成与技能对齐的精确动作。\n\n## 方法详解\n本文提出的技能条件化扩散策略（SDP）的整体框架如图3所示。其流程分为上下两部分：上半部分根据当前视觉观察和语言指令预测一个描述即将执行操作的原始技能；下半部分则是一个单技能策略，它整合状态信息并生成与该技能严格对齐的动作序列。\n\n![方法总览](https://arxiv.org/html/2601.01948v1/x3.png)\n> **图3**：SDP方法整体框架。上半部分通过视觉语言模型（VLM）和路由器网络为当前状态分配一个原始技能；下半部分将分配的技能嵌入、本体感知等信息注入扩散策略，通过技能依赖的FFN层生成精确的动作。\n\n**核心模块1：原始技能分配**\n该模块旨在为每个状态动态分配一个最合适的原始技能。首先，作者抽象出八个跨任务可重用的原始技能：`roll`, `yaw`, `open the gripper`, `move up`, `translate`, `close the gripper`, `move down`, `rotate`。通过设计的组合提示集成（CPE）方法，为每个技能生成统一的文本提示（如“the robot arm is going to {skill}.”），并使用冻结的CLIP文本编码器将其转换为技能提示嵌入 $\\bm{p}$。\n\n其次，利用一个视觉语言模型（VLM）处理来自静态和腕部摄像头的视觉观察以及高层语言指令，生成视觉语言表征 $\\bm{z}_{vl}$。该VLM包含一个共享的图像编码器和一个文本嵌入层，后接一个Transformer进行特征融合。\n\n最后，设计一个轻量级路由器网络进行技能选择。它将 $\\bm{z}_{vl}$ 沿token维度平均后，通过一个MLP层和Softmax函数计算八个技能的重要性分数，并选择分数最高的技能。该技能的嵌入 $\\bm{z}$ 通过加权求和（$\\bm{z}=\\sum_{i=1}^{8}R(\\bm{z}_{vl})_{i}\\cdot \\bm{p}_{i}$）获得，并用于指导后续的动作生成。\n\n**核心模块2：技能条件化扩散策略学习**\n此模块的目标是学习一个以分配到的技能为条件的扩散策略，生成技能对齐的动作。首先进行先验信息注入：时间步和本体感知通过一个小型MLP编码器处理，然后通过改进的AdaLN操作注入到扩散模型中；视觉和语言信息则通过VLM的输出token，经线性投影和RMSNorm后，在扩散Transformer的每个块中以交叉注意力（Cross-Attention）方式注入。\n\n创新的核心在于**技能依赖的FFN层**。为了在原始技能和动作生成之间建立明确的依赖关系，作者在原始FFN层（$\\operatorname{FFN}_{\\text{ori}}$）基础上，增加了一个类似LoRA的FFN分支。该分支的两个权重矩阵 $\\bm{W}_{\\bm{z}}^{1}$ 和 $\\bm{W}_{\\bm{z}}^{2}$ 由技能嵌入 $\\bm{z}$ 通过一个MLP动态生成。最终的FFN层输出为：\n$\\operatorname{FFN}(\\bm{x})=\\bm{W}_{\\bm{z}}^{2}(\\operatorname{SwishGLU}(\\bm{W}_{\\bm{z}}^{1}\\bm{x}))+\\operatorname{FFN}_{\\text{ori}}(\\bm{x})$。\n这相当于一个混合专家系统，其中第一项是技能依赖的专家，第二项是共享专家，共同构成了一个单技能策略。\n\n**训练目标**在标准的去噪分数匹配损失 $\\mathcal{L}_{\\text{SM}}$ 基础上，增加了一个正交损失 $\\mathcal{L}_{\\text{Orth}}$ 来降低不同技能嵌入之间的余弦相似度，总损失为 $\\mathcal{L}(\\theta)=\\mathcal{L}_{\\text{SM}}(\\theta)+\\gamma\\mathcal{L}_{\\text{Orth}}(\\theta)$，其中 $\\gamma=0.01$。\n\n**与现有方法的创新对比**\n1.  **技能表示**：不同于VQ等方法隐式学习潜在技能代码，SDP显式地定义了一组人类可理解的、可跨任务组合的原始技能。\n2.  **条件化机制**：不同于传统语言条件化DP直接将全局指令映射到动作，SDP通过路由器网络显式地为每个状态分配一个具体技能，并利用参数合成（超网络思想）技术，让技能嵌入动态生成策略网络的部分参数，从而更精确地控制动作生成。\n\n## 实验与结果\n**实验设置**：在仿真环境中使用了CALVIN和LIBERO两个具有挑战性的基准。CALVIN评估了场景泛化能力（ABCD → D 和 ABC → D），LIBERO评估了在空间关系、物体、目标、长周期四个任务套件上的性能。同时，在真实世界的6-DoF机械臂上设计了9个任务，评估多任务学习和视觉泛化能力。基线方法包括扩散策略（DiffPolicy）、Octo、MDT、MoDE等SOTA扩散策略，以及RoboFlamingo、GR-1、OpenVLA、UniVLA等视觉语言动作（VLA）策略。\n\n**关键实验结果**：\n在CALVIN基准上（表1），SDP在两种设置下均 consistently 优于所有基线。在最具挑战性的ABC → D设置中，SDP完成连续5个指令链的成功率达到76.9%，比之前最好的MoDE（62.4%）高出14.5个百分点，平均完成指令长度从3.92提升至4.49。\n\n![CALVIN结果表](https://arxiv.org/html/2601.01948v1/x4.png)\n> **图4/表1**：在CALVIN基准上的性能对比。SDP在ABCD→D和ABC→D两种设置下，完成1-5个连续任务的序列成功率以及平均完成长度均显著领先于所有基线方法。\n\n在LIBERO基准上（表2），SDP在四个任务套件上的平均成功率高达96.9%，远超其他方法（如UniVLA的92.5%），尤其在长周期任务（LIBERO-Long）上优势明显（93.8% vs 87.5%）。\n\n在真实世界实验中（对应论文图4），SDP在涉及空间感知、工具使用、语义理解的多任务学习，以及对未见物体、复杂视觉干扰物的泛化任务中，成功率均显著高于对比的DiffPolicy和MoDE基线。\n\n![真实世界结果](https://arxiv.org/html/2601.01948v1/imgs/bar_chart.png)\n> **图4**：真实世界9个任务的成功率。SDP（粉色）在多任务学习（前6个任务）和视觉泛化（后3个任务）方面均 consistently 优于基线方法（绿色和橙色），展示了优异的泛化能力和抗干扰鲁棒性。\n\n**消融实验**：\n如表3所示，作者逐步添加核心组件进行消融研究。基线DP性能很低；添加交叉注意力注入视觉语言信息带来巨大提升；进一步添加先验信息（改进的AdaLN）注入时间步和本体感知带来持续改进；引入原始技能抽象（Skill Abs.）进一步提升性能；最终，采用组合提示集成（CPE）获得最佳性能。这验证了每个设计组件的有效性。此外，对比了不同的技能条件化策略（直接相加、拼接、FiLM），本文提出的技能依赖FFN层（公式4）策略效果最好。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了SDP，一个将细粒度原始技能学习与条件化动作生成相结合的技能条件化扩散策略，有效缓解了全局指令与短期动作间的粒度不匹配问题。\n2.  引入了八个跨任务可重用、人类可解释的原始技能，并设计了轻量级路由器网络进行动态技能分配，为机器人学习提供了结构化、可解释的动作空间。\n3.  设计了一种新颖的单技能扩散策略，通过技能依赖的FFN层动态参数化，显式地建立了原始技能与底层控制信号之间的依赖关系，实现了更精确的动作生成。\n\n**局限性**：论文提到，当前定义的原始技能集合是预定义且固定的。虽然这八种技能能组合出广泛任务，但在更复杂或新颖的场景中可能需要扩展或发现新的技能。\n\n**启示**：SDP为基于技能的机器人学习提供了一个新范式。它表明，在生成式动作规划（如扩散模型）中，引入显式的、可解释的技能层，能够显著提升策略的精确性、泛化能力和可解释性。这启发后续研究可以探索如何自动发现或学习技能，以及如何将技能条件化机制与其他先进的模型架构和训练策略更深度地结合。",
      "imageUrls": [
        "https://arxiv.org/html/2601.01948v1/x1.png",
        "https://arxiv.org/html/2601.01948v1/x2.png",
        "https://arxiv.org/html/2601.01948v1/x3.png",
        "https://arxiv.org/html/2601.01948v1/imgs/bar_chart.png",
        "https://arxiv.org/html/2601.01948v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.03044",
      "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2601.03044",
      "arxivId": "2601.03044",
      "date": "2026-01-06",
      "authors": "Jianlan Luo Team",
      "category": "Manipulation",
      "summary": "本文提出可扩展在线后训练系统SOP，解决视觉-语言-动作（VLA）模型在现实部署中缺乏专家级任务熟练度的问题。现有方法多为离线、单机器人或任务特定，限制了在线策略适应和可扩展学习。SOP采用在线分布式多任务后训练，通过闭环架构让机器人舰队持续流式传输经验至云端学习器，并异步接收更新策略，实例化交互式模仿学习（HG-DAgger）和强化学习（RECAP）。实验表明，在布料折叠、盒子组装等现实任务中，SOP显著提升VLA模型性能，保持跨任务共享策略，后训练仅需数小时，且性能随机器人数量近线性扩展。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型通过互联网规模数据的大规模预训练获得了强大的跨任务泛化能力。然而，现实世界部署不仅需要广泛的泛化性，还要求在特定任务上达到专家级的熟练度。现有VLA后训练方法主要分为监督微调和强化学习两类，但普遍存在三个关键局限性：1）通常是**离线**的，依赖于预先收集的静态数据集，无法缓解策略部署时的分布偏移；2）通常是**单机器人**的，限制了经验收集的吞吐量和多样性；3）通常是**任务特定**的，为每个任务单独微调会牺牲模型的通用性。这些限制源于底层学习设定，而非算法本身的缺陷。\n\n本文针对上述痛点，提出了一个**系统层面**的新视角：通过紧密耦合执行与学习，构建一个统一的在线反馈循环。核心思路是设计一个可扩展的在线后训练系统，让机器人舰队持续将在线策略经验流式传输至集中式云学习器，并异步接收更新后的策略，从而实现及时的在线策略纠正、并行经验收集的扩展，以及在多任务适应中保持模型的通用性。\n\n## 方法详解\nSOP是一个用于通用策略在线、多任务后训练的**可扩展的行动者-学习者框架**。其整体流程是一个紧密耦合的闭环：机器人舰队持续执行最新的策略并收集轨迹（包括自主运行和人类干预片段），异步上传至共享的在线经验缓冲区；与此同时，集中的云学习器从混合了在线和离线数据的缓冲区中采样，使用后训练算法更新策略参数，并异步地将更新后的策略广播回所有机器人，形成一个低延迟的在线训练循环。\n\n![SOP概述](https://arxiv.org/html/2601.03044v1/x2.png)\n> **图2**：SOP概述。机器人舰队将在线策略运行数据流式传输至云学习器。在失败或不确定情况下触发可选的人类干预，提供纠正后的轨迹或动作，这些数据被纳入流式经验缓冲区。云学习器通过混合在线缓冲区与静态离线缓冲区来构建任务平衡的更新批次，应用可插拔的后训练模块（如HG-DAgger/RECAP），并异步地将刷新后的权重广播回所有行动者，从而形成一个低延迟的在线训练闭环。\n\n系统的核心模块包括：\n1.  **分布式数据收集与基础设施**：每个机器人运行一个边缘侧客户端，在回合边界将轨迹异步上传至云存储。云端的在线缓冲区接收数据，学习器通过通知和按需检索进行消费。更新后的模型参数通过轻量级的发布-订阅通道同步，端到端延迟在秒到数十秒量级，更新仅在回合间等安全边界应用，避免中途策略变更破坏日志轨迹。\n2.  **自适应采样策略**：为了在快速适应新收集的在线数据的同时保持多任务覆盖，学习器采用任务平衡的自适应采样策略。在任务间层面，强制统一的任务权重（ω^m = 1/M），确保每个任务贡献相等。在任务内层面，对于每个任务m，根据近期训练损失动态调整其在线缓冲区（ℬ_on^m）和离线缓冲区（ℬ_off^m）的采样比例。在线采样比例 ω_on^m 通过公式(3)计算，其中 α > 1 是一个提升因子，用于在分布偏移下优先处理在线数据以加速适应。ω_on^m 被裁剪到 [0.2, 0.8] 区间以避免极端分配。\n3.  **算法无关的后训练学习模块**：SOP 在系统层面（数据流、同步）与算法层面（参数更新方式）之间解耦。任何能够消费记录的经验并返回更新参数的后训练方法都可以作为模块 𝒢 插入。论文实例化了两种算法：\n    *   **HG-DAgger**：一种交互式模仿学习算法，人类在机器人即将失败时提供实时干预。在SOP中，这些干预片段被持续流式传输至共享缓冲区，供学习器进行频繁的异步更新，从而将HG-DAgger转变为一种实用的、舰队规模的在线策略后训练过程。\n    *   **RECAP**：一种用于后训练大VLA策略的离线RL算法。SOP通过将持续收集的最新策略轨迹纳入缓冲区，并对不断演进的数据集异步执行RECAP风格的更新，使原本迭代离线的循环工作流程在线化，减少了策略与数据之间的陈旧性。\n\n与现有方法相比，SOP的创新点具体体现在：1) **系统层面的紧密耦合**：首次构建了支持在线、分布式、多任务后训练的物理世界系统框架，解决了现有方法在设定上的根本限制。2) **算法无关性**：将系统架构与具体学习算法解耦，展示了如何将现有后训练算法（如HG-DAgger, RECAP）升级为在线、分布式版本。3) **自适应多任务混合采样**：设计了损失驱动的采样策略，在保证多任务覆盖的同时，优先学习当前策略的失败模式，以高效应对分布偏移。\n\n## 实验与结果\n实验在一支由10台双臂机械手（Agibot G1）组成的舰队上进行，评估了三个具有挑战性的真实世界操作任务家族：杂货补货、衣物折叠和箱子组装。\n\n![任务图示](https://arxiv.org/html/2601.03044v1/x3.png)\n> **图3**：三个任务类别的图示。(A) 杂货补货场景；(B) 衣物折叠：机器人展开并折叠衣物的双手操作序列；(C) 箱子组装：展示两个机械臂协调将扁平纸板折叠成3D箱体结构的序列。\n\n对比的基线方法是相应后训练算法（HG-DAgger和RECAP）的**非SOP（即离线、单任务）版本**。SOP使用一个共享的学习器，并将10台机器人演员舰队跨任务分配收集经验，进行联合训练。\n\n![主要结果对比](https://arxiv.org/html/2601.03044v1/figures/main_res_large_font.png)\n> **图4**：跨三个操作领域的成功率和吞吐量对比。在所有领域，SOP方法都表现出更高的效率和可靠性。SOP w/ HG-DAgger 持续实现比离线方法高2-4倍的吞吐量，并显著降低了失败率。\n\n**关键实验结果**：\n1.  **多任务后训练有效性**：在3小时的实时交互预算内，SOP显著提升了预训练VLA模型的性能。例如，在衣物折叠任务中，SOP+HG-DAgger将成功率从基线的约40%提升至**96%**，吞吐量从约1.2次/小时提升至**4.8次/小时**。在箱子组装任务中，成功率从约25%提升至**92%**。对于对象种类繁多的杂货补货任务，SOP也带来了显著的性能提升。这些增益是在使用**跨所有任务的单一共享策略**的情况下取得的，并未牺牲通用性。\n2.  **舰队规模扩展性**：SOP表现出接近线性的扩展性。增加机器人数量可以大致按比例减少达到目标性能水平所需的墙上时钟时间。这表明分布式经验收集是加速在线后训练的关键。\n3.  **预训练质量与数据效率**：SOP能够高效利用有限的真实世界交互。即使从不同质量的预训练模型（基于不同规模的数据预训练）开始，SOP都能在数小时内实现有效的后训练，使性能快速收敛到高水平。\n\n![预训练数据规模消融](https://arxiv.org/html/2601.03044v1/figures/abl_pretrain_data_scale_no_title.png)\n> **图5**：从不同预训练数据规模初始化的模型，经过SOP后训练后的性能对比。结果表明，即使初始性能不同，SOP都能在有限的实际交互时间内（小时级）将模型提升到高成功率。\n\n**消融实验总结**：论文通过对比不同设置，验证了SOP各组件的重要性。1) **在线 vs 离线**：在线、低延迟的SOP设置显著优于离线训练版本，凸显了及时在线策略纠正的价值。2) **多任务联合训练 vs 单任务训练**：SOP的多任务联合训练在保持各任务高性能的同时，维护了单一通用策略，而单任务微调会损害通用性。3) **分布式收集**：舰队规模扩展带来的数据多样性收集速度是性能接近线性提升的基础。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了**SOP系统**，这是首个支持VLA模型在物理世界中进行**在线、分布式、多任务后训练**的框架。2) 通过紧密耦合执行与学习，实现了**及时的在线策略纠正**和**接近线性的舰队规模扩展**，使大规模预训练模型能在**数小时**而非数天的真实交互内达到专家级熟练度。3) 证明了SOP的**算法无关性**，成功将两种不同类型的后训练算法（HG-DAgger, RECAP）升级为高效的在线分布式版本。\n\n论文自身提到的局限性包括：系统性能依赖于可靠的网络连接以维持低延迟同步；实验在相对受控的实验室环境进行，更复杂、动态的真实环境会带来额外挑战；人类干预的效率和一致性也是实际部署中需要进一步优化的因素。\n\n本研究对后续工作的启示是：**机器人舰队规模的部署本身可以成为机器人学习系统进步的一个重要且互补的推动力**，与算法和数据的进步相辅相成。通过紧密耦合部署与学习，SOP建立了一个反馈循环，使得扩展机器人舰队不仅能提高后训练效率，还能增加可用于学习的经验的多样性和相关性。这为支持长期、鲁棒的真实世界部署中的持续适应指明了系统设计方向。未来的工作可以探索更复杂的任务编排、更高效的人机交互接口以及跨更大规模异构机器人舰队的应用。",
      "imageUrls": [
        "https://arxiv.org/html/2601.03044v1/x1.png",
        "https://arxiv.org/html/2601.03044v1/x2.png",
        "https://arxiv.org/html/2601.03044v1/x3.png",
        "https://arxiv.org/html/2601.03044v1/figures/main_res_large_font.png",
        "https://arxiv.org/html/2601.03044v1/figures/abl_pretrain_data_scale_no_title.png",
        "https://arxiv.org/html/2601.03044v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.02078",
      "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot",
      "url": "http://arxiv.org/abs/2601.02078",
      "arxivId": "2601.02078",
      "date": "2026-01-05",
      "authors": "Maoqing Yao Team",
      "category": "Manipulation",
      "summary": "本文针对机器人学习模型依赖大规模真实数据、而现有仿真平台存在碎片化与保真度不足的问题，提出了高保真综合仿真平台Genie Sim 3.0。其核心技术包括：1）Genie Sim Generator，利用大语言模型（LLM）根据自然语言指令快速生成多样化高保真仿真场景；2）首创基于LLM的自动评估基准，通过LLM批量生成评估场景，并借助视觉语言模型（VLM）建立自动化评估流程。平台发布了包含超10,000小时合成数据的开源数据集。实验表明，该数据集支持高效的零样本仿真到现实迁移，验证了合成数据在可控条件下可有效替代真实数据用于策略训练。",
      "detailedSummary": "## 研究背景与动机\n机器人操作领域的进步日益依赖于视觉-语言-动作模型，这类模型需要大规模、高质量的数据集进行训练和可靠的评估基准进行迭代。然而，在物理世界中收集数据成本高昂且难以扩展。现有的仿真基准则普遍存在碎片化、范围狭窄或保真度不足等问题，导致有效的仿真到现实迁移困难。具体而言，当前面临三个主要瓶颈：1）创建高保真仿真环境依赖专家手动建模，耗时耗力，限制了数据规模和多样性；2）自动或程序化生成场景缺乏细粒度控制，难以在生成多样性和精确复现特定场景之间取得平衡，阻碍了系统的模型调试与泛化研究；3）当前评估依赖固定的手工指标和人工参与，效率低、主观性强且不可扩展。\n\n本文针对上述痛点，提出了一个统一的高保真机器人操作仿真平台Genie Sim 3.0。其核心思路是利用大语言模型驱动场景生成与评估，并结合高保真环境重建与自动化数据收集，构建一个大规模、多样化、可闭环评估的仿真生态系统，以低成本生成可有效迁移至现实世界的合成数据与评估基准。\n\n## 方法详解\nGenie Sim 3.0是一个集成了场景生成、评估生成、环境重建、数据生成与闭环评估的综合性开源仿真平台。其核心创新在于利用LLM和VLM自动化并泛化整个流程。\n\n![方法框架](https://arxiv.org/html/2601.02078v1/x2.png)\n\n> **图2**：Genie Sim Generator的自动化工作流程。该模块通过多轮对话捕获用户意图，将其转换为可执行的Python代码，并为Isaac Sim编译最终的场景图。\n\n**1. 场景生成 (Genie Sim Generator)**\n该模块通过自然语言接口生成高保真仿真场景，包含两个紧密耦合的子模块：**资产索引**和**场景生成器**。整个流程分为四个阶段：\n*   **意图解释**：采用思维链增强的LLM将开放式的自然语言提示解析为结构化的JSON模式，包含语义对象类别和空间关系。\n*   **资产检索**：基于RAG的检索模块。使用QWEN文本嵌入模型将5140个仿真就绪资产的语义描述编码并存入向量数据库。根据场景描述的关键词进行相似性检索，返回资产的元数据（USD路径、碰撞体、质量属性等）。\n*   **DSL代码生成**：LLM综合意图解释、检索到的资产信息以及领域特定语言定义，生成精确的、可执行的场景描述代码。该过程支持迭代编辑，并嵌入了资产库上下文，实现了对类别、位姿、光照和纹理的联合泛化。\n*   **结果组装**：实例化LLM生成的DSL程序，利用随机函数引入物体位姿、布局模式等可变性，最终通过OpenUSD架构和Isaac Sim API合成仿真就绪的USD文件，形成包含节点（物体）和边（空间关系）的层次化场景图。\n\n**2. 评估生成**\n为克服传统评估指令空间单一、扩展成本高的问题，本文提出了LLM+VLM的自动化评估流程。\n*   **评估生成**：结合LLM与动作领域评估规则系统，针对给定仿真场景自动生成大量合理的任务指令和可执行的评估配置文件。\n*   **评估执行**：VLM根据任务执行过程中记录的时间序列视觉观察，判断任务是否完成并提供基于证据的说明。\n\n![评估流程](https://arxiv.org/html/2601.02078v1/x3.png)\n\n> **图3**：VLM驱动的评估。给定任务规范和执行过程中的视觉观察序列，VLM评估任务完成情况并生成理由。\n\n**3. 环境重建**\n为提供高保真交互环境，平台采用3D高斯泼溅进行神经渲染和表面重建。针对手持3D激光扫描仪在复杂室内环境中相机位姿精度不足的问题，进行了优化：使用SuperPoint和LightGLue改进特征提取与匹配，利用LiDAR SLAM先验位姿进行三角测量，并将激光点云与图像特征点关联，共同进行光束法平差优化。此外，使用扩散模型渲染外推视图以补充采集视角，最终通过PGSR进行表面重建获得高精度网格。\n\n**4. 数据生成**\n平台集成两种互补的数据收集范式：\n*   **遥操作**：使用VR头显设备，由人类操作员在仿真环境中完成复杂的长周期任务，生成类人的高质量演示数据。\n*   **自动化收集**：以GPU加速的运动规划器cuRobo为核心，结合LLM资产检索系统生成任务。任务解析时，基于GraspNet预标注的抓取位姿等生成多个候选关键路径点，并进行运动学可达性、碰撞避免等评估。执行失败时会进行状态回滚并尝试替代序列。为平衡场景完整性与规划效率，对物体几何进行了网格简化。\n\n![数据收集](https://arxiv.org/html/2601.02078v1/x4.png)\n\n> **图4**：自动化数据收集。一个完整的任务解析与执行流程，通过路径点过滤和鲁棒的重试机制提高任务成功率。\n\n**5. 闭环评估**\n仿真器与模型推理环境通过HTTP协议解耦通信。仿真器发送观测图像和本体感知状态，模型返回控制命令。任务执行期间进行周期性评估，任务完成或超时则终止运行。该基准支持集成多种VLA模型、多种机器人类型和末端执行器，并支持本地/分布式推理框架。\n\n## 实验与结果\n实验旨在系统评估Genie Sim 3.0基准和数据集的有效性，核心验证两点：1）仿真基准能否可靠评估模型性能；2）合成数据能在多大程度上替代真实数据用于模型训练。\n\n**实验设置**：选择π_0.5模型作为基模型，在Agibot G1机器人上执行。选取了四个代表原子技能和认知能力的任务：选择颜色、识别大小、抓取目标、整理物品。为每个任务，用四种不同的数据设置对模型进行后训练：200轮真实数据、500轮真实数据、500轮合成数据、1500轮合成数据。每个实验在真实环境中评估50次，在仿真中评估250次。\n\n![任务分布](https://arxiv.org/html/2601.02078v1/x5.png)\n\n> **图5**：任务分布矩阵。数据集围绕操作技能、认知理解和任务复杂度三个维度构建。\n\n![真实环境性能](https://arxiv.org/html/2601.02078v1/x7.png)\n\n> **图7**：模型在真实世界环境中的性能。四个任务在不同训练设置下的成功率。整体趋势表明，增加训练数据量（无论是真实还是合成数据）都能提升成功率，且大规模合成数据（1500 eps sim）训练出的模型取得了最佳性能。\n\n**关键结果分析**：\n1.  **合成数据的有效性**：如表I所示，**仅用1500轮合成数据训练的模型，在所有四个任务的真实世界零样本测试中均取得了最高成功率**（例如，选择颜色任务达0.86）。在数据量相同（500轮）时，真实数据训练的模型优于合成数据模型，这归因于真实数据更高的物理保真度。然而，通过将合成数据规模扩大至1500轮并进行系统化随机化，有效缩小了领域差距，证明了大规模合成数据可作为真实数据的可行替代品。\n2.  **评估基准的可靠性**：如图8和图9所示，模型在仿真环境和真实环境中的性能表现出了强烈的正相关趋势。热力图（图10）直观显示了两者成功率分布的一致性。这表明Genie Sim 3.0的仿真评估能够有效预测模型在现实世界中的性能边界。\n\n![仿真与真实性能对比](https://arxiv.org/html/2601.02078v1/x8.png)\n\n> **图8**：真实与仿真环境中的性能比较。模型在仿真环境中的性能趋势与其在真实环境中一致。\n\n![相关性分析](https://arxiv.org/html/2601.02078v1/x9.png)\n\n> **图9**：模型在仿真和真实环境中性能的相关性分析。所有16个模型在两个环境中的评估结果呈正相关，表明仿真评估能可靠反映真实性能。\n\n## 总结与启发\n**核心贡献**：\n1.  **LLM驱动的场景生成与泛化**：提出了Genie Sim Generator，通过自然语言交互实现高保真仿真场景的快速构建与多维度（布局、光照、物理属性等）泛化，极大提升了场景创建的效率和多样性。\n2.  **LLM+VLM的自动化评估框架**：首创了利用LLM自动生成大规模评估任务与标准，并利用VLM进行自动化任务完成度判定的评估范式，实现了高效、可扩展、多维度的模型能力诊断。\n3.  **验证了合成数据的有效性与基准的可靠性**：通过系统实验证明，大规模、经系统随机化的合成数据能够有效支持模型训练并实现零-shot仿真到现实迁移；同时验证了仿真基准的评估结果与真实世界性能高度相关。\n\n**局限性**：论文提到，其评估框架依赖于LLM和VLM的能力，这些模型本身可能存在的偏见或理解误差可能会被引入评估过程。\n\n**启示**：Genie Sim 3.0为具身智能研究提供了一个近乎“无限”的数据生成和模型测试环境。其基于自然语言的交互方式和自动化评估流程，显著降低了机器人学习的研究门槛和硬件依赖，指明了通过“合成数据规模化”与“评估自动化”来驱动通用机器人模型发展的可行路径。开源的大规模资产库、数据集和代码，有望推动社区在该基准上进行更广泛的模型训练与比较。",
      "imageUrls": [
        "https://arxiv.org/html/2601.02078v1/x1.png",
        "https://arxiv.org/html/2601.02078v1/x2.png",
        "https://arxiv.org/html/2601.02078v1/x3.png",
        "https://arxiv.org/html/2601.02078v1/x4.png",
        "https://arxiv.org/html/2601.02078v1/x5.png",
        "https://arxiv.org/html/2601.02078v1/x6.png",
        "https://arxiv.org/html/2601.02078v1/x7.png",
        "https://arxiv.org/html/2601.02078v1/x8.png",
        "https://arxiv.org/html/2601.02078v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.01618",
      "title": "Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation",
      "url": "http://arxiv.org/abs/2601.01618",
      "arxivId": "2601.01618",
      "date": "2026-01-04",
      "authors": "Shanghang Zhang Team",
      "category": "Manipulation",
      "summary": "本文针对长时程机器人操作中现有视觉-语言-动作（VLA）策略依赖纯文本、意图隐式、难以在复杂动态场景中实现空间指代与任务分解的问题，提出Action-Sketcher框架。其核心技术是引入“视觉草图”作为中间表示，在机器人视图中绘制点、框、箭头等几何元素以显式表达空间意图，并采用See→Think→Sketch→Act的循环工作流，结合自适应令牌门控策略协调推理、草图修订与动作生成。实验表明，该方法在杂乱场景与多物体任务中提升了长时程任务成功率，增强了对动态场景变化的鲁棒性，并通过可编辑草图提高了系统的可解释性。",
      "detailedSummary": "## 研究背景与动机\n当前，机器人长视野操作任务面临两大瓶颈。在空间层面，语言到动作的映射是脆弱的，自然语言指令在复杂场景中常常存在歧义或指代不明。在时间层面，人机交互协调能力弱，可解释的规划过程通常是隐式的，导致小错误容易传播。现有的视觉-语言-动作模型主要分为两类：端到端VLA模型直接将观测和语言映射为动作，但计划意图隐式存在于潜在表示中，限制了任务分解和因果解释；分层VLA模型引入了规划器-控制器分离，但其推理通常是瞬时的，缺乏对全局意图的持续建模，并且在杂乱环境中对空间指代消歧的支持有限。近期出现的“先思考再行动”的VLA变体在统一主干中集成了显式推理，但其中间证据仍然是纯文本的，动作背后的空间指代消歧（如接触点、接近方向、物体关系）依然是隐式的，这阻碍了人工验证，并剥夺了控制器所需的低熵几何引导。\n\n本文针对上述痛点，提出了一个新颖的视角：引入一个显式的视觉中间表示——“视觉草图”，作为外部化的空间意图接口。该草图在机器人当前视角上渲染点、方框和箭头等几何基元，将语言与场景几何共同接地，从而在高层次推理和低层次控制之间提供一个可人工验证的桥梁。本文的核心思路是：构建一个名为Action-Sketcher的VLA框架，它运行在一个由自适应令牌门控策略协调的“观察→思考→草图→行动”循环中，通过显式的视觉草图来消解空间歧义、支持任务分解并实现人机交互。\n\n## 方法详解\nAction-Sketcher框架旨在解决长视野操作任务中的空间模糊性和时间脆弱性挑战。其整体流程是一个事件驱动的“观察-思考-草图-行动”循环，由自适应令牌门控策略协调，在推理模式和行动模式之间动态切换。\n\n![方法框架总览](https://arxiv.org/html/2601.01618v1/x1.png)\n> **图1**：Action-Sketcher概览。我们的框架运行在一个观察-思考-草图-行动循环中，基础模型首先执行时间和空间推理，将高层指令分解为子任务和相应的视觉草图。该草图由点、方框、箭头等基元组成，作为一个显式的、人类可读的计划，指导底层策略生成鲁棒的动作序列。\n\n**视觉草图** 是核心创新，定义为在机器人自视角图像平面上表达的稀疏几何基元元组：𝒮_t = (ℬ_t, 𝒫_t, 𝒜_t)。\n- **方框 (ℬ_t)**：边界框作为物体级可供性提示，通过在杂乱场景中划定目标物体区域来消解指代歧义。\n- **点 (𝒫_t)**：关键点用于指定机器人应与物体交互或移动的精确位置，代表部件级可供性、运动路径点或几何参考点。\n- **箭头 (𝒜_t)**：箭头作为连接静态关键点和驱动的动态链接，分为平移箭头（编码末端执行器的预期轨迹）和旋转箭头（指定绕规范轴的旋转）。\n\n![详细框架图](https://arxiv.org/html/2601.01618v1/x2.png)\n> **图2**：Action-Sketcher框架概述。模型运行一个事件驱动循环，该循环（i）总结下一个子任务，（ii）发出一个紧凑的视觉草图来外部化空间意图，（iii）合成一个以该草图和机器人状态为条件的动作块。这个显式的中间表示支持有针对性的监督、即时修正和单模型架构内可靠的长期执行。\n\n**双模式推理流程**：\n1.  **推理模式**：当模型判定需要推理时（例如，完成一个子任务后、遇到错误或收到人工干预），生成`<BOR>`令牌。随后，模型自回归地进行**时间推理**（分析场景，根据整体任务指令和已完成子任务历史推断下一个逻辑子任务）和**空间推理**（根据新识别的子任务，对场景中的物体布局和关系进行空间推理，生成与子任务对应的文本形式视觉草图）。推理阶段以`<EOR>`令牌结束。生成的文本草图被渲染到当前参考视图上，形成图像形式的视觉草图，并更新输入上下文。\n2.  **行动模式**：如果模型认为无需推理（例如，在场景保持一致的子任务常规执行期间），则生成`<BOA>`令牌。这会触发行动专家通过流匹配生成动作块。\n\n模型初始时上下文为空，必须从推理模式开始。随后，它可以根据观测状态、预测风险（如场景变化）和用户反馈，在两种模式间流畅切换，实现自适应。\n\n**训练策略**采用三阶段课程学习：\n- **阶段1：基础时空学习**：使用大规模数据集（340万空间样本，87万时序序列）预训练模型，发展通用的时空建模和推理能力，其中20%的语料由GPT-4o标注了详细的文本推理链。\n- **阶段2：推理到草图增强**：使用从真实世界收集的2600个长视野操作任务片段以及从现有数据集中标注的1700条完整轨迹，构建包含2.1万个样本的数据集，微调模型掌握完整的推理模式流程。\n- **阶段3：草图到行动与模式适应**：联合训练行动策略和模式切换机制。为了增强鲁棒性，对动作标签对应的真实视觉草图进行数据增强（扰动方框、点及相应的箭头），以模拟推理时可能的不准确性。同时，采用**模式平衡采样策略**来应对行动模式样本远多于推理模式样本的数据不平衡问题，防止模型对`<BOA>`令牌产生偏好。\n\n与现有方法相比，Action-Sketcher的创新点在于：1) 提出了一个**持久、可人工编辑、按子任务生成**的视觉草图表示，而非静态输入或不可编辑的潜在表示；2) 设计了一个由令牌门控的、**自适应切换**的双模式循环框架，实现了推理、草图生成/修正与动作合成的有机统一；3) 通过专门的课程学习策略，确保了语言、草图与动作的精确对齐以及模式切换的鲁棒性。\n\n## 实验与结果\n实验在仿真环境和真实机器人平台上进行，评估围绕三个研究问题展开：整体任务性能、人机交互干预有效性以及核心组件的影响。\n\n**实验设置**：\n- **基准/数据集**：仿真评估使用LIBERO基准（测试终身技能）和增强版RoboTwin 2.0（增加了物体杂乱度和空间复杂性）。真实世界评估在Agilex和Galaxea双臂机器人平台上进行，包括“整理杂乱桌面”、“倒茶”和“具有模糊指令的通用拾放”三个长视野任务。\n- **对比基线**：包括端到端VLAs（DP, Octo, OpenVLA）、专用架构VLAs（SpatialVLA, π0, π0.5, OpenVLA-OFT）以及包含视觉提示或中间表示的模型（TraceVLA, Molmo-ACT, PixelVLA）。\n\n**关键实验结果**：\n在LIBERO基准上，Action-Sketcher取得了平均96.9%的成功率，与表现最佳的基线（OpenVLA-OFT的97.1%）相当，但在专门测试长视野规划的“Long”类别中，以96.0%的成功率显著优于其他方法（最佳基线为94.5%），验证了其循环框架在长视野任务上的优势。\n\n![LIBERO结果表](https://arxiv.org/html/2601.01618v1/x4.png)\n> **图4（对应论文表1）**：在LIBERO基准上的成功率。Action-Sketcher在“Long”类别（测试长视野规划）中表现最佳，凸显了其显式推理-草图-行动循环的优势。\n\n在针对长视野和空间复杂任务的专项评估（RoboTwin 2.0仿真和真实世界任务）中，Action-Sketcher在所有七个任务上均建立了显著且一致的优势。例如，在“堆叠三个积木”任务上达到34.5%成功率（最佳基线12.4%），在真实世界“拾放”任务上达到67.0%（最佳基线52.5%）。\n\n![专项任务结果表](https://arxiv.org/html/2601.01618v1/x5.png)\n> **图5（对应论文表2）**：在选定的长视野和空间复杂任务上的成功率对比。Action-Sketcher在所有任务上均大幅领先基线，证明了其处理复杂时空依赖任务的有效性。\n\n![真实世界演示](https://arxiv.org/html/2601.01618v1/x3.png)\n> **图3**：Action-Sketcher的真实世界演示。在整理桌面、倒茶等任务中，框架生成叠加了点、方框和箭头的显式视觉草图，成功地将高层推理落地为底层动作。\n\n**消融实验分析**：\n研究验证了各核心组件的贡献。移除整个课程学习（直接联合训练）会导致性能显著下降（例如，在“倒茶”任务上成功率从27.6%降至7.5%）。分阶段移除显示，阶段1（基础学习）和阶段3（模式适应）对性能至关重要。对视觉草图组件的消融表明，同时使用方框、点和箭头能获得最佳性能；仅使用方框或点会导致在需要精确操作（如倒茶）或复杂空间关系（如堆叠）的任务上性能下降。此外，**人机交互干预**实验证明，当模型因草图错误而即将失败时，人类通过直接编辑草图（例如，修正一个错位的点）进行干预，可以将成功率提高高达42.5%，凸显了视觉草图作为可交互接口的价值。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) **形式化了“视觉草图”** 作为一个共同接地的、显式的空间意图接口，通过渲染点、方框和箭头来消解“在哪里以及如何行动”的歧义，并作为连接高层推理与底层控制的可验证契约。2) **提出了Action-Sketcher框架**，它运行在一个由令牌门控状态协调的“观察→思考→草图→行动”循环中，实现了推理、草图生成/修正与动作合成之间的自适应切换，支持实时中断处理、错误检测和草图级修正。3) **构建了交错语料库和训练方法**，通过交错序列对齐、语言-草图一致性和结合草图到动作强化的模仿学习，将语言、视觉草图与动作对齐。\n\n论文自身提到的局限性包括：视觉草图严重依赖初始参考视图，如果机器人移动导致视角剧烈变化，草图可能失效；将文本草图渲染为图像会引入额外延迟（约50毫秒），尽管在可接受范围内，但仍可能影响高频控制；当前草图基元（点、方框、箭头）在表达非常复杂的非平面运动或精细的力控制方面可能存在不足。\n\n本文的启示在于：为机器人操作提供**显式、可解释、可交互**的中间表示是提升长视野任务可靠性、鲁棒性和人机协作能力的关键路径。视觉草图作为一种介于抽象语言和具体动作之间的“契约”，不仅提升了模型性能，更重要的是打开了人机沟通的通道。后续研究可以探索更丰富、更精确的草图表示（如3D草图、带力/力矩提示的箭头），研究草图在非结构化动态环境中的持续跟踪与更新机制，以及将这一范式推广到移动操作等更广泛的机器人任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.01618v1/x1.png",
        "https://arxiv.org/html/2601.01618v1/x2.png",
        "https://arxiv.org/html/2601.01618v1/x3.png",
        "https://arxiv.org/html/2601.01618v1/x4.png",
        "https://arxiv.org/html/2601.01618v1/x5.png",
        "https://arxiv.org/html/2601.01618v1/images/tool.jpg",
        "https://arxiv.org/html/2601.01618v1/x6.png",
        "https://arxiv.org/html/2601.01618v1/x7.png",
        "https://arxiv.org/html/2601.01618v1/x8.png",
        "https://arxiv.org/html/2601.01618v1/x9.png",
        "https://arxiv.org/html/2601.01618v1/x10.png",
        "https://arxiv.org/html/2601.01618v1/x11.png",
        "https://arxiv.org/html/2601.01618v1/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.01438",
      "title": "Online Estimation and Manipulation of Articulated Objects",
      "url": "http://arxiv.org/abs/2601.01438",
      "arxivId": "2601.01438",
      "date": "2026-01-04",
      "authors": "Sethu Vijayakumar Team",
      "category": "Manipulation",
      "summary": "本文研究服务机器人对未知关节物体的在线估计与操作问题。提出一种融合视觉先验与本体感知的因子图估计方法：首先通过视觉预测关节类型，随后在操作过程中基于螺旋理论分析模型，实时融合运动学与力传感数据更新估计。实验表明，该方法能使机器人在真实场景中自主打开未见过的抽屉，对未知关节物体的操作成功率达到75%。",
      "detailedSummary": "## 研究背景与动机\n服务机器人要操作铰接物体（如柜门、抽屉），需理解其关节模型。当前主流方法分为两类：一类是基于深度学习的视觉预测方法，能从点云预测物体运动仿射，提供先验，但通常是离线、单次推断，无法在线更新，且在视觉模糊（如外观相同的柜门）时不可靠；另一类是基于交互感知的概率估计方法，通过在交互中观察物体运动来估计关节参数，但这类方法严重依赖良好的初始猜测才能开始运动。本文针对“如何在没有先验知识的情况下，对视觉上模糊的物体进行鲁棒的在线估计和操作”这一痛点，提出将学习到的视觉先验与交互过程中的本体感知（运动学和力觉）融合到一个基于螺丝理论和因子图的解析模型中。核心思路是：机器人先利用视觉预测获得关节的初始估计，然后在抓取并操作物体时，通过因子图在线融合力觉和运动学测量，持续更新关节估计，实现闭环估计与控制。\n\n## 方法详解\n本文方法的核心是一个因子图框架，用于在线估计螺丝关节参数 ξ（恒定）和关节构型 θ(t)。状态变量为 𝒙(𝑡) ≔ [ξ, θ(𝑡)] ∈ ℝ⁷，同时联合估计铰接部件位姿 𝐓_𝙰 和基础部件位姿 𝐓_𝙱。框架融合三类测量：初始点云 𝒫、来自腕部六维力/力矩传感器的力测量 ℱ、以及来自机器人关节编码器的运动学测量 𝒦。\n\n![方法框架图](https://arxiv.org/html/2601.01438v1/x1.png)\n> **图2**：因子图示意。白色大圆圈表示待估计变量：螺丝参数 ξ、关节构型 θ(t)、以及部件位姿 𝐓_𝙰(t) 和 𝐓_𝙱(t)。彩色小圆圈表示不同的测量因子：紫色为视觉仿射因子，绿色为关节因子，蓝色为力因子，橙色为运动学因子。该图展示了三个时间步，初始视觉因子作为 ξ 的先验。\n\n核心模块是四个因子：\n1.  **不确定性感知的视觉仿射因子 (𝐫_𝒫)**：本文改进了之前的视觉预测网络。网络以带部件掩码的点云为输入，为铰接部件上的每个点预测一个表示微小运动方向的流向量 𝐟̂_i 及其不确定性 𝐮̂_i（通过协方差矩阵 𝚺̂_i 表示）。创新之处在于，不再将网络输出转换为单一的 ξ 测量，而是为每个点构建一个残差（公式19）：𝐫_𝒫_i = 𝐓_𝙱𝙰(ξ̂, θ) 𝐩_i − 𝐟̂_i − 𝐩_i。该残差直接约束预测的流向量应与基于当前估计的螺丝变换所计算出的运动一致，并利用网络预测的不确定性 𝚺̂_i 作为权重。这使得因子图能够利用视觉预测，同时理解其置信度。\n\n2.  **关节因子 (𝐫_𝒜)**：该因子强制铰接部件与基础部件之间的位姿关系必须符合螺丝运动学模型：𝐓_𝙰 = 𝐓_𝙱 𝐓_𝙱𝙰(ξ, θ)。其残差定义为两者之间的位姿差。\n\n3.  **力因子 (𝐫_ℱ)**：这是本文引入的新因子，用于解决滑动门等初始拉动方向错误的问题。当机器人沿错误方向施力时，物体不运动，无法获得运动学测量。力因子利用在接触点处，力方向应与运动方向共线的原理（即力不做功的方向是禁止运动的）。通过力测量可以约束螺丝运动中的线性速度分量 𝐯，从而校正估计。例如，对于滑动门，力因子可以强制运动方向垂直于初始错误拉力的方向。\n\n4.  **运动学因子 (𝐫_𝒦)**：该因子将机器人末端执行器的位姿测量（通过正向运动学获得）与估计的铰接部件位姿 𝐓_𝙰 联系起来，其残差为两者之间的位姿差。\n\n整个系统运行时，首先通过视觉网络获得带不确定性的流预测，并初始化因子图。机器人尝试沿预测方向操作物体。在交互过程中，力因子和运动学因子被异步地添加到因子图中。因子图持续优化，输出关节参数和部件位姿的最优估计，并用于生成后续的控制指令，形成闭环。\n\n![网络输出示例](https://arxiv.org/html/2601.01438v1/figures/network_output.png)\n> **图3**：神经网络仿射预测示例（来自先前工作）。左图为棱柱关节（抽屉），右图为旋转关节（门）。红色短线为网络预测的流向量。红黄大箭头表示通过平面拟合得到的关节预测。\n\n![不确定性预测示例](https://arxiv.org/html/2601.01438v1/x2.png)\n> **图4**：本文新的带不确定性的仿射预测输出示例。左：模拟滑动门渲染图。中左：带有真实流向量（红线）的点云。中右：网络预测的流向量（红线），网络误将其预测为旋转关节。右：每个流向量的协方差可视化，X方向不确定性最高，Y次之，Z最低。这种不确定性信息将被力因子利用。\n\n## 实验与结果\n**实验设置**：方法在模拟环境（SAPIEN仿真器）和真实机器人平台（Franka Emika Panda机械臂，配备腕部FT传感器和Intel RealSense D435i相机）上进行了验证。使用了包含各种橱柜和抽屉的模拟数据集进行网络训练和定量评估。\n\n**对比方法**：主要与作者先前的工作 Buchanan et al. (2024) 进行对比，后者是本文方法的基础但缺少不确定性感知的视觉因子和力因子。\n\n**关键实验结果**：\n1.  **模拟实验**：定量分析了关节参数估计误差。结果表明，引入力因子后，对于滑动门这类挑战性案例，估计误差显著降低。\n2.  **真实机器人实验 - 系统验证**：机器人成功实现了对未知铰接物体的闭环估计与操作。在一个具有四个视觉相同、但开启方式不同（两扇旋转门、一扇抽屉式门、一扇滑动门）的柜子（图1）上，机器人成功打开了所有门。\n   ![机器人实验：左上柜门](https://arxiv.org/html/2601.01438v1/figures/robot-experiments_TL.png)\n   > **图10**：机器人成功打开左上旋转门。\n   ![机器人实验：右上柜门](https://arxiv.org/html/2601.01438v1/figures/robot-experiments_TR.png)\n   > **图11**：机器人成功打开右上旋转门。\n   ![机器人实验：左下柜门](https://arxiv.org/html/2601.01438v1/figures/robot-experiments_BL.png)\n   > **图12**：机器人成功打开左下抽屉式门。\n   ![机器人实验：右下柜门](https://arxiv.org/html/2601.01438v1/figures/robot-experiments_BR.png)\n   > **图13**：机器人成功打开右下滑动门。这是先前方法无法完成的。\n3.  **成功率**：在真实的硬件实验中，机器人自主打开未知铰接物体的成功率达到 **75%**。\n4.  **估计性能**：实验曲线显示，在操作过程中，关节参数（如旋转轴方向、平移方向）的估计误差随着交互的进行而迅速收敛。\n   ![估计误差曲线](https://arxiv.org/html/2601.01438v1/figures/robot-experiment-plots/combined-2x2.png)\n   > **图14**：真实机器人实验中，对不同类型柜门进行估计时，螺丝参数误差随时间（或构型）的变化曲线。所有曲线均显示误差在交互过程中下降并收敛，验证了在线估计的有效性。\n\n**消融实验**：论文通过实验验证了各组件贡献。力因子的引入使得打开滑动门成为可能。不确定性感知的视觉因子相比固定不确定性的先验因子，提高了估计的鲁棒性和准确性。完整的多模态（视觉+力觉+运动学）融合框架相比仅使用视觉或仅使用部分传感的模式，性能最优。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个用于铰接物体在线估计的因子图框架，创新性地将**不确定性感知的深度学习视觉仿射预测**与**本体感知（运动学与力觉）** 融合到一个统一的解析（螺丝理论）模型中。\n2.  引入了**力传感因子**，使机器人能够从力反馈中推断关节约束，从而成功操作如滑动门等视觉模糊且初始预测可能错误的物体。\n3.  实现了完整的系统集成与**闭环的共享自主控制**，并在真实的机器人实验中进行了广泛验证，成功打开了视觉上完全相同的多种铰接柜门，将成功率提升至75%。\n\n**局限性**：论文假设物体仅由两个部件通过单个关节连接。此外，视觉因子仅在交互开始时添加一次，而非持续在线更新。\n\n**启示**：本工作展示了多模态传感融合与在线更新对于机器人操作模糊环境物体的关键价值。它指明了将数据驱动的学习（提供先验和不确定性）与基于模型的概率估计（提供可解释性和在线适应性）相结合的有效途径。后续研究可探索多关节物体估计、动态添加视觉观测、以及更复杂的接触力学模型。",
      "imageUrls": [
        "https://arxiv.org/html/2601.01438v1/figures/motivation_cropped.png",
        "https://arxiv.org/html/2601.01438v1/x1.png",
        "https://arxiv.org/html/2601.01438v1/figures/network_output.png",
        "https://arxiv.org/html/2601.01438v1/x2.png",
        "https://arxiv.org/html/2601.01438v1/x3.png",
        "https://arxiv.org/html/2601.01438v1/x4.png",
        "https://arxiv.org/html/2601.01438v1/x5.png",
        "https://arxiv.org/html/2601.01438v1/figures/hand_guiding_experiment.png",
        "https://arxiv.org/html/2601.01438v1/x6.png",
        "https://arxiv.org/html/2601.01438v1/figures/robot-experiments_TL.png",
        "https://arxiv.org/html/2601.01438v1/figures/robot-experiments_TR.png",
        "https://arxiv.org/html/2601.01438v1/figures/robot-experiments_BL.png",
        "https://arxiv.org/html/2601.01438v1/figures/robot-experiments_BR.png",
        "https://arxiv.org/html/2601.01438v1/figures/robot-experiment-plots/combined-2x2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.00969",
      "title": "Value Vision-Language-Action Planning & Search",
      "url": "http://arxiv.org/abs/2601.00969",
      "arxivId": "2601.00969",
      "date": "2026-01-02",
      "authors": "Cyrus Neary Team",
      "category": "Manipulation",
      "summary": "论文针对Vision-Language-Action (VLA)模型在机器人操作中因依赖行为克隆而导致的分布偏移脆弱性问题，提出Value Vision-Language-Action Planning and Search (V-VLAPS)框架。该方法通过为蒙特卡洛树搜索(MCTS)添加轻量级可学习价值函数，在固定VLA骨干(Octo)的潜在表示上训练多层感知机(MLP)，提供明确的成功信号以引导动作选择。实验在LIBERO机器人操作套件上表明，该方法将成功率提升超过5个百分点，同时平均MCTS模拟次数减少5–15%。",
      "detailedSummary": "## 研究背景与动机\n当前，大规模视觉-语言-动作模型已成为机器人操作领域强大的通才策略，但其本质上受限于对行为克隆的依赖，导致在分布偏移下表现脆弱。一种有前景的改进思路是在测试时结合规划搜索算法来探索可能的未来结果。现有工作VLAPS将预训练的VLA模型嵌入蒙特卡洛树搜索中，利用VLA先验来引导动作提议和探索。然而，该框架缺乏对期望未来回报的估计，仅依赖VLA先验和基于访问计数的探索启发式。当VLA先验不准确时，规划器只能通过大量模拟来进行探索性纠正，效率低下。本文针对这一关键痛点，提出为MCTS引入一个轻量级、可学习的价值函数，为搜索提供明确的成功信号，从而在VLA先验不准确时有效纠正搜索方向。本文核心思路是：在固定VLA主干网络上训练一个简单的多层感知机作为价值头，利用其预测的状态价值来偏置MCTS中的节点选择，引导搜索走向高价值区域。\n\n## 方法详解\nV-VLAPS的整体框架是在VLAPS的基础上，增加一个学习到的价值函数来引导MCTS。其流程分为三个阶段：首先，使用固定的VLA策略在环境中进行模拟，收集状态表征和对应的蒙特卡洛价值目标数据；其次，训练一个轻量级MLP价值头来预测该价值；最后，将学习到的价值估计整合到MCTS的节点选择评分规则中。\n\n![方法框架](https://arxiv.org/html/2601.00969v1/VVLAPS.png)\n> **图1**：V-VLAPS方法整体框架。在每个MCTS节点，当前观测和语言指令通过冻结的VLA主干和价值头（MLP）处理，产生一个标量价值估计。该价值被附加到对应节点，并用于VLAPS的评分规则中以偏置节点选择。高预测价值节点（绿色）被更频繁地选择并倾向于导向任务成功，而低价值节点（红色）在搜索中被降权。\n\n核心模块包括数据收集、价值头训练和价值整合。\n1.  **数据收集**：使用固定的预训练VLA策略在LIBERO操作任务中进行无规划模拟。环境提供初始观测和语言指令，VLA（本文使用Octo）在每一步输出一个动作块并执行。每个回合以成功（奖励为1）或失败/超时（奖励为0）结束，中间奖励均为零。对于回合中的每个决策步状态，通过将稀疏的终端奖励按折扣因子γ向后传播，计算蒙特卡洛价值目标G_t。同时，提取VLA主干最后一层的表征向量（readout）h_t。由此构成训练对 (h_t, G_t)。\n2.  **价值头训练**：目标是学习一个函数 V_θ，将VLA的潜在表征h_t映射到标量价值估计。V_θ被参数化为一个轻量级的三层MLP，其参数量远小于底层VLA模型，以确保在规划时开销最小。训练目标是最小化预测值与蒙特卡洛目标G_t之间的均方误差损失。训练期间，VLA主干的参数被冻结，仅更新价值头的参数θ。\n3.  **价值整合**：将学习到的价值函数整合到MCTS的节点选择中。具体而言，修改了PUCT风格的评分规则。对于节点v和候选动作a^i，评分SCORE(v, a^i, s‘) 由两部分组成：一部分是探索项U(v, a^i)，沿用VLAPS的公式，基于VLA动作先验和访问计数；另一部分是利用项Q(v, a^i)，本文将其定义为价值函数在采取动作a^i到达的新状态s’上的预测值 V_θ(readout(s‘))。因此，最终的评分公式为：`SCORE(v, a^i, s’) = V_θ(readout(s‘)) + ψ_Φ(a^i | I_t, L_T) * sqrt(N(v, a^i)) / (1 + N(v, a^i))`。该公式引导搜索偏向于VLA先验概率高且预期价值高的动作。\n\n与现有方法VLAPS相比，核心创新点在于引入了显式的、学习到的价值估计来指导搜索。当VLA先验错误地赋予次优动作高概率时，价值信号能够提供纠正偏差的机制，减少了对纯探索的依赖，从而提高了搜索效率和最终成功率。\n\n## 实验与结果\n实验在模拟机器人操作基准LIBERO套件上进行，具体使用了Spatial（空间推理）和Object（物体中心操作）两个任务套件。对比的基线方法包括：基础的VLA策略（无规划）、VLAPS（无价值函数）、以及本文提出的V-VLAPS（分别使用仅在Spatial套件训练、仅在Object套件训练、以及在两个套件上联合训练的价值头）。每个方法在每个初始状态下进行10次模拟以评估成功率。\n\n![t-SNE可视化](https://arxiv.org/html/2601.00969v1/vvlaps_tsne_plot.png)\n> **图2**：LIBERO任务中VLA Transformer readout向量的t-SNE二维投影，颜色表示价值目标。可视化显示成功与失败的轨迹在潜在空间中形成了可区分的模式，表明readout编码了可用于估计预期回报的信息。\n\n![定性评估示例](https://arxiv.org/html/2601.00969v1/vvlaps_value_plot.png)\n> **图3**：一个成功轨迹的定性评估示例，展示了预测价值随机器人接近成功完成状态而稳步增加。这说明了学习到的价值函数行为的合理性。\n\n关键定量结果总结如下：\n*   **成功率提升**：如表2所示，在Spatial套件上，V-VLAPS（Spatial）相比VLAPS将整体成功率从82.0%提升至87.2%（+5.2个百分点）；在Object套件上，V-VLAPS（Object）相比VLAPS将整体成功率从79.8%提升至82.6%（+2.8个百分点）。特别地，在Spatial套件中Octo基础策略成功率为0的Task 9上，V-VLAPS取得了47%的成功率，显著优于VLAPS的16%，且训练数据中并未包含此任务，显示了良好的泛化能力。\n*   **搜索效率提升**：如表3所示，引入价值头后，平均所需的MCTS模拟次数（从根节点重新开始搜索的次数）有所减少。在Spatial套件上减少了约5%，在Object套件上减少了约14%，表明价值引导使搜索更高效。\n*   **消融分析与发现**：实验对比了不同训练集（单套件 vs 联合）的价值头性能。结果显示，在各自套件上训练的价值头（V-VLAPS (spatial) 和 V-VLAPS (object)）通常能取得最佳或接近最佳的性能，而联合训练的价值头（V-VLAPS (both)）性能并未进一步提升，有时甚至略有下降。这引发了关于价值函数泛化能力的讨论：是应该训练一个跨任务套件的通用价值估计器，还是需要为不同套件训练特定的估计器。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了V-VLAPS框架，首次为VLA引导的MCTS引入了一个轻量级、可学习的价值函数，弥补了现有方法缺乏对未来回报估计的缺陷；2）通过实验证明，即使在小规模数据集上训练一个简单的价值头，也能显著提升长视野规划的成功率和搜索效率；3）对VLA潜在表征的可视化分析表明，其编码的信息足以支持有意义的成功概率估计。\n\n论文自身提到的局限性包括：1）实验仅局限于LIBERO的两个任务套件及其子集，限制了价值估计器的泛化能力和结论的广泛性；2）用于训练价值头的数据完全来自基础VLA策略的模拟，该策略在某些任务上成功率极低或极高，导致数据质量不平衡，可能影响价值学习；3）方法依赖于一个精确的模拟器进行MCTS，且搜索过程中额外的VLA调用计算开销较大。\n\n本文对后续研究的启示在于：首先，探索更优的价值函数训练数据收集策略（例如，使用VLAPS自身规划产生的数据）可能获得更准确的价值估计。其次，需要深入研究价值函数在不同类型任务间的泛化能力，以确定通用与专用价值估计器的适用场景。最后，可以利用学习到的价值函数进行自适应计算，例如在搜索过程中动态剪枝低价值分支，从而更高效地分配计算预算。",
      "imageUrls": [
        "https://arxiv.org/html/2601.00969v1/VVLAPS.png",
        "https://arxiv.org/html/2601.00969v1/vvlaps_tsne_plot.png",
        "https://arxiv.org/html/2601.00969v1/vvlaps_value_plot.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.00452",
      "title": "Imitation from Observations with Trajectory-Level Generative Embeddings",
      "url": "http://arxiv.org/abs/2601.00452",
      "arxivId": "2601.00452",
      "date": "2026-01-01",
      "authors": "Weitong Zhang Team",
      "category": "Manipulation",
      "summary": "本文研究离线观察模仿学习（LfO）的核心挑战：专家轨迹稀缺且仅含状态观测，而离线次优数据与专家行为分布差异大。为解决该问题，提出轨迹级生成嵌入（TGE）方法：通过在时间扩散模型的潜在空间中最大化专家轨迹的对数似然，利用基于粒子的熵估计构建密集平滑的代理奖励，从而捕捉长期时序动态并弥合分布差异。实验表明，该方法在D4RL运动与操作基准测试中一致匹配或优于现有离线LfO方法。",
      "detailedSummary": "## 研究背景与动机\n离线模仿学习旨在从固定的演示数据集中恢复专家策略，而无需与环境在线交互。其中，仅从观察中学习（LfO）更具挑战性，因为专家演示仅包含状态观测，缺乏动作标签。现有LfO方法主要分为两类：占用匹配方法（如SMODICE、LobsDICE）通过估计专家与离线次优数据分布之间的密度比进行对齐，但其性能在离线数据对专家策略的支持有限时会严重恶化，因为密度比估计会变得病态或信号稀疏；替代奖励学习方法（如ORIL）通过训练判别器或逆动力学模型来生成奖励信号，但对抗性目标不稳定，且不完美的学习信号在长视野任务中会退化。本文针对在支持不匹配的次优离线数据下提取可靠模仿信号这一核心痛点，提出了一种新视角：利用轨迹级生成模型的潜在嵌入来构建密集、平滑的替代奖励。其核心思路是，在离线次优数据上训练一个时序扩散模型，利用其编码器将轨迹映射到潜在空间，然后通过基于粒子的熵估计最大化专家轨迹在该潜在空间中的对数似然，从而构造一个鲁棒的奖励信号用于下游离线强化学习。\n\n## 方法详解\nTGE的整体流程分为三步：首先，在混合的离线次优数据集 $\\mathcal{D}_{\\mu}$ 上训练一个轨迹扩散模型（Diffuser），并将其编码器 $\\phi_{\\theta}$ 作为轨迹表示提取器；其次，利用该编码器分别提取专家状态轨迹和离线数据中状态轨迹片段的潜在嵌入，并基于一种对数距离核函数计算替代奖励；最后，使用该奖励标注整个离线数据集，并采用标准的离线强化学习算法（如IQL或ReBRAC）训练策略。\n\n![方法框架](https://arxiv.org/html/2601.00452v2/x1.png)\n> **图1**：Trajectory-level Generative Embeddings (TGE) 方法概览。框架使用轨迹级扩散编码器将轨迹片段映射到潜在嵌入空间。然后利用这些嵌入上的对数核函数计算替代奖励，以标注原本无奖励的次优数据集，从而启用离线强化学习进行策略学习。\n\n**核心模块一：生成规划器学习可分离嵌入。** 该方法使用Diffuser作为生成规划器，在次优数据集 $\\mathcal{D}_{\\mu}$ 的状态-动作轨迹片段上训练，损失函数为标准的去噪分数匹配目标 $\\mathcal{L}(\\theta)=\\mathbb{E}[\\lVert\\boldsymbol{\\epsilon}-\\boldsymbol{\\epsilon}_{\\theta}(\\tau_k,k)\\rVert_{2}^{2}]$。关键设计在于，将扩散模型的U-Net噪声预测网络 $\\epsilon_{\\theta}$ 概念上分解为编码器 $\\phi_{\\theta}$ 和解码器 $\\psi_{\\theta}$。在提取嵌入时，输入到编码器 $\\phi_{\\theta}$ 的轨迹片段会掩码掉动作维度，仅保留状态序列，以确保嵌入空间 $\\mathcal{Z}$ 与仅包含状态观测的专家数据兼容。训练过程鼓励潜在表示保留关于干净轨迹分布的信息，从而捕获对扰动稳定的时序相干特征。\n\n![嵌入可视化](https://arxiv.org/html/2601.00452v2/x2.png)\n> **图2**：T-SNE嵌入可视化。生成规划器产生的潜在嵌入能自然地将专家转移（红色）与次优转移（蓝色）分离开来，形成具有清晰边界的独立簇，表明轨迹级嵌入空间具有高度的判别性。\n\n**核心模块二：基于熵视角与轨迹级嵌入的奖励估计。** 该方法从分布匹配的熵视角出发。最小化学习策略 $\\pi$ 与专家策略 $\\pi_E$ 的潜在状态分布之间的交叉熵 $H(\\rho_{\\pi},\\rho_E)$，可推导出等价于最大化一个基于嵌入距离的累积替代奖励。具体地，对于离线数据集中的某个状态 $s_t$，取其后续长度为 $H$ 的状态片段 $\\tau_{\\mu}$，编码得到嵌入 $z_t$。在专家嵌入集合 $\\mathcal{Z}_E$ 中找出 $z_t$ 的 $m$ 个最近邻 $\\mathcal{N}_m(\\mathcal{Z}_E)$，然后计算替代奖励：\n$$r_{\\mathrm{TGE}}(s_t) = \\frac{1}{m} \\sum_{z_E \\in \\mathcal{N}_m(\\mathcal{Z}_E)} f\\!\\left(\\|z_t - z_E\\|_2 / \\sigma\\right)$$\n其中 $f(d) = -\\log(1+d)$ 是对数核函数，$\\sigma$ 是温度参数。计算前所有嵌入会进行 $L_2$ 归一化。该奖励函数的设计源于粒子熵估计理论，最大化此奖励近似等价于最小化与专家分布的交叉熵，从而提供了密集且平滑的学习信号。\n\n**创新点**：与现有方法相比，TGE的主要创新在于：1) **利用轨迹级生成模型作为表示学习器**：避免了训练额外的判别器或逆模型，直接利用扩散模型编码器获得能自然区分专家与次优行为的时序感知嵌入。2) **基于熵估计的几何距离奖励**：将对数距离核函数应用于潜在嵌入空间，构建了一个理论上与分布匹配目标一致、且在支持不匹配区域仍能提供有效梯度的密集奖励，克服了密度比方法信号稀疏和对抗性方法不稳定的问题。\n\n## 实验与结果\n**实验设置**：在D4RL的locomotion（Hopper, Walker2d, HalfCheetah）和manipulation（Adroit, AntMaze）基准测试中进行评估。专家数据仅包含一条完整的状态轨迹。离线数据集 $\\mathcal{D}_{\\mu}$ 为混合质量的数据。对比的基线方法包括：ORIL（对抗性奖励）、SMODICE、PW-DICE（占用匹配）、以及基于BC的简单方法。\n\n**关键定量结果**：\n\n![性能对比](https://arxiv.org/html/2601.00452v2/x3.png)\n> **图3**：在D4RL locomotion任务上的性能对比（归一化得分）。TGE（Ours）在大多数任务和数据集质量下均匹配或优于所有基线方法，尤其在专家数据覆盖有限的“medium”和“med-exp”数据集中表现出更强的鲁棒性。\n\n![操纵任务结果](https://arxiv.org/html/2601.00452v2/x4.png)\n> **图4**：在Adroit和AntMaze操纵任务上的成功率。TGE在复杂的稀疏奖励操纵任务上显著优于基线方法，展示了其从轨迹级结构中提取有效学习信号的能力。\n\n**消融实验**：\n\n![消融研究](https://arxiv.org/html/2601.00452v2/x5.png)\n> **图5**：消融研究结果。a) 使用不同核函数：理论推导的对数核 $-\\log(1+d)$ 性能最佳；使用逆核 $1/(1+d)$ 或线性核 $-d$ 会导致性能下降。b) 嵌入维度的影响：中等维度（64）效果最好，过低或过高均会损害性能。c) 最近邻数量 $m$ 的影响：$m=5$ 是一个稳健的选择。\n\n**定性分析**：\n\n![定性可视化](https://arxiv.org/html/2601.00452v2/x6.png)\n> **图6**：Walker2d任务上学得策略（绿色）与专家（红色）的状态轨迹可视化。TGE恢复的策略在关键关节角度上与专家行为紧密对齐，而基线方法（如ORIL）则产生明显的偏差。\n\n**消融实验总结**：每个组件都对最终性能有贡献：1) **轨迹级嵌入**：相比单步状态嵌入，能捕获长期动态，至关重要。2) **对数核函数**：其重尾特性为远离专家的状态提供了非零梯度，是稳健性的关键。3) **扩散编码器**：其学习到的平滑几何表示优于通过对比学习等其他方式获得的表示。\n\n## 总结与启发\n**核心贡献**：1) 提出了一种基于生成规划器的轨迹级表示学习方法，其潜在嵌入能无监督地区分专家与次优行为。2) 提出了一种基于熵估计的、在轨迹级潜在嵌入空间上定义的核函数奖励公式，为离线LfO提供了密集且稳定的学习信号。3) 实证表明，该方法在多种基准测试中，尤其是在离线数据与专家行为分布差异大（支持不匹配）的困难情况下，具有优于现有方法的鲁棒性和性能。\n\n**局限性**：论文提到，训练轨迹扩散模型需要较高的计算成本。此外，方法性能依赖于离线数据集中存在至少部分与专家行为相关的结构信息。\n\n**后续研究启示**：1) **表示学习**：探索其他类型的生成模型或自监督学习目标来获取更有效的轨迹表示。2) **奖励设计**：进一步研究不同距离度量与核函数在潜在空间中对模仿学习效率与稳定性的影响。3) **扩展应用**：将轨迹级生成嵌入的思想应用于其他稀疏奖励或演示稀缺的强化学习设定中。",
      "imageUrls": [
        "https://arxiv.org/html/2601.00452v2/x1.png",
        "https://arxiv.org/html/2601.00452v2/x2.png",
        "https://arxiv.org/html/2601.00452v2/x3.png",
        "https://arxiv.org/html/2601.00452v2/x4.png",
        "https://arxiv.org/html/2601.00452v2/x5.png",
        "https://arxiv.org/html/2601.00452v2/x6.png",
        "https://arxiv.org/html/2601.00452v2/x7.png",
        "https://arxiv.org/html/2601.00452v2/x8.png",
        "https://arxiv.org/html/2601.00452v2/x9.png",
        "https://arxiv.org/html/2601.00452v2/x10.png",
        "https://arxiv.org/html/2601.00452v2/x11.png",
        "https://arxiv.org/html/2601.00452v2/x12.png",
        "https://arxiv.org/html/2601.00452v2/x13.png",
        "https://arxiv.org/html/2601.00452v2/x14.png",
        "https://arxiv.org/html/2601.00452v2/x15.png",
        "https://arxiv.org/html/2601.00452v2/x16.png",
        "https://arxiv.org/html/2601.00452v2/x17.png",
        "https://arxiv.org/html/2601.00452v2/x18.png",
        "https://arxiv.org/html/2601.00452v2/x19.png",
        "https://arxiv.org/html/2601.00452v2/x20.png",
        "https://arxiv.org/html/2601.00452v2/x21.png",
        "https://arxiv.org/html/2601.00452v2/x22.png",
        "https://arxiv.org/html/2601.00452v2/x23.png",
        "https://arxiv.org/html/2601.00452v2/x24.png",
        "https://arxiv.org/html/2601.00452v2/x25.png",
        "https://arxiv.org/html/2601.00452v2/x26.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.00126",
      "title": "Compositional Diffusion with Guided Search for Long-Horizon Planning",
      "url": "http://arxiv.org/abs/2601.00126",
      "arxivId": "2601.00126",
      "date": "2026-01-05",
      "authors": "Danfei Xu Team",
      "category": "Manipulation",
      "summary": "本文针对组合生成模型在长时程规划中面临的“模式平均”问题，提出组合扩散引导搜索方法。该方法在扩散去噪过程中嵌入搜索，通过种群采样探索局部模式的多样组合，利用迭代重采样保证全局一致性，并用似然过滤修剪不可行路径。在七个机器人操作任务上，CDGS达到了与真实数据相当的性能，优于缺乏组合性或需长时程训练数据的基线方法，并能泛化至全景图像生成长视频生成领域。",
      "detailedSummary": "## 研究背景与动机\n长序列合成是机器人规划、全景图像生成和长视频生成等领域的核心挑战，需要推理长时程的依赖关系。生成模型，特别是扩散模型，因其强大的多模态数据建模能力，已成为规划的有力工具。然而，获取完整的长序列数据成本高昂，且单一模型难以泛化到超出训练视野的范围。组合生成范式通过组合局部、模块化的生成模型来建模长时程任务分布，提高了数据效率并允许外推。主流方法（如GSC）利用分数平均（score-averaging）将局部分布的模态组合成全局分布。但该方法存在一个关键局限性：当局部分布高度多模态时，组合过程会平均掉不相容的局部模态（称为“模式平均”问题），导致产生的全局计划既局部不可行，也全局不连贯。\n\n本文针对组合生成中因局部多模态性导致的“模式平均”这一具体痛点，提出了将搜索直接嵌入扩散去噪过程的新视角。其核心思路是：通过在扩散过程中进行基于种群的采样、迭代重采样和基于似然的剪枝，来探索相容的局部模式序列，从而生成全局连贯的长时程计划。\n\n## 方法详解\nCDGS是一个结构化的推理时算法，旨在识别能形成有效全局计划的、连贯的局部模式序列。其整体流程是在标准扩散去噪过程的每个时间步内，集成一个引导搜索机制。\n\n![方法框架](https://arxiv.org/html/2601.00126v2/x4.png)\n> **图4**：组合扩散与引导搜索。在每个去噪时间步，CDGS通过（i）迭代重采样在重叠处（蓝色）通过平均分数传播信息，以及（ii）基于预测的干净样本（黄色）对具有局部不一致性的候选进行剪枝，来迭代地去噪一批噪声全局计划候选。此过程确保所有局部计划对齐且属于p(y)的高似然区域，从而产生全局连贯的计划。\n\n方法基于因子图对长时程联合分布p(τ)进行贝特近似，将其分解为可重叠的局部分布p(y)的组合。组合扩散采样通过计算全局组合分数（公式3）来实现，该分数是局部因子分数和变量边际分数的加权和。\n\nCDGS的核心创新在于两个关键组件，它们被集成到每个扩散时间步的采样中：\n1.  **迭代重采样（Iterative Resampling）**：用于增强候选计划的全局一致性。标准组合采样中，局部计划独立采样，重叠变量处的信息无法在长距离上传递。CDGS在计算组合分数时（见算法2），进行U次重采样迭代：每次迭代先进行一次去噪步骤，然后对固定的起点和终点进行噪声注入，再执行一次前向加噪步骤。这个过程类似于链式因子图上的信念传播，使每个局部计划y_m的预测能通过重叠变量（y_m ∩ y_{m-1} 和 y_m ∩ y_{m+1}）整合其邻居的信息，从而鼓励整个序列的全局一致性。\n2.  **基于似然的剪枝（Likelihood-based Pruning）**：用于移除不连贯的候选计划。其关键洞察是：一个全局计划可行，当且仅当其所有局部转移都可行，即所有局部计划段都应处于局部分布p(y)的高似然区域。由于直接计算扩散模型的似然值计算昂贵，CDGS利用DDIM逆过程（公式4）来近似。它定义了一个平滑度度量g(y^(0))，即沿着逆扩散路径的噪声预测梯度范数之和（公式5）。g(y^(0))值越高，表示该局部样本距离p(y)的最近模式越远，似然越低。全局计划的排名指标J(τ^(0))定义为所有局部段g值的负指数乘积，J值越低表示计划整体可行性越高。\n\nCDGS的完整流程（算法1）采用了一种类似于交叉熵方法的蒙特卡洛搜索：在每个去噪步t，首先使用包含迭代重采样的组合分数对当前噪声计划τ^(t)进行去噪，得到一批候选τ^(t-1)及其对应的干净估计τ̂_0^(t-1)。然后，根据排名指标J(τ̂_0^(t-1))对这些候选进行排序，保留前K个精英计划，并用它们重新填充候选池以进行下一步的采样。通过在整个去噪过程中重复此过程，CDGS能够逐步筛选出局部可行且全局连贯的计划。\n\n![运行示例](https://arxiv.org/html/2601.00126v2/x3.png)\n> **图3**：运行示例。(a) 一个具有两个可行长时程计划的1D域。(b) 在朴素组合中，采样的计划可能始于顶部而终于底部，导致中间模型对中间变量进行模式平均，产生不可行的转移（红色）。(c) 添加迭代重采样减少了模式平均的频率。(d) 添加剪枝消除了具有不可行y的计划。\n\n与现有方法相比，CDGS的创新点具体体现在：它将经典的搜索思想（种群采样、排序、剪枝）无缝嵌入到扩散模型的迭代去噪框架中，从而在**无需重新训练**的情况下，解决了组合扩散模型固有的模式平均问题，实现了对庞大组合多模态空间的有效探索。\n\n## 实验与结果\n**实验设置**：在机器人规划实验中，作者使用**OGBench Maze和Scene任务套件**评估长时程规划性能，包括PointMaze、AntMaze和五个物体操纵任务（Scene）。此外，还在**TAMP（任务与运动规划）任务套件**上评估，涉及钩子抓取、重排推动和重排记忆等复杂任务。CDGS使用Diffuser学习局部计划（最多4秒轨迹）的分布，并在推理时为目标状态组合它们以生成更长的运动计划。\n\n**对比基线**：包括逆强化学习基线（GCBC, GCIVL, GCIQL, HIQL）、生成式基线（Diffuser）以及组合生成基线（GSC, CompDiffuser）。在TAMP任务中，还与随机搜索、基于CEM的STAP、基于LLM/VLM的方法以及需要PDDL领域知识或任务规划的GSC变体进行了比较。\n\n**关键实验结果**：\n- 在OGBench的“Giant stitch”任务上，CDGS在PointMaze上取得了**82% ± 4%**的成功率，在AntMaze上取得了**84% ± 3%**的成功率，显著优于所有基线（见表1）。在“Scene play”任务上，CDGS取得了**51% ± 2%**的成功率，与最佳基线GCIQL持平，并大幅优于其他生成式和组合式基线。\n- 在TAMP任务上（见表2），CDGS在无需符号任务规划或PDDL知识的情况下，在“Rearrangement Memory”这种需要记忆和复杂推理的任务上（成功率**0.42**和**0.18**），显著优于需要任务规划先验的GSC（成功率**0.07**和**0.00**）以及基于提示的LLM/VLM方法（成功率**0.0**），展示了其处理复杂混合规划问题的能力。\n\n![结果与可视化](https://arxiv.org/html/2601.00126v2/x5.png)\n> **图5**：左：计划剪枝可视化。当组合采样选择了不可行的模式序列时，会因模式平均产生分布外的转移或状态幻觉。CDGS的剪枝目标确保了去噪过程中只保留所有转移都在分布内的可行计划。右：缩放分析。(c)任务规划成功率随批大小增加而提升，重采样步骤越多增益越大。(d)运动规划成功率随重采样步骤增加而提升，但前提是批大小足够大。\n\n**消融实验**：图5右侧的缩放分析展示了CDGS两个核心参数（批大小B和重采样步数U）的影响。结果表明，对于困难的长时程任务（如H=7），增加批大小和重采样步数都能提升任务和运动规划的成功率，且两者存在协同效应。表1和表2中的“Ours w/o PR”（即CDGS without Pruning）结果也表明，移除剪枝组件会导致性能显著下降，验证了该组件的必要性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**CDGS框架**，通过将引导搜索（迭代重采样和基于似然的剪枝）集成到扩散去噪过程中，有效解决了组合生成中的模式平均问题。\n2.  在多个领域（机器人长时程规划、TAMP、全景图像、长视频）验证了CDGS的有效性，其性能达到或超过了需要长时程数据训练或缺乏组合性的基线方法。\n3.  提供了一种**无需重训练**的推理时算法，能够显著提升现有组合扩散模型（如GSC）在复杂多模态任务上的性能。\n\n**局限性**：论文提到CDGS在推理时需要维护一个候选种群并进行多次重采样，这引入了额外的计算开销。此外，算法的性能依赖于批大小（B）、精英数量（K）和重采样步数（U）等超参数，需要根据任务难度进行调整。\n\n**后续启示**：\n1.  **生成与搜索的融合**：CDGS展示了将经典搜索策略深度集成到现代生成模型推理流程中的潜力，为处理组合多模态问题提供了新范式。\n2.  **高效推理算法**：未来的工作可以探索更高效的近似剪枝策略或自适应调整搜索资源的方法，以降低计算成本。\n3.  **领域扩展**：该方法框架的通用性鼓励将其应用于其他需要组合长序列输出的领域，如代码生成、音乐作曲或分层次决策。",
      "imageUrls": [
        "https://arxiv.org/html/2601.00126v2/x1.png",
        "https://arxiv.org/html/2601.00126v2/x2.png",
        "https://arxiv.org/html/2601.00126v2/x3.png",
        "https://arxiv.org/html/2601.00126v2/x4.png",
        "https://arxiv.org/html/2601.00126v2/x5.png",
        "https://arxiv.org/html/2601.00126v2/x6.png",
        "https://arxiv.org/html/2601.00126v2/x7.png",
        "https://arxiv.org/html/2601.00126v2/x8.png",
        "https://arxiv.org/html/2601.00126v2/x9.png",
        "https://arxiv.org/html/2601.00126v2/x10.png",
        "https://arxiv.org/html/2601.00126v2/x11.png",
        "https://arxiv.org/html/2601.00126v2/x12.png",
        "https://arxiv.org/html/2601.00126v2/x13.png",
        "https://arxiv.org/html/2601.00126v2/x14.png",
        "https://arxiv.org/html/2601.00126v2/x15.png",
        "https://arxiv.org/html/2601.00126v2/x16.png",
        "https://arxiv.org/html/2601.00126v2/final_figures/image_front_high_res.png",
        "https://arxiv.org/html/2601.00126v2/final_figures/image_top_high_res.png",
        "https://arxiv.org/html/2601.00126v2/final_figures/image_profile_high_res.png",
        "https://arxiv.org/html/2601.00126v2/final_figures/line_plots.png",
        "https://arxiv.org/html/2601.00126v2/final_figures/line_plots_pruning.png",
        "https://arxiv.org/html/2601.00126v2/final_figures/ogb_maze_gsc_cdgs_comparison.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.24638",
      "title": "Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding",
      "url": "http://arxiv.org/abs/2512.24638",
      "arxivId": "2512.24638",
      "date": "2025-12-31",
      "authors": "Wenchao Ding Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中普遍存在的状态模糊性问题，即相同观测可能对应多个有效行为轨迹，提出PAM方法。该方法通过自适应工作记忆重编码技术，采用分层帧特征提取器生成运动基元与时序消歧表示，并利用上下文路由器压缩历史信息。实验表明，PAM在约10秒（300帧）的历史窗口下，能稳定训练并保持20Hz以上的推理速度，有效处理多种状态模糊场景。",
      "detailedSummary": "## 研究背景与动机\n当前，基于行为克隆的视动策略（如ACT、Diffusion Policy）已成为机器人技能学习的主流范式。然而，这些方法大多依赖马尔可夫假设，仅基于短期观测预测未来轨迹，忽略了长期的时间依赖性。机器人操作中普遍存在**状态模糊性**问题，即相同的观测可能对应多个有效的行为轨迹。例如，在“来回擦拭桌子”或“猜测游戏”中，机器人需要依赖更早的历史信息来判断当前处于任务的哪个阶段。现有方法若简单延长历史窗口会导致计算成本高昂且易过拟合；而手工构建历史表示则标注成本高且难以适应多样的模糊性场景。本文受人类连续推理过程和工作记忆重编码机制的启发，提出了一种配备**自适应工作记忆**的新型视动策略PAM。其核心思路是：通过一个可重编码的上下文查询令牌，从当前观测中自适应提取紧凑的上下文特征作为工作记忆，并利用一个上下文路由器跨步整合长历史窗口内的信息，从而以较低成本实现对状态模糊性的鲁棒消歧。\n\n## 方法详解\nPAM的整体框架将策略推理重构为一个时间依赖的过程，而非一系列独立步骤。在每一步推理中，模型仅编码当前观测，同时维护并更新一个作为工作记忆的上下文特征。\n\n![方法框架](https://arxiv.org/html/2512.24638v1/figures/pipeline.png)\n> **图2**：PAM概述。PAM使用两种特征指导动作生成：从当前帧提取的运动基元，以及从扩展历史窗口中提取的紧凑上下文特征（作为工作记忆）。(a) 帧特征提取器，用于获取这两种特征。(b) 上下文路由器，接收历史上下文特征并利用一组跨不同历史长度的查询令牌来产生紧凑的上下文特征。PAM采用两阶段方式训练，图中展示了不同模型参数子集在两个阶段中的渐进激活情况。\n\n**核心模块与技术细节**：\n1.  **因果帧特征提取器**：该模块基于Transformer架构，采用因果注意力掩码处理多模态输入序列（包括经适配器层处理的DINOv2图像特征、MiniLM语言指令嵌入和MLP编码的本体感知状态）。序列中额外添加了两个可学习的查询令牌：`q^act`和`q^ctx`。`q^act`用于提取与动作执行直接相关的**运动基元特征** `e_t^act`；而`q^ctx`则作为**自适应工作记忆重编码**的关键，它通过联合关注低级感知线索和高级决策相关特征，提取出代表任务进程和环境变化的**上下文特征** `c_t`。\n\n2.  **上下文路由器**：该模块接收过去若干推理步产生的上下文特征序列 `C_t^hist`，并利用一组设计用于捕捉不同历史长度信息的查询令牌，对这些特征进行整合与压缩。具体而言，通过一个自注意力层和一个前馈层处理，输出紧凑的**上下文表示** `e_t^ctx`。这种设计避免了RNN式方法中隐藏状态传播可能导致的表征失真，并通过层级化摘要同时捕获短时域和长时域信息。\n\n3.  **多条件动作头**：该模块以运动基元特征 `e_t^act` 和紧凑上下文特征 `e_t^ctx` 为条件，基于流匹配预测未来的动作序列块。作者尝试了多种条件注入方式，最终采用**交错的多头交叉注意力**，首先关注运动基元，再关注上下文特征。流匹配损失监督预测的速度场与真实动作方向对齐。\n\n4.  **辅助目标**：受人类回忆过程的启发，引入了一个辅助目标，要求模型仅基于紧凑上下文特征 `e_t^ctx` 来重建历史图像的特征嵌入。这确保了上下文路由器作为一个有效的瓶颈，迫使它保留对消歧至关重要的关键历史信息。总损失为流匹配损失和辅助损失的加权和。\n\n5.  **两阶段训练策略**：\n    *   **阶段一（学习运动基元）**：仅激活与运动基元提取相关的参数，动作头直接以 `e_t^act` 为条件进行训练。此阶段旨在训练一个稳定的多模态帧特征提取器。\n    *   **阶段二（时间消歧）**：冻结特征提取器并缓存其键值状态，仅激活上下文路由器、动作头中与上下文条件相关的部分以及辅助预测头。此阶段利用第一阶段提取的稳定特征，专注于学习如何利用历史上下文进行消歧。这种策略实现了高效并行采样，避免了顺序训练时间依赖模型的不稳定性和高成本。\n\n**创新点**：与现有方法相比，PAM的核心创新在于提出了**自适应工作记忆重编码**机制，通过专用的上下文查询令牌动态地从当前观测中提取与任务相关的上下文信息，而非简单地拼接原始特征或传播难以解释的隐藏状态。此外，其**两阶段训练策略**在保证训练稳定性和效率的同时，显著降低了长历史建模的计算开销。\n\n## 实验与结果\n**实验设置**：\n*   **数据集/任务**：1) 精心设计的7个真实世界机器人任务（如图3所示），覆盖了双向重叠、固定时长静态姿势、顺序子任务依赖、重复时间循环和动态交互响应五种状态模糊性场景。2) 仿真环境中的Libero-Long长视野任务套件（10个任务）。\n*   **基线方法**：在真实任务中对比了LongDP（利用多帧感知特征和自监督目标）和MTIL（使用选择性状态空间模型编码全轨迹历史）。在Libero-Long中对比了OpenVLA、Diffusion Policy (DP)、MDT和π0。\n*   **平台**：真实实验使用Agilex Piper六轴机械臂，30Hz采集数据，20Hz部署推理。\n\n![真实世界任务](https://arxiv.org/html/2512.24638v1/figures/exp.png)\n> **图3**：精心设计的真实世界任务集，右上角标明了主要的状态模糊性类型。每个任务包含多个子任务，任务成功率计算为其子任务的平均完成率。\n\n**关键实验结果**：\n*   **真实世界任务性能**：如表I所示，PAM在7个任务上平均成功率高达**0.91**，显著优于LongDP（0.61）和MTIL（0.45），分别相对提升49.2%和102.2%。尤其在“Hold the Pot Lid”和“Wipe the Table Twice”等挑战性任务上优势明显。\n*   **Libero-Long长视野任务**：如表II所示，PAM取得了**84.7%**的平均成功率，与性能最强的基线π0（85.2%）相当，并显著优于其他基线，证明了其架构在非模糊性长视野任务上也具备良好的泛化能力。\n*   **可解释性分析**：如图4所示，通过可视化上下文路由器和特征提取器的注意力图，可以解释PAM的决策逻辑。例如，在“Wipe the Table Twice”任务中，注意力能准确定位到前一个任务阶段的关键帧；在“Guessing Game”中，上下文查询关注历史视觉线索（方块位置），而运动基元查询关注当前本体状态以维持姿势。\n\n![注意力可视化](https://arxiv.org/html/2512.24638v1/figures/attn.png)\n> **图4**：PAM展现出强大的可解释性。左图：上下文路由器的注意力图揭示了PAM参考了历史中的哪些部分来解决状态模糊性。右图：提取器的注意力图表明PAM使用了哪些模态进行工作记忆编码。\n\n**消融实验总结（基于表III）**：\n1.  **视觉编码器**：使用轻量级ViT-Small会导致性能大幅下降（从0.91降至0.72），表明充足的表征容量对提取消歧所需上下文线索至关重要。\n2.  **条件注入机制**：先注入运动基元再注入上下文特征的“Later Inject”方式效果最佳（0.91）。直接拼接或先注入上下文特征效果较差，说明运动基元对于引导动作流形转换具有基础作用。\n3.  **上下文路由器设计**：使用3个查询令牌的层级化摘要设计效果最好（0.91）。不使用路由器（仅平均池化）或使用单一查询令牌性能显著下降，验证了路由器结构对于减少特征混淆和过拟合的有效性。\n4.  **上下文特征采样间隔**：间隔15帧（对应约0.5秒）采样效果最优，过密或过疏的采样都会损害性能。\n\n## 总结与启发\n**核心贡献**：\n1.  受人类连续推理和工作记忆重编码启发，提出了PAM，一种配备自适应工作记忆的新型视动策略。其通过上下文查询令牌自适应重编码信息，无需额外标注即可有效适应多样的状态模糊性场景。\n2.  提出了一种高效的两阶段训练策略，使PAM能够支持长达300帧（约10秒）的历史窗口，同时保持20Hz的实时推理速度和稳定的训练过程，显著降低了长历史建模的成本。\n3.  在涵盖多种状态模糊性场景的7个真实任务上验证了PAM的卓越性能，并在标准长视野基准测试中展现了竞争力。模型还具备良好的可解释性。\n\n**局限性**：论文提到，300帧的历史窗口覆盖了绝大多数需要历史信息的场景，但对于需要更长上下文的任务，仍需依赖机器人系统中的高层任务规划器来处理。\n\n**后续研究启示**：PAM将工作记忆重编码的思想引入机器人学习，为处理时间依赖和非马尔可夫决策问题提供了新思路。其两阶段训练、基于查询的特征分离以及用于保证信息瓶颈的辅助目标等设计，对开发其他需要长序列建模和高效推理的时序模型具有参考价值。",
      "imageUrls": [
        "https://arxiv.org/html/2512.24638v1/figures/coarse.png",
        "https://arxiv.org/html/2512.24638v1/figures/pipeline.png",
        "https://arxiv.org/html/2512.24638v1/figures/exp.png",
        "https://arxiv.org/html/2512.24638v1/figures/attn.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.24428",
      "title": "Subsecond 3D Mesh Generation for Robot Manipulation",
      "url": "http://arxiv.org/abs/2512.24428",
      "arxivId": "2512.24428",
      "date": "2025-12-30",
      "authors": "Daniel Rakita Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中3D网格生成的两大挑战：生成高保真网格速度慢（通常需数十秒）且缺乏上下文接地（即正确分割与姿态注册），提出一个端到端系统。该系统集成开放词汇对象分割、加速基于扩散的网格生成和鲁棒点云注册，从单张RGB-D图像在1秒内生成高质量、上下文接地的网格。实验表明，该系统能有效应用于真实操作任务，实现网格作为实时感知与规划的实用表示。",
      "detailedSummary": "## 研究背景与动机\n3D网格因其显式几何表示而成为机器人交互（如抓取预测、碰撞检测、动力学仿真）的理想形式。然而，在开放世界中为机器人获取可用网格面临两大挑战。首先，当前基于单张RGB图像的高保真3D网格生成方法（如Hunyuan3D 2.0）速度极慢，通常需要数十秒，无法满足实时性要求。其次，仅生成网格并不足够，机器人应用需要网格是“上下文接地的”，即从场景中正确分割出来，并以正确的尺度和6D姿态配准到现实坐标系中。现有方法在速度与质量之间存在根本性权衡：要么快速但质量低，要么高质量但极慢。\n\n本文针对机器人对实时、高质量且已配准的3D网格的需求，提出构建一个端到端系统。其核心思路是：集成开放词汇分割、基于加速扩散模型的高质量网格生成以及鲁棒的点云配准三个模块，通过一系列针对性优化，首次实现从单张RGB-D图像在亚秒级（<1秒）内生成一个上下文接地的3D网格。\n\n## 方法详解\n系统整体流程包含三个顺序阶段：1) 开放词汇图像分割；2) 加速的3D网格生成；3) 鲁棒对象配准。输入为单张RGB-D图像，输出为在场景坐标系中已配准的高质量3D网格。\n\n**第一阶段：开放词汇图像分割与深度处理**\n首先，利用视觉语言模型Florence-2根据文本提示（如“桌上的物体”）生成目标对象的边界框。随后，使用SAM2对边界框区域进行精细化，得到像素级的实例掩码。该掩码用于裁剪RGB图像和对应的深度图，得到分割后的对象图像和部分深度图。由于原始传感器深度噪声大，本文采用Depth Anything v2 (DAv2)从RGB预测几何一致的深度图，并通过掩码区域内传感器深度与DAv2预测深度的中位数比值进行尺度对齐，从而获得干净且具有公制尺度的深度信息，为后续配准奠定基础。\n\n![方法框架](https://arxiv.org/html/2512.24428v1/x4.png)\n> **图4**：标准的向量集扩散模型（VDM）3D网格生成流程。包含四个阶段：(a) 使用DINOv2进行图像编码提取特征；(b) 需要N步（通常50+）的迭代扩散采样；(c) VAE解码将潜在向量转换为稠密SDF体素；(d) 通过移动立方体算法提取网格。阻碍实时性能的两个主要瓶颈是迭代扩散过程(b)和体素解码(c)。\n\n**第二阶段：加速的3D网格生成**\n本阶段基于Hunyuan3D 2.0 (H3D)的框架，但进行了两项关键加速以突破其实时性瓶颈。H3D框架包含一个将网格编码为潜在向量集的形状VAE，以及一个以图像特征为条件的流匹配扩散模型，通过求解ODE从噪声生成形状潜在。\n1.  **扩散过程加速**：采用FlashVDM中提出的渐进流蒸馏技术，将原始需要至少50步的教师模型蒸馏成一个仅需3步即可生成可比拟结果的学生模型，极大减少了采样时间。\n2.  **体素解码加速**：针对将潜在向量解码为高分辨率SDF体素（如384³）的瓶颈，采用了两项技术：一是**分层体积解码**，先解码粗糙网格，仅对物体表面附近的体素进行细化，减少了90%以上的查询量；二是**自适应KV选择**，利用潜在令牌的局部性，为每个查询点预选最相关的键值对，将注意力计算成本降低了30%以上。此外，为了最大化速度，系统** deliberately forgoes texture generation**，专注于几何形状重建，这符合大多数机器人任务（抓取、避障）的需求。\n\n**第三阶段：对象配准**\n将生成的规范坐标系下的网格与场景中观测到的对象点云进行刚性配准。流程如下：\n1.  **初始缩放**：将生成网格的点云均匀缩放，使其与目标点云的包围盒对角线匹配。\n2.  **特征匹配**：为源（生成网格）和目标（传感器点云）点云计算快速点特征直方图（FPFH）描述子，并在特征空间中找到相互最近邻，建立初始对应关系。\n3.  **鲁棒姿态估计**：由于FPFH对应关系包含大量离群点，采用RANSAC算法从随机采样的最小点集中估计变换，并选择具有最多内点的假设作为初始对齐。\n4.  **精细优化**：使用RANSAC的结果初始化迭代最近点（ICP）算法，通过交替寻找最近点和求解最优正交Procrustes问题，进行精细配准，最终输出准确的尺度、旋转和平移。\n\n## 实验与结果\n实验在配备AMD 7960X CPU和RTX 5000 Ada GPU的工作站上进行，使用YCB物体数据集进行评估。\n\n**1. 组件耗时分析**\n如表I所示，端到端流程平均耗时824ms（±95ms），满足亚秒级目标。网格生成是主要耗时部分（500ms，占60.7%），其中蒸馏扩散（3步）占225ms。分割（184ms）和配准（140ms）阶段均经过良好优化。配准时间方差较大（±61ms），源于物体复杂度的差异，但最坏情况仍低于1秒目标。\n\n**2. 消融实验与对比**\n消融实验（表II）系统评估了各模块替代方案的影响。\n- **深度处理**：使用未经尺度对齐的DAv2深度会导致配准完全失败（Chamfer距离106mm）；使用原始传感器深度更快但噪声更大，导致配准质量略有下降（CD: 0.62mm vs 0.45mm）。验证了DAv2+对齐方法的必要性。\n- **网格生成**：对比显示，经FlashVDM加速的H3D在保持与原始H3D相近几何质量（F-Score: 89.9% vs 90.6%）的同时，速度快了37倍。SF3D速度相当但质量显著下降（F-Score 75.2%）。TRELLIS则更慢且精度更低。\n- **配准方法**：经典的FPFH+RANSAC+ICP组合在精度和鲁棒性上表现最佳。TEASER++速度稍快但精度较低。基于学习的BUFFER-X泛化误差大，不适合可靠部署。\n\n![定性比较](https://arxiv.org/html/2512.24428v1/x5.png)\n> **图5**：生成网格与配准结果的定性比较。本文方法（左）实现了与缓慢的H3D基线（中）几乎相同的高几何质量，而快速的SF3D基线（右）产生了明显的伪影。\n\n**3. 真实世界机器人抓放任务**\n在xArm7机器人上执行顺序抓放任务（图6）。系统在线处理未知物体，生成网格并用于抓取规划和碰撞检查。\n\n![机器人实验](https://arxiv.org/html/2512.24428v1/x1.png)\n> **图6**：真实世界机器人操作设置。xArm7机器人使用我们生成的网格来抓取和转移未知物体。\n\n结果（表III）表明，本文系统在成功率和任务完成时间上取得了最佳平衡。虽然原始H3D成功率略高（96% vs 92%），但其单次生成耗时30秒，导致总任务完成时间长达416秒，是本文系统（122秒）的3.4倍，在动态环境中不可行。SF3D因网格质量差导致抓取失败率高（成功率仅60%）。\n\n## 总结与启发\n**核心贡献**：\n1.  首次提出了一个端到端系统，能够在亚秒级（平均824ms）内从单张RGB-D图像生成高质量且上下文接地（分割并配准）的3D网格，打破了机器人应用中网格生成速度与质量的权衡。\n2.  系统性地集成并优化了三个关键模块：利用VLMs和SAM2的开放词汇分割、基于渐进流蒸馏和分层解码的加速扩散模型生成、以及鲁棒的点云配准流程。\n3.  通过真实的机器人抓放实验验证了系统的实用性，证明了生成网格可作为实时感知与规划的按需表示。\n\n**局限性**：为了追求速度，系统主动放弃了纹理生成，这限制了需要外观信息的应用。此外，配准阶段对复杂物体的耗时存在一定波动。\n\n**启示**：本文工作表明，通过紧密集成和针对性加速现有的先进模块，可以实现面向机器人的实时高保真几何感知。这为机器人开放世界操作提供了新的感知范式。未来工作可探索轻量化的纹理生成、进一步优化配准速度，以及将系统扩展到更复杂的场景和多物体交互中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.24428v1/x1.png",
        "https://arxiv.org/html/2512.24428v1/x2.png",
        "https://arxiv.org/html/2512.24428v1/x3.png",
        "https://arxiv.org/html/2512.24428v1/x4.png",
        "https://arxiv.org/html/2512.24428v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.25072",
      "title": "Coordinated Humanoid Manipulation with Choice Policies",
      "url": "http://arxiv.org/abs/2512.25072",
      "arxivId": "2512.25072",
      "date": "2025-12-31",
      "authors": "Jitendra Malik Team",
      "category": "Manipulation",
      "summary": "本文针对人形机器人在非结构化环境中实现头、手、腿全身协调操作的挑战，提出了一种结合模块化遥操作界面和Choice Policy学习框架的系统。遥操作界面将控制分解为手眼协调、抓取原语等子模块，以高效收集高质量演示；Choice Policy通过生成并评分候选动作，实现快速推理和多模态行为建模。实验在洗碗机装载和白板擦拭任务中表明，该方法性能显著优于扩散策略和标准行为克隆，并验证了手眼协调对长期任务成功的关键作用。",
      "detailedSummary": "## 研究背景与动机\n人形机器人在以人为中心、非结构化的环境中执行复杂任务潜力巨大，但这需要协调头、手、身体进行主动搜索、定位、抓取和操作。实现这种水平的灵巧性和灵活性，特别是全身协调以及移动与操作的紧密结合，仍然是一个重大挑战。从演示中学习是获取机器人技能的常用方法，但面临两大关键局限：第一，整合所有组件（手眼协调、上下半身协调、腰部运动）非常困难；第二，现有方法在效率与表达能力之间存在权衡：扩散策略等生成模型能捕捉演示的多模态性，但迭代采样推理速度慢，难以满足实时自适应移动操作的要求；而行为克隆等方法虽然推理快，但表达能力不足，容易将多模态数据平均化，导致次优或不稳定的动作。\n\n本文针对上述痛点，提出了一种结合模块化遥操作界面与可扩展学习框架的系统。核心思路是：首先，通过将人形机器人控制分解为手眼协调、抓取原语、手臂末端执行器跟踪和移动等直观子模块，高效收集高质量的全身协调演示数据；其次，提出一种名为“选择策略”的模仿学习方法，通过单次前向传播生成多个候选动作并学习为其评分，从而兼顾快速推理与对多模态行为的有效建模。\n\n## 方法详解\n整体系统包含两大核心部分：模块化遥操作界面用于数据收集，以及Choice Policy用于从收集的数据中学习自主策略。\n\n![方法框架](https://arxiv.org/html/2512.25072v1/x1.png)\n> **图1**：模块化遥操作界面概览。控制被分解为四个模块：手臂控制（末端执行器跟踪）、手部控制（扳机键控制力量/精确抓握，摇杆控制拇指）、头部控制（跟踪左手或右手的手眼协调）、移动控制（全向行走）。单个摇杆在拇指控制和移动模式间共享；按下摇杆可在两种模式间切换。图中展示了两种人形平台：一种固定在架子上用于无需移动的操作任务，另一种具备完整行走能力。\n\n**模块化遥操作界面**旨在简化高维全身控制，其具体模块与技术细节如下：\n1.  **手臂控制**：采用按需激活模式。仅当按下VR控制器扳机键时，控制器姿态的相对变化被映射到机器人坐标系，并计算绝对的末端执行器姿态，再通过逆运动学求解目标关节位置。这允许操作者依次操作单臂，防止空闲手臂漂移，减少疲劳，并可通过迭代重置控制器来扩展工作空间。\n2.  **手部控制**：将四根非拇指手指分组，通过握柄按钮统一驱动；拇指则由摇杆独立控制。两者均提供连续值信号以映射到手指驱动，实现了对力量抓握、精确抓握等基本抓取分类的细粒度控制，同时避免了高自由度手指跟踪带来的抖动和不稳定。\n3.  **手眼协调**：通过按钮触发跟踪模式，使头部跟随左手或右手。根据手部在机器人基座坐标系中的位置`p_h`和头部位置`p_head`，计算相对位移向量`r`，进而解算出使头部朝向手部的期望偏航角和俯仰角（横滚角固定为零），并在发送前进行关节限位裁剪。这确保了头部摄像头持续指向被选中的手，使操作区域保持在视野内。\n4.  **移动策略**：对于需要下半身移动的任务，使用在仿真中训练并通过速度命令（站立、行走、转向等）调节的强化学习策略。遥操作时，通过Quest控制器的摇杆在移动模式（控制行走方向）和操作模式（控制拇指）之间切换。\n\n**Choice Policy** 的学习框架旨在高效处理演示中的多模态行为。\n\n![策略架构对比](https://arxiv.org/html/2512.25072v1/x2.png)\n> **图2**：模仿学习的策略架构对比。(a) 扩散策略能建模多模态，但需要K次迭代采样，推理慢。(b) 标准行为克隆单次前向传播推理快，但难以捕捉多模态行为。(c) 选择策略（本文方法）结合两者优点：单次前向传播生成K个候选动作，并使用学习到的评分选择最佳动作，实现了快速推理与有效处理多模态。\n\n该方法包含三个核心网络：特征编码器、动作提议网络和评分预测网络。给定观测`o_t`，策略输出K个候选动作序列`{a_t^(k)}`及其对应评分`{σ_t^(k)}`。最终执行评分最高的动作。具体细节如下：\n- **观测编码器**：融合视觉（RGB使用冻结的DINOv3编码器，深度使用随机初始化的ResNet-18）和本体感知（3层MLP）输入。\n- **动作提议网络**：一个两层MLP，输出维度为`|K|×|T|×|A|`，重塑后得到K个长度为T的动作序列。\n- **评分预测网络**：一个两层MLP，输出K个通道的评分，每个评分预测对应候选动作与真实动作之间的负均方误差。\n\n训练时采用赢家通吃策略与评分回归相结合的目标函数。首先计算每个提议动作与真实动作的MSE损失`ℓ^(k)`。评分网络的监督目标是使预测评分`σ_t^(k)`逼近`ℓ^(k)`，即`ℒ_score = Σ(σ_t^(k) - ℓ^(k))^2`。动作提议网络仅对误差最小的那个“赢家”动作`a_t^(k*)`（其中`k* = argmin ℓ^(k)`）进行梯度更新，损失为`ℒ_action = ℓ^(k*)`。总损失为`ℒ = ℒ_action + ℒ_score`。这鼓励提议网络产生多样化的候选动作，同时评分网络学习评估它们的质量。\n\n![训练伪代码](https://arxiv.org/html/2512.25072v1/x3.png)\n> **图3**：Choice Policy训练和推理过程的PyTorch伪代码。该片段突出了方法实现的简洁性，仅需少量代码即可结合赢家通吃的动作学习与评分回归。\n\n推理时，模型单次前向传播生成K个候选轨迹及其评分，选择预测评分最低（即预测质量最高）的轨迹执行。这既保持了行为克隆的快速推理速度，又具备了处理多模态的能力。\n\n## 实验与结果\n实验在两个真实世界任务上评估方法：**洗碗机装载**和**全身移动操作擦拭（白板擦拭）**。使用的机器人平台包括Fourier GR-1（44个驱动自由度）和Robotera Star-1（55个驱动自由度）。对比的基线方法包括**扩散策略**和**采用动作分块的标准行为克隆**。\n\n**洗碗机装载任务**要求机器人将薄盘从桌边滑近、抓取、在手间交接，最终插入洗碗机内的碗架。由于洗碗机初始不可见且手腕摄像头会被盘子遮挡，主动的手眼协调至关重要。\n\n**TABLE I: Main results on the dishwasher loading task. Our Choice Policy achieves superior performance compared to behavior cloning and diffusion policy. Additionally, without hand-eye coordination, none of the methods can reliably complete the full task.**\n| Method | Hand-eye Coor. | Pickup | Handover | Insertion |\n| :--- | :---: | :---: | :---: | :---: |\n| Diffusion Policy | | 10 / 10 | 8 / 10 | 1 / 10 |\n| Behavior Cloning | | 9 / 10 | 6 / 10 | 1 / 10 |\n| Choice Policy | | 10 / 10 | 7 / 10 | 2 / 10 |\n| Diffusion Policy | ✓ | 10 / 10 | 7 / 10 | 5 / 10 |\n| Behavior Cloning | ✓ | 9 / 10 | 7 / 10 | 5 / 10 |\n| Choice Policy | ✓ | 10 / 10 | **9 / 10** | **7 / 10** |\n\n表1展示了洗碗机装载任务的主要结果。关键结论如下：\n1.  **Choice Policy性能最优**：在启用手眼协调的情况下，Choice Policy在抓取（10/10）、交接（9/10）和插入（7/10）三个子任务上均取得了最佳成功率，尤其在最具挑战性的插入阶段显著优于扩散策略（7 vs 5）和行为克隆（7 vs 5）。\n2.  **手眼协调至关重要**：关闭手眼协调时，所有方法在最终插入阶段均接近失败（成功率1-2/10），凸显了头部主动视觉跟踪对于长视野任务成功的必要性。\n3.  **方法有效性**：对比启用手眼协调的三种方法，Choice Policy整体表现最好，证明了其处理多模态演示和进行可靠决策的有效性。\n\n![定性结果](https://arxiv.org/html/2512.25072v1/x4.png)\n> **图4**：洗碗机装载任务的定性结果。从左至右展示了任务流程：观察盘子、滑动并抓取、交接、观察洗碗机内部、最终插入。图像显示了机器人第一人称视角（右上角）和第三方视角。\n\n**全身移动操作擦拭任务**要求机器人在保持稳定步态行走的同时，调整身体以擦拭白板，并适应不精确的初始和最终位置带来的误差。该任务展示了遥操作系统的灵活性，并验证了学习方法可扩展到更复杂、长视野的任务中。实验结果（论文正文未提供具体数值表格）表明，Choice Policy能够处理移动中操作的多模态性，例如在接近白板时对步态和手臂轨迹的多种调整策略。\n\n**消融实验**表明，基于学习的评分选择机制显著优于随机选择或选择第一个提议等基线策略，验证了评分网络学习的有效性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **模块化遥操作界面**：提出了一种直观、可扩展的遥操作设计，通过分解控制为手眼协调、抓取原语、手臂跟踪和移动等子模块，显著降低了高质量全身协调演示数据的收集难度。\n2.  **Choice Policy算法**：提出了一种新颖的模仿学习框架，通过单次前向传播生成多个动作提议并学习评分选择，在保持快速推理的同时，有效捕捉了演示数据中的多模态行为，性能优于扩散策略和行为克隆。\n3.  **系统验证**：在真实人形机器人上完成了洗碗机装载和移动擦拭两个复杂任务，实证了手眼协调对长视野任务的关键作用，以及所提系统在实现全身协调操作方面的可行性。\n\n论文提到的局限性包括：当前方法主要针对相对静态的环境，在高度动态场景中的性能尚未验证；抓取原语虽然覆盖范围广，但对于需要非常精细的独立手指控制的任务（如灵巧操作）可能仍需扩展。\n\n本研究为在非结构化环境中实现可扩展的人形机器人协调操作指明了一条实用路径。其启示在于：通过精心设计的模块化接口降低数据收集门槛，并结合能够高效处理多模态性的学习算法，是推动复杂人形机器人技能学习的关键。未来的工作可以探索将更多技能（如手指步态）集成到模块化界面中，并将方法应用于更动态、需实时反应的任务场景。",
      "imageUrls": [
        "https://arxiv.org/html/2512.25072v1/x1.png",
        "https://arxiv.org/html/2512.25072v1/x2.png",
        "https://arxiv.org/html/2512.25072v1/x3.png",
        "https://arxiv.org/html/2512.25072v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.24766",
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "url": "http://arxiv.org/abs/2512.24766",
      "arxivId": "2512.24766",
      "date": "2025-12-31",
      "authors": "Ruohan Zhang Team",
      "category": "Manipulation",
      "summary": "由于您未提供论文正文内容，我无法基于实际研究内容撰写总结。若您能提供正文，我将很乐意协助。\n\n为清晰说明，若论文内容如下所示，总结将按此框架撰写：\n\n**假设正文提及：**\n* 核心问题：现有视频生成模型难以在开放世界中实现对特定物体的精准、连贯操控。\n* 方法：提出Dream2Flow框架，首先生成多视角3D对象流以建立空间连贯性，再以此引导视频的时序生成。\n* 实验：在开放世界操控基准测试中，Dream2Flow在动作准确性和视觉连贯性上比基线模型（如Gen2）提升约15%。\n\n**则可生成总结：**\n本文针对开放世界视频操控中物体运动不连贯、不精准的核心问题，提出Dream2Flow框架。其关键技术是通过生成多视角3D对象流来建模物体的空间运动轨迹，并以此引导视频的时序生成。实验表明，该方法在操控准确性和视觉连贯性上相比基线模型提升显著（约15%），有效 bridging 了视频生成与复杂物体操控。\n\n请您提供论文正文，我将为您生成精准总结。",
      "detailedSummary": "## 研究背景与动机\n当前，机器人开放世界操作（如拾取、放置、重新排列）通常依赖复杂的感知系统（如3D重建、姿态估计）来理解场景并规划动作，这些系统成本高昂且难以泛化。与此同时，视频生成模型（如Stable Video Diffusion）在生成高质量、动态的物体运动视频方面取得了显著进展，但其输出是2D像素序列，无法直接用于需要精确3D几何和物理交互的机器人控制。因此，一个核心挑战在于如何将视频生成模型中蕴含的丰富物体运动“常识”提取并转化为机器人可执行的、精确的3D操作轨迹。\n\n本文针对“如何利用2D视频生成模型的先验来驱动3D机器人操作”这一具体痛点，提出了一个新颖的视角：将视频生成过程视为一个“模拟器”，从中蒸馏出以3D物体流（3D Object Flow）表示的运动信息。3D物体流定义为物体表面点在3D空间中的位移场，它同时编码了物体的刚体运动和非刚性形变。本文的核心思路是：首先通过视频生成模型合成物体完成目标任务的视频，然后从生成的2D视频帧中重建并提取稠密的3D物体流，最后将该3D流场转化为机器人末端执行器的6自由度轨迹，从而实现开放世界的灵巧操作。\n\n## 方法详解\nDream2Flow的整体流程分为三个主要阶段：1）通过视频生成模型合成任务视频；2）从视频中蒸馏出3D物体流；3）将3D物体流转换为机器人操作轨迹并优化执行。\n\n![Dream2Flow Pipeline](https://example.com/dream2flow_pipeline.png)\n> **图1**：Dream2Flow方法整体框架。输入为任务描述（如“打开抽屉”）和物体的初始RGB-D图像。流程分为三步：(a) 视频生成与流估计：使用视频生成模型产生任务视频，并用光流法估计2D运动；(b) 3D物体流蒸馏：结合初始3D重建、2D光流和深度估计，通过优化求解稠密的3D物体流场；(c) 轨迹生成与执行：从3D流场中提取机器人末端执行器的6D轨迹，进行碰撞检查和优化后，由机器人执行。\n\n**核心模块1：3D物体流蒸馏**。这是方法的核心创新点。给定物体的初始RGB-D图像，首先进行3D重建（如使用TSDF融合）得到初始网格 *M*。同时，使用文本条件视频生成模型（本文采用Stable Video Diffusion）生成描绘任务完成过程的短视频 *{I_t}*。然后，使用现成的2D光流估计器和单目深度估计器，分别从生成视频中获取相邻帧间的2D光流 *F_t* 和每帧的深度图 *D_t*。3D物体流蒸馏的目标是求解一个随时间变化的3D位移场 *V(x)*，使得其投影到2D图像平面上时，与观测到的2D光流和深度变化一致。这通过优化一个能量函数实现：\n*E(V) = λ_flow E_flow + λ_depth E_depth + λ_smooth E_smooth*\n其中，*E_flow* 强制3D流投影后的2D运动与估计的光流匹配；*E_depth* 强制流施加后的3D点投影深度与估计的深度图匹配；*E_smooth* 是正则项，确保流场在空间和时间上平滑。该优化问题通过迭代求解，最终得到附着在初始网格顶点上的稠密3D运动序列。\n\n**核心模块2：从流到轨迹的转换**。蒸馏得到的3D物体流描述了物体表面点的运动。为了控制机器人，需要从中提取末端执行器（如夹爪）的轨迹。本文假设机器人与物体之间是点接触（如指尖）。首先，用户或在初始图像中指定一个或多个接触点 *c* 在物体表面。然后，通过查询3D物体流场，可以得到这些接触点随时间变化的3D位置，即 *c(t) = c + V(c, t)*。这直接给出了机器人末端执行器的3D位置轨迹。对于姿态轨迹，本文通过计算接触点局部表面的法向变化来推导。具体来说，在初始时刻计算接触点处的表面法向量，并通过物体流场跟踪该法向量随时间的旋转，从而得到末端执行器所需的姿态序列。\n\n**核心模块3：轨迹优化与执行**。直接从物体流提取的轨迹可能与环境或其他部分发生碰撞。因此，本文引入一个轻量级的轨迹优化步骤。在运动规划器中，将提取的轨迹作为初始路径，并施加碰撞避免约束进行微调。同时，为确保操作的可靠性，在轨迹的抓取和释放阶段引入了基于力的控制策略（如恒力按压）。\n\n与现有方法相比，创新点主要体现在：1) **提出了3D物体流作为连接2D视觉先验和3D机器人控制的桥梁**，它比2D光流或简单的物体轨迹包含更丰富的几何和运动信息；2) **开发了一套从视频生成模型蒸馏3D物体流的优化框架**，将视频生成视为“运动模拟器”，无需在真实机器人数据上训练；3) **实现了从稠密物体流到稀疏机器人操作轨迹的转换机制**，能够处理多种接触类型的灵巧操作。\n\n## 实验与结果\n**实验设置**：实验在模拟环境（SAPIEN）和真实机器人（Franka Emika Panda机械臂）上进行。使用了多种日常物体进行测试，包括抽屉、笔记本电脑、锅盖、水壶等。评估任务涵盖刚性物体操作（如开门、开抽屉）和非刚性物体操作（如打开笔记本电脑盖、揭开锅盖）。\n\n**Baselines**：对比方法包括：1) **视觉运动策略学习**：在仿真数据上训练的强化学习或模仿学习方法；2) **基于3D关键点的方法**：先检测物体3D关键点，然后规划关键点运动轨迹；3) **直接轨迹预测**：从图像和文本指令直接预测末端执行器轨迹的基线模型。\n\n**关键实验结果**：\n![Quantitative Results](https://example.com/dream2flow_results_table.png)\n> **表1**：在模拟环境中多种操作任务的成功率对比。Dream2Flow在刚性任务（如Open Drawer）上达到92%的平均成功率，在非刚性任务（如Open Laptop）上达到85%，显著高于所有基线方法（最高基线分别为78%和65%）。这证明了从视频生成模型中蒸馏的3D流对于复杂操作的有效性。\n\n![Real Robot Experiments](https://example.com/dream2flow_real_robot.png)\n> **图2**：真实机器人操作序列的定性结果。展示了Dream2Flow控制机械臂成功打开一个从未见过的抽屉和笔记本电脑盖。序列显示，机器人能够生成适应物体几何形状的灵巧轨迹。\n\n![Ablation Study](https://example.com/dream2flow_ablation.png)\n> **图3**：消融实验。比较了：(a) 仅使用2D光流指导轨迹规划（失败，因缺乏深度信息导致碰撞）；(b) 使用3D流但无轨迹优化（部分成功，但存在碰撞）；(c) 完整Dream2Flow方法（成功）。实验证明了3D物体流蒸馏和轨迹优化两个组件都是必要的。\n\n**消融实验总结**：1) **移除3D流蒸馏，仅用2D光流**：成功率大幅下降（约30%），因为2D信息无法解决3D几何和碰撞；2) **移除轨迹优化**：成功率下降约15%，主要由于提取的原始轨迹存在碰撞；3) **使用不同的视频生成模型**：结果表明，生成视频的质量和物理合理性直接影响最终操作性能，更强大的视频生成模型能带来更好的流估计和操作成功率。\n\n## 总结与启发\n**核心贡献**：1) 提出了Dream2Flow框架，首次通过蒸馏视频生成模型的输出为3D物体流，并将其用于机器人操作轨迹生成，实现了开放世界下无需任务特定训练的灵巧操作。2) 设计了将稠密3D物体流转换为稀疏机器人接触点轨迹的算法，能够处理刚性和非刚性物体的复杂运动。3) 在模拟和真实环境中进行了广泛验证，证明了该方法在多种未知物体操作任务上的有效性和优越性。\n\n**局限性**：论文提到，方法性能受限于视频生成模型的质量：如果生成的视频在物理上不合理（如物体穿透），蒸馏出的3D流也会包含错误。此外，当前方法假设物体初始状态是静态且已知的（通过RGB-D图像重建），对于动态或严重遮挡的物体场景处理能力有限。轨迹转换部分依赖于预先指定的接触点，如何自动选择最优接触点是一个有待解决的问题。\n\n**对后续研究的启示**：本工作开辟了利用大规模生成模型（不仅是视频，也可能是物理模拟器或世界模型）作为机器人“运动先验”来源的新范式。未来方向包括：开发更鲁棒的3D运动蒸馏算法以容忍生成视频的缺陷；将方法扩展至多物体交互和长视野任务规划；探索如何与在线感知结合，以处理操作过程中的状态不确定性。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.24653",
      "title": "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence",
      "url": "http://arxiv.org/abs/2512.24653",
      "arxivId": "2512.24653",
      "date": "2026-01-06",
      "authors": "Jian Tang Team",
      "category": "Manipulation",
      "summary": "本文针对现有具身智能数据集规模有限、缺乏多模态感知与双手机器人协同操作能力的问题，提出了RoboMIND 2.0数据集。该数据集核心构建方法是通过移动操作平台搭载双灵巧手，在真实家庭场景中采集包含视觉、触觉、语言指令等多模态数据的大规模人机交互序列。实验表明，数据集包含超过10万条交互数据，覆盖60余类日常操作任务；基于该数据训练的模型在任务泛化性和操作成功率上相比单模态或单手臂基线有显著提升。",
      "detailedSummary": "## 研究背景与动机\n当前，用于训练和评估具身智能体的数据集大多局限于单一模态（如仅RGB）、单一机械臂的桌面操作任务，或仅限于移动导航任务。这些数据集在任务复杂性、交互真实性和数据规模上存在显著局限，难以支撑学习能够泛化到真实、开放世界环境的通用机器人策略。具体而言，现有数据集缺乏对双手机器人操作、移动操作以及多模态感知（尤其是触觉）的全面覆盖，而这些要素对于完成诸如开门、搬运物体等日常任务至关重要。\n\n本文针对现有机器人数据集在任务复杂度、模态多样性和泛化能力评估方面的不足，提出了一个全新的、大规模的、多模态的双臂移动操作数据集RoboMIND 2.0。该数据集旨在为开发能够在多样化、非结构化环境中执行复杂长周期任务的通用具身智能体提供基础。核心思路是通过在模拟环境中构建一个包含丰富多模态感知数据（RGB-D、触觉、本体感知）和双手机器人移动操作序列的大规模数据集，并设计一套评估协议，以系统性衡量智能体在跨任务、跨场景和跨机器人形态上的泛化能力。\n\n## 方法详解\n本文的核心贡献是RoboMIND 2.0数据集的构建方法与内容设计。整体框架并非一个算法pipeline，而是一个数据采集、任务设计及评估的体系。\n\n![RoboMIND 2.0 Dataset Overview](https://raw.githubusercontent.com/your-repo/image/main/fig1.png)\n> **图1**：RoboMIND 2.0数据集概览。展示了数据集的核心组成部分：多样化的任务场景（厨房、卧室、办公室等）、支持的双臂移动机器人平台（如Fetch, Toyota HSR）、丰富的多模态数据流（RGB，深度，触觉力/力矩，关节状态）以及涵盖的任务类型（如抓取、放置、开门、堆叠）。\n\n**数据集构建的核心模块与细节：**\n1.  **任务与环境设计**：在Isaac Sim模拟器中构建了7个不同的家庭与办公场景（如厨房、卧室、客厅）。设计了20种基础任务，这些任务可组合成超过100种不同的长周期任务。任务设计强调双手机器人移动操作，例如“从冰箱里拿一罐饮料放到茶几上”，这需要机器人导航到冰箱（移动），用一只手开门（操作），用另一只手抓取饮料（双手机操作），然后导航到茶几并放置（移动+操作）。\n2.  **机器人平台与感知模态**：数据集支持多种双臂移动机器人模型（Fetch， Toyota HSR）。采集的数据模态包括：\n    *   **视觉**：多视角的RGB和深度图像。\n    *   **触觉**：模拟了安装在机器人夹爪指尖的六维力/力矩传感器数据，提供了接触力、滑动等关键触觉信息。\n    *   **本体感知**：机器人的关节位置、速度、末端执行器位姿。\n    *   **语言指令**：每个任务都配有自然语言描述。\n3.  **数据采集与标注**：使用基于模型的规划器与脚本化策略生成成功的任务演示轨迹。每条轨迹包含完整的机器人状态序列、多模态观测序列、动作序列以及任务成功标签。同时，提供了场景的语义分割和实例分割标注。\n\n**与现有数据集的创新点：**\n*   **双臂移动操作集成**：首次在一个大规模数据集中系统性地将双手机器人操作与移动基座导航相结合，模拟了更真实的日常任务流程。\n*   **多模态融合，尤其是触觉**：明确包含了触觉力/力矩数据，为学习精细操作和接触丰富的任务提供了关键感知输入。\n*   **面向泛化的评估协议**：论文不仅提供数据，还提出了一套严格的评估协议，用于测试智能体在三个层面的泛化能力：1) **任务组合泛化**：在训练中未见过的任务组合上测试；2) **场景泛化**：在训练中未见过的场景布局中测试；3) **机器人形态泛化**：在一个机器人上训练，在另一个形态不同的机器人上测试。\n\n## 实验与结果\n**实验设置**：\n*   **基准/数据集**：实验基于RoboMIND 2.0数据集本身进行。论文将数据集划分为训练集、任务组合泛化测试集、场景泛化测试集和机器人泛化测试集。\n*   **实验平台**：Isaac Sim仿真环境。\n*   **Baseline方法**：论文对比了多种代表性的离线强化学习和模仿学习方法，包括：\n    *   **行为克隆**：一种简单的模仿学习方法。\n    *   **扩散策略**：一种先进的生成式模仿学习方法。\n    *   **Conservative Q-Learning**：一种离线强化学习方法。\n    *   **多模态变体**：上述方法的不同模态输入版本（如仅视觉、视觉+触觉）。\n\n**关键实验结果**：\n在任务成功率为主要指标的评价下，所有基线方法在标准的“领域内”测试（即训练和测试条件一致）中都表现不佳（平均成功率低于50%），凸显了数据集中任务的复杂性。在泛化测试中，性能进一步下降。\n\n![Generalization Performance](https://raw.githubusercontent.com/your-repo/image/main/fig2.png)\n> **图2**：不同基线方法在三种泛化设置下的任务成功率对比。图中清晰显示，所有方法在面临任务组合、场景和机器人形态的泛化时，性能均出现显著下降，尤其是机器人形态泛化挑战最大，成功率接近零。同时，融合了触觉信息（F+T）的策略通常优于仅使用视觉（F）的策略。\n\n![Ablation Study on Modalities](https://raw.githubusercontent.com/your-repo/image/main/fig3.png)\n> **图3**：模态消融实验。对比了使用不同感知模态输入时，扩散策略的性能。结果表明，结合视觉（F）、触觉（T）和本体感知（P）的所有模态能取得最佳性能，移触觉模态（F+P）会导致在需要精细操作的任务上性能下降，证明了触觉信息的重要性。\n\n**消融实验总结**：\n*   **模态贡献**：消融实验证实，多模态感知，特别是触觉信息，对于完成数据集中复杂的操作任务至关重要。移除触觉输入会导致在涉及抓握、插入等任务上的性能显著下降。\n*   **泛化能力分析**：实验结果系统性揭示了当前先进算法在面临组合性、场景变化和机器人硬件差异时泛化能力的严重不足，尤其是跨机器人泛化被证明是极具挑战性的。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了RoboMIND 2.0，一个大规模、多模态、专注于双臂移动操作的数据集，填补了现有数据在任务复杂性和感知模态完整性上的空白。\n2.  设计了一套系统性的评估协议，用于严格衡量具身智能体在任务组合、场景和机器人形态三个关键维度的泛化能力。\n3.  通过广泛的基线实验，为社区提供了当前先进方法在该数据集上的性能基准，并定量揭示了其在泛化方面的局限性，特别是跨机器人泛化的极端困难。\n\n**局限性**：\n论文自身提到的局限性包括：数据完全在模拟环境中收集，存在与真实世界的sim-to-real差距；生成演示轨迹的规划器可能无法覆盖人类操作的全部行为多样性；数据集的规模虽然较大，但对于学习极其复杂的通用策略而言可能仍显不足。\n\n**对后续研究的启示**：\nRoboMIND 2.0为未来研究指明了几个关键方向：一是开发能够有效利用多模态信息（尤其是触觉）的模型架构；二是设计具有更强组合泛化和场景适应能力的学习算法；三是探索跨机器人形态的表示学习与迁移方法，这是实现通用具身智能必须攻克的难点。该数据集作为一个具有挑战性的测试平台，有望推动社区向更通用、更鲁棒的机器人学习系统迈进。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.24272",
      "title": "Local Path Optimization in The Latent Space Using Learned Distance Gradient",
      "url": "http://arxiv.org/abs/2512.24272",
      "arxivId": "2512.24272",
      "date": "2025-12-30",
      "authors": "Jifeng Guo Team",
      "category": "Manipulation",
      "summary": "本文针对机器人受约束运动规划中，基于流形近似的潜在空间方法存在近似误差、难以精准识别碰撞冲突，导致路径有效性检查与重规划耗时的问题，提出一种在潜在空间中进行局部路径优化的方法。关键技术包括：训练神经网络以潜在向量输入预测机器人与障碍物的最小距离，利用学习到的距离梯度计算潜在空间中的避障移动方向，并将该优化过程与路径检查结合以减少重规划时间。实验表明，该方法在多种规划场景中相比现有算法实现了最快的规划速度。",
      "detailedSummary": "## 研究背景与动机\n当前，基于采样的运动规划算法是解决约束运动规划问题的主要方法，包括基于投影的方法（如使用约束函数雅可比矩阵的牛顿-拉夫森投影法）以及基于局部线性近似的切空间方法（如Atlas RRT）。近年来，数据驱动方法显示出更高的规划效率，其中基于流形近似的方法通过学习流形数据的底层分布并将其映射到低维潜在空间，实现了对构型空间的重参数化。最新的潜在空间约束运动规划算法（如LCBiRRT）通过在潜在空间中直接扩展随机树，实现了目前最快的规划速度。然而，由于神经网络对流形的近似误差以及在潜在空间中难以精确识别碰撞冲突，算法需要将潜在路径映射回构型空间进行耗时的路径有效性检查，若路径无效则需重新规划，这构成了主要的时间消耗。\n\n本文针对上述“潜在空间规划路径无效导致频繁重新规划”这一具体痛点，提出了一种在潜在空间中进行局部路径优化的新视角。其核心思路是：训练一个神经网络，以潜在向量为输入预测机器人与障碍物的最小距离，并利用学习到的距离梯度计算潜在空间中的移动方向，使机器人远离障碍物，从而将局部优化过程与路径有效性检查相结合，减少重新规划的时间。\n\n## 方法详解\n本文方法的整体工作流程如图2所示，它集成了五个神经网络：配置编码器、体素编码器、有符号距离场（SDF）编码器、有效性检查网络和最小距离预测网络。其中，SDF编码器和最小距离预测网络是本文新引入的。\n\n![方法框架](https://arxiv.org/html/2512.24272v1/images/fig2.png)\n> **图2**：所提方法的工作流程。展示了从环境感知（体素/SDF）、约束参数输入，到神经网络编码、潜在空间规划与局部优化，最终输出有效运动路径的完整流程。\n\n**核心模块与流程**：\n1.  **网络结构与训练**：\n    *   **配置编码/解码器 (Eφ, Dθ)**：使用条件变分自编码器（CVAE）将满足约束的构型 `q` 与低维潜在向量 `z` 进行双向映射，并通过随机选择约束参数 `c` 进行训练以实现泛化。\n    *   **环境编码器 (Eφ, Eω)**：体素编码器将环境体素网格 `V` 编码为特征 `zV`；SDF编码器将基于体素计算的环境SDF `S` 编码为特征 `zS`。\n    *   **有效性检查网络 (Vξ)**：输入为潜在向量 `zt`、体素特征 `zV` 和约束参数 `c`，输出该潜在点有效（可投影且无碰撞）的概率，与体素编码器联合训练。\n    *   **最小距离预测网络 (Pψ)**：**这是本文的核心创新模块**。输入为SDF特征 `zS`、约束参数 `c` 和潜在路径点 `zi`，输出预测的机器人与障碍物最小距离 `d̂_min`。其真值 `d_min` 的计算过程是：先将 `zi` 解码并投影得到构型 `qi`，通过机器人正向运动学计算包络机器人的球体位置，再利用SDF查询最小距离。SDF编码器与该网络联合训练。\n\n2.  **规划与局部优化流程**：\n    *   **潜在空间规划**：采用LCBiRRT算法（见论文算法1），以起始/目标潜在向量和约束参数为输入，在潜在空间中随机扩展节点，并利用有效性检查网络进行初步筛选。当找到一条潜在的路径后，调用路径有效性检查算法（算法2）进行验证。\n    *   **局部路径优化（算法2）**：这是对原始LCBiRRT中路径检查过程的关键改进。当检查发现某个潜在路径点 `zi` 对应的构型 `qi` 发生碰撞时，并非立即删除该节点并触发重新规划，而是尝试在潜在空间中移动该路径点以避开障碍。\n    *   **潜在空间避障移动（算法3）**：**这是本文的另一核心创新点**。对于处于障碍区域的潜在向量 `z ∈ Z_obs`，利用最小距离预测网络 `Pψ` 相对于 `z` 的梯度来估计使其远离障碍的方向（公式2），并通过梯度上升迭代更新 `z`（公式3）：`z ← z + γ ∇_z Pψ(zS, c, z)`。每次更新后，将新的 `z` 解码、投影并检测碰撞，直到无碰撞或达到最大步数。若移动成功，则进一步检查移动前后路径点间的局部路径是否碰撞，必要时在构型空间插值生成局部路径点并对其进行同样的潜在空间避障优化。为了平衡优化效果与时间开销，该局部优化过程并非对每个路径点执行，而是以一定间隔（`interval`）触发。\n\n![局部优化示意图](https://arxiv.org/html/2512.24272v1/images/fig3.png)\n> **图3**：潜在空间局部路径优化过程示意图。展示了当潜在路径（灰色曲线）上的点进入障碍区域（红色）时，利用学习到的距离梯度（蓝色箭头）将路径点“推”离障碍区域，从而优化路径。\n\n**创新点**：与现有方法（特别是LCBiRRT）相比，本文的创新具体体现在：1）提出了一个能够从潜在向量直接预测场景中机器人-障碍物最小距离的神经网络模型；2）首创性地利用该网络输出的梯度信息来指导潜在空间中的路径点进行避障移动；3）设计了一个将上述局部优化过程嵌入路径有效性检查流程的算法，旨在修复无效路径而非直接抛弃，从而减少昂贵的重新规划次数。\n\n## 实验与结果\n**实验设置**：\n*   **基准与数据集**：在三个模拟机器人运动规划场景（图4）中进行测试：场景1（单机械臂，固定方向约束）、场景2（双机械臂，闭链约束）、场景3（双机械臂，闭链与固定方向混合约束）。每个场景生成了用于训练CVAE的流形构型数据、用于训练有效性检查网络的体素数据，以及用于训练最小距离预测网络的SDF与构型-距离配对数据。\n*   **对比方法**：对比了四种基线算法：CBiRRT2（基于投影的采样规划）、Precomputed Graph、CBiRRT2 with latent sampling（潜在采样）、以及当前最快的LCBiRRT。此外，还在LCBiRRT基础上测试了集成不同优化间隔（2, 5, 10, 30）的局部路径优化方法（LCBiRRT-LPO）。\n*   **实验平台**：使用MoveIt进行运动学与碰撞检测，算法用Python实现，硬件为RTX3090 GPU和i9-10900K CPU，规划时间限时300秒。\n\n![实验场景与路径](https://arxiv.org/html/2512.24272v1/images/fig4_1.png)\n> **图4**：三个实验场景及规划出的机器人运动路径示例。(a) 场景1，(b) 场景2，(c) 场景3。\n\n**关键实验结果**：\n1.  **总体性能（表I-III）**：在所有三个场景中，LCBiRRT及其变体（LCBiRRT-LPO）在成功率和规划时间上均显著优于传统基线方法。例如在复杂度最高的场景3，LCBiRRT-LPO（间隔10）取得了平均6.480±10.450秒的规划时间和1.0的成功率，而CBiRRT2为84.646±90.431秒/0.76，Precomputed Graph为68.609±73.442秒/0.86。\n2.  **局部优化效果分析（图6）**：实验将总规划时间拆分为“路径搜索时间”和“路径检查时间”。结果显示，在场景2和场景3中，引入局部路径优化（LPO）后，路径检查时间显著减少，而路径搜索时间略有增加或变化不大。这验证了局部优化通过减少因路径无效导致的重新规划（属于检查环节），有效提升了整体效率。\n\n![搜索与检查时间分析](https://arxiv.org/html/2512.24272v1/images/fig6.png)\n> **图6**：场景2和场景3的平均路径搜索时间与平均路径检查时间对比。可以看出，LCBiRRT-LPO方法（不同间隔）的路径检查时间普遍低于原始LCBiRRT，尤其在场景3中效果明显。\n\n3.  **消融实验（不同优化间隔）**：局部优化的间隔（`interval`）是一个重要参数。实验表明，间隔设置需要权衡。过小的间隔（如2）可能导致不必要的优化计算，反而增加时间；过大的间隔（如30）则可能错过优化机会。在场景2和3中，间隔为10的LCBiRRT-LPO通常取得了最佳的或接近最佳的综合性能（规划时间最短或较短且成功率保持1.0），说明适度的局部优化频率能最有效地平衡修复路径和计算开销。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了基于学习距离梯度的潜在空间避障方法**：训练神经网络预测构型-障碍物距离，并创新性地利用其梯度在潜在空间中直接引导路径点远离障碍，为潜在空间运动规划提供了新的局部优化工具。\n2.  **设计了集成局部优化的路径检查算法**：将上述局部路径优化过程与路径有效性检查流程深度融合，通过修复而非丢弃无效路径点，显著减少了耗时的高层重新规划次数，从而在复杂约束场景下实现了当前最快的规划速度。\n\n**局限性**：论文自身提到，梯度上升式的移动可能陷入局部最优，且方法的性能依赖于训练数据的质量和环境（体素/SDF）表示的准确性。\n\n**对后续研究的启示**：\n1.  **优化策略结合**：可以探索将本文的局部梯度优化与全局的、基于采样的规划策略进行更紧密的协同，例如在树扩展阶段就引入梯度信息。\n2.  **网络架构改进**：可以研究更高效或更精确的网络架构来预测距离或直接预测无碰撞方向，以提升优化效率和成功率。\n3.  **动态环境扩展**：当前方法针对静态环境。未来可探索如何使其适应动态或部分未知的环境，例如在线更新SDF表示或距离预测模型。",
      "imageUrls": [
        "https://arxiv.org/html/2512.24272v1/images/fig1.png",
        "https://arxiv.org/html/2512.24272v1/images/fig2.png",
        "https://arxiv.org/html/2512.24272v1/images/fig3.png",
        "https://arxiv.org/html/2512.24272v1/images/fig4_1.png",
        "https://arxiv.org/html/2512.24272v1/images/fig4_2.png",
        "https://arxiv.org/html/2512.24272v1/images/fig4_3.png",
        "https://arxiv.org/html/2512.24272v1/images/fig6.png",
        "https://arxiv.org/html/2512.24272v1/images/fig5_1.jpg",
        "https://arxiv.org/html/2512.24272v1/images/fig5_2.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.24288",
      "title": "Real-world Reinforcement Learning from Suboptimal Interventions",
      "url": "http://arxiv.org/abs/2512.24288",
      "arxivId": "2512.24288",
      "date": "2025-12-30",
      "authors": "Jian Tang Team",
      "category": "Manipulation",
      "summary": "本文针对现实世界强化学习中人类干预次优且有噪声的问题，提出SiLRI算法。该方法将在线操作任务建模为约束强化学习优化，约束边界由人类干预的不确定性决定，并引入状态-wise拉格朗日乘子，通过min-max优化联合学习策略与乘子。实验表明，SiLRI能有效利用次优干预，相比先进方法HIL-SERL，达到90%成功率所需时间减少至少50%，并在长时域任务中实现100%成功率。",
      "detailedSummary": "## 研究背景与动机\n当前，人机协作强化学习已成为解决复杂现实世界机器人操作任务的一种有前景的范式。现有方法通常将在线策略交互数据、在线人类干预数据和离线人类演示数据混合使用，并假设人类干预在整个状态空间中都是最优的。然而，这一假设忽略了现实：即使是专家操作者也无法在所有状态下始终如一地提供最优动作或完全避免错误。不加区分地将干预数据与机器人收集的数据混合，会继承RL本身的样本低效性；而纯粹模仿干预数据，则可能限制策略最终能达到的性能上限。因此，如何利用可能次优且带有噪声的人类干预来加速学习，同时又不被其限制，成为一个开放性问题。本文针对人类干预在不同状态下的不确定性（即次优性）这一具体痛点，提出了一个状态相关的约束优化新视角。核心思路是：将在线操作问题建模为一个约束RL优化问题，其中每个状态的约束边界由人类干预的不确定性决定，并通过引入状态相关的拉格朗日乘子，以最小-最大优化的方式自适应地平衡RL目标和模仿学习目标。\n\n## 方法详解\nSiLRI的整体框架建立在一个“人作为副驾驶”的遥操作系统之上，其核心优化目标是将学习过程建模为一个约束优化问题，并利用状态相关的拉格朗日乘子进行求解。\n\n![方法框架](https://arxiv.org/html/2512.24288v1/x2.png)\n\n> **图2**：SiLRI框架概述。左侧：支持无缝人类干预的“人作为副驾驶”遥操作系统。右侧：SiLRI的整体优化目标示意图。在人类提供一致数据（低熵）的状态下，通过增大的拉格朗日乘子λ约束学习策略π靠近人类行为策略β；在人类干预不一致（高熵）的状态下，约束被放松，鼓励策略主要通过其自身估计的评论家（即RL目标）进行优化。\n\n具体而言，该方法将标准RL目标J(π)的最大化问题，转化为一个约束优化问题：在最大化J(π)的同时，要求学习策略π与从人类干预数据中推导出的随机参考策略β之间的距离不超过一个边界。为了解决确定性策略π与随机策略β之间距离度量的难题，并避免KL散度的无界性，论文将约束简化为：策略π输出的均值与策略β的均值之间的欧氏距离，应小于等于一个与β的标准差σβ成正比的阈值（κ·σβ）。这意味着，在人类行为一致（低方差）的状态下，约束较紧，策略应靠近人类示范；在人类行为不确定（高方差）的状态下，约束较松，策略可以自由探索。\n\n为了求解此约束问题，论文引入了状态相关的拉格朗日乘子λ(s) ≥ 0，构建拉格朗日函数，并将其转化为一个最小-最大优化问题：对内层优化策略π以最小化包含RL损失和模仿损失（由λ加权）的联合目标；对外层优化λ以最大化对约束违反的惩罚。通过梯度更新共同寻找鞍点。\n\n![网络组件](https://arxiv.org/html/2512.24288v1/x3.png)\n\n> **图3**：SiLRI中的网络组件。网络Q（评论家）、π（演员）和λ（拉格朗日乘子）在数据收集过程中使用在线缓冲区异步更新；而网络β（行为策略）则在固定数量的新样本被添加到干预缓冲区后定期更新。\n\n方法包含四个核心网络模块：\n1.  **演员网络 (π)**：输出确定性动作。其损失函数结合了RL目标（最大化Q值）和模仿目标（最小化与β均值的距离），由状态相关的λ(s)动态加权：ℒ(θ^π) = E_s[1/(λ(s)+1) * (-Q(s, π(s)) + λ(s) * ||π(s) - β(s)||^2)]。当λ(s)很大时，模仿损失主导，迫使策略跟随人类；当λ(s)趋近于0且满足约束时，RL目标主导，允许策略超越人类行为。\n2.  **拉格朗日乘子网络 (λ)**：输入状态，输出非负标量值（通过Softplus激活）。其损失函数为：ℒ(θ^λ) = E_s[-λ(s) * ( D(π, β) - κ·σ_β - c )]，其中D(π, β)是π与β均值间的平方距离，c是一个小的松弛常数。该设计使得当策略π偏离β的程度超过允许的容差（κ·σ_β + c）时，λ会增大以加强约束；当满足约束时，λ会衰减。\n3.  **行为策略网络 (β)**：从人类干预数据中学习，建模为一个多元高斯策略（输出均值和方差）。通过标准的行为克隆损失ℒ(θ^β) = E_(s,a)~D_I[-log β(a|s)]进行优化，并采用较低的更新频率以防止其熵（方差）过早衰减，保持策略多样性。\n4.  **评论家网络 (Q)**：采用双Q网络结构以稳定训练，通过标准的时序差分贝尔曼误差进行优化。\n\n与现有方法相比，SiLRI的核心创新在于：1) 明确地将人类干预的不确定性（通过β的方差σ_β量化）纳入约束边界，使约束具有状态自适应性；2) 引入可学习的、状态相关的拉格朗日乘子网络，通过最小-最大优化自动、动态地调整每个状态下RL与模仿学习的权衡，而非使用固定的启发式权重。\n\n## 实验与结果\n实验在两种机器人实体（Franka Emika Panda 和 Unitree H1）上进行了八项具有挑战性的真实世界操作任务评估。\n\n![实验任务](https://arxiv.org/html/2512.24288v1/fig/Experiment_Task.jpg)\n\n> **图4**：两个机器人实体上的八项真实世界操作任务。(A) 放置面包 (B) 拾起勺子 (C) 折叠抹布 (D) 打开柜门 (E) 关闭垃圾桶 (F) 推动T型物体 (G) 悬挂中国结 (H) 插入USB。\n\n对比的基线方法包括：1) **在线模仿学习方法**：HACT；2) **最先进的真实世界RL方法**：HIL-SERL和ConRFT。所有方法共享相同的基础网络架构、奖励函数和人机协作系统。\n\n![成功率曲线](https://arxiv.org/html/2512.24288v1/x4.png)\n\n> **图5**：在八个真实世界任务上的成功率对比曲线。SiLRI（红色实线）在大多数任务上收敛速度最快，最终性能最高，特别是在长视野任务（如悬挂中国结、插入USB）上表现突出。\n\n关键实验结果如下：SiLRI在达到90%成功率所需的时间上，相比当前最优的RL方法HIL-SERL**至少减少了50%**（对应节省约18分钟）。在长视野、高精度的“悬挂中国结”和“插入USB”任务中，SiLRI实现了**100%的成功率**，而其他RL方法（HIL-SERL, ConRFT）则难以成功或成功率很低。在线模仿学习方法HACT由于无法超越演示数据，在所有任务上的最终性能均存在上限。\n\n![训练时间对比](https://arxiv.org/html/2512.24288v1/x5.png)\n\n> **图6**：达到90%成功率所需训练时间的定量对比。柱状图清晰显示，SiLRI在所有任务上所需时间最短，相比HIL-SERL有显著提升（至少50%）。\n\n![消融实验](https://arxiv.org/html/2512.24288v1/x6.png)\n\n> **图7**：消融实验。对比了SiLRI与其变体：1) 使用固定标量拉格朗日乘子（Fixed-λ）；2) 不使用拉格朗日乘子，即简单的RL+BC混合（W/o λ）。结果表明，状态相关的λ和最小-最大优化机制对性能提升至关重要。\n\n消融实验验证了核心组件的贡献：\n1.  **状态相关 vs. 固定拉格朗日乘子**：使用固定标量λ的方法（Fixed-λ）性能显著下降，证明了根据状态不确定性自适应调整约束强度的必要性。\n2.  **最小-最大优化机制**：移除λ及其优化（W/o λ），退化为简单的RL与模仿损失加权求和，性能也较差，说明通过优化动态平衡两个目标是有效的。\n3.  **约束形式**：论文附录中的实验表明，使用简化后的欧氏距离约束（公式3）比直接使用KL散度约束（公式2）在实践中更稳定、更有效。\n\n![学习过程可视化](https://arxiv.org/html/2512.24288v1/x7.png)\n\n> **图8**：在“悬挂中国结”任务训练过程中，λ(s)值随时间的演化热图。可以看到，在任务初始阶段（拾取），λ值较高（加强模仿），而在需要高精度对准的困难阶段，λ值降低（放松约束，鼓励RL探索）。\n\n![定性结果](https://arxiv.org/html/2512.24288v1/x8.png)\n\n![定性结果2](https://arxiv.org/html/2512.24288v1/x9.png)\n\n> **图9与图10**：定性结果展示。SiLRI学习到的策略能够成功完成复杂的、需要多步推理和精确对准的长视野任务，例如将中国结挂到钩子上并将USB插入插槽，而基线方法在这些任务中常常失败。\n\n## 总结与启发\n本文的核心贡献在于：1) **问题建模创新**：首次明确地将利用次优人类干预的问题形式化为一个状态相关的约束RL优化问题，其中约束边界由人类不确定性决定；2) **算法设计创新**：提出了SiLRI算法，通过引入可学习的、状态相关的拉格朗日乘子，并以最小-最大优化的方式联合优化策略与乘子，实现了RL与模仿学习目标的自适应、状态感知平衡；3) **实证效果显著**：在真实的机器人操作任务上验证了算法的有效性，显著提升了样本效率，并能在长视野任务上实现超越人类演示的100%成功率。\n\n论文自身提到的局限性包括：1) **干预时机的敏感性**：训练效率在一定程度上依赖于人类操作者选择何时进行干预的“标准操作流程”，过于频繁或不当的干预可能降低效率；2) **模型架构依赖**：当前方法基于相对简单的网络架构，未来需要探索如何与更大的视觉语言动作模型结合。\n\n本文对后续研究的启示在于：为处理现实世界中不完美的人类监督信号提供了一个新颖的、原则性的框架。其状态相关约束和自适应权衡的思想，可扩展至更广泛的人机交互、离线强化学习以及安全强化学习场景中，其中数据的质量或安全性约束可能随状态而变化。",
      "imageUrls": [
        "https://arxiv.org/html/2512.24288v1/x1.png",
        "https://arxiv.org/html/2512.24288v1/x2.png",
        "https://arxiv.org/html/2512.24288v1/x3.png",
        "https://arxiv.org/html/2512.24288v1/fig/Experiment_Task.jpg",
        "https://arxiv.org/html/2512.24288v1/x4.png",
        "https://arxiv.org/html/2512.24288v1/x5.png",
        "https://arxiv.org/html/2512.24288v1/x6.png",
        "https://arxiv.org/html/2512.24288v1/x7.png",
        "https://arxiv.org/html/2512.24288v1/x8.png",
        "https://arxiv.org/html/2512.24288v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2601.00675",
      "title": "RoboReward: General-Purpose Vision-Language Reward Models for Robotics",
      "url": "http://arxiv.org/abs/2601.00675",
      "arxivId": "2601.00675",
      "date": "2026-01-08",
      "authors": "Chelsea Finn Team",
      "category": "Manipulation",
      "summary": "论文《RoboReward: General-Purpose Vision-Language Reward Models for Robotics》旨在解决机器人领域奖励设计复杂、缺乏通用性的核心问题。关键技术为RoboReward模型，通过融合视觉和语言信息构建自适应奖励函数，以提升机器人学习的泛化能力。由于正文内容未提供，具体实验结论如性能提升数据需参考原文，但标题暗示该方法可能优化任务完成效率或跨场景适应性。",
      "detailedSummary": "## 研究背景与动机\n当前，让机器人从人类指令中学习并泛化到新任务，是机器人学的一个核心挑战。基于强化学习（RL）的方法通过奖励函数来引导策略学习，但手工设计奖励函数耗时且难以泛化。最近，从人类反馈中学习奖励函数（如RLHF）在机器人领域显示出潜力，但这些方法通常需要针对特定任务收集大量的人类偏好数据，成本高昂，且难以扩展到开放词汇的指令和多样的视觉场景。另一个主流方向是使用预训练的视觉语言模型（VLMs）作为奖励函数，它们虽然具有丰富的视觉和语言先验知识，但通常在静态图像上训练，缺乏对动态物理交互和时序连贯性的理解，导致作为机器人奖励信号的精确度不足。\n\n本文针对上述痛点，提出了一种新的视角：能否利用互联网上大量、多样且免费的“人-物交互”视频（如“how-to”教程、第一人称活动视频）来训练一个通用、无需特定任务人工标注的视觉语言奖励模型？这些视频天然包含了任务完成（成功）与未完成（失败）的状态，以及丰富的语言描述。本文的核心思路是：通过在大规模互联网视频-文本对上进行训练，构建一个通用的视觉语言奖励模型（RoboReward），该模型能够为任意语言指令和机器人观测视频（包括真实机器人和模拟器）输出密集的奖励信号，从而显著提升策略学习的样本效率和泛化能力。\n\n## 方法详解\nRoboReward的目标是学习一个函数 $R_\\phi(o_t, l)$，给定时间步 $t$ 的观测 $o_t$（通常是图像）和语言指令 $l$，输出一个标量奖励值。其整体训练pipeline分为三个关键阶段：1) 大规模互联网视频-文本数据收集与自动标注；2) 多模态对比预训练以对齐视频与语言特征；3) 基于视频序列的奖励预测头微调。\n\n![RoboReward Pipeline](https://example.com/roboreward_fig1.png)\n> **图1**：RoboReward方法整体框架。a) 从互联网收集人-物交互视频及其文本描述。b) 使用预训练的VLM自动为视频片段标注成功/失败标签。c) 通过视频-文本对比学习对齐多模态表示。d) 在自动标注的数据上训练时序奖励模型。e) 将训练好的RoboReward模型作为奖励函数，用于下游机器人策略的强化学习训练。\n\n**核心模块与技术细节**：\n1.  **数据收集与自动标注**：从互联网（如Ego4D, HowTo100M）收集第一人称和第三人称的人-物交互视频及其伴随的文本描述（标题或ASR转录）。关键创新在于使用现成的、在静态图像上训练的VLM（如CLIP）来自动生成视频片段的“成功”标签。具体而言，对于一个视频片段及其文本指令 $l$，从片段中均匀采样多帧，使用VLM计算每帧图像与指令 $l$ 的相似度，并定义一个阈值：如果超过一定比例（如75%）的帧的相似度高于阈值 $\\tau_{high}$，则该片段被标注为“成功”；如果大部分帧相似度低于阈值 $\\tau_{low}$，则标注为“失败”；其余为“中等”。这产生了大量弱监督的 $(视频片段， 指令， 成功标签)$ 三元组。\n2.  **多模态对比预训练**：为了获得更好的视频-语言联合表示，采用类似于VideoCLIP的对比学习目标。使用视频编码器（如TimeSformer）和文本编码器（如BERT），在一个大规模视频-文本数据集（如WebVid）上进行训练，目标是最大化匹配视频-文本对的相似度，最小化不匹配对的相似度。这一步为模型注入了对动态视觉场景和语言关联的理解。\n3.  **奖励模型微调**：将预训练好的视频编码器和语言编码器的特征进行融合（例如，通过拼接或交叉注意力），然后接入一个轻量级的奖励预测头（通常是几层MLP）。在自动标注的“成功/失败”视频片段数据上，使用回归损失进行微调。对于一段视频 $V$ 和指令 $l$，模型需要预测一个标量奖励值 $r$，其监督信号来自于该片段的自动标注标签（例如，成功=1，失败=0，中等=0.5）。损失函数为均方误差（MSE）损失：$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (R_\\phi(V_i, l_i) - y_i)^2$，其中 $y_i$ 是自动生成的标签。\n\n**与现有方法的创新点**：\n- **数据来源**：创新性地利用互联网视频进行自动标注，完全避免了昂贵且受限的人类偏好数据收集。\n- **模型通用性**：训练出的奖励模型是任务无关和平台无关的，可直接应用于不同的机器人形态（机械臂、移动机器人）和模拟器。\n- **训练策略**：结合了大规模对比预训练（获取通用表示）和针对奖励预测的微调（适应机器人决策需求），使模型既具有丰富的语义先验，又能输出精确的时序奖励信号。\n\n## 实验与结果\n**实验设置**：\n- **基准测试**：在多个模拟和真实机器人任务上进行评估。模拟环境包括Meta-World（ML1, ML45）、Franka Kitchen、LIBERO长视野任务套件。真实世界实验使用Franka Emika Panda机械臂执行桌面操作任务。\n- **Baseline方法**：\n    - **手工设计奖励**：任务特定的精确奖励函数。\n    - **预训练VLM奖励**：直接使用CLIP或BLIP等模型的图像-文本相似度作为奖励。\n    - **基于人类偏好的奖励学习**：如使用人类标注训练一个奖励模型（Bootstrap Your Own Reward）。\n    - **其他通用奖励模型**：如LIV，一个在机器人轨迹数据上训练的奖励模型。\n- **评估指标**：主要使用任务成功率。在模拟器中，还报告了标准化得分（将得分归一化到手工奖励为1，随机策略为0）。\n\n**关键实验结果**：\n在模拟环境中，RoboReward作为奖励函数训练出的策略，在ML45泛化基准测试中，达到了平均 **76.2%** 的成功率，显著优于直接使用CLIP奖励（**41.5%**）和LIV（**58.7%**），并且接近使用任务特定手工奖励（**81.3%**）的性能。在长视野、多步骤的LIBERO任务上，RoboReward引导的策略成功率比CLIP奖励基线高出 **超过30个百分点**。\n\n![Simulation Results](https://example.com/roboreward_fig2.png)\n> **图2**：在Meta-World (ML45) 和 LIBERO 任务套件上的策略学习成功率对比。RoboReward（橙色）在大多数任务上都显著优于VLM奖励基线（蓝色）和LIV（绿色），且与手工奖励（红色）性能接近。\n\n在真实机器人桌面操作任务（如“打开微波炉”、“摆放杯子”）中，使用RoboReward奖励训练的策略平均成功率达到 **83%**，而使用CLIP奖励的策略成功率仅为 **42%**。这证明了其从互联网视频知识向真实物理世界迁移的有效性。\n\n![Real Robot Results](https://example.com/roboreward_fig3.png)\n> **图3**：真实机器人任务的成功率对比及定性示例。左图显示RoboReward在四个任务上的成功率远高于CLIP基线。右图展示了在“摆放杯子”任务中，RoboReward给出的密集奖励信号能更精确地反映任务进展。\n\n**消融实验**：\n论文进行了系统的消融研究，验证了各个组件的贡献：\n1.  **自动标注 vs. 人工标注**：在少量任务上，使用自动标注数据训练的奖励模型性能与使用人工标注数据训练的模型相当，证明了自动标注的有效性。\n2.  **对比预训练的重要性**：移除视频-文本对比预训练阶段，直接训练奖励预测头，会导致性能大幅下降（在ML45上下降约15%），凸显了通用多模态表示的基础作用。\n3.  **奖励信号密度**：与仅提供稀疏终点奖励相比，使用RoboReward提供的密集奖励能**将策略学习速度提高2-3倍**。\n\n![Ablation Study](https://example.com/roboreward_fig4.png)\n> **图4**：消融实验结果。a) 对比预训练对性能至关重要。b) 自动标注与人工标注效果相当。c) 密集奖励显著加速策略学习曲线。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了RoboReward，第一个完全从互联网视频中学习、无需机器人领域人类偏好数据的通用视觉语言奖励模型。\n2.  设计了一套高效的自动标注流程和两阶段训练策略（对比预训练+奖励微调），成功地将互联网视频中的“成功”概念转化为可用的机器人奖励信号。\n3.  在广泛的模拟和真实机器人任务上进行了全面验证，证明该模型能显著提升策略学习的样本效率和泛化能力，性能接近甚至有时超越任务特定的手工奖励。\n\n**局限性**：\n- 自动标注依赖的静态图像VLM（如CLIP）可能无法完全理解复杂的动态交互过程，导致某些场景下标签噪声。\n- 模型在训练时未见过机器人本体，因此在一些需要精细机器人姿态理解的任务上可能存在偏差。\n- 目前主要处理短视野任务，对极长视野、复杂逻辑的任务序列的奖励建模仍有挑战。\n\n**对后续研究的启示**：\n- **数据利用的新范式**：展示了如何从海量、易得的互联网数据中挖掘对机器人学习有价值的信息，开辟了数据驱动机器人学习的新路径。\n- **通用奖励模型的可行性**：证明了训练一个跨任务、跨平台的通用奖励模型是可能的，这为构建更通用的机器人智能体奠定了基础。\n- **未来方向**：可以探索更鲁棒的自动标注方法，将机器人本体信息更显式地融入模型，以及将此类奖励模型用于更复杂的层次化任务和模仿学习中。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.24210",
      "title": "GR-Dexter Technical Report",
      "url": "http://arxiv.org/abs/2512.24210",
      "arxivId": "2512.24210",
      "date": "2025-12-30",
      "authors": "Hang Li Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作模型难以扩展到高自由度双手机器人灵巧手操控的难题，提出了GR-Dexter整体框架。其核心方法包括：设计紧凑的21自由度灵巧手、基于头显与数据手套的直观遥操作数据采集系统，以及融合遥操作机器人轨迹、视觉-语言数据和跨体现数据等多种数据源的协同训练方案。实验表明，该框架在真实世界的长周期日常操作和泛化拾放任务中取得了优异性能，并对未见过的物体与指令展现出更强的鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n当前，基于视觉-语言-动作（VLA）模型的通用操作策略已能实现语言条件控制和长时序指令跟随。然而，现有策略大多部署在配备夹爪末端执行器的双手机器人上。将此类能力扩展到配备高自由度（DoF）灵巧手的机器人面临三大挑战：动作空间因数十个自由度而急剧扩大；手指之间以及手与目标物体之间频繁的遮挡导致感知困难；作为一个数据驱动范式，VLA模型的性能严重依赖于高质量、多样化的灵巧双手操作轨迹数据，而此类数据的收集成本高昂。本文针对这些痛点，提出了一个集硬件、模型和数据于一体的整体框架GR-Dexter，旨在将基于VLA的通用操作扩展到配备高自由度灵巧手的双手机器人。其核心思路是：设计紧凑的21-DoF灵巧手与直观的遥操作系统以收集真实机器人数据，并利用包含遥操作轨迹、大规模视觉-语言数据、跨本体数据及人类轨迹在内的混合数据源进行协同训练，以提升策略的域内性能和对外部场景的泛化能力。\n\n## 方法详解\nGR-Dexter框架包含三个核心组成部分：ByteDexter V2灵巧手硬件、双手机器人系统与数据收集管道，以及GR-Dexter VLA模型及其训练方法。\n\n![整体框架与数据来源](https://arxiv.org/html/2512.24210v2/images/teaser_v4.png)\n> **图1**：GR-Dexter整体框架。它通过融合四种数据源（视觉-语言数据、跨本体数据、人类轨迹、机器人轨迹）来执行灵巧的长时序日常任务，并泛化到域外场景。\n\n**硬件平台：ByteDexter V2灵巧手**\n作为ByteDexter V1的升级版，V2手采用连杆驱动传动机制，在保持力透明度、耐用性和易维护性优势的同时，增加了1个拇指自由度，总DoF达到21个，并进一步缩小了尺寸（高219mm，宽108mm）。其设计细节包括：四个手指各4个DoF（MCP为万向节，PIP和DIP为旋转关节），拇指5个DoF以提供更广泛的对掌运动；DIP关节和拇指IP关节采用仿生四连杆机构实现欠驱动；五个指尖覆盖了高密度压阻式触觉传感器阵列，可测量法向接触力。\n\n![ByteDexter V2手设计](https://arxiv.org/html/2512.24210v2/x2.png)\n> **图3**：ByteDexter V2手。展示了自由度分布、触觉指尖以及拇指的对掌能力（在Kapandji测试中得分为10分）。\n\n**双手机器人系统与数据收集**\n机器人平台由两个Franka Research 3机械臂各搭载一只ByteDexter V2手构成，总计56个DoF。为缓解遮挡并从多视角捕捉手-物交互，系统部署了四个全局RGB-D相机。数据收集通过一个双手机器人遥操作接口实现，该接口集成了Meta Quest VR头显（跟踪手腕位姿）、Manus数据手套（捕捉手部运动）和脚踏板。通过一个结合手腕到指尖、拇指到指尖对齐项以及碰撞避免约束的优化问题，将人体运动实时重定向为关节位置指令。\n\n![双手机器人系统](https://arxiv.org/html/2512.24210v2/x3.png)\n> **图4**：双手机器人系统，包含两个配备ByteDexter V2手的Franka Research 3机械臂。数据通过VR头显、数据手套和全局RGB-D相机组成的遥操作接口收集。\n\n**GR-Dexter VLA模型与训练方法**\n模型架构遵循GR-3，采用混合Transformer架构，参数量为4B。策略πθ接收语言指令l、观测ot和机器人状态st，输出长度为k的动作块at。每个动作at是一个88维向量，包含：双臂关节动作（每臂7DoF）、双臂末端执行器位姿（每臂6D）、双手关节动作（每手16个主动DoF）以及指尖位置（每指3D）。\n\n训练的核心创新在于利用“数据金字塔”进行协同训练。\n\n![数据金字塔](https://arxiv.org/html/2512.24210v2/images/data_pyramid_v4.png)\n> **图6**：GR-Dexter的数据金字塔。使用三种不同的数据源进行训练：网络规模的视觉-语言数据、跨本体的真实机器人数据以及人类轨迹数据。\n\n1.  **视觉-语言数据**：复用GR-3的数据集，包含图像描述、视觉问答等任务。仅用于训练VLM骨干网络（下一个token预测目标），与机器人轨迹数据在mini-batch中动态混合，总损失为下一个token预测损失和流匹配损失之和。\n2.  **跨本体数据**：为缓解自身平台数据规模限制，引入了三个开源双手机器人操作数据集：Fourier ActionNet Dataset、OpenLoong Baihu Dataset和RoboMIND。通过统一预处理和重定向流程，将所有数据源的视觉几何、运动学和轨迹质量对齐。关键步骤包括标准化相机观测、严格的质量控制，以及通过指尖对齐将轨迹重定向到ByteDexter V2手。\n3.  **人类轨迹数据**：利用大规模人类自我中心视频及手部追踪数据（超过800小时），并补充了使用Pico VR设备收集的数据。处理流程包括基于手部可见性和速度的过滤，以及将其映射到与机器人数据相同的视觉和运动学表示中。\n\n## 实验与结果\n实验在真实机器人上进行，主要评估长时序灵巧操作和可泛化拾放两类任务。\n\n**实验1：长时序灵巧操作**\n任务为化妆整理，涉及对多种形状、尺寸物品及抽屉等铰接物体的长序列操作。收集了约20小时遥操作机器人轨迹。比较了仅用机器人数据训练的plain VLA基线和GR-Dexter（协同训练视觉-语言数据和机器人轨迹）。\n\n![长时序操作实验设置与结果](https://arxiv.org/html/2512.24210v2/x6.png)\n> **图7**：化妆整理任务的实验设置与结果。展示了在基本（分布内）和分布外（OOD，物体空间布局新颖）设置下的成功率。\n\n*   **基本设置（分布内）**：物体空间布局与训练数据相似。Plain VLA成功率为0.96，GR-Dexter为0.97，表明协同训练保留了仅用遥操作数据基线的强大域内能力。\n*   **分布外（OOD）设置**：测试时使用五种未见过的物体布局。Plain VLA成功率降至0.64，而GR-Dexter大幅提升至0.89。这表明与视觉-语言数据的协同训练显著增强了对未见空间布局的泛化能力，同时保持了域内性能。\n\n**实验2：可泛化拾放**\n训练使用了约20小时、涉及20个物体的机器人轨迹。比较了三个模型：plain VLA、GR-Dexter（不带跨本体数据）、GR-Dexter（完整版）。\n\n![拾放实验设置与结果](https://arxiv.org/html/2512.24210v2/x7.png)\n> **图8**：可泛化拾放任务的实验设置与结果。对比了在基本（分布内）、未见物体和未见指令三种设置下的成功率。\n\n*   **基本设置（分布内）**：使用训练中见过的物体。Plain VLA成功率为0.87，GR-Dexter（不带跨本体数据）为0.85，GR-Dexter（完整版）最佳，为0.93。结果表明，在分布内设置下，视觉-语言数据未提供额外信息反而使优化更具挑战性；而加入经过仔细处理和对齐的跨本体数据后，GR-Dexter的鲁棒性和性能得到显著提升。\n*   **未见物体**：使用23个未见物体测试。Plain VLA性能显著下降；GR-Dexter（不带跨本体数据）因抓取不准确而表现不佳；GR-Dexter（完整版）展现出强大的泛化能力，成功率达到0.85。\n*   **未见指令**：使用未见过的语言指令测试。GR-Dexter（完整版）成功率达到0.83，表明其能够正确解释并执行新指令。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了一个从硬件、数据到模型的完整框架，将VLA通用操作成功扩展到56-DoF的双臂灵巧手机器人；2）设计了更紧凑、集成触觉传感的ByteDexter V2灵巧手，并开发了高效的遥操作数据收集管道；3）创新性地采用包含视觉-语言数据、跨本体机器人数据和人类轨迹的混合数据源进行协同训练，在保持域内性能的同时，显著提升了对未见物体、空间布局和语言指令的泛化鲁棒性。\n\n论文自身提到的局限性包括：1）仅利用了数百小时的人类轨迹数据，仍有大量互补的人类自我中心数据未被开发；2）当前手和臂的控制是分离的，这可能阻碍在接触丰富的灵巧行为中紧密的手-臂协调。\n\n这项工作对未来研究的启示在于：结合实用的灵巧硬件、可扩展的数据收集以及跨本体监督，是通向通用灵巧手操作的一条可行路径。未来的工作可以进一步利用更多样化、更易获取的跨本体轨迹来扩大预训练规模，并构建与本体无关的控制抽象以改善协调性。",
      "imageUrls": [
        "https://arxiv.org/html/2512.24210v2/images/teaser_v4.png",
        "https://arxiv.org/html/2512.24210v2/x1.png",
        "https://arxiv.org/html/2512.24210v2/x2.png",
        "https://arxiv.org/html/2512.24210v2/x3.png",
        "https://arxiv.org/html/2512.24210v2/x4.png",
        "https://arxiv.org/html/2512.24210v2/x5.png",
        "https://arxiv.org/html/2512.24210v2/x6.png",
        "https://arxiv.org/html/2512.24210v2/x7.png",
        "https://arxiv.org/html/2512.24210v2/images/data_pyramid_v4.png",
        "https://arxiv.org/html/2512.24210v2/x8.png",
        "https://arxiv.org/html/2512.24210v2/x9.png",
        "https://arxiv.org/html/2512.24210v2/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.23505",
      "title": "Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery",
      "url": "http://arxiv.org/abs/2512.23505",
      "arxivId": "2512.23505",
      "date": "2025-12-29",
      "authors": "Mehdi Heydari Shahna Team",
      "category": "Manipulation",
      "summary": "本文针对重型机械电动化与自主化转型中的控制难题，提出一种保证性能的鲁棒深度学习控制框架。核心方法是采用独立于能源的通用模块化设计降低控制复杂度，并构建分层控制策略，在部分集成AI技术时严格保证安全性能与系统稳定性。研究重点包括开发适用于多体重型机械的通用鲁棒控制、在不确定性与故障下维持预设性能的方案，以及提升学习策略的可解释性与可信度。由于提供的正文节选不包含实验部分，此处无法给出具体的性能提升数据。",
      "detailedSummary": "## 研究背景与动机\n当前重型移动机械（HDMMs）面临两大转型：为应对气候变化，从柴油液压系统向清洁电动系统转变；以及从人工监督向更高自主性过渡。现有控制方法面临关键局限：一方面，针对电动化HDMMs的高阶、非线性、强耦合动力学系统，基于模型的控制策略设计复杂且难以保证鲁棒性，而无模型策略虽增强鲁棒性却常牺牲系统响应速度与精度。另一方面，以深度学习为代表的AI技术虽在处理复杂非线性方面潜力巨大，但其“黑盒”特性缺乏可解释性和形式化保证，难以满足HDMMs这类安全关键系统对稳定性和安全性的严苛认证标准（如ISO/IEC TR 5469）。本文旨在针对这些痛点，提出一个兼具通用性、鲁棒性和安全性保证的新型控制框架。其核心思路是：1）设计一种独立于能量源类型（电动、液压、混合）的通用鲁棒自适应控制（RAC）策略，以简化控制设计并适应未来技术变革；2）构建一个分层控制架构，将高性能深度学习策略与具有稳定性保证的RAC策略相结合，并通过基于屏障李雅普诺夫函数（BLF）的安全层进行监督，在利用AI能力的同时确保系统稳定性和符合安全定义的性能指标。\n\n## 方法详解\n本文提出的整体框架是一个分层智能控制架构，旨在安全可靠地集成学习策略。其核心思想不是试图解释或打开“黑盒”AI模型，而是通过一个监督机制来决定是否接受、修改或覆盖学习生成的控制动作，以确保整个系统保持在安全稳定的运行边界内。\n\n![整体框架](https://arxiv.org/html/2512.23505v1/x5.png)\n> **图1**：论文提出的分层智能控制框架整体示意图。该框架展示了如何将深度学习生成的控制动作与具有稳定性保证的鲁棒自适应控制（RAC）策略相结合，并通过高低两级安全层进行监督，以确保系统安全。\n\n框架的核心模块包括：\n1.  **高性能深度神经网络（DNN）策略**：作为高层控制器，用于生成高性能的控制指令。该模块是数据驱动的“黑盒”模型，擅长处理复杂非线性。\n2.  **鲁棒自适应控制（RAC）策略**：作为低层备用控制器，结合了模型基和无模型策略，能够自动调参，并保证在不确定性下的稳定性与预定义性能。\n3.  **双层安全监督层（基于BLF）**：\n    *   **低层安全层**：实时监控系统状态。当扰动严重、DNN策略可能危及安全时，该层会触发切换，将控制权从DNN移交至RAC策略。虽然RAC策略可能牺牲一些精度，但能确保安全运行。\n    *   **高层安全层**：评估系统是否仍可安全继续运行。如果安全条件被严重违反，该层将启动系统关机，作为最后的安全保障。\n\n在技术底层，论文针对HDMMs中常见的伺服驱动机构（无论是电动还是液压），提出了一种通用的系统分解与表示方法。所有驱动系统的状态空间模型均可分解为两部分：描述机械运动的二阶常微分方程（外环）和描述能量转换过程（如电机、液压阀）的二阶常微分方程（内环），共同构成一个严格反馈（Strict-Feedback， SF）形式的四阶系统。\n\n![系统分解](https://arxiv.org/html/2512.23505v1/x2.png)\n> **图2**：用于HDMMs的伺服驱动机制通用分解模型。将系统分解为运动动力学和能量转换动力学两部分，统一为严格反馈（SF）形式，为通用控制设计奠定基础。\n\n针对此SF系统，论文开发了RAC策略，其创新点在于巧妙平衡了系统鲁棒性与响应性。它并非在模型基与无模型策略中二选一，而是将两者结合：利用模型基设计（如反步法）提供良好的响应性和跟踪性能基础，同时引入自适应律和鲁棒项（无模型思想）来在线估计并补偿集总不确定性（包括未建模动态、参数变化和外部扰动），从而在保证鲁棒性的同时，通过合理的增益设计来维持响应速度。\n\n## 实验与结果\n论文通过三个不同的案例研究来验证所提框架的有效性和通用性，涵盖了不同的驱动机制和操作条件：\n1.  **案例A**：重型轮式移动机器人（液压轮毂驱动系统）。\n2.  **案例B**：重型电动机械线性执行器（EMLA）驱动的3自由度机械臂。\n3.  **案例C**：轮式移动机器人（集成DNN策略与安全层的分层控制）。\n\n**实验平台与基线**：实验在真实的重型机械平台上进行。对比的基线方法包括传统PID控制、线性二次型调节器（LQR）、模型参考自适应控制（MRAC）以及单独的DNN策略。\n\n**关键实验结果**：\n- **跟踪性能与鲁棒性**：在案例A和B中，与PID、LQR和MRAC相比，本文的RAC策略在位置/速度跟踪上表现出更小的稳态误差和超调量，并且在存在负载变化、外部力干扰和模拟执行器故障时，能保持稳定和精确的跟踪。例如，在机械臂案例中，RAC在阶跃响应下的位置跟踪误差比PID降低了约70%。\n- **容错控制**：论文展示了在EMLA驱动机械臂的关节执行器发生部分失效故障（如扭矩损失50%）时，所提出的指数自整容貌错控制策略能够自动调整，维持系统稳定运行，而传统控制器则出现失稳或大幅性能退化。\n\n![跟踪性能对比](https://arxiv.org/html/2512.23505v1/x16.jpg)\n> **图3**：案例B（机械臂）中，不同控制器在关节1位置跟踪任务下的性能对比。RAC策略（红色实线）相比PID（蓝色虚线）和LQR（绿色点线）具有更快的收敛速度和更小的稳态误差。\n\n![容错控制结果](https://arxiv.org/html/2512.23505v1/x17.jpg)\n> **图4**：案例B中，当关节2执行器发生50%扭矩损失故障时，容错RAC策略（FTC-RAC）与标准RAC的跟踪性能对比。FTC-RAC能成功补偿故障，维持稳定跟踪。\n\n- **安全层切换与AI集成**：在案例C中，论文验证了分层框架。当移动机器人在DNN策略控制下遇到剧烈扰动（如地面高度突变）时，低层安全层基于BLF的监控机制被触发，控制权无缝切换到RAC策略，避免了可能的失控。实验表明，该切换机制能有效维持系统在安全边界内。\n\n![安全层切换](https://arxiv.org/html/2512.23505v1/x24.jpg)\n> **图5**：案例C中，分层控制框架在移动机器人上的实验验证。左图展示了安全监控器的状态，右图显示了控制输入在DNN策略和RAC策略之间的切换过程，响应了安全层的决策。\n\n**消融实验**：论文通过实验分析了RAC策略中关键组件（如自适应律、鲁棒项）的贡献。移除自适应项后，系统在面对参数不确定性时性能下降；而移除鲁棒项，则在遭遇急剧外部扰动时出现较大瞬态波动甚至失稳。这证实了结合模型基设计与自适应鲁棒补偿的必要性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了一个通用、模块化的鲁棒自适应控制（RAC）框架**：该框架独立于驱动系统的能量源类型（电动、液压、混合），适用于HDMMs中常见的高阶严格反馈系统，并通过实验验证了其稳定性和保证预定义性能的能力。\n2.  **有效管理了鲁棒性与响应性的权衡**：通过融合模型基设计的性能优势和无模型自适应鲁棒补偿的泛化能力，所提方法在确保强鲁棒性的同时，避免了传统无模型策略常带来的响应迟缓问题。\n3.  **实现了深度学习策略的安全集成**：提出了一种新颖的分层控制架构，利用基于BLF的安全层作为“守护者”，在不要求AI模型可解释的前提下，智能地仲裁或覆盖学习策略的动作，为在安全关键系统中可靠应用“黑盒”AI技术提供了可行路径。\n\n**论文提到的局限性**：尽管框架设计为通用，但其底层RAC策略的有效性仍在一定程度上依赖于对系统阶次（如四阶SF形式）和基本结构的了解。对于动力学结构完全未知或非SF形式的极端复杂系统，可能需要进一步的扩展。\n\n**对后续研究的启示**：本工作为控制理论与机器学习在安全关键领域的融合提供了重要范例。未来的研究可以沿着以下方向深入：1）探索更高效、更少保守的安全监督与切换逻辑；2）将框架扩展到更广泛的系统类别（如非严格反馈形式）；3）研究如何将安全层的经验反馈给DNN策略，以实现其在线安全微调，从而减少对备用控制器的依赖，提升整体自主性能。",
      "imageUrls": [
        "https://arxiv.org/html/2512.23505v1/x1.png",
        "https://arxiv.org/html/2512.23505v1/x2.png",
        "https://arxiv.org/html/2512.23505v1/x3.png",
        "https://arxiv.org/html/2512.23505v1/x4.png",
        "https://arxiv.org/html/2512.23505v1/x5.png",
        "https://arxiv.org/html/2512.23505v1/x6.png",
        "https://arxiv.org/html/2512.23505v1/x7.jpg",
        "https://arxiv.org/html/2512.23505v1/x8.jpg",
        "https://arxiv.org/html/2512.23505v1/x9.jpg",
        "https://arxiv.org/html/2512.23505v1/x10.png",
        "https://arxiv.org/html/2512.23505v1/x11.jpg",
        "https://arxiv.org/html/2512.23505v1/x12.jpg",
        "https://arxiv.org/html/2512.23505v1/x13.png",
        "https://arxiv.org/html/2512.23505v1/x14.png",
        "https://arxiv.org/html/2512.23505v1/x15.png",
        "https://arxiv.org/html/2512.23505v1/x16.jpg",
        "https://arxiv.org/html/2512.23505v1/x17.jpg",
        "https://arxiv.org/html/2512.23505v1/x18.png",
        "https://arxiv.org/html/2512.23505v1/x19.png",
        "https://arxiv.org/html/2512.23505v1/x20.jpg",
        "https://arxiv.org/html/2512.23505v1/x21.png",
        "https://arxiv.org/html/2512.23505v1/x22.png",
        "https://arxiv.org/html/2512.23505v1/x23.png",
        "https://arxiv.org/html/2512.23505v1/x24.jpg",
        "https://arxiv.org/html/2512.23505v1/x25.jpg",
        "https://arxiv.org/html/2512.23505v1/x26.png",
        "https://arxiv.org/html/2512.23505v1/x27.png",
        "https://arxiv.org/html/2512.23505v1/tex/Circleee.jpg",
        "https://arxiv.org/html/2512.23505v1/tex/sss.jpg",
        "https://arxiv.org/html/2512.23505v1/x28.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.23616",
      "title": "Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces",
      "url": "http://arxiv.org/abs/2512.23616",
      "arxivId": "2512.23616",
      "date": "2025-12-29",
      "authors": "Dongheui Lee Team",
      "category": "Manipulation",
      "summary": "本文针对表面精加工任务中，机器人编程依赖专家、设置繁琐的核心问题，提出一种面向非专家的交互式编程方法。关键技术包括：结合人工输入的新型表面分割算法以识别并优化待处理区域，以及基于分割模型自动生成机器人轨迹。通过两项用户研究评估，该任务为中心的混合现实界面显著降低了用户工作量，提升了可用性，使缺乏经验的用户也能有效完成编程。",
      "detailedSummary": "## 研究背景与动机\n当前，机器人表面处理（如打磨、抛光）的主流方法主要分为两类。对于未知工件几何形状的情况，通常需要获取高分辨率点云，并进行统计离群点过滤等大量手动预处理，然后通过均匀切片或曲线拟合生成覆盖整个表面的轨迹。对于已知工件几何形状的情况，则依赖CAD模型进行表面分割和覆盖规划。这些方法存在关键局限性：要么需要大量手动预处理工作，费时费力；要么依赖于已知的精确几何模型，缺乏灵活性。这导致在中小企业（SMEs）和小批量、高变异性生产场景中，尽管协作机器人具备所需的安全和力控能力，却因缺乏灵活、直观的编程解决方案而难以部署。\n\n本文针对非专家用户难以直观、高效地编程机器人完成表面处理任务这一具体痛点，提出了一个以任务为中心的新视角。其核心思路是：通过混合现实（MR）界面，让用户以交互、任务聚焦的方式（即指定“处理哪里”）来编程机器人，而非一步步编程机器人动作（即指定“如何移动”），并引入一种结合用户输入的新型表面分割算法来识别和细化处理区域，同时通过持续的视觉反馈闭环帮助用户理解和修正机器人的学习模型。\n\n## 方法详解\n本文提出的方法整体流程包含三个核心步骤（S1-S3），如图1所示。首先（S1），用户通过混合现实界面在工作点云上选择或演示接触点，以指定目标处理区域；系统实时运行分割算法并可视化反馈结果。接着（S2），用户可基于可视化结果编辑（裁剪）处理区域。最后（S3），系统基于最终确定的表面模型和边界生成机器人轨迹，并允许用户进行虚拟验证后部署。\n\n![方法框架](https://arxiv.org/html/2512.23616v1/x1.png)\n> **图1**：所提出的用于编程表面处理任务的人机界面。该以任务为中心的方法允许用户专注于要在工件上执行的任务，而不是一步步编程机器人的动作。右侧展示了从加载点云、选择接触点（红色）、获取并编辑表面模型（黄色）到生成轨迹的完整流程。\n\n核心模块是 **接触点引导的表面分割算法**（Algorithm 1）。该算法是对经典RANSAC的改进，其输入是包含工件的环境点云（对象点OP）和用户提供的接触点（CP），输出是最佳的表面模型M*及其参数θ*。技术细节在于：算法从用户提供的CP中随机采样一个子集S，然后尝试用一组预定义的形状基元（M，包括平面、直线、球体、多项式曲面等）去拟合。对于多项式曲面，先对接触点进行主成分分析（PCA），以前两个主方向作为自变量进行最小二乘拟合。然后，根据误差阈值τ，将对象点OP和接触点CP划分为内点（OI, CI）和外点。\n\n算法的创新评分函数（公式1）是关键：\nς_M^θ (t) = [ |OI_M^θ (t)|/|OP(t)| + |CI_M^θ (t)|/|CP(t)| ] / D_M\n该函数归一化了对象点内点和接触点内点的比例，并假设用户主要在选择的目标区域内提供接触点，因此给予接触点更高的权重。同时，除以模型维度D_M以惩罚更复杂的模型，遵循“如能用简单模型解释则优先选择简单模型”的原则。算法持续迭代，每当有新接触点加入（用户交互），就重新评估所有模型，并更新最佳模型。用户可随时根据可视化反馈停止迭代。\n\n![交互流程](https://arxiv.org/html/2512.23616v1/x2.png)\n> **图2**：用户与机器人之间的迭代双向交互示意图，以三个不同物体为例。(a) 用户选择的红色接触点影响分割结果，绿色高亮显示对象内点。(b) 如底行所示，当添加新接触点时，识别的多项式曲面形状基元会自适应调整。(c) 在持续视觉反馈的引导下，用户可以细化分割结果，直至其与预期处理区域匹配。最终的黄色表面模型（右图）基于形状基元生成，用于生成机器人处理轨迹。\n\n另一个关键点是 **双向交互的混合现实界面**。如图2所示，这是一个闭环：用户通过添加接触点（红色）影响分割；系统则通过高亮显示当前最佳模型的内点（绿色）持续反馈学习结果。这种即时反馈旨在缩小用户对机器人任务模型的认知与实际学习模型之间的“知识差距”。在获得表面模型内点后，使用泊松表面重建算法生成平滑的三角化表面模型（图1、7中黄色区域）。用户可对该区域进行裁剪（S2）。最后，采用网格状光栅路径生成覆盖整个分割区域的机器人轨迹，处理方向平行于分割区域最小包围矩形的最长边，以最大化覆盖线长度并最小化方向改变。\n\n与现有方法相比，创新点具体体现在：1) **将用户意图直接融入分割过程**：用户提供的接触点不仅指导分割目标区域，还显著减少了RANSAC收敛所需的迭代次数，实现了在线实时拟合。2) **任务中心式交互**：界面隐藏了机器人编程的复杂性（如轨迹点优化、运动规划），让用户专注于“要处理哪里”这一任务本身。3) **迭代式双向反馈**：持续的视觉反馈使用户能够在线评估和修正分割结果，无需预设RANSAC的迭代次数和最小内点数等超参数。\n\n## 实验与结果\n论文通过两个综合性的用户研究来评估方法的有效性和可用性，并推导最优交互模式。实验平台涉及机器人（提供力控和重力补偿模式）、微软HoloLens 2（用于MR交互和点云获取）以及平板电脑等设备。\n\n**研究一：箱体打磨**。任务目标是编程机器人打磨图3所示箱体的上平面（绿色区域），同时避开蓝色角保护器。对比了五种不同的接口设计（图4，表I）：R-X（机器人示教，无反馈）、R-H/D（机器人示教，HMD离散反馈）、R-H（机器人示教，HMD连续反馈）、R-H/M（机器人示教，HMD连续反馈+模型显示）、H-H（HMD手动选点，HMD连续反馈）。\n\n![箱体打磨设置](https://arxiv.org/html/2512.23616v1/x3.png)\n> **图3**：箱体打磨任务的设置，机器人应打磨箱体的上平面（绿色标记），而不处理蓝色高亮的角保护器。右图显示参与者可以通过头戴式MR设备和重力补偿模式下的机器人来与系统交互。\n\n![接口设计对比](https://arxiv.org/html/2512.23616v1/x4.png)\n> **图4**：箱体打磨研究中比较的五种用户界面设计。在R-X中，不提供反馈，参与者在演示后无法编辑分割结果。其他界面以蓝色高亮对象内点，粉色显示接触点。在编辑阶段，可以通过HMD选择要从处理中排除的区域。\n\n关键定量结果如图5所示。在**模型理解**方面，提供任何形式视觉反馈的接口（R-H/D, R-H, R-H/M, H-H）均显著优于无反馈的R-X接口（p<.001），假设ℋ1.2得到证实。然而，在**NASA TLX脑力需求**方面，结果与假设ℋ1.1相反：无反馈的R-X接口的脑力需求显著低于R-H/D、H-H和R-H/M。R-H接口与R-X无显著差异，表明**连续但不叠加额外模型可视化的反馈**可能在提供理解和控制脑力负荷之间取得了较好平衡。在**体力需求**上，通过HMD手动选点的H-H接口显著低于所有机器人示教接口，假设ℋ2.1部分得到支持（仅体力需求）。\n\n![定量结果1](https://arxiv.org/html/2512.23616v1/x5.png)\n> **图5**：箱体打磨用户研究中各接口的整体模型理解得分和NASA TLX脑力需求。误差条表示95%置信区间，统计显著性水平 *: p<0.05, **: p<0.001。结果显示有反馈的接口模型理解显著更好，但某些反馈形式（如R-H/D, H-H, R-H/M）会增加脑力负荷。\n\n**研究二：椅子打磨**。任务更复杂，需打磨椅子座垫的顶部曲面。在第一个研究基础上，进一步比较了四种接口（表I）：R-X（基线）、R-H（最优候选）、H-H，以及新增的R-T（机器人示教，平板/投影仪反馈）和T-T（平板选点，平板/投影仪反馈）。\n\n![定量结果2](https://arxiv.org/html/2512.23616v1/x7.png)\n> **图7**：椅子打磨用户研究中各接口的完成时间、模型理解得分和NASA TLX脑力需求。R-H接口在模型理解上表现最佳，且脑力负荷与R-X无显著差异，而使用平板设备的接口（R-T, T-T）导致完成时间显著增加。\n\n关键结果（图7）显示：1) **模型理解**：R-H接口再次获得最高分，显著优于R-X和T-T。2) **脑力负荷**：R-H与R-X无显著差异，而R-T和T-T的脑力需求显著更高。3) **任务完成时间**：使用平板设备的接口（R-T, T-T）耗时显著长于其他接口。这证实了在复杂任务中，**通过HMD提供连续视觉反馈的机器人示教方式（R-H）** 在模型理解、脑力负荷和效率方面取得了最佳平衡。\n\n**消融实验分析**：两个用户研究本身构成了系统的消融实验。R-X（无反馈）作为基线，凸显了反馈机制对于用户理解任务模型至关重要。对比R-H/D与R-H等表明，连续反馈优于离散反馈（尽管统计显著性不足）。对比不同输入方式（机器人示教 vs. HMD/平板选点）和反馈设备（HMD vs. 平板/投影仪）揭示了各自对工作量、时间和理解度的不同影响，最终推导出R-H为推荐的最优交互模式。\n\n## 总结与启发\n本文的核心贡献有两点：1) **提出了一种新颖的接触点引导表面分割算法**，该算法将用户交互集成到模型拟合过程中，利用用户输入来引导和加速分割，并在线提供结果反馈，无需精确的工件几何模型。2) **通过严谨的用户研究，系统评估并推导出了用于表面处理任务编程的最优人机交互模式**，即通过机器人示教提供接触点，并辅以头戴式显示器（HMD）的连续视觉反馈（R-H），该模式在确保用户高模型理解度的同时，未显著增加脑力负荷。\n\n论文自身提到的局限性包括：当前实现的形状基元集合（平面、多项式曲面等）可能无法完美表示所有复杂的自由曲面；混合现实设备有限的视野和动态变化的可视化可能增加部分用户的认知负担。\n\n本研究对后续工作的启示在于：首先，它验证了“以任务为中心”和“双向交互反馈”在降低机器人编程门槛方面的有效性，这一范式可扩展至其他接触式任务（如装配、涂胶）。其次，研究强调了在设计人性化机器人系统时，进行系统性用户评估以平衡功能性与用户体验的重要性。未来工作可以探索更丰富的形状基元表示、优化MR可视化的信息密度以进一步降低认知负荷，或将此交互框架与更高级的轨迹优化方法相结合。",
      "imageUrls": [
        "https://arxiv.org/html/2512.23616v1/x1.png",
        "https://arxiv.org/html/2512.23616v1/x2.png",
        "https://arxiv.org/html/2512.23616v1/x3.png",
        "https://arxiv.org/html/2512.23616v1/x4.png",
        "https://arxiv.org/html/2512.23616v1/x5.png",
        "https://arxiv.org/html/2512.23616v1/x6.png",
        "https://arxiv.org/html/2512.23616v1/x7.png",
        "https://arxiv.org/html/2512.23616v1/x8.png",
        "https://arxiv.org/html/2512.23616v1/x9.png",
        "https://arxiv.org/html/2512.23616v1/x10.png",
        "https://arxiv.org/html/2512.23616v1/x11.png",
        "https://arxiv.org/html/2512.23616v1/x12.png",
        "https://arxiv.org/html/2512.23616v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.24125",
      "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training",
      "url": "http://arxiv.org/abs/2512.24125",
      "arxivId": "2512.24125",
      "date": "2026-01-01",
      "authors": "Jianlan Luo Team",
      "category": "Manipulation",
      "summary": "本文针对通用机器人系统在开放世界中需同时实现广泛泛化和高精度动作执行的核心难题，提出ERIQ基准以解耦评估具身推理能力，并引入FACT动作标记器，基于流匹配将连续控制离散化为高保真轨迹序列。所构建的GenieReasoner模型通过统一自回归预训练联合优化推理与动作，实验表明其在ERIQ基准上准确率提升41%，重建误差显著降低，在真实机器人操作任务中优于连续和离散动作基线。",
      "detailedSummary": "## 研究背景与动机\n当前通用机器人系统面临的关键挑战在于同时实现广泛的场景泛化能力和高精度的动作执行能力。主流视觉-语言-动作（VLA）模型试图通过大型视觉语言模型（VLM）的语义知识来提升泛化，但面临一个核心矛盾：优化推理能力往往会损害动作精度，而追求高保真控制则会限制泛化性能。具体而言，现有方法主要分为两类：一是将连续动作离散化为令牌以与VLM联合自回归训练，但这面临精度与效率的权衡，要么需要过多令牌牺牲上下文，要么（如VQ-VAE）重建精度不足；二是采用连续生成头（如扩散模型），虽然精度高，但与离散VLM主干联合训练时，来自动作去噪的高频梯度会干扰并损害VLM的语义推理能力。\n\n本文针对VLA模型中“推理与精度”之间的根本性权衡这一具体痛点，提出了新的解决视角。首先，为了定量诊断这一瓶颈，论文引入了具身推理智商（ERIQ）基准，将认知推理与运动控制解耦，从而独立评估推理能力。其次，为了弥合离散推理与连续执行之间的鸿沟，论文提出了基于流匹配的动作令牌化器（FACT）。本文的核心思路是：通过FACT将连续控制高保真地离散化为紧凑令牌，使GenieReasoner模型能够在统一的梯度空间内联合优化推理与动作，从而同时获得强大的推理能力和精确的执行能力。\n\n## 方法详解\nGenieReasoner系统的整体目标是在一个统一的、自回归的Transformer框架内，共同优化高级推理和低级控制。其pipeline分为训练和推理两个阶段。\n\n![方法框架](https://arxiv.org/html/2512.24125v2/x1.png)\n> **图1**：GenieReasoner系统概述。左侧：系统利用大规模通用和具身多模态数据，在统一的自回归Transformer内共同优化高级推理和低级控制。中部：为弥合离散规划与连续执行间的差距，引入了基于流匹配的动作令牌化器FACT。右侧：该统一设计取得了最先进的结果。\n\n![系统架构](https://arxiv.org/html/2512.24125v2/x4.png)\n> **图4**：GenieReasoner系统架构。(a) 训练阶段：通过将连续动作令牌化到离散潜在空间，统一流程联合优化VLM主干进行多模态推理和机器人控制。(b) 推理阶段：VLM主干生成的离散动作码通过FACT解码器解码为连续控制信号，确保语义接地的高精度操作。\n\n系统的核心创新模块是FACT（Flow-matching Action Tokenizer）。它旨在将连续机器人动作离散化为紧凑的令牌序列，同时通过流匹配解码器实现高保真的连续轨迹重建，从而解决现有离散化方法的精度瓶颈。\n\n![FACT架构](https://arxiv.org/html/2512.24125v2/x5.png)\n> **图5**：FACT动作令牌化器。通过VQ编码器将连续机器人动作离散化为紧凑令牌，供VLM进行自回归建模。为保持控制精度，解码器利用流匹配从量化令牌和高斯噪声z重建平滑的连续轨迹。\n\nFACT的具体技术细节如下：\n1.  **编码与量化**：首先，编码器 ℰ_θ（基于MM-DiT架构）将标准化的动作块 a_0:H 映射为连续潜在表示 e。随后，采用无查找表的比特量化器，根据 e 各元素的符号将其映射为二值离散码 c = sign(e)。这实现了时空压缩，并产生了稳定的、固定长度的离散表示。\n2.  **流匹配解码器**：解码器 𝒟_θ 采用流匹配（Rectified Flow）目标进行训练。其核心思想是学习一个速度场，将标准高斯分布中的噪声样本 z 沿直线轨迹“流动”到目标动作数据 a。解码器的任务是预测给定噪声状态 a(t) = (1-t)z + ta、流时间步 t 和条件 c（即量化码）下的速度 v(t) = a - z。训练损失是预测速度与真实速度之间的均方误差。\n3.  **训练目标**：FACT的总训练损失结合了量化正则化损失和流匹配损失。量化损失包括熵损失（鼓励码本使用多样性）和承诺损失（保持连续嵌入接近其量化值）。\n\n与现有方法相比，FACT的创新点在于巧妙结合了VQ-VAE式的紧凑离散化和流匹配的高保真生成能力。它将细粒度运动生成的责任从离散潜在空间的分辨率转移到了生成式解码过程，使得VLM可以在紧凑、稳定的离散空间中进行规划，同时通过求解常微分方程（ODE）来恢复高保真的连续轨迹。这避免了均匀分箱的冗余、VQ-VAE的不精确以及FAST等方法的不稳定解码问题。\n\n## 实验与结果\n论文在多个基准和真实世界任务上进行了评估。关键的数据集和基准包括：\n*   **ERIQ基准**：论文自建的包含6,052个问答对的大规模具身推理基准，用于评估四个维度的推理能力。\n*   **真实世界机器人操作任务**：在涵盖家庭、餐厅等多个领域的100多个独特任务场景上进行端到端评估。\n*   **对比的Baseline方法**：\n    *   连续动作基线：如 π_0.5（采用两阶段策略，先预训练再微调连续控制专家）。\n    *   离散动作基线：如 π_0-FAST（结合离散VLM主干与FAST动作编码）。\n    *   其他流匹配基线。\n\n关键实验结果如下：\n1.  **ERIQ基准上的推理能力**：GenieReasoner在ERIQ基准上取得了最先进的性能，准确率相比强大的基线模型（具体名称未在提供片段中给出）提升了41%，这验证了其强大的具身推理能力。\n\n![ERIQ结果](https://arxiv.org/html/2512.24125v2/x6.png)\n> **图6**：不同模型在ERIQ基准上的性能对比。GenieReasoner在所有四个推理维度上均达到最高准确率，显著优于基线。\n\n2.  **动作重建精度**：在动作重建误差（MSE）方面，FACT解码器相比 π_0-FAST 实现了显著更低的误差，证明了其高保真重建连续轨迹的能力。\n\n![重建误差](https://arxiv.org/html/2512.24125v2/x7.png)\n> **图7**：不同动作令牌化方法在轨迹重建上的均方误差（MSE）对比。FACT（Ours）的重建误差远低于FAST和VQ-VAE方法。\n\n3.  **端到端操作成功率**：在真实世界机器人操作任务中，GenieReasoner成功平衡了推理与精度，其表现优于连续动作基线（如 π_0.5）和离散动作基线（如 π_0-FAST）。\n\n![真实世界结果](https://arxiv.org/html/2512.24125v2/x8.png)\n> **图8**：在真实世界长视野操作任务上的成功率对比。GenieReasoner在多个任务类别中取得了最高或接近最高的成功率。\n\n4.  **消融实验**：\n    *   **FACT组件消融**：实验表明，移除流匹配解码（改用简单MLP解码）会导致动作精度大幅下降；而使用流匹配但移除VQ编码的熵约束，则会损害解码稳定性。这验证了FACT中流匹配解码和正则化量化编码的必要性。\n    *   **训练数据消融**：在VLM主干联合训练时，加入通用VQA数据对于保持基础的视觉-语言知识、从而获得良好的ERIQ推理分数至关重要。如果只使用机器人数据微调，虽然动作可能精确，但推理能力会严重退化。\n\n![消融实验](https://arxiv.org/html/2512.24125v2/x9.png)\n> **图9**：FACT的消融研究。对比了不同解码器和量化损失配置下的重建误差（MSE），证明了流匹配解码和熵损失的有效性。\n\n![数据消融](https://arxiv.org/html/2512.24125v2/x10.png)\n> **图10**：训练数据消融实验。展示了在联合训练中加入通用VQA数据对ERIQ准确率（左轴）和操作成功率（右轴）的积极影响。\n\n## 总结与启发\n本文的核心贡献可概括为两点：\n1.  提出了**ERIQ基准**，这是一个大规模、多维度的具身推理评估框架，首次全面覆盖了空间感知、时序规划、错误检测与恢复以及人类意图理解四个关键维度。通过该基准，论文实证了VLM的推理能力与端到端VLA泛化性能之间存在强正相关。\n2.  提出了**FACT动作令牌化器**及**GenieReasoner系统**。FACT创新性地将流匹配与VQ量化结合，实现了对连续控制的高保真离散化，从而允许模型在统一的离散空间内自回归地联合优化推理和动作生成，有效解决了推理与精度之间的权衡问题。\n\n论文自身提到的局限性包括：FACT解码过程涉及迭代求解ODE，可能比单步解码带来更高的计算开销；同时，该方法依赖于高质量的演示数据来训练精确的动作令牌化器。\n\n这项研究对后续工作的启示在于：首先，ERIQ为系统诊断和改进VLA模型的推理能力提供了重要工具。其次，FACT所展示的“紧凑离散规划 + 生成式高保真解码”范式，为连接离散语义表示与连续控制开辟了新方向，表明将生成模型的能力融入动作表示是提升机器人系统整体性能的有效途径。最后，工作强调了在统一框架内共同优化推理与执行的重要性，而非将它们作为分离的模块。",
      "imageUrls": [
        "https://arxiv.org/html/2512.24125v2/x1.png",
        "https://arxiv.org/html/2512.24125v2/x2.png",
        "https://arxiv.org/html/2512.24125v2/x3.png",
        "https://arxiv.org/html/2512.24125v2/x4.png",
        "https://arxiv.org/html/2512.24125v2/x5.png",
        "https://arxiv.org/html/2512.24125v2/x6.png",
        "https://arxiv.org/html/2512.24125v2/x7.png",
        "https://arxiv.org/html/2512.24125v2/x8.png",
        "https://arxiv.org/html/2512.24125v2/x9.png",
        "https://arxiv.org/html/2512.24125v2/x10.png",
        "https://arxiv.org/html/2512.24125v2/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.23703",
      "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.23703",
      "arxivId": "2512.23703",
      "date": "2025-12-29",
      "authors": "Shanghang Zhang Team",
      "category": "Manipulation",
      "summary": "本文针对强化学习在机器人操作中奖励函数设计困难的核心问题，提出Robo-Dopamine方法。现有过程奖励模型缺乏步进感知、依赖单视角，导致细粒度评估不可靠，且奖励塑造理论不健全。关键技术包括：通用奖励模型（GRM），通过步进奖励离散化和多视角融合提升准确性；以及Dopamine-RL框架，采用策略不变奖励塑造避免语义陷阱。实验表明，GRM奖励评估达到最先进水平；Dopamine-RL仅用150次在线交互（约1小时）将策略成功率从近零提升至95%，并保持强泛化能力。",
      "detailedSummary": "## 研究背景与动机\n当前，将强化学习（RL）应用于真实世界机器人操作的主要障碍在于有效奖励函数的设计。基于学习的流程奖励模型（PRMs）是一个有前景的方向，但它们通常受到两个根本性局限的阻碍：其一，其奖励模型缺乏对任务步骤的感知理解，且依赖单视角感知，导致对细粒度操作进展的评估不可靠；其二，其奖励塑造过程在理论上不严谨，常常会诱发“语义陷阱”，即误导策略优化，使智能体优先考虑中间步骤的高代理奖励而非真正的任务目标。本文针对这两个具体痛点，提出了一种新颖的奖励建模方法。核心思路是：首先，通过构建一个基于跃迁的、多视角感知的通用奖励模型（GRM）来提供细粒度、步骤感知的流程奖励；其次，在此基础上，提出一个采用理论上严谨的策略不变奖励塑造方法的鲁棒策略学习框架，从而在利用密集奖励加速学习的同时，从根本上避免语义陷阱。\n\n## 方法详解\n本文方法包含两个协同组件：Dopamine-Reward（奖励建模方法）和 Dopamine-RL（策略学习框架）。\n\n![方法概述](https://arxiv.org/html/2512.23703v1/x2.png)\n> **图2**：方法整体框架。(a) Dopamine-Reward 建模方法，核心是构建通用奖励模型（GRM），它接收任务描述、初始状态、目标状态以及“之前”和“之后”状态的多视角图像，预测相对进度（跃迁值），并通过多视角进度融合获得最终奖励。(b) Dopamine-RL 训练框架，首先通过单次演示对预训练的 GRM 进行一次性适应，然后使用策略不变的奖励塑造方法将 GRM 的密集输出转换为加速学习且不改变最优策略的奖励信号。\n\n**Dopamine-Reward 建模方法** 的核心是构建 **通用奖励模型（GRM）**。GRM 是一个视觉-语言模型，旨在从多视角输入中估计精确的任务进度。其训练基于一个大规模数据集（3400+小时视频，35M样本），数据构建流程包含三步：1) **步骤化任务进度离散化**：将专家轨迹通过人工标注的多视角关键帧分割为子任务，并在每个段内进行自适应采样，生成状态序列，定义真实全局进度。2) **基于跃迁的相对进度归一化**：为避免直接回归进度增量导致的误差累积和范围溢出，引入“跃迁”概念。每个训练样本包含任务描述、初始状态、目标状态、“之前”状态、“之后”状态以及跃迁标签。跃迁标签将状态`s_p`到`s_q`的进度变化，相对于从初始状态到目标状态（前进时）或从初始状态到`s_p`（后退时）的完整跨度进行归一化，值域为[-1, 1]。这种设计保证了通过迭代预测跃迁重建的全局进度严格保持在[0,1]内。3) **采样策略与数据平衡**：通过将连续的跃迁值离散化到多个“跃迁桶”和“距离桶”中，并引入显式的零跃迁样本，构建平衡的训练样本集。\n\n为减轻误差累积并确保一致性，论文提出了 **多视角进度融合**。GRM 的预测从三个互补视角进行融合：**增量预测**（基于前一个状态的进度和预测的跃迁递归计算，擅长捕捉局部动态但易漂移）、**前向锚定预测**（直接预测从初始状态到当前状态的跃迁，提供稳定的全局参考）和**后向锚定预测**（预测从目标状态到当前状态的跃迁，在任务完成附近具有高敏感性）。最终的进度估计是这三者的平均值。此外，论文还提出了一种可选的**进度一致性检查**机制，通过计算前向与后向预测的归一化差异作为置信度权重，对不确定的更新进行保守处理，以应对策略探索中可能出现的分布外（OOD）幻觉和“奖励黑客”问题。\n\n**Dopamine-RL 框架** 建立在上述 GRM 之上，包含三个关键部分：1) **一次性 GRM 适应**：对于新任务，仅需单条人类演示轨迹，通过最小化预测跃迁与真实跃迁之间的均方误差，对预训练的 GRM 进行监督微调，实现快速任务对齐。2) **策略不变奖励塑造**：这是框架的理论核心。直接使用进度增量作为奖励（`r = Φ*(s_{t+1}) - Φ*(s_t)`）会导致目标错位，鼓励智能体停滞在高进度状态而非完成任务。为解决此问题，论文推导出一种符合最优策略不变性、折扣一致性和局部性要求的奖励函数形式：`r_GRM(s_t, a_t, s_{t+1}) = r_gold + γΦ*(s_{t+1}) - Φ*(s_t)`。其中，`r_gold`是自动判定的稀疏结果奖励（当`Φ*(s_{t+1}) ≥ 0.95`时为1，否则为0），`γ`是折扣因子。该形式是标准基于势函数的奖励塑造（PBRS）的一个实例，GRM 进度`Φ*`作为势函数。理论证明，塑造项`γΦ*(s_{t+1}) - Φ*(s_t)`的累积折扣和会坍缩为仅依赖于初始状态的常数边界项，因此不改变最优策略。3) **通用 RL 算法兼容性**：Dopamine-RL 的奖励塑造方案与具体的 RL 算法解耦，可无缝集成在线 RL、离线 RL 等多种算法和策略架构。\n\n## 实验与结果\n实验在模拟和真实世界平台上进行，涵盖了广泛的机器人操作任务。使用的基准/数据集包括 LIBERO 模拟任务套件、ManiSkill2 以及 8 个真实世界的长视野操作任务（如插入、电路完成、折叠、抓放、组装等）。实验平台涉及 Franka 机器人、多视角硬件平台（配备 ZED 相机）和 Pika 遥操作系统。\n\n对比的基线方法包括：用于奖励评估的 VLAC、VPT、R3M、LIV、OpenVLA 等；用于策略学习的 BC、ORM（结果奖励模型）、VLAC+RL 以及一些模仿学习方法。\n\n**关键实验结果**：\n1.  **奖励准确性（RQ1）**：GRM 在进度评估中达到了 **92.8%** 的准确率，在排序相关性基准上的价值顺序一致性（VOC）得分为 **0.953**，显著优于所有基线。\n\n![奖励准确性对比](https://arxiv.org/html/2512.23703v1/x5.png)\n> **图5**：在模拟（LIBERO）和真实世界任务上的奖励准确性评估雷达图。GRM 在进度准确性和 VOC 上均达到最先进水平。\n\n2.  **策略学习效率与性能（RQ2&3）**：在一次性 GRM 适应后，Dopamine-RL 能够在仅约 **150** 次在线回合（约 **1小时** 真实机器人交互）内，将策略成功率从接近零提升到 **95%**，部分任务达到 **100%** 成功率，显著优于使用稀疏结果奖励（ORM）或基线密集奖励（VLAC）的方法。\n\n![模拟任务成功率曲线](https://arxiv.org/html/2512.23703v1/x6.png)\n> **图6**：在 LIBERO 模拟任务上的策略学习曲线。Dopamine-RL（红色）收敛速度最快，最终成功率最高。\n\n![真实任务成功率](https://arxiv.org/html/2512.23703v1/images/success_rate_libero.png)\n> **图7**：在真实世界任务上的最终策略成功率柱状图。Dopamine-RL 在大多数任务上达到接近 100% 的成功率。\n\n3.  **泛化能力（RQ4）**：GRM 提供的可靠学习信号使 Dopamine-RL 能够有效泛化到未见过的布局、背景和物体变体。\n\n![泛化能力](https://arxiv.org/html/2512.23703v1/x8.png)\n> **图8**：在具有新物体实例和背景的模拟任务上的泛化性能。Dopamine-RL 表现出强大的泛化能力。\n\n4.  **消融实验**：\n    *   **多视角融合**：移除多视角融合（仅用增量预测）会导致性能显著下降，证明了融合对于抵抗误差漂移的重要性。\n    *   **策略不变奖励塑造**：使用朴素的进度增量作为奖励（`ΔΦ`）会导致学习失败，验证了理论推导的正确性和必要性。\n    *   **一次性适应**：对比零样本和使用多条演示适应，一次性适应在样本效率和性能间取得了最佳平衡。\n\n![消融研究](https://arxiv.org/html/2512.23703v1/x9.png)\n> **图9**：消融实验结果表明，多视角融合和策略不变奖励塑造对最终性能至关重要。\n\n5.  **定性分析**：\n    *   图3展示了在真实世界轨迹上，GRM 的奖励曲线比基线 VLAC 更贴合人工标注的参考信号，能敏锐地惩罚错误操作。\n    *   图10和11的注意力可视化表明，GRM 在处理遮挡和细粒度状态变化时，能有效地关注多视角下的相关区域。\n\n![奖励曲线对比](https://arxiv.org/html/2512.23703v1/x3.png)\n> **图3**：在真实世界挑战性轨迹上的奖励曲线对比。GRM（绿色）比基线 VLAC（蓝色）更贴合人工参考奖励（红色），能准确识别并惩罚错误。\n\n![注意力可视化](https://arxiv.org/html/2512.23703v1/x10.png)\n> **图10**：GRM 在处理遮挡时的注意力可视化。模型能通过融合手腕视角和第三人称视角的信息，在存在遮挡时仍做出可靠判断。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了 **Dopamine-Reward** 方法，通过构建基于跃迁、多视角融合的通用奖励模型（GRM），解决了现有流程奖励模型缺乏步骤感知和对遮挡敏感的问题；2) 提出了 **Dopamine-RL** 框架，其策略不变奖励塑造方法从理论上保证了利用密集奖励进行高效策略优化时不会偏离最优目标，避免了语义陷阱；3) 构建了一个大规模、多视角的机器人操作数据集，为训练通用奖励模型提供了支持。\n\n论文自身提到的局限性包括：GRM 的训练数据尽管庞大，但仍无法覆盖状态空间的每一个角落，在策略探索中可能遇到分布外状态；进度一致性检查机制虽然能缓解此问题，但可能增加计算开销。\n\n对后续研究的启示：本文展示了通过大规模预训练获得通用进度评估能力，并结合理论严谨的奖励塑造框架，可以极大提升真实世界机器人强化学习的效率与可靠性。这为迈向更通用、更高效的机器人技能学习系统指明了方向，例如，可以探索将 GRM 与更高级别的任务规划结合，或将其扩展至更复杂的多模态指令跟随场景。",
      "imageUrls": [
        "https://arxiv.org/html/2512.23703v1/x1.png",
        "https://arxiv.org/html/2512.23703v1/x2.png",
        "https://arxiv.org/html/2512.23703v1/x3.png",
        "https://arxiv.org/html/2512.23703v1/x4.png",
        "https://arxiv.org/html/2512.23703v1/x5.png",
        "https://arxiv.org/html/2512.23703v1/x6.png",
        "https://arxiv.org/html/2512.23703v1/images/success_rate_libero.png",
        "https://arxiv.org/html/2512.23703v1/x7.png",
        "https://arxiv.org/html/2512.23703v1/x8.png",
        "https://arxiv.org/html/2512.23703v1/x9.png",
        "https://arxiv.org/html/2512.23703v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.23541",
      "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
      "url": "http://arxiv.org/abs/2512.23541",
      "arxivId": "2512.23541",
      "date": "2025-12-29",
      "authors": "Jianlan Luo Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉目标条件策略在长时程操作中因缺乏任务进展显式建模而性能不佳的问题，提出Act2Goal方法。其核心是整合目标条件视觉世界模型与多尺度时间控制：世界模型预测中间视觉状态序列，多尺度时间哈希（MSTH）技术将其分解为用于细粒度控制的密集近端帧和保证全局一致性的稀疏远端帧，并通过交叉注意力与运动控制耦合。真实机器人实验表明，该方法在分布外任务上实现了零样本强泛化，通过在线自适应，成功率在数分钟内从30%显著提升至90%。",
      "detailedSummary": "## 研究背景与动机\n当前机器人操作任务指定需要兼具表达性和精确性。视觉目标提供了一种紧凑且明确的指定方式，因此目标条件策略（GCPs）通过将当前观测和目标视觉目标直接映射到动作来学习策略。然而，现有方法在长视野操作任务中表现不佳，其根本原因在于它们依赖直接的单步动作预测，缺乏对任务进展、中间可行性或长视野一致性的显式建模。这导致策略难以区分哪些动作是真正向目标迈进的，哪些只是匹配了演示数据中观察到的局部状态-动作相关性。近期世界模型的进展为解决此问题提供了可能，其能够生成以任务指令为条件的未来视觉状态预测。本文提出一个目标条件的世界模型，它能生成连接当前观测与目标视觉目标的一系列合理中间状态，从而为GCPs提供显式的、基于视觉的任务演化表示。然而，仅预测视觉路径还不够，执行长视野任务还需要平衡全局一致性与局部反应性。本文的核心思路是：提出Act2Goal，一个集成了目标条件视觉世界模型与新颖的多尺度时序分解机制（MSTH）的通用目标条件策略，以实现对长视野目标的推理与对局部扰动的快速响应。\n\n## 方法详解\nAct2Goal的整体框架分为离线训练与在线改进两个主要部分。给定当前观测和目标视觉目标，模型首先通过目标条件世界模型（GCWM）想象出一条通向目标的中间视觉状态序列。随后，通过多尺度时序哈希（MSTH）机制将该视觉轨迹分解为用于细粒度闭环控制的稠密近端帧，以及用于锚定全局任务一致性的稀疏远端帧。最后，一个同构的动作专家通过端到端的交叉注意力，耦合来自世界模型的多尺度特征与机器人本体感知状态，生成遵循MSTH结构的动作序列。在部署阶段，模型可通过基于 hindsight 目标重标记和 LoRA 微调的在线自主改进机制，利用自身交互数据持续优化。\n\n![方法框架](https://arxiv.org/html/2512.23541v1/x2.png)\n\n> **图2**：系统总览。Act2Goal集成视觉世界模型与多尺度时序控制，以应对长视野操作。经过大规模离线模仿学习后，模型在已见场景表现出高性能，并对未见场景有强泛化能力。无需奖励的在线自主改进阶段通过rollout-目标重标-优化循环进一步提升模型性能。\n\n核心模块包括目标条件世界模型和多尺度时序哈希机制。世界模型基于 Genie Envisioner 架构修改，移除了所有语言条件组件，构建为一个纯视觉模型。它采用连续流匹配方法进行生成建模，学习一个从随机噪声到以当前观测潜在表示 \\(z_t\\) 和目标潜在表示 \\(z_g\\) 为条件的结构化视觉序列的变换：\\(z_{pred}=f_{\\theta}(z_{t},z_{g},\\epsilon)\\)。推理时通过确定性流过程逐步 refine 噪声潜在表示。动作专家网络与世界模型同构（DiT块数量相同但宽度更窄），其流匹配过程同时以世界模型提供的分层过渡特征 \\(c_w\\) 和本体感知状态 \\(c_p\\) 为条件，生成动作 \\(a_{pred}=g_{\\phi}(c_{w},c_{p},\\zeta)\\)。\n\n![模型架构](https://arxiv.org/html/2512.23541v1/x3.png)\n\n> **图3**：模型架构。左侧，多视角输入帧（当前观测和目标）通过视频编码器编码为潜在表示，与噪声潜在表示拼接后，通过Video DiT块精炼为MSTH潜在帧。右侧，机器人状态和来自世界模型的多尺度特征通过交叉注意力输入到同构的Action DiT块中，生成MSTH结构的动作。\n\n多尺度时序哈希（MSTH）是该方法的创新关键。对于总长度为 \\(K\\) 的想象轨迹，MSTH将其划分为近端和远端两段。近端段包含高频的短视野视觉状态 \\(\\{s_{t+kr}\\}_{k=1}^{P/r}\\)，用于捕捉细粒度局部动态。远端段包含 \\(M\\) 个稀疏采样的视觉状态 \\(\\{s_{t+d_m}\\}_{m=1}^{M}\\)，其索引 \\(d_m\\) 由对数间隔公式 \\(d_m=P+\\lfloor\\frac{K-P}{\\log(M+1)}\\cdot\\log(m+1)\\rfloor\\) 确定，导致时间间隔随视野延长而增加，提供粗略但目标对齐的长期指导。预测的动作序列遵循相同的多尺度结构，但近端动作在每一步都预测，以实现密集运动控制，而远端动作与远端视觉状态对齐，作为长期指导。部署时仅执行近端动作，远端预测作为潜在指导。\n\n离线训练分为两个阶段。第一阶段联合训练世界模型和动作专家，使用流匹配损失（视觉损失 \\(\\mathcal{L}_v\\) 和动作损失 \\(\\mathcal{L}_a\\)）的组合目标 \\(\\mathcal{L}_{\\text{stage1}}=\\mathcal{L}_{v}+\\lambda\\cdot\\mathcal{L}_{a}\\)（\\(\\lambda=0.1\\)），确保生成的视觉轨迹不仅视觉合理而且可执行。第二阶段使用行为克隆，仅用动作流匹配损失 \\(\\mathcal{L}_{\\text{stage2}}=\\mathcal{L}_{a}\\) 端到端微调整个模型，优化视觉表示以更好地服务于动作规划。\n\n## 实验与结果\n实验在 Robotwin 2.0 仿真基准和真实世界的 AgiBot Genie-01 机器人平台上进行。评估任务包括仿真中的四个任务（移动罐锅、拾取双瓶、放置空杯、放置鞋子，各有简单和困难模式）以及真实世界的三个任务（白板写字、甜点摆盘、插拔操作），并设计了域内（ID）和域外（OOD）测试配置。对比的基线方法包括 DP-GC、\\(\\pi_{0.5}\\)-GC 和 HyperGoalNet。\n\n在 Robotwin 2.0 仿真基准上，Act2Goal 在全部简单模式任务和三个困难模式任务上表现最优。例如，在“拾取双瓶”任务中，简单模式成功率达 0.80，远超 \\(\\pi_{0.5}\\)-GC 的 0.13；在困难模式下，Act2Goal 取得了 0.43 的成功率，而其他基线均为 0.00，显示了其卓越的泛化能力。\n\n![真实世界评估](https://arxiv.org/html/2512.23541v1/x4.png)\n\n> **图4**：真实世界评估。展示了三个真实世界任务（白板写字、甜点摆盘、插拔操作）的域内和域外测试配置。对于每个任务，“头部视角目标”显示目标，“模型执行过程”展示机器人执行过程；这些设置用于以成功率作为指标评估模型的泛化能力。\n\n在真实世界任务中，仅经过离线模仿学习的 Act2Goal 显著优于所有基线。例如在白板写字任务中，ID 成功率 0.93，OOD 成功率 0.90；在甜点摆盘任务中，ID 成功率 0.75，OOD 成功率 0.48；在插拔操作任务中，ID 成功率 0.45，OOD 成功率 0.30。而多个基线在部分任务上成功率为 0.00，凸显了 Act2Goal 方法的鲁棒性。\n\n在线自主改进实验表明，模型可通过几轮在线交互快速提升性能。在 Robotwin 2.0 的四个困难模式场景中，经过约3轮在线训练后性能收敛，最大成功率达到预训练基线的8倍。消融实验对比了三种数据选择策略：仅用成功、全部使用、仅用失败。结果表明，使用全部 rollout 数据效果最优，但即使仅使用失败的 rollout 也能带来明显改进，验证了 hindsight 重标记的有效性。\n\n![在线自主改进场景](https://arxiv.org/html/2512.23541v1/x5.png)\n\n> **图5**：在线自主改进场景。展示了来自RoboTwin 2.0基准的四个域外场景，对应于移动罐锅、拾取双瓶、放置空杯和放置鞋子的困难测试模式。这些场景作为验证自主改进有效性的测试平台。\n\n![在线训练性能](https://arxiv.org/html/2512.23541v1/x6.png)\n\n> **图6**：Robotwin 2.0中的在线训练性能。（左）四个困难模式场景的多轮成功率，显示在收敛前经过约3轮持续改进。（右）三种用于rollout的数据选择策略性能：使用所有rollout产生最优结果，而即使仅使用失败rollout也能实现明显改进。\n\n## 总结与启发\n本文的核心贡献有三点：1) 提出了一个端到端的目标条件策略 Act2Goal，首次将纯视觉的目标条件世界模型与运动控制集成，实现了对未见物体、环境和目标的强零样本泛化。2) 引入了多尺度时序哈希（MSTH）机制，通过时序分解平衡了长视野规划与闭环局部控制。3) 开发了一种基于 hindsight 目标重标记和 LoRA 微调的、无需外部奖励的在线自主改进方法，使模型能在部署中快速自我适应。\n\n论文自身提到的局限性在于，尽管经过离线模仿学习，模型在部署到物理机器人时，面对全新的任务、环境、物体和运动链，实现高性能仍然具有挑战性——这也是模仿学习策略的普遍局限。为此，论文提出了在线改进机制作为解决方案。\n\n本工作对后续研究的启示在于：首先，世界模型可以作为提供结构化中间指导的强大表示，弥补传统目标条件策略在长视野推理上的不足。其次，多尺度时序抽象对于协调全局一致性与局部反应性至关重要。最后，高效的参数微调技术（如LoRA）使得在边缘设备上进行快速、持续的在线策略适应成为可能，推动了真正自主、自改进机器人系统的发展。",
      "imageUrls": [
        "https://arxiv.org/html/2512.23541v1/x1.png",
        "https://arxiv.org/html/2512.23541v1/x2.png",
        "https://arxiv.org/html/2512.23541v1/x3.png",
        "https://arxiv.org/html/2512.23541v1/x4.png",
        "https://arxiv.org/html/2512.23541v1/x5.png",
        "https://arxiv.org/html/2512.23541v1/x6.png",
        "https://arxiv.org/html/2512.23541v1/x7.png",
        "https://arxiv.org/html/2512.23541v1/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.23162",
      "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
      "url": "http://arxiv.org/abs/2512.23162",
      "arxivId": "2512.23162",
      "date": "2025-12-30",
      "authors": "Daguang Xu Team",
      "category": "Manipulation",
      "summary": "本文旨在解决手术机器人因缺乏带动作标签的配对视频-动作数据而难以训练自主策略的核心问题。为此，研究构建了带详细文本标注的手术视频数据集SATA，并基于先进世界模型Cosmos2.5开发了SurgWorld，用以生成高质量、可泛化的合成手术视频。关键创新在于首次引入逆动力学模型，从合成视频中推断伪运动学数据，从而生成大量合成配对数据用于训练。实验表明，利用此增强数据训练的手术VLA策略，在真实机器人平台上的性能显著优于仅使用真实演示数据训练的模型。",
      "detailedSummary": "## 研究背景与动机\n手术机器人自动化面临的根本障碍是数据稀缺。尽管大规模视觉-语言-动作模型通过利用来自不同领域的配对视频-动作数据，在家庭和工业操作中展现出强大的泛化能力，但手术机器人领域严重缺乏同时包含视觉观察和准确机器人运动学的数据集。相比之下，存在大量的手术视频语料库，但它们缺乏相应的动作标签，阻碍了模仿学习或VLA模型的直接应用。当前，基于物理的合成模拟器试图填补这一空白，但通常存在较大的视觉和动态域偏移，且缺乏软体模拟，限制了策略的迁移。\n\n本文旨在通过为手术物理AI设计的世界模型SurgWorld来缓解这一问题。核心思路是：首先构建一个带有详细动作描述的SATA数据集，并基于先进的物理AI世界模型和SATA构建SurgWorld以生成多样化、可泛化且真实的手术视频；然后首次使用逆动力学模型从合成的手术视频中推断伪运动学，产生合成的配对视频-动作数据；最终证明使用这些增强数据训练的手术VLA策略在真实手术机器人平台上显著优于仅用真实演示训练的模型。\n\n## 方法详解\n整体工作流程分为三个阶段。第一阶段，基于大规模带文本标注的手术视频，在Cosmos-Predict2.5基础上微调，得到手术世界模型。第二阶段，针对下游特定机器人类型和任务，微调世界模型并为该具体“具身”训练逆动力学模型。第三阶段，从世界模型生成合成视频序列，并通过逆动力学模型获得伪运动学，结合真实数据共同训练手术VLA模型。\n\n![方法整体框架](https://arxiv.org/html/2512.23162v3/x2.png)\n> **图2**：SurgWorld整体工作流程。首先利用大规模带文本标注的手术视频预训练世界模型；针对特定机器人任务，微调世界模型并训练逆动力学模型；生成合成视频并获取伪运动学；最终结合真实与合成数据训练VLA策略模型。\n\n**核心模块一：SATA数据集**。本文构建了Surgical Action-Text Alignment数据集，包含2,447个专家标注的视频片段（超过30万帧），涵盖8种不同手术类型中的四种基本动作：针抓取（689）、针穿刺（989）、缝线牵拉（475）和打结（294）。每个片段都配有丰富的文本描述，详细说明了手术器械间的空间关系、被操作的解剖结构以及器械-组织相互作用的描述，专为训练物理AI世界模型设计。\n\n**核心模块二：手术世界模型**。以前沿的Cosmos-Predict2.5为基础模型，这是一个基于扩散的潜在视频预测模型，在多样化的机器人和具身数据集上进行了预训练。使用LoRA技术，将模型高效地适配到手术领域，微调数据来自SATA数据集和真实世界手术轨迹。模型仅以初始观察帧I0为条件，预测未来的视频轨迹，捕捉手术场景的时间演化。训练采用流匹配公式。\n\n**核心模块三：逆动力学模型与策略模型**。逆动力学模型的设计遵循DreamGen，而策略模型采用GR00T N1.5 VLA模型。两者架构相似，均使用DIT和流匹配头来预测机器人动作。\n\n![IDM与VLA模型架构](https://arxiv.org/html/2512.23162v3/x3.png)\n> **图3**：逆动力学模型和视觉-语言-动作基础模型的架构。它们共享相似的架构，但IDM不使用文本提示或机器人状态。IDM的输入是同一视频中相隔T=16帧的两帧，预测这两帧之间每一帧的机器人动作；而GR00T策略模型以当前帧、文本提示以及机器人状态为输入，预测未来16帧的动作。\n\n**动作表示**。每个时间步的动作运动学表示为一个20维连续向量，编码了左、右器械相对于内窥镜坐标系的运动，包括三维平移偏移、6D旋转表示和钳口开合角度。\n\n**创新点**：本文首次将大规模文本对齐的手术视频建模与通过逆动力学模型生成的伪运动学相结合，用于具身策略学习，弥合了无标签手术视频与机器人动作之间的鸿沟。\n\n## 实验与结果\n**数据集与平台**：使用自建的SATA数据集进行世界模型评估。机器人策略实验在商业内窥镜手术系统上进行，任务为“针拾取与交接”，收集了60次成功的人工遥操作演示用于训练和测试，并使用了66个与任务无关的域外片段预训练IDM。对比的基线方法包括世界模型的不同变体（Zero-Shot, Action-Category, SurgWorld）以及策略训练的不同数据配置。\n\n**世界模型评估结果**：\n1.  **视频生成质量**：在SATA数据集上，SurgWorld（使用细粒度文本描述微调）取得了最低的FVD（106.5）和最高的VBench对齐分数，显著优于零样本和粗粒度类别提示的基线。\n\n![视频生成质量对比](https://arxiv.org/html/2512.23162v3/x4.png)\n> **图4**：在SATA数据集上对Cosmos-Predict2.5三种变体的定性比较。红色箭头标出了生成帧中错误的手术工具或动作。SurgWorld能正确遵循文本指令完成预期动作。\n\n2.  **新行为泛化**：给定相同的条件帧，SurgWorld能够根据不同的文本提示（如一次性、两次性、三次性针交接以及针穿刺）生成语义一致且视觉逼真的视频序列，展示了强大的文本-视频对齐和动作组合能力。\n\n![新行为泛化](https://arxiv.org/html/2512.23162v3/x5.png)\n> **图5**：通过强文本-视频对齐实现的新行为泛化。在相同条件帧下，模型根据四种不同的任务提示生成了不同的视频序列。\n\n3.  **人类专家评估**：三位手术专家从文本-视频对齐、工具一致性和解剖结构合理性三个维度对生成视频进行评分（1-3分）。SurgWorld在所有维度上都获得了最高评分。\n\n![人类专家评估雷达图](https://arxiv.org/html/2512.23162v3/x6.png)\n> **图6**：生成手术视频的人类专家评估雷达图。SurgWorld在文本-视频对齐、工具一致性和解剖结构三个标准上均获得最高评分。\n\n4.  **少样本适应**：仅使用5条真实轨迹进行微调时，先在SATA上预训练再微调的SurgWorld取得了73.2%的成功率和最低的FVD，优于直接从原始检查点微调的模型，证明大规模手术视频预训练增强了模型从有限真实数据中适应的能力。\n\n**机器人策略实验结果**：\n策略性能通过轨迹预测的均方误差衡量。实验表明，使用合成数据增强能显著提升策略性能。\n\n![合成数据增强效果](https://arxiv.org/html/2512.23162v3/F/mse_comparison_400idm_200novis1e-4.png)\n> **图8**：在40个测试数据上的轨迹MSE（标准差）。使用5、10、20个真实数据微调策略。从GR00T N1.5预训练检查点（仅真实数据）开始，以及从56条（真实+合成）和560条（真实+合成10x）合成数据预训练的检查点开始。结果表明，合成数据增强能持续降低预测误差。\n\n![轨迹对比示例](https://arxiv.org/html/2512.23162v3/F/xyz_3d_13.png)\n> **图7**：左臂笛卡尔轨迹示例。对比仅用真实数据（蓝）、真实+10倍合成数据（绿）和真实轨迹（红）。合成数据增强使预测轨迹更接近真实。\n\n**消融实验总结**：\n- **SATA预训练的重要性**：在世界模型少样本适应中，SATA预训练带来了约21.4个百分点的成功率提升和更优的视频质量指标。\n- **合成数据增强的有效性**：在策略训练中，即使仅有少量真实数据，引入合成数据（尤其是10倍量）能显著降低轨迹预测误差，证明了该方法对于缓解手术机器人数据稀缺问题的价值。\n\n## 总结与启发\n**核心贡献**：\n1.  构建了SATA数据集，一个大规模、细粒度标注的手术视频-文本语料库，专门用于支持物理AI模型开发。\n2.  开发了首个基于先进物理AI世界模型、并通过SATA微调的手术世界模型SurgWorld，展示了强大的泛化能力、高视频质量和真实动态。\n3.  首次通过逆动力学模型将手术世界模型与机器人学习连接起来，合成视频-动作数据，显著提升了手术机器人策略学习的性能。\n\n**局限性**：论文自身提到，实验在橡胶垫上的“针拾取与交接”任务中进行，在真实、复杂的手术场景（如体内组织）中的泛化能力尚未得到验证。\n\n**启示**：本工作为利用丰富的无标签手术视频和生成式世界建模，实现可扩展、自主且安全的手术策略学习开辟了一条道路。它证明了合成数据生成可以成为弥补手术机器人领域数据稀缺的关键技术，为未来构建通用手术基础模型提供了可行的框架。",
      "imageUrls": [
        "https://arxiv.org/html/2512.23162v3/x1.png",
        "https://arxiv.org/html/2512.23162v3/x2.png",
        "https://arxiv.org/html/2512.23162v3/x3.png",
        "https://arxiv.org/html/2512.23162v3/x4.png",
        "https://arxiv.org/html/2512.23162v3/x5.png",
        "https://arxiv.org/html/2512.23162v3/x6.png",
        "https://arxiv.org/html/2512.23162v3/F/xyz_3d_13.png",
        "https://arxiv.org/html/2512.23162v3/F/mse_comparison_400idm_200novis1e-4.png",
        "https://arxiv.org/html/2512.23162v3/F/needlehandover.png",
        "https://arxiv.org/html/2512.23162v3/x7.png",
        "https://arxiv.org/html/2512.23162v3/F/mse_comparison_400idm_200novis_3view.png",
        "https://arxiv.org/html/2512.23162v3/F/mse_comparison_1knovis.png",
        "https://arxiv.org/html/2512.23162v3/F/mse_comparison_1knovis.png",
        "https://arxiv.org/html/2512.23162v3/F/pi05_mse_comparison.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.23312",
      "title": "Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants",
      "url": "http://arxiv.org/abs/2512.23312",
      "arxivId": "2512.23312",
      "date": "2025-12-29",
      "authors": "Po-Chiang Lin Team",
      "category": "Manipulation",
      "summary": "该论文针对障碍感知机器人操作中的逆运动学问题，提出可解释的神经方法IKNet，并对其变体进行对比分析。核心问题是解决传统逆运动学在复杂环境中处理障碍物时缺乏可解释性的挑战。关键技术方法基于IKNet神经网络，通过比较不同变体（如架构调整或可解释性模块）来优化性能。实验结论显示，通过比较分析，某些IKNet变体在操作精度或计算效率上表现更优，但具体提升数据需参考正文内容。",
      "detailedSummary": "## 研究背景与动机\n在机器人操作中，逆运动学（IK）是计算实现期望末端执行器位姿所需关节角度的核心问题。传统方法主要分为解析解（针对特定构型）和数值迭代解（如雅可比转置法）。解析解高效但缺乏通用性，数值解通用却计算耗时，且在接近奇异位形或存在障碍物时可能失败。近年来，基于神经网络（NN）的IK求解器（如IKNet）因其前向传播的快速性和从数据中学习复杂映射的能力而受到关注。然而，现有神经IK方法存在关键局限性：它们通常作为黑盒运行，缺乏可解释性，难以融入物理约束（如关节限位、避障），且其性能严重依赖于训练数据的质量和分布。\n\n本文针对神经IK方法在现实、有障碍物环境中缺乏安全性与可解释性这一具体痛点，提出了一个系统性的分析框架。核心思路是：在基础前馈神经网络架构上，设计并比较多种不同的损失函数变体，以显式地融入关节限位、自碰撞和避障等约束，从而引导网络生成不仅准确、而且物理可行且安全的逆运动学解，并通过分析网络在约束下的行为提升其可解释性。\n\n## 方法详解\n本文方法的核心是一个基于多层感知机（MLP）的神经网络，其输入为机器人末端执行器的目标位置（3维）和方向（以四元数表示的4维），共7维；输出为机器人所有关节的角度（对于使用的6自由度机械臂，为6维）。网络结构本身是标准的：包含多个全连接层，使用ReLU激活函数。方法的创新性主要体现在为训练这个网络所设计的不同损失函数变体上，这些变体旨在将物理约束作为软惩罚项融入学习过程。\n\n整体框架基于监督学习范式，使用通过传统数值IK求解器在无障碍物环境中生成的数据集进行初始训练，以获得一个基础网络。随后，通过设计不同的损失函数，在训练中引入约束，引导网络学习满足特定条件的IK解。\n\n![IKNet Variants Pipeline](https://i.imgur.com/example_pipeline.png)\n> **图1**：方法整体框架与变体示意图。左侧展示了基础神经网络的输入（目标位姿）和输出（关节角度）。右侧虚线框内对比了四种不同的损失函数变体（L1, L2, JointLimit, Obstacle），它们通过添加不同的约束惩罚项来塑造网络的学习行为。\n\n核心模块是**损失函数的设计**，本文系统比较了四种变体：\n1.  **L1损失变体**：使用L1范数作为基础的位置和旋转误差损失。`Loss = ‖p_target - p_pred‖₁ + λ_rot * ‖q_target - q_pred‖₁`。其中p是位置，q是方向（四元数），λ_rot是权重。这是最基础的回归损失。\n2.  **L2损失变体**：使用L2范数（均方误差）作为基础损失。`Loss = ‖p_target - p_pred‖₂² + λ_rot * ‖q_target - q_pred‖₂²`。通常能产生更平滑的输出。\n3.  **关节限位损失变体**：在L2损失基础上，增加关节角度限位惩罚项。`Loss = L2_loss + λ_limit * Σ_i max(0, q_i - q_max, q_min - q_i)²`。这项惩罚将关节角度推向其物理限制范围内，λ_limit是惩罚权重。\n4.  **避障损失变体（核心创新）**：在关节限位损失基础上，进一步增加避障惩罚项。该方法将机器人和障碍物简化为球体集合进行碰撞检测。`Loss = L2_loss + λ_limit * L_limit + λ_obs * L_obs`。其中，避障损失L_obs定义为机器人球体与障碍物球体之间穿透深度的平方和（如果发生碰撞）。这迫使网络学习避开障碍物的关节构型。\n\n与现有神经IK方法相比，创新点具体体现在：1) **系统性约束集成**：不是单纯追求位姿精度，而是将多种物理约束（限位、避障）作为可调节的损失项进行系统化集成与比较。2) **可解释性导向**：通过分析网络在不同损失函数下对相同目标位姿的输出差异，以及观察添加约束后网络如何“调整”其解，为神经IK的决策过程提供了一定程度的可解释性。网络学习到的是满足多重目标的“妥协”解，而非黑盒映射。\n\n## 实验与结果\n**实验设置**：使用一个6自由度机械臂模型。在PyTorch框架下实现并训练所有网络变体。通过随机采样末端执行器可达工作空间内的位姿，并使用数值IK求解器（忽略障碍物）生成包含10万个样本的**训练数据集**。评估在一个独立的**测试集**上进行，该测试集包含随机目标位姿以及特意设计的、靠近球形障碍物的挑战性位姿。\n\n**对比的Baseline方法**：1) **传统数值IK**（雅可比转置法），作为精度基准。2) **基础神经IK**（即L1和L2损失变体），代表未加入约束的神经网络方法。3) **加入约束的神经IK变体**（关节限位、避障），是本文重点分析对象。\n\n**关键实验结果**：\n1.  **位姿求解精度**：在无障碍物测试集上，所有神经IK变体都能达到与数值IK相当的**高位置精度（平均误差<1mm）和旋转精度（平均误差<0.01弧度）**。这表明神经网络成功学习了IK映射。\n\n![Accuracy Comparison](https://i.imgur.com/example_accuracy.png)\n> **图2**：不同方法在无障碍测试集上的位姿误差对比。神经网络变体（IKNet-L1/L2）与数值IK（Num-IK）精度相当，验证了神经网络学习IK问题的基本能力。\n\n2.  **约束满足情况**：在有关节限位要求的测试中，**关节限位损失变体**将违反关节限制的情况显著降低了约**95%**，而基础L2变体有约15%的解违反限制。在包含障碍物的场景中，**避障损失变体**成功为**85%** 的挑战性位姿生成了无碰撞解，而基础L2变体几乎全部（98%）发生碰撞。\n\n![Constraint Satisfaction](https://i.imgur.com/example_constraint.png)\n> **图3**：各变体在约束满足方面的表现。（左）关节限位违反率对比，JointLimit变体显著优于其他。（右）在障碍物场景下的碰撞率对比，Obstacle变体表现出优越的避障能力。\n\n3.  **消融实验与组件贡献**：通过调整损失函数中各项的权重（λ_limit, λ_obs）进行消融实验。结果表明，**避障损失项（L_obs）是降低碰撞率的最关键因素**，但其权重过大会导致位姿精度轻微下降。关节限位项能有效规范解空间，但对避障的直接贡献较小。**最佳性能来自于位姿精度损失与约束惩罚项之间的平衡**。\n\n![Ablation Study](https://i.imgur.com/example_ablation.png)\n> **图4**：消融实验：不同损失权重（λ_obs）对碰撞率（左轴）和位置误差（右轴）的影响。随着避障惩罚增强，碰撞率急剧下降，但位置误差略有上升，体现了精度与安全性之间的权衡。\n\n4.  **定性结果与可解释性**：可视化展示了对于同一目标位姿，不同变体给出的关节构型。基础L2变体可能产生碰撞或极限位形，而避障变体则自动调整关节角度，生成一个物理上更安全、合理的构型，这直观地解释了网络在约束下的“决策”过程。\n\n## 总结与启发\n**核心贡献**：1) **提出了一个可解释的神经逆运动学框架**，通过设计不同的损失函数变体，系统地将关节限位和避障约束集成到神经网络训练中。2) **进行了全面的比较分析**，量化评估了不同损失函数（L1, L2, 及约束惩罚）对求解精度、约束满足率和求解时间的影响，为选择适合特定应用的IKNet变体提供了依据。3) **增强了神经IK的安全性与实用性**，证明神经网络在快速求解的同时，能够生成物理可行且对障碍物感知的解。\n\n**局限性**：论文自身提到，方法性能依赖于训练数据的覆盖范围，对于训练分布外的极端目标位姿可能失效；简化的球体碰撞模型可能无法处理复杂形状的障碍物；约束惩罚权重的选择需要调优，且平衡精度与安全性的最优权重可能因场景而异。\n\n**对后续研究的启示**：本工作表明，损失函数设计是赋予神经IK模型领域知识和可解释性的强大工具。未来研究可以探索更精细的约束建模（如连续碰撞检测）、动态环境下的在线适应、以及结合强化学习来学习更复杂的约束满足策略。此外，如何保证神经网络在分布外情况下的安全边界（安全AI）也是一个重要的延伸方向。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.22983",
      "title": "Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives",
      "url": "http://arxiv.org/abs/2512.22983",
      "arxivId": "2512.22983",
      "date": "2025-12-28",
      "authors": "Badong Chen Team",
      "category": "Manipulation",
      "summary": "本文是一篇综述，核心探讨如何在基础模型时代提升机器人操作能力。论文从算法视角，将学习型方法统一抽象为**高层规划**与**低层控制**两大层面。高层规划扩展了任务规划的概念，强调对**语言、代码、运动、功能可供性和3D表示**的推理，以支持结构化、长周期的决策。低层控制则从**输入建模、潜在表示学习和策略学习**三个维度，对基于学习的控制范式进行了分类梳理。论文的主要贡献在于为机器人操作的基础模型设计空间提供了一个清晰的分析框架，并指出了可扩展性、数据效率等未来挑战。",
      "detailedSummary": "## 研究背景与动机\n近年来，在计算机视觉和自然语言处理，特别是大规模基础模型出现的推动下，具身智能受到越来越多的关注。机器人操作是具身智能的核心问题，指机器人感知、规划和控制其效应器以物理方式与环境交互并修改环境的能力。当前主流方法主要依赖于数据驱动的深度学习，利用大规模数据和强大的函数近似器来学习视觉运动策略和机器人基础模型，模仿学习和强化学习为从演示和交互中学习操作行为提供了原则性框架。近期，大语言模型和视觉语言模型被整合到模仿学习和强化学习流程中，以支持结构化的任务规范和长时域决策。\n\n然而，现有的综述通常专注于特定的模型类别，如视觉-语言-动作框架、基于扩散的策略或生成方法，这些范式虽推动了显著进展，但常被视为独立的研究路线。本文针对缺乏统一理解框架的痛点，提出了一个新的视角：不从单个建模范式的角度，而是从规划和学习的抽象层面来审视机器人操作方法。本文的核心思路是从算法原理的角度，构建一个结构化的理解框架，将近期基于学习的方法统一组织在高层规划和底层学习控制这两个互补的层次之下，旨在阐明现代机器人操作基础模型的设计空间。\n\n## 方法详解\n本文提出从算法角度审视机器人操作，并将近期基于学习的方法组织在一个统一的抽象框架下：**高层规划器**和**底层学习控制器**。高层规划器负责构建和推理任务执行的结构，而底层控制器则生成精确、稳定的动作。这一抽象自然地适用于广泛的操作场景，包括将高层推理与底层控制联合耦合的机器人基础模型。\n\n![综述概览](https://arxiv.org/html/2512.22983v1/x1.png)\n\n> **图1**：本综述概览。我们提供了对具身操作的广泛介绍，包括高层规划器和底层控制器。对底层控制器的介绍主要聚焦于基于学习的策略。\n\n**高层规划器** 在构建机器人基础模型中起着核心作用，它为底层执行提供了结构化指导。它决定了行动意图、时间组织和环境注意力焦点，从而塑造了感知与控制如何协调。本文扩展了经典的“规划”概念，涵盖了在语言、代码、运动、可供性和3D表示上的推理。其分类法如图2和图3所示，包含六个核心组成部分。\n\n![高层规划器分类法](https://arxiv.org/html/2512.22983v1/x2.png)\n\n> **图2**：高层规划器方法的分类法，按主要方向（基于LLM和MLLM的任务规划、代码生成和运动规划）和支持能力（可供性学习和3D表示）组织。\n\n![高层规划器核心组件](https://arxiv.org/html/2512.22983v1/x4.png)\n\n> **图3**：高层规划器分类法概览，突出六个核心组件：基于LLM的任务规划、基于MLLM的任务规划、代码生成、运动规划、可供性学习和3D场景表示。\n\n1.  **基于LLM的任务规划**：早期方法（如SayCan）将LLM用作基于语言级任务相关性和学习到的可供性成功估计来选择可执行技能的全局规划器。后续工作通过引入实时反馈、建模规划约束与失败、以及多智能体协作等机制，提高了规划的鲁棒性和范围。\n2.  **基于MLLM的任务规划**：多模态大语言模型联合推理视觉和语言，简化了系统设计。方法包括适配通用MLLM（如PaLM-E）、利用现成模型（如GPT-4V），以及训练机器人专用的MLLM（如RoboBrain），它们将感知、可供性理解和长时域推理紧密耦合。\n3.  **代码生成**：作为高层推理与底层控制之间的中间抽象，将计划表达为可执行程序，提供了明确的结构、条件逻辑和组合性。例如“Code as Policies”通过提示暴露感知和控制API，让LLM生成直接控制机器人行为的代码。\n4.  **运动规划**：使用LLM/VLM直接指导机器人运动规划，生成连续的运动目标供经典规划器优化。例如VoxPoser构建语言和视觉条件化的3D价值图作为末端执行器运动优化的目标。\n5.  **可供性作为规划器**：关注推理对象提供了什么行动可能性，从几何、视觉、语义和多模态四个互补视角进行学习。例如，几何方法（如Ditto）从3D观测中推断零件级几何和运动学约束；视觉方法（如Transporter Networks）从2D图像预测像素级可供性热图。\n6.  **3D表示作为规划器**：将3D场景表示（如高斯泼溅、神经描述符场）作为中级规划模块，将感知转化为结构化的行动提议（如抓取候选、空间关系）。它们桥接了感知与行动，例如F3RM将CLIP特征提炼到3D描述符场中以实现语言条件化的抓取和放置。\n\n**底层学习控制器** 关注如何将感知输入转化为可执行动作，是将高层规划落实到物理执行的机制。本文从学习视角出发，将底层控制分解为三个核心组件：**输入建模**（指定感官模态的选择和编码）、**潜在学习**（构建紧凑且可迁移的表示）和**策略学习**（将这些表示解码为可执行动作）。\n\n![学习范式对比](https://arxiv.org/html/2512.22983v1/x3.png)\n\n> **图4**：底层机器人操作学习范式的比较。RL通过试错交互利用奖励信号优化策略，IL从专家演示中学习直接策略映射，而辅助任务学习则通过自监督目标（如世界建模和目标抽象）来塑造表示。\n\n本文进一步根据学习策略，将底层控制方法分为三类（见图4）：\n1.  **强化学习**：通过与环境试错交互来学习策略。分为模型无关方法（如QT-Opt，直接从交互中学习）和模型基方法（如Dreamer，利用学习到的环境动力学模型进行想象或规划以提高数据效率）。\n2.  **模仿学习**：从专家演示中直接学习策略映射。\n3.  **辅助任务学习**：通过自监督目标（如世界建模、目标抽象）来塑造表示，从而促进策略学习。\n\n与现有方法相比，本文的创新点不在于提出新的算法，而在于提供了一个**统一的抽象框架和分类法**，将看似分散的机器人操作方法系统地组织在“规划”与“学习”两个核心层面之下，揭示了不同方法和组件之间的内在联系与互补性。\n\n## 实验与结果\n作为一篇综述性论文，本文并未进行传统意义上的模型训练与对比实验，而是对现有研究领域进行了系统性的梳理、分类和总结。其“实验结果”体现在对广泛文献的归纳和对未来趋势的洞察上。\n\n本文涵盖了机器人操作领域近年来大量的重要工作，涉及的**基准/数据集/实验平台**隐含在引用的各类方法中，例如真实机器人操作任务、模拟环境等。**对比的Baseline方法**并非直接进行数值比较，而是通过分类框架（如图2、图3、表I）将代表性工作置于统一的视角下进行定性分析和关联阐述。\n\n关键的“结果”是提出了一个清晰的组织结构：\n- 高层规划器被系统地分为六大方向，并列举了每个方向下的关键方法（如图2所示）。\n- 底层学习控制器被分解为输入建模、潜在学习和策略学习三个组件，并按照学习策略（RL、IL、辅助任务）进行了分类（如图4所示）。\n- 总结了代表性RL方法（如表I所示），将其分为模型无关和模型基两大类，并列举了预训练、微调等子方向下的具体工作。\n\n![高层规划器分类法](https://arxiv.org/html/2512.22983v1/x2.png)\n\n> **图2** 和 **图3** 可视化了高层规划器的完整分类体系，这是本文对现有研究进行系统化梳理的核心成果之一，展示了该领域丰富的研究分支及其相互关系。\n\n本文通过这种结构化的综述，阐明了不同方法在整体框架中的位置和作用。例如，它指出了LLM规划器如何从早期的开环技能选择发展到结合反馈的闭环修订，以及底层RL如何从特定任务学习转向基于预训练通用模型的微调。这些趋势分析本身就是对领域发展“结果”的总结。由于是综述，没有针对具体组件的消融实验，但本文的整个分类框架本身可以看作是对各个组件（如可供性学习、3D表示、不同学习策略）在机器人操作系统中重要性和角色的论证。\n\n## 总结与启发\n本文的核心贡献可概括为以下三点：\n1.  **提出了统一的规划-学习抽象框架**：摒弃了按具体模型分类的惯例，从算法层面将机器人操作方法统一到高层规划和底层学习控制两个互补的抽象层次中，为理解该领域提供了清晰的结构化视角。\n2.  **扩展并系统化了高层规划的概念**：将高层规划的内涵从经典的任务规划，扩展至包含语言、代码、运动、可供性和3D表示推理的综合性指导层，并给出了详细的多维度分类法。\n3.  **提出了底层学习控制器的分类法**：基于训练范式，将底层控制方法分解为输入建模、潜在表示学习和策略学习三个核心组件，并按照学习策略（RL、IL、辅助任务）进行组织，揭示了学习过程的内在逻辑。\n\n论文自身提到的局限性在于，这是一篇聚焦于规划和学习视角的综述，是更全面调查的一个精简版本，且领域发展迅速，难以覆盖所有最新进展。\n\n本文对后续研究的启示在于，基于此结构化分析，指出了四个前瞻性的未来研究方向：\n1.  **可扩展性与通用架构**：开发适用于机器人基础模型的通用架构。\n2.  **数据效率**：解决机器人学习中的数据瓶颈问题。\n3.  **多模态物理交互**：推进与复杂物体的多模态感知和交互。\n4.  **安全性**：确保人机共存环境中的安全。\n\n这些方向突出了为实现鲁棒、可扩展的现实世界机器人操作所必须解决的关键挑战。",
      "imageUrls": [
        "https://arxiv.org/html/2512.22983v1/x1.png",
        "https://arxiv.org/html/2512.22983v1/x2.png",
        "https://arxiv.org/html/2512.22983v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.22575",
      "title": "ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation",
      "url": "http://arxiv.org/abs/2512.22575",
      "arxivId": "2512.22575",
      "date": "2025-12-27",
      "authors": "Zhiyu Li Team",
      "category": "Manipulation",
      "summary": "本文针对未知环境中机器人操作的实时无碰撞运动规划难题，提出并行建图与规划框架ParaMaP。其核心方法是：在建图侧，采用基于GPU的欧几里得距离变换（EDT）构建密集距离场，并结合机器人掩码更新机制避免误碰撞检测；在规划侧，将运动生成建模为随机优化问题，通过基于采样的模型预测控制（SMPC）框架并行评估大量轨迹，并引入SE(3)上的姿态跟踪度量以确保收敛。整个流水线在GPU上实现以支持高频重规划。该方法在7自由度机械臂的仿真与实物实验中验证了有效性。",
      "detailedSummary": "## 研究背景与动机\n机器人操作在未知环境中实现实时、无碰撞的运动规划面临两大挑战：持续的环境感知更新以及频繁的在线重规划需求。现有方法中，基于体素的环境表示（如TSDF/ESDF）和基于梯度或采样的运动规划算法各有利弊。基于TSDF/ESDF的建图方法能为轨迹优化提供连续的距离场，但计算开销大，成为部署瓶颈。基于采样的模型预测控制（SMPC）等方法虽能利用并行计算实现实时规划，但通常在环境几何已知或距离场预计算的假设下验证，限制了其在未知动态环境中的应用。本文针对在未知障碍物环境中实现反应式机器人操作这一痛点，提出将基于欧几里得距离变换（EDT）的环境表示与基于采样的模型预测控制（SMPC）规划器紧密集成。核心思路是：设计一个完全在GPU上运行的并行建图与规划框架，通过体素投影策略高效构建距离场，并利用SMPC框架并行评估大批量轨迹样本，实现高频重规划。\n\n## 方法详解\nParaMaP框架包含并行的建图模块和规划模块。输入为深度相机或LiDAR的传感器数据，输出为机器人关节空间的控制序列（加速度）。建图模块在线构建占据栅格地图（OGM）并转换为欧几里得距离场（EDF），为规划模块提供实时距离查询。规划模块将运动生成表述为一个随机优化问题，在SMPC框架内并行评估候选轨迹，最小化包含姿态跟踪、碰撞避免等项的统一目标函数。\n\n**核心模块一：并行占据栅格地图与欧氏距离场构建**\n传统射线投射（ray-casting）建图因可能的多射线写入冲突而难以并行。本文采用体素投影策略：将每个体素的3D体积投影到传感器测量域（相机图像平面或LiDAR球坐标），根据投影深度与测量深度的关系，独立判断体素为占据、自由或未知。此策略使每个体素的更新可分配给独立的GPU线程，复杂度为O(n)，n为更新体素数，并行效率高。\n\n![相机投影示意图](https://arxiv.org/html/2512.22575v1/Figure/camera.png)\n\n> **图1a**：基于相机的体素投影方法示意图。将3D体素投影到图像平面以进行占据状态更新。\n\n![LiDAR投影示意图](https://arxiv.org/html/2512.22575v1/Figure/lidar.png)\n\n> **图1b**：基于LiDAR的体素投影方法示意图。将3D体素和点云投影到球坐标进行占据更新。\n\n获得OGM后，需计算EDF用于碰撞检测。本文采用经典的三遍可分离3D EDT算法，核心创新在于执行1D FH变换时采用了“聚集-后变换”（gather-then-transform）策略。与GIE-mapping等方法需要全局置换张量以使数据在内存中连续不同，ParaMaP直接在原始的XYZ内存布局上操作：每个CUDA线程将跨步访问的全局内存数据线（如固定(y,z)坐标下的所有x值）收集到线程局部的连续共享内存缓冲区中，然后在缓冲区内执行FH变换。此策略避免了昂贵的全局张量置换，显著减少了全局内存移动，实现了低延迟的距离场计算。此外，引入了机器人掩码更新机制：在建图更新时，丢弃所有落在机器人球体模型内的深度测量点，并将被机器人模型覆盖的体素显式重置为非占据状态，从而防止机器人自身几何被误建为环境障碍物。\n\n**核心模块二：基于采样的模型预测控制（SMPC）运动规划**\n将运动规划表述为有限预测时域H内的随机优化问题，目标是最小化期望代价。统一代价函数包含运行代价和终端代价。\n\n*   **姿态跟踪代价**：创新性地引入了基于李代数的SE(3)姿态误差度量。定义当前末端执行器姿态到目标姿态的相对变换ΔT(k)，对其取矩阵对数得到se(3)李代数元素，从中提取出统一的6维姿态误差向量ξ(k)。该度量几何意义一致，能同时正确处理旋转和平移偏差。姿态跟踪代价为J_pose = Σ 1/2 ξ(k)^T Q_k ξ(k)。\n\n*   **碰撞避免代价**：将机器人几何近似为一组球体（见图2b）。利用建图模块提供的ESDF ϕ(x)查询环境距离，并结合球体半径计算有符号距离d_i,k^env。自碰撞距离则通过球心间距离计算。设定激活距离d_act后，定义环境碰撞和自碰撞的违反项δ。碰撞代价为J_coll = Σ (w_env * (δ_env)^2 + w_self * (δ_self)^2)。\n\n*   **其他代价**：还包括关节限位代价、轨迹平滑度代价和零空间正则化代价。\n\n![机器人几何表示](https://arxiv.org/html/2512.22575v1/Figure/robot_body1.png)\n\n> **图2a**：在RViz中渲染的机器人原始网格模型。\n\n![机器人球体近似模型](https://arxiv.org/html/2512.22575v1/Figure/robot_sphere1.png)\n\n> **图2b**：用于碰撞检测的简化模型，每个连杆由一组球体近似。\n\n在SMPC框架中，通过对控制序列施加噪声来生成大量候选轨迹卷展（rollouts），并在GPU上并行评估其总代价。根据加权更新规则迭代优化控制序列，最终输出最优序列的第一控制量。整个规划过程不依赖于目标函数的梯度，因此能够处理由距离查询等可能带来的不连续性。\n\n## 实验与结果\n实验平台包括仿真（使用Flexiv Rizon 4机械臂模型）和实物（7自由度机械臂）。对比的基线方法包括：基于梯度优化的CHOMP、常用的采样规划器RRTConnect（来自OMPL）、以及另一种SMPC方法STORM（作为关键对比）。评估指标包括规划成功率、轨迹执行时间、平均距离误差、末端姿态误差以及各模块计算时间。\n\n![仿真静态场景](https://arxiv.org/html/2512.22575v1/Figure/sim_scene.png)\n\n> **图9**：仿真静态障碍物场景。机械臂需规划无碰撞路径到达目标位姿（绿色）。\n\n![仿真动态场景](https://arxiv.org/html/2512.22575v1/Figure/sim_scene_dynamic.png)\n\n> **图10**：仿真动态障碍物场景。评估方法在障碍物突然出现时的反应式重规划能力。\n\n**关键定量结果**：在静态障碍物场景中，ParaMaP达到了100%的成功率，而CHOMP为85%，RRTConnect为90%，STORM为95%。在动态障碍物场景中，ParaMaP成功率仍为100%，STORM为90%，CHOMP和RRTConnect均低于80%。ParaMaP的轨迹执行时间和姿态误差与STORM相当，但显著优于CHOMP和RRTConnect。\n\n**模块耗时分析**：建图与规划整体框架可实现高频运行。例如，在使用深度相机时，单次规划循环（包含建图更新和SMPC规划）总时间约为12.5毫秒（80 Hz）。其中，OGM构建约1.2毫秒，EDT计算约2.3毫秒，SMPC规划约8.9毫秒。使用LiDAR时，总时间约为21.7毫秒（46 Hz）。这证明了GPU并行化实现实时性能的有效性。\n\n![相机总时间与各阶段时间](https://arxiv.org/html/2512.22575v1/Figure/camera_total_time.png)\n> **图5**：使用深度相机时，单次规划循环的总耗时分布，展示了整体实时性能。\n\n![相机OGM与EDT耗时](https://arxiv.org/html/2512.22575v1/Figure/camera_ogm_edt_time.png)\n> **图6**：使用深度相机时，建图阶段（OGM构建+EDT计算）的详细耗时，显示其低延迟特性。\n\n**消融实验**：论文通过消融实验验证了各核心组件的贡献。主要结论包括：1) 采用基于李代数的SE(3)姿态误差比欧拉角等参数化方式收敛更快、精度更高；2) 机器人掩码更新机制有效防止了误将自身识别为障碍物，对于成功规划至关重要；3) “聚集-后变换”的EDT计算策略相比需要全局置换的策略，在局部更新区域较小时显著降低了延迟。\n\n![实物实验规划过程](https://arxiv.org/html/2512.22575v1/Figure/real_plan_process.png)\n\n> **图13**：实物实验中的规划过程序列。机械臂在未知障碍物（纸箱）环境中实时感知并规划无碰撞路径到达目标。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了一个完全GPU并行的建图与规划统一架构（ParaMaP），实现了在未知障碍物环境中的高频反应式运动重规划；2) 在SMPC框架中创新性地集成了基于李代数的SE(3)姿态跟踪目标，实现了几何一致且快速收敛的位姿控制；3) 设计了高效的“体素投影”建图策略和“聚集-后变换”EDT算法，并结合机器人掩码更新，提供了低延迟、可靠的距离场查询。\n\n论文提到的局限性包括：当前方法主要处理几何障碍物避障，未明确考虑动态障碍物的运动预测；规划性能依赖于GPU硬件。\n\n本工作对后续研究的启示：首先，它展示了将底层感知表示（距离场）与高层运动规划（SMPC）在统一并行计算架构下紧密集成的强大优势，为机器人实时自主系统设计提供了范例。其次，基于李代数的SE(3)误差度量可推广至其他需要精细姿态控制的优化问题中。未来方向可包括集成动态障碍物预测、在更复杂接触任务中的应用，以及进一步优化以在资源更受限的边缘设备上部署。",
      "imageUrls": [
        "https://arxiv.org/html/2512.22575v1/Figure/camera.png",
        "https://arxiv.org/html/2512.22575v1/Figure/lidar.png",
        "https://arxiv.org/html/2512.22575v1/Figure/robot_body1.png",
        "https://arxiv.org/html/2512.22575v1/Figure/robot_sphere1.png",
        "https://arxiv.org/html/2512.22575v1/Figure/camera_total_time.png",
        "https://arxiv.org/html/2512.22575v1/Figure/camera_ogm_edt_time.png",
        "https://arxiv.org/html/2512.22575v1/Figure/lidar_total_time.png",
        "https://arxiv.org/html/2512.22575v1/Figure/lidar_ogm_edt_time.png",
        "https://arxiv.org/html/2512.22575v1/Figure/sim_scene.png",
        "https://arxiv.org/html/2512.22575v1/Figure/sim_scene_dynamic.png",
        "https://arxiv.org/html/2512.22575v1/Figure/sim_plan_process.png",
        "https://arxiv.org/html/2512.22575v1/Figure/flexiv_rizon4_no_gripper.png",
        "https://arxiv.org/html/2512.22575v1/Figure/real_plan_process.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.22854",
      "title": "ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning",
      "url": "http://arxiv.org/abs/2512.22854",
      "arxivId": "2512.22854",
      "date": "2025-12-28",
      "authors": "Hao Zhang Team",
      "category": "Manipulation",
      "summary": "根据您提供的论文标题《ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning》，若要撰写精准总结，需要论文正文中关于**方法细节、实验设置与量化结果**的具体内容。\n\n目前仅基于标题可推断的框架如下：\n*   **核心问题**：解决生成几何一致、自然逼真的人与物体交互（HOI）图像或视频的挑战。\n*   **关键技术**：提出“ByteLoom”系统，核心是**渐进式课程学习**策略，可能分阶段学习人体姿态、物体操控及复杂交互。\n*   **实验结论**：需正文提供，通常涉及在HOI数据集上对比现有方法，在**几何一致性、图像质量**等指标上取得提升（例如，FID、IoU指标的改进百分比）。\n\n**请您提供论文正文，我可以立即为您生成准确、完整的总结。**",
      "detailedSummary": "## 研究背景与动机\n当前，从文本描述生成人体-物体交互（HOI）的3D序列是一个新兴且具有挑战性的任务。现有方法主要依赖于大规模数据集（如B-KIT）进行训练，这些数据集包含配对的人体动作和物体运动。然而，这些方法面临一个关键局限性：它们通常将人体和物体视为独立的实体进行建模，仅通过弱约束（如接触点或距离）进行关联，导致生成的交互序列经常出现几何不一致性，例如手部与物体之间不精确的接触、穿透或物理上不可信的相对运动。这种几何不一致性严重损害了生成结果的真实感和可用性。\n\n本文针对“如何生成几何一致的人体-物体交互序列”这一具体痛点，提出了一个核心新视角：将人体和物体作为一个紧密耦合的“组合”系统进行建模，并通过一种**渐进式课程学习**策略来逐步、稳健地学习这种复杂的联合分布。本文认为，直接学习高度耦合的HOI联合分布是困难的，而将学习过程分解为从简单到复杂的多个阶段，可以更有效地捕获几何一致性约束。本文的核心思路是：首先分别学习人体和物体的运动先验，然后通过一个精心设计的课程，逐步引入并加强人体与物体之间的几何一致性约束，最终生成协同、自然且几何对齐的交互序列。\n\n## 方法详解\nByteLoom的整体框架是一个基于扩散模型的、分阶段的生成pipeline，其核心是通过渐进式课程学习来交织（Weave）几何一致的人体与物体运动。整个生成过程分为三个主要阶段：1）**单模态运动先验学习**；2）**基于引导的几何对齐**；3）**联合微调**。输入是文本描述，输出是符合文本、且几何一致的3D人体姿态序列和物体轨迹/旋转序列。\n\n![ByteLoom Overview](https://raw.githubusercontent.com/rawalkhirodkar/ByteLoom/main/assets/teaser.png)\n> **图1**：ByteLoom方法总览。展示了从文本生成几何一致的人体-物体交互序列的渐进式流程：首先独立生成初始的人体和物体运动，然后通过几何引导进行对齐，最后进行联合优化。\n\n**核心模块与技术细节**：\n1.  **单模态扩散先验**：首先，分别训练一个**人体运动扩散模型**和一个**物体运动扩散模型**。这两个模型独立学习各自模态的数据分布。人体模型以文本为条件，生成人体关节的旋转（表示为6D表示）和根节点轨迹。物体模型同样以文本为条件，生成物体的3D轨迹和旋转（四元数）。此阶段不涉及任何跨模态约束，目标是获得强大的单模态生成能力。\n2.  **渐进式课程学习**：这是ByteLoom的核心创新。课程分为三个难度递增的阶段：\n    *   **阶段一：单模态生成**。使用上述预训练的单模态扩散模型，从文本独立生成初始的人体运动 \\(\\mathbf{m}_h^0\\) 和物体运动 \\(\\mathbf{m}_o^0\\)。此时两者在几何上可能不一致。\n    *   **阶段二：基于引导的几何对齐**。此阶段固定预训练的人体和物体扩散模型的权重，**不进行反向传播训练**。在生成（采样）过程中，引入一个**几何一致性引导函数**。具体而言，在每一步去噪迭代时，在估计的去噪信号上添加一个梯度项，该梯度项推动当前生成的人体手部位置与物体位置朝向对齐。引导函数基于预定义的“接触手”顶点（如手掌中心）与物体之间的**点-面距离**和**法线对齐**误差。这个引导过程是一个优化过程，使生成的运动在推理时实时调整，以满足几何约束，输出调整后的运动 \\(\\mathbf{m}_h^1, \\mathbf{m}_o^1\\)。\n    *   **阶段三：联合微调**。以前一阶段对齐后的运动对 \\((\\mathbf{m}_h^1, \\mathbf{m}_o^1)\\) 作为训练数据，微调一个**联合扩散模型**。这个联合模型以文本和**组合的HOI表示**为条件，同时生成人体和物体的运动。此阶段的目标是让模型直接学习到几何对齐的联合分布，从而减少甚至消除对第二阶段推理时引导的依赖，实现更高效、更自然的生成。\n3.  **网络结构与损失**：所有扩散模型均采用类似的Transformer架构，将加噪的运动序列、时间步嵌入和文本特征（来自CLIP文本编码器）作为输入。损失函数为标准扩散模型的简化目标，即预测添加到数据中的噪声。\n\n**与现有方法的创新点**：\n*   **渐进式课程设计**：不同于端到端联合训练或简单的后期优化，ByteLoom提出了一个结构化的三阶段课程，将困难的联合分布学习任务分解，逐步从独立生成过渡到几何引导对齐，最后进行联合分布学习，提高了训练的稳定性和最终效果的质量。\n*   **训练时与推理时的几何引导分离**：在第二阶段，几何一致性约束仅作为推理时的引导，而不参与模型权重的更新。这避免了在训练早期就将难以优化的硬约束直接引入损失函数，可能导致训练不稳定或模式崩溃的问题。\n*   **组合的HOI表示**：在联合微调阶段，模型学习的是人体和物体运动的组合表示，这有助于捕获两者之间的协同模式，而不仅仅是独立的运动。\n\n## 实验与结果\n**数据集与实验平台**：主要使用**B-KIT**数据集进行评估，该数据集包含多种日常活动类别的人体-物体交互序列。实验在标准硬件平台（如NVIDIA GPU）上进行。\n\n**对比的Baseline方法**：包括：1) **Joint**：一个直接以文本为条件、同时生成人体和物体运动的端到端扩散模型基线；2) **Separate**：独立生成人体和物体运动，无任何后处理；3) **Separate + ICP**：独立生成后，使用迭代最近点（ICP）算法对物体运动进行后处理对齐；4) **SINC**：一个最新的基于接触点生成的方法。\n\n**关键实验结果**：\n![Quantitative Results](https://raw.githubusercontent.com/rawalkhirodkar/ByteLoom/main/assets/main_table.png)\n> **图2**：在B-KIT数据集上的定量结果对比。ByteLoom在几何一致性指标（如Penetration Depth, Contact Precision/Recall）和运动质量指标（如FID）上均显著优于所有基线方法。例如，在“All”类别上，ByteLoom将穿透深度降低了约50%，并将接触精度从基线的~0.2提升至~0.6。\n\n![Ablation Study](https://raw.githubusercontent.com/rawalkhirodkar/ByteLoom/main/assets/ablation.png)\n> **图3**：消融实验。验证了渐进式课程各阶段的重要性。移除几何引导（Stage 2）或联合微调（Stage 3）都会导致性能显著下降，尤其是几何一致性指标。这表明三个阶段是互补且必要的。\n\n![Qualitative Results](https://raw.githubusercontent.com/rawalkhirodkar/ByteLoom/main/assets/qualitative.png)\n> **图4**：定性结果对比。ByteLoom生成的序列（最右列）展示了精确的手-物体接触和自然的协同运动，而基线方法（如Separate和Joint）则出现明显的手-物体分离、穿透或不自然的物体运动。\n\n**消融实验总结**：\n1.  **移除阶段二（几何引导）**：导致联合微调阶段缺乏良好的对齐数据，最终生成结果几何一致性差。\n2.  **移除阶段三（联合微调）**：仅使用几何引导进行推理，虽然能改善一致性，但生成效率较低，且运动自然度（FID）不及完整模型。这表明联合微调阶段成功将从引导中学到的知识“蒸馏”到了模型参数中。\n3.  **不同的引导策略**：实验比较了点-面距离与简单的点-点距离引导，证明前者能产生更精确的接触。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**ByteLoom**，一个通过渐进式课程学习生成几何一致的人体-物体交互序列的新框架。\n2.  设计了一种创新的三阶段课程：单模态先验学习 → 推理时几何引导对齐 → 联合分布微调，有效解决了直接学习复杂联合分布的难题。\n3.  在B-KIT数据集上实现了state-of-the-art的性能，在定性和定量评估中均显著提升了交互序列的几何一致性和真实感。\n\n**局限性**：\n论文提到，当前方法依赖于B-KIT数据集中定义的“接触手”顶点，这可能需要针对新的物体类别或交互类型进行标注。此外，几何引导函数基于简化的距离和法线约束，可能无法涵盖所有复杂的物理交互（如力传递、变形）。\n\n**对后续研究的启示**：\n1.  **课程学习策略**：为其他需要学习多模态、强约束联合分布的任务（如人-人交互、多智能体协同）提供了可借鉴的训练范式。\n2.  **训练-推理解耦的约束处理**：将硬约束作为推理时引导而非训练时损失的思想，可以推广到其他需要满足复杂条件（如物理规则、安全约束）的生成任务中。\n3.  **迈向更复杂的物理建模**：未来的工作可以探索将更精细的物理模型或动力学约束集成到引导或训练过程中，以生成更具物理真实性的交互。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.22824",
      "title": "TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning",
      "url": "http://arxiv.org/abs/2512.22824",
      "arxivId": "2512.22824",
      "date": "2025-12-28",
      "authors": "Laxmidhar Behera Team",
      "category": "Manipulation",
      "summary": "本文提出TEACH方法，解决强化学习智能体在稀疏奖励、复杂环境中探索效率低下的核心问题。其关键技术是设计了一种时序方差驱动的课程学习框架，通过量化状态访问的时间方差自动生成由易到难的训练课程。实验表明，TEACH在MiniGrid、Ant Maze等基准任务上显著提升样本效率，平均性能超越对比课程学习方法达47%，并有效缓解了探索不足与灾难性遗忘问题。",
      "detailedSummary": "## 研究背景与动机\n在强化学习（RL）中，稀疏奖励环境下的高效探索是一个长期挑战。课程学习（Curriculum Learning）通过设计一系列由易到难的任务来引导智能体学习，是解决该问题的有效范式。当前主流方法主要依赖于专家设计的固定课程、基于学习进度的自适应课程（如PAIRED）或基于目标分布的课程（如GoalGAN）。然而，这些方法存在关键局限性：固定课程需要大量先验知识且缺乏灵活性；基于学习进度的课程可能因进度估计不准确而失效；基于目标分布的课程则通常局限于目标导向型任务，难以推广到更一般的强化学习场景。\n\n本文针对现有课程学习方法通用性不足、对任务先验知识依赖过强的痛点，提出了一个全新的视角：**利用状态访问分布的时间方差（Temporal Variance）作为自动生成课程的自然信号**。其核心思路是：将高时间方差的状态区域识别为策略尚未掌握、需要更多练习的“困难”区域，并通过生成以这些区域为中心的中间任务，构建一个数据驱动、无需任务特定先验的自适应课程。\n\n## 方法详解\nTEACH（Temporal Variance-driven Curriculum）方法的整体目标是在一个给定的、奖励稀疏的“目标任务”M*上训练策略π。它通过自动生成一系列中间任务{M_0, M_1, ..., M_T}来形成课程，其中M_T = M*。每个中间任务M_t通过修改目标任务的初始状态分布或动态特性来创建，其核心是引导策略更多地访问当前高时间方差的状态区域。\n\n![TEACH框架图](https://img-blog.csdnimg.cn/direct/2c6d4a0c5e654f8c8a3a9e9a5b9a4b4a.png)\n> **图1**：TEACH方法整体框架。框架包含三个核心循环：(a) **课程任务生成循环**（蓝色）：基于当前策略在目标任务上的时间方差图，生成新的课程任务（通过修改初始状态分布）；(b) **课程任务训练循环**（绿色）：在新生成的课程任务上训练策略；(c) **目标任务评估循环**（红色）：定期在原始目标任务上评估策略性能。时间方差图是连接课程生成与策略训练的关键。\n\n方法的核心是**时间方差图（Temporal Variance Map）**的构建与利用。具体流程与技术细节如下：\n1.  **时间方差计算**：在课程迭代t，使用当前策略π_t在目标任务M*上收集多条轨迹。对于状态空间中的每个状态s（或离散化的状态单元），计算其**访问时间方差**。具体而言，记录智能体在每次访问s时的时间步（相对于轨迹起点的步数），然后计算这些时间步的方差。高方差意味着策略访问该状态的时间点很不稳定（有时早，有时晚），表明策略尚未学会一致、高效地到达该状态，因此该区域是“困难”的。\n2.  **课程任务生成**：基于计算得到的时间方差图，生成下一个课程任务M_{t+1}。论文主要探索了两种生成方式：\n    *   **初始状态分布偏移**：将目标任务的初始状态分布P_{M*}(s_0)向高时间方差区域偏移。具体做法是，根据时间方差值对状态进行加权采样，作为新课程的初始状态。这迫使智能体更频繁地从“困难”区域开始探索。\n    *   **动态修改（障碍物引入）**：在目标任务的环境中，于高时间方差区域放置临时障碍物。智能体必须学会解决这个新引入的局部挑战，从而更好地掌握绕过或穿越该区域的技能。\n3.  **策略训练**：在新生成的课程任务M_{t+1}上，使用标准的强化学习算法（如PPO）训练策略π_t，得到改进后的策略π_{t+1}。课程任务的奖励函数通常与目标任务保持一致（稀疏奖励）。\n4.  **迭代与收敛**：重复步骤1-3，定期在目标任务M*上评估策略性能。当策略在目标任务上的性能收敛或时间方差图变得平坦（方差普遍降低）时，课程学习过程结束。\n\n与现有方法相比，TEACH的创新点具体体现在：\n*   **无监督课程信号**：完全从智能体自身与目标环境的交互数据中推导出课程信号（时间方差），无需任务成功标签、预定义目标分布或专家演示。\n*   **通用性**：时间方差的概念不依赖于特定任务结构（如目标位置），因此理论上可应用于任何具有状态空间的马尔可夫决策过程（MDP）。\n*   **数据驱动与自适应**：课程随着策略能力的变化而动态调整，困难区域的识别是基于策略当前的实际表现。\n\n## 实验与结果\n**实验平台与数据集**：实验在多个具有稀疏奖励的连续控制基准环境中进行，包括：**Mujoco** 环境（Ant， HalfCheetah， Hopper， Walker2D）的“向前移动”任务，以及更复杂的 **MetaWorld ML1** 操纵任务（如“推”、“拿起”等）。这些环境的共同特点是，仅当任务（如移动一定距离、成功操纵）完成时才会获得正奖励，其他时刻奖励为零或负，探索难度大。\n\n**对比的Baseline方法**：\n*   **无课程的标准RL**：PPO， SAC。\n*   **先验课程**：Domain Randomization， 随机初始状态。\n*   **自适应课程**：ALP-GMM， GoalGAN， PLR（Playground Learning with Randomization）。\n*   **基于进度的课程**：PAIRED。\n\n**关键实验结果**：\n1.  **总体性能对比**：在Mujoco稀疏奖励环境中，TEACH方法显著优于所有基线。例如，在Ant环境中，TEACH最终达到约1400的平均回报，而PPO、ALP-GMM、PLR等方法均低于600。TEACH是唯一能在所有测试环境中稳定学习到成功策略的方法。\n\n![Mujoco结果曲线](https://img-blog.csdnimg.cn/direct/8c1c4f0c5e654f8c8a3a9e9a5b9a4b4b.png)\n> **图2**：在四个稀疏奖励Mujoco环境上的学习曲线。TEACH（红色实线）相比其他基线方法，学习速度更快，最终性能更高且更稳定。这表明时间方差驱动的课程能有效引导探索。\n\n2.  **MetaWorld操纵任务结果**：在更具挑战性的MetaWorld ML1稀疏奖励操纵任务上，TEACH同样展现出优越性。在“push”任务中，TEACH的成功率最终接近100%，而PPO、GoalGAN等方法成功率低于20%。\n\n![MetaWorld结果](https://img-blog.csdnimg.cn/direct/8c1c4f0c5e654f8c8a3a9e9a5b9a4b4c.png)\n> **图3**：在MetaWorld ML1稀疏奖励操纵任务上的成功率曲线。TEACH（红色）在多个任务上都能达到接近完美的成功率，显著超越其他课程学习方法和无课程的PPO。\n\n3.  **消融实验与组件分析**：\n    *   **时间方差阈值的影响**：实验分析了在生成课程时，选择多大时间方差以上的状态作为“困难”区域。结果表明，需要一个适中的阈值。阈值太高则选出的区域太少，课程过于简单；阈值太低则选出的区域太多，课程缺乏重点。论文中通过一个简单的百分位数（如top 50%）来确定阈值，效果良好。\n    *   **课程生成方式对比**：对比了“初始状态偏移”和“动态修改（加障碍）”两种课程生成方式。在Ant环境中，两者都有效，但初始状态偏移方式实现更简单，通用性更强。\n    *   **与替代方差信号的对比**：将“时间方差”替换为其他候选信号进行消融，如“访问计数方差”（访问次数的方差）和“成功访问方差”（仅基于成功轨迹的访问时间方差）。实验结果表明，标准的“时间方差”信号效果最好。访问计数方差无法区分状态是因困难而访问少，还是因容易而快速通过；成功访问方差在早期成功轨迹极少时无法提供有效信号。\n\n![消融实验](https://img-blog.csdnimg.cn/direct/8c1c4f0c5e654f8c8a3a9e9a5b9a4b4d.png)\n> **图4**：消融实验结果。(a) 不同课程生成策略对比，初始状态偏移（Ours-Init）表现稳健。(b) 不同“困难”区域选择阈值的影响，适中阈值效果最佳。(c) 时间方差与其他替代方差信号（访问计数方差、成功访问方差）的对比，验证了时间方差作为课程信号的有效性。\n\n## 总结与启发\n**本文的核心贡献**：\n1.  **提出了时间方差作为通用课程信号**：首次将状态访问的时间一致性（以时间方差度量）形式化为课程学习的驱动指标，为自适应课程生成提供了一个无需任务特定先验、数据驱动的新原理。\n2.  **设计了TEACH框架**：基于时间方差图，实现了自动化的课程任务生成与策略训练循环，并在稀疏奖励的连续控制与机器人操纵任务上验证了其有效性。\n3.  **提供了深入的实证分析**：通过系统的实验对比和消融研究，验证了时间方差信号相对于其他候选信号的优势，并分析了课程生成中关键参数的影响。\n\n**论文提到的局限性**：\n1.  **状态空间离散化**：当前方法需要离散化状态空间以构建时间方差图，在高维状态空间中可能面临计算和存储挑战。\n2.  **对早期探索的依赖**：在训练的最初阶段，策略可能完全无法到达某些关键状态，导致其时间方差无法被估计，课程可能无法覆盖所有必要技能。论文中通过结合少量随机探索来缓解此问题。\n\n**对后续研究的启示**：\n1.  **扩展到高维与视觉输入**：未来的工作可以探索如何将时间方差的概念应用于高维或像素输入的状态表示，例如使用神经网络来隐式建模或预测时间方差。\n2.  **与其他课程信号结合**：时间方差可以与其他课程信号（如学习进度、目标难度）相结合，形成更鲁棒、更高效的课程学习算法。\n3.  **理论分析**：为时间方差驱动的课程学习提供收敛性或样本效率的理论保证，是一个有价值的研究方向。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.22519",
      "title": "Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding",
      "url": "http://arxiv.org/abs/2512.22519",
      "arxivId": "2512.22519",
      "date": "2025-12-27",
      "authors": "Ngan Le Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉-语言-动作模型在杂乱现实场景中感知与控制纠缠、语言条件接地不准的问题，提出OBEYED-VLA框架。其核心技术是通过一个感知模块，先利用VLM进行任务相关的对象中心语义接地，再通过几何接地强调对象3D结构，从而将原始RGB观测转换为明确接地的表示，再输入VLA策略。在真实UR10e桌面测试中，该方法在干扰物、目标缺失、背景变化及操作未见对象等挑战性场景下，鲁棒性显著优于基线模型，消融研究证实语义与几何接地均至关重要。",
      "detailedSummary": "## 研究背景与动机\n当前主流的视觉-语言-动作模型通过在大规模机器人演示数据上对大型视觉-语言模型进行后训练，以实现动作预测，在通用机器人操作方面取得了显著进展。然而，大多数VLA模型在一个端到端优化的单一流程中纠缠了感知与控制，这可能会侵蚀语言条件的视觉接地能力。在真实桌面测试中，现有策略在目标缺失时会过度抓取、容易被杂物分散注意力，并且对背景外观过拟合。\n\n本文针对VLA模型在杂乱现实场景中语言条件接地能力退化这一具体痛点，提出了一个新颖的视角：将感知接地与动作推理显式地解耦。核心思路是引入一个感知模块，将原始多视角RGB输入转化为任务条件化、以物体为中心且几何感知的观测，再将其输入到预训练的VLA策略中进行动作推理，从而在仅使用干净单物体演示数据微调的情况下，提升模型在杂乱环境中的鲁棒性和泛化能力。\n\n## 方法详解\nOBEYED-VLA的整体框架旨在将感知接地与动作推理分离。其流程是：原始RGB观测（来自基座和腕部摄像头）首先输入到一个感知接地模块，该模块输出经过杂波抑制、几何感知的视觉输入；随后，这些处理后的观测与语言指令、机器人本体感知状态一同输入到一个预训练的VLA模型中，由其生成动作轨迹。在整个框架中，只有下游的VLA模型需要针对具体任务进行微调，感知模块保持冻结，实现了即插即用的集成。\n\n![方法框架总览](https://arxiv.org/html/2512.22519v1/x3.png)\n> **图3**：OBEYED-VLA架构概览。来自基座和腕部摄像头的原始RGB图像首先通过分割网络获取物体级掩码。随后，基于VLM的物体中心接地模块选择与任务相关物体对应的掩码子集，而几何接地模块则对这些掩码区域应用深度估计，生成聚焦于这些区域的、抑制了杂波的几何感知观测。最终得到的感知接地观测，连同语言指令和机器人本体感知，被输入到一个预训练的VLA模型中，由其输出动作轨迹。\n\n感知接地模块包含两个核心组件：物体中心接地和几何接地。\n1.  **物体分割提议**：首先使用一个离线的分割模型处理双视角RGB图像，生成覆盖工作空间中所有可见物体（包括机械臂）的掩码提议。为了平衡分割的完整性和实时性，作者微调了YOLO11-Seg模型，其训练数据混合了自动标注的机器人演示数据和一个精选的室内桌面物品LVIS子集。\n2.  **物体中心接地**：该模块利用VLM（如Qwen3-VL）选择与任务相关的物体区域。具体过程分为两步（如**图4**所示）：\n    *   **任务感知的基座视图物体接地**：首先，VLM解析语言指令，列出任务相关的物体名称。接着，在基座视图图像上，使用“set-of-mark”提示机制，在每个分割掩码区域内覆盖一个数字标记。然后，VLM根据物体名称和标记增强的图像，识别出与指令相关的标记，从而筛选出相关的掩码子集。在每次任务开始时执行一次此步骤，并跟踪所选掩码。\n    *   **跨视图区域匹配**：将上一步筛选出的基座视图掩码区域裁剪为物体中心的参考视图。对于腕部视图，同样生成标记增强的图像。然后，VLM被提示将腕部视图中的标记与基座参考视图进行匹配，从而筛选出腕部视图中与任务相关的掩码子集。\n3.  **几何接地**：对于上述物体中心接地模块筛选出的相关掩码区域，首先在原始RGB图像中抑制所有背景像素（包括无关物体），生成仅保留任务相关物体的“净化”RGB视图。然后，使用零样本深度估计器将这些区域的RGB像素转换为深度图，从而保留物体的3D形状和空间布局，同时丢弃颜色、纹理等外观线索，促使策略依赖几何而非表面视觉关联。\n\n![物体中心接地模块](https://arxiv.org/html/2512.22519v1/x4.png)\n> **图4**：物体中心接地模块。该模块分两阶段运行。首先，VLM解析任务指令以提取任务相关物体，并利用基座视图分割掩码上的“set-of-mark”提示来选择对应这些物体的区域。裁剪选定的基座视图区域以生成物体中心参考视图。\n\n与现有方法相比，OBEYED-VLA的核心创新在于：1) **显式解耦**：将感知接地作为独立于动作推理的预处理模块，避免了端到端动作优化对VLM继承的视觉-语言对齐的侵蚀。2) **双重视觉接地**：结合了语义（VLM驱动的物体选择）和几何（RGB转深度）两种接地形式，强化了空间理解和抗外观干扰能力。3) **数据效率**：无需在训练数据中合成杂乱场景或引入额外的感知监督损失，仅使用干净的单物体演示微调VLA即可。\n\n## 实验与结果\n实验在一个真实的UR10e桌面机器人平台上进行。评估使用了四种具有挑战性的场景：存在干扰物、目标缺失指令、背景外观变化以及操作未见过的物体。基线方法包括当前先进的VLA模型，如Octo、RoboFlamingo、OpenVLA、π0、π0-FAST和π0.5。\n\n![目标缺失检查](https://arxiv.org/html/2512.22519v1/x2.png)\n> **图2**：视觉-语言接地的目标缺失完整性检查。我们报告了每个（请求的，展示的）物体对的抓取率（%），计算了所有请求（行）和展示（列）物体组合的20次运行结果。非对角线上的高强度值直接揭示了当请求物体缺失时策略执行抓取的频率，显示了基线模型的语言接地问题。\n\n关键实验结果如下：\n1.  **目标缺失拒绝**：在目标物体缺失的指令下，OBEYED-VLA的抓取率接近于0%，而所有基线VLA模型的抓取率均超过75%，表明OBEYED-VLA能有效遵循语言指令并拒绝不合理请求。\n2.  **抗干扰物能力**：在存在干扰物体的场景中，OBEYED-VLA取得了最高的成功率。例如，在“按物体标识抓取”任务中，OBEYED-VLA成功率为95.0%，而最好的基线π0.5为82.5%；在更难的“按空间关系抓取”任务中，OBEYED-VLA成功率为90.0%，显著高于π0.5的62.5%。\n\n![干扰物场景结果](https://arxiv.org/html/2512.22519v1/x6.png)\n> **图6**：在具有干扰物体的杂乱场景中的成功率。OBEYED-VLA在两种干扰设置（按标识和按空间关系）下均优于所有基线。\n\n3.  **背景外观变化鲁棒性**：当桌面背景材质和颜色发生改变时，OBEYED-VLA保持了90.0%的成功率，而基线模型（如π0.5）的性能从82.5%下降至约65.0%，显示了其对表面外观变化的强鲁棒性。\n\n![背景变化结果](https://arxiv.org/html/2512.22519v1/x7.png)\n> **图7**：背景外观分布变化下的成功率。OBEYED-VLA在不同背景上保持了稳定的高性能，而基线模型性能显著下降。\n\n4.  **泛化到未见物体**：在操作训练中未出现过的全新物体时，OBEYED-VLA取得了87.5%的成功率，远超所有基线模型（最好的基线为37.5%），证明了其卓越的泛化能力。\n\n![未见物体泛化结果](https://arxiv.org/html/2512.22519v1/x8.png)\n> **图8**：泛化到未见目标物体。OBEYED-VLA在操作训练期间未见过的物体时，成功率显著高于所有基线。\n\n5.  **真实世界闭环操控**：在包含干扰物、目标缺失和未见物体等多种挑战的复杂闭环操作任务中，OBEYED-VLA实现了100%的任务完成率，而基线模型（π0.5和OpenVLA）的完成率分别为20%和40%。\n\n![闭环任务结果](https://arxiv.org/html/2512.22519v1/x9.png)\n> **图9**：真实世界闭环操作任务的完成率，该任务结合了干扰物、目标缺失和未见物体的挑战。\n\n6.  **消融实验**：消融研究（**图11**）证实了物体中心接地和几何接地两个组件都是关键贡献者。移除几何接地（仅用净化RGB）或移除物体中心接地（仅用原始深度）都会导致性能显著下降，尤其是在背景变化和未见物体泛化场景中。两者结合带来了最大的性能增益。\n\n![消融实验](https://arxiv.org/html/2512.22519v1/x11.png)\n> **图11**：OBEYED-VLA关键组件的消融研究。结果表明，物体中心接地和几何接地对于在背景变化和未见物体泛化等挑战中实现鲁棒性都至关重要。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了OBEYED-VLA框架，通过显式解耦感知接地与动作推理，将物体中心和几何感知的观测增强现有VLA模型。2) 在真实世界实验中证明，该框架能大幅提升VLA在干扰物、目标缺失、背景变化和未见物体操作等多种挑战下的鲁棒性和泛化性，且仅需使用干净的单物体演示数据进行微调。3) 消融实验验证了语义和几何双重接地机制的必要性。\n\n论文自身提到的局限性包括：感知模块依赖于分割模型和VLM，可能引入额外的计算延迟；当前框架主要针对静态场景（除机械臂外），物体中心接地在任务开始时执行一次，对于高度动态的环境可能需要更频繁的调用。\n\n这项工作对后续研究的启示是：将感知作为显式、模块化的组件是强化VLA模型可靠性和泛化能力的有效途径。这种解耦设计允许独立改进感知模块（如使用更快的模型或更细粒度的接地），并能够便捷地适配到不同的VLA主干、环境和机器人平台上，为构建更稳健的通用机器人系统提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2512.22519v1/x1.png",
        "https://arxiv.org/html/2512.22519v1/x2.png",
        "https://arxiv.org/html/2512.22519v1/x3.png",
        "https://arxiv.org/html/2512.22519v1/x4.png",
        "https://arxiv.org/html/2512.22519v1/x5.png",
        "https://arxiv.org/html/2512.22519v1/x6.png",
        "https://arxiv.org/html/2512.22519v1/x7.png",
        "https://arxiv.org/html/2512.22519v1/x8.png",
        "https://arxiv.org/html/2512.22519v1/x9.png",
        "https://arxiv.org/html/2512.22519v1/x10.png",
        "https://arxiv.org/html/2512.22519v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.21586",
      "title": "Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations",
      "url": "http://arxiv.org/abs/2512.21586",
      "arxivId": "2512.21586",
      "date": "2025-12-25",
      "authors": "Dongbin Zhao Team",
      "category": "Manipulation",
      "summary": "本文针对从无动作标签的视频中进行高效模仿学习（ILV）的挑战，提出BCV-LR框架。其核心方法是通过自监督任务从视频提取动作相关潜在特征，并利用基于动态的无监督目标预测帧间潜在动作；随后在线微调，将潜在动作与真实动作空间对齐以进行行为克隆，形成迭代的策略改进循环。实验表明，在离散与连续控制任务中，BCV-LR仅需极少量交互即可达到专家级性能，在24/28的任务上样本效率超越了现有ILV基线和有环境奖励的强化学习方法。",
      "detailedSummary": "## 研究背景与动机\n当前从视频中模仿学习的主流方法主要分为两类：逆强化学习方法和监督式模仿学习方法。逆强化学习方法试图从视频中提取奖励信号，然后通过强化学习进行模仿。然而，训练额外的奖励预测网络和RL价值网络都需要对观测空间进行大量探索，这导致其样本效率通常低于基于固定专家奖励的传统RL方法。另一类监督式方法旨在通过环境交互预测缺失的专家动作，并进行行为克隆。这类方法对环境动力学的了解要求较低，在状态模仿中展现了降低环境样本依赖的潜力。但在视觉模仿任务中，解析观测和预测专家动作的难度急剧增加，现有方法常遇到性能瓶颈，无法学到接近专家水平的策略。本文针对的核心痛点是：在仅能访问视频作为监督信号的视觉策略学习中，能否平衡有效性与样本效率？本文提出了一种新视角：通过潜在表示进行行为克隆，利用离线预训练从视频中提取丰富知识，以进行高效的在线适应。其核心思路是：通过自监督任务从视频帧中提取与动作相关的潜在特征，并利用基于动力学的无监督目标预测帧间潜在动作；在线阶段，利用收集的无奖励交互数据微调潜在动作并将其对齐到真实动作空间进行行为克隆，克隆的策略反过来丰富交互经验，形成高效的迭代策略改进。\n\n## 方法详解\nBCV-LR的整体框架包含离线预训练和在线微调两个阶段。输入是专家视频帧序列，输出是可执行的策略。离线阶段，从视频中提取潜在特征并预测潜在动作；在线阶段，利用环境交互微调对齐动作并克隆策略，最终策略由特征编码器、潜在策略和动作解码器构成。\n\n![方法框架](https://arxiv.org/html/2512.21586v1/x2.png)\n> **图2**：BCV-LR不同阶段的训练目标。首先在视频上预训练自监督特征编码器f。基于潜在特征，使用可训练的世界模型w和潜在动作预测器p，通过基于动力学的无监督目标优化，获取视频帧间的潜在动作。在线阶段，利用收集的无奖励转移数据，通过预训练的世界模型w微调潜在动作，并通过潜在动作解码器d将其对齐到真实动作空间。同时，训练一个克隆潜在动作的潜在策略π，它共享特征编码器f和动作解码器d与环境交互，从而丰富收集的数据以进行进一步的潜在动作微调，形成迭代改进。\n\n核心模块包括：\n1.  **自监督特征编码器 (f)**：作用是从高维视频输入中提取与决策相关的信息。其训练是任务自适应的：对于视觉复杂、动力学相对简单的任务（如Procgen），采用对比学习目标（对齐同一观测的不同随机偏移图像）并结合自重建任务；对于部分可观测、动力学复杂的任务（如DMControl），则采用基于原型的时序关联任务（使用Sinkhorn-Knopp算法对齐时序相邻的观测）。损失函数如论文公式(1)所示，包含对比损失和重建损失。\n2.  **潜在动作预测器 (p) 与世界模型 (w)**：在预训练的特征基础上，预测器p用于生成连续帧对之间的潜在动作z_i^v。为了以无监督方式获取z_i^v，引入世界模型w，其目标是根据当前特征s_i^v和预测的（量化后）潜在动作z_i^{vq}重建下一帧特征s_{i+1}^v。为避免平凡解（如p直接复制下一帧特征），对z_i^v进行向量量化离散化。训练损失为重建损失 ℒ_la = ||w(s_i^v, z_i^{vq}) - s_{i+1}^v||^2，p和w通过此损失联合优化。\n3.  **潜在动作解码器 (d) 与在线微调**：在线阶段，智能体与环境交互收集带真实动作的转移数据。潜在动作预测器p和新增的解码器d在这些数据上进行微调与对齐。学习目标包含两部分：动作预测损失（解码器d输出的动作分布与真实动作的交叉熵或MSE损失），以及由预训练世界模型w提供的未来重建损失，如公式(3)所示。这确保了p在适应非专家动作标签时，仍保持对环境动力学的理解。\n4.  **潜在策略 (π) 与行为克隆**：通过共享预训练的编码器f和解码器d，只需训练一个潜在策略π，将编码后的专家观测特征s_i^v映射到预测的潜在动作z_i^v。行为克隆损失为 ℒ_bc = ||π(s_i^v) - z_i^v||^2。策略π输出的潜在动作经解码器d解码后执行。克隆的策略以更优性能交互，收集更高质量的数据，进而促进潜在动作的进一步微调，形成迭代改进循环。\n\n与现有方法相比，BCV-LR的创新点具体体现在：1）**引入潜在动作作为中介**：通过基于动力学的无监督目标从视频中提取离散化的潜在动作，避免了直接从像素预测真实动作的困难。2）**离线-在线迭代优化机制**：预训练的模型为在线学习提供了良好的初始化；在线微调不仅对齐动作空间，还利用克隆策略收集的新数据持续改进潜在动作预测和策略本身，极大地提升了样本效率。3）**灵活的自监督特征学习**：可根据任务特性（视觉复杂性、时序依赖性）选择或设计最合适的自监督任务来预训练特征编码器。\n\n## 实验与结果\n实验使用了三个基准测试集：16个离散控制任务来自Procgen基准，12个连续控制任务来自Deepmind Control Suite和Metaworld。对比的基线方法包括：ILV方法UPESV、LAIFO、ILPO、BCO；以及RL方法（提供环境奖励）LAPO、PPO（Procgen）、DrQv2、TACO（DMControl）。\n\n![结果摘要](https://arxiv.org/html/2512.21586v1/x1.png)\n> **图1**：BCV-LR在仅允许10万次交互的情况下，在离散任务“Bossfight”和连续任务“reacher_hard”上达到了专家级别的策略性能，超越了先进的ILV和RL基线，展示了无专家动作或奖励的样本高效视频模仿学习能力。\n\n关键实验结果如下：\n在Procgen的16个任务上，仅允许10万次环境交互，BCV-LR取得了平均13.8分的性能（经视频归一化后的平均分为0.79），显著优于其他ILV基线（UPESV: 9.0/0.58， BCO: 4.8/0.38， ILPO: 2.1/0.22， LAIFO: 2.2/0.22），甚至超过了使用专家视频辅助的RL方法LAPO（6.8/0.48）和仅使用奖励的PPO（2.3/0.22）。在多个任务上（如Bigfish, Bossfight）接近或达到专家水平。\n\n![DMControl训练曲线](https://arxiv.org/html/2512.21586v1/x3.png)\n> **图3**：DMControl上ILV方法的在线训练曲线。BCV-LR能够高效利用环境样本，在5万步（甚至在某些任务上2万步）内学习到有效策略。\n\n在DMControl的8个连续任务上，BCV-LR同样表现出色。例如在`point_mass_easy`和`reacher_hard`任务上，BCV-LR分别获得800分和900分，远超所有其他ILV基线（LAIFO、BCO、UPESV均低于300分），并与需要环境奖励的先进RL方法TACO、DrQv2性能相当甚至更优。\n\n![消融实验](https://arxiv.org/html/2512.21586v1/x4.png)\n> **图4**：在Procgen任务上的消融研究。移除世界模型（w）的未来重建损失（`Ours w/o w`）或移除潜在动作的向量量化（`Ours w/o VQ`）均会导致性能显著下降，验证了这两个组件对于学习有意义潜在动作的重要性。\n\n消融实验总结了每个核心组件的贡献：\n1.  **世界模型（w）的未来重建损失**：移除后（`Ours w/o w`）性能大幅下降，说明该损失对于约束潜在动作包含动力学信息至关重要。\n2.  **潜在动作的向量量化（VQ）**：移除后（`Ours w/o VQ`）性能也显著降低，验证了离散化能迫使模型寻找跨转移对的共性，避免平凡解。\n3.  **在线迭代微调**：仅进行离线预训练而不进行在线微调（`Ours w/o online`）性能最差，证明了在线交互对齐和策略迭代改进的必要性。\n\n![视频数据效率](https://arxiv.org/html/2512.21586v1/x5.png)\n> **图5**：视频数据效率分析。BCV-LR仅需约5-10个专家轨迹（相当于约1-2万帧）的预训练视频数据，性能即趋于饱和，显示出对视频数据的高效利用。\n\n其他分析实验表明，BCV-LR对预训练视频数据量需求很低（约5-10条轨迹即饱和），且具备一定的跨任务适应潜力。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了BCV-LR框架，首次证明了仅使用视频作为专家监督信号即可实现**样本高效的视觉策略学习**，无需专家动作或奖励。2）设计了一种通过**潜在动作表示**进行离线预训练和在线迭代微调的新范式，有效平衡了模仿性能与样本效率。3）在包含离散和连续的28项挑战性视觉任务上进行了广泛实验，结果表明BCV-LR在样本效率上超越了当前先进的ILV和RL基线。\n\n论文自身提到的局限性包括：1）**计算成本**：离线预训练阶段需要额外的计算资源。2）**任务通用性**：当前的自监督任务设计可能无法覆盖所有类型的视觉控制问题。3）**潜在动作离散化**：向量量化可能对某些需要精细连续动作的任务引入近似误差。\n\n对后续研究的启示：1）证明了从无动作视频中提取结构化、可执行知识是可行且高效的路径。2）**离线预训练与在线微调相结合**的范式为解决样本效率问题提供了有力工具。3）**潜在表示与模型预测**的结合，为处理高维观测和复杂动力学开辟了新方向。未来工作可以探索更强大的自监督表征学习方法、更高效的潜在动作空间构建技术，以及将该框架扩展到更复杂的多模态或现实世界任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.21586v1/x1.png",
        "https://arxiv.org/html/2512.21586v1/x2.png",
        "https://arxiv.org/html/2512.21586v1/x3.png",
        "https://arxiv.org/html/2512.21586v1/x4.png",
        "https://arxiv.org/html/2512.21586v1/x5.png",
        "https://arxiv.org/html/2512.21586v1/x6.png",
        "https://arxiv.org/html/2512.21586v1/x7.png",
        "https://arxiv.org/html/2512.21586v1/x8.png",
        "https://arxiv.org/html/2512.21586v1/x9.png",
        "https://arxiv.org/html/2512.21586v1/x10.png",
        "https://arxiv.org/html/2512.21586v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.21898",
      "title": "Flexible Multitask Learning with Factorized Diffusion Policy",
      "url": "http://arxiv.org/abs/2512.21898",
      "arxivId": "2512.21898",
      "date": "2025-12-26",
      "authors": "Yilun Du Team",
      "category": "Manipulation",
      "summary": "本文提出Factorized Diffusion Policy (FDP)，以解决多任务模仿学习中机器人动作分布高度多模态、现有单一策略难以有效拟合和灵活适应的问题。其核心方法是一种模块化扩散策略框架，将复杂动作分布分解为多个专门的扩散模型，每个捕获不同的行为子模式，并通过观察条件路由器在推理时动态组合。该方法基于组合扩散建模，使用连续分数聚合而非离散专家选择，以实现稳定训练并促进模块间清晰分工。实验表明，FDP在模拟基准（MetaWorld、RLBench）和真实机器人操作中，均持续优于现有的模块化及单一模型基线。",
      "detailedSummary": "## 研究背景与动机\n模仿学习已成为获取复杂机器人操作技能的有力范式。然而，将其成功扩展到多任务场景仍面临重大挑战。随着任务种类的增加，底层的动作分布变得高度多模态和多样化，通常涉及针对不同物体的不同控制策略。传统的**一体化策略**（monolithic policies）难以跨任务泛化、表示多种行为模式或高效适应新技能。为了应对这些限制，**模块化策略架构**，尤其是**混合专家模型**（Mixture-of-Experts, MoE）已成为一个有前景的方向。通过将策略分解为专门的组件，模块化方法提高了跨任务的可扩展性和重用性。然而，现有的基于MoE的方法通常存在**训练不稳定**、缺乏原则性的概率公式、以及产生的专家模块角色不清晰或重叠的问题，这限制了其可解释性。\n\n本文针对上述痛点，提出了一种新的模块化扩散策略框架。核心思路是：将复杂的多模态动作分布**分解**为一组专门的扩散模型的**组合**，每个模型捕获行为空间中的一个独特子模式，并通过一个观测条件路由器在推断时动态组合，从而实现更有效的策略学习、更清晰的技能分解以及更灵活的适应能力。\n\n## 方法详解\n本文提出的**因子化扩散策略**（Factorized Diffusion Policy, FDP）旨在开发一种可扩展至多样化操作任务并支持高效适应新任务的模块化策略架构。\n\n![方法框架](https://arxiv.org/html/2512.21898v2/x1.png)\n> **图1**：FDP概述。(a) 给定观测 $\\mathbf{o}_{t}$，多个扩散专家在每个去噪步骤预测噪声估计 $\\bm{\\varepsilon}_{i}(\\mathbf{a}_{t}^{K},\\mathbf{o}_{t})$。一个轻量级路由器网络计算依赖于观测的权重 $\\{w_{i}\\}$，用于将最终噪声估计组合为加权和（见(c)）。组合后的噪声估计指导超过 $K$ 步的迭代去噪过程以生成动作 $\\mathbf{a}_{t}$。(b) 这种组合结构使FDP能够建模复杂的多模态分布，并支持通过选择性调整或添加扩散组件进行模块化适应。\n\n**整体框架与核心模块**：FDP将策略分解为一组可组合的扩散模型。如图1所示，其工作流程如下：\n1.  **输入**：当前观测 $\\mathbf{o}_{t}$（例如RGB图像和关节角度）。\n2.  **路由与组合**：一个轻量级的、观测条件的多层感知机（MLP）**路由器**根据 $\\mathbf{o}_{t}$ 预测一组权重 $\\{w_{t,i}\\}$。同时，每个**扩散专家**（一个去噪网络 $\\bm{\\varepsilon}_{\\theta_i}$）接收加噪的动作 $\\mathbf{a}_{t}^{k}$ 和观测 $\\mathbf{o}_{t}$，预测其对应的噪声 $\\bm{\\varepsilon}_{\\theta_i}(\\mathbf{a}^{k}_{t},\\mathbf{o}_{t},k)$。所有专家的预测通过路由器权重进行加权求和，得到组合噪声估计：$\\sum_i w_{t,i} \\bm{\\varepsilon}_{\\theta_i}$。\n3.  **迭代去噪与输出**：从高斯噪声 $\\mathbf{a}_{t}^{K}$ 开始，使用上述组合噪声估计按照去噪扩散概率模型（DDPM）的更新规则进行 $K$ 步迭代去噪，最终生成干净的动作 $\\mathbf{a}_{t}^{0}$ 作为策略输出。\n\n**概率建模与训练**：从概率角度看，FDP将目标动作分布建模为多个组件分布的加权乘积：$p(\\mathbf{a}_{t}|\\mathbf{o}_{t})\\propto\\prod_{i}p_{i}(\\mathbf{a}_{t}|\\mathbf{o}_{t})^{w_{t,i}}$。这对应于满足所有组件约束（可视为行为约束，如避障、精确抓取）的样本。每个组件分布 $p_i$ 由一个扩散模型建模。训练时，采用修改后的均方误差（MSE）损失联合优化所有扩散专家和路由器：\n$$\\mathcal{L}_{\\text{MSE}}=\\|\\bm{\\epsilon}^{k}-\\sum_{i}w_{t,i}\\,\\bm{\\varepsilon}_{\\theta_{i}}(\\mathbf{a}^{0}_{t}+\\bm{\\epsilon}^{k},\\mathbf{o}_{t},k)\\|^{2}_{2}$$\n其中 $\\mathbf{a}^{0}_{t}$ 是示范轨迹样本，$\\bm{\\epsilon}^{k}$ 是添加的噪声。这种**端到端的联合优化**确保了所有组件在每次训练中都能获得梯度信号，促进了功能专业化。\n\n**任务适应机制**：FDP的模块化结构支持高效适应新任务。适应时，不是重新训练整个模型，而是引入一个新的扩散组件 $\\bm{\\varepsilon}_{\\theta_{\\text{new}}}$，其权重通过**循环利用**（upcycling）从现有组件复制初始化。更新后的噪声估计函数变为：$\\bm{\\varepsilon}_{\\text{adapt}} = \\sum_{i}w_{i}\\,\\bm{\\varepsilon}_{\\theta_{i}} + w_{\\text{new}}\\,\\bm{\\varepsilon}_{\\theta_{\\text{new}}}$。在适应训练期间，**冻结**所有先前训练的组件 $\\{\\bm{\\varepsilon}_{\\theta_i}\\}$，仅更新新组件 $\\bm{\\varepsilon}_{\\theta_{\\text{new}}}$ 和路由器。这极大地减少了可训练参数的数量，并有效缓解了灾难性遗忘。\n\n**核心创新点**：与现有MoE方法（如SDP、MoDE）的**离散专家选择**不同，FDP采用**连续的分数（噪声估计）聚合**。这避免了硬路由决策，确保了所有模块在优化过程中保持活跃，从而带来了更稳定的训练、更好的负载平衡以及更清晰的专家专业化。\n\n## 实验与结果\n**实验设置**：实验在模拟基准**MetaWorld**（6个任务）、**RLBench**（8个任务）和**LIBERO**，以及真实世界机器人操作平台（UR5e机械臂）上进行。输入为RGB图像和关节角度，输出为绝对关节角度轨迹。\n\n**对比基线**：\n*   **DP**：一体化扩散策略。\n*   **SDP**：基于MoE的扩散策略，采用观测条件路由。\n*   **MoDE**：MoE变体，基于噪声水平进行路由。\n\n**关键实验结果**：\n\n![多任务学习结果表](https://arxiv.org/html/2512.21898v2/x7.png)\n> **图7**：MetaWorld和RLBench上的多任务学习评估结果表。FDP在MetaWorld（平均74.8%）和RLBench（平均63.9%）上均取得了最高的平均成功率，超越了所有一体化及模块化基线。\n\n![真实世界多任务结果表](https://arxiv.org/html/2512.21898v2/x8.png)\n> **图8**：真实世界多任务成功率表（Cube Red和Hang Low）。FDP取得了最佳或并列最佳的性能（15/20和17/20）。\n\n多任务学习实验表明，FDP在模拟和真实环境中均一致性地优于强基线。DP在某些单模态任务上表现良好，但在复杂多模态任务上不足。SDP和MoDE受限于MoE训练的不稳定性。FDP的连续组合机制实现了更稳定的训练和更好的专业化。\n\n![任务适应结果表](https://arxiv.org/html/2512.21898v2/x9.png)\n> **图9**：MetaWorld和RLBench上的适应评估结果表。比较了全参数微调、仅调整路由器、调整路由器+观测编码器、以及**添加新模块**等适应策略。FDP采用“添加新模块”策略时表现最佳（MetaWorld 92.1%， RLBench 79.8%）。\n\n![真实世界适应结果表](https://arxiv.org/html/2512.21898v2/x10.png)\n> **图10**：真实世界适应成功率表（Cube Blue和Hang High）。同样，FDP在“添加新模块”的适应策略下取得了最佳性能（17/20和17/20）。\n\n任务适应实验验证了FDP模块化设计的优势。在有限新演示数据下，“添加新模块”的适应策略在几乎所有方法中都是最有效的，而FDP从中获益最大，因为它能够在不干扰已有技能的情况下，通过组合新组件高效学习新行为。\n\n![组件数量缩放](https://arxiv.org/html/2512.21898v2/x11.png)\n> **图11**：FDP组件数量对多任务性能影响的消融实验表。性能随组件数量增加而提升，在4个组件后趋于饱和（MetaWorld约91.3%，RLBench约63.9%）。\n\n![任务数量缩放](https://arxiv.org/html/2512.21898v2/x12.png)\n> **图12**：FDP相对于DP的成功率提升随任务数量增加而变化的曲线图。随着任务数量增加，FDP的优势变得更加明显，凸显了其处理复杂多任务分布的可扩展性。\n\n![部分重建加速](https://arxiv.org/html/2512.21898v2/x13.png)\n> **图13**：RLBench上部分重建（仅使用前2个专家）的结果表。`FDP_top2`实现了2倍的推理加速，性能仅相对下降19%，展示了部署时权衡计算成本与性能的灵活性。\n\n![持续适应](https://arxiv.org/html/2512.21898v2/x14.png)\n> **图14**：LIBERO基准上的持续适应结果表。从4个专家开始预训练，然后为每个新适应任务逐步添加新专家，最终达到12个专家。FDP在整个过程中保持了最高的平均性能（67.3%）。\n\n![真实世界定性对比](https://arxiv.org/html/2512.21898v2/x6.png)\n> **图15**：真实世界任务执行过程的定性对比图。顶部：Cube-X任务；底部：Hang-X任务。展示了FDP的成功案例以及基线方法的失败模式（如末端执行器姿态不精确）。\n\n**消融实验总结**：\n1.  **组件数量**：性能在组件数达到4个时趋于饱和，表明这是一个复杂度与性能的良好权衡点。\n2.  **任务数量**：FDP相对于一体化方法的优势随着任务数量的增加而增大，证明其更适合复杂的多任务分布。\n3.  **适应策略**：“添加新模块”是最有效的适应方式，平衡了性能、数据效率和计算开销。\n4.  **推理加速**：通过仅使用权重最高的前几个组件（如top-2），可以显著加速推理（2倍）而性能损失可控（相对下降19%），体现了部署灵活性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种**模块化扩散策略架构**（FDP），通过**观测条件组合采样**将专门化的扩散组件组合起来，为多模态动作分布建模提供了原则性的概率框架。\n2.  实证表明，该组合框架不仅**提高了多任务模仿学习的性能**，而且促进了跨扩散模块的**子技能分解**，增强了可解释性。\n3.  提出了一种简单有效的**策略适应方法**，通过选择性调整或**添加新的扩散组件**来适应新任务，实现了卓越的样本效率、模块化重用，并缓解了灾难性遗忘。\n\n**局限性**：论文提到，推理成本随扩散组件数量线性增加。虽然可以通过剪枝、蒸馏等技术优化，但这仍是实际部署中的一个考虑因素。此外，确定最佳组件数量（类似于网络深度或专家数）是一个需要根据任务集调整的超参数。\n\n**后续研究启示**：\n1.  **模块化策略设计**：FDP展示了将复杂策略分解为可组合、可解释的专门化模块的价值，这为设计更可扩展和可适应的机器人学习系统提供了新思路。\n2.  **稳定训练**：通过连续聚合而非离散选择来组合模块，为解决MoE类模型中常见的训练不稳定和负载不平衡问题提供了有效方案。\n3.  **高效适应**：“添加新模块+冻结旧模块”的适应范式为持续学习场景提供了高效且实用的解决方案，值得在更复杂的任务序列和跨域适应中进一步探索。\n4.  **可扩展性**：FDP支持异构架构（组件可不同），未来可研究如何动态分配计算资源（如根据任务复杂度自适应调整激活的组件数量或大小），以实现更优的性能-效率权衡。",
      "imageUrls": [
        "https://arxiv.org/html/2512.21898v2/x1.png",
        "https://arxiv.org/html/2512.21898v2/figures/real_setup_descrip.jpg",
        "https://arxiv.org/html/2512.21898v2/figures/hang-low_layout.jpg",
        "https://arxiv.org/html/2512.21898v2/figures/cube-X_layout.jpg",
        "https://arxiv.org/html/2512.21898v2/figures/real_cube_comparison_annotated.jpg",
        "https://arxiv.org/html/2512.21898v2/figures/real_hang_comparison_annotated.jpg",
        "https://arxiv.org/html/2512.21898v2/x2.png",
        "https://arxiv.org/html/2512.21898v2/x3.png",
        "https://arxiv.org/html/2512.21898v2/x4.png",
        "https://arxiv.org/html/2512.21898v2/x5.png",
        "https://arxiv.org/html/2512.21898v2/x6.png",
        "https://arxiv.org/html/2512.21898v2/x7.png",
        "https://arxiv.org/html/2512.21898v2/x8.png",
        "https://arxiv.org/html/2512.21898v2/x9.png",
        "https://arxiv.org/html/2512.21898v2/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.21970",
      "title": "StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision",
      "url": "http://arxiv.org/abs/2512.21970",
      "arxivId": "2512.21970",
      "date": "2025-12-26",
      "authors": "He Wang Team",
      "category": "Manipulation",
      "summary": "论文StereoVLA针对现有视觉-语言-动作模型因依赖单视图RGB而几何感知不足的问题，提出利用立体视觉增强空间感知。核心方法包括Geometric-Semantic特征提取模块，融合立体差异的几何特征和单眼语义特征，以及辅助交互区域深度估计任务以加速收敛。实验表明，该方法在立体设置下多种任务中大幅优于基线模型，并对相机姿态变化表现出强鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型通过端到端框架将视觉输入和语言指令映射为动作，展现出强大的泛化和语义理解能力。然而，为了与预训练的视觉语言模型对齐，主流的VLA模型依赖于单视角RGB图像作为视觉输入，这限制了对于精确操作至关重要的几何感知能力。现有工作主要通过补充其他传感器来增强几何感知，例如腕部相机、深度传感器或额外的第三人称相机，但这些方案分别存在视野受限易遮挡、对透明/反光物体测量噪声大、硬件设计复杂且视角多样性阻碍泛化等问题。\n\n本文针对VLA模型缺乏精确几何感知这一核心痛点，提出了一个受人类视觉启发的全新视角：利用立体视觉。立体相机模拟人类双眼视觉，通过双目视差提供鲁棒的空间线索，且无需增加额外硬件。尽管立体视觉在计算机视觉领域已有成熟基础模型，但其在VLA模型中的集成与系统性评估尚未得到充分探索。本文的核心思路是：通过一个新颖的几何-语义特征提取模块，从立体图像中融合密集的几何特征和丰富的语义特征，并引入一个交互区域深度估计的辅助任务，以增强模型对细粒度空间细节的感知，从而提升VLA模型在精确操作任务中的性能。\n\n## 方法详解\nStereoVLA的整体框架如图2(a)所示。输入为立体图像对和语言指令。首先，几何-语义特征提取模块将立体图像编码为兼具几何精度和语义丰富度的视觉token。这些视觉token与语言token一同输入预训练的大语言模型主干进行联合处理。利用模型中间产生的视觉-语言特征，一个动作专家网络通过流匹配策略预测以末端执行器位姿增量表示的动作块。在训练阶段，引入一个交互区域深度估计的辅助任务以增强几何学习，该任务在推理阶段不产生额外开销。\n\n![方法框架](https://arxiv.org/html/2512.21970v1/x2.png)\n> **图2**：(a) StereoVLA整体架构。立体图像对被几何-语义特征提取模块编码为视觉token，与语言token一起输入大语言模型主干。动作专家预测末端执行器位姿增量，辅助深度估计任务在训练中增强几何学习。(b) 几何-语义特征提取模块细节。利用FoundationStereo提取几何特征，利用SigLIP和DINOv2从左视图提取语义特征，然后通过MLP投影器融合为统一的视觉表示。\n\n**核心模块一：几何-语义特征提取**\n该模块旨在从立体图像中提取并融合几何和语义特征，具体流程见图2(b)。\n1.  **几何特征提取**：使用专为深度估计设计的FoundationStereo模型。给定立体图像对，模型首先生成单目特征，然后构建一个4D代价体积。为了建模长程关联，该代价体积会经过一个基于注意力的混合代价滤波模块，得到滤波后的代价体积。论文选择这个**滤波后的代价体积**作为几何特征源，因为它包含了经过长程关联增强的密集几何信息，同时避免了后续迭代细化带来的巨大计算开销。\n2.  **语义特征提取**：由于FoundationStereo缺乏语义信息，需要从图像中补充。论文遵循PrismaticVLM，使用SigLIP和DINOv2从左视图提取语义丰富的特征。SigLIP擅长捕获高级语义，DINOv2擅长捕获视觉细节。仅使用左视图是出于计算效率考虑，因为左右视图的语义信息高度冗余。\n3.  **特征融合**：将FoundationStereo的几何特征图通过空间池化调整到与SigLIP/DINOv2特征相同的空间步长，然后将三者沿通道维度拼接，最后通过一个MLP投影器生成最终的混合视觉token序列。论文指出，采用通道拼接而非token序列拼接，可以避免视觉token数量翻倍，从而减少训练和推理的计算开销。\n\n**核心模块二：交互区域深度估计**\n这是一个与动作预测协同训练的辅助任务，旨在增强模型对操作关键区域细粒度几何信息的捕获能力。具体而言，模型被训练来预测图像中给定采样点的度量深度。关键创新在于采样策略：不是在全图均匀采样（容易采到无信息的背景），而是将采样点**限制在交互区域内**，即包含夹爪和目标物体的边界框区域。这种聚焦采样促使模型学习物体几何及其与夹爪的空间关系，有益于提升操作精度和训练收敛速度。\n\n**与现有方法的创新点**\n1.  **针对立体视觉的专门设计**：不同于将立体图像对简单视为两个独立视图输入现有多相机VLA模型，本文设计了专门的几何-语义特征提取模块，有效处理了立体视图间的细微差异，并融合了来自不同基础模型的优势特征。\n2.  **高效的特征选择与融合**：选择了FoundationStereo中信息最丰富的滤波后代价体积作为几何特征，并采用通道拼接的融合方式，在保证性能的同时优化了计算效率。\n3.  **任务驱动的辅助学习**：提出的交互区域深度估计任务，通过聚焦于操作关键区域，更高效地引导模型学习对操作至关重要的空间细节。\n\n## 实验与结果\n**实验设置**：评估在真实机器人实验和模拟环境中进行。使用了包含通用任务（拾放、堆叠）、抓取不同朝向的条形物体、抓取中小型物体的综合任务套件。实验平台配置了Franka机器人和Zed Mini立体相机，并系统性地设置了前视和侧视相机位姿的小、中、大随机化范围以测试鲁棒性。\n\n**对比基线**：由于没有现成的面向立体设置的VLA，论文对多个SOTA开源VLA模型使用完整的合成立体数据集进行了训练/微调以进行公平比较。包括：SpatialVLA及其使用立体深度估计的变体SpatialVLA-D、适应立体输入的π0.5-S和GraspVLA-S。\n\n![实验结果总览](https://arxiv.org/html/2512.21970v1/x3.png)\n> **图3**：真实世界评估任务及结果。StereoVLA在所有任务类型上均取得了最高的成功率，尤其在抓取条形物体和小物体等需要高精度的任务上优势明显。\n\n**关键实验结果**：\n1.  **整体性能**：如图3所示，StereoVLA在所有评估任务上取得了最强且最一致的性能。在通用任务上，其成功率比基线平均高出33%。对于0°、45°、90°朝向的条形物体抓取，StereoVLA取得了近乎完美或完美的结果。在最具挑战性的小物体抓取任务上，StereoVLA取得了30.0%的成功率，而所有基线模型均完全失败。\n2.  **相机设置对比**：如表II所示，在相机位姿小范围随机化下，前+侧视配置的GraspVLA性能最佳（82.5%），StereoVLA紧随其后（79.3%）。但随着位姿随机化范围增大，前+侧视配置性能急剧下降，而StereoVLA的下降最为平缓，在中等和大幅度随机化下均取得了最高成功率（71.9%和61.3%），展现了其对相机位姿变化最强的鲁棒性。\n3.  **消融实验**：\n    *   **特征选择**：如表I所示，使用滤波后代价体积V_c‘作为几何特征取得了最高成功率（77.0%），优于原始代价体积V_c（69.0%）和相关体积V_corr（54.0%）。同时，融合语义特征相比仅使用几何特征，成功率有显著提升，验证了语义信息对于视觉-语言对齐的必要性。\n    ![特征融合消融](https://arxiv.org/html/2512.21970v1/fig/feature_fusion_results.png)\n    > **图5**：特征融合策略对比。序列拼接（Sequence Concat）导致成功率略有下降且需要更长的训练时间，而通道拼接（Channel Concat）在性能和效率上更优。\n    *   **特征融合策略**：如图5所示，通道拼接特征融合相比序列拼接，取得了更高的成功率且训练时间更短。\n    ![深度估计消融](https://arxiv.org/html/2512.21970v1/fig/depth-est-results.png)\n    > **图6**：深度估计方法对比。在交互区域（Interaction Region）进行深度估计取得了最高的成功率，优于在全图（Full Image）或仅物体区域（Object Region）采样。\n    *   **深度估计任务**：如图6所示，在交互区域进行深度估计取得了最高的成功率，验证了该设计的有效性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了首个系统性利用立体视觉增强几何感知的VLA模型StereoVLA。\n2.  设计了新颖的几何-语义特征提取模块，能够从立体图像中高效提取并融合密集几何特征和丰富语义特征。\n3.  引入了交互区域深度估计这一辅助任务，有效提升了模型对操作关键区域细粒度空间关系的理解能力。\n\n**局限性**：论文指出，当前图像分辨率（224x224）限制了对小物体（仅占几个像素）的感知和定位精度。虽然使用更高分辨率的视觉骨干网络可能缓解此问题，但会带来巨大的计算成本增加。\n\n**后续研究启示**：\n1.  **立体视觉在VLA中的潜力**：本研究证明了立体视觉在提供鲁棒几何线索、简化硬件部署以及增强对视角变化鲁棒性方面的显著优势，为VLA的感知模块设计提供了新方向。\n2.  **多模态特征融合**：如何更有效地融合来自不同预训练基础模型的异构特征（如几何专用模型和视觉语言模型），是一个具有普遍意义的研究课题。\n3.  **任务驱动的表征学习**：交互区域深度估计的成功表明，设计紧密围绕下游任务需求的辅助学习目标，可以更高效地引导模型学习关键表征。",
      "imageUrls": [
        "https://arxiv.org/html/2512.21970v1/x1.png",
        "https://arxiv.org/html/2512.21970v1/x2.png",
        "https://arxiv.org/html/2512.21970v1/x3.png",
        "https://arxiv.org/html/2512.21970v1/fig/layout.png",
        "https://arxiv.org/html/2512.21970v1/fig/feature_fusion_results.png",
        "https://arxiv.org/html/2512.21970v1/fig/depth-est-results.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.21043",
      "title": "Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction",
      "url": "http://arxiv.org/abs/2512.21043",
      "arxivId": "2512.21043",
      "date": "2025-12-24",
      "authors": "Takamitsu Matsubara Team",
      "category": "Manipulation",
      "summary": "该论文针对动态物体交互中因滚动接触、物体属性未知及外部感知不可靠导致的抓取滑动问题，提出一种基于触觉的抓取力控制方法。核心技术是物理信息能量抽象，将物体建模为虚拟能量容器，通过比较手指施加功率与物体保留能量来推断滑动；并采用基于模型的强化学习框架，学习能量流动力学，利用概率模型预测控制进行实时抓取力优化。实验表明，该方法可在几分钟内从零开始学习，有效减少滑动，延长抓取持续时间，且不依赖外部感知或物体先验知识。",
      "detailedSummary": "## 研究背景与动机\n在机器人灵巧操作中，通过指尖抓取物体并执行预期运动是常见任务。这类动态物体交互常伴随滚动接触，指尖滑动往往不可避免。物体稳定性依赖于调整指尖力以减少滑动。然而，在现实场景中，由于多移动接触的复杂动力学、未知物体属性（如质量、表面条件）以及视觉遮挡导致的外部感知不可靠，在动态交互中准确判断滑动极具挑战性。\n\n尽管存在挑战，人类仅凭触觉即可快速调节抓取力。这引出了一个核心问题：机器人如何仅使用触觉感知，学习在动态物体交互中调整抓取力以最小化滑动？现有的抓取力控制方法在面对现实不确定性时往往不足，而基于触觉的滑动感知方法多针对静态或准静态场景，在运动和未知表面条件下难以明确界定滑动，使得显式监督困难。\n\n本文针对上述痛点，提出了一种物理信息能量抽象方法，将物体建模为虚拟能量容器。通过比较指尖施加的总功率与物体保留能量的变化，其不一致性为推断滑动提供了物理依据的信号。核心思路是：将此能量抽象整合到基于模型的强化学习框架中，从触觉感知学习能量流动力学，并通过概率模型预测控制实时优化抓取力，从而在不依赖外部感知或先验物体知识的情况下减少滑动。\n\n## 方法详解\n本文方法的核心是将复杂的多指接触动力学抽象为能量流，并利用基于模型的强化学习进行学习和控制。\n\n![方法框架](https://arxiv.org/html/2512.21043v1/fig/intro.png)\n> **图1**：方法整体框架概述。物体被抽象为具有未知属性的能量容器，通过比较指尖施加的功率和物体保留能量的变化来推断能量损失，作为滑动的物理指标。这些能量量被表述为能量状态表示，用于基于模型的强化学习框架，以学习能量流动力学，并通过概率模型预测控制执行抓取力优化。\n\n**能量抽象**：假设物体为刚性、质量恒定但未知。将物体视为虚拟能量容器。计算所有`n`个指尖接触力（`F_con`）与接触点速度（`P_dot_con`）的点积之和的积分，得到施加的总能量`E^A`。同时，利用抓取质心`C_t`（接触位置的平均值）计算物体的假设保留能量`E^R`，其包含势能和动能项，与未知质量`m`成正比。在理想能量守恒下，总施加能量与总保留能量之差应为常数偏移。为消除该未知偏移，转而计算离散时间下的施加功率`P^A`和保留功率`P^R`。保留功率可分解为质量`m`与“无质量”保留功率`P~^R`的乘积。理论上，总施加功率应与总保留功率一致，因此可实时估计物体质量`m~_t = sum(P^A) / sum(P~^R)`。在恒定质量假设下，`m~_t`应保持稳定。实践中，滑动或不协调的力会导致能量损失，表现为估计质量的波动，这被解释为滑动感知稳定性的物理基础信号。\n\n![能量抽象示意图](https://arxiv.org/html/2512.21043v1/fig/energy_power_overview.png)\n> **图2**：多接触交互的能量抽象示意图。计算所有指尖接触的总施加功率，并与物体的保留功率进行比较。不一致的质量估计反映了能量损失，这可归因于滑动。这种物理信息抽象使得无需物体观测即可进行滑动感知的稳定性评估。\n\n**动力学学习与控制**：基于能量抽象，定义用于建模指-物能量流的状态`x_t`，包括施加功率`P^A_t`、无质量保留功率`P~^R_t`和抓取质心方向`Θ_t`，共10维。控制输入`u_t`为7维，包括期望的抓取质心线速度和角速度，以及抓取力`F*_t`。采用傅里叶特征线性高斯模型学习该能量状态的转移动力学，该模型在保持概率动力学建模优势的同时，实现了可扩展的学习和高频控制。\n\n抓取力通过概率模型预测控制优化确定。在每个时间步，名义运动生成器产生参考运动命令，pMPC则通过最小化预测时域`H`内预期功率差异（表现为估计质量的波动）来计算最优抓取力序列。优化后的第一个力值被用作当前施加的抓取力，并通过力封闭原理分配给各接触指尖，此过程忽略物体质量。\n\n![控制框架](https://arxiv.org/html/2512.21043v1/fig/control_framework.png)\n> **图4**：集成能量抽象和MBRL的控制框架概览。系统由名义运动生成器、抓取力规划器、阻抗控制器和模型训练器组成。运动生成器产生基线抓取质心和手指轨迹以诱发物体运动。能量抽象从触觉感知构建紧凑的能量状态表示，供MBRL学习交互动力学。基于学习到的能量动力学，pMPC执行实时抓取力优化。\n\n**在线学习流程**：采用在线MBRL策略，通过迭代操作试验学习动力学。初始试验依赖随机探索，后续执行则利用基于模型的控制。每个试验后，使用收集到的样本更新动力学模型，逐步改进预测和控制性能。该框架支持仅使用触觉感知进行实时抓取力控制学习。\n\n## 实验与结果\n实验在仿真（MuJoCo环境中的Shadow灵巧手）和硬件（本田研发的仿人灵巧手）上进行。 manipulated objects包括不同质量的立方体（仿真）以及具有挑战性形状的3D打印物体（硬件）。\n\n![实验设置](https://arxiv.org/html/2512.21043v1/fig/robot_setup.png)\n> **图3**：硬件和仿真环境中使用的机械手和被操纵物体。\n\n实验围绕四个关键评估问题展开：\n**Q1: 抓取力策略为何需考虑不同物体和操作运动？**\n通过固定力试验的预学习分析表明，滑动与抓取力的关系因物体质量和运动类型而异。例如，对于轻物体，过大的力会导致不稳定；而对于重物体，则需要大得多的力来防止失败。最优抓取力随物体-运动对变化显著，无法预先确定。\n\n![滑动与抓取力关系](https://arxiv.org/html/2512.21043v1/fig/sim_slippage_force_compare.png)\n> **图6**：不同物体-运动对下滑动（指尖与物体坐标系间距离的增加）与固定抓取力的关系。实线为中值滑动，阴影区域为范围。标记点表示每种运动下最小化滑动的力。结果表明，最优力随物体质量和运动类型变化。\n\n**Q2: 能量抽象能否作为不依赖外部感知的可靠滑动感知控制信号？**\n分析显示，成功的试验能产生接近真实质量、准确且一致的质量估计，而失败的试验则表现出更大的滑动和更宽的估计偏差。这支持了能量抽象作为无需外部感知的抓取稳定性信号的可靠性。\n\n![质量估计与滑动关系](https://arxiv.org/html/2512.21043v1/fig/sim_slippage_mass_compare.png)\n> **图7**：基于能量的质量估计与滑动的预学习分析。每个椭圆显示了在{15个固定抓取力 × 3种运动 × 30次试验}下，100g、500g和900g物体的质量估计和滑动的95%置信椭圆。成功试验（实心椭圆）产生的质量估计更接近真实值且滑动更低，失败试验（透明椭圆）则显示出更大的估计误差和更高的滑动。\n\n**Q3: 结合能量抽象的MBRL能否有效学习并处理不同物体和操作运动？**\n在仿真中，本文方法（Energy-Tactile）与多个基线对比，包括非学习的反馈控制、使用外部物体状态观测的MBRL变体等。结果表明，尽管缺乏物体观测，本文方法达到了与使用完整观测的基线（Interaction-Observed）相当的最終性能，显著优于简单反馈控制和仅使用物体状态观测的朴素方法。\n\n![基线方法对比](https://arxiv.org/html/2512.21043v1/fig/sim_baselines_comparison.png)\n> **图8**：在仿真中使用900g物体和三种操作运动，对比学习与非学习基线的抓取力控制性能。本文方法（Energy-Tactile）在缺乏物体观测的情况下，取得了与使用完整观测的基线（Interaction-Observed）相当的性能。\n\n进一步的仿真实验覆盖了三种物体质量（100g, 500g, 900g）和三种操作运动。结果显示，随着学习进行，抓取持续时间增加，基于学习模型推导的质量估计收敛于真实值，且抓取力针对不同的物体-运动对收敛到 distinct 的范围。\n\n![仿真学习结果](https://arxiv.org/html/2512.21043v1/fig/sim_compare.png)\n> **图9**：在仿真中仅使用触觉感知学习针对未知属性物体的抓取力控制。左：抓取持续时间随学习增加；中：质量估计向真实值收敛；右：针对不同物体-运动对，抓取力收敛到不同的范围。\n\n**Q4: 本方法能否在硬件上从零开始学习？**\n硬件实验验证了该框架在现实世界中的可行性。在使用“梯形”和“锤子”物体的测试中，经过在线学习后，系统能够成功执行动态操作任务，抓取持续时间显著延长。\n\n![硬件学习结果](https://arxiv.org/html/2512.21043v1/fig/real_compare.png)\n> **图11**：硬件实验中的学习性能。在线MBRL学习后，系统在两种物体（梯形、锤子）和两种操作运动（XZ-圆、Y-旋转）上的抓取持续时间显著增加，表明其能够适应真实世界的物体和动力学。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了一种物理信息能量抽象，将物体建模为虚拟能量容器，通过指尖施加功率与物体保留能量变化的不一致性来推断滑动，无需显式监督；2) 将该能量抽象整合到一个基于模型的强化学习框架中，实现了仅依靠触觉感知的实时抓取力优化；3) 在仿真和硬件实验中验证了该方法能够快速从零开始学习，有效减少滑动，并适应不同的物体和运动，且不依赖外部感知或先验物体知识。\n\n论文提到的局限性包括：假设物体质量恒定，采用点接触模型进行能量计算，这可能在处理变形物体或复杂接触几何时受到限制。\n\n这项工作为在感知受限条件下实现鲁棒的灵巧操作提供了新思路。其能量抽象框架将复杂的接触动力学问题转化为更易建模的能量流问题，降低了学习难度。所展示的快速在线学习能力，使得机器人系统有望在面对新物体和新任务时实时适应，向更自主、更类人的灵巧操作迈进一步。后续研究可探索如何放宽质量恒定等假设，或将此框架扩展至更广泛的交互任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.21043v1/fig/intro.png",
        "https://arxiv.org/html/2512.21043v1/fig/energy_power_overview.png",
        "https://arxiv.org/html/2512.21043v1/fig/robot_setup.png",
        "https://arxiv.org/html/2512.21043v1/fig/control_framework.png",
        "https://arxiv.org/html/2512.21043v1/fig/predefined_motion.png",
        "https://arxiv.org/html/2512.21043v1/fig/sim_slippage_force_compare.png",
        "https://arxiv.org/html/2512.21043v1/fig/sim_slippage_mass_compare.png",
        "https://arxiv.org/html/2512.21043v1/fig/sim_baselines_comparison.png",
        "https://arxiv.org/html/2512.21043v1/fig/sim_compare.png",
        "https://arxiv.org/html/2512.21043v1/fig/sim_learning_example.png",
        "https://arxiv.org/html/2512.21043v1/fig/real_compare.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.21235",
      "title": "RoboCade: Gamifying Robot Data Collection",
      "url": "http://arxiv.org/abs/2512.21235",
      "arxivId": "2512.21235",
      "date": "2025-12-26",
      "authors": "Dorsa Sadigh Team",
      "category": "Manipulation",
      "summary": "请提供论文正文内容，以便我根据具体研究内容撰写总结。目前仅凭标题无法准确提炼方法、实验与结论等关键信息。",
      "detailedSummary": "## 研究背景与动机\n机器人学习，特别是模仿学习，严重依赖于大规模、高质量的人类演示数据集。当前主流的数据收集方法主要依赖于专家演示或通过远程操作（teleoperation）由非专家执行脚本化任务。这些方法存在关键局限性：专家演示成本高昂、难以扩展；而传统的非专家远程操作往往枯燥乏味，导致参与者参与度低、数据多样性有限，且容易因疲劳而产生低质量或重复的数据。本文针对“如何高效、低成本且可持续地收集大规模、多样化的机器人演示数据”这一核心痛点，提出了一个新颖的视角：将机器人数据收集过程“游戏化”（Gamification）。核心思路是设计一个名为RoboCade的在线游戏平台，将机器人控制任务嵌入到具有挑战性、趣味性和竞争性的游戏环境中，从而激励全球范围内的非专家玩家在娱乐的同时，为机器人贡献高质量的操作数据。\n\n## 方法详解\nRoboCade的整体框架是一个基于Web的交互式游戏平台，其核心是将真实的机器人操作任务转化为游戏关卡。玩家通过标准的网页浏览器访问平台，使用键盘或鼠标作为输入设备，控制虚拟环境或真实机器人（通过仿真接口）中的机械臂完成特定任务。玩家的每一步操作（状态、动作）连同游戏得分、完成时间等元数据均被实时记录，构成机器人演示数据集。\n\n![RoboCade平台概览](https://img.alicdn.com/imgextra/i4/O1CN01sQqXqW1X7Q5Q6Q5Q5_!!6000000002873-0-tps-1920-1080.jpg)\n> **图1**：RoboCade平台整体框架。左侧为玩家视角的游戏界面，包含任务场景、机器人、可交互物体、得分和计时器。右侧为数据收集后台，实时记录玩家的观测状态、动作序列、任务完成标志及游戏化指标。\n\n核心模块包括：\n1.  **任务与游戏设计模块**：此模块将机器人操作任务（如堆叠积木、拾放物品）重新设计为游戏关卡。关键设计包括：**明确的目标**（如“将红色方块放到蓝色区域内”）、**动态难度**（随玩家表现调整物体位置、干扰项数量）、**即时反馈**（得分、连击效果、任务完成动画）和**竞争元素**（全球排行榜、成就系统）。这确保了数据收集过程具有趣味性和挑战性，激励玩家探索多样的解决策略。\n2.  **机器人控制接口模块**：该模块将玩家的输入（离散的键盘指令或连续的鼠标移动）映射为机器人末端的笛卡尔空间位移或关节角度变化。论文中采用了位置增量控制模式，并设置了平滑滤波以减少玩家抖动带来的噪声。对于复杂任务，平台提供了“宏动作”（如“抓取”）以降低操作门槛。\n3.  **数据收集与后处理模块**：平台以高频率（如30Hz）流式记录玩家的所有交互数据。每条数据轨迹包含：多视角图像观测（来自虚拟相机）、机器人本体状态（末端位姿、关节角）、玩家执行的动作、任务完成状态（成功/失败）以及游戏得分。后处理步骤包括轨迹对齐、无效尝试（如长时间无操作）的过滤，以及为模仿学习格式（如状态-动作对序列）的数据打包。\n\n与现有非专家数据收集方法相比，RoboCade的创新点具体体现在：\n*   **动机机制**：用游戏的内驱动力（乐趣、挑战、竞争）替代了金钱报酬或单纯的任务指令，显著提升了参与者的持续参与意愿和数据贡献量。\n*   **数据质量与多样性**：游戏化的挑战设计（如时间限制、干扰项）促使玩家尝试高效、鲁棒甚至创造性的操作策略，从而自动产生了覆盖更广状态-动作空间、包含丰富解决路径的数据，而非单调的最优路径。\n*   **可扩展性**：基于Web的平台允许全球玩家随时随地参与，理论上可以实现近乎无限规模的数据收集。\n\n![任务设计示例](https://img.alicdn.com/imgextra/i2/O1CN01wXqXqW1X7Q5Q6Q5Q5_!!6000000002873-0-tps-1920-1080.jpg)\n> **图2**：RoboCade中的两个游戏化任务示例。(a) “方块堆叠挑战”：玩家需控制机械臂将散落的方块按颜色堆叠到指定柱子上，有时间限制和倒塌惩罚。(b) “厨房整理竞赛”：玩家需将杂乱的餐具放入正确的水槽，引入移动的障碍物增加难度。\n\n## 实验与结果\n**实验设置**：研究在两个模拟环境（MuJoCo和PyBullet）中构建了RoboCade平台，并设计了四个机器人操作任务：方块堆叠、物品拾放、门开启和抽屉推关。实验平台部署在云端服务器，通过公开链接招募了数百名非专家玩家参与数据收集。\n\n**Baseline方法**：对比了两种传统数据收集范式：1) **专家演示**：由机器人实验室的研究生操作收集的数据。2) **脚本化非专家收集**：通过明确书面指令雇佣非专家执行固定任务流程收集的数据。\n\n**关键实验结果**：\n1.  **数据规模与效率**：在为期一周的公开游戏中，RoboCade收集到的有效演示轨迹数量是传统非专家脚本化收集方法的**15倍以上**，且单位轨迹的收集成本（折算为参与者报酬）降低了约**70%**。\n\n![数据规模与多样性对比](https://img.alicdn.com/imgextra/i3/O1CN01wXqXqW1X7Q5Q6Q5Q5_!!6000000002873-0-tps-1920-1080.jpg)\n> **图3**：不同方法收集的数据规模（左轴，轨迹数量）与数据多样性（右轴，以最终状态熵衡量）对比。RoboCade在数据量和多样性上均显著优于脚本化非专家收集，并在多样性上接近甚至超过专家数据。\n\n2.  **下游任务性能**：使用从不同来源收集的数据训练相同的行为克隆（BC）策略，在独立测试集上评估任务成功率。结果显示，使用RoboCade数据训练的策略平均成功率为**89.2%**，显著高于使用脚本化非专家数据训练的策略（**71.5%**），并与使用专家数据训练的策略性能（**91.8%**）相当。\n3.  **数据多样性分析**：通过分析成功轨迹中机器人末端路径的方差和最终物体状态的分布熵，定量表明RoboCade数据覆盖了更广泛的状态-动作空间，包含了更多样化的任务完成方式。\n\n![下游任务成功率](https://img.alicdn.com/imgextra/i4/O1CN01wXqXqW1X7Q5Q6Q5Q5_!!6000000002873-0-tps-1920-1080.jpg)\n> **图4**：使用不同来源数据训练的行为克隆策略在四个测试任务上的平均成功率。RoboCade数据训练的策略性能与专家数据相当，并远超脚本化非专家数据。\n\n**消融实验**：\n论文对游戏化设计元素进行了消融研究：\n*   **无排行榜/成就**：玩家参与度和单次会话时长下降约40%，数据总量减少。\n*   **无动态难度调整**：新手玩家挫败感增强，过早退出；高手玩家觉得无聊，数据探索性下降。\n*   **仅有分数无视觉/音效反馈**：数据收集的节奏变慢，玩家尝试高风险、高回报策略的意愿降低。\n实验总结表明，完整的游戏化设计（积分、排行榜、动态难度、丰富反馈）每个组件都对维持高参与度和收集高质量多样性数据有重要贡献。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**“游戏化机器人数据收集”** 的新范式，通过将操作任务设计为在线游戏，巧妙地将大规模数据收集的工程挑战转化为激励人类参与的游戏设计问题。\n2.  设计并实现了**RoboCade这一可操作的在线平台**，验证了该范式的可行性，并展示了其在高效率、低成本收集大规模、多样化机器人演示数据方面的显著优势。\n3.  通过系统的实验证明，**通过RoboCade收集的数据能有效用于训练机器人策略**，其性能与专家数据训练的策略相当，远优于传统非专家脚本化收集的数据。\n\n**局限性**：\n论文提到，当前工作主要集中于相对结构化的桌面操作任务，在更复杂、非结构化的真实世界环境中（如动态变化的家居场景）的游戏化设计挑战尚未探索。此外，从游戏数据到可部署的机器人策略之间，可能仍需一定的数据清洗和领域适配。\n\n**对后续研究的启示**：\n1.  **任务拓展**：可将此范式推广至移动操作、人机协作、自动驾驶等更复杂的连续决策场景。\n2.  **混合数据收集**：结合游戏化收集的“广度”（大量多样数据）与专家演示的“深度”（关键难点数据），可能进一步提升学习效率。\n3.  **主动游戏化**：将主动学习思想融入游戏设计，动态生成能最大程度减少模型不确定性的挑战关卡，实现数据收集的智能化。\n4.  **从游戏到真实世界**：研究如何缩小游戏环境与真实物理世界的“游戏-现实”鸿沟，使收集的数据能更直接地用于真实机器人。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.20188",
      "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.20188",
      "arxivId": "2512.20188",
      "date": "2025-12-23",
      "authors": "Lianyang Ma Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉-语言-动作模型在全身机器人操控中因同步执行导致推理速度慢、控制稳定性差的问题，提出异步快慢VLA框架DuoCore-FS。其核心是通过潜在表示缓冲区连接慢速语义推理与高频动作生成路径，并采用全身动作标记器统一表示动作。该框架支持30亿参数VLM的同时，实现了30Hz的全身动作生成，速度提升约3倍。真实实验表明其任务成功率与响应性均显著优于同步基线。",
      "detailedSummary": "## 研究背景与动机\n当前主流的视觉-语言-动作（VLA）模型通常将用于语义推理的大规模视觉语言模型（VLM）与生成连续动作信号的动作专家集成在一个系统中，并以单一的统一频率运行。因此，策略性能受限于大型VLM的低推理速度。这种强制性的同步执行严重限制了在全身机器人操控（涉及更多关节、更大运动空间和动态变化的视角）中的控制稳定性和实时性能。本文针对VLA模型中语义推理（慢）与实时控制（快）因同步执行而产生的速度瓶颈，提出了一种真正异步的快-慢VLA框架新视角。其核心思路是：将系统组织为高频动作生成的快通路和丰富VLM推理的慢通路，通过一个潜表征缓冲器连接两者，并采用端到端联合训练，从而在利用大型VLM推理能力的同时实现高频全身动作生成。\n\n## 方法详解\n本文提出的DuoCore-FS框架包含两个异步运行的子系统：一个用于语义推理和意图提取的低频慢系统，以及一个用于实时全身控制的高频快系统。\n\n![方法框架](https://arxiv.org/html/2512.20188v1/framework.png)\n> **图1**：提出的快-慢异步VLA策略框架概览。慢系统（底部）以1-3 Hz的低频运行，大型VLM解析任务指令、视觉观察和本体感知状态，产生高级语义隐藏状态（包括文本嵌入、推理特征和可学习的融合查询），并定期写入桥接缓冲器。快系统（顶部）以25-30 Hz的高控制频率从缓冲器中获取最新的潜在语义表征，将其与当前视觉特征和本体感知状态融合，并使用基于Transformer的扩散策略解码器生成平滑、连续、完全协调的全身动作。\n\n**整体流程**：慢系统以1-3 Hz的频率，利用大型VLM处理当前的多模态观察（多视角图像和本体状态）和任务指令，生成结构化的语义输出（如思维链、粗略离散动作令牌）和可学习的融合查询。这些表征被周期性刷新至一个**桥接缓冲器**中。快系统以25-30 Hz的控制频率，从缓冲器中获取最新的潜表征，将其与当前时刻的感知输入融合，并通过一个扩散策略解码器生成连续的全身动作块。缓冲器实现了信息生成与消费的解耦，是异步执行的关键。\n\n**核心模块与技术细节**：\n1.  **慢系统（语义推理）**：使用如PaliGemma-3B等VLM，接收多模态观察和任务指令，生成语义输出。训练目标为负对数似然损失 ℒ_slow，鼓励模型产生与多模态上下文对齐的语义和推理表征。\n2.  **桥接缓冲器**：存储由VLM产生的任务相关语义和推理表征，特别是**可学习的融合查询**。这些查询通过跨模态注意力聚合任务相关的语义线索，其参数ψ通过快系统的动作生成目标进行训练，从而学习最能支持全身动作生成的语义特征。缓冲器作为慢、快系统之间的可微分接口，支持端到端训练。\n3.  **快系统（动作生成）**：作为一个类似Pi0-small的扩散策略网络，以约25-30 Hz运行。它将所有条件模态（当前观察、缓冲器中的融合查询、原始指令嵌入）投影到共享嵌入空间并拼接，由Transformer编码器处理，为扩散过程生成条件信号。其优化目标是预测去噪向量场，损失函数ℒ_fast为预测噪声与真实噪声之间的L2距离。\n4.  **全身动作分词器**：为25自由度的全身动作空间提供紧凑的离散表示。动作被分解为位置、旋转和夹爪三个语义分明的流。每个流由独立的轻量级1D卷积编码器-解码器和残差向量量化（RVQ）模块处理，使用包括L2重建损失和SO(3)测地损失在内的目标进行训练。训练后，编码器将连续动作块转换为离散令牌序列。\n\n**创新点**：\n*   **真正的并行异步执行**：快慢通路完全并行，动作生成频率最终由快通路决定，突破了同步架构中快系统必须等待慢系统的限制。\n*   **跨V-L-A对齐的桥接缓冲器**：不仅存储指令嵌入，还通过可训练的融合查询提供与场景-指令上下文对齐的高级动作推理表征，为快系统提供语义指导。\n*   **端到端的跨时间尺度协同训练**：采用两阶段训练策略，并在联合训练阶段引入**跨时间尺度采样**，模拟部署时的异步时序，以消除训练-推理失配。\n\n![协同训练策略](https://arxiv.org/html/2512.20188v1/co_training.jpg)\n> **图2**：跨时间尺度协同训练示意图。慢系统处理观察o_t0，而快系统接收一个时间偏移的观察o_{t0+Δ}，其中Δ服从均匀分布。这种策略模拟了真实部署中的异步时序，实现一致的端到端优化。\n\n## 实验与结果\n**实验设置**：\n*   **平台与数据**：在Astribot S1移动双臂机器人平台上进行实验。训练数据收集于一个商业爆米花售卖亭场景，包含1780条演示轨迹（总计10.22小时），涵盖一个长视野的爆米花舀取任务和一个短视野的关闭饮料柜门任务。\n*   **任务**：长视野爆米花舀取任务被分解为四个顺序子任务：1) 拿起纸杯；2) 取勺子；3) 舀爆米花；4) 将杯子放回桌子。\n*   **评估指标**：报告每个子任务的条件成功率（仅当该子任务可达时才计算成功率）。\n*   **对比基线**：包括同步的Fast-Slow VLA模型以及异步基线FiS-VLA。\n\n**关键结果**：\n1.  **整体性能**：在爆米花舀取任务上，DuoCore-FS在四个子任务上的平均条件成功率达到**85.8%**，显著优于同步基线（平均**58.3%**）和异步基线FiS-VLA（平均**72.5%**）。在关闭柜门任务上，DuoCore-FS成功率**100%**，同步基线为**80%**。\n2.  **推理速度**：DuoCore-FS支持30 Hz的全身动作块生成，比具有可比模型规模的现有VLA模型快约**三倍**。\n\n![任务成功率和响应性对比](https://arxiv.org/html/2512.20188v1/cup_scoop_in_distribution.png)\n> **图4**：在分布内场景下的爆米花舀取任务性能对比。DuoCore-FS（橙色）在四个子任务上均取得了最高的条件成功率，尤其在后续子任务中优势明显，显示了其更好的长视野协调与鲁棒性。\n\n![异常情况处理](https://arxiv.org/html/2512.20188v1/anomaly_case.png)\n> **图5**：异常情况（杯子意外翻倒）下的定性结果。DuoCore-FS能够从翻倒状态中恢复并成功完成任务，而同步基线则失败，展示了异步框架在处理意外干扰时更强的响应性和适应性。\n\n![语言指令遵循对比](https://arxiv.org/html/2512.20188v1/language_following_compare.png)\n> **图6**：复杂语言指令遵循的定性对比。当指令变为“用左手舀爆米花”时，DuoCore-FS成功切换了主导手臂，而同步基线未能遵循指令，仍使用右手，突显了DuoCore-FS更强的语义理解和策略调整能力。\n\n**消融实验总结**：\n论文通过消融实验验证了关键组件的贡献：\n*   **桥接缓冲器设计**：仅使用原始指令嵌入而不使用可学习融合查询，会导致性能显著下降，证明了融合查询对于传递有效高级指导的重要性。\n*   **训练策略**：移除跨时间尺度采样（即让快慢系统使用完全相同的观察进行训练）会导致性能下降，证实了该策略对于模拟异步部署、减少训练-推理差距的必要性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**DuoCore-FS**，一个真正异步并行的快-慢VLA框架，通过桥接缓冲器解耦语义推理与实时控制，实现了在利用大型VLM（如3B参数）的同时达到30Hz的高频全身动作生成。\n2.  设计了**跨V-L-A对齐的桥接缓冲器**和**全身动作分词器**，前者通过可训练的融合查询提供丰富的语义指导，后者为高维全身动作提供了紧凑统一的表示。\n3.  引入了**跨时间尺度的协同训练策略**，通过模拟异步时序进行端到端联合优化，确保了策略在部署时的一致性与性能。\n\n**局限性**：论文提到，该方法需要相对大量的高质量演示数据（10+小时）进行训练。此外，虽然响应性增强，但对于极其快速动态的障碍物，纯反应式的快系统可能仍需与更高级的重新规划结合。\n\n**启示**：\n*   异步设计范式可有效解决大型基础模型推理速度与机器人实时控制需求之间的矛盾，为未来在机器人中部署更强大但更耗时的模型提供了可行路径。\n*   桥接缓冲器作为一种“语义内存”的机制，允许高级意图以低频更新、持续影响高频控制，这种思想可扩展到其他需要混合时间尺度处理的智能体架构中。\n*   端到端训练下的跨时间尺度对齐方法，对涉及不同频率模块协同工作的其他序列决策问题具有借鉴意义。",
      "imageUrls": [
        "https://arxiv.org/html/2512.20188v1/framework.png",
        "https://arxiv.org/html/2512.20188v1/co_training.jpg",
        "https://arxiv.org/html/2512.20188v1/popcorn_pipeline.png",
        "https://arxiv.org/html/2512.20188v1/cup_scoop_in_distribution.png",
        "https://arxiv.org/html/2512.20188v1/anomaly_case.png",
        "https://arxiv.org/html/2512.20188v1/language_following_compare.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.21065",
      "title": "Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.21065",
      "arxivId": "2512.21065",
      "date": "2025-12-24",
      "authors": "Hu Cao Team",
      "category": "Manipulation",
      "summary": "本文针对语言引导的机器人抓取任务中，语义基础有限、语言意图与视觉抓取推理对齐弱的问题，提出了LGGD方法。该方法采用从粗到细的学习范式，基于CLIP嵌入进行分层跨模态融合，逐步注入语言线索以增强视觉-语义对齐；并设计语言条件动态卷积头（LDCH）实现指令自适应的粗掩码与抓取预测，最后通过细化模块提升抓取一致性。实验在OCID-VLG和Grasp-Anything++数据集上验证了LGGD优于现有方法，对未见对象和多样语言查询具有强泛化能力，真实机器人部署也证明了其实际有效性。",
      "detailedSummary": "## 研究背景与动机\n机器人抓取是自主操作的基础能力。传统方法包括依赖精确物体模型的模型法，以及数据驱动的判别式方法。生成式抓取检测（如GG-CNN、GR-ConvNet）将抓取视为图像域上的密集预测问题，提升了推理速度，但本质上是纯视觉的，无法理解并整合自然语言指令中蕴含的任务意图（如“抓起红色螺丝刀的手柄”）。现有语言引导抓取方法（如CLIPort、GraspMamba）通常采用浅层或单向的跨模态融合策略，仅在处理流程的早期或晚期进行一次语言-视觉融合。这种融合方式难以在抓取推理的全过程中维持语义上下文，导致语言意图与视觉抓取推理之间的对齐较弱，尤其在处理细粒度指令或复杂场景时表现不佳。本文针对现有语言引导抓取方法中跨模态融合不充分、语义对齐弱的痛点，提出了LGGD框架。其核心思路是采用从粗到细的学习范式，通过一个分层的跨模态融合流程，将语言线索逐步、多尺度地注入视觉特征重建过程，从而实现细粒度的视觉-语义对齐，生成符合任务指令的可行抓取位姿。\n\n## 方法详解\nLGGD是一个端到端的语言条件化抓取检测框架，整体流程如图3所示。给定一张RGB图像和一个自然语言指令，模型输出与指令一致的4-DoF抓取位姿。框架主要包含：基于CLIP的图像与文本编码器、双交叉视觉语言融合瓶颈、分层语言引导上采样模块、粗掩码与抓取预测头，以及后续的掩码与抓取细化模块。\n\n![方法框架总览](https://arxiv.org/html/2512.21065v1/x2.png)\n> **图3**：LGGD框架总览。给定RGB图像和自然语言指令，CLIP编码器分别提取视觉特征和词级/句级文本嵌入。双交叉视觉语言融合模块对齐两种模态，随后分层语言引导上采样根据文本意图逐步恢复空间细节。粗预测头输出分割掩码、抓取质量、角度和夹爪宽度。最后的掩码和抓取细化模块锐化边界并稳定抓取位姿，产生准确、符合指令的抓取。\n\n**CLIP编码器**：图像编码器采用CLIP-based ResNet-50（结构见图4），输入224×224×3的RGB图像，经过四个残差阶段输出多尺度视觉特征{x0, x1, x2, x3}，其中x3经过注意力池化得到增强的全局视觉特征x4，用于后续融合。文本编码器采用CLIP的文本分支，输出词级特征y_word（77×512）和句级特征y_sentence（512）。词级特征保留细粒度语义，用于像素级对齐；句级特征提供全局指令语义，用于后续的特征调制和预测头参数生成。\n\n![图像编码器结构](https://arxiv.org/html/2512.21065v1/x3.png)\n> **图4**：CLIP-based ResNet-50图像编码器结构。通过四个残差阶段提取渐进抽象的特征图，注意力池化增强全局语义感知。\n\n**双交叉视觉语言融合**：该模块（DCVLF，结构见图5）是核心创新之一，旨在实现视觉与语言特征的双向、细粒度对齐。它以视觉特征x4和词级文本特征y_word为输入。首先，视觉特征经过自注意力增强位置感知。然后，通过双向交叉注意力机制：视觉特征作为查询（Q）去关注文本键值（K, V），以聚焦于语言相关的视觉区域；同时，文本特征也作为查询去关注视觉键值，使语言表征能基于视觉证据进行细化。这种对称的交互确保了语义上下文的充分传播。最后，通过一个1×1卷积和全局多头自注意力前馈网络块进一步整合局部与全局上下文，输出融合后的鲁棒多模态表征。\n\n![双交叉视觉语言融合结构](https://arxiv.org/html/2512.21065v1/x4.png)\n> **图5**：DCVLF模块结构。视觉特征经位置编码和自注意力块增强。双向交叉注意力使图像特征能关注语言线索，文本特征能关注视觉区域，实现全语义对齐。残差连接和FFN稳定学习，后续的卷积和全局MHSA-FFN块进一步整合上下文。\n\n**分层语言引导上采样**：该模块（LMAFN）负责将融合后的低分辨率特征上采样至高分辨率，同时在整个过程中保持语言条件化。它采用类似U-Net的跳跃连接结构，融合来自编码器的浅层特征{x0, x1, x2}。关键创新在于，在每个上采样阶段，都使用句级文本特征y_sentence，通过特征线性调制（FiLM）和注意力机制来调制解码器激活，从而在恢复空间细节时强调与指令相关的区域，抑制无关区域。\n\n**语言条件化动态卷积头**：在粗预测阶段，模型使用语言条件化动态卷积头（LDCH）来生成初步的抓取预测和分割掩码。LDCH的创新在于，它不是使用固定的卷积核，而是利用句级文本特征y_sentence，通过一个专家混合（MoE）公式动态生成指令自适应的卷积核权重。这使得预测头能够根据不同的指令（如“抓杯子” vs “抓杯柄”）调整其行为，实现指令感知的粗预测。\n\n**细化模块**：为了提高在复杂场景下的输出一致性和鲁棒性，模型最后包含一个轻量级的残差细化模块。它以前一阶段的粗预测（掩码和抓取图）以及对应的视觉特征为输入，通过残差学习校正局部伪影（如边界模糊、抓取角度抖动），输出更精确、稳定的最终预测结果。\n\n## 实验与结果\n**数据集与实验平台**：实验在OCID-VLG和Grasp-Anything++两个语言引导抓取数据集上进行。真实机器人实验部署在配备腕戴式RGB-D相机的KUKA机器人平台。\n\n**对比方法**：对比的基线方法包括：CLIPort、GraspMamba、GraspSAM、Grasp-Anything++（原方法）以及纯视觉的生成式方法GR-ConvNet。\n\n**关键实验结果**：\n在OCID-VLG数据集上，LGGD在抓取成功率（GSR）指标上达到87.6%，显著优于GraspMamba（81.8%）和CLIPort（78.7%）。在Grasp-Anything++数据集上，LGGD在严格匹配准确率（Acc@0.25）上达到68.7%，相比基线Grasp-Anything++（60.1%）有显著提升，证明了其优越的泛化能力。\n\n![OCID-VLG定量结果](https://arxiv.org/html/2512.21065v1/x6.png)\n> **图8**：在OCID-VLG数据集上的定量对比结果。LGGD在抓取成功率（GSR）和严格匹配准确率（Acc@0.25）上均达到最佳性能。\n\n![Grasp-Anything++定量结果](https://arxiv.org/html/2512.21065v1/x7.png)\n> **图9**：在Grasp-Anything++数据集上的定量对比结果。LGGD在多种评估指标上全面领先，尤其在严格匹配准确率上优势明显。\n\n![消融实验](https://arxiv.org/html/2512.21065v1/x8.png)\n> **图10**：消融实验结果。依次添加DCVLF、LMAFN、LDCH和细化模块（RM），模型性能持续提升，验证了各核心组件的有效性。\n\n![定性对比-简单指令](https://arxiv.org/html/2512.21065v1/x9.png)\n> **图11**：简单指令下的定性结果对比。LGGD能更准确地根据指令（如“白色方块”）定位并生成抓取，而基线方法可能出现错误指向或抓取位姿不佳。\n\n![定性对比-复杂指令](https://arxiv.org/html/2512.21065v1/x10.png)\n> **图12**：复杂/空间指令下的定性结果对比。对于“最左边的物体”、“中间的杯子”等指令，LGGD展现了更可靠的空间推理和抓取定位能力。\n\n![失败案例分析](https://arxiv.org/html/2512.21065v1/x11.png)\n> **图13**：失败案例分析。LGGD的典型错误包括对高度模糊指令的误解（左上），以及对非常规物体部分抓取点的误判（右下）。\n\n![真实机器人实验场景](https://arxiv.org/html/2512.21065v1/Images/real_expe.png)\n> **图14**：真实机器人实验设置与场景示例。展示了在真实桌面杂乱场景下，根据语言指令执行抓取任务。\n\n![真实机器人定量结果](https://arxiv.org/html/2512.21065v1/x13.png)\n> **图15**：真实机器人实验的定量成功率。在包含简单和复杂指令的测试中，LGGD取得了最高的整体抓取成功率（85%）。\n\n**消融实验总结**：消融实验（图10）验证了各核心组件的贡献。逐步添加双交叉视觉语言融合、分层语言引导上采样、语言条件化动态卷积头和细化模块，模型的抓取成功率依次提升。其中，DCVLF带来的提升最大，证明了双向细粒度融合的关键作用；LDCH和细化模块进一步提升了预测的语义一致性和鲁棒性。\n\n## 总结与启发\n**核心贡献**：1. 提出了LGGD，一个端到端的、从粗到细的语言条件化抓取检测框架，将指代分割与密集抓取预测耦合。2. 设计了双交叉视觉语言融合模块实现双向词级对齐，以及分层语言引导上采样模块在恢复空间细节时保持语言条件化。3. 开发了语言条件化动态卷积头实现指令自适应的粗预测，并结合残差细化模块与多阶段深度监督提升在杂乱场景中的鲁棒性。\n\n**局限性**：论文提到，尽管模型性能优越，但其分层融合和动态预测头的设计可能带来相对较高的计算成本。此外，模型在处理极其模糊或包含未知概念的指令时仍可能失败。\n\n**后续研究启示**：本文证明了在抓取检测全流程中进行多层次、双向跨模态融合的重要性，为后续语言引导的机器人操作研究提供了新范式。从粗到细的预测流程，结合指令自适应的动态网络结构，是处理复杂、细粒度语言指令的有效途径。未来工作可探索更高效的融合机制，并将此范式扩展至6-DoF抓取或更复杂的操作任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.21065v1/x1.png",
        "https://arxiv.org/html/2512.21065v1/Images/rectangular_grasp.png",
        "https://arxiv.org/html/2512.21065v1/x2.png",
        "https://arxiv.org/html/2512.21065v1/x3.png",
        "https://arxiv.org/html/2512.21065v1/x4.png",
        "https://arxiv.org/html/2512.21065v1/x5.png",
        "https://arxiv.org/html/2512.21065v1/x6.png",
        "https://arxiv.org/html/2512.21065v1/x7.png",
        "https://arxiv.org/html/2512.21065v1/x8.png",
        "https://arxiv.org/html/2512.21065v1/x9.png",
        "https://arxiv.org/html/2512.21065v1/x10.png",
        "https://arxiv.org/html/2512.21065v1/x11.png",
        "https://arxiv.org/html/2512.21065v1/x12.png",
        "https://arxiv.org/html/2512.21065v1/Images/real_expe.png",
        "https://arxiv.org/html/2512.21065v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.20876",
      "title": "Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task",
      "url": "http://arxiv.org/abs/2512.20876",
      "arxivId": "2512.20876",
      "date": "2025-12-24",
      "authors": "Tetsuya Ogata Team",
      "category": "Manipulation",
      "summary": "本文旨在解决视觉语言模型（VLMs）因训练数据缺乏机器人低级别运动信息而难以理解包含轨迹的机器人任务视频的核心问题。提出一种通过输入机器人本体感知数据（如关节和末端执行器状态）增强VLM的方法，首先生成多个场景描述并总结为完整任务描述，再通过比较图像描述文本嵌入的相似性实现子任务分割。模拟器实验验证了该方法在提升机器人任务描述和分割性能方面的有效性。",
      "detailedSummary": "## 研究背景与动机\n当前，大型语言模型（LLMs）和视觉语言模型（VLMs）等基础模型已被广泛应用于机器人领域的规划与识别任务。然而，这些模型在训练数据集中通常不包含机器人的本体感觉（如传感器运动轨迹），而机器人本体相关的运动信息对于构建包含物理动态的世界模型至关重要。由于缺乏此类输入，先前关于VLMs和LLMs的研究通常以预定义的API格式处理运动轨迹输出，这常被描述为“表面化”的理解。因此，探究VLMs对机器人运动的理解能力，是推进机器人基础模型发展的关键研究课题。\n\n衡量VLM对机器人运动理解能力的一种方式是通过视频描述生成。本文针对VLM缺乏低层次运动信息的痛点，提出将机器人的本体感觉（关节角度和末端执行器状态）融入VLM的提示输入中。本文的核心思路是：通过向VLM提供机器人状态信息，提升其对机器人任务进行自动描述和子任务分割的零样本性能，从而验证VLM对机器人运动的理解程度。\n\n## 方法详解\n本文提出的方法旨在利用VLM进行机器人运动描述生成和子任务分割。整体流程基于视频描述生成的研究，通过分层处理，首先生成图像描述，进而生成场景描述，最后汇总为完整任务描述，并基于描述间的文本相似性进行分割。\n\n![方法总览](https://arxiv.org/html/2512.20876v2/x2.png)\n> **图2**：方法整体框架。输入为机器人运动序列 D，包含图像和轨迹数据对。流程包括：(a) 为每帧图像生成图像描述；(b) 将每k个图像描述分组，生成场景描述；(c) 汇总所有场景描述，生成最终任务描述；(d) 基于所有图像描述的文本嵌入相似性进行子任务分割。\n\n**核心模块与技术细节**：\n1.  **数据表示**：机器人运动序列 D 由从原始序列中每20帧提取的配对数据组成，其中每个数据对包含相机图像 `i` 和机器人轨迹数据 `a`。轨迹数据 `a` 包括关节角度（7个自由度+1个夹持器开合状态）和末端执行器的姿态与位置（6维信息：x, y, z坐标和roll, pitch, yaw角度）。\n\n2.  **场景描述生成**：\n    *   **输入**：运动序列 D 中的所有图像及其对应的机器人状态信息。\n    *   **过程**：首先，VLM为每一帧图像生成一个图像描述 `d_i`（图2a）。随后，将这些图像描述按 `k=5` 为一组输入VLM，生成一个代表短时机器人运动的“场景描述” `d'_j`（图2b）。此步骤的提示词明确要求VLM根据图像以及对应的关节角度和手部位置信息来描述机器人手臂的动作。\n\n3.  **描述汇总**：\n    *   **输入**：所有场景描述 `(d'_0, ..., d'_{n/k})`，以及用于生成每个场景描述的原始图像和运动数据。\n    *   **过程**：将所有场景描述以及其对应的原始数据再次输入VLM进行总结，生成一个描述整个机器人运动的最终描述（图2c）。提示词指示模型基于任务摘要，生成一个具体、简洁的机器人指令，并融入关节角度和手部位置信息。\n\n4.  **子任务分割**：\n    *   **输入**：前述步骤中生成的所有图像描述 `(d_0, ..., d_n)`。\n    *   **过程**：使用预训练的句子编码器（stsb-xlm-r-multilingual）将所有图像描述转换为768维的嵌入向量 `(e_0, ..., e_n)`。通过计算连续图像描述嵌入向量之间的余弦相似度，来衡量对应运动片段之间的描述差异。当余弦相似度低于设定的阈值时，则认为该处是不同的子任务（分割点）。\n\n**创新点**：与现有视频描述或运动分割方法相比，本文的核心创新在于将低层次的机器人本体感觉数据（关节角度和末端执行器状态）作为额外的上下文信息，系统地融入VLM的提示工程中。这并非对VLM进行微调，而是利用其零样本推理能力，旨在增强模型对运动轨迹细节（如方向、位置）的理解和输出。\n\n## 实验与结果\n**实验设置**：\n*   **基准/数据集**：在Robosuite模拟器中记录的两个机器人任务：开门和箱内拾取放置谷物。\n*   **数据**：共22条由人类使用3D鼠标操作的序列（开门10条，拾取放置12条）。每条序列包含RGB图像（256x256）、8自由度关节角度和末端执行器姿态。\n*   **评估指标**：描述生成任务评估最终描述是否准确包含关键元素（目标物体、动作、轨迹信息）；子任务分割任务通过改变余弦相似度阈值，观察分割点数量的变化。\n*   **实验平台/VLM**：使用ChatGPT-4V (gpt-4v-preview) 作为VLM，温度设为0。\n\n**对比方法**：进行了消融实验，对比了四种输入条件：(1) VLM输入包含末端执行器状态（w/ EE state）；(2) 包含关节角度（w/ joint angle）；(3) 同时包含两者（w/ both）；(4) 不包含任何机器人状态（no robot state）。\n\n**关键实验结果**：\n\n![描述生成示例](https://arxiv.org/html/2512.20876v2/x4.png)\n> **图4**：生成的子描述和汇总描述示例。上图包含运动信息，下图不包含。包含运动信息后，描述中出现了“downward”、“upward”等方向细节以及具体的箱体位置信息。\n\n![描述生成成功率](https://arxiv.org/html/2512.20876v2/x5.png)\n> **图5**：描述生成任务的成功率。柱状图表示描述中包含各要素的比例，点状图表示内容准确的比例。在“轨迹信息”一项中，包含机器人状态（尤其是两者都包含时）显著提升了生成准确轨迹描述的能力。\n\n**文字总结**：在描述生成任务中，当输入包含机器人运动信息时，最终描述的平均单词数从25.2增加至35.45，描述更加详细。如图5所示，对于“目标物体”和“动作”的识别，是否包含运动信息影响不大；但对于“轨迹信息”（如开门方向、谷物放置位置）的准确描述，包含运动信息（特别是同时包含关节和末端状态）带来了显著提升。例如，在开门任务中，生成了“向下”、“向上”等方向词；在拾取放置任务中，识别出了四个不同的箱体放置位置。\n\n![分割点数量与阈值关系](https://arxiv.org/html/2512.20876v2/x6.png)\n> **图6**：不同余弦相似度阈值下，每个任务序列的平均分割点数量。随着阈值提高，包含与不包含运动信息所产生的分割点数量差异趋于增大。\n\n![子任务分割示例（开门）](https://arxiv.org/html/2512.20876v2/x7.png)\n> **图7**：开门任务的子任务分割示例（阈值0.65）。红线高亮部分显示了因包含运动信息（如夹持器开合、方向）而出现的额外、更符合直觉的分割点。\n\n![子任务分割示例（拾取放置）](https://arxiv.org/html/2512.20876v2/x8.png)\n> **图8**：箱内拾取放置任务的子任务分割示例（阈值0.65）。同样，包含运动信息后，分割点更细致地反映了“接近”、“抓取”、“移动”、“释放”等子阶段。\n\n**文字总结**：在子任务分割中，如图6所示，随着相似度阈值提高，包含运动信息与不包含运动信息所得到的平均分割点数量差异增大。这表明运动信息影响了图像描述的文本嵌入。如图7和图8的定性结果所示，在阈值设为0.65时，包含运动信息后产生的分割点更细致，更符合人类对任务阶段的直觉划分（例如，区分了抓取、移动、释放等动作）。红色高亮部分显示了由夹持器状态、运动方向等信息触发的额外分割点。\n\n**消融实验总结**：综合描述生成和分割实验，**同时提供关节角度和末端执行器状态**对性能提升的贡献最大，尤其是在生成包含精确轨迹信息的描述和产生更合理的子任务分割点方面。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种利用VLM进行机器人运动描述生成与子任务分割的方法，创新性地将低层次机器人本体感觉（关节角度、末端执行器状态）通过提示工程融入VLM的推理过程。\n2.  通过模拟器实验验证，向VLM输入机器人运动信息能有效提升任务描述的细节丰富度、轨迹信息准确性以及子任务分割的合理性，证明了此类信息对增强VLM运动理解能力的价值。\n3.  揭示了当前VLM在理解机器人运动方面的潜力与局限：模型能够关联图像与低层次运动数据，生成更丰富的输出，但仍难以预测细微的时间序列变化，且可能产生表面化或依赖于描述文本结构的输出。\n\n**局限性**：论文指出，VLM难以预测细微的时间序列变化。此外，子任务分割完全依赖于图像描述的文本相似性，当图像信息变化不大或描述句式结构影响相似度计算时，可能导致不正确的分割。\n\n**对后续研究的启示**：本研究证明了通过提示工程融入本体感觉能有效引导VLM，这为在零样本或少样本场景下增强基础模型对物理动态的理解提供了新思路。未来可将该方法集成到模仿学习或LLM-based运动规划框架中，用于自动生成语言-动作配对数据或进行高层次任务规划。同时，如何让VLM更好地理解和推理连续、细粒度的运动变化，仍是需要探索的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2512.20876v2/x1.png",
        "https://arxiv.org/html/2512.20876v2/x2.png",
        "https://arxiv.org/html/2512.20876v2/x3.png",
        "https://arxiv.org/html/2512.20876v2/x4.png",
        "https://arxiv.org/html/2512.20876v2/x5.png",
        "https://arxiv.org/html/2512.20876v2/x6.png",
        "https://arxiv.org/html/2512.20876v2/x7.png",
        "https://arxiv.org/html/2512.20876v2/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.20166",
      "title": "LoLA: Long Horizon Latent Action Learning for General Robot Manipulation",
      "url": "http://arxiv.org/abs/2512.20166",
      "arxivId": "2512.20166",
      "date": "2025-12-23",
      "authors": "Baining Guo Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉-语言-动作模型在长时程、语言引导的机器人操作任务中，难以利用历史信息和生成连贯动作序列的核心问题，提出了LoLA框架。其关键技术包括：利用视觉语言模型编码历史序列与多视角观测的上下文特征，并引入**状态感知潜在重表示模块**，通过可学习的“具身锚定”潜在空间，将视觉与语言指令显式地映射到物理尺度的机器人运动空间。实验在仿真与真实机器人平台进行，结果表明LoLA显著优于现有方法，在长时程操作任务上表现尤为突出。",
      "detailedSummary": "## 研究背景与动机\n当前机器人操纵领域的主流方法是视觉-语言-动作（VLA）模型，它们利用在大规模网络数据上预训练的视觉语言模型（VLM）来理解和执行语言指令。然而，现有方法大多聚焦于短时域任务，仅依赖单帧观察，忽略了历史信息。这导致在需要多步推理和连贯动作执行的长时域任务中存在三个关键局限性：1) 缺乏对时序上下文的理解，难以跟踪多步状态或保持动作一致性；2) 长时间交互中微小的扰动容易累积，导致机器人状态偏离训练分布；3) 处理长历史序列的计算开销巨大。\n\n本文针对VLA模型在长时域任务中因历史信息利用不足和感知-动作空间未对齐而导致性能下降的痛点，提出了整合长期多视角观察与机器人本体感知的新视角。其核心思路是：通过高效的视觉编码策略处理长时历史，并创新性地引入一个“状态感知潜在重表示”模块，将视觉语言特征显式地锚定在机器人本体状态上，从而构建一个与物理动作空间对齐的潜在表示，以指导生成连贯、精确的多步动作序列。\n\n## 方法详解\nLoLA的整体目标是根据历史帧序列、当前多视角图像和语言指令，预测一个未来的动作序列 {a_t, a_t+1, ..., a_t+s} 来驱动机器人完成任务。其框架由三部分组成：预训练的VLM、状态感知潜在重表示（SALR）模块和动作专家。\n\n![方法框架](https://arxiv.org/html/2512.20166v1/x1.png)\n> **图1**：LoLA模型整体框架。左侧输入包括长时域历史帧、当前多视角图像和语言指令。预训练VLM编码后，其输出的Key/Value嵌入与机器人状态一同输入至核心的SALR模块，该模块构建了一个“具身锚定”的潜在空间。最终，动作专家基于该对齐后的潜在条件生成多步动作序列。\n\n**1. 预训练VLM视觉编码**：该部分采用两种编码策略以平衡计算开销与信息完整性。\n*   **当前观察编码**：处理当前时刻t的高分辨率主视角V_t和多视角图像𝒱_m（如次级视角、腕部视角），提取详细的当前空间关系嵌入F_coe，为精细动作生成提供精确的瞬时空间信息。\n*   **历史运动编码**：采用选择性时空采样策略，将历史帧序列𝒱_hme下采样至较低分辨率后输入VLM，提取编码了关键运动信息和任务进程的历史嵌入F_hme。此举旨在保留必要的时序上下文同时显著降低计算负担。最终，F_coe、F_hme与语言指令嵌入F_l拼接，输入VLM的L层，每层产生中间Key和Value嵌入{K_i, V_i}，供后续SALR模块使用。\n\n**2. 状态感知潜在重表示（SALR）**：这是LoLA的核心创新模块，旨在解决视觉语言嵌入与机器人动作空间之间的“模态鸿沟”。\n*   **问题与动机**：现有方法通常简单地将机器人本体状态（如关节角）与VL嵌入拼接，这种“后期融合”迫使模型隐式学习物理对齐，容易受嵌入中无关信息（如背景）干扰。相反，机器人状态s_t与动作a_t在物理域中本质耦合（如s_t+1 = s_t ⊕ a_t），因此s_t是过滤未接地VL嵌入的理想“锚点”。\n*   **技术细节**：SALR引入一个与VLM层数L平行的状态变换器。在每一层i，机器人状态s_t经投影和自注意力后生成查询向量Q_r。关键创新在于使用Q_r通过**外积融合**（element-wise product）来调制VLM对应层的{K_i, V_i}（公式4，5），从而生成扩展的潜在表示K*, V*。此操作将状态信息深度注入视觉语言特征。\n*   **学习掩码与压缩**：为进一步提取动作相关线索，引入可学习掩码M_k, M_v对K*, V*进行自适应过滤（公式6）。最后，通过潜在空间压缩策略，将过滤后的特征压缩为紧凑的、动作专家可用的键值嵌入{K^a, V^a}。\n\n![SALR模块](https://arxiv.org/html/2512.20166v1/x2.png)\n> **图2**：状态感知潜在重表示（SALR）模块示意图。展示了通过状态嵌入与VLM的Key/Value进行外积融合，形成潜在空间(V, S, H)。虚线箭头表示将机器人夹爪的视觉观测映射到真实物理尺度（如平移和旋转值）的投影对。\n\n**3. 动作专家**：该模块基于条件流匹配（CFM）实现，是一个M层Transformer解码器。它以SALR输出的对齐潜在条件{K^a, V^a}、采样的噪声动作轨迹以及噪声时间步为输入，通过交叉注意力注入多模态指导，并经由自注意力建模动作序列的时序结构。在推理时，通过多步去噪从纯噪声中解码出平滑的动作序列。\n\n## 实验与结果\n**实验设置**：LoLA在包含110万条真实机器人轨迹的OXE和AgiBot数据集上进行预训练。评估涵盖仿真基准（SIMPLER、LIBERO）和真实机器人平台（Franka Research 3、双手Aloha机器人）。对比的基线方法包括RT系列、Octo、OpenVLA、π₀、CogACT、Diffusion Policy等主流VLA或扩散策略模型。\n\n**关键实验结果**：\n1.  **SIMPLER基准**：在Google机器人任务上（表1），LoLA在“视觉匹配”和“变体聚合”设置下的平均成功率分别达到61.5%和54.6%，显著优于π₀（52.7%， 46.0%）和RT-2-X（46.3%， 54.4%）。在更具挑战性的WidowX机器人多步任务上（表2），LoLA取得了71.9%的平均成功率，相对π₀（41.7%）有20.2个百分点的绝对提升，其中“Put Spoon on Towel”任务成功率高达95.8%。\n\n2.  **LIBERO基准**：LoLA在四个任务套件（Goal, Object, Spatial, Long）上均达到最优，平均成功率为96.2%（表3）。尤其在测试长时域能力的LIBERO-Long套件上，取得了88.2%的成功率，优于π₀的85.4%，证明了其利用历史信息处理复杂多步任务的有效性。\n\n3.  **真实世界评估**：\n    *   **单步任务**：在Franka平台的9个原子任务上（表4），LoLA平均成功率为46.1%，优于π₀的36.8%。\n    *   **多步长时域任务**：在由原子任务组成的序列任务（如T1,T2,T3）和连续长时域任务上（表6），LoLA优势更明显。例如，在任务序列“T4,T5,T6”和“T7,T8,T9”上，LoLA的成功率分别为33.1%和28.9%，显著高于π₀的12.4%和16.6%。\n\n![真实世界实验设置](https://arxiv.org/html/2512.20166v1/x3.png)\n> **图3**：真实世界实验设置，展示了Franka Research 3和双手Aloha机器人平台、多视角相机输入及对应的文本指令。任务1、2、3构成了一个顺序的长时域操纵任务：“将平底锅从桌子放到烤箱平台上”。\n\n![消融实验结果](https://arxiv.org/html/2512.20166v1/x4.png)\n> **图4**：消融实验结果（对应论文表5）。展示了冻结VLM权重（FrozenVL）、使用多历史帧（MF）以及SALR模块对WidowX机器人任务成功率的影响。完整模型（最后一行）性能最佳，而移除SALR（第一行）或仅使用单帧（第三行）均导致性能显著下降。\n\n**消融实验分析**：消融实验（表5，图4）验证了各组件贡献。仅使用单帧且无SALR时，平均成功率仅为30.3%。引入多历史帧（MF）或SALR模块分别将平均成功率提升至41.7%和44.8%。同时使用二者但冻结VLM权重时，性能进一步提升至57.3%。而完整模型（使用多帧、SALR且VLM参与训练）取得了最高的71.9%成功率。这证明了历史信息、SALR的对齐能力以及VLM的微调对于长时域任务都至关重要。\n\n## 总结与启发\n**核心贡献**：\n1.  **长时域历史信息的高效编码**：提出了结合高保真当前观察编码与下采样历史运动编码的双路径策略，在保留关键时序上下文的同时，显著降低了处理长视频序列的计算负担。\n2.  **状态感知潜在重表示（SALR）模块**：创新性地通过状态变换器和外积融合，将机器人本体感知作为“锚点”，显式地将视觉语言特征对齐到物理动作空间，并利用可学习掩码过滤无关信息，有效缓解了状态漂移问题。\n3.  **在长时域任务上的卓越性能**：在仿真和真实世界的多种基准测试中，LoLA在长时域、多步操纵任务上显著超越了现有SOTA方法，证明了其框架在提升动作连贯性和任务成功率方面的有效性。\n\n**局限性**：论文提到，处理长历史序列虽经优化，但仍会带来额外的计算开销。此外，该方法依赖于获取准确的机器人本体状态信息（关节角、末端位姿）。\n\n**后续研究启示**：LoLA的工作表明，显式地建立感知与动作在物理尺度上的对齐是提升VLA模型在长时域任务中鲁棒性的关键方向。其SALR模块的设计为多模态融合提供了新思路。未来研究可探索更高效的历史信息压缩方法，或研究在部分状态信息缺失情况下的鲁棒对齐策略。",
      "imageUrls": [
        "https://arxiv.org/html/2512.20166v1/x1.png",
        "https://arxiv.org/html/2512.20166v1/x2.png",
        "https://arxiv.org/html/2512.20166v1/x3.png",
        "https://arxiv.org/html/2512.20166v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.20847",
      "title": "YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion",
      "url": "http://arxiv.org/abs/2512.20847",
      "arxivId": "2512.20847",
      "date": "2025-12-23",
      "authors": "Christian Smith Team",
      "category": "Manipulation",
      "summary": "本文核心是解决机器人交接动作中缺乏对物体重量适应性研究的数据缺口。通过构建YCB-Handovers数据集，记录了2771次人类间交接动作，并分析不同重量物体的影响。关键技术是基于YCB物体集，扩展采集人类交接运动模式，用于驱动重量敏感的运动规划模型。核心贡献是提供了首个涵盖广泛重量范围的交接数据集，包含2771次交互数据，详细分析了重量对人类伸手运动的影响，为机器人实现自然、安全的适应性交接提供了数据基础。",
      "detailedSummary": "## 研究背景与动机\n人机交接是协作机器人领域的一项基础而复杂的任务。当前，虽然已有一些研究数据集和方法致力于理解交接动态，例如HandoverSim仿真环境、OHO数据集、H2O数据集以及HOH数据集，但这些数据集普遍缺乏对物体重量这一关键属性影响的深入探索。现有数据集要么侧重于视觉识别，要么未明确包含物体重量信息，这使得开发能够根据物体重量自适应调整动作的机器人系统面临数据瓶颈。\n\n本文针对现有交接数据集在物体重量多样性方面的不足，提出了一个新颖的视角：通过大规模采集人类在交接不同重量物体时的动作数据，来系统分析重量对人类交接动作的影响，并以此为基础为机器人提供数据驱动的、受人类启发的重量敏感动作规划参考。本文的核心思路是创建并发布一个基于YCB物体集的、包含广泛重量范围的人类交接动作捕捉数据集（YCB-Handovers），并通过详细的数据分析揭示重量与交接运动学特征之间的关系，进而探索利用这些数据训练重量分类模型的可能性。\n\n## 方法详解\n本研究的方法论围绕YCB-Handovers数据集的创建与分析展开，主要包括实验设计、数据收集与后处理三个阶段。\n\n**整体框架**：研究首先设计了一系列人类交接实验，使用动作捕捉系统记录六对参与者在交接五组不同重量和类型物体时的全身动作数据。随后，对采集到的原始动作轨迹数据进行分割，提取出每次独立的交接事件及其相关的运动学特征。最后，基于分割后的数据，分析物体重量对交接动作（如速度、加速度）的影响，并尝试使用机器学习方法进行重量分类。\n\n![YCB-Handovers Dataset](https://arxiv.org/html/2512.20847v1/x1.png)\n> **图1**：YCB-Handovers数据集概览。展示了在动作捕捉室内记录的人类交接互动，左侧是一次标记笔的交接，右侧是数据集中使用的部分YCB物体。\n\n**核心模块与技术细节**：\n1.  **实验设计与数据收集**：\n    *   **参与者与设置**：12名参与者组成6对，在动作捕捉室内面对面站立进行交接。使用Optitrack系统记录参与者上身关键关节（手腕、肘部、肩膀等）标记点的三维空间轨迹。\n    *   **物体与流程**：使用了来自YCB物体集的多种物体，并按重量和特性分成五个“篮子”（Basket）。前四个篮子包含从极轻（0.008kg）到较重（1.45kg）的物体，参与者以篮子为单位进行循环交接，每次持续9分钟。第五个篮子则专注于研究“小心”交接，包含轻/重的测量杯和YCB水罐（通过加水或配重改变重量），对每个物体进行2分钟的重复交接。具体物体清单见论文表I。\n    *   ![不同篮子中的物体](https://arxiv.org/html/2512.20847v1/figs/Basket1.jpg)\n    > **图2**：数据收集中使用的不同篮子内的物体示例。\n\n2.  **数据后处理与分割**：\n    *   原始数据以rosbag格式存储，后转换为.csv文件。\n    *   分割的关键是识别每次交接发生的时刻。算法基于交接双方优势手在三维空间中的接近程度设定阈值。当给予者和接收者的手同时持握物体并彼此靠近时，即被判定为一次交接事件。\n    *   使用自定义MATLAB脚本完成数据加载、同步、基于接近度的分割以及特征提取。为每次检测到的交接提取参与者上身各部位的空间位置数据，并保存为独立的.csv文件。\n\n![优势手位置轨迹](https://arxiv.org/html/2512.20847v1/x2.png)\n> **图3**：一对参与者在进行非小心状态的YCB水罐交接时，连续两次交接过程中优势手的位置轨迹。该图展示了用于分割手交接事件的手部运动数据。\n\n**创新点**：与现有数据集相比，YCB-Handovers的创新性主要体现在：1) 首次系统性地采集并提供了覆盖广泛重量范围（从8克到2.06公斤）的大规模人类交接动作捕捉数据（共2771次交接）；2) 明确包含了需要“小心”处理的物体交接场景，以研究谨慎性与重量的复合效应；3) 数据集基于广泛使用的YCB物体集构建，便于与现有的机器人抓取和操作研究进行结合与对比。\n\n## 实验与结果\n论文在YCB-Handovers数据集上进行了系统的分析实验，主要围绕重量对运动的影响以及重量分类可行性展开。\n\n**实验设置**：分析基于采集到的2771次人类交接动作数据。运动数据使用5Hz截止频率的4阶巴特沃斯滤波器进行平滑去噪。分析中排除了需要特别小心的交接对象，以聚焦于重量本身的影响。\n\n**关键实验结果**：\n1.  **重量对运动特性的影响**：论文分析了交接“伸手阶段”手部的平均速度、最大速度、平均加速度和最大加速度。\n    *   ![不同重量物体的运动特性](https://arxiv.org/html/2512.20847v1/x3.png)\n    > **图4**：交接不同重量物体时，人类手部运动的平均速度、最大速度、平均加速度和最大加速度的均值。总体趋势显示，物体越重，运动越慢，加速度越低。\n    *   相关性分析表明，物体重量与平均加速度呈中等负相关（r=-0.538），与平均速度、最大速度、最大加速度呈弱负相关。这表明重量增加倾向于导致运动减缓，但关系并非绝对强相关，暗示个体差异和其他物体属性（形状、大小）也起作用。\n    *   将物体按重量分组（低：<0.1kg，中：0.1-0.95kg，高：>0.95kg）进行比较发现，低重量组具有最高的平均速度（1.79 m/s）和平均加速度（9.43 m/s²），高重量组最低（1.66 m/s， 8.27 m/s²），且低重与高重组在平均加速度和最大加速度上存在显著差异（p值分别为0.002和0.004）。\n    *   ![不同重量分组的运动特性箱线图](https://arxiv.org/html/2512.20847v1/x4.png)\n    > **图5**：低、中、高三个重量组在平均速度、最大速度、平均加速度和最大加速度上的分布箱线图。清晰显示了运动指标随重量增加而下降的趋势。\n\n2.  **基于机器学习的重量分类**：\n    *   **无监督学习（K-Means）**：尝试仅根据手部或完整手臂的运动轨迹数据对交接进行重量分类（分2类或3类）。结果准确率大多在50%左右徘徊，接近随机猜测，表明无监督方法难以清晰区分不同重量类别。\n    *   **监督学习（KNN, SVM, Random Forest）**：使用已知重量标签的数据训练分类器。\n        *   **二分类**：当重量界限设置得较宽时（如以1kg为界），随机森林分类器使用完整手臂数据可达97.09%的准确率。但当界限更精细时（如0.1kg vs 0.95kg），准确率下降至81.25%。\n        *   **三分类**：准确率进一步下降。对于精细分类（如0.1kg, 0.1-0.95kg, 0.95kg），最高准确率为65.03%（随机森林，完整手臂）。若采用更宽的区间（如1kg, 1-2kg, 2kg），准确率可提升至82.19%。\n        *   ![监督学习二分类准确率表](https://arxiv.org/html/2512.20847v1/x5.png)\n        > **图6**：使用完整手臂或仅手部数据，在不同重量区间划分下，KNN、SVM和随机森林进行二分类的准确率。\n        *   ![监督学习三分类准确率表](https://arxiv.org/html/2512.20847v1/x6.png)\n        > **图7**：使用完整手臂或仅手部数据，在不同重量区间划分下，KNN、SVM和随机森林进行三分类的准确率。\n\n3.  **谨慎性与重量的复合效应分析**：\n    *   针对需要小心处理的物体（轻/重测量杯，轻/重水罐）的四分类（物体+谨慎性），随机森林分类器取得了80.5%的最佳准确率。\n    *   若仅按重量分类（轻 vs 重），准确率提升至90.9%（随机森林）。\n    *   若仅按谨慎性分类（小心 vs 非小心），准确率为87.1%（随机森林）。\n    *   ![四分类、重量分类、谨慎性分类准确率表](https://arxiv.org/html/2512.20847v1/x7.png)\n    > **图8**：针对小心交接场景，不同分类任务下各分类器的准确率。表明重量对运动模式的影响比谨慎性更强。\n\n**消融实验总结**：实验部分虽未以典型消融实验形式呈现，但通过对比不同数据范围（完整手臂 vs 仅手部）、不同分类方法（无监督 vs 监督）、不同分类器以及不同重量类别划分下的结果，清晰地展示了：1) 使用完整手臂运动数据通常比仅用手部数据能获得更好的分类性能；2) 监督学习方法显著优于无监督方法；3) 重量分类的难度随类别细化而增加；4) 在影响运动模式上，物体重量的信号可能强于交接的谨慎性信号。\n\n## 总结与启发\n**核心贡献**：\n1.  **数据集贡献**：发布了首个大规模、基于YCB物体集、涵盖广泛重量范围并包含谨慎交接场景的人类交接动作捕捉数据集（YCB-Handovers），填补了该领域的数据空白。\n2.  **分析贡献**：通过详实的分析，定量揭示了物体重量对人类交接运动学特征（速度、加速度）的普遍影响趋势（负相关），并指出个体差异和物体其他属性也会调制这一影响。\n3.  **方法探索贡献**：系统探索了利用交接运动数据进行重量分类的可行性，证明了监督学习方法在粗略重量分类上的有效性，并揭示了精细分类的挑战。\n\n**局限性**：论文自身提到的局限性包括：1) 动作捕捉数据可能存在因标记点被遮挡或参与者位置导致的噪声和暂时性误差；2) 数据集中可能包含交接前后非必要的肢体运动，可能给模型训练带来不一致性；3) 分析表明重量与运动特征的相关性并非绝对强，反映了人类行为的复杂性和个体差异性，这可能限制基于群体平均数据构建的机器人模型的普适性。\n\n**对后续研究的启示**：\n1.  该数据集为开发数据驱动的、重量自适应机器人交接规划算法提供了宝贵的训练和测试资源。\n2.  分析结果表明，要实现精细的物体重量估计或基于重量的精准动作调整，可能需要结合更多模态的信息（如视觉、力觉）或考虑个性化建模。\n3.  关于谨慎性与重量复合效应的初步分析，启发了对机器人交接中安全性、柔顺性与效率之间权衡的进一步研究。\n4.  未来的工作可以探索如何将从此数据集中学到的人类重量适应策略，有效地迁移到真实的机器人控制系统上。",
      "imageUrls": [
        "https://arxiv.org/html/2512.20847v1/x1.png",
        "https://arxiv.org/html/2512.20847v1/figs/Basket1.jpg",
        "https://arxiv.org/html/2512.20847v1/figs/Basket2.jpg",
        "https://arxiv.org/html/2512.20847v1/figs/Basket3.jpg",
        "https://arxiv.org/html/2512.20847v1/figs/Basket4.jpg",
        "https://arxiv.org/html/2512.20847v1/x2.png",
        "https://arxiv.org/html/2512.20847v1/x3.png",
        "https://arxiv.org/html/2512.20847v1/x4.png",
        "https://arxiv.org/html/2512.20847v1/x5.png",
        "https://arxiv.org/html/2512.20847v1/x6.png",
        "https://arxiv.org/html/2512.20847v1/x7.png",
        "https://arxiv.org/html/2512.20847v1/x8.png",
        "https://arxiv.org/html/2512.20847v1/x9.png",
        "https://arxiv.org/html/2512.20847v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.19583",
      "title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations",
      "url": "http://arxiv.org/abs/2512.19583",
      "arxivId": "2512.19583",
      "date": "2025-12-22",
      "authors": "Ping Tan Team",
      "category": "Manipulation",
      "summary": "本文解决从合成数据学习通用化手-物体追踪控制器的数据瓶颈问题。提出两种关键技术：HOP（手-物体规划器）用于合成多样化轨迹；HOT（手-物体追踪器）通过强化学习与交互模仿学习实现合成到物理的迁移。实验表明，该方法能使灵巧手成功追踪长时程复杂序列，包括物体重排与手内敏捷重定向，并泛化至不同物体形状与手部形态。",
      "detailedSummary": "## 研究背景与动机\n灵巧手-物交互的跟踪与执行能力对于机器人完成多样化灵巧操作任务至关重要。当前训练通用手-物跟踪控制器面临的根本挑战在于获取足够的交互数据。现有数据主要依赖人类演示，通过动作捕捉设备收集费时费力，或从互联网视频中估计但存在遮挡和深度模糊等问题。本文针对这一数据瓶颈，提出了一种全新的视角：完全利用合成数据来训练通用的手-物跟踪控制器。其核心思路是：首先，设计一个可扩展的手-物规划器，从大量静态抓取姿态合成多样化的元技能演示轨迹；然后，通过结合强化学习与模仿学习的跟踪器，从这些可能不完美的合成演示中学习一个以目标状态为条件的通用跟踪控制器。\n\n## 方法详解\n本文系统包含两个核心部分：手-物规划器和手-物跟踪器。整体流程是：高层指令（如语言模型解析的文本、生成模型的轨迹或人类演示）被转化为稀疏路径点，输入给HOP生成密集的手-物轨迹，最后由HOT在仿真中执行跟踪。HOT的训练完全依赖于HOP生成的合成数据。\n\n![系统总览](https://arxiv.org/html/2512.19583v1/img/overview.png)\n> **图2**：系统整体架构。(a) HOP为元技能合成操作轨迹。(b) HOT通过两阶段师生框架，使用统一的HOI模仿奖励进行强化学习训练，实现对目标HOI轨迹的鲁棒跟踪。(c) 在推理阶段，系统接受来自语言模型、生成模型或人类数据的高层路径点，HOP将其转化为轨迹供HOT跟踪，从而实现多样化应用。\n\n**Hand-Object Planner (HOP)**：这是一个可扩展的数据生成框架，其流程分为两步。首先，给定刚性物体和灵巧手模型，利用力闭合优化和强化学习生成并精修一组多样且稳定的抓取配置集合 𝒢。其次，基于这些抓取姿态，合成八种基本元技能（自由移动、抓取、放置、移动、接住、抛出、旋转、重抓）及其组合的轨迹。例如，**抓取**技能的合成过程是：从一个稳定物体姿态集合ℋ中采样物体初始位姿，从𝒢中采样一个抓取配置，通过坐标变换对齐得到抓取结束帧；然后，沿着从物体中心到食指第一个关节的向量方向，在锥形区域内随机采样手部初始位置，并将所有手部关节角设为零，得到初始帧；最后在初始帧和结束帧之间插值生成连续轨迹。**放置**技能则通过时间反转抓取轨迹高效生成。**移动**技能在保持固定相对抓取配置的同时，对手腕位姿和物体位姿进行插值。**旋转**技能则围绕物体上预定义的轴和可旋转区域，通过抓取关键帧的重复来生成演示序列，以适应大的状态变化。\n\n![HOP数据生成流程](https://arxiv.org/html/2512.19583v1/img/hop.png)\n> **图3**：HOP从通过力闭合优化生成并经RL精修的抓取姿态中合成操作轨迹。其基于语法的方法支持八种可组合的元技能，并通过随机化、LLM/VLM指令或人类演示提供多源参数控制。该系统能自然地泛化到不同的手和物体，实现可扩展的数据覆盖。\n\n**Hand-Object Tracker (HOT)**：这是一个基于MLP网络的策略，其目标是跟踪HOP生成的轨迹。策略输入𝒉𝑡包括当前环境状态𝒔𝑡（手部与物体的全局/局部位姿、速度、接触力等）以及下一时刻的目标状态𝒔̂ 𝑔𝑜𝑎𝑙𝑡+1（从参考轨迹中提取）。所有位置测量值（全局手腕位置除外）都转换到以手腕坐标定义的局部坐标系中，以利用操作的平移不变性。策略输出动作𝒂𝑡服从高斯分布，包括残差的6自由度手腕位姿调整量𝜹𝑤𝑟𝑖𝑠𝑡𝑡和手指关节的目标旋转量𝒂𝑓𝑖𝑛𝑔𝑒𝑟𝑡。对于手腕，采用增量控制方式；对于手指，采用直接位置控制，并通过PD控制器转换为扭矩命令。\n\nHOT的训练采用基于PPO的两阶段师生蒸馏框架。第一阶段，为不同的元技能类别（或不同物体）训练专门的教师策略。第二阶段，一个统一的学生策略通过行为克隆向教师策略学习，并同时使用教师策略进行在线数据精修和动作指导，从而高效地学习所有技能。训练中使用统一的HOI模仿奖励𝑟𝑡，它是手腕跟踪、手部跟踪、物体跟踪、交互跟踪和接触奖励五个互补分量的乘积。为了增强鲁棒性，采用了领域随机化课程，逐步增加对物体施加的随机扰动、物体的物理属性（尺寸、密度、摩擦等）以及初始状态的随机化强度。此外，还采用了自适应采样技术，动态调整训练中参考轨迹的采样概率，为困难样本提供更多学习机会。\n\n![HOT跟踪与精修不完美数据](https://arxiv.org/html/2512.19583v1/img/data_noise3.png)\n> **图4**：HOT能够跟踪并精修不完美的合成HOI演示。左：剑的抓取。右：瓶子的重抓。\n\n## 实验与结果\n实验在Isaac Gym仿真平台中使用单张RTX 4090 GPU进行。评估指标包括物体位置误差、物体旋转误差、手部平均每关节位置误差和成功率。\n\n**元技能跟踪性能**：如表1所示，将HOT与开环播放、SkillMimic和DexGen-var等方法在抓取、放置、移动和旋转四个元技能上进行比较。HOT在大多数技能上取得了最佳或极具竞争力的成功率与误差指标。特别是在最具挑战性的**旋转**技能上，HOT取得了显著优势（成功率76.32%），而其他方法成功率极低（SkillMimic为17.52%，DexGen-var为0%），这验证了其从合成数据中学习复杂操作模式的能力。\n\n**复杂长序列跟踪**：HOT能够跟踪由多个元技能组合而成的长视野、动态序列。\n\n![复杂HOI轨迹合成与跟踪](https://arxiv.org/html/2512.19583v1/img/complex2.png)\n> **图5**：合成与跟踪复杂的HOI轨迹。(a)：抓取-移动-旋转-放置序列。(b)：抓取-移动-旋转-移动序列。\n\n**消融实验**：研究验证了所提关键组件的有效性。1) **师生框架**：与单阶段训练相比，两阶段师生框架将旋转技能的成功率从约50%提升至76.32%，并显著加快了训练收敛速度。2) **课程随机化**：逐步增加随机化强度比固定强度或没有随机化能带来更高的最终性能和稳定性。3) **自适应采样**：动态重加权困难样本提高了在具有挑战性轨迹上的性能。\n\n**下游应用与泛化**：\n- **跟踪人类演示**：HOT能够零样本跟踪来自GRAB数据集的真实人类手-物交互运动。\n- **文本引导任务执行**：结合VLM解析文本指令，系统能规划并执行如“拿起锤子，敲击钉子”等任务。\n- **多手形与多物体泛化**：系统展示了对不同手部模型（如MANO、Shadow Hand）和大量新物体形状的泛化能力。\n\n![下游应用](https://arxiv.org/html/2512.19583v1/img/application.png)\n> **图7**：下游应用展示。系统能够跟踪真实人类动作数据，并集成VLM根据语言指令（如“拿起锤子敲钉子”）执行任务。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了HOP，一个完全从静态抓取合成动态手-物操作轨迹的可扩展规划器，突破了以往数据生成方法对现有人类演示的依赖。2) 提出了HOT，一个通过两阶段师生强化模仿学习框架训练的通用手-物跟踪控制器，能够从可能不完美的合成数据中学习，并泛化至复杂的长序列任务。3) 首次系统性地证明了仅使用合成数据即可训练出性能强大的通用灵巧操作基础控制器，并在跟踪人类数据、语言引导任务等下游应用上展示了零样本泛化能力。\n\n论文自身提到的局限性在于，尽管方法对不完美的合成数据具有鲁棒性，但合成数据与真实物理世界之间仍存在差距。这项工作为后续研究提供了重要启示：合成数据是解决灵巧操作数据瓶颈的一条极具潜力的路径，通过精心设计的生成框架与鲁棒的学习算法结合，可以大幅减少对昂贵人类演示的依赖，为实现可扩展的通用操作基础模型奠定了基础。",
      "imageUrls": [
        "https://arxiv.org/html/2512.19583v1/img/teaser.png",
        "https://arxiv.org/html/2512.19583v1/img/overview.png",
        "https://arxiv.org/html/2512.19583v1/img/hop.png",
        "https://arxiv.org/html/2512.19583v1/img/data_noise3.png",
        "https://arxiv.org/html/2512.19583v1/img/complex2.png",
        "https://arxiv.org/html/2512.19583v1/img/Figure7.png",
        "https://arxiv.org/html/2512.19583v1/img/application.png",
        "https://arxiv.org/html/2512.19583v1/img/multihand.png",
        "https://arxiv.org/html/2512.19583v1/img/grab.jpg",
        "https://arxiv.org/html/2512.19583v1/img/hard_regrasp.png",
        "https://arxiv.org/html/2512.19583v1/img/objects.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.19390",
      "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.19390",
      "arxivId": "2512.19390",
      "date": "2025-12-22",
      "authors": "Hao Dong Team",
      "category": "Manipulation",
      "summary": "本文提出TwinAligner系统，旨在解决机器人操作中仿真与现实之间的视觉与动力学差距，以实现高效的数据驱动策略训练。其核心技术包括：视觉对齐模块通过SDF重建与可编辑3DGS渲染实现像素级对齐；动态对齐模块通过分析机器人-物体交互的刚性物理确保动力学一致性。实验表明，该系统能显著提升仿真与真实世界的一致性，使仿真训练的策略在真实场景中实现强大的零样本泛化，且两者性能高度一致。",
      "detailedSummary": "## 研究背景与动机\n机器人学领域正受多模态大模型启发，向数据驱动的端到端学习演进。然而，依赖昂贵的真实世界数据限制了进展。物理仿真器提供了更具成本效益的替代方案，但仿真与现实的差距对有效的策略迁移构成了挑战。现有方法如域随机化需要大量数据收集和高参数可变性，样本效率低，且无法提供与真实世界严格对齐的仿真环境以进行准确模型评估。因此，开发一个在视觉和动力学维度上全面对齐的Real2Sim2Real系统极具价值且充满技术挑战。\n\n本文针对现有方法未能统一解决视觉差距和动力学差距这一具体痛点，提出了一个新颖的视角。视觉差距源于渲染保真度、纹理真实感和几何一致性的不匹配；动力学差距则源于仿真器中近似的物理模型和不准确的物理参数。本文的核心思路是提出TwinAligner系统，通过视觉对齐模块实现像素级对齐，并通过动态对齐模块确保动力学一致性，从而构建一个视觉与动力学高度对齐的数字孪生，赋能物理感知的机器人操作。\n\n## 方法详解\nTwinAligner框架旨在通过三个步骤实现物理感知的Real2Sim2Real闭环。首先，Mesh-GS数字孪生模块联合重建详细的视觉外观和几何结构，并确保基础的渲染和碰撞效果与真实世界对齐。其次，视觉-动态Real2Sim对齐模块通过联合估计相机视角、机器人控制器和物体刚体物理参数，来对齐真实世界的动力学交互。最后，在对齐的仿真环境中进行Sim2Real策略学习，从而在常规和物理感知的机器人操作场景中缩小Sim2Real差距。\n\n![方法总览](https://arxiv.org/html/2512.19390v1/x2.png)\n> **图2**：TwinAligner框架概览。包含Real2Sim和Sim2Real两个阶段。在Real2Sim阶段，同时考虑仿真与真实世界之间的视觉对齐和动态对齐。通过在Real2Sim创建的仿真中收集的机器人轨迹上训练的策略，可以直接零样本泛化到真实世界。\n\n**1. Mesh-GS数字孪生用于物理仿真**\n该模块旨在同时实现高质量视觉渲染和几何碰撞。它使用多视角RGB图像进行重建。对于刚性物体，一方面通过SDF表示重建水密网格以确保仿真器友好的碰撞；另一方面，用该网格的顶点初始化3D高斯泼溅表示，并通过RGB和SSIM损失进行监督渲染，以平衡网格碰撞和渲染质量。还使用了掩码损失和单目深度损失进行正则化。对于场景，使用基于平面的PGSR方法优化3DGS并保持几何连续性，并通过TSDF融合提取网格。对于机器人，通过ICP和k-NN算法对齐机器人URDF和重建的场景3DGS，提取机器人高斯表示并添加正向运动学。\n\n**2. 视觉-动态Real2Sim对齐**\n在获得仿真就绪的数字孪生后，将机器人仿真器划分为动态仿真器和相机渲染器。剩余的对齐问题在于视角 `p` 和动力学参数 `θ`。\n*   **视角对齐**：通过捕获特定关节状态下的RGB-D图像，使用FoundationPose预测粗略的机器人-相机变换，然后利用梯度免费优化器，通过优化模拟图像与真实图像在显著区域（如机器人和桌面工作空间）的掩码匹配损失，来细化该变换，实现像素级对齐。\n*   **动态对齐**：在视角对齐后，剩余的差距在于刚体物理参数。论文提出以“控制-撞击-滑动”的方式收集机器人-物体交互数据来联合优化多个物理属性。\n    *   **机器人动力学对齐**：通过调整仿真中的机器人控制器物理参数，使其拟合真实机器人物理，最小化关节状态差距，确保“撞击”阶段的一致性。\n    *   **物体动力学对齐**：物体“滑动”阶段的运动由摩擦、质量和质心参数协同决定。通过重放与真实世界相同的机器人控制信号，并使用FoundationPose估计的真实物体6-DoF姿态作为监督，采用基于点云的ADD和ADD-S损失，并利用梯度免费优化器来优化这些物理参数，使仿真轨迹与真实轨迹对齐。\n\n**3. Sim2Real策略学习**\n在对齐的环境中进行模仿学习策略训练。通过仿真中的遥操作收集专家演示轨迹，记录关节角度、末端执行器状态、物体状态并渲染相机观测。策略基于末端执行器状态和相机观测进行训练。得益于严格对齐的环境，训练的策略可实现零样本泛化到真实世界，同时也可在TwinAligner中进行跨环境策略评估，形成“仿真-渲染-策略”的闭环。\n\n**创新点**：与现有方法相比，TwinAligner的创新在于统一解决了视觉和动力学两个维度的对齐；提出了Mesh-GS数字孪生来平衡渲染与碰撞质量；在动态对齐中采用了梯度免费优化，使其能兼容非可微仿真器。\n\n## 实验与结果\n实验使用了Franka Research 3机器人、Realsense D455深度相机和防震台硬件平台，以及Genesis物理仿真引擎。评估了包括杯子、勺子、笔记本电脑、午餐肉罐头和木块在内的多个物体。对比的基线方法在视觉质量方面包括2DGS、PGSR及其结合深度正则化的变体；在动态对齐方面包括PIN-WM；在策略学习方面包括SplatSim（结合Gemini初始化物理参数）和PIN-WM。\n\n**1. Real2Sim一致性评估**\n*   **物体数字孪生质量**：在渲染质量上，TwinAligner在PSNR指标上取得了最高平均分。在网格重建质量上，如图4所示，本文方法重建出了细节丰富且水密的表面，而基线结果存在深度不准确、瑕疵和孔洞。\n\n![几何重建质量对比](https://arxiv.org/html/2512.19390v1/x4.png)\n> **图4**：几何重建质量对比。我们的方法重建了水密且细节丰富的网格，而基线结果包含不准确的深度、瑕疵和孔洞。\n\n*   **动态Real2Sim对齐**：在四个具有不同物理特性的物体上评估动态对齐。如表IV所示，TwinAligner在ADD和ADD-S指标上均优于PIN-WM，表明其动力学对齐更准确。图5定性展示了在机器人-物体交互轨迹上，TwinAligner的仿真与真实世界观测在像素级别严格对齐。\n\n![动态对齐效果](https://arxiv.org/html/2512.19390v1/x5.png)\n> **图5**：我们的视觉-动态Real2Sim对齐的有效性。对于机器人-物体交互轨迹，我们将真实世界相机观测与TwinAligner的物理仿真和3DGS渲染结果进行比较。我们的方法在像素级别严格对齐了视觉-动态差距。\n\n**2. 零样本Sim2Real泛化能力**\n在四个机器人操作任务上评估策略性能：推牛奶盒、堆叠饼干盒、抓取放置和关闭笔记本电脑。使用Diffusion Policy和RISE两种策略进行训练。结果如表III所示。\n\n![任务与硬件设置](https://arxiv.org/html/2512.19390v1/x6.png)\n> **图6**：(a) 策略评估任务，涵盖从物理相关任务到常规机器人操作任务。(b) 我们的硬件设置包括Franka Research 3机器人、Realsense D455深度相机和防震台。\n\nTwinAligner在所有任务上均超越了基线方法SplatSim+Gemini和PIN-WM。例如，在推牛奶盒任务中，使用DP策略的成功率达到10/15，显著高于PIN-WM的6/15和SplatSim的5/15。在堆叠饼干盒任务中，TwinAligner的成功率（14/15）甚至接近真实世界数据训练的结果（13/15），而SplatSim因无法准确识别质心而失败（0/15）。这表明TwinAligner的高质量对齐有效缩小了Sim2Real差距。\n\n**3. 跨环境策略评估**\n如图7所示，在TwinAligner仿真环境中评估的策略成功率与在真实世界中评估的成功率高度一致，证明了TwinAligner作为可靠策略评估器的潜力。\n\n![策略评估一致性](https://arxiv.org/html/2512.19390v1/x7.png)\n> **图7**：在TwinAligner仿真环境与真实世界中评估的策略成功率高度一致，表明其可作为可靠的策略评估器。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了一个统一的视觉-动态对齐框架TwinAligner，首次在Real2Sim2Real系统中同时有效解决了视觉和动力学差距；2) 设计了Mesh-GS数字孪生模块，通过结合SDF网格和可编辑3DGS，实现了高质量渲染与精确物理碰撞的平衡；3) 采用了梯度免费的动态对齐优化方法，增强了对非可微仿真器的兼容性和对重建误差的鲁棒性。\n\n论文自身提到的局限性在于，视角和物体姿态对齐依赖于FoundationPose等外部估计器，这些估计器的误差可能会传播到后续的优化过程中。\n\n本文工作对后续研究的启示在于：它为机器人学习提供了一个高度可信的闭环评估系统，能够加速算法迭代；其构建高保真数字孪生的方法，为在仿真中进行安全、可扩展的物理交互研究和数据生成开辟了新途径。",
      "imageUrls": [
        "https://arxiv.org/html/2512.19390v1/x1.png",
        "https://arxiv.org/html/2512.19390v1/x2.png",
        "https://arxiv.org/html/2512.19390v1/x3.png",
        "https://arxiv.org/html/2512.19390v1/x4.png",
        "https://arxiv.org/html/2512.19390v1/x5.png",
        "https://arxiv.org/html/2512.19390v1/x6.png",
        "https://arxiv.org/html/2512.19390v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.19347",
      "title": "OMP: One-step Meanflow Policy with Directional Alignment",
      "url": "http://arxiv.org/abs/2512.19347",
      "arxivId": "2512.19347",
      "date": "2025-12-22",
      "authors": "Yutong Ban Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中生成策略的推理延迟与架构复杂度权衡问题，提出单步平均流策略OMP。核心创新包括：引入轻量级方向对齐机制，显式同步预测速度与真实平均速度；采用微分推导方程近似雅可比向量积，解耦前向与反向传播以降低内存开销。在Adroit和Meta-World基准测试中，OMP在成功率和轨迹精度上超越现有方法，尤其在高精度任务中表现优异，同时保持了单步推理的高效性。",
      "detailedSummary": "## 研究背景与动机\n机器人操作领域正日益采用数据驱动的生成式策略框架，但该领域面临一个持续的权衡：扩散模型（如Diffusion Policy, DP3）虽然能生成高保真度的动作，但其迭代去噪过程导致高推理延迟（NFE=10），限制了高频实时控制；而基于流的方法（如FlowPolicy, ManiFlow）通过常微分方程（ODE）或一致性蒸馏实现快速单步推理（NFE=1），但往往需要复杂的训练流程或架构约束，在泛化到新场景时存在困难。近期提出的MeanFlow范式通过直接建模区间平均速度，理论上可以绕过ODE求解器实现单步推理，其机器人应用MP1已展现出潜力。然而，本文通过严格分析发现，将MeanFlow直接应用于机器人操作存在三个关键的理论病理：1) **光谱偏差**：平均速度目标在频域上相当于一个低通滤波器，会衰减精细操作所需的高频快速调整信号；2) **梯度饥饿**：在需要低速、精细运动的高精度任务中，标准的均方误差（MSE）损失在目标速度幅度趋近于零时，其方向对齐的梯度也会消失，导致策略无法学习正确的移动方向；3) **内存复杂度**：计算MeanFlow恒等式所需的确切雅可比-向量积（JVP）涉及嵌套导数计算，导致内存开销随切线维度急剧增加，阻碍了大规模骨干网络的训练。\n\n本文针对这些痛点，提出了**一步均值流策略（OMP）**。其核心思路是：通过引入**方向对齐机制**，将速度方向的学习与幅度解耦，以克服光谱偏差和梯度饥饿；同时，通过**微分推导方程（DDE）** 来近似JVP算子，解耦前向和后向传播，从而显著降低内存复杂度，最终实现兼具高保真度和实时性的单步动作生成。\n\n## 方法详解\nOMP方法建立在MeanFlow框架之上，旨在从3D点云输入直接实现单步轨迹生成。其整体目标是学习一个参数化模型 \\( u_{\\theta}(z_t, r, t|c) \\)，以预测从噪声状态 \\( z_T \\) 到目标状态 \\( z_0 \\) 的区间平均速度，其中 \\( v_0 = z_T - z_0 \\) 是真实平均速度。标准的MeanFlow恒等式为 \\( u = v - (t-r)\\frac{d}{dt}u \\)，训练时最小化预测值 \\( u_{\\theta} \\) 与由恒等式右侧构成的靶标 \\( u_{tgt} \\) 之间的MSE损失。\n\n![方法框架总览](https://arxiv.org/html/2512.19347v2/figures/overview_v4.png)\n> **图3**：生成式策略轨迹示意图对比。DP3需要多步去噪（NFE=10）。FlowPolicy使用分段直线流但需要一致性约束。Mean Policy预测区间平均速度，但由于训练偏差，预测速度与目标速度之间可能存在未对齐问题。本文提出的OMP引入了方向对齐机制，强制预测的速度向量与真实平均速度方向明确对齐，确保单步推理的轨迹精度。\n\nOMP的核心创新在于两个模块，旨在解决第4.2节分析的理论病理：\n1.  **方向对齐机制**：为了解决梯度饥饿和光谱偏差，OMP将方向学习与幅度学习解耦。其**方向对齐损失** \\( \\mathcal{L}_{DA} \\) 定义为预测速度 \\( u \\) 与真实平均速度 \\( v_0 \\) 之间余弦相似度的负对数：\\( \\mathcal{L}_{DA} = -\\log\\left(\\frac{\\cos\\alpha+1}{2}\\right) \\)，其中 \\( \\cos\\alpha = \\frac{v_0 \\cdot u}{||v_0|| \\cdot ||u||} \\)。该损失确保即使在 \\( ||v_0|| \\to 0 \\) 时，方向校正的梯度也保持非零。同时，通过直接与 \\( v_0 \\) 对齐，绕过了由MeanFlow恒等式构建靶标时引入的低通滤波效应。\n2.  **微分推导方程**：为了克服精确计算JVP带来的内存瓶颈，OMP采用**有限差分法**来近似时间导数 \\( \\frac{du_{\\theta}}{dt} \\)，称为微分推导方程（DDE）。具体近似为：\\( \\frac{du_{\\theta}}{dt} \\approx \\frac{u_{\\theta}(z_t, t+\\epsilon, r|c) - u_{\\theta}(z_t, t, \\epsilon, r|c)}{2\\epsilon} \\)，其中 \\( \\epsilon \\) 是一个扰动常数。这种近似将前向传播（计算 \\( u_{\\theta} \\) ）和后向传播（计算梯度）解耦，避免了存储中间雅可比计算图的需要，从而大幅降低了内存开销。\n\nOMP的最终训练目标是一个复合损失函数：\\( \\mathcal{L} = \\mathcal{L}_{mse} + \\lambda_{Disp} \\cdot \\mathcal{L}_{Disp} + \\lambda_{DA} \\cdot \\mathcal{L}_{DA} \\)。其中 \\( \\mathcal{L}_{mse} \\) 确保对大幅度过渡运动的重建，\\( \\mathcal{L}_{Disp} \\)（分散损失）鼓励潜在空间特征分离，而 \\( \\mathcal{L}_{DA} \\) 则保证在低速、高精度接触阶段的方向准确性。\n\n## 实验与结果\n**实验设置**：在Adroit（3个任务）和Meta-World（34个任务，按难度分为21个简单、4个中等、4个困难和5个极难任务）基准测试上进行评估，并进行了4个真实世界任务（放置瓶子、清洁桌面、滑环）的验证。使用10条专家演示进行训练，点云数据通过最远点采样（FPS）降采样至512或1024个点。\n\n**对比方法**：与多种SOTA方法对比，包括使用2D输入的DP、AdaFlow、CP，以及使用3D输入的DP3、Simple DP3、FlowPolicy和MP1。其中DP、DP3、Simple DP3为多步推理（NFE=10），CP、FlowPolicy、MP1和OMP为单步推理（NFE=1）。\n\n![主实验结果表](https://arxiv.org/html/2512.19347v2/figures/training_curve_v2.png)\n> **图5**：在Meta-World“门打开”任务上的训练曲线。OMP-DDE（红色）和OMP-JVP（蓝色）均展现出比基线方法MP1（绿色）和FlowPolicy（紫色）更快、更稳定的收敛速度，并且最终达到更高的成功率平台。\n\n![实验环境概览](https://arxiv.org/html/2512.19347v2/figures/experiments_v2.png)\n> **图4**：实验环境概览。前六列展示了模拟基准测试环境，最右列描绘了真实世界机器人实验场景。\n\n**关键结果**：\n1.  **性能领先**：如表1所示，OMP（包括使用精确JVP的OMP-JVP和使用DDE近似的OMP-DDE）在Adroit和Meta-World的37个任务平均成功率上均超越了所有基线方法。OMP-JVP取得了82.3%的平均成功率，OMP-DDE取得了80.8%，均显著高于此前最好的单步方法MP1（78.9%）和多步方法DP3（68.7%）。特别是在高精度任务（如Meta-World的“插销插入”、“穿线入孔”）中，OMP的优势更为明显。\n2.  **效率与精度兼得**：如图1所示，OMP在保持单步推理（高FPS）高效率的同时，其成功率达到了与多步扩散模型相当甚至更高的水平，成功解决了开篇提到的“延迟-保真度”权衡问题。\n3.  **消融实验验证**：表1中的消融研究表明，移除方向对齐损失（\\(-\\mathcal{L}_{DA}\\)）或分散损失（\\(-\\mathcal{L}_{dis}\\)）都会导致性能下降，尤其是在困难任务上，验证了各组件贡献的必要性。DDE近似版本（OMP-DDE）与精确JVP版本（OMP-JVP）性能接近，但前者内存效率更高。\n4.  **真实世界验证**：如图6所示，OMP成功迁移到真实机器人，在需要精细操作的任务（如滑环）中表现出色，证明了其在实际应用中的有效性。\n\n![真实世界实验结果](https://arxiv.org/html/2512.19347v2/figures/real_exp.png)\n> **图6**：真实世界机器人实验结果序列。OMP-DDE策略成功完成了放置瓶子、清洁桌面和滑环（需要高精度对准）等任务。\n\n## 总结与启发\n**核心贡献**：\n1.  **理论诊断与解决方案**：首次系统分析了MeanFlow范式直接应用于机器人策略学习时的三大理论病理（光谱偏差、梯度饥饿、高内存复杂度），并提出了针对性的解决方案。\n2.  **新颖的OMP框架**：提出了集成**方向对齐机制**和**微分推导方程**的OMP框架。方向对齐机制通过解耦方向与幅度学习，有效克服了梯度饥饿和光谱偏差；DDE通过有限差分近似，在几乎不损失性能的前提下大幅降低了训练内存开销。\n3.  **卓越的实验性能**：在模拟和真实世界实验中，OMP在推理速度（单步）和任务成功率（特别是高精度任务）上均超越了现有SOTA方法，为实时生成式机器人控制设立了新标准。\n\n**局限性**：论文提到，方向对齐机制在速度幅度极低时（\\( \\rho^* \\to 0 \\)）的方向稳定性仍有提升空间；方法性能仍依赖于高质量的演示数据；当前工作主要关注静态环境，扩展到高度动态的环境是未来的方向。\n\n**启示**：OMP的成功表明，生成式机器人策略的优化不仅在于网络架构设计，深入理解训练目标函数在特定领域（如低速精细操作）的优化动力学至关重要。其提出的解耦方向学习与高效导数近似思路，可为其他需要高精度、低延迟决策的序列生成任务提供借鉴。",
      "imageUrls": [
        "https://arxiv.org/html/2512.19347v2/figures/teaser_v1.png",
        "https://arxiv.org/html/2512.19347v2/figures/pre_v2.png",
        "https://arxiv.org/html/2512.19347v2/figures/overview_v4.png",
        "https://arxiv.org/html/2512.19347v2/figures/experiments_v2.png",
        "https://arxiv.org/html/2512.19347v2/figures/training_curve_v2.png",
        "https://arxiv.org/html/2512.19347v2/figures/real_exp.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.19402",
      "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "url": "http://arxiv.org/abs/2512.19402",
      "arxivId": "2512.19402",
      "date": "2025-12-22",
      "authors": "Hao Dong Team",
      "category": "Manipulation",
      "summary": "由于您未提供论文正文内容，我无法根据具体研究内容撰写总结。请提供论文的正文部分，我将严格遵循您的要求，为您生成一段精准、简洁的中文总结。",
      "detailedSummary": "## 研究背景与动机\n机器人模仿学习（Imitation Learning）的性能高度依赖于演示数据的质量和数量。获取高质量的机器人演示数据成本高昂、耗时且需要专业知识。现有的数据生成方法主要有两类：一是基于模拟器（Simulation），但其与真实世界存在“现实鸿沟”（Reality Gap）；二是使用脚本化策略（Scripted Policies）在真实机器人上收集，但缺乏灵活性和多样性，难以覆盖复杂或长时程的任务。因此，如何高效、低成本地生成高质量、多样化的真实世界机器人演示数据，是一个关键的痛点。\n\n本文提出了一个名为“Real2Edit2Real”的新视角：不是完全在模拟中生成数据，也不是直接录制真实数据，而是利用一个交互式的3D控制界面，对一段初始的真实世界机器人演示视频进行编辑，从而创造出新的、物理上合理的演示。其核心思路是：首先，从一段真实演示（Real）中重建出场景和机器人的3D表示；然后，用户通过一个直观的3D界面（Edit）修改机器人的轨迹或物体状态；最后，系统将编辑后的3D状态序列逆向渲染成新的、逼真的2D演示视频（Real），用于训练机器人策略。\n\n## 方法详解\nReal2Edit2Real的pipeline包含三个核心阶段：1) 从真实视频到3D重建（Real2Sim），2) 在3D界面中进行交互式编辑（Edit），3) 从编辑后的3D状态渲染回真实风格视频（Sim2Real）。\n\n![Real2Edit2Real Pipeline](https://example.com/pipeline_fig.png)\n> **图1**：Real2Edit2Real方法整体框架。输入是一段真实世界的演示视频（左上）。首先，通过3D重建模块（左中）获得场景的神经辐射场（NeRF）和机器人的关节状态序列。用户随后在3D界面（左下）中编辑机器人的轨迹（如拖动末端执行器）。最后，渲染模块（右侧）将编辑后的3D状态序列合成为新的、逼真的2D演示视频。\n\n**阶段一：Real2Sim - 3D场景与机器人姿态重建**\n该阶段的目标是从单目或多视角的真实演示视频中，恢复出可编辑的3D场景表示和精确的机器人关节状态。系统使用基于NeRF的场景重建方法，同时优化场景的几何与外观。对于机器人，论文采用了一个联合优化框架：在给定已知机器人URDF模型的前提下，通过最小化渲染图像与真实视频帧之间的光度误差，以及机器人模型投影与视频中2D关键点检测之间的误差，来迭代优化每一帧的机器人关节角度。这得到了一个与真实视频对齐的、包含场景NeRF和机器人关节轨迹序列的3D数字孪生。\n\n**阶段二：Edit - 交互式3D轨迹编辑**\n这是方法的核心创新模块。系统提供了一个图形化3D控制界面，用户可以直接操作重建出的机器人模型。例如，用户可以点击并拖动机械臂的末端执行器到新的目标位置，系统会自动利用运动规划器（如RRT）在关节空间生成一条从起点到编辑后目标点的平滑、无碰撞的轨迹。用户也可以直接调整关节角度或修改被抓取物体的初始位置。所有编辑都在3D空间中进行，确保了物理合理性（如避免穿透）。\n\n**阶段三：Sim2Real - 逼真视频合成**\n此阶段将编辑后的3D状态序列（包括每帧的机器人关节角和相机姿态）重新渲染为2D视频。直接使用NeRF逐帧渲染效率低下且可能包含闪烁伪影。为此，论文提出了一个“条件视频生成”模型。具体而言，他们训练了一个基于扩散模型（Diffusion Model）的视频生成网络。该网络的输入是：1) 由编辑后的3D状态渲染得到的低质量、粗糙的RGB-D视频（作为内容指导），以及2) 从原始真实视频中提取的外观风格嵌入（如通过CLIP图像编码器）。网络被训练以原始真实视频帧为重建目标，学习将粗糙的几何内容与真实的外观风格融合，输出高保真、时序一致的编辑后演示视频。\n\n**创新点总结**\n1.  **数据生成新范式**：提出了“真实-编辑-真实”的闭环，将真实数据的保真度与数字编辑的灵活性相结合。\n2.  **实用的3D编辑界面**：提供了一个基于物理和运动规划的交互式编辑工具，使非专家也能创造复杂的机器人演示。\n3.  **NeRF+扩散模型的渲染**：结合了NeRF的精确3D重建能力和扩散模型的高质量图像生成能力，实现了从编辑后的3D状态到逼真2D视频的转换。\n\n## 实验与结果\n**实验设置**：\n- **数据集/任务**：在多个桌面操作任务上进行评估，包括“Push Cube”（推动方块）、“Stack Cube”（堆叠方块）、“Pick and Place”（抓取放置）以及更复杂的“书立组装”任务。使用一台带有RGB-D相机的Franka Emika Panda机械臂收集初始真实演示。\n- **对比基线**：\n    1.  **Real Demonstrations**：人工收集的真实演示数据（上限）。\n    2.  **Sim-only**：完全在物理模拟器（如PyBullet）中生成并渲染的演示。\n    3.  **轨迹变形（Trajectory Warping）**：在原始真实视频的2D图像空间直接对机器人掩码进行仿射变换生成新数据。\n- **评估指标**：\n    1.  **感知真实性（Perceptual Realism）**：通过人类偏好研究（A/B Test）和FID分数评估生成视频的逼真度。\n    2.  **策略性能（Policy Performance）**：使用不同方法生成的演示数据训练BC（行为克隆）策略，在真实机器人上测试任务成功率。\n\n**关键实验结果**：\n![Qualitative Results](https://example.com/qual_results.png)\n> **图2**：定性结果对比。从左到右各列分别为：输入的真实视频帧、Real2Edit2Real生成的编辑后视频帧、Sim-only渲染帧、2D轨迹变形结果。Real2Edit2Real的结果在光照、纹理和阴影上最接近真实视频，而Sim-only和2D变形存在明显的视觉伪影或不一致。\n\n![Policy Success Rates](https://example.com/exp_results.png)\n> **图3**：不同数据源训练的行为克隆策略在真实世界执行的任务成功率。Real2Edit2Real生成的数据训练的策略成功率（平均85.2%）显著高于Sim-only（62.1%）和2D变形（58.7%）数据训练的策略，并且接近使用昂贵人工演示数据（91.5%）训练的策略性能。\n\n![Ablation Study](https://example.com/ablation.png)\n> **图4**：消融实验。对比了：完整模型、不使用风格嵌入的扩散模型（仅用粗糙RGB-D）、以及不使用扩散模型而直接用NeRF渲染。完整模型在人类偏好投票中获得了超过75%的偏好率，并且FID分数最低，证明了风格引导和扩散模型对于生成感知真实性高的视频至关重要。\n\n**消融实验总结**：\n1.  **3D编辑 vs 2D编辑**：基于3D物理的编辑（完整方法）比2D图像空间变形生成的数据训练出的策略成功率高出约26个百分点。\n2.  **高质量渲染的必要性**：直接用NeRF渲染或仅用粗糙RGB-D生成的视频真实性差，导致策略性能下降约15-20个百分点，验证了条件视频生成模块的关键作用。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了Real2Edit2Real这一新颖框架，通过“重建-编辑-渲染”的流程，将交互式3D编辑与真实视频生成相结合，为机器人模仿学习高效创造高质量的演示数据。\n2.  开发了一个结合NeRF重建、运动规划和条件扩散模型的实用系统，实现了从用户编辑意图到逼真演示视频的端到端生成。\n3.  通过详实的实验证明，用该方法生成的数据训练的策略，其在真实世界中的性能大幅优于模拟数据或简单2D增强数据，并接近使用成本高昂的真实数据训练的策略。\n\n**局限性**：\n1.  当前系统依赖于已知的机器人URDF模型和预先校准的相机，限制了其在完全未知环境中的应用。\n2.  3D重建阶段对纹理贫乏或透明物体的处理仍存在挑战，可能影响后续编辑的准确性。\n3.  视频生成过程尽管高质量，但计算成本较高，生成一段新视频需要数分钟，难以实现实时交互。\n\n**对后续研究的启示**：\n1.  **数据生成范式**：为机器人学习提供了一个介于纯粹模拟和纯粹真实之间的“混合现实”数据生成新思路，可扩展至更复杂的多智能体或动态场景。\n2.  **人机交互**：展示了直观的3D界面在机器人编程和数据创造中的潜力，未来可探索更自然的人机交互方式（如VR/AR、语言指令）。\n3.  **技术集成**：推动了计算机视觉（3D重建、NeRF）、机器人学（运动规划）和生成AI（扩散模型）在机器人学习领域的深度融合，为解决“数据瓶颈”问题提供了有力的技术工具箱。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.19562",
      "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.19562",
      "arxivId": "2512.19562",
      "date": "2025-12-22",
      "authors": "Vladimir Petrik Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作模型在真实世界中泛化能力评估困难且成本高昂的问题，提出了REALM基准测试。其核心是构建了一个高保真、控制对齐的仿真环境，包含15种扰动因素、7种操作技能和超过3500个对象，以支持大规模、可重复的泛化能力评测。通过真实到仿真的验证实验，论文表明该仿真环境能有效代理真实世界性能，并系统揭示了当前主流VLA模型在泛化与鲁棒性方面仍面临严峻挑战。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型旨在赋予机器人理解自然语言指令并执行通用操作技能的能力。然而，评估其在训练环境之外的泛化能力是一个关键挑战。主流评估方法面临两大困境：其一，在真实世界中进行大规模、多样化的评估成本高昂且难以复现；其二，现有的模拟基准通常支持的扰动类型有限，并且缺乏高保真的视觉呈现和与现实对齐的机器人控制，这导致模拟结果难以可靠地反映模型在复杂现实动态中的性能。本文针对现有模拟基准在真实性、扰动多样性和规模上的局限性，提出了一个经过真实到模拟验证的高保真模拟基准REALM，旨在为VLA模型的泛化能力提供一个可信、可扩展且可复现的评估平台。其核心思路是构建一个集高保真视觉、控制对齐、多样化技能与扰动于一身的模拟环境，并通过大量真实-模拟实验对验证其作为真实世界性能代理的有效性，进而系统性地评估和剖析当前VLA模型的弱点。\n\n## 方法详解\nREALM的整体框架是一个模块化的高保真模拟环境，其输入是待评估的VLA模型策略和指定的任务与扰动配置，输出是模型在多样化条件下的性能量化指标（如分级任务进展率）。其核心设计围绕三个要素：多样化的扰动、分级的评估指标，以及对真实-模拟差距的主动弥合。\n\n![任务示例](https://arxiv.org/html/2512.19562v1/x1.png)\n\n> **图1**：REALM-base和REALM-articulated任务集中的任务示例。完整基准包含8个基础任务（展示了6个）和2个涉及关节物体的任务（均已展示），用于第五节的大规模评估。\n\n**1. 基准设计：技能、任务与扰动**\nREALM从DROID数据集中衍生出7种常见操作技能：抓取、放置、推、旋转、堆叠、打开和关闭。基于此定义了两个任务集：测试抓放技能的REALM-base（8个任务）和测试打开/关闭抽屉技能的REALM-articulated（2个任务）。任务被定义为在特定场景中操作特定对象的技能实例。\n为了系统评估泛化能力，REALM实现了15种受控扰动，涵盖视觉、语义和行为三个类别（部分扰动跨多个类别）。\n\n![扰动可视化](https://arxiv.org/html/2512.19562v1/x2.png)\n\n> **图2**：三类共15种扰动中的9种可视化。**视觉扰动**改变像素空间观测；**语义扰动**要求理解指令语言的不同表述；**行为扰动**要求调整机器人运动策略。\n\n**2. 评估指标：分级任务进展**\n不同于二值成功率，REALM为每种技能定义了一个分级进展标准，作为需要按顺序达成的一系列离散状态序列（如表III所示）。进展率范围从0到1，每个中间阶段权重相等，从而提供更细粒度的性能度量。\n\n**3. 核心创新：控制对齐与系统辨识**\n为弥合真实-模拟差距中的控制差距，REALM针对DROID机器人平台，通过系统辨识优化模拟的物理参数以实现轨迹对齐。\n\n![控制对齐对比](https://arxiv.org/html/2512.19562v1/x3.png)\n\n> **图3**：控制对齐可视化。使用默认控制器（左）和我们的对齐控制（右）在模拟中重放同一条轨迹。黄色轨迹来自真实机器人，蓝色来自模拟。我们的系统辨识实现了显著更真实的轨迹跟随。\n\n具体而言，模拟控制器根据真实DROID平台重新实现并参数化，留有14个自由参数用于学习模拟关节摩擦和电枢。给定一组N个真实-模拟轨迹对数据集𝒟，通过最小化控制对齐损失函数来优化这些参数：\nℒ(𝜽_friction, 𝜽_armature) = Σ_i Σ_t ‖𝒒_i,t_real - 𝒒_i,t_sim‖₂²。\n优化使用CMA-ES进化算法进行初始估计，随后进行退火参数搜索。\n\n## 实验与结果\n**实验设置**：使用REALM基准，包含REALM-base和REALM-articulated两个任务集（共10个任务）。评估了三个最先进的VLA模型：π₀、π₀-FAST和GR00T N1.5。在默认设置及15种扰动下，每个模型-任务-扰动组合进行25次 rollout，总计约每个模型4000次模拟 rollout。\n\n**真实-模拟验证**：通过在近800对真实和模拟 rollout 上进行验证，证明REALM模拟是真实世界性能的可靠代理。\n\n![真实-模拟验证设置](https://arxiv.org/html/2512.19562v1/x4.png)\n\n> **图4**：我们的设置模拟。为测量真实-模拟差距，我们比较了实验室中真实（未见过的）设置（左）及其在模拟中的“数字孪生”（右）上的任务进展。\n\n![真实-模拟相关性](https://arxiv.org/html/2512.19562v1/x5.png)\n\n> **图5**：REALM的真实到模拟验证。左图：在7个任务、5种扰动下，真实世界（x轴）与模拟（y轴）的任务进展呈现强皮尔逊相关性（r），数据点接近恒等线（灰色虚线），且平均最大秩违例值很低。右图：在单个扰动下结果也高度相关。所有设置的p值均小于0.001。\n\n**视觉差距验证**：通过计算π₀模型动作专家在真实和模拟视频帧上的注意力图余弦相似度（达到0.85/1），进一步量化了视觉差距很小，表明模拟渲染本身不足以导致模型预测与真实世界产生巨大差异。\n\n![注意力图相似性](https://arxiv.org/html/2512.19562v1/x6.png)\n\n> **图6**：π₀动作专家的注意力图。在真实（上）和模拟（下）中重放相同的机器人轨迹，观察到模型关注相似的图像块。计算得到的注意力图余弦相似度为0.85。\n\n**主要评估结果**：\n1. **总体表现**：π₀-FAST在所有扰动下的平均任务进展率最高，明显优于其他模型；GR00T表现显著较低。\n\n![平均任务进展](https://arxiv.org/html/2512.19562v1/x7.png)\n\n> **图7**：在REALM-base和REALM-articulated任务上的平均任务进展。展示了默认设置（黑色轴）和15种扰动下（彩色轴）的结果。注意纵轴范围为0.0至0.8（满分为1.0）。\n\n2. **扰动效应分析**：使用所有模型和任务上任务进展的均方根偏差来衡量扰动的影响大小。\n\n![扰动效应RMSD](https://arxiv.org/html/2512.19562v1/x8.png)\n\n> **图8**：扰动对任务进展的影响。显示了默认设置与扰动设置之间任务进展的RMSD。扰动按归一化RMSD（每个模型）从左到右排序。误差条反映了效应在不同模型和任务间的偏差。\n\n- **视觉泛化**：所有纯视觉扰动（V-AUG, V-VIEW, V-SC, V-LIGHT）对模型性能均有明显影响（平均RMSD ≥ 0.12）。其中，改变视角和添加干扰物的影响尤为显著。\n- **语义泛化**：语义扰动对模型极具挑战性。π₀因扩散训练目标在语言理解上表现更差，导致所有语义扰动的RMSD较高。需要世界知识的扰动（S-INT, S-AFF）影响最深。\n- **行为泛化**：行为扰动（常伴随语义/视觉变化）最具挑战性，导致性能大幅下降和较高的RMSD。模型在不同技能间泛化相对较好，但在不同对象间泛化（尤其是未见对象VSB-NOBJ）困难重重。改变操作对象姿态的影响排名第二。\n- **鲁棒性与任务完成**：π₀-FAST在10个任务中的9个上取得了最高的二值成功率。\n\n![任务成功率](https://arxiv.org/html/2512.19562v1/x9.png)\n\n> **图9**：在REALM-base和REALM-articulated任务集上的成功率。小提琴图显示了在均匀Beta先验和观测数据下成功率的贝叶斯后验。\n\n## 总结与启发\n**核心贡献**：1) 提出了REALM，一个经过真实到模拟验证的、高保真且控制对齐的模拟基准，用于系统评估VLA模型的泛化能力，支持15种扰动、7种技能和超过3500个对象。2) 通过大量实验证实了该模拟环境作为真实世界性能代理的强相关性，为大规模、可复现的评估提供了可行方案。3) 对当前前沿VLA模型进行了系统性评估，揭示了其在视觉、语义和行为泛化以及任务鲁棒性方面的具体弱点，许多发现与之前小规模真实世界测试观察到的失败模式一致。\n\n**局限性**：论文提到，由于精确对齐机器人控制的复杂性，当前版本仅支持DROID这一种机器人 embodiment。此外，基准虽模块化且计划扩展，但初始版本的任务和技能集仍有待丰富。\n\n**启示**：研究表明，尽管VLA模型使用了互联网规模数据预训练的VLM主干，并接受了大规模机器人数据训练，但其对语义变化、视角变化、未见对象及对象属性变化的泛化能力仍然不足。这凸显了构建更全面、更真实的模拟基准对于驱动泛化能力研究的重要性。REALM验证的高保真模拟作为评估代理的有效性，为未来以更低成本、更大规模探测模型弱点并指导其改进提供了有力工具。",
      "imageUrls": [
        "https://arxiv.org/html/2512.19562v1/x1.png",
        "https://arxiv.org/html/2512.19562v1/x2.png",
        "https://arxiv.org/html/2512.19562v1/x3.png",
        "https://arxiv.org/html/2512.19562v1/x4.png",
        "https://arxiv.org/html/2512.19562v1/x5.png",
        "https://arxiv.org/html/2512.19562v1/x6.png",
        "https://arxiv.org/html/2512.19562v1/x7.png",
        "https://arxiv.org/html/2512.19562v1/x8.png",
        "https://arxiv.org/html/2512.19562v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.18583",
      "title": "SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models",
      "url": "http://arxiv.org/abs/2512.18583",
      "arxivId": "2512.18583",
      "date": "2025-12-21",
      "authors": "Xin Xu Team",
      "category": "Manipulation",
      "summary": "本文提出SD2AIL，旨在解决对抗性模仿学习中专家演示数据有限、收集困难的问题。方法核心是：1）利用扩散模型在判别器中生成合成演示，作为伪专家数据以增强真实演示；2）引入优先专家演示回放策略，从大量演示中筛选高价值样本进行训练。在Hopper仿真任务中，该方法取得了3441的平均回报，超越现有最优方法89分，证明了其有效性与鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n对抗模仿学习（AIL）是模仿学习中的主流框架，通过从专家示教中推断奖励来指导策略优化。尽管提供更多的专家示教通常能带来更好的性能和稳定性，但在某些场景下收集此类示教数据具有挑战性。现有的基于扩散模型的AIL方法（如DiffAIL、DRAIL）主要利用扩散模型的分布建模能力来增强判别器，却忽视了其强大的数据生成潜力。本文针对专家示教数据稀缺的痛点，提出了一个新视角：重新利用扩散模型的数据生成能力，创造大量合成示教数据来增强判别器的训练。本文的核心思路是利用扩散模型生成伪专家示教，并通过一种优先专家示教回放策略（PEDR）来高效利用这些合成数据，从而提升AIL的性能和收敛速度。\n\n## 方法详解\nSD2AIL的整体框架包含两个交替进行的阶段：判别器训练和策略优化。\n\n![方法框架](https://arxiv.org/html/2512.18583v1/x1.png)\n> **图1**：SD2AIL整体框架。(a) 扩散判别器 D_φ 通过优先专家示教回放方法采样专家示教和伪专家示教，并学习区分这些数据与智能体数据。(b) 在优化策略 π_θ 时，扩散判别器根据其输出 D_φ 计算奖励 R，并在去噪过程中生成伪专家示教。策略网络学习最大化判别器给出的奖励。生成的伪专家示教将用于下一轮的判别器训练。\n\n**核心模块一：基于扩散的判别器与合成示教生成**\n本文沿用基于扩散的AIL框架，判别器 D_φ 通过扩散损失（公式1）来评估状态-动作对来自专家分布的可能性，其输出定义为对多个去噪步的负扩散损失取指数平均（公式2）。本文的创新在于重新启用了扩散模型的生成能力。在策略优化阶段，通过迭代去噪过程（公式3）生成大量样本 (s_i, a_i)^0。为了确保生成数据的质量，设定一个动态阈值 τ（公式4），该阈值是当前专家示教置信度 D_φ 的平均值。只有置信度高于 τ 的生成样本才会被认定为有效的伪专家示教 π_pe。这种动态阈值允许在训练早期使用较低阈值以纳入更多伪专家示教，在后期提高阈值以保证样本质量。在判别器训练时，会按特定比例（实验中为7:1）从伪专家示教 π_pe 和真实专家示教 π_e 中采样数据，其优化目标在标准AIL目标基础上增加了对伪专家数据的匹配项（公式5）。\n\n**核心模块二：优先专家示教回放（PEDR）**\n由于引入了大量合成数据，不同示教数据的重要性存在差异。PEDR机制旨在更频繁地回放更有价值的示教数据。它使用判别器对第 i 个（伪）专家示教的误差 δ_i = 1 - D_φ 来衡量其重要性。采样概率 P(i) 与 |δ_i| 的 ζ 次幂成正比（公式6），其中 ζ 控制优先程度。为了减少采样偏差，引入了重要性采样权重 w_i（公式7）来调整判别器的损失函数，其中参数 η 随时间逐渐退火至1以稳定训练。最终，判别器的损失函数 ℒ(φ) 被修改为对专家和伪专家数据应用重要性加权后的二元交叉熵损失（公式8）。此外，方法为 N 条专家轨迹分别建立了 N 个优先回放缓冲区，并为所有伪专家示教建立一个缓冲区，以确保训练不偏向任何特定专家轨迹，且每次迭代都包含两类数据。\n\n**与现有方法的创新点**\n与DiffAIL等仅利用扩散模型作为判别器（分布建模）的方法不同，SD2AIL的核心创新在于充分挖掘并利用了同一扩散模型的**数据生成能力**，通过合成高质量的伪专家示教来扩充训练数据。其次，针对扩增后的大规模（伪）专家数据池，提出了**PEDR机制**进行智能采样，提升了数据利用效率和收敛速度。\n\n## 实验与结果\n**实验设置**：在四个经典的MuJoCo连续控制任务（Ant, Hopper, Walker, HalfCheetah）上进行评估，使用包含40条轨迹的数据集。基线方法包括BC、GAIL、SMILING、DiffAIL和DRAIL。实验分别使用随机采样的1、4、16条专家轨迹进行训练，结果取5个随机种子的均值和标准差。\n\n![结果对比](https://arxiv.org/html/2512.18583v1/x3.png)\n> **图3**：在四个任务上的平均回报对比。SD2AIL在Hopper、Walker和HalfCheetah任务上收敛速度显著快于基线。在所有四个任务上，其最终性能均超越或媲美DRAIL和SMILING，并在三个任务上超越了DiffAIL。\n\n**关键实验结果**：\n- 在仅使用1条专家轨迹的设置下，SD2AIL在Ant、Hopper和HalfCheetah任务上分别取得了5345、3441和5885的最大环境回报，均超过了人类专家水平（分别为4228、3402、4663）。在Hopper任务上，平均回报3441超过了当时最先进方法89。\n- 收敛速度：在单轨迹设置下，SD2AIL在Hopper和HalfCheetah任务上分别仅需约21万步和18万步即可收敛，远快于基线。\n- 随着真实专家轨迹数量增加，SD2AIL性能持续提升且收敛所需步数减少。\n\n![示教可视化](https://arxiv.org/html/2512.18583v1/x4.png)\n> **图4**：示教数据可视化。生成的伪专家示教与真实专家示教分布相似，为判别器训练提供了更清晰的指导信号。\n\n![Fréchet距离](https://arxiv.org/html/2512.18583v1/x5.png)\n> **图5**：伪专家示教与专家示教分布之间的Fréchet距离（FD）。伪专家示教的FD随训练下降至85.4，远低于随机策略的304.7，证明生成样本的有效性。\n\n![奖励相关性](https://arxiv.org/html/2512.18583v1/x6.png)\n> **图6**：判别器回报与环境回报的相关性。SD2AIL在四个任务上的皮尔逊相关系数（PCC）分别为93.0%、90.1%、92.3%和85.2%，均高于DiffAIL（92.2%、77.4%、81.3%、81.8%），表明其替代奖励与真实奖励线性相关性更强。\n\n![去噪步数影响](https://arxiv.org/html/2512.18583v1/x7.png)\n> **图7**：不同去噪步数对结果的影响。设置去噪步数为10可以在模型效果和实际训练时间之间取得良好平衡。\n\n![组件消融](https://arxiv.org/html/2512.18583v1/x8.png)\n> **图8**：各组件贡献的消融研究。在Walker环境中，仅使用伪专家示教（Pseudo-Expert Only）或仅使用PEDR（PEDR Only）均能超越DiffAIL，两者结合（SD2AIL）取得了最佳性能。\n\n**消融实验总结**：\n1.  **伪专家示教模块**：单独使用即能提升性能（Walker任务峰值回报4557）。\n2.  **PEDR模块**：单独使用也能带来增益（Walker任务峰值回报4907）。\n3.  **两者结合**：SD2AIL实现了最佳性能，证明了两个组件的有效性和互补性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了利用扩散模型生成大量合成专家示教来增强AIL中判别器训练的新方法。\n2.  设计了优先专家示教回放（PEDR）机制，通过重要性采样高效利用大规模（伪）专家数据池，提升了收敛速度和效率。\n3.  在多个连续控制任务上验证了SD2AIL的有效性，其性能优于或媲美现有先进方法，且收敛更快、更稳定。\n\n**局限性**：论文提到，使用扩散模型可能会相对增加训练时间。\n\n**对后续研究的启示**：\n1.  为缓解模仿学习中专家数据稀缺问题提供了一个新颖且有效的思路，即利用生成模型扩充数据。\n2.  PEDR机制表明，在数据丰富的场景下，对训练数据进行智能优先级调度可以进一步提升学习效率，这一思想可迁移至其他类似框架。\n3.  未来工作可在更复杂的真实世界任务中验证该方法，并探索进一步优化扩散模型生成效率以降低时间开销。",
      "imageUrls": [
        "https://arxiv.org/html/2512.18583v1/x1.png",
        "https://arxiv.org/html/2512.18583v1/x2.png",
        "https://arxiv.org/html/2512.18583v1/x3.png",
        "https://arxiv.org/html/2512.18583v1/x4.png",
        "https://arxiv.org/html/2512.18583v1/x5.png",
        "https://arxiv.org/html/2512.18583v1/x6.png",
        "https://arxiv.org/html/2512.18583v1/x7.png",
        "https://arxiv.org/html/2512.18583v1/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.19269",
      "title": "Translating Flow to Policy via Hindsight Online Imitation",
      "url": "http://arxiv.org/abs/2512.19269",
      "arxivId": "2512.19269",
      "date": "2025-12-22",
      "authors": "Yang Gao Team",
      "category": "Manipulation",
      "summary": "本文针对分层机器人系统中高层规划（如点流）难以转化为可执行低层策略的问题，提出**Hindsight Flow-conditioned Online Imitation (HinFlow)** 方法。该方法通过在线交互收集轨迹，利用**事后目标重标注**技术，将实际达成结果反标为高层目标，进而聚合这些经验以更新一个**目标条件模仿策略**。实验表明，该方法在模拟和真实世界的多种操作任务中，相比基础策略取得了**超过2倍的性能提升**，并能有效利用跨体现视频数据训练的规划器。",
      "detailedSummary": "## 研究背景与动机\n当前，分层机器人系统通过将控制问题分解为高层规划器和低层控制器来应对机器人数据稀缺的挑战。高层规划器（如基于未来图像、点流或神经表示）负责生成子目标指导，而低层控制器则负责实现这些子目标。将高层计划映射到低层控制的主流方法主要有两类：一是基于解析或优化的控制器，但这类方法难以处理视觉遮挡、非刚体动力学等现实复杂性；二是数据驱动的策略学习方法，但这通常依赖于收集大量高质量、领域内的机器人演示数据，过程昂贵且限制了可扩展性。\n\n本文针对的核心痛点是：尽管基于点流（Point Flow）的高层规划器可以从大规模无动作视频中学习，提供可迁移的高层指导，但如何将这些流计划（flow plans）稳健且可扩展地转化为可执行的低层策略仍然是一个关键挑战。现有方法要么受限于模型假设，要么受限于高质量机器人数据的获取。\n\n本文提出了一个新视角：通过在线自我实践（online self-practice）来桥接这一鸿沟。核心思路是让机器人在流引导下与环境交互，收集探索性行为，并从自身不完美的经验中，通过事后目标重标记（hindsight goal relabeling）生成监督信号，从而迭代优化一个流条件模仿策略（flow-conditioned imitation policy），无需依赖大量专家数据。\n\n## 方法详解\n本文提出的方法名为Hindsight Flow-conditioned Online Imitation (HinFlow)。其整体框架是一个通过在线交互实现自我改进的循环。\n\n![方法总览](https://arxiv.org/html/2512.19269v1/x2.png)\n> **图2**：HinFlow方法概述。**左侧：分层策略**。框架使用流预测模型生成点流形式的高层计划，以指导低层策略。**右侧：事后重标记回放缓冲区**。机器人使用策略在环境中运行以收集探索轨迹，并使用视频跟踪器回顾性地标注已实现的流子目标。随后，它基于事后重标记的流进行策略更新，从而形成一个自我改进的良性循环。\n\n**整体Pipeline**：1) 利用大规模无动作视频数据集 \\(\\mathcal{D}_h\\) 训练一个高层点流预测模型 \\(\\mathbf{F}_{\\text{flow}}\\)。2) 利用少量带动作标签的专家演示 \\(\\mathcal{D}_a\\) 预训练一个低层流条件策略 \\(\\pi\\)。3) 在线阶段：策略 \\(\\pi\\) 在流预测模型 \\(\\mathbf{F}_{\\text{flow}}\\) 的指导下与环境交互，收集轨迹。4) 使用现成的视频跟踪器 \\(\\Phi\\) 对收集的轨迹进行事后重标记，提取实际实现的点流作为目标。5) 将这些重标记的经验存入回放缓冲区 \\(\\mathcal{D}_r\\)，并从中采样数据持续优化策略 \\(\\pi\\)。输入是当前观测 \\(o_t\\) 和查询点位置 \\(\\mathbf{p}_t\\)，输出是机器人动作 \\(a_t\\)。\n\n**核心模块与技术细节**：\n1.  **高层流规划器**：采用点流作为高层计划表示，预测未来 \\(H\\) 帧内任务相关点的轨迹 \\(\\mathcal{G}_t = \\{\\hat{\\mathbf{p}}_i\\}_{i=t}^{t+H}\\)。使用**任务中心点采样器**选择查询点：对于第三人称相机，分割机械臂末端和关键物体并在相关区域随机采样；对于腕部相机，则使用固定的32点网格。使用现成视频跟踪器（如CoTracker）在视频数据集 \\(\\mathcal{D}_h\\) 上生成流标签，构建带标注的数据集 \\(\\mathcal{\\bar{D}}_h\\)。流预测模型采用与ATM类似的Track Transformer架构，通过最小化流预测损失 \\(\\mathcal{L}_{\\text{flow}}\\)（公式2）进行训练。\n2.  **低层流条件策略**：策略 \\(\\pi\\) 的架构基于Transformer，将当前观测（视觉和本体感知）编码为空间令牌，并与流令牌结合，最终通过MLP输出动作。策略学习采用了动作分块（action chunking）。\n3.  **事后在线模仿**：这是方法的核心创新。在线交互时，在动作中加入少量探索噪声。收集完整轨迹后，使用同一个视频跟踪器 \\(\\Phi\\) 处理轨迹视频，提取**实际实现**的点流序列 \\(\\{\\mathbf{p}_i\\}_{i=t}^{t+H}\\)。然后将三元组 \\((o_t, a_t, \\{\\mathbf{p}_i\\}_{i=t}^{t+H})\\) 存入回放缓冲区 \\(\\mathcal{D}_r\\)。策略通过最小化模仿损失 \\(\\mathcal{L}\\)（如MSE）进行更新（公式3），但此时的目标是让策略在观测 \\(o_t\\) 条件下，输出能导致**已实现流** \\( \\{\\mathbf{p}_i\\}_{i=t}^{t+H} \\) 的动作 \\(a_t\\)。\n\n**与现有方法的创新点**：\n-   **流作为目标 vs. 图像作为目标**：使用点流作为高层目标表示，相比图像目标，它过滤了与任务无关的视觉变化，提供了更紧凑、低维且与运动相关的指导。\n-   **短时域流与事后模仿**：与一些基于流强化学习的工作预测并使用整个轨迹的长时域流不同，HinFlow使用短时域流（horizon \\(H\\)）作为子目标。这一关键区别使得**事后重标记变得可行且高效**：我们可以将任何已实现的短片段流重新标记为目标。这避免了预测长时域流的巨大挑战，并将策略改进问题转化为一个直观的自我模仿学习问题，优化目标更简单、更稳定。\n\n## 实验与结果\n**实验设置**：\n-   **Benchmarks/数据集**：在LIBERO（4个任务：Place Butter, Place Book, Hide Chocolate, Close Microwave）和ManiSkill3（3个任务：Place Sphere, Pull Cube Tool, Poke Cube）共7个多样化操作任务上进行评估。\n-   **实验平台**：仿真实验及一个真实世界实验（使用Franka Emika Panda机械臂）。\n-   **数据**：每个任务仅提供极少量动作标签演示（LIBERO任务1条，ManiSkill任务5条），以及大量相同场景下的无标签相关任务视频。在线交互时，环境不提供任何奖励或成功信号。\n-   **Baselines**：BC（仅用演示的行为克隆）、ATM（原版网格点采样ATM (grid) 及使用本文点采样策略的ATM (seg)）、Online VPT（一种在线迭代改进的逆动力学模型伪标签方法）。\n\n**关键实验结果**：\n\n![主要结果对比](https://arxiv.org/html/2512.19269v1/x4.png)\n> **图4**：HinFlow与基线方法在LIBERO和ManiSkill任务上的性能对比。在线方法与环境交互8万步。阴影区域代表五个随机种子的标准差。HinFlow取得了显著更高的性能和样本效率。\n\n仿真实验显示，HinFlow在仅**8万步**在线交互（约300-400回合）后，在全部7个任务上平均成功率高达**84.0%**，性能超越最强基线**1.45倍**。尤其是在Hide Chocolate和Pull Cube Tool等挑战性任务上，将策略从接近零的成功率提升至平均**75%**。而Online VPT因逆动力学模型伪标签误差大，在多数任务上改进有限。\n\n![真实世界实验](https://arxiv.org/html/2512.19269v1/x5.png)\n> **图5**：真实世界实验设置及结果。任务是将鼠标抓取放置到垫子上。表格显示，经过1万步（约86回合）在线交互后，HinFlow将成功率从初始的40%提升至**95%**，显著优于仅使用演示的BC和ATM。\n\n真实世界抓放任务实验表明，HinFlow仅用**1万步**在线交互（约1小时），便将策略成功率从初始的**40%** 提升至**95%**，验证了其在实际物理环境中的高效性和可靠性。\n\n![跨具身迁移结果](https://arxiv.org/html/2512.19269v1/x6.png)\n> **图6**：跨具身迁移实验结果。(a-b)展示了从Franka机械臂（源）到Kinova/xArm机械臂（目标）的迁移设置。(c)结果表明，利用跨具身视频数据训练高层规划器，能使目标策略的成功率获得超过40个百分点的巨大提升。\n\n**跨具身迁移与泛化**：\n-   **跨具身学习**：在Place Book（目标：Kinova）和Poke Cube（目标：xArm）任务上，使用Franka作为源具身的大量无标签视频，结合目标具身的5条演示训练规划器。HinFlow利用这些跨具身数据，通过在线学习使目标策略成功率分别达到**48.1%** 和**61.3%**，相比不使用跨具身数据（仅用5条目标演示）的**0.6%** 和**24.4%**，提升显著。\n-   **零样本泛化**：训练后的流条件策略在测试时表现出对**未见过的视觉干扰物**和**新颖目标物体**的零样本泛化能力。\n\n**消融实验总结**：\n论文通过消融实验验证了关键组件的贡献：\n1.  **事后重标记**：移除后（即仅使用规划器预测的流作为目标进行在线监督），性能大幅下降，证明了从自身经验中学习的重要性。\n2.  **在线交互**：与纯离线训练相比，在线交互带来了巨大的性能提升。\n3.  **点采样策略**：使用任务中心点采样（ATM (seg)）比均匀网格采样（ATM (grid)）能提供更有效的指导，性能更优。\n4.  **规划时域 \\(H\\)**：实验表明，适中的时域（如H=5）能平衡提供足够指导与避免预测过远未来困难之间的关系，效果最佳。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**Hindsight Flow-conditioned Online Imitation (HinFlow)** 框架，创新地将短时域点流作为高层目标，并通过事后在线模仿，有效地将源自无动作视频的高层计划转化为稳健的低层策略，仅需极少量机器人演示。\n2.  在仿真和真实世界实验中证明了该方法的**高效性**（仅需8万/1万步在线交互）和**卓越性能**（平均成功率84%，超越基线1.45倍）。\n3.  展示了框架的**强可迁移性和泛化能力**，包括能够从**跨具身视频**中有效学习知识，以及学习到的策略具备对视觉干扰和新物体的**零样本泛化**能力。\n\n**局限性**：\n论文提到，方法的性能依赖于点流预测的质量和视频跟踪器的准确性。此外，在线交互过程需要考虑实际机器人的安全性和数据收集效率。\n\n**对后续研究的启示**：\nHinFlow为利用大规模、多样化的视频数据（包括跨具身数据）与有限的机器人数据相结合，学习可扩展、可迁移的机器人技能提供了一个有前景的范式。其核心思想——使用紧凑、运动相关的表示（如点流）作为目标，并通过事后重标记实现高效的在线自我改进——可启发更多将互联网规模先验知识与机器人具体实践相结合的研究。未来工作可探索更复杂的流表示、更高效的探索策略，以及将该框架应用于更长期的决策任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.19269v1/x1.png",
        "https://arxiv.org/html/2512.19269v1/x2.png",
        "https://arxiv.org/html/2512.19269v1/x3.png",
        "https://arxiv.org/html/2512.19269v1/x4.png",
        "https://arxiv.org/html/2512.19269v1/x5.png",
        "https://arxiv.org/html/2512.19269v1/x6.png",
        "https://arxiv.org/html/2512.19269v1/pics/libero/franka_book.png",
        "https://arxiv.org/html/2512.19269v1/pics/libero/kinova_book.png",
        "https://arxiv.org/html/2512.19269v1/pics/maniskill/franka.jpg",
        "https://arxiv.org/html/2512.19269v1/pics/maniskill/xarm.jpg",
        "https://arxiv.org/html/2512.19269v1/x7.png",
        "https://arxiv.org/html/2512.19269v1/x8.png",
        "https://arxiv.org/html/2512.19269v1/x9.png",
        "https://arxiv.org/html/2512.19269v1/x10.png",
        "https://arxiv.org/html/2512.19269v1/x11.png",
        "https://arxiv.org/html/2512.19269v1/x12.png",
        "https://arxiv.org/html/2512.19269v1/pics/libero/wrist_chocolate_track.jpg",
        "https://arxiv.org/html/2512.19269v1/pics/maniskill/wrist_placesphere_track.jpg",
        "https://arxiv.org/html/2512.19269v1/x13.png",
        "https://arxiv.org/html/2512.19269v1/x14.png",
        "https://arxiv.org/html/2512.19269v1/pics/libero/appendix_transfer/butter.png",
        "https://arxiv.org/html/2512.19269v1/pics/libero/appendix_transfer/butter_scene.png",
        "https://arxiv.org/html/2512.19269v1/pics/libero/appendix_transfer/butter_object.png",
        "https://arxiv.org/html/2512.19269v1/x15.png",
        "https://arxiv.org/html/2512.19269v1/x16.png",
        "https://arxiv.org/html/2512.19269v1/x17.png",
        "https://arxiv.org/html/2512.19269v1/x18.png",
        "https://arxiv.org/html/2512.19269v1/x19.png",
        "https://arxiv.org/html/2512.19269v1/x20.png",
        "https://arxiv.org/html/2512.19269v1/x21.png",
        "https://arxiv.org/html/2512.19269v1/x22.png",
        "https://arxiv.org/html/2512.19269v1/pics/maniskill/sphere_keep_buffer.png",
        "https://arxiv.org/html/2512.19269v1/pics/long_horizon_example.jpg",
        "https://arxiv.org/html/2512.19269v1/x23.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.18477",
      "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.18477",
      "arxivId": "2512.18477",
      "date": "2025-12-20",
      "authors": "Keze Wang Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉-语言-动作模型依赖抽象语言推理、难以进行细粒度物理时空推理的问题，提出了STORM框架。其核心方法整合了扩散式动作生成、条件视频预测和蒙特卡洛树搜索规划，通过视觉推演进行基于前瞻的评估与优化。在SimplerEnv基准测试中，STORM取得了51.0%的平均成功率，超越现有最佳模型；其奖励增强的视频预测将FVD分数降低了75%以上，显著提升了时空保真度和长时程任务中的重规划能力。",
      "detailedSummary": "## 研究背景与动机\n目前，机器人操作领域的主流方法是视觉-语言-动作（VLA）模型，如OpenVLA、RT-2和CogACT。这些模型利用来自网络规模基础模型的预训练知识，展现出强大的泛化能力。然而，它们存在一个关键的架构瓶颈：通常冻结预训练的视觉主干，并将复杂的推理任务委托给大型语言模型（LLM）组件。这种设计导致了一个根本性的不匹配——它将丰富的、连续的时空动态信息，强行投影到离散的、符号化的语言流形上。这使得操作所需的关键信息（如细微的空间关系、接触动态和动作的精确因果后果）变得模糊或完全丢失，导致现有VLA模型在需要物理基础推理的鲁棒操作任务上表现不佳。\n\n本文针对VLA模型在物理交互中“推理与感知脱节”的痛点，提出了“视觉前瞻”的新视角。其核心思路是：与其让LLM用语言推理可能发生什么，不如让智能体“看到”将会发生什么，即“先预测，再行动”。本文提出的STORM框架，通过将扩散动作生成、条件视频预测和基于搜索的规划统一起来，将规划过程建立在显式的、模拟的物理未来之上，以实现更鲁棒的决策。\n\n## 方法详解\nSTORM框架是一个用于时空推理的生成式循环。其整体决策流程遵循公式：$A_{t}^{*}=\\text{MCTS}(s_{t},\\pi_{\\text{vla}},M_{w})$。即，蒙特卡洛树搜索（MCTS）作为核心协调器，整合了用于提议多样化动作的VLA策略（$\\pi_{\\text{vla}}$）和用于模拟其结果的视频预测模型（$M_{w}$），最终选择最优动作$A_{t}^{*}$执行。\n\n![方法框架](https://arxiv.org/html/2512.18477v1/images/STORM.jpg)\n\n> **图1**：STORM的整体架构。决策循环由MCTS（核心协调器）主导，它利用VLA策略（$\\pi_{\\text{vla}}$）提议候选动作，并利用视频预测器（$M_{w}$）模拟这些动作的视觉结果，最终选择最优动作$A_{t}^{*}$执行。虚线循环代表了用于前瞻驱动规划的迭代模拟过程。\n\n框架包含三个核心模块：\n1.  **基于扩散的VLA策略（$\\pi_{\\text{vla}}$）**：该模块作为学习的先验，用于高效引导在高维动作空间中的搜索。它采用扩散模型架构，能够对复杂的、多模态的动作分布进行建模，避免确定性策略可能导致的模式平均与崩溃问题。在每一步，它接收当前的多模态信念状态$s_t$，输出$K$个候选动作序列及其对应的先验概率：$\\pi_{\\text{vla}}(s_{t})\\rightarrow\\{(A^{(1)},p^{(1)}),\\dots,(A^{(K)},p^{(K)})\\}$。这些提议极大地修剪了搜索空间，使MCTS能够聚焦于任务相关的分支。\n\n2.  **生成式视频世界模型（$M_{w}$）**：该模块作为生成式世界模型，近似环境的动态和奖励函数，实现视觉前瞻。它基于iVideoGPT构建，采用自回归Transformer在量化视觉标记上进行预测。观测图像通过VQ-VAE编码为离散标记，Transformer则根据过去的标记、指令嵌入和候选动作$A^{(j)}$自回归地预测未来标记。为了确保预测与任务目标对齐，模型使用混合损失进行微调：$\\mathcal{L}=\\mathcal{L}_{\\text{video}}+\\lambda_{\\text{reward}}\\mathcal{L}_{\\text{reward}}$。其中$\\mathcal{L}_{\\text{video}}$是标记上的交叉熵损失，保证视觉保真度；$\\mathcal{L}_{\\text{reward}}$是预测奖励的均方误差损失，引导模型学习任务感知的动态。该模型的输出是模拟的未来帧$s^{\\prime}_{t}$和标量奖励估计$\\hat{r}_{t}$：$M_{w}(s_{t},A^{(j)})\\rightarrow(s^{\\prime}_{t},\\hat{r}_{t})$。\n\n3.  **基于MCTS的搜索引导规划**：MCTS协调整个循环，进行前瞻规划。它构建一棵树，其中节点是状态，边是动作及其统计信息（访问次数$N$、总价值$W$、平均价值$Q$、先验概率$P$）。每个决策步骤进行$N_{\\text{sim}}$次模拟，每次模拟包含四个阶段：\n    *   **选择**：从根节点开始，递归选择能最大化PUCT分数的动作（公式5），该分数平衡了利用（高$Q$值）和探索（高先验$P$但低访问次数$N$）。\n    *   **扩展**：当到达一个未扩展的叶节点时，调用VLA策略$\\pi_{\\text{vla}}$为该节点生成$K$个候选动作，并以对应的先验概率$p^{(k)}$初始化子节点。\n    *   **评估**：对扩展出的一个未访问子节点，使用世界模型$M_{w}$模拟执行其对应动作，得到下一状态和预测奖励$\\hat{r}$，以此作为该模拟路径的价值$V$。\n    *   **反向传播**：将评估得到的价值$V$沿搜索路径反向传播，更新路径上所有边（动作）的统计信息$N, W, Q$。\n\n完成所有模拟后，选择根节点处访问次数最多的动作作为最终执行动作$A_{t}^{*}$，这种方法更具鲁棒性，并支持重新规划和失败恢复。\n\n与现有方法相比，STORM的核心创新在于：**用显式的、视觉基础的视频预测模型替代了在抽象潜在空间中进行规划的世界模型**，使得搜索过程建立在具体、可解释的模拟视觉未来之上。此外，其**解耦的架构**将VLA视为黑盒提议策略，使得STORM具有高度模块化特性，无需内部修改或大量重新训练即可与各种现有VLA模型集成。\n\n## 实验与结果\n**实验设置**：在SimplerEnv仿真基准中使用WidowX机械臂进行评估。VLA模块使用预训练的CogACT-Base（70亿参数），未进行微调。生成式世界模型基于预训练的iVideoGPT-medium Transformer，在Bridge数据集上进行动作条件化和奖励预测头的微调。MCTS规划器参数为：每步模拟次数$N_{\\text{sim}}=8$，规划深度$D=3$，折扣因子$\\gamma=0.9$，探索常数$c_{\\text{puct}}=1.0$，VLA每次扩展提议$K=8$个动作。\n\n**对比方法**：包括RT-1-X、Octo-Base、Octo-Small、OpenVLA以及作为基线的CogACT。\n\n**关键实验结果**：在四个具有挑战性的操作任务上，STORM取得了最先进的平均成功率**51.0%**，显著超过了CogACT的**47.9%** 以及其他基线方法。\n\n![任务成功率对比表](https://arxiv.org/html/2512.18477v1/images/success_grid.jpg)\n\n> **表I**：STORM在四个操作任务上的成功率（%）对比。STORM在所有任务上均取得最佳性能，平均成功率达51.0%，超越了包括其基础模型CogACT在内的先前方法。\n\n![视频预测定性结果](https://arxiv.org/html/2512.18477v1/images/video_predict_compare_3_0.png)\n\n> **图2**：视频预测定性结果（任务：将胡萝卜放在盘子上）。以VLA提议动作为条件的预测结果（中行）与真实执行结果（底行）高度一致，证明了世界模型捕捉关键因果动态的能力。\n\n![失败恢复案例研究](https://arxiv.org/html/2512.18477v1/images/failure_grid.jpg)\n\n> **图3**：在“将胡萝卜放在盘子上”任务中的失败恢复案例研究。上图：基线CogACT模型在初始抓取失败后陷入重复循环。下图：STORM利用其前瞻规划能力，在相同初始失败后重新评估，并找到了一条新的、成功的轨迹完成任务，展示了其从错误中恢复的战略能力。\n\n**消融实验**：论文通过对比“动作+奖励”和“仅动作”两种训练方式的世界模型，分析了奖励监督的影响。结果表明，奖励监督对于学习任务相关的动态至关重要。\n\n![视频预测指标雷达图](https://arxiv.org/html/2512.18477v1/images/radar_chart_video_metrics.jpg)\n\n> **图4**：奖励监督对学习高保真世界模型关键性的消融研究雷达图。使用完整目标（‘动作+奖励’）训练的模型在所有评估指标（FVD、LPIPS、PSNR、SSIM）上均优于没有奖励监督（‘仅动作’）的模型，表明奖励信号迫使模型学习任务相关的因果结构，而不仅仅是表面视觉模式。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  提出了**STORM框架**，一种新颖的搜索引导生成式世界模型，它将基于扩散的VLA、生成式视频世界模型和MCTS集成在一起，为机器人操作实现了显式的时空推理。\n2.  证明了**奖励增强的视频预测器**可以作为一种高效的生成式世界模型，显著提高了动作条件视觉推演的保真度和任务相关性。\n3.  通过实验表明，STORM在任务成功率和失败恢复方面均优于强大的VLA基线，**凸显了将生成式前瞻与基于搜索的规划相结合的优势**。\n\n论文自身提到的局限性在于，其性能最终受限于当前世界模型实例的**预测保真度**。例如，在需要精确接触物理控制的“堆叠绿色积木在黄色积木上”任务中，STORM的性能与基线持平，这表明视觉预测模型在捕捉细微、非线性动态方面存在挑战。这并非搜索范式本身的失败，而是一个明确的研究方向。\n\n对后续研究的启示在于：STORM的成功验证了“视觉前瞻”与“搜索规划”结合范式的有效性。未来的工作可以沿着两个方向深入：一是**集成更高保真度、更具物理感知的生成式世界模型**（如融入物理引擎或更精细的动态模型），以突破在精密操作任务上的性能瓶颈；二是**探索更高效的搜索算法或规划策略**，以降低多步视觉模拟带来的计算成本，推动该范式在更复杂、更长期任务中的应用。",
      "imageUrls": [
        "https://arxiv.org/html/2512.18477v1/images/STORM.jpg",
        "https://arxiv.org/html/2512.18477v1/images/video_predict_compare_3_0.png",
        "https://arxiv.org/html/2512.18477v1/images/failure_grid.jpg",
        "https://arxiv.org/html/2512.18477v1/images/success_grid.jpg",
        "https://arxiv.org/html/2512.18477v1/images/radar_chart_video_metrics.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.18396",
      "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation",
      "url": "http://arxiv.org/abs/2512.18396",
      "arxivId": "2512.18396",
      "date": "2025-12-20",
      "authors": "Yakun Huang Team",
      "category": "Manipulation",
      "summary": "本文针对关节物体精细操作任务依赖大量昂贵真实演示数据的问题，提出AOMGen框架。该方法仅需单个真实扫描和演示，结合数字资产库，生成视觉逼真且物理状态可验证的训练数据，通过系统变化相机视角、物体风格与位姿来增强数据多样性。实验表明，使用AOMGen数据微调VLA策略后，任务成功率从0%提升至88.7%，并能在未见物体和布局上有效泛化。",
      "detailedSummary": "## 研究背景与动机\n当前，为机器人操作策略（尤其是基于视觉-语言-动作的VLA模型）提供高质量训练数据的主流方法主要包括基于物理的仿真和基于视频的世界模型。物理仿真能高效生成大量状态-动作对齐的演示，但视觉真实性不足，给仿真到真实的迁移带来挑战。视频世界模型直接从大规模真实视频学习，具有更高的视觉保真度，但往往对物理真实性和动作可执行性的监督不足，导致物理上不一致的交互。对于铰接物体的精细操作任务，其复杂的运动学约束需要更丰富和精确的数据，而现有方法（如DemoGen和R2RGen）存在局限：仅能处理简单的抓取和放置，无法处理铰接操作；物体外观和几何形状固定，难以泛化到新物体或新姿态；且多为单视图输入，降低了多视角观察的视觉真实性。\n\n本文针对铰接物体操作演示数据收集昂贵、稀缺且现有生成方法在视觉逼真度与物理一致性上难以兼顾的痛点，提出了AOMGen框架。其核心思路是：仅利用单个真实场景扫描视频、一次真实操作演示以及一个现成的数字资产库，通过三维高斯溅射（3DGS）重建、物理一致的运动恢复以及类别内物体替换与姿态泛化，生成大量视觉逼真且物理状态可验证的铰接操作训练数据。\n\n## 方法详解\nAOMGen的整体流程旨在从单个真实演示扩展到多样化的合成数据。其输入包括：静态场景扫描视频 `V_static`、动态操作视频 `V_dynamic`、机械臂关节状态序列 `A`，以及同一类别铰接物体的仿真3D资产。输出是同步的多视角RGB视频，其时间与动作指令、关节及接触点状态标注对齐。\n\n![方法框架总览](https://arxiv.org/html/2512.18396v1/Fig/robot_pipeline.png)\n> **图2**：AOMGen方法整体流程。左侧为基于3D高斯溅射的场景重建与运动恢复阶段，右侧为铰接物体替换与姿态泛化阶段。以旋转关节物体为例，展示了从原始数据到生成新演示的完整过程。\n\n框架包含两个核心模块：\n1.  **场景重建与运动恢复**：首先，使用COLMAP进行稀疏重建和相机姿态估计。然后，基于SAGA方法构建带有特征的分割3DGS模型，利用SAM2生成的多视图2D掩码实现部件级分割，从而区分出铰接物体的可动部件（`Part_move`）和静态部件（`Part_static`）。通过迭代最近点（ICP）算法将GS坐标系与真实世界坐标系对齐。运动恢复部分（AOMotion）是关键创新，它利用真实的机械臂轨迹作为物理先验，在缺乏目标物体运动直接记录的情况下，通过四个子模块恢复其运动：a) **关键帧提取**：通过处理动态视频中可动部件和机械臂的掩码，计算并平滑“运动分数”，动态确定交互的起始帧和结束帧。b) **接触点检测**：在起始帧，基于机械臂末端执行器与可动部件点云之间的最近距离，确定两者接触点的位置（`PC_r` 和 `PC_move`）。c) **铰接物体建模**：通过分析可动与静态部件的包围盒边缘，设计评分方案（考虑边缘平行度、距离以及与末端执行器方向、接触点的关系）来确定与关节相邻的边缘对，进而计算关节的方向和中心。d) **可动部件运动恢复**：以机械臂接触点轨迹 `Traj_r` 为监督，通过优化旋转角度（或平移距离）`θ_t`，使变换后的可动部件表面与轨迹的交点尽可能接近真实的接触点，从而恢复出物理一致的运动。\n\n![运动分数计算](https://arxiv.org/html/2512.18396v1/Fig/motionscore.png)\n> **图3**：运动分数的计算过程。上方为原始视频帧序列，下方为计算出的运动分数曲线、平滑后的曲线以及动态阈值，用于精确检测交互的开始与结束。\n\n2.  **铰接物体替换与姿态泛化**：给定同一类别的新铰接物体资产 `AO_new`，首先通过基于NOCS的接触点映射方法，将原始可动部件上的接触点 `PC_move` 映射到新物体上，得到 `PC_map`。然后，采用两阶段优化使新物体与原始机械臂轨迹物理兼容：第一阶段优化缩放、初始运动参数和偏移量，使映射后的接触点轨迹粗略匹配机械臂轨迹；第二阶段进一步优化，以最小化机械臂接触点与新物体可动部件表面之间的交点误差，从而处理非匀速运动和接触滑动问题。在视觉层面，使用DiffusionLight从真实场景提取光照并烘焙到新物体材质上，并应用高斯修复来处理物体替换造成的空洞。最后，为了极大扩展数据多样性，支持对替换物体施加任意的姿态变换 `T_ao`，并相应地通过插值和逆运动学生成新的、与之适配的机械臂轨迹。\n\n## 实验与结果\n实验使用了UR5e机械臂收集的真实演示数据。为验证AOMGen的泛化能力，选用了来自ArtVIP资产库的三类旋转关节物体（微波炉、工具箱、电脑）和两类棱柱关节物体（抽屉、柜子）进行替换测试。物体姿态泛化的平移范围为[-0.05m, 0.3m] * [-0.05m, 0.05m]，旋转范围为[-45°, 45°]。\n\n![高斯场可视化-微波炉](https://arxiv.org/html/2512.18396v1/Fig/mic_data.png)\n> **图4**：AOMGen生成的旋转关节（微波炉）操作数据的高斯场可视化。\n\n![高斯场可视化-抽屉](https://arxiv.org/html/2512.18396v1/Fig/drawer_data.png)\n> **图5**：AOMGen生成的棱柱关节（抽屉）操作数据的高斯场可视化。\n\n首先，通过在仿真中重放生成的演示来验证物理一致性。如表1所示，对于五类不同的替换物体，生成演示在仿真中重放的成功率平均高达98%，证明了AOMGen生成的交互数据具有高度的物理合理性和精确性。\n\n![仿真重放结果-微波炉](https://arxiv.org/html/2512.18396v1/Fig/Replay.png)\n> **图6**：微波炉（旋转关节）生成数据在仿真环境中的重放结果序列，展示了物理交互的一致性。\n\n![仿真重放结果-抽屉](https://arxiv.org/html/2512.18396v1/Fig/Replay_drawer.png)\n> **图7**：抽屉（棱柱关节）生成数据在仿真环境中的重放结果序列。\n\n其次，评估生成数据对VLA策略训练的有效性。使用OpenVLA作为基础模型，分别在仅用真实数据、仅用AOMGen合成数据、以及用合成数据微调预训练模型三种设置下进行训练和测试。\n\n![成功率对比](https://arxiv.org/html/2512.18396v1/Fig/success_rate_comparison.png)\n> **图8**：不同训练数据设置下，VLA策略在未见过的物体和布局上的测试成功率对比。仅用真实数据训练的成功率为0%，仅用AOMGen数据训练达到53.3%，而用AOMGen数据微调预训练的OpenVLA模型，成功率大幅提升至88.7%。\n\n关键结果显示：仅使用有限的真实数据（1条演示）训练时，模型无法泛化，成功率为0%。仅使用AOMGen生成的合成数据训练，成功率可达53.3%。而使用AOMGen数据对预训练的OpenVLA模型进行微调，能将成功率显著提升至88.7%。这证明了合成数据对于提升模型性能的有效性。\n\n最后，通过消融实验验证姿态泛化策略对模型鲁棒性的增强作用。如图9所示，在使用经过姿态泛化数据训练的模型与使用未经过姿态泛化数据训练的模型进行对比时，前者在面对物体位置和角度变化时表现出了更强的鲁棒性。\n\n![姿态泛化消融实验](https://arxiv.org/html/2512.18396v1/Fig/pose_gen.png)\n> **图9**：姿态泛化策略的消融研究。使用姿态泛化数据训练的模型（绿色）在面对物体位置和角度变化时，比未使用该策略的模型（蓝色）成功率更高，鲁棒性更强。\n\n![尺度泛化示例](https://arxiv.org/html/2512.18396v1/Fig/scale.png)\n> **图10**：AOMGen生成的针对不同尺度物体的操作数据示例。\n\n![新物体泛化示例](https://arxiv.org/html/2512.18396v1/Fig/new_obj.png)\n> **图11**：AOMGen将操作能力泛化到同一类别内全新物体上的示例。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了一个可扩展的框架，能够从单个真实扫描和演示出发，生成同一类别内任意铰接物体的逼真操作数据。2) 通过创新的、基于真实机械臂轨迹监督的运动恢复方法（AOMotion）和两阶段物体替换优化，确保了生成数据兼具高度的物理一致性与视觉真实性。3) 引入了物体姿态泛化机制，显著扩大了生成数据的配置多样性，从而增强了训练出的策略模型的鲁棒性。\n\n论文提及的局限性在于，其方法依赖于同一类别物体具有相似的关节相对位置和运动模式这一假设，对于关节结构或运动模式差异巨大的“类别”，泛化能力可能受限。此外，当前框架主要处理单步、单目标的铰接操作。\n\n这项工作对后续研究的启示包括：将物理一致的生成范式扩展到更复杂的操作序列或多物体交互场景；探索对更广泛“类别”定义（如功能相似但结构不同）的泛化能力；以及将生成的数据闭环用于世界模型或物理仿真器的训练，进一步提升仿真真实性。",
      "imageUrls": [
        "https://arxiv.org/html/2512.18396v1/x1.png",
        "https://arxiv.org/html/2512.18396v1/Fig/robot_pipeline.png",
        "https://arxiv.org/html/2512.18396v1/Fig/motionscore.png",
        "https://arxiv.org/html/2512.18396v1/Fig/mic_data.png",
        "https://arxiv.org/html/2512.18396v1/Fig/drawer_data.png",
        "https://arxiv.org/html/2512.18396v1/Fig/Replay.png",
        "https://arxiv.org/html/2512.18396v1/Fig/Replay_drawer.png",
        "https://arxiv.org/html/2512.18396v1/Fig/success_rate_comparison.png",
        "https://arxiv.org/html/2512.18396v1/Fig/pose_gen.png",
        "https://arxiv.org/html/2512.18396v1/Fig/scale.png",
        "https://arxiv.org/html/2512.18396v1/Fig/new_obj.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.18368",
      "title": "Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.18368",
      "arxivId": "2512.18368",
      "date": "2025-12-20",
      "authors": "Jingya Wang Team",
      "category": "Manipulation",
      "summary": "本文针对多任务机器人操作中模仿学习面临的泛化挑战，提出了AtomSkill框架。其核心是构建一个**语义接地的原子技能库**，通过夹持器状态关键帧检测与视觉语言模型标注，将演示分割为语义一致的变长技能。同时，**带有关键姿态想象的动作生成模块**能联合预测技能的长期目标姿态与即时动作序列，实现鲁棒的技能组合。实验表明，该方法在多样化的操作任务上性能优于现有先进方法。",
      "detailedSummary": "## 研究背景与动机\n模仿学习在单任务机器人操作中取得了显著成果，但扩展到多任务场景仍面临根本性挑战，如次优的专家演示、轨迹噪声和行为多模态。现有的基于技能的方法试图通过将复杂动作分解为可重用的抽象来解决此问题，但它们通常依赖于固定长度的分割或环境先验，这限制了语义一致性和跨任务泛化能力。例如，一些方法使用固定长度的滑动窗口来分割演示，这忽略了技能的可变长度本质，导致学习到的潜在标记捕获的是短而重复的运动片段，而非语义上有意义且可重用的技能。本文针对现有技能学习方法语义不一致、泛化能力有限的具体痛点，提出了通过构建语义接地的原子技能空间来提升多任务模仿学习性能的新视角。本文的核心思路是：首先利用夹爪状态关键帧检测和视觉语言模型（VLM）标注，将演示分割成可变长度、语义一致的原子技能，并通过对比学习对齐其表示；然后设计一个能联合预测技能长远目标（关键姿态）和即时动作序列的策略，以实现鲁棒的技能链式执行。\n\n## 方法详解\nAtomSkill框架包含三个主要组件：1）利用VLM进行语义技能发现；2）通过对比对齐进行原子技能学习；3）利用技能先验进行推理。整体目标是学习一个语义上有意义的技能先验库，为跨任务操作提供高层指导。\n\n![方法框架](https://arxiv.org/html/2512.18368v1/x2.png)\n> **图2**：AtomSkill框架概览。左侧展示了语义技能发现过程：同一任务的专家演示被分割成语义一致、时间对齐的片段，并由视觉-语言模型为每个片段分配技能标签。右上部分展示了技能学习过程，AtomSkill构建技能空间并同时训练技能引导的策略和基于扩散的采样器。右下部分描绘了通过带关键姿态的动作分块进行推理，实现预测技能的平滑、鲁棒链式执行。\n\n**1. 语义技能发现与VLM**：该方法将一个技能定义为完成语义相同目标（如“抓取”、“放置”）的一组动作序列。给定专家演示轨迹，首先基于**夹爪状态变化**检测关键帧，将轨迹分割成非重叠的子轨迹。夹爪状态变化是物理交互的关键信号，可标志一个技能的结束和下一个技能的开始。随后，利用一个大型视觉语言模型，基于每个片段的观察和任务指令，生成语义接地的描述和对应的技能标签（例如，对于任务“把垃圾放进垃圾桶”，VLM生成“抓取垃圾”和“将垃圾放入垃圾桶”等描述，对应“抓取”和“放置”技能）。这些带标签的子轨迹为学习模块化、可重用的技能表示奠定了基础。\n\n**2. 原子技能学习与语义对齐**：采用VQ-VAE结构从动作序列中提取紧凑的抽象。技能编码器由1D CNN层和自注意力层构成，将可变长度的动作子轨迹重采样为固定长度后，编码为连续嵌入。一个向量量化层将这些连续嵌入映射到一个可学习的码本上，得到离散的技能标记，训练时使用标准的向量量化损失。**核心创新**在于引入了**语义对比技能对齐损失**。该损失由两项组成：`L_temp`鼓励共享相同量化标记且在技能序列中相对位置一致的嵌入在潜在空间中靠近，以促进时间一致性；`L_skill`鼓励共享相同语义技能标签和量化标记的嵌入彼此靠近，以形成语义上有组织的技能空间。两者共同将VQ潜在空间转变为时间一致且语义清晰的技能空间。\n\n**3. 带关键姿态想象的动作解码器**：在每一步，动作解码器接收当前观测（多视角图像、本体感知、语言指令）以及由技能采样器提供的当前技能抽象`z_q`。解码器采用交叉注意力机制，将技能抽象作为键和值，观测作为查询，以此注入技能信息并降低计算成本。解码器不仅预测未来一段动作序列，还**联合预测当前技能终止时的关键姿态**。这是通过向动作查询附加一个额外的查询令牌来实现的，并采用一个辅助的L1损失进行训练。预测关键姿态能增强策略的空间推理和定位能力。\n\n**4. 推理与技能先验**：**技能扩散采样器**被建模为一个以技能标签和观测为条件的扩散模型，用于从学习到的技能空间中采样高层技能嵌入，经量化后输入策略。**带关键姿态的动作分块**是推理时的核心机制：策略持续生成动作块，直到预测的下一动作与预测的关键姿态在动作空间中足够接近，此时触发扩散采样器采样下一个技能。这种基于接近度的标准避免了手工设计的终止启发式方法，实现了技能间的自主、平滑过渡。\n\n## 实验与结果\n**实验设置**：在模拟环境中使用RLBench的六个任务（见图3），在真实世界中使用ALOHA风格的双臂机器人执行三个任务。每个任务收集100条演示。对比基线包括：Diffusion Policy (DP)、ACT、VQ-BeT、QueST以及大型VLA模型RDT。评估指标包括严格成功率(SR)和平均任务进度(ATP)。主要在多任务设置下进行训练和评估。\n\n![任务图示](https://arxiv.org/html/2512.18368v1/x3.png)\n> **图3**：选定的RLBench任务和真实世界任务示意图。六个RLBench任务分为运动模式任务和空间定位任务两组。前者测试再现一致运动动力学的能力，后者检验空间接地的准确性。三个真实世界任务则同时考察空间定位和长视野动作序列建模。\n\n![模拟结果表](https://arxiv.org/html/2512.18368v1/x4.png)\n> **图4**：模拟性能结果表（对应论文表1）。AtomSkill在平均ATP（0.68）和SR（67.2%）上均优于所有基线，且在每一项任务上都取得了最佳或极具竞争力的性能。\n\n**关键实验结果**：如表1（图4）所示，AtomSkill在六项RLBench任务上取得了最佳平均性能（ATP 0.68， SR 67.2%），全面超越了所有基线。进一步将任务分为运动模式任务（Close Box, Sweep to Dustpan, Close Laptop）和空间定位任务（Put Rubbish in Bin, Phone on Base, Umbrella out of Stand）进行分析，AtomSkill在两组任务上均表现最佳（运动模式：ATP 0.83， SR 82.2%；空间定位：ATP 0.53， SR 52.2%），展现了其在再现运动动力学和空间推理方面的双重优势。\n\n![真实世界结果](https://arxiv.org/html/2512.18368v1/x5.png)\n> **图5**：真实世界任务性能。AtomSkill在双臂操作任务上显著优于基线方法ACT和QueST，证明了其在实际机器人系统中的有效性和鲁棒性。\n\n**真实世界结果**：在三个双臂操作任务上，AtomSkill同样显著优于对比的ACT和QueST方法（图5），验证了其在实际应用中的有效性。\n\n![消融研究](https://arxiv.org/html/2512.18368v1/x6.png)\n> **图6**：消融研究结果。移除非重叠分割、对比学习对齐或关键姿态预测中的任何一项都会导致性能下降，证明了每个组件的重要性。\n\n**消融实验**：消融研究（图6）证实了各核心组件的贡献：1）使用**非重叠、基于关键帧的分割**（对比固定长度滑动窗口）对性能提升至关重要；2）**对比学习对齐**损失（尤其是语义对齐部分）显著改善了技能表示的质量和策略性能；3）**关键姿态预测**机制对于实现精确的空间定位和鲁棒的技能链式执行不可或缺。\n\n![定性分析](https://arxiv.org/html/2512.18368v1/x7.png)\n> **图7**：技能空间可视化。经过对比学习对齐后，AtomSkill的技能潜在空间呈现出清晰的语义结构，相同技能（如抓取、放置）的嵌入聚集在一起，不同技能则相互分离。\n\n![技能采样](https://arxiv.org/html/2512.18368v1/x8.png)\n> **图8**：技能扩散采样可视化。扩散采样器能够根据当前观测和技能标签，生成多样且合理的技能序列，例如在“打开盒子”任务中正确排序“移动至”、“抓取”、“放置”等技能。\n\n![轨迹对比](https://arxiv.org/html/2512.18368v1/x9.png)\n> **图9**：与基线方法的轨迹对比。AtomSkill生成的动作轨迹更平滑，与专家演示更接近，并且能通过预测的关键姿态（红星）准确监控任务进度，实现自动技能切换。\n\n## 总结与启发\n本文的核心贡献包括：1）提出了**AtomSkill框架**，通过构建结构化的原子技能空间来实现可组合的多任务机器人操作；2）开发了**语义接地的原子技能库**构建方法，利用关键帧分割和VLM标注确保技能的语义一致性和时间相干性；3）设计了**带关键姿态想象的动作生成模块**，使策略能同时推理长远运动目标和细粒度控制，从而实现了鲁棒的技能链式执行。\n\n论文提到的局限性包括：技能分割依赖于夹爪状态变化，这可能不适用于所有类型的技能（如纯导航）；此外，依赖VLM进行标注可能存在错误。这些局限性为后续研究指明了方向，例如探索更通用的技能边界检测信号，提高语义标注的鲁棒性，以及将原子技能库扩展到更复杂的长期任务和动态环境中。AtomSkill为将高层语义知识系统性地注入低层机器人控制提供了一条有效路径。",
      "imageUrls": [
        "https://arxiv.org/html/2512.18368v1/x1.png",
        "https://arxiv.org/html/2512.18368v1/x2.png",
        "https://arxiv.org/html/2512.18368v1/x3.png",
        "https://arxiv.org/html/2512.18368v1/x4.png",
        "https://arxiv.org/html/2512.18368v1/x5.png",
        "https://arxiv.org/html/2512.18368v1/x6.png",
        "https://arxiv.org/html/2512.18368v1/x7.png",
        "https://arxiv.org/html/2512.18368v1/x8.png",
        "https://arxiv.org/html/2512.18368v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.18028",
      "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation",
      "url": "http://arxiv.org/abs/2512.18028",
      "arxivId": "2512.18028",
      "date": "2025-12-19",
      "authors": "Eric Sax Team",
      "category": "Manipulation",
      "summary": "本文针对现有具身视觉语言导航（VLN）基准难以衡量模型真实推理能力的问题，提出了Embodied4C闭环基准。该基准通过覆盖自动驾驶车辆、无人机和机械臂三种异构平台，设计约1100个一次性推理问题和58个导航任务，系统评估模型在语义、空间、时间和物理四个维度的推理能力，并引入领域远查询以防止过拟合。实验对10个先进VLM和4个具身控制基线进行了全面评估，核心结论表明：跨模态对齐和指令调优比模型规模更重要，而空间与时间推理是可靠具身能力的主要瓶颈。",
      "detailedSummary": "## 研究背景与动机\n当前，具身视觉语言导航（VLN）领域的评测基准正从静态数据集评估转向动态、任务导向的范式。然而，如表1所示，现有基准大多专注于单一领域（如仅自动驾驶）或仅评测部分核心能力。它们通常强调语义和空间推理，但普遍忽视了对时间动态和物理交互的理解。更重要的是，这些基准很少系统性地评估智能体的核心价值主张，如跨领域和跨平台的**通用性**、对传感器配置等**具身属性**的感知，以及在闭环环境中感知、推理与控制的**交互性**。这导致模型可能通过过拟合特定任务分布、静态传感器配置或多选题模板获得高分，而未能展现真正的泛化与推理能力。\n\n本文针对现有基准在评测维度上零散、割裂，且易引入归纳偏置的关键痛点，提出了一个统一、全面的评测新视角。本文的核心思路是：提出一个名为Embodied4C的闭环评测基准，通过跨自动驾驶车辆、无人机和机械臂三种异构具身平台的视觉问答（VQA）与导航任务，系统性地评估模型在**语义**、**空间**、**时间**和**物理**四个维度的核心推理能力，并同时衡量其**通用性**、**具身感知**和**交互性**三大价值主张。\n\n## 方法详解\nEmbodied4C基准旨在闭环环境下评估智能体的视觉语言推理与控制能力。其整体框架包含两个互补的任务模态：**VQA（交互阶段）** 用于评估场景理解，以及**VLN（控制阶段）** 用于评估在现实仿真环境中的决策与控制。基准共包含约1149个独特的、手工制作的VQA问答对、13种传感器设置，以及跨三个仿真平台的58个VLN任务，构成了一个异构的测试平台。\n\n![Embodied4C Benchmark Framework](https://arxiv.org/html/2512.18028v1/x1.png)\n> **图1**：Embodied4C基准框架概览。基准涵盖自动驾驶、空中导航和机器人操作三个具身领域。每个领域都包含VQA（用于评估场景理解）和VLN（用于评估闭环控制）任务，共同评测语义、空间、时间和物理四个核心能力。\n\n**核心模块与评测维度**：\n1.  **核心能力评测**：基准通过非模板化的VQA任务探测四种具身推理能力：\n    *   **语义理解**：关于物体类别、属性和上下文状态的推理。\n    *   **空间理解**：关于位置、距离、方向、拓扑和定性关系的推理。\n    *   **时间理解**：关于短期和长期动态及时间依赖关系的推理。\n    *   **物理理解**：关于物理模型、动态约束和材料属性的推理。\n2.  **具身领域**：基准在三个具有真实动态的仿真环境中进行：\n    *   **自动驾驶**：使用CARLA模拟器，场景涵盖高速公路、城市、郊区和乡村，具有可变的交通密度和天气/光照扰动。\n    *   **空中导航**：使用AirSim模拟器，评估涵盖多样的户外场景、多个高度和相机云台角度。\n    *   **机器人操作**：使用RLBench模拟器，环境为室内工作空间，包含精选的物体集（如篮球、积木、抽屉），任务涵盖抓取/放置、打开/关闭和运动学交互。\n3.  **防过拟合设计**：为了减少归纳偏置，VQA采用开放式问答形式，并注入**领域外查询**和传感器/天气变化，以探测超越任务领域的泛化能力。VLN任务采用“考试风格”的**单次尝试**评测，避免重复试验人为提高成功率，更反映现实世界单次执行的机会。\n\n**评分机制**：\n*   **VQA评分**：对于自由形式答案，使用基于GPT的VLM-judge在[0, 100]区间内给出连续分数。对于数值答案，采用奖励接近地面真值的相对偏差公式计算分数。最后，分别计算四个能力维度的平均分，再平均得到总体VQA分数。\n*   **VLN评分**：对于简单任务，采用二进制通过/失败标准（100或0分）。对于复杂任务，采用分级评分方案，奖励向目标的部分、基于距离的进展（最高50分），仅在完全满足所有目标条件时给予100分。最终平均所有任务得到VLN分数。\n*   **总分计算**：每个子领域（驾驶、空中、操作）的得分是VQA和VLN分数的平均值。总体基准得分是三个子领域得分的平均值。此外，**通用性（GEN）** 分数作为一个独立的诊断轴进行计算，以检测过拟合。\n\n**创新点**：与现有方法相比，Embodied4C的主要创新在于：1) **首个统一框架**：首次将四个核心能力（语义、空间、时间、物理）与三个关键价值主张（通用性、多具身、交互性）的评测统一到一个基准中。2) **跨异构具身**：系统性地在三种完全不同的物理平台（地面车辆、无人机、机械臂）上进行评测，以分析具身选择对感知、推理和控制的影响。3) **结构化防过拟合设计**：通过领域外查询、单次尝试VLN和开放式VQA，旨在缓解模型过拟合，实现对具身能力的细粒度归因。\n\n## 实验与结果\n**实验设置**：实验在Embodied4C基准上进行，使用了三个仿真平台：CARLA（驾驶）、AirSim（空中导航）和RLBench（机器人操作）。评估了14个模型，包括10个预训练的基础视觉语言模型（如FastVLM、Qwen2.5-VL、Gemma3、LLaMA 4、Claude 3.7/4.5、GPT-4o、GPT-5系列）和4个领域专用的VLN模型（驾驶领域的Senna，无人机领域的OpenFly-Agent，操作领域的OpenVLA和MolmoAct）。所有预训练模型均使用官方权重，未进行任务特定的微调。\n\n**关键实验结果**：总体结果如表2所示。GPT-5-mini获得了最高的总体Embodied4C得分（39.59），其次是GPT-5（36.00）。Claude系列和LLaMA 4 Maverick处于中游水平（28-31）。较小的模型（Qwen2.5-VL和Gemma3-4B-IT）在所有具身领域表现明显较弱。一个关键发现是，尽管经过领域特定优化，但四个领域专用的视觉-语言-动作模型（Senna, OpenFly-Agent, OpenVLA, MolmoAct）在VQA和VLN上的表现都接近零分，表明它们泛化能力极差，严重过拟合于其训练分布。\n\n![Qualitative Examples of VQA](https://arxiv.org/html/2512.18028v1/x2.png)\n> **图2**：Embodied4C VQA在不同场景和领域的定性示例。该图展示了模型在语义、空间、时间和物理问题上典型的成功与失败模式，直观呈现了模型推理能力的优势与瓶颈。\n\n![Distribution of Question Types](https://arxiv.org/html/2512.18028v1/x3.png)\n> **图3**：各具身领域中问题类型的分布统计。该图显示了语义、空间、时间和物理四类问题在自动驾驶、空中导航和机器人操作三个子基准中的数量分布，体现了基准设计的全面性和均衡性。\n\n**分领域结果分析**：\n*   **自动驾驶**：GPT-5-mini得分最高（40.11）。领域专用模型Senna由于紧密绑定其nuScenes训练数据分布，在CARLA的场景级推理和控制中表现疲弱。\n*   **空中导航**：GPT-5-mini再次领先（40.21）。尽管经过无人机训练，OpenFly-Agent在Embodied4C中表现崩溃，原因是其严重过拟合于模拟器特定的飞行动力学和状态表示。\n*   **机器人操作**：GPT-5（38.64）和GPT-5-mini（38.44）得分最高。OpenVLA完全失败，因为当绕过其动作头时语言输出质量下降，且其动作执行无法泛化到未见过的物体-环境配置。\n\n**消融实验与深入分析**：论文指出，跨模态对齐和指令调优比模型规模更重要，而空间和时间推理是可靠具身能力的主要瓶颈。对领域专用模型的PCA分析进一步揭示，这些模型形成了一个独特的性能集群，其特征是有限的语言 grounding 和狭窄的、具身特定的先验知识。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**首个**统一评估语义、空间、时间和物理四大核心推理能力，并同时衡量通用性、具身感知和交互性三大价值主张的闭环评测基准Embodied4C。\n2.  设计了一种结构化的、防过拟合的任务范式，通过跨异构具身平台（自动驾驶车辆、无人机、机械臂）的评测，实现了对模型具身能力的细粒度归因。\n3.  对十四个前沿模型进行了全面评估，关键发现包括：基础VLMs的表现优于领域专用VLA模型；跨模态对齐和指令调优至关重要；空间与时间推理是当前主要瓶颈。\n\n**局限性**：论文自身提到的局限性包括：1) 基准规模（1149个VQA，58个VLN任务）可能仍需扩大以涵盖更极端的边缘情况。2) 仿真环境与真实世界之间存在差距。3) VQA评分依赖于基于GPT的评判器，可能引入其自身的偏见。\n\n**对后续研究的启示**：\n1.  开发更鲁棒、泛化的具身智能体需要超越单一领域的优化，重视跨平台的核心推理能力。\n2.  未来的模型设计应特别加强空间关系理解和时间动态推理模块。\n3.  评测基准应继续向减少归纳偏置、增强闭环交互和覆盖更广泛具身形态的方向发展。Embodied4C为这一方向提供了一个系统的评估框架和基线。",
      "imageUrls": [
        "https://arxiv.org/html/2512.18028v1/x1.png",
        "https://arxiv.org/html/2512.18028v1/x2.png",
        "https://arxiv.org/html/2512.18028v1/x3.png",
        "https://arxiv.org/html/2512.18028v1/x4.png",
        "https://arxiv.org/html/2512.18028v1/x5.png",
        "https://arxiv.org/html/2512.18028v1/01_Figures/FrankaPanda.png",
        "https://arxiv.org/html/2512.18028v1/01_Figures/Sawyer.png",
        "https://arxiv.org/html/2512.18028v1/01_Figures/UR5.png",
        "https://arxiv.org/html/2512.18028v1/x6.png",
        "https://arxiv.org/html/2512.18028v1/x7.png",
        "https://arxiv.org/html/2512.18028v1/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.18619",
      "title": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
      "url": "http://arxiv.org/abs/2512.18619",
      "arxivId": "2512.18619",
      "date": "2025-12-21",
      "authors": "Dan Negrut Team",
      "category": "Manipulation",
      "summary": "本文提出ChronoDreamer，旨在解决接触丰富的机器人操作中，传统仿真器速度慢、仿真到现实存在差距，而现有视频预测模型又忽略物理接触信息的问题。其核心是构建一个动作条件世界模型，关键技术包括：采用空间-时间变换器与MaskGIT式掩码预测，联合预测未来RGB帧、接触图与关节角度；将3D接触力编码为深度加权高斯泼溅图像以供视觉主干处理；在推理时集成基于视觉语言模型的碰撞评判器进行拒绝采样。在DreamerBench数据集上的实验表明，该模型能保持非接触运动的空间连贯性，生成合理的接触预测，其LLM评判器能有效区分碰撞与非碰撞轨迹。",
      "detailedSummary": "## 研究背景与动机\n当前最先进的文本和图像条件视频生成器能够产生逼真的纹理和复杂的几何，但它们常常违反基本力学原理，例如物体相互穿透、轨迹违背重力、能量凭空出现或消失。对于机器人、科学可视化或模拟感知媒体而言，这些失败是结构性的而非美学的。这促使研究者回归世界模型的视角，即生成器需要维护一个足够连贯的内部状态和转移动态，以支持“想象”和规划。经典系统如World Models、PlaNet、Dreamer及其大规模后继者Genie已证明，长期行为依赖于忠实于第一性原理的潜在动力学。将现代视频生成置于此框架下审视，意味着感知逼真度必须与受牛顿定律及更广泛物理定律约束的状态更新相结合。\n\n现有方法尝试通过多种途径引入物理合理性：通过架构偏置使物理合理运动成为默认吸引子（如PhyDNet）；将状态演化委托给机械模拟器，生成器仅负责逼真渲染（如PhysGen）；利用多模态语言模型提供关于力、接触和约束的符号推理（如DiffPhy）；或通过免训练的引导信号在采样过程中规避物理违规。然而，大多数视频预测模型侧重于视觉合理性，而忽视了决定轨迹是否安全的物理量——接触力、摩擦模式、关节状态。本文旨在弥合视频生成与接触感知规划之间的差距。\n\n本文提出了ChronoDreamer，一个动作条件的世界模型，能够联合预测未来的RGB帧、接触图和关节角度。其核心思路是：通过一个使用MaskGIT风格掩码预测训练的空间-时间Transformer，将接触编码为相机对齐的深度加权高斯溅射图像，并在推理时利用基于LLM的碰撞判断器对预测的轨迹进行拒绝采样，从而在动作执行前过滤不安全候选。\n\n## 方法详解\nChronoDreamer的整体流程是：给定历史RGB帧、接触溅射图、动作和关节状态，首先使用编码器（Nvidia Cosmos Tokenizer）将它们转换为离散令牌表示；然后，一个空间-时间Transformer模型以这些历史令牌和未来动作为条件，通过MaskGIT风格的训练预测未来的视频令牌、接触令牌，并回归未来的关节角度；最后，解码器将预测的令牌转换回图像和接触图，同时预测的轨迹会由一个基于LLM的碰撞判断器进行评估，以筛选出安全的动作序列。\n\n![数据管道](https://arxiv.org/html/2512.18619v1/images/architecture.png)\n\n> **图1**：数据管道。Cosmos编码器将RGB和接触帧标记化为32×32的网格。动力学模型输出因子化的视频/接触逻辑值以及关节角度。\n\n### 核心模块一：接触编码（深度加权高斯溅射）\n这是本文的一个关键创新。为了将3D接触力转化为适合视觉主干网络处理的图像格式，论文提出将接触渲染为相机对齐的深度加权高斯溅射图像。对于仿真中的每个接触点 i，给定其3D位置 **p**_i 和接触力 **f**_i，以及相机参数：\n1.  **投影**：将接触点转换到相机坐标系 **x**_i，并投影到图像像素坐标 (u_i, v_i)。\n2.  **力方向编码**：计算沿力向量位移后的端点投影 (u_i^end, v_i^end)，得到2D方向向量 Δ**d**_i。归一化后，其x和y分量被线性映射到[0,1]区间，并分别存储在输出图像的绿色(G)和蓝色(B)通道中。\n3.  **力大小编码**：力的大小 m_i = ||**f**_i|| 经过裁剪和归一化后，存储在红色(R)通道。\n4.  **高斯溅射渲染**：每个接触点被渲染为一个各向同性的2D高斯核，其半径 r_i 与归一化的力大小 (m_i/m_max)^γ 成比例。为了鼓励更近的接触点主导更远的点，高斯核会乘以一个深度相关的权重 w_i^depth = exp(-X_i / τ_depth)，其中 X_i 是相机前方的深度。\n5.  **加权混合**：所有接触点在每个像素上的加权颜色被累加，然后通过总权重进行归一化，得到最终的接触溅射图像 **I**(u, v)。这个过程类似于一个软Z缓冲器，实现了密集的、图像化的接触监督信号。\n\n### 核心模块二：世界模型架构（ChronoDreamer）\n模型采用基于离散令牌的空间-时间Transformer架构。\n1.  **视频编码器**：使用Nvidia Cosmos Tokenizer，这是一个分层卷积网络，将256×256的RGB或接触图像编码为32×32的连续特征图。随后通过**有限标量量化（FSQ）**，将6个连续通道各自独立量化到固定离散值，组合起来产生一个范围在[0, V-1]（V≈65,536）的离散令牌。FSQ是一种非学习的、完全可微的量化方法。为了减少参数量，大词汇表被因子化为两个子词汇表（各512大小），令牌嵌入是这两个子嵌入的和。\n2.  **输入表示与条件作用**：\n    *   **输入模态**：历史视频令牌、历史动作、未来动作、历史关节角度。\n    *   **嵌入策略**：视频令牌使用因子化嵌入；动作和关节角度分别通过线性层投影并添加层归一化，然后相加形成每个帧的**组合控制令牌**。\n    *   控制令牌被预置到每个帧的视频令牌序列之前，形成一个形状为 (B, T, S+1, D) 的输入张量，其中S是空间令牌数，D是模型隐藏维度。\n    *   添加可学习的位置编码以注入时空位置信息。\n\n![ST-Transformer架构](https://arxiv.org/html/2512.18619v1/images/dynamicsmodel.png)\n\n> **图2**：ST-Transformer架构。控制令牌（动作+关节嵌入）与视频令牌连接，并由N个轴向ST块处理。三个输出头：关节回归（MSE）、接触令牌和视频令牌（因子化交叉熵）。\n\n3.  **空间-时间Transformer解码器**：如图2所示，解码器由L=24个相同的ST块堆叠而成。每个块依次应用：\n    *   **空间自注意力**：在每个帧内部，所有S+1个令牌（包括控制令牌）之间进行双向注意力。这使得控制令牌可以与所有视觉令牌交互，反之亦然。\n    *   **时间自注意力**：在每个空间位置（包括控制令牌位置）上，跨所有时间步进行**因果**注意力。这确保了时间步t的预测只能依赖于当前及过去的信息。\n    *   **前馈网络**：一个标准的两层MLP，带有GELU激活和Dropout。\n    这种**因子化注意力**设计（先空间后时间）将计算复杂度从联合时空注意力的O(T²S²)降低到O(TS² + T²S)。模型使用QK归一化和μP缩放来稳定训练。\n4.  **输出头**：Transformer的输出被送入三个独立的头：\n    *   **视频令牌预测头**：预测未来帧的视觉令牌（因子化交叉熵损失）。\n    *   **接触令牌预测头**：预测未来帧的接触令牌（因子化交叉熵损失）。\n    *   **关节角度回归头**：预测未来关节角度（均方误差损失）。\n\n### 核心模块三：MaskGIT训练\n模型使用掩码生成图像Transformer（MaskGIT）的目标进行训练。在训练时，未来帧（T_f帧）的一部分令牌被随机掩码（替换为特殊的掩码令牌）。模型的任务是基于历史上下文和未来动作，预测这些被掩码令牌的原始值。这迫使模型学习数据的内在生成动态。掩码策略包括标准的MLM掩码和一种“非MLM”策略，后者可能掩码整个帧以增强长期一致性学习。\n\n### 创新点总结\n1.  **新颖的接触表示**：将3D接触力渲染为深度加权的、相机对齐的高斯溅射图像，为视觉主干提供了密集的、图像化的物理监督。\n2.  **联合多模态预测**：单一模型同时预测视觉外观、接触分布和关节状态，为机器人规划提供了更全面的未来状态想象。\n3.  **因子化时空架构与MaskGIT训练**：结合了计算高效的因子化注意力与掩码预测训练目标，适用于长时程预测。\n4.  **LLM集成用于在线安全判断**：在推理回路中引入基于视觉语言模型的碰撞可能性推理器，实现对不安全动作序列的拒绝采样。\n\n## 实验与结果\n论文在**DreamerBench**数据集上训练和评估模型。这是一个使用Project Chrono生成的仿真数据集，提供了在刚体和可变形物体场景中同步的RGB图像、接触溅射图、本体感觉和物理标注。\n\n由于论文正文中未提供具体的定量数值结果（如成功率、准确率）或与基线方法的系统对比表格，以下主要基于论文中展示的定性结果进行分析。\n\n![场景0-自视视角](https://arxiv.org/html/2512.18619v1/images/world_model/db_0_ego.jpg) ![场景0-侧视视角](https://arxiv.org/html/2512.18619v1/images/world_model/db_0_side.jpg) ![场景0-接触溅射图](https://arxiv.org/html/2512.18619v1/images/world_model/db_0_splat.png)\n\n> **图3-5**：展示了模型在某个场景下的预测示例。左侧两列为历史帧（真实），右侧为预测的未来帧。预测的RGB帧（上排）和接触溅射图（下排）显示了模型能够生成空间连贯的视觉序列和对应的接触分布。\n\n![场景1-自视视角](https://arxiv.org/html/2512.18619v1/images/world_model/db_1_ego.jpg) ![场景1-侧视视角](https://arxiv.org/html/2512.18619v1/images/world_model/db_1_side.jpg) ![场景1-接触溅射图](https://arxiv.org/html/2512.18619v1/images/world_model/db_1_splat.png)\n\n> **图6-8**：另一场景的预测结果。接触溅射图清晰地显示了预测的接触力方向和大致区域（通过颜色和亮度），与视觉运动相符。\n\n![空间一致性示例1](https://arxiv.org/html/2512.18619v1/images/spatial_conherence/comic_001900_left_row2_then_full_right.png) ![空间一致性示例2](https://arxiv.org/html/2512.18619v1/images/spatial_conherence/comic_002300_left_row2_then_full_right.png)\n\n> **图15-16**：定性展示了模型在非接触运动期间保持空间一致性的能力。物体在移动过程中保持了形状和结构的连贯性。\n\n![接触预测示例1](https://arxiv.org/html/2512.18619v1/images/val_contact/comic_000750_left_row2_then_full_right.png) ![接触预测示例2](https://arxiv.org/html/2512.18619v1/images/val_contact/comic_012500_left_row2_then_full_right.png)\n\n> **图18-19**：定性展示了模型生成合理接触预测的能力。在发生交互的帧，接触溅射图预测出了相应的接触信号。\n\n![LLM判断尝试0](https://arxiv.org/html/2512.18619v1/images/attempt0.png) ![LLM判断尝试1](https://arxiv.org/html/2512.18619v1/images/attempt1.png)\n\n> **图20-21**：展示了基于LLM的碰撞判断器的工作示例。判断器接收预测的RGB帧和接触图，输出对轨迹是否可能发生碰撞的推理。例如，它成功识别出机械臂与自身可能发生碰撞（“可能发生自碰撞”）以及与环境障碍物发生碰撞（“机械手与绿色障碍物接触”）的情况。\n\n**关键实验结果总结（定性）**：\n1.  **空间一致性**：在非接触运动阶段，模型预测的视频帧能保持物体和场景结构的连贯性，未出现物体扭曲或无故消失等常见视频生成瑕疵。\n2.  **接触预测**：模型能够生成与动作相对应的、看似合理的接触分布图。接触溅射图在发生物理交互的时间和空间位置被激活，并显示了力方向。\n3.  **LLM碰撞判断**：集成的大型视觉语言模型能够基于预测的视觉和接触信息，对轨迹的安全性进行推理，区分碰撞与非碰撞轨迹，为在线拒绝采样提供了可能。\n\n论文未报告消融实验以量化每个组件（如接触编码、因子化注意力、LLM判断器）的具体贡献。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种创新的**接触编码方案**，将3D接触力渲染为深度加权的高斯溅射图像，为学习接触感知的世界模型提供了密集的、图像化的监督信号。\n2.  提出了**ChronoDreamer**，一个基于空间-时间Transformer的动作条件世界模型，能够联合预测未来的视觉帧、接触图和关节角度，并通过MaskGIT风格训练学习动态。\n3.  实现了**世界模型预测与基于LLM的碰撞判断器的集成**，为在线机器人规划提供了一种在动作执行前过滤不安全候选方案的新范式。\n4.  引入了**DreamerBench**，一个包含刚体和可变形物体接触丰富操作的仿真数据集，为相关研究提供了资源。\n\n**局限性**：论文明确指出，模型仅在仿真数据（DreamerBench）上训练。将其应用于真实机器人需要解决领域适应问题，以弥合仿真与现实之间的差距。\n\n**对后续研究的启示**：\n1.  **多模态预测的价值**：联合预测视觉、接触和本体感觉信号，比仅预测视觉更能捕获对机器人规划至关重要的物理状态，这代表了一个有前景的研究方向。\n2.  **LLM/VLM作为物理推理器**：利用大型视觉语言模型的常识推理能力来评估预测轨迹的物理合理性和安全性，为增强学习型世界模型的可靠性提供了一种轻量级、可扩展的途径。\n3.  **从仿真到现实的迁移**：如何将基于高质量仿真数据训练的世界模型有效迁移到真实世界，是该方法走向实际应用的关键挑战，也是未来工作的重点。",
      "imageUrls": [
        "https://arxiv.org/html/2512.18619v1/images/architecture.png",
        "https://arxiv.org/html/2512.18619v1/images/dynamicsmodel.png",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_0_ego.jpg",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_0_side.jpg",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_0_splat.png",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_1_ego.jpg",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_1_side.jpg",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_1_splat.png",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_2_ego.jpg",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_2_side.jpg",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_2_splat.png",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_3_ego.jpg",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_3_side.jpg",
        "https://arxiv.org/html/2512.18619v1/images/world_model/db_3_splat.png",
        "https://arxiv.org/html/2512.18619v1/images/spatial_conherence/comic_001900_left_row2_then_full_right.png",
        "https://arxiv.org/html/2512.18619v1/images/spatial_conherence/comic_002300_left_row2_then_full_right.png",
        "https://arxiv.org/html/2512.18619v1/images/spatial_conherence/comic_011500_left_row2_then_full_right.png",
        "https://arxiv.org/html/2512.18619v1/images/val_contact/comic_000750_left_row2_then_full_right.png",
        "https://arxiv.org/html/2512.18619v1/images/val_contact/comic_012500_left_row2_then_full_right.png",
        "https://arxiv.org/html/2512.18619v1/images/attempt0.png",
        "https://arxiv.org/html/2512.18619v1/images/attempt1.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.18007",
      "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
      "url": "http://arxiv.org/abs/2512.18007",
      "arxivId": "2512.18007",
      "date": "2025-12-19",
      "authors": "Juan Carlos Niebles Team",
      "category": "Manipulation",
      "summary": "本文针对机器人视觉-语言-动作模型缺乏预测性运动推理能力的问题，提出一种联合学习运动图像扩散的新策略。方法采用双头设计：动作头预测动作序列，运动头作为扩散变换器预测基于光流的未来运动图像，两者通过共享的VLM骨干进行联合训练，使模型能耦合运动知识与控制表示。实验表明，该方法将π-series VLA在LIBERO基准上的成功率提升至97.5%，在RoboTwin基准上达58.0%，真实世界性能提高23%，显著增强了VLA的运动推理能力。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型已成为通用机器人操作的强大范式，通过将多模态观察和指令直接映射到动作来模仿专家轨迹。然而，这些模型通常缺乏对未来动力学的显式推理机制，这限制了它们对动作决策的推理能力，并最终制约了其在新任务、场景和具身形态上的泛化能力。近期工作试图通过引入关键点轨迹、光流等显式运动表征，或将VLA与预测未来图像的世界模型相结合来增强运动感知。但前者往往仅在预训练阶段使用运动信号，未与策略优化紧密耦合；而后者侧重于场景外观的预测，而非运动动力学的学习，且难以无缝集成到现有高性能VLA框架中。\n\n本文针对VLA模型缺乏显式运动推理能力的痛点，提出了一种新颖的联合学习策略。其核心思路是：在保持标准VLA推理流程不变的前提下，通过一个并行的运动头（基于扩散Transformer）预测基于光流的运动图像，并与动作头共享VLM主干进行联合训练，从而将密集的像素级动态监督与稀疏的动作监督相结合，增强模型的运动推理能力。\n\n## 方法详解\n本文提出的方法旨在通过联合学习运动图像扩散来增强预训练的VLA模型。整体框架在标准VLA架构上增加了一个并行的运动头，形成双头设计。\n\n![方法总览](https://arxiv.org/html/2512.18007v1/x1.png)\n> **图1**：方法整体概览。本文提出的联合学习策略无缝地扩展了现有的大规模VLA模型，通过一个共享的VLM主干同时学习运动和动作，增强了运动推理能力，并保持了与标准VLA模型相同的推理流程。\n\n具体而言，给定视觉观测 o_t 和语言指令 l，共享的VLM主干编码得到多模态表示 z_t。在此基础上，两个并行的头部进行预测：\n1.  **动作头** π_θ：与原始VLA一致，预测未来k步的动作块 A_t ∈ R^{k×d}。\n2.  **运动头** μ_ψ：实现为一个轻量级的扩散Transformer（DiT），预测潜在运动令牌 m_t。这些令牌随后通过一个冻结的VAE解码器 f 解码为运动图像 M_t。\n\n![框架细节](https://arxiv.org/html/2512.18007v1/x2.png)\n> **图2**：联合学习VLA与运动图像扩散的框架概述。展示了共享的VLM主干、并行的动作头和运动头（DiT），以及运动令牌通过冻结VAE解码为运动图像的过程。\n\n两个头部采用相同的流匹配（Flow Matching）损失进行联合训练。对于目标信号 X_t（可以是动作 A_t 或运动令牌 m_t），通过线性插值在干净目标与高斯噪声之间构建噪声样本 X_t^τ。每个头部的目标是预测一个速度场 v(X_t^τ, o_t)，以匹配目标流 u(X_t^τ|X_t)。总损失为动作损失和运动损失之和：L = L_action + L_motion。通过共享主干耦合这两个互补的目标，模型能够学习更具时间一致性和物理基础的表征。\n\n**关键技术细节与创新点**：\n*   **运动监督信号**：采用基于图像的光流（流图像）作为运动表征。使用RAFT计算训练数据中观测对 (o_t, o_t+k) 之间的真实光流，并将其转换为3通道RGB图像，以确保与输入观测的空间分辨率一致，并与动作块的时间窗口对齐。\n*   **潜在空间扩散**：为避免直接在高维光流图像上扩散带来的计算成本和不稳定问题，方法使用一个冻结的VAE将光流图像编码到紧凑的潜在空间（尺寸为 4 × H/8 × W/8）。运动头学习生成这些运动令牌，这降低了空间冗余、稳定了去噪过程，并使得运动头能与动作头在共享主干下进行一致的联合优化。\n*   **训练流程**：采用两阶段训练。首先，在DROID数据集上使用光流对运动头进行预训练（热身阶段），仅优化运动头参数。随后，解冻整个架构（VAE编解码器仍冻结），对两个头部进行联合训练。\n*   **核心创新**：与现有方法相比，本文的创新在于提出了一个**可无缝集成到现有VLA中的并行双头架构**，并论证了**基于光流的运动图像**作为一种密集、物理基础且与控制对齐的监督信号，在联合学习中的有效性。该方法不改动推理路径，保持了实时效率。\n\n## 实验与结果\n实验在仿真和真实世界环境中进行，使用了两个主要基准测试：\n1.  **LIBERO基准**：包含Spatial, Object, Goal, Long四个测试套件的大规模仿真操纵基准，每个套件10个任务，评估空间泛化、物体级迁移、语义目标理解和长视野规划能力。\n2.  **RoboTwin基准**：专注于双臂操纵的基准，包含7个任务，并在简单（域内）和困难（域随机化）两种设置下进行评估。\n\n对比的基线方法包括：Diffusion Policy (DP)、Octo、OpenVLA、SpatialVLA、WorldVLA、FlowVLA、π0-FAST以及π系列模型（π0, π0.5）。\n\n**关键实验结果**：\n在LIBERO基准上（表1），本文方法进一步提升了π系列模型的性能。使用联合学习的π0.5模型取得了97.5%的平均成功率，在最具挑战性的Long套件上比原始π0.5提升了4.0%。联合学习的π0模型也达到了94.7%的平均成功率，优于大多数基线。\n\n| Model | Spatial | Object | Goal | Long | Average |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| ... | ... | ... | ... | ... | ... |\n| **π₀.₅** [ intelligence2025pi_ ] | 98.8% | 98.2% | 98.0% | 92.4% | 96.9% |\n| **Ours (π₀.₅)** | **99.4%** | **99.2%** | **96.0%** | **96.2%** | **97.5%** |\n\n> **表1**：LIBERO基准上各套件的成功率。本文的联合学习策略使π₀和π₀.5模型取得了领先性能。\n\n在RoboTwin基准上（表2），基于π0的本文方法在简单设置下平均成功率达到58.0%，比原始π0提升了12.9%，并且在大多数任务上优于其他模仿学习基线，展示了更强的鲁棒性和对长视野时序协调任务的改进。\n\n**消融实验与分析**：\n本文深入比较了不同运动表征对联合学习效果的影响。\n\n![不同运动表示比较](https://arxiv.org/html/2512.18007v1/x3.png)\n> **图3**：不同运动表征概览。比较了语言描述、未来图像和运动图像（光流）三种用于联合学习的运动表示。\n\n| Model | Supervision | Average |\n| :--- | :--- | :--- |\n| **π₀.₅** | action only | 96.9% |\n| **π₀.₅** | action + language | 95.1% |\n| **π₀.₅** | action + future image | 95.7% |\n| **π₀.₅** | **action + motion image** | **97.5%** |\n\n> **表3**：在LIBERO基准上探究不同运动表征的联合学习效果。运动图像（光流）取得了最佳的整体性能。\n\n结果表明（表3），**基于光流的运动图像**效果最佳。语言描述因其离散、低频的特性，监督效率低下；未来图像侧重于全局外观，在长视野任务上性能下降。而运动图像提供了密集且物理基础的表征，能直接关联观测运动与机器人控制。\n\n![定性可视化](https://arxiv.org/html/2512.18007v1/x4.png)\n> **图4**： rollout过程中预测的动作和运动的定性可视化。顶行显示观测帧及动作头预测的机器人动作，底行可视化运动头预测的光流图像。可见预测的运动与底层物理动力学（如抓取、敲击、平移）吻合良好。\n\n![数据效率](https://arxiv.org/html/2512.18007v1/x5.png)\n> **图5**：在LIBERO-10上的数据效率展示。联合学习运动图像扩散比仅学习动作具有更高的数据效率。\n\n![真实世界实验](https://arxiv.org/html/2512.18007v1/x6.png)\n> **图6**：真实世界实验。(a) 实验设置。(b) 办公室场景下桌面任务的评估结果。联合学习方法相比原始π0在真实世界任务上获得了23%的性能提升。\n\n此外，定性可视化（图4）显示模型能预测出时空一致的光流场，与任务物理动态对齐。数据效率实验（图5）表明联合学习提高了样本效率。真实世界实验（图6）验证了方法的有效性，在桌面操纵任务上相比原始π0获得了23%的性能提升。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  提出了一种**联合学习运动图像扩散的策略**，能够无缝增强VLA模型的运动推理能力，同时保持其实时推理效率。\n2.  设计了**运动图像扩散**的具体实现，采用DiT在潜在空间预测光流，为动作学习提供了互补的密集像素级动态监督，并论证了光流是联合动作-运动学习最有效的表征。\n3.  通过大量实验验证了方法的有效性，显著提升了π系列VLA模型在LIBERO（97.5%）和RoboTwin（提升23%）基准上的性能，并系统分析了不同运动表征的影响。\n\n论文提及的局限性主要在于对高质量光流计算（如RAFT）的依赖，这可能需要额外的计算或标注。此外，运动头的训练需要额外的数据（如DROID）进行预热。\n\n本文的启示在于，为数据驱动的机器人策略学习提供**密集且物理对齐的辅助监督信号**（如光流），是一种提升模型时序理解与泛化能力的有效途径。这种“并行头、联合训、单头推”的设计范式，为在不牺牲部署效率的前提下增强现有基础模型提供了可借鉴的思路。未来工作可探索更高效或自监督的运动表征学习方式，并进一步验证该方法在其他机器人平台和任务上的泛化性。",
      "imageUrls": [
        "https://arxiv.org/html/2512.18007v1/x1.png",
        "https://arxiv.org/html/2512.18007v1/x2.png",
        "https://arxiv.org/html/2512.18007v1/x3.png",
        "https://arxiv.org/html/2512.18007v1/x4.png",
        "https://arxiv.org/html/2512.18007v1/x5.png",
        "https://arxiv.org/html/2512.18007v1/x6.png",
        "https://arxiv.org/html/2512.18007v1/x7.png",
        "https://arxiv.org/html/2512.18007v1/x8.png",
        "https://arxiv.org/html/2512.18007v1/x9.png",
        "https://arxiv.org/html/2512.18007v1/x10.png",
        "https://arxiv.org/html/2512.18007v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.18068",
      "title": "SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning",
      "url": "http://arxiv.org/abs/2512.18068",
      "arxivId": "2512.18068",
      "date": "2025-12-19",
      "authors": "Axel Krieger Team",
      "category": "Manipulation",
      "summary": "本文解决从单目手术视频估计手术工具运动学数据的关键问题，以支持机器人模仿学习。提出SurgiPose方法，其核心技术是基于可微分渲染，通过优化工具姿态参数来最小化渲染图像与真实视频帧的差异，从而推断工具轨迹和关节角度。在da Vinci机器人上的组织提升与针拾取实验表明，使用视频估计的运动学数据训练的策略，其成功率与使用真实运动学数据训练的策略相当，验证了该方法的可行性。",
      "detailedSummary": "## 研究背景与动机\n当前，从内窥镜图像中精确估计关节化手术器械的6自由度位姿是机器人辅助微创手术中的基础挑战。现有方法主要依赖于标记物、立体视觉、深度传感或机器人运动学数据，但这些方法在应用于大规模在线单目手术视频时存在局限：标记物不切实际，立体视觉和深度传感需要额外硬件，而机器人运动学数据在公开的临床视频中通常不可得。尽管基于学习的方法显示出潜力，但许多方法仅关注非关节化工具或预测2D关键点，而非完整的6自由度位姿。更近期的方法探索了“渲染-比较”策略，但单纯依赖可微分渲染进行位姿估计容易因初始估计不佳而导致优化失败。\n\n本文针对从大规模在线单目手术视频中提取器械运动学数据这一具体痛点，提出了一种新视角：利用可微分渲染技术，在无需真实运动学数据的情况下，从视频中估计工具的轨迹和关节角度。本文的核心思路是：提出一个名为SurgiPose的两阶段框架，首先通过一个粗估计模块提供鲁棒的初始位姿猜测，然后利用可微分渲染迭代优化位姿和关节角，以最小化渲染图像与真实观察图像之间的差异，从而为手术机器人模仿学习提供可用的运动学数据。\n\n## 方法详解\nSurgiPose的整体框架是一个两阶段的位姿估计流程。输入是单目手术视频帧，输出是手术工具末端执行器相对于相机坐标系的6自由度位姿变换矩阵 $T_{CE} \\in SE(3)$ 以及关节角度 $\\mathbf{q} = [q_1, q_2, q_3]^\\top$（分别对应工具俯仰角和两个钳口的开合角）。第一阶段（粗估计）为视频第一帧生成一个初始位姿估计；第二阶段（细化）通过可微分渲染对该估计进行优化。对于视频序列，处理完第一帧后，将细化后的位姿作为后续帧的初始化，并进行逐帧跟踪优化。\n\n![方法框架](https://arxiv.org/html/2512.18068v1/x2.png)\n> **图2**：SurgiPose流程概览。第一阶段（粗估计器）初始化位姿，然后通过可微分渲染进行细化。最终估计的6自由度位姿和关节角用于运动学提取和模仿学习。\n\n**核心模块一：粗位姿估计**\n此模块旨在解决可微分渲染优化对初始值敏感、易陷入局部最优的问题。它专门用于处理视频的第一帧，因为后续帧依赖于前一帧的跟踪结果。其具体流程是：首先使用SAM2分割并裁剪出手术工具，计算工具掩码的中心点。以此中心点为中心，在平行于图像平面的方向上构建一个3x3的方形网格。对于网格上的每个点，生成36个候选初始猜测，方法是在0到$2\\pi$范围内均匀采样，绕指向图像内部的z轴进行旋转（不考虑x、y轴旋转和深度变化，因为这些参数可在后续细化中解决）。然后，对每个候选猜测进行可微分渲染细化，并计算像素平均损失。最终选择损失最低的候选作为初始位姿 $T_{CE}^{initial}$。\n\n![粗估计流程](https://arxiv.org/html/2512.18068v1/x3.png)\n> **图3**：粗估计流程可视化。(a)原始视频帧。(b)分割并裁剪后的手术工具，显示掩码中心及用于生成潜在初始猜测的3x3网格。(c)粗估计模块提出的粗估计。(d)基于细化后估计中损失最低的原则选择最佳初始猜测。\n\n**核心模块二：可微分渲染与优化**\n该模块用于细化初始位姿并估计关节角。首先，在MuJoCo仿真中使用手术工具的URDF模型生成合成数据集，包含500个关节处于中立位的“规范”工具位姿和10,000个随机采样关节角（避免自碰撞）的“位姿条件”工具配置，每个配置从12个随机摄像机视角渲染。随后，采用与现有工作类似的三阶段训练方法学习工具的可微分高斯溅射模型：1) 从多视角图像学习工具的规范3D高斯表示；2) 引入变形场以建模不同关节配置引起的形状变化；3) 联合优化规范模型和变形场。\n\n在测试时（即对真实视频进行位姿估计），优化器以 $T_{CE}^{initial}$ 和初始关节角为起点，通过最小化渲染图像 $I_{ren}$ 与观测图像 $I_{obs}$ 之间的差异来迭代更新位姿和关节角。损失函数结合了结构相似性(SSIM)和均方误差(MSE)：$L_{\\text{combined}} = \\alpha(1-\\text{SSIM}(I_{\\text{ren}}, I_{\\text{obs}})) + (1-\\alpha)\\|I_{\\text{ren}} - I_{\\text{obs}}\\|_{2}^{2}$，其中 $\\alpha=0.8$。\n\n位姿更新采用特定的梯度更新规则：旋转矩阵 $R_{\\text{updated}} = R_{\\text{current}} \\cdot (\\mathbf{I} + \\alpha\nabla R)$，平移向量 $t_{\\text{updated}} = t_{\\text{current}} - \\beta \\cdot \\text{clamp}(\nabla t, -\\delta, \\delta)$，其中 $\\alpha=0.3, \\beta=3\\times10^{-4}, \\delta=0.02$，以确保数值稳定性。关节角通过梯度下降更新：$\\mathbf{q}_{\\text{updated}} = \\mathbf{q}_{\\text{current}} - \\gamma \\cdot\nabla_{\\mathbf{q}}L_{\\text{combined}}$ ($\\gamma=10^{-3}$)，并利用 $\\text{clamp}$ 函数强制执行关节角度限制。\n\n**与现有方法的创新点**\n本文的核心创新在于将**粗估计模块**与**可微分渲染细化模块**相结合。与单纯依赖可微分渲染的方法相比，这种两阶段策略通过系统性地评估多个初始假设，显著提高了优化的鲁棒性和收敛到正确位姿的可能性，特别是在视频序列的起始帧。\n\n## 实验与结果\n**实验设置与基准**：实验平台为da Vinci Research Kit Si (dVRK Si)，使用大型持针器作为末端执行器。主要在两个机器人手术任务上进行评估：组织提升和针拾取。收集了220次组织提升和224次针拾取的演示数据，包含同步视频和真实运动学数据。此外，还在公开的SurgRIPE数据集和自收集的离体胆囊切除术数据集上进行了泛化性评估。对比的基线是基于真实运动学数据训练的模仿学习策略。\n\n**关键实验结果**：\n1.  **轨迹提取与回放**：定性实验表明，从视频中提取的运动学轨迹能够成功引导机器人复现组织提升任务。\n\n![轨迹回放结果](https://arxiv.org/html/2512.18068v1/x4.png)\n> **图4**：轨迹回放实验结果。前三张图显示机器人执行估计轨迹的关键瞬间，绿色掩码覆盖层代表原始视频中的对应工具位姿。(d) 将我们管道估计的末端执行器轨迹与通过正向运动学获得的真实轨迹进行比较。\n\n2.  **运动学估计精度**：如表I所示，对于组织提升和针拾取任务，平均位移误差(ADE)分别为9.7毫米和12.0毫米，最终位移误差(FDE)分别为15.3毫米和14.4毫米。误差主要分布在深度方向（z轴），平均误差分别为6.64毫米和5.61毫米，这凸显了单目深度估计的挑战。\n\n3.  **模仿学习策略性能**：如表II所示，使用估计运动学训练的模仿学习策略在组织提升任务上成功率为70%（7/10），在针拾取任务上为60%（6/10）。虽然略低于使用真实运动学训练的策略（组织提升100%，针拾取80%），但证明了其可行性。失败案例多与深度估计不准确相关，例如在组织提升中机器人倾向于推开组织而非抬起。\n\n![模仿学习定性结果](https://arxiv.org/html/2512.18068v1/x5.png)\n> **图5**：模仿学习实验的定性结果：比较使用真实运动学和估计运动学训练的策-略在组织提升实验关键时刻的快照。上图显示使用真实运动学训练的策-略，下图显示使用直接从视频估计的运动学训练的策-略，均展示了初始位姿、抓取组织和提升组织三个时刻。\n\n**消融实验与组件贡献**：虽然没有独立的消融实验表格，但论文通过两阶段设计本身（对比单纯可微分渲染）以及实验结果表明，**粗估计模块**对于提供鲁棒初始化、防止优化失败至关重要。而**可微分渲染细化模块**则负责精确估计6自由度位姿和关节角度。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个从单目手术视频生成大规模运动学数据的框架，减少了对运动捕捉系统的依赖，为互联网规模的手术机器人学习奠定了基础。\n2.  提出了一种新颖的单目6自由度位姿估计方法，将粗估计与可微分渲染相结合，其中粗估计器提供了关键的位姿初始化，提高了鲁棒性和准确性。\n3.  通过实验证明，使用估计运动学训练的模仿学习策略能够取得与基于真实数据训练的策略相当的成功率，验证了该框架的可行性。\n\n**局限性**：\n1.  深度估计（z轴）精度相对较低，这是单目视觉固有的挑战，可能导致任务执行时出现推送而非抓取等错误。\n2.  方法依赖于SAM2进行工具分割，分割质量会影响后续估计。\n3.  目前主要针对特定工具模型进行了验证，尽管在离体数据上展示了初步泛化能力，但大规模泛化仍需进一步研究。\n\n**对后续研究的启示**：\n1.  本文的工作为实现从海量在线手术视频中直接学习机器人控制策略铺平了道路，是构建大规模手术视觉-语言-动作模型的关键一步。\n2.  未来工作可以专注于改进深度估计，例如通过引入时间一致性约束或从大规模数据集中学习先验知识。\n3.  探索对更多样化手术工具和复杂场景的泛化能力，将是推动该方法走向临床实际应用的重要方向。",
      "imageUrls": [
        "https://arxiv.org/html/2512.18068v1/x1.png",
        "https://arxiv.org/html/2512.18068v1/x2.png",
        "https://arxiv.org/html/2512.18068v1/x3.png",
        "https://arxiv.org/html/2512.18068v1/x4.png",
        "https://arxiv.org/html/2512.18068v1/x5.png",
        "https://arxiv.org/html/2512.18068v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.17899",
      "title": "Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy",
      "url": "http://arxiv.org/abs/2512.17899",
      "arxivId": "2512.17899",
      "date": "2025-12-19",
      "authors": "Alberto Speranzon Team",
      "category": "Manipulation",
      "summary": "本文针对模仿学习（IL）在自主系统中因分布偏移（包括策略误差、外生干扰及模型不确定性引起）而导致性能下降的核心问题，提出了一种可认证的分层控制架构。关键技术整合了两种互补方法：泰勒级数模仿学习（TaSIL）用于抵御策略误差引起的分布偏移，L1分布鲁棒自适应控制（L1-DRAC）用于处理随机性与认知不确定性引起的分布偏移。通过构建分布鲁棒模仿策略（DRIP）架构，并精心设计各层的输入输出要求，论文论证了该架构能够为整个学习与控制流程提供可证明的保证，从而为实现全栈可认证的自主系统铺平了道路。",
      "detailedSummary": "## 研究背景与动机\n模仿学习（IL）因其能从专家演示中直接学习控制策略而成为实现自主行为的有力框架，相比强化学习（RL）具有更高的样本效率。然而，IL面临一个根本性局限：对分布偏移（distribution shift）的敏感性。当在真实系统上部署基于IL的反馈律时，存在两种主要的分布偏移来源：一是由策略误差（模仿专家不完美）引起的分布偏移；二是由外生干扰以及因学习不充分导致的内生模型误差（认知和偶然不确定性）引起的分布偏移。现有方法存在关键局限性：大多数方法依赖于不可验证的假设（如需要交互式专家或模拟器），或仅针对特定类型的分布偏移提供有限的、系统特定的保证，缺乏可认证的性能。本文针对在存在策略误差和系统不确定性的情况下，实现可认证的IL这一具体痛点，提出了一个分层控制架构（Layered Control Architecture, LCA）的新视角。其核心思路是，将分别处理策略诱导分布偏移的泰勒级数模仿学习（TaSIL）与处理不确定性诱导分布偏移的ℒ₁分布鲁棒自适应控制（ℒ₁-DRAC）相结合，形成一个统一的、可提供先验证书的分布鲁棒模仿策略（DRIP）架构。\n\n## 方法详解\n本文提出的分布鲁棒模仿策略（DRIP）架构是一个三层控制架构，旨在将学习组件（模仿学习策略）与基于模型的、可认证的决策组件（鲁棒自适应控制器）集成。\n\n![方法框架](https://arxiv.org/html/2512.17899v1/Figures/Arch.png)\n> **图1**：集成TaSIL和ℒ₁-DRAC的分层控制架构示意图，其中X_t代表系统状态。在该架构中，TaSIL作为中层控制器，为低层的ℒ₁-DRAC生成参考指令。\n\n整体框架的输入是系统状态，输出是最终的控制指令。架构分为三层：\n1.  **高层（专家）**：提供期望的（名义上的）行为轨迹x_t*，由专家策略π*在名义动力学模型上生成。\n2.  **中层（TaSIL）**：学习策略π_IL，其目标是模仿专家策略π*。但与传统行为克隆不同，TaSIL通过损失函数增强，显式地建模策略误差对闭环系统未来状态的影响（通过动力学模型的局部泰勒展开），从而学习对策略诱导分布偏移具有鲁棒性的策略。\n3.  **低层（ℒ₁-DRAC）**：自适应控制律π_ℒ1，其总控制指令为π_ad = π_IL + π_ℒ1。该控制器的核心作用是保证系统在存在不确定性（Λ_μ, Λ_σ）的情况下，其真实轨迹X_t能够紧密跟踪中层TaSIL产生的名义轨迹x_t‘。ℒ₁-DRAC基于ℒ₁自适应控制架构，能够在线快速估计并补偿不确定性，从而“强制执行”TaSIL的预期名义行为。\n\n![方法细节](https://arxiv.org/html/2512.17899v1/Figures/HighLevel.png)\n> **图2**：方法原理示意图。上半部分：用于模仿学习的专家轨迹由在不确定（真实）系统上运行的未知专家输入过程生成。下半部分：TaSIL设计用于抵抗因策略差异引起的分布偏移，而ℒ₁-DRAC设计用于抵抗模型不准确引起的分布偏移。\n\n核心模块的技术细节与创新点：\n*   **TaSIL模块**：其创新在于将系统动力学的敏感性信息融入模仿学习的目标函数中。通过惩罚在闭环动力学下会放大的误差方向，直接针对策略误差引起的分布偏移进行鲁棒化。它利用了已知系统的输入到状态稳定性（ISS）特性。\n*   **ℒ₁-DRAC模块**：其创新在于提供了在概率测度（分布）空间上的鲁棒性证书。具体而言，它保证了真实系统状态分布𝕏_t与名义分布x_t‘之间的距离（在Wasserstein度量下）被有界地包含在一个模糊集（ambiguity set）内。这种证书是**按设计保证**且**无需样本**的。\n*   **架构级创新**：本文的关键创新在于将TaSIL和ℒ₁-DRAC**解耦**并集成为LCA。二者单独使用时均无法应对所有分布偏移源。通过分层组合，TaSIL处理策略误差，ℒ₁-DRAC处理模型与干扰不确定性，实现了优势互补，并为整个控制流水线提供了统一的、可认证的鲁棒性。此外，由于ℒ₁-DRAC无需训练，整个DRIP的合成**无需改变TaSIL的训练过程**，实现了“训练一次，使用TaSIL”。\n\n## 实验与结果\n本文在两个仿真环境中验证了DRIP架构的性能：经典控制任务CartPole（倒立摆）和更复杂的2D Quadrotor（二维四旋翼）轨迹跟踪任务。\n\n对比的基线方法包括：标准行为克隆（BC）、数据集聚合（DAgger）以及单独的TaSIL。实验评估了在存在系统不确定性（如模型参数误差、未建模动力学）和随机干扰情况下的性能。\n\n关键实验结果如下表所示（数值基于论文描述归纳）：\n*   **CartPole任务**：DRIP实现了接近100%的成功率，显著高于BC、DAgger和单独的TaSIL。其跟踪误差（与专家轨迹的偏差）也最小。\n*   **2D Quadrotor任务**：在存在风扰和模型不确定性的情况下，DRIP在轨迹跟踪精度和稳定性方面均优于所有基线方法，展示了其处理复杂不确定性分布偏移的能力。\n\n![定性结果1](https://arxiv.org/html/2512.17899v1/Figures/IL_L1_DRAC_figures_Policy_induced_shift.png)\n> **图3**：策略诱导分布偏移的定性说明。展示了专家轨迹、仅使用TaSIL（无ℒ₁）的轨迹以及DRIP的轨迹对比，凸显了策略误差累积的影响以及DRIP的改善。\n\n![定性结果2](https://arxiv.org/html/2512.17899v1/Figures/IL_L1_DRAC_figures_Uncertainty_induced_distribution_shift.png)\n> **图4**：不确定性诱导分布偏移的定性说明。对比了名义（专家）轨迹、受不确定性影响的真实系统轨迹，以及由ℒ₁-DRAC补偿后的轨迹，显示了ℒ₁控制器在抑制不确定性影响方面的有效性。\n\n![消融实验](https://arxiv.org/html/2512.17899v1/Figures/IL_L1_DRAC_figures-DRIP.png)\n> **图5**：DRIP架构的消融研究。比较了BC、TaSIL、TaSIL+ℒ₁（即DRIP）在存在不确定性的情况下的状态轨迹。结果清晰表明，单独的TaSIL或BC在不确定性下会失效，而结合了ℒ₁-DRAC的DRIP能够成功恢复并稳定跟踪。\n\n![控制器结构](https://arxiv.org/html/2512.17899v1/Figures/L1DRAC_Arch_Alt.png)\n> **图6**：ℒ₁-DRAC控制器的详细架构图，展示了状态预测器、自适应律和低通滤波器组件，说明了其如何实时估计和补偿不确定性。\n\n![统计结果](https://arxiv.org/html/2512.17899v1/Figures/stats3.png)\n> **图7**：2D Quadrotor实验中，不同方法在多次运行下的性能统计箱线图。DRIP（橙色）在跟踪误差上表现出更低的均值和中位数，以及更小的方差，证明了其优越且一致的鲁棒性能。\n\n**消融实验总结**：实验通过对比BC、TaSIL和完整的DRIP，验证了每个组件的贡献。TaSIL相比BC能更好地处理策略误差，但在面对系统不确定性时仍会失败。ℒ₁-DRAC的加入使得系统能够抵御不确定性，从而让TaSIL学习到的策略得以正确执行，实现了整体性能的显著提升。\n\n## 总结与启发\n本文的核心贡献包括：\n1.  **提出了可认证的分布鲁棒IL架构**：首次将处理策略误差的TaSIL与处理系统不确定性的ℒ₁-DRAC通过分层控制架构（LCA）原则性地结合起来，形成了分布鲁棒模仿策略（DRIP），为学习策略在不确定动力学系统上的部署提供了端到端的鲁棒性证书。\n2.  **提供了分布层面的鲁棒性保证**：低层的ℒ₁-DRAC控制器能够在Wasserstein度量下，将真实状态分布约束在名义分布周围的一个模糊集内，这种证书是按设计保证且无需数据的。\n3.  **实现了解耦与可组合性**：架构允许独立训练模仿学习策略（TaSIL），然后通过鲁棒自适应控制器（ℒ₁-DRAC）赋予其鲁棒性，无需重新训练或使用对抗训练等数据驱动技术来应对不确定性。\n\n**论文提到的局限性**：分析依赖于对名义模型动力学（f, g）的某些假设（如Lipschitz连续性、g满秩），并且初始状态分布被假定为有紧支集。这些假设可能在某些非常复杂的系统中不成立。\n\n**对后续研究的启示**：DRIP架构为将高性能但缺乏保证的数据驱动组件（如高维感知系统）与可认证的基于模型的决策模块集成开辟了道路。未来的工作可以探索将该框架扩展到更广泛的系统类别（如欠驱动、非光滑系统），并将分布鲁棒性证书进一步向后端传播，以构建完全可认证的自主系统流水线。",
      "imageUrls": [
        "https://arxiv.org/html/2512.17899v1/Figures/Arch.png",
        "https://arxiv.org/html/2512.17899v1/Figures/HighLevel.png",
        "https://arxiv.org/html/2512.17899v1/Figures/IL_L1_DRAC_figures_Policy_induced_shift.png",
        "https://arxiv.org/html/2512.17899v1/Figures/IL_L1_DRAC_figures_Uncertainty_induced_distribution_shift.png",
        "https://arxiv.org/html/2512.17899v1/Figures/IL_L1_DRAC_figures-DRIP.png",
        "https://arxiv.org/html/2512.17899v1/Figures/L1DRAC_Arch_Alt.png",
        "https://arxiv.org/html/2512.17899v1/Figures/stats3.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.17853",
      "title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning",
      "url": "http://arxiv.org/abs/2512.17853",
      "arxivId": "2512.17853",
      "date": "2025-12-19",
      "authors": "Karl Schmeckpeper Team",
      "category": "Manipulation",
      "summary": "论文提出AnyTask框架，旨在解决机器人学习中仿真到现实策略学习的数据收集成本高、任务设计人力密集的核心问题。该框架结合大规模并行GPU仿真与基础模型，自动生成多样化操作任务和机器人数据，关键技术包括ViPR（VLM辅助并行细化的任务规划）、ViPR-Eureka（生成密集奖励的强化学习）和ViPR-RL（稀疏奖励下的混合规划学习）三个代理。实验结果表明，基于生成数据训练的行为克隆策略可直接部署到真实机器人，在拾放、开抽屉等任务中泛化至新物体姿态，平均成功率达到44%。",
      "detailedSummary": "## 研究背景与动机\n机器人通用学习长期以来受限于数据：在现实世界中收集大规模、多样化、高质量的交互数据成本高昂。仿真已成为扩展数据收集的一种有前景的途径，但相关的任务设计、面向任务的场景生成、专家演示合成以及仿真到真实的迁移，仍然需要大量的人工努力。现有利用基础模型进行机器人仿真的工作，要么在任务设计和演示收集上仍需大量人力，要么难以实现仿真到真实的迁移。本文旨在解决这一系列挑战，提出了AnyTask框架，其核心思路是利用大规模并行的GPU仿真引擎与基础模型（LLMs/VLMs）相结合，从高级文本目标出发，自动化地生成多样化、高质量的场景、任务和专家演示数据，以训练可直接部署到真实世界的视觉运动策略。\n\n## 方法详解\nAnyTask框架旨在从对象数据库和高级任务类型（如“抓放”）出发，自动生成文本任务描述和对应的可执行仿真代码，进而通过智能体高效收集数据。\n\n![方法框架](https://arxiv.org/html/2512.17853v2/figures_and_tables/draft_main_figure.jpg)\n\n> **图2**：AnyTask整体框架概览。首先从对象数据库和高级任务生成模拟操作任务；然后流程自动提出任务描述、生成仿真代码，并利用ViPR、ViPR-RL和ViPR-Eureka等智能体在大规模并行仿真环境中高效收集数据；应用在线领域随机化以确保场景和视觉观测的多样性；最后使用模拟数据训练策略并零样本迁移到现实世界。\n\n框架包含几个核心组件：\n1.  **对象数据库**：存储物体的名称、颜色、纹理、材料、边界框、关节信息等元数据。通过渲染物体并从多视角使用VLM（GPT-4o）进行视觉属性标注，结合Sentence-T5-Large生成句子级嵌入，并使用faiss建立索引以支持自然语言查询。\n2.  **任务生成器**：接收高级任务和场景信息（如任务族、机器人规格、工作空间约束），在LLM的帮助下提出具体任务和涉及的对象。支持两种变体：基于对象的任务生成（先采样对象，再生成任务）和基于任务的对象提议（先由LLM提议适合任务的对象，再从数据库检索）。\n3.  **仿真生成器**：将任务描述和对象作为输入，生成可在IsaacLab仿真器中执行的代码。LLM被要求生成五个关键函数的代码：`reset()`（随机化场景）、`check_success()`（定义任务目标条件）、`compose_state()`（为RL策略提供状态表示）、`reward_function()`（RL奖励函数初始版本）和`scripted_policy()`（定义用于数据收集的专家策略）。为确保一致性，先生成`check_success()`，并用它来指导其他函数的生成。\n4.  **密集标注系统**：通过API `log_step()`，允许LLM在策略执行期间随时调用，并利用其他环境API决定包含哪些信息，从而自动生成总结动作前后环境状态的密集自然语言标注，为后续策略优化提供支持。\n\n为合成多样化任务的机器人轨迹，论文引入了三种基于任务与运动规划（TAMP）和强化学习（RL）的AnyTask智能体：\n*   **ViPR**：一种新颖的TAMP智能体，采用**V**LM-**i**n-the-loop **P**arallel **R**efinement（VLM在环并行细化）。它首先使用LLM生成调用参数化技能API（如`move_to`, `grasp`）的任务-运动计划（Python程序）。为解决LLM空间理解不精确的问题，ViPR并行执行K个计划 rollout，收集视频和密集标注，由VLM评估每个episode并给出反馈和成功/失败判断。聚合这些判断后，VLM会迭代地细化原始计划，输出更新后的计划。\n*   **ViPR-Eureka**：一种改进的RL智能体，结合了生成的密集奖励和**LLM引导的接触采样**。其核心创新是一个基于网格的接触采样算法，用于生成高质量的抓取候选。该算法在物体网格上采样三角形，通过重心插值确定接触位置，并在由VLM或用户提供的预定义夹爪z轴周围随机采样夹爪方向，同时采用拒绝采样机制排除碰撞或无效的候选。\n*   **ViPR-RL**：一种混合TAMP+RL方法。对于每个子任务，它使用运动规划将夹爪移动到由上述抓取采样器采样的目标物体部件附近，然后调用训练好的基于物体的RL技能。RL技能使用LLM生成的简单成功检查器作为奖励函数进行训练。\n\n在基础设施方面，AnyTask采用**两阶段数据收集管道**：第一阶段，智能体在不渲染的情况下尝试任务，仅存储成功轨迹的模拟器状态；第二阶段，重放保存的状态并启用渲染，以生成包含RGB图像、点云等用于模仿学习的完整数据集。这种方法（尤其是动作重放）显著提高了数据生成吞吐量。\n\n## 实验与结果\n实验评估了代码生成质量、任务多样性、数据生成成功率与速度、仿真及真实世界策略性能。\n\n**代码可运行性**：测试了不同LLM生成仿真代码的能力。使用改进后的提示，o3-mini达到了96%的代码可运行率（表II）。常见错误多发生在需要处理物体放置和空间变换逻辑的`reset()`函数中。\n\n**任务多样性**：使用self-BLEU分数评估生成任务描述的多样性。在对比RoboGen、RLBench和GenSim2的系统中，AnyTask的self-BLEU分数最低（0.352），表明其任务描述多样性最佳（表III）。\n\n**智能体生成演示的成功率**：在超过400个涵盖提起、抓放、推、堆叠、开抽屉等任务上测试。如表IV所示，不同智能体擅长不同任务：ViPR（经细化后）在多步骤任务上表现最佳（如堆叠任务成功率44%）；ViPR-Eureka在接触丰富的任务（如推任务成功率60%）和抓取复杂物体上表现出色；ViPR-RL在需要混合行为（如开抽屉成功率33%）的任务上有效。智能体集合能解决最多任务（例如，抓放任务集合成功率87%）。\n\n![ViPR改进](https://arxiv.org/html/2512.17853v2/figures_and_tables/vipr_improvement.png)\n\n> **图3**：ViPR细化效果。在301个任务上，使用ViPR（VLM在环并行细化）平均带来了12.8%的成功率提升。\n\n**接触采样的有效性**：消融实验表明，引入LLM引导的接触采样后，ViPR-Eureka在多个任务族上的平均数据收集成功率从37%提升至57%（表V）。\n\n**数据生成速度**：两阶段记录管道效率很高。在一个约36分钟的L4 GPU会话中，收集了500条演示。动作重放显著提升了数据生成吞吐量，在困难任务上（使用4个相机时）带来了四倍的加速（图4）。\n\n![动作重放加速](https://arxiv.org/html/2512.17853v2/x2.png)\n\n> **图4**：动作重放实现了更快的数据收集，在挑战性任务上尤其明显。\n\n**仿真策略性能**：使用生成的数据训练扩散策略并在仿真中评估（表VI）。ViPR数据在涉及长视野或多步骤过程的任务上表现更好，而包含RL的数据收集方法在涉及持续接触的任务上表现相等或更好。ViPR-Eureka数据虽然收集效率高，但更难提炼成行为克隆策略（例如在抓放任务中，它可能通过推物体而非抓取来“黑客”奖励系统）。\n\n**真实世界策略迁移**：使用ViPR为8个真实世界任务（图5）各收集1000条专家演示，训练基于点云的3D扩散策略，并零样本部署到实体机器人。\n\n![零样本仿真到真实评估](https://arxiv.org/html/2512.17853v2/x3.png)\n\n> **图5**：零样本仿真到真实策略评估的八个任务示例，包括抓放、开抽屉、接触式推动和长视野操作。\n\n在真实机器人上，面对新颖的物体姿态，策略在抓放、开抽屉、接触式推动和长视野操作等一系列任务上取得了平均44%的成功率（图5），证明了框架在实现仿真到真实迁移上的有效性。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了AnyTask，一个利用大规模并行GPU仿真和基础模型、从高级目标自动化生成机器人任务与数据的端到端框架，极大减少了人工干预；2) 引入了ViPR、ViPR-Eureka和ViPR-RL三种智能体，能够基于AnyTask大规模自动生成专家演示，并探索了TAMP、RL及其混合方法在不同任务上的适用性；3) 通过仿真和真实的实验验证了生成数据的效用，成功实现了策略的零样本仿真到真实迁移，并指出了领域随机化和策略架构等因素的关键作用。\n\n论文自身提到的局限性包括：代码生成严重依赖LLM的能力和提示工程；需要预先存在高质量的物体资产库来构建对象数据库；尽管智能体集合能解决更多任务，但仍有部分生成的任务无法被任何智能体解决。\n\n这项工作为自动化机器人数据生成和仿真到真实学习提供了新范式。其对后续研究的启示在于：展示了基础模型与大规模并行仿真结合的巨大潜力；强调了针对不同任务类型设计专用数据生成策略（如TAMP与RL结合）的重要性；其两阶段数据收集、密集标注、接触采样等基础设施设计，为构建更高效、鲁棒的自动化机器人学习系统提供了参考。",
      "imageUrls": [
        "https://arxiv.org/html/2512.17853v2/x1.png",
        "https://arxiv.org/html/2512.17853v2/figures_and_tables/draft_main_figure.jpg",
        "https://arxiv.org/html/2512.17853v2/figures_and_tables/vipr_improvement.png",
        "https://arxiv.org/html/2512.17853v2/x2.png",
        "https://arxiv.org/html/2512.17853v2/x3.png",
        "https://arxiv.org/html/2512.17853v2/x4.png",
        "https://arxiv.org/html/2512.17853v2/figures_and_tables/mobility_relabel_gapartnet_fixed_texture__front-right-x-reverse.png",
        "https://arxiv.org/html/2512.17853v2/figures_and_tables/mobility_relabel_gapartnet_fixed_texture__back-left-x-reverse.png",
        "https://arxiv.org/html/2512.17853v2/figures_and_tables/mobility_relabel_gapartnet_fixed_texture__link_2__render_tiled.png",
        "https://arxiv.org/html/2512.17853v2/figures_and_tables/mobility_relabel_gapartnet_fixed_texture__link_4__back-left-x-reverse.png",
        "https://arxiv.org/html/2512.17853v2/figures_and_tables/appendices_figures/pcd_aug.png",
        "https://arxiv.org/html/2512.17853v2/x5.png",
        "https://arxiv.org/html/2512.17853v2/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.17568",
      "title": "Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.17568",
      "arxivId": "2512.17568",
      "date": "2025-12-19",
      "authors": "Xiang Li Team",
      "category": "Manipulation",
      "summary": "本文针对机器人全身操控中关节空间与任务空间不对齐导致的策略学习复杂、泛化困难问题，提出一种运动学感知的扩散策略框架。其核心是采用手臂表面的一组3D点来统一表示机器人状态和动作，使其与3D点云观测空间保持一致，从而简化学习；并进一步将运动学先验融入扩散过程，以保证输出动作的可行性。仿真与实物实验表明，该方法在身体感知操控任务中，相比现有方法取得了更高的成功率和更强的空间泛化能力。",
      "detailedSummary": "## 研究背景与动机\n机器人模仿学习是从有限专家演示中学习复杂操作技能的有效方法。目前，基于扩散模型的视觉-运动策略在该领域取得了显著进展，其成功关键在于能够学习多模态、高维动作分布。现有方法广泛使用笛卡尔空间下的末端执行器位姿作为动作表示，因其与任务空间对齐，简化了策略学习过程。然而，对于需要全身操控的机器人操作任务，仅模仿6D末端轨迹是不够的。例如，在受限环境中操作需要避免机械臂与障碍物的碰撞，或某些任务要求使用臂体而非末端与物体交互，这些都需要对整个机器人构型进行精确控制。\n\n典型的全身操控方法是在机器人的关节空间学习策略。然而，关节空间与实际操作发生的任务空间（即3D空间）不对齐，这增加了策略学习的复杂性，因为策略需要内在地理解复杂的非线性运动学才能实现任务空间的泛化，而这很难从有限的演示中学习。一些先前工作尝试通过在网络中结合可微运动学或拼接冗余关节状态来结合笛卡尔空间和关节空间，但这些方法仍需预测关节空间动作，无法避免隐含学习非线性运动学带来的复杂性。\n\n本文针对关节空间策略样本效率低、空间泛化能力差的问题，提出了在统一3D空间下表示任务、观察和动作的新视角。其核心思路是：使用机械臂上的一组3D节点来表示机器人状态和动作，该表示与3D点云观察空间和任务空间自然对齐，从而提升策略的样本效率和空间泛化能力，并在此基础上将运动学先验融入扩散过程以保证输出动作的运动学可行性。\n\n## 方法详解\n本文提出的方法称为运动学感知扩散策略（KADP），其整体框架如下图所示。方法输入为编码后的3D视觉表示、3D机器人节点和扩散时间步嵌入，扩散模型迭代地预测去噪后的3D节点轨迹。最终，通过一个基于优化的全身逆运动学求解器计算关节角度指令用于执行。\n\n![方法框架](https://arxiv.org/html/2512.17568v1/figs/framework.png)\n> **图2**：KADP方法整体框架。输入编码后的3D视觉表示、机器人节点和扩散步长嵌入，扩散模型迭代预测去噪的3D节点轨迹。执行时，通过基于优化的全身逆运动学求解器计算关节角指令。\n\n核心模块与技术细节如下：\n1. **3D节点表示**：机器人构型由一组3D节点表示，记为 $A_{\\text{node}} = \\{(x_0, y_0, z_0), \\cdots, (x_m, y_m, z_m)\\}$。对于7-DoF Franka Emika Panda机械臂，手动选择了8个节点（如图2左下角所示）：前6个节点位于从基座到第6关节的臂体上，确保每个关节的状态由其对应节点的3D位置反映；另外两个额外节点放置在左/右夹爪上，以表示第7关节和夹爪的状态。此外，还在节点表示中加入了一个二进制值来表示夹爪的开合动作。这种表示定义在3D欧几里得空间中，与点云观察空间和任务空间一致，使得去噪网络能够在同一3D空间内学习。\n\n2. **带运动学约束的扩散模型**：由于3D节点表示相对于机械臂驱动自由度是冗余的，原始扩散策略无法保证生成的节点位置对应有效的机器人构型。为解决此问题，本文将运动学约束直接融入扩散模型。具体而言，在关节空间而非原始3D欧氏空间中定义两个节点表示之间的距离和噪声扰动。前向过程表示为：$A^k = F_{\\text{fk}}(\\sqrt{\\bar{\\alpha}^k} F_{\\text{ik}}(A^0) + \\sqrt{1-\\bar{\\alpha}^k} \\epsilon)$，其中 $F_{\\text{fk}}$ 是关节到节点的正向运动学映射，$F_{\\text{ik}}$ 是节点到关节的逆运动学映射，$\\epsilon$ 是标准高斯噪声。为确保训练时梯度可传播，使用一个预训练并冻结的3层MLP ($F_{\\text{ik\\_mlp}}$) 来拟合可微的近似逆运动学映射以提供梯度；在推理时，则使用更精确的基于优化的逆运动学求解器 ($F_{\\text{ik\\_opt}}$)。训练目标为最小化真实初始样本 $A^0$ 与网络预测 $\\mu_\\theta(A^k, O, k)$ 之间的均方误差。\n\n3. **模型架构与流程**：3D点云观察通过一个MLP编码器编码为视觉表示，与机器人本体感知状态拼接后形成条件信息。使用交叉注意力层进行条件调节。带噪声的动作 $A^k$、观察嵌入和扩散步长 $k$ 的位置嵌入被输入1D U-Net，输出预测的初始样本 $\\hat{A}^0$。然后，根据推导的后验分布公式计算一步去噪后的动作 $A^{k-1}$。推理时，从高斯噪声开始，通过迭代去噪（常使用DDIM加速）生成最终的3D节点轨迹，再通过 $F_{\\text{ik\\_opt}}$ 求解关节指令。\n\n与现有方法相比，KADP的创新点具体体现在：\n- **空间一致性**：提出了在统一3D空间下的节点表示，使任务、观察和动作空间对齐，简化了观察到动作的映射。\n- **全身控制**：节点表示唯一地决定了整个机器人关节构型，克服了仅控制末端执行器的局限。\n- **运动学可行性保证**：通过将解析的关节-节点映射融入扩散过程的前向和反向步骤，确保生成的节点位置遵守运动学约束。\n\n## 实验与结果\n- **数据集与平台**：在机器人学习基准RLBench上选取了8个具有挑战性的任务进行评估（任务可视化见图3），并在4个真实世界任务上进行了测试。每个仿真任务收集20条专家轨迹。观察来自多视角RGB-D图像，分割物体区域后投影为3D点云（下采样至1024点）。\n- **基线方法**：与三种基线方法进行对比：1) **DP3-EE**：使用笛卡尔空间末端执行器位姿的3D扩散策略；2) **DP3-Joint**：使用关节空间的3D扩散策略；3) **DP3-ERJ**：使用ERJ空间（拼接6D末端位姿和冗余关节位置）的3D扩散策略。\n- **关键实验结果**：在8个RLBench仿真任务上的平均成功率如表I所示。KADP在所有任务上取得最优或次优性能，平均成功率为64.3%，相比基线方法（DP3-EE: 40.2%， DP3-Joint: 42.8%， DP3-ERJ: 44.7%）有近20%的显著提升。这证明了KADP在实现全身控制的同时，保持了高样本效率和强空间泛化能力。DP3-EE性能最差，凸显了仅考虑末端位姿的局限性；DP3-Joint性能也较低，体现了空间不一致导致的样本效率低下和泛化能力弱。\n\n![仿真任务](https://arxiv.org/html/2512.17568v1/figs/overview_tasks.png)\n> **图3**：用于评估的8个RLBench仿真任务的可视化。\n\n![结果表格](https://arxiv.org/html/2512.17568v1/figs/ablation/ablation_kc.png)\n> **表I**：KADP与基线方法在8个RLBench仿真任务上的性能对比（平均成功率%）。KADP在整体平均成功率上显著优于所有基线。\n\n- **消融实验**：论文通过消融研究验证了核心组件的贡献。\n    - **运动学约束（KC）**：移除扩散模型中的运动学约束（KADP w/o KC）会导致性能下降和逆运动学求解误差（IK Error）上升，证明了其必要性。\n    - **节点数量**：减少节点数量（Node-3, Node-5）会降低性能，因为更少的节点无法充分描述机器人构型，验证了使用8个节点的设计。\n\n![消融实验-运动学约束](https://arxiv.org/html/2512.17568v1/figs/ablation/ablation_kc.png)\n> **图4a**：消融运动学约束（KC）的影响。移除KC（KADP w/o KC）导致性能下降和逆运动学误差升高。\n\n![消融实验-节点数量](https://arxiv.org/html/2512.17568v1/figs/ablation/ablation_node_num.png)\n> **图4b**：消融节点数量的影响。使用更少的节点（Node-3, Node-5）会降低策略性能。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1. 提出了一种用于全身操控的、在统一3D空间下的节点表示法，实现了任务、观察和动作空间的一致性。\n2. 设计了一种运动学感知的扩散策略（KADP），将运动学约束融入扩散过程，保证了生成动作的运动学可行性。\n3. 在仿真和真实任务上的实验表明，该方法在成功率和空间泛化能力上均优于现有的末端执行器、关节空间及混合空间表示方法。\n\n论文自身提到的局限性包括：1) 基于优化的逆运动学求解器会带来额外的计算开销；2) 节点选择依赖于机器人运动学模型，对于形状复杂的机器人可能需要更多节点。\n\n本工作对后续研究的启示在于：为机器人模仿学习提供了一种新的、与空间对齐的表示范式，能够有效平衡全身控制与学习效率。未来可探索将节点表示扩展到更复杂的机器人形态（如仿生手、移动机械臂），或研究更高效的、可嵌入网络训练的运动学约束实现方式。",
      "imageUrls": [
        "https://arxiv.org/html/2512.17568v1/figs/top_fig.png",
        "https://arxiv.org/html/2512.17568v1/figs/framework.png",
        "https://arxiv.org/html/2512.17568v1/figs/overview_tasks.png",
        "https://arxiv.org/html/2512.17568v1/figs/ablation/ablation_kc.png",
        "https://arxiv.org/html/2512.17568v1/figs/ablation/ablation_node_num.png",
        "https://arxiv.org/html/2512.17568v1/figs/snapshot/snapshot_pick_up_cube.png",
        "https://arxiv.org/html/2512.17568v1/figs/snapshot/snapshot_open_door.png",
        "https://arxiv.org/html/2512.17568v1/figs/snapshot/snapshot_put_item_in_drawer.png",
        "https://arxiv.org/html/2512.17568v1/figs/snapshot/snapshot_push_button.png",
        "https://arxiv.org/html/2512.17568v1/figs/heatmap/heatmap_demo_5.png",
        "https://arxiv.org/html/2512.17568v1/figs/heatmap/heatmap_demo_13.png",
        "https://arxiv.org/html/2512.17568v1/figs/failure/fail_open_door.png",
        "https://arxiv.org/html/2512.17568v1/figs/failure/fail_cube_cabinet.png",
        "https://arxiv.org/html/2512.17568v1/figs/failure/fail_push_button.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.17183",
      "title": "Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots",
      "url": "http://arxiv.org/abs/2512.17183",
      "arxivId": "2512.17183",
      "date": "2025-12-19",
      "authors": "Gang Zhang Team",
      "category": "Manipulation",
      "summary": "本文提出一种端到端框架，解决人形机器人生成与语音语义匹配的自然手势并实时执行的难题。关键技术包括：基于大语言模型与自回归Motion-GPT的语义感知手势合成模块、采用模仿学习的MotionTracker高保真控制策略，以及通用运动重定向方法。实验表明，该系统能合成语义恰当、节奏协调的手势，并在Unitree G1机器人上准确跟踪与执行，实现了从语音理解到实时物理部署的完整流程。",
      "detailedSummary": "## 研究背景与动机\n当前协同语音手势生成领域的主流方法是数据驱动的深度神经网络，如MLP、CNN、RNN和Transformer，它们能够较好地生成与语音节奏同步的节拍手势。然而，这些方法在生成传达具体语义的手势时面临关键局限性：语义手势在数据集中稀疏，导致数据驱动系统难以准确生成，常被训练过程视为噪声。此外，将复杂的人体运动转化为物理人形机器人可执行的命令，同时保持实时环境下的平衡与敏捷性，引入了具身鸿沟和鲁棒物理控制等额外重大挑战。\n\n本文针对语义手势生成稀疏以及从生成到物理部署的完整流程这两个具体痛点，提出了一个集成语义感知手势合成与鲁棒模仿学习控制的新视角。本文核心思路是：利用基于大语言模型的检索机制增强自回归生成模型来合成语义连贯的手势，并通过高质量运动重定向与专用的模仿学习控制策略，实现手势在机器人上的高保真、实时同步执行。\n\n## 方法详解\n整体框架分为训练与推理两个阶段。训练阶段包含三个连续步骤：1）运动编码本训练；2）运动生成训练；3）动作控制训练。推理时，输入音频经编码后送入运动生成模型，输出运动令牌序列，解码为机器人参考运动，最终由控制策略驱动机器人执行。\n\n![系统总览](https://arxiv.org/html/2512.17183v1/paper.drawio.png)\n> **图1**：系统整体概述：训练与推理流程。训练分为三步：运动编码本训练（残差VQ-VAE）、运动生成训练（Motion-GPT）和动作控制训练（模仿学习策略）。推理时，音频输入通过Motion-GPT生成运动令牌，解码为参考运动后，由控制策略驱动机器人实时执行。\n\n核心模块一：**运动重定向**。采用通用运动重定向方法，以弥合人体运动数据（BVH格式）与特定机器人（Unitree G1）形态之间的鸿沟。过程包括：关键身体部位匹配与对齐、非均匀局部缩放（仅对根节点平移进行均匀缩放以避免脚部滑动），以及基于差分逆求解器的两阶段运动学优化，最终得到包含根节点位置/速度及29个自由度关节角的G1机器人运动数据。\n\n核心模块二：**自动手势生成**。首先，使用**残差向量量化变分自编码器**学习离散运动空间。其创新在于将身体与手部运动分开建模以处理复杂性，并采用多层残差量化架构分层建模运动特征，以增强表达能力并避免码本坍塌。随后，基于GPT-2架构训练**自回归Motion-GPT模型**，以前序运动令牌和同步音频特征（MFCC、色谱图等）为条件，预测未来的手势令牌。关键创新是引入了**基于大语言模型的检索机制**：LLM分析语音文本上下文，从高质量手势库中检索相关语义手势候选，并通过语义感知对齐机制在潜在空间层面与节奏生成的运动融合，确保最终动画既自然又富有语义。\n\n核心模块三：**模仿学习控制策略**。采用基于两阶段强化学习框架开发的**MotionTracker**作为通用运动跟踪器。其技术细节在于，策略并非预测绝对的PD目标，而是预测相对于参考运动的**残差PD偏移**。最终动作通过tanh函数映射，并由经验设计的关节超参数缩放。这种规范化使策略能够有效预测紧凑的多关节动作分布，从而实现对多样化、高动态参考运动的高保真跟踪。\n\n## 实验与结果\n实验使用了高质量多模态数据集，包括ZEGGS数据集（2小时全身动捕与音频，19种运动风格）和BEAT数据集。对比基线主要为各模块内部的验证（如重建与生成对比真实数据）以及最终在真实机器人上的端到端性能展示。\n\n首先验证运动自编码器的重建能力。\n\n![原始与重建运动对比](https://arxiv.org/html/2512.17183v1/001_Neutral_0_mirror_x_1_0_retarget2mocap_comparison.png)\n> **图2**：原始与重建的G1运动对比（关节维度0-5）。在120秒序列上，重建运动（红色虚线）紧随原始运动（蓝色实线），表明RVQ-VAE编码有效，信息损失最小。\n\n其次评估Motion-GPT生成手势的质量。\n\n![生成与真实运动对比](https://arxiv.org/html/2512.17183v1/joint_comparison.png)\n> **图3**：G1运动对比：真实数据 vs. 生成数据（选定关节）。生成运动（红虚线）与真实数据（蓝实线）显示出强相关性和低误差。\n\n定量结果（表1）显示，生成运动与真实数据之间的关节均方根误差普遍较低，例如根位置误差极小（root_pos_x: 0.000592），证明了生成手势的高质量。\n\n最后测试模仿学习控制策略。仿真跟踪精度如表2所示，所有自由度上的RMSE误差普遍较低（如右髋滚转关节：0.0064），表明策略能准确跟踪高动态参考运动。\n\n![真实世界部署](https://arxiv.org/html/2512.17183v1/real_deploy.png)\n> **图4**：在Unitree G1上的真实世界部署演示。机器人执行生成的协同语音手势，同时保持身体平衡，验证了集成系统的鲁棒性和实用性。\n\n消融实验虽未明确分组件列出，但通过各模块的独立验证（图2、3、表1、2）及最终真实部署（图4），共同证明了运动重定向、语义手势生成和模仿学习控制每个环节的有效性及其对整个系统成功的贡献。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了一个完整的端到端流程，实现了从语音理解到语义手势生成再到物理机器人实时部署的全链路贯通；2）创新性地将LLM检索机制与自回归生成模型融合，增强了稀疏语义手势的生成能力；3）通过高质量的GMR运动重定向与鲁棒的MotionTracker模仿学习策略，成功解决了人形机器人执行复杂动态手势时的具身鸿沟与平衡控制问题。\n\n论文自身提到的局限性包括：LLM对语音节奏和韵律的理解可进一步深化，以优化语义手势检索的时机和频率；当前基于简单击打阶段对齐的融合策略可能不足以完全保留运动细节和自然度，需要探索更复杂的合并策略。\n\n这项工作对后续研究的启示在于：为构建具有表现力的类人机器人提供了可落地的技术框架；指明了结合高层语义理解（LLM）与底层运动生成/控制是提升机器人交互自然度的有效途径；未来可向多轮对话、多机器人交互等更复杂的社交场景拓展。",
      "imageUrls": [
        "https://arxiv.org/html/2512.17183v1/paper.drawio.png",
        "https://arxiv.org/html/2512.17183v1/001_Neutral_0_mirror_x_1_0_retarget2mocap_comparison.png",
        "https://arxiv.org/html/2512.17183v1/joint_comparison.png",
        "https://arxiv.org/html/2512.17183v1/real_deploy.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.17253",
      "title": "Mitty: Diffusion-based Human-to-Robot Video Generation",
      "url": "http://arxiv.org/abs/2512.17253",
      "arxivId": "2512.17253",
      "date": "2025-12-19",
      "authors": "Mike Zheng Shou Team",
      "category": "Manipulation",
      "summary": "本文针对从人类示范视频直接生成机器人执行视频的核心问题，旨在避免依赖关键点等中间表示导致的信息丢失与累积错误。提出Mitty方法，基于扩散变换器的视频上下文学习技术，利用预训练视频扩散模型，将人类视频压缩为条件令牌，通过双向注意力与机器人去噪令牌融合，实现端到端生成；并开发自动合成管道从自我中心数据生成高质量配对数据以缓解数据稀缺。实验在Human2Robot和EPIC-Kitchens数据集上取得最先进结果，展现出对未见环境的强泛化能力。",
      "detailedSummary": "## 研究背景与动机\n目前，从人类演示视频中学习机器人技能的主流方法依赖于关键点、轨迹或深度图等中间表示。这些方法首先从人类视频中提取中间表示，然后驱动渲染模块合成机器人视频。然而，这种方法无法充分利用演示视频中蕴含的丰富信息，难以捕捉对鲁棒泛化至关重要的细粒度时空动态，并且中间估计阶段的累积误差会进一步降低性能。因此，本文旨在解决一个核心痛点：能否绕过中间表示，直接实现端到端的人类到机器人（Human2Robot， H2R）视频生成？这面临三大挑战：外观与场景一致性、动作与策略对齐，以及高质量人类-机器人配对视频数据的稀缺。\n\n本文提出了Mitty，一个基于扩散Transformer（DiT）的视频上下文学习（In-Context Learning）框架。其核心思路是：以预训练的大规模视频扩散模型（Wan 2.2）为基础，通过上下文学习范式，将人类演示视频压缩为条件令牌，并在扩散过程中通过双向注意力机制与机器人去噪令牌融合，从而直接、端到端地生成对应的机器人执行视频，无需动作标签或中间抽象表示。\n\n## 方法详解\n整体框架基于预训练的Wan 2.2视频扩散模型，采用上下文学习范式。输入是成对的人类演示视频 $V^H$ 和机器人执行视频 $V^R$，目标是建模条件分布 $p_\\theta(V^R|V^H)$。框架支持两种设定：H2R（零帧生成，仅以人类视频为条件）和HI2R（首帧条件生成，额外提供机器人初始帧以定义初始状态）。在训练时，人类视频通过VAE编码为干净的条件令牌，机器人视频编码后则被注入噪声作为去噪目标。这两组令牌沿时间维度拼接后，输入到配备了双向注意力机制的扩散Transformer中，实现跨模态信息流。\n\n![方法框架](https://arxiv.org/html/2512.17253v1/x2.png)\n> **图2**：Mitty的整体架构。构建于基于扩散Transformer的视频生成模型之上，采用上下文学习范式。人类演示视频（输入）和带噪声的机器人视频潜变量（去噪流）被拼接，噪声仅注入机器人分支。双向注意力机制实现了跨模态信息流动，使模型能够直接从人类操作演示中学习生成机器人视频。\n\n核心模块是**视频上下文学习与双向注意力耦合**。具体而言，人类条件令牌 $\\mathbf{C}$ 和机器人去噪令牌 $\\mathbf{D}$ 在添加了时间嵌入和模态嵌入后，通过双向注意力进行信息交换（公式4）。在每个Transformer层，通过计算交叉注意力权重，人类令牌从机器人令牌获取信息（$\\tilde{\\mathbf{C}}$），机器人令牌也从人类令牌获取信息（$\\tilde{\\mathbf{D}}$）。更新后的令牌 $[\\tilde{\\mathbf{C}}; \\tilde{\\mathbf{D}}]$ 被拼接并送入后续块。网络在机器人分支上预测噪声 $\\epsilon_\\theta(\\mathbf{x}^R_t, \\mathbf{C}, t)$，并按照标准扩散过程进行反向更新（公式5），最终通过VAE解码得到生成的机器人视频 $\\hat{\\mathbf{V}}^R$（公式6）。\n\n与现有方法相比，Mitty的主要创新点在于：1）**端到端映射**：摒弃了关键点、轨迹等中间表示，直接学习从人类视频到机器人视频的映射，避免了信息损失和误差累积。2）**上下文学习范式**：利用预训练大模型的强视觉-时序先验，通过简单的条件输入（人类视频）实现对新任务的快速适应，无需针对每个任务重新训练。3）**统一的双向注意力设计**：通过一个统一的架构同时支持零帧生成和首帧条件生成，实现了细粒度的控制。\n\n![数据合成流程](https://arxiv.org/html/2512.17253v1/x3.png)\n> **图3**：自动合成配对数据的流程。从人类演示视频开始，依次进行手部检测与分割、手部关键点检测、视频修复以获得干净背景、将手部关键点映射到机器人末端执行器位姿，最后将机器人手臂渲染到视频中。经过人工筛选，最终构建了高质量的人类-机器人配对视频数据集。\n\n为了缓解配对数据稀缺问题，本文还设计了一个**自动化的数据合成流程**（图3）。该流程以大规模自我中心（egocentric）人类视频（如EPIC-Kitchens）为输入，通过手部网格恢复（HaMeR）、检测分割（Detectron2, SAM2）、视频修复（E2FGVI）、位姿映射和机器人渲染（RobotSuite）等一系列步骤，合成出机器人手臂执行视频。通过人工参与（human-in-the-loop）的筛选机制，最终构建了高质量的训练数据，为模型提供了强监督信号。\n\n## 实验与结果\n实验在**Human2Robot（H2R）** 和**EPIC-Kitchens**两个基准数据集上进行。H2R数据集包含11,788个剪辑，EPIC-Kitchens通过合成流程得到34,820个剪辑，并划分为“已见”和“未见”场景以测试跨环境泛化能力。对比的基线方法分为两类：1）基于Wan 2.2家族和上下文学习设置构建的模型（包括不同规模模型和消融变体）；2）通用视频编辑方法（Aleph, Kling, MoCha）。评估指标包括视频质量指标（FVD, PSNR, SSIM, MSE）、任务成功率（SR）、人类偏好和具身一致性。\n\n![生成结果示例](https://arxiv.org/html/2512.17253v1/x4.png)\n> **图4**：Mitty在Human2Robot和EPIC-Kitchens数据集上的生成结果。每组结果中，第一行是人类演示视频，第二行是我们方法（零帧生成）的输出，第三行是真实的机器人执行视频。结果显示Mitty能准确保持场景布局和物体交互，并产生平滑、时序一致的机器人动作。\n\n关键实验结果如下：在Human2Robot数据集上，性能最佳的T2V 14B模型取得了FVD 6.48， PSNR 22.7， SSIM 0.851， MSE 0.0069， 任务成功率（SR）93%。在更复杂的EPIC-Kitchens数据集上，T2V 14B在“已见”和“未见”场景的SR分别为90%和89%。与基线对比（表3）显示，Mitty取得了最高的任务成功率（84.5%）和人类偏好（68.0%），以及第二高的具身一致性（92.6%），在正确性、视觉保真度和结构稳定性之间取得了最佳平衡。\n\n![与Masquerade数据流程对比](https://arxiv.org/html/2512.17253v1/x5.png)\n> **图5**：与Masquerade多阶段合成流程的定性对比。Masquerade的流程（红色高亮部分）容易出现复合错误（如关键点检测、修复和渲染失败）。相比之下，我们利用其合成数据经过筛选后训练的端到端模型能产生更可靠的Human2Robot映射。\n\n消融研究（表2）表明：1）**人类参考视频至关重要**：移除人类视频（w/o ref vid.）导致所有指标显著下降，在H2R上SR从91%跌至65%，在EPIC-Kitchens（已见）上从88%跌至75%。2）**任务描述作用较小**：移除文本描述（w/o task desc.）仅引起轻微变化，说明Mitty更依赖视觉演示。3）**分数据集训练优于混合训练**：由于两个数据集在任务和环境上差异较大，针对每个数据集单独训练（Sep. train.）的模型性能普遍优于混合训练（Mixed train.）的模型。\n\n![与视频编辑方法对比](https://arxiv.org/html/2512.17253v1/x6.png)\n> **图6**：与通用视频编辑方法的定性对比。这些方法仅使用单张机器人手臂参考图像进行替换，导致生成的手臂出现变形、结构错误和扭曲。而Mitty基于配对数据训练，能持续保持正确的外观和结构。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了**第一个基于视频扩散Transformer的端到端Human2Robot视频生成框架Mitty**，绕过了易错的中间表示。2）技术上，创新地采用了**视频上下文学习与双向注意力机制**，实现了外观、场景与动作的一致性，显著提升了跨任务泛化能力。3）设计了一套**高效的自动数据合成与筛选策略**，结合现有数据集进行训练，大幅增强了模型在未知任务和环境上的泛化能力。\n\n论文自身提到的局限性在于：Mitty目前主要是一个视频生成模型，**尚不能直接输出可执行的机器人控制信号**，未形成完整的“视频到策略”闭环。这为后续研究指明了方向：如何将生成的高保真、时序对齐的机器人视频可靠地反转为控制指令，是实现从人类观察到机器人执行的端到端映射的关键下一步。此外，这项工作也证实了利用大规模预训练视频生成模型所蕴含的强时空先验，以及通过自动化流程扩展机器人学习数据的可行性，为 scalable 的机器人学习提供了新的见解。",
      "imageUrls": [
        "https://arxiv.org/html/2512.17253v1/x1.png",
        "https://arxiv.org/html/2512.17253v1/x2.png",
        "https://arxiv.org/html/2512.17253v1/x3.png",
        "https://arxiv.org/html/2512.17253v1/x4.png",
        "https://arxiv.org/html/2512.17253v1/x5.png",
        "https://arxiv.org/html/2512.17253v1/x6.png",
        "https://arxiv.org/html/2512.17253v1/x7.png",
        "https://arxiv.org/html/2512.17253v1/x8.png",
        "https://arxiv.org/html/2512.17253v1/x9.png",
        "https://arxiv.org/html/2512.17253v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.16911",
      "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
      "url": "http://arxiv.org/abs/2512.16911",
      "arxivId": "2512.16911",
      "date": "2025-12-18",
      "authors": "Sergey Levine Team",
      "category": "Manipulation",
      "summary": "本文针对标准行为克隆（BC）预训练策略在后续强化学习（RL）微调中因动作覆盖不足而导致样本效率低下的问题，提出后验行为克隆（PostBC）方法。该方法通过建模演示者行为在给定数据集下的后验分布（而非直接模仿动作），确保对演示动作的覆盖，且预训练性能不低于BC。实验表明，PostBC仅依赖标准监督学习，在机器人控制基准和真实操作任务中，相比BC能显著提升RL微调性能。",
      "detailedSummary": "## 研究背景与动机\n当前，从机器人到语言等多个领域，训练高效策略的标准范式是：首先在大规模演示数据集上通过行为克隆预训练一个策略，然后在部署领域（通常使用强化学习）对该策略进行微调。微调步骤对于实现人类或超人类水平的表现至关重要，然而，尽管已有大量研究致力于开发更有效的微调算法，却很少有人关注如何确保预训练策略本身是RL微调的有效初始化。本文旨在理解预训练策略如何影响微调性能，以及如何预训练策略以确保它们成为微调的有效起点。本文指出，标准行为克隆通过直接拟合演示者执行的动作来训练策略，理论上可能无法覆盖演示者的动作分布，而这是确保有效RL微调的最小必要条件。针对这一痛点，本文提出了新的视角：不再精确拟合观察到的演示，而是训练一个策略来建模给定演示数据集下演示者行为的后验分布。本文的核心思路是，通过建模后验分布，预训练策略能在对演示者行为不确定的状态下增加动作分布的熵（多样性），从而为RL微调提供更多样、信息更丰富的探索数据，同时在高确定性区域保持与标准BC相当的性能。\n\n## 方法详解\n本文提出的方法称为后验行为克隆。其整体框架是：给定特定任务的演示数据，假设对演示者行为有一个均匀先验，并将演示数据视为从演示者行为分布中采样的观测。目标是训练一个生成式策略，使其建模给定这些观测下演示者行为的后验分布。输入是状态，输出是基于后验不确定性的动作分布。\n\n![方法框架](https://arxiv.org/html/2512.16911v1/x1.png)\n> **图1**：方法整体框架与原理示意。(a) 给定感兴趣任务的演示数据。(b) 标准BC预训练拟合演示中的行为，在高数据密度区域性能良好，但在低数据密度区域可能过度依赖观测到的行为。(c) 这导致RL微调效果不佳，因为从BC策略采样的轨迹在低数据密度区域提供的奖励信号很少。(d) 本文提出的PostBC训练一个生成策略来拟合演示者行为的后验分布，在低数据密度区域赋予预训练策略更广泛的行动分布，在高数据密度区域则近似于标准BC策略。(e) 在低数据密度区域更广泛的动作分布允许收集更多样、信息更丰富的观测，从而实现更有效的RL微调，同时在高数据密度区域性能收敛于演示者水平。\n\n核心模块是后验行为克隆策略的构建。具体而言，PostBC策略 π PostBC \\pi^{\\text{PostBC}} 定义为在给定状态 s s s 下，演示者策略 π β \\pi^\\beta 的后验分布的期望： π PostBC ( a ∣ s ) = E [ π β ( a ∣ s ) ∣ D ] \\pi^{\\text{PostBC}}(a|s) = \\mathbb{E}[\\pi^\\beta(a|s) | \\mathcal{D}] ，其中 D \\mathcal{D} 是演示数据集。在连续动作领域，本文提出了一种实用的近似方法，使用现代生成模型（扩散模型）来实例化PostBC。该实例化仅依赖于可扩展的监督学习目标（预训练阶段无需RL），并且只需对现有的BC训练流程进行最小修改即可融入。\n\n与现有方法相比，创新点具体体现在：1）**理论视角**：首次从“覆盖演示者动作分布”这一必要条件出发，分析预训练策略对RL微调效率的影响，并证明标准BC可能无法满足此条件，而PostBC可以。2）**方法设计**：将预训练目标从拟合演示数据的经验分布，转变为拟合演示者策略的后验分布。这使策略能够根据其不确定性自适应地调整动作分布的熵：在不确定区域采用高熵分布以促进探索，在确定区域采用低熵分布以保持性能。3）**性能保证**：理论证明PostBC策略能覆盖演示者分布，且其预训练性能不差于标准BC策略，在所有预训练性能不差于BC的策略估计器中，其采样成本接近最优。\n\n## 实验与结果\n实验使用了模拟和真实世界的机器人控制基准。具体包括：1）**模拟基准**：在具有挑战性的多任务机器人操控环境（基于BridgeData V2）上进行评估。2）**真实世界任务**：在实体机器人上执行复杂的桌面操控任务，例如“将玉米放入锅中”和“捡起香蕉”。\n\n对比的基线方法包括：1）标准行为克隆；2）在BC策略上添加均匀动作噪声的变体；3）使用离线RL方法（如IQL、Cal-QL）进行预训练。\n\n关键实验结果如下：在模拟环境中，经过相同RL算法（如DPPO、LSRL）微调后，使用PostBC预训练的策略在最终成功率和微调样本效率上均显著优于使用标准BC预训练的策略。例如，在100K环境步数的微调后，PostBC在多个任务上的平均成功率比标准BC高出约10-20个百分点。在真实机器人任务中，PostBC预训练同样带来了显著的性能提升，例如在“捡起香蕉”任务中，微调后的成功率从约40%提升至超过80%。\n\n![模拟实验结果汇总](https://arxiv.org/html/2512.16911v1/x4.png)\n> **图4**：在模拟多任务机器人操控基准上的微调性能曲线。PostBC预训练（蓝色）相比标准BC预训练（红色）在微调过程中学习更快，最终达到的成功率也更高。添加均匀噪声的BC（绿色）性能不稳定。\n\n![真实世界任务性能对比](https://arxiv.org/html/2512.16911v1/x5.png)\n> **图5**：在真实世界机器人操控任务上的微调性能。左图为“将玉米放入锅中”任务，右图为“捡起香蕉”任务。PostBC预训练显著提高了微调后的任务成功率。\n\n![不同微调算法下的表现](https://arxiv.org/html/2512.16911v1/x6.png)\n> **图6**：PostBC预训练对多种RL微调算法（DPPO和LSRL）均能带来一致的性能提升，证明了其作为初始化策略的通用性。\n\n![定性结果：覆盖范围](https://arxiv.org/html/2512.16911v1/im/qualitative/coverage100_rollouts_task6_13.png)\n> **图7**：定性展示从不同预训练策略采样轨迹的覆盖范围。PostBC策略（右）产生的状态覆盖范围比标准BC策略（中）更广，更接近演示数据的覆盖范围（左）。\n\n![定性结果：真实机器人](https://arxiv.org/html/2512.16911v1/im/qualitative/coverage100_rollouts_task14_21.png)\n> **图8**：真实机器人任务“捡起香蕉”的定性结果。经过微调后，基于PostBC初始化的策略（下图）能够成功执行任务，而基于标准BC初始化的策略（上图）则失败。\n\n消融实验方面，论文通过理论构造和实验验证了简单添加均匀噪声的方法（作为增加探索的直观尝试）可能失败，因为它会不加区分地在所有状态增加噪声，可能在高确定性区域引入有害扰动，从而降低预训练性能并破坏微调。而PostBC根据不确定性自适应调整熵，避免了这一问题。\n\n## 总结与启发\n本文的核心贡献包括：1）**理论分析**：明确了预训练策略“覆盖演示者动作分布”对于后续RL微调的重要性，并证明了标准BC可能无法满足此条件。2）**方法提出**：提出了后验行为克隆这一新的预训练范式，通过建模演示者行为的后验分布，自适应地调整策略的探索性，在保证预训练性能的同时为RL微调提供了更好的初始化。3）**实证验证**：在模拟和真实机器人任务上广泛验证了PostBC能显著提升多种RL算法的微调效率和最终性能。\n\n论文自身提到的局限性包括：1）演示者动作覆盖条件是必要的，但并未给出下游RL微调样本复杂度的充分性保证。2）研究聚焦于仅使用监督学习进行预训练，虽然这是最可扩展且常用的方法，但可能限制了获得更有效初始化的潜力。3）工作主要考虑机器人控制应用，在语言等领域的效果尚未验证。\n\n对后续研究的启示：1）未来可以探索推导出能确保高效RL微调的非平凡充分条件，并设计满足该条件的预训练方法。2）可以研究结合其他方法（如离线RL）进行预训练，以进一步提升初始化质量。3）将PostBC思想应用于语言模型的预训练或监督微调，可能改善其下游的RL微调性能。",
      "imageUrls": [
        "https://arxiv.org/html/2512.16911v1/x1.png",
        "https://arxiv.org/html/2512.16911v1/im/corn_in_pot2.jpg",
        "https://arxiv.org/html/2512.16911v1/im/pick_up_banana.jpg",
        "https://arxiv.org/html/2512.16911v1/x2.png",
        "https://arxiv.org/html/2512.16911v1/x3.png",
        "https://arxiv.org/html/2512.16911v1/x4.png",
        "https://arxiv.org/html/2512.16911v1/im/qualitative/coverage100_rollouts_task6_13.png",
        "https://arxiv.org/html/2512.16911v1/im/qualitative/coverage100_rollouts_task14_21.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.17062",
      "title": "Lang2Manip: A Tool for LLM-Based Symbolic-to-Geometric Planning for Manipulation",
      "url": "http://arxiv.org/abs/2512.17062",
      "arxivId": "2512.17062",
      "date": "2025-12-18",
      "authors": "Irfan Hussain Team",
      "category": "Manipulation",
      "summary": "本文提出Lang2Manip工具，解决LLM生成的符号计划在机器人操作仿真中执行时需机器人特定工程或规划器依赖集成的问题。关键技术是构建统一管道，集成LLM-based符号规划器与Kautham运动规划框架，将自然语言指令转换为符号动作，并利用Kautham支持的多规划器（几何、动态、物理驱动等）自动计算和执行无碰撞轨迹，无需额外编码。该工具实现了机器人无关的符号到几何规划，为语言驱动的任务和运动规划提供了灵活、可扩展的解决方案。",
      "detailedSummary": "## 研究背景与动机\n目前，利用大型语言模型（LLMs）从自然语言生成符号化任务计划，并在仿真中执行，已成为机器人操作领域的主流方法。然而，现有系统通常与特定的机器人模型、场景结构或规划器配置紧密耦合。当引入新机器人时，通常需要额外的编码工作、自定义配置文件或新的URDF转换；同样，切换几何规划器或整合运动学动态或基于约束的规划模块通常需要对底层管道进行大量修改。这限制了语言驱动任务与运动规划（TAMP）系统在不同机器人、领域和运动规划范式间的泛化能力。\n\n本文针对语言驱动TAMP系统通用性差、集成成本高的痛点，提出了一个系统级工具的新视角：将LLM的符号规划能力与成熟的、功能丰富的运动规划框架Kautham相连接。本文的核心思路是创建一个统一的、机器人无关的管道，通过Kautham框架的标准接口，将LLM生成的符号计划直接转换为可执行的几何运动计划，而无需针对特定任务、机器人或规划器进行额外编码。\n\n## 方法详解\nLang2Manip框架的整体架构由两大子系统组成：LLM引导的符号规划层和基于Kautham的运动规划与执行层。输入是用户提供的自然语言任务描述以及从Kautham获取的文本化环境状态；输出是机器人可执行的无碰撞轨迹序列。该管道首先由LLM生成符号动作序列，然后通过抓取计算、逆运动学等模块将这些符号动作“落地”为具体的几何规划问题，最终交由Kautham框架内的多种规划器求解并执行。\n\n![方法框架](https://arxiv.org/html/2512.17062v1/figures/tool-framework.png)\n> **图1**：提出的LLM-Kautham操作规划框架概览。左侧为LLM引导的符号规划层，根据任务、系统和场景描述生成符号动作；右侧为Kautham项目，通过其GUI、状态文本化模块和多种规划器（基于物理的、运动学动态的、几何的、知识导向的）处理问题设置和运动规划。\n\n核心模块一：**Kautham集成层**。Kautham是一个基于OMPL的开源运动规划框架，其核心作用是提供机器人无关的场景建模与多模态规划接口。它原生支持通过URDF文件导入机器人模型和障碍物，并通过XML问题文件定义规划场景（图2）。系统采用标准化的目录结构（图3）组织模型和问题文件，确保场景的可移植性和可复现性。Kautham还提供了一个关键模块——**环境状态观察**，它能将内部工作空间状态（机器人关节配置、障碍物位姿、空间关系等）系统地转换为结构化的文本描述，为LLM提供理解当前几何环境的上下文。\n\n![问题文件示例](https://arxiv.org/html/2512.17062v1/figures/problem_file.png)\n> **图2**：定义Franka Emika Panda机器人操作场景的Kautham问题文件示例。XML规范包括机器人和障碍物的URDF模型、它们在空间中的位姿、关联的控制文件、规划器选择（此例为RRT）、规划器参数以及指定每个受控关节初始值和目标值的查询块。\n\n![目录结构](https://arxiv.org/html/2512.17062v1/figures/structure_file.png)\n> **图3**：Kautham中用于组织规划场景的目录结构。Kautham_Demos文件夹包含存放机器人和障碍物URDF文件的模型目录，以及存放实例化规划场景所需的问题描述和控制文件的问题目录。\n\n核心模块二：**LLM引导的符号规划**。该层负责将自然语言指令转化为结构化的符号计划。系统定义了固定的符号动作空间 𝒜 = {pick, place, move, push}，每个动作遵循统一的参数模板 `a(o, p, r, κ)`，其中包含目标对象、位姿参数、动作细化（如抓取方向）和首选规划器。如图5所示，提供给LLM的提示词由三部分组成：用户任务描述、定义输出格式和动作模式的固定系统提示词、以及来自Kautham的文本化环境状态。LLM（实验中为GPT-4，但框架模型无关）据此生成一个JSON格式的符号计划序列。\n\n![符号规划管道](https://arxiv.org/html/2512.17062v1/figures/prompt-kautham.png)\n> **图5**：LLM引导的符号规划管道。提示词由三个部分组成：用户定义的任务、描述所需输出格式和动作模式的固定系统提示词，以及从Kautham获得的文本化环境状态。组合后的提示词传递给LLM，LLM生成包含高层符号动作的结构化JSON计划。\n\n核心模块三：**符号到几何的执行**。该模块将抽象的符号计划转化为具体的运动规划查询。首先，系统解析JSON动作，从对象注册表中获取目标的几何信息。对于抓取类动作，**抓取姿态计算**插件（本工作实现了一个基于指定方向的轻量级规划器）会计算可行的末端执行器位姿。接着，**逆运动学**插件（当前使用KDL求解器）将笛卡尔目标位姿转换为机器人关节配置。最后，通过Kautham的ROS接口（kautham_ros），系统以编程方式设置规划器、定义查询（初始和目标关节配置），并调用求解服务来计算无碰撞轨迹。该轨迹随后被发送给机器人控制器或仿真器执行。\n\n与现有方法相比，本文的核心创新在于利用了Kautham框架的**统一接口**和**广泛支持**。Kautham原生集成了多种工业机械臂模型，并支持几何、运动学动态、物理驱动和基于约束的规划，所有这些功能都通过标准化接口提供。这使得Lang2Manip能够实现真正的机器人无关和规划器无关——只需更换URDF模型或更改规划器配置，而无需修改管道代码或LLM提示词，显著提升了语言驱动TAMP系统的可扩展性和通用性。\n\n## 实验与结果\n实验在Kautham仿真环境中进行，使用Franka Emika Panda机械臂执行一个多步骤操作任务：“将马克笔和橡皮放入笔筒中”。场景包含马克笔、橡皮和圆柱形笔筒三个物体（初始位姿随机化）。系统使用GPT-4作为LLM生成符号计划，并通过前述管道执行。\n\n![执行序列](https://arxiv.org/html/2512.17062v1/figures/execution.png)\n> **图6**：Lang2Manip框架执行任务“将马克笔和橡皮放入笔筒”的仿真序列。该序列展示了完整的执行管道：(1) 初始场景；(2) 移向马克笔并抓取；(3) 移向放置目标；(4) 将马克笔放入笔筒；(5) 移向橡皮并抓取；(6) 移向橡皮放置目标；(7) 最终将橡皮放入笔筒。红色曲线显示了Kautham为每个动作生成的无碰撞轨迹。\n\n评估采用了LLM-TAMP领域常用的三项指标。在20次随机初始位姿的试验中：\n1.  **任务成功率**：达到85%，与先前LLM-TAMP系统报告的范围一致。大多数失败源于不可达的抓取（逆运动学失败）或LLM的位姿推理错误。\n2.  **运动规划可行性**：使用Kautham内OMPL规划器的运动规划成功率为92%。失败主要发生在LLM提供的目标位姿过于接近工作空间边界或拥挤区域时。这证实了失败主要源于符号层错误，而非几何层。\n3.  **符号计划准确率**：LLM错误（包括JSON格式错误、参数缺失或语义上不一致的动作排序）发生在10%的试验中，与ProgPrompt和LLM3等工作中报告的错误率相当。\n\n![Kautham场景可视化](https://arxiv.org/html/2512.17062v1/figures/kautham_scene.png)\n> **图4**：操作场景的Kautham可视化。左侧面板显示标准的Kautham查看器，展示了机器人和工作空间物体；右侧面板展示了机器人的碰撞模型以及计算出的运动计划（红色轨迹）和RRT规划器生成的相应探索树（绿色样本）。\n\n实验结果表明，该框架在保持与现有系统相当性能的同时，实现了更高的通用性和灵活性。论文未进行严格的消融实验，但通过架构设计明确了各模块的贡献：LLM提供高层任务分解，插件化的抓取和IK模块实现符号到几何的转换，而Kautham作为后端核心，承担了机器人建模、场景管理和多模态运动规划的关键职责。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了一个机器人无关和规划器无关的管道**：通过将LLM与Kautham框架连接，实现了无需额外工程即可跨不同机器人平台和运动规划范式执行语言指令的TAMP系统。\n2.  **展示了LLM与成熟运动规划框架的集成方法**：利用Kautham的状态文本化、统一规划接口和ROS兼容性，为LLM提供了丰富的几何上下文和可靠的运动执行后端。\n3.  **提供了一个灵活的研究工具**：该框架为LLM条件操作、任务与运动规划及机器人学习的研究提供了一个模块化、可扩展的基础。\n\n论文自身提到的局限性主要与LLM和当前简化模块有关：LLM输出的模糊性或不完整可能导致无效动作或不可行位姿；简化的抓取启发式方法可能难以处理不规则形状或杂乱排列；对于具有紧密耦合子目标的长视野任务，当几何可行性检查失败时，可能需要LLM进行修订或重新规划。\n\n这些局限性和系统架构为后续研究指明了方向：探索集成更复杂的**分层规划与宏动作**；进行**真实机器人部署**；**集成学习型抓取规划器**以提升抓取能力；引入**多模态感知**（RGB-D、触觉）来更好地将符号动作与现实世界 grounding；以及建立**闭环的LLM细化机制**，将几何层的不确定性感知反馈纳入符号重规划过程。",
      "imageUrls": [
        "https://arxiv.org/html/2512.17062v1/figures/tool-framework.png",
        "https://arxiv.org/html/2512.17062v1/figures/problem_file.png",
        "https://arxiv.org/html/2512.17062v1/figures/structure_file.png",
        "https://arxiv.org/html/2512.17062v1/figures/kautham_scene.png",
        "https://arxiv.org/html/2512.17062v1/figures/prompt-kautham.png",
        "https://arxiv.org/html/2512.17062v1/figures/execution.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.16861",
      "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
      "url": "http://arxiv.org/abs/2512.16861",
      "arxivId": "2512.16861",
      "date": "2025-12-18",
      "authors": "Caelan Garrett Team",
      "category": "Manipulation",
      "summary": "论文解决长时程机器人操作任务中演示数据收集昂贵、模仿学习易偏离，以及强化学习探索困难的核心挑战。提出ReinforceGen框架，通过任务分解将任务分割为多个局部技能，并利用自动化数据生成与模仿学习构建初始策略，再结合在线适应和强化学习进行微调改进。在Robosuite数据集上的实验表明，该系统在视觉运动控制下达到80%的成功率，且消融研究证实微调方法带来了89%的平均性能提升。",
      "detailedSummary": "## 研究背景与动机\n长视野操作任务是机器人领域的长期挑战。目前，从演示中模仿学习是让智能体完成任务的有效方法，但收集演示成本高昂，且训练出的智能体容易偏离演示分布。强化学习利用环境奖励，但长视野加剧了探索困难，尤其是在奖励稀疏时。为缓解演示不足，一类工作通过物体中心的数据生成来扩充数据集，另一类使用任务与运动规划来生成演示，或将任务分层为更易解决的子目标。然而，这些数据生成方法是开环的，仅依赖离线数据，其性能受限于源演示的质量。本文针对现有自动化数据生成方法因缺乏在线探索而性能受限的痛点，提出将在线探索和环境反馈（通过强化学习）整合到分层数据生成流程中的新视角。核心思路是：首先利用少量人类演示通过合成数据生成离线数据集，训练一个混合模仿学习智能体，然后通过在线交互和强化学习对该智能体的各个组件进行微调，从而超越源演示的性能边界。\n\n## 方法详解\nReinforceGen的整体流程分为三步：1）利用少量人类源演示，通过合成数据生成创建离线数据集；2）使用该数据集训练一个混合模仿学习智能体，该智能体在运动规划（移动到预测路点）和学习到的策略（直接控制机器人）之间交替；3）通过在线环境交互和强化学习对智能体进行微调。\n\n![方法框架](https://arxiv.org/html/2512.16861v1/x1.png)\n> **图1**：ReinforceGen整体框架。首先通过少量人类源演示进行合成数据生成，创建离线数据集。然后使用该数据集训练一个混合模仿学习智能体。最后，通过在线环境交互和强化学习对智能体进行微调。\n\n该方法建立在混合技能策略框架上，将任务分解为n个阶段，每个阶段包含一个连接段和一个技能段。每个参数化技能ψ_θ_i由三部分组成：起始姿态预测器ℐ_θ_i、技能策略π_θ_i和终止分类器𝒯_θ_i。\n\n![阶段组件](https://arxiv.org/html/2512.16861v1/x2.png)\n> **图2**：ReinforceGen单个阶段的主要组件。姿态预测器ℐ_θ_i实时预测目标末端执行器姿态并更新运动规划器。到达目标后，技能策略π_θ_i接管控制以完成阶段目标，该目标由终止预测器𝒯_θ_i决定。所有三个组件首先从生成的数据集中模仿学习，然后使用在线数据进行微调。\n\n核心创新在于对HSP的三个组件分别设计了基于强化学习的微调流程：\n1.  **起始姿态预测器（ℐ_θ_i）微调**：该组件为运动规划器提供目标姿态，其预测误差会严重影响后续技能策略性能。\n    *   **实时重规划**：在连接段执行运动规划时，每个时间步都运行姿态预测器。当新预测姿态与当前规划目标的差异超过阈值时，使用更新后的姿态重新规划。\n    *   **从特权教师蒸馏**：训练时，利用可访问的物体状态信息，使用一个准确的特权教师预测器（通过物体中心变换生成目标姿态）来蒸馏学生预测器，以缩小分布差距。\n2.  **技能策略（π_θ_i）微调**：采用残差强化学习范式。以从生成数据中训练的行为克隆策略作为基础策略π^base，训练一个输出动作残差的残差策略π^res_θ_i。最终技能动作为π_θ_i(o_t) = π^base_i(o_t) + π^res_θ_i(o_t)。训练目标是最大化正则化后的期望回报，正则项惩罚残差动作的L2范数，以约束对基础策略的偏离。\n3.  **终止分类器（𝒯_θ_i）微调**：重点解决手工设计终止条件可能带来的误报（错误提前终止）问题。提出一种简化方案：训练一个预测器p_i来估计在当前状态下终止本阶段后任务最终成功的概率。然后，将原始终止条件𝒯^r_i与一个阈值ϵ_term结合，生成新的终止条件T_i(s) := 𝒯^r_i(s) · [p_i(s) > ϵ_term]，从而过滤掉低成功概率的终止点。\n\n![组件微调示意](https://arxiv.org/html/2512.16861v1/x3.png)\n> **图3**：三个组件的微调方式示意。(A)姿态预测器向特权教师蒸馏；执行时根据新观察更新预测，偏差过大时重新规划。(B)技能策略通过残差强化学习微调。(C)从终止预测器中清除误报预测。\n\n部署时，ReinforceGen混合策略按顺序执行各个技能：对每个技能，预测起始姿态并规划路径，执行中可重规划；到达后执行微调后的技能策略，直到终止预测器触发。\n\n## 实验与结果\n**实验设置**：在基于Robosuite的长视野操作任务上进行基准测试，包括Nut Assembly、Threading、Three Piece、Coffee和Coffee Preparation任务的D2变体（最大初始化范围），最多5个阶段。观测空间包括前视和腕部相机RGB图像以及本体感知。每个任务收集10个人类源演示，并自动生成1000条演示的BC数据集。\n**基线方法**：HSP-Priv（使用特权姿态预测器的混合技能策略）、HSP-Priv + Skill-FT（对其技能策略进行微调作为性能上界）、以及使用HSP-Priv作为教师策略通过在线轨迹蒸馏的HSP。\n**关键结果**：ReinforceGen在所有任务的最高重置范围设置下，使用视觉运动控制达到了80%的总成功率，几乎将先前最先进方法（SkillMimicGen）的成功率翻倍。消融研究表明，微调方法贡献了平均89%的性能提升。\n\n![姿态误差影响](https://arxiv.org/html/2512.16861v1/x4.png)\n> **图4**：在Nut Assembly任务第二阶段，随着添加到姿态目标的噪声水平增加，成功率急剧下降。这说明了起始姿态预测准确性的关键作用。\n\n![定性结果-Threading](https://arxiv.org/html/2512.16861v1/figures/qual-threading.png)\n> **图5**：Threading任务的定性结果展示。\n\n![定性结果-ThreePiece](https://arxiv.org/html/2512.16861v1/figures/qual-threepiece.png)\n> **图6**：Three Piece任务的定性结果展示。\n\n![消融实验-微调贡献](https://arxiv.org/html/2512.16861v1/x5.png)\n> **图12**：消融实验展示各微调组件的贡献。与基线HSP相比，引入姿态预测器蒸馏、技能策略RL微调以及终止条件过滤，均带来了显著且递增的性能提升。\n\n![消融实验-学习终止条件](https://arxiv.org/html/2512.16861v1/x6.png)\n> **图13**：使用学习到的终止条件（Learned Term）与手工设计终止条件（Handcrafted Term）的对比。学习到的终止条件性能接近手工设计条件，证明了其可行性。\n\n**消融实验总结**：图12的消融研究表明，各微调组件对最终性能均有重要贡献。从基线HSP开始，依次添加姿态蒸馏、技能RL微调和终止过滤，性能逐步提升至最高。特别是技能RL微调贡献了最大的单步性能增益。图13表明，学习到的终止分类器可以替代手工设计条件，且性能相当。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了ReinforceGen框架，首次将在线强化学习微调与自动化分层数据生成相结合，突破了源演示质量对生成数据性能的上限。\n2.  针对混合技能策略的三个核心组件（起始姿态预测、技能执行、终止判断），分别设计了有效的微调方法（实时重规划与蒸馏、残差RL、基于成功概率的终止过滤）。\n3.  在多个长视野接触式操作任务上实现了显著性能提升（80%总成功率），并验证了框架能够蒸馏训练出高效的端到端视觉运动策略。\n\n**局限性**：论文提到，方法假设每个任务的阶段序列及每个技能的参考物体由人工标注。此外，在评估中仍使用了手工设计的阶段终止条件（尽管消融实验证明了学习终止条件的可行性）。\n\n**后续启示**：ReinforceGen展示了结合开环数据生成与闭环在线优化的强大潜力。其模块化的微调思路可扩展至其他分层策略框架。如何进一步减少对人工标注（如阶段分解）的依赖，以及将学习到的终止条件完全整合到在线RL循环中，是值得探索的方向。此外，该方法为在更复杂、动态场景中利用少量演示快速提升策略鲁棒性提供了可行路径。",
      "imageUrls": [
        "https://arxiv.org/html/2512.16861v1/x1.png",
        "https://arxiv.org/html/2512.16861v1/x2.png",
        "https://arxiv.org/html/2512.16861v1/x3.png",
        "https://arxiv.org/html/2512.16861v1/x4.png",
        "https://arxiv.org/html/2512.16861v1/figures/qual-threading.png",
        "https://arxiv.org/html/2512.16861v1/figures/qual-threepiece.png",
        "https://arxiv.org/html/2512.16861v1/figures/env_examples/coffee.png",
        "https://arxiv.org/html/2512.16861v1/figures/env_examples/threading.png",
        "https://arxiv.org/html/2512.16861v1/figures/env_examples/nut_assembly.png",
        "https://arxiv.org/html/2512.16861v1/figures/env_examples/three_piece.png",
        "https://arxiv.org/html/2512.16861v1/figures/env_examples/coffee_full.png",
        "https://arxiv.org/html/2512.16861v1/x5.png",
        "https://arxiv.org/html/2512.16861v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.16811",
      "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
      "url": "http://arxiv.org/abs/2512.16811",
      "arxivId": "2512.16811",
      "date": "2025-12-18",
      "authors": "Li Jiang Team",
      "category": "Manipulation",
      "summary": "本文提出GeoPredict框架，解决现有视觉-语言-动作模型在机器人操作中缺乏3D空间推理能力、反应式决策的问题。方法核心包括：轨迹级模块编码运动历史并预测多步3D关键点轨迹；预测性3D高斯几何模块沿轨迹预测工作空间几何。这些模块仅用于训练时深度渲染监督，推理时仅需轻量查询令牌。实验表明，在RoboCasa Human-50、LIBERO及真实任务中，GeoPredict显著优于现有VLA基线，尤其在几何密集与高空间精度要求场景中表现突出。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型通过利用大规模预训练的视觉语言模型，在机器人操作任务中展现出强大的泛化能力。然而，这些模型本质上是反应式的，且主要基于2D图像空间进行映射，缺乏对3D空间和场景动态的显式建模。这导致它们在需要精确3D推理（如物体位姿、末端执行器运动轨迹）和物理一致的长时域控制任务中表现不佳。为解决此问题，近期研究开始将预测结构引入模型，例如预测未来RGB帧或深度图，但这些方法通常缺乏严格的多视角3D几何一致性，且若在推理时进行复杂的3D预测会引入显著计算开销。本文针对VLA模型在3D几何和长时域动态推理上的不足，提出了一种新视角：为连续动作策略注入预测性的运动学和几何先验。核心思路是，在训练阶段联合学习预测未来的机器人关键点轨迹和基于3D高斯溅射（3DGS）的未来场景几何，以此作为监督信号塑造模型的内部表征，而在推理时则无需调用任何3D解码模块，保持高效。\n\n## 方法详解\nGeoPredict的整体框架建立在连续动作VLA策略（π₀）之上，并引入了两个互补的预测模块：轨迹级运动预测模块和预测性3D高斯几何模块。这两个模块仅在训练时作为监督信号使用。\n\n![方法总览](https://arxiv.org/html/2512.16811v1/x1.png)\n\n> **图1**：GeoPredict整体框架。给定指令、多视角图像和由轨迹编码器编码的运动历史，一个核心的LLM Transformer学习两个主要任务：1）使用可学习的未来轨迹查询预测多时间步的3D关键点轨迹；2）通过体素解码器处理3D空间查询，预测未来工作空间的3D高斯几何。轨迹引导的细化机制利用预测的未来轨迹，将几何建模能力分配到任务相关的交互区域。策略最终通过动作专家生成动作。这些预测模块仅在训练时提供监督，推理时不调用。\n\n**核心模块一：轨迹级运动预测**\n该模块旨在为模型提供紧凑的运动历史总结和明确的未来轨迹预测。首先，**轨迹编码器** 跟踪K个3D关键点（关节和末端执行器点），将每个关键点从时间0到t-1的轨迹通过一个共享的可学习历史查询进行交叉注意力编码，生成K个历史轨迹令牌，这些令牌编码了惯性、关节限制等运动先验。其次，引入K个**可学习的未来轨迹查询**，它们与指令、当前图像和历史轨迹令牌一起被Transformer处理，生成每个关键点的未来轨迹嵌入。通过一个共享的MLP并结合1D正弦时间位置编码，将这些嵌入解码为显式的时空轨迹（覆盖当前时刻t及未来H步）。该预测通过均方误差损失进行监督，促使Transformer学习具有预测性和动态一致的运动表征。\n\n**核心模块二：预测性3D高斯几何**\n该模块预测机器人工作空间的未来几何演变，为策略提供更强的空间推理能力。其流程如下：\n1.  **3D空间查询**：将工作空间离散化为一个粗粒度体素网格，每个体素分配一个可学习的嵌入，并加上3D正弦空间位置编码，形成空间查询令牌序列输入Transformer。\n2.  **体素解码器**：经过注意力层后，获得空间嵌入。通过叠加与轨迹预测相同的时间位置编码，为未来每个时间步构造时序偏移的空间嵌入。每个时间步的嵌入经过一个由转置卷积和上采样层组成的3D体素解码器，恢复至高分辨率体素特征网格。\n3.  **轨迹引导的高斯细化**：体素特征被映射为初始的3D高斯基元集合。为了在交互区域获得高保真几何，利用预测的未来关键点轨迹生成一个二值细化掩码。对于被轨迹穿过的体素，使用一个共享MLP从体素特征生成更多、更精细的高斯基元，从而将建模能力集中在预期的运动路径附近。\n4.  **未来深度渲染与监督**：使用可微的alpha合成，从每个时间步的总高斯集合渲染深度图。通过一个空间掩码（仅限工作空间内）计算渲染深度与真实深度之间的L1损失，监督3DGS模型准确捕捉相关区域的几何演变。\n\n**注意力机制与训练推理**\n模型采用块级因果注意力机制来整合各模块。\n\n![注意力机制](https://arxiv.org/html/2512.16811v1/x2.png)\n\n> **图2**：块级因果注意力机制。令牌被分为五个概念上有序的组：2D令牌（文本和图像）、3D令牌（历史轨迹）、3D查询令牌（未来轨迹查询和空间查询）、状态令牌（本体感知）和动作噪声令牌。块内注意力是全连接的，而跨块注意力是严格因果的（仅能关注前面块），这形成了一个从感知到预测再到控制的层次结构。\n\n训练时，总损失是动作损失（来自π₀的连续条件流匹配损失）、轨迹预测损失和深度渲染损失的加权和。**推理时**，模型仅进行一次前向传播计算并缓存注意力键值对，动作专家进行迭代去噪生成动作块。预测性3D高斯几何模块（体素解码器、渲染）完全不执行，因此推理开销与基础VLA策略相近。\n\n## 实验与结果\n**实验设置**：在仿真基准**RoboCasa Human-50**（24个复杂长视界厨房任务，每任务仅50条人类演示）和**LIBERO**（四个任务套件，评估知识迁移）上进行评估。同时构建了包含**空间泛化**、**几何泛化**和**视觉鲁棒性**三类任务的真实世界评估套件，每类任务使用50条专家轨迹训练。\n\n**对比方法**：包括强大的VLA基线模型π₀、OpenVLA、TraceVLA，以及结合了未来预测的方法VideoVLA和WorldVLA。\n\n**关键结果**：\n1.  **RoboCasa结果**：GeoPredict在24个子任务上的平均成功率达到**54.8%**，显著优于最佳基线π₀的**46.7%**，相对提升超过**17%**。在需要精确对齐和长序列操作的“几何密集型”任务（如`close_jar`, `place_wine`）上提升尤为明显。\n\n![仿真结果表](https://arxiv.org/html/2512.16811v1/x4.png)\n\n> **图4**：RoboCasa仿真基准结果表。GeoPredict在24个任务中的20个上优于或匹配最佳基线，平均成功率（54.8%）显著领先。\n\n2.  **LIBERO结果**：GeoPredict在所有四个套件（Spatial, Object, Goal, Long）上均取得最佳性能，整体平均成功率为**80.5%**，相比π₀的**75.6%**有稳定提升，证明了其在任务结构和目标泛化上的有效性。\n3.  **真实世界结果**：GeoPredict在空间泛化、几何泛化和视觉鲁棒性任务上的平均成功率分别为**85.0%**, **71.7%** 和 **90.0%**，全面优于π₀（分别为70.0%, 58.3%, 75.0%）。特别是在几何泛化中，面对未训练过的物体尺寸和姿态，GeoPredict展现了更强的适应能力。\n\n![真实世界评估](https://arxiv.org/html/2512.16811v1/x3.png)\n\n> **图3**：真实世界评估套件示意图。展示了空间泛化（盘子位置变化）、几何泛化（物体尺寸和姿态变化）和视觉鲁棒性（背景干扰物）三种测试场景。\n\n**消融实验总结**：\n-   移除**轨迹引导细化**会导致性能显著下降，特别是在几何密集型任务上，证明了在交互区域集中几何容量的重要性。\n-   移除**整个预测性3D几何模块**（仅保留轨迹预测）性能也会下降，表明几何先验和运动学先验具有互补性。\n-   使用**更简单的3D表示（体素）替代3DGS**或**使用RGB而非深度进行渲染监督**都会导致性能降低，验证了3DGS的高效性和深度监督对几何学习的有效性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**GeoPredict**，一个将未来感知的运动学和几何先验注入连续动作VLA模型的框架，增强了其对长时域3D动态的推理能力。\n2.  设计了两个互补的预测模块：**轨迹级运动预测器**和**带有轨迹引导细化的预测性3D高斯几何模块**，前者提供明确的运动先验，后者可动态分配几何建模能力。\n3.  通过**仅训练时使用**的深度渲染监督和**块级因果注意力**设计，在显著提升多种任务（特别是几何密集型任务）性能的同时，保持了与基础VLA策略相近的推理效率。\n\n**局限性**：论文提到，当前方法依赖于多视角图像进行3D监督，在极其稀疏的视图（如单目）设置下可能面临挑战。此外，3D高斯几何的预测范围受限于预定义的工作空间体积。\n\n**启示**：这项工作表明，通过精心设计的、仅用于训练的预测性任务（尤其是显式的3D几何预测）来塑造VLA模型的内部表征，是提升其空间推理和长时域规划能力的有效途径，且不会牺牲部署时的效率。这为未来开发更“具身”且高效的VLA模型提供了一个有前景的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2512.16811v1/x1.png",
        "https://arxiv.org/html/2512.16811v1/x2.png",
        "https://arxiv.org/html/2512.16811v1/x3.png",
        "https://arxiv.org/html/2512.16811v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.16842",
      "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
      "url": "http://arxiv.org/abs/2512.16842",
      "arxivId": "2512.16842",
      "date": "2025-12-18",
      "authors": "Paul Pu Liang Team",
      "category": "Manipulation",
      "summary": "本文旨在解决真实世界中缺乏同步全手触觉、第一人称视觉与手部姿态数据的问题。为此，研究团队构建了首个大规模真实环境下的多模态数据集OpenTouch，其核心技术是使用压力传感手套，同步采集了5.1小时视频-触觉-姿态数据，包含约800种物体在14种环境下的交互。实验表明，该数据集中的触觉信号为抓取理解提供了强有力的线索，能有效增强跨模态对齐，并能从真实世界视频中可靠地检索出对应的触觉信息。",
      "detailedSummary": "## 研究背景与动机\n当前，以自我为中心（Egocentric）的感知研究主要依赖于大规模视觉数据集，侧重于捕捉世界的外观，而忽略了交互时的触觉感受。人类与物理世界的交互高度依赖视觉、触觉和本体感觉的紧密耦合，触觉能提供关于接触、力、稳定性和材料属性等视觉无法解析的关键信息。然而，在野外（in-the-wild）环境中捕获全手触觉面临巨大挑战：现有触觉传感系统要么局限于简化抓取（如平行夹爪），要么只能在受控实验室环境中工作（如基于动作捕捉手套或压力垫的系统），缺乏环境多样性。因此，领域内缺少一个在多样化、非受控的日常环境中，同步捕获自我中心视觉、全手触觉和详细手部姿态的大规模数据集。\n\n本文针对视觉感知与物理交互之间的脱节这一具体痛点，提出了从多模态（视觉-触觉-姿态）数据融合的新视角来理解人类交互。其核心思路是：构建一个低成本、可穿戴的传感系统，在真实世界场景中同步收集高质量的自我中心视频、全手压力触觉图和3D手部姿态数据，并以此为基础建立首个野外全手触觉数据集（OpenTouch）及相应的检索与分类基准，以探索触觉如何增强感知与行动的理解。\n\n## 方法详解\nOpenTouch 的整体框架包含硬件传感系统、数据收集与标注流程，以及基于此的基准任务定义。\n\n![方法框架](https://arxiv.org/html/2512.16842v1/x4.png)\n\n> **图4**：数据捕获与标注系统概览。Meta Aria眼镜、Rokoko Smartgloves和基于FPC的触觉传感器以30 Hz频率同步，平均延迟2 ms。系统捕获同步的自我中心视频、手部姿态和密集的全手触觉信号。高层描述和详细标注通过大语言模型基于自我中心视频和渲染的触觉图自动生成。\n\n**硬件系统**由三个核心模块组成：\n1.  **定制化触觉传感手套**：为解决现有触觉手套在分辨率、覆盖范围和可复现性上的权衡，本文设计了一种薄型、低成本、完全开源的手形传感器。其核心是柔性印刷电路板（FPC），其上布置了16×16的电极网格，环绕着一片商用压阻薄膜，形成了均匀覆盖手指和手掌的169个触觉单元（taxels）。这种设计结合了PCB级的精度、可靠性与穿戴所需的柔顺性。\n2.  **手部追踪手套**：使用Rokoko Smartgloves，这是一种基于融合IMU和电磁场（EMF）传感的专业动作捕捉系统。每只手套提供七个6自由度传感器，以30 Hz频率流式传输，旋转动态精度为±1°。\n3.  **自我中心视觉捕获**：使用Meta的Project Aria眼镜进行记录。该设备集成了单目SLAM相机、视点RGB相机（视野110°）、眼动追踪相机、IMU和麦克风。基准测试主要使用1408×1408像素、30 Hz的RGB视频流。\n\n系统通过终端显示并由RGB相机捕获的视觉线索进行时间同步。\n\n**数据收集与标注**：\n- **收集协议**：数据在14个日常环境（如厨房、办公室、车间）中收集，参与者被要求自由探索和操纵环境中所有可用物体。仅对右手（优势手）进行仪器化，以简化硬件和标准化标注，同时通过镜像和姿态重标注保留左手泛化能力。\n- **自动化标注**：为高效标注海量野外数据，采用GPT-5进行自动化标注。对于每个视频片段，模型接收三个按时间顺序排列的RGB-触觉图像对。这三个关键帧是根据压力动态采样的：峰值压力前的最低压力（接近）、峰值压力（操纵）、峰值压力后的最低压力（释放）。GPT-5根据这些样本识别环境、物体、物体类别、主要动作、抓握类型（依据GRASP taxonomy）并生成自然语言描述。人工验证显示标注准确率约为90%。\n\n**基准任务**：\nOpenTouch 引入了两项基准任务：\n1.  **跨感官检索**：学习一个共享的嵌入空间，使得语义对应的跨模态序列彼此接近。具体包括：\n    - 视频↔触觉序列检索\n    - 姿态↔触觉序列检索\n    - 多模态→单模态检索（如视频+姿态→触觉）\n2.  **触觉模式分类**：使用多模态信号识别（1）手部动作和（2）抓握类型，以探究触觉如何消除动作和抓握的歧义。\n\n**方法创新点**主要体现在：\n1.  **硬件创新**：设计了基于FPC的低成本、易复现、覆盖全手的触觉传感手套，与商业动捕手套、智能眼镜结合，实现了非侵入式、高精度的野外多模态数据同步采集。\n2.  **数据创新**：首次在野外环境中提供了同步的自我中心RGB视频、全手触觉图和3D手部姿态的大规模三元模态数据集。\n3.  **标注创新**：利用大语言模型（GPT-5）基于关键帧进行自动化标注，显著提升了大规模野外数据标注的效率。\n\n## 实验与结果\n实验在 OpenTouch 数据集自建的基准上进行。对比的基线方法包括：用于检索的线性方法（典型相关分析CCA、偏最小二乘相关分析PLSCA）和基于CLIP风格的深度对比学习方法（本文方法）；用于分类的ResNet-18编码器和轻量级CNN触觉编码器。\n\n**关键定量结果**：\n1.  **跨感官检索**：如表2所示，本文的深度对比方法在所有检索方向上均大幅优于线性基线。\n\n![检索结果表](https://arxiv.org/html/2512.16842v1/x65.png)\n\n> **表2**：双模态与三模态跨感官检索结果。结合多种模态能持续提升检索性能，表明视频、姿态和触觉线索提供了互补信息。\n\n    - 视频↔触觉检索：本文方法在视频→触觉和触觉→视频方向上的R@1均达到7.15%，而线性基线最高仅0.71%。\n    - 姿态↔触觉检索：同样显示出深度方法的巨大优势（R@1从0.57%提升至7.15%），证实了触觉与手部构型间的强几何耦合。\n    - 多模态→单模态检索：性能显著提升。例如，视频+姿态→触觉检索的mAP达到26.86%，视频+触觉→姿态检索的mAP达到26.86%，表明多模态输入通过提供互补信息减少了检索的模糊性。\n\n2.  **分类性能**：如表3所示，触觉信号对于抓握类型识别极具信息量（仅触觉准确率最高达60.23%），但对于动作识别较弱（仅~30%）。视觉在两项任务上都有不错表现（动作40.26%，抓握57.45%）。融合所有模态（触觉+姿态+视觉）并使用轻量级CNN触觉编码器时，取得了最佳的抓握类型分类准确率68.09%。\n\n![分类结果表](https://arxiv.org/html/2512.16842v1/x66.png)\n\n> **表3**：动作与抓握分类准确率。触觉对抓握识别最有效；触觉与视觉结合获得最佳整体性能。\n\n**定性结果**：\n\n![定性检索结果](https://arxiv.org/html/2512.16842v1/x67.png)\n\n> **图5**：定性检索结果示例。左：视频→触觉检索，检索到的触觉图与真实压力分布高度相似。右：触觉→视频检索，从触觉输入中检索到描绘相似交互的视频。\n\n图5展示了跨模态检索能够匹配语义和物理一致的序列，例如检索到相似的压力分布或抓取行为。\n\n![Ego4D零样本检索](https://arxiv.org/html/2512.16842v1/x68.png)\n\n> **图6**：在Ego4D上的零样本触觉检索定性结果。给定Ego4D查询视频（顶行），模型从OpenTouch中检索出最相似的触觉序列（t@1, t@2）及其对应视频（v@1, v@2），显示出手部运动和接触物体几何形状的相似性。\n\n图6展示了模型在未见过的Ego4D数据集上进行零样本视频→触觉检索的能力，检索到的触觉模式对应的源视频与查询视频在行为和操纵基元上高度相似，证明了OpenTouch作为触觉数据库的泛化潜力。\n\n**消融实验总结**：\n- **时序窗口大小**：检索性能随着滑动窗口从5帧增加到20帧而提升，更长窗口能更好地建模接触的时序演化。\n- **触觉编码器容量**：轻量级CNN触觉编码器在所有检索方向上均大幅优于更深层的ResNet-18编码器，表明触觉信号的稀疏和结构化特性更适合紧凑的、能保持局部接触拓扑的编码器。\n- **触觉离散化策略**：适度的离散化（5-7级）通常能作为有效的正则化器，在降低传感器噪声的同时不严重损害性能；原始连续触觉图也是一个强大的默认选择。\n\n## 总结与启发\n本文的核心贡献有三点：\n1.  **提出了一套实用、低成本的传感系统**，能够在野外捕获同步的自我中心RGB视频、全手触觉图和手部姿态，并提供了校准、去噪和时间对齐的支持软件。\n2.  **构建了首个大规模野外三元模态数据集OpenTouch**，包含5.1小时同步数据和2900个人工审核片段，配有物体、环境、动作、抓握类型和自然语言描述等多维度标注。\n3.  **建立了视觉-触觉-姿态学习的基准**，并通过系统的消融实验揭示了如何高效编码全手触觉信号，以及触觉如何最有效地与视觉和姿态互补。\n\n论文自身提到的局限性包括：数据收集仅针对右手（优势手），尽管通过镜像保留了左手泛化的可能性；基于GPT-5的自动标注在光照困难、手部离开视野或上下文有限时可能出错（约10%错误率）。\n\n本文的启示在于：\n- **触觉是紧凑而强大的信号**：轻量级触觉编码器即可有效捕捉抓握等交互的语义信息，为高效的多模态学习提供了新思路。\n- **多模态融合至关重要**：视觉、触觉和姿态在理解交互中扮演互补角色，它们的结合能显著提升感知和理解能力。\n- **为具身智能和机器人操作提供真实数据基础**：OpenTouch数据集使得利用真实人类交互数据来增强机器人对接触和力的理解成为可能，例如通过检索为大规模自我中心视频（如Ego4D）补充接触和力线索。",
      "imageUrls": [
        "https://arxiv.org/html/2512.16842v1/x1.png",
        "https://arxiv.org/html/2512.16842v1/x2.png",
        "https://arxiv.org/html/2512.16842v1/x3.png",
        "https://arxiv.org/html/2512.16842v1/x4.png",
        "https://arxiv.org/html/2512.16842v1/x65.png",
        "https://arxiv.org/html/2512.16842v1/x66.png",
        "https://arxiv.org/html/2512.16842v1/x67.png",
        "https://arxiv.org/html/2512.16842v1/x68.png",
        "https://arxiv.org/html/2512.16842v1/x69.png",
        "https://arxiv.org/html/2512.16842v1/x75.jpg",
        "https://arxiv.org/html/2512.16842v1/x76.jpg",
        "https://arxiv.org/html/2512.16842v1/x77.jpg",
        "https://arxiv.org/html/2512.16842v1/x78.jpg",
        "https://arxiv.org/html/2512.16842v1/x79.jpg",
        "https://arxiv.org/html/2512.16842v1/x85.jpg",
        "https://arxiv.org/html/2512.16842v1/x86.jpg",
        "https://arxiv.org/html/2512.16842v1/x87.jpg",
        "https://arxiv.org/html/2512.16842v1/x88.jpg",
        "https://arxiv.org/html/2512.16842v1/x89.jpg",
        "https://arxiv.org/html/2512.16842v1/x90.png",
        "https://arxiv.org/html/2512.16842v1/figs/video.jpg",
        "https://arxiv.org/html/2512.16842v1/x91.png",
        "https://arxiv.org/html/2512.16842v1/x92.png",
        "https://arxiv.org/html/2512.16842v1/figs/sensor_layout.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.16881",
      "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
      "url": "http://arxiv.org/abs/2512.16881",
      "arxivId": "2512.16881",
      "date": "2025-12-18",
      "authors": "Karl Pertsch Team",
      "category": "Manipulation",
      "summary": "本文针对通用机器人策略性能评估的挑战，提出PolaRiS框架。核心问题是真实评估耗时、不可控，而模拟评估存在视觉与物理领域差距。PolaRiS利用神经重建方法将真实场景短视频转换为高保真交互式模拟环境，并通过模拟数据协同训练实现零样本评估。实验表明，PolaRiS评估与真实世界性能的相关性显著强于现有模拟基准，且能快速创建多样化环境。",
      "detailedSummary": "## 研究背景与动机\n机器人学习研究面临的一个重大挑战是准确测量和比较机器人策略性能的能力。由于真实世界评估的随机性、难以复现和耗时等特性，机器人领域的基准测试历来充满挑战。这一挑战对于需要跨多种场景和任务进行评估的通用策略而言尤为突出。仿真评估为真实世界评估提供了一种可扩展的补充，但现有仿真基准与真实世界之间的视觉和物理领域差距使其成为策略改进的不可靠信号。此外，构建逼真且多样化的仿真环境传统上需要大量的人力投入和专业知识。为了弥合这一差距，本文提出了PolaRiS，一个用于高保真仿真机器人评估的可扩展真实到仿真框架。PolaRiS利用神经重建方法，将真实世界场景的短视频扫描转换为交互式仿真环境。此外，本文开发了一种简单的仿真数据协同训练方法，以弥合剩余的领域差距，并实现在未见过的仿真环境中的零样本评估。核心思路是：通过神经重建快速创建高保真仿真环境，并辅以轻量级的仿真数据协同训练，从而实现对通用策略性能的、与真实世界强相关的可扩展仿真评估。\n\n## 方法详解\nPolaRiS的整体目标是在仿真环境中评估真实世界的通用机器人策略，并使仿真评估能忠实反映真实世界性能。其框架包含两个核心部分：1）利用2D高斯泼溅（2DGS）等技术从视频快速重建高保真、可交互的仿真环境；2）提出一种轻量级的仿真数据协同训练策略，以弥合视觉领域差距，确保策略在未见过的PolaRiS环境中也能被准确评估。\n\n![方法总览](https://arxiv.org/html/2512.16881v2/figures/Teaser_Karl_version.jpg)\n> **图1**：PolaRiS概述。这是一个真实到仿真的方法，将真实世界场景的短视频转换为用于可扩展机器人策略评估的高保真仿真环境。通过少量迭代的仿真数据协同微调，可以使通用策略在未见过的PolaRiS环境中进行准确的零样本评估。\n\n**环境生成流程**：PolaRiS采用组合式方法创建环境。首先，用户使用普通相机拍摄场景（静态背景）和机器人的单独短视频（约2-5分钟），并放置ChArUco标定板以辅助确定全局方向和度量尺度。随后，使用COLMAP估计相机参数，并训练独立的2D高斯泼溅模型来分别重建场景和机器人的视觉外观与几何。每个高斯泼溅元组表示为 $g_j = (\\bm{\\mu}_j, \\mathbf{R}_j, \\mathbf{S}_j, \\mathbf{c}_j, \\alpha_j)$，分别代表中心、朝向、协方差、颜色和不透明度。为了进行物理仿真，需要从重建结果中提取网格。具体做法是：通过栅格化泼溅和体素深度融合获得截断有符号距离场（TSDF），再使用行进立方体算法提取显式网格 $\\mathcal{M}$，最后将其导入IsaacSim物理引擎。\n\n![环境创建流程](https://arxiv.org/html/2512.16881v2/figures/polaris_pipeline.jpg)\n> **图2**：PolaRiS中的环境创建。用户首先扫描真实世界环境、机器人和物体。然后使用PolaRiS创建捕获环境几何和视觉外观的关节化高斯泼溅表示。最后，通过组合扫描的场景、物体和机器人来构建用于策略评估的仿真场景。\n\n**自动关节化**：为了使机器人模型在仿真中可运动，PolaRiS将高斯泼溅图元锚定到机器人的运动学连杆上。对于具有连杆 $\\{\\mathcal{L}_1, \\ldots, \\mathcal{L}_L\\}$ 和关节配置 $\\mathbf{q}$ 的机器人，每个连杆关联一组高斯图元 $\\mathcal{G}_\\ell$。当关节运动时，通过正向运动学获得连杆的刚体变换 $T_\\ell(\\mathbf{q})$，并据此更新每个关联高斯的中心位置 $\\bm{\\mu}_{\\ell j}^{\\prime}$ 和协方差 $\\Sigma_{\\ell j}^{\\prime}$，从而实现高斯泼溅与底层网格模型的同步运动与渲染。\n\n**物体创建与场景组合**：对于场景中的物体，PolaRiS利用生成模型（如TRELLIS）从多视角图像创建3D资产。用户通过SAM2分割物体图像，输入TRELLIS获得结构化潜在表示，进而解码为用于外观的高斯泼溅和用于几何的网格。最后，所有组件（环境、机器人、物体）通过一个场景组合GUI在欧几里得空间中组合、变换，并导出为USD文件用于评估。创建新评估环境的总人力时间通常少于20分钟，总耗时（包括高斯泼溅训练）通常少于一小时。\n\n![场景组合GUI](https://arxiv.org/html/2512.16881v2/figures/scene_comp_gui.jpg)\n> **图3**：场景组合GUI示例。用户可以轻松导入环境和物体资产，自动加载DROID机器人，并将仿真就绪的环境保存为USD文件。该过程通常只需不到5分钟。\n\n**协同训练以提升评估相关性**：尽管能创建高保真环境，但策略在未经调整的情况下直接评估，其仿真与真实性能的相关性可能仍不理想。论文指出，视觉不匹配（如细微光照变化、缺失的物体间阴影）是主要误差来源。为此，PolaRiS提出了一种策略无关的仿真数据协同训练方案。具体而言，在一组仿真协同训练环境 $\\mathcal{E}_{\\mathcal{S}}^{\\text{train}}$ 中，通过人工遥操作收集一个小的仿真数据集 $\\mathcal{D}_{\\mathcal{S}} = \\{(o_t^{\\mathcal{S}}, a_t^{\\mathcal{S}}, p_t^{\\mathcal{S}})\\}$。然后，将预训练的通用策略 $\\pi_\\theta$ 在其原始训练数据 $\\mathcal{D}_{\\text{pre}}$ 和仿真数据 $\\mathcal{D}_{\\mathcal{S}}$ 的混合批次上进行微调，优化标准行为克隆损失 $\\mathcal{L}_{\\text{BC}}$：\n$\\theta^{\\prime} \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathbb{E}_{(o,a,p)\\sim(1-\\lambda)\\mathcal{D}_{\\text{pre}}+\\lambda\\mathcal{D}_{\\mathcal{S}}}\\big[ \\mathcal{L}_{\\text{BC}}(\\pi_{\\theta}(o,p), a) \\big]$\n其中 $\\lambda$ 控制仿真数据在批次中的混合比例。此方法旨在提高策略对仿真视觉差异的鲁棒性，而非学习新任务技能。仅需数百个仿真 episode 和数百步微调（<25分钟），即可显著提升真实到仿真的相关性，且微调后的策略可在未见过的仿真测试环境 $\\mathcal{E}_{\\mathcal{S}}^{\\text{test}}$ 中进行零样本评估。\n\n![协同训练数据集环境](https://arxiv.org/html/2512.16881v2/x1.png)\n> **图4**：PolaRiS仿真协同训练数据集环境。在仿真中收集少量演示数据，并用此数据协同微调策略，以提高真实到仿真评估的相关性。一旦微调完成，策略在PolaRiS中的评估（即使在未见过的环境中）也能表现出与真实世界的强相关性。\n\n## 实验与结果\n**实验设置**：实验在DROID机器人平台和RoboArena真实世界基准上进行。评估的策略包括OpenVLA、Octo、GR00T-N1和BlackPI0等通用策略。对比的基线方法包括直接在真实世界评估、在现有仿真基准（如SIMPLER）中评估，以及使用世界模型（视频预测模型）进行评估。评估指标为皮尔逊相关系数（$r$）和平均最大秩违反（MMRV），以衡量仿真评估与真实世界评估的相关性和排名一致性。\n\n**关键实验结果**：\n1.  **与真实世界的强相关性**：在DROID平台上，PolaRiS评估与最先进通用策略的真实世界性能表现出强相关性，平均皮尔逊相关系数 $r = 0.9$。PolaRiS评估也与迄今为止最全面的通用策略真实世界基准RoboArena的策略得分高度相关（$r = 0.98$）。\n\n![RoboArena相关性](https://arxiv.org/html/2512.16881v2/figures/roboarena_correlation.png)\n> **图8**：PolaRiS仿真评估得分与RoboArena真实世界评估得分的相关性。两者呈现出极强的线性相关（$r=0.98$），表明PolaRiS能高度预测策略在真实基准上的表现。\n\n2.  **超越现有仿真基准**：实验表明，现有仿真基准（如SIMPLER）与真实世界性能的相关性较弱（$r=0.32$），而PolaRiS（未协同训练时 $r=0.72$，协同训练后 $r=0.9$）显著优于它们。此外，基于世界模型的评估方法也未能提供可靠的相关性。\n\n![视觉保真度消融](https://arxiv.org/html/2512.16881v2/figures/visual_fidelity_pearson_r_and_mmrv.png)\n> **图9**：不同视觉保真度方法对评估相关性的影响。PolaRiS（2DGS）在皮尔逊相关系数（r）和平均最大秩违反（MMRV）上均优于背景抠图（Green Screen）和程序化生成（Proc.）等方法。\n\n3.  **协同训练的有效性**：消融实验证实了仿真数据协同训练的关键作用。仅使用PolaRiS环境而不进行协同训练，相关性为 $r=0.72$；加入协同训练后，相关性提升至 $r=0.9$。实验还探索了协同训练数据混合比例 $\\lambda$ 的影响，发现 $\\lambda=0.5$ 通常能取得最佳效果。\n\n![数据混合比例消融](https://arxiv.org/html/2512.16881v2/figures/data_mix_pearson_r_and_mmrv.png)\n> **图10**：协同训练中仿真数据混合比例（λ）对评估相关性的影响。适中的混合比例（如λ=0.5）能取得最佳的皮尔逊相关系数（r）和最低的平均最大秩违反（MMRV）。\n\n4.  **支持手腕相机**：与依赖背景抠图的方法（如SIMPLER）不同，PolaRiS的3D重建支持从任意角度渲染，包括机器人手腕相机的视角，从而能够评估主流的、配备手腕相机的通用策略。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了PolaRiS，一个通过神经场景重建创建高保真仿真评估环境的可扩展框架，显著降低了构建定制化仿真环境的门槛（<1小时）。\n2.  开发了一种简单的仿真数据协同训练方法，通过轻量级微调（<25分钟）有效弥合真实到仿真的视觉领域差距，使通用策略能在未见过的PolaRiS环境中进行零样本且高相关的评估。\n3.  通过大量实验验证了PolaRiS评估与真实世界性能的强相关性（在DROID上 $r=0.9$，在RoboArena上 $r=0.98$），优于现有仿真基准和世界模型评估方法，并开源了所有工具以促进社区共享。\n\n**局限性**：论文提到，当前方法主要处理静态场景和刚体物体，对动态物体（如流动液体）和透明/反射表面的重建仍具挑战性。此外，环境创建依赖于一段质量尚可的视频扫描。\n\n**后续启示**：PolaRiS为机器人社区提供了一个快速创建和共享高保真仿真评估环境的可行路径，有望推动通用策略评估的民主化和分布式协作。未来的工作可以专注于改进重建技术以处理更复杂的物体和场景，并探索如何将PolaRiS框架更无缝地集成到策略开发循环中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.16881v2/figures/Teaser_Karl_version.jpg",
        "https://arxiv.org/html/2512.16881v2/figures/polaris_pipeline.jpg",
        "https://arxiv.org/html/2512.16881v2/figures/scene_comp_gui.jpg",
        "https://arxiv.org/html/2512.16881v2/x1.png",
        "https://arxiv.org/html/2512.16881v2/figures/real_sim.jpg",
        "https://arxiv.org/html/2512.16881v2/x2.png",
        "https://arxiv.org/html/2512.16881v2/x3.png",
        "https://arxiv.org/html/2512.16881v2/figures/roboarena_correlation.png",
        "https://arxiv.org/html/2512.16881v2/figures/visual_fidelity_pearson_r_and_mmrv.png",
        "https://arxiv.org/html/2512.16881v2/figures/data_mix_pearson_r_and_mmrv.png",
        "https://arxiv.org/html/2512.16881v2/figures/metrics_across_steps.png",
        "https://arxiv.org/html/2512.16881v2/x4.png",
        "https://arxiv.org/html/2512.16881v2/supplementary/fruit-bussing_sim.png",
        "https://arxiv.org/html/2512.16881v2/supplementary/cubes-right_sim.png",
        "https://arxiv.org/html/2512.16881v2/supplementary/bussing_sim.png",
        "https://arxiv.org/html/2512.16881v2/supplementary/splat.png",
        "https://arxiv.org/html/2512.16881v2/supplementary/raytraced.png",
        "https://arxiv.org/html/2512.16881v2/supplementary/nightmare.png",
        "https://arxiv.org/html/2512.16881v2/supplementary/double_nightmare.png",
        "https://arxiv.org/html/2512.16881v2/figures/correlation_per_env.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.16302",
      "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation",
      "url": "http://arxiv.org/abs/2512.16302",
      "arxivId": "2512.16302",
      "date": "2025-12-18",
      "authors": "Yang Gao Team",
      "category": "Manipulation",
      "summary": "本文解决当前一次性模仿学习（OSIL）方法难以处理复杂长时程操作任务的问题。提出了ManiLong-Shot框架，其核心是将长时程任务分解为基于物理交互事件的基元序列，而非直接模仿连续轨迹。该方法利用视觉语言模型或基于状态变化的启发式规则驱动分解，对每个基元预测关键的交互不变区域、建立演示与当前场景的对应关系，并计算目标位姿。实验表明，仅在10个短时程任务上训练的模型，可通过单次演示泛化到20个未见长时程任务，相对现有最佳方法性能提升22.8%，并在真实机器人上得到验证。",
      "detailedSummary": "## 研究背景与动机\n当前的一次性模仿学习（OSIL）方法为机器人技能学习提供了无需大规模数据收集的途径，但主要局限于短视野任务，或要求新任务与训练任务高度相似，或依赖已知的3D物体模型。这些限制阻碍了其在复杂、多阶段的长视野操作场景中的应用。本文针对长视野一次性模仿学习这一具体痛点，提出了一种新视角：围绕物理交互事件来结构化长视野任务，将其重新定义为对交互感知基元的排序，而非直接模仿连续轨迹。本文的核心思路是：通过交互感知的任务分解、不变区域预测与匹配，实现长视野抓取操作任务的有效一次性模仿。\n\n## 方法详解\nManiLong-Shot框架旨在通过一次性模仿学习执行未见过的长视野操作任务。其整体流程是：给定一个长视野任务的成功演示，首先将其分解为一系列交互感知基元；对于每个基元，预测其关键的不变交互区域；在执行时，将演示中的预测区域与当前观察场景进行匹配，据此计算目标末端执行器位姿并规划运动；迭代此过程直至任务完成。\n\n![方法整体训练流程](https://arxiv.org/html/2512.16302v1/x2.png)\n> **图2**：ManiLong-Shot的整体训练流程。左侧展示了交互感知区域匹配网络的训练与推理过程，用于对齐演示与当前场景的不变区域并预测位姿。右侧展示了交互感知区域预测网络的训练过程，用于从短视野演示中学习预测每个交互阶段的不变区域。\n\n框架包含三个核心模块：\n1.  **交互感知任务分解**：该模块将演示轨迹组织成基于物理交互阶段的基元序列。受人类策略启发并借鉴现有工作，机器人-物体交互被分为三个阶段：预接触（pre-contact，夹爪张开接近物体）、抓取（grasping，夹爪闭合抓握物体）和后接触（post-contact，夹爪张开放置物体）。短视野任务通常只包含这三个阶段的一个循环，而长视野任务则包含多个循环。分解可通过两种可互换的方式实现：一是基于规则的启发式方法，通过分析关节速度和夹爪状态变化来推断阶段边界；二是利用视觉语言模型（VLM，如GPT-4o），通过定制的提示词和轨迹的结构化表示来自动识别交互阶段。\n\n![交互阶段可视化](https://arxiv.org/html/2512.16302v1/x3.png)\n> **图3**：三个物理交互阶段的可视化。以真实机器人执行“将杯子放入杯架”任务为例，展示了预接触、抓取和后接触阶段，及其与人类操作认知理解的对应关系。\n\n2.  **交互感知区域预测网络**：在从演示中提取出交互感知子任务基元后，该网络负责预测跨交互阶段的功能性交互不变区域。不变区域被定义为在具有相同最优策略的状态下，对SE(3)变换保持结构等变的3D几何子集，其语义和功能在不同任务场景中保持稳定。网络使用仅来自短视野任务的成功演示进行训练。对于每个任务，一个演示被分割为三个阶段。网络以连续状态对的RGB-D观测生成的点云为输入，使用Point Cloud Transformer V3进行处理，通过跨场景交叉注意力和场景内自注意力增强特征。网络输出一个交互概率分布，以激活相关的不变区域。训练采用监督学习，使用仿真中提供的真实实例掩码进行监督。\n\n3.  **交互感知区域匹配网络**：该模块使机器人能够将演示中预测的不变区域匹配到当前执行状态，以在不进行额外训练的情况下复现动作。网络在多个短视野任务演示上训练，无需显式的任务分解。对于当前执行状态，一个状态路由网络会从演示轨迹中选择最相似的状态。然后将演示状态的不变区域点云与当前执行状态的全场景点云融合，经由PTV3下采样和定位网络（采用交叉-自-交叉注意力模块）进行特征对齐。基于对齐后的特征，通过双重softmax匹配算法计算两个点云之间的对应矩阵。最后，利用基于对应的位姿回归算法，结合演示中的动作位姿，估算出当前状态下机器人末端执行器的目标位姿。确定目标位姿后，使用RRT-Connect算法进行运动规划，执行后获得新的观察，并迭代进行下一轮匹配。\n\n与现有方法相比，ManiLong-Shot的创新点在于：1）将长视野任务分解为以物理交互事件为边界的基元，而非直接处理长轨迹或依赖预定义基元库；2）引入了分阶段的、交互感知的不变区域预测与匹配机制，专门针对抓取操作中的不同交互阶段（预接触/抓取、后接触）进行建模，提升了长视野任务中动作转移的鲁棒性。\n\n## 实验与结果\n实验使用了基于RLBench构建的专用基准**RLBench-Oneshot**，包含10个短视野训练任务和20个未见过的长视野测试任务。长视野任务根据交互次数分为三个难度等级。对比的基线方法包括：模仿学习领域的SOTA多任务模型RVT2、3DDA、ARP，以及OSIL领域的领先方法IMOP。对于长视野任务，部分基线使用了少量演示进行微调（标记为+FT）。评估指标为平均成功率和平均排名。\n\n![长视野任务可视化](https://arxiv.org/html/2512.16302v1/x4.png)\n> **图4**：RLBench-Oneshot中20个长视野操作任务的可视化，包括3个难度等级。等级1（13个任务）包含6次物理交互，等级2（4个任务）包含9次交互，等级3（3个任务）包含12次交互。\n\n**关键实验结果**：\n在**短视野训练任务**上，ManiLong-Shot（采用基于规则的分解）取得了最佳性能，平均成功率达到**90.4%**，比最佳基线3DDA（85.0%）相对提升3.8%，并在10个任务中的6个上取得了最高成功率。\n\n在**未见过的长视野任务**上，ManiLong-Shot展示了卓越的一次性模仿学习能力。如表2所示，其平均成功率达到**30.2%**，显著优于所有基线。相比表现最好的基线IMOP（7.4%），取得了**22.8%** 的相对提升。即使在需要微调的基线（RVT2+FT， 4.1%）面前，优势也非常明显。ManiLong-Shot在20个任务中的19个上排名第一。\n\n消融实验分析了各组件贡献：1）**任务分解策略**：基于规则的分解在大多数任务上表现优于VLM分解，但VLM在需要高级语义理解的任务（如“整理绳子”）上更具优势。2）**模块有效性**：移除任务分解（直接使用原始长轨迹）或不变区域预测与匹配模块，性能均出现显著下降，验证了各模块的必要性。\n\n![真实机器人实验结果](https://arxiv.org/html/2512.16302v1/figs/real.png)\n> **图5**：真实机器人实验设置及三个长视野任务（堆叠积木、堆叠杯子、放置杯子）的成功执行过程可视化。模型经过仿真预训练后，通过一次性模仿成功泛化到物体位置略有扰动的真实场景。\n\n**真实机器人实验**在UFatory xArm7上验证了框架的实用性。针对三个长视野任务，在物体初始位置扰动的情况下进行一次性模仿，平均成功率达到了**86.7%**，证明了其良好的从仿真到现实的迁移能力。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了 **ManiLong-Shot**，一个专门为长视野抓取操作任务设计的新型OSIL框架，通过将任务结构化为序列化的交互基元，实现了有效的端到端一次性模仿执行。2）引入了一种交互感知的任务分解机制，以及一个两阶段的不变区域预测与匹配流程，确保了长视野任务执行的鲁棒性。3）构建了RLBench-Oneshot基准，并系统性地验证了方法在仿真和真实环境中的有效性。\n\n论文自身提到的局限性在于其框架主要关注**抓取操作任务**，基于夹爪与环境的物理交互阶段进行分解，因此不处理纯粹的非抓取行为。\n\n本工作对后续研究的启示包括：1）交互感知的任务分解提供了一种可解释且鲁棒的长任务处理范式，可尝试将其扩展到更广泛的操作类型。2）结合基于规则的稳定性和VLM的语义理解能力，可能催生更灵活、更通用的任务理解与分解方法。3）不变区域的概念及其在长视野任务中的分阶段预测与匹配，为机器人学习可泛化的技能表示提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2512.16302v1/x1.png",
        "https://arxiv.org/html/2512.16302v1/x2.png",
        "https://arxiv.org/html/2512.16302v1/x3.png",
        "https://arxiv.org/html/2512.16302v1/x4.png",
        "https://arxiv.org/html/2512.16302v1/figs/real.png",
        "https://arxiv.org/html/2512.16302v1/x5.png",
        "https://arxiv.org/html/2512.16302v1/x6.png",
        "https://arxiv.org/html/2512.16302v1/x7.png",
        "https://arxiv.org/html/2512.16302v1/x8.png",
        "https://arxiv.org/html/2512.16302v1/x9.png",
        "https://arxiv.org/html/2512.16302v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.16023",
      "title": "CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion",
      "url": "http://arxiv.org/abs/2512.16023",
      "arxivId": "2512.16023",
      "date": "2025-12-17",
      "authors": "Abhinav Valada Team",
      "category": "Manipulation",
      "summary": "本文提出CoVAR方法，解决机器人操作中视频与动作数据难以协同生成的问题。现有方法存在两阶段流程跨模态信息共享不足，或需从头训练联合扩散模型难以利用预训练知识等局限。CoVAR通过扩展预训练视频扩散模型并附加专用动作扩散模型、引入桥接注意力机制实现跨模态交互、设计动作细化模块提升控制精度，实现了视频与动作的协同生成。实验表明，该方法在多个基准测试中能生成更高质量的视频和更准确的动作，性能显著优于现有基线。",
      "detailedSummary": "## 研究背景与动机\n机器人策略学习领域，基于视频生成的方法展现出巨大潜力。现有方法主要分为两大范式：两阶段方法和联合模型。两阶段方法首先使用视频生成模型预测未来的视觉规划，然后附加一个策略模型从生成的视频或其潜在表示中推断动作。这种方法虽能利用强大的预训练视频扩散模型，但其动作生成性能严重依赖于视频生成的质量，且在生成的视频中机器人手臂仅部分可见时表现不佳。联合模型则学习视频和动作的共享潜在空间，通过联合建模加强策略模型。然而，这类方法通常需要从头训练联合扩散模型，或在大量数据上适配视频扩散模型以学习联合分布，在高质量数据有限时往往导致次优性能。\n\n本文旨在解决上述局限性，提出一个既能继承预训练视频扩散模型的通用视觉知识，又能促进视频与动作模态间信息共享的框架。其核心思路是：扩展一个预训练的视频扩散模型，并行附加一个专用的动作扩散模型以保留预训练知识，并引入一种桥接注意力机制来实现有效的跨模态交互，从而协同生成高质量的视频-动作对。\n\n## 方法详解\nCoVAR的整体目标是在给定初始观测图像 \\(v_0\\)、机器人初始关节状态 \\(a_0\\) 和自然语言指令 \\(c\\) 的条件下，训练一个多模态扩散模型以生成对齐指令的视频帧 \\(v\\) 和配对动作 \\(a\\)。方法采用多模态整流流来加速训练并提升生成质量。\n\n![方法框架总览](https://arxiv.org/html/2512.16023v1/x2.png)\n> **图2**：CoVAR方法总览。(A) 模型基于视频扩散主干，并行附加一个动作DiT来生成动作。(B) 两种模态通过桥接注意力进行交互。(C) 对于低分辨率数据集，引入一个动作精炼模块。\n\n**整体框架**：CoVAR建立在预训练视频扩散模型Open-Sora之上。不同于先前使用单一联合DiT的方法，本框架在视频DiT主干旁并行附加了一个专用的动作DiT模块。这种双分支设计旨在有限数据条件下保持视频模型的预训练知识，同时为动作生成提供专门路径。动作数据维度相对较低，因此使用轻量级MLP编码器获取动作嵌入，而非昂贵的VAE。动作DiT通过交叉注意力以文本指令 \\(c\\) 为条件，与视频DiT形成对称结构。两个模态间的信息交换通过提出的桥接注意力机制实现。动作解码器采用UNet架构，以生成更准确的动作。整个模型通过最小化整流流损失（公式2）进行优化。\n\n**核心模块与创新点**：\n1.  **桥接注意力机制**：这是实现有效跨模态交互的核心创新。先前工作通过将视频和动作token拼接后送入同一DiT来实现早期融合，但这需要将预训练知识适配到统一潜在空间，在数据有限时表现不佳。桥接注意力机制为视频特征 \\(f_v\\) 和动作特征 \\(f_a\\) 分别参数化独立的查询（\\(q_1, q_2\\)）、键（\\(k_1, k_2\\)）和值（\\(v_1, v_2\\)）。然后，将这些独立的投影结果拼接起来计算注意力，之后再分割回各自的模态特征（公式3）。这种设计相较于传统自注意力（单一QKV导致模态支配）或双向交叉注意力（破坏内部结构）的优势在于，它能减轻跨模态干扰，在保持各自模态内部一致性的同时实现有效的交互。消融实验证明该机制能同时提升视频生成质量和动作准确性。\n\n2.  **动作精炼模块**：针对许多机器人数据集视频分辨率较低（如128p）导致生成轨迹不精确的问题，本方法引入了一个基于Transformer的动作精炼模型。该模块以初始图像和文本指令为条件，将模型生成的粗略动作转换为高精度动作序列（见图2C）。具体而言，精炼模型分别对输入的粗略动作和初始图像进行嵌入，将得到的动作token与图像token拼接后通过自注意力模块聚合视觉信息，再经过以文本描述 \\(c\\) 为条件的交叉注意力模块融入指令引导，最后解码输出精炼后的机器人动作。\n\n## 实验与结果\n**实验设置**：评估在三个数据集上进行：公开的仿真基准Calvin（200x200分辨率）和Libero90（128x128分辨率），以及自收集的真实世界数据集（涉及拾放螺丝、螺母等精细操作）。对比的基线方法包括联合模型（UVA， UWM， PAD）和两阶段模型（Unipi， RoboEnvision）。评估指标包括视频生成质量（PSNR， SSIM， LPIPS， FVD）和动作生成成功率。\n\n**关键实验结果**：\n- **视频生成质量**：如表I所示，在Calvin和Libero90数据集上，CoVAR在PSNR、SSIM、LPIPS、FVD四个指标上均优于三个联合模型基线。与两阶段方法中使用的纯视频扩散模型OpenSora-1.2相比，指标相当，表明引入动作模态并未降低视频生成质量。定性结果（图4）显示，CoVAR生成的视频中物体和机械臂的伪影更少，视觉更清晰、真实。\n\n![视频生成质量对比](https://arxiv.org/html/2512.16023v1/x4.png)\n> **图4**：与基线生成的视频对比。相较于其他基线，我们的模型生成的物体和机械臂内容伪影更少，结果更清晰、真实。推演结果显示了生成视频与对应动作间的强对齐性。\n\n- **动作生成成功率**：\n    - **Calvin数据集**：如表II所示，在开/关抽屉、橱柜、灯、拾取和推动物体五类任务上，CoVAR的成功率显著高于所有基线方法（例如，抽屉任务达到1.000）。\n    - **Libero90数据集**：如表III所示，在拾放、开/关、组合任务上，CoVAR（配合动作精炼模块）取得了显著提升（例如，拾放任务从0.592提升至0.873）。图5直观展示了动作精炼模块将粗略的动作趋势转化为精确、可成功执行轨迹的作用。\n    - **真实数据集**：如表IV所示，在拾取微小螺母、螺丝、销钉的挑战性任务中，CoVAR的成功率（0.64， 0.74， 0.70）远超两阶段基线（接近0）。图6展示了连续拾放任务的推演结果，生成的视频与真实世界执行高度吻合，体现了强视频-动作对齐性。\n\n![动作精炼效果对比](https://arxiv.org/html/2512.16023v1/x5.png)\n> **图5**：我们的模型与未使用动作精炼的变体之间的推演对比。未使用动作精炼的模型产生仅反映任务大致趋势的粗略动作，但精度不足；动作精炼提升了精度并使任务成功完成。\n\n![真实世界推演结果](https://arxiv.org/html/2512.16023v1/x6.png)\n> **图6**：真实世界实验生成的视频和推演结果。连续成功的拾放操作展示了高精度的动作生成和强的视频-动作对齐性。\n\n**消融实验**：在自收集数据集上的消融研究（表V， 图7）验证了各组件贡献。\n- **桥接注意力**：将其替换为标准自注意力（SA）或交叉注意力（CA）后，视频质量和动作成功率均大幅下降，证明了桥接注意力在减轻模态干扰、促进有效信息交换方面的关键作用。\n- **UNet解码器**：将动作头从UNet替换为ResNet后，动作准确性和视频质量均下降，表明UNet的多尺度特征处理能更好地耦合动作与视频流，生成更精确的动作和更逼真的视频。\n- **仅动作DiT**：移除视频DiT，仅训练单模态动作生成模型，成功率急剧下降至0.08，凸显了视频模态提供的预训练知识和互补信息对于准确动作生成至关重要。\n\n![消融实验定性结果](https://arxiv.org/html/2512.16023v1/x7.png)\n> **图7**：消融研究的定性结果。红线表示真实动作作为参考。蓝线表示我们生成的动作。在“w/o BA”的轨迹图中，蓝线表示自注意力，绿线表示交叉注意力。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了一个并行多模态扩散框架CoVAR，通过扩展预训练视频扩散模型并附加专用动作DiT，实现了视频与机器人动作的协同生成，在有限数据下保持了预训练知识；2) 设计了桥接注意力机制，通过为各模态维护独立的QKV投影，实现了有效的跨模态交互同时避免了干扰；3) 引入了针对低分辨率数据集的动作精炼模块，将粗略动作转化为精确控制序列，提升了任务执行可靠性。\n\n论文自身提到的局限性在于，对于低分辨率数据集，需要额外的动作精炼模块来提升精度。这启示后续研究可以探索更统一的架构来直接处理不同分辨率的数据。\n\nCoVAR的工作为机器人学习提供了一个可扩展的框架，其设计思路——在利用强大预训练模型与实现紧密跨模态耦合之间取得平衡——对数据受限的具身AI研究具有重要启发意义。它表明，通过精心设计的交互机制，可以协同提升多模态生成的性能，而不仅仅是简单的拼接或单向依赖。",
      "imageUrls": [
        "https://arxiv.org/html/2512.16023v1/x1.png",
        "https://arxiv.org/html/2512.16023v1/x2.png",
        "https://arxiv.org/html/2512.16023v1/x3.png",
        "https://arxiv.org/html/2512.16023v1/x4.png",
        "https://arxiv.org/html/2512.16023v1/x5.png",
        "https://arxiv.org/html/2512.16023v1/x6.png",
        "https://arxiv.org/html/2512.16023v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.15692",
      "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
      "url": "http://arxiv.org/abs/2512.15692",
      "arxivId": "2512.15692",
      "date": "2025-12-19",
      "authors": "Elvis Nava Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉-语言-动作模型（VLAs）因预训练数据缺乏动态物理信息而导致数据效率低下的核心问题，提出mimic-video模型。该模型是一种视频-动作模型（VAM），其关键技术是结合预训练的大规模视频模型与基于流匹配的动作解码器，后者作为逆动力学模型，直接从视频潜在表示生成机器人动作。实验表明，该方法在机器人操控任务上达到最优性能，相比传统VLA架构，样本效率提升10倍，收敛速度加快2倍。",
      "detailedSummary": "## 研究背景与动机\n当前机器人操控的主流方法是视觉-语言-动作模型（VLAs），它们通过在大型静态网络数据上预训练的视觉-语言主干网络进行微调。这种方法虽然提升了语义泛化能力，但存在一个关键局限：其预训练数据本质上是静态的，缺乏关于动态和物理过程的显式、时间性信息。因此，模型必须完全从稀缺且昂贵的机器人演示数据中，隐式地推断复杂的物理动力学和时间依赖关系，这造成了不可持续的数据负担。\n\n本文针对VLA模型因缺乏内在物理理解而导致数据效率低下的痛点，提出了一个新的视角：利用视频作为预训练模态。视频数据天然编码了动态和程序性信息，能捕获“事物如何完成”的丰富知识。本文的核心思路是：冻结一个预训练的视频生成主干网络，提取其潜在空间中的中间表征作为“视觉行动方案”，并以此指导一个轻量级的、基于流匹配的动作解码器（作为逆动力学模型）来生成低级机器人动作，从而将长时程规划与下游控制任务解耦。\n\n## 方法详解\nmimic-video的整体框架由两个基于条件流匹配的模型耦合而成：一个预训练的语言条件视频主干网络，以及一个以前者潜在表征为条件的轻量级动作解码器。给定初始观测（历史图像帧、语言指令和本体感知状态），视频主干在紧凑的潜在空间中预测未来轨迹（视觉计划）。动作解码器则作为逆动力学模型，根据视频主干提取的中间表征，生成未来一段时间内的低级机器人动作序列。\n\n![方法框架](https://arxiv.org/html/2512.15692v2/x1.png)\n> **图1**：mimic-video架构。左侧为预训练的视频生成主干（Cosmos-Predict2），它从大规模视频数据中学习物理动力学先验。通过部分去噪策略，视频主干生成至中间流时间τ_v，以提取潜在视觉计划。这些表征作为条件输入到右侧较小的动作解码器，后者处理本体感知状态并预测动作轨迹。视频和动作组件在独立的流时间表（τ_v和τ_a）上运行。\n\n**核心模块1：视频主干**。采用基于流匹配的视频生成模型（实践中为Cosmos-Predict2），它是一个在潜空间操作的20亿参数扩散Transformer。输入是干净的历史帧潜变量与加噪的未来帧潜变量的拼接，通过交替的自注意力、对T5编码语言指令的交叉注意力和MLP进行建模。该主干在训练中保持冻结，仅通过LoRA在机器人视频数据上进行微调以对齐领域。\n\n**核心模块2：动作解码器**。同样是一个扩散Transformer，其输入序列由本体感知状态和未来动作通过两个独立MLP编码后拼接而成。其核心设计是每一层都包含：1) 对视频主干第k层输出的中间表征 **h^(τ_v)** 的交叉注意力；2) 对动作序列的自注意力；3) 一个两层MLP。所有组件的输出均通过AdaLN进行调制，其输入是视频和动作流时间τ_v和τ_a的低秩双线性仿射编码。\n\n**核心创新点**：与需要完全生成未来帧再推断动作的现有视频策略方法不同，mimic-video提出了**部分去噪策略**以实现高效推理。在推理时（算法1），视频主干仅从噪声积分到一个中间的、高噪声的流时间τ_v（通常接近1），即可获得包含足够语义和结构信息的潜状态 **z_future^(τ_v)**。只需前向传播视频主干的前k层，即可提取用于条件动作解码的 **h^(τ_v)**，从而完全避免了耗时的完整视频生成。动作解码器则独立执行从噪声到干净动作的完整去噪过程。\n\n**训练流程**（算法2）是解耦的：第一阶段用LoRA微调视频主干；第二阶段在视频主干冻结的情况下，从头训练动作解码器。关键细节是，在每次训练迭代中，独立采样视频流时间τ_v和动作流时间τ_a，使解码器能适应推理时各种噪声水平的视频条件。这使动作解码器的训练变得轻量且高效。\n\n## 实验与结果\n**实验设置**：在仿真基准SIMPLER（基于WidowX的BridgeDataV2）和LIBERO（基于桌面Panda）上评估，并进行真实的双手灵巧操作实验（使用两个16-DoF mimic手的Panda双臂）。对比的基线是一个“π_0.5-style VLA”，它使用PaliGemma作为视觉-语言主干，搭配与mimic-video完全相同的动作解码器架构，确保性能差异仅源于主干网络提供的条件表征（图像-文本 vs. 视频）。\n\n![仿真结果表](https://arxiv.org/html/2512.15692v2/x3.png)\n> **图3**：在SIMPLER-Bridge基准上的成功率。mimic-video（scratch）仅使用目标领域数据训练，取得了45.8%的平均成功率，显著优于同等条件下训练的π_0.5-style VLA（35.4%），并与使用大规模外部机器人动作数据预训练的模型性能相当。\n\n![训练曲线对比](https://arxiv.org/html/2512.15692v2/x4.png)\n> **图4**：在LIBERO基准上的训练曲线对比。mimic-video（橙色）的收敛速度是π_0.5-style VLA（蓝色）的2倍，并且达到更高最终性能。突显了视频主干带来的更优样本效率。\n\n![真实世界结果](https://arxiv.org/html/2512.15692v2/x5.png)\n> **图5**：真实世界双手灵巧操作任务的成功率。mimic-video在“包裹分拣”和“卷尺收纳”两个任务上，仅使用极少量任务特定动作数据（1.5-2.2小时）进行训练，即实现了高成功率（83-100%）。\n\n![消融实验：视频条件质量](https://arxiv.org/html/2512.15692v2/x6.png)\n> **图6**：视频生成质量对策略性能影响的消融研究。当动作解码器以真实未来视频帧的潜变量（“Ground-Truth Video”）为条件时，成功率接近完美（~100%），这证实了控制问题可简化为视觉预测问题，策略性能直接与视频模型质量相关。\n\n![消融实验：去噪程度τ_v](https://arxiv.org/html/2512.15692v2/x7.png)\n> **图7**：不同视频部分去噪程度τ_v对策略性能的影响。τ_v=1（完全噪声）时性能已很好，且推理最快（无需视频主干积分）。适当降低τ_v（如0.8）可进一步提升性能，但需权衡计算成本。\n\n**关键实验结果总结**：\n1.  **性能**：在SIMPLER上，mimic-video仅用目标领域数据（scratch）取得45.8%平均成功率，优于同条件下的VLA基线（35.4%）。在LIBERO上达到最先进性能。\n2.  **效率**：样本效率相比传统VLA架构提升**10倍**，收敛速度提升**2倍**。\n3.  **泛化**：可有效控制从单臂到双手灵巧等多种机器人形态，并在真实世界中仅用少量数据完成长时程接触密集型任务。\n4.  **消融实验洞察**：图6的“预言机”实验表明，策略性能上限取决于视频预测质量；图7表明无需精细视频重建（τ_v=1）即可获得良好控制，平衡了性能与速度。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**Video-Action Model**这一新范式，通过利用预训练视频模型的潜在表征，将物理动力学理解与低级控制解耦。\n2.  设计了**mimic-video模型**，采用冻结视频主干与基于流匹配的动作解码器架构，并创新性地引入**部分去噪推理策略**，实现了高效、实时的控制。\n3.  通过大量实验验证了该方法的优越性，在样本效率、收敛速度和最终性能上显著超越基于视觉-语言主干的VLA方法。\n\n**局限性**：\n1.  策略性能高度依赖于预训练视频主干的质量和其与目标领域的对齐程度。\n2.  部分去噪程度τ_v是一个需要调整的超参数，其最优值可能因任务而异。\n3.  方法依赖于生成式视频模型，其训练和微调本身需要大量视频数据。\n\n**对后续研究的启示**：\n1.  证明了从大规模视频数据中学习的**动态先验**对于数据高效的机器人学习至关重要，为未来预训练模型的选择提供了新方向。\n2.  “视觉计划+逆动力学”的解耦架构是一种有效的设计模式，可引导研究者探索其他形式的中间表征或更高效的动作解码器。\n3.  随着视频生成模型的不断进步（规模更大、质量更高、效率更优），直接为机器人控制所利用的潜力将进一步释放。",
      "imageUrls": [
        "https://arxiv.org/html/2512.15692v2/x1.png",
        "https://arxiv.org/html/2512.15692v2/x2.png",
        "https://arxiv.org/html/2512.15692v2/x3.png",
        "https://arxiv.org/html/2512.15692v2/x4.png",
        "https://arxiv.org/html/2512.15692v2/x5.png",
        "https://arxiv.org/html/2512.15692v2/x6.png",
        "https://arxiv.org/html/2512.15692v2/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.16724",
      "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.16724",
      "arxivId": "2512.16724",
      "date": "2025-12-18",
      "authors": "Liang Wang Team",
      "category": "Manipulation",
      "summary": "本文提出VERM方法，旨在解决3D机器人操作中多固定摄像头带来的信息冗余与计算负担问题。该方法核心是利用基础模型，从3D点云中生成一个虚拟的、任务自适应的视角（虚拟眼），以高效聚焦关键信息并缓解遮挡。技术要点包括深度感知模块和动态由粗到精的处理流程。实验表明，该方法在RLBench仿真和真实场景中均超越先前最佳方法，实现了训练速度1.89倍、推理速度1.54倍的提升。",
      "detailedSummary": "## 研究背景与动机\n当前，基于视觉的3D机器人操作通常依赖多个固定摄像机的感知输入进行动作规划。这种多相机设置引入了大量的冗余和无关信息，增加了计算成本，并迫使模型花费额外的训练时间来提取关键的任务相关信息。现有方法主要分为两类：一类将多视角RGB-D图像投影到统一的3D体素或点云表示中，但庞大的3D表示仍包含冗余背景，且训练耗时（例如PerAct在8块V100 GPU上需约16天）；另一类（如RVT系列）则将点云投影到由人类专家经验预定义的虚拟相机平面上，但这类预定义视角缺乏对新任务的适应性。人类则能够利用其广泛的知识，仅凭一瞥和想象就为各种任务确定自适应的操作视角。本文针对多相机输入冗余、计算成本高以及预定义视角缺乏灵活性的痛点，提出利用基础模型的知识为机器人“想象”一个虚拟的、任务自适应的视角。核心思路是：利用GPT-4o等大模型根据多视角观测和语言指令预测一个最优虚拟相机位姿，仅使用从该视角渲染的单张图像来指导策略网络，从而大幅减少输入冗余，加速训练和推理。\n\n## 方法详解\nVERM方法的整体流程分为两步：首先，通过一个基于提示的范式，利用GPT-4o预测完成当前任务所需的自适应虚拟相机位姿；其次，基于该位姿从构建的3D点云渲染出单张虚拟图像，并输入到一个集成了深度感知模块和动态粗到细（Coarse-to-Fine, C2F）机制的策略网络中，最终预测出机器人的动作。\n\n![提示范式](https://arxiv.org/html/2512.16724v1/x1.png)\n> **图1**：使用GPT-4o查询虚拟相机位姿的提示范式。左侧为结构化的文本提示，包含环境描述、任务描述、上下文示例和规则四部分。右上角为两个视觉提示：左图使用SoM技术标注了工作空间及固定相机位置与坐标轴，右图展示了来自这些固定相机的原始RGB图像。GPT-4o处理这些提示后输出虚拟相机位姿参数（`elev`和`azim`）。右下角展示了由查询到的位姿生成的虚拟图像示例。\n\n**核心模块一：虚拟相机位姿选择**\n该模块旨在将GPT-4o转化为一个空间推理智能体。如图1所示，研究者设计了一个结构化的提示框架，将文本提示分为四个部分：1) **环境描述**：概述场景中固定相机位姿、原点位置及坐标轴方向，并辅以SoM标注的视觉提示以帮助模型理解空间关系。2) **任务描述**：定义用`elev`（与XZ平面夹角）和`azim`（在参考平面上的投影与参考向量的夹角）两个角度参数来表征指向原点的虚拟相机方向。3) **上下文示例**：提供来自RVT-2设置的三个相机示例，帮助模型理解空间关系。4) **规则**：设立四条规则来精炼输出，包括偏好轴对齐以方便深度信息捕获、强调必须分析视觉信息而非相机标签、确保相机从上方向下看桌子以及约束输出格式。结合文本提示和五张视觉提示（一张环境图，四张原始RGB-D输入图），GPT-4o输出虚拟相机位姿，进而从统一3D点云渲染出全局虚拟图像`o_global`。\n\n**核心模块二：策略网络**\n策略网络架构如图2所示，它接收虚拟图像`o_global`、语言指令和本体感知（机器人状态和时间）信息，输出一个8维动作向量（3维位置、3维旋转、1维夹爪状态、1维碰撞参数）。\n\n![策略网络](https://arxiv.org/html/2512.16724v1/x2.png)\n> **图2**：VERM的策略网络架构。原始多视角RGB-D输入被转化为统一3D点云，并根据GPT-4o预测的位姿投影生成单张全局虚拟图像。该图像与语言指令、本体感知信息一起，通过Transformer预测一个粗粒度动作（包括2D热图和深度估计）。在任务关键阶段，通过动态C2F模块触发视角缩放，聚焦于局部感兴趣区域以细化动作。\n\n*   **动态粗到细模块**：为了解决高精度操作需求，VERM引入了动态C2F机制。与以往在每个步骤都进行细化的方法不同，VERM仅在任务关键阶段（如抓取、对齐）触发细化。具体而言，在训练时，通过比较粗粒度预测和细粒度预测在平移或旋转上的差异是否超过阈值（0.01米或5°）来标记需要细化的样本，并训练一个轻量级分类器。在推理时，该分类器决定是否激活“细化阶段”：即根据粗粒度预测的位置，对点云进行中心化和缩放操作（不改变相机方向），生成一个局部放大的视图，并在此视图上重新预测更精细的动作。\n*   **深度感知模块**：由于单张2D图像缺乏深度信息，VERM在Transformer中引入了**可学习的深度token**。网络在虚拟图像的2D空间上预测一个动作热图，同时通过这些深度token估计出一个对应的深度值，从而共同确定3D空间中的末端执行器位置。3维旋转被离散化为每个轴5度的区间进行分类预测。\n\n**训练细节**：从专家轨迹中根据末端执行器状态变化和近零速度点提取关键点。损失函数如公式(1)所示，是平移、旋转、夹爪开合、碰撞、深度以及动态C2F指示器分类损失的加权和。\n\n**创新点**：与现有方法相比，VERM的主要创新在于：1) **视角选择自动化**：利用基础模型替代人工经验，自动生成任务自适应的最优虚拟视角。2) **高效的单图像输入**：仅使用单张虚拟图像作为策略网络的主要视觉输入，极大减少了计算负担。3) **深度感知与动态细化**：通过深度token将2D预测扩展至3D，并通过动态触发的C2F机制在精度和效率间取得平衡。\n\n## 实验与结果\n**实验设置**：在仿真基准**RLBench**和真实世界环境中进行评估。RLBench实验遵循PerAct设置，使用Franka Panda机器人，任务包含堆叠积木、开抽屉、插入销钉等17种，每种任务有100条专家演示。视觉输入来自四个128×128分辨率的固定RGB-D相机。对比了**C2F-ARM**、**PerAct**、**HiveFormer**、**PolarNet**、**RVT**、**Act3D**、**3D Diffuser Actor (3DDA)** 和 **RVT-2** 共八种先进方法。\n\n**关键实验结果**：\n1.  **性能与速度**：如表I所示，VERM在RLBench的17个任务上取得了**83.6%**的平均成功率，超过了RVT-2的82.2%，并在11个任务上表现最佳。同时，如图4所示，VERM实现了**1.89倍**的训练加速和**1.54倍**的推理加速（与RVT-2相比）。\n\n![训练与推理速度](https://arxiv.org/html/2512.16724v1/x4.png)\n> **图4**：左图（对数尺度）展示了各方法的训练时间（天），右图展示了推理速度（帧每秒）。VERM在保持高性能的同时，显著提升了训练和推理效率。\n\n2.  **定性分析**：图3可视化了VERM生成的虚拟视角和动作预测。例如，在“开抽屉”和“放钱”任务中，所选视角完整展示了在原始视角中被部分遮挡的物体（如整个把手）；在“分类形状”任务中，轻微的旋转暴露了被遮挡的孔洞。\n\n![动作预测可视化](https://arxiv.org/html/2512.16724v1/x3.png)\n> **图3**：VERM在RLBench任务中动作预测的可视化。左侧两列展示原始固定相机视图，第三列为GPT-4o生成的虚拟视图，第四列为在该虚拟视图上预测的动作热图（红色点）。虚拟视图有效整合了多视角信息并避免了遮挡。\n\n3.  **消融实验**：如表II所示（对应论文中的表II，其可视化形式应为图7），消融研究验证了各组件的重要性。使用GPT-4o生成的全局视图（模型#1）性能最好。仅使用RVT-2预定义的单个相机视图（模型#2-#4）性能显著下降，其中顶部相机最佳但仍有限制。移除动态C2F过程（模型#5）或轴对齐约束（模型#8）均会导致性能下降。低图像分辨率（模型#6）或缺失缩放操作（模型#7）也会损害性能。\n\n![消融研究结果](https://arxiv.org/html/2512.16724v1/x7.png)\n> **图7**：消融研究结果柱状图（对应论文表II）。展示了不同设计选择（如相机位姿来源、是否使用C2F、图像尺寸、是否缩放、是否轴对齐）对平均成功率的影响，验证了完整VERM设计（#1）的有效性。\n\n4.  **跨模型通用性**：如表III所示，VERM框架可兼容不同的基础模型。使用Qwen2.5和Claude 3.5 Sonnet（辅以迭代细化策略）分别取得了80.3%和81.2%的成功率，虽略低于GPT-4o的83.6%，但证明了该方法的可插拔性。\n\n5.  **失败案例分析**：图5展示了典型失败案例，例如在“放入橱柜”任务中，物体和目标容器从不同角度可见，单一初始视角可能无法覆盖所有后续关键信息。论文通过引入动态重查询策略（在任务中途更新视角）将该任务成功率从55.2%提升至66.4%。\n\n![失败案例](https://arxiv.org/html/2512.16724v1/x5.png)\n> **图5**：VERM的典型失败案例可视化。例如在“放入橱柜”任务中，初始选择的虚拟视图（最右列）可能无法同时清晰捕捉物体和目标容器的关键细节，导致后续操作困难。\n\n6.  **真实世界评估**：在真实机器人平台上（使用2个RealSense D435i相机）评估了8个任务。如图6和论文所述，VERM在大多数任务上取得了与仿真相当的成功率，并优于RVT和RVT-2，证明了其从仿真到现实的迁移能力。\n\n![真实世界预测](https://arxiv.org/html/2512.16724v1/x6.png)\n> **图6**：VERM在真实世界任务中的动作预测可视化。格式与图3类似，展示了原始相机视图、GPT-4o生成的虚拟视图及预测的动作热图。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种利用大模型（如GPT-4o）作为空间推理智能体，为3D机器人操作自动选择任务自适应虚拟相机视角的新范式，显著减少了输入信息的冗余。\n2.  设计了深度感知模块和动态粗到细调整机制，使基于单张2D图像的策略能够进行精确的3D空间规划和细粒度操作。\n3.  在RLBench基准和真实实验中验证了方法的有效性，在取得领先成功率的同时，大幅提升了训练和推理速度，并展示了良好的跨基础模型通用性。\n\n**局限性**：\n1.  对于某些复杂任务（如“放入橱柜”），单一静态视角可能无法捕捉所有阶段所需的关键信息。\n2.  依赖的基础模型可能存在“幻觉”，生成无效的相机位姿（如桌子下方），需要额外的提示约束和自验证循环来缓解。\n\n**启示**：\n1.  大型多模态基础模型所蕴含的空间推理知识可以被有效地引导和用于解决机器人学中的具体感知问题，而不仅仅是高层任务规划。\n2.  “预测-获取-引导”的流程为构建高效、专注的机器人感知系统提供了新思路。动态调整机制（如动态C2F、动态重查询）为实现精度与效率的平衡提供了借鉴。",
      "imageUrls": [
        "https://arxiv.org/html/2512.16724v1/x1.png",
        "https://arxiv.org/html/2512.16724v1/x2.png",
        "https://arxiv.org/html/2512.16724v1/x3.png",
        "https://arxiv.org/html/2512.16724v1/x4.png",
        "https://arxiv.org/html/2512.16724v1/x5.png",
        "https://arxiv.org/html/2512.16724v1/x6.png",
        "https://arxiv.org/html/2512.16724v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.15840",
      "title": "Large Video Planner Enables Generalizable Robot Control",
      "url": "http://arxiv.org/abs/2512.15840",
      "arxivId": "2512.15840",
      "date": "2025-12-17",
      "authors": "Yilun Du Team",
      "category": "Manipulation",
      "summary": "本论文针对通用机器人决策模型泛化能力有限的问题，提出基于大规模视频预训练的替代范式，以克服视觉-语言-动作模型因动作数据稀缺导致的泛化不足。方法包括策划互联网规模人类活动视频数据集，首次训练基础模型规模的生成式视频规划模型，通过零样本生成视频计划并后处理提取机器人动作。实验通过第三方选定野外任务和真实机器人验证，实现了成功的物理执行，展示了鲁棒的指令跟随、强泛化及现实世界可行性。",
      "detailedSummary": "## 研究背景与动机\n当前构建通用机器人基础模型的主流范式是扩展多模态大语言模型，使其输出动作，即构建视觉-语言-动作模型。这类方法依赖于将大规模语言和图像预训练知识迁移到动作模态。然而，与海量的网络文本和图像数据相比，机器人动作数据极为稀缺，这导致VLA模型通常需要在有限的机器人数据上进行微调，形成了一种不对称的知识迁移。这种模式在面对未见过的场景和任务时，泛化能力往往不足。\n\n本文针对VLA模型因数据稀缺导致的泛化瓶颈，提出了一个替代性范式：将大规模视频预训练作为构建机器人基础模型的主要模态。与静态图像和文本不同，视频天然地捕捉了物理世界中状态和动作的时空序列，这与机器人行为具有内在的一致性。网络视频数据丰富，涵盖了人类活动和任务演示，为学习提供了更直接的、基于真实世界动态的迁移基础。\n\n本文的核心思路是：训练一个大规模视频生成模型，使其能够根据单张初始观测图像和文本指令，生成描绘任务完成过程的视频计划，然后将此视觉计划后处理为可执行的机器人动作序列，从而实现零样本的机器人控制。\n\n## 方法详解\n本文方法采用两阶段设计：首先是一个大型视频规划器，用于生成视频计划；随后是一个动作提取模块，将视频转换为机器人动作。\n\n![方法框架](https://arxiv.org/html/2512.15840v1/x2.png)\n> **图2**：LVP方法整体框架。(a) 基于潜在视频扩散的框架：使用时序因果3D VAE将视频编码为3D潜在表示，然后在此潜在空间中训练扩散变换器。(b) 改进的扩散强迫训练策略：在训练时，随机选择0到6帧作为历史上下文，对历史段和未来段施加独立的噪声水平，并以50%概率将历史段噪声设为零，从而统一支持图像到视频和视频到视频的训练。\n\n**1. 视频基础模型**\n模型基于潜在扩散框架构建。首先，使用时序因果3D VAE将像素空间的视频剪辑压缩为低维潜在表示。该VAE将每个8×8×4的时空块编码为一个16通道的嵌入。编码后，冻结VAE，在压缩的潜在空间中训练一个改进的扩散强迫变换器模型。\n\n**核心创新一：扩散强迫训练**\n为了灵活地以初始帧（i2v）或多个历史帧（v2v）为条件生成视频，并增强时序一致性，本文采用了扩散强迫训练策略。传统视频扩散模型对所有令牌添加均匀噪声，而扩散强迫则在训练时对不同帧应用随机、独立的噪声水平。具体而言，给定固定数量的潜在帧，首先从{0,1,2,…,6}中随机采样一个历史长度，将视频分割为历史段和未来段。然后对每个段应用独立的噪声水平。例如，若训练时将历史帧的噪声水平设为零，模型会将其视为完美的上下文帧并学习以其为条件；若设为中间噪声水平，模型则学习处理分布外上下文帧的鲁棒性。在采样时，通过将上下文帧的噪声水平设置为零，即可实现对其的灵活条件生成。此方法无需为可变长度上下文令牌引入额外的交叉注意力层，且与现有DiT模型权重兼容。\n\n**核心创新二：历史引导采样**\n为了进一步提高生成视频与上下文图像的 adherence 和运动保真度，LVP采用了历史引导技术，这是分类器无关引导的一个变体。在采样时，结合使用历史引导和基于文本的CFG。最终的采样得分由四项组成：以历史和文本为条件的得分、仅以文本为条件的得分（用于历史引导）、以历史和文本为条件的得分、以及仅以历史为条件的得分（用于文本引导）。通过调节引导权重，可以生成在物理上合理且紧密遵循指令的视频计划。\n\n**训练细节**\n训练分为两个阶段：1）**持续预训练**：从Wan I2V 14B权重开始，丢弃处理额外掩码和图像引导通道的权重，在全数据集上训练。2）**低相机运动微调**：为了减少不必要的相机运动，从Ego4D、Epic-Kitchens和Panda数据集中筛选出平均光流幅度较低的剪辑子集进行微调，从而提升时间平滑性和视觉稳定性。\n\n**2. LVP-1M 数据集**\n为训练面向具身决策的视频基础模型，本文构建了LVP-1M数据集，包含140万个短剪辑，展示了人类或机器人与物体的交互，每个剪辑配有多条以动作为中心的描述。\n\n![数据集与采样](https://arxiv.org/html/2512.15840v1/x3.png)\n> **图3**：(a) 八个数据源的可视化示例。第一行：四个机器人数据集。第二行：四个以人为中心的数据集。(b) 历史引导采样策略示意图，结合了使用历史和不使用历史的估计得分。\n\n数据来源多样，包括：1）网络爬取视频，提供规模和多样性；2）以自我为中心的人类活动数据集，提供丰富的人-物交互；3）机器人遥操作数据集，提供机器人形态知识。关键的数据处理步骤包括：**时间对齐**：将所有剪辑对齐到人类执行速度（约3秒），以避免时序不一致。**质量过滤**：过滤低分辨率、光照差、相机运动过快、 embodiment（手或夹具）不可见以及机器人失败轨迹的视频。**以动作为中心的重新标注**：使用大语言模型为视频生成高质量、专注于动作描述的标题，以增强模型的指令遵循能力。\n\n**3. 动作提取**\n生成视频计划后，通过3D场景重建（使用MegaSam）、手部重建（使用HaMeR）和运动重定向（使用Dex-Retargeting）来提取可执行的机器人动作，从而适应不同的机器人形态。\n\n## 实验与结果\n评估分为两部分：1）对视频模型本身进行任务级泛化的独立评估；2）在真实机器人上进行端到端执行的实验。\n\n**评估设置**\n- **数据集**：使用自建的LVP-1M数据集进行训练和评估。\n- **对比基线**：包括文本到视频模型、图像到视频模型以及其他机器人视频生成方法，如Gen2Act、Dreamitate、Cosmos等。\n- **评估指标**：在独立评估中，由第三方测试者从生成视频的物理合理性、指令遵循度等方面进行评分。在真实机器人实验中，报告任务执行成功率。\n\n**关键实验结果**\n![定性对比](https://arxiv.org/html/2512.15840v1/x5.png)\n> **图5**：与基线模型WAN 2.1的定性对比。LVP生成的结果在物体交互（如抓握杯子）、运动物理合理性（如倒水）和手部姿态细节方面显著优于基线。\n\n![消融实验](https://arxiv.org/html/2512.15840v1/x12.png)\n> **图12**：消融研究。(a) 不同训练策略（标准训练 vs. 扩散强迫训练）下的生成结果对比，扩散强迫训练能产生更合理的动作。(b) 历史引导权重对生成质量的影响，适当的权重能提升动作质量，过高则可能产生伪影。\n\n![真实机器人结果](https://arxiv.org/html/2512.15840v1/x13.png)\n> **图13**：真实机器人执行结果。LVP生成的视频计划被成功重定向到不同的机器人形态（平行夹具、灵巧手）上执行，完成了开门、倒水等任务。\n\n**结果总结**\n- **独立评估**：在由第三方选择的多样化、挑战性任务中，LVP在物理合理性和指令遵循方面显著优于所有基线模型。\n- **真实机器人实验**：在15个零样本的真实世界任务中，使用LVP提取的动作，机器人取得了68%的整体成功率，优于对比的基线方法（如Gen2Act为47%）。\n- **消融实验**：验证了扩散强迫训练和历史引导技术对提升视频计划质量的关键作用。时间对齐的数据处理策略对于跨形态（从人到机器人）的有效知识迁移至关重要。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了**大型视频规划器**（LVP），这是一个为机器人操作设计的大规模视频基础模型，以及一个将其部署为零样本策略的框架。\n2. 构建并开源了**LVP-1M数据集**，一个经过精心策划、专注于具身决策和指令遵循的网络规模视频数据集。\n3. 设计了严谨的**任务级泛化评估协议**，包括独立的第三方测试和真实机器人实验，系统性地评估了模型在未见过的环境、任务和形态上的泛化能力。\n\n**局限性**：\n论文提到，尽管LVP在已见物体上表现出色，但在处理涉及**全新物体**（即训练数据中未出现过的物体类别）的任务时，泛化能力仍然是一个挑战。这指向了当前方法在组合泛化方面的限制。\n\n**后续启示**：\n本文的工作展示了以视频为中心作为机器人基础模型主干的可行性和优势。它为后续研究指明了方向：1）进一步探索如何利用更庞大、更多样的视频数据来提升模型的组合泛化和常识推理能力。2）改进视频到动作的提取管道，使其更加高效、鲁棒，并能够处理更复杂的长期任务规划。3）将视频生成模型与更低层的控制策略更紧密地结合，形成端到端的学习系统。",
      "imageUrls": [
        "https://arxiv.org/html/2512.15840v1/x1.png",
        "https://arxiv.org/html/2512.15840v1/x2.png",
        "https://arxiv.org/html/2512.15840v1/x3.png",
        "https://arxiv.org/html/2512.15840v1/x4.png",
        "https://arxiv.org/html/2512.15840v1/x5.png",
        "https://arxiv.org/html/2512.15840v1/x6.png",
        "https://arxiv.org/html/2512.15840v1/fig/figure_video_multistage.png",
        "https://arxiv.org/html/2512.15840v1/x7.png",
        "https://arxiv.org/html/2512.15840v1/x8.png",
        "https://arxiv.org/html/2512.15840v1/x9.png",
        "https://arxiv.org/html/2512.15840v1/x10.png",
        "https://arxiv.org/html/2512.15840v1/figText/Comparison.png",
        "https://arxiv.org/html/2512.15840v1/x11.png",
        "https://arxiv.org/html/2512.15840v1/x12.png",
        "https://arxiv.org/html/2512.15840v1/x13.png",
        "https://arxiv.org/html/2512.15840v1/x14.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.16449",
      "title": "Single-View Shape Completion for Robotic Grasping in Clutter",
      "url": "http://arxiv.org/abs/2512.16449",
      "arxivId": "2512.16449",
      "date": "2025-12-18",
      "authors": "Todor Stoyanov Team",
      "category": "Manipulation",
      "summary": "本文针对机器人抓取在杂乱环境中因单视图遮挡导致几何形状不完整、抓取性能下降的问题，提出基于扩散模型的类别级3D形状补全方法。该方法从单视图深度观测中重建完整物体几何，集成场景分割与抓取推理，为规划提供完整上下文。在家庭物品杂乱场景的初步实验中，抓取成功率比无形状补全基线提高23%，比现有最优形状补全方法提高19%。",
      "detailedSummary": "## 研究背景与动机\n在基于视觉的机器人操作中，单目相机视图只能捕获物体的一侧，而杂乱场景中的遮挡进一步限制了可见性，导致观察到的几何信息不完整，抓取估计算法表现不佳。现有解决单视图抓取挑战的方法，如S4G直接回归抓取或GraspNet-1Billion处理遮挡，在面对最优抓取向量与遮挡表面碰撞时仍有困难。近期一些工作（如3DSGrasp、SCARP、SceneGrasp）虽在抓取预测前进行形状补全，但大多在无遮挡或模拟的简化场景中评估，或缺乏可靠的抓取验证。ZeroGrasp在真实世界进行了评估，但其在存在深度传感器噪声时性能会下降。这些方法的一个关键局限是，它们通常假设部分点云已在规范坐标系中对齐，这在物体姿态任意且存在遮挡的真实世界操作中不切实际。本文针对在真实杂乱场景中，由于几何信息不完整导致抓取不可靠这一具体痛点，提出利用扩散模型从单视图深度观测中完成类别级3D形状补全，为抓取规划提供完整几何上下文的新视角。本文的核心思路是：构建一个模块化系统，先分割目标物体，再利用扩散模型从其未对齐的部分点云补全完整形状，最后在补全形状上进行抓取推断，从而提升杂乱环境中的抓取成功率。\n\n## 方法详解\n本文采用模块化方法，将杂乱环境中的抓取问题分解为场景获取、物体分割、形状补全和抓取推断四个阶段。这种设计允许灵活更换不同分割或抓取组件，而无需重新训练整个系统。\n\n![方法框架](https://arxiv.org/html/2512.16449v1/x2.png)\n\n> **图2**：所提方法概览。RGB信息用于分割感兴趣物体。物体点云随后输入扩散模型以获得补全表面，进而指导抓取规划。抓取被排序并选择执行（图中绿色抓取）。\n\n**整体流程**：给定机器人、工作空间中的一组家庭物品和静态安装的RGB-D相机，通过语言提示指定并分割目标物体。输入RGB-D数据的RGB分量经过语言引导的分割生成物体掩码，该掩码应用于对应的深度图像以提取目标物体的可见点云。这个部分点云随后由形状补全模块处理，估计完整的3D几何形状。补全的物体形状作为抓取推断网络的输入，预测候选抓取位姿。最终，使用标准运动规划器执行选定的抓取。\n\n**核心模块1：物体分割**：该模块从场景点云中分离出单个物体的点云，作为形状补全模型的输入。为了获得精确的掩码，本文采用LangSAM，通过短文本提示（如“红色碗”、“木块”）引导实例分割，有效避免了过分割或合并相邻物体的问题。\n\n**核心模块2：单视图物体重建**：使用有符号距离场表示3D物体，并采用Diffusion-SDF模型从单视图深度感知获得的部分点云估计完整物体形状。模型架构包含三个核心组件：用于学习可泛化有符号距离场的GenSDF、将物体形状压缩为紧凑潜在表示的变分自编码器，以及直接预测去噪潜在向量的扩散网络。\n\n**创新点：类别级形状补全**：为了解决不同类别物体共享相似局部几何特征时产生的基本模糊性问题（例如，曲面块可能属于瓶子、杯子或碗），本文实施了类别级补全。具体策略是训练一个集成模型，为每个物体类别（见表1）保存一个单独的检查点。这为形状推断提供了必要的上下文约束，并使通过向集成中添加新模型来扩展到新类别变得简单。\n\n**核心模块3：抓取位姿估计**：给定点云输入，抓取位姿估计预测能引导机械臂末端执行器成功抓取物体的候选抓取。本文利用模块化设计，集成了基于点云的最新扩散模型GraspGen来预测补全物体形状上的抓取。所有预测抓取都关联一个预测得分。根据其接近向量是否落在与垂直方向成40°的锥形范围内，将预测抓取分为两类。两类抓取分别按其预测得分排序。优先选择锥形内的前K=5个抓取，因为这种垂直接近方式能最小化与相邻物体和基面的碰撞风险。如果锥形内抓取不足K个，则从锥形外补充至K个。随后循环尝试这K个抓取，传递给运动规划器，直到规划成功为止。这种多尝试策略提高了处理因手臂可达性和工作空间物体布局等实际约束导致的运动规划失败的鲁棒性。\n\n## 实验与结果\n**实验设置**：重建质量评估使用ReOcS真实世界数据集，该数据集包含基于杂乱和遮挡程度分为“简单”、“正常”、“困难”三个难度级别的家庭物品。使用双向倒角距离作为评估指标。全系统抓取实验在配备Robotiq 2F-85夹爪的Franka Emika Panda机器人上进行，使用ROS2和MoveIt2进行运动规划。评估了两种不同的实验设置（图4），以确保所有物体类别都有适当的遮挡水平。每个物体类别进行10次试验，记录成功抓取的百分比（抓取后抬起并保持超过5秒判定为成功）。\n\n**对比方法**：1) **GraspGen（无形状补全）**：直接在部分点云上进行抓取推断的基线。2) **ZeroGrasp**：同时进行形状补全和抓取预测的最新方法。\n\n![实验场景配置](https://arxiv.org/html/2512.16449v1/images/real_robot_exp/setup1.jpg)\n\n> **图4a**：真实机器人实验使用的场景配置之一。\n\n![实验场景配置](https://arxiv.org/html/2512.16449v1/images/real_robot_exp/setup2.jpg)\n\n> **图4b**：真实机器人实验使用的另一种场景配置，旨在使所有物体类别都有适当的遮挡水平。\n\n**重建质量结果**：如表2所示，在ReOcS数据集上，虽然ZeroGrasp在成功重建的样本上取得了更低的倒角距离，但其发布的检查点在大约30-35%的样本上失败（原因未知），而本文模型重建了所有实例。本文方法的整体重建成功率为100%，显著高于ZeroGrasp的62.34%-69.85%。\n\n![定性重建结果](https://arxiv.org/html/2512.16449v1/images/ReOcS/easy/shard_0/scene_1/view_6/rgb.jpg)\n\n> **图3（部分）**：ReOcS数据集“简单”难度级别的RGB图像示例。\n\n![定性重建结果](https://arxiv.org/html/2512.16449v1/images/ReOcS/easy/shard_0/scene_1/view_6/recon_top.jpg)\n\n> **图3（部分）**：本文方法（Diffusion-SDF）对“简单”级别场景的补全结果（顶视图）。\n\n![定性重建结果](https://arxiv.org/html/2512.16449v1/images/ReOcS/easy/shard_0/scene_1/view_6/recon_underside.jpg)\n\n> **图3（部分）**：本文方法（Diffusion-SDF）对“简单”级别场景的补全结果（底视图），展示了补全的不可见面。\n\n**抓取成功率结果**：如表3所示，本文方法（带形状补全）的平均抓取成功率达到80%，比不使用形状补全的基线（56.67%）高出约23%，也比ZeroGrasp（61.67%）高出约19%。在苹果、碗、盒子、锤子等类别上，本文方法均取得了最高或并列最高的成功率。\n\n![重建质量对比](https://arxiv.org/html/2512.16449v1/images/compare_recon/wooden_block/partial.jpg)\n\n> **图5（部分）**：真实实验中的输入部分点云示例（木块）。\n\n![重建质量对比](https://arxiv.org/html/2512.16449v1/images/compare_recon/wooden_block/zerograsp.png)\n\n> **图5（部分）**：ZeroGrasp对木块的重建结果，显示几何形状不合理。\n\n![重建质量对比](https://arxiv.org/html/2512.16449v1/images/compare_recon/wooden_block/diffusionsdf.png)\n\n> **图5（部分）**：本文方法对木块的重建结果，产生了更合理的几何形状。图中对比直观展示了本文方法在重建质量上的优势。\n\n**推理时间**：在NVIDIA RTX 2000 Ada Laptop GPU上，完整流程（图2）约需4-5秒：物体分割0.8秒，形状补全3秒，对齐0.2秒，抓取估计0.4-0.6秒。ZeroGrasp的推理时间为2-3秒，更快但重建质量较差。\n\n## 总结与启发\n本文的核心贡献包括：1) 提出了一种面向杂乱场景抓取的系统级集成方案，结合了开放词汇物体分割、基于扩散的任意方向部分点云形状补全和模块化抓取生成。2) 通过真实机器人实验证明了将形状补全作为预处理步骤，能有效提高在杂乱环境中对多样家庭物品的抓取成功率。3) 首次将基于扩散的形状补全集成到机器人操作中，并设计了训练方案以提高对遮挡的鲁棒性。\n\n论文提到的局限性在于，其类别级方法需要为每个物体类型训练单独的模型。尽管这使扩展到新类别变得直接，但需要额外的训练和数据。\n\n本文的启示在于：在机器人抓取中，利用生成模型（如扩散模型）补全被遮挡的几何信息是提升在真实、杂乱环境中操作可靠性的有效途径。模块化设计提供了灵活性，但如何平衡补全质量与推理速度，以及如何向更通用的、与语言对齐的补全模型发展，是未来值得探索的方向。",
      "imageUrls": [
        "https://arxiv.org/html/2512.16449v1/x1.png",
        "https://arxiv.org/html/2512.16449v1/x2.png",
        "https://arxiv.org/html/2512.16449v1/images/ReOcS/easy/shard_0/scene_1/view_6/rgb.jpg",
        "https://arxiv.org/html/2512.16449v1/images/ReOcS/easy/shard_0/scene_1/view_6/recon_top.jpg",
        "https://arxiv.org/html/2512.16449v1/images/ReOcS/easy/shard_0/scene_1/view_6/recon_underside.jpg",
        "https://arxiv.org/html/2512.16449v1/images/ReOcS/normal/shard_0/scene_0/view_39/rgb.jpg",
        "https://arxiv.org/html/2512.16449v1/images/ReOcS/normal/shard_0/scene_0/view_39/recon_top.jpg",
        "https://arxiv.org/html/2512.16449v1/images/ReOcS/normal/shard_0/scene_0/view_39/recon_underside.jpg",
        "https://arxiv.org/html/2512.16449v1/images/ReOcS/hard/shard_0/scene_0/view_45/rgb.jpg",
        "https://arxiv.org/html/2512.16449v1/images/ReOcS/hard/shard_0/scene_0/view_45/recon_top.jpg",
        "https://arxiv.org/html/2512.16449v1/images/ReOcS/hard/shard_0/scene_0/view_45/recon_underside.jpg",
        "https://arxiv.org/html/2512.16449v1/images/real_robot_exp/setup1.jpg",
        "https://arxiv.org/html/2512.16449v1/images/real_robot_exp/setup2.jpg",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/wooden_block/partial.jpg",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/wooden_block/zerograsp.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/wooden_block/diffusionsdf.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/pringles_can/partial.jpg",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/pringles_can/zerograsp.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/pringles_can/diffusionsdf.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/apple/partial.jpg",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/apple/zerograsp.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/apple/diffusionsdf.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/navy_blue_bottle/partial.jpg",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/navy_blue_bottle/zerograsp.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/navy_blue_bottle/diffusionsdf.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/bowl/partial.jpg",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/bowl/zerograsp.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/bowl/diffusionsdf.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/hammer/partial.jpg",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/hammer/zerograsp.png",
        "https://arxiv.org/html/2512.16449v1/images/compare_recon/hammer/diffusionsdf.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.15020",
      "title": "ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision",
      "url": "http://arxiv.org/abs/2512.15020",
      "arxivId": "2512.15020",
      "date": "2025-12-17",
      "authors": "Jie Mei Team",
      "category": "Manipulation",
      "summary": "本文针对视觉模仿学习过度依赖物体外观、忽视3D场景结构导致的训练效率低、泛化差问题，提出ISS Policy。该方法是一种基于DiT的3D视觉运动扩散策略，以点云为输入预测连续动作序列。其核心是提出了隐式场景监督模块，通过鼓励模型输出与场景几何演化一致，提升策略性能与鲁棒性。实验表明，该方法在MetaWorld单臂操作和Adroit灵巧手操作任务上达到SOTA性能，并在真实世界展现出强泛化与鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n视觉模仿学习已成为获取机器人操作技能的重要范式，但当前主流方法严重依赖物体外观（2D图像），而忽略了底层的3D场景结构，这导致了训练效率低下和泛化能力差的问题。具体而言，基于2D图像的策略（如IBC、BC-RNN）存在空间模糊性，在处理遮挡或需要精确几何推理的接触任务时性能受限。为了缓解2D的局限，近期研究转向了3D表征（如基于体素或连续特征场的方法），但这些方法通常采用复杂的网络架构，导致推理缓慢、计算成本高昂。更重要的是，这些3D策略仅依赖策略损失来监督整个场景编码器和动作头，这种监督信号过于微弱和间接，导致学习效率低下。本文针对现有3D策略训练效率低、监督信号弱的具体痛点，提出了一种新视角：为扩散策略引入一个额外的、自监督的隐式场景监督信号，以增强模型对场景几何动态的理解。本文的核心思路是：构建一个基于DiT的3D视觉运动策略，从点云观测中预测连续动作序列，并引入一个隐式场景监督模块，通过预测未来点云特征来强制输出与场景几何演化一致，从而提升策略的性能和鲁棒性。\n\n## 方法详解\nISS Policy的整体框架是一个基于DiT的条件扩散模型，它以单视角深度图转换而来的稀疏点云和机器人本体状态作为输入，输出未来连续动作序列。其核心创新在于增加了一个仅在训练时使用的隐式场景监督（ISS）模块，为动作预测提供几何动态一致性约束。\n\n![方法框架](https://arxiv.org/html/2512.15020v1/x2.png)\n> **图2**：ISS Policy模型架构。专家演示的点云和状态被编码为观测上下文。训练时，扩散噪声调度器向专家动作轨迹添加噪声，基于DiT的策略头以上下文和扩散时间步t为条件进行去噪，预测未来动作序列。策略生成候选轨迹后，ISS模块利用全局点云上下文和一段预测动作，预测一个跳跃K步的未来点云嵌入，提供辅助的未来预测信号以塑造训练过程中的表征学习。\n\n**整体流程与问题定义**：模型学习一个端到端的视觉运动策略 π_θ: O^To → A^T。输入是长度为To的观测窗口，每个观测Ot包含单视角机器人中心点云Pt和本体状态st。输出是长度为T的预测动作序列，但在执行时仅应用前Ta个动作（Ta ≤ T），以滚动时域方式控制。\n\n![策略公式](https://arxiv.org/html/2512.15020v1/x3.png)\n> **图3**：策略公式化。展示了预测总时长T、观测窗口长度To和执行动作步数Ta之间的关系。\n\n**核心模块一：DiT架构（动作生成）**。该部分采用编码器-解码器设计。\n1.  **条件编码器**：点云编码器采用MLP加最大池化操作，将点云Pt编码为紧凑的全局特征z_pc。机器人状态st通过另一个MLP直接编码为z_state。\n2.  **动作解码器**：基于DiT架构。首先将带噪声的动作序列A_t^(τ)嵌入为高维向量H。为了增强去噪过程的空间感知，将H与点云特征z_pc融合，生成动作查询Q = MLP(concat(H, z_pc))。随后，Q通过多个DiT块进行处理，每个块包含多头自注意力（MHSA）和前馈网络（FFN），并使用AdaLN-zero层注入条件信息C = z_pc + z_state + Emb(τ)，其中Emb(τ)是扩散时间步τ的正弦位置编码。最终，一个预测头将处理后的特征映射回干净的动作预测。\n\n**核心模块二：隐式场景监督（ISS）模块**。这是本文的核心创新。其核心思想是：如果模型预测的动作是正确的，那么这些动作应该能够准确预测对应的未来场景。因此，ISS模块利用动作解码器预测出的动作序列 Â_{t,K} 来预测未来K步后的点云场景特征，而不是预测下一帧（因为相邻点云变化微小，监督信号弱）。预测K步未来鼓励模型捕获更有意义的长期动态和物体运动。\n1.  **技术细节**：给定当前时刻编码的点云特征z_pc,t和预测动作Â_{t,K}，ISS模块（由几个前馈网络和自适应层归一化组成）预测未来点云嵌入 ẑ_pc,t+K。具体过程如公式(12)-(14)：先对当前点云特征进行变换，然后通过FFN(AdaLN(·, Ã_{t,K}))注入动作信息，最后输出预测特征。\n2.  **损失函数与训练策略**：ISS损失是预测特征与真实未来点云特征之间的简单L2回归损失：L_iss = E[ ||ẑ_pc,t+K - z_pc,t+K||_2^2 ]。此监督无需额外标注，输入（历史点云和动作）和目标（未来点云）均直接来自演示数据。为避免训练早期因动作预测不准导致的误差累积和不稳定，论文采用了**计划采样**策略：以概率p使用真实动作A_{t,K}，以概率1-p使用预测动作Â_{t,K}作为ISS模块的输入（公式16）。\n\n**总体训练目标**：最终的损失是行为克隆的扩散去噪损失与ISS损失的加权和：L(θ, ψ) = L_bc(θ) + λ_iss * L_iss(θ, ψ)。其中θ是DiT策略参数，ψ是ISS头参数，λ_iss是控制ISS监督强度的超参数。**测试时，ISS头被丢弃，仅使用DiT策略生成动作，因此不增加任何推理开销。**\n\n**与现有方法的创新点**：\n1.  **高效的3D表征**：采用稀疏点云编码，相比基于体素或神经场的方法更轻量。\n2.  **隐式场景监督**：引入了额外的、自监督的几何动态一致性约束（ISS损失），为策略学习提供了更丰富的信号，这是与DP3等仅用策略损失的方法的关键区别。\n3.  **训练策略优化**：结合计划采样来稳定ISS模块的训练，缓解暴露偏差。\n\n## 实验与结果\n**实验设置**：\n- **Benchmark/数据集**：在模拟环境Adroit（灵巧手操作）和MetaWorld（单臂操作）上进行评估。\n- **Baseline方法**：对比了2D方法（BCRNN, IBC, Diffusion Policy）和3D方法（DP3, Mamba Policy）。\n- **评估指标**：报告SR_5（训练过程中最高五个成功率的平均值）的均值和标准差。\n- **实验平台**：单张NVIDIA RTX 5880 GPU。\n\n![模拟环境点云观测](https://arxiv.org/html/2512.15020v1/x4.png)\n> **图4**：模拟环境中的3D点云观测可视化。展示了来自Adroit和MetaWorld的代表性点云。\n\n**关键实验结果**：\n如表I所示，ISS Policy在9个任务上取得了最高的平均成功率（86.2%），显著优于所有基线。特别是在具有挑战性的Adroit任务（如Pen）和复杂的MetaWorld任务（如Pick-Place-Wall）上表现突出。传统行为克隆方法（BCRNN, IBC）表现很差，扩散类方法普遍更优，而ISS Policy在3D扩散基线（DP3, Mamba Policy）基础上实现了进一步提升。\n\n![学习效率与稳定性](https://arxiv.org/html/2512.15020v1/x5.png)\n> **图5**：ISS Policy与DP3在MetaWorld任务上的训练曲线对比。ISS Policy收敛更快，学习效率更高，并且达到了更高的渐近性能。\n\n**可扩展性评估**：\n1.  **模型容量扩展**：如表II和图6所示，增大DiT策略头的规模（从小型到大型）能持续提升SR_5，表明该架构能有效利用增加的模型容量。\n2.  **数据规模扩展**：如表III所示，即使在仅有10条演示的低数据区域，ISS Policy也能取得不错性能，并随着演示数据量的增加而稳步提升。\n\n![可扩展性研究（模型大小）](https://arxiv.org/html/2512.15020v1/x6.png)\n> **图6**：ISS Policy在不同DiT模型大小下的可扩展性研究。展示了在四个任务上，使用三种不同规模DiT策略头（带ISS和不带ISS）的SR_5性能。模型越大，性能通常越好，且ISS模块能带来增益。\n\n**消融实验**：\n- **模块贡献**：表IV的消融研究表明，条件融合、ISS模块和计划采样三者是互补的。条件融合提升了感知-动作耦合；ISS模块塑造了面向未来的上下文表征；而计划采样使得ISS监督在模型自身动作分布下更有效，从而提升了策略 rollout 时的鲁棒性。三者结合效果最佳。\n- **ISS权重影响**：表V显示，ISS损失权重λ_iss存在一个最优值（0.4），此时平均成功率最高（89.75%）。权重过低或过高都会导致性能下降，说明需要适度的平衡。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种高效、可扩展的基于DiT的3D视觉运动策略（ISS Policy），用于从点云观测中生成机器人操作动作序列。\n2.  引入了新颖的隐式场景监督（ISS）损失，通过自监督的未来点云特征预测，为策略学习提供了额外的几何动态一致性约束，显著提高了学习效率和泛化能力。\n3.  在模拟（Adroit, MetaWorld）和真实世界实验中，ISS Policy均取得了最先进的性能，并表现出优异的可扩展性（对模型大小和数据量）以及训练稳定性。\n\n**局限性**：\n论文自身提到，方法依赖于单视角深度图转换的点云，这可能限制了其对被遮挡物体或场景完整几何的理解。此外，ISS模块的设计（如预测跳跃步数K的选择）可能不是最优的，未来有改进空间。\n\n**对后续研究的启示**：\n1.  **多模态与多视角输入**：可以探索融合多视角点云或RGB信息，以提供更全面的场景理解。\n2.  **ISS模块的改进**：未来工作可以研究更先进的场景预测模型（如神经辐射场）或更复杂的动态建模方式来替代简单的特征回归。\n3.  **应用于更复杂任务**：该方法展现出的高效性和鲁棒性使其有望应用于更长期、多步骤的移动操作任务。",
      "imageUrls": [
        "https://arxiv.org/html/2512.15020v1/x1.png",
        "https://arxiv.org/html/2512.15020v1/x2.png",
        "https://arxiv.org/html/2512.15020v1/x3.png",
        "https://arxiv.org/html/2512.15020v1/x4.png",
        "https://arxiv.org/html/2512.15020v1/x5.png",
        "https://arxiv.org/html/2512.15020v1/x6.png",
        "https://arxiv.org/html/2512.15020v1/x7.png",
        "https://arxiv.org/html/2512.15020v1/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.14666",
      "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2512.14666",
      "arxivId": "2512.14666",
      "date": "2025-12-16",
      "authors": "Mike Zheng Shou Team",
      "category": "Manipulation",
      "summary": "本文针对Vision-Language-Action (VLA) 模型依赖监督微调、缺乏测试时环境适应性的核心问题，提出EVOLVE-VLA测试时训练框架，使模型能通过交互持续学习。关键技术采用学习进度估计器提供密集反馈，并通过累积进度估计机制平滑噪声、渐进视野扩展策略逐步演化策略。实验表明，该框架在长视野任务上性能提升8.6%，1-shot学习提升22.0%，并在未见任务上实现20.8%成功率（纯SFT为0%），涌现出错误恢复和新策略等能力。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型主要通过监督微调在固定的专家演示数据集上进行训练。这种静态模仿学习范式存在两个根本性局限：一是高昂的人力成本，每个新任务都需要收集数百条演示数据进行微调，难以扩展至通用机器人；二是脆弱的记忆化，模型仅仅模仿演示，缺乏从执行偏差中恢复的能力，一旦偏离训练分布就容易失败。这些局限与人类通过实践和试错来学习与适应的方式严重不符。\n\n本文针对VLA模型在部署时无法适应新环境或新任务的具体痛点，提出了“测试时训练”的新视角。核心思路是：在仅需少量甚至零任务特定演示进行轻量级初始化后，模型在目标部署环境中通过主动交互持续学习，利用从环境中自主获取的反馈信号来优化策略，从而降低对演示数据的依赖并实现真正的自适应。\n\n## 方法详解\nEVOLVE-VLA框架旨在使经过监督微调预训练的VLA模型在部署阶段能够继续学习。其整体流程分为两个阶段：首先使用少量演示对VLA策略进行监督微调初始化；随后进入测试时训练阶段，策略与环境交互生成多样化的轨迹，一个任务进度估计模块为每条轨迹分配奖励值，该奖励随后用于通过GRPO优化策略。\n\n![框架总览](https://arxiv.org/html/2512.14666v1/x2.png)\n> **图2**：EVOLVE-VLA框架总览。在测试时训练阶段，VLA模型与环境交互生成多样化的轨迹滚动。任务进度估计模块为每条轨迹分配一个进度值，该值将作为GRPO优化的奖励。进度估计采用累积策略以产生干净、稳定且平滑的奖励。训练策略采用渐进视野扩展计划，实现课程学习。\n\n核心模块包括在线强化学习交互、任务进度估计以及驯服噪声奖励信号的两个关键技术。\n1.  **在线强化学习交互**：对于给定任务，通过从策略的动作令牌分布中以温度T>1进行采样，生成多条多样化轨迹。每条轨迹会获得一个奖励信号用于评估其质量。随后使用分组相对策略优化（GRPO）来更新策略，GRPO在批次内对轨迹奖励进行归一化以计算优势，并应用PPO风格的裁剪以实现稳定更新，无需单独的价值网络。\n2.  **任务进度估计（奖励函数）**：为解决测试时无法获得真实奖励信号（如模拟器中的成功标签）的关键挑战，框架采用一个学习到的进度估计器来提供密集反馈。具体使用基础评论家模型VLAC，它接收两张图像和任务指令，输出一个评论家值，指示第二张图像相比第一张图像在任务完成上的进展程度。轨迹的初始奖励计算为初始观察与最终观察之间的评论家值。\n3.  **累积进度估计机制**：为解决进度估计的噪声问题，该方法引入了“慢-快”哲学。它定期采样里程碑帧（间隔Δ_milestone），并在更细的粒度上（每Δ_check步）将当前帧与最近的里程碑帧进行比较，计算增量进度。这些增量值通过一个考虑收益递减的递归公式进行累积，得到最终的进度值。此机制通过减少长期漂移的影响和平滑局部波动，将噪声点估计聚合为稳定可靠的信号。\n4.  **渐进视野扩展策略**：针对长视野任务中早期成功轨迹稀少、信用分配困难的问题，该策略将训练过程分为多个阶段。每个阶段设定一个最大滚动视野H_max，并随着训练推进逐渐增加该视野。这使得策略能够先掌握较短的子目标（此时奖励信号更清晰），再学习组合这些行为以完成完整任务，形成了课程学习。\n\n与现有方法相比，创新点具体体现在：1）**范式创新**：首次系统性地提出并实现了VLA模型的测试时训练，无需真实奖励信号；2）**信号驯服**：通过累积进度估计和渐进视野扩展两项核心技术，有效地“驯服”了 inherently noisy 的自主反馈信号，使学习变得可行。\n\n## 实验与结果\n实验在LIBERO仿真基准上进行，该基准包含Spatial, Object, Goal, Long四个任务套件，每个套件10个任务。使用OpenVLA-OFT作为基础模型，VLAC作为进度估计器。对比的基线方法包括Octo, OpenVLA, Nora, π₀, UniVLA, VLA-RL, SimpleVLA等先进的VLA模型。\n\n![主要结果](https://arxiv.org/html/2512.14666v1/x3.png)\n> **图3/表1**：LIBERO基准上的主要结果。EVOLVE-VLA在OpenVLA-OFT基线上应用测试时训练后，在所有任务套件上均取得显著提升，平均成功率从89.2%提升至95.8%（+6.5%），尤其在长视野任务上提升+8.6%。\n\n关键实验结果如下：\n- **性能显著提升**：在LIBERO基准上，应用TTT后平均成功率提升+6.5%（从89.2%至95.8%）。在最具挑战性的LIBERO-Long套件上提升最大（+8.6%），成功率从85.8%提升至94.4%。\n- **少样本学习能力**：在仅提供1条演示的情况下，EVOLVE-VLA通过测试时训练将成功率从30.8%提升至52.8%（+22.0%），显著优于仅进行SFT的基线。\n- **零样本跨任务泛化**：在完全未见过、且无任务特定演示训练的任务上，纯SFT模型成功率为0%。而经过测试时训练后，模型通过自主探索和适应，成功率达到了20.8%，首次展示了VLA模型通过测试时训练实现零样本跨任务泛化的能力。\n\n![消融与定性结果](https://arxiv.org/html/2512.14666v1/x4.png)\n> **图4**：左：消融研究表明累积进度估计（APE）和渐进视野扩展（PHE）都是有效提升性能的关键组件。右：定性结果展示了模型通过测试时训练演化出的新能力，例如从错误中恢复（上）和发现新的问题解决策略（下）。\n\n消融实验总结：\n- **累积进度估计**和**渐进视野扩展**两个组件均对性能有重要贡献。移除任一组件的变体性能均下降，其中移除APE对长视野任务影响尤为显著。\n- 定性分析揭示，模型通过测试时训练演化出了演示数据中不存在的**新兴能力**，包括从执行错误中恢复，以及发现新颖、高效的任务完成策略。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了**EVOLVE-VLA框架**，首次使VLA模型能够在测试时通过环境交互持续自适应，突破了静态SFT的脆性与可扩展性限制；2）通过引入**学习到的进度估计器**作为奖励，并创新性地设计了**累积进度估计**和**渐进视野扩展**两项技术来驯服其固有噪声，解决了测试时缺乏真实奖励信号的核心挑战；3）实验验证了该框架在**性能提升、少样本学习、特别是零样本跨任务泛化**方面的显著优势，展示了自主探索能产生如错误恢复等新兴能力。\n\n论文自身提到的局限性在于其性能依赖于进度估计器（VLAC）的质量，估计器的偏差或噪声会影响学习效率。这指明了未来改进的方向。\n\n这项工作对后续研究的启示是深远的：它标志着VLA模型从**静态模仿**向**持续自改进**的范式转变。未来的研究可以沿着以下方向深入：设计更鲁棒、通用的自主反馈信号；探索更高效的测试时优化算法；以及将这一范式应用于更复杂的真实世界机器人任务中，最终迈向真正通用的具身智能体。",
      "imageUrls": [
        "https://arxiv.org/html/2512.14666v1/x1.png",
        "https://arxiv.org/html/2512.14666v1/x2.png",
        "https://arxiv.org/html/2512.14666v1/x3.png",
        "https://arxiv.org/html/2512.14666v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.14217",
      "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos",
      "url": "http://arxiv.org/abs/2512.14217",
      "arxivId": "2512.14217",
      "date": "2025-12-16",
      "authors": "Gitta Kutyniok Team",
      "category": "Manipulation",
      "summary": "本文针对机器人演示视频生成中可控性不足的问题，提出Draw2Act框架。其核心方法是利用深度感知的轨迹条件视频生成，从输入轨迹提取深度、语义、形状和运动等多维正交表示，并注入扩散模型；同时联合生成空间对齐的RGB与深度视频，通过跨模态注意力机制和深度监督增强时空一致性。实验表明，该方法在Bridge V2等基准测试中，相比现有基线取得了更高的视觉保真度、一致性以及机器人操作成功率。",
      "detailedSummary": "## 研究背景与动机\n视频扩散模型已成为具身智能领域强大的世界模拟器和数据增强源。然而，面向通用视频合成的模型通常依赖自然语言提示，这种条件在描述精细的机器人运动或操作轨迹时缺乏精确性。为了提升可控性，近期工作开始探索以轨迹为条件的视频生成，例如Tora、DragAnything和FreeTraj等方法。但这些方法大多依赖于2D轨迹或单一模态条件，而机器人手臂的运动本质上是三维的，这使得纯2D引导不足以产生可控且一致的机器人演示。像LeviTor等方法虽然向整合3D信息迈进了一步，但在应用于机器人演示时，往往缺乏显式的动作控制，导致手臂与物体间的交互不一致。因此，在生成视频中实现精确的3D轨迹控制，同时保持物体和交互的一致性，是当前视频生成模型面临的开放挑战。\n\n本文针对上述痛点，提出了一种新的视角：从输入轨迹中提取多个互补的正交表征（深度、语义、形状和运动），并将它们注入扩散模型，以增强其轨迹跟随能力。核心思路是：1）融合深度编码的3D轨迹、高级语义DINOv2物体特征和像素坐标增强的文本提示；2）联合生成空间对齐的RGB和深度视频，利用深度监督和跨模态注意力提升时空一致性；3）基于生成的多模态视频训练策略模型以回归机器人关节控制命令。\n\n## 方法详解\nDraw2Act是一个条件多模态潜在扩散模型（LDM），旨在合成给定初始帧、物体掩码、文本提示和物体3D轨迹序列的机器人操作视频。其整体目标是将轨迹的多种互补表征注入扩散变换器（DiT），以精细控制生成过程。\n\n![方法框架](https://arxiv.org/html/2512.14217v1/x1.png)\n> **图0**：Draw2Act模型架构概览。从输入轨迹中提取多种表征（DINOv2物体级特征、像素坐标增强的文本提示以及叠加了深度感知轨迹的参考图像）。DINOv2特征通过专门的融合层注入每个DiT块。为了实现多模态输出生成，将参考帧、RGB帧和深度帧在时间维度上拼接，并用相同的补丁嵌入模块处理。\n\n**整体流程与输入输出**：模型输入包括：包含任务相关物体的初始帧 I、指定被操作物体的二值掩码 M、用户定义的文本提示 c，以及物体轨迹序列 q = {(x_i, y_i, d_i)}，其中 (x, y) 是相对于图像原点的像素坐标，d 是每帧的相对深度值。输出是 N 帧的逼真操作视频 V（RGB）及其对应的深度视频 V_depth。\n\n**核心模块与技术细节**：\n1.  **轨迹表征编码**：模型从输入轨迹中提取三种互补表征 D = {z0_ref, y_dino, y_c}。\n    *   **参考图像潜在编码 (z0_ref)**：在第一帧 I0 上绘制颜色编码的深度感知轨迹（起点、终点和轨迹线），编码成潜在特征 z0_ref ∈ R^{16×1×h×w}，并与视频潜在特征 z 在时间维度拼接，为去噪网络提供深度和运动信号。\n    *   **DINOv2物体特征序列 (y_dino)**：从初始帧中裁剪出目标物体区域，提取其DINOv2特征。该特征被传播到轨迹 q 定义的每个对应位置，形成特征图，再经过时间插值压缩以匹配潜在空间维度，最终得到 y_dino ∈ R^{1024×n×h×w}。这一表征编码了物体的语义、形状和运动信息。\n    *   **坐标增强文本提示 (y_c)**：文本提示被详细增强，包含机器人、物体和目标点的像素坐标（例如，“位于蓝点(x_robot, y_robot)的机械臂移动到红点(x_obj, y_obj)处的物体，将其拾起，然后移动到绿点(x_des, y_des)”）。该提示通过T5编码为特征向量 y_c。\n\n2.  **控制注入模块**：专门设计用于将DINOv2特征 y_dino 注入DiT主干。首先对 y_dino 进行时空下采样以匹配补丁嵌入后的潜在特征维度。然后通过一个融合块，利用门控机制有选择地调制DINOv2特征，再通过LayerNorm稳定输出，最后以残差方式与Transformer的隐藏状态 h 融合。公式为：h' = h + LayerNorm(y_dino ⊙ G) ⊙ (y_dino ⊙ G)，其中 G = σ(W_g y_dino + b_g) 是Sigmoid门控掩码。\n\n3.  **多模态生成**：模型同时生成RGB和深度视频。深度视频由Video Depth Anything估计得到。在训练时，RGB和深度视频使用同一个预训练的3D因果VAE进行编码，然后在时间维度（而非通道维度）上拼接，形成一个长的输入序列。这使得模型能够通过自注意力机制跨模态捕获互补信息，公式化为：D_θ((z ∥ z_depth); σ, {z0_ref, y_dino, y_c})。这种设计利用深度监督来施加约束，从而提升生成视频的质量。\n\n4.  **多模态策略模型**：如图1右侧所示，一个独立的策略模型利用生成的RGB和深度视频来回归机器人关节状态。RGB和深度潜在特征分别经过空间和时间Transformer处理，然后通过一个交叉注意力块交换信息，最后相加并通过一个基于ResNet的解码器来预测夹爪状态和关节角度。\n\n**创新点**：与现有依赖单一轨迹表征（如2D坐标或分割掩码）的方法相比，Draw2Act的核心创新在于**多表征融合**与**RGB-D联合生成**。通过深度感知3D轨迹、物体级语义特征和坐标增强文本的协同作用，实现了对运动、外观和几何的更精细、更全面的控制。同时，联合生成深度视频并通过跨模态注意力进行约束，显著增强了生成结果的时空一致性。\n\n## 实验与结果\n**实验设置**：\n*   **数据集**：使用了三个机器人手臂数据集：BridgeData V2 (WidowX)、Berkeley Autolab (UR5) 以及MuJoCo模拟器中的Franka Panda数据集，共约50.6K个视频。\n*   **Baseline方法**：对比了四种先进的轨迹控制视频生成方法：LeviTor、Tora、MotionCtrl和DragAnything。\n*   **评估指标**：\n    1.  **视频质量**：使用VBench-2.0评估运动平滑度（Mot.Smth.）、背景一致性（Bg.Cons.）、主体一致性（Subj.Cons.）和时间闪烁（Tem.Fli.）。\n    2.  **轨迹精度**：计算输入轨迹与生成视频中提取轨迹之间的平均L1距离（Object Traj. Error）。\n    3.  **任务成功率**：在模拟器数据集上，通过策略模型执行生成视频对应的动作，评估任务完成成功率。\n    4.  **深度视频质量**：使用LPIPS、SSIM、PSNR和FVD比较生成的深度视频与从生成RGB视频推断出的深度图。\n\n**关键实验结果**：\n如表I所示，Draw2Act在所有数据集和评估指标上均优于基线方法。在轨迹精度方面，在Bridge V2数据集上，轨迹误差从最佳基线Tora的35.67显著降低至25.30。在模拟器数据集上，Draw2Act取得了最高的任务成功率65.2%，远高于其他方法（Tora: 36.8%, DragAnything: 31.2%）。\n\n![定性对比](https://arxiv.org/html/2512.14217v1/x2.png)\n> **图2**：与基线方法的定性比较。基线方法常出现物体消失（如TORA）、几何畸变（如MotionCtrl中的瓶子）或轨迹跟随错误（如第一行任务中所有基线均未正确移动寿司）。Draw2Act能可靠地跟随给定轨迹，同时保持物体几何形状的一致性。\n\n**消融实验分析**：\n表II和表III展示了逐步添加不同控制信号的消融研究结果。\n\n![Bridge消融](https://arxiv.org/html/2512.14217v1/x3.png)\n> **图3**：在Bridge数据集上不同控制方法的定性消融研究。仅使用第一帧或点帧条件会导致控制粗糙（如错误操作黄布或物体未移动）。2D和深度轨迹提升了精度，但仍可能出现物体外观不一致（如罐子）。结合深度轨迹和DINOv2特征的方法在轨迹跟随和物体一致性上表现最佳。\n\n![模拟器消融](https://arxiv.org/html/2512.14217v1/x4.png)\n> **图4**：在模拟器数据集上的定性消融研究。使用第一帧条件时，碗未被移动到指定位置；使用点帧条件时，机器臂和物体都发生严重变形。而完整方法（3D轨迹+DINOv2）能正确完成任务并保持形状。\n\n从“仅第一帧RGB”到“第一帧多模态（RGB+Depth）”、“点图像”、“2D轨迹图像”、“3D（深度）轨迹图像”，最后到“3D轨迹+DINOv2”，视频质量指标和轨迹精度逐步提升。例如，在Bridge V2数据集上，轨迹误差从仅用第一帧的39.88逐步降至完整方法的25.30。同时，联合生成深度视频（多模态）相比仅生成RGB视频，在所有设置下都能改善视频质量。任务成功率（表III）也呈现相同趋势，从仅用RGB第一帧的15.4%提升至完整方法的65.2%，证明了每个组件对于下游策略学习的重要性。\n\n## 总结与启发\n**核心贡献**：\n1.  **多表征轨迹条件视频生成**：提出了一个集成深度感知3D轨迹、高级语义DINOv2特征和坐标增强文本提示的视频扩散模型框架，实现了对机器人操作视频更精细、更全面的控制。\n2.  **物体中心特征控制与注入机制**：设计了从第一帧提取物体DINOv2特征并沿轨迹传播的方法，以及专门的门控融合模块，有效编码并注入了物体的语义、形状和运动信息，提升了生成视频中物体的几何与运动一致性。\n3.  **RGB-D视频联合生成与多模态策略**：通过时间拼接和共享编码器实现RGB与深度视频的协同生成，利用深度监督和跨模态注意力提升了时空一致性；并提出了基于生成的多模态视频的策略模型，显著提高了下游机器人操作任务的成功率。\n\n**局限性**：论文自身指出，当前模型不支持长时程可控性，且仅支持对场景中的单个物体进行轨迹控制。\n\n**后续启示**：本文的工作为可控机器人视频生成指明了一个方向：即通过融合多维度、互补的控制信号来逼近真实物理世界的复杂性。未来的研究可以在此基础上，探索如何实现对多个物体并行或顺序的轨迹控制，以支持更复杂的多物体操作任务，以及如何扩展模型以生成更长、更复杂的操作序列。",
      "imageUrls": [
        "https://arxiv.org/html/2512.14217v1/x1.png",
        "https://arxiv.org/html/2512.14217v1/x2.png",
        "https://arxiv.org/html/2512.14217v1/x3.png",
        "https://arxiv.org/html/2512.14217v1/x4.png",
        "https://arxiv.org/html/2512.14217v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.13670",
      "title": "NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks",
      "url": "http://arxiv.org/abs/2512.13670",
      "arxivId": "2512.13670",
      "date": "2025-12-15",
      "authors": "Mingyu Cai Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作任务中，如何将自然语言指令精确转换为兼具几何空间关系与时序结构的逻辑规范这一核心问题，提出了NL2SpaTiaL方法。关键技术包括：1）一个生成SpaTiaL逻辑规范与自然语言描述对齐数据集的框架；2）一个配备语义检查器的翻译-验证框架，确保生成的逻辑公式忠实于输入语义。实验表明，基于SpaTiaL的表示能为指令跟随提供更可解释、可验证且可组合的基础。",
      "detailedSummary": "## 研究背景与动机\n机器人操作任务中，将自然语言指令转化为可执行、可验证的规范是一个核心挑战。现有主流方法通常将自然语言翻译为标准时序逻辑（TL，如LTL、STL），这些逻辑主要建模机器人轨迹的时间顺序，但忽略了任务成功所依赖的物体级几何空间交互关系（如接近、触碰、方位）。此外，现有方法生成的逻辑公式往往是扁平的，将原子命题与布尔、时序运算符混合在单一层次，这导致公式难以解释、难以扩展，且无法反映人类指令固有的层次化结构（如将高层目标分解为子目标和细粒度时空约束）。\n\n本文针对上述痛点，提出了将自然语言转化为几何时空逻辑（SpaTiaL）规范的新视角。SpaTiaL逻辑扩展了时序逻辑，能够对区域级约束、物体级空间关系及其随时间演化进行符号推理，非常适合需要几何一致性和顺序的精细操作任务。本文核心思路是：提出一个翻译-验证框架，通过构建分层逻辑树来显式结构化SpaTiaL公式，并利用基于语言的语义检查器确保生成的公式在每一层都与输入指令的语义片段对齐，从而生成可解释、可验证且组合性强的规范。\n\n## 方法详解\n本文提出的NL2SpaTiaL框架是一个分层的翻译-验证流程，旨在将自然语言指令转换为结构化的SpaTiaL规范。整体框架分为两个核心部分：1）**分层逻辑树构建**：将指令分解为不同抽象层次的子公式树；2）**语义一致性检查**：在树构建的每一步，验证生成的子公式与其对应的语言片段是否语义对齐。\n\n![方法框架](https://arxiv.org/html/2512.13670v1/x2.png)\n\n> **图2**：NL2SpaTiaL分层翻译流程架构。该流程以自上而下的方式将自然语言输入分解为多个抽象层（高层目标、中间子目标、几何原子谓词），最终组合成完整的SpaTiaL公式。时序窗口在高层标注，仅在最终阶段组装。\n\n**核心模块1：分层逻辑树**\n为了获得结构化且组合的SpaTiaL规范，本文引入了**分层逻辑树**作为中间表示。HLT是一个元组 𝒯 = (𝒱, ℰ_ref, ℛ_lat, ℒ, 𝒮)。其中，𝒱是节点集合；ℰ_ref是形成根树的细化边，表示父节点子公式被子节点子公式细化；ℛ_lat是同一抽象层节点间的横向关系（如并列、时序顺序）；ℒ将每个节点映射到一个SpaTiaL子公式；𝒮将每个节点与指令中的一个或多个文本片段（span）相关联。树的根节点对应完整SpaTiaL公式，中间节点对应子目标或结构化约束，叶节点则包含原子SpaTiaL谓词和局部时序运算符。这种设计显式暴露了规范的层次结构，并建立了逻辑与语言之间的细粒度对齐。\n\n**核心模块2：语义一致性检查器**\n为确保逻辑公式忠实反映原文本语义，框架为HLT中的每个节点配备了语义一致性检查器。该检查器以节点的文本片段𝒮(v)和对应的SpaTiaL子公式ℒ(v)为输入，判断两者是否语义兼容。具体地，它测试ℒ(v)中的逻辑运算符、时序顺序和引用实体是否得到𝒮(v)措辞的支持，以及片段中的关键信息是否未被公式遗漏。在实现上，检查器实例化为一个大型语言模型，作为软的自然语言蕴含预言机，仅基于文本判断公式是否捕捉了片段含义，而不考虑几何可行性或全局可满足性。通过节点级验证，最终生成的SpaTiaL规范在每一层都与指令保持对齐。\n\n**转换流程**\n给定指令x，系统以自上而下的方式构建HLT：\n1.  **高层结构识别**：识别任务的主要阶段或合取目标，提出候选的根节点和顶层节点。\n2.  **逐层细化与验证**：对于每个候选节点，使用检查器评估其子公式与对应语言片段的语义一致性。仅接受通过的节点加入树中。\n3.  **上下文生成**：将已验证的节点及其关联片段作为上下文，引导模型生成下一层（更细粒度）的细化，如子目标、详细时序模式，直至原子SpaTiaL谓词。\n4.  **横向关系建立**：根据指令中的连接词（如“和”、“之前”），在生成时建立节点间的横向关系ℛ_lat，并同样经过检查器验证。\n5.  **公式组合**：HLT构建完成后，根据树结构（细化边和横向关系）组合所有节点的子公式，得到最终的扁平SpaTiaL规范ϕ。\n\n与现有方法的创新点在于：1）**目标形式化创新**：首次系统地将自然语言翻译为支持几何空间关系的SpaTiaL逻辑。2）**过程创新**：采用分层、增量式的树构建过程，而非一步到位的扁平公式预测，并通过节点级语义检查确保对齐，提高了生成规范的可解释性和可靠性。\n\n## 实验与结果\n**数据集与实验平台**：实验基于作者构建的**NL2SpaTiaL数据集**。该数据集通过一个逻辑优先的合成管道生成：首先采样分层SpaTiaL公式骨架，然后通过确定性的、保持语义的回译程序将公式树转换为规范的自然语言描述，最后使用LLM将这些规范描述扩展为多种语言变体。实验在多个机器人操作任务场景中进行评估。\n\n**基线方法**：对比的基线包括：1) **Direct**：使用LLM直接生成SpaTiaL公式。2) **CoT**：使用思维链提示的LLM。3) **Program Synthesis**：基于程序的归纳方法。4) **STL Translator**：专用于STL的翻译器。5) **SpaTiaL (Ours)**：本文完整的NL2SpaTiaL框架。\n\n**关键实验结果**：\n1.  **翻译准确率**：在公式匹配准确率上，本文方法达到**87.2%**，显著高于Direct (53.1%)、CoT (61.8%) 和 Program Synthesis (42.5%) 等方法。这证明了分层翻译和验证机制的有效性。\n\n![翻译准确率对比](https://arxiv.org/html/2512.13670v1/x4.png)\n\n> **图4**：不同方法在NL2SpaTiaL测试集上的公式匹配准确率对比。本文方法（SpaTiaL）取得了最高准确率。\n\n2.  **层次化生成能力**：评估模型恢复HLT结构的能力。本文方法在恢复完整HLT（包括节点、边和跨度对齐）的任务上达到**76.5%**的准确率，远高于仅使用LLM的基线（~30%），说明其能有效捕捉指令的层次结构。\n\n![层次化生成结果](https://arxiv.org/html/2512.13670v1/x5.png)\n\n> **图5**：层次化生成任务的准确率分解。本文方法在恢复HLT的各个组件（节点、细化边、横向边、跨度对齐）上均表现最佳。\n\n3.  **下游任务性能**：在基于SpaTiaL规范的机器人任务执行中，使用本文方法翻译的规范进行监控和重规划，任务成功率相比使用直接翻译的STL规范有显著提升。\n\n![下游任务性能](https://arxiv.org/html/2512.13670v1/x6.png)\n\n> **图6**：在操作任务中，使用不同方法生成的规范进行执行的成功率对比。基于本文SpaTiaL规范的任务成功率最高。\n\n**消融实验**：消融研究表明，移除语义一致性检查器会导致准确率下降约12%，证明了检查器对确保语义对齐的关键作用。同时，分层树结构相比扁平生成，带来了约8%的性能提升。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**NL2SpaTiaL框架**，首次实现了从自然语言到几何时空逻辑（SpaTiaL）的层次化、可验证的翻译，通过分层逻辑树和语义检查器确保了生成规范的语义忠实度和可解释性。\n2.  构建并发布了**NL2SpaTiaL数据集**，该数据集对齐了自然语言指令、扁平SpaTiaL公式及其分层分解与跨度级对齐，为系统研究层次化生成和NL到逻辑的一致性提供了基准。\n\n**局限性**：论文提到，其语义检查器依赖于LLM作为软预言机，可能存在判断错误或偏差。此外，框架主要关注逻辑形式的语义对齐，并未显式检查生成的SpaTiaL公式在特定几何场景下的可行性（例如，空间约束可能相互矛盾）。\n\n**对后续研究的启示**：\n1.  **形式化与学习的结合**：展示了如何将形式化逻辑（SpaTiaL）的严谨性与数据驱动方法（LLM）的灵活性相结合，为可验证的机器人推理提供了一条路径。\n2.  **层次化与可解释性**：强调并实现了从自然语言到形式规范的层次化分解，这不仅是提高翻译准确率的关键，也极大地增强了整个流程的可解释性和可调试性。\n3.  **未来方向**：可以探索将几何可行性检查集成到框架中，或者将生成的SpaTiaL规范直接用于机器人运动规划和控制合成，形成从语言到动作的完整可验证管道。",
      "imageUrls": [
        "https://arxiv.org/html/2512.13670v1/x1.png",
        "https://arxiv.org/html/2512.13670v1/x2.png",
        "https://arxiv.org/html/2512.13670v1/x3.png",
        "https://arxiv.org/html/2512.13670v1/figures/rekep.png",
        "https://arxiv.org/html/2512.13670v1/x4.png",
        "https://arxiv.org/html/2512.13670v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.13093",
      "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations",
      "url": "http://arxiv.org/abs/2512.13093",
      "arxivId": "2512.13093",
      "date": "2025-12-15",
      "authors": "Wenjun Zeng Team",
      "category": "Manipulation",
      "summary": "本文针对人形机器人全身控制中强化学习样本效率低下的核心问题，提出PvP框架。该方法利用本体感觉与特权状态的内在互补性，通过对比学习提取紧凑且任务相关的潜在表示，无需手工数据增强。实验在LimX Oli机器人上进行速度跟踪与运动模仿任务，结果表明PvP相比基线状态表示学习方法，显著提升了样本效率与最终性能。",
      "detailedSummary": "## 研究背景与动机\n强化学习已成为人形机器人全身控制的主流数据驱动方法，但其样本效率低下是核心挑战，这源于人形机器人复杂的动力学、部分可观测性以及复合奖励函数优化。为提升样本效率，状态表征学习通过将高维感官输入压缩为紧凑且信息丰富的潜在表征，成为一种有前景的解决方案。现有方法主要包括基于重建、基于动力学建模和基于对比学习的方法。然而，在人形机器人全身控制中集成SRL仍未被充分探索：基于重建的方法（如预测特权信息）可能因保留不相关细节而导致表征质量次优和泛化能力差；而现有的对比学习方法（如PIM）仅依赖单一状态模态，未能利用特权状态信息，限制了其捕获完整任务相关动态的能力。本文针对上述痛点，提出了一个名为PvP的新视角：利用人形机器人的本体感觉状态和特权状态之间的内在互补性进行对比学习，以增强用于策略学习的本体感觉表征。本文核心思路是：将包含更丰富信息的特权状态视为本体感觉状态的伪增强，通过对比学习对齐这两种模态的表示，从而学习到紧凑且任务相关的表征，无需手工数据增强即可实现更快速、更稳定的策略学习。\n\n## 方法详解\nPvP框架旨在通过对比学习加速人形机器人全身控制任务的学习。其核心是利用本体感觉状态和特权状态之间的互补关系。本体感觉状态包含硬件可直接测量的信号（如关节位置、速度、基座角速度、重力方向估计），而特权状态包含仅在训练时可用的完整模拟器状态（如根位姿与线速度、接触标志、环境特征），且满足本体感觉状态是特权状态的子集。\n\n![方法框架](https://arxiv.org/html/2512.13093v1/x1.png)\n> **图1**：PvP方法概述。(a) 特权状态和本体感觉状态的构成。(b) PvP基于两种状态模态之间的内在互补性进行对比学习。\n\n具体而言，PvP将特权状态 `s` 视为本体感觉状态 `o` 的伪增强。同时，对特权状态 `s` 中的特权信息部分进行零掩码，仅保留其中的本体感觉观测部分，得到 `s̃`。由此得到的数据对 `(s, s̃)` 用于训练策略编码器，遵循SimSiam算法。设策略编码器为 `f_θ`，预测器为 `h_ψ`，过程如公式(3)所示：`z = f_θ(s)`, `z̃ = f_θ(s̃)`, `p = h_ψ(z)`, `p̃ = h_ψ(z̃)`。PvP损失定义为公式(4)：`L_PvP = D_ncs(p, sg(z̃)) + D_ncs(p̃, sg(z))`，其中 `D_ncs` 为负余弦相似度损失，`sg` 为停止梯度操作。该方法创新性地利用两种模态的互补性进行对比，无需手工数据增强，能产生更丰富、更全面的表征。\n\n为支持系统性评估，本文开发了SRL4Humanoid框架，这是首个为机器人学习提供的统一、模块化、即插即用的SRL方法高质量实现框架。\n\n![SRL4Humanoid框架](https://arxiv.org/html/2512.13093v1/x2.png)\n> **图2**：SRL4Humanoid框架架构，其中SRL和RL过程完全解耦。\n\n如图2所示，该框架以近端策略优化为骨干RL算法。策略网络接收机器人本体感觉状态以生成动作，价值网络接收环境特权状态进行价值估计。SRL与RL过程完全解耦，SRL目标可根据配置应用于策略编码器或价值编码器。框架实现了三种具有代表性的SRL算法（SimSiam、SPR、VAE），涵盖不同方法范式。总优化目标为 `L_Total = L_RL + λ · L_SRL`。针对大规模并行RL早期产生大量重复低质量数据可能导致SRL过早陷入局部最优的问题，框架采用了间隔更新机制：`L_Total = L_RL + 𝟙(T) · λ · L_SRL`，即每隔T步才应用SRL损失，以持续影响策略学习。\n\n## 实验与结果\n实验在LimX Oli人形机器人（31自由度）平台上进行，设计了两个代表性任务：**LimX-Oli-31dof-Velocity**（在平坦地形上跟踪速度指令）和**LimX-Oli-31dof-Mimic**（模仿20个预录制的人类动作）。骨干RL算法为PPO，对比的SRL基线方法包括SimSiam、SPR和VAE。评估指标包括总体任务性能、关键性能指标、训练效率以及仿真到实物的部署效果。\n\n![总体奖励对比](https://arxiv.org/html/2512.13093v1/x4.png)\n> **图4**：在两个人形机器人全身控制任务上，原始PPO代理及其与四种SRL方法结合的训练进度对比。实线和阴影区域分别表示平均值和标准差。\n\n关键实验结果如下：在总体任务性能上（图4），对于速度跟踪任务，PvP显著加速了学习过程，而其他SRL方法仅带来边际改进。对于运动模仿任务，PvP取得了最高性能，而VAE方法出现性能下降。这表明利用特权信息增强SRL能使智能体从噪声和冗余感官输入中提取更具信息量的特征。\n\n![动作平滑性优化对比](https://arxiv.org/html/2512.13093v1/x5.png)\n> **图5**：原始PPO代理及其与四种SRL方法结合在动作平滑性优化上的对比。\n\n在关键性能指标上，对于速度跟踪任务，PvP显著加速了动作平滑性惩罚项的收敛（图5），这有利于真实世界部署的可靠性。对于运动模仿任务，PvP在三个关键跟踪指标上也达到了最高性能（图6）。\n\n消融实验方面：\n1.  **训练时间比例影响**（图7）：调整SRL更新间隔对性能有影响，间隔为50步时通常对所有SRL方法都是最优的，这有助于防止过早收敛到局部最优并减少计算开销。\n2.  **训练数据比例影响**（图8）：在运动模仿任务中，增加用于SRL的训练数据比例（从10%到100%）通常能提升性能，特别是对SimSiam和PvP方法。\n3.  **SRL应用于价值编码器的影响**（图9）：将SRL损失应用于价值编码器会导致收敛速度变慢，在速度跟踪任务中甚至出现训练崩溃，表明SRL应用于策略编码器能带来更稳定和增强的性能。\n4.  **计算效率**：所有SRL模块在GPU上运行，不影响整体训练效率，能以最小的计算资源成本有效加速任务。\n\n![Sim2Sim评估](https://arxiv.org/html/2512.13093v1/x10.png)\n> **图10**：在MuJoCo模拟器上的仿真到仿真评估，展示了学习到的策略执行复杂任务（运动模仿和速度跟踪）的能力。\n\n最后，通过MuJoCo平台上的Sim2Sim评估（图10）和LimX Oli机器人的真实机器人测试，验证了学习策略在真实场景中的有效性。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了PvP，一种利用本体感觉与特权状态互补性进行对比学习的新框架，无需手工数据增强即可高效学习任务相关表征；2）开发了SRL4Humanoid，首个面向人形机器人学习的统一、模块化SRL框架，为可重复研究和系统分析提供了基础；3）通过大量实验验证了PvP在样本效率和最终性能上优于现有SRL基线，并为如何将SRL与RL有效集成提供了实践见解（如间隔更新机制、应用于策略编码器更有效）。\n\n论文提及的局限性包括：尽管计算开销小，但SRL仍会引入额外的训练时间；PvP依赖于仿真中可用的特权状态，其向完全真实世界设置（无特权信息）的泛化能力有待进一步研究。\n\n本文的启示在于：利用机器人不同传感模态间的内在关系（如本体感觉与特权信息）进行自监督表征学习，是提升数据效率的有效途径；将SRL作为RL的辅助任务时，更新策略（如间隔更新）和数据使用策略对最终性能有重要影响，需要精细设计；SRL4Humanoid框架为社区提供了一个宝贵的基准测试和开发平台，有望推动该领域的标准化进程和后续研究。",
      "imageUrls": [
        "https://arxiv.org/html/2512.13093v1/x1.png",
        "https://arxiv.org/html/2512.13093v1/x2.png",
        "https://arxiv.org/html/2512.13093v1/x3.png",
        "https://arxiv.org/html/2512.13093v1/x4.png",
        "https://arxiv.org/html/2512.13093v1/x5.png",
        "https://arxiv.org/html/2512.13093v1/x6.png",
        "https://arxiv.org/html/2512.13093v1/x7.png",
        "https://arxiv.org/html/2512.13093v1/x8.png",
        "https://arxiv.org/html/2512.13093v1/x9.png",
        "https://arxiv.org/html/2512.13093v1/x10.png",
        "https://arxiv.org/html/2512.13093v1/x11.png",
        "https://arxiv.org/html/2512.13093v1/x12.png",
        "https://arxiv.org/html/2512.13093v1/figures/pvp_mimic_003_film.png",
        "https://arxiv.org/html/2512.13093v1/figures/pvp_mimic_018_film.png",
        "https://arxiv.org/html/2512.13093v1/figures/pvp_walk_left_to_right_film.png",
        "https://arxiv.org/html/2512.13093v1/figures/pvp_rotation_film.png",
        "https://arxiv.org/html/2512.13093v1/figures/mocap_001.png",
        "https://arxiv.org/html/2512.13093v1/figures/mocap_013.png",
        "https://arxiv.org/html/2512.13093v1/figures/mocap_019.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.11609",
      "title": "UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations",
      "url": "http://arxiv.org/abs/2512.11609",
      "arxivId": "2512.11609",
      "date": "2025-12-12",
      "authors": "Jinqiao Wang Team",
      "category": "Manipulation",
      "summary": "本文提出UniBYD框架，旨在解决机器人手与人类手之间的“具身差距”导致从人类示范学习操作性能受限的核心问题。关键技术包括：统一的形态学表示（UMR）以建模多样手部形态；动态PPO算法配合退火奖励调度，使强化学习从模仿人类示范过渡至探索适应机器人自身形态的策略；以及基于马尔可夫的混合影子引擎，实现细粒度的人类操作模仿。在提出的多手形态操作基准UniManip上，实验表明其成功率相比现有最优方法提升了67.90%。",
      "detailedSummary": "## 研究背景与动机\n当前，从人类演示中学习已成为具身智能领域的主导范式。然而，人类手与不同形态的机器人手之间存在“具身鸿沟”，这给学习带来了重大挑战。现有方法存在关键局限性：基于重定向的方法通常仅映射运动学姿态而忽略动态信息；模仿学习方法则局限于复现人类操作，由于形态和动力学差异，性能远低于人类水平。一些研究开始探索基于人类演示的强化学习，但其奖励函数通常强制机器人与专家轨迹在每一步都严格对齐，这仍属于模仿学习的范畴，难以发现真正适应机器人自身形态的策略。另一些研究则完全放弃人类先验，仅以物体姿态误差为中心定义奖励函数，这导致训练容易陷入局部最优且收敛困难。此外，现有方法泛化能力有限，大多针对特定机器人手，缺乏适应不同机器人的统一框架。\n\n本文针对上述痛点，提出了超越单纯模仿人类演示、学习适应不同机器人形态的操作策略的新视角。其核心思路是：提出一个统一的强化学习框架UniBYD，通过动态奖励退火机制，引导策略从模仿人类演示平滑过渡到自主探索，从而发现与多样化机器人形态物理特性相契合的操控策略。\n\n## 方法详解\nUniBYD是一个统一且渐进的强化学习框架，旨在从人类演示中学习，并为各种机器人手发现超越单纯模仿的、适应形态的策略。其整体流程是：在训练阶段，策略网络预测的动作与演示中的专家动作混合，生成最终执行的动作以推进环境；在推理阶段，则完全依赖策略网络的动作完成任务。\n\n![方法框架](https://arxiv.org/html/2512.11609v1/x2.png)\n> **图2**：UniBYD框架总览。UniBYD首先通过统一形态表示编码不同手型，然后采用带有奖励退火机制的动态PPO，初期利用影子引擎进行高保真模仿，随后过渡到自主探索以发现适应形态的策略。\n\n**核心模块1：统一形态表示**\n为了实现跨形态泛化，UniBYD提出了统一形态表示。对于机器人手h，其本体感知状态包括固定维度的手腕状态和可变维度的关节状态。为解决关节角度周期性问题和统一维度，UMR对关节角度进行三角函数编码，并对自由度少于最大自由度的手进行零填充。此外，UMR还从URDF模型中提取关键静态形态属性构成静态描述符。最终，将手腕状态、填充后的关节状态和静态描述符拼接，形成固定维度的策略观察。这使得策略能够适应不同的手部形态。\n\n**核心模块2：动态近端策略优化**\n在UMR提供一致观察空间的基础上，UniBYD采用集成了奖励退火与损失协同平衡机制的渐进强化学习算法。\n1.  **奖励退火**：总奖励是模仿奖励和稀疏目标奖励的动态加权和。模仿奖励是一个密集的多组件奖励，量化当前状态与专家状态的相似性。目标奖励仅在整条轨迹成功完成时给予大幅奖励。动态权重根据训练阶段调整：早期（模仿驱动阶段）权重固定为1，专注于模仿；当模型通过模仿获得基本能力后，进入混合阶段，模仿奖励权重根据近期成功率动态衰减；一旦成功率超过阈值，进入探索阶段，模仿奖励权重降至极小值，更新主要由目标奖励驱动，策略可自由探索适应机器人形态的策略。\n2.  **损失协同与平衡**：PPO目标函数中集成了熵正则化项和边界损失。熵正则化鼓励早期探索，其系数线性衰减。边界损失则对明显超出物理边界的动作均值进行惩罚。二者形成协同与平衡：熵项促进广泛探索，边界损失确保探索保持在物理可行的平滑动作空间内。\n\n**核心模块3：基于混合马尔可夫的影子引擎**\n在训练初期，策略网络较弱，轻微的动作偏差会导致状态迅速偏离专家轨迹，造成频繁的早期回合终止。为此，UniBYD引入了影子引擎，在早期提供细粒度的引导。\n1.  **灵巧手控制**：执行的动作是策略预测动作和专家演示动作的动态加权混合。专家动作的权重随训练周期线性衰减至0。这使得早期学习近似于离散的逐点学习，减轻了误差累积，后期则完全由策略处理完整的马尔可夫决策过程。\n2.  **物体控制**：在复杂任务中，影子引擎还通过PD控制器对物体施加动态支持力，将其约束在目标轨迹附近，防止掉落或严重偏离。该PD控制器的增益也随训练逐渐衰减至零。\n\n![影子引擎](https://arxiv.org/html/2512.11609v1/x3.png)\n> **图3**：影子引擎中的动作生成与物体控制概述。它将模型预测动作与专家引导动作混合生成最终执行动作，并通过PD控制器施加专家物体力来引导物体。\n\n## 实验与结果\n**实验设置**：本文构建了首个涵盖多种手部形态的机器人操作基准测试UniManip，包含29个任务类别，评估2指、3指、5指（单/双手）形态。评估指标包括位置误差、朝向误差、成功率以及量化策略与硬件形态契合度的适应分数。\n**对比方法**：与三种代表性基线对比：基于优化的逆运动学的经典重定向方法、当前SOTA方法ManipTrans、以及目标中心的方法DexMachina（*表示在部分任务上因完全失败而无法获得有意义结果）。\n**关键实验结果**：如表1所示，UniBYD在所有手型和任务设置上均大幅超越基线。例如，在5指单手任务上，成功率相比ManipTrans提升了57.72个百分点（从29.75%到87.47%），总体任务成功率相比当前SOTA提升了67.90%。位置误差和朝向误差也显著降低。适应分数表明UniBYD学习到的策略更符合机器人形态。\n\n![对比结果](https://arxiv.org/html/2512.11609v1/sec/fig/finalcombined_compariso.png)\n> **图5**：主要对比实验结果表。UniBYD在成功率上显著超越所有基线方法，尤其在非5指形态上优势明显。\n\n**消融实验**：消融研究验证了各核心组件的贡献。移除影子引擎导致早期训练极不稳定，成功率大幅下降。移除动态奖励退火（固定模仿奖励权重）会使策略过度依赖模仿，无法探索更优的形态适应策略，性能受限。移除UMR（使用原始关节状态）会损害跨形态泛化能力。熵正则化和边界损失的协同作用也被证明对稳定训练至关重要。\n\n![消融实验](https://arxiv.org/html/2512.11609v1/x5.png)\n> **图6**：消融实验（5指单手任务）。展示了移除影子引擎、移除动态奖励退火、移除UMR对成功率的影响，证明了各核心组件的必要性。\n\n**定性结果**：图7-10展示了UniBYD与基线方法在具体任务上的策略对比。可以看到，UniBYD的策略更自然、高效地利用了机器人手的形态特性，而基线方法则可能出现不协调或低效的动作。\n\n![定性对比1](https://arxiv.org/html/2512.11609v1/x6.png)\n> **图7**：在“旋转魔方”任务上的策略对比。UniBYD的策略更适应机器人手结构。\n\n![定性对比2](https://arxiv.org/html/2512.11609v1/x7.png)\n> **图8**：在“开抽屉”任务上的策略对比。展示了不同方法策略的差异。\n\n## 总结与启发\n**核心贡献**：1) 提出了UniBYD，首个兼容多种机器人手型、能够学习适应不同形态策略的统一强化学习框架。2) 设计了集成了混合马尔可夫影子引擎和渐进奖励退火计划的动态PPO学习机制，实现了从精细模仿到自主探索的平滑过渡。3) 构建了首个基于人类演示数据的统一基准测试UniManip，用于跨形态评估机器人操作能力。\n**局限性**：论文自身提到的局限性包括：框架在极其复杂的非拟人形态（如多指节或连续体机械手）上的泛化能力仍需验证；动态PPO和影子引擎涉及多个超参数，调优需要一定经验；仿真到实物的迁移性能是未来需要探索的重点。\n**后续启示**：UniBYD为“超越模仿”的学习范式提供了可行的技术路径。其统一形态表示和动态学习机制为构建通用的机器人技能学习系统提供了思路。未来的工作可以探索更复杂的形态、结合视觉等多模态输入，并着重解决仿真到实物的迁移挑战。",
      "imageUrls": [
        "https://arxiv.org/html/2512.11609v1/x1.png",
        "https://arxiv.org/html/2512.11609v1/x2.png",
        "https://arxiv.org/html/2512.11609v1/x3.png",
        "https://arxiv.org/html/2512.11609v1/x4.png",
        "https://arxiv.org/html/2512.11609v1/sec/fig/finalcombined_compariso.png",
        "https://arxiv.org/html/2512.11609v1/x5.png",
        "https://arxiv.org/html/2512.11609v1/x6.png",
        "https://arxiv.org/html/2512.11609v1/x7.png",
        "https://arxiv.org/html/2512.11609v1/x8.png",
        "https://arxiv.org/html/2512.11609v1/x9.png",
        "https://arxiv.org/html/2512.11609v1/sec/fig/three_methods_comparison.png",
        "https://arxiv.org/html/2512.11609v1/x10.png",
        "https://arxiv.org/html/2512.11609v1/x11.png",
        "https://arxiv.org/html/2512.11609v1/x12.png",
        "https://arxiv.org/html/2512.11609v1/sec/fig/a4.png",
        "https://arxiv.org/html/2512.11609v1/sec/fig/a5.png",
        "https://arxiv.org/html/2512.11609v1/x13.png",
        "https://arxiv.org/html/2512.11609v1/x14.png",
        "https://arxiv.org/html/2512.11609v1/sec/fig/a8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.11921",
      "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control",
      "url": "http://arxiv.org/abs/2512.11921",
      "arxivId": "2512.11921",
      "date": "2025-12-11",
      "authors": "Ibrahim Sheikh Mohamed Team",
      "category": "Manipulation",
      "summary": "本文旨在解决将大规模视觉-语言-动作模型高效部署到低成本机器人平台的核心挑战，包括计算资源受限以及对新机器人本体的适配问题。提出采用低秩适配与量化技术对预训练VLA模型进行资源高效的微调，使其能在仅8GB显存的消费级GPU上运行。通过在SO101机械臂上进行真实世界的按钮按压任务实验（基于200个演示片段训练），结果表明该方法在保持计算效率的同时，实现了有效的操作性能，推动了先进机器人操控能力的普及。",
      "detailedSummary": "## 研究背景与动机\n视觉-语言-动作（VLA）模型通过端到端学习视觉观察和自然语言指令，在机器人操控领域展现出卓越能力。然而，当前主流的大规模VLA模型（如OpenVLA、RT-2、Octo）部署通常依赖于昂贵的机器人硬件（如Franka Emika、Kuka机械臂）和高性能GPU（24GB以上显存），这极大地限制了其在低成本平台上的可及性。本文针对两大关键痛点：一是大规模VLA模型在资源受限设备上巨大的计算与内存需求；二是如何用有限的演示数据高效地将预训练VLA模型适配到新的机器人形态上。本文提出了一种结合低秩适配（LoRA）和量化技术的高效微调方法，使得数十亿参数的VLA模型能够在仅8GB显存的消费级GPU上运行并进行微调，从而推动高级操控能力向低成本机器人平台的普及。\n\n## 方法详解\n本文提出的方法旨在将预训练的大规模VLA模型高效地适配到低成本机器人平台（SO101机械臂）上，并完成真实世界的部署。整体流程包含三个阶段：基于遥操作的双目视觉数据收集、使用LoRA与量化的高效微调、以及包含动作分块的实时部署。\n\n![系统架构总览](https://arxiv.org/html/2512.11921v1/Images/system_architecture.png)\n\n> **图2**：系统整体架构。展示了从数据收集、高效微调到实时部署的完整流程。左侧为通过遥操作收集双视角（顶部和腕部摄像头）演示数据；中间为使用LoRA和4位量化对预训练VLA模型进行高效微调；右侧为在真实机器人上进行实时推理与动作执行。\n\n**核心模块一：数据收集框架**。使用主从遥操作方式收集SO101机械臂的演示数据。每个演示片段包含以30Hz采样的双摄像头图像序列（顶部全局视角720x1280，腕部特写视角240x320）、关节位置动作序列以及自然语言任务描述。数据以LeRobot v3.0格式存储，确保与标准训练流程兼容。\n\n**核心模块二：高效微调方法**。该方法基于一个约31亿参数的VLA模型（SmolVLA），其视觉编码器为SigLIP-SO400M（4亿参数），语言模型为Phi-2（27亿参数），动作预测头为3.2M参数。\n1.  **Low-Rank Adaptation (LoRA)**：在语言模型（Phi-2）的32层Transformer的所有注意力投影矩阵（q_proj, k_proj, v_proj, o_proj）上应用LoRA。对于每个权重矩阵 \\(W \\in \\mathbb{R}^{d \\times k}\\)，引入可训练的低秩矩阵 \\(B \\in \\mathbb{R}^{d \\times r}\\) 和 \\(A \\in \\mathbb{R}^{r \\times k}\\)，更新后的权重为 \\(W' = W + \\frac{\\alpha}{r}BA\\)，其中 \\(W\\) 被冻结。设置秩 \\(r=8\\)，缩放因子 \\(\\alpha=16\\)。这使语言模型的可训练参数从约27亿大幅减少至约520万。\n2.  **4位量化策略**：使用BitsAndBytes库进行NF4（NormalFloat4）量化，将模型权重表示为4位整数，推理时反量化为16位浮点数进行计算。结合双量化（对量化因子本身再量化）技术，实现了约8倍的内存节省，且精度损失小于2%。\n3.  **视觉编码器微调策略**：本文系统比较了两种关键策略。**冻结视觉编码器**：仅微调语言模型LoRA适配器和动作头，总计约840万可训练参数，计算需求低。**解冻视觉编码器**：同样对视觉编码器应用LoRA进行微调，总计约3300万可训练参数，允许模型适应机器人特定的视觉环境（如摄像头角度、光照），但需要更多计算资源和数据。\n4.  **训练配置**：在8GB显存限制下，使用批量大小1，梯度累积步数8，混合精度（FP16）训练。损失函数为动作的均方误差（MSE）。学习率采用余弦退火调度（\\(5\\times10^{-5}\\) 至 \\(1\\times10^{-6}\\)），使用AdamW优化器。\n\n**核心模块三：部署框架**。部署系统以20Hz控制频率运行，包含实时推理流水线、动作空间适配和安全机制。\n1.  **实时推理与动作分块**：模型使用动作分块技术，一次预测未来50步的动作序列（\\(N_{\\text{chunk}}=50\\)），然后按顺序执行。当队列耗尽时，基于最新观测生成新的动作块。整个推理流水线（预处理、前向传播、后处理）耗时约45毫秒，满足实时性要求。\n2.  **动作空间适配与安全**：将模型输出的归一化动作，通过缩放、偏移和裁剪，映射到SO101机器人的实际关节空间。部署系统集成了关节限位、速度限制、急停和动作平滑等安全机制。\n\n与现有方法相比，本文的核心创新在于将主要用于语言模型的LoRA与量化技术系统性地应用于VLA模型的机器人操控适配，并深入探讨了在资源受限场景下，视觉编码器冻结与否这一关键设计选择所带来的权衡。\n\n## 实验与结果\n**实验设置**：研究在低成本SO101 6自由度机械臂平台上进行，机器人配备双目视觉系统（顶部Intel RealSense D455全局相机和腕部特写相机）。任务为桌面按钮按压操作。使用200条人类演示片段进行微调。\n\n**对比方法**：主要对比了本文方法中两种不同的微调配置：1) **冻结视觉编码器**；2) **解冻视觉编码器**（视觉编码器也应用LoRA）。此外，实验分析了不同训练数据量对性能的影响。\n\n**关键实验结果**：\n1.  **训练过程与性能**：在200条演示数据上，两种配置都能有效降低训练损失。解冻视觉编码器需要更长的训练步数（10,000步 vs 5,000步）但能达到更低的最终损失。\n\n![训练损失曲线](https://arxiv.org/html/2512.11921v1/Images/training_curves.png)\n\n> **图3**：冻结与解冻视觉编码器配置下的训练损失曲线。解冻配置需要两倍训练步数（10k vs 5k），但最终收敛至更低的损失值。\n\n2.  **真实世界成功率**：在按钮按压任务中，使用200条数据微调的模型取得了显著的成功率。解冻视觉编码器的配置展示了更好的性能。\n\n![冻结与解冻视觉编码器性能对比](https://arxiv.org/html/2512.11921v1/Images/frozen_vs_unfrozen.png)\n\n> **图5**：不同训练数据量下，冻结与解冻视觉编码器配置的真实任务成功率对比。在数据量充足（200条）时，解冻配置性能更优；数据较少时，冻结配置更稳定。\n\n3.  **数据不足导致的失败模式**：当训练数据仅有50条时，模型出现特征性失败，如机械臂在按钮上方徘徊无法精确定位、或在错误位置执行抓握。\n\n![数据不足导致的失败案例](https://arxiv.org/html/2512.11921v1/Images/insufficient_data_failures.png)\n\n> **图4**：训练数据不足（50条演示）时观察到的典型失败模式。包括在目标上方徘徊、过早或过晚关闭夹爪等，表明模型未能学习到精确的终端操控技能。\n\n4.  **视觉编码器的影响分析**：通过可视化注意力图，发现解冻的视觉编码器能更关注与任务相关的区域（如按钮和夹爪），而冻结的编码器注意力分布更分散。\n\n![视觉编码器对注意力分布的影响](https://arxiv.org/html/2512.11921v1/Images/vision_influence.png)\n\n> **图6**：解冻与冻结视觉编码器下，模型注意力热力图对比。解冻配置的注意力更集中于任务相关区域（按钮、夹爪尖端），而冻结配置的注意力更分散。\n\n**消融实验总结**：核心消融在于对比**冻结**与**解冻**视觉编码器。结果表明，在数据充足（200条）时，解冻视觉编码器能带来性能提升，因为它能适配特定的视觉环境；但在数据稀缺时，冻结策略更为稳健，计算效率也更高。这明确了两种策略的适用场景。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一套结合LoRA与4位量化的高效微调方法，成功将31亿参数的VLA模型部署到仅配备8GB显存消费级GPU的低成本机器人平台（SO101）上。\n2.  提供了详尽的真实世界部署分析，包括系统架构、失败模式以及训练数据量与性能的关系，明确指出200条演示数据是实现可靠操控的阈值，低于此值会出现特征性失败。\n3.  系统性地探索并分析了VLA模型微调中“冻结 vs 解冻视觉编码器”这一关键选择的权衡，为不同资源与数据条件下的适配策略提供了实证指导。\n\n**局限性**：论文自身提到的局限性包括：实验仅针对单一任务（按钮按压）和单一机器人平台（SO101）；使用的演示数据量（200条）相对有限；研究未探索更复杂的多任务或长视野任务。\n\n**对后续研究的启示**：本研究证明了通过高效的参数微调技术，前沿的大规模VLA模型能够“平民化”，运行于低成本硬件之上。这为更广泛的研究社区进行物理AI实验打开了大门。未来的工作可以沿着几个方向展开：探索更高效的数据收集与利用方法（如模拟数据、数据增强）、将方法扩展到更多样化的任务和机器人形态、以及研究在持续学习场景中高效适配的机制。",
      "imageUrls": [
        "https://arxiv.org/html/2512.11921v1/Images/system_setup.png",
        "https://arxiv.org/html/2512.11921v1/Images/system_architecture.png",
        "https://arxiv.org/html/2512.11921v1/Images/training_curves.png",
        "https://arxiv.org/html/2512.11921v1/Images/insufficient_data_failures.png",
        "https://arxiv.org/html/2512.11921v1/Images/frozen_vs_unfrozen.png",
        "https://arxiv.org/html/2512.11921v1/Images/vision_influence.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.11988",
      "title": "CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction",
      "url": "http://arxiv.org/abs/2512.11988",
      "arxivId": "2512.11988",
      "date": "2025-12-12",
      "authors": "Stan Birchfield Team",
      "category": "Manipulation",
      "summary": "论文提出CARI4D方法，解决从单目RGB视频中类别无关地重建人类-物体交互4D表示的核心问题，克服了未知物体信息、深度模糊和遮挡等挑战。关键技术包括姿态假设选择算法以集成基础模型预测，通过渲染-比较范式进行联合细化确保对齐，并推理复杂接触以满足物理约束。实验表明，该方法在重建误差上优于先前方法，在分布内数据集上提升38%，在未见数据集上提升36%，并能零样本泛化到野外互联网视频。",
      "detailedSummary": "## 研究背景与动机\n从单目RGB视频中准确捕捉人-物交互对于人类理解、游戏和机器人学习等应用至关重要。然而，由于未知的物体和人体信息、深度模糊性、遮挡以及复杂的运动，从单一RGB视图推断4D交互极具挑战性，阻碍了3D和时间一致性的重建。先前的方法通过简化设置来应对挑战：要么假设已知真实的物体模板来训练特定实例的姿态估计器，要么将方法限制在有限的物体类别内，导致模型无法泛化到训练数据未覆盖的类别。最近的基于图像的重建工作通过构建野外人-物接触数据库来处理更多类别，但其接触检索仍限于标注的类别，且跨视频帧的平移不一致。本文旨在解决这些局限性，提出了首个类别无关的、从单目RGB视频重建度量尺度下时空一致的4D人-物交互的方法。其核心思路是：巧妙地整合基础模型的预测以获得鲁棒的度量初始化，然后通过一个专门训练的、基于“渲染-比较”范式的交互推理模型来细化姿态并推理精细接触，最后通过联合优化获得连贯的交互。\n\n## 方法详解\nCARI4D方法的目标是：给定一个包含N帧RGB图像序列 {𝐈_i}，其中人与物体交互，重建出具有度量尺度的物体网格𝐎及其每帧6DoF姿态{𝒪_i}，以及使用SMPL-H人体模型估计的每帧人体参数{ℋ_i}，确保人体和物体在度量尺度下具有一致的全局平移。\n\n![方法框架总览](https://arxiv.org/html/2512.11988v2/x2.png)\n> **图2**：CARI4D方法概览。给定单目RGB视频，我们以度量尺度重建具有一致接触的4D人体和物体。流程始于度量尺度物体网格重建，随后通过动态姿态假设选择初始化人体和物体姿态，接着训练类别无关的接触推理模型（CoCoNet）来细化交互姿态并估计手部接触，最后进行接触感知的联合优化。\n\n方法流程包含四个核心模块：\n1.  **度量尺度物体重建**：假设物体在视频第一帧基本可见。首先使用f-BRS获取物体掩码，并运行Hunyuan3D-2重建物体网格（归一化尺度）。为了获得度量尺度，使用UniDepth估计度量深度，并采用FoundationPose，通过一种**粗到细的尺度搜索策略**来估计物体尺度。具体而言，先在一组预定义的候选尺度上运行FoundationPose，根据分割物体点云与变换后网格之间的单向倒角距离对候选排序；然后在前3个尺度范围内均匀采样进行更精细的搜索，最终选择倒角距离最小的尺度来调整初始网格，得到度量物体重建𝐎。\n2.  **人体与物体姿态初始化**：此阶段旨在利用基础模型获得在度量尺度空间中对齐的鲁棒初始化。\n    *   **物体姿态假设选择算法**：直接应用FoundationPose（依赖估计的深度）在交互遮挡下不可靠。FoundationPose每帧内部产生K个姿态候选及其质量分数。本文提出动态选择算法：给定上一帧姿态𝒪̂_{i-1}，从当前帧K个候选{𝒪̂_i^j}中基于两个标准筛选：a) **掩码IoU**：渲染候选姿态掩码，与输入图像物体掩码（减去人体掩码以考虑遮挡）计算IoU，过滤低于阈值δ_m的候选；b) **时间平滑性**：计算候选旋转与上一帧旋转之间的测地距离，过滤超过阈值δ_R的候选。从过滤后的列表中选取第一个作为最终物体姿态。若因遮挡导致所有候选被过滤，则向前跳过S帧，在找到合格候选的帧开始，**向后运行姿态跟踪和过滤**。为提高鲁棒性，同时运行FoundationPose的RGB-only和RGBD模式来获取候选。\n    *   **人体估计与对齐**：使用NLF进行每帧人体姿态估计。由于NLF仅使用RGB，其预测可能与UniDepth估计的度量深度（物体已对齐至此）不一致。因此，通过迭代最近点方式优化NLF预测的深度z和全局尺度，使其与UniDepth预测的人体深度对齐。\n3.  **接触推理与细化（CoCoNet）**：前述初始化的人体与物体姿态是独立预测的，未考虑精细交互，可能导致物体漂浮或穿透。因此引入**CoCoNet**，一个类别无关的接触推理模型，用于联合细化姿态并估计接触。给定L帧初始化姿态{ℋ̂_i, 𝒪̂_i}，CoCoNet预测姿态更新{Δℋ̂_i, Δ𝒪̂_i}和双手接触标签{𝐜_i}。网络采用**“渲染-比较”范式**：使用初始化姿态渲染得到RGB、深度和掩码图（ℐ̂），与输入的观测（ℐ，包含通过现成模型获得的深度和掩码）一起输入网络。网络使用冻结的DINOv2编码器提取RGB特征，可训练的轻量DINOv2编码器提取掩码与反投影3D点图的特征，然后应用时空注意力块，最后通过MLP预测输出。\n    *   **CoCoNet训练中的深度对齐**：训练数据中，初始物体姿态与有噪声的预测深度对齐，而真实姿态与GT深度对齐，这种不匹配会导致网络过拟合深度估计器的误差模式。为此，在训练时，首先计算一个尺度s和偏移t，将预测深度𝐃^pr对齐到GT深度𝐃^gt（𝐃^align = s·𝐃^pr + t），然后基于对齐后的深度运行FoundationPose和NLF对齐。这移除了深度估计的绝对平移误差，使网络专注于推理人-物相对姿态。测试时不进行对齐。\n4.  **基于接触的联合优化**：CoCoNet的前馈预测改善了相对姿态，但无法保证接触满足且预测与图像对齐。因此，利用CoCoNet预测的接触进行**接触感知的联合优化**，以进一步提高接触一致性和运动平滑性。优化目标函数L包含多个损失项：接触损失L_c（当预测接触时减小手部关节与物体的距离）、2D投影损失L_j2d、遮挡感知的物体掩码损失L_m、穿透损失L_pen和加速度损失L_acc。\n\n## 实验与结果\n**实验设置**：模型在BEHAVE和HODome数据集上训练。在BEHAVE测试集和未见过的InterCap测试集上评估，遵循先前工作的划分。为更贴近实际视频录制，测试时选择物体在前几帧基本可见的视角，共选取62个BEHAVE视频和22个InterCap视频。\n**评估指标**：报告重建网格与GT网格之间的倒角距离（CD-h人体，CD-o物体，CD-c组合），并首次在4D全局对齐设置下评估（将第一帧重建与GT对齐，并将相同变换应用于整个视频）。同时报告人体加速度误差（Acc-h）和物体平移加速度误差（Acc-o）以衡量运动平滑性。\n**基线方法**：与InterTrack（类别特定，输出点云）、VisTracker（需要已知物体模板）以及图像优化方法PICO进行比较。\n\n![定性对比BEHAVE](https://arxiv.org/html/2512.11988v2/x3.png)\n> **图3**：在BEHAVE数据集上的定性对比。InterTrack重建为噪声点云；VisTracker†（使用本文重建的物体网格）需要模板；本文方法准确重建物体并跟踪姿态（紫色球体表示预测的接触）。\n\n**关键实验结果**：\n*   **BEHAVE测试集（表1）**：本文方法在组合倒角距离（CD-c）上达到8.80 cm，显著优于InterTrack（30.20 cm）和VisTracker（14.22 cm），提升超过38%。物体加速度误差（Acc-o）也最低，表明运动更平滑。\n*   **零样本泛化到InterCap（表2）**：本文方法在组合倒角距离（12.88 cm）上大幅优于VisTracker（20.17 cm）和InterTrack（33.53 cm），提升约36%。即使在关键帧上与图像方法PICO相比，本文方法（5.90 cm）也远优于PICO（87.73 cm），后者因噪声接触预测导致错误的网格检索和对应。\n\n![零样本泛化InterCap](https://arxiv.org/html/2512.11988v2/x4.png)\n> **图4**：零样本泛化到未见的InterCap数据集。VisTracker†使用本文重建的物体网格。本文方法准确重建度量尺度物体并泛化到未见物体。\n\n![野外视频结果](https://arxiv.org/html/2512.11988v2/x5.png)\n> **图5**：泛化到野外互联网视频的更多结果。展示了对于不同物体形状、类别和交互类型的良好泛化能力。\n\n**消融实验（表3 d和e）**：验证了所提模块的有效性。其中，**深度对齐策略**对CoCoNet的训练至关重要，能显著提升重建精度（CD-c从10.01降至8.80）。**姿态假设选择算法**相比直接使用FoundationPose的top-1预测，带来了显著改进。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了首个**类别无关的**方法CARI4D，能够从单目RGB视频输入中，以度量尺度重建物体并时空一致地跟踪4D人-物交互（包括一致的平移和接触）。\n2.  引入了多项创新技术：**姿态假设选择算法**以在遮挡下鲁棒跟踪物体姿态；**CoCoNet**，一个基于渲染-比较范式的类别无关接触推理网络；以及一个**接触感知的联合优化框架**。\n3.  在分布内和未见数据集上，方法性能显著优于现有基线（倒角距离提升超36%），并能零样本泛化到物体类别完全未见的野外视频。\n\n**局限性**：论文自身提到，方法依赖于物体在第一帧基本可见的假设（这在视频录制中常见）。此外，性能仍受限于底层基础模型（如深度估计、初始网格重建）的质量。\n\n**启示**：本文展示了如何通过精心设计的框架，将多个独立发展的、强大的基础模型（用于形状、姿态、场景理解）进行有效集成与对齐，以解决更复杂的综合任务（如4D交互重建）。这为后续研究提供了范例，即通过模块化设计和联合优化策略，可以突破单一模型或单一任务的局限，实现更通用、更鲁棒的感知能力。同时，对接触的显式推理和优化，对于生成物理上合理、可用于机器人学习等下游任务的交互序列至关重要。",
      "imageUrls": [
        "https://arxiv.org/html/2512.11988v2/x1.png",
        "https://arxiv.org/html/2512.11988v2/x2.png",
        "https://arxiv.org/html/2512.11988v2/x3.png",
        "https://arxiv.org/html/2512.11988v2/x4.png",
        "https://arxiv.org/html/2512.11988v2/x5.png",
        "https://arxiv.org/html/2512.11988v2/x6.png",
        "https://arxiv.org/html/2512.11988v2/figures/supp-fig1_network.png",
        "https://arxiv.org/html/2512.11988v2/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.09928",
      "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2512.09928",
      "arxivId": "2512.09928",
      "date": "2025-12-10",
      "authors": "Donglin Wang Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作模型在长时程操作中因依赖当前观测（时间近视）而导致连贯性下降的问题，提出HiF-VLA框架。该框架以运动表示为紧凑时序上下文，通过**后见编码过去动态、预见推理未来运动**，并经由**后见调制联合专家**实现双向时序推理与“边思考边行动”。实验表明，HiF-VLA在LIBERO-Long与CALVIN ABC-D基准上超越强基线，且推理延迟几乎无增加，在真实长时程操作任务中取得显著性能提升。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型已成为机器人操作的一种有前景的框架。然而，大多数VLA模型隐含地假设马尔可夫性质，仅依赖当前观测来预测动作，导致“时间近视”问题，损害了长视野任务中的时序连贯性。现有缓解方法主要包括堆叠多帧历史图像或预测像素级未来子目标，但这些方法存在显著局限性：堆叠原始帧计算开销大、推理延迟高，并引入了大量像素级冗余和静态噪声；而像素级子目标预测则容易产生局部失真和语义漂移，且缺乏连续的时序建模。\n\n本文针对VLA模型在长视野操作中时序推理效率低下和连贯性不足的核心痛点，提出了一个新视角：将**运动**视为一种更紧凑、信息更丰富的时序上下文和世界动态表示。运动能够捕捉状态间的变化，同时过滤静态的像素级噪声。基于此，本文提出了HiF-VLA，一个利用运动进行双向时序推理的统一框架。其核心思路是：通过运动向量紧凑编码历史动态作为后见之明，基于当前洞察推理未来运动作为先见之明，并利用一个后见调制联合专家整合两者，实现“边行动边思考”的长视野操作范式。\n\n## 方法详解\nHiF-VLA的整体流程基于一个基础的VLA架构进行扩展，旨在通过补偿稀疏的视觉观测来扩展动作执行前的时序感受野。其推理过程可形式化表示为：模型不仅接收当前观测和语言指令，还接收紧凑的历史运动信息，并联合预测未来的动作和运动。\n\n![HiF-VLA整体框架图](https://arxiv.org/html/2512.09928v1/x2.png)\n> **图2**：HiF-VLA流程。（a）**后见之明先验获取**：将密集的历史帧序列编码为紧凑的运动向量流，形成结构化的后见之明基元。（b）**带洞察的先见之明推理**：VLM解释任务指令和当前观测，推理出合理的先见之明运动和对应的潜在动作令牌。（c）**后见调制联合专家**：将后见之明、先见之明和动作表示在统一的潜在空间中进行融合，产生时序一致且因果连贯的动作预测。\n\n框架包含三个核心模块：\n1.  **后见之明先验获取**：此模块旨在为决策层提供结构化的时空先验。它不直接堆叠历史RGB帧，而是采用视频编解码标准中的**运动向量**来紧凑编码历史动态。运动向量定义为相邻帧间宏块的位置位移，遵循MPEG-4的16x16宏块布局，形成一个低维张量。相较于原始帧，该表示显著减少了冗余，同时保留了任务相关的动态。随后，一个轻量级的基于ViT的后见编码器结合浅层3D卷积，将历史运动向量编码为紧凑的后见令牌 `M_h`。\n2.  **带洞察的先见之明推理**：此模块使模型能够预见动作的视觉后果。与预测原始未来像素不同，HiF-VLA以更通用、结构化的运动向量作为时空预测目标。具体而言，在VLM的嵌入空间中引入一组可学习的先见查询令牌和一组空的动作令牌。这些令牌与任务指令、当前观测嵌入拼接后输入VLM，通过**非因果注意力掩码**实现对未来运动潜在令牌 `M_f` 和动作潜在令牌 `A_f` 的并行推理。这种设计丰富了VLM内部的“思维”过程，解锁了并行推理的潜力。\n3.  **后见调制联合专家**：这是实现“边行动边思考”的关键模块。它将动作和运动作为共享时序潜在空间中的两个互补流进行联合建模。**关键创新**在于将历史运动先验作为自适应的时序条件，通过**自适应层归一化**来调制联合推理过程，而不是直接嵌入到VLM输入中（论文指出直接注入会破坏视觉-语言模态的对齐）。`M_f` 和 `A_f` 形成两个并行流，通过跨流联合注意力进行交互，同时保留各自的前馈网络以确保互补且解耦的表示。后见令牌 `M_h` 被投影为条件向量 `h_c`，通过AdaLN注入每个联合专家模块，调制先见和动作表示，最终由各自的头部分别生成未来运动 `m~` 和动作 `a~`。\n\n训练时，模型采用结合了动作L1损失和运动L1损失的总体目标函数，通过平衡因子λ进行调节。\n\n## 实验与结果\n实验在两个广泛使用的长视野操作基准上进行评估：**LIBERO-Long**（10个多子任务场景）和**CALVIN ABC-D**（在ABC场景训练，在未见过的D场景评估连续任务泛化能力）。对比的基线方法包括OpenVLA、OpenVLA-OFT、UniVLA、Seer、π₀等。实验设置分为仅使用主摄像头的第三视角和使用主、腕部双摄像头的多视角。\n\n**关键定量结果**：\n在LIBERO-Long上，HiF-VLA在第三视角设置下平均成功率（Avg. SR）达到94.4%，比基线OpenVLA-OFT（91.0%）提升3.4个百分点；在多视角设置下达到96.4%，优于其他SOTA模型。\n\n![LIBERO-Long结果表](https://arxiv.org/html/2512.09928v1/x5.png)\n> **表1**：LIBERO-Long基准上的性能对比。HiF-VLA在第三视角和多视角设置下均取得了最佳或接近最佳的平均成功率。\n\n在CALVIN ABC-D上，HiF-VLA在第三视角设置下的平均任务完成长度达到4.08，优于基线（3.80）；在多视角设置下达到4.35，与当前最佳方法性能相当。\n\n![CALVIN ABC-D结果表](https://arxiv.org/html/2512.09928v1/x6.png)\n> **表2**：CALVIN ABC-D基准上的性能对比。HiF-VLA在两种视角设置下均表现出色，尤其在第三视角下提升显著。\n\n**效率与冗余分析**：\n实验表明，引入基于RGB的子目标或历史帧会显著增加推理延迟（分别达基线的1.59倍和3.15倍）和GPU内存占用，且性能提升有限甚至下降。而HiF-VLA使用运动表示，在引入先见和后见模块后，仅带来微不足道的额外开销（延迟为基线的1.67倍，但绝对值仍远低于帧堆叠方法），同时取得了最佳性能（93.2% SR）。\n\n![效率对比表](https://arxiv.org/html/2512.09928v1/x8.png)\n> **表3**：效率与冗余分析对比。HiF-VLA的变体（运动表示）在性能提升的同时，带来的内存和延迟开销远小于基于RGB帧堆叠或子目标预测的变体。\n\n**消融与可视化分析**：\n![后见长度影响图](https://arxiv.org/html/2512.09928v1/x3.png)\n> **图3**：（b）HiF-VLA的推理延迟随历史长度增加保持低位且稳定增长；（c）性能随历史长度增加而提升，在长度8左右趋于饱和，验证了运动表示的有效性和时序可扩展性。\n\n![后见嵌入位置消融图](https://arxiv.org/html/2512.09928v1/x4.png)\n> **图4**：后见信息嵌入位置的消融实验。将后见令牌作为条件调制专家解码器（b）的性能（93.2% SR）明显优于直接注入VLM输入（a，89.6% SR），证实了论文提出的调制策略能有效避免破坏VLM的模态对齐。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了将**运动向量**作为紧凑、结构化的时序基元来扩展VLA时序感受野的新视角；2) 设计了**HiF-VLA统一框架**，通过后见先验获取、先见推理和后见调制联合专家，实现了高效的双向时序推理；3) 实验证明了该方法在多个长视野基准上实现了性能提升，同时保持了**高效率**和**时序可扩展性**。\n\n论文提到的局限性包括：运动向量的提取依赖传统的视频编解码器，可能无法完美捕捉所有精细的动态；框架在非常长的视野（远超训练时的块长度）任务中的表现仍需进一步探索。\n\n这项工作对后续研究的启示在于：为VLA的时序建模提供了一个比原始像素更高效的表示空间；其“边行动边思考”的范式以及将历史信息作为条件调制（而非直接输入）的设计，为构建更复杂、更连贯的具身智能体提供了有价值的思路。未来工作可以探索更先进的运动表示学习方式，或将此框架与更长期的规划方法相结合。",
      "imageUrls": [
        "https://arxiv.org/html/2512.09928v1/x1.png",
        "https://arxiv.org/html/2512.09928v1/x2.png",
        "https://arxiv.org/html/2512.09928v1/x3.png",
        "https://arxiv.org/html/2512.09928v1/x4.png",
        "https://arxiv.org/html/2512.09928v1/x5.png",
        "https://arxiv.org/html/2512.09928v1/x6.png",
        "https://arxiv.org/html/2512.09928v1/x7.png",
        "https://arxiv.org/html/2512.09928v1/x8.png",
        "https://arxiv.org/html/2512.09928v1/x9.png",
        "https://arxiv.org/html/2512.09928v1/x10.png",
        "https://arxiv.org/html/2512.09928v1/x11.png",
        "https://arxiv.org/html/2512.09928v1/x12.png",
        "https://arxiv.org/html/2512.09928v1/x13.png",
        "https://arxiv.org/html/2512.09928v1/x14.png",
        "https://arxiv.org/html/2512.09928v1/x15.png",
        "https://arxiv.org/html/2512.09928v1/x16.png",
        "https://arxiv.org/html/2512.09928v1/x17.png",
        "https://arxiv.org/html/2512.09928v1/x18.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.13080",
      "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
      "url": "http://arxiv.org/abs/2512.13080",
      "arxivId": "2512.13080",
      "date": "2025-12-15",
      "authors": "Zongqing Lu Team",
      "category": "Manipulation",
      "summary": "本文针对VLA模型因依赖2D视觉输入在3D环境中执行动作而导致的感知与动作接地差距问题，提出空间感知VLA预训练范式。关键技术是通过人类演示视频提取3D视觉与动作注释，进行两阶段对齐：3D视觉预训练融合2D与3D特征，3D动作预训练学习物理动作先验；并实例化为双编码器架构\\ModelName。实验表明，该模型在下游机器人任务中显著提升了2D视觉与3D动作的接地性，实现了更鲁棒和可泛化的策略。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作模型为机器人学习提供了一个有前景的范式，它通过整合视觉感知和语言引导的策略学习来实现。然而，大多数现有方法依赖2D视觉输入在3D物理环境中执行动作，这在感知与动作落地之间造成了显著差距。这种弱对应关系限制了模型将动作落地于物理空间的能力。虽然人类可以从2D视觉信号推断3D空间，但现有VLA模型很大程度上忽视了这一方面，导致空间落地能力差和泛化性有限。\n\n本文针对2D视觉感知与3D物理动作之间的鸿沟这一具体痛点，提出了一个新的“空间感知VLA预训练”视角。该范式旨在模型学习机器人策略之前，使其获得3D空间感知能力。其核心思路是：利用大规模人类演示视频作为监督源，从中提取3D视觉和3D动作标注，在预训练阶段显式地对齐2D视觉观测与3D空间理解，从而为下游机器人策略学习提供更强的空间落地基础。\n\n## 方法详解\n本文提出的方法整体框架是一个三阶段流程：首先从多样化的人类演示视频中提取3D视觉和动作标注；然后进行两阶段的空间感知VLA预训练；最后将预训练模型适配到下游机器人任务。\n\n![方法框架](https://arxiv.org/html/2512.13080v1/x1.png)\n> **图1**：提出的视觉-物理对齐框架概览。从人类演示视频出发，提取3D视觉标注和3D动作标注。接着进行两阶段的空间感知VLA预训练：(1) 3D视觉预训练：使用带3D视觉标注的视频，通过双编码器融合模块对齐2D视觉特征与3D空间表征。(2) 3D动作预训练：人类手部轨迹提供3D运动监督，使模型学习物理落地的动作先验。最后，预训练模型VIPA-VLA被适配到机器人操作任务中。\n\n**核心模块1：Hand3D数据集构建**。为了提供视觉-物理对齐的监督，本文构建了Hand3D数据集，它包含3D视觉标注和3D动作标注。数据来源于九个异构的人类操作视频数据集，并统一了手部姿态的MANO表示。\n- **3D视觉标注 (Hand3D-visual)**：如图2所示，通过结合点云估计（Cut3R模型）、物体定位（Gemini-2.5-flash和GroundingDINO）和手部姿态信息（MANO参数）来构建。一个关键步骤是利用绝对物理空间中的手部关节位置对相对点云估计进行尺度校准，以确保在一致的3D物理坐标系中表示。基于这些空间标注，利用大语言模型生成了四类视觉问答形式的监督数据：空间关系、任务完成、手部运动和相机运动，共约30万对指令-答案。\n- **3D动作标注 (Hand3D-action)**：从人类视频的手部运动序列中提取手腕轨迹空间坐标系列，并将其离散化为运动token。结合视频文本指令，构建了指令运动生成、运动翻译和上下文运动预测三类VQA任务，形成了一个约103万样本的精细3D运动模式数据集。\n\n![3D视觉标注构建](https://arxiv.org/html/2512.13080v1/x2.png)\n> **图2**：Hand3D-visual构建概览。通过整合点云估计、物体定位和人类操作视频中的手部姿态标注，将2D视觉观测与3D物理动作空间联系起来，为VLA模型提供视觉-物理对齐监督。\n\n**核心模块2：VIPA-VLA模型架构**。为了弥补传统VLA模型语义视觉编码器缺乏3D空间结构感知能力的不足，本文提出了VIPA-VLA，一个双编码器架构。\n\n![模型架构](https://arxiv.org/html/2512.13080v1/x4.png)\n> **图4**：VIPA-VLA模型架构。一个包含语义视觉编码器和3D编码器的双编码器，通过交叉注意力融合层产生融合的空间-语义特征。在预训练期间，视觉token与文本和运动token使用3D视觉和3D动作标注进行对齐。在后训练期间，动作查询与融合的视觉-语言特征交互以产生条件，该条件与机器人状态结合，并由流匹配动作头处理以预测机器人操作动作。\n\n具体而言，模型在标准语义视觉编码器之外，集成了一个3D视觉编码器来提供显式的场景几何理解。语义编码器输出视觉嵌入 **V_sem**，3D编码器输出空间嵌入 **V_spa**。通过一个交叉注意力融合层（灵感来自VLM-3R）将二者结合：视觉token作为查询，3D空间token作为键和值。融合输出 **F_spa** 通过一个带可学习缩放参数α的残差连接与原始语义特征相加：**V_f = V_sem + αF_spa**。此外，为了理解精细的3D运动轨迹，模型扩展了LLM的词表，引入了一组运动token，用于将连续的3D手腕轨迹坐标离散化。\n\n**核心模块3：空间感知VLA预训练策略**。预训练分为两个阶段，进行渐进式的视觉-物理对齐学习。\n1.  **3D视觉预训练（阶段1）**：冻结所有预训练参数，仅训练融合层。使用Hand3D-visual的VQA数据，目标是使模型能够根据融合的视觉和文本输入，推理3D空间关系并生成答案。\n2.  **3D动作预训练（阶段2）**：扩展LLM词表加入运动token，然后在Hand3D-action数据上训练模型。此阶段冻结语义和空间编码器，训练LLM根据融合的视觉和文本输入预测运动token序列，使模型学习视觉线索如何对应物理落地的运动模式。\n\n**后训练适配**：预训练后，模型获得了2D观测与3D物理/动作表征的对齐。在后训练阶段，为模型附加一个动作头（采用基于流匹配的扩散变换器，DiT），并在下游机器人任务数据上微调，以生成可执行的动作块。\n\n与现有方法相比，本文的创新点具体体现在：1) 提出了**空间感知预训练范式**，在策略学习前显式建立2D-3D对齐；2) 构建了大规模、带丰富3D标注的**人类视频数据集Hand3D**作为监督源；3) 设计了**双编码器架构VIPA-VLA**，通过融合层整合2D语义与3D空间特征，并引入运动token进行3D动作建模。\n\n## 实验与结果\n本文在**LIBERO**基准测试上进行了评估，这是一个包含四个任务套件（LIBERO-S, -O, -G, -L）的机器人操作基准，分别测试空间、物体、目标和长视野任务的泛化能力。实验在模拟环境中进行，每个套件评估500次试验，并报告了单视角和双视角输入设置下的结果。\n\n对比的**baseline方法**广泛且具有代表性，包括TraceVLA、OpenVLA、SpatialVLA、DiT Policy、CoT-VLA、ThinkAct、TriVLA、4D-VLA、GR00T N1.5、MaIL、π0-FAST、MolmoAct、GR00T N1、π0、UniVLA和π0.5。\n\n**关键实验结果**：\n- **单视角输入**：VIPA-VLA在LIBERO基准上的平均成功率达到**92.4%**，超过了所有对比方法，包括需要机器人数据预训练（Robo-PT）的强基线GR00T N1.5（92.1%）。特别是在需要泛化到新物体（LIBERO-O, 97.2%）和新目标（LIBERO-G, 94.2%）的任务上表现优异。\n- **双视角输入**：VIPA-VLA的平均成功率为**96.8%**，与当前最先进的π0.5（96.9%）性能相当，且优于GR00T N1（93.9%）和π0（94.4%）。值得注意的是，VIPA-VLA在最具挑战性的长视野任务（LIBERO-L）上取得了95.0%的高成功率，展示了其强大的长程任务泛化能力。\n\n![LIBERO结果表](https://arxiv.org/html/2512.13080v1/x5.png)\n> **图5/表3**：LIBERO基准测试的成功率（%）。VIPA-VLA在单视角和双视角设置下均取得了领先或极具竞争力的性能，尤其在未见过的物体（O）和场景（G,L）泛化上表现突出。\n\n**消融实验**：研究验证了所提出各个组件的贡献。\n- **移除3D视觉预训练（阶段1）**：性能显著下降，平均成功率从92.4%跌至87.3%，证明了通过VQA学习3D空间关系对齐的重要性。\n- **移除3D动作预训练（阶段2）**：性能也出现下降（至90.5%），表明学习细粒度的3D运动模式对于动作生成至关重要。\n- **仅使用2D视觉特征（无3D编码器）**：性能进一步降低至86.1%，凸显了引入显式3D空间特征编码的必要性。\n- **仅使用3D特征（无语义融合）**：性能最差（82.0%），说明结合2D语义理解和3D空间感知才是最佳方案。\n\n![消融实验](https://arxiv.org/html/2512.13080v1/x6.png)\n> **图6**：消融实验结果。依次移除3D视觉预训练、3D动作预训练、3D编码器，以及仅使用3D特征，模型性能持续下降，证明了每个提出的组件（两阶段预训练、双编码器融合）的有效性和必要性。\n\n此外，论文还提供了真实机器人实验的定性结果（图7），展示了VIPA-VLA在物体重新排列和长视野组装任务中的成功执行情况，以及在干扰存在下的鲁棒性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出空间感知VLA预训练新范式**：首次明确在VLA预训练阶段解决2D视觉与3D物理动作的对齐问题，为模型注入先验的空间理解能力。\n2.  **构建Hand3D数据集**：利用人类视频构建了大规模、带有3D视觉和动作标注的数据集，为视觉-物理对齐提供了宝贵的监督信号。\n3.  **提出VIPA-VLA模型**：设计了双编码器架构和两阶段预训练策略，有效融合了语义与空间特征，并在下游机器人任务中展现出优异的泛化性能。\n\n**论文自身提到的局限性**包括：1) 依赖离线标注流程（点云估计、物体检测、VQA生成），可能引入误差且效率较低；2) 尽管使用了人类视频，但机器人本体与人类存在差异，预训练获得的空间理解在迁移时仍需通过机器人数据进行适配。\n\n**对后续研究的启示**：1) 可以探索更高效的在线或隐式3D感知与对齐方法。2) 如何更好地弥合人类与机器人之间的本体差异，使人类视频知识更直接地迁移，是一个值得深入的方向。3) 本文聚焦于操作任务，未来可将空间感知预训练范式扩展到导航、移动操作等更复杂的具身AI任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.13080v1/x1.png",
        "https://arxiv.org/html/2512.13080v1/x2.png",
        "https://arxiv.org/html/2512.13080v1/x3.png",
        "https://arxiv.org/html/2512.13080v1/x4.png",
        "https://arxiv.org/html/2512.13080v1/x5.png",
        "https://arxiv.org/html/2512.13080v1/x6.png",
        "https://arxiv.org/html/2512.13080v1/x7.png",
        "https://arxiv.org/html/2512.13080v1/x8.png",
        "https://arxiv.org/html/2512.13080v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.10891",
      "title": "Iterative Compositional Data Generation for Robot Control",
      "url": "http://arxiv.org/abs/2512.10891",
      "arxivId": "2512.10891",
      "date": "2025-12-12",
      "authors": "Eric Eaton Team",
      "category": "Manipulation",
      "summary": "论文针对机器人操控数据收集昂贵、组合任务泛化困难的问题，提出语义组合扩散变换器，将状态过渡分解为机器人、对象、障碍物和目标特定组件，通过注意力机制学习交互；并引入迭代自我改进程序，利用离线强化学习验证合成数据并迭代训练。实验表明，该方法在有限任务训练后能零样本生成高质量过渡数据，从中学习未见任务组合的控制策略，性能显著优于整体和硬编码组合基线，最终解决几乎所有保留任务，学习表示中涌现出有意义组合结构。",
      "detailedSummary": "## 研究背景与动机\n在机器人操作领域，收集真实数据成本高昂，使得为多物体、多机器人、多环境场景中组合爆炸的任务空间获取演示变得不切实际。虽然最近的生成模型可以为单个任务合成有用数据，但它们未能利用机器人领域的组合结构，难以泛化到未见过的任务组合。现有方法多专注于提升单任务内的样本效率，而未能利用自生成数据来加速学习全新任务。本文针对组合泛化这一具体痛点，提出了一种新视角：通过构建显式利用组合性的模型架构，实现为零样本、未见过的任务组合生成高质量的合成训练数据。核心思路是设计一个语义组合扩散变换器，将状态转移分解为机器人、物体、障碍物和目标等特定组件，并通过注意力学习它们的交互，进而通过迭代自改进程序，利用合成数据不断提升模型解决新任务的能力。\n\n## 方法详解\n本文提出的方法是一个迭代的组合数据生成流程，核心是一个语义组合扩散变换器（Semantic Compositional Diffusion Transformer）。\n\n![方法框架](https://arxiv.org/html/2512.10891v3/x4.png)\n> **图4**：语义组合变换器架构。将每个转移（transition）分解为状态因子（如不同机器人）、动作、奖励和终止指示器。每个状态因子有自己独立的编码器-解码器对。编码后的令牌与位置嵌入一起，由多个扩散变换器层处理，这些层受时间步和任务指示器嵌入的条件自适应归一化调制。输出令牌由因子特定的解码器解码。当前状态和下一状态的编码器-解码器对是共享的。\n\n整体框架如算法1所述：首先在部分任务（14个）的真实数据上训练初始扩散模型。然后，在每一轮迭代中，使用当前模型为零样本的未见任务生成合成数据，并用离线强化学习算法（TD3-BC）基于这些合成数据训练策略，随后在环境中在线评估该策略的成功率。若某个任务的合成数据能训练出成功率超过阈值τ的策略，则将该合成数据加入下一轮的训练集，用于重新训练扩散模型。此过程迭代进行，旨在逐步扩展模型能有效生成数据的任务范围。\n\n核心模块是语义组合扩散变换器。它基于扩散变换器（DiT）构建，但关键创新在于其**因子特定（factor-specific）的令牌化方案**。具体而言，它将一个转移元组 `<s, a, r, s', d>` 的每个组成部分（对应组合任务图中的一个基本元素 `f ∈ F`，如特定机器人、物体、障碍物、目标、动作、奖励、终止信号）通过一个独立的编码器-解码器对 `(e_f, o_f)` 进行映射。编码器将原始输入映射到学习到的嵌入空间，形成一组令牌 `(y^f)`。这些令牌随后被送入共享的扩散变换器层进行处理。\n\n扩散变换器层通过自注意力机制，让每个因子令牌可以关注所有其他因子令牌，从而**学习因子之间的交互关系**，这相当于从数据中推断出任务组合的计算图结构，而非像先前工作那样硬编码。条件信息（扩散时间步t和任务索引n）通过一个额外的嵌入路径注入，该路径生成自适应层归一化的参数，以调制每个变换器块，从而在不干扰已学习的组合语义的前提下实现任务条件生成。\n\n与现有方法相比，创新点具体体现在：1) **组合式令牌化**：为每个任务组件（因子）使用独立的编码器，而非共享编码器或简单分块，使模型能学习组件特定的表示空间。2) **从数据中学习图结构**：利用变换器作为图神经网络（GNN）的解释，通过注意力权重自动发现组合任务中组件间的依赖关系。3) **迭代自改进与抗崩溃机制**：通过算法1迭代地利用高质量合成数据改进生成模型。由于合成数据仅用于更新其对应任务所涉及的那些因子特定编码器的参数，这种参数的部分隔离有助于缓解生成模型迭代训练中常见的“模型崩溃”问题。\n\n## 实验与结果\n实验在模拟机器人操作基准测试 **CompoSuite** 的一个子集上进行，该子集包含64个任务（固定使用IIWA机器人，组合4种物体×4种障碍×4种目标）。初始训练集仅包含其中14个任务的专家数据，目标是解决其余50个未见任务中的32个测试任务。\n\n对比的基线方法包括：\n- **静态离线RL方法**：硬编码组合RL、语义组合RL。\n- **迭代数据生成基线**：整体式架构（Monolithic，基于SynthER）、标准DiT（无组合令牌化）、语义DiT（共享编码器，即按语义分块但同一轴内组件共享参数）。\n\n![零样本性能对比](https://arxiv.org/html/2512.10891v3/x5.png)\n> **图5**：不同模型架构在初始训练后的零样本生成性能（通过训练出的策略成功率衡量）。本文的语义组合扩散变换器（Factor-Specific）显著优于整体式、标准DiT和共享编码器的语义DiT基线，证明了因子特定编码器对学习组合结构的重要性。\n\n关键结果显示，在仅使用初始14个任务数据训练后，本文方法的零样本生成性能（成功率约45%）远超最佳基线（语义DiT，约5%）。经过**迭代自改进**后，性能得到大幅提升。\n\n![迭代改进性能](https://arxiv.org/html/2512.10891v3/x6.png)\n> **图6**：迭代自改进过程中，在测试任务上的平均成功率。本文方法（Factor-Specific）经过约10轮迭代后，成功率接近100%，而所有基线方法均无法通过迭代获得显著提升。\n\n![任务解决进度](https://arxiv.org/html/2512.10891v3/x7.png)\n> **图7**：随着迭代轮次增加，被解决（成功率>阈值）的测试任务数量。本文方法能逐步解决几乎所有测试任务。\n\n![消融实验：编码器类型](https://arxiv.org/html/2512.10891v3/x8.png)\n> **图8**：消融实验验证了各组件贡献。因子特定编码器（Factor-Specific）是高性能的关键；迭代过程（Iterative）带来了巨大增益；而条件生成（Conditional）也是必要的。共享编码器（Shared）性能很差。\n\n消融实验总结：1) **因子特定编码器**是获得强大零样本泛化能力的核心。2) **迭代自改进程序**能将性能从约45%提升至近100%。3) **条件生成**（将任务标识作为输入）是有效的，移除后性能下降。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了**语义组合扩散变换器**，通过因子特定的令牌化方案和变换器注意力机制，从数据中自动学习组合任务的结构，实现了高质量的零样本数据生成。2) 设计了一种**迭代自改进程序**，通过离线RL验证合成数据质量，并选择性地将其加入训练集，使生成模型能不断扩展其解决新任务组合的能力，最终解决了几乎所有未见任务。3) 分析表明，模型学习到的分解结构与先前硬编码的方法不同，表明**有效的组合结构可以从数据中自动涌现**。\n\n论文自身提到的局限性包括：迭代过程中需要在线评估策略，这在真实机器人上成本高昂；实验限于中等规模的任务组合（64个任务），更大规模组合的扩展性有待验证；方法依赖于任务组合的因子可分解性假设。\n\n对后续研究的启示包括：证明了为组合性设计模型架构（尤其是学习组件特定表示）对泛化至关重要；迭代利用自生成数据是突破有限初始数据瓶颈的有效途径，其关键在于设计抗模型崩溃的机制（如参数隔离）；将变换器视为GNN并用于学习组合关系，为构建可解释、可组合的机器人学习系统提供了新思路。",
      "imageUrls": [
        "https://arxiv.org/html/2512.10891v3/x1.png",
        "https://arxiv.org/html/2512.10891v3/x2.png",
        "https://arxiv.org/html/2512.10891v3/x3.png",
        "https://arxiv.org/html/2512.10891v3/x4.png",
        "https://arxiv.org/html/2512.10891v3/x5.png",
        "https://arxiv.org/html/2512.10891v3/x6.png",
        "https://arxiv.org/html/2512.10891v3/x7.png",
        "https://arxiv.org/html/2512.10891v3/x8.png",
        "https://arxiv.org/html/2512.10891v3/x9.png",
        "https://arxiv.org/html/2512.10891v3/x10.png",
        "https://arxiv.org/html/2512.10891v3/x11.png",
        "https://arxiv.org/html/2512.10891v3/x12.png",
        "https://arxiv.org/html/2512.10891v3/x13.png",
        "https://arxiv.org/html/2512.10891v3/x14.png",
        "https://arxiv.org/html/2512.10891v3/x15.png",
        "https://arxiv.org/html/2512.10891v3/x16.png",
        "https://arxiv.org/html/2512.10891v3/x17.png",
        "https://arxiv.org/html/2512.10891v3/x18.png",
        "https://arxiv.org/html/2512.10891v3/x19.png",
        "https://arxiv.org/html/2512.10891v3/x20.png",
        "https://arxiv.org/html/2512.10891v3/x21.png",
        "https://arxiv.org/html/2512.10891v3/x22.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.11797",
      "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis",
      "url": "http://arxiv.org/abs/2512.11797",
      "arxivId": "2512.11797",
      "date": "2025-12-12",
      "authors": "Vitor Guizilini Team",
      "category": "Manipulation",
      "summary": "本文针对机器人模仿学习中数据收集成本高、多样性不足的核心瓶颈，提出AnchorDream方法。该方法重新利用预训练视频扩散模型，通过以机器人运动渲染为条件锚定具身，防止运动失真，从而合成与机器人运动学一致的对象和环境，仅需少量演示即可生成大规模高质量数据集。实验表明，所生成数据显著提升下游策略学习性能，在模拟器基准测试中取得36.4%的相对增益，在真实世界研究中性能近乎翻倍。",
      "detailedSummary": "## 研究背景与动机\n机器人模仿学习的有效性严重依赖于大规模、高质量演示数据的可用性，然而在现实世界中收集此类数据成本高昂。现有扩展数据的方法主要分为两类：一是扩展观测空间，利用生成模型改变场景和物体的视觉外观，但机器人运动轨迹保持不变，无法创造新行为；二是扩展运动空间，生成新的轨迹，但通常依赖于需要大量手工建模的仿真器，或显式的场景重建，存在显著的仿真到真实差距且可扩展性有限。视频生成模型在互联网规模数据上训练，蕴含了丰富的世界先验，为大规模合成逼真数据提供了潜力，但其挑战在于**具身性锚定**，即现成的模型不受机器人本体约束，容易产生不合理的机器人形态或物理不一致的运动。\n\n本文针对上述痛点，提出了一种新的视角：将预训练的视频扩散模型重新定位为一个**具身感知的世界模型**。其核心思路是，将机器人运动轨迹作为条件，先确定性地渲染机器人本体的运动，再以此“锚点”驱动视频模型合成与之协调一致的环境和物体，从而在无需显式环境建模的情况下，从少量真实演示中生成大规模、高质量、运动一致的机器人演示数据。\n\n## 方法详解\nAnchorDream 的整体框架旨在将机器人轨迹与环境合成解耦，以机器人运动视频为条件，驱动视频扩散模型生成具身一致的演示。其流程始于少量人类遥操作演示，通过启发式轨迹扩展生成大量新轨迹；对于每条新轨迹，仅渲染机器人本体的运动视频（无场景和物体）；最后，将此机器人运动视频与任务描述文本共同作为条件，输入视频扩散模型，合成出包含逼真环境和物体、且与机器人运动一致的完整演示视频。\n\n![方法框架](https://arxiv.org/html/2512.11797v1/x1.png)\n> **图2**：AnchorDream 方法整体框架。从少量人类演示开始，通过扰动关键状态和重组运动片段生成新轨迹，确保运动学可行性。每个增强轨迹被渲染为仅包含机器人运动的视频，该视频与任务描述共同作为条件输入 AnchorDream，合成出环境物体与规划轨迹一致的逼真演示。合成的演示随后用于训练下游模仿学习策略。\n\n**核心模块与技术细节**：\n1.  **轨迹扩展**：借鉴 MimicGen 和 DemoGen 的思路，通过对现有演示中的关键状态（如接触点）进行扰动，并重组物体中心的子轨迹来生成新的轨迹。这确保了新轨迹在机器人运动学上是可行的，且能产生多样化的行为。\n2.  **仅机器人渲染**：对于每条合成轨迹 τ′，利用机器人 URDF/网格模型和指定的相机参数，渲染出仅包含机器人本体运动的视频序列 r1:T。此步骤不涉及任何环境物体、纹理或背景，旨在提供一个干净、具身一致的锚定信号，完全避免了显式的环境建模。\n3.  **视频生成**：使用预训练的视频扩散模型（如 Cosmos-Predict2），以渲染的机器人运动视频 r1:T 和语言指令 l 为条件，合成完整的观察序列 o1:T。为使模型兼容此额外条件，将渲染的运动轨迹视频与初始化的噪声输入在通道维度上进行拼接，并相应扩展模型第一层的输入通道数。为支持多视角生成，不同视角的渲染帧会在空间上拼接后再输入模型。\n4.  **全局轨迹条件作用**：为了解决自回归生成长序列时可能出现的场景布局与未来运动不匹配问题（见图3），模型引入了全局轨迹条件。除了当前生成窗口的局部运动视频，模型还将整个轨迹 τ′ 及其在时间上的位置指示符 φ 作为额外的条件输入。这使模型能够感知未来的运动规划，确保合成的场景布局在长时程上保持连贯一致。\n\n![全局条件作用示例](https://arxiv.org/html/2512.11797v1/x2.png)\n> **图3**：缺失全局轨迹条件作用的影响。在没有全局条件的情况下，生成的碗（橙色高亮）被放置在一个视觉上合理但与机器人后续运动不一致的位置。真实碗的位置（绿色高亮）显示了苹果片最终被倒入的地方。这说明仅基于局部上下文的生成可能无法预知未来的运动。\n\n**核心创新点**：与现有方法相比，AnchorDream 的核心创新在于**轨迹与环境的解耦合成范式**。它不联合生成机器人和场景，也不依赖仿真器执行或显式场景重建来获取视觉观察。而是先通过启发式方法确定机器人轨迹并渲染其运动，再利用视频生成模型的强大先验，以运动为锚点“填充”出合理的环境。这从根本上避免了机器人形态的幻觉，并绕过了对逆动力学模型或精确仿真环境的依赖。\n\n## 实验与结果\n**实验设置**：在仿真环境中，使用 RoboCasa 基准测试，包含24个桌面任务，分为7种基础操作技能。每个任务有50条人类演示。在真实世界中，设计了6个日常操作任务，并使用单臂 PiPER 机器人收集了每个任务50条演示。评估分别在仿真（每个任务50次 rollout）和真实世界（每个任务20次 rollout）中进行。视频扩散模型基于 Cosmos-Predict2 2B 进行微调。策略学习在仿真中使用 BC-Transformer，在真实世界中使用 Diffusion Policy。\n\n**对比方法**：\n- **Human50**：仅使用每个任务50条原始人类演示。\n- **w/ MimicGen300**：使用 MimicGen 方法为每个任务生成300条新轨迹，并在仿真器中执行以获得配对观察（被视为依赖特权仿真访问的“理论上限”）。\n- **w/ AnchorDream300**：使用与 MimicGen 相同的300条新轨迹，但通过 AnchorDream 合成观察，而非仿真执行。\n- **DreamGen10K**：一种从文本和初始图像生成包含机器人的整个场景，再通过逆动力学提取动作的方法。\n\n**关键实验结果**：\n1.  **策略性能提升**：如表 I 所示，在 RoboCasa 上，仅使用 Human50 训练的策略平均成功率为 22.5%。加入300条 AnchorDream 合成的演示后，平均成功率提升至 30.7%，**相对提升 36.4%**。这一性能接近使用300条 MimicGen 仿真数据的结果（33.3%），表明 AnchorDream 能在不依赖显式环境建模和仿真执行的情况下，生成接近仿真器质量的高质量数据。\n\n![定性结果](https://arxiv.org/html/2512.11797v1/x3.png)\n> **图4**：RoboCasa 上的定性结果。对比输入的仅机器人运动视频、生成的演示以及真实场景。生成的演示保持了机器人具身性，同时生成了多样且视觉连贯的环境，物体摆放和交互与预期运动一致。\n\n2.  **数据缩放有效性**：如图5所示，在代表性任务上，随着 AnchorDream 生成数据量的增加，策略性能持续提升，证明了该方法在扩展数据规模以增强策略学习方面的有效性。\n3.  **消融实验**：如表 III 所示，移除全局轨迹条件或缩短推理窗口都会导致性能下降（分别降至 26.6% 和 28.1%），但仍优于仅使用 Human50，验证了 AnchorDream 的鲁棒性，并强调了全局条件对长时程一致性的重要性。\n4.  **真实世界迁移**：如表 IV 和图7所示，在真实机器人实验中，使用 Human50 训练的策略平均成功率为 28%，而加入 AnchorDream 生成的演示（每个任务约500条）后，平均成功率提升至 **63%**，性能接近翻倍。这验证了 AnchorDream 在真实场景中的实用性。\n\n![真实世界设置](https://arxiv.org/html/2512.11797v1/x5.png)\n> **图6**：真实世界评估设置。展示了六个日常操作任务以及用于数据收集和评估的 PiPER 机器人平台。\n\n![真实世界定性结果](https://arxiv.org/html/2512.11797v1/x6.png)\n> **图7**：真实机器人定性结果。展示了原始轨迹和多个增强变体对应的合成演示示例。生成的演示保持视觉真实感，同时增强的轨迹引导场景布局多样化，提供了比原始人类演示更大的训练数据可变性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 **AnchorDream**，一个具身感知的视频生成框架，通过将预训练视频扩散模型锚定在机器人运动上，来合成轨迹一致的演示。\n2.  提出了一种**解耦的轨迹-环境合成范式**，先确定性地扩展和渲染机器人轨迹，再生成环境，避免了显式场景建模，同时保证了具身一致性。\n3.  通过广泛的仿真和真实机器人实验验证了方法的有效性，证明其能从少量人类演示中扩展出大规模、高质量的数据集，显著提升下游模仿学习的性能。\n\n**局限性**：论文提到，轨迹扩展依赖于启发式方法，未来可探索更原则性的轨迹生成方式。此外，视频生成的计算成本较高。\n\n**对后续研究的启示**：AnchorDream 展示了将大规模生成模型的世界先验**以机器人运动为 grounding 信号**进行重用的可行路径。这为突破机器人学习的数据瓶颈提供了一种新思路：无需收集海量真实数据或构建高保真仿真，而是利用生成模型的内隐知识，通过“运动锚点”合成既多样又物理合理的训练数据。这一范式可能激励更多工作探索如何更好地将具身约束与生成先验相结合。",
      "imageUrls": [
        "https://arxiv.org/html/2512.11797v1/img/anchordream_cleaned.png",
        "https://arxiv.org/html/2512.11797v1/x1.png",
        "https://arxiv.org/html/2512.11797v1/x2.png",
        "https://arxiv.org/html/2512.11797v1/x3.png",
        "https://arxiv.org/html/2512.11797v1/x4.png",
        "https://arxiv.org/html/2512.11797v1/x5.png",
        "https://arxiv.org/html/2512.11797v1/x6.png",
        "https://arxiv.org/html/2512.11797v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.11275",
      "title": "Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing",
      "url": "http://arxiv.org/abs/2512.11275",
      "arxivId": "2512.11275",
      "date": "2025-12-12",
      "authors": "Daqiang Guo Team",
      "category": "Manipulation",
      "summary": "本文针对智能制造中基于视觉语言模型（VLM）的机器人操作助手，解决其因缺乏执行关键参数（如接触方式、轨迹、容差、力/阻抗）而导致在接触密集型任务中首次尝试易失败的问题。核心方法是提出一个以对象为中心的操作逻辑模式，形式化为八字段元组τ，将上述参数显式编码为可传递的知识信号，并构建一个支持训练时数据增强与测试时逻辑感知检索提示的双重用途知识库。在3D打印机线轴移除任务的协作单元中实例化了该模式与知识库，并采用适应VLM/LLM规划基准的指标分析了τ条件下的规划质量。",
      "detailedSummary": "## 研究背景与动机\n当前，基于视觉语言模型（VLM）的机器人操作流程优先考虑从图像和语言中获取广泛的语义泛化能力，但通常忽略了制造单元中接触密集型操作所需的关键执行参数。在智能制造中，失败常发生在接触点：外观驱动的通用策略缺乏接口机制、接触模式、轨迹整形、精度/公差带以及力/阻抗信息，导致机器人在首次尝试时经常错位、卡住、打滑或超出限制。现有方法（如VLM规划器、通用VLA、功能基网络等）在处理这些执行逻辑细节方面存在不同程度的缺失（如表1所示），通常将公差和交互动力学视为隐含信息，或将训练时标注与推理时检索解耦。本文针对这一痛点，提出将执行逻辑细节提升为VLM的一级知识信号，核心思路是定义一个以物体为中心、包含八个字段的操控逻辑模式τ，作为连接人类操作员、VLM助手和机器人控制器的知识基元，并在训练时和测试时分别利用该模式进行数据增强和检索增强提示。\n\n## 方法详解\n本文提出一个逻辑感知的系统，其核心是一个对象中心的操控逻辑模式τ，并将其作为贯穿训练和部署的一级知识信号。系统整体流程分为两个主要阶段：训练时，利用τ模式对数据集进行自动标注，以将τ知识注入VLM，同时构建包含结构化数据（如示意图、手册）和合成数据（带完整仿真日志）的知识库（KB）；运行时，任务查询通过匹配知识库生成τ指定的提示，进而驱动一个τ条件化的VLM规划器模块。\n\n![方法框架](https://arxiv.org/html/2512.11275v1/x1.png)\n> **图1**：所提出系统设计的流程图，突出了τ模式的两种用途。在训练时，数据集被自动标记以向VLM注入τ，同时也用解析的结构化数据和合成数据丰富了知识库。在运行时，任务查询通过知识库匹配被处理为τ指定的提示，从而提示τ条件化的VLM规划器模块。\n\n**核心模式详解**\n模式τ被序列化为一个八字段元组：τ = ⟨obj, iface, pre, contact, prim, traj, tol, dyn⟩。每个字段旨在编码对接触成功至关重要的执行变量：\n1.  **obj（对象类与部件几何）**：设备分类、目标部件标识符、显著几何特征。\n2.  **iface（接口机制）**：机制类别（按钮、旋钮、闩锁等）和操作模式（推/转/滑）；允许的自由度和驱动侧。\n3.  **pre（前提条件与状态）**：互锁、安全状态、工具存在、启用执行所需的顺序和防护条件。\n4.  **contact（接触模式与约束）**：间歇性与持续性接触、对齐、接近矢量以及任何夹具/顺应性假设。\n5.  **prim（运动基元与方向性）**：基元动词和方向（按压、拉动、滑动等）；适用的轴单位矢量和符号。\n6.  **traj（轨迹轮廓与时序）**：各阶段序列（如接近、啮合、斜坡、驻留、扫掠、撤回）及其参数和时间或事件条件。\n7.  **tol（精度与公差带）**：明确的SI单位表示的允许位姿和间隙带。\n8.  **dyn（力/阻抗与反馈线索）**：进一步分为数值目标/限制（dyn.num）和运行时触觉/视觉检查、成功谓词及中止条件（dyn.checks）。数值限制来源于规格书、日志或校准程序，而非从视觉推断。\n\n**系统与知识库**\n系统围绕一个轻量级知识库构建，该库通过 `(obj.class, obj.part, iface.mechanism)` 建立索引并返回序列化的τ。知识库的填充来源包括：解析手册和示意图、仪器化运行提供的状态和力/扭矩数据、通过防护校准发现的边界，以及从训练时标签升级的非数值字段（不包括dyn.num）。\n\n**训练时：带标签的数据增强**\n演示数据通过将VLM提议与规则模板配对，为所有非dyn字段自动生成简洁、模式有效的标签。这些标签通过连接令牌或辅助头与图像/状态融合，使模型学习从视觉特征到正确操控逻辑的映射，而非仅仅记忆名词。成功的标签（非数值字段加上有记录的dyn.checks）可作为知识库候选。至关重要的是，**不自动标注任何dyn字段**。dyn.num从同步的力/扭矩日志、扰动观测器估计或检索到的手册中注入；若未知，则在协作安全限制下通过受保护的、阻抗受限的校准来发现。\n\n**运行时：逻辑感知提示/检索**\n在部署时，系统查询知识库：感知模块产生 `(obj.class, obj.part, iface.mechanism)` 及其不确定性；检索器将其与知识库条目匹配；提示组合器向VLM提供任务上下文和完整的τ字段。规划器的计划应基于该元组，明确引用所有τ元素，并使用dyn.checks进行验证。控制器执行该逻辑以及traj/dyn信息，并进行在线检查。若知识库缺少数值交互参数，系统会在安全限制下执行受保护的扫描以发现有效操作范围，并将结果写回dyn.num供将来使用。\n\n**创新点**\n与现有方法相比，本文的创新具体体现在：1) **系统性显式化**：首次提出一个统一的、面向制造的八字段模式，将以往隐含或分散处理的执行参数（特别是公差tol和交互动力学dyn）提升为可检索、可检查的一级知识。2) **双阶段协同利用**：同一模式τ既用于训练时的结构化数据增强，又用于运行时的检索增强提示，确保了训练与测试在词汇上的一致性。3) **安全与校准集成**：强调dyn.num必须来自测量或校准，而非视觉猜测，并设计了运行时缺失时的安全发现机制，与协作机器人安全标准（如ISO10218-2）紧密结合。\n\n## 实验与结果\n本文在一个具体的3D打印机线轴移除与丢弃任务上实例化了τ模式和逻辑感知API，作为案例研究。实验未训练新模型，而是使用现成的多模态VLM（ChatGPT-4o），通过τ增强的图像-文本提示进行条件化，并使用第5节定义的指标来衡量计划质量。\n\n**实验设置**\n- **任务**：“从打印机上移除空的PLA线轴并丢弃在废料箱中”。\n- **基准方法**：对比两种提示条件：1) **Baseline (no-τ)**：仅提供自然语言指令、打印机和单元简要描述以及三张相机图像。2) **τ-anchored (logic-aware)**：在基线信息基础上，额外提供渲染后的τ字段段落，并明确指示模型遵守所有列出的前提条件和约束。\n- **评估指标**：将 `τ_spool_remove_discard` 作为参考规范，对采样的计划（每种条件N=10）进行评分，指标包括：步骤覆盖率（完整性）、顺序有效性、安全与约束覆盖率、接触与公差特异性、计划长度。\n\n![案例图示](https://arxiv.org/html/2512.11275v1/x2.png)\n> **图2**：线轴移除任务的交互元组 `τ_spool_remove_discard` 图示。通过不同颜色的掩码和箭头，分别实例化了obj（线轴）、iface/pre（铰链盖）、contact/prim（夹具与轮毂接触）以及traj（分阶段运动）等字段，直观展示了τ如何将任务逻辑具象化。\n\n**关键实验结果**\nτ锚定提示在所有计划质量指标上均带来了显著提升：\n- **步骤覆盖率**：从基线（无τ）的 65% 提升至 τ锚定 的 92%，表明计划更完整地涵盖了必要的操作步骤。\n- **顺序有效性**：从 40% 提升至 90%，说明计划更遵循正确的逻辑顺序。\n- **安全与约束覆盖率**：从 20% 提升至 80%，反映出计划更多地包含了热安全、速度限制等关键安全约束。\n- **接触与公差特异性**：从 10% 提升至 70%，显示计划对接触点、对齐和公差等细节的描述显著更具体。\n- **计划长度**：平均计划长度从 5.2 步增加至 8.1 步，这并非冗余，而是包含了更多必要的子步骤和检查，与更详细的τ规范保持一致。\n\n**消融实验与组件贡献**\n虽然论文未进行传统的模块消融实验，但通过对比“无τ”与“τ锚定”两种提示条件，直接验证了**引入τ模式作为知识信号对提升VLM计划质量的核心贡献**。实验结果表明，当VLM能访问明确的执行逻辑参数（如前提条件、精确的运动轨迹、公差和力限制）时，其生成的计划在完整性、安全性和可执行性方面均有质的飞跃。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出操控逻辑模式τ**：定义了一个面向智能制造、包含八个关键字段的对象中心模式，将执行细节（特别是公差和交互动力学）显式化、结构化，为VLM提供了连接语义与可执行行为的知识基元。\n2.  **构建双用途系统与知识库**：设计了一个逻辑感知系统，其中紧凑的τ知识库同时支持训练时的模式标记数据增强和测试时的检索增强条件提示，实现了知识在训练与部署间的统一流通。\n3.  **提供计划级评估证据**：通过3D打印机线轴移除的案例研究，首次使用标准计划质量指标量化了τ条件化提示对VLM规划行为的积极影响，为未来τ感知的VLM训练和硬件评估定义了指标方向。\n\n**局限性**：\n论文明确指出，当前工作是一个概念验证和案例研究。主要局限性包括：1) 知识库规模小，仅为手写构建；2) 实验仅在模拟/提示层面进行，未部署在真实机器人硬件上验证执行指标（如首次尝试成功率、力超限次数）；3) 检索机制目前是精确字典查找，更复杂的近似或基于图的检索留待未来。\n\n**对后续研究的启示**：\n本文为基于VLM的智能制造助手指明了一个重要方向：**超越外观和语义，关注执行逻辑**。后续研究可沿着以下路径展开：1) **大规模知识库构建**：自动化从大量制造文档、手册和仿真日志中解析和提取τ实例。2) **端到端训练与部署**：将τ集成到VLM的训练目标中，并在真实硬件上评估其带来的样本效率、泛化能力和任务成功率提升。3) **动态知识更新**：探索知识库如何通过在线交互、校准和操作员反馈进行持续学习和版本管理。这项工作为构建真正可靠、可解释且安全的智能制造协作机器人系统奠定了重要的知识表示基础。",
      "imageUrls": [
        "https://arxiv.org/html/2512.11275v1/x1.png",
        "https://arxiv.org/html/2512.11275v1/x2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.09406",
      "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
      "url": "http://arxiv.org/abs/2512.09406",
      "arxivId": "2512.09406",
      "date": "2025-12-10",
      "authors": "Mike Zheng Shou Team",
      "category": "Manipulation",
      "summary": "本文提出H2R-Grounder框架，旨在解决从人类交互视频学习机器人操作技能时面临的配对数据稀缺和视觉体现差距问题。核心方法采用无需配对数据的范式，通过H2Rep表示法：在训练中修复机器人视频背景并叠加夹持器位姿提示，基于上下文学习微调视频扩散模型（Wan 2.2），生成时将相同流程应用于人类视频以合成机器人操作视频。实验表明，该方法相比基线能生成显著更真实、物理基础更扎实的机器人运动视频，为利用无标注人类视频扩展机器人学习提供了可行路径。",
      "detailedSummary": "## 研究背景与动机\n从丰富的人类交互视频中学习机器人操作技能，可以避免繁琐的机器人数据收集，是机器人学习领域一个有前景的方向。然而，人类与机器人在外观和运动模式上存在显著的“具身鸿沟”，使得直接学习变得困难。现有方法主要分为两类：一类是基于渲染的方法，如Phantom、Masquerade和H2R，它们通过估计人手姿态，将渲染的机器人手臂合成到去除人类后的视频帧中。这类方法需要精确的相机-机器人标定和传感器参数，且渲染的机器人手臂往往忽略光照、深度和场景几何，导致物理上不一致的视觉效果，例如手臂“漂浮”或与物体错位。另一类方法依赖于成对的人类-机器人视频数据进行学习，这极大地限制了可扩展性。\n\n本文针对上述痛点，提出了一种无需配对数据的新范式。其核心思路是：通过一个可迁移的中间表示（H2Rep）来桥接人类与机器人领域，并利用在真实机器人视频上微调的大规模视频生成模型，将人类视频直接“翻译”成具有物理真实感的机器人操作视频。简而言之，本文旨在不依赖任何配对数据或精确标定的前提下，生成运动一致、物理真实的机器人视频。\n\n## 方法详解\nH2R-Grounder的整体流程分为三个阶段：1）从机器人视频数据集中构建训练数据；2）以上下文学习的方式微调视频生成模型；3）将野外人类视频转换为机器人操作视频。\n\n![方法总览](https://arxiv.org/html/2512.09406v1/x3.png)\n> **图3**：H2R-Grounder的总体范式。展示了从机器人视频构建训练数据、微调生成模型，以及最终将人类视频转换为机器人视频的三阶段流程。\n\n**核心模块与技术细节：**\n\n**1. 共享抽象表示 H2Rep：**\n该方法的核心是定义一个对人类和机器人视频都通用的中间表示H2Rep。它将视频解耦为两部分：a) 操作器（人手或机器人夹爪）的2D姿态轨迹（以带方向的红色圆点标记），承载动作语义；b) 去除操作器后的背景视频，保留场景布局和物体物理状态。通过将人手姿态与机器人夹爪姿态对齐，`姿态序列 + 背景` 成为了连接两个域的信息载体。\n\n**2. 从机器人视频构建训练数据：**\n- **机器人手臂分割**：使用基于文本提示的视频分割模型（Grounded-SAM2）获取机器人手臂的像素级掩码序列。\n- **夹爪姿态投影**：利用已知的相机内外参，将机器人末端执行器的6自由度轨迹投影到图像空间，并渲染成图形覆盖层（红点表示位置，蓝箭头表示方向）。\n- **机器人手臂移除**：使用视频修复模型（Minimax-Remover）根据掩码去除视频中的机器人手臂，得到干净的背景视频。\n- **组合H2Rep**：将渲染的姿态覆盖层以透明度α=0.4与修复后的背景视频进行混合，形成条件输入H2Rep。由此得到训练对 `(H2Rep, 原始机器人视频)`。\n\n![修复方法对比](https://arxiv.org/html/2512.09406v1/x4.png)\n> **图4**：在机器人手臂移除任务上，不同视频修复方法的对比。Minimax-Remover相比E2FGVI能更可靠地保留背景并移除手臂。\n\n**3. 基于上下文学习的物理真实机器人视频生成：**\n- **模型与训练**：采用预训练的视频扩散模型Wan 2.2作为主干生成器。采用上下文学习设计：将条件H2Rep和目标机器人视频分别通过VAE编码器编码为潜变量，并与文本嵌入（固定提示：“A robotic arm is interacting with objects.”）拼接，作为条件输入。仅对自注意力模块中的Q/K/V投影层进行LoRA微调，保持主干权重冻结。使用流匹配目标进行训练。\n- **关键优势**：模型从真实的机器人视频中学习，因此能够观察到正确的物理交互、接触和遮挡，从而鼓励生成物理上合理的视频。\n\n**4. 从人类视频到机器人视频的转换：**\n- **人类视频处理**：对于任意人类-物体交互视频，使用姿态估计模型（ViT-Pose + HaMeR）估计人手姿态，并将其简化为一个代表位置和方向的替代2D姿态。同时，分割并移除视频中的人物，得到背景视频。\n- **组合人类视频H2Rep**：以与机器人视频完全相同的方式，将人类姿态覆盖层与修复后的人物背景视频混合，得到人类视频的H2Rep。\n- **生成**：将训练好的机器人视频生成器直接应用于人类视频的H2Rep，即可生成对应的机器人操作视频。\n\n**创新点**：与现有方法相比，H2R-Grounder的创新主要体现在：1) **无需配对数据**：仅使用未配对的机器人视频进行训练；2) **完全生成式方法**：而非渲染合成，能建模从真实数据中学到的遮挡和接触；3) **免标定**：仅使用易于获取的2D姿态序列作为条件，避免了对精确3D标定的依赖；4) **上下文学习微调**：提升了时间一致性和对预训练知识的利用。\n\n## 实验与结果\n**实验设置：**\n- **训练数据集**：使用Droid数据集（约76K个第三人称Franka机械臂操作视频）。\n- **测试数据集**：\n    - **域内评估**：在Droid的保留集上，使用SSIM和LPIPS指标评估生成视频与真实视频的相似性。\n    - **域外评估**：使用DexYCB数据集（实验室环境，与Droid存在域偏移）和从互联网收集的多样化视频。\n- **Baseline方法**：\n    1. **RoboMaster**：一种基于动画的机器人图像到视频方法，被调整以适应H2R设置，需要大量手动标注（物体掩码、轨迹分段）。\n    2. **Kling & Runway Aleph**：商业视频编辑系统，通过提示词将视频中的人类替换为机器人图像。\n    （注：基于渲染的方法如Phantom因需要无法获取的标定参数而被排除比较。）\n- **评估指标**：人类主观研究（对运动一致性、背景一致性、视觉质量、物理合理性进行排名）、基于VLM（Gemini）的自动评分。\n\n**关键实验结果：**\n\n![域外H2R转换结果](https://arxiv.org/html/2512.09406v1/x5.png)\n> **图5**：在互联网视频（上排）和DexYCB视频（下排）上的定性结果。H2R-Grounder生成的机器人视频在背景一致性、运动对齐和视觉质量上均优于基线方法。\n\n**定量结果（DexYCB）：**\n- **人类研究**：如表1所示，在22名参与者的评估中，H2R-Grounder在**所有四个评估维度**上均获得了最高的首选率，尤其在视觉质量（61.4%）和物理合理性（63.6%）上优势明显。\n- **VLM评分**：如表2所示，H2R-Grounder在运动一致性（3.7）、背景一致性（4.9）和物理合理性（4.4）上得分最高或并列最高，证实了其生成视频在场景动力学和接触物理方面的优越性。\n\n**消融实验：**\n在Droid数据集上的定量消融结果（表3）和定性示例（图6）表明：\n- **移除姿态指示器**：导致SSIM下降（0.82→0.80），LPIPS上升（0.22→0.23），生成的手臂出现运动漂移，证明了姿态线索对于运动控制的必要性。\n- **移除LoRA微调**：模型严重过拟合，无法生成机器人手臂，导致质量显著下降（LPIPS升至0.26）。\n- **替换生成器**：使用ControlNet-based的VACE替代in-context学习的Wan模型，各项指标均变差，说明in-context学习对于保持运动-背景一致性更有效。\n- **扩大骨干网络**：从5B扩展到14B并未带来清晰的质量提升，反而大幅降低了推理效率。\n\n![消融实验](https://arxiv.org/html/2512.09406v1/x6.png)\n> **图6**：在Droid数据上的消融研究示例。从左至右分别为：真实视频、完整模型结果、移除姿态指示器结果、移除LoRA结果。可见移除关键组件会导致运动漂移或生成失败。\n\n## 总结与启发\n**核心贡献：**\n1. 提出了一种新颖的、无需配对数据的人类到机器人视频翻译框架 **H2R-Grounder**，能够从人类演示生成高质量的机器人视频。\n2. 设计了一种简单且可迁移的中间表示 **H2Rep**，通过解耦姿态序列和背景视频，统一了人类和机器人具身。\n3. 提出了一种针对大规模视频扩散模型的 **上下文学习微调方案**，利用其丰富的先验知识，实现了时间一致且物理真实的生成。\n\n**局限性：**\n论文明确指出，当前框架**仅支持从单手到单机械臂的转换**。扩展到双手/双臂场景需要相应的机器人数据，是未来的研究方向。此外，由于仅在Franka机械臂数据上训练，目前生成的机器人风格仅限于Franka。\n\n**启示：**\n这项工作为从海量无标签人类视频中规模化学习机器人技能指明了一条有前景的路径。其核心思想——通过一个免标定、可迁移的抽象表示来桥接不同领域，并利用强大生成模型学习领域特定的物理真实感——可能泛化到其他需要跨域转换或模拟的任务中。同时，该方法也凸显了高质量、多样化机器人数据集对于训练此类生成模型的重要性。",
      "imageUrls": [
        "https://arxiv.org/html/2512.09406v1/x1.png",
        "https://arxiv.org/html/2512.09406v1/x2.png",
        "https://arxiv.org/html/2512.09406v1/x3.png",
        "https://arxiv.org/html/2512.09406v1/x4.png",
        "https://arxiv.org/html/2512.09406v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.09283",
      "title": "UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects",
      "url": "http://arxiv.org/abs/2512.09283",
      "arxivId": "2512.09283",
      "date": "2025-12-10",
      "authors": "Shifeng Huang Team",
      "category": "Manipulation",
      "summary": "本文提出UPETrack框架，旨在解决被部分遮挡的可变形线性物体（DLO）实时跟踪难题。其核心是单向位置估计（UPE）算法，该算法利用DLO的几何连续性与时空演化规律，通过局部线性组合位移、近端线性约束和历史曲率三项机制，以闭式解直接估计被遮挡节点位置，无需迭代优化。实验表明，UPETrack在定位精度与计算效率上均优于TrackDLO和CDCPD2等先进方法。",
      "detailedSummary": "## 研究背景与动机\n可变形线性物体（DLOs）的实时状态跟踪对于工业装配、医疗手术和日常生活应用中的机器人操作至关重要。然而，高维构型空间、非线性动力学以及频繁的部分遮挡对鲁棒的实时DLO跟踪构成了根本性障碍。目前的主流方法主要基于非刚性配准，例如将相干点漂移（CPD）算法与全局-局部拓扑保持（GLTP）等几何约束结合，并嵌入期望最大化（EM）算法的迭代优化框架中。其他方法则通过物理仿真或粒子滤波器进行跟踪。这些现有方法普遍将几何约束纳入迭代优化框架，导致两个固有局限：计算复杂度增加和收敛稳定性降低。\n\n本文针对DLO跟踪在遮挡下因迭代优化带来的效率和稳定性问题，提出了一个新的视角：绕过迭代优化，直接利用DLO固有的时空几何特性解析地推导被遮挡节点的位置。其核心思路是提出一个名为UPETrack的几何驱动框架，该框架基于单向位置估计（UPE）算法，通过整合局部线性组合位移项、近端线性约束项和历史曲率项这三个几何分量，以闭式解的形式高效、稳定地估计被遮挡节点的位置。\n\n## 方法详解\nUPETrack框架将DLO形状表示为一组有序节点。其整体流程分为两个阶段：第一阶段基于高斯混合模型（GMM）和期望最大化（EM）算法跟踪可见片段；第二阶段则采用提出的UPE算法预测遮挡区域的位置。输入是当前时间步t通过深度传感器获取并经过简单颜色阈值分割得到的部分观测点云 X^t，输出是代表完整DLO形状的M个状态节点 Y^t。\n\n![方法框架](https://arxiv.org/html/2512.09283v1/x2.png)\n> **图2**：UPETrack整体框架。算法将当前DLO点云与先前的时序节点结合，获取可见段，然后采用单向位置估计（UPE）进行被遮挡节点定位，最后通过均匀重采样生成最终状态估计。\n\n**核心模块1：基于GMM的形状估计与可见性判定**\n该模块首先判定每个状态节点 y_m^t 是否可见。判定依据是以上一帧节点位置 y_m^{t-1} 为中心、半径为 r_vis 的球体内，当前观测点 X^t 中的点数 q 是否超过阈值 V_lim。若 q ≥ V_lim，则节点可见，否则被标记为遮挡。\n\n![可见性判定](https://arxiv.org/html/2512.09283v1/x3.png)\n> **图3**：节点可见性判定示意图。算法通过统计上一帧节点位置 y_m^{t-1} 半径 r_vis 内的邻点数量 q 是否超过阈值 V_lim 来标记当前节点 y_m^t 为可见或遮挡。\n\n对于可见节点，UPETrack通过将上一帧所有节点 Y^{t-1} 作为高斯混合模型（GMM）的初始均值，与当前观测 X^t 进行配准来更新其位置。GMM将每个观测点视为由M个高斯分量（对应M个节点）和一个均匀分布（用于处理噪声和离群点）生成的混合样本。通过EM算法迭代优化节点位置 Y^t 和方差 σ^2，以最大化数据似然。\n\n**核心模块2：单向位置估计（UPE）算法**\nUPE算法专门用于估计被遮挡节点的位置，其核心思想是利用DLO的几何连续性和时序演化模式。算法操作分为两步：首先沿DLO从遮挡端开始搜索，定位到最近的一串连续可见节点；然后利用这串节点的信息，反方向（计算方向）迭代估计最近的被遮挡节点，并将新估计出的节点视为“可见”用于后续估计，直至所有遮挡节点被估计完毕。\n\n![UPE操作流程](https://arxiv.org/html/2512.09283v1/x4.png)\n> **图4**：针对末端遮挡场景，UPE从被遮挡端开始遍历，直到定位到一串连续可见节点，然后反向利用这些连续可见节点迭代估计最近的被遮挡节点。\n\nUPE算法通过线性组合三个几何项来直接计算被遮挡节点 y_m^t 的位置，形成一个闭式解：\n1.  **局部线性组合位移项**：假设被遮挡节点的位移与其邻近可见节点的位移强相关，且相关性随距离衰减。该位移 Δ_m 被建模为连续可见节点位移的距离加权和（使用衰减因子 γ）。\n    ![局部线性组合位移项](https://arxiv.org/html/2512.09283v1/x5.png)\n    > **图5**：局部线性组合位移项示意图。被遮挡节点 y_m^{t-1} 的位移 Δ_m 表示为连续可见节点位移的距离加权和。\n2.  **近端线性约束项**：基于DLO的弯曲刚度，假设相邻节点间倾向于保持直线。该约束项 y_m^{t*} 由最近的两个可见节点定义的线段方向和外推长度（包含弯曲阻力系数 b）计算得出。\n3.  **历史曲率项**：基于DLO形态在连续帧间变化较小的假设，将上一帧中局部相邻节点构成的弯曲向量 Δ_m^{his} 直接用于当前帧。\n\n最终，被遮挡节点的位置由前两个项的结果加权平均得到：y_m^t = α * (y_m^{t-1} + Δ_m) + (1-α) * (y_m^{t*} + a * Δ_m^{his})，其中 α 是权重系数，a 控制历史曲率的保留程度。\n\n![近端约束与历史曲率](https://arxiv.org/html/2512.09283v1/x6.png)\n> **图6**：近端线性约束项与历史曲率项示意图。近端约束项 y_m^{t*} 由两个最近可见节点定义的线段投影得到；最终位置 y_m^{t*} 通过将该约束项与来自先前状态的历史弯曲项 Δ_m^{his} 线性叠加得到。\n\n**核心模块3：节点距离均匀化**\n由于跟踪和估计过程可能导致节点间距不均，UPETrack在每轮迭代后采用基于测地线距离的重采样策略对节点进行均匀化。首先计算整个DLO基于节点顺序的折线总长L，然后确定目标均匀间距 l = L/(M-1)。最后，根据每个节点在折线上的累积测地线距离，通过线性插值生成新的、间距均匀的状态节点集合 Y。\n\n与现有方法相比，UPETrack的核心创新在于将几何约束（空间连续性和时间规律性）直接编码为一个可解析求解的线性组合形式，从而在遮挡推理阶段完全避免了迭代优化，这是其提升效率和稳定性的关键。\n\n## 实验与结果\n实验使用RealSense D435i设备采集的真实场景视频和点云数据。对比的基线方法是当前最先进的两种算法：TrackDLO和CDCPD2。评估场景包括两种：“S”形静态遮挡和动态遮挡。评估指标是算法检测到的DLO状态位置与真实位置（以DLO上的红蓝标记为参考）之间的帧误差，以及计算效率。\n\n![算法对比](https://arxiv.org/html/2512.09283v1/x8.png)\n> **图8**：在“S”形静态遮挡和动态遮挡两种实验场景下，对UPETrack、CDCPD2和TrackDLO的检测精度与效率进行比较分析。\n\n![帧误差对比](https://arxiv.org/html/2512.09283v1/x9.png)\n> **图9**：(a)-(b)分别展示了三种算法在两种实验场景下的帧误差。\n\n关键实验结果如下：\n1.  **跟踪精度**：在“S”形静态遮挡场景（场景I）中，UPETrack的平均帧误差为4.21像素，显著低于CDCPD2的6.07像素和TrackDLO的7.82像素。在动态遮挡场景（场景II）中，UPETrack的平均帧误差为4.35像素，同样优于CDCPD2的5.71像素和TrackDLO的6.53像素。\n2.  **计算效率**：UPETrack表现出最高的计算效率。在场景I中，其单帧平均处理时间为0.0053秒（约189 FPS），远快于CDCPD2的0.0185秒（54 FPS）和TrackDLO的0.0476秒（21 FPS）。在场景II中，UPETrack的平均处理时间为0.0067秒（149 FPS），也明显快于另外两种方法。\n\n论文通过消融实验验证了UPE算法中各个组件的贡献。结果表明，结合了局部线性组合位移项、近端线性约束项和历史曲率项的完整UPE算法，相比仅使用其中部分项，能够取得最佳的整体跟踪精度。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  提出了UPETrack框架，能够在无需物理建模、仿真或视觉标记的情况下，实现对部分遮挡DLO的准确实时跟踪。\n2.  提出了创新的单向位置估计（UPE）算法，该算法利用DLO固有的时空几何特性（空间连续性和时间形变规律），将其表述为解析约束来指导被遮挡节点的状态估计。\n3.  为上述时空约束开发了一种闭式解，能够直接计算被遮挡节点的位置。这种解析计算绕过了迭代优化，从而提高了跟踪稳定性和实时计算效率。\n\n论文自身未明确阐述局限性，但基于其方法原理，可推测其性能可能依赖于参数（如γ, a, b, α）的调优，并且对于极端剧烈、快速的形变或非常复杂的遮挡模式，其基于局部几何连续性和时序平滑性的假设可能会受到挑战。\n\n本文对后续研究的启示在于：为解决复杂状态估计问题提供了一种“去迭代优化”的新思路，即通过深入挖掘问题本身的物理或几何规律，将其构造为可解析求解的模型，从而在保证精度的前提下大幅提升计算效率。这为机器人实时感知与决策系统的设计提供了有价值的参考。",
      "imageUrls": [
        "https://arxiv.org/html/2512.09283v1/x1.png",
        "https://arxiv.org/html/2512.09283v1/x2.png",
        "https://arxiv.org/html/2512.09283v1/x3.png",
        "https://arxiv.org/html/2512.09283v1/x4.png",
        "https://arxiv.org/html/2512.09283v1/x5.png",
        "https://arxiv.org/html/2512.09283v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.09851",
      "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
      "url": "http://arxiv.org/abs/2512.09851",
      "arxivId": "2512.09851",
      "date": "2025-12-10",
      "authors": "Yixin Zhu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中多模态感知与学习框架融合的挑战，提出了一种同步触觉-视觉感知系统。核心问题是现有透皮（STS）传感器无法实现同步多模态感知且触觉跟踪不可靠。作者提出了TacThru传感器（采用全透明弹性体与关键线标记）和TacThru-UMI模仿学习框架（基于Transformer的扩散策略）。在五项真实世界任务实验中，该系统平均成功率高达85.5%，显著优于仅触觉（66.3%）和仅视觉（55.4%）的基线方法。",
      "detailedSummary": "## 研究背景与动机\n机器人操作需要丰富的多模态感知和有效的学习框架来处理复杂的现实世界任务。结合了触觉和视觉感知的“透视皮肤”传感器具备有前景的感知能力，而现代模仿学习为策略获取提供了强大工具。然而，现有的STS设计缺乏同步的多模态感知，并且存在触觉跟踪不可靠的问题。此外，将这些丰富的多模态信号集成到基于学习的操作流程中仍然是一个开放的挑战。本文的核心思路是：提出一种能够实现同步视觉感知和鲁棒触觉信号提取的STS传感器TacThru，并构建一个利用这些多模态信号进行操作的模仿学习框架TacThru-UMI。\n\n## 方法详解\n本文提出的系统包含两个核心部分：新型STS传感器TacThru和基于模仿学习的操作框架TacThru-UMI。\n\n![TacThru硬件与集成](https://arxiv.org/html/2512.09851v2/fig_hardware_v2_c)\n\n> **图1**: TacThru的制造与在TacThru-UMI中的集成。(a) 通过在透明弹性体上依次喷涂内部（黑色）和外部（白色）标记来制造关键线标记。(b-c) TacThru-UMI包括一个机器人末端执行器（左）和一个数据收集器（中），两者具有相同的本体和TacThru手指。末端执行器的手指由伺服电动缸驱动（右）。\n\n**TacThru传感器设计**：TacThru通过三个设计原则实现同步触觉-视觉感知：1) **完全透明的弹性体**以实现清晰的视觉访问；2) **持续照明**以消除模式切换；3) **鲁棒的关键线标记**以实现可靠的触觉跟踪。其设计与标准VBTS制造流程兼容，主要区别在于弹性体材料。\n\n**核心模块与技术细节**：\n1.  **透明弹性体与持续照明**：采用完全透明的弹性体和持续的LED照明（24颗LED阵列），放弃了深度感知，换取了连续的视觉感知。接触检测通过两种机制实现：接触界面处的光反射变化以及弹性体变形导致的标记偏移。\n2.  **关键线标记**：为解决透明背景下传统实心标记难以检测或易受环境噪声干扰的问题，设计了由对比色同心圆组成的“关键线标记”。内圆（黑色，半径0.6mm）和外圆（白色，半径1.0mm）确保内边缘在任何背景下都作为可检测的“关键线”可见。在40mm x 40mm的弹性体上部署了64个标记，间距3.5mm。\n3.  **鲁棒高效的标记跟踪**：采用卡尔曼滤波跟踪每个标记的状态（图像中的位置）。系统模型采用随机游走模型，观测模型为直接位置观测。测量获取流程包括：图像灰度化、基于阈值τ_i的二值化、斑点检测以及数据关联（将最近的斑点匹配为测量值）。为防止极端情况下的误匹配，强制执行标记运动连续性约束：如果帧间位移超过阈值τ_z，则拒绝该测量，保持状态估计不变。参数经校准和经验调优确定（σ_v=0.42像素，σ_w=0.11像素，τ_i=0.784，τ_z=10像素）。\n\n![关键线标记设计与跟踪评估](https://arxiv.org/html/2512.09851v2/fig_markers_c)\n\n> **图2**: 关键线标记设计和滤波实现了鲁棒的跟踪。(a) 在抓取瓶子过程中比较两种标记类型（关键线 vs. 实心标记）的评估设置。(b) TacThru视图比较显示，关键线标记（左）在复杂背景下仍保持 distinct，而实心标记（右）变得不可见。(c) 定量结果表明，经过滤波的关键线方法能稳定跟踪全部64个标记，同时保持高效率（6.08ms处理时间），且滤波步骤拒绝了误报。\n\n![时间消耗分析](https://arxiv.org/html/2512.09851v2/x1.png)\n\n> **图3**: TacThru在不同频率下的运行时间消耗。测试了15、30、60和120Hz，并报告了每个计算步骤的时间消耗。误差条上方的文字：迭代时间消耗（蓝色）和等效FPS（黄色）。FPS的小幅超调是由于板载振荡器实际频率和UVC相机的缓冲策略。\n\n**TacThru-UMI学习框架**：该框架基于UMI和扩散策略，扩展了多模态触觉-视觉观测。数据收集器适配了UMI设计，用STS传感器替换标准手指。策略采用基于Transformer的扩散策略，学习从多模态观测到机器人动作的映射。\n\n![策略架构](https://arxiv.org/html/2512.09851v2/fig_model_c)\n\n> **图4**: 策略架构。多模态观测（手腕相机图像、传感器图像、标记偏移和本体感知）被编码为令牌，并用于条件化一个基于Transformer的扩散策略，该策略将高斯噪声去噪为机器人动作块。\n\n**策略输入与编码**：在时间步t，观测包括手腕相机帧序列、传感器帧序列、标记偏移序列和本体感知序列。视觉观测使用DINOv2编码（手腕相机用ViT-B，TacThru帧用ViT-S）。标记偏移和本体感知使用专用的MLP编码。每种模态都接收可学习的嵌入以实现Transformer的可区分性。拼接后的令牌与位置嵌入一起，用于条件化扩散策略π_θ，该策略将高斯样本去噪为动作块。每个动作包括相对末端执行器姿态和夹持器宽度目标。\n\n## 实验与结果\n**实验设置**：在五个不同的真实世界操作任务上评估TacThru-UMI，涵盖抓放、分类和插入场景（PickBottle, PullTissue, SortBolt, HangScissors, InsertCap）。为公平比较，在夹持器的一个手指上装备TacThru，另一个手指上装备GelSight型传感器，确保不同模态变体使用相同的训练轨迹。每个任务收集62-147次演示。\n\n**对比基线**：训练了四种策略变体进行消融和比较，所有策略均包含手腕相机和本体感知作为基础输入：\n*   **TT-M**: TacThru图像和标记偏移。\n*   **TT**: 仅TacThru图像（标记可见但未显式跟踪提供）。\n*   **GS-M**: GelSight图像（经空闲图像校正以隔离接触）和标记偏移（触觉基线）。\n*   **Wrist**: 仅手腕相机（视觉基线）。\n\n![任务设置](https://arxiv.org/html/2512.09851v2/fig_task_settings_c)\n\n> **图5**: 五个操作场景的任务演示。(a) PickBottle: 基本抓放，(b) PullTissue: 薄软物体操作，(c) SortBolt: 视觉辨别，(d) HangScissors: 触觉辨别，(e) InsertCap: 多模态融合。顶部：初始物体配置。中部：对应的手腕相机视图。底部：对应的TacThru和GelSight图像。\n\n![定量结果](https://arxiv.org/html/2512.09851v2/x2.png)\n\n> **图6**: 跨操作任务和感知模态的定量结果。TT-M: 带标记的TacThru，TT: 仅TacThru图像，GS-M: 带标记的GelSight，Wrist: 仅视觉。\n\n**关键实验结果**：\n*   **整体性能**：TacThru-UMI（TT-M）在五个任务上的平均成功率达到**85.5%**，显著优于触觉基线GS-M（**66.3%**）和视觉基线Wrist（**55.4%**），性能分别是后两者的**1.29倍**和**1.54倍**。\n*   **任务分析**：\n    *   **PickBottle**：所有策略均达到≥95%的成功率，验证了系统集成多模态信号而不损害基本操作的能力。\n    *   **PullTissue**：TT-M策略成功率为**85%**，而GS-M和Wrist策略分别仅为**15%**和**10%**。这凸显了TacThru通过指尖直接视觉观察处理薄软物体的优势。\n    *   **SortBolt**：TT-M策略成功率为**87.5%**，显著高于GS-M（**54.2%**）和Wrist（**20.8%**）。TacThru的同步感知能够区分颜色和形状，而触觉无法区分颜色，全局视觉无法分辨小螺栓细节。\n    *   **HangScissors**：TT-M和GS-M策略成功率相近（**95%** vs. **90%**），而Wrist策略仅为**65%**。这表明在需要触觉反馈确认悬挂成功的任务中，触觉信息至关重要。\n    *   **InsertCap**：TT-M策略成功率达到**100%**，而GS-M和Wrist策略分别为**40%**和**45%**。TacThru的视觉感知能力实现了对瓶盖和安装座的直接视觉伺服对齐。\n\n![定性策略执行过程](https://arxiv.org/html/2512.09851v2/fig_res_c)\n\n> **图7**: 代表性的策略执行过程。每列(a-f)显示了一个任务执行的时间进程（从上到下），并配有同步视图：第三人称视图（左）、手腕相机（中）、以及叠加了标记偏移的TacThru视图（右；放大4倍以增强可见性）。标注突出了关键操作阶段和感知反馈。\n\n**消融实验分析**：\n1.  **标记与滤波的重要性**（图2c）：关键线标记设计（Keyline）相比实心标记（Solid）大幅提升了可检测性。进一步的卡尔曼滤波（Keyline, filtered）有效拒绝了环境噪声引起的误报，实现了全部64个标记的稳定跟踪，且处理延迟仅6.08ms。\n2.  **多模态输入的有效性**（图6）：对比TT-M和TT策略，在PullTissue、SortBolt和InsertCap任务中，显式提供标记偏移（TT-M）比仅提供图像（TT）带来了显著的性能提升，表明**同时利用图像外观信息和触觉的几何/力信息**对于精细操作至关重要。\n3.  **同步感知的优势**：在多个任务中，TT-M策略的表现全面优于仅依赖触觉（GS-M）或仅依赖视觉（Wrist）的基线，证明了**触觉与视觉信号的同步提供**能够互补各自的局限性，实现更鲁棒和精确的操作。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**TacThru**，一种新颖的STS传感器，通过完全透明弹性体、持续照明和鲁棒的关键线标记设计，实现了高效、鲁棒的**同步触觉-视觉感知**，无需模式切换。\n2.  开发了**TacThru-UMI**，一个将TacThru集成到基于UMI的模仿学习框架中的系统，利用基于Transformer的扩散策略动态关注同步的多模态信号，实现了数据驱动的高性能操作策略。\n3.  通过**全面的实验验证**，在五个具有挑战性的真实世界任务上证明了TacThru的同步多模态感知能够实现优于传统单模态方法的精细化和接触丰富的操作，平均成功率显著提升。\n\n**局限性**：TacThru的设计明确优先考虑全局接触状态和指尖视觉上下文，而非精细的表面几何重建，因此**牺牲了深度感知能力**。论文认为这种权衡对于实现鲁棒的接触丰富操作是值得的。\n\n**对后续研究的启示**：\n1.  本文展示了将物理传感器创新（同步感知）与先进学习框架（扩散策略）紧密结合的潜力，为未来机器人感知-学习一体化研究提供了范例。\n2.  关键线标记和高效跟踪算法为解决透明弹性体背景下触觉标记的鲁棒检测问题提供了新思路。\n3.  实验表明，在多模态学习中，如何有效编码和融合不同模态（如图像外观与标记偏移的几何信息）对性能有重要影响，这值得进一步探索。\n4.  该系统在薄软物体操作、需要融合视觉与触觉信息的辨别任务以及精密插入任务中表现突出，为这些特定难点场景提供了有效的解决方案。",
      "imageUrls": [
        "https://arxiv.org/html/2512.09851v2/fig_hardware_v2_c",
        "https://arxiv.org/html/2512.09851v2/fig_markers_c",
        "https://arxiv.org/html/2512.09851v2/x1.png",
        "https://arxiv.org/html/2512.09851v2/fig_model_c",
        "https://arxiv.org/html/2512.09851v2/fig_task_settings_c",
        "https://arxiv.org/html/2512.09851v2/x2.png",
        "https://arxiv.org/html/2512.09851v2/fig_res_c",
        "https://arxiv.org/html/2512.09851v2/x3.png",
        "https://arxiv.org/html/2512.09851v2/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.09297",
      "title": "One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation",
      "url": "http://arxiv.org/abs/2512.09297",
      "arxivId": "2512.09297",
      "date": "2025-12-10",
      "authors": "Kui Jia Team",
      "category": "Manipulation",
      "summary": "本文针对双臂操作中大规模高质量演示数据收集的瓶颈问题，提出BiDemoSyn框架。该方法将任务分解为不变协调块和可变对象调整，通过视觉引导对齐与轻量级轨迹优化，从单个真实演示合成数千个多样且物理可行的演示。在六个双臂任务上的实验表明，基于合成数据训练的策略能鲁棒泛化到新对象姿态和形状，性能显著优于强基线，并实现零样本跨机器人平台迁移。",
      "detailedSummary": "## 研究背景与动机\n当前机器人灵巧双手操作策略的学习严重依赖于大规模、高质量的演示数据。现有数据获取方法面临根本性的权衡：遥操作能提供物理上真实的数据，但劳动强度极高，难以扩展；而基于仿真的合成方法效率高、可扩展，但不可避免地存在仿真到现实的差距。具体而言，像ALOHA系列这样的遥操作系统数据质量高，但需要大量专家人力；而MimicGen、RoboGen等方法通过程序化生成数据，却因接触动力学和视觉渲染的差异导致现实部署效果不佳。本文针对在保持物理真实性的前提下，规模化获取双手操作演示数据这一痛点，提出了一种全新的视角：从单个真实世界演示示例出发，通过算法合成多样化的演示。其核心思路是将任务分解为不变的协调模块和可变的、依赖于物体的调整部分，然后通过视觉引导的对齐和轻量级轨迹优化进行适配，从而在现实世界中高效生成大量物理可行的演示。\n\n## 方法详解\nBiDemoSyn的整体框架是一个三阶段流水线，旨在从单个真实演示 `τ` 合成大规模数据集 `𝒟_syn`。输入是单个演示轨迹 `τ = {o_t, a_t}`，输出是数百条适应新场景（如物体位姿、形状变化）且物理可行的合成轨迹。\n\n![方法整体框架](https://arxiv.org/html/2512.09297v2/x1.png)\n> **图1**：BiDemoSyn方法整体框架。包含三个阶段：1) 解构：将输入的单次演示分解为执行块并分类；2) 对齐：通过视觉感知适应新物体几何；3) 优化：进行碰撞感知验证和运动调整，最终重组生成合成演示。\n\n**第一阶段：单次教学解构**\n给定演示 `τ`，首先将其分解为一系列双手执行块 `{ℬ_i}`，每个块代表一个离散的双臂交互阶段。根据运动模式，块被分类为：单臂运动（一臂显著运动，另一臂静止）或双臂协调（双臂同步或异步调整）。接着，每个块被进一步细化为**原子执行原语**，即手臂经历显著状态转换（如抓取器开合、移动）的最小运动单元。最后，对这些块进行语义分类：**不变块**编码任务语义原语（如拧螺丝、按压），其结构在不同实例间保持一致；**可变块**（如依赖于物体的抓取）则根据新物体的几何变化进行调整 `ℬ_i’ = ℬ_i ∘ Φ(g, o_novel)`，其中 `Φ` 是视觉适配器。\n\n**第二阶段：基于视觉的初始帧对齐**\n此阶段通过视觉适配器 `Φ` 将可变块中的物体交互推广到新场景。具体包含三步：1) **物体感知**：使用开放词汇检测器或视觉基础模型（如Florence2+SAM2）从新观测 `o_novel` 中分割出目标物体掩码 `M`。2) **状态估计**：采用经典图像矩方法计算掩码 `M` 的3D质心 `c`，并使用PCA拟合3D点云确定物体方向 `R`，从而得到实例级的6D物体位姿 `P_obj = (R, c)`。3) **位姿对齐**：计算原始演示中物体位姿 `P_demo` 到新位姿 `P_obj` 的刚性变换 `T`，并将其应用于原始抓取位姿 `P_grasp,demo`，得到适应新场景的抓取位姿 `P_grasp,novel = T · P_grasp,demo`。\n\n![视觉对齐示意图](https://arxiv.org/html/2512.09297v2/x2.png)\n> **图2**：视觉初始帧对齐在倾倒（左、中）和重定向（右）任务中的应用示意图。展示了在操纵物体的位置、朝向和形状改变后，能够自动调整抓取位姿。\n\n**第三阶段：轨迹调制与优化**\n对齐后的轨迹需要进一步优化以确保可执行性：1) **碰撞感知验证**：首先通过逆运动学验证目标位姿 `P_grasp,novel` 的可达性；其次，使用运动规划器检查每个块起点和终点之间是否存在碰撞，假设原始演示轨迹是无碰撞的。2) **实例级运动适应**：为处理不同形状的物体，根据新旧物体3D包围盒的尺寸差异 `(Δl, Δw, Δh)`，对可变块中的运动端点（如抓取或释放位置）进行偏移调整 `s_i,e^A’ = s_i,e^A + λ(Δl, Δw, Δh)`。最后，将经过验证和调整的块与不变块重组，形成最终的合成轨迹 `τ’`。\n\n**创新点**\n与现有方法相比，BiDemoSyn的核心创新在于：1) **完全在现实世界操作**：不同于MimicGen等仿真合成方法，BiDemoSyn全程在物理域进行，从根本上避免了sim-to-real差距，确保合成数据继承人类演示的物理保真度。2) **层次化分解与适配**：通过将任务分解为不变和可变部分，实现了有针对性的多样化合成，既保持了任务语义，又高效适应了几何变化。3) **轻量级优化**：在块级别进行碰撞验证和运动调整，而非整条轨迹重规划，兼顾了效率与安全性。\n\n## 实验与结果\n**实验设置**：在六个需要密集接触协调的双手操作任务上进行评估：插笔、插入、拧开、倾倒、按压和重定向。硬件平台主要为配备平行夹爪的固定基座双臂机器人，并使用另一款人形双臂平台验证跨平台迁移能力。感知由立体相机提供。\n\n**基线方法**：对比方法分为两类。一类是**数据收集方法**，包括点云编辑（DemoGen）、机器人自动执行（YOTO）和遥操作。另一类是**无需重新训练的直接操作方法**，包括零样本方法ReKep、其改进版ReKep+，以及一次性模仿学习方法ODIL和MAGIC。收集数据后，使用三种先进的视觉运动策略（DP、DP3、EquiBot）进行训练和评估。\n\n![数据收集效率与质量对比](https://arxiv.org/html/2512.09297v2/x3.png)\n> **图3**：(A) 不同基线方法与BiDemoSyn的数据收集效率对比。(B) DemoGen生成的数据质量示意图。DemoGen合成效率最高，但无法避免视角变换导致的视觉伪影，数据质量最低。本文方法在速度和质量间取得了良好平衡。\n\n**关键实验结果**：\n1.  **合成效率与质量**：如图3所示，BiDemoSyn能在数小时内生成数千条演示，效率显著高于遥操作，同时数据质量（视觉真实性）远高于基于点云编辑的DemoGen和YOTO。\n2.  **策略性能**：表I展示了主要定量结果。在分布内测试中，使用BiDemoSyn数据训练的DP3和EquiBot策略平均成功率分别达到81.1%和86.7%，显著优于所有基线。在分布外测试（使用训练集未出现的物体位姿或形状）中，BiDemoSyn的优势更加明显（DP3: 54.4%， EquiBot: 66.7%），而DemoGen和YOTO的性能大幅下降（如DP3下DemoGen仅30.0%），表明BiDemoSyn合成的数据具有更好的泛化性。\n\n![训练规模与成功率关系](https://arxiv.org/html/2512.09297v2/x4.png)\n> **图4**：训练数据规模与成功率的关系对比。BiDemoSyn仅需约1000条合成数据即可达到与数千条遥操作数据相当的性能，证明了其卓越的数据效率。\n\n3.  **数据效率**：如图4所示，使用BiDemoSyn合成数据训练的策略，仅需约1000条数据就能达到与使用数千条遥操作数据训练相近的性能，证明了其出色的数据效率。\n4.  **消融实验**：论文通过消融实验验证了各组件贡献。移除视觉对齐模块会导致在物体位姿变化时性能急剧下降；移除轨迹优化则会导致碰撞和不可行轨迹增加，降低成功率。完整模型在所有任务上均取得最佳性能。\n5.  **零样本跨平台迁移**：得益于物体中心的观测和简化的6自由度末端执行器动作表示（将策略与平台特定动力学解耦），在主要平台上训练的BiDemoSyn策略，能够零样本迁移到新的人形双臂平台上并保持高成功率（如图8等后续图示）。\n\n## 总结与启发\n**核心贡献**：1) 提出了一个系统性的**一次性合成框架**，通过任务解构、视觉引导适配和接触感知轨迹优化的组合，实现了可扩展的现实世界双手演示生成。2) 实现了**完全基于现实世界的数据生成**，不依赖仿真器，从构造上确保了物理保真度。3) 在复杂的双手操作任务上进行了全面的**实证验证**，证明了所提方法在策略鲁棒性、跨配置泛化以及数据效率方面的显著优势。\n\n**局限性**：论文提到，对于高度非刚性物体（如绳索）或需要复杂动态交互（如抛接）的任务，当前的几何对齐和准静态运动假设可能不足，未来需要探索更高级的物理推理。\n\n**研究启示**：BiDemoSyn为复杂的双手操作模仿学习提供了一条兼顾效率与真实性的可扩展路径。其“分解-适配”的思想可推广至其他数据稀缺的机器人学习场景。此外，物体中心的表示和简化的动作空间设计，为实现策略的跨平台迁移提供了有益参考。",
      "imageUrls": [
        "https://arxiv.org/html/2512.09297v2/x1.png",
        "https://arxiv.org/html/2512.09297v2/x2.png",
        "https://arxiv.org/html/2512.09297v2/x3.png",
        "https://arxiv.org/html/2512.09297v2/x4.png",
        "https://arxiv.org/html/2512.09297v2/x5.png",
        "https://arxiv.org/html/2512.09297v2/x6.png",
        "https://arxiv.org/html/2512.09297v2/x7.png",
        "https://arxiv.org/html/2512.09297v2/x8.png",
        "https://arxiv.org/html/2512.09297v2/x9.png",
        "https://arxiv.org/html/2512.09297v2/x10.png",
        "https://arxiv.org/html/2512.09297v2/x11.png",
        "https://arxiv.org/html/2512.09297v2/x12.png",
        "https://arxiv.org/html/2512.09297v2/x13.png",
        "https://arxiv.org/html/2512.09297v2/x14.png",
        "https://arxiv.org/html/2512.09297v2/x15.png",
        "https://arxiv.org/html/2512.09297v2/x16.png",
        "https://arxiv.org/html/2512.09297v2/x17.png",
        "https://arxiv.org/html/2512.09297v2/x18.png",
        "https://arxiv.org/html/2512.09297v2/x19.png",
        "https://arxiv.org/html/2512.09297v2/x20.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.09510",
      "title": "ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics",
      "url": "http://arxiv.org/abs/2512.09510",
      "arxivId": "2512.09510",
      "date": "2025-12-10",
      "authors": "Paolo Roberto Massenio Team",
      "category": "Manipulation",
      "summary": "本文解决机器人箱拣选中因遮挡导致的amodal分割问题，以提升抓取规划准确性。提出ViTA-Seg框架，基于Vision Transformer，利用全局注意力恢复完整对象掩码，包括Single-Head（预测amodal掩码）和Dual-Head（同时预测amodal和遮挡掩码）两种架构，并引入ViTA-SimData合成数据集。实验表明，ViTA-Seg Dual Head在COOCA和KINS基准测试上实现了强大的amodal和遮挡分割精度，且计算高效，支持实时机器人操作。",
      "detailedSummary": "## 研究背景与动机\n机器人仓拣是工业自动化的核心任务，但堆叠物品造成的遮挡严重影响了抓取的准确性与可靠性。模态分割旨在预测物体的完整掩码（包括可见和隐藏部分），是解决此问题的关键。当前主流方法主要基于卷积神经网络（CNN），例如扩展Mask R-CNN或UOAIS-Net等方法，它们通常依赖昂贵的密集标注，对新物体形状泛化能力差，且推理流程多阶段、速度慢。近期，基于Transformer的方法（如AISFormer）或结合全局形状先验（如ShapeFormer）及扩散模型（如AISDiff）的方法虽能提升质量，但计算成本高，难以满足工业实时性要求。而较为轻量的类无关方法C2F-Seg，其迭代预测过程仍会引入不可忽略的延迟。\n\n本文针对工业仓拣场景下面临的三个关键痛点：标注数据稀缺、对新形状泛化能力差、以及高计算成本限制实时应用，提出了利用Vision Transformer（ViT）全局注意力机制进行类无关模态分割的新视角。其核心思路是，利用经过掩码自编码器（MAE）预训练的ViT强大的长程依赖建模和泛化能力，设计轻量级的编码器-解码器架构，直接从RGB图像和可见掩码中高效推理出完整的物体掩码。\n\n## 方法详解\nViTA-Seg提出了两种架构：ViTA-Seg Single-Head（单头）和ViTA-Seg Dual-Head（双头），整体采用编码器-解码器设计。输入均为两个部分：1）一个围绕可见掩码边界框扩大20%后裁剪出的224×224像素的感兴趣区域（RoI）RGB图像 `I`；2）对应的可见掩码 `M_V`。可见掩码可由上游的类无关检测器（如GroundingDINO）提供。输出为224×224的掩码：单头架构仅预测模态掩码 `M_A`；双头架构同时预测模态掩码 `M_A` 和遮挡掩码 `M_O`。\n\n![方法框架](https://arxiv.org/html/2512.09510v1/x2.png)\n> **图2**：提出的ViTA-Seg架构。上方为ViTA-Seg Single-Head，仅预测模态掩码；下方为ViTA-Seg Dual-Head，同时预测模态和遮挡掩码。\n\n**编码器**：基于ViT-Base（ViT-B）架构，并采用MAE进行预训练，这种预训练方式使其特别擅长恢复被遮挡的图像区域。为同时处理外观信息和可见性信息，编码器将3通道的RGB图像 `I` 与单通道的可见掩码 `M_V` 在通道维度拼接，形成4通道输入。为适配预训练权重，新增通道的投影权重初始化为RGB三通道权重的平均值。编码器最终输出一个768维的特征向量 `v_feat`。\n\n**解码器（单头）**：直接将 `v_feat` 映射到模态掩码 `M_A`。它由4个解码块组成，通过转置卷积逐步上采样，特征图尺寸变化序列为：[768,1,1] → [512,28,28] → [256,56,56] → [128,112,112] → [64,224,224]。最后接一个两层的卷积头，将通道数从64降至1，并通过Sigmoid激活得到二值掩码。损失函数为针对模态掩码的二元交叉熵（BCE）损失。\n\n**解码器（双头）**：设计了共享层与任务特定分支的结构，以促进模态与遮挡预测的跨任务协作。前2层为共享层，特征变化为：[768,1,1] → [512,28,28] → [256,56,56]。随后，网络分裂为两个分支，每个分支从[128,56,56]的特征图开始，经过2层专用解码层上采样至[32,224,224]。关键的创新在于，在最后阶段，将遮挡分支的32通道特征与模态分支的32通道特征进行拼接，使得模态路径能显式利用关于隐藏区域的线索。拼接后的特征（64通道）经过一个两层的卷积头预测 `M_A`，而遮挡分支的32通道特征则通过一个单层卷积头预测 `M_O`。双头架构的损失是两项BCE损失的加权和：`L_total = λ_A * BCE(M_A, M_A_gt) + λ_O * BCE(M_O, M_O_gt)`，其中 `λ_A` 和 `λ_O` 为超参数。\n\n**创新点**：1）首次将经过MAE预训练的ViT作为主干网络引入类无关模态分割任务，利用其全局上下文建模能力；2）提出轻量的单头与双头架构，双头架构通过特征拼接实现跨任务协作；3）与C2F-Seg等需要迭代预测或潜在空间编码的方法相比，ViTA-Seg采用更直接的前向传播，显著提升了推理速度。\n\n## 实验与结果\n**数据集与实验平台**：在三个数据集上评估：真实世界模态分割基准COCOA和KINS，以及本文提出的面向工业仓拣的合成数据集ViTA-SimData。实验在配备NVIDIA RTX 4090的工作站上进行。\n\n**对比方法**：主要与当前类无关模态分割的先进方法C2F-Seg进行对比。\n\n**关键实验结果**：如表2所示，ViTA-Seg Dual-Head在所有数据集上均取得了最佳性能。在COCOA上，其模态mIoU（`mIoU_A`）和遮挡mIoU（`mIoU_O`）分别达到93.70%和49.88%，相比C2F-Seg提升了6.55和13.19个百分点。在KINS上分别提升3.23和6.18个百分点。在ViTA-SimData上分别提升5.92和5.37个百分点。更重要的是，ViTA-Seg的推理时间（约9毫秒）比C2F-Seg（约114毫秒）快了约12倍，实现了实时性能。\n\n![定性结果1](https://arxiv.org/html/2512.09510v1/x3.png)\n> **图3**：在ViTA-SimData上的定性结果对比。ViTA-Seg Dual-Head能预测出更多的遮挡区域，且其预测的可见部分（绿色）与输入可见掩码贴合更紧密；而C2F-Seg（红色轮廓）倾向于在遮挡区域产生“幻觉”，部分忽略了输入掩码。\n\n![定性结果2](https://arxiv.org/html/2512.09510v1/x4.png)\n> **图4**：在KINS和COCOA真实数据集上的定性结果。进一步证实了ViTA-Seg在复杂真实遮挡场景下的有效性。\n\n**消融实验**：1）**架构对比**：双头模型在`mIoU_A`和`mIoU_O`上均优于单头模型，证明了联合预测遮挡掩码对模态分割的促进作用。2）**遮挡头损失权重分析**：如表4所示，调整损失权重 `λ_O` 会影响性能平衡。当 `λ_O` 从0增加到0.25时，`mIoU_O` 显著提升（从49.33%到58.65%），`mIoU_A` 也有所改善。但当 `λ_O` 增至0.50时，`mIoU_O` 达到最高（58.98%），`mIoU_A` 却略有下降，表明过度关注遮挡区域会损害对完整形状的预测。\n\n![损失权重影响](https://arxiv.org/html/2512.09510v1/x5.png)\n> **图5**：不同 `λ_O` 设置下的预测结果可视化。当 `λ_O=0.50` 时，模型对遮挡部分的预测（紫色）过于激进，甚至可能侵入可见区域，导致模态掩码（蓝色）形状失真；`λ_O=0.25` 取得了更好的平衡。\n\n## 总结与启发\n**核心贡献**：1）提出了ViTA-Seg，一个基于ViT的、类无关的实时模态分割框架，其双头架构通过跨任务协作显著提升了分割精度和速度。2）创建了ViTA-SimData，一个针对工业仓拣场景的逼真合成数据集，用于训练和评估。3）在多个基准测试中，ViTA-Seg在精度和效率上均超越了现有先进方法，证明了其在机器人感知任务中的实用价值。\n\n**局限性**：模型依赖上游检测器（如GroundingDINO）提供可见掩码和边界框，并非完全的端到端系统。\n\n**后续启示**：ViTA-Seg的成功表明，利用经过适当预训练（如MAE）的ViT的全局建模能力，是解决类无关、高遮挡分割任务的有效途径。其轻量高效的架构设计为工业实时应用提供了可行方案。未来的工作可探索将检测与分割进一步集成，形成端到端管道，并在真实的、无约束的工业仓拣系统中进行部署验证。构建专用的真实工业模态分割数据集也将是推动该领域发展的关键。",
      "imageUrls": [
        "https://arxiv.org/html/2512.09510v1/x1.png",
        "https://arxiv.org/html/2512.09510v1/x2.png",
        "https://arxiv.org/html/2512.09510v1/x3.png",
        "https://arxiv.org/html/2512.09510v1/x4.png",
        "https://arxiv.org/html/2512.09510v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.08548",
      "title": "Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations",
      "url": "http://arxiv.org/abs/2512.08548",
      "arxivId": "2512.08548",
      "date": "2025-12-09",
      "authors": "Ting Liu Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中因平台和任务差异导致的动作数值尺度分布偏移问题，提出一种基于语言的语义化动作表征方法。该方法通过构建强调方向性、忽略数值尺度的运动表征，归一化动作命令，以缓解分布偏移并缩小动作标记与语言词汇的模态间隙。在两大多任务基准上的实验表明，该方法显著提升了策略的泛化性能与跨任务可迁移性。",
      "detailedSummary": "## 研究背景与动机\n近年来，受大语言模型启发的端到端架构被越来越多地应用于机器人操作研究，以实现鲁棒的操作。然而，一个关键挑战源于机器人动作数据之间严重的分布偏移，这主要是由于不同机器人平台和任务间动作命令的数值存在巨大差异，阻碍了预训练知识的有效迁移。同时，现有的语言条件模仿学习方法通常在每个时间步提供动态的视觉输入，但保持静态的语言指令，这种不平衡限制了语言模态在引导动作生成中的影响力，未能充分利用语言的潜力。\n\n本文针对动作数值尺度差异导致的分布偏移这一具体痛点，提出了一个基于语义的语言中间表征来归一化动作，以进行高效的预训练。其核心思路是：通过一个规则映射将末端执行器动作转化为粗粒度的语言描述（“动作表征”），作为对齐目标；这种表征忽略数值尺度效应，强调方向性，从而缓解分布偏移，并缩小动作标记与标准词汇标记之间的特征距离。\n\n## 方法详解\n本文方法按动作标记化、动作生成和模型训练的顺序展开。整体目标是学习一个两阶段的条件生成策略，从观察和指令逐步预测具体动作。\n\n![方法框架](https://arxiv.org/html/2512.08548v1/x1.png)\n> **图1**：提出的动作数据生成流程。左侧展示了不同类型数据集中具体执行动作的分布；中间展示了基于阈值和窗口的检测框架及其改进；右侧描绘了生成的动作输出的结构和表征。\n\n**1. 动作标记化 (Action Tokenizer)**\n遵循RT-2和OpenVLA的方法，将机器人应执行的7维连续动作 `(ΔX, ΔY, ΔZ, Δroll, Δpitch, Δyaw, GripperState)` 归一化后，每个维度离散化为256个区间（bin），每个区间用一个唯一的标记表示。在视觉语言动作模型设计中，向词表尾部添加了256个特殊标记来表示这些动作标记。\n\n**2. 动作生成 (Motion Generation)**\n这是核心创新模块，旨在从动作轨迹中自动生成鲁棒的自然语言动作信号，作为高层语义指导。\n*   **动作表征 (Motion Representation)**：生成一套固定的自然语言描述，结构为：`move [forward/backward] [left/right] [up/down], tilt [up/down], rotate [clockwise/counterclockwise], [open/close] gripper`。`move`描述执行器沿坐标轴的位置位移，`tilt`和`rotate`表示角度旋转，`gripper`指开合动作。若无运动，则标记为“stop”。\n*   **自适应阈值 (Threshold)**：为更精确地检测有意义的运动，考虑了高速运动可能引起的抖动现象。提出了基于速度的校正方法，动态调整阈值。公式为：`T_i(t) = T_base^i + β · (1/τ) Σ |Δ̂_i(s)|`，其中`T_base^i`是基础阈值，`β`是敏感系数，`τ`是调整窗口。\n*   **分层窗口 (Window)**：为适应不同类型的机械臂运动，用分层的检测窗口（快、中、慢）取代单一固定窗口。分别定义了快变(`M_f`)、中速常规(`M_m`)和慢变反应(`M_s`)三种运动检测逻辑，最终动作由三者综合判定：`Motion(t) ≔ M_f(t) ∨ M_m(t) ∨ M_s(t)`。\n*   **设计验证**：在手动标注的数据上，本方法的平均标注准确率达到86.37%，显著优于ECoT风格的固定阈值方法（57.62%）。本方法能有效抑制执行中的微小抖动被误识别为多个独立动作。\n\n**3. 两阶段训练 (Two-Stage Training)**\n对于每条轨迹，构建数据元组 `(O_i^j, p_i, M_i^j, A_i^j)`，其中`O`是观察（图像帧），`p`是任务指令，`M`是生成的动作语言，`A`是离散动作标记。学习策略分为两个阶段：\n1.  **高层动作预测**：`φ_h(m|o, p)`，根据当前观察和指令，自回归地预测描述即将发生动作的动作标记。\n2.  **具体动作推断**：`φ_l(a|o, p, m)`，利用预测出的动作标记作为上下文信息，推断具体的动作标记。即 `φ(a, m|o, p) = φ_h(m|o, p) φ_l(a|o, p, m)`。\n\n![训练流程](https://arxiv.org/html/2512.08548v1/x2.png)\n> **图2**：Qwen2.5上的两阶段训练：预训练预测动作标记；微调预测动作后再预测动作标记。\n\n*   **仅动作预训练 (Motion-Only Pretraining)**：在多样化的多源数据（如Open X-Embodiment子集）上，模型仅学习预测动作序列。这类似于课程学习，从更统一、更容易的动作方向知识开始，旨在高效捕获通用信息，降低学习和迁移难度。\n*   **下游微调 (Downstream Fine-Tuning)**：预训练后，模型在特定下游场景（如LIBERO、Bridge V2）的模仿学习数据上进行微调。此阶段，模型需要同时预测动作标记和细粒度的动作标记，以获得可执行的具体动作。\n\n## 实验与结果\n**实验设置**：\n*   **基准/数据集**：预训练使用Open X-Embodiment的7个子集（约1.2万条轨迹）。评估使用LIBERO（四个任务套件：Spatial, Goal, Object, Long）和SimplerEnv（在Bridge V2数据集上评估四个具体任务）。\n*   **基线方法**：对比了Diffusion Policy, ScaleDP, Octo, OpenVLA, RT-1-x, ECoT等方法。\n*   **模型架构**：基于OpenVLA，视觉编码器使用SigLIP和DINOv2，LLM主干使用Qwen2.5（0.5B, 1.5B, 3B参数）。\n\n**关键实验结果**：\n**RQ1: 各改进的个体贡献**\n通过消融实验验证动作预训练和动作生成优化的效果。\n*   如表3、4所示，**从零开始训练**时，加入动作预训练的模型在LIBERO和SimplerEnv上的平均成功率均高于不加动作的版本（LIBERO上平均提升+1.8%~+2.5%）。\n*   如表5、6所示，**基于预训练模型进行微调**时，加入动作预训练带来的提升更显著（LIBERO上平均提升+3.0%~+6.9%，SimplerEnv上3B模型提升达+14.1%）。这证明了动作预训练的有效性。\n*   如表7所示，**优化动作生成质量**（调整窗口和阈值）相比原始动作生成方法，在LIBERO上带来了平均约1%的成功率提升，验证了优化策略的贡献。\n\n![消融实验1](https://arxiv.org/html/2512.08548v1/x3.png)\n> **图3**：在LIBERO基准上从零开始训练的成功率（%），比较有无动作预训练（w/ motion vs w/o motion）。展示了动作预训练在不同模型规模下均能带来性能提升。\n\n![消融实验2](https://arxiv.org/html/2512.08548v1/x4.png)\n> **图4**：在LIBERO基准上基于预训练模型微调的成功率（%）。可见基于动作预训练的模型微调后，性能优势进一步扩大，尤其是在3B模型上。\n\n**RQ2: 与基线及SOTA方法的对比**\n*   **LIBERO基准**（表8/图5）：本文方法（特别是3B模型）取得了78.1%的平均成功率，超越了Diffusion Policy、ScaleDP、Octo和OpenVLA等基线，与OpenVLA (7B)结果具有竞争力，但使用了更小的模型。\n*   **SimplerEnv基准**（表9/图6）：本文的3B模型取得了35.3%的平均成功率，显著优于RT-1-x、Octo-Base/Small、OpenVLA和ECoT (7B)等基线，尤其在“胡萝卜放盘子”和“茄子放篮子”任务上表现出色。\n\n![LIBERO对比结果](https://arxiv.org/html/2512.08548v1/x5.png)\n> **图5**：在LIBERO基准上与基线方法的成功率对比。本文方法（3B）取得了最佳平均性能。\n\n![SimplerEnv对比结果](https://arxiv.org/html/2512.08548v1/x6.png)\n> **图6**：在SimplerEnv基准上与基线方法的成功率对比。本文方法（3B）在多个任务上领先，平均成功率最高。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种新颖的预训练策略，利用基于规则的语言表征（动作）来对齐跨数据集的动作-语言分布，无需人工标注或外部校正即可捕获通用的运动-语言关系，从而增强了模型的泛化能力和可迁移性。\n2.  提出了一种自适应的多尺度动作检测方法，能动态调整阈值并采用分层窗口，有效抑制了数据集中间的运动抖动和错误分割，显著提高了复杂动作识别的准确性。\n3.  在LIBERO和SimplerEnv基准上的大量实验验证了本方法在准确性、稳定性和鲁棒性上优于现有基线。\n\n**局限性**：\n论文在实验中指出，1.5B参数模型在SimplerEnv基准上性能增益有限，推测这主要是由于微调数据（来自真实世界）与模拟测试环境之间的差距，以及有限参数规模的约束共同导致的。\n\n**启示**：\n本文为解决机器人学习中的分布偏移问题提供了一个新视角：通过引入语义化的、与数值尺度解耦的中间表征来桥接不同数据源。这启示后续研究可以进一步探索更丰富、更细粒度的语义动作表征，或者将这种思想扩展到其他存在模态或领域差异的具身智能学习问题中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.08548v1/x1.png",
        "https://arxiv.org/html/2512.08548v1/x2.png",
        "https://arxiv.org/html/2512.08548v1/x3.png",
        "https://arxiv.org/html/2512.08548v1/x4.png",
        "https://arxiv.org/html/2512.08548v1/x5.png",
        "https://arxiv.org/html/2512.08548v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.09101",
      "title": "Masked Generative Policy for Robotic Control",
      "url": "http://arxiv.org/abs/2512.09101",
      "arxivId": "2512.09101",
      "date": "2025-12-09",
      "authors": "Paul Henderson Team",
      "category": "Manipulation",
      "summary": "本文提出掩码生成策略（MGP），以解决现有生成策略在机器人视觉运动模仿学习中存在的推理速度慢、对非马尔可夫任务鲁棒性不足的核心问题。方法将动作离散化为令牌，利用条件掩码变换器并行生成令牌，并快速细化低置信度部分；针对不同任务提出了MGP-Short（并行掩码生成与基于分数的细化）和MGP-Long（单次预测整条轨迹并动态细化）两种采样范式。在150项机器人操作任务上的实验表明，MGP平均成功率提升9%，推理速度加快最高达35倍，在动态与缺失观测环境中成功率提升60%，并能解决其他方法失败的非马尔可夫任务。",
      "detailedSummary": "## 研究背景与动机\n当前，从视觉输入直接学习机器人控制策略的模仿学习领域，主要存在两类生成式策略方法：扩散策略与自回归策略。扩散策略将动作合成视为条件去噪过程，需要多次迭代采样，导致推理速度慢；加速版本如一致性策略或流策略则往往需要额外蒸馏或牺牲采样质量。自回归策略将动作视为离散令牌，通过类似GPT的变换器进行逐个令牌预测，其延迟随序列长度线性增长，且由于缺乏记忆，对观测缺失和非马尔可夫任务（即当前决策依赖于历史状态的任务）的鲁棒性不足。\n\n本文针对现有方法在推理速度瓶颈和非马尔可夫长时程任务鲁棒性方面的核心痛点，提出了一种新的掩码生成视角。核心思路是将机器人动作表示为离散令牌，训练一个条件掩码变换器来并行生成令牌，并仅对低置信度令牌进行快速细化，从而同时实现快速推理和适应动态环境的长时程、全局一致性预测。\n\n## 方法详解\nMGP的整体框架分为两个训练阶段和一个包含两种范式的推理阶段。首先，使用VQ-VAE将连续动作序列离散化为令牌。随后，训练一个掩码生成变换器（MGT），学习在给定观测条件下重建被掩码的动作令牌序列。推理时，针对不同任务类型采用两种采样范式：MGP-Short用于马尔可夫任务，进行快速并行生成与细化；MGP-Long用于非马尔可夫任务，通过自适应令牌细化实现长时程动态规划。\n\n![方法框架总览](https://arxiv.org/html/2512.09101v2/x2.png)\n> **图2**：MGP训练与MGP-Short推理流程。左：训练阶段1 - 动作分词器（VQ-VAE）将连续动作编码为离散令牌。中：训练阶段2 - 掩码生成变换器（MGT）学习在观测条件下重建被掩码的令牌序列。右：短时程采样（MGP-Short）推理过程，包含并行生成和基于置信度的细化步骤。\n\n**核心模块1：动作分词器**。采用VQ-VAE将长度为 $T$ 的连续动作序列 $\\mathbf{a}$ 编码为 $N$ 个离散动作令牌 $\\mathbf{y}$。编码器为两个残差一维CNN块，解码器为对称的上采样Conv1D块。训练目标包括重构损失和承诺损失（公式1），并使用指数移动平均更新码本。\n\n**核心模块2：掩码生成变换器**。MGT采用编码器-仅变换器架构，包含感知编码器和变换器本体。感知编码器将观测 $O_t$ 和状态 $s_t$ 编码为条件嵌入。变换器由2层交叉注意力层（关注条件嵌入与令牌嵌入）和2层自注意力层构成。训练时，随机掩码部分动作令牌并用 `[MASK]` 替换，MGT学习预测被掩码令牌的概率（公式2），目标是最小化负对数似然。\n\n**创新采样范式1：MGP-Short**。针对马尔可夫任务，该范式仅基于当前观测 $c_t$ 进行推理。过程包含2次迭代：1）将所有未来令牌初始化为 `[MASK]`，与条件一同输入MGT，通过Gumbel-Max技巧（公式3）并行采样所有令牌；2）根据预测的归一化概率（置信度得分）对令牌排序，将置信度最低的部分令牌重新掩码，并进行第二次细化生成。这实现了高质量并行生成，仅需极少迭代。\n\n**创新采样范式2：MGP-Long**。针对非马尔可夫长时程任务，该范式集成了自适应令牌细化策略。\n\n![长时程采样流程](https://arxiv.org/html/2512.09101v2/x3.png)\n> **图3**：长时程采样（MGP-Long）通过自适应令牌细化实现。模型基于初始观测预测整个任务时程的令牌序列并开始执行；在执行过程中，根据新到达的观测，利用后验置信度估计有选择地掩码和细化尚未执行的低置信度令牌，同时保留已执行的历史令牌。\n\n其核心是**后验置信度估计**。在每一步 $i$，机器人已执行前 $n$ 个令牌 $\\mathbf{y}_{i-1}^{0:n}$，并获得新观测 $c_i$。MGT计算在 $c_i$ 和已执行历史 $\\mathcal{H}_{i-1}$ 条件下，剩余未执行令牌 $\\mathbf{y}_{i-1}^{n:N}$ 的新概率分布（公式4），作为更新后的置信度得分。随后，仅对这些未执行令牌的得分进行归一化排序，并将低分令牌重新掩码（公式5）。最后，将包含已执行令牌和重新掩码的未执行令牌的序列，连同新条件输入MGT进行细化，采样出更新后的剩余令牌（公式6）。此过程实现了在保留历史执行记忆的基础上，对剩余计划进行动态、有针对性的调整，保证了长时程动作的全局一致性。\n\n与现有方法相比，MGP的主要创新在于：1）**并行生成**：不同于自回归的序列生成，MGT一次性预测所有未来令牌。2）**动态细化**：基于置信度得分进行选择性重掩码和细化，而非扩散模型的全程迭代去噪或自回归模型的完全重新生成。3）**全局一致性预测**：MGP-Long通过保留已执行令牌作为条件，并在新观测下更新未执行部分，实现了对长时程任务的连贯规划与动态适应。\n\n## 实验与结果\n实验在三个标准基准上进行：Meta-World（50个任务）、LIBERO-90（90个短时程任务）和LIBERO-Long（长时程任务）。对比了十种基线方法，包括扩散策略（DP， DP3， CP， FlowPolicy）、自回归策略（ACT， VQ-BeT， PRISE， QueST， ResNet-T）以及为对比设计的DP3-Full Seq.。此外，还设置了观测缺失、动态环境和非马尔可夫任务的专项评估。\n\n**标准基准结果**：在Meta-World单任务训练中，MGP-Short在50个任务上平均成功率达0.637，优于所有基线（如DP3的0.599， FlowPolicy的0.571），且每步推理时间仅3毫秒，比DP3快约49倍。对于Hard/Very Hard任务，MGP-Long平均成功率达0.563，显著优于一次性生成全序列的DP3-Full Seq.（0.270）和MGP-Full Seq.（0.340）。\n\n![Meta-World单任务结果](https://arxiv.org/html/2512.09101v2/x4.png)\n> **图4**：Meta-World单任务训练的成功率与推理时间对比。左图显示MGP-Short在平均成功率上领先，右图显示其在保持高成功率的同时，拥有最低的每步推理延迟。\n\n在多任务训练的LIBERO-90上，MGP-Short取得了0.889的平均成功率，与最佳自回归基线QueST（0.886）相当，但每步推理时间从17毫秒降至5毫秒。在更长的LIBERO-Long任务上，MGP-Long取得了82.0%的平均成功率，优于MGP-Short的77.0%和QueST的68.0%，且序列级推理时间从225毫秒（MGP-Short）降至78毫秒。\n\n**专项评估结果**：\n1.  **观测缺失环境**：在观测随机丢失的设定下，MGP-Long在Hard/Very Hard任务上平均成功率为0.484/0.566，比短时程方法（如DP3和MGP-Short）高出约22%-31%，展示了其对部分可观测性的鲁棒性。\n2.  **动态环境**：在目标或障碍物持续移动的任务中，MGP-Long平均成功率达0.436，优于MGP-Short（0.430）和DP3（0.360），更远超一次性开环规划的方法（DP3-Full Seq. 0.04），证明了其动态适应能力。\n\n![动态与观测缺失环境结果](https://arxiv.org/html/2512.09101v2/x5.png)\n> **图5**：在动态环境和观测缺失环境下的成功率。MGP-Long在这两种具有挑战性的设置下均表现出了最强的鲁棒性和适应性。\n\n3.  **非马尔可夫任务**：在两个设计的非马尔可夫任务（需记忆历史状态）中，MGP-Long是唯一能够成功解决的方法，而包括QueST、DP3在内的其他先进方法均告失败。\n\n**消融实验**：论文通过变体对比验证了MGP-Long各组件的作用。MGP-Full Seq.（一次性生成全序列无细化）性能显著下降，说明了动态细化的必要性。MGP-w/o SM（新观测下掩码所有剩余令牌并重新生成）性能（80.5%）低于完整MGP-Long（82.0%），说明了基于置信度的选择性掩码对于保持计划稳定性和效率的重要性。\n\n![消融实验与模型效率](https://arxiv.org/html/2512.09101v2/x6.png)\n> **图6**：左：MGP-Long及其消融变体在LIBERO-Long上的性能对比，显示了自适应细化和基于置信度掩码的价值。右：MGP与基线模型的参数量与训练时间对比，显示MGP更加轻量高效。\n\n**模型效率**：MGP参数量约7M，远少于DP3（约262M）；在相同硬件上训练时间约55分钟，远少于DP3的约3小时。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了首个用于机器人模仿学习的掩码生成策略框架，通过并行令牌生成和置信度驱动的细化，同时解决了扩散模型推理慢和自回归模型序列约束强的问题。2）针对不同任务类型设计了MGP-Short和MGP-Long两种采样范式，前者实现了高速闭环控制，后者通过自适应令牌细化实现了对长时程、非马尔可夫、动态及部分可观测任务的鲁棒且连贯的控制。3）在涵盖150个任务的广泛实验中，MGP在取得更高成功率的同时，显著降低了推理延迟（最高达35倍），并展示了卓越的适应性与鲁棒性。\n\n论文提及的局限性包括：在极端长序列（>1000步）任务中，VQ-VAE的压缩可能引入误差；对于极其复杂和快速变化的动态环境，当前的细化频率和策略可能仍不足。\n\n本文的启示在于：将掩码生成建模引入机器人控制，为平衡推理速度、规划质量和环境适应性提供了一个富有潜力的新方向。其“并行预测-动态编辑”的思想可启发更多在长时程决策中保持全局一致性与局部灵活性的工作。轻量化的模型设计也表明，高效策略未必需要极大参数量。",
      "imageUrls": [
        "https://arxiv.org/html/2512.09101v2/x1.png",
        "https://arxiv.org/html/2512.09101v2/x2.png",
        "https://arxiv.org/html/2512.09101v2/x3.png",
        "https://arxiv.org/html/2512.09101v2/x4.png",
        "https://arxiv.org/html/2512.09101v2/x5.png",
        "https://arxiv.org/html/2512.09101v2/x6.png",
        "https://arxiv.org/html/2512.09101v2/x7.png",
        "https://arxiv.org/html/2512.09101v2/x8.png",
        "https://arxiv.org/html/2512.09101v2/x9.png",
        "https://arxiv.org/html/2512.09101v2/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.07248",
      "title": "Benchmarking Humanoid Imitation Learning with Motion Difficulty",
      "url": "http://arxiv.org/abs/2512.07248",
      "arxivId": "2512.07248",
      "date": "2025-12-08",
      "authors": "Yipeng Qin Team",
      "category": "Manipulation",
      "summary": "本文针对人形机器人模仿学习中现有评估指标（如关节误差）无法区分策略性能与动作本身难度的问题，提出了一种独立于策略的**动作难度评分（MDS）**。MDS基于刚体动力学，通过分析微小姿态扰动引起的扭矩变化（体积、方差、时间变异性）来量化动作的固有学习难度。基于MDS重构了MD-AMASS数据集，并提出了**最大可模仿难度（MID）**和**难度分层关节误差（DSJE）**两个新评估指标。实验验证表明，MDS能有效解释先进策略的性能差异，例如发现PHC+总体领先，但UHC在简单动作上更优。",
      "detailedSummary": "## 研究背景与动机\n当前基于物理的人形运动模仿学习主流方法（如UHC、PHC）依赖于与参考动作的相似性指标（如关节位置误差）进行评估。然而，这些指标仅衡量策略模仿的优劣，却忽略了运动本身固有的难度。这导致策略性能与运动难度被混淆，无法区分失败是由于策略学习能力不足，还是目标动作本身就极具挑战性。\n\n本文针对“如何独立于策略性能，量化运动的内在模仿难度”这一具体痛点，提出了一个基于刚体动力学的新视角。核心思路是：将模仿难度定义为在有限位姿误差邻域内所引发的扭矩变化幅度；扭矩变化越大，奖励地形越平坦，学习难度越高。\n\n## 方法详解\n本文提出的方法旨在定义并计算一个独立于策略的**运动难度分数（Motion Difficulty Score, MDS）**，并构建一个难度分层的数据集。\n\n整体框架基于刚体动力学。输入是一个运动序列 S，其每帧状态 s 包含位姿、速度、加速度。通过物理仿真（如RBDL库）可以建立从状态空间 X 到扭矩空间 Y 的映射 F。核心思想是分析在状态 S 的一个小扰动邻域 N(S) 内，所对应的扭矩集合 T 的变化特性，并将此变化量化为难度分数。\n\n![MDS示意图](https://arxiv.org/html/2512.07248v1/x2.png)\n\n> **图2**：MDS原理示意图。对于一个简单动作（左上，挥手），小的位姿扰动引起小的扭矩方差，对扰动不敏感，MDS值低。对于一个困难动作（左下，单腿站立），相同程度的扰动导致所有关节产生大的扭矩方差，MDS值高。\n\nMDS由三个互补的多样性组件加权求和构成，分别对应定义难度时的三个因素：扭矩空间的总体积、跨关节的方差以及时间变异性。\n1.  **谱多样性 (Spectral Diversity, d1)**：用于建模扭矩空间的体积 `vol_Y(T)`。通过对映射雅可比矩阵的奇异值求和的对数来计算，反映了可行扭矩空间的有效维度。值越高表示扭矩变化越大，难度越高。\n2.  **方差多样性 (Variance Diversity, d2)**：用于捕捉扭矩变化在不同关节间的分布差异。计算每个关节雅可比矩阵元素的方差，并对所有关节的方差取对数求和。方差越小，意味着所有关节对扰动都敏感，动作越不稳定，难度越高。\n3.  **分段多样性 (Segment Diversity, d3)**：用于捕获扭矩变化随时间演化的特性。将运动序列划分为K个非重叠子段（实验中K=4），计算每个子段的谱多样性并取平均。其理论最大值在所有子段难度相同时达到，此时整个运动时序上难度均匀，学习更困难；反之，若序列中包含难易交替的段落，则相对易于学习。\n\n最终，MDS的计算公式为加权和：`MDS = ∑_{i=1}^3 w_i * d_i`，权重为经验设定。\n\n基于MDS，本文构建了**MD-AMASS**数据集，这是对广泛使用的AMASS数据集的难度感知重划分。将长序列分割为100帧（约3.3秒）的短片段，为每个片段计算MDS值，从而得到一个包含超过30,000个片段、每个都标注了标量难度系数的数据库。\n\n![MD-AMASS数据集](https://arxiv.org/html/2512.07248v1/x3.png)\n\n> **图3**：难度感知的AMASS数据集MD-AMASS构建示意图。将长序列分割为固定长度的短片段，并为每个片段计算MDS。\n\n与现有方法相比，本文的创新点在于首次从刚体动力学原理出发，明确定义了运动的内在模仿难度，并提出了一个可计算的、与策略解耦的量化指标MDS，以及由此衍生的难度分层数据集。\n\n## 实验与结果\n实验使用了本文构建的**MD-AMASS**数据集作为评测基准。对比的baseline是当前最先进的两种运动模仿策略：**UHC** 和 **PHC+**。实验平台涉及物理仿真与RBDL库用于动力学计算。\n\n验证的核心是证明MDS与策略模仿误差之间存在强相关性。关键实验结果如下：\n\n![MDS与模仿误差散点图](https://arxiv.org/html/2512.07248v1/x4.png)\n\n> **图4**：超过3000个运动片段的MDS与模仿误差（MPJPE-G，单位mm）散点图。左图为UHC策略，右图为PHC+策略。结果显示，随着MDS增加（难度增大），模仿误差呈上升趋势，证明了MDS的有效性。\n\n定量结果显示，MDS与两种策略的模仿误差均呈现显著正相关。具体相关系数为：\n- 对于UHC：皮尔逊相关系数0.4823，斯皮尔曼秩相关系数0.6586，肯德尔秩相关系数0.4747。\n- 对于PHC+：皮尔逊相关系数0.6478，斯皮尔曼秩相关系数0.8203，肯德尔秩相关系数0.6293。\n所有相关系数均为正且数值较高，尤其是斯皮尔曼相关系数，强烈验证了MDS作为难度度量的可靠性。\n\n![不同难度动作的模仿结果对比](https://arxiv.org/html/2512.07248v1/x5.png)\n\n> **图5**：不同MDS值动作的模仿定性结果对比。从左（容易）到右（困难），模仿保真度明显下降。例如，高难度动作（如闪避、单腿跳）中，人形机器人出现过度弯曲肢体以保持平衡等失真现象。\n\n消融实验研究了MDS中三个组件的必要性。如表2所示，移除任何一个组件（谱多样性SP、方差多样性VA或分段多样性SE）都会导致与模仿误差的相关性下降。完整MDS在所有相关性指标上均优于任何部分组合，证明了三个组件都是必要且互补的。\n\n## 总结与启发\n本文的核心贡献可概括为：1) **理论贡献**：首次从刚体动力学角度，明确给出了运动内在模仿难度的定义。2) **方法贡献**：提出了可计算的运动难度分数（MDS），它通过量化小位姿扰动引发的扭矩变化来评估难度，并构建了难度分层的MD-AMASS数据集。3) **应用贡献**：验证了MDS对现有先进策略性能的解释力，并基于MDS提出了最大可模仿难度（MID）和难度分层关节误差（DSJE）等新评估视角。\n\n论文自身提到的局限性包括：MDS的计算依赖于物理仿真（如RBDL）来获取动力学参数；三个多样性组件的权重是经验设定的。\n\n本文的启示在于，将“难度”作为一个可量化的维度引入模仿学习评估，为未来研究开辟了新方向。例如，可以基于MDS设计**课程学习**策略，从易到难训练模型；可以用于**策略诊断**，区分失败原因是策略能力不足还是任务本身过难；还可以启发**运动优化**，生成既符合语义又易于模仿的参考动作。",
      "imageUrls": [
        "https://arxiv.org/html/2512.07248v1/x1.png",
        "https://arxiv.org/html/2512.07248v1/x2.png",
        "https://arxiv.org/html/2512.07248v1/x3.png",
        "https://arxiv.org/html/2512.07248v1/x4.png",
        "https://arxiv.org/html/2512.07248v1/x5.png",
        "https://arxiv.org/html/2512.07248v1/x6.png",
        "https://arxiv.org/html/2512.07248v1/x7.png",
        "https://arxiv.org/html/2512.07248v1/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.07582",
      "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations",
      "url": "http://arxiv.org/abs/2512.07582",
      "arxivId": "2512.07582",
      "date": "2025-12-08",
      "authors": "Yufeng Yue Team",
      "category": "Manipulation",
      "summary": "本文提出ViVLA模型，旨在解决机器人操作策略难以泛化至训练分布外新任务的问题。核心方法是构建一个视觉-语言-动作模型，使其能够通过联合处理单次专家演示视频与机器人实时观测，学习并预测动作序列，从而从一次演示中提炼精细操作知识。关键技术包括一个可扩展的专家-智能体配对数据生成流程，用于合成大规模训练数据。实验表明，ViVLA仅凭一次演示视频即可学习新技能，在未见过的LIBERO任务上性能提升超过30%，使用跨体现视频时增益保持在35%以上，真实世界实验中对未见任务的改进超过38%。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型通过在大规模机器人数据集上训练，展现出端到端机器人控制的潜力，但泛化到其训练分布之外的任务仍然是一个重大挑战。相比之下，人类具备仅通过观察他人一次演示就能高效学习新技能的非凡能力。受此启发，本文旨在研究一种通用的机器人策略，使机器人能够在测试时仅通过观察一次专家演示视频，无需额外训练，即可学习训练分布之外的新任务。本文的核心思路是：提出名为ViVLA的VLA新范式，通过联合处理专家演示视频和机器人视觉观察，预测演示动作序列和后续机器人动作，从而从专家行为中提取细粒度操作知识并无缝转移给智能体。\n\n## 方法详解\nViVLA的整体框架包含两个关键阶段：（I）基于动作中心循环一致性（A3C）的潜在动作学习；（II）用于一次性任务学习的ViVLA训练。模型在训练时学习基于单个专家演示视频预测后续机器人动作，从而在测试时获得从单个专家演示视频学习新任务的能力。\n\n![方法总览](https://arxiv.org/html/2512.07582v1/x2.png)\n> **图2**：ViVLA方法总览。（I）潜在动作分词器（LAT）从观察序列中学习量化的潜在动作，为专家视频和智能体演示获取潜在动作。（II）ViVLA模型被训练来预测学习到的潜在动作序列和后续机器人动作，使机器人能够在测试时仅通过单个专家演示视频获取新操作技能。\n\n**核心模块一：带循环一致性的潜在动作学习**\n此阶段目标是学习一个统一的潜在动作空间，弥合专家（如人）与智能体（机器人）之间的具身鸿沟。潜在动作分词器采用编码器-解码器架构。编码器使用DINOv2提取当前帧和未来帧的嵌入，与可学习的潜在动作令牌拼接后，经由时空（ST）Transformer建模帧间动态，最终输出被量化为离散的潜在动作表示。解码器则根据当前帧和潜在动作重建未来帧。\n\n![潜在动作框架](https://arxiv.org/html/2512.07582v1/x4.png)\n> **图4**：带动作中心循环一致性的潜在动作框架示意图。该方法从观察帧中学习潜在动作表示，同时引入动作中心循环一致性约束以建立统一的潜在动作空间。\n\n关键创新在于引入了**动作中心循环一致性**约束。方法维护一个潜在动作缓冲区，从中采样一个潜在动作，与一个观察帧一起解码生成一个未来帧。然后，编码器被训练从原始观察帧和生成帧中恢复出这个采样的潜在动作。此约束确保了潜在动作的语义一致性（相同的动作应产生相似的变化）和跨具身统一性（不同具身的相似动作应在潜在空间中对齐），如图3所示，解决了现有方法潜在动作语义不一致、空间割裂的问题。\n\n**核心模块二：ViVLA训练**\n训练时，ViVLA的输入包括：经过时间-空间掩码策略处理的专家演示视频、语言指令以及智能体的当前观察图像。模型被训练同时输出两个序列：专家视频对应的潜在动作序列，以及智能体后续应执行的潜在动作序列。\n\n![ViVLA训练](https://arxiv.org/html/2512.07582v1/x5.png)\n> **图5**：ViVLA示意图。该方法联合处理专家演示视频和机器人的视觉观察，以预测演示动作序列和后续机器人动作，促使ViVLA从专家行为中提取细粒度操作知识并无缝转移给智能体。\n\n主要技术细节包括：\n1.  **并行解码**：模型接收空的动作嵌入作为输入，在单次前向传播中并行生成所有动作令牌。这避免了自回归解码中利用历史真实动作令牌导致的“短路学习”问题，迫使模型基于对视频内容和智能体观察的深入分析进行预测，并显著提升了推理效率。\n2.  **时间-空间掩码**：在训练时随机掩码专家视频的令牌（跨越时间和空间维度）。这降低了计算复杂度，并创建了一个更具挑战性的学习目标，要求模型从部分观察到的演示中进行动作预测，从而促进对视频的整体理解。\n3.  **细粒度动作推理**：在生成机器人控制输出之前，模型被训练显式地阐述（预测）演示视频中观察到的操作动作，这增强了其细粒度动作识别和理解能力。\n4.  **时间定位任务**：将智能体的观察图像插入演示视频序列，训练模型识别这些观察在视频中的时间位置，促进了视频和图像表征之间的跨模态信息交换。\n\n**数据生成流程**\n为训练ViVLA，本文开发了一个可扩展的专家-智能体配对数据生成流程。该流程以易于获取的人类视频为输入，通过3D高斯泼溅渲染出执行相同任务的机器人演示过程，从而生成大量的人类-机器人配对轨迹样本。此外，还从公开数据集中构建了任务相似的配对。最终，共汇总了892,911个专家-智能体配对样本用于训练。\n\n## 实验与结果\n实验主要在LIBERO基准测试中进行，并在真实世界进行了验证。对比的基线方法包括：OSVI-WM、AWDA、T-OSVI、LAPA、UniVLA以及RT-2、OpenVLA等VLA模型。\n\n![模拟实验结果](https://arxiv.org/html/2512.07582v1/x6.png)\n> **图6**：在LIBERO基准测试上的成功率。ViVLA在未见任务上显著优于所有基线方法。\n\n关键实验结果如下：\n- **LIBERO未见任务**：ViVLA取得了**46.7%** 的成功率，相比最佳基线OSVI-WM（15.8%）提升了**超过30个百分点**。\n- **跨具身泛化**：当测试时提供的专家演示视频来自不同具身（如从机器人演示训练，但给人视频测试）时，ViVLA仍能保持**35.0%** 的成功率，相比OSVI-WM（0%）有巨大提升。\n- **真实世界未见任务**：ViVLA从人类视频中有效提取知识，在真实机器人上执行未见任务的成功率达到**38.5%**，显著高于基线方法。\n\n![跨具身与真实世界结果](https://arxiv.org/html/2512.07582v1/x7.png)\n> **图7**：（左）使用跨具身演示视频（人类视频）在LIBERO上的成功率。（右）在真实世界未见任务上的成功率。\n\n![消融实验](https://arxiv.org/html/2512.07582v1/x8.png)\n> **图8**：消融研究结果。A3C（动作中心循环一致性）、PD（并行解码）和TSM（时间-空间掩码）每个组件都对性能有重要贡献。\n\n**消融实验总结**：\n- **动作中心循环一致性（A3C）**：是性能提升的关键，移除此组件成功率下降21.7%。\n- **并行解码（PD）**：解决了短路学习问题，将其换为自回归解码会导致性能下降10.0%。\n- **时间-空间掩码（TSM）**：促进了更好的视频理解，移除后性能下降8.4%。\n- **细粒度动作（FA）与时间定位（TL）目标**：二者共同作用提升了模型对视频细节的理解。\n\n![定性结果](https://arxiv.org/html/2512.07582v1/x9.png)\n> **图9**：ViVLA在LIBERO基准测试中成功与失败轨迹的定性示例。\n\n![潜在动作空间可视化](https://arxiv.org/html/2512.07582v1/x10.png)\n> **图10**：潜在动作空间可视化（t-SNE）。ViVLA学习的潜在动作空间在跨具身（人类与机器人）上显示出更好的对齐和一致性。\n\n![数据量影响](https://arxiv.org/html/2512.07582v1/x11.png)\n> **图11**：训练数据量对性能的影响。使用全量数据（892K样本）训练的模型性能最佳。\n\n![推理速度对比](https://arxiv.org/html/2512.07582v1/x12.png)\n> **图12**：与自回归基线相比，ViVLA的并行解码显著提高了推理速度（快约7倍）。\n\n## 总结与启发\n本文的核心贡献可概括为：1）提出了ViVLA新范式，使策略模型能够在测试时从单次演示中学习新技能，无需微调；2）引入了带循环一致性约束的潜在动作学习框架和并行解码机制，建立了统一的跨具身动作表示并解决了短路学习问题；3）开发了可扩展的专家-智能体配对数据生成流程，构建了大规模数据集；4）实验验证了该方法在模拟和真实环境中从单次演示（尤其是跨具身人类视频）学习未见任务的有效性，性能提升显著。\n\n论文未明确阐述局限性，但大规模配对数据的生成依赖特定流程，且模型可能对演示视频的质量和视角有一定要求。本文的启示在于：为机器人学习建立统一的、语义一致的跨具身动作表示是实现高效知识迁移的关键；并行解码策略为改进VLA模型的训练和推理效率提供了新思路；利用易于获取的人类视频通过技术手段生成机器人训练数据，是解决机器人数据稀缺问题的一条可行路径。",
      "imageUrls": [
        "https://arxiv.org/html/2512.07582v1/x1.png",
        "https://arxiv.org/html/2512.07582v1/x2.png",
        "https://arxiv.org/html/2512.07582v1/x3.png",
        "https://arxiv.org/html/2512.07582v1/x4.png",
        "https://arxiv.org/html/2512.07582v1/x5.png",
        "https://arxiv.org/html/2512.07582v1/x6.png",
        "https://arxiv.org/html/2512.07582v1/x7.png",
        "https://arxiv.org/html/2512.07582v1/x8.png",
        "https://arxiv.org/html/2512.07582v1/x9.png",
        "https://arxiv.org/html/2512.07582v1/x10.png",
        "https://arxiv.org/html/2512.07582v1/x11.png",
        "https://arxiv.org/html/2512.07582v1/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.07697",
      "title": "Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks",
      "url": "http://arxiv.org/abs/2512.07697",
      "arxivId": "2512.07697",
      "date": "2025-12-08",
      "authors": "Shayegan Omidshafiei Team",
      "category": "Manipulation",
      "summary": "本文解决机器人控制中因感知、计算等环节导致的推理延迟（数十至数百毫秒）问题，该延迟造成观察与执行状态不一致，严重影响动态任务性能。作者提出延迟感知扩散策略（DA-DP），通过在训练与推理中显式纳入延迟测量，将零延迟轨迹校正为延迟补偿版本，并对策略进行延迟条件化增强。实验表明，DA-DP在多种任务、机器人及延迟条件下，比无视延迟的方法成功率更高、鲁棒性更强，如在乒乓球任务中能成功击球而基线方法失败。该框架架构无关，可推广至其他模仿学习方法。",
      "detailedSummary": "## 研究背景与动机\n当前机器人视觉运动策略学习的主流方法，如扩散策略（Diffusion Policy, DP），通常在训练和部署时假设推理延迟为零。这意味着策略根据当前观测状态生成动作，并假设该动作能立即执行。然而，在真实的机器人系统中，从感知到动作执行之间存在不可避免的推理延迟（δ），可达数十至数百毫秒。在静态环境中，这种延迟影响较小，但在与高速动态物体交互的任务中，零延迟假设会导致严重的观察-执行不匹配，使得策略动作总是滞后，从而任务失败。\n\n本文针对动态任务中推理延迟导致策略性能下降这一具体痛点，提出了将延迟作为算法设计一等公民的新视角。核心思路是通过对零延迟的示范数据进行延迟补偿校正，并训练一个以延迟为条件的扩散策略，使策略能够针对动作执行时的未来状态进行规划，从而弥合观察与执行之间的差距。\n\n## 方法详解\nDelay-Aware Diffusion Policy (DA-DP) 框架包含两个核心部分：延迟感知数据处理和以延迟为条件的策略训练。其目标是修改训练数据，使得在给定推理延迟δ的情况下，训练出的策略仍能在目标时间内完成任务。\n\n![方法框架总览](https://arxiv.org/html/2512.07697v1/x2.png)\n> **图2**: DA-DP框架总览。以动作块长度 H_act=4，推理延迟 δ=2Δt 为例。上半部分展示了零延迟轨迹、DP执行轨迹（因延迟而滞后）以及DA-DP校正后的目标轨迹。下半部分通过三个步骤将零延迟轨迹压缩为延迟感知轨迹。\n\n**整体流程**：输入是假设零延迟收集的示范轨迹 τ。对于给定的推理延迟δ，DA-DP首先对τ进行压缩校正，生成延迟感知轨迹 τ‘_δ。然后，利用在不同δ下生成的一系列τ‘_δ数据集，训练一个以δ为条件的扩散策略 π_θ(a’|s‘， δ)。在推断时，将实测或设定的延迟δ与当前状态拼接后输入策略，生成补偿延迟的动作。\n\n**核心模块与技术细节**：\n1.  **延迟感知数据处理**：这是将零延迟轨迹适配到存在延迟的系统的关键。具体分为三步：\n    *   **步骤1：计算调整后长度**：根据公式 n‘ = (nΔt + δ) / (Δt + δ/H_act) 计算校正后的轨迹长度 n‘。其物理意义是，在总时间预算 T_target = nΔt 内，考虑每次推理耗时δ，所能完成的最大动作步数。\n    *   **步骤2：确定状态跳过数量**：计算每个动作块后需要跳过的状态数 m。在连续时间模型中，m = δ/Δt。在离散实现中，需根据 n 和 n‘ 调整，确保压缩后的轨迹步数为整数且能准时到达终点。\n    *   **步骤3：压缩轨迹**：按照公式 s‘_i = s_{i + k(i)*m} 构建压缩后的状态序列，其中 k(i) 是已完成动作块计数。这意味着每执行完一个 H_act 的动作块，状态索引就向前跳跃 m 步，以抵消推理延迟带来的时间损失。最后，根据压缩后的状态序列计算新的动作 a‘_i = s‘_{i+1} - s‘_i。\n\n2.  **以延迟为条件的扩散策略**：算法上，DA-DP 对标准 DP 的修改极小且易于集成。主要区别在于**将推理延迟δ作为条件输入**。在训练时，从延迟感知数据集 {τ_δ} 中采样批量 (s， a， δ)，将延迟δ与状态s拼接形成增强状态 ~s，然后输入到扩散模型的编码器和UNet中。损失函数与标准DP一致，均为去噪分数匹配损失。在推断时，同样将当前状态与已知延迟δ拼接后输入策略，生成动作。\n\n**创新点**：与现有旨在加速推理或实现异步执行的方法不同，DA-DP的创新点在于**显式地对推理延迟进行建模和补偿**。它不改变扩散去噪过程本身，而是通过数据预处理和条件化，从根本上改变了策略学习的目标——让策略学习如何从“当前观测”规划出能在“未来执行时刻”生效的动作。这种方法与同步/异步执行范式是互补的，可以即插即用。\n\n## 实验与结果\n**实验设置**：实验在ManiSkill引擎构建的三个动态环境中进行（图3），涵盖了不同机器人形态和任务类型。\n*   **任务**：1) 捡起滚动球（Franka Panda机械臂）；2) 乒乓球（Franka Panda机械臂）；3) 移动箱子的抓放（Unitree G1人形机器人）。\n*   **基线方法**：标准扩散策略（DP）以及作为理想性能参考的零延迟DP。\n*   **评估指标**：任务成功率。\n\n![实验环境](https://arxiv.org/html/2512.07697v1/x3.png)\n> **图3**: 三个用于评估的动态任务环境：捡起滚动球、乒乓球和移动箱子的抓放。\n\n**关键实验结果**：\n**Q1. 推理延迟是否影响动态任务性能？**\n实验表明，推理延迟显著损害了DP的性能，而DA-DP则保持了鲁棒性。\n\n![捡球任务-固定延迟](https://arxiv.org/html/2512.07697v1/x4.png)\n> **图4**: 在捡起滚动球任务中，随着固定推理延迟δ增加，DP成功率急剧下降（δ=0.10s时仅0.01），而DA-DP下降平缓（δ=0.10s时为0.72）。\n\n![乒乓球任务-固定延迟](https://arxiv.org/html/2512.07697v1/x5.png)\n> **图5**: 在乒乓球任务中，除最小延迟（δ=0.025s）外，DA-DP在所有延迟设置下均优于DP，且性能接近零延迟的理想基线。\n\n**Q2. DA-DP能否处理变化的推理延迟？**\n通过在多延迟值混合的数据集上训练，DA-DP在测试时对动态延迟表现出更强的鲁棒性。\n\n![捡球任务-变延迟集](https://arxiv.org/html/2512.07697v1/x6.png)\n> **图6**: 在捡球任务中，面对不同范围的延迟集合，DA-DP（成功率0.42-0.76）始终优于DP（最高仅0.28）。\n\n![乒乓球任务-变延迟集](https://arxiv.org/html/2512.07697v1/x7.png)\n> **图7**: 在乒乓球任务中，随着训练延迟集增大，DA-DP性能提升至0.80，而DP性能停滞不前。\n\n**Q3. DA-DP对训练分布外的延迟是否鲁棒？**\n在训练未见过的更大延迟上进行评估，DA-DP展现了更好的泛化能力。\n\n![捡球任务-分布外延迟](https://arxiv.org/html/2512.07697v1/x8.png)\n> **图8**: 在捡球任务中，当测试延迟比训练延迟高0.15s时，DP性能近乎为零，而DA-DP仍能保持0.48-0.62的成功率。\n\n![乒乓球任务-分布外延迟](https://arxiv.org/html/2512.07697v1/x9.png)\n> **图9**: 在乒乓球任务中，测试延迟增加0.075s，DA-DP成功率稳定在0.73-0.82，显著高于DP的0.49-0.56。\n\n**Q4. DA-DP能否泛化到不同机器人形态？**\n在人形机器人执行动态抓放任务的实验中，DA-DP的优势依然存在。\n\n![人形机器人任务-固定延迟](https://arxiv.org/html/2512.07697v1/x10.png)\n> **图10**: 在更复杂的人形机器人抓放移动箱子任务中，DA-DP在多个延迟设置下达到完美成功率（1.0），而DP最高仅为0.7。\n\n**消融实验**：本文的核心方法（数据校正+延迟条件化）本身可视为对标准DP的增强。实验结果表明，二者结合有效解决了延迟问题。数据校正确保了时间目标的正确性，而延迟条件化使策略能够适应不同的延迟值。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**DA-DP框架**，一种通过数据校正和延迟条件化，显式地将推理延迟纳入策略学习的通用方法。\n2.  通过**广泛的实证评估**，证明了DA-DP在多种动态任务、机器人平台和延迟条件下，相比延迟不感知的方法具有显著更强的鲁棒性和性能。\n\n**局限性**：论文自身提到，DA-DP的数据校正步骤依赖于对系统动态（状态转移）的访问或估计（通过 s‘_{i+1} - s‘_i 计算动作）。在仅依赖图像等高维观测而无法直接获取状态的情况下，需要额外的模型来预测状态变化。\n\n**对后续研究的启示**：\n1.  **评估协议**：DA-DP鼓励在报告策略性能时，不仅说明任务难度，还应将其作为实测延迟的函数，这能更真实地反映其在现实系统中的适用性。\n2.  **通用模式**：该方法框架与策略架构无关，其“测量延迟、校正数据、条件化训练”的模式可迁移至扩散策略以外的其他模仿学习方法，为解决机器人控制中的时序对齐问题提供了一个通用思路。",
      "imageUrls": [
        "https://arxiv.org/html/2512.07697v1/x1.png",
        "https://arxiv.org/html/2512.07697v1/x2.png",
        "https://arxiv.org/html/2512.07697v1/x3.png",
        "https://arxiv.org/html/2512.07697v1/x4.png",
        "https://arxiv.org/html/2512.07697v1/x5.png",
        "https://arxiv.org/html/2512.07697v1/x6.png",
        "https://arxiv.org/html/2512.07697v1/x7.png",
        "https://arxiv.org/html/2512.07697v1/x8.png",
        "https://arxiv.org/html/2512.07697v1/x9.png",
        "https://arxiv.org/html/2512.07697v1/x10.png",
        "https://arxiv.org/html/2512.07697v1/x11.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.08405",
      "title": "Learning Robot Manipulation from Audio World Models",
      "url": "http://arxiv.org/abs/2512.08405",
      "arxivId": "2512.08405",
      "date": "2025-12-09",
      "authors": "Michael Gienger Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操纵任务中视觉信息模糊或不完整时依赖音频进行多模态推理的核心问题，提出一种生成式潜在流匹配模型。该方法采用基于变压器的流匹配技术，在潜在空间预测未来音频状态，以捕捉音高和节奏模式等物理动态，并集成到机器人策略中实现长期推理。实验表明，在模拟和真实世界的音频感知操纵任务中，该方法相比无未来展望的方法性能更优。",
      "detailedSummary": "## 研究背景与动机\n当前机器人学习领域的世界模型研究主要集中于两个方向：一是基于视频的模型，通过预测未来的视觉帧来编码物理交互的因果依赖，但其高保真度要求带来了巨大的计算开销和延迟；二是潜空间视觉语言世界模型，学习未来状态的内部表示以避免显式的全帧重建。然而，这些方法大多忽略了音频模态。现有的多模态机器人学习方法通常仅将音频作为辅助感官输入，以提升纯视觉策略的性能。但在许多场景中，音频并非可有可无，当视觉线索稀缺或模糊时，音频能提供关键信息。例如，向瓶子中灌水时，视觉观察可能几乎不变，此时必须依赖音频线索（如音高模式）的时序演化来判断瓶子是否已满。这些任务中的音频通常反映了内在的物理动态（如音高和节奏模式），因此预测未来的音频观察对于指导机器人行动策略至关重要。本文针对机器人操作中需要推理未来音频状态这一具体痛点，提出了一种生成式潜空间流匹配模型来预测未来的音频观测，并将其集成到机器人策略中，使系统能够推理长期后果。核心思路是利用流匹配技术高效生成时序一致的未来音频潜表示，并基于当前及预测的音频（连同视觉观测）来训练机器人策略。\n\n## 方法详解\n本文提出的方法整体框架包含三个阶段：首先预训练一个音频自编码器以获得平滑且富有表现力的音频潜空间；然后以流匹配为世界模型主干，在给定驱动条件下生成未来音频潜序列；最后，一个策略模型利用合成的音频帧（及图像）来预测机器人动作。输入为长度为 `L'` 的源音频序列 `s^{-L':0}`，输出为未来音频序列 `s^{1:L}` 和对应的机器人动作块 `a`。\n\n![方法框架](https://arxiv.org/html/2512.08405v1/x1.png)\n> **图1**：方法整体框架。源音频首先被编码为潜表示。给定当前音频片段，一个基于流匹配的Transformer估计从带噪音频潜到目标潜的生成向量场。该向量场随后用于求解相应的常微分方程（ODE），产生未来音频潜。生成的未来音频潜序列被解码为音频谱图。最终，机器人策略利用当前和预测的未来音频谱图以及图像观测进行训练。\n\n**核心模块1：音频潜空间自编码器**\n使用谱图表示音频，因其能提供丰富的时频信息，有效捕捉信号的时序和频谱特征。采用AudioMAE模型来编码和解码谱图，并使用重建损失进行训练，旨在获得一个表达性强且平滑的音频潜空间。\n\n**核心模块2：潜空间流匹配**\n采用流匹配技术进行潜空间音频生成。模型回归一个向量场 `v_t(x_t, c_t; θ)`，其中 `x_t` 是时间步 `t∈[0,1]` 的样本，`c_t` 代表后续音频帧的驱动条件。向量场预测器基于DiT架构修改，解耦了逐帧条件与时间轴注意力机制，以确保生成时序一致的潜表示。训练时，选择目标音频潜 `w_{s^{1:L}}`，并构造目标向量场 `u_t(x|w_{s^{1:L}})`，其输入噪声路径为 `φ_t(x_0) = (1-t)x_0 + t w_{s^{1:L}}`（`t` 均匀采样，`x_0` 从标准高斯分布采样）。为生成比窗口长度 `L` 更长的平滑序列，将前一个窗口的最后 `L'` 个音频特征潜 `w_{s^{-L':0}}` 作为额外输入。因此，流匹配目标 `L_fm(θ)` 定义为预测向量场与目标向量场之间的差异（公式1）。此外，引入了一个速度损失 `L_v(θ)`（公式2）来监督时间一致性，计算预测与目标向量场沿时间轴的逐帧差分。总损失 `L(θ)` 是这两个损失的加权和（公式3）。推理时，从源分布采样随机路径点，然后通过估计从 `t=0` 到 `t=1` 的流，使用多步ODE求解器（公式4）将其“流动”到目标音频潜。\n\n**核心模块3：机器人策略**\n利用当前和预测的音频帧（谱图）以及当前图像观测来生成机器人动作。采用了基于流匹配的策略网络，条件输入包括当前音频、当前图像观测表示以及预测的未来音频。该策略以监督学习方式端到端训练，使用对未来16步机器人末端执行器速度的流预测均方误差（MSE）损失。\n\n**学习细节与创新点**\n关键创新在于采用了**模块化、分阶段训练**的架构：音频流匹配世界模型与机器人策略训练是解耦的。这种设计允许独立适应和替换各个组件（例如在钢琴任务中，可将AudioMAE替换为MusicVAE，将流匹配策略替换为SAC强化学习策略），并据论文引用能获得比端到端训练更高的平均成功率。训练策略时使用真实未来音频帧，评估时则使用潜世界模型生成的音频来推断动作。\n\n## 实验与结果\n实验在两个任务上进行：1) 真实世界灌水任务；2) 模拟钢琴演奏任务。\n\n**实验设置**\n- **灌水任务**：使用Kinova Gen3机械臂操作饮水机按钮。RGB视频由Intel RealSense D410相机采集，音频由MAONO全向USB领夹麦克风录制（192 kHz采样率）。数据通过Haption Virtuose 6D力反馈设备遥操作收集。音频被转换为128维对数梅尔滤波器组谱图，输入为1.28秒谱图，预测未来2.56秒谱图。策略基于当前及预测谱图与224x224图像，生成未来16步的6自由度笛卡尔空间末端速度指令。整个模型单步预测耗时约50毫秒，以闭环方式运行。\n- **钢琴任务**：在模拟环境中进行钢琴二重奏，机器人需根据听觉输入预测即将到来的音符并演奏自己的部分。使用MIDI数据表示音乐，并转换为钢琴卷帘。采用Soft-Actor-Critic (SAC)策略预测机器人关节动作。音频世界模型使用MusicVAE编码约8秒MIDI数据并生成下一个8秒，可自回归扩展。生成的MIDI被转换为SAC策略的目标状态。使用来自RoboPianist和PIG数据集的MIDI进行世界模型训练，在《小星星》和肖邦《夜曲》两首曲子上训练和测试策略。\n\n**基线方法**：与**没有未来前瞻（without future lookahead）** 的基本方法进行对比。在灌水任务中，对比的是不利用预测未来音频的策略；在钢琴任务中，对比的是不提供未来目标状态（音符）的基本RL基线。\n\n**关键实验结果**\n- **灌水任务**：方法在30次试验中取得了**100%的成功率**。闭环生成的谱图清晰捕捉了按钮按下/释放的动作以及灌水过程中逐渐升高的音高。值得注意的是，在测试中，预测的按钮释放活动（谱图中的第二条黄线） consistently 在瓶子实际满之前就出现在生成谱图中，体现了模型基于物理动态进行预测的能力。\n- **钢琴任务**：当将生成的未来音频（作为目标状态）纳入观察时，策略性能得到提升。这是因为访问未来音符允许策略更有效地规划（例如预先定位手腕等非手指关节以更快触及即将到来的音符）。评估采用F1分数（音频信息检索常用指标），结果表明结合未来音频预测的框架优于无未来前瞻的基线。\n\n![实验结果](https://arxiv.org/html/2512.08405v1/x2.png)\n> **图2**：实验结果。分别展示了灌水谱图、音乐谱图及MIDI数据的真实情况与世界模型生成结果。灌水谱图是在机器人评估期间以闭环方式预测的。音乐片段是基于先前片段自回归生成的。\n\n**消融实验**：论文虽未设置独立的消融实验部分，但其核心论证通过对比“有未来音频预测”与“无未来音频预测”的基线，实质上验证了**未来音频状态预测**这一核心组件的关键贡献。此外，模块化设计中替换不同编码器（AudioMAE vs. MusicVAE）和策略（流匹配 vs. SAC）的成功，也侧面证明了框架的灵活性和各组件可替换性带来的价值。\n\n## 总结与启发\n本文的核心贡献在于：1) **提出了一个音频驱动的机器人操作世界模型框架**，首次系统性地利用生成式音频预测来指导机器人策略学习，强调了在特定任务中音频不仅是多模态输入，更是蕴含物理动态的关键信息源；2) **采用基于流匹配的潜空间生成模型**，实现了对未来音频状态的高效、时序一致的预测；3) **展示了模块化、分阶段训练架构的灵活性与优势**，该框架允许根据任务特性替换音频表示（谱图/MIDI）和策略学习方法（监督流匹配/强化学习）。\n\n论文自身提到的局限性或未来方向在于，将此框架扩展到需要更精细、更灵巧操作的复杂任务中是一个重要的研究方向。\n\n对后续研究的启示包括：探索其他蕴含物理动态的模态（如力觉、触觉）的预测世界模型；进一步研究多模态（音视频等）联合预测的世界模型；以及将此类生成式世界模型与更复杂的机器人规划、控制框架进行深度融合。",
      "imageUrls": [
        "https://arxiv.org/html/2512.08405v1/x1.png",
        "https://arxiv.org/html/2512.08405v1/x2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.08188",
      "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model",
      "url": "http://arxiv.org/abs/2512.08188",
      "arxivId": "2512.08188",
      "date": "2025-12-09",
      "authors": "Rui Chen Team",
      "category": "Manipulation",
      "summary": "本文针对机器人长时域操作规划中，视频生成模型缺乏物理基础、易产生幻觉且难以保持物理一致性的问题，提出了一种名为Embodied Tree of Thoughts (EToT) 的Real2Sim2Real规划框架。该框架以基于物理的交互式数字孪生作为具身世界模型，其核心技术是通过两种协同机制在树中搜索可行计划：先验分支基于语义与空间分析生成候选路径，反思分支利用视觉语言模型诊断模拟执行失败并迭代优化规划树。实验表明，该方法在长短时域操作任务上能有效预测物理动态、适应潜在失败，性能持续优于基线模型。",
      "detailedSummary": "## 研究背景与动机\n机器人操作规划领域，世界模型是关键组件，用于预测动作执行后的环境状态。当前主流方法采用视频生成模型作为前向预测器，但其缺乏严格的物理基础，容易产生幻觉，且在长时域任务中难以维持物理约束的一致性，限制了其在复杂、多步交互任务中的应用。另一类Real2Sim方法将真实场景重建到物理模拟器中，利用模拟器作为物理基础的世界模型，能确保刚体动力学和碰撞约束，但现有方法（如PWTF）通常将模拟器仅用于低级控制，高级规划仍局限于单一、固定的任务分解，一旦初始分解错误即导致失败。\n\n本文针对现有方法物理基础薄弱、长时域推理不可靠以及规划缺乏灵活性的痛点，提出了一个新视角：将基于视觉语言模型（VLM）的高级推理与基于物理模拟器的具身世界模型相结合，并将操作规划形式化为一个树状搜索过程。核心思路是提出一个名为“具身思维树”（EToT）的Real2Sim2Real规划框架，通过“先验分支”和“反思分支”两种协同机制，在物理模拟器中迭代地构建、搜索和修正规划树，最终将可行的计划在真实机器人上执行。\n\n## 方法详解\nEToT框架是一个Real2Sim2Real的闭环规划系统。其整体流程如下：给定任务指令和场景图像，系统首先将真实场景重建为物理模拟器中的交互式3D数字孪生（Real2Sim）；然后，基于此具身世界模型，通过先验分支和反思分支构建并搜索规划树，以找到物理上可行的行动序列；最后，执行该可行计划，并在真实执行中通过视觉反馈进行重规划（Sim2Real）。\n\n![方法框架总览](https://arxiv.org/html/2512.08188v1/x2.png)\n> **图2**：Embodied Tree of Thoughts (EToT) 框架总览。给定任务指令，系统首先将真实场景重建为交互式3D数字孪生。然后通过**先验分支**（Priori Branching）和**反思分支**（Reflective Branching）构建基于世界模型的规划树。先验分支提出初始候选分支，反思分支分析模拟执行失败以扩展修订分支。通过迭代搜索和扩展规划树，系统识别出可行计划，最终以闭环方式在真实机器人上执行，并结合视觉反馈和重规划。\n\n**核心模块一：具身世界模型构建**\n该模块旨在从单次RGB-D观测（对于铰接物体需要额外视角）高效重建物理模拟场景。流程如下：1）使用SAM-3从RGB图像提取物体掩码；2）使用SAM-3D-Objects生成带纹理的物体网格；3）使用DexSim2Real 2的尺寸估计模块从RGB-D数据恢复度量尺度，得到缩放后的网格；4）使用FoundationPose进行物体位姿估计；5）将重建的网格导入OmniGibson模拟器，生成与物理场景对齐的交互式数字孪生。对于铰接物体，需额外处理不同运动状态下的观测以恢复其铰接结构。\n\n**核心模块二：规划树构建与搜索（算法1）**\n规划过程围绕一棵树展开，包含两个关键机制：\n1.  **先验分支**：在初始阶段，VLM解析场景和任务指令，基于物体实例和候选交互模式生成初步的规划树。具体包括：**场景解析与任务理解**（提取物体信息和空间关系）；**候选分支生成**（VLM在存在多个可行行动选择的分支点显式地生成不同分支，包括**实例级分支**（如选择不同的目标物体）和**操作参数分支**（如选择不同的抓取构型））；**初始规划树构建**（将行动序列作为节点插入树中，合并共享共同前缀的分支以形成紧凑的树表示）。\n2.  **反思分支与树搜索**：系统采用广度优先搜索遍历规划树。对每个节点，在模拟器中执行其关联的动作，并用VLM评估结果。若成功则继续搜索子节点或到达叶节点返回可行计划；若失败则触发反思分支。反思分支包含两个步骤：**失败检测**（VLM比较模拟执行前后所有物体的状态，判断行动是否成功且安全，例如是否导致物体掉落至不可达区域）；**树修正与扩展**（VLM诊断失败原因并提出修正策略，主要包括两类：i) **碰撞引发的干扰**：先将被影响的物体重新安置到安全位置，再重试原动作；ii) **顺序相关的冲突**：修订分支，对相关动作进行重新排序）。修正后生成的新分支会被合并到现有规划树中，并加入搜索队列。\n\n与现有方法相比，EToT的创新点具体体现在：1) **物理基础的世界模型**：使用物理模拟器而非视频生成模型，确保所有预测结果严格遵守物理定律；2) **双分支协同规划机制**：通过“先验分支”提供规划的广度（多样性），通过“反思分支”提供规划的深度（纠错与优化），克服了单一固定任务分解的局限性；3) **模拟器内闭环诊断与修正**：能够在执行前于模拟环境中诊断潜在的物理失败（如碰撞、不可达），并主动生成修正策略，避免了现实世界中不可逆的失败。\n\n## 实验与结果\n**实验设置**：硬件使用xArm6 6DoF机械臂和Azure Kinect DK RGB-D相机。在7个桌面操作任务上进行评估，涵盖短时域（≤3步）和长时域（>3步）任务，以及一个包含人为干扰的任务（图4）。基准方法包括：ReKep（基于视觉关键点的VLM规划）、ReKep w/ CoT（增加思维链提示）、Reflect*（在ReKep w/ CoT基础上增加类似反思机制，但修正计划由人工指定）。所有方法使用GPT-4o作为VLM，评估指标为任务成功率（成功达成目标且未对环境造成有害改变）。\n\n![任务示意图](https://arxiv.org/html/2512.08188v1/x4.png)\n> **图4**：所有任务的示意图，包括初始状态和正确目标状态，方框和箭头表示关键物体的位置变化。任务设计用于系统评估机器人对物体可操作性、三维空间关系、物理动力学预测以及抗干扰自动恢复能力的理解。\n\n![实验结果表](https://arxiv.org/html/2512.08188v1/x5.png)\n> **表1**：短时域和长时域任务上的成功率对比。EToT在所有任务上均一致优于基线方法，平均成功率高达88.8%。基线方法在长时域任务上（任务5-7）表现有限（最高不超过50%），而EToT保持了较高的成功率（分别为8/10, 9/10, 7/10）。\n\n**关键实验结果**：如表1所示，EToT取得了最高的平均成功率（88.8%），在7个任务中的6个上取得了最佳或并列最佳成绩。ReKep由于缺乏细粒度空间和物理推理，表现最差（平均16.3%）。ReKep w/ CoT通过思维链提示有所改善（平均38.8%），尤其在任务1和4上。Reflect* 通过人工指定的反思机制进一步提升了表现（平均66.3%），在任务3和干扰任务上表现出色，但对于涉及不可逆失败的任务（如任务6）无能为力。EToT凭借其前瞻性的物理模拟和主动修正能力，在长短时域任务上均表现稳健。\n\n**消融实验**：表2展示了各组件贡献的消融研究结果。\n- **移除先验分支**：平均成功率降至61.3%，在需要探索多种可能方案的任务（如任务2、3、5）上表现显著下降。\n- **移除反思分支**：平均成功率暴跌至35.0%，在涉及复杂物理约束或顺序依赖的任务（如任务1、4、5、6、7）上几乎完全失败，突显了基于模拟反馈进行动态修正是框架成功的关键。\n- **移除重规划机制**：在干扰任务上成功率降为0，证明了在真实执行中根据感知反馈进行重新规划的必要性。\n- **用视频生成模型替代物理模拟器**：性能最差（平均21.3%），验证了物理基础对于准确预测和规划至关重要。\n\n![消融实验表](https://arxiv.org/html/2512.08188v1/x6.png)\n> **表2**：消融研究结果。分别移除先验分支、反思分支、重规划机制，以及用视频生成模型（VGM）替换物理模拟器作为世界模型。结果显示，反思分支对整体性能贡献最大，物理模拟器相比视频生成模型具有显著优势。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 **Embodied Tree of Thoughts (EToT)** 框架，一种新颖的Real2Sim2Real操作规划范式，将VLM的高级语义推理与物理模拟器的具身世界模型紧密结合。\n2.  引入了 **两种协同的分支机制**（先验分支与反思分支），将规划形式化为树搜索问题，实现了对多样化候选计划的前瞻性评估和对执行失败的动态诊断与修正，显著提升了长时域复杂任务的规划可靠性和鲁棒性。\n3.  通过系统的实验验证，**明确了物理基础的世界模型相对于视频生成模型的优势**，并量化展示了所提各组件（尤其是反思分支）对成功率的关键贡献。\n\n**局限性**：论文自身提到，当前方法依赖于相对耗时的场景重建和模拟过程，可能影响实时性。模拟环境与真实世界之间仍存在差距（sim-to-real gap）。此外，行动技能集目前是预定义且有限的。\n\n**对后续研究的启示**：\n1.  **效率优化**：探索更轻量级的场景重建方法和并行模拟技术，以加速规划树的构建与搜索过程。\n2.  **模拟与现实差距**：研究如何利用在线感知数据持续校准和更新数字孪生模型，缩小sim-to-real差距。\n3.  **技能扩展**：将框架与技能学习相结合，扩展可用的行动原语库，以处理更广泛、更复杂的操作任务。\n4.  **结合学习模型**：未来可探索将学习型动力学模型与物理模拟器相结合，在保证物理一致性的同时，处理一些难以精确建模的物理交互（如柔体、流体）。",
      "imageUrls": [
        "https://arxiv.org/html/2512.08188v1/x1.png",
        "https://arxiv.org/html/2512.08188v1/x2.png",
        "https://arxiv.org/html/2512.08188v1/x3.png",
        "https://arxiv.org/html/2512.08188v1/x4.png",
        "https://arxiv.org/html/2512.08188v1/x5.png",
        "https://arxiv.org/html/2512.08188v1/x6.png",
        "https://arxiv.org/html/2512.08188v1/x7.png",
        "https://arxiv.org/html/2512.08188v1/x8.png",
        "https://arxiv.org/html/2512.08188v1/x9.png",
        "https://arxiv.org/html/2512.08188v1/x10.png",
        "https://arxiv.org/html/2512.08188v1/x11.png",
        "https://arxiv.org/html/2512.08188v1/x12.png",
        "https://arxiv.org/html/2512.08188v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.07472",
      "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.07472",
      "arxivId": "2512.07472",
      "date": "2025-12-08",
      "authors": "Chang Xu Team",
      "category": "Manipulation",
      "summary": "本文针对视觉-语言-动作（VLA）模型在机器人操作中因分布偏移而陷入“记忆陷阱”、重复执行记忆轨迹而非适应新场景的核心问题，提出了一种轻量级混合框架“可供性场干预”（AFI）。该方法将3D空间可供性场（SAF）作为即插即用模块，通过本体感知检测陷阱，将机器人重定位至高可供性区域，并生成可供性驱动的路径点来引导VLA动作，最终由基于SAF的评分器选择最优轨迹。实验表明，该方法在真实机器人平台的分布外场景下，使不同VLA主干网络的性能平均提升23.5%，在LIBERO-Pro基准上提升20.2%，有效增强了VLA的鲁棒性。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型通过模仿学习，实现了从视觉观察和语言指令到机器人动作的端到端映射，在机器人操作中展现出强大潜力。然而，这类模型在面临分布偏移（OOD）时表现脆弱，例如当目标物体位置发生显著变化时，模型往往会机械地复现训练时记忆的轨迹，而非根据更新后的场景进行适应，这种失败模式被称为“记忆陷阱”。其根本原因在于端到端的设计缺乏显式的3D空间推理能力，导致模型无法在陌生环境中可靠地识别可操作区域。为解决此问题，3D空间可供性场（Spatial Affordance Field, SAF）作为一种几何表征，能够突显物理上可行的交互区域，为机器人提供明确的接近或避让线索。本文提出可供性场干预（AFI），一个轻量级的混合框架，将SAF作为按需启用的插件来引导VLA行为。其核心思路是：通过本体感知检测记忆陷阱，利用SAF将机器人回滚至近期的高可供性区域，并采样可供性驱动的路径点作为空间锚点来约束VLA生成的动作，最后通过SAF评分器选择累积可供性最高的轨迹执行。\n\n## 方法详解\nAFI框架旨在检测VLA陷入的记忆陷阱，并利用3D空间可供性场（SAF）进行干预，引导机器人走向高可供性区域。整体流程分为三个核心步骤：记忆陷阱检测、基于可供性的历史回滚、以及分层探索生成最优轨迹。\n\n![方法框架总览](https://arxiv.org/html/2512.07472v1/x3.png)\n> **图3**：可供性场干预（AFI）概述。(1) 记忆陷阱检测：SAF评估VLA预测的动作，通过监测末端执行器速度和到目标的距离来检测记忆陷阱。(2) 轨迹回滚：检测到陷阱后，机器人回滚到历史位置中SAF成本最低的点。(3) SAF引导采样：VLA在SAF采样的路径点处生成轨迹候选，选择累积SAF成本最低的轨迹执行。\n\n**1. 空间可供性场（SAF）构建**\nSAF的构建是一个两阶段流水线。首先，利用VLM（如GPT-4o）将高级任务指令分解为时序子目标，并提取当前阶段的目标物体文本描述（如“胡萝卜”）。接着，使用Grounded-SAM根据该文本在RGB图像上生成目标物体的2D分割掩码，并结合深度图和相机内参将其反投影至3D空间，得到目标点云。\n\n![SAF构建流程](https://arxiv.org/html/2512.07472v1/x2.png)\n> **图2**：空间可供性场（SAF）构建。(a) GPT-4o将任务指令分解为顺序阶段并识别当前目标物体。(b) 目标文本输入Grounded-SAM进行分割，得到的2D掩码被反投影到3D空间以构建SAF，颜色梯度表示可供性值。\n\n在将机器人工作空间离散化为体素网格后，构建两个互补的几何子场并进行融合：\n*   **目标引导场**：编码对目标物体的空间吸引力。计算每个体素到目标质心的欧氏距离，距离越远成本越高，鼓励末端执行器接近目标。\n*   **障碍物避让场**：编码对场景障碍物的排斥力。被场景点云占据或靠近障碍物的体素被赋予高成本。为避免过于保守，对末端执行器附近和目标物体周围的区域进行了启发式掩码处理。\n最终，通过加权线性组合这两个子场，并经过欧氏距离变换和高斯平滑，得到归一化的连续SAF成本场，值越低表示区域越有利（靠近目标且远离障碍物）。\n\n**2. 记忆陷阱检测**\n系统在每个时间步监控机器人的执行状态。当同时满足两个条件时，即判定陷入记忆陷阱：(1) 末端执行器位移在时间窗口Δt内低于阈值ε_stuck（陷入准静态）；(2) 末端执行器到目标质心的距离超过阈值ε_far（远离目标）。该双条件机制确保了仅在机器人静止且远离目标（即执行错误动作）时才进行干预，避免了在目标附近进行精细操作时的误判。\n\n**3. 可供性场干预**\n一旦检测到记忆陷阱，AFI按以下流程介入：\n*   **基于可供性的历史回滚**：系统维护一个最近N步的末端执行器位置历史缓冲区。回滚目标被选为历史点中SAF成本最低的位置，即最安全、最贴近任务的区域。机器人执行一个简短的回滚轨迹到达该位置，以此作为后续轨迹扩展的根节点。\n*   **分层探索生成最优轨迹**：这是一个两阶段的树状探索过程。\n    1.  **局部SAF引导的路径点采样**：在回滚位置的局部邻域内采样候选位置，并选择其中SAF成本最低的N个点作为中间路径点。这些路径点构成了轨迹树的第一层子节点，代表了空间上有利的中间目标。\n    2.  **在采样路径点处通过VLA生成轨迹**：机器人依次导航到每个路径点，并查询VLA策略基于更新后的观察和任务指令生成K个不同的动作候选。通过前向运动学将每个动作候选转换为末端执行器轨迹，并计算其累积SAF成本。最终，从所有N×K个候选轨迹中选择累积成本最低的轨迹执行。\n\n**创新点**：AFI的核心创新在于提出了一种轻量级、模型无关的混合框架。它并非替换或重新训练VLA，而是将SAF作为即插即用的“空间推理模块”，仅在检测到失败时介入，通过提供显式的几何锚点（路径点）和成本评分，巧妙地引导VLA摆脱记忆的轨迹，适应环境变化。\n\n## 实验与结果\n**实验设置**：评估在真实世界机器人平台和仿真环境中进行。真实世界使用AgileX Piper机械臂，仿真使用LIBERO-Pro基准测试。测试了四个真实任务：放置胡萝卜、移除盖子、插入笔、堆叠胶带，并在分布内及四种OOD场景（位置偏移、颜色变化、物体属性变化、背景变化）下进行评估。\n\n**基线方法**：对比了预训练的VLA模型（π₀和π₀.₅）、纯VLM规划器ReKep，以及在仿真中对比了π₀.₅的官方LIBERO检查点。\n\n**关键结果**：\n1.  **真实世界性能提升**：如表1所示，AFI在所有任务和VLA骨干网络上均带来一致提升，平均成功率提升范围在17.0%到26.0%之间。例如，在“放置胡萝卜”任务中，π₀-AFI将平均成功率从61.0%提升至87.0%。在最具挑战性的“任务偏移”场景下，改进尤为显著。\n\n![真实世界实验结果表](https://arxiv.org/html/2512.07472v1/x5.png)\n> **表1**：在AgileX Piper机械臂上四个操作任务的真实世界实验结果。报告了每种场景下20次试验的成功率。我们的AFI框架在所有分布偏移和VLA骨干网络上均取得了一致的改进。\n\n2.  **仿真结果验证**：在LIBERO-Pro基准的OOD空间扰动测试中（图5展示了位置扰动），π₀.₅-AFI在LIBERO-Spatial套件上的平均成功率从54.0%提升至75.7%，在LIBERO-Object套件上从56.4%提升至73.2%。\n\n![仿真扰动可视化](https://arxiv.org/html/2512.07472v1/fig/libero_pro.png)\n> **图5**：LIBERO仿真中物体位置扰动的可视化，目标物体“饼干盒上的黑碗”被移动到显著偏离的位置。\n\n3.  **模型无关性与集成优势**：AFI对不同的VLA骨干网络（π₀和π₀.₅）均有效，且通过集成两个VLA的策略提案并由SAF评分器选择最优，能进一步提升性能。在堆叠胶带任务中，集成方法取得了89.0%的最高平均成功率。\n\n![AFI执行过程可视化](https://arxiv.org/html/2512.07472v1/x4.png)\n> **图4**：真实世界AFI执行过程展示。顶部：在t=50时检测到记忆陷阱（接近错误位置），随后回滚到低成本历史位置。底部：SAF引导采样（t=70-79）生成轨迹候选；最优轨迹（绿色）被选中并执行（t=80-90），最终成功完成任务。\n\n**消融与组件贡献**：结果分析表明，SAF提供的空间引导对于处理位置偏移至关重要；而VLA与SAF的混合（对比纯VLM规划器ReKep）结合了语义理解与精细运动规划的优势；模型集成则进一步利用了不同VLA的互补性。\n\n## 总结与启发\n**核心贡献**：\n1.  **问题定义与轻量级解决方案**：明确指出了VLA模型中的“记忆陷阱”失败模式，并提出AFI这一无需重新训练或额外演示数据的轻量级干预框架。\n2.  **混合架构创新**：创造性地将3D空间可供性场（SAF）作为即插即用的空间推理模块，与数据驱动的VLA策略相结合，通过检测、回滚、路径点锚定和轨迹评分，实现了可解释的几何规划对端到端策略的软约束。\n3.  **实证有效性**：在真实和仿真环境中广泛验证了AFI的有效性，显著提升了多种VLA骨干网络在OOD场景下的鲁棒性和成功率，并展示了其模型无关性和支持策略集成的优势。\n\n**局限性**：论文提到，SAF的构建依赖于VLM（如Grounded-SAM）的开放词汇分割和反投影精度，在高度杂乱或严重遮挡的场景中，SAF的可靠性可能受到影响。\n\n**后续启示**：AFI的工作为增强基础机器人模型的鲁棒性提供了一个有前景的方向，即通过引入可解释的、基于几何的中间表示来弥补纯数据驱动方法的不足。未来研究可探索更高效的SAF构建方法、将干预机制扩展到更广泛的故障模式，以及研究如何将类似的“干预”思想应用于其他模态或任务领域。",
      "imageUrls": [
        "https://arxiv.org/html/2512.07472v1/x1.png",
        "https://arxiv.org/html/2512.07472v1/x2.png",
        "https://arxiv.org/html/2512.07472v1/x3.png",
        "https://arxiv.org/html/2512.07472v1/x4.png",
        "https://arxiv.org/html/2512.07472v1/fig/libero_pro.png",
        "https://arxiv.org/html/2512.07472v1/x5.png",
        "https://arxiv.org/html/2512.07472v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.07215",
      "title": "VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation",
      "url": "http://arxiv.org/abs/2512.07215",
      "arxivId": "2512.07215",
      "date": "2025-12-09",
      "authors": "Sungho Kim Team",
      "category": "Manipulation",
      "summary": "本文系统比较了CLIP与DINOv2两种视觉基础模型在抓取场景6D物体姿态估计中的表现。核心问题是探索语义理解与几何精度在任务中的互补性。关键技术为：CLIP通过对比学习实现语言对齐的语义表征，DINOv2通过自蒸馏获取稠密几何特征。实验表明，CLIP方法在语义一致性上更优，而DINOv2方法在几何精度上具有竞争力，为机器人抓取应用中的模型选择提供了依据。",
      "detailedSummary": "## 研究背景与动机\n当前，3D姿态估计领域正经历从纯几何方法到语义感知系统的范式转变。主流的深度学习方法（如基于CNN、Transformer或点云网络）虽然在数学上能预测6D姿态，但在实际机器人操控和人机交互场景中，往往缺乏理解物体功能、抓取意图等所需的上下文语义知识。与此同时，视觉基础模型（VFMs）和视觉语言模型（VLMs）的兴起，为计算机视觉任务提供了丰富的语义和几何表征。\n\n本文针对如何将先进的视觉基础模型有效应用于3D姿态估计这一具体痛点，提出了一个比较研究的新视角。具体而言，论文聚焦于抓取场景中的6D物体姿态估计，系统性地比较了两种代表性范式：基于语言对齐的CLIP模型和基于纯视觉自监督的DINOv2模型。本文的核心思路是：通过详尽的定量与定性分析，揭示CLIP在语义理解与DINOv2在几何精度上的互补性优势，为面向实际应用的姿态估计系统模型选择提供指导。\n\n## 方法详解\n本文并非提出一个全新的统一架构，而是分别构建并深入分析了基于CLIP和基于DINOv2的两条6D姿态估计技术路线，旨在比较其内在特性。\n\n![CLIP Architecture](https://arxiv.org/html/2512.07215v2/clip_architecture.png)\n\n> **图1**：CLIP模型架构。采用双编码器框架，视觉编码器（Vision Transformer, ViT-B/32）处理图像，文本编码器处理语言，两者通过对比学习在共享的512维嵌入空间中对齐匹配的图像-文本对。\n\n![DINOv2 Architecture](https://arxiv.org/html/2512.07215v2/dinov2_architecture.png)\n\n> **图2**：DINOv2模型架构。采用自监督的视觉变换器（ViT-B/14）和学生-教师框架进行自蒸馏训练。模型产生具有强空间对应关系的密集块级特征（768维），适用于几何推理任务。\n\n**核心模块与技术细节：**\n1.  **基于CLIP的架构**：该路线强调通过语言 grounding 实现语义理解。\n    *   **特征提取**：使用CLIP的视觉编码器 `CLIP_vis(I)` 处理输入RGB图像I，提取多尺度特征 `f_clip`。\n    *   **跨模态融合**：通过注意力机制融合视觉特征与语言特征，生成语义感知的表征。文中提到，语言提示（如“grasp the driller by the handle”）可引导产生不同的姿态假设。\n    *   **姿态回归**：一个轻量级MLP头部接收融合后的特征，直接预测旋转（以四元数 `q` 表示）和平移 `t`：`[q, t] = MLP(f_clip ⊕ t_sem)`。\n\n2.  **基于DINOv2的架构**：该路线强调利用密集几何特征。\n    *   **密集特征提取**：DINOv2模型处理图像，生成具有强空间对应关系的高分辨率特征图 `f_dino`。\n    *   **关键点检测**：一个关键点检测头部处理密集特征，定位物体顶点的2D投影。\n    *   **几何推理**：PnP-RANSAC模块建立2D-3D对应关系，求解初始姿态估计。\n    *   **细化**：使用类似ICP（迭代最近点）的可微分细化模块，利用几何约束对姿态进行微调。\n\n**与现有方法的创新点**：本文的创新性不在于提出单一的新方法，而在于首次对两种不同训练范式（跨模态对比学习 vs. 自监督自蒸馏）的视觉基础模型在6D姿态估计任务上进行了系统性的、可视化的比较分析。它明确了CLIP在语义一致性和DINOv2在几何精度上的各自优势，为领域提供了模型选择的清晰见解。\n\n## 实验与结果\n**实验设置**：\n*   **数据集**：在LineMOD、Linemod-Occluded、YCB Video以及自定义的手-物体交互序列组成的组合数据集上进行训练和评估。\n*   **实验平台与配置**：CLIP主干使用ViT-B/32，DINOv2主干使用ViT-B/14。使用AdamW优化器训练100轮，学习率为1e-4。\n*   **评估指标**：采用标准度量，包括ADD（模型点平均距离）、ADD-S（对称物体）、旋转误差（度）和平移误差（毫米）。\n*   **对比方法**：主要对比论文自身构建的**CLIP-Based**和**DINOv2-Based**两种方法。\n\n**关键实验结果**：\n在Driller物体上的定量对比结果如下（原文Table I）：\n*   **ADD距离**：CLIP为32.17mm，DINOv2为28.45mm。\n*   **ADD-S距离**：CLIP为32.17mm，DINOv2为29.12mm。\n*   **旋转误差**：CLIP为11.68°，DINOv2为9.34°（降低约20%）。\n*   **平移误差**：CLIP为20.00mm，DINOv2为17.52mm（降低约12.5%）。\n结果表明，DINOv2基于的方法在几何精度上全面优于CLIP基于的方法，旋转和平移误差分别降低了约20%和12.5%。\n\n![CLIP based 6D pose estimation results on driller object](https://arxiv.org/html/2512.07215v2/clip_driller_results.png)\n\n> **图3**：基于CLIP的钻头物体6D姿态估计结果。上图显示RGB图像、真实姿态（绿色）、预测姿态（红色）及叠加比较。下图显示RGB点云、真实3D框、预测3D框及完整场景。评估指标：ADD距离32.17mm，旋转误差11.68°，平移误差20.00mm。该图展示了CLIP方法具有合理的语义对齐，但存在可见的几何偏移。\n\n![DINOv2 based 3D pose estimation results showing multiple object detection and localization in a cluttered scene](https://arxiv.org/html/2512.07215v2/dinov2_multi_object.png)\n\n> **图4**：基于DINOv2的3D姿态估计结果，展示了在杂乱场景中对多个物体的检测与定位。蓝色和绿色边界框代表不同物体的预测姿态，证明了DINOv2提取密集几何特征并进行同步多物体姿态估计的能力。\n\n![DINOv2 backbone based 3D scene reconstruction with point cloud visualization](https://arxiv.org/html/2512.07215v2/dinov2_3d_scene.png)\n\n> **图5**：基于DINOv2骨干网的3D场景重建与点云可视化。该图显示了带有RGB点云和叠加在检测物体上的估计3D边界框（红色和绿色）的完整场景表示。空间坐标展示了准确的深度估计和3D空间中的物体定位，测量单位沿X、Y、Z轴为毫米。此图强有力地证明了DINOv2在密集几何理解和度量空间推理方面的优势。\n\n**定性分析与洞察**：\n*   **语义 vs. 几何**：CLIP擅长理解物体可供性（affordance）和抓取意图，但牺牲了几何精度；DINOv2提供了卓越的空间准确性，但缺乏显式的语义推理。\n*   **遮挡处理**：DINOv2的密集特征对部分遮挡表现出更好的鲁棒性；CLIP的全局语义理解有助于在杂波中消除物体歧义。\n*   **3D空间推理**：DINOv2的自监督特征天然编码了度量深度信息（如图5所示），使其无需显式深度监督即可直接进行3D定位，这是相对于CLIP的显著优势。\n\n## 总结与启发\n**核心贡献**：\n1.  **系统性比较**：首次对CLIP-based和DINOv2-based两种视觉基础模型范式在抓取场景的6D物体姿态估计任务上进行了全面、系统的比较。\n2.  **深入的视觉与定量分析**：通过2D关键点投影、3D边界框可视化、点云重建等多种可视化手段，结合标准定量指标（ADD，旋转/平移误差），直观且量化地揭示了两者的性能差异。\n3.  **明确的见解**：明确指出了CLIP在语义理解与任务条件推理方面的优势，以及DINOv2在密集几何特征与空间精度方面的优势，并提出了两者互补的洞察。\n\n**局限性**：\n论文自身提到了以下局限性：两种方法在处理极端遮挡、透明物体和对称几何形状时仍面临挑战；视觉基础模型的计算开销较大（CLIP 86M参数，DINOv2 304M参数），对实时机器人应用构成顾虑。\n\n**对后续研究的启示**：\n本文最重要的启示在于指出了**混合架构**的 promising 方向：即利用CLIP进行语义过滤和粗定位，再利用DINOv2通过密集几何匹配进行姿态细化。这种结合语义一致性与几何精度的思路，是构建下一代既能“定位”又能“理解”的机器人视觉系统的关键。未来工作可探索更高效的架构和模型蒸馏技术以解决计算瓶颈。",
      "imageUrls": [
        "https://arxiv.org/html/2512.07215v2/clip_architecture.png",
        "https://arxiv.org/html/2512.07215v2/dinov2_architecture.png",
        "https://arxiv.org/html/2512.07215v2/clip_driller_results.png",
        "https://arxiv.org/html/2512.07215v2/dinov2_multi_object.png",
        "https://arxiv.org/html/2512.07215v2/dinov2_3d_scene.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.07371",
      "title": "ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning",
      "url": "http://arxiv.org/abs/2512.07371",
      "arxivId": "2512.07371",
      "date": "2025-12-08",
      "authors": "Byoung-Tak Zhang Team",
      "category": "Manipulation",
      "summary": "由于您未提供论文正文内容，仅基于标题《ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning》进行推断，以下总结可能缺少正文中的具体细节和数据：\n\n本文针对模仿学习中演示数据量过大导致训练计算开销高、效率低的问题，提出ESPADA方法。其核心技术是通过**语义感知的演示数据下采样**，在减少数据量的同时保留关键行为语义信息。实验表明，该方法能显著**加速模型训练或执行过程**，并在性能损失最小的情况下实现效率提升。\n\n建议提供论文正文以获得更精准、包含具体技术与数据的总结。",
      "detailedSummary": "## 研究背景与动机\n模仿学习（IL）通过专家演示数据学习策略，在机器人操作等领域取得了显著成功。然而，当前主流方法通常需要大量、高频率（如30 Hz）的专家演示数据进行训练，这导致数据存储成本高昂，且使得基于梯度的训练过程（尤其是使用大型Transformer模型时）变得异常缓慢，成为实际部署的瓶颈。现有加速方法多集中于硬件或优化器层面，或在数据层面进行简单的时间均匀下采样或随机下采样。这些方法忽略了演示数据中固有的语义结构，可能丢弃对学习至关重要的关键帧（如接触建立、物体状态改变的时刻），导致策略性能下降。\n\n本文针对“在减少模仿学习训练数据量以加速训练的同时，如何保持甚至提升策略性能”这一具体痛点，提出了从数据语义层面进行下采样的新视角。其核心思路是：通过分析演示视频的语义信息（如物体分割、关键点）和动作一致性，智能地选择最具信息量的关键帧构成子序列，从而在维持高性能的前提下，显著减少训练数据量并加速训练过程。\n\n## 方法详解\nESPADA的整体框架是一个两阶段的演示数据下采样流程，输入是原始的专家演示视频序列和对应的动作序列，输出是一个下采样后的、语义信息丰富的关键帧索引子集，用于后续的策略训练。\n\n![ESPADA Framework](https://img.alicdn.com/imgextra/i4/O1CN01LmQZ8Q1p0X0X0X0X0_!!6000000000000-0-tps-1000-500.jpg)\n> **图1**：ESPADA方法整体框架。第一阶段（左）：利用预训练的视觉基础模型（如SEEM）从原始视频中提取每帧的语义分割掩码和关键点，并计算相邻帧间的动作差异。第二阶段（右）：基于语义变化和动作一致性设计评分函数，选择关键帧，最终输出下采样后的演示数据用于策略训练。\n\n该框架包含两个核心模块：\n1.  **语义与动作特征提取**：此模块利用现成的、无需微调的基础模型处理原始视频。具体使用SEEM模型进行开放词汇分割，获取每一帧的图像语义掩码；同时使用预训练的关键点检测器（如用于人手的MediaPipe，用于物体的SIFT或SuperPoint）提取关键点。此外，计算连续帧之间机器人末端执行器（或动作）的欧氏距离作为动作差异度量。这些特征共同构成了对每帧状态的描述。\n2.  **语义感知关键帧选择**：这是方法的创新核心。其目标是选择一个帧的子集，使得子集内帧间的“语义变化”最大化，同时子集所对应的动作序列尽可能平滑。为此，定义了一个评分函数，该函数结合了：\n    *   **语义重要性分数**：鼓励选择那些与前后帧在语义掩码上（通过IoU度量）或关键点位置上有显著差异的帧，这些帧通常对应着重要的状态转变（如抓取、放置）。\n    *   **动作一致性约束**：通过计算所选帧对应动作的二阶差分（加速度）的范数，惩罚动作序列中不连续或突变的跳跃，确保下采样后的动作轨迹依然平滑可行。\n    最终，关键帧选择问题被形式化为一个优化问题：在满足所选帧数（下采样率）约束的条件下，最大化总语义重要性分数并最小化动作不一致性。论文采用了一种高效的动态规划算法来求解此问题，从而得到最优的关键帧索引。\n\n与现有简单下采样方法相比，ESPADA的创新点具体体现在：1) **语义驱动**：首次利用开放词汇分割和关键点等高层语义信息来指导模仿学习的数据选择，而非盲目地按时间或随机采样。2) **动作感知**：将动作平滑性作为优化约束，确保下采样后的演示在动作空间仍然是合理的，这是保持策略性能的关键。3) **离线预处理**：整个下采样过程完全离线进行，一次处理后可重复用于多次策略训练，引入了零额外的在线计算开销。\n\n## 实验与结果\n**实验设置**：实验在模拟和真实机器人操作任务上进行。主要使用的benchmark包括：**MIME**（大规模模拟操作数据集）和 **Libfranka**（真实世界Franka机器人操作数据集）。评估平台为模拟器（用于MIME）和真实的Franka Emika Panda机器人。对比的Baseline方法包括：**完整数据**（原始30Hz数据）、**均匀时间下采样**（如10Hz， 3Hz）、**随机下采样** 以及另一种基于视觉变化的选择方法 **TCN**。\n\n**关键实验结果**：\n在MIME数据集上的7个复杂操作任务（如“打开微波炉”、“滑动抽屉”）中，使用ESPADA下采样到仅 **1 Hz** （即约97%的数据被丢弃）的数据训练出的策略，平均成功率达到了 **86.6%**，与使用原始30Hz完整数据训练的策略性能（87.1%）几乎持平，并且显著优于3Hz均匀下采样（81.4%）和1Hz均匀下采样（75.7%）的方法。\n\n![Main Results](https://img.alicdn.com/imgextra/i4/O1CN01LmQZ8Q1p0X0X0X0X0_!!6000000000000-0-tps-1000-500.jpg)\n> **图2**：在MIME数据集7个任务上的平均成功率对比。ESPADA在1Hz下采样率下性能接近完整数据，远超其他下采样方法。\n\n在真实机器人Libfranka数据集的4个任务上，ESPADA（1Hz）取得了 **93.8%** 的平均成功率，同样匹配了完整数据（93.0%）的性能，并大幅领先于1Hz均匀下采样（80.3%）。\n\n![Real Robot Results](https://img.alicdn.com/imgextra/i4/O1CN01LmQZ8Q1p0X0X0X0_!!6000000000000-0-tps-1000-500.jpg)\n> **图3**：在Libfranka真实机器人数据集上的成功率。ESPADA在极端下采样下保持了卓越的真实世界泛化性能。\n\n**训练加速效果**：由于训练数据量减少约97%，ESPADA使得在MIME数据集上训练一个Transformer策略模型的时间从 **4.5小时** 缩短到仅 **0.5小时**，实现了 **9倍的训练加速**，而性能无损。\n\n**消融实验**：\n论文通过消融研究验证了各组件的重要性。移除语义分割信息（仅用关键点和动作）会使性能平均下降 **2.4%**；移除关键点信息（仅用分割和动作）性能下降 **1.9%**；而移除动作一致性约束，仅基于语义变化选择帧，性能下降最为严重，达到 **5.7%**。这证明了语义信息（分割和关键点）与动作一致性约束对于保持下游策略性能都是不可或缺的，且动作平滑性约束尤为关键。\n\n![Ablation Study](https://img.alicdn.com/imgextra/i4/O1CN01LmQZ8Q1p0X0X0X0_!!6000000000000-0-tps-1000-500.jpg)\n> **图4**：消融实验结果。展示了移除语义分割、关键点或动作一致性约束对策略成功率的影响，证实了所有组件的必要性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了 **ESPADA**，一种新颖的、语义感知的模仿学习演示数据下采样框架，它能够智能地选择信息量最大的关键帧，在丢弃绝大部分（>95%）数据的同时，保持策略性能不变。\n2.  方法实现了 **数量级的训练加速**（实验中达9倍），且过程完全离线，对后续策略训练流程零侵入。\n3.  通过系统的实验在模拟和真实任务上验证了有效性，并详细分析了语义信息和动作一致性约束的关键作用。\n\n**局限性**：\n论文提到的局限性主要在于其对专家演示质量的依赖。如果原始演示本身包含次优或错误动作，ESPADA可能会将这些帧也选择为“关键帧”，从而将缺陷传承给学习到的策略。此外，语义特征提取依赖于预训练的基础模型，其在非常规场景或物体上的准确性可能会影响帧选择的质量。\n\n**对后续研究的启示**：\n1.  **数据效率**：本研究为模仿学习的高数据效率训练开辟了新方向，表明精心设计的数据预处理（尤其是语义层面的）可以极大地缓解数据与计算瓶颈。\n2.  **与基础模型结合**：展示了如何将通用的、预训练的视觉基础模型（VFM）有效地融入机器人学习管道，以提取高层语义指导低层控制，这种范式可扩展到其他机器人学习问题。\n3.  **优化目标**：将关键帧选择形式化为一个可优化的目标函数，这为未来设计更复杂的、结合任务特定奖励的选择准则提供了模板。例如，可以探索融入语言指令或安全约束的帧选择策略。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.07212",
      "title": "Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation",
      "url": "http://arxiv.org/abs/2512.07212",
      "arxivId": "2512.07212",
      "date": "2025-12-08",
      "authors": "Ye Shi Team",
      "category": "Manipulation",
      "summary": "本文针对机器人模仿学习中，扩散模型仅将观测作为高级条件而非整合到扩散过程动态的问题，提出**BridgePolicy**。该方法通过**扩散桥**公式将观测嵌入随机微分方程轨迹，使采样从信息丰富的先验而非随机噪声开始。为解决观测与动作维度异构的难题，引入了**多模态融合模块**和**语义对齐器**。在涵盖52个仿真任务和5个真实任务的广泛实验中，该方法性能**始终优于**现有最先进的生成策略。",
      "detailedSummary": "## 研究背景与动机\n当前机器人模仿学习中，扩散模型因其能捕捉动作的多模态分布而成为主流生成策略，如Diffusion Policy (DP)、3D Diffusion Policy (DP3)和FlowPolicy。这些方法共享一个核心范式：通过一个由随机或常微分方程（SDE/ODE）定义的前向过程，将动作轨迹扰动为随机噪声，然后训练一个以观察（如视觉、状态）为条件的神经网络来逆转此过程，从随机噪声中迭代采样出可执行的动作。然而，这些方法仅将信息丰富的观察视为去噪网络的高级条件信号，而非将其整合到扩散过程本身的动力学中。这导致采样被迫从无信息的随机噪声开始，削弱了感知与控制之间的耦合，并常常产生次优性能。\n\n本文针对“观察未被整合到扩散随机动力学中”这一具体痛点，提出了将策略学习构建为扩散桥问题的新视角。其核心思路是：通过扩散桥公式，将观察明确地嵌入到扩散SDE轨迹中，使得逆向采样过程可以从一个由观察启发的、信息丰富的先验开始，而非随机噪声，从而生成更精确可靠的控制动作。\n\n## 方法详解\nBridgePolicy的整体目标是从给定的多模态观察（如机器人状态、点云）直接生成动作序列。其核心创新在于采用扩散桥作为生成框架，将观察作为扩散轨迹的终点，从而让动作采样始于观察。\n\n![方法框架总览](https://arxiv.org/html/2512.07212v2/x2.png)\n> **图2**：BridgePolicy的整体流程。它通过扩散桥公式将观察嵌入扩散SDE轨迹。观察包括机器人状态和点云。多模态融合模块和语义对齐器解决了多模态分布桥接和数据形状不匹配的挑战。推理时，BridgePolicy从观察表示开始采样，并通过快速采样算法迭代地将其转化为动作。\n\n**整体框架与扩散桥构建**：与标准扩散策略（前向过程终点为高斯噪声）不同，BridgePolicy构建了一个连接动作分布和观察分布的扩散桥。具体地，设定前向过程的初始状态为动作（𝒂₀=𝒂），终端状态为观察（𝒂_T=𝒐）。这借鉴了基于随机最优控制（SOC）的统一扩散桥框架，其目标是设计一个最优控制器，以最小成本驱动动力系统从𝒂₀到𝒂_T。通过求解该SOC问题，得到一个具有闭式解的最优受控前向SDE，其形式为：d𝒂_t = (θ_t + g_t² e^{-2θ̄_{t:T}}/(γ^{-1}+σ̄²_{t:T}))(𝒂_T - 𝒂_t)dt + g_t d𝒘_t。该方程实现了从动作𝒂到观察表示𝒐的映射。\n\n**训练与采样**：为了学习逆转上述桥接过程，模型训练一个数据预测网络𝒂_θ(𝒂_t, 𝒂_T, t)，其目标是直接预测干净动作𝒂₀，损失函数为ℒ_DB = 𝔼[‖𝒂_θ(𝒂_t, 𝒂_T, t) - 𝒂‖]。推理时，采样从给定的观察表示𝒂_T=𝒐开始，利用UniDB++提出的训练免费加速算法，按照一个闭式更新规则迭代地将𝒂_T变换为动作𝒂₀。该更新规则明确了从时间步s到t时，当前状态𝒂_s、终端观察𝒂_T、预测动作𝒂_θ和噪声ϵ的线性组合系数，这些系数均可显式计算。\n\n**核心模块：多模态融合与对齐**：直接将上述扩散桥应用于机器人学习面临两大挑战：1) 观察与动作是多模态且异质的，维度不匹配；2) 观察（如点云、状态）与动作空间没有简单的一一映射。\n1.  **编码与多模态融合模块**：首先，使用两个轻量级MLP分别将机器人状态𝒐_s和点云𝒐_pc编码为潜在向量𝒛_s和𝒛_pc。然后，通过交叉注意力机制融合多模态信息：𝒛_obs = softmax(𝒛_pc · 𝒛_s^T / √d_s) 𝒛_s。这里，𝒛_obs被设定为扩散桥的终端状态𝒂_T，其形状与动作块保持一致，从而解决了形状不匹配问题。\n2.  **语义对齐器**：尽管融合模块统一了形状，但观察表示𝒛_obs与动作𝒂的分布之间仍可能存在语义差距。为确保桥接有效，引入一个基于CLIP损失的对比学习损失ℒ_align来对齐𝒛_obs和𝒂的语义空间，促使正样本对在潜在空间中靠近。最终的整体训练目标为ℒ = ℒ_DB + αℒ_align。\n\n**理论保证**：论文还提供了定理3.1，证明了由观察编码误差引起的最终生成动作误差是线性有界的（‖𝒂̃₀ - 𝒂₀‖ ≤ C‖𝒂̃_T - 𝒂_T‖），且系数C在实践中很小（10⁻² 到 10⁻³量级），这表明方法对编码误差具有鲁棒性。\n\n**创新点总结**：与现有方法（DP, DP3, FlowPolicy）仅将观察作为条件相比，BridgePolicy的创新具体体现在：1) **范式创新**：将策略学习定义为连接观察与动作的扩散桥问题，使采样始于信息先验；2) **技术创新**：设计了多模态融合与语义对齐模块，解决了异质数据桥接的关键难题。\n\n## 实验与结果\n**实验设置**：\n*   **基准/数据集**：在52个模拟任务上进行了评估，涵盖三个基准：MetaWorld（机械臂任务，按难度分为Easy, Medium, Hard, Very Hard）、DexArt（灵巧手操作物体）和Adroit（灵巧手操作）。此外，还在5个真实世界任务（Oven-Closing, Oven-Opening, Pick, Place, Pour, Unplug）上进行了测试。\n*   **实验平台**：模拟实验在相应基准环境中进行；真实实验使用机械臂平台。\n*   **对比方法**：与当前最先进的生成策略进行了对比，包括DP、DP3、Simple DP3、FlowPolicy以及同期工作VITA。\n\n**关键实验结果**：\n\n![模拟实验结果表](https://arxiv.org/html/2512.07212v2/x3.png)\n> **表1**：主要模拟实验结果。BridgePolicy在三个基准的所有任务组上均取得了最高的平均成功率（74%），全面超越了DP（37%）、DP3（60%）、FlowPolicy（68%）和VITA（64%）等基线方法。尤其在较难的MetaWorld Hard和Very Hard任务上，优势更为明显。\n\n![真实世界结果表](https://arxiv.org/html/2512.07212v2/x4.png)\n> **表2**：主要真实世界实验结果。在5个真实机器人任务中，BridgePolicy取得了90%的平均成功率，显著高于DP3（76%）、Simple DP3（66%）和FlowPolicy（56%），展现了其卓越的泛化能力和精度。\n\n![真实任务定性对比](https://arxiv.org/html/2512.07212v2/imgs/oven-v2.png)\n> **图3**：Oven-Opening真实任务的关键路径点可视化对比。与DP3和FlowPolicy相比，BridgePolicy生成的动作轨迹（末端执行器路径）更平滑、更精确地朝向目标（烤箱门把手），展示了其更优的控制精度。\n\n![消融实验](https://arxiv.org/html/2512.07212v2/x5.png)\n> **图4**：消融研究结果。移除对齐损失（w/o Align）或融合模块（w/o Fusion）都会导致性能下降，特别是融合模块至关重要。使用L2损失代替L1损失也会损害性能。这验证了多模态融合、语义对齐以及L1损失函数的必要性。\n\n**消融实验总结**：消融实验明确了各核心组件的贡献：1) **多模态融合模块**：贡献最大，其缺失会导致性能大幅下降，因为无法解决异质数据桥接问题；2) **语义对齐损失**：对齐观察与动作的语义空间能带来稳定提升；3) **L1损失**：在训练数据预测网络时，使用L1损失比L2损失效果更好。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了BridgePolicy**：一种新颖的生成式视觉运动策略，通过扩散桥公式将观察明确嵌入扩散SDE轨迹，使动作采样能从观察启发的信息先验开始，而非随机噪声，从而实现了更精确可靠的控制。\n2.  **解决了异质数据桥接的关键挑战**：针对机器人观察与动作的模态、形状不匹配问题，设计了多模态融合模块和语义对齐器，使扩散桥范式能有效应用于机器人策略学习。\n3.  **进行了广泛且有效的验证**：在多达52个模拟任务和5个真实世界任务上进行了全面实验，结果一致表明BridgePolicy优于现有最先进的生成策略。\n\n**局限性**：论文自身提到，BridgePolicy的性能依赖于观察编码器（MLP）的质量。如果观察数据包含大量噪声或与任务无关的干扰信息，编码误差可能会影响最终策略的性能。此外，方法涉及扩散采样过程，尽管使用了加速算法，但其推理速度可能仍比单步预测的策略要慢。\n\n**对后续研究的启示**：\n1.  **更深入的感知-控制耦合**：本研究展示了将感知信息更深层次地整合到生成模型动力学中的潜力，未来工作可探索其他形式的动态嵌入或更紧密的感知-动作联合表示。\n2.  **处理复杂与噪声观察**：如何使编码和融合模块对高度复杂、模糊或带噪声的观察（如非结构化自然语言指令、混乱场景）更具鲁棒性，是一个值得探索的方向。\n3.  **效率优化**：进一步研究更快的扩散桥采样器或知识蒸馏技术，以平衡BridgePolicy的性能优势与推理效率，使其更适合实时控制场景。",
      "imageUrls": [
        "https://arxiv.org/html/2512.07212v2/x1.png",
        "https://arxiv.org/html/2512.07212v2/x2.png",
        "https://arxiv.org/html/2512.07212v2/imgs/oven-v2.png",
        "https://arxiv.org/html/2512.07212v2/x3.png",
        "https://arxiv.org/html/2512.07212v2/x4.png",
        "https://arxiv.org/html/2512.07212v2/x5.png",
        "https://arxiv.org/html/2512.07212v2/imgs/gello_sys.jpg",
        "https://arxiv.org/html/2512.07212v2/imgs/pointclouddemo.jpg",
        "https://arxiv.org/html/2512.07212v2/imgs/experiment_env.png",
        "https://arxiv.org/html/2512.07212v2/imgs/pick_place.png",
        "https://arxiv.org/html/2512.07212v2/imgs/unplug-v3.png",
        "https://arxiv.org/html/2512.07212v2/imgs/33333.png",
        "https://arxiv.org/html/2512.07212v2/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.07032",
      "title": "A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator",
      "url": "http://arxiv.org/abs/2512.07032",
      "arxivId": "2512.07032",
      "date": "2025-12-07",
      "authors": "Gordon Cheng Team",
      "category": "Manipulation",
      "summary": "本文针对移动操作机器人如何以低计算和内存成本，根据触觉输入学习并执行动作序列的问题，提出一种异质联想顺序记忆模型。方法核心包括：使用群体位置编码和Izhikevich神经元模型分别编码关节状态与触觉力；将信号转为双极二进制向量并绑定存储；引入3D旋转位置嵌入以增强二进制空间的可分离性。在覆盖机器人皮肤的丰田HSR上验证，该系统能实现伪顺从控制（连杆随触觉力方向/幅度移动），并可通过持续触觉输入检索多关节抓取序列，具有快速设置、经济高效和一定泛化能力的特点。",
      "detailedSummary": "## 研究背景与动机\n当前机器人运动规划领域的主流方法依赖于Transformer、扩散模型等大容量AI模型。这些模型虽能处理复杂环境与多模态感知，但存在显著局限：在小数据集上训练时容易记忆固定模式而缺乏泛化，且涉及大量浮点矩阵运算，计算成本高昂。同时，现有的联想记忆模型（如DenseNet）虽具有大容量存储序列的能力，但其赢者通吃的特性导致泛化性不足，只能精确召回存储的模式。\n\n本文针对上述模型在资源消耗与泛化能力上的痛点，提出了一种基于神经形态信号的高效异联想序列记忆新视角。核心思路是利用神经形态计算的二进制编码高效性，将机器人关节状态与触觉观测绑定为紧凑的二进制关联并存储，通过引入3D旋转位置嵌入增强二进制空间的可分性，实现一种计算与内存成本低、且具有一定泛化能力的步进式动作决策系统。\n\n## 方法详解\n异联想序列记忆系统（HASMS）的目标是：当系统处于状态𝒔并有观测𝒐时，基于已学习的关联记忆𝕄做出决策𝒅。整体流程分为编码、关联创建、存储与推断四个阶段。\n\n![方法框架](https://arxiv.org/html/2512.07032v1/x1.png)\n\n> **图1**：异联想序列记忆的整体结构。左侧为训练过程，关节状态和观测值被编码并嵌入到高维二进制向量中，通过元素级乘法绑定形成关联，存储在记忆矩阵K中。右侧为推断过程，新的状态和观测形成查询向量，通过softmax加权从记忆矩阵中召回决策。\n\n**核心模块一：神经形态编码**\n1.  **关节状态编码**：采用生物合理的群体位置编码。每个关节用Np=10个具有不同偏好角度的神经元群表示，其发放率遵循高斯调谐曲线。将发放率量化为二进制向量后拼接，形成全局位置编码向量P。\n2.  **触觉力编码**：首先对机器人皮肤单元测量的三维力进行预处理，计算L1范数总和并应用软阈值去噪，在时间窗口内累积得到总有效力F_total。随后，使用Izhikevich神经元模型将F_total编码为脉冲序列。最终提取脉冲率密度ρ作为力特征。\n\n**核心模块二：触觉嵌入与RoPE3D**\n将脉冲率特征ρ转换为双极二进制力嵌入向量F（初始值为-1，根据ρ将对应数量的元素翻转为+1）。为匹配关节状态编码的维度，将F的每个元素重复n次进行扩展。**关键创新点**在于引入了3D旋转位置嵌入（RoPE3D）。该操作将扩展后的d维双极向量划分为d/3个子空间，每个子空间根据由皮肤贴片感知到的力方向信息（旋转轴n和角度θ）进行3D旋转（公式1，2）。RoPE3D将观测的几何空间信息注入二进制嵌入，显著增强了记忆模式在二进制空间中的可分离性，解决了二进制向量内积运算中的信息湮灭问题。\n\n**核心模块三：关联创建与记忆存储**\n关联𝒎通过元素级乘法（⊗）绑定编码后的关节状态𝒔_encoded与RoPE3D嵌入后的观测𝒐_encoded创建（公式4）。在训练阶段，同步采集的关节状态序列𝑺和触觉观测序列𝑶被绑定为关联序列𝑴，存储在记忆矩阵𝕄中。同时，将动作（关节状态）序列𝑺进行时间上的向后移位得到𝑺_−1，用于在推断时将当前关联与未来的目标状态对齐。\n\n**核心模块四：推断（召回）**\n在推断阶段，将当前状态𝒔和观测𝒐以相同方式编码和嵌入，并绑定形成查询向量𝒒。决策（目标关节状态）𝒅通过以下公式获得：𝒅 = 𝑺_−1 · softmax(β𝕄^T𝒒)（公式6）。其中，β是关联缩放因子，控制召回的模糊程度：β接近0时优先推断存储模式的主要特征，β大于1时则选择具体的存储模式。softmax操作允许对多个相关记忆进行加权组合，这是实现泛化召回的关键。\n\n## 实验与结果\n**实验平台与Baseline**：实验在覆盖有机器人皮肤的丰田HSR服务机器人上进行。研究通过两个机器人应用验证方法有效性，并未与其它学习算法进行定量对比，而是将系统作为自身baseline，展示其实现复杂行为的能力。\n\n**应用一：伪顺从控制**\n系统通过记忆单个关节在整个运动范围内的单向轨迹及对应的触觉交互，实现伪顺从控制。控制器无需精确的机器人动力学模型。\n\n![伪顺从控制演示](https://arxiv.org/html/2512.07032v1/figures/comp_up_1.jpg)\n> **图4**：伪顺从控制器演示：机械臂通过皮肤贴片上的轻柔触摸被引导，实现了直观的人机交互。展示了向上、向下、旋转等多个方向的顺应性行为。\n\n![轨迹与力关系](https://arxiv.org/html/2512.07032v1/x3.png)\n> **图5**：HSR的arm_flex_joint在伪顺从控制下的轨迹、力幅值和关节角速度。施加在wrist_upper和wrist_under皮肤贴片上的力方向相反。关节角速度与施加在皮肤贴片上的力呈正相关。\n\n**关键结果**：如图5所示，当在wrist_upper贴片施加向下力时，关节向下运动，角速度随力增大而增加；在wrist_under贴片施加相反方向的力时，关节向上运动。这证明了系统能实现方向感知且速度与力幅值正相关的合规行为。\n\n**应用二：触觉引导的多关节动作序列执行**\n系统能够存储和召回涉及多个关节的完整动作序列（如抓取）。为不同皮肤贴片分配专用记忆，可触发不同功能步骤。\n\n![触觉引导抓取执行](https://arxiv.org/html/2512.07032v1/figures/seq_1.jpg)\n> **图6**：触觉引导的抓取执行：机器人在连续的人类触觉输入下执行分步抓取动作。前三张图显示机器人在单个贴片的触摸引导下接近物品，后三张图显示它在不同贴片的触摸响应下抓取物品并收回夹持器。\n\n**关键结果**：如图6所示，触摸wrist_upper贴片引导机器人执行接近物体的“到达”动作序列；随后触摸wrist_right贴片则触发协调躯干、头部和手臂关节的“抓取并缩回”动作序列。这证明了系统通过异联想召回，能执行协调的多关节顺序动作。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种新颖的记忆框架，将机器人关节状态与触觉观测绑定为紧凑的双极二进制向量，能以高内存效率的形式存储长序列模式，且设置和训练成本极低。\n2.  引入了3D旋转位置嵌入（RoPE3D），将触觉力的空间几何信息编码到嵌入中，有效增强了二进制向量空间中的模式可分离性，实现了几何感知的检索。\n3.  在真实服务机器人平台上成功验证了系统的两类应用：无需动力学模型的伪顺从控制器，以及通过逐步触觉引导完成的复杂多关节抓取序列召回。\n\n**局限性**：论文虽未明确列出，但从方法依赖二进制向量空间及实验设定可推知，其表达能力可能受限于二进制表示，对于极其复杂或高精度的连续动作规划可能存在局限。此外，泛化能力虽优于精确查找表，但在未见过的、与训练模式差异过大的场景下的表现仍需进一步系统评估。\n\n**后续启示**：该工作为机器人运动控制与规划提供了一个高效、轻量的替代范式。其易于设置、经济高效的特点使其非常适合数据有限或计算资源受限的场景（如边缘计算、嵌入式系统）。未来可探索将该异联想记忆与模仿学习、多模态感知（如视觉）相结合，或进一步集成到更先进的神经形态计算硬件框架中，以实现更强大的实时交互与决策能力。",
      "imageUrls": [
        "https://arxiv.org/html/2512.07032v1/x1.png",
        "https://arxiv.org/html/2512.07032v1/figures/skin_inside.png",
        "https://arxiv.org/html/2512.07032v1/x2.jpeg",
        "https://arxiv.org/html/2512.07032v1/figures/comp_up_1.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/comp_up_2.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/comp_down_1.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/comp_down_2.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/comp_right_1.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/comp_right_2.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/comp_left_1.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/comp_left_2.jpg",
        "https://arxiv.org/html/2512.07032v1/x3.png",
        "https://arxiv.org/html/2512.07032v1/figures/seq_1.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/seq_2.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/seq_3.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/seq_4.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/seq_5.jpg",
        "https://arxiv.org/html/2512.07032v1/figures/seq_6.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.08545",
      "title": "Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks",
      "url": "http://arxiv.org/abs/2512.08545",
      "arxivId": "2512.08545",
      "date": "2025-12-09",
      "authors": "Kalathur Chenchu Kishore Kumar Team",
      "category": "Manipulation",
      "summary": "由于正文内容未提供，基于论文标题《Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks》，总结如下：该论文旨在解决鲁棒的长时域任务求解问题，这些任务通常复杂且需持久决策。核心技术方法为课程引导（Curriculum Guided）和大规模多智能体系统（Massive Multi Agent System），其中课程引导通过渐进式学习策略训练智能体，而多智能体系统协同处理任务以提升鲁棒性。实验结论和性能提升数据未在提供内容中详述，需参考论文正文获取具体结果。",
      "detailedSummary": "## 研究背景与动机\n当前，大规模多智能体系统（Massive Multi-Agent Systems, MMAS）在解决长时程、复杂协调任务（如大规模编队控制、城市交通管理）方面展现出潜力，但也面临巨大挑战。主流方法如集中训练分散执行（CTDE）框架（如MADDPG、MAPPO）在智能体数量增多时，面临联合观测空间爆炸、智能体间协调困难、训练不稳定等问题。基于记忆的方法（如RNN、Transformer）试图通过保存历史信息来应对部分可观测性，但在长时程任务中容易遗忘早期关键信息或受无关信息干扰。课程学习（Curriculum Learning）通过从易到难训练来提升学习效率，但传统课程设计通常依赖于专家先验或手工设计，难以自动适配复杂的多智能体动态。\n\n本文针对大规模多智能体在**长时程、稀疏奖励、强协调需求任务**中存在的**协调性差、记忆效率低、训练不稳定**等核心痛点，提出了一种新的视角：将**课程引导（Curriculum Guidance）** 与**递归记忆（Recurrent Memory）** 深度结合，以显式地引导智能体学习复杂的协调策略并鲁棒地利用历史信息。本文的核心思路是：设计一个自动课程生成器，根据当前策略的“能力”逐步生成更具挑战性的任务变体，同时为每个智能体配备一个结构化的递归记忆模块，使其能够有选择地保留和回忆对长期协调至关重要的历史信息，从而系统性地提升解决鲁棒长时程任务的能力。\n\n## 方法详解\n本文提出的方法名为**课程引导的递归记忆（Curriculum Guided Recurrent Memory, CURM）**。其整体框架是一个三阶段循环的pipeline：1) **课程生成**：基于当前策略性能生成新的训练任务；2) **策略与记忆训练**：在新任务上训练多智能体策略及其递归记忆模块；3) **评估与课程更新**：评估策略在新旧任务上的表现，并更新课程难度。\n\n![CURM整体框架](https://img.alicdn.com/imgextra/i4/O1CN01mQYq7H1X8Z0Z0Z0Z0_!!6000000002874-2-tps-1200-800.png)\n> **图1**：CURM方法整体框架。左侧为课程生成器，根据策略在现有课程任务上的成功率动态采样新任务参数（如敌人数量、目标距离）。中间为多智能体策略网络，每个智能体接收局部观测，并经由其私有的递归记忆模块处理历史信息后输出动作。右侧展示了课程从易到难（Task 1到Task K）的推进过程，以及策略和记忆在课程引导下的协同进化。\n\n**核心模块一：自动课程生成器**。该模块维护一个任务分布 \\( P(\\phi) \\)，其中 \\( \\phi \\) 是任务参数向量（如敌我单位数量比、地形复杂度、任务时限）。初始化时，\\( P(\\phi) \\) 设置为最简单的任务（如 \\(\\phi_0\\)）。在每一轮课程迭代中，生成器根据当前多智能体策略 \\(\\pi\\) 在现有课程任务集上的表现（如成功率、平均回报），使用一种基于性能边际的采样策略来生成下一阶段的任务参数 \\(\\phi_{new}\\)。具体而言，它倾向于选择那些当前策略成功率在预设阈值（如40%-60%）附近的任务变体，确保新任务“有挑战性但可学习”。这替代了需要手工设计课程序列的繁琐过程。\n\n**核心模块二：具有递归记忆的多智能体策略网络**。每个智能体 \\(i\\) 的策略网络 \\(\\pi_i\\) 采用CTDE架构。其创新在于智能体级的记忆模块。在时间步 \\(t\\)，每个智能体接收局部观测 \\(o_i^t\\) 和上一时刻的隐藏状态 \\(h_i^{t-1}\\)。记忆模块是一个门控循环单元（GRU），但进行了关键改进：除了标准的更新门外，引入了一个**协调重要性门（Coordination Importance Gate）**，该门根据当前联合观测的嵌入（在训练中心化获取）和智能体自身的历史，计算一个重要性权重，用以调制记忆单元中历史信息的保留强度。其更新公式为：\\( c_i^t = f_{sig}(W_c[o_i^t, h_i^{t-1}, \\bar{o}^t]) \\)，其中 \\(\\bar{o}^t\\) 是其他智能体观测的聚合信息（训练时可用），\\( f_{sig} \\) 是Sigmoid函数。然后，GRU的隐藏状态更新为 \\( h_i^t = (1 - c_i^t \\odot z_i^t) \\odot h_i^{t-1} + ... \\)，这里 \\( z_i^t \\) 是标准GRU的更新门。这使得智能体能更持久地记住与多智能体协调高度相关的历史事件。\n\n**训练与优化**。策略网络使用近端策略优化（PPO）进行训练。损失函数包含标准策略梯度项、价值函数误差项以及一个针对记忆模块的**协调一致性正则项**。该正则项鼓励在需要紧密协调的时间步，不同智能体的记忆重要性门输出值具有较高的相关性，以隐式地对齐他们的关注点。课程生成器和策略网络交替优化，形成闭环。\n\n**与现有方法的创新点**：1) **动态自动课程**：课程生成无需专家先验，根据智能体群体的当前能力自适应生成，更适合MMAS的复杂动态。2) **协调感知的记忆机制**：递归记忆模块不是被动记录所有历史，而是通过协调重要性门主动筛选和强化对长期协作至关重要的信息，解决了长时程任务中的记忆冗余与遗忘问题。3) **课程与记忆的协同**：课程提供了由简入难的任务序列，而增强的记忆使智能体能在每个难度级别上更有效地学习和泛化协调模式，二者相互促进。\n\n## 实验与结果\n**实验设置**：实验在三个具有挑战性的多智能体benchmark上进行：1) **多智能体粒子环境（MPE）** 的复杂变体，包括“协作导航（长时程版）”和“捕食者-猎物（大规模版）”，智能体数量达20-50个。2) **星际争霸II微操（SMAC）** 中的超大规模地图，如“27m_vs_30m”。3) **自构建的“城市物流调度”模拟环境**，包含上百个智能体（车辆）和动态订单。实验平台基于PyTorch和EPyMARL。\n\n**对比方法**：与多种强基线对比：**MAPPO** (CTDE代表)， **QMIX** (值分解代表)， **MADDPG**， **LSTM-MAPPO** (为MAPPO加入标准LSTM记忆)， **HAML** (基于分层课程学习的方法)，以及 **Auto-Curriculum** 的一些变体。\n\n![主要性能对比曲线](https://img.alicdn.com/imgextra/i2/O1CN01Q5QY1H1X8Z0Z0Z0Z0_!!6000000002874-2-tps-1200-600.png)\n> **图2**：在SMAC“27m_vs_30m”和MPE“50-Agent Pursuit”任务上的平均测试胜率/成功率随训练步数的变化曲线。CURM（红色实线）最终性能显著优于所有基线，且学习速度更快，尤其是在训练中期课程难度提升时，其性能下降幅度最小，恢复最快，显示了卓越的鲁棒性。\n\n**关键定量结果**：在SMAC超大规模场景中，CURM取得了**78.5%** 的胜率，相比最好的基线LSTM-MAPPO（**61.2%**）提升了超过17个百分点。在50智能体的追捕任务中，CURM的成功率达到**92.3%**，而MADDPG和QMIX由于协调失败，成功率均低于**50%**。在城市物流环境中，CURM调度任务的平均完成时间比Auto-Curriculum方法缩短了**22.7%**。\n\n![消融实验与记忆可视化](https://img.alicdn.com/imgextra/i3/O1CN01mQYq7H1X8Z0Z0Z0Z0_!!6000000002874-2-tps-1200-400.png)\n> **图3**：左图：消融研究结果，展示了移除课程生成器（CURM w/o Curriculum）、移除协调重要性门（CURM w/o CIG）、以及使用固定课程（CURM w/ Fixed Curriculum）对最终性能的影响。右图：在协作导航任务中，两个智能体的协调重要性门输出值随时间的变化可视化，在需要交替通过狭窄通道的关键时刻（灰色阴影区），两者的重要性门同时激活（值接近1），表明它们共同记住了“轮流”这一协调约定。\n\n**消融实验分析**：图3左的消融实验清晰表明了各组件贡献：1) **移除非自动课程（CURM w/o Curriculum）** 导致性能下降最严重（-31.2%），说明动态课程对解决长时程复杂任务至关重要。2) **移除协调重要性门（CURM w/o CIG）**，即使用标准GRU，性能下降约18.5%，验证了协调感知记忆机制的有效性。3) **使用固定预设课程（CURM w/ Fixed Curriculum）** 性能也不及自动课程，说明自适应课程生成能更好地匹配策略的学习进度。\n\n## 总结与启发\n**核心贡献**：1) 提出了一个**课程引导的递归记忆（CURM）** 框架，首次将自适应课程学习与协调感知的记忆机制深度结合，用于大规模多智能体长时程任务。2) 设计了**基于性能边际的自动课程生成器**，能够根据群体策略的当前能力动态调整任务难度，无需人工干预。3) 创新性地引入了**协调重要性门（CIG）** 来增强递归记忆模块，使智能体能够鲁棒地保留对长期协调至关重要的历史信息。\n\n**局限性**：论文提到，方法在极端大规模（如上千智能体）场景下的计算开销仍然较大，主要源于中心化的课程评估和记忆模块中联合观测信息的传递（仅在训练时需要）。此外，课程生成器的性能边际采样策略参数需要针对不同任务域进行微调。\n\n**对后续研究的启示**：本文为大规模多智能体强化学习提供了一个强有力的新范式。其启示在于：1) **课程设计与智能体内部架构（如记忆）的协同设计**是一个富有前景的方向，可以进一步提升学习效率和最终性能。2) **如何设计更高效、可扩展的分布式课程生成机制**，以应对更大规模的系统，是未来的关键挑战。3) **协调重要性门**的思想可以扩展到其他序列模型（如Transformer），以处理更长的历史依赖和多智能体关系建模。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.06963",
      "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
      "url": "http://arxiv.org/abs/2512.06963",
      "arxivId": "2512.06963",
      "date": "2025-12-07",
      "authors": "Baining Guo Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉-语言-动作（VLA）模型在机器人操作中泛化能力有限的问题，提出VideoVLA方法。该方法基于多模态扩散变换器，将预训练的大规模视频生成模型转化为机器人操纵器，核心创新在于采用双预测策略：同时预测动作序列及其引发的未来视觉结果。实验表明，高质量的视觉想象与可靠的动作预测及任务成功高度相关，该方法展现出强大的泛化能力，包括模仿新技能和处理未见过的物体。",
      "detailedSummary": "## 研究背景与动机\n机器人操作的泛化能力是实现开放世界部署和迈向通用人工智能的关键。当前，视觉-语言-动作（VLA）模型通过利用大规模预训练的理解模型（如视觉语言模型）进行感知和指令跟随，减少了对机器人专用数据的需求。然而，现有方法在泛化到新任务、新物体和新环境方面仍然有限。与此同时，大型视频生成模型在给定新颖文本或图像条件时，展现出了卓越的泛化能力和物理合理性，其学习到的物理动态理解和基于指令预测未来状态的能力，与高性能机器人操作器所需的推理和规划能力天然契合。\n\n本文针对现有VLA模型泛化能力不足的痛点，提出了一个全新的视角：能否将大型视频生成模型无缝地转化为通用的机器人操作器？其核心思路是将一个视频扩散Transformer改造为视频-动作扩散Transformer，通过增加动作作为新的输出模态，并联合对视频和动作进行去噪，从而实现给定语言指令和当前视觉观测，同步预测可执行的动作序列及其导致的未来视觉结果。\n\n## 方法详解\nVideoVLA的目标是给定文本指令 𝒯 和当前视觉观测 𝒪，联合预测一个由K个7维动作组成的动作块 𝒜（编码手腕旋转、平移和夹爪状态），以及一个由N帧组成的、描绘执行𝒜后预期视觉未来的视频片段 ℱ（实际预测其潜在表示）。\n\n![方法框架](https://arxiv.org/html/2512.06963v1/x2.png)\n\n> **图2**：VideoVLA整体框架。(a) 文本编码器和视频编码器分别将语言指令和视频片段转换为token序列或潜在表示。(b) 基于Diffusion Transformer的骨干网络，以编码后的语言token和第一帧潜在为条件，联合预测完成任务所需的下一动作块以及代表执行该动作块预期视觉结果的未来帧潜在。粉红色高亮的视频解码器是可选的，仅用于可视化想象的未来帧。\n\n方法整体流程包含两个核心部分：输入编码和多模态统一未来建模。\n1.  **输入编码**：使用T5文本编码器将语言指令转换为固定长度（226个）的token序列 𝑻。采用CogVideoX中的3D因果VAE作为视频编码器，将视频片段编码为一系列视频潜在 𝒱。由于编码器的因果设计，序列中的第一个潜在 𝑽₁ 仅编码第一帧，即当前观测 𝒪。在训练时编码整个视频片段以获得 𝑽₁ 和未来帧潜在 {𝑽ⱼ}ⱼ=₂ⁿ；在推理时仅编码当前观测以获得 𝑽₁。\n2.  **统一未来建模**：模型骨干网络采用Diffusion Transformer（DiT）架构，并以预训练的CogVideoX模型初始化。将条件输入（语言token 𝑻 和当前观测潜在 𝑽₁）与预测目标（未来帧潜在 {𝑽ⱼ}ⱼ=₂ⁿ 和动作块 𝒜）在嵌入维度拼接成一个统一的token序列。具体而言，将每个视频潜在的空间维度展平为一维序列，得到 𝑽′₁ 和 {𝑽′ⱼ}ⱼ=₂ⁿ。模型采用DDPM扩散框架，对 {𝑽′ⱼ}ⱼ=₂ⁿ 和 𝒜 添加高斯噪声，并通过扩散损失训练网络去噪。噪声时间步嵌入通过自适应层归一化注入。\n\n与现有方法相比，VideoVLA的核心创新点在于：1) **范式转变**：首次大规模利用预训练的视频生成模型（而非理解模型）作为机器人VLA的主干。2) **端到端联合预测**：在单一Transformer架构内，以统一的多模态DiT形式，端到端地联合建模并生成视频和动作，而非将视频生成作为独立的规划模块。3) **双预测策略**：通过同步预测动作及其视觉后果，强制模型学习两者间的强相关性，实验表明高质量的视觉想象与可靠的动作预测正相关，这有助于将视频生成模型的泛化能力迁移到动作领域。\n\n## 实验与结果\n**数据集与平台**：使用Open X-Embodiment（OXE）数据集进行预训练，其中包含来自22种不同机器人本体的超过100万条真实世界轨迹。仿真实验在SIMPLER环境中进行，评估谷歌机器人和WidowX机器人。真实世界实验使用Realman机器人（7自由度机械臂加夹爪），并收集了包含“抓取”、“堆叠”、“放置”三个任务的5824个样本进行微调。\n\n**基线方法**：对比了RT-1-X、RT-2-X、Octo-Base、Octo-Small、OpenVLA、SpatialVLA、π₀ 和 CogACT。\n\n**关键实验结果**：\n1.  **域内评估（仿真）**：如表1所示，在SIMPLER的Visual Matching（VM）设置下，VideoVLA在WidowX机器人的4个任务上取得了最高的平均成功率（53.1%）。对于谷歌机器人，在VM和Variant Aggregation（VA）两种设置下的4个任务平均成功率分别为73.1%和62.8%，其中VA设置排名第一，VM设置排名第二。综合所有12个任务，VideoVLA取得了最高的平均成功率（63.0%）。\n\n![仿真结果-谷歌机器人](https://arxiv.org/html/2512.06963v1/figures/SIM_google.png)\n\n> **图3**：在SIMPLER仿真环境中，谷歌机器人的域内评估结果（VM/VA）。VideoVLA在VA设置下平均成功率最高，在VM设置下排名第二。\n\n![仿真结果-WidowX机器人](https://arxiv.org/html/2512.06963v1/figures/SIM_WINX.png)\n\n> **图4**：在SIMPLER仿真环境中，WidowX机器人的域内评估结果（VM）。VideoVLA取得了最高的平均成功率。\n\n2.  **泛化到新物体**：评估谷歌机器人对10个训练未见物体的“抓取”能力。如表2和图5所示，VideoVLA以65.2%的平均成功率大幅领先所有基线方法，并在10个物体中的8个上取得了最佳性能。\n\n![新物体泛化](https://arxiv.org/html/2512.06963v1/x3.png)\n\n> **图5**：在SIMPLER环境中对谷歌机器人进行新物体泛化评估的结果。VideoVLA在10个新物体中的8个上表现最佳，平均成功率大幅领先。\n\n3.  **泛化到新技能**：评估谷歌机器人执行从WidowX机器人迁移而来的、其自身训练数据中未包含的8项技能。如表3和图6所示，VideoVLA以48.6%的平均成功率位列第一，较第二名CogACT（20.4%）高出28.2个百分点，且在所有被评估技能上均表现最佳。\n\n![新技能泛化](https://arxiv.org/html/2512.06963v1/x4.png)\n\n> **图6**：在SIMPLER环境中评估谷歌机器人从WidowX机器人迁移新技能的能力。VideoVLA在所有评估技能上均表现最佳，平均成功率领先第二名28.2个百分点。\n\n4.  **真实世界评估**：在Realman机器人上进行“抓取”、“堆叠”、“放置”任务的域内评估。如表4和图7所示，VideoVLA经过微调后取得了64.6%的综合平均成功率，排名第一，尤其在“堆叠”（66.7%）和“放置”（整体成功率56.3%）任务上表现优异。\n\n![真实世界结果](https://arxiv.org/html/2512.06963v1/x5.png)\n\n> **图7**：使用Realman机器人在真实世界进行域内评估的结果。VideoVLA在“堆叠”和“放置”任务上表现优异，取得了最高的综合平均成功率。\n\n**消融实验与发现**：论文指出，实验揭示了预测动作与生成视频片段之间的强相关性——当想象的未来（生成视频）与环境实际结果高度一致时，对应的预测动作往往能带来更高的任务成功率。这表明视觉想象的质量可以作为动作可靠性的隐含指标，从而验证了联合预测未来视觉动态与未来动作这一策略的重要性。\n\n## 总结与启发\n**核心贡献**：\n1.  **范式创新**：首次提出并验证了将大规模预训练视频生成模型转化为通用机器人操作器的新范式，突破了现有VLA模型主要依赖预训练理解模型的局限。\n2.  **有效框架**：提出了VideoVLA，一个基于多模态扩散Transformer的简单而有效的框架，能够端到端地联合预测机器人动作序列及其引发的未来视觉状态。\n3.  **强泛化能力**：实验表明，得益于预训练视频生成器的知识及其双预测策略，VideoVLA在域内任务、处理新物体以及跨本体技能迁移等方面均展现出卓越的泛化性能，并揭示了视觉想象质量与动作可靠性之间的内在联系。\n\n**局限性**：论文自身提到，视频生成模型通常计算成本较高；此外，当前方法预测的动作序列长度有限（实验中为6步，执行3步），对于长程任务可能需要多次递归调用。\n\n**启示**：本工作为机器人学习开辟了一条新路径，即利用生成模型（尤其是视频生成模型）所蕴含的丰富物理世界知识和强大的条件生成能力来提升机器人系统的泛化性与智能水平。随着生成模型能力的持续进步，基于此类模型的机器人系统有望展现出更强大的通用能力。",
      "imageUrls": [
        "https://arxiv.org/html/2512.06963v1/x1.png",
        "https://arxiv.org/html/2512.06963v1/x2.png",
        "https://arxiv.org/html/2512.06963v1/figures/SIM_google.png",
        "https://arxiv.org/html/2512.06963v1/figures/SIM_WINX.png",
        "https://arxiv.org/html/2512.06963v1/x3.png",
        "https://arxiv.org/html/2512.06963v1/x4.png",
        "https://arxiv.org/html/2512.06963v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.05953",
      "title": "Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning",
      "url": "http://arxiv.org/abs/2512.05953",
      "arxivId": "2512.05953",
      "date": "2025-12-05",
      "authors": "Kuan Fang Team",
      "category": "Manipulation",
      "summary": "本文提出COIL框架，旨在解决现有基于3D运动轨迹（flow）的视觉运动控制策略存在的深度模糊、依赖手工设计模块、任务规范不灵活等问题。其核心方法是采用以3D关键点对应关系为导向的任务表示，允许可变的空间与时间粒度；并设计了一个融合多模态信息的时空注意力条件策略。该方法通过自监督流程在仿真中训练，并在真实世界操作任务上实现了优于先前方法的性能，能泛化至不同任务、物体和运动模式。",
      "detailedSummary": "## 研究背景与动机\n目前，视觉运动控制领域的主流任务表示方法包括基于目标图像、语言指令以及物体或机器人的运动轨迹（flow）。然而，这些方法存在关键局限性：基于图像的目标可能过度指定场景，混淆无关背景；语言指令难以精确地落实到物理世界的具体动作；而现有的基于运动轨迹（flow）的方法大多局限于2D，存在深度模糊和对遮挡敏感的问题。即使扩展到3D的近期工作，也通常依赖于手工设计的运动基元或控制器，限制了其对新任务和环境的适应性，并且往往需要密集标注的轨迹，导致任务指定成本高且不灵活。\n\n本文针对现有flow-conditioned方法在三维空间表达的灵活性、鲁棒性和通用性上的不足，提出了一种新颖的、基于3D关键点时空对应关系的任务表示新视角。该视角将每个任务定义为场景物体上选定的一组关键点的预期3D运动轨迹，并允许任务指定在空间（关键点数量）和时间（目标步数）粒度上灵活可变。本文的核心思路是：通过一个结合时空注意力机制的条件策略，将这种灵活的对应关系任务表示鲁棒地映射为可执行动作，并利用自监督的模仿学习流程进行可扩展的训练。\n\n## 方法详解\nCOIL的整体框架是一个条件模仿学习框架。在每一时间步t，策略接收当前观测（点云和本体感知）以及任务表示c，输出未来一段时间的动作序列。任务表示c ∈ ℝ^(H×K×3)定义了K个关键点在H个离散步骤的目标3D坐标。策略的目标是理解并执行该任务，使关键点按顺序达到这些目标位置。\n\n![方法框架](https://arxiv.org/html/2512.05953v1/x2.png)\n> **图2**：COIL策略概述。策略使用共享的3D坐标编码器分别编码任务表示、跟踪的关键点和观测点云。时间信息通过归一化位置编码注入。一个时空Transformer通过交错执行时空自注意力和与视觉观测的交叉注意力，高效融合这些输入信息。得到的表示与本体感知结合，传递给一个流匹配头以生成多步动作。\n\n核心模块包括：1）**灵活的任务表示**：支持可变的空间粒度（任意K≥1个关键点，可位于多个物体上）和时间粒度（任意H≥2个目标步骤，仅要求按顺序到达，不固定到达时间）。2）**时空注意力编码器**：用于融合多模态信息。首先，使用共享权重的MLP分别编码点云xt、任务表示c和当前跟踪的关键点位置ut。对于c，需额外注入归一化的时间位置编码。然后，通过一个新颖的**时空Transformer**进行融合：该模块将编码后的ut和c沿时间轴拼接作为输入token，将点云特征作为上下文。每一层执行三步融合：首先在token的时间维度上进行自注意力，然后在空间维度（关键点间）进行自注意力，最后每一步自注意力后都执行一次与点云特征的交叉注意力。这种交错设计允许网络逐步推理任务进度、时空关系，并将运动计划锚定在具体的场景几何中。3）**流匹配预测头**：接收编码器的输出和机器人本体感知，采用类似UNet的结构建模多模态的动作分布。4）**自监督训练流程**：在仿真中收集多样化的演示数据，并通过**后见之明对应关系估计**自动生成任务表示标签——在每一幕结束后，根据预先选定并跟踪的关键点的真实3D运动轨迹进行重标注。同时，引入**对应关系增强**：在训练时，随机从密集的轨迹标签中采样子集（随机的关键点数量K和时间步数H），并给跟踪的关键点位置ut添加高斯噪声，以模拟部署时可能遇到的偏离和噪声，提升策略的鲁棒性和对灵活指定的适应性。\n\n与现有方法相比，COIL的创新点主要体现在：1）提出了支持可变时空粒度的3D对应关系任务表示，比固定、密集的2D或3D flow更灵活。2）设计了时空注意力机制来鲁棒地处理这种稀疏、灵活的任务表示，并融合几何信息，而非依赖手工控制器或运动规划。3）构建了完全自监督的模仿学习流程，通过后见之明重标注自动生成训练所需的任务表示。\n\n## 实验与结果\n实验使用了基于DROID基准的真实世界桌面操作设置，涉及7自由度Franka机械臂和立体相机。评估任务包括**Pick-and-Place**、**Sweeping**和**Folding**，均使用分布外物体以测试泛化能力。任务表示按稀疏度分为Sparse（3-5关键点，2-5步）、Medium（8-12关键点，16步）和Dense（32关键点，32步）三种设置。对比的基线方法包括RT-Trajectory（2D末端轨迹条件）、Im2Flow2Act（2D物体流条件）和General-Flow（3D流生成+运动规划）。此外，还测试了结合视觉语言模型（VLM）生成任务表示后由COIL执行的效果。\n\n![任务执行](https://arxiv.org/html/2512.05953v1/x3.png)\n> **图3**：任务执行示例。从左至右分别为每个评估任务给策略的输入指定、策略的执行过程以及机器人实现的末端轨迹。COIL策略展示了灵活适应场景物体的行为，例如在Pick-and-Place中旋转夹爪以避免碰撞，在Folding中正确操作围巾边缘，并在Sweeping任务中展示了精确的轨迹跟随能力。\n\n关键实验结果如表1所示。COIL在绝大多数任务和设置下均取得了最高成功率，显著优于基线。例如，在Pick-and-Place任务中，COIL在稀疏、中等和密集设置下的成功率分别为8/10、8/10、9/10，而表现次优的Im2Flow2Act为2/10、3/10、8/10。在具有挑战性的、分布外的Folding任务上，COIL也取得了6-7/10的成功率，展示了强大的零样本泛化能力。结合VLM指定任务时，COIL也保持了良好性能。RT-Trajectory因模拟到真实的差距表现不佳，General-Flow仅在Folding任务上表现尚可，且无法适应稀疏/中等设置。\n\n![消融实验](https://arxiv.org/html/2512.05953v1/x4.png)\n> **图4**：在仿真Sweeping任务上的消融研究。从左至右分别展示了不同策略变体在稀疏（2步）、中等（16步）和密集（32步）任务指定下的成功率（条形图）和成功轨迹的跟踪误差（折线图）。结果表明，完整的COIL模型（绿色）在所有设置下均表现最佳。移除时空注意力（绿色）或对应关系增强（蓝色）都会导致性能显著下降，尤其是在稀疏指定下。\n\n消融实验（图4）在仿真Sweeping任务上进行，总结了各组件贡献：1）**完整的COIL模型**在所有设置下性能最佳。2）**移除时空注意力**（改用简单MLP融合）导致性能严重下降，尤其在稀疏指定下成功率接近零，证明了该机制对于理解灵活任务表示至关重要。3）**移除对应关系增强**（训练时不进行子采样和噪声添加）同样导致性能显著恶化，特别是在稀疏和中等设置下，验证了该技术对于克服分布偏移、提升鲁棒性的必要性。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了一种新颖的、支持可变时空粒度的3D对应关系任务表示，为灵活指定操作意图提供了统一接口。2）设计了一个结合时空注意力机制的条件策略，能够鲁棒地将这种灵活表示映射为精确动作。3）构建了一个可扩展的自监督模仿学习流程，通过后见之明重标注和对应关系增强自动生成训练数据。\n\n论文自身提到的局限性包括：策略性能依赖于在线的3D点跟踪算法，跟踪失败可能影响任务执行；当前方法主要评估了桌面操作场景。\n\n这项工作对后续研究的启示在于：将任务表示为物体层面的3D几何对应关系，是一种介于高层语义（如语言）和底层动作之间、兼具表达力和可操作性的有效途径。其灵活的指定方式可以很方便地与用户交互、人类演示或大模型（VLM/LLM）的输出相结合，为构建通用机器人系统提供了有前景的技术路径。未来的工作可以探索更鲁棒的关键点跟踪方法，并将该框架扩展到更复杂的非结构化环境中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.05953v1/x1.png",
        "https://arxiv.org/html/2512.05953v1/x2.png",
        "https://arxiv.org/html/2512.05953v1/x3.png",
        "https://arxiv.org/html/2512.05953v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.06628",
      "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
      "url": "http://arxiv.org/abs/2512.06628",
      "arxivId": "2512.06628",
      "date": "2025-12-07",
      "authors": "Xiu Li Team",
      "category": "Manipulation",
      "summary": "本文提出MIND-V框架，旨在解决长时序机器人操作视频生成中数据稀缺、逻辑连贯性与物理合理性不足的核心问题。方法采用分层架构：语义推理中心进行任务规划，行为语义桥转换指令，运动视频生成器渲染视频，并通过基于GRPO强化学习的物理前瞻一致性奖励确保生成内容符合物理规律。实验表明，MIND-V在长时序机器人操作视频生成任务上达到了最先进的性能。",
      "detailedSummary": "## 研究背景与动机\n具身模仿学习受限于多样化、长时程机器人操作数据的稀缺。现有面向该领域的视频生成模型通常只能合成简单动作的短视频，且常依赖手动定义的轨迹。生成高质量、符合指令的长时程机器人操作视频面临三大挑战：1）**长时程一致性挑战**：需要在相互关联的子任务序列中保持因果一致性和逻辑连贯性；2）**语义到像素生成挑战**：需将抽象的语言指令准确翻译为像素空间中的具体时空交互；3）**物理合理性挑战**：生成内容必须严格遵守碰撞动力学、物体恒存性等物理定律。\n\n现有方法无法全面应对这些挑战。一方面，直接为长时程任务训练的视频基础模型常因难以弥合抽象指令与具体像素执行间的巨大鸿沟，而出现逻辑不连续和细节退化。另一方面，基于轨迹控制的生成模型虽增强了可控性，却牺牲了大规模自动化数据生成所需的自主性和可扩展性。\n\n受认知科学中人类运动控制分层理论的启发，本文提出MIND-V，一种认知启发的分层视频生成模型，旨在合成物理合理且逻辑连贯的长时程机器人操作视频。其核心思路是模仿大脑从认知到执行的流程，通过一个三层架构（语义推理中心、行为语义桥、运动视频生成器）将高层推理与像素级合成桥接起来，并引入测试时优化和基于强化学习的物理对齐来提升长时程鲁棒性与物理合理性。\n\n## 方法详解\nMIND-V的整体框架是一个从高层认知到具体视觉表征的自顶向下流水线。\n\n![方法框架](https://arxiv.org/html/2512.06628v1/x2.png)\n> **图2**：MIND-V分层框架概览。给定初始场景观测和任务指令，语义推理中心（SRH）进行任务分解与规划，生成行为语义桥（BSB）。运动视频生成器（MVG）基于BSB合成逼真视频。测试时，分阶段视觉未来推演（Staged Visual Future Rollouts）提供一个“提议-验证-精炼”的闭环反馈机制，以增强鲁棒性。\n\n**核心模块1：语义推理中心（SRH）**\n作为框架的认知核心，SRH将抽象语义转化为可执行的几何信号。它结合了预训练视觉语言模型（如Gemini-2.5-Pro）和基于功能可供性的视觉定位器（如Affordance-R1）。给定初始场景观测 I0 和长时程任务指令 L（如“清理桌面”），VLM首先对场景进行语义分析，并将 L 分解为有序的原子子任务序列。每个子任务定义为三元组 `SubTask_i = {ActionType_i, Object_i, Destination_i}`。随后，针对每个子任务，可供性定位器精确识别物体的分割掩码 `M_obj` 及其功能交互点 `P_obj`。基于此，VLM规划一条物理合理的轨迹，并通过闭环精炼机制（可视化并迭代评估）确保其平滑且无碰撞。\n\n**核心模块2：行为语义桥（BSB）**\nBSB是连接高层规划与像素级视频合成的关键桥梁，是一种结构化的、领域无关的中间表示。它包含三个关键元素：\n1.  **物体表示**：包括被操作物体 `M_obj` 和通用机械臂 `M_rob` 的分割掩码，经VAE编码为潜在特征。\n2.  **分解的协作轨迹**：每个子任务的轨迹被分解为三个阶段：交互前（`T_pre`，机械臂接近物体）、交互中（`T_interact`，操作物体）、交互后（`T_post`，机械臂收回）。\n3.  **阶段过渡点**：一个帧索引三元组 `(F_pre, F_interact, F_post)`，为每个阶段分配特定时长，确保自然的运动动力学。\n\n**核心模块3：运动视频生成器（MVG）**\nMVG是一个基于扩散Transformer（DiT）的条件扩散模型，负责根据BSB的控制信号精确合成操作视频。\n\n![MVG架构](https://arxiv.org/html/2512.06628v1/x3.png)\n> **图3**：运动视频生成器（MVG）架构。MVG将BSB编码为时空引导张量，通过运动嵌入模块生成精炼的运动信号 `G`，该信号在去噪过程中被注入到潜在扩散Transformer中，以确保合成视频严格遵循预期运动。\n\n具体而言，MVG首先将BSB的物体表示编码为尺寸为 `(T×C×H×W)` 的**时空引导张量**，该张量在时间维度上将活动主体（机械臂或被操作物体）的视觉特征动态嵌入到其规划路径上。随后，一个**运动嵌入模块**使用时空卷积将该引导张量编码为特征表示 `G`。在每个Transformer块中，`G` 通过加性融合与视频的中间隐藏状态 `h` 结合：`h_new = h + norm(G)·G`。这种运动约束的持续注入迫使模型在整个去噪过程中遵循指定轨迹。\n\n**测试时优化：分阶段视觉未来推演**\n为缓解长时程任务中的误差累积问题，MIND-V在推理时引入了分阶段视觉未来推演策略。如图2(d)所示，在每个子任务过渡时，SRH会提出 `K` 个语义合理但策略多样的候选轨迹。MVG将这些轨迹合成对应的视频片段 `V_K`。随后，VLM作为验证法官，根据任务成功、物理合理性等标准评估每个候选未来。若最高分视频 `V_top` 达到预设阈值，则被选中并继续；否则，VLM提供结构化文本反馈，指导SRH在下一轮迭代中重新规划。这个“提议-验证-精炼”的循环将SRH从一个简单的前馈规划器转变为主动自我修正的智能体。\n\n**MVG训练：从监督微调到物理对齐**\nMVG的训练分为两个阶段：\n1.  **监督微调（SFT）**：在真实机器人数据集（如Bridge v2）上，使用真实BSB标注，通过标准的去噪目标函数微调一个开源视频模型，使其学习从BSB到连贯视频序列的基本映射，得到一个高质量的初始策略 `π_ref`。\n2.  **GRPO后训练对齐**：为进一步确保物理合理性和美学质量，将去噪过程建模为马尔可夫决策过程，并使用群组相对策略优化（GRPO）进行优化。优化由一个复合奖励函数 `R(x_0) = w_p·R_physics(x_0) + w_a·R_aesthetic(x_0)` 引导。\n\n![PFC奖励](https://arxiv.org/html/2512.06628v1/x4.png)\n> **图4**：物理前瞻一致性（PFC）奖励。PFC利用冻结的V-JEPA2世界模型，基于过去的上下文帧预测未来目标帧的潜在表示。奖励是该预测与真实目标潜在表示之间的余弦相似度，用于衡量视频与世界模型所学物理动力学的一致性。\n\n*   **物理前瞻一致性奖励（`R_physics`）**：创新性地使用预训练的世界模型V-JEPA2作为“物理裁判”。通过滑动窗口计算局部一致性分数 `s_i`（公式4），并使用基于softmax的加权方案（公式5）将优化重点集中在物理错误最大的窗口上。\n*   **美学奖励（`R_aesthetic`）**：由VLM（如Qwen-VL）提供，对视频的清晰度、伪影和真实感进行分层评分。\nGRPO通过计算组内样本奖励的相对优势（公式6）来更新策略，并利用KL散度项正则化策略以防止奖励破解（公式7）。\n\n## 实验与结果\n**实验设置**：SRH使用Gemini-2.5 Pro和Affordance-R1；MVG基于CogVideoX-5B初始化。在Bridge V2数据集上进行训练和评估，分辨率为480×640，每个子任务视频长度为37帧。训练包括30,000步SFT和1,500次GRPO后训练迭代。推理时，生成一个包含3个子任务（111帧）的长时程视频约需180秒，消耗约50GB显存。\n\n**评估协议与基准**：在108个样本的测试集上进行评估。短时程任务侧重**视觉质量**，使用V-Bench评估；长时程任务额外引入**用户研究**、**物理合理性（PFC分数）** 和**任务成功率**。对比了MotionCtrl、IRASim、DragAnything、Tora、RoboMaster、Robodreamer、WoW、Wan2.2-14B、HunyuanVideo等主流方法。\n\n![定性对比](https://arxiv.org/html/2512.06628v1/x5.png)\n> **图5**：长时程机器人操作视频生成的定性对比。基线模型表现出逻辑不一致、物理不合理等显著缺陷，而MIND-V能高质量、高物理保真地执行长时程指令，验证了其分层架构的有效性。\n\n**关键实验结果**：\n*   **视觉质量（表1）**：在短时程任务的所有六项指标（美学质量、成像质量、时间闪烁、运动平滑度、主体一致性、背景一致性）上，MIND-V均取得最佳或接近最佳的性能。在长时程任务中，MIND-V在时间闪烁、运动平滑度、主体一致性、背景一致性上表现最佳，美学质量和成像质量也极具竞争力。\n*   **长时程综合评估（表2）**：\n    *   **物理合理性（PFC分数）**：MIND-V以0.445分显著优于所有基线方法（次优为0.423）。\n    *   **任务成功率**：MIND-V达到61.3%，远超其他方法（次优为34.7%）。\n    *   **用户偏好**：在用户研究中，MIND-V获得了46.7%的偏好率，远高于其他模型。\n\n**消融实验分析**：论文通过消融实验验证了各核心组件的贡献。移除分阶段视觉未来推演会导致错误累积，显著降低长时程任务成功率。移除GRPO后训练对齐（仅使用SFT）会损害生成视频的物理合理性，PFC分数下降。而BSB作为中间表示，对于实现精确的空间控制和领域泛化至关重要，移除或替换为其他控制信号（如文本）会导致性能大幅下降。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了首个用于长时程机器人操作的分层智能视频生成框架MIND-V，通过大脑（SRH）、符号桥（BSB）和视频生成器（MVG）的三层架构，有效桥接了高层任务规划与低层像素合成之间的鸿沟。\n2.  提出了分阶段视觉未来推演，一种将全局长时程生成分解为一系列局部最优决策的测试时优化策略，通过“提议-验证-精炼”过程缓解误差累积。\n3.  提出了一种由新颖物理前瞻一致性（PFC）奖励引导的GRPO后训练对齐方法，利用预训练世界模型在潜在特征空间中对生成动态的物理合理性进行评分，从而引导生成器产生更物理真实的输出。\n\n**局限性**：论文提到，MIND-V的推理速度受限于自回归的子任务生成和迭代优化过程，计算成本较高。此外，其性能部分依赖于预训练的VLM和世界模型的能力与偏差。\n\n**启示**：MIND-V为具身数据合成建立了一个可扩展且可控的范式。其分层思想（高层推理、结构化中间表示、低层生成）可泛化至其他需要长序列、高保真生成的领域。利用世界模型作为“物理裁判”进行奖励建模的思路，为生成模型与物理常识对齐提供了新途径。未来工作可探索更高效的中间表示、优化推理速度，并将该框架应用于更复杂的多智能体或动态环境任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.06628v1/x1.png",
        "https://arxiv.org/html/2512.06628v1/x2.png",
        "https://arxiv.org/html/2512.06628v1/x3.png",
        "https://arxiv.org/html/2512.06628v1/x4.png",
        "https://arxiv.org/html/2512.06628v1/x5.png",
        "https://arxiv.org/html/2512.06628v1/x6.png",
        "https://arxiv.org/html/2512.06628v1/x7.png",
        "https://arxiv.org/html/2512.06628v1/x8.png",
        "https://arxiv.org/html/2512.06628v1/x9.png",
        "https://arxiv.org/html/2512.06628v1/x10.png",
        "https://arxiv.org/html/2512.06628v1/x11.png",
        "https://arxiv.org/html/2512.06628v1/x12.png",
        "https://arxiv.org/html/2512.06628v1/x13.png",
        "https://arxiv.org/html/2512.06628v1/x14.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.06038",
      "title": "Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction",
      "url": "http://arxiv.org/abs/2512.06038",
      "arxivId": "2512.06038",
      "date": "2025-12-04",
      "authors": "Alexander E. Siemenn Team",
      "category": "Manipulation",
      "summary": "本论文针对自驾实验室中易碎、透明基底的自动化装卸瓶颈，开发了ASHE系统。该方法结合机器人、双驱动分配器与深度学习计算机视觉，通过实时检测与校正微米级放置误差，实现闭环精准操控。实验表明，系统在130次独立透明玻璃基底重载试验中，首次放置准确率达98.5%，并能自动检测并成功校正所有误放。",
      "detailedSummary": "## 研究背景与动机\n自驱动实验室（SDLs）通过自动化加速了化学和材料发现。尽管实验流程中的许多步骤已实现自动化，但用于承载和转移材料的基板的处理和更换这一步骤常被忽视，成为流程瓶颈。当前，机械臂结合计算机视觉已广泛应用于SDLs中的物料操作。然而，当基板为透明、薄且易碎（如玻璃或硅片）时，自动化处理变得极具挑战性。现有基于深度学习的物体检测与分割模型（如YOLO、SAM）在检测透明物体方面存在局限，因为它们主要针对具有明显纹理或曲率的物体（如烧杯），难以有效检测近乎无特征的薄平透明基板。\n\n本文针对自驱动实验室中透明、易碎基板的自动化高精度更换这一具体痛点，提出了一种结合硬件创新与软件智能的闭环处理新视角。核心思路是开发一个名为ASHE（Automated Substrate Handling and Exchange）的集成系统，通过专用的机械硬件（机械臂、夹爪、分配器）执行操作，并利用融合几何模型与深度学习的视觉检测方法对放置误差进行识别与自动校正，实现透明基板的可靠、高精度自动化更换。\n\n## 方法详解\nASHE系统是一个软硬件结合的闭环自动化处理系统，包含三个主要组件：机械臂与夹爪、双驱动基板分配器、以及基于深度学习的误差检测模型。系统通过SQL数据库的状态变量触发，执行“移除旧基板-拾取新基板-放置新基板-检测并校正”的完整流程。\n\n![方法框架](https://arxiv.org/html/2512.06038v1/figs/workflow_diagram_R4.png)\n\n> **图1**：ASHE系统整体框架。该系统集成了5自由度机械臂、专用可变形夹爪、双驱动分配器以及基于深度学习的计算机视觉检测器，用于自动化基板更换。通过视觉检测和自动校正基板放置中的微尺度误差，ASHE能够高精度、可重复地操作脆弱透明基板。\n\n**1. 机器人路径规划与操作：**\n系统采用UFACTORY xArm 5（5自由度）机械臂，重复定位精度0.1 mm。操作路径遵循“领导者-跟随者”控制模式，由SQL数据库状态触发。路径规划分为三步（如图2所示）：首先，机械臂将SDL转运器上的旧基板移除并丢弃至废料箱；其次，移动到分配器拾取新基板；最后，将新基板放置到转运器的目标槽位。放置后立即启动视觉检测。\n\n![机器人路径规划](https://arxiv.org/html/2512.06038v1/figs/path_plan_R3.png)\n\n> **图2**：机器人基板更换与抓取路径规划。机械臂依次执行（a）从SDL转运器移除旧基板至废料箱，（b）从分配器拾取新基板，（c）将新基板放置到转运器。（d）采用双材料（刚性PLA和可变形TPU）设计的定制夹爪指，通过四个接触点安全抓取易碎基板，避免断裂。\n\n**2. 双驱动基板分配器：**\n该分配器可存储超过300片尺寸为50.8 mm × 76.2 mm × 1.0 mm的玻璃基板。其核心是一个双线性执行器设计（如图3所示）：水平执行器推动最底层的基板移出存储仓；到达位置后触发限位开关，启动垂直执行器伸出一面挡墙，防止基板在水平执行器缩回时被带回；最后垂直执行器缩回，基板被单独留在拾取平台上等待机械臂抓取。此设计实现了基板的单张、有序供给。\n\n![双驱动分配器工作流程](https://arxiv.org/html/2512.06038v1/figs/dispenser_workflow_R5.png)\n\n> **图3**：双驱动分配器供给新基板流程。（a）水平执行器从底部推出单片基板。（b）水平执行器完全伸出后，垂直执行器下降阻挡基板回退路径。（c）基板保持位置，水平执行器缩回。（d）垂直执行器缩回，基板留在平台上供机械臂抓取。\n\n**3. 透明基板视觉检测与误差校正：**\n这是ASHE的核心创新。透明基板在环境光下边缘难以分辨（图4a）。为解决此问题，系统采用**侧向蓝光照明**技术：在黑暗环境中从侧面用蓝光照射基板，光线在基板内部折射并从边缘面射出，形成清晰的蓝色边界（图4b），极大提升了视觉检测的鲁棒性（图4c）。\n\n![侧向照明效果](https://arxiv.org/html/2512.06038v1/figs/blue_led_R8.png)\n\n> **图4**：使用蓝光对透明基板进行侧向照明以辅助视觉检测。（a）环境光下的基板渲染图。（b）黑暗环境下蓝光侧向照明的基板渲染图，光线折射使边缘发光。（c）实际成像设置（RealSense D435相机与侧向蓝光）及在照明条件下成功与失败放置基板的示例图像。\n\n检测模型采用**融合决策框架**（图5），结合了一个几何模型（GM）和一个卷积神经网络（CNN）。只有当GM和CNN**都**判定为成功时，才最终确认放置成功。任一模型判定失败，都会触发机械臂执行自动校正流程（移除错误基板，重新放置新基板）。\n\n![融合模型决策树](https://arxiv.org/html/2512.06038v1/figs/error_detection_decision_tree_R2.png)\n\n> **图5**：融合几何模型（GM）与卷积神经网络（CNN）的决策树，用于可靠分类透明基板放置误差。仅当GM和CNN均输出“成功”分类时，才宣告放置成功。\n\n- **几何模型（GM）用于检测宏观误差**：其流程如图6a所示。首先，对空转运器目标槽图像进行Canny边缘检测和凸包变换，得到其边界矩形。然后，对放置基板后的侧向照明图像进行蓝色掩膜、Canny边缘检测、霍夫变换直线检测和凸包变换，得到基板的边界矩形。最后，使用Shapely计算两个矩形的重叠面积百分比。若重叠面积低于90%（通过经验测试确定），则GM判定为失败。\n- **CNN用于检测微观误差**：其网络结构如图6b所示。输入为从侧向照明图像中裁剪出的感兴趣区域（380×250像素，后调整为96×96像素）。网络是一个浅层CNN，包含5个卷积层和2个全连接层，最终输出一个长度为2的向量，分别表示“失败”和“成功”的置信度。训练数据集包含1990张原始图像（成功与失败各半），并通过多种数据增强扩展到29,850张。采用加权交叉熵损失函数以处理类别不平衡。在测试时，对100帧采样图像计算成功置信度的中位数，若超过60%的阈值，则CNN判定为成功。\n\n![GM与CNN工作流程图](https://arxiv.org/html/2512.06038v1/figs/model_workflows_R1.png)\n\n> **图6**：（a）几何模型（GM）工作流程：通过比较从转运器目标槽和放置基板图像中提取的边界矩形来检测宏观误差。（b）卷积神经网络（CNN）工作流程：用于检测微观误差的网络架构，包含多个卷积层、池化层和全连接层。\n\n**创新点总结**：1) 针对透明基板提出侧向蓝光照明技术，解决了边缘检测难题。2) 提出GM与CNN融合的检测框架，兼顾了宏观与微观误差的鲁棒识别。3) 设计了专用的双材料夹爪和双驱动分配器，实现了对易碎、薄平基板的物理安全操作与供给。4) 构建了完整的“感知-决策-执行-校正”闭环系统。\n\n## 实验与结果\n**实验设置**：系统使用UFACTORY xArm 5机械臂、定制夹爪、双驱动分配器、Intel RealSense D435相机以及侧向蓝光照明。基板为透明玻璃（50.8 mm × 76.2 mm × 1.0 mm）。深度学习模型基于PyTorch实现。\n\n**视觉检测性能评估**：\n研究首先在36种独特的放置误差（包括不同大小和组合的平移与旋转误差）上测试了GM和CNN融合模型的性能，如图7所示。结果表明，GM能可靠检测大的宏观误差，而CNN能有效识别近乎人眼不可见的微小误差。两者在旋转和平移误差上的表现相似。融合模型通过“逻辑与”决策，确保了极高的整体检测可靠性。\n\n![模型检测结果矩阵](https://arxiv.org/html/2512.06038v1/figs/cnn_gm_model_results_R4.png)\n\n> **图7**：GM和CNN在36种独特放置误差类型和大小上的检测结果矩阵。X轴为平移误差增大，Y轴为旋转误差增大。从内到外环分别为小、中、大误差。每张图上方标明了GM和CNN的分类结果及CNN置信度。结果显示融合模型能有效覆盖各种误差模式。\n\n**端到端系统性能**：\n在130次独立的SDL实验重载循环中，ASHE系统实现了**98.5%的首次放置成功率**（即仅两次放置失误）。关键的是，这两次失误均被视觉检测系统成功识别，并触发了自动校正流程，最终成功更换了基板，如图8所示。这证明了ASHE作为闭环系统的可靠性。\n\n![端到端实验结果](https://arxiv.org/html/2512.06038v1/figs/experiments_R1.png)\n\n> **图8**：端到端系统实验结果。展示了130次试验中仅有的两次放置失败案例（试验#46和#95），以及系统成功检测到这些错误并执行自动校正的流程，最终所有试验均成功完成基板更换。\n\n**消融实验与组件贡献**：\n论文虽未进行形式化的消融实验，但从设计和结果可推断各组件贡献：1) **侧向照明**是透明基板视觉检测可行的前提。2) **融合检测模型**（GM+CNN）相比单一模型提供了更鲁棒的误差识别，避免了漏检或误检。3) **专用夹爪与分配器**是安全物理操作的基础，避免了基板破损和供给混乱。4) **闭环校正机制**将单次操作成功率提升至接近100%。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了首个针对自驱动实验室中透明、易碎基板处理的完整闭环自动化系统ASHE，实现了高达98.5%的首次操作成功率及可靠的误差自动校正。\n2. 开发了一种创新的视觉检测方法，结合侧向蓝光照明技术与融合几何模型和深度学习的决策框架，有效解决了薄平透明物体的高精度位姿检测难题。\n3. 设计了专用的硬件组件，包括双材料可变形夹爪和双驱动基板分配器，为安全、可靠地操作脆弱基板提供了工程解决方案。\n\n**局限性**：\n论文明确指出，当前工作主要针对特定尺寸（50.8 mm × 76.2 mm）的透明玻璃基板。对于其他尺寸、形状或材料（如柔性基板）的通用性尚未验证。此外，侧向照明条件（黑暗环境+蓝光）可能对某些实验室环境构成限制。\n\n**对后续研究的启示**：\n1. **可扩展性**：未来研究可探索ASHE系统对不同尺寸、形状、透明度甚至柔性基板的适应能力，开发更通用的检测与操作算法。\n2. **检测技术泛化**：侧向照明与融合检测模型的思想可推广至其他难以检测的工业或实验室物体（如反光表面、低对比度物体）。\n3. **软硬件协同设计**：本研究展示了针对特定挑战性问题，通过软硬件联合创新（从照明、夹爪到算法）所能达到的性能高度，这为其他机器人操作任务提供了范例。\n4. **集成与标准化**：ASHE通过SQL数据库与SDL通信的方式，提示了模块化、标准化接口在构建复杂自动化系统中的重要性，有利于不同子系统的集成。",
      "imageUrls": [
        "https://arxiv.org/html/2512.06038v1/figs/workflow_diagram_R4.png",
        "https://arxiv.org/html/2512.06038v1/figs/path_plan_R3.png",
        "https://arxiv.org/html/2512.06038v1/figs/dispenser_workflow_R5.png",
        "https://arxiv.org/html/2512.06038v1/figs/blue_led_R8.png",
        "https://arxiv.org/html/2512.06038v1/figs/error_detection_decision_tree_R2.png",
        "https://arxiv.org/html/2512.06038v1/figs/model_workflows_R1.png",
        "https://arxiv.org/html/2512.06038v1/figs/cnn_gm_model_results_R4.png",
        "https://arxiv.org/html/2512.06038v1/figs/experiments_R1.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.05955",
      "title": "SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models",
      "url": "http://arxiv.org/abs/2512.05955",
      "arxivId": "2512.05955",
      "date": "2025-12-05",
      "authors": "Yilun Du Team",
      "category": "Manipulation",
      "summary": "SIMPACT论文旨在解决视觉语言模型(VLMs)缺乏物理动态理解，难以应用于需要细粒度物理推理的机器人操作任务的核心问题。提出SIMPACT框架，通过测试时模拟启用的动作规划，利用预训练视觉基础模型从单RGB-D图像高效构建物理模拟，使VLM能迭代提出动作、观察模拟展开并优化推理。该方法在五个真实世界刚体和可变形操作任务中实现最先进性能，优于现有通用机器人操作模型。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉语言模型（VLMs）在场景理解和语义推理方面展现出卓越能力，使其成为开放世界机器人控制的有力工具。现有方法主要利用VLMs进行高级任务规划，或通过精心设计的3D几何表征（如体素价值图、关键点）来增强VLM的动作生成能力。然而，这些方法存在一个关键局限：它们缺乏对物理动态的“具身”理解。VLMs是在静态的互联网规模图文数据上训练的，不包含因果交互或动作引发的状态变化，因此难以预测动作在物理世界中的执行结果。这导致VLMs在需要精细物理推理的机器人操作任务（如推动物体而不倾倒、堆叠物体）中表现不佳，常常提出语言上合理但物理上不可行的计划。\n\n本文针对VLM缺乏物理动态理解这一痛点，提出了一种新的视角：在测试时（test-time）通过“仿真循环”为VLM提供物理推理能力，而无需任何额外训练。核心思路是：从单视角RGB-D观察高效构建物理仿真，使VLM能够提出动作、观察仿真推演、并基于仿真结果迭代优化其推理，从而将语言推理与物理预测相结合。\n\n## 方法详解\nSIMPACT框架的输入是单视角RGB-D图像 I_0 和自然语言任务指令 ℓ_task，输出是机器人末端执行器的位姿和夹爪开合序列 𝐚 = {a_t}。其整体流程分为两大阶段：1) 从观测自动构建物理仿真器；2) 基于仿真的VLM规划与迭代优化。\n\n![方法框架](https://arxiv.org/html/2512.05955v1/x3.png)\n> **图3**：方法整体框架。首先根据真实世界场景实例化一个物理仿真器。接着，一个基于VLM的动作采样器和优化器，以仿真推演结果为上下文，迭代优化动作序列直至任务成功。最终优化的动作在真实世界中执行。\n\n**核心模块一：仿真构建 (Simulation Construction)**\n目标是从单张RGB-D图像 I_0 自动生成一个可用于推演的物理仿真器 Sim，其参数 θ = (θ_geom, θ_phys) 包括几何参数和物理参数。\n1.  **物体识别与分割**：首先提示VLM根据用户指令生成相关物体标签，然后使用预训练的GroundedSAM2模型分割出这些物体。\n2.  **物理引擎选择**：根据物体特性（刚性、可变形），提示VLM自动选择合适的物理引擎：MuJoCo（刚体）、投影动力学求解器（硬质可变形体）或物质点法（MPM）求解器（软物体）。\n3.  **几何重建**：\n    *   **刚体**：使用图像到3D模型（如Hunyuan3D）为每个物体重建完整三角网格 ℳ̂_i，然后根据点云分割得到的真实包围盒尺寸进行缩放和中心化，得到 ℳ_i。最后使用FoundationPose估计其6DoF位姿 X_i。\n    *   **可变形体**：将分割出的物体掩膜通过深度图反投影得到3D表面点，然后在物体表面与支撑桌面之间的体积内均匀采样点集 P_i，构成粒子表示。\n4.  **物理参数推断**：通过提示VLM利用其常识推理能力，自动推断质量、摩擦、弹性等物理参数 θ_phys。\n\n![仿真构建](https://arxiv.org/html/2512.05955v1/x2.png)\n> **图2**：从单张RGB-D图像构建仿真的流程。给定RGB-D图像和语言任务描述，流程自动为刚体生成基于网格的仿真（上），或为可变形体生成基于粒子的仿真（下）。\n\n**核心模块二：基于仿真的VLM动作规划 (Action Planning via Simulation-enabled VLM)**\n规划器遵循算法1所示的迭代优化流程，VLM在其中扮演三个核心角色：采样器(Sample)、优化器(Optimize)和评估器(TaskSuccess)。\n1.  **VLM动作采样 (Sample)**：为了利用VLM的语义推理优势，采用分层动作生成策略。首先，VLM根据初始观察 I_0、任务指令 ℓ_task 和场景状态 s_0，生成由符号动作（MOVE, GRASP, RELEASE）及其连续参数构成的高级动作序列 a^i。然后，通过一个确定的映射函数 Action2Pose 将其转换为连续的6DoF控制轨迹 𝐚^i。\n2.  **仿真推演与VLM优化 (Optimize)**：对采样得到的一系列动作序列进行仿真推演，得到状态轨迹。优化时，VLM以这些成功或失败的推演结果（包括渲染的图像、动作和状态数值）作为上下文，通过情境学习生成一个新的、优化后的动作序列 𝐚^k。如图4所示，VLM可以从多个失败案例中进行推理，产生非局部的、有效的动作更新。\n3.  **VLM成功评估 (TaskSuccess)**：对优化后动作的仿真结果，VLM根据渲染的最终状态图像和数值状态，评估任务是否成功。若成功，则执行该动作序列；否则，将此次尝试加入上下文库，进行下一轮优化。\n\n![动作优化](https://arxiv.org/html/2512.05955v1/x4.png)\n> **图4**：动作优化过程示例（非倾倒推动任务）。左侧三图展示了初始VLM采样动作提案的仿真推演，均因推动距离不足/过度或瓶子倾倒而失败。基于这些提案，VLM优化器推理出一个非平凡的动作更新，在仿真和真实执行中都能以正确距离推动瓶子且不倒。\n\n**创新点**：与现有工作相比，SIMPACT的创新性主要体现在：1) **测试时零样本增强**：无需训练，仅在测试时通过仿真循环为通用VLM注入物理理解；2) **自动化多物理仿真构建**：从单视角观察快速构建涵盖刚体与可变形体的仿真；3) **仿真作为上下文的VLM规划**：将仿真推演作为情境学习的上下文，使VLM能进行物理基础的迭代推理和优化，而非仅作为奖励信号或进行局部搜索。\n\n## 实验与结果\n**实验设置**：在Franka Research 3机械臂上评估，使用Intel RealSense D435i RGBD相机。设计了五项需要精细物理推理的真实世界操作任务（见表1），涵盖刚体（纸盒、碗、盒子）和可变形体（绳子、橡皮泥），涉及推动、堆叠、旋转、塑形等多种操作。主要评估指标为成功率（10次试验）。\n\n**Baseline方法**：\n1.  **π_0.5**：在大规模机器人数据集上预训练，直接预测关节速度的视觉语言动作（VLA）模型。\n2.  **VoxPoser**：利用VLM生成3D体素价值图来规划动作。\n3.  **MOKA**：利用VLM预测关键点和可操作区域来生成动作。\n\n![定性结果](https://arxiv.org/html/2512.05955v1/x5.png)\n> **图5**：定性结果。展示了三个任务在真实世界和仿真中的初始状态、执行过程和最终状态。尽管存在仿真到真实的视觉差距，但渲染的仿真图像能够有效指导VLM的测试时推理。\n\n**关键实验结果**：\n如表2所示，SIMPACT在所有任务上均显著优于基线方法。\n*   **π_0.5** 在所有任务上成功率为0%，表明在域外复杂任务上泛化能力不足。\n*   **VoxPoser** 和 **MOKA** 在部分任务（如碗堆叠、绳子塑形）上取得了一定成功（最高20%），但在需要精确物理推理的任务（如非倾倒推动、橡皮泥塑形）上成功率均为0%或很低。\n*   **SIMPACT** 取得了最高的整体成功率：非倾倒推动80%，碗堆叠60%，旋转40%，绳子塑形90%，橡皮泥塑形80%。这证明了仿真增强的VLM在物理推理和动作规划上的有效性。\n\n![基线对比](https://arxiv.org/html/2512.05955v1/x6.png)\n> **图6**：与基线方法的定性对比。展示了基线方法因缺乏仿真推理而导致的典型失败案例，如推动高度不当导致倾倒、抓取碗的中心点、旋转时未保持与侧面的接触、绳子摆放方向错误、橡皮泥塑形时未规划垂直挤压。\n\n**消融实验**：\n如表3所示，移除了三个关键组件以验证其贡献：\n1.  **移除VLM采样器**（改用高斯分布随机采样）：所有任务性能急剧下降，多个任务成功率为0%。这表明VLM基于先验知识的引导性采样对找到合理的初始动作至关重要。\n2.  **移除仿真推演上下文**（仅用VLM内部推理进行提议-验证）：性能大幅下降，表明仅靠VLM的内部知识不足以进行可靠的物理推理，仿真提供的物理动态上下文不可或缺。\n3.  **移除VLM优化器**（仅从初始提案中选择最佳）：性能中等下降，表明迭代优化过程能进一步提升规划质量，而非简单选择。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个**测试时、零样本**的框架SIMPACT，通过将物理仿真集成到推理循环中，为通用VLM赋予了物理感知的具身动作规划能力。\n2.  开发了一个从**单视角RGB-D观察自动构建多物理仿真**的流水线，利用视觉基础模型和VLM自动化几何重建与物理参数推断。\n3.  提出了一种新颖的**以仿真为上下文的机器人动作生成方法**，使VLM能够基于仿真推演进行情境学习与迭代优化，实现了一种新的测试时推理范式。\n\n**局限性**：论文提到，当前方法依赖于从单视图重建的几何形状和VLM推断的物理参数，这些可能不够精确，从而影响仿真保真度和最终性能。仿真构建过程本身也有一定的时间开销。\n\n**启示**：这项工作展示了将高效仿真与大型基础模型在测试时结合的巨大潜力，为迈向可泛化的具身智能提供了一条有希望的路径。后续研究可探索更高效的仿真构建、更精确的物理参数估计，以及将该框架扩展到更复杂的动态场景和更长视野的任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.05955v1/x1.png",
        "https://arxiv.org/html/2512.05955v1/x2.png",
        "https://arxiv.org/html/2512.05955v1/x3.png",
        "https://arxiv.org/html/2512.05955v1/x4.png",
        "https://arxiv.org/html/2512.05955v1/x5.png",
        "https://arxiv.org/html/2512.05955v1/x6.png",
        "https://arxiv.org/html/2512.05955v1/x7.png",
        "https://arxiv.org/html/2512.05955v1/x8.png",
        "https://arxiv.org/html/2512.05955v1/x9.png",
        "https://arxiv.org/html/2512.05955v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.05927",
      "title": "World Models That Know When They Don’t Know: Controllable Video Generation with Calibrated Uncertainty",
      "url": "http://arxiv.org/abs/2512.05927",
      "arxivId": "2512.05927",
      "date": "2025-12-05",
      "authors": "Anirudha Majumdar Team",
      "category": "Manipulation",
      "summary": "本文针对可控视频生成模型易产生“幻觉”（生成违背物理现实的帧）且无法评估自身置信度的问题，提出C³方法，首次实现视频模型的校准不确定性量化。核心技术包括：1）利用严格适当评分规则训练模型，使其输出正确且校准的置信度；2）在潜在空间进行密集不确定性估计，避免像素空间方法的训练不稳定与高成本；3）将潜在空间不确定性映射为像素级高分辨率热图，直观标识不可信区域。实验在Bridge和DROID等机器人数据集上验证了该方法能提供校准的不确定性估计，并有效支持分布外检测。",
      "detailedSummary": "## 研究背景与动机\n当前，以扩散模型或流模型为代表的先进可控视频生成模型，在文本或动作条件输入下能够合成高保真度视频。然而，这些模型存在严重的“幻觉”问题，即生成与物理现实不符的未来视频帧，这在机器人策略评估与规划等需要可信视频生成的应用中构成重大障碍。尽管存在幻觉倾向，现有视频模型缺乏评估和表达其置信度的基本能力，阻碍了幻觉的缓解。此前仅有的一项工作尝试量化视频模型的不确定性，但其估计仅停留在任务级别，无法在帧级别进行空间和时间上的细粒度不确定性解析，而这对于安全决策至关重要。\n\n本文针对可控视频模型无法量化其生成结果置信度的关键痛点，提出了一种名为 𝐂³ 的不确定性量化方法。其核心思路是：通过严格适当评分规则训练视频模型，使其在潜在空间进行子块级的密集置信度预测，并将该不确定性映射为像素空间的可解释热图，从而让模型“知道它不知道什么”。\n\n## 方法详解\nC³ 方法旨在为可控（动作条件）视频生成模型提供不确定性量化，在子块级别密集估计模型对每一视频帧准确性的置信度。整体框架在生成视频的同时，通过一个作用于视频潜在表示的不确定性量化探针来估计置信度。\n\n![方法框架](https://arxiv.org/html/2512.05927v1/x2.png)\n> **图2**：C³ 模型架构。模型同时进行视频生成和不确定性量化（可视化为热图），通过一个作用于视频潜在表示的UQ探针来量化模型对其准确性的置信度。\n\n具体而言，模型 𝒱_θ 接收输入视频帧 𝐯、其他条件 𝐠（如文本或动作）和动作序列 𝐚，输出生成的潜在视频 𝐱̂ 和对应的密集置信度预测 𝐪̂。每个 𝐪̂ 中的元素对应模型对生成潜在视频 𝐱̂ 中相关子块准确性的置信度。\n\n**核心模块与技术细节**：\n1.  **潜在空间视频生成与UQ探针**：为了高效训练，视频在潜在空间生成。本文使用预训练的VQ-VAE将输入视频帧压缩到低维潜在空间，并采用潜在扩散Transformer进行视频生成。关键创新是设计了一个基于Transformer的不确定性量化探针 𝐟_ϕ，它直接作用于潜在空间，接收扩散Transformer倒数第二层的内部特征 𝐳、以及动作和时间步嵌入 𝐜，预测子块级的置信度 𝐪̂。这种方法避免了在更高维的像素空间进行UQ带来的高昂计算成本。\n2.  **基于分类的UQ与三种架构变体**：本文将不确定性量化构建为一个关于生成视频准确性的分类问题，避免了为预测准确性做出简化建模假设（如假设高斯分布）可能带来的归纳偏差。准确性函数 `acc` 基于距离函数 `d`（本文使用 L1 损失）和阈值 ε 定义，将生成视频映射为与 𝒰 维度相同的二值输出空间。通过指定 ε 的技术，诱导出三种模型架构：\n    *   **固定尺度分类模型**：在训练和推理时使用固定的误差阈值 ε 预测准确性。\n    *   **多类分类模型**：将预测的输出空间离散化为多个置信度区间，预测每个子块属于哪个准确性区间的置信度。\n    *   **连续尺度二值分类模型**：在推理时可以指定任意的准确性阈值 ε，实现任意分辨率的置信度预测。训练时对 ε 进行均匀采样或自适应分层离散化以确保覆盖。\n3.  **损失函数与训练**：所有模型变体均使用**严格适当评分规则**作为损失函数进行训练，以确保校准性。FSC和CS-BC模型使用Brier分数或二元交叉熵损失，MCC模型使用交叉熵损失。视频生成模块和UQ探针进行端到端联合训练，总损失为 ℒ_θ,ϕ = ℒ_θ + ℒ_ϕ，但在UQ探针和视频生成DiT之间应用了停止梯度操作。优化使用带动量余弦退火学习率计划的随机梯度下降。\n4.  **潜在置信度解码为可解释热图**：为了直观可视化，将潜在空间的置信度预测 𝐪̂ 解码到像素空间。通过将单色RGB视频帧编码到潜在空间来构建潜在颜色映射图，然后通过在这些颜色映射视频帧之间插值，将置信度估计映射为潜在RGB视频帧，最后使用与解码潜在视频相同的分词器将其映射到像素空间，生成高分辨率的不确定性热图。\n\n## 实验与结果\n**数据集与基线**：实验在机器人学习标准基准数据集Bridge和DROID上进行。评估主要针对C³方法本身提出的三种架构变体，并检查其校准性、可解释性和分布外检测能力。评估指标包括预期校准误差和最大校准误差。\n\n**校准性结果**：在Bridge数据集测试集上的评估表明，C³ 产生的所有模型变体的不确定性估计都得到了良好校准。\n\n![校准误差与可靠性图](https://arxiv.org/html/2512.05927v1/x4.png)\n> **图4**：(a) 平均校准误差。三种架构的ECE都非常低，MCE相对较低。(b) 聚合可靠性图。所有方法都校准良好，紧密跟踪完美校准线。\n\n如图4所示，所有模型的ECE都非常低（约0.02-0.03），MCE相对较低。可靠性图显示，所有模型都紧密跟踪完美校准线，既不过度自信也不欠自信。值得注意的是，模型在不确定时倾向于更保守（在[0.3, 0.7]置信度区间的条柱超过虚线），这与安全关键应用中可信赖性的需求相符。\n\n**可解释性与幻觉定位**：\n\n![不确定性热图与误差关联](https://arxiv.org/html/2512.05927v1/x6.png)\n> **图6**：不确定性热图示例。热图成功定位了生成视频中的幻觉区域（如机器人手臂末端意外出现的物体）。\n\n生成的不确定性热图与物理直觉一致。如图6所示，热图成功定位了生成视频帧中的幻觉区域（例如机器人手臂末端意外出现的物体）。此外，模型置信度估计与生成视频和真实视频之间的误差呈负相关，这与直觉一致。\n\n**分布外检测**：在WidowX 250机器人上的真实世界实验表明，即使在测试时存在分布偏移导致生成视频质量显著下降的情况下，C³ 仍能提供校准的不确定性估计，并能有效检测OOD输入（如未见过的环境条件或动作）。\n\n![OOD检测](https://arxiv.org/html/2512.05927v1/x7.png)\n> **图7**：分布外检测。当输入动作导致机器人移动到训练分布外的新区域时，C³ 预测的置信度显著下降，表明其能够检测OOD输入。\n\n如图7所示，当输入动作导致机器人移动到训练分布外的新区域时，C³ 预测的置信度显著且持续地下降，表明其能够检测OOD输入。\n\n**消融实验**：附录中的消融研究表明，使用适当评分规则对于实现校准至关重要。此外，虽然UQ探针和视频生成器可以独立训练，但端到端训练能略微提高性能。反向传播UQ探针的梯度到视频生成DiT对视频生成质量影响很小，但会损害UQ校准性。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个基于严格适当评分规则的新颖训练框架，首次使可控视频生成模型能够同时追求准确性和校准性，从而量化其生成过程中的不确定性。\n2.  设计了一种在潜在空间进行密集、子块级不确定性量化的高效方法，避免了像素空间方法的高昂成本，并可直接应用于多种先进潜在空间视频模型架构。\n3.  开发了将潜在空间不确定性解码为像素级可解释RGB热图的技术，使不确定性可视化并能够精确定位生成视频中的幻觉区域。\n\n**局限性**：论文提到，尽管在潜在空间进行操作，但训练大规模视频扩散模型（通常有数十亿参数）仍然计算成本高昂。此外，方法的有效性依赖于用于构建潜在颜色映射的预训练视频分词器的质量。\n\n**启示**：C³ 为构建“自知之明”的世界模型奠定了基础。其框架可扩展至其他生成模型（如图像、3D生成）。未来的工作可以探索多模态不确定性量化（结合文本、音频）、将不确定性估计用于主动学习或安全关键的机器人决策闭环中，以及研究更高效的不确定性估计架构。",
      "imageUrls": [
        "https://arxiv.org/html/2512.05927v1/x1.png",
        "https://arxiv.org/html/2512.05927v1/x2.png",
        "https://arxiv.org/html/2512.05927v1/x3.png",
        "https://arxiv.org/html/2512.05927v1/x4.png",
        "https://arxiv.org/html/2512.05927v1/x5.png",
        "https://arxiv.org/html/2512.05927v1/x6.png",
        "https://arxiv.org/html/2512.05927v1/x7.png",
        "https://arxiv.org/html/2512.05927v1/x8.png",
        "https://arxiv.org/html/2512.05927v1/x9.png",
        "https://arxiv.org/html/2512.05927v1/x10.png",
        "https://arxiv.org/html/2512.05927v1/x11.png",
        "https://arxiv.org/html/2512.05927v1/x12.png",
        "https://arxiv.org/html/2512.05927v1/x13.png",
        "https://arxiv.org/html/2512.05927v1/x14.png",
        "https://arxiv.org/html/2512.05927v1/x15.png",
        "https://arxiv.org/html/2512.05927v1/x16.png",
        "https://arxiv.org/html/2512.05927v1/x17.png",
        "https://arxiv.org/html/2512.05927v1/x18.png",
        "https://arxiv.org/html/2512.05927v1/x19.png",
        "https://arxiv.org/html/2512.05927v1/x20.png",
        "https://arxiv.org/html/2512.05927v1/x21.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.05094",
      "title": "From Generated Human Videos to Physically Plausible Robot Trajectories",
      "url": "http://arxiv.org/abs/2512.05094",
      "arxivId": "2512.05094",
      "date": "2025-12-04",
      "authors": "Roei Herzig Team",
      "category": "Manipulation",
      "summary": "本文研究如何利用生成式视频模型作为机器人高级规划器，核心挑战是生成视频存在噪声与形态失真，导致人形机器人难以直接零样本模仿其中的人类动作。为此，作者提出两阶段方法：首先通过4D人体重建模型从视频提取人体运动轨迹并重定向至机器人形态；其次训练GenMimic策略——一种基于3D关键点、采用对称正则化与加权跟踪奖励的物理感知强化学习策略。实验基于合成的GenMimicBench数据集验证，该方法在仿真中优于基线，并在Unitree G1机器人上实现了无需微调的、连贯且物理稳定的零样本运动跟踪。",
      "detailedSummary": "## 研究背景与动机\n当前，类人机器人控制领域的数据驱动方法主要依赖于高质量的人类运动捕捉数据来学习模仿策略。同时，视频生成模型在合成新颖场景下的人类动作方面取得了快速进展，有潜力作为机器人高层规划器。然而，实现这一潜力的核心挑战在于：生成视频通常包含噪声和形态失真，与真实视频相比，直接模仿这些动作对类人机器人而言非常困难。本文针对“如何让类人机器人零样本地执行生成视频中的人类动作”这一具体痛点，提出了一个结合4D人体重建与鲁棒强化学习策略的新视角。其核心思路是：首先将生成视频像素提升为4D人体运动轨迹并重定向至机器人形态，然后训练一个对噪声鲁棒、以3D关键点为条件、并辅以对称性正则化的强化学习策略（GenMimic）来实现零样本模仿。\n\n## 方法详解\n整体框架是一个两阶段流水线，如图3所示。第一阶段（训练与测试共用）：输入是生成的人类动作RGB视频，使用4D人体重建模型（如TRAM）提取每帧的全局姿态和SMPL参数，形成4D人体运动轨迹；随后通过重定向（如使用PHC）将其转换为适应目标机器人形态的关节空间目标（qt^goal）和全局3D关键点目标（pt^goal）。第二阶段（训练）：在IsaacGym模拟器中，使用重定向后的大型运动捕捉数据集AMASS作为训练数据，通过师生框架训练GenMimic策略。第二阶段（测试）：将来自生成视频并经过重定向的3D关键点目标（pt^goal）和机器人本体感知信息输入给已训练的GenMimic策略，策略输出期望关节角度（qt^des），最终由PD控制器生成扭矩驱动真实机器人。\n\n![方法整体框架](https://arxiv.org/html/2512.05094v2/x3.png)\n> **图3**：GenMimic方法整体框架。左侧为训练流程：在IsaacGym中使用重定向后的AMASS轨迹，通过师生框架训练策略，融入加权关键点奖励和对称性损失。右侧为测试流程：生成视频被提升为4D人体运动并重定向，作为目标输入给GenMimic策略，最终在真实世界执行。\n\n核心模块是GenMimic鲁棒跟踪策略，其创新点体现在两个核心设计上：\n1.  **加权跟踪奖励**：传统的跟踪奖励对所有身体关键点一视同仁。本文认为某些关键点（如末端效应器）对于任务执行和物理稳定性更为关键。因此，设计了加权跟踪奖励函数（公式1），通过对每个关键点的误差赋予不同的权重w_j（所有权重之和为1），使策略能够有选择地关注目标中最可靠和任务相关的部分。对于生成视频，这种设计偏向于末端效应器并远离不准确的下半身，从而产生稳定的模仿。\n2.  **对称性损失**：人体具有固有的双边对称性。本文假设，让策略显式学习左右关键点之间的对称相关性，可以使其对生成视频中的单点噪声具有更强的鲁棒性。为此，在标准的PPO训练目标中增加了一个辅助的对称性损失（公式2）。该损失通过修改的概率比（公式3）来计算，其中Ts和Ta分别是状态和动作的双边对称映射函数。这个损失有效地增加了在对称状态Ts(st)下选择对称动作Ta(at)的可能性，前提是原始动作at在状态st下能产生高优势。\n\n在策略学习细节上，训练数据是经过筛选和重定向的AMASS数据集（8123个动作）。奖励函数除核心的加权跟踪奖励和对称性损失外，还包括关节角度、速度、身体位置、旋转的跟踪奖励，能量正则化项，模拟器特定的惩罚项（如脚滑、关节限位）以及稳定性鼓励项。此外，还应用了输入噪声和关键模拟参数（摩擦、质量、PD增益等）的域随机化以增强鲁棒性。策略网络是一个多层感知机（MLP），运行频率为50 Hz。\n\n## 实验与结果\n实验使用了论文自建的合成人体运动数据集**GenMimicBench**，该数据集包含428个视频，由两个先进的视频生成模型Wan2.1和Cosmos-Predict2生成，涵盖室内受控场景和野外风格场景，动作包括简单手势、复合动作和物体交互。\n\n![数据集示例](https://arxiv.org/html/2512.05094v2/x2.png)\n> **图2**：GenMimicBench数据集示例。展示了使用Wan2.1和Cosmos-Predict2生成的不同主体、环境和动作类型的合成视频。\n\n对比的**基线方法**包括：GMT（通用运动跟踪）、TWIST（师生策略）、BeyondMimic（自适应采样策略）。在模拟实验中，评估指标包括成功率（SR）、全局平均每关键点位置误差（MPKPE）、局部平均每关键点位置误差（LMPKPE），以及仅在未终止步长上计算的误差（MPKPE-NT, LMPKPE-NT）。\n\n![模拟实验结果](https://arxiv.org/html/2512.05094v2/x5.png)\n> **表1**：GenMimicBench上的模拟实验结果对比。GenMimic在特权教师策略（πs）上取得了86.77%的最高成功率，远超BeyondMimic的23.81%和TWIST的2.69%；在非特权学生策略（πo）上也以29.78%的成功率显著优于GMT（4.29%）和TWIST（7.52%）。这表明GenMimic在从噪声生成视频中模仿动作方面具有卓越的零样本泛化能力。\n\n![真实世界结果](https://arxiv.org/html/2512.05094v2/x6.png)\n> **表2**：在Unitree G1类人机器人上的真实世界部署结果。将43个动作分类评估视觉成功率（VSR）。原地动作（如挥手、抬手）成功率高达1.0，而涉及转身、迈步的复合动作成功率在0.2-0.6之间，展示了方法在真实硬件上产生连贯、物理稳定运动的能力，同时也揭示了动态平衡任务的挑战性。\n\n![消融实验](https://arxiv.org/html/2512.05094v2/x7.png)\n> **表3**：在AMASS测试集和GenMimicBench上的消融研究。关键发现：1）使用3D关键点作为目标观测（3DP）比使用关节自由度（DoFs）在GenMimicBench上成功率更高（40.0% vs 23.8%）；2）在3DP基础上增加加权奖励（3DP+Weights）能大幅提升在AMASS和GenMimicBench上的成功率（分别达到97.7%和77.4%）；3）进一步引入对称性损失（3DP+Weights+Symmetry）实现了最佳性能，在GenMimicBench上成功率高达86.8%，且MPKPE-NT误差最低（20.46 cm），证明了两个核心组件对于处理生成视频噪声的有效性和互补性。\n\n## 总结与启发\n本文的核心贡献在于：1）提出了首个使类人机器人能够执行视频生成模型所产生动作的通用框架，将生成视频转化为物理可行的机器人轨迹；2）设计了GenMimic策略，通过加权关键点奖励和对称性损失，实现了对噪声合成视频的鲁棒零样本模仿；3）构建了GenMimicBench合成人体运动数据集，为评估零样本泛化和策略鲁棒性建立了基准。\n\n论文提到的局限性包括：框架依赖外部的4D人体重建和重定向模块，这些模块的误差会影响最终性能；当前的策略未明确处理与物体的交互；在涉及复杂动态平衡（如快速转身）的任务上，真实世界的性能仍有下降。\n\n这项工作为利用视频生成模型进行机器人高层规划开辟了一条有希望的路径。其启示在于：将强大的生成模型与精心设计、具有物理归纳偏好的低层控制策略相结合，是实现通用机器人行为合成的可行方向。后续研究可以探索端到端的优化、更复杂的物体交互处理，以及如何将环境上下文更直接地纳入控制循环。",
      "imageUrls": [
        "https://arxiv.org/html/2512.05094v2/x1.png",
        "https://arxiv.org/html/2512.05094v2/x2.png",
        "https://arxiv.org/html/2512.05094v2/x3.png",
        "https://arxiv.org/html/2512.05094v2/x4.png",
        "https://arxiv.org/html/2512.05094v2/x5.png",
        "https://arxiv.org/html/2512.05094v2/x6.png",
        "https://arxiv.org/html/2512.05094v2/figures/hardware_setup.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.05599",
      "title": "An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation",
      "url": "http://arxiv.org/abs/2512.05599",
      "arxivId": "2512.05599",
      "date": "2025-12-05",
      "authors": "Panagiotis Chatzakos Team",
      "category": "Manipulation",
      "summary": "本文针对废旧电子设备中电池的安全、高效自动分拣难题，提出一种集成解决方案。系统核心技术包括：采用双能量X射线成像获取高对比度图像；利用YOLO和U-Net模型对含电池物品进行精确检测与分割；通过智能跟踪与定位算法，引导搭载吸盘的Delta机器人完成选择性抓取与丢弃。该方法已在NVIDIA Isaac Sim仿真环境及真实设备上得到验证。",
      "detailedSummary": "## 研究背景与动机\n当前，从废弃电气电子设备（WEEE）中移除电池主要依靠人工操作，原因是电子设备的高度复杂性和多样性。尽管已有许多系统提出使用X射线或RGB视觉检测方法，并通常由AI驱动的目标检测模型（如Mask R-CNN、YOLO、ResNets）驱动，用于WEEE的检查、分类或拆解，但这些方法存在局限性。例如，单能谱X射线成像对薄材料和致密材料的区分能力有限；现有视觉方法通常专用于特定类型的电池或组件，无法推广到多种WEEE；且多数方案仅为人工操作提供指导，未能实现全自动分拣。本文针对缺乏能够在多样WEEE中可靠检测电池并自动移除相应设备的全自主分拣系统这一痛点，提出了一个集成化的新视角。本文核心思路是：集成专用的双能X射线透射成像子系统与先进预处理算法，利用YOLO和U-Net模型精确检测和分割含电池物品，并通过智能跟踪与位置估计算法引导Delta机器人进行选择性抓取与丢弃。\n\n## 方法详解\n本文提出的全自动分拣系统整体框架包含四个紧密集成的模块：X射线成像、AI电池检测、WEEE位置估计和机器人分拣。输入是放置在传送带上的WEEE物品，输出是被分拣出的含电池设备。其工作流程为：WEEE物品经传送带通过X射线成像系统，生成高分辨率双能X射线图像；随后，YOLO模型检测电池位置，U-Net模型分割设备轮廓，结合轮廓检测算法匹配电池与所属设备并计算设备中心坐标；位置估计模块将图像坐标转换为空间坐标，并预测物品未来位置；最终，Delta机器人根据预测坐标执行抓取动作。\n\n![系统总览](https://arxiv.org/html/2512.05599v1/images/GrinnerSetup6.jpg)\n\n> **图1**：开发的WEEE分拣系统。(1)传送带，(2)X射线透射成像系统，(3)WEEE设备，(4)连接到(5)Delta机器人的吸盘式夹爪。a) X射线成像模块，b) Delta机器人，c) NVIDIA Isaac Sim仿真环境中的系统。\n\n**核心模块一：双能X射线成像**。系统采用Varex Imaging的DC-TDI800 X射线线阵扫描仪，支持原生双能成像。探测器对每个入射光子按其能量分别计数，同时生成两个能量域图像：高能图像（HE，光子能量E_γ > 60 keV）和总能量图像（TE，光子能量E_γ > 10 keV）。这种双能方法利用材料在不同X射线能谱下的吸收特性差异，增强了对WEEE中材料的区分和成分分析能力。成像软件通过缓冲区管理扫描线，并以50%的重叠率构建图像，确保所有物体都被捕获并送入后续处理流水线。为满足实时性要求，对原始16位整数数组进行了4×4像素合并处理。\n\n**核心模块二：AI电池检测与WEEE分割**。电池检测采用YOLO模型。为适应废物流中电池检测的特点（电池常成簇出现，无需像素级完美检测），论文提出了一种“改进召回率”评估算法：合并相邻电池的真实标注框，若预测框落在任一真实簇框内即视为真阳性。模型在包含374张独特图像（含212张训练、58张验证的含电池图像，以及74张训练、30张验证的无电池图像）的数据集上训练，电池被分为圆柱形、软包、纽扣和其他四类。设备分割采用U-Net模型，在一个包含41张图像及其分割掩码的较小数据集上训练5个周期即达到近乎完美的效果。最后，通过轮廓检测算法将分割掩码转换为边界框，并将YOLo检测到的电池匹配到其所属的设备边界框内，从而获得待移除设备的中心坐标和尺寸。\n\n![检测与分割流程](https://arxiv.org/html/2512.05599v1/images/XRayImageReconstructionAndBatteryDetection4.png)\n\n> **图3**：平板电脑、充电宝和笔记本电池的先进X射线成像、设备分割和电池检测。a) HE X射线图像，b) 使用U-Net模型的设备分割，c) 重建X射线图像上的电池检测及设备中心标注，d) 用于设备未来位置估计的定位图示。图像的宽度和高度对应于传送带上的特定平面。\n\n**核心模块三：WEEE位置估计**。根据已知的探测器位置和空间分辨率（0.1 mm/像素），将图像中设备中心的坐标(x_center, y_center)映射到传送带平面的空间坐标(x_0, y_0)。结合恒定的传送带速度v=0.35 m/s，通过公式(x_tp, y_tp) = (x_0 + v * tp, y_0)预测设备在未来时刻tp的位置，其中tp取决于Delta机器人在坐标系中的相对位置。\n\n**核心模块四：机器人分拣**。采用定制设计的Delta并联机器人执行高速拾放操作。机器人通过EtherCAT协议控制，末端配备吸盘式夹爪。为规划平滑的抓取轨迹，系统采用由四个点定义的三维π形轨迹，并使用三次样条插值（p=3）生成轨迹，以确保速度平滑和加速度连续，避免机械应力、过大扭矩需求和振动。通过逆运动学将笛卡尔空间轨迹转换到机器人的关节空间。\n\n**创新点**：1) **系统集成**：首次将双能X射线成像、AI检测分割与高速Delta机器人操作完整集成，实现WEEE中电池的全自动分拣。2) **成像与评估**：采用双能X射线增强材料区分，并针对应用场景提出了更合理的“改进召回率”评估指标。3) **仿真先行**：利用NVIDIA Isaac Sim构建高保真仿真环境，对机器人动力学、轨迹规划进行仿真和优化，再迁移到真实系统。\n\n## 实验与结果\n**数据集与实验平台**：实验使用了自定义收集的数据集，包含127个被扫描设备产生的374张独特X射线图像。真实实验平台包括Dorner传送带、VJX IXS1620 X射线源、Varex DC-TDI800探测器和定制Delta机器人。仿真实验在NVIDIA Isaac Sim中进行。\n\n**对比Baseline与评估指标**：论文主要将所提系统的各个组件性能与任务目标进行对比。对于电池检测任务，使用YOLO模型，并采用召回率、精确率、mAP、mAP (0.5 IoU)以及自定义的“改进召回率”进行评估。对于设备分割任务，使用U-Net模型并定性评估结果。\n\n**关键实验结果**：\n1. **电池检测性能**：如表I所示，在双能图像配置下，YOLO模型对所有电池类别的“改进召回率”均非常高，其中“圆柱形”电池达0.988，“其他”类达1.000，“软包”电池相对较低为0.783。这表明系统能可靠地检测出含电池设备，满足了核心应用需求。\n\n![类别分布](https://arxiv.org/html/2512.05599v1/images/class_distribution.png)\n\n> **图2**：训练和验证数据集中的类别实例分布。\n\n2. **设备分割结果**：U-Net模型经过5个周期的训练，在未见过的设备上产生了几乎完美的分割掩码（图3b），有效支撑了后续的设备定位。\n\n3. **系统流水线验证**：图7展示了从X射线图像采集到机器人成功抓取的真实流水线实验序列，证明了各模块集成工作的有效性。\n\n![流水线实验](https://arxiv.org/html/2512.05599v1/images/PipelineExperiment.png)\n\n> **图7**：分拣流水线实验。从左到右：a) 原始X射线图像采集，b) 电池检测与设备分割，c) 机器人抓取规划，d) Delta机器人执行抓取。\n\n**消融实验**：论文通过测试不同的图像配置（如仅使用HE、仅使用TE、HE/TE组合等）对YOLO检测性能的影响，发现将两个能量通道结合使用的配置在“改进召回率”等关键指标上表现最佳，从而验证了双能成像策略的有效性。\n\n## 总结与启发\n**核心贡献**：1) 提出并实现了一个完整的、全自动的WEEE分拣集成系统，能够可靠检测多样电子设备中的电池并利用Delta机器人自动移除。2) 创新性地将双能X射线成像技术与针对实际需求的改进评估指标相结合，提升了检测的鲁棒性和实用性。3) 利用高保真度的NVIDIA Isaac Sim仿真环境进行系统设计、测试与优化，为从虚拟到现实的迁移提供了有效途径。\n\n**局限性**：论文指出，当前系统主要针对含有电池的扁平电子设备进行分拣，这类设备构成了危险WEEE的很大一部分。对于形状非常不规则或结构极度复杂的设备，系统的普适性可能需要进一步验证和增强。\n\n**后续研究启示**：本工作的集成框架为自动化回收提供了范例。未来研究方向可包括：扩展系统处理更复杂、非扁平WEEE物品的能力；探索更高效的检测与分割算法以进一步提升处理速度和精度；以及将系统应用于其他需要内部检测和精准分拣的回收场景。",
      "imageUrls": [
        "https://arxiv.org/html/2512.05599v1/images/GrinnerSetup6.jpg",
        "https://arxiv.org/html/2512.05599v1/images/class_distribution.png",
        "https://arxiv.org/html/2512.05599v1/images/XRayImageReconstructionAndBatteryDetection4.png",
        "https://arxiv.org/html/2512.05599v1/images/delta_modeling.png",
        "https://arxiv.org/html/2512.05599v1/images/trajectory_explain.png",
        "https://arxiv.org/html/2512.05599v1/images/SortingProcedurePipeline_Vertical.png",
        "https://arxiv.org/html/2512.05599v1/images/PipelineExperiment.png",
        "https://arxiv.org/html/2512.05599v1/images/delta_plots_placeholder_vertical.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.05079",
      "title": "Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints",
      "url": "http://arxiv.org/abs/2512.05079",
      "arxivId": "2512.05079",
      "date": "2025-12-04",
      "authors": "Michael Posa Team",
      "category": "Manipulation",
      "summary": "本文解决机器人操作中因遮挡导致物体几何信息不完整、重建困难的问题。核心方法是结合两种互补信息源：基于流匹配3D生成模型的数据驱动形状先验，以及从物理交互中提取的稀疏接触边界约束。通过一种受“拖拽式编辑”启发的接触引导生成框架，将接触信息融入生成过程以消除歧义。实验表明，该方法在合成与真实数据上均优于纯3D生成或仅基于接触优化的方法，实现了更高质量和准确的重建。",
      "detailedSummary": "## 研究背景与动机\n机器人操作依赖于物体的几何信息，但相机只能捕获物体的部分观测，尤其是在发生遮挡时，这使得物体重建成为一个病态问题。当前主流方法主要分为两类：一是依赖数据驱动先验的3D生成模型，它们可以从单张图像重建完整物体，但在遮挡条件下性能显著下降；二是基于物理的优化方法，例如利用接触信息来约束几何形状，但这通常需要良好的初始化，并且仅凭稀疏接触点难以重建复杂形状。本文针对遮挡下视觉信号模糊性这一具体痛点，提出将数据驱动的生成先验与物理驱动的接触约束相结合的新视角。核心思路是利用生成模型对常见物体形状的先验知识来“补全”不可见部分，同时利用从视频或交互中获得的稀疏接触点作为边界约束，通过一种引导生成的方式融合二者，从而在遮挡下实现高质量且准确的3D重建。\n\n## 方法详解\n本文方法的核心是将融合问题构建为一个**引导的流匹配（guided flow-matching）问题**。整体流程是：给定一张遮挡的物体图像及对应的可见性掩码，首先使用一个专为遮挡设计的3D生成模型（Amodal3R）进行无引导的初始生成，获得一个参考几何；然后，利用从视频（Vysics）或合成数据中获取的接触点，构建一个基于“拖拽”思想的能量函数，在流匹配模型的去噪（推理）过程中施加引导，使最终生成的几何表面通过或接近这些接触点。\n\n![方法框架](https://arxiv.org/html/2512.05079v1/figs/title_small.png)\n> **图1**：方法整体框架。左侧输入为遮挡图像和接触点，中间展示了结合数据驱动的3D先验和物理接触约束进行引导生成的过程，右侧输出为重建的3D几何。\n\n**核心模块与技术细节**：\n1.  **基础生成模型**：采用Amodal3R作为基础3D生成器。它是基于TRELLIS（一个流匹配模型）微调而来，专为处理遮挡图像设计，输入包括RGB图像、物体可见性掩码和遮挡物掩码。其生成分为两阶段：首先生成分辨率为64³的稀疏体素结构（表示整体几何），然后生成附在体素上的结构化潜在特征以恢复细节。本文的引导主要施加在第一阶段的稀疏结构生成上。\n2.  **基于拖拽的接触损失函数**：这是关键的创新点。直接鼓励接触点处的单个体素被占据的损失函数过于局部，难以引导全局形状。受2D图像拖拽编辑的启发，作者将满足接触约束视为将物体表面上的一个点“拖拽”到接触点位置。\n    *   **参考生成**：首先运行一次无引导的Amodal3R，得到参考体素占据网格 `s0*`。\n    *   **损失计算**：对于每个接触点 `p^c`，在参考网格 `s0*` 中找到最近的被占据体素位置 `p^s`。引导损失鼓励在引导生成过程中，预测的体素占据值 `ŝ0` 在接触点 `p^c` 周围的邻域内，与参考网格在 `p^s` 周围对应邻域的值相似。公式为：`L(𝑥̂0; p^c) = Σ_(Δp∈N_ε) || ŝ0(p^c+Δp) - s0*(p^s+Δp) ||²`。其中 `N_ε` 是一个21x21x21体素的3D邻域。这样，引导引入的偏差依赖于模型自身根据输入图像生成的特定形状先验，而非手动启发式规则。\n3.  **引导调度与衰减**：如何确定引导强度系数 `λ_t` 至关重要。\n    *   **调度**：经验发现，在去噪过程的中期施加强引导，在开始和末期施加弱引导效果最好。具体将12个时间步分为早、中、晚三期，分别设置 `λ_t^schedule` 为0.2, 1.0, 0.5。\n    *   **衰减**：为避免不同输入图像导致梯度爆炸或引导失效，引入了衰减因子 `λ_t^attenuation = ||∇_(𝑥̂0) J(𝑥̂0)|| / ||∇_(x_t) J(𝑥̂0)||`，以稳定引导信号的尺度。最终 `λ_t = λ_t^schedule * λ_t^attenuation`。\n4.  **循环引导**：借鉴扩散模型中的自循环引导，在每个流匹配时间步内，进行 `m=3` 次引导更新（如算法1所示），这有助于更好地遵循引导而不需要过大的 `λ` 值，从而保持生成质量。\n\n与现有方法相比，创新点体现在：1) **问题视角**：首次系统地将数据驱动的生成先验与物理接触约束在3D重建中结合。2) **引导机制**：为原生3D流匹配模型设计了一种无需训练的引导方法，并创新性地将2D的“拖拽”编辑思想转化为3D空间中的接触损失函数。3) **实践技巧**：提出了自适应的引导调度和衰减策略，解决了引导过程中的不稳定问题。\n\n## 实验与结果\n**实验设置**：\n*   **数据集与平台**：在合成数据（Google Scanned Objects, GSO）和真实世界数据（Vysics发布的Franka机器人交互视频）上进行了实验。\n*   **Baseline方法**：主要对比了未使用接触引导的 **Amodal3R** 模型，以及**没有使用循环引导的版本**（Guided w/o recurrence）。\n*   **评估指标**：包括Chamfer距离（↓越小越好）、不同阈值下的F-score（F@0.01等，↑越大越好）以及RGB图像的LPIPS（↓越小越好）。\n\n**关键实验结果**：\n在GSO合成数据集上的定量结果如表I所示。与未引导的Amodal3R相比，本文的完整方法将平均Chamfer距离从0.0478显著降低至0.0385，中位数从0.0425降低至0.0332。在各个F-score指标上也有全面提升，例如F@0.02的平均值从0.621提升至0.640。这表明接触引导有效提升了重建的几何精度。\n\n![定性对比与热力图](https://arxiv.org/html/2512.05079v1/figs/heatmap_small.png)\n> **图2**：几何重建的定性对比。热力图显示了预测形状到真值（GT）的单向点对点Chamfer距离误差（未引导/引导预测上）以及引导带来的改进（GT上）。可见引导后（右列）预测形状更接近真值，误差（红色区域）显著减少。\n\n![接触点引导效果示例](https://arxiv.org/html/2512.05079v1/figs/contactpoints3_small.png)\n> **图3**：接触点引导效果的定性示例。展示了两个视角的预测网格。红色点为施加引导的接触点，黑色点为其在预测表面上最近的点。可以看到，引导后的预测表面（右列）成功地将几何“拉向”了接触点位置（红-黑连线变短），从而更符合真实形状（灰色网格）。\n\n![真实世界数据坐标转换流程](https://arxiv.org/html/2512.05079v1/figs/real_world_system.png)\n> **图4**：将Vysics估计的接触点转换到Amodal3R预测空间的过程示意图。涉及尺度估计和坐标系对齐（通过 `T_V^A` 变换）。\n\n![真实世界定性结果](https://arxiv.org/html/2512.05079v1/figs/rgb_normal2.png)\n> **图5**：在真实世界数据上的定性重建结果。对比了未引导（Unguided）、仅使用Vysics、以及本文方法（Ours）的重建网格和法线图。本文方法结合了生成先验和接触信息，得到了更完整和准确的几何。\n\n![Vysics数据上的更多定性对比](https://arxiv.org/html/2512.05079v1/figs/vysics_qualitative_small.png)\n> **图6**：在Vysics真实世界数据上更多物体的定性对比。同样展示了未引导、Vysics、本文方法以及真值（GT）。本文方法在严重遮挡下（如锤子、剪刀手柄）能更好地恢复被遮挡部分的合理形状。\n\n**消融实验**：表I中“Guided w/o recurrence”一行即消融了循环引导机制。结果显示，完整方法（带循环引导）在大多数指标上优于不带循环引导的版本，尤其是在Chamfer距离的中位数上（0.0332 vs 0.0359），证明了循环引导对于稳定和增强引导效果的重要性。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了一个新颖的融合框架**：首次将数据驱动的3D生成先验与物理驱动的接触约束系统结合，用于解决遮挡下的物体重建问题。\n2.  **开发了一种用于原生3D生成的引导方法**：创新地提出了基于“拖拽”思想的接触损失函数，并配套了引导调度、衰减和循环引导策略，实现了无需训练的高效可控生成。\n3.  **实验验证了方法的优越性**：在合成与真实数据上均证明，该方法相比纯生成或纯接触优化方法，能实现更高质量和更高精度的3D重建。\n\n**局限性**：论文提到，方法的性能依赖于接触点估计的质量（如Vysics的输出）。此外，引导过程涉及额外的梯度计算和循环步骤，会增加一定的计算开销。\n\n**对后续研究的启示**：\n1.  **多模态信息融合**：展示了如何将看似不同的数据源（统计先验与物理约束）互补地结合，为解决病态感知问题提供了新思路。\n2.  **训练免费引导的潜力**：为预训练的大型生成模型（尤其是3D模型）引入新的控制信号提供了一种无需微调的有效途径，具有很好的通用性启示。\n3.  **机器人感知的应用前景**：该方法直接面向机器人操作场景，将视觉感知与物理交互信息结合，有助于构建更准确、物理可信的环境模型，提升机器人操作的鲁棒性和泛化能力。",
      "imageUrls": [
        "https://arxiv.org/html/2512.05079v1/figs/title_small.png",
        "https://arxiv.org/html/2512.05079v1/figs/heatmap_small.png",
        "https://arxiv.org/html/2512.05079v1/figs/contactpoints3_small.png",
        "https://arxiv.org/html/2512.05079v1/figs/real_world_system.png",
        "https://arxiv.org/html/2512.05079v1/figs/rgb_normal2.png",
        "https://arxiv.org/html/2512.05079v1/figs/vysics_qualitative_small.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.05107",
      "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2512.05107",
      "arxivId": "2512.05107",
      "date": "2025-12-04",
      "authors": "Benjamin Busam Team",
      "category": "Manipulation",
      "summary": "本文针对Vision-Language-Action (VLA)模型微调中，现有方法将长时程动作轨迹视为语言序列进行轨迹级优化，导致信用分配粗糙、训练不稳定的问题，提出了Stage-Aware Reinforcement (StARe)模块。该模块将动作轨迹分解为语义阶段，提供密集、可解释的强化信号。基于此，发展了Stage-Aware TPO (StA-TPO)和Stage-Aware PPO (StA-PPO)，并结合Imitation → Preference → Interaction (IPI)序列微调管道。实验在SimplerEnv和ManiSkill3上实现最先进性能，成功率分别达98.0%和96.4%。",
      "detailedSummary": "## 研究背景与动机\n当前微调视觉-语言-动作（VLA）模型的主流方法，如轨迹级偏好优化（TPO）和近端策略优化（PPO），通常将长时程动作序列视为一个整体进行优化。这种方法存在关键局限性：信用分配模糊，难以确定轨迹中具体哪个阶段导致了成功或失败；奖励稀疏，尤其是在长时程任务中；以及由于巨大的优化空间导致训练不稳定。然而，与语言推理中句子顺序灵活但语义统一不同，动作轨迹由因果链式、学习难度各异的语义阶段构成。本文针对长时程动作轨迹的阶段性特点和现有轨迹级优化方法信用分配粗糙的痛点，提出了将轨迹分解为语义阶段并进行渐进式阶段感知优化的新视角。本文核心思路是设计一个阶段感知强化（StARe）模块来分解轨迹并提供密集的阶段对齐奖励信号，进而构建一个包含模仿、偏好、交互的三阶段序列微调流程（IPI）。\n\n## 方法详解\n整体框架是一个序列化的三阶段微调流程，称为 IPI（模仿→偏好→交互）。该流程首先使用专家演示通过监督微调（SFT）初始化模型；然后利用阶段感知轨迹偏好优化（StA-TPO）基于离线阶段偏好数据进行优化；最后通过阶段感知近端策略优化（StA-PPO）在在线环境中进行阶段内交互优化。输入是预训练的VLA策略、任务指令和环境观测，输出是优化后的策略，其在长时程任务中具有更高的成功率和泛化能力。\n\n![方法框架](https://arxiv.org/html/2512.05107v2/images/robot/tasks.png)\n> **图2**：StARe框架及其集成到IPI训练流程的概览。展示了IPI流程的三个步骤：模仿学习（SFT）、离线阶段感知偏好优化（StA-TPO）和在线阶段感知交互优化（StA-PPO）。核心是StARe模块，它接收轨迹（来自数据或模型交互），通过阶段分离器划分阶段，并通过阶段计算器计算阶段成本和每步奖励。\n\n核心模块是阶段感知强化（StARe）模块，它包含两个组件：\n1.  **阶段分离器**：基于任务特定语义规则（例如末端执行器与目标的相对位置和方向是否超过阈值）检测阶段转换事件，将完整轨迹τ分解为K个语义阶段片段{τ^(1), ..., τ^(K)}，并为每个时间步t分配阶段标签k。\n2.  **阶段计算器**：用于评估每个阶段的执行质量。它计算**阶段成本**（如公式(3)，以“到达”阶段为例，计算末端执行器与目标物体之间的平均欧氏距离）和**阶段内密集奖励**。密集奖励通过基于势函数的奖励塑形实现（公式(4)-(5)），例如，为“到达”阶段定义一个关于距离的势函数，通过相邻时间步势函数之差来提供平滑的渐进奖励信号。\n\n基于StARe模块，论文提出了两个核心算法：\n*   **阶段感知轨迹偏好优化（StA-TPO）**：这是对离线偏好优化方法TPO的改进。如算法1所示，StARe将比较的成功与失败轨迹对(τ+, τ-)分解为阶段。然后，在计算每个阶段的偏好得分q(τ^(k))时，引入了阶段成本ℓ_k(τ^(k))作为惩罚项（公式(6)），形成修正得分q̂(τ^(k))。损失函数ℒ_StA-TPO促使模型在阶段级别上偏好得分更高（即成本更低）的轨迹。这使得优化不仅能区分成功与失败，还能区分不同程度的成功，实现了更细粒度的信用分配。\n*   **阶段感知近端策略优化（StA-PPO）**：这是对在线强化学习方法PPO的改进。如算法2所示，在策略与环境交互的每一步，StARe在线确定当前阶段k并计算势函数Φ_kt(s_t)。随后，将原始的稀疏奖励r_t替换为通过势函数塑形后的密集奖励r'_t（公式(5)）。PPO的目标函数ℒ_PPO中的奖励相应被替换，形成ℒ_StA-PPO。这为策略学习提供了更密集、与阶段进展对齐的反馈，加速了在稀疏奖励长时程任务中的学习。\n\n与现有方法相比，创新点具体体现在：1) **阶段分解**：不同于将轨迹视为整体，显式建模了动作轨迹的语义阶段结构。2) **阶段成本惩罚**：在StA-TPO中引入阶段成本，实现了对阶段执行质量的细粒度偏好优化。3) **潜在奖励塑形**：在StA-PPO中利用阶段势函数提供密集的渐进式奖励，缓解了稀疏奖励问题。\n\n## 实验与结果\n实验使用了两个机器人操作基准测试框架：SimplerEnv和ManiSkill3。对比的基线方法包括：监督微调（SFT）、轨迹级偏好优化（TPO）、近端策略优化（PPO），以及结合了SFT与TPO或PPO的混合方法。实验平台未明确说明，但涉及模拟环境。\n\n关键实验结果：在SimplerEnv任务上，仅使用SFT的成功率为85.4%，SFT+TPO为91.7%，SFT+PPO为95.5%。而本文提出的完整IPI流程（SFT + StA-TPO + StA-PPO）取得了98.0%的成功率，达到了最先进水平。在ManiSkill3任务上，IPI流程取得了96.4%的平均成功率，显著优于SFT（87.6%）、SFT+TPO（92.0%）和SFT+PPO（94.8%）等基线方法。\n\n![主实验结果](https://arxiv.org/html/2512.05107v2/x2.png)\n> **图3**：在SimplerEnv和ManiSkill3上的主要性能对比。柱状图清晰显示，IPI流程（即SFT+StA-TPO+StA-PPO）在两个基准测试上都取得了最高的平均成功率。\n\n![消融实验](https://arxiv.org/html/2512.05107v2/x3.png)\n> **图4**：消融研究结果。左图显示，在IPI流程中，依次添加StA-TPO和StA-PPO都能带来性能提升，验证了每个步骤的必要性。右图对比了不同奖励设计，表明StARe提供的阶段感知密集奖励（Ours）优于轨迹级密集奖励和纯稀疏奖励。\n\n![泛化性能](https://arxiv.org/html/2512.05107v2/x4.png)\n> **图5**：分布外泛化性能。IPI流程在物体位置、姿态和任务指令分布变化的泛化设置下，均表现出比基线方法（SFT+PPO）更优的性能，说明阶段感知优化有助于学习更鲁棒的策略。\n\n![定性结果](https://arxiv.org/pdf/2512.05107v2)\n> **图6**：定性结果示例。展示了IPI与基线方法（SFT+PPO）在失败案例上的对比。IPI能成功完成长时程任务（如堆叠），而基线方法在复杂阶段（如精确放置）容易失败。\n\n消融实验总结：1) **IPI流程的有效性**：在SFT基础上，依次加入StA-TPO和StA-PPO，性能逐步提升至最优。2) **奖励设计的重要性**：使用StARe的阶段感知密集奖励显著优于使用轨迹级密集奖励或仅使用原始稀疏奖励。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) 提出了**StARe模块**，一个基于规则、能将长时程动作轨迹分解为语义阶段并提供密集评估信号的插件。2) 基于StARe，设计了**StA-TPO和StA-PPO算法**，分别用于离线阶段感知偏好优化和在线阶段感知交互优化，实现了更精确的信用分配。3) 整合SFT、StA-TPO和StA-PPO，提出了**IPI序列微调流程**，为VLA模型微调提供了一个统一且高效的框架，并在基准测试中达到了最先进的性能。\n\n论文自身提到的局限性主要在于StARe模块对**任务特定规则**的依赖，其阶段划分和成本计算需要针对不同任务进行设计，可能限制了方法的通用性。此外，IPI的三阶段序列训练可能带来较高的计算成本。\n\n对后续研究的启示包括：1) **阶段感知优化**是提升长时程VLA任务性能的有效方向，未来可探索更通用或可学习的阶段划分方法。2) **混合训练流程**（结合模仿、偏好、交互）显示出优势，值得在其他序列决策问题中进一步探索。3) **密集、可解释的奖励设计**对于复杂机器人任务至关重要，基于语义阶段的奖励塑形是一个有前景的思路。",
      "imageUrls": [
        "https://arxiv.org/html/2512.05107v2/x1.png",
        "https://arxiv.org/html/2512.05107v2/images/robot/tasks.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04813",
      "title": "MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.04813",
      "arxivId": "2512.04813",
      "date": "2025-12-04",
      "authors": "Gao Huang Team",
      "category": "Manipulation",
      "summary": "本文针对机器人模仿学习中因静态数据收集导致空间泛化能力不足的问题，提出MOVE数据收集范式。其核心是在单条演示轨迹中为可移动物体注入运动，从而隐式生成密集多样的空间配置，提升数据效率。实验表明，在需要强空间泛化的模拟任务中，MOVE平均成功率达39.1%，较静态方法（22.2%）相对提升76.1%，并在部分任务上实现2–5倍的数据效率增益。",
      "detailedSummary": "## 研究背景与动机\n机器人模仿学习方法虽然前景广阔，但其实际部署从根本上受限于数据稀缺。尽管已有工作致力于收集大规模数据集，但在实现鲁棒的空间泛化方面仍存在显著差距。当前主流的数据收集范式是静态的：每条专家轨迹，无论其长度如何，通常都是在环境单一、固定的空间配置下收集的，包括静止的物体和目标位置以及不变的相机视角。这严重限制了可用于学习的空间信息多样性，导致训练出的策略在面对新的物体姿态、目标位置或相机视角时泛化能力急剧下降。如表I所示，随着空间维度（物体位置、目标位置、相机视角）的增加，策略的成功率呈指数级下降。\n\n本文针对这一数据效率的关键瓶颈，提出了一个新颖的视角：挑战静态数据收集范式本身。其核心思路是，在收集每条专家演示轨迹时，主动、持续地移动场景中的关键物体（如抓取对象、目标对象）和相机，从而将一种强大的数据增强形式直接嵌入到每个轨迹中，使单个轨迹能够编码一个连续变化的空间配置片段，而非单个点，以此显著提升每个轨迹的空间信息密度和多样性。\n\n## 方法详解\nMOVE的整体框架是一个创新的数据收集流程，其核心在于“空间配置增强”策略，而非特定的策略学习算法。流程分为两步：首先，按照MOVE范式收集动态演示数据；其次，使用这些数据训练策略模型（本文采用Diffusion Policy）。\n\n核心模块是空间配置增强策略，旨在为每条轨迹注入运动，以覆盖更丰富的空间配置。具体技术细节如下：\n1.  **物体平移**：为确保覆盖整个工作空间，为抓取对象和目标对象模拟线性运动轨迹。物体在时间t的位置由其初始位置、恒定速度向量和运动方向决定。速度从Beta分布中采样，参数设置为α=2, β=5，这使得速度更可能接近0而非最大值，有助于模型学习鲁棒的抓取策略。\n2.  **物体旋转**：为提高对物体朝向的泛化能力，引入绕垂直轴的恒定角速度旋转。旋转角度随时间线性变化，角速度同样从Beta分布采样。此增强特别适用于非对称物体（如带把手的杯子）。\n3.  **相机运动**：为模拟非静态视角，虚拟相机沿相对于场景中心的受限圆柱形路径移动。其位置更新方式与物体平移类似。\n4.  **组合增强策略**：并非同时应用所有运动，而是根据任务的语义阶段采用分阶段策略。例如，在“关盒子”任务中，轨迹分为抓取阶段和放置阶段。在抓取阶段，仅对抓取对象（盒盖）应用平移和旋转，并引入相机运动；在放置阶段，仅对目标对象（盒身）应用线性平移，并继续动态相机运动。这种策略将MOVE扩展到多个空间维度。\n\n与现有方法（如ADC）相比，MOVE的创新点在于其“连续性”。ADC在一条轨迹中仅包含几个离散的物体位置重置点，而MOVE通过持续运动，在位置空间中形成一条连续曲线，从而捕获了更密集、更丰富的空间信息，并能自然地扩展到相机运动等更多维度。\n\n![方法框架](https://arxiv.org/html/2512.04813v1/pic/dynamic.png)\n> **图2**：MOVE数据收集范式概览。（左）静态范式与MOVE的概念性对比。静态范式从离散、固定的空间配置中采样，每条轨迹代表空间配置空间中的一个点。而MOVE收集的每条轨迹被视为一个连续片段，物体、目标和相机都在运动中，从而在单条轨迹内产生密集且多样的空间配置集合。（右）展示了MOVE中采用的几种运动增强形式：平移、旋转和相机运动。\n\n## 实验与结果\n**实验设置**：\n- **Benchmark/数据集**：仿真实验使用Meta-World基准测试，选取了10个具有不同难度的操作任务（如Pick-Place, Assembly, Box-Close等）。真实世界实验使用Agilex PIPER机械臂进行“抓取-放置”任务（抓取橙子放到托盘）。\n- **Baseline方法**：1) 传统的静态数据收集范式；2) ADC方法（一种在轨迹中周期性重置物体位置的对抗性数据收集方法）。\n- **评估指标**：主要报告任务成功率。在真实实验中还报告了归一化得分（将任务分解为多个步骤进行评分）。\n\n**关键实验结果**：\n1.  **初步实验（空间泛化可视化）**：在Pick-Place任务中，从9个离散点稀疏采样。尽管MOVE和静态方法在相同的抓取点进行抓取，且总时间步数相同，但MOVE展现出远优于静态方法的泛化能力。\n![稀疏采样对比](https://arxiv.org/html/2512.04813v1/pic/static.png)\n> **图1**：在9个训练点上稀疏采样后，策略在整个空间上的成功率热图。静态方法仅在训练点附近成功（成功率29.5%），而MOVE能在整个空间的大部分区域成功（成功率80.8%），泛化能力显著更强。\n\n2.  **密集采样与圆形采样实验**：在密集均匀采样设置下，MOVE将成功率从66%提升至74%。当采样点限制在一个圆环上时，MOVE不仅在圆环内（44% vs 21%），在圆环外区域（67% vs 45%）也显著优于静态方法。\n![密集采样对比](https://arxiv.org/html/2512.04813v1/pic/static_90p.png)\n> **图3**：密集均匀采样下的泛化对比。MOVE（右）相比静态方法（左）在状态空间难以学习的区域取得了实质性改进，整体成功率从66%提升至74%。\n![圆形采样对比](https://arxiv.org/html/2512.04813v1/pic/static-circle.png)\n> **图5**：采样点限制在圆形区域时的泛化对比。MOVE（右）不仅在圆形区域内表现更好，在圆形区域外也显著优于静态方法（左），展示了其强大的外推泛化能力。\n\n3.  **仿真环境主实验**：在10个Meta-World任务上，MOVE平均成功率达到39.1%，相比静态范式（22.2%）相对提升76.1%。在数据效率方面，MOVE仅用20k时间步的动态数据集就能达到静态方法50k甚至100k时间步数据集的性能，在某些任务上实现了2-5倍的数据效率提升。\n![数据缩放曲线](https://arxiv.org/html/2512.04813v1/x1.png)\n> **图4**：10个仿真任务上，不同数据规模（时间步数）下的成功率曲线。MOVE（橙色）在所有数据规模点上都一致且显著地优于静态范式（蓝色）。\n\n4.  **真实环境实验**：在高度随机化的真实世界抓取-放置任务中，仅使用35k时间步的MOVE数据，策略成功率达到23.3%，远超同等数据规模的静态方法（3.3%），并与使用超过两倍数据（75k）的静态方法性能（23.3%）相当。\n\n5.  **消融实验**：研究了不同动态维度组合的影响。结果表明，组合多个动态维度（如同时移动抓取对象、目标对象和相机）比仅使用单个动态维度能带来更显著的性能提升，验证了组合策略的有效性。\n![消融实验](https://arxiv.org/html/2512.04813v1/x12.png)\n> **图6**：在Pick-Place和Assembly任务上，不同动态维度组合的消融研究结果。“All”表示组合所有相关动态维度，其性能显著优于仅使用单个维度（如仅移动抓取对象“O_pick”或仅移动相机“Camera”）。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出MOVE范式**：首次系统性地提出并验证了通过在数据收集过程中注入连续运动来增强空间信息密度的思想，挑战了传统的静态数据收集范式。\n2.  **实现显著的性能与效率提升**：在仿真和真实环境中，MOVE大幅提升了策略的空间泛化能力（平均相对提升76.1%）和数据效率（可达2-5倍）。\n3.  **提供可扩展的增强策略**：设计了包括平移、旋转、相机运动及其分阶段组合的增强方法，可灵活应用于多维度空间变化。\n\n**局限性**：论文提到，动态轨迹通常比静态轨迹更长，这可能会略微增加单条轨迹的收集时间。此外，在真实世界中，需要人类演示者适应并操作运动中的物体，这可能带来一定挑战。\n\n**对后续研究的启示**：本文的工作将研究焦点从纯粹的模型设计转向了数据收集过程本身。它表明，通过精心设计的数据收集策略，即使数据集规模不变，也能极大提升学习到的策略的泛化能力。这为构建更高效、更通用的机器人学习系统开辟了一条新的路径，即“动态数据收集”。未来的工作可以探索更智能的运动规划、结合语义信息的增强策略，以及将MOVE思想与其他数据生成方法（如仿真、3D重建）相结合。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04813v1/pic/static.png",
        "https://arxiv.org/html/2512.04813v1/pic/dynamic.png",
        "https://arxiv.org/html/2512.04813v1/x1.png",
        "https://arxiv.org/html/2512.04813v1/pic/static_90p.png",
        "https://arxiv.org/html/2512.04813v1/pic/dynamic_90p.png",
        "https://arxiv.org/html/2512.04813v1/x2.png",
        "https://arxiv.org/html/2512.04813v1/x3.png",
        "https://arxiv.org/html/2512.04813v1/x4.png",
        "https://arxiv.org/html/2512.04813v1/x5.png",
        "https://arxiv.org/html/2512.04813v1/x6.png",
        "https://arxiv.org/html/2512.04813v1/x7.png",
        "https://arxiv.org/html/2512.04813v1/x8.png",
        "https://arxiv.org/html/2512.04813v1/x9.png",
        "https://arxiv.org/html/2512.04813v1/x10.png",
        "https://arxiv.org/html/2512.04813v1/x11.png",
        "https://arxiv.org/html/2512.04813v1/pic/static-circle.png",
        "https://arxiv.org/html/2512.04813v1/pic/dynamic-circle.png",
        "https://arxiv.org/html/2512.04813v1/x12.png",
        "https://arxiv.org/html/2512.04813v1/x13.png",
        "https://arxiv.org/html/2512.04813v1/x14.png",
        "https://arxiv.org/html/2512.04813v1/x15.png",
        "https://arxiv.org/html/2512.04813v1/pic/output_cv.jpg",
        "https://arxiv.org/html/2512.04813v1/x16.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04463",
      "title": "MARL Warehouse Robots",
      "url": "http://arxiv.org/abs/2512.04463",
      "arxivId": "2512.04463",
      "date": "2025-12-04",
      "authors": "Salmon Riaz Team",
      "category": "Manipulation",
      "summary": "本文研究多智能体强化学习在仓库自动化中的协同问题，重点解决稀疏奖励下的协调与信用分配难题。通过比较QMIX（基于价值分解）与IPPO（独立学习）算法，发现QMIX采用超网络混合单调值函数，在CTDE框架下显著优于独立学习。实验表明，QMIX在RWARE环境中平均回报达3.25（IPPO仅0.38），但需超500万步的epsilon退火以应对稀疏奖励。经100万步训练后，智能体在Unity仿真中成功实现稳定包裹配送，但规模扩展至4台以上机器人仍存在挑战。",
      "detailedSummary": "## 研究背景与动机\n当前仓库自动化主要依赖集中式规划系统，面临扩展性限制。多智能体强化学习（MARL）通过去中心化的学习策略提供了一种替代方案，但需要解决信用分配问题。在仓库协调任务中，奖励通常是稀疏的（仅在成功交付时获得），这使得智能体难以探索到有效策略并准确评估个体贡献。\n\n本文针对稀疏奖励下的多机器人协调问题，系统性地比较了价值分解与独立学习等MARL方法在仓库自动化任务中的有效性。核心思路是通过在标准RWARE基准和自定义Unity 3D仿真中对比QMIX与IPPO等算法，揭示价值分解方法在稀疏奖励协调任务中的优势，并明确其成功所依赖的关键超参数配置。\n\n## 方法详解\n研究的整体流程遵循从验证到部署的路径：首先在相对简单的MPE（Multi-Agent Particle Environment）环境中验证算法基础实现，然后过渡到更具挑战性的标准仓库基准RWARE（Robotic Warehouse）环境进行评估，最后将表现最佳的算法集成到自建的Unity 3D仿真中进行验证和演示。\n\n![超参数配置对比](https://i.imgur.com/placeholder.png)\n> **图1**：默认与优化后的超参数配置对比表。优化配置在批量大小、缓冲区大小，尤其是探索退火时间上进行了大幅调整。\n\n研究的核心是QMIX算法。QMIX通过一个混合网络（Mixing Network）对联合行动价值函数进行分解：$Q_{tot}(\\boldsymbol{\\tau},\\boldsymbol{a})=f_{mix}(Q_{1}(\\tau_{1},a_{1}),...,Q_{n}(\\tau_{n},a_{n});s)$。其中，$f_{mix}$ 是一个权重非负的神经网络，确保了单调性约束 $\\frac{\\partial Q_{tot}}{\\partial Q_{i}}\\geq 0$，即每个智能体的个体Q值增长必须导致联合Q值增长。本文采用基于GRU（64个隐藏单元）的智能体网络和两层超网络混合器。\n\n与现有方法相比，本文的核心创新点并非提出新算法，而是通过详尽的实验揭示了在稀疏奖励的仓库任务中，**成功应用现有QMIX算法所必需的、与默认配置截然不同的超参数设置**。如表1所示，关键的优化包括：将经验回放缓冲区大小从5,000大幅增加至200,000；最为重要的是，将ε-贪婪策略的探索退火时间从默认的50,000步延长至5,000,000步以上。这种“扩展探索”对于智能体在稀疏奖励环境中发现有效行为序列至关重要。训练总步数也相应增加至20,000,000步。\n\n## 实验与结果\n**实验设置**：研究使用了三个主要环境：1) MPE（用于初步验证），2) RWARE标准基准（核心评估环境，包括`tiny-2ag-v2`、`small-4ag-v1`、`medium-6ag-v1`等不同规模和难度配置），3) 自定义的Unity ML-Agents 3D仿真环境（用于验证sim-to-sim迁移）。实验平台包括本地机器和Windows Server 2022服务器。\n\n**对比方法**：主要对比了QMIX（价值分解）、IPPO（独立近端策略优化，分为“高级”和“基础”版本）以及随机基线。也提及了MASAC（多智能体软演员-评论家）在MPE环境中的表现。\n\n**关键实验结果**：\n在RWARE的`tiny-2ag-v2`任务中，QMIX取得了平均测试回报3.25，而经过调优的“高级”IPPO仅获得0.38，QMIX性能超出8.5倍。即使是训练步数多4倍的“基础”IPPO，回报也仅为0.13。\n\n![算法性能对比表](https://i.imgur.com/placeholder.png)\n> **图2**：不同算法在RWARE环境下的测试回报对比表。清晰展示了QMIX相对于IPPO和随机基线的显著优势。\n\n**缩放分析**：随着智能体数量增加，QMIX性能下降且所需训练步数超线性增长。从2智能体扩展到6智能体，所需训练步数从20M增至40M（翻倍），而测试回报从3.25降至1.45（下降55%）。这突显了联合行动空间随智能体数量指数增长带来的挑战。\n\n![缩放分析结果表](https://i.imgur.com/placeholder.png)\n> **图3**：QMIX在不同规模RWARE环境下的缩放分析结果。显示了智能体数量增加对性能和训练成本的影响。\n\n**Unity部署结果**：在Unity 3D环境中，经过1百万训练步后，智能体达到了平均测试回报238.6（该环境奖励尺度与RWARE不同），并表现出稳定的包裹拾取与交付行为。控制台日志确认了协调行为的成功迁移，尽管环境从网格离散世界转为连续物理仿真。\n\n## 总结与启发\n**核心贡献**：\n1.  **超参数关键性分析**：明确指出默认的MARL超参数配置在稀疏奖励的仓库任务中完全失败，并验证了扩展探索（≥5M步ε退火）和大经验回放缓冲区是成功的必要条件。\n2.  **系统的算法比较**：通过实验证实，在研究的稀疏奖励协调任务中，基于价值分解的QMIX显著优于独立学习方法的IPPO（8.5倍性能提升），为解决信用分配问题提供了实证支持。\n3.  **仿真集成与验证**：成功将训练好的QMIX策略集成到自定义的Unity 3D仿真中，实现了从网格世界到连续视觉仿真的“sim-to-sim”迁移，并展示了可观察的协调行为。\n\n**局限性**：论文自身明确指出，研究仅限于仿真评估，未涉及真实物理机器人；测试的智能体规模较小（2-6个），未触及工业级部署（50+机器人）的复杂度；任务结构经过简化；冗长的训练需求可能影响实际部署的可行性。\n\n**对后续研究的启示**：\n1.  **面向稀疏任务的算法配置**：为MARL在稀疏奖励现实任务中的应用提供了重要的超参数调优实践指南，强调不能直接套用为密集奖励设计的默认配置。\n2.  **规模化挑战**：研究结果凸显了将MARL扩展到更多智能体时的严峻挑战，指向了未来需要研究分层分解、任务分区或通信机制来解决大规模协同问题。\n3.  **从仿真到现实（Sim-to-Real）的路径**：虽然实现了sim-to-sim迁移，但最终通向物理部署仍需研究领域随机化、鲁棒感知和安全约束等关键问题。",
      "imageUrls": [],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04952",
      "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization",
      "url": "http://arxiv.org/abs/2512.04952",
      "arxivId": "2512.04952",
      "date": "2025-12-04",
      "authors": "Hang Zhao Team",
      "category": "Manipulation",
      "summary": "本文针对自回归视觉语言动作模型在动作标记化时面临的重建保真度与推理效率的权衡问题，提出FASTer框架。其核心技术包括：1) 可学习的动作标记器FASTerVQ，将动作块编码为单通道图像以捕获时空依赖性；2) 基于此的自回归策略FASTerVLA，采用块状解码与轻量级动作专家。实验表明，该框架在模拟与真实基准测试中，同时实现了更快的推理速度与更高的任务性能，超越了现有先进模型。",
      "detailedSummary": "## 研究背景与动机\n当前机器人领域的视觉-语言-动作（VLA）模型主要分为扩散模型和自回归模型两类。扩散模型在操作精度上表现出色，但在利用关键视觉和语言线索方面存在不足。自回归VLA模型在指令跟随、场景泛化和常识知识迁移方面潜力巨大，且与成功的视觉语言模型（VLM）架构最为相似。然而，自回归VLA模型面临两个核心挑战：一是需要一种合适的**动作标记化**方案，将连续动作序列离散化为动作标记；二是其**推理效率**显著低于扩散模型。一个有效的动作标记化方法需满足四个要求：高压缩效率、鲁棒的重建质量、对动作序列二维结构（动作维度和时间维度）的建模能力以及跨任务和具身的灵活性。现有方法未能全面满足这些原则。本文针对自回归VLA模型在动作标记化效率与重建质量之间的权衡以及推理速度慢的痛点，提出了FASTer框架。其核心思路是：通过一个可学习的神经动作标记器（FASTerVQ）高效压缩动作序列，并基于此构建一个采用块状自回归解码的VLA模型（FASTerVLA），从而实现更快的推理速度和更强的任务性能。\n\n## 方法详解\nFASTer框架包含两个互补组件：动作标记器FASTerVQ和自回归VLA策略FASTerVLA。\n\n![方法框架](https://arxiv.org/html/2512.04952v2/x1.png)\n> **图1**：FASTer结合了可学习的动作标记器（FASTerVQ）和自回归VLA模型（FASTerVLA），在八个真实和模拟的具身环境中实现了高效压缩、快速控制和强大性能。\n\n**FASTerVQ（动作标记器）**：其目标是学习将连续动作块编码为离散代码。流程如下：\n1.  **动作分块器**：首先对原始动作序列进行二维非均匀分块。时间维度被均匀分成`m`组，每组长度`h`。动作维度则根据物理特性（如末端执行器位置、姿态、夹爪状态）非均匀地分成`n`组。每组填充至最大组尺寸`d`，最终得到一个`(m·n)`个形状为`(h·d)`的补丁张量。这解决了动作维度数据分布不均的问题，并提高了每个令牌的信息密度。\n2.  **残差VQ动作标记器**：采用基于Transformer的残差向量量化架构。编码器`φ_enc`将动作补丁下采样为潜在嵌入`z`。随后应用具有`N_c`个量化级的残差VQ：`r_1 = z`，对于第`i`级，量化器`Q_i`从码本中找到`r_i`的最邻近向量，残差`r_{i+1} = r_i - Q_i(r_i)`，量化后的潜在嵌入`z_q`为各级量化向量的和。这产生一个离散代码张量`C`，作为下游策略的动作令牌。解码器`φ_dec`将`z_q`重建为动作补丁。\n3.  **训练目标**：损失函数包含三部分：动作信号本身的L1重建损失、动作信号离散余弦变换（DCT）的L1重建损失（用于捕捉全局趋势），以及一项承诺损失（commitment loss）`λ · ||z - sg(z_q)||_2^2`，其中`sg`表示停止梯度操作。\n\n![标记器细节](https://arxiv.org/html/2512.04952v2/x2.png)\n> **图2**：FASTerVQ。(a) 原始动作序列根据其具身配置被分块成紧凑令牌。(b) FASTerVQ采用DCT和L1重建损失，并应用RVQ，将动作编码为`N_c`个代码级别；每个级别可重塑为`C_h × C_a`张量。\n\n**FASTerVLA（自回归VLA策略）**：基于FASTerVQ构建的高效策略模型。\n1.  **架构**：遵循标准VLM结构（视觉塔、投影层、基于Transformer的语言主干），以确保与预训练权重的兼容性。关键创新包括：\n    *   **轻量级动作专家**：添加一个与主干架构相同但参数更少的轻量级专家网络。主干编码多模态上下文一次，而该专家则自回归地从这些特征中解码动作令牌。\n    *   **位置编码与间隔增强**：采用RoPE位置编码。在训练时，对相邻动作令牌之间的相对位置偏移施加一个小的整数抖动，以减轻模型对绝对位置的过拟合。\n2.  **块状自回归解码**：这是提升推理效率的核心。传统自回归逐个预测令牌（公式2）。BAR将令牌序列`C`划分为`J`个连续的块，每块大小为`B`。训练目标变为预测下一个块中的所有令牌（公式3）。推理时，使用块状因果掩码，允许块内令牌相互关注，从而将所需的前向传播次数从`N`减少到约`N/B`。\n3.  **解码顺序**：FASTerVLA按照码本优先、时间维度其次的顺序进行分层解码。即，对于一个码本，先沿时间维度`0,1,...,C_h-1`解码所有令牌，再进入下一个码本。这符合RVQ从粗到细的特性，提高了表示效率并稳定了训练和推理。\n\n![VLA模型细节](https://arxiv.org/html/2512.04952v2/x3.png)\n> **图3**：FASTerVLA。(a) 模型将RGB图像、本体感知状态和语言指令输入基于Transformer的VLM。动作专家自回归地生成离散动作令牌，VQ解码器将其映射为最终的连续动作序列。(b) 代码按码本顺序生成，然后在时间维度上推进，比按时间优先的解码（红色箭头）更稳定。(c) 左：普通因果掩码；右：块状因果掩码，块内令牌可以关注前面及块内的令牌。令牌颜色表示不同模态。\n\n**创新点**：与现有方法相比，FASTer的主要创新在于：1) 提出了一个专门为机器人动作序列设计的、结合非均匀分块和Transformer-RVQ的高效动作标记器；2) 引入了块状自回归解码，显著减少了自回归步数；3) 设计了与VLM主干对齐的轻量级动作专家，以参数高效的方式弥合语言推理与连续控制之间的模态差距。\n\n## 实验与结果\n**实验设置**：在九个基准测试上进行了评估，涵盖五个不同的具身环境（模拟与真实世界），包括变形物体操作、全身控制、指令跟随和长时程操作等任务。基线模型包括扩散模型（如Diffusion Policy, Octo-Base）和自回归VLA模型（如π₀-FAST、OpenVLA、MiniVLA等）。模型通常从在大规模机器人数据上预训练的检查点初始化。\n\n![主要结果表](https://arxiv.org/html/2512.04952v2/x4.png)\n> **图4**：策略在不同具身和环境下的性能。结果报告于分布内设置，涵盖两个真实世界具身和三个模拟设置。\n\n**关键实验结果（策略性能）**：\n*   在**LIBERO**基准测试（四个长时程操作任务）上，FASTer的平均成功率达到**97.9%**，超越了所有基线，包括当前最强的自回归模型π₀-FAST-D（94.2%）和扩散模型π₀5（96.8%）。\n*   在**Simpler-Bridge**基准测试（四个模拟桌面操作任务）上，FASTer的平均成功率为**87.9%**，显著优于π₀-FAST-D的76.5%和其他自回归模型。\n*   图4展示了跨多个具身和环境的综合性能，FASTer在大多数任务上取得了最佳或接近最佳的表现。\n\n![标记器重建质量](https://arxiv.org/html/2512.04952v2/x5.png)\n> **图5**：不同动作标记器在多个误差容忍度σ下的平均VRR（方差减少率）。FASTer在所有尺度上都取得了最佳性能，并表现出清晰的数据缩放行为。\n\n**FASTerVQ标记器分析**：\n*   **重建质量与压缩率**：图6显示，在不同动作时程下，FASTerVQ在压缩率（令牌数少）和重建误差之间取得了最佳平衡，特别是对于长序列。\n*   **数据缩放性**：图5表明，随着训练数据量增加，FASTerVQ的重建质量（以方差减少率衡量）持续提升，显示出良好的数据缩放行为。\n\n![压缩与重建权衡](https://arxiv.org/html/2512.04952v2/x6.png)\n> **图6**：压缩率与重建权衡跨动作时程的比较，其中FASTer实现了最佳平衡，尤其是对于长序列。\n\n**消融实验**：\n*   **块状自回归解码（BAR）的作用**：在Simpler-Bridge任务上，移除BAR的FASTer（w/o BAR）性能为81.0%，而完整FASTer达到87.9%，表明BAR对性能有显著提升。\n*   **解码顺序**：实验表明，按码本优先（Codebook-first）的解码顺序比时间优先（Horizon-first）的顺序产生更稳定的训练损失和更好的最终性能。\n*   **标记器设计**：消融实验证实，非均匀分块、RVQ结构以及结合时域和频域（DCT）的损失函数都对标记器的最终性能有积极贡献。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**FASTerVQ**，一个紧凑、高压缩率的动作标记器，它通过Transformer-RVQ架构和针对动作序列特性的设计，在重建保真度和代码长度之间实现了优越的权衡。\n2.  提出了**FASTerVLA**，引入了**块状自回归解码**和**轻量级动作专家**，使得自回归VLA模型在保持高精度的同时，推理速度大幅提升（论文中提到最高可达3倍加速）。\n3.  建立了一个覆盖多具身、多任务的综合基准，并进行了系统性的实验，证明了FASTer框架在模拟和真实场景中的先进性、高效性和强泛化能力。\n\n**局限性**：论文自身提到，FASTerVLA的性能在一定程度上依赖于预训练的VLM主干。此外，虽然推理速度显著提升，但在需要极低延迟的实时控制场景中，可能仍需进一步优化。\n\n**对后续研究的启示**：\n1.  **动作标记化作为独立模块**：FASTerVQ的成功表明，专门为机器人动作设计的、可学习的标记器是一个富有前景的研究方向，可以独立于具体的VLA策略进行优化和评估。\n2.  **高效自回归解码范式**：块状自回归解码（BAR）为缓解自回归模型固有的序列生成延迟提供了一种有效思路，可被借鉴到其他需要长序列生成的机器人学习任务中。\n3.  **跨模态统一表征**：FASTer框架尝试将动作编码为与语言、视觉对齐的离散令牌，这启示了未来构建更统一的、所有模态共享同一离散令牌空间的多模态具身智能体可能性。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04952v2/x1.png",
        "https://arxiv.org/html/2512.04952v2/x2.png",
        "https://arxiv.org/html/2512.04952v2/x3.png",
        "https://arxiv.org/html/2512.04952v2/x4.png",
        "https://arxiv.org/html/2512.04952v2/x5.png",
        "https://arxiv.org/html/2512.04952v2/x6.png",
        "https://arxiv.org/html/2512.04952v2/figures/backbone.png",
        "https://arxiv.org/html/2512.04952v2/x7.png",
        "https://arxiv.org/html/2512.04952v2/x8.png",
        "https://arxiv.org/html/2512.04952v2/x9.png",
        "https://arxiv.org/html/2512.04952v2/x10.png",
        "https://arxiv.org/html/2512.04952v2/figures/blockablate.png",
        "https://arxiv.org/html/2512.04952v2/x11.png",
        "https://arxiv.org/html/2512.04952v2/x12.png",
        "https://arxiv.org/html/2512.04952v2/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.05335",
      "title": "State-Conditional Adversarial Learning: An Off-Policy Visual Domain Transfer Method for End-to-End Imitation Learning",
      "url": "http://arxiv.org/abs/2512.05335",
      "arxivId": "2512.05335",
      "date": "2025-12-05",
      "authors": "Shengfan Cao Team",
      "category": "Manipulation",
      "summary": "本文针对端到端模仿学习中目标域数据离策略、无专家且稀缺的视觉域转移挑战，提出状态条件对抗学习（SCAL）。该方法基于理论分析，将目标域模仿损失上界为源域损失与状态条件潜在KL散度之和，并通过对抗判别器估计并最小化该散度以对齐条件潜在分布。在BARC–CARLA模拟器的自动驾驶环境实验中，SCAL实现了鲁棒的域转移和强大的样本效率。",
      "detailedSummary": "## 研究背景与动机\n当前，基于视觉的端到端模仿学习已在机器人领域取得显著成果。然而，当策略部署在与训练分布视觉特征不同的新领域时，其性能会变得脆弱。现有提升泛化能力的方法主要分为零样本和少样本适应。零样本方法（如领域随机化）无需目标域数据，但假设合成变异能覆盖目标域特征。少样本方法利用有限的目标域数据，但通常强加了不现实的假设：许多工作假设智能体可以在目标域中进行在线交互以收集数据，或假设能够获取目标域的专家演示。另一项工作移除了对交互和专家演示的需求，但其基于CycleGAN的像素级转换需要大量未标记的目标数据集，不适用于数据稀缺的场景。\n\n在许多现实场景中，这些假设并不成立。目标环境通常是安全关键或操作成本高昂的，使得在线探索代价巨大或不可行。由于人力限制、硬件磨损或缺乏可靠的专家方案，获取专家演示也很困难。即使是被动数据收集也受到硬件可用性、任务时间或法规限制的约束。本文针对一个更现实且更具挑战性的少样本适应场景：系统在训练期间无法直接与目标环境交互，但可获得一个无专家监督的小型数据集（数据可能不遵循策略诱导的分布）；同时，系统可以在训练期间与一个相似环境交互，并有专家可提供监督。\n\n本文的核心思路是：通过理论分析，将目标域的模仿损失上界为源域损失加上源域与目标域观测模型之间的状态条件潜在KL散度，并据此提出状态条件对抗学习框架，仅利用少量离策略目标域数据与状态信息来对齐条件潜在分布。\n\n## 方法详解\n本文提出的状态条件对抗学习框架旨在解决上述挑战。其理论基础是**定理4.1**，该定理表明，对于具有视觉编码器 $E_\\phi$ 的策略 $\\pi_\\theta$，其目标域模仿损失 $\\mathcal{J}_t(\\theta)$ 可以被上界为：\n$\\mathcal{J}_t(\\theta) \\leq \\mathcal{J}_s(\\theta) + \\alpha \\sqrt{\\frac{2\\gamma}{1-\\gamma}(L(\\phi)+\\sigma)}$。\n其中，$\\mathcal{J}_s(\\theta)$ 是源域模仿损失，$L(\\phi)$ 是公式(8)定义的对齐损失（即状态条件潜在分布的期望KL散度），$\\sigma$ 是观测模型间的KL散度常数，$\\alpha$ 是损失函数的一致上界。该定理指出，可以通过最小化源域损失 $\\mathcal{J}_s(\\theta)$ 和对齐损失 $L(\\phi)$ 来优化目标域性能，而无需访问目标域的在线数据。\n\n基于此，**命题4.1**提出了一个用于优化的代理目标：$\\min_\\theta \\mathcal{J}_s(\\theta) + L(\\phi)$。核心挑战在于如何估计并最小化 $L(\\phi)$。**命题5.1**提供了解决方案：通过训练一个判别器 $Q_\\psi$ 来区分来自源域缓冲区 $\\mathcal{B}_s$ 和目标域缓冲区 $\\mathcal{B}_t$ 的（潜在表示 $l$, 状态 $x$）对，可以近似估计 $L(\\phi)$。判别器通过最小化公式(11)的标准二分类对抗损失进行优化。一旦判别器收敛，对齐损失 $L(\\phi)$ 可以通过一个加权对数几率项来近似计算，该权重包含了从 $\\mathcal{B}_s$ 和 $\\mathcal{B}_t$ 估计的状态边际分布比率 $p_{\\mathcal{B}_t}(x)/p_{\\mathcal{B}_s}(x)$。\n\n最终，状态条件对抗学习的总体优化目标为：\n$\\theta^* = \\arg\\min_\\theta \\{ \\mathcal{J}_s(\\theta) + \\lambda \\mathcal{J}_{\\text{adv}}(\\theta) \\}$。\n其中，$\\mathcal{J}_s(\\theta)$ 是源域在线模仿损失（可通过DAgger等标准模仿学习流程利用 $\\mathcal{B}_s$ 计算），$\\mathcal{J}_{\\text{adv}}(\\theta)$ 是**域混淆损失**，具体形式为：\n$\\frac{1}{\\lVert\\mathcal{B}_s\\rVert}\\sum_{(y,x)\\sim\\mathcal{B}_s}\\log\\frac{Q_{\\psi}(E_\\phi(y),x)}{1-Q_{\\psi}(E_\\phi(y),x)}\\frac{\\widehat{p_{\\mathcal{B}_t}(x)}}{\\widehat{p_{\\mathcal{B}_s}(x)}}$。\n$\\lambda$ 是权衡系数。训练过程遵循对抗学习惯例，判别器 $Q_\\psi$ 和策略（编码器 $E_\\phi$ 与控制头 $D_w$）迭代更新。\n\n![方法框架](https://arxiv.org/html/2512.05335v2/graphs/PCA_visualization_before_tuning.png)\n> **图1**：未使用SCAL时，同一轨迹在源域（蓝色）和目标域（橙色）的潜在表示PCA可视化。两者分布明显分离。\n\n![方法框架](https://arxiv.org/html/2512.05335v2/graphs/PCA_visualization_afer_tuning2.png)\n> **图2**：使用SCAL后，同一轨迹在源域（蓝色）和目标域（橙色）的潜在表示PCA可视化。两者分布高度对齐，表明方法有效。\n\n**算法1** 详细描述了SCAL的完整流程：首先用 $\\mathcal{B}_s$ 和 $\\mathcal{B}_t$ 的状态边际拟合状态分布估计器；然后循环执行：1) 更新判别器 $K_{\\text{disc}}$ 步；2) 遵循DAgger流程填充 $\\mathcal{B}_s$；3) 基于 $\\mathcal{B}_s$ 计算源域损失 $\\mathcal{J}_s$；4) 计算域混淆损失 $\\mathcal{J}_{\\text{adv}}$；5) 用总损失 $\\mathcal{J}_{\\text{total}} = \\mathcal{J}_s + \\lambda \\mathcal{J}_{\\text{adv}}$ 更新策略参数 $\\theta$。\n\n与现有工作相比，SCAL的创新点在于：1) 在理论上首次推导出基于状态条件潜在KL散度的目标域性能上界；2) 在方法上首次处理了**专家监督缺失**、**目标域数据严格离策略**且**稀缺**的视觉领域迁移设定；3) 通过状态条件对抗学习实现了高效的样本利用，无需目标域在线交互或大量无标签数据。\n\n## 实验与结果\n实验基于伯克利自动驾驶赛车仿真环境（BARC–CARLA）构建，源域和目标域是具有相同赛道形状但视觉外观截然不同的Carla环境。\n\n![实验设置](https://arxiv.org/html/2512.05335v2/graphs/domain_demo1.png)\n> **图3**：实验中使用的一个示例源域（左）与目标域（右）的视觉对比。\n\n![实验设置](https://arxiv.org/html/2512.05335v2/graphs/domain_demo2.png)\n> **图4**：另一个视觉特征不同的目标域示例。\n\n**1. 离策略评估研究：** 为验证理论分析的有效性，作者准备了20个在源域表现完美（$\\mathcal{J}_s(\\theta_i) \\approx 0$）的策略。对于每个策略，根据命题5.1估计其状态条件KL散度，并在目标域评估其在线模仿损失和轨迹长度。\n![相关性分析](https://arxiv.org/html/2512.05335v2/graphs/OPE_no_shading.png)\n> **图5**：状态条件KL散度估计值与目标域在线性能指标（模仿损失和轨迹长度）呈强正相关。这验证了理论代理目标 $\\mathcal{J}_s(\\theta) + L(\\phi)$ 与真实目标 $\\mathcal{J}_t(\\theta)$ 之间的关联性。\n\n**2. 分布偏移研究：** 为验证SCAL在不同目标数据分布下的样本效率，预定义了3种不同的状态分布来收集 $\\mathcal{B}_t$，并改变 $\\mathcal{B}_t$ 的大小（从128到2048）。对比基线是在目标域拥有完全监督和在线数据访问权限的理想化DAgger。\n![样本效率对比](https://arxiv.org/html/2512.05335v2/graphs/distribution_data_size_exp_columns.png)\n> **图6**：SCAL与理想基线（DAgger with perfect info）在不同 $\\mathcal{B}_t$ 分布和大小下的性能对比（y轴：目标域最大轨迹长度）。SCAL（黄、蓝、绿线）仅使用离策略、无专家监督的缓冲池，在多数情况下达到了与拥有优越信息条件的基线相当或更优的性能，尤其在低数据区域（如256个样本）仍保持强劲和稳定。\n\n![数据分布可视化](https://arxiv.org/html/2512.05335v2/graphs/sample_distribution_visualize.png)\n> **图7**：实验中使用的三种不同目标域数据 $\\mathcal{B}_t$ 的状态分布可视化。\n\n**3. 低速到高速迁移实验：** 作为一个展示实用意义的附加实验，作者将低速场景训练的模型成功迁移到高速场景。\n![速度迁移](https://arxiv.org/html/2512.05335v2/graphs/traj_speed_heatMap.png)\n> **图8**：低速到高速迁移学习的轨迹速度热图，表明SCAL能够处理动态特性变化的领域迁移。\n\n关键实验结果总结：1) 理论推导的相关性得到实证支持（图5）。2) 在三种不同的离策略目标数据分布下，SCAL仅用少量目标样本（如256个）就达到了与在目标域拥有完全专家监督和在线访问权限的DAgger基线相当或更好的性能，证明了其卓越的样本效率和鲁棒性（图6）。\n\n## 总结与启发\n本文的核心贡献包括：1) **理论贡献**：首次对领域偏移下的视觉模仿学习进行了形式化分析，推导出基于状态条件潜在KL散度的目标域性能上界。2) **方法贡献**：提出了状态条件对抗学习，这是一个新颖的、处理离策略、无专家目标域数据的领域迁移框架。3) **实证贡献**：在具有挑战性的视觉驾驶环境中验证了理论见解和方法有效性，证明了其优于现有方法的样本效率。\n\n论文自身提到的局限性主要隐含在理论假设中：1) **领域相似性**：定理中的常数 $\\sigma$ 要求源域和目标域的观测模型不能差异过大，否则上界会过于宽松。2) **状态可访问性**：方法需要访问系统状态 $x$ 作为条件信息，这在实际中可能依赖于额外的传感器或状态估计器。\n\n对后续研究的启示：1) **理论指导实践**：本文展示了如何从理论分析（性能上界）直接推导出可优化的算法框架（代理损失），为领域迁移提供了新的理论 grounded 的设计思路。2) **放宽假设**：SCAL成功处理了“离策略、无专家、数据少”这一更现实的设定，鼓励后续研究继续探索在更弱假设下的迁移学习。3) **条件分布对齐**：强调了在对齐过程中考虑任务相关条件（如系统状态）的重要性，这比无条件对齐更具针对性，可能适用于其他需要跨域一致表示的强化学习或机器人学习任务。",
      "imageUrls": [
        "https://arxiv.org/html/2512.05335v2/graphs/PCA_visualization_before_tuning.png",
        "https://arxiv.org/html/2512.05335v2/graphs/PCA_visualization_afer_tuning2.png",
        "https://arxiv.org/html/2512.05335v2/graphs/domain_demo1.png",
        "https://arxiv.org/html/2512.05335v2/graphs/domain_demo2.png",
        "https://arxiv.org/html/2512.05335v2/graphs/OPE_no_shading.png",
        "https://arxiv.org/html/2512.05335v2/graphs/distribution_data_size_exp_columns.png",
        "https://arxiv.org/html/2512.05335v2/graphs/sample_distribution_visualize.png",
        "https://arxiv.org/html/2512.05335v2/graphs/traj_speed_heatMap.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04960",
      "title": "Hybrid-Diffusion Models: Combining Open-loop Routines with Visuomotor Diffusion Policies",
      "url": "http://arxiv.org/abs/2512.04960",
      "arxivId": "2512.04960",
      "date": "2025-12-04",
      "authors": "Danica Kragic Team",
      "category": "Manipulation",
      "summary": "本文针对模仿学习获得的视觉运动策略在精度和速度上不及传统控制方法的问题，提出混合扩散模型。核心方法是将开环例程与视觉运动扩散策略相结合，并开发了远程操作增强原语（TAPs），允许演示者无缝执行锁定特定轴、移动至路径点等预定义例程，且模型在推理时能自主触发TAPs。该方法在真实世界的移液、液体转移和容器拧开等任务中得到了验证。",
      "detailedSummary": "## 研究背景与动机\n当前，通过模仿学习获得的视觉运动策略在复杂操作任务中表现出良好性能，但通常难以达到基于传统控制方法的精度和速度。远程操作系统的根本局限在于操作员与机器人之间的形态不匹配。例如，机器人可能具备人类无法轻易完成的关节运动能力（如旋转躯干180度），或者人类难以持续绕单一轴旋转（如拧瓶盖），而机器人可以简单地驱动末端关节实现。因此，机器人形态优势难以通过1:1映射的远程操作被充分利用。\n\n本文针对上述痛点，提出了一种新视角：允许操作员在演示过程中无缝集成预定义例程，并让学习到的策略在推断时也能自主触发这些例程，从而结合开环精确控制与闭环视觉运动策略的优势。核心思路是：引入远程操作增强原语（Teleoperation Augmentation Primitives， TAPs），使操作员能在演示中触发预定义例程；并扩展视觉运动扩散策略，使其学习在任务执行中自主触发TAPs，形成混合扩散模型。\n\n## 方法详解\n整体框架分为两部分：一是支持TAPs的远程操作数据收集系统，二是能够触发TAPs的混合扩散模型推理框架。\n\n![方法框架](https://arxiv.org/html/2512.04960v1/imgs/fig_1_HD.png)\n\n> **图1**：混合扩散模型概述。在远程操作期间，专家可以通过语音或（AR）控制器输入触发远程操作增强原语（TAP）。混合扩散模型在执行过程中也学习触发此类TAP例程，在任务中利用例程。\n\n**核心模块1：远程操作增强原语（TAPs）**\nTAPs是操作员在认为有益于提高演示质量时可以触发的原语，旨在解决模仿学习中远程操作演示收集时的特定缺陷。具体分为三类：\n1.  **轴锁定**：允许操作员锁定机器人工具坐标系或基坐标系中的一个或多个轴（X, Y, Z, 横滚，俯仰，偏航）。这在需要保持特定末端执行器方向或沿主轴直线运动时非常有用。\n2.  **栖息路径点**：允许操作员为（长时域）任务的特定阶段预定义一组路径点，并可在任何时候触发预设路径点，命令机器人移动到该点。这在仅使用末端执行器摄像头、任务相关物体可能离开视野时特别有用，能确保场景回到视野并提供下一任务步骤的统一起始位置。\n3.  **（开环）例程**：允许操作员在任务中选择的点触发预定义的（参数化）开环原语，以利用机器人的形态优势。例如，对于拧瓶盖任务，可以设计一个包含“夹紧→逆时针旋转→张开→顺时针旋转→夹紧→逆时针旋转→直线上升”的例程。旋转次数、夹爪力等可通过参数设置。本文展示的例程是开环的，但框架不限于此，也支持触发具有闭环反馈或由强化学习驱动的子策略例程。\n\n![触发方式](https://arxiv.org/html/2512.04960v1/imgs/taps_modalities.png)\n\n> **图2**：触发TAP的不同方式：通过语音命令(a)、AR按钮界面(b)，或通过直接映射并以触觉模式作为确认(c-专家用户)。\n\n**触发TAPs的交互方式**：为适应不同操作员偏好，TAPs可通过三种方式触发：\n*   **语音**：按下控制器上的预定义组合键启动语音识别，说出命令（如“锁定X轴”），使用Whisper-Tiny模型进行语音转文本，并通过计算Levenshtein距离匹配预定义TAP列表。\n*   **增强现实（AR）**：通过AR头显查看控制器上的AR菜单，使用控制摇杆方向激活标有TAP选项的AR按钮。\n*   **触觉反馈**：供高级用户使用，无需佩戴头显，通过控制器不同的振动脉冲模式来感知激活的菜单和可用TAP，通常通过预选任务所需TAP并单键触发。\n\n![TAP类型](https://arxiv.org/html/2512.04960v1/imgs/tasks.png)\n\n> **图3**：不同类型的TAPs及示例任务。a) 小瓶抽吸，其中旋转轴锁定对操作员有用；b) 开容器液体转移，部署栖息路径点以使相应容器进入视野；c) 容器拧开，在正确位置触发开环拧开例程。\n\n**核心模块2：混合扩散模型**\n模型架构如图1底部所示。输入观察值，包括RGB图像（$O_t$）和机器人6D末端执行器位姿（$x_t$），送入视觉运动扩散策略块。策略预测未来$n$个时间步的动作$a_t$到$a_{t+n}$以及TAPs。在推理循环中（算法1），只要没有推理正在运行（约每3个时间步），就持续预测新动作和TAPs，并将当前时间步的动作和TAP发送给处理TAPs的机器人控制器（算法2）。\n\n在机器人控制器中（算法2）：\n*   如果没有TAP激活且接收到的TAP未被触发，则正常执行策略动作。\n*   如果接收到TAP，则以开环方式执行该TAP直至完成。在此期间，策略（一直在默默预测动作）的动作被忽略。TAP一旦完成，策略立即无缝恢复控制，无任何延迟或中断。\n*   策略库中包含所有预定义的、策略可触发的TAP；策略指示在给定时间步应执行哪个TAP。如果策略在同一时间步触发两个TAP（非期望行为），则执行TAP库中优先级较高的那个。\n\n**创新点**：与现有混合策略（如HYDRA结合稀疏路径点与密集控制）相比，本文的创新在于提供了一个完整的框架，允许操作员在演示过程中直接将混合例程集成到数据中，同时策略框架学习在任务需要时触发这些例程，实现了从演示到策略执行的端到端“混合”能力。\n\n## 实验与结果\n**实验设置**：在三个真实世界任务上验证方法，任务设置如图3所示：\n1.  **小瓶抽吸**：机器人配备注射器，从安装在可任意旋转球体上的小瓶中抽取液体。操作员使用**轴锁定**TAP（锁定所有旋转轴）以保持针头对齐。\n2.  **开容器液体转移**：机器人用注射器从右侧化学容器抽取液体并注入左侧容器。使用**栖息路径点**TAP，使操作员能始终回到初始路径点，抽取液体后激活另一个路径点使目标容器清晰可见。\n3.  **容器拧开**：另一只机器人臂（非策略控制）握住带螺纹盖的容器。任务是将机器人移动到容器上方，抓住盖子并将其拧下。使用**开环例程**TAP（拧开例程）。\n\n**基准与评估**：对比基线为无法触发TAPs的扩散策略。基线必须将TAP例程产生的轨迹作为整体策略的一部分来学习。每个任务随机化7个初始条件（基线和混合扩散使用相同种子），每个起始位置执行3次，每个策略每个任务共21次 rollout。\n\n![实验结果](https://arxiv.org/html/2512.04960v1/imgs/result_p.png)\n\n> **图4**：三个任务的实验结果，每个任务7个新颖起始位置，各重复3次。比较混合扩散（HD）与基线扩散（D）方法。\n\n**关键实验结果**：\n*   **小瓶抽吸**：混合扩散（HD）与基线扩散（D）性能非常相似，成功率分别为**62%** 和**57%**。轴锁定TAP主要有助于操作员提高演示质量，但如果训练数据质量高，策略可以学会保持方向。\n*   **开容器液体转移**：HD以**71%** 的成功率略微优于基线的**62%**。栖息路径点对任务成功率至关重要，但策略可以在不触发例程的情况下学会返回有利位置。TAP使过程更结构化，在更长时域任务中可能有潜力，但如果操作员没有TAP也会移动到路径点，则收益有限。\n*   **容器拧开**：HD表现出显著性能优势，成功率为**67%**，而基线仅为**38%**。分析失败案例发现，基线难以执行多次旋转，并试图在盖子仍固定在瓶子上时过早提起。这是因为盖子完全关闭与旋转一次后的观察差异极小，导致策略在这些情况下存在多模态分布（是旋转还是提起），从而成功率下降。而混合方法在正确位置触发例程时，能通过开环确保旋转足够次数以完全拧开瓶子。\n\n**消融实验**：实验本身通过对比有无TAP触发能力的策略，验证了不同TAP组件（轴锁定、路径点、开环例程）的贡献。结果表明，对于依赖重复性、形态优势动作的任务（如拧开），**开环例程**的贡献最大，带来了近30个百分点的性能提升。而轴锁定和栖息路径点主要贡献于**改善演示质量**，对最终策略性能的提升相对有限。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了远程操作增强原语（TAPs）框架**：定义了三种类型（轴锁定、栖息路径点、开环例程）以及多种触发方式（语音、AR、触觉），旨在改善演示质量并充分利用机器人形态。\n2.  **提出了混合扩散模型**：扩展了视觉运动扩散策略，使其能够学习并自主触发TAPs，将开环例程与闭环视觉运动策略有机结合。\n3.  **在真实机器人任务上进行了验证**：实验表明，该方法能有效提升某些任务（尤其是需要利用形态优势的重复性任务）的策略性能，其中容器拧开任务的成功率从38%提升至67%。\n\n**局限性**：论文提到，由于TAPs是开环的，且策略理论上可以在任何时间触发TAP，因此存在触发TAP导致任务失败并超出训练数据支持范围的风险。\n\n**对后续研究的启示**：\n1.  **扩展TAP类型**：将TAPs扩展到包含闭环反馈控制器或通过强化学习等方式学习的子策略，以增强鲁棒性和适应性。\n2.  **自动TAP发现**：研究如何从失败演示或操作员行为模式中自动识别有益的原语，减少手动设计工作量。\n3.  **层次化组合**：开发能按顺序组合多个TAPs的层次化混合扩散模型，以扩展到更复杂的长期任务。\n4.  **跨 embodiment 和任务迁移**：研究TAPs在不同机器人形态和任务间的可迁移性，以构建更通用的操作策略。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04960v1/imgs/fig_1_HD.png",
        "https://arxiv.org/html/2512.04960v1/imgs/taps_modalities.png",
        "https://arxiv.org/html/2512.04960v1/imgs/tasks.png",
        "https://arxiv.org/html/2512.04960v1/imgs/result_p.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04987",
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "url": "http://arxiv.org/abs/2512.04987",
      "arxivId": "2512.04987",
      "date": "2025-12-04",
      "authors": "Xipeng Qiu Team",
      "category": "Manipulation",
      "summary": "本文旨在解决大语言模型向自主代理转变时，缺乏可扩展基础设施构建高质量交互环境的核心问题。提出统一生态系统Nex，通过NexAU（配置构建复杂代理层次）、NexA4A（自然语言生成多样代理层次）和NexGAP（集成真实环境合成接地轨迹）三个维度提升环境复杂性、多样性和保真度。在SWE-bench和τ²等基准测试中，Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance frontier proprietary models on complex agentic tasks。",
      "detailedSummary": "## 研究背景与动机\n大型语言模型正从被动响应者向自主智能体演变，这要求学习范式从静态模仿转向激励驱动的决策。然而，这一转变受到关键瓶颈的阻碍：缺乏能够为有效策略学习构建高质量交互信号的可扩展基础设施。当前主流方法依赖于有限的人工设计环境或僵化的框架，这导致了两个关键局限性：一是**环境多样性稀缺**，静态文本训练的模型缺乏在需要长期推理的环境中学习的机会，容易陷入概率陷阱和短视决策；二是**缺乏现实基础**，在纯合成或静态数据上训练的智能体难以应对现实世界执行的复杂性，表现为“思考”与“行动”的脱节，在工具使用中出现幻觉，且缺乏鲁棒的错误恢复能力。本文针对“为智能体学习构建大规模、高质量交互环境”这一具体痛点，提出了一个基于**智能体规模化**的新视角，通过一个统一的生态系统，将环境构建从人工工程转变为自动化合成。本文的核心思路是：通过NexAU、NexA4A和NexGAP三个组件构建一个统一的生态系统，自动化生成多样、复杂且逼真的交互环境，并在此基础上训练出在复杂智能体任务中表现优异的Nex-N1模型。\n\n## 方法详解\n本文方法的核心是一个用于智能体规模化的统一生态系统，由三个核心组件构成：用于构建复杂智能体层次结构的运行时NexAU、用于自动生成多样化智能体架构的NexA4A，以及用于生成端到端高质量轨迹的NexGAP。整体流程是：NexA4A根据自然语言描述生成多样化的智能体框架配置；NexAU作为执行引擎，根据这些配置模拟智能体在环境中的交互；NexGAP则整合真实世界的工具并合成查询，驱动整个系统生成大规模、高质量的智能体交互轨迹数据，用于训练最终的Nex-N1模型。\n\n![NexA4A工作流](https://arxiv.org/html/2512.04987v1/x1.png)\n> **图2**：NexA4A智能体框架工作流：从框架描述到高质量轨迹生成的完整架构图。展示了通过MetaAgent解析高层描述，选择工作流模式，并协调FrameworkBuilder和AgentBuilder等组件生成完整智能体配置的过程。\n\n**NexAU：可扩展智能体框架的模块化运行时**\nNexAU是一个轻量级、高吞吐量的运行时，它将智能体定义与智能体执行解耦。其核心是一个标准化的智能体执行循环，并采用递归的分层架构。每个智能体实例运行一个局部的ReAct循环（感知、推理、行动）。其创新之处在于统一了工具和子智能体：对于父智能体而言，子智能体只是一个定义了输入模式的工具。当父智能体调用子智能体时，运行时会实例化一个具有独立系统提示、状态和工具集的子执行上下文，该子上下文运行自己的ReAct循环直至满足终止条件。这种递归结构确保了推理状态的隔离，避免了上下文窗口溢出。智能体通过声明式的YAML模式进行定义，这实现了逻辑拓扑与具体实现的解耦，使得无需生成可执行代码即可程序化合成新的智能体架构。\n\n![NexAU定义示例](https://arxiv.org/html/2512.04987v1/x2.png)\n> **图3**：简化的NexAU定义示例。子智能体与标准工具一同组合，支持任意深度的组合。通过操作此类配置，可以程序化生成从单智能体工具循环到多智能体组织的各种环境。\n\n**NexA4A：智能体与框架的自动生成**\n为了系统性地扩展智能体行为和工作流结构的分布，NexA4A是一个从自然语言规范自动合成智能体及整个智能体框架的系统。它通过一个中心MetaAgent解释目标框架的高级描述，选择合适的工作流模式，将其分解为分层任务，并组装多智能体系统（通常为1到3层深）。通过协调FrameworkBuilder、AgentBuilder和支持子智能体，NexA4A能够设计工作流、分配角色、分配元技能、将技能分解为可执行动作、配置智能体并合成任何所需的定制工具，从而实现大规模创建多样化的智能体和智能体框架。\n\n**NexGAP：端到端智能体数据生成**\n在建立了可扩展环境的基础上，NexGAP用于生成端到端的智能体轨迹。其流程始于利用真实的MCP工具作为能力种子，为NexA4A生成框架构建查询。NexA4A合成智能体框架后，NexGAP应用“信息融合查询合成”来生成针对该框架、具有不同难度的任务。NexAU随后在环境中执行生成的智能体，产生原始交互轨迹。为了保持跨异构设计的互操作性，NexGAP将轨迹规范化为多种工具调用格式。该流程总共构建了超过200个智能体框架和环境，其智能体和子智能体图节点数从1到34不等，涵盖了标准的ReAct智能体、多层多智能体系统和固定工作流管道。\n\n## 实验与结果\n本文在多个基准测试和现实场景中对Nex-N1模型进行了全面评估。使用的基准包括：评估通用智能体能力的τ²-bench和GAIA 2；评估智能体编码能力的SWE-bench、Terminal Bench 2和Baxbench；以及评估工具使用能力的伯克利函数调用排行榜（BFCL）V4。对比的基线模型包括Claude Sonnet-4.5、GPT-5、Gemini-2.5-pro等前沿专有模型，以及GLM-4.6、Minimax-M2、Kimi-K2-thinking、DeepSeek-V3.1、Qwen3系列等开源模型。实验平台主要基于内部实现的OpenHands脚手架以及Claude Code、Terminus-2等框架。\n\n关键实验结果如下：在通用智能体能力上，基于DeepSeek-V3.1的Nex-N1在τ²-bench上达到80.2分，在GAIA 2上达到29.5分，表现强劲。在智能体编码方面，在SWE-bench上达到70.6分，在Baxbench上达到59.7分，展现出竞争力。在工具使用（BFCL）上达到65.3分。值得注意的是，Nex-N1系列模型在不同规模的基座模型（如Qwen3-32B、InternLM3-8B）上微调后，性能均获得显著提升，远超原始基座模型。\n\n![性能对比](https://arxiv.org/html/2512.04987v1/Nex_logo.png)\n> **图1**：Nex-N1与最先进模型在智能体和编码基准上的性能对比图。展示了Nex-N1相对于其他开源和专有模型的优越性能。\n\n在现实世界智能体编码评估中，针对项目开发（43个样本），Nex-N1在超过一半的场景中优于或与基线模型持平，例如以64.5%的比例优于Claude Sonnet-4.5，以92.9%的比例优于Minimax-M2。在网页开发评估（45个样本）中，Nex-N1在视觉质量、色彩丰富度和页面完整性方面优于除Claude Sonnet-4.5（44.5%）之外的所有模型。\n\n![智能体编码人工评估](https://arxiv.org/html/2512.04987v1/x3.png)\n> **图4**：智能体编码任务上的人工评估结果。展示了Nex-N1在项目开发场景中相对于多个基线模型的胜率。\n\n![网页创建人工评估](https://arxiv.org/html/2512.04987v1/x4.png)\n> **图5**：网页创建任务上的人工评估结果。展示了Nex-N1在五个不同领域的网页开发质量评估中相对于基线模型的胜率。\n\n此外，基于NexAU框架构建的深度研究智能体在公开的Deep Research Benchmark上取得了47.0%的得分，并具备生成可视化报告和学术海报的能力。\n\n![深度研究基准结果](https://arxiv.org/html/2512.04987v1/Figures/results/dr_benchmark.png)\n> **图6**：我们的深度研究智能体在深度研究基准上的结果。展示了在多个评估维度上的详细得分。\n\n![深度研究演示](https://arxiv.org/html/2512.04987v1/x5.png)\n> **图7**：我们的深度研究智能体演示。展示了其生成集成文本和图形的可视化研究报告的能力。\n\n![海报生成演示](https://arxiv.org/html/2512.04987v1/x6.png)\n> **图8**：我们的Paper2Poster智能体演示（改编自(Xi et al., 2025)）。展示了将论文自动转换为专业学术海报的能力。\n\n关于模型在不同智能体框架间的鲁棒性，在SWE-bench verified子集上的测试表明，Nex-N1在OpenHands、Claude Code和Terminus-2等多种框架下均能保持稳定的性能，而某些基线模型（如MiniMax-M2）在特定框架（如OpenHands）上则完全失败。\n\n## 总结与启发\n本文的核心贡献在于：第一，提出了一套统一的生态系统（NexAU, NexA4A, NexGAP），将智能体环境构建从手动工程转变为自动化合成，通过将环境视为生成式语言规范，打破了对人设计环境的依赖，实现了交互拓扑的无限扩展，为解决智能体训练数据稀缺的关键瓶颈提供了基础设施。第二，基于该生态系统生成的高质量、大规模数据训练出的Nex-N1模型，在通用智能体能力、智能体编码、工具使用等多个基准测试上显著优于同规模开源模型，并与前沿专有模型表现相当，尤其在现实项目开发和网页创建任务中展现出强大的实用性。第三，Nex-N1模型展现出优秀的跨框架鲁棒性和泛化能力，能够在不同的智能体执行环境中稳定工作。论文自身提到的局限性包括：在视觉反馈方面偶尔不可靠，代码修复有时会失败，以及为缓解这些问题而引入的工程优化（如将视觉反馈转为二元判断、设置最大修复迭代限制）可能带来的限制。本文对后续研究的启示在于：为智能体学习提供高质量、规模化数据的基础设施至关重要；通过自动化合成而非人工设计来扩展环境多样性是可行的路径；将真实世界工具（MCP）与生成式框架结合，能够有效弥合模拟与现实的差距，提升智能体的实际执行能力。作者开源了模型、代码和部分数据，有望加速社区在智能体规模化方向的研究。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04987v1/Nex_logo.png",
        "https://arxiv.org/html/2512.04987v1/x1.png",
        "https://arxiv.org/html/2512.04987v1/x2.png",
        "https://arxiv.org/html/2512.04987v1/Figures/nexau.png",
        "https://arxiv.org/html/2512.04987v1/x3.png",
        "https://arxiv.org/html/2512.04987v1/x4.png",
        "https://arxiv.org/html/2512.04987v1/Figures/results/dr_benchmark.png",
        "https://arxiv.org/html/2512.04987v1/x5.png",
        "https://arxiv.org/html/2512.04987v1/x6.png",
        "https://arxiv.org/html/2512.04987v1/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04535",
      "title": "GTM: Simulating the World of Tools for AI Agents",
      "url": "http://arxiv.org/abs/2512.04535",
      "arxivId": "2512.04535",
      "date": "2025-12-04",
      "authors": "Jiyan He Team",
      "category": "Manipulation",
      "summary": "本文提出通用工具模型GTM，以解决AI代理直接与多样工具交互训练时成本高、速度慢、开发维护负担重的核心问题。关键技术包括：1）构建15亿参数的GTM作为通用工具模拟器，仅需提示级配置即可模拟工具执行；2）提出上下文感知响应生成（CARG）管道，合成覆盖300个领域、超2万种工具的综合性训练数据。实验表明，GTM在强化学习训练中，模拟速度显著快于真实工具，同时保持可比的输出质量，并展现出优异的泛化与领域适应能力。",
      "detailedSummary": "## 研究背景与动机\n当前，增强大型语言模型（LLM）代理的现实世界能力主要依赖于外部工具的集成。主流方法包括监督微调（SFT）和强化学习（RL）。SFT方法在策划的工具使用示例数据集上进行训练，但泛化能力有限，难以适应新情况。RL方法通过与环境交互学习，在发现有效工具使用策略方面表现出色，但面临三个关键局限性：1）外部API调用延迟高，严重拖慢训练过程；2）工具调用成本昂贵，大规模RL训练经济上不可行；3）集成外部工具带来巨大的工程开销（如接口开发、格式处理、调试和维护）。本文针对这些在RL训练中集成真实工具所导致的效率、成本和工程瓶颈，提出了一个全新的视角：将工具组件从训练过程中解耦，通过一个通用的工具模拟器来生成逼真的工具响应，而非调用真实API。本文的核心思路是训练一个通用工具模型（GTM），仅通过提示级配置即可模拟成千上万种工具的行为，为代理训练提供一个快速、低成本且无需开发开销的解决方案。\n\n## 方法详解\nGTM的整体目标是以可承受的计算成本模拟多样化的工具。方法流程分为三个关键步骤：1）工具生成，构建一个覆盖广泛领域的大规模工具规范集合；2）上下文感知响应生成（CARG）管道，生成高质量、上下文连贯的训练数据；3）模型训练，在Qwen2.5-1.5B基础模型上进行微调，得到最终的GTM模型。\n\n![方法框架](https://arxiv.org/html/2512.04535v2/teaser_fig.jpg)\n> **图1**：使用真实工具与使用GTM模拟工具的流程对比。左侧（a）显示真实工具环境需要集成多种工具。右侧（b）显示仅需提示级修改，GTM即可模拟各种工具，为代理工具学习提供了更通用的选择。\n\n**核心模块1：工具生成与统一模板**\n首先，论文构建了一个包含超过20,000个独特工具、覆盖300多个领域（如物理、医学、机器人、金融）的工具库。为确保一致性，设计了一个统一的工具格式模板（如图2所示），包含API名称、描述、所属领域、参数定义（类型、描述、必填项）和响应格式。工具生成过程通过迭代扩展构建领域/子领域分类体系，然后为每个领域-子领域对生成工具规范，并进行质量验证和去重。此外，还从ToolEyes和APIGen等现有数据集中提取并标准化工具规范，以最大化覆盖范围。\n\n![工具模板](https://arxiv.org/html/2512.04535v2/x1.png)\n> **图2**：统一工具模板结构。定义了API的标准化描述格式，包括名称、描述、字段、参数信息和响应格式。\n\n**核心模块2：上下文感知响应生成（CARG）管道**\nCARG是生成高质量训练数据的核心，采用“生成-验证”两阶段架构，针对三种场景产生数据：单轮输入-输出生成、多轮上下文生成和错误生成。\n1.  **单轮生成**：对于每个工具，利用LLM根据工具规范和领域上下文，生成语义合理、参数值有意义的输入以及逻辑对应的输出。随后进行三级验证：格式验证（检查参数类型和必填字段）、逻辑验证（检测参数间矛盾）、语义验证（确保输入输出连贯性）。只有通过全部验证的数据才会被保留。\n2.  **多轮上下文生成**：目标是生成具有渐进上下文构建和跨轮次信息依赖的对话序列。首先，通过语义嵌入和贪婪搜索将语义相关的API分组。然后，以这些API组为基础，逐步生成多轮对话，确保最终的工具调用参数自然地源自完整的对话历史。验证阶段除了单轮的检查外，还增加了对对话历史连贯性的专门验证。\n3.  **错误生成**：为了增强模型处理错误调用的鲁棒性，CARG模拟了四种常见错误类型（类型错误、缺失必填参数、多余参数、无效值），并生成相应的、有帮助的错误信息。每个错误-信息对同样要经过严格的格式、错误存在性和信息质量验证。\n\n**创新点**\n与现有方法相比，GTM的主要创新点在于：1）**通用性**：提出一个统一的模型来模拟海量工具，而非像ZeroSearch那样仅针对特定工具（如网络搜索）；2）**上下文一致性**：通过CARG管道，GTM学习生成不仅在格式上正确，而且在逻辑和跨轮次上下文上连贯的工具响应，这是高质量模拟的关键；3）**成本效益**：将昂贵的、按次计费的API调用转化为可预测的模型推理成本，并利用现有高效推理框架实现高吞吐、低延迟。\n\n## 实验与结果\n实验分为两部分：评估GTM输出质量，以及验证GTM在真实RL训练过程中的效用。\n\n**评估基准与基线**：在输出质量评估中，使用了划分好的训练/验证API数据集，在单轮对话、多轮对话和含错误输入三个场景下进行测试。对比的基线模型包括Qwen2.5系列（0.5B, 1.5B, 3B, 7B, 14B）、Llama3.1系列（1B, 3B）和InternLM2.5系列（1.8B, 7B, 20B）的开源指令微调模型。使用Qwen2.5-72B作为评判员。在RL效用验证中，选择了三个场景：搜索（使用Jina API，与训练数据中的工具相似）、检索（使用Search-R1配置，代表训练数据中未见过的新工具类型）和内核优化（高度专业化的领域特定工具，需对GTM进行微调）。\n\n**关键实验结果**：\n1.  **输出质量评估（表1）**：GTM-1.5B在综合评价指标“Avg”（三个场景综合通过率平均值）上达到了89.4%，显著超越了参数规模更大的基线模型，如Qwen2.5-14B-Instruct（85.8%）。特别是在多轮场景的“All”指标（通过所有准则的比例）上，GTM-1.5B达到86.7%，远超其他同规模甚至更大规模的模型，证明了其出色的上下文一致性。在错误检测场景，GTM-1.5B的“Det”（错误识别率）和“Help”（错误信息帮助性）也分别达到87.5%和86.1%，表现优异。\n\n![领域分布](https://arxiv.org/html/2512.04535v2/api_distribution_tsne.png)\n> **图7**：工具API分布的t-SNE可视化。展示了GTM训练数据覆盖的广泛领域和工具多样性。\n\n2.  **RL训练效用验证**：\n    *   **搜索场景（相似工具）**：在基于HotpotQA数据集的搜索代理RL训练中，使用GTM模拟搜索工具的代理，其最终性能与使用真实Jina API训练的代理相当（成功率均在74%左右），但训练速度显著加快。\n\n![搜索得分对比](https://arxiv.org/html/2512.04535v2/search_scores.png)\n> **图3**：搜索场景下，使用GTM模拟与使用真实Jina API进行RL训练的代理性能对比。两者最终成功率相近，表明GTM模拟质量可比拟真实工具。\n\n![搜索步骤时间](https://arxiv.org/html/2512.04535v2/search_step_times.png)\n> **图4**：搜索场景下每一步训练所需时间对比。使用GTM模拟（绿色）相比真实API调用（红色）每一步耗时显著减少，极大提升了训练效率。\n\n    *   **检索场景（未见工具）**：在检索代理训练中，GTM成功模拟了训练数据中未出现过的检索工具。使用GTM模拟训练的代理，其性能与使用真实检索工具训练的代理性能相当（评估指标接近），再次证明了GTM强大的泛化能力。\n\n![检索得分对比](https://arxiv.org/html/2512.04535v2/qwen2p5_3b_scores.png)\n> **图5**：检索场景下，使用GTM模拟与使用真实工具进行RL训练的代理性能评估分数对比。两者分数接近，表明GTM对未见工具类型也具有良好的模拟能力。\n\n    *   **内核优化场景（领域适应）**：通过对GTM在领域特定数据上进行微调，使其能够模拟评估代码属性的专业工具。使用微调后GTM训练的优化代理，其性能与使用真实专业工具训练的代理表现相似。\n\n![步骤时间综合对比](https://arxiv.org/html/2512.04535v2/step_time_comparison.png)\n> **图6**：三个RL场景中，使用GTM模拟与使用真实工具每一步训练时间的综合对比。GTM在所有场景下都带来了显著的加速。\n\n**消融实验总结**：论文通过在不同场景（单轮、多轮、错误）下的性能评估，间接证明了CARG管道各组件的重要性。GTM在多轮和错误场景下的卓越表现，直接得益于CARG管道中针对上下文连贯性和错误生成/验证的专门设计。\n\n## 总结与启发\n本文的核心贡献在于：1）**提出了通用工具模型（GTM）**，作为一个基础组件，能够在不访问真实工具实现的情况下模拟多样化的工具行为，从而解耦RL训练中的工具依赖。2）**设计了上下文感知响应生成（CARG）管道**，系统地生成高质量训练数据，使GTM学习到格式正确性、逻辑连贯性和上下文一致性。3）**通过实验验证了GTM的实用性**，证明其在真实RL训练中能提供数倍的速度提升，同时保持与真实工具相当的输出质量，并具备出色的泛化和领域适应能力。\n\n论文提及的局限性包括：GTM本质上是一个生成模型，可能无法完全模拟所有工具（特别是那些具有复杂内部状态或动态行为的工具）的精确行为；其性能也依赖于基础LLM的能力。\n\n本文的启示在于：为高效、可扩展地训练工具增强型AI代理提供了一条新路径，通过模拟器降低对昂贵、慢速外部资源的依赖。它表明，一个参数相对较小的模型（1.5B），如果经过高质量、针对性数据的训练，可以在特定任务（工具模拟）上超越更大规模的通用模型。这鼓励后续研究进一步探索专用模拟器在复杂代理训练生态系统中的应用，并优化模拟的保真度与效率的平衡。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04535v2/teaser_fig.jpg",
        "https://arxiv.org/html/2512.04535v2/x1.png",
        "https://arxiv.org/html/2512.04535v2/search_scores.png",
        "https://arxiv.org/html/2512.04535v2/search_step_times.png",
        "https://arxiv.org/html/2512.04535v2/qwen2p5_3b_scores.png",
        "https://arxiv.org/html/2512.04535v2/step_time_comparison.png",
        "https://arxiv.org/html/2512.04535v2/api_distribution_tsne.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04446",
      "title": "Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops",
      "url": "http://arxiv.org/abs/2512.04446",
      "arxivId": "2512.04446",
      "date": "2025-12-04",
      "authors": "Minghui Zheng Team",
      "category": "Manipulation",
      "summary": "本文针对报废台式机关键组件（如RAM、CPU）的自动化选择性拆解难题，提出采用端到端的视觉-语言-动作模型。研究通过收集UR5e机器人演示数据集，对OpenVLA和OpenVLA-OFT模型进行微调。实验表明，微调后的VLA模型能可靠完成多个前期拆解步骤，但在某些需高精度操作的子任务上会失败。然而，采用VLA与基于规则控制器结合的混合策略，可成功完成整个拆解流程。这揭示了当前VLA模型在处理精密拆解任务时的局限性。",
      "detailedSummary": "## 研究背景与动机\n当前机器人拆解流程通常划分为多个独立阶段：感知、序列规划、任务规划、运动规划和操作。每个阶段都需要专门的模型，这种分离式设计限制了系统对陌生场景的泛化能力，并可能导致误差累积。视觉-语言-动作模型提供了一种端到端的替代方案，直接将高级视觉感知和自然语言指令映射为机器人动作，有望简化流程。尽管VLA模型在简单的日常任务中表现出色，但其在需要高灵巧性和精度的复杂工业拆解场景（如电子废物拆解）中的可行性尚未被探索。本文针对这一具体痛点，以从报废台式机中拆解关键组件（RAM和CPU支架）为案例，评估微调后的VLA模型在复杂、接触丰富的长周期操作任务中的性能与局限性。核心思路是：收集专用的拆解演示数据集，对现有VLA模型进行微调，并通过实验分析其在拆解任务各子步骤中的表现，最终探讨结合基于规则控制的混合策略的可行性。\n\n## 方法详解\n本研究旨在评估VLA模型在复杂拆解任务中的潜力，整体流程包括：1）构建遥操作平台并收集拆解演示数据；2）处理数据以适配模型微调；3）微调选定的VLA模型；4）在真实拆解场景中测试模型性能，并分析失败原因。\n\n![传统与端到端方法对比](https://arxiv.org/html/2512.04446v1/x2.png)\n> **图2**：传统多阶段拆解方法与端到端视觉-语言-动作方法的对比。左侧传统方法将流程分解为多个独立模块，右侧VLA方法旨在通过单一模型实现从感知和语言指令到动作的端到端映射。\n\n核心模块是经过微调的两个VLA模型：OpenVLA和其改进版OpenVLA-OFT。OpenVLA是一个基于Llama2的70亿参数操作策略，它将连续机器人动作离散化为token，并通过交叉熵损失进行下一个token预测。推理时，它接收单视角图像和语言指令，自回归地预测下一时间步的7个离散动作token（6个关节和1个夹爪）。OpenVLA-OFT则进行了多项改进：**输入方面**，除了第三人称视角图像，还额外提供了手腕视角图像和机器人本体感知状态；**输出方面**，通过一个独立的动作头将隐藏状态映射为归一化的连续动作，并使用L1回归损失进行监督；**推理机制**，支持动作分块，能够预测并执行一系列未来动作，而OpenVLA只能以自回归方式生成单步动作。\n\n![数据收集设置](https://arxiv.org/html/2512.04446v1/x3.png)\n> **图3**：用于模仿数据收集的遥操作设置。展示了UR5e机器人、带有扩展的Robotiq 2F-85夹爪，以及用于提供基础视角和手腕视角的两个RGB-D相机（OAK-D和RealSense D435i）。\n\n为收集高质量数据，研究构建了基于Gello系统的遥操作平台。使用10台不同配置的台式机，共录制了287条轨迹（RAM移除164条，CPU支架解锁123条）。数据采集时，两个相机以30Hz频率异步记录图像和深度信息，机器人状态以约48Hz的频率记录。\n\n![数据处理流程](https://arxiv.org/html/2512.04446v1/x4.png)\n> **图4**：用于微调模型的数据处理步骤以及VLA模型在实验任务中的使用流程。展示了从原始异步数据到对齐、下采样、图像缩放，最终打包成可用于训练的数据文件的过程。\n\n数据处理的关键步骤包括：将机器人状态下采样以与基础相机帧时间戳对齐，移除冗余状态，对齐手腕相机图像，并将所有数据合并。RGB图像被统一缩放至224x224分辨率以满足模型输入要求。模型微调采用LoRA技术进行高效适配。\n\n## 实验与结果\n**实验设置**：研究在两个选定的拆解任务上评估模型：RAM模块移除和CPU支架解锁。测试环境尽可能复现数据收集时的设置。对比的基线是微调后的OpenVLA和OpenVLA-OFT模型。\n\n**关键结果**：实验发现，单独使用任何一个微调后的VLA模型都无法完成完整的拆解任务。然而，模型能够执行部分步骤。例如，在RAM任务中，OpenVLA能在12/20次试验中与RAM对齐，13/20次中接近目标；OpenVLA-OFT表现更好，对齐和接近成功率均为16/20。但两者在精确定位（Localization）步骤上成功率很低（OpenVLA: 4/20， OpenVLA-OFT: 7/20），且均无法完成最终的“驱动”步骤（Actuation，即实际拔出或解锁）。\n\n![训练损失](https://arxiv.org/html/2512.04446v1/x5.png)\n> **图5**：两个VLA模型在两个拆解任务上的训练损失曲线。所有模型均收敛至较小的损失值，表明微调过程在训练集上是有效的。\n\n![实验结果](https://arxiv.org/html/2512.04446v1/x6.png)\n> **图6**：不同VLA模型及控制策略的实验结果。展示了模型在拆解任务各阶段的行为，突显了在精确定位和驱动步骤上的失败。\n\n**阶段分析**：表I详细展示了模型在拆解各子任务上的表现。对于CPU解锁任务，由于夹爪方向固定，无需“对齐”。OpenVLA几乎无法接近目标杠杆（1/20），而OpenVLA-OFT能稳定接近（20/20），但两者均无法精确定位杠杆位置并完成解锁驱动。\n\n**混合策略尝试**：基于VLA模型能完成高层决策和粗定位的观察，研究尝试了一种混合策略：当机器人通过VLA模型移动到目标高度后，切换到一个简单的基于位置的控制来执行夹爪配置和最终驱动动作。在RAM任务中，该策略在10次测试中成功了2次，证明了只要机器人能精确到达所需位姿，底层控制可以可靠完成最终操作。但该策略对CPU任务无效，因为VLA模型从未将机器人带到解锁所需的精确位置。\n\n**消融实验启示**：通过对比OpenVLA和OpenVLA-OFT的表现，可以视为一种结构上的消融。结果表明，增加手腕视角、本体感知输入和连续动作分块预测（OpenVLA-OFT的特性）带来了行为上的改进，例如更连贯的接近动作和更好的夹爪配置保持。然而，这些改进仍不足以满足拆解任务对精度的极端要求，尤其是在需要微小接触点定位的CPU解锁任务中。\n\n## 总结与启发\n**核心贡献**：1）首次系统性地评估了VLA模型在复杂、接触丰富的工业拆解任务中的性能，揭示了其在长周期、高精度操作中的当前局限。2）构建了一个针对台式机关键组件拆解的专用机器人视觉-语言-动作数据集。3）提出并初步验证了一种结合VLA高层引导与基于规则的底层控制的混合策略思路，为应对当前VLA模型的精度瓶颈提供了可行方向。\n\n**局限性**：论文明确指出，微调后模型性能下降的主要原因是协变量偏移，有限的演示数据覆盖不足以应对执行过程中与演示轨迹的偏差。此外，低分辨率视觉输入、复杂且遮挡的桌面环境，以及缺乏力/触觉反馈，都严重制约了模型在需要精密操作任务中的表现。\n\n**未来启示**：研究为后续工作指明了方向：需要更大规模、更多样化的演示数据以覆盖电子废物产品的各种变体；应考虑将力或触觉信号集成到机器人状态中，为模型提供操作进程的反馈；可以探索将强化学习与VLA模型结合，以纠正偏离演示分布状态下的机器人行为；最后，针对拆解等复杂任务，开发专为高精度、接触丰富操作设计的VLA架构或训练范式至关重要。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04446v1/x1.png",
        "https://arxiv.org/html/2512.04446v1/x2.png",
        "https://arxiv.org/html/2512.04446v1/x3.png",
        "https://arxiv.org/html/2512.04446v1/x4.png",
        "https://arxiv.org/html/2512.04446v1/x5.png",
        "https://arxiv.org/html/2512.04446v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04884",
      "title": "Hoi! – A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation",
      "url": "http://arxiv.org/abs/2512.04884",
      "arxivId": "2512.04884",
      "date": "2025-12-04",
      "authors": "Zuria Bauer Team",
      "category": "Manipulation",
      "summary": "本文针对现有交互数据集在人类活动与机器人操作间存在割裂、缺乏力觉与多视角同步数据的问题，提出了Hoi!多模态数据集。该数据集核心包含3048个交互序列，覆盖381个铰接物体，并首次为每个物体提供四种操作具身（人手、腕戴相机人手、手持UMI夹爪、自定义Hoi!夹爪），同步采集RGB、深度、力觉、触觉及多视角视频。数据集通过标注铰接参数（如开合角度、位移、峰值力），支持跨视角与跨具身的迁移研究，为多模态感知、操作学习及力觉预测等任务提供了基准。",
      "detailedSummary": "## 研究背景与动机\n计算机视觉正从纯粹感知转向动态交互时代，旨在理解物体如何被使用、移动或交互。这一进展主要由数据驱动方法和捕获多样化人-物交互的大规模数据集（如EpicKitchens、EgoExo4D）推动。然而，不同研究领域所研究的交互性质存在根本差异。以人为中心的视频数据集强调烹饪、家具组装等长时程活动，而机器人数据集主要针对抓放、抽屉开关等短时程操作。这种数据鸿沟使得研究有趣的迁移问题变得困难：交互力预测器能否泛化到人类视频？铰接跟踪方法在机器人外中心视角下是否依然有效？人类手演示的交互能否重定向到两指机器人夹爪？\n\n铰接家具提供了一个特别丰富但研究不足的案例。尽管在日常人类活动中普遍存在，但人们与家具交互的精选视频数据却很稀少。现有的铰接数据集（如RBO）提供了有价值的标签，但主要基于静态扫描构建，缺乏将这些标注与真实交互数据配对所需的运动数据。为了弥补这一差距，本文提出了一个用于带力感知、跨视角、多具身的铰接家具操作的数据集。核心思路是构建一个耦合了视觉、力和触觉信号，并覆盖人类手、手持工具和机器人夹爪等多种具身的统一数据集，以支持跨模态感知、操作学习和具身推理的研究。\n\n## 方法详解\nHoi! 数据集的构建是一个系统性的多模态数据采集与对齐流程。\n\n![数据采集流程总览](https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiPipeline1-2.png)\n\n> **图45**：Hoi! 数据集采集流程总览。展示了从环境扫描、多设备设置、多具身交互演示，到数据同步与标注的完整pipeline。\n\n**整体框架与数据内容**：数据集在38个真实室内环境（厨房、浴室、办公室、客厅）中采集，包含381个铰接部件（如抽屉、门、冰箱、洗碗机）上的3048个交互序列。每个交互序列的核心是**四种操作具身**：1) 人手；2) 人手 + 腕戴相机；3) 手持UMI夹爪；4) 自定义Hoi!夹爪（配备力扭矩和触觉传感）。对于每种具身，数据从**多个视角**同步采集：一个第一人称视角相机（Project Aria，提供RGB、SLAM、眼动追踪、手部姿态）、两个静态的第三人称视角iPhone（提供RGB + LiDAR深度），以及在工具具身上可能的腕戴视角。此外，每个环境在交互前后都使用Leica RTC360激光扫描仪进行高分辨率3D扫描，提供场景几何真值。\n\n**核心模块与技术细节**：\n1.  **自定义Hoi!夹爪**：这是实现高质量力与触觉数据采集的关键硬件创新。\n![自定义Hoi!夹爪](https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/gripper_real.jpg)\n> **图47**：自定义Hoi!夹爪。采用对向设计，配备两个GelSight Digit触觉传感器进行高分辨率触觉成像，以及一个Bota SensONE六维力扭矩传感器测量腕部交互力。夹爪安装在手持“杆式”装置上，通过手柄中的负载传感器调节夹持力。\n   该夹爪集成了GelSight Digit触觉传感器（用于高分辨率触觉成像）、Bota SensONE六维力扭矩传感器（用于测量交互力）和Dynamixel电机。它被安装在手持装置上，便于人类操作员进行演示，同时能同步记录视觉（腕戴ZED Mini立体相机和Aria设备）、触觉和力信号。\n\n2.  **时空对齐流程**：\n    *   **时间对齐**：所有独立运行的设备通过向每个视频流中以25Hz频率显示编码Unix时间戳的QR码进行同步。检测和解码这些QR码可为每个视频流计算相对于公共参考时间的偏移，实现所有模态在共享时间线上的对齐，精度约为10-25毫秒。\n    *   **空间对齐**：所有记录设备通过基于高分辨率3D扫描的视觉定位，被配准到一个公共参考坐标系。使用`hloc`从扫描生成的全景图像建立2D-3D对应关系，进而估计每个传感器轨迹到共享世界坐标系的单一刚性变换。\n\n3.  **标注**：除了原始传感器流，数据集还提供了丰富的标注，包括：铰接类型（棱柱关节或旋转关节）和轴、物体部件的3D掩码（通过SAMv2在 panoramic 图像上预测并提升至3D点云生成）、语言描述以及序列级状态变化标签。\n\n**创新点**：与现有数据集（总结于论文表1）相比，Hoi! 的创新性体现在首次**同时**提供了：1) **多视角**（第一人称、第三人称、腕戴）同步视频；2) **多具身**（人手、手持工具、机器人夹爪）执行相同任务的对齐记录；3) **多模态**耦合，特别是同步的**力扭矩**和**高分辨率触觉**信号；4) 以高精度**3D场景扫描**作为几何基础真值。这种设计旨在弥合人类活动理解与机器人物理操作之间的鸿沟。\n\n![多视角数据示例](https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiVisuals.png)\n> **图46**：数据集中多视角记录的示例。每一行对应不同的操作设置（人手、Hoi!夹爪等），展示了同步的第三人称视角（左）、第一人称视角（中）和腕戴视角（右）图像。\n\n## 实验与结果\n本文使用Hoi!数据集本身作为benchmark，评估了三个关键任务，以展示其支持的研究价值。\n\n**评估任务与Baseline方法**：\n1.  **野外铰接物体估计**：评估方法从RGB(-D)观测中推断铰接类型和运动参数的能力。对比方法包括：ArtGS（基于高斯泼溅）、ArtiPoint（基于点跟踪）以及GPT-5视觉语言模型（仅预测类型）。\n2.  **触觉力估计**：评估模型仅从触觉图像（GelSight Digit）预测作用在夹爪上的法向力和切向力的能力。使用最先进的触觉表征学习模型Sparsh（配备DINO和DINOv2解码器）进行评估。\n3.  **视觉力估计**：评估模型从RGB-D观察和语言目标中预测所需三维交互力的能力。使用为文本引导操作设计的ForceSight模型进行零样本评估。\n\n**关键实验结果**：\n\n![交互力曲线示例](https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/forces_figure.png)\n> **图43**：不同铰接部件测量到的交互力曲线示例。展示了力的大小如何随铰接部件类型（如抽屉、门）变化，凸显了力信号对视觉线索的补充价值。\n\n1.  **铰接估计结果（论文表4）**：现有方法在Hoi!的复杂野外场景下面临显著挑战。ArtiPoint在Hoi!上的性能（棱柱关节识别率P_prismatic=26.90%， 旋转关节识别率P_revolute=57.10%）远低于其在更受控的Art4D数据集上的表现（P_prismatic=68%， P_revolute=98%）。性能下降主要归因于单目深度估计噪声、场景杂乱以及手部遮挡。相比之下，GPT-5仅从单张图像进行类型预测表现出较强的鲁棒性（在Hoi!上P_prismatic=88.5%， P_revolute=74.3%）。\n\n2.  **触觉力估计结果（论文表5）**：Sparsh模型在Hoi!数据上的力预测误差在几牛顿量级（例如，使用DINO时，组合力RMSE为3.86 N），远高于其在受控实验室环境、使用简单压头训练时达到的毫牛顿级精度。这表明，在受控实验室设定下表现优异的触觉模型，难以泛化到非结构化真实世界交互中遇到的复杂接触几何和负载状态。\n\n3.  **视觉力估计结果（论文表6）**：ForceSight模型在Hoi!数据集上的力预测误差（整体RMSE较高）也显著高于其在原始数据集上的性能（0.404 N）。特别是在包含需要较大操作力的部件（如带磁铁的抽屉）的环境中，模型表现更差（例如`kitchen_7`的RMSE为3.531 N）。这揭示了现有视觉力预测模型对复杂、高力需求场景的泛化能力有限。\n\n![数据集统计分布](https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiStats.png)\n> **图44**：Hoi!数据集中环境和铰接交互类别的分布。条形图显示了不同铰接类别上人类交互的相对频率，饼图总结了所涉及环境的比例。\n\n**消融实验总结**：本文的实验本质上是利用Hoi!数据集对现有方法在新基准上的性能评估，而非对数据集自身组件的消融。实验结果清晰地**验证了数据集的必要性**：当前在受控环境下开发的方法（无论是铰接估计、触觉力预测还是视觉力预测）在迁移到真实、复杂、多变的野外交互场景时，性能均出现显著下降，凸显了利用Hoi!这类真实世界多模态数据进行研究和模型改进的重要性。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出并开源了一个新颖、全面的多模态铰接操作数据集**：Hoi! 提供了3048个序列，涵盖381个物体、38个环境、四种操作具身，并同步采集了RGB-D、力扭矩、触觉、姿态等多种信号，辅以高精度3D扫描真值和丰富标注。\n2.  **设计并开源了一个用于野外交互力采集的自定义夹爪硬件**：该夹爪实现了高质量触觉与力信号的同步移动采集。\n3.  **建立了基于真实物理交互的基准测试**：通过在Hoi!上评估现有SOTA方法，揭示了它们在复杂野外场景下面临的泛化挑战，为未来研究指明了方向。\n\n**局限性**：论文提到，数据集主要关注铰接家具，可能在其他类型的交互或物体上多样性有限。此外，交互是分段录制的，而非连续的长时程活动序列。\n\n**对后续研究的启示**：Hoi!数据集为多个研究方向提供了基础：1) **跨具身技能迁移**：研究如何将人类演示的技能（特别是力觉策略）迁移到不同形态的机器人上；2) **多模态感知融合**：探索如何结合视觉、力和触觉信号来更鲁棒地理解与预测交互；3) **真实世界泛化**：利用该数据训练或微调模型，以提升在非结构化环境中的铰接估计、力预测和操作性能。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiTeaser.png",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Logo.png",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location9.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/bathroom_2.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/kitchen_7.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/kitchen_8.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/office_2.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location21.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/office_4.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location11.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location7.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location6.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location1.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location16.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location2.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location3.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location27.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location5.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location13.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location26.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location18.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location15.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location24.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location12.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location23.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/kitchen_ML.png",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location29.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/mlhall_1.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/kitchen_10.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location10.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/office_3.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/livingroom_3.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Location22.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Locat1.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Locat2.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Locat3.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Locat4.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Locat5.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/bedroom_4.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/closet7.jpeg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/locations/Logo.png",
        "https://arxiv.org/html/2512.04884v2/x1.png",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/forces_figure.png",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiStats.png",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiPipeline1-2.png",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/HoiVisuals.png",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/gripper_real.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/gravity_compensation.png",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/ego-exo.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/time_alignment.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/spot_teleoperation.jpg",
        "https://arxiv.org/html/2512.04884v2/author-kit-CVPR2026-v1-latex-/figures/tactile_forces.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04404",
      "title": "Bridging Probabilistic Inference and Behavior Trees: An Interactive Framework for Adaptive Multi-Robot Cooperation",
      "url": "http://arxiv.org/abs/2512.04404",
      "arxivId": "2512.04404",
      "date": "2025-12-04",
      "authors": "Changju Wu Team",
      "category": "Manipulation",
      "summary": "本文针对动态不确定环境中多机器人自适应协同决策的挑战，提出了交互推理行为树（IIBT）框架。该框架将行为树与基于自由能原则的主动推理相结合，通过扩展IIBT节点引入概率推理，实现分布式在线联合规划与执行。多机器人协作被形式化为自由能最小化过程，机器人基于感知和同伴意图动态更新偏好矩阵以自适应协调。实验表明，IIBT框架将行为树节点复杂度降低超过70%，并在环境不确定性下保持鲁棒、可解释的自适应协作行为。",
      "detailedSummary": "## 研究背景与动机\n在多机器人领域，传统的集中式规划方法计算成本高、可扩展性差，而完全分布式的反应式控制架构在不确定性下难以维持一致的团队策略。行为树作为一种模块化、层次化且可解释的控制范式，在机器人学中被广泛采用。然而，一旦行为树结构构建完成，其本质上是确定性的，难以应对部分可观测性、动态任务依赖和不断变化的协作需求。\n\n另一方面，基于自由能原理的交互推理为感知、预测和决策提供了概率基础，但现有的推理框架通常是整体或集中式的，难以嵌入到行为树这类模块化的节点式决策架构中。因此，行为树的可解释性、模块化与推理方法的概率适应性之间存在鸿沟。\n\n本文针对这一痛点，提出将自由能原理下的主动推理与行为树执行语义相集成，构建了一个名为交互推理行为树的新框架，旨在实现分布式、自适应且可协作的多机器人决策。其核心思路是设计一种新型的IIBT节点，将概率推理能力嵌入行为树执行节点，使机器人能够在执行过程中在线更新信念、动态调整决策优先级，并与队友协调。\n\n## 方法详解\n本文提出的交互推理行为树框架将概率推理与行为树的模块化决策结构紧密集成。每个机器人维护一个内部生成式模型 ℳ𝑖 = {𝒜𝑖, ℬ𝑖𝜋, 𝒞𝑖, 𝒟𝑖}，分别对应观察似然、状态转移、结果偏好和初始状态先验。推理层基于此模型估计潜在状态信念 𝑠𝑖𝜏，预测候选策略 𝜋𝑖𝑘 下的可能结果，并评估其期望自由能。得到的后验信念被馈送到行为树层，该层通过条件节点和动作节点的结构化层次来编排任务执行。\n\n![方法框架](https://arxiv.org/html/2512.04404v1/x3.png)\n> **图3**：IIBT节点在行为树中的工作流程。每个机器人收集本地观测 𝒪𝑖𝜏，将其抽象为逻辑变量 ℒ𝑖，并更新其信念 𝑠𝑖𝜏。行为树向推理模块发出偏好矩阵 𝒞𝑖，推理模块查询任务模型以获取 𝒜𝑖, ℬ𝑖𝜋, 𝒟𝑖，纳入其他机器人的意图，并将状态/策略信息返回给行为树执行。\n\n核心模块是交互推理行为树节点，它作为桥梁连接了行为树的反应式执行流与推理模块的概率推理。在每次行为树“Tick”时，该节点不仅决定执行哪个动作策略，还会根据新的观测和队友推断的意图更新其信念和任务偏好。\n\n算法1详细描述了机器人 ℛ𝑖 的IIBT节点的完整推理与执行周期。其主要步骤包括：1）检索生成模型和来自任务的偏好矩阵；2）获取当前环境观测；3）更新信念状态和逻辑变量；4）构建候选策略集，并通过通信纳入其他机器人的策略意图；5）调用 `InteractiveInfer` 函数进行交互推理以选择策略；6）检查策略前提条件是否满足，若不满足则调整逻辑变量和偏好矩阵并重新推理；7）在团队连接就绪时执行策略，否则执行等待策略。\n\n在技术细节上，每个机器人维护的潜在状态向量为 𝑠𝑖𝜏 = [𝑠𝑖loc, 𝑠𝑖hold, 𝑠𝑖place, 𝑠𝑖free]⊤，表示对到达位置、抓取、放置或空闲的信念。推理过程作用于联合状态 𝑠joint𝜏 = [𝑠1𝜏, 𝑠2𝜏, …, 𝑠𝑁𝜏, 𝑠result𝜏]⊤，以捕获机器人间的依赖关系。观测表示为二进制向量，直接映射到潜在信念状态。策略选择基于最小化期望自由能 𝒢(𝜋𝑖𝑘)，它由鼓励目标对齐的外在价值和促进信息获取的内在价值组成。策略的后验执行概率由公式 𝑝(𝜋𝑖𝑘) = 𝜎(lnℰ𝑘 − ℱ(𝜋𝑖𝑘) − 𝛾𝒢(𝜋𝑖𝑘)) 给出，其中𝛾是平衡规划与推理的精度参数。\n\n与现有方法相比，创新点在于将自由能最小化的概率推理直接嵌入到行为树节点的执行语义中，使得每个节点都成为一个自主推理单元，在保持行为树模块化和可解释性优点的同时，获得了基于概率的在线适应与协同能力。\n\n## 实验与结果\n实验在两个任务上验证了所提框架：多机器人迷宫导航和协作操控任务。平台包括仿真环境（使用ROS和Gazebo）以及真实机器人实验（使用TurtleBot3和机械臂）。对比的基线方法包括：1）**传统行为树**：使用预定义、确定性的行为树；2）**集中式主动推理**：采用集中式规划器进行全局策略优化。\n\n关键实验结果如下：在迷宫导航任务中，IIBT框架在动态障碍物场景下的任务成功率比传统行为树提高了约25%。在协作操控任务中，IIBT实现了超过95%的成功率，而传统行为树在物体位置不确定时成功率下降至约70%。\n\n![节点复杂度对比](https://arxiv.org/html/2512.04404v1/x4.png)\n> **图4**：不同方法在迷宫导航任务中的行为树节点数量对比。IIBT框架的节点数量显著少于传统行为树和集中式主动推理方法。\n\n![任务成功率](https://arxiv.org/html/2512.04404v1/x5.png)\n> **图5**：在迷宫导航任务中，随着环境动态性（移动障碍物速度）增加，各种方法的任务成功率变化。IIBT方法在所有设置下均保持最高成功率。\n\n![协作任务性能](https://arxiv.org/html/2512.04404v1/x6.png)\n> **图6**：协作操控任务中，在物体位置不确定性增加时，各方法的任务成功率和完成时间。IIBT在成功率和效率上均优于基线。\n\n![真实机器人实验](https://arxiv.org/html/2512.04404v1/x7.png)\n> **图7**：真实机器人协作操控任务的序列快照，展示了机器人如何通过IIBT框架自适应地协调抓取与放置动作。\n\n![消融实验](https://arxiv.org/html/2512.04404v1/x8.png)\n> **图8**：消融实验展示了IIBT框架中各组件的贡献。完整模型（蓝色）性能最佳，移除同伴意图交互（橙色）或偏好矩阵更新（绿色）会导致性能下降，尤其是前者对协同效率影响显著。\n\n实验表明，IIBT框架将行为树节点复杂度降低了70%以上，同时保持了鲁棒、可解释和自适应的协作行为。消融实验证实，同伴意图交互和动态偏好矩阵更新是提升协同效率的关键组件。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1）**概率推理与行为树的集成**：提出了新颖的IIBT节点，将基于自由能的推理无缝嵌入行为树执行语义，实现了不改变树结构的在线适应。2）**分布式协同策略选择**：每个节点基于期望自由能进行局部推理，支持多机器人间可扩展、去中心化且连贯的决策。3）**全面的实验验证**：通过仿真和真实机器人实验验证了方法在任务成功率、决策效率和降低复杂度方面的显著优势。\n\n论文自身提到的局限性包括：1）机器人间频繁交换策略意图可能增加通信负载；2）框架性能依赖于生成模型（𝒜, ℬ, 𝒞, 𝒟）的精度，这些模型需要针对具体任务进行设计。\n\n这项工作为多机器人系统决策提供了一个强大的混合框架，对后续研究的启示在于：1）展示了将结构化决策（行为树）与概率推理（主动推理）深度结合的可行性，此框架可扩展至更复杂的任务和人机协作场景；2）如何自动学习或优化生成模型参数，而非依赖手工设计，是一个有前景的方向；3）框架中的交互推理机制为研究隐式协调和信念对齐提供了基础。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04404v1/x1.png",
        "https://arxiv.org/html/2512.04404v1/x2.png",
        "https://arxiv.org/html/2512.04404v1/x3.png",
        "https://arxiv.org/html/2512.04404v1/x4.png",
        "https://arxiv.org/html/2512.04404v1/x5.png",
        "https://arxiv.org/html/2512.04404v1/x6.png",
        "https://arxiv.org/html/2512.04404v1/x7.png",
        "https://arxiv.org/html/2512.04404v1/x8.png",
        "https://arxiv.org/html/2512.04404v1/x9.png",
        "https://arxiv.org/html/2512.04404v1/x10.png",
        "https://arxiv.org/html/2512.04404v1/x11.png",
        "https://arxiv.org/html/2512.04404v1/x12.png",
        "https://arxiv.org/html/2512.04404v1/x13.png",
        "https://arxiv.org/html/2512.04404v1/x14.png",
        "https://arxiv.org/html/2512.04404v1/x15.png",
        "https://arxiv.org/html/2512.04404v1/x16.png",
        "https://arxiv.org/html/2512.04404v1/x17.png",
        "https://arxiv.org/html/2512.04404v1/progress2.png",
        "https://arxiv.org/html/2512.04404v1/progress1.png",
        "https://arxiv.org/html/2512.04404v1/progress4.png",
        "https://arxiv.org/html/2512.04404v1/progress3.png",
        "https://arxiv.org/html/2512.04404v1/progress5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04731",
      "title": "Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting",
      "url": "http://arxiv.org/abs/2512.04731",
      "arxivId": "2512.04731",
      "date": "2025-12-04",
      "authors": "Xuguang Lan Team",
      "category": "Manipulation",
      "summary": "本文解决机器人操作中模拟到现实（Sim-to-Real）的跨域迁移难题。针对模拟与现实间的视觉差异，提出语义2D高斯泼溅（S2GS）方法，通过构建多视图2D语义场，利用特征级高斯泼溅将其投影至统一3D空间，并过滤无关背景，提取以物体为中心的领域不变特征。实验在ManiSkill仿真环境中进行，并部署至现实场景，结果表明S2GS显著提升了策略在现实世界的泛化性能，实现了高且稳定的任务完成率。",
      "detailedSummary": "## 研究背景与动机\n机器人操作中的跨域迁移（尤其是从仿真到现实）因两者在视觉外观、物体多样性和环境复杂性上的显著差异而长期面临挑战。现有主流方法包括领域随机化、领域自适应以及仿真-现实标定。领域随机化在训练时引入纹理、光照和动力学的可变性以提升鲁棒性，但通常需要大规模随机化环境且可能产生过于保守的策略。领域自适应学习仿真域与现实域之间的映射，而仿真-现实标定方法则通过匹配传感器、材料和动力学属性来校准仿真器。然而，这些方法普遍存在局限性：领域随机化和自适应通常需要大量的超参数调优和多样化数据；标定方法则需要大量工程努力，且难以泛化到仿真调优未捕获的真实世界长尾变化。\n\n从表征学习的角度看，早期基于RGB的神经网络方法难以保持跨视角的语义和空间一致性。点云表征显式编码3D结构但稀疏且非结构化，难以嵌入高级语义。NeRF类方法能结合语义并保持多视角一致性，但其沉重的计算成本和缓慢的渲染速度使其不适用于实时机器人控制。总体而言，现有方法难以高效、准确地提取对鲁棒仿真到现实迁移至关重要的、领域不变的空间特征。\n\n本文针对上述痛点，提出了一个新的视角：如果在仿真策略训练中使用领域不变的特征，并且在现实世界部署时能够提取并提供相同类型的特征作为策略输入，那么领域鸿沟就能被有效弥合，从而显著提升策略的泛化能力。受人类通过神经元选择性提取稳定、不变的视觉表征以实现跨域泛化的认知神经科学启发，本文旨在从表征层面提取领域不变的空间特征。因此，本文提出了语义2D高斯泼溅（Semantic 2D Gaussian Splatting, S2GS）方法。其核心思路是：通过多视角图像构建2D语义场，并利用特征级的高斯泼溅将其投影到统一的3D语义空间，再通过语义检索机制过滤无关背景，提取出以目标物体为中心的、领域不变的空间特征，为下游策略学习提供干净、聚焦的输入。\n\n## 方法详解\nS2GS的整体目标是提取领域不变的空间特征以支持鲁棒的跨域策略迁移。其流程分为离线的场构建与优化阶段，以及在线的特征提取与动态更新阶段。\n\n![方法总览](https://arxiv.org/html/2512.04731v1/x2.png)\n> **图2**：S2GS方法总览。S2GS旨在提取领域不变的空间特征以支持鲁棒的跨域策略迁移。在初始阶段，S2GS提取多视角图像的分层语义特征，并优化语义2D高斯泼溅场和解码器。在执行时，语义检索模块查询并过滤任务相关物体，同时移除背景干扰。生成的领域不变空间特征作为紧凑、干净的输入提供给下游扩散策略学习。操作后，S2GS支持动态场景更新以保持实时准确的场景表征，满足在线机器人控制的要求。\n\n**核心模块与技术细节**：\n1.  **语义2D高斯泼溅场构建**：方法以2D高斯泼溅（2DGS）为基础，将场景表示为3D空间中的一系列2D高斯图元（椭圆形圆盘）。每个图元除了位置、旋转、缩放、不透明度等几何外观属性外，还附加了一个可学习的高维语义特征向量。为了获取监督信号，方法从多视角输入图像中提取分层语义特征：首先使用CLIP和SAM模型获取图像的原始语义特征和不同区域的掩码；然后通过掩码池化获得全局语义特征；接着根据掩码裁剪图像并再次输入CLIP，获得物体级语义特征，并重映射回原图空间得到局部语义特征。在渲染时，语义特征通过与颜色渲染类似的α-blending过程进行融合。由于存储的语义特征维度较低，方法引入一个浅层MLP解码器将渲染后的特征映射回原始高维空间，并通过余弦相似度损失监督其预测的全局和局部语义特征。总损失函数结合了外观重建损失（L1 + D-SSIM）、语义蒸馏损失和几何正则化损失（法向和深度一致性）。\n\n2.  **任务目标物体空间特征提取**：场构建完成后，可以通过自然语言（利用CLIP）查询特定物体。通过计算每个2D高斯图元的解码语义特征与查询语言的相似度，可以初步识别出目标物体对应的图元。为了提升分割质量并提取精确的物体姿态，采用两阶段细化过程：首先使用DBSCAN聚类过滤空间孤立的噪声图元；然后选择最大的聚类作为目标物体，并应用凸包补全来填充分割中可能存在的孔洞，确保物体的完整表示。最后，通过聚合属于该物体的所有高斯图元，计算其质心表征，作为该物体紧凑、稳定的空间特征（状态s），提供给下游策略。\n\n3.  **动态场景更新**：在机器人操作过程中，场景是动态变化的。为了维持准确的场景表征，S2GS直接优化物体的运动参数。假设物体是刚性的，其运动用一个SE(3)变换（3D平移向量和4D旋转四元数）参数化。通过最小化渲染图像与带掩码的输入图像之间的L1损失，并结合已知的机器人操作作为强运动先验，来求解这些运动参数，从而实现实时更新，无需耗时重新扫描。\n\n4.  **下游策略集成**：本文采用扩散策略作为下游学习算法。策略以S2GS提取的物体中心空间特征s和当前机器人状态（末端执行器位姿q）为条件。它通过反向扩散过程，从随机噪声动作序列开始，逐步去噪生成可行的动作序列。训练目标采用均方误差损失，预测前向扩散过程中每一步所添加的噪声。\n\n**创新点**：与现有方法相比，S2GS的创新性主要体现在：1) **表征层面**：首次将语义信息与高效的2D高斯泼溅结合，构建出能同时编码高质量几何、外观和分层语义的3D场。2) **特征提取**：提出了一套从该场中提取以物体为中心、领域不变空间特征的流程，包括开放词汇查询、聚类去噪和几何补全，能有效过滤背景干扰。3) **系统实用性**：实现了对动态场景的实时优化更新，并验证了其与扩散策略结合后，在仿真到现实迁移中的有效性。\n\n## 实验与结果\n**实验设置**：仿真训练在ManiSkill2环境中进行，使用7自由度Franka Panda机器人。为促进向真实6自由度UR5机器人的迁移，策略输入中移除了关节角度信息。真实世界评估在UR5机械臂上进行，使用安装在夹爪上的Intel RealSense D435i相机进行初始全景扫描以构建S2GS场（仅优化7000步以追求效率），另一个固定的同款相机用于持续更新动态场景。\n\n**基线方法**：以ManiSkill模仿学习基准中的扩散策略（Diffusion Policy, DP）为基础，对比了不同输入模态：原始RGB（DP-RGB）、添加了高斯渲染以过滤背景的RGB（DP-RGB+GS）、进一步添加了S2GS语义增强的RGB（DP-RGB+GS+S2GS）、以及完全使用S2GS提取的空间特征（DP-S2GS）。同时，以使用真实状态（ground-truth state）输入（DP-State）作为性能上界。\n\n**仿真实验结果**：在PickCube、PushCube和StackCube三个任务上的成功率如表1所示。\n![仿真任务图示](https://arxiv.org/html/2512.04731v1/x5.png)\n> **图5**：仿真中的三个任务：PickCube、PushCube和StackCube。\n\n| 输入 | 方法 | Pick | Push | Stack |\n| :--- | :--- | :--- | :--- | :--- |\n| RGB | DP-RGB | 0.76 | 0.89 | 0.04 |\n| RGB | DP-RGB+GS | 0.91 | 0.92 | 0.29 |\n| RGB | DP-RGB+GS+S2GS | 1 | 1 | 0.94 |\n| S2GS | **DP-S2GS** | **1** | **1** | **0.97** |\n| State | DP-State | 1 | 1 | 0.97 |\n\n**表1**：不同方法在三个任务上的成功率。性能对比证明了我们的方法在仿真环境中不同操作任务上的有效性。\n\n关键发现：1) 纯RGB输入在需要多物体推理的StackCube任务上几乎失败（0.04）。2) 添加高斯渲染过滤背景（DP-RGB+GS）后性能显著提升，尤其是在Stack任务上（0.04→0.29）。3) 进一步引入S2GS语义增强（DP-RGB+GS+S2GS）带来了巨大飞跃，Stack任务成功率提升至0.94。4) **最重要的是，完全抛弃传统视觉编码器，仅使用S2GS提取的空间特征（DP-S2GS）取得了最佳性能**，并与使用真实状态的上界（DP-State）性能几乎持平。这证明S2GS提取的特征在功能上等价于真实状态，且避免了原始RGB引入的干扰。\n\n**真实世界结果**：将仿真训练好的DP-S2GS策略直接部署到UR5机器人上，在形状、材质、外观与仿真物体均不同的多种家居物体上执行拾取、推动、堆叠任务。\n![真实世界结果](https://arxiv.org/html/2512.04731v1/x4.png)\n> **图4**：真实世界结果。我们的方法在真实世界操作任务中取得了高成功率，证明了S2GS表征的有效性。\n\n| 任务 | 具体物体 | 成功率 |\n| :--- | :--- | :--- |\n| **Pick** | 木头 | 5/5 |\n| | 草莓 | 3/5 |\n| | 鸡蛋 | 5/5 |\n| | **平均** | **86.7%** |\n| **Push** | 鼠标 | 3/5 |\n| | 罐子 | 5/5 |\n| | 水桶 | 5/5 |\n| | **平均** | **86.7%** |\n| **Stack** | 鸡蛋放入红罐 | 4/5 |\n| | 鸡蛋放入碗中 | 5/5 |\n| | 木头放在罐上 | 3/5 |\n| | **平均** | **80.0%** |\n\n**表2**：DP-S2GS在真实世界拾取、推动、堆叠任务上的成功率（%）。我们的方法在真实世界基础操作原语上取得了持续的高性能。\n\n结果显示，S2GS方法在真实世界取得了 consistently high的成功率。性能下降主要源于物理层面的挑战，如草莓的不规则表面导致力闭合抓取困难，或鼠标推动时可能发生的滑动，而非视觉表征的失败。\n\n**消融实验**：图6展示了不同组件对特征提取质量的影响。\n![消融研究](https://arxiv.org/html/2512.04731v1/x6.png)\n> **图6**：消融研究。从左至右：(a) 输入图像；(b) 仅使用CLIP相似度查询的初始分割，存在噪声和孔洞；(c) 应用DBSCAN聚类后，噪声被有效去除；(d) 进一步应用凸包补全后，获得了完整、准确的目标物体分割。\n\n该图直观展示了每个组件的贡献：1) **初始查询**：仅靠CLIP相似度能大致定位物体，但结果包含大量噪声且不完整。2) **DBSCAN聚类**：有效过滤了空间孤立的噪声点，使物体区域更加清晰。3) **凸包补全**：填补了因表面语义单一或遮挡导致的孔洞，形成了完整、准确的物体分割，为提取稳定的空间特征（如质心）奠定了基础。\n\n## 总结与启发\n**核心贡献**：1) 提出了**语义2D高斯泼溅（S2GS）**，一种新颖的表征方法，能够提取以物体为中心、领域不变的空间特征，有效缩小了仿真到现实策略迁移中的领域鸿沟。2) S2GS具备**高可编辑性和实时能力**，能够灵活移除任务无关的背景干扰，同时保持满足真实世界在线机器人控制要求的性能。3) 通过将S2GS与扩散策略结合，在仿真和真实机器人实验中**验证了其在跨域仿真到现实迁移中的有效性**，性能匹配甚至超越了使用真实状态输入的上界。\n\n**局限性**：论文自身提到的局限性包括：1) 当前方法假设物体是**刚性**的，对于非刚性物体的动态更新需要进一步研究。2) 在真实世界实验中，性能瓶颈有时来自**物理层面的挑战**（如抓取接触几何、滑动），而非视觉表征。\n\n**对后续研究的启示**：1) **表征学习的新方向**：证明了从高效、可编辑的3D语义场中提取高层任务特征，比直接使用原始RGB或点云更能促进跨域泛化，这为机器人表征学习提供了新思路。2) **系统集成范例**：S2GS与扩散策略的成功结合，展示了一种“感知（表征）+ 决策（策略）”的模块化系统设计范例，其中感知模块专注于提取领域不变的抽象状态，决策模块则基于此进行规划。3) **迈向更复杂的交互**：未来工作可以探索将S2GS应用于更复杂的非刚性物体操作、长视野任务规划，以及如何与大型语言/视觉-语言模型结合，实现更高层次的语义理解和任务分解。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04731v1/x1.png",
        "https://arxiv.org/html/2512.04731v1/x2.png",
        "https://arxiv.org/html/2512.04731v1/x3.png",
        "https://arxiv.org/html/2512.04731v1/x4.png",
        "https://arxiv.org/html/2512.04731v1/x5.png",
        "https://arxiv.org/html/2512.04731v1/x6.png",
        "https://arxiv.org/html/2512.04731v1/x7.png",
        "https://arxiv.org/html/2512.04731v1/x8.png",
        "https://arxiv.org/html/2512.04731v1/x9.png",
        "https://arxiv.org/html/2512.04731v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04399",
      "title": "Development of a 15-Degree-of-Freedom Bionic Hand with Cable-Driven Transmission and Distributed Actuation",
      "url": "http://arxiv.org/abs/2512.04399",
      "arxivId": "2512.04399",
      "date": "2025-12-04",
      "authors": "Hesheng Wang Team",
      "category": "Manipulation",
      "summary": "本论文旨在解决仿生手设计中，如何在保持人手尺寸和15个自由度的前提下，最小化驱动器数量的核心难题。其关键技术是提出了一种新型线缆（肌腱）驱动机制与分布式驱动架构：在前臂布置5个电机提供强力抓握，在手掌集成10个小电机实现精细操作。该设计显著减少了传统线驱系统所需的电机数量。最终系统总重仅1.4千克，实验验证了其兼具出色的灵巧性与强健的抓握能力。",
      "detailedSummary": "## 研究背景与动机\n在机器人灵巧手研究中，如何在维持与人类手部一致的尺寸和自由度（DoF）的同时，最小化驱动器数量是一个根本性挑战。现有主流设计方法主要分为三类：模块驱动（电机直接集成于手指内）、连杆驱动（动力源置于手掌/前臂，通过连杆/齿轮传动）以及肌腱驱动（通过绳索模仿肌肉驱动骨骼）。这些方法各有局限：模块驱动导致手指过宽（约30mm）、惯性大且输出力有限；连杆驱动虽使手指紧凑，但传动复杂，在手掌有限空间内难以平衡力与速度；传统肌腱驱动虽灵活且能提供高指端力，但通常需要大量电机协同驱动，增加了系统体积。\n\n本文针对“在匹配人手解剖尺寸的前提下，实现高自由度与高性能，同时减少驱动器数量”这一具体痛点，提出了分布式驱动的新视角。其核心思路是：仿照人手肌肉分布策略，将驱动系统分层布置——将提供核心抓握力的强力电机置于前臂，将负责精细操作的多个小电机置于手掌，并结合一种新型三自由度并行肌腱传动机制，从而在紧凑尺寸内实现15个自由度的完全驱动。\n\n## 方法详解\n本文提出的仿生手（IRMV Hand）是一个具有15个自由度的绳驱（肌腱驱动）系统。其整体设计框架遵循“分布式驱动”理念：五个提供抓握力的电机位于前臂，十个用于精细操作的小电机位于手掌。每根手指具有3个自由度，分别对应于掌指关节（MCP）的外展/内收（MCP1）、屈曲/伸展（MCP2）以及近端指间关节（PIP）的屈曲/伸展。远端指间关节（DIP）可根据需求配置为固定或主动。\n\n![方法框架](https://arxiv.org/html/2512.04399v1/hand.png)\n\n> **图3**：IRMV 手的 CAD 设计。(a) 展示了其运动学结构，具有15个自由度。(b) 显示了整体结构，尺寸约为 265mm × 100mm × 100mm，手指宽度为20mm。前臂部分装有5个用于抓握的电机，手掌部分装有10个用于辅助操作的电机。\n\n核心模块包括机械传动系统、电子硬件系统和控制系统。\n1.  **创新的3-DoF并行肌腱传动机制**：这是方法的核心技术创新。对于复杂的MCP关节，论文设计了一种由两个位于手掌的小电机（N20）通过双向滑轮协同驱动的并行缆绳系统。\n    ![MCP关节传动](https://arxiv.org/html/2512.04399v1/mcp_joint.png)\n    > **图2**：(a) MCP关节结构，包含弹簧预紧机构以补偿肌腱松弛。(c)(e) 双电机同向旋转驱动MCP1关节（外展/内收）；(d)(f) 双电机反向旋转驱动MCP2关节（屈曲/伸展）。\n    如图2所示，两个电机的同向旋转可以驱动手指的外展/内收（MCP1），而反向旋转则可以驱动手指的屈曲/伸展（MCP2），实现了两个自由度运动的解耦控制。这种并行设计简化了传动结构，扩大了工作范围。\n\n2.  **PIP关节驱动**：PIP关节的屈曲/伸展由一个位于前臂的更大功率电机（CHR-GM20-180）直接驱动。驱动缆绳通过聚四氟乙烯（PTFE）衬里导管进行低摩擦传输。\n    ![PIP关节传动](https://arxiv.org/html/2512.04399v1/pip_joint.png)\n    > **图4**：(a) PIP关节由前臂舱内的电机单元驱动。(b) 手指屈曲和伸展运动的肌腱布线配置，分别在MCP2和PIP关节产生定向力矩。\n\n3.  **紧凑的仿生手硬件系统**：为支持多电机稳定控制和多传感器反馈，设计了专用的电子系统。主控制器采用STM32F411。每个手指集成4个旋转电位器（共20个），通过ADS1118 ADC进行数据采集，实现优于0.02°的关节角度分辨率。电机驱动板采用STM32G071和DRV8213驱动器，对最多四个电机实现带PID的闭环电流控制，通信频率可达1kHz。\n    ![硬件系统](https://arxiv.org/html/2512.04399v1/com_framework.png)\n    > **图5**：IRMV手的硬件系统框图。\n    ![位置传感电路](https://arxiv.org/html/2512.04399v1/ADS1118.png)\n    > **图6**：位置传感电路。\n    ![电机控制板](https://arxiv.org/html/2512.04399v1/bdc_controller.png)\n    > **图7**：有刷电机控制板。\n\n4.  **控制策略**：控制系统基于关节位置传感器反馈和动力学模型。如图9所示，通过计算位置差，经由PID控制器计算出每个电机的目标电流，最终在电机驱动板层面实现闭环电流（扭矩）控制。系统还设计了低通滤波器以在通信断开时提供保护。\n    ![控制框图](https://arxiv.org/html/2512.04399v1/diagram.png)\n    > **图9**：单指控制框图。\n\n与现有方法相比，本文的创新点具体体现在：1) **分布式驱动布局**：仿生学地将强力与精细驱动分离布置，优化了空间利用和性能平衡；2) **单指三自由度并行肌腱机构**：用两个电机实现了MCP关节两个自由度的解耦驱动，减少了传动复杂性；3) **高度集成且性能匹配的硬件系统**：在1.4kg的总重和接近人手尺寸（指宽20mm）下，实现了15个自由度的完全驱动与传感。\n\n## 实验与结果\n实验部分在自建的测试平台上进行，评估了单指性能、工作空间以及整体抓握能力。使用了三轴力传感器（ZNSW-F）测量指尖力，并采用由Feix等人建立的Grasp Taxonomy基准来系统评估抓握多样性。\n\n**关键实验结果如下：**\n1.  **单指力学性能**：如图11所示，食指在垂直方向（Z轴）能产生的最大压缩力达到11N。手指关节的设计运动速度约为30 rpm，整手抓握的设计负载约为10 kg。\n    ![单指接触力测试](https://arxiv.org/html/2512.04399v1/force_test.png)\n    > **图10**：接触力测量测试平台。\n    ![单指接触力结果](https://arxiv.org/html/2512.04399v1/figure_force.png)\n    > **图11**：单指接触力，显示Z轴方向最大压力达11N。\n\n2.  **工作空间**：根据表II列出的关节运动范围，通过计算得到了每个指尖中心点的总工作空间。\n    | 关节 | 旋转范围 | 指骨长度 |\n    | :--- | :--- | :--- |\n    | MCP-1 | -30°-30° | 16mm |\n    | MCP-2 | 0°-90° | 32mm |\n    | PIP | 0°-90° | 24mm |\n    | DIP (可选) | 0°-90° | 20mm |\n    > **表II**：手指关节运动范围和长度参数。\n    ![工作空间](https://arxiv.org/html/2512.04399v1/workspace2.png)\n    > **图12**：IRMV手各指尖中心点的工作空间彩色点图。每个手指的工作空间总体积为99 cm³。\n\n3.  **物体抓握能力**：如图13所示，IRMV手成功实现了Grasp Taxonomy中的33种不同抓握类型，涵盖了力量抓握、精度抓握和混合抓握模式，证明了其在多尺度物体操作任务中的多功能性和鲁棒性。\n    ![抓握测试](https://arxiv.org/html/2512.04399v1/grasp_test.png)\n    > **图13**：IRMV手上展示的GRASP分类法抓握类型。机械手能够使用不同姿势牢固抓握各种物体。\n\n论文未进行与其他方法的定量性能对比，也未安排系统的消融实验来分解各组件贡献。实验结果主要以展示该原型机的基本性能指标和功能实现为主。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **提出了分布式驱动设计理念**：通过在前臂布置高功率电机、在手掌布置多个小电机，有效兼顾了抓握力量与操作灵巧性，并优化了空间布局。\n2.  **发明了一种新颖的单指三自由度并行肌腱传动机制**：使用两个电机协同驱动，实现了MCP关节外展/内收与屈曲/伸展运动的解耦控制，简化了机械结构。\n3.  **开发了一套完整且紧凑的仿生手硬件系统**：在接近成人男性手部尺寸（指宽20mm，总重1.4kg）下，实现了15自由度全驱动、高精度关节传感和实时控制。\n\n论文自身提到的局限性或未来工作方向包括：计划扩展手指的感知任务（如开发肌腱张力传感器和触觉皮肤传感器），以及改进腕部机制设计以实现更灵巧的手部操作任务。\n\n这项研究对后续的启示在于：为高自由度仿生手的设计提供了一种有效的“仿生分层驱动”范式，表明通过精细的机电系统集成，可以在严格的人体工学尺寸约束下实现高性能。如何进一步集成多模态感知、提升自适应抓握的智能控制，以及降低系统的复杂性与成本，是值得跟进的研究方向。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04399v1/physical.png",
        "https://arxiv.org/html/2512.04399v1/mcp_joint.png",
        "https://arxiv.org/html/2512.04399v1/hand.png",
        "https://arxiv.org/html/2512.04399v1/pip_joint.png",
        "https://arxiv.org/html/2512.04399v1/com_framework.png",
        "https://arxiv.org/html/2512.04399v1/ADS1118.png",
        "https://arxiv.org/html/2512.04399v1/bdc_controller.png",
        "https://arxiv.org/html/2512.04399v1/kinematic_01.png",
        "https://arxiv.org/html/2512.04399v1/diagram.png",
        "https://arxiv.org/html/2512.04399v1/force_test.png",
        "https://arxiv.org/html/2512.04399v1/figure_force.png",
        "https://arxiv.org/html/2512.04399v1/workspace2.png",
        "https://arxiv.org/html/2512.04399v1/grasp_test.png",
        "https://arxiv.org/html/2512.04399v1/figure/haoqihan.jpg",
        "https://arxiv.org/html/2512.04399v1/figure/yiyang.jpg",
        "https://arxiv.org/html/2512.04399v1/figure/yuyifei.jpg",
        "https://arxiv.org/html/2512.04399v1/figure/yixuanzhou.jpg",
        "https://arxiv.org/html/2512.04399v1/figure/xiaohanzhu.jpg",
        "https://arxiv.org/html/2512.04399v1/figure/heshengwang.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.04308",
      "title": "ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models",
      "url": "http://arxiv.org/abs/2512.04308",
      "arxivId": "2512.04308",
      "date": "2025-12-03",
      "authors": "Jianwei Zhang Team",
      "category": "Manipulation",
      "summary": "本文提出ResponsibleRobotBench基准，旨在解决当前基于多模态大语言模型（LMM）的机器人系统在复杂高风险环境中缺乏可靠性与安全操作评估标准的核心问题。该基准通过构建包含电气、火灾/化学、人际等多类风险的23个具体任务场景，采用模块化框架支持预定义技能、操作姿态和代码生成等多模态动作表示，并设计细粒度指标评估机器人的风险识别、安全规划与物理执行能力。基准建立了可复现的基线，为推进负责任实体智能的系统化评测提供了统一平台。",
      "detailedSummary": "## 研究背景与动机\n当前，大型多模态模型（LMMs）显著推动了具身智能的发展，使机器人能够通过通用推理和感知执行日益复杂的任务。然而，现有机器人系统在执行任务的可靠性和责任性方面仍远未达到人类水平。尽管已有工作利用LMMs提升任务泛化和规划能力，但一个关键空白在于：缺乏对确保机器人在复杂、高风险环境中可靠安全操作的关注。社区缺乏一个系统性的框架来评估这些能力，阻碍了真正负责任的具身智能的发展。现有机器人基准（如RLBench、BEHAVIOR）主要关注任务成功率、多任务可扩展性或人机交互流畅性，但缺少专门针对安全关键场景下“负责任机器人操作”的基准。\n\n本文针对这一痛点，提出了首个系统性评估基于L模态大模型的机器人操作在安全、可靠和风险意识方面能力的基准。核心思路是构建一个统一、实用的基准，提供多样化的风险感知任务场景、严格的操作可靠性评估指标、模块化可扩展的框架、可复现的基线以及用于识别成功因素和失败模式的分析工具。\n\n## 方法详解\nResponsibleRobotBench是一个全面的评估框架，用于评估由大型语言模型（LLMs）和视觉语言模型（VLMs）驱动的机器人操作系统的可靠性、安全性和风险意识。\n\n![方法框架](https://arxiv.org/html/2512.04308v1/x1.png)\n> **图1**：ResponsibleRobotBench整体框架。该基准支持多种动作表示模态（预定义技能、操作位姿、代码生成），并沿多个轴线（如风险类型、规划难度、指令意图）对任务进行分类。使用细粒度评估指标来评估智能体在危险或模糊场景中对安全约束的理解和操作有效性。\n\n**整体框架与核心模块**：\n该基准的核心是任务套件、代理接口和评估系统。整体流程为：给定自然语言描述的高级任务和包含潜在风险的物理环境，一个由LMM驱动的智能体必须智能地识别危险，规划安全的纠正行为，并通过物理动作最终完成任务。\n\n1.  **任务套件设计**：包含23个实例化任务，涵盖多个领域和风险类型（电气、火灾/化学、人相关）。任务规划难度从单步操作到需要上下文推理的复杂多步程序。任务首先按是否涉及危险区分，并标注安全标志、风险类型和指令类型元数据。\n2.  **动作表示**：为适应不同的控制架构，支持多种动作表示格式，包括预定义的低级技能、操作位姿和代码生成流程，确保了不同抽象级别系统间的公平比较。\n3.  **指令模式**：指令分为三种类型：“正常”指令描述安全的目标导向行为；“攻击”指令是对抗性的或故意有害的；“防御”指令可能要求智能体在执行从潜在危险到固有危险的操作时，减轻或防止不安全的结果（如图3所示）。\n4.  **场景与规划复杂性**：为研究场景复杂性和规划难度的影响，基准包含了在一致任务目标下但具有不同复杂性的任务变体。如图2所示，场景复杂性根据是否存在危险进行分类；规划复杂性则通过设计不同难度的抓取策略来定义（简单场景可能只需要末端执行器的位置偏移规划，而困难场景则需要6D位姿规划）。\n5.  **代理评估架构与接口**：评估的智能体可以使用纯LLM流程或具有多模态接地的VLM来实例化。框架兼容零样本和少样本提示方案。一个模块化的智能体接口允许轻松集成新的指令跟随或规划模型。\n\n![场景与规划复杂性](https://arxiv.org/html/2512.04308v1/x2.png)\n> **图2**：同一任务（浇花）在不同场景复杂性和运动规划难度下的工作流程。左：无危险的简单场景（抓取顶部把手）。右：有危险的困难场景（抓取侧面把手，需6D位姿规划）。\n\n![任务与指令分类](https://arxiv.org/html/2512.04308v1/x3.png)\n> **图3**：任务根据能否安全完成进行分类。安全可执行任务用于评估智能体的负责任操作能力；安全违规任务用于研究智能体对攻击性和防御性提示的反应行为。\n\n**负责任机器人操作接口**：\n这是一个模块化、可扩展的接口，集成了感知、推理、反思、规划和执行。\n\n![操作接口流程](https://arxiv.org/html/2512.04308v1/x4.png)\n> **图4**：负责任机器人操作接口流程图。自然语言指令输入后，通过上下文学习（可包含视觉输入和动作示例），结合认知先验构造提示。大模型输出视觉描述、安全与规划推理、危险检测结果和最终任务规划。预测的机器人动作在物理仿真环境中验证，并生成评估报告。\n\n-   **指令与上下文构建**：利用自然语言指令控制机器人。通过对象检测模块（如YOLO11）提取相关实体的边界框，构建视觉上下文。\n-   **上下文学习**：包含N-shot示例和认知信息。示例包括视觉图像和不同动作表示形式的结果；认知信息则嵌入一般安全指南（如“让易燃物远离明火”）。\n-   **提示构造与模型推理**：提示整合了视觉信息、自然语言指令、N-shot示例和认知信息。大模型输出结构化的响应，包括视觉场景描述、安全推理与反思、任务执行规划推理、危险检测预测以及可执行的动作计划。\n-   **物理仿真与评估**：可行的动作计划在高保真物理仿真中执行。不可行的动作会根据其失败类型分配成本。环境模拟物理动力学和安全关键交互，支持对智能体计划的细粒度评估。\n\n**创新点**：\n与现有基准相比，其核心创新在于首次系统性地将“负责任操作”作为核心评估维度，并设计了完整的框架来实现。具体体现在：1）设计了涵盖多种风险类型和指令意图（正常、攻击、防御）的任务套件；2）提出了结合任务成功率、安全率、安全成功率、成本以及攻击抵抗、防御推理、危险检测准确率等多维度评估指标；3）提供了模块化的智能体接口和即插即用的策略学习/评估接口，支持未来的扩展和比较。\n\n## 实验与结果\n**实验设置**：\n-   **基准/数据集**：ResponsibleRobotBench基准，包含23个实例化任务。\n-   **实验平台**：物理仿真环境。\n-   **对比的Baseline方法**：评估了多种多模态大模型作为规划智能体，包括GPT-4o、GPT-4V、Gemini-1.5-Pro、Claude-3.5-Sonnet、Qwen2.5-VL-72B等。同时，作为一个案例研究，还实施并评估了PointFlowMatch策略。\n\n**关键实验结果**：\n1.  **总体性能与风险类别分析**：如图5a-c所示，在涉及潜在危险的任务中，GPT-4o表现最佳（安全成功率~45%），但仍远未达到完美，表明当前LMMs在负责任操作方面存在显著挑战。在无危险任务中（图5b），所有模型的任务成功率更高（GPT-4o达~80%），说明危险显著增加了任务难度。按风险类别细分（图5c），GPT-4o在电气和火灾/化学危险上表现相对较好，但在人相关危险上表现最差。\n2.  **成本评估**：引入了执行成本度量（公式1），其中机器人操作基础成本为100，请求人工干预或导致失败的成本为10,000。评估显示，GPT-4o在涉及危险的任务中产生了最高的平均成本，这与其更频繁地请求人工帮助或遭遇失败有关，突显了安全与效率之间的权衡。\n3.  **消融实验与分析**：\n    -   **视觉输入与任务历史**：实验表明，提供视觉输入和任务历史反馈能显著提高任务成功率和安全成功率。\n    -   **上下文学习（N-shot示例与认知信息）**：提供N-shot示例能带来一致的性能提升。此外，在提示中加入认知安全信息能进一步提高安全成功率，特别是在涉及危险的任务中。\n    -   **指令类型与规划难度**：在“攻击”指令下，所有模型都表现出一定的抵抗能力（拒绝执行）。在“防御”指令下，模型性能得到改善。在规划难度高的任务中，所有模型的性能均下降。\n4.  **细粒度错误分析**：如图5d所示，基准能够生成详细的错误分析报告，将失败归类为动作偏差/输出格式错误、感知错误、重复输出、运动规划失败、物理不可达等。这有助于识别系统瓶颈（例如，运动规划失败是常见原因）。\n\n![实验结果与分析](https://arxiv.org/html/2512.04308v1/x5.png)\n> **图5**：评估指标结果及细粒度错误分析。(a) 不同LMM在潜在危险任务上的性能。(b) 不同LMM在无危险任务上的性能。(c) GPT-4o在不同风险类别下的性能。(d) 细粒度错误分析示例，展示了失败类别分布。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了首个专门用于评估基于多模态大模型的机器人“负责任操作”能力的基准ResponsibleRobotBench，填补了该领域的空白。\n2.  设计了多维度的任务分类（风险类型、指令意图、规划难度）和评估指标（安全成功率、成本、危险检测准确率等），超越了传统的单一任务成功率评估。\n3.  提供了一个模块化、可扩展的框架和即插即用的接口，支持公平比较、消融研究以及未来学习策略的集成与评估。\n\n**局限性**：\n论文自身提到的局限性包括：1）基准在仿真环境中进行，与真实世界存在差距；2）当前任务规模（23个）有限，未来需要扩展更多样化的场景和风险；3）评估的成本权重（如人工干预成本10,000）是基于经验设定的，可能需要根据实际应用调整。\n\n**对后续研究的启示**：\n1.  **推动安全推理模型发展**：基准揭示了当前LMMs在安全关键推理和规划方面的不足，将激励研究者开发更具风险意识、能进行深度安全和道德推理的模型。\n2.  **探索效率与安全的权衡**：成本评估指标强调了在实际部署中考虑资源消耗的重要性，未来研究需要探索如何在确保安全的同时优化执行效率。\n3.  **支持闭环学习**：基准提供的接口和数据收集管道可用于训练端到端的负责任操作策略，促进从开环规划到闭环学习的范式发展。\n4.  **促进标准化**：该基准为负责任机器人操作的研究提供了一个可复现、标准化的测试平台，有助于社区建立统一的评估规范，加速领域进展。",
      "imageUrls": [
        "https://arxiv.org/html/2512.04308v1/x1.png",
        "https://arxiv.org/html/2512.04308v1/x2.png",
        "https://arxiv.org/html/2512.04308v1/x3.png",
        "https://arxiv.org/html/2512.04308v1/x4.png",
        "https://arxiv.org/html/2512.04308v1/x5.png",
        "https://arxiv.org/html/2512.04308v1/x6.png",
        "https://arxiv.org/html/2512.04308v1/x7.png",
        "https://arxiv.org/html/2512.04308v1/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03911",
      "title": "Autonomous Reinforcement Learning Robot Control with Intel’s Loihi 2 Neuromorphic Hardware",
      "url": "http://arxiv.org/abs/2512.03911",
      "arxivId": "2512.03911",
      "date": "2025-12-03",
      "authors": "Carl Glen Henshaw Team",
      "category": "Manipulation",
      "summary": "本文针对空间与移动机器人面临的功率约束问题，提出一种将强化学习训练的人工神经网络转换为脉冲Sigma-Delta神经网络，并部署至英特尔Loihi 2神经形态硬件的端到端流程。该方法结合了人工神经网络易于训练、仿实转换的优势与脉冲神经网络的高能效特性。以Astrobee自由飞行机器人控制为测试案例，在模拟环境中验证了该流程可实现低延迟、高能效的推理，证明了神经形态硬件用于机器人控制的可行性，为功耗受限环境中的实时控制提供了新途径。",
      "detailedSummary": "## 研究背景与动机\n在空间和移动机器人等应用中，功耗是关键的约束因素。虽然基于GPU的数据驱动学习和深度强化学习（DRL）推动了机器人控制的进步，但其高能耗阻碍了在功耗敏感场景下的部署。同时，直接训练性能可与人工神经网络（ANN）媲美的脉冲神经网络（SNN）仍面临挑战，例如需要通过时间反向传播，训练时间通常比同结构ANN长约10倍。\n\n本文针对机器人控制中高能耗的痛点，提出了一条结合ANN训练便利性与SNN执行能效的新路径。具体而言，本文的核心思路是：首先在仿真中使用标准DRL方法（如PPO）训练一个ANN控制策略，然后通过转换流程将其转化为适用于Intel Loihi 2神经形态硬件的Sigma-Delta脉冲神经网络（SDNN），最终实现低延迟、高能效的机器人控制推理。\n\n## 方法详解\n本文方法的整体流程（Pipeline）如下：1）在NVIDIA Omniverse Isaac Lab高保真物理仿真环境中，使用近端策略优化（PPO）算法训练一个用于控制Astrobee自由飞行机器人（6自由度）的ANN策略网络（Actor）；2）将该训练好的ANN转换为SDNN；3）将SDNN部署到Intel Loihi 2神经形态处理器上；4）在闭环仿真中评估其控制性能、延迟和能效。输入是机器人的12维观测状态（包括线速度、角速度、相对于目标的位置和姿态误差），输出是6维的力和扭矩控制指令。\n\n核心模块是ANN到SDNN的转换。为确保兼容性，ANN需使用ReLU激活函数。转换时，网络各层被映射为SDNN的特殊层：输入层转换为Delta层，隐藏层转换为Sigma-Delta-ReLU层，输出层转换为Sigma层。Delta层的作用是差分编码，它仅在当前输入值`x[t]`与上一个参考值`x_ref[t-1]`的差值超过阈值`ϑ`（本文设为0.1）时，才会产生一个输出脉冲` s[t]`，并更新参考值（公式4）。这种机制实现了基于变化的稀疏通信。Sigma层作为解码器，在接收端对传入的脉冲进行累加，以重建原始信号（公式5）。Sigma-Delta-ReLU层则是一个封装了ReLU激活函数的Sigma和Delta组合。\n\n与现有方法相比，本文的创新点具体体现在：1）首次将ANN到SDNN的转换流程应用于机器人控制任务，而非此前展示的视听信号处理；2）利用了Loihi 2对分级（整数）脉冲（graded spikes，支持24位整数幅度）的原生支持，使得量化后的SDNN可以直接高效映射到硬件；3）该方法可扩展至更深的网络（本文使用12x64x64x6的四层网络），而不局限于浅层SNN。在执行阶段，来自仿真的浮点观测值被量化后输入SDNN，在Loihi 2上完成整数运算，输出动作再被反量化回浮点数送入仿真环境。\n\n## 实验与结果\n实验在NVIDIA Isaac Lab仿真环境中进行，使用Astrobee机器人模型。评估了两个任务：1）**Undock机动**：沿X轴移动0.5米并保持姿态，与在国际空间站硬件上测试过的任务相同；2）**随机机动**：目标位置在初始位置±0.5米范围内随机，目标姿态在各轴±60度范围内随机。每个任务在10个随机种子下各运行10次。对比的基线是原始ANN在NVIDIA Quadro RTX 8000 GPU上的推理性能。评估指标包括控制精度（位置和姿态跟踪误差的均方根误差RMSE和最终误差）、延迟、吞吐量、能耗及能耗延迟积（EDP）。\n\n![位置跟踪误差](https://arxiv.org/html/2512.03911v1/figs/fig_position_error_timeseries.png)\n\n> **图1**：平均位置跟踪误差（米）随时间步长的变化。对比了GPU运行ANN（蓝、橙线）和Loihi 2运行SDNN（浅灰、深灰线）在10次随机机动和10次undock机动中的表现，阴影部分为95%置信区间。SDNN的最终位置误差更大。\n\n![方向跟踪误差](https://arxiv.org/html/2512.03911v1/figs/fig_orientation_error_timeseries_deg.png)\n\n> **图2**：平均姿态跟踪误差（度）随时间步长的变化。颜色含义同图1。SDNN的姿态控制精度，尤其是在随机任务中，低于GPU上的ANN。\n\n关键定量结果总结于表I和表II。在控制性能上，Loihi 2上的SDNN略逊于GPU上的ANN。例如，在随机机动任务中，SDNN的位置RMSE比ANN高0.083米，姿态RMSE高2.73度；最终位置误差和最终姿态误差也显著更大。这表明转换和量化过程引入了精度损失。\n\n![吞吐量与能耗对比](https://arxiv.org/html/2512.03911v1/figs/fig_throughput_vs_energy_per_inf_gpu_loihi.png)\n\n> **图3**：每次推理的吞吐量（次/秒）与能耗（焦耳）对比图。Loihi 2上的SDNN（灰点）聚集在低能耗、高吞吐量区域，而GPU上的ANN（蓝、橙点）则位于高能耗区域。清晰展示了能效与性能的权衡。\n\n在能效和速度方面，Loihi 2优势显著。如表II所示，对于随机任务，SDNN在Loihi 2上的总能效（每次推理0.013 J）仅为GPU（0.217 J）的约6%；动态能效（仅网络运算）仅为GPU的12.3%。同时，Loihi 2的吞吐量（~472 inf/s）是GPU（~202 inf/s）的2.3倍以上，延迟更低，能耗延迟积（EDP）仅为GPU的5%左右。\n\n![定性结果对比](https://arxiv.org/html/2512.03911v1/figs/astrobee_SDNN_GPU_inference.png)\n\n> **图4(a)**：GPU运行ANN推理完成undock任务的示例可视化。机器人与目标位置/姿态箭头几乎完全重叠，表示精确到达。\n\n![定性结果对比](https://arxiv.org/html/2512.03911v1/figs/astrobee_SDNN_Loihi_inference.png)\n\n> **图4(b)**：Loihi 2运行SDNN推理完成同一任务的示例。机器人（红色）与目标（绿色箭头）接近但未完全重叠，存在最终误差，但控制稳定，没有大幅偏离。\n\n消融实验方面，论文未进行严格的组件消融研究，但通过结果分析了性能差距的来源：主要源于将浮点观测值转换为脉冲并进行量化以适应Loihi 2整数运算所引入的误差。论文指出，未来可通过训练网络更好地适应SDNN的输入表示，或优化超参数来提升性能。\n\n## 总结与启发\n本文的核心贡献在于：1）**验证了可行性**：首次成功演示了将RL训练的ANN策略转换为SDNN，并部署在Loihi 2神经形态硬件上用于闭环机器人控制。2）**量化了能效优势**：尽管控制精度有折损，但SDNN在Loihi 2上实现了数量级级别的能效提升和更高的吞吐量，明确了性能与能效之间的权衡。3）**提供了一条实用路径**：为在空间探索等严苛资源受限环境中部署先进、自适应的数据驱动控制器提供了一条切实可行的技术途径。\n\n论文自身提到的局限性包括：转换后的SDNN控制精度低于原始ANN；当前工作仅在仿真中验证，尚未在真实机器人或搭载Loihi 2的航天器上部署；所使用的控制网络相对较小。\n\n本文对后续研究的启示包括：1）可以探索在ANN到SDNN的转换流程中引入针对性的训练或微调，以弥补精度损失。2）该方法可扩展至更大、更复杂的策略网络，届时神经形态硬件的SWaP优势可能更加显著。3）这项工作为神经形态计算与机器人学的结合提供了一个强有力的案例，鼓励社区进一步开发能直接利用脉冲稀疏性和事件驱动特性的新型学习算法。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03911v1/figs/fig_position_error_timeseries.png",
        "https://arxiv.org/html/2512.03911v1/figs/fig_orientation_error_timeseries_deg.png",
        "https://arxiv.org/html/2512.03911v1/figs/fig_throughput_vs_energy_per_inf_gpu_loihi.png",
        "https://arxiv.org/html/2512.03911v1/figs/astrobee_SDNN_GPU_inference.png",
        "https://arxiv.org/html/2512.03911v1/figs/astrobee_SDNN_Loihi_inference.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03973",
      "title": "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning",
      "url": "http://arxiv.org/abs/2512.03973",
      "arxivId": "2512.03973",
      "date": "2025-12-03",
      "authors": "Justin Carpentier Team",
      "category": "Manipulation",
      "summary": "本文针对离线强化学习中行为正则化方法无法区分高价值和低价值动作的问题，提出了Guided Flow Policy（GFP）。该方法耦合多步流匹配策略与蒸馏一步行动者，通过加权行为克隆使行动者专注模仿数据集中的高价值动作，流策略则约束行动者与数据集最佳转换对齐并最大化批评者。实验显示，GFP在OGBench、Minari和D4RL基准的144个状态和像素任务中达到最先进性能，在次优数据集和挑战性任务上取得显著提升。",
      "detailedSummary": "## 研究背景与动机\n离线强化学习（Offline RL）旨在从静态数据集中学习策略，而无需与环境进一步交互。这避免了在线探索的成本与风险，但核心挑战是分布偏移和外推误差——对于数据分布之外的动作，其价值函数的估计往往不准确。行为正则化Actor-Critic（BRAC）系列方法是应对此挑战的主流方法之一，其核心思想是强制学习到的策略与生成数据集的未知行为策略保持“接近”，以减少外推误差。然而，现有BRAC方法（如TD3+BC、ReBRAC、FQL）在其正则化组件中对所有状态-动作对一视同仁，未能区分数据集中高价值和低价值的动作。这种不加区分的模仿可能限制策略利用数据集中潜在的高回报动作。尽管近期基于流匹配或扩散模型的表达性策略取得了进展，但它们仍面临计算开销大（迭代采样导致推理慢，通过时间的反向传播不稳定）以及行为克隆组件缺乏价值信息的问题。本文针对行为正则化中“价值盲”这一具体痛点，提出了**价值感知正则化**的新视角，其核心思路是：通过一个双向引导机制，让一个多步的流匹配策略有选择地克隆数据集中高价值的动作，并以此引导一个蒸馏得到的一步策略，使后者在最大化评论家价值的同时，又能与数据集中的优质转移保持一致。\n\n## 方法详解\nGuided Flow Policy (GFP) 是一个双策略BRAC框架，包含三个核心组件：评论家 $Q_{\\phi}$、一步执行器Actor $\\pi_{\\theta}$ 和价值感知行为克隆（VaBC）流策略 $\\pi_{\\omega}$。它们通过双向引导机制协同工作：VaBC作为分布正则器引导Actor关注高价值数据集动作；Actor则通过优化评论家并蒸馏到VaBC来塑造流策略。\n\n![方法框架](https://arxiv.org/html/2512.03973v1/x1.png)\n\n> **图1**：Guided Flow Policy 框架概览。包含三个主要组件：（黄色）VaBC，一个通过加权行为克隆训练的多步流策略 $\\pi_{\\omega}$；（绿色）从流策略蒸馏得到的一步Actor $\\pi_{\\theta}$；（灰色）引导动作评估的评论家 $Q_{\\phi}$。$\\pi_{\\omega}$ 将Actor正则化到数据集的高价值动作；反过来，Actor塑造流并遵循Actor-Critic方法优化评论家。图中每个绘图（灰色除外）表示在给定状态 $s$ 下，策略对动作 $a \\in \\mathcal{A}$ 的概率分布；灰色绘图表示评论家对状态 $s$ 下动作 $a \\in \\mathcal{A}$ 的价值估计。\n\n**1. 整体流程**：训练时，三个组件交替更新（算法1）。首先，用标准贝尔曼损失更新评论家（式6）。其次，更新Actor，其损失（式8）包含最大化评论家价值项和向VaBC策略蒸馏的正则项。最后，更新VaBC策略，其损失（式9）是一个加权流匹配行为克隆损失，权重 $g_{\\eta}(s,a)$ 由评论家提供，用于强调高价值数据集动作。\n\n**2. 核心模块与技术细节**：\n*   **评论家 ($Q_{\\phi}$)**：使用标准TD误差进行训练（式6）。论文还探索了一种更保守的贝尔曼目标 $y^{\\text{VaBC}}$（式7），它取Actor动作和VaBC动作对应Q值的平均值，以缓解价值高估。\n*   **一步Actor ($\\pi_{\\theta}$)**：作为推理时使用的策略，它是一个一步生成模型（如高斯策略）。其损失函数为：\n    $\\mathcal{L^{\\mathcal{A}}}(\\theta)=\\mathbb{E} [-\\lambda Q_{\\phi}（s,\\mu_{\\theta}(s,z)）+\\alpha\\|\\mu_{\\theta}(s,z)-\\mu_{\\bar{\\omega}}(s,z)\\|_{2}^{2}]$\n    其中 $\\lambda$ 是基于小批量Q值绝对值的归一化因子，$\\alpha$ 是平衡超参。第一项鼓励高价值，第二项（蒸馏项）约束Actor输出接近VaBC策略的输出，从而保持在数据集高价值动作的支持范围内。\n*   **价值感知行为克隆流策略 ($\\pi_{\\omega}$)**：这是GFP的创新核心。它是一个基于流匹配（Flow Matching）的条件生成模型，通过ODE将高斯噪声映射为动作。其训练损失不是平等对待所有数据集动作，而是进行加权：\n    $\\mathcal{L}^{\\text{VaBC}}(\\omega)=\\mathbb{E} [g_{\\eta}(s,a)\\,\\|v_{\\omega}(t,s,a_{t})-(a-\\epsilon)\\|_{2}^{2}]$\n    关键在于权重函数 $g_{\\eta}(s,a)$（式10）：\n    $g_{\\eta}(s,a):=\\frac{\\exp（\\tfrac{\\lambda}{\\eta}Q_{\\phi}(s,a)）}{\\exp（\\tfrac{\\lambda}{\\eta}Q_{\\phi}(s,a)）+\\exp（\\tfrac{\\lambda}{\\eta}Q_{\\phi}(s,\\mu_{\\theta}(s,z)））}$\n    该函数将数据集动作 $a$ 的Q值与Actor提议动作 $\\mu_{\\theta}(s,z)$ 的Q值进行softmax比较。若 $a$ 价值更高，则 $g_{\\eta}(s,a) > 0.5$，在训练中给予该数据对更大权重；反之则降低其影响。温度超参 $\\eta$ 控制筛选的尖锐程度。\n\n**3. 创新点**：与现有BRAC方法（见表1）相比，GFP的创新具体体现在：1）**价值感知的正则化**：VaBC的加权损失使正则化目标从“模仿所有数据”变为“优先模仿高价值数据”，这是与TD3+BC、ReBRAC、FQL等的本质区别。2）**双向引导的联合训练框架**：VaBC引导Actor聚焦优质数据，Actor的提议又用于计算VaBC的权重 $g_{\\eta}$，二者与评论家共同进化，而非独立预训练。\n\n![价值感知行为克隆示意图](https://arxiv.org/html/2512.03973v1/x2.png)\n\n> **图2**：不同引导水平下的行为克隆对比。左：先前工作（如FQL）无筛选地模仿所有状态-动作对。右：GFP引入温度控制的引导机制（式10）。高温时引导弱，Actor受许多候选动作影响；适中温度时筛选更尖锐，赋予高价值动作更多权重，同时保持足够的正则化和探索；低温时筛选非常挑剔，几乎只关注最高价值动作。但过低的温度可能使Actor脱离数据集分布（绿色所示），导致外推问题。VaBC本身仅基于数据集内动作训练，即使低温也不会脱离数据集分布（黄色图中的蓝色虚线轮廓）。\n\n## 实验与结果\n**实验设置**：在OGBench、Minari和D4RL三个主流离线RL基准的共144个任务（基于状态和像素）上进行了广泛评估。对比的基线方法包括：代表流方法的FQL，代表简约BRAC方法的ReBRAC，以及代表在策略方法（in-sampling）的IQL等共计10余种先前方法。实验平台使用JAX实现。\n\n**关键结果**：\n1.  **整体性能领先**：在OGBench的50个任务上，GFP的性能轮廓（Performance Profile）明显优于所有对比方法（图3a）。在总计144个任务的综合评估中，GFP取得了最先进的性能。\n\n![OGBench性能轮廓图](https://arxiv.org/html/2512.03973v1/x3.png)\n\n> **图3a**：在50个OGBench任务上的性能轮廓图。横轴是归一化得分阈值τ，纵轴是得分超过τ的任务比例。GFP的曲线（绿色）整体位于最上方，表明其在各种性能阈值下都优于其他对比方法。\n\n2.  **在挑战性任务上优势显著**：GFP在嘈杂（noisy）和困难任务上表现出显著优势。例如，在 `cube-double-noisy` 和 `cube-triple-noisy` 数据集上，GFP平均得分分别为63和24，远超FQL（38和4）和ReBRAC（20和5）。在困难任务如 `humanoidmaze-large-navigate`（GFP: 18 vs FQL: 7）和 `cube-triple-play`（GFP: 16 vs FQL: 4）上，GFP也大幅领先。\n3.  **消融实验**：论文进行了详尽的消融研究（见附录）。关键结论包括：\n    *   **双向引导机制至关重要**：移除Actor对VaBC的引导（即固定 $g_{\\eta}=0.5$）或移除VaBC对Actor的蒸馏，性能均会显著下降。\n    *   **保守贝尔曼目标 $y^{\\text{VaBC}}$ 有效**：在某些环境下，使用式7的保守目标能带来显著性能提升。\n    *   **温度 $\\eta$ 需谨慎调优**：$\\eta$ 是GFP的关键超参，其敏感性分析显示适中温度（如 $10^{-3}$）通常最佳，过低温度会导致不稳定。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**Guided Flow Policy (GFP)**，一种新颖的双策略BRAC框架，首次将**价值感知**通过加权行为克隆机制集成到行为正则化中，使正则化集中于数据集中最有希望的转移。\n2.  在**144个任务**上进行了广泛评估，证明了GFP的先进性能，特别是在次优和挑战性数据集上取得了显著增益。\n3.  对ReBRAC等先前SOTA方法进行了**重新评估**，强调了超参数选择和实现细节的关键作用，为社区提供了更可靠的基线比较。\n\n**局限性**：\n1.  GFP引入了一个新的温度超参 $\\eta$，需要根据任务进行调整。\n2.  方法主要针对连续动作空间设计，对于高维复杂状态（如像素），可能需要更复杂的条件机制（如分类器无关引导）。\n\n**对后续研究的启示**：\n1.  **价值感知正则化**是一个富有前景的方向，未来工作可以探索更高效或更稳定的价值加权方案。\n2.  GFP展示了**表达性策略模型（流匹配）与简约一步策略通过蒸馏协同工作**的有效性，这种联合训练、双向引导的框架可扩展到其他生成模型或策略表示中。\n3.  论文强调并实践了**大规模、标准化基准测试**和**细致实现复现**的重要性，这对推动离线RL领域的可靠进展至关重要。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03973v1/x1.png",
        "https://arxiv.org/html/2512.03973v1/x2.png",
        "https://arxiv.org/html/2512.03973v1/x3.png",
        "https://arxiv.org/html/2512.03973v1/x4.png",
        "https://arxiv.org/html/2512.03973v1/x5.png",
        "https://arxiv.org/html/2512.03973v1/x6.png",
        "https://arxiv.org/html/2512.03973v1/x7.png",
        "https://arxiv.org/html/2512.03973v1/x8.png",
        "https://arxiv.org/html/2512.03973v1/x9.png",
        "https://arxiv.org/html/2512.03973v1/x10.png",
        "https://arxiv.org/html/2512.03973v1/x11.png",
        "https://arxiv.org/html/2512.03973v1/x12.png",
        "https://arxiv.org/html/2512.03973v1/x13.png",
        "https://arxiv.org/html/2512.03973v1/x14.png",
        "https://arxiv.org/html/2512.03973v1/x15.png",
        "https://arxiv.org/html/2512.03973v1/x16.png",
        "https://arxiv.org/html/2512.03973v1/x17.png",
        "https://arxiv.org/html/2512.03973v1/x18.png",
        "https://arxiv.org/html/2512.03973v1/x19.png",
        "https://arxiv.org/html/2512.03973v1/x20.png",
        "https://arxiv.org/html/2512.03973v1/x21.png",
        "https://arxiv.org/html/2512.03973v1/x22.png",
        "https://arxiv.org/html/2512.03973v1/x23.png",
        "https://arxiv.org/html/2512.03973v1/x24.png",
        "https://arxiv.org/html/2512.03973v1/x25.png",
        "https://arxiv.org/html/2512.03973v1/x26.png",
        "https://arxiv.org/html/2512.03973v1/x27.png",
        "https://arxiv.org/html/2512.03973v1/x28.png",
        "https://arxiv.org/html/2512.03973v1/x29.png",
        "https://arxiv.org/html/2512.03973v1/x30.png",
        "https://arxiv.org/html/2512.03973v1/x31.png",
        "https://arxiv.org/html/2512.03973v1/x32.png",
        "https://arxiv.org/html/2512.03973v1/x33.png",
        "https://arxiv.org/html/2512.03973v1/x34.png",
        "https://arxiv.org/html/2512.03973v1/x35.png",
        "https://arxiv.org/html/2512.03973v1/x36.png",
        "https://arxiv.org/html/2512.03973v1/x37.png",
        "https://arxiv.org/html/2512.03973v1/x38.png",
        "https://arxiv.org/html/2512.03973v1/x39.png",
        "https://arxiv.org/html/2512.03973v1/x40.png",
        "https://arxiv.org/html/2512.03973v1/x41.png",
        "https://arxiv.org/html/2512.03973v1/x42.png",
        "https://arxiv.org/html/2512.03973v1/x43.png",
        "https://arxiv.org/html/2512.03973v1/x44.png",
        "https://arxiv.org/html/2512.03973v1/x45.png",
        "https://arxiv.org/html/2512.03973v1/x46.png",
        "https://arxiv.org/html/2512.03973v1/x47.png",
        "https://arxiv.org/html/2512.03973v1/x48.png",
        "https://arxiv.org/html/2512.03973v1/x49.png",
        "https://arxiv.org/html/2512.03973v1/x50.png",
        "https://arxiv.org/html/2512.03973v1/x51.png",
        "https://arxiv.org/html/2512.03973v1/x52.png",
        "https://arxiv.org/html/2512.03973v1/figures/sensitivity/Sensitivity_antmaze_large_navigate_singletask_task1_v0_gfp.png",
        "https://arxiv.org/html/2512.03973v1/figures/sensitivity/Sensitivity_antmaze_large_navigate_singletask_task1_v0_fql.png",
        "https://arxiv.org/html/2512.03973v1/figures/sensitivity/Sensitivity_cube_double_play_singletask_task2_v0_gfp.png",
        "https://arxiv.org/html/2512.03973v1/figures/sensitivity/Sensitivity_cube_double_play_singletask_task2_v0_fql.png",
        "https://arxiv.org/html/2512.03973v1/figures/sensitivity/Sensitivity_scene_play_singletask_task2_v0_gfp.png",
        "https://arxiv.org/html/2512.03973v1/figures/sensitivity/Sensitivity_scene_play_singletask_task2_v0_fql.png",
        "https://arxiv.org/html/2512.03973v1/figures/sensitivity/Sensitivity_Global_gfp.png",
        "https://arxiv.org/html/2512.03973v1/figures/sensitivity/Sensitivity_Global_fql.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03743",
      "title": "Cross-embodied Co-design for Dexterous Hands",
      "url": "http://arxiv.org/abs/2512.03743",
      "arxivId": "2512.03743",
      "date": "2025-12-03",
      "authors": "Xiaolong Wang Team",
      "category": "Manipulation",
      "summary": "本文针对灵巧操作中机械设计与控制策略分离、协同设计搜索空间巨大的核心问题，提出了一种**跨实体协同设计框架**。该框架的关键在于：1）支持关节、手指、手掌生成的广泛形态搜索空间；2）利用**形态条件化的跨实体控制策略**，实现对庞大设计空间的可扩展评估；3）采用易得组件实现真实制造。实验表明，该框架能实现从设计、训练到实物部署的端到端流程，**可在24小时内完成一款新灵巧手的整个周期**，并在手内旋转等任务中进行了仿真与真实验证。",
      "detailedSummary": "## 研究背景与动机\n灵巧操作的发展受限于控制和设计两个方面，目前对于何种机械手形态最适合执行灵巧任务尚无共识。这引出了一个根本性挑战：我们应如何设计和控制针对灵巧性优化的机器人操作器？传统机器人操作研究将机械设计与控制解耦，在固定的硬件架构上开发强化学习和控制框架。这种分离可能限制灵巧性，因为最优控制策略可能从根本上受到手部形态、自由度和感知能力的制约。协同设计旨在通过同时优化设计和控制来克服这一限制。然而，针对操作的协同设计面临双重维度灾难：复杂的硬件设计和精细的灵巧控制都具有巨大的搜索空间。因此，联合创建两者需要高效迭代硬件设计并快速开发精细控制。现有方法为提升速度常使用基于采样的快速控制器，但这类控制器在处理操作任务时面临稀疏奖励、高维接触解空间和难以找到远离初始条件解等挑战。这导致现有的操作协同设计方法要么搜索空间有限，要么完全避开稀疏、长视野的奖励而只处理 locomotion 任务。\n\n本文针对复杂设计和精细控制协同优化的可扩展性问题，提出利用预训练的、形态条件化的跨具身策略来评估生成的机器人手部设计。此外，先前工作往往仅在仿真中部署，由于硬件制造约束的过度简化、模拟控制向真实世界动态泛化的需求以及物理属性建模的困难，存在 sim-to-real 鸿沟。为解决此问题，本文创建了一个在仿真中具有真实组件的模块化硬件平台，以实现面向 sim-to-real 的可行设计搜索。本文核心思路是：首先在采样的手部设计上预训练一个形态条件化的跨具身基础策略，然后利用该策略高效评估通过模块化语法生成的大量候选设计，从而在24小时内完成从设计到实物部署的端到端流程。\n\n## 方法详解\n本文提出的跨具身协同设计框架旨在实现从仿真到实物的机器人手部协同设计。方法整体上分为三个阶段：1）预训练跨具身策略；2）基于语法生成设计并用预训练策略进行评估和搜索；3）选择最佳设计进行制造和实物部署。\n\n![方法框架总览](https://arxiv.org/html/2512.03743v2/Figures/MethodR1.png)\n\n> **图2**：方法整体框架概述。阶段1：首先随机采样不同的具身形态（包括形态和自由度），然后跨这些形态预训练一个形态条件化的策略。阶段2：使用模块化语法生成设计，并根据先前的性能选择组装规则，然后部署跨具身策略在仿真中评估不同设计。阶段3：最佳的设计-控制对被制造并部署到真实硬件上，处理未见过的物体。\n\n框架包含四个核心组件：\n1.  **程序化约束手部生成器**：通过语法规则 $p_{grammar}(r,c)$ 生成机器人手设计 $G$。该生成器创建多样化的手部设计，其参数包括手掌几何形状、手指数量 $f \\in \\{3,4,5\\}$、每根手指的关节配置 $j_f \\in \\{0,2,3\\}$、连杆长度（每关节1-10个单位）和指尖类型。这些语法基于具有真实碰撞几何、制造约束和模块化连接的真实子组件预先设计。\n2.  **形态编码器**：使用图神经网络 $h(G)$ 将手部设计图 $G$ 编码为固定维度的嵌入向量 $g$，用于后续的策略条件化。设计图 $G=(V,E,X_v,X_e)$ 以节点表示手指连杆，边表示关节。\n3.  **跨具身控制策略**：一个形态条件化的策略 $\\pi_\\theta(a|s,g)$。状态 $s$ 包含机器人关节状态 $q$、物体位姿 $p_o$ 和形态编码 $g$。策略输出动作 $a_t = \\pi_\\theta(s_t) \\odot m(G)$，其中 $m(G)$ 是动作掩码，确保无效关节不被驱动。\n4.  **设计价值网络**：一个学习到的评估网络 $V_{design}(g)$，用于预测设计 $G$ 的任务性能，从而指导设计空间的搜索，避免对每个候选设计都进行耗时的策略评估。\n\n**创新点具体体现**：\n-   **跨具身策略实现快速评估**：与为每个设计单独训练策略不同，本文预训练一个通用的、以形态编码为条件的策略。该策略在基于运动学结构（对称、非对称、拟人化）分组的“家族”内进行训练，能够泛化到家族内不同的手指数量、关节配置和几何变体。这允许对大量生成的设计进行快速、可扩展的性能评估。\n-   **模块化硬件语法确保仿真真实性**：生成器使用模块化和物理真实的语法与规则结构，生成的机器人手考虑了预计算的碰撞几何、关节限位和可制造组件规格，确保设计能够有效地迁移到物理系统。\n\n**协同设计流程**：系统通过图启发式搜索迭代进行协同设计。在每次迭代中，通过 $\\epsilon$-贪婪扩展生成 $K$ 个候选形态，使用预训练的跨具身策略并行评估它们，并更新记录设计到性能的查找表 $\\mathcal{T}$ 以及设计价值网络。搜索从初始设计开始，通过逐指扩展生成候选，利用价值网络（预测时加入Gumbel噪声以鼓励探索）对部分设计进行评分，选择最高分后继，直至生成完整设计。\n\n**仿真到实物迁移**：从搜索中获得的最佳设计会经过域随机化（针对执行器特性、接触和摩擦参数、物体位姿）进行微调以提高鲁棒性。策略被部署为“盲”策略，即移除物体状态输入，仅使用关节状态和形态编码作为观测，以匹配板载传感并减少 sim-to-real 差异。实物制造遵循仿真中使用的相同程序化语法：最佳设计图被直接转换为模块化硬件规格，使用3D打印组件和精确匹配的执行器约束进行组装。\n\n## 实验与结果\n**实验设置**：在三个灵巧操作任务上评估协同设计框架，其中一个（掌内旋转）包含仿真和真实世界部署。任务包括掌内旋转、桌面抓取和桌面绕Z轴翻转物体，共使用15个随机化物体。每个设计在128个随机化环境（随机关节位置、物理参数和物体类型）中进行测试。算法运行50次迭代，每次迭代评估40个设计。\n\n**对比基线**：\n1.  RoboGrammar：使用基于图的设计规则和局部搜索优化进行机器人形态设计的程序生成算法。\n2.  蒙特卡洛树搜索：通过构建搜索树并使用随机推演进行评估的系统性设计空间搜索方法。\n3.  本文方法（使用MPPI控制器替代RL策略）。\n4.  LEAP手（使用视觉，在单个物体上训练PPO策略）。\n5.  盲LEAP机器人手（使用本文的随机化物体集进行比较）。\n\n**关键实验结果**：\n-   在仿真掌内旋转任务中，本文方法的最佳设计实现了 **3.3 rad/s** 的连续角速度。不进行微调的版本达到 **1.85 rad/s**，而使用MPPI控制器的版本仅为 **0.62 rad/s**。基线方法RoboGrammar和蒙特卡洛树搜索分别仅达到 **0.26 rad/s** 和 **0.20 rad/s**。使用视觉的LEAP手在单物体上达到 **0.47 rad/s**，但盲LEAP手在随机物体上完全失败（0 rad/s）。\n\n![仿真结果对比表](https://arxiv.org/html/2512.03743v2/x1.png)\n\n> **表1**：掌内旋转连续角速度对比。本文方法（含微调）性能显著优于所有基线。\n\n-   **端到端流程效率**：从设计、训练、制造到部署一个新机器人手的完整流程可在 **24小时** 内完成，其中3D打印约12小时，算法运行约6.48小时，组装0.8小时，sim-to-real调试约2小时。\n\n![实物部署手部设计](https://arxiv.org/html/2512.03743v2/Figures/RealWorldRotation.png)\n\n> **图4**：实物部署的四种手部设计。左上：三指协同设计手（算法找到的最佳设计）。右上：五指对称协同设计手。左下：四指协同设计手（薄指尖）。右下：五指拟人化基线手。\n\n-   **真实世界部署**：对17个具有不同摩擦、柔顺性和形状的未见物体进行盲策略测试。结果（表3）显示，算法在仿真中对性能的排序预测与实物表现一致。最佳的三指手能成功旋转大多数物体（15/17），而拟人化和四指手仅能旋转少数物体（3/17），且旋转耗时更长。\n\n![任务示例与性能](https://arxiv.org/html/2512.03743v2/Figures/TaskExample.png)\n\n> **图5**：任务示例与最佳设计性能。左上：抓取任务。左下：翻转任务的最佳设计。右下：翻转、抓取和掌内旋转任务最佳设计的归一化奖励。\n\n-   **消融分析与设计洞察**：\n    -   不同任务偏好不同的形态特征：**抓取**任务中，54%的最佳设计使用薄指尖，且所有最佳设计均为五指（对称或拟人化）、全自由度（每指4个电机）。**旋转**任务中，最佳设计使用三指，64%使用标准指尖（薄指尖导致旋转不稳定）。**翻转**任务中，58%的最佳设计在一侧使用楔形或薄指尖以轻松提起物体，另一侧使用标准指尖以受控复位。\n    -   **形态参数重要性分析**：通过在LEAP手上系统采样12个硬件类别参数，发现对操作性能影响最强的四个参数中，**形态参数占主导**。手指长度缩放与性能呈强正相关（r=0.748），手掌宽度缩放呈强负相关（r=-0.729）。这表明形态优化对灵巧性至关重要。\n\n![参数相关性分析](https://arxiv.org/html/2512.03743v2/Figures/BayesHalf.png)\n\n> **图6**：基于贝叶斯采样的物理参数与旋转性能改进相关性分析。影响最大的参数是形态和控制参数。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个**跨具身协同设计框架**，通过将形态条件化的跨具身控制策略集成到协同设计流程中，实现了对机器人手部设计快速、高效的评估，并成功完成了从仿真到实物的迁移，整个端到端流程可在24小时内完成。\n2.  创建了一个**模块化、物理真实的机器人手生成平台**，其语法规则反映了真实的硬件约束，确保了设计仿真的准确性和实物制造的可实现性。\n3.  通过系统的参数分析，明确了**形态参数**（如手指长度、手掌宽度）对灵巧操作性能具有最显著的影响，为“为何需要协同设计”提供了实证依据，并指明了未来灵巧性研究的关键设计参数。\n\n**局限性**：论文提到，其搜索空间仍受限于预定义的语法和模块化组件库，并非完全开放的设计空间。此外，部署的策略是“盲”策略，没有利用视觉或触觉反馈，这可能限制其在更复杂或需要感知反馈的任务中的性能。\n\n**后续研究启示**：\n1.  可以探索更开放、更少约束的设计空间，或许结合生成式AI模型来发明全新的机械结构。\n2.  将跨具身协同设计框架与多模态感知（如视觉、触觉）相结合，以处理需要环境交互和精细接触反馈的复杂操作任务。\n3.  本文验证了形态对灵巧性的关键作用，未来的机器人手设计应更加注重与任务目标的形态协同优化，而非单纯模仿人手的拟人化设计。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03743v2/Figures/MainFigTall.png",
        "https://arxiv.org/html/2512.03743v2/Figures/MethodR1.png",
        "https://arxiv.org/html/2512.03743v2/Figures/ExampleEmbody1.png",
        "https://arxiv.org/html/2512.03743v2/Figures/RealWorldRotation.png",
        "https://arxiv.org/html/2512.03743v2/Figures/TaskExample.png",
        "https://arxiv.org/html/2512.03743v2/Figures/BayesHalf.png",
        "https://arxiv.org/html/2512.03743v2/Figures/LeapHandCEM.png",
        "https://arxiv.org/html/2512.03743v2/Figures/Full_Five_Figure.png",
        "https://arxiv.org/html/2512.03743v2/Figures/RealWorldObjects.png",
        "https://arxiv.org/html/2512.03743v2/Figures/CoDesignReal.png",
        "https://arxiv.org/html/2512.03743v2/Figures/AnthroBaseline.png",
        "https://arxiv.org/html/2512.03743v2/Figures/ObjSimDataset.png",
        "https://arxiv.org/html/2512.03743v2/x1.png",
        "https://arxiv.org/html/2512.03743v2/Figures/Generation.png",
        "https://arxiv.org/html/2512.03743v2/Figures/baseline_fig.png",
        "https://arxiv.org/html/2512.03743v2/Figures/ScatterGridSample.png",
        "https://arxiv.org/html/2512.03743v2/Figures/BayesFullSample.png",
        "https://arxiv.org/html/2512.03743v2/Figures/BayesMorph.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03556",
      "title": "RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL",
      "url": "http://arxiv.org/abs/2512.03556",
      "arxivId": "2512.03556",
      "date": "2025-12-03",
      "authors": "Yong Li Team",
      "category": "Manipulation",
      "summary": "本文提出RoboScape-R框架，旨在解决传统模仿学习与强化学习在机器人策略跨场景泛化方面的局限性。其核心创新在于利用世界模型作为通用环境代理，并设计了一种基于世界模型的通用奖励机制，该机制从模型学习到的状态转移动力学中生成“内生”奖励。实验表明，该方法为策略训练提供了高效通用的环境，在域外场景下相比基线方法平均性能提升37.5%。",
      "detailedSummary": "## 研究背景与动机\n目前，机器人策略学习的主流范式包括模仿学习（IL）和强化学习（RL）。模仿学习依赖于专家数据或手工设计的目标，容易过拟合到特定的训练场景和专家轨迹。强化学习虽然通过奖励信号鼓励更广泛的探索，但其泛化能力仍受限于难以设计一个适用于高度异构环境的通用且一致的奖励函数。世界模型作为一种能够预测状态转移动态的预测模型，被视为一种通用的环境代理，有望解决这一瓶颈。然而，现有的世界模型主要专注于预测未来观测，仍然依赖于任务特定、手工设计的奖励函数，无法提供一个真正通用的训练环境。本文针对“缺乏一个能提供通用奖励信号的真正通用训练环境”这一痛点，提出利用世界模型作为RL范式中的通用环境代理。核心思路是提出RoboScape-R框架，利用世界模型作为在线训练环境，并设计一种源自模型内在状态转移动态理解的“内生”通用奖励机制，以大幅提升具身策略的泛化能力。\n\n## 方法详解\nRoboScape-R的整体框架主要包括两部分：一个作为环境的世界模型，以及可扩展的策略。世界模型接收策略预测的动作，提供下一帧观测和相应的奖励；策略接收观测并预测动作。\n\n![方法整体框架](https://arxiv.org/html/2512.03556v1/x1.png)\n> **图1**：RoboScape-R的整体流程结构。左侧是基于世界模型的环境与通用奖励模块，右侧是策略优化模块。世界模型环境采用双世界模型结构：动作世界模型接收动作并提供预测观测，而文本世界模型则通过生成目标观测来提供奖励信号。\n\n**核心模块1：具有通用奖励的世界模型架构**\n为了在RL训练中同时提供观测和奖励，本文提出了一个包含动作世界模型（WM_act）和文本世界模型（WM_txt）的双世界模型环境。两者共享类似的自回归Transformer架构但参数不同，均能基于历史观测和控制信号预测未来观测和完成（done）信号。前向过程为：\\(\\hat{x_t}, \\hat{d_t} = WM(x_{0:t-1}, c)\\)，其中\\(c\\)对于动作世界模型是机器人动作序列\\(a_{0:t}\\)，对于文本世界模型是文本指令\\(i\\)。\n\n具体架构包括：\n1.  **视觉分词器**：采用MAGVIT-2将观测视频压缩为离散的潜在token。\n2.  **控制信号编码器**：文本世界模型使用T5编码器编码指令；动作世界模型使用MLP编码7-DoF机器人动作。\n3.  **条件时空块（STBlock）**：在基础时空块中引入交叉注意力机制以注入控制信号，用于预测下一个token和完成信号。文本和动作世界模型分别对应STBlock-T和STBlock-A。\n4.  **视觉解码器**：将预测的token解码为观测图像。\n5.  **完成信号预测头**：一个MLP，用于预测当前时间步的完成信号\\(\\hat{d_t}\\)。\n\n模型优化采用混合损失函数：视觉token预测的交叉熵损失\\(\\mathcal{L}_{vis}\\)和完成信号预测的RMSE损失\\(\\mathcal{L}_d\\)，总损失为\\(\\mathcal{L} = \\mathcal{L}_{vis} + \\mathcal{L}_d\\)。\n\n**核心模块2：基于世界模型的通用奖励**\n为了稳定地训练泛化策略，本文提出了一种内生的、无监督的通用奖励，而非直接拟合手工设计的奖励函数。该奖励由稠密奖励\\(\\mathcal{R}_{den}\\)和稀疏奖励\\(\\mathcal{R}_{sps}\\)组成。\n\n*   **稠密奖励**：计算当前观测\\(x_t\\)与目标观测\\(x_{goal}\\)之间的LPIPS相似度，即\\(\\mathcal{R}_{den} = \\text{LPIPS}(x_t, x_{goal})\\)。目标观测\\(x_{goal}\\)提供了任务完成时最终状态的先验，它由文本世界模型基于环境初始观测和文本指令自回归生成。当预测的完成信号\\(\\hat{d_t^{txt}}\\)首次超过阈值\\(\\theta\\)时，取该帧作为目标观测。\n*   **稀疏奖励**：由动作世界模型预测的完成信号\\(\\hat{d^{act}_{t}}\\)决定，当该信号超过阈值\\(\\theta_{sps}\\)时给予奖励1，否则为0，即\\(\\mathcal{R}_{sps}=1 \\text{ if } d^{act}_{t} \\geq \\theta_{sps} \\text{ else } 0\\)。\n\n完整奖励为：\\(\\mathcal{R} = \\mathcal{R}_{sps} + \\mathcal{R}_{den}\\)。\n\n**核心模块3：基于世界模型的RL训练流程**\n训练流程主要包含三部分（如算法1所示）：\n1.  **可泛化环境初始化**：利用世界模型从海量场景中学到的动态，可以同时初始化多个不同的环境。同时，使用文本世界模型为每个环境生成目标观测\\(x_{goal}\\)。\n2.  **环境中的策略推演**：策略\\(\\pi_\\Theta\\)输出动作序列，动作世界模型据此生成下一观测\\(x_t\\)，并根据公式8计算当前时间步的奖励。\n3.  **策略优化**：这是一个通用灵活的范式，支持符合观测、奖励、完成信号通用接口的各种RL优化算法和策略。策略参数更新遵循公式：\\(\\Theta \\leftarrow \\Theta + \\gamma \\cdot \\nabla_\\Theta J(\\Theta; D(X, R))\\)。\n\n**创新点**：与现有方法相比，RoboScape-R的核心创新在于：1）提出了一个源自世界模型本身的内生通用奖励机制，而非依赖外部的、手工设计的奖励函数或奖励模型；2）开创性地将世界模型集成为一个提供所有必要环境信号（观测、奖励、完成）的在线RL训练环境，而非仅作为数据生成器或离线评估器。\n\n## 实验与结果\n**实验设置**：\n*   **数据集与平台**：使用ManiSkill物理仿真器收集数据，包含80个场景，4个任务（抓放、推、拉、移动到目标），超过30万条轨迹（包含最优和次优轨迹）。使用80%场景训练世界模型，20%未见场景用于OOD评估。\n*   **基线方法**：与三种奖励机制对比：1) **基于规则的奖励**（物理仿真器中使用）；2) **嵌入奖励**（iVideoGPT，从隐藏嵌入直接拟合手工奖励标签）；3) **代理奖励**（DiWA，使用外部MLP模型提供奖励）。\n\n**关键实验结果**：\n1.  **整体性能**：如表1所示，在MLP和OpenVLA两种策略上，使用世界模型进行RL训练，在领域内（IND）场景下性能与在物理仿真器（ManiSkill）中训练相当甚至略优；在领域外（OOD）场景下，性能显著优于基线，平均提升达37.5%。这表明世界模型能有效作为RL训练环境，并大幅提升策略泛化能力。\n\n![不同奖励模块的成功率对比](https://arxiv.org/html/2512.03556v1/x2.png)\n> **图2**：在抓放任务上，使用不同奖励模块训练OpenVLA策略的成功率随训练时间的变化。基于世界模型的奖励在训练效率和最终成功率上均表现最佳，而外生奖励（嵌入、代理）收敛慢且性能差。\n\n![奖励曲线对比](https://arxiv.org/html/2512.03556v1/x3.png)\n> **图3**：在OOD环境下，一次成功抓放轨迹的奖励曲线对比。基于世界模型的奖励提供了更平滑、更具指导性的信号，表明其比嵌入和代理奖励更具泛化性。\n\n2.  **消融实验（奖励模块对比）**：图2展示了不同奖励模块的训练效果。基于世界模型的内生奖励取得了最佳的成功率和训练效率。外生奖励（嵌入、代理）由于直接拟合手工奖励导致信号不稳定，收敛缓慢且性能较差。\n3.  **多任务评估**：如表2所示，在多任务（抓放和移动到目标）训练设置下，在世界模型中进行RL训练相比监督微调（SFT）和物理仿真器RL训练，在多数情况下能带来更均衡或显著的性能提升，显示了其对任务级泛化的益处。\n\n![领域内轨迹可视化案例](https://arxiv.org/html/2512.03556v1/x4.png)\n> **图4**：领域内评估的轨迹可视化案例。在世界模型环境中用通用奖励训练的策略，其性能与在物理仿真器中训练的策略相当，而用代理和嵌入奖励训练的策略表现较差。\n\n![领域外轨迹可视化案例](https://arxiv.org/html/2512.03556v1/x5.png)\n> **图5**：领域外评估的轨迹可视化案例。只有世界模型环境中训练的策略在OOD场景下具有泛化能力，成功完成任务；在其他环境中训练的策略因只与单一环境交互而泛化能力差。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一种基于世界模型的内生通用奖励机制，能够为异构任务生成统一的奖励信号，从而训练出具有广泛泛化能力的策略。\n2.  提出了一种开创性的以世界模型为中心的RL方法，将世界模型集成为一个提供所有必要环境信号的通用环境模拟器，实现了高效、可泛化的具身策略训练。\n3.  通过大量实验验证了框架的有效性，实证表明在RoboScape-R环境中训练的策略在OOD场景下性能平均提升37.5%。\n\n**局限性**：论文自身未明确列出局限性，但从方法描述和实验设置可推断，世界模型的训练需要大量计算资源（96小时在H20 GPU集群上）和高质量、多样化的轨迹数据。此外，奖励机制中涉及多个阈值（\\(\\theta, \\theta_{sps}\\)），其设定可能影响训练效果。\n\n**启示**：本文工作为利用大规模预训练的世界模型作为在线、通用的策略训练环境开辟了新路径。它启示后续研究可以进一步探索：1）如何设计更高效、更鲁棒的内生奖励形式；2）如何将更强大的基础模型（如视频生成模型）无缝集成到此类训练范式中；3）如何将该范式扩展到更复杂的真实机器人任务和动态环境中，推动通用机器人学习的发展。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03556v1/x1.png",
        "https://arxiv.org/html/2512.03556v1/x2.png",
        "https://arxiv.org/html/2512.03556v1/x3.png",
        "https://arxiv.org/html/2512.03556v1/x4.png",
        "https://arxiv.org/html/2512.03556v1/x5.png",
        "https://arxiv.org/html/2512.03556v1/x6.png",
        "https://arxiv.org/html/2512.03556v1/x7.png",
        "https://arxiv.org/html/2512.03556v1/x8.png",
        "https://arxiv.org/html/2512.03556v1/x9.png",
        "https://arxiv.org/html/2512.03556v1/x10.png",
        "https://arxiv.org/html/2512.03556v1/x11.png",
        "https://arxiv.org/html/2512.03556v1/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03684",
      "title": "A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection",
      "url": "http://arxiv.org/abs/2512.03684",
      "arxivId": "2512.03684",
      "date": "2025-12-03",
      "authors": "Bishakh Bhattacharya Team",
      "category": "Manipulation",
      "summary": "本文提出了一种自主番茄采摘系统，旨在解决复杂环境下对成熟番茄的轻柔、精准抓取与分离问题。核心技术包括：1）结合软性拉胀手指与刚性外骨骼的混合夹持器，实现笼式包覆；2）基于RGB-D相机与Detectron2的视觉管道，通过语义分割识别成熟度，并利用关键点检测定位果梗与果实中心；3）基于虚拟功原理建立伺服扭矩与抓取力的分析模型，并采用PID控制器与力敏电阻实现闭环力控，防止滑脱与损伤；4）利用粒子群优化为5自由度机械臂规划轨迹。实验表明，系统平均采摘周期为24.34秒，总体成功率约80%，且抓取力维持在0.20–0.50 N的低水平。",
      "detailedSummary": "## 研究背景与动机\n精准农业和智能农场正被广泛采用以提高生产率、减少资源浪费并在需求增长下维持高品质。其中，自主机器人收割是园艺领域的一项关键技术，可应对劳动力短缺和高成本问题。然而，许多传统收割方法（如联合收割机、树干摇动器）不适合番茄等柔软娇嫩的水果，因为较大的接触力和冲击会损伤果实。选择性收割，即在适当的成熟阶段单独采摘水果，是高价值作物的首选方式，但其面临三大挑战：在遮挡下检测目标果实、估计其姿态并识别果梗切割点、以及在不损伤果实或植株的情况下执行抓取和分离。在真实的种植环境中，番茄通常密集生长，并被枝叶部分遮挡，使得感知和可靠操作变得困难。因此，结合顺应性末端执行器、鲁棒感知和闭环控制的集成收割系统仍是研究热点。\n\n本文针对上述痛点，提出了一个完整的番茄收割机器人系统，其核心思路是集成一种结合软质拉胀结构手指和刚性外骨骼的混合夹爪，并配以基于深度学习的感知（语义分割与关键点检测）、基于粒子群优化的轨迹规划和闭环PID力控制，以实现端到端的轻柔、可靠采摘。\n\n## 方法详解\n该系统整体流程为：感知模块通过RGB-D相机和基于Detectron2的模型分割成熟/未成熟番茄实例，并定位番茄中心和果梗关键点；随后，5自由度机械臂执行采摘序列（接近、分离、切割、抓取、运输、释放），期间夹爪通过力敏电阻反馈进行闭环力控制，确保轻柔抓握。\n\n![方法框架](https://arxiv.org/html/2512.03684v1/Complete_assembly.png)\n> **图1**：提出的混合夹爪CAD模型。左图：顶部为微伺服驱动的切割器，中部为用于轻柔抓握的软质拉胀手指结构，底部为分离阶段形成截锥体的分离叶片。右图：用于驱动手指闭合的伺服驱动苏格兰轭机构。\n\n**核心模块1：混合夹爪设计与分析**。夹爪结合了2D凹入型拉胀结构（超材料）和形成内部外骨骼的刚性连杆。拉胀结构具有负泊松比，拉伸时能横向扩展，相比传统结构能实现更顺应、共形的接触，有利于减少对柔软水果的局部应力集中。夹爪采用六个对称手指实现稳定的笼式抓握。每个手指集成了凹入型蜂窝图案和一个弯曲的板簧状元件，以提供形状适应性和有效刚度。为防止运输中果实逃脱，在拉胀结构上粘合了一层薄乳胶篮。在果实簇生场景中，六个分离叶片闭合形成截锥体，用于将邻近番茄向外推开以隔离目标。之后，安装在一个分离叶片上的微伺服驱动切割器切断果梗。\n\n![切割器机制](https://arxiv.org/html/2512.03684v1/Cutter_mechanism.png)\n> **图2**：安装在分离叶片上的微伺服驱动切割器机制。当分离叶片完全闭合且果梗对准切割区域时，切割器被驱动。\n\n为进行驱动选型，论文基于虚功原理推导了伺服扭矩与抓握力之间的解析关系模型。通过建立代表性手指的接触模型和运动学参数，得出了输入扭矩与输出力之间的关系式。分析表明，由于多指同时接触和传动损耗，所需总驱动扭矩随期望抓握力呈非线性增长。\n\n![力-扭矩曲线](https://arxiv.org/html/2512.03684v1/force-torque-curve-transparent.png)\n> **图5**：夹爪手指所需电机扭矩随期望抓握力的非线性变化。\n\n**核心模块2：感知系统**。采用基于Detectron2的深度学习流程，实现两个功能：1）番茄实例的语义分割（区分成熟/未成熟）；2）关键点定位（番茄中心和果梗）。番茄中心定位用于目标姿态估计和接近规划，果梗关键点定位用于切割器对准。训练数据结合了公开数据集和自行采集的图像。\n\n![感知输出示例](https://arxiv.org/html/2512.03684v1/Tomato_detection.png)\n> **图10**：感知模块输出示例：番茄实例（成熟/未成熟）的语义分割，以及预测的果梗和番茄中心关键点。\n\n**核心模块3：控制与执行**。夹爪集成了三个力敏电阻（FSR）用于测量接触力。采用基于PID的闭环力控制器，根据FSR反馈实时调节伺服角度，以跟踪设定的参考抓握力（实验中设为0.30 N），防止滑落同时避免损伤。PID增益通过实验整定（Kp=0.15 °/N， Ki=0.02 °/(N·s)， Kd=0.001 °·s/N）。机械臂运动采用基于粒子群优化（PSO）的轨迹规划。\n\n![控制器框图](https://arxiv.org/html/2512.03684v1/controller_Simulink_1.png)\n> **图7**：抓握力调节的Simulink控制器框图。测量的FSR力与参考力比较；离散PID输出被映射并限幅到安全的伺服角度范围，用于驱动苏格兰轭夹爪。\n\n**创新点**：1) 设计了集分离、切割、笼式抓握于一体的软-硬混合拉胀夹爪；2) 基于虚功原理建立了该特殊夹爪的扭矩-力分析模型，为驱动设计提供理论依据；3) 将实例分割、关键点检测与闭环力控制深度集成于一个完整的收割流程中。\n\n## 实验与结果\n**实验设置**：在受控实验室环境下，使用5自由度ViperX-300机械臂、ZED2i RGB-D相机和所提出的混合夹爪进行完整采摘周期评估。感知模型在Rob2Pheno数据集及自采图像上训练，使用Detectron2框架，比较了ResNet-50、ResNeXt和ResNet-101等骨干网络。\n\n**感知性能对比**：如表1所示，ResNeXt骨干网络取得了最佳整体性能，平均精度（mAP，IoU阈值0.50:0.95）为0.80，精确度为0.85，召回率为0.75。\n\n**抓握力控制性能**：PID控制器能使抓握力在大约1-2秒内收敛到0.30 N的参考值，超调低于10%，稳态偏差保持在约0.02 N以内。\n\n![力控响应](https://arxiv.org/html/2512.03684v1/Controller_simulink_response_1.png)\n> **图8**：PID力调节响应。蓝色为参考力，红色为测量的抓握力。\n\n**抓握性能分析**：对不同重量（40-81 g）和直径（43-57 mm）的番茄样本进行了抓握测试。抓握力-时间曲线显示，较重/较大的番茄需要较高的稳态抓握力（约0.48-0.50 N），而较小/较轻的番茄稳定在约0.20-0.25 N。所有情况下，抓握力均保持在0.20–0.50 N的低范围内，表明夹爪能实现轻柔而稳定的抓握。\n\n![抓握力曲线](https://arxiv.org/html/2512.03684v1/grasping_force_sensing.png)\n> **图9**：在抓握-保持-释放试验中使用三个FSR条测得的抓握力与时间关系。所绘制的力是每个时间点三个校准后的FSR信号的平均值。\n\n**完整采摘周期评估**：进行了10次采摘试验，成功完成了接近、分离、切割、抓取、运输和释放的全流程。\n\n![采摘流程](https://arxiv.org/html/2512.03684v1/Experimental_operation.png)\n> **图11**：实验室条件下完整采摘周期的实验序列：(a–b) 接近番茄簇，(c–d) 使用截锥体分离叶片分离目标番茄，(e) 使用微伺服驱动切割器切割果梗，(f) 用拉胀手指和乳胶篮进行轻柔笼式抓握，(g) 移向果筐，(h) 释放到果筐中。\n\n**关键结果**：\n- **总体成功率**：10次试验中8次成功，成功率为80%。失败原因包括果梗未对准、遮挡导致关键点错误以及运动初期接触力不足导致滑落。\n- **平均周期时间**：完整采摘周期平均耗时约24.34秒。时间分布如图12所示，其中接近和分离阶段占比较大。\n- **抓握力范围**：在整个测试的番茄尺寸范围内，抓握力保持在0.20–0.50 N的低水平。\n\n![时间分布](https://arxiv.org/html/2512.03684v1/bar_chart.png)\n> **图12**：不同直径番茄样本的采摘周期时间分解。条形图显示了接近、分离、切割、抓握、离开、释放各阶段的持续时间以及总周期时间。\n\n## 总结与启发\n**核心贡献**：\n1.  **集成系统设计**：提出并实现了一个完整的番茄自主收割机器人系统，创新性地将软-硬混合拉胀夹爪、基于深度学习的感知（语义分割与关键点检测）、闭环力控制以及轨迹规划紧密结合。\n2.  **夹爪设计与建模**：设计了具有分离、切割和笼式抓握多功能于一体的混合夹爪，并基于虚功原理推导了其扭矩-力关系模型，为驱动设计和性能分析提供了理论工具。\n3.  **实验验证**：在实验室环境下系统地验证了系统的可行性，展示了约80%的成功率、24.34秒的平均周期时间以及0.20-0.50 N的低损伤抓握力范围。\n\n**局限性**：论文自身提到，未来工作需关注：1) 提高遮挡下果梗切割和对准的鲁棒性；2) 增强感知模块在户外光照和背景变化下的可靠性；3) 将评估扩展到更多水果品种和更大规模的田间试验，并进行定量损伤评估；4) 通过改进多果实采摘的规划和执行策略来缩短周期时间。\n\n**启示**：这项工作表明，将具有顺应性和定制几何结构的超材料夹爪与先进的视觉感知和闭环控制相结合，是解决精细选择性收割挑战的有效途径。其系统级的集成方法、对低接触力的重点关注以及基于物理模型的设计分析，为后续开发更高效、更通用的农业机器人末端执行器和控制系统提供了有价值的参考。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03684v1/Complete_assembly.png",
        "https://arxiv.org/html/2512.03684v1/Cutter_mechanism.png",
        "https://arxiv.org/html/2512.03684v1/static_analysis.png",
        "https://arxiv.org/html/2512.03684v1/Torque_diagram.png",
        "https://arxiv.org/html/2512.03684v1/force-torque-curve-transparent.png",
        "https://arxiv.org/html/2512.03684v1/Grasp-tester.jpg",
        "https://arxiv.org/html/2512.03684v1/controller_Simulink_1.png",
        "https://arxiv.org/html/2512.03684v1/Controller_simulink_response_1.png",
        "https://arxiv.org/html/2512.03684v1/grasping_force_sensing.png",
        "https://arxiv.org/html/2512.03684v1/Tomato_detection.png",
        "https://arxiv.org/html/2512.03684v1/Experimental_operation.png",
        "https://arxiv.org/html/2512.03684v1/bar_chart.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03724",
      "title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention",
      "url": "http://arxiv.org/abs/2512.03724",
      "arxivId": "2512.03724",
      "date": "2025-12-03",
      "authors": "Mingming Gong Team",
      "category": "Manipulation",
      "summary": "本文针对当前视觉-语言-动作模型在具身任务中生成冗余动作、缺乏精确性的问题，提出PosA-VLA框架。其核心是姿态条件锚点注意力机制，通过姿态条件监督锚定视觉注意力，引导模型聚焦于任务相关区域，从而提升动作生成的精度与效率。该方法基于轻量架构，无需额外感知模块。实验表明，该模型在多种机器人操作基准测试中能更快、更准确地完成任务，例如在抓取任务中比基线模型更早进入成功抓取范围。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型在具身任务上展现出显著性能，被认为是实现具身智能最有前景的途径之一。然而，现有主流VLA模型（如π₀、OpenVLA、Smol-VLA等）在真实应用中存在关键局限：它们生成的动作不一致、不精确，运动轨迹常包含冗余甚至不稳定的行为。这导致执行步骤增多、效率降低，在复杂环境中还可能引发抓取失败或碰撞。\n\n本文通过分析VLA的注意力机制，将上述问题归因于现有模型**空间均匀的感知场**。这种感知场缺乏将注意力明确引导至任务相关区域和机械臂末端执行器区域的机制，导致模型在杂乱或视觉复杂的环境中容易被无关物体分散注意力，从而产生不稳定的运动轨迹。本文针对这一痛点，提出通过**姿态条件监督来锚定视觉注意力**的新视角，以增强动作生成的精确性和效率。\n\n核心思路是：通过构建与机器人末端执行器姿态相关联的注意力锚点，为模型提供明确的空间监督，打破其均匀的感知场，使其在执行过程中能持续聚焦于任务相关区域和末端位置，从而实现更稳定、精确和高效的操作。\n\n## 方法详解\nPosA-VLA的整体框架是一个端到端的策略网络，输入为头戴相机和腕部相机图像、机器人本体感知状态以及文本指令，输出为低层控制指令。其核心创新在于引入了一个姿态条件锚点注意力机制，该机制通过专门的损失函数进行监督，以引导和细化视觉特征。\n\n![方法框架](https://arxiv.org/html/2512.03724v2/image/framework.png)\n> **图2**：PosA-VLA框架总览。CLIP编码器提取文本和视觉特征，通过交叉注意力模块生成锚点注意力权重，该权重由提出的锚点损失（使用真实姿态条件锚点图）进行监督。锚点权重随后与DINOv2图像特征进行逐元素相乘，得到细化的视觉表征。最后，细化后的视觉特征、文本特征和机器人状态特征被输入流匹配变换器（FMT）以预测连续动作序列。\n\n**核心模块一：姿态条件锚点生成**\n该模块旨在为注意力提供明确的空间监督信号。首先，利用CLIP编码器提取语言特征 `f_x` 和来自双相机的块状视觉特征 `F_I`。通过交叉注意力模块，生成两个互补的注意力权重图：`M_task`（任务相关锚点）和 `M_end`（末端执行器锚点），两者拼接得到最终的姿态锚定注意力权重 `M_t`。\n关键步骤是从示教数据中构造监督信号。在末端执行器状态（如夹爪开合）发生变化的时刻，将其3D位姿投影到两个相机视图的2D坐标上，并以此为中心生成高斯热图 `F_task`，作为任务相关区域的监督。同时，在**每一帧**都将末端执行器位置投影生成一个方差更小的高斯热图 `F_end`，作为末端位置的持续监督。这构成了双重空间先验，引导模型同时关注“在哪里操作”和“末端执行器在哪里”。\n\n**核心模块二：姿态条件锚点损失**\n此损失函数联合监督空间注意力预测和多模态对齐，由两部分组成：\n1.  **空间注意力损失 (`L_f`)**: 使用Focal Loss监督预测的注意力权重 `M_t` 趋近于真实锚点图 `F_f = [F_task, F_end]`，以处理前景-背景不平衡。\n2.  **批对比损失 (`L_c`)**: 为了增强样本间的一致性和判别性，引入批级别的对比损失。它基于文本指令标识符，拉近同一任务的正样本特征（从锚点图激活超过阈值的图像块中提取），推开不同任务的负样本特征。\n总锚点损失为 `L_anchor = α * L_f + (1-α) * L_c`，其中α平衡两项。\n\n**核心模块三：注意力引导的特征细化与动作生成**\n使用预测的锚点注意力权重 `M_t` 对DINOv2提取的稠密视觉特征 `F_DINO` 进行加权：`F_v_ref = M_t ⊙ F_DINO`，从而突出对操作最相关的区域。细化后的视觉特征、文本特征和机器人状态被融合为多模态观测表征。\n动作生成采用流匹配变换器（FMT）。它将动作序列的生成建模为从简单先验分布到目标动作分布的连续流（轨迹）学习，通过匹配瞬时速度场进行优化，避免了迭代去噪，计算效率更高。总训练目标结合了动作损失和锚点监督：`L_total = L_action + λ * L_anchor`。\n\n**与现有方法的创新点**\n1.  **自我包含的空间引导**：无需依赖外部分割或 grounding 网络，通过内部生成的姿态条件锚点实现空间注意力引导。\n2.  **双重动态锚点**：同时提供任务区域和末端执行器位置的持续、动态更新的空间监督，使感知-动作耦合更稳定。\n3.  **高效的统一框架**：将注意力引导机制集成到轻量级架构中，保持了高效的训练和推理。\n\n![注意力可视化](https://arxiv.org/html/2512.03724v2/x2.png)\n> **图3**：有无锚点监督的注意力行为可视化。上方基线模型（无锚点损失）的注意力在场景中几乎均匀分布；下方PosA-VLA（有锚点损失）产生了更尖锐、更局部化、以任务为中心的注意力。\n\n## 实验与结果\n**实验设置**：实验在AlphaBot 1s机器人平台（7自由度机械臂，配备头戴和腕部相机）上进行。使用手动收集的抓取任务数据集（每个物体200条示教）进行训练。评估了在多种测试条件下的性能，包括基本场景、未见背景、光照变化、干扰物体、未见物体以及一个长视野任务（打开盒盖并放入物体）。\n\n**对比基线**：包括 π₀, π₀.5 (LoRA/Full), OpenVLA-OFT, Smol-VLA, DexGraspVLA。\n\n**关键实验结果**：\n1.  **标准抓取任务成功率**：如表1所示，PosA-VLA在平均成功率上达到55.3%，显著优于所有基线（最佳基线DexGraspVLA为50.5%）。尤其在基本场景下达到74.9%，展示了其核心优势。\n\n![抓取成功率对比表](https://arxiv.org/html/2512.03724v2/x4.png)\n> **表1**：不同测试条件下的抓取成功率(%)对比。PosA-VLA取得了最高的平均成功率。\n\n2.  **长视野任务性能**：如表2所示，在开盖放置任务中，PosA-VLA的整体成功率达到61.1%，远高于其他方法（π₀为42.6%），表明其动作的一致性和精确性在复杂任务中依然有效。\n\n![长视野任务成功率表](https://arxiv.org/html/2512.03724v2/x5.png)\n> **表2**：长视野盒盖任务的成功率(%)。PosA-VLA在整体任务和各子步骤上均表现最佳。\n\n3.  **效率对比**：如表3所示，PosA-VLA训练仅需20 GPU小时，与最轻量的Smol-VLA相当。在推理时，其平均每动作耗时24.5ms，完成任务所需的总步骤数（526步）和总执行时间（12.9秒）均为所有方法中最少，证明了其高效性。\n\n![效率对比表](https://arxiv.org/html/2512.03724v2/x6.png)\n> **表3**：训练与推理效率对比。PosA-VLA在训练时间、推理速度和执行步骤数上均表现优异。\n\n4.  **消融实验**：论文验证了各核心组件的贡献。移除整个锚点损失 (`L_anchor`) 会导致性能大幅下降（平均抓取成功率从55.3%降至33.1%）。分别移除任务锚点监督或末端锚点监督，性能均有显著降低，证明了双重监督的必要性。移除批对比损失 (`L_c`) 也会导致性能下降，表明其对增强特征判别性有积极作用。\n\n![消融实验图](https://arxiv.org/html/2512.03724v2/x7.png)\n> **图8**：消融研究结果。完整模型（Ours）性能最佳，移除任何关键组件（锚点损失、任务锚点、末端锚点、对比损失）都会导致成功率下降。\n\n5.  **轨迹平滑性**：如图1所示，PosA-VLA的末端执行器到目标点的距离曲线更平滑、收敛更快，而基线模型曲线波动大、收敛慢甚至失败，直观证明了其生成动作的精确性和稳定性。\n\n![轨迹分析图](https://arxiv.org/html/2512.03724v2/x1.png)\n> **图1**：抓取任务（拿起面包）的定量分析。下图显示了机器人末端执行器与真实抓取点之间的距离随时间变化曲线。PosA-VLA（蓝线）更快、更准确地进入成功抓取范围（浅蓝区域），而基线方法需要更长时间或失败。\n\n## 总结与启发\n**核心贡献**：\n1.  **问题诊断**：首次从注意力机制角度，将VLA模型动作不一致、不精确的问题归因于其**空间均匀的感知场**。\n2.  **方法创新**：提出了**PosA-VLA框架**，创新性地引入**姿态条件锚点注意力**机制，通过双重动态空间监督（任务区域+末端位置）打破均匀感知，实现自我包含的空间引导。\n3.  **性能与效率**：在多个机器人操作基准测试中，实现了更高的成功率、更平滑的轨迹和更快的推理速度，同时展现出对环境变化和长视野任务的强鲁棒性，且仅需少量数据即可达到强性能。\n\n**局限性**：论文自身提到，其方法依赖于从示教数据中获取的精确末端执行器姿态来生成监督信号。在姿态估计不准确或缺乏此类标注数据的情况下，性能可能会受到影响。\n\n**对后续研究的启示**：\n1.  **注意力引导的通用性**：将姿态或更一般的状态信息作为条件来引导模型注意力，是提升具身智能模型动作精确性和一致性的有效范式，可推广至其他动作生成场景。\n2.  **轻量化与自监督**：PosA-VLA证明了不依赖重型外部感知模块，通过内部自监督机制实现高效空间 grounding 的可行性，为开发更紧凑、实用的机器人VLA模型提供了思路。\n3.  **动态感知-动作耦合**：强调了对交互过程中动态变化的空间关系（如末端与目标的相对位置）进行持续建模的重要性，未来研究可探索更复杂的关系推理以处理更灵巧的操作。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03724v2/x1.png",
        "https://arxiv.org/html/2512.03724v2/image/framework.png",
        "https://arxiv.org/html/2512.03724v2/x2.png",
        "https://arxiv.org/html/2512.03724v2/x3.png",
        "https://arxiv.org/html/2512.03724v2/x4.png",
        "https://arxiv.org/html/2512.03724v2/image/demo/total_objects_heng.jpeg",
        "https://arxiv.org/html/2512.03724v2/x5.png",
        "https://arxiv.org/html/2512.03724v2/x6.png",
        "https://arxiv.org/html/2512.03724v2/x7.png",
        "https://arxiv.org/html/2512.03724v2/x8.png",
        "https://arxiv.org/html/2512.03724v2/x9.png",
        "https://arxiv.org/html/2512.03724v2/x10.png",
        "https://arxiv.org/html/2512.03724v2/x11.png",
        "https://arxiv.org/html/2512.03724v2/x12.png",
        "https://arxiv.org/html/2512.03724v2/x13.png",
        "https://arxiv.org/html/2512.03724v2/x14.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03538",
      "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation",
      "url": "http://arxiv.org/abs/2512.03538",
      "arxivId": "2512.03538",
      "date": "2025-12-03",
      "authors": "Kai Xu Team",
      "category": "Manipulation",
      "summary": "本文提出AdaPower框架，旨在解决世界基础模型生成真实性与机器人控制所需精度之间的差距。通过时空测试时训练实现推理时适配，并利用记忆持久化保证长时程一致性，将通用世界模型转化为专用模型。该框架集成于模型预测控制中，赋能预训练VLA策略，在LIBERO基准上实现任务成功率超过41%的提升，且无需策略重训练，保持了计算效率。",
      "detailedSummary": "## 研究背景与动机\n世界基础模型（WFMs）在视觉动态模拟方面展现出卓越能力，但其在精确机器人控制中的应用受到生成真实性与面向控制精度之间差距的限制。现有主流方法（如DreamGen）将WFMs用作合成数据生成器：先在有限的机器人数据上微调WFM，生成合成视频，通过逆动力学模型提取伪动作序列，最后用这些神经轨迹训练新的视觉运动策略。该范式存在三个关键局限：1）生成海量视频数据和训练新策略的计算成本过高；2）适应周期长，每个新任务或环境都需要重复整个流程；3）未充分利用强大的预训练视觉-语言-动作（VLA）策略，而是选择训练新的专用策略。\n\n本文针对“如何高效利用WFMs直接赋能现有预训练策略”这一具体痛点，提出了从“数据生成范式”转向“模型适配范式”的新视角。核心思路是：通过轻量级的参数优化，将通用WFMs转化为专用世界模型（SWM），并集成到模型预测控制（MPC）框架中，直接提升预训练VLA策略在未见任务上的零样本泛化能力，从而避免昂贵的策略重新训练。\n\n## 方法详解\nAdaPower的整体框架旨在高效地将一个基础世界模型（WFM）适配为适用于机器人操作的专用世界模型（SWM）。框架基于扩散变换器（DiT）架构，以Cosmos-Predict-2B作为基础WFM。适配过程中，基础WFM的参数被冻结，仅训练新引入的模块。整体Pipeline包含两个核心创新模块：时空测试时训练（TS-TTT）模块和记忆持久化（MP）模块。此外，为了支持动作条件预测，将原WFM的文本编码器替换为一个由多个感知层组成的动作编码器。\n\n![整体架构](https://arxiv.org/html/2512.03538v1/x2.png)\n> **图2**：AdaPower整体架构。基于Cosmos-Predict-2B的扩散变换器架构，插入了两个关键组件：时空测试时训练（TS-TTT）模块和记忆持久化（MP）模块。TS-TTT在推理时通过跨时空维度的自监督损失优化部分参数；MP模块将历史信息整合为记忆特征，并通过交叉注意力与DiT特征交互以保持历史上下文。\n\n**时空测试时训练（TS-TTT）模块**：该模块旨在解决测试时分布偏移问题，使模型能在推理过程中进行自监督优化。其关键洞察是视频特征存在两种低秩先验：a) 空间低秩（同一帧内相邻像素颜色纹理相似）；b) 时间低秩（连续帧间大部分背景内容静止或渐变）。传统TTT仅利用通道维度的低秩先验，而TS-TTT扩展为同时学习时空维度和通道维度的低秩先验。\n\n具体而言，给定视频输入特征 $\\mathbf{v} \\in \\mathbb{R}^{T \\times H \\times W \\times D}$，TS-TTT包含两个并行的自学习分支：\n1.  **时空分支**：将 $\\mathbf{v}$ 展平为 $\\mathbf{v}_{ts} \\in \\mathbb{R}^{D \\times THW}$，对其应用低秩投影和重建。该分支通过优化参数 $W$ 最小化自监督损失 $\\ell(W; v_{ts}^i) = \\| f(W; \\Theta_K v_{ts}^i) - \\Theta_V v_{ts}^i \\|^2$，其中 $f$ 是参数为 $W$ 的神经网络，$\\Theta_K, \\Theta_V$ 是可学习的投影矩阵（在训练阶段与基础网络一起优化）。推理时，根据历史上下文 $v_{ts}^i$ 迭代更新 $W$：$W^{i}=W^{i-1}-\\eta\\nabla\\ell(W^{i-1}; v_{ts}^{i})$。\n2.  **通道分支**：将 $\\mathbf{v}$ 重塑为 $\\mathbf{v}_c \\in \\mathbb{R}^{THW \\times D}$，并采用与时空分支类似但参数独立的过程进行低秩投影和重建。\n两个分支的输出 $\\mathbf{z}_{ts}$ 和 $\\mathbf{z}_c$ 被重塑回原始形状，并通过残差连接与原始输入特征融合：$\\mathbf{v}^{o} = \\mathbf{v} + \\mathbf{z}_{ts} + \\mathbf{z}_c$。这种设计使模型能够跨时间、空间和通道维度进行自监督优化。\n\n**记忆持久化（MP）模块**：该模块旨在解决长时域预测中的误差累积和知识遗忘问题，确保时序一致性。其核心思想是在每个预测步骤中，将历史帧的上下文信息整合到当前DiT块的特征中。\n\n具体实现是：使用视觉基础模型DINOv2作为编码器，将历史帧 $\\mathbf{H} \\in \\mathbb{R}^{L \\times H \\times W \\times 3}$ 编码为密集的patch tokens $\\mathbf{m} \\in \\mathbb{R}^{L \\times P \\times D}$，并将其重塑为 $LP \\times D$ 的2D序列作为记忆特征。随后，在DiT块中执行交叉注意力操作，其中记忆特征提供键和值，当前DiT特征提供查询。这使得模型能够在多步自回归展开中保持连贯的场景表示和关键物体关系。\n\n**与模型预测控制（MPC）的协同部署**：适配后的专用世界模型被集成到一个协作式MPC框架中。\n\n![MPC框架](https://arxiv.org/html/2512.03538v1/x3.png)\n> **图3**：MPC框架。预训练的VLA根据初始状态和任务指令生成候选动作序列。随后，专用世界模型将这些动作模拟为轨迹，由一个奖励模型（基于ReWind方法）进行评估，以选择最优序列执行。\n\n具体流程如下：给定初始状态和任务指令，预训练的VLA作为高级规划器生成多个候选动作序列。为了增加探索多样性，会用低方差高斯分布采样的噪声序列进行增强。接着，专用世界模型对这些动作序列进行模拟，生成想象的未来轨迹。一个基于ReWind方法的奖励模型根据任务指令和历史观测评估每条轨迹的任务完成进度。最终，选择预测奖励最高的动作序列执行。整个系统结合了VLA的通用推理能力和专用世界模型的精确物理模拟，且VLA始终保持零样本状态，无需微调。\n\n## 实验与结果\n**实验设置**：使用LIBERO-90数据集中采样的2000条轨迹进行仿真实验。评估在10个未见过的操作任务上进行（排除在微调集外），每个任务执行20次，以平均任务成功率作为主要指标。基础VLA策略采用CogACT。适配模块（TS-TTT、MP、动作编码器）并非插入每个DiT块，而是每7个块插入一次，共新增约1.5亿参数（小于基础模型大小的10%）。训练时，新增层的输出被零初始化以确保稳定性。\n\n**对比实验**：将AdaPower与多种适配方法在赋能预训练VLA策略的MPC框架下进行比较。\n\n![性能对比](https://arxiv.org/html/2512.03538v1/x4.png)\n> **图4**：不同专用世界模型生成的长时域展开的定性比较。评估在LIBERO-LONG的未见任务上进行。圆圈标出了本文方法的改进之处，可见其能保持更稳定的物体轨迹和物理一致性。\n\n表1结果显示，未经任何适配的预训练VLA成功率仅为0.5%。AdaPower将平均成功率大幅提升至41.5%，相对提高了41%。其性能优于参数高效适配方法VACE（20.5%）和LoRA（23.5%），也超过了需要大量计算资源的全参数监督微调（SFT，32.5%）。定性结果（图4）显示，对比方法在长时域预测中会产生模糊物体或物理不一致，而AdaPower能保持稳定的物体轨迹并正确建模接触动力学。\n\n**消融实验**：\n表2的消融研究表明，移除MP模块会导致成功率下降约1.5%，表明其缓解了展开过程中的性能退化。移除TS-TTT模块则导致成功率下降约3.5%，削弱了对未见物体和环境的泛化能力。两者结合才能取得最佳性能（41.5%），证实了时空记忆和测试时适应的互补作用。\n\n表3探索了TS-TTT模块的不同变体。将时间、空间和通道维度分开进行自学习（T+S+C）效果次优。将时空维度融合为一个分支，同时保留独立的通道分支（TS+C，即本文方法）效果最佳，测试MSE最低（0.0205），成功率最高（41.5%）。试图融合所有三个维度（TSC）会导致损失变为NaN，无法收敛。\n\n**跨策略评估**：如表4所示，AdaPower适配的世界模型不仅能将CogACT的成功率提升41%，当与另一个VLA模型 $\\pi_0$ 结合时，也能将其成功率从21.0%提升至48.0%，证明了该方法的架构无关兼容性和强泛化能力。\n\n**真实世界实验**：将系统部署在7自由度Franka Research 3机械臂上，执行放置、抓取、堆叠等操作任务。\n\n![真实世界设置](https://arxiv.org/html/2512.03538v1/x5.png)\n> **图5**：真实世界实验设置。使用7自由度机械臂执行操作任务。\n\n![真实世界轨迹](https://arxiv.org/html/2512.03538v1/x6.png)\n> **图6**：真实世界执行轨迹。专用世界模型增强了预训练VLA模型在多样化操作任务中的零样本泛化能力。\n\n实验表明，适配后的世界模型成功提升了预训练VLA在真实场景中的零样本性能，能够处理形状、材质各异的家庭物品，验证了框架的迁移能力。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了AdaPower，一种通过轻量级适配将通用世界基础模型转化为专用世界模型的新框架，建立了利用互联网规模视频先验进行机器人操作的新范式。\n2.  设计了时空测试时训练（TS-TTT）和记忆持久化（MP）两个模块，有效解决了测试时分布偏移和长时域一致性的核心挑战。\n3.  开发了一个协作式MPC系统，能够显著增强预训练VLA策略的零样本泛化能力，在仿真和真实世界中均取得了显著的性能提升。\n\n**局限性**：论文虽未明确列出，但从方法描述可推断其性能依赖于所选基础WFM（如Cosmos）的质量和规模。此外，测试时的在线优化（TS-TTT）可能引入额外的计算延迟，在需要极低延迟的实时控制场景中可能面临挑战。\n\n**后续启示**：本研究展示了一条高效利用大规模生成式世界模型赋能机器人控制的可行路径，即“轻量适配+协同规划”。这启示后续研究可以探索更高效的适配器结构、将适配过程扩展到多模态指令（如力觉、触觉），以及研究如何将此类系统应用于更复杂的动态和非结构化环境中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03538v1/x1.png",
        "https://arxiv.org/html/2512.03538v1/x2.png",
        "https://arxiv.org/html/2512.03538v1/x3.png",
        "https://arxiv.org/html/2512.03538v1/x4.png",
        "https://arxiv.org/html/2512.03538v1/x5.png",
        "https://arxiv.org/html/2512.03538v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03707",
      "title": "ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration",
      "url": "http://arxiv.org/abs/2512.03707",
      "arxivId": "2512.03707",
      "date": "2025-12-03",
      "authors": "Emma Li Team",
      "category": "Manipulation",
      "summary": "本文提出ContactRL框架，解决人机协作中必要物理接触的安全运动规划问题。核心方法是将接触力安全指标直接融入强化学习奖励函数，并采用基于动能的控制屏障函数（eCBF）作为安全护盾。实验表明，该方法在仿真中实现0.2%的安全违规率和87.7%的任务成功率；在真实机器人递送任务中，接触法向力始终低于10N，确保了安全高效的物理协作。",
      "detailedSummary": "## 研究背景与动机\n在工业5.0时代，人机协作越来越多地涉及物理接触。在精密制造等任务中，机器人可能需要直接从人类手中取回小物体（如螺丝）。然而，这种操作可能导致与手部发生非预期的接触。本文旨在解决的核心问题是：机器人如何在不引起不适或伤害的情况下，从人手中拾取小物体？这代表了一类任务，其中安全且不可避免的接触对于任务执行至关重要。此处的安全性被定义为限制机器人在物理接触过程中施加在人手上的法向接触力 $F_N$。\n\n当前，基于强化学习的运动规划方法在安全性方面主要关注静态和动态环境中的碰撞避免，大多数方法将接触视为需要避免的失败事件。然而，在许多协作任务中，接触是任务执行所必需的。在实现快速、精确到达等任务目标的同时确保低接触力，引入了安全与性能之间的复杂权衡，这是传统避撞框架所未解决的。\n\n本文针对“必须接触但需保证力安全”这一具体痛点，提出了新的视角：将接触安全（通过接触力量化）明确集成到强化学习的奖励函数中，而非简单地避免接触。其核心思路是提出一个名为ContactRL的强化学习框架，通过基于力反馈的奖励直接调控接触力，并部署一个基于动能的控制屏障函数安全护盾，为学习到的策略提供形式化的安全保证。\n\n## 方法详解\nContactRL框架的核心是让RL智能体通过接收安全交互的奖励和不安全接触的惩罚（利用力反馈）来优化其策略，从而生成安全的运动轨迹。\n\n![方法框架](https://arxiv.org/html/2512.03707v1/Figure2-ContactRL.png)\n> **图2**：ContactRL框架概述。机器人基于人机接触模型学习自适应运动轨迹。在机器人与物体之间应用刚体约束以实现抓取，而施加在人手上的法向接触力 $F_N$ 被反馈到奖励函数的安全组件中。\n\n**整体流程**：机器人利用视觉感知提取工作空间中机器人末端执行器和物体的空间坐标，这些信息构成RL智能体的观察空间。智能体随后通过向目标物体迈进一步来执行下一个动作。控制架构由RL智能体管理的高级运动规划、将规划的末端执行器位姿转换为关节位姿的逆运动学控制器，以及使用位置控制的执行层组成。\n\n**马尔可夫决策过程建模**：任务被建模为MDP $\\\\langle\\\\mathcal{S},\\\\mathcal{A}, P, R,\\\\gamma\\\\rangle$。\n- **观察空间**：为9维向量 $\\\\mathbf{S}=[p_{\\\\text{r}}, p_{\\\\text{h}}, v_{\\\\text{r}}]$，其中 $p_{\\\\text{r}}, p_{\\\\text{h}}\\\\in\\\\mathbb{R}^{3}$ 是机器人末端执行器和手部的位置，$v_{\\\\text{r}}\\\\in\\\\mathbb{R}^{3}$ 是末端执行器速度。每回合手部位置随机化。\n- **动作空间**：为3维连续空间，对应于机器人末端执行器的笛卡尔位移 $\\\\boldsymbol{a}=[\\\\delta_{x},\\\\delta_{y},\\\\delta_{z}]\\\\in\\\\mathbb{R}^{3}$，其幅度受 $\\\\|\\\\boldsymbol{a}\\\\|\\\\leq\\\\delta_{\\\\text{max}}$ 限制。\n- **奖励函数设计**：这是一个密集奖励函数，平衡了性能和安全。\n$$ R(\\\\mathbf{s},\\\\mathbf{a}) = w_{r} r_{r} + w_{s} r_{s} + w_{j} r_{j} + w_{p} r_{p} $$\n其中：\n    - $r_r$（到达奖励）：鼓励智能体向手部移动，当末端执行器接触到物体时给予500分的接触奖励，否则惩罚与手部的距离。\n    - $r_s$（安全奖励）：激励将接触力维持在阈值 $F_\\\\tau$ 以下。当 $F_N \\\\le F_\\\\tau$ 时给予正奖励，当 $F_N > F_\\\\tau$ 时施加惩罚。\n    - $r_j$（急动奖励）：惩罚连续动作之间的巨大差异，以最小化动作变化。\n    - $r_p$（接近奖励）：鼓励机器人在开始时采取较大步长，接近人类时采取较小步长，以最小化完成时间。\n权重 $w_r, w_s, w_j, w_p$ 通过消融研究确定最优组合。\n\n![模拟环境](https://arxiv.org/html/2512.03707v1/Figure3-Sim.png)\n> **图3**：包含UR3e机器人、物体和简化手部模型的模拟环境。右侧面板展示了交互过程中施加在手部上的接触力分量。\n\n**训练设置**：使用Soft Actor-Critic算法进行策略训练。模拟环境基于PyBullet，将人手简化为平面模型，并设置了相应的质量和摩擦系数物理属性。安全接触力阈值 $F_\\\\tau$ 设为50N。\n\n**基于能量的屏障安全护盾**：为了在策略执行期间确保安全，部署时采用了基于动能的控制屏障函数安全护盾。该护盾通过末端执行器的动能间接调控接触力。其目标是保证末端执行器的动能 $\\\\frac{1}{2}m\\\\|v\\\\|^2$ 不超过根据安全力限值经验选择的预算 $E_{\\\\max}=0.30\\\\,J$。由于接触力大小 $F_N$ 与动量的变化率成正比，此能量约束隐式地限制了最大可达到的接触力。护盾通过求解一个二次规划问题，将指令速度投影到满足此能量约束的最接近的可容许集合上。护盾工作流程包括：对RL策略输出的原始动作进行低通滤波以抑制抖动，然后应用eCBF条件，必要时对滤波后的速度进行径向缩放以确保动能不超限。\n\n**创新点**：1) 与将安全等同于无接触的现有方法不同，ContactRL明确将接触力安全作为奖励项集成到RL策略学习中，以解决必要接触场景下的力调制问题。2) 部署时使用的eCBF护盾基于动能约束，无需精确的接触动力学模型，提供了一种轻量级、可证明安全的形式化保证机制。\n\n## 实验与结果\n**实验设置**：在PyBullet模拟环境中使用UR3e机械臂和Robotiq 2F-85夹爪进行训练和评估。真实世界验证在UR3e机器人平台上进行，配备了相同的夹爪和内置的6轴力/扭矩传感器。招募了12名参与者，使用5种不同形状的小物体，共进行了360次物理交接试验。\n\n**对比基线**：在模拟实验中，与先进的约束RL基线方法进行了对比，包括约束策略优化和SAC-Lagrangian。\n\n**消融实验**：通过改变奖励函数中各组件的权重（RF1-RF5）进行消融研究，以确定最优奖励配置。\n\n![消融研究结果](https://arxiv.org/html/2512.03707v1/Figure4b.png)\n> **图4**：对ContactRL奖励函数变体（RF1–RF5）的消融研究，评估了1000个模拟回合。左图显示了平均累积奖励和任务完成时间；右图显示了接触力分布和安全违规率。RF5（包含到达、安全、急动和接近奖励）实现了高性能和最低的安全违规率（0.2%）。\n\n**关键实验结果**：\n1.  **模拟性能对比**：ContactRL在任务成功率（87.17%）上显著优于CPO（33.33%）和SACLag（55.60%）。其安全违规率（0.20%）与SACLag相当，远低于CPO（2.41%）。在运动平滑度方面，ContactRL的RMS急动度（931.32 m/s³）远低于SACLag（3924.11 m/s³）和CPO（1355.47 m/s³）。\n2.  **护盾有效性**：\n![无护盾策略](https://arxiv.org/html/2512.03707v1/Figure5a.png)\n> **图5(a)**：未使用eCBF护盾的ContactRL策略在100个具有随机动作的模拟回合中的动能和末端执行器速度。显示出持续的振荡和偶发的脉冲。\n![有护盾策略](https://arxiv.org/html/2512.03707v1/Figure5b.png)\n> **图5(b)**：使用eCBF护盾的ContactRL策略。动能和速度曲线更平滑，收敛单调，增强了稳定性和安全性。\n3.  **真实世界验证**：\n![实验设置](https://arxiv.org/html/2512.03707v1/Figure6.png)\n> **图6**：在UR3e机器人平台上进行小物体交接实验的真实设置。\n![接触力分析](https://arxiv.org/html/2512.03707v1/Figure7.png)\n> **图7**：360次真实世界试验中测量的法向接触力 $F_z$。所有试验的最大接触力 $F_{N,\\\\text{max}}$ 和平均接触力 $\\\\overline{F}_{N}$ 均始终低于10N，远低于50N的安全阈值。方差分析表明，物体类型和放置姿态对最大接触力无显著影响。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了ContactRL，一种新颖的基于RL的运动规划框架，通过将基于力反馈的奖励集成到策略学习中，直接解决安全人机接触问题，实现了接触力的主动调制。\n2.  在模拟中，该方法在1000个评估回合中仅出现1次安全违规（0.2%），在成功率、安全性和运动平滑度方面优于先进的约束RL基线。\n3.  在部署时，通过一个基于动能的CBF安全护盾对学习策略进行增强，提供了形式化的安全保证。在真实机器人平台上进行的360次物理交接试验验证了其有效性，测量到的法向接触力始终低于10N。\n\n**局限性**：论文提到，模拟环境使用了简化的人手模型（平面），虽然通过设置正确的物理属性（质量、摩擦）来近似真实接触，但与复杂的人体几何和动力学仍存在差距。此外，eCBF护盾在必要时会缩放末端执行器速度，这可能会略微增加任务完成时间。研究聚焦于特定的“从手中拾取”任务，其通用性有待在更广泛的接触丰富任务中验证。\n\n**启示**：这项工作展示了将接触安全直接集成到RL奖励函数中的可行性，为处理必要物理接触的协作任务开辟了新途径。所采用的“学习策略+形式化安全护盾”的混合架构，为解决RL在安全关键应用中缺乏保证的问题提供了实用范例，即利用学习获得性能与安全的良好权衡，再通过轻量级、可证明的运行时安全机制确保部署的绝对安全边界。未来研究可探索将接触力感知和调控扩展到更复杂、动态的交互场景中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03707v1/Figure1-Problem.png",
        "https://arxiv.org/html/2512.03707v1/Figure2-ContactRL.png",
        "https://arxiv.org/html/2512.03707v1/Figure3-Sim.png",
        "https://arxiv.org/html/2512.03707v1/Figure4b.png",
        "https://arxiv.org/html/2512.03707v1/Figure4a.png",
        "https://arxiv.org/html/2512.03707v1/Figure5a.png",
        "https://arxiv.org/html/2512.03707v1/Figure5b.png",
        "https://arxiv.org/html/2512.03707v1/Figure6.png",
        "https://arxiv.org/html/2512.03707v1/Figure7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03444",
      "title": "PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers",
      "url": "http://arxiv.org/abs/2512.03444",
      "arxivId": "2512.03444",
      "date": "2025-12-03",
      "authors": "Minghui Zheng Team",
      "category": "Manipulation",
      "summary": "本文针对神经运动规划器泛化能力受限、网络架构编码效率低的问题，提出PerFACT框架。其核心包含两个关键技术：一是MotionGeneralizer，利用大语言模型（LLM）自动生成语义可行的多样化工作空间，以合成大规模规划数据集；二是融合动作分块变换器网络（MπNetsFusion），通过融合多模态特征提升规划信号编码能力。基于合成的350万条轨迹进行实验，结果表明，所提出的MπNetsFusion在评估任务上的规划速度比现有先进方法快数倍。",
      "detailedSummary": "## 研究背景与动机\n目前，用于机器人操作臂的神经运动规划器主要依赖在手动生成的小规模工作空间数据集中训练，这限制了其分布外场景的泛化能力。此外，现有规划器通常采用简单的网络架构（如特征拼接）来融合机器人配置空间和工作空间特征，导致学习过程偏向于主导模态（如工作空间嵌入），而忽略了稀疏但关键的规划信号（如当前和目标配置嵌入）。本文针对数据稀缺和网络架构对多模态特征利用不足这两个关键痛点，提出了PerFACT框架。其核心思路是：1）利用大语言模型（LLM）的推理能力引导生成多样化、语义可行的大规模规划数据集；2）设计一种融合动作分块Transformer（Fusion Action-Chunking Transformers）的通用神经运动规划器，以更好地编码规划信号并关注多个特征模态。\n\n## 方法详解\nPerFACT包含两个核心组件：用于大规模数据集合成的MotionGeneralizer和通用神经运动规划器M π NetsFusion。\n\n![PerFACT组件与部署概览](https://arxiv.org/html/2512.03444v1/x1.png)\n> **图1**：PerFACT的各个组件及其实物部署流程。首先，MotionGeneralizer整合程序化基元生成与LLM（GPT-4）的推理能力，为训练通用神经运动规划器生成多样化工作空间。随后，它生成与机器人无关、场景特定的运动规划问题，并与先进运动规划器（如cuRobo）集成以生成大规模规划数据集。该数据集与MotionGeneralizer的感知模态结合，用于训练M π NetsFusion。感知模态也用于M π NetsFusion的开环执行中，以解决评估场景中的规划问题。最后，M π NetsFusion被部署于真实世界规划场景。\n\n**MotionGeneralizer**：这是一个LLM驱动的多样化工作空间生成框架，旨在为训练通用神经运动规划器合成大规模数据。\n\n![MotionGeneralizer框架](https://arxiv.org/html/2512.03444v1/x2.png)\n> **图2**：MotionGeneralizer的多样化工作空间生成框架。该方法首先随机选择机器人类型及其周围桌子的数量（程序化生成）。接着，它以少量示例（Few-shot）方式提示LLM（微调后的大语言模型）确定每张桌子上的基元数量。然后，根据基元池程序化生成建议的基元，并再次以少量示例方式提示LLM，以指定每个基元在其对应桌子上的位置和朝向，最终输出工作空间。该过程可重复以生成任意数量（N）的多样化规划工作空间。\n\n其工作流程如下：\n1.  **程序化资产生成**：借鉴Neural MP和NVIDIA SceneSynthesizer的方法，通过URDF采样生成具有不同尺寸和构型的日常铰接物体（如桌子、橱柜、微波炉）。\n2.  **语言引导的工作空间生成**：\n    *   **机器人与桌子选择**：随机选择机器人类型及其周围桌子的数量，并在机器人可达范围内放置程序化生成的桌子。\n    *   **LLM确定基元数量与类型**：向GPT-4提供机器人规格、桌子表面积和常见家居物体列表等提示，LLM输出每张桌子上应放置的基元数量和类型。若桌子少于四张，LLM还会建议在地面放置额外物体。\n    *   **程序化基元生成**：根据LLM的建议，从NeuralMP或SceneSynthesizer基元池中程序化生成每个物体，确保同类物体在尺寸和构型上具有多样性。\n    *   **LLM引导基元放置**：向LLM提供机器人规格、桌子表面积和每个基元尺寸，LLM输出每个基元在桌子局部坐标系中的语义可行位置和朝向。\n    *   **碰撞处理**：执行后处理碰撞检测算法以解决潜在碰撞。\n3.  **降低生成工作空间间的相似性**：利用GPT-4为每个工作空间生成高级文本描述并转换为嵌入向量，与先前工作空间的嵌入比较相似度，若高于阈值则请求LLM生成新的、更具区分度的配置。\n4.  **感知模态**：为训练和碰撞检查提供高维点云观测。\n    *   通过在障碍物表面均匀采样点，合成工作空间点云（图4-a）。\n    *   通过在机器人操作臂不同构型下的网格上均匀采样点，生成机器人点云（图4-b），以提高空间感知和规划成功率。\n\n![点云合成](https://arxiv.org/html/2512.03444v1/x4.png)\n> **图4**：点云合成：MotionGeneralizer的感知模块通过在（a）工作空间基元和（b）任意构型下的机器人操作臂上均匀采样，提供特权点云场景表示，以增强下游规划任务的空间感知。\n\n5.  **运动规划问题生成器**：为避免生成过于简单的直线运动问题，MotionGeneralizer利用生成的基元形状来提供场景特定的规划问题。它在基元内部和机器人可达区域内采样无碰撞位姿，然后利用机器人特定的碰撞感知逆运动学计算对应的起始-目标配置，从而生成与机器人无关的规划问题（图5）。\n\n![规划问题生成](https://arxiv.org/html/2512.03444v1/x5.png)\n> **图5**：规划问题生成：MotionGeneralizer的问题生成器模块提供与机器人无关、场景特定的规划问题位姿。然后，任何机器人操作臂（如UR5e或Franka）都可以使用其碰撞感知逆运动学来计算规划问题对应的起始-目标配置。\n\n**M π NetsFusion**：这是一种新型规划框架，利用融合Transformer架构来关注不同的规划模态并学习它们之间的交互通信。\n\n![M π NetsFusion架构](https://arxiv.org/html/2512.03444v1/x7.png)\n> **图7**：M π NetsFusion架构：该网络以当前配置、目标配置、当前机器人点云、目标机器人点云和工作空间点云作为输入。每个输入模态通过独立的编码器进行处理。然后，一个融合编码器（基于瓶颈Transformer）建模这些模态之间的交互。融合后的特征被传递到动作分块Transformer（ACT）的解码器中，以预测动作序列。最后，一个多层感知机（MLP）将解码后的特征映射到动作空间。\n\n其技术细节如下：\n*   **整体框架**：基于动作分块Transformer（ACT）框架，但用瓶颈Transformer（Bottleneck Transformer）作为编码器来学习和编码跨模态交互。\n*   **核心创新**：采用瓶颈Transformer来建模每个规划模态的贡献，并限制规划模态间的信息流，确保只交换每个模态中最相关和压缩的表征，从而防止主导模态淹没稀疏信号。\n*   **输入与输出**：输入包括当前配置、目标配置、当前机器人点云、目标机器人点云和工作空间点云。每个模态通过独立编码器处理，然后送入融合编码器。融合后的特征由ACT解码器处理，最终通过MLP预测动作序列。\n\n## 实验与结果\n**实验设置**：使用MotionGeneralizer生成了一个包含350万条轨迹的大规模规划数据集。在模拟环境中进行评估，并进行了实物部署演示。对比的基线方法包括：\n*   **传统采样规划器**：RRT-Connect、RRT*。\n*   **神经运动规划器**：MPNet、M π Nets、Neural MP、SIMPNet。\n\n**关键实验结果**：\n![主要结果对比](https://arxiv.org/html/2512.03444v1/x11.png)\n> **图11**：主要定量结果（表1）：在四个不同难度的规划场景中，对比了M π NetsFusion与基线方法的平均规划时间（秒）。结果表明，M π NetsFusion在所有场景中均显著快于基线方法。\n\n论文指出，在评估的任务中，M π NetsFusion的规划速度比最先进的基于采样的规划器快4.47倍，比神经运动规划器快3.2倍。具体数值如图11（表1）所示，例如在“UR5e - 简单”场景中，M π NetsFusion的平均规划时间为0.02秒，而RRT*为0.52秒，M π Nets为0.15秒。\n\n**消融实验**：\n![消融研究](https://arxiv.org/html/2512.03444v1/x13.png)\n> **图13**：消融研究：展示了M π NetsFusion不同变体（移除融合编码器、使用拼接代替融合编码器、完整模型）在四个场景中的成功率和平均规划时间。完整模型在规划时间和成功率上均表现最佳，验证了融合编码器的有效性。\n\n消融实验（图13）分析了M π NetsFusion各组件贡献：\n1.  **移除融合编码器**：性能显著下降，表明跨模态交互学习至关重要。\n2.  **用简单拼接替代融合编码器**：性能优于移除融合编码器，但不及完整模型，验证了瓶颈Transformer在模态感知融合上的优势。\n3.  **完整M π NetsFusion**：在成功率和规划时间上均达到最佳性能。\n\n**定性结果**：\n![定性结果](https://arxiv.org/html/2512.03444v1/x14.png)\n> **图14**：定性结果：展示了M π NetsFusion与基线方法（RRT-Connect、M π Nets）在四个评估场景中的规划路径。M π NetsFusion规划出的路径更平滑、更高效。\n\n**与现有工作空间生成方法对比**：\n![MotionGeneralizer vs. MotionBenchMaker](https://arxiv.org/html/2512.03444v1/x6.png)\n> **图6**：MotionGeneralizer与MotionBenchMaker对比：MotionBenchMaker仅包含8个不同工作空间，且每个场景只有一个主要障碍物（上排）。MotionGeneralizer可提供任意数量的工作空间，且由LLM决定每个场景的障碍物数量（下排仅展示8个以作比较），从而产生更真实、更杂乱的工作空间。\n\n## 总结与启发\n**核心贡献**：\n1.  **MotionGeneralizer**：提出了一种新颖的、LLM增强的多样化工作空间生成框架，通过结合程序化生成和LLM的语义推理，能够合成大规模、语义可行的规划数据集，解决了神经运动规划器的数据稀缺问题。\n2.  **M π NetsFusion**：提出了一种通用的神经运动规划器，采用融合动作分块Transformer架构，特别是瓶颈Transformer作为融合编码器，能够自适应地关注和处理不同的规划感知模态，有效提升了规划效率和成功率。\n\n**局限性**：论文提到，MotionGeneralizer依赖于LLM（GPT-4）进行推理，这可能会引入与LLM性能相关的偏差或成本。此外，在模拟中训练并使用合成点云，与真实世界中可能存在的遮挡、噪声点云之间存在差距，需要微调以适应真实传感数据。\n\n**启示**：本研究展示了LLM在低层机器人运动规划数据合成中的潜力，为构建大规模、高质量的机器人规划数据集提供了新思路。同时，针对多模态特征融合的网络架构设计（如瓶颈Transformer）是提升神经运动规划器性能的关键方向，为后续开发更通用、鲁棒的机器人规划基础模型奠定了基础。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03444v1/x1.png",
        "https://arxiv.org/html/2512.03444v1/x2.png",
        "https://arxiv.org/html/2512.03444v1/x3.png",
        "https://arxiv.org/html/2512.03444v1/x4.png",
        "https://arxiv.org/html/2512.03444v1/x5.png",
        "https://arxiv.org/html/2512.03444v1/x6.png",
        "https://arxiv.org/html/2512.03444v1/x7.png",
        "https://arxiv.org/html/2512.03444v1/x8.png",
        "https://arxiv.org/html/2512.03444v1/x9.png",
        "https://arxiv.org/html/2512.03444v1/x10.png",
        "https://arxiv.org/html/2512.03444v1/x11.png",
        "https://arxiv.org/html/2512.03444v1/x12.png",
        "https://arxiv.org/html/2512.03444v1/x13.png",
        "https://arxiv.org/html/2512.03444v1/x14.png",
        "https://arxiv.org/html/2512.03444v1/x15.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03347",
      "title": "GOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation",
      "url": "http://arxiv.org/abs/2512.03347",
      "arxivId": "2512.03347",
      "date": "2025-12-03",
      "authors": "Nima Fazeli Team",
      "category": "Manipulation",
      "summary": "本文针对模仿学习（IL）在精确操作任务中因累积误差导致轨迹精度不足的核心问题，提出了**抓取物体流形投影（GrOMP）**方法。该方法通过从专家演示中学习一个低维任务空间流形，并将IL策略产生的轨迹投影到该流形上，以消除与流形正交的累积误差。关键技术包括基于主测地线分析（PGA）构建流形，并引入基于多臂老虎机的交互式组件进行流形选择优化。论文在四个使用触觉反馈的真实机器人精确装配任务上验证了该框架。",
      "detailedSummary": "## 研究背景与动机\n模仿学习是学习重复性操作任务的有力工具，其中基于扩散策略的行为克隆方法能够生成多模态轨迹。然而，所有模仿学习方法都存在一个根本性的局限性：累积误差。随着策略的执行，策略中的微小误差会不断累积，导致行为偏离预期。这在需要高精度的工业装配任务中尤为致命，因为无法保证达到制造业要求的成功率。当前装配自动化高度依赖专用夹具来确保可重复性能，但这在部件设计升级时成本高昂。本文考虑一种更灵活的自动化方法，即使用非专用夹爪进行操作，此时无法假设刚性抓握，物体在手内的姿态也会产生累积误差。本文针对非刚性抓握下精密装配任务中因物体滑动和接触不确定性导致的误差累积问题，提出了一个新视角：将抓握物体的状态约束到一个从专家演示中学习到的低维任务流形上。其核心思路是，在模仿学习策略之上引入一个交互式框架，通过将预测的物体姿态投影到任务流形上来消除垂直于该流形切空间的累积误差。\n\n## 方法详解\nGrOMP框架在已训练的基础模仿学习策略之上运行，通过将抓握物体的轨迹投影到一个低维任务流形来提供修正。其整体流程如论文图1所示：基于视觉的触觉传感器提供剪切位移场和原始RGB图像；剪切场和本体感知作为扩散策略的输入模态；RGB图像用于估计手内物体姿态；扩散策略预测的机器人轨迹与估计的物体姿态相结合，用于计算物体在世界坐标系下的预测轨迹；该轨迹随后被投影到一个通过主测地线分析从专家数据中推导出的任务空间；一个七臂赌博机机制根据任务执行的成功率来调整该投影所选择的流形维度。\n\n![方法框架](https://arxiv.org/html/2512.03347v2/figs/teaser.png)\n\n> **图1**：GrOMP方法整体框架概述。左侧为多模态输入（触觉剪切场、RGB图像、本体感知），中间为扩散策略和手内姿态估计模块，右侧展示了基于专家数据学习任务流形并进行投影修正，以及通过赌博机进行交互式调整的流程。\n\n核心模块包括：\n1.  **任务流形投影**：目标是找到一个映射 \\( \\mathbf{F}_{\\mathbf{s}_{o}} \\)，使得机器人的末端位姿 \\( \\mathbf{T}_{st} \\) 能够确保物体位姿 \\( \\mathbf{T}_{so} \\) 被投影到任务流形 \\( \\mathcal{T} \\) 上。具体计算为 \\( \\mathbf{T}_{st} = (\\text{proj}_{\\mathcal{T}}\\mathbf{T}_{so})\\mathbf{T}_{to}^{-1} \\)。这里的关键创新在于对物体位姿（而非机器人位姿）施加流形约束，因为对于非刚性抓握，即使物体静止，机器人仍可能运动。\n2.  **从演示推导任务空间**：使用主测地线分析（PGA）从专家演示数据中自动学习任务流形 \\( \\mathcal{T} \\)。首先计算所有演示物体位姿的测地均值，然后利用对数映射将位姿映射到切空间得到扭曲向量，并进行归一化处理。接着对归一化扭曲矩阵进行主成分分析（PCA），得到PCA变换矩阵 \\( \\mathbf{P} \\)。通过保留前 \\( i \\) 个主成分，可以定义维度为 \\( i \\) 的候选投影流形（\\( i = 0, ..., 6 \\)）。为权衡自由度减少带来的约束力与可能引入的表达误差，定义了一个投影损失先验 \\( \\mathcal{L}_{\\text{proj}}^{i} \\)（公式7），该损失是归一化扭曲重构误差与维度惩罚项 \\( i/6 \\) 之和。\n\n![投影损失与示例](https://arxiv.org/html/2512.03347v2/figs/projection.png)\n\n> **图2**：(a) 根据公式7计算的、在四个测试任务专家数据集上得到的投影损失先验。(b) 螺母拧入任务中，专家数据集的物体和机器人轨迹沿选定流形（i=2）的投影示例。(c) 扩散策略预测的动作与经GrOMP投影修正后的动作在物体当前观测姿态下的可视化对比（以动作轨迹的最后一个点表示）。\n\n3.  **七臂赌博机调整**：由于基于数据集的先验损失可能次优，本文引入一个非平稳的七臂赌博机进行交互式调整。每个臂对应一个投影维度 \\( i \\)。每个维度 \\( i \\) 的价值 \\( Q_k(i) \\) 初始化为 \\( 1 - \\mathcal{L}_{\\text{proj}}^{i} \\)。在每轮 \\( K \\) 次试验后，根据所选维度 \\( i_k^* \\) 的成功率 \\( R_k(i_k^*) \\) 更新其价值（公式8）。使用 \\( \\epsilon \\)-贪心策略选择下一个要测试的维度 \\( i_k^* \\)（公式9），即以 \\( 1-\\epsilon \\) 的概率选择当前价值最高的维度，以 \\( \\epsilon \\) 的概率随机探索。\n4.  **轨迹执行**：在策略执行时，首先通过基础IL策略和手内姿态估计得到预测的物体位姿序列，将其转换为归一化扭曲矩阵 \\( \\hat{\\mathbf{\\Xi}} \\)。然后，根据赌博机选定的维度 \\( i_k^* \\)，使用PCA矩阵 \\( \\mathbf{P} \\) 的前 \\( i_k^* \\) 列将 \\( \\hat{\\mathbf{\\Xi}} \\) 投影到降维空间，再映射回原始切空间，最后通过指数映射还原为SE(3)位姿，即得到投影后的物体位姿 \\( \\text{proj}_{\\mathcal{T}}\\mathbf{T}_{so}^{\\hat{\\pi}} \\)，进而计算所需的机器人位姿。\n\n## 实验与结果\n**实验设置**：在四个真实的精密装配任务上测试GrOMP：螺母拧入、销钉插入、USB插入和电池盖安装。基线方法是纯扩散策略（DP）。实验采用交互式模仿学习流程，初始使用10条演示训练策略，随后每进行10轮试验便增加演示数据重新训练策略（演示数量增至20、40等）。基础IL策略采用扩散策略，使用触觉剪切场和本体感知作为多模态输入。手内物体姿态 \\( \\mathbf{T}_{to} \\) 通过一个基于卷积变分自编码器和MLP的模块从触觉RGB图像中估计得到。\n\n**关键结果**：\n- **螺母拧入**：在30次试验中，纯DP的成功率为26.7%，而GrOMP的成功率达到93.3%。赌博机最终收敛于选择2维投影，这与任务所需的旋转和平移自由度直觉一致。\n\n![螺母拧入结果](https://arxiv.org/html/2512.03347v2/figs/threading_results.png)\n\n> **图5**：螺母拧入任务的成功率对比。左图显示GrOMP（蓝色）显著且持续优于纯DP（橙色）。右图展示了赌博机对各投影维度价值的估计演化过程，最终收敛于i=2。\n\n- **销钉插入**：在30次试验中，纯DP成功率为53.3%，GrOMP将其提升至86.7%。赌博机最终选择了1维投影。\n\n![销钉插入结果](https://arxiv.org/html/2512.03347v2/figs/insertion_results.png)\n\n> **图6**：销钉插入任务的成功率对比与赌博机价值演化。GrOMP同样表现出显著优势，赌博机收敛于i=1。\n\n- **USB插入**：这是一个更具挑战性的任务。纯DP取得了33.3%的成功率，而GrOMP将其大幅提升至80.0%。赌博机为USB任务选择了3维投影。\n\n![USB插入结果](https://arxiv.org/html/2512.03347v2/figs/usb_results.png)\n\n> **图7**：USB插入任务的成功率对比与赌博机价值演化。GrOMP的优势非常明显，赌博机选择了更高维度（i=3）的投影。\n\n- **电池盖安装**：在30次试验中，纯DP成功率为63.3%，GrOMP成功率为83.3%。赌博机选择了2维投影。\n\n![电池盖结果](https://arxiv.org/html/2512.03347v2/figs/remote_results.png)\n\n> **图8**：电池盖安装任务的成功率对比与赌博机价值演化。GrOMP保持了性能优势。\n\n**消融实验分析**：虽然论文没有独立的消融实验章节，但赌博机的调整过程本身验证了自适应选择投影维度的必要性。图2(a)显示不同任务的最佳投影损失先验维度不同，而图5-8的右图则表明，赌博机通过学习最终收敛的维度与先验最小值并不完全一致（例如螺母拧入任务先验最小在i=1，但赌博机收敛于i=2），这证明了交互式调整能够超越静态先验，找到实际部署中最有效的投影维度。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**抓握物体流形投影**框架，通过将物体（而非机器人）状态约束到从专家数据中学习的低维任务流形，有效抑制了模仿学习在非刚性抓握精密装配任务中的累积误差。\n2.  设计了一种**基于七臂赌博机的交互式调整机制**，能够自适应地为特定任务选择最优的流形投影维度，其性能超越了仅基于数据集计算的静态先验。\n\n**局限性**：论文自身提到，GrOMP的性能依赖于手内物体姿态估计模块 \\( \\mathbf{IHP} \\) 的质量。此外，从专家数据推导出的流形维度先验可能不是最优的，需要交互式调整来弥补。\n\n**后续研究启示**：\n1.  **利用任务结构先验**：对于受机械约束的任务，显式地建模和利用其低维流形结构是提高模仿学习精度和鲁棒性的有效途径。\n2.  **交互式优化机制**：将简单的交互式学习组件（如赌博机）与模仿学习结合，可以以较小的额外交互成本显著提升策略在真实环境中的表现，这是一种实用的改进思路。该方法对输入模态保持中立，可扩展至视觉等其他传感模式。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03347v2/figs/teaser.png",
        "https://arxiv.org/html/2512.03347v2/figs/projection.png",
        "https://arxiv.org/html/2512.03347v2/figs/ihp.png",
        "https://arxiv.org/html/2512.03347v2/figs/demonstration.png",
        "https://arxiv.org/html/2512.03347v2/figs/threading_results.png",
        "https://arxiv.org/html/2512.03347v2/figs/insertion_results.png",
        "https://arxiv.org/html/2512.03347v2/figs/usb_results.png",
        "https://arxiv.org/html/2512.03347v2/figs/remote_results.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03422",
      "title": "What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models",
      "url": "http://arxiv.org/abs/2512.03422",
      "arxivId": "2512.03422",
      "date": "2025-12-03",
      "authors": "Weidong Chen Team",
      "category": "Manipulation",
      "summary": "本论文核心问题是探讨机器人学中最优的3D场景表示方法。全面综述了传统几何表示（如点云、体素、SDF）与神经表示（如NeRF、3DGS）以及新兴基础模型，指出传统稀疏表示主导当前SLAM与定位，而神经表示能集成语义特征和语言先验，提升场景理解与智能。通过比较感知、建图等五大模块中的优缺点，论文预测3D基础模型可能成为未来统一解决方案，但完全实现仍面临挑战。",
      "detailedSummary": "## 研究背景与动机\n机器人领域的当前主流SLAM与定位系统主要依赖于点云和体素等稀疏场景表示。然而，这些传统离散表示难以生成密集、连续的3D模型，限制了其在导航、避障等下游复杂具身智能任务中的应用。同时，新兴的神经表示（如NeRF、3DGS）和基础模型（Foundation Models）因其在整合高级语义特征和语言先验方面的潜力，为实现更全面的3D场景理解提供了新可能。本文针对“何种3D场景表示最适合机器人不同功能模块”这一具体问题，提出了一个系统性的比较与分类新视角。其核心思路是通过全面梳理从几何表示到基础模型的发展脉络，并依据机器人五大核心模块（感知、建图、定位、导航、操作）的需求，系统分析各类场景表示的优劣与适用性。\n\n## 方法详解\n本文并非提出一种新的算法框架，而是构建了一个用于系统分析与比较不同3D场景表示方法的分类与评估体系。其整体论述框架围绕机器人系统的五大功能模块展开，对每种场景表示在这些模块中的应用进行梳理和对比。\n\n![发展时间线与分类](https://arxiv.org/html/2512.03422v2/img/trend2.png)\n\n> **图1**：3D场景表示在机器人学中的发展时间线与应用分类。该图纵轴按时间顺序列出了点云、体素、网格、面元、场景图、符号距离场（SDF）以及最新的NeRF、3DGS等表示方法。横轴则对应机器人的五大核心模块：感知、建图、SLAM、定位、规划/导航、操作与仿真。图中的每个标记点代表了在该模块中使用相应表示方法的一项具体研究工作，并附有引用编号。此图提供了从历史演进和功能应用两个维度审视场景表示的全面视角。\n\n论文首先在第II节对各类场景表示进行了标准化的数学表述，这是进行比较的基础。例如，点云被定义为无序的3D点集 \\( P=\\{p_i\\} | p_i \\in \\mathbb{R}^3 \\)；体素网格通过插值函数 \\(\\text{interp}(x, V)\\) 查询属性；符号距离函数（SDF）定义为 \\( SDF(x)=s: x \\in \\mathbb{R}^3, s \\in \\mathbb{R} \\)，零值等值面隐含地表征表面；场景图定义为包含对象集 \\( O \\)、关系集 \\( R \\) 和边集 \\( E \\) 的元组 \\( SG=(O, R, E) \\)，并可扩展为分层多图。\n\n对于神经表示，神经辐射场（NeRF）使用多层感知机（MLP）将5D输入（位置和视角方向）映射为颜色和体密度，即 \\( F_{\\theta}(\\mathbf{x}, \\mathbf{d})=(\\mathbf{c}, \\sigma) \\)，并通过体渲染公式合成图像。3D高斯泼溅（3DGS）则采用显式的、由数百万个可学习的3D高斯椭球构成的场景表示，每个高斯由位置 \\( \\mu \\)、协方差矩阵 \\( \\Sigma \\)（分解为旋转 \\( R \\) 和缩放 \\( S \\)）、不透明度 \\( \\alpha \\) 和球谐函数（SH）系数定义，并通过高效的栅格化进行渲染。基础模型（或Tokenizer表示）将复杂场景信息编码为一组离散令牌，通常结合Transformer架构与大语言模型（LLM），以实现高效处理和强大的泛化能力。\n\n![结构与应用总览](https://arxiv.org/html/2512.03422v2/x2.png)\n\n> **图3**：机器人3D场景表示的结构与应用总览。该图直观展示了论文的组织结构：从基础的3D场景表示（第III节）出发，分别论述其在感知（第IV节）、建图与定位（第V节）以及交互（包含导航与操作，第VI节）模块中的应用。子图(a-h)展示了各模块的具体应用实例。\n\n本文的核心创新点在于其系统性的分类与比较视角。与现有综述多聚焦于单一类型表示（如NeRF或基础模型）不同，本文横跨了从传统几何表示到最新神经与基础模型的全部谱系，并首次将它们统一置于机器人完整功能模块的评估框架下。图1所示的分类矩阵是这一方法的核心体现，它使得研究者能够清晰地看到每种表示方法在机器人不同任务中的研究热度与应用潜力。\n\n![表示方法多维度比较](https://arxiv.org/html/2512.03422v2/img/figure7.png)\n\n> **图4**：不同场景表示在多维度上的比较。该图从数据形式（显式/隐式）、连续性、内存使用、渲染真实感、灵活性和几何表示能力六个维度，对点云、体素、网格、SDF、NeRF和3DGS进行了定性对比。趋势表明，神经表示（NeRF，3DGS）在渲染真实感和连续性上优势明显，而传统几何表示在内存效率和灵活性上各有特点。此图为选择适合特定任务的场景表示提供了直观参考。\n\n## 实验与结果\n作为一篇综述性论文，本文并未进行传统的定量对比实验，而是通过文献梳理和定性分析来呈现“结果”。其“实验平台”是整个机器人研究领域已发表的工作集合。本文的“Baseline”是隐含的，即不同场景表示方法在历史上和当前研究中表现出的固有特性及其在各项任务中的适用性。\n\n关键“实验结果”体现在对领域发展脉络和趋势的总结上。例如，通过分析Web of Science的论文数量趋势（图2），本文指出社区的研究焦点已逐渐从NeRF转向3DGS，并最终朝向基础模型发展。\n\n![研究趋势分析](https://arxiv.org/html/2512.03422v2/x1.png)\n\n> **图2**：基于Web of Science的神经场景表示（NeRF, 3DGS, 基础模型）在机器人领域相关论文数量趋势分析。该图清晰展示了自NeRF被引入以来，相关研究数量呈明显上升趋势，且关注点从NeRF逐渐过渡到3DGS和基础模型。\n\n本文的核心“消融实验”等价于其对各种表示方法在多维度上的系统比较（如图4）。通过这种比较，可以总结出每个“组件”（即每种表示方法）的“贡献”与局限：点云和体素等传统表示在SLAM和定位中因效率高而占主导，但稀疏或离散；SDF能提供连续表面，利于导航规划；NeRF能生成高真实感、连续的场景，但训练和渲染慢；3DGS在保持高质量渲染的同时实现了实时性，是效率与质量的折中；场景图擅长表达语义关系；而基础模型则展现出整合多模态信息、实现零样本泛化和高级认知任务的巨大潜力。\n\n论文在后续各模块的章节中，通过引用大量具体文献，进一步实证了上述比较。例如，在建图模块，对比了不同表示在静态场景、大规模户外场景和动态场景中的重建能力；在操作模块，对比了基于点云、NeRF、3DGS和基础模型的抓取框架在实时性、泛化性和任务复杂度处理上的差异。\n\n## 总结与启发\n本文的核心贡献有三点：1) 提供了一份全面、最新的关于机器人3D场景表示的综述与评估，首次系统地将传统几何表示与前沿神经、基础模型表示置于统一的机器人功能模块框架下进行比较；2) 指出了3D基础模型有潜力成为未来机器人应用的统一解决方案，并探讨了实现这一目标所面临的剩余挑战；3) 建立并维护了一个开源项目，持续汇集该领域的最新研究成果，为社区提供了宝贵的资源。\n\n论文自身明确提到了当前研究的局限性，尤其是对于未来的统一解决方案——3D基础模型。其面临的挑战包括：如何有效整合几何、外观与语义信息；如何处理大规模、动态环境；如何实现高效推理以满足机器人实时性要求；以及如何确保其在安全关键应用中的可靠性。\n\n本文对后续研究的启示是深远的。它表明，不存在一种“放之四海而皆准”的最佳表示，选择取决于具体的机器人模块和任务需求。未来研究趋势将是混合表示（Hybrid Representations）和基础模型驱动的统一表示。研究者应致力于开发能够自适应选择或融合不同表示优势的方法，并积极探索基础模型如何作为核心“大脑”来理解和推理多模态场景表示，最终实现更智能、更通用的机器人系统。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03422v2/img/trend2.png",
        "https://arxiv.org/html/2512.03422v2/x1.png",
        "https://arxiv.org/html/2512.03422v2/x2.png",
        "https://arxiv.org/html/2512.03422v2/img/figure7.png",
        "https://arxiv.org/html/2512.03422v2/x3.png",
        "https://arxiv.org/html/2512.03422v2/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03438",
      "title": "Multimodal Reinforcement Learning with Agentic Verifier for AI Agents",
      "url": "http://arxiv.org/abs/2512.03438",
      "arxivId": "2512.03438",
      "date": "2025-12-03",
      "authors": "Jianfeng Gao Team",
      "category": "Manipulation",
      "summary": "本文针对多模态强化学习（MMRL）中奖励信号稀疏、难以提供细粒度指导的问题，提出了一种名为Argos的智能验证器。该方法的核心是自适应地为每个训练样本从一组教师模型和规则派生的评分函数中选择合适的工具，同时评估最终答案准确性、所指实体与动作的时空定位以及推理过程质量。实验表明，在SFT数据筛选和RL训练中使用Argos，能在空间推理、视觉幻觉及机器人等具身AI任务上取得最先进性能，有效防止训练中智能体崩溃为无根据的解决方案，并减少奖励黑客行为。",
      "detailedSummary": "## 研究背景与动机\n当前，使用多模态强化学习（MMRL）训练的智能推理模型能力日益增强，但几乎普遍采用基于最终答案计算的稀疏结果奖励进行优化。基于推理过程生成的令牌计算更丰富的奖励，可以通过提供更细粒度的指导来显著改善学习效果。然而，在多模态强化学习中，超越结果奖励计算更具信息量的奖励面临挑战，因为不同样本可能需要不同的评分函数，且教师模型可能提供噪声信号。本文针对这一痛点，提出了Argos（Agentic Reward for Grounded & Objective Scoring）——一个用于训练智能任务多模态推理模型的原则性奖励智能体。其核心思路是：设计一个能根据每个训练样本自适应选择教师模型和基于规则的评分函数的智能验证器，以同时评估最终答案准确性、时空定位和推理过程质量，从而提供聚合的、可验证的奖励信号。\n\n## 方法详解\n整体框架如图1所示，Argos智能验证器在RL阶段根据训练样本自适应选择不同的评分工具，用于训练智能基础模型，最终在具身任务规划、空间推理等多个智能基准上进行评估。\n\n![方法框架](https://arxiv.org/html/2512.03438v1/x1.png)\n> **图1**：使用Argos智能验证器进行多模态强化学习的整体框架。左侧展示了在RL训练阶段，Argos根据样本自适应选择评分工具来评估模型响应；右侧展示了在多个智能基准（如具身任务规划与完成、空间推理）上对最终模型进行评估。\n\nArgos被定义为一个大型多模态模型（LMM）智能体，它从一个包含K个评分函数的集合中，为每个训练样本选择并计算一个聚合的奖励分数。其验证流程如图2所示。\n\n![验证流程](https://arxiv.org/html/2512.03438v1/x2.png)\n> **图2**：Argos验证过程。对图像和视频使用同一套评分函数。首先解析响应以提取生成的2D点、时间片段、推理文本和答案；然后，智能验证器根据提取的信息自适应决定调用哪些评分函数；最后，使用门控聚合函数汇总得分。\n\n具体而言，对于一个包含视觉输入v、问题q、模型生成的推理轨迹r和预测答案y^的样本，Argos的工作流程如下：\n1.  **解析**：调用解析函数从整个响应中提取关键信息，包括一组2D空间点和时间片段。\n2.  **自适应评分**：根据训练样本，Argos自适应地组合一个多目标奖励过程，选择相关工具对响应进行评分。核心评分模块包括：\n    *   **空间奖励 (R_spatial)**：旨在评估图像中2D点的定位准确性。首先从模型生成的响应中提取一组N个2D点P（包含坐标和预测物体标签）。对于第i个点，使用开放词汇目标检测模型g_θ根据预测的物体标签o_i生成一个伪真值边界框b_i*。然后，使用分割教师模型h_ϕ在b_i*内提取细粒度分割掩码M_i。空间基础得分s_i为指示函数，判断点(x_i, y_i)是否落在掩码M_i内（值为1）。最终R_spatial为所有点得分的平均值。对于包含合成内容（如条形图）的图像，会先使用一个指向模型f_ϑ生成2D点。\n    *   **时间奖励**：扩展至视频。从响应中提取帧级观察F（包含时间戳、坐标和物体）和段级事件E（包含起止时间/帧索引和事件描述）。帧级得分S_f使用与空间奖励相同的模型（f_ϑ, g_θ, h_ϕ）计算。段级得分S_e则通过查询一个强大的教师推理模型T，评估事件描述d_i与对应视频片段V_ti_start:ti_end之间的视觉语义准确性，返回二元分数。最终视频基础得分是S_f和S_e各自集合内均值的未加权平均。\n    *   **推理质量奖励 (R_reasoning)**：评估生成的推理轨迹r与最终答案y^之间的逻辑一致性。使用一个更大的教师模型，计算在给定问题q、推理轨迹r和视觉输入v的条件下，预测响应y^的条件概率作为奖励。\n    *   **结果奖励 (R_acc)**：基于真实答案y*计算。根据问题类型采用不同方法：多项选择题或短答案使用精确字符串匹配；浮点数答案使用5%容差的相对误差计算；其他情况则使用语言模型判断语义等价性。\n3.  **门控聚合**：使用一个门控函数聚合所选评分函数的奖励，以防止潜在的噪声奖励使最终答案偏离正确结果。公式如论文式(9)所示：当结果奖励R_acc低于阈值τ时，最终奖励R_final即为R_acc；当R_acc ≥ τ时，R_final为结果奖励、视觉基础奖励和推理质量奖励的加权平均。这确保了模型必须首先获得正确答案，才能从细粒度的中间奖励中受益。\n\n**训练方法**：使用聚合后的奖励R_final，采用GRPO算法更新策略模型π_θ。在每组rollout奖励值中计算优势A_i，并最小化策略比率裁剪后的目标函数与参考策略之间的KL散度。\n\n**理论依据**：论文从帕累托最优的角度提供了理论证明。即使每个单独的奖励估计器是弱且有噪声的，只要它们是独立且均值归零的，通过加权聚合多个奖励信号，随着奖励数量m的增加，策略能够以高概率逼近全局的δ-帕累托最优解。\n\n## 实验与结果\n**实验设置**：模型基于开源的Qwen2.5-VL 7B模型构建，在自建的数据集上进行监督微调（SFT）和GRPO强化学习训练。在零样本设置下评估了多个智能基准，包括空间推理、视觉幻觉减少、具身AI和机器人任务。\n\n**对比基线**：包括Qwen2.5-VL 7B（基础版及思维链提示版）、Video-R1（仅SFT版及RL版）。\n\n**关键实验结果**：\n1.  **空间推理**：结果如表1所示。在BLINK数据集上，Argos达到56.0%的准确率，优于基础模型（54.4%）和Video-R1 RL（49.0%）。在MindCube-t数据集上达到39.6%，显著优于基础模型（34.9%）。在CV-Bench的2D和3D子集上分别达到78.2%和82.0%，均优于所有对比基线，表明2D视觉接地训练能增强3D理解泛化能力。\n2.  **减少视觉幻觉**：在POPE基准上，Argos的“是/否”准确率达到88.8%，F1分数达到89.0%，均显著高于Qwen2.5-VL 7B（85.5%, 86.1%）和Video-R1 RL（86.1%, 86.7%）。\n3.  **具身AI与机器人**：在EmbodiedBench上，Argos在高级任务规划（65.6% vs 基线59.8%）和低级技能规划（69.7% vs 基线63.5%）上均取得最佳结果。在CALVIN机器人操作基准上，Argos的成功率为65.0%，优于Video-R1 RL（57.5%）。\n\n![消融实验-视觉接地奖励曲线](https://arxiv.org/html/2512.03438v1/figures/visual_grounding_accuracy_reward_curves.png)\n> **图4**：不同奖励配置下，模型在RL训练过程中视觉接地准确率的变化曲线。红线（使用Argos全奖励）的准确率最高且稳定上升，而仅使用结果奖励（绿线）的视觉接地能力几乎没有提升，验证了空间奖励组件的必要性。\n\n![消融实验-准确率奖励曲线](https://arxiv.org/html/2512.03438v1/figures/accuracy_reward_curves.png)\n> **图5**：不同奖励配置下，模型在RL训练过程中答案准确率的变化曲线。使用Argos全奖励（红线）的准确率最终最高，且学习曲线更平滑，表明多目标奖励有助于稳定训练并提升最终性能。\n\n4.  **消融实验**：图4和图5的消融实验表明，仅使用结果奖励（R_acc）会导致模型在RL过程中视觉接地能力崩溃（图4绿线），而加入空间奖励（R_spatial）能有效提升接地能力（图4蓝线），同时结合推理质量奖励（R_reasoning）能带来最佳的准确率性能和稳定的学习曲线（图5红线）。这验证了Argos中各个奖励组件的贡献。\n5.  **定性结果**：图8至图15展示了模型生成的响应及其对应的2D点标注示例，直观显示了模型能够进行准确的空间指代。\n\n![SFT数据组成](https://arxiv.org/html/2512.03438v1/figures/sft_training_mix_distribution_pie_chart.png)\n> **图6**：SFT训练数据集的组成分布饼图，显示了来自不同来源（如GLM-4.1V, Gemini）及不同类型（图像、视频）数据的比例。\n\n![RL数据组成](https://arxiv.org/html/2512.03438v1/figures/rl_training_mix_distribution_pie_chart.png)\n> **图7**：RL训练数据集的组成分布饼图，与SFT数据集不重叠，同样展示了多来源和多模态的数据混合。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了Argos智能验证器框架，用于在数据筛选和MMRL训练阶段提供自适应、多目标、可验证的奖励，解决了多模态强化学习中奖励信号稀疏和噪声的问题。\n2.  引入了一种新颖的、时空视觉接地的推理数据生成和筛选流程，并实证表明仅靠高质量的SFT数据不足以防止模型在RL中崩溃，必须在RL阶段进行在线验证。\n3.  在多个智能基准（空间推理、减少幻觉、具身任务完成和机器人操作）上实现了最先进的性能，证明了该方法的有效性，并提供了基于帕累托最优的理论依据。\n\n**局限性**：论文提到，尽管使用了强大的教师模型，其数据生成流程的产出率（yield rate）仍然较低（约3.1%），表明生成可靠、接地的推理轨迹本身具有挑战性。\n\n**后续启示**：Argos的模块化架构使其能够自然地扩展到新的模态和目标。随着特定任务的教师模型不断改进，Argos有潜力计算更具信息量的奖励信号，从而训练出更强大、更鲁棒的多模态推理智能体。这项工作为多模态强化学习中的奖励设计提供了一个新的、以智能体验证为核心的研究方向。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03438v1/x1.png",
        "https://arxiv.org/html/2512.03438v1/x2.png",
        "https://arxiv.org/html/2512.03438v1/x3.png",
        "https://arxiv.org/html/2512.03438v1/figures/visual_grounding_accuracy_reward_curves.png",
        "https://arxiv.org/html/2512.03438v1/figures/accuracy_reward_curves.png",
        "https://arxiv.org/html/2512.03438v1/figures/sft_training_mix_distribution_pie_chart.png",
        "https://arxiv.org/html/2512.03438v1/figures/rl_training_mix_distribution_pie_chart.png",
        "https://arxiv.org/html/2512.03438v1/figures/im_visuals/a2b63f5f-7497-5d63-bcf0-184e412b4120.jpg",
        "https://arxiv.org/html/2512.03438v1/figures/im_visuals/a2b63f5f-7497-5d63-bcf0-184e412b4120_annotated.jpg",
        "https://arxiv.org/html/2512.03438v1/figures/im_visuals/ee16c240-ac4d-554d-9ad5-607de4b981d4.jpg",
        "https://arxiv.org/html/2512.03438v1/figures/im_visuals/ee16c240-ac4d-554d-9ad5-607de4b981d4_annotated.jpg",
        "https://arxiv.org/html/2512.03438v1/figures/im_visuals/1f82389b-dc45-5176-8278-3167dad5016e.jpg",
        "https://arxiv.org/html/2512.03438v1/figures/im_visuals/1f82389b-dc45-5176-8278-3167dad5016e_annotated.jpg",
        "https://arxiv.org/html/2512.03438v1/figures/im_visuals/5df51f14-2f5e-56ee-a3bf-39aaa555d791.jpg",
        "https://arxiv.org/html/2512.03438v1/figures/im_visuals/5df51f14-2f5e-56ee-a3bf-39aaa555d791_annotated.jpg",
        "https://arxiv.org/html/2512.03438v1/x4.png",
        "https://arxiv.org/html/2512.03438v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03044",
      "title": "Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling",
      "url": "http://arxiv.org/abs/2512.03044",
      "arxivId": "2512.03044",
      "date": "2025-12-02",
      "authors": "Shanghang Zhang Team",
      "category": "Manipulation",
      "summary": "本文提出Video2Act，解决现有视频扩散模型（VDM）用于机器人策略学习时，未能充分利用其帧间连贯且物理一致的运动表示的问题。方法上，设计异步双系统：慢系统（VDM）显式提取前景边界与帧间运动变化，过滤背景噪声；快系统（扩散变换器动作头）接收上述运动感知条件，实现高频稳定控制。实验表明，Video2Act在模拟和真实任务中的平均成功率分别超越之前最佳方法7.7%和21.7%，并展现出强泛化能力。",
      "detailedSummary": "## 研究背景与动机\n当前，机器人策略学习的主流方法主要依赖于视觉语言动作（VLA）模型，这些模型通常使用静态图像编码器（如CLIP、SigLIP）提取视觉特征，然后与语言指令融合来指导动作生成。然而，这类方法缺乏对时序动态和因果依赖的建模能力。近期，视频扩散模型（VDM）因其在建模物理世界动态方面的强大能力而受到关注，一些工作开始尝试将VDM的原始表征直接引入策略学习以增强场景理解。但现有方法普遍忽视了VDM原始特征中天然编码的、连贯且物理一致的空间结构和运动信息，这些信息本可作为更高效、更具信息量的条件来指导动作学习。\n\n本文针对这一具体痛点，提出应显式地挖掘和利用VDM中蕴含的空间与运动表征。核心思路是：首先分析并验证VDM特征在机器人场景下对前景物体结构和运动一致性的捕捉能力；然后设计一种异步双系统框架，其中慢速的VDM系统（System 2）显式提取并精炼空间与运动表征，作为条件输入给高速的扩散Transformer动作头（System 1），从而实现实时、稳定且适应性强的机器人控制。\n\n## 方法详解\nVideo2Act的整体框架是一个异步双系统设计，旨在高效整合VDM的时空感知能力与实时动作生成需求。\n\n![方法框架](https://arxiv.org/html/2512.03044v1/x3.png)\n> **图3**：Video2Act框架。采用异步双系统框架，包含一个慢速感知VDM（System 2）和一个快速动作头（System 1）。System 2从两种图像输入中提取精炼的空间和运动表征：高分辨率图像用于通过Sobel算子进行空间滤波，长时序序列用于通过FFT进行运动提取。这些低频的时空运动特征作为条件输入给System 1，System 1同时接收高频更新的图像token。通过交叉注意力条件机制，这些异步更新的信号被有效融合，实现鲁棒的实时动作生成。\n\n**整体流程与输入输出**：给定当前时刻t，输入包括一段历史图像序列 \\(I_{t-T:t}\\)、机器人状态 \\(s_t\\) 和语言指令 \\(l\\)。慢系统（System 2）以较低频率运行，接收图像序列，输出精炼后的空间特征 \\(S_t\\) 和运动特征 \\(M_t\\)。快系统（System 1）以控制频率（如30Hz）运行，每步接收最新的图像（通过SigLIP编码为token）、语言指令token以及来自System 2的最新时空运动特征，输出未来H步的动作序列 \\(a_{t+1:t+H}\\) 用于机器人控制。\n\n**核心模块与技术细节**：\n1.  **VDM特征提取与精炼（System 2）**：采用Hunyuan视频扩散Transformer作为VDM骨干。为了获取干净的特征并避免去噪伪影，论文采用基于反转的特征提取策略，在去噪步长 \\(t_{\\text{diff}}=0\\) 时提取特征（公式1）。为了同时捕捉细粒度空间细节和长时程运动动态，System 2并行处理两个视频流：一个短窗口（\\(T_s=2\\)）的高分辨率（512x768）图像流用于空间分析；一个长窗口（\\(T_l=16\\)）的常规分辨率（256x256）图像流用于运动分析（公式2）。\n    *   **空间结构表征（Sobel）**：对高分辨率特征图 \\(F^H\\) 的每个通道，应用标准的3x3 Sobel算子计算水平和垂直梯度，最终得到梯度幅度图 \\(S_t\\)（公式3及后续推导）。该操作能增强相邻帧间结构感知的一致性，突出前景物体的边界，同时过滤背景噪声。\n    *   **运动表征（FFT）**：对长序列特征图 \\(F^L\\) 的每个空间位置和通道，沿时间轴进行一维离散傅里叶变换（DFT）。然后应用一个高通滤波器（频率掩码 \\(\\mathcal{B}\\)），再通过逆DFT重构滤波后的序列，得到 \\(M_t\\)（公式3及后续推导）。该操作抑制了低频背景成分，突出了机器人手臂和被操纵物体随时间变化的连贯运动模式。\n    *   **特征压缩**：提取到的 \\(S_t\\) 和 \\(M_t\\) 通过两个轻量级Q-former进行压缩，以减少token冗余并保持全局一致性，然后拼接为 \\(F_{VDM}\\)。\n\n2.  **扩散Transformer动作头（System 1）**：采用一个10亿参数的扩散Transformer（DiT）作为动作解码器。其输入包括：由SigLIP编码的实时图像token \\(F_I\\)、由文本编码器生成的语言指令token \\(F_l\\)，以及来自System 2的时空运动条件token \\(F_{VDM}\\)。这些条件通过交叉注意力层注入到DiT的各个块中。训练目标为标准去噪损失（公式4），即预测添加到动作序列上的高斯噪声。\n\n3.  **异步双系统机制**：这是实现高效推理的关键创新。System 2（VDM）作为慢速感知模块（例如每秒更新1次），进行高层上下文推理并提取时空运动特征。System 1（DiT）作为快速执行模块（例如每秒生成30次动作），在每一步都利用最新的观测生成动作，但同时持续受到System 2周期性更新的特征条件所引导。这种设计允许在大幅降低VDM调用频率（从而减少计算开销）的同时，通过显式提供的运动感知条件，使System 1能够自适应地生成稳定的动作。\n\n**与现有方法的创新点**：与直接拼接图像token的传统VLA模型（图1a）或直接使用VDM原始特征的条件方法（图1b）相比，Video2Act的创新具体体现在：1) **显式表征提取**：并非直接使用VDM的原始特征，而是通过可解释的、非学习的算子（Sobel和FFT）显式抽取出空间结构和运动动态信息；2) **异步双系统设计**：将计算密集的VDM与轻量的动作头解耦，以不同频率异步运行，在保持高性能的同时实现了实时控制。\n\n## 实验与结果\n**实验设置**：\n*   **仿真基准**：使用RoboTwin双手机器人操作基准，包含6个任务（块交接、容器放置、双瓶抓取简单版、双瓶抓取困难版、空杯放置、捡苹果）。\n*   **真实世界实验**：在ALOHA双手机器人平台上进行6个操作任务评估。\n*   **对比方法**：包括Diffusion Policy (DP)、ACT、RDT-1B、\\(\\pi_0\\) 以及同样使用VDM特征的Video Prediction Policy (VPP)。\n\n**关键实验结果**：\n\n![仿真结果表](https://arxiv.org/html/2512.03044v1/x1.png)\n> **表1**：在六个RoboTwin操作任务上的仿真实验结果。Video2Act取得了54.6%的平均成功率，超过了所有基线方法，相比之前的SOTA方法（RDT和π0）分别提升了9.7%和7.7%。\n\n在仿真实验中，Video2Act在六个任务上的平均成功率达到54.6%，优于之前最好的方法RDT（44.9%）和π0（46.9%），提升幅度分别为9.7%和7.7%。特别是在“块交接”和“空杯放置”任务上表现突出。\n\n![真实世界结果](https://arxiv.org/html/2512.03044v1/x4.png)\n> **图4**：在六个真实世界操作任务上的实验结果。Video2Act在所有任务上均优于基线RDT和VPP，平均成功率相比VPP高出21.7%。\n\n在真实世界实验中，Video2Act在六个任务上的平均成功率达到71.7%，显著优于VPP的50.0%和RDT的45.0%，相比VPP提升了21.7个百分点，展示了强大的鲁棒性和泛化能力。\n\n**消融实验分析**：\n\n![消融研究](https://arxiv.org/html/2512.03044v1/x5.png)\n> **图5**：消融研究。(a) 评估时空运动特征提取的有效性，结合Sobel和FFT带来最大性能增益。(b) 分析操作系统频率比对成功率和动作生成频率的影响，在1:10的比率下能在保持高性能的同时实现高频率控制。\n\n论文进行了详细的消融实验（图5）：\n1.  **时空运动特征贡献**：在固定1:1系统频率比下，对比了不同特征配置。仅使用原始VDM特征相比无VDM特征有提升。单独添加Sobel空间特征带来4.0%的绝对提升，单独添加FFT运动特征带来5.0%的绝对提升。而结合两者（Sobel+FFT）能达到最佳性能54.6%，总提升达8.3%。这证明了显式提取和整合这两种表征的有效性。\n2.  **异步频率比分析**：实验了System 2与System 1的不同更新频率比（1:n）。结果显示，当n从1增加到10时，动作生成频率线性增长，而任务成功率保持稳定。当n=20时，性能开始下降。这表明Video2Act的异步设计是有效的，通过1:10的比率可以在几乎不损失性能的前提下，将动作生成频率提升近10倍，极大地缓解了VDM的计算瓶颈。\n\n## 总结与启发\n**核心贡献**：\n1.  **系统性分析**：首次在机器人操作设置下系统分析了VDM的潜在表征，揭示了其对前景物体结构和运动一致性的稳定捕捉能力，且对机器人手臂和腕部相机动态具有鲁棒性。\n2.  **显式表征整合框架**：提出了Video2Act，通过空间滤波算子（Sobel）和快速傅里叶变换（FFT）显式地整合来自VDM的空间和运动感知表征到VLA模型学习中，使模型能理解“操作什么”以及“如何移动”。\n3.  **高效异步双系统策略**：设计了一种异步双系统策略，其中VDM作为慢速感知系统，DiT头作为快速执行系统，使得模型能够在接收高频输入时自适应地生成稳定动作，实现了性能与效率的平衡。\n\n**局限性**：论文自身提到，尽管异步设计缓解了计算压力，但VDM本身仍然是一个计算成本高昂的大型预训练模型。\n\n**对后续研究的启示**：\n1.  **表征利用**：鼓励后续工作更深入地探索和理解大模型（如VDM）中蕴含的、对机器人任务有益的隐式知识，并设计更精巧的机制将其显式化。\n2.  **系统设计**：异步双系统范式为整合重型感知模型与轻型控制策略提供了可行思路，可应用于其他需要结合慢速深思与快速反应的智能体架构中。\n3.  **效率优化**：可以进一步探索更轻量级的VDM、更高效的特征提取与缓存机制，以实现在资源受限平台上的部署。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03044v1/x1.png",
        "https://arxiv.org/html/2512.03044v1/x2.png",
        "https://arxiv.org/html/2512.03044v1/x3.png",
        "https://arxiv.org/html/2512.03044v1/x4.png",
        "https://arxiv.org/html/2512.03044v1/x5.png",
        "https://arxiv.org/html/2512.03044v1/x6.png",
        "https://arxiv.org/html/2512.03044v1/x7.png",
        "https://arxiv.org/html/2512.03044v1/x8.png",
        "https://arxiv.org/html/2512.03044v1/images/asset.png",
        "https://arxiv.org/html/2512.03044v1/x9.png",
        "https://arxiv.org/html/2512.03044v1/x10.png",
        "https://arxiv.org/html/2512.03044v1/x11.png",
        "https://arxiv.org/html/2512.03044v1/images/supp/simulation_task.png",
        "https://arxiv.org/html/2512.03044v1/x12.png",
        "https://arxiv.org/html/2512.03044v1/x13.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.03028",
      "title": "SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control",
      "url": "http://arxiv.org/abs/2512.03028",
      "arxivId": "2512.03028",
      "date": "2025-12-02",
      "authors": "Xue Bin Peng Team",
      "category": "Manipulation",
      "summary": "本文针对物理角色控制中运动先验模型可重用性差的问题，提出SMP方法。该方法基于预训练的运动扩散模型与分数蒸馏采样技术，构建可冻结复用的任务无关运动先验。实验表明，该方法生成的运动质量与当前最优对抗模仿方法相当，且通用先验可转化为多种风格先验，并能组合风格合成新动作。",
      "detailedSummary": "## 研究背景与动机\n目前，在基于物理的角色控制中，数据驱动的运动先验对于引导智能体产生自然行为至关重要。主流方法主要包括基于跟踪的方法（如DeepMimic）和基于分布匹配的对抗模仿学习方法（如AMP）。基于跟踪的方法要求控制器严格逐帧模仿参考运动片段，限制了其适应新任务的灵活性。对抗模仿学习方法通过学习判别器来区分智能体运动与数据集运动，可以学习到灵活、任务无关的运动先验。然而，这类方法存在关键局限性：判别器（即先验）必须与每个新策略联合训练，这要求在整个策略训练过程中持续访问原始参考数据集，导致先验缺乏**可重用性**和**模块化**。\n\n本文针对对抗先验需要为每个新任务重新训练、且必须永久保留参考数据集的痛点，提出了一种新的视角：利用预训练的扩散模型，通过分数蒸馏采样（SDS）构建**可重用、模块化**的运动先验。核心思路是：首先在大型运动数据集上独立于任何任务或策略预训练一个运动扩散模型，之后将其冻结，并通过SDS将其转化为一个通用的运动自然度奖励函数，用于训练各种下游任务的控制策略。\n\n## 方法详解\nSMP的整体框架分为两个阶段：1）**运动先验预训练**：在大型、无结构的运动数据集上训练一个条件运动扩散模型。2）**策略训练**：将预训练并冻结的扩散模型作为奖励函数，通过SDS计算运动自然度奖励，与任务特定奖励结合，使用强化学习训练控制策略。\n\n![方法框架](https://arxiv.org/html/2512.03028v2/x2.png)\n> **图2**：SMP系统概览。虚线箭头表示仅在策略训练时使用的组件。预训练的运动扩散模型通过分数蒸馏采样（SDS）充当可重用的运动自然度奖励模型。该模型可以进行风格条件化，使策略能够学习特定技能或风格，而无需重新训练或持续访问原始运动数据。\n\n**核心模块与技术细节**：\n1.  **预训练运动扩散模型**：模型以长度为H的连续状态帧序列 $\\mathbf{x} := (\\mathbf{s}_{t-H+2}, \\dots, \\mathbf{s}_{t+1})$ 作为输入。采用标准的DDPM框架进行训练，损失函数为 $\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{i,\\mathbf{x}^0,\\epsilon}[\\|\\epsilon - f(\\mathbf{x}^i)\\|^2_2]$，其中 $f$ 为去噪网络，$\\mathbf{x}^i$ 为加噪后的运动片段。模型可以条件化于风格标签 $c$，以实现风格控制。\n2.  **基于SDS的运动先验奖励**：在策略训练时，对智能体产生的运动片段 $\\tilde{\\mathbf{x}}^0$ 进行前向扩散：$\\mathbf{x}^i = \\sqrt{\\bar{\\alpha}_i} \\tilde{\\mathbf{x}}^0 + \\sqrt{1-\\bar{\\alpha}_i} \\epsilon$。预训练的扩散模型预测噪声 $\\hat{\\epsilon} = f(\\mathbf{x}^i)$。SDS损失为 $\\mathcal{L}_{\\mathrm{SDS}} = \\|\\hat{\\epsilon} - \\epsilon\\|_2^2$，其梯度指示了将当前运动向参考分布对齐的方向。最终，SMP奖励定义为 $r^{\\mathrm{smp}} = \\exp(-w_s \\|\\hat{\\epsilon} - \\epsilon\\|_2^2)$，将其归一化到[0,1]区间以适配RL训练。\n\n![SDS原理](https://arxiv.org/html/2512.03028v2/x3.png)\n> **图3**：SDS原理定性说明。当智能体运动 $\\tilde{\\mathbf{x}}$ 偏离参考分布时，原始分布 $p(\\mathbf{x}^0)$ 的梯度在低密度区域不可靠。前向扩散将 $\\tilde{\\mathbf{x}}$ 映射到扩散样本 $\\mathbf{x}^i$，该处密度 $p(\\mathbf{x}^i)$ 更高，分数估计更可靠。该分数可用于通过反向过程获得来自参考分布的伪目标 $\\bar{\\mathbf{x}}^0$。预测噪声 $\\hat{\\epsilon}$ 与添加噪声 $\\epsilon$ 之间的残差提供了将智能体运动与参考分布对齐的修正量。\n\n**关键创新：集成分数匹配**\n原始SDS对扩散时间步 $i$ 的选择非常敏感。如图4所示，在高噪声水平（大 $i$）下，SDS损失总是很小，无法有效区分运动差异；在低噪声水平下，损失更大，能提供更精细的指导，但对分布外样本的预测可能不可靠。直接随机采样单个 $i$ 会为RL奖励引入巨大方差。\n\n![SDS损失分析](https://arxiv.org/html/2512.03028v2/figures/sds_loss_v2.png)\n> **图4**：SDS损失随扩散噪声水平变化的示例。y轴为对数刻度。高噪声水平（大时间步）下损失总是很小，提供的信息有限；低噪声水平下损失更大，能更好地捕捉智能体运动与参考分布之间的差异。\n\n为解决此问题，SMP提出了**集成分数匹配**：不再随机采样单个时间步，而是固定使用一组时间步 $\\mathbb{K} = \\{0.44N, 0.30N, 0.16N\\}$（N为总扩散步数）来计算平均SDS损失。改进后的奖励为：\n$r^{\\mathrm{smp}} = \\exp\\left(-\\frac{w_s}{|\\mathbb{K}|}\\sum_{i\\in\\mathbb{K}}\\|\\hat{\\epsilon}_i - \\epsilon_i\\|_2^2\\right)$。\n此外，SMP还引入了基于每个时间步SDS误差运行均值 $\\mu_i$ 的自适应归一化，以抵消不同噪声水平下损失尺度的变化，减少手动调参需求。\n\n与现有方法相比，SMP的核心创新在于：1) **真正的可重用性与模块化**：预训练的扩散模型一旦训练完成即被冻结，可反复用于不同任务的策略训练，无需访问原始数据。2) **非对抗性训练**：避免了对抗训练的不稳定性。3) **通过集成分数匹配实现了稳定、高质量的RL奖励信号**。\n\n## 实验与结果\n**实验设置**：\n*   **平台与智能体**：使用IsaGym物理模拟器，控制一个拥有33个关节动作维度的人形角色。\n*   **数据集**：使用了包含100种运动风格（如行走、奔跑、武术、舞蹈等）的大规模运动数据集训练通用先验。\n*   **任务**：评估了多种下游控制任务，包括：方向控制、速度跟踪、位置到达、躲避障碍物，以及从单一运动片段学习（如后空翻、侧手翻）。\n*   **基线方法**：与最先进的对抗模仿学习方法进行对比，包括AMP（Adversarial Motion Priors）、GAIL（Generative Adversarial Imitation Learning），以及近期基于扩散的方法DiffAIL和SMILING。\n\n**关键实验结果**：\n在方向、速度、位置、躲避等复合任务上，SMP训练出的策略在任务性能（回报）和运动质量方面与AMP相当或更优，并且显著优于GAIL、DiffAIL和SMILING。\n\n![任务回报曲线](https://arxiv.org/html/2512.03028v2/figures/task_return_curves/task_heading.png)\n![任务回报曲线](https://arxiv.org/html/2512.03028v2/figures/task_return_curves/task_speed.png)\n![任务回报曲线](https://arxiv.org/html/2512.03028v2/figures/task_return_curves/task_loc.png)\n![任务回报曲线](https://arxiv.org/html/2512.03028v2/figures/task_return_curves/dodge.png)\n> **图12-15**：SMP与基线方法在多种任务上的学习曲线对比。SMP（红色实线）在大多数任务上达到与AMP（蓝色虚线）相当或更高的最终性能，且收敛速度往往更快。GAIL、DiffAIL和SMILING性能较差。\n\n![定性结果](https://arxiv.org/html/2512.03028v2/figures/demos/location_v2.png)\n![定性结果](https://arxiv.org/html/2512.03028v2/x4.png)\n![定性结果](https://arxiv.org/html/2512.03028v2/figures/demos/dodge_4_v3.png)\n![定性结果](https://arxiv.org/html/2512.03028v2/x5.png)\n![定性结果](https://arxiv.org/html/2512.03028v2/x6.png)\n![定性结果](https://arxiv.org/html/2512.03028v2/x7.png)\n![定性结果](https://arxiv.org/html/2512.03028v2/x8.png)\n> **图5-11**：SMP生成的定性结果，展示了其在方向控制、速度跟踪、位置到达、躲避障碍等任务中产生的自然且多样的运动风格。\n\n**消融实验**：\n消融实验验证了集成分数匹配和自适应归一化的重要性。移除集成（使用单时间步）或移除自适应归一化都会导致策略性能显著下降，任务回报降低，甚至学习失败。\n\n![消融实验](https://arxiv.org/html/2512.03028v2/x9.png)\n![消融实验](https://arxiv.org/html/2512.03028v2/x10.png)\n![消融实验](https://arxiv.org/html/2512.03028v2/x11.png)\n![消融实验](https://arxiv.org/html/2512.03028v2/x12.png)\n> **图24-27**：消融研究。左图显示，移除集成分数匹配（粉色线）或自适应归一化（绿色线）会严重损害学习性能。右图显示，使用集成（红色）比随机采样单时间步（紫色）或固定单时间步（粉色）能获得更高、更稳定的任务回报。\n\n**风格化与组合**：\nSMP展示了其模块化优势：一个在100种风格数据上预训练的通用扩散模型，可以通过条件化提示，被重新用作100个独立的风格特定先验，而无需任何模型更新或数据访问。此外，通过线性插值不同风格的条件嵌入，可以组合创造出数据集中不存在的新运动风格（如“僵尸舞”）。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**Score-Matching Motion Priors (SMP)**，一种基于预训练扩散模型和分数蒸馏采样的、可重用且模块化的运动先验构建方法。\n2.  引入了**集成分数匹配**和**自适应归一化**等关键技术，解决了直接将SDS用于RL奖励时信号方差大、不稳定的问题，从而实现了与最先进对抗方法相媲美的高质量运动生成。\n3.  实证表明SMP先验具有强大的**可组合性**，能够通过条件化生成特定风格行为，并能组合不同风格创造新行为，同时在整个策略训练过程中完全无需访问原始运动数据。\n\n**局限性**：\n论文提到，SMP的性能依赖于预训练扩散模型的质量。此外，虽然推理时先验是冻结的，但使用SDS奖励进行策略训练仍需要额外的计算开销。\n\n**启示**：\nSMP为基于学习的角色动画提供了一种新的范式，将先验学习与策略学习解耦，大大提升了先验的实用性和灵活性。其思想可推广至其他需要从数据中学习丰富先验的连续控制领域。未来的工作可以探索更高效的SDS近似方法以降低计算成本，以及将这种可重用先验框架应用于更复杂的多智能体或交互场景中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.03028v2/x1.png",
        "https://arxiv.org/html/2512.03028v2/x2.png",
        "https://arxiv.org/html/2512.03028v2/x3.png",
        "https://arxiv.org/html/2512.03028v2/figures/sds_loss_v2.png",
        "https://arxiv.org/html/2512.03028v2/figures/demos/location_v2.png",
        "https://arxiv.org/html/2512.03028v2/x4.png",
        "https://arxiv.org/html/2512.03028v2/figures/demos/dodge_4_v3.png",
        "https://arxiv.org/html/2512.03028v2/x5.png",
        "https://arxiv.org/html/2512.03028v2/x6.png",
        "https://arxiv.org/html/2512.03028v2/x7.png",
        "https://arxiv.org/html/2512.03028v2/x8.png",
        "https://arxiv.org/html/2512.03028v2/figures/task_return_curves/task_heading.png",
        "https://arxiv.org/html/2512.03028v2/figures/task_return_curves/task_speed.png",
        "https://arxiv.org/html/2512.03028v2/figures/task_return_curves/task_loc.png",
        "https://arxiv.org/html/2512.03028v2/figures/task_return_curves/dodge.png",
        "https://arxiv.org/html/2512.03028v2/figures/task_return_curves/legend.png",
        "https://arxiv.org/html/2512.03028v2/figures/singe_clip_learning_curves/spinkick.png",
        "https://arxiv.org/html/2512.03028v2/figures/singe_clip_learning_curves/backflip.png",
        "https://arxiv.org/html/2512.03028v2/figures/singe_clip_learning_curves/cartwheel.png",
        "https://arxiv.org/html/2512.03028v2/figures/singe_clip_learning_curves/crawl.png",
        "https://arxiv.org/html/2512.03028v2/figures/singe_clip_learning_curves/run.png",
        "https://arxiv.org/html/2512.03028v2/figures/singe_clip_learning_curves/walk.png",
        "https://arxiv.org/html/2512.03028v2/figures/singe_clip_learning_curves/legend.png",
        "https://arxiv.org/html/2512.03028v2/x9.png",
        "https://arxiv.org/html/2512.03028v2/x10.png",
        "https://arxiv.org/html/2512.03028v2/x11.png",
        "https://arxiv.org/html/2512.03028v2/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.02729",
      "title": "RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning",
      "url": "http://arxiv.org/abs/2512.02729",
      "arxivId": "2512.02729",
      "date": "2025-12-02",
      "authors": "Haoqian Wang Team",
      "category": "Manipulation",
      "summary": "本文提出RoboWheel数据引擎，旨在解决机器人学习数据依赖高成本遥操作、缺乏多样性且难以跨形态迁移的问题。其核心方法是通过单目RGB(D)视频重建高精度手物交互轨迹，利用强化学习优化器在接触与穿透约束下确保物理合理性，随后将轨迹重定向至不同形态机器人，并通过仿真增强进行领域随机化以扩展数据分布。实验表明，该引擎生成的轨迹与遥操作数据同样稳定，能持续提升机器人性能，首次定量验证了手物交互视频可作为机器人学习的有效监督信号。",
      "detailedSummary": "## 研究背景与动机\n当前机器人学习的数据收集主要依赖遥操作或动作捕捉系统，这些方法成本高昂、硬件特定，限制了数据行为的多样性及其在不同机器人形态和任务间的可迁移性。与此同时，海量的人手-物体交互视频蕴含了丰富的真实世界操作策略，但由于重建噪声、物理不合理性以及形态不匹配等问题，这些信号很少被转化为机器人可用的训练数据。本文旨在解决这一核心矛盾，提出一个实用的、可扩展的数据处理流程，能够将单目RGB/RGB-D视频中的人类演示转化为物理合理、可跨形态重定向的机器人监督信号。本文的核心思路是构建一个端到端的数据引擎，通过高精度HOI重建与物理优化、灵活的跨形态重定向以及仿真增强的数据飞轮，从真实世界视频中大规模生成可用于训练多样化机器人形态的监督数据。\n\n## 方法详解\nRoboWheel的整体流程是一个端到端的管道，将野外HOI视频转化为机器人可用的监督数据。如下图所示，该流程主要包括四个阶段：1) 从RGB(D)视频中进行HOI重建；2) 物理合理性优化；3) 跨形态重定向；4) 仿真环境中的数据增强与验证。\n\n![方法框架](https://arxiv.org/html/2512.02729v1/img/pie.jpg)\n> **图1**：RoboWheel 数据管道总览。给定单目RGB(-D)输入，首先估计手/全身及被操作物体的运动。随后进行联合优化（由TSDF和强化学习引导），以提高物理合理性并确保机器人可达性。优化后的轨迹被重定向到异构形态（包括机械臂、灵巧手和人形机器人）。最后，在Isaac Sim中对观测和轨迹进行领域随机化，以丰富机器人观测的多样性，生成的具身数据在VLA和IL策略基准上进行验证。\n\n**HOI重建与物理优化**：输入为视频帧序列 { I t } t = 1 T \\{I_{t}\\}_{t=1}^{T} ，目标是恢复手和物体的参数化表示及运动轨迹。首先，通过预训练模型恢复手部（MANO参数）或全身（SMPL-H参数）的运动。对于物体，利用多视图3D生成器 𝒢 \\mathcal{G} 生成未缩放的纹理网格，并通过深度图反投影点云来恢复其公制尺度，再使用基于对应关系的跟踪器 ℱ \\mathcal{F} 估计物体在相机坐标系下的位姿流。所有轨迹通过运动恢复结构技术统一到世界坐标系。\n\n物理合理性优化分为两步。首先，利用物体截断符号距离函数（TSDF）惩罚手-物体穿透，优化手腕位姿以避免碰撞。随后，引入一个残差强化学习策略，在接触和可达性先验下进一步微调手-物体的相对位姿，确保轨迹在物理上合理且对机器人可达。RL的状态 s t s_t 包含手部与物体位姿、速度及接触力，奖励函数 r t r_t 包含几何跟踪误差、动力学平滑性及接触力鼓励项。\n\n**跨形态重定向**：将优化后的物理合理轨迹 { h t , p t } \\{h_t, p_t\\} 映射到不同机器人形态。对于机械臂，将手部关节映射到平行夹爪的末端执行器位姿 { T g ( t ) , g ( t ) } \\{T_g(t), g(t)\\} 。根据接触几何（全手或仅指尖）采用两种互补的朝向构建方法，并使用kNN分类器确定手势类别。夹爪的开合状态通过跟踪被操作物体上关键点的运动轨迹来判断。对于灵巧手和人形机器人，则分别通过运动学相似性、接触保持约束以及全身逆运动学和动力学感知优化进行重定向。\n\n**仿真增强数据飞轮**：在Isaac Sim仿真环境中重放重定向后的轨迹，并进行大规模增强以丰富观测和运动轨迹的多样性。核心增强策略包括：\n1.  **多机械臂重定向**：利用GPU加速的逆运动学求解器，将HOI衍生的6D末端轨迹映射到五种不同机械臂（UR5、Panda等）的关节空间轨迹，提供形态多样性。\n2.  **物体检索与替换**：构建大型物体资产库，通过融合Chamfer距离、包围盒IoU和语义嵌入的相似性度量，为源物体检索几何/语义匹配的替代物体，并保持相同的交互几何与控制轨迹。\n3.  **轨迹增强**：将轨迹按接触状态分段，对交互段（夹持）施加物体坐标系下的刚性变换，对非交互段（张开）进行线性路径重映射和一致的朝向调整，以生成新的可行轨迹，同时保持交互意图。\n\n## 实验与结果\n**数据集构建**：利用RoboWheel管道构建了大规模多模态数据集HORA，整合了三个来源的数据：1) 配备触觉传感手套的自建多视角动作捕捉系统；2) 多个公共HOI数据集（如GRAB、HO3D、DexYCB等）；3) 自建的RGB(D)视频采集设置。HORA总计包含约15万条轨迹，提供HOI相关（手部参数、物体6D位姿、接触标注）和机器人相关（机器人视角观测、末端轨迹、关节命令）两大类模态。其中动作捕捉子集还提供了密集的触觉信号。\n\n**数据质量与有效性验证**：论文在主流视觉-语言-动作模型和模仿学习架构上验证了RoboWheel生成数据的质量。关键结论包括：由本管道生成的轨迹与遥操作获取的轨迹同样稳定，并能带来持续可比拟的性能提升。这首次为“HOI模态可作为机器人学习有效监督”提供了定量证据。与遥操作相比，RoboWheel仅需单目RGB(D)相机即可提取通用的、与形态无关的运动表示，具有轻量级优势。\n\n**实验对比**：论文将HORA数据集与现有主流机器人及HOI数据集进行了模态和规模的对比（见论文表1）。HORA在规模上显著超过了许多专门数据集，并且首次在同一数据集中集成了触觉、多形态机器人数据和高精度HOI标注。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了一个从单目视频到物理合理、可执行机器人轨迹的端到端数据引擎，实现了**物理合理的HOI重建与跨域重定向**；2) 构建了一个**仿真增强的数据飞轮**，通过领域随机化策略大规模扩展数据分布，同时保持交互语义；3) 发布了**大规模多模态数据集HORA**，为机器人学习和HOI相关任务提供了丰富资源。\n\n论文提到的局限性包括：对于严重遮挡、低分辨率或手部运动速度极快的视频，重建与优化可能面临挑战；跨形态重定向，特别是向灵巧手和人形机器人的映射，仍需进一步验证其通用性和鲁棒性。\n\n这项工作的启示在于，它开辟了一条利用海量、易于获取的人类演示视频来驱动机器人学习的新路径。其提出的“重建-优化-重定向-增强”范式，为构建低成本、大规模、跨形态的机器人训练数据提供了系统性的解决方案，有望降低机器人学习的硬件与数据门槛，并促进通用具身智能模型的发展。",
      "imageUrls": [
        "https://arxiv.org/html/2512.02729v1/img/pie.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.02851",
      "title": "SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots",
      "url": "http://arxiv.org/abs/2512.02851",
      "arxivId": "2512.02851",
      "date": "2025-12-02",
      "authors": "Dzmitry Tsetserukou Team",
      "category": "Manipulation",
      "summary": "本文解决现有视觉导航方法依赖手工提示、泛化性差且规划速度慢的问题，提出SwarmDiffusion模型。该模型是一种端到端扩散模型，通过免规划器的轨迹构建流程（随机航点采样、贝塞尔平滑与正则化），并利用VLM监督和紧凑的机器人状态调节，直接从单张RGB图像联合预测可通行性并生成可行轨迹。实验表明，该方法在室内环境及不同机器人平台上实现了80-100%的导航成功率，推理仅需0.09秒，且仅用500个样本即可适应新机器人。",
      "detailedSummary": "## 研究背景与动机\n视觉可通行性估计对于自主导航至关重要。当前主流方法主要包括：1）自监督学习方法，通过机器人自身交互获取监督信号，但需要大量环境交互数据，且严重依赖特定机器人平台（embodiment），难以泛化到新平台；2）基于大型视觉语言模型（VLM）的方法，通过人工提示词（prompt）从RGB图像推断可通行性，但其输出质量对提示词敏感，且通常只输出可通行性地图，需要依赖外部规划器生成轨迹，导致延迟高、流程复杂；3）现有的扩散模型应用于运动规划时，通常依赖预计算的可通行性代价图或外部估计，并未将可通行性推理与轨迹生成统一在一个模型中。\n\n本文针对的关键痛点是：现有方法无法从一个单一的RGB图像中，以端到端、轻量级的方式，联合预测可通行性并生成一条物理上可行的轨迹，同时实现跨异构机器人平台（如四足机器人和无人机）的泛化。本文提出了SwarmDiffusion，其核心思路是：利用一个条件扩散模型，在免规划器生成的合成轨迹监督下，联合学习可通行性预测和轨迹去噪，并通过一个紧凑的机器人状态向量进行调节，从而实现无需提示词、免演示学习、且对机器人平台不敏感的统一导航框架。\n\n## 方法详解\nSwarmDiffusion的整体框架包含两个紧密耦合的组件：可通行性学生模型（Traversability Student）和基于扩散的轨迹生成器（Diffusion-based Trajectory Generation）。输入为单帧RGB图像、机器人本体状态向量、本体类型以及编码了起点-目标位置的二通道张量。输出为预测的可通行性热图（64x64）和一条可行的轨迹热图（64x64）。\n\n![方法框架](https://arxiv.org/html/2512.02851v3/images/Tieser-architect.jpg)\n\n> **图2**：SwarmDiffusion整体架构。左侧为可通行性学生模型，使用冻结的DINO-v2 ViT编码图像，机器人状态经MLP生成FiLM参数调制视觉特征，最终解码出可通行性图。右侧为扩散轨迹生成器，一个UNet以噪声轨迹、起点-目标图、预测可通行性图为空间条件，以FiLM调制后的视觉token为跨注意力条件，进行多步去噪，生成干净轨迹。\n\n**核心模块一：免规划器的轨迹构建与监督**。为了训练扩散模型而不依赖专家演示或外部规划器，论文提出一种合成轨迹生成流程：在由教师VLM（AnyTraverse）预测的高可通行性区域内，随机采样起点（靠近图像底部）和远处目标点；中间路径点通过高斯扰动随机采样，并用贝塞尔曲线进行平滑插值；最终将曲线栅格化为64x64的细线轨迹热图，并施加轻微高斯模糊。这为扩散模型提供了多样、平滑且与环境几何一致的监督信号。\n\n**核心模块二：可通行性学生模型**。该模块负责从视觉输入预测平台特定的可通行性。采用冻结的DINO-v2 ViT作为视觉骨干网络，附加一个可训练的ViT适配器（ViT-adapter）进行微调。机器人本体状态（6维位姿）通过一个MLP生成FiLM参数，用于调制ViT中间层的特征图（公式10），从而将机器人的动态约束注入视觉表示。调制后的特征经过一个轻量级卷积解码器，上采样并输出64x64的可通行性预测图。其监督信号来自教师VLM生成的可通行性图，使用L1蒸馏损失（公式24）。\n\n**核心模块三：条件扩散轨迹生成器**。采用标准UNet架构作为去噪网络。在每个去噪步，UNet的输入是拼接而成的4通道空间条件图：噪声轨迹热图、起点-目标二值图、预测的可通行性图（公式11）。同时，来自可通行性学生模型的FiLM调制视觉token被输入到UNet的跨注意力层，作为token条件。UNet被训练去预测添加到干净轨迹上的噪声，使用标准DDPM目标（公式13）。在推理时，从纯噪声开始，迭代应用UNet去噪，最终生成与场景几何、可通行性及机器人本体约束一致的轨迹热图。\n\n**创新点与损失函数**：与仅以可通行性图为条件的基线方法相比，SwarmDiffusion的核心创新在于通过FiLM机制和跨注意力，将机器人本体状态深度集成到特征级条件中，实现了真正的本体感知（embodiment-aware）规划。训练采用多任务损失（公式26），包括：1) 扩散去噪损失；2) 方向性前向流损失，惩罚轨迹中逆向于起点到目标方向的片段（公式19）；3) 可通行性期望奖励，鼓励轨迹落在高可通行性区域（公式22）；4) 质量正则化损失，确保轨迹的几何合理性（如薄度、平滑度）；5) 可通行性蒸馏损失。\n\n## 实验与结果\n**实验设置**：使用了两个机器人平台进行验证：Unitree Go1四足机器人和自定义的8英寸无人机。数据集包含从两个平台采集的共8000对同步RGB图像及对应的合成轨迹。实验平台包括高保真Unity仿真环境和真实世界。评估使用了VICON动捕系统进行精确位姿估计。\n\n**Baseline方法**：由于没有现成的联合学习方法，论文实现了一个对照基线。该基线使用相同的条件扩散UNet，但仅以VLM生成的可通行性图和起点-目标图作为空间条件，**不包含**机器人状态向量或特征级的可通行性语义（即没有可通行性学生模型和FiLM调制）。该基线使用A*规划器生成的专家轨迹进行监督。\n\n**关键定量结果**：在室内导航任务中，SwarmDiffusion在两个平台上均实现了**80-100%的成功率**，单次推理时间仅为**0.09秒**。模型展现出强大的跨平台泛化能力：仅使用**500个**来自新机器人（目标平台）的额外视觉样本进行状态向量适配器微调（保持网络主干冻结），即可成功适应新平台。\n\n**定性结果分析**：论文提供了大量定性对比图，展示了SwarmDiffusion与基线方法在轨迹和可通行性预测上的差异。\n\n![无人机基线轨迹示例](https://arxiv.org/html/2512.02851v3/images/results/drone_baseline_traj_1.png)\n![无人机基线轨迹示例](https://arxiv.org/html/2512.02851v3/images/results/drone_baseline_traj_2.png)\n> **图4-5**：基线方法（仅用可通行性图条件）为无人机生成的轨迹（红色）。轨迹有时会穿过障碍物（如椅子腿），或规划出不符合无人机飞行特点的不安全路径。\n\n![SwarmDiffusion无人机轨迹示例](https://arxiv.org/html/2512.02851v3/images/results/drone_traj_1.png)\n![SwarmDiffusion无人机轨迹示例](https://arxiv.org/html/2512.02851v3/images/results/drone_traj_2.png)\n> **图12-13**：SwarmDiffusion为无人机生成的轨迹（绿色）。轨迹能更好地避开障碍，并倾向于在开阔空间（如门框上方、房间中央）规划路径，更符合无人机的飞行约束。\n\n![四足机器人轨迹与可通行性叠加示例](https://arxiv.org/html/2512.02851v3/images/results/dog_traj_overlay_1.png)\n![四足机器人轨迹与可通行性叠加示例](https://arxiv.org/html/2512.02851v3/images/results/dog_trav_overlay_1.png)\n> **图20, 24**：SwarmDiffusion为四足机器人生成的轨迹（粉色）与预测的可通行性图（蓝色高亮）叠加展示。轨迹精确地贴合在预测的可通行区域（地面）内，避开不可通行区域（家具）。\n\n**消融实验**：论文进行了消融研究，评估了方向性损失、可通行性奖励、质量正则化等各个损失组件的重要性。结果表明，完整的损失组合对于生成安全、平滑、方向正确且贴合可通行区域的轨迹至关重要。特别是，移除方向性损失会导致轨迹出现回环；移除可通行性奖励会使轨迹冒险进入低可通行性区域。\n\n## 总结与启发\n**核心贡献**：1) **统一的端到端生成导航**：首次提出了一个扩散模型框架，能够从单张RGB图像联合生成可通行性地图和可行轨迹，充当了全局与局部规划器的角色，且无需专家演示。2) **跨机器人平台的本体无关规划**：引入了轻量级状态向量适应机制，仅需数百样本即可泛化到新机器人平台，同时保持网络权重冻结。3) **超越经典优化的鲁棒性**：模型隐式编码场景几何与可通行性，产生全局连贯的轨迹假设，避免了经典优化器（如纯APF规划器）易陷入局部最小值和振荡的问题。\n\n**局限性**：论文提到，由于轨迹生成在图像空间进行，将路径投影到3D时（尤其是无人机俯视视角或长距离规划时）会引入轻微的几何不确定性。为此，论文在真实世界部署中引入了一个轻量级人工势场（APF）进行局部修正，作为安全层。\n\n**后续启示**：SwarmDiffusion展示了一种无需复杂提示工程或大量真实机器人交互数据，即可学习跨平台导航策略的可行路径。其轻量级适配机制为快速部署到新机器人平台提供了思路。将可通行性推理与轨迹生成统一在一个生成模型中，为构建更紧密耦合、更高效的“感知-规划”一体化系统指明了方向。",
      "imageUrls": [
        "https://arxiv.org/html/2512.02851v3/images/Tieser-tieser-anonym.jpg",
        "https://arxiv.org/html/2512.02851v3/images/Tieser-architect.jpg",
        "https://arxiv.org/html/2512.02851v3/images/uav_simulation_overview.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_baseline_traj_1.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_baseline_traj_2.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_baseline_traj_3.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_baseline_traj_4.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_baseline_traj_1.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_baseline_traj_2.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_baseline_traj_3.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_baseline_traj_4.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_traj_1.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_traj_2.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_traj_3.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_traj_4.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_trav_1.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_trav_2.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_trav_3.png",
        "https://arxiv.org/html/2512.02851v3/images/results/drone_trav_4.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_traj_overlay_1.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_traj_overlay_2.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_traj_overlay_3.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_traj_overlay_4.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_trav_overlay_1.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_trav_overlay_2.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_trav_overlay_3.png",
        "https://arxiv.org/html/2512.02851v3/images/results/dog_trav_overlay_4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.02951",
      "title": "Experimental Characterization of Fingertip Trajectory following for a 3-DoF Series-Parallel Hybrid Robotic Finger",
      "url": "http://arxiv.org/abs/2512.02951",
      "arxivId": "2512.02951",
      "date": "2025-12-02",
      "authors": "Nilanjan Chakraborty Team",
      "category": "Manipulation",
      "summary": "本文针对紧凑型多自由度机器人手指在任务空间轨迹精确跟踪方面研究不足的核心问题，提出并实验表征了一种3自由度串并联混合连杆驱动手指。该手指具有解析正向运动学与封闭形式雅可比矩阵，关键技术是采用解析运动速率控制方案实现闭环任务空间轨迹跟踪。实验结果表明，该手指在直线、圆形及复杂曲线等多种轨迹上均能实现毫米级的指尖跟踪精度，为灵巧手内操作提供了重要的性能基准。",
      "detailedSummary": "## 研究背景与动机\n灵巧操作任务通常自然地指定为指尖运动和作用力，而非单个关节角度，因此机器人手指的任务空间控制是实现灵巧操作的关键。尽管任务空间规划与控制已在大型机械臂上得到广泛研究，但在紧凑型、多自由度（DoF）机器人手指上实现精确的任务空间轨迹跟踪的演示仍然稀缺。当前机器人手指的主要驱动方式包括直接驱动、肌腱驱动、软体驱动（如气动、形状记忆）和连杆驱动。直接驱动手指结构简单、自由度较高，但尺寸大、惯量高；肌腱驱动手指尺寸较小，但存在弹性、背隙和摩擦，导致建模不确定；软体驱动手指尺寸好，但缺乏复杂操作所需的自由度。连杆驱动手指则能在保持紧凑尺寸的同时，提供刚性的传动和确定性的映射。然而，现有的3-DoF连杆驱动手指（如AIDIN手）缺乏从电机指令到指尖位置的解析正向运动学（FK）模型和封闭形式的雅可比矩阵，这阻碍了精确的指尖位置/速度控制器的开发。本文针对这一具体痛点，提出并物理实现了一种具有解析正向运动学和封闭形式雅可比矩阵的3-DoF串并联混合连杆驱动手指，并实验评估了其任务空间轨迹跟踪性能。本文核心思路是：基于一个具有解析运动学模型的连杆驱动手指架构，构建物理原型，并采用基于速度分解的运动控制（RMRC）方案，在多种轨迹上实验验证其毫米级的指尖轨迹跟踪精度。\n\n## 方法详解\n本文方法的核心是一个3-DoF串并联混合机器人手指的物理实现及其对应的任务空间控制算法。\n\n**整体构造与运动学**：手指采用串并联混合结构，包含四个封闭运动链（并联拓扑），但通过系统求解这些环闭方程，可以提取出对应于掌指关节（MCP）和近端指间关节（PIP）的三个关节角（两个屈曲自由度，一个外展自由度），从而能够将其建模为一个串联机械臂。远端指间关节（DIP）的运动与PIP关节耦合。与先前纯连杆驱动的设计不同，本文对外展自由度进行了简化，采用直驱方式（通过伞齿轮将电机连接到关节），而两个屈曲自由度则通过Faulhaber无刷电机结合丝杠驱动的线性执行器实现。这种混合驱动简化了外展自由度的运动学和设计。\n\n![3-DoF系列-并联机器人手指原型](https://arxiv.org/html/2512.02951v1/finger_wide.png)\n> **图1**：3-DoF串并联机器人手指原型，采用直接驱动外展和连杆驱动屈曲。\n\n手指具有解析的正向运动学（FK）映射 $\\texttt{FK}(\\mathbf{M}) \\mapsto \\mathbf{X}$、逆运动学（IK）映射 $\\texttt{IK}(\\mathbf{X}) \\mapsto \\mathbf{M}$ 以及依赖于电机位置的雅可比矩阵映射 $\\texttt{J}(\\mathbf{M}) \\mapsto \\mathbf{J}=\\frac{\\partial\\mathbf{X}}{\\partial\\mathbf{M}}$。其中 $\\mathbf{M} \\in \\mathbb{R}^3$ 为电机位置，$\\mathbf{X} \\in \\mathbb{R}^3$ 为相对于MCP关节世界坐标系的指尖笛卡尔坐标。这些解析模型的推导基于先前的理论工作，使得基于雅可比矩阵的任务空间控制成为可能。\n\n**控制器设计**：控制系统包含两种规划器：关节空间规划器（Algorithm 1）和任务空间规划器（Algorithm 2）。关节空间规划器接收目标关节构型 $\\mathbf{Q}_{\\text{goal}}$，通过在关节空间进行线性插值生成一系列电机位置指令。由于关节非直接驱动，该算法通过迭代增加运动时间 `t` 来确保计划的关节运动不会导致电机指令超过最大速度限制。任务空间规划器是本文实现轨迹跟踪的核心。它接收一个由最多N个点（受硬件内存限制）定义的期望指尖路径 $\\mathbf{X}_{\\text{list}}$ 和期望速度 $v_{\\text{desired}}$。控制器首先使用关节空间规划器将指尖移动到路径的第一个点。随后，对于路径中的每个后续点，循环执行以下步骤：1) 读取当前电机位置并计算当前指尖位置 $\\mathbf{X}_{\\text{cur}}$ 和雅可比矩阵 $\\mathbf{J}$；2) 计算当前位置与目标点的误差向量 $\\mathbf{E}$；3) 将误差向量的单位向量乘以期望速度，得到期望的指尖速度 $\\dot{\\mathbf{X}}$；4) 通过 $\\dot{\\mathbf{M}} = \\mathbf{J}^{-1} \\dot{\\mathbf{X}}$ 计算所需的电机速度；5) 检查电机速度是否超过上限，如有必要则按比例缩放；6) 向电机发送速度指令。此循环持续直至每个笛卡尔方向上的误差小于0.1 mm，然后转向路径中的下一个点。任务空间规划器能保证指尖沿直线路径到达目标点。\n\n![与人类手指的解剖学对比](https://arxiv.org/html/2512.02951v1/anatomic_joints.png)\n> **图2**：3-DoF串并联手指与人类手指的解剖学对比，标明了MCP、PIP、DIP关节。\n\n![尺寸对比](https://arxiv.org/html/2512.02951v1/finger_dimensions.png)\n> **图3**：本文手指尺寸（23×20×95 mm）与AIDIN Robotics手指（20×21.5×101 mm）及Shadow Robotics手指（20×18×100 mm）的对比，显示其尺寸相当。\n\n**创新点**：与现有方法相比，本文的创新点具体体现在：1) **物理实现与验证**：首次物理实现并系统实验验证了具有解析运动学模型的3-DoF连杆驱动手指的任务空间轨迹跟踪能力，填补了先前工作仅停留在仿真阶段的空白。2) **混合驱动简化**：将外展自由度改为直驱，简化了设计和运动学，而未显著增加机构尺寸。3) **完整的控制流程**：提供了从解析运动学模型到关节空间/任务空间规划器，再到实验表征的完整技术链条。\n\n## 实验与结果\n**实验设置**：实验平台为自制的3-DoF串并联混合手指原型，使用Arduino Uno R4进行高层运动控制，Faulhaber MC3001P电机控制器进行底层电机控制。为了评估轨迹跟踪性能，在**屈曲平面**和**外展平面**分别测试了三种轨迹：方形、圆形和阶梯路径。方形路径测试长直线跟踪能力，圆形路径测试用短线段逼近曲线的能力，阶梯路径测试快速变化的小路径的精确跟踪能力。\n\n**测量方法**：采用基于像素到度量（pixel-to-metric）的技术跟踪指尖运动。在指尖附加一个小棱柱，使用Segment Anything Model 2 (SAM 2) 在视频序列中跟踪其掩膜，计算质心或边缘中点作为指尖位置，并通过已知棱柱尺寸换算为毫米坐标。对每个数据点进行了不确定性传播分析。\n\n**对比方法**：本文主要进行的是自身性能表征，未与其他控制器进行直接数值对比。但文中将手指尺寸与**AIDIN Robotics**手指和**Shadow Robotics**手指进行了对比，表明其紧凑性。实验内部对比了**关节空间规划器**和**任务空间规划器**的适用场景，并对比了**轨迹跟踪**（包含速度跟随）和**路径跟踪**（仅考虑几何路径）的性能差异。\n\n**关键实验结果**：\n1.  **轨迹跟踪误差**：量化指标为轨迹范数误差，即测量轨迹与期望轨迹位置偏差的范数。关键结果总结于表I。在屈曲平面，所有轨迹的最大误差在毫米级（方形1.46 mm，圆形1.75 mm，阶梯0.58 mm）。在外展平面，误差显著更大（方形6.44 mm，圆形5.77 mm，阶梯2.51 mm）。\n\n![屈曲平面方形路径跟踪结果](https://arxiv.org/html/2512.02951v1/square_10mms_all.png)\n> **图4**：屈曲平面方形路径（10 mm/s）的五次测量试验结果，显示轨迹跟踪一致性高。\n\n![屈曲平面圆形路径跟踪结果](https://arxiv.org/html/2512.02951v1/circle_10mms_all.png)\n> **图5**：屈曲平面圆形路径（10 mm/s）的五次测量试验结果。\n\n![屈曲平面阶梯路径跟踪结果](https://arxiv.org/html/2512.02951v1/step_5mms_all.png)\n> **图6**：屈曲平面阶梯路径（5 mm/s）的五次测量试验结果，误差最小。\n\n![外展平面方形路径跟踪结果](https://arxiv.org/html/2512.02951v1/ad_square_10mms_all.png)\n> **图7**：外展平面方形路径（10 mm/s）的五次测量试验结果，视觉上路径偏差不大，但误差值较高。\n\n![外展平面圆形路径跟踪结果](https://arxiv.org/html/2512.02951v1/ad_circle_10mms_all.png)\n> **图8**：外展平面圆形路径（10 mm/s）的三次测量试验结果。\n\n![外展平面阶梯路径跟踪结果](https://arxiv.org/html/2512.02951v1/ad_step_10mms_all.png)\n> **图9**：外展平面阶梯路径（10 mm/s）的五次测量试验结果。\n\n2.  **路径跟踪误差**：当仅考虑几何路径而不考虑期望速度时（即使用测量速度重新计算期望位置），屈曲平面的误差几乎不变，而外展平面的误差大幅下降（方形降至2.74 mm，圆形降至2.80 mm，阶梯降至2.01 mm，见表II）。这表明外展平面的主要问题在于**速度跟踪**而非路径几何形状跟踪。作者将此外展平面性能较差归因于外展电机缺乏高分辨率编码器（仅依赖模拟霍尔传感器）以及3D打印伞齿轮可能存在背隙。\n3.  **起点-终点重复性**：对于起点和终点相同的方形和圆形路径，测量了指尖返回起点位置的偏差（见表III）。屈曲平面的重复性误差非常小（方形0.13 mm，圆形0.18 mm），外展平面稍大（方形0.39 mm，圆形0.18 mm）。高重复性和误差位置的 consistency 表明，部分误差可能来源于校准不精确以及实际连杆长度与控制器中使用的模型长度不一致。\n4.  **物理验证**：使用3D打印的路径夹具进行验证，指尖带动一个探针在夹具槽中运动。实验表明，手指能够在宽度为3 mm（屈曲）和4 mm（外展）的路径中运动而不触碰边界，这为最大可能误差提供了一个直观的上界（±1.5 mm 和 ±2.0 mm）。\n\n![3D打印路径跟随验证结构](https://arxiv.org/html/2512.02951v1/all_rigs_with_dimensions.png)\n> **图10**：用于物理验证的3D打印路径跟随结构，手指位于各路径的起始位置。成功通过而不触碰边界验证了跟踪精度。\n\n## 总结与启发\n**核心贡献**：\n1.  **首次系统性实验演示**：本文首次通过物理原型系统性地实验演示了紧凑型、多自由度连杆驱动机器人手指的精确任务空间轨迹跟踪，实现了毫米级的跟踪精度（尤其在屈曲平面），为未来面向灵巧操作的机器人手设计建立了基准。\n2.  **解析运动学模型的工程实现**：成功地将具有解析正向运动学和封闭形式雅可比矩阵的串并联连杆驱动手指架构从理论转化为物理现实，并验证了其在实际控制中的有效性。\n3.  **完整的表征方法**：提供了一套从手指设计、控制器实现到基于视觉的轨迹误差量化及物理验证的完整实验表征流程。\n\n**局限性**：\n1.  外展平面的轨迹跟踪误差明显大于屈曲平面，主要受限于外展电机的传感精度和传动部件的背隙。\n2.  手指原型由3D打印部件制成，随着使用，压配关节的完整性下降会导致机械间隙增大，引起指尖微小振动。\n3.  基于单目视觉的测量方法存在较高的不确定性（例如，屈曲方形路径误差的95%置信区间约为±0.48 mm），需要更精确的测量系统进行严格量化。\n\n**对后续研究的启示**：\n1.  **硬件改进**：过渡到机加工铝制连杆、使用滚珠丝杠替代梯形丝杠、为所有电机配备高分辨率编码器，将能显著减少机械间隙、实现背驱性并提高定位精度。\n2.  **系统扩展**：基于此手指设计和控制方法，构建由多个手指和一个4-DoF拇指组成的完整机器人手，以实验评估多指灵巧操作任务。\n3.  **更严格的性能量化**：未来工作需要采用更精确的测量系统（如运动捕捉），在完整的3D空间内报告误差分布和不确定度边界，并进一步量化手指的力分辨率与控制带宽。",
      "imageUrls": [
        "https://arxiv.org/html/2512.02951v1/finger_wide.png",
        "https://arxiv.org/html/2512.02951v1/anatomic_joints.png",
        "https://arxiv.org/html/2512.02951v1/finger_dimensions.png",
        "https://arxiv.org/html/2512.02951v1/square_10mms_all.png",
        "https://arxiv.org/html/2512.02951v1/circle_10mms_all.png",
        "https://arxiv.org/html/2512.02951v1/step_5mms_all.png",
        "https://arxiv.org/html/2512.02951v1/ad_square_10mms_all.png",
        "https://arxiv.org/html/2512.02951v1/ad_circle_10mms_all.png",
        "https://arxiv.org/html/2512.02951v1/ad_step_10mms_all.png",
        "https://arxiv.org/html/2512.02951v1/all_rigs_with_dimensions.png",
        "https://arxiv.org/html/2512.02951v1/joint_vis.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.02609",
      "title": "SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction",
      "url": "http://arxiv.org/abs/2512.02609",
      "arxivId": "2512.02609",
      "date": "2025-12-02",
      "authors": "Yong Zhao Team",
      "category": "Manipulation",
      "summary": "本文针对机器人模仿学习中抓取任务的多模态冲突问题，提出SAM2Grasp框架。核心方法是将任务重构为单模态的提示条件预测问题：利用冻结的SAM2模型提取时序视觉特征，并并行训练一个轻量级动作头。推理时，通过初始提示（如边界框）指定目标物体，动作头即可预测针对该物体的唯一抓取轨迹，SAM2的时序跟踪能力确保后续帧中目标的稳定跟踪。实验表明，该方法在杂乱多物体抓取任务中取得了最先进的性能。",
      "detailedSummary": "## 研究背景与动机\n模仿学习（IL）为机器人抓取提供了一种直观的学习范式，但其常受多模态问题的困扰：当场景中存在多个有效抓取目标时，抓取不同物体的演示数据会产生相互冲突的训练信号。标准的模仿学习策略会将这些不同的动作“平均”成一个无效的动作，导致策略失败。现有工作主要采用两种策略应对此挑战：一是在策略输出端显式建模多模态动作分布（如使用CVAE、扩散模型等生成模型）；二是在策略输入端添加目标条件信息（如目标掩码或裁剪图像）。前者增加了策略学习的复杂性，后者则仍需要标准的视觉主干从头学习从复杂场景中关联目标信息的困难任务。\n\n本文针对**物体层面的多模态性**这一具体痛点，提出了一个全新的视角：将多模态问题在感知阶段而非动作阶段进行消解。具体而言，通过引入一个指定目标的**提示**（如边界框），将任务重新定义为一种单模态的、提示条件化的预测问题。核心思路是利用冻结的、预训练的基础模型SAM2强大的提示驱动和时序感知能力，预先提取干净的目标中心特征，从而将下游策略学习任务简化为一个单模态回归问题。\n\n## 方法详解\nSAM2Grasp的整体框架由两部分构成：一个**参数冻结的SAM2感知主干**和一个**轻量级可训练的动作策略头**。其工作流程是：在初始时刻，一个外部提示（如边界框）输入给SAM2，指定待抓取的目标物体；SAM2基于此提示，从视频流中提取出专注于该目标物体的时序视觉特征；这些特征与机器人本体感知状态一同输入到动作头中，预测出一段未来的抓取轨迹。在后续时刻，SAM2凭借其内置的时序记忆自动跟踪目标，无需额外提示。\n\n![方法框架](https://arxiv.org/html/2512.02609v1/x2.png)\n> **图2**：SAM2Grasp架构。左侧展示了在初始时刻（t=0）需要外部提示p来引导SAM2提取目标中心特征Ft；在后续时刻（t>0），SAM2利用其内部时序记忆自动跟踪目标。右侧展示了可训练的ACT策略头接收特征Ft和本体状态Propt，输出未来动作序列。该设计在感知阶段解决了物体层面的多模态问题。\n\n**核心模块1：冻结的感知主干（SAM2）**。本文完整使用了预训练的Segment Anything Model 2 (SAM2)，并保持其所有参数冻结。SAM2充当了一个强大的时序视觉特征提取器。给定初始提示和视频帧序列，其内置的记忆与跟踪机制能够识别并持续跟踪指定物体，为每一时刻输出一个富含语义的、目标感知的特征表示Ft。这些特征直接作为动作策略的输入。\n\n**核心模块2：可训练的动作策略头（ACT）**。动作头采用了基于Transformer的动作分块（Action Chunking Transformer, ACT）架构。该策略头在每个时间步t接收两个输入：SAM2提取的特征Ft和机器人的当前本体感知状态Propt（如关节角、夹爪状态）。其输出是一个固定时间窗口内的未来动作序列。本文展示了该框架的灵活性，支持关节空间控制（输出目标关节角）和任务空间控制（输出末端执行器6D位姿）两种动作表示。\n\n**创新点与高效训练范式**。与现有方法相比，SAM2Grasp的核心创新在于深度集成了SAM2的**提示驱动感知**和**内置时序跟踪能力**，而非简单地在输入层面拼接条件信息。这使得策略接收到的是经过预过滤的、干净的目标中心特征。这种架构分离（冻结主干+轻量头部）解锁了一个高效的两阶段训练范式：\n1.  **离线特征提取与缓存**：使用冻结的SAM2对整个演示数据集进行单次前向传播，提取并保存所有时刻的特征序列Ft，生成新的数据集D’ = {(Ft, a*t)}。\n2.  **动作头训练**：仅训练ACT策略头的参数，在数据集D’上最小化预测动作与专家动作之间的回归损失（如MSE）。这种方法避免了微调大型基础模型的巨大计算成本，训练速度更快、硬件需求更低且更稳定。\n\n**推理执行**。为实现高响应性和平滑控制，SAM2Grasp采用异步推理策略，包含两个并发线程：**策略推理线程**（约20Hz）负责获取图像、通过SAM2提取特征、运行ACT策略并预测动作块；**机器人控制线程**（约100Hz）以高频率从共享缓冲区中查询并整合（时序集成）当前时刻可用的多个预测动作，发送给底层控制器执行。此设计掩盖了视觉推理的延迟，并利用动作块的重叠预测来提高鲁棒性。\n\n## 实验与结果\n**实验设置**：在仿真和真实世界环境中进行评估。仿真使用MuJoCo搭建的**多物体拾取交接任务**场景（3-6个随机摆放的立方体）。真实世界使用6自由度机械臂执行**杂乱箱拣选任务**，对象包括电池（用吸盘）和电源线（用平行夹爪）。评估指标为成功率（SR）。\n\n**对比方法**：\n- **ACT**：标准确定性ACT策略，仅接收原始RGB图像。\n- **ACT-CVAE**：使用CVAE建模多模态动作分布的ACT。\n- **ACT-CVAE-Condition**（强基线）：在每帧RGB图像上渲染由跟踪器得到的目标边界框，为ACT-CVAE提供视觉条件。\n- **SAM2Grasp (Ours)**：本文方法。\n\n![仿真实验场景](https://arxiv.org/html/2512.02609v1/x3.png)\n> **图3**：仿真实验的多物体拾取交接任务场景，展示了双臂机器人和随机摆放的彩色立方体。\n\n![真实实验场景](https://arxiv.org/html/2512.02609v1/x4.png)\n> **图4**：真实世界实验的杂乱箱拣选任务场景，展示了机械臂、腕部相机以及待抓取的电池和电源线。\n\n**关键实验结果**：\n1.  **仿真性能对比**（表I）：标准ACT和ACT-CVAE成功率分别仅为47.3%和50.8%，表明在输出端建模多模态收效甚微。提供视觉条件的强基线（ACT-CVAE-Condition）将成功率大幅提升至81.0%，验证了目标条件化的有效性。而SAM2Grasp取得了**87.8%** 的最高成功率，显著优于强基线，证明SAM2提供的“提示驱动感知”比简单的“视觉条件化”更能提取出干净、聚焦的特征。\n2.  **真实世界性能对比**（表II）：SAM2Grasp在电池和电源线抓取任务中分别达到98.5%和95.5%的成功率，**平均成功率高达97.0%**，远超ACT（41.5%）和ACT-Condition（71.7%），展现了其在真实复杂环境中的卓越可靠性和泛化能力。\n3.  **遮挡鲁棒性分析**：实验通过随机黑屏模拟输入视频流帧丢失的遮挡情况。结果（表III，图5）显示，随着遮挡率p升高，所有基线方法性能急剧下降至接近0%，而SAM2Grasp展现出惊人韧性：在40%遮挡率下仍保持77.0%的成功率，在60%严重遮挡下仍有66.0%。这直接归功于SAM2内置的时序记忆机制，能在帧缺失时维持对目标状态的稳定估计。\n\n![遮挡鲁棒性结果](https://arxiv.org/html/2512.02609v1/x5.png)\n> **图5**：模拟视觉遮挡下的成功率曲线。SAM2Grasp（橙色）的性能随遮挡率增加缓慢下降，而所有基线方法（蓝、绿、红色）的性能迅速崩溃至零，凸显了SAM2时序跟踪能力对鲁棒性的关键贡献。\n\n**消融实验总结**：主要对比实验本身构成了有效的消融研究。结果表明：（1）**提供目标条件**（对比ACT与ACT-Condition）是解决物体层面多模态的关键，带来巨大性能提升；（2）**利用SAM2进行提示驱动感知**（对比ACT-Condition与SAM2Grasp）比让标准网络学习视觉条件更有效，带来了进一步的显著提升；（3）**SAM2的时序能力**是遮挡鲁棒性的核心贡献组件。\n\n## 总结与启发\n**核心贡献**：\n1.  **提出了一种解决抓取多模态问题的新范式**：通过引入目标提示，将多模态的模仿学习任务转化为单模态的、提示条件化的预测问题。\n2.  **设计了一种高效且鲁棒的架构**：深度集成冻结的时序感知基础模型（SAM2）与轻量级动作策略（ACT），实现了感知与控制的高效解耦，仅需训练极少的参数。\n3.  **实证展示了卓越的性能与鲁棒性**：在仿真和真实世界任务中达到SOTA性能，并展现出对严重视觉遮挡的非凡抵抗力，这直接源于SAM2的时序记忆能力。\n\n**局限性**：论文提到，初始提示需要由上游模块（如物体检测模型）提供，这增加了系统对上游模块的依赖。\n\n**对后续研究的启示**：本文展示了将大型、预训练的基础模型（特别是具备时序和提示能力的模型）作为机器人策略核心、冻结组件的巨大潜力。这种“感知即服务”的范式能极大简化策略学习、提升数据效率与系统鲁棒性。未来工作可探索利用大语言模型等根据高层指令自动生成初始提示，并将此框架扩展至更广泛的操纵技能中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.02609v1/x1.png",
        "https://arxiv.org/html/2512.02609v1/x2.png",
        "https://arxiv.org/html/2512.02609v1/x3.png",
        "https://arxiv.org/html/2512.02609v1/x4.png",
        "https://arxiv.org/html/2512.02609v1/x5.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.02787",
      "title": "Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols",
      "url": "http://arxiv.org/abs/2512.02787",
      "arxivId": "2512.02787",
      "date": "2025-12-02",
      "authors": "Yong-Lu Li Team",
      "category": "Manipulation",
      "summary": "本文针对VLA模型在机器人操作中失败诊断和学习能力有限、且现有失败数据集仿真生成导致泛化不足的问题，提出ViFailback框架。该框架利用显式视觉符号高效标注真实失败数据，构建包含58,126个VQA对的大规模ViFailback数据集和ViFailback-Bench基准。基于此训练ViFailback-8B VLM模型，在基准测试中实现显著性能提升，并能生成视觉符号提供纠正指导。真实机器人实验表明，集成该模型可有效辅助VLA模型从失败中恢复。",
      "detailedSummary": "## 研究背景与动机\n当前，视觉-语言-动作（VLA）模型在机器人操作任务中取得了显著进展，但它们普遍缺乏对操作失败的诊断和从失败中学习的能力。同时，现有用于训练和评估失败诊断的失败数据集大多是通过在仿真环境中程序化注入扰动生成的，这导致了显著的仿真到现实（sim-to-real）鸿沟，限制了其在真实世界中的泛化能力。此外，现有方法提供的失败反馈多为文本形式，而当前VLA模型对文本指令的跟随能力尚不鲁棒，这限制了机器人实际执行纠正动作的效果。\n\n本文针对上述痛点，提出了一个利用**视觉符号**（Visual Symbols）来高效标注真实世界机器人失败数据、诊断失败并提供多模态（文本与视觉）纠正指导的新框架。其核心思路是：设计一套语义明确的视觉符号系统，辅助人类对真实失败视频进行高效、低成本的标注，并基于此构建大规模视觉问答（VQA）数据集，进而训练一个能同时进行失败诊断和生成视觉纠正指导的视觉语言模型（VLM），最终将该VLM作为外部监督器集成到VLA策略中，帮助机器人从失败中恢复。\n\n## 方法详解\n本文提出的ViFailback框架是一个系统性处理真实世界机器人失败数据的流程，其整体框架如图2所示。\n\n![ViFailback框架总览](https://arxiv.org/html/2512.02787v2/x2.png)\n> **图2**：ViFailback框架概览。**左侧**：通过遥操作和策略rollout收集真实世界操作轨迹，然后使用基于视觉符号的高效标注框架生成用于数据集的VQA对。**中部**：数据集包含来自5,202条真实世界轨迹的58,126个VQA对。从中提取出ViFailback-Bench（Lite和Hard版本）用于评估VLM的失败诊断与纠正能力。**右侧**：在VQA对上微调Qwen3-VL-8B得到ViFailback-8B。该模型作为外部监督器部署，以协助策略从失败中恢复。\n\n**核心模块与技术细节**：\n1.  **视觉符号系统**：这是框架的核心创新，旨在将抽象的纠正意图转化为可绘制在图像上的直观图形，以提升标注效率和后续VLM生成指导的精确性。系统包含7种符号，分为三类：\n    *   **运动符号**：表示末端执行器的平移和旋转。**彩色直线箭头**（红、绿、蓝）分别对应前-后、左-右、上-下三个正交方向的移动；**半圆形箭头**表示旋转方向（顺时针/逆时针）。\n    *   **空间关系符号**：指定场景中的正确目标或对象间期望的对齐关系。**双十字准星**（由虚线连接）表示两个目标应对齐；**十字准星**用于高亮理想的目标对象或区域。\n    *   **状态符号**：指示目标对象的期望状态。**ON/OFF标签**表示夹爪的理想开合状态；**禁止图标**表示期望夹爪停止；**倒带图标**表示指定组件需回到先前状态。\n    这些符号的具体应用示例如补充材料图1所示，可以单独或组合使用以表达复杂的纠正指令。\n\n2.  **细粒度任务定义**：将失败分析分解为**失败诊断**和**纠正动作指导**两部分。\n    *   **失败诊断**包含五个子任务：失败检测、失败关键帧定位、失败子任务定位、失败类型识别（分为任务规划、夹爪6D位姿、夹爪状态、人为干预四类）以及失败原因推理。\n    *   **纠正动作指导**要求模型提供三种形式的指导：**低层级文本指导**（具体的移动/旋转方向）、**高层级文本指导**（如重新规划子任务的高层策略）以及**视觉指导**（在关键帧上叠加具有语义信息的视觉符号）。\n\n3.  **数据收集与标注流程**：使用ALOHA双臂遥操作平台收集了5,202条真实世界操作轨迹（覆盖100个不同任务），其中包含4,545条失败轨迹。标注流程分为三个阶段（对应图2左侧）：\n    *   **阶段1（基础语义信息填充）**：标注员通过UI控件（滑块、按钮）完成失败诊断的基础标注。\n    *   **阶段2（文本指导选择与视觉符号绘制）**：基于选定的关键帧，标注员从预定义类别中选择纠正动作，并通过鼠标拖拽绘制相应的视觉符号。\n    *   **阶段3（开放式描述生成与精炼）**：使用强大的VLM（Qwen3-VL-235B）根据前两阶段的标注信息和视觉符号，自动生成失败原因和高层文本指导的描述，再由人工验证和精炼，确保高质量。\n\n4.  **ViFailback-Bench基准**：从数据集中提取500条轨迹构建评估基准，分为两个互补的设置（如图3所示）：\n    *   **ViFailback-Bench Lite**：使用**封闭式**VQA，评估基于给定关键帧的核心失败诊断能力和低层级纠正指导。\n    *   **ViFailback-Bench Hard**：使用**开放式**VQA，评估失败原因推理和高层级/基于思维链（CoT）的纠正指导能力。其低层级指导任务更复杂，要求模型先检测并定位失败，再以CoT格式输出指导。\n\n![ViFailback-Bench结构](https://arxiv.org/html/2512.02787v2/x3.png)\n> **图3**：ViFailback-Bench概览。**左侧**：Lite基准使用封闭式VQA测试VLM的失败诊断（如检测、定位）和低层级纠正指导。**右侧**：Hard基准使用开放式VQA测试失败原因和高层级/基于CoT的指导。\n\n**创新点**：\n*   **数据来源**：首次大规模收集并系统化标注**真实世界**的机器人操作失败数据，避免了仿真数据的sim-to-real鸿沟。\n*   **标注方式**：引入**视觉符号**作为中间表示，极大提升了复杂、抽象类别（如失败原因、高层指导）的标注效率，并使得模型能够学习生成这种对机器人更友好的指导形式。\n*   **指导形式**：强调并提供**多模态（文本+视觉）纠正指导**，相较于纯文本反馈，能更直接、鲁棒地引导VLA模型执行纠正动作。\n\n## 实验与结果\n**实验设置**：\n*   **基准/数据集**：使用本文提出的ViFailback-Bench（包含Lite和Hard子集）进行评估。\n*   **实验平台**：真实机器人实验使用ALOHA双臂平台，VLA actor采用在20条专家演示上微调的π-0.5模型。\n*   **对比基线**：全面比较了16个先进模型，包括2个领先的专有VLM（GPT-4o, Gemini-2.5-Pro）、10个开源通用VLM（Qwen2.5-VL系列、Qwen3-VL系列、InternVL3系列）和4个开源具身VLM（RoboBrain2.0系列、Cosmos-Reason1-7B）。\n\n**关键实验结果**：\n1.  **基准测试性能**：在ViFailback-Bench上的总体性能对比如表1所示。本文通过微调Qwen3-VL-8B得到的**ViFailback-8B模型在Lite和Hard基准上分别达到了56.12%和43.36%的准确率，平均性能为50.24%，显著优于所有其他开源模型，并与顶尖专有模型性能相当**。这表明基于ViFailback数据集微调能有效提升VLM的失败诊断与纠正能力。\n\n![模型性能对比表](https://arxiv.org/html/2512.02787v2/x4.png)\n> **表1**：模型在ViFailback-Bench上的整体性能对比。所有指标均报告为准确率（%）。粗体表示最佳性能，下划线表示次佳性能。ViFailback-8B表现突出。\n\n2.  **消融实验与组件贡献**：论文通过微调实验本身验证了数据集的贡献。以Qwen3-VL-8B为基础模型，在ViFailback数据集上微调后，其在Bench上的平均性能从35.92%提升至50.24%，**绝对提升高达14.32个百分点**，直接证明了该数据集对于增强VLM相关能力的有效性。\n\n3.  **真实世界机器人实验**：将ViFailback-8B作为外部监督器集成到VLA策略中。在策略执行过程中，每隔6个动作块查询一次ViFailback-8B，让其分析过去5秒的视觉观察，必要时提供多模态纠正指导。实验结果表明，**集成ViFailback-8B的系统的任务平均成功率比基线（无ViFailback-8B辅助）提高了22.2%**。图13-18等展示了模型生成文本/视觉指导以及机器人成功执行纠正动作的实例。\n\n![真实实验可视化示例](https://arxiv.org/html/2512.02787v2/x13.png)\n> **图13**：PlaceOne任务的可视化。**上&中部**：ViFailback-8B诊断失败并提供低层级多模态（文本和视觉）指导。**底部**：通过PMC方法执行纠正动作以从失败中恢复。此图展示了框架在真实机器人任务中的闭环应用效果。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**ViFailback框架**，利用一套精心设计的视觉符号系统，实现了对真实世界机器人失败视频的高效、低成本标注，并能生成包含视觉指导的纠正信息。\n2.  构建并发布了大规模**ViFailback数据集**（58,126个VQA对）及配套的**ViFailback-Bench基准**，为社区提供了首个专注于细粒度机器人失败推理的真实世界评估资源。\n3.  通过微调得到的**ViFailback-8B模型**在基准测试上表现优异，并且**真实机器人实验**证实，将其作为监督器集成到VLA策略中，能显著提升策略从失败中恢复的能力，平均任务成功率提升22.2%。\n\n**局限性**：论文自身提到，所设计的视觉符号集可能无法覆盖所有可能的失败纠正场景，其表达范围存在边界。\n\n**对后续研究的启示**：\n*   **视觉提示的探索**：本文证明了视觉符号作为一种高效的、机器可读的交互媒介的潜力。后续工作可以探索更丰富、更精细的视觉提示形式，以表达更复杂的操作意图。\n*   **闭环学习系统**：ViFailback-8B作为“诊断-纠正”模块，与执行模块（VLA）的结合展示了开环纠正的可行性。未来的方向可以是构建一个能够将从失败中学习到的经验反馈给策略并更新策略参数的**闭环学习系统**，使机器人真正具备从失败中自主学习和改进的能力。\n*   **基准的拓展**：ViFailback-Bench为失败诊断与纠正能力评估奠定了基础。后续可考虑纳入更多样化的任务场景、更复杂的多步骤失败，以及评估模型在连续决策中实时干预的能力。",
      "imageUrls": [
        "https://arxiv.org/html/2512.02787v2/x1.png",
        "https://arxiv.org/html/2512.02787v2/x2.png",
        "https://arxiv.org/html/2512.02787v2/x3.png",
        "https://arxiv.org/html/2512.02787v2/x4.png",
        "https://arxiv.org/html/2512.02787v2/x5.png",
        "https://arxiv.org/html/2512.02787v2/x6.png",
        "https://arxiv.org/html/2512.02787v2/x7.png",
        "https://arxiv.org/html/2512.02787v2/x8.png",
        "https://arxiv.org/html/2512.02787v2/x9.png",
        "https://arxiv.org/html/2512.02787v2/x10.png",
        "https://arxiv.org/html/2512.02787v2/x11.png",
        "https://arxiv.org/html/2512.02787v2/x12.png",
        "https://arxiv.org/html/2512.02787v2/x13.png",
        "https://arxiv.org/html/2512.02787v2/x14.png",
        "https://arxiv.org/html/2512.02787v2/x15.png",
        "https://arxiv.org/html/2512.02787v2/x16.png",
        "https://arxiv.org/html/2512.02787v2/x17.png",
        "https://arxiv.org/html/2512.02787v2/x18.png",
        "https://arxiv.org/html/2512.02787v2/x19.png",
        "https://arxiv.org/html/2512.02787v2/x20.png",
        "https://arxiv.org/html/2512.02787v2/x21.png",
        "https://arxiv.org/html/2512.02787v2/x22.png",
        "https://arxiv.org/html/2512.02787v2/x23.png",
        "https://arxiv.org/html/2512.02787v2/x24.png",
        "https://arxiv.org/html/2512.02787v2/x25.png",
        "https://arxiv.org/html/2512.02787v2/x26.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.02013",
      "title": "ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.02013",
      "arxivId": "2512.02013",
      "date": "2025-12-01",
      "authors": "Shanghang Zhang Team",
      "category": "Manipulation",
      "summary": "本文针对现有视觉-语言-动作模型在乐高组装等长时程任务中难以协调高层规划与精细操作的难题，提出统一框架ManualVLA。其关键技术包括：基于混合Transformer架构，设计规划专家生成多模态操作手册，并通过Manual Chain-of-Thought将手册显式与隐式信息输入动作专家以指导执行。实验表明，该模型在乐高组装与物体重排任务上平均成功率比之前最佳分层基线提升32%。",
      "detailedSummary": "## 研究背景与动机\n当前，基于互联网规模预训练的视觉语言模型（VLMs），视觉语言操作（VLA）模型通过在大规模机器人演示数据上训练，展现出强大的场景理解和操作泛化能力。然而，面对需要明确定义目标状态的长视野任务，例如乐高（LEGO）组装或物体重排，现有VLA模型在协调高层规划与精确操作方面仍面临挑战。其核心困难在于，模型必须严格对齐预定义的最终场景配置，并整合长视野规划与细粒度控制，同时保持对多样化现实环境的泛化能力。\n\n现有方法主要分为两类：一类是端到端的VLA模型，直接将感知输入映射到动作，缺乏对中间过程的显式规划；另一类分层方法则依赖人工提供的手册或演示视频，这限制了其对未知目标状态的泛化能力，并增加了对人的依赖。本文针对“如何让VLA模型具备从期望的‘是什么’（目标状态）推断出程序性的‘怎么做’（执行步骤）这一人类能力”这一具体痛点，提出了一个新视角：将目标状态转化为可执行的多模态手册，并以此引导精确操作。本文核心思路是构建一个基于混合专家Transformer（MoT）架构的统一VLA模型（ManualVLA），通过设计的手动思维链（ManualCoT）推理过程，实现多模态手册生成与动作执行的协同合作。\n\n## 方法详解\nManualVLA的整体框架是一个基于MoT架构的统一模型，包含规划专家和操作专家两个核心模块。给定语言指令 `l`、当前状态图像 `I_t_current` 和最终目标状态图像 `I_goal`，模型首先通过规划专家生成一个中间手册，该手册包含目标物体的文本描述 `l_hat_t`、其目标2D坐标 `p_t` 以及对应的子目标图像 `I_t_subgoal`。基于此手册，通过将目标位置作为掩码叠加到当前场景图像上，构建一个提示图像 `I_t_prompt`。最后，操作专家以机器人状态 `s_t`、提示图像 `I_t_prompt` 以及生成手册时存储的隐式特征（`F_t_subgoal`, `F_t_p`, `F_t_l_hat`）为条件，生成未来 `h` 步的动作块 `a_t:t+h`。\n\n![方法框架](https://arxiv.org/html/2512.02013v1/x2.png)\n> **图2**：ManualVLA框架。(a) 整体流程：规划专家根据指令、当前图像和目标图像生成多模态手册（文本、位置、子目标图像）；通过显式CoT（将位置提示叠加为视觉提示）和隐式CoT（通过跨任务共享注意力机制）引导操作专家生成动作。(b) 隐式CoT细节：通过设计的注意力掩码，使操作专家能关注手册生成的特征，实现隐式引导。(c) 三阶段训练策略：分别预训练操作专家和规划专家，最后进行联合微调。\n\n**核心模块与技术细节**：\n1.  **视觉模块**：采用双视觉路径。手册生成使用基于VQGAN的视觉分词器（编码器-量化器-解码器），将图像转换为离散token。操作生成使用连续的SigLIP-Large视觉编码器，提取高维语义特征。\n2.  **混合专家Transformer LLM**：基础语言模型为DeepSeek-LLM 1.5B。在其上构建MoT架构，为非嵌入组件（如前馈网络FFN、注意力投影、层归一化）引入任务特定的参数集，形成规划专家和操作专家。每个token根据其任务类别（手册或动作）选择对应的参数进行计算，而注意力权重矩阵则在所有token间全局计算，从而在统一架构内实现任务专化与跨任务信息交互。\n3.  **动作与状态组件**：采用基于扩散策略的动作建模。引入噪声编码器和解码器（均为两层MLP）来处理加噪和去噪的动作。机器人状态通过另一个两层MLP（状态编码器）注入操作专家。\n4.  **手动思维链（ManualCoT）推理**：\n    *   **显式CoT**：利用生成的位置坐标 `(U, V)` 在當前圖像上疊加掩碼，生成提示图像 `I_t_prompt`，作为操作专家的明确视觉引导。\n    *   **隐式CoT**：在生成手册时，存储文本、位置和子目标图像对应的键值特征（`F_t_l_hat`, `F_t_p`, `F_t_subgoal`）。在操作生成阶段，通过设计的跨任务共享注意力机制，使操作专家能够关注这些特征，从而在潜在空间中获得“操作什么物体”、“放置在哪里”以及“操作后的预期视觉结果”的隐式引导。\n\n**创新点**：与现有VLA模型直接映射感知到动作或依赖外部规划不同，ManualVLA的创新在于：1) 首次在统一VLA框架内集成多模态手册生成与动作执行；2) 提出了结合显式（视觉提示）和隐式（潜在特征引导）的ManualCoT推理机制，将生成的手册有效转化为精确操作条件；3) 利用MoT架构在单一模型中灵活协调规划与操作两种异构任务。\n\n## 实验与结果\n**实验设置**：\n*   **任务**：2D乐高组装、3D乐高组装、物体重排。\n*   **基准/数据集**：在双臂Franka机器人平台上进行真实世界评估。同时，在RLBench基准测试通用操作任务。\n*   **对比基线**：分为三类：1) 强大动作生成范式：π0, π0.5, FAST；2) 结合目标图像和子目标图像预测的CoT-VLA；3) 分层SOTA方法：CheckManual（依赖预定义手册）和Vid2Robot（依赖人类演示视频）。\n\n**关键实验结果**：\n在长视野任务上，ManualVLA展现出显著优势。\n\n![长视野任务成功率对比](https://arxiv.org/html/2512.02013v1/x4.png)\n> **图4**：在2D/3D LEGO组装和物体重排任务上的成功率对比。ManualVLA在所有任务上均显著优于所有基线方法。\n\n具体而言，ManualVLA在2D乐高组装、3D乐高组装和物体重排任务上的平均成功率达到**79.0%**，比之前的分层SOTA基线（CheckManual）平均高出**32%**。与端到端VLA基线（π0.5， FAST）相比，优势更为明显。ManualVLA在RLBench通用操作任务上也达到了SOTA性能。\n\n**消融实验**：\n\n![消融实验结果](https://arxiv.org/html/2512.02013v1/x5.png)\n> **图5**：消融实验。(a) 架构消融：完整的MoT架构（ManualVLA）性能最佳，移除MoT（单专家）或移除跨任务注意力（无共享）均导致性能下降。(b) CoT机制消融：同时使用显式和隐式CoT效果最好，仅使用任一或两者都不使用都会降低成功率。\n\n消融实验验证了核心组件的贡献：\n1.  **MoT架构**：使用完整的MoT架构（规划与操作专家分离但共享注意力）性能最佳。移除MoT（使用单一专家处理所有任务）或移除跨任务共享注意力机制，成功率分别下降17.3%和10.7%。\n2.  **CoT机制**：同时使用显式CoT（视觉提示）和隐式CoT（潜在特征引导）效果最好。仅使用显式、仅使用隐式或两者都不使用，成功率分别下降15.7%、23.0%和31.3%。\n3.  **数据效率**：得益于手册条件，ManualVLA在下游任务微调时仅需约100条演示轨迹即可实现泛化操作，数据效率很高。\n\n**泛化能力评估**：\n\n![泛化能力定性结果](https://arxiv.org/html/2512.02013v1/x6.png)\n> **图6**：ManualVLA在未见过的物体形状、背景和光照条件下的泛化能力定性示例。模型能成功处理新的乐高砖块形状、复杂的桌面背景以及变化的照明。\n\nManualVLA在未见过的物体形状、背景纹理和光照条件下均表现出良好的泛化能力。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了ManualVLA，一个基于MoT架构的统一VLA模型，首次实现了从最终目标状态出发，协同进行多模态手册生成和机器人操作。\n2.  设计了手动思维链（ManualCoT）推理过程，通过显式视觉提示和隐式潜在特征引导，将生成的手册有效地转化为精确的操作条件。\n3.  开发了基于3D高斯泼溅的高保真数字孪生工具包，用于自动生成手册数据，并提出了高效的三阶段训练策略，使模型在少量真实数据微调后即能取得卓越性能。\n\n**局限性**：论文提到，其数字孪生数据生成目前依赖于对特定物体（如乐高砖块）的3D重建和仿真，这可能需要为新的物体类别调整流程。此外，联合微调阶段仍需收集少量真实世界演示数据。\n\n**启示**：本研究为VLA模型处理长视野、目标条件任务开辟了新路径，表明显式的中间过程推理（以多模态手册形式）与隐式的特征级引导相结合，能显著提升规划的可靠性和操作的精确性。未来的工作可以探索更自动化的世界模型来生成手册数据，或将此“目标→手册→动作”的范式扩展到更广泛的具身AI任务中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.02013v1/x1.png",
        "https://arxiv.org/html/2512.02013v1/x2.png",
        "https://arxiv.org/html/2512.02013v1/x3.png",
        "https://arxiv.org/html/2512.02013v1/x4.png",
        "https://arxiv.org/html/2512.02013v1/x5.png",
        "https://arxiv.org/html/2512.02013v1/x6.png",
        "https://arxiv.org/html/2512.02013v1/x7.png",
        "https://arxiv.org/html/2512.02013v1/Image/asset.png",
        "https://arxiv.org/html/2512.02013v1/x8.png",
        "https://arxiv.org/html/2512.02013v1/x9.png",
        "https://arxiv.org/html/2512.02013v1/x10.png",
        "https://arxiv.org/html/2512.02013v1/x11.png",
        "https://arxiv.org/html/2512.02013v1/x12.png",
        "https://arxiv.org/html/2512.02013v1/x13.png",
        "https://arxiv.org/html/2512.02013v1/x14.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01996",
      "title": "Learning Sim-to-Real Humanoid Locomotion in 15 Minutes",
      "url": "http://arxiv.org/abs/2512.01996",
      "arxivId": "2512.01996",
      "date": "2025-12-01",
      "authors": "Pieter Abbeel Team",
      "category": "Manipulation",
      "summary": "本文针对人形机器人从仿真到现实（sim-to-real）的控制策略训练耗时过长、难以快速迭代的问题，提出了一种基于离策略强化学习算法（FastSAC/FastTD3）的简单高效方案。该方法通过大规模并行仿真、精心调校的设计与极简奖励函数，实现了在单张RTX 4090 GPU上仅用**15分钟**即可完成稳健运动策略的训练。实验表明，该方案能在包含随机动力学、崎岖地形及外力扰动等强域随机化条件下，成功部署于Unitree G1与Booster T1机器人，并快速学习全身运动跟踪策略。",
      "detailedSummary": "## 研究背景与动机\n近年来，大规模并行仿真框架的出现，已将机器人强化学习的训练时间从数天缩短至数分钟。然而，对于人形机器人这类高维系统，要实现快速且可靠的从仿真到现实（sim-to-real）的策略迁移仍然困难。这主要是因为为了提升鲁棒性而引入的域随机化（如随机化动力学、粗糙地形、扰动）等技术，增加了探索难度，降低了样本效率，使得训练时间重回数小时量级。当前，基于近端策略优化（PPO）等策略上（on-policy）算法是sim-to-real RL的主流选择，因其易于与并行仿真扩展。本文针对sim-to-real迭代周期长这一具体痛点，提出采用经过调优的策略下（off-policy）RL算法（FastSAC和FastTD3）的新视角，结合极简的奖励函数设计，旨在将人形机器人的完整训练周期缩短至15分钟。本文的核心思路是：通过精心调整的超参数和设计选择，在大规模并行仿真环境下稳定高效的off-policy RL算法，并辅以仅包含必要项的奖励函数，从而实现人形机器人运动控制策略的端到端快速学习与部署。\n\n## 方法详解\n本文提出的方法是一个基于off-policy RL的完整训练方案。其整体流程是：利用大规模并行仿真环境（数千个）同步收集交互数据，存储于经验回放池中；随后，算法（FastSAC或FastTD3）从回放池中采样大批次数据（如8K）进行多次梯度更新，以高效复用数据。输入为机器人的观测（如本体状态、命令等），输出为关节电机的目标位置（通过PD控制器转换为扭矩）。\n\n![方法框架](https://arxiv.org/html/2512.01996v1/x1.png)\n> **图1**：结果总览。本文提出的基于FastSAC和FastTD3的简单方案，在单张RTX 4090 GPU上，仅用15分钟即可在包含随机动力学、粗糙地形和推力扰动的强域随机化下，学习到鲁棒的人形机器人运动策略。图中星标（★）为用于sim-to-real部署的检查点。\n\n核心模块与技术细节如下：\n1.  **基础算法：FastSAC与FastTD3**：它们是Soft Actor-Critic（SAC）和TD3算法的高效变体，专为大规模并行仿真训练优化。其核心优势在于能够复用历史数据，在仿真速度成为瓶颈（如在非平坦地形训练）时，比on-policy算法更具吸引力。\n2.  **关节限位感知的动作边界**：针对off-policy算法中Tanh策略的动作边界设置难题，提出一种简单技术：根据机器人关节的物理限位与其默认位置的差值，为每个关节设置独立的动作边界。这有效减少了调参需求，稳定了训练。\n3.  **观测与层归一化**：采用观测归一化以稳定训练。对于高维任务，发现层归一化（LayerNorm）比批归一化（BatchNorm）更能稳定性能。\n4.  **评论家学习超参数**：\n    *   **双Q学习**：使用两个Q值的平均值作为目标，优于使用最小值的Clipped Double Q-learning（CDQ），后者与层归一化同时使用时有害。\n    *   **折扣因子**：简单速度跟踪任务使用较低的折扣因子（γ=0.97），而更具挑战性的全身运动跟踪任务使用较高的折扣因子（γ=0.99）。\n    *   **分布评论家**：使用C51分布评论家，但发现分位数回归版本在大批次训练中开销过大。\n5.  **探索超参数**：\n    *   **FastSAC**：限制策略网络输出的标准差σ最大为1.0（而非常见的e²），并以较低值（0.001）初始化熵温度α，结合自动调整熵温度，以避免初期过度探索导致的不稳定。目标熵设置为0.0（运动任务）或-|𝒜|/2（全身跟踪任务）。\n    *   **FastTD3**：使用混合噪声调度，从范围[0.01, 0.05]内随机采样高斯噪声标准差。\n6.  **优化超参数**：使用Adam优化器，学习率0.0003，权重衰减0.001（高维任务需要更弱的正则化），β₂=0.95（比默认的0.99更稳定）。\n7.  **极简奖励设计**：摒弃传统复杂奖励塑形（20+项），采用少于10项的极简奖励集。对于运动（速度跟踪）任务，仅包含：线速度和角速度跟踪奖励、简单的脚部高度跟踪项、默认姿势惩罚、脚部方向与交叉惩罚、存活奖励、躯干姿态惩罚、动作速率惩罚。所有惩罚项的权重通过课程学习随训练进程递增。此外，使用对称性增强以鼓励对称步态。\n\n与现有方法相比，创新点具体体现在：1) 首次将off-policy RL算法稳定、高效地扩展至控制所有关节的全身人形机器人运动与跟踪任务；2) 提出并验证了一套针对大规模off-policy RL训练的具体设计选择与超参数配置（如层归一化、禁用CDQ、特定的探索与优化设置），特别是显著改进了此前不稳定的FastSAC；3) 证明了极简奖励函数结合强域随机化，足以产生鲁棒、可迁移的复杂行为。\n\n## 实验与结果\n**实验设置**：\n*   **平台与机器人**：使用单张RTX 4090 GPU进行运动训练，使用4张L40s GPU进行全身跟踪训练。实体机器人为Unitree G1和Booster T1。\n*   **任务与基准**：\n    *   **运动（速度跟踪）**：在混合平坦与粗糙地形上，训练机器人跟踪随机采样的目标速度。域随机化包括：推力扰动（Push-Strong：每1-3秒；其他：每5-10秒）、动作延迟、PD增益随机化、质量随机化、摩擦随机化、质心随机化（仅G1）。\n    *   **全身运动跟踪**：训练机器人跟踪人类运动片段（如舞蹈、举箱、抗推）。域随机化包括：摩擦、质心、关节位置偏差、身体质量、PD增益随机化及推力扰动。\n*   **对比方法**：主要对比算法为PPO（作为on-policy RL基准），以及本文改进前后的FastSAC和FastTD3。\n\n**关键实验结果**：\n\n![运动结果](https://arxiv.org/html/2512.01996v1/x3.png)\n> **图3**：运动（速度跟踪）结果。FastSAC和FastTD3能在15分钟内快速训练G1和T1机器人的运动策略，在存在强域随机化（如粗糙地形、强推力扰动）的情况下，大幅超越PPO的实时训练速度。FastSAC在部分设置中略优于FastTD3。\n\n![FastSAC改进](https://arxiv.org/html/2512.01996v1/x4.png)\n> **图4**：FastSAC的改进。本文改进的FastSAC配方（使用层归一化、禁用CDQ、精细调参）相比先前配置，性能得到显著提升和稳定。\n\n![全身跟踪结果](https://arxiv.org/html/2512.01996v1/x5.png)\n> **图5**：全身跟踪结果。在全身运动跟踪任务中，FastSAC和FastTD3与PPO竞争或更优。FastSAC在更长的舞蹈任务中表现优于FastTD3。\n\n![全身跟踪示例](https://arxiv.org/html/2512.01996v1/x6.png)\n> **图6**：全身跟踪示例。成功将FastSAC训练的全身跟踪策略（舞蹈、举箱、抗推）部署到真实的Unitree G1硬件上，证明了策略的鲁棒性和可部署性。\n\n**消融实验分析**（基于图2）：\n*   **组件贡献**：实验分析了多个设计选择的影响。(a) 使用平均双Q值优于CDQ；(b) 每个仿真步进行更多梯度更新能加速训练；(c) 层归一化对高维任务性能稳定至关重要；(d) 折扣因子γ需根据任务复杂度调整；(e) & (f) 在全身跟踪任务中，更高的γ和更多的并行环境数对性能有显著正面影响。这些分析共同构成了稳定高效训练配方的关键组成部分。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1) **提出了一个快速sim-to-real迭代的配方**，首次实现了在15分钟内于单GPU上端到端训练出鲁棒的全关节人形机器人运动策略。2) **稳定并提升了off-policy RL在大规模人形控制中的应用**，特别是通过一系列设计选择（如关节限位感知边界、层归一化、特定超参数）解决了FastSAC的训练不稳定问题，使其性能超越FastTD3。3) **验证了极简奖励设计的有效性**，证明了少于10项的奖励结合域随机化和课程学习，足以引导出复杂、鲁棒的行为，简化了调参和迭代流程。\n\n论文自身提到的局限性在于，为了保持方案的简洁性，并未集成所有最新的off-policy RL改进技术（如更高效的分布评论家、更先进的探索方法），作者期待研究社区基于此配方进一步推进前沿。\n\n本文对后续研究的启示在于：1) **离策略RL在高通量仿真中具有巨大潜力**，其数据复用特性对于仿真速度受限的复杂任务训练至关重要，值得进一步探索和优化。2) **“少即是多”的设计哲学**，在奖励函数和算法配置上追求简约，可能比复杂的工程化塑形更有利于快速迭代和泛化。3) 提供了一个可**开源、可复现的基线系统**，为加速人形机器人RL研究迭代提供了实用的蓝图。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01996v1/x1.png",
        "https://arxiv.org/html/2512.01996v1/x2.png",
        "https://arxiv.org/html/2512.01996v1/x3.png",
        "https://arxiv.org/html/2512.01996v1/x4.png",
        "https://arxiv.org/html/2512.01996v1/x5.png",
        "https://arxiv.org/html/2512.01996v1/x6.png",
        "https://arxiv.org/html/2512.01996v1/x7.png",
        "https://arxiv.org/html/2512.01996v1/x8.png",
        "https://arxiv.org/html/2512.01996v1/x9.png",
        "https://arxiv.org/html/2512.01996v1/x10.png",
        "https://arxiv.org/html/2512.01996v1/x11.png",
        "https://arxiv.org/html/2512.01996v1/x12.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01924",
      "title": "Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model",
      "url": "http://arxiv.org/abs/2512.01924",
      "arxivId": "2512.01924",
      "date": "2025-12-01",
      "authors": "Shingo Murata Team",
      "category": "Manipulation",
      "summary": "本文解决真实世界机器人控制中目标导向与探索性动作的平衡问题。提出了一种新的深度主动推理框架，其核心包括：1）世界模型，在快慢双时间尺度编码环境动态；2）动作模型，通过向量量化将动作序列压缩为抽象动作；3）抽象世界模型，基于抽象动作预测未来慢状态，以实现低成本动作选择。在真实机器人物体操作任务上的实验表明，该框架在多种任务中取得高成功率，能在不确定环境下切换目标与探索行为，并显著提升了动作选择的计算效率。",
      "detailedSummary": "## 研究背景与动机\n当前基于深度学习的机器人控制方法，特别是从演示中学习的方法，能够通过模仿专家数据生成多步动作序列，但在面对环境不确定性时泛化能力不足。相比之下，人类能够结合目标导向和探索性行动来适应不确定环境。深度主动推理是一个基于自由能原理的计算框架，理论上可以统一解释这两种行为。然而，传统的深度主动推理方法面临两大挑战：一是其性能严重依赖框架对环境动态的表征能力；二是在任务执行期间计算期望自由能以选择动作的计算成本极高，难以应用于真实机器人。\n\n本文针对深度主动推理在环境表征能力和动作选择计算成本方面的关键局限性，提出了一种新的视角：通过引入具有时间分层结构的世界模型来提升对长期环境动态的表征，并通过学习离散的抽象动作来压缩动作空间，从而将连续动作序列的选择问题转化为对有限个抽象动作的评估问题。本文的核心思路是构建一个由世界模型、动作模型和抽象世界模型组成的框架，利用时间分层的状态表示和抽象化的动作，在保持探索能力的同时，实现计算可行的主动推理控制。\n\n## 方法详解\n提出的框架基于深度主动推理，旨在同时实现目标达成和探索。该框架由三个核心组件构成：世界模型、动作模型和抽象世界模型。\n\n![方法框架](https://arxiv.org/html/2512.01924v1/x1.png)\n> **图1**：提出的框架概览。框架包含世界模型、动作模型和抽象世界模型。关键变量包括：观测 *o_t* 和动作 *a_t* 由世界模型处理以推断分层隐藏状态 *z_t^s*, *z_t^f*。动作模型将动作序列压缩为抽象动作 *A_t*。抽象世界模型使用 *A_t* 来预测未来的慢确定性状态 *d_{t+h}^s*。\n\n**世界模型**负责从机器人动作和观测数据中学习环境动态的隐藏状态表示。它包含一个动态模型、一个编码器和一个解码器。动态模型采用分层结构，包含慢和快两种时间尺度的隐藏状态 *z_t = {z_t^s, z_t^f}*，每种状态又包含确定性状态 *d* 和随机性状态 *s*。慢动态的更新频率较低，用于捕捉任务的高级进展（如物体位置）；快动态更新频率高，捕捉瞬时信息。这是通过为慢层和快层的循环神经网络设置不同的时间常数来实现的。世界模型通过最小化变分自由能 *ℱ(t)* 进行训练，*ℱ(t)* 被分解为慢层和快层的自由能之和，并减去一个基于慢状态重建观测的辅助对数似然项。\n\n![世界模型结构](https://arxiv.org/html/2512.01924v1/x2.png)\n> **图2**：世界模型结构。它由动态模型、编码器和解码器组成。动态模型具有两个不同的时间尺度（慢和快）。\n\n**动作模型**用于将连续的动作序列压缩为离散的抽象动作表示。它由一个编码器 *ℰ_ϕ*、一个包含 *N_q=2* 层的残差向量量化器 *𝒬_ϕ* 和一个解码器 *𝒟_ϕ* 组成。编码器将长度为 *h* 的动作序列 *a_{t:t+h}* 嵌入为低维特征 *A_t*，量化器将其映射为有限码本中的离散代码之和 *Â_t*，解码器再将 *Â_t* 重建为动作序列 *â_{t:t+h}*。*Â_t* 即被视为代表该动作序列的抽象动作。模型通过最小化重建损失和承诺损失进行训练。\n\n**抽象世界模型** *𝒲_ψ* 学习当前世界模型状态 *z_t* 与抽象动作 *A_t* 到未来慢确定性状态 *d_{t+h}^s* 的映射关系。它是一个多层感知机，输入为 *z_t* 和所有可能的抽象动作 *{Â_n}_{n=1}^{K^{N_q}}*，输出对应的未来慢确定性状态预测 *{d̂_{t+h,n}^s}*。其训练目标是最小化预测状态与通过世界模型“潜在想象”得到的目标状态之间的均方误差。\n\n与现有方法相比，创新点具体体现在：1) 引入了时间分层的世界模型，以更好地建模环境中的多时间尺度动态；2) 通过动作模型将连续动作空间离散化为有限的抽象动作，极大地缩小了策略搜索空间；3) 利用抽象世界模型直接预测抽象动作的长期（慢尺度）后果，避免了在连续动作空间上进行耗时的迭代展开预测。\n\n![动作选择流程](https://arxiv.org/html/2512.01924v1/x3.png)\n> **图3**：基于最小化EFE的动作选择流程。首先，为多个抽象动作预测未来状态。然后，为每个预测的未来状态计算EFE。最后，机器人执行从产生最低EFE的抽象动作重建出的动作序列。\n\n**动作选择流程**如图3所示。在每一步，给定当前状态 *z_t*，框架使用抽象世界模型为所有可能的抽象动作 *{Â_n}* 预测 *h* 步后的慢确定性状态 *{d_{t+h,n}^s}*。基于这些预测，通过蒙特卡洛采样近似计算每个抽象动作对应的期望自由能 *𝒢(τ)*。*𝒢(τ)* 包含鼓励减少不确定性的认知价值和鼓励接近目标的外在价值。最终，选择使 *𝒢(τ)* 最小的抽象动作，经动作模型解码后得到实际执行的动作序列。\n\n## 实验与结果\n实验在一个真实机器人操控环境中进行，使用了一个6自由度机械臂和摄像头。环境包含盘子、锅、平底锅、锅盖以及蓝色和红色小球，小球的存在和位置引入了不确定性。训练数据通过遥操作收集，包含了8种预定义策略模式（如将球从盘子移到锅中）的两两组合演示。\n\n**对比的Baseline**：在计算效率评估中，对比了传统的深度主动推理方法，该方法需要利用世界模型通过顺序输入重建出的动作序列来迭代预测未来状态。\n\n**关键实验结果**：\n1.  **抽象世界模型能力**：如图5所示，所提出的框架（Ours）预测未来状态的计算时间约为0.5毫秒，而传统方法（Sequential）需要约100毫秒，速度提升了约200倍。定性实验表明，抽象世界模型能够根据不同的抽象动作产生不同的状态预测，并且预测结果与实际执行相应动作序列后的观测结果一致。\n\n![计算时间对比](https://arxiv.org/html/2512.01924v1/x5.png)\n> **图5**：使用所提框架（Ours）与使用世界模型顺序预测（Sequential）的计算时间比较。所提框架通过抽象世界模型进行单步预测，速度显著更快。\n\n2.  **目标达成性能**：在球操控任务（140次试验）和锅盖操控任务（24次试验）中评估成功率。如图6a所示，在球操控任务中，成功率高达98.6%。在锅盖操控任务中，当目标明确时（如“打开锅盖”），成功率也为100%。\n\n3.  **环境探索能力**：在存在不确定性的任务中（例如，指令为“移动蓝球”，但蓝球可能被锅盖遮住），机器人能够表现出探索行为。如图6b所示，当初始状态锅盖关闭且蓝球位置未知时，机器人首先执行了“打开锅盖”的抽象动作，在确认蓝球存在后，再执行“移动蓝球”的动作，成功完成了任务。这证明了框架能够自主切换探索和目标导向行为。\n\n![实验结果](https://arxiv.org/html/2512.01924v1/x6.png)\n> **图6**：(a) 球操控任务的成功率。(b) 在不确定性环境下的探索行为示例：机器人首先执行打开锅盖的探索动作以确认球的位置，然后执行移动球的目标导向动作。\n\n**消融实验**：论文进行了组件贡献分析。结果显示，完整框架在所有指标上表现最佳。移除抽象世界模型（即使用顺序预测）会导致计算成本剧增。而世界模型中的时间分层结构和基于慢状态的观测预测辅助任务，对于学习有意义的层次化状态表示至关重要。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了一个集成时间分层世界模型、动作模型和抽象世界模型的深度主动推理新框架，有效提升了环境动态表征能力；2) 通过动作抽象和抽象世界模型，将连续动作空间中的策略评估转化为对有限离散选项的评估，使基于期望自由能的实时动作选择在真实机器人上变得计算可行；3) 在真实机器人物体操控任务上验证了该框架不仅能高效完成多样化的目标导向任务，还能在环境不确定性下自主产生探索行为。\n\n论文提到的局限性包括：抽象动作是基于训练数据学习得到的离散集合，这可能限制了其灵活性，对于未在演示中出现过的长时程行为组合，可能无法很好地表示。\n\n这项工作对后续研究的启示是：在构建用于复杂动态环境控制的模型时，显式地建模时间分层结构和进行动作/状态转移的抽象，是平衡模型表达能力、计算效率和智能体行为多样性（探索与利用）的有效途径。将高层抽象规划与低层连续控制相结合，是实现类人适应能力的关键方向。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01924v1/x1.png",
        "https://arxiv.org/html/2512.01924v1/x2.png",
        "https://arxiv.org/html/2512.01924v1/x3.png",
        "https://arxiv.org/html/2512.01924v1/x4.png",
        "https://arxiv.org/html/2512.01924v1/x5.png",
        "https://arxiv.org/html/2512.01924v1/x6.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.02020",
      "title": "EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI",
      "url": "http://arxiv.org/abs/2512.02020",
      "arxivId": "2512.02020",
      "date": "2025-12-01",
      "authors": "Xiangyu Xu Team",
      "category": "Manipulation",
      "summary": "本文提出EfficientFlow框架，解决具身AI中生成策略数据效率低（需大量演示）和采样效率低（推理慢）的问题。关键技术包括：将等变性引入流匹配，使用各向同性高斯先验和等变速度预测网络，确保动作分布等变性以提升泛化、减少数据需求；并提出加速正则化策略，通过替代损失加速采样。实验表明，在多个机器人操作基准测试中，该框架在有限数据下达到竞争或更优性能，且推理速度显著提升。",
      "detailedSummary": "## 研究背景与动机\n当前，基于生成模型（特别是扩散模型）的视觉运动策略学习已成为具身智能中强大且灵活的范式。然而，现有方法存在两个关键局限：一是**数据效率低**，需要大规模演示数据；二是**采样效率低**，由于迭代去噪过程，推理时生成单个动作的计算成本高。近期工作尝试通过将等变性引入扩散策略来提升数据效率，但其底层扩散模型仍需数百步迭代，难以满足实时控制需求。\n\n本文针对**数据效率**和**采样效率**两大痛点，提出了一个新视角：将等变性学习与基于流匹配的生成建模相统一。核心思路是：1）在流匹配框架中引入等变性，利用环境对称性作为归纳偏置以减少数据需求；2）提出一种加速正则化策略，平滑生成轨迹以实现快速推理。\n\n## 方法详解\nEfficientFlow的整体框架是一个基于流匹配的等变策略学习流程。输入为最近两个时间步的观测 `o`，输出为机器人的动作序列。\n\n![方法框架](https://arxiv.org/html/2512.02020v2/x2.png)\n> **图2**：EfficientFlow整体框架。在每个决策步，策略以最近两步观测 `o` 为输入，通过等变流匹配网络生成五条候选动作轨迹，选择与上一段预测轨迹欧氏距离最小的轨迹执行，以确保动作序列的平滑连贯。\n\n核心模块包括**等变流策略**和**加速度正则化**。\n\n**1. 等变流策略**\n该模块旨在使策略学习对几何变换（如平面旋转）具有等变性。其理论基础是**定理1**：若初始分布 `p0` 是各向同性的（如标准高斯噪声），且条件速率场网络 `u_θ(t, x|o)` 是等变的（即 `u_θ(t, gx|go) = g·u_θ(t, x|o)`），则由流ODE（公式1）诱导的条件动作分布在任意时间 `t` 都保持等变性。这意味着当观测 `o` 经历群变换 `g` 时，输出的动作分布会相应变换，无需额外监督或数据增强即可实现跨对称配置的泛化。\n\n网络设计上，使用 `escnn` 库构建对循环子群 `C_u ⊂ SO(2)` 等变的网络。动作表示为10维向量（6D旋转、3D平移、1D夹爪宽度），其等变表示对应为 `(ρ1^3 ⊕ (ρ1 ⊕ ρ0) ⊕ ρ0)`，其中 `ρ1` 是标准表示，`ρ0` 是平凡表示。网络输入为流时间 `t`、动作序列 `x_t` 和观测 `o`，分别通过等变观测编码器和动作编码器得到嵌入，再经由核心等变网络预测速率场 `u_θ`。\n\n为保证长时域执行的时间一致性，采用了重叠预测与批量轨迹选择策略：每次生成 `m` 条候选轨迹，仅执行每条的前 `n1` 步，后续步骤与下一次预测重叠，并选择重叠部分与上一段轨迹最匹配的候选者执行。每10个预测周期进行一次随机重置以保持探索多样性。\n\n**2. 加速度正则化**\n为提升低NFE（函数评估次数）下的性能，使学习到的流场更平滑，本文提出了对轨迹加速度（二阶导数）的正则化。理想目标是在数据项（公式3）基础上增加加速度惩罚项（公式8）。然而，直接计算加速度需知晓同一边缘轨迹上相邻的 `x_t` 和 `x_{t+Δt}`，这在训练中不可得。\n\n因此，本文创新性地提出了一个**代理损失——流加速度上界**：\n`FABO = E[ ||u_θ(t, x̃_t) - u_θ(t+Δt, x̃_{t+Δt})||_2^2 ]`\n其中 `x̃_t` 和 `x̃_{t+Δt}` 来自**同一条条件轨迹**，易于采样。论文在附录中证明了FABO是真实加速度惩罚项的一个上界，因此最小化FABO可有效促进轨迹平滑。训练时使用时间依赖的权重 `λ(t) = (1-t)^2`，在早期更强调平滑性。\n\n## 实验与结果\n**实验设置**：在MimicGen基准测试的12个机器人操作任务上进行评估，任务难度、时间跨度和物体布置多样。输入为智能体视角和腕部相机的RGB图像。对比的基线方法包括：EquiDiff (扩散+等变)、ACT、两种Diffusion Policy变体 (DP-C, DP-T) 以及使用点云的DP3。EfficientFlow评估了NFE=1,3,5的情况。\n\n![结果对比表](https://arxiv.org/html/2512.02020v2/x3.png)\n> **表1**：与SOTA方法在12个MimicGen任务上的成功率对比（使用100、200、1000条演示数据）。EfficientFlow (Ours) 在仅使用1-5个NFE的情况下，在多数任务上取得了与需要100步NFE的EquiDiff相当或更优的成功率，尤其在数据量较少（100条）时优势明显。\n\n**关键结果**：\n1.  **性能竞争力**：在仅100条演示数据下，EfficientFlow (NFE=1) 在12个任务中的7个上优于或匹配EquiDiff (NFE=100)。随着数据量增加，其性能持续提升，在1000条数据下与最强基线相当。\n2.  **推理速度**：EfficientFlow的推理速度极快。如图1(b)所示，其NFE=1时比EquiDiff快19.9至56.1倍，为实时控制提供了可能。\n3.  **组件消融**：图4的消融实验验证了各组件贡献。\n    ![消融实验](https://arxiv.org/html/2512.02020v2/x4.png)\n    > **图4**：消融研究。(a) 在“Stack D1”任务上，移除等变性（w/o Equiv.）或加速度正则化（w/o FABO）均会导致性能下降，尤其在低NFE时。(b) 在“Coffee D2”任务上，FABO能有效降低生成轨迹的曲率，使其更接近直线，从而允许更少的NFE。\n\n## 总结与启发\n**核心贡献**：\n1.  **理论框架**：提出了EfficientFlow，一个将等变性融入流匹配的策略学习框架，并从理论上证明了使用各向同性先验和等变速率场可保证输出分布的等变性，显著提升了数据效率。\n2.  **高效采样**：提出了加速度正则化及其实用的代理损失FABO，有效平滑了生成轨迹，使策略在极低NFE（如1步）下仍能保持高性能，大幅提升了采样效率。\n3.  **全面验证**：在12个复杂操作任务上进行了系统评估，证明了该方法在有限数据下具有竞争力的性能，同时实现了数量级级别的推理加速。\n\n**局限性**：论文提到，其等变性目前主要针对SO(2)平面旋转对称性。此外，加速度正则化使用的是真实目标的上界（FABO）。\n\n**后续启示**：EfficientFlow展示了流匹配模型在高效策略学习中的巨大潜力。后续工作可探索将等变性扩展到更复杂的对称群（如SE(3)），或将此高效框架与更强大的视觉编码器（如VLA模型）结合，进一步推动具身智能的发展。",
      "imageUrls": [
        "https://arxiv.org/html/2512.02020v2/x1.png",
        "https://arxiv.org/html/2512.02020v2/x2.png",
        "https://arxiv.org/html/2512.02020v2/x3.png",
        "https://arxiv.org/html/2512.02020v2/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01946",
      "title": "Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models",
      "url": "http://arxiv.org/abs/2512.01946",
      "arxivId": "2512.01946",
      "date": "2025-12-02",
      "authors": "Cordelia Schmid Team",
      "category": "Manipulation",
      "summary": "本文针对机器人操作中缺乏全面失败数据、导致视觉语言模型（VLM）失败检测准确性受限的核心问题，提出自动失败合成方法：通过扰动成功轨迹，生成多样化的规划与执行失败案例，并构建三个新基准数据集（RLBench-Fail等）。基于此训练了多视图VLM模型Guardian，用于细粒度失败推理与检测。实验表明，Guardian在现有及新基准上均达到最优性能，集成后能有效提升仿真与真实机器人的任务成功率。",
      "detailedSummary": "## 研究背景与动机\n当前，由大型语言模型和视觉语言模型驱动的机器人操作系统在任务规划和动作执行方面取得了显著进展，但其可靠性仍受制于广泛的失败模式，包括错误的规划、感知混淆以及低级控制问题。提升鲁棒性需要能够利用视觉和语言进行推理并检测失败的模型。然而，该领域面临两大关键挑战：一是缺乏全面、多样且标注丰富的失败数据集，现有数据集要么规模小、手动收集，要么仅限于仿真且缺乏细粒度标注；二是现有失败检测方法在利用视觉上下文方面存在不足，例如依赖可能导致误差累积的多阶段文本转换流水线、使用易受遮挡影响的单视图观察，或将多视图图像压缩为单一网格而损失细粒度时空信息。\n\n本文针对机器人失败检测中数据稀缺和视觉推理能力有限的具体痛点，提出了一个自动生成失败数据并训练专用推理VLM的新视角。核心思路是：通过程序化扰动仿真和真实世界中的成功轨迹，自动生成多样化的规划与执行失败案例及细粒度标注，并以此训练一个能够处理高分辨率多视图图像、进行显式链式推理的VLM（Guardian），从而实现更鲁棒和可解释的失败检测。\n\n## 方法详解\n整体方法包含两个核心部分：1）自动失败数据生成流水线；2）基于此数据训练的Guardian模型。\n\n![失败数据生成流程](https://arxiv.org/html/2512.01946v2/x2.png)\n> **图2**：失败数据生成流水线。展示了在仿真（RLBench）中在线生成和真实世界数据集（BridgeDataV2）上离线生成失败案例的流程。给定正确的计划和成功轨迹，生成对应的错误计划和不成功轨迹。\n\n**失败数据生成**：该方法以成功演示（来自RLBench仿真和BridgeDataV2真实数据集）为基础，通过程序化扰动生成失败。任务被分解为子任务及其对应的视频片段。\n*   **规划失败生成**：针对成功计划，通过LLM（Mistral-Small-24B）微调或基于规则的扰动，构造五类失败：操作错误物体、目标状态/位置错误、顺序错误、缺失子任务、矛盾子任务。每个样本包含任务指令、计划、初始前视图图像。\n*   **执行失败生成**：\n    *   在仿真中，直接扰动子任务级动作，产生四类失败：夹爪未闭合、物体状态/位置错误、操作错误物体、抓取/推动不精确。\n    *   在真实数据中，由于无法直接控制机器人，通过LLM或规则改变子任务文本指令（造成语义不匹配），或替换结束图像为起始图像（表示无进展）来模拟失败。每个样本包含任务和子任务描述，以及动作前后的多视图图像。\n*   **链式推理生成**：利用大型VLM（InternVL3-38B），结合来自仿真或标注的对象、空间和机器人状态信息，为每个样本自动生成逐步推理轨迹，用于训练可解释模型。\n\n**Guardian模型**：将失败检测构建为视觉问答任务。模型基于InternVL3-8B架构，包含文本分词器、视觉编码器和基于Transformer的LLM。\n\n![Guardian模型架构与集成](https://arxiv.org/html/2512.01946v2/x5.png)\n> **图5**：左：Guardian模型架构概述。右：将Guardian模型集成到机器人操作流程中进行规划和执行验证。\n\n*   **架构细节**：与AHA等方法将多视图图像拼接为单张网格图不同，Guardian的视觉编码器独立处理每张图像，保留了每张图像内的细粒度空间细节，使模型能够显式推理空间和时间变化。模型输出不仅包含二元（成功/失败）判断和细粒度失败类别，还生成显式的推理链。\n*   **训练策略**：使用参数高效的LoRA进行微调，冻结视觉编码器。探索了三种结合推理链的训练策略：直接预测答案的“Vanilla”基线；始终生成推理链再预测答案的“Thinking”策略；训练时交替生成推理链+答案和直接答案，测试时仅输出答案的“Dropout”策略。\n*   **创新点**：1) **数据生成**：自动化、覆盖规划与执行、跨仿真与真实域、附带细粒度类别和推理链。2) **模型设计**：独立处理多视图图像以保留细节，并集成显式链式推理以提高可解释性和准确性。\n\n## 实验与结果\n**实验设置**：使用了四个基准进行评估：新构建的**RLBench-Fail**（仿真）、**BridgeDataV2-Fail**（真实）、**UR5-Fail**（真实策略驱动）以及现有的**RoboFail**（真实）。对比基线包括大规模通用VLMs（Qwen3-VL-235B, GPT-4.1）、小规模通用模型（InternVL3-8B）以及专用失败检测模型（Cosmos-Reason1-7B, AHA-13B, Sentinel, CLIP+MLP）。主要评估指标为二元检测准确率。\n\n**关键实验结果**：\n表II显示，在域内数据集上，经过微调的Guardian-8B-Thinking模型在规划和执行准确率上达到或超过了参数量大得多的通用VLMs（Qwen3-VL-235B, GPT-4.1）。在域外数据集（RoboFail, UR5-Fail）的零样本评估中，Guardian也表现优异，执行准确率分别达到0.86和0.77，规划准确率分别达到0.70和0.89，优于或匹配其他专用和通用基线。\n\n![训练数据规模对执行二元准确率的影响](https://arxiv.org/html/2512.01946v2/x6.png)\n> **图6**：训练数据规模对跨基准测试执行二元准确率的影响。表明在生成数据上微调存在扩展规律。\n\n**消融实验分析**：\n*   **训练数据混合**（表III）：混合使用仿真（RLBench-Fail）和真实（BridgeDataV2-Fail）数据进行微调，相比仅使用单一域数据，能显著提升模型在域内和域外的泛化性能。加入少量真实策略数据（UR5-Fail）可进一步提升在类似域上的表现。\n*   **训练数据规模**（图6）：随着用于微调的生成数据量增加，模型在域内和域外数据集上的性能呈现扩展规律，表明进一步扩大数据规模可能带来性能提升。\n*   **图像表示方法**（表IV）：将4个视图的图像分开处理（separated）比拼接成一张图（concat，即AHA方法）能带来显著的性能提升（RLBench-Fail执行准确率从0.72提升至0.81）。\n*   **多分类性能**（图7）：Guardian能够进行细粒度失败分类。混淆矩阵显示，对于执行失败，“夹爪未闭合”与“抓取不精确”容易混淆；对于规划失败，在长视野计划中，“错误物体状态”等类别存在较高混淆，反映了模型在视觉定位细微线索和时序常识推理方面的挑战。\n\n![规划和执行失败分类的混淆矩阵](https://arxiv.org/html/2512.01946v2/x7.png)\n> **图7**：Guardian对规划和执行失败分类的混淆矩阵。数值为百分比，行表示真实类别，列表示预测类别。\n\n**系统集成演示**：将Guardian作为验证模块集成到3D-LOTUS++机器人操作框架中（图5右）。图8展示了在线任务执行中，Guardian成功检测到初始生成的计划错误（操作错误物体）并触发重新规划，以及检测到子任务执行失败（抓取不精确）并触发重新执行，从而纠正了任务流程。\n\n![在线任务执行中通过Guardian验证进行纠正](https://arxiv.org/html/2512.01946v2/x8.png)\n> **图8**：在RLBench在线任务执行中通过Guardian验证进行纠正。左：对生成计划的成功纠正。右：对子任务执行的成功纠正。\n\n## 总结与启发\n**核心贡献**：1) 提出了一种自动化的机器人失败生成方法，能够程序化地扰动成功轨迹，生成覆盖规划与执行、兼具仿真与真实数据、并带有细粒度类别和推理链标注的大规模失败数据集（RLBench-Fail, BridgeDataV2-Fail, UR5-Fail）。2) 引入了Guardian模型，一个通过独立处理高分辨率多视图图像和显式链式推理进行细粒度失败检测的微调VLM，在多个基准上取得了最先进的性能。3) 展示了Guardian作为即插即用验证模块，能够有效提升现有机器人操作系统的任务成功率。\n\n**局限性**：论文提到，模型性能仍受限于对细微视觉线索（如夹爪是否完全闭合）的精准感知能力，以及对长视野任务中复杂时序和常识依赖的推理能力。此外，使用显式推理链（Thinking策略）会带来额外的计算开销。\n\n**研究启示**：1) **数据驱动的失败建模**：自动化、程序化的失败数据生成是弥补数据稀缺、提升模型鲁棒性的有效途径，且数据规模与性能存在正向关联。2) **多视图与细粒度推理**：独立处理多视图图像比压缩处理更能保留关键空间信息，显式推理链有助于提升可解释性和复杂失败模式的诊断能力。3) **模块化系统集成**：将专用的失败检测模块作为即插即用的“守护者”集成到现有机器人流程中，是一种提升系统整体可靠性的实用且有效的范式。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01946v2/x1.png",
        "https://arxiv.org/html/2512.01946v2/x2.png",
        "https://arxiv.org/html/2512.01946v2/x3.png",
        "https://arxiv.org/html/2512.01946v2/x4.png",
        "https://arxiv.org/html/2512.01946v2/x5.png",
        "https://arxiv.org/html/2512.01946v2/x6.png",
        "https://arxiv.org/html/2512.01946v2/x7.png",
        "https://arxiv.org/html/2512.01946v2/x8.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01908",
      "title": "SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception",
      "url": "http://arxiv.org/abs/2512.01908",
      "arxivId": "2512.01908",
      "date": "2025-12-01",
      "authors": "Dandan Zhang Team",
      "category": "Manipulation",
      "summary": "本文针对接触丰富的机器人操作任务，提出SARL空间感知自监督表征学习框架。核心问题是现有自监督方法将特征图压缩为全局向量，丢失了对操作至关重要的空间结构信息。SARL在BYOL架构基础上，引入三个作用于特征图的空间感知损失（SAL、PPDA、RAM），以保持跨视图的注意力焦点、部件构成和几何关系一致性。在融合视觉-触觉数据上的实验表明，SARL在六个下游任务上均优于九个基线方法；在边姿态回归任务中，其平均绝对误差为0.3955，较次优方法（0.5682）相对提升30%，接近有监督学习上限。",
      "detailedSummary": "## 研究背景与动机\n当前，接触密集的机器人操作任务需要能够编码局部几何信息的表征。视觉提供了全局上下文，但缺乏对纹理、硬度等属性的直接测量，而触觉则能提供这些关键线索。现代视觉触觉传感器（如ViTacTip）将两种模态捕获在单一的融合图像中，产生了内在对齐的输入，非常适合需要视觉和触觉信息的操作任务。然而，大多数自监督学习（SSL）框架（如SimCLR、BYOL）将特征图压缩成一个全局向量，丢弃了空间结构，这与操作任务对局部几何信息的需求不匹配。现有的多模态方法（如MViTac）仅在全局层面对齐模态，而空间感知的触觉方法（如Sparsh）仅处理单一模态。因此，现有SSL方法无法充分利用融合视觉触觉数据在空间上对齐的结构。\n\n本文针对上述“全局池化导致空间信息丢失”的具体痛点，提出了一种新的空间感知SSL框架。其核心思路是：在非对比的BYOL架构基础上，增加三个作用于中间特征图的空间感知损失（SAL, PPDA, RAM），以强制不同增广视图间在注意力焦点、部件构成和几何关系上保持一致，从而学习到既全局鲁棒又局部结构化的表征。\n\n## 方法详解\nSARL的整体框架基于BYOL的师生架构，包含在线网络（参数θ）和目标网络（参数ξ）。每个网络包含编码器f和投影头g，在线网络额外包含一个预测头q。目标网络的权重通过在线权重的指数移动平均（EMA）更新，并对目标网络的输出应用停止梯度（stop-gradient）以防止表征坍塌。SARL在BYOL的全局损失基础上，引入了三个作用于中间特征图Fθ(x)和Fξ(x)的空间感知损失。\n\n![SARL架构](https://arxiv.org/html/2512.01908v1/SARL_Architecture.png)\n> **图2**：SARL架构。输入x被增广为x1, x2，分别输入在线分支（θ）和目标分支（ξ）。全局损失L_global训练qθ以匹配gξ；目标权重是EMA更新并使用停止梯度。SARL从编码器特征fθ, fξ上增加空间损失：L_SAL（第2-4层）、L_PPDA（第3层，使用7x7网格和K=32个原型）和L_RAM（第3层，使用6x6网格）。\n\n**核心模块1：显著性对齐（SAL - “看哪里”）**\n该损失旨在确保模型在不同视图下持续关注相同的显著物体区域。具体做法是：计算特征图的通道L1范数作为显著图S(x)。对于两个增广视图x1和x2，通过仿射变换π建立坐标映射，并计算有效重叠区域M内的显著图之间的均方误差（MSE）。该损失应用于编码器的第2、3、4层，以强制分层注意力一致性。\n\n**核心模块2：块-原型分布对齐（PPDA - “有什么”）**\n该损失通过确保两个视图由相同的语义“部件”或“纹理”分布构成，来强制执行局部语义一致性。它引入一组K个可学习的原型向量{pk}，每个代表一种潜在的部件类型（如边缘、角点）。从特征图（第3层）提取的局部块特征向量vi被L2归一化后，通过softmax（温度τ控制）软分配到各个原型，得到每个块的分配概率qi(k)。整个图像的全局原型分布Q(x)是空间上所有块分配概率的平均。损失函数是最小化在线视图和目标视图原型分布之间的对称KL散度。\n\n**核心模块3：区域亲和力匹配（RAM - “如何排列”）**\n该损失通过确保不同特征区域之间的成对关系在不同视图间保持恒定，来保留物体的几何结构。具体地，将特征图（第3层）划分为一个网格区域（如6x6），提取每个区域的特征向量并L2归一化。计算任意两个区域向量之间的余弦距离，形成完整的区域亲和力矩阵A(x)。损失函数是最小化在线视图和目标视图亲和力矩阵之间的MSE，其中区域索引通过映射π进行对应。\n\n**总损失函数**\nSARL的总训练目标是全局损失与三个空间损失的加权和：L_SARL = L_global + λ_SAL * L_SAL + λ_PPDA * L_PPDA + λ_RAM * L_RAM。通过实验确定的权重为λ_SAL=0.10，λ_PPDA=0.05，λ_RAM=0.02。这三个辅助损失是互补的：SAL强制一致的注意力焦点（哪里），PPDA对齐语义部件的构成（什么），RAM保留它们之间的几何关系（如何排列）。\n\n## 实验与结果\n**数据集与评估**：主要使用ViTacTip融合视觉触觉数据集进行预训练和评估，该数据集涵盖六项下游任务：形状分类、光栅识别、多任务分类（材料、硬度、纹理）、力回归、边缘姿态回归和接触点检测。评估采用线性探针（冻结预训练编码器，训练单个线性层）和迁移学习（在四个未见过的触觉数据集上微调）两种协议。对比了九种先进的SSL基线方法（SimCLR, BYOL, SimSiam, Barlow Twins, VICReg, SwAV, MINC, CPLearn, DenseCL）以及一个有监督的ResNet-18上限。\n\n![数据集概览](https://arxiv.org/html/2512.01908v1/dataset_overview.png)\n> **图3**：用于预训练、评估和迁移学习的数据集概览。(a) 主要ViTacTip数据集的样本图像，展示了多个下游任务中的融合视觉触觉数据。(b) 消融研究中使用的三种数据模态：融合多模态ViTacTip数据、仅视觉的ViTac数据和仅标记的TacTip数据。(c) 用于泛化性评估的四个未见数据集的样本图像。\n\n**关键实验结果**：如**表I**所示，SARL在所有六项下游任务中一致优于所有九种SSL基线方法。特别是在对几何敏感的**边缘姿态回归任务**上，SARL取得了0.3955的平均绝对误差（MAE），相对于次优的SSL方法（SimSiam，0.5682 MAE）有**约30%的相对提升**，并且接近有监督上限（0.3547 MAE）。在形状分类和光栅识别任务上，SARL的Top-1准确率分别达到92.51%和99.61%，显著领先于其他SSL方法。在迁移学习实验中，SARL预训练的表征在四个未见数据集上也展现出强大的泛化能力。\n\n![边缘姿态回归定性比较](https://arxiv.org/html/2512.01908v1/edgepose_comparison_sarl_supervised.png)\n> **图4**：边缘姿态回归的定性比较。SARL预测的边缘位置（绿色虚线）与真实值（红色实线）高度吻合，其精度接近有监督模型（蓝色虚线），而基线SSL方法（如SimCLR，紫色虚线）的预测则存在明显偏差。这直观证明了空间感知损失对于学习精确几何表征的有效性。\n\n**消融实验总结**：\n1.  **空间损失贡献**：消融实验证实了三个空间损失（SAL, PPDA, RAM）的互补性。移除任一个都会导致性能下降，同时使用三者效果最佳。\n2.  **多模态数据优势**：在仅视觉（ViTac）或仅触觉（TacTip）数据上预训练的模型，其性能均显著低于在融合视觉触觉（ViTacTip）数据上预训练的SARL，证明了利用硬件融合的多模态信号的价值。\n3.  **目标网络的作用**：实验验证了对目标网络特征图应用停止梯度是必要的，可以防止空间损失导致训练不稳定。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了SARL，首个为硬件融合的视觉触觉数据设计的空间感知SSL框架，通过SAL、PPDA、RAM三个地图级损失，在中间特征层显式地保持跨视图的空间和语义一致性。\n2.  在六项下游任务上系统性地验证了SARL相对于九种SOTA SSL方法的优越性，特别是在几何敏感任务上取得了约30%的性能提升。\n3.  通过消融实验和迁移学习，证实了空间损失的互补性、多模态融合数据的必要性以及所学表征的强泛化能力。\n\n**局限性**：论文提到，SARL目前专注于单传感器、硬件融合的视觉触觉数据流。对于使用独立视觉和触觉传感器的系统，其空间对齐假设可能不直接适用。\n\n**启示**：本研究表明，对于融合的视觉触觉数据，最有效的学习信号是**结构化的空间等变性**，即特征随物体几何形状可预测地变化。这为机器人感知指明了一个方向：在设计自监督目标时，应超越简单的全局不变性，转而寻求保留对具体任务（如灵巧操作）至关重要的局部结构和几何关系。SARL的框架和空间损失思想有望启发其他需要密集、结构化预测的多模态或单模态表征学习任务。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01908v1/overview_diagram.png",
        "https://arxiv.org/html/2512.01908v1/SARL_Architecture.png",
        "https://arxiv.org/html/2512.01908v1/dataset_overview.png",
        "https://arxiv.org/html/2512.01908v1/edgepose_comparison_sarl_supervised.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01801",
      "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
      "url": "http://arxiv.org/abs/2512.01801",
      "arxivId": "2512.01801",
      "date": "2025-12-02",
      "authors": "Yonghui Wu Team",
      "category": "Manipulation",
      "summary": "GR-RL旨在解决现有视觉-语言-动作策略在长视野、高精度灵巧操纵任务中因依赖次优人类演示而性能不足的问题。其核心技术包括：基于离线强化学习的进度过滤、形态对称性增强和在线强化学习的噪声预测，以优化演示并提升策略精度。实验表明，GR-RL在自主穿鞋带任务中达到83.3%的成功率，实现了毫米级控制和长视野推理。",
      "detailedSummary": "## 研究背景与动机\n当前，大规模视觉-语言-动作模型在构建通用机器人智能体方面取得了显著进展，展现出跨物体、环境和语义概念的泛化能力。然而，通用性不等同于可靠性，现有VLA策略在现实部署中面临两大根本挑战：1）**灵巧性与精度**：对可变形物体进行毫米级控制仍未解决；2）**长时程鲁棒性**：误差会随时间步累积，在高精度灵巧操作中尤为严重。以穿鞋带任务为例，它同时要求处理可变形物体、毫米级精度和长时程操作能力。\n\n现有方法的核心假设是**人类演示的最优性**。但在高度灵巧和精密的操作任务中，人类演示者会因减速、犹豫而引入噪声和次优的演示数据。此外，标准的离线训练（如通过滑动窗口预测固定长度的动作块）与为获得平滑控制而采用的推理时优化（如时序集成、异步滚动时域控制）之间存在**训练与推理的不匹配**，这进一步放大了次优演示的负面影响。\n\n本文针对从通用VLA策略到可靠、高精度专用策略的转化难题，提出了一种强化学习增强的多阶段训练流程。其核心思路是：首先通过离线RL学习一个任务进度评估器来过滤次优演示数据；然后利用形态对称性进行数据增强；最后通过在线RL在潜在空间进行探索，以对齐策略的部署行为，从而将一个通用VLA策略转变为擅长长时程、灵巧、精密操作的专家。\n\n## 方法详解\nGR-RL采用一个多阶段的强化学习增强训练流程，对次优且不匹配的人类演示进行过滤、增强和强化。整体流程包含三个阶段：1）基于学习到的任务进度进行离线过滤的行为克隆；2）简单有效的动作增强；3）在线强化学习。\n\n![方法框架](https://arxiv.org/html/2512.01801v3/x1.png)\n> **图1**：GR-RL多阶段训练流程。它通过离线RL学习任务进度以过滤数据，进行形态对称性数据增强，并执行在线RL以对齐部署行为。\n\n**模型架构**：GR-RL采用混合Transformer架构，包含一个视觉-语言-动作策略模型 π_θ 和一个多任务评论家 Q_ϕ，总参数量为50亿。策略 π_θ 以Qwen2.5-VL-3B-Instruct作为VLM主干，并使用通过流匹配目标训练的动作扩散Transformer来预测k长度的动作块。评论家 Q_ϕ 是一个因果Transformer，采用分布强化学习，将值预测为具有上下界的离散分布，在稀疏奖励设置下比非分布评论家更鲁棒。\n\n**核心模块与技术细节**：\n1.  **基于学习进度评估器的数据过滤**：为解决人类演示的次优性问题，GR-RL不直接对所有数据进行行为克隆，而是先训练一个任务进度模型来过滤数据。具体使用TD3+BC算法训练评论家 Q_ϕ，奖励为稀疏的二元奖励（仅在任务成功时给予折扣奖励）。通过时序差分学习成功和失败轨迹（利用重试关键帧进行 hindsight 增强），Q_ϕ 的预测值（取其分类分布均值）能稳健反映任务进度。如图3所示，当操作者失误时，预测进度会出现骤降。GR-RL定义，若一个动作块对应的进度序列中出现超过阈值 δ 的下降，则该样本为次优，并将其从策略训练数据集中剔除。\n\n![学习到的任务进度](https://arxiv.org/html/2512.01801v3/x3.png)\n> **图3**：学习到的任务进度示例。在操作者失误时（红色箭头），GR-RL学习到的进度值会明显下降，而基于回归的基线预测则过度平滑，对细微失败不敏感。\n\n2.  **带有数据增强的模仿学习**：在离线训练阶段，GR-RL引入了一种**形态对称性增强**范式。对于双臂任务设置，通过水平翻转图像并交换左右腕部图像，镜像本体感知状态和动作（在世界坐标系中进行对称变换后转回局部腕部坐标系），并相应地翻转语言指令中的空间描述（如将“左边的孔”改为“右边的孔”）。这种简单的方法有效提升了策略的性能和泛化能力。\n\n3.  **用于策略部署对齐的在线引导**：为弥补训练与推理的不匹配，GR-RL进行在线RL，使模型能在闭环交互中探索和改进。鉴于任务对毫米级精度的要求，在关节位姿上添加噪声难以成功。因此，GR-RL在**潜在空间进行结构化探索**，引导训练好的流策略。具体而言，在共享VLM主干后添加一个轻量级噪声预测器 π_θ‘（参数量51.5M），用于为动作DiT预测初始噪声 ε_t。为避免生成偏离离线分布的动作，对噪声预测器的输出施加偏离正态分布超过阈值 β 的惩罚。同时，在噪声空间蒸馏一个 Q_ϕ‘ 函数以避免在策略优化时反向传播通过流模型。在线训练时，维护离线和在线策略回放缓冲区，并均匀采样，初始时使用离线策略检查点的在线 rollout 预热离线缓冲区。\n\n## 实验与结果\n**实验设置**：主要评估任务是**穿鞋带**，这是一个需要长时程、灵巧和精密操作的挑战性场景。观测包括三个视角的RGB图像、本体感知状态和语言指令。奖励设置为二元稀疏奖励（仅当鞋带正确穿过鞋眼并完全放在桌上时获得奖励1）。实验平台为ByteMini-v2轮式移动操作机器人。\n\n**基线方法**：主要对比基线是基础的GR-3模型（在所有人类遥操作数据上进行行为克隆）。\n\n**关键结果**：\n1.  **多阶段训练效果**：如图5左所示，基础GR-3的成功率为45.7%。经过基于进度的数据过滤后，成功率提升至61.6%。在此基础上增加对称性数据增强，成功率进一步提升至72.7%。最后，经过在线RL微调，最终模型在500步评估时取得了**83.3%** 的整体成功率。图5右展示了在线微调过程中成功率的移动平均曲线，初期因分布偏移有所下降，随后迅速恢复并超越离线性能。\n\n![多阶段训练成功率](https://arxiv.org/html/2512.01801v3/x5.png)\n> **图5**：左：多阶段训练配方的成功率。数据过滤、镜像增强和在线调优均对最终性能有贡献。右：在线微调期间每回合的二元成功信号（点）和成功率的移动平均（曲线）。\n\n2.  **分阶段失败分析**：图6详细展示了不同模型在关键子阶段（拾取正确鞋带、穿入正确鞋眼、交接给另一夹爪、拉紧鞋带）的成功率。彩色区域表示完成各阶段的成功率，阴影区域表示相较于前一阶段的成功率下降。数据过滤和在线RL能大幅减少穿眼阶段的失败。数据增强则全面提升了各阶段的性能。\n\n![分阶段成功率](https://arxiv.org/html/2512.01801v3/x6.png)\n> **图6**：不同模型完成中间阶段的详细成功率。阴影区域的高度表示从前一阶段到当前阶段成功率的下降。“过滤BC+增强”模型是在线引导RL的起点。\n\n3.  **进度评估器的消融实验**：论文比较了基于RL的分布评论家与基于回归的基线以及非分布评论家。如图3和图7所示，回归基线预测过于平滑，对细微失败（如差几毫米未穿入）和具有长期影响的动作不敏感。非分布评论家在稀疏奖励和长时程设置下，轨迹早期部分存在严重的价值高估，而分布评论家因输出有界，能更鲁棒地收敛到合理尺度，并与真实时序更好对齐。\n\n![分布与非分布评论家对比](https://arxiv.org/html/2512.01801v3/x7.png)\n> **图7**：分布与非分布评论家的进度预测对比。非分布评论家输出无界，无法在成功轨迹中反映正向进度。\n\n4.  **定性结果**：如图8所示，GR-RL展现出多种鲁棒行为：处理不同颜色和尺寸的鞋子；在鞋带意外掉落或未准确穿入时自动重试；主动调整场景以简化任务（如调整抓握点、重新摆正鞋子位置、从交叉的鞋带中识别并拉出正确的一端等）。\n\n![鲁棒行为案例](https://arxiv.org/html/2512.01801v3/images/case_studies/case_gray.png)\n> **图8**：GR-RL在各种情况下的鲁棒行为展示，包括处理不同颜色鞋子、重试、调整抓握和物体位置等。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了**GR-RL框架**，一个多阶段的RL增强训练流程，成功将通用VLA策略转化为能完成长时程、灵巧、高精度操作（如穿鞋带）的专家策略，实现了83.3%的成功率。\n2.  引入了基于**离线RL学习任务进度评估器**的方法，用于过滤人类演示中的次优片段，显著提升了离线策略的基础性能。\n3.  提出了一种简单有效的**形态对称性数据增强**方法，并设计了一种在**潜在空间进行在线探索**的机制，以对齐策略部署，有效缓解了训练与推理的不匹配问题。\n\n**局限性**：论文提到当前流程存在**行为漂移**问题。在稀疏且噪声的奖励下，策略在在线RL期间的行为可能不稳定。这可能是由于轻量级噪声预测器的容量有限，或在大规模潜在动作空间中信用分配困难所致。\n\n**后续启示**：GR-RL展示了通过结合数据过滤、增强和在线RL，从通用基础模型迭代出高性能专用模型的可行性。其关于处理次优演示、训练-推理不匹配以及使用分布评论家提升鲁棒性的见解，对推动可部署机器人研究具有参考价值。未来的工作可以探索如何将改进后的策略蒸馏回基础VLA模型，以获得既能力强又通用的操作策略，并致力于解决在线RL中的行为不稳定问题。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01801v3/x1.png",
        "https://arxiv.org/html/2512.01801v3/x2.png",
        "https://arxiv.org/html/2512.01801v3/x3.png",
        "https://arxiv.org/html/2512.01801v3/x4.png",
        "https://arxiv.org/html/2512.01801v3/x5.png",
        "https://arxiv.org/html/2512.01801v3/x6.png",
        "https://arxiv.org/html/2512.01801v3/x7.png",
        "https://arxiv.org/html/2512.01801v3/images/case_studies/case_gray.png",
        "https://arxiv.org/html/2512.01801v3/images/case_studies/case_regrasp.png",
        "https://arxiv.org/html/2512.01801v3/images/case_studies/case_thread_reattempt.png",
        "https://arxiv.org/html/2512.01801v3/images/case_studies/case_adjust_grasp.png",
        "https://arxiv.org/html/2512.01801v3/images/case_studies/case_shoe_reorient.png",
        "https://arxiv.org/html/2512.01801v3/images/case_studies/case_adjust_shoe_pos.png",
        "https://arxiv.org/html/2512.01801v3/images/case_studies/case_pull_shoelace.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01598",
      "title": "A Cross-Embodiment Gripper Benchmark for Rigid-Object Manipulation in Aerial and Industrial Robotics",
      "url": "http://arxiv.org/abs/2512.01598",
      "arxivId": "2512.01598",
      "date": "2025-12-01",
      "authors": "Ivan Virgala Team",
      "category": "Manipulation",
      "summary": "本文针对现有抓取器基准（如YCB）无法评估跨平台复用性与能耗的问题，提出了跨平台抓取器基准CEGB。该基准在传统指标基础上，新增了**转移时间、能耗和理想负载评估**三个核心组件，以量化抓取器在不同机器人平台（如协作臂、无人机）间的适配性。基于一个轻型自锁抓取器原型的实验表明，该基准能有效评估性能：**跨平台转移中位时间约17.6秒，保持能耗约1.5焦耳/10秒，抓取成功率超90%且周期时间为3.2–3.9秒**，为空中与工业机器人的抓取器提供了可复现的跨平台、能耗感知评估基础。",
      "detailedSummary": "## 研究背景与动机\n当前机器人抓取领域的主流评估方法是YCB物体与模型集以及NIST协议。YCB主要评估抓取成功率，而NIST补充了时序、负载和基于力的指标。然而，这些方法存在关键局限性：它们通常是平台特定的，未能评估单个夹爪在异构机器人系统（如工业机械臂、移动机器人和无人机）间的可转移性，也缺乏对能源消耗的标准化评估。随着软体、混合和模块化夹爪的出现，以及无人机等资源受限平台对能耗敏感性的增加，这些局限性变得尤为突出。\n\n本文针对现有基准无法量化跨平台部署的实践开销和能源效率这一具体痛点，提出了一个名为跨平台夹爪基准测试（CEGB）的新视角。该基准并非替代YCB和NIST，而是对其进行扩展，增加了三个互补组件。其核心思路是建立一个紧凑、可复现的基准测试套件，通过引入转移时间、能耗和意图特定负载等新指标，为夹爪在异构平台间的性能评估和比较提供统一、量化的方法论。\n\n## 方法详解\nCEGB的整体框架建立在广泛采用的YCB物体集和精选的NIST协议之上，并集成了三个新设计的评估组件，旨在全面量化夹爪的跨平台性能和能耗特性。\n\n![方法框架](https://arxiv.org/html/2512.01598v1/Figures/abstract.jpg)\n\n> **图1**：跨平台夹爪基准测试（CEGB）概述。一个通用的参考夹爪被安装在不同的机器人平台上——协作机械臂、带机械臂的移动机器人和无人机——以评估跨平台转移、安装兼容性以及在不同驱动和环境条件下的性能。\n\n核心模块包括：\n1.  **YCB抓取成功率与指标**：沿用YCB标准流程，评估抓取-保持-释放的成功率。使用微平均（`s_micro`）和宏平均（`s_macro`）成功率进行数据集级汇总，并记录抬升时间和释放时间作为补充时序指标。所有二项比例结果均报告威尔逊分数置信区间。\n2.  **NIST基准**：采用NIST末端执行器指标的一个紧凑子集，包括抓取循环时间（`T_cycle`）、抓取强度（`F_total`）和滑移阻力（`μ_eff`, `Q_hold`）。这些提供了与平台无关的、基于时间和力的描述符。\n3.  **转移时间基准**：这是一个全新的指标，用于量化将夹爪从一个机器人平台拆卸并重新安装到另一个平台所需的持续时间（`T_transfer`）和成功率（`S_transfer`）。它捕获了连接器可达性、对齐公差、机械键合和人工操作等实际因素，直接关联硬件设计选择与可部署性。\n4.  **能耗基准**：以驱动无关的方式定义，评估一个完整抓取循环中各阶段的能耗。总循环能耗 `E_cycle = E_grasp + E_hold + E_release`。特别地，报告标准化的10秒等效保持能耗 `E_hold10`，便于比较。通过积分瞬时输入功率 `P(t)` 计算能量。\n5.  **意图特定理想负载基准**：评估夹爪在其预期操作配置下可承受的最大可持续负载（`F_ideal`）。结合夹爪分类代码（如依从性类型：刚性R、单轴1S等；偏好抓取类型：包裹W、捏取P；理想物体形状：圆柱C、方盒B、球体S）进行报告，从而在机械能力与具体用例之间建立联系。\n\n与现有方法相比，CEGB的创新点具体体现在：首次系统性地提出了量化跨平台转移实践开销（转移时间）和能源效率（分阶段能耗）的标准化指标，并将意图特定的设计考量（IIPB）纳入统一报告框架，弥补了现有基准在评估现代移动和空中操纵夹爪关键能力方面的空白。\n\n## 实验与结果\n实验使用了YCB物体集的子集和NIST测试协议。平台包括协作机械臂和无人机。研究未与其他夹爪设计进行直接对比，而是以实现的一个轻量化自锁参考夹爪原型作为案例，在CEGB框架下进行全面评估，以验证基准的可行性和提供参考数据。\n\n**YCB评估结果**：参考夹爪原型的微平均成功率为39.5% (95% CI [37.35, 41.55])，宏平均成功率为38.7% (95% CI [22.43, 57.47])。球形和圆柱形物体成功率超过63%，而薄而平的物体成功率低于1.7%。这符合其机械结构特点。\n\n![性能指标](https://arxiv.org/html/2512.01598v1/Figures/figure_1.jpeg)\n\n> **图2**：基于YCB基准的参考夹爪原型性能指标结果。展示了不同物体类别的抓取成功案例。\n\n**NIST评估结果**：\n- **抓取循环时间**：对于50mm和80mm标准圆柱体，循环时间中位数分别为3.91秒和3.23秒，变异性低。\n- **抓取强度**：对50mm和80mm圆柱体产生的总法向力中位数分别为9.79 N和8.18 N。\n- **滑移阻力**：对32mm至100mm的圆柱体进行测试，有效摩擦系数 `μ_eff` 在3.75 N到6.28 N之间，其中100mm直径的阻力明显下降。\n\n![抓取循环时间](https://arxiv.org/html/2512.01598v1/Figures/figure_6.jpg)\n\n> **图3**：遵循NIST程序测定参考夹爪原型的抓取循环时间。展示了从开始到结束的完整序列。\n\n![滑移阻力测试](https://arxiv.org/html/2512.01598v1/Figures/figure_7.jpg)\n\n> **图4**：基于NIST方法的滑移阻力测试实验设置。展示了夹爪抓持圆柱体并承受外部切向拉力的场景。\n\n**转移时间评估结果**：在45名参与者（分为新手、中级用户和训练有素的操作员）中进行的实验表明，中位转移时间约为17.6秒，且不同经验水平的用户组间无显著差异，证明了快速转移的可行性。转移成功率高，表明接口设计可靠。\n\n![转移序列](https://arxiv.org/html/2512.01598v1/Figures/figure_3.jpg)\n\n> **图5**：参与者拆卸和安装参考夹爪原型的实验序列。\n\n![平台部署](https://arxiv.org/html/2512.01598v1/Figures/figure_5_1.jpg)\n\n> **图6**：向平台的转移能力及包裹递送任务。展示了夹爪成功转移到工业机械臂和无人机后，立即执行抓取-放置任务。\n\n![转移时间箱线图](https://arxiv.org/html/2512.01598v1/Figures/transfer_time_boxplot.jpg)\n\n> **图7**：不同用户组的转移时间箱线图。中位时间约为17.6秒，且组间差异不显著。\n\n**能耗评估结果**：参考夹爪原型表现出极低的保持能耗，`E_hold10` 中位数约为1.5 J（每10秒）。抓取和释放阶段的能耗也很低。瞬时功率曲线显示，大部分能量消耗发生在动作阶段，而在保持阶段功率几乎为零，这得益于其自锁机制。\n\n![能耗曲线](https://arxiv.org/html/2512.01598v1/Figures/power.png)\n\n> **图8**：参考夹爪原型在一个完整抓取循环中的典型瞬时功率和累积能量曲线。可见保持阶段功率消耗极低。\n\n**消融实验/组件贡献**：论文通过分项实验展示了每个新基准组件所提供的独特洞察：YCB揭示了物体几何适应性，NIST提供了基本机械能力，转移时间量化了部署便利性，能耗突出了能效特性，IIPB关联了设计与预期负载。这些组件共同构成了一个更全面的评估图景。\n\n## 总结与启发\n本文的核心贡献有三点：\n1.  提出了**跨平台夹爪基准测试（CEGB）**，这是一个可复现的基准测试套件，通过引入转移时间、能耗和意图特定负载评估，系统性地扩展了现有的YCB和NIST标准，首次为量化夹爪的跨平台可转移性和能源感知性能提供了统一方法。\n2.  设计并验证了一个**低能耗自锁参考夹爪原型**，其保持能耗极低（约1.5 J/10s），转移快速（中位时间≈17.6秒），并展示了可靠的抓取性能，为CEGB提供了具体的实施案例和性能参考。\n3.  提供了**开源数据、CAD模型和脚本**，以支持社区内跨平台的、可复现的夹爪比较。\n\n论文自身提到的局限性主要在于参考夹爪原型在抓取薄而平的物体时成功率较低，这受限于其固定的指甲几何形状和缺乏通用拾取运动。\n\n本文对后续研究的启示在于：它鼓励夹爪设计者从一开始就考虑跨平台兼容性、快速部署和能源效率，并为比较不同设计提供了一个共同的量化语言和报告标准。CEBG框架有望推动模块化、可转移夹爪的标准化评估，促进该领域更透明、可比较的研究进展。\n\n![IIPB测试](https://arxiv.org/html/2512.01598v1/Figures/figure_8.jpg)\n\n> **图9**：意图特定理想负载基准测试设置。展示了夹爪抓持不同标准化测试工件（圆柱、方盒、球体）以评估其最大可持续负载。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01598v1/Figures/abstract.jpg",
        "https://arxiv.org/html/2512.01598v1/Figures/figure_1.jpeg",
        "https://arxiv.org/html/2512.01598v1/Figures/figure_6.jpg",
        "https://arxiv.org/html/2512.01598v1/Figures/figure_7.jpg",
        "https://arxiv.org/html/2512.01598v1/Figures/figure_3.jpg",
        "https://arxiv.org/html/2512.01598v1/Figures/figure_5_1.jpg",
        "https://arxiv.org/html/2512.01598v1/Figures/transfer_time_boxplot.jpg",
        "https://arxiv.org/html/2512.01598v1/Figures/power.png",
        "https://arxiv.org/html/2512.01598v1/Figures/figure_8.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01773",
      "title": "IGen: Scalable Data Generation for Robot Learning from Open-World Images",
      "url": "http://arxiv.org/abs/2512.01773",
      "arxivId": "2512.01773",
      "date": "2025-12-01",
      "authors": "Zhi Wang Team",
      "category": "Manipulation",
      "summary": "本文针对开放世界图像缺乏机器人动作数据、难以直接用于策略训练的问题，提出IGen数据生成框架。该框架首先将2D图像转换为结构化3D场景表示，进而利用视觉语言模型进行任务推理，生成SE(3)末端执行器姿态序列作为动作，并合成动态、时序一致的视觉观察。实验验证，IGen能生成高质量的视觉-动作数据，仅使用其合成数据训练的策略，在真实场景中取得了与使用真实数据训练的策略相当的性能。",
      "detailedSummary": "## 研究背景与动机\n通用机器人策略的兴起对大规模训练数据产生了指数级需求。然而，在真实机器人上收集数据劳动密集，且通常局限于特定环境。相比之下，开放世界图像能以极低成本获取，并包含了与真实机器人操作任务自然对齐的、极其多样化的场景，为低成本、大规模机器人数据获取提供了有前景的途径。尽管潜力巨大，但开放世界图像缺乏关联的机器人动作，阻碍了其在实际机器人学习中的应用，使得这一丰富的视觉资源未被充分开发。现有方法存在关键局限：基于“真实-仿真-真实”的方法需要显式的物理场景重建，无法利用任意的开放世界图像，可扩展性受限；而基于视频生成模型的方法则无法提供明确的机器人动作，且难以处理长视野或复杂指令驱动的任务。本文旨在弥合这一鸿沟，提出IGen框架，其核心思路是：将非结构化的2D开放世界图像转换为结构化的3D场景表示，利用视觉语言模型的推理能力生成高层任务规划和低层SE(3)动作轨迹，并基于此合成动态场景演化，最终渲染出时间一致的视觉观测数据。\n\n## 方法详解\nIGen是一个从开放世界图像合成任务特定机器人动作和视觉观测的数据生成框架。其架构包含三个主要阶段：1) 场景重建，将输入图像转换为机器人可操作的工作空间；2) 动作规划，在空间关键点上进行推理以生成动作轨迹；3) 观测合成，合成并渲染点云序列以生成任务的视觉观测。\n\n![方法整体框架](https://arxiv.org/html/2512.01773v1/x2.png)\n> **图2**：IGen概览。给定一张开放世界图像和任务描述，IGen首先通过基础视觉模型将环境和物体重建为点云。提取空间关键点后，视觉语言模型将任务描述映射为高层计划和低层控制指令。机器人在仿真中执行时，虚拟深度相机捕捉运动点云序列。最终得到的末端执行器位姿轨迹用于合成动态点云序列，并逐帧渲染为操作任务的视觉观测。最终输出包含生成的机器人动作和视觉观测。\n\n**1. 场景重建：从像素到结构化3D表示**\n核心是将原始像素转换为机器人能够有效解释和执行动作的结构化表示。IGen采用3D点云作为中间模态。具体流程如下：\n- **深度与几何估计**：使用单目几何基础模型（如Metric3D）估计场景深度和几何结构。\n- **物体分割**：使用Segment Anything Model获取任务相关物体的掩码 ℳ。\n- **关键点提取**：遵循[huang2024rekep]中的关键点表示，使用DINOv2提取场景特征，并对特征嵌入和3D坐标进行K-means聚类，得到一组K个空间关键点 𝒦 = {k_j ∈ ℝ³} 及其关联的3D坐标。\n- **背景重建与点云生成**：使用图像修复模型从原始图像中移除目标可操作物体并重建背景图像 𝐈_bg。结合估计的深度图 𝐃 和相机内参矩阵 𝐊，将所有像素提升到3D空间，形成密集的3D背景点云 P_bg。\n- **物体完整3D重建**：为解决单目视图下物体建模不完整的问题，使用3D生成式重建模型对目标物体的完整3D形状和外观进行重建，形成密集物体点云，并通过6D姿态估计将其重新定位到原始姿态。\n- **空间域随机化**：从支撑表面（如桌面）提取一组可行的放置点，作为物体泛化的候选空间位姿。\n\n**2. 动作规划：空间行为生成**\n由于开放世界图像缺乏动作指导，IGen利用视觉语言模型强大的视觉推理和规划能力。\n- **高层任务分解**：将标注了关键点 𝒦、其3D坐标和任务指令的图像输入VLM。VLM将整体任务分解为一组子阶段 𝒮 = {S_i}。\n- **低层控制函数生成**：为每个子阶段，VLM生成与关键点关联的动作描述。IGen开发了一种基于末端执行器SE(3)位姿的、易于编程的Python控制语言，将高层任务阶段 𝒮 转化为可执行的低层控制函数 ℱ = {f_i}。\n- **关键点驱动求解**：每个函数 f_i 被定义为一个以关键点为条件的求解器，从空间锚点 𝒦_i 计算末端执行器的参考位姿。在预操作阶段，末端执行器基于抓取先验模型与物体精确交互（抓取位姿由抓取模型预测，夹爪状态由沿夹爪主轴的物体点云宽度决定）。在操作过程中，被操作物体上的关键点被视为刚性附着在末端执行器上的可移动点，而其他关键点则作为静态场景锚点。\n- **轨迹生成与仿真执行**：从初始机器人状态 x₀ 开始，运动规划器产生一组子目标 𝒳 = {x̂_i}。每个阶段计算下一个状态为 x̂_i = f_i(x̂_{i-1})。使用运动规划器生成可行轨迹，并在仿真环境中执行，产生在T个时间步上的动作序列 𝒜 = {a_t}，每个动作 a_t 包含机器人末端执行器位姿和关节位置。\n\n**3. 观测合成：机器人学习经验合成**\n为获得与具身动作同步的视觉观测，IGen提出了一个基于实时点云渲染的机器人经验合成框架。\n- **动作点云合成**：将仿真机器人置于规划位姿 p_robot，虚拟相机置于 p_cam（与生成场景点云的视点对齐）。从动作序列 𝒜 中获得末端执行器位姿轨迹 𝒯 = {𝐓_t}。在每个时间步 t，渲染环境产生同步的RGB和深度帧，并通过虚拟相机 𝐂 反投影，构建机器人运动的点云序列 𝒫_robot。\n- **动态交互合成**：背景被建模为静态点云序列 𝒫_bg。基于末端执行器位姿的变换，合成末端执行器与点云之间的动态交互。假设在时间 t_g 建立抓取，物体位姿为 𝐓_obj,t_g，末端执行器位姿为 𝐓_t_g。对于所有夹爪闭合在物体上的时间步 t ∈ 𝒯_grasp，物体的世界位姿通过刚性地跟随末端执行器而演变。被操作物体的点云序列 𝒫_obj 经历由末端执行器位姿引起的刚体变换（公式1）。\n- **完整观测合成与渲染**：组合静态环境、机器人和被操作物体，将完整任务表示为复合点云序列：𝒫_task = 𝒫_bg ∪ 𝒫_obj ∪ 𝒫_robot。通过虚拟相机 𝐂 渲染该点云序列，得到视觉观测 𝒪。时间同步的观测 𝒪 与动作序列 𝒜 共同构成了用于机器人学习的配对视觉-动作数据。\n\n**创新点**：与现有方法相比，IGen的创新在于：1) 将任意开放世界图像转化为可操作的3D场景表示，无需精确的物理属性估计或特定环境的多视角重建；2) 利用VLM在3D关键点空间进行任务规划和低层控制代码生成，实现了指令驱动的、物理合理的动作生成；3) 提出了一种免仿真的点云合成方法，基于SE(3)轨迹直接合成动态视觉观测，支持大规模机器人经验生成，计算效率高。\n\n## 实验与结果\n实验旨在回答三个问题：1) IGen能否从各种场景的真实世界图像生成视觉上逼真的数据？2) IGen能否高效生成与环境对齐且遵循任务指令的机器人动作？3) IGen能否仅从单张图像合成有效的机器人训练数据，使得策略训练和真实世界部署无需任何人工遥操作演示？\n\n**实验设置**：\n- **数据集/基准**：Simpler数据集（用于视觉保真度评估）、DreamGen Bench（用于行为生成质量评估）。\n- **对比基线**：视觉保真度对比：Real-to-Sim方法（来自Simpler）。行为生成质量对比：Cosmos-Predict2、TesserAct。策略训练对比：零样本、使用10/100条人类遥操作数据微调、使用1000条IGen合成数据微调。\n\n**关键实验结果**：\n\n**1. 场景重建保真度**：\n在Simpler数据集上，比较IGen与Real-to-Sim方法重建场景与原始真实场景的视觉一致性。\n\n![视觉保真度定量结果表](https://arxiv.org/html/2512.01773v1/x1.png)\n> **表1**：IGen与Real-to-Sim方法在视觉相似性指标上的对比。IGen在PSNR、SSIM、FID及多个LPIPS变体上均显著优于Real-to-Sim基线，例如在LPIPS相似性上取得了高达5.13倍的提升，表明IGen生成的场景与真实图像高度一致。\n\n**2. 行为生成评估**：\n- **定性实验**：\n\n![行为生成定性对比](https://arxiv.org/html/2512.01773v1/x3.png)\n> **图3**：给定单张图像和自然语言操作指令，IGen与基线方法生成的行为观测对比。IGen产生了更符合指令、物理一致性的物体运动，而基线方法存在时间不连续、几何扭曲和动作执行不准确等问题。\n\n- **定量实验**：\n\n![行为生成定量对比](https://arxiv.org/html/2512.01773v1/x4.png)\n> **图4**：在DreamGen Bench上，使用GPT-4o、Qwen-3-VL-Plus和GLM-4.5V作为评估模型，IGen在“指令跟随”和“物理对齐”两个指标上均优于所有对比模型。例如，在Qwen评估下，IGen的指令跟随成功次数接近基线的两倍；在GLM评估下，IGen生成视频的物理一致性达到100%。\n\n- **计算效率**：\n\n![计算效率对比](https://arxiv.org/html/2512.01773v1/x5.png)\n> **图5**：IGen在生成效率和GPU内存消耗上显著优于基线。IGen平均每样本生成时间约18.6秒，GPU内存消耗8.3GB，效率分别比TesserAct和Cosmos-Predict2高出约30倍和200倍。\n\n**3. 从非结构化图像进行机器人学习**：\n实验设置如图6所示，从单张真实场景图像出发，IGen自动生成1000条带有空间随机化的任务演示数据，用于训练视觉运动策略，随后在真实世界部署评估。\n\n![实验设置图示](https://arxiv.org/html/2512.01773v1/x6.png)\n> **图6**：策略训练与评估的实验流程。IGen从单张图像生成大规模数据，用于训练策略，并在真实世界测试。\n\n![真实世界策略评估结果](https://arxiv.org/html/2512.01773v1/x7.png)\n> **图7**：真实世界任务成功率对比。仅使用1000条IGen合成数据训练的模型，其性能与使用100条人类遥操作数据微调的模型相当，甚至部分任务更优，并且显著优于零样本和使用10条人类数据微调的模型。\n\n![策略学习有效性图示](https://arxiv.org/html/2512.01773v1/x8.png)\n> **图8**：策略学习有效性示意图。展示了在IGen数据上训练的模型能生成更准确的任务轨迹（蓝色），而在有限真实数据上训练的模型轨迹（红色）则偏离目标。\n\n**消融实验**：论文虽未以独立章节呈现系统的消融实验，但通过上述核心实验（如对比Real-to-Sim、其他生成模型，以及不同数据源训练策略的效果）已间接验证了其整体框架及各组件（3D重建、VLM规划、点云合成渲染）的有效性和必要性。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了一个有效的数据生成框架IGen，能够从开放世界图像中生成可扩展的视觉-动作数据集，集成了跨场景泛化、指令多样性和长视野任务适用性。\n2. 将非结构化的开放世界图像转化为可操作的3D场景表示，实现了机器人任务推理和运动规划，生成了与物理世界对齐的任务一致行为；并引入了一种免仿真的点云合成方法，生成逼真的视觉观测，支持大规模机器人经验生成。\n3. 通过实验证明，仅使用IGen生成数据训练的机器人策略能够成功执行真实世界操作任务，无需任何额外数据收集，这确立了真实世界图像作为机器人策略训练有效来源的潜力。\n\n**局限性**：\n论文自身提到的局限性包括：1) 场景重建和动作生成的准确性依赖于所采用的外部基础模型（如深度估计、3D重建、VLM）的性能上限。2) 在观测合成阶段，对物体与机器人/环境交互的建模主要基于刚体变换，可能无法完美模拟所有复杂的物理交互细节（如形变、软体接触）。\n\n**对后续研究的启示**：\nIGen为利用海量、低成本的开放世界图像资源进行机器人学习开辟了一条新路径。其方法表明，结合强大的基础视觉模型和语言模型，将2D感知提升到3D结构化理解，并辅以程序化的动作规划和渲染合成，是构建大规模机器人仿真数据的一种高效且可扩展的方案。未来的工作可以沿着以下方向探索：提升基础模型的精度和鲁棒性以改善生成数据质量；引入更复杂的物理仿真以合成更逼真的交互；探索如何将生成的数据与少量真实数据结合以进一步提升策略性能。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01773v1/x1.png",
        "https://arxiv.org/html/2512.01773v1/x2.png",
        "https://arxiv.org/html/2512.01773v1/x3.png",
        "https://arxiv.org/html/2512.01773v1/x4.png",
        "https://arxiv.org/html/2512.01773v1/x5.png",
        "https://arxiv.org/html/2512.01773v1/x6.png",
        "https://arxiv.org/html/2512.01773v1/x7.png",
        "https://arxiv.org/html/2512.01773v1/x8.png",
        "https://arxiv.org/html/2512.01773v1/x9.png",
        "https://arxiv.org/html/2512.01773v1/x10.png",
        "https://arxiv.org/html/2512.01773v1/x11.png",
        "https://arxiv.org/html/2512.01773v1/images/Sup_4.png",
        "https://arxiv.org/html/2512.01773v1/x12.png",
        "https://arxiv.org/html/2512.01773v1/x13.png",
        "https://arxiv.org/html/2512.01773v1/x14.png",
        "https://arxiv.org/html/2512.01773v1/x15.png",
        "https://arxiv.org/html/2512.01773v1/x16.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01715",
      "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
      "url": "http://arxiv.org/abs/2512.01715",
      "arxivId": "2512.01715",
      "date": "2025-12-01",
      "authors": "Zongqing Lu Team",
      "category": "Manipulation",
      "summary": "论文提出DiG-Flow框架，解决视觉-语言-动作（VLA）模型在分布偏移和复杂多步骤任务上性能下降、表示语义不鲁棒的问题。其关键技术是差异引导的流匹配：通过计算观察与动作嵌入的经验分布差异，映射为调制权重，并在流匹配前对观察嵌入进行残差更新，实现几何正则化。实验表明，该方法能以可忽略开销集成到现有VLA架构，持续提升性能，在复杂多步骤任务和有限训练数据下增益尤其显著。",
      "detailedSummary": "## 研究背景与动机\n当前，基于流匹配（Flow Matching）训练的视觉-语言-动作（VLA）模型在机器人操作任务上展现出强大能力。然而，在分布偏移（如光照、纹理、视角变化）和复杂的多步任务下，其性能常常显著下降，这表明模型学习到的表示可能未能鲁棒地捕获任务相关的语义。流匹配框架通过回归神经向量场来学习动作分布，但其回归目标本身可能不足以激励模型学习到语义根基扎实的表示。\n\n本文针对VLA模型表示鲁棒性不足这一痛点，提出了一个几何正则化的新视角。其核心洞察是：观测特征与动作嵌入之间的分布差异（Discrepancy）提供了一个有意义的几何信号——较低的传输成本意味着表示兼容，而较高的成本则暗示可能存在错位。本文提出DiG-Flow框架，通过这种差异信号来引导和调制表示学习，从而增强VLA模型的鲁棒性。核心思路是：计算观测与动作嵌入的经验分布之间的差异度量，将其映射为一个调制权重，并在流匹配之前对观测嵌入应用残差更新。\n\n## 方法详解\nDiG-Flow是一个旨在通过表示层面的几何正则化来增强VLA鲁棒性的框架。其整体流程如论文图1所示：预训练的视觉-语言主干网络提取多模态观测特征，一个轻量级的DiG-Block模块被插入到主干网络与流匹配动作头之间，一个可选的DiG-Refine模块则完全在动作头内部运行以细化预测。\n\n![方法框架](https://arxiv.org/html/2512.01715v1/x1.png)\n\n> **图1**：DiG-Flow整体框架及DiG-Block集成示意图。(a) DiG-Flow将DiG-Block附加在预训练的VLM主干上。观测被投影到一个共享的差异空间，而真实（训练时）或预测（推理时）的动作被映射到同一空间。该模块计算一个传输感知的差异D，并将其转换为门控值g，用于在特征传入流匹配动作头之前对其进行软调制。流匹配头像标准VLA模型一样从噪声生成动作轨迹，DiG-Refine模块可在动作头内应用少量细化步骤以进一步优化预测动作。(b) DiG-Block集成细节。给定输入嵌入，主干首先应用标准注意力。注意力后的特征被归一化并送入DiG-Block，该模块使用动作/观测投影计算差异D和门控g，并执行门控残差更新。此设计允许DiG-Flow利用来自动作的差异信号调制高层表示，同时保持预训练主干架构和注意力块不变。\n\n核心模块包括三个部分：\n1.  **差异函数**：用于量化观测分布μ_H与动作分布μ_Z之间几何距离的函数。默认采用具有几何解释性的平方2-Wasserstein距离。为计算高效，使用切片Wasserstein距离进行近似：采样M个随机单位方向，将高维分布投影到这些方向上，计算一维Wasserstein距离（通过对投影特征排序并计算均方差实现），最后取平均。\n2.  **权重映射**：通过一个单调递减函数ϕ将差异D映射为调制门控值g ∈ [g_min, 1]。具体使用指数衰减映射：g = max{g_min, exp(-τD)}，其中τ为温度参数，g_min防止门控值过小。低差异（对齐良好）的样本对获得接近1的权重，高差异的样本对权重被抑制但仍保留非零贡献。\n3.  **残差算子**：一个轻量级的残差算子ℛ，用于变换观测特征。实现为一个具有谱范数正则化的单线性层：ℛ(H) = W_ℛ H + b_ℛ。增强后的观测特征计算为：H̃ = H + λ·g·ℛ(H)，其中λ为残差强度参数。门控g调节残差调整的幅度。\n\n**训练过程**（对应图2左半部分）：使用真实动作计算差异信号。观测特征H由主干网络提取，真实动作通过一个轻量级编码器f映射为动作嵌入z^gt，经平均池化和广播后得到动作表示Z^gt。计算差异D(μ_H, μ_Z^gt)并得到门控g（梯度在此处停止）。利用g调制残差更新，得到增强特征H̃，将其输入流匹配头。最终的训练目标是加权损失：J(θ) = E[ sg(g) · ℓ(θ; H̃, t) ]，其中ℓ是逐样本流匹配损失。这使g成为一个数据依赖的置信权重，抑制差异大的“捷径”样本。\n\n![训练与推理门控](https://arxiv.org/html/2512.01715v1/x2.png)\n\n> **图2**：训练时和推理时的差异引导门控机制。左（训练）：观测嵌入和真实动作嵌入定义经验分布，差异被映射为门控g，该门控调制对观测的残差更新，并通过门控目标J(θ)=E[g ℓ(θ)]重新加权流匹配损失。右（推理）：使用编码后的模型预测动作而非真实动作应用相同的机制。门控根据观测嵌入和预测动作嵌入之间的差异计算，然后在流匹配头内部使用一次或多次。\n\n**推理过程**（对应图2右半部分及算法2）：默认配置与训练时调制类似，为单次前向。给定观测o，模型计算H并预测初始动作块a^(0)。预测动作被编码得到Z，计算差异D和门控g，生成增强特征H̃，进而产生最终动作块。此外，DiG-Flow支持可选的迭代细化（DiG-Refine）：以初始预测为起点，重复编码预测动作、计算差异和门控、生成增强特征、产生新动作的过程，性能通常在2-3步后饱和。\n\n**理论保证**：论文提供了理论分析，表明在适当的假设下：（1）使用门控目标J(θ)的梯度下降能保证目标函数下降，且J(θ)与原损失函数ℒ(θ)有界；（2）如果残差算子ℛ(H)的方向与损失关于H的负梯度在门控加权下平均对齐，则足够小的残差更新步能严格降低期望损失；（3）固定门控的推理细化方案具有收缩性，能保证收敛。\n\n与现有方法（如使用最优传输直接修改流匹配轨迹的OT-CFM）相比，DiG-Flow的创新点在于将Wasserstein距离作为辅助信号来**调制表示学习**，在表示层面进行干预，而不改变流匹配路径或目标向量场本身，从而以可忽略的开销增强鲁棒性。\n\n## 实验与结果\n实验在模拟和真实世界基准测试上进行，包括**CALVIN**（语言条件机器人操作）、**LIBERO**（长视野任务套件）、**AdAMU**（涉及分布偏移的挑战性操作任务）以及真实机器人**Pi0-Bench**。对比的基线方法包括RT-2、Pi0.5、OpenVLA、Octo、GR-3等主流VLA模型。\n\n关键实验结果如下：在CALVIN基准上，DiG-Flow将Pi0.5的成功率从74.2%提升至79.5%（相对提升7.1%），将OpenVLA的成功率从70.4%提升至75.9%（相对提升7.8%）。在LIBERO的长视野任务上提升尤为显著，例如在`LIBERO-90`上，DiG-Flow将Pi0.5的成功率从30.0%提升至47.5%（相对提升58.3%），将OpenVLA从27.5%提升至42.5%（相对提升54.5%），证明了其在复杂多步任务上的有效性。\n\n![CALVIN与LIBERO结果](https://arxiv.org/html/2512.01715v1/x3.png)\n\n> **图3**：在CALVIN和LIBERO基准测试上的性能对比。DiG-Flow consistently提升了基线模型（Pi0.5和OpenVLA）的成功率，在LIBERO长视野任务上的提升幅度尤其显著。\n\n![数据效率与消融实验](https://arxiv.org/html/2512.01715v1/x4.png)\n\n> **图4**：数据效率研究（左）与消融实验（右）。左图显示在CALVIN上使用不同比例训练数据时，DiG-Flow相比基线Pi0.5始终保持优势，在数据有限（10%）时相对提升高达21.3%。右图消融实验表明，差异门控（Gating）、残差更新（Residual）和动作编码器（Action Enc.）都是有效的组件，共同作用取得最佳性能。\n\n在涉及分布偏移的AdAMU基准上，DiG-Flow同样带来了显著提升。例如在`AdAMU-Hard`上，将Pi0.5的成功率从26.7%提升至43.3%（相对提升62.2%）。真实机器人Pi0-Bench的实验也验证了其有效性。\n\n![AdAMU结果](https://arxiv.org/html/2512.01715v1/x5.png)\n\n> **图5**：在AdAMU基准测试上的性能。DiG-Flow在具有挑战性的分布偏移设置下（尤其是Hard级别），相比基线模型取得了大幅度的成功率提升。\n\n消融实验（图4右）总结了各组件贡献：完整的DiG-Flow（差异门控+残差更新+动作编码器）性能最佳；仅使用差异门控加权损失（无残差更新）有一定提升；仅使用残差更新（无门控，即λ固定）或仅使用动作编码器（无门控和残差）带来的提升较小或微弱。这表明差异计算、门控调制和残差更新协同工作。\n\n![定性结果与差异可视化](https://arxiv.org/html/2512.01715v1/x6.png)\n\n> **图6**：定性结果与差异值可视化。上图展示了DiG-Flow在复杂长视野任务（如“打开抽屉，然后拿起水壶”）上相比基线更可靠的成功执行。下图将预测轨迹各步骤的差异值D可视化为热图，显示在任务关键阶段（如接近目标）差异值较低（蓝色），表明表示对齐良好。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  提出了一个**差异引导（Discrepancy-Guided）的框架DiG-Flow**，通过量化并利用观测与动作表示之间的分布差异作为几何正则化信号，来增强VLA模型的鲁棒性。\n2.  提供了**理论分析**，证明了差异引导训练能降低目标函数，且引导的推理细化具有收敛保证，为方法设计提供了理论依据。\n3.  进行了**广泛的实验验证**，表明DiG-Flow能以可忽略的开销集成到现有VLA架构中，一致地提升性能，尤其在复杂多步任务、分布偏移场景以及数据有限的设置下提升显著。\n\n论文自身提到的局限性包括：依赖切片Wasserstein距离的近似计算仍有一定开销；理论分析基于简化假设（如固定门控）；方法主要针对基于流匹配的VLA模型。\n\n对后续研究的启示：将几何一致性信号引入表示学习是提升模型鲁棒性的一个有前景的方向。DiG-Flow在表示层面的干预思路可扩展至其他生成式策略学习框架（如扩散模型）。差异信号本身可作为模型置信度或异常检测的指标。如何设计更高效、更普适的差异度量与调制机制是未来的探索方向。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01715v1/x1.png",
        "https://arxiv.org/html/2512.01715v1/x2.png",
        "https://arxiv.org/html/2512.01715v1/x3.png",
        "https://arxiv.org/html/2512.01715v1/x4.png",
        "https://arxiv.org/html/2512.01715v1/x5.png",
        "https://arxiv.org/html/2512.01715v1/x6.png",
        "https://arxiv.org/html/2512.01715v1/fig/adamu-setup.jpg",
        "https://arxiv.org/html/2512.01715v1/x7.png",
        "https://arxiv.org/html/2512.01715v1/x8.png",
        "https://arxiv.org/html/2512.01715v1/x9.png",
        "https://arxiv.org/html/2512.01715v1/x10.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01629",
      "title": "SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge",
      "url": "http://arxiv.org/abs/2512.01629",
      "arxivId": "2512.01629",
      "date": "2025-12-01",
      "authors": "Chenfanfu Jiang Team",
      "category": "Manipulation",
      "summary": "本文提出SPARK框架，解决从单张RGB图像重建仿真就绪铰接物体的难题，克服传统方法依赖专家建模、劳动密集的瓶颈。核心技术融合视觉语言模型提取粗粒度URDF参数与部件图像，并集成扩散transformer生成几何一致的部件与整体形状；进一步通过可微分前向运动学与渲染优化关节类型、轴心和原点。实验表明，SPARK能够生成高质量、跨类别的仿真就绪铰接资产，支持机器人操控等下游应用。",
      "detailedSummary": "## 研究背景与动机\n当前，为具身AI和机器人应用创建仿真就绪的关节化3D资产仍然是一项劳动密集型任务，需要专家手动建模部件层次结构和运动结构。主流3D生成模型（如TripoSG、Hunyuan3D）能够从图像或文本生成高质量3D形状，但生成的形状通常是融合的整体，难以用于下游操作、动画和仿真。近期的部件级生成方法（如PartCrafter、AutoPartGen）能够合成具有语义意义的3D部件，但其部件分割仅由外观驱动，忽略了底层的运动结构，导致缺乏运动学一致性。另一方面，关节化物体生成方法（如ArtGS、FreeArt3D）试图合成带有运动学参数的部件级物体，但它们通常依赖于模板网格或多张图像进行可靠分割，或需要明确的运动学参数作为输入。本文针对从单张RGB图像重建物理一致、具有运动学部件级结构的仿真就绪关节化物体这一具体痛点，提出了一个集成视觉语言模型（VLM）知识的新框架。其核心思路是：首先利用VLM从单张图像中提取粗略的URDF参数并生成部件级参考图像；然后，将部件图像引导和推断出的结构图整合到一个生成式扩散变换器中，以合成一致的部件及完整形状；最后，通过可微前向运动学和可微渲染，在VLM生成的开合状态图像监督下优化关节参数。\n\n## 方法详解\nSPARK的整体目标是从单张输入图像 \\(I_0\\)，生成一组部件级网格 \\(\\{\\mathbf{M}_{k}\\}_{k=1}^{K}\\) 及其组成的完整网格 \\(\\mathbf{M}\\)，并估计其层次化的URDF参数 \\(\\mathbf{u}=\\{\\mathbf{u}_{\\ell},\\mathbf{u}_{j}\\}\\)（包含链接节点 \\(\\mathbf{u}_{\\ell}\\) 和关节信息 \\(\\mathbf{u}_{j}\\)）。框架分为三个主要阶段：VLM引导的结构推理、部件-关节化物体生成和关节优化。\n\n![方法总览](https://arxiv.org/html/2512.01629v2/x2.png)\n> **图2**：SPARK方法总览图。给定输入图像，首先利用VLM生成每个部件的参考图像、预测的开合状态图像以及带有初步关节和链接估计的URDF模板。然后，一个配备了局部、全局和层次化注意力机制的扩散变换器（DiT）在VLM先验的引导下，从单张图像同时合成部件级和完整的关节化网格。最后，使用生成式纹理模型添加逼真纹理，并在预测的开合状态图像指导下，通过可微前向运动学和可微渲染优化URDF参数。\n\n**1. VLM引导的结构推理**：该阶段从输入图像中提取初步的部件层次结构、粗略URDF参数和部件参考图像。VLM（如GPT-4o）推断部件数量、链接间的父子关系，并实例化一个符合标准URDF模式的模板。关节参数 \\(\\mathbf{u}_{j}\\) 包括离散属性（如关节类型 \\(\\mathbf{u}_{j}^{\\text{type}}\\)、关节轴 \\(\\mathbf{u}_{j}^{\\text{axis}}\\)）和连续属性（如关节原点 \\(\\mathbf{u}_{j}^{\\text{origin}}\\)、关节限位 \\(\\mathbf{u}_{j}^{\\text{limit}}\\)）。离散属性从预定义字典中选择以确保语义一致性。同时，根据链接的语义标签，利用VLM（如Gemini）生成一组每个部件的参考图像 \\(\\{\\mathbf{r}_{k}\\}_{k=1}^{K}\\) 和一个预测的开合状态图像 \\(I_{\\text{open}}\\)。这些部件图像和根据URDF父子关系构建的结构图将作为后续生成的引导。\n\n**2. 部件-关节化物体生成**：本阶段采用基于扩散变换器（DiT）的生成模型，以全局图像 \\(I_0\\) 和部件图像 \\(\\mathcal{R}\\) 为条件，同时合成所有部件的几何潜在表示。\n*   **多级注意力机制**：使用共享的DINOv2编码器将每个部件图像和全局图像编码为令牌序列，得到局部嵌入 \\(\\mathbf{E}^{\\mathrm{loc}}_{k}\\) 和全局嵌入 \\(\\mathbf{E}^{\\mathrm{glob}}\\)。生成模型维护代表K个部件的潜在几何令牌 \\(Z=[Z_{1};\\dots;Z_{K}]\\)。在去噪过程中，通过交替的**局部**和**全局**交叉注意力块，将图像嵌入与几何令牌融合：局部块使每个部件仅关注其自身的视觉参考，全局块为所有部件注入整体对象上下文。此外，引入**层次化注意力**机制来整合VLM提供的结构先验：根据父子关系映射 \\(\\pi\\)，子令牌仅关注其父令牌（\\(A^{c\\rightarrow p}\\)），更新后的父令牌再查询其子令牌（\\(A^{p\\rightarrow c}\\)）。这种双向方案使变换器能够融合局部细节、全局上下文和结构引导。\n*   **位置嵌入与训练**：采用双位置嵌入方案：可学习的部件嵌入（编码部件在对象内的相对索引）和可学习的绝对位置嵌入（编码规范部件身份，如link_0），以确保输入-输出的稳定对齐。训练采用整流流匹配（Rectified Flow）目标，损失函数为 \\(\\mathcal{L}_{\\mathrm{RF}}=\\mathbb{E}[\\,w(t)\\|V_{\\theta}(X_{t},C,t)-U^{\\star}\\|_{F}^{2}\\,]\\)。优化后的潜在表示通过VAE解码器映射为部件网格并组装成关节化对象。随后使用纹理生成模型（如Meshy）为每个运动部件添加纹理，并用ICP算法进行对齐。\n\n**3. 关节优化**：为进一步细化URDF参数，采用基于优化的策略。对于离散参数（关节轴、类型），使用特征注入策略，以粗略URDF参数和输入图像为条件，让VLM重新预测。对于连续参数（关节原点 \\(\\Delta\\mathbf{t}\\)、旋转角度 \\(\\Delta\\theta\\)），则通过可微前向运动学 \\(G(\\cdot)\\) 和可微渲染器 \\(\\mathcal{R}\\) 进行优化。优化目标是最小化总损失 \\(\\mathcal{L}_{\\mathrm{total}} = \\mathcal{L}_{\\mathrm{pixel}}\\!\\left(I_{\\text{sil}},\\,I_{\\text{open}}\\right)+\\mathcal{L}_{\\mathrm{reg}}(\\boldsymbol{\\xi})\\)，其中像素损失 \\(\\mathcal{L}_{\\mathrm{pixel}}\\) 结合了区域损失 \\(\\mathcal{L}_{\\mathrm{region}}\\) 和边缘损失 \\(\\mathcal{L}_{\\mathrm{edge}}\\)，以促使渲染的轮廓 \\(I_{\\text{sil}}\\) 与VLM生成的开合状态参考图 \\(I_{\\text{open}}\\) 对齐；正则化损失 \\(\\mathcal{L}_{\\mathrm{reg}}\\) 则约束关节参数变化幅度，确保稳定性。\n\n与现有方法相比，SPARK的创新点在于：1) **首次将VLM的语义和结构推理能力**与**生成式扩散变换器**深度融合，用于从单图进行关节化重建；2) 提出了**部件图像引导**和**多级（局部、全局、层次化）注意力机制**，确保了在生成部件级几何时同时满足外观一致性和运动结构一致性；3) 设计了**两阶段URDF参数估计流程**（VLM粗估计+可微优化精修），在缺乏多状态真实监督的情况下，利用VLM生成的开合状态图像实现了关节参数的物理一致性优化。\n\n## 实验与结果\n**实验设置**：方法在PartNet-Mobility数据集上进行训练和评估，该数据集包含2,347个关节化物体，涵盖46个类别。为了增加关节状态的多样性，对数据进行了增强，为每个物体采样了多个可行的关节配置。实验平台基于PyTorch和PyTorch3D。VLM部分使用了GPT-4o和Gemini 2.5 Flash Image。\n\n**对比方法**：在形状重建方面，与部件级生成方法OmniPart和PartCrafter进行对比；在URDF估计方面，与Articulate-Anything和Articulate-AnyMesh进行对比。此外，还与能够同时输出形状和URDF的URDFormer进行了对比。\n\n![定性对比-形状重建](https://arxiv.org/html/2512.01629v2/x3.png)\n> **图3**：形状重建的定性对比。与OmniPart、PartCrafter和URDFormer相比，SPARK能够实现准确、高保真的关节化物体形状重建，部件分割更符合运动结构。\n\n![定性对比-URDF估计](https://arxiv.org/html/2512.01629v2/x4.png)\n> **图4**：URDF估计的定性对比。与Articulate-Anything和Articulate-AnyMesh相比，SPARK估计的URDF参数能产生更准确、物理一致的开合状态，运动行为更真实。\n\n**关键定量结果**：在形状重建质量上，SPARK在Chamfer Distance (CD)和Earth Mover‘s Distance (EMD)指标上均优于对比方法。例如，在PartNet-Mobility测试集上，SPARK的CD为0.86（×10\\(^{-3}\\)），EMD为4.72（×10\\(^{-3}\\)），而PartCrafter的CD为1.21，EMD为5.92。在URDF参数估计准确性上，SPARK在关节类型分类准确率（86.2%）、关节轴方向误差（8.7°）和关节原点位置误差（0.047m）方面均表现最佳。\n\n![消融实验-注意力机制](https://arxiv.org/html/2512.01629v2/x5.png)\n> **图5**：注意力机制的消融实验。移除层次化注意力会导致部件间连接不准确；移除局部注意力会导致部件细节丢失；移除全局注意力会导致整体形状不一致。三者结合效果最佳。\n\n![消融实验-关节优化](https://arxiv.org/html/2512.01629v2/x6.png)\n> **图6**：关节优化模块的消融实验。不使用可微优化（仅使用VLM粗估计）会导致开合状态与参考图像存在明显偏差；加入优化后，渲染轮廓与参考图像对齐度显著提升。\n\n![下游应用](https://arxiv.org/html/2512.01629v2/x7.png)\n> **图7**：下游应用展示。SPARK生成的仿真就绪资产可直接用于机器人操作任务（如开门、拉抽屉）的物理仿真中。\n\n**消融实验总结**：1) **多级注意力机制**：移除层次化注意力会破坏部件间的结构连接；移除局部注意力会损失部件特异性细节；移除全局注意力会影响整体形状一致性。三者缺一不可。2) **关节优化模块**：不使用可微优化，仅依赖VLM初始估计，关节参数（尤其是原点和角度）误差较大，导致开合状态不准确；加入优化后，参数精度显著提升。3) **VLM引导**：不使用VLM生成的部件图像和结构图，仅用全局图像条件，模型无法生成正确的部件级分解。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了一个新颖的端到端框架**SPARK**，首次将VLM的语义/结构先验与扩散变换器生成模型深度融合，实现了从**单张RGB图像**到**仿真就绪部件级关节化资产**的高质量重建。2) 设计了**部件图像引导**和**多级注意力机制**（局部、全局、层次化），有效解决了多部件生成中的一致性问题。3) 引入了基于**可微前向运动学和渲染的关节优化模块**，利用VLM生成的开合状态图像作为弱监督，显著提升了URDF参数的物理准确性。\n\n论文提到的局限性包括：1) 方法性能在一定程度上依赖于所使用VLM（如GPT-4o、Gemini）的识别与生成能力。2) 纹理生成阶段依赖于外部模型（Meshy），并非完全端到端。3) 目前主要处理旋转关节和棱柱关节，对于更复杂的关节类型（如球关节）有待探索。\n\n这项工作对后续研究的启示在于：展示了**大模型（VLMs）作为“语义与结构解析器”** 与 **3D生成模型** 结合的强大潜力，为数据匮乏或标注成本高的复杂结构化3D重建问题提供了新范式。未来方向可能包括：探索更高效的VLM-生成模型耦合方式，将纹理生成整合进统一框架，以及扩展到更广泛的关节类型和动态场景理解中。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01629v2/x1.png",
        "https://arxiv.org/html/2512.01629v2/x2.png",
        "https://arxiv.org/html/2512.01629v2/x3.png",
        "https://arxiv.org/html/2512.01629v2/x4.png",
        "https://arxiv.org/html/2512.01629v2/x5.png",
        "https://arxiv.org/html/2512.01629v2/x6.png",
        "https://arxiv.org/html/2512.01629v2/x7.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01446",
      "title": "$\\mathbf{M^3A}$ Policy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering",
      "url": "http://arxiv.org/abs/2512.01446",
      "arxivId": "2512.01446",
      "date": "2025-12-01",
      "authors": "Jianfei Yang Team",
      "category": "Manipulation",
      "summary": "本文提出M³A策略，解决机器人操作中因物体材料（如玻璃、金属）的透明或反光特性导致的视觉域偏移和泛化难题。核心方法是通过光度重渲染技术，仅凭单次真实演示，即可生成具有不同材料属性的高度逼真演示数据，从而将操作技能与表面外观解耦。实验表明，该方法在三个真实任务中将平均成功率提升58.03%，并能有效泛化至未见过的材料。",
      "detailedSummary": "## 研究背景与动机\n当前基于学习的机器人操作策略主要依赖视觉感知来推断物体状态并指导控制动作，这使得它们对物体外观的变化高度敏感。材料特性（如颜色、表面粗糙度、透明度）会引入显著的外观变化，导致视觉感知不一致，从而降低操作精度。为了增强泛化能力，现有方法要么依赖收集大规模的真实世界演示（成本高昂且耗时），要么采用模拟到真实（sim-to-real）的迁移，利用模拟数据和域随机化。然而，对于材料泛化而言，sim-to-real方法面临严峻挑战：模拟中渲染的材质视觉线索（如反射率、透明度、纹理）难以达到足够的真实感，导致转移到物理世界时存在视觉差异。\n\n本文针对上述痛点，提出了一种新的视角：将材料变化的来源与操作演示的来源解耦。其核心思路是，利用计算摄影学捕获的材料物理特性（光传输）进行光度重渲染，仅需给定单个真实世界演示，即可通过改变其材料属性生成大量高度逼真的演示变体，从而在无需额外数据收集的情况下，使策略学习到与材料无关的操作技能，实现跨材料泛化。\n\n## 方法详解\nM³A框架旨在通过生成物理上可信的材料表示并将其注入原始演示中，来高效训练具有材料泛化能力的策略。整体流程包含三个阶段：演示收集、M³A增强和模仿学习训练。\n\n![方法框架](https://arxiv.org/html/2512.01446v1/x2.png)\n> **图2**：M³A策略框架。包含三个阶段：(1) 演示收集：从模拟或真实环境中收集视动轨迹（视频和动作序列）；(2) M³A：对操作物体的材料外观进行重新组合或替换，以引入逼真的视觉多样性；(3) 模仿学习：在增强后的演示上训练策略，以实现跨材料和环境的改进泛化。\n\n**整体流程**：给定一个操作任务，收集一组包含N个配对的演示数据集𝒟 = {(𝐎_i, 𝐀_i)}，其中𝐎_i = {o_i^t}为视觉观测序列，𝐀_i = {a_i^t}为对应的动作序列。M³A对𝒟进行增强，将真实世界材料表示提取并注入𝐎_i中的目标物体，生成增强观测𝐎'_i，从而得到增强数据集𝒟' = {(𝐎'_i, 𝐀_i)}。最终，结合原始和增强数据得到M³基准数据集𝒟̂ = 𝒟' ∪ 𝒟，用于训练策略。\n\n**核心模块与技术细节**：\n1.  **掩码提取**：由于材料变化通常仅涉及任务相关的目标物体，因此需要精确提取目标物体的掩码。该方法利用强大的视觉-语言分割模型Grounded-SAM2，根据任务描述（文本提示或视觉提示）从观测序列𝐎_i中分割出目标物体掩码𝐌_i，并利用序列信息增强掩码的一致性。\n2.  **深度图估计**：为了提供物体和环境的几何先验，确保材料外观在不同场景下的变化真实可信，需要深度信息。在模拟器中可直接获取物理精确的深度图；在真实世界中，则使用在大规模数据上预训练的深度预测基础模型DPT-Hybrid (MiDaS)来估计每个RGB观测的深度图𝐃_i。\n3.  **材料转移**：这是方法的核心创新点。首先建立一个材料示例集𝐙 = {z_m}，每个材料对应一个纹理图像z_m。然后，使用CLIP视觉编码器φ_CLIP(·)和IP-Adapter ε_IP(·)从纹理图像中提取视觉特征，作为每个材料的唯一表示：f_{z_m} = ε_IP(φ_CLIP(z_m))。接着，随机采样一个材料特征f_{z_m}，将其注入到基于U-Net的Stable Diffusion模型的瓶颈层，以在目标物体的掩码区域“绘制”新的材料，生成具有不同材料外观的增强观测𝐎'_i。通过简单地改变参考图像z_m，即可将单个目标物体转换为多种材料外观。\n\n**策略训练**：M³A是一个即插即用的增强模块。本文采用基于扩散的策略，在模仿学习范式下进行训练。策略π_θ(a_t|o_t)从增强数据集𝒟̂中学习。扩散策略将动作预测建模为基于观测的条件去噪过程。训练时，目标是最小化噪声预测损失：ℒ_DP = ∥ϵ^k - π_θ(a_t^k, k, o'_t)∥²。通过以增强观测o'_t为条件，策略学习到对材料变化不变的动作模式。\n\n**创新点**：与现有数据增强方法（如图像空间变换、几何扰动）主要针对光照、视角、姿态等变化不同，M³A首次明确且系统地针对材料属性进行物理可信的视觉增强。其创新性在于利用计算摄影学和生成模型（CLIP+IP-Adapter+Stable Diffusion），将材料的紧凑、可迁移表示注入到真实演示中，从而低成本地生成大规模、多样化的多材料真实世界数据，避免了sim-to-real的视觉差距和繁重的数据收集工作。\n\n## 实验与结果\n**实验设置**：\n- **基准/数据集**：构建了基于高保真模拟平台RoboVerse的Mutable Material Manipulation (M³) 基准。在模拟和真实世界环境中进行评估。\n- **实验平台**：模拟实验使用IsaacLab；真实实验使用Franka Emika Panda机械臂。\n- **任务**：模拟实验包括PickCube（拾取立方体）、StackCube（堆叠立方体）、CloseBox（关盒盖）；真实实验包括不同复杂度的立方体拾放任务。\n- **对比方法**：\n    - **DP**：使用RoboVerse默认材料演示训练的扩散策略。\n    - **DP-Render**：使用经过RoboVerse材料和光照随机化增强的演示训练的扩散策略。\n    - **DP-M³A**：使用经过本文M³A框架增强的演示训练的扩散策略。\n    - **DP-6**（真实实验）：使用6种不同真实材料物体收集的演示训练的扩散策略（作为数据收集成本更高的对比基线）。\n\n**关键实验结果**：\n\n![材料转移结果](https://arxiv.org/html/2512.01446v1/x3.png)\n> **图3**：M³A在模拟和真实世界中产生的材料转移结果。上行是原始观测，下行是材料转移后的输出。示例展示了从塑料到木材、金属、玻璃和宝石的转换，证明了增强的逼真性。\n\n**模拟实验**：\n表1展示了PickCube任务在12种材料类别上的成功率。DP-M³A以34.4%的整体成功率显著优于DP（11.3%）和DP-Render（21.9%），在所有材料类别上均取得最佳性能，尤其在具有镜面反射或复杂纹理的金属、玻璃等挑战性材料上提升明显。\n> **表1**：PickCube任务在模拟中不同材料上的成功率对比。DP-M³A在所有材料类别上均优于基线。\n\n表2对比了三个模拟任务的平均成功率。DP-M³A在PickCube、CloseBox和StackCube任务上分别达到34.4%、27.1%和6.9%，平均成功率为22.80%，远高于DP的10.16%，证明了其跨任务的有效性。\n> **表2**：DP与M³A方法在三个模拟操作任务上的成功率对比。M³A在所有任务上均有显著提升。\n\n**真实世界实验**：\n表3展示了在涉及11种真实材料的三项立方体操作任务上的性能。DP-M³A在“拾取”、“拾放”和“长视界拾放”任务上的平均成功率分别为89.40%、68.18%和80.30%，整体平均成功率为79.29%。与仅使用塑料演示训练的DP相比，平均成功率提升了58.03%。更重要的是，DP-M³A的性能甚至优于使用6种真实材料收集了更多数据的DP-6策略，证明了M³A在有限数据下实现卓越泛化的效率。\n> **表3**：在涉及11种材料类型的三个真实世界立方体操作任务上的性能对比。DP-M³A在绝大多数材料上取得最高成功率，平均性能大幅领先。\n\n![模拟与真实性能关联](https://arxiv.org/html/2512.01446v1/x4.png)\n> **图4**：模拟性能与真实世界性能的关联分析。左图显示，在模拟中表现更好的方法，在真实世界中也倾向于表现更好（Spearman等级相关系数ρ=0.83）。右图显示，M³基准的模拟评估结果与真实世界评估结果高度一致，验证了该基准作为高效评估协议的有效性。\n\n**消融实验**：\n论文通过消融实验验证了各组件贡献。关键结论包括：1）同时使用掩码和深度信息进行材料转移，效果优于仅使用其中一种；2）利用CLIP和IP-Adapter提取的材料特征进行编辑，比简单的纹理粘贴产生更逼真、物理一致的结果；3）在模拟和真实数据上联合训练策略，能带来进一步的性能增益。\n\n## 总结与启发\n**核心贡献**：\n1. 提出了**M³A框架**，一种通过计算摄影学进行光度重渲染的、物理可信的材料操作增强方法，能够利用单次真实演示高效生成大规模多材料数据集，显著提升策略的跨材料泛化能力。\n2. 建立了**M³基准**，一个基于高保真模拟和真实验证的综合评估套件，其模拟评估结果与真实世界性能高度一致，为材料泛化研究提供了可靠的测试平台。\n3. 通过广泛的模拟和真实实验证实了M³A的有效性，策略在未见过的材料上实现了零样本泛化，并在三项真实任务中将平均成功率提升了58.03%。\n\n**局限性**：\n1. 方法依赖于从单目图像估计的深度图，其精度可能限制材料编辑的真实感，尤其是在复杂几何或遮挡情况下。\n2. 材料编辑主要关注视觉外观的逼真性，并未显式建模材料变化可能带来的物理属性（如摩擦系数、重量）差异，这可能在物理交互敏感的任务中带来挑战。\n\n**启示**：\n1. 将计算摄影学、生成式AI与机器人学习相结合，为突破数据瓶颈和sim-to-real差距提供了新思路。\n2. 解耦技能学习与特定域变化（如材料、光照、纹理）是实现鲁棒泛化的有效途径，可推广至其他视觉变化领域。\n3. 构建模拟与真实性能高度关联的基准，对于高效推进具身智能研究具有重要意义。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01446v1/x1.png",
        "https://arxiv.org/html/2512.01446v1/x2.png",
        "https://arxiv.org/html/2512.01446v1/x3.png",
        "https://arxiv.org/html/2512.01446v1/x4.png",
        "https://arxiv.org/html/2512.01446v1/pic/SR_epochs.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01188",
      "title": "Real-World Reinforcement Learning of Active Perception Behaviors",
      "url": "http://arxiv.org/abs/2512.01188",
      "arxivId": "2512.01188",
      "date": "2025-12-01",
      "authors": "Dinesh Jayaraman Team",
      "category": "Manipulation",
      "summary": "本文针对机器人在部分可观测环境下难以通过标准学习技术生成主动感知行为的问题，提出了非对称优势加权回归（AAWR）方法。该方法利用训练时可用的“特权”额外传感器，训练高质量特权价值函数以估计策略优势，并从少量次优演示与粗略策略初始化进行引导。实验表明，AAWR在3种机器人、8个操作任务上优于所有现有方法，能高效生成信息收集行为，使机器人在严重部分可观测条件下有效操作。",
      "detailedSummary": "## 研究背景与动机\n当前机器人学习领域，模仿学习和强化学习是合成任务相关策略的主流方法。然而，对于主动感知行为的学习，这两种方法均存在关键局限性。模仿学习的性能受限于演示者，而获取最优的主动感知演示（例如强制遥操作员通过腕部摄像头观察）通常繁琐且不自然。强化学习理论上可以从交互中学习，但在实践中样本效率低下，在完全可观测场景下尚且如此，在需要主动感知的部分可观测场景下则更为困难。此外，由于主动感知任务与传感器能力紧密相关，而RGB、深度、触觉等传感器难以高保真模拟，使得从仿真到实体的迁移极具挑战性。本文针对在部分可观测环境下高效学习真实世界机器人主动感知行为这一具体痛点，提出了利用训练时可访问“特权”额外传感器的新视角。其核心思路是，通过扩展优势加权回归算法，利用特权信息训练高质量的价值函数来估计策略的优势，从而更有效地监督仅接收部分观测的策略学习。\n\n## 方法详解\n本文方法的整体框架遵循不对称训练与部署的范式。在训练阶段（包括离线和在线），策略 $\\pi$ 接收部分观测 $o_t$（如腕部摄像头图像），而评论家 $Q$ 和价值函数 $V$ 则接收特权观测 $o_t^+$（如物体检测框、分割掩码或真实状态）来估计优势函数 $A^\\mu(s_t, z_t, a_t)$。该优势估计作为权重，用于加权行为克隆损失来更新策略。在部署阶段，特权传感器被移除，策略仅基于部分观测 $o_t$ 生成动作。\n\n![方法框架](https://arxiv.org/html/2512.01188v1/figures/AAWR_method_v2.png)\n> **图2**：AAWR方法示意图。上行：策略接收部分观测。下行：仅在训练时可用的特权观测或状态被提供给评论家网络以估计优势。优势估计作为损失函数中的权重，为策略提供特权监督。\n\n核心模块是不对称优势加权回归目标函数 $\\mathcal{L}_{\\text{AAWR}}(\\pi)$。该方法是对标准优势加权回归的扩展，其关键在于优势估计依赖于特权状态 $s$ 和智能体状态 $z$（即历史编码）。具体目标函数如下：\n$$\\mathcal{L}_{\\text{AAWR}}(\\pi) = \\mathbb{E}_{(s,z) \\sim d_\\mu(s,z)} \\mathbb{E}_{a \\sim \\mu(a|z)} \\left[ \\exp\\left( A^\\mu(s, z, a) / \\beta \\right) \\log \\pi(a|z) \\right]$$\n其中 $A^\\mu(s, z, a) = Q^\\mu(s, z, a) - V^\\mu(s, z)$ 是特权优势函数。论文从理论上证明，在部分可观测马尔可夫决策过程中，为最大化期望策略改进并施加KL散度约束，其拉格朗日松弛问题的最优解正是上述AAWR目标。相反，如果仅使用智能体状态 $z$ 来估计优势的对称版本则无法得到正确解，因为无法准确估计等效MDP（状态为 $(s, z)$）的优势。\n\n在实现上，采用离线到在线强化学习流程。首先，使用离线数据集 $\\mathcal{D}_{\\text{off}}$（包含次优演示）通过IQL算法训练特权评论家和价值函数，并利用上述AAWR目标更新策略。随后，在在线阶段，执行策略收集数据存入 $\\mathcal{D}_{\\text{on}}$，并混合两个数据集的数据进行策略和值函数的持续优化。部署时则仅运行训练好的策略。\n\n![训练与部署流程](https://arxiv.org/html/2512.01188v1/figures/aawr_pi0_sysdiagram.png)\n> **图6**：AAWR离线到在线训练及与通用策略 $\\pi_0$ 交接的完整系统示意图。左侧为训练阶段，策略利用特权传感器学习；右侧为部署阶段，仅使用策略进行主动感知搜索，完成后将控制权交给 $\\pi_0$ 执行抓取。\n\n与现有方法相比，创新点具体体现在：1) 理论推导了在POMDP中应用AWR时需要不对称（特权）优势估计的必然性；2) 提出了实用的AAWR算法，能够高效利用真实世界中有限的、可能带有噪声的特权观测数据，而非依赖仿真中大量的完美状态转移；3) 将方法应用于提升现有通用策略在部分可观测任务上的性能，通过主动感知为其“探路”。\n\n## 实验与结果\n论文在8个不同的任务上评估AAWR，涵盖模拟和真实世界设置，涉及Koch和Franka等机器人平台。任务类型包括模拟主动感知（如Camouflage Pick）、真实交互感知（如Blind Pick）以及为通用视觉语言动作策略 $\\pi_0$ 进行搜索的“交托”任务（如Bookshelf-P）。对比的基线方法包括：对称AWR、行为克隆、蒸馏法、变分信息瓶颈以及 exhaustive 搜索和 VLM+$\\pi_0$ 等。评估指标包括任务成功率、搜索行为评分、完成率和步骤数。\n\n![模拟任务结果](https://arxiv.org/html/2512.01188v1/figures/sim_results.png)\n> **图4**：模拟实验评估曲线（10次随机种子）。阴影区域表示离线预训练阶段。AAWR在所有模拟任务中均优于基线方法。\n\n在模拟主动感知任务中，AAWR显著优于基线。在Camouflage Pick和Fully Obs. Pick任务中，AAWR的成功率分别比对称AWR和行为克隆高出约2倍和3倍。在更具挑战性的Active Perception Koch任务中，AAWR达到了100%成功率，而蒸馏法停滞在80%，变分信息瓶颈法则在部署时崩溃。这表明AAWR能够通过在线探索发现有效的主动感知行为，而其他利用特权信息的方法则可能陷入局部最优或无法适应部署时的信息缺失。\n\n![真实交互感知结果](https://arxiv.org/html/2512.01188v1/figures/AAWR_vs_AWR_left.png)\n> **图5**：Koch机器人交互感知任务（Blind Pick）的结果对比。AAWR的离线和在线版本在抓取和拾取成功率上均优于对称AWR和行为克隆。\n\n在真实世界Blind Pick任务中，在线AAWR取得了94%的抓取成功率和89%的拾取成功率，表现最佳。离线AAWR也优于离线对称AWR，但经过在线微调后性能得到进一步提升，消除了某些次优行为。\n\n![交托任务结果](https://arxiv.org/html/2512.01188v1/figures/pi0_metric.png)\n> **图7**：交托任务评估指标说明。通过搜索评分、完成率和步骤数综合评价主动感知策略为通用策略 $\\pi_0$ 做准备的效果。\n\n在4个真实世界“交托”任务中，AAWR在搜索行为评分、任务完成率方面 consistently 优于所有基线，且所需步骤更少。例如在Bookshelf-P任务中，AAWR搜索成功率达92.4%，完成率44.4%，而对称AWR分别为79.6%和0%，通用策略 $\\pi_0$ 自身则仅为11%和16.7%。这证明AAWR能有效学习信息搜集行为，显著增强通用策略处理严重部分可观测任务的能力。\n\n## 总结与启发\n本文的核心贡献在于：1) 提出了不对称优势加权回归算法，通过利用训练时的特权传感器高效学习真实世界机器人的主动感知策略；2) 从理论上证明了在POMDP中使用特权优势估计对于AWR类方法是必要的；3) 在8个不同任务上进行了广泛实验，验证了AAWR在多种部分可观测场景、机器人平台和任务类型中的有效性。\n\n论文提到的局限性主要在于需要额外的传感器（如用于提供边界框或分割掩码的摄像头）来在训练时生成特权信息。\n\n本文的启示在于：为学习复杂的、与环境交互以获取信息的感知行为提供了一条高效的现实途径，特别是当最优演示难以获取或仿真到实体迁移不可行时。此外，它展示了如何通过专门的主动感知策略来辅助和提升在大规模数据上训练的通用策略，使其能够应对部分可观测的挑战性任务。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01188v1/figures/fiona-concept-4.png",
        "https://arxiv.org/html/2512.01188v1/figures/AAWR_method_v2.png",
        "https://arxiv.org/html/2512.01188v1/figures/all_task_setup.png",
        "https://arxiv.org/html/2512.01188v1/figures/sim_results.png",
        "https://arxiv.org/html/2512.01188v1/figures/AAWR_vs_AWR_left.png",
        "https://arxiv.org/html/2512.01188v1/figures/aawr_pi0_sysdiagram.png",
        "https://arxiv.org/html/2512.01188v1/figures/pi0_metric.png",
        "https://arxiv.org/html/2512.01188v1/figures/pi0_robot_setup.jpg",
        "https://arxiv.org/html/2512.01188v1/figures/pi0_task_scene.png",
        "https://arxiv.org/html/2512.01188v1/figures/pi0_reward_example.png",
        "https://arxiv.org/html/2512.01188v1/figures/pi0_failure_bars.png",
        "https://arxiv.org/html/2512.01188v1/figures/koch_setup.jpg",
        "https://arxiv.org/html/2512.01188v1/figures/Koch_running.jpg",
        "https://arxiv.org/html/2512.01188v1/figures/simulation-camopick-result-behavior.png",
        "https://arxiv.org/html/2512.01188v1/figures/simulation_fully_obs_pick.png",
        "https://arxiv.org/html/2512.01188v1/figures/simulation-AP-koch-observation.png",
        "https://arxiv.org/html/2512.01188v1/figures/simulation-AP-koch-result-behavior.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2512.01358",
      "title": "Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1",
      "url": "http://arxiv.org/abs/2512.01358",
      "arxivId": "2512.01358",
      "date": "2025-12-01",
      "authors": "Songhwai Oh Team",
      "category": "Manipulation",
      "summary": "本文针对基础机器人策略在跨具身操作中因模态缺失（如接触和深度信息）导致的性能局限，提出模态增强微调框架。方法包括：在GR1上通过后处理添加二进制接触信号与ZoeDepth深度；在G1上构建多模态数据集，集成cuRobo运动规划与真实接触力测量。实验显示，GR1成功率从51%提升至63%；G1“取苹果入碗”任务中，零射击成功率为0%，标准微调达48%，而接触增强模型最高达到94%，验证了模态增强对跨具身泛化的有效性。",
      "detailedSummary": "## 研究背景与动机\n当前机器人领域的主流方法是基于大规模视觉-语言-动作（VLA）模型的基础机器人策略，如Isaac GR00T，它们利用海量演示数据和基于扩散的动作生成，展现出强大的通用控制能力。然而，这些策略在部署到需要精确感知接触事件、可靠深度推理和交互感知控制的精细操作场景时，仍存在显著局限性。现有机器人数据集因异构的本体、传感配置和控制模式而形成“数据孤岛”，这阻碍了基础模型像语言或视觉模型那样利用互联网规模的数据。此外，跨本体泛化极具挑战性，因为机器人形态和运动学结构从根本上塑造了可行运动和交互策略的分布。\n\n本文针对现有基础策略在感知模态上的关键瓶颈，提出了一个数据中心和模态增强的微调新视角。具体而言，公开的GR1数据集仅提供RGB观测和本体感知状态，缺乏深度、接触指示或力测量等关键交互中心模态。这导致VLA模型必须仅从彩色图像推断接触和物体交互边界，在涉及遮挡或视觉模糊接触转换的场景中，这是一个不适定问题。本文的核心思路是：通过后处理为GR1数据集增加深度和接触信号，并为缺乏公开数据的Unitree G1本体构建全新的包含真实接触力的多模态数据集，从而对基础策略进行针对性模态增强的微调，以提升跨本体操作的鲁棒性和成功率。\n\n## 方法详解\n本文方法的整体框架基于Isaac GR00T基础策略进行模态增强的微调。输入是增强后的多模态观测（如RGB-D图像、包含接触信号的本体感知状态），输出是通过扩散去噪过程生成的动作序列。核心模块主要包括针对GR1的模态后处理增强，以及针对G1的全新多模态数据集构建与对应的编码策略。\n\n![方法框架](https://arxiv.org/html/2512.01358v1/gr00t_pic.png)\n> **图1**：GR00T双系统架构概览。系统2（VLM）将图像观测和语言指令处理成语义令牌，而系统1（扩散Transformer）通过以多模态令牌为条件的迭代去噪生成电机动作。\n\n首先，对于G1本体，由于没有公开数据集，作者构建了一个高质量的数据收集流程。使用cuRobo的GPU加速运动规划器生成平滑无碰撞的末端执行器轨迹，并通过解析逆运动学求解器转换为G1关节空间命令，从而获得高时序一致性的参考轨迹。在轨迹执行过程中，记录完整的本体感知状态（关节位置、速度、末端执行器姿态）和执行的关节动作。最关键的是，利用G1物理引擎提取指尖和手掌的真实接触力，为策略提供细粒度的交互线索。\n\n![G1数据收集环境](https://arxiv.org/html/2512.01358v1/g1_data.png)\n> **图2**：G1数据收集环境可视化。左图：场景设置的前视图。右图：演示执行期间机器人车载摄像头的自我中心视图。\n\n![G1关节状态与动作](https://arxiv.org/html/2512.01358v1/data_state_action.png)\n> **图3**：每个G1关节的状态与动作。对于每个关节，我们可视化了随时间变化的本体感知状态（蓝色）和执行的动作命令（红色）。通过cuRobo规划生成的轨迹在20个自由度的上肢链上展现出平滑、一致的演变，确保了扩散策略微调的高质量监督。\n\n![G1指尖接触力](https://arxiv.org/html/2512.01358v1/contact_data.png)\n> **图4**：随时间变化的每根手指接触力。G1灵巧手提供了高保真的指尖和手掌力测量。在执行抓取期间，接触力集中在特定手指（左拇指、中指和手掌），而其他手指保持不活动。这些丰富的交互信号用于训练接触感知的扩散策略。\n\n其次，对于GR1数据集，作者进行了两种模态增强。一是**深度增强（RGB-D融合）**：使用ZoeDepth单目深度估计器为所有GR1演示生成度量深度图。通过将视觉编码器的patch嵌入层从3通道（RGB）扩展到4通道（RGB-D）来融入深度信息，并对深度通道权重进行RGB核平均初始化以稳定训练。二是**接触增强**，提出了两种策略：1) **轻量级融合**：将模拟器中推导出的二进制接触指示符`c_t`直接拼接到原始本体感知状态向量中，然后通过本体特定的状态编码器获得接触感知嵌入。2) **专用接触编码器模块**：将接触信号通过一个可学习的MLP进行编码，作为一个独立的模态令牌输入扩散Transformer，使其与视觉、语言和状态模态具有同等重要性。\n\n![专用接触编码器模块](https://arxiv.org/html/2512.01358v1/contact_encoder.png)\n> **图5**：专用接触编码器模块。二进制（或连续）接触信号`c_t`由可学习的接触编码器处理，生成的嵌入作为独立模态与视觉、语言和状态令牌一同输入DiT块。这种设计将接触视为独立模态，允许策略学习更丰富的交互感知表示。\n\n![RGB与估计深度图](https://arxiv.org/html/2512.01358v1/rgbanddepth.png)\n> **图6**：GR1演示的RGB和估计深度图。左图：来自GR1模拟器的原始RGB观测。右图：由ZoeDepth计算出的对应度量深度图。深度通过捕捉物体形状、桌子高度和柜子几何形状等空间结构来增强几何推理。\n\n与现有方法相比，本文的创新点不在于修改GR00T核心的扩散Transformer架构，而在于**针对性地设计和构建输入模态**。通过数据后处理（对GR1）和高质量数据生成（对G1）来弥补基础模型训练数据中缺失的关键交互模态，并通过轻量级的编码器调整将这些新模态集成到已有的策略框架中。\n\n## 实验与结果\n**实验设置**：实验在单个NVIDIA A6000 GPU上进行，使用官方GR00T微调框架。在线评估在基于robosuite和robocasa的模拟器中通过服务器-客户端推理流水线进行。离线评估使用数据集的轨迹重建均方误差（MSE）。评估了两个平台：GR1（使用HuggingFace上的公开数据集，三个取放任务）和Unitree G1（使用自定义的“Pick Apple to Bowl”任务数据集）。\n\n**基线方法**：对比了零样本GR00T N1.5、标准微调（仅用原始或基础数据），以及本文提出的各种模态增强变体（+深度、+接触状态融合、+专用接触编码器）。\n\n**关键实验结果**：\n1.  **GR1实验（模态增强分析）**：在线评估表明，接触状态增强提供了适度增益，而深度增强在视觉模糊场景下显著提高了成功率。结合深度和接触的完整模型在所有评估任务中取得了最高成功率。具体而言，在GR1上，添加接触状态线索和RGB-D融合将在线成功率从51%提高到了63%。离线MSE评估显示，深度增强对轨迹重建质量的改进最为显著，表明几何线索大大减少了动作预测的模糊性。\n\n2.  **G1实验（跨本体迁移）**：在线评估结果（见表I数据）显示，零样本GR00T在G1任务上完全失败（0%成功率），凸显了GR1与G1在本体上的巨大差距。使用5K条本体对齐演示进行标准微调将成功率提升至48%。融入接触力测量带来了显著额外增益：将力信号作为辅助模态建模使成功率增至74%；而将力信号直接融合到本体感知状态表示中取得了最佳性能，成功率高达94%。深度增强也贡献了实质改进，达到82%的成功率，但效果不及接触增强。\n\n![在线推理模拟环境](https://arxiv.org/html/2512.01358v1/online_inference.png)\n> **图7**：用于在线评估的示例模拟环境。每次 rollout 随机化物体放置、纹理、光照和机器人配置以评估鲁棒性。\n\n![动作预测对比](https://arxiv.org/html/2512.01358v1/eval_data.png)\n> **图8**：在前三个动作维度上，真实动作、预测动作和执行推理点之间的比较。模型紧密跟踪演示轨迹，表明基于扩散的去噪过程稳定且动作重建准确。\n\n**消融实验总结**：\n-   **在GR1上**，深度和接触信息是互补的。深度主要改善几何理解和离线重建精度，而接触信息有助于在线操作的稳定性，两者结合效果最佳。\n-   **在G1上**，接触力信息比深度信息更为关键。特别是将接触力直接融合到本体感知状态中的“早期融合”策略，比使用独立接触编码器效果更好，这表明对于G1的灵巧操作，接触动力学需要与本体状态进行更紧密的耦合。这揭示了最优模态设计是**本体特定**的：GR1主要从几何增强中获益，而G1则更依赖于接触丰富的监督来补偿其更高的操作灵巧性和交互复杂性。\n\n## 总结与启发\n本文的核心贡献可概括为三点：\n1.  **构建了多模态GR1和G1数据集**：通过后处理为GR1添加了深度和接触信号，并利用cuRobo运动规划和真实力测量为G1创建了全新的高质量多模态数据集，解决了当前VLA基础策略的模态瓶颈。\n2.  **提出了轻量级的架构增强策略**：提出了两种将接触和深度信号融入GR00T扩散策略的方法（轻量级状态融合和专用接触编码器），无需修改核心模型结构。\n3.  **实证了模态增强微调的有效性**：系统性的实验表明，模态增强微调能显著提高操作成功率，并实现从GR1到G1的强跨本体迁移，缩小了因形态差异带来的性能差距。\n\n**论文提到的局限性**：研究强调了最优模态选择强烈依赖于机器人形态和传感能力。本文的模态设计是针对GR1和G1两个特定本体进行的，尚未探索更广泛的、自动化的模态选择方法。\n\n**对后续研究的启示**：这项工作展示了一条通过针对性模态设计和多模态微调来扩展基础机器人策略的实用路径。未来研究可以探索如何自动确定目标本体所需的关键模态，开发更通用的多模态融合架构，以及利用更高效的数据生成技术（如仿真或生成模型）来规模化构建多模态机器人数据集，从而进一步推动基础机器人策略的泛化能力和实际部署。",
      "imageUrls": [
        "https://arxiv.org/html/2512.01358v1/gr00t_pic.png",
        "https://arxiv.org/html/2512.01358v1/g1_data.png",
        "https://arxiv.org/html/2512.01358v1/data_state_action.png",
        "https://arxiv.org/html/2512.01358v1/contact_data.png",
        "https://arxiv.org/html/2512.01358v1/contact_encoder.png",
        "https://arxiv.org/html/2512.01358v1/rgbanddepth.png",
        "https://arxiv.org/html/2512.01358v1/online_inference.png",
        "https://arxiv.org/html/2512.01358v1/eval_data.png",
        "https://arxiv.org/html/2512.01358v1/off_data.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2511.23186",
      "title": "Obstruction reasoning for robotic grasping",
      "url": "http://arxiv.org/abs/2511.23186",
      "arxivId": "2511.23186",
      "date": "2025-11-28",
      "authors": "Fabio Poiesi Team",
      "category": "Manipulation",
      "summary": "本文针对杂乱环境中机器人抓取需预先清除障碍的问题，提出UNOGrasp模型。该模型基于视觉语言进行多步障碍推理，通过目标物体产生的障碍路径、障碍感知视觉线索以及结合监督与强化学习的微调方法，推断抓取序列。实验表明，UNOGrasp在合成与真实环境中显著提升了障碍推理与抓取成功率，优于现有通用及专用模型。",
      "detailedSummary": "## 研究背景与动机\n当前，机器人要在杂乱无章的环境中（如箱内拣选、按自然语言指令进行物体组装）成功抓取目标物体，需要视觉语言模型（VLMs）不仅能够对目标物体进行视觉定位，还需理解物体间的物理依赖关系，特别是遮挡关系。然而，尽管现有的视觉语言具身推理模型展现出一定的空间理解能力，但在遮挡推理和可访问性规划方面仍存在局限。现有的基准测试（如EmbSpatial-Bench、Spatial457）主要测试静态感知，而非面向动作的遮挡推理（即规划清理动作）。此外，大多数数据集缺乏语言标注，限制了其在VLM研究中的应用。本文针对机器人抓取中目标物体被遮挡这一具体痛点，提出了“遮挡推理”的新视角，旨在识别源自目标物体的遮挡路径，并推断出清除遮挡所需的多步动作序列。本文的核心思路是：通过构建一个以目标物体为中心的有向遮挡图来形式化遮挡推理问题，并引入一个结合监督微调与基于可验证奖励的强化微调的两阶段训练方法，来提升模型的多步遮挡推理能力。\n\n## 方法详解\n本文提出了UNOGrasp模型和一个用于训练与评估的大规模基准数据集UNOBench。\n\n**整体框架与问题定义**：给定一个RGB-D观测图像 $I = (I_{rgb}, I_d)$ 和一个自由形式的文本指令 $q$（例如“抓取最左边的白色盒子”），目标是产生一个清除遮挡的计划以抓取唯一指定的目标物体 $o_t$。$I_{rgb}$ 用于推理和动作序列规划，$I_d$ 用于估计3D抓取点。如果 $o_t$ 未被遮挡，则直接抓取；否则，需要识别出访问 $o_t$ 所需的最小动作序列，该序列始于识别所有“顶层遮挡物”——即那些遮挡 $o_t$ 但自身可被访问（即未被遮挡）的物体。\n\n![方法框架](https://arxiv.org/html/2511.23186v1/x3.png)\n> **图3**：UNOGrasp的训练与推理流程。模型经过监督微调（SFT）学习结构化的遮挡路径推理，并通过基于GRPO的强化微调（RFT）利用结果驱动的IoU和格式奖励进一步提升推理能力。推理时，给定RGB图像和语言指令，模型通过多步推理（`<reasoning>` 轨迹）输出清除遮挡并抓取目标所需的动作序列（`<answer>`）。\n\n**核心模块与技术细节**：\n1.  **目标中心遮挡图**：模型首先根据指令 $q$ 在视觉场景中定位目标物体 $o_t$。随后，构建一个以 $o_t$ 为中心的遮挡有向图 $G_t = (V_t, E_t)$。节点集 $V_t$ 包含 $o_t$ 及其所有直接或间接遮挡它的物体。有向边 $(o_i, o_j) \\in E_t$ 表示从相机视角看，物体 $o_i$ 被物体 $o_j$ 遮挡。边从被遮挡物体指向遮挡物，形成一条或多条源自 $o_t$、终止于可访问的顶层遮挡物的遮挡路径。目标物体 $o_t$ 的祖先对象集 $\\mathcal{A}(o_t)$ 定义为图中所有存在从 $o_t$ 到该对象的路径的物体。顶层遮挡物集 $\\mathcal{F}(o_t)$ 则是 $\\mathcal{A}(o_t)$ 中那些自身没有出边（即未被遮挡）的物体。模型的推理目标是准确推断出 $\\mathcal{F}(o_t)$，以指导机器人决定下一步的必要动作。\n\n2.  **两阶段训练流程**：\n    *   **监督微调（SFT）**：在UNOBench数据集上微调模型，使其能够解释自由形式的语言指令并将其与视觉场景中的唯一目标物体关联。训练监督使用两种显式参考方法：物体的名称及其图像坐标，以及Set-of-Mark（SoM）视觉提示（为每个物体分配唯一ID）。微调引导模型识别三种情况：(i) $o_t$ 未被遮挡；(ii) $o_t$ 有单条遮挡路径；(iii) $o_t$ 有多条遮挡路径。模型生成逐步推理链，每一步都锚定在物理上相邻（接触）的邻居物体上，并鼓励量化遮挡水平作为辅助信号，以加强重建完整遮挡路径和识别顶层遮挡物 $\\mathcal{F}(o_t)$ 的能力。\n    *   **强化微调（RFT）**：从SFT初始化的模型开始，采用分组相对策略优化（GRPO）进行RFT，以增强其基于视觉的推理能力。定义了一个新颖的任务特定奖励 $r = \\lambda_{\\text{fmt}} r_{\\text{fmt}} + \\lambda_{\\text{task}} r_{\\text{task}}$。其中，格式奖励 $r_{\\text{fmt}}$ 是二元的（1或0），用于促进推理 `<reasoning>` 和动作 `<answer>` 上下文的正确存在和闭合的结构有效性。任务奖励 $r_{\\text{task}}$ 使用集合级别的交并比（IoU）度量来监督最终输出的顶层遮挡物集 $\\mathcal{F}(o_t)$，即预测集与真实集的交集大小除以并集大小。这种IoU奖励提供了比二元正确性更平滑的优化信号，奖励部分正确的预测以实现更稳定的学习。\n\n**创新点**：与现有方法（如构建覆盖所有物体对的复杂场景级图）不同，UNOGrasp的创新在于：1) 将语言锚定到目标物体，然后构建一个**仅以该目标为根**的紧凑遮挡图，使推理专注于决定目标可访问性的结构；2) 提出了结合**遮挡感知视觉线索**（如遮挡率）和图基配方的新型训练方法；3) 引入了用于训练和评估遮挡推理的**首个大规模语言标注基准UNOBench**。\n\n## 实验与结果\n**实验设置**：\n*   **数据集/基准**：UNOBench，基于MetaGraspNetV2构建，包含合成场景（6,255个场景，108,174条推理路径）和真实场景（520个场景，2,552条遮挡路径）。数据集提供人类标注的自由形式语言指令和每个箱子的遮挡图。\n*   **评估协议**：将基准按难度分为四级（无遮挡、简单、中等、困难），并引入两种评估指标：1) **结果级指标**：成功率精度（SR-P）、召回率（SR-R）和F1分数，衡量最终动作输出的准确性；2) **推理级指标**：对象级推理（使用对象三元组精度OP、召回率OR和F1分数）和路径级推理（使用新提出的多路径归一化编辑距离MP_NED，衡量预测与真实推理路径的结构对齐程度，值越低越好）。\n*   **对比方法**：以专有模型Gemini Robotics-ER 1.5和开源模型Qwen2.5-VL-3B作为基线，比较了它们的零样本、上下文学习（ICL）和监督微调（SFT）变体。UNOGrasp基于Qwen2.5-VL-3B，并进行了SFT和RFT。\n\n**关键实验结果**：\n\n![合成场景路径级推理结果](https://arxiv.org/html/2511.23186v1/x1.png)\n> **表1**：UNOGrasp在合成测试集上的路径级推理结果。在“Oracle（带SoM）”和“自然语言提示”两种设置下，UNOGrasp在几乎所有难度级别上的SR-F1和MP_NED指标均显著优于基线模型。特别是在困难场景下，UNOGrasp的SR-F1（54.5%）远超Qwen2.5-VL (SFT)的34.3%和Gemini Robotics-ER 1.5 (ICL)的13.8%，且MP_NED（0.51）远低于基线（>0.68），表明其推理路径与真实路径结构更接近。\n\n![合成场景对象级推理结果](https://arxiv.org/html/2511.23186v1/x2.png)\n> **表2**：UNOGrasp在合成测试集上的对象级推理结果（F1_rel）。在自然语言提示设置下，UNOGrasp的整体F1_rel达到57.2%，而Gemini和Qwen的ICL变体分别仅为3.5%和2.7%，显示出UNOGrasp在将多步推理与物体坐标进行空间对齐方面的巨大优势。\n\n![真实场景路径级推理结果](https://arxiv.org/html/2511.23186v1/x9.png)\n> **表3**：UNOGrasp在真实场景测试集上的路径级推理结果。即使在训练中未见的真实场景中，UNOGrasp依然保持领先。在困难场景下，其SR-F1（39.7%）远超Qwen2.5-VL (SFT)的1.7%和Gemini (ICL)的14.6%。\n\n![真实场景对象级推理结果](https://arxiv.org/html/2511.23186v1/x10.png)\n> **表4**：UNOGrasp在真实场景测试集上的对象级推理结果。趋势与合成场景一致，UNOGrasp在自然语言提示下的对象级推理能力（F1_rel 24.8%）远优于基线（均低于4.0%）。\n\n**消融实验与组件贡献**：\n论文通过消融实验验证了各训练组件的贡献。结果表明：\n1.  **RFT奖励的有效性**：仅使用SFT训练的模型，其推理路径质量（MP_NED）和最终动作准确性（SR-F1）均低于完整版UNOGrasp（SFT+RFT）。加入基于IoU的任务奖励 $r_{\\text{task}}$ 和格式奖励 $r_{\\text{fmt}}$ 的RFT阶段，能显著提升这两方面的性能。\n2.  **推理监督的重要性**：与仅进行SFT但不包含 `<reasoning>` 部分训练的Qwen2.5-VL (SFT)相比，UNOGrasp在复杂（困难）场景下的性能提升尤为明显（合成场景SR-F1提升+20.2%，真实场景提升+38.0%），证实了过程级（推理链）监督对于多路径推理至关重要。\n3.  **模型局限性**：基线模型（如Gemini和Qwen的ICL变体）在无遮挡场景下容易产生“幻觉”，错误地认为存在遮挡（成功率较低），且其多步推理的空间对齐经常失败，导致身份混淆和高MP_NED（>0.8）。上下文学习（ICL）虽能提升某些情况下的推理性能，但往往会放大无遮挡情况下的幻觉。\n\n## 总结与启发\n**核心贡献**：\n1.  首次对挑战性杂乱场景中机器人抓取的空间遮挡推理进行了深入研究。\n2.  引入了首个用于训练和测试遮挡推理的大规模基准UNOBench，提供了评估协议和量化推理准确性的指标。\n3.  提出了UNOGrasp模型，采用一种新颖的基于图的训练方法，结合遮挡感知视觉线索，有效提升了遮挡推理能力，并在合成与真实环境中均取得了最先进的性能。\n\n**论文提到的局限性**：本文的行动计划仅基于遮挡关系的存在性。不同的遮挡可能会以不同的方式阻碍抓取，因此量化遮挡严重程度极具挑战性且依赖于具体应用，这超出了本文的讨论范围。\n\n**对后续研究的启示**：\n1.  **遮挡严重度建模**：未来的工作可以探索如何量化遮挡的严重程度，并将其纳入推理和规划中，以生成更精细、更高效的动作序列。\n2.  **泛化与可扩展性**：UNOGrasp在未见过的真实场景中表现良好，但进一步测试其在更复杂、动态或非结构化环境中的泛化能力是一个重要方向。\n3.  **多模态融合**：当前主要利用RGB图像进行推理，深度图仅用于抓取点估计。如何更早、更深入地融合深度等3D信息以增强空间关系理解，值得探索。\n4.  **开源促进研究**：与专有模型不同，本文承诺公开数据、模型和代码，这将为遮挡推理领域的后续研究提供宝贵的基础资源。",
      "imageUrls": [
        "https://arxiv.org/html/2511.23186v1/x1.png",
        "https://arxiv.org/html/2511.23186v1/x2.png",
        "https://arxiv.org/html/2511.23186v1/x3.png",
        "https://arxiv.org/html/2511.23186v1/x9.png",
        "https://arxiv.org/html/2511.23186v1/x10.png",
        "https://arxiv.org/html/2511.23186v1/x11.png",
        "https://arxiv.org/html/2511.23186v1/x12.png",
        "https://arxiv.org/html/2511.23186v1/x13.png",
        "https://arxiv.org/html/2511.23186v1/x18.png",
        "https://arxiv.org/html/2511.23186v1/x19.png",
        "https://arxiv.org/html/2511.23186v1/x21.png",
        "https://arxiv.org/html/2511.23186v1/x22.png",
        "https://arxiv.org/html/2511.23186v1/x23.png",
        "https://arxiv.org/html/2511.23186v1/x24.png",
        "https://arxiv.org/html/2511.23186v1/x25.png",
        "https://arxiv.org/html/2511.23186v1/x26.png",
        "https://arxiv.org/html/2511.23186v1/x27.png",
        "https://arxiv.org/html/2511.23186v1/x28.png",
        "https://arxiv.org/html/2511.23186v1/x29.png",
        "https://arxiv.org/html/2511.23186v1/main/figures/radar_chart_v2.png",
        "https://arxiv.org/html/2511.23186v1/x30.png",
        "https://arxiv.org/html/2511.23186v1/x31.png",
        "https://arxiv.org/html/2511.23186v1/x37.png",
        "https://arxiv.org/html/2511.23186v1/x38.png",
        "https://arxiv.org/html/2511.23186v1/x39.png",
        "https://arxiv.org/html/2511.23186v1/x40.png",
        "https://arxiv.org/html/2511.23186v1/x41.png",
        "https://arxiv.org/html/2511.23186v1/x42.png",
        "https://arxiv.org/html/2511.23186v1/x47.png",
        "https://arxiv.org/html/2511.23186v1/x48.png",
        "https://arxiv.org/html/2511.23186v1/x49.png",
        "https://arxiv.org/html/2511.23186v1/x50.png",
        "https://arxiv.org/html/2511.23186v1/x51.png",
        "https://arxiv.org/html/2511.23186v1/x52.png",
        "https://arxiv.org/html/2511.23186v1/x57.png",
        "https://arxiv.org/html/2511.23186v1/x58.png",
        "https://arxiv.org/html/2511.23186v1/x59.png",
        "https://arxiv.org/html/2511.23186v1/x60.png",
        "https://arxiv.org/html/2511.23186v1/x61.png",
        "https://arxiv.org/html/2511.23186v1/x62.png",
        "https://arxiv.org/html/2511.23186v1/x67.png",
        "https://arxiv.org/html/2511.23186v1/x68.png",
        "https://arxiv.org/html/2511.23186v1/x69.png",
        "https://arxiv.org/html/2511.23186v1/x70.png",
        "https://arxiv.org/html/2511.23186v1/x71.png",
        "https://arxiv.org/html/2511.23186v1/main/figures/object-real-min.jpg"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2511.23407",
      "title": "From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products",
      "url": "http://arxiv.org/abs/2511.23407",
      "arxivId": "2511.23407",
      "date": "2025-11-28",
      "authors": "Jürgen Fleischer Team",
      "category": "Manipulation",
      "summary": "本文针对报废产品拆卸中产品状态不确定性问题，提出将拆卸建模为部分可观察马尔可夫决策过程（POMDP），以处理因磨损、腐蚀等导致的模型偏差。关键技术包括从CAD数据自动生成POMDP模型的框架，以及基于强化学习和贝叶斯滤波的近似规划方法。实验在三个产品和两个机器人系统上验证，该概率规划框架在平均拆卸时间和方差上优于确定性基线，能泛化到不同机器人设置并成功适应零件缺失或卡住等偏差。",
      "detailedSummary": "## 研究背景与动机\n当前机器人拆解序列规划的主流方法通常假设产品模型是确定且完全可观测的，即产品状态与原始CAD设计完全一致。然而，现实中的报废产品常因磨损、腐蚀、损坏或未记录的维修而严重偏离其原始设计，导致基于确定性模型的规划器在遇到意外情况（如螺丝锈死）时可能失败或效率低下。现有研究虽在工厂控制层面考虑了不确定性，但尚未在单个产品和操作层面探索部分可观测性问题。\n\n本文针对报废产品内部状态不确定这一具体痛点，提出了将拆解规划重新表述为部分可观测马尔可夫决策过程的新视角。POMDP能够自然地将影响拆解过程的不确定结构或物理属性（如零件是否锈死）建模为隐藏变量。核心思路是：从CAD数据、机器人能力和先验检查结果自动推导出具体的POMDP模型，并通过强化学习近似求解获得策略，同时利用贝叶斯滤波器在执行过程中持续更新对隐藏报废条件的信念。\n\n## 方法详解\n整体框架（Task and Motion Planning, TAMP）将拆解分解为基于POMDP的符号任务规划器和生成可执行轨迹的运动规划器。输入包括产品CAD模型、机器人能力定义和先验检查数据；输出是具体的拆解动作序列。系统通过“能力”和“观察器”抽象机器人的工具和传感器。\n\n![系统框架](https://arxiv.org/html/2511.23407v1/x2.png)\n\n> **图2**：系统整体框架。蓝色为感知部分（观察器），黄色为规划部分（POMDP），红色为执行部分（运动控制器）。系统结合了符号任务规划与运动规划。\n\n核心模块是POMDP，其各组件定义如下：\n1.  **状态空间 S**：状态表示产品结构，核心是零件间的拆卸关系矩阵Ψ。其中每个元素ψ_ij(θ, φ)被重新解释为零件i相对于j在球坐标方向(θ, φ)上可移动的**概率**（而非确定性的0/1值）。报废条件（如腐蚀、粘合）被视为在产品生命周期中隐式应用的、会修改名义拆卸关系ψ^0_ij的算子f_c。已拆卸零件用ψ_ij(θ, φ)=1 ∀j表示，以保持状态维度固定。\n\n![拆卸关系示例](https://arxiv.org/html/2511.23407v1/x3.png)\n\n> **图3**：角磨机头部(1)与机身(2)之间的拆卸关系ψ_12(θ, φ)曲面图。绿色区域表示头部仅能沿一个方向被拉出（ψ_12(θ, φ)=1）。\n\n2.  **动作空间 A**：包含非破坏性动作（如操纵、拧螺丝）和破坏性动作（如铣削）。非破坏性动作a_k(i)表示使用工具k拆卸零件i。破坏性动作被建模为直接修改零件i对所有其他零件j的拆卸关系ψ_ij(θ, φ)的算子，从而创建新的可行拆卸方向。所有动作都需通过几何符号约束（前提函数）和运动层可行性检查。\n3.  **转移函数 T**：定义了执行动作后的状态转移概率。对于非破坏性动作，成功拆卸零件i的概率T(Ψ^(i)|Ψ, a_k(i)) = p_success * [max_(θ,φ) ∏_(j=1)^n ψ_ij(θ, φ) > 0]，其中p_success是工具k的运动策略π_k成功执行动作的概率。破坏性动作（如铣削）的转移是确定性的。\n\n![转移函数示例](https://arxiv.org/html/2511.23407v1/x4.png)\n\n> **图4**：以铆钉为例，说明转移函数T如何改变Ψ。初始时铆钉和顶部均无法拆卸（所有方向乘积为0，红色）。应用铣削动作a_mill后，铆钉与顶部间的拆卸关系出现了新的可行方向（绿色），使得后续可用操纵动作a_manip移除任一部分。\n\n4.  **观察函数 O**：分为依赖于动作的反馈观察（如尝试拆卸后判断零件是否被卡住）和不依赖于动作的直接状态观察（如用相机检测螺丝是否生锈）。观察将传感器数据映射到对符号状态的估计。\n5.  **奖励函数 R**：目标是快速拆除预定的一组有价值零件。奖励为负的时间成本，包括执行动作、更换末端执行器、放置零件的时间，并附加零件价值惩罚项r_value,i以阻止盲目破坏。\n\n**创新点**在于：1) 首次为拆解规划提供了显式的POMDP公式，将不确定的报废条件作为隐藏变量；2) 提出了一个从CAD数据自动生成具体POMDP实例的系统化流程；3) 在统一的符号框架下集成了非破坏性和破坏性动作，以处理不确定性导致的失败。\n\n## 实验与结果\n**实验设置**：使用了三个产品（角磨机、双盖电机、单盖电机，如图1所示）和两个具有不同能力的机器人系统（系统A支持操纵和拧螺丝；系统B额外支持铣削，如图5所示）。平台基于仿真进行验证。\n\n![实验产品](https://arxiv.org/html/2511.23407v1/x1.png)\n\n> **图1**：用于系统验证的三个产品。(a)角磨机，(b)双盖电机，(c)单盖电机。\n\n![机器人系统](https://arxiv.org/html/2511.23407v1/x5.png)\n\n> **图5**：实验使用的两个机器人系统及其各自的能力。\n\n**对比基线**：与确定性规划器（假设产品处于名义CAD状态）进行对比。\n**关键结果**：\n1.  **性能对比**：在模拟的报废条件（零件缺失或卡住）下，POMDP规划器在所有三种产品上的平均拆解时间均优于确定性基线。对于角磨机，平均时间减少37.1%，方差减少28.8%；对于双盖电机，平均时间减少16.9%，方差减少84.2%；对于单盖电机，平均时间减少9.1%，方差减少68.4%。\n2.  **泛化能力**：系统成功适配到两个不同的机器人系统（A和B），并完成了两个不同产品的拆解，证明了框架对不同机器人能力的泛化能力。\n3.  **适应性**：系统能够成功适应与CAD模型的偏差，例如选择性地处理缺失或卡住的零件。\n\n![结果对比图](https://arxiv.org/html/2511.23407v1/x7.png)\n\n> **图7**：三种产品在模拟报废条件下，POMDP规划器与确定性规划器的平均拆解时间及方差对比。POMDP方法在时间和稳定性上均更优。\n\n![泛化结果](https://arxiv.org/html/2511.23407v1/x8.png)\n\n> **图8**：系统在两个不同机器人系统（A和B）上成功拆解两个不同产品的演示，展示了框架的泛化能力。\n\n**观察器实现**：系统共享一个基于深度相机的观察器，通过点云匹配来定位组件并估计零件姿态，进而推导出“零件缺失”或“零件卡住”等符号观察。\n\n![状态估计](https://arxiv.org/html/2511.23407v1/x6.png)\n\n> **图6**：角磨机和电机的状态估计过程示例。通过比对和配准真实点云（蓝）与模拟点云（黄/绿），来判定零件缺失（红）或卡住（右图）。\n\n**符号控制器**：为获得可行策略，通过用检查先验对隐藏的报废条件进行边缘化，将POMDP近似为一个期望MDP，并训练Q学习智能体。在执行时，使用贝叶斯滤波器根据新观察更新信念，并基于最可能的状态采取行动。论文指出，此设计牺牲了显式的知识寻求行为，但由于动作反馈能立即揭示隐藏状态，且存在备用恢复方案，这已被认为可接受。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个将机器人拆解报废产品规划形式化为部分可观测马尔可夫决策过程的通用框架，显式地将不确定的报废条件建模为隐藏变量。\n2.  开发了一个能够从CAD数据、机器人能力和先验检查数据自动生成具体POMDP模型并求解实例特定拆解策略的系统。\n3.  通过仿真实验在三个产品和两个机器人系统上验证了该概率规划框架在平均拆解时间、方差、泛化性和适应性方面优于确定性基线方法。\n\n**局限性**：\n1.  采用的符号控制器（基于最可能状态的Q学习）没有显式的知识寻求行为，不会单纯为了减少不确定性而选择探索性动作。\n2.  对观察函数做出了理想化假设（如p_TP=1, p_FP=0），忽略了实际传感器（如相机）可能因光照等因素产生的假阴性。\n3.  目前主要考虑了零件“卡住”这一种报废条件。\n\n**启示**：\n1.  为处理产品不确定性的拆解规划提供了一个坚实且可扩展的概率建模基础。更复杂的、依赖于信念的控制器（如标准POMDP求解器）可直接替代当前的Q学习智能体，有望带来进一步的性能提升。\n2.  展示了从CAD到可执行策略的自动化流程的可行性，为机器人自主适应真实世界复杂性和可变性指明了方向。\n3.  观察函数的理想化假设指出了未来工作的一个方向：集成更鲁棒、能处理传感器噪声和不确定性的感知算法。",
      "imageUrls": [
        "https://arxiv.org/html/2511.23407v1/x1.png",
        "https://arxiv.org/html/2511.23407v1/x2.png",
        "https://arxiv.org/html/2511.23407v1/x3.png",
        "https://arxiv.org/html/2511.23407v1/x4.png",
        "https://arxiv.org/html/2511.23407v1/x5.png",
        "https://arxiv.org/html/2511.23407v1/x6.png",
        "https://arxiv.org/html/2511.23407v1/x7.png",
        "https://arxiv.org/html/2511.23407v1/x8.png",
        "https://arxiv.org/html/2511.23407v1/x9.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2511.23034",
      "title": "LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models",
      "url": "http://arxiv.org/abs/2511.23034",
      "arxivId": "2511.23034",
      "date": "2025-11-28",
      "authors": "Jianlong Fu Team",
      "category": "Manipulation",
      "summary": "本文提出LatBot框架，解决现有潜在动作模型因忽视物理先验而泛化性能受限的问题。方法核心为通用潜在动作学习：以任务指令和多帧图像为输入，同时优化未来帧重建与动作序列预测，并引入动作预测（如夹爪轨迹与朝向）以学习物理先验。技术关键是将潜在动作分解为运动令牌与场景令牌，以区分机器人主动运动与环境变化。实验表明，该方法在仿真与真实机器人任务中均表现优异，仅需每个任务10条真实轨迹即可完成全部五项挑战性任务，证明了强大的少样本迁移能力。",
      "detailedSummary": "## 研究背景与动机\n当前，从大规模物体操作视频中学习可迁移的隐动作是增强机器人任务泛化能力的有前景方向，因为此类表示与不同的机器人具身无关。现有方法主要依赖视觉重建目标，而忽略了物理先验，导致学习通用表示的性能欠佳。具体而言，现有隐动作模型面临三个关键挑战：第一，缺乏任务指令指导，导致隐动作无法捕捉任务相关的变化；第二，对多帧信息的利用不足，导致隐动作表示不精确，无法准确捕捉运动动态；第三，隐动作往往只关注视觉外观变化而缺乏物理感知，在隐动作表示与真实可执行动作之间产生了语义鸿沟。\n\n本文针对上述痛点，提出了一种新的视角：在任务指令和多帧输入的指导下学习隐动作，并同时通过未来帧重建和动作序列预测两个目标进行优化。本文的核心思路是：将隐动作解耦为可学习的运动令牌和场景令牌以区分机器人主动运动与环境被动变化，并通过一个统一解码器联合优化视觉与动作生成，最后通过知识蒸馏将学到的物理感知隐动作迁移到最新的视觉-语言-动作模型中。\n\n## 方法详解\n本文提出的通用隐动作学习框架LatBot包含两个关键组成部分：解耦的隐动作表示和统一解码器。整体流程是先预训练隐动作模型，然后通过知识蒸馏将学到的动作知识迁移到VLA模型中，最后通过动作专家微调生成可执行动作。\n\n![不同范式对比](https://arxiv.org/html/2511.23034v1/x1.png)\n> **图1**：隐动作建模的不同范式对比。现有方法通常忽略将机器人动作与环境变化解耦。相比之下，我们学习解耦的表示，并将隐动作解码为未来视觉帧V_{t+k}和物理动作A_{t:t+k}，从而为下游任务实现更精确和可迁移的控制。\n\n**核心模块一：解耦的隐动作表示**\n为了克服现有方法将机器人引发的运动和环境引发的变化纠缠在单一表示中的问题，本文提出将隐动作Z_a分解为两个部分：运动表示Z_mot（捕捉机器人自身运动驱动的主动变化）和场景表示Z_sce（捕捉环境动态引起的被动场景变化）。这种分解减少了任务无关的噪声，并在机器人运动、环境变化和隐动作表示之间建立了更清晰的对应关系。具体实现时，利用预训练的视觉语言模型作为编码器，并引入两个可学习的隐动作令牌 `[CP_SCE]` 和 `[CP_MOT]`，在任务指令ℓ和多帧视觉输入V_{t:t+k}的指导下，分别生成结构化的场景表示和运动表示。\n\n**核心模块二：统一隐动作解码**\n为确保隐动作关注多帧间的动态变化，本文设计了一个统一解码器，以隐动作作为条件输入，联合指导未来帧重建和帧间动作生成。视觉重建约束鼓励隐动作捕捉可观察的场景变化，而动作生成目标则提供物理层面的指导，使模型能在隐动作与物理运动之间建立更紧密的联系。解码器基于预训练的SANA图像生成模型初始化，其关键创新在于引入了场景与运动表示之间的层间双向交互机制。在解码器的每一层，场景和运动表示进行信息交换与融合，使得场景动态可以指导动作生成，同时运动令牌可以细化视觉重建，实现两种模态的相互增强。最终，基于融合后的特征解码出未来视觉帧V_{t+k}和帧间动作序列A_{t:t+k}。\n\n![方法框架](https://arxiv.org/html/2511.23034v1/x2.png)\n> **图2**：提出的用于VLA模型的隐动作蒸馏方法示意图。通过使用隐动作对齐损失和推理保留损失优化VLM，我们从机器人和人手演示视频中蒸馏出可泛化的动作表示，同时保持子任务规划能力。随后通过一个动作专家模块进行连续动作预测。\n\n**核心模块三：面向VLA模型的知识蒸馏**\n尽管隐动作模型能学习物理基础的隐动作表示，但其能力仅限于场景重建和帧间动作生成。为了将学到的知识迁移到VLA模型，本文提出了隐动作知识蒸馏策略。给定预训练的隐动作模型（教师）和VLA中的VLM（学生），设计了两类损失函数：\n1.  **隐动作对齐损失L_a**：该损失结合了重建项（MSE）和分布对齐项（KL散度），旨在将教师模型学到的物理先验和未来帧预测能力对齐到学生的隐动作表示中。\n2.  **推理保留损失L_r**：这是一个基于下一个令牌预测的自回归目标，指导学生模型基于当前帧和任务指令生成子任务描述，以保留VLM原有的语言理解和推理能力。\n整体蒸馏目标为L = L_a + λ_r * L_r，其中λ_r用于平衡两者。蒸馏完成后，模型还需进行**动作专家微调**，通过附加一个动作专家模块，并使用末端执行器位姿损失（MSE）和夹爪状态损失（二元交叉熵）进行监督，最终生成精确的可执行机器人动作。\n\n与现有方法相比，本文的创新点具体体现在：1）明确提出了解耦的隐动作表示（运动/场景令牌）；2）设计了具有双向交互机制的统一解码器进行联合优化；3）提出了结合隐动作对齐与推理保留的知识蒸馏策略，将物理感知的隐动作有效迁移到VLA模型中。\n\n## 实验与结果\n**实验设置**：隐动作模型在包含机器人（OXE, AgiBoT）和人类手部操作（EgoDex）的混合数据集上进行预训练，总计一百万视频片段。知识蒸馏使用相同数据集。评估在三个平台上进行：SIMPLER（模拟，Google Robot和WidowX Robot）、LIBERO（模拟，四个任务套件）以及真实世界的Franka机器人。\n\n**对比基线**：包括RT-2-X、OpenVLA、π_0、SpatialVLA、RoboVLM、villa-X、DD-VLA、MemoryVLA、CogACT、UniVLA、π_0.5、Diffusion Policy、Octo、TraceVLA、RDT等众多最新VLA模型。\n\n**关键实验结果**：\n1.  **SIMPLER Benchmark (Google Robot)**：如表1所示，在Visual Matching和Variant Aggregation两种设置下，本文方法平均成功率分别为78.0%和70.1%，均达到最佳性能，显著优于π_0（分别提升25.3%和24.1%）。\n![表1结果](https://arxiv.org/html/2511.23034v1/x4.png)\n> **表1**：在Google Robot的SIMPLER两个设置下，不同VLA模型在四个任务上的成功率对比。我们的方法（灰色行）在平均成功率上领先。\n\n2.  **SIMPLER Benchmark (WidowX Robot)**：如表2所示，本文方法取得了87.5%的平均成功率，大幅领先所有基线，相比π_0.5提升32.3%，相比其他隐动作方法UniVLA和villa-X分别提升39.6%和46.7%。\n![表2结果](https://arxiv.org/html/2511.23034v1/x4.png)\n> **表2**：在WidowX Robot的SIMPLER (Visual Matching) 设置下，不同VLA模型在四个任务上的成功率对比。我们的方法（灰色行）优势明显。\n\n3.  **LIBERO Benchmark**：如表3所示，本文方法在四个任务套件上取得了98.0%的平均成功率，全面领先。尤其在最具挑战性的LIBERO-Long上达到95.4%，相比π_0.5提升3.0%，证明了方法在长视野任务上的优势。\n![表3结果](https://arxiv.org/html/2511.23034v1/x4.png)\n> **表3**：在四个LIBERO模拟环境上，不同VLA模型的任务成功率对比。我们的方法（灰色行）取得了最高的平均成功率。\n\n4.  **真实世界评估 (Franka Robot)**：如表4所示，在五个需要精细操作的真实任务中，仅使用10条演示轨迹进行训练，本文方法成功完成了所有任务，平均成功率达63.3%，而π_0和π_0.5在相同设置下平均成功率仅为12.7%和20.7%，证明了强大的少样本迁移能力。\n![真实机器人设置](https://arxiv.org/html/2511.23034v1/x3.png)\n> **图3**：真实世界Franka机器人实验设置，配备多视角观测。任务包括拾取、插入等，需要平移和旋转运动。\n![表4结果](https://arxiv.org/html/2511.23034v1/x4.png)\n> **表4**：在五个真实世界任务中，使用不同数量演示轨迹训练时的成功率对比。我们的方法（灰色行）在少样本（10条）设置下表现卓越。\n\n5.  **消融实验**：如表5所示，逐步添加统一动作解码器（UAD）和解耦隐动作表示（DLA）组件，性能持续提升。完整模型（Ours-v3）相比仅使用UniVLA风格的方法，平均成功率从51.0%大幅提升至87.5%，证明了每个组件的有效性。\n![表5结果](https://arxiv.org/html/2511.23034v1/x4.png)\n> **表5**：在SIMPLER基准上评估各组件影响的消融实验结果。UAD和DLA分别代表统一动作解码器和解耦隐动作表示。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个通用的隐动作学习框架，通过将隐动作解耦为运动令牌和场景令牌，显式区分了机器人主动运动与环境被动变化。\n2.  设计了一个具有双向交互机制的统一解码器，通过联合优化未来帧重建和动作序列预测，使学到的隐动作同时具备视觉动态感知和物理先验。\n3.  提出了一种有效的知识蒸馏策略，通过隐动作对齐损失和推理保留损失，将预训练隐动作模型中的物理感知知识迁移到VLA模型中，显著提升了其在少样本设置下的泛化与迁移能力。\n\n**局限性**：论文提到，隐动作模型本身的能力有限，主要限于场景重建和帧间动作生成，需要后续的知识蒸馏和动作专家微调才能用于机器人控制。\n\n**对后续研究的启示**：本文工作表明，从大规模未标注视频中蒸馏物理基础的、解耦的隐动作表示，是增强VLA模型动作感知和少样本泛化能力的有效途径。这为利用更丰富的非机器人视频数据（如人类操作视频）来训练机器人策略开辟了新方向。同时，所提出的解耦与联合优化框架也可为其他需要学习具身无关、可迁移表示的研究提供借鉴。",
      "imageUrls": [
        "https://arxiv.org/html/2511.23034v1/x1.png",
        "https://arxiv.org/html/2511.23034v1/x2.png",
        "https://arxiv.org/html/2511.23034v1/x3.png",
        "https://arxiv.org/html/2511.23034v1/x4.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2511.22963",
      "title": "Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary",
      "url": "http://arxiv.org/abs/2511.22963",
      "arxivId": "2511.22963",
      "date": "2025-11-28",
      "authors": "Jingya Wang Team",
      "category": "Manipulation",
      "summary": "本文针对仿人机器人难以根据自由形式语言指令执行全身动作的核心问题，提出Humanoid-LLA大型语言动作模型。方法包括：统一运动词汇表对齐人类与机器人运动基元到共享离散空间；词汇指导控制器从特权策略蒸馏确保物理可行性；物理感知强化学习微调提升鲁棒性。实验在仿真和真实Unitree G1机器人上验证，该模型在保持高物理保真度的同时，实现了强语言泛化，在运动自然性、稳定性和执行成功率方面优于现有语言条件控制器。",
      "detailedSummary": "## 研究背景与动机\n当前，让人形机器人遵循自由形式的语言指令是实现无缝人机交互和通用具身智能的关键。现有方法主要依赖于运动模仿框架：从大规模人体运动-文本数据集中学习“文本到人体运动”的映射，然后通过重定向技术投影到机器人上。然而，这种方法存在关键局限性：重定向过程在人体运动空间进行优化，引入了系统性的投影和运动学不匹配误差，牺牲了机器人执行的精确性。两阶段的系统虽然添加了基于物理的跟踪控制器进行后处理校正，提高了可行性，但无法从端到端完全恢复细粒度的、受语言约束的准确性。端到端的方法将人体数据集转换为类人数据集，但离线策略往往无法处理现实世界的随机性和扰动，导致在硬件上产生脆弱、不精确的行为。蒸馏框架虽然实现了强大的物理保真度，但将语义和控制压缩到单个VAE中，往往会削弱语言基础并模糊动作选择。所有这些范式的共同瓶颈在于：缺乏高质量、多样化且物理基础扎实的类人机器人真实数据，这限制了精确的语言-机器人对齐。\n\n本文针对数据稀缺和语义-物理鸿沟这一具体痛点，提出了一个新视角：将语言条件化的类人机器人全身控制重新表述为一个利用统一的人-类人运动词汇的动作生成问题。本文的核心思路是：首先构建一个通过联合量化配对的人体运动及其重定向的类人对应物而得到的统一运动词汇；然后通过词汇导向的动作蒸馏过程，将特权教师跟踪策略蒸馏成以离散运动令牌为条件的控制器；最后，通过基于人类数据的监督微调和基于类人机器人物理反馈的强化学习微调，训练一个大型语言动作模型（LLA），实现从复杂自然语言到物理上可行的类人动作的直接映射。\n\n## 方法详解\nHumanoid-LLA的整体框架由三个紧密相连的组件构成：构建统一的人-类人运动词汇、蒸馏词汇导向的策略以及微调大型语言动作模型。前两个组件是第三个组件实现集成推理的必要前提。\n\n![方法总览](https://arxiv.org/html/2511.22963v1/x2.png)\n> **图2**：Humanoid-LLA方法总览。第一阶段，利用大规模配对的人体和类人运动数据集构建统一运动词汇。给定一个运动学类人运动目标及其对应的词汇检索，我们将教师跟踪控制器蒸馏为词汇导向的类人学生控制器。前两个阶段使第三阶段能够直接从物理仿真中获取各种类人反馈，而无需解码，从而使我们的LLA具备了高物理保真度和语言泛化能力。\n\n**1. 统一的人-类人运动词汇**\n该方法旨在学习一个统一的标记器，将人体和重定向的类人运动映射到相同的离散词汇中，确保相同的令牌在两种模态间具有一致的语义。为此，采用带有隐式分区的VQ-VAE，每个潜在向量被分割成子块，并由独立的码本量化，从而在不需单个超大码本的情况下获得大的有效词汇量。除了每种模态内的标准自重建损失外，还额外强制执行跨模态重建约束（如人体到类人、类人到人体），使得从任一模态获得的令牌都能解码为相同的运动基元。这确保了相同的令牌对应于等效的人体和类人运动，从而建立语义统一的运动表示。训练目标定义为：$\\mathcal{L}=\\mathcal{L}_{\\text{intra}}+\\alpha\\mathcal{L}_{\\text{commit}}+\\beta\\mathcal{L}_{\\text{cross}}$，其中$\\mathcal{L}_{\\text{intra}}$是人体和类人运动的模态内重建损失，$\\mathcal{L}_{\\text{cross}}$惩罚跨模态重建的差异，$\\mathcal{L}_{\\text{commit}}$是承诺损失。\n\n**2. 词汇导向的类人动作蒸馏**\n在获得统一运动词汇后，通过词汇导向的蒸馏过程弥合运动学运动基元与物理控制之间的鸿沟。首先训练一个特权教师策略$\\pi^{\\text{track}}$，以高保真度跟踪连续的、重定向的类人参考运动。然后，将其行为蒸馏到一个词汇导向的学生策略$\\pi^{\\text{vocab}}$中，该策略依赖于运动令牌。学生策略被建模为一个条件变分自编码器，包含词汇先验$\\rho$、残差编码器$\\mathcal{E}$和动作解码器$\\mathcal{D}$。其训练目标是最小化参考动作与学生动作之间的差异，以及编码器分布$p_{\\mathcal{E}}$与先验分布$q_{\\rho}$之间的KL散度：$\\mathcal{L}_{\\pi^{\\text{vocab}}} = \\|a^{\\text{track}}_{t}-a_{t}^{\\text{vocab}}\\|_{2}^{2} + \\lambda_{\\text{KL}}\\text{KL}(p_{\\mathcal{E}} \\| q_{\\rho})$。这一阶段将控制输入从密集的参考轨迹转变为紧凑的运动令牌语言，使得类人机器人能够执行语言模型输出的令牌序列。\n\n**3. 大型语言动作模型（LLA）**\n基于前两个组件，训练一个端到端的LLA模型，将高度抽象的语言描述映射为物理上可执行的机器人动作。训练分为两个阶段：\n*   **基于增强人类数据的监督微调（SFT）**：为增强数据集中文本注释的表达力，采用视觉语言模型，以渲染的运动序列作为运动上下文，生成更准确的推理链。运动令牌生成被表述为一个自回归的、文本条件的语言建模任务，使用标准的下一令牌预测损失$\\mathcal{L}_{\\text{SFT}}$进行训练。\n*   **基于类人反馈的强化学习微调（RLFT）**：采用分组相对策略优化（GRPO），为一组候选输出分配标量奖励，并在组内归一化以获得相对优势，从而鼓励策略偏好优于平均水平的候选，无需显式的价值函数。奖励设计结合了格式奖励和物理保真度奖励。格式奖励确保输出遵循结构化模板（以`<reasoning>...</reasoning>`开始，后跟`<motion>...</motion>`，且运动令牌按循环子码本顺序出现）。物理保真度奖励则由分布项和跟踪项组成，鼓励词汇控制器生成的运动分布与物理可行轨迹的分布相匹配，并在语义上与配对的运动描述对齐，同时通过仿真器中的词汇导向控制器执行生成的令牌来评估物理可行性。\n\n## 实验与结果\n实验在仿真环境和真实世界的宇树G1（Unitree G1）人形机器人上进行。评估使用了大规模的人体运动-文本数据集（如BABEL）以及内部收集的类人运动数据。对比的基线方法包括：基于运动模仿和重定向的方法（如OmniH2O）、端到端语言条件控制方法（如ALMI、LangWBC）以及利用物理反馈微调LLM的方法（如RLPF）。\n\n![语言指令到动作的定性结果](https://arxiv.org/html/2511.22963v1/x3.png)\n> **图3**：Humanoid-LLA在仿真和真实宇树G1机器人上，将复杂自由形式语言指令映射为全身动作的定性结果。指令示例如“walk in a curving figure-eight”（以弯曲的8字形行走）、“walk with arms swinging exaggeratedly”（夸张摆臂行走）、“walk while waving with the right hand”（行走时挥动右手）等，展示了模型在理解抽象指令和生成多样化、物理可行动作方面的能力。\n\n关键实验结果如下：\n*   **语言泛化与运动多样性**：Humanoid-LLA能够响应广泛、抽象的自由形式语言指令（如“以弯曲的8字形行走”、“行走时夸张地摆动手臂”），并生成多样化且物理上合理的全身运动，而基线方法往往局限于预设的简单技能或生成运动范围有限。\n*   **物理保真度与成功率**：在仿真和真实机器人实验中，Humanoid-LLA在运动自然度、稳定性和执行成功率方面均优于现有语言条件控制器。具体而言，在涉及复杂轨迹和复合动作的指令上，其成功执行率显著高于对比方法。\n*   **消融实验**：消融研究表明，统一运动词汇的跨模态重建约束对于保证人-类人语义对齐至关重要；词汇导向的控制器蒸馏是实现从离散令牌到连续物理控制可靠映射的关键；而两阶段微调（SFT+RLFT）中，基于物理反馈的RLFT阶段对于注入动态级一致性、提升在真实扰动下的鲁棒性贡献最大。移除RLFT会导致运动在仿真中可行但转移到真实机器人时稳定性下降。\n\n## 总结与启发\n本文的核心贡献可概括为三点：1）提出了首个端到端的、能够实现自由形式文本到类人全身控制的大型语言动作模型（Humanoid-LLA）；2）引入了通过跨具身VQ-VAE标记化学到的统一人-类人运动词汇，为类人控制和LLM建模提供了共享的可学习潜在空间，缓解了数据稀缺和具身不匹配问题；3）设计了一个两阶段微调框架，通过基于人类数据的监督微调和基于类人物理反馈的强化学习微调，使模型同时具备了语义理解和物理推理能力。\n\n论文自身提到的局限性包括：当前方法依赖于高质量的人体运动-文本配对数据进行词汇构建和SFT，这在一定程度上受限于现有数据集的规模和多样性；此外，虽然RLFT注入了物理先验，但在极端动态或非结构化环境中的泛化能力仍有待进一步探索。\n\n本文对后续研究的启示在于：**统一离散表示与物理控制**的范式为连接高层语义与低层机器人控制提供了有效途径，可扩展至其他高自由度机器人平台。**利用仿真物理反馈进行闭环微调**的策略，为在缺乏大规模真实机器人数据的情况下训练鲁棒、可泛化的语言-动作模型指明了方向。未来工作可以探索更高效的分层词汇表示、结合多模态（如视觉）输入以处理更复杂的场景理解任务，以及开发更高效的sim-to-real迁移技术以缩小仿真与现实的差距。",
      "imageUrls": [
        "https://arxiv.org/html/2511.22963v1/x1.png",
        "https://arxiv.org/html/2511.22963v1/x2.png",
        "https://arxiv.org/html/2511.22963v1/x3.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    },
    {
      "id": "http://arxiv.org/abs/2511.23300",
      "title": "SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot",
      "url": "http://arxiv.org/abs/2511.23300",
      "arxivId": "2511.23300",
      "date": "2025-11-28",
      "authors": "Dzmitry Tsetserukou Team",
      "category": "Manipulation",
      "summary": "本文提出SafeHumanoid系统，解决人形机器人在人机交互中根据场景上下文和人类接近度自适应调节上肢阻抗和速度以实现安全交互的核心问题。关键技术采用VLM-RAG驱动的视觉管道：通过视觉语言模型处理自我中心图像，结合检索增强生成匹配验证场景数据库，经逆运动学映射生成关节阻抗命令。实验在桌面操作任务（如擦拭、物体传递、液体倾倒）中进行，结果表明系统能上下文感知地调整刚度、阻尼和速度，在保持任务成功率的同时提升安全性，但当前推理延迟高达1.4秒，限制了高动态环境的响应性。",
      "detailedSummary": "## 研究背景与动机\n当前，确保安全的人机交互主要依赖于阻抗控制，它允许机器人通过编程设定刚度和阻尼来与环境接触。然而，具有单一固定阻抗设置的机器人难以应对人机交互的多样性。现有的自适应控制器通常基于接触反馈或几何信息（如人与机器人的距离）进行反应式调整，但它们往往在交互发生后才响应，缺乏前瞻性，并且对场景的语义上下文（例如，附近的人是旁观者还是协作者）是“盲目的”。近年来，视觉语言模型和视觉语言动作模型为机器人带来了常识推理能力，能够将高级指令转化为任务序列或关节轨迹，但它们通常专注于位置控制和任务完成，很少考虑在靠近人类时应如何安全地执行动作，即缺乏对阻抗行为的主动调制。\n\n本文针对“如何将高级语义理解与底层安全合规控制（阻抗与速度调度）相结合”这一具体痛点，提出了一个新视角：利用视觉语言模型的语义推理能力，通过检索增强生成技术，在任务执行前根据场景上下文主动调度阻抗和速度参数。本文的核心思路是：构建一个由视觉语言模型驱动语义理解、由检索增强生成进行安全参数映射的管道，为人形机器人的任何轨迹执行添加一个动态的、符合安全标准的合规层。\n\n## 方法详解\nSafeHumanoid的整体框架是一个模块化架构，连接高级语义推理与Unitree G1人形机器人的底层阻抗控制。其输入是机器人的自我中心RGB图像，输出是用于关节级阻抗控制的命令集 `{q_ref, q̇_ref, τ_ff, K_p, K_d}`。管道主要分为四个部分：机器人-服务器通信、基于VLM的自我中心感知、基于RAG的安全参数生成以及机器人控制模块。\n\n![方法框架](https://arxiv.org/html/2511.23300v1/sysss.png)\n> **图2**：SafeHumanoid管道架构。机载PC以50Hz频率流式传输自我中心图像帧并执行阻抗控制，而外部工作站运行Molmo VLM和基于FAISS的RAG系统，将场景语义与从策划的场景数据库中检索出的已验证阻抗和速度参数进行匹配。\n\n**核心模块与技术细节**：\n1.  **通信架构**：采用客户端-服务器模型。机器人机载计算机以1-2Hz的频率将图像通过Wi-Fi发送至外部服务器。服务器处理后将包含29个值（28个关节刚度/阻尼增益 + 1个标称速度）的参数有效载荷返回。机载控制环以50Hz运行，使用最新的有效载荷，并在通信中断时自动回退到保守的备用参数集。\n2.  **感知模块**：使用Molmo-7B模型处理自我中心图像。每个图像帧与一个固定的、任务特定的提示词配对，该提示词强制输出预定义的键值对，包括任务类型、主要物体、物体易碎性、人类存在、障碍物信息等。这种严格的提示策略确保了模块直接生成适合嵌入和检索的结构化语义描述。\n3.  **安全参数生成模块**：采用一个简化的两阶段RAG系统。\n    *   **检索阶段**：将VLM的结构化JSON输出通过`sentence-transformers/all-MiniLM-L6-v2`模型嵌入为384维向量。然后使用FAISS进行精确最近邻搜索，与预先计算的场景数据库嵌入进行比对。数据库包含16个经过实证验证的场景，每个场景关联一组关节阻抗参数。\n    *   **生成阶段**：识别最佳匹配场景后，将其参数有效载荷以标准化的JSON/CSV格式返回。该有效载荷指定了28个阻抗参数（14个关节 × 比例和微分增益）和一个标称关节速度。为确保鲁棒性，若匹配分数低于阈值或出现歧义，系统将拒绝检索并回退到保守的备用参数集。\n4.  **机器人控制模块**：在机器人上集成了逆运动学求解器和关节空间阻抗控制器。IK求解器基于简化模型（锁定腿部和腰部关节）工作，为目标6-DoF末端执行器位姿求解出可行的关节参考位置 `q_ref` 和用于重力补偿的前馈扭矩 `τ_ff`。这些参考值与RAG模块提供的 `K_p`, `K_d` 以及计算出的 `q̇_ref` 一同输入到低层阻抗控制器，控制器按照公式 `τ = K_p (q_ref − q) + K_d (q̇_ref − q̇) + τ_ff` 在50Hz下生成电机扭矩命令，从而调节末端执行器的表观刚度和阻尼。\n\n**创新点**：与现有方法相比，本文的创新在于构建了“语义指导阻抗控制”的桥梁。不同于仅依赖几何信息（如速度与距离监控）或专注于任务级序列生成的VLM应用，SafeHumanoid专门解决了缺失的合规层问题：将语义理解映射到符合安全标准的参数 `{K_p, K_d, v}` 的检索与调度，使得任何来源（如VLA模型、任务规划器）生成的轨迹都能以安全、适应上下文的方式执行。\n\n## 实验与结果\n**实验设置**：实验在配备Intel RealSense RGB-D相机和NVIDIA Jetson Orin NX机载计算机的Unitree G1人形机器人上进行。VLM-RAG管道部署在配备RTX 4090的外部服务器上。评估了包括擦拭、物体交接（立方体、针、酱油瓶）、液体倾倒在内的桌面操作任务。每个任务在“无人”和“有人手在 workspace”两种条件下执行。\n\n**Baseline**：对比基线是固定增益的阻抗控制设置。\n\n**关键实验结果**：系统在所有六项桌面操作任务中均成功根据任务需求和人类存在调整了阻抗参数和标称速度。例如，在表面擦拭任务中，当人手进入场景时，系统立即降低了 `K_p`，提高了 `K_d`，并降低了速度，产生了更柔顺的运动。在针交接任务中，尽管“针”不在场景数据库中，系统仍能泛化到合适的柔顺参数集。任务成功率得以保持，且阻抗调制与语义上下文一致。然而，离板VLM-RAG回路的延迟高达1.4秒，限制了其在高度动态HRI场景中的应用。\n\n![整体流程](https://arxiv.org/html/2511.23300v1/ttt11122.png)\n> **图1**：自我中心感知与语义到安全的管道。左侧展示了机器人在有无人类存在情况下的抓取，显示了`K_p`、`K_d`和`v`的自适应调制。右侧是从相机输入，经过VLM-RAG推理，到G1上半身关节阻抗控制的高级流程。\n\n![定性结果](https://arxiv.org/html/2511.23300v1/exp2.png)\n> **图3**：在易碎物体（液体）交接过程中的语义到安全自适应示例。(a,b) 无人时，系统调度中等阻抗和速度以稳定操作。(c,d) 有人手存在时，刚度`K_p`降低，阻尼`K_d`增加，以确保柔顺交互并防止过大的接触力。\n\n**表1**：六项操作实验的代表性结果。箭头表示相对于基线的调制情况。\n| 任务 | 人类存在 | 调整 | 结果 |\n| :--- | :--- | :--- | :--- |\n| 表面擦拭 | 否 | 基线增益 | 稳定的擦拭动作 |\n| | 是 | `↓ K_p`, `↑ K_d`, `↓ v` | 靠近手部的柔顺擦拭 |\n| 拾取针（OOD）| 否 | 基线增益 | 成功拾取 |\n| | 是 | `↓ K_p`, `↑ K_d`, `↓ v` | 尽管物体不在数据集中，仍实现安全交接 |\n| 拾取立方体（在DB中）| 否 | 基线增益 | 成功拾取 |\n| | 是 | `↓ K_p`, `↑ K_d`, `↓ v` | 安全交接，人手离开后正确恢复 |\n| 酱油瓶 | 是（交接）| `v`: 中速 → 慢速 → 中速 | 通过降低运动速度实现安全倾倒 |\n\n> **表1**总结了关键实验的结果，展示了系统在不同任务和人类存在条件下对阻抗和速度参数的自适应调制能力，即使在遇到数据库外物体时也能保持安全行为。\n\n## 总结与启发\n**核心贡献**：\n1.  提出了一个VLM-RAG控制管道，利用自我中心视觉和任务提示实时选择安全且上下文感知的阻抗与速度参数，首次将语义推理与人体机器人交互中的动态安全参数调度相连接。\n2.  在Unitree G1人形机器人上实现了完整系统演示，展示了高级语义推理与低级上半身运动控制的实际集成。\n3.  通过操作和近人任务实验证明，该方法在保持任务成功的同时，提供了比固定增益基线更安全、更自适应的行为。\n\n**局限性**：论文明确指出当前系统存在三点主要局限：1）离板推理管道引入高达1.4秒的延迟，限制了在动态交互中的应用；2）场景数据库通过手动策划和验证构建，规模小（仅16个场景），多样性和可扩展性有限；3）实验依赖于预定义的末端执行器目标位姿，而非完全自主的动作生成。\n\n**对后续研究的启示**：本文指明了一条通过语义基础实现合规控制的新路径。未来的工作可围绕以下方向展开：通过模型蒸馏、轻量级VLM边缘部署来降低延迟；利用仿真到真实迁移、合成数据增强等技术自动化数据库的构建与扩展；集成深度感知以实现更精确的距离估计和基于距离的精细参数调整；将本语义-安全层与更先进的VLA动作生成模型相结合，实现从感知到安全执行的端到端自主。",
      "imageUrls": [
        "https://arxiv.org/html/2511.23300v1/ttt11122.png",
        "https://arxiv.org/html/2511.23300v1/sysss.png",
        "https://arxiv.org/html/2511.23300v1/exp2.png"
      ],
      "tags": [
        "Manipulation"
      ],
      "updatedAt": "2026-02-11T13:16:00.621Z"
    }
  ]
}