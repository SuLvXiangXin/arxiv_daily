{"generatedAt":"2026-02-12T18:37:30.655Z","source":"https://jiangranlv.github.io/robotics_arXiv_daily/","items":[{"id":"http://arxiv.org/abs/2602.10109","title":"ST4VLA: Spatially Guided Training for Vision-Language-Action Models","arxivId":"2602.10109","date":"2026-02-10","authors":"Jiangmiao Pang Team","category":"Manipulation","summary":"本文提出ST4VLA，旨在解决大型视觉语言模型（VLM）在具体任务中难以将抽象指令转化为低级动作的问题。方法采用**空间引导训练**，包含两个阶段：**空间基础预训练**（通过点、框和轨迹预测学习可迁移的空间先验）和**空间引导动作后训练**（通过空间提示引导动作生成）。实验表明，该方法显著提升了VLA模型的性能，在Google Robot上准确率从66.1提升至84.6，在WidowX Robot上从54.7提升至73.2，并在泛化性和抗干扰性上表现出优势。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.10105","title":"DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos","arxivId":"2602.10105","date":"2026-02-10","authors":"Jiangmiao Pang Team","category":"Manipulation","summary":"论文标题为 \"DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos\"，本文关注机器人相关问题，给出方法与实验结果概述。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.10101","title":"Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction","arxivId":"2602.10101","date":"2026-02-10","authors":"Jiangmiao Pang Team","category":"Manipulation","summary":"本文针对机器人操作中3D感知精度不足的问题，提出Robo3R模型。该方法直接从RGB图像与机器人状态进行前馈式3D重建，核心技术包括：联合推断尺度不变的局部几何与相对相机位姿，通过学习的全局相似变换将其统一到机器人坐标系；采用掩码点云头生成精细点云，以及基于关键点的PnP公式优化相机外参与全局对齐。模型在包含400万帧的合成数据集上训练，实验表明其性能持续优于现有先进重建方法与深度传感器，并在多项下游操作任务中带来性能提升。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.10093","title":"UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking","arxivId":"2602.10093","date":"2026-02-10","authors":"Yao Mu Team","category":"Manipulation","summary":"本文针对机器人接触密集型操作任务中触觉数据获取困难、缺乏统一评估平台的问题，提出统一仿真平台UniVTAC。其核心包括：1）支持三种常用触觉视觉传感器的数据生成平台；2）基于仿真合成数据训练的触觉视觉编码器UniVTAC Encoder；3）包含八个代表性任务的基准测试UniVTAC Benchmark。实验表明，集成该编码器使基准测试平均成功率提升17.1%，真实机器人实验任务成功率提高25%。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.10015","title":"RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments","arxivId":"2602.10015","date":"2026-02-11","authors":"Laxmidhar Behera Team","category":"Manipulation","summary":"论文旨在解决从长视频中分割出机器人可执行的细粒度子任务，以实现安全的人机技能转移。提出了RoboSubtaskNet多阶段框架，结合注意力增强的I3D特征（RGB+光流）与修改的MS-TCN，采用斐波那契膨胀调度捕捉短时转换，并通过交叉熵和时间正则化损失优化分割。引入了RoboSubtask数据集用于医疗和工业场景。实验显示，在GTEA数据集上F1@50达79.5%，编辑准确率88.6%；在自建RoboSubtask数据集上F1@50达94.2%，编辑准确率95.6%。物理实验中，7-DoF机械臂整体任务成功率约91.25%，验证了从感知到执行的可行性。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.10013","title":"Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper","arxivId":"2602.10013","date":"2026-02-10","authors":"Yen-Ling Kuo Team","category":"Manipulation","summary":"本文旨在解决机器人对易损物体（如薯片）进行精细力控操作的难题。研究提出了一种低成本（约150美元）的触觉力控夹爪TF-Gripper（力范围0.45–45N），并设计了RETAF学习框架，将高频触觉力调节与机械臂姿态预测解耦。实验表明，相比位置控制，直接力控显著提升了抓取稳定性与任务成功率；触觉反馈对力调节至关重要，RETAF框架在多种任务中均优于基线方法。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09973","title":"RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation","arxivId":"2602.09973","date":"2026-02-10","authors":"Jiangmiao Pang Team","category":"Manipulation","summary":"论文针对机器人操作中现有数据集成本高、本体依赖性强、多样性不足，导致视觉-语言-动作（VLA）模型泛化困难的核心问题，提出了RoboInter中间表示套件。关键技术包括：RoboInter-Tool半自动标注工具、RoboInter-Data大规模数据集（含超过230k个episodes和10+类中间表示）、RoboInter-VQA具身VQA基准（覆盖29个空间与时间类别）以及RoboInter-VLA集成“计划-然后-执行”框架。该套件通过提供细粒度、多样化的中间表示，为推进鲁棒和可泛化的机器人学习奠定了实用基础。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09940","title":"Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation","arxivId":"2602.09940","date":"2026-02-10","authors":"Laxmidhar Behera Team","category":"Manipulation","summary":"本文提出Instruct2Act框架，旨在解决机器人在资源受限环境下难以理解和执行自由形式人类指令的问题。核心方法包括：1）基于BiLSTM与多头注意力自编码器的指令解析模块，将自然语言指令分解为原子动作序列；2）结合动态自适应轨迹径向网络（DATRN）与YOLOv8视觉分析器的机器人动作网络，生成精确控制轨迹。实验表明，该系统在自定义数据集上子动作预测准确率达91.5%，在四种真实机器人任务中整体成功率为90%，单次子动作推理时间小于3.8秒。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09888","title":"TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback","arxivId":"2602.09888","date":"2026-02-10","authors":"Weiming Zhi Team","category":"Manipulation","summary":"本文针对移动操作机器人全身遥操作中，操作者需同时协调底盘与双臂、并兼顾避障与接触的难题，提出TriPilot-FF系统。其核心创新在于引入脚部操作的踏板，通过低成本激光雷达生成基于障碍物距离的触觉阻力，引导操作者无碰撞移动；同时结合双臂主从遥操作与力反馈，提升接触感知与操作可达性。实验表明，系统能有效辅助操作者完成长时间、需精确底盘协调的任务，并将反馈信号融入ACT策略后进一步提升了性能。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09878","title":"MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation","arxivId":"2602.09878","date":"2026-02-10","authors":"Xiangyu Yue Team","category":"Manipulation","summary":"本文提出MVISTA-4D，旨在解决机器人操作中现有世界模型无法预测完整、几何一致的4D场景动态的问题。方法核心包括：1）一个能从单视角RGBD观测生成任意视角、跨模态一致RGBD的4D世界模型，通过跨视角与跨模态特征融合确保几何对齐；2）测试时动作优化策略，通过生成模型反向传播推断最优轨迹潜在变量，并结合残差逆动力学模型将其转化为精确动作。实验在三个数据集上验证了该方法在4D场景生成与下游操作任务上的优越性能，消融实验明确了关键设计的有效性。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09767","title":"Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning","arxivId":"2602.09767","date":"2026-02-10","authors":"Wei Li Team","category":"Manipulation","summary":"本文针对四足机器人无监督技能发现中，单一策略学习效率低、技能表征易重叠以及奖励信号易被黑客攻击导致技能多样性不足的问题，提出了正交混合专家（OMoE）架构和多判别器框架。OMoE防止行为表征坍塌，使单一策略能掌握广泛运动技能；多判别器在不同观测空间运作以缓解奖励黑客。在Unitree A1机器人上的实验表明，该方法提升了训练效率，并使状态空间覆盖率比基线提升了18.3%。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09638","title":"VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model","arxivId":"2602.09638","date":"2026-02-10","authors":"Hui Xiong Team","category":"Manipulation","summary":"本文解决3D可操作区域（affordance）定位问题，指出现有方法依赖静态语言/图像线索，缺乏动态交互上下文。为此，作者构建了大规模视频数据集VIDA，并提出VideoAfford模型。关键技术包括：利用多模态大语言模型增强分割与推理能力；通过潜在动作编码器从人-物交互视频中提取动态先验；引入空间感知损失以学习3D空间知识。实验表明，该模型显著优于现有方法，并展现出强大的开放世界泛化与推理能力。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09583","title":"Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation","arxivId":"2602.09583","date":"2026-02-10","authors":"Danica Kragic Team","category":"Manipulation","summary":"本文研究如何让机器人在操作可变形物体（如布料）时适应人类个性化偏好。针对偏好难以量化且演示数据有限的问题，作者提出了RKO方法，该方法融合了RPO和KPO框架的优势，能够高效地对预训练的视觉运动扩散策略进行偏好对齐微调。在真实布料折叠任务上的实验表明，采用RKO等偏好对齐策略相比标准微调方法，在任务性能和样本效率上均表现出显著优越性。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09153","title":"SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes","arxivId":"2602.09153","date":"2026-02-09","authors":"Russ Tedrake Team","category":"Manipulation","summary":"本文针对机器人仿真训练中室内场景过于简化、缺乏真实物理复杂性的问题，提出了SceneSmith框架。该框架采用分层智能体架构，通过设计师、评论家和协调员等VLM智能体，分阶段从语言提示生成仿真就绪的3D场景，并整合文本到3D合成与物理属性估计。实验表明，其生成场景的物体数量是基线方法的3-6倍，物体间碰撞率低于2%，物理稳定性达96%，在用户研究中真实性与提示忠实度胜率均超过90%。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09023","title":"TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation","arxivId":"2602.09023","date":"2026-02-09","authors":"Shanghang Zhang Team","category":"Manipulation","summary":"论文提出了一种名为TwinRL-VLA的数字孪生驱动强化学习框架，旨在解决现实世界中机器人操作任务中由于高昂的专家演示成本和不足的实际交互导致的Vision-Language-Action (VLA)模型泛化能力受限的问题。该方法通过智能手机捕捉场景高效重建高保真度的数字孪生环境，实现真实与模拟环境之间的双向传输。在监督微调（SFT）预热阶段，利用数字孪生扩展探索空间，增强数据轨迹分布的支持。基于此初始化，进一步提出了从模拟到现实的引导探索策略，显著提升了VLA模型在实际操作中的性能。实验结果表明，TwinRL-VLA有效提高了在线强化学习的探索效率和探索空间。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09021","title":"$χ_{0}$ : Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies","arxivId":"2602.09021","date":"2026-02-09","authors":"Yibo Yuan Team","category":"Manipulation","summary":"本文针对高可靠性长时机器人操作中的分布不一致性问题，提出了一种资源高效的框架χ₀。该框架通过三个关键技术解决这一问题：（i）模型算术，一种权重空间合并策略，有效吸收不同演示的多样化分布；（ii）阶段优势，一种阶段感知的优势估计器，提供稳定、密集的进度信号；（iii）训练-部署对齐，通过时空增强、启发式DAgger校正和时间块平滑来弥合分布差距。实验表明，该方法使双臂机器人能够协同完成从展平、折叠到挂起不同衣物的长时任务，并表现出高可靠性的自主操作能力。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.09017","title":"Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models","arxivId":"2602.09017","date":"2026-02-09","authors":"Nur Muhammad Mahi Shafiullah Team","category":"Manipulation","summary":"本文提出了一种名为接触锚定策略（Contact-Anchored Policies, CAP）的方法，通过物理接触信息来调节多模态策略。CAP能够在零样本情况下对新对象和场景进行泛化，并且在数据量、计算资源和模型参数方面比前沿行为模型少几个数量级的情况下，仍然在原子技能训练上表现出更好的性能。实验结果表明，CAP方法在多种任务中均优于现有的前沿行为模型。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.08602","title":"Mimic Intent, Not Just Trajectories","arxivId":"2602.08602","date":"2026-02-09","authors":"Panpan Cai Team","category":"Manipulation","summary":"该论文针对模仿学习（IL）在环境变化适应和技能迁移方面的不足，提出了一种新的方法“Mimic Intent, Not just Trajectories”（MINT）。MINT通过多尺度频域分词技术，将行为意图与执行细节分离。具体来说，它使用多尺度粗到细的结构来学习动作标记，其中最粗的标记捕捉低频全局结构，较细的标记编码高频细节。这种方法生成了一个抽象的意图标记，有助于规划和迁移，并生成了多尺度的执行标记，以适应环境动态。实验结果表明，MINT在多个操作基准测试和真实机器人上表现出色，具有更高的成功率、更优的推理效率、更强的抗干扰能力和有效的单次技能迁移能力。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.08245","title":"STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction","arxivId":"2602.08245","date":"2026-02-09","authors":"Guohao Dai Team","category":"Manipulation","summary":"本文提出了一种名为STEP的轻量级时空一致性预测机制，旨在解决扩散策略在机器人视觉运动控制中因迭代去噪导致的高推理延迟问题。STEP通过生成高质量的初始动作，这些动作既接近目标动作的分布又具有时间一致性，从而在不牺牲原始扩散策略生成能力的前提下减少延迟。此外，还引入了速度感知扰动注入机制，自适应地调整动作激励以防止执行停滞。理论分析表明，该预测方法能诱导局部收缩映射，确保在扩散细化过程中动作误差收敛。实验结果表明，在RoboMimic基准测试和实际任务中，STEP仅用2步即可分别比BRIDGER和DDIM提高21.6%和27.5%的成功率，显著提升了推理延迟和成功率之间的帕累托前沿。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.07388","title":"Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation","arxivId":"2602.07388","date":"2026-02-07","authors":"Jianfei Yang Team","category":"Manipulation","summary":"本文针对长时机器人操作任务中由于视觉相似但需不同动作导致的多模态动作歧义问题，提出了一种基于扩散模型的轨迹聚焦策略（TF-DP）。该方法通过显式地将动作生成条件化于机器人的执行历史，利用历史运动轨迹提供阶段感知的上下文信息，并在视觉观察空间中突出与历史运动相关的任务相关区域，从而提高对背景视觉干扰的鲁棒性。实验结果表明，TF-DP在多模态动作歧义任务上比普通扩散策略提升了80.56%的性能，在视觉干扰条件下提升了86.11%的性能，同时仅增加了6.4%的运行时间。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.07341","title":"Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions","arxivId":"2602.07341","date":"2026-02-07","authors":"Zhuo Zou Team","category":"Manipulation","summary":"本文针对灵巧机器人手臂系统的可扩展操作学习问题，提出了一种基于增强现实（AR）的远程人机交互方法，以提高专家演示数据收集效率。该方法分为两个阶段：首先通过行为克隆（BC）方式预训练策略，利用AR系统收集的数据；其次，采用对比学习增强的强化学习（RL）方法进一步优化策略，并设计投影头加速学习过程。实验结果表明，与经典的近端策略优化和软演员-评论家策略相比，该方法不仅显著提高了推理速度，还在完成操作任务的成功率上表现更优。消融研究表明，提出的对比学习强化学习方法有效克服了策略崩溃问题。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.07326","title":"Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing","arxivId":"2602.07326","date":"2026-02-07","authors":"Seokhwan Jeong Team","category":"Manipulation","summary":"该论文探讨了在极简传感条件下实现可靠的多指抓取问题，仅依赖单轴指尖力反馈和关节本体感觉，无需视觉或高分辨率触觉传感器。研究采用了一种高效的教师-学生训练框架，其中强化学习的教师利用模拟中的特权观察生成演示，以提炼出基于Transformer的学生策略，该策略仅使用实际部署中可用的传感模式。实验结果表明，在18个物体上（包括分布内和分布外的情况），该方法实现了98.3%的整体抓取成功率，展示了强大的鲁棒性和泛化能力。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.07082","title":"MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation","arxivId":"2602.07082","date":"2026-02-06","authors":"Wei Gao Team","category":"Manipulation","summary":"论文针对嵌入式AI中视觉语言模型空间推理能力弱的问题，尤其是在跨帧复杂空间关系任务上，提出了MosaicThinker技术。该方法通过迭代构建空间表示，将多帧碎片化信息整合为统一的全局语义地图，并利用视觉提示引导VLM进行推理。实验结果表明，该技术能极大地提高资源受限设备上跨帧空间推理的准确性，适用于各种类型和复杂度的任务。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.06620","title":"Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique","arxivId":"2602.06620","date":"2026-02-06","authors":"Toshiaki Tsuji Team","category":"Manipulation","summary":"本文解决接触式任务中，如何从易得的位置轨迹生成适配特定硬件的精确力命令这一核心问题。提出了力生成模仿学习方法，通过无记忆的力生成模型结合反馈控制机制，将给定位置轨迹映射为力命令。实验表明，该方法确保了反馈控制的稳定性，有效提升了模型对未见位置轨迹的泛化能力，并在真实机器人书写任务中取得了性能改进。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.06512","title":"Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation","arxivId":"2602.06512","date":"2026-02-06","authors":"Heng Tao Shen Team","category":"Manipulation","summary":"这篇论文针对机器人模仿学习中训练数据呈现长尾分布的核心问题，即模型在数据丰富的头部任务上表现良好，但在数据稀缺的尾部任务上泛化能力差。研究发现，传统长尾学习策略（如重采样）对提升尾部任务性能效果有限，其根本原因是数据稀缺损害了策略的空间推理能力。为此，作者提出了“接近阶段增强”（APA）方法，通过将头部任务的知识迁移至尾部任务，无需外部演示数据。实验表明，APA方法在模拟和真实机器人操作任务中均能有效提升尾部任务的性能。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.06508","title":"World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy","arxivId":"2602.06508","date":"2026-02-06","authors":"Mike Zheng Shou Team","category":"Manipulation","summary":"本文提出World-VLA-Loop闭环框架，旨在解决现有视频世界模型在机器人学习中动作跟随精度不足的问题。方法核心包括：1）状态感知视频世界模型，联合预测未来观测与奖励信号，充当高保真交互模拟器；2）引入SANS数据集，利用近成功轨迹提升世界模型的动作-结果对齐；3）构建世界模型与视觉-语言-动作（VLA）策略的协同进化闭环，利用策略失败经验迭代优化模型。实验表明，经过两轮联合优化，真实世界策略成功率提升36.7%，显著减少了物理交互需求。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.05468","title":"TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation","arxivId":"2602.05468","date":"2026-02-05","authors":"Shigeki Sugano Team","category":"Manipulation","summary":"本文针对机器人灵巧抓取操作中难以区分自接触与外部物体接触触觉信号的核心问题，提出TaSA框架。该方法采用两阶段深度预测学习：第一阶段显式学习自接触动力学模型；第二阶段将该模型整合至动作学习中，以衰减自接触信号并突出外部接触。在铅笔芯、硬币、回形针等多种精细插入任务上的实验表明，基于TaSA训练的策略取得了显著高于基线方法的成功率。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.05233","title":"MobileManiBench: Simplifying Model Verification for Mobile Manipulation","arxivId":"2602.05233","date":"2026-02-05","authors":"Baining Guo Team","category":"Manipulation","summary":"本文针对移动操作中视觉-语言-动作模型验证困难的问题，提出“仿真优先”的验证框架。核心是构建了MobileManiBench大规模基准，其关键技术是基于NVIDIA Isaac Sim与强化学习，自动生成包含丰富标注的多样化操作轨迹。该基准包含2种移动机器人、630个物体、5种核心技能，在100个场景中生成30万条轨迹，为系统化研究机器人构型、感知模态与策略架构提供了可控、可扩展的测试平台，并已用于代表性VLA模型的基准测试。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.05049","title":"VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models","arxivId":"2602.05049","date":"2026-02-04","authors":"Dongdong Chen Team","category":"Manipulation","summary":"本文针对视觉-语言-动作模型中视觉条件控制不精确的问题，提出了一种名为“轨迹跟随偏好优化”的新方法。该方法通过优化模型对指定视觉轨迹的跟随偏好，显著提升了基于视觉指令的机器人操作精度。实验表明，VISTA在多个标准任务上实现了性能的大幅提升，成功率平均提高了15%以上。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.04243","title":"Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation","arxivId":"2602.04243","date":"2026-02-04","authors":"Wenzhao Lian Team","category":"Manipulation","summary":"本文针对机器人模仿学习中固定摄像头视角限制适应性的问题，提出MAE-Select框架，实现动态主动视角选择。该方法基于预训练多视角掩码自编码器（MAE）的表征，无需视角标注，即可根据当前视觉与动作信息动态预测并选择信息量最大的下一视角。实验表明，该方法显著提升了单摄像头系统的操作性能，部分任务甚至优于多摄像头配置。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.04231","title":"GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning","arxivId":"2602.04231","date":"2026-02-04","authors":"Hongliang Ren Team","category":"Manipulation","summary":"本文针对语言引导抓取在杂乱、遮挡或低纹理场景中泛化能力差、现有方法多阶段流程导致融合有限的问题，提出GeoLanG框架。该框架基于CLIP架构，统一RGB-D多模态学习，通过深度引导几何模块（DGGM）将深度信息转换为几何先验注入注意力机制，并采用自适应密集通道集成平衡多层特征贡献。实验在OCID-VLG数据集及仿真和真实环境中验证，GeoLanG实现了精确鲁棒的语言引导抓取。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.04228","title":"Reshaping Action Error Distributions for Reliable Vision-Language-Action Models","arxivId":"2602.04228","date":"2026-02-04","authors":"Badong Chen Team","category":"Manipulation","summary":"本文针对连续动作视觉-语言-动作（VLA）模型使用均方误差（MSE）回归时对个体预测误差约束过强、忽略整体误差分布的问题，提出通过重塑动作误差分布来提升模型可靠性。关键技术是引入信息论中的最小误差熵（MEE），设计轨迹级MEE目标及其两个加权变体，与MSE结合进行训练。实验在标准、少样本和噪声设置下，使用LIBERO等模拟基准和真实机器人任务，结果表明该方法能持续提高成功率和鲁棒性，在数据不平衡时增益稳定，且额外训练成本可忽略。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.04213","title":"InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons","arxivId":"2602.04213","date":"2026-02-04","authors":"Reid Simmons Team","category":"Manipulation","summary":"本文提出InterPReT方法，旨在解决非专业用户难以通过传统模仿学习有效教导AI智能体的难题。其核心技术为“交互式策略重构与训练”，允许用户通过指令交互式地更新策略结构，并通过演示优化参数，从而让用户能监控性能并审查决策过程。在一项34人参与的赛车游戏教学用户研究中，与通用模仿学习基线相比，该方法在由非专业用户提供演示并决定训练停止时，能产生更鲁棒的策略，且不损害系统可用性，证明了其对无技术背景用户的适用性。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.03973","title":"VLS: Steering Pretrained Robot Policies via Vision-Language Models","arxivId":"2602.03973","date":"2026-02-03","authors":"Ranjay Krishna Team","category":"Manipulation","summary":"本文提出VLS框架，解决预训练扩散或流匹配策略在测试时遭遇空间配置或任务语义分布偏移时的适应性问题。核心方法为无需训练的推理时引导技术：利用视觉语言模型合成轨迹可微的奖励函数，在去噪采样过程中直接调整冻结策略的动作分布，无需修改参数。实验表明，VLS在CALVIN和LIBERO-PRO基准上分别取得31%和13%的性能提升，并在真实机器人上验证了对空间与语义偏移的鲁棒适应性。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.03668","title":"MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction","arxivId":"2602.03668","date":"2026-02-03","authors":"Jungwoo Lee Team","category":"Manipulation","summary":"本文提出MVP-LAM模型，旨在解决从无标注视频中学习潜在动作时，因视角变化引入噪声、导致潜在动作与真实动作关联性弱的核心问题。其关键技术是利用时间同步的多视角视频，通过跨视角重建目标进行训练：从一个视角推断的潜在动作必须能预测另一视角的未来状态，从而剥离视角特异性干扰。实验表明，在Bridge V2数据集上，MVP-LAM学得的潜在动作与真实动作互信息更高，动作预测性能更优（包括分布外评估）。使用该潜在动作预训练VLA模型，在SIMPLER和LIBERO-Long基准上提升了下游操作性能。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.02839","title":"Language Movement Primitives: Grounding Language Models in Robot Motion","arxivId":"2602.02839","date":"2026-02-02","authors":"Simon Stepputtis Team","category":"Manipulation","summary":"本文解决机器人根据自然语言指令执行新操作任务时，高层语义推理与底层运动控制脱节的核心问题。提出语言运动基元（LMP）框架，其关键技术是将大视觉语言模型（VLM）的推理能力，通过动态运动基元（DMP）的参数化进行落地，从而将语言指令直接转化为连续、稳定的机器人轨迹。在20个真实桌面操作任务上的实验表明，该方法实现了零样本操作，任务成功率高达80%，显著优于基线方法（31%）。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.10983","title":"Scaling World Model for Hierarchical Manipulation Policies","arxivId":"2602.10983","date":"2026-02-11","authors":"Xinghang Li Team","category":"Manipulation","summary":"本文针对视觉-语言-动作模型在分布外场景中泛化能力弱、依赖大量真实机器人数据的问题，提出分层框架VISTA。其核心是使用大规模预训练世界模型作为高层规划器，将操作任务分解为带目标图像的子任务序列；低层VLA策略依据文本与视觉引导生成动作序列。目标图像提供了具体视觉与物理约束，显著提升了泛化能力。实验表明，在相同结构VLA中，借助世界模型引导，其在未见场景下的性能从14%大幅提升至69%。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.10793","title":"Semi-Supervised Cross-Domain Imitation Learning","arxivId":"2602.10793","date":"2026-02-11","authors":"Ping-Chun Hsieh Team","category":"Manipulation","summary":"本文提出半监督跨域模仿学习（SS-CDIL），旨在解决跨域模仿学习中目标域专家数据稀缺且收集成本高的问题。方法基于离线数据，仅需少量目标专家演示与未标记轨迹，通过设计跨域损失函数学习域间状态-动作映射，并引入自适应权重平衡源域与目标域知识。在MuJoCo与Robosuite上的实验表明，该方法较基线模型性能持续提升，实现了稳定且数据高效的政策学习。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.11018","title":"OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories","arxivId":"2602.11018","date":"2026-02-11","authors":"Balaraman Ravindran Team","category":"Manipulation","summary":"本文提出OSIL算法，解决离线安全模仿学习问题：在缺乏显式安全成本标注的演示数据中，学习同时满足安全约束与奖励最大化的策略。核心技术是：1）将问题建模为约束马尔可夫决策过程；2）从“非偏好轨迹”（即包含不安全行为的演示）中推断安全约束，而非依赖人工标注的成本函数；3）推导奖励目标的下界并学习成本模型来估计非偏好行为可能性。实验表明，OSIL能学习到高回报、低成本的策略，在速度约束和导航任务上，其性能比最佳基线提升约2.8倍。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.10717","title":"Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation","arxivId":"2602.10717","date":"2026-02-11","authors":"Yanwei Fu Team","category":"Manipulation","summary":"本文针对指令驱动的机器人操作任务，提出了一种学习视频世界模型的方法，以解决现有系统缺乏环境演化预测能力、导致操作错误与低效的问题。关键技术包括：1）选用并适配稳健的视频生成模型以保障预测可靠性；2）采用对抗蒸馏实现快速、少步数的视频生成；3）训练动作模型，结合生成视频与真实观测以修正空间误差。实验表明，该方法能生成时空一致、空间准确的视频预测，显著提升了体现一致性、空间指代能力与任务完成率。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.11150","title":"YOR: Your Own Mobile Manipulator for Generalizable Robotics","arxivId":"2602.11150","date":"2026-02-11","authors":"Zichen Jeff Cui Team","category":"Manipulation","summary":"本文针对移动操作机器人平台成本高、功能受限且难以扩展的核心问题，提出了YOR——一个开源、低成本的移动操作机器人。其关键技术包括集成全向基座、伸缩垂直升降机和双臂抓取器，实现全身移动与操作；设计强调模块化、使用现成组件，组装简便，物料成本低于1万美元。实验表明，YOR能成功完成需协调全身控制、双手操作及自主导航的任务，为移动操作研究提供了经济高效的竞争性平台。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"},{"id":"http://arxiv.org/abs/2602.10594","title":"Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning","arxivId":"2602.10594","date":"2026-02-11","authors":"Penny Sweetser Team","category":"Manipulation","summary":"本文针对模仿学习需要大量演示、成本高昂的问题，以及现有流（flow）方法在利用人类视频时无法描述交互运动、泛化能力有限的缺陷，提出了SFCrP方法。该方法包含SFCr场景流预测模型（用于跨体现学习，从人机视频预测点轨迹）和FCrP策略（以流和裁剪点云为条件，遵循流运动并调整精确动作）。实验表明，该方法在多种真实任务中优于最先进基线，并展现出对仅见于人类视频的场景的强空间和实例泛化能力。","tags":["Manipulation"],"updatedAt":"2026-02-12T18:37:30.655Z"}]}