const fs = require("fs");
const path = require("path");
const { marked } = require("marked");
const pdfParse = require("pdf-parse");
const { SOURCE_URL, OUTPUT_PATH, MAX_ITEMS } = require("./config");
const { runConcurrent, filterByRelevance, generateSummary, generateDetailedSummary } = require("./llm");
const { normalizeArxivId, normalizeArxivInput } = require("./arxiv");

/* â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const stripTags = (value) =>
  value
    .replace(/<script[\s\S]*?<\/script>/gi, " ")
    .replace(/<style[\s\S]*?<\/style>/gi, " ")
    .replace(/<[^>]+>/g, " ")
    .replace(/&amp;/g, "&")
    .replace(/&lt;/g, "<")
    .replace(/&gt;/g, ">")
    .replace(/&quot;/g, '"')
    .replace(/&#39;/g, "'")
    .replace(/\s+/g, " ")
    .trim();

const readExisting = (filePath) => {
  if (!fs.existsSync(filePath)) return { items: [] };
  try {
    return JSON.parse(fs.readFileSync(filePath, "utf-8"));
  } catch {
    console.warn("Failed to parse existing data, will regenerate.");
    return { items: [] };
  }
};

/* â”€â”€ parse table rows from source page â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const extractCurrentCategory = (html, pos) => {
  const before = html.slice(0, pos);
  const headings = [...before.matchAll(/<h2[^>]*>([\s\S]*?)<\/h2>/gi)];
  if (!headings.length) return "";
  return stripTags(headings[headings.length - 1][1]);
};

const extractItems = (html) => {
  const rowRegex = /<tr>([\s\S]*?)<\/tr>/gi;
  const items = [];
  const seen = new Set();

  let match;
  while ((match = rowRegex.exec(html)) !== null) {
    const row = match[1];
    const cells = [...row.matchAll(/<td[^>]*>([\s\S]*?)<\/td>/gi)].map((m) =>
      m[1]
    );
    if (cells.length < 4) continue;

    // Col 0: date, Col 1: title (<strong>), Col 2: authors, Col 3: PDF link
    const titleRaw = stripTags(cells[1]);
    const dateRaw = stripTags(cells[0]);
    const authorsRaw = stripTags(cells[2]);

    const linkMatch = cells[3].match(
      /<a[^>]*href=["'](https?:\/\/arxiv\.org\/abs\/[^"']+)["']/i
    );
    const url = linkMatch ? linkMatch[1] : null;
    if (!url || seen.has(url)) continue;

    const category = extractCurrentCategory(html, match.index);

    items.push({
      url,
      title: titleRaw || "arXiv Paper",
      date: dateRaw,
      authors: authorsRaw,
      category,
    });
    seen.add(url);

    if (items.length >= MAX_ITEMS) break;
  }

  return items;
};

/* â”€â”€ fetch arXiv HTML full text and images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const fetchArxivHtmlLink = async (absUrl) => {
  try {
    const r = await fetch(absUrl, { redirect: "follow" });
    if (!r.ok) return null;
    const html = await r.text();
    // Look for: <a href="https://arxiv.org/html/XXXX" ... id="latexml-download-link">
    const m = html.match(
      /<a[^>]*href=["'](https:\/\/arxiv\.org\/html\/[^"']+)["'][^>]*id=["']latexml-download-link["']/i
    );
    if (m) return m[1];
    // fallback: any link to arxiv.org/html/
    const m2 = html.match(
      /href=["'](https:\/\/arxiv\.org\/html\/[^"']+)["']/i
    );
    return m2 ? m2[1] : null;
  } catch {
    return null;
  }
};

const fetchArxivContent = async (htmlUrl) => {
  try {
    const r = await fetch(htmlUrl, { redirect: "follow" });
    if (!r.ok) return { fullText: "", imageUrls: [] };
    const html = await r.text();

    // Extract all image URLs (relative â†’ absolute)
    const baseUrl = htmlUrl.endsWith("/") ? htmlUrl : htmlUrl + "/";
    const imgMatches = [...html.matchAll(/<img[^>]*src=["']([^"']+)["'][^>]*/gi)];
    const imageUrls = imgMatches
      .map((m) => m[1])
      .filter((src) => !src.startsWith("data:"))
      .map((src) => {
        if (src.startsWith("http")) return src;
        return new URL(src, baseUrl).href;
      });

    // Extract text content
    const fullText = html
      .replace(/<script[\s\S]*?<\/script>/gi, " ")
      .replace(/<style[\s\S]*?<\/style>/gi, " ")
      .replace(/<nav[\s\S]*?<\/nav>/gi, " ")
      .replace(/<header[\s\S]*?<\/header>/gi, " ")
      .replace(/<footer[\s\S]*?<\/footer>/gi, " ")
      .replace(/<[^>]+>/g, " ")
      .replace(/&amp;/g, "&")
      .replace(/&lt;/g, "<")
      .replace(/&gt;/g, ">")
      .replace(/&quot;/g, '"')
      .replace(/&#39;/g, "'")
      .replace(/\s+/g, " ")
      .trim();

    return { fullText, imageUrls };
  } catch (e) {
    console.warn(`Failed to fetch arXiv HTML: ${e.message}`);
    return { fullText: "", imageUrls: [] };
  }
};

const fetchAbsPageMeta = async (absUrl) => {
  try {
    const r = await fetch(absUrl, { redirect: "follow" });
    if (!r.ok) return null;
    const html = await r.text();

    const titleM = html.match(/<meta\s+name=["']citation_title["']\s+content=["']([^"']+)["']/i);
    const title = titleM ? titleM[1] : "arXiv Paper";

    const authorMs = [...html.matchAll(/<meta\s+name=["']citation_author["']\s+content=["']([^"']+)["']/gi)];
    const authors = authorMs.map((m) => m[1]).join(", ");

    const dateM = html.match(/<meta\s+name=["']citation_date["']\s+content=["']([^"']+)["']/i);
    const date = dateM ? dateM[1] : "";

    const catM = html.match(/primary-subject[^>]*>([^<]+)</i);
    const category = catM ? catM[1].trim() : "Robotics";

    return { title, authors, date, category };
  } catch {
    return null;
  }
};

/* â”€â”€ build detail HTML page for a paper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const buildDetailPage = (paper) => {
  const bodyHtml = marked.parse(paper.detailedSummary || "", {
    breaks: true,
    gfm: true,
  });

  return `<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>${paper.title} - Robotics arXiv Daily</title>
  <link rel="preconnect" href="https://fonts.googleapis.com"/>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="../assets/styles.css"/>
  <link rel="stylesheet" href="../assets/detail.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/>
</head>
<body>
  <div class="bg-orbit"></div>
  <header class="site-header">
    <div class="brand">
      <a href="../index.html" class="back-link">â† è¿”å›åˆ—è¡¨</a>
    </div>
  </header>
  <main class="detail-main">
    <article class="detail-card">
      <span class="detail-category">${paper.category || "Robotics"}</span>
      <h1>${paper.title}</h1>
      <div class="detail-meta">
        <span>arXiv: <a href="${paper.url}" target="_blank" rel="noreferrer">${paper.arxivId}</a></span>
        <span>ä½œè€…: ${paper.authors || "--"}</span>
        <span>æ—¥æœŸ: ${paper.date || "--"}</span>
      </div>
      <section class="detail-body">
        <h2>ğŸ“ è¯¦ç»†è§£è¯»</h2>
        ${bodyHtml || "<p>æ‘˜è¦ç”Ÿæˆä¸­...</p>"}
      </section>
      <section class="detail-tldr">
        <h2>ğŸ’¡ ä¸€å¥è¯æ€»ç»“</h2>
        <p>${paper.summary || "æš‚æ— "}</p>
      </section>
      <div class="detail-actions">
        <a href="${paper.url}" target="_blank" rel="noreferrer" class="btn">æŸ¥çœ‹ arXiv åŸæ–‡</a>
        <a href="../index.html" class="btn btn-outline">è¿”å›åˆ—è¡¨</a>
      </div>
    </article>
  </main>
  <footer class="site-footer">
    <span>æ•°æ®æŠ“å–æ¥æºäº Robotics arXiv Daily</span>
    <span>æœ¬é¡µç”± GitHub Actions å®šæ—¶æ›´æ–°</span>
  </footer>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false}],throwOnError:false})"></script>
</body>
</html>`;
};

/* â”€â”€ enrich items with summaries (incremental save) â”€â”€â”€â”€ */

const SUMMARY_CONCURRENCY = 16; // parallel paper processing

const buildOutput = async (items, existing, outputPath, pagesDir) => {
  const existingMap = new Map(
    (existing.items || []).map((item) => [item.url, item])
  );

  const generatedAt = new Date().toISOString();

  // Separate cached vs new
  const cached = [];
  const toProcess = [];
  for (const item of items) {
    const previous = existingMap.get(item.url);
    if (previous?.summary && previous?.detailedSummary) {
      cached.push({
        ...previous,
        title: item.title,
        date: item.date,
        authors: item.authors,
        category: item.category,
        updatedAt: generatedAt,
      });
    } else {
      toProcess.push({ item, previous });
    }
  }

  console.log(`  Cached (skip): ${cached.length}, Need LLM: ${toProcess.length}`);

  // All completed papers (cached + newly done) â€” shared across workers
  const allDone = [...cached];

  // Helper: save current progress to disk
  const saveProgress = () => {
    const payload = { generatedAt, source: SOURCE_URL, items: [...allDone] };
    fs.writeFileSync(outputPath, JSON.stringify(payload, null, 2));
  };

  // Save cached papers immediately
  fs.mkdirSync(path.dirname(outputPath), { recursive: true });
  fs.mkdirSync(pagesDir, { recursive: true });
  saveProgress();

  if (toProcess.length === 0) {
    return { generatedAt, source: SOURCE_URL, items: allDone };
  }

  let doneCount = 0;
  const total = toProcess.length;

  const tasks = toProcess.map(({ item, previous }) => async () => {
    const arxivId = normalizeArxivId(item.url);

    // Fetch full text from arXiv HTML page
    const absUrl = item.url.replace("http://", "https://");
    const htmlLink = await fetchArxivHtmlLink(absUrl);
    let fullText = "";
    let imageUrls = [];
    let noHtmlAvailable = false;
    if (htmlLink) {
      const content = await fetchArxivContent(htmlLink);
      fullText = content.fullText;
      imageUrls = content.imageUrls;
    } else {
      noHtmlAvailable = true;
    }

    const NO_HTML_MSG = "ç›®æ ‡ä¸å­˜åœ¨htmlç•Œé¢ï¼Œè·å–å¤±è´¥â€¦â€¦";

    const summary = noHtmlAvailable
      ? NO_HTML_MSG
      : (previous?.summary ||
        (await generateSummary({ title: item.title, fullText })));
    const detailedSummary = noHtmlAvailable
      ? NO_HTML_MSG
      : (previous?.detailedSummary ||
        (await generateDetailedSummary({ title: item.title, fullText, imageUrls })));
    const tags = previous?.tags || (item.category ? [item.category] : []);

    const paper = {
      id: item.url,
      title: item.title,
      url: item.url,
      arxivId,
      date: item.date,
      authors: item.authors,
      category: item.category,
      summary,
      detailedSummary,
      imageUrls,
      tags,
      updatedAt: generatedAt,
    };

    // Immediately save this paper
    allDone.push(paper);
    saveProgress();

    // Generate detail page immediately
    const pageHtml = buildDetailPage(paper);
    const fileName = `${arxivId.replace(/[\/\\]/g, "_")}.html`;
    fs.writeFileSync(path.join(pagesDir, fileName), pageHtml);

    doneCount++;
    const pct = ((doneCount / total) * 100).toFixed(0);
    console.log(`  [Summary ${doneCount}/${total} ${pct}%] ${item.title}  (text:${fullText.length} imgs:${imageUrls.length}) âœ“ saved`);

    return paper;
  });

  console.log(`  Starting ${total} summaries with concurrency=${SUMMARY_CONCURRENCY}...`);
  await runConcurrent(tasks, SUMMARY_CONCURRENCY);

  return { generatedAt, source: SOURCE_URL, items: allDone };
};

const saveIndex = (payload) => {
  const indexItems = payload.items.map((p) => ({
    id: p.id,
    title: p.title,
    arxivId: p.arxivId,
    date: p.date,
    authors: p.authors,
    category: p.category,
    summary: p.summary,
    tags: p.tags,
    updatedAt: p.updatedAt,
  }));
  const indexPath = path.resolve(__dirname, "..", "data", "papers-index.json");
  fs.writeFileSync(
    indexPath,
    JSON.stringify({ generatedAt: payload.generatedAt, source: payload.source, items: indexItems })
  );
  console.log(`Index file saved to data/papers-index.json (${(Buffer.byteLength(JSON.stringify({generatedAt: payload.generatedAt, source: payload.source, items: indexItems})) / 1024).toFixed(0)} KB)`);
};

/* â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const main = async () => {
  const outputPath = path.resolve(__dirname, "..", OUTPUT_PATH);
  const pagesDir = path.resolve(__dirname, "..", "papers");
  const forceArxivInput = (process.env.FORCE_ARXIV_INPUT || "").trim();
  const forceAddSingle = /^(1|true|yes)$/i.test(String(process.env.FORCE_ADD_SINGLE || ""));

  if (forceArxivInput && forceAddSingle) {
    console.log(`\n========== Force Add æ¨¡å¼ ==========`);

    const normalized = normalizeArxivInput(forceArxivInput);
    if (!normalized) {
      throw new Error(`Invalid FORCE_ARXIV_INPUT: ${forceArxivInput}`);
    }

    const existing = readExisting(outputPath);
    const existingById = new Map(
      (existing.items || [])
        .map((item) => {
          const id = normalizeArxivId(item.arxivId || item.url || item.id || "");
          return id ? [id, item] : null;
        })
        .filter(Boolean)
    );

    if (existingById.has(normalized.arxivId)) {
      console.log(`è®ºæ–‡å·²å­˜åœ¨: ${normalized.arxivId}ï¼Œè·³è¿‡æ–°å¢ã€‚`);
      return;
    }

    const meta = await fetchAbsPageMeta(normalized.absUrl);
    if (!meta) {
      throw new Error(`Failed to fetch metadata from ${normalized.absUrl}`);
    }

    const forcedItem = {
      url: normalized.absUrl,
      title: meta.title,
      date: meta.date,
      authors: meta.authors,
      category: meta.category,
    };

    console.log(`å‡†å¤‡å¼ºåˆ¶æ–°å¢è®ºæ–‡: ${forcedItem.title}`);

    const payload = await buildOutput([forcedItem], existing, outputPath, pagesDir);
    const oldPapers = (existing.items || []).filter((paper) => {
      const id = normalizeArxivId(paper.arxivId || paper.url || paper.id || "");
      return id !== normalized.arxivId;
    });

    payload.items = [...payload.items, ...oldPapers];
    fs.writeFileSync(outputPath, JSON.stringify(payload, null, 2));
    saveIndex(payload);

    console.log(`\n========== å®Œæˆ ==========`);
    console.log(`Force add å®Œæˆ: ${normalized.arxivId}`);
    return;
  }

  if (forceArxivInput && !forceAddSingle) {
    console.log("FORCE_ARXIV_INPUT å·²æä¾›ï¼Œä½† FORCE_ADD_SINGLE æœªå¼€å¯ï¼Œå¿½ç•¥å•ç¯‡å¼ºåˆ¶å…¥åº“å‚æ•°ã€‚");
  }

  const response = await fetch(SOURCE_URL);
  if (!response.ok) {
    throw new Error(`Failed to fetch source: ${response.status}`);
  }

  const html = await response.text();
  const allItems = extractItems(html);

  console.log(`\n========== Phase 1: è§£æ ==========`);
  console.log(`ä»æºé¡µé¢è§£æåˆ° ${allItems.length} ç¯‡è®ºæ–‡`);

  const existing = readExisting(outputPath);

  // â”€â”€ incremental filtering with cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  console.log(`\n========== Phase 2: è¿‡æ»¤ ==========`);
  const cachePath = path.resolve(__dirname, "..", "data", "filter_cache.json");
  let filterCache = { kept: [], rejected: [] };
  try {
    if (fs.existsSync(cachePath)) {
      filterCache = JSON.parse(fs.readFileSync(cachePath, "utf-8"));
    }
  } catch { /* ignore */ }

  const knownUrls = new Set([
    ...(filterCache.kept || []),
    ...(filterCache.rejected || []),
  ]);
  const existingUrls = new Set((existing.items || []).map((i) => i.url));

  // Split into already-known and truly-new papers
  const alreadyKept = [];
  const newItems = [];
  for (const item of allItems) {
    if (existingUrls.has(item.url) || (filterCache.kept || []).includes(item.url)) {
      alreadyKept.push(item);
    } else if ((filterCache.rejected || []).includes(item.url)) {
      // previously rejected â†’ skip
    } else {
      newItems.push(item);
    }
  }

  console.log(`Already processed: ${alreadyKept.length}, New to filter: ${newItems.length}`);

  let newlyKept = [];
  if (newItems.length > 0) {
    console.log(`Filtering ${newItems.length} new papers by relevance...`);
    newlyKept = await filterByRelevance(newItems);
    console.log(`${newlyKept.length} / ${newItems.length} new papers passed filter`);

    // Update cache
    const newlyKeptUrls = new Set(newlyKept.map((i) => i.url));
    filterCache.kept = [...new Set([...(filterCache.kept || []), ...newlyKeptUrls])];
    filterCache.rejected = [
      ...new Set([
        ...(filterCache.rejected || []),
        ...newItems.filter((i) => !newlyKeptUrls.has(i.url)).map((i) => i.url),
      ]),
    ];
  } else {
    console.log(`No new papers to filter`);
  }

  const items = [...alreadyKept, ...newlyKept];
  console.log(`è¿‡æ»¤å®Œæˆï¼Œå…± ${items.length} ç¯‡æ¥è‡ªæºé¡µé¢çš„ç›¸å…³è®ºæ–‡`);

  // Save filter cache immediately after filtering
  fs.mkdirSync(path.dirname(outputPath), { recursive: true });
  fs.writeFileSync(cachePath, JSON.stringify(filterCache, null, 2));
  console.log(`è¿‡æ»¤ç¼“å­˜å·²ä¿å­˜åˆ° data/filter_cache.json`);

  // â”€â”€ Merge back old papers not on the current source page â”€â”€
  const currentUrls = new Set(items.map((i) => i.url));
  const oldPapers = (existing.items || []).filter((p) => !currentUrls.has(p.url || p.id));
  console.log(`ä¿ç•™ ${oldPapers.length} ç¯‡å†å²è®ºæ–‡ï¼ˆä¸åœ¨å½“å‰æºé¡µé¢ä¸Šï¼‰`);

  console.log(`\n========== Phase 3: ç”Ÿæˆæ‘˜è¦ ==========`);
  const payload = await buildOutput(items, existing, outputPath, pagesDir);

  // Append old papers that are no longer on the source page
  payload.items = [...payload.items, ...oldPapers];

  // Final save (ensure consistency)
  fs.writeFileSync(outputPath, JSON.stringify(payload, null, 2));

  // Generate lightweight index for fast main-page loading
  saveIndex(payload);

  console.log(`\n========== å®Œæˆ ==========`);
  console.log(
    `Saved ${payload.items.length} papers to ${OUTPUT_PATH} + detail pages`
  );
};

main().catch((error) => {
  console.error(error);
  process.exit(1);
});
